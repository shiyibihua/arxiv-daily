---
layout: default
title: Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment
---

# Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment

**arXiv**: [2512.09573v1](https://arxiv.org/abs/2512.09573) | [PDF](https://arxiv.org/pdf/2512.09573.pdf)

**ä½œè€…**: Yuan Li, Zitang Sun, Yen-Ju Chen, Shin'ya Nishida

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä½Žå±‚å¤±çœŸæ„ŸçŸ¥ä»»åŠ¡ä»¥å¢žå¼ºåŸºäºŽè§†è§‰è¯­è¨€æ¨¡åž‹çš„å›¾åƒè´¨é‡è¯„ä¼°**

**å…³é”®è¯**: `å›¾åƒè´¨é‡è¯„ä¼°` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ä½Žå±‚è§†è§‰æ„ŸçŸ¥` `è§†è§‰è¯­è¨€å¯¹é½` `å¤±çœŸæ£€æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨å›¾åƒè´¨é‡è¯„ä¼°ä¸­éš¾ä»¥å¯é æ£€æµ‹ä½Žå±‚å¤±çœŸï¼Œå¦‚æ¨¡ç³Šå’Œå™ªå£°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç»„ä»¶åˆ†æžæ­ç¤ºè§†è§‰è¯­è¨€å¯¹é½é˜¶æ®µå‰Šå¼±ä½Žå±‚ç‰¹å¾ï¼Œå¹¶å¼•å…¥è§†è§‰ç¼–ç å™¨ä¸“ç”¨çº¦æŸã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç»„ä»¶å¾®è°ƒåŽï¼Œå¤±çœŸè¯†åˆ«å‡†ç¡®çŽ‡ä»Ž14.92%æå‡è‡³84.43%ï¼Œæé«˜è¯„ä¼°ä¸€è‡´æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.

