---
layout: default
title: Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning
---

# Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning

**arXiv**: [2512.09706v1](https://arxiv.org/abs/2512.09706) | [PDF](https://arxiv.org/pdf/2512.09706.pdf)

**ä½œè€…**: Kaichen He, Zihao Wang, Muyao Li, Anji Liu, Yitao Liang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCrossAgentç»Ÿä¸€æ™ºèƒ½ä½“æ¨¡åž‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æŽŒæ¡è·¨å±‚çº§å¼‚æž„åŠ¨ä½œç©ºé—´ä»¥æå‡åŠ¨æ€çŽ¯å¢ƒé€‚åº”æ€§ã€‚**

**å…³é”®è¯**: `æ™ºèƒ½ä½“æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `å¼‚æž„åŠ¨ä½œç©ºé—´` `è‡ªé€‚åº”äº¤äº’` `MinecraftçŽ¯å¢ƒ` `ç­–ç•¥ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ™ºèƒ½ä½“å—é™äºŽé™æ€é¢„å®šä¹‰åŠ¨ä½œç©ºé—´ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€çŽ¯å¢ƒä¸­äº¤äº’ç²’åº¦å˜åŒ–ã€‚
2. é‡‡ç”¨ç›‘ç£å¾®è°ƒä¸Žå¤šè½®ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œå®žçŽ°è‡ªé€‚åº”åŠ¨ä½œåˆ‡æ¢ï¼Œæ— éœ€äººå·¥è§„åˆ™ã€‚
3. åœ¨Minecraftå¼€æ”¾ä¸–ç•Œ800å¤šä¸ªä»»åŠ¡ä¸ŠéªŒè¯ï¼Œæ€§èƒ½è¶…è¶Šå›ºå®šåŠ¨ä½œåŸºçº¿ï¼Œå±•çŽ°ä¼˜è¶Šæ³›åŒ–ä¸Žé•¿æ—¶æŽ¨ç†æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA

