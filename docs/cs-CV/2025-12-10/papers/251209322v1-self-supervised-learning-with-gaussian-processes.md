---
layout: default
title: Self-Supervised Learning with Gaussian Processes
---

# Self-Supervised Learning with Gaussian Processes

**arXiv**: [2512.09322v1](https://arxiv.org/abs/2512.09322) | [PDF](https://arxiv.org/pdf/2512.09322.pdf)

**ä½œè€…**: Yunshan Duan, Sinead Williamson

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ–¯è¿‡ç¨‹è‡ªç›‘ç£å­¦ä¹ ä»¥è§£å†³è¡¨ç¤ºç©ºé—´å¹³æ»‘æ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `é«˜æ–¯è¿‡ç¨‹` `è¡¨ç¤ºå­¦ä¹ ` `ä¸ç¡®å®šæ€§é‡åŒ–` `æ ¸æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªç›‘ç£å­¦ä¹ ä¸­ç”Ÿæˆç›¸ä¼¼æ ·æœ¬å¯¹å›°éš¾ï¼Œä¸”ç¼ºä¹ä¸ç¡®å®šæ€§é‡åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨é«˜æ–¯è¿‡ç¨‹å…ˆéªŒï¼Œé€šè¿‡åæ–¹å·®å‡½æ•°è‡ªç„¶èšåˆç›¸ä¼¼è¡¨ç¤ºï¼Œæ— éœ€æ˜¾å¼æ­£æ ·æœ¬ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆ†ç±»å’Œå›žå½’ä»»åŠ¡ä¸­ï¼ŒGPSSLåœ¨å‡†ç¡®æ€§ã€ä¸ç¡®å®šæ€§é‡åŒ–å’Œè¯¯å·®æŽ§åˆ¶æ–¹é¢ä¼˜äºŽä¼ ç»Ÿæ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Self supervised learning (SSL) is a machine learning paradigm where models learn to understand the underlying structure of data without explicit supervision from labeled samples. The acquired representations from SSL have demonstrated useful for many downstream tasks including clustering, and linear classification, etc. To ensure smoothness of the representation space, most SSL methods rely on the ability to generate pairs of observations that are similar to a given instance. However, generating these pairs may be challenging for many types of data. Moreover, these methods lack consideration of uncertainty quantification and can perform poorly in out-of-sample prediction settings. To address these limitations, we propose Gaussian process self supervised learning (GPSSL), a novel approach that utilizes Gaussian processes (GP) models on representation learning. GP priors are imposed on the representations, and we obtain a generalized Bayesian posterior minimizing a loss function that encourages informative representations. The covariance function inherent in GPs naturally pulls representations of similar units together, serving as an alternative to using explicitly defined positive samples. We show that GPSSL is closely related to both kernel PCA and VICReg, a popular neural network-based SSL method, but unlike both allows for posterior uncertainties that can be propagated to downstream tasks. Experiments on various datasets, considering classification and regression tasks, demonstrate that GPSSL outperforms traditional methods in terms of accuracy, uncertainty quantification, and error control.

