---
layout: default
title: Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane
---

# Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane

**arXiv**: [2512.09343v1](https://arxiv.org/abs/2512.09343) | [PDF](https://arxiv.org/pdf/2512.09343.pdf)

**ä½œè€…**: Ashik E Rasul, Humaira Tasnim, Ji Yu Kim, Young Hyun Lim, Scott Schmitz, Bruce W. Jo, Hyung-Jin Yoon

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§å››è½´å›ºå®šç¿¼ç³»ç»Ÿï¼Œç”¨äºŽåŠ¨æ€æ— GPSçŽ¯å¢ƒä¸‹çš„é«˜æ•ˆè§†è§‰è‡ªä¸»ç€é™†ä¸Žè§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡ã€‚**

**å…³é”®è¯**: `å››è½´å›ºå®šç¿¼` `è§†è§‰è‡ªä¸»ç€é™†` `è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡` `è¾¹ç¼˜AI` `éžç»“æž„åŒ–çŽ¯å¢ƒ` `é•¿ç¨‹æ— äººæœº`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿ç¨‹å››è½´å›ºå®šç¿¼åœ¨æ— GPSæˆ–æ‚ä¹±çŽ¯å¢ƒä¸­ï¼Œé¢ä¸´ç€é™†åŒºéžç»“æž„åŒ–ã€è¾¹ç¼˜AIèµ„æºå—é™åŠé«˜æƒ¯æ€§é£žè¡Œç‰¹æ€§çš„æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼€å‘è½»é‡çº§ç¡¬ä»¶å¹³å°ä¸Žä¼˜åŒ–éƒ¨ç½²æ¡†æž¶ï¼Œç»“åˆæ·±åº¦ç¥žç»ç½‘ç»œå­¦ä¹ ç€é™†ç‰¹å¾ï¼Œå®žçŽ°å®žæ—¶æ£€æµ‹ä¸ŽæŽ§åˆ¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæœªçŸ¥å…·ä½“å®žéªŒæ•°æ®ï¼Œä½†ç³»ç»Ÿä¸ºåŠ¨æ€æ— GPSçŽ¯å¢ƒä¸­çš„è‡ªä¸»ç€é™†éƒ¨ç½²å¥ å®šäº†åŸºç¡€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> QuadPlanes combine the range efficiency of fixed-wing aircraft with the maneuverability of multi-rotor platforms for long-range autonomous missions. In GPS-denied or cluttered urban environments, perception-based landing is vital for reliable operation. Unlike structured landing zones, real-world sites are unstructured and highly variable, requiring strong generalization capabilities from the perception system. Deep neural networks (DNNs) provide a scalable solution for learning landing site features across diverse visual and environmental conditions. While perception-driven landing has been shown in simulation, real-world deployment introduces significant challenges. Payload and volume constraints limit high-performance edge AI devices like the NVIDIA Jetson Orin Nano, which are crucial for real-time detection and control. Accurate pose estimation during descent is necessary, especially in the absence of GPS, and relies on dependable visual-inertial odometry. Achieving this with limited edge AI resources requires careful optimization of the entire deployment framework. The flight characteristics of large QuadPlanes further complicate the problem. These aircraft exhibit high inertia, reduced thrust vectoring, and slow response times further complicate stable landing maneuvers. This work presents a lightweight QuadPlane system for efficient vision-based autonomous landing and visual-inertial odometry, specifically developed for long-range QuadPlane operations such as aerial monitoring. It describes the hardware platform, sensor configuration, and embedded computing architecture designed to meet demanding real-time, physical constraints. This establishes a foundation for deploying autonomous landing in dynamic, unstructured, GPS-denied environments.

