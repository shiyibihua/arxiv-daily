---
layout: default
title: Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning
---

# Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning

**arXiv**: [2512.09310v1](https://arxiv.org/abs/2512.09310) | [PDF](https://arxiv.org/pdf/2512.09310.pdf)

**ä½œè€…**: Kwang Bin Lee, Jiho Kang, Sung-Hee Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœºæ™¯æ— å…³çš„åŒè‡‚ä»»åŠ¡è§„åˆ’æ¡†æž¶ï¼Œé€šè¿‡è§†è§‰å¯ä¾›æ€§æŽ¨ç†å®žçŽ°åè°ƒæ“ä½œ**

**å…³é”®è¯**: `åŒè‡‚ä»»åŠ¡è§„åˆ’` `è§†è§‰å¯ä¾›æ€§æŽ¨ç†` `åœºæ™¯æ— å…³æ“ä½œ` `å­ç›®æ ‡è§„åˆ’` `åè°ƒåŠ¨ä½œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æœºå™¨äººä»»åŠ¡è§„åˆ’å™¨å¤šä¸ºå•è‡‚ï¼Œéš¾ä»¥å¤„ç†åœºæ™¯æ— å…³åŒè‡‚æ“ä½œçš„ç©ºé—´ã€å‡ ä½•å’Œåè°ƒæŒ‘æˆ˜
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆè§†è§‰ç‚¹å®šä½ã€åŒè‡‚å­ç›®æ ‡è§„åˆ’å’Œäº¤äº’ç‚¹é©±åŠ¨æç¤ºæ¨¡å—ï¼Œå®žçŽ°è¯­ä¹‰æŽ¨ç†ä¸Ž3Dæ‰§è¡Œæ¡¥æŽ¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ‚ä¹±æœªçŸ¥åœºæ™¯ä¸­ç”Ÿæˆç´§å‡‘å¯è¡ŒåŒè‡‚è®¡åˆ’ï¼Œæ— éœ€é‡è®­ç»ƒï¼Œå±•ç¤ºç¨³å¥åœºæ™¯æ³›åŒ–èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.

