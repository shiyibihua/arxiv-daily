---
layout: default
title: Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization
---

# Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization

**arXiv**: [2512.09608v1](https://arxiv.org/abs/2512.09608) | [PDF](https://arxiv.org/pdf/2512.09608.pdf)

**ä½œè€…**: Zhiheng Li, Weihua Wang, Qiang Shen, Yichen Zhao, Zheng Fang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSuper4DRæ¡†æž¶ï¼Œä»¥4Dé›·è¾¾ä¸ºä¸­å¿ƒï¼Œé€šè¿‡è‡ªç›‘ç£é‡Œç¨‹è®¡å’Œé«˜æ–¯åœ°å›¾ä¼˜åŒ–è§£å†³æ¶åŠ£çŽ¯å¢ƒä¸‹çš„SLAMé—®é¢˜ã€‚**

**å…³é”®è¯**: `4Dé›·è¾¾SLAM` `è‡ªç›‘ç£é‡Œç¨‹è®¡` `é«˜æ–¯åœ°å›¾ä¼˜åŒ–` `æ¶åŠ£çŽ¯å¢ƒæ„ŸçŸ¥` `å¤šæ¨¡æ€æ¸²æŸ“`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼š4Dé›·è¾¾ç‚¹äº‘ç¨€ç–å™ªå£°å¯¼è‡´é‡Œç¨‹è®¡ä¸å‡†å’Œåœ°å›¾ç»“æž„æ¨¡ç³Šä¸å®Œæ•´ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡èšç±»æ„ŸçŸ¥é‡Œç¨‹è®¡ç½‘ç»œå’Œè‡ªç›‘ç£æœºåˆ¶ï¼Œç»“åˆ3Dé«˜æ–¯è¡¨ç¤ºä¸Žé›·è¾¾ç‰¹å®šç­–ç•¥ä¼˜åŒ–åœ°å›¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ€§èƒ½æå‡67%ï¼ŒæŽ¥è¿‘ç›‘ç£é‡Œç¨‹è®¡ï¼Œç¼©å°ä¸ŽLiDARåœ°å›¾è´¨é‡å·®è·å¹¶æ”¯æŒå¤šæ¨¡æ€æ¸²æŸ“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather. Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures. Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization. First, we design a cluster-aware odometry network that incorporates object-level cues from the clustered radar points for inter-frame matching, alongside a hierarchical self-supervision mechanism to overcome outliers through spatio-temporal consistency, knowledge transfer, and feature contrast. Second, we propose using 3D gaussians as an intermediate representation, coupled with a radar-specific growth strategy, selective separation, and multi-view regularization, to recover blurry map areas and those undetected based on image texture. Experiments show that Super4DR achieves a 67% performance gain over prior self-supervised methods, nearly matches supervised odometry, and narrows the map quality disparity with LiDAR while enabling multi-modal image rendering.

