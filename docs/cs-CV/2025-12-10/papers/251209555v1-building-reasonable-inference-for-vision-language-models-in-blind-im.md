---
layout: default
title: Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment
---

# Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment

**arXiv**: [2512.09555v1](https://arxiv.org/abs/2512.09555) | [PDF](https://arxiv.org/pdf/2512.09555.pdf)

**ä½œè€…**: Yuan Li, Zitang Sun, Yen-ju Chen, Shin'ya Nishida

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸¤é˜¶æ®µè°ƒä¼˜æ–¹æ³•ä»¥è§£å†³ç›²å›¾åƒè´¨é‡è¯„ä¼°ä¸­è§†è§‰è¯­è¨€æ¨¡åž‹çš„æŽ¨ç†çŸ›ç›¾ä¸Žä¸ç¨³å®šæ€§é—®é¢˜**

**å…³é”®è¯**: `ç›²å›¾åƒè´¨é‡è¯„ä¼°` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æŽ¨ç†ç¨³å®šæ€§` `ä¸¤é˜¶æ®µè°ƒä¼˜` `è§†è§‰ç‰¹å¾å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åˆ†æžè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨ç›²å›¾åƒè´¨é‡è¯„ä¼°ä¸­äº§ç”ŸçŸ›ç›¾é¢„æµ‹å’Œä¸ç¨³å®šæ€§çš„åŽŸå› ï¼Œå¦‚ç‰¹å¾ä¸Žé¢„æµ‹é€»è¾‘è¿žæŽ¥å¼±
2. å¼•å…¥ä¸¤é˜¶æ®µè°ƒä¼˜æ–¹æ³•ï¼Œå…ˆå­¦ä¹ è§†è§‰ç‰¹å¾ï¼Œå†åŸºäºŽç‰¹å¾æŽ¨æ–­è´¨é‡ï¼Œä»¥æ¨¡æ‹Ÿäººç±»æŽ¨ç†è¿‡ç¨‹
3. å®žéªŒè¡¨æ˜Žæ–¹æ³•åœ¨SPAQå’ŒKONIQä¸Šé™ä½Žä¸ç¨³å®šæ€§è‡³12.39%ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæå‡SRCC/PLCCæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.

