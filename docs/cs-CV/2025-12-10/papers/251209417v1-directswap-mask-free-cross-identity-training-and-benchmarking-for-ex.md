---
layout: default
title: DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping
---

# DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping

**arXiv**: [2512.09417v1](https://arxiv.org/abs/2512.09417) | [PDF](https://arxiv.org/pdf/2512.09417.pdf)

**ä½œè€…**: Yanan Wang, Shengcai Liao, Panwen Hu, Xin Li, Fan Yang, Xiaodan Liang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDirectSwapæ¡†æž¶å’ŒHeadSwapBenchæ•°æ®é›†ï¼Œä»¥è§£å†³è§†é¢‘å¤´éƒ¨äº¤æ¢ä¸­èº«ä»½æ³„æ¼å’Œè¿åŠ¨è¡¨æƒ…ä¸€è‡´æ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘å¤´éƒ¨äº¤æ¢` `è·¨èº«ä»½è®­ç»ƒ` `é…å¯¹æ•°æ®é›†` `æ‰©æ•£æ¨¡åž‹` `è¿åŠ¨è¡¨æƒ…ä¸€è‡´æ€§` `æ— æŽ©ç ä¿®å¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–åŒäººè·¨å¸§è®­ç»ƒå’ŒæŽ©ç ä¿®å¤ï¼Œå¯¼è‡´èº«ä»½æ³„æ¼ã€è¾¹ç•Œä¼ªå½±åŠé®æŒ¡ä¿¡æ¯æ¢å¤å›°éš¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºè·¨èº«ä»½é…å¯¹æ•°æ®é›†HeadSwapBenchï¼Œå¹¶è®¾è®¡æ— æŽ©ç ç›´æŽ¥äº¤æ¢æ¡†æž¶DirectSwapï¼Œç»“åˆè¿åŠ¨æ¨¡å—å’ŒMEARæŸå¤±å¢žå¼ºä¸€è‡´æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šæ ·çœŸå®žè§†é¢‘åœºæ™¯ä¸­å®žçŽ°æœ€å…ˆè¿›çš„è§†è§‰è´¨é‡ã€èº«ä»½ä¿çœŸåº¦åŠè¿åŠ¨è¡¨æƒ…ä¸€è‡´æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\TrainNum{} videos) and benchmarking (\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.

