---
layout: default
title: ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators
---

# ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators

**arXiv**: [2512.09427v1](https://arxiv.org/abs/2512.09427) | [PDF](https://arxiv.org/pdf/2512.09427.pdf)

**ä½œè€…**: Guoqiang Zou, Wanyu Wang, Hao Zheng, Longxiang Yin, Yinhe Han

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºODMAæ¡†æž¶ä»¥è§£å†³éšæœºè®¿é—®å—é™åŠ é€Ÿå™¨ä¸ŠLLMæœåŠ¡çš„å†…å­˜åˆ†é…é—®é¢˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹æœåŠ¡` `å†…å­˜ç®¡ç†` `éšæœºè®¿é—®å—é™åŠ é€Ÿå™¨` `åŠ¨æ€åˆ†é…` `æ€§èƒ½ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å†…å­˜ç®¡ç†å™¨åœ¨éšæœºè®¿é—®å¸¦å®½å·®çš„åŠ é€Ÿå™¨ä¸Šå¯¼è‡´å†…å­˜æµªè´¹æˆ–æ€§èƒ½ä½Žä¸‹
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè½»é‡çº§é•¿åº¦é¢„æµ‹å™¨ã€åŠ¨æ€æ¡¶åˆ†åŒºå’Œå¤§æ¡¶ä¿æŠ¤æœºåˆ¶ï¼Œä¼˜åŒ–å†…å­˜åˆ†é…
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Cambricon MLU370-X4ä¸Šï¼Œå†…å­˜åˆ©ç”¨çŽ‡ä»Ž55.05%æå‡è‡³72.45%ï¼ŒRPSå’ŒTPSæé«˜çº¦30%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.

