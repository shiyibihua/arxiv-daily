---
layout: default
title: UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories
---

# UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories

**arXiv**: [2512.09607v1](https://arxiv.org/abs/2512.09607) | [PDF](https://arxiv.org/pdf/2512.09607.pdf)

**ä½œè€…**: Yanghong Mei, Yirong Yang, Longteng Guo, Qunbo Wang, Ming-Ming Yu, Xingjian He, Wenjun Wu, Jing Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUrbanNavæ¡†æž¶ï¼Œåˆ©ç”¨ç½‘ç»œè§„æ¨¡äººç±»è½¨è¿¹è®­ç»ƒå…·èº«ä»£ç†éµå¾ªè‡ªç”±è¯­è¨€æŒ‡ä»¤è¿›è¡ŒåŸŽå¸‚å¯¼èˆª**

**å…³é”®è¯**: `åŸŽå¸‚å¯¼èˆª` `è¯­è¨€å¼•å¯¼å¯¼èˆª` `å…·èº«ä»£ç†` `å¤§è§„æ¨¡è½¨è¿¹æ•°æ®` `ç©ºé—´æŽ¨ç†` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŸŽå¸‚çŽ¯å¢ƒä¸­è‡ªç„¶è¯­è¨€å¯¼èˆªé¢ä¸´å™ªå£°æŒ‡ä»¤ã€æ¨¡ç³Šç©ºé—´å‚è€ƒå’ŒåŠ¨æ€åœºæ™¯æŒ‘æˆ˜ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–ç²¾ç¡®ç›®æ ‡æ ¼å¼ï¼Œé™åˆ¶å®žé™…åº”ç”¨
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç½‘ç»œè§„æ¨¡åŸŽå¸‚è¡Œèµ°è§†é¢‘ï¼Œå¼€å‘å¯æ‰©å±•æ ‡æ³¨æµç¨‹ï¼Œå¯¹é½äººç±»è½¨è¿¹ä¸ŽåŸºäºŽçœŸå®žåœ°æ ‡çš„è¯­è¨€æŒ‡ä»¤ï¼Œæž„å»ºå¤§è§„æ¨¡æ•°æ®é›†
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹åœ¨å¤æ‚åŸŽå¸‚åœºæ™¯ä¸­å±•çŽ°ä¼˜è¶Šç©ºé—´æŽ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼ŒéªŒè¯å¤§è§„æ¨¡ç½‘ç»œè§†é¢‘æ•°æ®æ½œåŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.

