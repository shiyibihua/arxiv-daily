---
layout: default
title: Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power
---

# Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power

**arXiv**: [2512.09673v1](https://arxiv.org/abs/2512.09673) | [PDF](https://arxiv.org/pdf/2512.09673.pdf)

**ä½œè€…**: Yuzhu Chen, Tian Qin, Xinmei Tian, Fengxiang He, Dacheng Tao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºç­‰å˜ç½‘ç»œè¡¨è¾¾åŠ›å—é™åŠå…¶é€šè¿‡æ‰©å¤§æ¨¡åž‹è§„æ¨¡è¡¥å¿çš„æœºåˆ¶**

**å…³é”®è¯**: `ç­‰å˜ç¥žç»ç½‘ç»œ` `è¡¨è¾¾åŠ›åˆ†æž` `æ¨¡åž‹è§„æ¨¡è¡¥å¿` `æ³›åŒ–èƒ½åŠ›` `ReLUç½‘ç»œ` `å¯¹ç§°æ€§ç¼–ç `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç­‰å˜çº¦æŸå¯èƒ½ä¸¥æ ¼é™åˆ¶ç¥žç»ç½‘ç»œçš„è¡¨è¾¾åŠ›ï¼Œå½±å“å…¶æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åˆ†æžReLUç½‘ç»œçš„è¾¹ç•Œè¶…å¹³é¢å’Œé€šé“å‘é‡ï¼Œæž„å»ºç¤ºä¾‹è¯æ˜Žè¡¨è¾¾åŠ›å—é™ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå±•ç¤ºæ‰©å¤§æ¨¡åž‹è§„æ¨¡å¯è¡¥å¿æ­¤ç¼ºç‚¹ï¼Œä¸”ç­‰å˜ç½‘ç»œä»å…·è¾ƒä½Žå¤æ‚åº¦ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.

