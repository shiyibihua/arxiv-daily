---
layout: default
title: Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models
---

# Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models

**arXiv**: [2512.09927v1](https://arxiv.org/abs/2512.09927) | [PDF](https://arxiv.org/pdf/2512.09927.pdf)

**ä½œè€…**: Yifan Ye, Jiaqi Ma, Jun Cen, Zhihe Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTEAM-VLAè®­ç»ƒæ— å…³çš„ä»¤ç‰ŒåŽ‹ç¼©æ¡†æž¶ï¼Œä»¥åŠ é€Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹æŽ¨ç†å¹¶ä¿æŒæ€§èƒ½ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ä»¤ç‰ŒåŽ‹ç¼©` `è®­ç»ƒæ— å…³ä¼˜åŒ–` `æŽ¨ç†åŠ é€Ÿ` `æœºå™¨äººæ„ŸçŸ¥æŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è§„æ¨¡VLAæ¨¡åž‹åœ¨å®žæ—¶éƒ¨ç½²ä¸­é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿ŸæŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åŠ¨æ€ä»¤ç‰Œæ‰©å±•å’ŒåŠ¨ä½œæ„ŸçŸ¥åˆå¹¶ï¼Œåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­åŽ‹ç¼©ä»¤ç‰Œã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LIBEROåŸºå‡†ä¸Šæå‡æŽ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ç»´æŒæˆ–è¶…è¶ŠåŽŸå§‹æ¨¡åž‹ä»»åŠ¡æˆåŠŸçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}

