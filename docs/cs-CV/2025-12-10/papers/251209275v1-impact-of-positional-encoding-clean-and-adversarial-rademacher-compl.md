---
layout: default
title: Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression
---

# Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression

**arXiv**: [2512.09275v1](https://arxiv.org/abs/2512.09275) | [PDF](https://arxiv.org/pdf/2512.09275.pdf)

**ä½œè€…**: Weiyi He, Yue Xing

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æžå¯è®­ç»ƒä½ç½®ç¼–ç å¯¹Transformeråœ¨ä¸Šä¸‹æ–‡å›žå½’ä¸­æ³›åŒ–ä¸Žé²æ£’æ€§çš„å½±å“**

**å…³é”®è¯**: `ä½ç½®ç¼–ç ` `Transformer` `ä¸Šä¸‹æ–‡å›žå½’` `æ³›åŒ–åˆ†æž` `å¯¹æŠ—é²æ£’æ€§` `Rademacherå¤æ‚åº¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä½ç½®ç¼–ç å¯¹Transformeræ³›åŒ–ä¸Žé²æ£’æ€§çš„å½±å“æœªçŸ¥
2. æ–¹æ³•è¦ç‚¹ï¼šæŽ¨å¯¼å•å±‚Transformeråœ¨ä¸Šä¸‹æ–‡å›žå½’ä¸­çš„æ³›åŒ–ä¸Žå¯¹æŠ—Rademacherç•Œ
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡æ‹Ÿç ”ç©¶éªŒè¯ç†è®ºç•Œï¼Œæ˜¾ç¤ºä½ç½®ç¼–ç æ”¾å¤§æ³›åŒ–å·®è·ä¸Žè„†å¼±æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Positional encoding (PE) is a core architectural component of Transformers, yet its impact on the Transformer's generalization and robustness remains unclear. In this work, we provide the first generalization analysis for a single-layer Transformer under in-context regression that explicitly accounts for a completely trainable PE module. Our result shows that PE systematically enlarges the generalization gap. Extending to the adversarial setting, we derive the adversarial Rademacher generalization bound. We find that the gap between models with and without PE is magnified under attack, demonstrating that PE amplifies the vulnerability of models. Our bounds are empirically validated by a simulation study. Together, this work establishes a new framework for understanding the clean and adversarial generalization in ICL with PE.

