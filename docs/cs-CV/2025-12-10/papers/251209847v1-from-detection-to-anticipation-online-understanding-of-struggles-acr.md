---
layout: default
title: From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities
---

# From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities

**arXiv**: [2512.09847v1](https://arxiv.org/abs/2512.09847) | [PDF](https://arxiv.org/pdf/2512.09847.pdf)

**ä½œè€…**: Shijia Feng, Michael Wray, Walterio Mayol-Cuevas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœ¨çº¿æ£€æµ‹ä¸Žé¢„æµ‹æ¨¡åž‹ï¼Œä»¥å®žæ—¶è¯†åˆ«å’Œé¢„æµ‹ç”¨æˆ·åœ¨ä»»åŠ¡ä¸­çš„å›°éš¾ï¼Œé€‚ç”¨äºŽæ™ºèƒ½è¾…åŠ©ç³»ç»Ÿã€‚**

**å…³é”®è¯**: `åœ¨çº¿å›°éš¾æ£€æµ‹` `å›°éš¾é¢„æµ‹` `æ™ºèƒ½è¾…åŠ©ç³»ç»Ÿ` `å®žæ—¶åº”ç”¨` `è·¨ä»»åŠ¡æ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç ”ç©¶å¤šå…³æ³¨ç¦»çº¿å›°éš¾åˆ†ç±»ä¸Žå®šä½ï¼Œä½†å®žæ—¶åº”ç”¨éœ€åœ¨çº¿æ£€æµ‹ä¸Žé¢„æµ‹å›°éš¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†å›°éš¾å®šä½é‡æž„ä¸ºåœ¨çº¿æ£€æµ‹ä»»åŠ¡ï¼Œå¹¶æ‰©å±•è‡³é¢„æµ‹ï¼Œé‡‡ç”¨çŽ°æˆæ¨¡åž‹ä½œä¸ºåŸºçº¿ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çº¿æ£€æµ‹è¾¾åˆ°70-80%æ¯å¸§mAPï¼Œé¢„æµ‹æ€§èƒ½ç•¥æœ‰ä¸‹é™ï¼Œæ¨¡åž‹åœ¨è·¨ä»»åŠ¡æ³›åŒ–ä¸­ä¼˜äºŽéšæœºåŸºçº¿4-20%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.

