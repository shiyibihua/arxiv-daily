---
layout: default
title: COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning
---

# COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning

**arXiv**: [2512.09349v1](https://arxiv.org/abs/2512.09349) | [PDF](https://arxiv.org/pdf/2512.09349.pdf)

**ä½œè€…**: Lin Li, Yuxin Cai, Jianwu Fang, Jianru Xue, Chen Lv

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCOVLM-RLæ¡†æž¶ï¼Œé€šè¿‡VLMå¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ è§£å†³è‡ªåŠ¨é©¾é©¶çš„æ³›åŒ–ä¸Žå¯è§£é‡Šæ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `å…³é”®å¯¹è±¡æŽ¨ç†` `å¯è§£é‡Šæ€§` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¡†æž¶åœ¨æ³›åŒ–ã€è®­ç»ƒæ•ˆçŽ‡å’Œå¯è§£é‡Šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå…³é”®å¯¹è±¡æŽ¨ç†ä¸ŽVLMå¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨CoTæç¤ºç”Ÿæˆè¯­ä¹‰å†³ç­–å…ˆéªŒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CARLAæ¨¡æ‹Ÿå™¨ä¸­ï¼Œè®­ç»ƒçŽ¯å¢ƒæˆåŠŸçŽ‡æå‡30%ï¼Œæœªè§çŽ¯å¢ƒæå‡50%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> End-to-end autonomous driving frameworks face persistent challenges in generalization, training efficiency, and interpretability. While recent methods leverage Vision-Language Models (VLMs) through supervised learning on large-scale datasets to improve reasoning, they often lack robustness in novel scenarios. Conversely, reinforcement learning (RL)-based approaches enhance adaptability but remain data-inefficient and lack transparent decision-making. % contribution To address these limitations, we propose COVLM-RL, a novel end-to-end driving framework that integrates Critical Object-oriented (CO) reasoning with VLM-guided RL. Specifically, we design a Chain-of-Thought (CoT) prompting strategy that enables the VLM to reason over critical traffic elements and generate high-level semantic decisions, effectively transforming multi-view visual inputs into structured semantic decision priors. These priors reduce the input dimensionality and inject task-relevant knowledge into the RL loop, accelerating training and improving policy interpretability. However, bridging high-level semantic guidance with continuous low-level control remains non-trivial. To this end, we introduce a consistency loss that encourages alignment between the VLM's semantic plans and the RL agent's control outputs, enhancing interpretability and training stability. Experiments conducted in the CARLA simulator demonstrate that COVLM-RL significantly improves the success rate by 30\% in trained driving environments and by 50\% in previously unseen environments, highlighting its strong generalization capability.

