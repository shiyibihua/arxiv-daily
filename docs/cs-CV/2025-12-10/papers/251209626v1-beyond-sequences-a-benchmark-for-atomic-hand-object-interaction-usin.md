---
layout: default
title: Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder
---

# Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder

**arXiv**: [2512.09626v1](https://arxiv.org/abs/2512.09626) | [PDF](https://arxiv.org/pdf/2512.09626.pdf)

**ä½œè€…**: Yousef Azizi Movahed, Fatemeh Ziaeetabar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé™æ€RNNç¼–ç å™¨ç”¨äºŽåŽŸå­æ‰‹-ç‰©äº¤äº’åˆ†ç±»ï¼Œåœ¨MANIACæ•°æ®é›†ä¸Šå®žçŽ°97.60%å‡†ç¡®çŽ‡ã€‚**

**å…³é”®è¯**: `æ‰‹-ç‰©äº¤äº’è¯†åˆ«` `åŽŸå­äº¤äº’åˆ†ç±»` `é™æ€RNNç¼–ç å™¨` `ç»Ÿè®¡-è¿åŠ¨ç‰¹å¾` `MANIACæ•°æ®é›†` `ç»†ç²’åº¦è¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç»†ç²’åº¦åˆ†ç±»æ‰‹-ç‰©äº¤äº’çš„åŽŸå­çŠ¶æ€ï¼ˆæŽ¥è¿‘ã€æŠ“å–ã€æŒæœ‰ï¼‰ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†è§†é¢‘è½¬æ¢ä¸ºç»Ÿè®¡-è¿åŠ¨ç‰¹å¾å‘é‡ï¼Œä½¿ç”¨åºåˆ—é•¿åº¦è®¾ä¸º1çš„åŒå‘RNNä½œä¸ºé™æ€ç¼–ç å™¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹åœ¨æœ€å…·æŒ‘æˆ˜çš„'æŠ“å–'ç±»ä¸ŠèŽ·å¾—0.90å¹³è¡¡F1åˆ†æ•°ï¼Œå‡†ç¡®çŽ‡è¾¾97.60%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reliably predicting human intent in hand-object interactions is an open challenge for computer vision. Our research concentrates on a fundamental sub-problem: the fine-grained classification of atomic interaction states, namely 'approaching', 'grabbing', and 'holding'. To this end, we introduce a structured data engineering process that converts raw videos from the MANIAC dataset into 27,476 statistical-kinematic feature vectors. Each vector encapsulates relational and dynamic properties from a short temporal window of motion. Our initial hypothesis posited that sequential modeling would be critical, leading us to compare static classifiers (MLPs) against temporal models (RNNs). Counter-intuitively, the key discovery occurred when we set the sequence length of a Bidirectional RNN to one (seq_length=1). This modification converted the network's function, compelling it to act as a high-capacity static feature encoder. This architectural change directly led to a significant accuracy improvement, culminating in a final score of 97.60%. Of particular note, our optimized model successfully overcame the most challenging transitional class, 'grabbing', by achieving a balanced F1-score of 0.90. These findings provide a new benchmark for low-level hand-object interaction recognition using structured, interpretable features and lightweight architectures.

