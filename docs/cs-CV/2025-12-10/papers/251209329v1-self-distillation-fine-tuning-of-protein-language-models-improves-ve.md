---
layout: default
title: Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design
---

# Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design

**arXiv**: [2512.09329v1](https://arxiv.org/abs/2512.09329) | [PDF](https://arxiv.org/pdf/2512.09329.pdf)

**ä½œè€…**: Amin Tavakoli, Raswanth Murugan, Ozan Gokdemir, Arvind Ramanathan, Frances Arnold, Anima Anandkumar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªè’¸é¦å¾®è°ƒæ–¹æ³•ä»¥æå‡è›‹ç™½è´¨è¯­è¨€æ¨¡åž‹åœ¨è›‹ç™½è´¨è®¾è®¡ä¸­çš„é€šç”¨æ€§**

**å…³é”®è¯**: `è›‹ç™½è´¨è¯­è¨€æ¨¡åž‹` `ç›‘ç£å¾®è°ƒ` `è‡ªè’¸é¦` `è›‹ç™½è´¨è®¾è®¡` `åºåˆ—ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè›‹ç™½è´¨è¯­è¨€æ¨¡åž‹å¾®è°ƒç¼ºä¹é«˜è´¨é‡æ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´ç”Ÿæˆåºåˆ—ä¿çœŸåº¦ä½Žã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨æ¨¡åž‹è‡ªèº«è¾“å‡ºæž„å»ºè®­ç»ƒæ•°æ®ï¼Œç»“åˆè½»é‡çº§ç­›é€‰ç®¡é“è¿›è¡Œç›‘ç£å¾®è°ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è‰²æ°¨é…¸åˆé…¶å®¶æ—ä¸­éªŒè¯ï¼Œç”Ÿæˆåºåˆ—æ›´æ–°é¢–ä¸”ç¨³å®šæ€§ä¸ŽåŠŸèƒ½æ€§æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.

