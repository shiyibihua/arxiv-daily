---
layout: default
title: Mixture of Lookup Key-Value Experts
---

# Mixture of Lookup Key-Value Experts

**arXiv**: [2512.09723v1](https://arxiv.org/abs/2512.09723) | [PDF](https://arxiv.org/pdf/2512.09723.pdf)

**ä½œè€…**: Zongcheng Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoLKVæ¨¡åž‹ä»¥è§£å†³MoLEä¸Šä¸‹æ–‡æ— å…³ä¸“å®¶é€‰æ‹©é™åˆ¶ï¼Œæå‡èµ„æºå—é™è®¾å¤‡ä¸ŠLLMæ€§èƒ½ã€‚**

**å…³é”®è¯**: `ä¸“å®¶æ··åˆæ¨¡åž‹` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥æœºåˆ¶` `é”®å€¼å¯¹ä¸“å®¶` `èµ„æºå—é™è®¾å¤‡` `LLMæŽ¨ç†ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MoLEåŸºäºŽè¾“å…¥IDé€‰æ‹©ä¸“å®¶ï¼Œä¸Šä¸‹æ–‡æ— å…³ï¼Œå¯èƒ½é™åˆ¶æ¨¡åž‹æ€§èƒ½ã€‚
2. MoLKVå°†ä¸“å®¶æž„å»ºä¸ºé”®å€¼å¯¹ï¼Œé€šè¿‡è¾“å…¥æŸ¥è¯¢ä¸Žç¼“å­˜é”®å€¼äº¤äº’å®žçŽ°ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¾“å‡ºã€‚
3. å°è§„æ¨¡è¯„ä¼°æ˜¾ç¤ºMoLKVæ˜¾è‘—é™ä½ŽéªŒè¯æŸå¤±ï¼Œä¼˜äºŽMoLEã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \textbf{M}ixture \textbf{o}f \textbf{L}ookup \textbf{K}ey-\textbf{V}alue Experts (\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.

