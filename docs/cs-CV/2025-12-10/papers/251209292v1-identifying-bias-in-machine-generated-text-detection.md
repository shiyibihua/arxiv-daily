---
layout: default
title: Identifying Bias in Machine-generated Text Detection
---

# Identifying Bias in Machine-generated Text Detection

**arXiv**: [2512.09292v1](https://arxiv.org/abs/2512.09292) | [PDF](https://arxiv.org/pdf/2512.09292.pdf)

**ä½œè€…**: Kevin Stowe, Svetlana Afanaseva, Rodolfo Raimundo, Yitao Sun, Kailash Patil

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°è‹±è¯­æœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹ç³»ç»Ÿåœ¨æ€§åˆ«ã€ç§æ—ã€è¯­è¨€å’Œç»æµŽå±žæ€§ä¸Šçš„æ½œåœ¨åè§**

**å…³é”®è¯**: `æœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹` `åè§è¯„ä¼°` `å…¬å¹³æ€§åˆ†æž` `å›žå½’æ¨¡åž‹` `å­ç¾¤åˆ†æž` `è‹±è¯­å­¦ä¹ è€…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹ç³»ç»Ÿå¯èƒ½å¯¹æ€§åˆ«ã€ç§æ—ã€è‹±è¯­å­¦ä¹ è€…çŠ¶æ€å’Œç»æµŽåœ°ä½ç­‰å±žæ€§å­˜åœ¨åè§ï¼Œå¯¼è‡´ä¸å…¬å¹³åˆ†ç±»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå­¦ç”Ÿè®ºæ–‡æ•°æ®é›†ï¼Œä½¿ç”¨å›žå½’æ¨¡åž‹å’Œå­ç¾¤åˆ†æžè¯„ä¼°16ä¸ªæ£€æµ‹ç³»ç»Ÿçš„åè§æ˜¾è‘—æ€§å’Œå½±å“ç¨‹åº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°åè§åœ¨ä¸åŒç³»ç»Ÿä¸­ä¸ä¸€è‡´ï¼Œä½†è‹±è¯­å­¦ä¹ è€…è®ºæ–‡æ›´æ˜“è¢«è¯¯åˆ¤ä¸ºæœºå™¨ç”Ÿæˆï¼Œéžç™½äººè‹±è¯­å­¦ä¹ è€…å°¤å…¶å—å½±å“ï¼Œè€Œäººç±»æ ‡æ³¨è€…æ— æ˜¾è‘—åè§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.

