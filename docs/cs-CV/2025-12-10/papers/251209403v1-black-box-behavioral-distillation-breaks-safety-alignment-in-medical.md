---
layout: default
title: Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs
---

# Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs

**arXiv**: [2512.09403v1](https://arxiv.org/abs/2512.09403) | [PDF](https://arxiv.org/pdf/2512.09403.pdf)

**ä½œè€…**: Sohely Jahan, Ruimin Sun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé»‘ç›’è¡Œä¸ºè’¸é¦æ”»å‡»ï¼Œæ­ç¤ºåŒ»ç–—å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨å¯¹é½çš„è„†å¼±æ€§ã€‚**

**å…³é”®è¯**: `åŒ»ç–—å¤§è¯­è¨€æ¨¡åž‹` `å®‰å…¨å¯¹é½` `é»‘ç›’è’¸é¦` `å¯¹æŠ—æ”»å‡»` `æ¨¡åž‹æå–` `é›¶å¯¹é½ç›‘ç£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŒ»ç–—å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨å¯¹é½åœ¨ä»…è¾“å‡ºè®¿é—®ä¸‹æ˜“è¢«æ”»å‡»ï¼Œå¯¼è‡´åŠŸèƒ½ä¿ç•™ä½†å®‰å…¨æœºåˆ¶å¤±æ•ˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æŒ‡ä»¤æŸ¥è¯¢å’Œé›¶å¯¹é½ç›‘ç£ï¼Œä½Žæˆæœ¬è’¸é¦LLaMA3 8Bä»£ç†æ¨¡åž‹ï¼Œæ— éœ€æ¨¡åž‹æƒé‡æˆ–å®‰å…¨è¿‡æ»¤å™¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šä»£ç†æ¨¡åž‹åœ¨86%å¯¹æŠ—æç¤ºä¸‹äº§ç”Ÿä¸å®‰å…¨è¾“å‡ºï¼Œè¿œè¶…åŽŸæ¨¡åž‹å’ŒåŸºç¡€æ¨¡åž‹ï¼Œæš´éœ²åŠŸèƒ½-ä¼¦ç†å·®è·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored.
>   We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments.
>   Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.

