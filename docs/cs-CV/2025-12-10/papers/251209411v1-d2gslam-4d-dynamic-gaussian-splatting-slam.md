---
layout: default
title: D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM
---

# D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM

**arXiv**: [2512.09411v1](https://arxiv.org/abs/2512.09411) | [PDF](https://arxiv.org/pdf/2512.09411.pdf)

**ä½œè€…**: Siting Zhu, Yuxiang Huang, Wenhua Wu, Chaokang Jiang, Yongbo Chen, I-Ming Chen, Hesheng Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDÂ²GSLAMç³»ç»Ÿï¼Œåˆ©ç”¨é«˜æ–¯è¡¨ç¤ºåœ¨åŠ¨æ€çŽ¯å¢ƒä¸­åŒæ—¶å®žçŽ°å‡†ç¡®åŠ¨æ€é‡å»ºä¸Žç¨³å¥è·Ÿè¸ªã€‚**

**å…³é”®è¯**: `åŠ¨æ€SLAM` `é«˜æ–¯è¡¨ç¤º` `åŠ¨é™å¤åˆå»ºæ¨¡` `å‡ ä½•ä¸€è‡´æ€§` `ç›¸æœºè·Ÿè¸ª` `åŠ¨æ€é‡å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŠ¨æ€çŽ¯å¢ƒä¸­å¯†é›†SLAMæŒ‘æˆ˜ï¼ŒçŽ°æœ‰æ–¹æ³•å¿½ç•¥åŠ¨æ€ç‰©ä½“è¿åŠ¨ä¿¡æ¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå‡ ä½•æç¤ºåŠ¨æ€åˆ†ç¦»ã€åŠ¨é™å¤åˆè¡¨ç¤ºã€æ¸è¿›å§¿æ€ä¼˜åŒ–å’Œè¿åŠ¨ä¸€è‡´æ€§æŸå¤±ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŠ¨æ€åœºæ™¯ä¸­å±•ç¤ºä¼˜è¶Šçš„å»ºå›¾å’Œè·Ÿè¸ªç²¾åº¦ï¼Œæ”¯æŒå‡†ç¡®åŠ¨æ€å»ºæ¨¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.

