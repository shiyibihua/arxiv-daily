---
layout: default
title: Composing Concepts from Images and Videos via Concept-prompt Binding
---

# Composing Concepts from Images and Videos via Concept-prompt Binding

**arXiv**: [2512.09824v1](https://arxiv.org/abs/2512.09824) | [PDF](https://arxiv.org/pdf/2512.09824.pdf)

**ä½œè€…**: Xianghao Kong, Zeyu Zhang, Yuwei Guo, Zhuoran Zhao, Songchun Zhang, Anyi Rao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBind & Composeæ–¹æ³•ï¼Œé€šè¿‡æ¦‚å¿µ-æç¤ºç»‘å®šå®žçŽ°å›¾åƒä¸Žè§†é¢‘çš„çµæ´»è§†è§‰æ¦‚å¿µç»„åˆã€‚**

**å…³é”®è¯**: `è§†è§‰æ¦‚å¿µç»„åˆ` `æ‰©æ•£å˜æ¢å™¨` `æ¦‚å¿µ-æç¤ºç»‘å®š` `æ—¶é—´è§£è€¦` `å¤šæ ·åŒ–å¸æ”¶æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰æ¦‚å¿µç»„åˆåœ¨å‡†ç¡®æå–å¤æ‚æ¦‚å¿µå’Œçµæ´»ç»“åˆå›¾åƒä¸Žè§†é¢‘æ¦‚å¿µæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†å±‚ç»‘å®šç»“æž„å’Œå¤šæ ·åŒ–å¸æ”¶æœºåˆ¶ï¼Œç»“åˆæ—¶é—´è§£è€¦ç­–ç•¥ï¼Œæå‡æ¦‚å¿µç»‘å®šå‡†ç¡®æ€§å’Œå…¼å®¹æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºåœ¨æ¦‚å¿µä¸€è‡´æ€§ã€æç¤ºä¿çœŸåº¦å’Œè¿åŠ¨è´¨é‡ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ‹“å±•è§†è§‰åˆ›æ„å¯èƒ½æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.

