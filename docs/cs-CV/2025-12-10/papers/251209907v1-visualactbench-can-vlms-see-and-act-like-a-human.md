---
layout: default
title: VisualActBench: Can VLMs See and Act like a Human?
---

# VisualActBench: Can VLMs See and Act like a Human?

**arXiv**: [2512.09907v1](https://arxiv.org/abs/2512.09907) | [PDF](https://arxiv.org/pdf/2512.09907.pdf)

**ä½œè€…**: Daoan Zhang, Pai Liu, Xiaofei Zhou, Yuan Ge, Guangchen Lan, Jing Bi, Christopher Brinton, Ehsan Hoque, Jiebo Luo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVisualActBenchåŸºå‡†ä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æ— æ–‡æœ¬æç¤ºä¸‹çš„ä¸»åŠ¨è§†è§‰åŠ¨ä½œæŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰åŠ¨ä½œæŽ¨ç†` `è§†è§‰è¯­è¨€æ¨¡åž‹è¯„ä¼°` `ä¸»åŠ¨è§†è§‰ç†è§£` `äººç±»å¯¹é½åŸºå‡†` `è§†é¢‘åŠ¨ä½œæ ‡æ³¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨ä»…å‡­è§†è§‰è¾“å…¥è¿›è¡Œä¸»åŠ¨æŽ¨ç†å’Œè¡ŒåŠ¨çš„èƒ½åŠ›å°šæœªå……åˆ†æŽ¢ç´¢
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è§†è§‰åŠ¨ä½œæŽ¨ç†ä»»åŠ¡ï¼Œæž„å»ºåŒ…å«1074ä¸ªè§†é¢‘å’Œ3733ä¸ªäººå·¥æ ‡æ³¨åŠ¨ä½œçš„å¤§è§„æ¨¡åŸºå‡†
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°29ä¸ªæ¨¡åž‹ï¼Œå‘çŽ°å‰æ²¿æ¨¡åž‹è¡¨çŽ°ç›¸å¯¹è¾ƒå¼ºï¼Œä½†ä¸Žäººç±»æŽ¨ç†æ°´å¹³ä»æœ‰æ˜¾è‘—å·®è·

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.

