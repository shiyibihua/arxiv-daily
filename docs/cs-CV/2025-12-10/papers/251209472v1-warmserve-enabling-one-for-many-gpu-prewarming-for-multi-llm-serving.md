---
layout: default
title: WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving
---

# WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving

**arXiv**: [2512.09472v1](https://arxiv.org/abs/2512.09472) | [PDF](https://arxiv.org/pdf/2512.09472.pdf)

**ä½œè€…**: Chiheng Lou, Sheng Qi, Rui Kang, Yong Zhang, Chen Sun, Pengcheng Wang, Bingyang Liu, Xuanzhe Liu, Xin Jin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWarmServeç³»ç»Ÿï¼Œé€šè¿‡é€šç”¨GPUå·¥ä½œå™¨å®žçŽ°ä¸€å¯¹å¤šé¢„çƒ­ï¼Œä»¥ä¼˜åŒ–å¤šLLMæœåŠ¡ä¸­çš„æŽ¨ç†æ€§èƒ½ã€‚**

**å…³é”®è¯**: `å¤šLLMæœåŠ¡` `GPUé¢„çƒ­` `è´Ÿè½½é¢„æµ‹` `æ¨¡åž‹æ”¾ç½®` `å†…å­˜ç®¡ç†` `æŽ¨ç†æ€§èƒ½ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç³»ç»Ÿå› å¿½è§†æœªæ¥è´Ÿè½½ç‰¹æ€§ï¼Œå¯¼è‡´GPUåˆ©ç”¨çŽ‡ä¼˜åŒ–ä¸ŽæŽ¨ç†æ€§èƒ½ï¼ˆå¦‚é¦–è¯å»¶è¿Ÿï¼‰ä¹‹é—´çš„æƒè¡¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡é€šç”¨GPUå·¥ä½œå™¨ï¼ŒåŸºäºŽè´Ÿè½½é¢„æµ‹è¿›è¡Œä¸»åŠ¨é¢„çƒ­ï¼Œé‡‡ç”¨é©±é€æ„ŸçŸ¥æ¨¡åž‹æ”¾ç½®å’Œé›¶å¼€é”€å†…å­˜åˆ‡æ¢æœºåˆ¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæ•°æ®é›†ä¸Šï¼Œç›¸æ¯”çŽ°æœ‰ç³»ç»Ÿï¼Œé¦–è¯å»¶è¿Ÿæå‡æœ€é«˜è¾¾50.8å€ï¼Œè¯·æ±‚æœåŠ¡èƒ½åŠ›æå‡è‡³2.5å€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.
>   We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\times$ more requests compared to the GPU-sharing system.

