---
layout: default
title: FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation
---

# FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation

**arXiv**: [2512.09617v1](https://arxiv.org/abs/2512.09617) | [PDF](https://arxiv.org/pdf/2512.09617.pdf)

**ä½œè€…**: Hubert Kompanowski, Varun Jampani, Aaryaman Vasishta, Binh-Son Hua

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå°‘æ ·æœ¬è‡ªæ³¨æ„åŠ›é€‚é…çš„å¤šè§†è§’æè´¨å¤–è§‚è¿ç§»æ–¹æ³•ï¼Œä»¥å¢žå¼ºå¤šè§†è§’æ‰©æ•£æ¨¡åž‹çš„å¤–è§‚æ“æŽ§èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `å¤šè§†è§’æ‰©æ•£æ¨¡åž‹` `å¤–è§‚è¿ç§»` `è‡ªæ³¨æ„åŠ›é€‚é…` `å°‘æ ·æœ¬å­¦ä¹ ` `æè´¨ç¼–è¾‘` `ç”Ÿæˆå¼3Dè¡¨ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šè§†è§’æ‰©æ•£æ¨¡åž‹åœ¨å¤–è§‚æ“æŽ§ï¼ˆå¦‚æè´¨ã€çº¹ç†ï¼‰æ–¹é¢å—é™ï¼Œéš¾ä»¥å®žçŽ°ç²¾ç¡®æŒ‡å®šã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è½»é‡çº§é€‚é…ï¼Œç»“åˆè¾“å…¥å›¾åƒçš„å¯¹è±¡èº«ä»½å’Œå‚è€ƒå›¾åƒçš„å¤–è§‚çº¿ç´¢ï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›ç‰¹å¾èšåˆå®žçŽ°å¤šè§†è§’ä¸€è‡´è¾“å‡ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šä»…éœ€å°‘é‡è®­ç»ƒæ ·æœ¬ï¼Œå³å¯åœ¨é¢„è®­ç»ƒæ¨¡åž‹ä¸Šå®žçŽ°å¤šæ ·å¤–è§‚çš„å¤šè§†è§’ç”Ÿæˆï¼Œä¿æŒå‡ ä½•å’Œè§†è§’ä¸€è‡´æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.
>   In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.

