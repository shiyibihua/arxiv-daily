---
layout: default
title: Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation
---

# Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation

**arXiv**: [2512.09801v1](https://arxiv.org/abs/2512.09801) | [PDF](https://arxiv.org/pdf/2512.09801.pdf)

**ä½œè€…**: Tien-Dat Chung, Ba-Thinh Lam, Thanh-Huy Nguyen, Thien Nguyen, Nguyen Lan Vi Vu, Hoang-Loc Cao, Phat Kim Huynh, Min Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨¡æ€ç‰¹å®šå¢žå¼ºä¸Žäº’è¡¥èžåˆæ¡†æž¶ä»¥è§£å†³åŠç›‘ç£å¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²ä¸­çš„è·¨æ¨¡æ€å·®å¼‚é—®é¢˜**

**å…³é”®è¯**: `åŠç›‘ç£å­¦ä¹ ` `å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²` `è„‘è‚¿ç˜¤åˆ†å‰²` `è·¨æ¨¡æ€èžåˆ` `æ³¨æ„åŠ›æœºåˆ¶` `ä¸€è‡´æ€§æ­£åˆ™åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŠç›‘ç£å¤šæ¨¡æ€æ–¹æ³•å› è¯­ä¹‰å·®å¼‚å’Œé”™ä½éš¾ä»¥åˆ©ç”¨æ¨¡æ€é—´äº’è¡¥ä¿¡æ¯
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ¨¡æ€ç‰¹å®šå¢žå¼ºæ¨¡å—å’Œå¯å­¦ä¹ äº’è¡¥ä¿¡æ¯èžåˆæ¨¡å—ï¼Œä¼˜åŒ–æ··åˆç›®æ ‡å‡½æ•°
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨BraTS 2019æ•°æ®é›†ä¸Šä¼˜äºŽåŸºçº¿ï¼Œæå‡Diceå’ŒSensitivityåˆ†æ•°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\%, 5\%, and 10\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.

