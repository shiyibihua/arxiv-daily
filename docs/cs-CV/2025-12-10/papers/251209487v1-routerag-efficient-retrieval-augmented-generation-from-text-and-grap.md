---
layout: default
title: RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning
---

# RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning

**arXiv**: [2512.09487v1](https://arxiv.org/abs/2512.09487) | [PDF](https://arxiv.org/pdf/2512.09487.pdf)

**ä½œè€…**: Yucan Guo, Miao Su, Saiping Guan, Zihao Sun, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRouteRAGæ¡†æž¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®žçŽ°æ–‡æœ¬ä¸Žå›¾æ··åˆæ£€ç´¢å¢žå¼ºç”Ÿæˆï¼Œä»¥æ”¯æŒè‡ªé€‚åº”é«˜æ•ˆçš„å¤šè½®æŽ¨ç†**

**å…³é”®è¯**: `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `å›¾æ–‡æœ¬æ··åˆæ£€ç´¢` `å¤šè½®æŽ¨ç†` `è‡ªé€‚åº”æ£€ç´¢` `é—®ç­”ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ··åˆæ£€ç´¢ç³»ç»Ÿä¾èµ–å›ºå®šæµç¨‹ï¼Œæ— æ³•è‡ªé€‚åº”æ•´åˆæ–‡æœ¬ä¸Žå›¾è¯æ®ï¼Œä¸”å›¾æ£€ç´¢æˆæœ¬é«˜
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå¼ºåŒ–å­¦ä¹ è”åˆä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ï¼Œå­¦ä¹ ä½•æ—¶æŽ¨ç†ã€ä»Žä½•å¤„æ£€ç´¢åŠä½•æ—¶è¾“å‡ºç­”æ¡ˆ
3. å®žéªŒæ•ˆæžœï¼šåœ¨äº”ä¸ªé—®ç­”åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶ŠçŽ°æœ‰RAGåŸºçº¿ï¼ŒéªŒè¯äº†ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.

