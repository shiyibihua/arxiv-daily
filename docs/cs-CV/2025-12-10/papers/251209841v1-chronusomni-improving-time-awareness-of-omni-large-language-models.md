---
layout: default
title: ChronusOmni: Improving Time Awareness of Omni Large Language Models
---

# ChronusOmni: Improving Time Awareness of Omni Large Language Models

**arXiv**: [2512.09841v1](https://arxiv.org/abs/2512.09841) | [PDF](https://arxiv.org/pdf/2512.09841.pdf)

**ä½œè€…**: Yijing Chen, Yihan Wu, Kaisi Guan, Yuchen Ren, Yuyue Wang, Ruihua Song, Liyun Ru

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºChronusOmniä»¥å¢žå¼ºå…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†å¬æ—¶åºç†è§£ä¸­çš„æ˜¾å¼å’Œéšå¼æ—¶åºæ„ŸçŸ¥èƒ½åŠ›**

**å…³é”®è¯**: `å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†å¬æ—¶åºç†è§£` `è·¨æ¨¡æ€éšå¼æ—¶åº` `å¼ºåŒ–å­¦ä¹ ` `æ—¶åºå»ºæ¨¡` `é•¿è§†é¢‘åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨è§†å¬æ—¶åºç†è§£ä¸­éŸ³é¢‘æ¨¡æ€åˆ©ç”¨ä¸è¶³ï¼Œä¸”å¿½è§†è·¨æ¨¡æ€éšå¼æ—¶åºå…³ç³»ï¼Œå¦‚è§†è§‰ä¸ŽéŸ³é¢‘çš„äº¤å‰å…³è”ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ–‡æœ¬æ—¶é—´æˆ³ä»¤ç‰Œä¸Žè§†å¬è¡¨ç¤ºäº¤é”™ï¼Œå®žçŽ°ç»Ÿä¸€æ—¶åºå»ºæ¨¡ï¼›ç»“åˆå¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°å¼ºåŒ–æ—¶åºæŽ’åºå’Œç»†ç²’åº¦æŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è‡ªå»ºæ•°æ®é›†ChronusAVä¸Šæ€§èƒ½æå‡è¶…30%ï¼Œå¹¶åœ¨å…¶ä»–æ—¶åºåŸºå‡†æµ‹è¯•ä¸­å–å¾—é¢†å…ˆï¼ŒåŒæ—¶ä¿æŒé€šç”¨è§†å¬ç†è§£èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

