---
layout: default
title: DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation
---

# DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation

**arXiv**: [2512.09814v1](https://arxiv.org/abs/2512.09814) | [PDF](https://arxiv.org/pdf/2512.09814.pdf)

**ä½œè€…**: Zhizhong Wang, Tianyi Chu, Zeyi Huang, Nanyang Wang, Kehan Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDynaIPåŠ¨æ€å›¾åƒæç¤ºé€‚é…å™¨ï¼Œä»¥å¢žå¼ºé›¶æ ·æœ¬ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç»†ç²’åº¦ä¿çœŸåº¦ã€æ¦‚å¿µ-æç¤ºå¹³è¡¡å’Œä¸»é¢˜å¯æ‰©å±•æ€§ã€‚**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `é›¶æ ·æœ¬å­¦ä¹ ` `å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨` `åŠ¨æ€è§£è€¦ç­–ç•¥` `åˆ†å±‚ç‰¹å¾èžåˆ` `å¤šä¸»é¢˜å¯æ‰©å±•æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨æ¦‚å¿µä¿ç•™ä¸Žæç¤ºè·Ÿéšå¹³è¡¡ã€ç»†ç²’åº¦ç»†èŠ‚ä¿ç•™å’Œå¤šä¸»é¢˜å¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽMM-DiTçš„è§£è€¦å­¦ä¹ è¡Œä¸ºï¼Œè®¾è®¡åŠ¨æ€è§£è€¦ç­–ç•¥å’Œåˆ†å±‚ä¸“å®¶æ··åˆç‰¹å¾èžåˆæ¨¡å—ï¼Œæå‡æ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å•ä¸»é¢˜å’Œå¤šä¸»é¢˜ä»»åŠ¡ä¸­éªŒè¯DynaIPä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒæŽ¨åŠ¨é¢†åŸŸè¿›å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.

