---
layout: default
title: Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs
---

# Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs

**arXiv**: [2512.09874v1](https://arxiv.org/abs/2512.09874) | [PDF](https://arxiv.org/pdf/2512.09874.pdf)

**ä½œè€…**: Pius Horn, Janis Keuper

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽåˆæˆPDFä¸ŽLLMè¯„ä¼°çš„åŸºå‡†æ¡†æž¶ï¼Œä»¥è§£å†³æ•°å­¦å…¬å¼æå–çš„è¯„æµ‹éš¾é¢˜ã€‚**

**å…³é”®è¯**: `PDFè§£æžåŸºå‡†` `æ•°å­¦å…¬å¼æå–` `LLMè¯„ä¼°` `åˆæˆæ•°æ®ç”Ÿæˆ` `è¯­ä¹‰åŒ¹é…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰PDFè§£æžå™¨è¯„æµ‹åŸºå‡†ç¼ºä¹å¯¹æ•°å­¦å…¬å¼çš„è¯­ä¹‰è¯„ä¼°ï¼Œå½±å“ä¸‹æ¸¸åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åˆæˆPDFç”Ÿæˆç²¾ç¡®LaTeXçœŸå€¼ï¼Œå¹¶å¼•å…¥LLMä½œä¸ºè¯­ä¹‰è¯„ä¼°å™¨ï¼Œç»“åˆä¸¤é˜¶æ®µåŒ¹é…å¤„ç†è¾“å‡ºä¸ä¸€è‡´æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡äººç±»éªŒè¯ï¼ŒLLMè¯„ä¼°ä¸Žäººç±»åˆ¤æ–­ç›¸å…³æ€§é«˜ï¼ˆPearson r=0.78ï¼‰ï¼Œè¯„æµ‹20+è§£æžå™¨æ­ç¤ºæ€§èƒ½å·®å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench

