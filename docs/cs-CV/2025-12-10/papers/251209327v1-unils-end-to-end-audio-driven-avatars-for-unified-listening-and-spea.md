---
layout: default
title: UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking
---

# UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking

**arXiv**: [2512.09327v1](https://arxiv.org/abs/2512.09327) | [PDF](https://arxiv.org/pdf/2512.09327.pdf)

**ä½œè€…**: Xuangeng Chu, Ruicong Liu, Yifei Huang, Yun Liu, Yichen Peng, Bo Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniLSæ¡†æž¶ï¼Œé€šè¿‡åŒè½¨éŸ³é¢‘ç«¯åˆ°ç«¯ç”Ÿæˆç»Ÿä¸€å¬è¯´çš„è™šæ‹Ÿäººè¡¨æƒ…ï¼Œè§£å†³å¬è€…å»ºæ¨¡åƒµç¡¬é—®é¢˜ã€‚**

**å…³é”®è¯**: `éŸ³é¢‘é©±åŠ¨è™šæ‹Ÿäºº` `å¬è¯´ç»Ÿä¸€ç”Ÿæˆ` `ä¸¤é˜¶æ®µè®­ç»ƒ` `å¬è€…å»ºæ¨¡` `ç«¯åˆ°ç«¯æ¡†æž¶` `è¡¨æƒ…ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¬è€…è¡¨æƒ…å»ºæ¨¡å›°éš¾ï¼ŒéŸ³é¢‘é©±åŠ¨è®­ç»ƒå¯¼è‡´åƒµç¡¬ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–é¢å¤–è¿åŠ¨æ•°æ®æˆ–éžç«¯åˆ°ç«¯è®¾è®¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼Œå…ˆå­¦ä¹ å†…éƒ¨è¿åŠ¨å…ˆéªŒï¼Œå†å¼•å…¥åŒè½¨éŸ³é¢‘å¾®è°ƒï¼Œå®žçŽ°éŸ³é¢‘é©±åŠ¨çš„å¬è¯´ç»Ÿä¸€ç”Ÿæˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¬è¯´å‡†ç¡®æ€§ä¸Šè¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œå¬è€…æŒ‡æ ‡æå‡44.1%ï¼Œç”Ÿæˆæ›´è‡ªç„¶å¤šæ ·çš„è¡¨æƒ…ï¼Œç¼“è§£åƒµç¡¬é—®é¢˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.

