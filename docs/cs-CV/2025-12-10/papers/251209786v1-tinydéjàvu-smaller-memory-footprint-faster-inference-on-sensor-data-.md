---
layout: default
title: TinyD√©j√†Vu: Smaller Memory Footprint & Faster Inference on Sensor Data Streams with Always-On Microcontrollers
---

# TinyD√©j√†Vu: Smaller Memory Footprint & Faster Inference on Sensor Data Streams with Always-On Microcontrollers

**arXiv**: [2512.09786v1](https://arxiv.org/abs/2512.09786) | [PDF](https://arxiv.org/pdf/2512.09786.pdf)

**‰ΩúËÄÖ**: Zhaolan Huang, Emmanuel Baccelli

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TinyD√©j√†VuÊ°ÜÊû∂‰ª•‰ºòÂåñÂæÆÊéßÂà∂Âô®‰∏ä‰º†ÊÑüÂô®Êï∞ÊçÆÊµÅÊé®ÁêÜÁöÑÂÜÖÂ≠òÂç†Áî®ÂíåËÆ°ÁÆóÊïàÁéá**

**ÂÖ≥ÈîÆËØç**: `ÂæÆÊéßÂà∂Âô®Êé®ÁêÜ` `ÂÜÖÂ≠ò‰ºòÂåñ` `‰º†ÊÑüÂô®Êï∞ÊçÆÊµÅ` `ÊªëÂä®Á™óÂè£` `Á•ûÁªèÁΩëÁªúÊïàÁéá` `ÂºÄÊ∫êÊ°ÜÊû∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºöÂæÆÊéßÂà∂Âô®ÂÜÖÂ≠òÊúâÈôêÔºàÂ¶Ç128kB RAMÔºâÔºåÈúÄ‰ºòÂåñÁ•ûÁªèÁΩëÁªúÂ±ÇÈó¥Êï∞ÊçÆÊµÅ‰ª•Èôç‰ΩéËÉΩËÄóÂíåÂª∂ÈïøÁîµÊ±†ÂØøÂëΩ
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöËÆæËÆ°Êñ∞ÁÆóÊ≥ïÂáèÂ∞ëRAMÂç†Áî®ÔºåÈÄöËøáÊ∂àÈô§ÈáçÂè†ÊªëÂä®Á™óÂè£ËæìÂÖ•ÁöÑÂÜó‰ΩôËÆ°ÁÆóÊèêÂçáÊé®ÁêÜÈÄüÂ∫¶
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöÂºÄÊ∫êÂÆûÁé∞ÔºåÁ°¨‰ª∂Âü∫ÂáÜÊµãËØïÊòæÁ§∫RAM‰ΩøÁî®ÂáèÂ∞ëË∂Ö60%ÔºåÂÜó‰ΩôËÆ°ÁÆóÊ∂àÈô§Ëææ90%

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Always-on sensors are increasingly expected to embark a variety of tiny neural networks and to continuously perform inference on time-series of the data they sense. In order to fit lifetime and energy consumption requirements when operating on battery, such hardware uses microcontrollers (MCUs) with tiny memory budget e.g., 128kB of RAM. In this context, optimizing data flows across neural network layers becomes crucial. In this paper, we introduce TinyD√©j√†Vu, a new framework and novel algorithms we designed to drastically reduce the RAM footprint required by inference using various tiny ML models for sensor data time-series on typical microcontroller hardware. We publish the implementation of TinyD√©j√†Vu as open source, and we perform reproducible benchmarks on hardware. We show that TinyD√©j√†Vu can save more than 60% of RAM usage and eliminate up to 90% of redundant compute on overlapping sliding window inputs.

