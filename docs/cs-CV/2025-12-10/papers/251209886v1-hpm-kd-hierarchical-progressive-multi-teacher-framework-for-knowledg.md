---
layout: default
title: HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression
---

# HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression

**arXiv**: [2512.09886v1](https://arxiv.org/abs/2512.09886) | [PDF](https://arxiv.org/pdf/2512.09886.pdf)

**ä½œè€…**: Gustavo Coelho Haase, Paulo Henrique Dourado da Silva

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHPM-KDæ¡†æž¶ä»¥è§£å†³çŸ¥è¯†è’¸é¦ä¸­çš„è¶…å‚æ•°æ•æ„Ÿã€å®¹é‡å·®è·å’Œå¤šæ•™å¸ˆåè°ƒé—®é¢˜**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `æ¨¡åž‹åŽ‹ç¼©` `å¤šæ•™å¸ˆå­¦ä¹ ` `å…ƒå­¦ä¹ ` `å¹¶è¡Œå¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŸ¥è¯†è’¸é¦å­˜åœ¨è¶…å‚æ•°æ•æ„Ÿã€å®¹é‡å·®è·ã€å¤šæ•™å¸ˆåè°ƒå·®å’Œè®¡ç®—èµ„æºä½Žæ•ˆé—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆè‡ªé€‚åº”é…ç½®ã€æ¸è¿›è’¸é¦é“¾ã€æ³¨æ„åŠ›åŠ æƒå¤šæ•™å¸ˆé›†æˆç­‰å…­ç»„ä»¶
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIFARå’Œè¡¨æ ¼æ•°æ®é›†ä¸Šå®žçŽ°10-15å€åŽ‹ç¼©ï¼Œä¿æŒ85%ç²¾åº¦ï¼Œå‡å°‘30-40%è®­ç»ƒæ—¶é—´

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.

