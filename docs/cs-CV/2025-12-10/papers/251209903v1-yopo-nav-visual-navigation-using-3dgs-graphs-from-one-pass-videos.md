---
layout: default
title: YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos
---

# YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos

**arXiv**: [2512.09903v1](https://arxiv.org/abs/2512.09903) | [PDF](https://arxiv.org/pdf/2512.09903.pdf)

**ä½œè€…**: Ryan Meegan, Adam D'Souza, Bryan Bo Cao, Shubham Jain, Kristin Dana

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºYOPO-Navæ–¹æ³•ï¼Œåˆ©ç”¨å•æ¬¡è§†é¢‘æž„å»º3DGSå›¾å®žçŽ°è§†è§‰å¯¼èˆª**

**å…³é”®è¯**: `è§†è§‰å¯¼èˆª` `3Dé«˜æ–¯æ³¼æº…` `å•æ¬¡è§†é¢‘å­¦ä¹ ` `è§†è§‰åœ°ç‚¹è¯†åˆ«` `æœºå™¨äººæŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰å¯¼èˆªä¾èµ–3Dåœ°å›¾æž„å»ºï¼Œè®¡ç®—å’Œå†…å­˜å¼€é”€å¤§ï¼Œéœ€é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å•æ¬¡æŽ¢ç´¢è§†é¢‘ç¼–ç ä¸ºå±€éƒ¨3DGSæ¨¡åž‹å›¾ï¼Œç»“åˆVPRç²—å®šä½å’Œ3DGSç²¾è°ƒè¿›è¡Œå¯¼èˆªæŽ§åˆ¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨YOPO-Campusæ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œç‰©ç†æœºå™¨äººå®žéªŒæ˜¾ç¤ºåœ¨çœŸå®žåœºæ™¯ä¸­æ€§èƒ½ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.

