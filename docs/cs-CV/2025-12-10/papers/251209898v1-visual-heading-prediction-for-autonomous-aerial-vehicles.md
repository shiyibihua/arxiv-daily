---
layout: default
title: Visual Heading Prediction for Autonomous Aerial Vehicles
---

# Visual Heading Prediction for Autonomous Aerial Vehicles

**arXiv**: [2512.09898v1](https://arxiv.org/abs/2512.09898) | [PDF](https://arxiv.org/pdf/2512.09898.pdf)

**ä½œè€…**: Reza Ahmari, Ahmad Mohammadi, Vahid Hemmati, Mohammed Mynuddin, Parham Kebria, Mahmoud Nabil Mahmoud, Xiaohong Yuan, Abdollah Homaifar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽè§†è§‰çš„æ— äººæœº-æ— äººè½¦é›†æˆæ¡†æž¶ï¼Œç”¨äºŽGPSç¼ºå¤±çŽ¯å¢ƒä¸‹çš„å®žæ—¶å¯¼èˆªä¸Žåè°ƒã€‚**

**å…³é”®è¯**: `æ— äººæœº-æ— äººè½¦é›†æˆ` `è§†è§‰èˆªå‘é¢„æµ‹` `YOLOv5æ£€æµ‹` `è½»é‡ç¥žç»ç½‘ç»œ` `GPSç¼ºå¤±çŽ¯å¢ƒ` `å®žæ—¶åè°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ— äººæœºä¸Žæ— äººè½¦åœ¨GPSä¸å¯ç”¨æˆ–é™çº§æ—¶å®žæ—¶åè°ƒå›°éš¾ï¼Œéœ€ç²¾ç¡®æ£€æµ‹ä¸Žèˆªå‘é¢„æµ‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¾®è°ƒYOLOv5æ£€æµ‹æ— äººè½¦å¹¶æå–ç‰¹å¾ï¼Œè½»é‡ANNé¢„æµ‹æ— äººæœºèˆªå‘è§’ï¼Œä»…éœ€å•ç›®ç›¸æœºè¾“å…¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å—æŽ§å®žéªŒå®¤çŽ¯å¢ƒä¸­æ”¶é›†è¶…13,000å¼ æ ‡æ³¨å›¾åƒï¼ŒANNé¢„æµ‹è¯¯å·®å°ï¼Œæ£€æµ‹å‡†ç¡®çŽ‡è¾¾95%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506Â° and a root mean squared error of 0.1957Â°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration

