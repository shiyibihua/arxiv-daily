---
layout: default
title: Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices
---

# Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices

**arXiv**: [2512.09313v1](https://arxiv.org/abs/2512.09313) | [PDF](https://arxiv.org/pdf/2512.09313.pdf)

**ä½œè€…**: Yuki Oda, Yuta Ono, Hiroshi Nakamura, Hideki Takase

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHetero-SplitEEæ–¹æ³•ï¼Œé€šè¿‡å¼‚æž„æ—©æœŸé€€å‡ºæ”¯æŒå¼‚æž„ç‰©è”ç½‘è®¾å¤‡çš„åˆ†å‰²å­¦ä¹ **

**å…³é”®è¯**: `åˆ†å‰²å­¦ä¹ ` `å¼‚æž„ç‰©è”ç½‘` `æ—©æœŸé€€å‡º` `åä½œè®­ç»ƒ` `ç¥žç»ç½‘ç»œè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åˆ†å‰²å­¦ä¹ å‡è®¾è®¾å¤‡åŒè´¨ï¼Œä¸é€‚ç”¨äºŽè®¡ç®—èµ„æºå¼‚æž„çš„ç‰©è”ç½‘ç³»ç»Ÿ
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆå¼‚æž„æ—©æœŸé€€å‡ºï¼Œå…è®¸å®¢æˆ·ç«¯æ ¹æ®è®¡ç®—èƒ½åŠ›é€‰æ‹©ä¸åŒåˆ†å‰²ç‚¹ï¼Œå¹¶æå‡ºé¡ºåºå’Œå¹³å‡ä¸¤ç§åä½œè®­ç»ƒç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIFAR-10ç­‰æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œä¿æŒç«žäº‰æ€§ç²¾åº¦ï¼Œé«˜æ•ˆæ”¯æŒå¤šæ ·è®¡ç®—çº¦æŸ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.

