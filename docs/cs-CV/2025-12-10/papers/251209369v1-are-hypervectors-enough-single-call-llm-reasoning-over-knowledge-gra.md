---
layout: default
title: Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs
---

# Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs

**arXiv**: [2512.09369v1](https://arxiv.org/abs/2512.09369) | [PDF](https://arxiv.org/pdf/2512.09369.pdf)

**ä½œè€…**: Yezi Liu, William Youngwoo Chung, Hanning Chen, Calvin Yeung, Mohsen Imani

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPathHDæ¡†æž¶ï¼Œé€šè¿‡è¶…ç»´è®¡ç®—å’Œå•æ¬¡LLMè°ƒç”¨å®žçŽ°é«˜æ•ˆçŸ¥è¯†å›¾è°±æŽ¨ç†**

**å…³é”®è¯**: `çŸ¥è¯†å›¾è°±æŽ¨ç†` `è¶…ç»´è®¡ç®—` `å¤§è¯­è¨€æ¨¡åž‹` `é«˜æ•ˆæŽ¨ç†` `å¯è§£é‡Šæ€§` `å•æ¬¡è°ƒç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰çŸ¥è¯†å›¾è°±æŽ¨ç†æ–¹æ³•ä¾èµ–é‡ç¥žç»ç¼–ç å™¨æˆ–å¤šè½®LLMè°ƒç”¨ï¼Œå¯¼è‡´é«˜å»¶è¿Ÿã€é«˜æˆæœ¬å’Œä¸é€æ˜Žå†³ç­–
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è¶…ç»´è®¡ç®—ç¼–ç å…³ç³»è·¯å¾„ï¼ŒåŸºäºŽå—ä½™å¼¦ç›¸ä¼¼åº¦æŽ’åºå€™é€‰ï¼Œå•æ¬¡LLMè£å†³ç”Ÿæˆç­”æ¡ˆå’Œå¯è§£é‡Šè·¯å¾„
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æˆ–è¶…è¶ŠåŸºçº¿æ€§èƒ½ï¼Œé™ä½Žå»¶è¿Ÿ40-60%ï¼Œå‡å°‘GPUå†…å­˜3-5å€ï¼Œæä¾›å¿ å®žè·¯å¾„è§£é‡Š

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) have enabled strong reasoning over both structured and unstructured knowledge. When grounded on knowledge graphs (KGs), however, prevailing pipelines rely on heavy neural encoders to embed and score symbolic paths or on repeated LLM calls to rank candidates, leading to high latency, GPU cost, and opaque decisions that hinder faithful, scalable deployment. We propose PathHD, a lightweight and encoder-free KG reasoning framework that replaces neural path scoring with hyperdimensional computing (HDC) and uses only a single LLM call per query. PathHD encodes relation paths into block-diagonal GHRR hypervectors, ranks candidates with blockwise cosine similarity and Top-K pruning, and then performs a one-shot LLM adjudication to produce the final answer together with cited supporting paths. Technically, PathHD is built on three ingredients: (i) an order-aware, non-commutative binding operator for path composition, (ii) a calibrated similarity for robust hypervector-based retrieval, and (iii) a one-shot adjudication step that preserves interpretability while eliminating per-path LLM scoring. On WebQSP, CWQ, and the GrailQA split, PathHD (i) attains comparable or better Hits@1 than strong neural baselines while using one LLM call per query; (ii) reduces end-to-end latency by $40-60\%$ and GPU memory by $3-5\times$ thanks to encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that improve error diagnosis and controllability. These results indicate that carefully designed HDC representations provide a practical substrate for efficient KG-LLM reasoning, offering a favorable accuracy-efficiency-interpretability trade-off.

