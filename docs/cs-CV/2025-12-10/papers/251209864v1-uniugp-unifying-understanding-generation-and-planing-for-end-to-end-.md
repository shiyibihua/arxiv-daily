---
layout: default
title: UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving
---

# UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving

**arXiv**: [2512.09864v1](https://arxiv.org/abs/2512.09864) | [PDF](https://arxiv.org/pdf/2512.09864.pdf)

**ä½œè€…**: Hao Lu, Ziyang Liu, Guangfeng Jiang, Yuanfei Luo, Sheng Chen, Yangang Zhang, Ying-Cong Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniUGPæ¡†æž¶ä»¥ç»Ÿä¸€ç†è§£ã€ç”Ÿæˆå’Œè§„åˆ’ï¼Œæå‡è‡ªåŠ¨é©¾é©¶åœ¨é•¿å°¾åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `è§†é¢‘ç”Ÿæˆ` `è½¨è¿¹è§„åˆ’` `é•¿å°¾åœºæ™¯` `æ··åˆä¸“å®¶æž¶æž„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨é•¿å°¾åœºæ™¯ä¸­å› ä¸–ç•ŒçŸ¥è¯†æœ‰é™å’Œè§†è§‰åŠ¨æ€å»ºæ¨¡å¼±è€Œè¡¨çŽ°ä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºä¸“ç”¨æ•°æ®é›†ï¼Œé€šè¿‡æ··åˆä¸“å®¶æž¶æž„æ•´åˆè§†è§‰è¯­è¨€æ¨¡åž‹å’Œè§†é¢‘ç”Ÿæˆæ¨¡åž‹ï¼Œå®žçŽ°åœºæ™¯æŽ¨ç†ã€æœªæ¥è§†é¢‘ç”Ÿæˆå’Œè½¨è¿¹è§„åˆ’çš„ç»Ÿä¸€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ„ŸçŸ¥ã€æŽ¨ç†å’Œå†³ç­–æ–¹é¢è¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œå¯¹æŒ‘æˆ˜æ€§é•¿å°¾åœºæ™¯å…·æœ‰ä¼˜è¶Šæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.

