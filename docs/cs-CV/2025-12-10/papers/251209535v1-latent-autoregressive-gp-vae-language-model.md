---
layout: default
title: Latent-Autoregressive GP-VAE Language Model
---

# Latent-Autoregressive GP-VAE Language Model

**arXiv**: [2512.09535v1](https://arxiv.org/abs/2512.09535) | [PDF](https://arxiv.org/pdf/2512.09535.pdf)

**ä½œè€…**: Yves Ruffenach

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽé«˜æ–¯è¿‡ç¨‹çš„æ½œåœ¨è‡ªå›žå½’GP-VAEè¯­è¨€æ¨¡åž‹ï¼Œå°†åºåˆ—åŠ¨æ€è½¬ç§»è‡³æ½œåœ¨ç©ºé—´ä»¥æ”¯æŒéƒ¨åˆ†æ—¶é—´ç»“æž„ã€‚**

**å…³é”®è¯**: `æ½œåœ¨è‡ªå›žå½’æ¨¡åž‹` `é«˜æ–¯è¿‡ç¨‹å˜åˆ†è‡ªç¼–ç å™¨` `è¯­è¨€æ¨¡åž‹` `åºåˆ—å»ºæ¨¡` `éžè‡ªå›žå½’è§£ç ` `æ¦‚çŽ‡å‡ ä½•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ¢ç´¢è¯­è¨€æ¨¡åž‹ä¸­æ—¶é—´ç»“æž„æ˜¯å¦å¯ç”±æ½œåœ¨ç©ºé—´çš„æ¦‚çŽ‡å‡ ä½•è€Œéžæ˜¾å¼ç¥žç»æ“ä½œæ”¯æŒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆé«˜æ–¯è¿‡ç¨‹å…ˆéªŒå’Œç»“æž„åŒ–æ‘Šé”€åŽéªŒï¼Œé‡‡ç”¨æ­£åˆ™åŒ–ELBOè®­ç»ƒåè®®ï¼Œå®žçŽ°éžè‡ªå›žå½’è§£ç ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¦‚å¿µéªŒè¯æ¡†æž¶ä¸‹ï¼Œæ¨¡åž‹è®­ç»ƒç¨³å®šï¼Œåºåˆ—ä¸Žå¹¶è¡Œé‡‡æ ·å˜ä½“è¡¨çŽ°ä¸€è‡´ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.

