---
layout: default
title: Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport
---

# Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport

**arXiv**: [2512.09530v1](https://arxiv.org/abs/2512.09530) | [PDF](https://arxiv.org/pdf/2512.09530.pdf)

**ä½œè€…**: Antonio Candelieri, Alessandro Quadrio

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæœ€ä¼˜è¿è¾“çš„è‡ªæ³¨æ„åŠ›è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºŽè¡¨æ ¼æ•°æ®åˆ†ç±»ï¼Œä»¥æå‡æ•ˆçŽ‡ä¸Žå¯æ‰©å±•æ€§ã€‚**

**å…³é”®è¯**: `è¡¨æ ¼æ•°æ®åˆ†ç±»` `è‡ªæ³¨æ„åŠ›è®­ç»ƒ` `æœ€ä¼˜è¿è¾“` `è®¡ç®—æ•ˆçŽ‡` `æœºå™¨å­¦ä¹ ç®—æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªæ³¨æ„åŠ›è®­ç»ƒè½¨è¿¹æ•ˆçŽ‡ä½Žï¼Œå½±å“è¡¨æ ¼åˆ†ç±»æ€§èƒ½ä¸Žè®¡ç®—æˆæœ¬ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æœ€ä¼˜è¿è¾“åº¦é‡åˆ†æžè‡ªæ³¨æ„åŠ›æ˜ å°„ï¼Œå¹¶è®¾è®¡åŸºäºŽOTçš„ç®—æ³•ç”Ÿæˆç±»ç‰¹å®šåˆ†å¸ƒè¿›è¡Œå¯¹é½è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆ†ç±»ä»»åŠ¡ä¸­å®žçŽ°ä¸ŽTransformerç›¸å½“çš„ç²¾åº¦ï¼Œé™ä½Žè®¡ç®—æˆæœ¬ï¼Œä½†å¯¹è™šæ‹Ÿåˆ†å¸ƒè®¾è®¡æ•æ„Ÿã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This thesis examines self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics, including Wasserstein distance, Monge gap, optimality, and efficiency. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.
>   Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping. The method achieves accuracy comparable to Transformers while reducing computational cost and scaling more efficiently under standardized inputs, though its performance depends on careful dummy-geometry design. All experiments and implementations are conducted in R.

