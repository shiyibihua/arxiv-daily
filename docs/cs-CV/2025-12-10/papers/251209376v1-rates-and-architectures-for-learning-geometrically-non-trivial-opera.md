---
layout: default
title: Rates and architectures for learning geometrically non-trivial operators
---

# Rates and architectures for learning geometrically non-trivial operators

**arXiv**: [2512.09376v1](https://arxiv.org/abs/2512.09376) | [PDF](https://arxiv.org/pdf/2512.09376.pdf)

**ä½œè€…**: T. Mitchell Roddenberry, Leo Tzou, Ivan DokmaniÄ‡, Maarten V. de Hoop, Richard G. Baraniuk

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå­¦ä¹ å‡ ä½•éžå¹³å‡¡ç®—å­çš„ç†è®ºä¸Žæž¶æž„ï¼Œå®žçŽ°è¶…ä»£æ•°è¯¯å·®è¡°å‡ä¸Žæ•°æ®é«˜æ•ˆæ€§ã€‚**

**å…³é”®è¯**: `ç®—å­å­¦ä¹ ` `å‡ ä½•ç§¯åˆ†ç®—å­` `åŒçº¤ç»´å˜æ¢` `ç§‘å­¦æœºå™¨å­¦ä¹ ` `æ•°æ®é«˜æ•ˆæ€§` `äº¤å‰æ³¨æ„åŠ›æž¶æž„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©å±•å­¦ä¹ ç†è®ºè‡³åŒçº¤ç»´å˜æ¢ï¼Œæ¶µç›–å¹¿ä¹‰Radonå’Œæµ‹åœ°çº¿å°„çº¿å˜æ¢ç­‰å‡ ä½•ç§¯åˆ†ç®—å­ã€‚
2. è¯æ˜Žè¯¥ç±»ç®—å­æ— ç»´åº¦è¯…å’’ï¼Œè¯¯å·®éšè®­ç»ƒæ ·æœ¬æ•°å€’æ•°è¡°å‡å¿«äºŽä»»æ„å›ºå®šå¹‚æ¬¡ã€‚
3. è®¾è®¡åŸºäºŽæ°´å¹³é›†æ–¹æ³•çš„äº¤å‰æ³¨æ„åŠ›æž¶æž„ï¼Œå®žçŽ°é€šç”¨ã€ç¨³å®šä¸”æ•°æ®é«˜æ•ˆçš„å­¦ä¹ ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep learning methods have proven capable of recovering operators between high-dimensional spaces, such as solution maps of PDEs and similar objects in mathematical physics, from very few training samples. This phenomenon of data-efficiency has been proven for certain classes of elliptic operators with simple geometry, i.e., operators that do not change the domain of the function or propagate singularities. However, scientific machine learning is commonly used for problems that do involve the propagation of singularities in a priori unknown ways, such as waves, advection, and fluid dynamics. In light of this, we expand the learning theory to include double fibration transforms--geometric integral operators that include generalized Radon and geodesic ray transforms. We prove that this class of operators does not suffer from the curse of dimensionality: the error decays superalgebraically, that is, faster than any fixed power of the reciprocal of the number of training samples. Furthermore, we investigate architectures that explicitly encode the geometry of these transforms, demonstrating that an architecture reminiscent of cross-attention based on levelset methods yields a parameterization that is universal, stable, and learns double fibration transforms from very few training examples. Our results contribute to a rapidly-growing line of theoretical work on learning operators for scientific machine learning.

