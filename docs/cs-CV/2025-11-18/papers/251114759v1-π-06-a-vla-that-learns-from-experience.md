---
layout: default
title: $Ï€^{*}_{0.6}$: a VLA That Learns From Experience
---

# $Ï€^{*}_{0.6}$: a VLA That Learns From Experience

**arXiv**: [2511.14759v1](https://arxiv.org/abs/2511.14759) | [PDF](https://arxiv.org/pdf/2511.14759.pdf)

**ä½œè€…**: Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace Connors, James Darpinian, Karan Dhabalia, Jared DiCarlo, Danny Driess, Michael Equi, Adnan Esmail, Yunhao Fang, Chelsea Finn, Catherine Glossop, Thomas Godden, Ivan Goryachev, Lachy Groom, Hunter Hancock, Karol Hausman, Gashon Hussein, Brian Ichter, Szymon Jakubczak, Rowan Jen, Tim Jones, Ben Katz, Liyiming Ke, Chandra Kuchi, Marinda Lamb, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Yao Lu, Vishnu Mano, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Charvi Sharma, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, Will Stoeckle, Alex Swerdlow, James Tanner, Marcel Torne, Quan Vuong, Anna Walling, Haohuan Wang, Blake Williams, Sukwon Yoo, Lili Yu, Ury Zhilinsky, Zhiyuan Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRECAPæ–¹æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨çœŸå®žä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººå­¦ä¹ ` `å¼‚æž„æ•°æ®æ•´åˆ` `çœŸå®žä¸–ç•Œéƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹å¦‚ä½•é€šè¿‡çœŸå®žéƒ¨ç½²å’Œå¼ºåŒ–å­¦ä¹ å®žçŽ°è‡ªæˆ‘æ”¹è¿›ã€‚
2. RECAPæ–¹æ³•æ•´åˆå¼‚æž„æ•°æ®ï¼ŒåŒ…æ‹¬æ¼”ç¤ºã€åœ¨çº¿æ”¶é›†å’Œä¸“å®¶å¹²é¢„ï¼Œè¿›è¡Œä¼˜åŠ¿æ¡ä»¶ç­–ç•¥è®­ç»ƒã€‚
3. å®žéªŒæ˜¾ç¤ºï¼ŒRECAPåœ¨å è¡£ç‰©ã€ç»„è£…ç®±å­å’Œåˆ¶ä½œå’–å•¡ç­‰ä»»åŠ¡ä¸­æ˜¾è‘—æå‡åžåé‡å’Œé™ä½Žå¤±è´¥çŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $Ï€^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $Ï€^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.

