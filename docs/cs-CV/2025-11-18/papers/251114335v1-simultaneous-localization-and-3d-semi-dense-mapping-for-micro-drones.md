---
layout: default
title: Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors
---

# Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors

**arXiv**: [2511.14335v1](https://arxiv.org/abs/2511.14335) | [PDF](https://arxiv.org/pdf/2511.14335.pdf)

**ä½œè€…**: Jeryes Danial, Yosi Ben Asher, Itzik Klein

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§å•ç›®SLAMç³»ç»Ÿï¼Œç»“åˆç¨€ç–å§¿æ€ä¼°è®¡ä¸Žç¨ å¯†è¾¹ç¼˜é‡å»ºï¼Œè§£å†³æ— äººæœºå®žæ—¶å»ºå›¾ä¸Žå¯¼èˆªé—®é¢˜ã€‚**

**å…³é”®è¯**: `å•ç›®SLAM` `è¾¹ç¼˜é‡å»º` `æƒ¯æ€§èžåˆ` `å®žæ—¶å»ºå›¾` `æ— äººæœºå¯¼èˆª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•ç›®SLAMå­˜åœ¨å°ºåº¦æ¨¡ç³Šå’Œç¨€ç–æ–¹æ³•å‡ ä½•ç»†èŠ‚ä¸è¶³ï¼Œå­¦ä¹ é©±åŠ¨æ–¹æ³•è®¡ç®—é‡å¤§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šèžåˆæ·±åº¦å­¦ä¹ æ·±åº¦é¢„æµ‹ä¸Žè¾¹ç¼˜æ£€æµ‹ï¼Œç»“åˆæƒ¯æ€§æ•°æ®é€šè¿‡æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨ä¼˜åŒ–å‡ ä½•ä¸€è‡´æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä½ŽåŠŸè€—å¹³å°å®žæ—¶è¿è¡Œï¼Œå®žçŽ°å®¤å†…èµ°å»Šå’ŒTUM RGBDæ•°æ®é›†ä¸Šçš„è‡ªä¸»å¯¼èˆªä¸Žé¿éšœã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments.

