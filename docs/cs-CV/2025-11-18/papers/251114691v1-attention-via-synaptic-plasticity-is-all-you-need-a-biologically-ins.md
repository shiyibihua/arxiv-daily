---
layout: default
title: Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer
---

# Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer

**arXiv**: [2511.14691v1](https://arxiv.org/abs/2511.14691) | [PDF](https://arxiv.org/pdf/2511.14691.pdf)

**ä½œè€…**: Kallol Mondal, Ankush Kumar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽSTDPçš„è„‰å†²Transformerï¼Œå®žçŽ°èŠ‚èƒ½é«˜æ•ˆçš„ç¥žç»å½¢æ€æ³¨æ„åŠ›æœºåˆ¶ã€‚**

**å…³é”®è¯**: `è„‰å†²ç¥žç»ç½‘ç»œ` `ç¥žç»å½¢æ€è®¡ç®—` `æ³¨æ„åŠ›æœºåˆ¶` `STDPå­¦ä¹ ` `èŠ‚èƒ½AI` `ç¡¬ä»¶å‹å¥½æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformeræ³¨æ„åŠ›æœºåˆ¶ä¾èµ–ç‚¹ç§¯ç›¸ä¼¼æ€§ï¼Œå¯¼è‡´é«˜èƒ½è€—å’Œç¡¬ä»¶ç“¶é¢ˆã€‚
2. ä½¿ç”¨è„‰å†²æ—¶åºä¾èµ–å¯å¡‘æ€§å®žçŽ°è‡ªæ³¨æ„åŠ›ï¼Œå°†æŸ¥è¯¢-é”®ç›¸å…³æ€§åµŒå…¥çªè§¦æƒé‡ã€‚
3. åœ¨CIFARæ•°æ®é›†ä¸Šå®žçŽ°é«˜ç²¾åº¦å’Œ88.47%èƒ½è€—é™ä½Žï¼Œå¢žå¼ºå¯è§£é‡Šæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\% and 78.08\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.

