---
layout: default
title: Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning
---

# Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning

**arXiv**: [2511.14396v1](https://arxiv.org/abs/2511.14396) | [PDF](https://arxiv.org/pdf/2511.14396.pdf)

**ä½œè€…**: Xiuxiu Qi, Yu Yang, Jiannong Cao, Luyao Bai, Chongshan Fan, Chengtai Cao, Hongpeng Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¿žç»­è§†è§‰-è¯­è¨€-åŠ¨ä½œååŒå­¦ä¹ æ¡†æž¶ä»¥è§£å†³è¡Œä¸ºå…‹éš†ä¸­çš„è¯­ä¹‰-ç‰©ç†é”™ä½é—®é¢˜**

**å…³é”®è¯**: `è¡Œä¸ºå…‹éš†` `è§†è§‰-è¯­è¨€-åŠ¨ä½œååŒå­¦ä¹ ` `è¯­ä¹‰-ç‰©ç†å¯¹é½` `æœºå™¨äººæ“ä½œ` `å¤åˆè¯¯å·®ç¼“è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¡Œä¸ºå…‹éš†ä¸­åºåˆ—åŠ¨ä½œå†³ç­–çš„å¤åˆè¯¯å·®å¯¼è‡´ç‰©ç†ä¸è¿žç»­å’Œè¯­ä¹‰-ç‰©ç†é”™ä½
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åŒå‘è·¨æ³¨æ„åŠ›å®žçŽ°è¯­ä¹‰ä¸Žè§†è§‰è¿åŠ¨è¡¨ç¤ºçš„é”šå®šï¼Œç¡®ä¿è¿žç»­ååŒå­¦ä¹ 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººæµ‹è¯•ä¸­å¹³å‡æå‡8.0%ï¼Œæ³›åŒ–æ€§å¼º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.

