---
layout: default
title: Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model
---

# Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.14716" target="_blank" class="toolbar-btn">arXiv: 2511.14716v1</a>
    <a href="https://arxiv.org/pdf/2511.14716.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14716v1" 
            onclick="toggleFavorite(this, '2511.14716v1', 'Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiyuan Wang, Muhan Zhang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-18

**Â§áÊ≥®**: Tech Report. 10 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DSDÊ°ÜÊû∂ÔºåÂÆûÁé∞Á´ØÂà∞Á´ØÊΩúÂú®Êâ©Êï£Ê®°ÂûãÂçïÁΩëÁªúËÆ≠ÁªÉÔºåËß£ÂÜ≥Â§öÈò∂ÊÆµËÆ≠ÁªÉ‰ΩéÊïàÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£Ê®°Âûã` `Ëá™Ëí∏È¶è` `Á´ØÂà∞Á´ØËÆ≠ÁªÉ` `ÂõæÂÉèÁîüÊàê` `ÊΩúÂú®Á©∫Èó¥` `ÂçïÁΩëÁªúÊû∂ÊûÑ` `ImageNet`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊΩúÂú®Êâ©Êï£Ê®°ÂûãÁªìÊûÑÂ§çÊùÇÔºåÂåÖÂê´ÁºñÁ†ÅÂô®„ÄÅËß£Á†ÅÂô®ÂíåÊâ©Êï£ÁΩëÁªúÔºåËÆ≠ÁªÉËøáÁ®ãÂàÜÈò∂ÊÆµËøõË°åÔºåÊïàÁéáËæÉ‰Ωé„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Diffusion as Self-Distillation (DSD)Ê°ÜÊû∂ÔºåÈÄöËøáÊîπËøõËÆ≠ÁªÉÁõÆÊ†áÔºåÁ®≥ÂÆöÊΩúÂú®Á©∫Èó¥ÔºåÂÆûÁé∞Á´ØÂà∞Á´ØÂçïÁΩëÁªúËÆ≠ÁªÉ„ÄÇ
3. DSDÂú®ImageNet 256x256ÂõæÂÉèÁîüÊàê‰ªªÂä°‰∏äÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑFIDÂàÜÊï∞Ôºå‰∏îÂèÇÊï∞ÈáèËæÉÂ∞èÔºåÊó†ÈúÄÊó†ÂàÜÁ±ªÂô®ÊåáÂØº„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê†áÂáÜÁöÑÊΩúÂú®Êâ©Êï£Ê®°Âûã‰æùËµñ‰∫é‰∏Ä‰∏™Â§çÊùÇÁöÑ‰∏âÈÉ®ÂàÜÊû∂ÊûÑÔºåÂåÖÊã¨Áã¨Á´ãÁöÑÁºñÁ†ÅÂô®„ÄÅËß£Á†ÅÂô®ÂíåÊâ©Êï£ÁΩëÁªúÔºåËøô‰∫õÁΩëÁªúÈúÄË¶ÅÁªèËøáÂ§ö‰∏™Èò∂ÊÆµÁöÑËÆ≠ÁªÉ„ÄÇËøôÁßçÊ®°ÂùóÂåñËÆæËÆ°Âú®ËÆ°ÁÆó‰∏äÊïàÁéá‰Ωé‰∏ãÔºåÂØºËá¥Ê¨°‰ºòÁöÑÊÄßËÉΩÔºåÂπ∂‰∏îÈòªÁ¢ç‰∫ÜÊâ©Êï£Ê®°Âûã‰∏éËßÜËßâÂü∫Á°ÄÊ®°Âûã‰∏≠Â∏∏ËßÅÁöÑÂçïÁΩëÁªúÊû∂ÊûÑÁöÑÁªü‰∏Ä„ÄÇÊú¨ÊñáÊó®Âú®Â∞ÜËøô‰∏â‰∏™ÁªÑ‰ª∂Áªü‰∏ÄÂà∞‰∏Ä‰∏™Âçï‰∏ÄÁöÑ„ÄÅÁ´ØÂà∞Á´ØÂèØËÆ≠ÁªÉÁöÑÁΩëÁªú‰∏≠„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁî±‰∫é‚ÄúÊΩúÂú®Â¥©Ê∫É‚ÄùÔºåÊú¥Á¥†ÁöÑËÅîÂêàËÆ≠ÁªÉÊñπÊ≥ï‰ºöÂΩªÂ∫ïÂ§±Ë¥•ÔºåÂÖ∂‰∏≠Êâ©Êï£ËÆ≠ÁªÉÁõÆÊ†á‰ºöÂπ≤Êâ∞ÁΩëÁªúÂ≠¶‰π†ËâØÂ•ΩÊΩúÂú®Ë°®Á§∫ÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÂ∞ÜÊâ©Êï£‰∏éÂü∫‰∫éËá™Ëí∏È¶èÁöÑÊó†ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïËøõË°åÁ±ªÊØîÔºåÊè≠Á§∫‰∫ÜËøôÁßç‰∏çÁ®≥ÂÆöÊÄßÁöÑÊ†πÊú¨ÂéüÂõ†„ÄÇÂü∫‰∫éÊ≠§ÔºåÊèêÂá∫‰∫ÜÊâ©Êï£Âç≥Ëá™Ëí∏È¶èÔºàDSDÔºâÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÂØπËÆ≠ÁªÉÁõÆÊ†áÁöÑÂÖ≥ÈîÆ‰øÆÊîπÊù•Á®≥ÂÆöÊΩúÂú®Á©∫Èó¥„ÄÇËøôÁßçÊñπÊ≥ïÈ¶ñÊ¨°ÂÆûÁé∞‰∫ÜÂçï‰∏™ÁΩëÁªúÁöÑÁ®≥ÂÆöÁ´ØÂà∞Á´ØËÆ≠ÁªÉÔºåËØ•ÁΩëÁªúÂêåÊó∂Â≠¶‰π†ÁºñÁ†Å„ÄÅËß£Á†ÅÂíåÊâßË°åÊâ©Êï£„ÄÇDSDÂú®ImageNet $256	imes 256$ Êù°‰ª∂ÁîüÊàê‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÂá∫Ëâ≤ÁöÑÊÄßËÉΩÔºö‰ªÖ‰ΩøÁî®42M/118M/205MÂèÇÊï∞ÔºåÂú®ImageNet‰∏äËÆ≠ÁªÉ50‰∏™epochÔºåFIDÂàÜÂà´‰∏∫13.44/6.38/4.25Ôºå‰∏îÊú™‰ΩøÁî®Êó†ÂàÜÁ±ªÂô®ÊåáÂØº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊΩúÂú®Êâ©Êï£Ê®°ÂûãÈÄöÂ∏∏ÈááÁî®Â§öÊ®°Âùó„ÄÅÂ§öÈò∂ÊÆµÁöÑËÆ≠ÁªÉÊñπÂºèÔºåËÆ°ÁÆóÊïàÁéá‰ΩéÔºåÊÄßËÉΩÂ≠òÂú®‰ºòÂåñÁ©∫Èó¥Ôºå‰∏îÈöæ‰ª•‰∏éËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑÂçïÁΩëÁªúÊû∂ÊûÑÁªü‰∏Ä„ÄÇÁõ¥Êé•ËøõË°åÁ´ØÂà∞Á´ØËÅîÂêàËÆ≠ÁªÉ‰ºöÂØºËá¥‚ÄúÊΩúÂú®Â¥©Ê∫É‚ÄùÈóÆÈ¢òÔºåÂç≥Êâ©Êï£ËÆ≠ÁªÉÁõÆÊ†á‰ºöÂπ≤Êâ∞ÁΩëÁªúÂ≠¶‰π†ÊúâÊïàÁöÑÊΩúÂú®Ë°®Á§∫„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÊâ©Êï£ËøáÁ®ãÁ±ªÊØî‰∏∫Ëá™Ëí∏È¶èÂ≠¶‰π†ÔºåÈÄöËøáÂàÜÊûêËá™Ëí∏È¶èÂ≠¶‰π†ÁöÑÂéüÁêÜÔºåÊâæÂà∞ÂØºËá¥ÊΩúÂú®Â¥©Ê∫ÉÁöÑÂéüÂõ†ÔºåÂπ∂ÂØπËÆ≠ÁªÉÁõÆÊ†áËøõË°å‰øÆÊîπÔºå‰ªéËÄåÁ®≥ÂÆöÊΩúÂú®Á©∫Èó¥ÔºåÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑÂçïÁΩëÁªúËÆ≠ÁªÉ„ÄÇËøôÁßçÁ±ªÊØî‰ΩøÂæóËÉΩÂ§üÂÄüÈâ¥Ëá™Ëí∏È¶èÂ≠¶‰π†‰∏≠ÁöÑÊäÄÂ∑ßÊù•Ëß£ÂÜ≥Êâ©Êï£Ê®°ÂûãËÆ≠ÁªÉ‰∏≠ÁöÑÈóÆÈ¢ò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDSDÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊòØ‰∏Ä‰∏™ÂçïÁΩëÁªúÁªìÊûÑÔºåËØ•ÁΩëÁªúÂêåÊó∂ÊâøÊãÖÁºñÁ†Å„ÄÅËß£Á†ÅÂíåÊâ©Êï£ÁöÑ‰ªªÂä°„ÄÇËÆ≠ÁªÉËøáÁ®ã‰∏çÂÜçÊòØÂàÜÈò∂ÊÆµÁöÑÔºåËÄåÊòØÁ´ØÂà∞Á´ØÁöÑ„ÄÇÂÖ≥ÈîÆÂú®‰∫é‰øÆÊîπÂêéÁöÑËÆ≠ÁªÉÁõÆÊ†áÔºåËØ•ÁõÆÊ†áËÉΩÂ§üÈÅøÂÖçÊΩúÂú®Â¥©Ê∫ÉÔºåÂπ∂‰øÉËøõÁΩëÁªúÂ≠¶‰π†ÊúâÊïàÁöÑÊΩúÂú®Ë°®Á§∫„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨ÔºöËæìÂÖ•ÂõæÂÉèÁªèËøáÁΩëÁªúÁºñÁ†Å‰∏∫ÊΩúÂú®Ë°®Á§∫ÔºåÁÑ∂ÂêéËøõË°åÊâ©Êï£ËøáÁ®ãÔºåÊúÄÂêéÈÄöËøáÁΩëÁªúËß£Á†ÅÈáçÊûÑÂõæÂÉè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂèëÁé∞‰∫ÜÊâ©Êï£ËøáÁ®ã‰∏éËá™Ëí∏È¶èÂ≠¶‰π†‰πãÈó¥ÁöÑËÅîÁ≥ªÔºåÂπ∂Âü∫‰∫éÊ≠§ÊèêÂá∫‰∫ÜÊñ∞ÁöÑËÆ≠ÁªÉÁõÆÊ†á„ÄÇËØ•ËÆ≠ÁªÉÁõÆÊ†áÈÄöËøáÁ®≥ÂÆöÊΩúÂú®Á©∫Èó¥ÔºåËß£ÂÜ≥‰∫ÜÁ´ØÂà∞Á´ØËÆ≠ÁªÉ‰∏≠ÁöÑÊΩúÂú®Â¥©Ê∫ÉÈóÆÈ¢òÔºå‰ΩøÂæóÂçïÁΩëÁªúÁªìÊûÑÁöÑÊΩúÂú®Êâ©Êï£Ê®°ÂûãÊàê‰∏∫ÂèØËÉΩ„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåDSDÊó†ÈúÄÂçïÁã¨ËÆ≠ÁªÉÁºñÁ†ÅÂô®ÂíåËß£Á†ÅÂô®ÔºåÂ§ßÂ§ßÁÆÄÂåñ‰∫ÜËÆ≠ÁªÉÊµÅÁ®ã„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDSDÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫é‰øÆÊîπÂêéÁöÑËÆ≠ÁªÉÁõÆÊ†áÂáΩÊï∞„ÄÇÂÖ∑‰ΩìÁöÑ‰øÆÊîπÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞ÔºåÂåÖÊã¨Â¶Ç‰ΩïÂÄüÈâ¥Ëá™Ëí∏È¶èÂ≠¶‰π†‰∏≠ÁöÑÊäÄÂ∑ßÊù•Á®≥ÂÆöÊΩúÂú®Á©∫Èó¥Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÂπ≥Ë°°ÁºñÁ†Å„ÄÅËß£Á†ÅÂíåÊâ©Êï£‰∏â‰∏™‰ªªÂä°‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÊ≠§Â§ñÔºåÁΩëÁªúÁªìÊûÑÁöÑÂÖ∑‰ΩìËÆæËÆ°‰πüÂØπÊÄßËÉΩÊúâÂΩ±ÂìçÔºåËÆ∫Êñá‰∏≠ÂèØËÉΩÈááÁî®‰∫ÜÁâπÂÆöÁöÑÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÁªìÊûÑÊàñTransformerÁªìÊûÑ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DSDÂú®ImageNet 256x256Êù°‰ª∂ÁîüÊàê‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÊàêÊûú„ÄÇ‰ªÖ‰ΩøÁî®42MÂèÇÊï∞ÁöÑÊ®°ÂûãÔºåFIDËææÂà∞13.44Ôºõ‰ΩøÁî®118MÂèÇÊï∞ÁöÑÊ®°ÂûãÔºåFIDËææÂà∞6.38Ôºõ‰ΩøÁî®205MÂèÇÊï∞ÁöÑÊ®°ÂûãÔºåFIDËææÂà∞4.25„ÄÇËøô‰∫õÁªìÊûúÊòØÂú®‰ªÖËÆ≠ÁªÉ50‰∏™epoch‰∏îÊú™‰ΩøÁî®Êó†ÂàÜÁ±ªÂô®ÊåáÂØºÁöÑÊÉÖÂÜµ‰∏ãËé∑ÂæóÁöÑÔºåË°®ÊòéDSDÂÖ∑ÊúâÂæàÈ´òÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÁîüÊàêË¥®Èáè„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DSDÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂõæÂÉèÁîüÊàê„ÄÅÂõæÂÉèÁºñËæë„ÄÅËßÜÈ¢ëÁîüÊàêÁ≠âÈ¢ÜÂüü„ÄÇÁî±‰∫éÂÖ∂ÂçïÁΩëÁªúÁªìÊûÑÂíåÁ´ØÂà∞Á´ØËÆ≠ÁªÉÊñπÂºèÔºåÊõ¥Êòì‰∫éÈÉ®ÁΩ≤ÂíåÂ∫îÁî®„ÄÇÊú™Êù•ÔºåDSDÊúâÊúõÊàê‰∏∫ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑÈáçË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºåÊé®Âä®‰∫∫Â∑•Êô∫ËÉΩÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÈ¢ÜÂüüÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet $256\times 256$ conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.

