---
layout: default
title: Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models
---

# Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models

**arXiv**: [2511.14120v1](https://arxiv.org/abs/2511.14120) | [PDF](https://arxiv.org/pdf/2511.14120.pdf)

**ä½œè€…**: Hao Zhen, Yunxiang Yang, Jidong J. Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè§†è§’é˜¶æ®µæ„ŸçŸ¥è¡Œäºº-è½¦è¾†äº‹æ•…æŽ¨ç†æ¡†æž¶ï¼Œä»¥æå‡åŸŽå¸‚äº¤é€šå®‰å…¨åˆ†æž**

**å…³é”®è¯**: `å¤šè§†è§’è§†é¢‘åˆ†æž` `è¡Œäººè¡Œä¸ºé˜¶æ®µåˆ†å‰²` `è§†è§‰è¯­è¨€æ¨¡åž‹` `äº‹æ•…æŽ¨ç†` `äº¤é€šå®‰å…¨è¯Šæ–­`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘ç³»ç»Ÿéš¾ä»¥è§£æžè¡Œäººè¡Œä¸ºè®¤çŸ¥é˜¶æ®µï¼Œé™åˆ¶äº‹æ•…å› æžœåˆ†æžã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ¡†æž¶åˆ†å››é˜¶æ®µå¤„ç†å¤šè§†è§’è§†é¢‘ï¼Œé›†æˆä¸“ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹è¿›è¡Œé˜¶æ®µåˆ†å‰²ä¸Žåˆ†æžã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Wovenæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œèƒ½ç”Ÿæˆå¯æ“ä½œæŠ¥å‘Šï¼Œé—®ç­”å‡†ç¡®çŽ‡è¾¾64.70%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Pedestrian-vehicle incidents remain a critical urban safety challenge, with pedestrians accounting for over 20% of global traffic fatalities. Although existing video-based systems can detect when incidents occur, they provide little insight into how these events unfold across the distinct cognitive phases of pedestrian behavior. Recent vision-language models (VLMs) have shown strong potential for video understanding, but they remain limited in that they typically process videos in isolation, without explicit temporal structuring or multi-view integration. This paper introduces Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR), a unified framework that systematically processes multi-view video streams into structured diagnostic reports through four stages: (1) event-triggered multi-view video acquisition, (2) pedestrian behavior phase segmentation, (3) phase-specific multi-view reasoning, and (4) hierarchical synthesis and diagnostic reasoning. The framework operationalizes behavioral theory by automatically segmenting incidents into cognitive phases, performing synchronized multi-view analysis within each phase, and synthesizing results into causal chains with targeted prevention strategies. Particularly, two specialized VLMs underpin the MP-PVIR pipeline: TG-VLM for behavioral phase segmentation (mIoU = 0.4881) and PhaVR-VLM for phase-aware multi-view analysis, achieving a captioning score of 33.063 and up to 64.70% accuracy on question answering. Finally, a designated large language model is used to generate comprehensive reports detailing scene understanding, behavior interpretation, causal reasoning, and prevention recommendations. Evaluation on the Woven Traffic Safety dataset shows that MP-PVIR effectively translates multi-view video data into actionable insights, advancing AI-driven traffic safety analytics for vehicle-infrastructure cooperative systems.

