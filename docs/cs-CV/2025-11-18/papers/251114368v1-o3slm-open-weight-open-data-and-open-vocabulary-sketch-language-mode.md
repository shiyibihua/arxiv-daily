---
layout: default
title: O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model
---

# O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.14368" target="_blank" class="toolbar-btn">arXiv: 2511.14368v1</a>
    <a href="https://arxiv.org/pdf/2511.14368.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14368v1" 
            onclick="toggleFavorite(this, '2511.14368v1', 'O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Rishi Gupta, Mukilan Karuppasamy, Shyam Marjit, Aditay Tripathi, Anirban Chakraborty

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

**å¤‡æ³¨**: Accepted to AAAI 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**O3SLMï¼šå¼€æ”¾æƒé‡ã€æ•°æ®å’Œè¯æ±‡çš„è‰å›¾-è¯­è¨€æ¨¡å‹ï¼Œæå‡æŠ½è±¡è§†è§‰è¾“å…¥ç†è§£èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `è‰å›¾ç†è§£` `å¤§è§„æ¨¡æ•°æ®é›†` `æŒ‡ä»¤å¾®è°ƒ` `å›¾åƒæ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LVLMåœ¨ç†è§£æŠ½è±¡è§†è§‰è¾“å…¥ï¼Œç‰¹åˆ«æ˜¯æ‰‹ç»˜è‰å›¾æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºO3SLMï¼Œé€šè¿‡æ„å»ºå¤§è§„æ¨¡å›¾åƒ-è‰å›¾-æŒ‡ä»¤ä¸‰å…ƒç»„æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒO3SLMåœ¨å¯¹è±¡å®šä½ã€è®¡æ•°ã€å›¾åƒæ£€ç´¢å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰LVLMã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å®é™…åº”ç”¨ä¸­æ—¥ç›Šæ™®åŠï¼Œä½†å…¶ç†è§£æŠ½è±¡è§†è§‰è¾“å…¥çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚ç‰¹åˆ«æ˜¯åœ¨ç†è§£æ‰‹ç»˜è‰å›¾æ–¹é¢ï¼Œè‰å›¾æä¾›äº†ä¸€ç§ç›´è§‚çš„æ–¹å¼æ¥è¡¨è¾¾éš¾ä»¥ç”¨æ–‡å­—æè¿°çš„æ¦‚å¿µï¼ŒLVLMsè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è®¤ä¸ºä¸»è¦ç“¶é¢ˆåœ¨äºç¼ºä¹ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†èƒ½å¤Ÿè”åˆå»ºæ¨¡è‰å›¾ã€é€¼çœŸå›¾åƒå’Œç›¸åº”çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼š(1)ä¸€ä¸ªæ–°çš„ã€å¤§è§„æ¨¡çš„å›¾åƒ-è‰å›¾-æŒ‡ä»¤ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œæ—¨åœ¨ä¿ƒè¿›é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒï¼›(2) O3SLMï¼Œä¸€ä¸ªåŸºäºè¯¥æ•°æ®é›†è®­ç»ƒçš„LVLMã€‚åœ¨å¤šä¸ªåŸºäºè‰å›¾çš„ä»»åŠ¡ä¸Šçš„ç»¼åˆè¯„ä¼°ï¼ŒåŒ…æ‹¬(a)å¯¹è±¡å®šä½ï¼Œ(b)è®¡æ•°ï¼Œ(c)å›¾åƒæ£€ç´¢(å³SBIRå’Œç»†ç²’åº¦SBIR)ï¼Œä»¥åŠ(d)è§†è§‰é—®ç­”(VQA)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒO3SLMåœ¨è‰å›¾ç†è§£å’Œæ¨ç†æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„LVLMï¼ŒåŒæ—¶æ•´åˆäº†ç°æœ‰çš„ä¸‰ä¸ªè‰å›¾æ•°æ®é›†ï¼Œå³QuickDraw!ã€Sketchyå’ŒTu Berlinï¼Œä»¥åŠæˆ‘ä»¬ç”Ÿæˆçš„SketchVCLæ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£æŠ½è±¡è§†è§‰è¾“å…¥ï¼Œå°¤å…¶æ˜¯æ‰‹ç»˜è‰å›¾æ–¹é¢å­˜åœ¨å›°éš¾ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å»ºæ¨¡è‰å›¾ã€çœŸå®å›¾åƒå’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è‰å›¾è¿™ç§ç›´è§‚çš„è¡¨è¾¾æ–¹å¼ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨éœ€è¦æŠ½è±¡è§†è§‰ç†è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„å›¾åƒ-è‰å›¾-æŒ‡ä»¤ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒä¸€ä¸ªå¼€æ”¾çš„è§†è§‰è¯­è¨€æ¨¡å‹O3SLMã€‚é€šè¿‡è”åˆå»ºæ¨¡è‰å›¾ã€å›¾åƒå’Œè¯­è¨€ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è‰å›¾æ‰€è¡¨è¾¾çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæå‡å…¶åœ¨ç›¸å…³ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†å¼¥è¡¥ç°æœ‰æ•°æ®é›†çš„ä¸è¶³ï¼Œå¹¶ä¸ºLVLMæä¾›æ›´ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šO3SLMçš„æ•´ä½“æ¡†æ¶æ˜¯ä¸€ä¸ªæ ‡å‡†çš„è§†è§‰è¯­è¨€æ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬è§†è§‰ç¼–ç å™¨ã€è¯­è¨€æ¨¡å‹å’Œè·¨æ¨¡æ€äº¤äº’æ¨¡å—ã€‚é¦–å…ˆï¼Œè§†è§‰ç¼–ç å™¨å°†å›¾åƒå’Œè‰å›¾ç¼–ç æˆè§†è§‰ç‰¹å¾ï¼›ç„¶åï¼Œè¯­è¨€æ¨¡å‹å¤„ç†è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼›æœ€åï¼Œè·¨æ¨¡æ€äº¤äº’æ¨¡å—å°†è§†è§‰ç‰¹å¾å’Œè¯­è¨€ç‰¹å¾èåˆï¼Œç”¨äºé¢„æµ‹æˆ–ç”Ÿæˆç›®æ ‡è¾“å‡ºã€‚è¯¥æ¡†æ¶çš„å…³é”®åœ¨äºä½¿ç”¨å¤§è§„æ¨¡çš„å›¾åƒ-è‰å›¾-æŒ‡ä»¤ä¸‰å…ƒç»„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°è‰å›¾å’Œè¯­è¨€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†SketchVCLï¼Œè¯¥æ•°æ®é›†åŒ…å«å›¾åƒã€è‰å›¾å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„ä¸‰å…ƒç»„ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒSketchVCLè§„æ¨¡æ›´å¤§ï¼Œè¦†ç›–çš„è¯­ä¹‰ä¿¡æ¯æ›´ä¸°å¯Œï¼Œæ›´é€‚åˆç”¨äºè®­ç»ƒèƒ½å¤Ÿç†è§£è‰å›¾çš„LVLMã€‚æ­¤å¤–ï¼ŒO3SLMçš„å¼€æ”¾æƒé‡ã€å¼€æ”¾æ•°æ®å’Œå¼€æ”¾è¯æ±‡çš„è®¾è®¡ä¹Ÿä¿ƒè¿›äº†ç ”ç©¶çš„å¤ç°å’Œè¿›ä¸€æ­¥å‘å±•ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†SketchVCLçš„æ„å»ºé‡‡ç”¨äº†åŠè‡ªåŠ¨åŒ–çš„æ–¹æ³•ï¼Œé¦–å…ˆåˆ©ç”¨ç°æœ‰çš„å›¾åƒæ•°æ®é›†å’Œè‰å›¾ç”Ÿæˆç®—æ³•ç”Ÿæˆè‰å›¾ï¼Œç„¶åäººå·¥ç¼–å†™ç›¸åº”çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚åœ¨æ¨¡å‹è®­ç»ƒæ–¹é¢ï¼Œé‡‡ç”¨äº†å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¼˜åŒ–å¤šä¸ªæŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬å›¾åƒ-è‰å›¾åŒ¹é…æŸå¤±ã€è¯­è¨€ç”ŸæˆæŸå¤±å’Œä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æ ¹æ®ä¸åŒçš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

O3SLMåœ¨å¤šä¸ªåŸºäºè‰å›¾çš„ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»†ç²’åº¦è‰å›¾å›¾åƒæ£€ç´¢(FG-SBIR)ä»»åŠ¡ä¸Šï¼ŒO3SLMçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰LVLMã€‚æ­¤å¤–ï¼ŒO3SLMåœ¨å¯¹è±¡å®šä½ã€è®¡æ•°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨è‰å›¾ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‰å›¾è¾…åŠ©è®¾è®¡ã€å›¾åƒæ£€ç´¢ã€è§†è§‰é—®ç­”ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç»˜åˆ¶è‰å›¾æ¥æœç´¢å›¾åƒï¼Œæˆ–ä¸AIç³»ç»Ÿè¿›è¡ŒåŸºäºè‰å›¾çš„äº¤äº’ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ•™è‚²ã€è‰ºæœ¯åˆ›ä½œã€äººæœºäº¤äº’ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œå·¥ä½œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.

