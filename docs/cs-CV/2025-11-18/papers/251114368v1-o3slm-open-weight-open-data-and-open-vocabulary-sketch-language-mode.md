---
layout: default
title: O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model
---

# O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model

**arXiv**: [2511.14368v1](https://arxiv.org/abs/2511.14368) | [PDF](https://arxiv.org/pdf/2511.14368.pdf)

**ä½œè€…**: Rishi Gupta, Mukilan Karuppasamy, Shyam Marjit, Aditay Tripathi, Anirban Chakraborty

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºO3SLMæ¨¡åž‹ä¸Žæ•°æ®é›†ä»¥è§£å†³å¤§è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è‰å›¾ç†è§£ä¸Šçš„ç“¶é¢ˆ**

**å…³é”®è¯**: `è‰å›¾è¯­è¨€æ¨¡åž‹` `å¤§è§„æ¨¡æ•°æ®é›†` `è§†è§‰è¯­è¨€ç†è§£` `è‰å›¾è§†è§‰é—®ç­”` `å¤šæ¨¡æ€å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è§†è§‰è¯­è¨€æ¨¡åž‹éš¾ä»¥ç†è§£æ‰‹ç»˜è‰å›¾ç­‰æŠ½è±¡è§†è§‰è¾“å…¥ï¼Œç¼ºä¹å¤§è§„æ¨¡è‰å›¾-å›¾åƒ-æŒ‡ä»¤æ•°æ®é›†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤§è§„æ¨¡å›¾åƒ-è‰å›¾-æŒ‡ä»¤ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒO3SLMæ¨¡åž‹è¿›è¡Œé¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒä¼˜ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è‰å›¾å®šä½ã€è®¡æ•°ã€æ£€ç´¢å’ŒVQAä»»åŠ¡ä¸­ï¼ŒO3SLMå®žçŽ°SOTAæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.

