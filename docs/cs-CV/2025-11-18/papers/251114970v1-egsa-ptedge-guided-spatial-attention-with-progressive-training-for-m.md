---
layout: default
title: EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects
---

# EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects

**arXiv**: [2511.14970v1](https://arxiv.org/abs/2511.14970) | [PDF](https://arxiv.org/pdf/2511.14970.pdf)

**ä½œè€…**: Gbenga Omotara, Ramy Farag, Seyed Mohamad Ali Tousi, G. N. DeSouza

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEGSA-PTï¼Œé€šè¿‡è¾¹ç¼˜å¼•å¯¼ç©ºé—´æ³¨æ„åŠ›å’Œæ¸è¿›å¼è®­ç»ƒæå‡é€æ˜Žç‰©ä½“æ·±åº¦ä¼°è®¡ä¸Žåˆ†å‰²æ€§èƒ½**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `é€æ˜Žç‰©ä½“æ„ŸçŸ¥` `æ·±åº¦ä¼°è®¡` `è¯­ä¹‰åˆ†å‰²` `è¾¹ç¼˜å¼•å¯¼` `ç©ºé—´æ³¨æ„åŠ›` `å¤šæ¨¡æ€å­¦ä¹ ` `æ¸è¿›å¼è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é€æ˜Žç‰©ä½“æ„ŸçŸ¥æ˜¯è®¡ç®—æœºè§†è§‰éš¾é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ä¸Šè¡¨çŽ°ä¸ä½³ï¼Œå¤šä»»åŠ¡å­¦ä¹ æ˜“å—è´Ÿé¢è·¨ä»»åŠ¡å½±å“ã€‚
2. æå‡ºè¾¹ç¼˜å¼•å¯¼ç©ºé—´æ³¨æ„åŠ›ï¼ˆEGSAï¼‰æœºåˆ¶ï¼Œèžåˆè¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ï¼Œåˆ©ç”¨è¾¹ç¼˜ä¿¡æ¯ç¼“è§£è´Ÿé¢äº¤äº’ï¼Œæå‡é€æ˜Žç‰©ä½“æ„ŸçŸ¥ã€‚
3. å¼•å…¥å¤šæ¨¡æ€æ¸è¿›å¼è®­ç»ƒï¼Œä»ŽRGBè¾¹ç¼˜åˆ°é¢„æµ‹æ·±åº¦è¾¹ç¼˜ï¼Œæ— éœ€ground-truthæ·±åº¦å³å¯å¼•å¯¼å­¦ä¹ ï¼Œå¹¶åœ¨Syn-TODDå’ŒClearPoseä¸ŠéªŒè¯æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€æ˜Žç‰©ä½“æ„ŸçŸ¥æ˜¯è®¡ç®—æœºè§†è§‰ç ”ç©¶ä¸­çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå› ä¸ºé€æ˜Žæ€§ä¼šæ··æ·†æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ã€‚æœ€è¿‘çš„ç ”ç©¶æŽ¢ç´¢äº†å¤šä»»åŠ¡å­¦ä¹ æ¡†æž¶ä»¥æé«˜é²æ£’æ€§ï¼Œä½†è´Ÿé¢çš„è·¨ä»»åŠ¡äº¤äº’é€šå¸¸ä¼šé˜»ç¢æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¾¹ç¼˜å¼•å¯¼ç©ºé—´æ³¨æ„åŠ›ï¼ˆEGSAï¼‰èžåˆæœºåˆ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†è¾¹ç•Œä¿¡æ¯æ•´åˆåˆ°è¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ä¹‹é—´çš„èžåˆä¸­æ¥å‡è½»ç ´åæ€§äº¤äº’ã€‚åœ¨Syn-TODDå’ŒClearPoseåŸºå‡†æµ‹è¯•ä¸­ï¼ŒEGSAå§‹ç»ˆä¼˜äºŽå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ˆMODESTï¼‰ï¼Œæé«˜äº†æ·±åº¦ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«žäº‰åŠ›çš„åˆ†å‰²æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨é€æ˜ŽåŒºåŸŸçš„æ”¹è¿›æœ€ä¸ºæ˜¾è‘—ã€‚é™¤äº†èžåˆè®¾è®¡ä¹‹å¤–ï¼Œæœ¬æ–‡çš„ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯ä¸€ç§å¤šæ¨¡æ€æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œå…¶ä¸­å­¦ä¹ ä»ŽRGBå›¾åƒå¯¼å‡ºçš„è¾¹ç¼˜è¿‡æ¸¡åˆ°ä»Žé¢„æµ‹æ·±åº¦å›¾åƒå¯¼å‡ºçš„è¾¹ç¼˜ã€‚è¿™ç§æ–¹æ³•å…è®¸ç³»ç»Ÿä»ŽRGBå›¾åƒä¸­åŒ…å«çš„ä¸°å¯Œçº¹ç†ä¸­å¼•å¯¼å­¦ä¹ ï¼Œç„¶åŽåˆ‡æ¢åˆ°æ·±åº¦å›¾ä¸­æ›´ç›¸å…³çš„å‡ ä½•å†…å®¹ï¼ŒåŒæ—¶æ¶ˆé™¤äº†è®­ç»ƒæ—¶å¯¹ground-truthæ·±åº¦çš„éœ€æ±‚ã€‚æ€»ä¹‹ï¼Œè¿™äº›è´¡çŒ®çªå‡ºäº†è¾¹ç¼˜å¼•å¯¼èžåˆä½œä¸ºä¸€ç§èƒ½å¤Ÿæé«˜é€æ˜Žç‰©ä½“æ„ŸçŸ¥çš„é²æ£’æ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé€æ˜Žç‰©ä½“çš„æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„éš¾ç‚¹ã€‚ç”±äºŽé€æ˜Žç‰©ä½“ç¼ºä¹çº¹ç†å’Œé¢œè‰²ä¿¡æ¯ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å‡†ç¡®ä¼°è®¡å…¶æ·±åº¦å’Œè¿›è¡Œåˆ†å‰²ã€‚çŽ°æœ‰çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•è¯•å›¾åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œä½†å¸¸å¸¸å—åˆ°è´Ÿé¢è·¨ä»»åŠ¡äº¤äº’çš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¾¹ç¼˜ä¿¡æ¯æ¥å¼•å¯¼ç‰¹å¾èžåˆï¼Œä»Žè€Œå‡è½»è´Ÿé¢è·¨ä»»åŠ¡äº¤äº’ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è¾¹ç¼˜å¼•å¯¼ç©ºé—´æ³¨æ„åŠ›ï¼ˆEGSAï¼‰æœºåˆ¶ï¼Œå°†è¾¹ç¼˜ä¿¡æ¯æ•´åˆåˆ°è¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾çš„èžåˆä¸­ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´åŠ å…³æ³¨ç‰©ä½“çš„è¾¹ç•Œï¼Œä»Žè€Œæé«˜æ·±åº¦ä¼°è®¡å’Œåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¤šæ¨¡æ€æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥å¼•å¯¼æ¨¡åž‹å­¦ä¹ ï¼Œé¿å…äº†å¯¹ground-truthæ·±åº¦çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…å«æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ä¸¤ä¸ªåˆ†æ”¯ï¼Œä»¥åŠEGSAèžåˆæ¨¡å—å’Œæ¸è¿›å¼è®­ç»ƒç­–ç•¥ã€‚é¦–å…ˆï¼ŒRGBå›¾åƒåˆ†åˆ«è¾“å…¥åˆ°æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ç½‘ç»œä¸­ï¼Œæå–å‡ ä½•ç‰¹å¾å’Œè¯­ä¹‰ç‰¹å¾ã€‚ç„¶åŽï¼ŒEGSAæ¨¡å—åˆ©ç”¨è¾¹ç¼˜ä¿¡æ¯å¯¹è¿™ä¸¤ä¸ªç‰¹å¾è¿›è¡Œèžåˆï¼Œå¾—åˆ°èžåˆåŽçš„ç‰¹å¾ã€‚æœ€åŽï¼Œåˆ©ç”¨èžåˆåŽçš„ç‰¹å¾è¿›è¡Œæ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ã€‚æ¸è¿›å¼è®­ç»ƒç­–ç•¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨RGBå›¾åƒçš„è¾¹ç¼˜ä¿¡æ¯è¿›è¡Œè®­ç»ƒï¼›ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨é¢„æµ‹æ·±åº¦å›¾åƒçš„è¾¹ç¼˜ä¿¡æ¯è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†è¾¹ç¼˜å¼•å¯¼ç©ºé—´æ³¨æ„åŠ›ï¼ˆEGSAï¼‰æœºåˆ¶å’Œå¤šæ¨¡æ€æ¸è¿›å¼è®­ç»ƒç­–ç•¥ã€‚EGSAæœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°èžåˆè¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¾¹ç¼˜ä¿¡æ¯æ¥å¼•å¯¼ç‰¹å¾èžåˆï¼Œä»Žè€Œæé«˜æ·±åº¦ä¼°è®¡å’Œåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚å¤šæ¨¡æ€æ¸è¿›å¼è®­ç»ƒç­–ç•¥èƒ½å¤Ÿé€æ­¥å¼•å¯¼æ¨¡åž‹å­¦ä¹ ï¼Œé¿å…äº†å¯¹ground-truthæ·±åº¦çš„ä¾èµ–ï¼Œå¹¶ä¸”èƒ½å¤Ÿåˆ©ç”¨RGBå›¾åƒå’Œæ·±åº¦å›¾åƒçš„äº’è¡¥ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šEGSAæ¨¡å—ä½¿ç”¨ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®è¾¹ç¼˜ä¿¡æ¯å¯¹ç‰¹å¾å›¾çš„ä¸åŒåŒºåŸŸè¿›è¡ŒåŠ æƒã€‚è¾¹ç¼˜ä¿¡æ¯é€šè¿‡Cannyè¾¹ç¼˜æ£€æµ‹å™¨ä»ŽRGBå›¾åƒæˆ–é¢„æµ‹æ·±åº¦å›¾åƒä¸­æå–ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ·±åº¦ä¼°è®¡æŸå¤±å’Œè¯­ä¹‰åˆ†å‰²æŸå¤±ã€‚æ·±åº¦ä¼°è®¡æŸå¤±é‡‡ç”¨L1æŸå¤±æˆ–Smooth L1æŸå¤±ã€‚è¯­ä¹‰åˆ†å‰²æŸå¤±é‡‡ç”¨äº¤å‰ç†µæŸå¤±ã€‚ç½‘ç»œç»“æž„å¯ä»¥é‡‡ç”¨å„ç§çŽ°æœ‰çš„æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼Œä¾‹å¦‚ResNetã€UNetç­‰ã€‚æ¸è¿›å¼è®­ç»ƒç­–ç•¥ä¸­ï¼Œä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒè½®æ•°å’Œå­¦ä¹ çŽ‡éœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

EGSAåœ¨Syn-TODDå’ŒClearPoseæ•°æ®é›†ä¸Šå‡ä¼˜äºŽå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•MODESTï¼Œå°¤å…¶åœ¨é€æ˜ŽåŒºåŸŸçš„æ·±åº¦ä¼°è®¡ç²¾åº¦æå‡æ˜¾è‘—ã€‚åœ¨ä¿æŒåˆ†å‰²æ€§èƒ½çš„åŒæ—¶ï¼Œæ·±åº¦ä¼°è®¡ç²¾åº¦å¾—åˆ°äº†æœ‰æ•ˆæé«˜ï¼Œè¯æ˜Žäº†è¾¹ç¼˜å¼•å¯¼èžåˆç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæå‡ºçš„æ¸è¿›å¼è®­ç»ƒç­–ç•¥æ— éœ€ground-truthæ·±åº¦å³å¯å®žçŽ°æœ‰æ•ˆè®­ç»ƒã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€å¢žå¼ºçŽ°å®žç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæŠ“å–é€æ˜Žç‰©ä½“æ—¶ï¼Œå‡†ç¡®çš„æ·±åº¦ä¼°è®¡å’Œåˆ†å‰²æ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œè¯†åˆ«é€æ˜Žç‰©ä½“ï¼ˆå¦‚çŽ»ç’ƒã€æ°´é¢ï¼‰æœ‰åŠ©äºŽæé«˜å®‰å…¨æ€§ã€‚åœ¨å¢žå¼ºçŽ°å®žä¸­ï¼Œå¯ä»¥å®žçŽ°æ›´é€¼çœŸçš„é€æ˜Žç‰©ä½“æ¸²æŸ“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.

