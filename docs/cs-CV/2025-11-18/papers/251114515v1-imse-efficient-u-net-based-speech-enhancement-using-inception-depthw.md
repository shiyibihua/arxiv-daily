---
layout: default
title: IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention
---

# IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention

**arXiv**: [2511.14515v1](https://arxiv.org/abs/2511.14515) | [PDF](https://arxiv.org/pdf/2511.14515.pdf)

**ä½œè€…**: Xinxin Tang, Bin Qin, Yufang Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIMSEç½‘ç»œï¼Œä½¿ç”¨MALAå’ŒIDConvä¼˜åŒ–è¯­éŸ³å¢žå¼ºï¼Œå®žçŽ°è¶…è½»é‡é«˜æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è¯­éŸ³å¢žå¼º` `è¶…è½»é‡ç½‘ç»œ` `çº¿æ€§æ³¨æ„åŠ›` `æ·±åº¦å·ç§¯` `U-Netæž¶æž„` `å‚æ•°ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¦‚MUSEåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šå­˜åœ¨æ•ˆçŽ‡ç“¶é¢ˆï¼Œå‚æ•°å†—ä½™å’Œè®¡ç®—è´Ÿæ‹…é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æŒ¯å¹…æ„ŸçŸ¥çº¿æ€§æ³¨æ„åŠ›MALAå’ŒInceptionæ·±åº¦å·ç§¯IDConvï¼Œæå‡å…¨å±€å»ºæ¨¡å’Œç‰¹å¾æå–æ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VoiceBank+DEMANDæ•°æ®é›†ä¸Šï¼Œå‚æ•°å‡å°‘16.8%ï¼ŒPESQè¾¾3.373ï¼Œæ€§èƒ½åª²ç¾ŽSOTAã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex "approximate-compensate" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the "amplitude-ignoring" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.

