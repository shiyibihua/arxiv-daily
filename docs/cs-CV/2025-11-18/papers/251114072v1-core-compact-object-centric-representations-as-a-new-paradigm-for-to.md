---
layout: default
title: CORE: Compact Object-centric REpresentations as a New Paradigm for Token Merging in LVLMs
---

# CORE: Compact Object-centric REpresentations as a New Paradigm for Token Merging in LVLMs

**arXiv**: [2511.14072v1](https://arxiv.org/abs/2511.14072) | [PDF](https://arxiv.org/pdf/2511.14072.pdf)

**ä½œè€…**: Jingyu Lei, Gaoang Wang, Der-Horng Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCOREå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºèŒƒå¼ï¼Œä»¥è§£å†³LVLMè§†è§‰ä»¤ç‰ŒåŽ‹ç¼©ä¸­çš„è¯­ä¹‰ç¼ºå¤±é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ä»¤ç‰ŒåŽ‹ç¼©` `å¯¹è±¡ä¸­å¿ƒè¡¨ç¤º` `è¯­ä¹‰åˆ†å‰²` `é«˜æ•ˆè®¡ç®—` `åŸºå‡†æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LVLMè§†è§‰ä»¤ç‰Œéšå›¾åƒåˆ†è¾¨çŽ‡äºŒæ¬¡å¢žé•¿ï¼Œå¯¼è‡´è®¡ç®—å’Œå†…å­˜æˆæœ¬é«˜æ˜‚
2. COREåˆ©ç”¨åˆ†å‰²è§£ç å™¨ç”Ÿæˆå¯¹è±¡æŽ©ç ï¼ŒæŒ‡å¯¼ä»¤ç‰Œåˆå¹¶ä¸ºç´§å‡‘å¯¹è±¡ä¸­å¿ƒè¡¨ç¤º
3. å®žéªŒæ˜¾ç¤ºCOREåœ¨å›ºå®šå’Œè‡ªé€‚åº”åŽ‹ç¼©çŽ‡ä¸‹å‡å®žçŽ°SOTAï¼Œæžç«¯åŽ‹ç¼©ä¸‹æ€§èƒ½æŸå¤±æžå°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) usually suffer from prohibitive computational and memory costs due to the quadratic growth of visual tokens with image resolution. Existing token compression methods, while varied, often lack a high-level semantic understanding, leading to suboptimal merges, information redundancy, or context loss. To address these limitations, we introduce CORE (Compact Object-centric REpresentations), a new paradigm for visual token compression. CORE leverages an efficient segmentation decoder to generate object masks, which serve as a high-level semantic prior to guide the merging of visual tokens into a compact set of object-centric representations. Furthermore, a novel centroid-guided sorting mechanism restores a coherent spatial order to the merged tokens, preserving vital positional information. Extensive experiments show that CORE not only establishes a new state-of-the-art on six authoritative benchmarks for fixed-rate compression, but also achieves dramatic efficiency gains in adaptive-rate settings. Even under extreme compression, after aggressively retaining with only 2.2% of all visual tokens, CORE still maintains 97.4% of baseline performance. Our work demonstrates the superiority of object-centric representations for efficient and effective LVLM processing.

