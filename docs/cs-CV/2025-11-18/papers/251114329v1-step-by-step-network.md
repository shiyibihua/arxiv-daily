---
layout: default
title: Step by Step Network
---

# Step by Step Network

**arXiv**: [2511.14329v1](https://arxiv.org/abs/2511.14329) | [PDF](https://arxiv.org/pdf/2511.14329.pdf)

**ä½œè€…**: Dongchen Han, Tianzhu Ye, Zhuofan Xia, Kaiyi Chen, Yulin Wang, Hanting Chen, Gao Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStep by Step Networkä»¥è§£å†³æ·±åº¦ç½‘ç»œä¸­çš„æ·å¾„é€€åŒ–å’Œå®½åº¦é™åˆ¶é—®é¢˜**

**å…³é”®è¯**: `æ·±åº¦ç¥žç»ç½‘ç»œ` `æ®‹å·®æž¶æž„` `æ·å¾„é€€åŒ–` `å®½åº¦é™åˆ¶` `æ¸è¿›å­¦ä¹ ` `å›¾åƒåˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ·±åº¦æ®‹å·®ç½‘ç»œå­˜åœ¨æ·å¾„é€€åŒ–å’Œå®½åº¦é™åˆ¶ï¼Œé˜»ç¢ç†è®ºèƒ½åŠ›å‘æŒ¥
2. æ–¹æ³•è¦ç‚¹ï¼šæ²¿é€šé“ç»´åº¦åˆ†ç¦»ç‰¹å¾ï¼Œé€šè¿‡å®½åº¦é€’å¢žçš„å—å †å å®žçŽ°æ¸è¿›å­¦ä¹ 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­ä¸€è‡´ä¼˜äºŽæ®‹å·®æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Scaling up network depth is a fundamental pursuit in neural architecture design, as theory suggests that deeper models offer exponentially greater capability. Benefiting from the residual connections, modern neural networks can scale up to more than one hundred layers and enjoy wide success. However, as networks continue to deepen, current architectures often struggle to realize their theoretical capacity improvements, calling for more advanced designs to further unleash the potential of deeper networks. In this paper, we identify two key barriers that obstruct residual models from scaling deeper: shortcut degradation and limited width. Shortcut degradation hinders deep-layer learning, while the inherent depth-width trade-off imposes limited width. To mitigate these issues, we propose a generalized residual architecture dubbed Step by Step Network (StepsNet) to bridge the gap between theoretical potential and practical performance of deep models. Specifically, we separate features along the channel dimension and let the model learn progressively via stacking blocks with increasing width. The resulting method mitigates the two identified problems and serves as a versatile macro design applicable to various models. Extensive experiments show that our method consistently outperforms residual models across diverse tasks, including image classification, object detection, semantic segmentation, and language modeling. These results position StepsNet as a superior generalization of the widely adopted residual architecture.

