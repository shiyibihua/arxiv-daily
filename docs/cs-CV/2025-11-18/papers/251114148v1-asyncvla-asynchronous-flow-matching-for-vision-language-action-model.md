---
layout: default
title: AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models
---

# AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models

**arXiv**: [2511.14148v1](https://arxiv.org/abs/2511.14148) | [PDF](https://arxiv.org/pdf/2511.14148.pdf)

**ä½œè€…**: Yuhua Jiang, Shuang Cheng, Yan Ding, Feifei Gao, Biqing Qi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼‚æ­¥æµåŒ¹é…VLAä»¥è§£å†³é•¿è§†é‡Žä»»åŠ¡ä¸­åŠ¨ä½œç”Ÿæˆä¸ç¨³å®šé—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `å¼‚æ­¥æµåŒ¹é…` `åŠ¨ä½œè‡ªæ ¡æ­£` `æœºå™¨äººæ“ä½œ` `é•¿è§†é‡Žä»»åŠ¡` `ç½®ä¿¡åº¦è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸVLAæ¨¡åž‹ä½¿ç”¨åŒæ­¥æµåŒ¹é…ï¼Œç¼ºä¹åŠ¨ä½œä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œæ˜“åœ¨é•¿è§†é‡Žä»»åŠ¡ä¸­å¤±è´¥
2. å¼•å…¥å¼‚æ­¥æµåŒ¹é…å’Œç½®ä¿¡åº¦è¯„ä¼°å™¨ï¼Œå®žçŽ°éžå‡åŒ€æ—¶é—´è°ƒåº¦å’ŒåŠ¨ä½œè‡ªæ ¡æ­£
3. åœ¨æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ•°æ®é«˜æ•ˆä¸”è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå±•ç¤ºè‡ªæ ¡æ­£èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.

