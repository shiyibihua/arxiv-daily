---
layout: default
title: FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation
---

# FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation

**arXiv**: [2511.14712v1](https://arxiv.org/abs/2511.14712) | [PDF](https://arxiv.org/pdf/2511.14712.pdf)

**ä½œè€…**: Yunfeng Wu, Jiayi Song, Zhenxiong Tan, Zihao He, Songhua Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFreeSwimæ–¹æ³•ä»¥è®­ç»ƒå…è´¹ç”Ÿæˆè¶…é«˜åˆ†è¾¨çŽ‡è§†é¢‘ï¼Œé€šè¿‡æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶è§£å†³è®¡ç®—å¤æ‚åº¦é—®é¢˜ã€‚**

**å…³é”®è¯**: `è¶…é«˜åˆ†è¾¨çŽ‡è§†é¢‘ç”Ÿæˆ` `æ»‘åŠ¨çª—å£æ³¨æ„åŠ›` `è®­ç»ƒå…è´¹æ–¹æ³•` `æ‰©æ•£Transformer` `äº¤å‰æ³¨æ„åŠ›æœºåˆ¶` `è§†é¢‘åˆæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šTransformeræ³¨æ„åŠ›æœºåˆ¶åœ¨è¶…é«˜åˆ†è¾¨çŽ‡è§†é¢‘ç”Ÿæˆä¸­è®¡ç®—å¤æ‚åº¦é«˜ï¼Œå¯¼è‡´è®­ç»ƒæˆæœ¬å·¨å¤§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å‘å†…æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸ŽåŒè·¯å¾„ç®¡é“ï¼Œç»“åˆäº¤å‰æ³¨æ„åŠ›è¦†ç›–ç­–ç•¥ï¼Œç¡®ä¿å±€éƒ¨ç»†èŠ‚ä¸Žå…¨å±€ä¸€è‡´æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VBenchä¸Šè¡¨çŽ°ä¼˜å¼‚ï¼Œç”Ÿæˆè§†é¢‘ç»†èŠ‚ä¸°å¯Œã€æ•ˆçŽ‡é«˜ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim

