---
layout: default
title: Parameter Aware Mamba Model for Multi-task Dense Prediction
---

# Parameter Aware Mamba Model for Multi-task Dense Prediction

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.14503" target="_blank" class="toolbar-btn">arXiv: 2511.14503v1</a>
    <a href="https://arxiv.org/pdf/2511.14503.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14503v1" 
            onclick="toggleFavorite(this, '2511.14503v1', 'Parameter Aware Mamba Model for Multi-task Dense Prediction')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xinzhuo Yu, Yunzhi Zhuge, Sitong Gong, Lu Zhang, Pingping Zhang, Huchuan Lu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-18

**Â§áÊ≥®**: Accepted to IEEE Transactions on Cybernetics

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/CQC-gogopro/PAMM)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂèÇÊï∞ÊÑüÁü•MambaÊ®°ÂûãPAMMÔºåÁî®‰∫éÂ§ö‰ªªÂä°ÂØÜÈõÜÈ¢ÑÊµãÔºåÊèêÂçá‰ªªÂä°Èó¥‰∫íËÅîÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§ö‰ªªÂä°Â≠¶‰π†` `ÂØÜÈõÜÈ¢ÑÊµã` `Áä∂ÊÄÅÁ©∫Èó¥Ê®°Âûã` `Mamba` `ÂèÇÊï∞ÊÑüÁü•`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ö‰ªªÂä°ÂØÜÈõÜÈ¢ÑÊµã‰∏≠‰∏ªË¶Å‰æùËµñÂç∑ÁßØÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂Êé¢Á¥¢‰ªªÂä°Èó¥‰∫§‰∫íÔºåÁº∫‰πèÂØπ‰ªªÂä°ÂÜÖÂú®Â±ûÊÄßÁöÑÊúâÊïàÂª∫Ê®°„ÄÇ
2. PAMMÂà©Áî®Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMÔºâÁöÑÂèÇÊï∞ÂåñËÉΩÂäõÔºåÈÄöËøáÂèåÁä∂ÊÄÅÁ©∫Èó¥ÂèÇÊï∞‰∏ìÂÆ∂ÈõÜÊàê‰ªªÂä°ÁâπÂÆöÂÖàÈ™åÔºåÂ¢ûÂº∫‰ªªÂä°‰∫íËÅîÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåPAMMÂú®NYUD-v2ÂíåPASCAL-ContextÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Â§ö‰ªªÂä°ÂØÜÈõÜÈ¢ÑÊµã‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éËß£Á†ÅÂô®ÁöÑÊ°ÜÊû∂ÔºåÂç≥ÂèÇÊï∞ÊÑüÁü•MambaÊ®°ÂûãÔºàPAMMÔºâÔºå‰∏ìÈó®‰∏∫Â§ö‰ªªÂä°Â≠¶‰π†ÁéØÂ¢É‰∏ãÁöÑÂØÜÈõÜÈ¢ÑÊµãËÄåËÆæËÆ°„ÄÇ‰∏é‰ΩøÁî®TransformerÂª∫Ê®°Êï¥‰Ωì‰ªªÂä°ÂÖ≥Á≥ªÁöÑÊñπÊ≥ï‰∏çÂêåÔºåPAMMÂà©Áî®Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMÔºâ‰∏∞ÂØå‰∏îÂèØÊâ©Â±ïÁöÑÂèÇÊï∞Êù•Â¢ûÂº∫‰ªªÂä°Èó¥ÁöÑ‰∫íËÅîÊÄß„ÄÇÂÆÉÈááÁî®ÂèåÁä∂ÊÄÅÁ©∫Èó¥ÂèÇÊï∞‰∏ìÂÆ∂ÔºåÈõÜÊàêÂπ∂ËÆæÁΩÆ‰ªªÂä°ÁâπÂÆöÁöÑÂèÇÊï∞ÂÖàÈ™åÔºå‰ªéËÄåÊçïËé∑ÊØè‰∏™‰ªªÂä°ÁöÑÂÜÖÂú®Â±ûÊÄß„ÄÇËøôÁßçÊñπÊ≥ï‰∏ç‰ªÖ‰øÉËøõ‰∫ÜÁ≤æÁ°ÆÁöÑÂ§ö‰ªªÂä°‰∫§‰∫íÔºåËøòÂÖÅËÆ∏ÈÄöËøáÁªìÊûÑÂåñÁöÑÁä∂ÊÄÅÁ©∫Èó¥Â∫èÂàóÊ®°ÂûãÔºàS4ÔºâËøõË°å‰ªªÂä°ÂÖàÈ™åÁöÑÂÖ®Â±ÄÈõÜÊàê„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÈááÁî®Â§öÊñπÂêëHilbertÊâ´ÊèèÊñπÊ≥ïÊù•ÊûÑÂª∫Â§öËßíÂ∫¶ÁâπÂæÅÂ∫èÂàóÔºå‰ªéËÄåÂ¢ûÂº∫Â∫èÂàóÊ®°ÂûãÂØπ2DÊï∞ÊçÆÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇÂú®NYUD-v2ÂíåPASCAL-ContextÂü∫ÂáÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åËØÅÊòé‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§ö‰ªªÂä°ÂØÜÈõÜÈ¢ÑÊµãÊó®Âú®ÂêåÊó∂È¢ÑÊµãÂõæÂÉèÁöÑÂ§ö‰∏™Â±ûÊÄßÔºå‰æãÂ¶ÇÊ∑±Â∫¶„ÄÅËØ≠‰πâÂàÜÂâ≤ÂíåË°®Èù¢Ê≥ïÁ∫ø„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÂü∫‰∫éÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNÔºâÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊ®°ÂûãÔºåÂú®ÊçïÊçâ‰ªªÂä°Èó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ªÂíåÂà©Áî®‰ªªÂä°ÁâπÂÆöÂÖàÈ™åÁü•ËØÜÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄßÔºåÈöæ‰ª•ÂÖÖÂàÜÊåñÊéò‰ªªÂä°Èó¥ÁöÑ‰∫íË°•‰ø°ÊÅØ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöPAMMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMÔºâÔºåÁâπÂà´ÊòØMambaÊû∂ÊûÑÔºåÂÖ∂ÂÖ∑ÊúâÂº∫Â§ßÁöÑÂ∫èÂàóÂª∫Ê®°ËÉΩÂäõÂíåÂèÇÊï∞ÂåñÊïàÁéáÔºåÊù•ÊòæÂºèÂú∞Âª∫Ê®°‰ªªÂä°Èó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÂíå‰ªªÂä°ÁâπÂÆöÁöÑÂÖàÈ™åÁü•ËØÜ„ÄÇÈÄöËøáÂèÇÊï∞ÂåñÁöÑÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºåPAMMËÉΩÂ§üÂ≠¶‰π†Âà∞ÊØè‰∏™‰ªªÂä°ÁöÑÂÜÖÂú®Â±ûÊÄßÔºåÂπ∂Â∞ÜÂÖ∂ËûçÂÖ•Âà∞Â§ö‰ªªÂä°Â≠¶‰π†ËøáÁ®ã‰∏≠Ôºå‰ªéËÄåÊèêÂçáÊï¥‰ΩìÈ¢ÑÊµãÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPAMMÊòØ‰∏Ä‰∏™Âü∫‰∫éËß£Á†ÅÂô®ÁöÑÊ°ÜÊû∂Ôºå‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ÁâπÂæÅÊèêÂèñÂô®ÔºöÁî®‰∫éÊèêÂèñËæìÂÖ•ÂõæÂÉèÁöÑÁâπÂæÅË°®Á§∫„ÄÇ2) ÂèåÁä∂ÊÄÅÁ©∫Èó¥ÂèÇÊï∞‰∏ìÂÆ∂ÔºöÂåÖÂê´‰∏§‰∏™Áã¨Á´ãÁöÑSSMÔºåÂàÜÂà´Áî®‰∫éÂ≠¶‰π†‰ªªÂä°ÁâπÂÆöÁöÑÂèÇÊï∞ÂÖàÈ™å„ÄÇ3) Áä∂ÊÄÅÁ©∫Èó¥Â∫èÂàóÊ®°ÂûãÔºàS4ÔºâÔºöÁî®‰∫éÂÖ®Â±ÄÈõÜÊàê‰ªªÂä°ÂÖàÈ™åÔºåÂπ∂Âª∫Ê®°‰ªªÂä°Èó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇ4) Â§öÊñπÂêëHilbertÊâ´ÊèèÔºöÁî®‰∫éÂ∞Ü2DÁâπÂæÅÂõæËΩ¨Êç¢‰∏∫Â∫èÂàóÔºå‰ª•‰æøSSMËøõË°åÂ§ÑÁêÜ„ÄÇ5) Ëß£Á†ÅÂô®ÔºöÁî®‰∫éÂ∞ÜÂ∫èÂàóË°®Á§∫ËΩ¨Êç¢‰∏∫ÊúÄÁªàÁöÑÂØÜÈõÜÈ¢ÑÊµãÁªìÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPAMMÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÂºïÂÖ•ÂèåÁä∂ÊÄÅÁ©∫Èó¥ÂèÇÊï∞‰∏ìÂÆ∂ÔºåÊòæÂºèÂú∞Âª∫Ê®°‰ªªÂä°ÁâπÂÆöÁöÑÂèÇÊï∞ÂÖàÈ™å„ÄÇ2) Âà©Áî®MambaÊû∂ÊûÑÁöÑÂº∫Â§ßÂ∫èÂàóÂª∫Ê®°ËÉΩÂäõÔºåÂÖ®Â±ÄÈõÜÊàê‰ªªÂä°ÂÖàÈ™åÔºåÂπ∂Âª∫Ê®°‰ªªÂä°Èó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇ3) ÈááÁî®Â§öÊñπÂêëHilbertÊâ´ÊèèÔºåÂ¢ûÂº∫Â∫èÂàóÊ®°ÂûãÂØπ2DÊï∞ÊçÆÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåPAMMËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®‰ªªÂä°Èó¥ÁöÑ‰∫íË°•‰ø°ÊÅØÔºåÂπ∂ÊèêÂçáÂ§ö‰ªªÂä°ÂØÜÈõÜÈ¢ÑÊµãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂèåÁä∂ÊÄÅÁ©∫Èó¥ÂèÇÊï∞‰∏ìÂÆ∂‰ΩøÁî®‰∏§‰∏™Áã¨Á´ãÁöÑMambaÂùóÔºåÂàÜÂà´Â≠¶‰π†‰ªªÂä°ÁâπÂÆöÁöÑÂèÇÊï∞ÂÖàÈ™å„ÄÇS4Ê®°Âùó‰ΩøÁî®Ê†áÂáÜÁöÑS4Êû∂ÊûÑÔºåÁî®‰∫éÂÖ®Â±ÄÈõÜÊàê‰ªªÂä°ÂÖàÈ™å„ÄÇÂ§öÊñπÂêëHilbertÊâ´ÊèèÈááÁî®Âõõ‰∏™ÊñπÂêëÁöÑHilbertÊõ≤Á∫øÔºåÂ∞Ü2DÁâπÂæÅÂõæËΩ¨Êç¢‰∏∫Âõõ‰∏™Â∫èÂàó„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®Âä†ÊùÉÁöÑÂ§ö‰ªªÂä°ÊçüÂ§±ÂáΩÊï∞ÔºåÊùÉÈáçÊ†πÊçÆ‰ªªÂä°ÁöÑÈáçË¶ÅÊÄßËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®NYUD-v2ÂíåPASCAL-ContextÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPAMMÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÂ§ö‰ªªÂä°ÂØÜÈõÜÈ¢ÑÊµãÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®NYUD-v2Êï∞ÊçÆÈõÜ‰∏äÔºåPAMMÂú®Ê∑±Â∫¶È¢ÑÊµã„ÄÅË°®Èù¢Ê≥ïÁ∫øÈ¢ÑÊµãÂíåËØ≠‰πâÂàÜÂâ≤‰ªªÂä°‰∏äÂùáÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩ„ÄÇ‰∏é‰πãÂâçÁöÑÊúÄ‰Ω≥ÊñπÊ≥ïÁõ∏ÊØîÔºåPAMMÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PAMMÂú®Â§ö‰ªªÂä°ÂØÜÈõÜÈ¢ÑÊµãÊñπÈù¢ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂ÔºàÂêåÊó∂È¢ÑÊµãÊ∑±Â∫¶„ÄÅËØ≠‰πâÂàÜÂâ≤Âíå‰∫§ÈÄöÊ†áÂøóÔºâ„ÄÅÊú∫Âô®‰∫∫ÂØºËà™ÔºàÂêåÊó∂È¢ÑÊµãÁéØÂ¢ÉÂú∞ÂõæÂíåÁâ©‰ΩìÁ±ªÂà´ÔºâÂíåÂåªÂ≠¶ÂõæÂÉèÂàÜÊûêÔºàÂêåÊó∂È¢ÑÊµãÂô®ÂÆòÂàÜÂâ≤ÂíåÁóÖÁÅ∂Ê£ÄÊµãÔºâ„ÄÇËØ•Á†îÁ©∂ÁöÑÊàêÊûúÂèØ‰ª•ÊèêÂçáËøô‰∫õÂ∫îÁî®Âú∫ÊôØÁöÑÊÑüÁü•ËÉΩÂäõÂíåÂÜ≥Á≠ñËÉΩÂäõÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Understanding the inter-relations and interactions between tasks is crucial for multi-task dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, Parameter Aware Mamba Model (PAMM), specifically designed for dense prediction in multi-task learning setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state space models to enhance task interconnectivity. It features dual state space parameter experts that integrate and set task-specific parameter priors, capturing the intrinsic properties of each task. This approach not only facilitates precise multi-task interactions but also allows for the global integration of task priors through the structured state space sequence model (S4). Furthermore, we employ the Multi-Directional Hilbert Scanning method to construct multi-angle feature sequences, thereby enhancing the sequence model's perceptual capabilities for 2D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM.

