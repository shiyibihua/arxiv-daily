---
layout: default
title: iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion
---

# iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion

**arXiv**: [2511.14149v1](https://arxiv.org/abs/2511.14149) | [PDF](https://arxiv.org/pdf/2511.14149.pdf)

**ä½œè€…**: Hao Wang, Linqing Zhao, Xiuwei Xu, Jiwen Lu, Haibin Yan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

**å¤‡æ³¨**: IROS 2025

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/pythongod-exe/iGaussian)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºiGaussianä»¥è§£å†³å®žæ—¶ç›¸æœºä½å§¿ä¼°è®¡é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸ŽåŒ¹é… (Video Extraction & Matching)**

**å…³é”®è¯**: `ç›¸æœºä½å§¿ä¼°è®¡` `3Dé«˜æ–¯` `å®žæ—¶å¤„ç†` `æ·±åº¦å­¦ä¹ ` `è§†è§‰å¯¼èˆª` `SLAM` `ç‰¹å¾åŒ¹é…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽè¿­ä»£çš„æ¸²æŸ“-æ¯”è¾ƒ-ä¼˜åŒ–å¾ªçŽ¯ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ï¼Œéš¾ä»¥å®žçŽ°å®žæ—¶æ€§èƒ½ã€‚
2. æå‡ºçš„iGaussianæ¡†æž¶é€šè¿‡ç›´æŽ¥çš„3Dé«˜æ–¯åæ¼”å®žçŽ°ç›¸æœºä½å§¿çš„å¿«é€Ÿä¼°è®¡ï¼Œé‡‡ç”¨äº†ç©ºé—´å‡åŒ€é‡‡æ ·å’Œå¼•å¯¼æ³¨æ„æœºåˆ¶ã€‚
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒiGaussianæ˜¾è‘—é™ä½Žäº†æ—‹è½¬è¯¯å·®å¹¶æå‡äº†è·Ÿè¸ªé€Ÿåº¦ï¼Œå…·æœ‰è‰¯å¥½çš„å®žæ—¶æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒSLAMå’Œè§†è§‰å¯¼èˆªé¢†åŸŸé€æ¸é‡‡ç”¨3Dé«˜æ–¯ä½œä¸ºåœºæ™¯è¡¨ç¤ºï¼Œå¼ºè°ƒä»Žå•å¹…å›¾åƒä¸­ä¼°è®¡ç›¸æœºä½å§¿çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽè¿­ä»£çš„æ¸²æŸ“-æ¯”è¾ƒ-ä¼˜åŒ–å¾ªçŽ¯ï¼Œè¿™ä¸€è¿‡ç¨‹è®¡ç®—å¼€é”€å¤§ï¼Œéš¾ä»¥å®žçŽ°å®žæ—¶æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†iGaussianï¼Œä¸€ä¸ªä¸¤é˜¶æ®µçš„å‰é¦ˆæ¡†æž¶ï¼Œé€šè¿‡ç›´æŽ¥çš„3Dé«˜æ–¯åæ¼”å®žçŽ°å®žæ—¶ç›¸æœºä½å§¿ä¼°è®¡ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨åŸºäºŽé«˜æ–¯åœºæ™¯å…ˆéªŒçš„ä½å§¿å›žå½’ç½‘ç»œå›žå½’ç²—ç•¥çš„6DoFä½å§¿ï¼Œç„¶åŽé€šè¿‡ç‰¹å¾åŒ¹é…å’Œå¤šæ¨¡åž‹èžåˆè¿›è¡Œç²¾ç»†åŒ–ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒiGaussianåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œæ—‹è½¬è¯¯å·®ä¸­ä½æ•°é™è‡³0.2Â°ï¼Œåœ¨ç§»åŠ¨æœºå™¨äººä¸Šå®žçŽ°äº†2.87 FPSçš„è·Ÿè¸ªé€Ÿåº¦ï¼Œç›¸è¾ƒäºŽåŸºäºŽä¼˜åŒ–çš„æ–¹æ³•æå‡äº†10å€ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»Žå•å¹…å›¾åƒä¸­å®žæ—¶ä¼°è®¡ç›¸æœºä½å§¿çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ä¾èµ–äºŽè¿­ä»£çš„æ¸²æŸ“-æ¯”è¾ƒ-ä¼˜åŒ–æµç¨‹ï¼Œå¯¼è‡´è®¡ç®—æ•ˆçŽ‡ä½Žä¸‹ï¼Œæ— æ³•æ»¡è¶³å®žæ—¶åº”ç”¨çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šiGaussianæ¡†æž¶é€šè¿‡ç›´æŽ¥çš„3Dé«˜æ–¯åæ¼”æ¥å®žçŽ°ç›¸æœºä½å§¿çš„å¿«é€Ÿä¼°è®¡ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„å¤šè½®è¿­ä»£è¿‡ç¨‹ï¼Œä»Žè€Œæé«˜äº†è®¡ç®—æ•ˆçŽ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨é«˜æ–¯åœºæ™¯å…ˆéªŒçš„ä½å§¿å›žå½’ç½‘ç»œè¿›è¡Œç²—ç•¥ä½å§¿å›žå½’ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡ç‰¹å¾åŒ¹é…å’Œå¤šæ¨¡åž‹èžåˆè¿›è¡Œç²¾ç»†åŒ–ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç©ºé—´å‡åŒ€é‡‡æ ·ã€å¼•å¯¼æ³¨æ„æœºåˆ¶å’Œäº¤å‰ç›¸å…³æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºŽäº¤å‰ç›¸å…³æ¨¡å—ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ²¡æœ‰å¯å¾®åˆ†æ¸²æŸ“çš„æƒ…å†µä¸‹å¯¹å›¾åƒåµŒå…¥ä¸Ž3Dé«˜æ–¯å±žæ€§è¿›è¡Œå¯¹é½ï¼ŒåŒæ—¶å¼•å…¥åŠ æƒå¤šè§†å›¾é¢„æµ‹å™¨ï¼Œèžåˆæ¥è‡ªå¤šä¸ªæˆ˜ç•¥é‡‡æ ·è§†ç‚¹çš„ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æž„ä¸Šï¼Œé‡‡ç”¨äº†ç©ºé—´å‡åŒ€é‡‡æ ·å’Œå¼•å¯¼æ³¨æ„æœºåˆ¶æ¥æé«˜ä½å§¿å›žå½’çš„å‡†ç¡®æ€§ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šåˆ™æ³¨é‡äºŽå‡å°‘æ—‹è½¬è¯¯å·®ï¼Œç¡®ä¿æ¨¡åž‹åœ¨ä¸åŒè§†è§’ä¸‹çš„é²æ£’æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒiGaussianåœ¨NeRF Syntheticã€Mip-NeRF 360å’ŒT&T+DBæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œæ—‹è½¬è¯¯å·®ä¸­ä½æ•°é™è‡³0.2Â°ï¼Œåœ¨ç§»åŠ¨æœºå™¨äººä¸Šå®žçŽ°äº†2.87 FPSçš„è·Ÿè¸ªé€Ÿåº¦ï¼Œç›¸è¾ƒäºŽä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•æå‡äº†10å€ï¼Œå±•ç¤ºäº†å…¶åœ¨å®žæ—¶åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€å¢žå¼ºçŽ°å®žå’Œæ— äººé©¾é©¶ç­‰åœºæ™¯ã€‚é€šè¿‡å®žçŽ°å®žæ—¶çš„ç›¸æœºä½å§¿ä¼°è®¡ï¼ŒiGaussianèƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›é¢†åŸŸçš„ç³»ç»Ÿæ€§èƒ½å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰å¹¿æ³›çš„å®žé™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\&T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2Â° while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian

