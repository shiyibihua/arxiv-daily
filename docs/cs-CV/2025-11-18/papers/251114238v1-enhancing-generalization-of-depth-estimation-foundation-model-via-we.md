---
layout: default
title: Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization
---

# Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization

**arXiv**: [2511.14238v1](https://arxiv.org/abs/2511.14238) | [PDF](https://arxiv.org/pdf/2511.14238.pdf)

**ä½œè€…**: Yan Huang, Yongyi Su, Xin Lin, Le Zhang, Xun Xu

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

**å¤‡æ³¨**: Accepted by AAAI 2026

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWeSTARæ¡†æž¶ï¼Œé€šè¿‡å¼±ç›‘ç£è‡ªè®­ç»ƒå’Œæ­£åˆ™åŒ–æå‡æ·±åº¦ä¼°è®¡åŸºç¡€æ¨¡åž‹æ³›åŒ–èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å•ç›®æ·±åº¦ä¼°è®¡` `å¼±ç›‘ç£å­¦ä¹ ` `è‡ªè®­ç»ƒ` `é¢†åŸŸè‡ªé€‚åº”` `æ­£åˆ™åŒ–` `æ·±åº¦å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å•ç›®æ·±åº¦ä¼°è®¡åŸºç¡€æ¨¡åž‹åœ¨é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½†é’ˆå¯¹ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ä»æœ‰æå‡ç©ºé—´ã€‚
2. WeSTARæ¡†æž¶åˆ©ç”¨å¼±ç›‘ç£è‡ªè®­ç»ƒå’Œæ­£åˆ™åŒ–ï¼Œåœ¨ä¿è¯å‚æ•°æ•ˆçŽ‡çš„åŒæ—¶ï¼Œå¢žå¼ºæ¨¡åž‹åœ¨æœªè§é¢†åŸŸä¸­çš„é²æ£’æ€§ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒWeSTARåœ¨å„ç§æ•°æ®é›†ä¸Šå‡èƒ½æ˜¾è‘—æå‡æ·±åº¦ä¼°è®¡çš„æ³›åŒ–æ€§èƒ½ï¼Œè¾¾åˆ°å½“å‰æœ€ä½³æ°´å¹³ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ä¼°è®¡åŸºç¡€æ¨¡åž‹çš„å‡ºçŽ°æ˜¾è‘—æå‡äº†å•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼ŒDepth Anythingç³»åˆ—æ˜¯å…¶ä¸­çš„å…¸åž‹ä»£è¡¨ã€‚ç„¶è€Œï¼Œå¦‚æžœèƒ½å¤ŸèŽ·å–ä¸‹æ¸¸ä»»åŠ¡çš„ä¸€äº›æ•°æ®ï¼Œä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜æ˜¯ï¼šè¿™äº›æ¨¡åž‹çš„æ€§èƒ½èƒ½å¦è¿›ä¸€æ­¥æé«˜ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†WeSTARï¼Œä¸€ä¸ªå‚æ•°é«˜æ•ˆçš„æ¡†æž¶ï¼Œå®ƒæ‰§è¡Œå¸¦æ­£åˆ™åŒ–çš„å¼±ç›‘ç£è‡ªè®­ç»ƒé€‚åº”ï¼Œæ—¨åœ¨å¢žå¼ºMDEåŸºç¡€æ¨¡åž‹åœ¨æœªè§è¿‡çš„å¤šæ ·åŒ–é¢†åŸŸä¸­çš„é²æ£’æ€§ã€‚æˆ‘ä»¬é¦–å…ˆé‡‡ç”¨å¯†é›†è‡ªè®­ç»ƒç›®æ ‡ä½œä¸ºç»“æž„è‡ªç›‘ç£çš„ä¸»è¦æ¥æºã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é²æ£’æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥çš„åˆ†å±‚å½’ä¸€åŒ–ï¼Œå®ƒåˆ©ç”¨å®žä¾‹çº§åˆ†å‰²å›¾æ¥æ‰§è¡Œæ›´ç¨³å®šå’Œå¤šå°ºåº¦çš„ç»“æž„å½’ä¸€åŒ–ã€‚é™¤äº†å¯†é›†ç›‘ç£ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ç»æµŽé«˜æ•ˆçš„å¼±ç›‘ç£å½¢å¼ï¼Œå³æˆå¯¹åºæ•°æ·±åº¦æ ‡æ³¨ï¼Œä»¥è¿›ä¸€æ­¥æŒ‡å¯¼é€‚åº”è¿‡ç¨‹ï¼Œè¿™å¼ºåˆ¶æ‰§è¡Œä¿¡æ¯ä¸°å¯Œçš„åºæ•°çº¦æŸï¼Œä»¥å‡è½»å±€éƒ¨æ‹“æ‰‘é”™è¯¯ã€‚æœ€åŽï¼Œé‡‡ç”¨æƒé‡æ­£åˆ™åŒ–æŸå¤±æ¥é”šå®šLoRAæ›´æ–°ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§å¹¶ä¿ç•™æ¨¡åž‹çš„å¯æ³›åŒ–çŸ¥è¯†ã€‚åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸‹ï¼Œå¯¹çœŸå®žå’ŒæŸåçš„åˆ†å¸ƒå¤–æ•°æ®é›†è¿›è¡Œçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒWeSTARå§‹ç»ˆå¦‚ä¸€åœ°æé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„æ·±åº¦ä¼°è®¡åŸºç¡€æ¨¡åž‹è™½ç„¶å…·å¤‡ä¸€å®šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹åˆ†å¸ƒå¤–æ•°æ®æ—¶ï¼Œæ€§èƒ½ä»æœ‰å¾…æé«˜ã€‚ç›´æŽ¥åœ¨ç›®æ ‡åŸŸä¸Šè¿›è¡Œå¾®è°ƒå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨æœ‰é™çš„æ•°æ®å’Œè®¡ç®—èµ„æºä¸‹ï¼Œæå‡æ¨¡åž‹åœ¨æœªè§é¢†åŸŸä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šWeSTARçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼±ç›‘ç£è‡ªè®­ç»ƒå’Œæ­£åˆ™åŒ–ç­–ç•¥ï¼Œåœ¨ä¿ç•™åŸºç¡€æ¨¡åž‹é€šç”¨çŸ¥è¯†çš„åŒæ—¶ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç›®æ ‡åŸŸçš„æ•°æ®åˆ†å¸ƒã€‚é€šè¿‡è‡ªè®­ç»ƒç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œæä¾›å¯†é›†çš„ç»“æž„è‡ªç›‘ç£ï¼›åˆ©ç”¨å¼±ç›‘ç£çš„åºæ•°æ·±åº¦ä¿¡æ¯ï¼Œçº æ­£å±€éƒ¨æ‹“æ‰‘é”™è¯¯ï¼›å¹¶é‡‡ç”¨æƒé‡æ­£åˆ™åŒ–ï¼Œé˜²æ­¢æ¨¡åž‹è¿‡åº¦åç¦»åŽŸå§‹çŠ¶æ€ï¼Œä»Žè€Œå®žçŽ°æ›´ç¨³å®šçš„é€‚åº”è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šWeSTARæ¡†æž¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š1) å¯†é›†è‡ªè®­ç»ƒæ¨¡å—ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡åž‹ç”Ÿæˆä¼ªæ·±åº¦å›¾ï¼Œä½œä¸ºè‡ªç›‘ç£ä¿¡å·ï¼›2) è¯­ä¹‰æ„ŸçŸ¥çš„åˆ†å±‚å½’ä¸€åŒ–æ¨¡å—ï¼Œåˆ©ç”¨å®žä¾‹åˆ†å‰²ä¿¡æ¯è¿›è¡Œå¤šå°ºåº¦å½’ä¸€åŒ–ï¼Œæé«˜é²æ£’æ€§ï¼›3) å¼±ç›‘ç£åºæ•°æ·±åº¦çº¦æŸæ¨¡å—ï¼Œåˆ©ç”¨æˆå¯¹çš„æ·±åº¦å…³ç³»æ ‡æ³¨ï¼Œçº æ­£å±€éƒ¨æ‹“æ‰‘é”™è¯¯ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æƒé‡æ­£åˆ™åŒ–æŸå¤±ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆè¿›è¡Œè‡ªè®­ç»ƒï¼Œç„¶åŽç»“åˆå¼±ç›‘ç£ä¿¡æ¯è¿›è¡Œå¾®è°ƒï¼Œæœ€åŽé€šè¿‡æ­£åˆ™åŒ–çº¦æŸæ¨¡åž‹å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šWeSTARçš„å…³é”®åˆ›æ–°åœ¨äºŽç»“åˆäº†å¯†é›†è‡ªè®­ç»ƒã€è¯­ä¹‰æ„ŸçŸ¥çš„åˆ†å±‚å½’ä¸€åŒ–å’Œå¼±ç›‘ç£åºæ•°æ·±åº¦çº¦æŸï¼Œå½¢æˆäº†ä¸€ç§äº’è¡¥çš„ç›‘ç£ä¿¡å·ã€‚ä¸Žä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒWeSTARæ›´åŠ æ³¨é‡åˆ©ç”¨æ¨¡åž‹è‡ªèº«çš„çŸ¥è¯†å’Œæ•°æ®ä¸­çš„ç»“æž„ä¿¡æ¯ï¼Œä»Žè€Œåœ¨æœ‰é™çš„ç›‘ç£ä¸‹å®žçŽ°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå‚æ•°é«˜æ•ˆçš„LoRAæ›´æ–°æ–¹å¼ä¹Ÿé™ä½Žäº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯­ä¹‰æ„ŸçŸ¥çš„åˆ†å±‚å½’ä¸€åŒ–åˆ©ç”¨å®žä¾‹åˆ†å‰²å›¾å°†å›¾åƒåˆ’åˆ†ä¸ºä¸åŒçš„è¯­ä¹‰åŒºåŸŸï¼Œç„¶åŽå¯¹æ¯ä¸ªåŒºåŸŸè¿›è¡Œç‹¬ç«‹çš„å½’ä¸€åŒ–ï¼Œä»Žè€Œæ›´å¥½åœ°é€‚åº”ä¸åŒåŒºåŸŸçš„æ·±åº¦åˆ†å¸ƒã€‚å¼±ç›‘ç£åºæ•°æ·±åº¦çº¦æŸé€šè¿‡æ¯”è¾ƒå›¾åƒä¸­ä¸¤ä¸ªåƒç´ çš„æ·±åº¦å…³ç³»ï¼Œæž„å»ºæŸå¤±å‡½æ•°ï¼Œå¼•å¯¼æ¨¡åž‹å­¦ä¹ æ­£ç¡®çš„æ·±åº¦é¡ºåºã€‚æƒé‡æ­£åˆ™åŒ–æŸå¤±é‡‡ç”¨L2æ­£åˆ™åŒ–ï¼Œçº¦æŸLoRAæ›´æ–°çš„å¹…åº¦ï¼Œé˜²æ­¢æ¨¡åž‹å‚æ•°å‘ç”Ÿå‰§çƒˆå˜åŒ–ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

WeSTARåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨çœŸå®žæ•°æ®é›†å’ŒæŸåçš„åˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šï¼ŒWeSTARå‡ä¼˜äºŽçŽ°æœ‰çš„æ·±åº¦ä¼°è®¡æ–¹æ³•ï¼Œå¹¶åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒWeSTARèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æ·±åº¦ä¼°è®¡æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

WeSTARæ¡†æž¶å¯åº”ç”¨äºŽå„ç§éœ€è¦å•ç›®æ·±åº¦ä¼°è®¡çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢žå¼ºçŽ°å®žç­‰ã€‚é€šè¿‡æå‡æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœ¨å¤æ‚çŽ¯å¢ƒä¸‹çš„æ€§èƒ½å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å‚æ•°é«˜æ•ˆçš„ç‰¹ç‚¹ä½¿å…¶æ›´æ˜“äºŽéƒ¨ç½²åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.

