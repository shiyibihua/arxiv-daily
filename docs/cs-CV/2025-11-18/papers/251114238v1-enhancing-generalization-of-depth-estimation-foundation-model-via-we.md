---
layout: default
title: Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization
---

# Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization

**arXiv**: [2511.14238v1](https://arxiv.org/abs/2511.14238) | [PDF](https://arxiv.org/pdf/2511.14238.pdf)

**ä½œè€…**: Yan Huang, Yongyi Su, Xin Lin, Le Zhang, Xun Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWeSTARæ¡†æž¶ï¼Œé€šè¿‡å¼±ç›‘ç£è‡ªé€‚åº”å¢žå¼ºæ·±åº¦ä¼°è®¡åŸºç¡€æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›**

**å…³é”®è¯**: `å•ç›®æ·±åº¦ä¼°è®¡` `åŸºç¡€æ¨¡åž‹é€‚åº”` `å¼±ç›‘ç£å­¦ä¹ ` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `æ³›åŒ–å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŸºç¡€æ¨¡åž‹åœ¨é›¶æ ·æœ¬æ·±åº¦ä¼°è®¡ä¸­æ³›åŒ–ä¸è¶³ï¼Œéœ€åˆ©ç”¨ä¸‹æ¸¸æ•°æ®æå‡æ€§èƒ½
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè‡ªè®­ç»ƒã€è¯­ä¹‰æ„ŸçŸ¥å½’ä¸€åŒ–ã€å¼±ç›‘ç£å’Œæƒé‡æ­£åˆ™åŒ–è¿›è¡Œå‚æ•°é«˜æ•ˆé€‚åº”
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šæ ·åˆ†å¸ƒå¤–æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œæå‡æ³›åŒ–å¹¶è¾¾åˆ°å…ˆè¿›æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.

