---
layout: default
title: Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization
---

# Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.14238" target="_blank" class="toolbar-btn">arXiv: 2511.14238v1</a>
    <a href="https://arxiv.org/pdf/2511.14238.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14238v1" 
            onclick="toggleFavorite(this, '2511.14238v1', 'Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yan Huang, Yongyi Su, Xin Lin, Le Zhang, Xun Xu

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-18

**Â§áÊ≥®**: Accepted by AAAI 2026

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫WeSTARÊ°ÜÊû∂ÔºåÈÄöËøáÂº±ÁõëÁù£Ëá™ËÆ≠ÁªÉÂíåÊ≠£ÂàôÂåñÊèêÂçáÊ∑±Â∫¶‰º∞ËÆ°Âü∫Á°ÄÊ®°ÂûãÊ≥õÂåñËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°` `Âº±ÁõëÁù£Â≠¶‰π†` `Ëá™ËÆ≠ÁªÉ` `È¢ÜÂüüËá™ÈÄÇÂ∫î` `Ê≠£ÂàôÂåñ` `Ê∑±Â∫¶Â≠¶‰π†` `Ê≥õÂåñËÉΩÂäõ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Âü∫Á°ÄÊ®°ÂûãÂú®Èõ∂Ê†∑Êú¨Ê≥õÂåñÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÈíàÂØπÁâπÂÆö‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊÄßËÉΩ‰ªçÊúâÊèêÂçáÁ©∫Èó¥„ÄÇ
2. WeSTARÊ°ÜÊû∂Âà©Áî®Âº±ÁõëÁù£Ëá™ËÆ≠ÁªÉÂíåÊ≠£ÂàôÂåñÔºåÂú®‰øùËØÅÂèÇÊï∞ÊïàÁéáÁöÑÂêåÊó∂ÔºåÂ¢ûÂº∫Ê®°ÂûãÂú®Êú™ËßÅÈ¢ÜÂüü‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåWeSTARÂú®ÂêÑÁßçÊï∞ÊçÆÈõÜ‰∏äÂùáËÉΩÊòæËëóÊèêÂçáÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÊ≥õÂåñÊÄßËÉΩÔºåËææÂà∞ÂΩìÂâçÊúÄ‰Ω≥Ê∞¥Âπ≥„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑±Â∫¶‰º∞ËÆ°Âü∫Á°ÄÊ®°ÂûãÁöÑÂá∫Áé∞ÊòæËëóÊèêÂçá‰∫ÜÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÔºàMDEÔºâÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõÔºåDepth AnythingÁ≥ªÂàóÊòØÂÖ∂‰∏≠ÁöÑÂÖ∏Âûã‰ª£Ë°®„ÄÇÁÑ∂ËÄåÔºåÂ¶ÇÊûúËÉΩÂ§üËé∑Âèñ‰∏ãÊ∏∏‰ªªÂä°ÁöÑ‰∏Ä‰∫õÊï∞ÊçÆÔºå‰∏Ä‰∏™Ëá™ÁÑ∂ÁöÑÈóÆÈ¢òÊòØÔºöËøô‰∫õÊ®°ÂûãÁöÑÊÄßËÉΩËÉΩÂê¶Ëøõ‰∏ÄÊ≠•ÊèêÈ´òÔºü‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜWeSTARÔºå‰∏Ä‰∏™ÂèÇÊï∞È´òÊïàÁöÑÊ°ÜÊû∂ÔºåÂÆÉÊâßË°åÂ∏¶Ê≠£ÂàôÂåñÁöÑÂº±ÁõëÁù£Ëá™ËÆ≠ÁªÉÈÄÇÂ∫îÔºåÊó®Âú®Â¢ûÂº∫MDEÂü∫Á°ÄÊ®°ÂûãÂú®Êú™ËßÅËøáÁöÑÂ§öÊ†∑ÂåñÈ¢ÜÂüü‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊàë‰ª¨È¶ñÂÖàÈááÁî®ÂØÜÈõÜËá™ËÆ≠ÁªÉÁõÆÊ†á‰Ωú‰∏∫ÁªìÊûÑËá™ÁõëÁù£ÁöÑ‰∏ªË¶ÅÊù•Ê∫ê„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÊèêÈ´òÈ≤ÅÊ£íÊÄßÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜËØ≠‰πâÊÑüÁü•ÁöÑÂàÜÂ±ÇÂΩí‰∏ÄÂåñÔºåÂÆÉÂà©Áî®ÂÆû‰æãÁ∫ßÂàÜÂâ≤ÂõæÊù•ÊâßË°åÊõ¥Á®≥ÂÆöÂíåÂ§öÂ∞∫Â∫¶ÁöÑÁªìÊûÑÂΩí‰∏ÄÂåñ„ÄÇÈô§‰∫ÜÂØÜÈõÜÁõëÁù£‰πãÂ§ñÔºåÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁªèÊµéÈ´òÊïàÁöÑÂº±ÁõëÁù£ÂΩ¢ÂºèÔºåÂç≥ÊàêÂØπÂ∫èÊï∞Ê∑±Â∫¶Ê†áÊ≥®Ôºå‰ª•Ëøõ‰∏ÄÊ≠•ÊåáÂØºÈÄÇÂ∫îËøáÁ®ãÔºåËøôÂº∫Âà∂ÊâßË°å‰ø°ÊÅØ‰∏∞ÂØåÁöÑÂ∫èÊï∞Á∫¶ÊùüÔºå‰ª•ÂáèËΩªÂ±ÄÈÉ®ÊãìÊâëÈîôËØØ„ÄÇÊúÄÂêéÔºåÈááÁî®ÊùÉÈáçÊ≠£ÂàôÂåñÊçüÂ§±Êù•ÈîöÂÆöLoRAÊõ¥Êñ∞ÔºåÁ°Æ‰øùËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂπ∂‰øùÁïôÊ®°ÂûãÁöÑÂèØÊ≥õÂåñÁü•ËØÜ„ÄÇÂú®ÂêÑÁßçÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂú∫ÊôØ‰∏ãÔºåÂØπÁúüÂÆûÂíåÊçüÂùèÁöÑÂàÜÂ∏ÉÂ§ñÊï∞ÊçÆÈõÜËøõË°åÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåWeSTARÂßãÁªàÂ¶Ç‰∏ÄÂú∞ÊèêÈ´ò‰∫ÜÊ≥õÂåñËÉΩÂäõÔºåÂπ∂Âú®ÂêÑÁßçÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÊ∑±Â∫¶‰º∞ËÆ°Âü∫Á°ÄÊ®°ÂûãËôΩÁÑ∂ÂÖ∑Â§á‰∏ÄÂÆöÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõÔºå‰ΩÜÂú®ÁâπÂÆö‰∏ãÊ∏∏‰ªªÂä°‰∏äÔºåÂ∞§ÂÖ∂ÊòØÂú®Èù¢ÂØπÂàÜÂ∏ÉÂ§ñÊï∞ÊçÆÊó∂ÔºåÊÄßËÉΩ‰ªçÊúâÂæÖÊèêÈ´ò„ÄÇÁõ¥Êé•Âú®ÁõÆÊ†áÂüü‰∏äËøõË°åÂæÆË∞ÉÂÆπÊòìËøáÊãüÂêàÔºå‰∏îËÆ°ÁÆóÊàêÊú¨ËæÉÈ´ò„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂú®ÊúâÈôêÁöÑÊï∞ÊçÆÂíåËÆ°ÁÆóËµÑÊ∫ê‰∏ãÔºåÊèêÂçáÊ®°ÂûãÂú®Êú™ËßÅÈ¢ÜÂüü‰∏≠ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöWeSTARÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Âº±ÁõëÁù£Ëá™ËÆ≠ÁªÉÂíåÊ≠£ÂàôÂåñÁ≠ñÁï•ÔºåÂú®‰øùÁïôÂü∫Á°ÄÊ®°ÂûãÈÄöÁî®Áü•ËØÜÁöÑÂêåÊó∂Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÁõÆÊ†áÂüüÁöÑÊï∞ÊçÆÂàÜÂ∏É„ÄÇÈÄöËøáËá™ËÆ≠ÁªÉÁîüÊàê‰º™Ê†áÁ≠æÔºåÊèê‰æõÂØÜÈõÜÁöÑÁªìÊûÑËá™ÁõëÁù£ÔºõÂà©Áî®Âº±ÁõëÁù£ÁöÑÂ∫èÊï∞Ê∑±Â∫¶‰ø°ÊÅØÔºåÁ∫†Ê≠£Â±ÄÈÉ®ÊãìÊâëÈîôËØØÔºõÂπ∂ÈááÁî®ÊùÉÈáçÊ≠£ÂàôÂåñÔºåÈò≤Ê≠¢Ê®°ÂûãËøáÂ∫¶ÂÅèÁ¶ªÂéüÂßãÁä∂ÊÄÅÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Á®≥ÂÆöÁöÑÈÄÇÂ∫îËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöWeSTARÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºö1) ÂØÜÈõÜËá™ËÆ≠ÁªÉÊ®°ÂùóÔºåÂà©Áî®Âü∫Á°ÄÊ®°ÂûãÁîüÊàê‰º™Ê∑±Â∫¶ÂõæÔºå‰Ωú‰∏∫Ëá™ÁõëÁù£‰ø°Âè∑Ôºõ2) ËØ≠‰πâÊÑüÁü•ÁöÑÂàÜÂ±ÇÂΩí‰∏ÄÂåñÊ®°ÂùóÔºåÂà©Áî®ÂÆû‰æãÂàÜÂâ≤‰ø°ÊÅØËøõË°åÂ§öÂ∞∫Â∫¶ÂΩí‰∏ÄÂåñÔºåÊèêÈ´òÈ≤ÅÊ£íÊÄßÔºõ3) Âº±ÁõëÁù£Â∫èÊï∞Ê∑±Â∫¶Á∫¶ÊùüÊ®°ÂùóÔºåÂà©Áî®ÊàêÂØπÁöÑÊ∑±Â∫¶ÂÖ≥Á≥ªÊ†áÊ≥®ÔºåÁ∫†Ê≠£Â±ÄÈÉ®ÊãìÊâëÈîôËØØ„ÄÇÊ≠§Â§ñÔºåËøòÂºïÂÖ•‰∫ÜÊùÉÈáçÊ≠£ÂàôÂåñÊçüÂ§±Ôºå‰ª•Á®≥ÂÆöËÆ≠ÁªÉËøáÁ®ã„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÂÖàËøõË°åËá™ËÆ≠ÁªÉÔºåÁÑ∂ÂêéÁªìÂêàÂº±ÁõëÁù£‰ø°ÊÅØËøõË°åÂæÆË∞ÉÔºåÊúÄÂêéÈÄöËøáÊ≠£ÂàôÂåñÁ∫¶ÊùüÊ®°ÂûãÂèÇÊï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöWeSTARÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫ÜÂØÜÈõÜËá™ËÆ≠ÁªÉ„ÄÅËØ≠‰πâÊÑüÁü•ÁöÑÂàÜÂ±ÇÂΩí‰∏ÄÂåñÂíåÂº±ÁõëÁù£Â∫èÊï∞Ê∑±Â∫¶Á∫¶ÊùüÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁßç‰∫íË°•ÁöÑÁõëÁù£‰ø°Âè∑„ÄÇ‰∏é‰º†ÁªüÁöÑÂæÆË∞ÉÊñπÊ≥ïÁõ∏ÊØîÔºåWeSTARÊõ¥Âä†Ê≥®ÈáçÂà©Áî®Ê®°ÂûãËá™Ë∫´ÁöÑÁü•ËØÜÂíåÊï∞ÊçÆ‰∏≠ÁöÑÁªìÊûÑ‰ø°ÊÅØÔºå‰ªéËÄåÂú®ÊúâÈôêÁöÑÁõëÁù£‰∏ãÂÆûÁé∞Êõ¥Â•ΩÁöÑÊ≥õÂåñÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂèÇÊï∞È´òÊïàÁöÑLoRAÊõ¥Êñ∞ÊñπÂºè‰πüÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËØ≠‰πâÊÑüÁü•ÁöÑÂàÜÂ±ÇÂΩí‰∏ÄÂåñÂà©Áî®ÂÆû‰æãÂàÜÂâ≤ÂõæÂ∞ÜÂõæÂÉèÂàíÂàÜ‰∏∫‰∏çÂêåÁöÑËØ≠‰πâÂå∫ÂüüÔºåÁÑ∂ÂêéÂØπÊØè‰∏™Âå∫ÂüüËøõË°åÁã¨Á´ãÁöÑÂΩí‰∏ÄÂåñÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏çÂêåÂå∫ÂüüÁöÑÊ∑±Â∫¶ÂàÜÂ∏É„ÄÇÂº±ÁõëÁù£Â∫èÊï∞Ê∑±Â∫¶Á∫¶ÊùüÈÄöËøáÊØîËæÉÂõæÂÉè‰∏≠‰∏§‰∏™ÂÉèÁ¥†ÁöÑÊ∑±Â∫¶ÂÖ≥Á≥ªÔºåÊûÑÂª∫ÊçüÂ§±ÂáΩÊï∞ÔºåÂºïÂØºÊ®°ÂûãÂ≠¶‰π†Ê≠£Á°ÆÁöÑÊ∑±Â∫¶È°∫Â∫è„ÄÇÊùÉÈáçÊ≠£ÂàôÂåñÊçüÂ§±ÈááÁî®L2Ê≠£ÂàôÂåñÔºåÁ∫¶ÊùüLoRAÊõ¥Êñ∞ÁöÑÂπÖÂ∫¶ÔºåÈò≤Ê≠¢Ê®°ÂûãÂèÇÊï∞ÂèëÁîüÂâßÁÉàÂèòÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

WeSTARÂú®Â§ö‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ÁúüÂÆûÊï∞ÊçÆÈõÜÂíåÊçüÂùèÁöÑÂàÜÂ∏ÉÂ§ñÊï∞ÊçÆÈõÜ‰∏äÔºåWeSTARÂùá‰ºò‰∫éÁé∞ÊúâÁöÑÊ∑±Â∫¶‰º∞ËÆ°ÊñπÊ≥ïÔºåÂπ∂Âú®ÂêÑÁßçÂü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWeSTARËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òÊ∑±Â∫¶‰º∞ËÆ°Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

WeSTARÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠â„ÄÇÈÄöËøáÊèêÂçáÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåÂèØ‰ª•ÊèêÈ´òËøô‰∫õÂ∫îÁî®Âú®Â§çÊùÇÁéØÂ¢É‰∏ãÁöÑÊÄßËÉΩÂíåÂèØÈù†ÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂèÇÊï∞È´òÊïàÁöÑÁâπÁÇπ‰ΩøÂÖ∂Êõ¥Êòì‰∫éÈÉ®ÁΩ≤Âú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.

