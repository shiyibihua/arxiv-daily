---
layout: default
title: StreamingTalker: Audio-driven 3D Facial Animation with Autoregressive Diffusion Model
---

# StreamingTalker: Audio-driven 3D Facial Animation with Autoregressive Diffusion Model

**arXiv**: [2511.14223v1](https://arxiv.org/abs/2511.14223) | [PDF](https://arxiv.org/pdf/2511.14223.pdf)

**ä½œè€…**: Yifan Yang, Zhi Cen, Sida Peng, Xiangwei Chen, Yifu Deng, Xinyu Zhu, Fan Jia, Xiaowei Zhou, Hujun Bao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªå›žå½’æ‰©æ•£æ¨¡åž‹ä»¥è§£å†³è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»çš„é•¿åºåˆ—å¤„ç†å»¶è¿Ÿé—®é¢˜**

**å…³é”®è¯**: `è¯­éŸ³é©±åŠ¨åŠ¨ç”»` `è‡ªå›žå½’æ‰©æ•£æ¨¡åž‹` `3Dé¢éƒ¨åŠ¨ç”»` `æµå¼å¤„ç†` `å®žæ—¶åˆæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¤„ç†é•¿éŸ³é¢‘åºåˆ—æ—¶æ€§èƒ½ä¸‹é™ä¸”å»¶è¿Ÿé«˜
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æµå¼å¤„ç†ç»“åˆåŽ†å²è¿åŠ¨ä¸Šä¸‹æ–‡ï¼Œè¿­ä»£ç”Ÿæˆé¢éƒ¨è¿åŠ¨å¸§
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žçŽ°ä½Žå»¶è¿Ÿå®žæ—¶åˆæˆï¼Œå¹¶å‘å¸ƒäº¤äº’æ¼”ç¤ºéªŒè¯æœ‰æ•ˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper focuses on the task of speech-driven 3D facial animation, which aims to generate realistic and synchronized facial motions driven by speech inputs.Recent methods have employed audio-conditioned diffusion models for 3D facial animation, achieving impressive results in generating expressive and natural animations.However, these methods process the whole audio sequences in a single pass, which poses two major challenges: they tend to perform poorly when handling audio sequences that exceed the training horizon and will suffer from significant latency when processing long audio inputs. To address these limitations, we propose a novel autoregressive diffusion model that processes input audio in a streaming manner. This design ensures flexibility with varying audio lengths and achieves low latency independent of audio duration. Specifically, we select a limited number of past frames as historical motion context and combine them with the audio input to create a dynamic condition. This condition guides the diffusion process to iteratively generate facial motion frames, enabling real-time synthesis with high-quality results. Additionally, we implemented a real-time interactive demo, highlighting the effectiveness and efficiency of our approach. We will release the code at https://zju3dv.github.io/StreamingTalker/.

