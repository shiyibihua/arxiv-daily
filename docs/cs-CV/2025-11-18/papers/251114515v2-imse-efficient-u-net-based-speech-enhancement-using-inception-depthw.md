---
layout: default
title: IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention
---

# IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention

**arXiv**: [2511.14515v2](https://arxiv.org/abs/2511.14515) | [PDF](https://arxiv.org/pdf/2511.14515.pdf)

**ä½œè€…**: Xinxin Tang, Bin Qin, Yufang Li

**åˆ†ç±»**: cs.SD, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18 (æ›´æ–°: 2025-12-01)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**IMSEï¼šåˆ©ç”¨Inceptionæ·±åº¦å¯åˆ†ç¦»å·ç§¯å’Œå¹…åº¦æ„ŸçŸ¥çº¿æ€§æ³¨æ„åŠ›çš„é«˜æ•ˆU-Netè¯­éŸ³å¢žå¼º**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è¯­éŸ³å¢žå¼º` `è½»é‡åŒ–æ¨¡åž‹` `U-Net` `çº¿æ€§æ³¨æ„åŠ›` `æ·±åº¦å¯åˆ†ç¦»å·ç§¯` `å¹…åº¦æ„ŸçŸ¥` `èµ„æºå—é™è®¾å¤‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è¯­éŸ³å¢žå¼ºæ–¹æ³•åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéš¾ä»¥å…¼é¡¾è½»é‡åŒ–å’Œé«˜æ€§èƒ½ï¼Œå­˜åœ¨æ•ˆçŽ‡ç“¶é¢ˆã€‚
2. IMSEé€šè¿‡å¹…åº¦æ„ŸçŸ¥çº¿æ€§æ³¨æ„åŠ›å’ŒInceptionæ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼Œå®žçŽ°é«˜æ•ˆå…¨å±€å»ºæ¨¡å’Œç‰¹å¾æå–ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒIMSEåœ¨å‚æ•°é‡æ˜¾è‘—é™ä½Žçš„åŒæ—¶ï¼Œä¿æŒäº†ä¸ŽçŽ°æœ‰æœ€ä½³æ–¹æ³•ç›¸å½“çš„è¯­éŸ³å¢žå¼ºæ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºIMSEï¼Œä¸€ä¸ªç³»ç»Ÿä¼˜åŒ–ä¸”è¶…è½»é‡çº§çš„è¯­éŸ³å¢žå¼ºç½‘ç»œï¼Œæ—¨åœ¨å¹³è¡¡è½»é‡åŒ–è®¾è®¡å’Œé«˜æ€§èƒ½ã€‚çŽ°æœ‰æ–¹æ³•å¦‚MUSEè™½ç„¶å‚æ•°é‡ä»…ä¸º0.51Mï¼Œä½†æ•ˆçŽ‡ä»æœ‰ç“¶é¢ˆã€‚MUSEä¸­çš„METæ¨¡å—ä¾èµ–å¤æ‚çš„â€œè¿‘ä¼¼-è¡¥å¿â€æœºåˆ¶æ¥ç¼“è§£æ³°å‹’å±•å¼€æ³¨æ„åŠ›çš„å±€é™æ€§ï¼Œè€Œå¯å˜å½¢åµŒå…¥çš„åç§»è®¡ç®—å¼•å…¥äº†é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚IMSEå¼•å…¥äº†ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼š1) ä½¿ç”¨å¹…åº¦æ„ŸçŸ¥çº¿æ€§æ³¨æ„åŠ›ï¼ˆMALAï¼‰æ›¿æ¢METæ¨¡å—ï¼Œé€šè¿‡æ˜¾å¼ä¿ç•™æŸ¥è¯¢å‘é‡çš„èŒƒæ•°ä¿¡æ¯ï¼Œä»Žæ ¹æœ¬ä¸Šçº æ­£çº¿æ€§æ³¨æ„åŠ›ä¸­â€œå¿½ç•¥å¹…åº¦â€çš„é—®é¢˜ï¼Œå®žçŽ°é«˜æ•ˆçš„å…¨å±€å»ºæ¨¡ã€‚2) ä½¿ç”¨Inceptionæ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆIDConvï¼‰æ›¿æ¢DEæ¨¡å—ï¼Œå°†å¤§æ ¸æ“ä½œåˆ†è§£ä¸ºé«˜æ•ˆçš„å¹¶è¡Œåˆ†æ”¯ï¼ˆæ­£æ–¹å½¢ã€æ°´å¹³å’Œåž‚ç›´æ¡ï¼‰ï¼Œä»¥æžä½Žçš„å‚æ•°å†—ä½™æ•èŽ·é¢‘è°±å›¾ç‰¹å¾ã€‚åœ¨VoiceBank+DEMANDæ•°æ®é›†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œä¸ŽMUSEç›¸æ¯”ï¼ŒIMSEåœ¨å‚æ•°é‡å‡å°‘16.8%ï¼ˆä»Ž0.513Måˆ°0.427Mï¼‰çš„åŒæ—¶ï¼Œåœ¨PESQæŒ‡æ ‡ä¸Šå®žçŽ°äº†ä¸Žæœ€å…ˆè¿›æ°´å¹³ç›¸å½“çš„æ€§èƒ½ï¼ˆ3.373ï¼‰ã€‚è¿™é¡¹ç ”ç©¶ä¸ºè¶…è½»é‡çº§è¯­éŸ³å¢žå¼ºä¸­æ¨¡åž‹å¤§å°å’Œè¯­éŸ³è´¨é‡ä¹‹é—´çš„æƒè¡¡è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¯­éŸ³å¢žå¼ºä»»åŠ¡ä¸­ï¼Œå¦‚ä½•åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå®žçŽ°é«˜æ€§èƒ½å’Œä½Žè®¡ç®—å¤æ‚åº¦çš„å¹³è¡¡é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œå¦‚MUSEï¼Œè™½ç„¶å‚æ•°é‡è¾ƒå°ï¼Œä½†å…¶METæ¨¡å—å’ŒDEæ¨¡å—ä»ç„¶å­˜åœ¨æ•ˆçŽ‡ç“¶é¢ˆï¼Œä¾‹å¦‚METæ¨¡å—éœ€è¦å¤æ‚çš„è¿‘ä¼¼è¡¥å¿æœºåˆ¶ï¼ŒDEæ¨¡å—éœ€è¦é¢å¤–çš„åç§»è®¡ç®—ï¼Œå¯¼è‡´è®¡ç®—è´Ÿæ‹…å¢žåŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ”¹è¿›æ³¨æ„åŠ›æœºåˆ¶å’Œå·ç§¯æ“ä½œï¼Œåœ¨ä¸æŸå¤±æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½Žæ¨¡åž‹çš„å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨å¹…åº¦æ„ŸçŸ¥çº¿æ€§æ³¨æ„åŠ›ï¼ˆMALAï¼‰æ›¿ä»£å¤æ‚çš„METæ¨¡å—ï¼Œè§£å†³çº¿æ€§æ³¨æ„åŠ›å¿½ç•¥å¹…åº¦ä¿¡æ¯çš„é—®é¢˜ï¼›ä½¿ç”¨Inceptionæ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆIDConvï¼‰æ›¿ä»£DEæ¨¡å—ï¼Œä»¥æ›´é«˜æ•ˆçš„æ–¹å¼æå–é¢‘è°±å›¾ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šIMSEé‡‡ç”¨U-Netç»“æž„ä½œä¸ºæ•´ä½“æ¡†æž¶ï¼Œç¼–ç å™¨æå–è¾“å…¥å™ªå£°è¯­éŸ³çš„ç‰¹å¾ï¼Œè§£ç å™¨é‡å»ºå¢žå¼ºåŽçš„è¯­éŸ³ã€‚MALAæ¨¡å—è¢«é›†æˆåˆ°U-Netçš„ç“¶é¢ˆå±‚ï¼Œç”¨äºŽå…¨å±€å»ºæ¨¡ã€‚IDConvæ¨¡å—è¢«ç”¨äºŽç¼–ç å™¨å’Œè§£ç å™¨çš„å·ç§¯å±‚ï¼Œç”¨äºŽé«˜æ•ˆçš„ç‰¹å¾æå–ã€‚æ•´ä¸ªç½‘ç»œç»“æž„ç®€æ´é«˜æ•ˆï¼Œæ˜“äºŽéƒ¨ç½²åˆ°èµ„æºå—é™çš„è®¾å¤‡ä¸Šã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†å¹…åº¦æ„ŸçŸ¥çº¿æ€§æ³¨æ„åŠ›ï¼ˆMALAï¼‰å’ŒInceptionæ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆIDConvï¼‰ã€‚MALAé€šè¿‡æ˜¾å¼ä¿ç•™æŸ¥è¯¢å‘é‡çš„èŒƒæ•°ä¿¡æ¯ï¼Œè§£å†³äº†çº¿æ€§æ³¨æ„åŠ›å¿½ç•¥å¹…åº¦ä¿¡æ¯çš„é—®é¢˜ï¼Œå®žçŽ°äº†é«˜æ•ˆçš„å…¨å±€å»ºæ¨¡ã€‚IDConvå°†å¤§æ ¸å·ç§¯åˆ†è§£ä¸ºå¤šä¸ªå¹¶è¡Œçš„æ·±åº¦å¯åˆ†ç¦»å·ç§¯åˆ†æ”¯ï¼Œå‡å°‘äº†å‚æ•°å†—ä½™ï¼Œæé«˜äº†è®¡ç®—æ•ˆçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šMALAæ¨¡å—çš„å…³é”®è®¾è®¡åœ¨äºŽåœ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡æ—¶ï¼Œæ˜¾å¼åœ°è€ƒè™‘äº†æŸ¥è¯¢å‘é‡çš„èŒƒæ•°ä¿¡æ¯ï¼Œä»Žè€Œé¿å…äº†ä¿¡æ¯æŸå¤±ã€‚IDConvæ¨¡å—çš„å…³é”®è®¾è®¡åœ¨äºŽå°†å¤§æ ¸å·ç§¯åˆ†è§£ä¸ºæ­£æ–¹å½¢ã€æ°´å¹³å’Œåž‚ç›´æ¡çŠ¶çš„æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼Œä»Žè€Œåœ¨ä¸åŒæ–¹å‘ä¸Šæå–ç‰¹å¾ï¼Œå¹¶å‡å°‘å‚æ•°é‡ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨å¸¸ç”¨çš„æ—¶åŸŸæˆ–é¢‘åŸŸæŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚L1æŸå¤±æˆ–å‡æ–¹è¯¯å·®ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒIMSEåœ¨VoiceBank+DEMANDæ•°æ®é›†ä¸Šï¼Œç›¸æ¯”äºŽMUSEåŸºçº¿ï¼Œå‚æ•°é‡å‡å°‘äº†16.8%ï¼ˆä»Ž0.513Måˆ°0.427Mï¼‰ï¼ŒåŒæ—¶åœ¨PESQæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†3.373ï¼Œä¸Žæœ€å…ˆè¿›æ°´å¹³ç›¸å½“ã€‚è¿™è¡¨æ˜ŽIMSEåœ¨æ¨¡åž‹å¤§å°å’Œè¯­éŸ³è´¨é‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œä¸ºè¶…è½»é‡çº§è¯­éŸ³å¢žå¼ºè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§èµ„æºå—é™çš„è¯­éŸ³å¢žå¼ºåœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿã€æ™ºèƒ½å®¶å±…è®¾å¤‡ç­‰ã€‚é€šè¿‡é™ä½Žæ¨¡åž‹å¤æ‚åº¦å’Œè®¡ç®—é‡ï¼ŒIMSEèƒ½å¤Ÿåœ¨è¿™äº›è®¾å¤‡ä¸Šå®žçŽ°å®žæ—¶çš„è¯­éŸ³å¢žå¼ºï¼Œæé«˜è¯­éŸ³é€šä¿¡è´¨é‡å’Œç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºŽåŠ©å¬å™¨ç­‰è¾…åŠ©è®¾å¤‡ï¼Œå¸®åŠ©å¬åŠ›å—æŸäººå£«æ›´å¥½åœ°ç†è§£è¯­éŸ³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex "approximate-compensate" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the "amplitude-ignoring" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.

