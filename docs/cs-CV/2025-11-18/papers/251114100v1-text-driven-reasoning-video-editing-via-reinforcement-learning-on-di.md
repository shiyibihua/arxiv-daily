---
layout: default
title: Text-Driven Reasoning Video Editing via Reinforcement Learning on Digital Twin Representations
---

# Text-Driven Reasoning Video Editing via Reinforcement Learning on Digital Twin Representations

**arXiv**: [2511.14100v1](https://arxiv.org/abs/2511.14100) | [PDF](https://arxiv.org/pdf/2511.14100.pdf)

**ä½œè€…**: Yiqing Shen, Chenjia Li, Mathias Unberath

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRIVERæ¨¡åž‹ï¼Œé€šè¿‡æ•°å­—å­ªç”Ÿå’Œå¼ºåŒ–å­¦ä¹ è§£å†³æ–‡æœ¬é©±åŠ¨çš„æŽ¨ç†è§†é¢‘ç¼–è¾‘ä»»åŠ¡ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `æ–‡æœ¬é©±åŠ¨è§†é¢‘ç¼–è¾‘` `æŽ¨ç†ç¼–è¾‘` `æ•°å­—å­ªç”Ÿ` `å¼ºåŒ–å­¦ä¹ ` `å¤šè·³æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–‡æœ¬é©±åŠ¨è§†é¢‘ç¼–è¾‘æ–¹æ³•ä¾èµ–äºŽç²¾ç¡®çš„ç¼–è¾‘ç›®æ ‡æè¿°ï¼Œéš¾ä»¥å¤„ç†è¯­ä¹‰å±žæ€§æˆ–å¯¹è±¡å…³ç³»çš„éšå¼æŸ¥è¯¢ã€‚
2. RIVERæ¨¡åž‹åˆ©ç”¨æ•°å­—å­ªç”Ÿè¡¨ç¤ºè§£è€¦æŽ¨ç†å’Œç”Ÿæˆï¼Œé€šè¿‡å¤§åž‹è¯­è¨€æ¨¡åž‹è¿›è¡Œå¤šè·³æŽ¨ç†ï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡åž‹è¿›è¡Œç¼–è¾‘ã€‚
3. RIVERåœ¨RVEBenchmarkä¸Šå–å¾—æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨VegGIEå’ŒFiVEä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘ç¼–è¾‘ä»»åŠ¡â€”â€”æŽ¨ç†è§†é¢‘ç¼–è¾‘ï¼Œè¯¥ä»»åŠ¡è¦æ±‚æ¨¡åž‹é€šè¿‡å¤šè·³æŽ¨ç†æ¥ç†è§£éšå¼æŸ¥è¯¢ï¼Œä»Žè€ŒæŽ¨æ–­å‡ºç¼–è¾‘ç›®æ ‡å¹¶æ‰§è¡Œä¿®æ”¹ã€‚ä¸ºæ­¤ï¼Œä½œè€…ä»¬æå‡ºäº†RIVERï¼ˆReasoning-based Implicit Video Editorï¼‰æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹é€šè¿‡è§†é¢‘å†…å®¹çš„æ•°å­—å­ªç”Ÿè¡¨ç¤ºæ¥è§£è€¦æŽ¨ç†å’Œç”Ÿæˆï¼Œæ•°å­—å­ªç”Ÿè¡¨ç¤ºä¿ç•™äº†ç©ºé—´å…³ç³»ã€æ—¶é—´è½¨è¿¹å’Œè¯­ä¹‰å±žæ€§ã€‚ç„¶åŽï¼Œå¤§åž‹è¯­è¨€æ¨¡åž‹å°†å¤„ç†æ­¤è¡¨ç¤ºä»¥åŠéšå¼æŸ¥è¯¢ï¼Œæ‰§è¡Œå¤šè·³æŽ¨ç†ä»¥ç¡®å®šä¿®æ”¹ï¼Œå¹¶è¾“å‡ºç»“æž„åŒ–æŒ‡ä»¤ï¼ŒæŒ‡å¯¼åŸºäºŽæ‰©æ•£çš„ç¼–è¾‘å™¨æ‰§è¡Œåƒç´ çº§æ›´æ”¹ã€‚RIVERè®­ç»ƒä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œå¥–åŠ±è¯„ä¼°æŽ¨ç†å‡†ç¡®æ€§å’Œç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼Œä½œè€…ä»¬è¿˜æå‡ºäº†RVEBenchmarkï¼Œä¸€ä¸ªåŒ…å«100ä¸ªè§†é¢‘å’Œ519ä¸ªéšå¼æŸ¥è¯¢çš„åŸºå‡†ï¼Œä¸“é—¨ç”¨äºŽæŽ¨ç†è§†é¢‘ç¼–è¾‘ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒRIVERåœ¨RVEBenchmarkä¸Šè¡¨çŽ°æœ€ä½³ï¼Œå¹¶åœ¨VegGIEå’ŒFiVEä¸¤ä¸ªé¢å¤–çš„è§†é¢‘ç¼–è¾‘åŸºå‡†ä¸Šå®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†å…­ç§åŸºçº¿æ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬é©±åŠ¨çš„æŽ¨ç†è§†é¢‘ç¼–è¾‘é—®é¢˜ï¼Œå³ç”¨æˆ·é€šè¿‡éšå¼æŸ¥è¯¢ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºŽè¯­ä¹‰å±žæ€§æˆ–å¯¹è±¡å…³ç³»ï¼‰æ¥ç¼–è¾‘è§†é¢‘ï¼Œè€ŒçŽ°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦æ˜Žç¡®çš„ç¼–è¾‘ç›®æ ‡æè¿°ï¼Œæ— æ³•å¤„ç†è¿™ç§å¤æ‚çš„æŽ¨ç†éœ€æ±‚ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽç¼ºä¹å¯¹è§†é¢‘å†…å®¹æ·±å±‚æ¬¡çš„ç†è§£å’ŒæŽ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥å°†éšå¼æŸ¥è¯¢è½¬åŒ–ä¸ºå…·ä½“çš„ç¼–è¾‘æŒ‡ä»¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æŽ¨ç†å’Œç”Ÿæˆè§£è€¦ã€‚é¦–å…ˆï¼Œé€šè¿‡æ•°å­—å­ªç”Ÿè¡¨ç¤ºå¯¹è§†é¢‘å†…å®¹è¿›è¡Œç¼–ç ï¼Œä¿ç•™è§†é¢‘ä¸­çš„ç©ºé—´å…³ç³»ã€æ—¶é—´è½¨è¿¹å’Œè¯­ä¹‰å±žæ€§ã€‚ç„¶åŽï¼Œåˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹å¯¹æ•°å­—å­ªç”Ÿè¡¨ç¤ºå’Œéšå¼æŸ¥è¯¢è¿›è¡Œå¤šè·³æŽ¨ç†ï¼Œç”Ÿæˆç»“æž„åŒ–çš„ç¼–è¾‘æŒ‡ä»¤ã€‚æœ€åŽï¼Œä½¿ç”¨åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„ç¼–è¾‘å™¨æ ¹æ®æŒ‡ä»¤æ‰§è¡Œåƒç´ çº§åˆ«çš„ä¿®æ”¹ã€‚è¿™ç§è§£è€¦çš„è®¾è®¡ä½¿å¾—æ¨¡åž‹å¯ä»¥ä¸“æ³¨äºŽæŽ¨ç†å’Œç”Ÿæˆä¸¤ä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œä»Žè€Œæé«˜ç¼–è¾‘çš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šRIVERæ¨¡åž‹çš„æ•´ä½“æ¡†æž¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ•°å­—å­ªç”Ÿè¡¨ç¤ºæ¨¡å—ã€æŽ¨ç†æ¨¡å—å’Œç”Ÿæˆæ¨¡å—ã€‚æ•°å­—å­ªç”Ÿè¡¨ç¤ºæ¨¡å—è´Ÿè´£å°†è§†é¢‘ç¼–ç æˆä¿ç•™ç©ºé—´ã€æ—¶é—´å’Œè¯­ä¹‰ä¿¡æ¯çš„è¡¨ç¤ºã€‚æŽ¨ç†æ¨¡å—ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼Œç»“åˆæ•°å­—å­ªç”Ÿè¡¨ç¤ºå’Œéšå¼æŸ¥è¯¢ï¼Œè¿›è¡Œå¤šè·³æŽ¨ç†ï¼Œç”Ÿæˆç»“æž„åŒ–çš„ç¼–è¾‘æŒ‡ä»¤ã€‚ç”Ÿæˆæ¨¡å—åˆ™æ˜¯ä¸€ä¸ªåŸºäºŽæ‰©æ•£æ¨¡åž‹çš„è§†é¢‘ç¼–è¾‘å™¨ï¼Œæ ¹æ®æŽ¨ç†æ¨¡å—ç”Ÿæˆçš„æŒ‡ä»¤ï¼Œå¯¹è§†é¢‘è¿›è¡Œåƒç´ çº§åˆ«çš„ä¿®æ”¹ã€‚æ•´ä¸ªæµç¨‹é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œå¥–åŠ±å‡½æ•°åŒæ—¶è€ƒè™‘æŽ¨ç†çš„å‡†ç¡®æ€§å’Œç”Ÿæˆçš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šRIVERæ¨¡åž‹æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå¼•å…¥äº†æ•°å­—å­ªç”Ÿè¡¨ç¤ºå’Œè§£è€¦æŽ¨ç†ä¸Žç”Ÿæˆçš„è®¾è®¡ã€‚æ•°å­—å­ªç”Ÿè¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰è§†é¢‘ä¸­çš„å¤æ‚å…³ç³»ï¼Œä¸ºæŽ¨ç†æä¾›ä¸°å¯Œçš„ä¿¡æ¯ã€‚è§£è€¦çš„è®¾è®¡ä½¿å¾—æ¨¡åž‹å¯ä»¥åˆ†åˆ«ä¼˜åŒ–æŽ¨ç†å’Œç”Ÿæˆæ¨¡å—ï¼Œä»Žè€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒä¹Ÿä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çš„æŽ¨ç†ç¼–è¾‘ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°å­—å­ªç”Ÿè¡¨ç¤ºæ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰æ¨¡åž‹æ¥æå–è§†é¢‘å¸§çš„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å›¾ç¥žç»ç½‘ç»œæ¥å»ºæ¨¡å¯¹è±¡ä¹‹é—´çš„å…³ç³»ã€‚åœ¨æŽ¨ç†æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†é¢„è®­ç»ƒçš„å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼Œå¹¶é’ˆå¯¹è§†é¢‘ç¼–è¾‘ä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒã€‚åœ¨ç”Ÿæˆæ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„è§†é¢‘ç¼–è¾‘å™¨ï¼Œå¹¶æ ¹æ®æŽ¨ç†æ¨¡å—çš„æŒ‡ä»¤è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°åŒ…æ‹¬æŽ¨ç†å‡†ç¡®æ€§å¥–åŠ±å’Œç”Ÿæˆè´¨é‡å¥–åŠ±ï¼Œå…¶ä¸­æŽ¨ç†å‡†ç¡®æ€§å¥–åŠ±é€šè¿‡è¯„ä¼°ç”Ÿæˆçš„ç¼–è¾‘æŒ‡ä»¤ä¸ŽçœŸå®žæŒ‡ä»¤çš„åŒ¹é…ç¨‹åº¦æ¥è®¡ç®—ï¼Œç”Ÿæˆè´¨é‡å¥–åŠ±é€šè¿‡è¯„ä¼°ç¼–è¾‘åŽçš„è§†é¢‘ä¸Žç”¨æˆ·æœŸæœ›çš„åŒ¹é…ç¨‹åº¦æ¥è®¡ç®—ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

RIVERåœ¨æå‡ºçš„RVEBenchmarkä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¯æ˜Žäº†å…¶åœ¨æŽ¨ç†è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒRIVERè¿˜åœ¨VegGIEå’ŒFiVEä¸¤ä¸ªè§†é¢‘ç¼–è¾‘åŸºå‡†ä¸Šå®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…­ç§åŸºçº¿æ–¹æ³•ï¼Œè¡¨æ˜Žå…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®žéªŒç»“æžœéªŒè¯äº†æ•°å­—å­ªç”Ÿè¡¨ç¤ºå’Œè§£è€¦æŽ¨ç†ä¸Žç”Ÿæˆè®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

RIVERæ¨¡åž‹åœ¨è§†é¢‘å†…å®¹åˆ›ä½œã€è‡ªåŠ¨åŒ–è§†é¢‘ç¼–è¾‘ã€ä¸ªæ€§åŒ–è§†é¢‘ç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„æ–‡æœ¬æè¿°ï¼Œå¿«é€Ÿä¿®æ”¹è§†é¢‘å†…å®¹ï¼Œå®žçŽ°åˆ›æ„æƒ³æ³•ã€‚è¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºŽæ™ºèƒ½ç›‘æŽ§ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œæé«˜è§†é¢‘åˆ†æžå’Œç†è§£çš„å‡†ç¡®æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-driven video editing enables users to modify video content only using text queries. While existing methods can modify video content if explicit descriptions of editing targets with precise spatial locations and temporal boundaries are provided, these requirements become impractical when users attempt to conceptualize edits through implicit queries referencing semantic properties or object relationships. We introduce reasoning video editing, a task where video editing models must interpret implicit queries through multi-hop reasoning to infer editing targets before executing modifications, and a first model attempting to solve this complex task, RIVER (Reasoning-based Implicit Video Editor). RIVER decouples reasoning from generation through digital twin representations of video content that preserve spatial relationships, temporal trajectories, and semantic attributes. A large language model then processes this representation jointly with the implicit query, performing multi-hop reasoning to determine modifications, then outputs structured instructions that guide a diffusion-based editor to execute pixel-level changes. RIVER training uses reinforcement learning with rewards that evaluate reasoning accuracy and generation quality. Finally, we introduce RVEBenchmark, a benchmark of 100 videos with 519 implicit queries spanning three levels and categories of reasoning complexity specifically for reasoning video editing. RIVER demonstrates best performance on the proposed RVEBenchmark and also achieves state-of-the-art performance on two additional video editing benchmarks (VegGIE and FiVE), where it surpasses six baseline methods.

