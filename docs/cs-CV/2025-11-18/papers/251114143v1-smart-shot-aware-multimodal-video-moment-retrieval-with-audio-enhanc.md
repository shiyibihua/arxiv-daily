---
layout: default
title: SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM
---

# SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM

**arXiv**: [2511.14143v1](https://arxiv.org/abs/2511.14143) | [PDF](https://arxiv.org/pdf/2511.14143.pdf)

**ä½œè€…**: An Yu, Weiheng Lu, Jian Li, Zhenfei Zhang, Yunhang Shen, Felix X. -F. Ye, Ming-Ching Chang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSMARTæ¡†æž¶ï¼Œé€šè¿‡éŸ³é¢‘å¢žå¼ºå’Œé•œå¤´æ„ŸçŸ¥åŽ‹ç¼©è§£å†³è§†é¢‘æ—¶åˆ»æ£€ç´¢ä¸­çš„ç»†ç²’åº¦å®šä½é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘æ—¶åˆ»æ£€ç´¢` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `éŸ³é¢‘å¢žå¼º` `é•œå¤´æ„ŸçŸ¥åŽ‹ç¼©` `ç»†ç²’åº¦å®šä½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘æ—¶åˆ»æ£€ç´¢æ–¹æ³•ä¾èµ–ç²—ç²’åº¦æ—¶åºå’Œå•ä¸€è§†è§‰æ¨¡æ€ï¼Œéš¾ä»¥å¤„ç†å¤æ‚è§†é¢‘ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆéŸ³é¢‘çº¿ç´¢å’Œé•œå¤´çº§ç»“æž„ï¼Œé‡‡ç”¨é•œå¤´æ„ŸçŸ¥ä»¤ç‰ŒåŽ‹ç¼©ä»¥å‡å°‘å†—ä½™å¹¶ä¿ç•™ç»†èŠ‚ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Charades-STAå’ŒQVHighlightsæ•°æ®é›†ä¸Šï¼Œæ€§èƒ½è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼ŒæŒ‡æ ‡æ˜¾è‘—æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video Moment Retrieval is a task in video understanding that aims to localize a specific temporal segment in an untrimmed video based on a natural language query. Despite recent progress in moment retrieval from videos using both traditional techniques and Multimodal Large Language Models (MLLM), most existing methods still rely on coarse temporal understanding and a single visual modality, limiting performance on complex videos. To address this, we introduce \textit{S}hot-aware \textit{M}ultimodal \textit{A}udio-enhanced \textit{R}etrieval of \textit{T}emporal \textit{S}egments (SMART), an MLLM-based framework that integrates audio cues and leverages shot-level temporal structure. SMART enriches multimodal representations by combining audio and visual features while applying \textbf{Shot-aware Token Compression}, which selectively retains high-information tokens within each shot to reduce redundancy and preserve fine-grained temporal details. We also refine prompt design to better utilize audio-visual cues. Evaluations on Charades-STA and QVHighlights show that SMART achieves significant improvements over state-of-the-art methods, including a 1.61\% increase in R1@0.5 and 2.59\% gain in R1@0.7 on Charades-STA.

