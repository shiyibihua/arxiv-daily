---
layout: default
title: NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards
---

# NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards

**arXiv**: [2511.14659v1](https://arxiv.org/abs/2511.14659) | [PDF](https://arxiv.org/pdf/2511.14659.pdf)

**ä½œè€…**: Chia-Yu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNORA-1.5 VLAæ¨¡åž‹ï¼Œé€šè¿‡ä¸–ç•Œæ¨¡åž‹å’ŒåŠ¨ä½œå¥–åŠ±å¢žå¼ºå¯é æ€§ï¼Œé€‚ç”¨äºŽå…·èº«ä»»åŠ¡éƒ¨ç½²ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ä¸–ç•Œæ¨¡åž‹å¥–åŠ±` `ç›´æŽ¥åå¥½ä¼˜åŒ–` `å…·èº«æ™ºèƒ½` `åŽè®­ç»ƒå¼ºåŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹åœ¨è·¨çŽ¯å¢ƒå’Œå…·èº«ä»»åŠ¡ä¸­å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæµåŒ¹é…åŠ¨ä½œä¸“å®¶å’ŒåŸºäºŽä¸–ç•Œæ¨¡åž‹ä¸ŽåŠ¨ä½œåå·®çš„å¥–åŠ±æ¨¡åž‹è¿›è¡ŒåŽè®­ç»ƒä¼˜åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æ˜¾è‘—æå‡ï¼ŒéªŒè¯å¥–åŠ±åŽè®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.

