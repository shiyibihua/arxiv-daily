---
layout: default
title: DoGCLR: Dominance-Game Contrastive Learning Network for Skeleton-Based Action Recognition
---

# DoGCLR: Dominance-Game Contrastive Learning Network for Skeleton-Based Action Recognition

**arXiv**: [2511.14179v1](https://arxiv.org/abs/2511.14179) | [PDF](https://arxiv.org/pdf/2511.14179.pdf)

**ä½œè€…**: Yanshan Li, Ke Ma, Miaomiao Wei, Linhui Dai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDoGCLRä»¥è§£å†³éª¨æž¶åŠ¨ä½œè¯†åˆ«ä¸­å¯¹æ¯”å­¦ä¹ çš„è¿åŠ¨ä¿¡æ¯æŸå¤±å’Œè´Ÿæ ·æœ¬é€‰æ‹©é—®é¢˜**

**å…³é”®è¯**: `éª¨æž¶åŠ¨ä½œè¯†åˆ«` `è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ` `åšå¼ˆè®º` `æ—¶ç©ºæƒé‡å®šä½` `ç†µé©±åŠ¨ç­–ç•¥` `è´Ÿæ ·æœ¬ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•å‡åŒ€å¤„ç†éª¨æž¶åŒºåŸŸå¹¶ä½¿ç”¨FIFOé˜Ÿåˆ—ï¼Œå¯¼è‡´è¿åŠ¨ä¿¡æ¯æŸå¤±å’Œéžæœ€ä¼˜è´Ÿæ ·æœ¬é€‰æ‹©
2. åŸºäºŽåšå¼ˆè®ºå»ºæ¨¡æ­£è´Ÿæ ·æœ¬æž„å»ºä¸ºæ”¯é…æ¸¸æˆï¼Œç»“åˆæ—¶ç©ºæƒé‡å®šä½å’Œç†µé©±åŠ¨ç­–ç•¥ä¼˜åŒ–æ ·æœ¬
3. åœ¨NTU RGB+Då’ŒPKU-MMDæ•°æ®é›†ä¸Šè¶…è¶ŠSOTAï¼Œæœ€é«˜æå‡2.7%ï¼Œåœ¨æŒ‘æˆ˜åœºæ™¯ä¸­é²æ£’æ€§å¼º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing self-supervised contrastive learning methods for skeleton-based action recognition often process all skeleton regions uniformly, and adopt a first-in-first-out (FIFO) queue to store negative samples, which leads to motion information loss and non-optimal negative sample selection. To address these challenges, this paper proposes Dominance-Game Contrastive Learning network for skeleton-based action Recognition (DoGCLR), a self-supervised framework based on game theory. DoGCLR models the construction of positive and negative samples as a dynamic Dominance Game, where both sample types interact to reach an equilibrium that balances semantic preservation and discriminative strength. Specifically, a spatio-temporal dual weight localization mechanism identifies key motion regions and guides region-wise augmentations to enhance motion diversity while maintaining semantics. In parallel, an entropy-driven dominance strategy manages the memory bank by retaining high entropy (hard) negatives and replacing low-entropy (weak) ones, ensuring consistent exposure to informative contrastive signals. Extensive experiments are conducted on NTU RGB+D and PKU-MMD datasets. On NTU RGB+D 60 X-Sub/X-View, DoGCLR achieves 81.1%/89.4% accuracy, and on NTU RGB+D 120 X-Sub/X-Set, DoGCLR achieves 71.2%/75.5% accuracy, surpassing state-of-the-art methods by 0.1%, 2.7%, 1.1%, and 2.3%, respectively. On PKU-MMD Part I/Part II, DoGCLR performs comparably to the state-of-the-art methods and achieves a 1.9% higher accuracy on Part II, highlighting its strong robustness on more challenging scenarios.

