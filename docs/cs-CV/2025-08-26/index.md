---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-26
---

# cs.CVï¼ˆ2025-08-26ï¼‰

ğŸ“Š å…± **20** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250819165v1-dual-enhancement-on-3d-vision-language-perception-for-monocular-3d-v.html">Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding</a></td>
  <td>æå‡ºåŒé‡å¢å¼ºæ–¹æ³•ä»¥è§£å†³å•ç›®3Dè§†è§‰å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19165v1" data-paper-url="./papers/250819165v1-dual-enhancement-on-3d-vision-language-perception-for-monocular-3d-v.html" onclick="toggleFavorite(this, '2508.19165v1', 'Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250818632v1-decouple-reorganize-and-fuse-a-multimodal-framework-for-cancer-survi.html">Decouple, Reorganize, and Fuse: A Multimodal Framework for Cancer Survival Prediction</a></td>
  <td>æå‡ºDeReFæ¡†æ¶ä»¥è§£å†³ç™Œç—‡ç”Ÿå­˜é¢„æµ‹ä¸­çš„ä¿¡æ¯èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18632v1" data-paper-url="./papers/250818632v1-decouple-reorganize-and-fuse-a-multimodal-framework-for-cancer-survi.html" onclick="toggleFavorite(this, '2508.18632v1', 'Decouple, Reorganize, and Fuse: A Multimodal Framework for Cancer Survival Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250818772v1-beyond-the-textual-generating-coherent-visual-options-for-mcqs.html">Beyond the Textual: Generating Coherent Visual Options for MCQs</a></td>
  <td>æå‡ºè·¨æ¨¡æ€é€‰é¡¹åˆæˆæ¡†æ¶ä»¥ç”Ÿæˆè§†è§‰é€‰é¡¹çš„å¤šé¡¹é€‰æ‹©é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18772v1" data-paper-url="./papers/250818772v1-beyond-the-textual-generating-coherent-visual-options-for-mcqs.html" onclick="toggleFavorite(this, '2508.18772v1', 'Beyond the Textual: Generating Coherent Visual Options for MCQs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250819242v1-autoregressive-universal-video-segmentation-model.html">Autoregressive Universal Video Segmentation Model</a></td>
  <td>æå‡ºè‡ªå›å½’é€šç”¨è§†é¢‘åˆ†å‰²æ¨¡å‹ä»¥è§£å†³æ— æç¤ºåˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19242v1" data-paper-url="./papers/250819242v1-autoregressive-universal-video-segmentation-model.html" onclick="toggleFavorite(this, '2508.19242v1', 'Autoregressive Universal Video Segmentation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250818904v1-event-enriched-image-analysis-grand-challenge-at-acm-multimedia-2025.html">Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025</a></td>
  <td>æå‡ºEVENTAæŒ‘æˆ˜ä»¥è§£å†³äº‹ä»¶çº§å¤šæ¨¡æ€ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18904v1" data-paper-url="./papers/250818904v1-event-enriched-image-analysis-grand-challenge-at-acm-multimedia-2025.html" onclick="toggleFavorite(this, '2508.18904v1', 'Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250818634v2-owlcap-harmonizing-motion-detail-for-video-captioning-via-hmd-270k-a.html">OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward</a></td>
  <td>æå‡ºOwlCapä»¥è§£å†³è§†é¢‘å­—å¹•ç”Ÿæˆä¸­çš„è¿åŠ¨ç»†èŠ‚ä¸å¹³è¡¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18634v2" data-paper-url="./papers/250818634v2-owlcap-harmonizing-motion-detail-for-video-captioning-via-hmd-270k-a.html" onclick="toggleFavorite(this, '2508.18634v2', 'OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/250818696v1-colorgs-high-fidelity-surgical-scene-reconstruction-with-colored-gau.html">ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting</a></td>
  <td>æå‡ºColorGSä»¥è§£å†³å†…çª¥é•œè§†é¢‘ä¸­ç»„ç»‡é‡å»ºçš„è‰²å½©ä¸å˜å½¢å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18696v1" data-paper-url="./papers/250818696v1-colorgs-high-fidelity-surgical-scene-reconstruction-with-colored-gau.html" onclick="toggleFavorite(this, '2508.18696v1', 'ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250818971v1-can-we-make-nerf-based-visual-localization-privacy-preserving.html">Can we make NeRF-based visual localization privacy-preserving?</a></td>
  <td>æå‡ºppNeSFä»¥è§£å†³NeRFè§†è§‰å®šä½ä¸­çš„éšç§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18971v1" data-paper-url="./papers/250818971v1-can-we-make-nerf-based-visual-localization-privacy-preserving.html" onclick="toggleFavorite(this, '2508.18971v1', 'Can we make NeRF-based visual localization privacy-preserving?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250818788v1-pseudomaptrainer-learning-online-mapping-without-hd-maps.html">PseudoMapTrainer: Learning Online Mapping without HD Maps</a></td>
  <td>æå‡ºPseudoMapTrainerä»¥è§£å†³åœ¨çº¿åœ°å›¾è®­ç»ƒä¾èµ–é«˜æ¸…åœ°å›¾çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18788v1" data-paper-url="./papers/250818788v1-pseudomaptrainer-learning-online-mapping-without-hd-maps.html" onclick="toggleFavorite(this, '2508.18788v1', 'PseudoMapTrainer: Learning Online Mapping without HD Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250819182v1-soccernet-2025-challenges-results.html">SoccerNet 2025 Challenges Results</a></td>
  <td>SoccerNet 2025æŒ‘æˆ˜æ¨åŠ¨è¶³çƒè§†é¢‘ç†è§£ç ”ç©¶è¿›å±•</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19182v1" data-paper-url="./papers/250819182v1-soccernet-2025-challenges-results.html" onclick="toggleFavorite(this, '2508.19182v1', 'SoccerNet 2025 Challenges Results')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250818799v2-robust-and-label-efficient-deep-waste-detection.html">Robust and Label-Efficient Deep Waste Detection</a></td>
  <td>æå‡ºåŸºäºé›†æˆçš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ä»¥æå‡åºŸç‰©æ£€æµ‹æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18799v2" data-paper-url="./papers/250818799v2-robust-and-label-efficient-deep-waste-detection.html" onclick="toggleFavorite(this, '2508.18799v2', 'Robust and Label-Efficient Deep Waste Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250819320v2-midas-multimodal-interactive-digital-human-synthesis-via-real-time-a.html">MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation</a></td>
  <td>æå‡ºMIDASæ¡†æ¶ä»¥è§£å†³å®æ—¶å¤šæ¨¡æ€äº¤äº’æ•°å­—äººåˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19320v2" data-paper-url="./papers/250819320v2-midas-multimodal-interactive-digital-human-synthesis-via-real-time-a.html" onclick="toggleFavorite(this, '2508.19320v2', 'MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250819305v1-geo2vec-shape-and-distance-aware-neural-representation-of-geospatial.html">Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities</a></td>
  <td>æå‡ºGeo2Vecä»¥è§£å†³åœ°ç†å®ä½“è¡¨ç¤ºå­¦ä¹ ä¸­çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">spatial relationship</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19305v1" data-paper-url="./papers/250819305v1-geo2vec-shape-and-distance-aware-neural-representation-of-geospatial.html" onclick="toggleFavorite(this, '2508.19305v1', 'Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250818726v1-flatness-aware-curriculum-learning-via-adversarial-difficulty.html">Flatness-aware Curriculum Learning via Adversarial Difficulty</a></td>
  <td>æå‡ºå¯¹æŠ—æ€§éš¾åº¦åº¦é‡ä»¥è§£å†³è¯¾ç¨‹å­¦ä¹ ä¸å¹³å¦æœ€å°å€¼ç»“åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">curriculum learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18726v1" data-paper-url="./papers/250818726v1-flatness-aware-curriculum-learning-via-adversarial-difficulty.html" onclick="toggleFavorite(this, '2508.18726v1', 'Flatness-aware Curriculum Learning via Adversarial Difficulty')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250818641v1-clustering-based-feature-representation-learning-for-oracle-bone-ins.html">Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection</a></td>
  <td>æå‡ºåŸºäºèšç±»çš„ç‰¹å¾è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ä»¥è§£å†³ç”²éª¨æ–‡æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18641v1" data-paper-url="./papers/250818641v1-clustering-based-feature-representation-learning-for-oracle-bone-ins.html" onclick="toggleFavorite(this, '2508.18641v1', 'Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250818753v2-rethinking-human-object-interaction-evaluation-for-both-vision-langu.html">Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods</a></td>
  <td>æå‡ºæ–°åŸºå‡†æ•°æ®é›†ä»¥è¯„ä¼°äººæœºäº¤äº’æ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span> <span class="paper-tag">HOI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18753v2" data-paper-url="./papers/250818753v2-rethinking-human-object-interaction-evaluation-for-both-vision-langu.html" onclick="toggleFavorite(this, '2508.18753v2', 'Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250818896v1-dqen-dual-query-enhancement-network-for-detr-based-hoi-detection.html">DQEN: Dual Query Enhancement Network for DETR-based HOI Detection</a></td>
  <td>æå‡ºåŒæŸ¥è¯¢å¢å¼ºç½‘ç»œä»¥è§£å†³DETRåŸºç¡€çš„HOIæ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span> <span class="paper-tag">HOI</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18896v1" data-paper-url="./papers/250818896v1-dqen-dual-query-enhancement-network-for-detr-based-hoi-detection.html" onclick="toggleFavorite(this, '2508.18896v1', 'DQEN: Dual Query Enhancement Network for DETR-based HOI Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250819209v1-omnihuman-15-instilling-an-active-mind-in-avatars-via-cognitive-simu.html">OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation</a></td>
  <td>æå‡ºOmniHuman-1.5ä»¥è§£å†³è§†é¢‘åŒ–èº«åŠ¨ç”»çš„æƒ…æ„Ÿè¡¨è¾¾é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span> <span class="paper-tag">character animation</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19209v1" data-paper-url="./papers/250819209v1-omnihuman-15-instilling-an-active-mind-in-avatars-via-cognitive-simu.html" onclick="toggleFavorite(this, '2508.19209v1', 'OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250819195v1-all-in-one-slider-for-attribute-manipulation-in-diffusion-models.html">All-in-One Slider for Attribute Manipulation in Diffusion Models</a></td>
  <td>æå‡ºå…¨èƒ½æ»‘å—ä»¥è§£å†³ç”Ÿæˆå›¾åƒå±æ€§æ“æ§éš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19195v1" data-paper-url="./papers/250819195v1-all-in-one-slider-for-attribute-manipulation-in-diffusion-models.html" onclick="toggleFavorite(this, '2508.19195v1', 'All-in-One Slider for Attribute Manipulation in Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250818621v1-wan-s2v-audio-driven-cinematic-video-generation.html">Wan-S2V: Audio-Driven Cinematic Video Generation</a></td>
  <td>æå‡ºWan-S2Vä»¥è§£å†³å¤æ‚å½±è§†åŠ¨ç”»ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">character animation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18621v1" data-paper-url="./papers/250818621v1-wan-s2v-audio-driven-cinematic-video-generation.html" onclick="toggleFavorite(this, '2508.18621v1', 'Wan-S2V: Audio-Driven Cinematic Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)