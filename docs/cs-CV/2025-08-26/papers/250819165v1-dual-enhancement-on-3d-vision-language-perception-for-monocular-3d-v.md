---
layout: default
title: Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding
---

# Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19165" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19165v1</a>
  <a href="https://arxiv.org/pdf/2508.19165.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19165v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19165v1', 'Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuzhen Li, Min Liu, Yuan Bian, Xueping Wang, Zhaoyang Li, Gen Li, Yaonan Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

**å¤‡æ³¨**: 10 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒé‡å¢å¼ºæ–¹æ³•ä»¥è§£å†³å•ç›®3Dè§†è§‰å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å•ç›®3Dè§†è§‰` `è§†è§‰å®šä½` `å¤šæ¨¡æ€å­¦ä¹ ` `æ–‡æœ¬å¢å¼º` `å‡ ä½•æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ–‡æœ¬æè¿°ä¸­çš„æ•°å€¼å•ä½æ—¶å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´3Dæ„ŸçŸ¥èƒ½åŠ›å¼±ã€‚
2. è®ºæ–‡æå‡º3Dæ–‡æœ¬å¢å¼ºå’Œæ–‡æœ¬å¼•å¯¼å‡ ä½•å¢å¼ºä¸¤ç§æ–¹æ³•ï¼Œä»¥æ”¹å–„æ–‡æœ¬åµŒå…¥å¯¹å‡ ä½•ä¿¡æ¯çš„ç†è§£ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨Mono3DReferæ•°æ®é›†ä¸Šå–å¾—äº†æ–°çš„æœ€ä¼˜ç»“æœï¼Œå‡†ç¡®ç‡æå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å•ç›®3Dè§†è§‰å®šä½æ˜¯ä¸€é¡¹æ–°å…´ä»»åŠ¡ï¼Œæ—¨åœ¨åˆ©ç”¨å¸¦æœ‰å‡ ä½•ä¿¡æ¯çš„æ–‡æœ¬æè¿°åœ¨RGBå›¾åƒä¸­å®šä½3Dç‰©ä½“ã€‚å°½ç®¡æ–‡æœ¬ä¸­åŒ…å«å‡ ä½•ç»†èŠ‚ï¼Œä½†æˆ‘ä»¬å‘ç°æ–‡æœ¬åµŒå…¥å¯¹æ•°å€¼å¤§å°æ•æ„Ÿï¼Œå´å¿½è§†äº†æµ‹é‡å•ä½çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥å¢å¼ºæ¨¡å‹å¯¹æ–‡æœ¬åµŒå…¥å’Œå‡ ä½•ç‰¹å¾çš„3Dæ„ŸçŸ¥ã€‚é¦–å…ˆï¼Œæå‡ºäº†3Dæ–‡æœ¬å¢å¼ºï¼ˆ3DTEï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¢åŠ æ–‡æœ¬æŸ¥è¯¢ä¸­è·ç¦»æè¿°ç¬¦çš„å¤šæ ·æ€§æ¥æ”¹å–„å•ä½ä¹‹é—´çš„æ˜ å°„å…³ç³»ç†è§£ã€‚å…¶æ¬¡ï¼Œæå‡ºäº†æ–‡æœ¬å¼•å¯¼å‡ ä½•å¢å¼ºï¼ˆTGEï¼‰æ¨¡å—ï¼Œå°†åŸºæœ¬æ–‡æœ¬ç‰¹å¾æŠ•å½±åˆ°å‡ ä½•ä¸€è‡´çš„ç©ºé—´ä¸­ï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼º3Dæ–‡æœ¬ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Mono3DReferæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶åœ¨â€œè¿œâ€åœºæ™¯ä¸‹æé«˜äº†11.94%çš„å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å•ç›®3Dè§†è§‰å®šä½ä¸­ï¼Œæ–‡æœ¬æè¿°å¯¹æ•°å€¼å•ä½æ•æ„Ÿè€Œå¿½è§†å…¶å½±å“çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒå•ä½æ—¶ï¼Œå®¹æ˜“å¯¼è‡´3Dæ„ŸçŸ¥çš„è¯¯å¯¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥3Dæ–‡æœ¬å¢å¼ºï¼ˆ3DTEï¼‰å’Œæ–‡æœ¬å¼•å¯¼å‡ ä½•å¢å¼ºï¼ˆTGEï¼‰æ¨¡å—ï¼Œå¢å¼ºæ¨¡å‹å¯¹æ–‡æœ¬åµŒå…¥å’Œå‡ ä½•ç‰¹å¾çš„ç†è§£ï¼Œä»è€Œæé«˜3Då®šä½çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š3Dæ–‡æœ¬å¢å¼ºæ¨¡å—ç”¨äºä¸°å¯Œæ–‡æœ¬æè¿°çš„å¤šæ ·æ€§ï¼Œæ–‡æœ¬å¼•å¯¼å‡ ä½•å¢å¼ºæ¨¡å—ç”¨äºå°†æ–‡æœ¬ç‰¹å¾æŠ•å½±åˆ°å‡ ä½•ä¸€è‡´çš„ç©ºé—´ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†3Dæ–‡æœ¬å¢å¼ºå’Œæ–‡æœ¬å¼•å¯¼å‡ ä½•å¢å¼ºä¸¤ä¸ªæ¨¡å—ï¼Œæ˜¾è‘—æ”¹å–„äº†æ–‡æœ¬ç‰¹å¾å¯¹å‡ ä½•ä¿¡æ¯çš„å¼•å¯¼èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå¢å¼ºäº†3Dç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨3Dæ–‡æœ¬å¢å¼ºä¸­ï¼Œé€šè¿‡å¤šæ ·åŒ–è·ç¦»æè¿°ç¬¦æ¥å¢å¼ºå•ä½æ˜ å°„å…³ç³»ï¼›åœ¨æ–‡æœ¬å¼•å¯¼å‡ ä½•å¢å¼ºä¸­ï¼Œè®¾è®¡äº†ç‰¹å¾æŠ•å½±æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ–‡æœ¬ç‰¹å¾ä¸å‡ ä½•ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨Mono3DReferæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€ä¼˜ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨â€œè¿œâ€åœºæ™¯ä¸‹ï¼Œå‡†ç¡®ç‡æå‡äº†11.94%ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ€§èƒ½æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰ï¼Œèƒ½å¤Ÿæå‡è®¡ç®—æœºå¯¹ç¯å¢ƒçš„ç†è§£èƒ½åŠ›ï¼Œè¿›è€Œæ”¹å–„äººæœºäº¤äº’å’Œè‡ªåŠ¨åŒ–å†³ç­–çš„æ•ˆæœã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šåœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Monocular 3D visual grounding is a novel task that aims to locate 3D objects in RGB images using text descriptions with explicit geometry information. Despite the inclusion of geometry details in the text, we observe that the text embeddings are sensitive to the magnitude of numerical values but largely ignore the associated measurement units. For example, simply equidistant mapping the length with unit "meter" to "decimeters" or "centimeters" leads to severe performance degradation, even though the physical length remains equivalent. This observation signifies the weak 3D comprehension of pre-trained language model, which generates misguiding text features to hinder 3D perception. Therefore, we propose to enhance the 3D perception of model on text embeddings and geometry features with two simple and effective methods. Firstly, we introduce a pre-processing method named 3D-text Enhancement (3DTE), which enhances the comprehension of mapping relationships between different units by augmenting the diversity of distance descriptors in text queries. Next, we propose a Text-Guided Geometry Enhancement (TGE) module to further enhance the 3D-text information by projecting the basic text features into geometrically consistent space. These 3D-enhanced text features are then leveraged to precisely guide the attention of geometry features. We evaluate the proposed method through extensive comparisons and ablation studies on the Mono3DRefer dataset. Experimental results demonstrate substantial improvements over previous methods, achieving new state-of-the-art results with a notable accuracy gain of 11.94\% in the "Far" scenario. Our code will be made publicly available.

