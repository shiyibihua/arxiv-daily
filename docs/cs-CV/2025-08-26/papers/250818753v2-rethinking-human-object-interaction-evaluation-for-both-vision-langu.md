---
layout: default
title: Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods
---

# Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18753" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.18753v2</a>
  <a href="https://arxiv.org/pdf/2508.18753.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18753v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18753v2', 'Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Qinqian Lei, Bo Wang, Robby T. Tan

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-26 (Êõ¥Êñ∞: 2025-09-29)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Êñ∞Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰ª•ËØÑ‰º∞‰∫∫Êú∫‰∫§‰∫íÊ£ÄÊµãÊñπÊ≥ïÁöÑÊúâÊïàÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫îÔºö‰∫§‰∫í‰∏éÂèçÂ∫î (Interaction & Reaction)**

**ÂÖ≥ÈîÆËØç**: `‰∫∫Êú∫‰∫§‰∫í` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Âü∫ÂáÜÊï∞ÊçÆÈõÜ` `Â§öÁ≠îÊ°àÈÄâÊã©` `Â§çÊùÇÂú∫ÊôØ` `ÊÄßËÉΩËØÑ‰º∞` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑHOIÊ£ÄÊµãÊñπÊ≥ïÂæÄÂæÄ‰æùËµñ‰∫éÁ≤æÁ°ÆÊ†áÁ≠æÂåπÈÖçÔºåÊó†Ê≥ïÊúâÊïàËØÑ‰º∞ÁîüÊàêÊÄßVLMÁöÑÂ§öÊ†∑ÂåñËæìÂá∫„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÂ∞ÜHOIÊ£ÄÊµãËßÜ‰∏∫Â§öÁ≠îÊ°àÈÄâÊã©‰ªªÂä°Ôºå‰ª•ÊîØÊåÅVLMÂíåHOIÁâπÂÆöÊñπÊ≥ïÁöÑÁªü‰∏ÄËØÑ‰º∞„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂ§ßÂûãVLMÂú®Â§ßÂ§öÊï∞ÊåáÊ†á‰∏äË∂ÖË∂ä‰∫Ü‰º†ÁªüHOIÁâπÂÆöÊñπÊ≥ïÔºå‰ΩÜÂú®Â§çÊùÇÂú∫ÊôØ‰∏≠‰ªçÂ≠òÂú®ËØØÂà§ÈóÆÈ¢ò„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∫∫Êú∫‰∫§‰∫íÔºàHOIÔºâÊ£ÄÊµã‰º†Áªü‰∏ä‰æùËµñ‰∫éÁâπÂÆö‰ªªÂä°Ê®°ÂûãÔºåËøëÂπ¥Êù•ÈöèÁùÄÂ§ßÂûãÁîüÊàêÊÄßËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑÂÖ¥Ëµ∑ÔºåÁ†îÁ©∂ËÄÖÂºÄÂßãÊé¢ËÆ®Ëøô‰∫õÊ®°ÂûãÂú®HOIÊ£ÄÊµã‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÁé∞ÊúâÂü∫ÂáÜÂ¶ÇHICO-DETÂú®Áé∞‰ª£VLMÂá∫Áé∞‰πãÂâçÂºÄÂèëÔºå‰æùËµñ‰∫éÁ≤æÁ°ÆÊ†áÁ≠æÂåπÈÖçÔºåËøô‰∏éÁîüÊàêÊÄßËæìÂá∫ÁöÑÂ§öÊ†∑ÊÄßÁõ∏ÂÜ≤Á™Å„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÂ∞ÜHOIÊ£ÄÊµãÈáçÊñ∞ÂÆö‰πâ‰∏∫Â§öÁ≠îÊ°àÈÄâÊã©‰ªªÂä°ÔºåÂº∫Ë∞ÉÂ§ö‰∫∫ÁöÑÂ§çÊùÇÂú∫ÊôØÔºåÂéªÈô§ÁÆÄÂçïÊ°à‰æãÔºåÂπ∂Á≠ñÂàíÂõ∞ÈöæÁöÑË¥üÊ†∑Êú¨ÈÄâÊã©„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ§ßÂûãVLMÂú®Â§ßÂ§öÊï∞ÊåáÊ†á‰∏äË∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑHOIÁâπÂÆöÊñπÊ≥ïÔºåÂêåÊó∂ÂàÜÊûêÊè≠Á§∫‰∫ÜVLMÂú®Â§çÊùÇÂú∫ÊôØ‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâHOIÊ£ÄÊµãÊñπÊ≥ïÂú®ËØÑ‰º∞ÁîüÊàêÊÄßVLMÊó∂ÁöÑÂ±ÄÈôêÊÄßÔºåÁâπÂà´ÊòØÁ≤æÁ°ÆÊ†áÁ≠æÂåπÈÖçÂØºËá¥ÁöÑËØÑ‰º∞‰∏çÂÖ¨„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂ∞ÜHOIÊ£ÄÊµãÈáçÊñ∞ÂÆö‰πâ‰∏∫Â§öÁ≠îÊ°àÈÄâÊã©‰ªªÂä°ÔºåÂÖÅËÆ∏Â§ö‰∏™ÊúâÊïàÁöÑËæìÂá∫Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ËØÑ‰º∞VLMÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊñ∞Âü∫ÂáÜÊï∞ÊçÆÈõÜÂåÖÂê´Â§ö‰∫∫ÁöÑÂ§çÊùÇÂú∫ÊôØÔºåÂéªÈô§‰∫ÜÁÆÄÂçïÊ°à‰æãÔºåÂπ∂Á≠ñÂàí‰∫ÜÂõ∞ÈöæÁöÑË¥üÊ†∑Êú¨ÈÄâÊã©Ôºå‰ª•ÊèêÈ´òËØÑ‰º∞ÁöÑÊåëÊàòÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÈáçÊñ∞ÂÆö‰πâHOIÊ£ÄÊµã‰ªªÂä°Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÈÄÇÂ∫îÁîüÊàêÊÄßÊ®°ÂûãÁöÑËæìÂá∫ÁâπÊÄßÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊï∞ÊçÆÈõÜËÆæËÆ°‰∏≠ÔºåÂ¢ûÂä†‰∫ÜÂ§ö‰∫∫ÁöÑÂú∫ÊôØÊØî‰æãÔºåÂéªÈô§‰∫ÜËøá‰∫éÁÆÄÂçïÁöÑÊ°à‰æãÔºåÂπ∂Á≤æÂøÉÁ≠ñÂàí‰∫ÜË¥üÊ†∑Êú¨ÔºåÁ°Æ‰øùËØÑ‰º∞ÁöÑÂÖ®Èù¢ÊÄßÂíåÊåëÊàòÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ§ßÂûãVLMÂú®Â§ßÂ§öÊï∞ËØÑ‰º∞ÊåáÊ†á‰∏äË∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑHOIÁâπÂÆöÊñπÊ≥ïÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®Â§ö‰∏™Â§çÊùÇÂú∫ÊôØ‰∏≠ÔºåVLMÁöÑÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü15%ÔºåËÄåÂú®Â§ö‰∫∫ÁöÑ‰∫§‰∫íÂú∫ÊôØ‰∏≠ÔºåVLMÁöÑË°®Áé∞Â∞§‰∏∫Á™ÅÂá∫„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÁõëÊéß„ÄÅÊú∫Âô®‰∫∫‰∫§‰∫íÂíå‰∫∫Êú∫Âçè‰ΩúÁ≠âÂú∫ÊôØÔºåËÉΩÂ§üÊèêÂçáÁ≥ªÁªüÂØπ‰∫∫Êú∫‰∫§‰∫íÁöÑÁêÜËß£ÂíåÂìçÂ∫îËÉΩÂäõ„ÄÇÊú™Êù•ÔºåÈöèÁùÄVLMÊäÄÊúØÁöÑ‰∏çÊñ≠ÂèëÂ±ïÔºåËØ•Âü∫ÂáÜÊï∞ÊçÆÈõÜÂ∞Ü‰∏∫Áõ∏ÂÖ≥È¢ÜÂüüÁöÑÁ†îÁ©∂Êèê‰æõÈáçË¶ÅÂèÇËÄÉ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Human-object interaction (HOI) detection has traditionally been approached with task-specific models, sometimes augmented by early vision-language models (VLMs) such as CLIP. With the rise of large, generative VLMs, however, a natural question emerges: can standalone VLMs effectively perform HOI detection, and how do they compare to specialized HOI methods? Addressing this requires a benchmarking dataset and protocol that support both paradigms. Existing benchmarks such as HICO-DET were developed before modern VLMs and rely on exact label matching. This clashes with generative outputs, which may yield multiple equally valid interpretations. For example, in a single image, a person mid-motion with a frisbee might plausibly be described as 'throwing' or 'catching', yet only one is annotated as correct. Such rigid evaluation penalizes valid predictions from both VLMs and HOI-specific methods, but disproportionately underestimates VLM performance because their outputs are less constrained. We introduce a new benchmarking dataset that reformulates HOI detection as a multiple-answer multiple-choice task. It emphasizes challenging scenarios by (i) including a higher proportion of multi-person scenes where individuals perform different interactions, (ii) removing overly simple cases, and (iii) curating hard negative choices. This makes the benchmark more challenging than prior HOI datasets, while still supporting systematic evaluation of both standalone VLMs and HOI-specific methods under a unified protocol. Our results show that large VLMs already surpass state-of-the-art HOI-specific methods across most metrics, while analysis further uncovers key limitations: VLMs often misattribute surrounding people's interactions to the target person and struggle in complex multi-person or occluded scenarios.

