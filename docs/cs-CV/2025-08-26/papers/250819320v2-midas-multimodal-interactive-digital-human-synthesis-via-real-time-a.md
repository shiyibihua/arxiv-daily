---
layout: default
title: MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation
---

# MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19320" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19320v2</a>
  <a href="https://arxiv.org/pdf/2508.19320.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19320v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19320v2', 'MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Songlin Tang, Jiwen Liu, Borui Liao, Hejia Chen, Xiaoqiang Liu, Pengfei Wan

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-08-28)

**å¤‡æ³¨**: Technical Report. Project Page: https://chenmingthu.github.io/milm/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMIDASæ¡†æ¶ä»¥è§£å†³å®æ—¶å¤šæ¨¡æ€äº¤äº’æ•°å­—äººåˆæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€äº¤äº’` `æ•°å­—äººåˆæˆ` `è‡ªå›å½’ç”Ÿæˆ` `ä½å»¶è¿Ÿæ¨æ–­` `æ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„äº¤äº’å¼æ•°å­—äººè§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨å®æ—¶å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œå¯æ§æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªå›å½’è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå®æ—¶å¤„ç†éŸ³é¢‘ã€å§¿æ€å’Œæ–‡æœ¬ç­‰å¤šæ¨¡æ€è¾“å…¥ï¼Œè¾“å‡ºä¸€è‡´çš„ç©ºé—´å’Œè¯­ä¹‰è¡¨ç¤ºã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åŒå‘å¯¹è¯å’Œå¤šè¯­è¨€äººåˆæˆä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„ä½å»¶è¿Ÿå’Œé«˜æ•ˆç‡ï¼Œæå‡äº†å¤šæ¨¡æ€å¯æ§æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œäº¤äº’å¼æ•°å­—äººè§†é¢‘ç”Ÿæˆå—åˆ°å¹¿æ³›å…³æ³¨å¹¶å–å¾—æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å®æ—¶å¤„ç†å¤šæ ·è¾“å…¥ä¿¡å·æ—¶é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œæœ‰é™å¯æ§æ€§çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªå›å½’è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ”¯æŒå¤šæ¨¡æ€æ§åˆ¶å’Œä½å»¶è¿Ÿæ¨æ–­ï¼Œèƒ½å¤Ÿå®æ—¶å¤„ç†éŸ³é¢‘ã€å§¿æ€å’Œæ–‡æœ¬ç­‰å¤šç§æ¡ä»¶ç¼–ç ï¼Œå¹¶ç”Ÿæˆç©ºé—´å’Œè¯­ä¹‰ä¸€è‡´çš„è¡¨ç¤ºä»¥æŒ‡å¯¼å»å™ªè¿‡ç¨‹ã€‚ä¸ºæ”¯æŒè¯¥æ¡†æ¶ï¼Œæ„å»ºäº†ä¸€ä¸ªçº¦20,000å°æ—¶çš„å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†ï¼Œæä¾›ä¸°å¯Œçš„å¯¹è¯åœºæ™¯ç”¨äºè®­ç»ƒã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨ï¼Œæœ€å¤§å¯å®ç°64å€çš„å‹ç¼©æ¯”ï¼Œæœ‰æ•ˆå‡è½»äº†è‡ªå›å½’æ¨¡å‹çš„é•¿æ—¶é—´æ¨ç†è´Ÿæ‹…ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒå‘å¯¹è¯ã€å¤šè¯­è¨€äººåˆæˆå’Œäº¤äº’å¼ä¸–ç•Œæ¨¡å‹ä¸­å±•ç°å‡ºä½å»¶è¿Ÿã€é«˜æ•ˆç‡å’Œç»†ç²’åº¦å¤šæ¨¡æ€å¯æ§æ€§çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰äº¤äº’å¼æ•°å­—äººè§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨å®æ—¶å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶çš„é«˜è®¡ç®—æˆæœ¬å’Œæœ‰é™å¯æ§æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåº”å¯¹å¤šæ ·åŒ–çš„è¾“å…¥ä¿¡å·ï¼Œå¯¼è‡´ç”Ÿæˆæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§è‡ªå›å½’è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå…è®¸å¤šæ¨¡æ€æ§åˆ¶å’Œä½å»¶è¿Ÿæ¨æ–­ã€‚é€šè¿‡å¯¹æ ‡å‡†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæœ€å°ä¿®æ”¹ï¼Œæ¡†æ¶èƒ½å¤Ÿå¤„ç†éŸ³é¢‘ã€å§¿æ€å’Œæ–‡æœ¬ç­‰å¤šç§è¾“å…¥ï¼Œç”Ÿæˆç©ºé—´å’Œè¯­ä¹‰ä¸€è‡´çš„è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥æ¨¡å—ï¼ˆæ¥æ”¶å¤šæ¨¡æ€æ¡ä»¶ç¼–ç ï¼‰ã€ç”Ÿæˆæ¨¡å—ï¼ˆè‡ªå›å½’è§†é¢‘ç”Ÿæˆï¼‰å’Œå»å™ªæ¨¡å—ï¼ˆåˆ©ç”¨æ‰©æ•£å¤´è¿›è¡Œå»å™ªï¼‰ã€‚æ¡†æ¶æ”¯æŒå®æ—¶æµå¼ç”Ÿæˆï¼Œç¡®ä¿ä½å»¶è¿Ÿè¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥äº†æ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨ï¼Œæœ€å¤§å¯å®ç°64å€çš„å‹ç¼©æ¯”ï¼Œæœ‰æ•ˆå‡è½»äº†è‡ªå›å½’æ¨¡å‹çš„é•¿æ—¶é—´æ¨ç†è´Ÿæ‹…ã€‚è¿™ä¸€åˆ›æ–°æ˜¾è‘—æé«˜äº†ç”Ÿæˆæ•ˆç‡ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„æ¥å¢å¼ºå¤šæ¨¡æ€è¾“å…¥çš„èåˆèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMIDASæ¡†æ¶åœ¨åŒå‘å¯¹è¯ä»»åŠ¡ä¸­å®ç°äº†ä½äº100æ¯«ç§’çš„å»¶è¿Ÿï¼Œä¸”åœ¨å¤šè¯­è¨€äººåˆæˆä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡äº†çº¦30%çš„ç”Ÿæˆæ•ˆç‡ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€äº¤äº’ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€åœ¨çº¿æ•™è‚²å’Œç¤¾äº¤åª’ä½“ç­‰ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´åŠ ç”ŸåŠ¨å’Œäº’åŠ¨çš„æ•°å­—äººä½“éªŒã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œæœªæ¥å¯èƒ½åœ¨æ›´å¤šè¡Œä¸šä¸­å®ç°ä¸ªæ€§åŒ–å’Œå®æ—¶äº¤äº’çš„æ•°å­—äººåˆæˆï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œå‚ä¸æ„Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with heavy computational cost and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.

