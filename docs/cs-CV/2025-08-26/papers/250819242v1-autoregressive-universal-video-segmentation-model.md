---
layout: default
title: Autoregressive Universal Video Segmentation Model
---

# Autoregressive Universal Video Segmentation Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19242" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19242v1</a>
  <a href="https://arxiv.org/pdf/2508.19242.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19242v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19242v1', 'Autoregressive Universal Video Segmentation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Miran Heo, Sukjun Hwang, Min-Hung Chen, Yu-Chiang Frank Wang, Albert Gu, Seon Joo Kim, Ryo Hachiuma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªå›å½’é€šç”¨è§†é¢‘åˆ†å‰²æ¨¡å‹ä»¥è§£å†³æ— æç¤ºåˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘åˆ†å‰²` `è‡ªå›å½’æ¨¡å‹` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `æ— æç¤ºåˆ†å‰²` `å¹¶è¡Œè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘åˆ†å‰²æ–¹æ³•å¤šä¾èµ–äºæç¤ºï¼Œæ— æ³•æ»¡è¶³æ— æç¤ºåˆ†å‰²çš„å®é™…éœ€æ±‚ï¼Œå¯¼è‡´ä»»åŠ¡ç‰¹å®šæ¨¡å‹å’Œæµç¨‹çš„ç¢ç‰‡åŒ–ã€‚
2. æœ¬æ–‡æå‡ºè‡ªå›å½’é€šç”¨åˆ†å‰²æ¨¡å‹ï¼ˆAUSMï¼‰ï¼Œå°†æµè§†é¢‘åˆ†å‰²è§†ä¸ºåºåˆ—æ©ç é¢„æµ‹ï¼Œç»Ÿä¸€äº†æç¤ºå’Œæ— æç¤ºåˆ†å‰²çš„å¤„ç†æ–¹å¼ã€‚
3. åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒAUSMçš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨16å¸§åºåˆ—ä¸Šå®ç°äº†é«˜è¾¾2.5å€çš„è®­ç»ƒé€Ÿåº¦æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè§†é¢‘åŸºç¡€æ¨¡å‹å¦‚SAM2åœ¨æç¤ºè§†é¢‘åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†è®¸å¤šå®é™…åœºæ™¯éœ€è¦æ— æç¤ºåˆ†å‰²ï¼Œå³åœ¨æ²¡æœ‰å¤–éƒ¨æç¤ºçš„æƒ…å†µä¸‹æ£€æµ‹å’Œè·Ÿè¸ªè§†é¢‘ä¸­çš„æ‰€æœ‰å¯¹è±¡ã€‚æœ¬æ–‡å°†æµè§†é¢‘åˆ†å‰²é‡æ–°å®šä¹‰ä¸ºåºåˆ—æ©ç é¢„æµ‹ï¼Œæå‡ºè‡ªå›å½’é€šç”¨åˆ†å‰²æ¨¡å‹ï¼ˆAUSMï¼‰ï¼Œè¯¥æ¨¡å‹ç»Ÿä¸€äº†æç¤ºå’Œæ— æç¤ºè§†é¢‘åˆ†å‰²ã€‚AUSMåŸºäºæœ€æ–°çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„é•¿åº¦çš„è§†é¢‘æµï¼Œå¹¶ä¸”æ‰€æœ‰ç»„ä»¶å‡è®¾è®¡ä¸ºè·¨å¸§å¹¶è¡Œè®­ç»ƒï¼Œä»è€Œæ˜¾è‘—åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒAUSMè¶…è¶Šäº†ä¹‹å‰çš„é€šç”¨æµè§†é¢‘åˆ†å‰²æ–¹æ³•ï¼Œå¹¶åœ¨16å¸§åºåˆ—ä¸Šå®ç°äº†æœ€é«˜2.5å€çš„è®­ç»ƒåŠ é€Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰è§†é¢‘åˆ†å‰²æ–¹æ³•åœ¨æ— æç¤ºåˆ†å‰²åœºæ™¯ä¸­çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤–éƒ¨æç¤ºï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­æ— æ³•æœ‰æ•ˆæ£€æµ‹å’Œè·Ÿè¸ªæ‰€æœ‰å¯¹è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„è‡ªå›å½’é€šç”¨åˆ†å‰²æ¨¡å‹ï¼ˆAUSMï¼‰é€šè¿‡å°†è§†é¢‘åˆ†å‰²è§†ä¸ºåºåˆ—æ©ç é¢„æµ‹ï¼Œå€Ÿé‰´è¯­è¨€å»ºæ¨¡çš„æ€æƒ³ï¼Œèƒ½å¤Ÿåœ¨æ— æç¤ºçš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„å¯¹è±¡æ£€æµ‹å’Œè·Ÿè¸ªã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAUSMåŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œç»´æŠ¤å›ºå®šå¤§å°çš„ç©ºé—´çŠ¶æ€ï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„é•¿åº¦çš„è§†é¢‘æµã€‚æ¨¡å‹çš„æ‰€æœ‰ç»„ä»¶å‡è®¾è®¡ä¸ºæ”¯æŒè·¨å¸§å¹¶è¡Œè®­ç»ƒï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šAUSMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€äº†æç¤ºå’Œæ— æç¤ºè§†é¢‘åˆ†å‰²çš„å¤„ç†æ–¹å¼ï¼Œé‡‡ç”¨è‡ªå›å½’çš„æ–¹å¼è¿›è¡Œåºåˆ—æ©ç é¢„æµ‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒAUSMé‡‡ç”¨äº†å›ºå®šå¤§å°çš„ç©ºé—´çŠ¶æ€ï¼Œç»“åˆå¹¶è¡Œè®­ç»ƒç­–ç•¥ï¼Œä¼˜åŒ–äº†è®­ç»ƒé€Ÿåº¦å’Œæ•ˆç‡ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥é€‚åº”è§†é¢‘åˆ†å‰²ä»»åŠ¡çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚DAVIS17ã€YouTube-VOS 2018 & 2019ç­‰ï¼‰ä¸­ï¼ŒAUSMçš„æ€§èƒ½è¶…è¶Šäº†ä»¥å¾€çš„é€šç”¨æµè§†é¢‘åˆ†å‰²æ–¹æ³•ï¼Œå¹¶åœ¨16å¸§åºåˆ—ä¸Šå®ç°äº†é«˜è¾¾2.5å€çš„è®­ç»ƒé€Ÿåº¦æå‡ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆç‡ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡å®ç°é«˜æ•ˆçš„æ— æç¤ºè§†é¢‘åˆ†å‰²ï¼ŒAUSMèƒ½å¤Ÿæå‡å¯¹è±¡æ£€æµ‹å’Œè·Ÿè¸ªçš„å‡†ç¡®æ€§ï¼Œä¸ºå®æ—¶è§†é¢‘åˆ†ææä¾›æ”¯æŒï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent video foundation models such as SAM2 excel at prompted video segmentation by treating masks as a general-purpose primitive. However, many real-world settings require unprompted segmentation that aims to detect and track all objects in a video without external cues, leaving today's landscape fragmented across task-specific models and pipelines. We recast streaming video segmentation as sequential mask prediction, analogous to language modeling, and introduce the Autoregressive Universal Segmentation Model (AUSM), a single architecture that unifies both prompted and unprompted video segmentation. Built on recent state-space models, AUSM maintains a fixed-size spatial state and scales to video streams of arbitrary length. Furthermore, all components of AUSM are designed for parallel training across frames, yielding substantial speedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS 2018 & 2019, MOSE, YouTube-VIS 2019 & 2021, and OVIS) AUSM outperforms prior universal streaming video segmentation methods and achieves up to 2.5x faster training on 16-frame sequences.

