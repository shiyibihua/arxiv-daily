---
layout: default
title: CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation
---

# CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation

**arXiv**: [2511.03992v1](https://arxiv.org/abs/2511.03992) | [PDF](https://arxiv.org/pdf/2511.03992.pdf)

**ä½œè€…**: Yuwen Tao, Kanglei Zhou, Xin Tan, Yuan Xie

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-06

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CaRFï¼šé€šè¿‡å¢žå¼ºå¤šè§†è§’ä¸€è‡´æ€§æ”¹è¿›Referring 3Dé«˜æ–¯æº…å°„åˆ†å‰²**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `Referring 3Dåˆ†å‰²` `é«˜æ–¯æº…å°„` `å¤šè§†è§’ä¸€è‡´æ€§` `ç›¸æœºæ„ŸçŸ¥` `è·¨æ¨¡æ€å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰Referring 3Dé«˜æ–¯æº…å°„åˆ†å‰²æ–¹æ³•ä¾èµ–2Dæ¸²æŸ“ä¼ªç›‘ç£ï¼Œå¯¼è‡´è§†è§’ä¸ä¸€è‡´ï¼Œé™åˆ¶äº†åˆ†å‰²æ€§èƒ½ã€‚
2. CaRFé€šè¿‡å¼•å…¥ç›¸æœºæ„ŸçŸ¥çš„é«˜æ–¯åœºç¼–ç å’Œè®­ç»ƒæ—¶é…å¯¹è§†è§’ç›‘ç£ï¼Œç›´æŽ¥åœ¨3Dé«˜æ–¯ç©ºé—´ä¸­å®žçŽ°å¤šè§†è§’ä¸€è‡´æ€§ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒCaRFåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†Referring 3Dåˆ†å‰²çš„mIoUï¼Œä¼˜äºŽçŽ°æœ‰æŠ€æœ¯æ°´å¹³ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Referring 3D Gaussian Splatting Segmentation (R3DGS) æ—¨åœ¨è§£æžè‡ªç”±å½¢å¼çš„è¯­è¨€è¡¨è¾¾ï¼Œå¹¶åœ¨é«˜æ–¯åœºä¸­å®šä½ç›¸åº”çš„ 3D åŒºåŸŸã€‚å°½ç®¡æœ€è¿‘çš„ç ”ç©¶åœ¨è¯­è¨€å’Œ 3D å‡ ä½•ä¹‹é—´å¼•å…¥äº†è·¨æ¨¡æ€å¯¹é½ï¼Œä½†ç”±äºŽçŽ°æœ‰æµç¨‹ä¾èµ–äºŽ 2D æ¸²æŸ“çš„ä¼ªç›‘ç£å’Œç‰¹å®šè§†è§’çš„ç‰¹å¾å­¦ä¹ ï¼Œå› æ­¤ä»ç„¶éš¾ä»¥å®žçŽ°è·¨è§†è§’ä¸€è‡´æ€§ã€‚æœ¬æ–‡æå‡ºäº† Camera Aware Referring Field (CaRF)ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¯å¾®çš„æ¡†æž¶ï¼Œç›´æŽ¥åœ¨é«˜æ–¯ 3D ç©ºé—´ä¸­è¿è¡Œå¹¶å®žçŽ°å¤šè§†è§’ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒCaRF å¼•å…¥äº† Gaussian Field Camera Encoding (GFCE)ï¼Œå®ƒå°†ç›¸æœºå‡ ä½•ä¿¡æ¯èžå…¥é«˜æ–¯æ–‡æœ¬äº¤äº’ä¸­ï¼Œä»¥æ˜¾å¼åœ°å»ºæ¨¡è§†è§’ç›¸å…³çš„å˜åŒ–å¹¶å¢žå¼ºå‡ ä½•æŽ¨ç†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº† In Training Paired View Supervision (ITPVS)ï¼Œç”¨äºŽåœ¨è®­ç»ƒæœŸé—´å¯¹é½æ ¡å‡†è§†è§’ä¹‹é—´çš„æ¯ä¸ªé«˜æ–¯é€»è¾‘å€¼ï¼Œä»Žè€Œæœ‰æ•ˆåœ°ç¼“è§£å•è§†è§’è¿‡æ‹Ÿåˆï¼Œå¹¶æš´éœ²è§†è§’é—´çš„å·®å¼‚ä»¥è¿›è¡Œä¼˜åŒ–ã€‚åœ¨ä¸‰ä¸ªä»£è¡¨æ€§åŸºå‡†ä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒCaRF åœ¨ Ref LERFã€LERF OVS å’Œ 3D OVS æ•°æ®é›†ä¸Šï¼Œç›¸å¯¹äºŽæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹³å‡ mIoU åˆ†åˆ«æé«˜äº† 16.8%ã€4.3% å’Œ 2.0%ã€‚æ­¤å¤–ï¼Œè¿™é¡¹å·¥ä½œä¿ƒè¿›äº†æ›´å¯é å’Œè§†è§’ä¸€è‡´çš„ 3D åœºæ™¯ç†è§£ï¼Œå¹¶ä¸ºå…·èº« AIã€AR/VR äº¤äº’å’Œè‡ªä¸»æ„ŸçŸ¥å¸¦æ¥äº†æ½œåœ¨çš„å¥½å¤„ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰Referring 3Dé«˜æ–¯æº…å°„åˆ†å‰²æ–¹æ³•åœ¨å¤„ç†è·¨è§†è§’ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚å®ƒä»¬ä¾èµ–äºŽä»Ž2Dæ¸²æŸ“å›¾åƒèŽ·å¾—çš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯¼è‡´æ¨¡åž‹å­¦ä¹ åˆ°è§†è§’ç›¸å…³çš„ç‰¹å¾ï¼Œä»Žè€Œåœ¨ä¸åŒè§†è§’ä¸‹äº§ç”Ÿä¸ä¸€è‡´çš„åˆ†å‰²ç»“æžœã€‚è¿™ç§ä¸ä¸€è‡´æ€§é™åˆ¶äº†æ¨¡åž‹åœ¨å®žé™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCaRFçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨3Dé«˜æ–¯ç©ºé—´ä¸­ç›´æŽ¥è¿›è¡ŒæŽ¨ç†å’Œå­¦ä¹ ï¼Œé¿å…ä¾èµ–2Dæ¸²æŸ“çš„ä¼ªæ ‡ç­¾ã€‚é€šè¿‡å¼•å…¥ç›¸æœºæ„ŸçŸ¥çš„ç¼–ç æ–¹å¼ï¼Œå°†ç›¸æœºå‡ ä½•ä¿¡æ¯èžå…¥åˆ°é«˜æ–¯ç‰¹å¾ä¸­ï¼Œä»Žè€Œæ˜¾å¼åœ°å»ºæ¨¡è§†è§’ç›¸å…³çš„å˜åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨è®­ç»ƒæ—¶å¯¹é½ä¸åŒè§†è§’ä¸‹çš„é«˜æ–¯ç‰¹å¾ï¼Œè¿›ä¸€æ­¥å¢žå¼ºæ¨¡åž‹çš„å¤šè§†è§’ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šCaRFæ¡†æž¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šGaussian Field Camera Encoding (GFCE) å’Œ In Training Paired View Supervision (ITPVS)ã€‚GFCEæ¨¡å—å°†ç›¸æœºå‚æ•°ï¼ˆå¦‚ä½ç½®å’Œæ–¹å‘ï¼‰ç¼–ç åˆ°æ¯ä¸ªé«˜æ–¯ç²’å­çš„ç‰¹å¾ä¸­ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ„ŸçŸ¥è§†è§’ä¿¡æ¯ã€‚ITPVSæ¨¡å—åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯¹æ¥è‡ªä¸åŒè§†è§’çš„åŒä¸€é«˜æ–¯ç²’å­çš„é¢„æµ‹ç»“æžœè¿›è¡Œå¯¹é½ï¼Œä»Žè€Œå¢žå¼ºæ¨¡åž‹çš„å¤šè§†è§’ä¸€è‡´æ€§ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆä½¿ç”¨GFCEå¯¹é«˜æ–¯ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œç„¶åŽä½¿ç”¨ITPVSè¿›è¡Œè®­ç»ƒï¼Œæœ€ç»ˆå¾—åˆ°å…·æœ‰å¤šè§†è§’ä¸€è‡´æ€§çš„åˆ†å‰²ç»“æžœã€‚

**å…³é”®åˆ›æ–°**ï¼šCaRFçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶å®Œå…¨åœ¨3Dé«˜æ–¯ç©ºé—´ä¸­è¿›è¡ŒæŽ¨ç†å’Œå­¦ä¹ ï¼Œé¿å…äº†å¯¹2Dæ¸²æŸ“çš„ä¾èµ–ã€‚GFCEæ¨¡å—æ˜¾å¼åœ°å»ºæ¨¡äº†è§†è§’ç›¸å…³çš„å˜åŒ–ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£3Dåœºæ™¯çš„å‡ ä½•ä¿¡æ¯ã€‚ITPVSæ¨¡å—é€šè¿‡å¯¹é½ä¸åŒè§†è§’çš„é¢„æµ‹ç»“æžœï¼Œæœ‰æ•ˆåœ°å¢žå¼ºäº†æ¨¡åž‹çš„å¤šè§†è§’ä¸€è‡´æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCaRFèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åˆ†å‰²3Dåœºæ™¯ä¸­çš„ç›®æ ‡ç‰©ä½“ï¼Œå¹¶ä¸”åœ¨ä¸åŒè§†è§’ä¸‹å…·æœ‰æ›´é«˜çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šGFCEæ¨¡å—ä½¿ç”¨ä¸€ä¸ªå°åž‹ç¥žç»ç½‘ç»œå°†ç›¸æœºå‚æ•°ç¼–ç ä¸ºé«˜æ–¯ç‰¹å¾çš„é™„åŠ å‘é‡ã€‚ITPVSæ¨¡å—ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥å¯¹é½ä¸åŒè§†è§’ä¸‹çš„é«˜æ–¯é€»è¾‘å€¼ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºŽæ¯ä¸ªé«˜æ–¯ç²’å­ï¼Œä»Žä¸¤ä¸ªä¸åŒçš„è§†è§’æ¸²æŸ“å¾—åˆ°ä¸¤ä¸ªé€»è¾‘å€¼å‘é‡ï¼Œç„¶åŽè®¡ç®—è¿™ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„äº¤å‰ç†µæŸå¤±ï¼Œå¹¶å°†å…¶ä½œä¸ºè®­ç»ƒç›®æ ‡ä¹‹ä¸€ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ä½¿ç”¨äº†æ ‡å‡†çš„é«˜æ–¯æº…å°„æ¸²æŸ“æŠ€æœ¯å’Œè¯­è¨€ç¼–ç å™¨æ¥æå–æ–‡æœ¬ç‰¹å¾ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

CaRFåœ¨Ref LERFã€LERF OVSå’Œ3D OVSä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®žéªŒç»“æžœè¡¨æ˜Žï¼ŒCaRFæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒCaRFåœ¨Ref LERFæ•°æ®é›†ä¸Šå–å¾—äº†16.8%çš„mIoUæå‡ï¼Œåœ¨LERF OVSæ•°æ®é›†ä¸Šå–å¾—äº†4.3%çš„mIoUæå‡ï¼Œåœ¨3D OVSæ•°æ®é›†ä¸Šå–å¾—äº†2.0%çš„mIoUæå‡ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒCaRFèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜Referring 3Dåˆ†å‰²çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

CaRFåœ¨å…·èº«AIã€AR/VRäº¤äº’å’Œè‡ªä¸»æ„ŸçŸ¥ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨CaRFæ¥ç†è§£äººç±»çš„æŒ‡ä»¤ï¼Œå¹¶åœ¨3DçŽ¯å¢ƒä¸­å®šä½ç›®æ ‡ç‰©ä½“ã€‚åœ¨AR/VRåº”ç”¨ä¸­ï¼ŒCaRFå¯ä»¥ç”¨äºŽå¢žå¼ºç”¨æˆ·ä¸Žè™šæ‹ŸçŽ¯å¢ƒçš„äº¤äº’ä½“éªŒï¼Œä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è¯­éŸ³æŒ‡ä»¤æ¥æ“ä½œè™šæ‹Ÿç‰©ä½“ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒCaRFå¯ä»¥ç”¨äºŽè¯†åˆ«å’Œåˆ†å‰²é“è·¯ä¸Šçš„äº¤é€šæ ‡å¿—å’Œè¡Œäººï¼Œä»Žè€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR/VR interaction, and autonomous perception.

