---
layout: default
title: Learning from Online Videos at Inference Time for Computer-Use Agents
---

# Learning from Online Videos at Inference Time for Computer-Use Agents

**arXiv**: [2511.04137v1](https://arxiv.org/abs/2511.04137) | [PDF](https://arxiv.org/pdf/2511.04137.pdf)

**ä½œè€…**: Yujian Liu, Ze Wang, Hao Chen, Ximeng Sun, Xiaodong Yu, Jialian Wu, Jiang Liu, Emad Barsoum, Zicheng Liu, Shiyu Chang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»Žåœ¨çº¿è§†é¢‘ä¸­åŠ¨æ€å­¦ä¹ æ¡†æž¶ï¼Œä»¥æå‡è®¡ç®—æœºä½¿ç”¨ä»£ç†åœ¨æŽ¨ç†æ—¶çš„æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è®¡ç®—æœºä½¿ç”¨ä»£ç†` `åœ¨çº¿è§†é¢‘å­¦ä¹ ` `è½¨è¿¹åˆ†å‰²` `åŠ¨æ€é€‰æ‹©` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æŽ¨ç†æ—¶å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè®¡ç®—æœºä½¿ç”¨ä»£ç†ç¼ºä¹ç‰¹å®šé¢†åŸŸç¨‹åºçŸ¥è¯†ï¼Œéš¾ä»¥å¤„ç†å¤šæ­¥éª¤å·¥ä½œæµã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ£€ç´¢è§†é¢‘ã€è½¬æ¢ä¸ºç»“æž„åŒ–è½¨è¿¹ï¼Œå¹¶åŠ¨æ€é€‰æ‹©è½¨è¿¹ä½œä¸ºä¸Šä¸‹æ–‡æŒ‡å¯¼ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽä»…ä½¿ç”¨æ–‡æœ¬æ•™ç¨‹æˆ–è½¬å½•çš„åŸºçº¿ä»£ç†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Computer-use agents can operate computers and automate laborious tasks, but
> despite recent rapid progress, they still lag behind human users, especially
> when tasks require domain-specific procedural knowledge about particular
> applications, platforms, and multi-step workflows. Humans can bridge this gap
> by watching video tutorials: we search, skim, and selectively imitate short
> segments that match our current subgoal. In this paper, we study how to enable
> computer-use agents to learn from online videos at inference time effectively.
> We propose a framework that retrieves and filters tutorial videos, converts
> them into structured demonstration trajectories, and dynamically selects
> trajectories as in-context guidance during execution. Particularly, using a
> VLM, we infer UI actions, segment videos into short subsequences of actions,
> and assign each subsequence a textual objective. At inference time, a two-stage
> selection mechanism dynamically chooses a single trajectory to add in context
> at each step, focusing the agent on the most helpful local guidance for its
> next decision. Experiments on two widely used benchmarks show that our
> framework consistently outperforms strong base agents and variants that use
> only textual tutorials or transcripts. Analyses highlight the importance of
> trajectory segmentation and selection, action filtering, and visual
> information, suggesting that abundant online videos can be systematically
> distilled into actionable guidance that improves computer-use agents at
> inference time. Our code is available at
> https://github.com/UCSB-NLP-Chang/video_demo.

