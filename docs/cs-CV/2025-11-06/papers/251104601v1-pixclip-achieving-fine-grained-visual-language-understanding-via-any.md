---
layout: default
title: PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning
---

# PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning

**arXiv**: [2511.04601v1](https://arxiv.org/abs/2511.04601) | [PDF](https://arxiv.org/pdf/2511.04601.pdf)

**ä½œè€…**: Yicheng Xiao, Yu Chen, Haoxuan Ma, Jiale Hong, Caorui Li, Lingxiang Wu, Haiyun Guo, Jinqiao Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPixCLIPæ¡†æž¶ï¼Œé€šè¿‡åƒç´ çº§å¯¹é½å’Œé•¿æ–‡æœ¬å¤„ç†è§£å†³ç»†ç²’åº¦è§†è§‰è¯­è¨€ç†è§£é—®é¢˜**

**å…³é”®è¯**: `åƒç´ çº§å¯¹é½` `é•¿æ–‡æœ¬å¤„ç†` `è§†è§‰è¯­è¨€ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `ç»†ç²’åº¦å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPæ¨¡åž‹åœ¨ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½æ–¹é¢å—é™ï¼Œæ–‡æœ¬ç¼–ç å™¨æ— æ³•å¤„ç†é•¿æ–‡æœ¬åºåˆ—
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºLongGRITæ•°æ®é›†ï¼Œé‡‡ç”¨ä¸‰åˆ†æ”¯æ¡†æž¶å®žçŽ°ä»»æ„ç²’åº¦åƒç´ -æ–‡æœ¬å¯¹é½å­¦ä¹ 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åƒç´ çº§äº¤äº’å’Œé•¿æ–‡æœ¬å¤„ç†ä¸Šå–å¾—çªç ´ï¼Œè¾¾åˆ°å…ˆè¿›æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While the Contrastive Language-Image Pretraining(CLIP) model has achieved
> remarkable success in a variety of downstream vison language understanding
> tasks, enhancing its capability for fine-grained image-text alignment remains
> an active research focus. To this end, most existing works adopt the strategy
> of explicitly increasing the granularity of visual information processing,
> e.g., incorporating visual prompts to guide the model focus on specific local
> regions within the image. Meanwhile, researches on Multimodal Large Language
> Models(MLLMs) have demonstrated that training with long and detailed textual
> descriptions can effectively improve the model's fine-grained vision-language
> alignment. However, the inherent token length limitation of CLIP's text encoder
> fundamentally limits CLIP to process more granular textual information embedded
> in long text sequences. To synergistically leverage the advantages of enhancing
> both visual and textual content processing granularity, we propose PixCLIP, a
> novel framework designed to concurrently accommodate visual prompt inputs and
> process lengthy textual descriptions. Specifically, we first establish an
> automated annotation pipeline capable of generating pixel-level localized,
> long-form textual descriptions for images. Utilizing this pipeline, we
> construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
> samples. Secondly, we replace CLIP's original text encoder with the LLM and
> propose a three-branch pixel-text alignment learning framework, facilitating
> fine-grained alignment between image regions and corresponding textual
> descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
> showcases breakthroughs in pixel-level interaction and handling long-form
> texts, achieving state-of-the-art performance.

