---
layout: default
title: Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks
---

# Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks

**arXiv**: [2511.04494v1](https://arxiv.org/abs/2511.04494) | [PDF](https://arxiv.org/pdf/2511.04494.pdf)

**ä½œè€…**: Alper Kalle, Theo Rudkiewicz, Mohamed-Oumar Ouerfelli, Mohamed Tamaazousti

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å¸ƒæ„ŸçŸ¥å¼ é‡åˆ†è§£æ–¹æ³•ä»¥åŽ‹ç¼©å·ç§¯ç¥žç»ç½‘ç»œï¼Œæ— éœ€å¾®è°ƒä¿æŒé«˜ç²¾åº¦**

**å…³é”®è¯**: `ç¥žç»ç½‘ç»œåŽ‹ç¼©` `å¼ é‡åˆ†è§£` `æ•°æ®æ„ŸçŸ¥ä¼˜åŒ–` `å·ç§¯ç¥žç»ç½‘ç»œ` `ä½Žç§©è¿‘ä¼¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¥žç»ç½‘ç»œåŽ‹ç¼©ä¸­ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨æƒé‡ç©ºé—´å„å‘åŒæ€§èŒƒæ•°ï¼Œå¿½ç•¥æ•°æ®åˆ†å¸ƒå½±å“ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ•°æ®æ„ŸçŸ¥èŒƒæ•°ï¼Œæœ€å°åŒ–å±‚è¾“å‡ºåˆ†å¸ƒå˜åŒ–ï¼Œä¼˜åŒ–Tucker-2å’ŒCPDåˆ†è§£ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šä¸ªCNNå’Œæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œæ— éœ€å¾®è°ƒå³å¯å®žçŽ°ç«žäº‰æ€§ç²¾åº¦ï¼Œä¸”å¯è·¨æ•°æ®é›†è¿ç§»ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Neural networks are widely used for image-related tasks but typically demand
> considerable computing power. Once a network has been trained, however, its
> memory- and compute-footprint can be reduced by compression. In this work, we
> focus on compression through tensorization and low-rank representations.
> Whereas classical approaches search for a low-rank approximation by minimizing
> an isotropic norm such as the Frobenius norm in weight-space, we use
> data-informed norms that measure the error in function space. Concretely, we
> minimize the change in the layer's output distribution, which can be expressed
> as $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$ where $\Sigma^{1/2}$ is
> the square root of the covariance matrix of the layer's input and $W$,
> $\widetilde{W}$ are the original and compressed weights. We propose new
> alternating least square algorithms for the two most common tensor
> decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike
> conventional compression pipelines, which almost always require
> post-compression fine-tuning, our data-informed approach often achieves
> competitive accuracy without any fine-tuning. We further show that the same
> covariance-based norm can be transferred from one dataset to another with only
> a minor accuracy drop, enabling compression even when the original training
> dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50,
> and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100)
> confirm the advantages of the proposed method.

