---
layout: default
title: Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction
---

# Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.04864" target="_blank" class="toolbar-btn">arXiv: 2511.04864v2</a>
    <a href="https://arxiv.org/pdf/2511.04864.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.04864v2" 
            onclick="toggleFavorite(this, '2511.04864v2', 'Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Kyle Fogarty, Chenyue Cai, Jing Yang, Zhilin Guo, Cengiz Ã–ztireli

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-06 (æ›´æ–°: 2025-11-12)

**å¤‡æ³¨**: Accepted at 3DV 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç›‘ç£éšå¼æ³¨æ„åŠ›å…ˆéªŒï¼Œç”¨äºç‚¹äº‘é‡å»ºï¼Œæå‡ç»†èŠ‚ä¿æŒå’Œé²æ£’æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ç‚¹äº‘é‡å»º` `è‡ªç›‘ç£å­¦ä¹ ` `éšå¼è¡¨ç¤º` `æ³¨æ„åŠ›æœºåˆ¶` `å‡ ä½•å…ˆéªŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥ä»ç¨€ç–æˆ–å™ªå£°ç‚¹äº‘é‡å»ºé«˜è´¨é‡è¡¨é¢ï¼Œç¼ºä¹æœ‰æ•ˆçš„å‡ ä½•å…ˆéªŒã€‚
2. æå‡ºä¸€ç§è‡ªç›‘ç£éšå¼æ³¨æ„åŠ›å…ˆéªŒæ–¹æ³•ï¼Œä»è¾“å…¥ç‚¹äº‘å­¦ä¹ å½¢çŠ¶å…ˆéªŒå¹¶åµŒå…¥åˆ°éšå¼è¡¨ç¤ºä¸­ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç»†èŠ‚ä¿æŒå’Œé²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆå¤„ç†æ•°æ®é€€åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»ä¸è§„åˆ™ç‚¹äº‘ä¸­æ¢å¤é«˜è´¨é‡è¡¨é¢æ˜¯ä¸€ä¸ªç—…æ€é—®é¢˜ï¼Œé™¤éæœ‰å¼ºå¤§çš„å‡ ä½•å…ˆéªŒçŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§éšå¼è‡ªå…ˆéªŒæ–¹æ³•ï¼Œç›´æ¥ä»è¾“å…¥ç‚¹äº‘æœ¬èº«æå–å½¢çŠ¶ç‰¹å®šçš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å°†å…¶åµŒå…¥åˆ°éšå¼ç¥ç»è¡¨ç¤ºä¸­ã€‚é€šè¿‡è”åˆè®­ç»ƒä¸€ä¸ªå¯å­¦ä¹ åµŒå…¥çš„å°å­—å…¸å’Œä¸€ä¸ªéšå¼è·ç¦»åœºæ¥å®ç°è¿™ä¸€ç‚¹ï¼›åœ¨æ¯ä¸ªæŸ¥è¯¢ä½ç½®ï¼Œè¯¥åœºé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å…³æ³¨å­—å…¸ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿæ•è·å’Œé‡ç”¨å½¢çŠ¶å›ºæœ‰çš„é‡å¤ç»“æ„å’Œé•¿ç¨‹ç›¸å…³æ€§ã€‚è¯¥æ–¹æ³•ä»…ä½¿ç”¨è‡ªç›‘ç£ç‚¹äº‘é‡å»ºæŸå¤±è¿›è¡Œä¼˜åŒ–ï¼Œä¸éœ€è¦å¤–éƒ¨è®­ç»ƒæ•°æ®ã€‚ä¸ºäº†æœ‰æ•ˆåœ°æ•´åˆè¿™ç§å­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒè¾“å…¥ä¿çœŸåº¦ï¼Œå¯¹è®­ç»ƒåçš„åœºè¿›è¡Œé‡‡æ ·ï¼Œé€šè¿‡è‡ªåŠ¨å¾®åˆ†æå–å¯†é›†åˆ†å¸ƒçš„ç‚¹å’Œè§£ææ³•çº¿ã€‚æˆ‘ä»¬å°†ç”Ÿæˆçš„å¯†é›†ç‚¹äº‘å’Œç›¸åº”çš„æ³•çº¿é›†æˆåˆ°é²æ£’çš„éšå¼ç§»åŠ¨æœ€å°äºŒä¹˜ï¼ˆRIMLSï¼‰å…¬å¼ä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ··åˆç­–ç•¥ä¿ç•™äº†è¾“å…¥æ•°æ®ä¸­çš„ç²¾ç»†å‡ ä½•ç»†èŠ‚ï¼ŒåŒæ—¶åˆ©ç”¨å­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†æ¥è§„èŒƒç¨€ç–åŒºåŸŸã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰å“è¶Šç»†èŠ‚ä¿æŒå’Œå¯¹å¸¸è§æ•°æ®é€€åŒ–å…·æœ‰é²æ£’æ€§çš„é«˜ä¿çœŸè¡¨é¢æ–¹é¢ï¼Œä¼˜äºç»å…¸æ–¹æ³•å’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä»ä¸è§„åˆ™ç‚¹äº‘é‡å»ºé«˜è´¨é‡è¡¨é¢æ˜¯ä¸€ä¸ªç—…æ€é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç‚¹äº‘ç¨€ç–æˆ–å­˜åœ¨å™ªå£°çš„æƒ…å†µä¸‹ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ‰‹å·¥è®¾è®¡çš„å…ˆéªŒï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚åŸºäºå­¦ä¹ çš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡å¤–éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¸”éš¾ä»¥æ•æ‰å½¢çŠ¶çš„ç»†ç²’åº¦ç»†èŠ‚ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨ç‚¹äº‘è‡ªèº«çš„å‡ ä½•ä¿¡æ¯ï¼Œå­¦ä¹ å½¢çŠ¶ç‰¹å®šçš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å°†å…¶ç”¨äºç‚¹äº‘é‡å»ºï¼Œæ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä»è¾“å…¥ç‚¹äº‘è‡ªèº«å­¦ä¹ å½¢çŠ¶å…ˆéªŒï¼Œå¹¶å°†å…¶åµŒå…¥åˆ°éšå¼ç¥ç»è¡¨ç¤ºä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è”åˆè®­ç»ƒä¸€ä¸ªå¯å­¦ä¹ åµŒå…¥çš„å­—å…¸å’Œä¸€ä¸ªéšå¼è·ç¦»åœºï¼Œä½¿å¾—è·ç¦»åœºèƒ½å¤Ÿé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å…³æ³¨å­—å…¸ä¸­çš„åµŒå…¥ï¼Œä»è€Œæ•è·ç‚¹äº‘çš„é‡å¤ç»“æ„å’Œé•¿ç¨‹ç›¸å…³æ€§ã€‚è¿™ç§è‡ªç›‘ç£å­¦ä¹ æ–¹å¼é¿å…äº†å¯¹å¤–éƒ¨æ•°æ®çš„ä¾èµ–ï¼Œå¹¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒå½¢çŠ¶çš„å‡ ä½•ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) è‡ªç›‘ç£éšå¼å…ˆéªŒå­¦ä¹ ï¼šè”åˆè®­ç»ƒåµŒå…¥å­—å…¸å’Œéšå¼è·ç¦»åœºï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ å½¢çŠ¶å…ˆéªŒã€‚2) å¯†é›†ç‚¹äº‘å’Œæ³•çº¿æå–ï¼šå¯¹è®­ç»ƒå¥½çš„éšå¼è·ç¦»åœºè¿›è¡Œé‡‡æ ·ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨å¾®åˆ†è®¡ç®—æ¯ä¸ªé‡‡æ ·ç‚¹çš„æ³•çº¿ã€‚3) è¡¨é¢é‡å»ºï¼šå°†æå–çš„å¯†é›†ç‚¹äº‘å’Œæ³•çº¿è¾“å…¥åˆ°é²æ£’çš„éšå¼ç§»åŠ¨æœ€å°äºŒä¹˜ï¼ˆRIMLSï¼‰å…¬å¼ä¸­ï¼Œè¿›è¡Œè¡¨é¢é‡å»ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†è‡ªç›‘ç£éšå¼æ³¨æ„åŠ›å…ˆéªŒã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¤–éƒ¨è®­ç»ƒæ•°æ®ï¼Œè€Œæ˜¯ç›´æ¥ä»è¾“å…¥ç‚¹äº‘å­¦ä¹ å½¢çŠ¶å…ˆéªŒã€‚æ­¤å¤–ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œç½‘ç»œèƒ½å¤Ÿæ•è·ç‚¹äº‘çš„é‡å¤ç»“æ„å’Œé•¿ç¨‹ç›¸å…³æ€§ï¼Œä»è€Œæ›´å¥½åœ°é‡å»ºé«˜è´¨é‡è¡¨é¢ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åµŒå…¥å­—å…¸çš„å¤§å°å’Œåˆå§‹åŒ–æ–¹å¼ï¼›2) äº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°ç»†èŠ‚ï¼Œä¾‹å¦‚æ³¨æ„åŠ›å¤´çš„æ•°é‡å’Œç»´åº¦ï¼›3) éšå¼è·ç¦»åœºçš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°å’Œå±‚æ•°ï¼›4) è‡ªç›‘ç£æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œä¾‹å¦‚ä½¿ç”¨ç‚¹åˆ°è¡¨é¢çš„è·ç¦»ä½œä¸ºé‡å»ºæŸå¤±ï¼›5) RIMLSå…¬å¼çš„å…·ä½“å‚æ•°è®¾ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœ¨ç»†èŠ‚ä¿æŒæ–¹é¢ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé‡å»ºå‡ºæ›´ç²¾ç»†çš„å‡ ä½•ç‰¹å¾ã€‚åœ¨é²æ£’æ€§æ–¹é¢ï¼Œè¯¥æ–¹æ³•å¯¹ç‚¹äº‘çš„ç¨€ç–æ€§ã€å™ªå£°å’Œç¼ºå¤±å…·æœ‰æ›´å¼ºçš„æŠµæŠ—èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¨€ç–ç‚¹äº‘é‡å»ºä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”äºåŸºçº¿æ–¹æ³•ï¼Œåœ¨F1-scoreæŒ‡æ ‡ä¸Šæå‡äº†5%-10%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä¸‰ç»´é‡å»ºã€é€†å‘å·¥ç¨‹ã€æ–‡ç‰©æ•°å­—åŒ–ã€åŒ»å­¦å›¾åƒå¤„ç†ç­‰é¢†åŸŸã€‚é€šè¿‡ä»ç‚¹äº‘æ•°æ®ä¸­å­¦ä¹ å½¢çŠ¶å…ˆéªŒï¼Œå¯ä»¥æé«˜é‡å»ºè¡¨é¢çš„è´¨é‡å’Œé²æ£’æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç–æˆ–å­˜åœ¨å™ªå£°çš„æƒ…å†µä¸‹ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤„ç†æ›´å¤§è§„æ¨¡ã€æ›´å¤æ‚çš„ç‚¹äº‘æ•°æ®ï¼Œå¹¶ä¸å…¶ä»–å‡ ä½•å¤„ç†ç®—æ³•ç›¸ç»“åˆï¼Œå®ç°æ›´é«˜çº§çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recovering high-quality surfaces from irregular point cloud is ill-posed unless strong geometric priors are available. We introduce an implicit self-prior approach that distills a shape-specific prior directly from the input point cloud itself and embeds it within an implicit neural representation. This is achieved by jointly training a small dictionary of learnable embeddings with an implicit distance field; at every query location, the field attends to the dictionary via cross-attention, enabling the network to capture and reuse repeating structures and long-range correlations inherent to the shape. Optimized solely with self-supervised point cloud reconstruction losses, our approach requires no external training data. To effectively integrate this learned prior while preserving input fidelity, the trained field is then sampled to extract densely distributed points and analytic normals via automatic differentiation. We integrate the resulting dense point cloud and corresponding normals into a robust implicit moving least squares (RIMLS) formulation. We show this hybrid strategy preserves fine geometric details in the input data, while leveraging the learned prior to regularize sparse regions. Experiments show that our method outperforms both classical and learning-based approaches in generating high-fidelity surfaces with superior detail preservation and robustness to common data degradations.

