---
layout: default
title: Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm
---

# Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm

**arXiv**: [2511.04570v1](https://arxiv.org/abs/2511.04570) | [PDF](https://arxiv.org/pdf/2511.04570.pdf)

**ä½œè€…**: Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º'è§†é¢‘æ€ç»´'èŒƒå¼ï¼Œåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡åž‹ç»Ÿä¸€å¤šæ¨¡æ€æŽ¨ç†ã€‚**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `å¤šæ¨¡æ€æŽ¨ç†` `Sora-2æ¨¡åž‹` `VideoThinkBench` `è‡ªä¸€è‡´æ€§å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å›¾åƒå’Œæ–‡æœ¬åˆ†ç¦»èŒƒå¼æ— æ³•è¡¨ç¤ºåŠ¨æ€è¿‡ç¨‹ï¼Œé™åˆ¶å¤šæ¨¡æ€ç†è§£ä¸Žç”Ÿæˆã€‚
2. å¼•å…¥è§†é¢‘ç”Ÿæˆæ¨¡åž‹å¦‚Sora-2ï¼Œåœ¨ç»Ÿä¸€æ—¶åºæ¡†æž¶ä¸­æ¡¥æŽ¥è§†è§‰ä¸Žæ–‡æœ¬æŽ¨ç†ã€‚
3. åœ¨VideoThinkBenchè¯„ä¼°ä¸­ï¼ŒSora-2åœ¨è§†è§‰å’Œæ–‡æœ¬ä»»åŠ¡ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œæ”¯æŒè‡ªä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> "Thinking with Text" and "Thinking with Images" paradigm significantly
> improve the reasoning ability of large language models (LLMs) and Vision
> Language Models (VLMs). However, these paradigms have inherent limitations. (1)
> Images capture only single moments and fail to represent dynamic processes or
> continuous changes, and (2) The separation of text and vision as distinct
> modalities, hindering unified multimodal understanding and generation. To
> overcome these limitations, we introduce "Thinking with Video", a new paradigm
> that leverages video generation models, such as Sora-2, to bridge visual and
> textual reasoning in a unified temporal framework. To support this exploration,
> we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
> encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
> Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
> evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
> Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
> surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
> tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
> Furthermore, we systematically analyse the source of these abilities. We also
> find that self-consistency and in-context learning can improve Sora-2's
> performance. In summary, our findings demonstrate that the video generation
> model is the potential unified multimodal understanding and generation model,
> positions "thinking with video" as a unified multimodal reasoning paradigm.

