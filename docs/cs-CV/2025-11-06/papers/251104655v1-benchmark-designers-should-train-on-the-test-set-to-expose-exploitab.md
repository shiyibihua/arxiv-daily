---
layout: default
title: Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts
---

# Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts

**arXiv**: [2511.04655v1](https://arxiv.org/abs/2511.04655) | [PDF](https://arxiv.org/pdf/2511.04655.pdf)

**ä½œè€…**: Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºå‡†è¯Šæ–­ä¸ŽåŽ»åæ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€åŸºå‡†ä¸­éžè§†è§‰æ·å¾„é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŸºå‡†è¯„ä¼°` `éžè§†è§‰æ·å¾„` `æµ‹è¯•é›†åŽ‹åŠ›æµ‹è¯•` `è¿­ä»£åç½®å‰ªæž` `åŸºå‡†åŽ»å` `è§†è§‰ç†è§£è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€åŸºå‡†æ˜“è¢«æ¨¡åž‹åˆ©ç”¨éžè§†è§‰åè§å’Œè¯­è¨€å…ˆéªŒï¼Œè€ŒéžçœŸå®žè§†è§‰ç†è§£
2. é‡‡ç”¨æµ‹è¯•é›†åŽ‹åŠ›æµ‹è¯•å’Œè¿­ä»£åç½®å‰ªæžæ–¹æ³•ï¼Œè¯†åˆ«å¹¶è¿‡æ»¤é«˜åç½®æ ·æœ¬
3. åœ¨å¤šä¸ªåŸºå‡†ä¸ŠéªŒè¯ï¼Œåˆ›å»ºåŽ»åç‰ˆæœ¬ï¼Œé™ä½Žéžè§†è§‰å¯è§£æ€§å¹¶å¢žå¼ºè§†è§‰ç›²æ€§èƒ½å·®è·

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Robust benchmarks are crucial for evaluating Multimodal Large Language Models
> (MLLMs). Yet we find that models can ace many multimodal benchmarks without
> strong visual understanding, instead exploiting biases, linguistic priors, and
> superficial patterns. This is especially problematic for vision-centric
> benchmarks that are meant to require visual inputs. We adopt a diagnostic
> principle for benchmark design: if a benchmark can be gamed, it will be.
> Designers should therefore try to ``game'' their own benchmarks first, using
> diagnostic and debiasing procedures to systematically identify and mitigate
> non-visual biases. Effective diagnosis requires directly ``training on the test
> set'' -- probing the released test set for its intrinsic, exploitable patterns.
>   We operationalize this standard with two components. First, we diagnose
> benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.
> Our primary diagnostic tool involves fine-tuning a powerful Large Language
> Model via $k$-fold cross-validation on exclusively the non-visual, textual
> inputs of the test set to reveal shortcut performance and assign each sample a
> bias score $s(x)$. We complement this with a lightweight Random Forest-based
> diagnostic operating on hand-crafted features for fast, interpretable auditing.
> Second, we debias benchmarks by filtering high-bias samples using an
> ``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four
> benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive
> non-visual biases. As a case study, we apply our full framework to create
> VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider
> vision-blind performance gap than the original.

