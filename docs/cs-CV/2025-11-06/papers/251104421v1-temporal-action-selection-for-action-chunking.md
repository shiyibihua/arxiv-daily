---
layout: default
title: Temporal Action Selection for Action Chunking
---

# Temporal Action Selection for Action Chunking

**arXiv**: [2511.04421v1](https://arxiv.org/abs/2511.04421) | [PDF](https://arxiv.org/pdf/2511.04421.pdf)

**ä½œè€…**: Yueyang Weng, Xiaopeng Zhang, Yongjin Mu, Yingcong Zhu, Yanjie Li, Qi Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTemporal Action Selectorä»¥è§£å†³åŠ¨ä½œåˆ†å—ä¸­çš„ååº”æ€§ä¸è¶³é—®é¢˜**

**å…³é”®è¯**: `åŠ¨ä½œåˆ†å—` `å­¦ä¹ æ¼”ç¤º` `ååº”æ€§ä¼˜åŒ–` `å†³ç­–ä¸€è‡´æ€§` `è¿åŠ¨è¿žè´¯æ€§` `æ®‹å·®å¼ºåŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŠ¨ä½œåˆ†å—å‡å°‘å†³ç­–é¢‘çŽ‡ï¼Œå¯¼è‡´å¯¹å™ªå£°å’ŒåŠ¨æ€çŽ¯å¢ƒååº”æ€§ä¸‹é™
2. TASç¼“å­˜å¤šæ­¥é¢„æµ‹åŠ¨ä½œå—ï¼Œé€šè¿‡è½»é‡é€‰æ‹©å™¨åŠ¨æ€é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
3. å®žéªŒæ˜¾ç¤ºTASæ˜¾è‘—æå‡æˆåŠŸçŽ‡ï¼Œå¹¶å¢žå¼ºæ®‹å·®å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•ˆçŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Action chunking is a widely adopted approach in Learning from Demonstration
> (LfD). By modeling multi-step action chunks rather than single-step actions,
> action chunking significantly enhances modeling capabilities for human expert
> policies. However, the reduced decision frequency restricts the utilization of
> recent observations, degrading reactivity - particularly evident in the
> inadequate adaptation to sensor noise and dynamic environmental changes.
> Existing efforts to address this issue have primarily resorted to trading off
> reactivity against decision consistency, without achieving both. To address
> this limitation, we propose a novel algorithm, Temporal Action Selector (TAS),
> which caches predicted action chunks from multiple timesteps and dynamically
> selects the optimal action through a lightweight selector network. TAS achieves
> balanced optimization across three critical dimensions: reactivity, decision
> consistency, and motion coherence. Experiments across multiple tasks with
> diverse base policies show that TAS significantly improves success rates -
> yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a
> base policy with residual reinforcement learning (RL) substantially enhances
> training efficiency and elevates the performance plateau. Experiments in both
> simulation and physical robots confirm the method's efficacy.

