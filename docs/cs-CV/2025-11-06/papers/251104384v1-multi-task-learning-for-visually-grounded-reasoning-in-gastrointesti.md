---
layout: default
title: Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA
---

# Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA

**arXiv**: [2511.04384v1](https://arxiv.org/abs/2511.04384) | [PDF](https://arxiv.org/pdf/2511.04384.pdf)

**ä½œè€…**: Itbaan Safwan, Muhammad Annas Shaikh, Muhammad Haaris, Ramail Khan, Muhammad Atif Tahir

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šä»»åŠ¡æ¡†æž¶ä»¥æå‡èƒƒè‚ é“è§†è§‰é—®ç­”çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§**

**å…³é”®è¯**: `å¤šä»»åŠ¡å­¦ä¹ ` `è§†è§‰é—®ç­”` `åŒ»å­¦å›¾åƒåˆ†æž` `è§†è§‰å®šä½` `è§£é‡Šç”Ÿæˆ` `LoRAå¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŒ»å­¦è§†è§‰é—®ç­”ä¸­è§†è§‰æŽ¨ç†å’Œè§£é‡Šç”Ÿæˆä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨LoRAå¾®è°ƒFlorence-2æ¨¡åž‹ï¼Œé›†æˆå¤šä»»åŠ¡å­¦ä¹ 
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¤šä»»åŠ¡å­¦ä¹ æ˜¾è‘—ä¼˜äºŽå•ä»»åŠ¡åŸºçº¿ï¼Œæé«˜ç­”æ¡ˆå‡†ç¡®æ€§å’Œè§†è§‰å®šä½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present a multi-task framework for the MediaEval Medico 2025 challenge,
> leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
> answering (VQA), explanation generation, and visual grounding. The proposed
> system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
> learning, (2) a synthetically enriched explanation dataset offering structured
> medical reasoning, and (3) text-to-region pairs linking visual features with
> segmentation masks. This multi-task setup enables the model to jointly learn
> visual grounding, reasoning, and interpretation, producing responses that are
> both accurate and interpretable. Extensive evaluation demonstrates that our
> approach substantially improves over single-task baselines in both answer
> accuracy and visual localization, highlighting the effectiveness of grounded
> multi-task learning for medical VQA applications.

