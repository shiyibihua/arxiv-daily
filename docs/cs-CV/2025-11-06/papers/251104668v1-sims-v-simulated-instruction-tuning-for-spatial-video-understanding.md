---
layout: default
title: SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding
---

# SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding

**arXiv**: [2511.04668v1](https://arxiv.org/abs/2511.04668) | [PDF](https://arxiv.org/pdf/2511.04668.pdf)

**ä½œè€…**: Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSIMS-Væ¡†æž¶ï¼Œåˆ©ç”¨3Dæ¨¡æ‹Ÿå™¨ç”Ÿæˆç©ºé—´è§†é¢‘æ•°æ®ä»¥è§£å†³å¤šæ¨¡æ€æ¨¡åž‹ç©ºé—´æŽ¨ç†ä¸è¶³é—®é¢˜**

**å…³é”®è¯**: `ç©ºé—´è§†é¢‘ç†è§£` `æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ` `å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹` `ç©ºé—´æŽ¨ç†` `æŒ‡ä»¤å¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹åœ¨æ—¶ç©ºç©ºé—´æŽ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼ŒçœŸå®žè§†é¢‘æ•°æ®æ ‡æ³¨ç¨€ç¼ºæ˜¯ç“¶é¢ˆ
2. ä½¿ç”¨3Dæ¨¡æ‹Ÿå™¨ç”Ÿæˆç©ºé—´ä¸°å¯Œè§†é¢‘æ•°æ®ï¼Œé€šè¿‡ç³»ç»Ÿæ¶ˆèžè¯†åˆ«ä¸‰ç§å…³é”®é—®é¢˜ç±»åž‹
3. åœ¨7Bå‚æ•°æ¨¡åž‹ä¸Šä»…ç”¨25Kæ¨¡æ‹Ÿæ•°æ®å¾®è°ƒï¼Œè¶…è¶Šå¤§åŸºçº¿å¹¶åœ¨çœŸå®žåŸºå‡†ä¸Šè¡¨çŽ°ä¼˜å¼‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite impressive high-level video comprehension, multimodal language models
> struggle with spatial reasoning across time and space. While current spatial
> training approaches rely on real-world video data, obtaining diverse footage
> with precise spatial annotations remains a bottleneck. To alleviate this
> bottleneck, we present SIMS-V -- a systematic data-generation framework that
> leverages the privileged information of 3D simulators to create spatially-rich
> video training data for multimodal language models. Using this framework, we
> investigate which properties of simulated data drive effective real-world
> transfer through systematic ablations of question types, mixes, and scales. We
> identify a minimal set of three question categories (metric measurement,
> perspective-dependent reasoning, and temporal tracking) that prove most
> effective for developing transferable spatial intelligence, outperforming
> comprehensive coverage despite using fewer question types. These insights
> enable highly efficient training: our 7B-parameter video LLM fine-tuned on just
> 25K simulated examples outperforms the larger 72B baseline and achieves
> competitive performance with proprietary models on rigorous real-world spatial
> reasoning benchmarks. Our approach demonstrates robust generalization,
> maintaining performance on general video understanding while showing
> substantial improvements on embodied and real-world spatial tasks.

