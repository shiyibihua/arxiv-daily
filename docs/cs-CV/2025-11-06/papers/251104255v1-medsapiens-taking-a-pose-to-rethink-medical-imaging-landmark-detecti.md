---
layout: default
title: MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection
---

# MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection

**arXiv**: [2511.04255v1](https://arxiv.org/abs/2511.04255) | [PDF](https://arxiv.org/pdf/2511.04255.pdf)

**ä½œè€…**: Marawan Elbatel, Anbang Wang, Keyuan Liu, Kaouther Mouheb, Enrique Almar-Munoz, Lizhuo Lin, Yanqi Yang, Karim Lekadir, Xiaomeng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMedSapiensï¼Œå°†äººä½“å§¿æ€åŸºç¡€æ¨¡åž‹é€‚é…äºŽåŒ»å­¦å½±åƒè§£å‰–æ ‡å¿—æ£€æµ‹**

**å…³é”®è¯**: `åŒ»å­¦å½±åƒåˆ†æž` `è§£å‰–æ ‡å¿—æ£€æµ‹` `åŸºç¡€æ¨¡åž‹é€‚é…` `å§¿æ€ä¼°è®¡` `å¤šæ•°æ®é›†é¢„è®­ç»ƒ` `å°‘æ ·æœ¬å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŒ»å­¦å½±åƒè§£å‰–æ ‡å¿—æ£€æµ‹ä¾èµ–é¢†åŸŸç‰¹å®šæ¨¡åž‹ï¼Œæœªå……åˆ†åˆ©ç”¨äººä½“ä¸­å¿ƒåŸºç¡€æ¨¡åž‹æ½œåŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¤šæ•°æ®é›†é¢„è®­ç»ƒï¼Œå°†Sapiensæ¨¡åž‹é€‚é…åˆ°åŒ»å­¦å½±åƒï¼Œä¼˜åŒ–ç©ºé—´å®šä½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®žçŽ°SDRæœ€é«˜21.81%æå‡ï¼Œå¹¶åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­è¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper does not introduce a novel architecture; instead, it revisits a
> fundamental yet overlooked baseline: adapting human-centric foundation models
> for anatomical landmark detection in medical imaging. While landmark detection
> has traditionally relied on domain-specific models, the emergence of
> large-scale pre-trained vision models presents new opportunities. In this
> study, we investigate the adaptation of Sapiens, a human-centric foundation
> model designed for pose estimation, to medical imaging through multi-dataset
> pretraining, establishing a new state of the art across multiple datasets. Our
> proposed model, MedSapiens, demonstrates that human-centric foundation models,
> inherently optimized for spatial pose localization, provide strong priors for
> anatomical landmark detection, yet this potential has remained largely
> untapped. We benchmark MedSapiens against existing state-of-the-art models,
> achieving up to 5.26% improvement over generalist models and up to 21.81%
> improvement over specialist models in the average success detection rate (SDR).
> To further assess MedSapiens adaptability to novel downstream tasks with few
> annotations, we evaluate its performance in limited-data settings, achieving
> 2.69% improvement over the few-shot state of the art in SDR. Code and model
> weights are available at https://github.com/xmed-lab/MedSapiens .

