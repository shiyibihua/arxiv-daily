---
layout: default
title: Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment
---

# Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment

**arXiv**: [2511.04555v1](https://arxiv.org/abs/2511.04555) | [PDF](https://arxiv.org/pdf/2511.04555.pdf)

**ä½œè€…**: Tao Lin, Yilei Zhong, Yuxin Du, Jingjing Zhang, Jiting Liu, Yinxinyu Chen, Encheng Gu, Ziyan Liu, Hongyi Cai, Yanwen Zou, Lixing Zou, Zhaoye Zhou, Gen Li, Bo Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹Evo-1ï¼Œä»¥é™ä½Žè®¡ç®—æˆæœ¬å¹¶æå‡éƒ¨ç½²æ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `è½»é‡çº§æž¶æž„` `è·¨è°ƒåˆ¶æ‰©æ•£å˜æ¢å™¨` `ä¸¤é˜¶æ®µè®­ç»ƒ` `æœºå™¨äººæŽ§åˆ¶` `é«˜æ•ˆæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰VLAæ¨¡åž‹å‚æ•°åºžå¤§ã€ä¾èµ–æœºå™¨äººæ•°æ®é¢„è®­ç»ƒï¼Œå¯¼è‡´é«˜è®¡ç®—æˆæœ¬å’Œéƒ¨ç½²å›°éš¾ã€‚
2. Evo-1åŸºäºŽåŽŸç”ŸVLMï¼Œé‡‡ç”¨è·¨è°ƒåˆ¶æ‰©æ•£å˜æ¢å™¨å’Œä¼˜åŒ–é›†æˆæ¨¡å—ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒä¿æŒè¯­ä¹‰å¯¹é½ã€‚
3. åœ¨Meta-Worldå’ŒRoboTwinä¸Šè¶…è¶Šæœ€ä½³æ¨¡åž‹12.4%å’Œ6.9%ï¼ŒçœŸå®žä¸–ç•ŒæˆåŠŸçŽ‡78%ï¼ŒæŽ¨ç†é«˜æ•ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have emerged as a powerful framework that
> unifies perception, language, and control, enabling robots to perform diverse
> tasks through multimodal understanding. However, current VLA models typically
> contain massive parameters and rely heavily on large-scale robot data
> pretraining, leading to high computational costs during training, as well as
> limited deployability for real-time inference. Moreover, most training
> paradigms often degrade the perceptual representations of the vision-language
> backbone, resulting in overfitting and poor generalization to downstream tasks.
> In this work, we present Evo-1, a lightweight VLA model that reduces
> computation and improves deployment efficiency, while maintaining strong
> performance without pretraining on robot data. Evo-1 builds on a native
> multimodal Vision-Language model (VLM), incorporating a novel cross-modulated
> diffusion transformer along with an optimized integration module, together
> forming an effective architecture. We further introduce a two-stage training
> paradigm that progressively aligns action with perception, preserving the
> representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1
> achieves state-of-the-art results on the Meta-World and RoboTwin suite,
> surpassing the previous best models by 12.4% and 6.9%, respectively, and also
> attains a competitive result of 94.8% on LIBERO. In real-world evaluations,
> Evo-1 attains a 78% success rate with high inference frequency and low memory
> overhead, outperforming all baseline methods. We release code, data, and model
> weights to facilitate future research on lightweight and efficient VLA models.

