---
layout: default
title: Cambrian-S: Towards Spatial Supersensing in Video
---

# Cambrian-S: Towards Spatial Supersensing in Video

**arXiv**: [2511.04670v1](https://arxiv.org/abs/2511.04670) | [PDF](https://arxiv.org/pdf/2511.04670.pdf)

**ä½œè€…**: Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCambrian-Sæ¨¡åž‹ä¸ŽVSI-SUPERåŸºå‡†ä»¥æŽ¨åŠ¨è§†é¢‘ç©ºé—´è¶…æ„ŸçŸ¥èƒ½åŠ›**

**å…³é”®è¯**: `ç©ºé—´è¶…æ„ŸçŸ¥` `è§†é¢‘ç†è§£åŸºå‡†` `è‡ªç›‘ç£é¢„æµ‹` `é•¿è§†é¢‘è®°å¿†` `é¢„æµ‹å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰å¤šæ¨¡æ€ç³»ç»Ÿç¼ºä¹ç©ºé—´è¶…æ„ŸçŸ¥ï¼ŒåŒ…æ‹¬è¯­ä¹‰æ„ŸçŸ¥ã€äº‹ä»¶è®¤çŸ¥ã€3Dç©ºé—´æŽ¨ç†å’Œé¢„æµ‹å»ºæ¨¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥VSI-SUPERåŸºå‡†å’Œè‡ªç›‘ç£é¢„æµ‹å™¨ï¼Œåˆ©ç”¨é¢„æµ‹è¯¯å·®é©±åŠ¨è®°å¿†ä¸Žäº‹ä»¶åˆ†å‰²ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VSI-Benchä¸Šæå‡30%ï¼Œä½†VSI-SUPERè¡¨çŽ°æœ‰é™ï¼Œæ˜¾ç¤ºè§„æ¨¡ä¸è¶³éœ€é¢„æµ‹æ„ŸçŸ¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We argue that progress in true multimodal intelligence calls for a shift from
> reactive, task-driven systems and brute-force long context towards a broader
> paradigm of supersensing. We frame spatial supersensing as four stages beyond
> linguistic-only understanding: semantic perception (naming what is seen),
> streaming event cognition (maintaining memory across continuous experiences),
> implicit 3D spatial cognition (inferring the world behind pixels), and
> predictive world modeling (creating internal models that filter and organize
> information). Current benchmarks largely test only the early stages, offering
> narrow coverage of spatial cognition and rarely challenging models in ways that
> require true world modeling. To drive progress in spatial supersensing, we
> present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
> recall) and VSC (continual visual spatial counting). These tasks require
> arbitrarily long video inputs yet are resistant to brute-force context
> expansion. We then test data scaling limits by curating VSI-590K and training
> Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
> sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
> indicating that scale alone is insufficient for spatial supersensing. We
> propose predictive sensing as a path forward, presenting a proof-of-concept in
> which a self-supervised next-latent-frame predictor leverages surprise
> (prediction error) to drive memory and event segmentation. On VSI-SUPER, this
> approach substantially outperforms leading proprietary baselines, showing that
> spatial supersensing requires models that not only see but also anticipate,
> select, and organize experience.

