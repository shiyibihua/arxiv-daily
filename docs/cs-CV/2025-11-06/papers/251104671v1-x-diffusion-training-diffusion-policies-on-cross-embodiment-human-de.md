---
layout: default
title: X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations
---

# X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations

**arXiv**: [2511.04671v1](https://arxiv.org/abs/2511.04671) | [PDF](https://arxiv.org/pdf/2511.04671.pdf)

**ä½œè€…**: Maximus A. Pace, Prithwish Dan, Chuanruo Ning, Atiksh Bhardwaj, Audrey Du, Edward W. Duan, Wei-Chiu Ma, Kushal Kedia

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºX-Diffusionæ¡†æž¶ï¼Œåˆ©ç”¨è·¨å…·èº«äººç±»æ¼”ç¤ºè®­ç»ƒæ‰©æ•£ç­–ç•¥ä»¥è§£å†³åŠ¨ä½œæ‰§è¡Œä¸åŒ¹é…é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ‰©æ•£ç­–ç•¥` `è·¨å…·èº«å­¦ä¹ ` `äººç±»æ¼”ç¤º` `åŠ¨ä½œé‡å®šå‘` `æœºå™¨äººæ“ä½œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šäººç±»ä¸Žæœºå™¨äººå…·èº«å·®å¼‚å¯¼è‡´ç›´æŽ¥åŠ¨ä½œé‡å®šå‘äº§ç”Ÿç‰©ç†ä¸å¯è¡ŒåŠ¨ä½œã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å™ªå£°æ·»åŠ æ¨¡ç³Šä½Žå±‚æ‰§è¡Œå·®å¼‚ï¼Œä¿ç•™é«˜å±‚ä»»åŠ¡æŒ‡å¯¼ï¼Œè®­ç»ƒåˆ†ç±»å™¨æŽ§åˆ¶å™ªå£°æ°´å¹³ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨äº”ä¸ªæ“ä½œä»»åŠ¡ä¸­ï¼Œå¹³å‡æˆåŠŸçŽ‡æ¯”æœ€ä½³åŸºçº¿æé«˜16%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Human videos can be recorded quickly and at scale, making them an appealing
> source of training data for robot learning. However, humans and robots differ
> fundamentally in embodiment, resulting in mismatched action execution. Direct
> kinematic retargeting of human hand motion can therefore produce actions that
> are physically infeasible for robots. Despite these low-level differences,
> human demonstrations provide valuable motion cues about how to manipulate and
> interact with objects. Our key idea is to exploit the forward diffusion
> process: as noise is added to actions, low-level execution differences fade
> while high-level task guidance is preserved. We present X-Diffusion, a
> principled framework for training diffusion policies that maximally leverages
> human data without learning dynamically infeasible motions. X-Diffusion first
> trains a classifier to predict whether a noisy action is executed by a human or
> robot. Then, a human action is incorporated into policy training only after
> adding sufficient noise such that the classifier cannot discern its embodiment.
> Actions consistent with robot execution supervise fine-grained denoising at low
> noise levels, while mismatched human actions provide only coarse guidance at
> higher noise levels. Our experiments show that naive co-training under
> execution mismatches degrades policy performance, while X-Diffusion
> consistently improves it. Across five manipulation tasks, X-Diffusion achieves
> a 16% higher average success rate than the best baseline. The project website
> is available at https://portal-cornell.github.io/X-Diffusion/.

