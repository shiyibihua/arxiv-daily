---
layout: default
title: DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification
---

# DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification

**arXiv**: [2511.04281v1](https://arxiv.org/abs/2511.04281) | [PDF](https://arxiv.org/pdf/2511.04281.pdf)

**ä½œè€…**: Yujie Yang, Shuang Li, Jun Ye, Neng Dong, Fan Li, Huafeng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDinoGRLæ¡†æž¶ï¼Œåˆ©ç”¨DINOv2å­¦ä¹ æ­¥æ€ç‰¹å¾ä»¥è§£å†³è§†é¢‘å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«é—®é¢˜ã€‚**

**å…³é”®è¯**: `è¡Œäººé‡è¯†åˆ«` `è·¨æ¨¡æ€æ£€ç´¢` `æ­¥æ€ç‰¹å¾å­¦ä¹ ` `è§†é¢‘åºåˆ—åˆ†æž` `DINOv2åº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¿½è§†æ­¥æ€ç‰¹å¾ï¼Œéš¾ä»¥å»ºæ¨¡è·¨æ¨¡æ€è§†é¢‘åŒ¹é…çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆDINOv2è¯­ä¹‰å…ˆéªŒç”Ÿæˆè½®å»“ï¼Œå¹¶é€šè¿‡åŒå‘äº¤äº’å¢žå¼ºæ­¥æ€ä¸Žå¤–è§‚ç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨HITSZ-VCMå’ŒBUPTæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰å…ˆè¿›æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video-based Visible-Infrared person re-identification (VVI-ReID) aims to
> retrieve the same pedestrian across visible and infrared modalities from video
> sequences. Existing methods tend to exploit modality-invariant visual features
> but largely overlook gait features, which are not only modality-invariant but
> also rich in temporal dynamics, thus limiting their ability to model the
> spatiotemporal consistency essential for cross-modal video matching. To address
> these challenges, we propose a DINOv2-Driven Gait Representation Learning
> (DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn
> gait features complementary to appearance cues, facilitating robust
> sequence-level representations for cross-modal retrieval. Specifically, we
> introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which
> generates and enhances silhouette representations with general-purpose semantic
> priors from DINOv2 and jointly optimizes them with the ReID objective to
> achieve semantically enriched and task-adaptive gait feature learning.
> Furthermore, we develop a Progressive Bidirectional Multi-Granularity
> Enhancement (PBMGE) module, which progressively refines feature representations
> by enabling bidirectional interactions between gait and appearance streams
> across multiple spatial granularities, fully leveraging their complementarity
> to enhance global representations with rich local details and produce highly
> discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets
> demonstrate the superiority of our approach, significantly outperforming
> existing state-of-the-art methods.

