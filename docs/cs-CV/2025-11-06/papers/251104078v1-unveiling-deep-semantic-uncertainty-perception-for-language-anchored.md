---
layout: default
title: Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment
---

# Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment

**arXiv**: [2511.04078v1](https://arxiv.org/abs/2511.04078) | [PDF](https://arxiv.org/pdf/2511.04078.pdf)

**ä½œè€…**: Zehui Feng, Chenqi Zhang, Mingru Wang, Minuo Wei, Shiwei Cheng, Cuntai Guan, Ting Han

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBratrixæ¡†æž¶ï¼Œé€šè¿‡è¯­è¨€é”šå®šå¤šæ¨¡æ€å¯¹é½è§£å†³è§†è§‰-è„‘ä¿¡å·è¯­ä¹‰è§£ç æŒ‘æˆ˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹é½` `è„‘ä¿¡å·è§£ç ` `è¯­ä¹‰ä¸ç¡®å®šæ€§` `è¯­è¨€é”šå®š` `è§†è§‰-è„‘æŽ¥å£` `EEGæ£€ç´¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰ä¿¡å·ä¸Žè„‘ä¿¡å·å¯¹é½å—é™äºŽè¯­ä¹‰ç¼ºå¤±å’Œå™ªå£°ï¼Œå½±å“è§£ç é²æ£’æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è¯­è¨€é”šå®šè¯­ä¹‰çŸ©é˜µå’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨¡å—ï¼Œå®žçŽ°è§†è§‰-è¯­è¨€-è„‘å…±äº«ç©ºé—´å¯¹é½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨EEGã€MEGã€fMRIåŸºå‡†ä¸Šæå‡æ£€ç´¢ã€é‡å»ºå’Œæè¿°æ€§èƒ½ï¼ŒEEGæ£€ç´¢ä»»åŠ¡æå‡14.3%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI
> remains a fundamental challenge due to subject variability and the entangled
> nature of visual features. Existing approaches primarily align neural activity
> directly with visual embeddings, but visual-only representations often fail to
> capture latent semantic dimensions, limiting interpretability and deep
> robustness. To address these limitations, we propose Bratrix, the first
> end-to-end framework to achieve multimodal Language-Anchored Vision-Brain
> alignment. Bratrix decouples visual stimuli into hierarchical visual and
> linguistic semantic components, and projects both visual and brain
> representations into a shared latent space, enabling the formation of aligned
> visual-language and brain-language embeddings. To emulate human-like perceptual
> reliability and handle noisy neural signals, Bratrix incorporates a novel
> uncertainty perception module that applies uncertainty-aware weighting during
> alignment. By leveraging learnable language-anchored semantic matrices to
> enhance cross-modal correlations and employing a two-stage training strategy of
> single-modality pretraining followed by multimodal fine-tuning, Bratrix-M
> improves alignment precision. Extensive experiments on EEG, MEG, and fMRI
> benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and
> captioning performance compared to state-of-the-art methods, specifically
> surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.

