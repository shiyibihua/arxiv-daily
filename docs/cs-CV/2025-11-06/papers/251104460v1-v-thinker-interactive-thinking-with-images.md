---
layout: default
title: V-Thinker: Interactive Thinking with Images
---

# V-Thinker: Interactive Thinking with Images

**arXiv**: [2511.04460v1](https://arxiv.org/abs/2511.04460) | [PDF](https://arxiv.org/pdf/2511.04460.pdf)

**ä½œè€…**: Runqi Qiao, Qiuna Tan, Minghan Yang, Guanting Dong, Peiqing Yang, Shiqiang Lang, Enhui Wan, Xiaowan Wang, Yida Xu, Lan Yang, Chong Sun, Chen Li, Honggang Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºV-Thinkeré€šè¿‡å¼ºåŒ–å­¦ä¹ å®žçŽ°å›¾åƒäº¤äº’å¼æŽ¨ç†ï¼Œè§£å†³å¤šæ¨¡æ€æ¨¡åž‹è§†è§‰å·¥å…·å—é™é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡åž‹` `å›¾åƒäº¤äº’æŽ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®åˆæˆ` `è§†è§‰åŸºå‡†` `é•¿ç¨‹æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€æ¨¡åž‹åœ¨å›¾åƒäº¤äº’ä¸Žé•¿ç¨‹æŽ¨ç†ä¸­è§†è§‰å·¥å…·ç©ºé—´æœ‰é™ï¼Œä»»åŠ¡ç‰¹å®šè®¾è®¡çº¦æŸè¿›å±•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ•°æ®è¿›åŒ–é£žè½®è‡ªåŠ¨åˆæˆæ•°æ®é›†ï¼Œè§†è§‰æ¸è¿›è®­ç»ƒè¯¾ç¨‹ç»“åˆå¼ºåŒ–å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VTBenchåŸºå‡†ä¸Šä¼˜äºŽåŸºçº¿æ¨¡åž‹ï¼Œæå‡é€šç”¨ä¸Žäº¤äº’æŽ¨ç†æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Empowering Large Multimodal Models (LMMs) to deeply integrate image
> interaction with long-horizon reasoning capabilities remains a long-standing
> challenge in this field. Recent advances in vision-centric reasoning explore a
> promising "Thinking with Images" paradigm for LMMs, marking a shift from
> image-assisted reasoning to image-interactive thinking. While this milestone
> enables models to focus on fine-grained image regions, progress remains
> constrained by limited visual tool spaces and task-specific workflow designs.
> To bridge this gap, we present V-Thinker, a general-purpose multimodal
> reasoning assistant that enables interactive, vision-centric thinking through
> end-to-end reinforcement learning. V-Thinker comprises two key components: (1)
> a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies
> interactive reasoning datasets across three dimensions-diversity, quality, and
> difficulty; and (2) a Visual Progressive Training Curriculum that first aligns
> perception via point-level supervision, then integrates interactive reasoning
> through a two-stage reinforcement learning framework. Furthermore, we introduce
> VTBench, an expert-verified benchmark targeting vision-centric interactive
> reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently
> outperforms strong LMM-based baselines in both general and interactive
> reasoning scenarios, providing valuable insights for advancing
> image-interactive reasoning applications.

