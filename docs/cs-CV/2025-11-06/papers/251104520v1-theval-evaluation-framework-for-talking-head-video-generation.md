---
layout: default
title: THEval. Evaluation Framework for Talking Head Video Generation
---

# THEval. Evaluation Framework for Talking Head Video Generation

**arXiv**: [2511.04520v1](https://arxiv.org/abs/2511.04520) | [PDF](https://arxiv.org/pdf/2511.04520.pdf)

**ä½œè€…**: Nabyl Quignon, Baptiste Chopin, Yaohui Wang, Antitza Dantcheva

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTHEvalè¯„ä¼°æ¡†æž¶ä»¥è§£å†³è¯´è¯å¤´è§†é¢‘ç”Ÿæˆä¸­è¯„ä»·æŒ‡æ ‡ä¸è¶³çš„é—®é¢˜**

**å…³é”®è¯**: `è¯´è¯å¤´è§†é¢‘ç”Ÿæˆ` `è§†é¢‘è¯„ä¼°æ¡†æž¶` `å”‡åŒæ­¥è¯„ä¼°` `è‡ªç„¶æ€§åˆ†æž` `è´¨é‡æŒ‡æ ‡` `ç”Ÿæˆæ¨¡åž‹åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¯´è¯å¤´è§†é¢‘ç”Ÿæˆå¿«é€Ÿå‘å±•ï¼Œä½†çŽ°æœ‰è¯„ä»·æŒ‡æ ‡æœ‰é™ï¼Œæ— æ³•å…¨é¢è¯„ä¼°è´¨é‡ã€è‡ªç„¶æ€§å’ŒåŒæ­¥æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡åŒ…å«8ä¸ªæŒ‡æ ‡çš„æ¡†æž¶ï¼Œå¼ºè°ƒæ•ˆçŽ‡å’Œäººç±»åå¥½å¯¹é½ï¼Œåˆ†æžå¤´éƒ¨ã€å˜´éƒ¨å’Œçœ‰æ¯›çš„ç»†ç²’åº¦åŠ¨æ€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨17ä¸ªå…ˆè¿›æ¨¡åž‹ç”Ÿæˆçš„85,000ä¸ªè§†é¢‘ä¸Šæµ‹è¯•ï¼Œå‘çŽ°æ¨¡åž‹åœ¨å”‡åŒæ­¥è¡¨çŽ°å¥½ï¼Œä½†è¡¨è¾¾æ€§å’Œæ— ä¼ªå½±ç»†èŠ‚ç”Ÿæˆæœ‰æŒ‘æˆ˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video generation has achieved remarkable progress, with generated videos
> increasingly resembling real ones. However, the rapid advance in generation has
> outpaced the development of adequate evaluation metrics. Currently, the
> assessment of talking head generation primarily relies on limited metrics,
> evaluating general video quality, lip synchronization, and on conducting user
> studies. Motivated by this, we propose a new evaluation framework comprising 8
> metrics related to three dimensions (i) quality, (ii) naturalness, and (iii)
> synchronization. In selecting the metrics, we place emphasis on efficiency, as
> well as alignment with human preferences. Based on this considerations, we
> streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as
> well as face quality. Our extensive experiments on 85,000 videos generated by
> 17 state-of-the-art models suggest that while many algorithms excel in lip
> synchronization, they face challenges with generating expressiveness and
> artifact-free details. These videos were generated based on a novel real
> dataset, that we have curated, in order to mitigate bias of training data. Our
> proposed benchmark framework is aimed at evaluating the improvement of
> generative methods. Original code, dataset and leaderboards will be publicly
> released and regularly updated with new methods, in order to reflect progress
> in the field.

