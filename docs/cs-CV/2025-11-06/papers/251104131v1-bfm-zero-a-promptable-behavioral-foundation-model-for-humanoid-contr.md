---
layout: default
title: BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning
---

# BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning

**arXiv**: [2511.04131v1](https://arxiv.org/abs/2511.04131) | [PDF](https://arxiv.org/pdf/2511.04131.pdf)

**ä½œè€…**: Yitang Li, Zhengyi Luo, Tonghe Zhang, Cunxi Dai, Anssi Kanervisto, Andrea Tirinzoni, Haoyang Weng, Kris Kitani, Mateusz Guzek, Ahmed Touati, Alessandro Lazaric, Matteo Pirotta, Guanya Shi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBFM-Zeroæ¡†æž¶ï¼Œé€šè¿‡æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ å®žçŽ°äººå½¢æœºå™¨äººå¯æç¤ºè¡Œä¸ºåŸºç¡€æ¨¡åž‹**

**å…³é”®è¯**: `è¡Œä¸ºåŸºç¡€æ¨¡åž‹` `æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ ` `äººå½¢æœºå™¨äººæŽ§åˆ¶` `æ½œåœ¨è¡¨ç¤ºå­¦ä¹ ` `é›¶æ ·æœ¬æŽ¨ç†` `æ¨¡æ‹Ÿåˆ°çœŸå®žè¿ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å±€é™äºŽæ¨¡æ‹ŸçŽ¯å¢ƒæˆ–ç‰¹å®šä»»åŠ¡ï¼Œéš¾ä»¥ç»Ÿä¸€äººå½¢æœºå™¨äººæŽ§åˆ¶
2. æ–¹æ³•è¦ç‚¹ï¼šå­¦ä¹ å…±äº«æ½œåœ¨è¡¨ç¤ºï¼ŒåµŒå…¥åŠ¨ä½œã€ç›®æ ‡å’Œå¥–åŠ±ï¼Œæ”¯æŒé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æŽ¨ç†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žUnitree G1æœºå™¨äººä¸Šå®žçŽ°å¤šä»»åŠ¡æŽ§åˆ¶ï¼Œå¹¶é€šè¿‡æ¶ˆèžå®žéªŒéªŒè¯è®¾è®¡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Building Behavioral Foundation Models (BFMs) for humanoid robots has the
> potential to unify diverse control tasks under a single, promptable generalist
> policy. However, existing approaches are either exclusively deployed on
> simulated humanoid characters, or specialized to specific tasks such as
> tracking. We propose BFM-Zero, a framework that learns an effective shared
> latent representation that embeds motions, goals, and rewards into a common
> space, enabling a single policy to be prompted for multiple downstream tasks
> without retraining. This well-structured latent space in BFM-Zero enables
> versatile and robust whole-body skills on a Unitree G1 humanoid in the real
> world, via diverse inference methods, including zero-shot motion tracking, goal
> reaching, and reward optimization, and few-shot optimization-based adaptation.
> Unlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds
> upon recent advancements in unsupervised RL and Forward-Backward (FB) models,
> which offer an objective-centric, explainable, and smooth latent representation
> of whole-body motions. We further extend BFM-Zero with critical reward shaping,
> domain randomization, and history-dependent asymmetric learning to bridge the
> sim-to-real gap. Those key design choices are quantitatively ablated in
> simulation. A first-of-its-kind model, BFM-Zero establishes a step toward
> scalable, promptable behavioral foundation models for whole-body humanoid
> control.

