---
layout: default
title: Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection
---

# Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection

**arXiv**: [2511.04347v1](https://arxiv.org/abs/2511.04347) | [PDF](https://arxiv.org/pdf/2511.04347.pdf)

**ä½œè€…**: Sanjay Kumar, Tim Brophy, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤©æ°”è‡´ä¼ æ„Ÿå™¨é®æŒ¡å¯¹BEVFusionåœ¨3Dç‰©ä½“æ£€æµ‹ä¸­çš„å½±å“**

**å…³é”®è¯**: `3Dç‰©ä½“æ£€æµ‹` `BEVèžåˆ` `ä¼ æ„Ÿå™¨é®æŒ¡` `nuScenesæ•°æ®é›†` `å¤šæ¨¡æ€é›†æˆ` `çŽ¯å¢ƒé²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ æ„Ÿå™¨é®æŒ¡ï¼ˆå¦‚é›¾ã€éœ¾ï¼‰å¯¹BEVèžåˆæž¶æž„3Dæ£€æµ‹ç²¾åº¦çš„å½±å“æœªçŸ¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨BEVFusionæž¶æž„ï¼Œåˆ†æžç›¸æœºå’ŒLiDARåœ¨nuScenesæ•°æ®é›†ä¸Šçš„é®æŒ¡æ•ˆåº”ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç›¸æœºé®æŒ¡è‡´mAPé™41.3%ï¼ŒLiDARé®æŒ¡é™47.3%ï¼Œèžåˆæ—¶æ›´ä¾èµ–LiDARã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurate 3D object detection is essential for automated vehicles to navigate
> safely in complex real-world environments. Bird's Eye View (BEV)
> representations, which project multi-sensor data into a top-down spatial
> format, have emerged as a powerful approach for robust perception. Although
> BEV-based fusion architectures have demonstrated strong performance through
> multimodal integration, the effects of sensor occlusions, caused by
> environmental conditions such as fog, haze, or physical obstructions, on 3D
> detection accuracy remain underexplored. In this work, we investigate the
> impact of occlusions on both camera and Light Detection and Ranging (LiDAR)
> outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.
> Detection performance is measured using mean Average Precision (mAP) and the
> nuScenes Detection Score (NDS). Our results show that moderate camera
> occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is
> based only on the camera. On the other hand, LiDAR sharply drops in performance
> only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),
> with a severe impact on long-range detection. In fused settings, the effect
> depends on which sensor is occluded: occluding the camera leads to a minor 4.1%
> drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%
> drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task
> of 3D object detection. Our results highlight the need for future research into
> occlusion-aware evaluation methods and improved sensor fusion techniques that
> can maintain detection accuracy in the presence of partial sensor failure or
> degradation due to adverse environmental conditions.

