---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-11-06
---

# cs.CVï¼ˆ2025-11-06ï¼‰

ğŸ“Š å…± **17** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (11 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251104283v3-fastgs-training-3d-gaussian-splatting-in-100-seconds.html">FastGS: Training 3D Gaussian Splatting in 100 Seconds</a></td>
  <td>FastGSï¼šåŸºäºå¤šè§†è§’ä¸€è‡´æ€§çš„3Dé«˜æ–¯æº…å°„åŠ é€Ÿè®­ç»ƒæ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.04283v3" onclick="toggleFavorite(this, '2511.04283v3', 'FastGS: Training 3D Gaussian Splatting in 100 Seconds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251103992v1-carf-enhancing-multi-view-consistency-in-referring-3d-gaussian-splat.html">CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation</a></td>
  <td>CaRFï¼šé€šè¿‡å¢å¼ºå¤šè§†è§’ä¸€è‡´æ€§æ”¹è¿›Referring 3Dé«˜æ–¯æº…å°„åˆ†å‰²</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.03992v1" onclick="toggleFavorite(this, '2511.03992v1', 'CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251104388v1-bore-depth-self-supervised-monocular-depth-estimation-with-boundary-.html">BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems</a></td>
  <td>æå‡ºBoRe-Depthæ¨¡å‹ï¼Œåœ¨åµŒå…¥å¼ç³»ç»Ÿä¸Šå®ç°é«˜ç²¾åº¦ã€é«˜æ•ˆç‡çš„å•ç›®æ·±åº¦ä¼°è®¡ï¼Œå¹¶æå‡è¾¹ç•Œè´¨é‡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.04388v1" onclick="toggleFavorite(this, '2511.04388v1', 'BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251103988v1-simple-3d-pose-features-support-human-and-machine-social-scene-under.html">Simple 3D Pose Features Support Human and Machine Social Scene Understanding</a></td>
  <td>æå‡ºåŸºäº3Då§¿æ€ç‰¹å¾çš„äººæœºç¤¾äº¤åœºæ™¯ç†è§£æ–¹æ³•ï¼Œè¶…è¶Šç°æœ‰AIæ¨¡å‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.03988v1" onclick="toggleFavorite(this, '2511.03988v1', 'Simple 3D Pose Features Support Human and Machine Social Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251104595v1-unisplat-unified-spatio-temporal-fusion-via-3d-latent-scaffolds-for-.html">UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction</a></td>
  <td>UniSplatï¼šé€šè¿‡3Dæ½œåœ¨æ”¯æ¶å®ç°åŠ¨æ€é©¾é©¶åœºæ™¯çš„ç»Ÿä¸€æ—¶ç©ºèåˆé‡å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.04595v1" onclick="toggleFavorite(this, '2511.04595v1', 'UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251105623v1-registration-free-monitoring-of-unstructured-point-cloud-data-via-in.html">Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties</a></td>
  <td>æå‡ºä¸€ç§å…é…å‡†çš„ç‚¹äº‘æ•°æ®ç›‘æ§æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹3Dç‰©ä½“å‡ ä½•ç²¾åº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.05623v1" onclick="toggleFavorite(this, '2511.05623v1', 'Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251104864v2-self-supervised-implicit-attention-priors-for-point-cloud-reconstruc.html">Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction</a></td>
  <td>æå‡ºè‡ªç›‘ç£éšå¼æ³¨æ„åŠ›å…ˆéªŒï¼Œç”¨äºç‚¹äº‘é‡å»ºï¼Œæå‡ç»†èŠ‚ä¿æŒå’Œé²æ£’æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.04864v2" onclick="toggleFavorite(this, '2511.04864v2', 'Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251103943v3-temporal-zoom-networks-distance-regression-and-continuous-depth-for-.html">Temporal Zoom Networks: Distance Regression and Continuous Depth for Efficient Action Localization</a></td>
  <td>æå‡ºè¾¹ç•Œè·ç¦»å›å½’ä¸è‡ªé€‚åº”æ—¶é—´ç»†åŒ–ä»¥æå‡åŠ¨ä½œå®šä½æ•ˆç‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.03943v3" onclick="toggleFavorite(this, '2511.03943v3', 'Temporal Zoom Networks: Distance Regression and Continuous Depth for Efficient Action Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251104128v2-dmsort-an-efficient-parallel-maritime-multi-object-tracking-architec.html">DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms</a></td>
  <td>DMSORTï¼šä¸€ç§é«˜æ•ˆçš„å¹¶è¡Œæµ·äº‹å¤šç›®æ ‡è·Ÿè¸ªæ¶æ„ï¼Œé€‚ç”¨äºæ— äººèˆ¹å¹³å°</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.04128v2" onclick="toggleFavorite(this, '2511.04128v2', 'DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251103970v1-room-envelopes-a-synthetic-dataset-for-indoor-layout-reconstruction-.html">Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images</a></td>
  <td>æå‡ºRoom Envelopesæ•°æ®é›†ï¼Œç”¨äºå›¾åƒå®¤å†…å¸ƒå±€é‡å»ºï¼Œæå‡åœºæ™¯ç†è§£èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.03970v1" onclick="toggleFavorite(this, '2511.03970v1', 'Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251103950v1-improving-multi-view-reconstruction-via-texture-guided-gaussian-mesh.html">Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization</a></td>
  <td>æå‡ºçº¹ç†å¼•å¯¼çš„é«˜æ–¯-ç½‘æ ¼è”åˆä¼˜åŒ–æ–¹æ³•ï¼Œæå‡å¤šè§†è§’é‡å»ºè´¨é‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.03950v1" onclick="toggleFavorite(this, '2511.03950v1', 'Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/251104281v1-dinov2-driven-gait-representation-learning-for-video-based-visible-i.html">DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification</a></td>
  <td>æå‡ºDinoGRLæ¡†æ¶ï¼Œåˆ©ç”¨DINOv2é©±åŠ¨çš„æ­¥æ€ç‰¹å¾å­¦ä¹ ï¼Œè§£å†³è§†é¢‘å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.04281v1" onclick="toggleFavorite(this, '2511.04281v1', 'DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251104797v1-3d-gaussian-point-encoders.html">3D Gaussian Point Encoders</a></td>
  <td>æå‡ºåŸºäº3Dé«˜æ–¯ç‚¹ç¼–ç å™¨çš„ç‚¹äº‘è¡¨ç¤ºæ–¹æ³•ï¼ŒåŠ é€Ÿ3Dè¯†åˆ«ä»»åŠ¡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.04797v1" onclick="toggleFavorite(this, '2511.04797v1', '3D Gaussian Point Encoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251105609v1-walking-the-schrÃ¶dinger-bridge-a-direct-trajectory-for-text-to-3d-ge.html">Walking the SchrÃ¶dinger Bridge: A Direct Trajectory for Text-to-3D Generation</a></td>
  <td>æå‡ºåŸºäºSchrÃ¶dingeræ¡¥çš„ç›´æ¥è½¨è¿¹ä»¥è§£å†³æ–‡æœ¬åˆ°3Dç”Ÿæˆä¸­çš„ä¼ªå½±é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.05609v1" onclick="toggleFavorite(this, '2511.05609v1', 'Walking the SchrÃ¶dinger Bridge: A Direct Trajectory for Text-to-3D Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251104779v1-eetnet-a-cnn-for-gaze-detection-and-tracking-for-smart-eyewear.html">EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear</a></td>
  <td>EETnetï¼šä¸ºæ™ºèƒ½çœ¼é•œè®¾è®¡çš„åŸºäºäº‹ä»¶çš„ä½åŠŸè€—æ³¨è§†æ£€æµ‹ä¸è·Ÿè¸ªCNN</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.04779v1" onclick="toggleFavorite(this, '2511.04779v1', 'EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251104029v3-faithful-contouring-near-lossless-3d-voxel-representation-free-from-.html">Faithful Contouring: Near-Lossless 3D Voxel Representation Free from Iso-surface</a></td>
  <td>æå‡º Faithful Contouringï¼Œå®ç°è¿‘ä¹æ— æŸçš„3Dä½“ç´ è¡¨ç¤ºï¼Œæ— éœ€ç­‰å€¼é¢æå–ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.04029v3" onclick="toggleFavorite(this, '2511.04029v3', 'Faithful Contouring: Near-Lossless 3D Voxel Representation Free from Iso-surface')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/251104394v1-doraemon-a-unified-library-for-visual-object-modeling-and-representa.html">DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale</a></td>
  <td>DORAEMONï¼šä¸€ä¸ªç”¨äºå¤§è§„æ¨¡è§†è§‰å¯¹è±¡å»ºæ¨¡å’Œè¡¨å¾å­¦ä¹ çš„ç»Ÿä¸€åº“</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.04394v1" onclick="toggleFavorite(this, '2511.04394v1', 'DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)