---
layout: default
title: SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models
---

# SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21843" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.21843v1</a>
  <a href="https://arxiv.org/pdf/2509.21843.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21843v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21843v1', 'SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jingkai Guo, Chaitali Chakrabarti, Deliang Fan

**ÂàÜÁ±ª**: cs.CR, cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-26

**Â§áÊ≥®**: 10 pages, 4 figures, 5 tables, 2 equations. Topics: Bit-flip attacks, adversarial attacks, large language models (LLMs)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SBFAÔºöÂçïÊØîÁâπÁøªËΩ¨ÊîªÂáªÁ†¥Ëß£Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÊè≠Á§∫‰∏•ÈáçÂÆâÂÖ®ÈöêÊÇ£**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÊØîÁâπÁøªËΩ¨ÊîªÂáª` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Ê®°ÂûãÂÆâÂÖ®` `ÂØπÊäóÊîªÂáª` `ÂèÇÊï∞ÊïèÊÑüÊÄßÂàÜÊûê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊØîÁâπÁøªËΩ¨ÊîªÂáªÔºàBFAÔºâÊñπÊ≥ïÁº∫‰πèÁÅµÊ¥ªÊÄßÔºåÈÄöÂ∏∏‰ªÖÈíàÂØπÊï¥Êï∞ÊàñÊµÆÁÇπÊ®°ÂûãÔºå‰∏îÂú®ÊµÆÁÇπÊ®°Âûã‰∏≠ÂÆπÊòì‰∫ßÁîüÊï∞ÂÄºÈîôËØØ„ÄÇ
2. SBFAÈÄöËøáËø≠‰ª£ÊêúÁ¥¢ÂíåImpactScoreÊåáÊ†áÔºåÂú®‰øùÊåÅÂèÇÊï∞ÂàÜÂ∏ÉËâØÊÄßÁöÑÂâçÊèê‰∏ãÔºå‰ªÖÈúÄÂçïÊØîÁâπÁøªËΩ¨Âç≥ÂèØÁ†¥ÂùèLLMÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSBFAÂú®Qwen„ÄÅLLaMAÂíåGemmaÁ≠âÊ®°Âûã‰∏äÔºå‰ªÖÈúÄÂçïÊØîÁâπÁøªËΩ¨Âç≥ÂèØÂ∞ÜÂáÜÁ°ÆÁéáÈôçËá≥ÈöèÊú∫Ê∞¥Âπ≥‰ª•‰∏ãÔºåÂá∏ÊòæÂÆâÂÖ®È£éÈô©„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂ§ßËßÑÊ®°Âú®Á∫øÈÉ®ÁΩ≤ÔºåÂÖ∂Ê®°ÂûãÂÆåÊï¥ÊÄßÂ∑≤Êàê‰∏∫‰∏Ä‰∏™Á¥ßËø´ÁöÑÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇÂÖàÂâçÁöÑÊØîÁâπÁøªËΩ¨ÊîªÂáªÔºàBFAsÔºâ‰Ωú‰∏∫‰∏ÄÁßçÊµÅË°åÁöÑAIÊùÉÈáçÂÜÖÂ≠òÊïÖÈöúÊ≥®ÂÖ•ÊäÄÊúØÔºåÂèØ‰ª•‰∏•ÈáçÊçüÂÆ≥Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÔºàDNNsÔºâ„ÄÇÂç≥‰ΩøÂè™ÊúâÂá†ÂçÅ‰∏™ÊØîÁâπÁøªËΩ¨Ôºå‰πüËÉΩ‰ΩøÂáÜÁ°ÆÁéáÈôç‰ΩéÂà∞ÈöèÊú∫ÁåúÊµãÁöÑÊ∞¥Âπ≥„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Â∞ÜBFAsÊâ©Â±ïÂà∞LLMsÔºåÂπ∂Êè≠Á§∫Â∞ΩÁÆ°Áõ¥Ëßâ‰∏äËÆ§‰∏∫Ê®°ÂùóÂåñÂíåÂÜó‰Ωô‰ºöÂ∏¶Êù•Êõ¥Â•ΩÁöÑÈ≤ÅÊ£íÊÄßÔºå‰ΩÜÂ∞ëÈáèÁöÑÂØπÊäóÊÄßÊØîÁâπÁøªËΩ¨‰πü‰ºöÂØºËá¥LLMsÁöÑÁÅæÈöæÊÄßÁ≤æÂ∫¶‰∏ãÈôç„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑBFAÊñπÊ≥ïÈÄöÂ∏∏ÂàÜÂà´ÂÖ≥Ê≥®Êï¥Êï∞ÊàñÊµÆÁÇπÊ®°ÂûãÔºåÈôêÂà∂‰∫ÜÊîªÂáªÁöÑÁÅµÊ¥ªÊÄß„ÄÇÊ≠§Â§ñÔºåÂú®ÊµÆÁÇπÊ®°Âûã‰∏≠ÔºåÈöèÊú∫ÊØîÁâπÁøªËΩ¨ÈÄöÂ∏∏‰ºöÂØºËá¥Êâ∞Âä®ÂèÇÊï∞ËææÂà∞ÊûÅÁ´ØÂÄºÔºà‰æãÂ¶ÇÔºåÂú®ÊåáÊï∞‰ΩçÁøªËΩ¨ÔºâÔºå‰ΩøÂÖ∂‰∏çÈöêËîΩÂπ∂ÂØºËá¥Êï∞ÂÄºËøêË°åÊó∂ÈîôËØØÔºà‰æãÂ¶ÇÔºåÊó†ÊïàÁöÑÂº†ÈáèÂÄºÔºàNaN/InfÔºâÔºâ„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨È¶ñÊ¨°ÊèêÂá∫‰∫ÜSBFAÔºàSneaky Bit-Flip AttackÔºâÔºåÂÆÉ‰ªÖÈÄöËøáÂçï‰∏™ÊØîÁâπÁøªËΩ¨Â∞±ËÉΩ‰ΩøLLMÊÄßËÉΩÂ¥©Ê∫ÉÔºåÂêåÊó∂‰øùÊåÅÊâ∞Âä®ÂÄºÂú®ËâØÊÄßÁöÑÈÄêÂ±ÇÊùÉÈáçÂàÜÂ∏ÉËåÉÂõ¥ÂÜÖ„ÄÇËøôÊòØÈÄöËøáËø≠‰ª£ÊêúÁ¥¢ÂíåÈÄöËøáÊàë‰ª¨ÂÆö‰πâÁöÑÂèÇÊï∞ÊïèÊÑüÊÄßÊåáÊ†áImpactScoreËøõË°åÊéíÂ∫èÊù•ÂÆûÁé∞ÁöÑÔºåËØ•ÊåáÊ†áÁªìÂêà‰∫ÜÊ¢ØÂ∫¶ÊïèÊÑüÊÄßÂíåÂèóËâØÊÄßÈÄêÂ±ÇÊùÉÈáçÂàÜÂ∏ÉÁ∫¶ÊùüÁöÑÊâ∞Âä®ËåÉÂõ¥„ÄÇËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËΩªÈáèÁ∫ßSKIPÊêúÁ¥¢ÁÆóÊ≥ïÔºå‰ª•Â§ßÂ§ßÈôç‰ΩéÊêúÁ¥¢Â§çÊùÇÂ∫¶Ôºå‰ªéËÄå‰ΩøSOTA LLMÁöÑSBFAÊêúÁ¥¢‰ªÖÈúÄËä±Ë¥πÊï∞ÂçÅÂàÜÈíüÂç≥ÂèØÊàêÂäü„ÄÇÂú®Qwen„ÄÅLLaMAÂíåGemmaÊ®°Âûã‰∏≠Ôºå‰ªÖÈÄöËøáÂçï‰∏™ÊØîÁâπÁøªËΩ¨ÔºåSBFAÂ∞±ËÉΩÊàêÂäüÂú∞Â∞ÜBF16ÂíåINT8Êï∞ÊçÆÊ†ºÂºèÁöÑMMLUÂíåSST-2ÁöÑÂáÜÁ°ÆÁéáÈôç‰ΩéÂà∞ÈöèÊú∫Ê∞¥Âπ≥‰ª•‰∏ã„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂú®Êï∞ÂçÅ‰∫ø‰∏™ÂèÇÊï∞‰∏≠ÁøªËΩ¨Âçï‰∏™ÊØîÁâπÊè≠Á§∫‰∫ÜSOTA LLMÊ®°ÂûãÁöÑ‰∏•ÈáçÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊØîÁâπÁøªËΩ¨ÊîªÂáªÊñπÊ≥ïÂú®ÊîªÂáªÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊó∂Â≠òÂú®ÁöÑÂ±ÄÈôêÊÄß„ÄÇÁé∞ÊúâÊñπÊ≥ïË¶Å‰πàÂè™ËÉΩÂ§ÑÁêÜÁâπÂÆöÊï∞ÊçÆÁ±ªÂûãÁöÑÊ®°ÂûãÔºàÊï¥Êï∞ÊàñÊµÆÁÇπÊï∞ÔºâÔºåË¶Å‰πàÂú®ÊµÆÁÇπÊï∞Ê®°Âûã‰∏äËøõË°åÊîªÂáªÊó∂ÂÆπÊòìÂØºËá¥ÂèÇÊï∞ÂÄºË∂ÖÂá∫Ê≠£Â∏∏ËåÉÂõ¥ÔºåÂºïÂèëÊï∞ÂÄºÈîôËØØÔºå‰ªéËÄåÊö¥Èú≤ÊîªÂáªË°å‰∏∫„ÄÇËøô‰∫õÈóÆÈ¢òÈôêÂà∂‰∫ÜÊîªÂáªÁöÑÈöêËîΩÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂØªÊâæÂØπÊ®°ÂûãÊÄßËÉΩÂΩ±ÂìçÊúÄÂ§ßÔºåÂêåÊó∂Âèà‰∏ç‰ºöÊòæËëóÊîπÂèòÂèÇÊï∞ÂàÜÂ∏ÉÁöÑÂçï‰∏™ÊØîÁâπ‰ΩçËøõË°åÁøªËΩ¨„ÄÇËøôÁßç‚ÄúÈöêËîΩ‚ÄùÁöÑÊîªÂáªÊñπÂºèÊó®Âú®ÈÅøÂÖçËß¶ÂèëÊ®°ÂûãÁöÑÈò≤Âæ°Êú∫Âà∂ÊàñÂØºËá¥Êï∞ÂÄºËÆ°ÁÆóÈîôËØØ„ÄÇÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑÂèÇÊï∞ÊïèÊÑüÊÄßÊåáÊ†áÂíåÊêúÁ¥¢ÁÆóÊ≥ïÔºåÂèØ‰ª•Âú®Â§ßÈáèÂèÇÊï∞‰∏≠È´òÊïàÂú∞ÊâæÂà∞Ëøô‰∏™ÂÖ≥ÈîÆÊØîÁâπ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSBFAÊîªÂáªÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) **ÂèÇÊï∞ÊïèÊÑüÊÄßËØÑ‰º∞**Ôºö‰ΩøÁî®ImpactScoreÊåáÊ†áËØÑ‰º∞ÊØè‰∏™ÂèÇÊï∞ÁöÑÊïèÊÑüÊÄßÔºåËØ•ÊåáÊ†áÁªìÂêà‰∫ÜÊ¢ØÂ∫¶ÊïèÊÑüÊÄßÂíåÊâ∞Âä®ËåÉÂõ¥Á∫¶Êùü„ÄÇ2) **ÂÄôÈÄâÊØîÁâπ‰ΩçÈÄâÊã©**ÔºöÊ†πÊçÆImpactScoreÈÄâÊã©ÊΩúÂú®ÁöÑÊîªÂáªÁõÆÊ†áÊØîÁâπ‰Ωç„ÄÇ3) **SKIPÊêúÁ¥¢ÁÆóÊ≥ï**ÔºöÂà©Áî®ËΩªÈáèÁ∫ßÁöÑSKIPÊêúÁ¥¢ÁÆóÊ≥ïÔºåÂú®ÂÄôÈÄâÊØîÁâπ‰Ωç‰∏≠È´òÊïàÂú∞ÊêúÁ¥¢ÊúÄ‰Ω≥ÊîªÂáªÊØîÁâπ„ÄÇ4) **ÊîªÂáªÂÆûÊñΩ**ÔºöÁøªËΩ¨ÈÄâÂÆöÁöÑÊØîÁâπ‰ΩçÔºåÂπ∂ËØÑ‰º∞Ê®°ÂûãÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSBFAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) **ÂçïÊØîÁâπÁøªËΩ¨ÊîªÂáª**Ôºö‰ªÖÈúÄÁøªËΩ¨Âçï‰∏™ÊØîÁâπÂç≥ÂèØÊòæËëóÈôç‰ΩéÊ®°ÂûãÊÄßËÉΩÔºåÊèêÈ´ò‰∫ÜÊîªÂáªÁöÑÈöêËîΩÊÄß„ÄÇ2) **ImpactScoreÊåáÊ†á**ÔºöÁªìÂêàÊ¢ØÂ∫¶ÊïèÊÑüÊÄßÂíåÊâ∞Âä®ËåÉÂõ¥Á∫¶ÊùüÔºåÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞ÂèÇÊï∞ÁöÑÊïèÊÑüÊÄß„ÄÇ3) **SKIPÊêúÁ¥¢ÁÆóÊ≥ï**ÔºöÊòæËëóÈôç‰Ωé‰∫ÜÊêúÁ¥¢Â§çÊùÇÂ∫¶Ôºå‰ΩøÂæóÂú®Â§ßÂûãÊ®°Âûã‰∏äËøõË°åÊîªÂáªÊàê‰∏∫ÂèØËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöImpactScoreÊåáÊ†áÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆ„ÄÇÂÆÉÁªºÂêàËÄÉËôë‰∫ÜÊ¢ØÂ∫¶‰ø°ÊÅØÔºàÂèçÊò†ÂèÇÊï∞ÂØπÊ®°ÂûãËæìÂá∫ÁöÑÂΩ±ÂìçÔºâÂíåÂèÇÊï∞Êâ∞Âä®ËåÉÂõ¥ÔºàÁ°Æ‰øùÁøªËΩ¨ÂêéÁöÑÂèÇÊï∞ÂÄº‰ªçÂú®ÂêàÁêÜÁöÑÂàÜÂ∏ÉËåÉÂõ¥ÂÜÖÔºâ„ÄÇSKIPÊêúÁ¥¢ÁÆóÊ≥ïÈÄöËøáË∑≥Ëøá‰∏çÈáçË¶ÅÁöÑÊØîÁâπ‰ΩçÔºåÂáèÂ∞ë‰∫ÜÊêúÁ¥¢Á©∫Èó¥„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÈíàÂØπBF16ÂíåINT8Á≠â‰∏çÂêåÊï∞ÊçÆÊ†ºÂºèËøõË°å‰∫Ü‰ºòÂåñÔºåÁ°Æ‰øùÊîªÂáªÁöÑÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSBFAÂú®Qwen„ÄÅLLaMAÂíåGemmaÁ≠âSOTA LLMÊ®°Âûã‰∏äÔºå‰ªÖÈúÄÁøªËΩ¨Âçï‰∏™ÊØîÁâπÔºåÂ∞±ËÉΩÂ∞ÜMMLUÂíåSST-2Êï∞ÊçÆÈõÜ‰∏äÁöÑÂáÜÁ°ÆÁéáÈôç‰ΩéÂà∞ÈöèÊú∫Ê∞¥Âπ≥‰ª•‰∏ã„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∫õÊ®°Âûã‰∏äÔºåÂáÜÁ°ÆÁéá‰ªéÂéüÊú¨ÁöÑÈ´òÊ∞¥Âπ≥Áõ¥Êé•ÈôçËá≥10%‰ª•‰∏ãÔºåËØÅÊòé‰∫ÜSBFAÊîªÂáªÁöÑÊúâÊïàÊÄßÂíåLLMÊ®°ÂûãÊΩúÂú®ÁöÑÂÆâÂÖ®È£éÈô©„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SBFAÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËØÑ‰º∞ÂíåÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß„ÄÇÈÄöËøáÊ®°ÊãüËøôÁßçÊîªÂáªÔºåÂèØ‰ª•ÂèëÁé∞Ê®°Âûã‰∏≠ÁöÑËÑÜÂº±ÁÇπÔºåÂπ∂ÂºÄÂèëÁõ∏Â∫îÁöÑÈò≤Âæ°Êú∫Âà∂Ôºå‰æãÂ¶ÇÂºÇÂ∏∏Ê£ÄÊµã„ÄÅÂèÇÊï∞ÂÆåÊï¥ÊÄßÊ†°È™åÁ≠â„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂‰πüÊèêÈÜíÂºÄÂèëËÄÖÂú®Ê®°ÂûãÈÉ®ÁΩ≤Êó∂ÈúÄË¶ÅÊõ¥Âä†ÈáçËßÜÂÆâÂÖ®ÊÄßÔºåÈò≤Ê≠¢ÊÅ∂ÊÑèÊîªÂáªËÄÖÂà©Áî®Á±ª‰ººÁöÑÊºèÊ¥ûÁ™ÉÂèñ‰ø°ÊÅØÊàñÁ†¥ÂùèÊúçÂä°„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Model integrity of Large language models (LLMs) has become a pressing security concern with their massive online deployment. Prior Bit-Flip Attacks (BFAs) -- a class of popular AI weight memory fault-injection techniques -- can severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs and reveal that, despite the intuition of better robustness from modularity and redundancy, only a handful of adversarial bit flips can also cause LLMs' catastrophic accuracy degradation. However, existing BFA methods typically focus on either integer or floating-point models separately, limiting attack flexibility. Moreover, in floating-point models, random bit flips often cause perturbed parameters to extreme values (e.g., flipping in exponent bit), making it not stealthy and leading to numerical runtime error (e.g., invalid tensor values (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky Bit-Flip Attack), which collapses LLM performance with only one single bit flip while keeping perturbed values within benign layer-wise weight distribution. It is achieved through iterative searching and ranking through our defined parameter sensitivity metric, ImpactScore, which combines gradient sensitivity and perturbation range constrained by the benign layer-wise weight distribution. A novel lightweight SKIP searching algorithm is also proposed to greatly reduce searching complexity, which leads to successful SBFA searching taking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma models, with only one single bit flip, SBFA successfully degrades accuracy to below random levels on MMLU and SST-2 in both BF16 and INT8 data formats. Remarkably, flipping a single bit out of billions of parameters reveals a severe security concern of SOTA LLM models.

