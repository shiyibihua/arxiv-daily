---
layout: default
title: S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models
---

# S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22099" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22099v1</a>
  <a href="https://arxiv.org/pdf/2509.22099.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22099v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22099v1', 'S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaoning Sun, Jiachen Yu, Zongqi Wang, Xuewei Yang, Tianle Gu, Yujiu Yang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºS2Jæ–¹æ³•ï¼Œå¼¥åˆç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ä¸­æ±‚è§£èƒ½åŠ›ä¸åˆ¤æ–­èƒ½åŠ›ä¹‹é—´çš„å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹` `å¥–åŠ±å»ºæ¨¡` `é—®é¢˜è§£å†³èƒ½åŠ›` `åˆ¤æ–­èƒ½åŠ›` `æ±‚è§£-åˆ¤æ–­å·®è·` `è‡ªæˆ‘è¿›åŒ–` `ç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹åœ¨é—®é¢˜è§£å†³èƒ½åŠ›å¼ºçš„æƒ…å†µä¸‹ï¼Œå¯¹éƒ¨åˆ†é—®é¢˜åˆ¤æ–­å‡†ç¡®ç‡ä»æœ‰ä¸è¶³ï¼Œå­˜åœ¨æ±‚è§£-åˆ¤æ–­å·®è·ã€‚
2. S2Jæ–¹æ³•åŒæ—¶åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ±‚è§£å’Œåˆ¤æ–­èƒ½åŠ›è¿›è¡Œç›‘ç£ï¼Œæ˜¾å¼å…³è”é—®é¢˜è§£å†³å’Œè¯„ä¼°èƒ½åŠ›ï¼Œç¼©å°å·®è·ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒS2Jæœ‰æ•ˆé™ä½æ±‚è§£-åˆ¤æ–­å·®è·ï¼Œæå‡æ¨¡å‹åˆ¤æ–­æ€§èƒ½ï¼Œå¹¶åœ¨å°æ•°æ®é›†ä¸Šè¾¾åˆ°SOTAï¼Œæ— éœ€å¤–éƒ¨æ¨¡å‹è’¸é¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºå¥–åŠ±å»ºæ¨¡å’Œè¯„ä¼°ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºé€šè¿‡åœ¨åå¥½æ•°æ®é›†ä¸Šä¼˜åŒ–GRMï¼Œå¹¶å°†åˆ¤æ–­æ­£ç¡®æ€§ä½œä¸ºç›‘ç£ä¿¡å·æ¥è®­ç»ƒä¸“é—¨çš„GRMã€‚è™½ç„¶äººä»¬æ™®éè®¤ä¸ºå…·æœ‰æ›´å¼ºé—®é¢˜è§£å†³èƒ½åŠ›çš„GRMé€šå¸¸è¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„åˆ¤æ–­èƒ½åŠ›ï¼Œä½†æˆ‘ä»¬é¦–å…ˆå‘ç°ï¼Œåœ¨æ£€æŸ¥å•ä¸ªæŸ¥è¯¢æ—¶ï¼Œå­˜åœ¨æ˜¾è‘—çš„æ±‚è§£-åˆ¤æ–­å·®è·ã€‚å…·ä½“è€Œè¨€ï¼Œæ±‚è§£-åˆ¤æ–­å·®è·æŒ‡çš„æ˜¯GRMåœ¨å®Œå…¨æœ‰èƒ½åŠ›è§£å†³æŸäº›æŸ¥è¯¢çš„æƒ…å†µä¸‹ï¼Œå´éš¾ä»¥å¯¹è¿™äº›æŸ¥è¯¢åšå‡ºæ­£ç¡®åˆ¤æ–­çš„ç°è±¡ï¼ˆ14%-37%ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Solve-to-Judgeï¼ˆS2Jï¼‰æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒS2JåŒæ—¶åˆ©ç”¨å•ä¸ªGRMè¾“å‡ºçš„æ±‚è§£å’Œåˆ¤æ–­èƒ½åŠ›è¿›è¡Œç›‘ç£ï¼Œä»è€Œåœ¨æ¨¡å‹ä¼˜åŒ–è¿‡ç¨‹ä¸­æ˜¾å¼åœ°å°†GRMçš„é—®é¢˜è§£å†³èƒ½åŠ›å’Œè¯„ä¼°èƒ½åŠ›è”ç³»èµ·æ¥ï¼Œä»è€Œç¼©å°å·®è·ã€‚æˆ‘ä»¬å…¨é¢çš„å®éªŒè¡¨æ˜ï¼ŒS2Jæœ‰æ•ˆåœ°å°†æ±‚è§£-åˆ¤æ–­å·®è·ç¼©å°äº†16.2%ï¼Œä»è€Œå°†æ¨¡å‹çš„åˆ¤æ–­æ€§èƒ½æé«˜äº†5.8%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒS2Jåœ¨åŸºäºç›¸åŒåŸºç¡€æ¨¡å‹æ„å»ºçš„GRMä¸­å®ç°äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨äº†æ˜æ˜¾æ›´å°çš„è®­ç»ƒæ•°æ®é›†ã€‚æ­¤å¤–ï¼ŒS2Jé€šè¿‡è‡ªæˆ‘è¿›åŒ–æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè€Œæ— éœ€ä¾èµ–æ›´å¼ºå¤§çš„å¤–éƒ¨æ¨¡å‹è¿›è¡Œè’¸é¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰åœ¨é—®é¢˜è§£å†³èƒ½åŠ›å’Œåˆ¤æ–­èƒ½åŠ›ä¹‹é—´å­˜åœ¨çš„å·®è·ã€‚å³ä½¿GRMèƒ½å¤ŸæˆåŠŸè§£å†³æŸä¸ªé—®é¢˜ï¼Œå®ƒä»ç„¶å¯èƒ½æ— æ³•æ­£ç¡®åˆ¤æ–­å…¶è§£å†³æ–¹æ¡ˆçš„ä¼˜åŠ£ã€‚è¿™ç§â€œæ±‚è§£-åˆ¤æ–­å·®è·â€é™åˆ¶äº†GRMåœ¨å¥–åŠ±å»ºæ¨¡å’Œè¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨äºä½¿ç”¨åˆ¤æ–­æ­£ç¡®æ€§ä½œä¸ºç›‘ç£ä¿¡å·æ¥è®­ç»ƒGRMï¼Œè€Œå¿½ç•¥äº†åˆ©ç”¨GRMè‡ªèº«çš„æ±‚è§£èƒ½åŠ›æ¥æå‡åˆ¤æ–­èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šS2Jçš„æ ¸å¿ƒæ€è·¯æ˜¯åŒæ—¶åˆ©ç”¨GRMçš„æ±‚è§£èƒ½åŠ›å’Œåˆ¤æ–­èƒ½åŠ›è¿›è¡Œç›‘ç£ï¼Œä»è€Œæ˜¾å¼åœ°å°†GRMçš„é—®é¢˜è§£å†³èƒ½åŠ›å’Œè¯„ä¼°èƒ½åŠ›è”ç³»èµ·æ¥ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨å…¶æ±‚è§£èƒ½åŠ›æ¥æŒ‡å¯¼åˆ¤æ–­è¿‡ç¨‹ï¼Œä»è€Œç¼©å°æ±‚è§£-åˆ¤æ–­å·®è·ã€‚è¿™ç§æ–¹æ³•é¼“åŠ±æ¨¡å‹åœ¨è§£å†³é—®é¢˜çš„åŒæ—¶ï¼Œä¹Ÿå­¦ä¹ å¦‚ä½•è¯„ä¼°è§£å†³æ–¹æ¡ˆçš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šS2Jæ–¹æ³•çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) ä½¿ç”¨GRMç”Ÿæˆé—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚2) ä½¿ç”¨GRMè¯„ä¼°ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆçš„è´¨é‡ã€‚3) ä½¿ç”¨é—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆå’Œè§£å†³æ–¹æ¡ˆçš„è´¨é‡è¯„ä¼°ç»“æœä½œä¸ºç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶ä¼˜åŒ–GRMçš„æ±‚è§£èƒ½åŠ›å’Œåˆ¤æ–­èƒ½åŠ›ã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥è¿­ä»£è¿›è¡Œï¼Œä»¥ä¸æ–­æå‡GRMçš„æ€§èƒ½ã€‚æ•´ä½“æµç¨‹æ˜¯ç«¯åˆ°ç«¯çš„ï¼Œæ²¡æœ‰å¤æ‚çš„æ¨¡å—åˆ’åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šS2Jæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå®ƒåŒæ—¶åˆ©ç”¨äº†GRMçš„æ±‚è§£èƒ½åŠ›å’Œåˆ¤æ–­èƒ½åŠ›è¿›è¡Œç›‘ç£ã€‚ä¸ä»¥å¾€åªå…³æ³¨åˆ¤æ–­æ­£ç¡®æ€§çš„æ–¹æ³•ä¸åŒï¼ŒS2Jæ˜¾å¼åœ°å°†é—®é¢˜è§£å†³å’Œè¯„ä¼°èƒ½åŠ›è”ç³»èµ·æ¥ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æå‡äº†GRMçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒS2Jé€šè¿‡è‡ªæˆ‘è¿›åŒ–æ¥å®ç°æ€§èƒ½æå‡ï¼Œè€Œæ— éœ€ä¾èµ–æ›´å¼ºå¤§çš„å¤–éƒ¨æ¨¡å‹è¿›è¡Œè’¸é¦ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šS2Jçš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°ç»“åˆæ±‚è§£å’Œåˆ¤æ–­çš„ç›‘ç£ä¿¡å·ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚æŸå¤±å‡½æ•°ã€ç½‘ç»œç»“æ„ç­‰ï¼‰è®ºæ–‡ä¸­å¯èƒ½æ²¡æœ‰è¯¦ç»†è¯´æ˜ï¼Œå±äºé€šç”¨GRMçš„è®­ç»ƒç»†èŠ‚ã€‚å…³é”®åœ¨äºè®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä½¿å¾—æ¨¡å‹æ—¢èƒ½å­¦ä¹ ç”Ÿæˆæ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œåˆèƒ½å­¦ä¹ è¯„ä¼°è§£å†³æ–¹æ¡ˆçš„è´¨é‡ã€‚å‚æ•°è®¾ç½®æ–¹é¢ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æ±‚è§£å’Œåˆ¤æ–­æŸå¤±çš„æƒé‡ï¼Œä»¥å¹³è¡¡ä¸¤ç§èƒ½åŠ›çš„å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

S2Jæ–¹æ³•æœ‰æ•ˆåœ°å°†æ±‚è§£-åˆ¤æ–­å·®è·ç¼©å°äº†16.2%ï¼Œä»è€Œå°†æ¨¡å‹çš„åˆ¤æ–­æ€§èƒ½æé«˜äº†5.8%ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒS2Jåœ¨åŸºäºç›¸åŒåŸºç¡€æ¨¡å‹æ„å»ºçš„GRMä¸­å®ç°äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨äº†æ˜æ˜¾æ›´å°çš„è®­ç»ƒæ•°æ®é›†ã€‚è¿™è¡¨æ˜S2Jæ–¹æ³•åœ¨æå‡GRMæ€§èƒ½çš„åŒæ—¶ï¼Œè¿˜å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„æˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

S2Jæ–¹æ³•å¯åº”ç”¨äºå„ç§éœ€è¦å¥–åŠ±å»ºæ¨¡å’Œè¯„ä¼°çš„åœºæ™¯ï¼Œä¾‹å¦‚å¼ºåŒ–å­¦ä¹ ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡GRMçš„åˆ¤æ–­èƒ½åŠ›ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹çš„è¾“å‡ºè´¨é‡ï¼Œä»è€Œæ”¹è¿›æ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºæ„å»ºæ›´å¯é ã€æ›´é«˜æ•ˆçš„AIç³»ç»Ÿï¼Œå¹¶é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid development of large language models (LLMs), generative reward models (GRMs) have been widely adopted for reward modeling and evaluation. Previous studies have primarily focused on training specialized GRMs by optimizing them on preference datasets with the judgment correctness as supervision. While it's widely accepted that GRMs with stronger problem-solving capabilities typically exhibit superior judgment abilities, we first identify a significant solve-to-judge gap when examining individual queries. Specifically, the solve-to-judge gap refers to the phenomenon where GRMs struggle to make correct judgments on some queries (14%-37%), despite being fully capable of solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to address this problem. Specifically, S2J simultaneously leverages both the solving and judging capabilities on a single GRM's output for supervision, explicitly linking the GRM's problem-solving and evaluation abilities during model optimization, thereby narrowing the gap. Our comprehensive experiments demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%, thereby enhancing the model's judgment performance by 5.8%. Notably, S2J achieves state-of-the-art (SOTA) performance among GRMs built on the same base model while utilizing a significantly smaller training dataset. Moreover, S2J accomplishes this through self-evolution without relying on more powerful external models for distillation.

