---
layout: default
title: ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models
---

# ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21826" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21826v1</a>
  <a href="https://arxiv.org/pdf/2509.21826.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21826v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21826v1', 'ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihan Lin, Xiaohan Wang, Jie Cao, Jiajun Chai, Guojun Yin, Wei Lin, Ran He

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ResTï¼šé‡å¡‘Tokençº§ç­–ç•¥æ¢¯åº¦ï¼Œæå‡LLMå·¥å…·ä½¿ç”¨èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å·¥å…·ä½¿ç”¨` `å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥æ¢¯åº¦` `ç­–ç•¥ç†µ` `Tokené‡åŠ æƒ` `æ™ºèƒ½ä½“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å·¥å…·ä½¿ç”¨LLMçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¾èµ–ç¨€ç–å¥–åŠ±ï¼Œå¿½ç•¥äº†ä»»åŠ¡ç‰¹æ€§ï¼Œå¯¼è‡´ç­–ç•¥æ¢¯åº¦æ–¹å·®å¤§ï¼Œè®­ç»ƒæ•ˆç‡ä½ã€‚
2. ResTé€šè¿‡ç†µæ„ŸçŸ¥çš„tokené‡åŠ æƒæ¥é‡å¡‘ç­–ç•¥æ¢¯åº¦ï¼Œé€æ­¥æå‡æ¨ç†tokençš„æƒé‡ï¼Œä»è€Œç¨³å®šå¤šè½®å·¥å…·ä½¿ç”¨ä»»åŠ¡çš„æ”¶æ•›ã€‚
3. ResTåœ¨BFCLå’ŒAPI-Bankä¸Šå–å¾—äº†SOTAç»“æœï¼Œå¹¶åœ¨4B LLMå¾®è°ƒåè¶…è¶ŠGPT-4oï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é€šè¿‡è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œè¶…è¶Šäº†è¢«åŠ¨ç”Ÿæˆï¼Œæˆä¸ºç›®æ ‡å¯¼å‘çš„æ™ºèƒ½ä½“ã€‚å¼ºåŒ–å­¦ä¹ (RL)ä¸ºä¼˜åŒ–è¿™äº›æ¶Œç°çš„å·¥å…·ä½¿ç”¨ç­–ç•¥æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§æ¡†æ¶ï¼Œä½†ç›®å‰çš„ä¸»æµèŒƒå¼ä»…ä¾èµ–äºç¨€ç–çš„ç»“æœå¥–åŠ±ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹å·¥å…·ä½¿ç”¨ä»»åŠ¡ç‰¹æ®Šæ€§çš„è€ƒè™‘ï¼Œä»è€Œå¢åŠ äº†ç­–ç•¥æ¢¯åº¦æ–¹å·®ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£å’Œè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ç­–ç•¥ç†µä¸å·¥å…·ä½¿ç”¨ä»»åŠ¡è®­ç»ƒç¨³å®šæ€§ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œæ­ç¤ºäº†ç»“æ„åŒ–çš„ä½ç†µtokenæ˜¯å¥–åŠ±çš„ä¸»è¦å†³å®šå› ç´ ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå·¥å…·ä½¿ç”¨ä»»åŠ¡çš„é‡å¡‘Tokençº§ç­–ç•¥æ¢¯åº¦(ResT)ã€‚ResTé€šè¿‡ç†µæ„ŸçŸ¥çš„tokené‡åŠ æƒæ¥é‡å¡‘ç­–ç•¥æ¢¯åº¦ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ­¥æé«˜æ¨ç†tokençš„æƒé‡ã€‚è¿™ç§ç†µæ„ŸçŸ¥æ–¹æ¡ˆå®ç°äº†ä»ç»“æ„æ­£ç¡®æ€§åˆ°è¯­ä¹‰æ¨ç†çš„å¹³æ»‘è¿‡æ¸¡ï¼Œå¹¶ç¨³å®šäº†å¤šè½®å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸­çš„æ”¶æ•›ã€‚åœ¨BFCLå’ŒAPI-Bankä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒResTå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œä¼˜äºç°æœ‰æ–¹æ³•é«˜è¾¾8.76%ã€‚å½“åœ¨4BåŸºç¡€LLMä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼ŒResTåœ¨å•è½®ä»»åŠ¡ä¸Šè¶…è¿‡GPT-4o 4.11%ï¼Œåœ¨å¤šè½®åŸºç¡€ä»»åŠ¡ä¸Šè¶…è¿‡GPT-4o 1.50%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸­ï¼Œç”±äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¾èµ–ç¨€ç–å¥–åŠ±å’Œå¿½ç•¥ä»»åŠ¡ç‰¹æ€§ï¼Œå¯¼è‡´çš„ç­–ç•¥æ¢¯åº¦æ–¹å·®è¿‡å¤§å’Œè®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåŒºåˆ†ç»“æ„æ€§tokenå’Œè¯­ä¹‰æ¨ç†tokençš„é‡è¦æ€§ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥ä»ç»“æ„æ­£ç¡®æ€§å¹³æ»‘è¿‡æ¸¡åˆ°è¯­ä¹‰æ¨ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç­–ç•¥ç†µæ¥æŒ‡å¯¼tokençº§åˆ«çš„ç­–ç•¥æ¢¯åº¦é‡å¡‘ã€‚é€šè¿‡åˆ†æç­–ç•¥ç†µä¸è®­ç»ƒç¨³å®šæ€§çš„å…³ç³»ï¼Œå‘ç°ä½ç†µçš„ç»“æ„åŒ–tokenæ˜¯å¥–åŠ±çš„å…³é”®å†³å®šå› ç´ ã€‚å› æ­¤ï¼Œé€šè¿‡é€æ­¥æå‡æ¨ç†tokençš„æƒé‡ï¼Œå¯ä»¥ä½¿æ¨¡å‹æ›´å…³æ³¨è¯­ä¹‰æ¨ç†ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šResTçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨LLMç”Ÿæˆå·¥å…·ä½¿ç”¨åºåˆ—ï¼›2) è®¡ç®—æ¯ä¸ªtokençš„ç­–ç•¥ç†µï¼›3) æ ¹æ®ç­–ç•¥ç†µå¯¹tokençš„ç­–ç•¥æ¢¯åº¦è¿›è¡Œé‡åŠ æƒï¼Œé€æ­¥æå‡æ¨ç†tokençš„æƒé‡ï¼›4) ä½¿ç”¨é‡åŠ æƒåçš„ç­–ç•¥æ¢¯åº¦æ›´æ–°LLMçš„ç­–ç•¥ã€‚è¯¥æ¡†æ¶é€šè¿‡ç†µæ„ŸçŸ¥çš„tokené‡åŠ æƒï¼Œå®ç°äº†ä»ç»“æ„æ­£ç¡®æ€§åˆ°è¯­ä¹‰æ¨ç†çš„å¹³æ»‘è¿‡æ¸¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šResTæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ç†µæ„ŸçŸ¥çš„tokené‡åŠ æƒç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒResTä¸æ˜¯ç®€å•åœ°ä½¿ç”¨ç¨€ç–å¥–åŠ±æ¥è®­ç»ƒLLMï¼Œè€Œæ˜¯åˆ©ç”¨ç­–ç•¥ç†µæ¥åŒºåˆ†ä¸åŒtokençš„é‡è¦æ€§ï¼Œå¹¶æ ¹æ®å…¶é‡è¦æ€§å¯¹ç­–ç•¥æ¢¯åº¦è¿›è¡Œé‡åŠ æƒã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®­ç»ƒæ•°æ®ï¼Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šResTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç­–ç•¥ç†µçš„è®¡ç®—æ–¹å¼ï¼Œè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†æŸç§ç‰¹å®šçš„ç†µè®¡ç®—å…¬å¼ï¼Œä¾‹å¦‚äº¤å‰ç†µæˆ–KLæ•£åº¦ï¼›2) tokené‡åŠ æƒçš„ç­–ç•¥ï¼Œè®ºæ–‡å¯èƒ½è®¾è®¡äº†ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®tokençš„ç­–ç•¥ç†µæ¥ç¡®å®šå…¶æƒé‡ï¼Œå¹¶éšç€è®­ç»ƒçš„è¿›è¡Œé€æ­¥æå‡æ¨ç†tokençš„æƒé‡ï¼›3) å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©ï¼Œè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†æŸç§ç‰¹å®šçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚PPOæˆ–Actor-Criticã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ResTåœ¨BFCLå’ŒAPI-Bankæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•é«˜è¾¾8.76%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå½“åœ¨4BåŸºç¡€LLMä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼ŒResTåœ¨å•è½®ä»»åŠ¡ä¸Šè¶…è¿‡GPT-4o 4.11%ï¼Œåœ¨å¤šè½®åŸºç¡€ä»»åŠ¡ä¸Šè¶…è¿‡GPT-4o 1.50%ã€‚è¿™äº›å®éªŒç»“æœå……åˆ†è¯æ˜äº†ResTåœ¨æå‡LLMå·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ResTçš„ç ”ç©¶æˆæœå¯ä»¥å¹¿æ³›åº”ç”¨äºéœ€è¦LLMè¿›è¡Œå·¥å…·ä½¿ç”¨çš„å„ç§åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å®¢æœã€ä»£ç ç”Ÿæˆã€ç§‘å­¦ç ”ç©¶ç­‰ã€‚é€šè¿‡æé«˜LLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´é«˜æ•ˆã€æ›´æ™ºèƒ½çš„è‡ªåŠ¨åŒ–ä»»åŠ¡å¤„ç†ï¼Œä»è€Œæå‡ç”Ÿäº§æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œæ›´å¹¿æ³›çš„LLMåº”ç”¨é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) transcend passive generation and act as goal-directed agents by invoking external tools. Reinforcement learning (RL) offers a principled framework for optimizing these emergent tool-use policies, yet the prevailing paradigm relies exclusively on sparse outcome rewards and lacks consideration of the particularity of tool-use tasks, inflating policy-gradient variance and resulting in inefficient training. To better understand and address these challenges, we first establish a theoretical link between policy entropy and training stability of tool-use tasks, which reveals that structured, low-entropy tokens are primary determinants of rewards. Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy gradient through entropy-informed token reweighting, progressively upweighting reasoning tokens as training proceeds. This entropy-aware scheme enables a smooth shift from structural correctness to semantic reasoning and stabilizes convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows that ResT achieves state-of-the-art results, outperforming prior methods by up to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by $4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

