---
layout: default
title: InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models
---

# InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22536" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22536v4</a>
  <a href="https://arxiv.org/pdf/2509.22536.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22536v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22536v4', 'InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenjun Wang, Shuo Cai, Congkai Xie, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Hongxia Yang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-10-17)

**å¤‡æ³¨**: This paper has been withdrawn by the authors due to a significant bug discovered in our data processing pipeline. This bug affects the validity of the experimental results, and we can no longer stand by the conclusions presented

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**InfiR2ï¼šé¢å‘æ¨ç†å¢å¼ºè¯­è¨€æ¨¡å‹çš„å…¨é¢FP8è®­ç»ƒæ–¹æ¡ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `FP8è®­ç»ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æŒç»­é¢„è®­ç»ƒ` `ç›‘ç£å¾®è°ƒ` `æ··åˆç²¾åº¦é‡åŒ–` `æ¨ç†å¢å¼º` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œé˜»ç¢äº†åˆ›æ–°ï¼ŒFP8è®­ç»ƒè™½æœ‰æ½œåŠ›ï¼Œä½†ç¼ºä¹å®Œæ•´å¼€æºæ–¹æ¡ˆã€‚
2. æå‡ºInfiR2ï¼Œä¸€ç§ç«¯åˆ°ç«¯FP8è®­ç»ƒæ–¹æ¡ˆï¼Œç»“åˆæŒç»­é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼Œé‡‡ç”¨æ··åˆç²’åº¦é‡åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆç¨³å®šä¸”æ— æŸï¼Œæ¨ç†æ€§èƒ½ä¸BF16ç›¸å½“ï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´å‡å°‘22%ï¼Œå†…å­˜å‡å°‘14%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å·¨å¤§è®¡ç®—æˆæœ¬æ˜¯åˆ›æ–°çš„ä¸»è¦éšœç¢ã€‚è™½ç„¶FP8è®­ç»ƒåœ¨ç†è®ºä¸Šæä¾›äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œä½†ç”±äºç¼ºä¹å…¨é¢çš„å¼€æºè®­ç»ƒæ–¹æ¡ˆï¼Œå…¶å¹¿æ³›åº”ç”¨å—åˆ°é˜»ç¢ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç«¯åˆ°ç«¯çš„FP8è®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæ— ç¼é›†æˆäº†æŒç»­é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§ç»†ç²’åº¦çš„æ··åˆç²’åº¦é‡åŒ–ç­–ç•¥ï¼Œä»¥ä¿æŒæ•°å€¼ç²¾åº¦ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°æé«˜è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡åŒ…æ‹¬åœ¨1600äº¿tokenè¯­æ–™åº“ä¸ŠæŒç»­é¢„è®­ç»ƒæ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ¡ˆä¸ä»…éå¸¸ç¨³å®šï¼Œè€Œä¸”åŸºæœ¬ä¸Šæ˜¯æ— æŸçš„ï¼Œåœ¨ä¸€ç³»åˆ—æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸BF16åŸºçº¿ç›¸å½“çš„æ€§èƒ½ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œè¿™é€šè¿‡æ˜¾è‘—çš„æ•ˆç‡æ”¹è¿›æ¥å®ç°ï¼ŒåŒ…æ‹¬è®­ç»ƒæ—¶é—´å‡å°‘é«˜è¾¾22%ï¼Œå³°å€¼å†…å­˜ä½¿ç”¨é‡å‡å°‘14%ï¼Œååé‡æé«˜19%ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜FP8æ˜¯BF16çš„ä¸€ç§å®ç”¨ä¸”ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæˆ‘ä»¬å°†å‘å¸ƒéšé™„çš„ä»£ç ä»¥è¿›ä¸€æ­¥æ™®åŠå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚BF16è®­ç»ƒï¼Œè™½ç„¶è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢å­˜åœ¨ç“¶é¢ˆã€‚FP8è®­ç»ƒç†è®ºä¸Šå¯ä»¥æ˜¾è‘—æé«˜æ•ˆç‡ï¼Œä½†ç¼ºä¹ä¸€ä¸ªå…¨é¢ã€æ˜“äºä½¿ç”¨çš„è®­ç»ƒæ–¹æ¡ˆï¼Œé˜»ç¢äº†å…¶å¹¿æ³›åº”ç”¨ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªç¨³å®šã€é«˜æ•ˆä¸”æ€§èƒ½è‰¯å¥½çš„FP8è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥é™ä½LLMçš„è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿç›¸å…³ç ”ç©¶å’Œåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ä¸ªç«¯åˆ°ç«¯çš„FP8è®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿæ— ç¼é›†æˆæŒç»­é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼Œå¹¶é‡‡ç”¨ç»†ç²’åº¦çš„æ··åˆç²’åº¦é‡åŒ–ç­–ç•¥ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ—¢èƒ½å……åˆ†åˆ©ç”¨FP8çš„è®¡ç®—æ•ˆç‡ä¼˜åŠ¿ï¼Œåˆèƒ½æœ€å¤§é™åº¦åœ°ä¿æŒæ•°å€¼ç²¾åº¦ï¼Œé¿å…å› é‡åŒ–å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆåŠ›æ±‚ç®€å•æ˜“ç”¨ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆå¿«é€Ÿä¸Šæ‰‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šInfiR2çš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šæŒç»­é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒã€‚åœ¨æŒç»­é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨ä¸€ä¸ªå¤§è§„æ¨¡è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ é€šç”¨çš„è¯­è¨€è¡¨ç¤ºã€‚åœ¨ç›‘ç£å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”ç‰¹å®šä»»åŠ¡çš„éœ€æ±‚ã€‚è¿™ä¸¤ä¸ªé˜¶æ®µéƒ½é‡‡ç”¨FP8æ ¼å¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨æ··åˆç²’åº¦é‡åŒ–ç­–ç•¥æ¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡å’Œæ•°å€¼ç²¾åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„ã€å¯ç›´æ¥ä½¿ç”¨çš„FP8è®­ç»ƒæ–¹æ¡ˆï¼Œå¹¶éªŒè¯äº†å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæ··åˆç²’åº¦é‡åŒ–ç­–ç•¥æ˜¯å…¶é‡è¦çš„åˆ›æ–°ç‚¹ï¼Œå®ƒå…è®¸æ ¹æ®ä¸åŒå±‚æˆ–å‚æ•°çš„é‡è¦æ€§ï¼Œé‡‡ç”¨ä¸åŒçš„é‡åŒ–ç²¾åº¦ï¼Œä»è€Œåœ¨è®¡ç®—æ•ˆç‡å’Œæ•°å€¼ç²¾åº¦ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆçš„ç«¯åˆ°ç«¯è®¾è®¡ä¹Ÿä½¿å…¶æ˜“äºé›†æˆåˆ°ç°æœ‰çš„è®­ç»ƒæµç¨‹ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šæ··åˆç²’åº¦é‡åŒ–ç­–ç•¥æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ï¼Œå…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ï¼Œä½†æ¨æµ‹å¯èƒ½åŒ…æ‹¬ï¼šå¯¹æ¿€æ´»å€¼å’Œæƒé‡é‡‡ç”¨ä¸åŒçš„é‡åŒ–æ–¹æ¡ˆï¼›å¯¹ä¸åŒå±‚é‡‡ç”¨ä¸åŒçš„é‡åŒ–ä½å®½ï¼›åŠ¨æ€è°ƒæ•´é‡åŒ–å‚æ•°ç­‰ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©å’Œä¼˜åŒ–å™¨çš„é…ç½®ä¹Ÿæ˜¯é‡è¦çš„æŠ€æœ¯ç»†èŠ‚ï¼Œä½†è®ºæ–‡æ‘˜è¦ä¸­æœªæåŠå…·ä½“ç»†èŠ‚ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒInfiR2åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸BF16åŸºçº¿ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´å‡å°‘é«˜è¾¾22%ï¼Œå³°å€¼å†…å­˜ä½¿ç”¨é‡å‡å°‘14%ï¼Œååé‡æé«˜19%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFP8è®­ç»ƒæ˜¯ä¸€ç§å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨ä¸æŸå¤±æ€§èƒ½çš„å‰æä¸‹æ˜¾è‘—æé«˜è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡é™ä½è®­ç»ƒæˆæœ¬ï¼Œè¯¥æ–¹æ¡ˆæœ‰æœ›åŠ é€ŸLLMåœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶ä¿ƒè¿›ç›¸å…³æŠ€æœ¯çš„åˆ›æ–°å’Œå‘å±•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆçš„å¼€æºå‘å¸ƒå°†è¿›ä¸€æ­¥æ¨åŠ¨LLMçš„æ™®åŠå’Œ democratize å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The immense computational cost of training Large Language Models (LLMs) presents a major barrier to innovation. While FP8 training offers a promising solution with significant theoretical efficiency gains, its widespread adoption has been hindered by the lack of a comprehensive, open-source training recipe. To bridge this gap, we introduce an end-to-end FP8 training recipe that seamlessly integrates continual pre-training and supervised fine-tuning. Our methodology employs a fine-grained, hybrid-granularity quantization strategy to maintain numerical fidelity while maximizing computational efficiency. Through extensive experiments, including the continue pre-training of models on a 160B-token corpus, we demonstrate that our recipe is not only remarkably stable but also essentially lossless, achieving performance on par with the BF16 baseline across a suite of reasoning benchmarks. Crucially, this is achieved with substantial efficiency improvements, including up to a 22% reduction in training time, a 14% decrease in peak memory usage, and a 19% increase in throughput. Our results establish FP8 as a practical and robust alternative to BF16, and we will release the accompanying code to further democratize large-scale model training.

