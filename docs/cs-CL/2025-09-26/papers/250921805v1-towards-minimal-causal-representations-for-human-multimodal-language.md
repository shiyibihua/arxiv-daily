---
layout: default
title: Towards Minimal Causal Representations for Human Multimodal Language Understanding
---

# Towards Minimal Causal Representations for Human Multimodal Language Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21805" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21805v1</a>
  <a href="https://arxiv.org/pdf/2509.21805.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21805v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21805v1', 'Towards Minimal Causal Representations for Human Multimodal Language Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Menghua Jiang, Yuncheng Jiang, Haifeng Hu, Sijie Mai

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCausal Multimodal Information Bottleneckä»¥è§£å†³å¤šæ¨¡æ€è¯­è¨€ç†è§£ä¸­çš„åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å› æœæ¨æ–­` `å¤šæ¨¡æ€è¯­è¨€ç†è§£` `ä¿¡æ¯ç“¶é¢ˆ` `æ¨¡å‹æ³›åŒ–` `æƒ…æ„Ÿåˆ†æ` `å¹½é»˜æ£€æµ‹` `è®½åˆºæ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€è¯­è¨€ç†è§£æ–¹æ³•å®¹æ˜“å—åˆ°æ•°æ®é›†åå·®çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆåŒºåˆ†å› æœç‰¹å¾ä¸ç»Ÿè®¡æ·å¾„ã€‚
2. æœ¬æ–‡æå‡ºçš„CaMIBæ¨¡å‹é€šè¿‡å› æœåŸåˆ™æ¥è¿‡æ»¤å’Œè§£è€¦å¤šæ¨¡æ€è¾“å…¥ï¼Œå¢å¼ºæ¨¡å‹çš„å› æœæ¨æ–­èƒ½åŠ›ã€‚
3. åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€å¹½é»˜æ£€æµ‹å’Œè®½åˆºæ£€æµ‹ç­‰ä»»åŠ¡ä¸­ï¼ŒCaMIBåœ¨OODæµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»å¤šæ¨¡æ€è¯­è¨€ç†è§£ï¼ˆMLUï¼‰æ—¨åœ¨é€šè¿‡æ•´åˆæ¥è‡ªä¸åŒæ¨¡æ€çš„ç›¸å…³çº¿ç´¢æ¥æ¨æ–­äººç±»æ„å›¾ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦éµå¾ªâ€œå­¦ä¹ æ³¨æ„â€çš„èŒƒå¼ï¼Œæœ€å¤§åŒ–æ•°æ®ä¸æ ‡ç­¾ä¹‹é—´çš„äº’ä¿¡æ¯ä»¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å®¹æ˜“å—åˆ°æ•°æ®é›†åå·®çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹å°†ç»Ÿè®¡æ·å¾„ä¸çœŸæ­£çš„å› æœç‰¹å¾æ··æ·†ï¼Œä»è€Œé™ä½äº†åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å› æœå¤šæ¨¡æ€ä¿¡æ¯ç“¶é¢ˆï¼ˆCaMIBï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å› æœåŸåˆ™è€Œéä¼ ç»Ÿçš„ä¼¼ç„¶æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆåº”ç”¨ä¿¡æ¯ç“¶é¢ˆæ¥è¿‡æ»¤å•æ¨¡æ€è¾“å…¥ï¼Œå»é™¤ä¸ä»»åŠ¡æ— å…³çš„å™ªå£°ã€‚ç„¶åï¼Œä½¿ç”¨å‚æ•°åŒ–çš„æ©ç ç”Ÿæˆå™¨å°†èåˆçš„å¤šæ¨¡æ€è¡¨ç¤ºè§£è€¦ä¸ºå› æœå’Œæ·å¾„å­è¡¨ç¤ºã€‚ä¸ºäº†ç¡®ä¿å› æœç‰¹å¾çš„å…¨å±€ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å·¥å…·å˜é‡çº¦æŸï¼Œå¹¶é€šè¿‡éšæœºé‡ç»„å› æœå’Œæ·å¾„ç‰¹å¾æ¥è¿›ä¸€æ­¥é‡‡ç”¨åé—¨è°ƒæ•´ï¼Œä»¥ç¨³å®šå› æœä¼°è®¡ã€‚å¤§é‡åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€å¹½é»˜æ£€æµ‹å’Œè®½åˆºæ£€æµ‹ä¸Šçš„å®éªŒè¡¨æ˜CaMIBçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€è¯­è¨€ç†è§£æ–¹æ³•åœ¨é¢å¯¹æ•°æ®é›†åå·®æ—¶çš„è„†å¼±æ€§ï¼Œå°¤å…¶æ˜¯æ¨¡å‹å¯¹ç»Ÿè®¡æ·å¾„çš„ä¾èµ–ï¼Œå¯¼è‡´åœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå› æœå¤šæ¨¡æ€ä¿¡æ¯ç“¶é¢ˆï¼ˆCaMIBï¼‰æ¨¡å‹ï¼Œé€šè¿‡å› æœåŸåˆ™è€Œéä¼ ç»Ÿçš„ä¼¼ç„¶æ€§æ¥å¢å¼ºæ¨¡å‹çš„å› æœæ¨æ–­èƒ½åŠ›ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†å› æœç‰¹å¾ä¸å™ªå£°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCaMIBæ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¿¡æ¯ç“¶é¢ˆæ¨¡å—ç”¨äºè¿‡æ»¤å•æ¨¡æ€è¾“å…¥ã€å‚æ•°åŒ–æ©ç ç”Ÿæˆå™¨ç”¨äºè§£è€¦å¤šæ¨¡æ€è¡¨ç¤ºï¼Œä»¥åŠå·¥å…·å˜é‡çº¦æŸå’Œåé—¨è°ƒæ•´æœºåˆ¶æ¥ç¨³å®šå› æœä¼°è®¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCaMIBçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥å› æœä¿¡æ¯ç“¶é¢ˆçš„æ¦‚å¿µï¼Œé€šè¿‡è§£è€¦å› æœç‰¹å¾ä¸æ·å¾„ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨OODæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ä¾èµ–äºç»Ÿè®¡ç‰¹å¾çš„æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹ä¸­ä½¿ç”¨çš„å‚æ•°åŒ–æ©ç ç”Ÿæˆå™¨è®¾è®¡ä¸ºèƒ½å¤ŸåŠ¨æ€è°ƒæ•´ç‰¹å¾çš„è§£è€¦è¿‡ç¨‹ï¼ŒæŸå¤±å‡½æ•°ç»“åˆäº†ä¿¡æ¯ç“¶é¢ˆå’Œå› æœçº¦æŸï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCaMIBåœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€å¹½é»˜æ£€æµ‹å’Œè®½åˆºæ£€æµ‹ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå°¤å…¶åœ¨OODæµ‹è¯•é›†ä¸Šï¼Œæ¨¡å‹çš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€ç¤¾äº¤åª’ä½“å†…å®¹ç†è§£ã€ä»¥åŠäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„å› æœæ¨æ–­èƒ½åŠ›ï¼ŒCaMIBèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£äººç±»æ„å›¾ï¼Œä»è€Œæå‡å¤šæ¨¡æ€ç³»ç»Ÿçš„æ™ºèƒ½æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨è‡ªåŠ¨åŒ–å®¢æœã€æ™ºèƒ½æ¨èç³»ç»Ÿç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human Multimodal Language Understanding (MLU) aims to infer human intentions by integrating related cues from heterogeneous modalities. Existing works predominantly follow a ``learning to attend" paradigm, which maximizes mutual information between data and labels to enhance predictive performance. However, such methods are vulnerable to unintended dataset biases, causing models to conflate statistical shortcuts with genuine causal features and resulting in degraded out-of-distribution (OOD) generalization. To alleviate this issue, we introduce a Causal Multimodal Information Bottleneck (CaMIB) model that leverages causal principles rather than traditional likelihood. Concretely, we first applies the information bottleneck to filter unimodal inputs, removing task-irrelevant noise. A parameterized mask generator then disentangles the fused multimodal representation into causal and shortcut subrepresentations. To ensure global consistency of causal features, we incorporate an instrumental variable constraint, and further adopt backdoor adjustment by randomly recombining causal and shortcut features to stabilize causal estimation. Extensive experiments on multimodal sentiment analysis, humor detection, and sarcasm detection, along with OOD test sets, demonstrate the effectiveness of CaMIB. Theoretical and empirical analyses further highlight its interpretability and soundness.

