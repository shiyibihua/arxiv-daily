---
layout: default
title: Fine-tuning Done Right in Model Editing
---

# Fine-tuning Done Right in Model Editing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22072" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22072v2</a>
  <a href="https://arxiv.org/pdf/2509.22072.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22072v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22072v2', 'Fine-tuning Done Right in Model Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wanli Yang, Fei Sun, Rui Tang, Hongyu Zang, Du Su, Qi Cao, Jingang Wang, Huawei Shen, Xueqi Cheng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-09-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é‡å¡‘å¾®è°ƒåœ¨æ¨¡å‹ç¼–è¾‘ä¸­çš„åœ°ä½ï¼šæå‡ºLocFT-BFå¤§å¹…è¶…è¶Šç°æœ‰æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹ç¼–è¾‘` `å¾®è°ƒ` `å¹¿åº¦ä¼˜å…ˆ` `å±€éƒ¨è°ƒæ•´` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹ç¼–è¾‘æ–¹æ³•é‡‡ç”¨æ·±åº¦ä¼˜å…ˆç­–ç•¥å¾®è°ƒï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆå’Œç¼–è¾‘é—´çš„ç›¸äº’å¹²æ‰°ï¼Œé™åˆ¶äº†å¾®è°ƒåœ¨æ¨¡å‹ç¼–è¾‘ä¸­çš„åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºLocFT-BFï¼Œé€šè¿‡æ¢å¤å¹¿åº¦ä¼˜å…ˆå¾®è°ƒå¹¶ç»“åˆå±€éƒ¨å‚æ•°è°ƒæ•´ï¼Œæœ‰æ•ˆé¿å…äº†è¿‡æ‹Ÿåˆå’Œå¹²æ‰°é—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLocFT-BFåœ¨å¤šä¸ªLLMå’Œæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶é¦–æ¬¡å®ç°äº†å¯¹100Kç¼–è¾‘å’Œ72Bå‚æ•°æ¨¡å‹çš„æœ‰æ•ˆç¼–è¾‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒæ˜¯è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ–¹æ³•ï¼Œä½†é•¿æœŸä»¥æ¥è¢«è®¤ä¸ºä¸é€‚ç”¨äºæ¨¡å‹ç¼–è¾‘ã€‚æœ¬æ–‡å¯¹æ­¤æå‡ºè´¨ç–‘ï¼Œè®¤ä¸ºç°æœ‰æ–¹æ³•å¤±æ•ˆå¹¶éå¾®è°ƒæœ¬èº«å›ºæœ‰é™åˆ¶ï¼Œè€Œæ˜¯å› ä¸ºå°†å¾®è°ƒé€‚é…äºç¼–è¾‘ä»»åŠ¡çš„é¡ºåºç‰¹æ€§ï¼Œå³å•æ¬¡æ·±åº¦ä¼˜å…ˆçš„æµæ°´çº¿ï¼Œè¯¥æµæ°´çº¿åœ¨å¤„ç†ä¸‹ä¸€ä¸ªæ ·æœ¬ä¹‹å‰å°†æ¯ä¸ªæ ·æœ¬ä¼˜åŒ–åˆ°æ”¶æ•›ã€‚è¿™ç§æ·±åº¦ä¼˜å…ˆæµæ°´çº¿ä¸é€æ ·æœ¬æ›´æ–°ç›¸ç»“åˆï¼Œè¿‡åº¦ä¼˜åŒ–æ¯ä¸ªç¼–è¾‘ï¼Œå¹¶å¯¼è‡´ç¼–è¾‘ä¹‹é—´çš„å¹²æ‰°ã€‚å—æ§å®éªŒè¡¨æ˜ï¼Œå°†å¾®è°ƒæ¢å¤åˆ°æ ‡å‡†çš„å¹¿åº¦ä¼˜å…ˆï¼ˆå³åŸºäºepochï¼‰æµæ°´çº¿ï¼Œå¹¶é‡‡ç”¨å°æ‰¹é‡ä¼˜åŒ–ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å…¶åœ¨æ¨¡å‹ç¼–è¾‘ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œç¼–è¾‘ä¸­çš„å¾®è°ƒè¿˜å—åˆ°ä»å…ˆå‰æ–¹æ³•ç»§æ‰¿çš„æ¬¡ä¼˜è°ƒæ•´å‚æ•°ä½ç½®çš„å½±å“ã€‚é€šè¿‡å¯¹è°ƒæ•´ä½ç½®çš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºLocFT-BFï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¢å¤çš„å¾®è°ƒæ¡†æ¶æ„å»ºçš„ç®€å•æœ‰æ•ˆçš„å±€éƒ¨ç¼–è¾‘æ–¹æ³•ã€‚åœ¨å„ç§LLMå’Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLocFT-BFçš„æ€§èƒ½å¤§å¤§ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªåœ¨ä¸ç‰ºç‰²é€šç”¨èƒ½åŠ›çš„æƒ…å†µä¸‹ï¼Œç»´æŒ10ä¸‡æ¬¡ç¼–è¾‘å’Œ720äº¿å‚æ•°æ¨¡å‹çš„æ–¹æ³•ï¼Œæ¯”ä¹‹å‰çš„å®è·µæé«˜äº†10å€ã€‚é€šè¿‡æ¾„æ¸…ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„è¯¯è§£ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªæœ‰åŸåˆ™çš„å±€éƒ¨è°ƒæ•´ç­–ç•¥ï¼Œæˆ‘ä»¬å°†å¾®è°ƒä»ä¸€ä¸ªè¢«ä½ä¼°çš„åŸºçº¿æå‡ä¸ºæ¨¡å‹ç¼–è¾‘çš„é¢†å…ˆæ–¹æ³•ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ¨¡å‹ç¼–è¾‘æ—¨åœ¨ä¿®æ”¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­çš„ç‰¹å®šçŸ¥è¯†ï¼Œä½¿å…¶ç¬¦åˆæ–°çš„äº‹å®æˆ–è§„åˆ™ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æ·±åº¦ä¼˜å…ˆçš„å¾®è°ƒç­–ç•¥ï¼Œå³é’ˆå¯¹æ¯ä¸ªç¼–è¾‘æ ·æœ¬è¿›è¡Œå……åˆ†ä¼˜åŒ–ï¼Œç„¶åå†å¤„ç†ä¸‹ä¸€ä¸ªæ ·æœ¬ã€‚è¿™ç§ç­–ç•¥å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œä½¿å¾—æ¨¡å‹è¿‡åº¦é€‚åº”å•ä¸ªç¼–è¾‘ï¼Œä»è€Œå½±å“å…¶æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é€ æˆä¸åŒç¼–è¾‘ä¹‹é—´çš„ç›¸äº’å¹²æ‰°ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨é€‰æ‹©å¾®è°ƒå‚æ•°çš„ä½ç½®æ–¹é¢ä¹Ÿå­˜åœ¨ä¸è¶³ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡æ–°å®¡è§†å¾®è°ƒåœ¨æ¨¡å‹ç¼–è¾‘ä¸­çš„ä½œç”¨ï¼Œå¹¶æå‡ºä¸€ç§æ›´æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥ã€‚ä½œè€…è®¤ä¸ºï¼Œæ·±åº¦ä¼˜å…ˆçš„å¾®è°ƒç­–ç•¥æ˜¯å¯¼è‡´å¾®è°ƒåœ¨æ¨¡å‹ç¼–è¾‘ä¸­è¡¨ç°ä¸ä½³çš„ä¸»è¦åŸå› ã€‚å› æ­¤ï¼Œä»–ä»¬æå‡ºæ¢å¤æ ‡å‡†çš„å¹¿åº¦ä¼˜å…ˆå¾®è°ƒç­–ç•¥ï¼Œå¹¶ç»“åˆå±€éƒ¨å‚æ•°è°ƒæ•´ï¼Œä»¥é¿å…è¿‡æ‹Ÿåˆå’Œå¹²æ‰°é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLocFT-BFæ–¹æ³•çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) æ¢å¤å¹¿åº¦ä¼˜å…ˆçš„å¾®è°ƒç­–ç•¥ï¼Œå³é‡‡ç”¨åŸºäºepochçš„è®­ç»ƒæ–¹å¼ï¼Œå¯¹æ‰€æœ‰ç¼–è¾‘æ ·æœ¬è¿›è¡Œå¤šè½®è®­ç»ƒã€‚2) é‡‡ç”¨å°æ‰¹é‡ä¼˜åŒ–ï¼Œä»¥å‡å°‘æ¯ä¸ªç¼–è¾‘æ ·æœ¬å¯¹æ¨¡å‹å‚æ•°çš„å½±å“ã€‚3) é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œç¡®å®šæœ€ä½³çš„å±€éƒ¨å‚æ•°è°ƒæ•´ä½ç½®ï¼Œä»¥æé«˜ç¼–è¾‘çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚4) ä½¿ç”¨æ ‡å‡†çš„å¾®è°ƒæŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šLocFT-BFæ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) é‡æ–°å®¡è§†äº†å¾®è°ƒåœ¨æ¨¡å‹ç¼–è¾‘ä¸­çš„ä½œç”¨ï¼Œå¹¶è¯æ˜äº†å¾®è°ƒæ˜¯ä¸€ç§æœ‰æ•ˆçš„æ¨¡å‹ç¼–è¾‘æ–¹æ³•ã€‚2) æå‡ºäº†å¹¿åº¦ä¼˜å…ˆçš„å¾®è°ƒç­–ç•¥ï¼Œæœ‰æ•ˆé¿å…äº†è¿‡æ‹Ÿåˆå’Œå¹²æ‰°é—®é¢˜ã€‚3) æå‡ºäº†å±€éƒ¨å‚æ•°è°ƒæ•´ç­–ç•¥ï¼Œæé«˜äº†ç¼–è¾‘çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šLocFT-BFçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é‡‡ç”¨AdamWä¼˜åŒ–å™¨ï¼Œå¹¶è®¾ç½®åˆé€‚çš„å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡ç³»æ•°ã€‚2) ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚3) é€‰æ‹©åˆé€‚çš„å±€éƒ¨å‚æ•°è°ƒæ•´ä½ç½®ï¼Œä¾‹å¦‚Transformerå±‚çš„æ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œã€‚4) è®¾ç½®åˆé€‚çš„è®­ç»ƒepochæ•°å’Œå°æ‰¹é‡å¤§å°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LocFT-BFåœ¨å¤šä¸ªæ•°æ®é›†å’ŒLLMä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒLocFT-BFçš„ç¼–è¾‘æˆåŠŸç‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒLocFT-BFè¿˜é¦–æ¬¡å®ç°äº†å¯¹100Kç¼–è¾‘å’Œ72Bå‚æ•°æ¨¡å‹çš„æœ‰æ•ˆç¼–è¾‘ï¼Œæ¯”ä¹‹å‰çš„å®è·µæé«˜äº†10å€ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚è¿™äº›å®éªŒç»“æœå……åˆ†è¯æ˜äº†LocFT-BFæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒçŸ¥è¯†æ›´æ–°æˆ–ä¿®æ­£çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šäº‹å®çº æ­£ã€çŸ¥è¯†åº“æ›´æ–°ã€æ¨¡å‹ä¸ªæ€§åŒ–å®šåˆ¶ç­‰ã€‚é€šè¿‡LocFT-BFæ–¹æ³•ï¼Œå¯ä»¥æ›´é«˜æ•ˆã€æ›´å‡†ç¡®åœ°ç¼–è¾‘æ¨¡å‹ï¼Œä½¿å…¶é€‚åº”æ–°çš„çŸ¥è¯†å’Œéœ€æ±‚ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½å’Œå¯é æ€§ã€‚è¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºé™ä½æ¨¡å‹ç¼–è¾‘çš„æˆæœ¬å’Œéš¾åº¦ï¼Œä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning, a foundational method for adapting large language models, has long been considered ineffective for model editing. Here, we challenge this belief, arguing that the reported failure arises not from the inherent limitation of fine-tuning itself, but from adapting it to the sequential nature of the editing task, a single-pass depth-first pipeline that optimizes each sample to convergence before moving on. While intuitive, this depth-first pipeline coupled with sample-wise updating over-optimizes each edit and induces interference across edits. Our controlled experiments reveal that simply restoring fine-tuning to the standard breadth-first (i.e., epoch-based) pipeline with mini-batch optimization substantially improves its effectiveness for model editing. Moreover, fine-tuning in editing also suffers from suboptimal tuning parameter locations inherited from prior methods. Through systematic analysis of tuning locations, we derive LocFT-BF, a simple and effective localized editing method built on the restored fine-tuning framework. Extensive experiments across diverse LLMs and datasets demonstrate that LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x beyond prior practice, without sacrificing general capabilities. By clarifying a long-standing misconception and introducing a principled localized tuning strategy, we advance fine-tuning from an underestimated baseline to a leading method for model editing, establishing a solid foundation for future research.

