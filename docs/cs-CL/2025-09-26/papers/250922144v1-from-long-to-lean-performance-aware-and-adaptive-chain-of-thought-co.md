---
layout: default
title: From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement
---

# From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22144" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22144v1</a>
  <a href="https://arxiv.org/pdf/2509.22144.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22144v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22144v1', 'From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Zike Yuan, Yang Xiang, Buzhou Tang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 17 pages, 8 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Leon221220/MACC)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMACCæ¡†æ¶ï¼Œé€šè¿‡å¤šè½®ç»†åŒ–è‡ªé€‚åº”å‹ç¼©CoTï¼Œæå‡æ¨ç†æ•ˆç‡ä¸å‡†ç¡®ç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Chain-of-Thought` `CoTå‹ç¼©` `å¤šè½®ç»†åŒ–` `è‡ªé€‚åº”å‹ç¼©` `æ€§èƒ½é¢„æµ‹` `tokenå¼¹æ€§` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. CoTæ¨ç†è™½æå‡å¤æ‚ä»»åŠ¡æ€§èƒ½ï¼Œä½†å…¶å†—é•¿æ€§å¯¼è‡´æ¨ç†å»¶è¿Ÿè¿‡é«˜ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚
2. MACCæ¡†æ¶é€šè¿‡å¤šè½®ç»†åŒ–è‡ªé€‚åº”å‹ç¼©CoTï¼Œåˆ©ç”¨tokenå¼¹æ€§ç°è±¡ä¼˜åŒ–å‹ç¼©æ·±åº¦ï¼Œæå‡æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMACCåœ¨å‡†ç¡®ç‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½CoTé•¿åº¦å’Œæ¨ç†å»¶è¿Ÿï¼Œä¸”æ€§èƒ½å¯é¢„æµ‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Chain-of-Thought (CoT) æ¨ç†åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå…¶å†—é•¿æ€§å¯¼è‡´æ¨ç†å»¶è¿Ÿæ˜¾è‘—ã€‚æœ¬æ–‡æå‡ºå¤šè½®è‡ªé€‚åº”Chain-of-Thoughtå‹ç¼©ï¼ˆMACCï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨tokenå¼¹æ€§ç°è±¡ï¼ˆå³è¿‡å°çš„tokené¢„ç®—åè€Œä¼šå¢åŠ è¾“å‡ºé•¿åº¦ï¼‰é€šè¿‡å¤šè½®ç»†åŒ–é€æ­¥å‹ç¼©CoTã€‚è¿™ç§è‡ªé€‚åº”ç­–ç•¥ä½¿MACCèƒ½å¤Ÿç¡®å®šæ¯ä¸ªè¾“å…¥çš„æœ€ä½³å‹ç¼©æ·±åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMACCåœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šæ¯”æœ€å…ˆè¿›çš„åŸºçº¿æé«˜äº†5.6%ï¼ŒåŒæ—¶å¹³å‡å‡å°‘äº†47ä¸ªtokençš„CoTé•¿åº¦ï¼Œå¹¶æ˜¾è‘—é™ä½äº†å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†æµ‹è¯•æ—¶æ€§èƒ½ï¼ˆå‡†ç¡®ç‡å’Œtokené•¿åº¦ï¼‰å¯ä»¥ä½¿ç”¨å¯è§£é‡Šçš„ç‰¹å¾ï¼ˆå¦‚è®­ç»ƒé›†ä¸Šçš„å›°æƒ‘åº¦å’Œå‹ç¼©ç‡ï¼‰è¿›è¡Œå¯é é¢„æµ‹ã€‚åœ¨ä¸åŒæ¨¡å‹ä¸Šè¿›è¡Œè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ— éœ€é‡å¤å¾®è°ƒå³å¯å®ç°é«˜æ•ˆçš„æ¨¡å‹é€‰æ‹©å’Œé¢„æµ‹ï¼Œè¯æ˜CoTå‹ç¼©æ—¢æœ‰æ•ˆåˆå¯é¢„æµ‹ã€‚ä»£ç å°†åœ¨https://github.com/Leon221220/MACC ä¸Šå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³Chain-of-Thought (CoT) æ¨ç†ä¸­ç”±äºå†—é•¿æ€§å¯¼è‡´çš„æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å‹ç¼©CoTæ—¶ï¼Œå¾€å¾€é‡‡ç”¨å›ºå®šçš„å‹ç¼©ç­–ç•¥ï¼Œæ— æ³•æ ¹æ®ä¸åŒè¾“å…¥çš„ç‰¹æ€§è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼Œå¯¼è‡´å‹ç¼©æ•ˆæœä¸ä½³ï¼Œç”šè‡³å¯èƒ½é™ä½æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆé¢„æµ‹å‹ç¼©åçš„æ€§èƒ½ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨tokenå¼¹æ€§ç°è±¡ï¼Œå³åœ¨tokené¢„ç®—è¿‡å°çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¸ºäº†æ»¡è¶³è¾“å‡ºéœ€æ±‚åè€Œä¼šç”Ÿæˆæ›´é•¿çš„æ–‡æœ¬ã€‚é€šè¿‡å¤šè½®ç»†åŒ–ï¼Œé€æ­¥å‹ç¼©CoTï¼Œå¹¶æ ¹æ®æ¯ä¸€è½®çš„å‹ç¼©æ•ˆæœè‡ªé€‚åº”åœ°è°ƒæ•´å‹ç¼©æ·±åº¦ï¼Œä»è€Œåœ¨ä¿è¯å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå°½å¯èƒ½åœ°å‡å°‘CoTçš„é•¿åº¦å’Œæ¨ç†å»¶è¿Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMACCæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆå§‹CoTç”Ÿæˆï¼šä½¿ç”¨åŸå§‹çš„CoTæ–¹æ³•ç”Ÿæˆåˆå§‹çš„æ¨ç†é“¾ã€‚2) å¤šè½®å‹ç¼©ï¼šé€šè¿‡é€æ­¥å‡å°‘tokené¢„ç®—ï¼Œå¯¹CoTè¿›è¡Œå¤šè½®å‹ç¼©ã€‚3) æ€§èƒ½é¢„æµ‹ï¼šåˆ©ç”¨è®­ç»ƒé›†ä¸Šçš„å›°æƒ‘åº¦å’Œå‹ç¼©ç‡ç­‰ç‰¹å¾ï¼Œé¢„æµ‹æµ‹è¯•æ—¶æ€§èƒ½ã€‚4) è‡ªé€‚åº”è°ƒæ•´ï¼šæ ¹æ®æ€§èƒ½é¢„æµ‹ç»“æœï¼Œè‡ªé€‚åº”åœ°é€‰æ‹©æœ€ä½³çš„å‹ç¼©æ·±åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šMACCçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†å¤šè½®ç»†åŒ–çš„CoTå‹ç¼©æ–¹æ³•ï¼Œèƒ½å¤Ÿé€æ­¥ä¼˜åŒ–å‹ç¼©æ•ˆæœã€‚2) åˆ©ç”¨tokenå¼¹æ€§ç°è±¡ï¼Œé¿å…è¿‡åº¦å‹ç¼©å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚3) æå‡ºäº†åŸºäºå›°æƒ‘åº¦å’Œå‹ç¼©ç‡çš„æ€§èƒ½é¢„æµ‹æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼å‹ç¼©æ·±åº¦çš„é€‰æ‹©ã€‚4) å®ç°äº†è‡ªé€‚åº”çš„CoTå‹ç¼©ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒè¾“å…¥çš„ç‰¹æ€§è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¤šè½®å‹ç¼©é˜¶æ®µï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸åŒçš„tokené¢„ç®—è¿›è¡Œå‹ç¼©ï¼Œå¹¶ç›‘æ§æ¯ä¸€è½®çš„å‹ç¼©æ•ˆæœã€‚æ€§èƒ½é¢„æµ‹æ¨¡å‹ä½¿ç”¨äº†å›°æƒ‘åº¦å’Œå‹ç¼©ç‡ä½œä¸ºè¾“å…¥ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚è‡ªé€‚åº”è°ƒæ•´ç­–ç•¥åˆ™æ ¹æ®æ€§èƒ½é¢„æµ‹ç»“æœï¼Œé€‰æ‹©èƒ½å¤Ÿæœ€å¤§åŒ–å‡†ç¡®ç‡å¹¶æœ€å°åŒ–tokené•¿åº¦çš„å‹ç¼©æ·±åº¦ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMACCåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šï¼ŒMACCæ¯”æœ€å…ˆè¿›çš„åŸºçº¿æé«˜äº†5.6%ï¼ŒåŒæ—¶å¹³å‡å‡å°‘äº†47ä¸ªtokençš„CoTé•¿åº¦ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†æ€§èƒ½é¢„æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å¯ä»¥ä½¿ç”¨è®­ç»ƒé›†ä¸Šçš„ç‰¹å¾å¯é åœ°é¢„æµ‹æµ‹è¯•æ—¶æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMACCæ˜¯ä¸€ç§æœ‰æ•ˆä¸”å¯é¢„æµ‹çš„CoTå‹ç¼©æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MACCæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ™ºèƒ½é—®ç­”ã€çŸ¥è¯†å›¾è°±æ¨ç†ç­‰ã€‚é€šè¿‡å‹ç¼©CoTï¼Œå¯ä»¥æ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼ŒMACCçš„æ€§èƒ½é¢„æµ‹èƒ½åŠ›å¯ä»¥å¸®åŠ©å¼€å‘è€…é€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œå‹ç¼©ç­–ç•¥ï¼Œä»è€Œä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ã€‚è¯¥ç ”ç©¶å¯¹äºæ¨åŠ¨CoTæ¨ç†åœ¨å®é™…åº”ç”¨ä¸­çš„è½åœ°å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-Thought (CoT) reasoning improves performance on complex tasks but introduces significant inference latency due to verbosity. We propose Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that leverages the token elasticity phenomenon--where overly small token budgets can paradoxically increase output length--to progressively compress CoTs via multiround refinement. This adaptive strategy allows MACC to determine the optimal compression depth for each input. Our method achieves an average accuracy improvement of 5.6 percent over state-of-the-art baselines, while also reducing CoT length by an average of 47 tokens and significantly lowering latency. Furthermore, we show that test-time performance--accuracy and token length--can be reliably predicted using interpretable features like perplexity and compression rate on the training set. Evaluated across different models, our method enables efficient model selection and forecasting without repeated fine-tuning, demonstrating that CoT compression is both effective and predictable. Our code will be released in https://github.com/Leon221220/MACC.

