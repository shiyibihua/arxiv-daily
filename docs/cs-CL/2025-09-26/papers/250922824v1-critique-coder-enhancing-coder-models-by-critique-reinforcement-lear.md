---
layout: default
title: Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning
---

# Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22824" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22824v1</a>
  <a href="https://arxiv.org/pdf/2509.22824.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22824v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22824v1', 'Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chi Ruan, Dongfu Jiang, Yubo Wang, Wenhu Chen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCritique-Coderï¼Œé€šè¿‡æ‰¹åˆ¤å¼ºåŒ–å­¦ä¹ æå‡ä»£ç ç”Ÿæˆæ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä»£ç ç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `æ‰¹åˆ¤å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒä»£ç ç”Ÿæˆæ¨¡å‹æ—¶ï¼Œç¼ºä¹å¯¹æ¨¡å‹æ‰¹åˆ¤å’Œåæ€èƒ½åŠ›çš„æ˜ç¡®åŸ¹å…»ã€‚
2. æå‡ºæ‰¹åˆ¤å¼ºåŒ–å­¦ä¹ (CRL)ï¼Œé€šè¿‡è®©æ¨¡å‹ç”Ÿæˆå¯¹ä»£ç çš„æ‰¹åˆ¤å¹¶æ ¹æ®æ‰¹åˆ¤çš„æ­£ç¡®æ€§è¿›è¡Œå¥–åŠ±ï¼Œæ¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. Critique-Coderåœ¨ä»£ç ç”Ÿæˆå’Œé€šç”¨æ¨ç†ä»»åŠ¡ä¸Šå‡ä¼˜äºä»…ä½¿ç”¨RLè®­ç»ƒçš„æ¨¡å‹ï¼Œè¡¨æ˜CRLèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„æ¨ç†å’Œæ‰¹åˆ¤èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)å·²æˆä¸ºä¸€ç§æµè¡Œçš„è®­ç»ƒèŒƒå¼ï¼Œå°¤å…¶æ˜¯åœ¨ä¸æ¨ç†æ¨¡å‹ç»“åˆä½¿ç”¨æ—¶ã€‚ç„¶è€Œï¼Œå®ƒä¸»è¦å…³æ³¨ç”Ÿæˆå“åº”ï¼Œç¼ºä¹æ˜ç¡®åŸ¹å…»æ‰¹åˆ¤æˆ–åæ€çš„æœºåˆ¶ã€‚æœ€è¿‘çš„ä¸€äº›ç ”ç©¶è¡¨æ˜ï¼Œæ˜ç¡®åœ°æ•™å¯¼å¤§å‹è¯­è¨€æ¨¡å‹(LLM)å¦‚ä½•è¿›è¡Œæ‰¹åˆ¤æ˜¯æœ‰ç›Šçš„ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰¹åˆ¤å¼ºåŒ–å­¦ä¹ (CRL)ï¼Œå…¶ä¸­æ¨¡å‹è¢«è¦æ±‚ä¸ºç»™å®šçš„(é—®é¢˜ï¼Œè§£å†³æ–¹æ¡ˆ)å¯¹ç”Ÿæˆæ‰¹åˆ¤ã€‚å¥–åŠ±å®Œå…¨å–å†³äºç”Ÿæˆçš„æ‰¹åˆ¤çš„æœ€ç»ˆåˆ¤æ–­æ ‡ç­¾æ˜¯å¦ä¸çœŸå®åˆ¤æ–­ä¸€è‡´ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†Critique-Coderï¼Œå®ƒé€šè¿‡å°†20%çš„æ ‡å‡†RLæ•°æ®æ›¿æ¢ä¸ºCRLæ•°æ®ï¼Œåœ¨RLå’ŒCRLçš„æ··åˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„åŸºå‡†ä¸Šå¯¹å¤šä¸ªæ¨¡å‹(Critique-Coder)è¿›è¡Œå¾®è°ƒå’Œè¯„ä¼°ï¼Œä»¥å±•ç¤ºå®ƒä»¬ç›¸å¯¹äºä»…ä½¿ç”¨RLçš„æ¨¡å‹çš„ä¼˜åŠ¿ã€‚ç»“æœè¡¨æ˜ï¼ŒCritique-Coderåœ¨æ‰€æœ‰è¯„ä¼°çš„åŸºå‡†ä¸Šå§‹ç»ˆä¼˜äºä»…ä½¿ç”¨RLçš„åŸºçº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„Critique-Coder-8Båœ¨LiveCodeBench (v5)ä¸Šå¯ä»¥è¾¾åˆ°60%ä»¥ä¸Šï¼Œä¼˜äºå…¶ä»–æ¨ç†æ¨¡å‹ï¼Œå¦‚DeepCoder-14Bå’ŒGPT-o1ã€‚é™¤äº†ä»£ç ç”Ÿæˆï¼ŒCritique-Coderè¿˜è¡¨ç°å‡ºå¢å¼ºçš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œè¿™ä½“ç°åœ¨å…¶åœ¨BBEHæ•°æ®é›†ä¸­çš„é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šçš„æ›´å¥½æ€§èƒ½ã€‚è¿™è¡¨æ˜åœ¨ç¼–ç æ•°æ®é›†ä¸Šåº”ç”¨CRLå¯ä»¥å¢å¼ºé€šç”¨æ¨ç†å’Œæ‰¹åˆ¤èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›å¯ä»¥è·¨å¹¿æ³›çš„ä»»åŠ¡è½¬ç§»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºCRLæ˜¯LLMæ¨ç†çš„æ ‡å‡†RLçš„ä¸€ä¸ªå¾ˆå¥½çš„è¡¥å……ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»£ç ç”Ÿæˆæ¨¡å‹ç¼ºä¹æœ‰æ•ˆæ‰¹åˆ¤å’Œåæ€èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦å…³æ³¨ç”Ÿæˆæ­£ç¡®çš„ä»£ç ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹å¯¹è‡ªèº«ä»£ç è¿›è¡Œè¯„ä¼°å’Œæ”¹è¿›çš„èƒ½åŠ›ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ‰¹åˆ¤ç¯èŠ‚ï¼Œè®©æ¨¡å‹å­¦ä¹ å¦‚ä½•è¯„ä¼°å’Œæ”¹è¿›è‡ªèº«ç”Ÿæˆçš„ä»£ç ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹éœ€è¦ä¸ºç»™å®šçš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆç”Ÿæˆæ‰¹åˆ¤ï¼Œå¹¶æ ¹æ®æ‰¹åˆ¤çš„æ­£ç¡®æ€§è·å¾—å¥–åŠ±ã€‚è¿™ç§æ–¹å¼èƒ½å¤Ÿä¿ƒä½¿æ¨¡å‹æ›´æ·±å…¥åœ°ç†è§£é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Œä»è€Œæå‡ä»£ç ç”Ÿæˆçš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCritique-Coderçš„è®­ç»ƒæ¡†æ¶æ˜¯åŸºäºå¼ºåŒ–å­¦ä¹ çš„ã€‚åœ¨æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡å¼•å…¥äº†æ‰¹åˆ¤å¼ºåŒ–å­¦ä¹ (CRL)çš„æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œå°†20%çš„æ ‡å‡†RLæ•°æ®æ›¿æ¢ä¸ºCRLæ•°æ®ã€‚åœ¨CRLé˜¶æ®µï¼Œæ¨¡å‹æ¥æ”¶é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆå¯¹è§£å†³æ–¹æ¡ˆçš„æ‰¹åˆ¤ï¼Œå¹¶æ ¹æ®æ‰¹åˆ¤çš„æ­£ç¡®æ€§è·å¾—å¥–åŠ±ã€‚æœ€ç»ˆçš„æ¨¡å‹é€šè¿‡æ··åˆRLå’ŒCRLçš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æ‰¹åˆ¤å¼ºåŒ–å­¦ä¹ (CRL)çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºä»£ç ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒä¸­ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒCRLèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„æ¨ç†å’Œæ‰¹åˆ¤èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCritique-Coderé€šè¿‡æ··åˆRLå’ŒCRLçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†æ€§èƒ½çš„æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨CRLé˜¶æ®µï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ã€‚è®ºæ–‡é‡‡ç”¨çš„å¥–åŠ±å‡½æ•°æ˜¯åŸºäºç”Ÿæˆçš„æ‰¹åˆ¤çš„æœ€ç»ˆåˆ¤æ–­æ ‡ç­¾æ˜¯å¦ä¸çœŸå®åˆ¤æ–­ä¸€è‡´ã€‚å¦‚æœç”Ÿæˆçš„æ‰¹åˆ¤æ˜¯æ­£ç¡®çš„ï¼Œåˆ™ç»™äºˆæ­£å‘å¥–åŠ±ï¼›å¦åˆ™ï¼Œç»™äºˆè´Ÿå‘å¥–åŠ±ã€‚è¿™ç§å¥–åŠ±å‡½æ•°èƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹å­¦ä¹ å¦‚ä½•ç”Ÿæˆæ­£ç¡®çš„æ‰¹åˆ¤ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Critique-Coderåœ¨LiveCodeBench (v5)ä¸Šè¾¾åˆ°äº†è¶…è¿‡60%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºDeepCoder-14Bå’ŒGPT-o1ç­‰å…¶ä»–æ¨ç†æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒCritique-Coderåœ¨BBEHæ•°æ®é›†çš„é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCRLèƒ½å¤Ÿæœ‰æ•ˆæå‡ä»£ç ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§ä»£ç ç”Ÿæˆåœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘ã€ä»£ç è¡¥å…¨ã€ç¨‹åºä¿®å¤ç­‰ã€‚é€šè¿‡æå‡ä»£ç ç”Ÿæˆæ¨¡å‹çš„æ¨ç†å’Œæ‰¹åˆ¤èƒ½åŠ›ï¼Œå¯ä»¥æé«˜ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ï¼Œé™ä½è½¯ä»¶å¼€å‘çš„æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–éœ€è¦æ¨ç†å’Œæ‰¹åˆ¤èƒ½åŠ›çš„é¢†åŸŸï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) has emerged as a popular training paradigm, particularly when paired with reasoning models. While effective, it primarily focuses on generating responses and lacks mechanisms to explicitly foster critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT) and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly teaching LLMs how to critique. Motivated by them, we propose Critique Reinforcement Learning (CRL), where the model is tasked with generating a critique for a given (question, solution) pair. The reward is determined solely by whether the final judgment label $c \in \{\texttt{True}, \texttt{False}\}$ of the generated critique aligns with the ground-truth judgment $c^*$. Building on this point, we introduce \textsc{Critique-Coder}, which is trained on a hybrid of RL and CRL by substituting 20\% of the standard RL data with CRL data. We fine-tune multiple models (\textsc{Critique-Coder}) and evaluate them on different benchmarks to show their advantages over RL-only models. We show that \textsc{Critique-Coder} consistently outperforms RL-only baselines on all the evaluated benchmarks. Notably, our \textsc{Critique-Coder-8B} can reach over 60\% on LiveCodeBench (v5), outperforming other reasoning models like DeepCoder-14B and GPT-o1. Beyond code generation, \textsc{Critique-Coder} also demonstrates enhanced general reasoning abilities, as evidenced by its better performance on logic reasoning tasks from the BBEH dataset. This indicates that the application of CRL on coding datasets enhances general reasoning and critique abilities, which are transferable across a broad range of tasks. Hence, we believe that CRL works as a great complement to standard RL for LLM reasoning.

