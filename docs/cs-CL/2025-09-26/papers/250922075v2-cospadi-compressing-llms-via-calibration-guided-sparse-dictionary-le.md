---
layout: default
title: COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning
---

# COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22075" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.22075v2</a>
  <a href="https://arxiv.org/pdf/2509.22075.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22075v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22075v2', 'COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Dmitriy Shopkhoev, Denis Makhov, Magauiya Zhussip, Ammar Ali, Stamatios Lefkimmiatis

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-26 (Êõ¥Êñ∞: 2025-10-06)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫COSPADIÔºåÈÄöËøáÊ†°ÂáÜÂºïÂØºÁöÑÁ®ÄÁñèÂ≠óÂÖ∏Â≠¶‰π†ÂéãÁº©LLMÔºåÊèêÂçáÂéãÁº©ÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°ÂûãÂéãÁº©` `Á®ÄÁñèÂ≠óÂÖ∏Â≠¶‰π†` `Ê®°ÂûãÈáèÂåñ` `ÂêéËÆ≠ÁªÉÂéãÁº©` `ÁªìÊûÑÂåñÁ®ÄÁñèÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâLLMÂéãÁº©ÊñπÊ≥ï‰æùËµñ‰ΩéÁß©Ëøë‰ººÔºå‰ΩÜÂÖ∂ÁªìÊûÑÁ∫¶ÊùüËøá‰∫é‰∏•Ê†ºÔºåÂØºËá¥Ê®°ÂûãÁ≤æÂ∫¶ÊòæËëó‰∏ãÈôç„ÄÇ
2. CoSpaDiÈááÁî®Á®ÄÁñèÂ≠óÂÖ∏Â≠¶‰π†ÔºåÁî®Á®†ÂØÜÂ≠óÂÖ∏ÂíåÁ®ÄÁñèÁ≥ªÊï∞Áü©ÈòµË°®Á§∫ÊùÉÈáçÔºåÂÆûÁé∞Êõ¥ÁÅµÊ¥ªÁöÑÂ≠êÁ©∫Èó¥ËÅîÂêàË°®Á§∫„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåCoSpaDiÂú®LlamaÂíåQwenÊ®°Âûã‰∏ä‰ºò‰∫éÁé∞Êúâ‰ΩéÁß©ÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜÂéãÁº©ÊÄßËÉΩÂíåÊ®°ÂûãÁ≤æÂ∫¶„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫CoSpaDiÔºàÈÄöËøáÁ®ÄÁñèÂ≠óÂÖ∏Â≠¶‰π†ËøõË°åÂéãÁº©ÔºâÁöÑÂÖçËÆ≠ÁªÉÂéãÁº©Ê°ÜÊû∂ÔºåÁî®‰∫éÂéãÁº©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ„ÄÇCoSpaDi‰ΩøÁî®Á®†ÂØÜÂ≠óÂÖ∏ÂíåÂàóÁ®ÄÁñèÁ≥ªÊï∞Áü©ÈòµÊù•Ë°®Á§∫ÊùÉÈáçÁü©ÈòµÔºåÂèñ‰ª£‰∫Ü‰º†ÁªüÁöÑ‰ΩéÁß©ÂàÜËß£„ÄÇËøôÁßçÊñπÊ≥ïÂÆûÁé∞‰∫ÜÂ≠êÁ©∫Èó¥ËÅîÂêàË°®Á§∫ÔºåÂÖÅËÆ∏ÂéüÂßãÊùÉÈáçÁü©ÈòµÁöÑ‰∏çÂêåÂàóÂú®Ëá™ÈÄÇÂ∫îÈÄâÊã©ÁöÑÂ≠óÂÖ∏ÂéüÂ≠êÊâÄË∑®Ë∂äÁöÑ‰∏çÂêåÂ≠êÁ©∫Èó¥‰∏≠ËøõË°åËøë‰ººÔºåÊèê‰æõ‰∫ÜÊØîÂçï‰∏Ä‰∏çÂèòÂü∫Êõ¥Â§ßÁöÑË°®ËææËÉΩÂäõ„ÄÇCoSpaDiÂà©Áî®Â∞èÂûãÊ†°ÂáÜÊï∞ÊçÆÈõÜ‰ºòÂåñÂàÜËß£Ôºå‰ΩøÂéãÁº©ÊäïÂΩ±Â±ÇÁöÑËæìÂá∫ÊøÄÊ¥ª‰∏éÂéüÂßãÊøÄÊ¥ªÁ¥ßÂØÜÂåπÈÖçÔºå‰ªéËÄåÊúÄÂ∞èÂåñÂäüËÉΩÈáçÂª∫ËØØÂ∑Æ„ÄÇËøôÁßçÊï∞ÊçÆÊÑüÁü•Á≠ñÁï•Âú®ÂêàÁêÜÁöÑÂéãÁº©Áéá‰∏ã‰øùÊåÅ‰∫ÜÊõ¥Â•ΩÁöÑÊ®°Âûã‰øùÁúüÂ∫¶ÔºåÊó†ÈúÄ‰ªª‰ΩïÂæÆË∞É„ÄÇÊ≠§Â§ñÔºåÁî±Ê≠§‰∫ßÁîüÁöÑÁªìÊûÑÂåñÁ®ÄÁñèÊÄßÂÖÅËÆ∏È´òÊïàÁöÑÁ®ÄÁñè-Á®†ÂØÜÁü©Èòµ‰πòÊ≥ïÔºåÂπ∂‰∏éÂêéËÆ≠ÁªÉÈáèÂåñÂÖºÂÆπÔºå‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÈ´òÂÜÖÂ≠òÂíåÂª∂ËøüÊÄßËÉΩ„ÄÇÂú®Â§ö‰∏™LlamaÂíåQwenÊ®°Âûã‰∏äÔºå‰ª•20-50%ÁöÑÂéãÁº©ÁéáËøõË°åËØÑ‰º∞ÔºåÁªìÊûúË°®ÊòéCoSpaDiÂú®ÂáÜÁ°ÆÊÄßÂíåÂõ∞ÊÉëÂ∫¶ÊñπÈù¢Âùá‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊï∞ÊçÆÊÑüÁü•‰ΩéÁß©ÊñπÊ≥ï„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÁªìÊûÑÂåñÁ®ÄÁñèÂ≠óÂÖ∏Â≠¶‰π†ÊòØÈ´òÊïàLLMÈÉ®ÁΩ≤‰∏≠‰º†Áªü‰ΩéÁß©ÊñπÊ≥ïÁöÑÂº∫Â§ßÊõø‰ª£ÊñπÊ°à„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂéãÁº©ÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫é‰ΩéÁß©ÂàÜËß£ÁöÑÊñπÊ≥ïÔºåËôΩÁÑ∂ËÆ°ÁÆóÊïàÁéáÈ´òÔºå‰ΩÜÁî±‰∫éÂÖ∂Âõ∫ÂÆöÁöÑÁªìÊûÑÁ∫¶ÊùüÔºåÂú®ÂéãÁº©ËøáÁ®ã‰∏≠ÂÆπÊòìÂØºËá¥Ê®°ÂûãÁ≤æÂ∫¶ÊòæËëó‰∏ãÈôç„ÄÇËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏Â∞ÜÊùÉÈáçÁü©ÈòµÁöÑÊØè‰∏ÄÂàóÊäïÂΩ±Âà∞Âêå‰∏Ä‰∏™‰ΩéÁª¥Â≠êÁ©∫Èó¥ÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõÔºåÊó†Ê≥ïÂÖÖÂàÜÊçïÊçâÂéüÂßãÊùÉÈáçÁü©ÈòµÁöÑÂ§çÊùÇÁªìÊûÑ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCoSpaDiÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÁî®‰∏Ä‰∏™Êõ¥ÁÅµÊ¥ªÁöÑÁªìÊûÑÂåñÁ®ÄÁñèÂàÜËß£Êù•Êõø‰ª£‰º†ÁªüÁöÑ‰ΩéÁß©ÂàÜËß£„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉ‰ΩøÁî®‰∏Ä‰∏™Á®†ÂØÜÂ≠óÂÖ∏Âíå‰∏Ä‰∏™ÂàóÁ®ÄÁñèÁöÑÁ≥ªÊï∞Áü©ÈòµÊù•Ë°®Á§∫ÊØè‰∏™ÊùÉÈáçÁü©Èòµ„ÄÇËøôÁßçÊñπÊ≥ïÂÖÅËÆ∏ÂéüÂßãÊùÉÈáçÁü©ÈòµÁöÑ‰∏çÂêåÂàóÂú®Áî±Ëá™ÈÄÇÂ∫îÈÄâÊã©ÁöÑÂ≠óÂÖ∏ÂéüÂ≠êÊâÄÂº†ÊàêÁöÑ‰∏çÂêåÂ≠êÁ©∫Èó¥‰∏≠ËøõË°åËøë‰ººÔºå‰ªéËÄåÂÆûÁé∞‰∫Ü‰∏ÄÁßçÂ≠êÁ©∫Èó¥ËÅîÂêàË°®Á§∫„ÄÇËøôÁßçË°®Á§∫ÊñπÊ≥ïÊØîÂçï‰∏Ä‰∏çÂèòÂü∫ÂÖ∑ÊúâÊõ¥Â§ßÁöÑË°®ËææËÉΩÂäõÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞‰øùÁïôÂéüÂßãÊùÉÈáçÁü©ÈòµÁöÑ‰ø°ÊÅØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCoSpaDiÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) **Â≠óÂÖ∏Â≠¶‰π†**Ôºö‰ΩøÁî®‰∏Ä‰∏™Á®†ÂØÜÂ≠óÂÖ∏Êù•Ë°®Á§∫ÊùÉÈáçÁü©Èòµ„ÄÇ2) **Á®ÄÁñèÁºñÁ†Å**Ôºö‰∏∫ÊØè‰∏™ÊùÉÈáçÁü©ÈòµÊâæÂà∞‰∏Ä‰∏™ÂàóÁ®ÄÁñèÁöÑÁ≥ªÊï∞Áü©ÈòµÔºå‰ΩøÂæóÂ≠óÂÖ∏ÂíåÁ≥ªÊï∞Áü©ÈòµÁöÑ‰πòÁßØËÉΩÂ§üËøë‰ººÂéüÂßãÁöÑÊùÉÈáçÁü©Èòµ„ÄÇ3) **Ê†°ÂáÜ‰ºòÂåñ**Ôºö‰ΩøÁî®‰∏Ä‰∏™Â∞èÁöÑÊ†°ÂáÜÊï∞ÊçÆÈõÜÊù•‰ºòÂåñÂ≠óÂÖ∏ÂíåÁ≥ªÊï∞Áü©ÈòµÔºå‰ΩøÂæóÂéãÁº©ÂêéÁöÑÊäïÂΩ±Â±ÇÁöÑËæìÂá∫ÊøÄÊ¥ª‰∏éÂéüÂßãÊ®°ÂûãÁöÑËæìÂá∫ÊøÄÊ¥ªÂ∞ΩÂèØËÉΩÊé•Ëøë„ÄÇËøô‰∏™ËøáÁ®ãÊó®Âú®ÊúÄÂ∞èÂåñÂäüËÉΩÈáçÂª∫ËØØÂ∑ÆÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖËøë‰ººÊùÉÈáç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCoSpaDiÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂÆÉ‰ΩøÁî®ÁªìÊûÑÂåñÁ®ÄÁñèÂ≠óÂÖ∏Â≠¶‰π†Êù•Ë°®Á§∫ÊùÉÈáçÁü©ÈòµÔºåËÄå‰∏çÊòØ‰º†ÁªüÁöÑ‰ΩéÁß©ÂàÜËß£„ÄÇËøôÁßçÊñπÊ≥ïÂÖÅËÆ∏Ê®°ÂûãÂú®‰∏çÂêåÁöÑÂ≠êÁ©∫Èó¥‰∏≠Ëøë‰ºº‰∏çÂêåÁöÑÊùÉÈáçÂàóÔºå‰ªéËÄåÊèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄßÂíåË°®ËææËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåCoSpaDiËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊï∞ÊçÆÊÑüÁü•ÁöÑÊ†°ÂáÜ‰ºòÂåñÁ≠ñÁï•ÔºåÈÄöËøáÊúÄÂ∞èÂåñÂéãÁº©Ê®°ÂûãÁöÑËæìÂá∫ÊøÄÊ¥ª‰∏éÂéüÂßãÊ®°ÂûãÁöÑËæìÂá∫ÊøÄÊ¥ª‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊù•ÊèêÈ´òÂéãÁº©ÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCoSpaDiÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) **Á®ÄÁñèÊÄßÁ∫¶Êùü**ÔºöÈÄöËøáÂØπÁ≥ªÊï∞Áü©ÈòµÊñΩÂä†Á®ÄÁñèÊÄßÁ∫¶ÊùüÔºåÂèØ‰ª•ÂáèÂ∞ëÊ®°ÂûãÁöÑÂ≠òÂÇ®Á©∫Èó¥ÂíåËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ2) **Ê†°ÂáÜÊï∞ÊçÆÈõÜ**Ôºö‰ΩøÁî®‰∏Ä‰∏™Â∞èÁöÑÊ†°ÂáÜÊï∞ÊçÆÈõÜÊù•‰ºòÂåñÂ≠óÂÖ∏ÂíåÁ≥ªÊï∞Áü©ÈòµÔºå‰ΩøÂæóÂéãÁº©Ê®°ÂûãÁöÑËæìÂá∫ÊøÄÊ¥ª‰∏éÂéüÂßãÊ®°ÂûãÁöÑËæìÂá∫ÊøÄÊ¥ªÂ∞ΩÂèØËÉΩÊé•Ëøë„ÄÇ3) **ÊçüÂ§±ÂáΩÊï∞**Ôºö‰ΩøÁî®ÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâ‰Ωú‰∏∫ÊçüÂ§±ÂáΩÊï∞ÔºåÊù•Ë°°ÈáèÂéãÁº©Ê®°ÂûãÁöÑËæìÂá∫ÊøÄÊ¥ª‰∏éÂéüÂßãÊ®°ÂûãÁöÑËæìÂá∫ÊøÄÊ¥ª‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇ4) **ÂéãÁº©ÊØî‰æã**ÔºöÈÄöËøáË∞ÉÊï¥Á®ÄÁñèÊÄßÁ∫¶ÊùüÁöÑÂº∫Â∫¶Êù•ÊéßÂà∂ÂéãÁº©ÊØî‰æã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CoSpaDiÂú®LlamaÂíåQwenÊ®°Âûã‰∏äËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÔºåÂú®20-50%ÁöÑÂéãÁº©Áéá‰∏ãÔºåCoSpaDiÂú®ÂáÜÁ°ÆÊÄßÂíåÂõ∞ÊÉëÂ∫¶ÊñπÈù¢Âùá‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊï∞ÊçÆÊÑüÁü•‰ΩéÁß©ÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∫õÊ®°Âûã‰∏äÔºåCoSpaDiÂú®‰øùÊåÅÁõ∏ÂêåÂáÜÁ°ÆÁéáÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆûÁé∞‰∫ÜÊØîÁé∞ÊúâÊñπÊ≥ïÊõ¥È´òÁöÑÂéãÁº©ÁéáÔºåÊàñËÄÖÂú®Áõ∏ÂêåÂéãÁº©Áéá‰∏ãÔºåÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÂáÜÁ°ÆÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CoSpaDiÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÉ®ÁΩ≤ÂíåÂ∫îÁî®‰∏≠ÂÖ∑ÊúâÂπøÊ≥õÁöÑÊΩúÂäõ„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÂú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÈÉ®ÁΩ≤LLMÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§áÂíåËæπÁºòËÆ°ÁÆóËÆæÂ§á„ÄÇÊ≠§Â§ñÔºåCoSpaDiËøòÂèØ‰ª•Áî®‰∫éÂä†ÈÄüLLMÁöÑÊé®ÁêÜÈÄüÂ∫¶Ôºå‰ªéËÄåÊèêÈ´òÁî®Êà∑‰ΩìÈ™å„ÄÇËØ•ÊñπÊ≥ïÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊô∫ËÉΩÂØπËØùÁ≥ªÁªü„ÄÅÊñáÊú¨ÁîüÊàêÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÈáçË¶ÅÁöÑÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Post-training compression of large language models (LLMs) largely relies on low-rank weight approximation, which represents each column of a weight matrix in a shared low-dimensional subspace. While this is a computationally efficient strategy, the imposed structural constraint is rigid and can lead to a noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression via Sparse Dictionary Learning), a novel training-free compression framework that replaces low-rank decomposition with a more flexible structured sparse factorization in which each weight matrix is represented with a dense dictionary and a column-sparse coefficient matrix. This formulation enables a union-of-subspaces representation: different columns of the original weight matrix are approximated in distinct subspaces spanned by adaptively selected dictionary atoms, offering greater expressiveness than a single invariant basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the factorization such that the output activations of compressed projection layers closely match those of the original ones, thereby minimizing functional reconstruction error rather than mere weight approximation. This data-aware strategy preserves better model fidelity without any fine-tuning under reasonable compression ratios. Moreover, the resulting structured sparsity allows efficient sparse-dense matrix multiplication and is compatible with post-training quantization for further memory and latency gains. We evaluate CoSpaDi across multiple Llama and Qwen models under per-layer and per-group settings at 20-50\% compression ratios, demonstrating consistent superiority over state-of-the-art data-aware low-rank methods both in accuracy and perplexity. Our results establish structured sparse dictionary learning as a powerful alternative to conventional low-rank approaches for efficient LLM deployment.

