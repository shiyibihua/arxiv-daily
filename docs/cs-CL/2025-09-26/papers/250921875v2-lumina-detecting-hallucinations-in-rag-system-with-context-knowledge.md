---
layout: default
title: LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals
---

# LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21875" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21875v2</a>
  <a href="https://arxiv.org/pdf/2509.21875.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21875v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21875v2', 'LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Samuel Yeh, Sharon Li, Tanwi Mallick

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-10-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LUMINAï¼šåˆ©ç”¨ä¸Šä¸‹æ–‡-çŸ¥è¯†ä¿¡å·æ£€æµ‹RAGç³»ç»Ÿä¸­çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¹»è§‰æ£€æµ‹` `ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡` `çŸ¥è¯†åˆ©ç”¨ç‡` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RAGç³»ç»Ÿä»å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œç°æœ‰æ£€æµ‹æ–¹æ³•ä¾èµ–å¤§é‡è¶…å‚æ•°è°ƒæ•´ï¼Œæ³›åŒ–æ€§å—é™ã€‚
2. LUMINAé€šè¿‡é‡åŒ–å¤–éƒ¨ä¸Šä¸‹æ–‡å’Œå†…éƒ¨çŸ¥è¯†çš„åˆ©ç”¨ç‡æ¥æ£€æµ‹å¹»è§‰ï¼Œæ— éœ€å¤§é‡è¶…å‚æ•°è°ƒæ•´ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLUMINAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”å¯¹æ£€ç´¢è´¨é‡å’Œæ¨¡å‹åŒ¹é…å…·æœ‰é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ—¨åœ¨é€šè¿‡å°†å“åº”å»ºç«‹åœ¨æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸Šæ¥å‡è½»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰ã€‚ç„¶è€Œï¼Œå³ä½¿æä¾›äº†æ­£ç¡®ä¸”å……åˆ†çš„ä¸Šä¸‹æ–‡ï¼ŒåŸºäºRAGçš„LLMä»ç„¶ä¼šäº§ç”Ÿå¹»è§‰ã€‚è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™æºäºæ¨¡å‹ä½¿ç”¨å¤–éƒ¨ä¸Šä¸‹æ–‡å’Œå†…éƒ¨çŸ¥è¯†ä¹‹é—´çš„ä¸å¹³è¡¡ï¼Œå¹¶ä¸”ä¸€äº›æ–¹æ³•è¯•å›¾é‡åŒ–è¿™äº›ä¿¡å·ä»¥è¿›è¡Œå¹»è§‰æ£€æµ‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡çš„è¶…å‚æ•°è°ƒæ•´ï¼Œé™åˆ¶äº†å®ƒä»¬çš„æ³›åŒ–æ€§ã€‚æˆ‘ä»¬æå‡ºäº†LUMINAï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡-çŸ¥è¯†ä¿¡å·æ£€æµ‹RAGç³»ç»Ÿä¸­çš„å¹»è§‰ï¼šå¤–éƒ¨ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡é€šè¿‡åˆ†å¸ƒè·ç¦»æ¥é‡åŒ–ï¼Œè€Œå†…éƒ¨çŸ¥è¯†åˆ©ç”¨ç‡é€šè¿‡è·Ÿè¸ªé¢„æµ‹çš„tokenåœ¨transformerå±‚ä¸­çš„æ¼”å˜æ¥è¡¡é‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ä¸ªæ¡†æ¶æ¥ç»Ÿè®¡éªŒè¯è¿™äº›æµ‹é‡ç»“æœã€‚åœ¨å¸¸è§çš„RAGå¹»è§‰åŸºå‡†å’Œå››ä¸ªå¼€æºLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLUMINAå®ç°äº†å§‹ç»ˆå¦‚ä¸€çš„é«˜AUROCå’ŒAUPRCåˆ†æ•°ï¼Œåœ¨HalluRAGä¸Šä¼˜äºå…ˆå‰çš„åŸºäºåˆ©ç”¨ç‡çš„æ–¹æ³•é«˜è¾¾+13% AUROCã€‚æ­¤å¤–ï¼ŒLUMINAåœ¨å…³äºæ£€ç´¢è´¨é‡å’Œæ¨¡å‹åŒ¹é…çš„å®½æ¾å‡è®¾ä¸‹ä»ç„¶å…·æœ‰é²æ£’æ€§ï¼Œæä¾›äº†æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­LLMäº§ç”Ÿçš„å¹»è§‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºåˆ©ç”¨ç‡çš„æ–¹æ³•ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„è¶…å‚æ•°è°ƒæ•´ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒåœºæ™¯å’Œæ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°é‡åŒ–å¤–éƒ¨ä¸Šä¸‹æ–‡å’Œå†…éƒ¨çŸ¥è¯†çš„åˆ©ç”¨ç‡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLUMINAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é‡åŒ–å¤–éƒ¨ä¸Šä¸‹æ–‡å’Œå†…éƒ¨çŸ¥è¯†çš„åˆ©ç”¨ç‡æ¥åˆ¤æ–­æ˜¯å¦å­˜åœ¨å¹»è§‰ã€‚å¦‚æœæ¨¡å‹è¿‡åº¦ä¾èµ–å†…éƒ¨çŸ¥è¯†è€Œå¿½ç•¥æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œåˆ™æ›´æœ‰å¯èƒ½äº§ç”Ÿå¹»è§‰ã€‚é€šè¿‡åˆ†åˆ«è¡¡é‡ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡å’ŒçŸ¥è¯†åˆ©ç”¨ç‡ï¼Œå¹¶ç»“åˆç»Ÿè®¡éªŒè¯ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°æ£€æµ‹å¹»è§‰ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLUMINAæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸Šä¸‹æ–‡åˆ©ç”¨ç‡é‡åŒ–å’Œå†…éƒ¨çŸ¥è¯†åˆ©ç”¨ç‡é‡åŒ–ã€‚ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡é€šè¿‡è®¡ç®—ç”Ÿæˆæ–‡æœ¬çš„tokenåˆ†å¸ƒä¸æ£€ç´¢æ–‡æ¡£çš„tokenåˆ†å¸ƒä¹‹é—´çš„è·ç¦»æ¥è¡¡é‡ã€‚å†…éƒ¨çŸ¥è¯†åˆ©ç”¨ç‡é€šè¿‡è·Ÿè¸ªé¢„æµ‹tokenåœ¨Transformerå±‚ä¸­çš„å˜åŒ–æ¥è¡¡é‡ï¼Œå¦‚æœtokenåœ¨æ—©æœŸå±‚å°±ç¡®å®šï¼Œåˆ™è¡¨æ˜æ¨¡å‹æ›´å¤šåœ°ä¾èµ–å†…éƒ¨çŸ¥è¯†ã€‚æœ€åï¼Œä½¿ç”¨ç»Ÿè®¡éªŒè¯æ¡†æ¶æ¥è¯„ä¼°è¿™äº›æµ‹é‡ç»“æœçš„æ˜¾è‘—æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šLUMINAçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ— éœ€å¤§é‡è¶…å‚æ•°è°ƒæ•´å³å¯æœ‰æ•ˆé‡åŒ–ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†åˆ©ç”¨ç‡çš„æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨åˆ†å¸ƒè·ç¦»æ¥è¡¡é‡ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡ï¼Œå¹¶è·Ÿè¸ªtokenåœ¨Transformerå±‚ä¸­çš„æ¼”å˜æ¥è¡¡é‡çŸ¥è¯†åˆ©ç”¨ç‡ï¼ŒLUMINAèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ£€æµ‹å¹»è§‰ã€‚æ­¤å¤–ï¼Œç»Ÿè®¡éªŒè¯æ¡†æ¶è¿›ä¸€æ­¥æé«˜äº†æ£€æµ‹çš„å¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šä¸Šä¸‹æ–‡åˆ©ç”¨ç‡çš„é‡åŒ–ä½¿ç”¨äº†Jensen-Shannonæ•£åº¦æ¥è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œæ£€ç´¢æ–‡æ¡£çš„tokenåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚å†…éƒ¨çŸ¥è¯†åˆ©ç”¨ç‡çš„é‡åŒ–é€šè¿‡è®¡ç®—æ¯ä¸ªtokenåœ¨ä¸åŒTransformerå±‚ä¹‹é—´çš„é¢„æµ‹æ¦‚ç‡å˜åŒ–æ¥è¡¡é‡ã€‚ç»Ÿè®¡éªŒè¯æ¡†æ¶ä½¿ç”¨bootstrapæ–¹æ³•æ¥è¯„ä¼°æµ‹é‡ç»“æœçš„æ˜¾è‘—æ€§ï¼Œå¹¶è®¾ç½®é˜ˆå€¼æ¥åˆ¤æ–­æ˜¯å¦å­˜åœ¨å¹»è§‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LUMINAåœ¨HalluRAGç­‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒAUROCæŒ‡æ ‡æœ€é«˜æå‡äº†13%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLUMINAä¼˜äºç°æœ‰çš„åŸºäºåˆ©ç”¨ç‡çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ£€ç´¢è´¨é‡å’Œæ¨¡å‹åŒ¹é…çš„å®½æ¾å‡è®¾ä¸‹ä»ç„¶å…·æœ‰é²æ£’æ€§ã€‚è¿™è¡¨æ˜LUMINAå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LUMINAå¯åº”ç”¨äºå„ç§åŸºäºRAGçš„LLMåº”ç”¨ä¸­ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦å’Œå¯¹è¯ç”Ÿæˆã€‚é€šè¿‡æ£€æµ‹å’Œå‡å°‘å¹»è§‰ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶å¯¹äºæ„å»ºæ›´å€¼å¾—ä¿¡èµ–å’Œå®ç”¨çš„LLMç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶æœ‰åŠ©äºæ¨åŠ¨LLMåœ¨å®é™…åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large language models (LLMs) by grounding responses in retrieved documents. Yet, RAG-based LLMs still hallucinate even when provided with correct and sufficient context. A growing line of work suggests that this stems from an imbalance between how models use external context and their internal knowledge, and several approaches have attempted to quantify these signals for hallucination detection. However, existing methods require extensive hyperparameter tuning, limiting their generalizability. We propose LUMINA, a novel framework that detects hallucinations in RAG systems through context-knowledge signals: external context utilization is quantified via distributional distance, while internal knowledge utilization is measured by tracking how predicted tokens evolve across transformer layers. We further introduce a framework for statistically validating these measurements. Experiments on common RAG hallucination benchmarks and four open-source LLMs show that LUMINA achieves consistently high AUROC and AUPRC scores, outperforming prior utilization-based methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under relaxed assumptions about retrieval quality and model matching, offering both effectiveness and practicality.

