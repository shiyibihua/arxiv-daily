---
layout: default
title: We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong
---

# We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22510" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22510v1</a>
  <a href="https://arxiv.org/pdf/2509.22510.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22510v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22510v1', 'We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gautam Siddharth Kashyap, Mark Dras, Usman Naseem

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”å¤šåˆ†æ”¯å¼•å¯¼ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯¹é½æŠ€æœ¯` `å¤šç›®æ ‡ä¼˜åŒ–` `è‡ªé€‚åº”å¼•å¯¼` `å®‰å…¨æ€§` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä¼˜åŒ–å•ä¸€å¯¹é½ç›®æ ‡æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´å…¶ä»–ç›®æ ‡çš„è¡¨ç¤ºè¢«è¦†ç›–ï¼Œé€ æˆç¾éš¾æ€§é—å¿˜ã€‚
2. æœ¬æ–‡æå‡ºè‡ªé€‚åº”å¤šåˆ†æ”¯å¼•å¯¼ï¼ˆAMBSï¼‰ï¼Œé€šè¿‡å…±äº«è¡¨ç¤ºå’Œç­–ç•¥å‚è€ƒæœºåˆ¶ï¼Œå®ç°å¤šç›®æ ‡å¯¹é½çš„ä¸€è‡´æ€§å’Œé«˜æ•ˆæ€§ã€‚
3. åœ¨å¤šä¸ª7B LLMåŸºç¡€æ¨¡å‹ä¸Šï¼ŒAMBSæ˜¾è‘—æå‡äº†HHHå¯¹é½æ•ˆæœï¼Œå°¤å…¶åœ¨DeepSeek-7Bä¸Šï¼Œå¹³å‡å¯¹é½åˆ†æ•°æé«˜32.4%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹é½ï¼Œå°¤å…¶æ˜¯åœ¨æœ‰ç”¨æ€§ã€æ— å®³æ€§å’Œè¯šå®æ€§ï¼ˆHHHï¼‰æ–¹é¢ï¼Œå¯¹äºå®‰å…¨å¯é çš„éƒ¨ç½²è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å‘éšè—çŠ¶æ€æ³¨å…¥æ§åˆ¶ä¿¡å·æ¥å¼•å¯¼LLMè¾“å‡ºï¼Œä½†é€šå¸¸ä¼šå¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€‚æœ¬æ–‡æå‡ºè‡ªé€‚åº”å¤šåˆ†æ”¯å¼•å¯¼ï¼ˆAMBSï¼‰ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„1å¯¹Næ¡†æ¶å®ç°ç»Ÿä¸€é«˜æ•ˆçš„å¤šç›®æ ‡å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAMBSåœ¨å¤šä¸ª7B LLMåŸºç¡€æ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†HHHå¯¹é½æ•ˆæœï¼Œå°¤å…¶åœ¨DeepSeek-7Bä¸Šï¼Œå¹³å‡å¯¹é½åˆ†æ•°æé«˜äº†32.4%ï¼Œä¸å®‰å…¨è¾“å‡ºå‡å°‘äº†11.0%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯¹é½å¤šä¸ªç›®æ ‡æ—¶çš„ç¾éš¾æ€§é—å¿˜å’Œæ¨ç†ç¢ç‰‡åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¼˜åŒ–å•ä¸€ç›®æ ‡æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´å…¶ä»–ç›®æ ‡çš„è¡¨ç¤ºè¢«è¦†ç›–ï¼Œå½±å“æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè‡ªé€‚åº”å¤šåˆ†æ”¯å¼•å¯¼ï¼ˆAMBSï¼‰ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„1å¯¹Næ¡†æ¶ï¼Œé¦–å…ˆè®¡ç®—å…±äº«è¡¨ç¤ºï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œç›®æ ‡ç‰¹å®šçš„å¼•å¯¼ï¼Œä»è€Œå®ç°è·¨ç›®æ ‡çš„ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAMBSçš„æ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µè®¡ç®—Transformerå±‚çš„åæ³¨æ„åŠ›éšè—çŠ¶æ€ä»¥å½¢æˆå…±äº«è¡¨ç¤ºï¼›ç¬¬äºŒé˜¶æ®µå°†è¯¥è¡¨ç¤ºå…‹éš†åˆ°å¤šä¸ªå¹¶è¡Œåˆ†æ”¯ï¼Œå¹¶é€šè¿‡ç­–ç•¥å‚è€ƒæœºåˆ¶è¿›è¡Œå¼•å¯¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šAMBSçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å…±äº«è¡¨ç¤ºå’Œç­–ç•¥å‚è€ƒæœºåˆ¶ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å„ç›®æ ‡ç‹¬ç«‹ä¼˜åŒ–å¯¼è‡´çš„æ¨ç†ä¸ä¸€è‡´é—®é¢˜ï¼Œå®ç°äº†å¤šç›®æ ‡å¯¹é½çš„ç»Ÿä¸€æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒAMBSé‡‡ç”¨äº†å…±äº«è¡¨ç¤ºçš„è®¡ç®—æ–¹å¼ï¼Œç¡®ä¿äº†å„ç›®æ ‡ä¹‹é—´çš„è¡¨ç¤ºä¸€è‡´æ€§ï¼ŒåŒæ—¶åœ¨å¼•å¯¼è¿‡ç¨‹ä¸­ä½¿ç”¨äº†ç­–ç•¥å‚è€ƒæœºåˆ¶ï¼Œä»¥å®ç°ç›®æ ‡ç‰¹å®šçš„æ§åˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAMBSåœ¨DeepSeek-7Bæ¨¡å‹ä¸Šå¹³å‡å¯¹é½åˆ†æ•°æé«˜äº†32.4%ï¼Œä¸å®‰å…¨è¾“å‡ºå‡å°‘äº†11.0%ã€‚ä¸ä¼ ç»Ÿçš„1å¯¹NåŸºçº¿ç›¸æ¯”ï¼ŒAMBSåœ¨HHHå¯¹é½æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ä¿æŒç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¯¹è¯ç³»ç»Ÿã€å†…å®¹ç”Ÿæˆå’Œè‡ªåŠ¨é—®ç­”ç­‰ï¼Œèƒ½å¤Ÿæé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„å¯¹é½ç­–ç•¥ï¼Œæœªæ¥å¯èƒ½åœ¨æ›´å¹¿æ³›çš„äººå·¥æ™ºèƒ½åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œä¿ƒè¿›äººæœºäº¤äº’çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Alignment of Large Language Models (LLMs) along multiple objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe and reliable deployment. Prior work has used steering vector-small control signals injected into hidden states-to guide LLM outputs, typically via one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single alignment objective can inadvertently overwrite representations learned for other objectives, leading to catastrophic forgetting. More recent approaches extend steering vectors via one-to-many (1-to-N) Transformer decoders. While this alleviates catastrophic forgetting, naive multi-branch designs optimize each objective independently, which can cause inference fragmentation-outputs across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch Steering (AMBS), a two-stage 1-to-N framework for unified and efficient multi-objective alignment. In Stage I, post-attention hidden states of the Transformer layer are computed once to form a shared representation. In Stage II, this representation is cloned into parallel branches and steered via a policy-reference mechanism, enabling objective-specific control while maintaining cross-objective consistency. Empirical evaluations on Alpaca, BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared to a naive 1-to-N baseline, while remaining competitive with state-of-the-art methods.

