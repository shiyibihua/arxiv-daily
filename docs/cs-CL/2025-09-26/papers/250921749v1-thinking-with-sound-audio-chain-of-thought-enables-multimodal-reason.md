---
layout: default
title: Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models
---

# Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21749" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21749v1</a>
  <a href="https://arxiv.org/pdf/2509.21749.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21749v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21749v1', 'Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhen Xiong, Yujun Cai, Zhecheng Li, Junsong Yuan, Yiwei Wang

**åˆ†ç±»**: cs.CL, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºThinking-with-Soundæ¡†æ¶ï¼Œå¢å¼ºLALMåœ¨å¤æ‚å£°å­¦åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€æ¨ç†` `é“¾å¼æ€è€ƒ` `é²æ£’æ€§` `å£°å­¦åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LALMåœ¨å¤æ‚å£°å­¦åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œç¼ºä¹å¯¹éŸ³é¢‘ä¿¡å·çš„æœ‰æ•ˆåˆ†æå’Œå¤„ç†å·¥å…·ã€‚
2. TwSæ¡†æ¶é€šè¿‡ç»“åˆè¯­è¨€æ¨ç†å’Œå®æ—¶éŸ³é¢‘åŸŸåˆ†æï¼Œä½¿LALMèƒ½å¤Ÿä¸»åŠ¨â€œæ€è€ƒâ€éŸ³é¢‘ï¼Œè¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚
3. åœ¨MELD-Hard1kåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTwSæ˜¾è‘—æé«˜äº†LALMçš„é²æ£’æ€§ï¼Œå‡†ç¡®ç‡æå‡é«˜è¾¾36.61%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¤§å‹éŸ³é¢‘-è¯­è¨€æ¨¡å‹(LALM)åœ¨è¯­éŸ³ç¿»è¯‘å’ŒéŸ³é¢‘é—®ç­”ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚å£°å­¦åœºæ™¯ä¸­çš„éŸ³é¢‘æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—å±€é™ã€‚è¿™äº›åœºæ™¯é€šå¸¸éœ€è¦å™ªå£°æŠ‘åˆ¶ã€å£°æºåˆ†ç¦»å’Œç²¾ç¡®çš„æ—¶é—´å¯¹é½ç­‰å£°å­¦å·¥å…·ï¼Œè€Œç›®å‰çš„LALMç¼ºä¹è¿™äº›èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Thinking-with-Sound (TwS)æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè¯­è¨€æ¨ç†å’Œå®æ—¶éŸ³é¢‘åŸŸåˆ†æï¼Œä¸ºLALMé…å¤‡äº†Audio CoTã€‚ä¸å°†éŸ³é¢‘è§†ä¸ºé™æ€è¾“å…¥çš„æ–¹æ³•ä¸åŒï¼ŒTwSä½¿æ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨åœ°â€œæ€è€ƒâ€éŸ³é¢‘ä¿¡å·ï¼Œé€šè¿‡å¤šæ¨¡æ€æ¨ç†æ‰§è¡Œæ•°å€¼åˆ†æå’Œæ•°å­—æ“ä½œã€‚ä¸ºäº†è¯„ä¼°è¯¥æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†MELD-Hard1kï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼•å…¥å„ç§å£°å­¦æ‰°åŠ¨è€Œåˆ›å»ºçš„æ–°çš„é²æ£’æ€§åŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„LALMåœ¨MELD-Hard1kä¸Šæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä¸å¹²å‡€éŸ³é¢‘ç›¸æ¯”ï¼Œå‡†ç¡®ç‡ä¸‹é™è¶…è¿‡50%ã€‚TwSåœ¨é²æ£’æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼šå°å‹æ¨¡å‹çš„ç»å¯¹å‡†ç¡®ç‡æé«˜äº†24.73%ï¼Œè€Œå¤§å‹æ¨¡å‹çš„æ”¹è¿›å¹…åº¦ä¸€è‡´åœ°æ‰©å±•åˆ°36.61%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒAudio CoTå¯ä»¥æ˜¾è‘—æé«˜é²æ£’æ€§è€Œæ— éœ€é‡æ–°è®­ç»ƒï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„éŸ³é¢‘ç†è§£ç³»ç»Ÿå¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹éŸ³é¢‘-è¯­è¨€æ¨¡å‹(LALM)åœ¨å¤æ‚å£°å­¦åœºæ™¯ä¸‹è¿›è¡ŒéŸ³é¢‘æ¨ç†æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†éŸ³é¢‘è§†ä¸ºé™æ€è¾“å…¥ï¼Œç¼ºä¹åˆ©ç”¨å£°å­¦å·¥å…·ï¼ˆå¦‚å™ªå£°æŠ‘åˆ¶ã€å£°æºåˆ†ç¦»ç­‰ï¼‰è¿›è¡ŒåŠ¨æ€åˆ†æå’Œå¤„ç†çš„èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨å­˜åœ¨å™ªå£°ã€æ··å“ç­‰å¹²æ‰°æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥â€œThinking-with-Sound (TwS)â€æ¡†æ¶ï¼Œèµ‹äºˆLALMåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨åˆ†æå’Œå¤„ç†éŸ³é¢‘ä¿¡å·çš„èƒ½åŠ›ã€‚TwSé€šè¿‡ç»“åˆè¯­è¨€æ¨ç†å’Œå®æ—¶éŸ³é¢‘åŸŸåˆ†æï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåƒäººç±»ä¸€æ ·ï¼Œåˆ©ç”¨å£°å­¦å·¥å…·å¯¹éŸ³é¢‘è¿›è¡Œé¢„å¤„ç†å’Œåˆ†æï¼Œä»è€Œæé«˜åœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTwSæ¡†æ¶çš„æ ¸å¿ƒæ˜¯Audio Chain-of-Thought (Audio CoT)ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1) æ¥æ”¶éŸ³é¢‘å’Œæ–‡æœ¬è¾“å…¥ï¼›2) LALMæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ¨ç†æ­¥éª¤ï¼Œè¿™äº›æ­¥éª¤å¯èƒ½åŒ…æ‹¬è°ƒç”¨å£°å­¦å·¥å…·è¿›è¡ŒéŸ³é¢‘åˆ†ææˆ–å¤„ç†ï¼›3) æ ¹æ®æ¨ç†æ­¥éª¤ï¼Œè°ƒç”¨ç›¸åº”çš„å£°å­¦å·¥å…·å¯¹éŸ³é¢‘è¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å¤„ç†ç»“æœåé¦ˆç»™LALMï¼›4) LALMç»“åˆå¤„ç†åçš„éŸ³é¢‘ä¿¡æ¯å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œè¿›è¡Œä¸‹ä¸€æ­¥æ¨ç†ï¼Œç›´è‡³å¾—åˆ°æœ€ç»ˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†éŸ³é¢‘å¤„ç†å·¥å…·é›†æˆåˆ°LALMçš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€åœ°åˆ©ç”¨è¿™äº›å·¥å…·æ¥æ”¹å–„éŸ³é¢‘è´¨é‡å’Œæå–ç›¸å…³ä¿¡æ¯ã€‚ä¸ç°æœ‰æ–¹æ³•å°†éŸ³é¢‘è§†ä¸ºé™æ€è¾“å…¥ä¸åŒï¼ŒTwSå…è®¸æ¨¡å‹ä¸»åŠ¨åœ°â€œæ€è€ƒâ€éŸ³é¢‘ï¼Œå¹¶æ ¹æ®éœ€è¦è°ƒç”¨ä¸åŒçš„å£°å­¦å·¥å…·ã€‚

**å…³é”®è®¾è®¡**ï¼šTwSæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•é€‰æ‹©å’Œé›†æˆåˆé€‚çš„å£°å­¦å·¥å…·ï¼›2) å¦‚ä½•è®¾è®¡æœ‰æ•ˆçš„æ–‡æœ¬æç¤ºï¼Œå¼•å¯¼LALMè°ƒç”¨è¿™äº›å·¥å…·ï¼›3) å¦‚ä½•å°†å£°å­¦å·¥å…·çš„å¤„ç†ç»“æœæœ‰æ•ˆåœ°èå…¥åˆ°LALMçš„æ¨ç†è¿‡ç¨‹ä¸­ã€‚è®ºæ–‡ä¸­å¯èƒ½æ¶‰åŠå…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ï¼Œä½†æ‘˜è¦ä¸­æœªæ˜ç¡®æåŠï¼Œå…·ä½“ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTwSæ¡†æ¶åœ¨MELD-Hard1kåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†LALMçš„é²æ£’æ€§ã€‚å°å‹æ¨¡å‹çš„ç»å¯¹å‡†ç¡®ç‡æé«˜äº†24.73%ï¼Œè€Œå¤§å‹æ¨¡å‹çš„æå‡å¹…åº¦é«˜è¾¾36.61%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒTwSæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¢å¼ºLALMåœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¯­éŸ³åŠ©æ‰‹ã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œæå‡è®¾å¤‡åœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«å’Œç†è§£èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨å˜ˆæ‚çš„è¡—é“ä¸Šï¼Œè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯ä»¥åˆ©ç”¨TwSæ¡†æ¶å¯¹è¯­éŸ³æŒ‡ä»¤è¿›è¡Œé™å™ªå¤„ç†ï¼Œä»è€Œæ›´å‡†ç¡®åœ°ç†è§£é©¾é©¶å‘˜çš„æ„å›¾ï¼Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨äººæœºäº¤äº’æ›´åŠ è‡ªç„¶å’Œæ™ºèƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent Large Audio-Language Models (LALMs) have shown strong performance on various audio understanding tasks such as speech translation and Audio Q\&A. However, they exhibit significant limitations on challenging audio reasoning tasks in complex acoustic scenarios. These situations would greatly benefit from the use of acoustic tools like noise suppression, source separation, and precise temporal alignment, but current LALMs lack access to such tools. To address this limitation, we introduce Thinking-with-Sound (TwS), a framework that equips LALMs with Audio CoT by combining linguistic reasoning with on-the-fly audio-domain analysis. Unlike existing approaches that treat audio as static input, TwS enables models to actively think with audio signals, performing numerical analysis and digital manipulation through multimodal reasoning. To evaluate this approach, we construct MELD-Hard1k, a new robustness benchmark created by introducing various acoustic perturbations. Experiments reveal that state-of-the-art LALMs suffer dramatic performance degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared to clean audio. TwS achieves substantial improvements in robustness, demonstrating both effectiveness and scalability: small models gain $24.73\%$ absolute accuracy, with improvements scaling consistently up to $36.61\%$ for larger models. Our findings demonstrate that Audio CoT can significantly enhance robustness without retraining, opening new directions for developing more robust audio understanding systems.

