---
layout: default
title: Transformers Can Learn Connectivity in Some Graphs but Not Others
---

# Transformers Can Learn Connectivity in Some Graphs but Not Others

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22343" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22343v1</a>
  <a href="https://arxiv.org/pdf/2509.22343.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22343v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22343v1', 'Transformers Can Learn Connectivity in Some Graphs but Not Others')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amit Roy, Abulhair Saparov

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: Under Review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¡¨æ˜Transformeråœ¨ç½‘æ ¼çŠ¶å›¾ä¸Šå­¦ä¹ è¿é€šæ€§ï¼Œä½†åœ¨å¤æ‚å›¾ä¸Šå­˜åœ¨å›°éš¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Transformer` `å›¾ç¥ç»ç½‘ç»œ` `è¿é€šæ€§` `ä¼ é€’å…³ç³»` `å›¾æ¨ç†` `æ¨¡å‹æ³›åŒ–` `ç½‘æ ¼å›¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹Transformeræ¨¡å‹ä»è®­ç»ƒæ ·æœ¬ä¸­å­¦ä¹ ä¼ é€’å…³ç³»ï¼ˆå›¾è¿é€šæ€§ï¼‰èƒ½åŠ›çš„æ·±å…¥æ¢ç´¢ï¼Œä»¥åŠæ¨¡å‹è§„æ¨¡å¯¹è¯¥èƒ½åŠ›çš„å½±å“ã€‚
2. æœ¬æ–‡é€šè¿‡ç”Ÿæˆä¸åŒç±»å‹çš„æœ‰å‘å›¾ï¼Œè®­ç»ƒä¸åŒè§„æ¨¡çš„Transformeræ¨¡å‹ï¼Œç ”ç©¶å…¶å­¦ä¹ å›¾è¿é€šæ€§çš„èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTransformeræ“…é•¿å­¦ä¹ ä½ç»´ç½‘æ ¼å›¾çš„è¿é€šæ€§ï¼Œä½†éš¾ä»¥å¤„ç†é«˜ç»´æˆ–åŒ…å«å¤§é‡ä¸è¿é€šç»„ä»¶çš„å¤æ‚å›¾ï¼Œä¸”æ¨¡å‹è§„æ¨¡å¯¹ç½‘æ ¼å›¾å­¦ä¹ æœ‰ç§¯æå½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†Transformeræ¨¡å‹å­¦ä¹ æœ‰å‘å›¾è¿é€šæ€§çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºç¡®ä¿åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„æ­£ç¡®æ€§è‡³å…³é‡è¦ã€‚ä»¥å¾€ç ”ç©¶ä¸»è¦å…³æ³¨Transformeré€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œä¼ é€’å…³ç³»æ¨ç†ï¼Œè€Œå¿½ç•¥äº†ä»è®­ç»ƒæ ·æœ¬ä¸­å­¦ä¹ çš„èƒ½åŠ›ä»¥åŠæ¨¡å‹è§„æ¨¡çš„å½±å“ã€‚æœ¬æ–‡é€šè¿‡ç”Ÿæˆä¸åŒå¤§å°çš„æœ‰å‘å›¾æ¥è®­ç»ƒä¸åŒè§„æ¨¡çš„Transformeræ¨¡å‹ï¼Œå¹¶è¯„ä¼°å…¶åœ¨ä¸åŒå›¾å¤§å°ä¸‹æ¨æ–­ä¼ é€’å…³ç³»çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒTransformerèƒ½å¤Ÿå­¦ä¹ â€œç½‘æ ¼çŠ¶â€æœ‰å‘å›¾çš„è¿é€šæ€§ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹å¯ä»¥åµŒå…¥åˆ°ä½ç»´å­ç©ºé—´ä¸­ï¼Œå¹¶ä¸”è¿é€šæ€§å¯ä»¥ä»èŠ‚ç‚¹çš„åµŒå…¥ä¸­è½»æ¾æ¨æ–­å‡ºæ¥ã€‚ç½‘æ ¼å›¾çš„ç»´åº¦æ˜¯Transformerå­¦ä¹ è¿é€šæ€§ä»»åŠ¡èƒ½åŠ›çš„ä¸€ä¸ªå¼ºé¢„æµ‹æŒ‡æ ‡ï¼Œé«˜ç»´ç½‘æ ¼å›¾æ¯”ä½ç»´ç½‘æ ¼å›¾æ›´å…·æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œå¢åŠ æ¨¡å‹è§„æ¨¡å¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°ç½‘æ ¼å›¾ä¸Šçš„è¿é€šæ€§æ¨æ–­ã€‚ç„¶è€Œï¼Œå¦‚æœå›¾ä¸æ˜¯ç½‘æ ¼å›¾ï¼Œå¹¶ä¸”åŒ…å«è®¸å¤šä¸è¿é€šçš„ç»„ä»¶ï¼Œåˆ™Transformerå¾ˆéš¾å­¦ä¹ è¿é€šæ€§ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨ç»„ä»¶æ•°é‡å¾ˆå¤§æ—¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç ”ç©¶Transformeræ¨¡å‹å­¦ä¹ æœ‰å‘å›¾è¿é€šæ€§çš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨Transformerçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå¿½ç•¥äº†å…¶ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ è¿é€šæ€§çš„èƒ½åŠ›ï¼Œä»¥åŠæ¨¡å‹è§„æ¨¡å¯¹å­¦ä¹ æ•ˆæœçš„å½±å“ã€‚æ­¤å¤–ï¼Œç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹ä¸åŒç±»å‹å›¾ç»“æ„ä¸‹Transformerå­¦ä¹ èƒ½åŠ›çš„ç³»ç»Ÿæ€§åˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ§åˆ¶å›¾çš„ç»“æ„ï¼Œç‰¹åˆ«æ˜¯å›¾çš„ç»´åº¦å’Œè¿é€šæ€§ï¼Œæ¥ç ”ç©¶Transformeræ¨¡å‹å­¦ä¹ è¿é€šæ€§çš„èƒ½åŠ›ã€‚é€šè¿‡æ¯”è¾ƒTransformeråœ¨ä¸åŒç±»å‹å›¾ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºå…¶å­¦ä¹ è¿é€šæ€§çš„å†…åœ¨æœºåˆ¶å’Œå±€é™æ€§ã€‚æ ¸å¿ƒå‡è®¾æ˜¯Transformeræ›´æ“…é•¿å­¦ä¹ å…·æœ‰è§„åˆ™ç»“æ„çš„å›¾ï¼Œä¾‹å¦‚ä½ç»´ç½‘æ ¼å›¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) ç”Ÿæˆä¸åŒç±»å‹çš„æœ‰å‘å›¾ï¼ŒåŒ…æ‹¬ç½‘æ ¼å›¾å’Œéšæœºå›¾ï¼›2) ä½¿ç”¨ç”Ÿæˆçš„å›¾æ•°æ®è®­ç»ƒä¸åŒè§„æ¨¡çš„Transformeræ¨¡å‹ï¼›3) è¯„ä¼°è®­ç»ƒåçš„Transformeræ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¿é€šæ€§é¢„æµ‹å‡†ç¡®ç‡ï¼›4) åˆ†ææ¨¡å‹è§„æ¨¡ã€å›¾ç»“æ„å’Œè¿é€šæ€§é¢„æµ‹å‡†ç¡®ç‡ä¹‹é—´çš„å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†Transformeræ¨¡å‹åœ¨ä¸åŒç±»å‹å›¾ç»“æ„ä¸‹å­¦ä¹ è¿é€šæ€§çš„èƒ½åŠ›ã€‚é€šè¿‡æ§åˆ¶å›¾çš„ç»“æ„ï¼Œæ­ç¤ºäº†Transformeræ¨¡å‹æ›´æ“…é•¿å­¦ä¹ å…·æœ‰è§„åˆ™ç»“æ„çš„å›¾ï¼Œä¾‹å¦‚ä½ç»´ç½‘æ ¼å›¾ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†æ¨¡å‹è§„æ¨¡å¯¹å­¦ä¹ æ•ˆæœçš„å½±å“ï¼Œå‘ç°å¢åŠ æ¨¡å‹è§„æ¨¡å¯ä»¥æé«˜åœ¨ç½‘æ ¼å›¾ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ä¸åŒç»´åº¦çš„ç½‘æ ¼å›¾æ¥æ§åˆ¶å›¾çš„ç»“æ„ï¼›2) ä½¿ç”¨éšæœºå›¾æ¥æ¨¡æ‹Ÿæ›´å¤æ‚çš„å›¾ç»“æ„ï¼›3) ä½¿ç”¨ä¸åŒè§„æ¨¡çš„Transformeræ¨¡å‹æ¥ç ”ç©¶æ¨¡å‹è§„æ¨¡çš„å½±å“ï¼›4) ä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºè¯„ä¼°è¿é€šæ€§é¢„æµ‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTransformeræ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ ä½ç»´ç½‘æ ¼å›¾çš„è¿é€šæ€§ï¼Œä¸”æ¨¡å‹è§„æ¨¡è¶Šå¤§ï¼Œæ³›åŒ–èƒ½åŠ›è¶Šå¼ºã€‚ç„¶è€Œï¼Œåœ¨é«˜ç»´ç½‘æ ¼å›¾å’ŒåŒ…å«å¤§é‡ä¸è¿é€šç»„ä»¶çš„éšæœºå›¾ä¸Šï¼ŒTransformeræ¨¡å‹çš„å­¦ä¹ æ•ˆæœæ˜¾è‘—ä¸‹é™ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†Transformeræ¨¡å‹åœ¨å­¦ä¹ å›¾ç»“æ„åŒ–æ•°æ®æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å›¾è°±æ¨ç†ã€å› æœå…³ç³»æ¨æ–­ç­‰ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚é€šè¿‡ç†è§£Transformeræ¨¡å‹åœ¨ä¸åŒå›¾ç»“æ„ä¸‹çš„å­¦ä¹ ç‰¹æ€§ï¼Œå¯ä»¥è®¾è®¡æ›´æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ¨¡å‹æ¶æ„ï¼Œä»è€Œæé«˜LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¯¹äºå¼€å‘æ›´é«˜æ•ˆçš„å›¾ç¥ç»ç½‘ç»œä¹Ÿå…·æœ‰ä¸€å®šçš„å€Ÿé‰´æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning capability is essential to ensure the factual correctness of the responses of transformer-based Large Language Models (LLMs), and robust reasoning about transitive relations is instrumental in many settings, such as causal inference. Hence, it is essential to investigate the capability of transformers in the task of inferring transitive relations (e.g., knowing A causes B and B causes C, then A causes C). The task of inferring transitive relations is equivalent to the task of connectivity in directed graphs (e.g., knowing there is a path from A to B, and there is a path from B to C, then there is a path from A to C). Past research focused on whether transformers can learn to infer transitivity from in-context examples provided in the input prompt. However, transformers' capability to infer transitive relations from training examples and how scaling affects the ability is unexplored. In this study, we seek to answer this question by generating directed graphs to train transformer models of varying sizes and evaluate their ability to infer transitive relations for various graph sizes. Our findings suggest that transformers are capable of learning connectivity on "grid-like'' directed graphs where each node can be embedded in a low-dimensional subspace, and connectivity is easily inferable from the embeddings of the nodes. We find that the dimensionality of the underlying grid graph is a strong predictor of transformers' ability to learn the connectivity task, where higher-dimensional grid graphs pose a greater challenge than low-dimensional grid graphs. In addition, we observe that increasing the model scale leads to increasingly better generalization to infer connectivity over grid graphs. However, if the graph is not a grid graph and contains many disconnected components, transformers struggle to learn the connectivity task, especially when the number of components is large.

