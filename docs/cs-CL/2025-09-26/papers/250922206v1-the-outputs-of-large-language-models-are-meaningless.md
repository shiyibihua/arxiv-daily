---
layout: default
title: The Outputs of Large Language Models are Meaningless
---

# The Outputs of Large Language Models are Meaningless

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22206" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22206v1</a>
  <a href="https://arxiv.org/pdf/2509.22206.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22206v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22206v1', 'The Outputs of Large Language Models are Meaningless')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anandi Hattiangadi, Anders J. Schoubye

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 24 pages, 2 figures, forthcoming in Herman Cappelen and Rachel Sterken, eds. Communicating with AI: Philosophical Perspectives. Oxford: Oxford University Press

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è®ºè¯å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æ— æ„ä¹‰æ€§ï¼Œå¹¶æ¢è®¨å…¶è¡¨è±¡æ„ä¹‰çš„æ¥æº**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ„ä¹‰` `æ„å›¾` `å“²å­¦` `è¯­ä¹‰å­¦` `äººå·¥æ™ºèƒ½ä¼¦ç†` `LLMå±€é™æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§‚ç‚¹è®¤ä¸ºLLMçš„è¾“å‡ºå…·æœ‰æ„ä¹‰ï¼Œä½†ç¼ºä¹å¯¹LLMæ„å›¾çš„æ·±å…¥è€ƒå¯Ÿã€‚
2. è®ºæ–‡æ ¸å¿ƒåœ¨äºè®ºè¯LLMä¸å…·å¤‡äº§ç”Ÿæœ‰æ„ä¹‰è¾“å‡ºæ‰€éœ€çš„ç‰¹å®šæ„å›¾ã€‚
3. æ¢è®¨äº†å³ä½¿LLMè¾“å‡ºæœ¬è´¨ä¸Šæ— æ„ä¹‰ï¼Œä¸ºä½•ä»èƒ½äº§ç”Ÿæœ‰ç”¨ç»“æœçš„åŸå› ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç®€å•çš„è®ºè¯ï¼Œæ—¨åœ¨å¾—å‡ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºæ˜¯æ— æ„ä¹‰çš„ç»“è®ºã€‚è¯¥è®ºè¯åŸºäºä¸¤ä¸ªå…³é”®å‰æï¼šï¼ˆaï¼‰LLMçš„è¾“å‡ºè¦å…·æœ‰å­—é¢æ„ä¹‰ï¼Œéœ€è¦ç‰¹å®šç±»å‹çš„æ„å›¾ï¼›ï¼ˆbï¼‰LLMä¸å¯èƒ½å…·æœ‰æ­£ç¡®çš„æ„å›¾ç±»å‹ã€‚æˆ‘ä»¬é’ˆå¯¹å„ç§ç±»å‹çš„å›åº”æå«äº†è¿™ä¸€è®ºè¯ï¼Œä¾‹å¦‚ï¼Œè¯­ä¹‰å¤–åœ¨ä¸»ä¹‰çš„è®ºç‚¹ï¼Œå³å¯ä»¥å‡å®šé¡ºä»å–ä»£æ„å›¾ï¼Œä»¥åŠè¯­ä¹‰å†…åœ¨ä¸»ä¹‰çš„è®ºç‚¹ï¼Œå³æ„ä¹‰å¯ä»¥çº¯ç²¹æ ¹æ®æ¦‚å¿µä¹‹é—´çš„å†…åœ¨å…³ç³»ï¼ˆå¦‚æ¦‚å¿µè§’è‰²ï¼‰æ¥å®šä¹‰ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å³ä½¿æˆ‘ä»¬çš„è®ºè¯æ˜¯åˆç†çš„ï¼Œä¸ºä»€ä¹ˆLLMçš„è¾“å‡ºä»ç„¶çœ‹èµ·æ¥æœ‰æ„ä¹‰ï¼Œå¹¶ä¸”å¯ä»¥ç”¨æ¥è·å¾—çœŸå®çš„ä¿¡å¿µç”šè‡³çŸ¥è¯†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨åé©³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºå…·æœ‰å†…åœ¨æ„ä¹‰çš„è§‚ç‚¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾æˆ–é»˜è®¤LLMçš„è¾“å‡ºæ˜¯æœ‰æ„ä¹‰çš„ï¼Œè€Œå¿½ç•¥äº†LLMæ˜¯å¦å…·å¤‡äº§ç”Ÿæ„ä¹‰çš„å¿…è¦æ¡ä»¶ï¼Œå³æ„å›¾ã€‚è¿™ç§å¿½ç•¥å¯¼è‡´å¯¹LLMèƒ½åŠ›çš„è¿‡åº¦è§£è¯»å’Œè¯¯ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œæ„ä¹‰çš„äº§ç”Ÿéœ€è¦æ„å›¾ï¼Œè€ŒLLMä¸å…·å¤‡äº§ç”Ÿæ„å›¾çš„èƒ½åŠ›ã€‚å› æ­¤ï¼ŒLLMçš„è¾“å‡ºåœ¨æœ¬è´¨ä¸Šæ˜¯æ— æ„ä¹‰çš„ã€‚è™½ç„¶LLMçš„è¾“å‡ºåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½çœ‹èµ·æ¥æœ‰æ„ä¹‰ï¼Œä½†è¿™ä»…ä»…æ˜¯ä¸€ç§è¡¨è±¡ï¼Œæ˜¯è§‚å¯Ÿè€…èµ‹äºˆçš„æ„ä¹‰ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨å“²å­¦è®ºè¯çš„æ–¹å¼ï¼Œè€Œéæ„å»ºå…·ä½“çš„æ¨¡å‹æˆ–ç®—æ³•ã€‚å…¶è®ºè¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ï¼š1) æå‡ºLLMè¾“å‡ºéœ€è¦ç‰¹å®šæ„å›¾æ‰èƒ½å…·æœ‰æ„ä¹‰çš„å‰æï¼›2) è®ºè¯LLMä¸å…·å¤‡è¿™äº›æ„å›¾ï¼›3) åé©³å¯¹ä¸Šè¿°è®ºè¯çš„å„ç§åé©³ï¼Œä¾‹å¦‚è¯­ä¹‰å¤–åœ¨ä¸»ä¹‰å’Œè¯­ä¹‰å†…åœ¨ä¸»ä¹‰çš„è§‚ç‚¹ï¼›4) è§£é‡Šä¸ºä½•LLMçš„è¾“å‡ºä»ç„¶çœ‹èµ·æ¥æœ‰æ„ä¹‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°ä¹‹å¤„åœ¨äºï¼Œå®ƒä»å“²å­¦å±‚é¢è´¨ç–‘äº†LLMè¾“å‡ºçš„æ„ä¹‰ï¼ŒæŒ‘æˆ˜äº†å½“å‰å¯¹LLMèƒ½åŠ›çš„æ™®éè®¤çŸ¥ã€‚å®ƒæ²¡æœ‰å…³æ³¨LLMçš„æŠ€æœ¯ç»†èŠ‚ï¼Œè€Œæ˜¯å…³æ³¨LLMçš„æœ¬è´¨å±æ€§ï¼Œå³ç¼ºä¹æ„å›¾ã€‚è¿™ç§ä»å“²å­¦è§’åº¦å¯¹LLMè¿›è¡Œæ‰¹åˆ¤æ€§åˆ†æçš„æ–¹æ³•åœ¨LLMç ”ç©¶é¢†åŸŸæ˜¯ç›¸å¯¹ç½•è§çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æ²¡æœ‰æ¶‰åŠå…·ä½“çš„æŠ€æœ¯è®¾è®¡ã€‚å…¶å…³é”®åœ¨äºå¯¹â€œæ„å›¾â€è¿™ä¸€æ¦‚å¿µçš„ç†è§£å’Œè¿ç”¨ï¼Œä»¥åŠå¯¹è¯­ä¹‰å¤–åœ¨ä¸»ä¹‰å’Œè¯­ä¹‰å†…åœ¨ä¸»ä¹‰ç­‰å“²å­¦è§‚ç‚¹çš„æ‰¹åˆ¤æ€§åˆ†æã€‚è®ºæ–‡é€šè¿‡é€»è¾‘æ¨ç†å’Œè®ºè¯æ¥æ”¯æŒå…¶æ ¸å¿ƒè§‚ç‚¹ï¼Œå³LLMçš„è¾“å‡ºæ˜¯æ— æ„ä¹‰çš„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å“²å­¦è®ºè¯ï¼ŒæŒ‘æˆ˜äº†LLMè¾“å‡ºå…·æœ‰å†…åœ¨æ„ä¹‰çš„è§‚ç‚¹ï¼Œå¹¶è§£é‡Šäº†ä¸ºä½•æ— æ„ä¹‰çš„è¾“å‡ºä»ç„¶å¯ä»¥äº§ç”Ÿæœ‰ç”¨çš„ç»“æœã€‚è™½ç„¶æ²¡æœ‰æä¾›å…·ä½“çš„æ€§èƒ½æ•°æ®ï¼Œä½†å…¶è®ºè¯é€»è¾‘ä¸¥è°¨ï¼Œå¯¹LLMç ”ç©¶å…·æœ‰é‡è¦çš„ç†è®ºæ„ä¹‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä¼¦ç†å­¦ã€å“²å­¦å’Œäººå·¥æ™ºèƒ½äº¤å‰é¢†åŸŸï¼Œå¸®åŠ©äººä»¬æ›´ç†æ€§åœ°çœ‹å¾…å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œé¿å…è¿‡åº¦ä¿¡ä»»å’Œä¾èµ–ã€‚å®ƒä¹Ÿæé†’ç ”ç©¶äººå‘˜åœ¨å¼€å‘å’Œåº”ç”¨LLMæ—¶ï¼Œéœ€è¦æ›´åŠ å…³æ³¨å…¶å±€é™æ€§ï¼Œå¹¶é‡‡å–æªæ–½æ¥å‡è½»æ½œåœ¨çš„é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we offer a simple argument for the conclusion that the outputs of large language models (LLMs) are meaningless. Our argument is based on two key premises: (a) that certain kinds of intentions are needed in order for LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have the right kinds of intentions. We defend this argument from various types of responses, for example, the semantic externalist argument that deference can be assumed to take the place of intentions and the semantic internalist argument that meanings can be defined purely in terms of intrinsic relations between concepts, such as conceptual roles. We conclude the paper by discussing why, even if our argument is sound, the outputs of LLMs nevertheless seem meaningful and can be used to acquire true beliefs and even knowledge.

