---
layout: default
title: Why Chain of Thought Fails in Clinical Text Understanding
---

# Why Chain of Thought Fails in Clinical Text Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21933" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21933v2</a>
  <a href="https://arxiv.org/pdf/2509.21933.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21933v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21933v2', 'Why Chain of Thought Fails in Clinical Text Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiageng Wu, Kevin Xie, Bowen Gu, Nils KrÃ¼ger, Kueiyu Joshua Lin, Jie Yang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-12-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿç ”ç©¶é“¾å¼æ€ç»´åœ¨ä¸´åºŠæ–‡æœ¬ç†è§£ä¸­çš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é“¾å¼æ€ç»´` `ä¸´åºŠæ–‡æœ¬ç†è§£` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç”µå­å¥åº·è®°å½•` `å¯è§£é‡Šæ€§` `å¯é æ€§` `åŒ»å­¦æ¨ç†` `ç³»ç»Ÿç ”ç©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é“¾å¼æ€ç»´æ–¹æ³•åœ¨ä¸´åºŠæ–‡æœ¬ç†è§£ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç”µå­å¥åº·è®°å½•æ—¶ï¼Œæ¨¡å‹æ€§èƒ½æ™®éä¸‹é™ã€‚
2. æœ¬æ–‡é€šè¿‡ç³»ç»Ÿè¯„ä¼°95ä¸ªLLMsåœ¨87ä¸ªä¸´åºŠæ–‡æœ¬ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†é“¾å¼æ€ç»´åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å±€é™æ€§ã€‚
3. ç ”ç©¶å‘ç°ï¼Œ86.3%çš„æ¨¡å‹åœ¨é“¾å¼æ€ç»´è®¾ç½®ä¸‹æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯è¾ƒå¼±æ¨¡å‹çš„è¡¨ç°æ˜¾è‘—æ¶åŒ–ï¼Œå¼ºè°ƒäº†å¯è§£é‡Šæ€§ä¸å¯é æ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸´åºŠæŠ¤ç†ä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œå‡†ç¡®æ€§å’Œé€æ˜æ¨ç†å¯¹äºå®‰å…¨å’Œå¯ä¿¡çš„éƒ¨ç½²è‡³å…³é‡è¦ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ³•åœ¨å¤šç§ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ€§èƒ½å’Œå¯è§£é‡Šæ€§çš„æå‡ï¼Œä½†åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹CoTåœ¨ä¸´åºŠæ–‡æœ¬ç†è§£ä¸­çš„åº”ç”¨è¿›è¡Œäº†å¤§è§„æ¨¡ç³»ç»Ÿç ”ç©¶ï¼Œè¯„ä¼°äº†95ä¸ªå…ˆè¿›çš„LLMsåœ¨87ä¸ªçœŸå®ä¸–ç•Œä¸´åºŠæ–‡æœ¬ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œ86.3%çš„æ¨¡å‹åœ¨CoTè®¾ç½®ä¸‹è¡¨ç°ä¸€è‡´ä¸‹é™ï¼Œå°¤å…¶æ˜¯è¾ƒå¼±çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†CoTåœ¨ä¸´åºŠç¯å¢ƒä¸­å¤±è´¥çš„ç³»ç»Ÿæ€§æ¨¡å¼ï¼Œå¼ºè°ƒäº†å¯è§£é‡Šæ€§ä¸å¯é æ€§ä¹‹é—´çš„å…³é”®æ‚–è®ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é“¾å¼æ€ç»´åœ¨ä¸´åºŠæ–‡æœ¬ç†è§£ä¸­çš„æœ‰æ•ˆæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç”µå­å¥åº·è®°å½•æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹95ä¸ªå…ˆè¿›LLMsåœ¨87ä¸ªçœŸå®ä¸–ç•Œä¸´åºŠæ–‡æœ¬ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œåˆ†æé“¾å¼æ€ç»´åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å¤±è´¥åŸå› ï¼Œæ­ç¤ºå…¶å¯¹æ¨¡å‹å¯é æ€§çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¤§è§„æ¨¡çš„å®éªŒè®¾è®¡ï¼Œæ¶µç›–9ç§è¯­è¨€å’Œ8ç§ä»»åŠ¡ç±»å‹ï¼Œç»“åˆLLMä½œä¸ºè¯„åˆ¤è€…çš„è¯„ä¼°æ–¹æ³•å’Œä¸´åºŠä¸“å®¶çš„è¯„ä¼°ï¼Œè¿›è¡Œç»†è‡´çš„åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†é“¾å¼æ€ç»´åœ¨ä¸´åºŠæ–‡æœ¬ç†è§£ä¸­çš„å±€é™æ€§ï¼Œæ­ç¤ºäº†å¯è§£é‡Šæ€§ä¸å¯é æ€§ä¹‹é—´çš„æ‚–è®ºï¼Œæä¾›äº†ä¸´åºŠæ¨ç†ç­–ç•¥çš„å®è¯åŸºç¡€ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­é‡‡ç”¨äº†ç»†ç²’åº¦çš„åˆ†ææ–¹æ³•ï¼Œå…³æ³¨æ¨ç†é•¿åº¦ã€åŒ»å­¦æ¦‚å¿µå¯¹é½å’Œé”™è¯¯ç‰¹å¾ï¼Œç»“åˆå¤šç§è¯„ä¼°æ–¹å¼ï¼Œç¡®ä¿ç»“æœçš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ86.3%çš„æ¨¡å‹åœ¨é“¾å¼æ€ç»´è®¾ç½®ä¸‹è¡¨ç°ä¸€è‡´ä¸‹é™ï¼Œå°¤å…¶æ˜¯è¾ƒå¼±æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—æ¶åŒ–ã€‚è¿™ä¸€å‘ç°ä¸å…¶ä»–é¢†åŸŸçš„ç ”ç©¶ç»“æœå½¢æˆé²œæ˜å¯¹æ¯”ï¼Œå¼ºè°ƒäº†åœ¨ä¸´åºŠæ–‡æœ¬ä»»åŠ¡ä¸­å¯è§£é‡Šæ€§ä¸å¯é æ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿã€ç”µå­å¥åº·è®°å½•åˆ†æå’ŒåŒ»ç–—æ–‡æœ¬æŒ–æ˜ç­‰ã€‚é€šè¿‡æ­ç¤ºé“¾å¼æ€ç»´åœ¨ä¸´åºŠæ–‡æœ¬ç†è§£ä¸­çš„å±€é™æ€§ï¼Œç ”ç©¶ä¸ºæœªæ¥å¼€å‘æ›´é€æ˜å’Œå¯é çš„ä¸´åºŠæ¨ç†æ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒï¼Œæ¨åŠ¨äº†åŒ»ç–—äººå·¥æ™ºèƒ½çš„å®‰å…¨åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are increasingly being applied to clinical care, a domain where both accuracy and transparent reasoning are critical for safe and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits step-by-step reasoning, has demonstrated improvements in performance and interpretability across a wide range of tasks. However, its effectiveness in clinical contexts remains largely unexplored, particularly in the context of electronic health records (EHRs), the primary source of clinical documentation, which are often lengthy, fragmented, and noisy. In this work, we present the first large-scale systematic study of CoT for clinical text understanding. We assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9 languages and 8 task types. Contrary to prior findings in other domains, we observe that 86.3\% of models suffer consistent performance degradation in the CoT setting. More capable models remain relatively robust, while weaker ones suffer substantial declines. To better characterize these effects, we perform fine-grained analyses of reasoning length, medical concept alignment, and error profiles, leveraging both LLM-as-a-judge evaluation and clinical expert evaluation. Our results uncover systematic patterns in when and why CoT fails in clinical contexts, which highlight a critical paradox: CoT enhances interpretability but may undermine reliability in clinical text tasks. This work provides an empirical basis for clinical reasoning strategies of LLMs, highlighting the need for transparent and trustworthy approaches.

