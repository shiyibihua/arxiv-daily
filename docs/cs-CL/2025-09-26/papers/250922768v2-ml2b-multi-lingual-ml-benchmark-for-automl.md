---
layout: default
title: ML2B: Multi-Lingual ML Benchmark For AutoML
---

# ML2B: Multi-Lingual ML Benchmark For AutoML

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22768" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22768v2</a>
  <a href="https://arxiv.org/pdf/2509.22768.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22768v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22768v2', 'ML2B: Multi-Lingual ML Benchmark For AutoML')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ekaterina Trofimova, Zosia Shamina, Maria Selifanova, Artem Zaitsev, Remi Savchuk, Maxim Minets, Daria Ozerova, Emil Sataev, Denis Zuenko, Andrey E. Ustyuzhanin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-10-06)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/enaix/ml2b)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ML2Bï¼šé¦–ä¸ªç”¨äºAutoMLçš„å¤šè¯­è¨€æœºå™¨å­¦ä¹ åŸºå‡†æµ‹è¯•ï¼Œå¡«è¡¥éè‹±è¯­MLä»£ç ç”Ÿæˆè¯„ä¼°ç©ºç™½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æœºå™¨å­¦ä¹ ` `ä»£ç ç”Ÿæˆ` `AutoML` `åŸºå‡†æµ‹è¯•` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨å­¦ä¹ ä»£ç ç”ŸæˆåŸºå‡†ä¸»è¦é›†ä¸­äºè‹±è¯­ï¼Œå¿½ç•¥äº†å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„å®é™…åº”ç”¨éœ€æ±‚ã€‚
2. ML2Bé€šè¿‡æä¾›å¤šè¯­è¨€æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ä¿ƒè¿›å¤šè¯­è¨€æœºå™¨å­¦ä¹ ä»£ç ç”Ÿæˆçš„ç ”ç©¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œéè‹±è¯­ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†å¤šè¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘åœ¨ç”Ÿæˆæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»£ç æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ„å»ºç«¯åˆ°ç«¯çš„pipelineã€‚ç„¶è€Œï¼Œç°æœ‰çš„MLä»£ç ç”ŸæˆåŸºå‡†ä¸»è¦å±€é™äºè‹±è¯­ï¼Œå¿½ç•¥äº†MLç ”ç©¶å’Œå®è·µçš„å…¨çƒåŒ–å’Œå¤šè¯­è¨€ç‰¹æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ML2Bï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šè¯­è¨€MLä»£ç ç”Ÿæˆçš„åŸºå‡†ã€‚ML2BåŒ…å«30ä¸ªKaggleç«èµ›ï¼Œç¿»è¯‘æˆ13ç§è‡ªç„¶è¯­è¨€ï¼Œæ¶µç›–è¡¨æ ¼æ•°æ®ã€æ–‡æœ¬æ•°æ®å’Œå›¾åƒæ•°æ®ç±»å‹ï¼Œå…·æœ‰ç»“æ„åŒ–å…ƒæ•°æ®å’Œç»è¿‡éªŒè¯çš„äººå·¥å®¡æ ¸ç¿»è¯‘ã€‚ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†AIDEï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç«¯åˆ°ç«¯è¯„ä¼°æ•°æ®ç§‘å­¦pipelineçš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¹¶æä¾›äº†å¯¹è·¨è¯­è¨€æ¨¡å‹æ€§èƒ½çš„è§è§£ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨éè‹±è¯­ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™15-45%ï¼Œçªå‡ºäº†å¤šè¯­è¨€è¡¨ç¤ºå­¦ä¹ åœ¨ä»£ç ç”Ÿæˆæ–¹é¢é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†ã€è¯„ä¼°æ¡†æ¶å’Œå…¨é¢çš„ç»“æœé€šè¿‡æˆ‘ä»¬çš„GitHubå­˜å‚¨åº“æä¾›ï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨å¤šè¯­è¨€MLä»£ç ç”Ÿæˆæ–¹é¢çš„ç ”ç©¶ï¼šhttps://github.com/enaix/ml2bã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æœºå™¨å­¦ä¹ ä»£ç ç”ŸæˆåŸºå‡†ä¸»è¦é›†ä¸­äºè‹±è¯­ï¼Œç¼ºä¹å¯¹å…¶ä»–è¯­è¨€çš„æ”¯æŒã€‚è¿™é™åˆ¶äº†LLMåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„åº”ç”¨ï¼Œé˜»ç¢äº†å…¨çƒèŒƒå›´å†…MLç ”ç©¶å’Œå®è·µçš„å‘å±•ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªå¤šè¯­è¨€çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å’Œæå‡LLMåœ¨ç”Ÿæˆéè‹±è¯­MLä»£ç æ–¹é¢çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šML2Bçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ›å»ºä¸€ä¸ªåŒ…å«å¤šç§è¯­è¨€çš„ã€é«˜è´¨é‡çš„æœºå™¨å­¦ä¹ ä»»åŠ¡æ•°æ®é›†ï¼Œå¹¶æä¾›ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¾¿å…¨é¢è¯„ä¼°LLMåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡åˆ†æLLMåœ¨ä¸åŒè¯­è¨€ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£å…¶åœ¨å¤šè¯­è¨€è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„ä¼˜åŠ¿å’Œä¸è¶³ï¼Œä»è€ŒæŒ‡å¯¼æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šML2Bçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) æ•°æ®é›†æ„å»ºï¼šé€‰æ‹©30ä¸ªKaggleç«èµ›ä½œä¸ºåŸºç¡€ï¼Œæ¶µç›–è¡¨æ ¼æ•°æ®ã€æ–‡æœ¬æ•°æ®å’Œå›¾åƒæ•°æ®ç±»å‹ã€‚å°†è¿™äº›ç«èµ›çš„ä»»åŠ¡æè¿°å’Œæ•°æ®ç¿»è¯‘æˆ13ç§è‡ªç„¶è¯­è¨€ï¼Œå¹¶è¿›è¡Œäººå·¥å®¡æ ¸ï¼Œç¡®ä¿ç¿»è¯‘è´¨é‡ã€‚2) è¯„ä¼°æ¡†æ¶ï¼šé‡‡ç”¨AIDEè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯è¯„ä¼°æ•°æ®ç§‘å­¦pipelineçš„æ€§èƒ½ã€‚AIDEèƒ½å¤Ÿè‡ªåŠ¨è¿è¡Œç”Ÿæˆçš„ä»£ç ï¼Œå¹¶æ ¹æ®é¢„å®šä¹‰çš„æŒ‡æ ‡è¯„ä¼°å…¶æ€§èƒ½ã€‚3) æ€§èƒ½åˆ†æï¼šåˆ†æLLMåœ¨ä¸åŒè¯­è¨€ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Šï¼Œä»¥ä¾¿ç ”ç©¶äººå‘˜äº†è§£LLMåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„ä¼˜åŠ¿å’Œä¸è¶³ã€‚

**å…³é”®åˆ›æ–°**ï¼šML2Bçš„å…³é”®åˆ›æ–°åœ¨äºå®ƒæ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šè¯­è¨€MLä»£ç ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ã€‚å®ƒä¸ä»…æä¾›äº†å¤šè¯­è¨€æ•°æ®é›†ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥æ–¹ä¾¿åœ°è¯„ä¼°LLMåœ¨ä¸åŒè¯­è¨€ä¸Šçš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒML2Bè¿˜æä¾›äº†è¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Šï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜äº†è§£LLMåœ¨å¤šè¯­è¨€è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„ä¼˜åŠ¿å’Œä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šML2Bçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„Kaggleç«èµ›ï¼Œæ¶µç›–ä¸åŒçš„æ•°æ®ç±»å‹å’Œä»»åŠ¡ç±»å‹ã€‚2) é‡‡ç”¨é«˜è´¨é‡çš„äººå·¥ç¿»è¯‘ï¼Œç¡®ä¿ç¿»è¯‘çš„å‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚3) ä½¿ç”¨AIDEè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå®ç°ç«¯åˆ°ç«¯çš„æ€§èƒ½è¯„ä¼°ã€‚4) æä¾›è¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ä¸åŒè¯­è¨€ä¸Šçš„æ€§èƒ½æŒ‡æ ‡å’Œé”™è¯¯åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨éè‹±è¯­ä»»åŠ¡ä¸Šï¼ŒLLMçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™15-45%ï¼Œè¿™çªæ˜¾äº†å¤šè¯­è¨€è¡¨ç¤ºå­¦ä¹ åœ¨ä»£ç ç”Ÿæˆæ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„LLMåœ¨å¤„ç†éè‹±è¯­ä»»åŠ¡æ—¶ä»ç„¶å­˜åœ¨æ˜æ˜¾çš„ä¸è¶³ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ”¹è¿›ã€‚ML2BåŸºå‡†çš„å‘å¸ƒï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªè¯„ä¼°å’Œæå‡LLMåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„ä»£ç ç”Ÿæˆèƒ½åŠ›çš„å¹³å°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ML2BåŸºå‡†æµ‹è¯•å¯ä»¥åº”ç”¨äºè¯„ä¼°å’Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æœºå™¨å­¦ä¹ ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚å®ƒæœ‰åŠ©äºå¼€å‘æ›´é€šç”¨ã€æ›´æ˜“äºä½¿ç”¨çš„AutoMLå·¥å…·ï¼Œé™ä½æœºå™¨å­¦ä¹ çš„é—¨æ§›ï¼Œä½¿æ›´å¤šçš„äººèƒ½å¤Ÿåˆ©ç”¨æœºå™¨å­¦ä¹ è§£å†³å®é™…é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†è¿˜å¯ä»¥ä¿ƒè¿›å¤šè¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„ç ”ç©¶ï¼Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ çš„äº¤å‰èåˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have recently demonstrated strong capabilities in generating machine learning (ML) code, enabling end-to-end pipeline construction from natural language instructions. However, existing benchmarks for ML code generation are mainly restricted to English, overlooking the global and multilingual nature of ML research and practice. To address this gap, we present ML2B, the first benchmark for evaluating multilingual ML code generation. ML2B consists of 30 Kaggle competitions translated into 13 natural languages, covering tabular, text, and image data types, with structured metadata and validated human-reviewed translations. For evaluation, we employ AIDE, an automated framework for end-to-end assessment of data science pipelines, and provide insights into cross-lingual model performance. Our results reveal substantial 15-45% performance degradation on non-English tasks, highlighting critical challenges in multilingual representation learning for code generation. The benchmark, evaluation framework, and comprehensive results are made available through our GitHub repository to facilitate future research in multilingual ML code generation: https://github.com/enaix/ml2b.

