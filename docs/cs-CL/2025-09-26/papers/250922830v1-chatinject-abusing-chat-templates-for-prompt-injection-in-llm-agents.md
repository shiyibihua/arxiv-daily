---
layout: default
title: ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents
---

# ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22830" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22830v1</a>
  <a href="https://arxiv.org/pdf/2509.22830.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22830v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22830v1', 'ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hwan Chang, Yonghyun Jun, Hwanhee Lee

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ChatInjectï¼šåˆ©ç”¨èŠå¤©æ¨¡æ¿åœ¨LLM Agentä¸­è¿›è¡Œæç¤ºæ³¨å…¥æ”»å‡»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LLM Agent` `æç¤ºæ³¨å…¥` `èŠå¤©æ¨¡æ¿` `å®‰å…¨æ¼æ´` `å¯¹æŠ—æ”»å‡»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLM Agentæ˜“å—é—´æ¥æç¤ºæ³¨å…¥æ”»å‡»ï¼Œæ”»å‡»è€…å¯åˆ©ç”¨å¤–éƒ¨ç¯å¢ƒè¾“å‡ºä¸­çš„æ¶æ„æŒ‡ä»¤æ§åˆ¶Agentè¡Œä¸ºã€‚
2. ChatInjectæ”»å‡»é€šè¿‡æ¨¡ä»¿åŸç”ŸèŠå¤©æ¨¡æ¿æ ¼å¼åŒ–æ¶æ„payloadï¼Œåˆ©ç”¨LLMçš„æŒ‡ä»¤éµå¾ªç‰¹æ€§å®ç°æ³¨å…¥ã€‚
3. å®éªŒè¡¨æ˜ChatInjectæ˜¾è‘—æå‡äº†æ”»å‡»æˆåŠŸç‡ï¼Œä¸”å…·æœ‰è·¨æ¨¡å‹è¿ç§»æ€§ï¼Œç°æœ‰é˜²å¾¡æªæ–½æ•ˆæœä¸ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Agentä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’çš„æ—¥ç›Šæ™®åŠï¼Œä¸ºå¯¹æŠ—æ€§æ“çºµåˆ›é€ äº†æ–°çš„æ”»å‡»é¢ã€‚å…¶ä¸­ä¸€ä¸ªä¸»è¦å¨èƒæ˜¯é—´æ¥æç¤ºæ³¨å…¥ï¼Œæ”»å‡»è€…å°†æ¶æ„æŒ‡ä»¤åµŒå…¥åˆ°å¤–éƒ¨ç¯å¢ƒçš„è¾“å‡ºä¸­ï¼Œå¯¼è‡´Agentå°†å…¶è§£é‡Šå¹¶æ‰§è¡Œä¸ºåˆæ³•çš„æç¤ºã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨çº¯æ–‡æœ¬æ³¨å…¥æ”»å‡»ä¸Šï¼Œè€Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªé‡è¦ä½†æœªè¢«å……åˆ†æ¢ç´¢çš„æ¼æ´ï¼šLLMå¯¹ç»“æ„åŒ–èŠå¤©æ¨¡æ¿çš„ä¾èµ–æ€§ä»¥åŠå®ƒä»¬é€šè¿‡æœ‰è¯´æœåŠ›çš„å¤šè½®å¯¹è¯è¿›è¡Œä¸Šä¸‹æ–‡æ“çºµçš„æ•æ„Ÿæ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ChatInjectï¼Œä¸€ç§å°†æ¶æ„payloadæ ¼å¼åŒ–ä¸ºæ¨¡ä»¿åŸç”ŸèŠå¤©æ¨¡æ¿çš„æ”»å‡»ï¼Œä»è€Œåˆ©ç”¨æ¨¡å‹å›ºæœ‰çš„æŒ‡ä»¤éµå¾ªå€¾å‘ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºè¯´æœçš„å¤šè½®å˜ä½“ï¼Œé€šè¿‡å¯¹è¯è½®æ¬¡æ¥å¼•å¯¼Agentæ¥å—å¹¶æ‰§è¡ŒåŸæœ¬å¯ç–‘çš„æ“ä½œã€‚é€šè¿‡å¯¹å‰æ²¿LLMçš„å…¨é¢å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šï¼ˆ1ï¼‰ChatInjectå®ç°äº†æ¯”ä¼ ç»Ÿæç¤ºæ³¨å…¥æ–¹æ³•æ˜¾è‘—æ›´é«˜çš„å¹³å‡æ”»å‡»æˆåŠŸç‡ï¼Œåœ¨AgentDojoä¸Šä»5.18%æé«˜åˆ°32.05%ï¼Œåœ¨InjecAgentä¸Šä»15.13%æé«˜åˆ°45.90%ï¼Œå…¶ä¸­å¤šè½®å¯¹è¯åœ¨InjecAgentä¸Šè¡¨ç°å‡ºç‰¹åˆ«å¼ºçš„æ€§èƒ½ï¼Œå¹³å‡æˆåŠŸç‡ä¸º52.33%ï¼›ï¼ˆ2ï¼‰åŸºäºèŠå¤©æ¨¡æ¿çš„payloadè¡¨ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡å‹è¿ç§»æ€§ï¼Œå³ä½¿å¯¹äºé—­æºLLMä¹Ÿæœ‰æ•ˆï¼Œå°½ç®¡å®ƒä»¬çš„æ¨¡æ¿ç»“æ„æœªçŸ¥ï¼›ï¼ˆ3ï¼‰ç°æœ‰çš„åŸºäºæç¤ºçš„é˜²å¾¡æªæ–½å¯¹è¿™ç§æ”»å‡»æ–¹æ³•åŸºæœ¬æ— æ•ˆï¼Œç‰¹åˆ«æ˜¯å¯¹å¤šè½®å˜ä½“ã€‚è¿™äº›å‘ç°çªå‡ºäº†å½“å‰Agentç³»ç»Ÿä¸­çš„æ¼æ´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³LLM Agentä¸­å­˜åœ¨çš„é—´æ¥æç¤ºæ³¨å…¥æ¼æ´ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨èŠå¤©æ¨¡æ¿è¿›è¡Œæ”»å‡»çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨çº¯æ–‡æœ¬æ³¨å…¥ï¼Œå¿½ç•¥äº†LLMå¯¹ç»“æ„åŒ–èŠå¤©æ¨¡æ¿çš„ä¾èµ–æ€§ï¼Œä»¥åŠé€šè¿‡å¤šè½®å¯¹è¯è¿›è¡Œä¸Šä¸‹æ–‡æ“çºµçš„å¯èƒ½æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMå¯¹èŠå¤©æ¨¡æ¿çš„å›ºæœ‰ä¾èµ–æ€§ï¼Œæ„é€ æ¶æ„payloadï¼Œä½¿å…¶çœ‹èµ·æ¥åƒæ˜¯åˆæ³•çš„èŠå¤©æ¶ˆæ¯ï¼Œä»è€Œè¯±å¯¼Agentæ‰§è¡Œæ¶æ„æŒ‡ä»¤ã€‚é€šè¿‡æ¨¡ä»¿èŠå¤©æ¨¡æ¿çš„ç»“æ„ï¼Œæ”»å‡»è€…å¯ä»¥ç»•è¿‡ä¸€äº›ç®€å•çš„é˜²å¾¡æœºåˆ¶ï¼Œå¹¶æé«˜æ”»å‡»çš„æˆåŠŸç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šChatInjectæ”»å‡»ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå•è½®æ”»å‡»å’Œå¤šè½®æ”»å‡»ã€‚å•è½®æ”»å‡»ç›´æ¥æ„é€ æ¨¡ä»¿èŠå¤©æ¨¡æ¿çš„æ¶æ„payloadã€‚å¤šè½®æ”»å‡»åˆ™é€šè¿‡å¤šè½®å¯¹è¯ï¼Œé€æ­¥å¼•å¯¼Agentè¿›å…¥æ”»å‡»è€…è®¾å®šçš„æƒ…å¢ƒï¼Œæœ€ç»ˆæ‰§è¡Œæ¶æ„æŒ‡ä»¤ã€‚å¤šè½®æ”»å‡»çš„å…³é”®åœ¨äºè®¾è®¡å…·æœ‰è¯´æœåŠ›çš„å¯¹è¯ç­–ç•¥ï¼Œä½¿Agenté€æ¸æ¥å—æ”»å‡»è€…çš„æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå‘ç°äº†LLM Agentå¯¹èŠå¤©æ¨¡æ¿çš„ä¾èµ–æ€§ï¼Œå¹¶åˆ©ç”¨è¿™ä¸€ç‰¹æ€§è®¾è®¡äº†ChatInjectæ”»å‡»ã€‚ä¸ä¼ ç»Ÿçš„çº¯æ–‡æœ¬æ³¨å…¥æ”»å‡»ç›¸æ¯”ï¼ŒChatInjectæ”»å‡»æ›´å…·éšè”½æ€§å’Œæœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿç»•è¿‡ä¸€äº›ç®€å•çš„é˜²å¾¡æœºåˆ¶ã€‚å¤šè½®æ”»å‡»è¿›ä¸€æ­¥å¢å¼ºäº†æ”»å‡»çš„æˆåŠŸç‡ï¼Œä½¿å…¶èƒ½å¤Ÿåº”å¯¹æ›´å¤æ‚çš„é˜²å¾¡ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šChatInjectæ”»å‡»çš„å…³é”®è®¾è®¡åœ¨äºpayloadçš„æ„é€ ï¼Œéœ€è¦ç²¾ç¡®æ¨¡ä»¿ç›®æ ‡LLMçš„èŠå¤©æ¨¡æ¿ã€‚å¯¹äºé—­æºLLMï¼Œå¯ä»¥é€šè¿‡è¯•é”™çš„æ–¹å¼æ¨æ–­å…¶èŠå¤©æ¨¡æ¿ç»“æ„ã€‚å¤šè½®æ”»å‡»çš„å…³é”®åœ¨äºè®¾è®¡å…·æœ‰è¯´æœåŠ›çš„å¯¹è¯ç­–ç•¥ï¼Œå¯ä»¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•æ¥ä¼˜åŒ–å¯¹è¯ç­–ç•¥ï¼Œæé«˜æ”»å‡»çš„æˆåŠŸç‡ã€‚è®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®æåŠå…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ï¼Œè¿™éƒ¨åˆ†ç»†èŠ‚å¯èƒ½éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒChatInjectæ”»å‡»åœ¨AgentDojoå’ŒInjecAgentä¸Šçš„å¹³å‡æ”»å‡»æˆåŠŸç‡åˆ†åˆ«ä»5.18%æé«˜åˆ°32.05%å’Œä»15.13%æé«˜åˆ°45.90%ã€‚å¤šè½®å¯¹è¯æ”»å‡»åœ¨InjecAgentä¸Šè¾¾åˆ°äº†52.33%çš„å¹³å‡æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒChatInjectæ”»å‡»å…·æœ‰å¾ˆå¼ºçš„è·¨æ¨¡å‹è¿ç§»æ€§ï¼Œå³ä½¿å¯¹äºé—­æºLLMä¹Ÿæœ‰æ•ˆã€‚ç°æœ‰çš„åŸºäºæç¤ºçš„é˜²å¾¡æªæ–½å¯¹ChatInjectæ”»å‡»åŸºæœ¬æ— æ•ˆï¼Œç‰¹åˆ«æ˜¯å¯¹å¤šè½®æ”»å‡»ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¯„ä¼°å’Œæå‡LLM Agentçš„å®‰å…¨æ€§ï¼Œå¸®åŠ©å¼€å‘è€…è¯†åˆ«å’Œä¿®å¤æ½œåœ¨çš„æç¤ºæ³¨å…¥æ¼æ´ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„é˜²å¾¡æœºåˆ¶æä¾›äº†æ€è·¯ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡æ£€æµ‹å’Œè¿‡æ»¤æ¶æ„èŠå¤©æ¨¡æ¿ï¼Œæˆ–è€…é€šè¿‡å¢å¼ºLLMå¯¹ä¸Šä¸‹æ–‡çš„ç†è§£èƒ½åŠ›ï¼Œä»è€Œé™ä½æç¤ºæ³¨å…¥æ”»å‡»çš„é£é™©ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´å¤æ‚çš„æ”»å‡»åœºæ™¯å’Œé˜²å¾¡ç­–ç•¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The growing deployment of large language model (LLM) based agents that interact with external environments has created new attack surfaces for adversarial manipulation. One major threat is indirect prompt injection, where attackers embed malicious instructions in external environment output, causing agents to interpret and execute them as if they were legitimate prompts. While previous research has focused primarily on plain-text injection attacks, we find a significant yet underexplored vulnerability: LLMs' dependence on structured chat templates and their susceptibility to contextual manipulation through persuasive multi-turn dialogues. To this end, we introduce ChatInject, an attack that formats malicious payloads to mimic native chat templates, thereby exploiting the model's inherent instruction-following tendencies. Building on this foundation, we develop a persuasion-driven Multi-turn variant that primes the agent across conversational turns to accept and execute otherwise suspicious actions. Through comprehensive experiments across frontier LLMs, we demonstrate three critical findings: (1) ChatInject achieves significantly higher average attack success rates than traditional prompt injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13% to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong performance at average 52.33% success rate on InjecAgent, (2) chat-template-based payloads demonstrate strong transferability across models and remain effective even against closed-source LLMs, despite their unknown template structures, and (3) existing prompt-based defenses are largely ineffective against this attack approach, especially against Multi-turn variants. These findings highlight vulnerabilities in current agent systems.

