---
layout: default
title: What Is The Political Content in LLMs' Pre- and Post-Training Data?
---

# What Is The Political Content in LLMs' Pre- and Post-Training Data?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22367" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22367v1</a>
  <a href="https://arxiv.org/pdf/2509.22367.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22367v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22367v1', 'What Is The Political Content in LLMs\' Pre- and Post-Training Data?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tanise Ceron, Dmitry Nikolaev, Dominik Stammbach, Debora Nozza

**åˆ†ç±»**: cs.CL, cs.AI, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 9 pages, under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æLLMè®­ç»ƒæ•°æ®ä¸­çš„æ”¿æ²»å€¾å‘ï¼Œæ­ç¤ºæ¨¡å‹åè§ä¸æ•°æ®åå·®çš„ç›¸å…³æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ”¿æ²»åè§` `è®­ç»ƒæ•°æ®åˆ†æ` `æ•°æ®åå·®` `å¼€æºæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¯¹LLMè®­ç»ƒæ•°æ®çš„æ”¿æ²»å†…å®¹åˆ†æä¸è¶³ï¼Œéš¾ä»¥è§£é‡Šæ¨¡å‹æ”¿æ²»åè§çš„æ¥æºã€‚
2. é€šè¿‡åˆ†æOLMO2çš„é¢„è®­ç»ƒå’Œåè®­ç»ƒè¯­æ–™åº“ï¼Œè¯„ä¼°æ”¿æ²»å€¾å‘ä¸æ¨¡å‹ç«‹åœºçš„ç›¸å…³æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒæ•°æ®ä¸­çš„æ”¿æ²»ç«‹åœºä¸æ¨¡å‹åœ¨æ”¿ç­–é—®é¢˜ä¸Šçš„åè§å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå¸¦æœ‰æ”¿æ²»åè§çš„æ–‡æœ¬å·²æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„äº‹å®ï¼Œä½†è¿™ç§åè§æ˜¯å¦‚ä½•äº§ç”Ÿçš„ä»ç„¶ä¸æ¸…æ¥šã€‚åˆ†æè®­ç»ƒæ•°æ®æ˜¯è§£å†³æ­¤é—®é¢˜çš„å…³é”®ä¸€æ­¥ï¼Œç„¶è€Œå½“å‰LLMç ”ç©¶å¯¹è®­ç»ƒæ•°æ®çš„æ”¿æ²»å†…å®¹æ¢ç´¢ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡åˆ†æäº†OLMO2çš„é¢„è®­ç»ƒå’Œåè®­ç»ƒè¯­æ–™åº“ï¼ŒOLMO2æ˜¯æœ€å¤§çš„å®Œå…¨å¼€æºæ¨¡å‹ï¼Œå¹¶å‘å¸ƒäº†å®Œæ•´çš„æ•°æ®é›†ã€‚ä»è¿™äº›è¯­æ–™åº“ä¸­ï¼Œæˆ‘ä»¬æŠ½å–äº†å¤§é‡çš„éšæœºæ ·æœ¬ï¼Œè‡ªåŠ¨æ ‡æ³¨æ–‡æ¡£çš„æ”¿æ²»å€¾å‘ï¼Œå¹¶åˆ†æå…¶æ¥æºé¢†åŸŸå’Œå†…å®¹ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯„ä¼°äº†è®­ç»ƒæ•°æ®ä¸­çš„æ”¿æ²»å†…å®¹ä¸æ¨¡å‹åœ¨ç‰¹å®šæ”¿ç­–é—®é¢˜ä¸Šçš„ç«‹åœºä¹‹é—´çš„ç›¸å…³æ€§ã€‚åˆ†æè¡¨æ˜ï¼Œå·¦å€¾æ–‡æ¡£åœ¨æ•°æ®é›†ä¸­å ä¸»å¯¼åœ°ä½ï¼Œå¹¶ä¸”é¢„è®­ç»ƒè¯­æ–™åº“åŒ…å«æ¯”åè®­ç»ƒæ•°æ®æ˜æ˜¾æ›´å¤šçš„æ”¿æ²»å‚ä¸å†…å®¹ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œå·¦å€¾å’Œå³å€¾æ–‡æ¡£é€šè¿‡ä¸åŒçš„ä»·å€¼è§‚å’Œåˆæ³•æ€§æ¥æºæ¥æ„å»ºç›¸ä¼¼çš„ä¸»é¢˜ã€‚æœ€åï¼Œè®­ç»ƒæ•°æ®ä¸­çš„ä¸»è¦ç«‹åœºä¸æ¨¡å‹åœ¨è¯„ä¼°æ”¿ç­–é—®é¢˜æ—¶çš„æ”¿æ²»åè§å¯†åˆ‡ç›¸å…³ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å°†æ”¿æ²»å†…å®¹åˆ†ææ•´åˆåˆ°æœªæ¥çš„æ•°æ®ç®¡ç†æµç¨‹ä¸­ï¼Œä»¥åŠå¯¹è¿‡æ»¤ç­–ç•¥è¿›è¡Œæ·±å…¥è®°å½•ä»¥æé«˜é€æ˜åº¦çš„å¿…è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè¯¥è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ”¿æ²»åè§æ¥æºä¸æ˜ç¡®çš„é—®é¢˜ã€‚ç°æœ‰çš„ç ”ç©¶å¯¹LLMè®­ç»ƒæ•°æ®çš„æ”¿æ²»å†…å®¹åˆ†æä¸è¶³ï¼Œæ— æ³•è§£é‡Šæ¨¡å‹äº§ç”Ÿæ”¿æ²»åè§çš„åŸå› ã€‚å› æ­¤ï¼Œéœ€è¦æ·±å…¥åˆ†æè®­ç»ƒæ•°æ®ï¼Œç‰¹åˆ«æ˜¯é¢„è®­ç»ƒå’Œåè®­ç»ƒæ•°æ®ï¼Œä»¥äº†è§£å…¶ä¸­çš„æ”¿æ²»å€¾å‘ï¼Œå¹¶æ¢ç©¶å…¶ä¸æ¨¡å‹åè§ä¹‹é—´çš„å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†æLLMçš„è®­ç»ƒæ•°æ®ï¼ˆOLMO2çš„é¢„è®­ç»ƒå’Œåè®­ç»ƒè¯­æ–™åº“ï¼‰ä¸­çš„æ”¿æ²»å†…å®¹ï¼Œæ¥æ­ç¤ºæ¨¡å‹æ”¿æ²»åè§çš„æ¥æºã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å¯¹è®­ç»ƒæ•°æ®è¿›è¡ŒæŠ½æ ·ã€è‡ªåŠ¨æ ‡æ³¨æ”¿æ²»å€¾å‘ã€åˆ†ææ¥æºé¢†åŸŸå’Œå†…å®¹ï¼Œä»¥åŠè¯„ä¼°æ”¿æ²»å†…å®¹ä¸æ¨¡å‹åœ¨ç‰¹å®šæ”¿ç­–é—®é¢˜ä¸Šçš„ç«‹åœºä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œç†è§£è®­ç»ƒæ•°æ®ä¸­çš„æ”¿æ²»å€¾å‘å¦‚ä½•å½±å“æ¨¡å‹çš„è¾“å‡ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š
1. **æ•°æ®æ”¶é›†**ï¼šæ”¶é›†OLMO2çš„é¢„è®­ç»ƒå’Œåè®­ç»ƒè¯­æ–™åº“ã€‚
2. **æ•°æ®æŠ½æ ·**ï¼šä»è¯­æ–™åº“ä¸­æŠ½å–å¤§é‡çš„éšæœºæ ·æœ¬ã€‚
3. **æ”¿æ²»å€¾å‘æ ‡æ³¨**ï¼šä½¿ç”¨è‡ªåŠ¨åŒ–çš„æ–¹æ³•å¯¹æŠ½æ ·æ–‡æ¡£è¿›è¡Œæ”¿æ²»å€¾å‘æ ‡æ³¨ï¼ˆå·¦å€¾ã€å³å€¾ç­‰ï¼‰ã€‚
4. **å†…å®¹åˆ†æ**ï¼šåˆ†ææ–‡æ¡£çš„æ¥æºé¢†åŸŸå’Œå†…å®¹ï¼Œè¯†åˆ«ä¸åŒæ”¿æ²»å€¾å‘çš„æ–‡æ¡£åœ¨ä¸»é¢˜ã€ä»·å€¼è§‚å’Œåˆæ³•æ€§æ¥æºä¸Šçš„å·®å¼‚ã€‚
5. **ç›¸å…³æ€§è¯„ä¼°**ï¼šè¯„ä¼°è®­ç»ƒæ•°æ®ä¸­çš„æ”¿æ²»å†…å®¹ä¸æ¨¡å‹åœ¨ç‰¹å®šæ”¿ç­–é—®é¢˜ä¸Šçš„ç«‹åœºä¹‹é—´çš„ç›¸å…³æ€§ã€‚
6. **ç»“æœåˆ†æä¸æ€»ç»“**ï¼šåˆ†æå®éªŒç»“æœï¼Œæ€»ç»“è®­ç»ƒæ•°æ®ä¸­çš„æ”¿æ²»å€¾å‘ä¸æ¨¡å‹åè§ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æå‡ºæ”¹è¿›å»ºè®®ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1. **å…¨é¢åˆ†æå¼€æºLLMçš„è®­ç»ƒæ•°æ®**ï¼šé¦–æ¬¡å¯¹å®Œå…¨å¼€æºçš„LLMï¼ˆOLMO2ï¼‰çš„é¢„è®­ç»ƒå’Œåè®­ç»ƒè¯­æ–™åº“è¿›è¡Œäº†å…¨é¢çš„æ”¿æ²»å†…å®¹åˆ†æã€‚
2. **æ­ç¤ºæ”¿æ²»å€¾å‘ä¸æ¨¡å‹åè§çš„ç›¸å…³æ€§**ï¼šé€šè¿‡å®éªŒè¯æ˜ï¼Œè®­ç»ƒæ•°æ®ä¸­çš„æ”¿æ²»å€¾å‘ä¸æ¨¡å‹åœ¨æ”¿ç­–é—®é¢˜ä¸Šçš„åè§å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚
3. **å¼ºè°ƒæ•°æ®ç®¡ç†çš„é‡è¦æ€§**ï¼šå¼ºè°ƒäº†åœ¨LLMçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯¹æ•°æ®è¿›è¡Œæ”¿æ²»å†…å®¹åˆ†æå’Œè¿‡æ»¤çš„é‡è¦æ€§ï¼Œä»¥å‡å°‘æ¨¡å‹åè§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š
1. **è‡ªåŠ¨æ”¿æ²»å€¾å‘æ ‡æ³¨æ–¹æ³•**ï¼šä½¿ç”¨äº†è‡ªåŠ¨åŒ–çš„æ–¹æ³•æ¥æ ‡æ³¨æ–‡æ¡£çš„æ”¿æ²»å€¾å‘ï¼Œå…·ä½“æ–¹æ³•æœªçŸ¥ï¼Œä½†éœ€è¦ä¿è¯æ ‡æ³¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚
2. **æ”¿ç­–é—®é¢˜è¯„ä¼°æ–¹æ³•**ï¼šè®¾è®¡äº†è¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šæ”¿ç­–é—®é¢˜ä¸Šçš„ç«‹åœºçš„æ–¹æ³•ï¼Œå…·ä½“æ–¹æ³•æœªçŸ¥ï¼Œä½†éœ€è¦ä¿è¯è¯„ä¼°çš„å®¢è§‚æ€§å’Œå¯é æ€§ã€‚
3. **ç›¸å…³æ€§åˆ†ææ–¹æ³•**ï¼šä½¿ç”¨äº†ç»Ÿè®¡æ–¹æ³•æ¥è¯„ä¼°è®­ç»ƒæ•°æ®ä¸­çš„æ”¿æ²»å†…å®¹ä¸æ¨¡å‹åœ¨æ”¿ç­–é—®é¢˜ä¸Šçš„ç«‹åœºä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå…·ä½“æ–¹æ³•æœªçŸ¥ï¼Œä½†éœ€è¦ä¿è¯ç›¸å…³æ€§åˆ†æçš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶å‘ç°ï¼ŒOLMO2çš„è®­ç»ƒæ•°æ®ä¸­å·¦å€¾æ–‡æ¡£å ä¸»å¯¼åœ°ä½ï¼Œä¸”é¢„è®­ç»ƒæ•°æ®æ¯”åè®­ç»ƒæ•°æ®åŒ…å«æ›´å¤šæ”¿æ²»å†…å®¹ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè®­ç»ƒæ•°æ®ä¸­çš„æ”¿æ²»ç«‹åœºä¸æ¨¡å‹åœ¨æ”¿ç­–é—®é¢˜ä¸Šçš„åè§å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ï¼Œè¿™è¡¨æ˜æ•°æ®åå·®æ˜¯å¯¼è‡´æ¨¡å‹åè§çš„é‡è¦åŸå› ã€‚è¯¥ç ”ç©¶ç»“æœä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ”¹è¿›LLMçš„è®­ç»ƒæµç¨‹ï¼Œé€šè¿‡åˆ†æå’Œè¿‡æ»¤è®­ç»ƒæ•°æ®ä¸­çš„æ”¿æ²»åè§ï¼Œå‡å°‘æ¨¡å‹è¾“å‡ºçš„æ”¿æ²»å€¾å‘æ€§ã€‚è¿™æœ‰åŠ©äºæé«˜LLMåœ¨ä¿¡æ¯æ£€ç´¢ã€å†…å®¹ç”Ÿæˆç­‰é¢†åŸŸçš„å…¬å¹³æ€§å’Œå®¢è§‚æ€§ï¼Œå¹¶ä¸ºæ„å»ºæ›´å€¼å¾—ä¿¡èµ–çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºå…¶ä»–LLMçš„è®­ç»ƒæ•°æ®åˆ†ææä¾›äº†å‚è€ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are known to generate politically biased text, yet how such biases arise remains unclear. A crucial step toward answering this question is the analysis of training data, whose political content remains largely underexplored in current LLM research. To address this gap, we present in this paper an analysis of the pre- and post-training corpora of OLMO2, the largest fully open-source model released together with its complete dataset. From these corpora, we draw large random samples, automatically annotate documents for political orientation, and analyze their source domains and content. We then assess how political content in the training data correlates with models' stance on specific policy issues. Our analysis shows that left-leaning documents predominate across datasets, with pre-training corpora containing significantly more politically engaged content than post-training data. We also find that left- and right-leaning documents frame similar topics through distinct values and sources of legitimacy. Finally, the predominant stance in the training data strongly correlates with models' political biases when evaluated on policy issues. These findings underscore the need to integrate political content analysis into future data curation pipelines as well as in-depth documentation of filtering strategies for transparency.

