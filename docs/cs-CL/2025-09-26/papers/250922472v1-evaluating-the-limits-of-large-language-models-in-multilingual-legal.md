---
layout: default
title: Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning
---

# Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22472" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22472v1</a>
  <a href="https://arxiv.org/pdf/2509.22472.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22472v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22472v1', 'Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Antreas Ioannou, Andreas Shiamishis, Nora Hollenstein, Nezihe Merve GÃ¼rel

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 39 pages, 36 figures. Code and evaluation pipeline available at https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€æ³•å¾‹æ¨ç†ä¸­çš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ³•å¾‹æ¨ç†` `å¤šè¯­è¨€` `å¯¹æŠ—é²æ£’æ€§` `åŸºå‡†æµ‹è¯•` `LLMè¯„ä¼°` `æ³•å¾‹ç§‘æŠ€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ã€è·¨å¸æ³•ç®¡è¾–åŒºçš„æ³•å¾‹æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸‹ã€‚
2. è®ºæ–‡æå‡ºä¸€ä¸ªå¼€æºã€æ¨¡å—åŒ–çš„è¯„ä¼°æµç¨‹ï¼Œç”¨äºå¤šè¯­è¨€ã€ä»»åŠ¡å¤šæ ·çš„æ³•å¾‹ä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°LLMçš„æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ³•å¾‹ä»»åŠ¡å¯¹LLMæ„æˆæŒ‘æˆ˜ï¼Œä¸”æ¨¡å‹åœ¨ä¸åŒè¯­è¨€å’Œå¯¹æŠ—æ€§æ”»å‡»ä¸‹çš„è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸»å¯¼çš„æ—¶ä»£ï¼Œç†è§£å®ƒä»¬çš„èƒ½åŠ›å’Œå±€é™æ€§è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æ³•å¾‹ç­‰é«˜é£é™©é¢†åŸŸã€‚å°½ç®¡Metaçš„LLaMAã€OpenAIçš„ChatGPTã€Googleçš„Geminiã€DeepSeekå’Œå…¶ä»–æ–°å…´æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°è¢«æ•´åˆåˆ°æ³•å¾‹å·¥ä½œæµç¨‹ä¸­ï¼Œä½†å®ƒä»¬åœ¨å¤šè¯­è¨€ã€è·¨å¸æ³•ç®¡è¾–åŒºå’Œå¯¹æŠ—æ€§ç¯å¢ƒä¸­çš„è¡¨ç°ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†LLaMAå’ŒGeminiåœ¨å¤šè¯­è¨€æ³•å¾‹å’Œéæ³•å¾‹åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œå¹¶é€šè¿‡å­—ç¬¦å’Œå•è¯çº§åˆ«çš„æ‰°åŠ¨è¯„ä¼°äº†å®ƒä»¬åœ¨æ³•å¾‹ä»»åŠ¡ä¸­çš„å¯¹æŠ—é²æ£’æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨LLM-as-a-Judgeæ–¹æ³•è¿›è¡Œä¸äººç±»å¯¹é½çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¼€æºçš„ã€æ¨¡å—åŒ–çš„è¯„ä¼°æµç¨‹ï¼Œæ—¨åœ¨æ”¯æŒå¯¹ä»»æ„LLMå’Œæ•°æ®é›†ç»„åˆè¿›è¡Œå¤šè¯­è¨€ã€ä»»åŠ¡å¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œç‰¹åˆ«å…³æ³¨æ³•å¾‹ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ€»ç»“ã€å¼€æ”¾æ€§é—®é¢˜å’Œä¸€èˆ¬æ¨ç†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¯å®ï¼Œæ³•å¾‹ä»»åŠ¡å¯¹LLMæå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œåœ¨LEXamç­‰æ³•å¾‹æ¨ç†åŸºå‡†ä¸Šçš„å‡†ç¡®ç‡é€šå¸¸ä½äº50%ï¼Œè€Œåœ¨XNLIç­‰é€šç”¨ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡70%ã€‚æ­¤å¤–ï¼Œè™½ç„¶è‹±è¯­é€šå¸¸äº§ç”Ÿæ›´ç¨³å®šçš„ç»“æœï¼Œä½†å¹¶ä¸æ€»æ˜¯èƒ½å¸¦æ¥æ›´é«˜çš„å‡†ç¡®ç‡ã€‚æç¤ºæ•æ„Ÿæ€§å’Œå¯¹æŠ—è„†å¼±æ€§ä¹ŸæŒç»­å­˜åœ¨äºå„ç§è¯­è¨€ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§è¯­è¨€çš„æ€§èƒ½ä¸å…¶ä¸è‹±è¯­çš„å¥æ³•ç›¸ä¼¼æ€§ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°LLaMAæ¯”Geminiå¼±ï¼Œåè€…åœ¨åŒä¸€ä»»åŠ¡ä¸­å¹³å‡æœ‰çº¦24ä¸ªç™¾åˆ†ç‚¹çš„ä¼˜åŠ¿ã€‚å°½ç®¡è¾ƒæ–°çš„LLMæœ‰æ‰€æ”¹è¿›ï¼Œä½†åœ¨å…³é”®çš„å¤šè¯­è¨€æ³•å¾‹åº”ç”¨ä¸­å¯é åœ°éƒ¨ç½²å®ƒä»¬ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€æ³•å¾‹æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œå±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹LLMsåœ¨æ³•å¾‹é¢†åŸŸçš„æ·±å…¥è¯„ä¼°ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè¯­è¨€ç¯å¢ƒã€è·¨å¸æ³•ç®¡è¾–åŒºä»¥åŠå¯¹æŠ—æ€§æ”»å‡»ä¸‹çš„é²æ£’æ€§ã€‚ç°æœ‰è¯„ä¼°æ–¹æ³•éš¾ä»¥å…¨é¢è¡¡é‡LLMsåœ¨æ³•å¾‹é¢†åŸŸçš„å®é™…åº”ç”¨èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºå¤šè¯­è¨€æ³•å¾‹åŸºå‡†æµ‹è¯•ï¼Œå¹¶é‡‡ç”¨LLM-as-a-Judgeçš„è¯„ä¼°æ–¹æ³•ï¼Œå…¨é¢è¯„ä¼°LLMsåœ¨æ³•å¾‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚é€šè¿‡å¯¹æŠ—æ€§æ”»å‡»ï¼Œåˆ†æLLMsçš„é²æ£’æ€§ã€‚é€šè¿‡åˆ†æä¸åŒè¯­è¨€çš„è¡¨ç°ï¼Œæ­ç¤ºè¯­è¨€ç‰¹æ€§å¯¹LLMsæ€§èƒ½çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å¤šè¯­è¨€æ³•å¾‹åŸºå‡†æµ‹è¯•æ•°æ®é›†çš„æ„å»ºï¼Œæ¶µç›–åˆ†ç±»ã€æ€»ç»“ã€å¼€æ”¾æ€§é—®é¢˜å’Œä¸€èˆ¬æ¨ç†ç­‰ä»»åŠ¡ï¼›2) åŸºäºLLM-as-a-Judgeçš„è¯„ä¼°æ–¹æ³•ï¼Œåˆ©ç”¨LLMå¯¹å…¶ä»–LLMçš„è¾“å‡ºè¿›è¡Œè¯„ä¼°ï¼Œä»¥å®ç°ä¸äººç±»å¯¹é½çš„è¯„ä¼°ï¼›3) å¯¹æŠ—æ€§æ”»å‡»æ¨¡å—ï¼Œé€šè¿‡å­—ç¬¦å’Œå•è¯çº§åˆ«çš„æ‰°åŠ¨ï¼Œè¯„ä¼°LLMsçš„é²æ£’æ€§ï¼›4) æ€§èƒ½åˆ†ææ¨¡å—ï¼Œåˆ†æLLMsåœ¨ä¸åŒè¯­è¨€å’Œä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶æ¢ç©¶å½±å“æ€§èƒ½çš„å› ç´ ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ„å»ºäº†ä¸€ä¸ªå¼€æºã€æ¨¡å—åŒ–çš„å¤šè¯­è¨€æ³•å¾‹åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡ŒLLMçš„è¯„ä¼°å’Œæ¯”è¾ƒï¼›2) é‡‡ç”¨LLM-as-a-Judgeçš„è¯„ä¼°æ–¹æ³•ï¼Œæé«˜äº†è¯„ä¼°çš„æ•ˆç‡å’Œä¸€è‡´æ€§ï¼›3) æ·±å…¥åˆ†æäº†LLMsåœ¨å¤šè¯­è¨€æ³•å¾‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¤šè¯­è¨€æ³•å¾‹åŸºå‡†æµ‹è¯•æ•°æ®é›†çš„é€‰å–å’Œæ„å»ºï¼Œä¿è¯äº†æ•°æ®é›†çš„å¤šæ ·æ€§å’Œä»£è¡¨æ€§ï¼›2) å¯¹æŠ—æ€§æ”»å‡»ç­–ç•¥çš„è®¾è®¡ï¼ŒåŒ…æ‹¬å­—ç¬¦çº§åˆ«å’Œå•è¯çº§åˆ«çš„æ‰°åŠ¨ï¼Œä»¥æ¨¡æ‹ŸçœŸå®çš„å¯¹æŠ—æ€§ç¯å¢ƒï¼›3) LLM-as-a-Judgeè¯„ä¼°æ–¹æ³•çš„å…·ä½“å®ç°ï¼ŒåŒ…æ‹¬æç¤ºè¯çš„è®¾è®¡å’Œè¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMåœ¨æ³•å¾‹ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡é€šå¸¸ä½äº50%ï¼Œè¿œä½äºé€šç”¨ä»»åŠ¡çš„70%ã€‚Geminiçš„è¡¨ç°ä¼˜äºLLaMAï¼Œå¹³å‡æœ‰24ä¸ªç™¾åˆ†ç‚¹çš„ä¼˜åŠ¿ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œè¯­è¨€çš„å¥æ³•ç»“æ„ä¸è‹±è¯­çš„ç›¸ä¼¼æ€§ä¸LLMåœ¨è¯¥è¯­è¨€ä¸Šçš„æ€§èƒ½ç›¸å…³ã€‚å¯¹æŠ—æ€§æ”»å‡»æ˜¾è‘—é™ä½äº†LLMçš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶é²æ£’æ€§æœ‰å¾…æé«˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ³•å¾‹å’¨è¯¢ã€åˆåŒå®¡æŸ¥ã€æ³•å¾‹æ–‡ä¹¦ç”Ÿæˆç­‰é¢†åŸŸï¼Œæœ‰åŠ©äºæé«˜æ³•å¾‹æœåŠ¡çš„æ•ˆç‡å’Œè´¨é‡ã€‚é€šè¿‡è¯„ä¼°LLMåœ¨å¤šè¯­è¨€æ³•å¾‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¯ä»¥ä¸ºå¼€å‘æ›´å¯é ã€æ›´æ™ºèƒ½çš„æ³•å¾‹AIç³»ç»Ÿæä¾›æŒ‡å¯¼ï¼Œä¿ƒè¿›æ³•å¾‹ç§‘æŠ€çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In an era dominated by Large Language Models (LLMs), understanding their capabilities and limitations, especially in high-stakes fields like law, is crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini, DeepSeek, and other emerging models are increasingly integrated into legal workflows, their performance in multilingual, jurisdictionally diverse, and adversarial contexts remains insufficiently explored. This work evaluates LLaMA and Gemini on multilingual legal and non-legal benchmarks, and assesses their adversarial robustness in legal tasks through character and word-level perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation. We moreover present an open-source, modular evaluation pipeline designed to support multilingual, task-diverse benchmarking of any combination of LLMs and datasets, with a particular focus on legal tasks, including classification, summarization, open questions, and general reasoning. Our findings confirm that legal tasks pose significant challenges for LLMs with accuracies often below 50% on legal reasoning benchmarks such as LEXam, compared to over 70% on general-purpose tasks like XNLI. In addition, while English generally yields more stable results, it does not always lead to higher accuracy. Prompt sensitivity and adversarial vulnerability is also shown to persist across languages. Finally, a correlation is found between the performance of a language and its syntactic similarity to English. We also observe that LLaMA is weaker than Gemini, with the latter showing an average advantage of about 24 percentage points across the same task. Despite improvements in newer LLMs, challenges remain in deploying them reliably for critical, multilingual legal applications.

