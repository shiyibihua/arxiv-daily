---
layout: default
title: Representing LLMs in Prompt Semantic Task Space
---

# Representing LLMs in Prompt Semantic Task Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22506" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22506v1</a>
  <a href="https://arxiv.org/pdf/2509.22506.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22506v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22506v1', 'Representing LLMs in Prompt Semantic Task Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Idan Kashani, Avi Mendelson, Yaniv Nemcovsky

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: Accepted to Findings of the Association for Computational Linguistics: EMNLP 2025

**DOI**: [10.18653/v1/2025.findings-emnlp.456](https://doi.org/10.18653/v1/2025.findings-emnlp.456)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å…è®­ç»ƒæ–¹æ³•ï¼Œå°†LLMè¡¨ç¤ºä¸ºæç¤ºè¯­ä¹‰ä»»åŠ¡ç©ºé—´ä¸­çš„çº¿æ€§ç®—å­ï¼Œç”¨äºæ¨¡å‹é€‰æ‹©ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹è¡¨ç¤º` `æç¤ºå·¥ç¨‹` `æ¨¡å‹é€‰æ‹©` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè¡¨ç¤ºå­¦ä¹ æ–¹æ³•å¯æ‰©å±•æ€§å·®ï¼Œéœ€æ˜‚è´µçš„å†è®­ç»ƒä»¥é€‚åº”æ–°æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä¸”è¡¨ç¤ºç©ºé—´éš¾ä»¥è§£é‡Šã€‚
2. æå‡ºå°†LLMè¡¨ç¤ºä¸ºæç¤ºè¯­ä¹‰ä»»åŠ¡ç©ºé—´ä¸­çš„çº¿æ€§ç®—å­ï¼Œæ— éœ€è®­ç»ƒï¼Œåˆ©ç”¨å‡ ä½•å±æ€§çš„é—­å¼è®¡ç®—ã€‚
3. åœ¨æˆåŠŸé¢„æµ‹å’Œæ¨¡å‹é€‰æ‹©ä»»åŠ¡ä¸ŠéªŒè¯ï¼Œå®ç°äº†æœ‰ç«äº‰åŠ›çš„æˆ–æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ ·æœ¬å¤–åœºæ™¯ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººç©ç›®çš„æˆæœï¼Œå¹¶ä¸”ä¸æ–­æ‰©å¤§çš„å…¬å…±å­˜å‚¨åº“ä¸­åŒ…å«äº†å¤§é‡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å› æ­¤ï¼Œä¸ºç»™å®šä»»åŠ¡è¯†åˆ«æœ€ä½³æ€§èƒ½çš„LLMæ˜¯ä¸€ä¸ªé‡å¤§çš„æŒ‘æˆ˜ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»æå‡ºäº†å­¦ä¹ LLMè¡¨ç¤ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•çš„å¯æ‰©å±•æ€§æœ‰é™ï¼Œå¹¶ä¸”éœ€è¦æ˜‚è´µçš„å†è®­ç»ƒæ‰èƒ½åŒ…å«é¢å¤–çš„æ¨¡å‹å’Œæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„è¡¨ç¤ºåˆ©ç”¨äº†éš¾ä»¥è§£é‡Šçš„ä¸åŒç©ºé—´ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ã€å…è®­ç»ƒçš„æ–¹æ³•ï¼Œå°†LLMè¡¨ç¤ºä¸ºæç¤ºè¯­ä¹‰ä»»åŠ¡ç©ºé—´ä¸­çš„çº¿æ€§ç®—å­ï¼Œä»è€Œæä¾›äº†æ¨¡å‹åº”ç”¨çš„é«˜åº¦å¯è§£é‡Šçš„è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å‡ ä½•å±æ€§çš„é—­å¼è®¡ç®—ï¼Œå¹¶ç¡®ä¿å“è¶Šçš„å¯æ‰©å±•æ€§å’Œå®æ—¶é€‚åº”åŠ¨æ€æ‰©å±•çš„å­˜å‚¨åº“ã€‚æˆ‘ä»¬åœ¨æˆåŠŸé¢„æµ‹å’Œæ¨¡å‹é€‰æ‹©ä»»åŠ¡ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨æ ·æœ¬å¤–åœºæ™¯ä¸­å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æˆ–æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨ä¸ºç‰¹å®šä»»åŠ¡é€‰æ‹©æœ€ä½³LLMæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å·²æœ‰çš„LLMè¡¨ç¤ºå­¦ä¹ æ–¹æ³•å¯æ‰©å±•æ€§å·®ï¼Œéœ€è¦å¤§é‡è®¡ç®—èµ„æºè¿›è¡Œå†è®­ç»ƒä»¥é€‚åº”æ–°çš„æ¨¡å‹å’Œæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•äº§ç”Ÿçš„è¡¨ç¤ºä½äºä¸åŒçš„ç©ºé—´ä¸­ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥ç›´æ¥ç”¨äºæ¨¡å‹é€‰æ‹©å’Œæ€§èƒ½é¢„æµ‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMè§†ä¸ºæç¤ºè¯­ä¹‰ä»»åŠ¡ç©ºé—´ä¸Šçš„çº¿æ€§ç®—å­ã€‚é€šè¿‡åˆ†æLLMå¯¹ä¸åŒæç¤ºçš„å“åº”ï¼Œæå–å…¶åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„å‡ ä½•å±æ€§ï¼Œä»è€Œæ„å»ºLLMçš„è¡¨ç¤ºã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå¯ä»¥å¿«é€Ÿé€‚åº”æ–°çš„LLMå’Œä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1. å®šä¹‰æç¤ºè¯­ä¹‰ä»»åŠ¡ç©ºé—´ï¼šé€‰æ‹©ä¸€ç»„å…·æœ‰ä»£è¡¨æ€§çš„æç¤ºï¼Œæ„æˆä»»åŠ¡ç©ºé—´çš„åŸºç¡€ã€‚2. LLMå“åº”æ”¶é›†ï¼šä½¿ç”¨ä¸åŒçš„LLMå¯¹è¿™äº›æç¤ºè¿›è¡Œå“åº”ï¼Œè®°å½•è¾“å‡ºç»“æœã€‚3. å‡ ä½•å±æ€§è®¡ç®—ï¼šåŸºäºLLMçš„å“åº”ï¼Œè®¡ç®—å…¶åœ¨æç¤ºè¯­ä¹‰ä»»åŠ¡ç©ºé—´ä¸­çš„å‡ ä½•å±æ€§ï¼Œä¾‹å¦‚çº¿æ€§å˜æ¢çŸ©é˜µã€‚4. LLMè¡¨ç¤ºæ„å»ºï¼šå°†è®¡ç®—å¾—åˆ°çš„å‡ ä½•å±æ€§ä½œä¸ºLLMçš„è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å°†LLMè¡¨ç¤ºä¸ºæç¤ºè¯­ä¹‰ä»»åŠ¡ç©ºé—´ä¸­çš„çº¿æ€§ç®—å­çš„æ€æƒ³ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å‡ ä½•å±æ€§çš„é—­å¼è®¡ç®—ï¼Œå¯ä»¥é«˜æ•ˆåœ°æ„å»ºLLMçš„è¡¨ç¤ºã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. æç¤ºçš„é€‰æ‹©ï¼šé€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„æç¤ºï¼Œä»¥è¦†ç›–ä»»åŠ¡ç©ºé—´çš„ä¸åŒæ–¹é¢ã€‚2. å‡ ä½•å±æ€§çš„è®¡ç®—ï¼šé€‰æ‹©åˆé€‚çš„å‡ ä½•å±æ€§ï¼Œä»¥åæ˜ LLMåœ¨ä»»åŠ¡ç©ºé—´ä¸­çš„è¡Œä¸ºã€‚3. çº¿æ€§ç®—å­çš„æ„å»ºï¼šä½¿ç”¨è®¡ç®—å¾—åˆ°çš„å‡ ä½•å±æ€§ï¼Œæ„å»ºLLMçš„çº¿æ€§ç®—å­è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨æˆåŠŸé¢„æµ‹å’Œæ¨¡å‹é€‰æ‹©ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„æˆ–æœ€å…ˆè¿›çš„ç»“æœã€‚å°¤å…¶åœ¨æ ·æœ¬å¤–åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨LLMè¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºLLMæ¨¡å‹é€‰æ‹©ã€æ€§èƒ½é¢„æµ‹å’Œæ¨¡å‹ä¼˜åŒ–ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚ï¼Œå¿«é€Ÿé€‰æ‹©æœ€é€‚åˆçš„LLMæ¨¡å‹ï¼Œæé«˜ä»»åŠ¡å®Œæˆæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºåˆ†æLLMçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œä¸ºæ¨¡å‹æ”¹è¿›æä¾›æŒ‡å¯¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ›´å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å’Œæ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) achieve impressive results over various tasks, and ever-expanding public repositories contain an abundance of pre-trained models. Therefore, identifying the best-performing LLM for a given task is a significant challenge. Previous works have suggested learning LLM representations to address this. However, these approaches present limited scalability and require costly retraining to encompass additional models and datasets. Moreover, the produced representation utilizes distinct spaces that cannot be easily interpreted. This work presents an efficient, training-free approach to representing LLMs as linear operators within the prompts' semantic task space, thus providing a highly interpretable representation of the models' application. Our method utilizes closed-form computation of geometrical properties and ensures exceptional scalability and real-time adaptability to dynamically expanding repositories. We demonstrate our approach on success prediction and model selection tasks, achieving competitive or state-of-the-art results with notable performance in out-of-sample scenarios.

