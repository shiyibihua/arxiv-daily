---
layout: default
title: StateX: Enhancing RNN Recall via Post-training State Expansion
---

# StateX: Enhancing RNN Recall via Post-training State Expansion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22630" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22630v1</a>
  <a href="https://arxiv.org/pdf/2509.22630.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22630v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22630v1', 'StateX: Enhancing RNN Recall via Post-training State Expansion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingyu Shen, Yingfa Chen, Zhen Leng Thai, Xu Han, Zhiyuan Liu, Maosong Sun

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**StateXï¼šé€šè¿‡åè®­ç»ƒçŠ¶æ€æ‰©å±•å¢å¼ºRNNçš„å¬å›èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¾ªç¯ç¥ç»ç½‘ç»œ` `RNN` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `çº¿æ€§æ³¨æ„åŠ›` `åè®­ç»ƒ` `æ¨¡å‹æ‰©å±•` `é•¿æ–‡æœ¬å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformeræ¨¡å‹å¤„ç†é•¿æ–‡æœ¬æˆæœ¬é«˜ï¼Œè€ŒRNNè™½ç„¶å¤æ‚åº¦ä½ï¼Œä½†åœ¨é•¿æ–‡æœ¬ä¿¡æ¯å¬å›æ–¹é¢å­˜åœ¨å›°éš¾ã€‚
2. StateXé€šè¿‡åè®­ç»ƒæ‰©å±•é¢„è®­ç»ƒRNNçš„çŠ¶æ€ï¼Œåœ¨ä¸æ˜¾è‘—å¢åŠ å‚æ•°é‡çš„å‰æä¸‹ï¼Œæå‡æ¨¡å‹å®¹é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒStateXèƒ½æœ‰æ•ˆæå‡RNNçš„å¬å›èƒ½åŠ›å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œä¸”åè®­ç»ƒæˆæœ¬è¾ƒä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Transformeræ¨¡å‹åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é«˜å¤æ‚åº¦å¯¼è‡´å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶æˆæœ¬é«˜æ˜‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œçº¿æ€§æ³¨æ„åŠ›å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ç­‰å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å› å…¶æ’å®šçš„å•tokenå¤æ‚åº¦è€Œå¤‡å—æ¬¢è¿ã€‚ç„¶è€Œï¼Œè¿™äº›å¾ªç¯æ¨¡å‹åœ¨éœ€è¦å‡†ç¡®å›å¿†é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºæ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯éƒ½è¢«å‹ç¼©åˆ°å›ºå®šå¤§å°çš„å¾ªç¯çŠ¶æ€ä¸­ã€‚ä»¥å¾€ç ”ç©¶è¡¨æ˜ï¼Œå¬å›èƒ½åŠ›ä¸å¾ªç¯çŠ¶æ€å¤§å°å‘ˆæ­£ç›¸å…³ï¼Œä½†ç›´æ¥è®­ç»ƒå…·æœ‰è¾ƒå¤§å¾ªç¯çŠ¶æ€çš„RNNä¼šå¯¼è‡´é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†StateXï¼Œä¸€ç§é€šè¿‡åè®­ç»ƒæœ‰æ•ˆæ‰©å±•é¢„è®­ç»ƒRNNçŠ¶æ€çš„è®­ç»ƒæµç¨‹ã€‚é’ˆå¯¹çº¿æ€§æ³¨æ„åŠ›å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹è¿™ä¸¤ç§æµè¡Œçš„RNNï¼Œæˆ‘ä»¬è®¾è®¡äº†åè®­ç»ƒæ¶æ„ä¿®æ”¹ï¼Œä»¥æ‰©å±•çŠ¶æ€å¤§å°ï¼Œè€Œæ¨¡å‹å‚æ•°å‡ ä¹æ²¡æœ‰å¢åŠ ã€‚åœ¨é«˜è¾¾13äº¿å‚æ•°çš„æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒStateXæœ‰æ•ˆåœ°å¢å¼ºäº†RNNçš„å¬å›å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œè€Œä¸ä¼šäº§ç”Ÿé«˜æ˜‚çš„åè®­ç»ƒæˆæœ¬æˆ–æŸå®³å…¶ä»–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šRNNåœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œéœ€è¦å°†æ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯å‹ç¼©åˆ°å›ºå®šå¤§å°çš„å¾ªç¯çŠ¶æ€ä¸­ï¼Œè¿™å¯¼è‡´ä¿¡æ¯æŸå¤±ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å‡†ç¡®å›å¿†é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä»»åŠ¡ä¸­ï¼Œå¬å›èƒ½åŠ›ä¸è¶³ã€‚ç›´æ¥å¢å¤§RNNçš„çŠ¶æ€å¤§å°å¯ä»¥æå‡å¬å›èƒ½åŠ›ï¼Œä½†ä¼šæ˜¾è‘—å¢åŠ è®­ç»ƒæˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šStateXçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨é¢„è®­ç»ƒçš„RNNåŸºç¡€ä¸Šï¼Œé€šè¿‡åè®­ç»ƒçš„æ–¹å¼æ‰©å±•å…¶çŠ¶æ€å¤§å°ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¬å›èƒ½åŠ›ï¼ŒåŒæ—¶é¿å…ä»å¤´è®­ç»ƒå¤§çŠ¶æ€RNNå¸¦æ¥çš„é«˜æ˜‚æˆæœ¬ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å·²ç»å­¦ä¹ åˆ°çš„çŸ¥è¯†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œé«˜æ•ˆçš„çŠ¶æ€æ‰©å±•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šStateXçš„æ•´ä½“æµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨è¾ƒå°çš„çŠ¶æ€å¤§å°é¢„è®­ç»ƒRNNæ¨¡å‹ï¼›2) è®¾è®¡ç‰¹å®šçš„æ¶æ„ä¿®æ”¹ï¼Œä»¥åœ¨åè®­ç»ƒé˜¶æ®µæ‰©å±•RNNçš„çŠ¶æ€å¤§å°ï¼ŒåŒæ—¶å°½å¯èƒ½å‡å°‘å‚æ•°é‡çš„å¢åŠ ï¼›3) ä½¿ç”¨å°‘é‡æ•°æ®å¯¹æ‰©å±•åçš„æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œä»¥ä½¿å…¶é€‚åº”æ›´å¤§çš„çŠ¶æ€ç©ºé—´ã€‚è¯¥æ¡†æ¶é’ˆå¯¹çº¿æ€§æ³¨æ„åŠ›å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ç­‰ç‰¹å®šç±»å‹çš„RNNè¿›è¡Œäº†ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šStateXçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åè®­ç»ƒçŠ¶æ€æ‰©å±•æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—å¢åŠ æ¨¡å‹å‚æ•°é‡çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæå‡RNNçš„å¬å›èƒ½åŠ›ã€‚ä¸ç›´æ¥è®­ç»ƒå¤§çŠ¶æ€RNNç›¸æ¯”ï¼ŒStateXå¤§å¤§é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼ŒStateXè¿˜é’ˆå¯¹ä¸åŒç±»å‹çš„RNNï¼ˆå¦‚çº¿æ€§æ³¨æ„åŠ›å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰è®¾è®¡äº†ç‰¹å®šçš„æ¶æ„ä¿®æ”¹æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šStateXçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é’ˆå¯¹çº¿æ€§æ³¨æ„åŠ›å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œè®¾è®¡äº†ç‰¹å®šçš„çŠ¶æ€æ‰©å±•æ¶æ„ä¿®æ”¹æ–¹æ¡ˆï¼Œä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡å¢åŠ çº¿æ€§æ³¨æ„åŠ›å¤´çš„æ•°é‡æˆ–æ‰©å±•çŠ¶æ€ç©ºé—´æ¨¡å‹çš„ç»´åº¦æ¥å®ç°çŠ¶æ€æ‰©å±•ï¼›2) åœ¨åè®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»¥ä½¿æ¨¡å‹é€‚åº”æ›´å¤§çš„çŠ¶æ€ç©ºé—´ï¼Œå¹¶ä¿æŒå…¶åŸæœ‰çš„æ€§èƒ½ï¼›3) åœ¨æ‰©å±•çŠ¶æ€æ—¶ï¼Œå°½é‡ä¿æŒæ¨¡å‹å‚æ•°é‡çš„ç¨³å®šï¼Œä»¥é¿å…å¼•å…¥è¿‡å¤šçš„è®¡ç®—è´Ÿæ‹…ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

StateXåœ¨é«˜è¾¾13äº¿å‚æ•°çš„æ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡RNNçš„å¬å›èƒ½åŠ›å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œè€Œä¸ä¼šäº§ç”Ÿé«˜æ˜‚çš„åè®­ç»ƒæˆæœ¬æˆ–æŸå®³å…¶ä»–èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®å’Œå¯¹æ¯”åŸºçº¿ä¿¡æ¯åœ¨åŸæ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

StateXæŠ€æœ¯å¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿æ–‡æœ¬åºåˆ—çš„ä»»åŠ¡ï¼Œä¾‹å¦‚é•¿æ–‡æ¡£æ‘˜è¦ã€å¯¹è¯ç³»ç»Ÿã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡RNNçš„å¬å›èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯æ—¶çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥æŠ€æœ¯è¿˜æœ‰åŠ©äºé™ä½é•¿åºåˆ—å»ºæ¨¡çš„è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—èµ„æºå—é™çš„è®¾å¤‡ä¹Ÿèƒ½è¿è¡Œå¤æ‚çš„è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Transformer-based models have demonstrated remarkable language modeling performance, their high complexities result in high costs when processing long contexts. In contrast, recurrent neural networks (RNNs) such as linear attention and state space models have gained popularity due to their constant per-token complexities. However, these recurrent models struggle with tasks that require accurate recall of contextual information from long contexts, because all contextual information is compressed into a constant-size recurrent state. Previous works have shown that recall ability is positively correlated with the recurrent state size, yet directly training RNNs with larger recurrent states results in high training costs. In this paper, we introduce StateX, a training pipeline for efficiently expanding the states of pre-trained RNNs through post-training. For two popular classes of RNNs, linear attention and state space models, we design post-training architectural modifications to scale up the state size with no or negligible increase in model parameters. Experiments on models up to 1.3B parameters demonstrate that StateX efficiently enhances the recall and in-context learning ability of RNNs without incurring high post-training costs or compromising other capabilities.

