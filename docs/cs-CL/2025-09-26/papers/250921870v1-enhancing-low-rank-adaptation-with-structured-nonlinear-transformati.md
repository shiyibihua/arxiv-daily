---
layout: default
title: Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations
---

# Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21870" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21870v1</a>
  <a href="https://arxiv.org/pdf/2509.21870.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21870v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21870v1', 'Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guanzhi Deng, Mingyang Liu, Dapeng Wu, Yinqiao Li, Linqi Song

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: This manuscript has been submitted to IEEE Journal of Selected Topics in Signal Processing (JSTSP) for review. Until the moment I submitted the manuscript to arXiv, we haven't received any review comments from JSTSP

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoRANï¼šé€šè¿‡ç»“æ„åŒ–éçº¿æ€§å˜æ¢å¢å¼ºä½ç§©è‡ªé€‚åº”èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©è‡ªé€‚åº”` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `éçº¿æ€§å˜æ¢` `æ¿€æ´»å‡½æ•°è®¾è®¡` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LoRAçš„çº¿æ€§ç‰¹æ€§é™åˆ¶äº†å…¶è¡¨è¾¾èƒ½åŠ›ï¼Œéš¾ä»¥æ•æ‰å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚
2. LoRANé€šè¿‡å¯¹LoRAçš„ä½ç§©æ›´æ–°åº”ç”¨è½»é‡çº§éçº¿æ€§å˜æ¢æ¥å¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLoRANåœ¨æ‘˜è¦å’Œåˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºQLoRAï¼ŒSinteræ¿€æ´»å‡½æ•°è¡¨ç°æœ€ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½ç§©è‡ªé€‚åº”(LoRA)æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚ç„¶è€Œï¼Œå…¶çº¿æ€§ç‰¹æ€§é™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†LoRANï¼Œä¸€ç§LoRAçš„éçº¿æ€§æ‰©å±•ï¼Œå®ƒå¯¹ä½ç§©æ›´æ–°åº”ç”¨è½»é‡çº§å˜æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†Sinterï¼Œä¸€ç§åŸºäºæ­£å¼¦çš„æ¿€æ´»å‡½æ•°ï¼Œå®ƒåœ¨ä¸å¢åŠ å‚æ•°æ•°é‡çš„æƒ…å†µä¸‹æ·»åŠ ç»“æ„åŒ–æ‰°åŠ¨ã€‚åœ¨æ‘˜è¦å’Œåˆ†ç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLoRANå§‹ç»ˆä¼˜äºQLoRAã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒSinterä¼˜äºSigmoidã€ReLUå’ŒTanhç­‰æ ‡å‡†æ¿€æ´»å‡½æ•°ï¼Œçªå‡ºäº†æ¿€æ´»å‡½æ•°è®¾è®¡åœ¨ä½ç§©å¾®è°ƒä¸­çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šLoRAä½œä¸ºä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œè¢«å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼ŒLoRAæœ¬è´¨ä¸Šæ˜¯çº¿æ€§çš„ï¼Œè¿™é™åˆ¶äº†å…¶è¡¨è¾¾èƒ½åŠ›ï¼Œä½¿å…¶éš¾ä»¥æ•æ‰å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œä»è€Œå½±å“å¾®è°ƒæ•ˆæœã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¢åŠ æ¨¡å‹å‚æ•°çš„æ–¹å¼æ¥æå‡éçº¿æ€§èƒ½åŠ›ï¼Œä½†è¿™ä¸å‚æ•°é«˜æ•ˆå¾®è°ƒçš„åˆè¡·ç›¸æ‚–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLoRANçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨LoRAçš„ä½ç§©æ›´æ–°è¿‡ç¨‹ä¸­å¼•å…¥éçº¿æ€§å˜æ¢ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå‚æ•°é«˜æ•ˆæ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒLoRANåœ¨ä½ç§©çŸ©é˜µçš„æ›´æ–°åï¼Œåº”ç”¨ä¸€ä¸ªè½»é‡çº§çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLoRANçš„æ•´ä½“æ¡†æ¶ä¸LoRAç±»ä¼¼ï¼Œä»ç„¶æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡çŸ©é˜µä¸Šæ·»åŠ ä½ç§©çŸ©é˜µè¿›è¡Œæ›´æ–°ã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼ŒLoRANåœ¨ä½ç§©çŸ©é˜µæ›´æ–°åï¼Œä¼šåº”ç”¨ä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š1. åˆå§‹åŒ–ä½ç§©çŸ©é˜µAå’ŒBï¼›2. å‰å‘ä¼ æ’­æ—¶ï¼Œå°†åŸå§‹æƒé‡çŸ©é˜µWåŠ ä¸Šä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ABï¼›3. åœ¨ABçš„ç»“æœä¸Šåº”ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼›4. åå‘ä¼ æ’­æ—¶ï¼Œåªæ›´æ–°ä½ç§©çŸ©é˜µAå’ŒBçš„å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šLoRANçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå¹¶ä¸”ç‰¹åˆ«è®¾è®¡äº†ä¸€ç§åä¸ºSinterçš„åŸºäºæ­£å¼¦çš„æ¿€æ´»å‡½æ•°ã€‚Sinteræ¿€æ´»å‡½æ•°èƒ½å¤Ÿåœ¨ä¸å¢åŠ å‚æ•°æ•°é‡çš„æƒ…å†µä¸‹ï¼Œä¸ºæ¨¡å‹å¼•å…¥ç»“æ„åŒ–çš„æ‰°åŠ¨ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„Sigmoidã€ReLUç­‰æ¿€æ´»å‡½æ•°ç›¸æ¯”ï¼ŒSinteræ¿€æ´»å‡½æ•°æ›´é€‚åˆäºä½ç§©å¾®è°ƒçš„åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šSinteræ¿€æ´»å‡½æ•°çš„è®¾è®¡æ˜¯LoRANçš„å…³é”®ã€‚Sinteræ¿€æ´»å‡½æ•°çš„å…·ä½“å½¢å¼ä¸ºï¼šSinter(x) = sin(Ï‰x)ï¼Œå…¶ä¸­Ï‰æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„é¢‘ç‡å‚æ•°ã€‚é€šè¿‡è°ƒæ•´Ï‰çš„å€¼ï¼Œå¯ä»¥æ§åˆ¶Sinteræ¿€æ´»å‡½æ•°çš„é¢‘ç‡ï¼Œä»è€Œå½±å“æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLoRANè¿˜é‡‡ç”¨äº†å¤šç§ä¸åŒçš„éçº¿æ€§æ¿€æ´»å‡½æ•°è¿›è¡Œå®éªŒï¼ŒåŒ…æ‹¬Sigmoidã€ReLUã€Tanhç­‰ï¼Œä»¥éªŒè¯Sinteræ¿€æ´»å‡½æ•°çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRANåœ¨æ‘˜è¦å’Œåˆ†ç±»ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºQLoRAã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ‘˜è¦ä»»åŠ¡ä¸Šï¼ŒLoRANç›¸æ¯”QLoRAå–å¾—äº†æ˜¾è‘—çš„ROUGEæŒ‡æ ‡æå‡ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒSinteræ¿€æ´»å‡½æ•°ä¼˜äºSigmoidã€ReLUå’ŒTanhç­‰æ ‡å‡†æ¿€æ´»å‡½æ•°ï¼Œè¯æ˜äº†å…¶åœ¨ä½ç§©å¾®è°ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªåˆ†ç±»æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨Sinteræ¿€æ´»å‡½æ•°çš„LoRANç›¸æ¯”ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°çš„LoRANï¼Œå‡†ç¡®ç‡æå‡äº†1-2ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LoRANå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å‚æ•°é«˜æ•ˆå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬æ‘˜è¦ã€æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚LoRANèƒ½å¤Ÿæå‡æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ï¼Œä½¿å…¶æˆä¸ºä¸€ç§å®ç”¨çš„å¾®è°ƒæ–¹æ³•ã€‚æœªæ¥ï¼ŒLoRANå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è®¡ç®—æœºè§†è§‰å’Œè¯­éŸ³è¯†åˆ«ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning method for large language models. However, its linear nature limits expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies lightweight transformations to the low-rank updates. We further introduce Sinter, a sine-based activation that adds structured perturbations without increasing parameter count. Experiments across summarization and classification tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh, highlighting the importance of activation design in lowrank tuning.

