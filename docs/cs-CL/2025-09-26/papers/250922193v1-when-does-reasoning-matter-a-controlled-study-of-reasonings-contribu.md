---
layout: default
title: When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance
---

# When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22193" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22193v1</a>
  <a href="https://arxiv.org/pdf/2509.22193.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22193v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22193v1', 'When Does Reasoning Matter? A Controlled Study of Reasoning\'s Contribution to Model Performance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Kevin El-Haddad, CÃ©line Hudelot, Pierre Colombo

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶æ¨ç†èƒ½åŠ›å¯¹å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæ­ç¤ºå…¶åœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹è§„æ¨¡ä¸‹çš„æœ‰æ•ˆæ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æŒ‡ä»¤å¾®è°ƒ` `åˆæˆæ•°æ®` `æ¨¡å‹æ€§èƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¯¹æ¨ç†èƒ½åŠ›åœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹è§„æ¨¡ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠè®­ç»ƒå’Œæ¨ç†æˆæœ¬çš„æ¢ç´¢ä¸è¶³ã€‚
2. è®ºæ–‡é‡‡ç”¨åˆæˆæ•°æ®è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡å¤§è§„æ¨¡ç›‘ç£ç ”ç©¶ï¼Œå¯¹æ¯”æŒ‡ä»¤å¾®è°ƒï¼ˆIFTï¼‰å’Œæ¨ç†æ¨¡å‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ¨ç†èƒ½å¤ŸæŒç»­æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¯†é›†å‹å’Œå¼€æ”¾å¼ä»»åŠ¡ä¸Šï¼Œä¼˜äºå¤§å‹IFTæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡­å€Ÿå…¶æ¨ç†èƒ½åŠ›ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å°½ç®¡å…¶ç»éªŒä¸Šçš„æˆåŠŸï¼Œä½†æ¨ç†å˜å¾—æœ‰æ•ˆçš„ä»»åŠ¡å’Œæ¨¡å‹è§„æ¨¡ï¼Œä»¥åŠå…¶è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¾èµ–äºåˆæˆæ•°æ®è’¸é¦æ¡†æ¶æ¥è¿›è¡Œå¤§è§„æ¨¡çš„ç›‘ç£ç ”ç©¶ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†æŒ‡ä»¤å¾®è°ƒï¼ˆIFTï¼‰å’Œä¸åŒå¤§å°çš„æ¨ç†æ¨¡å‹ï¼Œæ¶µç›–äº†å¹¿æ³›çš„ä»¥æ•°å­¦ä¸ºä¸­å¿ƒå’Œé€šç”¨ä»»åŠ¡ï¼Œè¯„ä¼°äº†å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼æ ¼å¼ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¨ç†èƒ½å¤ŸæŒç»­æé«˜æ¨¡å‹æ€§èƒ½ï¼Œé€šå¸¸åŒ¹é…æˆ–è¶…è¿‡æ˜æ˜¾æ›´å¤§çš„IFTç³»ç»Ÿã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶IFTåœ¨è®­ç»ƒå’Œæ¨ç†æˆæœ¬æ–¹é¢ä»ç„¶æ˜¯å¸•ç´¯æ‰˜æœ€ä¼˜çš„ï¼Œä½†éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œæ¨ç†æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šæœ‰ä»·å€¼ï¼Œå…‹æœäº†IFTåœ¨æ¨ç†å¯†é›†å‹å’Œå¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„æ€§èƒ½é™åˆ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†æ¨ç†èƒ½åŠ›ä½•æ—¶ä»¥åŠå¦‚ä½•æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒä»»åŠ¡ç±»å‹å’Œæ¨¡å‹è§„æ¨¡ä¸‹ï¼Œä»ç„¶ç¼ºä¹æ·±å…¥ç ”ç©¶ã€‚æ­¤å¤–ï¼Œæ¨ç†èƒ½åŠ›çš„è®­ç»ƒå’Œæ¨ç†æˆæœ¬ä¹Ÿéœ€è¦è¿›ä¸€æ­¥åˆ†æã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚å•çº¯çš„æŒ‡ä»¤å¾®è°ƒï¼ˆIFTï¼‰ï¼Œå¯èƒ½åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šå­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ§åˆ¶å˜é‡çš„æ–¹å¼ï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶æ¨ç†èƒ½åŠ›å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡åˆæˆæ•°æ®è’¸é¦æ¡†æ¶ï¼Œç”Ÿæˆå¯æ§çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶å¯¹æ¯”ä¸åŒå¤§å°çš„IFTæ¨¡å‹å’Œæ¨ç†æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æ¸…æ™°åœ°æ­ç¤ºæ¨ç†èƒ½åŠ›åœ¨ä¸åŒåœºæ™¯ä¸‹çš„ä»·å€¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨çš„æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **æ•°æ®åˆæˆ**ï¼šåˆ©ç”¨åˆæˆæ•°æ®è’¸é¦æ¡†æ¶ç”ŸæˆåŒ…å«ä¸åŒæ¨ç†éš¾åº¦çš„æ•°å­¦å’Œé€šç”¨ä»»åŠ¡æ•°æ®ã€‚2) **æ¨¡å‹è®­ç»ƒ**ï¼šè®­ç»ƒä¸åŒå¤§å°çš„IFTæ¨¡å‹å’Œæ¨ç†æ¨¡å‹ã€‚3) **æ€§èƒ½è¯„ä¼°**ï¼šåœ¨å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼æ ¼å¼çš„ä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚4) **æˆæœ¬åˆ†æ**ï¼šåˆ†æè®­ç»ƒå’Œæ¨ç†çš„æˆæœ¬ï¼Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **ç³»ç»Ÿæ€§çš„å¯¹æ¯”ç ”ç©¶**ï¼šé€šè¿‡æ§åˆ¶å˜é‡ï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†æ¨ç†èƒ½åŠ›å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæ­ç¤ºäº†æ¨ç†èƒ½åŠ›åœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹è§„æ¨¡ä¸‹çš„ä»·å€¼ã€‚2) **åˆæˆæ•°æ®è’¸é¦æ¡†æ¶çš„åº”ç”¨**ï¼šåˆ©ç”¨åˆæˆæ•°æ®è’¸é¦æ¡†æ¶ç”Ÿæˆå¯æ§çš„è®­ç»ƒæ•°æ®ï¼Œä¸ºç ”ç©¶æ¨ç†èƒ½åŠ›æä¾›äº†ä¾¿åˆ©ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **ä»»åŠ¡é€‰æ‹©**ï¼šé€‰æ‹©äº†ä»¥æ•°å­¦ä¸ºä¸­å¿ƒå’Œé€šç”¨ä»»åŠ¡ï¼Œæ¶µç›–äº†ä¸åŒç±»å‹çš„æ¨ç†éœ€æ±‚ã€‚2) **æ¨¡å‹è§„æ¨¡**ï¼šè®­ç»ƒäº†ä¸åŒå¤§å°çš„IFTæ¨¡å‹å’Œæ¨ç†æ¨¡å‹ï¼Œä»¥ä¾¿ç ”ç©¶æ¨¡å‹è§„æ¨¡å¯¹æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚3) **è¯„ä¼°æŒ‡æ ‡**ï¼šé‡‡ç”¨äº†å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼æ ¼å¼çš„ä»»åŠ¡ï¼Œä»¥ä¾¿å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚4) **æˆæœ¬åˆ†æ**ï¼šåˆ†æäº†è®­ç»ƒå’Œæ¨ç†çš„æˆæœ¬ï¼Œä»¥ä¾¿æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†èƒ½åŠ›èƒ½å¤ŸæŒç»­æé«˜æ¨¡å‹æ€§èƒ½ï¼Œé€šå¸¸åŒ¹é…æˆ–è¶…è¿‡æ˜æ˜¾æ›´å¤§çš„IFTç³»ç»Ÿã€‚å°¤å…¶æ˜¯åœ¨æ¨ç†å¯†é›†å‹å’Œå¼€æ”¾å¼ä»»åŠ¡ä¸Šï¼Œæ¨ç†æ¨¡å‹èƒ½å¤Ÿå…‹æœIFTçš„æ€§èƒ½é™åˆ¶ã€‚è™½ç„¶IFTåœ¨è®­ç»ƒå’Œæ¨ç†æˆæœ¬æ–¹é¢ä»ç„¶æ˜¯å¸•ç´¯æ‰˜æœ€ä¼˜çš„ï¼Œä½†éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œæ¨ç†æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šæœ‰ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæŒ‡å¯¼å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚é€šè¿‡äº†è§£æ¨ç†èƒ½åŠ›åœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹è§„æ¨¡ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œè®­ç»ƒå‡ºæ›´å¼ºå¤§çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›å¯¹å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœºåˆ¶çš„ç†è§£ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡æä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on a wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on a synthetic data distillation framework to conduct a large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on a wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks.

