---
layout: default
title: Evaluating Open-Source Large Language Models for Technical Telecom Question Answering
---

# Evaluating Open-Source Large Language Models for Technical Telecom Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21949" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21949v1</a>
  <a href="https://arxiv.org/pdf/2509.21949.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21949v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21949v1', 'Evaluating Open-Source Large Language Models for Technical Telecom Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Arina Caraus, Alessio Buscemi, Sumit Kumar, Ion Turcanu

**åˆ†ç±»**: cs.NI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: Accepted at the IEEE GLOBECOM Workshops 2025: "Large AI Model over Future Wireless Networks"

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¼€æºå¤§è¯­è¨€æ¨¡å‹åœ¨ç”µä¿¡æŠ€æœ¯é—®ç­”ä¸­çš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç”µä¿¡æŠ€æœ¯` `é—®ç­”ç³»ç»Ÿ` `æ€§èƒ½è¯„ä¼°` `å¼€æºæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”µä¿¡ç­‰æŠ€æœ¯é¢†åŸŸçš„æ€§èƒ½è¯„ä¼°ä¸è¶³ï¼Œç¼ºä¹é’ˆå¯¹æ€§åŸºå‡†ã€‚
2. è®ºæ–‡æ„å»ºç”µä¿¡é¢†åŸŸé—®ç­”åŸºå‡†ï¼Œè¯„ä¼°Gemmaå’ŒDeepSeekä¸¤ä¸ªå¼€æºLLMçš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGemmaåœ¨è¯­ä¹‰ä¿çœŸåº¦ä¸Šæ›´ä¼˜ï¼ŒDeepSeekåœ¨è¯æ±‡ä¸€è‡´æ€§ä¸Šç•¥èƒœä¸€ç­¹ï¼Œä½†éƒ½å­˜åœ¨å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸéƒ½å±•ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç”µä¿¡ç­‰æŠ€æœ¯é¢†åŸŸçš„æ€§èƒ½ä»æœ‰å¾…æ¢ç´¢ã€‚æœ¬æ–‡è¯„ä¼°äº†ä¸¤ä¸ªå¼€æºLLMï¼ŒGemma 3 27Bå’ŒDeepSeek R1 32Bï¼Œåœ¨åŸºäºé«˜çº§æ— çº¿é€šä¿¡ææ–™çš„äº‹å®æ€§å’Œæ¨ç†æ€§é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«105ä¸ªé—®ç­”å¯¹çš„åŸºå‡†ï¼Œå¹¶ä½¿ç”¨è¯æ±‡æŒ‡æ ‡ã€è¯­ä¹‰ç›¸ä¼¼æ€§å’ŒLLM-as-a-judgeè¯„åˆ†æ¥è¯„ä¼°æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æºå±æ€§å’Œåˆ†æ•°æ–¹å·®åˆ†æäº†ä¸€è‡´æ€§ã€åˆ¤æ–­å¯é æ€§å’Œå¹»è§‰ã€‚ç»“æœè¡¨æ˜ï¼ŒGemmaåœ¨è¯­ä¹‰ä¿çœŸåº¦å’ŒLLMè¯„åˆ†çš„æ­£ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€ŒDeepSeekåœ¨è¯æ±‡ä¸€è‡´æ€§æ–¹é¢ç•¥é«˜ã€‚å…¶ä»–å‘ç°å¼ºè°ƒäº†å½“å‰ç”µä¿¡åº”ç”¨çš„å±€é™æ€§ï¼Œä»¥åŠå¯¹é¢†åŸŸè‡ªé€‚åº”æ¨¡å‹çš„éœ€æ±‚ï¼Œä»¥æ”¯æŒå·¥ç¨‹é¢†åŸŸä¸­å€¼å¾—ä¿¡èµ–çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åŠ©æ‰‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è¯„ä¼°é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”µä¿¡æŠ€æœ¯é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹LLMåœ¨ç‰¹å®šæŠ€æœ¯é¢†åŸŸï¼ˆå¦‚ç”µä¿¡ï¼‰çš„æ·±å…¥è¯„ä¼°ï¼Œå¹¶ä¸”ç¼ºä¹ä¸“é—¨çš„åŸºå‡†æ•°æ®é›†æ¥è¡¡é‡å…¶æ€§èƒ½ã€‚ç°æœ‰é€šç”¨LLMå¯èƒ½æ— æ³•å……åˆ†ç†è§£ç”µä¿¡é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¯¼è‡´å›ç­”ä¸å‡†ç¡®æˆ–äº§ç”Ÿå¹»è§‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨é’ˆå¯¹ç”µä¿¡é¢†åŸŸçš„é—®ç­”åŸºå‡†ï¼Œå¹¶ä½¿ç”¨å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼ˆåŒ…æ‹¬è¯æ±‡æŒ‡æ ‡ã€è¯­ä¹‰ç›¸ä¼¼æ€§å’ŒLLM-as-a-judgeè¯„åˆ†ï¼‰æ¥å…¨é¢è¯„ä¼°LLMçš„æ€§èƒ½ã€‚é€šè¿‡åˆ†æLLMåœ¨ä¸åŒæŒ‡æ ‡ä¸Šçš„è¡¨ç°ï¼Œå¯ä»¥æ·±å…¥äº†è§£å…¶åœ¨ç”µä¿¡é¢†åŸŸçš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) æ„å»ºç”µä¿¡é¢†åŸŸé—®ç­”åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«105ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–äº‹å®æ€§å’Œæ¨ç†æ€§é—®é¢˜ã€‚2) é€‰æ‹©ä¸¤ä¸ªå¼€æºLLMï¼ˆGemma 3 27Bå’ŒDeepSeek R1 32Bï¼‰è¿›è¡Œè¯„ä¼°ã€‚3) ä½¿ç”¨è¯æ±‡æŒ‡æ ‡ï¼ˆå¦‚BLEUï¼‰ã€è¯­ä¹‰ç›¸ä¼¼æ€§æŒ‡æ ‡ï¼ˆå¦‚BERTScoreï¼‰å’ŒLLM-as-a-judgeè¯„åˆ†æ¥è¯„ä¼°LLMçš„æ€§èƒ½ã€‚4) åˆ†æLLMçš„ä¸€è‡´æ€§ã€åˆ¤æ–­å¯é æ€§å’Œå¹»è§‰ï¼Œé€šè¿‡æºå±æ€§å’Œåˆ†æ•°æ–¹å·®è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ç”µä¿¡é¢†åŸŸçš„é—®ç­”åŸºå‡†æ•°æ®é›†ï¼Œè¿™å¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬LLM-as-a-judgeè¯„åˆ†ï¼Œä»è€Œæ›´å…¨é¢åœ°è¯„ä¼°äº†LLMçš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶æ›´æ³¨é‡å¯¹LLMåœ¨ç‰¹å®šæŠ€æœ¯é¢†åŸŸçš„æ·±å…¥è¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šåŸºå‡†æ•°æ®é›†åŒ…å«105ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–é«˜çº§æ— çº¿é€šä¿¡ææ–™ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ï¼šBLEU (Bilingual Evaluation Understudy) ç”¨äºè¯„ä¼°è¯æ±‡ä¸€è‡´æ€§ï¼ŒBERTScore ç”¨äºè¯„ä¼°è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä»¥åŠä½¿ç”¨ GPT-4 ä½œä¸ºè£åˆ¤çš„ LLM-as-a-judge è¯„åˆ†æ¥è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¿˜é€šè¿‡åˆ†æç­”æ¡ˆçš„æ¥æºå’Œåˆ†æ•°æ–¹å·®æ¥è¯„ä¼° LLM çš„å¹»è§‰å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGemmaåœ¨è¯­ä¹‰ä¿çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€ŒDeepSeekåœ¨è¯æ±‡ä¸€è‡´æ€§æ–¹é¢ç•¥èƒœä¸€ç­¹ã€‚å…·ä½“æ¥è¯´ï¼ŒGemmaåœ¨LLM-as-a-judgeè¯„åˆ†ä¸­è¡¨ç°æ›´å¥½ï¼Œè¡¨æ˜å…¶ç”Ÿæˆçš„ç­”æ¡ˆæ›´ç¬¦åˆäººç±»ä¸“å®¶çš„åˆ¤æ–­ã€‚ç„¶è€Œï¼Œä¸¤ä¸ªæ¨¡å‹éƒ½å­˜åœ¨å¹»è§‰å’Œä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œè¡¨æ˜éœ€è¦è¿›ä¸€æ­¥çš„é¢†åŸŸè‡ªé€‚åº”è®­ç»ƒæ‰èƒ½æ»¡è¶³ç”µä¿¡é¢†åŸŸçš„éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘ç”µä¿¡é¢†åŸŸçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€æŠ€æœ¯æ”¯æŒåŠ©æ‰‹å’Œæ•™è‚²å·¥å…·ã€‚é€šè¿‡é¢†åŸŸè‡ªé€‚åº”è®­ç»ƒï¼Œå¯ä»¥æå‡LLMåœ¨ç”µä¿¡é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä»è€Œä¸ºå·¥ç¨‹å¸ˆå’ŒæŠ€æœ¯äººå‘˜æä¾›æ›´å‡†ç¡®ã€å¯é çš„ä¿¡æ¯æ”¯æŒï¼Œå¹¶ä¿ƒè¿›ç”µä¿¡æŠ€æœ¯çš„åˆ›æ–°å’Œå‘å±•ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•åˆ©ç”¨è¿™äº›æ¨¡å‹æ¥è‡ªåŠ¨åŒ–ç”µä¿¡ç½‘ç»œçš„è®¾è®¡ã€ä¼˜åŒ–å’Œæ•…éšœæ’é™¤ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown remarkable capabilities across various fields. However, their performance in technical domains such as telecommunications remains underexplored. This paper evaluates two open-source LLMs, Gemma 3 27B and DeepSeek R1 32B, on factual and reasoning-based questions derived from advanced wireless communications material. We construct a benchmark of 105 question-answer pairs and assess performance using lexical metrics, semantic similarity, and LLM-as-a-judge scoring. We also analyze consistency, judgment reliability, and hallucination through source attribution and score variance. Results show that Gemma excels in semantic fidelity and LLM-rated correctness, while DeepSeek demonstrates slightly higher lexical consistency. Additional findings highlight current limitations in telecom applications and the need for domain-adapted models to support trustworthy Artificial Intelligence (AI) assistants in engineering.

