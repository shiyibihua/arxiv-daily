---
layout: default
title: Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs
---

# Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22251" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22251v1</a>
  <a href="https://arxiv.org/pdf/2509.22251.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22251v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22251v1', 'Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 11 pages, 5 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yfangZhang/SSKG-LLM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSSKG-LLMï¼Œé€šè¿‡ç»“æ„åŒ–å›¾ç¼–ç å’Œè‡ªé€‚åº”ç©ºé—´å¯¹é½ç¼“è§£LLMå¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†å›¾è°±` `å¹»è§‰é—®é¢˜` `ç»“æ„åŒ–ç¼–ç ` `ç©ºé—´å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè§£å†³å¹»è§‰é—®é¢˜æ—¶ï¼Œå¯¹çŸ¥è¯†å›¾è°±çš„å¤„ç†æ–¹å¼è¿‡äºç®€å•ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å…¶ç»“æ„ä¿¡æ¯ã€‚
2. SSKG-LLMæ¨¡å‹é€šè¿‡çŸ¥è¯†å›¾è°±æ£€ç´¢ã€ç¼–ç å’Œé€‚é…æ¨¡å—ï¼Œå°†çŸ¥è¯†å›¾è°±çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯æœ‰æ•ˆèå…¥LLMæ¨ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆçŸ¥è¯†å›¾è°±ç»“æ„ä¿¡æ¯èƒ½å¤Ÿæ˜¾è‘—æå‡LLMçš„äº‹å®æ¨ç†èƒ½åŠ›ï¼Œæœ‰æ•ˆç¼“è§£å¹»è§‰é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›®å‰ï¼Œè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹»è§‰é—®é¢˜çš„ä¸»è¦æ–¹æ³•æ˜¯èå…¥çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ã€‚ç„¶è€Œï¼ŒLLMé€šå¸¸å°†KGè§†ä¸ºçº¯æ–‡æœ¬ï¼Œä»…æå–è¯­ä¹‰ä¿¡æ¯ï¼Œé™åˆ¶äº†KGå…³é”®ç»“æ„åŒ–æ–¹é¢çš„åˆ©ç”¨ã€‚å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯KGç¼–ç å™¨å’ŒLLMæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„åµŒå…¥ç©ºé—´å­˜åœ¨å·®è·ï¼Œé˜»ç¢äº†ç»“æ„åŒ–çŸ¥è¯†çš„æœ‰æ•ˆæ•´åˆã€‚ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæˆ‘ä»¬æå‡ºäº†SSKG-LLMï¼Œä¸€ç§åˆ›æ–°çš„æ¨¡å‹æ¶æ„ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°å°†KGçš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯æ•´åˆåˆ°LLMçš„æ¨ç†è¿‡ç¨‹ä¸­ã€‚SSKG-LLMåŒ…å«çŸ¥è¯†å›¾è°±æ£€ç´¢ï¼ˆKGRï¼‰æ¨¡å—å’ŒçŸ¥è¯†å›¾è°±ç¼–ç ï¼ˆKGEï¼‰æ¨¡å—ï¼Œä»¥ä¿ç•™è¯­ä¹‰å¹¶åˆ©ç”¨ç»“æ„ã€‚ç„¶åï¼ŒåŠ å…¥çŸ¥è¯†å›¾è°±é€‚é…ï¼ˆKGAï¼‰æ¨¡å—ï¼Œä½¿LLMèƒ½å¤Ÿç†è§£KGåµŒå…¥ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„åˆ†æï¼Œä»¥æ¢ç´¢ç»“åˆKGçš„ç»“æ„ä¿¡æ¯å¦‚ä½•å¢å¼ºLLMçš„äº‹å®æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨https://github.com/yfangZhang/SSKG-LLM è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æ—¶ï¼Œé€šå¸¸å°†å…¶è§†ä¸ºçº¯æ–‡æœ¬ï¼Œä»…å…³æ³¨è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†KGä¸­è•´å«çš„ä¸°å¯Œç»“æ„ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒKGç¼–ç å™¨å’ŒLLMæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„ç©ºé—´å·®å¼‚ï¼Œä½¿å¾—ç»“æ„åŒ–çŸ¥è¯†éš¾ä»¥æœ‰æ•ˆæ•´åˆï¼Œå¯¼è‡´LLMåœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å®¹æ˜“å‡ºç°å¹»è§‰é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSSKG-LLMçš„æ ¸å¿ƒæ€è·¯æ˜¯åŒæ—¶åˆ©ç”¨KGçš„è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”ç©ºé—´å¯¹é½ï¼Œå¼¥åˆKGåµŒå…¥å’ŒLLMæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å·®è·ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£å’Œåˆ©ç”¨KGä¸­çš„çŸ¥è¯†ï¼Œä»è€Œå‡å°‘å¹»è§‰çš„äº§ç”Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSSKG-LLMåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šçŸ¥è¯†å›¾è°±æ£€ç´¢ï¼ˆKGRï¼‰æ¨¡å—ã€çŸ¥è¯†å›¾è°±ç¼–ç ï¼ˆKGEï¼‰æ¨¡å—å’ŒçŸ¥è¯†å›¾è°±é€‚é…ï¼ˆKGAï¼‰æ¨¡å—ã€‚é¦–å…ˆï¼ŒKGRæ¨¡å—è´Ÿè´£ä»KGä¸­æ£€ç´¢ç›¸å…³çŸ¥è¯†ã€‚ç„¶åï¼ŒKGEæ¨¡å—å¯¹æ£€ç´¢åˆ°çš„KGè¿›è¡Œç¼–ç ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯ã€‚æœ€åï¼ŒKGAæ¨¡å—å°†KGåµŒå…¥ç©ºé—´ä¸LLMæ–‡æœ¬åµŒå…¥ç©ºé—´å¯¹é½ï¼Œä½¿LLMèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨KGçŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šSSKG-LLMçš„å…³é”®åˆ›æ–°åœ¨äºå…¶èƒ½å¤ŸåŒæ—¶åˆ©ç”¨KGçš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”ç©ºé—´å¯¹é½æ–¹æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSSKG-LLMä¸ä»…å…³æ³¨KGçš„è¯­ä¹‰ä¿¡æ¯ï¼Œè¿˜å……åˆ†åˆ©ç”¨äº†å…¶ç»“æ„ä¿¡æ¯ï¼Œä»è€Œæ›´å…¨é¢åœ°ç†è§£å’Œåˆ©ç”¨KGä¸­çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”ç©ºé—´å¯¹é½æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¼¥åˆKGåµŒå…¥å’ŒLLMæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å·®è·ï¼Œä½¿å¾—LLMèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨KGçŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šKGRæ¨¡å—çš„å…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼ŒKGEæ¨¡å—å¯èƒ½é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œç­‰æ–¹æ³•å¯¹KGè¿›è¡Œç¼–ç ï¼ŒKGAæ¨¡å—å¯èƒ½é‡‡ç”¨å¯¹æŠ—è®­ç»ƒæˆ–æ˜ å°„å‡½æ•°ç­‰æ–¹æ³•è¿›è¡Œç©ºé—´å¯¹é½ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†SSKG-LLMçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜ç»“åˆçŸ¥è¯†å›¾è°±ç»“æ„ä¿¡æ¯èƒ½å¤Ÿæ˜¾è‘—æå‡LLMçš„äº‹å®æ¨ç†èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®ã€å¯¹æ¯”åŸºçº¿å’Œæå‡å¹…åº¦åœ¨æ‘˜è¦ä¸­æœªæåŠï¼Œéœ€æŸ¥é˜…è®ºæ–‡å…¨æ–‡è·å–è¯¦ç»†ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºé—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±è¡¥å…¨ã€æ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡æå‡LLMçš„äº‹å®æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘ç”Ÿæˆæ–‡æœ¬ä¸­çš„é”™è¯¯ä¿¡æ¯ï¼Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨åŒ»ç–—ã€é‡‘èç­‰å¯¹ä¿¡æ¯å‡†ç¡®æ€§è¦æ±‚è¾ƒé«˜çš„é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Currently, the main approach for Large Language Models (LLMs) to tackle the hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs typically treat KGs as plain text, extracting only semantic information and limiting their use of the crucial structural aspects of KGs. Another challenge is the gap between the embedding spaces of KGs encoders and LLMs text embeddings, which hinders the effective integration of structured knowledge. To overcome these obstacles, we put forward the SSKG-LLM, an innovative model architecture that is designed to efficiently integrate both the Structural and Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph Encoding (KGE) module to preserve semantics while utilizing structure. Then, the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to understand KGs embeddings. We conduct extensive experiments and provide a detailed analysis to explore how incorporating the structural information of KGs can enhance the factual reasoning abilities of LLMs. Our code are available at https://github.com/yfangZhang/SSKG-LLM.

