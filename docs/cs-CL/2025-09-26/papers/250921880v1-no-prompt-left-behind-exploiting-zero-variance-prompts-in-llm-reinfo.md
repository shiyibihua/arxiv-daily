---
layout: default
title: No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping
---

# No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21880" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21880v1</a>
  <a href="https://arxiv.org/pdf/2509.21880.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21880v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21880v1', 'No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thanh-Long V. Le, Myeongho Jeon, Kim Vu, Viet Lai, Eunho Yang

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRL-ZVPç®—æ³•ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­é›¶æ–¹å·®æç¤ºæå‡æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `é›¶æ–¹å·®æç¤º` `æ•°å­¦æ¨ç†` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¿½ç•¥äº†é›¶æ–¹å·®æç¤ºï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚
2. RL-ZVPç®—æ³•åˆ©ç”¨é›¶æ–¹å·®æç¤ºï¼Œç›´æ¥å¥–åŠ±æ­£ç¡®ç­”æ¡ˆå¹¶æƒ©ç½šé”™è¯¯ç­”æ¡ˆï¼Œæå‡å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRL-ZVPåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå‡†ç¡®ç‡å’Œé€šè¿‡ç‡å‡æœ‰æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)æ˜¯æå‡å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¦‚GRPOä»…ä¾èµ–äºæ¨¡å‹å¯¹ç›¸åŒè¾“å…¥äº§ç”Ÿä¸åŒæ­£ç¡®æ€§å“åº”çš„é—®é¢˜ï¼Œå¿½ç•¥äº†æ‰€æœ‰å“åº”è·å¾—ç›¸åŒå¥–åŠ±çš„æƒ…å†µï¼Œå³æ‰€è°“çš„é›¶æ–¹å·®æç¤ºã€‚æœ¬æ–‡è®¤ä¸ºï¼Œè¿™äº›æç¤ºå¹¶éæ— ç”¨ï¼Œå®é™…ä¸Šå¯ä»¥ä¸ºç­–ç•¥ä¼˜åŒ–æä¾›æœ‰æ„ä¹‰çš„åé¦ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé›¶æ–¹å·®æç¤ºçš„å¼ºåŒ–å­¦ä¹ (RL-ZVP)ï¼Œä¸€ç§ä»é›¶æ–¹å·®æç¤ºä¸­æå–å­¦ä¹ ä¿¡å·çš„æ–°ç®—æ³•ã€‚RL-ZVPç›´æ¥å¥–åŠ±æ­£ç¡®æ€§å¹¶æƒ©ç½šé”™è¯¯ï¼Œå³ä½¿æ²¡æœ‰å¯¹æ¯”å“åº”ï¼Œä¹Ÿèƒ½é€šè¿‡tokençº§åˆ«çš„ç‰¹å¾æ¥è°ƒèŠ‚åé¦ˆï¼Œä»è€Œä¿ç•™ä¿¡æ¯ä¸°å¯Œä¸”ç»†è‡´çš„ä¿¡å·ã€‚åœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRL-ZVPç›¸å¯¹äºGRPOå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå‡†ç¡®ç‡æé«˜äº†8.61ä¸ªç™¾åˆ†ç‚¹ï¼Œé€šè¿‡ç‡æé«˜äº†7.77ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºå…¶ä»–è¿‡æ»¤é›¶æ–¹å·®æç¤ºçš„åŸºçº¿ã€‚è¿™äº›ç»“æœçªå‡ºäº†åœ¨RLVRä¸­ä»é›¶æ–¹å·®æç¤ºä¸­å­¦ä¹ çš„æœªå¼€å‘æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¦‚GRPOï¼Œåœ¨è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œä¸»è¦ä¾èµ–äºæ¨¡å‹å¯¹ç›¸åŒè¾“å…¥äº§ç”Ÿä¸åŒæ­£ç¡®æ€§å“åº”çš„æ ·æœ¬ã€‚ç„¶è€Œï¼Œå¤§é‡æ ·æœ¬ä¸­ï¼Œæ¨¡å‹å¯¹ç›¸åŒè¾“å…¥çš„å“åº”å…·æœ‰ä¸€è‡´çš„æ­£ç¡®æ€§ï¼ˆå³é›¶æ–¹å·®æç¤ºï¼‰ï¼Œè¿™äº›æ ·æœ¬è¢«ç°æœ‰æ–¹æ³•å¿½ç•¥ï¼Œé€ æˆäº†ä¿¡æ¯æµªè´¹ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRL-ZVPçš„æ ¸å¿ƒæ€è·¯æ˜¯å……åˆ†åˆ©ç”¨é›¶æ–¹å·®æç¤ºä¸­è•´å«çš„ä¿¡æ¯ã€‚å³ä½¿æ¨¡å‹å¯¹åŒä¸€è¾“å…¥çš„å¤šæ¬¡å“åº”éƒ½æ­£ç¡®æˆ–éƒ½é”™è¯¯ï¼Œè¿™äº›ä¿¡æ¯ä»ç„¶å¯ä»¥ç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡ç›´æ¥å¥–åŠ±æ­£ç¡®ç­”æ¡ˆå¹¶æƒ©ç½šé”™è¯¯ç­”æ¡ˆï¼Œå³ä½¿æ²¡æœ‰å¯¹æ¯”ï¼Œä¹Ÿèƒ½ä¸ºç­–ç•¥ä¼˜åŒ–æä¾›æœ‰ä»·å€¼çš„åé¦ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRL-ZVPçš„æ•´ä½“æ¡†æ¶ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ç±»ä¼¼ï¼Œä½†å…³é”®åœ¨äºå¦‚ä½•å¤„ç†é›¶æ–¹å·®æç¤ºã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) ä»æ•°æ®é›†ä¸­ç­›é€‰å‡ºé›¶æ–¹å·®æç¤ºï¼›2) å¯¹äºæ¯ä¸ªé›¶æ–¹å·®æç¤ºï¼Œæ ¹æ®æ¨¡å‹çš„å“åº”ç»“æœï¼Œç»™äºˆç›¸åº”çš„å¥–åŠ±æˆ–æƒ©ç½šï¼›3) ä½¿ç”¨è¿™äº›å¥–åŠ±ä¿¡å·æ¥æ›´æ–°æ¨¡å‹çš„ç­–ç•¥ã€‚æ¡†æ¶çš„å…³é”®åœ¨äºå¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†æ­£ç¡®å’Œé”™è¯¯çš„å“åº”ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œæ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šRL-ZVPæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå®ƒèƒ½å¤Ÿä»é›¶æ–¹å·®æç¤ºä¸­æå–æœ‰ç”¨çš„å­¦ä¹ ä¿¡å·ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒRL-ZVPå¹¶ä¸ç®€å•åœ°ä¸¢å¼ƒè¿™äº›æç¤ºï¼Œè€Œæ˜¯é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œå°†è¿™äº›æç¤ºè½¬åŒ–ä¸ºæœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å……åˆ†åœ°åˆ©ç”¨æ•°æ®ï¼Œæé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šRL-ZVPçš„å…³é”®è®¾è®¡åœ¨äºå…¶å¥–åŠ±å‡½æ•°ã€‚è¯¥å¥–åŠ±å‡½æ•°ä¸ä»…è€ƒè™‘äº†å“åº”çš„æ­£ç¡®æ€§ï¼Œè¿˜è€ƒè™‘äº†tokençº§åˆ«çš„ç‰¹å¾ï¼Œä¾‹å¦‚tokençš„ç†µã€‚é€šè¿‡tokençº§åˆ«çš„ç†µæ¥è°ƒèŠ‚åé¦ˆï¼Œå¯ä»¥ä¿ç•™ä¿¡æ¯ä¸°å¯Œä¸”ç»†è‡´çš„ä¿¡å·ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ­£ç¡®çš„å“åº”ï¼Œå¥–åŠ±å‡½æ•°ä¼šç»™äºˆæ­£å‘å¥–åŠ±ï¼Œå¯¹äºé”™è¯¯çš„å“åº”ï¼Œå¥–åŠ±å‡½æ•°ä¼šç»™äºˆè´Ÿå‘å¥–åŠ±ã€‚å¥–åŠ±çš„å¤§å°ä¼šæ ¹æ®tokençš„ç†µè¿›è¡Œè°ƒæ•´ï¼Œç†µè¶Šé«˜ï¼Œå¥–åŠ±æˆ–æƒ©ç½šè¶Šå¤§ã€‚æ­¤å¤–ï¼ŒRL-ZVPè¿˜å¯èƒ½åŒ…å«ä¸€äº›æ­£åˆ™åŒ–é¡¹ï¼Œä»¥é˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRL-ZVPåœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºGRPOå’Œå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒRL-ZVPçš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾8.61ä¸ªç™¾åˆ†ç‚¹ï¼Œé€šè¿‡ç‡æé«˜äº†é«˜è¾¾7.77ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRL-ZVPèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨é›¶æ–¹å·®æç¤ºï¼Œä»è€Œæ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RL-ZVPç®—æ³•å¯å¹¿æ³›åº”ç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¾‹å¦‚æ•°å­¦æ¨ç†ã€å¸¸è¯†æ¨ç†å’Œä»£ç ç”Ÿæˆã€‚é€šè¿‡æ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®­ç»ƒæ•°æ®ï¼Œè¯¥æ–¹æ³•å¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼Œæé«˜æ¨¡å‹ç²¾åº¦ï¼Œå¹¶ä¿ƒè¿›å¤§è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²ã€ç§‘ç ”å’Œå·¥ä¸šç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward - so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR.

