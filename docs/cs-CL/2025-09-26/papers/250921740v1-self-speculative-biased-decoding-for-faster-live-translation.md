---
layout: default
title: Self-Speculative Biased Decoding for Faster Live Translation
---

# Self-Speculative Biased Decoding for Faster Live Translation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21740" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21740v1</a>
  <a href="https://arxiv.org/pdf/2509.21740.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21740v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21740v1', 'Self-Speculative Biased Decoding for Faster Live Translation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Linxiao Zeng, Haoyun Deng, Kangyuan Shu, Shizhen Wang

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæ¨æµ‹åç½®è§£ç ï¼ŒåŠ é€Ÿä½å»¶è¿Ÿç›´æ’­ç¿»è¯‘ï¼Œæ— éœ€é¢å¤–æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›´æ’­ç¿»è¯‘` `ä½å»¶è¿Ÿ` `è‡ªæ¨æµ‹è§£ç ` `åç½®è§£ç ` `æµå¼ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨ç›´æ’­ç¿»è¯‘ç­‰æµå¼åº”ç”¨ä¸­ï¼Œé¢ä¸´è®¡ç®—æˆæœ¬é«˜ã€å»¶è¿Ÿéš¾ä»¥æ»¡è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´ç”¨æˆ·ä½“éªŒä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºè‡ªæ¨æµ‹åç½®è§£ç ï¼Œåˆ©ç”¨å…ˆå‰è¾“å‡ºä½œä¸ºè‰ç¨¿ï¼Œå¹¶é€šè¿‡åç½®è§£ç æé«˜è‰ç¨¿æ¥å—ç‡ï¼Œé¿å…é‡å¤ç”Ÿæˆï¼Œé™ä½å»¶è¿Ÿã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸æŸå¤±ç¿»è¯‘è´¨é‡çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾1.7å€çš„åŠ é€Ÿï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†80%çš„é—ªçƒç°è±¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘åœ¨å„ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨æµå¼åº”ç”¨ï¼ˆå¦‚ç›´æ’­ç¿»è¯‘ï¼‰ä¸­ç›´æ¥ä½¿ç”¨å®ƒä»¬ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºåœ¨ç›´æ’­ç¿»è¯‘ä¸­ï¼Œè¾“å‡ºå¿…é¡»éšç€è¾“å…¥ä¸Šä¸‹æ–‡çš„æ‰©å±•è€Œä¸æ–­æ›´æ–°ï¼ŒåŒæ—¶ä»éœ€ä¿æŒåˆç†çš„è®¡ç®—æˆæœ¬ä»¥æ»¡è¶³å»¶è¿Ÿè¦æ±‚ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†ç”¨äºåŒå£°ç¿»è¯‘çš„é‡è¯‘æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨ç†èŒƒå¼â€”â€”è‡ªæ¨æµ‹åç½®è§£ç ï¼Œæ—¨åœ¨é¿å…ä¸ºæŒç»­å¢é•¿çš„è¾“å…¥æµé‡å¤ä»å¤´å¼€å§‹ç”Ÿæˆè¾“å‡ºã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨æœ€æ–°çš„è¾“å‡ºä½œä¸ºå½“å‰å¢é•¿çš„è¾“å…¥ä¸Šä¸‹æ–‡çš„è‰ç¨¿ã€‚åœ¨éªŒè¯é˜¶æ®µï¼Œè¾“å‡ºå°†åå‘äºè‰ç¨¿tokenï¼Œä»¥è·å¾—æ›´é«˜çš„è‰ç¨¿æ¥å—ç‡ã€‚è¿™ç§ç­–ç•¥ä¸ä»…æœ€å¤§é™åº¦åœ°å‡å°‘äº†å¯èƒ½åˆ†æ•£ç”¨æˆ·æ³¨æ„åŠ›çš„é—ªçƒï¼Œè€Œä¸”è¿˜å¸¦æ¥äº†æ›´é«˜çš„åŠ é€Ÿã€‚åœ¨è‰ç¨¿éªŒè¯ä¹‹åï¼Œå¸¸è§„è§£ç å¯ä»¥ä»å‘æ•£ç‚¹å¼€å§‹ï¼Œå¹¶ç»§ç»­ç›´åˆ°æ»¡è¶³ç»“æŸæ¡ä»¶ã€‚ä¸ç°æœ‰çš„æ¨æµ‹è§£ç ç­–ç•¥ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¶ˆé™¤äº†è‰ç¨¿è®¡ç®—çš„éœ€è¦ï¼Œä½¿å…¶æˆä¸ºä¸€ç§æ¨¡å‹æ— å…³çš„ã€å³æ’å³ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºåŠ é€Ÿå¯¹å»¶è¿Ÿæ•æ„Ÿçš„æµå¼åº”ç”¨ã€‚åœ¨åŒæ­¥æ–‡æœ¬åˆ°æ–‡æœ¬é‡è¯‘ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„è‡ªå›å½’é‡è¯‘ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸å½±å“è´¨é‡çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾1.7å€çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»“åˆä»…æ˜¾ç¤ºæ©ç -kæŠ€æœ¯ï¼Œå®ƒæ˜¾è‘—å‡å°‘äº†80%çš„é—ªçƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç›´æ’­ç¿»è¯‘ç­‰æµå¼åº”ç”¨ä¸­ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé‡è¯‘æ—¶è®¡ç®—æˆæœ¬é«˜ã€å»¶è¿Ÿå¤§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å¯¹æ¯ä¸ªå¢é•¿çš„è¾“å…¥æµä»å¤´å¼€å§‹ç”Ÿæˆè¾“å‡ºï¼Œå¯¼è‡´é‡å¤è®¡ç®—å’Œè¾ƒé«˜çš„å»¶è¿Ÿï¼Œæ— æ³•æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å…ˆå‰ç”Ÿæˆçš„è¾“å‡ºä½œä¸ºå½“å‰è¾“å…¥çš„â€œè‰ç¨¿â€ï¼Œé¿å…ä»å¤´å¼€å§‹ç”Ÿæˆã€‚é€šè¿‡åœ¨è§£ç è¿‡ç¨‹ä¸­å¯¹è‰ç¨¿tokenè¿›è¡Œåç½®ï¼Œæé«˜è‰ç¨¿çš„æ¥å—ç‡ï¼Œä»è€Œå‡å°‘éœ€è¦é‡æ–°è®¡ç®—çš„éƒ¨åˆ†ï¼Œé™ä½æ•´ä½“å»¶è¿Ÿã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºäººç±»çš„æ ¡å¯¹è¿‡ç¨‹ï¼Œå³å…ˆå¿«é€Ÿç”Ÿæˆä¸€ä¸ªè‰ç¨¿ï¼Œç„¶åè¿›è¡Œä¿®æ”¹å’Œæ¶¦è‰²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šè‰ç¨¿ç”Ÿæˆå’Œè‰ç¨¿éªŒè¯ã€‚è‰ç¨¿ç”Ÿæˆé˜¶æ®µç›´æ¥ä½¿ç”¨ä¸Šä¸€æ¬¡çš„è¾“å‡ºä½œä¸ºè‰ç¨¿ã€‚è‰ç¨¿éªŒè¯é˜¶æ®µï¼Œæ¨¡å‹åœ¨è§£ç æ—¶ä¼šåå‘äºè‰ç¨¿ä¸­çš„tokenã€‚å¦‚æœè‰ç¨¿tokenè¢«æ¥å—ï¼Œåˆ™ç»§ç»­ä½¿ç”¨è‰ç¨¿ï¼›å¦‚æœè‰ç¨¿tokenè¢«æ‹’ç»ï¼Œåˆ™ä»å½“å‰ä½ç½®å¼€å§‹è¿›è¡Œå¸¸è§„çš„è‡ªå›å½’è§£ç ï¼Œç›´åˆ°æ»¡è¶³ç»“æŸæ¡ä»¶ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é‡‡ç”¨äº†â€œä»…æ˜¾ç¤ºæ©ç -kâ€æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥å‡å°‘è¾“å‡ºçš„é—ªçƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæ— éœ€é¢å¤–çš„è‰ç¨¿æ¨¡å‹ï¼Œè€Œæ˜¯ç›´æ¥åˆ©ç”¨è‡ªèº«çš„å…ˆå‰è¾“å‡ºæ¥ç”Ÿæˆè‰ç¨¿ï¼Œè¿™ä½¿å¾—è¯¥æ–¹æ³•å…·æœ‰æ¨¡å‹æ— å…³æ€§å’Œå³æ’å³ç”¨çš„ç‰¹æ€§ï¼Œå¯ä»¥æ–¹ä¾¿åœ°åº”ç”¨äºå„ç§ç°æœ‰çš„è¯­è¨€æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„æ¨æµ‹è§£ç æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é¿å…äº†è‰ç¨¿æ¨¡å‹çš„è®­ç»ƒå’Œç»´æŠ¤æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åç½®è§£ç ç­–ç•¥ï¼Œé€šè¿‡è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒï¼Œä½¿æ¨¡å‹æ›´å€¾å‘äºé€‰æ‹©è‰ç¨¿ä¸­çš„tokenï¼›2) â€œä»…æ˜¾ç¤ºæ©ç -kâ€æŠ€æœ¯ï¼Œé€šè¿‡é™åˆ¶æ¯æ¬¡æ˜¾ç¤ºçš„tokenæ•°é‡ï¼Œå‡å°‘è¾“å‡ºçš„é—ªçƒï¼›3) åŠ¨æ€è°ƒæ•´åç½®å¼ºåº¦ï¼Œä»¥å¹³è¡¡ç¿»è¯‘è´¨é‡å’ŒåŠ é€Ÿæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒå£°æ–‡æœ¬åˆ°æ–‡æœ¬é‡è¯‘ä»»åŠ¡ä¸Šï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„è‡ªå›å½’é‡è¯‘ï¼Œå®ç°äº†é«˜è¾¾1.7å€çš„åŠ é€Ÿï¼ŒåŒæ—¶æ²¡æœ‰æ˜¾è‘—é™ä½ç¿»è¯‘è´¨é‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥â€œä»…æ˜¾ç¤ºæ©ç -kâ€æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•è¿˜æ˜¾è‘—å‡å°‘äº†80%çš„è¾“å‡ºé—ªçƒï¼Œè¿›ä¸€æ­¥æå‡äº†ç”¨æˆ·ä½“éªŒã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ é€Ÿä½å»¶è¿Ÿæµå¼æ–‡æœ¬ç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦ä½å»¶è¿Ÿçš„æµå¼æ–‡æœ¬ç”Ÿæˆåœºæ™¯ï¼Œä¾‹å¦‚ç›´æ’­ç¿»è¯‘ã€å®æ—¶å­—å¹•ç”Ÿæˆã€è¯­éŸ³åŠ©æ‰‹ç­‰ã€‚é€šè¿‡é™ä½å»¶è¿Ÿå’Œå‡å°‘é—ªçƒï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œä½¿å¾—è¿™äº›åº”ç”¨æ›´åŠ å®ç”¨å’Œé«˜æ•ˆã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„åºåˆ—ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚ä»£ç ç”Ÿæˆã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have recently demonstrated impressive capabilities in various text generation tasks. However, it remains challenging to use them off-the-shelf in streaming applications (such as live translation), where the output must continually update as the input context expands, while still maintaining a reasonable computational cost to meet the latency requirement.
>   In this work, we reexamine the re-translation approach to simultaneous translation and propose Self-Speculative Biased Decoding, a novel inference paradigm designed to avoid repeatedly generating output from scratch for a consistently growing input stream. We propose using the most recent output as a draft for the current growing input context. During the verification stage, the output will be biased towards the draft token for a higher draft acceptance rate. This strategy not only minimizes flickering that might distract users but also leads to higher speedups. Conventional decoding may take charge from the point of divergence after draft verification and continue until the end condition is met.
>   Unlike existing speculative decoding strategies, our approach eliminates the need for draft computations, making it a model-agnostic and plug-and-play solution for accelerating latency-sensitive streaming applications. Experimental results on simultaneous text-to-text re-translation demonstrate that our approach achieves up to 1.7x speedup compared to conventional auto-regressive re-translation without compromising quality. Additionally, it significantly reduces flickering by 80% by incorporating the display-only mask-k technique.

