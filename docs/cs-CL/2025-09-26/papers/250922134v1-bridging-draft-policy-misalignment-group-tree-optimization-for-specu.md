---
layout: default
title: Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding
---

# Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22134" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22134v1</a>
  <a href="https://arxiv.org/pdf/2509.22134.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22134v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22134v1', 'Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shijing Hu, Jingyang Li, Zhihui Lu, Pan Zhou

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGroup Tree Optimizationï¼Œè§£å†³æ¨æµ‹è§£ç ä¸­è‰ç¨¿ç­–ç•¥ä¸å¯¹é½é—®é¢˜ï¼Œæå‡LLMæ¨ç†é€Ÿåº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨æµ‹è§£ç ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹åŠ é€Ÿ` `ç­–ç•¥å¯¹é½` `æ ‘æœç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨æµ‹è§£ç æ–¹æ³•è®­ç»ƒç›®æ ‡ä¸å®é™…è§£ç ç­–ç•¥ä¸ä¸€è‡´ï¼Œå¯¼è‡´è‰ç¨¿æ¨¡å‹æ€§èƒ½å—é™ï¼Œæ— æ³•å……åˆ†åŠ é€ŸLLMæ¨ç†ã€‚
2. GTOé€šè¿‡Draft Tree Rewardç›´æ¥ä¼˜åŒ–è§£ç æ€§èƒ½ï¼Œå¹¶ä½¿ç”¨Group-based Draft Policy Trainingç¨³å®šè®­ç»ƒè‰ç¨¿æ¨¡å‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGTOåœ¨å¤šä¸ªLLMå’Œä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ¥å—é•¿åº¦å’Œæ¨ç†é€Ÿåº¦ï¼Œä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨æµ‹è§£ç é€šè¿‡è½»é‡çº§è‰ç¨¿æ¨¡å‹å¹¶è¡Œç”Ÿæˆå¤šä¸ªtokenï¼Œä¾›ç›®æ ‡æ¨¡å‹éªŒè¯ï¼Œä»è€ŒåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è®­ç»ƒç›®æ ‡ä»…ä¼˜åŒ–å•ä¸€è´ªå©ªè‰ç¨¿è·¯å¾„ï¼Œè€Œè§£ç è¿‡ç¨‹éµå¾ªæ ‘ç­–ç•¥ï¼Œå¯¹å¤šä¸ªåˆ†æ”¯è¿›è¡Œé‡æ’åºå’ŒéªŒè¯ã€‚è¿™ç§è‰ç¨¿ç­–ç•¥çš„ä¸å¯¹é½é™åˆ¶äº†å¯å®ç°çš„åŠ é€Ÿã€‚æˆ‘ä»¬å¼•å…¥äº†Group Tree Optimization (GTO)ï¼Œé€šè¿‡ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ä½¿è®­ç»ƒä¸è§£ç æ—¶çš„æ ‘ç­–ç•¥å¯¹é½ï¼šï¼ˆiï¼‰Draft Tree Rewardï¼Œä¸€ç§æ— é‡‡æ ·çš„ç›®æ ‡ï¼Œç­‰äºç›®æ ‡æ¨¡å‹ä¸‹è‰ç¨¿æ ‘çš„é¢„æœŸæ¥å—é•¿åº¦ï¼Œç›´æ¥è¡¡é‡è§£ç æ€§èƒ½ï¼›ï¼ˆiiï¼‰Group-based Draft Policy Trainingï¼Œä¸€ç§ç¨³å®šçš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œå¯¹æ¯”æ¥è‡ªå½“å‰å’Œå†»ç»“çš„å‚è€ƒè‰ç¨¿æ¨¡å‹çš„æ ‘ï¼Œå½¢æˆå»åçš„ç»„æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼Œå¹¶æ²¿ç€æœ€é•¿æ¥å—åºåˆ—åº”ç”¨PPOé£æ ¼çš„æ›¿ä»£ç›®æ ‡ï¼Œä»¥å®ç°ç¨³å¥çš„æ›´æ–°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œå¢åŠ æˆ‘ä»¬çš„Draft Tree Rewardå¯ä»¥æ˜¾è‘—æé«˜æ¥å—é•¿åº¦å’ŒåŠ é€Ÿã€‚åœ¨å¯¹è¯ï¼ˆMT-Benchï¼‰ã€ä»£ç ï¼ˆHumanEvalï¼‰å’Œæ•°å­¦ï¼ˆGSM8Kï¼‰ä»¥åŠå¤šä¸ªLLMï¼ˆä¾‹å¦‚ï¼ŒLLaMA-3.1-8Bã€LLaMA-3.3-70Bã€Vicuna-1.3-13Bã€DeepSeek-R1-Distill-LLaMA-8Bï¼‰ä¸Šï¼ŒGTOå°†æ¥å—é•¿åº¦æé«˜äº†7.4%ï¼Œå¹¶æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›EAGLE-3å®ç°äº†é¢å¤–çš„7.7%çš„åŠ é€Ÿã€‚é€šè¿‡å¼¥åˆè‰ç¨¿ç­–ç•¥çš„ä¸å¯¹é½ï¼ŒGTOä¸ºé«˜æ•ˆçš„LLMæ¨ç†æä¾›äº†ä¸€ç§å®ç”¨ã€é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ¨æµ‹è§£ç æ–¹æ³•åœ¨è®­ç»ƒè‰ç¨¿æ¨¡å‹æ—¶ï¼Œé€šå¸¸åªå…³æ³¨å•ä¸€çš„è´ªå©ªè§£ç è·¯å¾„ï¼Œè€Œå®é™…è§£ç è¿‡ç¨‹ä¸­ä¼šæ¢ç´¢å¤šä¸ªåˆ†æ”¯ï¼Œå½¢æˆä¸€æ£µæ ‘ã€‚è¿™ç§è®­ç»ƒç›®æ ‡ä¸è§£ç ç­–ç•¥çš„ä¸ä¸€è‡´ï¼ˆdraft policy misalignmentï¼‰å¯¼è‡´è‰ç¨¿æ¨¡å‹æ— æ³•å……åˆ†åˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„åé¦ˆï¼Œé™åˆ¶äº†æ¨æµ‹è§£ç çš„åŠ é€Ÿæ•ˆæœã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè§£å†³è¿™ç§ä¸å¯¹é½é—®é¢˜ï¼Œå¯¼è‡´è‰ç¨¿æ¨¡å‹é¢„æµ‹çš„tokenè¢«ç›®æ ‡æ¨¡å‹æ¥å—çš„æ¯”ä¾‹è¾ƒä½ï¼Œé™ä½äº†æ•´ä½“æ¨ç†é€Ÿåº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGTOçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è‰ç¨¿æ¨¡å‹çš„è®­ç»ƒç›®æ ‡ä¸å®é™…è§£ç æ—¶çš„æ ‘ç­–ç•¥å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼ŒGTOç›´æ¥ä¼˜åŒ–è‰ç¨¿æ ‘çš„é¢„æœŸæ¥å—é•¿åº¦ï¼Œå³ç›®æ ‡æ¨¡å‹æ¥å—çš„tokenæ•°é‡çš„æœŸæœ›å€¼ã€‚é€šè¿‡æœ€å¤§åŒ–è¿™ä¸ªæœŸæœ›å€¼ï¼ŒGTOé¼“åŠ±è‰ç¨¿æ¨¡å‹ç”Ÿæˆæ›´æœ‰å¯èƒ½è¢«ç›®æ ‡æ¨¡å‹æ¥å—çš„tokenåºåˆ—ï¼Œä»è€Œæé«˜æ¨æµ‹è§£ç çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒGTOè¿˜å¼•å…¥äº†ä¸€ç§ç¨³å®šçš„è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥é¿å…è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§å’Œå´©æºƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGTOä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šDraft Tree Rewardå’ŒGroup-based Draft Policy Trainingã€‚Draft Tree Rewardæ˜¯ä¸€ä¸ªæ— é‡‡æ ·çš„ç›®æ ‡å‡½æ•°ï¼Œç”¨äºè¡¡é‡è‰ç¨¿æ ‘çš„è´¨é‡ã€‚Group-based Draft Policy Trainingæ˜¯ä¸€ç§åŸºäºPPOçš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œç”¨äºæ›´æ–°è‰ç¨¿æ¨¡å‹çš„å‚æ•°ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å¯¹æ¯”å½“å‰è‰ç¨¿æ¨¡å‹å’Œå†»ç»“çš„å‚è€ƒè‰ç¨¿æ¨¡å‹ç”Ÿæˆçš„æ ‘ï¼Œè®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼Œå¹¶ä½¿ç”¨è¯¥ä¼˜åŠ¿å‡½æ•°æ¥æ›´æ–°è‰ç¨¿æ¨¡å‹ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œä½¿ç”¨è‰ç¨¿æ¨¡å‹ç”Ÿæˆä¸€æ£µè‰ç¨¿æ ‘ï¼›ç„¶åï¼Œä½¿ç”¨ç›®æ ‡æ¨¡å‹éªŒè¯è‰ç¨¿æ ‘ä¸­çš„tokenï¼›æ¥ç€ï¼Œè®¡ç®—Draft Tree Rewardï¼›æœ€åï¼Œä½¿ç”¨Group-based Draft Policy Trainingæ›´æ–°è‰ç¨¿æ¨¡å‹çš„å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šGTOæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶ç›´æ¥ä¼˜åŒ–è‰ç¨¿æ ‘çš„é¢„æœŸæ¥å—é•¿åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒGTOä¸ä¾èµ–äºå•ä¸€çš„è´ªå©ªè§£ç è·¯å¾„ï¼Œè€Œæ˜¯è€ƒè™‘äº†æ•´ä¸ªè‰ç¨¿æ ‘çš„ç»“æ„ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ è‰ç¨¿æ¨¡å‹çš„å®é™…è§£ç æ€§èƒ½ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„è®­ç»ƒã€‚æ­¤å¤–ï¼ŒGroup-based Draft Policy Trainingé€šè¿‡å¼•å…¥å‚è€ƒæ¨¡å‹å’Œç»„æ ‡å‡†åŒ–ï¼Œæé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œé¿å…äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å´©æºƒã€‚

**å…³é”®è®¾è®¡**ï¼šDraft Tree Rewardçš„è®¾è®¡å…³é”®åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°è®¡ç®—è‰ç¨¿æ ‘çš„é¢„æœŸæ¥å—é•¿åº¦ã€‚GTOé‡‡ç”¨äº†ä¸€ç§æ— é‡‡æ ·çš„è®¡ç®—æ–¹æ³•ï¼Œé¿å…äº†é‡‡æ ·å¸¦æ¥çš„æ–¹å·®ã€‚Group-based Draft Policy Trainingçš„å…³é”®åœ¨äºå¦‚ä½•é€‰æ‹©åˆé€‚çš„å‚è€ƒæ¨¡å‹å’Œå¦‚ä½•è®¡ç®—ä¼˜åŠ¿å‡½æ•°ã€‚GTOé€‰æ‹©å†»ç»“çš„è‰ç¨¿æ¨¡å‹ä½œä¸ºå‚è€ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç»„æ ‡å‡†åŒ–æ¥å‡å°‘æ–¹å·®ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨PPOé£æ ¼çš„æ›¿ä»£ç›®æ ‡å‡½æ•°ï¼Œä»¥ä¿è¯è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGTOåœ¨å¤šä¸ªLLMï¼ˆå¦‚LLaMA-3.1-8Bã€LLaMA-3.3-70Bã€Vicuna-1.3-13Bã€DeepSeek-R1-Distill-LLaMA-8Bï¼‰å’Œä»»åŠ¡ï¼ˆå¦‚MT-Benchã€HumanEvalã€GSM8Kï¼‰ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚GTOå°†æ¥å—é•¿åº¦æé«˜äº†7.4%ï¼Œå¹¶æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›EAGLE-3å®ç°äº†é¢å¤–çš„7.7%çš„åŠ é€Ÿã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒGTOæ˜¯ä¸€ç§æœ‰æ•ˆçš„æ¨æµ‹è§£ç åŠ é€Ÿæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GTOå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦åŠ é€ŸLLMæ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€ä»£ç ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡æé«˜LLMçš„æ¨ç†é€Ÿåº¦ï¼ŒGTOå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¿ƒè¿›LLMåœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚æœªæ¥ï¼ŒGTOå¯ä»¥ä¸å…¶ä»–åŠ é€ŸæŠ€æœ¯ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜LLMçš„æ¨ç†æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Speculative decoding accelerates large language model (LLM) inference by letting a lightweight draft model propose multiple tokens that the target model verifies in parallel. Yet existing training objectives optimize only a single greedy draft path, while decoding follows a tree policy that re-ranks and verifies multiple branches. This draft policy misalignment limits achievable speedups. We introduce Group Tree Optimization (GTO), which aligns training with the decoding-time tree policy through two components: (i) Draft Tree Reward, a sampling-free objective equal to the expected acceptance length of the draft tree under the target model, directly measuring decoding performance; (ii) Group-based Draft Policy Training, a stable optimization scheme that contrasts trees from the current and a frozen reference draft model, forming debiased group-standardized advantages and applying a PPO-style surrogate along the longest accepted sequence for robust updates. We further prove that increasing our Draft Tree Reward provably improves acceptance length and speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B, DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By bridging draft policy misalignment, GTO offers a practical, general solution for efficient LLM inference.

