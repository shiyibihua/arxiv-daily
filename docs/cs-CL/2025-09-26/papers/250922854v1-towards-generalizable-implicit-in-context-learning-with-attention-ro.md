---
layout: default
title: Towards Generalizable Implicit In-Context Learning with Attention Routing
---

# Towards Generalizable Implicit In-Context Learning with Attention Routing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22854" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22854v1</a>
  <a href="https://arxiv.org/pdf/2509.22854.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22854v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22854v1', 'Towards Generalizable Implicit In-Context Learning with Attention Routing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaqian Li, Yanshu Li, Ligong Han, Ruixiang Tang, Wenya Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIn-Context Routing (ICR)ï¼Œé€šè¿‡æ³¨æ„åŠ›è·¯ç”±å®ç°é€šç”¨éšå¼ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éšå¼ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶` `è·¯ç”±ç½‘ç»œ` `æ³›åŒ–èƒ½åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éšå¼ICLæ–¹æ³•ä¾èµ–äºä»»åŠ¡ç‰¹å®šçš„æ®‹å·®æµï¼Œç¼ºä¹å¯¹ICLåº•å±‚ç»“æ„æœºåˆ¶çš„åˆ©ç”¨ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. æå‡ºIn-Context Routing (ICR)ï¼Œé€šè¿‡å­¦ä¹ å¯é‡ç”¨çš„ç»“æ„æ–¹å‘å¹¶è°ƒèŠ‚æ³¨æ„åŠ›logitsï¼Œå®ç°é€šç”¨éšå¼ICLã€‚
3. åœ¨å¤šä¸ªæ•°æ®é›†å’ŒLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒICRä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨é¢†åŸŸå¤–ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšå¼ä¸Šä¸‹æ–‡å­¦ä¹ (ICL)ä½œä¸ºä¸€ç§æ–°å…´èŒƒå¼ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå¤§å‹è¯­è¨€æ¨¡å‹(LLM)è¡¨å¾ç©ºé—´ä¸­çš„ICLè¡Œä¸ºï¼Œä»¥é›¶æ ·æœ¬æˆæœ¬è·å¾—å°‘æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå°†ç§»ä½å‘é‡æ³¨å…¥æ®‹å·®æµï¼Œè€Œè¿™äº›æ®‹å·®æµé€šå¸¸ç”±å¸¦æ ‡ç­¾çš„æ¼”ç¤ºæˆ–ç‰¹å®šäºä»»åŠ¡çš„å¯¹é½æ„å»ºã€‚è¿™ç§è®¾è®¡æœªèƒ½å……åˆ†åˆ©ç”¨ICLçš„åº•å±‚ç»“æ„æœºåˆ¶ï¼Œå¹¶ä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºIn-Context Routing (ICR)ï¼Œä¸€ç§æ–°é¢–çš„éšå¼ICLæ–¹æ³•ï¼Œå®ƒåœ¨æ³¨æ„åŠ›logitsçº§åˆ«å†…åŒ–å¯æ³›åŒ–çš„ICLæ¨¡å¼ã€‚å®ƒæå–ICLæœŸé—´å‡ºç°çš„å¯é‡ç”¨ç»“æ„æ–¹å‘ï¼Œå¹¶é‡‡ç”¨å¯å­¦ä¹ çš„è¾“å…¥æ¡ä»¶è·¯ç”±å™¨æ¥ç›¸åº”åœ°è°ƒèŠ‚æ³¨æ„åŠ›logitsï¼Œä»è€Œå®ç°ä¸€æ¬¡è®­ç»ƒã€å¤šæ¬¡å¤ç”¨çš„æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨æ¶µç›–ä¸åŒé¢†åŸŸçš„12ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†å’Œå¤šä¸ªLLMä¸Šè¯„ä¼°ICRã€‚ç»“æœè¡¨æ˜ï¼ŒICRå§‹ç»ˆä¼˜äºéœ€è¦ç‰¹å®šäºä»»åŠ¡çš„æ£€ç´¢æˆ–è®­ç»ƒçš„ç°æœ‰éšå¼ICLæ–¹æ³•ï¼ŒåŒæ—¶å¯¹ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†çš„é¢†åŸŸå¤–ä»»åŠ¡è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä½¿ICRèƒ½å¤Ÿæ¨åŠ¨ICLçš„å®é™…ä»·å€¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰éšå¼ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ä¾èµ–äºå°†ç§»ä½å‘é‡æ³¨å…¥æ®‹å·®æµï¼Œè¿™äº›ç§»ä½å‘é‡é€šå¸¸éœ€è¦å¸¦æ ‡ç­¾çš„æ¼”ç¤ºæ•°æ®æˆ–é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¯¹é½ã€‚è¿™ç§åšæ³•çš„ç—›ç‚¹åœ¨äºï¼Œå®ƒæœªèƒ½å……åˆ†åˆ©ç”¨ICLå†…åœ¨çš„ç»“æ„æ€§æœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥é€‚åº”æ–°çš„ã€æœªè§è¿‡çš„ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šICRçš„æ ¸å¿ƒæ€è·¯åœ¨äºï¼Œå®ƒä¸å†ä¾èµ–äºä»»åŠ¡ç‰¹å®šçš„æ®‹å·®æµï¼Œè€Œæ˜¯è¯•å›¾å­¦ä¹ ä¸€ç§é€šç”¨çš„ã€å¯é‡ç”¨çš„ICLæ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæå–åœ¨ICLè¿‡ç¨‹ä¸­å‡ºç°çš„ç»“æ„æ–¹å‘ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ–¹å‘æ¥è°ƒèŠ‚æ³¨æ„åŠ›logitsã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒICRèƒ½å¤Ÿå†…åŒ–ICLçš„åº•å±‚ç»“æ„ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡èƒŒåçš„é€»è¾‘æ˜¯ï¼ŒICLçš„åº•å±‚ç»“æ„åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´å¯èƒ½å­˜åœ¨å…±æ€§ï¼Œé€šè¿‡å­¦ä¹ è¿™äº›å…±æ€§ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°é€‚åº”æ–°çš„ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šICRçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šç»“æ„æ–¹å‘æå–æ¨¡å—å’Œæ³¨æ„åŠ›è·¯ç”±æ¨¡å—ã€‚é¦–å…ˆï¼Œç»“æ„æ–¹å‘æå–æ¨¡å—è´Ÿè´£ä»ICLè¿‡ç¨‹ä¸­æå–å¯é‡ç”¨çš„ç»“æ„æ–¹å‘ã€‚è¿™äº›æ–¹å‘ä»£è¡¨äº†ICLè¿‡ç¨‹ä¸­é‡è¦çš„ä¿¡æ¯æµåŠ¨æ¨¡å¼ã€‚ç„¶åï¼Œæ³¨æ„åŠ›è·¯ç”±æ¨¡å—åˆ©ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„è¾“å…¥æ¡ä»¶è·¯ç”±å™¨ï¼Œæ ¹æ®è¾“å…¥æ¥è°ƒèŠ‚æ³¨æ„åŠ›logitsã€‚è¿™ä¸ªè·¯ç”±å™¨å†³å®šäº†å¦‚ä½•å°†æå–çš„ç»“æ„æ–¹å‘åº”ç”¨åˆ°å½“å‰çš„è¾“å…¥ä¸Šï¼Œä»è€Œå®ç°å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„åŠ¨æ€è°ƒæ•´ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ä¸€æ¬¡è®­ç»ƒã€å¤šæ¬¡å¤ç”¨çš„æ¨¡å¼ï¼Œå³æ¨¡å‹åªéœ€è¦åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå°±å¯ä»¥æ³›åŒ–åˆ°å…¶ä»–æ•°æ®é›†ä¸Šã€‚

**å…³é”®åˆ›æ–°**ï¼šICRæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå®ƒå°†ICLçš„åº•å±‚ç»“æ„å†…åŒ–åˆ°æ³¨æ„åŠ›logitsçº§åˆ«ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒICRä¸å†ä¾èµ–äºä»»åŠ¡ç‰¹å®šçš„æ®‹å·®æµï¼Œè€Œæ˜¯ç›´æ¥å­¦ä¹ ICLçš„ç»“æ„æ€§æ¨¡å¼ã€‚è¿™ç§åšæ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒICRè¯•å›¾å­¦ä¹ ä¸€ç§é€šç”¨çš„ICLæœºåˆ¶ï¼Œè€Œä¸æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒICRèƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æ–°çš„ä»»åŠ¡ä¸Šã€‚

**å…³é”®è®¾è®¡**ï¼šICRçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç»“æ„æ–¹å‘æå–æ¨¡å—çš„å…·ä½“å®ç°æ–¹å¼ï¼Œä¾‹å¦‚å¦‚ä½•é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°æ¥é¼“åŠ±æ¨¡å‹å­¦ä¹ å¯é‡ç”¨çš„ç»“æ„æ–¹å‘ï¼›2) æ³¨æ„åŠ›è·¯ç”±æ¨¡å—çš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚è·¯ç”±å™¨çš„å±‚æ•°ã€æ¿€æ´»å‡½æ•°ç­‰ï¼›3) å¦‚ä½•å°†æå–çš„ç»“æ„æ–¹å‘åº”ç”¨åˆ°æ³¨æ„åŠ›logitsä¸Šï¼Œä¾‹å¦‚é‡‡ç”¨åŠ æƒæ±‚å’Œçš„æ–¹å¼ï¼Œå…¶ä¸­æƒé‡ç”±è·¯ç”±å™¨å†³å®šã€‚æ­¤å¤–ï¼Œè®­ç»ƒæ•°æ®çš„é€‰æ‹©å’Œé¢„å¤„ç†ä¹Ÿæ˜¯å…³é”®çš„è®¾è®¡å› ç´ ï¼Œéœ€è¦ä»”ç»†è€ƒè™‘å¦‚ä½•é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ICRåœ¨12ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†ä¸åŒçš„é¢†åŸŸå’Œå¤šä¸ªLLMã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒICRå§‹ç»ˆä¼˜äºéœ€è¦ä»»åŠ¡ç‰¹å®šæ£€ç´¢æˆ–è®­ç»ƒçš„ç°æœ‰éšå¼ICLæ–¹æ³•ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒICRåœ¨ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†çš„é¢†åŸŸå¤–ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šçš„æ€§èƒ½å’Œå®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ICRå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§éœ€è¦å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šæ™ºèƒ½å®¢æœã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚å®ƒèƒ½å¤Ÿé™ä½æ¨¡å‹å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ï¼Œä»è€Œé™ä½å¼€å‘æˆæœ¬ï¼ŒåŠ é€Ÿæ¨¡å‹éƒ¨ç½²ã€‚æœªæ¥ï¼ŒICRæœ‰æœ›æˆä¸ºä¸€ç§é€šç”¨çš„ICLè§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Implicit in-context learning (ICL) has newly emerged as a promising paradigm that simulates ICL behaviors in the representation space of Large Language Models (LLMs), aiming to attain few-shot performance at zero-shot cost. However, existing approaches largely rely on injecting shift vectors into residual flows, which are typically constructed from labeled demonstrations or task-specific alignment. Such designs fall short of utilizing the structural mechanisms underlying ICL and suffer from limited generalizability. To address this, we propose In-Context Routing (ICR), a novel implicit ICL method that internalizes generalizable ICL patterns at the attention logits level. It extracts reusable structural directions that emerge during ICL and employs a learnable input-conditioned router to modulate attention logits accordingly, enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world datasets spanning diverse domains and multiple LLMs. The results show that ICR consistently outperforms prior implicit ICL methods that require task-specific retrieval or training, while demonstrating robust generalization to out-of-domain tasks where existing methods struggle. These findings position ICR to push the boundary of ICL's practical value.

