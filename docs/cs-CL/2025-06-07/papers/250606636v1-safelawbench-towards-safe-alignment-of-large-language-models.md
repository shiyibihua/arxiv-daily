---
layout: default
title: SafeLawBench: Towards Safe Alignment of Large Language Models
---

# SafeLawBench: Towards Safe Alignment of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06636" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06636v1</a>
  <a href="https://arxiv.org/pdf/2506.06636.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06636v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06636v1', 'SafeLawBench: Towards Safe Alignment of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chuxue Cao, Han Zhu, Jiaming Ji, Qichao Sun, Zhenghao Zhu, Yinyu Wu, Juntao Dai, Yaodong Yang, Sirui Han, Yike Guo

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-07

**å¤‡æ³¨**: Accepted to ACL2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSafeLawBenchä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å®‰å…¨è¯„ä¼°` `æ³•å¾‹è§†è§’` `å¤šé€‰é¢˜` `é—®ç­”ä»»åŠ¡` `æ¨¡å‹æ€§èƒ½` `å®‰å…¨æ€§ç ”ç©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å®‰å…¨è¯„ä¼°æ–¹æ³•ç¼ºä¹å®¢è§‚æ ‡å‡†ï¼Œå¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§è¯„ä¼°é¢ä¸´æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºSafeLawBenchåŸºå‡†ï¼Œä»æ³•å¾‹è§’åº¦ç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMsçš„å®‰å…¨æ€§ï¼Œåˆ†ç±»å®‰å…¨é£é™©ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰ä¸»æµæ¨¡å‹åœ¨å¤šé€‰ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡æœªè¶…è¿‡80.5%ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸º68.8%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ™®åŠï¼Œå…¶å®‰å…¨æ€§å¼•å‘äº†é‡å¤§å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºç°æœ‰å®‰å…¨åŸºå‡†çš„ä¸»è§‚æ€§ï¼Œç¼ºä¹æ˜ç¡®çš„è¯„ä¼°æ ‡å‡†ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–æ¬¡ä»æ³•å¾‹è§†è§’æ¢ç´¢LLMsçš„å®‰å…¨è¯„ä¼°ï¼Œæå‡ºäº†SafeLawBenchåŸºå‡†ã€‚è¯¥åŸºå‡†æ ¹æ®æ³•å¾‹æ ‡å‡†å°†å®‰å…¨é£é™©åˆ†ä¸ºä¸‰ä¸ªçº§åˆ«ï¼Œæä¾›äº†ç³»ç»Ÿå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«24,860ä¸ªå¤šé€‰é¢˜å’Œ1,106ä¸ªå¼€æ”¾é¢†åŸŸé—®ç­”ä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¶µç›–äº†2ä¸ªé—­æºLLMså’Œ18ä¸ªå¼€æºLLMsï¼Œçªå‡ºäº†æ¯ä¸ªæ¨¡å‹çš„å®‰å…¨ç‰¹æ€§ï¼Œå¹¶è¯„ä¼°äº†å…¶å®‰å…¨ç›¸å…³æ¨ç†çš„ç¨³å®šæ€§å’Œæ‹’ç»è¡Œä¸ºã€‚æˆ‘ä»¬å‘ç°ï¼Œå¤šæ•°æŠ•ç¥¨æœºåˆ¶å¯ä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æ˜¯é¢†å…ˆçš„SOTAæ¨¡å‹å¦‚Claude-3.5-Sonnetå’ŒGPT-4oåœ¨SafeLawBenchçš„å¤šé€‰ä»»åŠ¡ä¸­å‡†ç¡®ç‡ä¹Ÿæœªè¶…è¿‡80.5%ï¼Œè€Œ20ä¸ªLLMsçš„å¹³å‡å‡†ç¡®ç‡ä¸º68.8%ã€‚æˆ‘ä»¬å‘¼åç¤¾åŒºä¼˜å…ˆå…³æ³¨LLMsçš„å®‰å…¨ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨è¯„ä¼°ç¼ºä¹å®¢è§‚æ ‡å‡†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºè¯„ä¼°æ ‡å‡†çš„ä¸»è§‚æ€§ï¼Œæ— æ³•æœ‰æ•ˆåæ˜ æ¨¡å‹çš„å®‰å…¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºSafeLawBenchåŸºå‡†ï¼Œé€šè¿‡æ³•å¾‹è§†è§’å¯¹LLMsçš„å®‰å…¨æ€§è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œåˆ†ç±»å®‰å…¨é£é™©ä¸ºä¸‰ä¸ªçº§åˆ«ï¼Œæä¾›å…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSafeLawBenchåŒ…å«24,860ä¸ªå¤šé€‰é¢˜å’Œ1,106ä¸ªå¼€æ”¾é¢†åŸŸé—®ç­”ä»»åŠ¡ï¼Œè¯„ä¼°åŒ…æ‹¬é—­æºå’Œå¼€æºæ¨¡å‹ï¼Œé‡‡ç”¨é›¶-shotå’Œfew-shotæç¤ºæ–¹æ³•ï¼Œåˆ†ææ¨¡å‹çš„å®‰å…¨ç‰¹æ€§å’Œæ¨ç†ç¨³å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºä»æ³•å¾‹è§’åº¦å‡ºå‘ï¼Œå»ºç«‹äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„å®‰å…¨è¯„ä¼°æ¡†æ¶ï¼Œå¡«è¡¥äº†ç°æœ‰æ–¹æ³•çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šæ•°æŠ•ç¥¨æœºåˆ¶æ¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œè¯„ä¼°äº†æ¨¡å‹åœ¨å®‰å…¨ç›¸å…³æ¨ç†çš„ç¨³å®šæ€§å’Œæ‹’ç»è¡Œä¸ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä½¿ç”¨äº†å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚Claude-3.5-Sonnetå’ŒGPT-4oï¼Œä½†åœ¨SafeLawBenchçš„å¤šé€‰ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡æœ€é«˜ä»…ä¸º80.5%ã€‚è€Œ20ä¸ªLLMsçš„å¹³å‡å‡†ç¡®ç‡ä¸º68.8%ï¼Œæ˜¾ç¤ºå‡ºå½“å‰æ¨¡å‹åœ¨å®‰å…¨æ€§è¯„ä¼°æ–¹é¢çš„ä¸è¶³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ³•å¾‹åˆè§„ã€è‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿå’Œå®‰å…¨æ€§è¯„ä¼°å·¥å…·ã€‚é€šè¿‡æä¾›ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼ŒSafeLawBenchå¯ä»¥å¸®åŠ©å¼€å‘è€…å’Œç ”ç©¶è€…æ›´å¥½åœ°ç†è§£å’Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œä¿ƒè¿›å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨ä½¿ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the growing prevalence of large language models (LLMs), the safety of LLMs has raised significant concerns. However, there is still a lack of definitive standards for evaluating their safety due to the subjective nature of current safety benchmarks. To address this gap, we conducted the first exploration of LLMs' safety evaluation from a legal perspective by proposing the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three levels based on legal standards, providing a systematic and comprehensive framework for evaluation. It comprises 24,860 multi-choice questions and 1,106 open-domain question-answering (QA) tasks. Our evaluation included 2 closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot prompting, highlighting the safety features of each model. We also evaluated the LLMs' safety-related reasoning stability and refusal behavior. Additionally, we found that a majority voting mechanism can enhance model performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench, while the average accuracy of 20 LLMs remains at 68.8\%. We urge the community to prioritize research on the safety of LLMs.

