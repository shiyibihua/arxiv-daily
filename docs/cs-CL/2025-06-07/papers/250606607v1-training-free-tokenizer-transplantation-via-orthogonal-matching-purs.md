---
layout: default
title: Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit
---

# Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06607" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06607v1</a>
  <a href="https://arxiv.org/pdf/2506.06607.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06607v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06607v1', 'Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Charles Goddard, Fernando Fernandes Neto

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-07

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— è®­ç»ƒçš„æ ‡è®°å™¨ç§»æ¤æ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„æ ‡è®°å™¨ä¸åŒ¹é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ ‡è®°å™¨ç§»æ¤` `æ­£äº¤åŒ¹é…è¿½è¸ª` `å¤§è¯­è¨€æ¨¡å‹` `é›¶-shotå­¦ä¹ ` `çŸ¥è¯†è’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒæ ‡è®°å™¨ä¹‹é—´çš„åµŒå…¥ä¸åŒ¹é…æ—¶ï¼Œå¾€å¾€éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ­£äº¤åŒ¹é…è¿½è¸ªï¼ˆOMPï¼‰æ–¹æ³•ï¼Œå®ç°æ— è®­ç»ƒçš„æ ‡è®°å™¨ç§»æ¤ï¼Œåˆ©ç”¨ç¨€ç–çº¿æ€§ç»„åˆé‡æ„æ ‡è®°åµŒå…¥ã€‚
3. åœ¨Llamaåˆ°Mistral NeMoå’ŒQwenåˆ°Llamaçš„è·¨æ ‡è®°å™¨ä»»åŠ¡ä¸­ï¼ŒOMPæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæœ€ä½³çš„é›¶-shotæ€§èƒ½ä¿æŒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡æ­£äº¤åŒ¹é…è¿½è¸ªï¼ˆOMPï¼‰åœ¨é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ç§»æ¤æ ‡è®°å™¨ï¼Œé‡æ„æœªè§çš„æ ‡è®°åµŒå…¥ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªè¶…å‡ºè¯æ±‡è¡¨çš„æ ‡è®°è¿‘ä¼¼ä¸ºå…±äº«æ ‡è®°çš„ç¨€ç–çº¿æ€§ç»„åˆï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨ä¸€å°ç»„å…±äº«é”šå®šæ ‡è®°è®¡ç®—æ–°æ ‡è®°åœ¨æèµ è€…åµŒå…¥ç©ºé—´ä¸­çš„è¡¨ç¤ºï¼Œç„¶åå°†è¿™äº›ç¨€ç–ç³»æ•°è½¬ç§»å›åŸºç¡€æ¨¡å‹çš„åµŒå…¥ç©ºé—´ã€‚åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è·¨æ ‡è®°å™¨ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬å±•ç¤ºäº†OMPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³çš„é›¶-shotæ€§èƒ½ä¿æŒï¼Œè€Œå…¶ä»–é›¶-shotæ–¹æ³•æ˜¾è‘—ä¸‹é™ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒOMPå§‹ç»ˆå®ç°æœ€ä½³æ•´ä½“æ€§èƒ½ï¼Œæœ‰æ•ˆå¼¥åˆäº†å¤§å‹æ ‡è®°å™¨ä¹‹é—´çš„å·®å¼‚ï¼Œè€Œæ— éœ€æ¢¯åº¦æ›´æ–°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ä¸­ä¸åŒæ ‡è®°å™¨ä¹‹é—´çš„åµŒå…¥ä¸åŒ¹é…é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨è·¨æ ‡è®°å™¨ä»»åŠ¡ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ­£äº¤åŒ¹é…è¿½è¸ªï¼ˆOMPï¼‰æ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒå³å¯å®ç°æ ‡è®°å™¨çš„ç§»æ¤ã€‚å…·ä½“è€Œè¨€ï¼Œåˆ©ç”¨å…±äº«æ ‡è®°çš„ç¨€ç–çº¿æ€§ç»„åˆæ¥è¿‘ä¼¼æœªè§çš„æ ‡è®°åµŒå…¥ï¼Œä»è€Œä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨ä¸€å°ç»„å…±äº«é”šå®šæ ‡è®°è®¡ç®—æ–°æ ‡è®°åœ¨æèµ è€…åµŒå…¥ç©ºé—´ä¸­çš„è¡¨ç¤ºï¼›ç¬¬äºŒé˜¶æ®µï¼Œå°†è¿™äº›ç¨€ç–ç³»æ•°è½¬ç§»å›åŸºç¡€æ¨¡å‹çš„åµŒå…¥ç©ºé—´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†æ— è®­ç»ƒçš„æ ‡è®°å™¨ç§»æ¤æ–¹æ³•ï¼Œåˆ©ç”¨OMPæœ‰æ•ˆåœ°å¼¥åˆäº†å¤§å‹æ ‡è®°å™¨ä¹‹é—´çš„å·®å¼‚ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„éœ€è¦è®­ç»ƒçš„æ ‡è®°å™¨ç§»æ¤æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬é€‰æ‹©åˆé€‚çš„å…±äº«é”šå®šæ ‡è®°å­—å…¸ï¼Œä»¥åŠåœ¨ç¨€ç–çº¿æ€§ç»„åˆä¸­å¦‚ä½•æœ‰æ•ˆè®¡ç®—å’Œè½¬ç§»ç³»æ•°ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æ¨¡å‹åœ¨ä¸è¿›è¡Œæ¢¯åº¦æ›´æ–°çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½å¤Ÿä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Llamaåˆ°Mistral NeMoï¼ˆ12Bï¼‰å’ŒQwenåˆ°Llamaï¼ˆ1Bï¼‰çš„è·¨æ ‡è®°å™¨ä»»åŠ¡ä¸­ï¼ŒOMPæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³çš„é›¶-shotæ€§èƒ½ä¿æŒï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼ˆå¦‚é›¶åˆå§‹åŒ–ã€å‡å€¼åˆå§‹åŒ–åŠç°æœ‰æ–¹æ³•WECHSELã€FOCUSã€ZETTï¼‰ï¼ŒOMPè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è·¨æ ‡è®°å™¨çŸ¥è¯†è’¸é¦ã€æ¨æµ‹è§£ç ã€é›†æˆã€åˆå¹¶ä»¥åŠç‰¹å®šé¢†åŸŸè¯æ±‡çš„é€‚é…ã€‚é€šè¿‡ç›´æ¥é‡ç”¨é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œç ”ç©¶æˆæœèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹åœ¨ä¸åŒæ ‡è®°å™¨ä¸‹çš„é€‚åº”æ€§å’Œæ€§èƒ½ï¼Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a training-free method to transplant tokenizers in pretrained large language models (LLMs) by reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). Specifically, we approximate each out-of-vocabulary token as a sparse linear combination of shared tokens, in two phases: first, compute each new token's representation in the donor embedding space with a small dictionary of shared anchor tokens, then transfer these same sparse coefficients back into the base model's embedding space.
>   On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of the base model's performance across multiple benchmarks, while other zero-shot approaches degrade significantly. Compared to baselines (zero-init, mean-init, and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves the best overall performance, effectively bridging large tokenizer discrepancies without gradient updates. Our analysis further identifies mismatched numerical tokenization schemes as a critical challenge for preserving mathematical reasoning capabilities. This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. We integrate our method into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment.

