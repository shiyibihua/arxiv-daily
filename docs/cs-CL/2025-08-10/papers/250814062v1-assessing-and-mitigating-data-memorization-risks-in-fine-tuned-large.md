---
layout: default
title: Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models
---

# Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14062" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14062v1</a>
  <a href="https://arxiv.org/pdf/2508.14062.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14062v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14062v1', 'Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Badrinath Ramakrishnan, Akshaya Balaji

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-10

**å¤‡æ³¨**: 14 pages, 2 figures. Code and experimental framework available at https://github.com/akshayaaa10/llm-privacy-research

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šå±‚éšç§ä¿æŠ¤æ¡†æ¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æ•°æ®è®°å¿†é£é™©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éšç§ä¿æŠ¤` `å¤§è¯­è¨€æ¨¡å‹` `æ•°æ®è®°å¿†` `å·®åˆ†éšç§` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å¾®è°ƒ` `æ•°æ®å»é‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å®¹æ˜“è®°å¿†è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´ä¸¥é‡çš„éšç§æ³„éœ²é£é™©ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºå¤šå±‚éšç§ä¿æŠ¤æ¡†æ¶ï¼Œç»“åˆè¯­ä¹‰å»é‡ã€å·®åˆ†éšç§ç­‰æŠ€æœ¯ï¼Œé™ä½æ•°æ®æ³„éœ²é£é™©ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨æ–°æ–¹æ³•åï¼Œéšç§æ³„éœ²ç‡å¯é™è‡³0%ï¼Œä¸”æ¨¡å‹æ•ˆç”¨ä¿æŒåœ¨94.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯¹è®­ç»ƒæ•°æ®çš„è®°å¿†å€¾å‘å¸¦æ¥äº†æ˜¾è‘—çš„éšç§é£é™©ã€‚æœ¬æ–‡å¯¹å¾®è°ƒåçš„LLMsä¸­çš„æ•°æ®è®°å¿†è¿›è¡Œäº†å…¨é¢çš„å®è¯åˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šå±‚éšç§ä¿æŠ¤æ¡†æ¶ã€‚é€šè¿‡å¯¹ç°ä»£LLMæ¶æ„ï¼ˆåŒ…æ‹¬GPT-2ã€Phi-3å’ŒGemma-2ï¼‰çš„æ§åˆ¶å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨é‡å¤æ•æ„Ÿæ•°æ®è¿›è¡Œå¾®è°ƒä¼šä½¿éšç§æ³„éœ²ç‡ä»åŸºçº¿çš„0-5%å¢åŠ åˆ°60-75%ï¼Œå¹³å‡å¢åŠ 64.2%ã€‚æˆ‘ä»¬æå‡ºå¹¶ä¸¥æ ¼è¯„ä¼°äº†å››ç§äº’è¡¥çš„éšç§ä¿æŠ¤æ–¹æ³•ï¼šè¯­ä¹‰æ•°æ®å»é‡ã€ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å·®åˆ†éšç§ã€åŸºäºç†µçš„è¿‡æ»¤å’ŒåŸºäºæ¨¡å¼çš„å†…å®¹è¿‡æ»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥å°†æ•°æ®æ³„éœ²é™ä½åˆ°0%ï¼ŒåŒæ—¶ä¿æŒ94.7%çš„åŸå§‹æ¨¡å‹æ•ˆç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯¹è®­ç»ƒæ•°æ®çš„è®°å¿†é—®é¢˜ï¼Œè¿™ç§è®°å¿†ä¼šå¯¼è‡´éšç§æ³„éœ²ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§å¤šå±‚éšç§ä¿æŠ¤æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤šç§æŠ€æœ¯æ‰‹æ®µï¼Œæ—¨åœ¨é™ä½æ•°æ®æ³„éœ²é£é™©å¹¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šè¯­ä¹‰æ•°æ®å»é‡ã€ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å·®åˆ†éšç§ã€åŸºäºç†µçš„è¿‡æ»¤å’ŒåŸºäºæ¨¡å¼çš„å†…å®¹è¿‡æ»¤ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œä»¥å®ç°éšç§ä¿æŠ¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å››ç§äº’è¡¥çš„éšç§ä¿æŠ¤æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥å·®åˆ†éšç§å’ŒåŸºäºæ¨¡å¼çš„å†…å®¹è¿‡æ»¤ï¼Œè¿™äº›æ–¹æ³•åœ¨æœ‰æ•ˆæ€§ä¸Šä¸ç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿éšç§ä¿æŠ¤ä¸æ¨¡å‹æ•ˆç”¨ä¹‹é—´çš„å¹³è¡¡ï¼Œç¡®ä¿åœ¨é™ä½æ•°æ®æ³„éœ²çš„åŒæ—¶ï¼Œæ¨¡å‹æ€§èƒ½ä¸å—æ˜¾è‘—å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ–°æå‡ºçš„éšç§ä¿æŠ¤æ–¹æ³•åï¼Œæ•°æ®æ³„éœ²ç‡ä»åŸºçº¿çš„0-5%æå‡è‡³60-75%ï¼Œè€Œåœ¨åº”ç”¨æ–°æŠ€æœ¯åï¼Œæ•°æ®æ³„éœ²ç‡é™è‡³0%ï¼ŒåŒæ—¶ä¿æŒ94.7%çš„æ¨¡å‹æ•ˆç”¨ï¼Œå±•ç°äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œç¤¾äº¤åª’ä½“ç­‰å¯¹éšç§è¦æ±‚æé«˜çš„åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆé™ä½æ•°æ®æ³„éœ²é£é™©ï¼Œèƒ½å¤Ÿå¢å¼ºç”¨æˆ·å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¿¡ä»»ï¼Œä¿ƒè¿›å…¶åœ¨æ•æ„Ÿé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³æ”¿ç­–å’ŒæŠ€æœ¯æ ‡å‡†çš„åˆ¶å®šã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, but their tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes. This paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. Through controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. We propose and rigorously evaluate four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. Our experimental results show that these techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.

