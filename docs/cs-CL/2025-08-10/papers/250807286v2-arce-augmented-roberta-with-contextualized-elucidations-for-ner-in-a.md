---
layout: default
title: Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking
---

# Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07286" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07286v2</a>
  <a href="https://arxiv.org/pdf/2508.07286.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07286v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07286v2', 'Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jian Chen, Jinbao Tian, Yankui Li, Yuqi Lu, Zhou Li

**åˆ†ç±»**: cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-08-10 (æ›´æ–°: 2025-09-10)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/nxcc-lab/ARCE)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºARCEä»¥è§£å†³AECé¢†åŸŸå‘½åå®ä½“è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‘½åå®ä½“è¯†åˆ«` `è‡ªåŠ¨åŒ–è§„åˆ™æ£€æŸ¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `RoBERTa` `ä¸Šä¸‹æ–‡åŒ–è§£é‡Š` `å»ºç­‘å·¥ç¨‹` `ä¿¡æ¯æå–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‘½åå®ä½“è¯†åˆ«æ–¹æ³•åœ¨AECé¢†åŸŸé¢ä¸´é¢†åŸŸå·®è·ï¼Œéš¾ä»¥å¤„ç†ä¸“ä¸šæœ¯è¯­å’Œå¤æ‚å…³ç³»ã€‚
2. ARCEé€šè¿‡ç”Ÿæˆç®€å•çš„ä¸Šä¸‹æ–‡åŒ–è§£é‡Šæ¥å¢å¼ºRoBERTaæ¨¡å‹çš„é¢„è®­ç»ƒï¼Œä¼˜åŒ–çŸ¥è¯†ç”Ÿæˆè¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒARCEåœ¨AECæ•°æ®é›†ä¸Šè¾¾åˆ°äº†77.20%çš„Macro-F1åˆ†æ•°ï¼Œåˆ›ä¸‹æ–°çºªå½•ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»ä¸“ä¸šæ–‡æœ¬ä¸­å‡†ç¡®æå–ä¿¡æ¯æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å»ºç­‘ã€å·¥ç¨‹å’Œæ–½å·¥ï¼ˆAECï¼‰é¢†åŸŸçš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸­ï¼Œä»¥æ”¯æŒè‡ªåŠ¨åŒ–è§„åˆ™æ£€æŸ¥ï¼ˆARCï¼‰ã€‚æ ‡å‡†é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½å¸¸å¸¸å—åˆ°é¢†åŸŸå·®è·çš„é™åˆ¶ï¼Œéš¾ä»¥ç†è§£AECæ–‡æœ¬ä¸­çš„ä¸“ä¸šæœ¯è¯­å’Œå¤æ‚å…³ç³»ã€‚è™½ç„¶é€šè¿‡åœ¨å¤§å‹äººç±»ç­–åˆ’çš„é¢†åŸŸè¯­æ–™åº“ä¸Šè¿›ä¸€æ­¥é¢„è®­ç»ƒå¯ä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†è¿™ç§æ–¹æ³•æ—¢è´¹åŠ›åˆæˆæœ¬é«˜ã€‚å› æ­¤ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè‡ªåŠ¨çŸ¥è¯†ç”Ÿæˆæˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ARCEï¼ˆå¢å¼ºå‹RoBERTaä¸ä¸Šä¸‹æ–‡åŒ–é˜é‡Šï¼‰ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢å’Œä¼˜åŒ–è¿™ä¸€ç”Ÿæˆè¿‡ç¨‹ã€‚ARCEé¦–å…ˆåˆ©ç”¨LLMç”Ÿæˆç®€å•ç›´æ¥çš„è§£é‡Šè¯­æ–™ï¼Œç„¶åé€æ­¥å¯¹RoBERTaæ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæœ€ç»ˆåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARCEåœ¨åŸºå‡†AECæ•°æ®é›†ä¸Šè¾¾åˆ°äº†77.20%çš„Macro-F1åˆ†æ•°ï¼Œæ˜¾ç¤ºå‡ºç®€å•çš„åŸºäºè§£é‡Šçš„çŸ¥è¯†åœ¨æ­¤ä»»åŠ¡ä¸­æ¯”å¤æ‚çš„åŸºäºè§’è‰²çš„æ¨ç†æ›´æœ‰æ•ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³AECé¢†åŸŸå‘½åå®ä½“è¯†åˆ«ä¸­çš„ä¿¡æ¯æå–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å› é¢†åŸŸå·®è·è€Œè¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥ç†è§£ä¸“ä¸šæœ¯è¯­å’Œå¤æ‚å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šARCEçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç®€å•çš„ä¸Šä¸‹æ–‡åŒ–è§£é‡Šï¼Œä»¥æ­¤å¢å¼ºRoBERTaæ¨¡å‹çš„é¢„è®­ç»ƒæ•ˆæœï¼Œä»è€Œæé«˜NERçš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šARCEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆä½¿ç”¨LLMç”Ÿæˆè§£é‡Šè¯­æ–™ï¼ˆCoteï¼‰ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šå¯¹RoBERTaè¿›è¡Œå¢é‡é¢„è®­ç»ƒï¼Œæœ€ååœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šARCEçš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡ç”Ÿæˆç®€å•çš„è§£é‡ŠçŸ¥è¯†æ¥æ›¿ä»£å¤æ‚çš„è§’è‰²æ¨ç†ï¼Œè¿™ä¸€æ–¹æ³•åœ¨AECé¢†åŸŸçš„NERä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒARCEé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„è§£é‡Šèƒ½å¤Ÿæœ‰æ•ˆåœ°æ”¯æŒRoBERTaçš„é¢„è®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ARCEåœ¨åŸºå‡†AECæ•°æ®é›†ä¸Šå–å¾—äº†77.20%çš„Macro-F1åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†ç®€å•è§£é‡ŠçŸ¥è¯†çš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºå¤æ‚æ¨ç†æ–¹æ³•æå‡æ˜æ˜¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å»ºç­‘ã€å·¥ç¨‹å’Œæ–½å·¥è¡Œä¸šçš„è‡ªåŠ¨åŒ–è§„åˆ™æ£€æŸ¥ï¼Œèƒ½å¤Ÿæé«˜ä¿¡æ¯æå–çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨å…¶ä»–ä¸“ä¸šé¢†åŸŸçš„å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­å±•ç°å‡ºå¹¿æ³›çš„åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate information extraction from specialized texts is a critical challenge, particularly for named entity recognition (NER) in the architecture, engineering, and construction (AEC) domain to support automated rule checking (ARC). The performance of standard pre-trained models is often constrained by the domain gap, as they struggle to interpret the specialized terminology and complex relational contexts inherent in AEC texts. Although this issue can be mitigated by further pre-training on large, human-curated domain corpora, as exemplified by methods like ARCBERT, this approach is both labor-intensive and cost-prohibitive. Consequently, leveraging large language models (LLMs) for automated knowledge generation has emerged as a promising alternative. However, the optimal strategy for generating knowledge that can genuinely enhance smaller, efficient models remains an open question. To address this, we propose ARCE (augmented RoBERTa with contextualized elucidations), a novel approach that systematically explores and optimizes this generation process. ARCE employs an LLM to first generate a corpus of simple, direct explanations, which we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa model prior to its fine-tuning on the downstream task. Our extensive experiments show that ARCE establishes a new state-of-the-art on a benchmark AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a key finding: simple, explanation-based knowledge proves surprisingly more effective than complex, role-based rationales for this task. The code is publicly available at:https://github.com/nxcc-lab/ARCE.

