---
layout: default
title: CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation
---

# CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07295" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07295v2</a>
  <a href="https://arxiv.org/pdf/2508.07295.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07295v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07295v2', 'CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yexing Du, Kaiyuan Liu, Youcheng Pan, Zheng Chu, Bo Yang, Xiaocheng Feng, Ming Liu, Yang Xiang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-10 (æ›´æ–°: 2025-12-01)

**å¤‡æ³¨**: Accepted in AAAI 2026

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yxduir/ccfqa)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCCFQAåŸºå‡†ä»¥è§£å†³å¤šè¯­è¨€å¤šæ¨¡æ€äº‹å®æ€§è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è·¨è¯­è¨€è¯„ä¼°` `äº‹å®æ€§è¯„ä¼°` `è¯­éŸ³ç†è§£` `è¿ç§»å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `é—®ç­”ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸»è¦é›†ä¸­äºè‹±è¯­ï¼Œç¼ºä¹å¯¹å¤šè¯­è¨€å’Œè¯­éŸ³è¾“å…¥çš„æœ‰æ•ˆè¯„ä¼°ã€‚
2. æå‡ºCCFQAåŸºå‡†ï¼ŒåŒ…å«8ç§è¯­è¨€çš„å¹³è¡Œè¯­éŸ³-æ–‡æœ¬äº‹å®æ€§é—®é¢˜ï¼Œç³»ç»Ÿè¯„ä¼°MLLMsçš„è·¨è¯­è¨€å’Œè·¨æ¨¡æ€èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå½“å‰MLLMsåœ¨CCFQAåŸºå‡†ä¸Šè¡¨ç°ä¸ä½³ï¼ŒåŒæ—¶æå‡ºçš„å°‘é‡è¿ç§»å­¦ä¹ ç­–ç•¥æ˜¾è‘—æå‡äº†å¤šè¯­è¨€é—®ç­”æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„æ™®åŠï¼Œç¡®ä¿æ— å¹»è§‰çš„äº‹å®æ€§å˜å¾—å°¤ä¸ºé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¯„ä¼°åŸºå‡†ä¸»è¦é›†ä¸­äºæ–‡æœ¬æˆ–è§†è§‰æ¨¡æ€ï¼Œå¹¶ä¸”ä¸»è¦ä»¥è‹±è¯­ä¸ºä¸»ï¼Œè¿™åœ¨å¤„ç†å¤šè¯­è¨€è¾“å…¥æ—¶å°¤å…¶å­˜åœ¨è¯„ä¼°ç©ºç™½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è·¨è¯­è¨€å’Œè·¨æ¨¡æ€äº‹å®æ€§åŸºå‡†ï¼ˆCCFQAï¼‰ï¼Œè¯¥åŸºå‡†åŒ…å«8ç§è¯­è¨€çš„å¹³è¡Œè¯­éŸ³-æ–‡æœ¬äº‹å®æ€§é—®é¢˜ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°MLLMsçš„è·¨è¯­è¨€å’Œè·¨æ¨¡æ€äº‹å®æ€§èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„MLLMsåœ¨CCFQAåŸºå‡†ä¸Šä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°‘é‡è¿ç§»å­¦ä¹ ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å°†LLMsåœ¨è‹±è¯­ä¸­çš„é—®ç­”èƒ½åŠ›è½¬ç§»åˆ°å¤šè¯­è¨€å£è¯­é—®ç­”ä»»åŠ¡ä¸­ï¼Œä»…ä½¿ç”¨5æ¬¡è®­ç»ƒä¾¿å®ç°äº†ä¸GPT-4o-mini-Audioçš„ç«äº‰æ€§è¡¨ç°ã€‚æˆ‘ä»¬å°†CCFQAä½œä¸ºåŸºç¡€ç ”ç©¶èµ„æºå‘å¸ƒï¼Œä»¥ä¿ƒè¿›MLLMsåœ¨è¯­éŸ³ç†è§£èƒ½åŠ›ä¸Šçš„å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€å’Œè¯­éŸ³è¾“å…¥ä¸‹çš„äº‹å®æ€§è¯„ä¼°ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºè‹±è¯­æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ï¼Œå¯¼è‡´åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­è¯„ä¼°æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºCCFQAåŸºå‡†ï¼Œé€šè¿‡è®¾è®¡å¹³è¡Œçš„è¯­éŸ³-æ–‡æœ¬äº‹å®æ€§é—®é¢˜ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°MLLMsåœ¨è·¨è¯­è¨€å’Œè·¨æ¨¡æ€çš„äº‹å®æ€§èƒ½åŠ›ï¼Œå¡«è¡¥ç°æœ‰è¯„ä¼°çš„ç©ºç™½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCCFQAåŸºå‡†åŒ…å«å¤šä¸ªæ¨¡å—ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€æ•°æ®é›†æ„å»ºã€äº‹å®æ€§é—®é¢˜è®¾è®¡å’Œè¯„ä¼°æŒ‡æ ‡åˆ¶å®šã€‚å®éªŒä¸­é‡‡ç”¨å°‘é‡è¿ç§»å­¦ä¹ ç­–ç•¥ï¼Œå°†è‹±è¯­é—®ç­”èƒ½åŠ›è¿ç§»è‡³å¤šè¯­è¨€å£è¯­é—®ç­”ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCCFQAåŸºå‡†æ˜¯é¦–ä¸ªä¸“æ³¨äºå¤šè¯­è¨€å’Œå¤šæ¨¡æ€çš„äº‹å®æ€§è¯„ä¼°å·¥å…·ï¼Œæ˜¾è‘—æå‡äº†å¯¹MLLMsçš„è¯„ä¼°èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è¯­éŸ³è¾“å…¥æ–¹é¢ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œé‡‡ç”¨äº†8ç§è¯­è¨€çš„å¹³è¡Œè¯­éŸ³-æ–‡æœ¬å¯¹ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ã€‚å°‘é‡è¿ç§»å­¦ä¹ ç­–ç•¥é€šè¿‡5-shotè®­ç»ƒå®ç°äº†ä¸GPT-4o-mini-Audioçš„ç«äº‰æ€§è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨CCFQAåŸºå‡†ä¸Šé¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤šè¯­è¨€å’Œè¯­éŸ³è¾“å…¥æ—¶ã€‚åŒæ—¶ï¼Œé‡‡ç”¨çš„å°‘é‡è¿ç§»å­¦ä¹ ç­–ç•¥åœ¨5-shotè®­ç»ƒä¸‹å®ç°äº†ä¸GPT-4o-mini-Audioç›¸å½“çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„è¿ç§»èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€è¯­éŸ³åŠ©æ‰‹ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œå¤šæ¨¡æ€äººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§è¯„ä¼°èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æœåŠ¡äºå…¨çƒç”¨æˆ·ï¼Œæ¨åŠ¨å¤šè¯­è¨€æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As Large Language Models (LLMs) are increasingly popularized in the multilingual world, ensuring hallucination-free factuality becomes markedly crucial. However, existing benchmarks for evaluating the reliability of Multimodal Large Language Models (MLLMs) predominantly focus on textual or visual modalities with a primary emphasis on English, which creates a gap in evaluation when processing multilingual input, especially in speech. To bridge this gap, we propose a novel Cross-lingual and Cross-modal Factuality benchmark (CCFQA). Specifically, the CCFQA benchmark contains parallel speech-text factual questions across 8 languages, designed to systematically evaluate MLLMs' cross-lingual and cross-modal factuality capabilities. Our experimental results demonstrate that current MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we propose a few-shot transfer learning strategy that effectively transfers the Question Answering (QA) capabilities of LLMs in English to multilingual Spoken Question Answering (SQA) tasks, achieving competitive performance with GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a foundational research resource to promote the development of MLLMs with more robust and reliable speech understanding capabilities. Our code and dataset are available at https://github.com/yxduir/ccfqa.

