---
layout: default
title: Positional Biases Shift as Inputs Approach Context Window Limits
---

# Positional Biases Shift as Inputs Approach Context Window Limits

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07479" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07479v1</a>
  <a href="https://arxiv.org/pdf/2508.07479.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07479v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07479v1', 'Positional Biases Shift as Inputs Approach Context Window Limits')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Blerta Veseli, Julian Chibane, Mariya Toneva, Alexander Koller

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-10

**æœŸåˆŠ**: Conference on Language Modeling (COLM) 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç›¸å¯¹è¾“å…¥é•¿åº¦åˆ†æä»¥è§£å†³é•¿è¾“å…¥ä¸­çš„ä½ç½®ä¿¡æ¯åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è¾“å…¥å¤„ç†` `ä½ç½®ä¿¡æ¯åå·®` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯æ£€ç´¢` `æ¨ç†æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è¾“å…¥æ—¶ï¼Œå¸¸å¸¸æ— æ³•æœ‰æ•ˆåˆ©ç”¨ä½ç½®ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡é€šè¿‡ç›¸å¯¹è¾“å…¥é•¿åº¦åˆ†æï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è§†è§’æ¥ç†è§£å’Œè¯„ä¼°æ¨¡å‹çš„ä½ç½®ä¿¡æ¯åå·®ã€‚
3. ç ”ç©¶å‘ç°ï¼Œè¾“å…¥é•¿åº¦å ä¸Šä¸‹æ–‡çª—å£çš„50%ä»¥å†…æ—¶ï¼ŒLiMæ•ˆåº”æœ€å¼ºï¼Œä¸”æ¨¡å‹æ€§èƒ½ä¸ä¿¡æ¯è·ç¦»å¯†åˆ‡ç›¸å…³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœ‰æ•ˆåˆ©ç”¨é•¿è¾“å…¥ä¿¡æ¯æ–¹é¢å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ä»¥å¾€ç ”ç©¶å‘ç°äº†ä½ç½®ä¿¡æ¯åå·®ç°è±¡ï¼Œå¦‚â€œä¸­é—´è¿·å¤±æ•ˆåº”â€ï¼ˆLiMï¼‰ï¼Œå³æ¨¡å‹åœ¨è¾“å…¥çš„å¼€å¤´ï¼ˆé¦–å› åå·®ï¼‰æˆ–ç»“å°¾ï¼ˆè¿‘å› åå·®ï¼‰è¡¨ç°æ›´å¥½ï¼Œè€Œåœ¨ä¸­é—´è¡¨ç°è¾ƒå·®ã€‚ç„¶è€Œï¼Œé•¿ä¸Šä¸‹æ–‡ç ”ç©¶æœªèƒ½ä¸€è‡´å¤ç°è¿™äº›æ•ˆåº”ï¼Œå¯¼è‡´å¯¹å…¶å¼ºåº¦åŠè¡¨ç°æ¡ä»¶çš„è´¨ç–‘ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é€šè¿‡ç›¸å¯¹è¾“å…¥é•¿åº¦è¿›è¡Œå…¨é¢åˆ†æï¼Œå‘ç°LiMæ•ˆåº”åœ¨è¾“å…¥å ç”¨æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£çš„50%ä»¥å†…æ—¶æœ€å¼ºï¼Œè¶…è¿‡æ­¤æ¯”ä¾‹åï¼Œé¦–å› åå·®å‡å¼±ï¼Œè€Œè¿‘å› åå·®ç›¸å¯¹ç¨³å®šã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€ç§åŸºäºè·ç¦»çš„åå·®ï¼Œå³ç›¸å…³ä¿¡æ¯è¶Šæ¥è¿‘è¾“å…¥æœ«å°¾ï¼Œæ¨¡å‹æ€§èƒ½è¶Šå¥½ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒæˆåŠŸçš„ä¿¡æ¯æ£€ç´¢æ˜¯LLMsæ¨ç†çš„å‰æï¼Œæ¨ç†ä¸­çš„ä½ç½®ä¿¡æ¯åå·®ä¸»è¦æºäºæ£€ç´¢ã€‚è¿™äº›å‘ç°å¯¹é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ã€æœªæ¥LLMåŸºå‡†è®¾è®¡åŠè¯„ä¼°æ–¹æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿è¾“å…¥ä¸­ä½ç½®ä¿¡æ¯åå·®çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ä¸­é—´è¿·å¤±æ•ˆåº”ï¼ˆLiMï¼‰æœªèƒ½åœ¨é•¿ä¸Šä¸‹æ–‡ç ”ç©¶ä¸­ä¸€è‡´å¤ç°çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç›¸å¯¹è¾“å…¥é•¿åº¦çš„åˆ†æï¼Œå®šä¹‰è¾“å…¥é•¿åº¦ç›¸å¯¹äºæ¨¡å‹ä¸Šä¸‹æ–‡çª—å£çš„æ¯”ä¾‹ï¼Œæ¢è®¨ä¸åŒä½ç½®çš„ä¿¡æ¯å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä»è€Œæ­ç¤ºä½ç½®ä¿¡æ¯åå·®çš„å˜åŒ–è§„å¾‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨ç›¸å¯¹è¾“å…¥é•¿åº¦è¿›è¡Œå®éªŒï¼Œåˆ†ææ¨¡å‹åœ¨ä¸åŒè¾“å…¥å æ¯”ä¸‹çš„è¡¨ç°ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†åŸºäºç›¸å¯¹è¾“å…¥é•¿åº¦çš„åˆ†ææ–¹æ³•ï¼Œæ­ç¤ºäº†LiMæ•ˆåº”çš„å¼ºåº¦ä¸è¾“å…¥é•¿åº¦å æ¯”çš„å…³ç³»ï¼Œå¼ºè°ƒäº†è·ç¦»åå·®çš„é‡è¦æ€§ï¼Œä¸ä¼ ç»Ÿç»å¯¹é•¿åº¦åˆ†ææ–¹æ³•å½¢æˆå¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„è¾“å…¥é•¿åº¦æ¯”ä¾‹ï¼Œä½¿ç”¨æ ‡å‡†çš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼Œç¡®ä¿äº†å®éªŒç»“æœçš„å¯é‡å¤æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“è¾“å…¥é•¿åº¦å æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£çš„50%ä»¥å†…æ—¶ï¼ŒLiMæ•ˆåº”æœ€ä¸ºæ˜¾è‘—ï¼Œè€Œè¶…è¿‡æ­¤æ¯”ä¾‹åï¼Œé¦–å› åå·®å‡å¼±ï¼Œè¿‘å› åå·®ä¿æŒç¨³å®šã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œæ¨¡å‹æ€§èƒ½ä¸ä¿¡æ¯è·ç¦»å¯†åˆ‡ç›¸å…³ï¼Œç›¸å…³ä¿¡æ¯è¶Šæ¥è¿‘è¾“å…¥æœ«å°¾ï¼Œæ¨¡å‹è¡¨ç°è¶Šå¥½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„å‘ç°å¯¹é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¿¡æ¯æ£€ç´¢å’Œå¯¹è¯ç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹å¯¹ä½ç½®ä¿¡æ¯çš„åˆ©ç”¨ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„è¡¨ç°ï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often struggle to use information across long inputs effectively. Prior work has identified positional biases, such as the Lost in the Middle (LiM) effect, where models perform better when information appears at the beginning (primacy bias) or end (recency bias) of the input, rather than in the middle. However, long-context studies have not consistently replicated these effects, raising questions about their intensity and the conditions under which they manifest. To address this, we conducted a comprehensive analysis using relative rather than absolute input lengths, defined with respect to each model's context window. Our findings reveal that the LiM effect is strongest when inputs occupy up to 50% of a model's context window. Beyond that, the primacy bias weakens, while recency bias remains relatively stable. This effectively eliminates the LiM effect; instead, we observe a distance-based bias, where model performance is better when relevant information is closer to the end of the input. Furthermore, our results suggest that successful retrieval is a prerequisite for reasoning in LLMs, and that the observed positional biases in reasoning are largely inherited from retrieval. These insights have implications for long-context tasks, the design of future LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

