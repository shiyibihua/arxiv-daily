---
layout: default
title: Gradient Surgery for Safe LLM Fine-Tuning
---

# Gradient Surgery for Safe LLM Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07172" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07172v1</a>
  <a href="https://arxiv.org/pdf/2508.07172.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07172v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07172v1', 'Gradient Surgery for Safe LLM Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Biao Yi, Jiahao Li, Baolei Zhang, Lihai Nie, Tong Li, Tiansheng Huang, Zheli Liu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSafeGradä»¥è§£å†³LLMå¾®è°ƒä¸­çš„å®‰å…¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®‰å…¨å¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¢¯åº¦æ‰‹æœ¯` `å¤šç›®æ ‡ä¼˜åŒ–` `å¯¹é½æŸå¤±` `é²æ£’æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å®‰å…¨å¾®è°ƒæ–¹æ³•åœ¨å¤„ç†æ¶æ„ç¤ºä¾‹æ¯”ä¾‹æ—¶è¡¨ç°å‡ºé«˜åº¦æ•æ„Ÿæ€§ï¼Œé˜²å¾¡æ•ˆæœéšç€æ¶æ„æ¯”ä¾‹çš„å¢åŠ è€Œæ€¥å‰§ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºSafeGradï¼Œé€šè¿‡æ¢¯åº¦æ‰‹æœ¯æŠ€æœ¯åœ¨æ£€æµ‹åˆ°å†²çªæ—¶æ¶ˆé™¤æœ‰å®³æ¢¯åº¦æˆåˆ†ï¼Œä»è€Œä¿æŒå®‰å…¨æ€§ä¸ä»»åŠ¡æ€§èƒ½çš„å¹³è¡¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSafeGradåœ¨å¤šç§LLMså’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨é«˜æ¶æ„æ¯”ä¾‹ä¸‹ä¿æŒå®‰å…¨æ€§ï¼Œä¸”ä¸å½±å“ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒå³æœåŠ¡å¼•å…¥äº†ä¸€ä¸ªå…³é”®æ¼æ´ï¼Œå³ç”¨æˆ·å¾®è°ƒæ•°æ®é›†ä¸­æ··å…¥å°‘é‡æ¶æ„ç¤ºä¾‹å¯èƒ½ä¼šç ´åå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨å¯¹é½ã€‚ç°æœ‰çš„å®‰å…¨å¾®è°ƒè§£å†³æ–¹æ¡ˆåœ¨å¤„ç†æ¶æ„ç¤ºä¾‹æ¯”ä¾‹æ—¶è¡¨ç°å‡ºé«˜åº¦æ•æ„Ÿæ€§ï¼Œé˜²å¾¡æ•ˆæœéšç€æ¶æ„æ¯”ä¾‹çš„å¢åŠ è€Œæ€¥å‰§ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SafeGradï¼Œä¸€ç§æ–°é¢–çš„æ¢¯åº¦æ‰‹æœ¯æ–¹æ³•ã€‚å½“æ£€æµ‹åˆ°å†²çªæ—¶ï¼ŒSafeGradé€šè¿‡å°†ç”¨æˆ·ä»»åŠ¡æ¢¯åº¦çš„æœ‰å®³æˆåˆ†æŠ•å½±åˆ°å¯¹é½æ¢¯åº¦çš„æ­£äº¤å¹³é¢ä¸Šæ¥æ¶ˆé™¤è¿™äº›æˆåˆ†ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²å®‰å…¨æ€§çš„æƒ…å†µä¸‹å­¦ä¹ ç”¨æˆ·ä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSafeGradåœ¨å„ç§LLMså’Œæ•°æ®é›†ä¸Šæä¾›äº†æœ€å…ˆè¿›çš„é˜²å¾¡æ•ˆæœï¼Œå³ä½¿åœ¨é«˜æ¶æ„æ¯”ä¾‹ä¸‹ä¹Ÿèƒ½ä¿æŒå®‰å…¨æ€§è€Œä¸å½±å“ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œæ¶æ„ç¤ºä¾‹æ··å…¥æ•°æ®é›†å¯¼è‡´çš„å®‰å…¨æ€§å¯¹é½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ¶æ„æ¯”ä¾‹å¢åŠ æ—¶ï¼Œé˜²å¾¡æ•ˆæœæ˜¾è‘—ä¸‹é™ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹è¿™ç§æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„SafeGradæ–¹æ³•é€šè¿‡æ¢¯åº¦æ‰‹æœ¯æŠ€æœ¯ï¼Œæ£€æµ‹åˆ°ç”¨æˆ·ä»»åŠ¡æ¢¯åº¦ä¸å®‰å…¨ç›®æ ‡ä¹‹é—´çš„å†²çªæ—¶ï¼Œæ¶ˆé™¤æœ‰å®³æˆåˆ†ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²å®‰å…¨æ€§çš„æƒ…å†µä¸‹å®Œæˆç”¨æˆ·ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSafeGradçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å†²çªæ£€æµ‹æ¨¡å—å’Œæ¢¯åº¦è°ƒæ•´æ¨¡å—ã€‚é¦–å…ˆï¼Œæ£€æµ‹ç”¨æˆ·ä»»åŠ¡æ¢¯åº¦ä¸å®‰å…¨å¯¹é½æ¢¯åº¦ä¹‹é—´çš„å†²çªï¼Œç„¶åé€šè¿‡æŠ•å½±æŠ€æœ¯è°ƒæ•´æ¢¯åº¦ï¼Œç¡®ä¿æ¨¡å‹å­¦ä¹ çš„å®‰å…¨æ€§ä¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šSafeGradçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¢¯åº¦æ‰‹æœ¯æ–¹æ³•ï¼Œé€šè¿‡å°†æœ‰å®³æ¢¯åº¦æˆåˆ†æŠ•å½±åˆ°å¯¹é½æ¢¯åº¦çš„æ­£äº¤å¹³é¢ä¸Šï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†å†²çªã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…åœ¨é«˜æ¶æ„æ¯”ä¾‹ä¸‹å®¹æ˜“å¤±æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šSafeGradé‡‡ç”¨KLæ•£åº¦å¯¹é½æŸå¤±å‡½æ•°ï¼Œä»¥å­¦ä¹ è‰¯å¥½å¯¹é½åŸºç¡€æ¨¡å‹çš„ä¸°å¯Œåˆ†å¸ƒå®‰å…¨ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨é«˜æ¶æ„æ¯”ä¾‹ä¸‹ä»èƒ½ä¿æŒä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSafeGradåœ¨å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é˜²å¾¡æ•ˆæœã€‚åœ¨é«˜è¾¾30%çš„æ¶æ„æ¯”ä¾‹ä¸‹ï¼Œä»èƒ½ä¿æŒè¶…è¿‡90%çš„ä»»åŠ¡å‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„é˜²å¾¡æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„é²æ£’æ€§å’Œæ•°æ®æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§ï¼ŒSafeGradèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢æ¶æ„æ”»å‡»ï¼Œä¿éšœç”¨æˆ·æ•°æ®çš„å®‰å…¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning-as-a-Service introduces a critical vulnerability where a few malicious examples mixed into the user's fine-tuning dataset can compromise the safety alignment of Large Language Models (LLMs). While a recognized paradigm frames safe fine-tuning as a multi-objective optimization problem balancing user task performance with safety alignment, we find existing solutions are critically sensitive to the harmful ratio, with defenses degrading sharply as harmful ratio increases. We diagnose that this failure stems from conflicting gradients, where the user-task update directly undermines the safety objective. To resolve this, we propose SafeGrad, a novel method that employs gradient surgery. When a conflict is detected, SafeGrad nullifies the harmful component of the user-task gradient by projecting it onto the orthogonal plane of the alignment gradient, allowing the model to learn the user's task without sacrificing safety. To further enhance robustness and data efficiency, we employ a KL-divergence alignment loss that learns the rich, distributional safety profile of the well-aligned foundation model. Extensive experiments show that SafeGrad provides state-of-the-art defense across various LLMs and datasets, maintaining robust safety even at high harmful ratios without compromising task fidelity.

