---
layout: default
title: ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models
---

# ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07484" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07484v1</a>
  <a href="https://arxiv.org/pdf/2508.07484.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07484v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07484v1', 'ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Archchana Sindhujan, Shenbin Qian, Chan Chi Chun Matthew, Constantin Orasan, Diptesh Kanojia

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-10

**å¤‡æ³¨**: Accepted to COLM 2025 Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºALOPEæ¡†æ¶ä»¥æå‡æœºå™¨ç¿»è¯‘è´¨é‡ä¼°è®¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨ç¿»è¯‘` `è´¨é‡ä¼°è®¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªé€‚åº”ä¼˜åŒ–` `è·¨è¯­è¨€å¯¹é½` `ä½èµ„æºè¯­è¨€` `å›å½’ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMåŸºç¡€è´¨é‡ä¼°è®¡ç³»ç»Ÿåœ¨ä½èµ„æºè¯­è¨€å’Œå›å½’ä»»åŠ¡ä¸Šå­˜åœ¨æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚
2. ALOPEæ¡†æ¶é€šè¿‡å±‚çº§é€‚åº”å’ŒåŠ¨æ€åŠ æƒç­‰ç­–ç•¥ï¼Œä¼˜åŒ–LLMçš„è¡¨ç¤ºä»¥æå‡è´¨é‡ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒè¯æ˜ALOPEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨è·¨è¯­è¨€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æœºå™¨ç¿»è¯‘è´¨é‡ä¼°è®¡ï¼ˆQEï¼‰ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€çš„æƒ…å†µä¸‹ã€‚ç°æœ‰çš„LLMåŸºç¡€QEç³»ç»Ÿä¸»è¦ä¸ºå› æœè¯­è¨€å»ºæ¨¡è€Œé¢„è®­ç»ƒï¼Œç¼ºä¹é’ˆå¯¹å›å½’ä»»åŠ¡çš„ä¼˜åŒ–ã€‚æœ¬æ–‡æå‡ºALOPEï¼Œä¸€ä¸ªè‡ªé€‚åº”å±‚ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡å±‚çº§é€‚åº”é‡æ„Transformerè¡¨ç¤ºï¼Œæ—¨åœ¨æå‡LLMåŸºç¡€QEçš„å›å½’é¢„æµ‹èƒ½åŠ›ã€‚ALOPEç»“åˆä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰ä¸å›å½’ä»»åŠ¡å¤´ï¼Œé‡‡ç”¨åŠ¨æ€åŠ æƒå’Œå¤šå¤´å›å½’ç­–ç•¥ï¼Œæ˜¾è‘—æ”¹å–„äº†è·¨è¯­è¨€å¯¹é½ã€‚å®éªŒè¯æ˜ï¼ŒALOPEåœ¨å¤šç§ç°æœ‰LLMåŸºç¡€QEæ–¹æ³•ä¸Šå‡æœ‰æå‡ï¼Œå¹¶å…¬å¼€äº†æ¨¡å‹å’Œæ¡†æ¶ä»£ç ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMåŸºç¡€æœºå™¨ç¿»è¯‘è´¨é‡ä¼°è®¡ï¼ˆQEï¼‰ç³»ç»Ÿåœ¨ä½èµ„æºè¯­è¨€å’Œå›å½’ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¸ºå› æœè¯­è¨€å»ºæ¨¡è€Œé¢„è®­ç»ƒï¼Œç¼ºä¹é’ˆå¯¹QEä»»åŠ¡çš„ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šALOPEæ¡†æ¶é€šè¿‡è‡ªé€‚åº”å±‚ä¼˜åŒ–ï¼Œé‡æ„Transformerè¡¨ç¤ºï¼Œç»“åˆä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰å’Œå›å½’ä»»åŠ¡å¤´ï¼Œæå‡LLMåœ¨QEä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚åŠ¨æ€åŠ æƒå’Œå¤šå¤´å›å½’ç­–ç•¥è¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„å›å½’èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šALOPEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å±‚çº§é€‚åº”æ¨¡å—ã€åŠ¨æ€åŠ æƒæ¨¡å—å’Œå¤šå¤´å›å½’æ¨¡å—ã€‚å±‚çº§é€‚åº”æ¨¡å—é€‰æ‹©é€‚åˆçš„é¢„è®­ç»ƒTransformerå±‚ï¼ŒåŠ¨æ€åŠ æƒæ¨¡å—æ ¹æ®ä»»åŠ¡éœ€æ±‚è‡ªé€‚åº”ç»„åˆå¤šå±‚è¡¨ç¤ºï¼Œå¤šå¤´å›å½’æ¨¡å—åˆ™èšåˆæ¥è‡ªå¤šä¸ªå¤´çš„å›å½’æŸå¤±ã€‚

**å…³é”®åˆ›æ–°**ï¼šALOPEçš„ä¸»è¦åˆ›æ–°åœ¨äºè‡ªé€‚åº”å±‚ä¼˜åŒ–å’ŒåŠ¨æ€åŠ æƒç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿçš„LLMåŸºç¡€QEæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…é€šå¸¸ä¸è€ƒè™‘å±‚çº§ä¿¡æ¯å’Œå¤šå¤´æŸå¤±çš„èšåˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒALOPEä½¿ç”¨ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰æ¥å‡å°‘å‚æ•°é‡ï¼ŒåŒæ—¶é€šè¿‡åŠ¨æ€åŠ æƒæœºåˆ¶è°ƒæ•´ä¸åŒå±‚çš„è´¡çŒ®ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒè¯­è¨€å¯¹ä¸Šçš„é€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒALOPEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMåŸºç¡€è´¨é‡ä¼°è®¡æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼Œå±•ç¤ºäº†å…¶åœ¨è·¨è¯­è¨€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ALOPEæ¡†æ¶åœ¨æœºå™¨ç¿»è¯‘è´¨é‡ä¼°è®¡é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæå‡å¤šè¯­è¨€ç¿»è¯‘ç³»ç»Ÿçš„è´¨é‡è¯„ä¼°èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­ã€‚è¯¥ç ”ç©¶çš„æˆæœå¯ä¸ºç¿»è¯‘æœåŠ¡æä¾›æ›´ç²¾å‡†çš„è´¨é‡åé¦ˆï¼Œæ¨åŠ¨è·¨è¯­è¨€äº¤æµçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown remarkable performance across a wide range of natural language processing tasks. Quality Estimation (QE) for Machine Translation (MT), which assesses the quality of a source-target pair without relying on reference translations, remains a challenging cross-lingual task for LLMs. The challenges stem from the inherent limitations of existing LLM-based QE systems, which are pre-trained for causal language modelling rather than regression-specific tasks, further elevated by the presence of low-resource languages given pre-training data distribution. This paper introduces ALOPE, an adaptive layer-optimization framework designed to enhance LLM-based QE by restructuring Transformer representations through layer-wise adaptation for improved regression-based prediction. Our framework integrates low-rank adapters (LoRA) with regression task heads, leveraging selected pre-trained Transformer layers for improved cross-lingual alignment. In addition to the layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting, which adaptively combines representations from multiple layers, and multi-head regression, which aggregates regression losses from multiple heads for QE. Our framework shows improvements over various existing LLM-based QE approaches. Empirical evidence suggests that intermediate Transformer layers in LLMs provide contextual representations that are more aligned with the cross-lingual nature of the QE task. We make resultant models and framework code publicly available for further research, also allowing existing LLM-based MT frameworks to be scaled with QE capabilities.

