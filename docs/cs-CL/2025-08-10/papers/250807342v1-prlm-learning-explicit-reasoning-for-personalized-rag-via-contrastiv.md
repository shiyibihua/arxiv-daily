---
layout: default
title: PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization
---

# PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07342" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07342v1</a>
  <a href="https://arxiv.org/pdf/2508.07342.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07342v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07342v1', 'PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kepu Zhang, Teng Shi, Weijie Yu, Jun Xu

**åˆ†ç±»**: cs.IR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPrLMæ¡†æ¶ä»¥è§£å†³ä¸ªæ€§åŒ–RAGä¸­çš„æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–ç”Ÿæˆ` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `ç”¨æˆ·åå¥½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä¸ªæ€§åŒ–RAGæ–¹æ³•è¿‡äºä¾èµ–æ£€ç´¢è´¨é‡ï¼Œå¯¼è‡´ç”Ÿæˆçš„å“åº”ä¸ç”¨æˆ·åå¥½ä¸ä¸€è‡´ã€‚
2. PrLMé€šè¿‡å¼ºåŒ–å­¦ä¹ æ˜¾å¼æ¨ç†ç”¨æˆ·èµ„æ–™ï¼Œåˆ©ç”¨å¯¹æ¯”è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œæå‡ç”Ÿæˆè´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPrLMåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”åœ¨ä¸åŒæ¡ä»¶ä¸‹è¡¨ç°ç¨³å®šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ªæ€§åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ—¨åœ¨é€šè¿‡ç»“åˆæ£€ç´¢åˆ°çš„ç”¨æˆ·èµ„æ–™ä¸è¾“å…¥æŸ¥è¯¢ï¼Œç”Ÿæˆç”¨æˆ·å®šåˆ¶çš„å“åº”ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æå‡æ£€ç´¢æ•ˆæœï¼Œå¹¶ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éšå¼æ•´åˆæ£€ç´¢ä¸Šä¸‹æ–‡ä¸æŸ¥è¯¢ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¾€å¾€å¯¹æ£€ç´¢è´¨é‡æ•æ„Ÿï¼Œå¯èƒ½ç”Ÿæˆä¸ç”¨æˆ·åå¥½ä¸ä¸€è‡´çš„å“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†PrLMï¼Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè®­ç»ƒLLMsæ˜¾å¼æ¨ç†æ£€ç´¢åˆ°çš„ç”¨æˆ·èµ„æ–™ã€‚åœ¨å¯¹æ¯”è®­ç»ƒçš„ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹çš„æŒ‡å¯¼ä¸‹ï¼ŒPrLMæœ‰æ•ˆåœ°ä»ç”¨æˆ·å“åº”ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€æ ‡æ³¨æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPrLMåœ¨ä¸‰ä¸ªä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ä¸åŒæ£€ç´¢å™¨å’Œæ£€ç´¢èµ„æ–™æ•°é‡ä¸‹ä¿æŒç¨³å¥æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸ªæ€§åŒ–RAGä¸­ç”Ÿæˆå“åº”ä¸ç”¨æˆ·åå¥½ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–æ£€ç´¢è´¨é‡ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœçš„ä¸ªæ€§åŒ–ç¨‹åº¦ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPrLMæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æ˜¾å¼æ¨ç†ç”¨æˆ·èµ„æ–™ï¼Œåˆ©ç”¨å¯¹æ¯”è®­ç»ƒçš„ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹æ¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œæå‡ç”Ÿæˆå“åº”çš„ä¸ªæ€§åŒ–å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPrLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç”¨æˆ·èµ„æ–™æ£€ç´¢æ¨¡å—ã€å¥–åŠ±æ¨¡å‹æ¨¡å—å’Œç”Ÿæˆæ¨¡å‹æ¨¡å—ã€‚é¦–å…ˆï¼Œç³»ç»Ÿæ£€ç´¢ç”¨æˆ·èµ„æ–™ï¼Œç„¶åé€šè¿‡å¥–åŠ±æ¨¡å‹è¯„ä¼°ç”Ÿæˆçš„å“åº”ï¼Œæœ€åä¼˜åŒ–ç”Ÿæˆæ¨¡å‹ä»¥æé«˜å“åº”è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šPrLMçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†å¯¹æ¯”è®­ç»ƒçš„ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡æ³¨æ¨ç†è·¯å¾„çš„æƒ…å†µä¸‹ï¼Œä»ç”¨æˆ·åé¦ˆä¸­å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†ä¸ªæ€§åŒ–ç”Ÿæˆçš„æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼ŒæŸå¤±å‡½æ•°ç»“åˆäº†ç”Ÿæˆè´¨é‡å’Œä¸ªæ€§åŒ–ç¨‹åº¦çš„è¯„ä¼°ï¼Œç¡®ä¿ç”Ÿæˆçš„å“åº”æ—¢ç¬¦åˆç”¨æˆ·éœ€æ±‚åˆå…·å¤‡é«˜è´¨é‡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡åœ¨å®éªŒä¸­ç»è¿‡å¤šæ¬¡è°ƒä¼˜ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPrLMåœ¨ä¸‰ä¸ªä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%-15%ã€‚åœ¨ä¸åŒæ•°é‡çš„æ£€ç´¢èµ„æ–™å’Œä¸åŒæ£€ç´¢å™¨çš„æƒ…å†µä¸‹ï¼ŒPrLMè¡¨ç°å‡ºè‰¯å¥½çš„ç¨³å¥æ€§ï¼ŒéªŒè¯äº†å…¶å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸ªæ€§åŒ–å®¢æœã€æ™ºèƒ½åŠ©æ‰‹å’Œå†…å®¹æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡ç”Ÿæˆå“åº”çš„ä¸ªæ€§åŒ–ç¨‹åº¦ï¼ŒPrLMèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œå¢å¼ºç”¨æˆ·ä¸ç³»ç»Ÿçš„äº’åŠ¨æ•ˆæœï¼Œæœªæ¥å¯èƒ½åœ¨å¤šä¸ªè¡Œä¸šä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Personalized retrieval-augmented generation (RAG) aims to produce user-tailored responses by incorporating retrieved user profiles alongside the input query. Existing methods primarily focus on improving retrieval and rely on large language models (LLMs) to implicitly integrate the retrieved context with the query. However, such models are often sensitive to retrieval quality and may generate responses that are misaligned with user preferences. To address this limitation, we propose PrLM, a reinforcement learning framework that trains LLMs to explicitly reason over retrieved user profiles. Guided by a contrastively trained personalization reward model, PrLM effectively learns from user responses without requiring annotated reasoning paths. Experiments on three personalized text generation datasets show that PrLM outperforms existing methods and remains robust across varying numbers of retrieved profiles and different retrievers.

