---
layout: default
title: Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race
---

# Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10304" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10304v1</a>
  <a href="https://arxiv.org/pdf/2508.10304.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10304v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10304v1', 'Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gustavo Bonil, Simone Hashiguti, Jhessica Silva, JoÃ£o Gondim, Helena Maia, NÃ¡dia Silva, Helio Pedrini, Sandra Avila

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-14

**å¤‡æ³¨**: 29 pages, 3 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®šæ€§æ¡†æ¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ€§åˆ«ä¸ç§æ—åè§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç®—æ³•åè§` `å®šæ€§åˆ†æ` `æ€§åˆ«åè§` `ç§æ—åè§` `è¯è¯­åˆ†æ` `äººå·¥æ™ºèƒ½ä¼¦ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åè§æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–å®šé‡åˆ†æï¼Œæ— æ³•æ•æ‰åˆ°è‡ªç„¶è¯­è¨€ä¸­åè§çš„ç»†å¾®è¡¨ç°ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å®šæ€§çš„è¯è¯­åˆ†ææ¡†æ¶ï¼Œé€šè¿‡æ‰‹åŠ¨åˆ†æç”Ÿæˆçš„æ–‡æœ¬ï¼Œæ·±å…¥æ¢è®¨æ€§åˆ«å’Œç§æ—åè§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé»‘äººå¥³æ€§å’Œç™½äººå¥³æ€§åœ¨è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ•…äº‹ä¸­è¢«æç»˜çš„æ–¹å¼å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œåæ˜ äº†å›ºåŒ–çš„ç¤¾ä¼šè¯è¯­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¯èƒ½ä¼šé‡ç°åè§ï¼Œå¦‚æ­§è§†å’Œç§æ—åŒ–ï¼Œå¹¶ç»´æŒéœ¸æƒè¯è¯­ã€‚ç°æœ‰çš„åè§æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–å®šé‡çš„è‡ªåŠ¨åŒ–æ‰‹æ®µï¼Œå¾€å¾€å¿½è§†äº†åè§åœ¨è‡ªç„¶è¯­è¨€ä¸­ç»†å¾®çš„è¡¨ç°æ–¹å¼ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å®šæ€§çš„è¯è¯­æ¡†æ¶ï¼Œä»¥è¡¥å……è¿™äº›æ–¹æ³•ã€‚é€šè¿‡å¯¹ç”Ÿæˆçš„çŸ­ç¯‡æ•…äº‹è¿›è¡Œæ‰‹åŠ¨åˆ†æï¼Œæˆ‘ä»¬æ¢è®¨äº†æ€§åˆ«å’Œç§æ—åè§ã€‚ç»“æœæ˜¾ç¤ºï¼Œé»‘äººå¥³æ€§å¸¸è¢«æç»˜ä¸ºä¸ç¥–å…ˆå’ŒæŠµæŠ—ç›¸å…³ï¼Œè€Œç™½äººå¥³æ€§åˆ™å‡ºç°åœ¨è‡ªæˆ‘å‘ç°çš„è¿‡ç¨‹ä¸­ã€‚è¿™äº›æ¨¡å¼åæ˜ äº†è¯­è¨€æ¨¡å‹å¦‚ä½•å¤åˆ¶å›ºåŒ–çš„è¯è¯­è¡¨ç°ï¼Œå¼ºåŒ–äº†æœ¬è´¨åŒ–å’Œç¤¾ä¼šæµåŠ¨æ€§çš„ç¼ºå¤±ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ç®—æ³•çš„æ„è¯†å½¢æ€åŠŸèƒ½ï¼Œå¯¹äººå·¥æ™ºèƒ½çš„ä¼¦ç†ä½¿ç”¨å’Œå¼€å‘å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ€§åˆ«ä¸ç§æ—åè§çš„å†ç°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å®šé‡åˆ†æï¼Œæ— æ³•æ·±å…¥ç†è§£åè§åœ¨è¯­è¨€ä¸­çš„ç»†å¾®è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å®šæ€§çš„è¯è¯­åˆ†ææ¡†æ¶ï¼Œé€šè¿‡å¯¹ç”Ÿæˆæ–‡æœ¬çš„æ‰‹åŠ¨åˆ†æï¼Œæ­ç¤ºåè§çš„å…·ä½“è¡¨ç°å½¢å¼ï¼Œä»¥å¸®åŠ©å¼€å‘è€…å’Œç”¨æˆ·æ›´å¥½åœ°è¯†åˆ«å’Œç¼“è§£è¿™äº›åè§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å®šæ€§åˆ†ææ–¹æ³•ï¼Œé¦–å…ˆæ”¶é›†ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„çŸ­ç¯‡æ•…äº‹ï¼Œç„¶åå¯¹è¿™äº›æ•…äº‹è¿›è¡Œé€ç¯‡åˆ†æï¼Œé‡ç‚¹å…³æ³¨æ€§åˆ«å’Œç§æ—çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥å®šæ€§åˆ†æä½œä¸ºè¡¥å……æ‰‹æ®µï¼Œå¼ºè°ƒäº†å¯¹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æ·±åº¦ç†è§£ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„å®šé‡åè§æ£€æµ‹æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åˆ†æè¿‡ç¨‹ä¸­ï¼Œç ”ç©¶è€…å…³æ³¨æ–‡æœ¬ä¸­çš„è§’è‰²æç»˜ã€æƒ…èŠ‚å‘å±•å’Œè¯­è¨€ä½¿ç”¨ç­‰æ–¹é¢ï¼Œè¯†åˆ«å‡ºé»‘äººå¥³æ€§ä¸ç™½äººå¥³æ€§åœ¨æ•…äº‹ä¸­çš„ä¸åŒè¡¨ç°ï¼Œæ­ç¤ºäº†æ½œåœ¨çš„ç¤¾ä¼šåè§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé»‘äººå¥³æ€§åœ¨ç”Ÿæˆæ•…äº‹ä¸­å¸¸å¸¸è¢«æç»˜ä¸ºä¸ç¥–å…ˆå’ŒæŠµæŠ—ç›¸å…³ï¼Œè€Œç™½äººå¥³æ€§åˆ™æ›´å€¾å‘äºè‡ªæˆ‘å‘ç°çš„ä¸»é¢˜ã€‚è¿™ç§è¡¨ç°åæ˜ äº†è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶çš„åè§ï¼Œä¸”åœ¨å°è¯•çº æ­£æ—¶ï¼Œæ¨¡å‹æä¾›çš„ä¿®æ”¹å¾€å¾€è¡¨é¢åŒ–ï¼Œæœªèƒ½æœ‰æ•ˆæ¶ˆé™¤é—®é¢˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººå·¥æ™ºèƒ½ä¼¦ç†ã€è¯­è¨€æ¨¡å‹çš„å¼€å‘ä¸è¯„ä¼°ã€ä»¥åŠç¤¾ä¼šç§‘å­¦ç ”ç©¶ã€‚é€šè¿‡æä¾›å®šæ€§åˆ†ææ¡†æ¶ï¼Œå¼€å‘è€…å¯ä»¥æ›´æœ‰æ•ˆåœ°è¯†åˆ«å’Œç¼“è§£æ¨¡å‹ä¸­çš„åè§ï¼Œä»è€Œä¿ƒè¿›æ›´å…¬å¹³å’ŒåŒ…å®¹çš„AIåº”ç”¨ã€‚æœªæ¥ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½æ¨åŠ¨è·¨å­¦ç§‘çš„åˆä½œï¼Œæå‡AIç³»ç»Ÿçš„ç¤¾ä¼šè´£ä»»æ„Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the advance of Artificial Intelligence (AI), Large Language Models (LLMs) have gained prominence and been applied in diverse contexts. As they evolve into more sophisticated versions, it is essential to assess whether they reproduce biases, such as discrimination and racialization, while maintaining hegemonic discourses. Current bias detection approaches rely mostly on quantitative, automated methods, which often overlook the nuanced ways in which biases emerge in natural language. This study proposes a qualitative, discursive framework to complement such methods. Through manual analysis of LLM-generated short stories featuring Black and white women, we investigate gender and racial biases. We contend that qualitative methods such as the one proposed here are fundamental to help both developers and users identify the precise ways in which biases manifest in LLM outputs, thus enabling better conditions to mitigate them. Results show that Black women are portrayed as tied to ancestry and resistance, while white women appear in self-discovery processes. These patterns reflect how language models replicate crystalized discursive representations, reinforcing essentialization and a sense of social immobility. When prompted to correct biases, models offered superficial revisions that maintained problematic meanings, revealing limitations in fostering inclusive narratives. Our results demonstrate the ideological functioning of algorithms and have significant implications for the ethical use and development of AI. The study reinforces the need for critical, interdisciplinary approaches to AI design and deployment, addressing how LLM-generated discourses reflect and perpetuate inequalities.

