---
layout: default
title: MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding
---

# MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15802" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15802v1</a>
  <a href="https://arxiv.org/pdf/2508.15802.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15802v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15802v1', 'MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohan Jiang, Jin Gao, Jiahao Zhan, Dequan Wang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-14

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/mhjiang0408/MAC_Bench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMACåŸºå‡†ä»¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç§‘å­¦ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç§‘å­¦ç†è§£` `åŠ¨æ€åŸºå‡†` `è·¨æ¨¡æ€æ¨ç†` `å›¾åƒ-æ–‡æœ¬å¯¹` `DADæ–¹æ³•` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›ºå®šåŸºå‡†æ— æ³•æœ‰æ•ˆè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„å±€é™æ€§ã€‚
2. æå‡ºäº†MACåŸºå‡†ï¼Œåˆ©ç”¨å¤§é‡å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œæ—¨åœ¨æŒ‘æˆ˜MLLMsåœ¨ç§‘å­¦å†…å®¹çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMLLMsåœ¨æ„ŸçŸ¥èƒ½åŠ›ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›æœ‰é™ï¼ŒDADæ–¹æ³•èƒ½å¤Ÿæå‡å…¶æ€§èƒ½è¾¾11%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½åŠ›çš„æå‡ï¼Œå›ºå®šåŸºå‡†é€æ¸å¤±å»åœ¨é«˜æ°´å¹³ç§‘å­¦ç†è§£è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€å­¦æœ¯å°é¢åŸºå‡†ï¼ˆMACï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿéšç€ç§‘å­¦è¿›å±•å’Œæ¨¡å‹è¿›æ­¥ä¸æ–­æ¼”å˜çš„åŠ¨æ€åŸºå‡†ã€‚MACåˆ©ç”¨æ¥è‡ªé¡¶çº§ç§‘å­¦æœŸåˆŠï¼ˆå¦‚ã€Šè‡ªç„¶ã€‹ã€ã€Šç§‘å­¦ã€‹å’Œã€Šç»†èƒã€‹ï¼‰çš„è¶…è¿‡25,000ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ï¼ŒæŒ‘æˆ˜MLLMsåœ¨æŠ½è±¡è§†è§‰å’Œæ–‡æœ¬ç§‘å­¦å†…å®¹ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡MLLMsåœ¨æ„ŸçŸ¥èƒ½åŠ›ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†å…¶è·¨æ¨¡æ€ç§‘å­¦æ¨ç†ä»ç„¶æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†DADï¼Œä¸€ç§è½»é‡çº§æ¨ç†æ—¶é—´æ–¹æ³•ï¼Œé€šè¿‡æ‰©å±•MLLMçš„è§†è§‰ç‰¹å¾ä¸è¯­è¨€ç©ºé—´æ¨ç†ç›¸ç»“åˆï¼Œæå‡æ€§èƒ½è¾¾11%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å›ºå®šåŸºå‡†åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç§‘å­¦ç†è§£èƒ½åŠ›æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è·¨æ¨¡æ€æ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMACåŸºå‡†ï¼Œåˆ©ç”¨æ¥è‡ªé¡¶çº§ç§‘å­¦æœŸåˆŠçš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼ŒåŠ¨æ€æ›´æ–°ä»¥é€‚åº”ç§‘å­¦è¿›å±•ï¼ŒæŒ‘æˆ˜MLLMsçš„æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶å¼•å…¥DADæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè§†è§‰ç‰¹å¾ä¸è¯­è¨€æ¨ç†ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMACåŸºå‡†åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è¯„ä¼°å’ŒåŠ¨æ€æ›´æ–°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ”¶é›†é˜¶æ®µä»é¡¶çº§æœŸåˆŠè·å–å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œæ¨¡å‹è¯„ä¼°é˜¶æ®µé€šè¿‡æ ‡å‡†åŒ–æµ‹è¯•è¯„ä¼°MLLMsçš„æ¨ç†èƒ½åŠ›ï¼ŒåŠ¨æ€æ›´æ–°é˜¶æ®µæ ¹æ®æœ€æ–°ç§‘å­¦è¿›å±•è°ƒæ•´åŸºå‡†å†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMACåŸºå‡†çš„åŠ¨æ€ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿä¸ç§‘å­¦å‰æ²¿ä¿æŒä¸€è‡´ï¼ŒDADæ–¹æ³•åˆ™é€šè¿‡è½»é‡çº§æ¨ç†å¢å¼ºäº†MLLMsçš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè¿™æ˜¯ä¸ç°æœ‰é™æ€åŸºå‡†çš„æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DADæ–¹æ³•ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–è§†è§‰ç‰¹å¾ä¸è¯­è¨€æ¨ç†çš„ç»“åˆï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†æ—¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ä¸¤ç§æ¨¡æ€çš„ä¿¡æ¯ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚å°šæœªå…¬å¼€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLLMsåœ¨MACåŸºå‡†ä¸Šçš„è¡¨ç°æ­ç¤ºäº†å…¶åœ¨æ„ŸçŸ¥èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ï¼Œä½†åœ¨è·¨æ¨¡æ€ç§‘å­¦æ¨ç†æ–¹é¢ä»æ˜¾ä¸è¶³ã€‚é€šè¿‡DADæ–¹æ³•ï¼Œæ¨¡å‹æ€§èƒ½æå‡è¾¾11%ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•åœ¨å¢å¼ºæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§‘å­¦ç ”ç©¶ã€æ•™è‚²å’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼ŒMACåŸºå‡†èƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°å¤„ç†å’Œåˆ†æç§‘å­¦æ–‡çŒ®ï¼Œä¿ƒè¿›ç§‘å­¦çŸ¥è¯†çš„ä¼ æ’­ä¸åº”ç”¨ï¼Œæœªæ¥å¯èƒ½å¯¹ç§‘å­¦ç ”ç©¶çš„æ•ˆç‡å’Œè´¨é‡äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As multimodal large language models (MLLMs) grow increasingly capable, fixed benchmarks are gradually losing their effectiveness in evaluating high-level scientific understanding. In this paper, we introduce the Multimodal Academic Cover benchmark (MAC), a live benchmark that could continuously evolve with scientific advancement and model progress. MAC leverages over 25,000 image-text pairs sourced from issues of top-tier scientific journals such as Nature, Science, and Cell, challenging MLLMs to reason across abstract visual and textual scientific content. Experiments on our most recent yearly snapshot, MAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities, their cross-modal scientific reasoning remains limited. To bridge this gap, we propose DAD, a lightweight inference-time approach that enhances MLLMs by extending MLLM visual features with language space reasoning, achieving performance improvements of up to 11%. Finally, we highlight the live nature of MAC through experiments on updating journal covers and models for curation, illustrating its potential to remain aligned with the frontier of human knowledge. We release our benchmark at https://github.com/mhjiang0408/MAC_Bench.

