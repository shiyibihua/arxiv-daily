---
layout: default
title: Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models
---

# Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20954" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20954v1</a>
  <a href="https://arxiv.org/pdf/2512.20954.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20954v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20954v1', 'Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiang Zhang, Jiaqi Wei, Yuejin Yang, Zijie Qiu, Yuhan Chen, Zhiqiang Gao, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Wanli Ouyang, Chenyu You, Siqi Sun

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåå°„é¢„è®­ç»ƒï¼Œä½¿ç”Ÿç‰©åºåˆ—æ¨¡å‹å…·å¤‡tokençº§è‡ªçº é”™èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”Ÿç‰©åºåˆ—æ¨¡å‹` `åå°„é¢„è®­ç»ƒ` `è‡ªçº é”™` `è¯­è¨€è¡¨è¾¾èƒ½åŠ›` `è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç”Ÿç‰©åºåˆ—æ¨¡å‹tokenè¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„CoTæ¨ç†æ–¹æ³•ã€‚
2. æå‡ºåå°„é¢„è®­ç»ƒï¼Œé€šè¿‡å¼•å…¥è¾…åŠ©â€œæ€è€ƒtokenâ€å¢å¼ºç”Ÿç‰©åºåˆ—æ¨¡å‹çš„è¯­è¨€è¡¨è¾¾èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡è›‹ç™½è´¨æ¨¡å‹çš„è‡ªçº é”™èƒ½åŠ›ï¼Œå¹¶å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ç”Ÿç‰©åºåˆ—æ¨¡å‹ï¼ˆå¦‚è›‹ç™½è´¨å’ŒRNAè¯­è¨€æ¨¡å‹ï¼‰çš„åå°„é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨éè‡ªç„¶è¯­è¨€é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚ä¸è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„Chain-of-Thought (CoT) promptingä¸åŒï¼Œç”Ÿç‰©åºåˆ—æ¨¡å‹ç”±äºtokenç©ºé—´çš„è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨CoTã€‚æœ¬æ–‡é¦–å…ˆå®šä¹‰äº†è¯­è¨€è¡¨è¾¾èƒ½åŠ›çš„æ¦‚å¿µï¼Œå¹¶æŒ‡å‡ºè›‹ç™½è´¨è¯­è¨€çš„è¡¨è¾¾èƒ½åŠ›ä¸è¶³é™åˆ¶äº†CoTçš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†åå°„é¢„è®­ç»ƒï¼Œé€šè¿‡ç”Ÿæˆè¾…åŠ©çš„â€œæ€è€ƒtokenâ€ï¼Œå¢å¼ºæ¨¡å‹çš„ä¸­é—´æ¨ç†èƒ½åŠ›ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œæ‰©å……çš„tokené›†åˆæ˜¾è‘—æå‡äº†ç”Ÿç‰©è¯­è¨€çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ•´ä½“æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é¢„è®­ç»ƒæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡è›‹ç™½è´¨æ¨¡å‹çš„è‡ªçº é”™èƒ½åŠ›ï¼Œå¹¶æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è›‹ç™½è´¨å’ŒRNAè¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ç”Ÿç‰©åºåˆ—ä»»åŠ¡æ—¶ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨tokençº§åˆ«è¿›è¡Œè‡ªçº é”™ã€‚è¿™æ˜¯å› ä¸ºç”Ÿç‰©åºåˆ—çš„tokenç©ºé—´ï¼ˆå¦‚æ°¨åŸºé…¸ï¼‰è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œæ— æ³•åƒè‡ªç„¶è¯­è¨€é‚£æ ·é€šè¿‡Chain-of-Thought (CoT) promptingç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„æ¨ç†æ·±åº¦å’Œå‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥åå°„é¢„è®­ç»ƒï¼Œæ‰©å±•ç”Ÿç‰©åºåˆ—æ¨¡å‹çš„tokenç©ºé—´ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆè¾…åŠ©çš„â€œæ€è€ƒtokenâ€ã€‚è¿™äº›æ€è€ƒtokenç±»ä¼¼äºCoTä¸­çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¯ä»¥å¸®åŠ©æ¨¡å‹è¿›è¡Œæ›´æ·±å…¥çš„æ¨ç†å’Œè‡ªçº é”™ã€‚é€šè¿‡å¢å¼ºè¯­è¨€çš„è¡¨è¾¾èƒ½åŠ›ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£ç”Ÿç‰©åºåˆ—çš„å¤æ‚å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œå®šä¹‰å¹¶æ‰©å……ç”Ÿç‰©åºåˆ—æ¨¡å‹çš„tokené›†åˆï¼Œå¼•å…¥æ–°çš„â€œæ€è€ƒtokenâ€ï¼Œè¿™äº›tokenä»£è¡¨äº†æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸­é—´çŠ¶æ€æˆ–æ€è€ƒè¿‡ç¨‹ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨åå°„é¢„è®­ç»ƒç­–ç•¥ï¼Œè®­ç»ƒæ¨¡å‹ç”Ÿæˆå’Œåˆ©ç”¨è¿™äº›æ€è€ƒtokenï¼Œä»è€Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ•´ä½“æµç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼šè¾“å…¥ç”Ÿç‰©åºåˆ— -> æ¨¡å‹ç”Ÿæˆæ€è€ƒtoken -> æ¨¡å‹åŸºäºæ€è€ƒtokenè¿›è¡Œæ¨ç† -> è¾“å‡ºæœ€ç»ˆç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡å°†åå°„é¢„è®­ç»ƒçš„æ¦‚å¿µå¼•å…¥ç”Ÿç‰©åºåˆ—æ¨¡å‹ï¼Œå¹¶æå‡ºäº†é€šè¿‡æ‰©å……tokené›†åˆæ¥å¢å¼ºç”Ÿç‰©è¯­è¨€è¡¨è¾¾èƒ½åŠ›çš„æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„é¢„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œåå°„é¢„è®­ç»ƒèƒ½å¤Ÿä½¿æ¨¡å‹å…·å¤‡æ›´å¼ºçš„æ¨ç†èƒ½åŠ›å’Œè‡ªçº é”™èƒ½åŠ›ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†å¤æ‚çš„ç”Ÿç‰©åºåˆ—ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„æŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬ï¼š(1) æ€è€ƒtokençš„è®¾è®¡ï¼šéœ€è¦æ ¹æ®å…·ä½“çš„ç”Ÿç‰©åºåˆ—ä»»åŠ¡è®¾è®¡åˆé€‚çš„æ€è€ƒtokenï¼Œä¾‹å¦‚ï¼Œå¯ä»¥è®¾è®¡ä»£è¡¨åºåˆ—ç»“æ„ã€åŠŸèƒ½æˆ–ç›¸äº’ä½œç”¨çš„tokenã€‚(2) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼šéœ€è¦è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæœ‰æ„ä¹‰çš„æ€è€ƒtokenï¼Œå¹¶åˆ©ç”¨è¿™äº›tokenè¿›è¡Œå‡†ç¡®çš„æ¨ç†ã€‚(3) ç½‘ç»œç»“æ„çš„é€‰æ‹©ï¼šå¯ä»¥ä½¿ç”¨Transformerç­‰å¸¸ç”¨çš„åºåˆ—æ¨¡å‹ä½œä¸ºåŸºç¡€æ¶æ„ï¼Œå¹¶æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥å¼•å…¥é¢å¤–çš„æ³¨æ„åŠ›æœºåˆ¶æ¥æ›´å¥½åœ°æ•æ‰æ€è€ƒtokenä¸åŸå§‹åºåˆ—ä¹‹é—´çš„å…³ç³»ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20954v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20954v1/fig2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20954v1/x2.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡åå°„é¢„è®­ç»ƒï¼Œè›‹ç™½è´¨æ¨¡å‹çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”äºæ ‡å‡†é¢„è®­ç»ƒæ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡äº†è¶…è¿‡10%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜å±•ç°å‡ºäº†æ›´å¼ºçš„è‡ªçº é”™èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«å’Œçº æ­£é¢„æµ‹è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹ã€è¯ç‰©è®¾è®¡ã€åŸºå› ç¼–è¾‘ç­‰ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸã€‚é€šè¿‡æå‡ç”Ÿç‰©åºåˆ—æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°é¢„æµ‹è›‹ç™½è´¨çš„ç»“æ„å’ŒåŠŸèƒ½ï¼ŒåŠ é€Ÿæ–°è¯çš„ç ”å‘è¿‡ç¨‹ï¼Œå¹¶ä¸ºåŸºå› æ²»ç–—æä¾›æ›´å¯é çš„ä¾æ®ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨ç”Ÿç‰©è®¡ç®—å’Œäººå·¥æ™ºèƒ½åœ¨ç”Ÿå‘½ç§‘å­¦é¢†åŸŸçš„æ·±åº¦èåˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary "thinking tokens" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.

