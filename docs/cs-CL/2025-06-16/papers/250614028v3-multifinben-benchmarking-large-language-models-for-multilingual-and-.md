---
layout: default
title: MultiFinBen: Benchmarking Large Language Models for Multilingual and Multimodal Financial Application
---

# MultiFinBen: Benchmarking Large Language Models for Multilingual and Multimodal Financial Application

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14028" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14028v3</a>
  <a href="https://arxiv.org/pdf/2506.14028.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14028v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14028v3', 'MultiFinBen: Benchmarking Large Language Models for Multilingual and Multimodal Financial Application')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xueqing Peng, Lingfei Qian, Yan Wang, Ruoyu Xiang, Yueru He, Yang Ren, Mingyang Jiang, Vincent Jim Zhang, Yuqing Guo, Jeff Zhao, Huan He, Yi Han, Yun Feng, Yuechen Jiang, Yupeng Cao, Haohang Li, Yangyang Yu, Xiaoyu Wang, Penglei Gao, Shengyuan Lin, Keyi Wang, Shanshan Yang, Yilun Zhao, Zhiwei Liu, Peng Lu, Jerry Huang, Suyuchen Wang, Triantafillos Papadopoulos, Polydoros Giannouris, Efstathia Soufleri, Nuo Chen, Zhiyang Deng, Heming Fu, Yijia Zhao, Mingquan Lin, Meikang Qiu, Kaleb E Smith, Arman Cohan, Xiao-Yang Liu, Jimin Huang, Guojun Xiong, Alejandro Lopez-Lira, Xi Chen, Junichi Tsujii, Jian-Yun Nie, Sophia Ananiadou, Qianqian Xie

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16 (æ›´æ–°: 2025-10-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMultiFinBenä»¥è§£å†³å¤šè¯­è¨€å¤šæ¨¡æ€é‡‘èåˆ†æè¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€å¤„ç†` `å¤šæ¨¡æ€åˆ†æ` `é‡‘èæ¨ç†` `OCRæŠ€æœ¯` `åŸºå‡†è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡‘èåˆ†æè¯„ä¼°æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•ä¸€è¯­è¨€å’Œæ–‡æœ¬æ•°æ®ï¼Œç¼ºä¹å¯¹å¤šè¯­è¨€å’Œå¤šæ¨¡æ€ä¿¡æ¯çš„ç»¼åˆè¯„ä¼°ã€‚
2. è®ºæ–‡æå‡ºMultiFinBenåŸºå‡†ï¼Œç»“åˆå¤šè¯­è¨€å’Œå¤šæ¨¡æ€æ•°æ®ï¼Œè®¾è®¡äº†å¤šè¯­è¨€é‡‘èæ¨ç†å’Œé‡‘èOCRä¸¤å¤§ä»»åŠ¡ï¼Œä»¥æå‡è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ï¼Œæ•´ä½“å¾—åˆ†ä»…ä¸º46.01%ï¼Œæ­ç¤ºäº†è¯¥é¢†åŸŸçš„ç ”ç©¶ç©ºç™½å’Œæ”¹è¿›ç©ºé—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°å®ä¸–ç•Œçš„é‡‘èåˆ†ææ¶‰åŠå¤šç§è¯­è¨€å’Œæ¨¡æ€çš„ä¿¡æ¯ï¼Œä»æŠ¥å‘Šå’Œæ–°é—»åˆ°æ‰«ææ–‡ä»¶å’Œä¼šè®®å½•éŸ³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡‘èé¢†åŸŸçš„è¯„ä¼°å¤§å¤šä»…é™äºæ–‡æœ¬ã€å•è¯­è¨€ï¼Œå¹¶ä¸”ç°æœ‰æ¨¡å‹çš„è¡¨ç°å·²è¶‹äºé¥±å’Œã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MultiFinBenï¼Œè¿™æ˜¯é¦–ä¸ªä¸“å®¶æ³¨é‡Šçš„å¤šè¯­è¨€ï¼ˆäº”ç§è¯­è¨€ï¼‰å’Œå¤šæ¨¡æ€ï¼ˆæ–‡æœ¬ã€è§†è§‰ã€éŸ³é¢‘ï¼‰åŸºå‡†ï¼Œç”¨äºåœ¨çœŸå®é‡‘èç¯å¢ƒä¸­è¯„ä¼°LLMsã€‚MultiFinBenå¼•å…¥äº†ä¸¤ä¸ªæ–°çš„ä»»åŠ¡ç³»åˆ—ï¼šå¤šè¯­è¨€é‡‘èæ¨ç†å’Œé‡‘èOCRã€‚é€šè¿‡å¯¹21ä¸ªé¢†å…ˆçš„LLMsè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯å‰æ²¿çš„å¤šæ¨¡æ€æ¨¡å‹å¦‚GPT-4oåœ¨æ•´ä½“è¡¨ç°ä¸Šä¹Ÿä»…è¾¾åˆ°46.01%ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†åœ¨å¤šè¯­è¨€ã€å¤šæ¨¡æ€å’Œä¸“å®¶çº§é‡‘èæ¨ç†æ–¹é¢çš„æŒç»­å±€é™æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰é‡‘èåˆ†æè¯„ä¼°æ–¹æ³•åœ¨å¤šè¯­è¨€å’Œå¤šæ¨¡æ€æ•°æ®å¤„ç†ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹å¯¹è·¨è¯­è¨€å’Œå¤šæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºMultiFinBenåŸºå‡†ï¼Œè®ºæ–‡å¼•å…¥äº†å¤šè¯­è¨€é‡‘èæ¨ç†å’Œé‡‘èOCRä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°LLMsåœ¨çœŸå®é‡‘èåœºæ™¯ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯è·¨è¯­è¨€å’Œå¤šæ¨¡æ€çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMultiFinBençš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ä»»åŠ¡è®¾è®¡ã€æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æå››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ”¶é›†é˜¶æ®µæ¶µç›–å¤šç§è¯­è¨€å’Œæ¨¡æ€çš„ä¿¡æ¯ï¼Œä»»åŠ¡è®¾è®¡åˆ™èšç„¦äºé‡‘èæ¨ç†å’ŒOCRã€‚

**å…³é”®åˆ›æ–°**ï¼šMultiFinBençš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å¤šè¯­è¨€å’Œå¤šæ¨¡æ€çš„ç»“åˆï¼Œå°¤å…¶æ˜¯åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨ä¸Šï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ç©ºç™½ï¼Œå¹¶æä¾›äº†ç»“æ„åŒ–çš„ã€éš¾åº¦æ„ŸçŸ¥çš„æ•°æ®é€‰æ‹©ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é€‰æ‹©è¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡é‡‡ç”¨äº†åŸºäºæ¨¡å‹è¡¨ç°çš„éš¾åº¦æ„ŸçŸ¥é€‰æ‹©ç­–ç•¥ï¼Œç¡®ä¿è¯„ä¼°ä»»åŠ¡çš„å¹³è¡¡æ€§ï¼Œå¹¶å»é™¤äº†å†—ä½™ä»»åŠ¡ï¼Œä»¥æå‡è¯„ä¼°çš„æœ‰æ•ˆæ€§å’ŒæŒ‘æˆ˜æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒå®Œæ•´è®ºæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ21ä¸ªé¢†å…ˆçš„LLMsåœ¨MultiFinBenåŸºå‡†ä¸Šçš„æ•´ä½“å¾—åˆ†ä»…ä¸º46.01%ï¼Œå…¶ä¸­è§†è§‰å’ŒéŸ³é¢‘ä»»åŠ¡è¡¨ç°è¾ƒå¼ºï¼Œä½†åœ¨å¤šè¯­è¨€è®¾ç½®ä¸‹æ˜¾è‘—ä¸‹é™ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†å¤šè¯­è¨€é‡‘èä¿¡æ¯æ—¶çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†æœªæ¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èåˆ†æã€æŠ•èµ„å†³ç­–æ”¯æŒå’Œè·¨å›½å…¬å¸çš„è´¢åŠ¡æŠ¥å‘Šåˆ†æã€‚é€šè¿‡æä¾›å¤šè¯­è¨€å’Œå¤šæ¨¡æ€çš„è¯„ä¼°åŸºå‡†ï¼ŒMultiFinBenèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é‡‘èç¯å¢ƒä¸­çš„èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-world financial analysis involves information across multiple languages and modalities, from reports and news to scanned filings and meeting recordings. Yet most existing evaluations of LLMs in finance remain text-only, monolingual, and largely saturated by current models. To bridge these gaps, we present MultiFinBen, the first expert-annotated multilingual (five languages) and multimodal (text, vision, audio) benchmark for evaluating LLMs in realistic financial contexts. MultiFinBen introduces two new task families: multilingual financial reasoning, which tests cross-lingual evidence integration from filings and news, and financial OCR, which extracts structured text from scanned documents containing tables and charts. Rather than aggregating all available datasets, we apply a structured, difficulty-aware selection based on advanced model performance, ensuring balanced challenge and removing redundant tasks. Evaluating 21 leading LLMs shows that even frontier multimodal models like GPT-4o achieve only 46.01% overall, stronger on vision and audio but dropping sharply in multilingual settings. These findings expose persistent limitations in multilingual, multimodal, and expert-level financial reasoning. All datasets, evaluation scripts, and leaderboards are publicly released.

