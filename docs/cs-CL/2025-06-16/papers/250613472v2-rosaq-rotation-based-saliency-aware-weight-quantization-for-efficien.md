---
layout: default
title: ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models
---

# ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13472" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13472v2</a>
  <a href="https://arxiv.org/pdf/2506.13472.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13472v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13472v2', 'ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junho Yoon, Geom Lee, Donghyeon Jeon, Inho Kang, Seung-Hoon Na

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16 (æ›´æ–°: 2025-06-17)

**å¤‡æ³¨**: 10 pages, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºROSAQä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é‡åŒ–æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æŠ€æœ¯` `å¤§è¯­è¨€æ¨¡å‹` `æ˜¾è‘—æ€§æ„ŸçŸ¥` `ä¸»æˆåˆ†åˆ†æ` `æ··åˆç²¾åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆè¯†åˆ«å’Œåˆ©ç”¨æ˜¾è‘—ç‰¹å¾ï¼Œå¯¼è‡´æ€§èƒ½æŸå¤±ã€‚
2. ROSAQé€šè¿‡åˆ©ç”¨å˜æ¢å™¨çš„æ—‹è½¬ä¸å˜æ€§ï¼Œæå‡ºäº†ä¸€ç§åœ¨æŠ•å½±ç‰¹å¾ç©ºé—´ä¸­è¯†åˆ«æ˜¾è‘—é€šé“çš„é‡åŒ–æ–¹æ³•ï¼Œæå‡äº†é‡åŒ–æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒROSAQåœ¨ç”Ÿæˆ256ä¸ªtokenæ—¶ï¼Œç›¸æ¯”äºFP16å®ç°ï¼Œé€Ÿåº¦æå‡çº¦2.3å€ï¼Œä¸”åœ¨æ˜¾è‘—æ€§æ„ŸçŸ¥é‡åŒ–ä¸Šè¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é‡åŒ–ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†…å­˜éœ€æ±‚ï¼Œå¹¶å¯èƒ½æ”¹å–„å»¶è¿Ÿæ—¶é—´ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ—‹è½¬çš„æ˜¾è‘—æ€§æ„ŸçŸ¥æƒé‡é‡åŒ–æ–¹æ³•ï¼ˆROSAQï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨æŠ•å½±ç‰¹å¾ç©ºé—´ä¸­è¯†åˆ«æ˜¾è‘—é€šé“ï¼Œè€Œéåœ¨åŸå§‹ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œè¯†åˆ«ã€‚ROSAQåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š1ï¼‰åŸºäºä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰çš„æŠ•å½±ï¼Œé¦–å…ˆå¯¹æ ¡å‡†é›†è¿›è¡ŒPCAåˆ†æå¹¶é€šè¿‡PCAæŠ•å½±è¿›è¡Œè½¬æ¢ï¼›2ï¼‰æ˜¾è‘—é€šé“è¯†åˆ«ï¼Œé€‰æ‹©ä¸Kä¸ªæœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç»´åº¦ä½œä¸ºæ˜¾è‘—é€šé“ï¼›3ï¼‰æ˜¾è‘—æ€§æ„ŸçŸ¥çš„æ··åˆç²¾åº¦é‡åŒ–ï¼Œå¯¹æ˜¾è‘—ç»´åº¦ä½¿ç”¨FP16ï¼Œå¯¹å…¶ä»–ç»´åº¦ä½¿ç”¨INT3/4ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒROSAQåœ¨åŸå§‹ç‰¹å¾ç©ºé—´çš„æ˜¾è‘—æ€§æ„ŸçŸ¥é‡åŒ–å’Œå…¶ä»–ç°æœ‰é‡åŒ–æ–¹æ³•ä¸Šå‡æœ‰æ”¹è¿›ã€‚é€šè¿‡å†…æ ¸èåˆï¼ŒROSAQåœ¨ç”Ÿæˆ256ä¸ªtokenæ—¶ï¼Œç›¸æ¯”FP16å®ç°é€Ÿåº¦æå‡çº¦2.3å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹é‡åŒ–è¿‡ç¨‹ä¸­æ˜¾è‘—ç‰¹å¾è¯†åˆ«ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åŸå§‹ç‰¹å¾ç©ºé—´ä¸­éš¾ä»¥æœ‰æ•ˆè¯†åˆ«æ˜¾è‘—é€šé“ï¼Œå¯¼è‡´é‡åŒ–æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šROSAQçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å˜æ¢å™¨çš„æ—‹è½¬ä¸å˜æ€§ï¼Œåœ¨æŠ•å½±ç‰¹å¾ç©ºé—´ä¸­è¯†åˆ«æ˜¾è‘—é€šé“ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æƒé‡é‡åŒ–ã€‚é€šè¿‡é€‰æ‹©ä¸æœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç»´åº¦ï¼ŒROSAQèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™é‡è¦ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šROSAQçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰PCAæŠ•å½±æ¨¡å—ï¼Œå¯¹æ ¡å‡†é›†è¿›è¡Œä¸»æˆåˆ†åˆ†æå¹¶è¿›è¡Œç‰¹å¾è½¬æ¢ï¼›2ï¼‰æ˜¾è‘—é€šé“è¯†åˆ«æ¨¡å—ï¼Œé€‰æ‹©Kä¸ªæœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç»´åº¦ä½œä¸ºæ˜¾è‘—é€šé“ï¼›3ï¼‰æ··åˆç²¾åº¦é‡åŒ–æ¨¡å—ï¼Œå¯¹æ˜¾è‘—ç»´åº¦ä½¿ç”¨FP16ï¼Œå¯¹å…¶ä»–ç»´åº¦ä½¿ç”¨INT3/4è¿›è¡Œé‡åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šROSAQçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åœ¨æŠ•å½±ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œæ˜¾è‘—æ€§æ„ŸçŸ¥é‡åŒ–ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿåœ¨åŸå§‹ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œé‡åŒ–çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«å’Œåˆ©ç”¨æ˜¾è‘—ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ROSAQä¸­ï¼ŒPCAçš„å‚æ•°è®¾ç½®å’ŒKå€¼çš„é€‰æ‹©æ˜¯å…³é”®è®¾è®¡å› ç´ ï¼Œç¡®ä¿æ˜¾è‘—é€šé“çš„å‡†ç¡®è¯†åˆ«å’Œé‡åŒ–æ•ˆæœçš„ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæ··åˆç²¾åº¦é‡åŒ–çš„è®¾è®¡ä½¿å¾—åœ¨ä¸åŒç»´åº¦ä¸Šä½¿ç”¨ä¸åŒçš„ç²¾åº¦ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ROSAQåœ¨å®éªŒä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç”Ÿæˆ256ä¸ªtokenæ—¶ï¼Œç›¸æ¯”äºFP16å®ç°é€Ÿåº¦æå‡çº¦2.3å€ã€‚æ­¤å¤–ï¼ŒROSAQåœ¨æ˜¾è‘—æ€§æ„ŸçŸ¥é‡åŒ–æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå±•ç°äº†å…¶åœ¨é‡åŒ–æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–ã€‚ROSAQèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ¨¡å‹çš„å†…å­˜å ç”¨ï¼Œæé«˜æ¨ç†é€Ÿåº¦ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Quantization has been widely studied as an effective technique for reducing the memory requirement of large language models (LLMs), potentially improving the latency time as well. Utilizing the characteristic of rotational invariance of transformer, we propose the rotation-based saliency-aware weight quantization (ROSAQ), which identifies salient channels in the projection feature space, not in the original feature space, where the projected "principal" dimensions are naturally considered as "salient" features. The proposed ROSAQ consists of 1) PCA-based projection, which first performs principal component analysis (PCA) on a calibration set and transforms via the PCA projection, 2) Salient channel dentification, which selects dimensions corresponding to the K-largest eigenvalues as salient channels, and 3) Saliency-aware quantization with mixed-precision, which uses FP16 for salient dimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ shows improvements over the baseline saliency-aware quantization on the original feature space and other existing quantization methods. With kernel fusion, ROSAQ presents about 2.3x speed up over FP16 implementation in generating 256 tokens with a batch size of 64.

