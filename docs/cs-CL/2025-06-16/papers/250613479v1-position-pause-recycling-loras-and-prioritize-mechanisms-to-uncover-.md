---
layout: default
title: Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness
---

# Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13479" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13479v1</a>
  <a href="https://arxiv.org/pdf/2506.13479.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13479v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13479v1', 'Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mei-Yen Chen, Thi Thu Uyen Hoang, Michael Hahn, M. Saquib Sarfraz

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé‡ç”¨LoRAsçš„æœ‰æ•ˆæ€§åˆ†æä»¥è§£å†³æ¨¡å‹æ•´åˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©é€‚é…å™¨` `çŸ¥è¯†æ•´åˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹ä¼˜åŒ–` `æ•°æ®ç¨€ç¼º` `ç†è®ºåˆ†æ` `é€‚é…å™¨æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LoRAsåˆå¹¶æˆ–è·¯ç”±æ–¹æ³•åœ¨çŸ¥è¯†æ•´åˆä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæ¢è®¨é‡ç”¨LoRAsçš„æœ‰æ•ˆæ€§åŠå…¶å±€é™æ€§ã€‚
3. å®éªŒè¯æ˜é‡ç”¨LoRAsåœ¨é€»è¾‘çŸ¥è¯†æ•´åˆä¸Šå¸¸å¸¸å¤±è´¥ï¼Œå°¤å…¶æ˜¯åœ¨çŸ¥è¯†ç¨€ç¼ºçš„åœºæ™¯ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½ç§©é€‚é…å™¨ï¼ˆLoRAsï¼‰çš„åˆå¹¶æˆ–è·¯ç”±å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„çƒ­é—¨è§£å†³æ–¹æ¡ˆï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®è®¿é—®å—åˆ°ç›‘ç®¡æˆ–é¢†åŸŸç‰¹å®šé™åˆ¶æ—¶ã€‚æœ¬æ–‡ä¸»å¼ ç ”ç©¶ç•Œåº”å°†é‡ç‚¹ä»å¼€å‘æ–°ç®—æ³•è½¬å‘ç†è§£é‡ç”¨LoRAsçš„æœ‰æ•ˆæ¡ä»¶ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œåˆæˆçš„ä¸¤æ­¥æ¨ç†åŠæ•°å­¦é—®é¢˜ä»»åŠ¡ï¼Œç ”ç©¶å‘ç°é‡ç”¨LoRAså¸¸å¸¸æ— æ³•åœ¨ä¸åŒçš„å¾®è°ƒæ•°æ®é›†ä¹‹é—´é€»è¾‘æ•´åˆçŸ¥è¯†ï¼Œå°¤å…¶æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´çŸ¥è¯†æœªè¢«å……åˆ†ä»£è¡¨çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬çš„å®è¯ç»“æœå’Œç†è®ºè§è§£è¡¨æ˜ï¼Œé‡ç”¨LoRAsåœ¨æœªè§ä»»åŠ¡ä¸­çš„å¯è¡Œæ€§å­˜ç–‘ï¼Œå‘¼åæš‚åœå¯¹æ–°æ–¹æ³•çš„è¿½æ±‚ï¼Œå¹¶å¼ºè°ƒéœ€è¦ä¸¥æ ¼çš„æœºåˆ¶æ¥æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é‡ç”¨ä½ç§©é€‚é…å™¨ï¼ˆLoRAsï¼‰åœ¨ä¸åŒå¾®è°ƒæ•°æ®é›†ä¹‹é—´çš„çŸ¥è¯†æ•´åˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºæˆ–çŸ¥è¯†æœªå……åˆ†ä»£è¡¨çš„æƒ…å†µä¸‹ï¼Œå¾€å¾€æ— æ³•å®ç°æœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæ¢è®¨é‡ç”¨LoRAsçš„æ¡ä»¶å’Œå±€é™æ€§ï¼Œå¼ºè°ƒåœ¨æœªè§ä»»åŠ¡ä¸­é‡ç”¨çš„æœ‰æ•ˆæ€§å­˜ç–‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨ç†è®ºåˆ†æä¸å®è¯å®éªŒç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬ç†è®ºæ¨å¯¼ã€åˆæˆæ¨ç†ä»»åŠ¡å’Œæ•°å­¦é—®é¢˜ä»»åŠ¡çš„è®¾è®¡ä¸è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å¯¹é‡ç”¨LoRAsçš„æœ‰æ•ˆæ€§è¿›è¡Œç³»ç»Ÿæ€§åˆ†æï¼Œè´¨ç–‘å…¶ä½œä¸ºæ•°æ®æ— å…³æ–¹æ³•çš„å¯è¡Œæ€§ï¼Œä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…³æ³¨é‡ç”¨æ¡ä»¶è€Œéç®—æ³•æœ¬èº«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­é‡‡ç”¨äº†å‚æ•°å¹³å‡å’ŒåŠ¨æ€é€‚é…å™¨é€‰æ‹©ä¸¤ç§æ•°æ®æ— å…³çš„æ–¹æ³•ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†çŸ¥è¯†æ•´åˆçš„é€»è¾‘æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡ç”¨LoRAsåœ¨é€»è¾‘çŸ¥è¯†æ•´åˆä¸Šå¸¸å¸¸å¤±è´¥ï¼Œå°¤å…¶æ˜¯åœ¨çŸ¥è¯†ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚é€šè¿‡å¯¹æ¯”å®éªŒï¼Œå‘ç°å‚æ•°å¹³å‡å’ŒåŠ¨æ€é€‚é…å™¨é€‰æ‹©æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°æœªèƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„ç»¼åˆèƒ½åŠ›ï¼ŒéªŒè¯äº†é‡ç”¨çš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–å’Œé€‚é…å™¨æŠ€æœ¯çš„å¼€å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®å—é™çš„ç¯å¢ƒä¸­ã€‚é€šè¿‡æ·±å…¥ç†è§£LoRAsçš„æœ‰æ•ˆæ€§ï¼Œå¯ä»¥ä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡æä¾›ç†è®ºæ”¯æŒï¼Œæ¨åŠ¨é€‚é…å™¨æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Merging or routing low-rank adapters (LoRAs) has emerged as a popular solution for enhancing large language models, particularly when data access is restricted by regulatory or domain-specific constraints. This position paper argues that the research community should shift its focus from developing new merging or routing algorithms to understanding the conditions under which reusing LoRAs is truly effective. Through theoretical analysis and synthetic two-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs enables genuine compositional generalization or merely reflects shallow pattern matching. Evaluating two data-agnostic methods--parameter averaging and dynamic adapter selection--we found that reusing LoRAs often fails to logically integrate knowledge across disjoint fine-tuning datasets, especially when such knowledge is underrepresented during pretraining. Our empirical results, supported by theoretical insights into LoRA's limited expressiveness, highlight the preconditions and constraints of reusing them for unseen tasks and cast doubt on its feasibility as a truly data-free approach. We advocate for pausing the pursuit of novel methods for recycling LoRAs and emphasize the need for rigorous mechanisms to guide future academic research in adapter-based model merging and practical system designs for practitioners.

