---
layout: default
title: Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text
---

# Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14012" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14012v1</a>
  <a href="https://arxiv.org/pdf/2506.14012.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14012v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14012v1', 'Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amr Mohamed, Yang Zhang, Michalis Vazirgiannis, Guokan Shang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä»£ç åˆ‡æ¢æ–‡æœ¬çš„ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç åˆ‡æ¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šè¯­è¨€å¤„ç†` `ç†è§£èƒ½åŠ›` `å¾®è°ƒæŠ€æœ¯` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä»£ç åˆ‡æ¢æ–‡æœ¬æ—¶ï¼ŒLLMsçš„ç†è§£èƒ½åŠ›å—åˆ°å¤–è¯­è¯æ±‡å¹²æ‰°çš„æ˜¾è‘—å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡é€šè¿‡ç”Ÿæˆä»£ç åˆ‡æ¢å˜ä½“çš„æ¨ç†å’Œç†è§£åŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°LLMsåœ¨å¤šè¯­è¨€è¾“å…¥ä¸‹çš„è¡¨ç°ï¼Œæ¢ç´¢å¾®è°ƒçš„æœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåµŒå…¥è‹±è¯­äºå…¶ä»–è¯­è¨€ä¸­èƒ½æ”¹å–„ç†è§£ï¼Œè€Œå¾®è°ƒæ–¹æ³•åœ¨é™çº§ç¼“è§£æ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„ç¨³å®šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»£ç åˆ‡æ¢ï¼ˆCSWï¼‰æ˜¯æŒ‡åœ¨å•ä¸€è¯è¯­ä¸­äº¤æ›¿ä½¿ç”¨ä¸¤ç§æˆ–å¤šç§è¯­è¨€çš„ç°è±¡ï¼Œå¹¿æ³›å­˜åœ¨äºå¤šè¯­è¨€ç¤¾åŒºä¸­ï¼Œå°¤å…¶æ˜¯åœ¨åœ¨çº¿å†…å®¹ä¸­ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å’Œç”Ÿæˆå†…å®¹æ—¶ï¼Œå¸¸å¸¸æ¥è§¦åˆ°ä»£ç åˆ‡æ¢çš„è¾“å…¥ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†LLMsåœ¨ä»£ç åˆ‡æ¢æƒ…å†µä¸‹çš„ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡ç”ŸæˆCSWå˜ä½“çš„æ¨ç†å’Œç†è§£åŸºå‡†è¿›è¡Œæµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å¤–è¯­è¯æ±‡å¹²æ‰°è‹±è¯­æ–‡æœ¬æ—¶ï¼Œç†è§£èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œè€Œå°†è‹±è¯­åµŒå…¥å…¶ä»–è¯­è¨€ä¸­é€šå¸¸èƒ½æé«˜ç†è§£èƒ½åŠ›ã€‚å°½ç®¡æç¤ºæ–¹æ³•æ•ˆæœä¸ä¸€ï¼Œå¾®è°ƒåˆ™æä¾›äº†æ›´ç¨³å®šçš„é™çº§ç¼“è§£è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä»£ç åˆ‡æ¢æ–‡æœ¬æ—¶çš„ç†è§£èƒ½åŠ›ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¤–è¯­è¯æ±‡å¯¹è‹±è¯­æ–‡æœ¬çš„å¹²æ‰°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç”Ÿæˆä»£ç åˆ‡æ¢çš„å˜ä½“ï¼Œè®ºæ–‡è¯„ä¼°LLMsåœ¨å¤šè¯­è¨€è¾“å…¥ä¸‹çš„ç†è§£èƒ½åŠ›ï¼Œæ¢ç´¢ä¸åŒè¾“å…¥å½¢å¼å¯¹æ¨¡å‹è¡¨ç°çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å¾®è°ƒçš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆæ„å»ºäº†åŒ…å«ä»£ç åˆ‡æ¢çš„æ¨ç†å’Œç†è§£åŸºå‡†ï¼Œç„¶åå¯¹LLMsè¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œæ¯”è¾ƒä¸åŒè¾“å…¥å½¢å¼ä¸‹çš„è¡¨ç°ï¼Œæœ€ååˆ†æå¾®è°ƒå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMsåœ¨ä»£ç åˆ‡æ¢æ–‡æœ¬ä¸‹çš„ç†è§£èƒ½åŠ›ï¼Œå¹¶æå‡ºå¾®è°ƒä½œä¸ºä¸€ç§æœ‰æ•ˆçš„é™çº§ç¼“è§£ç­–ç•¥ï¼Œæ˜¾è‘—åŒºåˆ«äºä»¥å¾€çš„ç ”ç©¶æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†å¤šç§è¯­è¨€æ··åˆçš„è¾“å…¥å½¢å¼ï¼Œå¹¶é€šè¿‡å¾®è°ƒæŠ€æœ¯ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œä»¥æé«˜å…¶åœ¨ä»£ç åˆ‡æ¢æ–‡æœ¬ä¸‹çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å¤–è¯­è¯æ±‡å¹²æ‰°è‹±è¯­æ–‡æœ¬æ—¶ï¼ŒLLMsçš„ç†è§£èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œé™å¹…å¯è¾¾20%ã€‚è€Œå°†è‹±è¯­åµŒå…¥å…¶ä»–è¯­è¨€ä¸­ï¼Œç†è§£èƒ½åŠ›æå‡å¹…åº¦å¯è¾¾15%ã€‚å¾®è°ƒæ–¹æ³•åœ¨é™çº§ç¼“è§£æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§ï¼Œæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€èŠå¤©æœºå™¨äººã€ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æå’Œè·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚éšç€å…¨çƒåŒ–è¿›ç¨‹çš„åŠ å¿«ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä»£ç åˆ‡æ¢æ–‡æœ¬çš„æ¨¡å‹å°†æå¤§æå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæµç•…æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\unicode{x2013}$even under linguistic constraints$\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.

