---
layout: default
title: BOW: Reinforcement Learning for Bottlenecked Next Word Prediction
---

# BOW: Reinforcement Learning for Bottlenecked Next Word Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13502" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13502v2</a>
  <a href="https://arxiv.org/pdf/2506.13502.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13502v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13502v2', 'BOW: Reinforcement Learning for Bottlenecked Next Word Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ming Shen, Zhikun Xu, Jacob Dineen, Xiao Ye, Ben Zhou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16 (æ›´æ–°: 2025-09-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBOWæ–¹æ³•ä»¥è§£å†³è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸‹ä¸€ä¸ªè¯é¢„æµ‹` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `è‡ªç„¶è¯­è¨€å¤„ç†` `è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹æ–¹æ³•åœ¨æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨¡å‹ç”Ÿæˆçš„å†…å®¹æµç•…ä½†ç¼ºä¹æ·±åº¦ã€‚
2. æœ¬æ–‡æå‡ºäº†BOWæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ä¸­é—´æ¨ç†ç“¶é¢ˆï¼Œè¿«ä½¿æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ä¹‹å‰ç”Ÿæˆæ¨ç†è½¨è¿¹ï¼Œä»è€Œå¢å¼ºæ¨ç†èƒ½åŠ›ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒBOWæ–¹æ³•åœ¨é›¶-shotæ¨ç†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒå¼ºçš„æŒç»­é¢„è®­ç»ƒåŸºçº¿æå‡è¿‘5%ï¼Œå¹¶åœ¨10ä¸ªè¯„ä¼°ä¸­è·å¾—7ä¸ªæœ€ä½³ç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸é€šè¿‡ä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼ˆNWPï¼‰è¿›è¡Œé¢„è®­ç»ƒï¼Œè™½ç„¶åœ¨è¡¨é¢æµç•…æ€§ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å¯¹æ¨¡å‹è¿›è¡Œæ˜ç¡®æ¨ç†çš„å‹åŠ›æœ‰é™ã€‚æœ¬æ–‡ç ”ç©¶äº†é€šè¿‡æ”¹å˜ç›‘ç£ä¿¡å·æ˜¯å¦èƒ½æ›´å¥½åœ°å¼•å¯¼æ˜ç¡®æ¨ç†ï¼Œå¹¶å¢å¼ºæ¨¡å‹çš„æ•´ä½“æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ç“¶é¢ˆä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼ˆBOWï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†ä¸­é—´æ¨ç†ç“¶é¢ˆæ’å…¥çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å½¢å¼çš„NWPã€‚æ¨¡å‹é¦–å…ˆç”Ÿæˆä¸‹ä¸€ä¸ªè¯çš„æ¨ç†è½¨è¿¹ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªå†»ç»“çš„è¯„åˆ†å™¨ä¸ºè¯¥è½¨è¿¹åˆ†é…è½¯çš„åˆ†å¸ƒå¼å¥–åŠ±ï¼Œä»¥æŒ‡å¯¼RLä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒBOWåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†é›¶-shotæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨10ä¸ªå†…åœ¨NWPè¯„ä¼°ä¸­å–å¾—äº†7ä¸ªæœ€ä½³ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸‹ä¸€ä¸ªè¯é¢„æµ‹ä¸­ç¼ºä¹æ˜ç¡®æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨ç”Ÿæˆæµç•…çš„æ–‡æœ¬ï¼Œè€Œå¿½è§†äº†æ¨ç†èƒ½åŠ›çš„åŸ¹å…»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBOWæ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ä¸ªæ¨ç†ç“¶é¢ˆï¼Œè¦æ±‚æ¨¡å‹åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ªè¯ä¹‹å‰ï¼Œé¦–å…ˆç”Ÿæˆä¸€ä¸ªæ¨ç†è½¨è¿¹ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¿ƒä½¿å…¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„æ€è€ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBOWçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ”¿ç­–æ¨¡å‹å’Œè¯„åˆ†å™¨ã€‚æ”¿ç­–æ¨¡å‹è´Ÿè´£ç”Ÿæˆæ¨ç†è½¨è¿¹ï¼Œè€Œå†»ç»“çš„è¯„åˆ†å™¨åˆ™æ ¹æ®è¯¥è½¨è¿¹ä¸ºæ¨¡å‹æä¾›å¥–åŠ±ä¿¡å·ï¼ŒæŒ‡å¯¼å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šBOWçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†æ¨ç†è¿‡ç¨‹å¼•å…¥ä¸‹ä¸€ä¸ªè¯é¢„æµ‹ä¸­ï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»Ÿçš„ç›´æ¥é¢„æµ‹æ–¹æ³•ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œæ›´ä¸ºå¤æ‚çš„æ¨ç†ï¼Œä»è€Œæå‡æ•´ä½“æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¥–åŠ±è®¾è®¡ä¸Šï¼ŒBOWé‡‡ç”¨äº†è½¯åˆ†å¸ƒå¼å¥–åŠ±æœºåˆ¶ï¼ŒåŸºäºæ¨ç†è½¨è¿¹çš„æ¦‚ç‡æ¥æŒ‡å¯¼ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§å¯é€‰çš„L1é£æ ¼æ­£åˆ™åŒ–ï¼Œä»¥é˜²æ­¢æ¨¡å‹é‡‡ç”¨ç®€å•çš„â€œå‘½åç­”æ¡ˆâ€ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBOWæ–¹æ³•åœ¨10ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¾ƒå¼ºçš„æŒç»­é¢„è®­ç»ƒåŸºçº¿å¹³å‡æå‡è¿‘5%ã€‚åœ¨10ä¸ªå†…åœ¨NWPè¯„ä¼°ä¸­ï¼ŒBOWå–å¾—äº†7ä¸ªæœ€ä½³ç»“æœï¼Œè¡¨æ˜å…¶åœ¨é›¶-shotæ¨ç†èƒ½åŠ›ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BOWæ–¹æ³•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ·±åº¦æ¨ç†çš„ä»»åŠ¡ä¸­ï¼Œå¦‚é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç”Ÿæˆå’Œæ–‡æœ¬æ‘˜è¦ç­‰ã€‚é€šè¿‡å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒBOWæœ‰æœ›æå‡è¿™äº›åº”ç”¨çš„æ™ºèƒ½æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are typically pretrained with next-word prediction (NWP), which yields strong surface fluency but places limited pressure on models to form explicit reasoning before emitting tokens. We study whether shifting the supervision signal can better elicit explicit reasoning and, more broadly, strengthen models' general reasoning capability. We present BOttlenecked next-Word prediction (BOW), a RL formulation of NWP that inserts an intermediate reasoning bottleneck. Instead of predicting the next word directly from context, the policy model must first generate a next-word reasoning trajectory. A frozen scorer then assigns this trajectory a soft, distributional reward equal to the probability of the gold next token conditioned solely on the trajectory to guide the RL optimization. We also propose an optional L1-style regularizer on the reward to discourage "name-the-answer" shortcuts. Across ten benchmarks, a brief BOW adaptation phase on Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct improves zero-shot reasoning and outperforms strong continual-pretraining baselines, including an RL variant with a hard, binary reward and a supervised finetuning approach with augmented data, by nearly 5% on average, while achieving the top result in 7 of 10 intrinsic NWP evaluations. These results indicate that BOW is a viable alternative to vanilla NWP, inducing explicit next-word reasoning and strengthening general reasoning ability.

