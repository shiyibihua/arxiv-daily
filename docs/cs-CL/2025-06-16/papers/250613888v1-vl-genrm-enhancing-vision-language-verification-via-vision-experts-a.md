---
layout: default
title: VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training
---

# VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13888" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13888v1</a>
  <a href="https://arxiv.org/pdf/2506.13888.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13888v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13888v1', 'VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jipeng Zhang, Kehao Miao, Renjie Pi, Zhaowei Wang, Runtao Liu, Rui Pan, Tong Zhang

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVL-GenRMä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `è¿­ä»£è®­ç»ƒ` `æ¨¡æ€åå·®` `å¤šæ¨¡æ€æ¨ç†` `æ•°æ®ç²¾ç‚¼` `æ€ç»´é“¾æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•é¢ä¸´æ•°æ®è´¨é‡å’Œåè§å¾ªç¯çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è¿­ä»£è®­ç»ƒæ¡†æ¶ï¼Œç»“åˆè§†è§‰ä¸“å®¶å’Œæ€ç»´é“¾æ¨ç†ï¼Œæ—¨åœ¨æå‡è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªVL-RMåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹»è§‰æ£€æµ‹å’Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åœ¨å¯éªŒè¯å¥–åŠ±æ–¹é¢æ¨åŠ¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œä½†åœ¨è§†è§‰è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹ä¸­ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è§†è§‰è¯­è¨€å¥–åŠ±æ¨¡å‹ï¼ˆVL-RMï¼‰æ˜¯å¯¹é½VLæ¨¡å‹çš„å…³é”®ï¼Œä½†è®­ç»ƒæœ‰æ•ˆçš„VL-RMé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šé¦–å…ˆï¼Œä¼˜è´¨è®­ç»ƒæ•°æ®ä¾èµ–äºå·²ç»å¼ºå¤§çš„VLæ¨¡å‹ï¼Œå¯¼è‡´è‡ªç”Ÿæˆç›‘ç£å¼ºåŒ–ç°æœ‰åè§ï¼›å…¶æ¬¡ï¼Œå½“VLæ¨¡å‹äº§ç”Ÿé”™è¯¯çš„è§†è§‰å±æ€§æ—¶ï¼Œä¼šå‡ºç°æ¨¡æ€åå·®å’Œè´Ÿä¾‹æ”¾å¤§ï¼Œè¿›ä¸€æ­¥è¯¯å¯¼è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨è§†è§‰ä¸“å®¶ã€æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å’ŒåŸºäºè¾¹é™…çš„æ‹’ç»é‡‡æ ·çš„è¿­ä»£è®­ç»ƒæ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¹»è§‰æ£€æµ‹å’Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œæ¨åŠ¨äº†VLæ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ çš„å¯¹é½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„æ•°æ®è´¨é‡å’Œåè§å¾ªç¯é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¼ºå¤§çš„VLæ¨¡å‹ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹åè§çš„è‡ªæˆ‘å¼ºåŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„è¿­ä»£è®­ç»ƒæ¡†æ¶é€šè¿‡å¼•å…¥è§†è§‰ä¸“å®¶å’Œæ€ç»´é“¾æ¨ç†ï¼Œä¼˜åŒ–è®­ç»ƒæ•°æ®å’Œåé¦ˆæœºåˆ¶ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¯¹é½æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€åå¥½æ•°æ®ç²¾ç‚¼ã€ç»“æ„åŒ–æ‰¹è¯„å’Œè¿­ä»£è®­ç»ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é€šè¿‡ä¸æ–­è¿­ä»£ï¼Œé€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆè§†è§‰ä¸“å®¶çš„åé¦ˆå’Œæ€ç»´é“¾æ¨ç†ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å­˜åœ¨çš„æ¨¡æ€åå·®å’Œè´Ÿä¾‹æ”¾å¤§çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºè¾¹é™…çš„æ‹’ç»é‡‡æ ·æŠ€æœ¯ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­æ›´å…·é²æ£’æ€§ã€‚é€šè¿‡ç²¾ç»†åŒ–çš„å‚æ•°è®¾ç½®ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªVL-RMåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€ææ–¹æ³•åœ¨å¹»è§‰æ£€æµ‹å’Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©ç†ã€è‡ªåŠ¨å›¾åƒæè¿°ç”Ÿæˆå’Œå¤šæ¨¡æ€æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´å‡†ç¡®çš„ç»“æœï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.

