---
layout: default
title: Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization
---

# Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13541" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.13541v1</a>
  <a href="https://arxiv.org/pdf/2506.13541.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13541v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13541v1', 'Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Guanghui Song, Dongping Liao, Yiren Zhao, Kejiang Ye, Cheng-zhong Xu, Xitong Gao

**ÂàÜÁ±ª**: cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫mixSGA‰ª•Ëß£ÂÜ≥TransformerÊ®°ÂûãÂä®ÊÄÅKV‰ºòÂåñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Transformer` `Âä®ÊÄÅ‰ºòÂåñ` `Ê∑∑Âêà‰∏ìÂÆ∂` `ÈîÆÂÄºÁºìÂ≠ò` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `ËµÑÊ∫êÂàÜÈÖç` `Ê®°ÂûãÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Âä®ÊÄÅËµÑÊ∫êÂàÜÈÖç‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜtokenÁöÑÈáçË¶ÅÊÄßÂèòÂåñÔºåÂØºËá¥ËµÑÊ∫êÊµ™Ë¥π„ÄÇ
2. mixSGAÈÄöËøáÂä®ÊÄÅË∑ØÁî±Êú∫Âà∂ÂíåÊùÉÈáçÂÖ±‰∫´Á≠ñÁï•Ôºå‰ºòÂåñtokenÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òÂàÜÈÖçÔºåÈÅøÂÖç‰∫Ü‰Ωé‰ºòÂÖàÁ∫ßtokenÁöÑ‰∏¢ÂºÉ„ÄÇ
3. Âú®Llama3„ÄÅTinyLlamaÁ≠âÊ®°Âûã‰∏äÔºåmixSGAÂú®Êåá‰ª§Ë∑üÈöèÂíåÊåÅÁª≠È¢ÑËÆ≠ÁªÉ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòË∂äÔºåROUGE-LÊõ¥È´ò‰∏îÂõ∞ÊÉëÂ∫¶Êõ¥‰Ωé„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

TransformerÊ®°ÂûãÂú®Âõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°‰∏≠Èù¢‰∏¥ÂèØÊâ©Â±ïÊÄßÊåëÊàòÔºå‰∏ªË¶ÅÁî±‰∫éÂØπ‰∏çÊñ≠Â¢ûÈïøÁöÑÈîÆÂÄº(KV)ÁºìÂ≠òÁöÑÂÜÖÂ≠òÂàÜÈÖçÊïàÁéá‰Ωé‰∏ãÔºåÂØºËá¥ËÆ°ÁÆóÂíåÂ≠òÂÇ®ËµÑÊ∫êÁ¥ßÂº†„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇÂàÜÁªÑÊü•ËØ¢Ê≥®ÊÑèÂäõ(GQA)ÂíåÂü∫‰∫étokenÁöÑKV‰ºòÂåñËôΩÁÑ∂ÊèêÈ´ò‰∫ÜÊïàÁéáÔºå‰ΩÜ‰æùËµñ‰∫éÂàöÊÄßÁöÑËµÑÊ∫êÂàÜÈÖçÔºåÂ∏∏Â∏∏‰∏¢ÂºÉ‚Äú‰Ωé‰ºòÂÖàÁ∫ß‚ÄùtokenÊàñÈùôÊÄÅÂàÜÁªÑÔºåÊú™ËÉΩÊúâÊïàÂ∫îÂØπtokenÈáçË¶ÅÊÄßÁöÑÂä®ÊÄÅÂèòÂåñ„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∑∑Âêà‰∏ìÂÆ∂(moE)ÊñπÊ≥ïmixSGAÔºåËÉΩÂ§üÂä®ÊÄÅ‰ºòÂåñtokenÁ∫ßÂà´ÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òÂàÜÈÖç„ÄÇ‰∏é‰ª•ÂæÄÊñπÊ≥ï‰∏çÂêåÔºåmixSGA‰øùÁïôÊâÄÊúâtokenÔºåÂπ∂Ê†πÊçÆÂ≠¶‰π†Âà∞ÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞Ëá™ÈÄÇÂ∫îÂú∞Â∞ÜÂÖ∂Ë∑ØÁî±Âà∞ÂÖ∑Êúâ‰∏çÂêåKVÁªÑÂ§ßÂ∞èÁöÑ‰∏ì‰∏ö‰∏ìÂÆ∂Ôºå‰ªéËÄåÂú®Á≤íÂ∫¶ÂíåÊïàÁéá‰πãÈó¥ÂèñÂæóÂπ≥Ë°°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥TransformerÊ®°ÂûãÂú®Âõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°‰∏≠Âõ†KVÁºìÂ≠òÂÜÖÂ≠òÂàÜÈÖçÊïàÁéá‰Ωé‰∏ãËÄåÂØºËá¥ÁöÑÂèØÊâ©Â±ïÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇGQAÂíåtokenÁ∫ßKV‰ºòÂåñÂú®ËµÑÊ∫êÂàÜÈÖç‰∏äËøá‰∫éÂàöÊÄßÔºåÂ∏∏Â∏∏‰∏¢ÂºÉ‰Ωé‰ºòÂÖàÁ∫ßtokenÔºåÊó†Ê≥ïÈÄÇÂ∫îtokenÈáçË¶ÅÊÄßÁöÑÂä®ÊÄÅÂèòÂåñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºömixSGAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂä®ÊÄÅ‰ºòÂåñtokenÁ∫ßÂà´ÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òÂàÜÈÖçÔºå‰øùÁïôÊâÄÊúâtokenÔºåÂπ∂Ê†πÊçÆÂ≠¶‰π†Âà∞ÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞Ëá™ÈÄÇÂ∫îÂú∞Â∞ÜÂÖ∂Ë∑ØÁî±Âà∞‰∏çÂêåÁöÑ‰∏ìÂÆ∂Ôºå‰ªéËÄåÂÆûÁé∞ËµÑÊ∫êÁöÑÊúâÊïàÂà©Áî®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºömixSGAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™token-wise‰∏ìÂÆ∂ÈÄâÊã©Ë∑ØÁî±Êú∫Âà∂„ÄÅÊùÉÈáçÂÖ±‰∫´ÁöÑÂàÜÁªÑÊ≥®ÊÑèÂäõÊäïÂΩ±Ê®°ÂùóÔºå‰ª•Âèä‰∏Ä‰∏™ËæÖÂä©ÊçüÂ§±ÂáΩÊï∞‰ª•Á°Æ‰øùËÆ≠ÁªÉÂíåÊé®ÁêÜÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºömixSGAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂä®ÊÄÅË∑ØÁî±Êú∫Âà∂ÂíåÊùÉÈáçÂÖ±‰∫´Á≠ñÁï•ÔºåËÉΩÂ§üÂú®‰∏ç‰∏¢ÂºÉtokenÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞ËµÑÊ∫êÁöÑÂä®ÊÄÅÂàÜÈÖçÔºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÁÅµÊ¥ªÊÄßÂíåÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏äÔºåmixSGAÈááÁî®‰∫ÜÂü∫‰∫éÂ≠¶‰π†ÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞ÁöÑË∑ØÁî±Êú∫Âà∂ÔºåÁ°Æ‰øù‰∫ÜËµÑÊ∫êÁöÑÊØî‰æãÂàÜÈÖçÔºõÂêåÊó∂ÔºåÈÄöËøáÂàÜÁªÑÊ≥®ÊÑèÂäõÁöÑÊùÉÈáçÂÖ±‰∫´ÔºåÈôç‰Ωé‰∫ÜÂèÇÊï∞ÂºÄÈîÄ„ÄÇÊ≠§Â§ñÔºåËæÖÂä©ÊçüÂ§±ÂáΩÊï∞ÁöÑÂºïÂÖ•Á°Æ‰øù‰∫ÜËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑ‰∏ÄËá¥ÊÄßÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÁ®≥ÂÆöÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™Ê®°ÂûãÔºàÂ¶ÇLlama3„ÄÅTinyLlama„ÄÅOPTÂíåGemma2ÔºâÁöÑËØÑ‰º∞‰∏≠ÔºåmixSGAÁõ∏ËæÉ‰∫éÈùôÊÄÅÂü∫Á∫øË°®Áé∞Âá∫ÊòæËëó‰ºòÂäø„ÄÇÂú®Êåá‰ª§Ë∑üÈöèÂíåÊåÅÁª≠È¢ÑËÆ≠ÁªÉ‰ªªÂä°‰∏≠ÔºåmixSGAÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑROUGE-LÂàÜÊï∞ÂíåÊõ¥‰ΩéÁöÑÂõ∞ÊÉëÂ∫¶ÔºåÊòæÁ§∫Âá∫Âú®Áõ∏ÂêåKVÈ¢ÑÁÆó‰∏ãÁöÑ‰ºòË∂äÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÂØπËØùÁ≥ªÁªüÂíåÊô∫ËÉΩÂä©ÊâãÁ≠âÔºåËÉΩÂ§üÊúâÊïàÊèêÂçáÊ®°ÂûãÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îËÉΩÂäõÂíåËµÑÊ∫êÂà©Áî®ÊïàÁéá„ÄÇÊú™Êù•ÔºåmixSGAÊúâÊúõÂú®Êõ¥Â§ßËßÑÊ®°ÁöÑËØ≠Ë®ÄÊ®°Âûã‰∏≠Êé®ÂπøÂ∫îÁî®ÔºåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Transformer models face scalability challenges in causal language modeling (CLM) due to inefficient memory allocation for growing key-value (KV) caches, which strains compute and storage resources. Existing methods like Grouped Query Attention (GQA) and token-level KV optimization improve efficiency but rely on rigid resource allocation, often discarding "low-priority" tokens or statically grouping them, failing to address the dynamic spectrum of token importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that dynamically optimizes token-wise computation and memory allocation. Unlike prior approaches, mixSGA retains all tokens while adaptively routing them to specialized experts with varying KV group sizes, balancing granularity and efficiency. Our key novelties include: (1) a token-wise expert-choice routing mechanism guided by learned importance scores, enabling proportional resource allocation without token discard; (2) weight-sharing across grouped attention projections to minimize parameter overhead; and (3) an auxiliary loss to ensure one-hot routing decisions for training-inference consistency in CLMs. Extensive evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show mixSGA's superiority over static baselines. On instruction-following and continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower perplexity under the same KV budgets.

