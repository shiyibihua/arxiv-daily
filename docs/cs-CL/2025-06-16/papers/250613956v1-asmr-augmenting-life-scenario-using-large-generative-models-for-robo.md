---
layout: default
title: ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection
---

# ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13956" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13956v1</a>
  <a href="https://arxiv.org/pdf/2506.13956.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13956v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13956v1', 'ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shang-Chi Tsai, Seiya Kawano, Angel Garcia Contreras, Koichiro Yoshino, Yun-Nung Chen

**åˆ†ç±»**: cs.CL, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

**å¤‡æ³¨**: IWSDS 2024 Best Paper Award

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°æ¡†æ¶ä»¥å¢å¼ºæœºå™¨äººå¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åˆ†ç±»` `æ•°æ®å¢å¼º` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¨³å®šæ‰©æ•£æ¨¡å‹` `æœºå™¨äººæŠ€æœ¯` `äººæœºäº¤äº’` `æ„å›¾ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ”¶é›†åŒ…å«è§†è§‰å’Œè¯­è¨€å…ƒç´ çš„å¤§è§„æ¨¡æ•°æ®é›†æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒæ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å’Œç¨³å®šæ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°æ®å¢å¼ºï¼Œä»¥ç”Ÿæˆå¯¹è¯å’Œç¯å¢ƒå›¾åƒï¼Œæå‡æœºå™¨äººå¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®åœºæ™¯æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æœºå™¨äººçš„è¡ŒåŠ¨é€‰æ‹©èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è®¾è®¡ç”¨äºæ—¥å¸¸äººç±»æ´»åŠ¨çš„æœºå™¨äººæ—¶ï¼Œå¢å¼ºç”¨æˆ·è¯·æ±‚çš„è§†è§‰çº¿ç´¢å¯¹äºæé«˜æ„å›¾ç†è§£è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œä¸“æ³¨äºåœ¨æœºå™¨äººè¾…åŠ©åœºæ™¯ä¸­è¿›è¡Œæ•°æ®å¢å¼ºï¼Œæ¶µç›–å¯¹è¯å’Œç›¸å…³ç¯å¢ƒå›¾åƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿæ½œåœ¨å¯¹è¯å’Œç¯å¢ƒä¸Šä¸‹æ–‡ï¼Œå¹¶ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆæç»˜è¿™äº›ç¯å¢ƒçš„å›¾åƒã€‚ç”Ÿæˆçš„æ•°æ®ç”¨äºä¼˜åŒ–æœ€æ–°çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ ¹æ®æœ‰é™çš„ç›®æ ‡æ•°æ®ç¡®å®šé€‚å½“çš„è¡ŒåŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æœºå™¨äººçš„è¡ŒåŠ¨é€‰æ‹©èƒ½åŠ›ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨æ—¥å¸¸äººç±»æ´»åŠ¨ä¸­å¯¹ç”¨æˆ·æ„å›¾ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ”¶é›†å’Œåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒæ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ•°æ®å¢å¼ºæ¥æå‡æœºå™¨äººå¯¹ç”¨æˆ·è¯·æ±‚çš„ç†è§£èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ½œåœ¨å¯¹è¯ï¼Œå¹¶ç»“åˆç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆç›¸å…³ç¯å¢ƒå›¾åƒï¼Œä»è€Œä¸°å¯Œè®­ç»ƒæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿç”¨æˆ·ä¸æœºå™¨äººä¹‹é—´çš„å¯¹è¯ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸å¯¹è¯ç›¸å…³çš„ç¯å¢ƒå›¾åƒã€‚ç”Ÿæˆçš„æ•°æ®å°†ç”¨äºè®­ç»ƒå’Œä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ç”Ÿæˆçš„å¤šæ¨¡æ€æ•°æ®æ¥å¢å¼ºæœºå™¨äººå¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ•°æ®æ”¶é›†æ–¹å¼ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†æ•°æ®å‡†å¤‡çš„æ—¶é—´å’Œæˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡è¯­è¨€å’Œè§†è§‰ä¿¡æ¯çš„è´¡çŒ®ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿ç”Ÿæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨çœŸå®åœºæ™¯æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æœºå™¨äººçš„è¡ŒåŠ¨é€‰æ‹©èƒ½åŠ›ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—è¾…åŠ©æœºå™¨äººä»¥åŠæ™ºèƒ½å®¶å±…ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„äººæœºäº¤äº’ä½“éªŒï¼Œå¢å¼ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œå®ç”¨æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.

