---
layout: default
title: Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study
---

# Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13464" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13464v2</a>
  <a href="https://arxiv.org/pdf/2506.13464.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13464v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13464v2', 'Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengyu Hu, Jianxun Lian, Zheyuan Xiao, Seraphina Zhang, Tianfu Wang, Nicholas Jing Yuan, Xing Xie, Hui Xiong

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16 (æ›´æ–°: 2025-10-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè®¤çŸ¥æ¡†æ¶ä»¥æ­ç¤ºè¯­è¨€æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `å­¦ä¹ èƒ½åŠ›` `è®¤çŸ¥å¿ƒç†å­¦` `æ•™è‚²æŠ€æœ¯` `å®è¯ç ”ç©¶` `é€‚åº”æ€§æ¨¡å‹` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å­¦ä¹ èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œå°¤å…¶æ˜¯åœ¨çŸ¥è¯†è·å–å’Œé€‚åº”æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶å°†å­¦ä¹ èƒ½åŠ›åˆ†è§£ä¸ºä¸‰ä¸ªç»´åº¦ï¼Œåˆ†åˆ«æ˜¯ä»æŒ‡å¯¼è€…ã€æ¦‚å¿µå’Œç»éªŒä¸­å­¦ä¹ ï¼Œä»¥æ›´å…¨é¢åœ°ç†è§£LLMsçš„å­¦ä¹ è¿‡ç¨‹ã€‚
3. å®è¯ç ”ç©¶è¡¨æ˜ï¼Œäº’åŠ¨èƒ½æ˜¾è‘—æå‡å­¦ä¹ æ•ˆæœï¼Œä¸”è¾ƒå¤§æ¨¡å‹åœ¨æ¦‚å¿µç†è§£ä¸Šè¡¨ç°æ›´ä½³ï¼ŒLLMsåœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ä¸­æ•ˆæœçªå‡ºï¼Œä½†åœ¨å¤šæ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°ä¸ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦ã€ç¼–ç å’Œæ¨ç†ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶å­¦ä¹ èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥ä¸€ä¸ªå—è®¤çŸ¥å¿ƒç†å­¦å’Œæ•™è‚²å¯å‘çš„æ¡†æ¶ï¼Œè§£å†³äº†è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬å°†ä¸€èˆ¬å­¦ä¹ èƒ½åŠ›åˆ†è§£ä¸ºä¸‰ä¸ªäº’è¡¥çš„ç»´åº¦ï¼šä»æŒ‡å¯¼è€…å­¦ä¹ ã€ä»æ¦‚å¿µå­¦ä¹ å’Œä»ç»éªŒå­¦ä¹ ã€‚é€šè¿‡å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°äº’åŠ¨èƒ½æå‡å­¦ä¹ æ•ˆæœï¼Œæ¦‚å¿µç†è§£æ˜¯è§„æ¨¡çªç°çš„ï¼Œä¸”LLMsåœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤šæ ·æœ¬å­¦ä¹ ä¸­æ•ˆæœä¸ä½³ã€‚åŸºäºæ­¤æ¡†æ¶å’Œå®è¯å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºå‡†ï¼Œæä¾›å¯¹LLMsä¸€èˆ¬å­¦ä¹ èƒ½åŠ›çš„ç»Ÿä¸€å’Œç°å®è¯„ä¼°ï¼Œæ”¯æŒæ›´å…·é€‚åº”æ€§å’Œäººæ€§åŒ–æ¨¡å‹çš„è¯„ä¼°ä¸å¼€å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­å­¦ä¹ èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ¢è®¨å…¶é€‚åº”æ€§å’ŒçŸ¥è¯†è·å–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ä¸€ä¸ªåŸºäºè®¤çŸ¥å¿ƒç†å­¦çš„æ¡†æ¶ï¼Œå°†å­¦ä¹ èƒ½åŠ›åˆ†è§£ä¸ºä¸‰ä¸ªç»´åº¦ï¼Œå¸®åŠ©æ·±å…¥ç†è§£LLMsçš„å­¦ä¹ è¿‡ç¨‹åŠå…¶å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä»æŒ‡å¯¼è€…å­¦ä¹ ã€ä»æ¦‚å¿µå­¦ä¹ å’Œä»ç»éªŒå­¦ä¹ ã€‚æ¯ä¸ªæ¨¡å—ç‹¬ç«‹è¯„ä¼°LLMsåœ¨ä¸åŒå­¦ä¹ æƒ…å¢ƒä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å­¦ä¹ èƒ½åŠ›ç»†åˆ†ä¸ºä¸‰ä¸ªäº’è¡¥ç»´åº¦ï¼Œæä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’æ¥è¯„ä¼°å’Œç†è§£LLMsçš„å­¦ä¹ è¿‡ç¨‹ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ­ç¤ºå…¶å­¦ä¹ æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾è®¡äº†ä¸åŒçš„äº¤äº’æ–¹å¼å’Œåé¦ˆæœºåˆ¶ï¼Œä»¥è¯„ä¼°å…¶å¯¹å­¦ä¹ æ•ˆæœçš„å½±å“ï¼ŒåŒæ—¶é‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹åœ¨ä¸åŒå­¦ä¹ ç»´åº¦ä¸Šçš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œäº’åŠ¨æ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆæœï¼Œæ¦‚å¿µç†è§£èƒ½åŠ›åœ¨è¾ƒå¤§æ¨¡å‹ä¸­è¡¨ç°æ›´ä½³ã€‚æ­¤å¤–ï¼ŒLLMsåœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°20%ï¼Œè€Œåœ¨å¤šæ ·æœ¬å­¦ä¹ ä¸­æ•ˆæœä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºå…¶å­¦ä¹ èƒ½åŠ›çš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æ›´å¥½åœ°ç†è§£è¯­è¨€æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥å¼€å‘å‡ºæ›´å…·é€‚åº”æ€§å’Œäººæ€§åŒ–çš„æ™ºèƒ½ç³»ç»Ÿï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œå­¦ä¹ æ•ˆæœã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶ä¹Ÿå¯èƒ½æ¨åŠ¨å¯¹å…¶ä»–ç±»å‹äººå·¥æ™ºèƒ½æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ç ”ç©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have shown impressive capabilities across tasks such as mathematics, coding, and reasoning, yet their learning ability, which is crucial for adapting to dynamic environments and acquiring new knowledge, remains underexplored. In this work, we address this gap by introducing a framework inspired by cognitive psychology and education. Specifically, we decompose general learning ability into three distinct, complementary dimensions: Learning from Instructor (acquiring knowledge via explicit guidance), Learning from Concept (internalizing abstract structures and generalizing to new contexts), and Learning from Experience (adapting through accumulated exploration and feedback). We conduct a comprehensive empirical study across the three learning dimensions and identify several insightful findings, such as (i) interaction improves learning; (ii) conceptual understanding is scale-emergent and benefits larger models; and (iii) LLMs are effective few-shot learners but not many-shot learners. Based on our framework and empirical findings, we introduce a benchmark that provides a unified and realistic evaluation of LLMs' general learning abilities across three learning cognition dimensions. It enables diagnostic insights and supports evaluation and development of more adaptive and human-like models.

