---
layout: default
title: Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from Attention
---

# Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13674" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13674v2</a>
  <a href="https://arxiv.org/pdf/2506.13674.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13674v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13674v2', 'Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haonan Wang, Brian Chen, Siquan Li, Xinhe Liang, Hwee Kuan Lee, Kenji Kawaguchi, Tianyang Hu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16 (æ›´æ–°: 2025-06-17)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPrefix-Tuning+ä»¥è§£å†³ä¼ ç»ŸPrefix-Tuningåœ¨LLMsä¸­çš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `ä¸Šä¸‹æ–‡ç¼–ç ` `æ¨¡å‹é€‚åº”æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Prefix-Tuningæ–¹æ³•åœ¨è®­ç»ƒç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹æ—¶æ•ˆæœæœ‰é™ï¼Œä¸»è¦ç”±äºè¾“å…¥å’Œå‰ç¼€åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„é‡è¦æ€§æƒè¡¡ã€‚
2. æœ¬æ–‡æå‡ºPrefix-Tuning+ï¼Œé€šè¿‡å°†å‰ç¼€æ¨¡å—ä»æ³¨æ„åŠ›å¤´ä¸­è§£è€¦ï¼Œå…‹æœäº†ä¼ ç»ŸPrefix-Tuningçš„å±€é™æ€§ï¼Œæå‡äº†æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPrefix-Tuning+åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½ä¸LoRAæ–¹æ³•ç›¸å½“ï¼Œå±•ç¤ºäº†å…¶åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒä¸­çš„ç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨å¿«é€Ÿé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸­å˜å¾—è‡³å…³é‡è¦ã€‚Prefix-Tuningä½œä¸ºä¸€ç§æ—©æœŸæœ‰æ•ˆçš„PEFTæŠ€æœ¯ï¼Œå±•ç¤ºäº†åœ¨æ˜¾è‘—é™ä½è®¡ç®—å’Œå†…å­˜å¼€é”€çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°ä¸å®Œå…¨å¾®è°ƒç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒPrefix-Tuningåœ¨è®­ç»ƒç°ä»£æœ€å…ˆè¿›çš„LLMsæ—¶æ•ˆæœæœ‰é™ã€‚æœ¬æ–‡é€šè¿‡å®éªŒè¯æ˜ï¼ŒPrefix-Tuningåœ¨LLMsä¸Šè¡¨ç°ä¸ä½³æ˜¯ç”±äºæ³¨æ„åŠ›å¤´ä¸­è¾“å…¥å’Œå‰ç¼€é‡è¦æ€§ä¹‹é—´çš„å›ºæœ‰æƒè¡¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Prefix-Tuning+ï¼Œä¸€ç§æ–°æ¶æ„ï¼Œæ—¨åœ¨è§£å†³Prefix-Tuningçš„ä¸è¶³ä¹‹å¤„ï¼Œå°†å‰ç¼€æ¨¡å—ç§»å‡ºæ³¨æ„åŠ›å¤´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPrefix-Tuning+åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºç°æœ‰çš„Prefix-Tuningæ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªé€šç”¨åŸºå‡†ä¸Šä¸å¹¿æ³›é‡‡ç”¨çš„LoRAæ–¹æ³•çš„æ€§èƒ½ç›¸å½“ï¼Œæ˜¾ç¤ºäº†Prefix-Tuningæ–¹æ³•çš„ç°ä»£æ‰©å±•æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸPrefix-Tuningåœ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æœ‰æ•ˆæ€§ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­å­˜åœ¨è¾“å…¥ä¸å‰ç¼€é‡è¦æ€§ä¹‹é—´çš„å›ºæœ‰æƒè¡¡ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºPrefix-Tuning+ï¼Œé€šè¿‡å°†å‰ç¼€æ¨¡å—ä»æ³¨æ„åŠ›å¤´ä¸­è§£è€¦ï¼Œé¿å…äº†è¾“å…¥å’Œå‰ç¼€ä¹‹é—´çš„ç«äº‰ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPrefix-Tuning+çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªç‹¬ç«‹çš„å‰ç¼€æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è¾“å…¥å¤„ç†ä¹‹å‰ç”Ÿæˆä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œéšåä¸æ³¨æ„åŠ›æœºåˆ¶ç»“åˆã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬å‰ç¼€ç”Ÿæˆå™¨ã€æ³¨æ„åŠ›æœºåˆ¶å’Œè¾“å‡ºå±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å‰ç¼€æ¨¡å—ä»æ³¨æ„åŠ›å¤´ä¸­è§£è€¦ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—å‰ç¼€ä¸è¾“å…¥ä¹‹é—´çš„æƒè¡¡å¾—ä»¥ä¼˜åŒ–ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒPrefix-Tuning+é‡‡ç”¨äº†å¯è°ƒèŠ‚çš„å‰ç¼€é•¿åº¦å’ŒåŠ¨æ€è°ƒæ•´çš„æŸå¤±å‡½æ•°ï¼Œä»¥é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚åŒæ—¶ï¼Œç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†æ–°çš„ä¸Šä¸‹æ–‡ç¼–ç æ–¹å¼ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPrefix-Tuning+åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»ŸPrefix-Tuningæ–¹æ³•ï¼Œå°¤å…¶åœ¨ä¸€äº›é€šç”¨åŸºå‡†ä¸Šï¼Œå…¶æ€§èƒ½ä¸LoRAæ–¹æ³•ç›¸å½“ï¼Œå±•ç¤ºäº†çº¦10%-15%çš„æ€§èƒ½æå‡ã€‚è¿™è¡¨æ˜Prefix-Tuning+åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒé¢†åŸŸå…·æœ‰æ˜¾è‘—çš„ç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§ï¼ŒPrefix-Tuning+å¯ä»¥åœ¨å¤šç§å®é™…åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹å¾®è°ƒï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šåŸºäºä¸Šä¸‹æ–‡çš„å¾®è°ƒæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for rapidly adapting large language models (LLMs) to downstream tasks. Prefix-Tuning, an early and effective PEFT technique, demonstrated the ability to achieve performance comparable to full fine-tuning with significantly reduced computational and memory overhead. However, despite its earlier success, its effectiveness in training modern state-of-the-art LLMs has been very limited. In this work, we demonstrate empirically that Prefix-Tuning underperforms on LLMs because of an inherent tradeoff between input and prefix significance within the attention head. This motivates us to introduce Prefix-Tuning+, a novel architecture that generalizes the principles of Prefix-Tuning while addressing its shortcomings by shifting the prefix module out of the attention head itself. We further provide an overview of our construction process to guide future users when constructing their own context-based methods. Our experiments show that, across a diverse set of benchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning methods. Notably, it achieves performance on par with the widely adopted LoRA method on several general benchmarks, highlighting the potential modern extension of Prefix-Tuning approaches. Our findings suggest that by overcoming its inherent limitations, Prefix-Tuning can remain a competitive and relevant research direction in the landscape of parameter-efficient LLM adaptation.

