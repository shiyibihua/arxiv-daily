---
layout: default
title: TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices
---

# TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13514" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13514v1</a>
  <a href="https://arxiv.org/pdf/2506.13514.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13514v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13514v1', 'TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingxue Xu, Yao Lei Xu, Danilo P. Mandic

**åˆ†ç±»**: cs.CL, cs.LG, math.NA

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

**å¤‡æ³¨**: ICML 2025 Workshop on Tiny Titans: The next wave of On-Device Learning for Foundational Models (TTODLer-FM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTensorSLMä»¥è§£å†³ä½ç«¯è®¾å¤‡ä¸Šè¯­è¨€æ¨¡å‹èƒ½æ•ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°å‹è¯­è¨€æ¨¡å‹` `åµŒå…¥å‹ç¼©` `å¼ é‡åˆ†è§£` `è¾¹ç¼˜è®¡ç®—` `èƒ½æ•ˆä¼˜åŒ–` `ä½ç«¯è®¾å¤‡` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šåº”ç”¨æ—¶é¢ä¸´èƒ½æ•ˆå’Œé€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶åœ¨ç”µæ± å¯¿å‘½å—é™çš„æƒ…å†µä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒæ— å…³çš„åµŒå…¥å‹ç¼©æ–¹æ³•ï¼Œåˆ©ç”¨å¼ é‡-è®­ç»ƒåˆ†è§£å°†åµŒå…¥å‘é‡è½¬åŒ–ä¸ºä½ç»´è¡¨ç¤ºï¼Œä»¥æé«˜èƒ½æ•ˆå’Œé€‚åº”æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å‹ç¼©æ¯”å’Œèƒ½è€—ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸæ¨¡å‹ç›¸å½“çš„è¯­è¨€ä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ç›¸è¾ƒäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰æ›´å°‘çš„å‚æ•°ï¼Œé€‚åˆåœ¨ä½ç«¯è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ é‡åˆ†è§£çš„è®­ç»ƒæ— å…³çš„åµŒå…¥å‹ç¼©æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜SLMsåœ¨è¾¹ç¼˜åº”ç”¨ä¸­çš„é€‚åº”æ€§å’Œèƒ½æ•ˆã€‚é€šè¿‡å°†é¢„è®­ç»ƒçš„åµŒå…¥å‘é‡è½¬æ¢ä¸ºä½ç»´çŸ©é˜µä¹˜ç§¯çŠ¶æ€ï¼ˆMPSï¼‰ï¼Œæˆ‘ä»¬åœ¨Raspberry Piç­‰ä½ç«¯è®¾å¤‡ä¸Šè¯„ä¼°äº†å‹ç¼©æ¯”ã€è¯­è¨€ä»»åŠ¡æ€§èƒ½ã€å»¶è¿Ÿå’Œèƒ½è€—ã€‚ä»¥GPT-2å’ŒOPTæ¨¡å‹ä¸ºä¾‹ï¼Œæ‰€ææ–¹æ³•åœ¨ä¿æŒè¯­è¨€ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†çº¦2.0å€çš„åµŒå…¥å±‚å‹ç¼©ï¼Œå¹¶å°†å•æ¬¡æŸ¥è¯¢çš„èƒ½è€—é™ä½äº†ä¸€åŠã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹åœ¨ä½ç«¯è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶çš„èƒ½æ•ˆå’Œé€‚åº”æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸æœªèƒ½è€ƒè™‘è®¾å¤‡çš„ç”µæ± å¯¿å‘½é™åˆ¶ï¼Œå¯¼è‡´èƒ½è€—é«˜ä¸”é€‚åº”æ€§å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºå¼ é‡-è®­ç»ƒåˆ†è§£ï¼ˆTTDï¼‰çš„è®­ç»ƒæ— å…³åµŒå…¥å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡å°†é¢„è®­ç»ƒçš„åµŒå…¥å‘é‡è½¬åŒ–ä¸ºä½ç»´çŸ©é˜µä¹˜ç§¯çŠ¶æ€ï¼ˆMPSï¼‰ï¼Œä»¥å®ç°é«˜æ•ˆçš„åµŒå…¥è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒåµŒå…¥çš„ä½ç»´è½¬æ¢ã€ä½ç§©ç»“æ„çš„æå–åŠå…¶åœ¨ä½ç«¯è®¾å¤‡ä¸Šçš„è¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬åµŒå…¥å‹ç¼©æ¨¡å—å’Œæ€§èƒ½è¯„ä¼°æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åµŒå…¥å‹ç¼©æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å°å‹è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„èƒ½æ•ˆå’Œé€‚åº”æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„å‹ç¼©æ¯”å’Œæ›´ä½çš„èƒ½è€—ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ä½ç§©çŸ©é˜µä¹˜ç§¯çŠ¶æ€è¡¨ç¤ºï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºä¿æŒè¯­è¨€ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ä¼˜åŒ–èƒ½è€—ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™é‡‡ç”¨äº†é€‚åˆä½ç«¯è®¾å¤‡çš„è½»é‡çº§è®¾è®¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å‹ç¼©æ¯”ä¸Šè¾¾åˆ°äº†çº¦2.0å€ï¼ŒåŒæ—¶åœ¨è¯­è¨€ä»»åŠ¡æ€§èƒ½ä¸Šä¸åŸå§‹æ¨¡å‹ç›¸å½“ã€‚å•æ¬¡æŸ¥è¯¢çš„èƒ½è€—é™ä½äº†ä¸€åŠï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨èƒ½æ•ˆå’Œæ€§èƒ½ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨ç§»åŠ¨è®¾å¤‡ã€å•æ¿è®¡ç®—æœºç­‰ä½ç«¯è®¾å¤‡ä¸Šå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦é«˜æ•ˆèƒ½è€—å’Œå¿«é€Ÿå“åº”çš„è¾¹ç¼˜è®¡ç®—åœºæ™¯ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€å®æ—¶ç¿»è¯‘å’Œè¯­éŸ³è¯†åˆ«ç­‰ã€‚æœªæ¥ï¼Œéšç€è®¾å¤‡æ€§èƒ½çš„æå‡ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Small Language Models (SLMs, or on-device LMs) have significantly fewer parameters than Large Language Models (LLMs). They are typically deployed on low-end devices, like mobile phones and single-board computers. Unlike LLMs, which rely on increasing model size for better generalisation, SLMs designed for edge applications are expected to have adaptivity to the deployment environments and energy efficiency given the device battery life constraints, which are not addressed in datacenter-deployed LLMs. This paper addresses these two requirements by proposing a training-free token embedding compression approach using Tensor-Train Decomposition (TTD). Each pre-trained token embedding vector is converted into a lower-dimensional Matrix Product State (MPS). We comprehensively evaluate the extracted low-rank structures across compression ratio, language task performance, latency, and energy consumption on a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion parameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our approach achieves a comparable language task performance to the original model with around $2.0\times$ embedding layer compression, while the energy consumption of a single query drops by half.

