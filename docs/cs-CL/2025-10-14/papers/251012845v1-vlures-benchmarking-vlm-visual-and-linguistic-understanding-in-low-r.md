---
layout: default
title: VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages
---

# VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.12845" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.12845v1</a>
  <a href="https://arxiv.org/pdf/2510.12845.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.12845v1" onclick="toggleFavorite(this, '2510.12845v1', 'VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jesse Atuhurra, Iqra Ali, Tomoya Iwakura, Hidetaka Kamigaito, Tatsuya Hiraoka

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-14

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VLUResï¼šæå‡ºå¤šè¯­ç§è§†è§‰è¯­è¨€ç†è§£åŸºå‡†ï¼Œè¯„ä¼°ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹VLMçš„ç»†ç²’åº¦èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šè¯­ç§åŸºå‡†` `ä½èµ„æºè¯­è¨€` `é•¿æ–‡æœ¬ç†è§£` `ç»†ç²’åº¦è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMè¯„ä¼°ä¸»è¦é›†ä¸­äºè‹±è¯­ç¯å¢ƒï¼Œç¼ºä¹å¯¹é•¿æ–‡æœ¬å’Œä½èµ„æºè¯­è¨€çš„ç»†ç²’åº¦ç†è§£èƒ½åŠ›è¯„ä¼°ã€‚
2. VLUResåŸºå‡†åŒ…å«å…«ä¸ªè§†è§‰è¯­è¨€ä»»åŠ¡å’Œä¸€ä¸ªä¸ç›¸å…³æ€§ä»»åŠ¡ï¼Œè¦†ç›–è‹±è¯­ã€æ—¥è¯­ã€æ–¯ç“¦å¸Œé‡Œè¯­å’Œä¹Œå°”éƒ½è¯­ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯GPT-4oï¼Œåœ¨ä½èµ„æºè¯­è¨€å’Œå¤æ‚ä»»åŠ¡ä¸Šä¸äººç±»æ°´å¹³ä»æœ‰å·®è·ï¼Œå¼€æºæ¨¡å‹å·®è·æ›´å¤§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¯¹äºæå‡æ™ºèƒ½ä»£ç†çš„æ„ŸçŸ¥èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¯¹VLMçš„è¯„ä¼°ä»ç„¶ä¸»è¦é›†ä¸­åœ¨ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„åŸºå‡†ä¸Šï¼Œè¿™äº›åŸºå‡†ä¸­çš„å›¾åƒ-æ–‡æœ¬å¯¹åŒ…å«çš„æ–‡æœ¬è¾ƒçŸ­ã€‚ä¸ºäº†è¯„ä¼°VLMçš„ç»†ç²’åº¦èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨é•¿æ–‡æœ¬è®¾ç½®ä¸‹ï¼Œé’ˆå¯¹å››ç§è¯­è¨€ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„å¤šè¯­ç§åŸºå‡†VLUResï¼Œå…¶ä¸­åŒ…å«å…«ä¸ªè§†è§‰å’Œè¯­è¨€ä»»åŠ¡ï¼Œä»¥åŠä¸€ä¸ªå¼€åˆ›æ€§çš„ä¸ç›¸å…³æ€§ä»»åŠ¡ï¼Œä»¥æ¢æµ‹VLMåœ¨è‹±è¯­ã€æ—¥è¯­å’Œä½èµ„æºè¯­è¨€ï¼ˆæ–¯ç“¦å¸Œé‡Œè¯­å’Œä¹Œå°”éƒ½è¯­ï¼‰ä¸­çš„ç»†ç²’åº¦è§†è§‰å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ä»ç›®æ ‡è¯­è¨€çš„ç½‘ç»œèµ„æºä¸­æ”¶é›†ï¼ŒåŒ…å«åä¸ªä¸åŒçš„å›¾åƒç±»åˆ«å’Œä¸°å¯Œçš„æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œä¸ºæ–¯ç“¦å¸Œé‡Œè¯­å’Œä¹Œå°”éƒ½è¯­å¼•å…¥äº†æœ‰ä»·å€¼çš„è§†è§‰è¯­è¨€èµ„æºã€‚é€šè¿‡æç¤ºVLMç”Ÿæˆå“åº”å’Œç†ç”±ï¼Œå¹¶ç”±è‡ªåŠ¨è¯„ä¼°å’Œæ¯è¯­äººå£«è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°äº†æ™ºèƒ½ä»£ç†å…³é”®ä»»åŠ¡ï¼ˆå¦‚å¯¹è±¡è¯†åˆ«ã€åœºæ™¯ç†è§£å’Œå…³ç³»ç†è§£ï¼‰ä¸­ä¸åŒè¯­è¨€å’Œä»»åŠ¡ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚æˆ‘ä»¬ä½¿ç”¨VLUResè¯„ä¼°äº†åä¸ªVLMã€‚æ€§èƒ½æœ€ä½³çš„æ¨¡å‹GPT-4oå®ç°äº†90.8%çš„æ€»ä½“å‡†ç¡®ç‡ï¼Œæ¯”äººç±»æ€§èƒ½ä½6.7%ï¼Œä½†å¼€æºæ¨¡å‹çš„å·®è·æ›´å¤§ã€‚è¿™ä¸€å·®è·çªæ˜¾äº†VLUResåœ¨å¼€å‘æ™ºèƒ½ä»£ç†ä»¥è§£å†³å¤šæ¨¡æ€è§†è§‰æ¨ç†æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è‹±è¯­æ•°æ®é›†ä¸Šï¼Œå¹¶ä¸”é€šå¸¸ä½¿ç”¨çŸ­æ–‡æœ¬ã€‚è¿™ä½¿å¾—æˆ‘ä»¬éš¾ä»¥è¯„ä¼°VLMåœ¨é•¿æ–‡æœ¬å’Œä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹çš„ç»†ç²’åº¦è§†è§‰å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•æ— æ³•å……åˆ†æµ‹è¯•VLMåœ¨å¤æ‚åœºæ™¯ç†è§£ã€å…³ç³»æ¨ç†ä»¥åŠå¤„ç†ä¸åŒè¯­è¨€æ–‡åŒ–èƒŒæ™¯ä¸‹çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤šè¯­ç§ã€é•¿æ–‡æœ¬çš„è§†è§‰è¯­è¨€ç†è§£åŸºå‡†ï¼ˆVLUResï¼‰ï¼Œè¯¥åŸºå‡†åŒ…å«å¤šä¸ªä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°VLMåœ¨ä¸åŒè¯­è¨€å’Œä»»åŠ¡ä¸Šçš„ç»†ç²’åº¦èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ä½èµ„æºè¯­è¨€ï¼ˆæ–¯ç“¦å¸Œé‡Œè¯­å’Œä¹Œå°”éƒ½è¯­ï¼‰ï¼Œå¯ä»¥æ›´å¥½åœ°äº†è§£VLMåœ¨èµ„æºåŒ®ä¹æƒ…å†µä¸‹çš„è¡¨ç°ã€‚åŒæ—¶ï¼Œé•¿æ–‡æœ¬çš„å¼•å…¥å¯ä»¥æµ‹è¯•VLMå¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLUResåŸºå‡†åŒ…å«å…«ä¸ªè§†è§‰å’Œè¯­è¨€ä»»åŠ¡ï¼Œä»¥åŠä¸€ä¸ªä¸ç›¸å…³æ€§ä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡æ¶µç›–äº†å¯¹è±¡è¯†åˆ«ã€åœºæ™¯ç†è§£ã€å…³ç³»ç†è§£ç­‰å¤šä¸ªæ–¹é¢ã€‚æ•°æ®é›†çš„æ„å»ºä¸»è¦é€šè¿‡ä»ç›®æ ‡è¯­è¨€çš„ç½‘ç»œèµ„æºä¸­æ”¶é›†å›¾åƒå’Œæ–‡æœ¬ï¼Œå¹¶è¿›è¡Œäººå·¥æ ‡æ³¨å’ŒéªŒè¯ã€‚è¯„ä¼°è¿‡ç¨‹åŒ…æ‹¬æç¤ºVLMç”Ÿæˆå“åº”å’Œç†ç”±ï¼Œç„¶åä½¿ç”¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°ç›¸ç»“åˆçš„æ–¹å¼æ¥è¯„ä¼°VLMçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šVLUResåŸºå‡†çš„ä¸»è¦åˆ›æ–°ç‚¹åœ¨äºå…¶å¤šè¯­ç§å’Œé•¿æ–‡æœ¬çš„ç‰¹æ€§ã€‚å®ƒé¦–æ¬¡å°†ä½èµ„æºè¯­è¨€ï¼ˆæ–¯ç“¦å¸Œé‡Œè¯­å’Œä¹Œå°”éƒ½è¯­ï¼‰å¼•å…¥åˆ°VLMçš„è¯„ä¼°ä¸­ï¼Œå¹¶ä½¿ç”¨é•¿æ–‡æœ¬æ¥æµ‹è¯•VLMçš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒVLUResè¿˜å¼•å…¥äº†ä¸€ä¸ªä¸ç›¸å…³æ€§ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°VLMåŒºåˆ†ç›¸å…³å’Œä¸ç›¸å…³ä¿¡æ¯çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šVLUResåŸºå‡†çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ï¼Œä»¥å…¨é¢è¯„ä¼°VLMçš„ç»†ç²’åº¦èƒ½åŠ›ï¼›2ï¼‰æ„å»ºåŒ…å«ä¸°å¯Œæ–‡æœ¬ä¸Šä¸‹æ–‡çš„æ•°æ®é›†ï¼Œä»¥æµ‹è¯•VLMçš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼›3ï¼‰é‡‡ç”¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°ç›¸ç»“åˆçš„æ–¹å¼ï¼Œä»¥ç¡®ä¿è¯„ä¼°ç»“æœçš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼›4ï¼‰é’ˆå¯¹ä½èµ„æºè¯­è¨€ï¼Œé‡‡ç”¨æ•°æ®å¢å¼ºå’Œè¿ç§»å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œä»¥æé«˜VLMçš„æ€§èƒ½ï¼ˆå…·ä½“ç»†èŠ‚æœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4oåœ¨VLUResåŸºå‡†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®ç‡è¾¾åˆ°90.8%ï¼Œä½†ä¸äººç±»æ€§èƒ½ä»æœ‰6.7%çš„å·®è·ã€‚å¼€æºæ¨¡å‹çš„æ€§èƒ½å·®è·æ›´å¤§ï¼Œè¡¨æ˜ç°æœ‰VLMåœ¨ä½èµ„æºè¯­è¨€å’Œå¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä»æœ‰æå‡ç©ºé—´ã€‚VLUResåŸºå‡†çš„å¼•å…¥ä¸ºè¯„ä¼°å’Œæ”¹è¿›VLMåœ¨å¤šè¯­ç§ç¯å¢ƒä¸‹çš„æ€§èƒ½æä¾›äº†é‡è¦å·¥å…·ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´é€šç”¨çš„å¤šæ¨¡æ€æ™ºèƒ½ä»£ç†ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºæ„å»ºèƒ½å¤Ÿç†è§£å½“åœ°è¯­è¨€å’Œæ–‡åŒ–çš„æ™ºèƒ½å®¢æœã€æ•™è‚²æœºå™¨äººæˆ–åŒ»ç–—åŠ©æ‰‹ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†è¿˜å¯ä»¥ä¿ƒè¿›VLMåœ¨è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ã€æœºå™¨ç¿»è¯‘å’Œå¤šæ¨¡æ€å†…å®¹ç”Ÿæˆç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision Language Models (VLMs) are pivotal for advancing perception in intelligent agents. Yet, evaluation of VLMs remains limited to predominantly English-centric benchmarks in which the image-text pairs comprise short texts. To evaluate VLM fine-grained abilities, in four languages under long-text settings, we introduce a novel multilingual benchmark VLURes featuring eight vision-and-language tasks, and a pioneering unrelatedness task, to probe the fine-grained Visual and Linguistic Understanding capabilities of VLMs across English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets, curated from web resources in the target language, encompass ten diverse image categories and rich textual context, introducing valuable vision-language resources for Swahili and Urdu. By prompting VLMs to generate responses and rationales, evaluated automatically and by native speakers, we uncover performance disparities across languages and tasks critical to intelligent agents, such as object recognition, scene understanding, and relationship understanding. We conducted evaluations of ten VLMs with VLURes. The best performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human performance by 6.7%, though the gap is larger for open-source models. The gap highlights VLURes' critical role in developing intelligent agents to tackle multi-modal visual reasoning.

