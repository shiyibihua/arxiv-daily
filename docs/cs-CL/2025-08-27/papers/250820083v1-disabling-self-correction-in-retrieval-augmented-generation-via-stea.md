---
layout: default
title: Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning
---

# Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20083" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20083v1</a>
  <a href="https://arxiv.org/pdf/2508.20083.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20083v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20083v1', 'Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanbo Dai, Zhenlan Ji, Zongjie Li, Kuan Li, Shuai Wang

**åˆ†ç±»**: cs.CR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDisarmRAGä»¥è§£å†³RAGç³»ç»Ÿè‡ªæˆ‘çº é”™èƒ½åŠ›çš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `è‡ªæˆ‘çº é”™èƒ½åŠ›` `æ¯’åŒ–æ”»å‡»` `å¯¹æ¯”å­¦ä¹ ` `æ¨¡å‹ç¼–è¾‘` `å®‰å…¨æ€§æµ‹è¯•` `å¯¹æŠ—æ€§æ”»å‡»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RAGç³»ç»Ÿçš„è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼ˆSCAï¼‰ä½¿å¾—æ”»å‡»è€…éš¾ä»¥é€šè¿‡æ¯’åŒ–çŸ¥è¯†åº“å®ç°é¢„æœŸçš„è¾“å‡ºã€‚
2. æœ¬æ–‡æå‡ºDisarmRAGï¼Œé€šè¿‡ç ´åæ£€ç´¢å™¨æœ¬èº«æ¥æŠ‘åˆ¶SCAï¼Œä»è€Œå®ç°æ”»å‡»è€…é€‰æ‹©çš„è¾“å‡ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDisarmRAGåœ¨å¤šç§é˜²å¾¡æç¤ºä¸‹çš„æ”»å‡»æˆåŠŸç‡è¶…è¿‡90%ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§å’Œéšè”½æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯é æ€§çš„æ ‡å‡†æ–¹æ³•ã€‚ä»¥å¾€ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æ¯’åŒ–çŸ¥è¯†åº“å¯ä»¥è¯¯å¯¼RAGç³»ç»Ÿç”Ÿæˆæ”»å‡»è€…é€‰æ‹©çš„è¾“å‡ºã€‚ç„¶è€Œï¼Œæœ¬è®ºæ–‡å‘ç°ç°ä»£LLMsçš„å¼ºå¤§è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼ˆSCAï¼‰å¯ä»¥å‡è½»æ­¤ç±»æ”»å‡»ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¯’åŒ–èŒƒå¼DisarmRAGï¼Œæ—¨åœ¨ç ´åæ£€ç´¢å™¨æœ¬èº«ä»¥æŠ‘åˆ¶SCAå¹¶å¼ºåˆ¶ç”Ÿæˆæ”»å‡»è€…é€‰æ‹©çš„è¾“å‡ºã€‚é€šè¿‡å¯¹æ£€ç´¢å™¨è¿›è¡Œå±€éƒ¨å’Œéšè”½çš„ç¼–è¾‘ï¼Œç¡®ä¿å…¶ä»…å¯¹ç‰¹å®šå—å®³æŸ¥è¯¢è¿”å›æ¶æ„æŒ‡ä»¤ï¼ŒåŒæ—¶ä¿æŒè‰¯æ€§æ£€ç´¢è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDisarmRAGåœ¨å…­ä¸ªLLMså’Œä¸‰ä¸ªQAåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ”»å‡»æˆåŠŸç‡è¶…è¿‡90%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼ˆSCAï¼‰å¯¹æ”»å‡»è€…çš„é˜²å¾¡æ•ˆæœã€‚ç°æœ‰çš„æ¯’åŒ–æ–¹æ³•ä¸»è¦é’ˆå¯¹çŸ¥è¯†åº“ï¼Œæœªèƒ½æœ‰æ•ˆåº”å¯¹SCAçš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºDisarmRAGï¼Œé€šè¿‡ç›´æ¥æ¯’åŒ–æ£€ç´¢å™¨æ¥æŠ‘åˆ¶SCAï¼Œä½¿å¾—æ”»å‡»è€…èƒ½å¤ŸåµŒå…¥æ¶æ„æŒ‡ä»¤ï¼Œä»è€Œç»•è¿‡è‡ªæˆ‘çº é”™æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¯’åŒ–æ£€ç´¢å™¨çš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹ç¼–è¾‘æŠ€æœ¯ï¼Œç¡®ä¿æ£€ç´¢å™¨åœ¨ç‰¹å®šæŸ¥è¯¢ä¸‹è¿”å›æ¶æ„æŒ‡ä»¤ï¼ŒåŒæ—¶ä¿æŒæ­£å¸¸çš„æ£€ç´¢è¡Œä¸ºã€‚è¯¥æ¡†æ¶è¿˜åŒ…å«è¿­ä»£å…±åŒä¼˜åŒ–æœºåˆ¶ï¼Œä»¥å‘ç°èƒ½å¤Ÿç»•è¿‡é˜²å¾¡çš„å¼ºå¥æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šDisarmRAGçš„åˆ›æ–°åœ¨äºå…¶é’ˆå¯¹æ£€ç´¢å™¨çš„æ¯’åŒ–æ–¹æ³•ï¼Œä¸ä»¥å¾€ä¸»è¦é’ˆå¯¹çŸ¥è¯†åº“çš„æ¯’åŒ–ç­–ç•¥æœ¬è´¨ä¸åŒï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶SCAã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹ç¼–è¾‘è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•è¿›è¡Œå±€éƒ¨ç¼–è¾‘ï¼Œç¡®ä¿æ¶æ„æŒ‡ä»¤ä»…åœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­ç”Ÿæ•ˆï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§å¼ºçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æŒ‡ä»¤çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDisarmRAGåœ¨å…­ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†è¿‘ä¹å®Œç¾çš„æ¶æ„æŒ‡ä»¤æ£€ç´¢ï¼Œæ”»å‡»æˆåŠŸç‡åœ¨å¤šç§é˜²å¾¡æç¤ºä¸‹è¶…è¿‡90%ã€‚æ­¤å¤–ï¼Œç»è¿‡ç¼–è¾‘çš„æ£€ç´¢å™¨åœ¨å¤šç§æ£€æµ‹æ–¹æ³•ä¸‹ä»ä¿æŒéšè”½æ€§ï¼Œçªæ˜¾äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ€§æµ‹è¯•ã€æ¶æ„è½¯ä»¶æ£€æµ‹å’Œå¯¹æŠ—æ€§æ”»å‡»é˜²å¾¡ç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£RAGç³»ç»Ÿçš„è„†å¼±æ€§ï¼Œå¯ä»¥ä¸ºæœªæ¥çš„AIç³»ç»Ÿè®¾è®¡æ›´å¼ºçš„é˜²å¾¡æœºåˆ¶ï¼Œæå‡å…¶å®‰å…¨æ€§å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) has become a standard approach for improving the reliability of large language models (LLMs). Prior work demonstrates the vulnerability of RAG systems by misleading them into generating attacker-chosen outputs through poisoning the knowledge base. However, this paper uncovers that such attacks could be mitigated by the strong \textit{self-correction ability (SCA)} of modern LLMs, which can reject false context once properly configured. This SCA poses a significant challenge for attackers aiming to manipulate RAG systems.
>   In contrast to previous poisoning methods, which primarily target the knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that compromises the retriever itself to suppress the SCA and enforce attacker-chosen outputs. This compromisation enables the attacker to straightforwardly embed anti-SCA instructions into the context provided to the generator, thereby bypassing the SCA. To this end, we present a contrastive-learning-based model editing technique that performs localized and stealthy edits, ensuring the retriever returns a malicious instruction only for specific victim queries while preserving benign retrieval behavior. To further strengthen the attack, we design an iterative co-optimization framework that automatically discovers robust instructions capable of bypassing prompt-based defenses. We extensively evaluate DisarmRAG across six LLMs and three QA benchmarks. Our results show near-perfect retrieval of malicious instructions, which successfully suppress SCA and achieve attack success rates exceeding 90\% under diverse defensive prompts. Also, the edited retriever remains stealthy under several detection methods, highlighting the urgent need for retriever-centric defenses.

