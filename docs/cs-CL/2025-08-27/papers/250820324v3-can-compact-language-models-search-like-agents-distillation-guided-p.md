---
layout: default
title: Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities
---

# Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20324" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20324v3</a>
  <a href="https://arxiv.org/pdf/2508.20324.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20324v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20324v3', 'Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rikuto Kotoge, Mai Nishimura, Jiaxin Ma

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27 (æ›´æ–°: 2025-10-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè’¸é¦å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ä»¥æå‡ç´§å‡‘è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½æœç´¢èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è’¸é¦è®­ç»ƒ` `ç´§å‡‘æ¨¡å‹` `æ™ºèƒ½æœç´¢` `ç­–ç•¥ä¼˜åŒ–` `RAGèƒ½åŠ›` `æ•™å¸ˆæ¨¡å‹` `æ¨¡å‹å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºç´§å‡‘æ¨¡å‹æ—¶ï¼Œé¢ä¸´åˆå§‹æ€§èƒ½å·®ã€å¥–åŠ±ç¨€ç–å’Œè®­ç»ƒä¸ç¨³å®šç­‰æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºçš„è’¸é¦å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆDGPOï¼‰é€šè¿‡æ•™å¸ˆç¤ºèŒƒçš„å†·å¯åŠ¨å’ŒæŒç»­æŒ‡å¯¼ï¼Œæå‡ç´§å‡‘æ¨¡å‹çš„æ™ºèƒ½è¡Œä¸ºã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDGPOä½¿ç´§å‡‘æ¨¡å‹å®ç°äº†å¤æ‚çš„æ™ºèƒ½æœç´¢è¡Œä¸ºï¼Œéƒ¨åˆ†æƒ…å†µä¸‹è¶…è¶Šäº†å¤§å‹æ•™å¸ˆæ¨¡å‹çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºä»è¯­è¨€æ¨¡å‹ä¸­å¼•å¯¼æ™ºèƒ½RAGè¡Œä¸ºï¼ˆå¦‚æœç´¢å’Œè§„åˆ’ï¼‰çš„ä¸»è¦åè®­ç»ƒæ–¹æ³•ã€‚å°½ç®¡åœ¨å¤§å‹æ¨¡å‹ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºç´§å‡‘æ¨¡å‹ï¼ˆå¦‚0.5-1Bå‚æ•°ï¼‰é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ã€‚ç´§å‡‘æ¨¡å‹çš„åˆå§‹æ€§èƒ½è¾ƒå·®ï¼Œå¯¼è‡´ç¨€ç–å¥–åŠ±å’Œä¸ç¨³å®šè®­ç»ƒã€‚ä¸ºå…‹æœè¿™äº›å›°éš¾ï¼Œæœ¬æ–‡æå‡ºäº†è’¸é¦å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆDGPOï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ•™å¸ˆç¤ºèŒƒçš„å†·å¯åŠ¨åˆå§‹åŒ–å’Œåœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­æŒç»­çš„æ•™å¸ˆæŒ‡å¯¼ã€‚é€šè¿‡å¼•å…¥æ™ºèƒ½RAGèƒ½åŠ›ï¼ˆARCï¼‰è¿™ä¸€ç»†ç²’åº¦æŒ‡æ ‡ï¼Œåˆ†ææ¨ç†ã€æœç´¢åè°ƒå’Œå“åº”åˆæˆã€‚å®éªŒè¡¨æ˜ï¼ŒDGPOä½¿ç´§å‡‘æ¨¡å‹èƒ½å¤Ÿå®ç°å¤æ‚çš„æ™ºèƒ½æœç´¢è¡Œä¸ºï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†å¤§å‹æ•™å¸ˆæ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç´§å‡‘è¯­è¨€æ¨¡å‹åœ¨åº”ç”¨å¼ºåŒ–å­¦ä¹ æ—¶çš„æ€§èƒ½ä¸è¶³é—®é¢˜ï¼Œå°¤å…¶æ˜¯åˆå§‹æ€§èƒ½å·®å’Œè®­ç»ƒä¸ç¨³å®šå¯¼è‡´çš„ç¨€ç–å¥–åŠ±é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè’¸é¦å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆDGPOï¼‰ï¼Œé€šè¿‡æ•™å¸ˆç¤ºèŒƒè¿›è¡Œå†·å¯åŠ¨åˆå§‹åŒ–ï¼Œå¹¶åœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­æä¾›æŒç»­çš„æ•™å¸ˆæŒ‡å¯¼ï¼Œä»¥æå‡ç´§å‡‘æ¨¡å‹çš„æ™ºèƒ½è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDGPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯é€šè¿‡æ•™å¸ˆæ¨¡å‹è¿›è¡Œå†·å¯åŠ¨åˆå§‹åŒ–ï¼Œå…¶æ¬¡æ˜¯åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„åé¦ˆè¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œç¡®ä¿ç´§å‡‘æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¿æŒç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šDGPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆäº†æ•™å¸ˆç¤ºèŒƒä¸å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œä½¿å¾—ç´§å‡‘æ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ™ºèƒ½RAGèƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DGPOä¸­ï¼Œè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡æ•™å¸ˆæŒ‡å¯¼ä¸æ¨¡å‹è‡ªä¸»å­¦ä¹ çš„å…³ç³»ï¼ŒåŒæ—¶é‡‡ç”¨äº†é€‚åº”æ€§çš„å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼Œä»¥åº”å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDGPOä½¿å¾—ç´§å‡‘æ¨¡å‹åœ¨æ™ºèƒ½æœç´¢è¡Œä¸ºä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œéƒ¨åˆ†æƒ…å†µä¸‹å…¶æ€§èƒ½è¶…è¿‡äº†å¤§å‹æ•™å¸ˆæ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å®ç°å¤æ‚æ™ºèƒ½è¡Œä¸ºçš„å¯èƒ½æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æœç´¢å¼•æ“å’Œèµ„æºå—é™çš„åµŒå…¥å¼ç³»ç»Ÿã€‚é€šè¿‡æå‡ç´§å‡‘æ¨¡å‹çš„æ™ºèƒ½æœç´¢èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„ä»»åŠ¡æ‰§è¡Œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning has emerged as a dominant post-training approach to elicit agentic RAG behaviors such as search and planning from language models. Despite its success with larger models, applying RL to compact models (e.g., 0.5--1B parameters) presents unique challenges. The compact models exhibit poor initial performance, resulting in sparse rewards and unstable training. To overcome these difficulties, we propose Distillation-Guided Policy Optimization (DGPO), which employs cold-start initialization from teacher demonstrations and continuous teacher guidance during policy optimization. To understand how compact models preserve agentic behavior, we introduce Agentic RAG Capabilities (ARC), a fine-grained metric analyzing reasoning, search coordination, and response synthesis. Comprehensive experiments demonstrate that DGPO enables compact models to achieve sophisticated agentic search behaviors, even outperforming the larger teacher model in some cases. DGPO makes agentic RAG feasible in computing resource-constrained environments.

