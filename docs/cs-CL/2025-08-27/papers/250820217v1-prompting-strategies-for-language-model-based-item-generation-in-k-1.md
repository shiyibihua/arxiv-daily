---
layout: default
title: Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models
---

# Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20217" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20217v1</a>
  <a href="https://arxiv.org/pdf/2508.20217.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20217v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20217v1', 'Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammad Amini, Babak Ahmadi, Xiaomeng Xiong, Yilin Zhang, Christopher Qiao

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“æ„åŒ–æç¤ºç­–ç•¥ä»¥æå‡K-12æ•™è‚²ä¸­çš„é¢˜ç›®ç”Ÿæˆè´¨é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨ç”Ÿæˆé¢˜ç›®` `è¯­è¨€æ¨¡å‹` `K-12æ•™è‚²` `ç»“æ„åŒ–æç¤º` `å¾®è°ƒç­–ç•¥` `è¯„ä¼°å·¥å…·` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰‹åŠ¨æµ‹è¯•å¼€å‘æ–¹æ³•æˆæœ¬é«˜ä¸”ä¸ä¸€è‡´ï¼Œéš¾ä»¥æ»¡è¶³K-12æ•™è‚²çš„éœ€æ±‚ã€‚
2. æœ¬ç ”ç©¶æå‡ºé€šè¿‡ç»“æ„åŒ–æç¤ºç­–ç•¥æ¥ä¼˜åŒ–ä¸­å‹è¯­è¨€æ¨¡å‹çš„é¢˜ç›®ç”Ÿæˆèƒ½åŠ›ï¼Œå°¤å…¶æ˜¯ç»“åˆæ€ç»´é“¾å’Œé¡ºåºè®¾è®¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGemmaæ¨¡å‹åœ¨ç”Ÿæˆçš„é¢˜ç›®è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºGPT-3.5ï¼Œä¸”æç¤ºè®¾è®¡å¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ä»¥è¿›è¡Œå½¢æ€è¯„ä¼°ï¼Œæ—¨åœ¨é™ä½äººå·¥æµ‹è¯•å¼€å‘çš„æˆæœ¬å’Œä¸ä¸€è‡´æ€§ã€‚ç ”ç©¶é‡‡ç”¨äº†åŒé‡æ–¹æ³•ï¼Œé¦–å…ˆæ¯”è¾ƒäº†ç»è¿‡å¾®è°ƒçš„ä¸­å‹æ¨¡å‹ï¼ˆGemma, 2Bï¼‰ä¸æœªè°ƒä¼˜çš„å¤§å‹æ¨¡å‹ï¼ˆGPT-3.5, 175Bï¼‰ã€‚å…¶æ¬¡ï¼Œè¯„ä¼°äº†ä¸ƒç§ç»“æ„åŒ–æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶-shotã€few-shotã€æ€ç»´é“¾ã€è§’è‰²åŸºç¡€ã€é¡ºåºå’Œç»„åˆã€‚ç”Ÿæˆçš„é¢˜ç›®é€šè¿‡è‡ªåŠ¨åŒ–æŒ‡æ ‡å’Œä¸“å®¶è¯„åˆ†åœ¨äº”ä¸ªç»´åº¦ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»“æ„åŒ–æç¤ºï¼Œå°¤å…¶æ˜¯ç»“åˆæ€ç»´é“¾å’Œé¡ºåºè®¾è®¡çš„ç­–ç•¥ï¼Œæ˜¾è‘—æ”¹å–„äº†Gemmaçš„è¾“å‡ºã€‚Gemmaé€šå¸¸ç”Ÿæˆçš„é¢˜ç›®åœ¨æ„å»ºå¯¹é½å’Œæ•™å­¦é€‚å®œæ€§æ–¹é¢ä¼˜äºGPT-3.5çš„é›¶-shotå“åº”ï¼Œæç¤ºè®¾è®¡åœ¨ä¸­å‹æ¨¡å‹æ€§èƒ½ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†ç»“æ„åŒ–æç¤ºå’Œé«˜æ•ˆå¾®è°ƒå¦‚ä½•åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹æå‡ä¸­å‹æ¨¡å‹çš„è‡ªåŠ¨ç”Ÿæˆèƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³K-12æ•™è‚²ä¸­æ‰‹åŠ¨æµ‹è¯•å¼€å‘çš„é«˜æˆæœ¬å’Œä¸ä¸€è‡´æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢˜ç›®ç”Ÿæˆä¸Šç¼ºä¹æ•ˆç‡å’Œä¸€è‡´æ€§ï¼Œå¯¼è‡´æ•™è‚²è¯„ä¼°çš„è´¨é‡å—åˆ°å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡ç»“æ„åŒ–æç¤ºç­–ç•¥æ¥æå‡ä¸­å‹è¯­è¨€æ¨¡å‹ï¼ˆGemmaï¼‰çš„é¢˜ç›®ç”Ÿæˆèƒ½åŠ›ï¼Œå°¤å…¶æ˜¯ç»“åˆæ€ç»´é“¾å’Œé¡ºåºè®¾è®¡ï¼Œä»¥æé«˜ç”Ÿæˆé¢˜ç›®çš„è´¨é‡å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨åŒé‡æ–¹æ³•ï¼Œé¦–å…ˆæ¯”è¾ƒäº†å¾®è°ƒçš„ä¸­å‹æ¨¡å‹ä¸æœªè°ƒä¼˜çš„å¤§å‹æ¨¡å‹ï¼Œå…¶æ¬¡è¯„ä¼°äº†ä¸ƒç§ä¸åŒçš„ç»“æ„åŒ–æç¤ºç­–ç•¥ã€‚ç”Ÿæˆçš„é¢˜ç›®é€šè¿‡è‡ªåŠ¨åŒ–æŒ‡æ ‡å’Œä¸“å®¶è¯„åˆ†è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿å…¶è´¨é‡å’Œé€‚åº”æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å¤šç§ç»“æ„åŒ–æç¤ºç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯æ€ç»´é“¾ä¸é¡ºåºè®¾è®¡çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†ä¸­å‹æ¨¡å‹åœ¨é¢˜ç›®ç”Ÿæˆä¸Šçš„è¡¨ç°ï¼Œä¸ä¼ ç»Ÿçš„é›¶-shotæ–¹æ³•ç›¸æ¯”ï¼Œæ•ˆæœæ›´ä½³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æç¤ºç­–ç•¥ï¼Œè®¾ç½®äº†ä¸åŒçš„å‚æ•°ä»¥é€‚åº”æ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ï¼Œä½¿ç”¨è‡ªåŠ¨åŒ–è¯„åˆ†ä¸ä¸“å®¶è¯„åˆ†ç›¸ç»“åˆçš„æ–¹å¼æ¥è¯„ä¼°ç”Ÿæˆé¢˜ç›®çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGemmaæ¨¡å‹åœ¨ç”Ÿæˆçš„é¢˜ç›®è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºGPT-3.5çš„é›¶-shotå“åº”ï¼Œå°¤å…¶åœ¨æ„å»ºå¯¹é½å’Œæ•™å­¦é€‚å®œæ€§æ–¹é¢è¡¨ç°çªå‡ºã€‚ç»“åˆæ€ç»´é“¾å’Œé¡ºåºè®¾è®¡çš„ç»“æ„åŒ–æç¤ºç­–ç•¥ä½¿Gemmaçš„è¾“å‡ºè´¨é‡æå‡äº†æ˜¾è‘—çš„å¹…åº¦ï¼ŒéªŒè¯äº†æç¤ºè®¾è®¡çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬K-12æ•™è‚²çš„è¯„ä¼°å·¥å…·å¼€å‘ï¼Œèƒ½å¤Ÿä¸ºæ•™å¸ˆæä¾›é«˜è´¨é‡çš„è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•é¢˜ç›®ï¼Œå‡è½»å…¶å·¥ä½œè´Ÿæ‹…ã€‚æ­¤å¤–ï¼Œç ”ç©¶æˆæœå¯æ¨å¹¿è‡³å…¶ä»–æ•™è‚²é¢†åŸŸï¼Œæå‡æ•™è‚²è¯„ä¼°çš„æ•ˆç‡å’Œä¸€è‡´æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This study explores automatic generation (AIG) using language models to create multiple choice questions (MCQs) for morphological assessment, aiming to reduce the cost and inconsistency of manual test development. The study used a two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B) with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven structured prompting strategies, including zero-shot, few-shot, chain-of-thought, role-based, sequential, and combinations. Generated items were assessed using automated metrics and expert scoring across five dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate human scoring at scale. Results show that structured prompting, especially strategies combining chain-of-thought and sequential design, significantly improved Gemma's outputs. Gemma generally produced more construct-aligned and instructionally appropriate items than GPT-3.5's zero-shot responses, with prompt design playing a key role in mid-size model performance. This study demonstrates that structured prompting and efficient fine-tuning can enhance midsized models for AIG under limited data conditions. We highlight the value of combining automated metrics, expert judgment, and large-model simulation to ensure alignment with assessment goals. The proposed workflow offers a practical and scalable way to develop and validate language assessment items for K-12.

