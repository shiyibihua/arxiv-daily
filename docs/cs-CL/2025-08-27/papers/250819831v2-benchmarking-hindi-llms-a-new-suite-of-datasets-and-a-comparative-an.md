---
layout: default
title: Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis
---

# Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19831" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19831v2</a>
  <a href="https://arxiv.org/pdf/2508.19831.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19831v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19831v2', 'Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anusha Kamath, Kanishk Singla, Rakesh Paul, Raviraj Joshi, Utkarsh Vaidya, Sanjay Singh Chauhan, Niranjan Wartikar

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27 (æ›´æ–°: 2025-10-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº”ä¸ªå°åœ°è¯­LLMè¯„ä¼°æ•°æ®é›†ä»¥è§£å†³è¯„ä¼°æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°åœ°è¯­LLM` `åŸºå‡†æµ‹è¯•` `æ•°æ®é›†åˆ›å»º` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨ç¿»è¯‘` `äººæœºäº¤äº’` `ä½èµ„æºè¯­è¨€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å°åœ°è¯­LLMè¯„ä¼°ç¼ºä¹é«˜è´¨é‡åŸºå‡†ï¼Œç›´æ¥ç¿»è¯‘çš„è‹±è¯­æ•°æ®é›†æ— æ³•æœ‰æ•ˆæ•æ‰å°åœ°è¯­çš„è¯­è¨€å’Œæ–‡åŒ–ç‰¹å¾ã€‚
2. æœ¬æ–‡æå‡ºäº†äº”ä¸ªä¸“é—¨ä¸ºå°åœ°è¯­è®¾è®¡çš„è¯„ä¼°æ•°æ®é›†ï¼Œç»“åˆäººç±»æ³¨é‡Šå’Œç¿»è¯‘éªŒè¯çš„æ–¹æ³•ï¼Œç¡®ä¿æ•°æ®é›†çš„è´¨é‡å’Œé€‚ç”¨æ€§ã€‚
3. é€šè¿‡å¯¹å¼€æºå°åœ°è¯­LLMçš„åŸºå‡†æµ‹è¯•ï¼Œæœ¬æ–‡æä¾›äº†è¯¦ç»†çš„æ¯”è¾ƒåˆ†æï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯„ä¼°å°åœ°è¯­çš„æŒ‡ä»¤è°ƒä¼˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç¼ºä¹é«˜è´¨é‡çš„åŸºå‡†æ•°æ®é›†ï¼Œç›´æ¥ç¿»è¯‘çš„è‹±è¯­æ•°æ®é›†æ— æ³•æ•æ‰é‡è¦çš„è¯­è¨€å’Œæ–‡åŒ–ç»†èŠ‚ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†äº”ä¸ªå°åœ°è¯­LLMè¯„ä¼°æ•°æ®é›†ï¼šIFEval-Hiã€MT-Bench-Hiã€GSM8K-Hiã€ChatRAG-Hiå’ŒBFCL-Hiã€‚è¿™äº›æ•°æ®é›†é‡‡ç”¨ä»é›¶å¼€å§‹çš„äººç±»æ³¨é‡Šä¸ç¿»è¯‘éªŒè¯ç›¸ç»“åˆçš„æ–¹æ³•åˆ›å»ºã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€æ•°æ®é›†è¿›è¡Œå¼€æºå°åœ°è¯­LLMçš„å¹¿æ³›åŸºå‡†æµ‹è¯•ï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„æ¯”è¾ƒåˆ†æã€‚è¿™ä¸€ç­–åˆ’è¿‡ç¨‹ä¹Ÿä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€çš„åŸºå‡†å¼€å‘æä¾›äº†å¯å¤åˆ¶çš„æ–¹æ³•è®ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°åœ°è¯­LLMè¯„ä¼°ä¸­ç¼ºä¹é«˜è´¨é‡åŸºå‡†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè‹±è¯­æ•°æ®é›†çš„ç¿»è¯‘ï¼Œæ— æ³•æœ‰æ•ˆåæ˜ å°åœ°è¯­çš„è¯­è¨€ç‰¹æ€§å’Œæ–‡åŒ–èƒŒæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„è§£å†³æ–¹æ¡ˆæ˜¯åˆ›å»ºäº”ä¸ªä¸“é—¨é’ˆå¯¹å°åœ°è¯­çš„è¯„ä¼°æ•°æ®é›†ï¼Œé‡‡ç”¨ä»é›¶å¼€å§‹çš„äººç±»æ³¨é‡Šä¸ç¿»è¯‘éªŒè¯ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä»¥ç¡®ä¿æ•°æ®é›†çš„å‡†ç¡®æ€§å’Œé€‚ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†çš„è®¾è®¡ã€åˆ›å»ºå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œé€šè¿‡äººç±»ä¸“å®¶è¿›è¡Œæ³¨é‡Šï¼Œç¡®ä¿æ•°æ®çš„è´¨é‡ï¼›å…¶æ¬¡ï¼Œè¿›è¡Œç¿»è¯‘éªŒè¯ï¼Œç¡®ä¿æ•°æ®é›†çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ï¼›æœ€åï¼Œåˆ©ç”¨è¿™äº›æ•°æ®é›†å¯¹å°åœ°è¯­LLMè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºç»“åˆäº†äººç±»æ³¨é‡Šå’Œç¿»è¯‘éªŒè¯çš„åŒé‡æ–¹æ³•ï¼Œç¡®ä¿äº†æ•°æ®é›†çš„é«˜è´¨é‡å’Œé€‚ç”¨æ€§ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€ç¿»è¯‘ç­–ç•¥å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†åˆ›å»ºè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶æ ‡å‡†ï¼ŒåŒ…æ‹¬å¤šè½®å®¡æ ¸å’Œåé¦ˆæœºåˆ¶ï¼Œä»¥ç¡®ä¿æ•°æ®é›†çš„å‡†ç¡®æ€§å’Œä»£è¡¨æ€§ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†é€‚åˆå°åœ°è¯­ç‰¹æ€§çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ä¾¿æ›´å¥½åœ°åæ˜ æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ–°åˆ›å»ºçš„å°åœ°è¯­æ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„LLMåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚å…·ä½“è€Œè¨€ï¼ŒæŸäº›æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆå°åœ°è¯­æ–‡æœ¬çš„èƒ½åŠ›ä¸Šæå‡äº†20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºç°æœ‰çš„åŸºå‡†æµ‹è¯•ç»“æœï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„å°åœ°è¯­LLMè¯„ä¼°åŸºå‡†ï¼Œç ”ç©¶å¯ä»¥ä¿ƒè¿›å°åœ°è¯­ç›¸å…³æŠ€æœ¯çš„å‘å±•ï¼Œæå‡å°åœ°è¯­åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„åº”ç”¨ä»·å€¼ï¼Œå¹¶ä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€çš„ç ”ç©¶æä¾›å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is challenging due to a lack of high-quality benchmarks, as direct translation of English datasets fails to capture crucial linguistic and cultural nuances. To address this, we introduce a suite of five Hindi LLM evaluation datasets: IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created using a methodology that combines from-scratch human annotation with a translate-and-verify process. We leverage this suite to conduct an extensive benchmarking of open-source LLMs supporting Hindi, providing a detailed comparative analysis of their current capabilities. Our curation process also serves as a replicable methodology for developing benchmarks in other low-resource languages.

