---
layout: default
title: Logical Reasoning with Outcome Reward Models for Test-Time Scaling
---

# Logical Reasoning with Outcome Reward Models for Test-Time Scaling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19903" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19903v1</a>
  <a href="https://arxiv.org/pdf/2508.19903.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19903v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19903v1', 'Logical Reasoning with Outcome Reward Models for Test-Time Scaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ramya Keerthy Thatikonda, Wray Buntine, Ehsan Shareghi

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**å¤‡æ³¨**: EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“æœå¥–åŠ±æ¨¡å‹ä»¥æå‡æ¨ç†ä»»åŠ¡ä¸­çš„é€»è¾‘æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é€»è¾‘æ¨ç†` `ç»“æœå¥–åŠ±æ¨¡å‹` `é“¾å¼æ€ç»´` `å›å£°ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¼”ç»æ¨ç†` `æ•°æ®å¢å¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¼”ç»é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­æœªèƒ½å……åˆ†åˆ©ç”¨ç»“æœå¥–åŠ±æ¨¡å‹ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›æå‡æœ‰é™ã€‚
2. æœ¬æ–‡æå‡ºçš„ç»“æœå¥–åŠ±æ¨¡å‹é€šè¿‡é“¾å¼æ€ç»´ç”Ÿæˆæ•°æ®ï¼Œå¹¶å¼•å…¥å›å£°ç”ŸæˆæŠ€æœ¯ï¼Œæ‰©å±•è®­ç»ƒæ•°æ®é›†ä¸­çš„é”™è¯¯ç±»å‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºCoTå’Œå›å£°å¢å¼ºæ•°æ®è®­ç»ƒçš„ORMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†LLMsçš„æ¨ç†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€»è¾‘æ¨ç†æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½åŠ›çš„é‡è¦åŸºå‡†ï¼Œåæ˜ å…¶ä»ç»™å®šå‰æä¸­æ¨å¯¼æœ‰æ•ˆç»“è®ºçš„èƒ½åŠ›ã€‚å°½ç®¡å°†æµ‹è¯•æ—¶é—´æ‰©å±•ä¸ä¸“é—¨çš„ç»“æœæˆ–è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ç»“åˆèµ·æ¥ä¸ºæå‡LLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œä½†åœ¨æ¼”ç»é€»è¾‘æ¨ç†é¢†åŸŸè¿™ä¸€æ–¹å‘å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç»„ç”¨äºæ¼”ç»æ¨ç†çš„ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰ã€‚æˆ‘ä»¬ä¸»è¦é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç”Ÿæˆå•æ ·æœ¬å’Œå¤šæ ·æœ¬æ•°æ®æ¥è®­ç»ƒORMsã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æ‰©å±•ORMè®­ç»ƒæ•°æ®é›†ä¸­æ¶µç›–çš„é”™è¯¯ç±»å‹ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨å›å£°ç”ŸæˆæŠ€æœ¯æå–é¢å¤–è®­ç»ƒæ•°æ®ï¼Œè¦†ç›–å…ˆå‰æœªæ¢ç´¢çš„é”™è¯¯ç±»å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºCoTå’Œå›å£°å¢å¼ºæ•°æ®è®­ç»ƒçš„ORMåœ¨FOLIOã€JustLogicå’ŒProverQAæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæå‡äº†å››ç§ä¸åŒLLMsçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¼”ç»é€»è¾‘æ¨ç†æ–¹æ³•åœ¨æ¨ç†èƒ½åŠ›æå‡æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å’Œé”™è¯¯ç±»å‹çš„è¦†ç›–ä¸Šå­˜åœ¨çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰å’Œå›å£°ç”ŸæˆæŠ€æœ¯ï¼Œæœ¬æ–‡æ—¨åœ¨å¢å¼ºLLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ç”Ÿæˆå¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®æ¥è¦†ç›–æ›´å¤šé”™è¯¯ç±»å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—ï¼ˆä½¿ç”¨é“¾å¼æ€ç»´ç”Ÿæˆå•æ ·æœ¬å’Œå¤šæ ·æœ¬æ•°æ®ï¼‰ï¼Œä»¥åŠå›å£°ç”Ÿæˆæ¨¡å—ï¼ˆæå–é¢å¤–è®­ç»ƒæ•°æ®ä»¥è¦†ç›–æœªæ¢ç´¢çš„é”™è¯¯ç±»å‹ï¼‰ï¼Œæœ€åé€šè¿‡è®­ç»ƒORMsæ¥æå‡æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå›å£°ç”ŸæˆæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨LLMsåœ¨æç¤ºä¸­åæ˜ çš„é”™è¯¯å‡è®¾ï¼Œç³»ç»Ÿæ€§åœ°æ‰©å±•äº†è®­ç»ƒæ•°æ®é›†ä¸­çš„é”™è¯¯ç±»å‹ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ORMsçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ•°æ®ç”Ÿæˆæ—¶è®¾ç½®äº†å‚æ•°ä»¥ç¡®ä¿ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å’Œè¦†ç›–æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºCoTå’Œå›å£°å¢å¼ºæ•°æ®è®­ç»ƒçš„ORMåœ¨FOLIOã€JustLogicå’ŒProverQAæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå››ç§ä¸åŒLLMsçš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%è‡³20%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ¼”ç»é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ³•å¾‹æ¨ç†ã€åŒ»ç–—å†³ç­–ç­‰éœ€è¦å¤æ‚é€»è¾‘æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æå‡LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨è¿™äº›é¢†åŸŸä¸­å®ç°æ›´é«˜æ•ˆçš„å†³ç­–æ”¯æŒå’Œé—®é¢˜è§£å†³ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Logical reasoning is a critical benchmark for evaluating the capabilities of large language models (LLMs), as it reflects their ability to derive valid conclusions from given premises. While the combination of test-time scaling with dedicated outcome or process reward models has opened up new avenues to enhance LLMs performance in complex reasoning tasks, this space is under-explored in deductive logical reasoning. We present a set of Outcome Reward Models (ORMs) for deductive reasoning. To train the ORMs we mainly generate data using Chain-of-Thought (CoT) with single and multiple samples. Additionally, we propose a novel tactic to further expand the type of errors covered in the training dataset of the ORM. In particular, we propose an echo generation technique that leverages LLMs' tendency to reflect incorrect assumptions made in prompts to extract additional training data, covering previously unexplored error types. While a standard CoT chain may contain errors likely to be made by the reasoner, the echo strategy deliberately steers the model toward incorrect reasoning. We show that ORMs trained on CoT and echo-augmented data demonstrate improved performance on the FOLIO, JustLogic, and ProverQA datasets across four different LLMs.

