---
layout: default
title: LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation
---

# LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19614" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19614v2</a>
  <a href="https://arxiv.org/pdf/2508.19614.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19614v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19614v2', 'LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Sun, Zhiyong Xie, Dan Luo, Long Zhang, Liming Dong, Yunwei Zhao, Xixun Lin, Yanxiong Lu, Chenliang Li, Lixin Zou

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27 (æ›´æ–°: 2025-10-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå±‚èåˆè§£ç ä»¥ä¼˜åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å‹çš„å¤–éƒ¨çŸ¥è¯†åˆ©ç”¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å±‚èåˆè§£ç ` `å¤–éƒ¨çŸ¥è¯†åˆ©ç”¨` `å¤§å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†æå–` `ç”Ÿæˆè´¨é‡æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å‹åœ¨æ•´åˆå¤–éƒ¨çŸ¥è¯†æ—¶é¢ä¸´å™ªå£°å¹²æ‰°çš„é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡ä¸ç¨³å®šã€‚
2. æœ¬æ–‡æå‡ºå±‚èåˆè§£ç ï¼ˆLFDï¼‰ï¼Œé€šè¿‡ç»“åˆä¸­é—´å±‚å’Œæœ€ç»ˆå±‚çš„è¾“å‡ºï¼Œä¼˜åŒ–å¤–éƒ¨çŸ¥è¯†çš„åˆ©ç”¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLFDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†RAGç³»ç»Ÿçš„çŸ¥è¯†æå–æ•ˆç‡ï¼Œæˆæœ¬å´ä¿æŒæœ€ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å°†å¤–éƒ¨çŸ¥è¯†èå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæå‡å…¶å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§å¹¶æ”¯æŒä¿¡æ¯æ›´æ–°ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿‘æœŸå®è¯ç ”ç©¶è¡¨æ˜ï¼Œå‘æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ä¸­æ³¨å…¥å™ªå£°åè€Œä¿ƒè¿›äº†å¤–éƒ¨çŸ¥è¯†çš„åˆ©ç”¨å¹¶æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚å°½ç®¡è¿™ä¸€ç°è±¡åç›´è§‰ä¸”éš¾ä»¥åº”ç”¨ï¼Œä½†å®ƒä¸ºLLMå¦‚ä½•æ•´åˆå¤–éƒ¨çŸ¥è¯†æä¾›äº†ç»†è‡´çš„æ§åˆ¶å’Œåˆ†æã€‚å› æ­¤ï¼Œæœ¬æ–‡é€šè¿‡å™ªå£°æ³¨å…¥å¹²é¢„ï¼Œå»ºç«‹äº†LLMå†…éƒ¨çš„å±‚çº§åŠŸèƒ½åˆ’åˆ†ï¼šæµ…å±‚ä¸“æ³¨äºå±€éƒ¨ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œä¸­é—´å±‚èšç„¦äºæ•´åˆè¿œç¨‹å¤–éƒ¨äº‹å®çŸ¥è¯†ï¼Œè€Œæ·±å±‚ä¸»è¦ä¾èµ–äºå‚æ•°åŒ–çš„å†…éƒ¨çŸ¥è¯†ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œæå‡ºäº†å±‚èåˆè§£ç ï¼ˆLFDï¼‰ï¼Œä¸€ç§ç®€å•çš„è§£ç ç­–ç•¥ï¼Œç›´æ¥å°†ä¸­é—´å±‚çš„è¡¨ç¤ºä¸æœ€ç»ˆå±‚çš„è§£ç è¾“å‡ºç»“åˆï¼Œä»¥å……åˆ†åˆ©ç”¨å¤–éƒ¨äº‹å®çŸ¥è¯†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å‹åœ¨æ•´åˆå¤–éƒ¨çŸ¥è¯†æ—¶ç”±äºå™ªå£°å¹²æ‰°å¯¼è‡´çš„ç”Ÿæˆè´¨é‡ä¸ç¨³å®šé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼Œå½±å“äº†æ¨¡å‹çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„å±‚èåˆè§£ç ï¼ˆLFDï¼‰ç­–ç•¥ï¼Œé€šè¿‡å°†ä¸­é—´å±‚çš„çŸ¥è¯†è¡¨ç¤ºä¸æœ€ç»ˆå±‚çš„è§£ç è¾“å‡ºç»“åˆï¼Œæ—¨åœ¨ä¼˜åŒ–å¤–éƒ¨çŸ¥è¯†çš„æ•´åˆï¼Œæå‡ç”Ÿæˆè´¨é‡ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨é•¿è·ç¦»çš„å¤–éƒ¨äº‹å®çŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒå±€éƒ¨ä¸Šä¸‹æ–‡çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦å±‚æ¬¡ï¼šæµ…å±‚è´Ÿè´£å±€éƒ¨ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œä¸­é—´å±‚æ•´åˆå¤–éƒ¨çŸ¥è¯†ï¼Œæ·±å±‚ä¾èµ–å†…éƒ¨çŸ¥è¯†ã€‚LFDé€šè¿‡é€‰æ‹©æœ€ä¼˜çš„ä¸­é—´å±‚è¾“å‡ºä¸æœ€ç»ˆå±‚ç»“åˆï¼Œå½¢æˆæœ€ç»ˆçš„ç”Ÿæˆç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†å†…éƒ¨çŸ¥è¯†è¯„åˆ†ï¼ˆIKSï¼‰æ ‡å‡†ï¼Œä»¥é€‰æ‹©åœ¨ååŠéƒ¨åˆ†å±‚ä¸­å…·æœ‰æœ€ä½IKSå€¼çš„ä¸­é—´å±‚ã€‚è¿™ä¸€æ–¹æ³•ä½¿å¾—æ¨¡å‹åœ¨æ•´åˆå¤–éƒ¨çŸ¥è¯†æ—¶æ›´åŠ é«˜æ•ˆï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ç»†è‡´çš„æ§åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒIKSæ ‡å‡†çš„å¼•å…¥æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚æ­¤å¤–ï¼ŒLFDçš„è§£ç è¿‡ç¨‹ç®€åŒ–äº†ä¼ ç»Ÿçš„å¤šå±‚è§£ç ç­–ç•¥ï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡çš„æå‡ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLFDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ï¼Œå°¤å…¶æ˜¯åœ¨çŸ¥è¯†æå–æ•ˆç‡ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»ŸRAGæ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼Œä¸”è®¡ç®—æˆæœ¬ä¿æŒåœ¨æœ€ä½æ°´å¹³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç”Ÿæˆä»¥åŠä¿¡æ¯æ£€ç´¢ç­‰åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–å¤–éƒ¨çŸ¥è¯†çš„åˆ©ç”¨ï¼ŒLFDèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨åŠ¨æ€çŸ¥è¯†ç¯å¢ƒä¸­çš„é€‚åº”æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-augmented generation (RAG) incorporates external knowledge into large language models (LLMs), improving their adaptability to downstream tasks and enabling information updates. Surprisingly, recent empirical evidence demonstrates that injecting noise into retrieved relevant documents paradoxically facilitates exploitation of external knowledge and improves generation quality. Although counterintuitive and challenging to apply in practice, this phenomenon enables granular control and rigorous analysis of how LLMs integrate external knowledge. Therefore, in this paper, we intervene on noise injection and establish a layer-specific functional demarcation within the LLM: shallow layers specialize in local context modeling, intermediate layers focus on integrating long-range external factual knowledge, and deeper layers primarily rely on parametric internal knowledge. Building on this insight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that directly combines representations from an intermediate layer with final-layer decoding outputs to fully exploit the external factual knowledge. To identify the optimal intermediate layer, we introduce an internal knowledge score (IKS) criterion that selects the layer with the lowest IKS value in the latter half of layers. Experimental results across multiple benchmarks demonstrate that LFD helps RAG systems more effectively surface retrieved context knowledge with minimal cost.

