---
layout: default
title: Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking
---

# Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19558" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19558v1</a>
  <a href="https://arxiv.org/pdf/2508.19558.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19558v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19558v1', 'Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhuohao Li, Wenqing Chen, Jianxing Yu, Zhichao Lu

**åˆ†ç±»**: cs.SE, cs.CL, cs.PL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠŸèƒ½ä¸€è‡´æ€§æ¡†æ¶ä»¥æå‡ä»£ç åµŒå…¥æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç åµŒå…¥` `åŠŸèƒ½ä¸€è‡´æ€§` `æ•°æ®åˆæˆ` `æœºå™¨å­¦ä¹ ` `ä»£ç ç†è§£` `åµŒå…¥æ¨¡å‹` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºä»£ç å…‹éš†æ£€æµ‹ï¼Œå¼ºè°ƒè¯­æ³•ç›¸ä¼¼æ€§ï¼Œç¼ºä¹å¯¹ä»£ç åŠŸèƒ½ä¸€è‡´æ€§çš„æ·±å…¥ç†è§£ã€‚
2. æœ¬æ–‡æå‡ºåŠŸèƒ½å¯¼å‘ä»£ç è‡ªæ¼”åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå¤šæ ·åŒ–çš„ä»£ç ç¤ºä¾‹æ¥æ„å»ºæ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼Œæå‡åŠŸèƒ½ä¸€è‡´æ€§è¯„ä¼°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»£ç å…‹éš†æ£€æµ‹ã€åŠŸèƒ½ä¸€è‡´æ€§è¯†åˆ«å’Œä»£ç æ£€ç´¢ç­‰ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨æ¼”åŒ–æ•°æ®é›†çš„åµŒå…¥æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åµŒå…¥æ¨¡å‹åœ¨èšç±»ã€æ£€ç´¢å’Œç‰¹å¾æå–ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨ä»£ç çº§åŠŸèƒ½è¯­ä¹‰çš„åæ˜ èƒ½åŠ›å°šä¸æ˜ç¡®ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºä»£ç å…‹éš†æ£€æµ‹ï¼Œå¼ºè°ƒè¯­æ³•ç›¸ä¼¼æ€§è€Œå¿½è§†åŠŸèƒ½ç†è§£ã€‚æœ¬æ–‡å…³æ³¨LLMä»£ç åµŒå…¥çš„åŠŸèƒ½ä¸€è‡´æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºåŠŸèƒ½å¯¼å‘ä»£ç è‡ªæ¼”åŒ–çš„æ•°æ®åˆæˆæ¡†æ¶ï¼Œä»¥æ„å»ºå¤šæ ·ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ã€‚é€šè¿‡å®šä¹‰å››ç±»è¯­ä¹‰å’Œè¯­æ³•çš„ä»£ç ç¤ºä¾‹ï¼Œå‘ç°ç°æœ‰æ•°æ®é›†ä¸»è¦æ•æ‰è¯­æ³•ç‰¹æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä»å•ä¸ªä»£ç å®ä¾‹ç”Ÿæˆå››ç§ç‹¬ç‰¹å˜ä½“ï¼Œæä¾›æ›´å¹¿æ³›çš„ä»£ç ç¤ºä¾‹ï¼Œåæ˜ åŠŸèƒ½å·®å¼‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒåµŒå…¥æ¨¡å‹åœ¨æˆ‘ä»¬çš„æ¼”åŒ–æ•°æ®é›†ä¸Šè®­ç»ƒåæ˜¾è‘—æå‡æ€§èƒ½ï¼Œæ¨åŠ¨äº†ä»£ç çš„åŠŸèƒ½ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä»£ç åµŒå…¥æ¨¡å‹åœ¨åŠŸèƒ½è¯­ä¹‰åæ˜ ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç°æœ‰ç ”ç©¶è¿‡äºå…³æ³¨è¯­æ³•ç›¸ä¼¼æ€§è€Œå¿½è§†åŠŸèƒ½ä¸€è‡´æ€§çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŠŸèƒ½å¯¼å‘ä»£ç è‡ªæ¼”åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä»å•ä¸€ä»£ç å®ä¾‹ç”Ÿæˆå¤šç§å˜ä½“ï¼Œæ„å»ºå¤šæ ·åŒ–çš„åŸºå‡†æ•°æ®é›†ï¼Œä»¥æ›´å¥½åœ°åæ˜ ä»£ç çš„åŠŸèƒ½å·®å¼‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ¡†æ¶åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä»£ç ç¤ºä¾‹å®šä¹‰ï¼Œæ¶µç›–å››ç±»è¯­ä¹‰å’Œè¯­æ³•ï¼›2) å˜ä½“ç”Ÿæˆæ¨¡å—ï¼Œä»å•ä¸€å®ä¾‹ç”Ÿæˆå››ç§å˜ä½“ï¼›3) æ•°æ®é›†æ„å»ºï¼Œæ•´åˆç”Ÿæˆçš„å˜ä½“ï¼›4) æ€§èƒ½è¯„ä¼°ï¼Œé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡ŒéªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†åŠŸèƒ½å¯¼å‘çš„è‡ªæ¼”åŒ–æœºåˆ¶ï¼Œä½¿å¾—ç”Ÿæˆçš„ä»£ç ç¤ºä¾‹ä¸ä»…åœ¨è¯­æ³•ä¸Šå¤šæ ·åŒ–ï¼Œæ›´åœ¨åŠŸèƒ½ä¸Šå…·æœ‰ä¸€è‡´æ€§ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®åˆæˆè¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†å¤šç§å‚æ•°ä»¥ç¡®ä¿ç”Ÿæˆå˜ä½“çš„å¤šæ ·æ€§ï¼ŒåŒæ—¶é‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åŠŸèƒ½ä¸€è‡´æ€§ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°ä»£ç çš„åŠŸèƒ½è¯­ä¹‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åŠŸèƒ½å¯¼å‘ä»£ç è‡ªæ¼”åŒ–æ¡†æ¶ç”Ÿæˆçš„æ•°æ®é›†ï¼ŒåµŒå…¥æ¨¡å‹åœ¨ä»£ç å…‹éš†æ£€æµ‹ã€åŠŸèƒ½ä¸€è‡´æ€§è¯†åˆ«å’Œä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡20%ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ•°æ®é›†çš„åŸºçº¿è¡¨ç°ï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä»£ç ç†è§£ã€è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œè½¯ä»¶ç»´æŠ¤ç­‰ã€‚é€šè¿‡æå‡ä»£ç åµŒå…¥æ¨¡å‹å¯¹åŠŸèƒ½ä¸€è‡´æ€§çš„ç†è§£ï¼Œèƒ½å¤Ÿåœ¨å®é™…å¼€å‘ä¸­æ›´å¥½åœ°æ”¯æŒä»£ç é‡ç”¨ã€ä¼˜åŒ–å’Œé”™è¯¯æ£€æµ‹ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embedding models have demonstrated strong performance in tasks like clustering, retrieval, and feature extraction while offering computational advantages over generative models and cross-encoders. Benchmarks such as MTEB have shown that text embeddings from large language models (LLMs) capture rich semantic information, but their ability to reflect code-level functional semantics remains unclear. Existing studies largely focus on code clone detection, which emphasizes syntactic similarity and overlooks functional understanding. In this paper, we focus on the functional consistency of LLM code embeddings, which determines if two code snippets perform the same function regardless of syntactic differences. We propose a novel data synthesis framework called Functionality-Oriented Code Self-Evolution to construct diverse and challenging benchmarks. Specifically, we define code examples across four semantic and syntactic categories and find that existing datasets predominantly capture syntactic properties. Our framework generates four unique variations from a single code instance, providing a broader spectrum of code examples that better reflect functional differences. Extensive experiments on three downstream tasks-code clone detection, code functional consistency identification, and code retrieval-demonstrate that embedding models significantly improve their performance when trained on our evolved datasets. These results highlight the effectiveness and generalization of our data synthesis framework, advancing the functional understanding of code.

