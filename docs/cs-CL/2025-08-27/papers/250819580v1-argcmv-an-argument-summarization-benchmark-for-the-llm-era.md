---
layout: default
title: ArgCMV: An Argument Summarization Benchmark for the LLM-era
---

# ArgCMV: An Argument Summarization Benchmark for the LLM-era

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19580" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19580v1</a>
  <a href="https://arxiv.org/pdf/2508.19580.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19580v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19580v1', 'ArgCMV: An Argument Summarization Benchmark for the LLM-era')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Omkar Gurjar, Agam Goyal, Eshwar Chandrasekharan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºArgCMVæ•°æ®é›†ä»¥è§£å†³ç°æœ‰è®ºç‚¹æ‘˜è¦åŸºå‡†ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è®ºç‚¹æ‘˜è¦` `å…³é”®ç‚¹æå–` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®é›†æ„å»º` `åœ¨çº¿è¾©è®º` `å¤æ‚æ€§åˆ†æ` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å…³é”®ç‚¹æå–æ–¹æ³•åœ¨ArgKP21æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œå­˜åœ¨ä»£è¡¨æ€§ä¸è¶³å’Œå¤æ‚æ€§ä½çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ArgCMVæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªçœŸå®åœ¨çº¿è¾©è®ºçš„12Kä¸ªè®ºç‚¹ï¼Œè¦†ç›–æ›´å¹¿æ³›çš„ä¸»é¢˜å’Œå¤æ‚æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ArgCMVæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œæ¨åŠ¨äº†å¯¹æ–°åŸºå‡†çš„éœ€æ±‚å’Œç ”ç©¶æ–¹å‘çš„æ¢ç´¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…³é”®ç‚¹æå–æ˜¯è®ºç‚¹æ‘˜è¦ä¸­çš„é‡è¦ä»»åŠ¡ï¼Œæ¶‰åŠä»è®ºç‚¹ä¸­æå–é«˜å±‚æ¬¡çš„ç®€çŸ­æ‘˜è¦ã€‚ç°æœ‰çš„å…³é”®ç‚¹æå–æ–¹æ³•ä¸»è¦åœ¨æµè¡Œçš„ArgKP21æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æœ¬æ–‡æŒ‡å‡ºäº†ArgKP21æ•°æ®é›†çš„ä¸€äº›ä¸»è¦å±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†éœ€è¦æ›´å…·ä»£è¡¨æ€§çš„æ–°åŸºå‡†ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œç­–åˆ’äº†ä¸€ä¸ªæ–°çš„è®ºç‚¹å…³é”®ç‚¹æå–æ•°æ®é›†ArgCMVï¼ŒåŒ…å«çº¦12Kä¸ªæ¥è‡ªå®é™…åœ¨çº¿äººç±»è¾©è®ºçš„è®ºç‚¹ï¼Œè¦†ç›–3000å¤šä¸ªä¸»é¢˜ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å±•ç°äº†æ›´é«˜çš„å¤æ‚æ€§ï¼ŒåŒ…æ‹¬æ›´é•¿çš„å…±æŒ‡è®ºç‚¹ã€æ›´é«˜çš„ä¸»è§‚è¯è¯­å•å…ƒçš„å­˜åœ¨ï¼Œä»¥åŠæ¯”ArgKP21æ›´å¹¿æ³›çš„ä¸»é¢˜èŒƒå›´ã€‚æˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰æ–¹æ³•åœ¨ArgCMVä¸Šé€‚åº”æ€§ä¸ä½³ï¼Œå¹¶é€šè¿‡å¯¹ç°æœ‰åŸºçº¿å’Œæœ€æ–°å¼€æºæ¨¡å‹çš„å®éªŒæä¾›äº†å¹¿æ³›çš„åŸºå‡†ç»“æœã€‚æ­¤é¡¹å·¥ä½œä¸ºé•¿ä¸Šä¸‹æ–‡åœ¨çº¿è®¨è®ºçš„å…³é”®ç‚¹æå–æ•°æ®é›†å¼•å…¥äº†æ–°è§†è§’ï¼Œä¸ºä¸‹ä¸€ä»£åŸºäºLLMçš„æ‘˜è¦ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è®ºç‚¹æ‘˜è¦åŸºå‡†ï¼ˆArgKP21ï¼‰åœ¨ä»£è¡¨æ€§å’Œå¤æ‚æ€§æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†çœŸå®äººç±»å¯¹è¯æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ä½¿ç”¨æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæœ¬æ–‡ç­–åˆ’äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ArgCMVï¼Œä»¥æ›´å¥½åœ°åæ˜ å®é™…åœ¨çº¿è¾©è®ºçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šArgCMVæ•°æ®é›†çš„æ„å»ºæµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç­›é€‰å’Œæ ‡æ³¨ï¼Œç¡®ä¿æ¶µç›–å¤šæ ·çš„ä¸»é¢˜å’Œå¤æ‚çš„è®ºç‚¹ç»“æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šArgCMVæ•°æ®é›†çš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶é«˜å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼ŒåŒ…å«é•¿ç¯‡å…±æŒ‡è®ºç‚¹å’Œä¸»è§‚è¯è¯­å•å…ƒï¼Œæ˜¾è‘—åŒºåˆ«äºArgKP21çš„ç®€å•ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ä¸¥æ ¼çš„ç­›é€‰æ ‡å‡†å’Œæ ‡æ³¨æŒ‡å—ï¼Œä»¥ç¡®ä¿æ•°æ®çš„è´¨é‡å’Œä»£è¡¨æ€§ï¼ŒåŒæ—¶ä½¿ç”¨äº†æœ€æ–°çš„å¼€æºæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„å…³é”®ç‚¹æå–æ–¹æ³•åœ¨ArgCMVæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä½äºé¢„æœŸï¼Œè¡¨æ˜ç°æœ‰æŠ€æœ¯åœ¨å¤„ç†å¤æ‚åœ¨çº¿è®¨è®ºæ—¶çš„å±€é™æ€§ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†ArgCMVä½œä¸ºæ–°åŸºå‡†çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åœ¨çº¿è¾©è®ºåˆ†æã€ç¤¾äº¤åª’ä½“å†…å®¹æ‘˜è¦å’Œè‡ªåŠ¨åŒ–ä¿¡æ¯æå–ç­‰ã€‚é€šè¿‡æä¾›æ›´å…·ä»£è¡¨æ€§å’Œå¤æ‚æ€§çš„æ•°æ®é›†ï¼ŒArgCMVä¸ºæœªæ¥çš„LLMé©±åŠ¨çš„æ‘˜è¦ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œæ¨åŠ¨äº†ç›¸å…³é¢†åŸŸçš„è¿›æ­¥ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Key point extraction is an important task in argument summarization which involves extracting high-level short summaries from arguments. Existing approaches for KP extraction have been mostly evaluated on the popular ArgKP21 dataset. In this paper, we highlight some of the major limitations of the ArgKP21 dataset and demonstrate the need for new benchmarks that are more representative of actual human conversations. Using SoTA large language models (LLMs), we curate a new argument key point extraction dataset called ArgCMV comprising of around 12K arguments from actual online human debates spread across over 3K topics. Our dataset exhibits higher complexity such as longer, co-referencing arguments, higher presence of subjective discourse units, and a larger range of topics over ArgKP21. We show that existing methods do not adapt well to ArgCMV and provide extensive benchmark results by experimenting with existing baselines and latest open source models. This work introduces a novel KP extraction dataset for long-context online discussions, setting the stage for the next generation of LLM-driven summarization research.

