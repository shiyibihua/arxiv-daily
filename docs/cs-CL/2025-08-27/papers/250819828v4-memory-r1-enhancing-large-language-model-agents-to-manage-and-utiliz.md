---
layout: default
title: Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning
---

# Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19828" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19828v4</a>
  <a href="https://arxiv.org/pdf/2508.19828.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19828v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19828v4', 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Kristian Kersting, Jeff Z. Pan, Hinrich SchÃ¼tze, Volker Tresp, Yunpu Ma

**åˆ†ç±»**: cs.CL, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27 (æ›´æ–°: 2025-10-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMemory-R1ä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„è®°å¿†ç®¡ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `è®°å¿†ç®¡ç†` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ—¶é—´æ¨ç†æ—¶å—é™äºä¸Šä¸‹æ–‡çª—å£ï¼Œç¼ºä¹æœ‰æ•ˆçš„è®°å¿†ç®¡ç†æœºåˆ¶ã€‚
2. Memory-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèµ‹äºˆå¤§è¯­è¨€æ¨¡å‹ä¸»åŠ¨ç®¡ç†å¤–éƒ¨è®°å¿†çš„èƒ½åŠ›ï¼ŒåŒ…å«è®°å¿†ç®¡ç†å™¨å’Œç­”æ¡ˆä»£ç†ã€‚
3. åœ¨ä»…ä½¿ç”¨152ä¸ªè®­ç»ƒé—®ç­”å¯¹çš„æƒ…å†µä¸‹ï¼ŒMemory-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶æœ¬è´¨ä¸Šæ˜¯æ— çŠ¶æ€çš„ï¼Œå—é™äºæœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œéš¾ä»¥è¿›è¡Œé•¿æ—¶é—´çš„æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒMemory-R1æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤Ÿä¸»åŠ¨ç®¡ç†å’Œåˆ©ç”¨å¤–éƒ¨è®°å¿†ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªä¸“é—¨çš„ä»£ç†ï¼šè®°å¿†ç®¡ç†å™¨å’Œç­”æ¡ˆä»£ç†ï¼Œæ¥å­¦ä¹ ç»“æ„åŒ–æ“ä½œï¼Œå¦‚æ·»åŠ ã€æ›´æ–°ã€åˆ é™¤å’Œæ— æ“ä½œã€‚ç»è¿‡ç»“æœé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ŒMemory-R1åœ¨ä»…ä½¿ç”¨152ä¸ªè®­ç»ƒé—®ç­”å¯¹çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œå¹¶åœ¨å¤šç§é—®é¢˜ç±»å‹ã€ä¸‰ä¸ªåŸºå‡†ï¼ˆLoCoMoã€MSCã€LongMemEvalï¼‰å’Œå¤šä¸ªæ¨¡å‹è§„æ¨¡ï¼ˆ3B-14Bï¼‰ä¸Šå®ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿æ—¶é—´æ¨ç†ä¸­çš„æ— çŠ¶æ€æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¸ºé™æ€å’Œå¯å‘å¼é©±åŠ¨ï¼Œç¼ºä¹å­¦ä¹ æœºåˆ¶æ¥å†³å®šå­˜å‚¨ã€æ›´æ–°æˆ–æ£€ç´¢å†…å®¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMemory-R1é€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿å¾—å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨ç®¡ç†å¤–éƒ¨è®°å¿†ï¼Œè®¾è®¡äº†è®°å¿†ç®¡ç†å™¨å’Œç­”æ¡ˆä»£ç†ï¼Œä»¥å®ç°åŠ¨æ€çš„è®°å¿†æ“ä½œå’Œä¿¡æ¯æ£€ç´¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè®°å¿†ç®¡ç†å™¨è´Ÿè´£å­¦ä¹ å¦‚ä½•æ·»åŠ ã€æ›´æ–°ã€åˆ é™¤å’Œæ— æ“ä½œï¼Œç­”æ¡ˆä»£ç†åˆ™é¢„é€‰å¹¶æ¨ç†ç›¸å…³æ¡ç›®ã€‚ä¸¤è€…é€šè¿‡ç»“æœé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ï¼ˆPPOå’ŒGRPOï¼‰è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†å­¦ä¹ æœºåˆ¶æ¥åŠ¨æ€ç®¡ç†å¤–éƒ¨è®°å¿†ï¼Œè€Œä¸æ˜¯ä¾èµ–é™æ€çš„å¯å‘å¼æ–¹æ³•ï¼Œè¿™ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶æ›´å…·çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†152ä¸ªé—®ç­”å¯¹è¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨äº†é€‚åº”æ€§è¾ƒå¼ºçš„æŸå¤±å‡½æ•°å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤šç§é—®é¢˜ç±»å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜è¶Šè¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Memory-R1åœ¨ä»…ä½¿ç”¨152ä¸ªè®­ç»ƒé—®ç­”å¯¹çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†å¤šä¸ªå¼ºåŸºçº¿ï¼Œåœ¨LoCoMoã€MSCå’ŒLongMemEvalç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤„ç†å¤šç§é—®é¢˜ç±»å‹æ—¶ï¼Œæå‡å¹…åº¦æ˜¾è‘—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å¯¹è¯ä»£ç†å’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„è®°å¿†ç®¡ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæå‡å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking a learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns structured operations, including ADD, UPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over relevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management with minimal supervision. With only 152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes across diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and multiple model scales (3B-14B).

