---
layout: default
title: Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval
---

# Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19740" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19740v4</a>
  <a href="https://arxiv.org/pdf/2508.19740.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19740v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19740v4', 'Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27 (æ›´æ–°: 2025-10-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpotlight Attentionä»¥è§£å†³LLMç”Ÿæˆä¸­çš„KVç¼“å­˜æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é”®å€¼ç¼“å­˜` `éçº¿æ€§å“ˆå¸Œ` `æ¨ç†æ•ˆç‡` `CUDAä¼˜åŒ–` `Bradley-TerryæŸå¤±` `æ£€ç´¢ç²¾åº¦` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€é€‰æ‹©KVç¼“å­˜æ—¶æ•ˆç‡ä½ä¸‹ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹LLMsä¸­çš„æŸ¥è¯¢å’Œé”®çš„æ­£äº¤åˆ†å¸ƒé—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºSpotlight Attentionï¼Œé€šè¿‡éçº¿æ€§å“ˆå¸Œå‡½æ•°ä¼˜åŒ–æŸ¥è¯¢å’Œé”®çš„åµŒå…¥åˆ†å¸ƒï¼Œä»è€Œæé«˜ç¼–ç æ•ˆç‡å’Œé²æ£’æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSpotlight Attentionåœ¨æ£€ç´¢ç²¾åº¦ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå“ˆå¸Œç é•¿åº¦ç¼©çŸ­è‡³å°‘5å€ï¼Œä¸”åœ¨A100 GPUä¸Šå®ç°äº†é«˜æ•ˆçš„æ£€ç´¢é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜è´Ÿæ‹…æ˜¾è‘—åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚åŠ¨æ€é€‰æ‹©è§£ç è¿‡ç¨‹ä¸­å…³é”®çš„KVç¼“å­˜æœ‰åŠ©äºä¿æŒæ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨éšæœºçº¿æ€§å“ˆå¸Œæ¥è¯†åˆ«é‡è¦çš„tokenï¼Œä½†ç”±äºLLMsä¸­æŸ¥è¯¢å’Œé”®åœ¨ä¸¤ä¸ªç‹­çª„é”¥ä½“å†…çš„æ­£äº¤åˆ†å¸ƒï¼Œè¿™ç§æ–¹æ³•æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†Spotlight Attentionï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé‡‡ç”¨éçº¿æ€§å“ˆå¸Œå‡½æ•°ä¼˜åŒ–æŸ¥è¯¢å’Œé”®çš„åµŒå…¥åˆ†å¸ƒï¼Œä»è€Œå¢å¼ºç¼–ç æ•ˆç‡å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpotlight Attentionåœ¨æé«˜æ£€ç´¢ç²¾åº¦çš„åŒæ—¶ï¼Œå°†å“ˆå¸Œç é•¿åº¦ç¼©çŸ­è‡³å°‘5å€ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçº¿æ€§å“ˆå¸Œï¼Œå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­KVç¼“å­˜çš„æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰çš„éšæœºçº¿æ€§å“ˆå¸Œæ–¹æ³•åœ¨å¤„ç†æŸ¥è¯¢å’Œé”®çš„æ­£äº¤åˆ†å¸ƒæ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºSpotlight Attentionï¼Œé€šè¿‡éçº¿æ€§å“ˆå¸Œå‡½æ•°ä¼˜åŒ–æŸ¥è¯¢å’Œé”®çš„åµŒå…¥åˆ†å¸ƒï¼Œæ—¨åœ¨æé«˜KVç¼“å­˜çš„æ£€ç´¢æ•ˆç‡å’Œé²æ£’æ€§ã€‚æ­¤è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”LLMsçš„ç‰¹æ€§ï¼Œå…‹æœä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬éçº¿æ€§å“ˆå¸Œæ¨¡å—å’ŒåŸºäºBradley-Terryæ’åçš„æŸå¤±å‡½æ•°ã€‚è¯¥æ¡†æ¶æ”¯æŒåœ¨16GBå†…å­˜çš„GPUä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼Œè®­ç»ƒæ—¶é—´çº¦ä¸º8å°æ—¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé‡‡ç”¨éçº¿æ€§å“ˆå¸Œå‡½æ•°ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢ç²¾åº¦ï¼Œå¹¶å°†å“ˆå¸Œç é•¿åº¦ç¼©çŸ­è‡³å°‘5å€ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„çº¿æ€§å“ˆå¸Œæ–¹æ³•å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šæŸå¤±å‡½æ•°é‡‡ç”¨Bradley-Terryæ’åæœºåˆ¶ï¼Œä¼˜åŒ–äº†éçº¿æ€§å“ˆå¸Œæ¨¡å—çš„è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ä¸“é—¨çš„CUDAå†…æ ¸å®ç°äº†512K tokensçš„å¿«é€Ÿæ£€ç´¢ï¼Œæ£€ç´¢æ—¶é—´ä½äº100å¾®ç§’ï¼Œæ•´ä½“ååé‡æ¯”ä¼ ç»Ÿè§£ç é«˜å‡º3å€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSpotlight Attentionåœ¨æ£€ç´¢ç²¾åº¦ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå“ˆå¸Œç é•¿åº¦ç¼©çŸ­è‡³å°‘5å€ã€‚ä½¿ç”¨ä¸“é—¨çš„CUDAå†…æ ¸ï¼Œ512K tokensçš„æ£€ç´¢æ—¶é—´ä½äº100å¾®ç§’ï¼Œæ•´ä½“ååé‡æ¯”ä¼ ç»Ÿè§£ç æé«˜äº†3å€ï¼Œå±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜LLMsçš„æ¨ç†æ•ˆç‡ï¼ŒSpotlight Attentionèƒ½å¤Ÿåœ¨å®æ—¶åº”ç”¨ä¸­æä¾›æ›´å¿«çš„å“åº”æ—¶é—´ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤§è§„æ¨¡æ¨¡å‹çš„åº”ç”¨ï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$Î¼$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.

