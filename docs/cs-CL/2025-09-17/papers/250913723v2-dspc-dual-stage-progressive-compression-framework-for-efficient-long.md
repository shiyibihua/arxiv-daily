---
layout: default
title: DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning
---

# DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13723" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13723v2</a>
  <a href="https://arxiv.org/pdf/2509.13723.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13723v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13723v2', 'DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yaxin Gao, Yao Lu, Zongfei Zhang, Jiaqi Nie, Shanqing Yu, Qi Xuan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17 (æ›´æ–°: 2025-09-18)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDSPCåŒé˜¶æ®µæ¸è¿›å‹ç¼©æ¡†æ¶ï¼Œæ— éœ€è®­ç»ƒå³å¯é«˜æ•ˆå‹ç¼©é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œæå‡LLMæ¨ç†æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æœ¬å‹ç¼©` `æç¤ºå‹ç¼©` `åŒé˜¶æ®µå‹ç¼©` `å…è®­ç»ƒæ–¹æ³•` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é•¿æ–‡æœ¬å‹ç¼©æ–¹æ³•é€šå¸¸éœ€è¦è®­ç»ƒé¢å¤–çš„è¾…åŠ©æ¨¡å‹ï¼Œå¢åŠ äº†è®¡ç®—è´Ÿæ‹…ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚
2. DSPCæ¡†æ¶é€šè¿‡ç²—ç²’åº¦çš„å¥å­è¿‡æ»¤å’Œç»†ç²’åº¦çš„tokenä¿®å‰ªï¼Œåœ¨ä¸è¿›è¡Œæ¨¡å‹è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„æç¤ºå‹ç¼©ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDSPCåœ¨é•¿æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å‹ç¼©æ–¹æ³•ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ä¸ºäº†è·å¾—æ›´å‡†ç¡®çš„è¾“å‡ºï¼Œé©±åŠ¨LLMsçš„æç¤ºå˜å¾—è¶Šæ¥è¶Šé•¿ï¼Œè¿™å¯¼è‡´äº†æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªæç¤ºè†¨èƒ€é—®é¢˜ï¼Œå·²ç»æå‡ºäº†æç¤ºå‹ç¼©ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éœ€è¦è®­ç»ƒä¸€ä¸ªå°çš„è¾…åŠ©æ¨¡å‹è¿›è¡Œå‹ç¼©ï¼Œä»è€Œå¯¼è‡´å¤§é‡çš„é¢å¤–è®¡ç®—ã€‚ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µã€å…è®­ç»ƒçš„æ–¹æ³•ï¼Œç§°ä¸ºåŒé˜¶æ®µæ¸è¿›å‹ç¼©(DSPC)ã€‚åœ¨ç²—ç²’åº¦é˜¶æ®µï¼Œè¯­ä¹‰ç›¸å…³çš„å¥å­è¿‡æ»¤åŸºäºTF-IDFåˆ é™¤è¯­ä¹‰ä»·å€¼ä½çš„å¥å­ã€‚åœ¨ç»†ç²’åº¦é˜¶æ®µï¼Œä½¿ç”¨æ³¨æ„åŠ›è´¡çŒ®ã€è·¨æ¨¡å‹æŸå¤±å·®å¼‚å’Œä½ç½®é‡è¦æ€§æ¥è¯„ä¼°tokené‡è¦æ€§ï¼Œä»è€Œèƒ½å¤Ÿåœ¨ä¿ç•™è¯­ä¹‰çš„åŒæ—¶ä¿®å‰ªä½æ•ˆç”¨çš„tokenã€‚æˆ‘ä»¬åœ¨LLaMA-3.1-8B-Instructå’ŒGPT-3.5-Turboä¸Šï¼Œåœ¨å—é™çš„tokené¢„ç®—ä¸‹éªŒè¯äº†DSPCï¼Œå¹¶è§‚å¯Ÿåˆ°äº†ä¸€è‡´çš„æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œåœ¨Longbenchæ•°æ®é›†çš„FewShotä»»åŠ¡ä¸­ï¼ŒDSPCä»…ä½¿ç”¨3å€å°‘çš„tokenå°±å®ç°äº†49.17çš„æ€§èƒ½ï¼Œä¼˜äºæœ€ä½³çš„state-of-the-artåŸºçº¿LongLLMLingua 7.76ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ç”±äºé•¿ä¸Šä¸‹æ–‡æç¤ºå¯¼è‡´çš„è®¡ç®—æˆæœ¬é«˜æ˜‚é—®é¢˜ã€‚ç°æœ‰çš„æç¤ºå‹ç¼©æ–¹æ³•é€šå¸¸éœ€è¦è®­ç»ƒé¢å¤–çš„è¾…åŠ©æ¨¡å‹ï¼Œè¿™å¢åŠ äº†è®¡ç®—è´Ÿæ‹…ï¼Œå¹¶ä¸”å¯èƒ½å¼•å…¥é¢å¤–çš„å¤æ‚æ€§ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ— éœ€è®­ç»ƒä¸”é«˜æ•ˆçš„æç¤ºå‹ç¼©æ–¹æ³•ï¼Œä»¥é™ä½LLMsçš„æ¨ç†æˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨ä¸€ç§åŒé˜¶æ®µæ¸è¿›å‹ç¼©ç­–ç•¥ï¼Œå³DSPCï¼Œè¯¥ç­–ç•¥åˆ†ä¸ºç²—ç²’åº¦çš„å¥å­è¿‡æ»¤å’Œç»†ç²’åº¦çš„tokenä¿®å‰ªã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åœ¨ä¿ç•™å…³é”®è¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶ï¼Œé€æ­¥å‡å°‘æç¤ºçš„é•¿åº¦ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚è¿™ç§è®¾è®¡é¿å…äº†è®­ç»ƒé¢å¤–æ¨¡å‹çš„éœ€æ±‚ï¼Œæé«˜äº†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDSPCæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š
1. **ç²—ç²’åº¦å¥å­è¿‡æ»¤**ï¼šä½¿ç”¨TF-IDFç®—æ³•è¯„ä¼°å¥å­çº§åˆ«çš„è¯­ä¹‰é‡è¦æ€§ï¼Œå¹¶ç§»é™¤è¯­ä¹‰ä»·å€¼è¾ƒä½çš„å¥å­ï¼Œä»è€Œå‡å°‘æç¤ºçš„é•¿åº¦ã€‚
2. **ç»†ç²’åº¦tokenä¿®å‰ª**ï¼šé€šè¿‡ç»¼åˆè€ƒè™‘æ³¨æ„åŠ›è´¡çŒ®ã€è·¨æ¨¡å‹æŸå¤±å·®å¼‚å’Œä½ç½®é‡è¦æ€§ç­‰å› ç´ ï¼Œè¯„ä¼°tokençº§åˆ«çš„é‡è¦æ€§ï¼Œå¹¶ä¿®å‰ªä½æ•ˆç”¨çš„tokenï¼Œè¿›ä¸€æ­¥å‹ç¼©æç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šDSPCçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å…è®­ç»ƒçš„è®¾è®¡å’ŒåŒé˜¶æ®µæ¸è¿›å‹ç¼©ç­–ç•¥ã€‚ä¸éœ€è¦è®­ç»ƒè¾…åŠ©æ¨¡å‹çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒDSPCå®Œå…¨ä¾èµ–äºç°æœ‰çš„LLMå’Œç®€å•çš„ç»Ÿè®¡æ–¹æ³•ï¼ˆå¦‚TF-IDFï¼‰è¿›è¡Œå‹ç¼©ï¼Œä»è€Œé¿å…äº†é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚åŒé˜¶æ®µç­–ç•¥èƒ½å¤Ÿæ›´ç²¾ç»†åœ°æ§åˆ¶å‹ç¼©è¿‡ç¨‹ï¼Œåœ¨ä¿è¯è¯­ä¹‰å®Œæ•´æ€§çš„å‰æä¸‹ï¼Œå®ç°æ›´é«˜çš„å‹ç¼©ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼š
* **TF-IDFå¥å­è¿‡æ»¤**ï¼šä½¿ç”¨TF-IDFå€¼ä½œä¸ºå¥å­è¯­ä¹‰é‡è¦æ€§çš„åº¦é‡ï¼Œè®¾å®šé˜ˆå€¼æ¥å†³å®šå“ªäº›å¥å­è¢«ä¿ç•™ã€‚
* **Tokené‡è¦æ€§è¯„ä¼°**ï¼šç»¼åˆè€ƒè™‘ä»¥ä¸‹ä¸‰ä¸ªå› ç´ ï¼š
    * æ³¨æ„åŠ›è´¡çŒ®ï¼šåŸºäºLLMçš„æ³¨æ„åŠ›æƒé‡æ¥è¯„ä¼°tokençš„é‡è¦æ€§ã€‚
    * è·¨æ¨¡å‹æŸå¤±å·®å¼‚ï¼šé€šè¿‡æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œå‹ç¼©åæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æŸå¤±å·®å¼‚æ¥è¯„ä¼°tokençš„é‡è¦æ€§ã€‚
    * ä½ç½®é‡è¦æ€§ï¼šæ ¹æ®tokenåœ¨æç¤ºä¸­çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œå¼€å¤´æˆ–ç»“å°¾ï¼‰æ¥èµ‹äºˆä¸åŒçš„é‡è¦æ€§æƒé‡ã€‚
* **Tokenä¿®å‰ªç­–ç•¥**ï¼šæ ¹æ®tokençš„é‡è¦æ€§å¾—åˆ†ï¼Œè®¾å®šé˜ˆå€¼æ¥å†³å®šå“ªäº›tokenè¢«ä¿®å‰ªã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DSPCåœ¨Longbenchæ•°æ®é›†çš„FewShotä»»åŠ¡ä¸­ï¼Œä»…ä½¿ç”¨3å€å°‘çš„tokenå°±å®ç°äº†49.17çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å½“å‰æœ€ä½³çš„åŸºçº¿æ–¹æ³•LongLLMLingua 7.76ä¸ªç™¾åˆ†ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSPCèƒ½å¤Ÿåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æå‡LLMçš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DSPCæ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿æ–‡æœ¬è¾“å…¥çš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚ï¼šæ–‡æ¡£æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡é™ä½LLMçš„è®¡ç®—æˆæœ¬ï¼ŒDSPCèƒ½å¤Ÿæé«˜æ¨ç†é€Ÿåº¦ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ï¼Œå¹¶ä½¿å¾—åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡ŒLLMæˆä¸ºå¯èƒ½ã€‚æœªæ¥ï¼ŒDSPCå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤šæ¨¡æ€åœºæ™¯ï¼Œä¾‹å¦‚å‹ç¼©å›¾åƒæˆ–éŸ³é¢‘ç­‰è¾“å…¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable success in many natural language processing (NLP) tasks. To achieve more accurate output, the prompts used to drive LLMs have become increasingly longer, which incurs higher computational costs. To address this prompt inflation problem, prompt compression has been proposed. However, most existing methods require training a small auxiliary model for compression, incurring a significant amount of additional computation. To avoid this, we propose a two-stage, training-free approach, called Dual-Stage Progressive Compression (DSPC). In the coarse-grained stage, semantic-related sentence filtering removes sentences with low semantic value based on TF-IDF. In the fine-grained stage, token importance is assessed using attention contribution, cross-model loss difference, and positional importance, enabling the pruning of low-utility tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo under a constrained token budget and observe consistent improvements. For instance, in the FewShot task of the Longbench dataset, DSPC achieves a performance of 49.17 by using only 3x fewer tokens, outperforming the best state-of-the-art baseline LongLLMLingua by 7.76.

