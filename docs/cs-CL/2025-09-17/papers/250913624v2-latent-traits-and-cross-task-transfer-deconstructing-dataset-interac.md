---
layout: default
title: Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning
---

# Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13624" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13624v2</a>
  <a href="https://arxiv.org/pdf/2509.13624.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13624v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13624v2', 'Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shambhavi Krishna, Atharva Naik, Chaitali Agarwal, Sudharshan Govindan, Taesung Lee, Haw-Shiuan Chang

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17 (æ›´æ–°: 2025-11-08)

**å¤‡æ³¨**: Proceedings of the 14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ½œåœ¨ç‰¹å¾å’Œè·¨ä»»åŠ¡è¿ç§»ï¼Œè§£æ„LLMå¾®è°ƒä¸­çš„æ•°æ®é›†äº¤äº’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¿ç§»å­¦ä¹ ` `æ•°æ®é›†äº¤äº’` `æ½œåœ¨ç‰¹å¾` `å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ä¸åŒç‰¹å¾æ•°æ®é›†è¿›è¡ŒLLMå¾®è°ƒï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åˆ†ææ¡†æ¶ï¼Œé€šè¿‡è¿ç§»å­¦ä¹ çŸ©é˜µå’Œé™ç»´æŠ€æœ¯ï¼Œå‰–æè·¨ä»»åŠ¡äº¤äº’ï¼Œæ­ç¤ºæ½œåœ¨èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ•°æ®é›†çš„éšè—ç»Ÿè®¡å› ç´ å’Œè¯­è¨€ç‰¹å¾æ¯”è¡¨é¢ç›¸ä¼¼æ€§æ›´èƒ½å½±å“è¿ç§»å­¦ä¹ æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°éƒ¨ç½²åœ¨å„ç§åº”ç”¨ä¸­ï¼Œè¿™äº›åº”ç”¨é€šå¸¸åŒ…å«LLMåœ¨è®­ç»ƒæœŸé—´æœªé‡åˆ°çš„ä»»åŠ¡ã€‚è¿™æ„å‘³ç€æšä¸¾å’Œè·å–æ‰€æœ‰ä»»åŠ¡çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®æ˜¯ä¸å¯è¡Œçš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦ä¾èµ–äºä½¿ç”¨å…·æœ‰ä¸åŒç‰¹å¾çš„æ•°æ®é›†è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œå¹¶é¢„æµ‹åˆ†å¸ƒå¤–çš„è¯·æ±‚ã€‚å—åˆ°è¿™ç§å®é™…éœ€æ±‚çš„é©±åŠ¨ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œæ„å»ºè¿ç§»å­¦ä¹ çŸ©é˜µå’Œé™ç»´ï¼Œä»¥å‰–æè¿™äº›è·¨ä»»åŠ¡äº¤äº’ã€‚æˆ‘ä»¬è®­ç»ƒå¹¶åˆ†æäº†10ä¸ªæ¨¡å‹ï¼Œä»¥è¯†åˆ«æ½œåœ¨èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼Œæ¨ç†ã€æƒ…æ„Ÿåˆ†ç±»ã€NLUã€ç®—æœ¯ï¼‰ï¼Œå¹¶å‘ç°è¿ç§»å­¦ä¹ çš„å‰¯ä½œç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ€§èƒ½çš„æé«˜å¸¸å¸¸æ— æ³•ç”¨è¡¨é¢å±‚é¢çš„æ•°æ®é›†ç›¸ä¼¼æ€§æˆ–æºæ•°æ®è´¨é‡æ¥è§£é‡Šã€‚ç›¸åï¼Œæºæ•°æ®é›†çš„éšè—ç»Ÿè®¡å› ç´ ï¼Œå¦‚ç±»åˆ«åˆ†å¸ƒå’Œç”Ÿæˆé•¿åº¦çš„å€¾å‘ï¼Œä»¥åŠç‰¹å®šçš„è¯­è¨€ç‰¹å¾ï¼Œå®é™…ä¸Šæ›´å…·å½±å“åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºè¿ç§»å­¦ä¹ çš„å¤æ‚åŠ¨æ€æä¾›äº†è§è§£ï¼Œä¸ºæ›´å¯é¢„æµ‹å’Œæœ‰æ•ˆçš„LLMé€‚åº”é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨é¢†åŸŸåº”ç”¨ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨ä¸åŒç‰¹å¾çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œä»¥æå‡æ¨¡å‹åœ¨æœªè§è¿‡ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºè¡¨é¢å±‚é¢çš„æ•°æ®é›†ç›¸ä¼¼æ€§æˆ–æºæ•°æ®è´¨é‡ï¼Œä½†å¿½ç•¥äº†æ•°æ®é›†å†…éƒ¨éšè—çš„ç»Ÿè®¡å› ç´ å’Œè¯­è¨€ç‰¹å¾ï¼Œå¯¼è‡´è¿ç§»å­¦ä¹ æ•ˆæœä¸ç¨³å®šï¼Œéš¾ä»¥é¢„æµ‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºè¿ç§»å­¦ä¹ çŸ©é˜µå’Œåº”ç”¨é™ç»´æŠ€æœ¯ï¼Œæ¥åˆ†æä¸åŒæ•°æ®é›†ä¹‹é—´çš„äº¤äº’ä½œç”¨ï¼Œä»è€Œæ­ç¤ºå½±å“è¿ç§»å­¦ä¹ æ•ˆæœçš„å…³é”®å› ç´ ã€‚è¿™ç§æ–¹æ³•ä¸å†ä»…ä»…å…³æ³¨æ•°æ®é›†çš„è¡¨é¢ç‰¹å¾ï¼Œè€Œæ˜¯æ·±å…¥æŒ–æ˜æ•°æ®é›†å†…éƒ¨çš„æ½œåœ¨ç‰¹å¾å’Œç»Ÿè®¡è§„å¾‹ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°æ•°æ®é›†çš„è¿ç§»æ½œåŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ„å»ºåŒ…å«å¤šä¸ªä»»åŠ¡çš„æ•°æ®é›†ï¼›2) ä½¿ç”¨è¿™äº›æ•°æ®é›†å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œæ„å»ºè¿ç§»å­¦ä¹ çŸ©é˜µï¼Œè®°å½•ä¸åŒæ•°æ®é›†ä¹‹é—´çš„è¿ç§»æ•ˆæœï¼›3) åº”ç”¨é™ç»´æŠ€æœ¯ï¼ˆå¦‚ä¸»æˆåˆ†åˆ†æï¼‰å¯¹è¿ç§»å­¦ä¹ çŸ©é˜µè¿›è¡Œåˆ†æï¼Œæå–æ½œåœ¨ç‰¹å¾ï¼›4) åˆ†ææ½œåœ¨ç‰¹å¾ä¸æ•°æ®é›†ç»Ÿè®¡ç‰¹å¾ï¼ˆå¦‚ç±»åˆ«åˆ†å¸ƒã€ç”Ÿæˆé•¿åº¦ï¼‰å’Œè¯­è¨€ç‰¹å¾ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæ­ç¤ºå½±å“è¿ç§»å­¦ä¹ æ•ˆæœçš„å…³é”®å› ç´ ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„åˆ†ææ¡†æ¶ï¼Œç”¨äºè§£æ„LLMå¾®è°ƒä¸­æ•°æ®é›†ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚è¯¥æ¡†æ¶ä¸ä»…è€ƒè™‘äº†æ•°æ®é›†çš„è¡¨é¢ç‰¹å¾ï¼Œè¿˜æ·±å…¥æŒ–æ˜äº†æ•°æ®é›†å†…éƒ¨çš„éšè—ç»Ÿè®¡å› ç´ å’Œè¯­è¨€ç‰¹å¾ï¼Œä»è€Œæ›´å…¨é¢åœ°è¯„ä¼°æ•°æ®é›†çš„è¿ç§»æ½œåŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹è¿ç§»å­¦ä¹ æ•ˆæœï¼Œå¹¶ä¸ºLLMçš„æœ‰æ•ˆé€‚åº”æä¾›æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è¿ç§»å­¦ä¹ çŸ©é˜µçš„æ„å»ºæ–¹å¼ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œä»¥åŠå¦‚ä½•è¡¡é‡ä¸åŒæ•°æ®é›†ä¹‹é—´çš„è¿ç§»æ•ˆæœï¼›2) é™ç»´æŠ€æœ¯çš„é€‰æ‹©ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„é™ç»´ç®—æ³•ï¼Œä»¥æå–æœ€å…·ä»£è¡¨æ€§çš„æ½œåœ¨ç‰¹å¾ï¼›3) æ•°æ®é›†ç»Ÿè®¡ç‰¹å¾å’Œè¯­è¨€ç‰¹å¾çš„æå–æ–¹æ³•ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„æŒ‡æ ‡æ¥é‡åŒ–æ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒã€ç”Ÿæˆé•¿åº¦å’Œè¯­è¨€ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ•°æ®é›†çš„éšè—ç»Ÿè®¡å› ç´ ï¼ˆå¦‚ç±»åˆ«åˆ†å¸ƒå’Œç”Ÿæˆé•¿åº¦çš„å€¾å‘ï¼‰ä»¥åŠç‰¹å®šçš„è¯­è¨€ç‰¹å¾æ¯”è¡¨é¢å±‚é¢çš„æ•°æ®é›†ç›¸ä¼¼æ€§æˆ–æºæ•°æ®è´¨é‡æ›´èƒ½å½±å“è¿ç§»å­¦ä¹ æ•ˆæœã€‚é€šè¿‡åˆ†æ10ä¸ªæ¨¡å‹çš„è®­ç»ƒç»“æœï¼Œè®ºæ–‡æ­ç¤ºäº†æ¨ç†ã€æƒ…æ„Ÿåˆ†ç±»ã€NLUå’Œç®—æœ¯ç­‰æ½œåœ¨èƒ½åŠ›åœ¨è¿ç§»å­¦ä¹ ä¸­çš„ä½œç”¨ï¼Œå¹¶å‘ç°äº†è¿ç§»å­¦ä¹ çš„å‰¯ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è·¨é¢†åŸŸçŸ¥è¯†è¿ç§»çš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚é€šè¿‡åˆ†æä¸åŒæ•°æ®é›†ä¹‹é—´çš„äº¤äº’ä½œç”¨ï¼Œå¯ä»¥é€‰æ‹©æ›´åˆé€‚çš„æºæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥å¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£LLMçš„å†…éƒ¨æœºåˆ¶ï¼Œä¸ºLLMçš„ä¼˜åŒ–å’Œæ”¹è¿›æä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models are increasingly deployed across diverse applications. This often includes tasks LLMs have not encountered during training. This implies that enumerating and obtaining the high-quality training data for all tasks is infeasible. Thus, we often need to rely on transfer learning using datasets with different characteristics, and anticipate out-of-distribution requests. Motivated by this practical need, we propose an analysis framework, building a transfer learning matrix and dimensionality reduction, to dissect these cross-task interactions. We train and analyze 10 models to identify latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic) and discover the side effects of the transfer learning. Our findings reveal that performance improvements often defy explanations based on surface-level dataset similarity or source data quality. Instead, hidden statistical factors of the source dataset, such as class distribution and generation length proclivities, alongside specific linguistic features, are actually more influential. This work offers insights into the complex dynamics of transfer learning, paving the way for more predictable and effective LLM adaptation.

