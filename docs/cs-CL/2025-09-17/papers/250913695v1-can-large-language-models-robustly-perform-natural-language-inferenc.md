---
layout: default
title: Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?
---

# Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13695" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13695v1</a>
  <a href="https://arxiv.org/pdf/2509.13695.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13695v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13695v1', 'Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yosuke Mikami, Daiki Matsuoka, Hitomi Yanaka

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

**å¤‡æ³¨**: To appear in Proceedings of the 16th International Conference on Computational Semantics (IWCS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ„å»ºæ—¥è¯­æ¯”è¾ƒå¥NLIæ•°æ®é›†ï¼Œè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªç„¶è¯­è¨€æ¨ç†` `æ—¥è¯­` `æ¯”è¾ƒå¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®é›†` `é›¶æ ·æœ¬å­¦ä¹ ` `å°‘æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰NLIæ–¹æ³•åœ¨å¤„ç†åŒ…å«æ•°å€¼å’Œé€»è¾‘è¡¨è¾¾å¼çš„æ¯”è¾ƒå¥æ¨ç†æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ—¥è¯­ç­‰éä¸»æµè¯­è¨€ä¸Šã€‚
2. è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªæ—¥è¯­æ¯”è¾ƒå¥NLIæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨æ­¤ä»»åŠ¡ä¸Šçš„é²æ£’æ€§ï¼Œå¹¶æ¢ç´¢æœ‰æ•ˆçš„æç¤ºæ–¹æ³•ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMsçš„æ€§èƒ½å—æç¤ºæ ¼å¼å’Œå°‘æ ·æœ¬ç¤ºä¾‹æ ‡ç­¾çš„å½±å“ï¼Œä¸”éš¾ä»¥å¤„ç†æ—¥è¯­ç‰¹æœ‰çš„è¯­è¨€ç°è±¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œæ¶‰åŠæ•°å€¼å’Œé€»è¾‘è¡¨è¾¾å¼çš„NLIä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ¯”è¾ƒå¥æ˜¯ä¸æ­¤ç±»æ¨ç†ç›¸å…³çš„å…³é”®è¯­è¨€ç°è±¡ï¼Œä½†LLMså¤„ç†æ¯”è¾ƒå¥çš„é²æ£’æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹è®­ç»ƒæ•°æ®ä¸­ä¸å ä¸»å¯¼åœ°ä½çš„è¯­è¨€ï¼ˆå¦‚æ—¥è¯­ï¼‰æ–¹é¢ï¼Œå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¸“æ³¨äºæ¯”è¾ƒå¥çš„æ—¥è¯­NLIæ•°æ®é›†ï¼Œå¹¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­è¯„ä¼°äº†å„ç§LLMsã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹çš„æ€§èƒ½å¯¹é›¶æ ·æœ¬è®¾ç½®ä¸­çš„æç¤ºæ ¼å¼æ•æ„Ÿï¼Œå¹¶å—åˆ°å°‘æ ·æœ¬ç¤ºä¾‹ä¸­é»„é‡‘æ ‡ç­¾çš„å½±å“ã€‚LLMsä¹Ÿéš¾ä»¥å¤„ç†æ—¥è¯­ç‰¹æœ‰çš„è¯­è¨€ç°è±¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ŒåŒ…å«é€»è¾‘è¯­ä¹‰è¡¨ç¤ºçš„æç¤ºæœ‰åŠ©äºæ¨¡å‹é¢„æµ‹æ­£ç¡®æ ‡ç­¾ï¼Œå³ä½¿åœ¨å°‘æ ·æœ¬ç¤ºä¾‹ä¸­éš¾ä»¥è§£å†³çš„æ¨ç†é—®é¢˜ä¹Ÿèƒ½å¾—åˆ°è§£å†³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ—¥è¯­æ¯”è¾ƒå¥è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰ä»»åŠ¡ä¸Šçš„é²æ£’æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ¶‰åŠæ•°å€¼å’Œé€»è¾‘è¡¨è¾¾å¼çš„æ¯”è¾ƒå¥æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ—¥è¯­è¿™ç§è®­ç»ƒæ•°æ®ç›¸å¯¹è¾ƒå°‘çš„è¯­è¨€ä¸Šï¼Œç¼ºä¹é’ˆå¯¹æ€§çš„æ•°æ®é›†å’Œè¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªä¸“é—¨é’ˆå¯¹æ—¥è¯­æ¯”è¾ƒå¥çš„NLIæ•°æ®é›†ï¼Œæ¥ç³»ç»Ÿåœ°è¯„ä¼°LLMsåœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚åŒæ—¶ï¼Œæ¢ç´¢ä¸åŒçš„æç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ä»¥åŠåŒ…å«é€»è¾‘è¯­ä¹‰è¡¨ç¤ºçš„æç¤ºï¼Œä»¥æé«˜LLMsçš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ„å»ºæ—¥è¯­æ¯”è¾ƒå¥NLIæ•°æ®é›†ï¼›2) åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼Œä½¿ç”¨ä¸åŒçš„æç¤ºæ ¼å¼è¯„ä¼°å„ç§LLMsï¼›3) åˆ†æLLMsåœ¨å¤„ç†æ—¥è¯­ç‰¹æœ‰è¯­è¨€ç°è±¡æ—¶çš„è¡¨ç°ï¼›4) æ¢ç´¢åŒ…å«é€»è¾‘è¯­ä¹‰è¡¨ç¤ºçš„æç¤ºå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹æ—¥è¯­æ¯”è¾ƒå¥çš„NLIæ•°æ®é›†ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†åŒ…å«é€»è¾‘è¯­ä¹‰è¡¨ç¤ºçš„æç¤ºæ–¹æ³•ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨æé«˜LLMsæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•°æ®é›†çš„æ„å»ºï¼Œéœ€è¦ä»”ç»†è®¾è®¡æ¯”è¾ƒå¥çš„ç±»å‹å’Œéš¾åº¦ï¼Œä»¥åŠå¯¹åº”çš„æ¨ç†æ ‡ç­¾ï¼›2) æç¤ºæ ¼å¼çš„é€‰æ‹©ï¼Œéœ€è¦è€ƒè™‘ä¸åŒæ ¼å¼å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼›3) é€»è¾‘è¯­ä¹‰è¡¨ç¤ºçš„æ„å»ºï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„è¡¨ç¤ºæ–¹æ³•ï¼Œå¹¶å°†å…¶èå…¥åˆ°æç¤ºä¸­ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ—¥è¯­æ¯”è¾ƒå¥NLIä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯¹æç¤ºæ ¼å¼éå¸¸æ•æ„Ÿã€‚åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­ï¼Œé»„é‡‘æ ‡ç­¾ä¼šå½±å“æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚æ­¤å¤–ï¼ŒLLMséš¾ä»¥å¤„ç†æ—¥è¯­ç‰¹æœ‰çš„è¯­è¨€ç°è±¡ã€‚ç„¶è€Œï¼ŒåŒ…å«é€»è¾‘è¯­ä¹‰è¡¨ç¤ºçš„æç¤ºå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å›°éš¾æ¨ç†é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ç­‰è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†æ—¥è¯­æ¯”è¾ƒå¥çš„åœºæ™¯ä¸‹ã€‚é€šè¿‡æé«˜LLMsåœ¨æ¯”è¾ƒå¥æ¨ç†æ–¹é¢çš„é²æ£’æ€§ï¼Œå¯ä»¥æå‡ç›¸å…³åº”ç”¨çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›åŸºå‡†å’Œæ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) perform remarkably well in Natural Language Inference (NLI). However, NLI involving numerical and logical expressions remains challenging. Comparatives are a key linguistic phenomenon related to such inference, but the robustness of LLMs in handling them, especially in languages that are not dominant in the models' training data, such as Japanese, has not been sufficiently explored. To address this gap, we construct a Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in zero-shot and few-shot settings. Our results show that the performance of the models is sensitive to the prompt formats in the zero-shot setting and influenced by the gold labels in the few-shot examples. The LLMs also struggle to handle linguistic phenomena unique to Japanese. Furthermore, we observe that prompts containing logical semantic representations help the models predict the correct labels for inference problems that they struggle to solve even with few-shot examples.

