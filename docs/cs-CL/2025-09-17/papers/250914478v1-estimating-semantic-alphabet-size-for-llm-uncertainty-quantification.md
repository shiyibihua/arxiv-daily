---
layout: default
title: Estimating Semantic Alphabet Size for LLM Uncertainty Quantification
---

# Estimating Semantic Alphabet Size for LLM Uncertainty Quantification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14478" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14478v1</a>
  <a href="https://arxiv.org/pdf/2509.14478.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14478v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14478v1', 'Estimating Semantic Alphabet Size for LLM Uncertainty Quantification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lucas H. McCabe, Rimon Melamed, Thomas Hartvigsen, H. Howie Huang

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ”¹è¿›çš„è¯­ä¹‰å­—æ¯è¡¨å¤§å°ä¼°è®¡å™¨ï¼Œæå‡LLMä¸ç¡®å®šæ€§é‡åŒ–çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸ç¡®å®šæ€§é‡åŒ–` `è¯­ä¹‰ç†µ` `å¹»è§‰æ£€æµ‹` `è¯­ä¹‰å­—æ¯è¡¨å¤§å°ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºé‡‡æ ·çš„LLMä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åœ¨å®é™…åº”ç”¨ä¸­æ¨å¹¿ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ”¹è¿›çš„è¯­ä¹‰å­—æ¯è¡¨å¤§å°ä¼°è®¡å™¨ï¼Œç”¨äºæ ¡æ­£ç¦»æ•£è¯­ä¹‰ç†µï¼Œä»è€Œæ›´å‡†ç¡®åœ°ä¼°è®¡LLMçš„ä¸ç¡®å®šæ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨LLMå¹»è§‰æ£€æµ‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”ä¿æŒäº†é«˜åº¦çš„å¯è§£é‡Šæ€§ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šç”¨äºé‡åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç¡®å®šæ€§çš„é»‘ç›’æŠ€æœ¯ä¾èµ–äºé‡å¤çš„LLMé‡‡æ ·ï¼Œè¿™åœ¨è®¡ç®—ä¸Šå¯èƒ½éå¸¸æ˜‚è´µã€‚å› æ­¤ï¼Œå®é™…åº”ç”¨éœ€è¦ä»å°‘é‡æ ·æœ¬ä¸­è¿›è¡Œå¯é çš„ä¼°è®¡ã€‚è¯­ä¹‰ç†µï¼ˆSEï¼‰æ˜¯ä¸€ç§æµè¡Œçš„åŸºäºæ ·æœ¬çš„ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ï¼Œå…¶ç¦»æ•£å…¬å¼å¯¹äºé»‘ç›’è®¾ç½®å¾ˆæœ‰å¸å¼•åŠ›ã€‚æœ€è¿‘è¯­ä¹‰ç†µçš„æ‰©å±•åœ¨æ”¹è¿›LLMå¹»è§‰æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä½†ä½¿ç”¨äº†è¾ƒå°‘å¯è§£é‡Šçš„æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†é¢å¤–çš„è¶…å‚æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†è§„èŒƒçš„ç¦»æ•£è¯­ä¹‰ç†µä¼°è®¡å™¨ï¼Œå‘ç°å®ƒä½ä¼°äº†â€œçœŸå®â€çš„è¯­ä¹‰ç†µï¼Œæ­£å¦‚ç†è®ºæ‰€é¢„æœŸçš„é‚£æ ·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„è¯­ä¹‰å­—æ¯è¡¨å¤§å°ä¼°è®¡å™¨ï¼Œå¹¶è¯´æ˜ä½¿ç”¨å®ƒæ¥è°ƒæ•´ç¦»æ•£è¯­ä¹‰ç†µä»¥è¿›è¡Œæ ·æœ¬è¦†ç›–ï¼Œå¯ä»¥åœ¨æˆ‘ä»¬æ„Ÿå…´è¶£çš„è®¾ç½®ä¸­è·å¾—æ›´å‡†ç¡®çš„è¯­ä¹‰ç†µä¼°è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„å­—æ¯è¡¨å¤§å°ä¼°è®¡å™¨èƒ½å¤Ÿå¾ˆå¥½åœ°æ ‡è®°ä¸æ­£ç¡®çš„LLMå“åº”ï¼Œç”šè‡³ä¼˜äºæœ€è¿‘è¡¨ç°æœ€ä½³çš„æ–¹æ³•ï¼Œå¹¶ä¸”å…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§çš„ä¼˜ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè¯­ä¹‰ç†µçš„LLMä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ç¦»æ•£è¯­ä¹‰ç†µä¼°è®¡å™¨ï¼Œåœ¨æ ·æœ¬é‡è¾ƒå°‘æ—¶ä¼šä½ä¼°çœŸå®çš„è¯­ä¹‰ç†µï¼Œå¯¼è‡´ä¸å‡†ç¡®çš„ä¸ç¡®å®šæ€§è¯„ä¼°ã€‚åŒæ—¶ï¼Œä¸€äº›æ”¹è¿›çš„è¯­ä¹‰ç†µæ–¹æ³•è™½ç„¶æé«˜äº†æ€§èƒ½ï¼Œä½†å¼•å…¥äº†é¢å¤–çš„è¶…å‚æ•°ï¼Œé™ä½äº†å¯è§£é‡Šæ€§ï¼Œå¢åŠ äº†è°ƒå‚éš¾åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ”¹è¿›è¯­ä¹‰å­—æ¯è¡¨å¤§å°çš„ä¼°è®¡ï¼Œé€šè¿‡æ›´å‡†ç¡®åœ°ä¼°è®¡è¯­ä¹‰ç©ºé—´çš„å¤§å°ï¼Œæ¥æ ¡æ­£ç¦»æ•£è¯­ä¹‰ç†µçš„åå·®ã€‚æ ¸å¿ƒåœ¨äºè®¤è¯†åˆ°ç¦»æ•£è¯­ä¹‰ç†µçš„ä½ä¼°æºäºå¯¹è¯­ä¹‰ç©ºé—´è¦†ç›–ä¸è¶³ï¼Œå› æ­¤éœ€è¦æ›´å‡†ç¡®åœ°ä¼°è®¡è¯­ä¹‰å­—æ¯è¡¨çš„å¤§å°ï¼Œä»è€Œå¯¹è¯­ä¹‰ç†µè¿›è¡Œä¿®æ­£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä»LLMä¸­é‡‡æ ·ç”Ÿæˆå¤šä¸ªæ–‡æœ¬æ ·æœ¬ï¼›2) ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­ä¹‰ç¼–ç å™¨ï¼ˆå¦‚Sentence-BERTï¼‰å°†æ–‡æœ¬æ ·æœ¬ç¼–ç ä¸ºè¯­ä¹‰å‘é‡ï¼›3) æå‡ºæ–°çš„è¯­ä¹‰å­—æ¯è¡¨å¤§å°ä¼°è®¡å™¨ï¼Œä¼°è®¡è¯­ä¹‰ç©ºé—´çš„å¤§å°ï¼›4) ä½¿ç”¨ä¼°è®¡çš„è¯­ä¹‰å­—æ¯è¡¨å¤§å°æ¥è°ƒæ•´ç¦»æ•£è¯­ä¹‰ç†µï¼Œå¾—åˆ°æ ¡æ­£åçš„è¯­ä¹‰ç†µä¼°è®¡å€¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªæ”¹è¿›çš„è¯­ä¹‰å­—æ¯è¡¨å¤§å°ä¼°è®¡å™¨ã€‚è¯¥ä¼°è®¡å™¨èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ LLMç”Ÿæˆçš„æ–‡æœ¬æ ·æœ¬æ‰€è¦†ç›–çš„è¯­ä¹‰ç©ºé—´å¤§å°ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ ¡æ­£ç¦»æ•£è¯­ä¹‰ç†µçš„åå·®ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜ä¸ç¡®å®šæ€§é‡åŒ–å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†é«˜åº¦çš„å¯è§£é‡Šæ€§ï¼Œé¿å…äº†å¼•å…¥é¢å¤–çš„è¶…å‚æ•°ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„å­—æ¯è¡¨å¤§å°ä¼°è®¡å™¨è®¾è®¡ç»†èŠ‚æœªçŸ¥ï¼Œè®ºæ–‡ä¸­å¯èƒ½æ¶‰åŠå…·ä½“çš„æ•°å­¦å…¬å¼å’Œç®—æ³•å®ç°ã€‚æ¨æµ‹å¯èƒ½åˆ©ç”¨äº†æ ·æœ¬çš„è¯­ä¹‰å‘é‡åˆ†å¸ƒä¿¡æ¯ï¼Œä¾‹å¦‚å‘é‡ä¹‹é—´çš„è·ç¦»ã€å¯†åº¦ç­‰ï¼Œæ¥ä¼°è®¡è¯­ä¹‰ç©ºé—´çš„å¤§å°ã€‚æ­¤å¤–ï¼Œå¦‚ä½•å°†ä¼°è®¡çš„å­—æ¯è¡¨å¤§å°æœ‰æ•ˆåœ°èå…¥åˆ°ç¦»æ•£è¯­ä¹‰ç†µçš„è®¡ç®—ä¸­ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå…³é”®çš„è®¾è®¡ç‚¹ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æå‡ºçš„æ”¹è¿›è¯­ä¹‰å­—æ¯è¡¨å¤§å°ä¼°è®¡å™¨ï¼Œåœ¨LLMå¹»è§‰æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«ä¸æ­£ç¡®çš„LLMå“åº”ï¼Œæ€§èƒ½ä¼˜äºæˆ–è‡³å°‘ä¸å½“å‰æœ€ä¼˜æ–¹æ³•æŒå¹³ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¯¥æ–¹æ³•ä¿æŒäº†é«˜åº¦çš„å¯è§£é‡Šæ€§ï¼Œæ— éœ€é¢å¤–çš„è¶…å‚æ•°è°ƒæ•´ï¼Œæ›´æ˜“äºå®é™…åº”ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è¯„ä¼°LLMè¾“å‡ºå¯é æ€§çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šè‡ªåŠ¨é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡å‡†ç¡®é‡åŒ–LLMçš„ä¸ç¡®å®šæ€§ï¼Œå¯ä»¥æé«˜ç³»ç»Ÿçš„é²æ£’æ€§å’Œå®‰å…¨æ€§ï¼Œé¿å…å› LLMäº§ç”Ÿé”™è¯¯æˆ–å¹»è§‰è€Œå¯¼è‡´çš„ä¸è‰¯åæœã€‚è¯¥æ–¹æ³•è¿˜å¯ç”¨äºè¯„ä¼°ä¸åŒLLMçš„å¯é æ€§ï¼Œä¸ºæ¨¡å‹é€‰æ‹©æä¾›ä¾æ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Many black-box techniques for quantifying the uncertainty of large language models (LLMs) rely on repeated LLM sampling, which can be computationally expensive. Therefore, practical applicability demands reliable estimation from few samples. Semantic entropy (SE) is a popular sample-based uncertainty estimator with a discrete formulation attractive for the black-box setting. Recent extensions of semantic entropy exhibit improved LLM hallucination detection, but do so with less interpretable methods that admit additional hyperparameters. For this reason, we revisit the canonical discrete semantic entropy estimator, finding that it underestimates the "true" semantic entropy, as expected from theory. We propose a modified semantic alphabet size estimator, and illustrate that using it to adjust discrete semantic entropy for sample coverage results in more accurate semantic entropy estimation in our setting of interest. Furthermore, our proposed alphabet size estimator flags incorrect LLM responses as well or better than recent top-performing approaches, with the added benefit of remaining highly interpretable.

