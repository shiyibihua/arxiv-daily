---
layout: default
title: Do Large Language Models Understand Word Senses?
---

# Do Large Language Models Understand Word Senses?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13905" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13905v1</a>
  <a href="https://arxiv.org/pdf/2509.13905.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13905v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13905v1', 'Do Large Language Models Understand Word Senses?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Domenico Meconi, Simone Stirpe, Federico Martelli, Leonardo Lavalle, Roberto Navigli

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

**å¤‡æ³¨**: 20 pages, to be published in EMNLP2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è¯ä¹‰ç†è§£èƒ½åŠ›ï¼Œå¹¶éªŒè¯å…¶åœ¨è¯ä¹‰æ¶ˆæ­§ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯ä¹‰æ¶ˆæ­§` `è¯ä¹‰ç†è§£` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç”Ÿæˆä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯¹LLMè¯ä¹‰ç†è§£èƒ½åŠ›çš„è¯„ä¼°ä¸è¶³ï¼Œç¼ºä¹ä¸ä¸“é—¨WSDç³»ç»Ÿçš„å¯¹æ¯”ã€‚
2. é€šè¿‡WSDä»»åŠ¡å’Œç”Ÿæˆä»»åŠ¡ï¼Œè¯„ä¼°LLMåœ¨ç†è§£å’Œç”Ÿæˆè¯ä¹‰æ–¹é¢çš„èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMåœ¨WSDä»»åŠ¡ä¸­å¯ä¸ä¸“ç”¨ç³»ç»Ÿåª²ç¾ï¼Œç”Ÿæˆä»»åŠ¡ä¸­è¯ä¹‰è§£é‡Šå‡†ç¡®ç‡é«˜è¾¾98%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç†è§£ä¸Šä¸‹æ–‡ä¸­è¯è¯­çš„å«ä¹‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€é¡¹åŸºæœ¬èƒ½åŠ›ã€‚å°½ç®¡è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œä½†LLMçœŸæ­£æŒæ¡è¯ä¹‰çš„ç¨‹åº¦ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é€šè¿‡è¯„ä¼°i)æŒ‡ä»¤è°ƒä¼˜çš„LLMçš„è¯ä¹‰æ¶ˆæ­§ï¼ˆWSDï¼‰èƒ½åŠ›ï¼Œå°†å…¶æ€§èƒ½ä¸ä¸“é—¨ä¸ºæ­¤ä»»åŠ¡è®¾è®¡çš„æœ€å…ˆè¿›ç³»ç»Ÿè¿›è¡Œæ¯”è¾ƒï¼Œä»¥åŠii)ä¸¤ä¸ªè¡¨ç°æœ€ä½³çš„å¼€æºå’Œé—­æºLLMåœ¨ä¸‰ç§ç”Ÿæˆè®¾ç½®ï¼ˆå®šä¹‰ç”Ÿæˆã€è‡ªç”±å½¢å¼è§£é‡Šå’Œç¤ºä¾‹ç”Ÿæˆï¼‰ä¸­ç†è§£è¯ä¹‰çš„èƒ½åŠ›ï¼Œæ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨WSDä»»åŠ¡ä¸­ï¼ŒGPT-4oå’ŒDeepSeek-V3ç­‰é¢†å…ˆæ¨¡å‹çš„æ€§èƒ½ä¸ä¸“é—¨çš„WSDç³»ç»Ÿç›¸å½“ï¼ŒåŒæ—¶åœ¨ä¸åŒé¢†åŸŸå’Œéš¾åº¦çº§åˆ«ä¸Šè¡¨ç°å‡ºæ›´å¤§çš„é²æ£’æ€§ã€‚åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç»“æœè¡¨æ˜LLMå¯ä»¥è§£é‡Šä¸Šä¸‹æ–‡ä¸­è¯è¯­çš„å«ä¹‰ï¼Œå‡†ç¡®ç‡é«˜è¾¾98ï¼…ï¼Œå…¶ä¸­åœ¨è‡ªç”±å½¢å¼è§£é‡Šä»»åŠ¡ä¸­è§‚å¯Ÿåˆ°æœ€é«˜çš„æ€§èƒ½ï¼Œè¿™ä¸å®ƒä»¬çš„ç”Ÿæˆèƒ½åŠ›æœ€å»åˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦çœŸæ­£ç†è§£è¯ä¹‰ï¼Œå¹¶é‡åŒ–å…¶ç†è§£ç¨‹åº¦ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é€šç”¨èƒ½åŠ›è¯„ä¼°ï¼Œç¼ºä¹å¯¹LLMè¯ä¹‰ç†è§£èƒ½åŠ›çš„é’ˆå¯¹æ€§è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯ä¸ä¸“é—¨çš„è¯ä¹‰æ¶ˆæ­§ï¼ˆWSDï¼‰ç³»ç»Ÿç›¸æ¯”ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•è¾ƒå°‘å…³æ³¨LLMåœ¨ç”Ÿæˆä»»åŠ¡ä¸­å¯¹è¯ä¹‰çš„ç†è§£å’Œåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡WSDä»»åŠ¡å’Œç”Ÿæˆä»»åŠ¡ï¼Œç›´æ¥è¯„ä¼°LLMå¯¹è¯ä¹‰çš„ç†è§£èƒ½åŠ›ã€‚WSDä»»åŠ¡æ—¨åœ¨è€ƒå¯ŸLLMåœ¨åŒºåˆ†è¯è¯­ä¸åŒå«ä¹‰æ–¹é¢çš„èƒ½åŠ›ï¼Œè€Œç”Ÿæˆä»»åŠ¡åˆ™è€ƒå¯ŸLLMåœ¨ç”Ÿæˆå®šä¹‰ã€è§£é‡Šå’Œç¤ºä¾‹æ—¶å¯¹è¯ä¹‰çš„è¿ç”¨èƒ½åŠ›ã€‚é€šè¿‡å°†LLMä¸ä¸“é—¨çš„WSDç³»ç»Ÿè¿›è¡Œæ¯”è¾ƒï¼Œå¯ä»¥æ›´å®¢è§‚åœ°è¯„ä¼°LLMçš„è¯ä¹‰ç†è§£æ°´å¹³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„è¯„ä¼°æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šWSDä»»åŠ¡å’Œç”Ÿæˆä»»åŠ¡ã€‚WSDä»»åŠ¡ä½¿ç”¨æ ‡å‡†WSDæ•°æ®é›†ï¼Œè¯„ä¼°LLMçš„è¯ä¹‰æ¶ˆæ­§å‡†ç¡®ç‡ã€‚ç”Ÿæˆä»»åŠ¡åŒ…å«ä¸‰ä¸ªå­ä»»åŠ¡ï¼šå®šä¹‰ç”Ÿæˆã€è‡ªç”±å½¢å¼è§£é‡Šå’Œç¤ºä¾‹ç”Ÿæˆã€‚å¯¹äºæ¯ä¸ªå­ä»»åŠ¡ï¼Œç ”ç©¶äººå‘˜è®¾è®¡äº†ç›¸åº”çš„æç¤ºè¯­ï¼Œå¹¶è¯„ä¼°LLMç”Ÿæˆçš„æ–‡æœ¬çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬WSDå‡†ç¡®ç‡å’Œç”Ÿæˆæ–‡æœ¬çš„è¯­ä¹‰å‡†ç¡®ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºåŒæ—¶è¯„ä¼°LLMåœ¨åˆ¤åˆ«å¼ä»»åŠ¡ï¼ˆWSDï¼‰å’Œç”Ÿæˆå¼ä»»åŠ¡ä¸­å¯¹è¯ä¹‰çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§ç»¼åˆè¯„ä¼°ï¼Œå¯ä»¥æ›´å…¨é¢åœ°äº†è§£LLMçš„è¯ä¹‰ç†è§£æ°´å¹³ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¯”è¾ƒäº†LLMä¸ä¸“é—¨çš„WSDç³»ç»Ÿï¼Œä»è€Œæ›´å®¢è§‚åœ°è¯„ä¼°äº†LLMçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨WSDä»»åŠ¡ä¸­ï¼Œè®ºæ–‡ä½¿ç”¨äº†å¤šä¸ªWSDæ•°æ®é›†ï¼Œæ¶µç›–ä¸åŒçš„é¢†åŸŸå’Œéš¾åº¦çº§åˆ«ã€‚åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè®ºæ–‡è®¾è®¡äº†æ¸…æ™°æ˜ç¡®çš„æç¤ºè¯­ï¼Œå¼•å¯¼LLMç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬ã€‚è¯„ä¼°æŒ‡æ ‡æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†å‡†ç¡®ç‡ç­‰å¸¸ç”¨æŒ‡æ ‡ï¼Œå¹¶è¿›è¡Œäº†äººå·¥è¯„ä¼°ï¼Œä»¥ç¡®ä¿è¯„ä¼°ç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨WSDä»»åŠ¡ä¸­ï¼ŒGPT-4oå’ŒDeepSeek-V3ç­‰é¢†å…ˆæ¨¡å‹çš„æ€§èƒ½ä¸ä¸“é—¨çš„WSDç³»ç»Ÿç›¸å½“ï¼Œå¹¶ä¸”åœ¨ä¸åŒé¢†åŸŸå’Œéš¾åº¦çº§åˆ«ä¸Šè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒLLMå¯ä»¥è§£é‡Šä¸Šä¸‹æ–‡ä¸­è¯è¯­çš„å«ä¹‰ï¼Œå‡†ç¡®ç‡é«˜è¾¾98ï¼…ï¼Œå…¶ä¸­è‡ªç”±å½¢å¼è§£é‡Šä»»åŠ¡è¡¨ç°æœ€ä½³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦å’Œé—®ç­”ç³»ç»Ÿã€‚æ›´å‡†ç¡®çš„è¯ä¹‰ç†è§£æœ‰åŠ©äºLLMç”Ÿæˆæ›´æµç•…ã€æ›´è‡ªç„¶çš„æ–‡æœ¬ï¼Œå¹¶æ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„æ„å›¾ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥æŒ‡å¯¼LLMçš„è®­ç»ƒï¼Œä½¿å…¶æ›´å¥½åœ°æŒæ¡è¯ä¹‰çŸ¥è¯†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding the meaning of words in context is a fundamental capability for Large Language Models (LLMs). Despite extensive evaluation efforts, the extent to which LLMs show evidence that they truly grasp word senses remains underexplored. In this paper, we address this gap by evaluating both i) the Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs, comparing their performance to state-of-the-art systems specifically designed for the task, and ii) the ability of two top-performing open- and closed-source LLMs to understand word senses in three generative settings: definition generation, free-form explanation, and example generation. Notably, we find that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve performance on par with specialized WSD systems, while also demonstrating greater robustness across domains and levels of difficulty. In the generation tasks, results reveal that LLMs can explain the meaning of words in context up to 98\% accuracy, with the highest performance observed in the free-form explanation task, which best aligns with their generative capabilities.

