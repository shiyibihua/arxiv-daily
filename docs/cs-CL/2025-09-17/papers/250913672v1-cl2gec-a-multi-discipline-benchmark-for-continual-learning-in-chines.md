---
layout: default
title: CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction
---

# CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13672" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13672v1</a>
  <a href="https://arxiv.org/pdf/2509.13672.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13672v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13672v1', 'CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shang Qin, Jingheng Ye, Yinghui Li, Hai-Tao Zheng, Qi Li, Jinxiao Shan, Zhixing Li, Hong-Gee Kim

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCL$^2$GECåŸºå‡†ï¼Œç”¨äºè¯„ä¼°ä¸­æ–‡è¯­æ³•çº é”™ç³»ç»Ÿåœ¨å¤šé¢†åŸŸæŒç»­å­¦ä¹ ä¸­çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸­æ–‡è¯­æ³•çº é”™` `æŒç»­å­¦ä¹ ` `å¤šå­¦ç§‘åŸºå‡†` `ç¾éš¾æ€§é—å¿˜` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰CGECç ”ç©¶ç¼ºä¹å¤šå­¦ç§‘å†™ä½œçš„ä¸“ç”¨åŸºå‡†ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸé—´çš„é€‚åº”èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºCL$^2$GECåŸºå‡†ï¼Œæ¨¡æ‹ŸçœŸå®ç¼–è¾‘åœºæ™¯ï¼Œè¯„ä¼°æ¨¡å‹åœ¨æŒç»­å­¦ä¹ ç¯å¢ƒä¸‹çš„è¯­æ³•çº é”™èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒåŸºäºæ­£åˆ™åŒ–çš„æŒç»­å­¦ä¹ æ–¹æ³•åœ¨å‡è½»ç¾éš¾æ€§é—å¿˜æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é’ˆå¯¹ä¸åŒå­¦æœ¯é¢†åŸŸè‡ªåŠ¨å†™ä½œè¾…åŠ©çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œè¿™éœ€è¦é²æ£’çš„ä¸­æ–‡è¯­æ³•çº é”™(CGEC)ç³»ç»Ÿèƒ½å¤Ÿè·¨å­¦ç§‘é€‚åº”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„CGECç ”ç©¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç¼ºä¹é’ˆå¯¹å¤šå­¦ç§‘å†™ä½œçš„ä¸“ç”¨åŸºå‡†ï¼Œå¿½ç•¥äº†æŒç»­å­¦ä¹ (CL)ä½œä¸ºå¤„ç†é¢†åŸŸç‰¹å®šè¯­è¨€å˜å¼‚å’Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜çš„æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€å…³é”®ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CL$^2$GECï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸­æ–‡æ–‡çŒ®è¯­æ³•çº é”™çš„æŒç»­å­¦ä¹ åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è·¨å¤šä¸ªå­¦æœ¯é¢†åŸŸçš„è‡ªé€‚åº”CGECã€‚æˆ‘ä»¬çš„åŸºå‡†åŒ…æ‹¬10,000ä¸ªç”±äººå·¥æ ‡æ³¨çš„å¥å­ï¼Œæ¶µç›–10ä¸ªå­¦ç§‘ï¼Œæ¯ä¸ªå­¦ç§‘éƒ½è¡¨ç°å‡ºç‹¬ç‰¹çš„è¯­è¨€é£æ ¼å’Œé”™è¯¯æ¨¡å¼ã€‚CL$^2$GECä¾§é‡äºåœ¨æŒç»­å­¦ä¹ ç¯å¢ƒä¸­è¯„ä¼°è¯­æ³•çº é”™ï¼Œæ¨¡æ‹Ÿé¡ºåºæ¥è§¦ä¸åŒçš„å­¦æœ¯é¢†åŸŸï¼Œä»¥åæ˜ çœŸå®çš„ç¼–è¾‘åŠ¨æ€ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¡ºåºå¾®è°ƒã€å‚æ•°é«˜æ•ˆé€‚åº”å’Œå››ç§ä»£è¡¨æ€§CLç®—æ³•ä¸‹çš„æ€§èƒ½ï¼Œä½¿ç”¨æ ‡å‡†GECæŒ‡æ ‡å’Œé€‚åº”äºä»»åŠ¡çº§åˆ«å˜åŒ–çš„æŒç»­å­¦ä¹ æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ­£åˆ™åŒ–çš„æ–¹æ³•æ¯”åŸºäºé‡æ”¾æˆ–æœ´ç´ é¡ºåºæ–¹æ³•æ›´æœ‰æ•ˆåœ°å‡è½»é—å¿˜ã€‚æˆ‘ä»¬çš„åŸºå‡†ä¸ºæœªæ¥åœ¨ä¸åŒå­¦æœ¯é¢†åŸŸè¿›è¡Œè‡ªé€‚åº”è¯­æ³•çº é”™çš„ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä¸­æ–‡è¯­æ³•çº é”™(CGEC)ç³»ç»Ÿåœ¨å¤šå­¦ç§‘é¢†åŸŸåº”ç”¨æ—¶ï¼Œç”±äºé¢†åŸŸå·®å¼‚å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰CGECç ”ç©¶ç¼ºä¹é’ˆå¯¹å¤šå­¦ç§‘çš„æŒç»­å­¦ä¹ åŸºå‡†ï¼Œä½¿å¾—æ¨¡å‹éš¾ä»¥é€‚åº”ä¸åŒé¢†åŸŸçš„è¯­è¨€é£æ ¼å’Œé”™è¯¯æ¨¡å¼ï¼Œå®¹æ˜“å‘ç”Ÿç¾éš¾æ€§é—å¿˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤šå­¦ç§‘çš„CGECæŒç»­å­¦ä¹ åŸºå‡†CL$^2$GECï¼Œé€šè¿‡æ¨¡æ‹Ÿæ¨¡å‹åœ¨ä¸åŒå­¦ç§‘é¢†åŸŸæ•°æ®ä¸Šçš„é¡ºåºå­¦ä¹ è¿‡ç¨‹ï¼Œæ¥è¯„ä¼°æ¨¡å‹åœ¨æŒç»­å­¦ä¹ ç¯å¢ƒä¸‹çš„è¯­æ³•çº é”™èƒ½åŠ›ã€‚è¿™æ ·å¯ä»¥æ›´çœŸå®åœ°åæ˜ å®é™…åº”ç”¨åœºæ™¯ï¼Œå¹¶ä¿ƒè¿›ç›¸å…³ç®—æ³•çš„ç ”ç©¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCL$^2$GECåŸºå‡†åŒ…å«10ä¸ªå­¦ç§‘çš„10,000ä¸ªäººå·¥æ ‡æ³¨å¥å­ï¼Œæ¯ä¸ªå­¦ç§‘éƒ½å…·æœ‰ç‹¬ç‰¹çš„è¯­è¨€é£æ ¼å’Œé”™è¯¯æ¨¡å¼ã€‚è®ºæ–‡ä½¿ç”¨è¯¥åŸºå‡†è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¡ºåºå¾®è°ƒã€å‚æ•°é«˜æ•ˆé€‚åº”ä»¥åŠå››ç§ä»£è¡¨æ€§æŒç»­å­¦ä¹ ç®—æ³•ä¸‹çš„æ€§èƒ½ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬æ ‡å‡†çš„GECæŒ‡æ ‡ä»¥åŠé’ˆå¯¹ä»»åŠ¡çº§åˆ«å˜åŒ–çš„æŒç»­å­¦ä¹ æŒ‡æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†é¦–ä¸ªä¸­æ–‡æ–‡çŒ®è¯­æ³•çº é”™çš„æŒç»­å­¦ä¹ åŸºå‡†CL$^2$GECã€‚è¯¥åŸºå‡†çš„ç‰¹ç‚¹æ˜¯å¤šå­¦ç§‘æ€§ï¼Œèƒ½å¤Ÿæ›´çœŸå®åœ°æ¨¡æ‹Ÿå®é™…åº”ç”¨åœºæ™¯ï¼Œå¹¶ä¸ºæŒç»­å­¦ä¹ ç®—æ³•åœ¨CGECä»»åŠ¡ä¸Šçš„ç ”ç©¶æä¾›äº†å¹³å°ã€‚ä¸ç°æœ‰CGECæ•°æ®é›†ç›¸æ¯”ï¼ŒCL$^2$GECæ›´å…³æ³¨æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸé—´çš„é€‚åº”èƒ½åŠ›å’Œé¿å…ç¾éš¾æ€§é—å¿˜çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šCL$^2$GECåŸºå‡†çš„æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­ï¼Œéœ€è¦ä¿è¯æ¯ä¸ªå­¦ç§‘çš„æ•°æ®é‡å’Œè´¨é‡ï¼Œå¹¶ç¡®ä¿ä¸åŒå­¦ç§‘ä¹‹é—´å…·æœ‰ä¸€å®šçš„å·®å¼‚æ€§ã€‚åœ¨å®éªŒè¯„ä¼°ä¸­ï¼Œè®ºæ–‡é‡‡ç”¨äº†å¤šç§æŒç»­å­¦ä¹ ç®—æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ­£åˆ™åŒ–ã€åŸºäºé‡æ”¾å’Œæœ´ç´ é¡ºåºæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨äº†æ ‡å‡†çš„GECæŒ‡æ ‡å’Œé’ˆå¯¹ä»»åŠ¡çº§åˆ«å˜åŒ–çš„æŒç»­å­¦ä¹ æŒ‡æ ‡æ¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨CL$^2$GECåŸºå‡†ä¸Šï¼ŒåŸºäºæ­£åˆ™åŒ–çš„æŒç»­å­¦ä¹ æ–¹æ³•åœ¨å‡è½»ç¾éš¾æ€§é—å¿˜æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œä¼˜äºåŸºäºé‡æ”¾æˆ–æœ´ç´ é¡ºåºæ–¹æ³•ã€‚è¿™è¡¨æ˜æ­£åˆ™åŒ–æ–¹æ³•æ›´é€‚åˆå¤„ç†å¤šå­¦ç§‘CGECä»»åŠ¡ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å†™ä½œè¾…åŠ©ã€åœ¨çº¿æ•™è‚²ã€å­¦æœ¯è®ºæ–‡æ¶¦è‰²ç­‰é¢†åŸŸã€‚é€šè¿‡æŒç»­å­¦ä¹ ï¼ŒCGECç³»ç»Ÿèƒ½å¤Ÿé€‚åº”ä¸åŒå­¦ç§‘çš„è¯­è¨€é£æ ¼ï¼Œæé«˜è¯­æ³•çº é”™çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¸®åŠ©ç”¨æˆ·æ’°å†™é«˜è´¨é‡çš„å­¦æœ¯è®ºæ–‡å’Œæ–‡æ¡£ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå­¦æœ¯æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The growing demand for automated writing assistance in diverse academic domains highlights the need for robust Chinese Grammatical Error Correction (CGEC) systems that can adapt across disciplines. However, existing CGEC research largely lacks dedicated benchmarks for multi-disciplinary academic writing, overlooking continual learning (CL) as a promising solution to handle domain-specific linguistic variation and prevent catastrophic forgetting. To fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning benchmark for Chinese Literature Grammatical Error Correction, designed to evaluate adaptive CGEC across multiple academic fields. Our benchmark includes 10,000 human-annotated sentences spanning 10 disciplines, each exhibiting distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating grammatical error correction in a continual learning setting, simulating sequential exposure to diverse academic disciplines to reflect real-world editorial dynamics. We evaluate large language models under sequential tuning, parameter-efficient adaptation, and four representative CL algorithms, using both standard GEC metrics and continual learning metrics adapted to task-level variation. Experimental results reveal that regularization-based methods mitigate forgetting more effectively than replay-based or naive sequential approaches. Our benchmark provides a rigorous foundation for future research in adaptive grammatical error correction across diverse academic domains.

