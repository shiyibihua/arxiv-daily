---
layout: default
title: Improving Context Fidelity via Native Retrieval-Augmented Reasoning
---

# Improving Context Fidelity via Native Retrieval-Augmented Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13683" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13683v1</a>
  <a href="https://arxiv.org/pdf/2509.13683.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13683v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13683v1', 'Improving Context Fidelity via Native Retrieval-Augmented Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Suyuchen Wang, Jinlin Wang, Xinyu Wang, Shiqi Li, Xiangru Tang, Sirui Hong, Xiao-Wen Chang, Chenglin Wu, Bang Liu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

**å¤‡æ³¨**: Accepted as a main conference paper at EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAREæ¡†æ¶ï¼Œé€šè¿‡åŸç”Ÿæ£€ç´¢å¢å¼ºæ¨ç†æå‡LLMä¸Šä¸‹æ–‡å¿ å®åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `ä¸Šä¸‹æ–‡å¿ å®åº¦` `å¤§å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡` `æ¨ç†é“¾` `å¯¹æ¯”å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­ï¼Œéš¾ä»¥ä¿è¯ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å¿ å®æ€§ï¼Œå¯¼è‡´å›ç­”ä¸ä¸€è‡´ã€‚
2. CAREæ¡†æ¶é€šè¿‡è®©LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜¾å¼æ•´åˆæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡è¯æ®ï¼Œæå‡ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCAREåœ¨æ£€ç´¢å‡†ç¡®æ€§å’Œç­”æ¡ˆç”Ÿæˆæ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ‰€éœ€æ ‡æ³¨æ•°æ®æ›´å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸Šä¸‹æ–‡å¿ å®åº¦æ–¹é¢å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œå½“åŸºäºæä¾›çš„ä¿¡æ¯å›ç­”é—®é¢˜æ—¶ï¼Œä¼šäº§ç”Ÿä¸ä¸€è‡´çš„ç­”æ¡ˆã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–æ˜‚è´µçš„ç›‘ç£å¾®è°ƒæ¥ç”Ÿæˆç­”æ¡ˆåçš„è¯æ®ï¼Œè¦ä¹ˆè®­ç»ƒæ¨¡å‹æ‰§è¡Œç½‘ç»œæœç´¢ï¼Œä½†å¹¶ä¸ä¸€å®šèƒ½æé«˜ç»™å®šä¸Šä¸‹æ–‡çš„åˆ©ç”¨ç‡ã€‚æˆ‘ä»¬æå‡ºäº†CAREï¼Œä¸€ç§æ–°é¢–çš„åŸç”Ÿæ£€ç´¢å¢å¼ºæ¨ç†æ¡†æ¶ï¼Œå®ƒæ•™å¯¼LLMåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ£€ç´¢èƒ½åŠ›ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜¾å¼åœ°æ•´åˆä¸Šä¸‹æ–‡è¯æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦æœ‰é™çš„æ ‡æ³¨è¯æ®æ•°æ®ï¼ŒåŒæ—¶é€šè¿‡ç­–ç•¥æ€§åœ°æ£€ç´¢æ¨ç†é“¾ä¸­çš„ä¸Šä¸‹æ–‡tokenï¼Œæ˜¾è‘—æé«˜æ£€ç´¢å‡†ç¡®æ€§å’Œç­”æ¡ˆç”Ÿæˆæ€§èƒ½ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œå’Œåäº‹å®QAåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒã€ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•å’Œå¤–éƒ¨æ£€ç´¢è§£å†³æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œä»£è¡¨äº†åœ¨ä½¿LLMå¯¹äºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ›´åŠ å‡†ç¡®ã€å¯é å’Œé«˜æ•ˆæ–¹é¢çš„ä¸€ä¸ªæ ¹æœ¬æ€§è¿›æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹é—®ç­”ä»»åŠ¡ä¸­ï¼Œä¸Šä¸‹æ–‡å¿ å®åº¦ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›‘ç£å¾®è°ƒå’Œä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œè¦ä¹ˆæˆæœ¬é«˜æ˜‚ï¼Œè¦ä¹ˆæ— æ³•æœ‰æ•ˆåˆ©ç”¨ç»™å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´å›ç­”ä¸å‡†ç¡®æˆ–ä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCAREçš„æ ¸å¿ƒæ€è·¯æ˜¯è®©LLMå­¦ä¼šâ€œåŸç”Ÿâ€åœ°è¿›è¡Œæ£€ç´¢å¢å¼ºæ¨ç†ã€‚è¿™æ„å‘³ç€æ¨¡å‹åœ¨ç”Ÿæˆç­”æ¡ˆçš„è¿‡ç¨‹ä¸­ï¼Œä¸ä»…åˆ©ç”¨å¤–éƒ¨æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œè¿˜åˆ©ç”¨è‡ªèº«çš„èƒ½åŠ›è¿›è¡Œæ£€ç´¢ï¼Œå¹¶å°†æ£€ç´¢åˆ°çš„è¯æ®æ˜¾å¼åœ°èå…¥åˆ°æ¨ç†é“¾ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCAREæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) é—®é¢˜ç¼–ç å™¨ï¼šå°†è¾“å…¥é—®é¢˜ç¼–ç æˆå‘é‡è¡¨ç¤ºã€‚2) æ£€ç´¢å™¨ï¼šåŸºäºé—®é¢˜å‘é‡ï¼Œä»ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­æ£€ç´¢ç›¸å…³çš„è¯æ®ç‰‡æ®µã€‚3) æ¨ç†å™¨ï¼šå°†é—®é¢˜ã€æ£€ç´¢åˆ°çš„è¯æ®ä»¥åŠä¸Šä¸‹æ–‡ä¿¡æ¯è¾“å…¥åˆ°LLMä¸­ï¼Œç”Ÿæˆç­”æ¡ˆã€‚å…³é”®åœ¨äºï¼Œæ¨ç†å™¨åœ¨ç”Ÿæˆç­”æ¡ˆçš„è¿‡ç¨‹ä¸­ï¼Œä¼šæ˜¾å¼åœ°åˆ©ç”¨æ£€ç´¢åˆ°çš„è¯æ®ï¼Œå¹¶å°†å…¶èå…¥åˆ°æ¨ç†é“¾ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šCAREçš„å…³é”®åˆ›æ–°åœ¨äºå…¶â€œåŸç”Ÿæ£€ç´¢å¢å¼ºæ¨ç†â€çš„ç†å¿µã€‚ä¸ä¼ ç»Ÿçš„RAGæ–¹æ³•ä¸åŒï¼ŒCAREä¸æ˜¯ç®€å•åœ°å°†æ£€ç´¢åˆ°çš„ä¿¡æ¯æ‹¼æ¥åœ¨è¾“å…¥ä¸­ï¼Œè€Œæ˜¯è®©LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨åœ°åˆ©ç”¨æ£€ç´¢åˆ°çš„è¯æ®ï¼Œå¹¶å°†å…¶èå…¥åˆ°æ¨ç†é“¾ä¸­ã€‚è¿™ç§æ–¹å¼å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒCAREè¿˜é€šè¿‡ç­–ç•¥æ€§åœ°æ£€ç´¢ä¸Šä¸‹æ–‡tokenï¼Œè¿›ä¸€æ­¥æå‡äº†æ£€ç´¢å‡†ç¡®æ€§å’Œç­”æ¡ˆç”Ÿæˆæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šCAREçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ¥è®­ç»ƒæ£€ç´¢å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ£€ç´¢åˆ°ç›¸å…³çš„è¯æ®ç‰‡æ®µã€‚2) åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥æ˜¾å¼åœ°åˆ©ç”¨æ£€ç´¢åˆ°çš„è¯æ®ã€‚3) è®¾è®¡äº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ç”Ÿæˆç­”æ¡ˆçš„è¿‡ç¨‹ä¸­ï¼Œæ›´åŠ å…³æ³¨æ£€ç´¢åˆ°çš„è¯æ®ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œæ­¤å¤„ä¸å†èµ˜è¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCAREåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œå’Œåäº‹å®QAåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›åŸºå‡†ä¸Šï¼ŒCAREçš„å‡†ç¡®ç‡æ¯”ç›‘ç£å¾®è°ƒæ–¹æ³•æé«˜äº†10%ä»¥ä¸Šï¼Œæ¯”ä¼ ç»Ÿçš„RAGæ–¹æ³•æé«˜äº†15%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒCAREè¿˜è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡ä¸Šå–å¾—ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CAREæ¡†æ¶å¯åº”ç”¨äºå„ç§çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€æ–‡æ¡£æ‘˜è¦ã€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜LLMçš„ä¸Šä¸‹æ–‡å¿ å®åº¦ï¼ŒCAREå¯ä»¥ä½¿è¿™äº›åº”ç”¨æ›´åŠ å‡†ç¡®ã€å¯é å’Œé«˜æ•ˆã€‚æœªæ¥ï¼ŒCAREè¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æç­‰ï¼Œä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´æ™ºèƒ½åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) often struggle with context fidelity, producing inconsistent answers when responding to questions based on provided information. Existing approaches either rely on expensive supervised fine-tuning to generate evidence post-answer or train models to perform web searches without necessarily improving utilization of the given context. We propose CARE, a novel native retrieval-augmented reasoning framework that teaches LLMs to explicitly integrate in-context evidence within their reasoning process with the model's own retrieval capabilities. Our method requires limited labeled evidence data while significantly enhancing both retrieval accuracy and answer generation performance through strategically retrieved in-context tokens in the reasoning chain. Extensive experiments on multiple real-world and counterfactual QA benchmarks demonstrate that our approach substantially outperforms supervised fine-tuning, traditional retrieval-augmented generation methods, and external retrieval solutions. This work represents a fundamental advancement in making LLMs more accurate, reliable, and efficient for knowledge-intensive tasks.

