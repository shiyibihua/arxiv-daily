---
layout: default
title: Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale
---

# Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14008" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14008v1</a>
  <a href="https://arxiv.org/pdf/2509.14008.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14008v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14008v1', 'Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hasan Abed Al Kader Hammoud, Mohammad Zbeeb, Bernard Ghanem

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

**å¤‡æ³¨**: Technical Report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHalaæ¨¡å‹ä»¥æå‡é˜¿æ‹‰ä¼¯è¯­æŒ‡ä»¤ä¸ç¿»è¯‘çš„è´¨é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é˜¿æ‹‰ä¼¯è¯­å¤„ç†` `æŒ‡ä»¤ç¿»è¯‘` `æ¨¡å‹å‹ç¼©` `åŒè¯­ç›‘ç£` `è½»é‡çº§æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é˜¿æ‹‰ä¼¯è¯­æŒ‡ä»¤å’Œç¿»è¯‘æ¨¡å‹åœ¨è´¨é‡å’Œæ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å¤§è§„æ¨¡åº”ç”¨éœ€æ±‚ã€‚
2. Halaæ¨¡å‹é€šè¿‡å‹ç¼©æ•™å¸ˆæ¨¡å‹å¹¶åˆ©ç”¨åŒè¯­ç›‘ç£æ•°æ®ï¼Œç»“åˆè½»é‡çº§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæå‡äº†ç¿»è¯‘è´¨é‡å’Œå¤„ç†é€Ÿåº¦ã€‚
3. åœ¨é˜¿æ‹‰ä¼¯è¯­åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHalaæ¨¡å‹åœ¨å°å‹å’Œçº³ç±³ç±»åˆ«ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†Halaï¼Œä¸€ä¸ªä»¥é˜¿æ‹‰ä¼¯è¯­ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤å’Œç¿»è¯‘æ¨¡å‹ç³»åˆ—ï¼Œé‡‡ç”¨æˆ‘ä»¬çš„ç¿»è¯‘ä¸è°ƒä¼˜ç®¡é“æ„å»ºã€‚æˆ‘ä»¬é¦–å…ˆå°†å¼ºå¤§çš„é˜¿æ‹‰ä¼¯è¯­ä¸è‹±è¯­æ•™å¸ˆæ¨¡å‹å‹ç¼©è‡³FP8æ ¼å¼ï¼Œå®ç°çº¦2å€çš„ååé‡æå‡ä¸”æ— è´¨é‡æŸå¤±ï¼Œå¹¶åˆ©ç”¨å…¶ç”Ÿæˆé«˜ä¿çœŸçš„åŒè¯­ç›‘ç£æ•°æ®ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¯¹è½»é‡çº§è¯­è¨€æ¨¡å‹LFM2-1.2Bè¿›è¡Œå¾®è°ƒï¼Œä»¥å°†é«˜è´¨é‡çš„è‹±è¯­æŒ‡ä»¤é›†ç¿»è¯‘æˆé˜¿æ‹‰ä¼¯è¯­ï¼Œæ„å»ºäº†ä¸€ä¸ªç™¾ä¸‡è§„æ¨¡çš„ä¸“é—¨ç”¨äºæŒ‡ä»¤è·Ÿéšçš„è¯­æ–™åº“ã€‚Halaæ¨¡å‹åœ¨å¤šä¸ªå‚æ•°è§„æ¨¡ä¸‹è®­ç»ƒï¼Œå¹¶åœ¨é˜¿æ‹‰ä¼¯è¯­åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„ç»“æœï¼Œè¶…è¶Šäº†åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹ã€æ•°æ®ã€è¯„ä¼°å’Œæ–¹æ³•ï¼Œä»¥åŠ é€Ÿé˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é˜¿æ‹‰ä¼¯è¯­æŒ‡ä»¤å’Œç¿»è¯‘æ¨¡å‹åœ¨è´¨é‡å’Œæ•ˆç‡ä¸Šçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ»¡è¶³å¤§è§„æ¨¡åº”ç”¨çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å‹ç¼©å¼ºå¤§çš„é˜¿æ‹‰ä¼¯è¯­ä¸è‹±è¯­æ•™å¸ˆæ¨¡å‹è‡³FP8æ ¼å¼ï¼Œæå‡ååé‡ï¼ŒåŒæ—¶åˆ©ç”¨ç”Ÿæˆçš„é«˜ä¿çœŸåŒè¯­ç›‘ç£æ•°æ®å¯¹è½»é‡çº§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜ç¿»è¯‘è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•™å¸ˆæ¨¡å‹å‹ç¼©ã€åŒè¯­ç›‘ç£æ•°æ®ç”Ÿæˆã€è½»é‡çº§è¯­è¨€æ¨¡å‹å¾®è°ƒåŠæœ€ç»ˆçš„æ¨¡å‹è®­ç»ƒï¼Œæ¶µç›–å¤šä¸ªå‚æ•°è§„æ¨¡çš„Halaæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šHalaæ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡slerpåˆå¹¶æŠ€æœ¯å¹³è¡¡é˜¿æ‹‰ä¼¯è¯­ä¸“ä¸šåŒ–ä¸åŸºç¡€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒå‚æ•°è®¾ç½®ï¼ˆ350Mã€700Mã€1.2Bå’Œ9Bï¼‰ï¼Œå¹¶è®¾è®¡äº†é€‚åˆé˜¿æ‹‰ä¼¯è¯­çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Halaæ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨çº³ç±³ï¼ˆâ‰¤2Bï¼‰å’Œå°å‹ï¼ˆ7-9Bï¼‰ç±»åˆ«ä¸­ï¼Œå‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¶…è¶Šäº†åŸºç¡€æ¨¡å‹ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨ç¿»è¯‘è´¨é‡å’Œå¤„ç†æ•ˆç‡ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œæ¨åŠ¨äº†é˜¿æ‹‰ä¼¯è¯­NLPçš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Halaæ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•™è‚²ã€ç¿»è¯‘å’Œä¿¡æ¯æ£€ç´¢ç­‰åœºæ™¯ä¸­ã€‚å…¶é«˜è´¨é‡çš„æŒ‡ä»¤ç¿»è¯‘èƒ½åŠ›èƒ½å¤Ÿå¸®åŠ©é˜¿æ‹‰ä¼¯è¯­ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œä½¿ç”¨æŠ€æœ¯äº§å“ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚æœªæ¥ï¼ŒHalaæ¨¡å‹çš„ç ”ç©¶æˆæœå¯èƒ½ä¼šä¿ƒè¿›é˜¿æ‹‰ä¼¯è¯­å¤„ç†æŠ€æœ¯çš„è¿›ä¸€æ­¥åˆ›æ–°ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Hala, a family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress a strong AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing a million-scale corpus tailored to instruction following. We train Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within both the "nano" ($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP.

