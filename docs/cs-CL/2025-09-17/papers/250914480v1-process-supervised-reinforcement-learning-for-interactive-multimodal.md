---
layout: default
title: Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents
---

# Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14480" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14480v1</a>
  <a href="https://arxiv.org/pdf/2509.14480.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14480v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14480v1', 'Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weiting Tan, Xinghua Qu, Ming Tu, Meng Ge, Andy T. Liu, Philipp Koehn, Lu Lu

**åˆ†ç±»**: cs.CL, cs.AI, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTurn-level Adjudicated RLï¼Œè§£å†³äº¤äº’å¼å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨Agentçš„è®­ç»ƒéš¾é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `å·¥å…·ä½¿ç”¨` `äººæœºäº¤äº’` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡ç”¨åˆ†é…` `äº¤äº’å¼Agent`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰äº¤äº’å¼å·¥å…·ä½¿ç”¨Agentåœ¨å¤šè½®è§„åˆ’å’Œé•¿ä¸Šä¸‹æ–‡å¯¹è¯ç®¡ç†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­ã€‚
2. æå‡ºTurn-level Adjudicated Reinforcement Learning (TARL)ï¼Œåˆ©ç”¨LLMè¿›è¡Œturn-levelè¯„ä¼°ï¼Œè§£å†³é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚
3. é€šè¿‡æ··åˆä»»åŠ¡è®­ç»ƒè¯¾ç¨‹ï¼Œåœ¨æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­ä»»åŠ¡é€šè¿‡ç‡æå‡è¶…è¿‡6%ï¼Œå¹¶éªŒè¯äº†æ¡†æ¶åœ¨å¤šæ¨¡æ€Agentå¾®è°ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè®­ç»ƒäº¤äº’å¼å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨Agentçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸“æ³¨äºå·¥å…·é›†æˆæ¨ç†(TIR)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶‰åŠå¤šè½®è§„åˆ’å’Œé•¿ä¸Šä¸‹æ–‡å¯¹è¯ç®¡ç†çš„å¤æ‚è¿‡ç¨‹ã€‚ä¸ºäº†è®­ç»ƒAgentå¤„ç†è¿™ç§åŠ¨æ€è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ”¯æŒäº¤é”™è¯­éŸ³-æ–‡æœ¬rolloutçš„å¼ºåŒ–å­¦ä¹ sandboxç¯å¢ƒã€‚æˆ‘ä»¬çš„æ ¸å¿ƒç­–ç•¥ï¼ŒTurn-level Adjudicated Reinforcement Learning (TARL)ï¼Œé€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä½œä¸ºè£åˆ¤æ¥æä¾›turn-levelè¯„ä¼°ï¼Œä»è€Œè§£å†³äº†é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚ä¸ºäº†å¢å¼ºæ¢ç´¢ï¼Œæˆ‘ä»¬æ•´åˆäº†ä¸€ä¸ªæ··åˆä»»åŠ¡è®­ç»ƒè¯¾ç¨‹ï¼Œå…¶ä¸­åŒ…å«æ•°å­¦æ¨ç†é—®é¢˜ã€‚è¿™ç§ç»Ÿä¸€çš„æ–¹æ³•ä½¿åŸºäºæ–‡æœ¬çš„$Ï„$-benchä¸Šçš„ä»»åŠ¡é€šè¿‡ç‡æ¯”å¼ºå¤§çš„RLåŸºçº¿æé«˜äº†6%ä»¥ä¸Šã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶é€‚ç”¨äºå¾®è°ƒå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä»¥ç”¨äºAgentä»»åŠ¡ã€‚é€šè¿‡åœ¨äº¤é”™çš„è¯­éŸ³-æ–‡æœ¬rolloutä¸Šè®­ç»ƒåŸºç¡€å¤šæ¨¡æ€LLMï¼Œæˆ‘ä»¬ä½¿å…¶å…·å¤‡äº†å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä¸ºæ›´è‡ªç„¶çš„ã€è¯­éŸ³é©±åŠ¨çš„äº¤äº’å¼Agenté“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰äº¤äº’å¼å·¥å…·ä½¿ç”¨Agentåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œé¢ä¸´å·¥å…·é›†æˆæ¨ç†(TIR)çš„æŒ‘æˆ˜ï¼Œå…·ä½“è¡¨ç°ä¸ºå¤šè½®è§„åˆ’å’Œé•¿ä¸Šä¸‹æ–‡å¯¹è¯ç®¡ç†çš„å›°éš¾ã€‚å°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸‹ï¼ŒAgentéœ€è¦åŒæ—¶ç†è§£å’Œå¤„ç†è¯­éŸ³å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œè¿™è¿›ä¸€æ­¥åŠ å‰§äº†è®­ç»ƒçš„å¤æ‚æ€§ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­å­˜åœ¨ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œéš¾ä»¥æœ‰æ•ˆæŒ‡å¯¼Agentçš„å­¦ä¹ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œå°†å…¶ä½œä¸ºè£åˆ¤ï¼Œå¯¹Agentåœ¨æ¯ä¸ªturnçš„è¡Œä¸ºè¿›è¡Œè¯„ä¼°ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„ä¿¡ç”¨åˆ†é…ã€‚è¿™ç§Turn-level Adjudicated Reinforcement Learning (TARL)æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼Agentåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­çš„å­¦ä¹ ï¼Œå¹¶æé«˜å…¶å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé€šè¿‡æ··åˆä»»åŠ¡è®­ç»ƒè¯¾ç¨‹ï¼Œå¢å¼ºAgentçš„æ¢ç´¢èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„ä»»åŠ¡åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ sandboxç¯å¢ƒï¼Œæ”¯æŒäº¤é”™çš„è¯­éŸ³-æ–‡æœ¬rolloutã€‚Agentä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œç”Ÿæˆè¯­éŸ³å’Œæ–‡æœ¬åºåˆ—ã€‚LLMè£åˆ¤å¯¹Agentåœ¨æ¯ä¸ªturnçš„è¡Œä¸ºè¿›è¡Œè¯„ä¼°ï¼Œç”Ÿæˆå¥–åŠ±ä¿¡å·ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•åˆ©ç”¨è¿™äº›å¥–åŠ±ä¿¡å·æ›´æ–°Agentçš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æ··åˆä»»åŠ¡è®­ç»ƒè¯¾ç¨‹ï¼ŒåŒ…å«æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ï¼Œä»¥å¢å¼ºAgentçš„æ¢ç´¢èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºTurn-level Adjudicated Reinforcement Learning (TARL)æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒTARLèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°è¿›è¡Œä¿¡ç”¨åˆ†é…ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼Agentçš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMä½œä¸ºè£åˆ¤ï¼Œé¿å…äº†äººå·¥è®¾è®¡å¥–åŠ±å‡½æ•°çš„å›°éš¾ï¼Œå¹¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„ä»»åŠ¡åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šLLMè£åˆ¤çš„è®¾è®¡æ˜¯å…³é”®ã€‚éœ€è¦é€‰æ‹©åˆé€‚çš„LLMï¼Œå¹¶è®¾è®¡åˆé€‚çš„promptï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®è¯„ä¼°Agentçš„è¡Œä¸ºã€‚æ··åˆä»»åŠ¡è®­ç»ƒè¯¾ç¨‹çš„è®¾è®¡ä¹Ÿéœ€è¦ä»”ç»†è€ƒè™‘ï¼Œéœ€è¦é€‰æ‹©ä¸ç›®æ ‡ä»»åŠ¡ç›¸å…³çš„ä»»åŠ¡ï¼Œå¹¶åˆç†å®‰æ’è®­ç»ƒé¡ºåºã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„TARLæ–¹æ³•åœ¨åŸºäºæ–‡æœ¬çš„$Ï„$-benchä¸Šï¼Œä»»åŠ¡é€šè¿‡ç‡æ¯”å¼ºå¤§çš„RLåŸºçº¿æé«˜äº†6%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨äº¤é”™çš„è¯­éŸ³-æ–‡æœ¬rolloutä¸Šè®­ç»ƒåŸºç¡€å¤šæ¨¡æ€LLMï¼Œä½¿å…¶å…·å¤‡äº†å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶é€‚ç”¨äºå¾®è°ƒå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä»¥ç”¨äºAgentä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„äº¤äº’å¼Agentï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€å®¢æœæœºå™¨äººç­‰ã€‚è¿™äº›Agentèƒ½å¤Ÿç†è§£ç”¨æˆ·çš„è¯­éŸ³å’Œæ–‡æœ¬æŒ‡ä»¤ï¼Œå¹¶åˆ©ç”¨å„ç§å·¥å…·å®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚å°¤å…¶æ˜¯åœ¨éœ€è¦å¤šè½®äº¤äº’å’Œé•¿ä¸Šä¸‹æ–‡ç†è§£çš„åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨äººæœºäº¤äº’æ–¹å¼çš„å˜é©ï¼Œä½¿äººä»¬èƒ½å¤Ÿæ›´æ–¹ä¾¿åœ°ä½¿ç”¨å„ç§å·¥å…·å’ŒæœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effective interactive tool use requires agents to master Tool Integrated Reasoning (TIR): a complex process involving multi-turn planning and long-context dialogue management. To train agents for this dynamic process, particularly in multi-modal contexts, we introduce a sandbox environment for reinforcement learning (RL) that supports interleaved speech-text rollouts. Our core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses the challenge of credit assignment in long-horizon tasks by employing a Large Language Model (LLM) as a judge to provide turn-level evaluation. To enhance exploration, we integrate a mixed-task training curriculum with mathematical reasoning problems. This unified approach boosts the task pass rate on the text-based $Ï„$-bench by over 6% compared to strong RL baselines. Crucially, we demonstrate our framework's suitability for fine-tuning a multi-modal foundation model for agentic tasks. By training a base multi-modal LLM on interleaved speech-text rollouts, we equip it with tool-use abilities, paving the way for more natural, voice-driven interactive agents.

