---
layout: default
title: Early Stopping Chain-of-thoughts in Large Language Models
---

# Early Stopping Chain-of-thoughts in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14004" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14004v1</a>
  <a href="https://arxiv.org/pdf/2509.14004.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14004v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14004v1', 'Early Stopping Chain-of-thoughts in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minjia Mao, Bowen Yin, Yu Zhu, Xiao Fang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºES-CoTï¼Œé€šè¿‡æå‰åœæ­¢CoTç”Ÿæˆé™ä½å¤§è¯­è¨€æ¨¡å‹æ¨ç†æˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ€ç»´é“¾` `æ¨ç†åŠ é€Ÿ` `æå‰åœæ­¢` `ç­”æ¡ˆæ”¶æ•›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰CoTæ¨ç†æ–¹æ³•ç”Ÿæˆå†—é•¿çš„æ¨ç†é“¾ï¼Œå¯¼è‡´å¤§è¯­è¨€æ¨¡å‹æ¨ç†æˆæœ¬é«˜æ˜‚ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. ES-CoTé€šè¿‡ç›‘æ§æ¨ç†è¿‡ç¨‹ä¸­ç­”æ¡ˆçš„æ”¶æ•›æƒ…å†µï¼Œåœ¨ç­”æ¡ˆè¶‹äºç¨³å®šæ—¶æå‰åœæ­¢ç”Ÿæˆï¼Œä»è€Œç¼©çŸ­æ¨ç†é“¾ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒES-CoTåœ¨ä¿æŒå‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå¹³å‡å‡å°‘äº†çº¦41%çš„æ¨ç†tokenæ•°é‡ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºES-CoTçš„æ¨ç†æ—¶æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ£€æµ‹ç­”æ¡ˆæ”¶æ•›å¹¶æå‰åœæ­¢ç”Ÿæˆï¼Œä»è€Œç¼©çŸ­å¤§è¯­è¨€æ¨¡å‹ä¸­æ€ç»´é“¾ï¼ˆCoTï¼‰çš„ç”Ÿæˆè¿‡ç¨‹ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘æ€§èƒ½æŸå¤±ã€‚åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ç»“æŸæ—¶ï¼Œè¯¥æ–¹æ³•æç¤ºå¤§è¯­è¨€æ¨¡å‹è¾“å‡ºå…¶å½“å‰çš„æœ€ç»ˆç­”æ¡ˆï¼Œç§°ä¸ºæ­¥ç­”æ¡ˆã€‚ç„¶åï¼Œè·Ÿè¸ªè¿ç»­ç›¸åŒæ­¥ç­”æ¡ˆçš„è¿è¡Œé•¿åº¦ï¼Œä»¥æ­¤ä½œä¸ºç­”æ¡ˆæ”¶æ•›çš„åº¦é‡ã€‚ä¸€æ—¦è¿è¡Œé•¿åº¦å‡ºç°æ€¥å‰§å¢åŠ å¹¶è¶…è¿‡æœ€å°é˜ˆå€¼ï¼Œåˆ™ç»ˆæ­¢ç”Ÿæˆã€‚æˆ‘ä»¬ä¸ºè¿™ç§å¯å‘å¼æ–¹æ³•æä¾›äº†ç»éªŒå’Œç†è®ºæ”¯æŒï¼šæ­¥ç­”æ¡ˆç¨³å®šåœ°æ”¶æ•›åˆ°æœ€ç»ˆç­”æ¡ˆï¼Œå¹¶ä¸”å¤§çš„è¿è¡Œé•¿åº¦è·³è·ƒå¯é åœ°æ ‡å¿—ç€è¿™ç§æ”¶æ•›ã€‚åœ¨ä¸‰ä¸ªå¤§è¯­è¨€æ¨¡å‹çš„äº”ä¸ªæ¨ç†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒES-CoTå¹³å‡å‡å°‘äº†çº¦41%çš„æ¨ç†tokenæ•°é‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸æ ‡å‡†CoTç›¸å½“çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒES-CoTä¸è‡ªæ´½æ€§æç¤ºæ— ç¼é›†æˆï¼Œå¹¶åœ¨è¶…å‚æ•°é€‰æ‹©ä¸­ä¿æŒç¨³å¥æ€§ï¼Œçªæ˜¾äº†å…¶ä½œä¸ºä¸€ç§é«˜æ•ˆæ¨ç†çš„å®ç”¨æœ‰æ•ˆæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹é€šè¿‡ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¥è§£å†³å¤æ‚é—®é¢˜ï¼Œä½†CoTçš„é•¿åº¦ç›´æ¥å½±å“æ¨ç†æˆæœ¬ã€‚è¿‡é•¿çš„CoTä¼šå¯¼è‡´ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ï¼Œé™ä½æ¨ç†æ•ˆç‡ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿è¯æ¨ç†æ€§èƒ½çš„å‰æä¸‹ï¼Œå‡å°‘CoTçš„é•¿åº¦ï¼Œé™ä½æ¨ç†æˆæœ¬ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šES-CoTçš„æ ¸å¿ƒæ€è·¯æ˜¯è§‚å¯Ÿåˆ°åœ¨CoTæ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ç»™å‡ºçš„ä¸­é—´ç­”æ¡ˆä¼šé€æ¸æ”¶æ•›åˆ°æœ€ç»ˆç­”æ¡ˆã€‚é€šè¿‡ç›‘æ§ä¸­é—´ç­”æ¡ˆçš„è¿ç»­é‡å¤æ¬¡æ•°ï¼ˆè¿è¡Œé•¿åº¦ï¼‰ï¼Œå¯ä»¥åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦å·²ç»æ”¶æ•›ã€‚å½“è¿è¡Œé•¿åº¦è¶…è¿‡ä¸€å®šé˜ˆå€¼æ—¶ï¼Œè®¤ä¸ºç­”æ¡ˆå·²ç»ç¨³å®šï¼Œå¯ä»¥æå‰åœæ­¢æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šES-CoTçš„æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š
1.  **CoTç”Ÿæˆ**ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”ŸæˆCoTï¼Œæ¯ä¸€æ­¥ç”Ÿæˆä¸€ä¸ªæ¨ç†æ­¥éª¤ã€‚
2.  **æ­¥ç­”æ¡ˆæå–**ï¼šåœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ç»“æŸåï¼Œæç¤ºå¤§è¯­è¨€æ¨¡å‹è¾“å‡ºå…¶å½“å‰çš„æœ€ç»ˆç­”æ¡ˆï¼ˆæ­¥ç­”æ¡ˆï¼‰ã€‚
3.  **è¿è¡Œé•¿åº¦è®¡ç®—**ï¼šè®¡ç®—è¿ç»­ç›¸åŒæ­¥ç­”æ¡ˆçš„è¿è¡Œé•¿åº¦ã€‚
4.  **æå‰åœæ­¢åˆ¤æ–­**ï¼šå¦‚æœè¿è¡Œé•¿åº¦è¶…è¿‡é¢„è®¾çš„æœ€å°é˜ˆå€¼ï¼Œåˆ™ç»ˆæ­¢CoTç”Ÿæˆï¼Œå¹¶å°†å½“å‰æ­¥ç­”æ¡ˆä½œä¸ºæœ€ç»ˆç­”æ¡ˆã€‚
5.  **è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ**ï¼šè¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šES-CoTçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åŸºäºç­”æ¡ˆæ”¶æ•›çš„æå‰åœæ­¢ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„CoTæ–¹æ³•ç›¸æ¯”ï¼ŒES-CoTä¸éœ€è¦ç”Ÿæˆå®Œæ•´çš„CoTï¼Œè€Œæ˜¯åœ¨ç­”æ¡ˆè¶‹äºç¨³å®šæ—¶æå‰åœæ­¢ï¼Œä»è€ŒèŠ‚çœäº†è®¡ç®—èµ„æºã€‚ä¸ä¸€äº›éœ€è¦é¢å¤–è®­ç»ƒçš„æå‰åœæ­¢æ–¹æ³•ä¸åŒï¼ŒES-CoTæ˜¯ä¸€ç§æ¨ç†æ—¶æ–¹æ³•ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šES-CoTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š
1.  **æ­¥ç­”æ¡ˆæå–æ–¹å¼**ï¼šé€šè¿‡ç‰¹å®šçš„promptæ¥å¼•å¯¼LLMè¾“å‡ºå½“å‰æ­¥çš„ç­”æ¡ˆã€‚
2.  **è¿è¡Œé•¿åº¦é˜ˆå€¼**ï¼šéœ€è¦è®¾ç½®ä¸€ä¸ªæœ€å°è¿è¡Œé•¿åº¦é˜ˆå€¼ï¼Œä»¥é¿å…è¿‡æ—©åœæ­¢æ¨ç†ã€‚è®ºæ–‡ä¸­æåˆ°ES-CoTå¯¹è¶…å‚æ•°é€‰æ‹©å…·æœ‰é²æ£’æ€§ï¼Œå› æ­¤é˜ˆå€¼çš„é€‰æ‹©å¯èƒ½ä¸æ˜¯éå¸¸æ•æ„Ÿã€‚
3.  **è¿è¡Œé•¿åº¦è·³è·ƒæ£€æµ‹**ï¼šè®ºæ–‡æåˆ°è¿è¡Œé•¿åº¦çš„æ€¥å‰§å¢åŠ æ˜¯ç­”æ¡ˆæ”¶æ•›çš„å¯é æ ‡å¿—ï¼Œå…·ä½“å¦‚ä½•æ£€æµ‹â€œæ€¥å‰§å¢åŠ â€å¯èƒ½éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒES-CoTåœ¨äº”ä¸ªæ¨ç†æ•°æ®é›†å’Œä¸‰ä¸ªå¤§è¯­è¨€æ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ•ˆæœã€‚å¹³å‡è€Œè¨€ï¼ŒES-CoTå‡å°‘äº†çº¦41%çš„æ¨ç†tokenæ•°é‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸æ ‡å‡†CoTç›¸å½“çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒES-CoTä¸è‡ªæ´½æ€§æç¤ºæ— ç¼é›†æˆï¼Œå¹¶åœ¨è¶…å‚æ•°é€‰æ‹©ä¸­è¡¨ç°å‡ºé²æ£’æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒES-CoTæ˜¯ä¸€ç§å®ç”¨ä¸”æœ‰æ•ˆçš„CoTæ¨ç†åŠ é€Ÿæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ES-CoTå¯åº”ç”¨äºå„ç§éœ€è¦å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¤æ‚æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡é™ä½æ¨ç†æˆæœ¬ï¼ŒES-CoTå¯ä»¥ä½¿å¤§è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ›´é«˜æ•ˆåœ°è¿è¡Œï¼Œå¹¶ä¿ƒè¿›å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºå¯¹æ¨ç†å»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning large language models (LLMs) have demonstrated superior capacities in solving complicated problems by generating long chain-of-thoughts (CoT), but such a lengthy CoT incurs high inference costs. In this study, we introduce ES-CoT, an inference-time method that shortens CoT generation by detecting answer convergence and stopping early with minimal performance loss. At the end of each reasoning step, we prompt the LLM to output its current final answer, denoted as a step answer. We then track the run length of consecutive identical step answers as a measure of answer convergence. Once the run length exhibits a sharp increase and exceeds a minimum threshold, the generation is terminated. We provide both empirical and theoretical support for this heuristic: step answers steadily converge to the final answer, and large run-length jumps reliably mark this convergence. Experiments on five reasoning datasets across three LLMs show that ES-CoT reduces the number of inference tokens by about 41\% on average while maintaining accuracy comparable to standard CoT. Further, ES-CoT integrates seamlessly with self-consistency prompting and remains robust across hyperparameter choices, highlighting it as a practical and effective approach for efficient reasoning.

