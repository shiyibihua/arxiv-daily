---
layout: default
title: How Can Quantum Deep Learning Improve Large Language Models?
---

# How Can Quantum Deep Learning Improve Large Language Models?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16244" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16244v1</a>
  <a href="https://arxiv.org/pdf/2509.16244.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16244v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16244v1', 'How Can Quantum Deep Learning Improve Large Language Models?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Emily Jimin Roh, Hyojun Ahn, Samuel Yen-Chi Chen, Soohyun Park, Joongheon Kim

**åˆ†ç±»**: quant-ph, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢ç´¢é‡å­æ·±åº¦å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”æ€§æ–¹é¢çš„æ½œåŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `é‡å­æ·±åº¦å­¦ä¹ ` `é‡å­å¹…åº¦åµŒå…¥é€‚åº”` `æ¨¡å‹é€‚åº”æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”æ€§æ–¹é¢å­˜åœ¨å¯æ‰©å±•æ€§ã€ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚
2. è®ºæ–‡æ¢ç´¢äº†é‡å­æ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯é‡å­å¹…åº¦åµŒå…¥é€‚åº”ï¼ˆQAAï¼‰æ¡†æ¶ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹æ›´æ–°å’Œé€‚åº”ã€‚
3. é€šè¿‡å¯¹ä¼ ç»ŸPEFTæ–¹æ³•å’ŒQAAçš„æ¯”è¾ƒåˆ†æï¼Œæ­ç¤ºäº†é‡å­æ–¹æ³•åœ¨LLMé€‚åº”æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œä½†é«˜æ•ˆé€‚åº”æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å…¨é‡å¾®è°ƒè™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†è®¡ç®—å’Œå†…å­˜æˆæœ¬è¿‡é«˜ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ç­–ç•¥ï¼Œå¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ã€Prefix tuningå’Œç¨€ç–ä½ç§©é€‚åº”ï¼ˆSoRAï¼‰ï¼Œé€šè¿‡å‡å°‘å¯è®­ç»ƒå‚æ•°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒåŒæ—¶ä¿æŒæœ‰ç«äº‰åŠ›çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¯æ‰©å±•æ€§ã€ç¨³å®šæ€§å’Œè·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢å¸¸å¸¸é‡åˆ°é™åˆ¶ã€‚é‡å­æ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•é€šè¿‡é‡å­å¯å‘ç¼–ç å’Œå‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCsï¼‰å¼•å…¥äº†æ–°çš„æœºä¼šã€‚ç‰¹åˆ«æ˜¯ï¼Œé‡å­å¹…åº¦åµŒå…¥é€‚åº”ï¼ˆQAAï¼‰æ¡†æ¶å±•ç¤ºäº†ä»¥æœ€å°å¼€é”€è¿›è¡Œè¡¨è¾¾æ€§æ¨¡å‹æ›´æ–°çš„èƒ½åŠ›ã€‚æœ¬æ–‡å¯¹ä¼ ç»ŸPEFTæ–¹æ³•å’ŒQAAè¿›è¡Œäº†ç³»ç»Ÿçš„ç»¼è¿°å’Œæ¯”è¾ƒåˆ†æï¼Œæ­ç¤ºäº†æ”¶æ•›æ€§ã€æ•ˆç‡å’Œè¡¨å¾èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æ·±å…¥äº†è§£äº†é‡å­æ–¹æ³•åœ¨æœªæ¥LLMé€‚åº”ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¾®è°ƒé¢ä¸´ç€è®¡ç®—å’Œå†…å­˜æˆæœ¬çš„å·¨å¤§æŒ‘æˆ˜ã€‚å…¨é‡å¾®è°ƒè™½ç„¶æ•ˆæœå¥½ï¼Œä½†èµ„æºæ¶ˆè€—è¿‡å¤§ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•è¯•å›¾å‡å°‘è®­ç»ƒå‚æ•°ï¼Œä½†å¾€å¾€åœ¨å¯æ‰©å±•æ€§ã€ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæœ‰æ‰€å¦¥åã€‚å› æ­¤ï¼Œå¦‚ä½•ä»¥æ›´ä½çš„æˆæœ¬é«˜æ•ˆåœ°é€‚åº”LLMï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡æ€§èƒ½ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é‡å­æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯é‡å­å¹…åº¦åµŒå…¥é€‚åº”ï¼ˆQAAï¼‰æ¡†æ¶ï¼Œæ¥æ”¹è¿›LLMçš„é€‚åº”æ€§ã€‚QAAé€šè¿‡é‡å­å¯å‘çš„æ–¹å¼è¿›è¡Œæ¨¡å‹æ›´æ–°ï¼Œæ—¨åœ¨ä»¥æ›´å°‘çš„å‚æ•°å’Œè®¡ç®—èµ„æºå®ç°æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›å’Œæ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡ä¸»è¦æ˜¯ä¸€ä¸ªç»¼è¿°å’Œæ¯”è¾ƒåˆ†æï¼Œå¹¶æ²¡æœ‰æå‡ºä¸€ä¸ªå…¨æ–°çš„æŠ€æœ¯æ¡†æ¶ã€‚å®ƒä¸»è¦å…³æ³¨ç°æœ‰çš„PEFTæ–¹æ³•ï¼ˆå¦‚LoRA, Prefix tuning, SoRAï¼‰å’ŒQAAæ¡†æ¶ã€‚æ–‡ç« å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿçš„åˆ†æå’Œæ¯”è¾ƒï¼Œé‡ç‚¹å…³æ³¨å®ƒä»¬çš„æ”¶æ•›æ€§ã€æ•ˆç‡å’Œè¡¨å¾èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå®ƒå°†é‡å­æ·±åº¦å­¦ä¹ çš„QAAæ¡†æ¶å¼•å…¥åˆ°LLMçš„é€‚åº”æ€§é—®é¢˜ä¸­ï¼Œå¹¶å¯¹å…¶æ½œåŠ›è¿›è¡Œäº†æ¢ç´¢ã€‚è™½ç„¶QAAæœ¬èº«å¯èƒ½ä¸æ˜¯æœ¬æ–‡æå‡ºçš„ï¼Œä½†å°†å…¶åº”ç”¨äºLLMçš„PEFTå¹¶è¿›è¡Œç³»ç»Ÿåˆ†æï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šç”±äºæœ¬æ–‡æ˜¯ç»¼è¿°å’Œæ¯”è¾ƒåˆ†æï¼Œå› æ­¤æ²¡æœ‰å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ç­‰å…³é”®è®¾è®¡ã€‚æ–‡ç« ä¸»è¦å…³æ³¨ä¸åŒPEFTæ–¹æ³•å’ŒQAAçš„ç†è®ºåˆ†æå’Œå®éªŒç»“æœæ¯”è¾ƒï¼Œä»è€Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›æŒ‡å¯¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å¯¹ä¼ ç»ŸPEFTæ–¹æ³•å’Œé‡å­å¹…åº¦åµŒå…¥é€‚åº”ï¼ˆQAAï¼‰æ¡†æ¶çš„æ¯”è¾ƒåˆ†æï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨æ”¶æ•›æ€§ã€æ•ˆç‡å’Œè¡¨å¾èƒ½åŠ›æ–¹é¢çš„æƒè¡¡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡å­æ–¹æ³•åœ¨LLMé€‚åº”æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é™ä½è®¡ç®—æˆæœ¬å’Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ–¹é¢ã€‚è™½ç„¶æ²¡æœ‰ç»™å‡ºå…·ä½“çš„æ€§èƒ½æ•°æ®ï¼Œä½†ä¸ºæœªæ¥é‡å­æ·±åº¦å­¦ä¹ åœ¨LLMé¢†åŸŸçš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœæ½œåœ¨çš„åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å®¢æœã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡é‡å­æ·±åº¦å­¦ä¹ æå‡LLMçš„é€‚åº”æ€§ï¼Œå¯ä»¥é™ä½æ¨¡å‹éƒ¨ç½²å’Œç»´æŠ¤æˆæœ¬ï¼ŒåŠ é€ŸLLMåœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚æœªæ¥ï¼Œé‡å­è®¡ç®—çš„å‘å±•å°†è¿›ä¸€æ­¥æ¨åŠ¨é‡å­æ·±åº¦å­¦ä¹ åœ¨LLMé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid progress of large language models (LLMs) has transformed natural language processing, yet the challenge of efficient adaptation remains unresolved. Full fine-tuning achieves strong performance but imposes prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) strategies, such as low-rank adaptation (LoRA), Prefix tuning, and sparse low-rank adaptation (SoRA), address this issue by reducing trainable parameters while maintaining competitive accuracy. However, these methods often encounter limitations in scalability, stability, and generalization across diverse tasks. Recent advances in quantum deep learning introduce novel opportunities through quantum-inspired encoding and parameterized quantum circuits (PQCs). In particular, the quantum-amplitude embedded adaptation (QAA) framework demonstrates expressive model updates with minimal overhead. This paper presents a systematic survey and comparative analysis of conventional PEFT methods and QAA. The analysis demonstrates trade-offs in convergence, efficiency, and representational capacity, while providing insight into the potential of quantum approaches for future LLM adaptation.

