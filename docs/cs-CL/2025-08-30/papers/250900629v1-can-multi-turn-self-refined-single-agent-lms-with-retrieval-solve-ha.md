---
layout: default
title: Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?
---

# Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00629" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00629v1</a>
  <a href="https://arxiv.org/pdf/2509.00629.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00629v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00629v1', 'Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Md Tanzib Hosain, Md Kishor Morol

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

**å¤‡æ³¨**: Accepted in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Student Research Workshop), 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/kraritt/zolve)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè½®è‡ªæˆ‘ç²¾ç‚¼å•ä»£ç†è¯­è¨€æ¨¡å‹ä»¥è§£å†³å¤æ‚ç¼–ç¨‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `ç¼–ç¨‹é—®é¢˜` `å¤šè½®æ¨ç†` `è‡ªæˆ‘åˆ¤æ–­` `ä¿¡æ¯æ£€ç´¢` `ç®—æ³•æ€ç»´` `ICPCåŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚ç¼–ç¨‹é—®é¢˜æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç®—æ³•æ€ç»´å’Œä»£ç ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤šè½®è‡ªæˆ‘åˆ¤æ–­ã€åæ€å’Œæ£€ç´¢çš„æ¨ç†æŠ€æœ¯ï¼Œä»¥æé«˜è¯­è¨€æ¨¡å‹åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨ç†æŠ€æœ¯çš„é€šè¿‡ç‡ä»19.1%æå‡è‡³42.2%ï¼Œå¹¶ä¸”åœ¨ç‰¹å®šæŒ‡ä»¤ä¸‹ï¼Œo1æ¨¡å‹èƒ½è§£å†³17ä¸ªä¹‹å‰æ— æ³•è§£å†³çš„é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç«äº‰ç¼–ç¨‹ä¸­ï¼Œè§£å†³å¤æ‚çš„ç®—æ³•é—®é¢˜å¯¹äººç±»æ¥è¯´æ˜¯æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡è¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°è¶³å¤Ÿå…³æ³¨ï¼Œæœ¬æ–‡æå‡ºäº†ICPCåŸºå‡†ï¼ŒåŒ…å«254ä¸ªå›½é™…å¤§å­¦ç¼–ç¨‹ç«èµ›ä»»åŠ¡ã€‚é€šè¿‡é›¶-shoté“¾å¼æ€ç»´æç¤ºï¼Œo1æ¨¡å‹çš„é€šè¿‡ç‡ä»…ä¸º19.1%ã€‚è€Œç»“åˆå¤šè½®è‡ªæˆ‘åˆ¤æ–­ã€åæ€å’Œæ£€ç´¢çš„æœ€ä½³æ¨ç†æŠ€æœ¯ï¼Œä½¿é€šè¿‡ç‡æå‡è‡³42.2%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡äººæœºåä½œæ·±å…¥æ¢è®¨äº†æ¨¡å‹çš„å±€é™æ€§ï¼Œå‘ç°o1åœ¨ç‰¹å®šæŒ‡ä»¤ä¸‹èƒ½å¤Ÿè§£å†³17ä¸ªä¹‹å‰æ— æ³•è§£å†³çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå…·å¤‡æ‰å®ã€å¯Œæœ‰æƒ³è±¡åŠ›å’Œç®—æ³•æ€ç»´çš„è¯­è¨€æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œå¹¶å¼€æºäº†ç›¸å…³ä»£ç å’Œæ•°æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¼–ç¨‹é—®é¢˜ä¸Šçš„ä½é€šè¿‡ç‡ï¼Œç°æœ‰æ–¹æ³•åœ¨ç®—æ³•æ€ç»´å’Œä»£ç ç”Ÿæˆæ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹ç«äº‰ç¼–ç¨‹çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šè½®è‡ªæˆ‘åˆ¤æ–­å’Œåæ€ç»“åˆæ£€ç´¢æŠ€æœ¯ï¼Œåˆ©ç”¨å†å²ä¿¡æ¯æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è§£å†³ç¼–ç¨‹é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œåˆ©ç”¨ICPCåŸºå‡†æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œç„¶ååœ¨æ¨ç†é˜¶æ®µåº”ç”¨å¤šè½®è‡ªæˆ‘åˆ¤æ–­å’Œåæ€æœºåˆ¶ï¼Œæœ€åé€šè¿‡æ£€ç´¢æŠ€æœ¯å¢å¼ºæ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å¤šè½®è‡ªæˆ‘åˆ¤æ–­ä¸æ£€ç´¢ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„æ¨ç†æ¡†æ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•è½®æ¨ç†æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è§£å†³èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¤šè½®å¯¹è¯çš„ç”Ÿæˆæ•ˆæœï¼ŒåŒæ—¶è®¾ç½®äº†å‚æ•°ä»¥å¹³è¡¡è‡ªæˆ‘åˆ¤æ–­ä¸æ£€ç´¢ä¿¡æ¯çš„æƒé‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†æ—¶èƒ½å¤Ÿå……åˆ†åˆ©ç”¨å†å²ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œo1æ¨¡å‹åœ¨æœ€ä½³æ¨ç†æŠ€æœ¯ä¸‹çš„é€šè¿‡ç‡ä»19.1%æå‡è‡³42.2%ï¼Œå¹¶ä¸”åœ¨ç‰¹å®šæŒ‡ä»¤ä¸‹æˆåŠŸè§£å†³äº†17ä¸ªä¹‹å‰æ— æ³•è§£å†³çš„é—®é¢˜ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç¼–ç¨‹è¾…åŠ©å·¥å…·å’Œè‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¯ä»¥ä¸ºå­¦ç”Ÿå’Œå¼€å‘è€…æä¾›æ›´æœ‰æ•ˆçš„å­¦ä¹ å’Œå·¥ä½œæ”¯æŒï¼Œæœªæ¥å¯èƒ½åœ¨è½¯ä»¶å¼€å‘å’Œç®—æ³•è®¾è®¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Among the hardest tasks for humans are those found in competitive programming where problems require sophisticated algorithmic thinking, puzzle solving, and the creation of effective code. As a domain to assess language models (LMs), it has not received enough attention, though. This study presents the ICPC benchmark, which consists of 254 international collegiate programming contest (ICPC) tasks. Each problem includes official analysis, reference code, and sample, high-quality unit, and hidden tests. We are able to develop and evaluate a variety of LM inference techniques for competitive programming with these resources. With zero-shot chain-of-thought prompting, we find that o1 only achieves a 19.1\% pass@1 solve rate. With our best inference technique, which combines multi-turn self-judge with reflection and retrieval over episodic information, raises this to 42.2\%. Furthermore, we conduct a new human-in-the-loop investigation to gain a deeper understanding of the remaining difficulties. Surprisingly, we discover that o1 can solve 17 out of 18 problems that were previously unsolvable by any model or technique with just a few specific instructions. A footstep toward LMs with grounded, imaginative, and algorithmic thinking is provided by our quantitative findings and qualitative research. We open-source our code and data at https://github.com/kraritt/zolve.

