---
layout: default
title: GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction
---

# GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00388" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00388v1</a>
  <a href="https://arxiv.org/pdf/2509.00388.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00388v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00388v1', 'GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuelin Li, Xiangqi Jin, Linfeng Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGraphKVä»¥è§£å†³KVç¼“å­˜ç®¡ç†ä¸­çš„åŠ¨æ€é€‰æ‹©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é”®å€¼ç¼“å­˜` `åŠ¨æ€é€‰æ‹©` `å›¾ç»“æ„` `ä¿¡æ¯ä¼ æ’­` `ä¸Šä¸‹æ–‡ç›¸å…³æ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„KVç¼“å­˜é©±é€ç­–ç•¥ä¾èµ–é™æ€å¯å‘å¼æ–¹æ³•ï¼Œæ— æ³•é€‚åº”æ¨ç†è¿‡ç¨‹ä¸­ä»¤ç‰Œé—´çš„åŠ¨æ€ä¾èµ–å…³ç³»ã€‚
2. GraphKVé€šè¿‡å°†ä»¤ç‰Œå»ºæ¨¡ä¸ºå›¾ä¸­çš„èŠ‚ç‚¹ï¼Œåˆ©ç”¨ä¿¡æ¯ä¼ æ’­æœºåˆ¶åŠ¨æ€æ›´æ–°ä»¤ç‰Œçš„é‡è¦æ€§åˆ†æ•°ï¼Œæå‡äº†ç¼“å­˜ç®¡ç†çš„çµæ´»æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphKVåœ¨KVç¼“å­˜ç®¡ç†ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ç•™ä¸Šä¸‹æ–‡ç›¸å…³çš„ä»¤ç‰Œã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜æ•ˆçš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ç®¡ç†å¯¹äºå¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„é•¿æ–‡æœ¬åºåˆ—è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œä¼ ç»Ÿçš„KVé©±é€ç­–ç•¥å¦‚åŸºäºæ³¨æ„åŠ›åˆ†æ•°çš„top-ké€‰æ‹©ï¼Œä¾èµ–é™æ€å¯å‘å¼æ–¹æ³•ï¼Œæ— æ³•æ•æ‰æ¨ç†è¿‡ç¨‹ä¸­ä»¤ç‰Œä¹‹é—´ä¸æ–­æ¼”å˜çš„éšå«ä¾èµ–å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºGraphKVï¼Œä¸€ä¸ªåŸºäºå›¾çš„æ¡†æ¶ï¼Œé‡æ–°å®šä¹‰äº†KVç¼“å­˜å‹ç¼©ä¸­çš„ä»¤ç‰Œé€‰æ‹©ã€‚åœ¨GraphKVä¸­ï¼Œä»¤ç‰Œè¢«å»ºæ¨¡ä¸ºå…·æœ‰é‡è¦æ€§åˆ†æ•°çš„èŠ‚ç‚¹ï¼Œè¾¹åˆ™è¡¨ç¤ºå®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼å…³ç³»ã€‚é€šè¿‡è¡°å‡ä¿¡å·ä¼ æ’­æœºåˆ¶ï¼Œä»¤ç‰Œçš„é‡è¦æ€§é€šè¿‡å›¾ä¸­çš„ä¿¡æ¯ä¼ æ’­åŠ¨æ€æ›´æ–°ï¼Œä»è€Œå®ç°å¯¹æœ€å…·ä¸Šä¸‹æ–‡æ„ä¹‰çš„ä»¤ç‰Œçš„è‡ªé€‚åº”ä¿ç•™ã€‚GraphKVå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„KVç¼“å­˜é©±é€æ–¹æ³•ä¸­ï¼Œå¦‚SnapKVå’ŒPyramidKVï¼Œå…·æœ‰å³æ’å³ç”¨çš„ç‰¹æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸKVç¼“å­˜ç®¡ç†æ–¹æ³•åœ¨åŠ¨æ€ä»¤ç‰Œé€‰æ‹©ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯é™æ€å¯å‘å¼æ–¹æ³•æ— æ³•é€‚åº”ä»¤ç‰Œé—´çš„å˜åŒ–ä¾èµ–å…³ç³»çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGraphKVçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä»¤ç‰Œè§†ä¸ºå›¾ä¸­çš„èŠ‚ç‚¹ï¼Œé€šè¿‡å»ºç«‹èŠ‚ç‚¹é—´çš„ç›¸ä¼¼æ€§è¾¹ï¼Œåˆ©ç”¨ä¿¡æ¯ä¼ æ’­æœºåˆ¶åŠ¨æ€æ›´æ–°ä»¤ç‰Œçš„é‡è¦æ€§ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„ç¼“å­˜ç®¡ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGraphKVçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»¤ç‰Œå»ºæ¨¡ã€ç›¸ä¼¼æ€§è®¡ç®—ã€ä¿¡æ¯ä¼ æ’­å’Œé‡è¦æ€§æ›´æ–°å››ä¸ªä¸»è¦æ¨¡å—ã€‚ä»¤ç‰Œé€šè¿‡å›¾ç»“æ„è¿æ¥ï¼Œä¿¡æ¯åœ¨èŠ‚ç‚¹é—´ä¼ æ’­ä»¥æ›´æ–°é‡è¦æ€§åˆ†æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šGraphKVçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†å›¾ç»“æ„å’ŒåŠ¨æ€ä¿¡æ¯ä¼ æ’­æœºåˆ¶ï¼Œä½¿å¾—ä»¤ç‰Œçš„é‡è¦æ€§èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´ï¼Œè¿™ä¸ä¼ ç»Ÿçš„é™æ€é€‰æ‹©æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒGraphKVé‡‡ç”¨äº†è¡°å‡ä¿¡å·ä¼ æ’­æœºåˆ¶ï¼Œç¡®ä¿é‡è¦æ€§åˆ†æ•°èƒ½å¤ŸåŠæ—¶åæ˜ ä»¤ç‰Œçš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Œæ­¤å¤–ï¼Œå‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGraphKVåœ¨KVç¼“å­˜ç®¡ç†ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å¦‚SnapKVå’ŒPyramidKVï¼Œæ˜¾è‘—æé«˜äº†ä»¤ç‰Œä¿ç•™çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨åŠ¨æ€é€‰æ‹©ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GraphKVåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„KVç¼“å­˜ç®¡ç†ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡é•¿æ–‡æœ¬åºåˆ—å¤„ç†çš„æ•ˆç‡ã€‚å…¶åŠ¨æ€é€‰æ‹©æœºåˆ¶ä¸ä»…é€‚ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œè¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦é«˜æ•ˆç¼“å­˜ç®¡ç†çš„é¢†åŸŸï¼Œå¦‚å›¾åƒå¤„ç†å’Œæ¨èç³»ç»Ÿï¼Œæœªæ¥å¯èƒ½å¯¹ç›¸å…³æŠ€æœ¯çš„å‘å±•äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient Key-Value (KV) cache management is essential for processing long text sequences in large language models (LLMs), where memory constraints often limit performance. Conventional KV eviction strategies, such as top-k selection based on attention scores, depend on static heuristics that fail to capture the evolving implicit dependencies among tokens during inference. To overcome this, we propose GraphKV, a graph-based framework that redefines token selection for KV cache compression. In GraphKV, tokens are modeled as nodes with importance scores, and edges represent their similarity relationships. Through a decay-signal-propagation mechanism, token importance is dynamically updated by propagating information across the graph, enabling adaptive retention of the most contextually significant tokens. GraphKV can be seamlessly utilized in existing KV cache eviction methods such as SnapKV and PyramidKV in a plug-and-play manner. Codes will be released on Github.

