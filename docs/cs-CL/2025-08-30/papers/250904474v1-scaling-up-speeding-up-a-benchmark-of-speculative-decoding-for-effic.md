---
layout: default
title: Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling
---

# Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04474" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04474v1</a>
  <a href="https://arxiv.org/pdf/2509.04474.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04474v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04474v1', 'Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shengyin Sun, Yiming Li, Xing Li, Yingzhao Lian, Weizhe Lin, Hui-Ling Zhen, Zhiyuan Yang, Chen Chen, Xianzhi Yu, Mingxuan Yuan, Chen Ma

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

**å¤‡æ³¨**: 18 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºå‡†æµ‹è¯•ä»¥æå‡LLMæ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨æµ‹è§£ç ` `æµ‹è¯•æ—¶é—´æ‰©å±•` `n-gramæ–¹æ³•` `æ¨ç†æ•ˆç‡` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿå†—ä½™å’Œé‡å¤çš„æ¨ç†ç—•è¿¹ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒçš„æ¨æµ‹è§£ç æ–¹æ³•ï¼Œä»¥åŠ é€ŸLLMçš„æµ‹è¯•æ—¶é—´æ‰©å±•ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œn-gramæ–¹æ³•åœ¨æ•æ‰é‡å¤æ¨¡å¼æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æµ‹è¯•æ—¶é—´æ‰©å±•å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„é‡è¦èŒƒå¼ï¼Œä½†ç”±äºç”Ÿæˆå†—ä½™å’Œé‡å¤çš„æ¨ç†ç—•è¿¹ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ç¬¬ä¸€ä¸ªç»¼åˆåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨æµ‹è§£ç æ–¹æ³•åœ¨åŠ é€ŸLLMæµ‹è¯•æ—¶é—´æ‰©å±•ä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ä¸€è‡´çš„å®éªŒåè®®ï¼Œæ¯”è¾ƒäº†ä¸‰ç§ä¸»è¦çš„æ¨æµ‹è§£ç æ–¹æ³•ï¼šåŸºäºæ¨¡å‹ã€åŸºäºè®­ç»ƒå’ŒåŸºäºn-gramçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç®€å•çš„n-gramæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é‡å¤æ¨¡å¼ï¼Œå±•ç¤ºäº†ä¸å…¶ä»–æ–¹æ³•ç»“åˆçš„æ½œåŠ›ï¼Œä¿ƒè¿›äº†æ›´å¿«çš„æ¨ç†èƒ½åŠ›ã€‚å¸Œæœ›è¯¥åŸºå‡†èƒ½æ¨åŠ¨æ¨æµ‹è§£ç åœ¨æµ‹è¯•æ—¶é—´æ‰©å±•ä¸­çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨æµ‹è¯•æ—¶é—´æ‰©å±•ä¸­ï¼Œç”±äºå†—ä½™æ¨ç†å¯¼è‡´çš„è®¡ç®—å¼€é”€è¿‡å¤§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºå‡†æµ‹è¯•ï¼Œç³»ç»Ÿè¯„ä¼°æ¨æµ‹è§£ç æ–¹æ³•åœ¨æµ‹è¯•æ—¶é—´æ‰©å±•ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«å…³æ³¨å¦‚ä½•å‡å°‘å†—ä½™æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) å®éªŒåè®®è®¾è®¡ï¼Œç¡®ä¿ä¸åŒæ¨æµ‹è§£ç æ–¹æ³•çš„å…¬å¹³æ¯”è¾ƒï¼›2) ä¸‰ç§æ¨æµ‹è§£ç æ–¹æ³•çš„å®ç°ï¼šåŸºäºæ¨¡å‹ã€åŸºäºè®­ç»ƒå’ŒåŸºäºn-gramï¼›3) å®éªŒç»“æœåˆ†æï¼Œè¯„ä¼°å„æ–¹æ³•åœ¨åŠ é€Ÿæ¨ç†ä¸­çš„æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†æ¨æµ‹è§£ç åœ¨æµ‹è¯•æ—¶é—´æ‰©å±•ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯n-gramæ–¹æ³•åœ¨æ•æ‰é‡å¤æ¨ç†æ¨¡å¼æ–¹é¢çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„å‚æ•°ä»¥ä¼˜åŒ–n-gramæ–¹æ³•çš„æ€§èƒ½ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ¨ç†çš„é€Ÿåº¦ä¸å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œn-gramæ–¹æ³•åœ¨åŠ é€Ÿæ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡äº†çº¦30%çš„æ•ˆç‡ã€‚è¿™ä¸€å‘ç°ä¸ºæ¨æµ‹è§£ç çš„è¿›ä¸€æ­¥ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ•ˆç‡ï¼Œå¯ä»¥æ˜¾è‘—æå‡è¿™äº›åº”ç”¨çš„å“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Test-time scaling has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs) by allocating additional computational resources during inference. However, this paradigm is inherently inefficient due to the generation of redundant and repetitive reasoning traces, leading to significant computational overhead. Speculative decoding offers a promising avenue for mitigating this inefficiency, yet its efficacy in the structured, repetition-rich context of test-time scaling remains largely unexplored. To bridge this gap, we introduce the first comprehensive benchmark designed to evaluate speculative decoding methods for accelerating LLM test-time scaling. Our benchmark provides consistent experimental protocols across representative test-time scaling paradigms (e.g., Best-of-N sampling and multi-round thinking), enabling a fair comparison of three major categories of speculative decoding: model-based, training-based, and n-gram-based methods. Extensive experiments reveal that simple n-gram-based methods effectively capture repetitive patterns, demonstrating unique potential in accelerating test-time scaling. This phenomenon demonstrates the value of integrating n-gram-based methods with model-based or training-based approaches to balance acceleration for both repetitive and diverse reasoning in test-time scaling. We hope this benchmark spurs further research on speculative decoding for test-time scaling, enabling faster and more practical reasoning in LLMs through better handling of repetitive and diverse reasoning paths.

