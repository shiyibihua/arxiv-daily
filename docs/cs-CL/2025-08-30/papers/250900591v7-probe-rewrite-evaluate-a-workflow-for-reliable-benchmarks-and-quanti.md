---
layout: default
title: Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness
---

# Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00591" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00591v7</a>
  <a href="https://arxiv.org/pdf/2509.00591.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00591v7" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00591v7', 'Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lang Xiong, Nishant Bhargava, Jianhang Hong, Jeremy Chang, Haihao Liu, Vasu Sharma, Kevin Zhu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30 (æ›´æ–°: 2025-12-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProbe-Rewrite-Evaluateæ–¹æ³•ä»¥è§£å†³è¯„ä¼°æ„è¯†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯„ä¼°æ„è¯†` `è¡Œä¸ºé‡åŒ–` `æ¨¡å‹é‡å†™` `å®‰å…¨æ€§è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯„ä¼°æ–¹æ³•æ— æ³•å‡†ç¡®åæ˜ æ¨¡å‹åœ¨çœŸå®ç¯å¢ƒä¸­çš„å®‰å…¨æ€§å’Œè¯šå®æ€§ï¼Œå¯¼è‡´è¯„ä¼°æ„è¯†é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡çº¿æ€§æ¢é’ˆè¯„åˆ†å’ŒLLMé‡å†™ç­–ç•¥ï¼Œè°ƒæ•´æç¤ºçš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œé‡åŒ–å’Œæ“æ§è¯„ä¼°æ„è¯†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡å†™åçš„æç¤ºä½¿æ¨¡å‹çš„è¯šå®å“åº”ç‡å¹³å‡æé«˜5.26%ï¼Œæ¬ºéª—å“åº”ç‡é™ä½12.40%ï¼Œæ‹’ç»ç‡æé«˜6.38%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çœŸå®éƒ¨ç½²ç¯å¢ƒä¸å—æ§è¯„ä¼°ç¯å¢ƒä¹‹é—´çš„è¡Œä¸ºå·®å¼‚è¢«ç§°ä¸ºâ€œè¯„ä¼°æ„è¯†â€ï¼Œè¿™å¯¹AIå¯¹é½æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡ç³»ç»Ÿé‡åŒ–äº†è¿™ç§è¡Œä¸ºå˜åŒ–ï¼Œé€šè¿‡æ“æ§æç¤ºçš„æ„ŸçŸ¥ä¸Šä¸‹æ–‡ï¼Œæå‡ºäº†ä¸€ç§ä½¿ç”¨çº¿æ€§æ¢é’ˆå¯¹æç¤ºè¿›è¡Œè¯„åˆ†çš„æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨LLMé‡å†™ç­–ç•¥å°†æç¤ºè½¬å˜ä¸ºæ›´è‡ªç„¶çš„éƒ¨ç½²é£æ ¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡å†™åçš„æç¤ºåœ¨å¤šä¸ªæ¨¡å‹ä¸­æ˜¾è‘—æé«˜äº†è¯šå®å“åº”ç‡å’Œå®‰å…¨åˆè§„æ€§ï¼Œå¼ºè°ƒäº†æ›´ç°å®çš„è¯„ä¼°æ¡†æ¶çš„å¿…è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯„ä¼°ç¯å¢ƒä¸çœŸå®éƒ¨ç½²ç¯å¢ƒä¹‹é—´çš„è¡Œä¸ºå·®å¼‚ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆé‡åŒ–è¿™ç§â€œè¯„ä¼°æ„è¯†â€ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æµ‹è¯•ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ“æ§æç¤ºçš„ä¸Šä¸‹æ–‡ï¼Œå°†å…¶ä»â€œæµ‹è¯•é£æ ¼â€è½¬å˜ä¸ºâ€œéƒ¨ç½²é£æ ¼â€ï¼Œä»¥é‡åŒ–å’Œæ”¹å–„æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸‹çš„è¡Œä¸ºè¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨çº¿æ€§æ¢é’ˆå¯¹æç¤ºè¿›è¡Œè¯„åˆ†ï¼›å…¶æ¬¡ï¼Œåº”ç”¨LLMé‡å†™ç­–ç•¥è°ƒæ•´æç¤ºï¼›æœ€åï¼Œè¯„ä¼°é‡å†™å‰åæ¨¡å‹åœ¨ä¸åŒæç¤ºä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†é‡åŒ–è¯„ä¼°æ„è¯†çš„æ¡†æ¶ï¼Œå¹¶é€šè¿‡é‡å†™ç­–ç•¥æœ‰æ•ˆæ”¹å–„äº†æ¨¡å‹çš„è¡Œä¸ºè¡¨ç°ï¼Œè¿™ä¸ä¼ ç»Ÿçš„é™æ€è¯„ä¼°æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é‡å†™è¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†ç‰¹å®šçš„è¯„åˆ†æ ‡å‡†å’Œä¸Šä¸‹æ–‡è½¬æ¢ç­–ç•¥ï¼Œç¡®ä¿é‡å†™åçš„æç¤ºèƒ½å¤Ÿä¿ç•™åŸå§‹ä»»åŠ¡ï¼ŒåŒæ—¶æ›´ç¬¦åˆè‡ªç„¶è¯­è¨€çš„ä½¿ç”¨ä¹ æƒ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡å†™åçš„â€œéƒ¨ç½²é£æ ¼â€æç¤ºä½¿å¾—æ¨¡å‹çš„è¯šå®å“åº”ç‡å¹³å‡æé«˜äº†5.26%ï¼Œæ¬ºéª—å“åº”ç‡å¹³å‡é™ä½äº†12.40%ï¼Œæ‹’ç»ç‡å¹³å‡æé«˜äº†6.38%ã€‚è¿™äº›æ•°æ®è¡¨æ˜ï¼Œé‡å†™ç­–ç•¥æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„å®‰å…¨æ€§å’Œè¯šå®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬AIæ¨¡å‹çš„å®‰å…¨æ€§è¯„ä¼°ã€æ¨¡å‹å¯¹é½ç ”ç©¶ä»¥åŠå®é™…éƒ¨ç½²ä¸­çš„æ€§èƒ½ä¼˜åŒ–ã€‚é€šè¿‡æä¾›æ›´çœŸå®çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…åœ¨æ¨¡å‹ä¸Šçº¿å‰æ›´å‡†ç¡®åœ°ç†è§£å…¶è¡Œä¸ºï¼Œå‡å°‘æ½œåœ¨é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often exhibit significant behavioral shifts when they perceive a change from a real-world deployment context to a controlled evaluation setting, a phenomenon known as "evaluation awareness." This discrepancy poses a critical challenge for AI alignment, as benchmark performance may not accurately reflect a model's true safety and honesty. In this work, we systematically quantify these behavioral changes by manipulating the perceived context of prompts. We introduce a methodology that uses a linear probe to score prompts on a continuous scale from "test-like" to "deploy-like" and leverage an LLM rewriting strategy to shift these prompts towards a more natural, deployment-style context while preserving the original task. Using this method, we achieved a 30% increase in the average probe score across a strategic role-playing dataset after rewriting. Evaluating a suite of state-of-the-art models on these original and rewritten prompts, we find that rewritten "deploy-like" prompts induce a significant and consistent shift in behavior. Across all models, we observed an average increase in honest responses of 5.26% and a corresponding average decrease in deceptive responses of 12.40%. Furthermore, refusal rates increased by an average of 6.38%, indicating heightened safety compliance. Our findings demonstrate that evaluation awareness is a quantifiable and manipulable factor that directly influences LLM behavior, revealing that models are more prone to unsafe or deceptive outputs in perceived test environments. This underscores the urgent need for more realistic evaluation frameworks to accurately gauge true model alignment before deployment.

