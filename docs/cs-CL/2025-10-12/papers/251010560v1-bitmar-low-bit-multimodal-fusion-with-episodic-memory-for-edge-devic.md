---
layout: default
title: BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices
---

# BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10560" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10560v1</a>
  <a href="https://arxiv.org/pdf/2510.10560.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10560v1" onclick="toggleFavorite(this, '2510.10560v1', 'BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Euhid Aman, Esteban Carlin, Hsing-Kuo Pao, Giovanni Beltrame, Ghaluh Indah Permata Sari, Yie-Tarng Chen

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12

**å¤‡æ³¨**: 6 pages, BabyLM Workshop, EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BitMarï¼šé¢å‘è¾¹ç¼˜è®¾å¤‡çš„ä½æ¯”ç‰¹å¤šæ¨¡æ€èåˆä¸æƒ…æ™¯è®°å¿†æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `è¾¹ç¼˜è®¡ç®—` `ä½æ¯”ç‰¹é‡åŒ–` `æƒ…æ™¯è®°å¿†` `å›¾åƒæè¿°` `BitNet` `Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è·¨æ¨¡æ€æ¨¡å‹è®¡ç®—é‡å¤§ï¼Œéš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. BitMaræå‡ºä¸€ç§é‡åŒ–çš„å¤šæ¨¡æ€Transformerï¼Œåˆ©ç”¨ä½æ¯”ç‰¹ç¼–ç å™¨å’Œæƒ…æ™¯è®°å¿†ï¼Œé™ä½è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBitMaråœ¨ä½å»¶è¿Ÿå’Œå°æ¨¡å‹ä½“ç§¯ä¸‹ï¼Œå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„å›¾åƒæè¿°å’Œå¤šæ¨¡æ€ç†è§£æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è·¨æ³¨æ„åŠ›Transformerå’Œå…¶ä»–å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¯¹é½å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åºå¤§ä¸”å…¨ç²¾åº¦çš„éª¨å¹²ç½‘ç»œä½¿å…¶éš¾ä»¥éƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚è®°å¿†å¢å¼ºæ¶æ„å¯ä»¥æé«˜è¿‡å»ä¸Šä¸‹æ–‡çš„åˆ©ç”¨ç‡ï¼Œä½†å¤§å¤šæ•°å·¥ä½œå¾ˆå°‘å°†å…¶ä¸é¢å‘è¾¹ç¼˜çš„æ¿€è¿›é‡åŒ–ç›¸ç»“åˆã€‚æˆ‘ä»¬æå‡ºäº†BitMarï¼Œä¸€ç§é‡åŒ–çš„å¤šæ¨¡æ€Transformerï¼Œå®ƒæå‡ºäº†ä¸€ç§ç±»ä¼¼äººç±»çš„å¤–éƒ¨æƒ…æ™¯è®°å¿†ï¼Œç”¨äºåœ¨èµ„æºæœ‰é™çš„ç¡¬ä»¶ä¸Šè¿›è¡Œæœ‰æ•ˆçš„å›¾åƒ-æ–‡æœ¬ç”Ÿæˆã€‚BitMaråˆ©ç”¨1.58æ¯”ç‰¹çš„ç¼–ç å™¨ï¼Œä¸€ä¸ªç”¨äºæ–‡æœ¬ï¼ˆBitNeté£æ ¼ï¼‰ï¼Œä¸€ä¸ªç”¨äºè§†è§‰ï¼ˆåŸºäºDiNOv2ï¼‰ï¼Œä»¥åˆ›å»ºç´§å‡‘çš„åµŒå…¥ï¼Œè¿™äº›åµŒå…¥è¢«ç»„åˆå¹¶ç”¨äºæŸ¥è¯¢å›ºå®šå¤§å°çš„é”®å€¼æƒ…æ™¯è®°å¿†ã€‚åœ¨å‘é‡æ£€ç´¢æœŸé—´ï¼ŒBitNetè§£ç å™¨åº”ç”¨é€å±‚è°ƒèŠ‚ï¼Œä»è€Œæé«˜ç”Ÿæˆå†…å®¹çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚è§£ç å™¨è¿˜é‡‡ç”¨å¸¦æœ‰æ»‘åŠ¨çª—å£æœºåˆ¶çš„æ³¨æ„åŠ›æ±‡èšï¼Œä»¥åœ¨ä¸¥æ ¼çš„å†…å­˜é¢„ç®—ä¸‹å¤„ç†é•¿è¾“å…¥æˆ–æµå¼è¾“å…¥ã€‚é€å±‚è°ƒèŠ‚å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›çš„ç»“åˆå®ç°äº†å¼ºå¤§çš„è´¨é‡-é€Ÿåº¦æƒè¡¡ï¼Œä»¥ä½å»¶è¿Ÿå’Œå°æ¨¡å‹å ç”¨ç©ºé—´æä¾›æœ‰ç«äº‰åŠ›çš„å›¾åƒæè¿°å’Œå¤šæ¨¡æ€ç†è§£ã€‚è¿™äº›ç‰¹æ€§ä½¿BitMaréå¸¸é€‚åˆè¾¹ç¼˜éƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è·¨æ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¦‚åŸºäºTransformerçš„æ¨¡å‹ï¼Œé€šå¸¸å…·æœ‰åºå¤§çš„å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨éœ€è¦å®æ—¶å“åº”å’Œæœ¬åœ°å¤„ç†çš„åº”ç”¨åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å¿½ç•¥äº†æ¨¡å‹é‡åŒ–ä¸å¤–éƒ¨è®°å¿†ç»“åˆï¼Œæ— æ³•åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆé™ä½èµ„æºæ¶ˆè€—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBitMarçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä½æ¯”ç‰¹é‡åŒ–æŠ€æœ¯å’Œæƒ…æ™¯è®°å¿†æœºåˆ¶ï¼Œåœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦å’Œå­˜å‚¨éœ€æ±‚ã€‚é€šè¿‡ä½æ¯”ç‰¹ç¼–ç å™¨æå–ç´§å‡‘çš„å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ï¼Œå¹¶åˆ©ç”¨æƒ…æ™¯è®°å¿†å­˜å‚¨å’Œæ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è·¨æ¨¡æ€ä¿¡æ¯èåˆå’Œç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBitMarçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä½æ¯”ç‰¹ç¼–ç å™¨ã€æƒ…æ™¯è®°å¿†æ¨¡å—å’ŒBitNetè§£ç å™¨ã€‚é¦–å…ˆï¼Œä½¿ç”¨1.58æ¯”ç‰¹çš„æ–‡æœ¬ï¼ˆBitNeté£æ ¼ï¼‰å’Œè§†è§‰ï¼ˆDiNOv2-basedï¼‰ç¼–ç å™¨æå–å›¾åƒå’Œæ–‡æœ¬çš„ç´§å‡‘åµŒå…¥ã€‚ç„¶åï¼Œå°†è¿™äº›åµŒå…¥ç»„åˆèµ·æ¥ï¼ŒæŸ¥è¯¢å›ºå®šå¤§å°çš„é”®å€¼æƒ…æ™¯è®°å¿†ï¼Œæ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚æœ€åï¼ŒBitNetè§£ç å™¨åˆ©ç”¨é€å±‚è°ƒèŠ‚å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”Ÿæˆæœ€ç»ˆçš„æ–‡æœ¬æè¿°æˆ–å®Œæˆå…¶ä»–å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šBitMarçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) é‡‡ç”¨æä½æ¯”ç‰¹ï¼ˆ1.58bitï¼‰çš„ç¼–ç å™¨ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å’Œå­˜å‚¨å¼€é”€ï¼›2) å¼•å…¥æƒ…æ™¯è®°å¿†æ¨¡å—ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åˆ©ç”¨èƒ½åŠ›ï¼›3) BitNetè§£ç å™¨é‡‡ç”¨é€å±‚è°ƒèŠ‚å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡å’Œå¤„ç†é•¿åºåˆ—çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šBitMarçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨BitNeté£æ ¼çš„æ–‡æœ¬ç¼–ç å™¨å’ŒDiNOv2-basedçš„è§†è§‰ç¼–ç å™¨ï¼Œä»¥å®ç°é«˜æ•ˆçš„ç‰¹å¾æå–ï¼›2) æƒ…æ™¯è®°å¿†æ¨¡å—é‡‡ç”¨å›ºå®šå¤§å°çš„é”®å€¼å­˜å‚¨ï¼Œä»¥é™åˆ¶å†…å­˜å ç”¨ï¼›3) BitNetè§£ç å™¨é‡‡ç”¨é€å±‚è°ƒèŠ‚ï¼Œæ ¹æ®ä¸åŒå±‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è°ƒæ•´ç”Ÿæˆè¿‡ç¨‹ï¼›4) æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹å¤„ç†é•¿è¾“å…¥åºåˆ—ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„å†…å­˜å ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

BitMaråœ¨ä½å»¶è¿Ÿå’Œå°æ¨¡å‹ä½“ç§¯ä¸‹å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„å›¾åƒæè¿°å’Œå¤šæ¨¡æ€ç†è§£æ€§èƒ½ã€‚é€šè¿‡1.58æ¯”ç‰¹çš„ç¼–ç å™¨å’Œæƒ…æ™¯è®°å¿†æœºåˆ¶ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦å’Œå­˜å‚¨éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBitMaråœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„è·¨æ¨¡æ€ä¿¡æ¯èåˆå’Œç”Ÿæˆï¼Œä¸ºè¾¹ç¼˜è®¡ç®—åº”ç”¨æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BitMaré€‚ç”¨äºå„ç§è¾¹ç¼˜è®¡ç®—åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½ç›‘æ§ã€æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…ç­‰ã€‚å®ƒå¯ä»¥ç”¨äºå›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ã€å¤šæ¨¡æ€å¯¹è¯ç­‰ä»»åŠ¡ï¼Œä¸ºè¾¹ç¼˜è®¾å¤‡æä¾›å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„è¾¹ç¼˜è®¡ç®—ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Cross-attention transformers and other multimodal vision-language models excel at grounding and generation; however, their extensive, full-precision backbones make it challenging to deploy them on edge devices. Memory-augmented architectures enhance the utilization of past context; however, most works rarely pair them with aggressive edge-oriented quantization. We introduce BitMar, a quantized multimodal transformer that proposes an external human-like episodic memory for effective image-text generation on hardware with limited resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and one for vision (DiNOv2-based), to create compact embeddings that are combined and used to query a fixed-size key-value episodic memory. During vector retrieval, the BitNet decoder applies per-layer conditioning, which increases the contextual relevance of generated content. The decoder also employs attention sinks with a sliding-window mechanism to process long or streaming inputs under tight memory budgets. The combination of per-layer conditioning and sliding-window attention achieves a strong quality-speed trade-off, delivering competitive captioning and multimodal understanding at low latency with a small model footprint. These characteristics make BitMar well-suited for edge deployment.

