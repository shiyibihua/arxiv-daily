---
layout: default
title: Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents
---

# Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21252" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21252v1</a>
  <a href="https://arxiv.org/pdf/2506.21252.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21252v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21252v1', 'Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyi Men, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: ACL 2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAgent-RewardBenchä»¥è§£å†³å¤šæ¨¡æ€æ™ºèƒ½ä½“å¥–åŠ±å»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ™ºèƒ½ä½“` `å¥–åŠ±å»ºæ¨¡` `åŸºå‡†è¯„ä¼°` `è‡ªæˆ‘ä¿®æ­£` `å†³ç­–ä¼˜åŒ–` `ç°å®åœºæ™¯` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ™ºèƒ½ä½“ç¼ºä¹å¤–éƒ¨åé¦ˆï¼Œå¯¼è‡´è‡ªæˆ‘ä¿®æ­£å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æå‡ºAgent-RewardBenchåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¥–åŠ±å»ºæ¨¡èƒ½åŠ›ï¼Œæ¶µç›–æ„ŸçŸ¥ã€è§„åˆ’å’Œå®‰å…¨ç­‰å¤šä¸ªç»´åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨å¥–åŠ±å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°æœ‰é™ï¼Œå¼ºè°ƒäº†ä¸“é—¨è®­ç»ƒçš„å¿…è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ï¼Œå¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨ç½‘é¡µå¯¼èˆªå’Œå…·èº«æ™ºèƒ½ç­‰ç°å®ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤–éƒ¨åé¦ˆï¼Œè¿™äº›æ™ºèƒ½ä½“åœ¨è‡ªæˆ‘ä¿®æ­£å’Œæ³›åŒ–æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä½¿ç”¨å¥–åŠ±æ¨¡å‹ä½œä¸ºå¤–éƒ¨åé¦ˆæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œä½†å°šæ— æ˜ç¡®çš„é€‰æ‹©æ ‡å‡†ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦å»ºç«‹ä¸€ä¸ªé’ˆå¯¹æ™ºèƒ½ä½“çš„å¥–åŠ±åŸºå‡†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Agent-RewardBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMså¥–åŠ±å»ºæ¨¡èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†å…·æœ‰ä¸‰ä¸ªå…³é”®ç‰¹å¾ï¼šå¤šç»´åº¦å’Œç°å®åœºæ™¯è¯„ä¼°ã€é€æ­¥å¥–åŠ±è¯„ä¼°ä»¥åŠé€‚å½“çš„éš¾åº¦å’Œé«˜è´¨é‡çš„æ•°æ®é‡‡æ ·ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹è¡¨ç°æœ‰é™ï¼Œå¼ºè°ƒäº†åœ¨æ™ºèƒ½ä½“å¥–åŠ±å»ºæ¨¡æ–¹é¢è¿›è¡Œä¸“é—¨è®­ç»ƒçš„å¿…è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨ç¼ºä¹å¤–éƒ¨åé¦ˆæƒ…å†µä¸‹çš„å¥–åŠ±å»ºæ¨¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆé€‰æ‹©é€‚åˆæ™ºèƒ½ä½“çš„å¥–åŠ±æ¨¡å‹ï¼Œå¯¼è‡´æ™ºèƒ½ä½“åœ¨è‡ªæˆ‘ä¿®æ­£å’Œæ³›åŒ–æ–¹é¢çš„èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºAgent-RewardBenchåŸºå‡†ï¼Œé€šè¿‡å¤šç»´åº¦è¯„ä¼°å’Œé€æ­¥å¥–åŠ±è¯„ä¼°æ¥æå‡æ™ºèƒ½ä½“çš„å¥–åŠ±å»ºæ¨¡èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´ç»†è‡´åœ°åˆ†ææ™ºèƒ½ä½“åœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå¤šç»´åº¦è¯„ä¼°æ¨¡å—ï¼Œé€æ­¥å¥–åŠ±è¯„ä¼°æ¨¡å—ï¼Œä»¥åŠæ•°æ®é‡‡æ ·ä¸éªŒè¯æ¨¡å—ã€‚å¤šç»´åº¦è¯„ä¼°æ¶µç›–æ„ŸçŸ¥ã€è§„åˆ’å’Œå®‰å…¨ç­‰åœºæ™¯ï¼Œé€æ­¥å¥–åŠ±è¯„ä¼°åˆ™å…³æ³¨ä»»åŠ¡æ‰§è¡Œçš„æ¯ä¸€æ­¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†é€æ­¥å¥–åŠ±è¯„ä¼°æœºåˆ¶ï¼Œä½¿å¾—å¯¹æ™ºèƒ½ä½“èƒ½åŠ›çš„è¯„ä¼°æ›´åŠ ç»†è‡´å’Œå…¨é¢ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æ•´ä½“è¯„ä¼°æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é‡‡æ ·æ–¹é¢ï¼Œè®ºæ–‡ä»10ä¸ªå¤šæ ·åŒ–æ¨¡å‹ä¸­è¿›è¡Œç²¾å¿ƒæŠ½æ ·ï¼Œå¹¶é€šè¿‡äººå·¥éªŒè¯ç¡®ä¿æ•°æ®çš„å®Œæ•´æ€§ã€‚åŒæ—¶ï¼Œéš¾åº¦æ§åˆ¶ç­–ç•¥è¢«åº”ç”¨äºä»»åŠ¡è®¾è®¡ï¼Œä»¥ä¿æŒæŒ‘æˆ˜æ€§å¹¶ä¿ƒè¿›æ™ºèƒ½ä½“çš„å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨Agent-RewardBenchåŸºå‡†ä¸Šè¡¨ç°æœ‰é™ï¼Œå¼ºè°ƒäº†åœ¨å¥–åŠ±å»ºæ¨¡æ–¹é¢è¿›è¡Œä¸“é—¨è®­ç»ƒçš„å¿…è¦æ€§ã€‚è¿™ä¸€å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦çš„æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹ŸåŠ©æ‰‹ç­‰å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„å¼€å‘ã€‚é€šè¿‡æå‡å¥–åŠ±å»ºæ¨¡èƒ½åŠ›ï¼ŒAgent-RewardBenchèƒ½å¤Ÿå¸®åŠ©æ™ºèƒ½ä½“åœ¨å¤æ‚çš„ç°å®ç¯å¢ƒä¸­æ›´å¥½åœ°è¿›è¡Œå†³ç­–å’Œè‡ªæˆ‘ä¼˜åŒ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github.

