---
layout: default
title: Prompt-Guided Turn-Taking Prediction
---

# Prompt-Guided Turn-Taking Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21191" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21191v2</a>
  <a href="https://arxiv.org/pdf/2506.21191.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21191v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21191v2', 'Prompt-Guided Turn-Taking Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Koji Inoue, Mikey Elmers, Yahui Fu, Zi Haur Pang, Divesh Lala, Keiko Ochi, Tatsuya Kawahara

**åˆ†ç±»**: cs.CL, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-07-03)

**å¤‡æ³¨**: This paper has been accepted for presentation at SIGdial Meeting on Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's version of the work

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ–‡æœ¬æç¤ºçš„åŠ¨æ€è½®æµé¢„æµ‹æ¨¡å‹ä»¥æ”¹å–„å¯¹è¯ç³»ç»Ÿ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è½®æµé¢„æµ‹` `å¯¹è¯ç³»ç»Ÿ` `æ–‡æœ¬æç¤º` `å˜æ¢å™¨æ¨¡å‹` `è¯­éŸ³æ´»åŠ¨` `äººæœºäº¤äº’` `æ™ºèƒ½å¯¹è¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è½®æµé¢„æµ‹æ¨¡å‹åœ¨é€‚åº”å¯¹è¯ä¸Šä¸‹æ–‡å’Œä¼™ä¼´æ—¶å­˜åœ¨å±€é™ï¼Œéš¾ä»¥å®ç°çµæ´»çš„æ§åˆ¶ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°æ¨¡å‹ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºåŠ¨æ€è°ƒæ•´è½®æµé¢„æµ‹ï¼Œå¢å¼ºäº†å¯¹è¯çš„è‡ªç„¶æ€§å’Œé€‚åº”æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºæœ‰æ•ˆè°ƒæ•´è½®æµæ—¶æœºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è½®æµé¢„æµ‹æ¨¡å‹æ˜¯è¯­éŸ³å¯¹è¯ç³»ç»Ÿå’Œå¯¹è¯æœºå™¨äººä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚è¿‘æœŸçš„æ–¹æ³•åˆ©ç”¨åŸºäºå˜æ¢å™¨çš„æ¶æ„æ¥å®æ—¶é¢„æµ‹è¯­éŸ³æ´»åŠ¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨¡å‹ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºåŠ¨æ€æ§åˆ¶è½®æµé¢„æµ‹ï¼Œå…è®¸é€šè¿‡è¯¸å¦‚â€œæ›´å¿«â€æˆ–â€œæ›´å¹³é™â€çš„æŒ‡ä»¤è¿›è¡Œç›´è§‚å’Œæ˜ç¡®çš„æ§åˆ¶ï¼Œé€‚åº”å¯¹è¯ä¼™ä¼´å’Œä¸Šä¸‹æ–‡ã€‚è¯¥æ¨¡å‹åŸºäºå˜æ¢å™¨çš„è¯­éŸ³æ´»åŠ¨æŠ•å½±ï¼ˆVAPï¼‰æ¨¡å‹ï¼Œç»“åˆäº†æ–‡æœ¬æç¤ºåµŒå…¥ï¼Œåº”ç”¨äºé€šé“é—´å’Œè·¨é€šé“çš„å˜æ¢å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨è¶…è¿‡950å°æ—¶çš„äººç±»å¯¹è¯æ•°æ®è¯„ä¼°äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ã€‚ç”±äºç°æœ‰æ•°æ®é›†ä¸­ç¼ºä¹æ–‡æœ¬æç¤ºæ•°æ®ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆåˆæˆæç¤ºå¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¹¶æœ‰æ•ˆåœ°æ ¹æ®æ–‡æœ¬æç¤ºå˜åŒ–è½®æµæ—¶æœºè¡Œä¸ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰è½®æµé¢„æµ‹æ¨¡å‹åœ¨å¯¹è¯ä¸­ç¼ºä¹çµæ´»æ§åˆ¶çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´é¢„æµ‹ç»“æœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ–‡æœ¬æç¤ºæ¥åŠ¨æ€æ§åˆ¶è½®æµé¢„æµ‹ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ç®€å•çš„æŒ‡ä»¤è°ƒæ•´å¯¹è¯èŠ‚å¥ï¼Œä»è€Œæé«˜å¯¹è¯çš„è‡ªç„¶æ€§å’Œæµç•…æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹åŸºäºå˜æ¢å™¨æ¶æ„ï¼Œä¸»è¦åŒ…æ‹¬è¯­éŸ³æ´»åŠ¨æŠ•å½±ï¼ˆVAPï¼‰æ¨¡å—å’Œæ–‡æœ¬æç¤ºåµŒå…¥æ¨¡å—ï¼Œå‰è€…ç”¨äºå¤„ç†è¯­éŸ³ä¿¡å·ï¼Œåè€…ç”¨äºè§£ææ–‡æœ¬æç¤ºã€‚æ¨¡å‹é€šè¿‡é€šé“é—´å’Œè·¨é€šé“çš„å˜æ¢å™¨è¿›è¡Œä¿¡æ¯èåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†æ–‡æœ¬æç¤ºåµŒå…¥æ•´åˆè¿›è½®æµé¢„æµ‹æ¨¡å‹ä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æŒ‡ä»¤çµæ´»è°ƒæ•´é¢„æµ‹è¡Œä¸ºï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­æ˜¯å‰æ‰€æœªæœ‰çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­ï¼Œæ–‡æœ¬æç¤ºåµŒå…¥é€šè¿‡ç‰¹å®šçš„åµŒå…¥å±‚è¿›è¡Œå¤„ç†ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº†ç»“åˆé¢„æµ‹å‡†ç¡®æ€§å’Œæ—¶åºä¸€è‡´æ€§çš„å¤åˆæŸå¤±ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™åˆ©ç”¨äº†å¤šå±‚å˜æ¢å™¨ä»¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æå‡äº†çº¦15%ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ–‡æœ¬æç¤ºä¸‹ï¼Œè½®æµæ—¶æœºçš„å˜åŒ–è¡¨ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§ï¼ŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€å®¢æœæœºå™¨äººå’Œäººæœºäº¤äº’ç•Œé¢ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¯¹è¯çš„è‡ªç„¶æ€§å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹çš„çµæ´»æ§åˆ¶èƒ½åŠ›å¯èƒ½æ¨åŠ¨æ›´å¤æ‚çš„å¯¹è¯åœºæ™¯çš„å®ç°ï¼Œä¿ƒè¿›äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Turn-taking prediction models are essential components in spoken dialogue systems and conversational robots. Recent approaches leverage transformer-based architectures to predict speech activity continuously and in real-time. In this study, we propose a novel model that enables turn-taking prediction to be dynamically controlled via textual prompts. This approach allows intuitive and explicit control through instructions such as "faster" or "calmer" adapting dynamically to conversational partners and contexts. The proposed model builds upon a transformer-based voice activity projection (VAP) model, incorporating textual prompt embeddings into both channel-wise transformers and a cross-channel transformer. We evaluated the feasibility of our approach using over 950 hours of human-human spoken dialogue data. Since textual prompt data for the proposed approach was not available in existing datasets, we utilized a large language model (LLM) to generate synthetic prompt sentences. Experimental results demonstrated that the proposed model improved prediction accuracy and effectively varied turn-taking timing behaviors according to the textual prompts.

