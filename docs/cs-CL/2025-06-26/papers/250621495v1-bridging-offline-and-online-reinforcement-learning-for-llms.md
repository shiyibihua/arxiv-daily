---
layout: default
title: Bridging Offline and Online Reinforcement Learning for LLMs
---

# Bridging Offline and Online Reinforcement Learning for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21495" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21495v1</a>
  <a href="https://arxiv.org/pdf/2506.21495.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21495v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21495v1', 'Bridging Offline and Online Reinforcement Learning for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jack Lanchantin, Angelica Chen, Janice Lan, Xian Li, Swarnadeep Saha, Tianlu Wang, Jing Xu, Ping Yu, Weizhe Yuan, Jason E Weston, Sainbayar Sukhbaatar, Ilia Kulikov

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„åœ¨çº¿ä¸ç¦»çº¿è®­ç»ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `åœ¨çº¿å­¦ä¹ ` `ç¦»çº¿è®­ç»ƒ` `å¤šä»»åŠ¡å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œç¦»çº¿åˆ°åœ¨çº¿çš„è½¬å˜å­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Œå°¤å…¶åœ¨å¤„ç†ä¸åŒç±»å‹ä»»åŠ¡æ—¶æ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆåœ¨çº¿å’ŒåŠåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–å’Œç¾¤ä½“å¥–åŠ±ç­–ç•¥ä¼˜åŒ–æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¯éªŒè¯å’Œä¸å¯éªŒè¯ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç¦»çº¿è®­ç»ƒæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç¦»çº¿ã€åŠåœ¨çº¿å’Œå®Œå…¨åœ¨çº¿çš„è½¬å˜è¿‡ç¨‹ä¸­ï¼Œé’ˆå¯¹å¯éªŒè¯å’Œä¸å¯éªŒè¯ä»»åŠ¡è¿›è¡Œå®éªŒã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åœ¨çº¿å’ŒåŠåœ¨çº¿çš„ç›´æ¥åå¥½ä¼˜åŒ–åŠç¾¤ä½“å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ç›®æ ‡ï¼Œå‘ç°è¿™äº›æ–¹æ³•åœ¨æ€§èƒ½å’Œæ”¶æ•›æ€§ä¸Šè¡¨ç°ç›¸ä¼¼ï¼Œå‡æ˜¾è‘—ä¼˜äºç¦»çº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†è®­ç»ƒåŠ¨æ€å’Œè¶…å‚æ•°é€‰æ‹©ç­–ç•¥ï¼Œä»¥å®ç°æœ€ä½³ç»“æœã€‚æœ€åï¼Œç ”ç©¶è¡¨æ˜åŒæ—¶å¤„ç†å¯éªŒè¯å’Œä¸å¯éªŒè¯å¥–åŠ±çš„å¤šä»»åŠ¡å­¦ä¹ èƒ½å¤Ÿæå‡ä¸¤ç±»ä»»åŠ¡çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç¦»çº¿åˆ°åœ¨çº¿è®­ç»ƒè½¬å˜è¿‡ç¨‹ä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨å¯éªŒè¯å’Œä¸å¯éªŒè¯ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ç±»å‹ä¸‹çš„é€‚åº”æ€§å’Œæ•ˆç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†åœ¨çº¿å’ŒåŠåœ¨çº¿å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–å’Œç¾¤ä½“å¥–åŠ±ç­–ç•¥ä¼˜åŒ–æ¥æå‡æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨é€šè¿‡åŠ¨æ€è°ƒæ•´å­¦ä¹ ç­–ç•¥æ¥é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šç¦»çº¿è®­ç»ƒé˜¶æ®µã€åŠåœ¨çº¿å¾®è°ƒé˜¶æ®µå’Œå®Œå…¨åœ¨çº¿å­¦ä¹ é˜¶æ®µã€‚æ¯ä¸ªé˜¶æ®µéƒ½é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡ç±»å‹å’Œæ•°æ®ç‰¹æ€§è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿æ¨¡å‹åœ¨å„ä¸ªé˜¶æ®µçš„è¡¨ç°éƒ½èƒ½è¾¾åˆ°æœ€ä½³ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åœ¨çº¿å’ŒåŠåœ¨çº¿å­¦ä¹ çš„ç»“åˆç­–ç•¥ï¼Œå‘ç°è¿™ä¸¤ç§æ–¹æ³•åœ¨æ€§èƒ½å’Œæ”¶æ•›æ€§ä¸Šè¡¨ç°ç›¸ä¼¼ï¼Œä¸”å‡ä¼˜äºä¼ ç»Ÿçš„ç¦»çº¿æ–¹æ³•ã€‚è¿™ä¸€å‘ç°ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¶…å‚æ•°è®¾ç½®ä¸Šï¼Œè®ºæ–‡è¯¦ç»†åˆ†æäº†ä¸åŒä»»åŠ¡çš„æœ€ä½³å‚æ•°é€‰æ‹©ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¯éªŒè¯æ•°å­¦ä»»åŠ¡å’Œä¸å¯éªŒè¯æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œæ‰€æœ‰åœ¨çº¿å’ŒåŠåœ¨çº¿æ–¹æ³•çš„æ€§èƒ½å‡æ˜¾è‘—ä¼˜äºç¦»çº¿æ–¹æ³•ï¼Œä¸”æ”¶æ•›é€Ÿåº¦ç›¸ä¼¼ï¼Œæ˜¾ç¤ºå‡ºå¼ºåŒ–å­¦ä¹ åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–æ•™è‚²ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate the effectiveness of reinforcement learning methods for finetuning large language models when transitioning from offline to semi-online to fully online regimes for both verifiable and non-verifiable tasks. Our experiments cover training on verifiable math as well as non-verifiable instruction following with a set of benchmark evaluations for both. Across these settings, we extensively compare online and semi-online Direct Preference Optimization and Group Reward Policy Optimization objectives, and surprisingly find similar performance and convergence between these variants, which all strongly outperform offline methods. We provide a detailed analysis of the training dynamics and hyperparameter selection strategies to achieve optimal results. Finally, we show that multi-tasking with verifiable and non-verifiable rewards jointly yields improved performance across both task types.

