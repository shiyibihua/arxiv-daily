---
layout: default
title: Towards Transparent AI: A Survey on Explainable Large Language Models
---

# Towards Transparent AI: A Survey on Explainable Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21812" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21812v1</a>
  <a href="https://arxiv.org/pdf/2506.21812.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21812v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21812v1', 'Towards Transparent AI: A Survey on Explainable Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Avash Palikhe, Zhenyu Yu, Zichong Wang, Wenbin Zhang

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°å¯è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶è¿›å±•ä¸æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `å¤§å‹è¯­è¨€æ¨¡å‹` `å˜æ¢å™¨æ¶æ„` `å†³ç­–é€æ˜æ€§` `é«˜é£é™©åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨é«˜é£é™©é¢†åŸŸçš„åº”ç”¨ã€‚
2. æœ¬æ–‡é€šè¿‡åˆ†ç±»ä¸åŒçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½æ–¹æ³•ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†åŸºäºå˜æ¢å™¨æ¶æ„çš„å¯è§£é‡Šæ€§æŠ€æœ¯ã€‚
3. ç ”ç©¶æ¢è®¨äº†è¿™äº›å¯è§£é‡Šæ€§æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘å’ŒæŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œä½†å…¶å†³ç­–è¿‡ç¨‹å¾€å¾€éš¾ä»¥è§£é‡Šï¼Œå¯¼è‡´å…¶æˆä¸ºâ€œé»‘ç®±â€ï¼Œè¿™åœ¨é«˜é£é™©é¢†åŸŸåº”ç”¨ä¸­å°¤ä¸ºçªå‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†å¤šç§å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨ä¸ºLLMsæä¾›äººç±»å¯ç†è§£çš„è§£é‡Šã€‚ç„¶è€Œï¼Œç›®å‰å¯¹è¿™äº›æ–¹æ³•çš„ç³»ç»Ÿæ€§ç†è§£ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ç»¼è¿°äº†åŸºäºLLMsçš„ä¸åŒå˜æ¢å™¨æ¶æ„ï¼ˆç¼–ç å™¨ã€è§£ç å™¨å’Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼‰åˆ†ç±»çš„å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œå¹¶æ¢è®¨äº†è¿™äº›æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„è¯„ä¼°å’Œåˆ©ç”¨ï¼Œæœ€åè®¨è®ºäº†å¯ç”¨èµ„æºã€ç ”ç©¶æŒ‘æˆ˜åŠæœªæ¥æ–¹å‘ï¼Œä»¥æ¨åŠ¨é€æ˜å’Œè´Ÿè´£ä»»çš„LLMsçš„å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸è¶³é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é«˜é£é™©é¢†åŸŸçš„åº”ç”¨å—åˆ°é™åˆ¶ï¼Œç¼ºä¹ç³»ç»Ÿçš„ç†è§£å’Œè¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹å¯è§£é‡Šäººå·¥æ™ºèƒ½æ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼ŒåŸºäºä¸åŒçš„å˜æ¢å™¨æ¶æ„ï¼ˆç¼–ç å™¨ã€è§£ç å™¨ã€ç¼–ç å™¨-è§£ç å™¨ï¼‰ï¼Œæä¾›äººç±»å¯ç†è§£çš„è§£é‡Šï¼Œå¢å¼ºæ¨¡å‹é€æ˜åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹ä¸åŒXAIæ–¹æ³•çš„åˆ†ç±»ã€è¯„ä¼°æ ‡å‡†çš„åˆ¶å®šä»¥åŠåœ¨å®é™…åº”ç”¨ä¸­çš„æ¡ˆä¾‹åˆ†æï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ–¹æ³•åˆ†ç±»ã€è¯„ä¼°æŒ‡æ ‡å’Œåº”ç”¨å®ä¾‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°åˆ†ç±»å’Œè¯„ä¼°å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®å¯¹LLMså¯è§£é‡Šæ€§æ–¹æ³•çš„ç†è§£ç©ºç™½ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•åˆ†ç±»ä¸­ï¼Œè€ƒè™‘äº†ä¸åŒå˜æ¢å™¨æ¶æ„çš„ç‰¹ç‚¹ï¼Œè®¾è®¡äº†ç›¸åº”çš„è¯„ä¼°æ ‡å‡†ï¼Œç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆè¡¡é‡å¯è§£é‡Šæ€§æŠ€æœ¯çš„å®é™…æ•ˆæœã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©å°šæœªè¯¦ç»†æŠ«éœ²ï¼Œå¾…è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡åˆ†ç±»å’Œè¯„ä¼°ä¸åŒçš„å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®å°šæœªæŠ«éœ²ï¼Œä½†ç ”ç©¶æŒ‡å‡ºï¼Œé‡‡ç”¨è¿™äº›æ–¹æ³•åï¼Œæ¨¡å‹åœ¨é«˜é£é™©é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å¾—åˆ°äº†æå‡ï¼Œç”¨æˆ·ä¿¡ä»»åº¦ä¹Ÿæœ‰æ‰€å¢åŠ ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ç­‰é«˜é£é™©è¡Œä¸šï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ¨¡å‹çš„å¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„é€æ˜åº¦ï¼Œèƒ½å¤Ÿå¢å¼ºç”¨æˆ·ä¿¡ä»»ï¼Œä¿ƒè¿›å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›é‡‡ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½å†³ç­–çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have played a pivotal role in advancing Artificial Intelligence (AI). However, despite their achievements, LLMs often struggle to explain their decision-making processes, making them a 'black box' and presenting a substantial challenge to explainability. This lack of transparency poses a significant obstacle to the adoption of LLMs in high-stakes domain applications, where interpretability is particularly essential. To overcome these limitations, researchers have developed various explainable artificial intelligence (XAI) methods that provide human-interpretable explanations for LLMs. However, a systematic understanding of these methods remains limited. To address this gap, this survey provides a comprehensive review of explainability techniques by categorizing XAI methods based on the underlying transformer architectures of LLMs: encoder-only, decoder-only, and encoder-decoder models. Then these techniques are examined in terms of their evaluation for assessing explainability, and the survey further explores how these explanations are leveraged in practical applications. Finally, it discusses available resources, ongoing research challenges, and future directions, aiming to guide continued efforts toward developing transparent and responsible LLMs.

