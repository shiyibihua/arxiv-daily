---
layout: default
title: Potemkin Understanding in Large Language Models
---

# Potemkin Understanding in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21521" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21521v2</a>
  <a href="https://arxiv.org/pdf/2506.21521.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21521v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21521v2', 'Potemkin Understanding in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Marina Mancoridis, Bec Weeks, Keyon Vafa, Sendhil Mullainathan

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-06-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå½¢å¼æ¡†æ¶ä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç†è§£èƒ½åŠ›` `åŸºå‡†æµ‹è¯•` `å½¢å¼æ¡†æ¶` `æ¦‚å¿µè¡¨ç¤º` `è¯„ä¼°æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¾èµ–äºåŸºå‡†æ•°æ®é›†ï¼Œä½†ç¼ºä¹å¯¹LLMsç†è§£èƒ½åŠ›çš„æ·±å…¥åˆ†æï¼Œå¯èƒ½å¯¼è‡´è¯¯å¯¼æ€§ç»“è®ºã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå½¢å¼æ¡†æ¶ï¼Œå¼ºè°ƒåŸºå‡†æµ‹è¯•çš„æœ‰æ•ˆæ€§ä¾èµ–äºLLMsä¸äººç±»çš„ç†è§£ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡äº†é‡åŒ–æ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç†è§£å¹»è§‰åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­æ™®éå­˜åœ¨ï¼Œä¸”åæ˜ äº†æ¦‚å¿µè¡¨ç¤ºçš„æ·±å±‚ä¸ä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸é€šè¿‡åŸºå‡†æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œä½†å¦‚ä½•åˆç†æ¨æ–­å…¶èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚æœ¬æ–‡é¦–å…ˆå¼•å…¥ä¸€ä¸ªå½¢å¼æ¡†æ¶ï¼ŒæŒ‡å‡ºç”¨äºæµ‹è¯•LLMsçš„åŸºå‡†ï¼ˆå¦‚APè€ƒè¯•ï¼‰åŒæ ·ç”¨äºæµ‹è¯•äººç±»ã€‚ç„¶è€Œï¼Œè¿™æ„å‘³ç€è¿™äº›åŸºå‡†åªæœ‰åœ¨LLMsçš„è¯¯è§£æ–¹å¼ä¸äººç±»ç›¸ä¼¼æ—¶æ‰æœ‰æ•ˆã€‚å¦åˆ™ï¼ŒåŸºå‡†çš„æˆåŠŸä»…å±•ç¤ºäº†è¡¨é¢ç†è§£ï¼Œå³é€šè¿‡ä¸äººç±»ç†è§£ä¸ç¬¦çš„ç­”æ¡ˆæ‰€é©±åŠ¨çš„ç†è§£å¹»è§‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§é‡åŒ–è¿™ç§ç°è±¡çš„æ–¹æ³•ï¼Œå‘ç°è¿™ç§ç†è§£å¹»è§‰åœ¨æ¨¡å‹ã€ä»»åŠ¡å’Œé¢†åŸŸä¸­æ™®éå­˜åœ¨ï¼Œå¹¶ä¸”è¿™äº›å¤±è´¥ä¸ä»…åæ˜ äº†é”™è¯¯çš„ç†è§£ï¼Œè¿˜æ­ç¤ºäº†æ¦‚å¿µè¡¨ç¤ºä¸­çš„æ·±å±‚å†…éƒ¨ä¸ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•åˆç†è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºåŸºå‡†æµ‹è¯•ï¼Œä½†æœªè€ƒè™‘LLMsä¸äººç±»ç†è§£çš„å·®å¼‚ï¼Œå¯èƒ½å¯¼è‡´è¯¯å¯¼æ€§ç»“è®ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å½¢å¼æ¡†æ¶ï¼Œå¼ºè°ƒåŸºå‡†æµ‹è¯•çš„æœ‰æ•ˆæ€§ä¾èµ–äºLLMsä¸äººç±»çš„ç†è§£ä¸€è‡´æ€§ã€‚é€šè¿‡è®¾è®¡ç‰¹å®šçš„åŸºå‡†å’Œé€šç”¨ç¨‹åºæ¥é‡åŒ–ç†è§£å¹»è§‰çš„å­˜åœ¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯åŸºäºç‰¹å®šé¢†åŸŸçš„åŸºå‡†æµ‹è¯•ï¼ŒäºŒæ˜¯æä¾›ç†è§£å¹»è§‰æ™®éæ€§çš„ä¸‹é™ä¼°è®¡çš„é€šç”¨ç¨‹åºã€‚è¿™ä¸¤ä¸ªæ¨¡å—ç›¸è¾…ç›¸æˆï¼Œå…±åŒéªŒè¯LLMsçš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†é‡åŒ–ç†è§£å¹»è§‰çš„ä¸¤ç§æ–¹æ³•ï¼Œæ­ç¤ºäº†LLMsåœ¨å¤šä¸ªæ¨¡å‹å’Œä»»åŠ¡ä¸­æ™®éå­˜åœ¨çš„ç†è§£å¹»è§‰ç°è±¡ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„è¯„ä¼°æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡å…³æ³¨äº†åŸºå‡†æµ‹è¯•çš„é€‰æ‹©å’Œæ„å»ºï¼Œç¡®ä¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåæ˜ äººç±»çš„ç†è§£æ–¹å¼ï¼Œå¹¶é€šè¿‡ç»Ÿè®¡åˆ†ææ–¹æ³•æ¥è¯„ä¼°ç†è§£å¹»è§‰çš„å­˜åœ¨å’Œç¨‹åº¦ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç†è§£å¹»è§‰åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­æ™®éå­˜åœ¨ï¼Œä¸”è¿™ç§ç°è±¡ä¸ä»…åæ˜ äº†é”™è¯¯çš„ç†è§£ï¼Œè¿˜æ­ç¤ºäº†æ¦‚å¿µè¡¨ç¤ºçš„æ·±å±‚ä¸ä¸€è‡´æ€§ã€‚å…·ä½“æ•°æ®è¡¨æ˜ï¼Œå¤šä¸ªæ¨¡å‹åœ¨ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ç†è§£åå·®ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è®ºæ–‡çš„æ ¸å¿ƒè§‚ç‚¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²è¯„ä¼°ã€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é€æ˜æ€§å’Œå¯è§£é‡Šæ€§ç­‰ã€‚é€šè¿‡æ›´å‡†ç¡®åœ°è¯„ä¼°LLMsçš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ·±åº¦ç†è§£çš„ä»»åŠ¡ä¸­ï¼Œå¦‚æ³•å¾‹ã€åŒ»å­¦å’Œæ•™è‚²ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are regularly evaluated using benchmark datasets. But what justifies making inferences about an LLM's capabilities based on its answers to a curated set of questions? This paper first introduces a formal framework to address this question. The key is to note that the benchmarks used to test LLMs -- such as AP exams -- are also those used to test people. However, this raises an implication: these benchmarks are only valid tests if LLMs misunderstand concepts in ways that mirror human misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin understanding: the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept. We present two procedures for quantifying the existence of potemkins: one using a specially designed benchmark in three domains, the other using a general procedure that provides a lower-bound on their prevalence. We find that potemkins are ubiquitous across models, tasks, and domains. We also find that these failures reflect not just incorrect understanding, but deeper internal incoherence in concept representations.

