---
layout: default
title: Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning
---

# Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22510" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22510v1</a>
  <a href="https://arxiv.org/pdf/2506.22510.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22510v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22510v1', 'Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihao Zhao, Xinlong Zhai, Jinyu Yang, Chuan Shi

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: 16 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMDGCLä»¥è§£å†³å¤šé¢†åŸŸå›¾å¯¹æ¯”å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾å¯¹æ¯”å­¦ä¹ ` `å¤šé¢†åŸŸè¿ç§»` `åŸºç¡€æ¨¡å‹` `é¢†åŸŸæ³¨æ„æœºåˆ¶` `çŸ¥è¯†è½¬ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾å¯¹æ¯”å­¦ä¹ æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†ä¸åŒé¢†åŸŸä¹‹é—´çš„è¯­ä¹‰å’Œå±æ€§å·®å¼‚ï¼Œå¯¼è‡´çŸ¥è¯†è¿ç§»æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºMDGCLæ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ ç­–ç•¥è¯†åˆ«é¢†åŸŸå·®å¼‚ï¼Œå¹¶å¼•å…¥é¢†åŸŸä»¤ç‰Œå’Œé¢†åŸŸæ³¨æ„æœºåˆ¶ä»¥å®ç°çŸ¥è¯†è½¬ç§»ã€‚
3. åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMDGCLåœ¨å‡†ç¡®ç‡å’ŒMacro-F1å¾—åˆ†ä¸Šåˆ†åˆ«æå‡äº†19.33%å’Œ19.13%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºç¡€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä¸»è¦å¾—ç›Šäºå…¶åœ¨é¢„è®­ç»ƒä¸­æ•´åˆå¤šé¢†åŸŸçŸ¥è¯†çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé’ˆå¯¹å›¾æ•°æ®ï¼Œå°¤å…¶æ˜¯æ²¡æœ‰æ–‡æœ¬ç‰¹å¾çš„å›¾ï¼Œç°æœ‰çš„å¯¹æ¯”é¢„è®­ç»ƒç­–ç•¥æœªèƒ½æœ‰æ•ˆå¸æ”¶ä¸åŒé¢†åŸŸçš„çŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šé¢†åŸŸé¢„è®­ç»ƒå’Œè·¨é¢†åŸŸè¿ç§»æ¡†æ¶MDGCLï¼Œé€šè¿‡è®¾è®¡å¯¹æ¯”å­¦ä¹ ç­–ç•¥è¯†åˆ«é¢†åŸŸå·®å¼‚ï¼Œå¹¶å¼•å…¥é¢†åŸŸæ³¨æ„æœºåˆ¶å®ç°ç»†ç²’åº¦çŸ¥è¯†è½¬ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå‡†ç¡®ç‡å’ŒMacro-F1å¾—åˆ†åˆ†åˆ«æå‡äº†19.33%å’Œ19.13%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å›¾å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨å¤šé¢†åŸŸåœºæ™¯ä¸‹çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ä¸åŒé¢†åŸŸå›¾ä¹‹é—´çš„è¯­ä¹‰å’Œå±æ€§å·®å¼‚å¯¼è‡´çš„çŸ¥è¯†è¿ç§»æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMDGCLæ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡å¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥è¯†åˆ«å’Œæ•æ‰é¢†åŸŸå·®å¼‚ï¼Œå¹¶å¼•å…¥é¢†åŸŸä»¤ç‰Œä»¥ç¼–ç é¢†åŸŸçº§çš„å…¨å±€ä¿¡æ¯ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„çŸ¥è¯†è½¬ç§»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMDGCLæ¡†æ¶åˆ†ä¸ºé¢„è®­ç»ƒé˜¶æ®µå’Œä¸‹æ¸¸ä»»åŠ¡é˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ ç­–ç•¥è¯†åˆ«é¢†åŸŸå·®å¼‚ï¼›åœ¨ä¸‹æ¸¸é˜¶æ®µï¼Œå¼•å…¥é¢†åŸŸæ³¨æ„æœºåˆ¶ä»¥å®ç°ç»†ç²’åº¦çš„çŸ¥è¯†è½¬ç§»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥é¢†åŸŸä»¤ç‰Œå’Œé¢†åŸŸæ³¨æ„æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œåˆ©ç”¨ä¸åŒé¢†åŸŸçš„çŸ¥è¯†ï¼Œä»è€Œå…‹æœä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–é¢†åŸŸå·®å¼‚çš„è¯†åˆ«ï¼ŒåŒæ—¶åœ¨é¢†åŸŸæ³¨æ„æœºåˆ¶ä¸­è®¾ç½®äº†å‚æ•°ä»¥æ§åˆ¶ä¸åŒé¢†åŸŸçŸ¥è¯†çš„æƒé‡ï¼Œä»è€Œå®ç°æ›´ç²¾ç»†çš„çŸ¥è¯†è½¬ç§»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMDGCLåœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æå‡äº†19.33%ï¼ŒMacro-F1å¾—åˆ†æå‡äº†19.13%ï¼Œæ˜¾è‘—è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿå’Œå…¶ä»–éœ€è¦å¤„ç†å›¾æ•°æ®çš„å¤šé¢†åŸŸä»»åŠ¡ã€‚é€šè¿‡æœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ï¼ŒMDGCLèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models have achieved great success in natural language processing (NLP) and computer vision (CV). Their success largely stems from the ability to integrate multi-domain knowledge in pre-training and transfer it to target domains. Considering graph data, especially graphs without textual features, is ubiquitous in real-world applications such as social networks and recommendation systems, some researchers have attempted to extend this paradigm to the graph field, aiming to construct graph foundation models. However, unlike CV and NLP, there are huge gaps among the semantics and properties of graphs in different domains, while current works still adopt traditional contrastive pre-training strategies designed in the single-domain scenario, which regard contrastive samples from different domains as equivalent. From experimental investigations, we discovered that inherent domain-specific differences prevent these strategies from effectively absorbing knowledge from different domains to generate informative representations. In this paper, we propose a novel multi-domain pre-training and cross-domain transfer framework, namely MDGCL.In the pre-training stage, we design a contrastive learning strategy to substantially recognize and capture domain differences, and introduce domain tokens to encode domain-level global information. In the downstream stage, we introduce a domain attention mechanism to enable fine-grained domain knowledge transfer. Extensive experiments on five benchmark datasets have demonstrated that our method outperforms state-of-the-art significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\% on Macro-F1 score.

