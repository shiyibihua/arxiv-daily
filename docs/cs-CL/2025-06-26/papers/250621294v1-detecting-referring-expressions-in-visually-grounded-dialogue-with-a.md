---
layout: default
title: Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models
---

# Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21294" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21294v1</a>
  <a href="https://arxiv.org/pdf/2506.21294.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21294v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21294v1', 'Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bram Willemsen, Gabriel Skantze

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: Accepted for publication at XLLM @ ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–‡æœ¬è‡ªå›å½’æ¨¡å‹ä»¥è§£å†³è§†è§‰å¯¹è¯ä¸­çš„æŒ‡ç§°è¡¨è¾¾æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒ‡ç§°è¡¨è¾¾` `è§†è§‰å¯¹è¯` `è‡ªå›å½’æ¨¡å‹` `è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€é—®é¢˜` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§†è§‰å¯¹è¯ä¸­ç¼ºä¹æœ‰æ•ˆçš„æŒ‡ç§°è¡¨è¾¾æ£€æµ‹ï¼Œå°¤å…¶æ˜¯ä»…ä¾èµ–è¯­è¨€ä¸Šä¸‹æ–‡æ—¶çš„å±€é™æ€§ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡è°ƒæ•´é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¥è¯†åˆ«å¯¹è¯ä¸­çš„æŒ‡ç§°è¡¨è¾¾èŒƒå›´ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–‡æœ¬è‡ªå›å½’æ–¹æ³•åœ¨å°æ•°æ®é›†ä¸Šä¹Ÿèƒ½æœ‰æ•ˆå·¥ä½œï¼Œå¼ºè°ƒäº†è¯­è¨€ä¸Šä¸‹æ–‡çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨æ–‡æœ¬è‡ªå›å½’è¯­è¨€æ¨¡å‹ä»è§†è§‰åŸºç¡€å¯¹è¯ä¸­æå–æŒ‡ç§°è¡¨è¾¾çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶æ—¨åœ¨è€ƒå¯Ÿä»…é€šè¿‡è¯­è¨€ä¸Šä¸‹æ–‡èƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¸®åŠ©è¯†åˆ«åœ¨å¯¹è¯è§†è§‰èƒŒæ™¯ä¸­å¯æ„ŸçŸ¥çš„æŒ‡ç§°ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¯¹é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€šè¿‡ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¥æ ‡è®°å¯¹è¯ä¸­çš„æŒ‡ç§°èŒƒå›´è¾¹ç•Œã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨ä¸­ç­‰è§„æ¨¡çš„è¯­è¨€æ¨¡å‹å’Œç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ï¼Œæ–‡æœ¬è‡ªå›å½’æ–¹æ³•åœ¨æ­¤ä»»åŠ¡ä¸­ä¾ç„¶æœ‰æ•ˆï¼Œå¼ºè°ƒäº†è¯­è¨€ä¸Šä¸‹æ–‡çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œä½œè€…ä¹ŸæŒ‡å‡ºè¯¥ä»»åŠ¡æœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€çš„ï¼Œè®¨è®ºäº†å•æ¨¡æ€æ–¹æ³•çš„å±€é™æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è§†è§‰å¯¹è¯ä¸­æŒ‡ç§°è¡¨è¾¾çš„æ£€æµ‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä»…ä¾èµ–è¯­è¨€ä¸Šä¸‹æ–‡æ—¶å­˜åœ¨è¯†åˆ«å‡†ç¡®æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è°ƒæ•´é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨å…¶è‡ªå›å½’ç‰¹æ€§è¿›è¡ŒæŒ‡ç§°è¡¨è¾¾çš„èŒƒå›´æ ‡è®°ï¼Œæ—¨åœ¨æ¢è®¨è¯­è¨€ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è°ƒæ•´ã€ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œç»“æœè¯„ä¼°ã€‚ä¸»è¦æ¨¡å—ä¸ºè¯­è¨€æ¨¡å‹çš„è¾“å…¥å¤„ç†å’Œè¾“å‡ºçš„æŒ‡ç§°èŒƒå›´æ ‡è®°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨æ–‡æœ¬è‡ªå›å½’æ¨¡å‹è¿›è¡ŒæŒ‡ç§°è¡¨è¾¾æ£€æµ‹ï¼Œå¼ºè°ƒäº†è¯­è¨€ä¸Šä¸‹æ–‡çš„ä½œç”¨ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„å¤šæ¨¡æ€æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç›¸å¯¹å°çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºé€‚åº”ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼Œç½‘ç»œç»“æ„åŸºäºç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ä¸­ç­‰è§„æ¨¡çš„è¯­è¨€æ¨¡å‹å’Œå°æ•°æ®é›†ï¼Œæ–‡æœ¬è‡ªå›å½’æ–¹æ³•åœ¨æŒ‡ç§°è¡¨è¾¾æ£€æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡æ˜¾è‘—é«˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒéªŒè¯äº†è¯­è¨€ä¸Šä¸‹æ–‡çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è§†è§‰å¯¹è¯ç³»ç»Ÿã€æ™ºèƒ½åŠ©æ‰‹å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜æŒ‡ç§°è¡¨è¾¾çš„æ£€æµ‹èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºç³»ç»Ÿçš„ç†è§£å’Œå“åº”èƒ½åŠ›ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šæ¨åŠ¨å¤šæ¨¡æ€äº¤äº’æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we explore the use of a text-only, autoregressive language modeling approach for the extraction of referring expressions from visually grounded dialogue. More specifically, the aim is to investigate the extent to which the linguistic context alone can inform the detection of mentions that have a (visually perceivable) referent in the visual context of the conversation. To this end, we adapt a pretrained large language model (LLM) to perform a relatively course-grained annotation of mention spans in unfolding conversations by demarcating mention span boundaries in text via next-token prediction. Our findings indicate that even when using a moderately sized LLM, relatively small datasets, and parameter-efficient fine-tuning, a text-only approach can be effective, highlighting the relative importance of the linguistic context for this task. Nevertheless, we argue that the task represents an inherently multimodal problem and discuss limitations fundamental to unimodal approaches.

