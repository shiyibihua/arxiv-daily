---
layout: default
title: FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language
---

# FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20920" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20920v1</a>
  <a href="https://arxiv.org/pdf/2506.20920.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20920v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20920v1', 'FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guilherme Penedo, Hynek KydlÃ­Äek, Vinko SabolÄec, Bettina Messmer, Negar Foroutan, Amir Hossein Kargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, Thomas Wolf

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFineWeb2ä»¥è§£å†³å¤šè¯­è¨€é¢„è®­ç»ƒæ•°æ®å¤„ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€å¤„ç†` `æ•°æ®é›†ç­–åˆ’` `é¢„è®­ç»ƒæ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šè¯­è¨€LLMsè®­ç»ƒé¢ä¸´æ•°æ®å¤„ç†ç®¡é“éš¾ä»¥é€‚åº”å¤šç§è¯­è¨€çš„æŒ‘æˆ˜ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„FineWebæ•°æ®é›†ç­–åˆ’ç®¡é“ï¼Œèƒ½å¤Ÿè‡ªåŠ¨é€‚åº”ä»»ä½•è¯­è¨€ï¼Œæå‡å¤šè¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥ç®¡é“ç”Ÿæˆçš„éè‹±è¯­è¯­æ–™åº“æ¨¡å‹æ€§èƒ½ä¼˜äºä»¥å¾€æ•°æ®é›†ï¼Œå¹¶ä¸”å®ç°äº†æ•°æ®é›†çš„é‡å¹³è¡¡ï¼Œè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦å¤§é‡å¹²å‡€ä¸”å¤šæ ·åŒ–çš„æ–‡æœ¬æ•°æ®ã€‚å°½ç®¡åœ¨é«˜è´¨é‡è‹±è¯­é¢„è®­ç»ƒæ•°æ®é›†çš„å¼€å‘ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è®­ç»ƒé«˜æ€§èƒ½çš„å¤šè¯­è¨€LLMsä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºè¿‡æ»¤å’Œå»é‡ç®¡é“éš¾ä»¥é€‚åº”å¤šç§è¯­è¨€ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºFineWebçš„æ–°é¢„è®­ç»ƒæ•°æ®é›†ç­–åˆ’ç®¡é“ï¼Œèƒ½å¤Ÿè‡ªåŠ¨é€‚åº”ä»»ä½•è¯­è¨€ã€‚æˆ‘ä»¬åœ¨ä¹ç§ä¸åŒè¯­è¨€ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèå®éªŒï¼Œå¹¶é€šè¿‡ä¸€å¥—åŸºäºå¯æµ‹é‡æ ‡å‡†çš„æ–°é¢–é€‰æ‹©è¿‡ç¨‹é€‰å‡ºçš„è¯„ä¼°ä»»åŠ¡è¿›è¡ŒæŒ‡å¯¼ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥ç®¡é“èƒ½å¤Ÿåˆ›å»ºéè‹±è¯­è¯­æ–™åº“ï¼Œç”Ÿæˆæ¯”ä»¥å¾€æ•°æ®é›†æ›´é«˜æ€§èƒ½çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®€å•ä¸”æœ‰åŸåˆ™çš„æ•°æ®é›†é‡å¹³è¡¡æ–¹æ³•ï¼Œè€ƒè™‘äº†é‡å¤è®¡æ•°å’Œè´¨é‡ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ç®¡é“æ‰©å±•è‡³1000å¤šç§è¯­è¨€ï¼Œåˆ©ç”¨è¿‘100ä¸ªCommon Crawlå¿«ç…§ç”Ÿæˆäº†FineWeb2ï¼Œä¸€ä¸ªæ–°çš„20TBï¼ˆ50äº¿æ–‡æ¡£ï¼‰å¤šè¯­è¨€æ•°æ®é›†ï¼Œå¹¶å‘å¸ƒäº†ç›¸å…³ä»£ç åº“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€é¢„è®­ç»ƒæ•°æ®å¤„ç†ä¸­çš„è¿‡æ»¤å’Œå»é‡ç®¡é“éš¾ä»¥é€‚åº”å¤šç§è¯­è¨€çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šè¯­è¨€æ•°æ®æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆå»é™¤é‡å¤å’Œä½è´¨é‡æ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„FineWebæ•°æ®é›†ç­–åˆ’ç®¡é“èƒ½å¤Ÿè‡ªåŠ¨é€‚åº”ä»»ä½•è¯­è¨€ï¼Œé€šè¿‡è®¾è®¡çµæ´»çš„è¿‡æ»¤å’Œå»é‡æœºåˆ¶ï¼Œæå‡å¤šè¯­è¨€LLMsçš„è®­ç»ƒæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç®¡é“çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€è¿‡æ»¤ã€å»é‡å’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ”¶é›†é˜¶æ®µåˆ©ç”¨Common Crawlå¿«ç…§è·å–å¤šè¯­è¨€æ–‡æœ¬ï¼Œè¿‡æ»¤å’Œå»é‡æ¨¡å—åˆ™æ ¹æ®è®¾å®šçš„æ ‡å‡†è¿›è¡Œå¤„ç†ï¼Œæœ€åé€šè¿‡è¯„ä¼°æ¨¡å—éªŒè¯æ•°æ®é›†çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†é‡å¹³è¡¡æ–¹æ³•ï¼Œè€ƒè™‘äº†é‡å¤è®¡æ•°å’Œè´¨é‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„å•ä¸€è¿‡æ»¤æœºåˆ¶æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šå±‚æ¬¡çš„è¿‡æ»¤æ ‡å‡†ï¼Œå¹¶å¼•å…¥äº†å¯æµ‹é‡çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ•°æ®é›†åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¸Šè¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨FineWeb2ç”Ÿæˆçš„å¤šè¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºä»¥å¾€çš„æ•°æ®é›†ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°15%-30%ã€‚æ­¤å¤–ï¼Œé‡å¹³è¡¡æ–¹æ³•çš„å¼•å…¥è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥ç®¡é“çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œå¤šè¯­è¨€å¯¹è¯ç³»ç»Ÿã€‚FineWeb2æ•°æ®é›†çš„å‘å¸ƒå°†ä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æä¾›ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ï¼Œæ¨åŠ¨å¤šè¯­è¨€æ¨¡å‹çš„ç ”ç©¶ä¸åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains a challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to a large number of languages. In this work, we introduce a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. We extensively ablate our pipeline design choices on a set of nine diverse languages, guided by a set of meaningful and informative evaluation tasks that were chosen through a novel selection process based on measurable criteria. Ultimately, we show that our pipeline can be used to create non-English corpora that produce more performant models than prior datasets. We additionally introduce a straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, we scale our pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document) multilingual dataset which we release along with our pipeline, training, and evaluation codebases.

