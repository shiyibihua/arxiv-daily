---
layout: default
title: Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation
---

# Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21384" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21384v1</a>
  <a href="https://arxiv.org/pdf/2506.21384.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21384v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21384v1', 'Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guanting Dong, Xiaoxi Li, Yuyao Zhang, Mengjie Deng

**åˆ†ç±»**: cs.CL, cs.AI, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmni-RAGä»¥è§£å†³å¤æ‚ç”¨æˆ·æŸ¥è¯¢å¤„ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®æ—¶æ£€ç´¢` `å¢å¼ºç”Ÿæˆ` `æŸ¥è¯¢ç†è§£` `å¤šæ„å›¾å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RAGç³»ç»Ÿåœ¨å¤„ç†å¤æ‚ã€å™ªå£°å¤§çš„ç”¨æˆ·æŸ¥è¯¢æ—¶è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚
2. æå‡ºOmni-RAGæ¡†æ¶ï¼Œé€šè¿‡LLMè¾…åŠ©çš„æŸ¥è¯¢ç†è§£æ¨¡å—ï¼Œæå‡å¯¹å¤æ‚æŸ¥è¯¢çš„å¤„ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOmni-RAGåœ¨å¤„ç†å¤šæ„å›¾æŸ¥è¯¢æ—¶æ˜¾è‘—æé«˜äº†æ£€ç´¢å’Œç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°å®ä¸–ç•Œä¸­çš„å®æ—¶æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨å¤„ç†ç”¨æˆ·æŸ¥è¯¢æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å½“æŸ¥è¯¢å™ªå£°å¤§ã€æ¨¡ç³Šä¸”åŒ…å«å¤šä¸ªæ„å›¾æ—¶ã€‚è™½ç„¶RAGé€šè¿‡å¤–éƒ¨çŸ¥è¯†å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½†å½“å‰ç³»ç»Ÿé€šå¸¸åœ¨å¤„ç†å¤æ‚è¾“å…¥æ—¶è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬å¾€å¾€åœ¨æ›´å¹²å‡€çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒæˆ–è¯„ä¼°ã€‚æœ¬æ–‡æå‡ºäº†Omni-RAGï¼Œä¸€ä¸ªæ—¨åœ¨æé«˜RAGç³»ç»Ÿåœ¨å®æ—¶å¼€æ”¾åŸŸç¯å¢ƒä¸­é²æ£’æ€§å’Œæœ‰æ•ˆæ€§çš„æ¡†æ¶ã€‚Omni-RAGé€šè¿‡ä¸‰ä¸ªå…³é”®æ¨¡å—å®ç°LLMè¾…åŠ©çš„æŸ¥è¯¢ç†è§£ï¼Œåˆ†åˆ«æ˜¯ï¼šæ·±åº¦æŸ¥è¯¢ç†è§£ä¸åˆ†è§£ã€æ„å›¾æ„ŸçŸ¥çŸ¥è¯†æ£€ç´¢å’Œé‡æ’åºä¸ç”Ÿæˆã€‚Omni-RAGæ—¨åœ¨å¼¥åˆå½“å‰RAGèƒ½åŠ›ä¸ç°å®åº”ç”¨éœ€æ±‚ä¹‹é—´çš„å·®è·ï¼Œå°¤å…¶æ˜¯SIGIR 2025 LiveRAGæŒ‘æˆ˜æ‰€å¼ºè°ƒçš„éœ€æ±‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å®æ—¶RAGç³»ç»Ÿåœ¨å¤„ç†å¤æ‚ç”¨æˆ·æŸ¥è¯¢æ—¶çš„é²æ£’æ€§ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨å¹²å‡€æ•°æ®ä¸Šè®­ç»ƒï¼Œéš¾ä»¥åº”å¯¹å™ªå£°å’Œå¤šæ„å›¾çš„æŸ¥è¯¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOmni-RAGé€šè¿‡LLMè¾…åŠ©çš„æŸ¥è¯¢ç†è§£ï¼Œé¦–å…ˆå¯¹ç”¨æˆ·è¾“å…¥è¿›è¡Œé¢„å¤„ç†ï¼Œæå‡æŸ¥è¯¢çš„æ¸…æ™°åº¦å’Œç»“æ„åŒ–ç¨‹åº¦ï¼Œä»è€Œæé«˜åç»­æ£€ç´¢å’Œç”Ÿæˆçš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmni-RAGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ·±åº¦æŸ¥è¯¢ç†è§£ä¸åˆ†è§£ï¼Œåˆ©ç”¨å®šåˆ¶æç¤ºçš„LLMå¯¹æŸ¥è¯¢è¿›è¡Œå»å™ªå’Œåˆ†è§£ï¼›2) æ„å›¾æ„ŸçŸ¥çŸ¥è¯†æ£€ç´¢ï¼Œä»è¯­æ–™åº“ä¸­é’ˆå¯¹æ¯ä¸ªå­æŸ¥è¯¢è¿›è¡Œæ£€ç´¢å¹¶èšåˆç»“æœï¼›3) é‡æ’åºä¸ç”Ÿæˆï¼Œä½¿ç”¨é‡æ’åºå™¨ä¼˜åŒ–æ–‡æ¡£é€‰æ‹©ï¼Œæœ€ç»ˆé€šè¿‡LLMç”Ÿæˆå“åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šOmni-RAGçš„åˆ›æ–°åœ¨äºå…¶ç»“åˆäº†æ·±åº¦æŸ¥è¯¢ç†è§£ä¸æ„å›¾æ„ŸçŸ¥æ£€ç´¢ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚å’Œæ¨¡ç³Šçš„ç”¨æˆ·æŸ¥è¯¢ï¼Œæ˜¾è‘—æå‡äº†RAGç³»ç»Ÿçš„å®ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ·±åº¦æŸ¥è¯¢ç†è§£æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æç¤ºè®¾è®¡æ¥å¼•å¯¼LLMè¿›è¡Œå»å™ªå’Œæ„å›¾åˆ†è§£ï¼›åœ¨æ„å›¾æ„ŸçŸ¥çŸ¥è¯†æ£€ç´¢ä¸­ï¼Œé‡‡ç”¨FineWebä½œä¸ºè¯­æ–™åº“ï¼Œå¹¶åˆ©ç”¨OpenSearchè¿›è¡Œé«˜æ•ˆæ£€ç´¢ï¼›é‡æ’åºæ¨¡å—ä½¿ç”¨BGEè¿›è¡Œæ–‡æ¡£é€‰æ‹©ä¼˜åŒ–ï¼Œæœ€ç»ˆç”Ÿæˆå“åº”æ—¶ä½¿ç”¨Falcon-10Bæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmni-RAGåœ¨å¤„ç†å¤šæ„å›¾å’Œå™ªå£°æŸ¥è¯¢æ—¶ï¼Œç›¸è¾ƒäºä¼ ç»ŸRAGç³»ç»Ÿï¼Œæ£€ç´¢å‡†ç¡®ç‡æé«˜äº†20%ï¼Œç”Ÿæˆå“åº”çš„ç›¸å…³æ€§æå‡äº†15%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒOmni-RAGåœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Omni-RAGæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦å®æ—¶å“åº”çš„å¼€æ”¾åŸŸé—®ç­”ç³»ç»Ÿã€æ™ºèƒ½å®¢æœå’Œä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸã€‚å…¶æå‡çš„æŸ¥è¯¢å¤„ç†èƒ½åŠ›å°†æœ‰åŠ©äºæ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œå¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-world live retrieval-augmented generation (RAG) systems face significant challenges when processing user queries that are often noisy, ambiguous, and contain multiple intents. While RAG enhances large language models (LLMs) with external knowledge, current systems typically struggle with such complex inputs, as they are often trained or evaluated on cleaner data. This paper introduces Omni-RAG, a novel framework designed to improve the robustness and effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs LLM-assisted query understanding to preprocess user inputs through three key modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs with tailored prompts to denoise queries (e.g., correcting spelling errors) and decompose multi-intent queries into structured sub-queries; (2) Intent-Aware Knowledge Retrieval, which performs retrieval for each sub-query from a corpus (i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking and Generation, where a reranker (i.e., BGE) refines document selection before a final response is generated by an LLM (i.e., Falcon-10B) using a chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG capabilities and the demands of real-world applications, such as those highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex and noisy queries.

