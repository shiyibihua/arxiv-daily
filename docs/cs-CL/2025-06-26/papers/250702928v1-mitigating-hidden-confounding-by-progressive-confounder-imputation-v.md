---
layout: default
title: Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models
---

# Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.02928" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.02928v1</a>
  <a href="https://arxiv.org/pdf/2507.02928.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.02928v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.02928v1', 'Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hao Yang, Haoxuan Li, Luyu Chen, Haoxiang Wang, Xu Chen, Mingming Gong

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProCIæ¡†æ¶ä»¥è§£å†³éšæ€§æ··æ·†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éšæ€§æ··æ·†` `å› æœæ¨æ–­` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ··æ·†å˜é‡æ’è¡¥` `åˆ†å¸ƒæ¨ç†` `è¯­ä¹‰æ¨ç†` `åäº‹å®æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. éšæ€§æ··æ·†æ˜¯å› æœæ¨æ–­ä¸­çš„æ ¸å¿ƒé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–æ— æ··æ·†å‡è®¾ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚
2. æœ¬æ–‡æå‡ºProCIæ¡†æ¶ï¼Œé€šè¿‡LLMsçš„è¯­ä¹‰æ¨ç†å’Œä¸–ç•ŒçŸ¥è¯†ï¼Œé€æ­¥ç”Ÿæˆå’ŒéªŒè¯éšæ€§æ··æ·†å˜é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProCIåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ²»ç–—æ•ˆæœä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œå‘ç°äº†æœ‰æ„ä¹‰çš„æ··æ·†å˜é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšæ€§æ··æ·†æ˜¯ä»è§‚å¯Ÿæ•°æ®ä¸­ä¼°è®¡æ²»ç–—æ•ˆæœçš„ä¸»è¦æŒ‘æˆ˜ï¼Œå› ä¸ºæœªè§‚å¯Ÿåˆ°çš„å˜é‡å¯èƒ½å¯¼è‡´å› æœä¼°è®¡åå·®ã€‚å°½ç®¡è¿‘æœŸç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å› æœæ¨æ–­ä¸­çš„åº”ç”¨ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä»ä¾èµ–äºæ— æ··æ·†å‡è®¾ã€‚æœ¬æ–‡é¦–æ¬¡å°è¯•åˆ©ç”¨LLMsç¼“è§£éšæ€§æ··æ·†ï¼Œæå‡ºäº†ProCIï¼ˆæ¸è¿›æ··æ·†å˜é‡æ’è¡¥ï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨LLMsçš„è¯­ä¹‰æ¨ç†èƒ½åŠ›å’Œä¸–ç•ŒçŸ¥è¯†ï¼Œè¿­ä»£ç”Ÿæˆã€æ’è¡¥å’ŒéªŒè¯éšæ€§æ··æ·†å˜é‡ã€‚ProCIé‡‡ç”¨åˆ†å¸ƒæ¨ç†ç­–ç•¥ä»¥æé«˜é²æ£’æ€§ï¼Œé¿å…ç›´æ¥å€¼æ’è¡¥å¯¼è‡´çš„è¾“å‡ºå´©æºƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒProCIèƒ½å¤Ÿå‘ç°æœ‰æ„ä¹‰çš„æ··æ·†å˜é‡ï¼Œå¹¶æ˜¾è‘—æ”¹å–„å„ç§æ•°æ®é›†å’ŒLLMsä¸Šçš„æ²»ç–—æ•ˆæœä¼°è®¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šéšæ€§æ··æ·†å¯¼è‡´å› æœä¼°è®¡åå·®ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–æ— æ··æ·†å‡è®¾ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†æœªè§‚å¯Ÿåˆ°çš„æ··æ·†å˜é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šProCIæ¡†æ¶åˆ©ç”¨LLMsçš„å¼ºå¤§è¯­ä¹‰æ¨ç†èƒ½åŠ›å’Œå†…åµŒçš„ä¸–ç•ŒçŸ¥è¯†ï¼Œé€šè¿‡è¿­ä»£çš„æ–¹å¼ç”Ÿæˆå’ŒéªŒè¯éšæ€§æ··æ·†å˜é‡ï¼Œä»è€Œç¼“è§£æ··æ·†é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šProCIçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç”Ÿæˆæ¨¡å—å’ŒéªŒè¯æ¨¡å—ã€‚ç”Ÿæˆæ¨¡å—ä»ç»“æ„åŒ–å’Œéç»“æ„åŒ–è¾“å…¥ä¸­å‘ç°æ½œåœ¨çš„æ··æ·†å˜é‡ï¼ŒéªŒè¯æ¨¡å—åˆ™é€šè¿‡åäº‹å®æ¨ç†æ¥éªŒè¯è¿™äº›å˜é‡çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šProCIçš„åˆ›æ–°åœ¨äºé‡‡ç”¨åˆ†å¸ƒæ¨ç†ç­–ç•¥ï¼Œè€Œéç›´æ¥å€¼æ’è¡¥ï¼Œé¿å…äº†è¾“å‡ºå´©æºƒçš„é—®é¢˜ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„éšæ€§æ··æ·†æ—¶æ›´åŠ é²æ£’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒProCIä½¿ç”¨äº†å¤šç§æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆç”Ÿæˆå’ŒéªŒè¯çš„åŒé‡ç›®æ ‡ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ··æ·†å˜é‡å…·æœ‰å®é™…æ„ä¹‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒProCIåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ²»ç–—æ•ˆæœä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œä¸”èƒ½å¤Ÿæœ‰æ•ˆå‘ç°æœ‰æ„ä¹‰çš„æ··æ·†å˜é‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å› æœæ¨æ–­ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€ç¤¾ä¼šç§‘å­¦å’Œç»æµå­¦ç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä»è§‚å¯Ÿæ•°æ®ä¸­è¿›è¡Œå› æœæ¨æ–­çš„åœºæ™¯ã€‚ProCIæ¡†æ¶èƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å‡†ç¡®åœ°ä¼°è®¡æ²»ç–—æ•ˆæœï¼Œä»è€Œä¸ºæ”¿ç­–åˆ¶å®šå’Œä¸´åºŠå†³ç­–æä¾›æ›´å¯é çš„ä¾æ®ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šå½±å“å› æœæ¨æ–­çš„ç ”ç©¶æ–¹å‘ï¼Œæ¨åŠ¨æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hidden confounding remains a central challenge in estimating treatment effects from observational data, as unobserved variables can lead to biased causal estimates. While recent work has explored the use of large language models (LLMs) for causal inference, most approaches still rely on the unconfoundedness assumption. In this paper, we make the first attempt to mitigate hidden confounding using LLMs. We propose ProCI (Progressive Confounder Imputation), a framework that elicits the semantic and world knowledge of LLMs to iteratively generate, impute, and validate hidden confounders. ProCI leverages two key capabilities of LLMs: their strong semantic reasoning ability, which enables the discovery of plausible confounders from both structured and unstructured inputs, and their embedded world knowledge, which supports counterfactual reasoning under latent confounding. To improve robustness, ProCI adopts a distributional reasoning strategy instead of direct value imputation to prevent the collapsed outputs. Extensive experiments demonstrate that ProCI uncovers meaningful confounders and significantly improves treatment effect estimation across various datasets and LLMs.

