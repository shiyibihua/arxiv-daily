---
layout: default
title: KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model
---

# KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20923" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20923v5</a>
  <a href="https://arxiv.org/pdf/2506.20923.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20923v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20923v5', 'KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinping Zhao, Xinshuo Hu, Zifei Shan, Shouzheng Huang, Yao Zhou, Xin Zhang, Zetian Sun, Zhenyu Liu, Dongfang Li, Xinyuan Wei, Youcheng Pan, Yang Xiang, Meishan Zhang, Haofen Wang, Jun Yu, Baotian Hu, Min Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-10-14)

**å¤‡æ³¨**: 32 pages, 16 tables, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKaLM-Embedding-V2ä»¥æå‡æ–‡æœ¬åµŒå…¥æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬åµŒå…¥` `è®­ç»ƒæŠ€æœ¯` `æ•°æ®è´¨é‡` `å¯¹æ¯”è’¸é¦` `å¤šé˜¶æ®µè®­ç»ƒ` `é«˜è´¨é‡æ•°æ®` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨è®­ç»ƒæŠ€æœ¯å’Œæ•°æ®è´¨é‡æ–¹é¢æ¢ç´¢ä¸è¶³ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½æå‡ã€‚
2. æå‡ºKaLM-Embedding-V2ï¼Œé€šè¿‡ä¼˜è¶Šçš„è®­ç»ƒæŠ€æœ¯å’Œé«˜è´¨é‡æ•°æ®ï¼Œç³»ç»Ÿæ€§æå‡LLMsçš„åµŒå…¥èƒ½åŠ›ã€‚
3. KaLM-Embedding-V2åœ¨å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶ŠåŒç±»æ¨¡å‹ï¼Œè®¾å®šäº†æ–°çš„æ€§èƒ½æ ‡å‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨æ•°æ®æ‰©å±•æˆ–åˆæˆä¸Šï¼Œç„¶è€Œå¯¹è®­ç»ƒæŠ€æœ¯å’Œæ•°æ®è´¨é‡çš„æ¢ç´¢æœ‰é™ï¼Œåˆ¶çº¦äº†æ€§èƒ½ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†KaLM-Embedding-V2ç³»åˆ—å¤šåŠŸèƒ½ç´§å‡‘å‹åµŒå…¥æ¨¡å‹ï¼Œé€šè¿‡ä¼˜è¶Šçš„è®­ç»ƒæŠ€æœ¯å’Œé«˜è´¨é‡æ•°æ®ç³»ç»Ÿæ€§åœ°æ¿€åŠ±LLMsçš„åµŒå…¥èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨0.5Bç´§å‡‘å‹æ¨¡å‹ä¸Šå®ç°äº†ç®€å•çš„å‡å€¼æ± åŒ–ä»¥ç”Ÿæˆå›ºå®šé•¿åº¦çš„åµŒå…¥ï¼Œå¹¶ç§»é™¤äº†å› æœæ³¨æ„åŠ›æ©ç ä»¥å®ç°å®Œå…¨åŒå‘è¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡é€æ­¥å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆä»»åŠ¡ç‰¹å®šæŒ‡ä»¤å’Œå›°éš¾æ ·æœ¬æŒ–æ˜ï¼Œæˆ‘ä»¬çš„KaLM-Embedding-V2ç³»åˆ—åœ¨å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†åŒç±»æ¨¡å‹ï¼Œå¹¶ä¸3-26å€æ›´å¤§çš„æ¨¡å‹ç›¸åª²ç¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸­è®­ç»ƒæŠ€æœ¯å’Œæ•°æ®è´¨é‡ä¸è¶³çš„é—®é¢˜ï¼Œè¿™å¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºKaLM-Embedding-V2ï¼Œé€šè¿‡å¼•å…¥å…ˆè¿›çš„è®­ç»ƒæŠ€æœ¯å’Œé«˜è´¨é‡çš„æ•°æ®ï¼Œç³»ç»Ÿæ€§åœ°æå‡åµŒå…¥æ¨¡å‹çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å°å‹æ¨¡å‹ä¸­å®ç°é«˜æ•ˆçš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆåœ¨å¼±ç›‘ç£çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå…¶æ¬¡åœ¨é«˜è´¨é‡çš„ç›‘ç£æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæœ€åé€šè¿‡å¯¹æ¯”è’¸é¦å’Œç»†ç²’åº¦è½¯ä¿¡å·è¿›è¡Œè®­ç»ƒï¼Œç»“åˆç„¦ç‚¹å¼é‡åŠ æƒå’Œåœ¨çº¿å›°éš¾è´Ÿæ ·æœ¬æ··åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€æ­¥å¤šé˜¶æ®µçš„è®­ç»ƒæµç¨‹å’Œé«˜è´¨é‡æ•°æ®çš„ç²¾å¿ƒç­–åˆ’ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€è®­ç»ƒé˜¶æ®µå’Œæ•°æ®è´¨é‡ä¸è¶³å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨0.5Bçš„ç´§å‡‘ç»“æ„ï¼Œä½¿ç”¨ç®€å•çš„å‡å€¼æ± åŒ–ç”Ÿæˆå›ºå®šé•¿åº¦åµŒå…¥ï¼Œç§»é™¤å› æœæ³¨æ„åŠ›æ©ç ä»¥å®ç°åŒå‘å­¦ä¹ ï¼ŒåŒæ—¶åœ¨æ•°æ®ç­–åˆ’ä¸­æ¶µç›–è¶…è¿‡20ä¸ªç±»åˆ«ç”¨äºé¢„è®­ç»ƒå’Œ100ä¸ªç±»åˆ«ç”¨äºå¾®è°ƒä¸å¯¹æ¯”è’¸é¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒKaLM-Embedding-V2ç³»åˆ—åœ¨å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†åŒç±»æ¨¡å‹ï¼Œå¹¶ä¸3-26å€æ›´å¤§çš„æ¨¡å‹ç›¸åª²ç¾ï¼Œè®¾å®šäº†æ–°çš„æ€§èƒ½æ ‡å‡†ï¼Œå±•ç¤ºäº†å…¶åœ¨ç´§å‡‘å‹åµŒå…¥æ¨¡å‹ä¸­çš„å“è¶Šèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¿¡æ¯æ£€ç´¢å’Œæ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„æ€§èƒ½ï¼ŒKaLM-Embedding-V2èƒ½å¤Ÿä¸ºå¤šç§ä¸‹æ¸¸ä»»åŠ¡æä¾›æ›´é«˜è´¨é‡çš„ç‰¹å¾è¡¨ç¤ºï¼Œè¿›è€Œæé«˜ç³»ç»Ÿçš„æ•´ä½“æ•ˆæœå’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in Large Language Models (LLMs)-based text embedding models primarily focus on data scaling or synthesis, yet limited exploration of training techniques and data quality, thereby constraining performance. In this work, we propose KaLM-Embedding-V2, a series of versatile and compact embedding models, systematically incentivizing advanced embedding capability in LLMs by superior training techniques and high-quality data. For model architecture, we implement the models on a 0.5B compact size with simple mean-pooling to produce fixed-length embeddings and remove the causal attention mask to enable fully bidirectional representation learning. For training techniques, we propose a progressive multi-stage training pipeline: pre-training on weakly supervised large-scale datasets, fine-tuning with supervised high-quality datasets, and contrastive distillation with fine-grained soft signals, integrated with focal-style reweighting and online hard-negative mixing to emphasize difficult samples and enrich hard negatives, respectively. For training data, we curate over 20 categories for pre-training and 100 categories for fine-tuning and contrastive distillation, to improve both performance and generalization, leveraging task-specific instructions, hard-negative mining, and example-based multi-class labeling to ensure high quality. Combining these techniques, our KaLM-Embedding-V2 series achieves state-of-the-art performance on the Massive Text Embedding Benchmark, outperforming models of comparable size and rivaling models 3-26x larger, setting a new standard for versatile and compact embedding models under 1B parameters.

