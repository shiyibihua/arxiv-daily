---
layout: default
title: Learning from Diverse Reasoning Paths with Routing and Collaboration
---

# Learning from Diverse Reasoning Paths with Routing and Collaboration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16861" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.16861v1</a>
  <a href="https://arxiv.org/pdf/2508.16861.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16861v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16861v1', 'Learning from Diverse Reasoning Paths with Routing and Collaboration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenyu Lei, Zhen Tan, Song Wang, Yaochen Zhu, Zihan Chen, Yushun Dong, Jundong Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-23

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/LzyFischer/Distill)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQR-Distillä»¥è§£å†³çŸ¥è¯†è’¸é¦ä¸­çš„è·¯å¾„è´¨é‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `è·¯å¾„è´¨é‡` `æ¡ä»¶è·¯ç”±` `åˆä½œå­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨æ•æ‰æ•™å¸ˆæ¨¡å‹çš„å…¨é¢æ¨ç†æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä¼ ç»Ÿçš„tokençº§ç›‘ç£æ–¹å¼é™åˆ¶äº†æœ‰æ•ˆæ€§ã€‚
2. è®ºæ–‡æå‡ºçš„QR-Distillé€šè¿‡è´¨é‡è¿‡æ»¤ã€æ¡ä»¶è·¯ç”±å’Œåˆä½œåŒè¡Œæ•™å­¦æ¥ä¼˜åŒ–çŸ¥è¯†è½¬ç§»è¿‡ç¨‹ï¼Œæå‡å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQR-Distillåœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„è’¸é¦æ–¹æ³•ï¼Œä¸”æ¶ˆèç ”ç©¶éªŒè¯äº†å„ä¸ªç»„ä»¶çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸­éƒ¨ç½²å—åˆ°é™åˆ¶ã€‚çŸ¥è¯†è’¸é¦é€šè¿‡å°†å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°ç´§å‡‘é€æ˜çš„å­¦ç”Ÿæ¨¡å‹ä¸­æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºä¼ ç»Ÿçš„tokençº§ç›‘ç£çš„å±€é™æ€§ï¼Œæœ‰æ•ˆæ•æ‰æ•™å¸ˆçš„å…¨é¢æ¨ç†æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä½¿ç”¨å¤šä¸ªæ¨ç†è·¯å¾„å¯ä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å°†æ¯ä¸ªè·¯å¾„è§†ä¸ºç›¸åŒçš„åšæ³•å¹¶ä¸ç†æƒ³ï¼Œå› ä¸ºè·¯å¾„åœ¨ä»»åŠ¡å’Œæ¨¡å‹é—´çš„è´¨é‡å’Œé€‚ç”¨æ€§å·®å¼‚å¾ˆå¤§ã€‚æˆ‘ä»¬æå‡ºäº†è´¨é‡è¿‡æ»¤è·¯ç”±ä¸åˆä½œè’¸é¦ï¼ˆQR-Distillï¼‰ï¼Œç»“åˆè·¯å¾„è´¨é‡è¿‡æ»¤ã€æ¡ä»¶è·¯ç”±å’Œåˆä½œåŒè¡Œæ•™å­¦ã€‚å®éªŒè¡¨æ˜ï¼ŒQR-Distillåœ¨ä¼ ç»Ÿçš„å•è·¯å¾„å’Œå¤šè·¯å¾„è’¸é¦æ–¹æ³•ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–å’Œè½¬ç§»çŸ¥è¯†åˆ°èµ„æºå—é™çš„å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚ç°æœ‰æ–¹æ³•åœ¨è·¯å¾„è´¨é‡å’Œé€‚ç”¨æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´çŸ¥è¯†è½¬ç§»æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šQR-Distillçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è´¨é‡è¿‡æ»¤å’Œæ¡ä»¶è·¯ç”±æ¥ä¼˜åŒ–æ¨ç†è·¯å¾„çš„é€‰æ‹©ï¼ŒåŒæ—¶åˆ©ç”¨åˆä½œåŒè¡Œæ•™å­¦æ¥å¼¥è¡¥å­¦ç”Ÿæ¨¡å‹é—´çš„çŸ¥è¯†å·®è·ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜çŸ¥è¯†è’¸é¦çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQR-Distillçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè´¨é‡è¿‡æ»¤æ¨¡å—ç”¨äºç­›é€‰å‡ºæ­£ç¡®çš„æ¨ç†è·¯å¾„ï¼Œæ¡ä»¶è·¯ç”±æ¨¡å—æ ¹æ®å­¦ç”Ÿçš„å­¦ä¹ çŠ¶æ€åŠ¨æ€åˆ†é…è·¯å¾„ï¼Œåˆä½œåŒè¡Œæ•™å­¦æ¨¡å—åˆ™ä¿ƒè¿›å­¦ç”Ÿé—´çš„çŸ¥è¯†å…±äº«ä¸äº’åŠ©ã€‚

**å…³é”®åˆ›æ–°**ï¼šQR-Distillçš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆäº†è·¯å¾„è´¨é‡è¿‡æ»¤å’Œæ¡ä»¶è·¯ç”±çš„åŠ¨æ€è°ƒæ•´æœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€è·¯å¾„å¤„ç†æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒä»»åŠ¡å’Œæ¨¡å‹çš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè´¨é‡è¿‡æ»¤ä¾èµ–äºåŸºäºLLMçš„è¯„ä¼°æœºåˆ¶ï¼Œæ¡ä»¶è·¯ç”±åˆ™è€ƒè™‘äº†å­¦ç”Ÿæ¨¡å‹çš„å½“å‰å­¦ä¹ çŠ¶æ€ï¼Œåˆä½œåŒè¡Œæ•™å­¦åˆ™é€šè¿‡äº’ç›¸è’¸é¦å¤šæ ·åŒ–çš„è§è§£æ¥è§£å†³çŸ¥è¯†åå·®é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒQR-Distillåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºä¼ ç»Ÿçš„å•è·¯å¾„å’Œå¤šè·¯å¾„è’¸é¦æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œä¸”æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†è´¨é‡è¿‡æ»¤ã€æ¡ä»¶è·¯ç”±å’ŒåŒè¡Œæ•™å­¦çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–çŸ¥è¯†è’¸é¦è¿‡ç¨‹ï¼ŒQR-Distillèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æœ‰æ•ˆæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Advances in large language models (LLMs) significantly enhance reasoning capabilities but their deployment is restricted in resource-constrained scenarios. Knowledge distillation addresses this by transferring knowledge from powerful teacher models to compact and transparent students. However, effectively capturing the teacher's comprehensive reasoning is challenging due to conventional token-level supervision's limited scope. Using multiple reasoning paths per query alleviates this problem, but treating each path identically is suboptimal as paths vary widely in quality and suitability across tasks and models. We propose Quality-filtered Routing with Cooperative Distillation (QR-Distill), combining path quality filtering, conditional routing, and cooperative peer teaching. First, quality filtering retains only correct reasoning paths scored by an LLM-based evaluation. Second, conditional routing dynamically assigns paths tailored to each student's current learning state. Finally, cooperative peer teaching enables students to mutually distill diverse insights, addressing knowledge gaps and biases toward specific reasoning styles. Experiments demonstrate QR-Distill's superiority over traditional single- and multi-path distillation methods. Ablation studies further highlight the importance of each component including quality filtering, conditional routing, and peer teaching in effective knowledge transfer. Our code is available at https://github.com/LzyFischer/Distill.

