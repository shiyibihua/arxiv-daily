---
layout: default
title: Token Homogenization under Positional Bias
---

# Token Homogenization under Positional Bias

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17126" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17126v1</a>
  <a href="https://arxiv.org/pdf/2508.17126.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17126v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17126v1', 'Token Homogenization under Positional Bias')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Viacheslav Yusupov, Danil Maksimov, Ameliia Alaeva, Tatiana Zaitceva, Antipina Anna, Anna Vasileva, Chenlin Liu, Rayuth Chheng, Danil Sazanakov, Andrey Chetvergov, Alina Ermilova, Egor Shvetsov

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶æ ‡è®°å‡è´¨åŒ–ä¸ä½ç½®åå·®çš„å…³ç³»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ ‡è®°å‡è´¨åŒ–` `ä½ç½®åå·®` `å˜æ¢å™¨æ¨¡å‹` `è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿‡ç¨‹ä¸­ï¼Œæ ‡è®°è¡¨ç¤ºè¶‹å‘å‡è´¨åŒ–ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±å’Œç‹¬ç‰¹æ€§é™ä½ã€‚
2. æœ¬æ–‡é€šè¿‡å®è¯åˆ†æå’Œæ§åˆ¶å®éªŒï¼Œæ¢è®¨äº†ä½ç½®åå·®å¦‚ä½•å½±å“æ ‡è®°å‡è´¨åŒ–ç°è±¡ï¼Œæå‡ºäº†æ–°çš„åˆ†ææ¡†æ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ ‡è®°åœ¨å¤„ç†è¿‡ç¨‹ä¸­ç¡®å®ä¼šå¤±å»ç‹¬ç‰¹æ€§ï¼Œå°¤å…¶æ˜¯åœ¨åå‘æç«¯ä½ç½®æ—¶ï¼ŒéªŒè¯äº†å‡è´¨åŒ–çš„å­˜åœ¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æ ‡è®°å‡è´¨åŒ–ç°è±¡ï¼Œå³åœ¨å˜æ¢å™¨å±‚ä¸­æ ‡è®°è¡¨ç¤ºå‘å‡åŒ€æ€§æ”¶æ•›çš„è¿‡ç¨‹ï¼Œä»¥åŠå…¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä½ç½®åå·®ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†å‡è´¨åŒ–æ˜¯å¦å‘ç”Ÿä»¥åŠä½ç½®åå·®å¦‚ä½•æ”¾å¤§è¿™ä¸€æ•ˆåº”ã€‚é€šè¿‡é€å±‚ç›¸ä¼¼æ€§åˆ†æå’Œæ§åˆ¶å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ ‡è®°åœ¨å¤„ç†è¿‡ç¨‹ä¸­ç³»ç»Ÿæ€§åœ°å¤±å»ç‹¬ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åå‘æç«¯ä½ç½®æ—¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœç¡®è®¤äº†å‡è´¨åŒ–çš„å­˜åœ¨åŠå…¶å¯¹ä½ç½®æ³¨æ„æœºåˆ¶çš„ä¾èµ–æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ ‡è®°å‡è´¨åŒ–çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ­ç¤ºä½ç½®åå·®å¯¹æ ‡è®°ç‹¬ç‰¹æ€§çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡é€å±‚ç›¸ä¼¼æ€§åˆ†æï¼Œè®ºæ–‡æ¢è®¨äº†ä½ç½®åå·®å¦‚ä½•åŠ å‰§æ ‡è®°å‡è´¨åŒ–ç°è±¡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å®éªŒè®¾è®¡æ¥éªŒè¯è¿™ä¸€å‡è®¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†åˆ†å±‚åˆ†æçš„æ–¹æ³•ï¼Œé¦–å…ˆè¿›è¡Œæ ‡è®°è¡¨ç¤ºçš„ç›¸ä¼¼æ€§è®¡ç®—ï¼Œç„¶åé€šè¿‡æ§åˆ¶å®éªŒè§‚å¯Ÿä½ç½®åå·®å¯¹å‡è´¨åŒ–çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†ä½ç½®åå·®ä¸æ ‡è®°å‡è´¨åŒ–ä¹‹é—´çš„å…³ç³»ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­é‡‡ç”¨äº†å¤šå±‚æ¬¡çš„ç›¸ä¼¼æ€§åº¦é‡ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æ§åˆ¶å®éªŒä»¥ç¡®ä¿ç»“æœçš„å¯é æ€§ï¼Œå…³æ³¨æç«¯ä½ç½®çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åå‘æç«¯ä½ç½®æ—¶ï¼Œæ ‡è®°çš„ç‹¬ç‰¹æ€§æ˜¾è‘—é™ä½ï¼Œå‡è´¨åŒ–ç°è±¡åœ¨å¤šå±‚æ¬¡åˆ†æä¸­å¾—åˆ°äº†éªŒè¯ã€‚é€šè¿‡æ§åˆ¶å®éªŒï¼Œå‘ç°ä½ç½®åå·®èƒ½å¤Ÿæ˜¾è‘—æ”¾å¤§å‡è´¨åŒ–æ•ˆåº”ï¼Œæä¾›äº†æ–°çš„å®è¯è¯æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶æä¾›äº†æ–°çš„è§†è§’ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„æ ‡è®°è¡¨ç¤ºå‡è´¨åŒ–é—®é¢˜ã€‚æœªæ¥å¯èƒ½åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬ç”Ÿæˆå’Œå¯¹è¯ç³»ç»Ÿç­‰é¢†åŸŸå…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ï¼Œå¸®åŠ©ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å’Œæå‡ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper investigates token homogenization - the convergence of token representations toward uniformity across transformer layers and its relationship to positional bias in large language models. We empirically examine whether homogenization occurs and how positional bias amplifies this effect. Through layer-wise similarity analysis and controlled experiments, we demonstrate that tokens systematically lose distinctiveness during processing, particularly when biased toward extremal positions. Our findings confirm both the existence of homogenization and its dependence on positional attention mechanisms.

