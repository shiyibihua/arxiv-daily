---
layout: default
title: KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF
---

# KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17000" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17000v1</a>
  <a href="https://arxiv.org/pdf/2508.17000.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17000v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17000v1', 'KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jason R Brown, Lennie Wells, Edward James Young, Sergio Bacallado

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKLæ­£åˆ™åŒ–Qå­¦ä¹ ä»¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è¯­è¨€æ¨¡å‹` `KLæ•£åº¦` `ç­–ç•¥ä¼˜åŒ–` `å¯¹è¯ç”Ÿæˆ` `æ–‡æœ¬æ‘˜è¦` `äººç±»åé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„PPOç®—æ³•åœ¨å¤„ç†KLæ•£åº¦çº¦æŸæ—¶ç¼ºä¹ç³»ç»Ÿæ€§ï¼Œå­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„KLæ­£åˆ™åŒ–Qå­¦ä¹ ï¼ˆKLQï¼‰æ–¹æ³•ä¸ºLM-RLHFè®¾ç½®æä¾›äº†ä¸€ç§æ–°çš„åŠ¨ä½œå€¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKLQåœ¨è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸Šä¸PPOè¡¨ç°ç›¸å½“ï¼Œå¹¶åœ¨è¯„ä¼°ä¸­èƒœç‡æ›´é«˜ï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œå¹¿æ³›åº”ç”¨äºåŸºäºäººç±»åé¦ˆçš„è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆLM-RLHFï¼‰ã€‚å°½ç®¡PPOåœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å…¶å¤„ç†KLæ•£åº¦çº¦æŸçš„æ–¹å¼è¾ƒä¸ºéšæ„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨ä½œå€¼å¼ºåŒ–å­¦ä¹ æ–¹æ³•KLæ­£åˆ™åŒ–Qå­¦ä¹ ï¼ˆKLQï¼‰ï¼Œå¹¶è¯æ˜è¯¥æ–¹æ³•åœ¨æŸç§ç‰¹å®šæ„ä¹‰ä¸Šç­‰ä»·äºPPOï¼Œå°½ç®¡å…¶åŠ¨æœºæˆªç„¶ä¸åŒã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…³é”®çš„è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸Šå¯¹KLQè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜KLQåœ¨ä¼˜åŒ–LM-RLHFç›®æ ‡æ—¶ä¸PPOè¡¨ç°ç›¸å½“ï¼Œå¹¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­å¯¹PPOçš„èƒœç‡æ›´é«˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰PPOç®—æ³•åœ¨å¤„ç†KLæ•£åº¦çº¦æŸæ—¶çš„éšæ„æ€§å’Œç¼ºä¹ç³»ç»Ÿæ€§çš„é—®é¢˜ï¼Œæå‡LM-RLHFçš„æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šKLæ­£åˆ™åŒ–Qå­¦ä¹ ï¼ˆKLQï¼‰é€šè¿‡å¼•å…¥KLæ•£åº¦æ­£åˆ™åŒ–é¡¹ï¼Œæä¾›äº†ä¸€ç§æ–°çš„åŠ¨ä½œå€¼å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKLQçš„æ•´ä½“æ¶æ„åŒ…æ‹¬çŠ¶æ€è¡¨ç¤ºã€åŠ¨ä½œé€‰æ‹©å’Œä»·å€¼è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é€šè¿‡å¼•å…¥KLæ•£åº¦çº¦æŸï¼ŒKLQèƒ½å¤Ÿåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¿æŒç­–ç•¥çš„ç¨³å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šKLQçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å°†åŠ¨ä½œå€¼å­¦ä¹ ä¸KLæ•£åº¦æ­£åˆ™åŒ–ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å­¦ä¹ æœºåˆ¶ï¼Œä¸ä¼ ç»Ÿçš„PPOæ–¹æ³•åœ¨åŠ¨æœºå’Œå®ç°ä¸Šæœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨KLQä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬KLæ•£åº¦çš„æƒé‡è®¾ç½®å’ŒåŠ¨ä½œä»·å€¼å‡½æ•°çš„è®¾è®¡ï¼ŒæŸå¤±å‡½æ•°åˆ™ç»“åˆäº†ä¼ ç»Ÿçš„Qå­¦ä¹ æŸå¤±å’ŒKLæ­£åˆ™åŒ–é¡¹ï¼Œä»¥ç¡®ä¿å­¦ä¹ è¿‡ç¨‹çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒKLQåœ¨ä¼˜åŒ–LM-RLHFç›®æ ‡æ—¶ä¸PPOè¡¨ç°ç›¸å½“ï¼Œå¹¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„ä¼°è€…çš„æµ‹è¯•ä¸­ï¼ŒKLQçš„èƒœç‡ consistently é«˜äºPPOï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ç”Ÿæˆç­‰ä»»åŠ¡ã€‚é€šè¿‡ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼ŒKLQèƒ½å¤Ÿæå‡ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å’Œç›¸å…³æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Proximal Policy Optimisation (PPO) is an established and effective policy gradient algorithm used for Language Model Reinforcement Learning from Human Feedback (LM-RLHF). PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner. In this paper, we develop a a new action-value RL method for the LM-RLHF setting, KL-regularised Q-Learning (KLQ). We then show that our method is equivalent to a version of PPO in a certain specific sense, despite its very different motivation. Finally, we benchmark KLQ on two key language generation tasks -- summarisation and single-turn dialogue. We demonstrate that KLQ performs on-par with PPO at optimising the LM-RLHF objective, and achieves a consistently higher win-rate against PPO on LLM-as-a-judge evaluations.

