---
layout: default
title: ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks
---

# ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16889" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.16889v4</a>
  <a href="https://arxiv.org/pdf/2508.16889.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16889v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16889v4', 'ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hyunjun Kim, Junwoo Ha, Sangyoon Yu, Haon Park

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-23 (Êõ¥Êñ∞: 2025-10-08)

**Â§áÊ≥®**: NeurIPS 2025 Workshop on MTI-LLM

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/hyunjun1121/ObjexMT_dataset)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ObjexMT‰ª•Ëß£ÂÜ≥Â§öËΩÆÂØπËØù‰∏≠ÁöÑÁõÆÊ†áÊèêÂèñ‰∏éÂÖÉËÆ§Áü•Ê†°ÂáÜÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÁõÆÊ†áÊèêÂèñ` `ÂÖÉËÆ§Áü•Ê†°ÂáÜ` `Â§öËΩÆÂØπËØù` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ËØÑÂà§ËÄÖËµÑÊ†º` `ËØ≠‰πâÁõ∏‰ººÂ∫¶` `È£éÈô©ËØÑ‰º∞` `Êú∫Âô®Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§öËΩÆÂØπËØù‰∏≠Èöæ‰ª•ÂáÜÁ°ÆÊèêÂèñÈöêÂê´ÁõÆÊ†áÔºå‰∏îÂØπËØù‰∏ä‰∏ãÊñáÁöÑÂÜóÈïøÊÄßÂΩ±ÂìçÊ®°ÂûãÊÄßËÉΩ„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ObjexMTÂü∫ÂáÜÔºåË¶ÅÊ±ÇÊ®°Âûã‰ªéÂ§öËΩÆÂØπËØù‰∏≠ÊèêÂèñÁõÆÊ†áÂπ∂Êä•ÂëäÁΩÆ‰ø°Â∫¶ÔºåÊó®Âú®ÊèêÂçáÁõÆÊ†áÊèêÂèñÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåkimi-k2Âú®ÁõÆÊ†áÊèêÂèñÂáÜÁ°ÆÊÄß‰∏äËææÂà∞0.612Ôºåclaude-sonnet-4Âú®È£éÈô©ÂíåÊ†°ÂáÜÊñπÈù¢Ë°®Áé∞ÊúÄ‰Ω≥ÔºåÂ±ïÁ§∫‰∫ÜÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

LLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÔºàLLMaaJÔºâËÉΩÂ§üÂÆûÁé∞ÂèØÊâ©Â±ïÁöÑËØÑ‰º∞Ôºå‰ΩÜÁº∫‰πèÂØπËØÑÂà§ËÄÖËµÑÊ†ºÁöÑÂÜ≥ÂÆöÊÄßÊµãËØïÔºöÂÆÉËÉΩÂê¶ÊÅ¢Â§çÂØπËØù‰∏≠ÁöÑÈöêÂê´ÁõÆÊ†áÂπ∂Âà§Êñ≠Êé®Êñ≠ÁöÑÂèØÈù†ÊÄßÔºüÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÊó†ÂÖ≥ÊàñÂÜóÈïøÁöÑ‰∏ä‰∏ãÊñáÊó∂Ë°®Áé∞‰∏ãÈôçÔºåËÄåÂ§öËΩÆË∂äÁã±‰ºöÂ∞ÜÁõÆÊ†áÂàÜÊï£Âú®Â§ö‰∏™ÂõûÂêà‰∏≠„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜObjexMTÔºå‰∏Ä‰∏™Áî®‰∫éÁõÆÊ†áÊèêÂèñÂíåÂÖÉËÆ§Áü•ÁöÑÂü∫ÂáÜ„ÄÇÁªôÂÆöÂ§öËΩÆÂØπËØùËÆ∞ÂΩïÔºåÊ®°ÂûãÂøÖÈ°ªËæìÂá∫‰∏ÄÂè•ËØùÁöÑÂü∫Êú¨ÁõÆÊ†áÂíåËá™ÊàëÊä•ÂëäÁöÑÁΩÆ‰ø°Â∫¶„ÄÇÈÄöËøá‰∏éÈáëÊ†áÂáÜÁõÆÊ†áÁöÑËØ≠‰πâÁõ∏‰ººÂ∫¶Êù•ËØÑÂàÜÂáÜÁ°ÆÊÄßÔºåÂπ∂Âú®300‰∏™Ê†°ÂáÜÈ°πÁõÆ‰∏äËøõË°åÈòàÂÄºÂ§ÑÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºåkimi-k2Âú®ÁõÆÊ†áÊèêÂèñÂáÜÁ°ÆÊÄß‰∏äË°®Áé∞ÊúÄ‰Ω≥ÔºåËÄåclaude-sonnet-4Âú®ÈÄâÊã©ÊÄßÈ£éÈô©ÂíåÊ†°ÂáÜÊñπÈù¢Ë°®Áé∞ÊúÄ‰Ω≥„ÄÇÈ´òÁΩÆ‰ø°Â∫¶ÈîôËØØ‰ªçÁÑ∂Â≠òÂú®ÔºåObjexMT‰∏∫LLMËØÑÂà§ËÄÖÊèê‰æõ‰∫ÜÂèØÊìç‰ΩúÁöÑÊµãËØï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®Â§öËΩÆÂØπËØù‰∏≠ÔºåÂ¶Ç‰ΩïÂáÜÁ°ÆÊèêÂèñÈöêÂê´ÁõÆÊ†áÂèäËØÑÂà§ËÄÖÁöÑÂÖÉËÆ§Áü•ËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂÜóÈïø‰∏ä‰∏ãÊñáÊó∂ÔºåÂÆπÊòìÂØºËá¥ÁõÆÊ†áÊé®Êñ≠ÈîôËØØÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§öËΩÆË∂äÁã±Âú∫ÊôØ‰∏≠„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑObjexMTÂü∫ÂáÜË¶ÅÊ±ÇÊ®°Âûã‰ªéÂ§öËΩÆÂØπËØù‰∏≠ÊèêÂèñÂá∫‰∏ÄÂè•ËØùÁöÑÂü∫Êú¨ÁõÆÊ†áÔºåÂπ∂Ëá™ÊàëÊä•ÂëäÁΩÆ‰ø°Â∫¶„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°Âûã‰∏ç‰ªÖÈúÄË¶ÅÁêÜËß£ÂØπËØùÂÜÖÂÆπÔºåËøòÈúÄËØÑ‰º∞Ëá™Ë∫´Êé®Êñ≠ÁöÑÂèØÈù†ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÁõÆÊ†áÊèêÂèñÊ®°ÂùóÂíåÂÖÉËÆ§Áü•Ê†°ÂáÜÊ®°Âùó„ÄÇÁõÆÊ†áÊèêÂèñÊ®°ÂùóË¥üË¥£‰ªéÂØπËØù‰∏≠ÊèêÂèñÁõÆÊ†áÔºåËÄåÂÖÉËÆ§Áü•Ê†°ÂáÜÊ®°ÂùóÂàôËØÑ‰º∞Ê®°ÂûãÁöÑÁΩÆ‰ø°Â∫¶ÂíåÊé®Êñ≠ÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊ®°ÂûãÁöÑÊÄßËÉΩÈÄöËøá‰∏éÈáëÊ†áÂáÜÁõÆÊ†áÁöÑËØ≠‰πâÁõ∏‰ººÂ∫¶ËøõË°åËØÑ‰º∞ÔºåÂπ∂Âú®Â§ö‰∏™Ê†°ÂáÜÈ°πÁõÆ‰∏äËøõË°åÈòàÂÄºÂ§ÑÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂÖÉËÆ§Áü•Ê†°ÂáÜÁöÑÊ¶ÇÂøµÔºå‰ΩøÂæóÊ®°Âûã‰∏ç‰ªÖÂÖ≥Ê≥®ÁõÆÊ†áÊèêÂèñÁöÑÂáÜÁ°ÆÊÄßÔºåËøòÂÖ≥Ê≥®Ëá™Ë∫´Êé®Êñ≠ÁöÑÂèØÈù†ÊÄß„ÄÇËøô‰∏é‰º†ÁªüÊñπÊ≥ïÁöÑÂçï‰∏ÄÁõÆÊ†áÊèêÂèñÊúâÊâÄÂå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠Ôºå‰ΩøÁî®‰∫Ü300‰∏™Ê†°ÂáÜÈ°πÁõÆËøõË°åÈòàÂÄºÂ§ÑÁêÜÔºåËÆæÂÆö‰∫ÜÁΩÆ‰ø°Â∫¶ÈòàÂÄº$œÑ^	ext{star} = 0.66$ÔºåÂπ∂ÈÄöËøáBrierÂàÜÊï∞ÂíåÊúüÊúõÊ†°ÂáÜËØØÂ∑ÆÁ≠âÊåáÊ†áËØÑ‰º∞Ê®°ÂûãÁöÑÂÖÉËÆ§Áü•ËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºåkimi-k2Âú®ÁõÆÊ†áÊèêÂèñÂáÜÁ°ÆÊÄß‰∏äËææÂà∞0.612Ôºåclaude-sonnet-4Âú®ÈÄâÊã©ÊÄßÈ£éÈô©ÂíåÊ†°ÂáÜÊñπÈù¢Ë°®Áé∞ÊúÄ‰Ω≥ÔºàAURC 0.242ÔºåECE 0.206ÔºåBrier 0.254Ôºâ„ÄÇÈ´òÁΩÆ‰ø°Â∫¶ÈîôËØØÁöÑÊØî‰æãÂú®‰∏çÂêåÊ®°ÂûãÈó¥Â∑ÆÂºÇÊòæËëóÔºåË°®ÊòéÊ®°ÂûãÂú®Â§ÑÁêÜÈöêÂê´ÁõÆÊ†áÊó∂‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂÆ¢Êúç„ÄÅÂØπËØùÁ≥ªÁªüÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÂú∫ÊôØ„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÂú®Â§öËΩÆÂØπËØù‰∏≠ÁöÑÁõÆÊ†áÊèêÂèñËÉΩÂäõÂíåËá™ÊàëËØÑ‰º∞ËÉΩÂäõÔºåÂèØ‰ª•ÊòæËëóÊîπÂñÑÁî®Êà∑‰ΩìÈ™åÂíåÁ≥ªÁªüÁöÑÂÜ≥Á≠ñË¥®Èáè„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÂú®Êõ¥Â§çÊùÇÁöÑÂØπËØùÂú∫ÊôØ‰∏≠ÂæóÂà∞Â∫îÁî®ÔºåÊé®Âä®Êô∫ËÉΩÂØπËØùÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> LLM-as-a-Judge (LLMaaJ) enables scalable evaluation, yet we lack a decisive test of a judge's qualification: can it recover the hidden objective of a conversation and know when that inference is reliable? Large language models degrade with irrelevant or lengthy context, and multi-turn jailbreaks can scatter goals across turns. We present ObjexMT, a benchmark for objective extraction and metacognition. Given a multi-turn transcript, a model must output a one-sentence base objective and a self-reported confidence. Accuracy is scored by semantic similarity to gold objectives, then thresholded once on 300 calibration items ($œÑ^\star = 0.66$; $F_1@œÑ^\star = 0.891$). Metacognition is assessed with expected calibration error, Brier score, Wrong@High-Confidence (0.80 / 0.90 / 0.95), and risk--coverage curves. Across six models (gpt-4.1, claude-sonnet-4, Qwen3-235B-A22B-FP8, kimi-k2, deepseek-v3.1, gemini-2.5-flash) evaluated on SafeMTData\_Attack600, SafeMTData\_1K, and MHJ, kimi-k2 achieves the highest objective-extraction accuracy (0.612; 95\% CI [0.594, 0.630]), while claude-sonnet-4 (0.603) and deepseek-v3.1 (0.599) are statistically tied. claude-sonnet-4 offers the best selective risk and calibration (AURC 0.242; ECE 0.206; Brier 0.254). Performance varies sharply across datasets (16--82\% accuracy), showing that automated obfuscation imposes challenges beyond model choice. High-confidence errors remain: Wrong@0.90 ranges from 14.9\% (claude-sonnet-4) to 47.7\% (Qwen3-235B-A22B-FP8). ObjexMT therefore supplies an actionable test for LLM judges: when objectives are implicit, judges often misinfer them; exposing objectives or gating decisions by confidence is advisable. All experimental data are in the Supplementary Material and at https://github.com/hyunjun1121/ObjexMT_dataset.

