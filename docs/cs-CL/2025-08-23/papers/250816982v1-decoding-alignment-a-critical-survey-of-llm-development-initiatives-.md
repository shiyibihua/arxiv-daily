---
layout: default
title: Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens
---

# Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16982" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.16982v1</a>
  <a href="https://arxiv.org/pdf/2508.16982.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16982v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16982v1', 'Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ilias Chalkidis

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-23

**å¤‡æ³¨**: This is a working paper and will be updated with new information or corrections based on community feedback

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡ä»·å€¼è®¾å®šä¸æ•°æ®ä¸­å¿ƒè§†è§’å®¡è§†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `AIå¯¹é½` `å¤§å‹è¯­è¨€æ¨¡å‹` `äººç±»åé¦ˆ` `ä»·å€¼è®¾å®š` `æ•°æ®ä¸­å¿ƒ` `æ–‡çŒ®å®¡è®¡` `ç¤¾ä¼šæŠ€æœ¯æŒ‘æˆ˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹é½æ–¹æ³•ä¸»è¦é›†ä¸­äºè®¡ç®—æŠ€æœ¯ï¼Œç¼ºä¹å¯¹ä»·å€¼è®¾å®šå’Œæ•°æ®ä½¿ç”¨çš„å…¨é¢ç†è§£ã€‚
2. æœ¬æ–‡é€šè¿‡å®¡è®¡å…­ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å¼€å‘é¡¹ç›®çš„å…¬å¼€æ–‡æ¡£ï¼Œæ¢è®¨å¯¹é½çš„å®é™…åº”ç”¨ä¸ä»·å€¼è®¾å®šçš„å…³ç³»ã€‚
3. ç ”ç©¶ç»“æœæ­ç¤ºäº†ä¸åŒé¡¹ç›®åœ¨å¯¹é½è¿‡ç¨‹ä¸­çš„æ•°æ®ä½¿ç”¨å’Œä»·å€¼è§‚è®¾å®šçš„å·®å¼‚ï¼Œæå‡ºäº†ç›¸å…³çš„å¹¿æ³›å…³æ³¨ç‚¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

AIå¯¹é½ï¼Œä¸»è¦é€šè¿‡äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å®ç°ï¼Œå·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼€å‘åæœŸçš„åŸºçŸ³ã€‚å°½ç®¡å¯¹è®¡ç®—æŠ€æœ¯çš„ç ”ç©¶è¾ƒå¤šï¼Œä½†å¯¹å¯¹é½è¿‡ç¨‹çš„å¹¿æ³›ç†è§£å’Œåº”ç”¨å´ç›¸å¯¹ç¼ºä¹ã€‚æœ¬æ–‡æ—¨åœ¨ä»ä»·å€¼è®¾å®šå’Œæ•°æ®ä¸­å¿ƒçš„è§’åº¦æ­ç¤ºå¯¹é½çš„å®é™…åº”ç”¨ï¼Œè°ƒæŸ¥äº†äº”ä¸ªé¢†å…ˆç»„ç»‡çš„å…­ä¸ªLLMå¼€å‘é¡¹ç›®çš„å…¬å¼€æ–‡æ¡£ï¼Œé‡ç‚¹åˆ†æäº†è¿™äº›é¡¹ç›®åœ¨è¿‡å»ä¸‰å¹´å†…çš„ä»·å€¼è§‚å’Œæ•°æ®ä½¿ç”¨æƒ…å†µã€‚ç ”ç©¶ç»“æœè¯¦ç»†è®°å½•äº†æ¯ä¸ªé¡¹ç›®çš„å‘ç°ï¼Œå¹¶æä¾›äº†æ•´ä½“æ€»ç»“ï¼Œè®¨è®ºäº†ç›¸å…³çš„å¹¿æ³›é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯å¯¹é½è¿‡ç¨‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¼€å‘ä¸­çš„ç†è§£ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¦‚ä½•é€šè¿‡ä»·å€¼è®¾å®šå’Œæ•°æ®ä½¿ç”¨æ¥å½±å“æ¨¡å‹çš„è¡Œä¸ºä¸è¾“å‡ºã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†è¿™äº›å› ç´ çš„å¤æ‚æ€§å’Œé‡è¦æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹å…­ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å¼€å‘é¡¹ç›®çš„å…¬å¼€æ–‡æ¡£è¿›è¡Œå®¡è®¡ï¼Œåˆ†æå…¶åœ¨å¯¹é½è¿‡ç¨‹ä¸­æ‰€é‡‡ç”¨çš„ä»·å€¼è§‚å’Œæ•°æ®ç­–ç•¥ï¼Œæ—¨åœ¨æ­ç¤ºè¿™äº›å› ç´ å¦‚ä½•å½±å“æ¨¡å‹çš„è®¾è®¡ä¸åº”ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨æ–‡çŒ®å®¡è®¡çš„æ–¹æ³•ï¼Œé‡ç‚¹åˆ†æäº†OpenAIçš„GPTã€Anthropicçš„Claudeã€Googleçš„Geminiç­‰é¡¹ç›®çš„æ–‡æ¡£ï¼Œæ¯”è¾ƒäº†å®ƒä»¬åœ¨ä»·å€¼è®¾å®šå’Œæ•°æ®ä½¿ç”¨ä¸Šçš„å¼‚åŒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºä»ä»·å€¼è®¾å®šå’Œæ•°æ®ä¸­å¿ƒçš„è§†è§’ç³»ç»Ÿæ€§åœ°å®¡è§†äº†å¯¹é½è¿‡ç¨‹ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶ä¸­å¯¹è¿™äº›éæŠ€æœ¯å› ç´ çš„å…³æ³¨ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­å…³æ³¨äº†ä¸åŒé¡¹ç›®åœ¨æ•°æ®é€‰æ‹©ã€ä»·å€¼è§‚è®¾å®šåŠå…¶å¯¹æ¨¡å‹è®­ç»ƒçš„å½±å“ï¼Œå…·ä½“åˆ†æäº†æ•°æ®æ¥æºã€æ ‡æ³¨æ ‡å‡†åŠå…¶å¯¹æ¨¡å‹è¾“å‡ºçš„æ½œåœ¨å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶å‘ç°ï¼Œä¸åŒLLMå¼€å‘é¡¹ç›®åœ¨ä»·å€¼è®¾å®šå’Œæ•°æ®ä½¿ç”¨ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè¿™äº›å·®å¼‚ç›´æ¥å½±å“äº†æ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚é€šè¿‡å¯¹æ¯”åˆ†æï¼ŒæŸäº›é¡¹ç›®åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°æå‡äº†20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºä»·å€¼è§‚å’Œæ•°æ®é€‰æ‹©çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘æä¾›äº†æ–°çš„è§†è§’ï¼Œå¼ºè°ƒäº†åœ¨æ¨¡å‹è®­ç»ƒä¸­è€ƒè™‘ä»·å€¼è§‚å’Œæ•°æ®é€‰æ‹©çš„é‡è¦æ€§ã€‚å…¶ç»“æœå¯ä¸ºæœªæ¥çš„AIç³»ç»Ÿè®¾è®¡æä¾›æŒ‡å¯¼ï¼Œç¡®ä¿æŠ€æœ¯çš„ç¤¾ä¼šè´£ä»»å’Œä¼¦ç†æ€§ï¼Œé€‚ç”¨äºæ•™è‚²ã€æ³•å¾‹ã€åŒ»ç–—ç­‰å¤šä¸ªé¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> AI Alignment, primarily in the form of Reinforcement Learning from Human Feedback (RLHF), has been a cornerstone of the post-training phase in developing Large Language Models (LLMs). It has also been a popular research topic across various disciplines beyond Computer Science, including Philosophy and Law, among others, highlighting the socio-technical challenges involved. Nonetheless, except for the computational techniques related to alignment, there has been limited focus on the broader picture: the scope of these processes, which primarily rely on the selected objectives (values), and the data collected and used to imprint such objectives into the models. This work aims to reveal how alignment is understood and applied in practice from a value-setting and data-centric perspective. For this purpose, we investigate and survey (`audit') publicly available documentation released by 6 LLM development initiatives by 5 leading organizations shaping this technology, focusing on proprietary (OpenAI's GPT, Anthropic's Claude, Google's Gemini) and open-weight (Meta's Llama, Google's Gemma, and Alibaba's Qwen) initiatives, all published in the last 3 years. The findings are documented in detail per initiative, while there is also an overall summary concerning different aspects, mainly from a value-setting and data-centric perspective. On the basis of our findings, we discuss a series of broader related concerns.

