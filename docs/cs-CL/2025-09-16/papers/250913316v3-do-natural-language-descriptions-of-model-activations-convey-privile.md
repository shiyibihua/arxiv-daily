---
layout: default
title: Do Natural Language Descriptions of Model Activations Convey Privileged Information?
---

# Do Natural Language Descriptions of Model Activations Convey Privileged Information?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13316" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13316v3</a>
  <a href="https://arxiv.org/pdf/2509.13316.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13316v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13316v3', 'Do Natural Language Descriptions of Model Activations Convey Privileged Information?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Millicent Li, Alberto Mario Ceballos Arroyo, Giordano Rogers, Naomi Saphra, Byron C. Wallace

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-16 (æ›´æ–°: 2025-12-09)

**å¤‡æ³¨**: 40 pages, 6 figures. Updated and added content

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°LLMæ¿€æ´»è‡ªç„¶è¯­è¨€æè¿°æ˜¯å¦æ³„éœ²æ¨¡å‹å†…éƒ¨ç‰¹æƒä¿¡æ¯ï¼Œæ­ç¤ºç°æœ‰è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `æ¿€æ´»verbalization` `æ¨¡å‹ç†è§£` `å—æ§å®éªŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä½¿ç”¨LLMå°†ç›®æ ‡æ¨¡å‹çš„æ¿€æ´»è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ï¼Œè¯•å›¾ç†è§£æ¨¡å‹å†…éƒ¨è¿ä½œï¼Œä½†å…¶æœ‰æ•ˆæ€§å­˜ç–‘ã€‚
2. è®ºæ–‡é€šè¿‡å®éªŒåˆ†æè¡¨æ˜ï¼Œverbalizationæ–¹æ³•å¯èƒ½ä»…ä»…åæ˜ äº†verbalizer LLMçš„çŸ¥è¯†ï¼Œè€Œéç›®æ ‡æ¨¡å‹ã€‚
3. ç ”ç©¶å¼ºè°ƒéœ€è¦æ›´ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•å’Œå®éªŒæ§åˆ¶ï¼Œä»¥è¯„ä¼°verbalizationæ–¹æ³•æ˜¯å¦çœŸæ­£æ­ç¤ºäº†LLMçš„å†…éƒ¨æœºåˆ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ€è¿‘çš„å¯è§£é‡Šæ€§æ–¹æ³•æå‡ºä½¿ç”¨ç¬¬äºŒä¸ªè¯­è¨€æ¨¡å‹ï¼ˆverbalizer LLMï¼‰å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…éƒ¨è¡¨ç¤ºè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œæ—¨åœ¨é˜æ˜ç›®æ ‡æ¨¡å‹å¦‚ä½•è¡¨ç¤ºå’Œå¤„ç†è¾“å…¥ã€‚ä½†æ˜¯ï¼Œè¿™ç§æ¿€æ´»verbalizationæ–¹æ³•æ˜¯å¦çœŸæ­£æä¾›äº†å…³äºç›®æ ‡æ¨¡å‹å†…éƒ¨å·¥ä½œåŸç†çš„ç‰¹æƒçŸ¥è¯†ï¼Œæˆ–è€…ä»…ä»…ä¼ è¾¾äº†å…³äºå…¶è¾“å…¥çš„ä¿¡æ¯ï¼Ÿæˆ‘ä»¬æ‰¹åˆ¤æ€§åœ°è¯„ä¼°äº†å…ˆå‰å·¥ä½œä¸­ä½¿ç”¨çš„æµè¡Œverbalizationæ–¹æ³•ï¼Œå‘ç°å®ƒä»¬å¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•ç›®æ ‡æ¨¡å‹å†…éƒ¨ä¿¡æ¯çš„æƒ…å†µä¸‹æˆåŠŸå®ŒæˆåŸºå‡†æµ‹è¯•ï¼Œè¿™è¡¨æ˜è¿™äº›æ•°æ®é›†å¯èƒ½ä¸é€‚åˆè¯„ä¼°verbalizationæ–¹æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›è¡Œäº†å—æ§å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œverbalizationé€šå¸¸åæ˜ äº†ç”Ÿæˆå®ƒä»¬çš„verbalizer LLMçš„å‚æ•°çŸ¥è¯†ï¼Œè€Œä¸æ˜¯è¢«è§£ç çš„ç›®æ ‡LLMçš„çŸ¥è¯†ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜éœ€è¦æœ‰é’ˆå¯¹æ€§çš„åŸºå‡†å’Œå®éªŒæ§åˆ¶ï¼Œä»¥ä¸¥æ ¼è¯„ä¼°verbalizationæ–¹æ³•æ˜¯å¦èƒ½æä¾›å¯¹LLMæ“ä½œçš„æœ‰æ„ä¹‰çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§£é‡Šæ€§æ–¹æ³•å°è¯•é€šè¿‡å°†LLMå†…éƒ¨æ¿€æ´»è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°æ¥ç†è§£æ¨¡å‹è¡Œä¸ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æ˜¯å¦çœŸçš„æ­ç¤ºäº†ç›®æ ‡æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ï¼Œè¿˜æ˜¯ä»…ä»…åæ˜ äº†è¾“å…¥ä¿¡æ¯ï¼Œå°šä¸æ˜ç¡®ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¯èƒ½å­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•æœ‰æ•ˆåŒºåˆ†è¿™ä¸¤ç§æƒ…å†µã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡å—æ§å®éªŒï¼Œåˆ†æverbalizationæ–¹æ³•åœ¨æ²¡æœ‰è®¿é—®ç›®æ ‡æ¨¡å‹å†…éƒ¨ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œä»¥åŠåœ¨ä¸åŒverbalizer LLMä¸‹ï¼Œèƒ½å¦æˆåŠŸå®Œæˆç‰¹å®šä»»åŠ¡ã€‚å¦‚æœverbalizationæ–¹æ³•ä»…ä¾èµ–verbalizer LLMçš„çŸ¥è¯†å°±èƒ½æˆåŠŸï¼Œåˆ™è¡¨æ˜å…¶å¹¶æœªçœŸæ­£æ­ç¤ºç›®æ ‡æ¨¡å‹çš„å†…éƒ¨è¿ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨å®éªŒåˆ†æçš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œåœ¨ç°æœ‰æ•°æ®é›†ä¸Šè¯„ä¼°verbalizationæ–¹æ³•ï¼Œè§‚å¯Ÿå…¶åœ¨æ²¡æœ‰ç›®æ ‡æ¨¡å‹ä¿¡æ¯æ—¶çš„è¡¨ç°ã€‚ç„¶åï¼Œè®¾è®¡å—æ§å®éªŒï¼Œæ”¹å˜verbalizer LLMï¼Œè§‚å¯Ÿverbalizationç»“æœçš„å˜åŒ–ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæƒ…å†µä¸‹çš„è¡¨ç°ï¼Œåˆ¤æ–­verbalizationæ–¹æ³•æ˜¯å¦ä¾èµ–äºç›®æ ‡æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹ç°æœ‰verbalizationæ–¹æ³•çš„è¯„ä¼°æ–¹å¼æå‡ºäº†è´¨ç–‘ï¼Œå¹¶è®¾è®¡äº†å—æ§å®éªŒæ¥åŒºåˆ†verbalizationç»“æœä¸­æ¥è‡ªç›®æ ‡æ¨¡å‹å’Œverbalizer LLMçš„ä¿¡æ¯ã€‚è¿™æœ‰åŠ©äºæ›´å‡†ç¡®åœ°è¯„ä¼°verbalizationæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒè®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç°æœ‰æ•°æ®é›†è¯„ä¼°verbalizationæ–¹æ³•åœ¨æ²¡æœ‰ç›®æ ‡æ¨¡å‹ä¿¡æ¯æ—¶çš„è¡¨ç°ï¼›2) æ›´æ¢ä¸åŒçš„verbalizer LLMï¼Œè§‚å¯Ÿverbalizationç»“æœçš„å˜åŒ–ï¼›3) åˆ†æverbalizationç»“æœä¸ç›®æ ‡æ¨¡å‹å’Œverbalizer LLMçš„çŸ¥è¯†ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºæ‰€ä½¿ç”¨çš„verbalizationæ–¹æ³•å’ŒLLMã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„verbalizationæ–¹æ³•å¯ä»¥åœ¨æ²¡æœ‰è®¿é—®ç›®æ ‡æ¨¡å‹å†…éƒ¨ä¿¡æ¯çš„æƒ…å†µä¸‹æˆåŠŸå®ŒæˆåŸºå‡†æµ‹è¯•ï¼Œè¿™è¡¨æ˜è¿™äº›æ•°æ®é›†å¯èƒ½ä¸é€‚åˆè¯„ä¼°verbalizationæ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°verbalizationç»“æœæ›´å¤šåœ°åæ˜ äº†verbalizer LLMçš„çŸ¥è¯†ï¼Œè€Œéç›®æ ‡LLMçš„çŸ¥è¯†ã€‚è¿™äº›å‘ç°å¯¹ç°æœ‰verbalizationæ–¹æ³•çš„æœ‰æ•ˆæ€§æå‡ºäº†è´¨ç–‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯¹äºå¼€å‘æ›´å¯é ã€æ›´æœ‰æ•ˆçš„LLMå¯è§£é‡Šæ€§æ–¹æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚é€šè¿‡æ›´å‡†ç¡®åœ°è¯„ä¼°verbalizationæ–¹æ³•ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£LLMçš„å†…éƒ¨è¿ä½œæœºåˆ¶ï¼Œä»è€Œæ”¹è¿›æ¨¡å‹è®¾è®¡ã€æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶å¢å¼ºäººä»¬å¯¹LLMçš„ä¿¡ä»»ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºå…¶ä»–å¯è§£é‡Šæ€§æ–¹æ³•çš„è®¾è®¡å’Œè¯„ä¼°æä¾›äº†å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they can succeed at benchmarks without any access to target model internals, suggesting that these datasets may not be ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the knowledge of the target LLM whose activations are decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs.

