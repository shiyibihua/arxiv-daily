---
layout: default
title: Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky
---

# Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04399" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.04399v1</a>
  <a href="https://arxiv.org/pdf/2508.04399.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04399v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04399v1', 'Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xu Zhang, Mei Chen

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.IR, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-06

**Â§áÊ≥®**: 19 pages, 2 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊèêÂçáËÇØÂ°îÂü∫Â∑û‰∫ãÊïÖÊï∞ÊçÆË¥®Èáè**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `‰∫ãÊïÖÊï∞ÊçÆÂàÜÊûê` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÂæÆË∞ÉÂèòÊç¢Âô®` `‰∫§ÈÄöÂÆâÂÖ®` `Êï∞ÊçÆË¥®ÈáèÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑ‰∫ãÊïÖÊï∞ÊçÆÂ§ÑÁêÜÊñπÊ≥ïÂú®Êï∞ÊçÆË¥®ÈáèÂíåÊïàÁéá‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∫åÊ¨°‰∫ãÊïÖÁöÑËØÜÂà´‰∏ä„ÄÇ
2. Êú¨Á†îÁ©∂ÊèêÂá∫Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂæÆË∞ÉÁöÑÂèòÊç¢Âô®Ê®°ÂûãÊù•ÊåñÊéòÂíåÂàÜÊûê‰∫ãÊïÖÂèôËø∞Ôºå‰ª•ÊèêÂçáÊï∞ÊçÆË¥®Èáè„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂæÆË∞ÉÁöÑRoBERTaÊ®°ÂûãÂú®F1-scoreÂíåÂáÜÁ°ÆÁéá‰∏äÂùá‰ºò‰∫é‰º†ÁªüÈÄªËæëÂõûÂΩíÔºå‰∏îÂ§ÑÁêÜÈÄüÂ∫¶ÊòæËëóÊèêÈ´ò„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Á†îÁ©∂ËØÑ‰º∞‰∫ÜÂÖàËøõÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊäÄÊúØÔºåÈÄöËøáÊåñÊéò‰∫ãÊïÖÂèôËø∞Êù•ÊèêÈ´ò‰∫ãÊïÖÊï∞ÊçÆË¥®ÈáèÔºå‰ª•ËÇØÂ°îÂü∫Â∑ûÁöÑ‰∫åÊ¨°‰∫ãÊïÖËØÜÂà´‰∏∫Ê°à‰æã„ÄÇÁ†îÁ©∂Âü∫‰∫é2015-2022Âπ¥Èó¥ÊâãÂä®ÂÆ°Êü•ÁöÑ16,656Êù°ÂèôËø∞ÔºåÊØîËæÉ‰∫Ü‰∏âÁ±ªÊ®°ÂûãÔºöÈõ∂Ê†∑Êú¨ÂºÄÊ∫êÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ„ÄÅÂæÆË∞ÉÁöÑÂèòÊç¢Âô®Ê®°ÂûãÂíå‰º†ÁªüÁöÑÈÄªËæëÂõûÂΩí‰Ωú‰∏∫Âü∫Á∫ø„ÄÇÂæÆË∞ÉÁöÑÂèòÊç¢Âô®Ê®°ÂûãË°®Áé∞‰ºòÂºÇÔºåÂÖ∂‰∏≠RoBERTaÁöÑF1-scoreËææÂà∞0.90ÔºåÂáÜÁ°ÆÁéá‰∏∫95%„ÄÇÂ∞ΩÁÆ°LLMsÂú®Êüê‰∫õÂèò‰ΩìÁöÑÂè¨ÂõûÁéá‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜËÆ°ÁÆóÊàêÊú¨ËæÉÈ´òÔºåËÄåÂæÆË∞ÉÊ®°ÂûãÂú®ÁªèËøáÁÆÄÁü≠ËÆ≠ÁªÉÂêéËÉΩÂú®Âá†ÁßíÂÜÖÂ§ÑÁêÜÊµãËØïÈõÜ„ÄÇÁ†îÁ©∂ÁªìÊûúÂº∫Ë∞É‰∫ÜÂáÜÁ°ÆÊÄß„ÄÅÊïàÁéáÂíåÊï∞ÊçÆÈúÄÊ±Ç‰πãÈó¥ÁöÑÊùÉË°°ÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂèØÂ§çÂà∂ÁöÑÊñπÊ°à‰ª•Âà©Áî®ÂÖàËøõÁöÑNLPÊèêÂçá‰∫ãÊïÖÊï∞ÊçÆË¥®Èáè„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨Á†îÁ©∂Êó®Âú®Ëß£ÂÜ≥Áé∞Êúâ‰∫ãÊïÖÊï∞ÊçÆÂ§ÑÁêÜÊñπÊ≥ïÂú®Êï∞ÊçÆË¥®ÈáèÂíåÊïàÁéá‰∏äÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØ‰∫åÊ¨°‰∫ãÊïÖÁöÑËØÜÂà´Âõ∞Èöæ„ÄÇ‰º†ÁªüÊñπÊ≥ïÂæÄÂæÄ‰æùËµñ‰∫éÊâãÂä®ÂÆ°Êü•ÔºåÊïàÁéá‰Ωé‰∏ã‰∏îÂÆπÊòìÂá∫Èîô„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉËß£ÂÜ≥ÊÄùË∑ØÊòØÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂæÆË∞ÉÁöÑÂèòÊç¢Âô®Ê®°ÂûãÔºåÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊäÄÊúØËá™Âä®ÂåñÂàÜÊûê‰∫ãÊïÖÂèôËø∞Ôºå‰ªéËÄåÊèêÈ´òÊï∞ÊçÆË¥®ÈáèÂíåÂ§ÑÁêÜÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÊî∂ÈõÜ„ÄÅÊ®°ÂûãËÆ≠ÁªÉÂíåÊµãËØï‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇÈ¶ñÂÖàÊî∂ÈõÜ2015-2022Âπ¥ÁöÑ‰∫ãÊïÖÂèôËø∞Êï∞ÊçÆÔºåÁÑ∂ÂêéÂØπ‰∏çÂêåÊ®°ÂûãËøõË°åËÆ≠ÁªÉÔºåÊúÄÂêéÂú®2022Âπ¥ÁöÑÊï∞ÊçÆ‰∏äËøõË°åÊµãËØïÂíåËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÊØîËæÉ‰∫ÜÂ§öÁßçÊ®°ÂûãÔºåÂåÖÊã¨Èõ∂Ê†∑Êú¨Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂæÆË∞ÉÁöÑÂèòÊç¢Âô®Ê®°ÂûãÔºåÂèëÁé∞ÂæÆË∞ÉÊ®°ÂûãÂú®ÂáÜÁ°ÆÊÄßÂíåÊïàÁéá‰∏äÂÖ∑ÊúâÊòæËëó‰ºòÂäøÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÈÄüÂ∫¶‰∏ä„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆ≠ÁªÉ‰∏≠ÔºåÈááÁî®‰∫ÜÂ§öÁßçÂèòÊç¢Âô®Êû∂ÊûÑÔºàÂ¶ÇRoBERTa„ÄÅBERTÁ≠âÔºâËøõË°åÂæÆË∞ÉÔºåËÆæÁΩÆ‰∫ÜÈÄÇÂΩìÁöÑË∂ÖÂèÇÊï∞‰ª•‰ºòÂåñÊ®°ÂûãÊÄßËÉΩÔºåÊçüÂ§±ÂáΩÊï∞ÈÄâÊã©‰∫ÜÈÄÇÂêàÂàÜÁ±ª‰ªªÂä°ÁöÑ‰∫§ÂèâÁÜµÊçüÂ§±„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂæÆË∞ÉÁöÑRoBERTaÊ®°ÂûãÂú®F1-score‰∏äËææÂà∞0.90ÔºåÂáÜÁ°ÆÁéá‰∏∫95%ÔºåÊòæËëó‰ºò‰∫é‰º†ÁªüÈÄªËæëÂõûÂΩíÔºàF1:0.66Ôºâ„ÄÇÈõ∂Ê†∑Êú¨LLaMA3:70BÊ®°ÂûãÁöÑF1-score‰∏∫0.86Ôºå‰ΩÜÊé®ÁêÜÊó∂Èó¥ÈïøËææ139ÂàÜÈíüÔºåËÄåÂæÆË∞ÉÊ®°ÂûãÂàôÂú®Âá†ÁßíÂÜÖÂÆåÊàêÊµãËØïÔºåÂ±ïÁé∞Âá∫Êõ¥È´òÁöÑÊïàÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰∫§ÈÄöÂÆâÂÖ®ÁÆ°ÁêÜ„ÄÅ‰∫ãÊïÖÊï∞ÊçÆÂàÜÊûêÂíåÂÖ¨ÂÖ±ÊîøÁ≠ñÂà∂ÂÆö„ÄÇÈÄöËøáÊèêÂçá‰∫ãÊïÖÊï∞ÊçÆÁöÑË¥®ÈáèÔºåÁõ∏ÂÖ≥ÈÉ®Èó®ËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ËØÜÂà´‰∫ãÊïÖÂéüÂõ†ÔºåÂà∂ÂÆöÈíàÂØπÊÄßÁöÑÂÆâÂÖ®Êé™ÊñΩÔºå‰ªéËÄåÂáèÂ∞ë‰∫§ÈÄö‰∫ãÊïÖÁöÑÂèëÁîüÁéáÔºåÊèêÂçáÂÖ¨ÂÖ±ÂÆâÂÖ®Ê∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This study evaluates advanced natural language processing (NLP) techniques to enhance crash data quality by mining crash narratives, using secondary crash identification in Kentucky as a case study. Drawing from 16,656 manually reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we compare three model classes: zero-shot open-source large language models (LLMs) (LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers (BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic regression as baseline. Models were calibrated on 2015-2021 data and tested on 1,771 narratives from 2022. Fine-tuned transformers achieved superior performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy (95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139 minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred high computational costs (up to 723 minutes for DeepSeek-R1:70B), while fine-tuned models processed the test set in seconds after brief training. Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can rival larger counterparts in performance while reducing runtime, suggesting opportunities for optimized deployments. Results highlight trade-offs between accuracy, efficiency, and data requirements, with fine-tuned transformer models balancing precision and recall effectively on Kentucky data. Practical deployment considerations emphasize privacy-preserving local deployment, ensemble approaches for improved accuracy, and incremental processing for scalability, providing a replicable scheme for enhancing crash-data quality with advanced NLP.

