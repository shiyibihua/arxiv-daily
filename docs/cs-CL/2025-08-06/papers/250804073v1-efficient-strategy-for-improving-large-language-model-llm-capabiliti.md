---
layout: default
title: Efficient Strategy for Improving Large Language Model (LLM) Capabilities
---

# Efficient Strategy for Improving Large Language Model (LLM) Capabilities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04073" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04073v1</a>
  <a href="https://arxiv.org/pdf/2508.04073.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04073v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04073v1', 'Efficient Strategy for Improving Large Language Model (LLM) Capabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: JuliÃ¡n Camilo Velandia GutiÃ©rrez

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: Based on master's thesis in Systems and Computer Engineering, Universidad Nacional de Colombia (2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆç­–ç•¥ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®å¤„ç†` `è®­ç»ƒç­–ç•¥` `æ¶æ„è°ƒæ•´` `èµ„æºä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²é¢ä¸´è®¡ç®—èµ„æºéœ€æ±‚é«˜çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ•°æ®å¤„ç†ã€é€‰æ‹©æŠ€æœ¯å’Œæ¶æ„è°ƒæ•´ç­‰æ–¹æ³•ï¼Œä»åŸºç¡€æ¨¡å‹å‡ºå‘æå‡LLMsçš„æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç­–ç•¥åœ¨èƒ½åŠ›ã€å“åº”æ—¶é—´ç­‰æ–¹é¢æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå·²æˆä¸ºé‡è¦é‡Œç¨‹ç¢‘ã€‚ç„¶è€Œï¼Œå…¶å¤§è§„æ¨¡éƒ¨ç½²å—åˆ°è®¡ç®—èµ„æºéœ€æ±‚çš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»åŸºç¡€æ¨¡å‹å‡ºå‘çš„ç­–ç•¥ï¼Œé€šè¿‡æ•°æ®å¤„ç†ã€æ•°æ®é€‰æ‹©æŠ€æœ¯ã€è®­ç»ƒç­–ç•¥å’Œæ¶æ„è°ƒæ•´ï¼Œæå‡LLMsåœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„æ•ˆç‡ã€‚ç ”ç©¶æ–¹æ³•åŒ…æ‹¬å®šä¹‰æ„å»ºå¯é æ•°æ®é›†çš„æ ‡å‡†ï¼Œè¿›è¡Œä¸åŒé…ç½®çš„æ§åˆ¶å®éªŒï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°å„å˜ä½“åœ¨èƒ½åŠ›ã€çµæ´»æ€§ã€å“åº”æ—¶é—´å’Œå®‰å…¨æ€§æ–¹é¢çš„è¡¨ç°ã€‚æœ€åï¼Œé€šè¿‡æ¯”è¾ƒæµ‹è¯•éªŒè¯äº†æ‰€æç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²å›°éš¾çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ™®ééœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ä»åŸºç¡€æ¨¡å‹å‡ºå‘ï¼Œç»“åˆæ•°æ®å¤„ç†ã€é€‰æ‹©æŠ€æœ¯å’Œæ¶æ„è°ƒæ•´ï¼Œæå‡æ¨¡å‹åœ¨ç‰¹å®šçŸ¥è¯†åº“å†…çš„æ•ˆç‡ï¼Œæ—¨åœ¨é™ä½è®¡ç®—èµ„æºæ¶ˆè€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€å®éªŒè®¾è®¡å’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå®šä¹‰å¯é æ•°æ®é›†æ ‡å‡†ï¼Œç„¶åè¿›è¡Œä¸åŒé…ç½®çš„æ§åˆ¶å®éªŒï¼Œæœ€åç³»ç»Ÿè¯„ä¼°å„å˜ä½“çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„æ–¹æ³•æ¥ä¼˜åŒ–LLMsçš„æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¼ºè°ƒäº†æ•°æ®é€‰æ‹©å’Œå¤„ç†çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†å¤šç§å‚æ•°é…ç½®ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨èƒ½åŠ›å’Œå“åº”æ—¶é—´ä¸Šçš„ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„ç­–ç•¥åœ¨èƒ½åŠ›å’Œå“åº”æ—¶é—´ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†çº¦20%-30%ï¼ŒéªŒè¯äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼å’Œå†…å®¹ç”Ÿæˆç­‰ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹æœ‰æ•ˆéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼å’Œå½±å“åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have become a milestone in the field of artificial intelligence and natural language processing. However, their large-scale deployment remains constrained by the need for significant computational resources. This work proposes starting from a base model to explore and combine data processing and careful data selection techniques, training strategies, and architectural adjustments to improve the efficiency of LLMs in resource-constrained environments and within a delimited knowledge base. The methodological approach included defining criteria for building reliable datasets, conducting controlled experiments with different configurations, and systematically evaluating the resulting variants in terms of capability, versatility, response time, and safety. Finally, comparative tests were conducted to measure the performance of the developed variants and to validate the effectiveness of the proposed strategies. This work is based on the master's thesis in Systems and Computer Engineering titled "Efficient Strategy for Improving the Capabilities of Large Language Models (LLMs)".

