---
layout: default
title: P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis
---

# P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04626" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04626v1</a>
  <a href="https://arxiv.org/pdf/2508.04626.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04626v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04626v1', 'P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Feifan Song, Bofei Gao, Yifan Song, Yi Liu, Weimin Xiong, Yuyang Song, Tianyu Liu, Guoyin Wang, Houfeng Wang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºP-Alignerä»¥å®ç°è¯­è¨€æ¨¡å‹çš„é¢„å¯¹é½æŒ‡ä»¤ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `æŒ‡ä»¤ç”Ÿæˆ` `åå¥½å¯¹é½` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `æ•°æ®é›†åˆæˆ` `äººæœºäº¤äº’` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æŒ‡ä»¤ç”Ÿæˆè¿‡ç¨‹ä¸­å­˜åœ¨é«˜æ˜‚çš„è®¡ç®—æˆæœ¬å’Œä¸æ˜ç¡®çš„ç›®æ ‡ï¼Œå¯¼è‡´è¯­è¨€æ¨¡å‹æ— æ³•æœ‰æ•ˆå¯¹é½äººç±»åå¥½ã€‚
2. P-Aligneré€šè¿‡ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„æŒ‡ä»¤ï¼Œæ¥å®ç°æŒ‡ä»¤çš„é¢„å¯¹é½ï¼Œä»è€Œæé«˜è¯­è¨€æ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒP-Aligneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰å¼ºåŸºçº¿ï¼Œæå‡å¹…åº¦è¾¾åˆ°28.35%å’Œ8.69%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸äººç±»ç”¨æˆ·äº’åŠ¨æ—¶ï¼ŒæœŸæœ›èƒ½å¤Ÿç”Ÿæˆå®‰å…¨ã€æœ‰å¸®åŠ©å’Œè¯šå®çš„å†…å®¹ï¼Œä½†åœ¨é¢å¯¹ä¸å®Œå–„çš„æŒ‡ä»¤æ—¶ï¼Œå¸¸å¸¸æ— æ³•å¯¹é½è¿™äº›ä»·å€¼è§‚ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–é«˜æ˜‚çš„æµ‹è¯•æ—¶é—´æœç´¢æˆæœ¬ï¼Œè¦ä¹ˆéœ€è¦å®šåˆ¶è®­ç»ƒè¯­æ–™åº“çš„ç«¯åˆ°ç«¯æ¨¡å‹é‡å†™ï¼Œç›®æ ‡ä¸æ˜ç¡®ã€‚æœ¬æ–‡æå‡ºP-Alignerï¼Œä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œé€šè¿‡ç”Ÿæˆä¿ç•™åŸå§‹æ„å›¾ä½†ä»¥æ›´ç¬¦åˆäººç±»åå¥½çš„å½¢å¼è¡¨è¾¾çš„æŒ‡ä»¤ï¼Œå®ç°é«˜æ•ˆçš„åå¥½å¯¹é½ã€‚P-Aligneråœ¨UltraPromptæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¯¥æ•°æ®é›†é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢çš„åŸåˆ™å¼•å¯¼ç®¡é“åˆæˆï¼Œç³»ç»Ÿæ€§æ¢ç´¢ä¸äººç±»åå¥½ç´§å¯†ç›¸å…³çš„å€™é€‰æŒ‡ä»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒP-Aligneråœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­æ™®éä¼˜äºå¼ºåŸºçº¿ï¼ŒGPT-4-turboå’ŒGemma-2-SimPOçš„å¹³å‡èƒœç‡åˆ†åˆ«æå‡28.35%å’Œ8.69%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶ä¸äººç±»åå¥½ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´é«˜æ˜‚çš„æµ‹è¯•æ—¶é—´æˆæœ¬å’Œä¸æ˜ç¡®çš„è®­ç»ƒç›®æ ‡ï¼Œå¯¼è‡´æŒ‡ä»¤ç”Ÿæˆæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šP-Alignerçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„æŒ‡ä»¤æ¥å®ç°é¢„å¯¹é½ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¾“å‡ºè´¨é‡å’Œå®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¿ç•™åŸå§‹æ„å›¾ï¼ŒåŒæ—¶ä¼˜åŒ–è¡¨è¾¾å½¢å¼ï¼Œè¾¾åˆ°é«˜æ•ˆå¯¹é½çš„ç›®çš„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šP-Alignerçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†UltraPromptçš„æ„å»ºã€æŒ‡ä»¤ç”Ÿæˆæ¨¡å—å’Œåå¥½å¯¹é½æ¨¡å—ã€‚æ•°æ®é›†é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢åˆæˆï¼Œç³»ç»Ÿæ¢ç´¢ä¸äººç±»åå¥½ç›¸å…³çš„æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šP-Alignerçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è½»é‡çº§è®¾è®¡å’ŒåŸºäºåŸåˆ™å¼•å¯¼çš„æŒ‡ä»¤åˆæˆæ–¹æ³•ï¼Œä¸ç°æœ‰ä¾èµ–æ˜‚è´µæœç´¢æˆ–é‡å†™çš„æŠ€æœ¯æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒP-Alignerä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç”ŸæˆæŒ‡ä»¤çš„è´¨é‡ï¼Œå¹¶é€šè¿‡è¿­ä»£éƒ¨ç½²ç­–ç•¥æ¥æé«˜æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒP-Aligneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒGPT-4-turboå’ŒGemma-2-SimPOçš„å¹³å‡èƒœç‡åˆ†åˆ«æå‡28.35%å’Œ8.69%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æŒ‡ä»¤ç”Ÿæˆå’Œåå¥½å¯¹é½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

P-Alignerçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼ã€å†…å®¹ç”Ÿæˆç­‰é¢†åŸŸï¼Œæå‡è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨æ›´å®‰å…¨å’Œé«˜æ•ˆçš„äººæœºäº¤äº’ï¼Œä¿ƒè¿›äººå·¥æ™ºèƒ½çš„æ™®åŠä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are expected to produce safe, helpful, and honest content during interaction with human users, but they frequently fail to align with such values when given flawed instructions, e.g., missing context, ambiguous directives, or inappropriate tone, leaving substantial room for improvement along multiple dimensions. A cost-effective yet high-impact way is to pre-align instructions before the model begins decoding. Existing approaches either rely on prohibitive test-time search costs or end-to-end model rewrite, which is powered by a customized training corpus with unclear objectives. In this work, we demonstrate that the goal of efficient and effective preference alignment can be achieved by P-Aligner, a lightweight module generating instructions that preserve the original intents while being expressed in a more human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree Search, which systematically explores the space of candidate instructions that are closely tied to human preference. Experiments across different methods show that P-Aligner generally outperforms strong baselines across various models and benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness and efficiency through multiple perspectives, including data quality, search strategies, iterative deployment, and time overhead.

