---
layout: default
title: Factor Augmented Supervised Learning with Text Embeddings
---

# Factor Augmented Supervised Learning with Text Embeddings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06548" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06548v1</a>
  <a href="https://arxiv.org/pdf/2508.06548.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06548v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06548v1', 'Factor Augmented Supervised Learning with Text Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhanye Luo, Yuefeng Han, Xiufan Yu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAEALTæ¡†æ¶ä»¥è§£å†³é«˜ç»´æ–‡æœ¬åµŒå…¥çš„æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åµŒå…¥` `è‡ªåŠ¨ç¼–ç å™¨` `ç»´åº¦é™ä½` `ç›‘ç£å­¦ä¹ ` `å¼‚å¸¸æ£€æµ‹` `åˆ†ç±»ä»»åŠ¡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é«˜ç»´æ–‡æœ¬åµŒå…¥æ—¶æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬å¢åŠ ï¼Œå½±å“ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºAEALTæ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å¢å¼ºçš„è‡ªåŠ¨ç¼–ç å™¨ç›´æ¥åœ¨LLMå·¥ä½œæµç¨‹ä¸­å®ç°ç»´åº¦é™ä½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAEALTåœ¨åˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥èƒ½å¤Ÿæ•æ‰è¯è¯­çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡å…³ç³»ï¼Œä½†å…¶é«˜ç»´ç‰¹æ€§å¾€å¾€å¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆç‡ä½ä¸‹å’Œè®¡ç®—æˆæœ¬å¢åŠ ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªåŠ¨ç¼–ç å™¨å¢å¼ºå­¦ä¹ æ¡†æ¶ï¼ˆAEALTï¼‰ï¼Œè¯¥æ¡†æ¶å°†ç»´åº¦é™ä½ç›´æ¥èå…¥é¢„è®­ç»ƒçš„LLMå·¥ä½œæµç¨‹ä¸­ã€‚é¦–å…ˆï¼Œä»æ–‡æœ¬æ–‡æ¡£ä¸­æå–åµŒå…¥ï¼Œç„¶åé€šè¿‡ç›‘ç£å¢å¼ºçš„è‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ ä½ç»´ã€ä»»åŠ¡ç›¸å…³çš„æ½œåœ¨å› å­ã€‚AEALTé€šè¿‡å»ºæ¨¡å¤æ‚åµŒå…¥çš„éçº¿æ€§ç»“æ„ï¼Œè¶…è¶Šäº†ä¾èµ–åŸå§‹åµŒå…¥çš„ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚é€šè¿‡åœ¨å¤šä¸ªçœŸå®å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œåˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹ä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†å…¶å¹¿æ³›é€‚ç”¨æ€§ï¼Œæ•°å€¼ç»“æœè¡¨æ˜AEALTåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸå§‹åµŒå…¥å’Œå‡ ç§æ ‡å‡†çš„ç»´åº¦é™ä½æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é«˜ç»´æ–‡æœ¬åµŒå…¥åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ•ˆç‡ä½ä¸‹å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºåŸå§‹åµŒå…¥ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨å…¶æ½œåœ¨ç»“æ„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAEALTæ¡†æ¶é€šè¿‡å¼•å…¥ç›‘ç£å¢å¼ºçš„è‡ªåŠ¨ç¼–ç å™¨ï¼Œå­¦ä¹ ä½ç»´çš„ä»»åŠ¡ç›¸å…³æ½œåœ¨å› å­ï¼Œä»è€Œæé«˜åµŒå…¥çš„æœ‰æ•ˆæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAEALTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä»æ–‡æœ¬æ–‡æ¡£ä¸­æå–é«˜ç»´åµŒå…¥ï¼›å…¶æ¬¡ï¼Œå°†è¿™äº›åµŒå…¥è¾“å…¥åˆ°ç›‘ç£å¢å¼ºçš„è‡ªåŠ¨ç¼–ç å™¨ä¸­ï¼Œå­¦ä¹ ä½ç»´è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šAEALTçš„åˆ›æ–°åœ¨äºå°†ç»´åº¦é™ä½ä¸é¢„è®­ç»ƒçš„LLMå·¥ä½œæµç¨‹ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆå»ºæ¨¡å¤æ‚åµŒå…¥çš„éçº¿æ€§ç»“æ„ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ä½ç»´è¡¨ç¤ºçš„ä»»åŠ¡ç›¸å…³æ€§ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„æ¥æé«˜æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAEALTåœ¨å¤šä¸ªä»»åŠ¡ä¸Šç›¸æ¯”äºåŸå§‹åµŒå…¥å’Œæ ‡å‡†ç»´åº¦é™ä½æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚ä¾‹å¦‚ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒAEALTçš„å‡†ç¡®ç‡æé«˜äº†15%ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹ä¸­ï¼ŒF1åˆ†æ•°æå‡äº†20%ã€‚è¿™äº›ç»“æœè¡¨æ˜AEALTåœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AEALTæ¡†æ¶åœ¨æ–‡æœ¬åˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹ç­‰å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„ç»´åº¦é™ä½èƒ½åŠ›å¯ä»¥å¸®åŠ©ä¼ä¸šå’Œç ”ç©¶æœºæ„åœ¨å¤„ç†å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®æ—¶é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) generate text embeddings from text data, producing vector representations that capture the semantic meaning and contextual relationships of words. However, the high dimensionality of these embeddings often impedes efficiency and drives up computational cost in downstream tasks. To address this, we propose AutoEncoder-Augmented Learning with Text (AEALT), a supervised, factor-augmented framework that incorporates dimension reduction directly into pre-trained LLM workflows. First, we extract embeddings from text documents; next, we pass them through a supervised augmented autoencoder to learn low-dimensional, task-relevant latent factors. By modeling the nonlinear structure of complex embeddings, AEALT outperforms conventional deep-learning approaches that rely on raw embeddings. We validate its broad applicability with extensive experiments on classification, anomaly detection, and prediction tasks using multiple real-world public datasets. Numerical results demonstrate that AEALT yields substantial gains over both vanilla embeddings and several standard dimension reduction methods.

