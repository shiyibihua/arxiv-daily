---
layout: default
title: Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis
---

# Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10013" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10013v1</a>
  <a href="https://arxiv.org/pdf/2508.10013.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10013v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10013v1', 'Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Linqing Chen, Hanmeng Zhong, Wentao Wu, Weilei Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰æ¡¥æ¥æ¡†æ¶ä»¥è§£å†³å¤šè·³æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè·³æ¨ç†` `é—®ç­”ç”Ÿæˆ` `è¯­ä¹‰å›¾` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯æ§ç”Ÿæˆ` `é¢†åŸŸç‰¹å®šæ•°æ®` `æ¨ç†é“¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–è¡¨é¢æ¨¡å¼ï¼Œæ— æ³•ç”Ÿæˆå¯æ§çš„å¤æ‚å¤šè·³æ¨ç†é—®é¢˜ï¼Œé™åˆ¶äº†LLMè®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚
2. æå‡ºäº†è¯­ä¹‰æ¡¥æ¥æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰å›¾ç¼–ç»‡å®ç°ä»ä»»æ„æ¥æºç”Ÿæˆå¤æ‚æ¨ç†é—®é¢˜ï¼Œå…·å¤‡å¯æ§æ€§ã€‚
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œç”Ÿæˆçš„é—®é¢˜å¯¹åœ¨å¤æ‚æ€§å’Œå¯å›ç­”æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œæå‡å¹…åº¦è¾¾18.3%-25.4%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒé¢ä¸´ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼šé«˜è´¨é‡ã€æ¨ç†å¯†é›†å‹é—®ç­”å¯¹çš„ç¨€ç¼ºï¼Œå°¤å…¶æ˜¯åœ¨ç¨€ç–çš„é¢†åŸŸç‰¹å®šæ¥æºå¦‚PubMedè®ºæ–‡æˆ–æ³•å¾‹æ–‡æ¡£ä¸­ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–è¡¨é¢æ¨¡å¼ï¼Œæœªèƒ½ç”Ÿæˆå¯æ§çš„å¤æ‚å¤šè·³æ¨ç†é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†è¯­ä¹‰æ¡¥æ¥ï¼ˆSemantic Bridgeï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¯æ§ç”Ÿæˆå¤æ‚å¤šè·³æ¨ç†é—®é¢˜çš„é€šç”¨æ¡†æ¶ã€‚æˆ‘ä»¬çš„åˆ›æ–°åœ¨äºè¯­ä¹‰å›¾ç¼–ç»‡ï¼Œé€šè¿‡ä¸‰ç§äº’è¡¥çš„æ¡¥æ¥æœºåˆ¶ï¼ˆå®ä½“æ¡¥æ¥ã€è°“è¯é“¾æ¡¥æ¥å’Œå› æœæ¡¥æ¥ï¼‰ï¼Œç³»ç»Ÿæ„å»ºæ–‡æ¡£é—´çš„å¤æ‚è·¯å¾„ã€‚æˆ‘ä»¬çš„å¤šæ¨¡æ€AMRç®¡é“åœ¨è´¨é‡ä¸Šæå‡äº†9.5%ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆçš„é—®é¢˜å¯¹åœ¨å¤æ‚æ€§ã€å¯å›ç­”æ€§å’Œæ¨¡å¼è¦†ç›–ç‡ä¸Šå‡ä¼˜äºåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­é«˜è´¨é‡æ¨ç†é—®é¢˜å¯¹çš„ç¨€ç¼ºé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºè¡¨é¢æ¨¡å¼ï¼Œæ— æ³•ç”Ÿæˆå¤æ‚çš„å¤šè·³æ¨ç†é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„ç†è§£èƒ½åŠ›å’Œåº”ç”¨åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè¯­ä¹‰æ¡¥æ¥æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰å›¾ç¼–ç»‡æŠ€æœ¯ï¼Œåˆ©ç”¨ä¸‰ç§æ¡¥æ¥æœºåˆ¶ï¼ˆå®ä½“æ¡¥æ¥ã€è°“è¯é“¾æ¡¥æ¥å’Œå› æœæ¡¥æ¥ï¼‰ç³»ç»Ÿæ€§åœ°æ„å»ºæ–‡æ¡£é—´çš„å¤æ‚æ¨ç†è·¯å¾„ï¼Œä»è€Œå®ç°å¯æ§çš„å¤šè·³æ¨ç†é—®é¢˜ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå¤šæ¨¡æ€AMRç®¡é“ï¼Œé¦–å…ˆè¿›è¡Œè¯­ä¹‰åˆ†æï¼Œç„¶åé€šè¿‡ä¸‰ç§æ¡¥æ¥æœºåˆ¶æ„å»ºæ¨ç†è·¯å¾„ï¼Œæœ€åç”Ÿæˆé—®é¢˜å¯¹ã€‚æ¯ä¸ªæ¨¡å—éƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿ç”Ÿæˆé—®é¢˜çš„å¤æ‚æ€§å’Œå¯æ§æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€å¤§çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†è¯­ä¹‰å›¾ç¼–ç»‡çš„æ¦‚å¿µï¼Œé€šè¿‡ä¸‰ç§äº’è¡¥çš„æ¡¥æ¥æœºåˆ¶ï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†ç°æœ‰æ–¹æ³•æ— æ³•ç”Ÿæˆå¤æ‚å¤šè·³æ¨ç†é—®é¢˜çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç”Ÿæˆé—®é¢˜çš„è´¨é‡ï¼Œå¹¶é€šè¿‡è°ƒèŠ‚æ¡¥æ¥æœºåˆ¶çš„å‚æ•°è®¾ç½®ï¼Œå®ç°å¯¹é—®é¢˜å¤æ‚æ€§å’Œç±»å‹çš„ç²¾ç»†æ§åˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆçš„é—®é¢˜å¯¹åœ¨å¤æ‚æ€§ä¸Šæé«˜äº†23.4%ï¼Œå¯å›ç­”æ€§æå‡äº†18.7%ï¼Œæ¨¡å¼è¦†ç›–ç‡æé«˜äº†31.2%ã€‚åœ¨å¤šä¸ªè¯­è¨€ï¼ˆè‹±è¯­ã€ä¸­æ–‡ã€æ³•è¯­ã€å¾·è¯­ï¼‰ä¸Šï¼Œç”Ÿæˆçš„é—®é¢˜å¯¹åœ¨å››ä¸ªè¯­è¨€ä¸Šå‡è¡¨ç°å‡º18.3%-25.4%çš„æ€§èƒ½æå‡ï¼Œä¸”ç”Ÿæˆçš„é—®ç­”å¯¹åœ¨è´¨é‡ä¸Šä¼˜äº600ä¸ªäººå·¥æ ‡æ³¨ç¤ºä¾‹ï¼Œææ–™ä½¿ç”¨é‡å‡å°‘67%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ³•å¾‹å’ŒåŒ»å­¦ç­‰éœ€è¦å¤æ‚æ¨ç†çš„é—®é¢˜ç”Ÿæˆåœºæ™¯ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆæœï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½é—®ç­”ç³»ç»Ÿçš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM) training faces a critical bottleneck: the scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources like PubMed papers or legal documents. Existing methods rely on surface patterns, fundamentally failing to generate controllable, complex multi-hop reasoning questions that test genuine understanding-essential for advancing LLM training paradigms. We present \textbf{Semantic Bridge}, the first universal framework for controllably generating sophisticated multi-hop reasoning questions from arbitrary sources. Our breakthrough innovation is \textit{semantic graph weaving}-three complementary bridging mechanisms (entity bridging for role-varying shared entities, predicate chain bridging for temporal/causal/logical sequences, and causal bridging for explicit reasoning chains)-that systematically construct complex pathways across documents, with fine-grained control over complexity and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to 9.5% better round-trip quality, enabling production-ready controllable QA generation. Extensive evaluation demonstrates performance across both general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It yields consistent 18.3%-25.4% gains over baselines across four languages (English, Chinese, French, German). Question pairs generated from 200 sources outperform 600 native human annotation examples with 67% fewer materials. Human evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM training data synthesis, enabling controllable generation of targeted reasoning questions from sparse sources. We will release our core code and semantic bridge model.

