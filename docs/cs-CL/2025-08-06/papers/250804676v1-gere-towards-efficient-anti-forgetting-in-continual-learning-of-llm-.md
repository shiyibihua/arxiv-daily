---
layout: default
title: GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay
---

# GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04676" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04676v1</a>
  <a href="https://arxiv.org/pdf/2508.04676.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04676v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04676v1', 'GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunan Zhang, Shuoran Jiang, Mengchen Zhao, Yuefeng Li, Yang Fan, Xiangping Wu, Qingcai Chen

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Qznan/GeRe)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGeReæ¡†æ¶ä»¥é«˜æ•ˆè§£å†³å¤§è¯­è¨€æ¨¡å‹çš„é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `æŠ—é—å¿˜` `æ ·æœ¬é‡æ”¾` `æ¿€æ´»çŠ¶æ€ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æŒç»­å­¦ä¹ ä¸­é¢ä¸´ç¾éš¾æ€§é—å¿˜ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ–°ä»»åŠ¡å­¦ä¹ æ—¶æ˜¾è‘—ä¸§å¤±å…ˆå‰ä»»åŠ¡çš„èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºçš„GeReæ¡†æ¶é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒæ–‡æœ¬è¿›è¡Œé€šç”¨æ ·æœ¬é‡æ”¾ï¼Œè§£å†³äº†é—å¿˜é—®é¢˜å¹¶ä¿æŒæ¿€æ´»çŠ¶æ€ä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨é˜ˆå€¼è¾¹é™…ï¼ˆTMï¼‰æŸå¤±çš„é‡æ”¾ç­–ç•¥åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸”é²æ£’æ€§æ›´å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æŒç»­å­¦ä¹ èƒ½åŠ›å¯¹äºæ¨åŠ¨äººå·¥é€šç”¨æ™ºèƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨ä¸åŒé¢†åŸŸå¯¹LLMsè¿›è¡ŒæŒç»­å¾®è°ƒæ—¶ï¼Œå¸¸å¸¸ä¼šé­é‡ç¾éš¾æ€§é—å¿˜ï¼Œè¡¨ç°ä¸ºï¼š1ï¼‰æ˜¾è‘—é—å¿˜å…¶ä¸€èˆ¬èƒ½åŠ›ï¼Œ2ï¼‰åœ¨å…ˆå‰å­¦ä¹ ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚ä¸ºåŒæ—¶ä»¥ç®€å•è€Œç¨³å®šçš„æ–¹å¼è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€šç”¨æ ·æœ¬é‡æ”¾ï¼ˆGeReï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¸¸è§çš„é¢„è®­ç»ƒæ–‡æœ¬å®ç°é«˜æ•ˆçš„æŠ—é—å¿˜ã€‚æˆ‘ä»¬é¦–æ¬¡éªŒè¯äº†ä¸€å°ç»„å›ºå®šçš„é¢„æ”¶é›†é€šç”¨é‡æ”¾æ ·æœ¬è¶³ä»¥è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜â€”â€”ä¿ç•™ä¸€èˆ¬èƒ½åŠ›ï¼ŒåŒæ—¶æå‡é¡ºåºä»»åŠ¡çš„æ•´ä½“æ€§èƒ½ã€‚é€šè¿‡æ§åˆ¶å®éªŒï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†TMä¸GeReæ¡†æ¶ä¸‹ä¸åŒé‡æ”¾ç­–ç•¥çš„è¡¨ç°ï¼Œç»“æœè¡¨æ˜TMåœ¨æ€§èƒ½å’Œé²æ£’æ€§ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„å…·ä½“é—®é¢˜æ˜¯å¤§è¯­è¨€æ¨¡å‹åœ¨æŒç»­å­¦ä¹ ä¸­é­é‡çš„ç¾éš¾æ€§é—å¿˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†é—å¿˜æ—¶å¾€å¾€æ— æ³•å…¼é¡¾ä¿ç•™ä¸€èˆ¬èƒ½åŠ›ä¸æå‡æ–°ä»»åŠ¡æ€§èƒ½çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯å¼•å…¥é€šç”¨æ ·æœ¬é‡æ”¾ï¼ˆGeReï¼‰ï¼Œåˆ©ç”¨å¸¸è§çš„é¢„è®­ç»ƒæ–‡æœ¬è¿›è¡Œé‡æ”¾å­¦ä¹ ï¼Œå¹¶é€šè¿‡é˜ˆå€¼è¾¹é™…ï¼ˆTMï¼‰æŸå¤±ä¿æŒæ¿€æ´»çŠ¶æ€çš„ä¸€è‡´æ€§ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘é—å¿˜ç°è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ ·æœ¬é€‰æ‹©ã€é‡æ”¾å­¦ä¹ å’Œæ¿€æ´»çŠ¶æ€ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆé€‰æ‹©å›ºå®šçš„é€šç”¨æ ·æœ¬è¿›è¡Œé‡æ”¾ï¼Œç„¶ååœ¨é‡æ”¾è¿‡ç¨‹ä¸­åº”ç”¨TMæŸå¤±è¿›è¡Œä¼˜åŒ–ï¼Œæœ€åç¡®ä¿æ¿€æ´»çŠ¶æ€çš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡éªŒè¯äº†å°è§„æ¨¡çš„å›ºå®šé€šç”¨é‡æ”¾æ ·æœ¬è¶³ä»¥è§£å†³é—å¿˜é—®é¢˜ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•ä¾èµ–å¤§é‡æ ·æœ¬çš„åšæ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é˜ˆå€¼è¾¹é™…æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¿€æ´»çŠ¶æ€ï¼Œå¹¶åœ¨é‡æ”¾è¿‡ç¨‹ä¸­ä½¿ç”¨L1/L2æŸå¤±è¿›è¡Œç‰¹å¾æ¨¡ä»¿ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ–°ä»»åŠ¡å­¦ä¹ æ—¶ä¸é—å¿˜æ—§ä»»åŠ¡çš„çŸ¥è¯†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨é˜ˆå€¼è¾¹é™…ï¼ˆTMï¼‰æŸå¤±çš„GeReæ¡†æ¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„æ ‡ç­¾æ‹Ÿåˆå’Œç‰¹å¾æ¨¡ä»¿ç­–ç•¥ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œä¸”åœ¨é²æ£’æ€§ä¸Šè¡¨ç°æ›´ä½³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„è¡¨ç°ï¼Œå‡å°‘é—å¿˜ç°è±¡ï¼Œè¿›è€Œæ¨åŠ¨äººå·¥æ™ºèƒ½çš„æ™ºèƒ½åŒ–å‘å±•ã€‚æœªæ¥ï¼ŒGeReæ¡†æ¶æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„é¢†åŸŸä¸­åº”ç”¨ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹çš„æŒç»­å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns--retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1/L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at https://github.com/Qznan/GeRe.

