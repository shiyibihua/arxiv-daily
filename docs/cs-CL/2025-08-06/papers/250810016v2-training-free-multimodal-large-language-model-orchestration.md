---
layout: default
title: Training-Free Multimodal Large Language Model Orchestration
---

# Training-Free Multimodal Large Language Model Orchestration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10016" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10016v2</a>
  <a href="https://arxiv.org/pdf/2508.10016.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10016v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10016v2', 'Training-Free Multimodal Large Language Model Orchestration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyu Xie, Yuhang Wu, Yongdong Luo, Jiayi Ji, Xiawu Zheng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06 (æ›´æ–°: 2025-08-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç¼–æ’ä»¥è§£å†³æ¨¡å‹é›†æˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹ç¼–æ’` `æ— è®­ç»ƒé›†æˆ` `æ™ºèƒ½äº¤äº’` `å¯è§£é‡Šæ€§` `æ–‡æœ¬åˆ°è¯­éŸ³` `è·¨æ¨¡æ€è®°å¿†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ— æ³•ç›´æ¥é›†æˆï¼Œè®­ç»ƒè¿‡ç¨‹å¤æ‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚
2. æå‡ºçš„MLLMç¼–æ’æ–¹æ³•é€šè¿‡ä¸­å¿ƒæ§åˆ¶å™¨å’Œæ™ºèƒ½ä¿¡æ¯æ•´åˆå®ç°æ— è®­ç»ƒé›†æˆï¼Œæå‡äº¤äº’æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½å’Œå“åº”é€Ÿåº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸”å¯è§£é‡Šæ€§å¢å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸åŒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ— æ³•ç›´æ¥é›†æˆæˆç»Ÿä¸€çš„å¤šæ¨¡æ€è¾“å…¥è¾“å‡ºç³»ç»Ÿã€‚ä»¥å¾€çš„ç ”ç©¶è®¤ä¸ºè®­ç»ƒæ˜¯ä¸å¯é¿å…çš„ç»„æˆéƒ¨åˆ†ï¼Œå› æ¨¡æ€å¯¹é½ã€æ–‡æœ¬åˆ°è¯­éŸ³æ•ˆç‡ç­‰é›†æˆé—®é¢˜è€Œé¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç¼–æ’ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒå³å¯åˆ›å»ºäº¤äº’å¼å¤šæ¨¡æ€AIç³»ç»Ÿçš„æœ‰æ•ˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡æ˜ç¡®çš„å·¥ä½œæµç¨‹åè°ƒä¸“é—¨æ¨¡å‹ï¼Œå®ç°è‡ªç„¶çš„å¤šæ¨¡æ€äº¤äº’ï¼ŒåŒæ—¶ä¿æŒæ¨¡å—åŒ–ã€æé«˜å¯è§£é‡Šæ€§ï¼Œå¹¶æ˜¾è‘—å¢å¼ºè®¡ç®—æ•ˆç‡ã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†ä¸Šç›¸è¾ƒäºä¼ ç»Ÿè”åˆè®­ç»ƒæ–¹æ³•æ€§èƒ½æå‡é«˜è¾¾7.8%ï¼Œå»¶è¿Ÿé™ä½10.3%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸åŒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ— æ³•ç›´æ¥é›†æˆçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ¨¡æ€å¯¹é½å’Œæ•ˆç‡ä¸Šå­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¯¼è‡´æ— æ³•å®ç°è‡ªç„¶çš„å¤šæ¨¡æ€äº¤äº’ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„MLLMç¼–æ’æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ä¸­å¿ƒæ§åˆ¶å™¨åŠ¨æ€è·¯ç”±ä»»åŠ¡åˆ°ä¸“é—¨æ¨¡å‹ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¿…é¡»è¿›è¡Œé¢å¤–è®­ç»ƒçš„å±€é™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä¸­å¿ƒæ§åˆ¶å™¨LLMã€å¹¶è¡Œæ–‡æœ¬åˆ°è¯­éŸ³æ¶æ„ä»¥åŠè·¨æ¨¡æ€è®°å¿†æ•´åˆç³»ç»Ÿï¼Œç¡®ä¿åœ¨å¤šæ¨¡æ€äº¤äº’ä¸­ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ä¸­å¿ƒæ§åˆ¶å™¨å’Œæ™ºèƒ½ä¿¡æ¯æ•´åˆå®ç°æ— è®­ç»ƒçš„å¤šæ¨¡æ€äº¤äº’ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®¾è®¡ä¸­é‡‡ç”¨äº†ç²¾å¿ƒè®¾è®¡çš„ä»£ç†æ¥åˆ†æç”¨æˆ·è¾“å…¥ï¼Œå¹¶åŠ¨æ€è·¯ç”±ä»»åŠ¡ï¼ŒåŒæ—¶å¹¶è¡Œæ–‡æœ¬åˆ°è¯­éŸ³æ¶æ„æ”¯æŒå…¨åŒå·¥äº¤äº’ï¼Œè·¨æ¨¡æ€è®°å¿†ç³»ç»Ÿåˆ™é€šè¿‡æ™ºèƒ½ä¿¡æ¯åˆæˆå’Œæ£€ç´¢æ¥ç»´æŠ¤ä¸Šä¸‹æ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMLLMç¼–æ’æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†ä¸Šç›¸è¾ƒäºä¼ ç»Ÿè”åˆè®­ç»ƒæ–¹æ³•æ€§èƒ½æå‡é«˜è¾¾7.8%ï¼Œå»¶è¿Ÿé™ä½10.3%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ˜ç¡®çš„ç¼–æ’è¿‡ç¨‹æ˜¾è‘—å¢å¼ºäº†å¯è§£é‡Šæ€§ï¼Œæå‡äº†ç”¨æˆ·äº¤äº’ä½“éªŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼ã€åŒ»ç–—å’¨è¯¢ç­‰å¤šæ¨¡æ€äº¤äº’åœºæ™¯ã€‚é€šè¿‡æ— è®­ç»ƒçš„é›†æˆæ–¹å¼ï¼Œå¯ä»¥å¿«é€Ÿéƒ¨ç½²å¤šæ¨¡æ€AIç³»ç»Ÿï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’æ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Different Multimodal Large Language Models (MLLMs) cannot be integrated into a unified multimodal input-output system directly. In previous work, training has been considered as an inevitable component due to challenges in modal alignment, Text-to-Speech efficiency and other integration issues. In this paper, we introduce Multimodal Large Language Model Orchestration, an effective approach for creating interactive multimodal AI systems without additional training. MLLM Orchestration leverages the inherent reasoning capabilities of large language models to coordinate specialized models through explicit workflows, enabling natural multimodal interactions while maintaining modularity, improving interpretability, and significantly enhancing computational efficiency. Our orchestration framework is built upon three key innovations: (1) a central controller LLM that analyzes user inputs and dynamically routes tasks to appropriate specialized models through carefully designed agents; (2) a parallel Text-to-Speech architecture that enables true full-duplex interaction with seamless interruption handling and natural conversational flow; and (3) a cross-modal memory integration system that maintains coherent context across modalities through intelligent information synthesis and retrieval, selectively avoiding unnecessary modality calls in certain scenarios to improve response speed. Extensive evaluations demonstrate that MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.

