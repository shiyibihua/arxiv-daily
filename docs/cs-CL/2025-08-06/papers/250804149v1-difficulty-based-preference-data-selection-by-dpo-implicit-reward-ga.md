---
layout: default
title: Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap
---

# Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04149" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04149v1</a>
  <a href="https://arxiv.org/pdf/2508.04149.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04149v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04149v1', 'Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuan Qi, Rongwu Xu, Zhijing Jin

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: Our code and data are available at https://github.com/Difficulty-Based-Preference-Data-Select/Difficulty-Based-Preference-Data-Select

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºéš¾åº¦çš„æ•°æ®é€‰æ‹©ç­–ç•¥ä»¥æå‡åå¥½æ•°æ®æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åå¥½æ•°æ®é€‰æ‹©` `DPOéšå¼å¥–åŠ±` `æ•°æ®æ•ˆç‡` `æ¨¡å‹å¯¹é½` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½æ—¶ï¼Œé€šå¸¸ä¾èµ–äºå¤§é‡æ˜‚è´µçš„åå¥½æ•°æ®é›†ï¼Œç¼ºä¹é«˜æ•ˆçš„æ•°æ®é€‰æ‹©ç­–ç•¥ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºDPOéšå¼å¥–åŠ±æœºåˆ¶çš„éš¾åº¦æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œé€šè¿‡é€‰æ‹©éš¾åº¦æ›´é«˜çš„åå¥½æ•°æ®ç¤ºä¾‹æ¥æé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’Œå¯¹é½ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä»…ä½¿ç”¨10%çš„åŸå§‹æ•°æ®ä¾¿æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½æ˜¯äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­çš„ä¸€é¡¹é‡è¦æŒ‘æˆ˜ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰æ–¹æ³•è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤§é‡æ˜‚è´µçš„åå¥½æ•°æ®é›†ã€‚ç›®å‰ç¼ºä¹ä¸“é—¨é’ˆå¯¹åå¥½æ•°æ®çš„é«˜è´¨é‡æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºéš¾åº¦çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼ŒåŸºäºDPOéšå¼å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡é€‰æ‹©å…·æœ‰è¾ƒå°DPOéšå¼å¥–åŠ±å·®è·çš„åå¥½æ•°æ®ç¤ºä¾‹ï¼Œä»è€Œæé«˜æ•°æ®æ•ˆç‡å’Œæ¨¡å‹å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨å¤šä¸ªæ•°æ®é›†å’Œå¯¹é½ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºäº”ä¸ªå¼ºåŸºçº¿ï¼Œä»…ä½¿ç”¨åŸå§‹æ•°æ®çš„10%ä¾¿å®ç°äº†æ›´ä¼˜çš„æ€§èƒ½ã€‚è¿™ç§åŸåˆ™æ€§ã€æœ‰æ•ˆçš„æ•°æ®é€‰æ‹©æ–¹æ³•ä¸ºåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹æ‰©å±•LLMå¯¹é½æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åå¥½æ•°æ®é€‰æ‹©æ–¹æ³•åœ¨æ•°æ®æ•ˆç‡å’Œè´¨é‡ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡æ˜‚è´µçš„åå¥½æ•°æ®é›†ï¼Œç¼ºä¹æœ‰æ•ˆçš„é€‰æ‹©æœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯åŸºäºDPOéšå¼å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡é€‰æ‹©å…·æœ‰è¾ƒå°å¥–åŠ±å·®è·çš„åå¥½æ•°æ®ç¤ºä¾‹ï¼Œæ¥èšç„¦äºæ›´å…·æŒ‘æˆ˜æ€§çš„æ¡ˆä¾‹ï¼Œä»è€Œæé«˜æ•°æ®çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹çš„å¯¹é½ç¨‹åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é€‰æ‹©æ¨¡å—å’Œæ¨¡å‹è®­ç»ƒæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è®¡ç®—æ¯ä¸ªæ•°æ®ç¤ºä¾‹çš„DPOéšå¼å¥–åŠ±ï¼Œç­›é€‰å‡ºå¥–åŠ±å·®è·è¾ƒå°çš„ç¤ºä¾‹ï¼Œç„¶åå°†è¿™äº›ç¤ºä¾‹ç”¨äºæ¨¡å‹çš„è®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åŸºäºéš¾åº¦çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œåˆ©ç”¨DPOéšå¼å¥–åŠ±æœºåˆ¶æ¥è¯†åˆ«å’Œé€‰æ‹©æ›´å…·æŒ‘æˆ˜æ€§çš„åå¥½æ•°æ®ç¤ºä¾‹ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºéšæœºæˆ–ç®€å•é€‰æ‹©çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé€‰æ‹©äº†é€‚å½“çš„é˜ˆå€¼æ¥å®šä¹‰â€œéš¾åº¦â€ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹çš„å¯¹é½æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ä¸Šé‡‡ç”¨äº†é€‚åº”æ€§è°ƒæ•´æœºåˆ¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¤„ç†ä¸åŒéš¾åº¦çš„æ•°æ®ç¤ºä¾‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’Œå¯¹é½ä»»åŠ¡ä¸­å‡ä¼˜äºäº”ä¸ªå¼ºåŸºçº¿ï¼Œä½¿ç”¨ä»…10%çš„åŸå§‹æ•°æ®ä¾¿å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ•°æ®é€‰æ‹©æ•ˆç‡å’Œæ¨¡å‹å¯¹é½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜åå¥½æ•°æ®çš„é€‰æ‹©æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹æ›´å¥½åœ°å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½ï¼Œä»è€Œæå‡æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½ç³»ç»Ÿä¸äººç±»éœ€æ±‚çš„æ›´å¥½å¯¹æ¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Aligning large language models (LLMs) with human preferences is a critical challenge in AI research. While methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they often rely on large, costly preference datasets. The current work lacks methods for high-quality data selection specifically for preference data. In this work, we introduce a novel difficulty-based data selection strategy for preference datasets, grounded in the DPO implicit reward mechanism. By selecting preference data examples with smaller DPO implicit reward gaps, which are indicative of more challenging cases, we improve data efficiency and model alignment. Our approach consistently outperforms five strong baselines across multiple datasets and alignment tasks, achieving superior performance with only 10\% of the original data. This principled, efficient selection method offers a promising solution for scaling LLM alignment with limited resources.

