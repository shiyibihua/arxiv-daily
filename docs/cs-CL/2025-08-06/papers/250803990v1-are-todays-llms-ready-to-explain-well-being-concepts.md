---
layout: default
title: Are Today's LLMs Ready to Explain Well-Being Concepts?
---

# Are Today's LLMs Ready to Explain Well-Being Concepts?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03990" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03990v1</a>
  <a href="https://arxiv.org/pdf/2508.03990.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03990v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03990v1', 'Are Today\'s LLMs Ready to Explain Well-Being Concepts?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bohan Jiang, Dawei Li, Zhen Tan, Chengshuai Zhao, Huan Liu

**åˆ†ç±»**: cs.CL, cs.AI, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: 9 pages, 4 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLLMçš„è¯„ä¼°æ¡†æ¶ä»¥æå‡å¹¸ç¦æ„Ÿæ¦‚å¿µè§£é‡Šè´¨é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¹¸ç¦æ„Ÿ` `è§£é‡Šç”Ÿæˆ` `å¾®è°ƒ` `è¯„ä¼°æ¡†æ¶` `åå¥½å­¦ä¹ ` `å¿ƒç†å¥åº·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰LLMsåœ¨ç”Ÿæˆå¹¸ç¦æ„Ÿæ¦‚å¿µè§£é‡Šæ—¶ï¼Œé¢ä¸´å‡†ç¡®æ€§å’Œå—ä¼—é€‚åº”æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºåŸåˆ™çš„LLMè¯„ä¼°æ¡†æ¶ï¼Œå¹¶é€šè¿‡åŒé‡è¯„ä¼°è€…æ¥è¯„ä¼°è§£é‡Šè´¨é‡ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå¾®è°ƒåçš„æ¨¡å‹åœ¨è§£é‡Šè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºæœªå¾®è°ƒçš„æ¨¡å‹ï¼Œä¸”ä¸äººç±»è¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¹¸ç¦æ„Ÿæ¶µç›–å¿ƒç†ã€èº«ä½“å’Œç¤¾ä¼šç­‰å¤šä¸ªç»´åº¦ï¼Œå¯¹ä¸ªäººæˆé•¿å’Œæ˜æ™ºç”Ÿæ´»å†³ç­–è‡³å…³é‡è¦ã€‚éšç€äººä»¬è¶Šæ¥è¶Šå¤šåœ°å’¨è¯¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥ç†è§£å¹¸ç¦æ„Ÿï¼Œå…³é”®æŒ‘æˆ˜åœ¨äºLLMsèƒ½å¦ç”Ÿæˆæ—¢å‡†ç¡®åˆé€‚åˆä¸åŒå—ä¼—çš„è§£é‡Šã€‚é«˜è´¨é‡çš„è§£é‡Šéœ€è¦äº‹å®æ­£ç¡®æ€§å’Œæ»¡è¶³ä¸åŒä¸“ä¸šæ°´å¹³ç”¨æˆ·æœŸæœ›çš„èƒ½åŠ›ã€‚æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«43,880ä¸ªå¹¸ç¦æ„Ÿæ¦‚å¿µè§£é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†åŸºäºåŸåˆ™çš„LLMè¯„ä¼°æ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡è¯„ä¼°è€…æ¥è¯„ä¼°è§£é‡Šè´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹å¼€æºLLMè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”Ÿæˆè§£é‡Šçš„è´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå¹¸ç¦æ„Ÿæ¦‚å¿µè§£é‡Šæ—¶ï¼Œå­˜åœ¨çš„å‡†ç¡®æ€§å’Œå—ä¼—é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚ï¼Œå¯¼è‡´è§£é‡Šè´¨é‡ä¸å‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„å¹¸ç¦æ„Ÿæ¦‚å¿µè§£é‡Šæ•°æ®é›†ï¼Œå¹¶å¼•å…¥åŸºäºåŸåˆ™çš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨åŒé‡è¯„ä¼°è€…æ¥æå‡è§£é‡Šçš„è´¨é‡å’Œé€‚åº”æ€§ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†ç¡®ä¿ç”Ÿæˆçš„è§£é‡Šä¸ä»…å‡†ç¡®ï¼Œè€Œä¸”èƒ½å¤Ÿæ»¡è¶³ä¸åŒä¸“ä¸šæ°´å¹³ç”¨æˆ·çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ”¶é›†å¹¶ç”Ÿæˆå¹¸ç¦æ„Ÿæ¦‚å¿µçš„è§£é‡Šæ•°æ®ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–å¯¹LLMè¿›è¡Œè®­ç»ƒï¼›æœ€åï¼Œé‡‡ç”¨åŒé‡è¯„ä¼°è€…å¯¹ç”Ÿæˆçš„è§£é‡Šè¿›è¡Œè´¨é‡è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åŸºäºåŸåˆ™çš„LLMè¯„ä¼°æ¡†æ¶å’ŒåŒé‡è¯„ä¼°è€…æœºåˆ¶ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€è¯„ä¼°è€…æ¨¡å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°è§£é‡Šè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„æ–¹æ³•ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·åå¥½ç”Ÿæˆæ›´é«˜è´¨é‡çš„è§£é‡Šã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºDPOå’ŒSFTå¾®è°ƒçš„æ¨¡å‹åœ¨è§£é‡Šè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºæœªå¾®è°ƒçš„æ¨¡å‹ï¼Œä¸”ä¸äººç±»è¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ï¼Œè¡¨æ˜åå¥½å­¦ä¹ åœ¨ä¸“ä¸šè§£é‡Šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œå¾®è°ƒæ¨¡å‹åœ¨å¤šä¸ªå—ä¼—å’Œç±»åˆ«ä¸‹çš„è¡¨ç°å‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¿ƒç†å¥åº·å’¨è¯¢ã€æ•™è‚²å’Œä¸ªäººå‘å±•ç­‰ã€‚é€šè¿‡æå‡LLMsåœ¨å¹¸ç¦æ„Ÿæ¦‚å¿µè§£é‡Šä¸Šçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨ç›¸å…³çŸ¥è¯†ï¼Œä»è€Œä¿ƒè¿›ä¸ªäººçš„å¿ƒç†å’Œç¤¾ä¼šç¦ç¥‰ã€‚æœªæ¥ï¼Œè¿™ä¸€ç ”ç©¶å¯èƒ½å¯¹äººæœºäº¤äº’å’Œæ™ºèƒ½åŠ©æ‰‹çš„è®¾è®¡äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks.

