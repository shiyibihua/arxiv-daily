---
layout: default
title: GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy
---

# GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04349" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04349v5</a>
  <a href="https://arxiv.org/pdf/2508.04349.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04349v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04349v5', 'GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongze Tan, Jianfei Pan, Jinghao Lin, Tao Chen, Zhihang Zheng, Zhihao Tang, Haihua Yang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06 (æ›´æ–°: 2025-09-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€ç†µåŠ æƒæœºåˆ¶ä»¥è§£å†³é•¿é“¾æ¨ç†ä¸­çš„å¥–åŠ±åˆ†é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€ç†µåŠ æƒ` `é•¿é“¾æ¨ç†` `å¥–åŠ±å¡‘é€ ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨ç²—ç²’åº¦å¥–åŠ±åˆ†é…çš„é—®é¢˜ï¼Œæ— æ³•æœ‰æ•ˆåŒºåˆ†å„ä¸ªæ ‡è®°çš„è´¡çŒ®ã€‚
2. æœ¬æ–‡æå‡ºåŠ¨æ€ç†µåŠ æƒæœºåˆ¶ï¼Œé€šè¿‡GTPOå’ŒGRPO-Sç®—æ³•å®ç°å¯¹æ¯ä¸ªæ ‡è®°çš„ç†µåŠ æƒå¥–åŠ±ï¼Œæå‡å¥–åŠ±åˆ†é…çš„ç²¾ç»†åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶ŠDAPOåŸºçº¿ï¼ŒéªŒè¯äº†ç†µåŠ æƒæœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿç®—æ³•é€šå¸¸é‡‡ç”¨ç²—ç²’åº¦çš„å¥–åŠ±åˆ†é…æ–¹å¼ï¼Œå¯¹åºåˆ—ä¸­çš„æ‰€æœ‰æ ‡è®°æ–½åŠ ç»Ÿä¸€å¥–åŠ±ï¼Œè¿™åœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚æœ¬æ–‡æå‡ºåŠ¨æ€ç†µåŠ æƒæœºåˆ¶ï¼Œé€šè¿‡ä¸¤ç§æ–°ç®—æ³•ï¼šç»„æ ‡è®°ç­–ç•¥ä¼˜åŒ–ï¼ˆGTPOï¼‰å’Œåºåˆ—çº§GRPOï¼ˆGRPO-Sï¼‰ï¼Œå®ç°å¯¹æ¯ä¸ªæ ‡è®°çš„ç»†ç²’åº¦å¥–åŠ±åˆ†é…ã€‚æˆ‘ä»¬å‡è®¾æ¨ç†è·¯å¾„ä¸­çš„é«˜ç­–ç•¥ç†µæ˜¯è®¤çŸ¥åŠªåŠ›çš„é‡è¦å¯ç¤ºï¼Œå¯ä»¥è½¬åŒ–ä¸ºå­¦ä¹ ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿DAPOï¼ŒéªŒè¯äº†ç†µåŠ æƒæœºåˆ¶æ˜¯æ€§èƒ½æå‡çš„å…³é”®é©±åŠ¨å› ç´ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ä¸­å¯¹å¥–åŠ±çš„ç²—ç²’åº¦åˆ†é…é—®é¢˜ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®è¯„ä¼°æ¯ä¸ªæ ‡è®°çš„è´¡çŒ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŠ¨æ€ç†µåŠ æƒæœºåˆ¶ï¼Œé€šè¿‡å¯¹æ¯ä¸ªæ ‡è®°åˆ†é…åŸºäºç†µçš„å¥–åŠ±ï¼Œåˆ©ç”¨é«˜ç­–ç•¥ç†µä½œä¸ºè®¤çŸ¥åŠªåŠ›çš„å¯ç¤ºï¼Œè½¬åŒ–ä¸ºæœ‰æ•ˆçš„å­¦ä¹ ä¿¡å·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šGTPOå’ŒGRPO-Sã€‚GTPOä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…ç†µåŠ æƒå¥–åŠ±ï¼Œè€ŒGRPO-Såˆ™åœ¨åºåˆ—çº§åˆ«è¿›è¡Œç±»ä¼¼çš„å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ç­–ç•¥ç†µç”¨äºå¥–åŠ±å¡‘é€ ï¼Œå®ç°çœŸæ­£çš„é€æ ‡è®°ä¿¡ç”¨åˆ†é…ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•çš„ç»Ÿä¸€å¥–åŠ±æœºåˆ¶å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ç†µæƒé‡çš„è®¡ç®—æ–¹å¼å’Œå¥–åŠ±å‡½æ•°çš„æ„å»ºï¼Œç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆåæ˜ æ¯ä¸ªæ ‡è®°åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æGTPOå’ŒGRPO-Sæ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºDAPOåŸºçº¿ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼ŒéªŒè¯äº†ç†µåŠ æƒæœºåˆ¶åœ¨å¥–åŠ±åˆ†é…ä¸­çš„å…³é”®ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½é—®ç­”ç­‰ã€‚é€šè¿‡æå‡é•¿é“¾æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) is a pivotal task for enhancing Large Language Model (LLM) reasoning. Conventional algorithms, however, typically adhere to a coarse-grained credit assignment paradigm, applying a uniform reward to all tokens in a sequence, a critical flaw in long-chain reasoning tasks. In this paper, we address this challenge and propose Dynamic Entropy Weighting, a novel mechanism that facilitates fine-grained rewards through two new algorithms: Group Token Policy Optimization (GTPO), which assigns an entropy-weighted reward to each token, and the analogous algorithm Sequence-Level GRPO (GRPO-S). Our approach is founded on the hypothesis that high policy entropy within a reasoning path is a powerful heuristic for cognitive effort at pivotal junctures, which can be repurposed into a learning signal. By repurposing policy entropy for reward shaping, we achieve true per-token credit assignment. Experimental results across challenging reasoning benchmarks validate the superiority of our approach, showing our methods significantly outperform a strong DAPO baseline and confirming our entropy-weighting mechanism as the key driver of this performance boost.

