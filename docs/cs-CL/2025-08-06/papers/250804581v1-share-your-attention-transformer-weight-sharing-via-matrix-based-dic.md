---
layout: default
title: Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning
---

# Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04581" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04581v1</a>
  <a href="https://arxiv.org/pdf/2508.04581.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04581v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04581v1', 'Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMASAæ¡†æ¶ä»¥è§£å†³å˜æ¢å™¨å±‚é—´å†—ä½™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å˜æ¢å™¨å‹ç¼©` `æƒé‡å…±äº«` `å­—å…¸å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰å˜æ¢å™¨` `å‚æ•°æ•ˆç‡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‹ç¼©æŠ€æœ¯ä¸»è¦é›†ä¸­åœ¨å˜æ¢å™¨å†…éƒ¨å—çš„ä¼˜åŒ–ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨å±‚é—´çš„å†—ä½™æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„MASAæ¡†æ¶é€šè¿‡ç»“æ„åŒ–æƒé‡å…±äº«ï¼Œå‡å°‘äº†å˜æ¢å™¨å±‚ä¹‹é—´çš„å‚æ•°å†—ä½™ï¼Œæå‡äº†æ¨¡å‹æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMASAåœ¨å¤šä¸ªå‚æ•°è§„æ¨¡ä¸‹çš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ä½ç§©åŸºçº¿å’Œåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æ–¹æ³•ï¼Œä¸”å‚æ•°å‡å°‘è¾¾66.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½åº”ç”¨ä¸­å–å¾—äº†é©å‘½æ€§è¿›å±•ï¼Œä½†å…¶é«˜è®¡ç®—å’Œå†…å­˜éœ€æ±‚é™åˆ¶äº†å…¶å¹¿æ³›éƒ¨ç½²ã€‚ç°æœ‰çš„å‹ç¼©æŠ€æœ¯ä¸»è¦é›†ä¸­åœ¨å†…éƒ¨å—ä¼˜åŒ–ä¸Šï¼Œè€Œå˜æ¢å™¨çš„é‡å¤å±‚ç»“æ„æš—ç¤ºäº†æ˜¾è‘—çš„å±‚é—´å†—ä½™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºçŸ©é˜µå­—å…¸å­¦ä¹ çš„ç»“æ„åŒ–æƒé‡å…±äº«æ¡†æ¶MASAï¼Œé€šè¿‡å°†æ³¨æ„åŠ›æŠ•å½±çŸ©é˜µåˆ†è§£ä¸ºå…±äº«å­—å…¸åŸå­ï¼Œå‡å°‘äº†66.7%çš„å‚æ•°ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚MASAä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿä¸æ ‡å‡†ä¼˜åŒ–å™¨ä¸€èµ·è®­ç»ƒï¼Œå¹¶å°†æ¯å±‚çš„æƒé‡è¡¨ç¤ºä¸ºå…±äº«çŸ©é˜µåŸå­çš„çº¿æ€§ç»„åˆã€‚å®éªŒè¡¨æ˜ï¼ŒMASAåœ¨å¤šä¸ªå‚æ•°è§„æ¨¡ä¸‹çš„åŸºå‡†å‡†ç¡®æ€§å’Œå›°æƒ‘åº¦ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸”åœ¨è§†è§‰å˜æ¢å™¨ä¸­åŒæ ·è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å˜æ¢å™¨å±‚é—´çš„å†—ä½™é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å±‚é—´çš„ç›¸ä¼¼æ€§ï¼Œå¯¼è‡´å‚æ•°ä½¿ç”¨æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMASAæ¡†æ¶é€šè¿‡å°†æ³¨æ„åŠ›æŠ•å½±çŸ©é˜µåˆ†è§£ä¸ºå…±äº«çš„å­—å…¸åŸå­ï¼Œå…è®¸ä¸åŒå±‚å…±äº«ç›¸åŒçš„å‚æ•°ï¼Œä»è€Œå‡å°‘å†—ä½™å¹¶æé«˜æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMASAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) å­—å…¸å­¦ä¹ æ¨¡å—ï¼Œç”¨äºç”Ÿæˆå…±äº«çš„çŸ©é˜µåŸå­ï¼›2) æƒé‡è¡¨ç¤ºæ¨¡å—ï¼Œå°†æ¯å±‚çš„æƒé‡è¡¨ç¤ºä¸ºå…±äº«åŸå­çš„çº¿æ€§ç»„åˆï¼›3) è®­ç»ƒæ¨¡å—ï¼Œä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šMASAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»“æ„åŒ–æƒé‡å…±äº«æœºåˆ¶ï¼Œå…è®¸è·¨å±‚å…±äº«å‚æ•°ï¼Œè€Œæ— éœ€å¤æ‚çš„è’¸é¦æˆ–æ¶æ„å˜æ›´ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒMASAé‡‡ç”¨äº†æ ‡å‡†çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ï¼Œå­—å…¸çš„å¤§å°å’Œå…±äº«è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ç»è¿‡æ¶ˆèå®éªŒéªŒè¯ï¼Œç¡®ä¿äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMASAåœ¨100Måˆ°700Må‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­ï¼ŒåŸºå‡†å‡†ç¡®æ€§å’Œå›°æƒ‘åº¦å‡ä¼˜äºåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰å’Œä½ç§©åŸºçº¿ï¼Œä¸”åœ¨å‚æ•°é¢„ç®—ç›¸å½“çš„æƒ…å†µä¸‹ï¼Œå‚æ•°å‡å°‘è¾¾66.7%ã€‚åœ¨è§†è§‰å˜æ¢å™¨ä¸­ï¼ŒMASAåŒæ ·å®ç°äº†ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå±•ç°äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MASAæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆå‚æ•°ä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰å˜æ¢å™¨ä¸­ã€‚å…¶ç»“æ„åŒ–æƒé‡å…±äº«ç­–ç•¥å¯ä»¥æ˜¾è‘—é™ä½æ¨¡å‹çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œä½¿å¾—åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²å¤§å‹æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚æ­¤å¤–ï¼ŒMASAè¿˜å¯ä»¥åº”ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°å‹ç¼©ï¼Œè¿›ä¸€æ­¥æå‡å…¶å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.

