---
layout: default
title: Hierarchical Text Classification Using Black Box Large Language Models
---

# Hierarchical Text Classification Using Black Box Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04219" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.04219v1</a>
  <a href="https://arxiv.org/pdf/2508.04219.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04219v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04219v1', 'Hierarchical Text Classification Using Black Box Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Kosuke Yoshimura, Hisashi Kashima

**ÂàÜÁ±ª**: cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-06

**Â§áÊ≥®**: 16 pages, 6 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Âà©Áî®ÈªëÁÆ±Â§ßËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂ±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ª‰ª•Â∫îÂØπÊï∞ÊçÆÁ®ÄÁº∫ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ª` `ÈªëÁÆ±Ê®°Âûã` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÊèêÁ§∫Á≠ñÁï•` `Â∞ëÊ†∑Êú¨Â≠¶‰π†` `Êï∞ÊçÆÁ®ÄÁº∫` `Êú∫Âô®Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÈù¢‰∏¥Êï∞ÊçÆÁ®ÄÁº∫ÂíåÊ®°ÂûãÂ§çÊùÇÊÄßÁ≠âÊåëÊàòÔºå‰º†ÁªüÊñπÊ≥ï‰æùËµñÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆ„ÄÇ
2. Êú¨Á†îÁ©∂ÊèêÂá∫Âà©Áî®ÈªëÁÆ±Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÈÄöËøáAPIÂÆûÁé∞Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÔºåÊé¢Á¥¢‰∏çÂêåÊèêÁ§∫Á≠ñÁï•ÁöÑÊúâÊïàÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂ∞ëÊ†∑Êú¨ËÆæÁΩÆÊòæËëóÊèêÈ´òÂàÜÁ±ªÂáÜÁ°ÆÊÄßÔºåLLMsÂú®Ê∑±Â±ÇÊ¨°Ê†áÁ≠æÂ±ÇÊ¨°‰∏ä‰ºò‰∫é‰º†ÁªüÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÔºàHTCÔºâÊó®Âú®Â∞ÜÊñáÊú¨ÂàÜÈÖçÂà∞ÁªìÊûÑÂåñÊ†áÁ≠æÂ±ÇÊ¨°‰∏≠Ôºå‰ΩÜÁî±‰∫éÊï∞ÊçÆÁ®ÄÁº∫ÂíåÊ®°ÂûãÂ§çÊùÇÊÄßÈù¢‰∏¥ÊåëÊàò„ÄÇÊú¨Á†îÁ©∂Êé¢Á¥¢ÈÄöËøáAPIËÆøÈóÆÈªëÁÆ±Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®HTC‰∏≠ÁöÑÂèØË°åÊÄßÔºå‰Ωú‰∏∫‰º†ÁªüÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ïÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÊàë‰ª¨ËØÑ‰º∞‰∫Ü‰∏âÁßçÊèêÁ§∫Á≠ñÁï•‚Äî‚ÄîÁõ¥Êé•Âè∂Ê†áÁ≠æÈ¢ÑÊµãÔºàDLÔºâ„ÄÅÁõ¥Êé•Â±ÇÊ¨°Ê†áÁ≠æÈ¢ÑÊµãÔºàDHÔºâÂíåËá™‰∏äËÄå‰∏ãÂ§öÊ≠•Â±ÇÊ¨°Ê†áÁ≠æÈ¢ÑÊµãÔºàTMHÔºâÔºåÂú®Èõ∂Ê†∑Êú¨ÂíåÂ∞ëÊ†∑Êú¨ËÆæÁΩÆ‰∏ãÊØîËæÉËøô‰∫õÁ≠ñÁï•ÁöÑÂáÜÁ°ÆÊÄßÂíåÊàêÊú¨ÊïàÁõä„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂ∞ëÊ†∑Êú¨ËÆæÁΩÆÂú®ÂàÜÁ±ªÂáÜÁ°ÆÊÄß‰∏äÂßãÁªà‰ºò‰∫éÈõ∂Ê†∑Êú¨ËÆæÁΩÆ„ÄÇÂ∞ΩÁÆ°‰º†ÁªüÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÂú®ÊµÖÂ±ÇÊ¨°Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜLLMsÔºåÂ∞§ÂÖ∂ÊòØDHÁ≠ñÁï•ÔºåÂú®Ê∑±Â±ÇÊ¨°Êï∞ÊçÆÈõÜ‰∏äÂæÄÂæÄË∂ÖË∂äÊú∫Âô®Â≠¶‰π†Ê®°Âûã„ÄÇAPIÊàêÊú¨Âõ†Ê∑±Â±ÇÊ†áÁ≠æÂ±ÇÊ¨°ÊâÄÈúÄÁöÑÊõ¥È´òËæìÂÖ•Ê†áËÆ∞ËÄåÊòæËëóÂ¢ûÂä†„ÄÇËøô‰∫õÁªìÊûúÂº∫Ë∞É‰∫ÜÂáÜÁ°ÆÊÄßÊèêÂçá‰∏éÊèêÁ§∫Á≠ñÁï•ËÆ°ÁÆóÊàêÊú¨‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ª‰∏≠ÁöÑÊï∞ÊçÆÁ®ÄÁº∫ÂíåÊ®°ÂûãÂ§çÊùÇÊÄßÈóÆÈ¢ò„ÄÇ‰º†ÁªüÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ïÈúÄË¶ÅÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÔºåÈöæ‰ª•ÈÄÇÂ∫îÊ∑±Â±ÇÊ¨°Ê†áÁ≠æÂ±ÇÊ¨°ÁöÑÈúÄÊ±Ç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨Á†îÁ©∂ÊèêÂá∫Âà©Áî®ÈªëÁÆ±Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÈÄöËøáAPIËøõË°åÂ±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÔºåÊé¢Á¥¢‰∏çÂêåÁöÑÊèêÁ§∫Á≠ñÁï•‰ª•ÊèêÈ´òÂàÜÁ±ªÂáÜÁ°ÆÊÄßÂíåÈôç‰ΩéÊàêÊú¨„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆËæìÂÖ•„ÄÅÊèêÁ§∫Á≠ñÁï•ÈÄâÊã©„ÄÅÊ®°ÂûãË∞ÉÁî®ÂíåÁªìÊûúËæìÂá∫„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨Áõ¥Êé•Âè∂Ê†áÁ≠æÈ¢ÑÊµãÔºàDLÔºâ„ÄÅÁõ¥Êé•Â±ÇÊ¨°Ê†áÁ≠æÈ¢ÑÊµãÔºàDHÔºâÂíåËá™‰∏äËÄå‰∏ãÂ§öÊ≠•Â±ÇÊ¨°Ê†áÁ≠æÈ¢ÑÊµãÔºàTMHÔºâ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨Á†îÁ©∂ÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÈªëÁÆ±LLMsÂ∫îÁî®‰∫éÂ±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÔºåÂ∞§ÂÖ∂ÊòØDHÁ≠ñÁï•Âú®Ê∑±Â±ÇÊ¨°Ê†áÁ≠æÂ±ÇÊ¨°‰∏äË∂ÖË∂ä‰º†ÁªüÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÊñ∞ÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊèêÁ§∫Á≠ñÁï•‰∏≠ÔºåDL„ÄÅDHÂíåTMHÁöÑËÆæËÆ°ËÄÉËôë‰∫ÜËæìÂÖ•Ê†áËÆ∞ÁöÑÊï∞ÈáèÂíåÂ±ÇÊ¨°ÁªìÊûÑÁöÑÂ§çÊùÇÊÄßÔºåÂ∞§ÂÖ∂ÊòØDHÁ≠ñÁï•Âú®Ê∑±Â±ÇÊ¨°Ê†áÁ≠æÂ±ÇÊ¨°‰∏äÁöÑË°®Áé∞ÔºåÈúÄÂÖ≥Ê≥®APIË∞ÉÁî®ÁöÑÊàêÊú¨„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Â∞ëÊ†∑Êú¨ËÆæÁΩÆ‰∏ãÔºåÂàÜÁ±ªÂáÜÁ°ÆÊÄßÊòæËëóÊèêÈ´òÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê∑±Â±ÇÊ¨°Ê†áÁ≠æÂ±ÇÊ¨°‰∏äÔºåDHÁ≠ñÁï•ÁöÑË°®Áé∞‰ºò‰∫é‰º†ÁªüÊú∫Âô®Â≠¶‰π†Ê®°Âûã„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåLLMsÂú®Ê∑±Â±ÇÊ¨°Êï∞ÊçÆÈõÜ‰∏äÁöÑÂáÜÁ°ÆÊÄßÊèêÂçáÂπÖÂ∫¶ÊòéÊòæÔºåÂ∞ΩÁÆ°APIÊàêÊú¨‰πüÈöè‰πãÂ¢ûÂä†„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊñáÊú¨ÂàÜÁ±ª„ÄÅ‰ø°ÊÅØÊ£ÄÁ¥¢ÂíåÂÜÖÂÆπÊé®ËçêÁ≠â„ÄÇÈÄöËøáÂà©Áî®ÈªëÁÆ±Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂú®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞È´òÊïàÁöÑÂ±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÔºåÂÖ∑ÊúâÂÆûÈôÖ‰ª∑ÂÄº„ÄÇÊú™Êù•ÔºåÈöèÁùÄÊ®°ÂûãÂíåËÆ°ÁÆóËµÑÊ∫êÁöÑËøõÊ≠•ÔºåËØ•ÊñπÊ≥ïÂèØËÉΩÂú®Êõ¥Â§öÈ¢ÜÂüüÂæóÂà∞ÂπøÊ≥õÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Hierarchical Text Classification (HTC) aims to assign texts to structured label hierarchies; however, it faces challenges due to data scarcity and model complexity. This study explores the feasibility of using black box Large Language Models (LLMs) accessed via APIs for HTC, as an alternative to traditional machine learning methods that require extensive labeled data and computational resources. We evaluate three prompting strategies -- Direct Leaf Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and few-shot settings, comparing the accuracy and cost-effectiveness of these strategies. Experiments on two datasets show that a few-shot setting consistently improves classification accuracy compared to a zero-shot setting. While a traditional machine learning model achieves high accuracy on a dataset with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the machine learning model on a dataset with a deeper hierarchy. API costs increase significantly due to the higher input tokens required for deeper label hierarchies on DH strategy. These results emphasize the trade-off between accuracy improvement and the computational cost of prompt strategy. These findings highlight the potential of black box LLMs for HTC while underscoring the need to carefully select a prompt strategy to balance performance and cost.

