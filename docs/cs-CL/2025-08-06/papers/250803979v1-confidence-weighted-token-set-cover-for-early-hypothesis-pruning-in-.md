---
layout: default
title: Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency
---

# Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03979" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03979v1</a>
  <a href="https://arxiv.org/pdf/2508.03979.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03979v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03979v1', 'Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Md Arafat Sultan, RamÃ³n Fernandez Astudillo

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç½®ä¿¡åº¦åŠ æƒçš„å‡è®¾ä¿®å‰ªæ–¹æ³•ä»¥æé«˜è‡ªä¸€è‡´æ€§æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªä¸€è‡´æ€§` `å‡è®¾ä¿®å‰ª` `ä»£å¸æ•ˆç‡` `é•¿é“¾æ¨ç†` `åŠ æƒé›†åˆè¦†ç›–` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°å­¦åŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªä¸€è‡´æ€§æ–¹æ³•åœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨é«˜ä»£å¸æ¶ˆè€—çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç½®ä¿¡åº¦å’Œè¯æ±‡è¦†ç›–ç‡çš„æ—©æœŸå‡è®¾ä¿®å‰ªæ–¹æ³•ï¼Œä»¥æé«˜ä»£å¸æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†10-35%çš„ä»£å¸æ•ˆç‡æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡è‡ªä¸€è‡´æ€§æ–¹æ³•ç®€å•æœ‰æ•ˆï¼Œä½†å…¶é«˜ä»£å¸æ¶ˆè€—é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ—©æœŸå‡è®¾ä¿®å‰ªæé«˜è‡ªä¸€è‡´æ€§åœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ä¸­çš„ä»£å¸æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒå…¶å¹¶è¡Œæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¹¶è¡Œç”Ÿæˆæ‰€æœ‰è§£å†³æ–¹æ¡ˆï¼Œä½†å®šæœŸä¿®å‰ªåŸºäºä¸¤ä¸ªè½»é‡çº§æŒ‡æ ‡è¢«è®¤ä¸ºä¸å¿…è¦çš„ä¸­é—´å‡è®¾ï¼šæ¨¡å‹å¯¹ä¸ªåˆ«å‡è®¾çš„è‡ªä¿¡åº¦å’Œå½“å‰å‡è®¾çš„è¯æ±‡è¦†ç›–ç‡ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¿«é€Ÿçš„åŠ æƒé›†åˆè¦†ç›–ç®—æ³•ï¼Œè¯„ä¼°äº†äº”ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸‰ä¸ªæ•°å­¦åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨è®¸å¤šæƒ…å†µä¸‹å¯ä»¥æé«˜10-35%çš„ä»£å¸æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªä¸€è‡´æ€§æ–¹æ³•åœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ä¸­é«˜ä»£å¸æ¶ˆè€—çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå’Œè¯„ä¼°å‡è®¾æ—¶ï¼Œå¾€å¾€ä¼šäº§ç”Ÿå¤§é‡ä¸å¿…è¦çš„ä¸­é—´å‡è®¾ï¼Œå¯¼è‡´èµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ—©æœŸå‡è®¾ä¿®å‰ªæ¥æé«˜ä»£å¸æ•ˆç‡ã€‚å…·ä½“è€Œè¨€ï¼Œåˆ©ç”¨æ¨¡å‹å¯¹å‡è®¾çš„ç½®ä¿¡åº¦å’Œå½“å‰å‡è®¾çš„è¯æ±‡è¦†ç›–ç‡æ¥åˆ¤æ–­å“ªäº›å‡è®¾å¯ä»¥è¢«ä¿®å‰ªï¼Œä»è€Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¹¶è¡Œç”Ÿæˆæ‰€æœ‰è§£å†³æ–¹æ¡ˆçš„æ¨¡å—ï¼Œä»¥åŠå®šæœŸè¯„ä¼°å’Œä¿®å‰ªä¸­é—´å‡è®¾çš„æ¨¡å—ã€‚ä¿®å‰ªè¿‡ç¨‹ä¾èµ–äºä¸¤ä¸ªè½»é‡çº§æŒ‡æ ‡ï¼Œç¡®ä¿åœ¨ä¿ç•™é‡è¦å‡è®¾çš„åŒæ—¶å‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†ç½®ä¿¡åº¦å’Œè¯æ±‡è¦†ç›–ç‡è¿™ä¸¤ä¸ªæŒ‡æ ‡æ¥æŒ‡å¯¼å‡è®¾çš„ä¿®å‰ªã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å‡è®¾è¯„ä¼°æ–¹å¼ä¸åŒï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå¹¶è¡Œæ€§çš„åŒæ—¶æ˜¾è‘—æé«˜ä»£å¸æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŠ æƒé›†åˆè¦†ç›–ç®—æ³•ï¼Œç»“åˆäº†ç½®ä¿¡åº¦å’Œè¯æ±‡è¦†ç›–ç‡çš„è®¡ç®—ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨è¯¥æ–¹æ³•çš„äº”ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸‰ä¸ªæ•°å­¦åŸºå‡†ä¸Šå‡å®ç°äº†10-35%çš„ä»£å¸æ•ˆç‡æå‡ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›è¡¨æ˜ï¼Œæ—©æœŸå‡è®¾ä¿®å‰ªèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ï¼Œæå‡æ¨¡å‹çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é•¿é“¾æ¨ç†ä»»åŠ¡ï¼Œå¦‚æ•°å­¦é—®é¢˜æ±‚è§£å’Œå¤æ‚æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡æé«˜ä»£å¸æ•ˆç‡ï¼Œè¯¥æ–¹æ³•å¯ä»¥é™ä½è®¡ç®—èµ„æºçš„æ¶ˆè€—ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ é«˜æ•ˆå’Œç»æµã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šå½±å“æ›´å¤šåŸºäºè‡ªä¸€è‡´æ€§çš„æ–¹æ³•ï¼Œæ¨åŠ¨å…¶åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite its simplicity and efficacy, the high token expenditure of self-consistency can limit its practical utility. Here we investigate if self-consistency can be made more token-efficient for long chain-of-thought reasoning tasks, while preserving its parallelism, through early hypothesis pruning. Concretely, we generate all solutions in parallel, but periodically prune intermediate hypotheses that are deemed unnecessary based on two lightweight indicators: (a) the model's own confidence in individual hypotheses, and (b) lexical coverage of all current hypotheses by candidate subsets that are under consideration for continued retention. We design a fast weighted set cover algorithm that utilizes the two indicators; our evaluation of five LLMs on three math benchmarks shows that this method can improve token efficiency for all models, by 10-35% in many cases.

