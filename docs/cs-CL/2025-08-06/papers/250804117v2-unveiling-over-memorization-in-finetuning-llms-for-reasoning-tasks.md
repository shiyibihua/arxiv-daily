---
layout: default
title: Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks
---

# Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04117" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04117v2</a>
  <a href="https://arxiv.org/pdf/2508.04117.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04117v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04117v2', 'Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiwen Ruan, Yun Chen, Yutao Hou, Peng Li, Yang Liu, Guanhua Chen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06 (æ›´æ–°: 2025-09-28)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„è¿‡åº¦è®°å¿†ç°è±¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `è¿‡åº¦è®°å¿†` `é²æ£’æ€§` `æ³›åŒ–èƒ½åŠ›` `ç”Ÿæˆå¤šæ ·æ€§` `å­¦ä¹ åŠ¨æ€` `æ£€æŸ¥ç‚¹æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡åº¦è®°å¿†è®­ç»ƒæ•°æ®ï¼Œå½±å“å…¶æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡åˆ†æå¾®è°ƒé˜¶æ®µçš„å­¦ä¹ åŠ¨æ€ï¼Œè¯†åˆ«å¹¶ç¼“è§£è¿‡åº¦è®°å¿†ç°è±¡ï¼Œä»¥æå‡æ¨¡å‹çš„å®é™…åº”ç”¨æ•ˆæœã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œè¿‡åº¦è®°å¿†çš„æ¨¡å‹åœ¨æµ‹è¯•å‡†ç¡®ç‡ä¸Šä¸æ­£å¸¸æ¨¡å‹ç›¸å½“ï¼Œä½†åœ¨é²æ£’æ€§å’Œç”Ÿæˆå¤šæ ·æ€§æ–¹é¢æ˜¾è‘—ä¸‹é™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢„è®­ç»ƒçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»¥æå‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½ã€‚æœ¬æ–‡ç ”ç©¶äº†LLMå¾®è°ƒåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„å­¦ä¹ åŠ¨æ€ï¼Œæ­ç¤ºäº†åœ¨ç‰¹å®šé˜¶æ®µå‡ºç°çš„è¿‡åº¦è®°å¿†ç°è±¡ã€‚åœ¨æ­¤é˜¶æ®µï¼ŒLLMsè¿‡åº¦è®°å¿†è®­ç»ƒæ•°æ®ï¼Œå°½ç®¡æµ‹è¯•å‡†ç¡®ç‡è‰¯å¥½ï¼Œä½†æµ‹è¯•å›°æƒ‘åº¦å´å¾ˆé«˜ã€‚æˆ‘ä»¬æ¢è®¨äº†å¯¼è‡´è¿‡åº¦è®°å¿†çš„æ¡ä»¶ï¼Œå‘ç°è¿™ä¸€é—®é¢˜åœ¨å¤šç§ä»»åŠ¡ã€æ¨¡å‹å’Œå¾®è°ƒæ–¹æ³•ä¸­æ™®éå­˜åœ¨ï¼Œä¸”è®­ç»ƒæ—¶é—´è¿‡é•¿å’Œå­¦ä¹ ç‡è¿‡å¤§åŠ å‰§äº†è¿™ä¸€é—®é¢˜ã€‚å°½ç®¡è¿‡åº¦è®°å¿†çš„æ¨¡å‹åœ¨æµ‹è¯•å‡†ç¡®ç‡ä¸Šä¸æ­£å¸¸æ¨¡å‹ç›¸å½“ï¼Œä½†å…¶é²æ£’æ€§é™ä½ã€å¯¹åˆ†å¸ƒå¤–æ•°æ®çš„æ³›åŒ–èƒ½åŠ›å·®ä»¥åŠç”Ÿæˆå¤šæ ·æ€§å‡å°‘ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬æä¾›äº†æ£€æŸ¥ç‚¹é€‰æ‹©çš„å»ºè®®ï¼Œå¹¶æå‡ºäº†æ£€æŸ¥ç‚¹åˆå¹¶å’Œè®°å¿†æ„ŸçŸ¥é‡åŠ æƒç­‰æŠ€æœ¯ä»¥å‡è½»è¿™ä¸€å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å‡ºç°çš„è¿‡åº¦è®°å¿†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªèƒ½æœ‰æ•ˆç›‘æµ‹å’Œæ§åˆ¶æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„è¿‡åº¦è®°å¿†ï¼Œå¯¼è‡´æ¨¡å‹åœ¨çœŸå®åº”ç”¨ä¸­çš„æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ·±å…¥åˆ†æå¾®è°ƒé˜¶æ®µçš„å­¦ä¹ åŠ¨æ€ï¼Œè¯†åˆ«å‡ºå¯¼è‡´è¿‡åº¦è®°å¿†çš„å› ç´ ï¼Œå¹¶æå‡ºç›¸åº”çš„ç¼“è§£ç­–ç•¥ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹å¾®è°ƒè¿‡ç¨‹çš„ç›‘æ§ã€è¿‡åº¦è®°å¿†ç°è±¡çš„è¯†åˆ«ã€ä»¥åŠåŸºäºæ­¤æå‡ºçš„æ£€æŸ¥ç‚¹é€‰æ‹©å’Œé‡åŠ æƒæŠ€æœ¯ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®ç›‘æµ‹ã€æ¨¡å‹è¯„ä¼°å’Œç­–ç•¥å®æ–½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºè¯†åˆ«äº†å¾®è°ƒé˜¶æ®µçš„è¿‡åº¦è®°å¿†ç°è±¡ï¼Œå¹¶æå‡ºäº†æ£€æŸ¥ç‚¹åˆå¹¶å’Œè®°å¿†æ„ŸçŸ¥é‡åŠ æƒç­‰æ–°æ–¹æ³•ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…³æ³¨æ¨¡å‹çš„é•¿æœŸå­¦ä¹ åŠ¨æ€ï¼Œè€Œéä»…ä»…ä¾èµ–äºçŸ­æœŸæ€§èƒ½æŒ‡æ ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹å­¦ä¹ ç‡å’Œè®­ç»ƒæ—¶é—´çš„ä¼˜åŒ–è®¾ç½®ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å‡†ç¡®ç‡ä¸æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨æ¨¡å‹è¯„ä¼°ä¸­å¼•å…¥å¤šæ ·æ€§æŒ‡æ ‡ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿‡åº¦è®°å¿†çš„æ¨¡å‹åœ¨æµ‹è¯•å‡†ç¡®ç‡ä¸Šä¸æ­£å¸¸æ¨¡å‹ç›¸å½“ï¼Œä½†åœ¨é²æ£’æ€§æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹åˆ†å¸ƒå¤–æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ä¸Šé™ä½äº†çº¦20%ã€‚é€šè¿‡é‡‡ç”¨æ£€æŸ¥ç‚¹åˆå¹¶å’Œè®°å¿†æ„ŸçŸ¥é‡åŠ æƒæŠ€æœ¯ï¼Œæ¨¡å‹çš„ç”Ÿæˆå¤šæ ·æ€§æå‡äº†15%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡æ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œç ”ç©¶æˆæœèƒ½å¤Ÿæå‡è¿™äº›ç³»ç»Ÿåœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ï¼Œå‡å°‘å› è¿‡åº¦è®°å¿†å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The pretrained large language models (LLMs) are finetuned with labeled data for better instruction following ability and alignment with human values. In this paper, we study the learning dynamics of LLM finetuning on reasoning tasks and reveal the uncovered over-memorization phenomenon during a specific stage of LLM finetuning. At this stage, the LLMs have excessively memorized training data and exhibit high test perplexity while maintaining good test accuracy. We explore the conditions that contribute to over-memorization and discover that this issue is prevalent across various tasks, models, and fine-tuning methods, with prolonged training and large learning rates exacerbating the problem. Although models with over-memorization demonstrate comparable test accuracy to normal models, they suffer from reduced robustness, poor out-of-distribution generalization, and decreased generation diversity. In light of our findings on over-memorization, we offer recommendations for checkpoint selection and propose techniques such as checkpoint merging and memorization-aware reweighting to mitigate this effect.

