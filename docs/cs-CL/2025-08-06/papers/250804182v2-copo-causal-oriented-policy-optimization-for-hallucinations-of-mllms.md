---
layout: default
title: COPO: Causal-Oriented Policy Optimization for Hallucinations of MLLMs
---

# COPO: Causal-Oriented Policy Optimization for Hallucinations of MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04182" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04182v2</a>
  <a href="https://arxiv.org/pdf/2508.04182.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04182v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04182v2', 'COPO: Causal-Oriented Policy Optimization for Hallucinations of MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peizheng Guo, Jingyao Wang, Wenwen Qiang, Jiahuan Zhou, Changwen Zheng, Gang Hua

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06 (æ›´æ–°: 2025-11-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCOPOä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¹»è§‰é—®é¢˜` `å› æœå¯¼å‘` `ç­–ç•¥ä¼˜åŒ–` `ç”Ÿæˆæ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `è™šå‡ç›¸å…³æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†ä»»åŠ¡æ—¶ï¼Œå®¹æ˜“å—åˆ°ä¸ä»»åŠ¡æ— å…³çš„èƒŒæ™¯ä¿¡æ¯å½±å“ï¼Œå¯¼è‡´å¹»è§‰ç°è±¡ã€‚
2. æå‡ºå› æœå¯¼å‘ç­–ç•¥ä¼˜åŒ–ï¼ˆCOPOï¼‰ï¼Œé€šè¿‡ä»¤ç‰Œçº§çš„å……åˆ†æ€§å’Œå¿…è¦æ€§çº¦æŸï¼Œå‡å°‘è™šå‡ç›¸å…³æ€§ï¼Œä»è€Œæ”¹å–„ç”Ÿæˆè´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCOPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç”Ÿæˆå‡†ç¡®æ€§ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¯èƒ½ä¼šé­é‡å¹»è§‰é—®é¢˜ã€‚å®è¯ç ”ç©¶å‘ç°ï¼Œä¸ä»…æ–‡æœ¬çš„è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒMLLMså¯¹ä¸ä»»åŠ¡æ— å…³çš„èƒŒæ™¯åŒºåŸŸçš„å…³æ³¨ç¨‹åº¦è¿‡é«˜ï¼Œè¿™æš—ç¤ºäº†è™šå‡çš„èƒŒæ™¯-ç­”æ¡ˆç›¸å…³æ€§ã€‚æˆ‘ä»¬åˆ†æè®¤ä¸ºï¼ŒåŸºäºç»“æœçš„å¥–åŠ±å¯èƒ½æ˜¯å¯¼è‡´è™šå‡ç›¸å…³æ€§çš„é‡è¦å› ç´ ï¼Œè€Œè™šå‡ç›¸å…³æ€§åˆå¯èƒ½å¯¼è‡´å¹»è§‰ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†å› æœå¯¼å‘ç­–ç•¥ä¼˜åŒ–ï¼ˆCOPOï¼‰ï¼Œæ—¨åœ¨å‡è½»è¿™äº›è™šå‡ç›¸å…³æ€§ï¼Œä»è€Œè§£å†³å¹»è§‰é—®é¢˜ã€‚è¯¥æ–¹æ³•æ–½åŠ äº†ä»¤ç‰Œçº§çš„å……åˆ†æ€§å’Œå¿…è¦æ€§çº¦æŸï¼Œä»¥è¡¡é‡æ¯ä¸ªæ¨ç†ä»¤ç‰Œçš„å› æœè´¡çŒ®ï¼Œä»è€Œç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºæ˜¯æ­£ç¡®ä¸”åŸºäºè¯æ®çš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCOPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è§‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†ä¸ä»»åŠ¡æ— å…³çš„èƒŒæ™¯ä¿¡æ¯å¯¼è‡´çš„è™šå‡ç›¸å…³æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå› æœå¯¼å‘ç­–ç•¥ä¼˜åŒ–ï¼ˆCOPOï¼‰ï¼Œé€šè¿‡æ–½åŠ ä»¤ç‰Œçº§çš„å……åˆ†æ€§å’Œå¿…è¦æ€§çº¦æŸï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„å› æœæœ‰æ•ˆæ€§ï¼Œä»è€Œå‡å°‘å¹»è§‰ç°è±¡çš„å‘ç”Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCOPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯å› æœå®Œæ•´æ€§å¥–åŠ±çš„è®¡ç®—ï¼Œç”¨äºè¯„ä¼°æ¯ä¸ªä»¤ç‰Œçš„å› æœè´¡çŒ®ï¼›å…¶æ¬¡æ˜¯åœ¨GRPOä¼˜åŒ–æ¡†æ¶å†…æ„å»ºå› æœä¿¡æ¯é©±åŠ¨çš„ä¼˜åŠ¿å‡½æ•°ï¼Œä»¥å¼•å¯¼æ¨¡å‹å…³æ³¨å› æœå……åˆ†ä¸”å¿…è¦çš„ä»¤ç‰Œã€‚

**å…³é”®åˆ›æ–°**ï¼šCOPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥å› æœå®Œæ•´æ€§å¥–åŠ±ï¼Œé‡åŒ–æ¯ä¸ªä»¤ç‰Œçš„å› æœè´¡çŒ®ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„åŸºäºç»“æœçš„å¥–åŠ±æœºåˆ¶æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è™šå‡ç›¸å…³æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCOPOé‡‡ç”¨äº†æ–°çš„å¥–åŠ±æœºåˆ¶ï¼Œç¡®ä¿æ¯ä¸ªæ¨ç†ä»¤ç‰Œçš„å› æœè´¡çŒ®è¢«å‡†ç¡®è¯„ä¼°ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–ç®—æ³•å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´ä¸ºå‡†ç¡®çš„è¾“å‡ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCOPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ç”Ÿæˆæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡å‡è½»å¹»è§‰ç°è±¡ï¼ŒCOPOèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite Multimodal Large Language Models (MLLMs) having shown impressive capabilities, they may suffer from hallucinations. Empirically, we find that MLLMs attend disproportionately to task-irrelevant background regions compared with text-only LLMs, implying spurious background-answer correlations. We claim and analyze that (i) outcome-based rewards can be an important factor leading to spurious correlations, and (ii) spurious correlations can be an important factor leading to hallucinations. Based on these results, we propose Causal-Oriented Policy Optimization (COPO) to mitigate these spurious correlations, thus addressing the issue of hallucinations. It imposes token-level sufficiency and necessity constraints to measure each inference token's causal contribution, thus ensuring correct and evidence-grounded output. Specifically, we first evaluate each token's causal contribution via a newly proposed causal completeness reward. This reward is then used to construct a causally informed advantage function within the GRPO optimization framework, encouraging the model to focus on tokens that are causally sufficient and necessary for accurate generation. Experimental results across various benchmarks demonstrate the advantages of COPO.

