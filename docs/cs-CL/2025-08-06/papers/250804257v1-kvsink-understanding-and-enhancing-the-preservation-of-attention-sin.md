---
layout: default
title: "KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs"
---

# KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04257" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04257v1</a>
  <a href="https://arxiv.org/pdf/2508.04257.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04257v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04257v1', 'KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zunhai Su, Kehong Yuan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: Published as a conference paper at COLM 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKVSinkä»¥å¢å¼ºKVç¼“å­˜é‡åŒ–ä¸­æ³¨æ„åŠ›æ±‡èšçš„ä¿æŠ¤**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `KVç¼“å­˜é‡åŒ–` `æ³¨æ„åŠ›æœºåˆ¶` `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†ä¼˜åŒ–` `æ¨¡å‹å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„KVç¼“å­˜é‡åŒ–æ–¹æ³•åœ¨ä¿æŠ¤æ³¨æ„åŠ›æ±‡èšæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯æœªèƒ½è€ƒè™‘æ±‡èšåœ¨åˆå§‹tokenä½ç½®ä¹‹å¤–çš„æƒ…å†µã€‚
2. è®ºæ–‡æå‡ºKVSinkï¼Œé€šè¿‡æ·±å…¥ç†è§£æ³¨æ„åŠ›æ±‡èšçš„æœºåˆ¶ï¼Œè®¾è®¡äº†ä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•æ¥é¢„æµ‹æ±‡èštokenï¼Œä»è€Œå¢å¼ºä¿æŠ¤æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKVSinkåœ¨KVç¼“å­˜é‡åŒ–ä¸­æ˜¾è‘—ä¼˜äºPFNç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹çš„å›°æƒ‘åº¦ï¼ˆPPLï¼‰ï¼Œå¹¶å‡å°‘äº†å¯¹16ä½æ•°å€¼å¼‚å¸¸å€¼çš„ä¾èµ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Key-Value (KV)ç¼“å­˜é‡åŒ–å·²æˆä¸ºé«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†çš„å¹¿æ³›ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å‡å°‘KVç¼“å­˜å†…å­˜ä½¿ç”¨å’Œç¼“è§£å†…å­˜çº¦æŸæ¥æé«˜æ•ˆç‡ã€‚è¿‘æœŸç ”ç©¶å¼ºè°ƒäº†åœ¨å‰å‡ ä¸ªtokenä¸­ä¿æŒKVsåŸå§‹ç²¾åº¦çš„é‡è¦æ€§ï¼Œä»¥ç¡®ä¿ä¿æŠ¤æ³¨æ„åŠ›æ±‡èšã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„åŸºæœ¬åŸç†å°šä¸å……åˆ†ç†è§£ï¼Œå¹¶ä¸”æœªèƒ½è§£å†³æ³¨æ„åŠ›æ±‡èšå¯èƒ½åœ¨åˆå§‹tokenä½ç½®ä¹‹å¤–å‡ºç°çš„æœ€æ–°å‘ç°ã€‚æœ¬æ–‡é˜æ˜äº†æ¨ç†è¿‡ç¨‹ä¸­æ³¨æ„åŠ›æ±‡èšçš„åŸºæœ¬æœºåˆ¶ï¼Œå¹¶åˆ†æäº†æ³¨æ„åŠ›æ±‡èšä¸KVç¼“å­˜é‡åŒ–ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚åŸºäºå¯¹è¿™äº›æœºåˆ¶çš„æ·±å…¥ç†è§£ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºKVSinkçš„å³æ’å³ç”¨æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹æ±‡èštokenï¼Œä¸”å¼€é”€æå°ï¼Œä»è€Œå®ç°æ›´å…¨é¢çš„ä¿æŠ¤ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒKVSinkåœ¨KVç¼“å­˜é‡åŒ–è¿‡ç¨‹ä¸­ä¼˜äºç°æœ‰çš„ä¿ç•™é¦–Nä¸ªï¼ˆPFNï¼‰ç­–ç•¥ï¼Œæä¾›äº†æ›´æœ‰æ•ˆçš„æ³¨æ„åŠ›æ±‡èšä¿æŠ¤ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³KVç¼“å­˜é‡åŒ–ä¸­æ³¨æ„åŠ›æ±‡èšçš„ä¿æŠ¤ä¸è¶³é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†ç†è§£æ±‡èšçš„æœºåˆ¶ï¼Œä¸”å¿½è§†äº†æ±‡èšåœ¨åˆå§‹tokenä½ç½®ä¹‹å¤–çš„æ½œåœ¨å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†ææ³¨æ„åŠ›æ±‡èšåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä½œç”¨ï¼Œæå‡ºKVSinkæ–¹æ³•æ¥é¢„æµ‹æ±‡èštokenï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°ä¿æŠ¤è¿™äº›tokençš„ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKVSinkæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹æ³¨æ„åŠ›æ±‡èšæœºåˆ¶çš„æ·±å…¥åˆ†æã€æ±‡èštokençš„é¢„æµ‹æ¨¡å—ä»¥åŠä¸KVç¼“å­˜é‡åŒ–çš„ç»“åˆï¼Œå½¢æˆä¸€ä¸ªé—­ç¯ä¼˜åŒ–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šKVSinkçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é¢„æµ‹æ±‡èštokenï¼Œæ˜¾è‘—æé«˜äº†KVç¼“å­˜é‡åŒ–çš„æ•ˆæœï¼Œä¸ä¼ ç»Ÿçš„PFNç­–ç•¥ç›¸æ¯”ï¼Œæä¾›äº†æ›´å…¨é¢çš„ä¿æŠ¤ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨KVSinkçš„è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ±‡èštokençš„å‡†ç¡®é¢„æµ‹ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥å‡å°‘è®¡ç®—å¼€é”€ã€‚å…·ä½“ç»†èŠ‚åŒ…æ‹¬å¯¹æ¨¡å‹çš„è®­ç»ƒç­–ç•¥å’Œæ•°æ®é›†çš„é€‰æ‹©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒKVSinkåœ¨KVç¼“å­˜é‡åŒ–ä¸­æ˜¾è‘—ä¼˜äºPFNç­–ç•¥ï¼Œå…·ä½“è¡¨ç°ä¸ºå›°æƒ‘åº¦ï¼ˆPPLï¼‰é™ä½ï¼Œä¸”å¯¹16ä½æ•°å€¼å¼‚å¸¸å€¼çš„ä¾èµ–å‡å°‘ï¼Œæå‡å¹…åº¦å…·ä½“æ•°æ®æœªæä¾›ï¼Œä½†æ•ˆæœæ˜¾è‘—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæœºå™¨ç¿»è¯‘ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼ŒKVSinkæ–¹æ³•å¯èƒ½ä¼šè¢«å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆæ¨ç†çš„AIç³»ç»Ÿä¸­ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \textit{\textbf{KVSink} }, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers.

