---
layout: default
title: DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting
---

# DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04239" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04239v1</a>
  <a href="https://arxiv.org/pdf/2508.04239.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04239v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04239v1', 'DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chanjuan Liu, Shengzhi Wang, Enqiang Zhu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDP-GPT4MTSä»¥è§£å†³æ–‡æœ¬ä¸æ•°å€¼æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—é¢„æµ‹` `å¤šæ¨¡æ€å­¦ä¹ ` `æ–‡æœ¬ä¿¡æ¯æ•´åˆ` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•å¤šé›†ä¸­äºæ•°å€¼æ•°æ®ï¼Œå¿½è§†äº†æ–‡æœ¬ä¿¡æ¯çš„å½±å“ï¼Œå¯¼è‡´é¢„æµ‹å‡†ç¡®æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºDP-GPT4MTSæ¡†æ¶ï¼Œé€šè¿‡åŒæç¤ºæœºåˆ¶æœ‰æ•ˆæ•´åˆæ–‡æœ¬å’Œæ•°å€¼ä¿¡æ¯ï¼Œæå‡é¢„æµ‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›ç®—æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ—¶é—´åºåˆ—é¢„æµ‹åœ¨å„è¡Œä¸šçš„æˆ˜ç•¥è§„åˆ’å’Œå†³ç­–ä¸­è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿé¢„æµ‹æ¨¡å‹ä¸»è¦å…³æ³¨æ•°å€¼æ—¶é—´åºåˆ—æ•°æ®ï¼Œå¸¸å¸¸å¿½è§†äº‹ä»¶å’Œæ–°é—»ç­‰é‡è¦æ–‡æœ¬ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯å¯¹é¢„æµ‹å‡†ç¡®æ€§æœ‰æ˜¾è‘—å½±å“ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•´åˆå¤šæ¨¡æ€æ•°æ®æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†ç°æœ‰çš„å•æç¤ºæ¡†æ¶éš¾ä»¥æœ‰æ•ˆæ•æ‰æ—¶é—´æˆ³æ–‡æœ¬çš„è¯­ä¹‰ï¼Œå¯¼è‡´å†—ä½™ä¿¡æ¯å½±å“æ¨¡å‹æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†DP-GPT4MTSï¼ˆåŒæç¤ºGPT2-baseç”¨äºå¤šæ¨¡æ€æ—¶é—´åºåˆ—ï¼‰ï¼Œä¸€ç§æ–°é¢–çš„åŒæç¤ºå¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œç»“åˆäº†æ˜ç¡®çš„ä»»åŠ¡æŒ‡ä»¤æç¤ºå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ—¶é—´æˆ³æ•°æ®æ–‡æœ¬æç¤ºã€‚é€šè¿‡åœ¨å¤šæ ·çš„æ–‡æœ¬-æ•°å€¼æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šè¿›è¡Œçš„å…¨é¢å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›ç®—æ³•ï¼Œçªæ˜¾äº†é€šè¿‡åŒæç¤ºæœºåˆ¶æ•´åˆæ–‡æœ¬ä¸Šä¸‹æ–‡ä»¥å®ç°æ›´å‡†ç¡®é¢„æµ‹çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿæ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹æ— æ³•æœ‰æ•ˆæ•´åˆæ–‡æœ¬ä¿¡æ¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨æ•°å€¼æ•°æ®ï¼Œå¿½ç•¥äº†äº‹ä»¶å’Œæ–°é—»ç­‰æ–‡æœ¬ä¿¡æ¯ï¼Œå¯¼è‡´é¢„æµ‹å‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDP-GPT4MTSæ¡†æ¶é‡‡ç”¨åŒæç¤ºæœºåˆ¶ï¼Œç»“åˆæ˜ç¡®çš„ä»»åŠ¡æŒ‡ä»¤æç¤ºå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ–‡æœ¬æç¤ºï¼Œä»¥æ›´å¥½åœ°æ•æ‰æ—¶é—´æˆ³æ–‡æœ¬çš„è¯­ä¹‰ï¼Œå‡å°‘å†—ä½™ä¿¡æ¯çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç”Ÿæˆæ˜ç¡®ä»»åŠ¡æŒ‡ä»¤çš„tokenizerå’Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸å‰é¦ˆç½‘ç»œä¼˜åŒ–çš„æ–‡æœ¬æç¤ºåµŒå…¥ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šé¦–å…ˆç”Ÿæˆæ˜ç¡®æç¤ºï¼Œç„¶åæå–å’Œä¼˜åŒ–æ–‡æœ¬æç¤ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåŒæç¤ºæœºåˆ¶çš„å¼•å…¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤ŸåŒæ—¶å¤„ç†ä»»åŠ¡æŒ‡ä»¤å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†å¯¹æ—¶é—´åºåˆ—æ•°æ®çš„ç†è§£èƒ½åŠ›ï¼Œä¸ç°æœ‰å•æç¤ºæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥ä¼˜åŒ–æ–‡æœ¬æç¤ºåµŒå…¥ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ•°å€¼å’Œæ–‡æœ¬ä¿¡æ¯çš„å½±å“ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDP-GPT4MTSåœ¨å¤šç§æ–‡æœ¬-æ•°å€¼æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›ç®—æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†åŒæç¤ºæœºåˆ¶åœ¨æé«˜é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºé¢„æµ‹ã€ä¾›åº”é“¾ç®¡ç†ã€æ°”è±¡é¢„æŠ¥ç­‰å¤šä¸ªè¡Œä¸šï¼Œèƒ½å¤Ÿå¸®åŠ©å†³ç­–è€…æ›´å‡†ç¡®åœ°æŠŠæ¡è¶‹åŠ¿å’Œåšå‡ºæˆ˜ç•¥å†³ç­–ã€‚æœªæ¥ï¼Œéšç€å¤šæ¨¡æ€æ•°æ®çš„æ™®éåº”ç”¨ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå±•ç°å…¶ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Time series forecasting is crucial in strategic planning and decision-making across various industries. Traditional forecasting models mainly concentrate on numerical time series data, often overlooking important textual information such as events and news, which can significantly affect forecasting accuracy. While large language models offer a promise for integrating multimodal data, existing single-prompt frameworks struggle to effectively capture the semantics of timestamped text, introducing redundant information that can hinder model performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt GPT2-base for Multimodal Time Series), a novel dual-prompt large language model framework that combines two complementary prompts: an explicit prompt for clear task instructions and a textual prompt for context-aware embeddings from time-stamped data. The tokenizer generates the explicit prompt while the embeddings from the textual prompt are refined through self-attention and feed-forward networks. Comprehensive experiments conducted on diverse textural-numerical time series datasets demonstrate that this approach outperforms state-of-the-art algorithms in time series forecasting. This highlights the significance of incorporating textual context via a dual-prompt mechanism to achieve more accurate time series predictions.

