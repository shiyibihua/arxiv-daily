---
layout: default
title: Chain of Questions: Guiding Multimodal Curiosity in Language Models
---

# Chain of Questions: Guiding Multimodal Curiosity in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04350" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04350v1</a>
  <a href="https://arxiv.org/pdf/2508.04350.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04350v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04350v1', 'Chain of Questions: Guiding Multimodal Curiosity in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nima Iji, Kia Dashtipour

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV, cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºChain of Questionsæ¡†æ¶ä»¥å¢å¼ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `å¥½å¥‡å¿ƒé©±åŠ¨` `é—®é¢˜ç”Ÿæˆ` `æ¨¡æ€é€‰æ‹©` `ä¿¡æ¯æ•´åˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­æœªèƒ½å……åˆ†åˆ©ç”¨æ„ŸçŸ¥æ¨¡æ€ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚
2. æå‡ºçš„CoQæ¡†æ¶é€šè¿‡ç”Ÿæˆé’ˆå¯¹æ€§é—®é¢˜ï¼ŒåŠ¨æ€æ¿€æ´»ç›¸å…³æ¨¡æ€ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoQæ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›é€šè¿‡é“¾å¼æ€ç»´å’Œé€æ­¥è§£é‡Šç­‰æ–¹æ³•å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œè¿™äº›è¿›å±•å°šæœªå®Œå…¨è½¬ç§»åˆ°å¤šæ¨¡æ€ç¯å¢ƒä¸­ã€‚æœ¬æ–‡æå‡ºäº†Chain of Questionsï¼ˆCoQï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¥½å¥‡å¿ƒçš„æ¨ç†æ–¹æ³•ï¼Œé¼“åŠ±å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åŠ¨æ€ç”Ÿæˆæœ‰å…³å…¶ç¯å¢ƒçš„é’ˆå¯¹æ€§é—®é¢˜ã€‚è¿™äº›ç”Ÿæˆçš„é—®é¢˜å¼•å¯¼æ¨¡å‹é€‰æ‹©æ€§åœ°æ¿€æ´»ç›¸å…³çš„æ„ŸçŸ¥æ¨¡æ€ï¼Œä»è€Œæ”¶é›†å¿…è¦çš„ä¿¡æ¯ä»¥è¿›è¡Œå‡†ç¡®çš„æ¨ç†å’Œå“åº”ç”Ÿæˆã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ¡†æ¶ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒCoQæ–¹æ³•æé«˜äº†åŸºç¡€æ¨¡å‹è¯†åˆ«å’Œæ•´åˆç›¸å…³æ„ŸçŸ¥ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä»è€Œæ”¹å–„äº†å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ¨ç†è¿‡ç¨‹ä¸å¤šæ ·åŒ–å¤šæ¨¡æ€ä»»åŠ¡çš„ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•´åˆä¸åŒæ„ŸçŸ¥æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†ç»“æœçš„å‡†ç¡®æ€§å’Œå¯é æ€§é™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoQæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç”Ÿæˆé’ˆå¯¹æ€§çš„é—®é¢˜ï¼Œæ¿€åŠ±æ¨¡å‹ä¸»åŠ¨æ¢ç´¢å’Œé€‰æ‹©åˆé€‚çš„æ„ŸçŸ¥æ¨¡æ€ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æå‡äº†æ¨¡å‹çš„ä¸»åŠ¨æ€§ï¼Œè¿˜å¢å¼ºäº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoQæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé—®é¢˜ç”Ÿæˆæ¨¡å—ã€æ¨¡æ€é€‰æ‹©æ¨¡å—å’Œä¿¡æ¯æ•´åˆæ¨¡å—ã€‚é—®é¢˜ç”Ÿæˆæ¨¡å—è´Ÿè´£æ ¹æ®ç¯å¢ƒä¿¡æ¯ç”Ÿæˆé—®é¢˜ï¼Œæ¨¡æ€é€‰æ‹©æ¨¡å—æ ¹æ®é—®é¢˜çš„æ€§è´¨é€‰æ‹©æ¿€æ´»çš„æ„ŸçŸ¥æ¨¡æ€ï¼Œä¿¡æ¯æ•´åˆæ¨¡å—åˆ™è´Ÿè´£æ•´åˆæ¥è‡ªä¸åŒæ¨¡æ€çš„ä¿¡æ¯ä»¥è¿›è¡Œæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºå¥½å¥‡å¿ƒçš„æ¨ç†æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€ç”Ÿæˆé—®é¢˜å¹¶é€‰æ‹©æ€§æ¿€æ´»æ¨¡æ€ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„é™æ€ä¿¡æ¯å¤„ç†æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é—®é¢˜ç”Ÿæˆçš„è´¨é‡ï¼ŒåŒæ—¶åœ¨æ¨¡æ€é€‰æ‹©ä¸­å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°èšç„¦äºæœ€ç›¸å…³çš„ä¿¡æ¯æºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCoQæ–¹æ³•åœ¨æ–°æ„å»ºçš„å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®æ€§æé«˜äº†çº¦15%ï¼Œæ¨ç†è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ä¹Ÿå¾—åˆ°äº†å¢å¼ºã€‚è¿™äº›ç»“æœéªŒè¯äº†CoQæ¡†æ¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰å¤šæ¨¡æ€äº¤äº’åœºæ™¯ã€‚é€šè¿‡å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œä¿¡æ¯æ•´åˆèƒ½åŠ›ï¼ŒCoQæ¡†æ¶èƒ½å¤Ÿæå‡è¿™äº›ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–è´¨é‡å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning capabilities in large language models (LLMs) have substantially advanced through methods such as chain-of-thought and explicit step-by-step explanations. However, these improvements have not yet fully transitioned to multimodal contexts, where models must proactively decide which sensory modalities such as vision, audio, or spatial perception to engage when interacting with complex real-world environments. In this paper, we introduce the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach that encourages multimodal language models to dynamically generate targeted questions regarding their surroundings. These generated questions guide the model to selectively activate relevant modalities, thereby gathering critical information necessary for accurate reasoning and response generation. We evaluate our framework on a novel multimodal benchmark dataset, assembled by integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results demonstrate that our CoQ method improves a foundation model's ability to effectively identify and integrate pertinent sensory information. This leads to improved accuracy, interpretability, and alignment of the reasoning process with diverse multimodal tasks.

