---
layout: default
title: Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models
---

# Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04325" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04325v1</a>
  <a href="https://arxiv.org/pdf/2508.04325.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04325v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04325v1', 'Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zizhan Ma, Wenxuan Wang, Guo Yu, Yiu-Fai Cheung, Meidan Ding, Jie Liu, Wenting Chen, Linlin Shen

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV, cs.LG, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMedCheckæ¡†æ¶ä»¥è§£å†³åŒ»ç–—åŸºå‡†è¯„ä¼°çš„å¯é æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»ç–—åŸºå‡†` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯„ä¼°æ¡†æ¶` `æ•°æ®å®Œæ•´æ€§` `ä¸´åºŠå®è·µ` `å®‰å…¨è¯„ä¼°` `AIåœ¨åŒ»ç–—ä¸­çš„åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŒ»ç–—åŸºå‡†è¯„ä¼°æ–¹æ³•ç¼ºä¹ä¸´åºŠçœŸå®æ€§å’Œç¨³å¥çš„æ•°æ®ç®¡ç†ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¯é ã€‚
2. æå‡ºMedCheckæ¡†æ¶ï¼Œé€šè¿‡äº”ä¸ªé˜¶æ®µçš„ç”Ÿå‘½å‘¨æœŸè¯„ä¼°ï¼Œæä¾›46ä¸ªåŒ»å­¦å®šåˆ¶æ ‡å‡†ï¼Œæ—¨åœ¨æå‡åŸºå‡†çš„å¯é æ€§å’Œé€æ˜åº¦ã€‚
3. å¯¹53ä¸ªåŒ»ç–—LLMåŸºå‡†çš„å®è¯è¯„ä¼°æ­ç¤ºäº†ç³»ç»Ÿæ€§é—®é¢˜ï¼Œå¼ºè°ƒäº†MedCheckä½œä¸ºè¯Šæ–­å·¥å…·å’ŒæŒ‡å¯¼æ–¹é’ˆçš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸå±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œä¿ƒä½¿äº†ä¼—å¤šåŸºå‡†çš„è¯„ä¼°ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºå‡†çš„å¯é æ€§å—åˆ°è´¨ç–‘ï¼Œå¸¸å¸¸ç¼ºä¹ä¸´åºŠçœŸå®æ€§ã€ç¨³å¥çš„æ•°æ®ç®¡ç†å’Œå®‰å…¨å¯¼å‘çš„è¯„ä¼°æŒ‡æ ‡ã€‚ä¸ºäº†è§£å†³è¿™äº›ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†MedCheckï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºåŒ»ç–—åŸºå‡†è®¾è®¡çš„ç”Ÿå‘½å‘¨æœŸå¯¼å‘è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†åŸºå‡†å¼€å‘åˆ†è§£ä¸ºäº”ä¸ªè¿ç»­é˜¶æ®µï¼Œå¹¶æä¾›äº†46ä¸ªåŒ»å­¦å®šåˆ¶æ ‡å‡†çš„ç»¼åˆæ£€æŸ¥æ¸…å•ã€‚é€šè¿‡MedCheckï¼Œæˆ‘ä»¬å¯¹53ä¸ªåŒ»ç–—LLMåŸºå‡†è¿›è¡Œäº†æ·±å…¥çš„å®è¯è¯„ä¼°ï¼Œå‘ç°äº†å¹¿æ³›çš„ç³»ç»Ÿæ€§é—®é¢˜ï¼ŒåŒ…æ‹¬ä¸ä¸´åºŠå®è·µçš„æ·±åˆ»è„±èŠ‚ã€æ•°æ®å®Œæ•´æ€§å±æœºä»¥åŠå¯¹æ¨¡å‹ç¨³å¥æ€§å’Œä¸ç¡®å®šæ€§æ„è¯†ç­‰å®‰å…¨å…³é”®è¯„ä¼°ç»´åº¦çš„ç³»ç»Ÿæ€§å¿½è§†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŒ»ç–—åŸºå‡†è¯„ä¼°æ–¹æ³•çš„å¯é æ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯å…¶ç¼ºä¹ä¸´åºŠç›¸å…³æ€§å’Œæ•°æ®ç®¡ç†çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMedCheckæ¡†æ¶é€šè¿‡å°†åŸºå‡†å¼€å‘è¿‡ç¨‹åˆ†ä¸ºäº”ä¸ªé˜¶æ®µï¼Œæä¾›äº†ä¸€å¥—å…¨é¢çš„åŒ»å­¦æ ‡å‡†æ£€æŸ¥æ¸…å•ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„ç§‘å­¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMedCheckæ¡†æ¶åŒ…æ‹¬è®¾è®¡ã€å®æ–½ã€è¯„ä¼°ã€æ²»ç†å’Œåé¦ˆäº”ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰ç‰¹å®šçš„è¯„ä¼°æ ‡å‡†å’Œæµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMedCheckçš„åˆ›æ–°åœ¨äºå…¶ç”Ÿå‘½å‘¨æœŸå¯¼å‘çš„è¯„ä¼°æ–¹æ³•ï¼Œå¼ºè°ƒäº†ä»è®¾è®¡åˆ°æ²»ç†çš„è¿ç»­æ€§ï¼Œä¸ä¼ ç»Ÿçš„é™æ€è¯„ä¼°æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¡†æ¶ä¸­åŒ…å«46ä¸ªåŒ»å­¦å®šåˆ¶æ ‡å‡†ï¼Œæ¶µç›–ä¸´åºŠå®è·µçš„ç›¸å…³æ€§ã€æ•°æ®å®Œæ•´æ€§å’Œå®‰å…¨æ€§ç­‰å…³é”®ç»´åº¦ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œæ·±åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

é€šè¿‡MedCheckæ¡†æ¶çš„åº”ç”¨ï¼Œæˆ‘ä»¬å¯¹53ä¸ªåŒ»ç–—LLMåŸºå‡†è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå‘ç°äº†ä¸ä¸´åºŠå®è·µçš„ä¸¥é‡è„±èŠ‚å’Œæ•°æ®å®Œæ•´æ€§é—®é¢˜ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†MedCheckåœ¨æå‡åŒ»ç–—åŸºå‡†è¯„ä¼°å¯é æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œæ¨åŠ¨äº†å¯¹ç°æœ‰è¯„ä¼°æ–¹æ³•çš„åæ€å’Œæ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MedCheckæ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºåŒ»ç–—AIçš„è¯„ä¼°å’Œå¼€å‘ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å»ºç«‹æ›´å¯é çš„åŒ»ç–—åŸºå‡†ï¼Œæå‡AIåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æ•ˆæœå’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›æ¨åŠ¨åŒ»ç–—AIè¯„ä¼°æ ‡å‡†çš„ç»Ÿä¸€åŒ–å’Œè§„èŒƒåŒ–ï¼Œä¿ƒè¿›æŠ€æœ¯çš„å¥åº·å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) show significant potential in healthcare, prompting numerous benchmarks to evaluate their capabilities. However, concerns persist regarding the reliability of these benchmarks, which often lack clinical fidelity, robust data management, and safety-oriented evaluation metrics. To address these shortcomings, we introduce MedCheck, the first lifecycle-oriented assessment framework specifically designed for medical benchmarks. Our framework deconstructs a benchmark's development into five continuous stages, from design to governance, and provides a comprehensive checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis uncovers widespread, systemic issues, including a profound disconnect from clinical practice, a crisis of data integrity due to unmitigated contamination risks, and a systematic neglect of safety-critical evaluation dimensions like model robustness and uncertainty awareness. Based on these findings, MedCheck serves as both a diagnostic tool for existing benchmarks and an actionable guideline to foster a more standardized, reliable, and transparent approach to evaluating AI in healthcare.

