---
layout: default
title: Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M
---

# Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09055" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09055v1</a>
  <a href="https://arxiv.org/pdf/2509.09055.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09055v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09055v1', 'Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Piyush Pant

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

**å¤‡æ³¨**: 17 pages, 3 figures. Code and dataset available at https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SFTä¸DPOç»“åˆæå‡OPT-350Må®‰å…¨æ€§ä¸æœ‰ç”¨æ€§ï¼šä¸€ç§ç»¼åˆå¯¹é½ç­–ç•¥ç ”ç©¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹å¯¹é½` `ç›‘ç£å¾®è°ƒ` `ç›´æ¥åå¥½ä¼˜åŒ–` `å®‰å…¨æ€§` `æœ‰ç”¨æ€§` `OPT-350M` `RLHFæ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ä¸æœ‰ç”¨æ€§ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„æ¨¡å‹ä¸Šã€‚
2. è®ºæ–‡æå‡ºç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„ç­–ç•¥ï¼Œä»¥å®ç°æ›´ä¼˜çš„æ¨¡å‹å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSFT+DPOæ¨¡å‹åœ¨æ— å®³ç‡ã€æœ‰ç”¨ç‡å’Œç»¼åˆå¯¹é½å¾—åˆ†ä¸Šå‡ä¼˜äºå•ç‹¬ä½¿ç”¨SFTæˆ–DPOçš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¯¹é½æŠ€æœ¯ï¼Œå³ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä»¥åŠSFT+DPOç»„åˆæ–¹æ³•ï¼Œåœ¨æå‡OPT-350Mè¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åˆ©ç”¨Anthropic Helpful-Harmless RLHFæ•°æ®é›†è®­ç»ƒå¹¶è¯„ä¼°äº†å››ä¸ªæ¨¡å‹ï¼šåŸºç¡€OPT350Mæ¨¡å‹ã€SFTæ¨¡å‹ã€DPOæ¨¡å‹ä»¥åŠSFT+DPOæ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªå…³é”®è¯„ä¼°æŒ‡æ ‡ï¼šæ— å®³ç‡ï¼ˆHmRï¼‰ã€æœ‰ç”¨ç‡ï¼ˆHpRï¼‰å’Œç»¼åˆå¯¹é½å¾—åˆ†ï¼ˆCASï¼‰ï¼Œæ‰€æœ‰è¿™äº›æŒ‡æ ‡å‡æ¥è‡ªå¥–åŠ±æ¨¡å‹è¾“å‡ºã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶SFTä¼˜äºDPOï¼Œä½†SFT+DPOæ¨¡å‹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¯æ˜äº†è¿™äº›æŠ€æœ¯çš„äº’è¡¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¿˜å¼ºè°ƒäº†å™ªå£°æ•°æ®ã€æœ‰é™çš„GPUèµ„æºå’Œè®­ç»ƒçº¦æŸæ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å…¨é¢å±•ç¤ºäº†å¾®è°ƒç­–ç•¥å¦‚ä½•å½±å“æ¨¡å‹å¯¹é½ï¼Œå¹¶ä¸ºæœªæ¥æ›´å¼ºå¤§çš„å¯¹é½ç®¡é“å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆæå‡å°å‹è¯­è¨€æ¨¡å‹ï¼ˆOPT-350Mï¼‰çš„å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§ï¼Œç°æœ‰æ–¹æ³•å¦‚å•ç‹¬ä½¿ç”¨SFTæˆ–DPOå¯èƒ½æ— æ³•è¾¾åˆ°æœ€ä½³æ•ˆæœï¼Œä¸”åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹è®­ç»ƒå¯¹é½æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯å°†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç›¸ç»“åˆï¼Œåˆ©ç”¨SFTå¿«é€Ÿå­¦ä¹ ä»»åŠ¡ï¼ŒDPOåˆ™åŸºäºäººç±»åå¥½è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œäº’è¡¥ä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå®ç°æ›´å¥½çš„æ¨¡å‹å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨Anthropic Helpful-Harmless RLHFæ•°æ®é›†ï¼›2ï¼‰è®­ç»ƒå››ä¸ªæ¨¡å‹ï¼šåŸºç¡€OPT350Mã€SFTæ¨¡å‹ã€DPOæ¨¡å‹å’ŒSFT+DPOæ¨¡å‹ï¼›3ï¼‰ä½¿ç”¨å¥–åŠ±æ¨¡å‹è¯„ä¼°æ¨¡å‹çš„æ— å®³ç‡ï¼ˆHmRï¼‰ã€æœ‰ç”¨ç‡ï¼ˆHpRï¼‰å’Œç»¼åˆå¯¹é½å¾—åˆ†ï¼ˆCASï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå‘ç°SFTå’ŒDPOçš„äº’è¡¥æ€§ï¼Œå¹¶è¯æ˜å°†ä¸¤è€…ç»“åˆä½¿ç”¨å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§ï¼Œä¼˜äºå•ç‹¬ä½¿ç”¨ä»»ä½•ä¸€ç§æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é’ˆå¯¹å°å‹æ¨¡å‹å’Œæœ‰é™èµ„æºç¯å¢ƒè¿›è¡Œäº†ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†Anthropic Helpful-Harmless RLHFæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«äººç±»å¯¹æ¨¡å‹è¾“å‡ºçš„åå¥½ä¿¡æ¯ã€‚SFTä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ŒDPOä½¿ç”¨æ ‡å‡†çš„DPOæŸå¤±å‡½æ•°ã€‚SFT+DPOæ¨¡å‹å…ˆè¿›è¡ŒSFTè®­ç»ƒï¼Œç„¶åå†è¿›è¡ŒDPOè®­ç»ƒã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬æ— å®³ç‡ï¼ˆHmRï¼‰ã€æœ‰ç”¨ç‡ï¼ˆHpRï¼‰å’Œç»¼åˆå¯¹é½å¾—åˆ†ï¼ˆCASï¼‰ï¼Œè¿™äº›æŒ‡æ ‡åŸºäºå¥–åŠ±æ¨¡å‹è¾“å‡ºè®¡ç®—å¾—å‡ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSFT+DPOæ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒSFT+DPOæ¨¡å‹åœ¨æ— å®³ç‡ï¼ˆHmRï¼‰ã€æœ‰ç”¨ç‡ï¼ˆHpRï¼‰å’Œç»¼åˆå¯¹é½å¾—åˆ†ï¼ˆCASï¼‰ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†SFTå’ŒDPOçš„äº’è¡¥æ€§ã€‚è™½ç„¶SFTå•ç‹¬è®­ç»ƒæ•ˆæœä¼˜äºDPOï¼Œä½†SFT+DPOçš„ç»„åˆç­–ç•¥å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å®‰å…¨å’Œæœ‰ç”¨çš„è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€å†…å®¹ç”Ÿæˆã€æ•™è‚²è¾…åŠ©ç­‰ã€‚é€šè¿‡ç»“åˆSFTå’ŒDPOï¼Œå¯ä»¥è®­ç»ƒå‡ºæ›´åŠ ç¬¦åˆäººç±»ä»·å€¼è§‚å’Œéœ€æ±‚çš„è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå¹¶é™ä½æ½œåœ¨é£é™©ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„åœºæ™¯ï¼Œä¸ºå°å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This research investigates the effectiveness of alignment techniques, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a combined SFT+DPO approach on improving the safety and helpfulness of the OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset, we train and evaluate four models: the base OPT350M, an SFT model, a DPO model, and a model trained with both SFT and DPO. We introduce three key evaluation metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined Alignment Score (CAS), all derived from reward model outputs. The results show that while SFT outperforms DPO, The combined SFT+DPO model outperforms all others across all metrics, demonstrating the complementary nature of these techniques. Our findings also highlight challenges posed by noisy data, limited GPU resources, and training constraints. This study offers a comprehensive view of how fine-tuning strategies affect model alignment and provides a foundation for more robust alignment pipelines in future work.

