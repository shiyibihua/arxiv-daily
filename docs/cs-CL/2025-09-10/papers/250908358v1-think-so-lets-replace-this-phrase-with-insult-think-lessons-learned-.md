---
layout: default
title: <think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs
---

# <think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08358" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08358v1</a>
  <a href="https://arxiv.org/pdf/2509.08358.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08358v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08358v1', '<think> So let\'s replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sergey Pletenev, Daniil Moskovskiy, Alexander Panchenko

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¡¨æ˜ï¼šLLMç”Ÿæˆçš„æœ‰æ¯’æ–‡æœ¬åœ¨æ–‡æœ¬è§£æ¯’ä»»åŠ¡ä¸­è¡¨ç°ä¸å¦‚äººå·¥æ ‡æ³¨æ•°æ®**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬è§£æ¯’` `åˆæˆæ•°æ®ç”Ÿæˆ` `æ¯’æ€§æ£€æµ‹` `è¯æ±‡å¤šæ ·æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬è§£æ¯’æ¨¡å‹ä¾èµ–äººå·¥æ ‡æ³¨çš„æœ‰æ¯’æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥è¦†ç›–æ‰€æœ‰æ¯’æ€§è¡¨è¾¾ã€‚
2. æœ¬æ–‡æ¢ç´¢ä½¿ç”¨LLMç”Ÿæˆåˆæˆæœ‰æ¯’æ•°æ®ï¼Œä»¥æ›¿ä»£äººå·¥æ ‡æ³¨æ•°æ®è®­ç»ƒè§£æ¯’æ¨¡å‹ï¼Œé™ä½æˆæœ¬ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMç”Ÿæˆçš„æœ‰æ¯’æ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä½äºäººå·¥æ•°æ®ï¼Œå­˜åœ¨è¯æ±‡å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆåˆæˆæ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ–‡æœ¬è§£æ¯’ç­‰æ•æ„Ÿé¢†åŸŸçš„æ€§èƒ½å°šæœªå—åˆ°ç§‘å­¦ç•Œçš„å……åˆ†å…³æ³¨ã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨LLMç”Ÿæˆçš„åˆæˆæœ‰æ¯’æ•°æ®ä½œä¸ºäººå·¥ç”Ÿæˆæ•°æ®çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºè®­ç»ƒè§£æ¯’æ¨¡å‹çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨Llama 3å’ŒQwenæ¿€æ´»ä¿®è¡¥æ¨¡å‹ï¼Œä¸ºParaDetoxå’ŒSST-2æ•°æ®é›†ä¸­æ€§æ–‡æœ¬ç”Ÿæˆäº†åˆæˆçš„æœ‰æ¯’å¯¹åº”æ–‡æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åˆæˆæ•°æ®ä¸Šå¾®è°ƒçš„æ¨¡å‹å§‹ç»ˆæ¯”åœ¨äººå·¥æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°æ›´å·®ï¼Œè”åˆæŒ‡æ ‡çš„æ€§èƒ½ä¸‹é™é«˜è¾¾30%ã€‚æ ¹æœ¬åŸå› åœ¨äºå…³é”®çš„è¯æ±‡å¤šæ ·æ€§å·®è·ï¼šLLMä½¿ç”¨å°‘é‡é‡å¤çš„ä¾®è¾±æ€§è¯æ±‡ç”Ÿæˆæœ‰æ¯’å†…å®¹ï¼Œæ— æ³•æ•æ‰äººç±»æ¯’æ€§çš„ç»†å¾®å·®åˆ«å’Œå¤šæ ·æ€§ã€‚è¿™äº›å‘ç°çªå‡ºäº†å½“å‰LLMåœ¨è¯¥é¢†åŸŸçš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†å¤šæ ·åŒ–çš„äººå·¥æ ‡æ³¨æ•°æ®å¯¹äºæ„å»ºå¼ºå¤§çš„è§£æ¯’ç³»ç»Ÿçš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨ç ”ç©¶èƒ½å¦ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„åˆæˆæœ‰æ¯’æ–‡æœ¬æ•°æ®ï¼Œæ›¿ä»£äººå·¥æ ‡æ³¨çš„æœ‰æ¯’æ–‡æœ¬æ•°æ®ï¼Œç”¨äºè®­ç»ƒæ–‡æœ¬è§£æ¯’æ¨¡å‹ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§é‡äººå·¥æ ‡æ³¨çš„æœ‰æ¯’æ•°æ®ï¼Œè¿™ä¸ä»…æˆæœ¬é«˜æ˜‚ï¼Œè€Œä¸”éš¾ä»¥è¦†ç›–æ‰€æœ‰ç±»å‹çš„æ¯’æ€§è¡¨è¾¾ï¼Œå­˜åœ¨æ ‡æ³¨åå·®å’Œæ•°æ®ç¨€ç¼ºç­‰é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤§é‡çš„æœ‰æ¯’æ–‡æœ¬æ•°æ®ï¼Œç„¶åä½¿ç”¨è¿™äº›æ•°æ®æ¥è®­ç»ƒæ–‡æœ¬è§£æ¯’æ¨¡å‹ã€‚æœŸæœ›é€šè¿‡è¿™ç§æ–¹å¼ï¼Œé™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæé«˜è§£æ¯’æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°LLMç”Ÿæˆçš„æœ‰æ¯’æ–‡æœ¬å­˜åœ¨è¯æ±‡å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) é€‰æ‹©ä¸­æ€§æ–‡æœ¬æ•°æ®é›†ï¼ˆParaDetoxå’ŒSST-2ï¼‰ï¼›2) ä½¿ç”¨Llama 3å’ŒQwenæ¿€æ´»ä¿®è¡¥æ¨¡å‹ï¼Œå°†ä¸­æ€§æ–‡æœ¬è½¬æ¢ä¸ºæœ‰æ¯’æ–‡æœ¬ï¼›3) ä½¿ç”¨åˆæˆçš„æœ‰æ¯’æ•°æ®å’Œäººå·¥æ ‡æ³¨çš„æœ‰æ¯’æ•°æ®åˆ†åˆ«è®­ç»ƒæ–‡æœ¬è§£æ¯’æ¨¡å‹ï¼›4) è¯„ä¼°ä¸åŒæ¨¡å‹åœ¨è§£æ¯’ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºæ¢ç´¢äº†ä½¿ç”¨LLMç”Ÿæˆåˆæˆæœ‰æ¯’æ•°æ®æ¥è®­ç»ƒè§£æ¯’æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰LLMåœ¨ç”Ÿæˆå¤šæ ·åŒ–æœ‰æ¯’æ–‡æœ¬æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæé«˜è§£æ¯’æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†Llama 3å’ŒQwenæ¿€æ´»ä¿®è¡¥æ¨¡å‹è¿›è¡Œæœ‰æ¯’æ–‡æœ¬ç”Ÿæˆã€‚æ¿€æ´»ä¿®è¡¥æ˜¯ä¸€ç§æ§åˆ¶LLMè¾“å‡ºçš„æŠ€æœ¯ï¼Œé€šè¿‡ä¿®æ”¹ç‰¹å®šç¥ç»å…ƒçš„æ¿€æ´»å€¼æ¥å¼•å¯¼LLMç”ŸæˆæœŸæœ›çš„æ–‡æœ¬ã€‚å®éªŒä¸­ï¼Œä½¿ç”¨äº†ParaDetoxå’ŒSST-2æ•°æ®é›†ä½œä¸ºä¸­æ€§æ–‡æœ¬çš„æ¥æºï¼Œå¹¶é‡‡ç”¨æ ‡å‡†çš„æ–‡æœ¬åˆ†ç±»æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ã€F1å€¼ï¼‰æ¥è¯„ä¼°è§£æ¯’æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LLMç”Ÿæˆçš„åˆæˆæœ‰æ¯’æ•°æ®è®­ç»ƒçš„è§£æ¯’æ¨¡å‹ï¼Œå…¶æ€§èƒ½æ¯”ä½¿ç”¨äººå·¥æ ‡æ³¨æ•°æ®è®­ç»ƒçš„æ¨¡å‹ä¸‹é™é«˜è¾¾30%ã€‚è¿™è¡¨æ˜å½“å‰LLMåœ¨ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„æœ‰æ¯’æ–‡æœ¬æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è¯æ±‡å¤šæ ·æ€§æ–¹é¢ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†äººå·¥æ ‡æ³¨æ•°æ®åœ¨æ„å»ºé²æ£’è§£æ¯’ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ç»“æœå¯¹æ–‡æœ¬è§£æ¯’ã€å†…å®¹å®¡æ ¸ã€ä»¥åŠå…¶ä»–éœ€è¦å¤„ç†æœ‰å®³ä¿¡æ¯çš„é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚è™½ç„¶å½“å‰LLMç”Ÿæˆçš„æœ‰æ¯’æ•°æ®æ•ˆæœä¸ä½³ï¼Œä½†æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢æ”¹è¿›LLMç”Ÿæˆå¤šæ ·åŒ–æ¯’æ€§æ–‡æœ¬çš„æ–¹æ³•ï¼Œä»è€Œé™ä½å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œæå‡ç›¸å…³åº”ç”¨çš„æ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern Large Language Models (LLMs) are excellent at generating synthetic data. However, their performance in sensitive domains such as text detoxification has not received proper attention from the scientific community. This paper explores the possibility of using LLM-generated synthetic toxic data as an alternative to human-generated data for training models for detoxification. Using Llama 3 and Qwen activation-patched models, we generated synthetic toxic counterparts for neutral texts from ParaDetox and SST-2 datasets. Our experiments show that models fine-tuned on synthetic data consistently perform worse than those trained on human data, with a drop in performance of up to 30% in joint metrics. The root cause is identified as a critical lexical diversity gap: LLMs generate toxic content using a small, repetitive vocabulary of insults that fails to capture the nuances and variety of human toxicity. These findings highlight the limitations of current LLMs in this domain and emphasize the continued importance of diverse, human-annotated data for building robust detoxification systems.

