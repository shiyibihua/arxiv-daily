---
layout: default
title: Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation
---

# Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09735" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09735v1</a>
  <a href="https://arxiv.org/pdf/2509.09735.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09735v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09735v1', 'Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Willem Huijzer, Jieying Chen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

**å¤‡æ³¨**: 7 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¹¶ç¼“è§£LLMåœ¨å†³ç­–å’Œæ‘˜è¦ä»»åŠ¡ä¸­çš„è·¨è¯­è¨€åè§ï¼Œå…³æ³¨èƒŒæ™¯ã€æ€§åˆ«å’Œå¹´é¾„æ­§è§†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åè§è¯„ä¼°` `è·¨è¯­è¨€åˆ†æ` `å†³ç­–ä»»åŠ¡` `æ‘˜è¦ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨å†³ç­–å’Œæ‘˜è¦ä»»åŠ¡ä¸­å­˜åœ¨åè§ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠä¸åŒèƒŒæ™¯ã€æ€§åˆ«å’Œå¹´é¾„çš„äººç¾¤æ—¶ï¼Œå¯èƒ½å¯¼è‡´ä¸å…¬å¹³çš„ç»“æœã€‚
2. è¯¥ç ”ç©¶é€šè¿‡æ„å»ºåŒ…å«å¤šç§äººå£ç»Ÿè®¡å˜é‡å’ŒæŒ‡ä»¤çš„æç¤ºï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†GPT-3.5å’ŒGPT-4oåœ¨å†³ç­–å’Œæ‘˜è¦ä»»åŠ¡ä¸­çš„åè§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMåœ¨å†³ç­–ä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—åè§ï¼Œä¸”åè§æ¨¡å¼åœ¨ä¸åŒè¯­è¨€é—´å…·æœ‰ç›¸ä¼¼æ€§ï¼Œæç¤ºæŒ‡å¯¼çš„ç¼“è§£ç­–ç•¥èƒ½å¤Ÿéƒ¨åˆ†å‡å°‘åè§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¿«é€Ÿèå…¥å„ä¸ªé¢†åŸŸï¼Œå¼•å‘äº†å¯¹ç¤¾ä¼šä¸å¹³ç­‰å’Œä¿¡æ¯åè§çš„æ‹…å¿§ã€‚æœ¬ç ”ç©¶è€ƒå¯Ÿäº†LLMä¸­ä¸èƒŒæ™¯ã€æ€§åˆ«å’Œå¹´é¾„ç›¸å…³çš„åè§ï¼Œé‡ç‚¹å…³æ³¨å…¶å¯¹å†³ç­–å’Œæ‘˜è¦ä»»åŠ¡çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ£€éªŒäº†è¿™äº›åè§çš„è·¨è¯­è¨€ä¼ æ’­ï¼Œå¹¶è¯„ä¼°äº†æç¤ºæŒ‡å¯¼ç¼“è§£ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨Tamkinç­‰äººï¼ˆ2023ï¼‰æ•°æ®é›†çš„æ”¹ç¼–ç‰ˆæœ¬ï¼ˆç¿»è¯‘æˆè·å…°è¯­ï¼‰ï¼Œä¸ºå†³ç­–ä»»åŠ¡åˆ›å»ºäº†151,200ä¸ªç‹¬ç‰¹çš„æç¤ºï¼Œä¸ºæ‘˜è¦ä»»åŠ¡åˆ›å»ºäº†176,400ä¸ªã€‚åœ¨GPT-3.5å’ŒGPT-4oä¸Šæµ‹è¯•äº†å„ç§äººå£ç»Ÿè®¡å˜é‡ã€æŒ‡ä»¤ã€æ˜¾è‘—æ€§æ°´å¹³å’Œè¯­è¨€ã€‚åˆ†æè¡¨æ˜ï¼Œä¸¤ç§æ¨¡å‹åœ¨å†³ç­–è¿‡ç¨‹ä¸­éƒ½å­˜åœ¨æ˜¾è‘—åè§ï¼Œåå‘äºå¥³æ€§ã€å¹´è½»å¹´é¾„å’ŒæŸäº›èƒŒæ™¯ï¼ˆå¦‚éè£”ç¾å›½äººèƒŒæ™¯ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ‘˜è¦ä»»åŠ¡æ˜¾ç¤ºå‡ºæå°‘çš„åè§è¯æ®ï¼Œä½†GPT-3.5åœ¨è‹±è¯­ä¸­å‡ºç°äº†æ˜¾è‘—çš„å¹´é¾„ç›¸å…³å·®å¼‚ã€‚è·¨è¯­è¨€åˆ†æè¡¨æ˜ï¼Œè‹±è¯­å’Œè·å…°è¯­ä¹‹é—´çš„åè§æ¨¡å¼å¤§è‡´ç›¸ä¼¼ï¼Œä½†åœ¨ç‰¹å®šäººå£ç»Ÿè®¡ç±»åˆ«ä¸­è§‚å¯Ÿåˆ°æ˜¾è‘—å·®å¼‚ã€‚æ–°æå‡ºçš„ç¼“è§£æŒ‡ä»¤è™½ç„¶æ— æ³•å®Œå…¨æ¶ˆé™¤åè§ï¼Œä½†æ˜¾ç¤ºå‡ºå‡å°‘åè§çš„æ½œåŠ›ã€‚æœ€æœ‰æ•ˆçš„æŒ‡ä»¤å¹³å‡å‡å°‘äº†27%çš„æœ€æœ‰åˆ©å’Œæœ€ä¸åˆ©äººå£ç»Ÿè®¡æ•°æ®ä¹‹é—´çš„å·®è·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸GPT-3.5ç›¸åï¼ŒGPT-4oåœ¨è‹±è¯­çš„æ‰€æœ‰æç¤ºä¸­éƒ½æ˜¾ç¤ºå‡ºåè§å‡å°‘ï¼Œè¡¨æ˜äº†æ–°æ¨¡å‹ä¸­åŸºäºæç¤ºçš„ç¼“è§£çš„ç‰¹å®šæ½œåŠ›ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†è°¨æ…é‡‡ç”¨LLMå’Œç‰¹å®šäºä¸Šä¸‹æ–‡çš„åè§æµ‹è¯•çš„é‡è¦æ€§ï¼Œçªå‡ºäº†æŒç»­å¼€å‘æœ‰æ•ˆç¼“è§£ç­–ç•¥ä»¥ç¡®ä¿è´Ÿè´£ä»»åœ°éƒ¨ç½²AIçš„å¿…è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨é‡åŒ–å’Œå‡è½»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å†³ç­–å’Œæ‘˜è¦ä»»åŠ¡ä¸­å­˜åœ¨çš„åè§ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹è·¨è¯­è¨€åè§ä¼ æ’­çš„ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œå¹¶ä¸”ç¼“è§£ç­–ç•¥çš„æ•ˆæœæœ‰å¾…éªŒè¯ã€‚LLMçš„åè§å¯èƒ½å¯¼è‡´å¯¹ç‰¹å®šäººç¾¤çš„ä¸å…¬å¹³å¾…é‡ï¼Œå› æ­¤éœ€è¦æ·±å…¥ç ”ç©¶å¹¶æå‡ºæœ‰æ•ˆçš„ç¼“è§£æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºåŒ…å«ä¸åŒäººå£ç»Ÿè®¡å˜é‡ï¼ˆèƒŒæ™¯ã€æ€§åˆ«ã€å¹´é¾„ï¼‰å’ŒæŒ‡ä»¤çš„æç¤ºï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMåœ¨å†³ç­–å’Œæ‘˜è¦ä»»åŠ¡ä¸­çš„åè§ã€‚åŒæ—¶ï¼Œé€šè¿‡å°†æç¤ºç¿»è¯‘æˆå¤šç§è¯­è¨€ï¼Œç ”ç©¶åè§çš„è·¨è¯­è¨€ä¼ æ’­ã€‚æ­¤å¤–ï¼Œè®¾è®¡åŸºäºæç¤ºçš„ç¼“è§£ç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘LLMçš„åè§è¾“å‡ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®é›†æ„å»ºï¼šåŸºäºTamkinç­‰äºº(2023)çš„æ•°æ®é›†ï¼Œæ„å»ºåŒ…å«å¤šç§äººå£ç»Ÿè®¡å˜é‡å’ŒæŒ‡ä»¤çš„æç¤ºï¼Œå¹¶ç¿»è¯‘æˆè·å…°è¯­ã€‚2) æ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨GPT-3.5å’ŒGPT-4oå¯¹æ„å»ºçš„æç¤ºè¿›è¡Œè¯„ä¼°ï¼Œåˆ†æå…¶åœ¨å†³ç­–å’Œæ‘˜è¦ä»»åŠ¡ä¸­çš„åè§ã€‚3) è·¨è¯­è¨€åˆ†æï¼šæ¯”è¾ƒè‹±è¯­å’Œè·å…°è¯­çš„åè§æ¨¡å¼ï¼Œç ”ç©¶åè§çš„è·¨è¯­è¨€ä¼ æ’­ã€‚4) ç¼“è§£ç­–ç•¥ï¼šè®¾è®¡åŸºäºæç¤ºçš„ç¼“è§£ç­–ç•¥ï¼Œå¹¶è¯„ä¼°å…¶æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºï¼š1) ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†LLMåœ¨å†³ç­–å’Œæ‘˜è¦ä»»åŠ¡ä¸­çš„è·¨è¯­è¨€åè§ã€‚2) æå‡ºäº†åŸºäºæç¤ºçš„ç¼“è§£ç­–ç•¥ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨å‡å°‘åè§æ–¹é¢çš„æ½œåŠ›ã€‚3) æ­ç¤ºäº†GPT-4oåœ¨å‡å°‘åè§æ–¹é¢ä¼˜äºGPT-3.5çš„ç‰¹æ€§ï¼Œè¡¨æ˜äº†æ–°æ¨¡å‹åœ¨åè§ç¼“è§£æ–¹é¢çš„æ½œåŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥ç ”ç©¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ„å»ºåŒ…å«å¤šç§äººå£ç»Ÿè®¡å˜é‡ï¼ˆèƒŒæ™¯ã€æ€§åˆ«ã€å¹´é¾„ï¼‰å’ŒæŒ‡ä»¤çš„æç¤ºï¼Œä»¥å…¨é¢è¯„ä¼°LLMçš„åè§ã€‚2) ä½¿ç”¨å†³ç­–å’Œæ‘˜è¦ä¸¤ç§ä»»åŠ¡ï¼Œä»¥ç ”ç©¶åè§åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚3) å°†æç¤ºç¿»è¯‘æˆè·å…°è¯­ï¼Œä»¥ç ”ç©¶åè§çš„è·¨è¯­è¨€ä¼ æ’­ã€‚4) è®¾è®¡åŸºäºæç¤ºçš„ç¼“è§£ç­–ç•¥ï¼Œä¾‹å¦‚ï¼ŒæŒ‡ç¤ºæ¨¡å‹è€ƒè™‘å…¬å¹³æ€§æˆ–æä¾›æ›´å¹³è¡¡çš„è¾“å‡ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-3.5å’ŒGPT-4oåœ¨å†³ç­–ä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—åè§ï¼Œåå‘äºå¥³æ€§ã€å¹´è½»å¹´é¾„å’ŒæŸäº›èƒŒæ™¯ã€‚è·¨è¯­è¨€åˆ†æè¡¨æ˜ï¼Œè‹±è¯­å’Œè·å…°è¯­ä¹‹é—´çš„åè§æ¨¡å¼å¤§è‡´ç›¸ä¼¼ã€‚æå‡ºçš„ç¼“è§£æŒ‡ä»¤èƒ½å¤Ÿéƒ¨åˆ†å‡å°‘åè§ï¼Œæœ€æœ‰æ•ˆçš„æŒ‡ä»¤å¹³å‡å‡å°‘äº†27%çš„æœ€æœ‰åˆ©å’Œæœ€ä¸åˆ©äººå£ç»Ÿè®¡æ•°æ®ä¹‹é—´çš„å·®è·ã€‚GPT-4oåœ¨å‡å°‘åè§æ–¹é¢ä¼˜äºGPT-3.5ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä½¿ç”¨LLMè¿›è¡Œå†³ç­–æˆ–å†…å®¹ç”Ÿæˆçš„é¢†åŸŸï¼Œä¾‹å¦‚æ‹›è˜ã€ä¿¡è´·è¯„ä¼°ã€æ–°é—»æ‘˜è¦ç­‰ã€‚é€šè¿‡è¯†åˆ«å’Œç¼“è§£LLMä¸­çš„åè§ï¼Œå¯ä»¥æé«˜å†³ç­–çš„å…¬å¹³æ€§å’Œé€æ˜åº¦ï¼Œé¿å…å¯¹ç‰¹å®šäººç¾¤é€ æˆæ­§è§†ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ›´æœ‰æ•ˆçš„ç¼“è§£ç­–ç•¥ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ›´å¹¿æ³›çš„LLMå’Œä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid integration of Large Language Models (LLMs) into various domains raises concerns about societal inequalities and information bias. This study examines biases in LLMs related to background, gender, and age, with a focus on their impact on decision-making and summarization tasks. Additionally, the research examines the cross-lingual propagation of these biases and evaluates the effectiveness of prompt-instructed mitigation strategies. Using an adapted version of the dataset by Tamkin et al. (2023) translated into Dutch, we created 151,200 unique prompts for the decision task and 176,400 for the summarisation task. Various demographic variables, instructions, salience levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed that both models were significantly biased during decision-making, favouring female gender, younger ages, and certain backgrounds such as the African-American background. In contrast, the summarisation task showed minimal evidence of bias, though significant age-related differences emerged for GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were broadly similar between English and Dutch, though notable differences were observed across specific demographic categories. The newly proposed mitigation instructions, while unable to eliminate biases completely, demonstrated potential in reducing them. The most effective instruction achieved a 27\% mean reduction in the gap between the most and least favorable demographics. Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts in English, indicating the specific potential for prompt-based mitigation within newer models. This research underscores the importance of cautious adoption of LLMs and context-specific bias testing, highlighting the need for continued development of effective mitigation strategies to ensure responsible deployment of AI.

