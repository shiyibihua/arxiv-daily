---
layout: default
title: Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora
---

# Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08824" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08824v1</a>
  <a href="https://arxiv.org/pdf/2509.08824.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08824v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08824v1', 'Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thales Sales Almeida, Rodrigo Nogueira, Helio Pedrini

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‘¡è„ç‰™è¯­LLMé«˜è´¨é‡æ•°æ®é›†æ„å»ºæ–¹æ³•ï¼Œæ€§èƒ½åª²ç¾å·¥ä¸šçº§è¯­æ–™åº“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‘¡è„ç‰™è¯­LLM` `æ•°æ®é›†æ„å»º` `æ•°æ®è¿‡æ»¤` `æŒç»­é¢„è®­ç»ƒ` `å¤šè¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè®­ç»ƒæ•°æ®æ„å»ºä¸»è¦é›†ä¸­äºè‹±è¯­ï¼Œç¼ºä¹é’ˆå¯¹å…¶ä»–è¯­è¨€çš„æœ‰æ•ˆæ–¹æ³•ã€‚
2. è®ºæ–‡æ¢ç´¢äº†å¯æ‰©å±•çš„Webè¯­æ–™åº“æ„å»ºæ–¹æ³•ï¼Œå¹¶é’ˆå¯¹è‘¡è„ç‰™è¯­è¿›è¡Œäº†ä¼˜åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç‰¹å®šè¯­è¨€çš„è¿‡æ»¤å’Œé¢„å¤„ç†èƒ½æœ‰æ•ˆæå‡LLMåœ¨ç›®æ ‡è¯­è¨€ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½æ·±å—å…¶è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œç»„æˆçš„å½±å“ã€‚è™½ç„¶ç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä¸Šï¼Œä½†å¯¹äºå¦‚ä½•æ„å»ºå…¶ä»–è¯­è¨€çš„æœ‰æ•ˆè®­ç»ƒè¯­æ–™åº“ä»ç„¶å­˜åœ¨å·®è·ã€‚æœ¬æ–‡æ¢ç´¢äº†ç”¨äºæ„å»ºLLMçš„åŸºäºWebçš„å¯æ‰©å±•è¯­æ–™åº“çš„æ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ„å»ºä¸€ä¸ªæ–°çš„1200äº¿tokençš„è‘¡è„ç‰™è¯­è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“çš„æ€§èƒ½ä¸å·¥ä¸šçº§è¯­æ–™åº“ç›¸å½“ã€‚é€šè¿‡æŒç»­é¢„è®­ç»ƒè®¾ç½®ï¼Œç ”ç©¶äº†ä¸åŒçš„æ•°æ®é€‰æ‹©å’Œé¢„å¤„ç†ç­–ç•¥åœ¨å°†æœ€åˆç”¨è‹±è¯­è®­ç»ƒçš„æ¨¡å‹è¿‡æ¸¡åˆ°å¦ä¸€ç§è¯­è¨€æ—¶å¦‚ä½•å½±å“LLMçš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜äº†ç‰¹å®šäºè¯­è¨€çš„è¿‡æ»¤ç®¡é“çš„ä»·å€¼ï¼ŒåŒ…æ‹¬ç”¨äºæ•™è‚²ã€ç§‘å­¦ã€æŠ€æœ¯ã€å·¥ç¨‹å’Œæ•°å­¦ï¼ˆSTEMï¼‰ä»¥åŠæœ‰å®³å†…å®¹çš„åˆ†ç±»å™¨ã€‚ç»“æœè¡¨æ˜ï¼Œå°†æ¨¡å‹é€‚åº”ç›®æ ‡è¯­è¨€å¯ä»¥æé«˜æ€§èƒ½ï¼Œä»è€ŒåŠ å¼ºäº†é«˜è´¨é‡ã€ç‰¹å®šäºè¯­è¨€çš„æ•°æ®çš„é‡è¦æ€§ã€‚è™½ç„¶æ¡ˆä¾‹ç ”ç©¶ä¾§é‡äºè‘¡è„ç‰™è¯­ï¼Œä½†æœ¬æ–‡çš„æ–¹æ³•é€‚ç”¨äºå…¶ä»–è¯­è¨€ï¼Œä¸ºå¤šè¯­è¨€LLMçš„å¼€å‘æä¾›äº†è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä¸ºè‘¡è„ç‰™è¯­ç­‰éè‹±è¯­è¯­è¨€æ„å»ºé«˜è´¨é‡LLMè®­ç»ƒè¯­æ–™åº“çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éè‹±è¯­è¯­æ–™æ—¶ï¼Œç¼ºä¹é’ˆå¯¹ç‰¹å®šè¯­è¨€çš„æœ‰æ•ˆè¿‡æ»¤å’Œé¢„å¤„ç†ç­–ç•¥ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚æ­¤å¤–ï¼Œå¦‚ä½•é«˜æ•ˆåœ°ä»å¤§è§„æ¨¡Webæ•°æ®ä¸­æå–é«˜è´¨é‡æ•°æ®ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªç‰¹å®šäºè‘¡è„ç‰™è¯­çš„ã€åŒ…å«é«˜è´¨é‡å†…å®¹çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡æŒç»­é¢„è®­ç»ƒçš„æ–¹å¼ï¼Œå°†ä¸€ä¸ªå·²ç»åœ¨è‹±è¯­ä¸Šè®­ç»ƒè¿‡çš„LLMè¿ç§»åˆ°è‘¡è„ç‰™è¯­ã€‚é€šè¿‡è¯­è¨€ç›¸å…³çš„è¿‡æ»¤ç®¡é“ï¼ŒåŒ…æ‹¬STEMå’Œæœ‰å®³å†…å®¹åˆ†ç±»å™¨ï¼Œæ¥æé«˜æ•°æ®é›†çš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä»Common Crawlç­‰Webæ•°æ®æºæ”¶é›†åŸå§‹æ–‡æœ¬æ•°æ®ï¼›2) åº”ç”¨è¯­è¨€è¯†åˆ«æŠ€æœ¯ç­›é€‰å‡ºè‘¡è„ç‰™è¯­æ–‡æœ¬ï¼›3) ä½¿ç”¨ç‰¹å®šäºè¯­è¨€çš„è¿‡æ»¤ç®¡é“ï¼ŒåŒ…æ‹¬STEMå†…å®¹åˆ†ç±»å™¨å’Œæœ‰å®³å†…å®¹æ£€æµ‹å™¨ï¼Œè¿‡æ»¤ä½è´¨é‡å’Œæœ‰å®³å†…å®¹ï¼›4) å¯¹è¿‡æ»¤åçš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä¾‹å¦‚å»é™¤é‡å¤æ–‡æœ¬ã€æ ‡å‡†åŒ–æ ¼å¼ç­‰ï¼›5) ä½¿ç”¨å¤„ç†åçš„æ•°æ®å¯¹LLMè¿›è¡ŒæŒç»­é¢„è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªé’ˆå¯¹è‘¡è„ç‰™è¯­çš„ã€åŒ…å«é«˜è´¨é‡å†…å®¹çš„æ•°æ®é›†æ„å»ºæµç¨‹ï¼Œå¹¶éªŒè¯äº†è¯¥æµç¨‹çš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«åœ°ï¼Œè®ºæ–‡å¼ºè°ƒäº†è¯­è¨€ç‰¹å®šè¿‡æ»¤ç®¡é“çš„é‡è¦æ€§ï¼Œä¾‹å¦‚STEMå†…å®¹åˆ†ç±»å™¨å’Œæœ‰å®³å†…å®¹æ£€æµ‹å™¨ï¼Œè¿™äº›ç®¡é“èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æ•°æ®é›†çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼ŒSTEMå†…å®¹åˆ†ç±»å™¨å’Œæœ‰å®³å†…å®¹æ£€æµ‹å™¨æ˜¯å…³é”®è®¾è®¡ã€‚STEMå†…å®¹åˆ†ç±»å™¨ç”¨äºè¯†åˆ«å’Œä¿ç•™ä¸æ•™è‚²ã€ç§‘å­¦ã€æŠ€æœ¯ã€å·¥ç¨‹å’Œæ•°å­¦ç›¸å…³çš„é«˜è´¨é‡å†…å®¹ã€‚æœ‰å®³å†…å®¹æ£€æµ‹å™¨ç”¨äºè¯†åˆ«å’Œè¿‡æ»¤åŒ…å«ä»‡æ¨è¨€è®ºã€è¾±éª‚ç­‰æœ‰å®³å†…å®¹ã€‚è¿™äº›åˆ†ç±»å™¨å¯ä»¥ä½¿ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿå¯ä»¥ä»å¤´å¼€å§‹è®­ç»ƒã€‚æ­¤å¤–ï¼ŒæŒç»­é¢„è®­ç»ƒçš„å‚æ•°è®¾ç½®ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€batch sizeç­‰ï¼Œä¹Ÿéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æ„å»ºäº†ä¸€ä¸ª1200äº¿tokençš„è‘¡è„ç‰™è¯­è¯­æ–™åº“ï¼Œå¹¶ä½¿ç”¨è¯¥è¯­æ–™åº“å¯¹LLMè¿›è¡Œäº†æŒç»­é¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥è¯­æ–™åº“è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¸ä½¿ç”¨å·¥ä¸šçº§è¯­æ–™åº“è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ç›¸å½“ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œç‰¹å®šäºè¯­è¨€çš„è¿‡æ»¤ç®¡é“èƒ½å¤Ÿæ˜¾è‘—æé«˜æ•°æ®é›†çš„è´¨é‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ„å»ºå„ç§éè‹±è¯­è¯­è¨€çš„é«˜è´¨é‡LLMè®­ç»ƒæ•°æ®é›†ï¼Œæå‡å¤šè¯­è¨€LLMçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨æœºå™¨ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ã€å¤šè¯­è¨€å†…å®¹ç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œæœ‰åŠ©äºæ¨åŠ¨å…¨çƒèŒƒå›´å†…çš„äººå·¥æ™ºèƒ½å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The performance of large language models (LLMs) is deeply influenced by the quality and composition of their training data. While much of the existing work has centered on English, there remains a gap in understanding how to construct effective training corpora for other languages. We explore scalable methods for building web-based corpora for LLMs. We apply them to build a new 120B token corpus in Portuguese that achieves competitive results to an industrial-grade corpus. Using a continual pretraining setup, we study how different data selection and preprocessing strategies affect LLM performance when transitioning a model originally trained in English to another language. Our findings demonstrate the value of language-specific filtering pipelines, including classifiers for education, science, technology, engineering, and mathematics (STEM), as well as toxic content. We show that adapting a model to the target language leads to performance improvements, reinforcing the importance of high-quality, language-specific data. While our case study focuses on Portuguese, our methods are applicable to other languages, offering insights for multilingual LLM development.

