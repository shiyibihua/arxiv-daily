---
layout: default
title: A Survey of Reinforcement Learning for Large Reasoning Models
---

# A Survey of Reinforcement Learning for Large Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08827" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08827v3</a>
  <a href="https://arxiv.org/pdf/2509.08827.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08827v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08827v3', 'A Survey of Reinforcement Learning for Large Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10 (æ›´æ–°: 2025-10-09)

**å¤‡æ³¨**: Fixed typos; added missing and recent citations (117 -> 120 pages)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°ï¼šå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„å¤§å‹æ¨ç†æ¨¡å‹ç ”ç©¶è¿›å±•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `ç»¼è¿°` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•ã€‚
2. åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒLLMï¼Œä½¿å…¶å…·å¤‡æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¯è§£å†³è¯¥é—®é¢˜çš„æ ¸å¿ƒæ€è·¯ã€‚
3. è¯¥ç»¼è¿°å…¨é¢å›é¡¾äº†RLåœ¨LLMæ¨ç†èƒ½åŠ›æå‡æ–¹é¢çš„ç ”ç©¶è¿›å±•ï¼Œå¹¶å±•æœ›äº†æœªæ¥å‘å±•æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç»¼è¿°äº†è¿‘å¹´æ¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚RLåœ¨æå‡LLMèƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³æ•°å­¦å’Œç¼–ç¨‹ç­‰å¤æ‚é€»è¾‘ä»»åŠ¡æ–¹é¢ã€‚å› æ­¤ï¼ŒRLå·²æˆä¸ºå°†LLMè½¬åŒ–ä¸ºå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„åŸºç¡€æ–¹æ³•ã€‚éšç€è¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œè¿›ä¸€æ­¥æ‰©å±•RLä»¥åº”ç”¨äºLRMä¸ä»…é¢ä¸´è®¡ç®—èµ„æºæ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¿˜åœ¨ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½æ–¹é¢é¢ä¸´åŸºç¡€æ€§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œé‡æ–°å®¡è§†è¯¥é¢†åŸŸçš„å‘å±•å†ç¨‹ï¼Œé‡æ–°è¯„ä¼°å…¶å‘å±•è½¨è¿¹ï¼Œå¹¶æ¢ç´¢å¢å¼ºRLå¯æ‰©å±•æ€§çš„ç­–ç•¥ä»¥å®ç°äººå·¥è¶…æ™ºèƒ½ï¼ˆASIï¼‰æ­£å½“å…¶æ—¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†RLåº”ç”¨äºLLMå’ŒLRMä»¥æé«˜æ¨ç†èƒ½åŠ›çš„ç ”ç©¶ï¼Œå°¤å…¶æ˜¯åœ¨DeepSeek-R1å‘å¸ƒä¹‹åï¼ŒåŒ…æ‹¬åŸºç¡€ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ï¼Œä»¥è¯†åˆ«è¿™ä¸ªå¿«é€Ÿå‘å±•é¢†åŸŸçš„æœªæ¥æœºé‡å’Œæ–¹å‘ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡ç»¼è¿°èƒ½å¤Ÿä¿ƒè¿›æœªæ¥å¯¹æ›´å¹¿æ³›æ¨ç†æ¨¡å‹çš„RLç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚é€»è¾‘ä»»åŠ¡ï¼Œå¦‚æ•°å­¦å’Œç¼–ç¨‹æ—¶ï¼Œä»å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œç®—æ³•è®¾è®¡ã€‚æ­¤å¤–ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§ï¼Œè®¡ç®—èµ„æºã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½ä¹Ÿæˆä¸ºé™åˆ¶RLåº”ç”¨äºLRMçš„ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»¼è¿°å½“å‰åˆ©ç”¨RLæå‡LLMæ¨ç†èƒ½åŠ›çš„ç ”ç©¶è¿›å±•ï¼Œå¹¶åˆ†æå…¶é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥çš„å‘å±•æ–¹å‘ã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•çš„æ¢³ç†å’Œæ€»ç»“ï¼Œä¸ºåç»­ç ”ç©¶æä¾›å‚è€ƒï¼Œå¹¶ä¿ƒè¿›RLåœ¨æ›´å¹¿æ³›æ¨ç†æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥è®ºæ–‡æ˜¯ä¸€ç¯‡ç»¼è¿°ï¼Œå…¶æŠ€æœ¯æ¡†æ¶ä¸»è¦ä½“ç°åœ¨å¯¹ç°æœ‰ç ”ç©¶çš„åˆ†ç±»å’Œæ€»ç»“ä¸Šã€‚å®ƒæ¶µç›–äº†RLåº”ç”¨äºLLMæ¨ç†çš„åŸºç¡€ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ç­‰æ–¹é¢ã€‚é€šè¿‡å¯¹è¿™äº›æ–¹é¢çš„åˆ†æï¼Œè®ºæ–‡è¯•å›¾æ„å»ºä¸€ä¸ªå®Œæ•´çš„RL for LRMçš„çŸ¥è¯†ä½“ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹RLåœ¨LLMæ¨ç†é¢†åŸŸåº”ç”¨çš„å…¨é¢ç»¼è¿°å’Œæœªæ¥æ–¹å‘çš„å±•æœ›ã€‚å®ƒä¸ä»…æ€»ç»“äº†ç°æœ‰æ–¹æ³•çš„ä¼˜ç‚¹å’Œä¸è¶³ï¼Œè¿˜æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶å¯èƒ½é¢ä¸´çš„æŒ‘æˆ˜å’Œæœºé‡ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥è®ºæ–‡çš„å…³é”®è®¾è®¡åœ¨äºå…¶å¯¹ç°æœ‰ç ”ç©¶çš„åˆ†ç±»å’Œç»„ç»‡æ–¹å¼ã€‚å®ƒä»åŸºç¡€ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ç­‰å¤šä¸ªç»´åº¦å¯¹RL for LRMçš„ç ”ç©¶è¿›è¡Œäº†æ¢³ç†ï¼Œä½¿å¾—è¯»è€…èƒ½å¤Ÿæ›´æ¸…æ™°åœ°äº†è§£è¯¥é¢†åŸŸçš„å‘å±•ç°çŠ¶å’Œæœªæ¥è¶‹åŠ¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°é‡ç‚¹å…³æ³¨äº†DeepSeek-R1å‘å¸ƒåï¼ŒRLåœ¨LLMæ¨ç†èƒ½åŠ›æå‡æ–¹é¢çš„ç ”ç©¶è¿›å±•ï¼Œæ¶µç›–äº†åŸºç¡€ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ç­‰å¤šä¸ªæ–¹é¢ã€‚é€šè¿‡å¯¹ç°æœ‰ç ”ç©¶çš„æ¢³ç†å’Œæ€»ç»“ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶å¯èƒ½é¢ä¸´çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯¹å¼€å‘å…·æœ‰æ›´å¼ºæ¨ç†èƒ½åŠ›çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–ç¼–ç¨‹ã€ç§‘å­¦ç ”ç©¶ç­‰ã€‚é€šè¿‡æå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°ç†è§£å’Œè§£å†³å¤æ‚é—®é¢˜ï¼Œä»è€Œåœ¨å„ä¸ªé¢†åŸŸå‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

