---
layout: default
title: Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation
---

# Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08825" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08825v2</a>
  <a href="https://arxiv.org/pdf/2509.08825.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08825v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08825v2', 'Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Joachim Baumann, Paul RÃ¶ttger, Aleksandra Urman, Albert WendsjÃ¶, Flor Miriam Plaza-del-Arco, Johannes B. Gruber, Dirk Hovy

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10 (æ›´æ–°: 2025-10-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é‡åŒ–LLMæ–‡æœ¬æ ‡æ³¨çš„æ½œåœ¨é£é™©ï¼šæ­ç¤ºLLMç ´è§£ç°è±¡åŠåº”å¯¹ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬æ ‡æ³¨` `LLMç ´è§£` `é£é™©è¯„ä¼°` `ç¤¾ä¼šç§‘å­¦ç ”ç©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–LLMè¿›è¡Œæ–‡æœ¬æ ‡æ³¨ï¼Œä½†æ¨¡å‹é€‰æ‹©å’Œæç¤ºç­–ç•¥çš„å·®å¼‚å¯èƒ½å¯¼è‡´åå·®å’Œé”™è¯¯ç»“è®ºã€‚
2. è®ºæ–‡æå‡ºâ€œLLMç ´è§£â€æ¦‚å¿µï¼ŒæŒ‡é…ç½®é€‰æ‹©å¯¼è‡´é”™è¯¯ç»“è®ºçš„ç°è±¡ï¼Œå¹¶ç ”ç©¶æœ‰æ„å’Œæ„å¤–çš„ç ´è§£é£é™©ã€‚
3. å®éªŒè¡¨æ˜LLMç ´è§£é£é™©é«˜ï¼Œå³ä½¿æ˜¯å…ˆè¿›æ¨¡å‹ä¹Ÿæ˜“å—å½±å“ï¼Œäººå·¥æ ‡æ³¨å’Œå›å½’æ ¡æ­£å¯æœ‰æ•ˆç¼“è§£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£è¿…é€Ÿæ”¹å˜ç¤¾ä¼šç§‘å­¦ç ”ç©¶ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®æ ‡æ³¨å’Œæ–‡æœ¬åˆ†æç­‰åŠ³åŠ¨å¯†é›†å‹ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒLLMçš„è¾“å‡ºç»“æœå› ç ”ç©¶äººå‘˜çš„å®æ–½é€‰æ‹©ï¼ˆå¦‚æ¨¡å‹é€‰æ‹©æˆ–æç¤ºç­–ç•¥ï¼‰è€Œå¼‚ï¼Œè¿™å¯èƒ½å¼•å…¥ç³»ç»Ÿæ€§åå·®å’Œéšæœºè¯¯å·®ï¼Œè¿›è€Œå¯¼è‡´Iå‹ï¼ˆå‡é˜³æ€§ï¼‰ã€IIå‹ï¼ˆå‡é˜´æ€§ï¼‰ã€Så‹ï¼ˆé”™è¯¯ç¬¦å·ï¼‰æˆ–Må‹ï¼ˆå¤¸å¤§æ•ˆåº”ï¼‰é”™è¯¯ã€‚æˆ‘ä»¬å°†è¿™ç§é…ç½®é€‰æ‹©å¯¼è‡´é”™è¯¯ç»“è®ºçš„ç°è±¡ç§°ä¸ºLLMç ´è§£ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæœ‰æ„çš„LLMç ´è§£éå¸¸ç®€å•ã€‚é€šè¿‡å¤åˆ¶21é¡¹å·²å‘è¡¨ç¤¾ä¼šç§‘å­¦ç ”ç©¶ä¸­çš„37é¡¹æ•°æ®æ ‡æ³¨ä»»åŠ¡ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œåªéœ€å°‘é‡æç¤ºé‡Šä¹‰ï¼Œå‡ ä¹ä»»ä½•ç»“æœéƒ½å¯ä»¥å‘ˆç°ä¸ºå…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ã€‚é™¤äº†æœ‰æ„æ“çºµï¼Œå¯¹æ¥è‡ª18ä¸ªä¸åŒLLMçš„1300ä¸‡ä¸ªæ ‡ç­¾åœ¨2361ä¸ªå®é™…å‡è®¾ä¸Šçš„åˆ†æè¡¨æ˜ï¼Œå³ä½¿éµå¾ªæ ‡å‡†ç ”ç©¶å®è·µï¼Œä¹Ÿå­˜åœ¨æ„å¤–LLMç ´è§£çš„é«˜é£é™©ã€‚å¯¹äºæœ€å…ˆè¿›çš„LLMï¼Œå¤§çº¦31%çš„å‡è®¾å¾—å‡ºä¸æ­£ç¡®çš„ç»“è®ºï¼Œè€Œå¯¹äºè¾ƒå°çš„è¯­è¨€æ¨¡å‹ï¼Œè¿™ä¸€æ¯”ä¾‹ä¸ºä¸€åŠã€‚è™½ç„¶æ›´é«˜çš„ä»»åŠ¡æ€§èƒ½å’Œæ›´å¼ºçš„é€šç”¨æ¨¡å‹èƒ½åŠ›é™ä½äº†LLMç ´è§£é£é™©ï¼Œä½†å³ä½¿æ˜¯é«˜åº¦å‡†ç¡®çš„æ¨¡å‹ä»ç„¶å®¹æ˜“å—åˆ°å½±å“ã€‚LLMç ´è§£çš„é£é™©éšç€æ•ˆåº”é‡çš„å¢åŠ è€Œé™ä½ï¼Œè¡¨æ˜éœ€è¦å¯¹æ¥è¿‘æ˜¾è‘—æ€§é˜ˆå€¼çš„åŸºäºLLMçš„å‘ç°è¿›è¡Œæ›´ä¸¥æ ¼çš„éªŒè¯ã€‚æˆ‘ä»¬åˆ†æäº†21ç§ç¼“è§£æŠ€æœ¯ï¼Œå‘ç°äººå·¥æ ‡æ³¨ä¸ºé˜²æ­¢å‡é˜³æ€§æä¾›äº†å…³é”®ä¿æŠ¤ã€‚å¸¸è§çš„å›å½’ä¼°è®¡å™¨æ ¡æ­£æŠ€æœ¯å¯ä»¥æ¢å¤æœ‰æ•ˆçš„æ¨æ–­ï¼Œä½†éœ€è¦åœ¨Iå‹å’ŒIIå‹é”™è¯¯ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ç³»åˆ—å®ç”¨çš„å»ºè®®ï¼Œä»¥é˜²æ­¢LLMç ´è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ–‡æœ¬æ ‡æ³¨æ—¶ï¼Œç”±äºæ¨¡å‹é€‰æ‹©ã€æç¤ºå·¥ç¨‹ç­‰é…ç½®é€‰æ‹©ä¸å½“ï¼Œå¯¼è‡´ç ”ç©¶ç»“è®ºå‡ºç°åå·®ç”šè‡³é”™è¯¯çš„â€œLLMç ´è§£â€é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹è¿™ç§é£é™©çš„é‡åŒ–è¯„ä¼°å’Œæœ‰æ•ˆç¼“è§£æªæ–½ï¼Œä½¿å¾—ç ”ç©¶ç»“æœçš„å¯é æ€§å—åˆ°å¨èƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡åŒ–LLMåœ¨æ–‡æœ¬æ ‡æ³¨ä»»åŠ¡ä¸­äº§ç”Ÿé”™è¯¯ç»“è®ºçš„é£é™©ï¼Œå¹¶åˆ†æä¸åŒå› ç´ ï¼ˆå¦‚æ¨¡å‹å¤§å°ã€ä»»åŠ¡éš¾åº¦ã€æç¤ºç­–ç•¥ï¼‰å¯¹è¯¥é£é™©çš„å½±å“ã€‚é€šè¿‡æ¨¡æ‹ŸçœŸå®çš„ç ”ç©¶åœºæ™¯ï¼Œè¯„ä¼°æœ‰æ„å’Œæ„å¤–çš„LLMç ´è§£å‘ç”Ÿçš„å¯èƒ½æ€§ï¼Œå¹¶æ¢ç´¢æœ‰æ•ˆçš„ç¼“è§£ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) é€‰å–å·²å‘è¡¨çš„ç¤¾ä¼šç§‘å­¦ç ”ç©¶ä½œä¸ºåŸºå‡†ï¼Œå¤åˆ¶å…¶æ•°æ®æ ‡æ³¨ä»»åŠ¡ã€‚2) ä½¿ç”¨ä¸åŒçš„LLMå’Œæç¤ºç­–ç•¥è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶åˆ†æç»“æœçš„å·®å¼‚ã€‚3) é€šè¿‡ç»Ÿè®¡åˆ†æï¼Œé‡åŒ–LLMç ´è§£çš„é£é™©ï¼Œå¹¶è¯†åˆ«å¯¼è‡´é”™è¯¯çš„å› ç´ ã€‚4) è¯„ä¼°ä¸åŒçš„ç¼“è§£æŠ€æœ¯ï¼ˆå¦‚äººå·¥æ ‡æ³¨ã€å›å½’æ ¡æ­£ï¼‰çš„æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) é¦–æ¬¡æå‡ºäº†â€œLLMç ´è§£â€çš„æ¦‚å¿µï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†é‡åŒ–è¯„ä¼°ã€‚2) æ­ç¤ºäº†å³ä½¿æ˜¯å…ˆè¿›çš„LLMä¹Ÿå­˜åœ¨è¾ƒé«˜çš„ç ´è§£é£é™©ï¼Œå¹¶åˆ†æäº†é£é™©çš„å½±å“å› ç´ ã€‚3) è¯„ä¼°äº†å¤šç§ç¼“è§£æŠ€æœ¯çš„æ•ˆæœï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†å®ç”¨çš„å»ºè®®ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰å–å…·æœ‰ä»£è¡¨æ€§çš„ç¤¾ä¼šç§‘å­¦ç ”ç©¶ä½œä¸ºåŸºå‡†ï¼Œä¿è¯äº†ç ”ç©¶çš„å®é™…æ„ä¹‰ã€‚2) ä½¿ç”¨å¤šç§LLMå’Œæç¤ºç­–ç•¥ï¼Œè¦†ç›–äº†ä¸åŒçš„é…ç½®é€‰æ‹©ã€‚3) é‡‡ç”¨ä¸¥æ ¼çš„ç»Ÿè®¡åˆ†ææ–¹æ³•ï¼Œé‡åŒ–äº†LLMç ´è§£çš„é£é™©ã€‚4) è¯„ä¼°äº†å¤šç§ç¼“è§£æŠ€æœ¯ï¼Œå¹¶ç»™å‡ºäº†å®ç”¨çš„å»ºè®®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LLMï¼Œåœ¨çº¦31%çš„å‡è®¾ä¸­ä¹Ÿä¼šå¾—å‡ºä¸æ­£ç¡®çš„ç»“è®ºã€‚é€šè¿‡å°‘é‡æç¤ºé‡Šä¹‰ï¼Œå‡ ä¹ä»»ä½•ç»“æœéƒ½å¯ä»¥å‘ˆç°ä¸ºå…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ã€‚äººå·¥æ ‡æ³¨ä¸ºé˜²æ­¢å‡é˜³æ€§æä¾›äº†å…³é”®ä¿æŠ¤ï¼Œè€Œå¸¸è§çš„å›å½’ä¼°è®¡å™¨æ ¡æ­£æŠ€æœ¯å¯ä»¥æ¢å¤æœ‰æ•ˆçš„æ¨æ–­ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç¤¾ä¼šç§‘å­¦ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜åœ¨ä½¿ç”¨LLMè¿›è¡Œæ–‡æœ¬æ ‡æ³¨æ—¶ï¼Œæ›´å¥½åœ°è¯„ä¼°å’Œæ§åˆ¶é£é™©ï¼Œæé«˜ç ”ç©¶ç»“æœçš„å¯é æ€§ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºLLMçš„å®‰å…¨æ€§è¯„ä¼°å’Œé£é™©ç®¡ç†æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection or prompting strategy). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I (false positive), Type II (false negative), Type S (wrong sign), or Type M (exaggerated effect) errors. We call this phenomenon where configuration choices lead to incorrect conclusions LLM hacking.
>   We find that intentional LLM hacking is strikingly simple. By replicating 37 data annotation tasks from 21 published social science studies, we show that, with just a handful of prompt paraphrases, virtually anything can be presented as statistically significant.
>   Beyond intentional manipulation, our analysis of 13 million labels from 18 different LLMs across 2361 realistic hypotheses shows that there is also a high risk of accidental LLM hacking, even when following standard research practices. We find incorrect conclusions in approximately 31% of hypotheses for state-of-the-art LLMs, and in half the hypotheses for smaller language models. While higher task performance and stronger general model capabilities reduce LLM hacking risk, even highly accurate models remain susceptible. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of LLM-based findings near significance thresholds. We analyze 21 mitigation techniques and find that human annotations provide crucial protection against false positives. Common regression estimator correction techniques can restore valid inference but trade off Type I vs. Type II errors.
>   We publish a list of practical recommendations to prevent LLM hacking.

