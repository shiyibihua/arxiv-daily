---
layout: default
title: From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs
---

# From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16795" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16795v1</a>
  <a href="https://arxiv.org/pdf/2512.16795.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16795v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16795v1', 'From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shubham Mishra, Samyek Jain, Gorang Mehrishi, Shiv Tiwari, Harsh Sharma, Pratik Narang, Dhruv Kumar

**åˆ†ç±»**: cs.CL, cs.AI, cs.CY, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Under Review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨ç†è¿½è¸ªå¢å¼ºçš„RAGæ¡†æ¶ï¼Œè§£å†³æ£€ç´¢ä¿¡æ¯å†²çªå’Œä¸»è§‚æ€§é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å†²çªåˆ†æ` `æ¨ç†è¿½è¸ª` `å¯è§£é‡Šæ€§` `çŸ¥è¯†æ¨ç†` `ä¿¡æ¯èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RAGæ–¹æ³•åœ¨å¤„ç†å†²çªã€è¿‡æ—¶æˆ–ä¸»è§‚ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹ç»Ÿä¸€çš„æ¨ç†ç›‘ç£ã€‚
2. æå‡ºæ¨ç†è¿½è¸ªå¢å¼ºçš„RAGæ¡†æ¶ï¼Œé€šè¿‡æ–‡æ¡£è£å†³ã€å†²çªåˆ†æå’Œè¯æ®ç»¼åˆå®ç°å¯è§£é‡Šæ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Qwenæ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†ç­”æ¡ˆæ­£ç¡®ç‡å’Œè¡Œä¸ºä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä½¿å¤§å‹è¯­è¨€æ¨¡å‹(LLM)èƒ½å¤Ÿåˆ©ç”¨å¤–éƒ¨è¯æ®ï¼Œä½†å½“æ£€ç´¢åˆ°çš„ä¿¡æ¯å­˜åœ¨å†²çªã€è¿‡æ—¶æˆ–ä¸»è§‚æ€§æ—¶ï¼ŒRAGä¼šå¤±æ•ˆã€‚ç°æœ‰å·¥ä½œåˆ†åˆ«è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†ç¼ºä¹ç»Ÿä¸€çš„æ¨ç†ç›‘ç£ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨ç†è¿½è¸ªå¢å¼ºçš„RAGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¸‰ä¸ªé˜¶æ®µæ·»åŠ äº†ç»“æ„åŒ–çš„ã€å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼š(1)æ–‡æ¡£çº§è£å†³ï¼Œ(2)å†²çªåˆ†æï¼Œ(3)åŸºäºè¯æ®çš„ç»¼åˆï¼Œä»è€Œç”Ÿæˆå¸¦æœ‰å¼•ç”¨çš„ç­”æ¡ˆæˆ–åˆç†çš„æ‹’ç»å›ç­”ã€‚å¼•å…¥äº†ä¸€ç§å†²çªæ„ŸçŸ¥ä¿¡ä»»è¯„åˆ†(CATS)æµç¨‹ï¼Œè¯¥æµç¨‹ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤è€…æ¥è¯„ä¼°è¯æ®å……åˆ†æ€§ã€äº‹å®æ­£ç¡®æ€§ã€æ‹’ç»å›ç­”çš„å‡†ç¡®æ€§ä»¥åŠå†²çªè¡Œä¸ºçš„ä¸€è‡´æ€§ã€‚æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«539ä¸ªæŸ¥è¯¢çš„æ¨ç†æ•°æ®é›†å’Œè¯„ä¼°æµç¨‹ï¼Œä¸ºå†²çªæ„ŸçŸ¥ã€å¯è§£é‡Šçš„RAGç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨Qwenæ¨¡å‹ä¸Šï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼Œç«¯åˆ°ç«¯ç­”æ¡ˆæ­£ç¡®ç‡ä»0.069æé«˜åˆ°0.883ï¼Œè¡Œä¸ºä¸€è‡´æ€§ä»0.074æé«˜åˆ°0.722ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰RAGæ¨¡å‹åœ¨é¢å¯¹æ£€ç´¢åˆ°çš„ä¿¡æ¯å­˜åœ¨å†²çªã€è¿‡æ—¶æˆ–ä¸»è§‚æ€§æ—¶ï¼Œæ— æ³•æœ‰æ•ˆåˆ¤æ–­å’Œæ•´åˆä¿¡æ¯ï¼Œå¯¼è‡´ç”Ÿæˆé”™è¯¯æˆ–ä¸å¯é çš„ç­”æ¡ˆã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆé€šå¸¸ç‹¬ç«‹è§£å†³è¿™äº›é—®é¢˜ï¼Œç¼ºä¹ç»Ÿä¸€çš„æ¨ç†ç›‘ç£æœºåˆ¶ï¼Œéš¾ä»¥ä¿è¯ç”Ÿæˆç»“æœçš„è´¨é‡å’Œå¯ä¿¡åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ç»“æ„åŒ–çš„æ¨ç†è¿‡ç¨‹ï¼Œæ˜¾å¼åœ°å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œè£å†³ã€å†²çªåˆ†æå’Œè¯æ®ç»¼åˆï¼Œä»è€Œæé«˜RAGæ¨¡å‹åœ¨å¤æ‚ä¿¡æ¯ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆç»“æœçš„å¯é æ€§ã€‚é€šè¿‡å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªå¯è§£é‡Šçš„æ­¥éª¤ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œå¹¶è¿›è¡Œé’ˆå¯¹æ€§çš„ä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ–‡æ¡£çº§è£å†³ï¼šå¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œè¯„ä¼°ï¼Œåˆ¤æ–­å…¶å¯é æ€§å’Œç›¸å…³æ€§ï¼›2) å†²çªåˆ†æï¼šè¯†åˆ«æ–‡æ¡£ä¹‹é—´çš„å†²çªä¿¡æ¯ï¼Œå¹¶åˆ†æå†²çªçš„åŸå› ï¼›3) åŸºäºè¯æ®çš„ç»¼åˆï¼šæ ¹æ®è£å†³ç»“æœå’Œå†²çªåˆ†æï¼Œç»¼åˆå„ä¸ªæ–‡æ¡£çš„ä¿¡æ¯ï¼Œç”Ÿæˆå¸¦æœ‰å¼•ç”¨çš„ç­”æ¡ˆæˆ–åˆç†çš„æ‹’ç»å›ç­”ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å†²çªæ„ŸçŸ¥ä¿¡ä»»è¯„åˆ†(CATS)æµç¨‹ï¼Œä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤è€…æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†ç»“æ„åŒ–çš„æ¨ç†è¿½è¸ªï¼Œå°†RAGæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹æ˜¾å¼åŒ–ï¼Œä½¿å…¶æ›´æ˜“äºç†è§£å’Œè°ƒè¯•ã€‚é€šè¿‡æ–‡æ¡£è£å†³å’Œå†²çªåˆ†æï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¿‡æ»¤æ‰ä¸å¯é çš„ä¿¡æ¯ï¼Œæé«˜ç”Ÿæˆç»“æœçš„è´¨é‡ã€‚æ­¤å¤–ï¼ŒCATSæµç¨‹æä¾›äº†ä¸€ç§è‡ªåŠ¨åŒ–çš„è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°RAGæ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ–‡æ¡£è£å†³æ¨¡å—ä½¿ç”¨LLMå¯¹æ–‡æ¡£çš„å¯é æ€§å’Œç›¸å…³æ€§è¿›è¡Œè¯„åˆ†ï¼›2) å†²çªåˆ†ææ¨¡å—ä½¿ç”¨LLMè¯†åˆ«æ–‡æ¡£ä¹‹é—´çš„å†²çªä¿¡æ¯ï¼Œå¹¶åˆ†æå†²çªçš„åŸå› ï¼›3) è¯æ®ç»¼åˆæ¨¡å—ä½¿ç”¨LLMæ ¹æ®è£å†³ç»“æœå’Œå†²çªåˆ†æï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚CATSæµç¨‹ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤è€…ï¼Œè¯„ä¼°ç”Ÿæˆç­”æ¡ˆçš„è¯æ®å……åˆ†æ€§ã€äº‹å®æ­£ç¡®æ€§ã€æ‹’ç»å›ç­”çš„å‡†ç¡®æ€§ä»¥åŠå†²çªè¡Œä¸ºçš„ä¸€è‡´æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Qwenæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼Œç«¯åˆ°ç«¯ç­”æ¡ˆæ­£ç¡®ç‡ä»0.069æé«˜åˆ°0.883ï¼Œè¡Œä¸ºä¸€è‡´æ€§ä»0.074æé«˜åˆ°0.722ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜RAGæ¨¡å‹åœ¨å¤„ç†å†²çªä¿¡æ¯æ—¶çš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆç»“æœçš„å¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦å¤„ç†å¤§é‡å†²çªæˆ–ä¸ç¡®å®šä¿¡æ¯çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€é‡‘èåˆ†æã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚é€šè¿‡æé«˜RAGæ¨¡å‹åœ¨å¤æ‚ä¿¡æ¯ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ç”Ÿæˆæ›´å‡†ç¡®ã€å¯é çš„ç­”æ¡ˆï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œå†³ç­–æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.

