---
layout: default
title: On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval
---

# On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11499" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11499v1</a>
  <a href="https://arxiv.org/pdf/2506.11499.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11499v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11499v1', 'On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seongbo Jang, Seonghyeon Lee, Dongha Lee, Hwanjo Yu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: 9 pages, 1 figure

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€å¯¹è¯å“åº”æ£€ç´¢é›†æˆæ–¹æ³•ä»¥æå‡ç³»ç»Ÿæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹è¯` `å“åº”æ£€ç´¢` `é›†æˆæ–¹æ³•` `ç«¯åˆ°ç«¯å­¦ä¹ ` `å‚æ•°å…±äº«` `æœºå™¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿåœ¨å“åº”ç”Ÿæˆæ—¶é¢ä¸´æ¨¡æ€èåˆå’Œä¸Šä¸‹æ–‡ç†è§£çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€‚
2. è®ºæ–‡æå‡ºäº†ä¸‰ç§é›†æˆæ–¹æ³•ï¼Œåˆ†åˆ«åŸºäºä¸¤æ­¥æ³•å’Œç«¯åˆ°ç«¯æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šæ¨¡æ€å“åº”æ£€ç´¢è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç«¯åˆ°ç«¯æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¸ä¼ ç»Ÿä¸¤æ­¥æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶é€šè¿‡å‚æ•°å…±äº«æå‡äº†ç³»ç»Ÿæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€èŠå¤©æœºå™¨äººå·²æˆä¸ºå¯¹è¯ç³»ç»Ÿç ”ç©¶å’Œå·¥ä¸šç•Œçš„ä¸»è¦è¯é¢˜ã€‚æœ¬æ–‡æ¢è®¨äº†å¯¹è¯ç³»ç»Ÿå¦‚ä½•åœ¨æ–‡æœ¬å’Œå›¾åƒç­‰å¤šç§æ¨¡æ€ä¸­è¾“å‡ºå“åº”ã€‚æˆ‘ä»¬é¦–å…ˆå°†å¤šæ¨¡æ€å¯¹è¯å“åº”æ£€ç´¢ä»»åŠ¡å®šä¹‰ä¸ºä¸‰ä¸ªå­ä»»åŠ¡çš„ç»„åˆï¼Œéšåæå‡ºåŸºäºä¸¤æ­¥æ³•å’Œç«¯åˆ°ç«¯æ–¹æ³•çš„ä¸‰ç§é›†æˆæ–¹æ³•ï¼Œå¹¶æ¯”è¾ƒäº†å„è‡ªçš„ä¼˜ç¼ºç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç«¯åˆ°ç«¯æ–¹æ³•åœ¨æ²¡æœ‰ä¸­é—´æ­¥éª¤çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸ä¸¤æ­¥æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œå‚æ•°å…±äº«ç­–ç•¥ä¸ä»…å‡å°‘äº†å‚æ•°æ•°é‡ï¼Œè¿˜é€šè¿‡è·¨å­ä»»åŠ¡å’Œæ¨¡æ€çš„çŸ¥è¯†è½¬ç§»æå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¯¹è¯å“åº”æ£€ç´¢ä»»åŠ¡ä¸­çš„æ¨¡æ€èåˆå’Œä¸Šä¸‹æ–‡ç†è§£é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤æ‚çš„ä¸­é—´æ­¥éª¤ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä¸‰ç§é›†æˆæ–¹æ³•ï¼Œåˆ©ç”¨ä¸¤æ­¥æ³•å’Œç«¯åˆ°ç«¯æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œç®€åŒ–å“åº”ç”Ÿæˆè¿‡ç¨‹å¹¶æå‡ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¾“å…¥å¤„ç†æ¨¡å—ã€å“åº”ç”Ÿæˆæ¨¡å—å’Œè¾“å‡ºè¯„ä¼°æ¨¡å—ã€‚è¾“å…¥å¤„ç†æ¨¡å—è´Ÿè´£æ¥æ”¶å’Œé¢„å¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼Œå“åº”ç”Ÿæˆæ¨¡å—åˆ™æ ¹æ®å¤„ç†åçš„æ•°æ®ç”Ÿæˆæœ€ç»ˆå“åº”ï¼Œè¾“å‡ºè¯„ä¼°æ¨¡å—ç”¨äºè¯„ä¼°ç”Ÿæˆå“åº”çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ²¡æœ‰ä¸­é—´æ­¥éª¤çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°ä¸ä¼ ç»Ÿä¸¤æ­¥æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶é€šè¿‡å‚æ•°å…±äº«æå‡äº†ç³»ç»Ÿçš„å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å…±äº«å‚æ•°ç­–ç•¥ï¼Œå‡å°‘äº†æ¨¡å‹çš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†å¤šæ¨¡æ€ç‰¹å¾çš„èåˆï¼Œç¡®ä¿äº†ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç«¯åˆ°ç«¯æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¸ä¼ ç»Ÿä¸¤æ­¥æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”åœ¨å‚æ•°å…±äº«ç­–ç•¥çš„å¸®åŠ©ä¸‹ï¼Œç³»ç»Ÿçš„å‚æ•°æ•°é‡æ˜¾è‘—å‡å°‘ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹å’Œç¤¾äº¤æœºå™¨äººç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿçš„å“åº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼Œæä¾›æ›´ä¸ºä¸°å¯Œå’Œä¸ªæ€§åŒ–çš„äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ•™è‚²ã€åŒ»ç–—å’Œå¨±ä¹ç­‰å¤šä¸ªè¡Œä¸šä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal chatbots have become one of the major topics for dialogue systems in both research community and industry. Recently, researchers have shed light on the multimodality of responses as well as dialogue contexts. This work explores how a dialogue system can output responses in various modalities such as text and image. To this end, we first formulate a multimodal dialogue response retrieval task for retrieval-based systems as the combination of three subtasks. We then propose three integration methods based on a two-step approach and an end-to-end approach, and compare the merits and demerits of each method. Experimental results on two datasets demonstrate that the end-to-end approach achieves comparable performance without an intermediate step in the two-step approach. In addition, a parameter sharing strategy not only reduces the number of parameters but also boosts performance by transferring knowledge across the subtasks and the modalities.

