---
layout: default
title: Maximally-Informative Retrieval for State Space Model Generation
---

# Maximally-Informative Retrieval for State Space Model Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12149" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12149v1</a>
  <a href="https://arxiv.org/pdf/2506.12149.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12149v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12149v1', 'Maximally-Informative Retrieval for State Space Model Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Evan Becker, Benjamin Bowman, Matthew Trager, Tian Yu Liu, Luca Zancato, Wei Xia, Stefano Soatto

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRICOæ–¹æ³•ä»¥ä¼˜åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ç”Ÿæˆä¸­çš„ä¿¡æ¯æ£€ç´¢**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¿¡æ¯æ£€ç´¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ— ç›‘ç£å­¦ä¹ ` `æ–‡æ¡£é€‰æ‹©` `æ¨¡å‹ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨ç†ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¨ç†æ—¶æ— æ³•æœ‰æ•ˆåˆ©ç”¨æ‰€æœ‰å¯ç”¨ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†æŸ¥è¯¢æ—¶çš„ä¸ç¡®å®šæ€§è¾ƒé«˜ã€‚
2. è®ºæ–‡æå‡ºçš„RICOæ–¹æ³•é€šè¿‡åˆ©ç”¨LLMçš„æ¢¯åº¦ä¿¡æ¯ï¼Œå­¦ä¹ æœ€ä¼˜æ–‡æ¡£ç»„åˆï¼Œä»è€Œæå‡æ¨ç†æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRICOåœ¨æ— ç›‘ç£æŸå¤±ç›®æ ‡ä¸‹çš„è¡¨ç°ä¸BM25ç›¸å½“ï¼Œä¸”åœ¨æœ€ç»ˆé¢„æµ‹è´¨é‡ä¸Šè¶…è¶Šäº†å¾®è°ƒçš„å¯†é›†æ£€ç´¢å™¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ£€ç´¢æ–¹æ³•â€”â€”æ£€ç´¢ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆRICOï¼‰ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªèº«çš„æ¢¯åº¦ä¿¡æ¯æ¥ä¼˜åŒ–æ–‡æ¡£çš„é€‰æ‹©ï¼Œä»è€Œåœ¨æ¨ç†æ—¶å‡å°‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼ˆRAGï¼‰ä¾èµ–å¤–éƒ¨å¯å‘å¼æ–¹æ³•è¿›è¡Œæ–‡æ¡£æ£€ç´¢ï¼Œè€ŒRICOåˆ™é€šè¿‡æ¨¡å‹çš„ç›´æ¥åé¦ˆæ¥å®ç°æ›´ä¼˜çš„æ–‡æ¡£ç»„åˆã€‚ç†è®ºä¸Šï¼Œè®ºæ–‡å±•ç¤ºäº†æ ‡å‡†çš„top-$k$æ£€ç´¢å¯ä»¥è¿‘ä¼¼è¯¥ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡æ— ç›‘ç£æŸå¤±ç›®æ ‡çš„æœ€å°åŒ–ï¼Œå®éªŒè¯æ˜RICOåœ¨æ£€ç´¢æ€§èƒ½ä¸Šä¸BM25ç›¸å½“ï¼Œä¸”æ— éœ€å¾®è°ƒï¼Œä¸”åœ¨æœ€ç»ˆé¢„æµ‹è´¨é‡ä¸Šå¸¸å¸¸è¶…è¶Šå¾®è°ƒçš„å¯†é›†æ£€ç´¢å™¨å¦‚E5ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•æœ‰æ•ˆé€‰æ‹©ä¸å½“å‰æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ï¼Œå¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨æ‰€æœ‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRICOæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¢¯åº¦ä¿¡æ¯ï¼Œä¼˜åŒ–æ–‡æ¡£çš„é€‰æ‹©ï¼Œä»¥å‡å°‘æ¨¡å‹åœ¨ç‰¹å®šæŸ¥è¯¢ä¸‹çš„ä¸ç¡®å®šæ€§ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æŸ¥è¯¢çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRICOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æŸ¥è¯¢è¾“å…¥ã€æ–‡æ¡£æ£€ç´¢å’ŒåŸºäºæ¨¡å‹åé¦ˆçš„ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œè¾“å…¥æŸ¥è¯¢åï¼Œç³»ç»Ÿä¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶åé€šè¿‡æ¨¡å‹çš„æ¢¯åº¦ä¿¡æ¯å¯¹æ–‡æ¡£è¿›è¡Œä¼˜åŒ–é€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šRICOçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„åé¦ˆè¿›è¡Œæ–‡æ¡£æ£€ç´¢ï¼Œè€Œä¸æ˜¯ä¾èµ–å¤–éƒ¨å¯å‘å¼æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ–‡æ¡£é€‰æ‹©æ›´åŠ ç²¾å‡†ï¼Œæå‡äº†æ¨ç†çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒRICOé‡‡ç”¨äº†æ— ç›‘ç£çš„æŸå¤±å‡½æ•°å½¢å¼ï¼Œå…·ä½“ä¸ºé—®é¢˜å›°æƒ‘åº¦ï¼Œä»¥æ­¤æ¥ä¼˜åŒ–æ–‡æ¡£é€‰æ‹©è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRICOåœ¨æ— ç›‘ç£æŸå¤±ç›®æ ‡ä¸‹çš„æ£€ç´¢æ€§èƒ½ä¸BM25ç›¸å½“ï¼Œä¸”åœ¨æœ€ç»ˆé¢„æµ‹è´¨é‡ä¸Šå¸¸å¸¸è¶…è¶Šå¾®è°ƒçš„å¯†é›†æ£€ç´¢å™¨E5ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¿¡æ¯æ£€ç´¢å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ–‡æ¡£æ£€ç´¢è¿‡ç¨‹ï¼ŒRICOå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å“åº”é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Given a query and dataset, the optimal way of answering the query is to make use all the information available. Modern LLMs exhibit impressive ability to memorize training data, but data not deemed important during training is forgotten, and information outside that training set cannot be made use of. Processing an entire dataset at inference time is infeasible due to the bounded nature of model resources (e.g. context size in transformers or states in state space models), meaning we must resort to external memory. This constraint naturally leads to the following problem: How can we decide based on the present query and model, what among a virtually unbounded set of known data matters for inference? To minimize model uncertainty for a particular query at test-time, we introduce Retrieval In-Context Optimization (RICO), a retrieval method that uses gradients from the LLM itself to learn the optimal mixture of documents for answer generation. Unlike traditional retrieval-augmented generation (RAG), which relies on external heuristics for document retrieval, our approach leverages direct feedback from the model. Theoretically, we show that standard top-$k$ retrieval with model gradients can approximate our optimization procedure, and provide connections to the leave-one-out loss. We demonstrate empirically that by minimizing an unsupervised loss objective in the form of question perplexity, we can achieve comparable retriever metric performance to BM25 with \emph{no finetuning}. Furthermore, when evaluated on quality of the final prediction, our method often outperforms fine-tuned dense retrievers such as E5.

