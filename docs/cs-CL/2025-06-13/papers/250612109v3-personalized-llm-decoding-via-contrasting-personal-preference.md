---
layout: default
title: Personalized LLM Decoding via Contrasting Personal Preference
---

# Personalized LLM Decoding via Contrasting Personal Preference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12109" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12109v3</a>
  <a href="https://arxiv.org/pdf/2506.12109.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12109v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12109v3', 'Personalized LLM Decoding via Contrasting Personal Preference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hyungjune Bu, Chanjoo Jung, Minjae Kang, Jaehyung Kim

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-11-24)

**å¤‡æ³¨**: EMNLP 2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoPeä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–è§£ç é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–ç”Ÿæˆ` `å¤§è¯­è¨€æ¨¡å‹` `è§£ç ç®—æ³•` `å¥–åŠ±å¼•å¯¼` `æ–‡æœ¬ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸ªæ€§åŒ–å¤§è¯­è¨€æ¨¡å‹æ–¹æ³•å¤šé›†ä¸­äºæç¤ºå’Œè®­ç»ƒï¼Œä½†è§£ç æ—¶çš„ç®—æ³•å¼€å‘ä»æ˜¾ä¸è¶³ï¼Œé™åˆ¶äº†ä¸ªæ€§åŒ–æ•ˆæœçš„æå‡ã€‚
2. æœ¬æ–‡æå‡ºçš„CoPeæ–¹æ³•é€šè¿‡åœ¨ç”¨æˆ·ç‰¹å®šæ•°æ®ä¸Šè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒåï¼Œåˆ©ç”¨å¥–åŠ±å¼•å¯¼è§£ç æ¥å®ç°ä¸ªæ€§åŒ–ï¼Œæœ€å¤§åŒ–ç”¨æˆ·çš„éšæ€§å¥–åŠ±ä¿¡å·ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoPeåœ¨äº”ä¸ªä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒROUGE-LæŒ‡æ ‡å¹³å‡æå‡10.57%ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§å®é™…åº”ç”¨ä¸­çš„é€æ­¥éƒ¨ç½²ï¼Œä¸ªæ€§åŒ–å˜å¾—æ„ˆå‘é‡è¦ã€‚å°½ç®¡å·²æœ‰å¤šç§ä¸ªæ€§åŒ–æ–¹æ³•è¢«æ¢ç´¢ï¼Œä½†è§£ç æ—¶ç®—æ³•çš„å¼€å‘ä»è¢«å¿½è§†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£ç æ—¶æ–¹æ³•CoPeï¼ˆContrasting Personal Preferenceï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨ç”¨æˆ·ç‰¹å®šæ•°æ®ä¸Šè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒååº”ç”¨ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æœ€å¤§åŒ–æ¯ä¸ªç”¨æˆ·çš„éšæ€§å¥–åŠ±ä¿¡å·ï¼Œåˆ©ç”¨å¥–åŠ±å¼•å¯¼è§£ç è¿›è¡Œä¸ªæ€§åŒ–ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå¼€æ”¾å¼ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¯„ä¼°äº†CoPeï¼Œå®éªŒè¯æ˜å…¶åœ¨ROUGE-LæŒ‡æ ‡ä¸Šå¹³å‡æå‡äº†10.57%ï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–é¢å¤–è®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸ªæ€§åŒ–è§£ç æ—¶çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºè®­ç»ƒé˜¶æ®µï¼Œç¼ºä¹æœ‰æ•ˆçš„è§£ç æ—¶ä¸ªæ€§åŒ–ç­–ç•¥ï¼Œå¯¼è‡´ä¸ªæ€§åŒ–æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoPeæ–¹æ³•çš„æ ¸å¿ƒåœ¨äºé€šè¿‡å¥–åŠ±å¼•å¯¼è§£ç ï¼Œæœ€å¤§åŒ–æ¯ä¸ªç”¨æˆ·çš„éšæ€§å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå®ç°ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆã€‚æ­¤è®¾è®¡æ—¨åœ¨æå‡ç”¨æˆ·ä½“éªŒï¼Œä½¿ç”Ÿæˆå†…å®¹æ›´ç¬¦åˆç”¨æˆ·åå¥½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoPeçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆåœ¨ç”¨æˆ·ç‰¹å®šæ•°æ®ä¸Šè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ï¼Œç„¶ååœ¨è§£ç è¿‡ç¨‹ä¸­åº”ç”¨å¥–åŠ±å¼•å¯¼ç­–ç•¥ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œç®€åŒ–äº†ä¸ªæ€§åŒ–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoPeçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†å¥–åŠ±å¼•å¯¼è§£ç åº”ç”¨äºä¸ªæ€§åŒ–ä»»åŠ¡ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–è®­ç»ƒé˜¶æ®µçš„ä¼˜åŒ–ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§£ç æ—¶ä¸ªæ€§åŒ–ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…·ä½“å®ç°ä¸­ï¼ŒCoPeè®¾ç½®äº†é€‚åº”ç”¨æˆ·åå¥½çš„æŸå¤±å‡½æ•°ï¼Œå¹¶è®¾è®¡äº†é€‚åˆä¸ªæ€§åŒ–çš„è§£ç ç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ ç”¨æˆ·çš„éšæ€§éœ€æ±‚ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒCoPeåœ¨ä¸ªæ€§åŒ–ç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„æ•ˆæœæå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoPeåœ¨äº”ä¸ªå¼€æ”¾å¼ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒROUGE-LæŒ‡æ ‡å¹³å‡æå‡10.57%ã€‚è¿™ä¸€æå‡åœ¨ä¸ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå±•ç¤ºäº†CoPeçš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººã€æ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–èƒ½åŠ›ï¼ŒCoPeèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ç¬¦åˆå…¶éœ€æ±‚çš„æ–‡æœ¬ç”ŸæˆæœåŠ¡ï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) are progressively deployed in various real-world applications, personalization of LLMs has become increasingly important. While various approaches to LLM personalization such as prompt-based and training-based methods have been actively explored, the development of effective decoding-time algorithms remains largely overlooked, despite their demonstrated potential. In this paper, we propose CoPe (Contrasting Personal Preference), a novel decoding-time approach applied after performing parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is to leverage reward-guided decoding specifically for personalization by maximizing each user's implicit reward signal. We evaluate CoPe across five open-ended personalized text generation tasks. Our empirical results demonstrate that CoPe achieves strong performance, improving personalization by an average of 10.57% in ROUGE-L, without relying on external reward models or additional training procedures.

