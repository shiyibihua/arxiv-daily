---
layout: default
title: DART: Distilling Autoregressive Reasoning to Silent Thought
---

# DART: Distilling Autoregressive Reasoning to Silent Thought

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11752" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11752v2</a>
  <a href="https://arxiv.org/pdf/2506.11752.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11752v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11752v2', 'DART: Distilling Autoregressive Reasoning to Silent Thought')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nan Jiang, Ziming Wu, De-Chuan Zhan, Fuming Lai, Shaobing Lian

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-08-28)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDARTä»¥è§£å†³è‡ªå›å½’æ¨ç†çš„è®¡ç®—å¼€é”€é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªå›å½’æ¨ç†` `é™é»˜æ€ç»´` `è’¸é¦è®­ç»ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†æ¼”å˜æ¨¡å—` `é«˜æ•ˆæ¨ç†` `å»¶è¿Ÿæ•æ„Ÿåº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é“¾å¼æ¨ç†æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å…¶è‡ªå›å½’ç‰¹æ€§å¯¼è‡´è®¡ç®—å¼€é”€å¤§ï¼Œé™åˆ¶äº†å®æ—¶åº”ç”¨çš„å¯èƒ½æ€§ã€‚
2. DARTæ¡†æ¶é€šè¿‡è‡ªè’¸é¦æŠ€æœ¯ï¼Œæå‡ºäº†CoTå’ŒSTä¸¤æ¡è®­ç»ƒè·¯å¾„ï¼Œæ—¨åœ¨å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—è´Ÿæ‹…ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDARTåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„éè‡ªå›å½’åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†å»¶è¿Ÿä¸å˜ï¼Œå±•ç°äº†å…¶é«˜æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“¾å¼æ¨ç†ï¼ˆCoTï¼‰æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä½†å…¶è‡ªå›å½’èŒƒå¼å¯¼è‡´äº†æ˜¾è‘—çš„è®¡ç®—å¼€é”€ï¼Œé™åˆ¶äº†åœ¨å»¶è¿Ÿæ•æ„Ÿåº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†DARTï¼ˆå°†è‡ªå›å½’æ¨ç†è’¸é¦ä¸ºé™é»˜æ€ç»´ï¼‰ï¼Œä¸€ä¸ªè‡ªè’¸é¦æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤Ÿç”¨éè‡ªå›å½’çš„é™é»˜æ€ç»´ï¼ˆSTï¼‰æ›¿ä»£è‡ªå›å½’CoTã€‚DARTå¼•å…¥äº†ä¸¤æ¡è®­ç»ƒè·¯å¾„ï¼šCoTè·¯å¾„ç”¨äºä¼ ç»Ÿæ¨ç†ï¼ŒSTè·¯å¾„åˆ™ç›´æ¥ä»å°‘é‡STæ ‡è®°ç”Ÿæˆç­”æ¡ˆã€‚STè·¯å¾„åˆ©ç”¨è½»é‡çº§çš„æ¨ç†æ¼”å˜æ¨¡å—ï¼ˆREMï¼‰å¯¹é½å…¶éšè—çŠ¶æ€ä¸CoTè·¯å¾„ï¼Œä½¿STæ ‡è®°æ¼”å˜ä¸ºä¿¡æ¯ä¸°å¯Œçš„åµŒå…¥ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä»…æ¿€æ´»STè·¯å¾„ï¼Œåˆ©ç”¨æ¼”å˜çš„STæ ‡è®°ç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDARTåœ¨ä¸å¢åŠ æ¨ç†å»¶è¿Ÿçš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºç°æœ‰çš„éè‡ªå›å½’åŸºçº¿æä¾›äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªå›å½’æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è®¡ç®—å¼€é”€é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨åœºæ™¯ä¸­éš¾ä»¥éƒ¨ç½²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDARTé€šè¿‡è‡ªè’¸é¦æ¡†æ¶ï¼Œå°†è‡ªå›å½’çš„é“¾å¼æ¨ç†æ›¿æ¢ä¸ºéè‡ªå›å½’çš„é™é»˜æ€ç»´ï¼Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDARTåŒ…å«ä¸¤æ¡ä¸»è¦çš„è®­ç»ƒè·¯å¾„ï¼šCoTè·¯å¾„ç”¨äºä¼ ç»Ÿæ¨ç†ï¼ŒSTè·¯å¾„åˆ™ç›´æ¥ä»å°‘é‡STæ ‡è®°ç”Ÿæˆç­”æ¡ˆã€‚STè·¯å¾„é€šè¿‡æ¨ç†æ¼”å˜æ¨¡å—ï¼ˆREMï¼‰å¯¹é½éšè—çŠ¶æ€ï¼Œç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ã€‚

**å…³é”®åˆ›æ–°**ï¼šDARTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†é™é»˜æ€ç»´è·¯å¾„ï¼Œå…è®¸æ¨¡å‹åœ¨æ¨ç†æ—¶ä»…ä¾èµ–äºæ¼”å˜çš„STæ ‡è®°ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒREMæ¨¡å—çš„ç»“æ„å’Œå‚æ•°è®¾ç½®è‡³å…³é‡è¦ï¼Œç¡®ä¿STæ ‡è®°èƒ½å¤Ÿæœ‰æ•ˆæ¼”å˜ä¸ºæœ‰ç”¨çš„åµŒå…¥ï¼ŒåŒæ—¶æŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿå½±å“äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDARTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰çš„éè‡ªå›å½’åŸºçº¿ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒæ¨ç†å»¶è¿Ÿä¸å˜ï¼Œå±•ç°äº†å…¶ä½œä¸ºé«˜æ•ˆæ¨ç†æ–¹æ¡ˆçš„å¯è¡Œæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DARTçš„ç ”ç©¶æˆæœåœ¨å»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨åœºæ™¯ä¸­å…·æœ‰å¹¿æ³›çš„æ½œåœ¨åº”ç”¨ï¼Œå¦‚å®æ—¶å¯¹è¯ç³»ç»Ÿã€åœ¨çº¿å®¢æœå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡é™ä½æ¨ç†å»¶è¿Ÿï¼ŒDARTèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-Thought (CoT) reasoning has significantly advanced Large Language Models (LLMs) in solving complex tasks. However, its autoregressive paradigm leads to significant computational overhead, hindering its deployment in latency-sensitive applications. To address this, we propose \textbf{DART} (\textbf{D}istilling \textbf{A}utoregressive \textbf{R}easoning to Silent \textbf{T}hought), a self-distillation framework that enables LLMs to replace autoregressive CoT with non-autoregressive Silent Thought (ST). Specifically, DART introduces two training pathways: the CoT pathway for traditional reasoning and the ST pathway for generating answers directly from a few ST tokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM) to align its hidden states with the CoT pathway, enabling the ST tokens to evolve into informative embeddings. During inference, only the ST pathway is activated, leveraging evolving ST tokens to deliver the answer directly. Extensive experimental results demonstrate that DART offers significant performance gains compared with existing non-autoregressive baselines without extra inference latency, serving as a feasible alternative for efficient reasoning.

