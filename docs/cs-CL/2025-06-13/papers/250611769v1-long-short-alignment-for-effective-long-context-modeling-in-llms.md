---
layout: default
title: Long-Short Alignment for Effective Long-Context Modeling in LLMs
---

# Long-Short Alignment for Effective Long-Context Modeling in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11769" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.11769v1</a>
  <a href="https://arxiv.org/pdf/2506.11769.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11769v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11769v1', 'Long-Short Alignment for Effective Long-Context Modeling in LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Tianqi Du, Haotian Huang, Yifei Wang, Yisen Wang

**ÂàÜÁ±ª**: cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-13

**Â§áÊ≥®**: ICML 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/PKU-ML/LongShortAlignment)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÈïøÁü≠ÂØπÈΩêÊñπÊ≥ï‰ª•Ëß£ÂÜ≥Èïø‰∏ä‰∏ãÊñáÂª∫Ê®°ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Èïø‰∏ä‰∏ãÊñáÂª∫Ê®°` `ÈïøÂ∫¶Ê≥õÂåñ` `ÈïøÁü≠ÂØπÈΩê` `ËæìÂá∫ÂàÜÂ∏É` `Ê≠£ÂàôÂåñÊñπÊ≥ï` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈïø‰∏ä‰∏ãÊñáÂª∫Ê®°ÊñπÊ≥ïÂèóÂà∞Âõ∫ÂÆö‰∏ä‰∏ãÊñáÁ™óÂè£ÁöÑÈôêÂà∂ÔºåÂØºËá¥ÈïøÂ∫¶Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜÈïøÁü≠ÂØπÈΩêÁöÑÊ¶ÇÂøµÔºåÂÖ≥Ê≥®Ê®°ÂûãËæìÂá∫ÂàÜÂ∏ÉÁöÑ‰∏ÄËá¥ÊÄßÔºå‰ª•ÊèêÂçáÈïøÂ∫¶Ê≥õÂåñËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈïøÁü≠ÂØπÈΩêÁöÑÊ≠£ÂàôÂåñÈ°πÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂú®Èïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ±ïÁé∞‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊÄßËÉΩÂíåÊÑèÂ§ñÁöÑÊ∂åÁé∞ÁâπÊÄß„ÄÇÁÑ∂ËÄåÔºåÂõ∫ÂÆöÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£ÈôêÂà∂‰∫ÜÂÖ∂Âú®Èïø‰∏ä‰∏ãÊñáÂª∫Ê®°‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈïøÂ∫¶Ê≥õÂåñÊñπÈù¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßíÔºåÂº∫Ë∞ÉËæìÂá∫ÂàÜÂ∏ÉÁöÑ‰∏ÄËá¥ÊÄßÔºåÂç≥ÈïøÁü≠ÂØπÈΩê„ÄÇÈÄöËøáÂú®ÂêàÊàê‰ªªÂä°‰∏äÁöÑÊ°à‰æãÁ†îÁ©∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ÈïøÁü≠‰∏çÂØπÈΩêÁöÑÂ∫¶ÈáèÔºåÈáèÂåñËøô‰∏ÄÁé∞Ë±°ÔºåÂπ∂ÂèëÁé∞ËØ•Â∫¶Èáè‰∏éÈïøÂ∫¶Ê≥õÂåñÊÄßËÉΩ‰πãÈó¥Â≠òÂú®Âº∫Áõ∏ÂÖ≥ÊÄß„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçÊ≠£ÂàôÂåñÈ°πÔºå‰ª•‰øÉËøõËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÈïøÁü≠ÂØπÈΩê„ÄÇÂ§ßÈáèÂÆûÈ™åÈ™åËØÅ‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºå‰∏∫LLMsÁöÑÈïø‰∏ä‰∏ãÊñáÂª∫Ê®°Êèê‰æõ‰∫ÜÊñ∞ÁöÑËßÅËß£„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Èïø‰∏ä‰∏ãÊñáÂª∫Ê®°‰∏≠ÁöÑÈïøÂ∫¶Ê≥õÂåñÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®ËæìÂÖ•ÁâπÂæÅÔºåÂ¶Ç‰ΩçÁΩÆÁºñÁ†ÅÔºåÊú™ËÉΩÊúâÊïàÊèêÂçáÊ®°ÂûãÂØπÈïøÂ∫èÂàóÁöÑÂ§ÑÁêÜËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫ÜÈïøÁü≠ÂØπÈΩêÁöÑÊ¶ÇÂøµÔºåÂº∫Ë∞ÉËæìÂá∫ÂàÜÂ∏ÉÂú®‰∏çÂêåÈïøÂ∫¶Â∫èÂàó‰∏äÁöÑ‰∏ÄËá¥ÊÄßÔºåËÆ§‰∏∫ËøôÊòØÊèêÂçáÈïøÂ∫¶Ê≥õÂåñËÉΩÂäõÁöÑÂÖ≥ÈîÆ„ÄÇÈÄöËøáÂºïÂÖ•ÈïøÁü≠‰∏çÂØπÈΩêÂ∫¶ÈáèÔºåÈáèÂåñËæìÂá∫‰∏ÄËá¥ÊÄßÔºåËøõËÄå‰ºòÂåñÊ®°ÂûãËÆ≠ÁªÉ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÊ®°ÂûãËÆ≠ÁªÉÂíåËØÑ‰º∞‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇÂú®ËÆ≠ÁªÉÈò∂ÊÆµÔºåÂä†ÂÖ•ÈïøÁü≠ÂØπÈΩêÁöÑÊ≠£ÂàôÂåñÈ°πÔºå‰ª•ÂºïÂØºÊ®°ÂûãÂ≠¶‰π†Êõ¥‰∏ÄËá¥ÁöÑËæìÂá∫ÂàÜÂ∏É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÈïøÁü≠ÂØπÈΩêÁöÑÂ∫¶ÈáèÊñπÊ≥ïÔºåÂπ∂ÈÄöËøáÊ≠£ÂàôÂåñÈ°πÂú®ËÆ≠ÁªÉ‰∏≠ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†á„ÄÇËøô‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåÂêéËÄÖ‰∏ªË¶ÅÂÖ≥Ê≥®ËæìÂÖ•ÁâπÂæÅÁöÑËÆæËÆ°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆ≠ÁªÉ‰∏≠ÔºåËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰øÉËøõÈïøÁü≠ÂØπÈΩêÔºåÂêåÊó∂Ë∞ÉÊï¥‰∫ÜÁΩëÁªúÁªìÊûÑ‰ª•ÈÄÇÂ∫îÈïø‰∏ä‰∏ãÊñáÁöÑÈúÄÊ±Ç„ÄÇÂÆûÈ™å‰∏≠‰ΩøÁî®‰∫ÜÂ§öÁßçÂêàÊàê‰ªªÂä°ÂíåËá™ÁÑ∂ËØ≠Ë®Ä‰ªªÂä°ËøõË°åÈ™åËØÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÈááÁî®ÈïøÁü≠ÂØπÈΩêÊ≠£ÂàôÂåñÁöÑÊ®°ÂûãÂú®ÈïøÂ∫¶Ê≥õÂåñ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊ®°ÂûãÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞15%‰ª•‰∏äÔºåÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÂØπËØùÁ≥ªÁªüÂíåÊñáÊú¨ÁîüÊàêÁ≠â„ÄÇÈÄöËøáÊèêÂçáÈïø‰∏ä‰∏ãÊñáÂª∫Ê®°ËÉΩÂäõÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÁîüÊàêÈïøÁØáÊñáÊú¨ÔºåÂÖ∑ÊúâÊõ¥ÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂ∫îÁî®ÂâçÊôØÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÂ§ÑÁêÜÂ§çÊùÇ‰∏ä‰∏ãÊñáÁöÑÂú∫ÊôØ‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) have exhibited impressive performance and surprising emergent properties. However, their effectiveness remains limited by the fixed context window of the transformer architecture, posing challenges for long-context modeling. Among these challenges, length generalization -- the ability to generalize to sequences longer than those seen during training -- is a classical and fundamental problem. In this work, we propose a fresh perspective on length generalization, shifting the focus from the conventional emphasis on input features such as positional encodings or data structures to the output distribution of the model. Specifically, through case studies on synthetic tasks, we highlight the critical role of \textbf{long-short alignment} -- the consistency of output distributions across sequences of varying lengths. Extending this insight to natural language tasks, we propose a metric called Long-Short Misalignment to quantify this phenomenon, uncovering a strong correlation between the metric and length generalization performance. Building on these findings, we develop a regularization term that promotes long-short alignment during training. Extensive experiments validate the effectiveness of our approach, offering new insights for achieving more effective long-context modeling in LLMs. Code is available at https://github.com/PKU-ML/LongShortAlignment.

