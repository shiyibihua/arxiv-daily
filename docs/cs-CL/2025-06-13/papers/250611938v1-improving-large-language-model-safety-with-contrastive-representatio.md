---
layout: default
title: Improving Large Language Model Safety with Contrastive Representation Learning
---

# Improving Large Language Model Safety with Contrastive Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11938" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11938v1</a>
  <a href="https://arxiv.org/pdf/2506.11938.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11938v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11938v1', 'Improving Large Language Model Safety with Contrastive Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Samuel Simko, Mrinmaya Sachan, Bernhard SchÃ¶lkopf, Zhijing Jin

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/samuelsimko/crl-llm-defense)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æ¯”è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯¹æŠ—æ€§æ”»å‡»` `å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ ` `æ¨¡å‹é˜²å¾¡` `é²æ£’æ€§æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é˜²å¾¡æ–¹æ³•åœ¨åº”å¯¹ä¸åŒç±»å‹çš„å¯¹æŠ—æ€§æ”»å‡»æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œå¯¼è‡´å®‰å…¨æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é˜²å¾¡æ¡†æ¶ï¼Œå°†æ¨¡å‹é˜²å¾¡è§†ä¸ºå¯¹æ¯”è¡¨ç¤ºå­¦ä¹ é—®é¢˜ï¼Œé€šè¿‡ä¸‰å…ƒç»„æŸå¤±å’Œéš¾è´Ÿæ ·æœ¬æŒ–æ˜å®ç°è‰¯æ€§ä¸æœ‰å®³è¡¨ç¤ºçš„åˆ†ç¦»ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡äº†å¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ï¼Œä¸”æœªæŸå®³æ¨¡å‹çš„æ ‡å‡†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå¤šæ ·åŒ–å’Œä¸å—æ§è¾“å…¥çš„å“åº”æ—¶ï¼Œå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ã€‚ç°æœ‰çš„é˜²å¾¡æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨ä¸åŒæ”»å‡»ç±»å‹ä¹‹é—´è¿›è¡Œæœ‰æ•ˆæ³›åŒ–ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†æ¨¡å‹é˜²å¾¡è§†ä¸ºå¯¹æ¯”è¡¨ç¤ºå­¦ä¹ ï¼ˆCRLï¼‰é—®é¢˜çš„æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨åŸºäºä¸‰å…ƒç»„çš„æŸå¤±å‡½æ•°å’Œå¯¹æŠ—æ€§éš¾è´Ÿæ ·æœ¬æŒ–æ˜ï¼Œé¼“åŠ±è‰¯æ€§å’Œæœ‰å®³è¡¨ç¤ºä¹‹é—´çš„åˆ†ç¦»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šè¶…è¶Šäº†ä»¥å¾€çš„è¡¨ç¤ºå·¥ç¨‹é˜²å¾¡ï¼Œæå‡äº†å¯¹è¾“å…¥çº§å’ŒåµŒå…¥ç©ºé—´æ”»å‡»çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¸å½±å“æ ‡å‡†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶çš„å®‰å…¨æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåº”å¯¹ä¸åŒç±»å‹çš„æ”»å‡»ï¼Œå¯¼è‡´æ¨¡å‹æ˜“å—æ”»å‡»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶æå‡ºå°†æ¨¡å‹é˜²å¾¡è§†ä¸ºå¯¹æ¯”è¡¨ç¤ºå­¦ä¹ é—®é¢˜ï¼Œé€šè¿‡ä¼˜åŒ–ä¸‰å…ƒç»„æŸå¤±å‡½æ•°ï¼Œå¢å¼ºè‰¯æ€§å’Œæœ‰å®³æ ·æœ¬ä¹‹é—´çš„è¡¨ç¤ºåˆ†ç¦»ï¼Œä»è€Œæå‡æ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ä¸‰å…ƒç»„æ ·æœ¬ç”Ÿæˆã€æ¨¡å‹å¾®è°ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆç”Ÿæˆè‰¯æ€§å’Œæœ‰å®³æ ·æœ¬çš„ä¸‰å…ƒç»„ï¼Œç„¶åé€šè¿‡å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ åº”ç”¨äºæ¨¡å‹é˜²å¾¡ï¼Œé€šè¿‡éš¾è´Ÿæ ·æœ¬æŒ–æ˜æå‡äº†æ¨¡å‹å¯¹æŠ—æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨åŸºäºä¸‰å…ƒç»„çš„æŸå¤±å‡½æ•°ï¼Œç»“åˆå¯¹æŠ—æ€§éš¾è´Ÿæ ·æœ¬æŒ–æ˜ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°è‰¯æ€§ä¸æœ‰å®³æ ·æœ¬çš„åŒºåˆ†ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç»“æ„è®¾è®¡ä¸Šä¿æŒäº†ä¸ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å…¼å®¹æ€§ï¼Œç¡®ä¿äº†æ€§èƒ½çš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šå‡ä¼˜äºä»¥å¾€çš„è¡¨ç¤ºå·¥ç¨‹é˜²å¾¡ï¼Œå°¤å…¶åœ¨å¯¹æŠ—æ€§æ”»å‡»çš„é²æ£’æ€§æ–¹é¢ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ ‡å‡†æ€§èƒ½ä¸å˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¯¹æŠ—æ€§é˜²å¾¡ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿçš„å®‰å…¨æ€§æå‡ä»¥åŠè‡ªåŠ¨å†…å®¹ç”Ÿæˆçš„é£é™©ç®¡ç†ã€‚é€šè¿‡å¢å¼ºæ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå¯ä»¥åœ¨æ›´å¹¿æ³›çš„åœºæ™¯ä¸­åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‡å°‘å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåœ¨é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are powerful tools with profound societal impacts, yet their ability to generate responses to diverse and uncontrolled inputs leaves them vulnerable to adversarial attacks. While existing defenses often struggle to generalize across varying attack types, recent advancements in representation engineering offer promising alternatives. In this work, we propose a defense framework that formulates model defense as a contrastive representation learning (CRL) problem. Our method finetunes a model using a triplet-based loss combined with adversarial hard negative mining to encourage separation between benign and harmful representations. Our experimental results across multiple models demonstrate that our approach outperforms prior representation engineering-based defenses, improving robustness against both input-level and embedding-space attacks without compromising standard performance. Our code is available at https://github.com/samuelsimko/crl-llm-defense

