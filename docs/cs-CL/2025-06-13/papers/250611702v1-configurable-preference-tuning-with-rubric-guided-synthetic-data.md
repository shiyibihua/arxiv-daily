---
layout: default
title: Configurable Preference Tuning with Rubric-Guided Synthetic Data
---

# Configurable Preference Tuning with Rubric-Guided Synthetic Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11702" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11702v1</a>
  <a href="https://arxiv.org/pdf/2506.11702.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11702v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11702v1', 'Configurable Preference Tuning with Rubric-Guided Synthetic Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: VÃ­ctor Gallego

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: Accepted to ICML 2025 Workshop on Models of Human Feedback for AI Alignment

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/vicgalle/configurable-preference-tuning)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯é…ç½®åå¥½è°ƒä¼˜æ¡†æ¶ä»¥è§£å†³é™æ€åå¥½é™åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `äººç±»åé¦ˆæ¨¡å‹` `å¯é…ç½®åå¥½è°ƒä¼˜` `åŠ¨æ€è°ƒæ•´` `åˆæˆæ•°æ®` `ç»†ç²’åº¦æ§åˆ¶` `è¯­è¨€æ¨¡å‹` `ç”¨æˆ·æ»¡æ„åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„äººç±»åé¦ˆæ¨¡å‹é€šå¸¸ä¾èµ–äºé™æ€çš„åå¥½é›†ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹å¤šæ ·åŒ–éœ€æ±‚æ—¶ç¼ºä¹çµæ´»æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„å¯é…ç½®åå¥½è°ƒä¼˜ï¼ˆCPTï¼‰æ¡†æ¶ï¼Œå…è®¸è¯­è¨€æ¨¡å‹æ ¹æ®å…·ä½“çš„ã€å¯è§£é‡Šçš„æŒ‡ä»¤åŠ¨æ€è°ƒæ•´è¾“å‡ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCPTæ¡†æ¶æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç°ï¼Œæä¾›äº†æ›´ç»†è‡´çš„æ§åˆ¶èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»åé¦ˆæ¨¡å‹åœ¨AIå¯¹é½ä¸­ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œé€šå¸¸é‡‡ç”¨å•ä¸€é™æ€åå¥½é›†ï¼Œé™åˆ¶äº†é€‚åº”æ€§ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥å¯é…ç½®åå¥½è°ƒä¼˜ï¼ˆCPTï¼‰æ¡†æ¶ï¼ŒæŒ‘æˆ˜äº†å•ä¸€åå¥½çš„å‡è®¾ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ˜ç¡®çš„äººç±»å¯è§£é‡ŠæŒ‡ä»¤åŠ¨æ€è°ƒæ•´å…¶è¡Œä¸ºã€‚CPTåˆ©ç”¨åˆæˆç”Ÿæˆçš„åå¥½æ•°æ®ï¼ŒåŸºäºç»“æ„åŒ–çš„ç»†ç²’åº¦è¯„åˆ†æ ‡å‡†å®šä¹‰æ‰€éœ€å±æ€§ï¼Œå¦‚å†™ä½œé£æ ¼ã€‚é€šè¿‡ä½¿ç”¨è¿™äº›è¯„åˆ†æŒ‡å¯¼çš„åå¥½è¿›è¡Œå¾®è°ƒï¼ŒLLMèƒ½å¤Ÿåœ¨æ¨ç†æ—¶æ ¹æ®ç³»ç»Ÿæç¤ºè°ƒèŠ‚å…¶è¾“å‡ºï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¿™ç§æ–¹æ³•ä¸ä»…æä¾›äº†ç»†ç²’åº¦æ§åˆ¶ï¼Œè¿˜ä¸ºå»ºæ¨¡æ›´ç»†è‡´å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„äººç±»åé¦ˆæä¾›äº†æœºåˆ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†äººç±»åé¦ˆæ—¶çš„é™æ€åå¥½é™åˆ¶é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹é€‚åº”æ€§ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³å¤šæ ·åŒ–çš„ç”¨æˆ·éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¯é…ç½®åå¥½è°ƒä¼˜ï¼ˆCPTï¼‰æ¡†æ¶ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ˜ç¡®çš„ã€å¯è§£é‡Šçš„æŒ‡ä»¤åŠ¨æ€è°ƒæ•´å…¶è¡Œä¸ºï¼Œä»è€Œæé«˜æ¨¡å‹çš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCPTæ¡†æ¶åŒ…æ‹¬åˆæˆç”Ÿæˆçš„åå¥½æ•°æ®ã€åŸºäºç»“æ„åŒ–è¯„åˆ†æ ‡å‡†çš„ç³»ç»Ÿæç¤ºå’Œå¾®è°ƒè¿‡ç¨‹ã€‚æ¨¡å‹åœ¨æ¨ç†æ—¶æ ¹æ®è¾“å…¥çš„ç³»ç»Ÿæç¤ºè°ƒæ•´è¾“å‡ºï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šCPTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡åˆæˆçš„åå¥½æ•°æ®å’Œç»†ç²’åº¦è¯„åˆ†æ ‡å‡†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†é˜¶æ®µåŠ¨æ€è°ƒæ•´è¾“å‡ºï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„é™æ€åå¥½è®¾ç½®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç»“æ„åŒ–çš„è¯„åˆ†æ ‡å‡†æ¥å®šä¹‰æ‰€éœ€çš„å†™ä½œé£æ ¼ç­‰å±æ€§ï¼Œå¹¶é€šè¿‡å¾®è°ƒè¿‡ç¨‹ä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿçµæ´»å“åº”ä¸åŒçš„ç”¨æˆ·æŒ‡ä»¤ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨CPTæ¡†æ¶çš„æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œè¾“å‡ºçš„ç›¸å…³æ€§å’Œç”¨æˆ·æ»¡æ„åº¦æé«˜äº†20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ¨æ€é€‚åº”æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸ªæ€§åŒ–å†…å®¹ç”Ÿæˆã€æ•™è‚²é¢†åŸŸçš„è‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿä»¥åŠäººæœºäº¤äº’ä¸­çš„æ™ºèƒ½åŠ©æ‰‹ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´æ¨¡å‹è¡Œä¸ºï¼ŒCPTæ¡†æ¶èƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨å¤šä¸ªè¡Œä¸šä¸­äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning

