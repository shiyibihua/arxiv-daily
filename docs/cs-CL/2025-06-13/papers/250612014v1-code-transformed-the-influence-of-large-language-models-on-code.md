---
layout: default
title: code_transformed: The Influence of Large Language Models on Code
---

# code_transformed: The Influence of Large Language Models on Code

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12014" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12014v1</a>
  <a href="https://arxiv.org/pdf/2506.12014.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12014v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12014v1', 'code_transformed: The Influence of Large Language Models on Code')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuliang Xu, Siming Huang, Mingmeng Geng, Yao Wan, Xuanhua Shi, Dongping Chen

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, cs.SE

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: We release all the experimental dataset and source code at: https://github.com/ignorancex/LLM_code

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶LLMå¯¹ä»£ç é£æ ¼çš„å½±å“åŠå…¶ç‰¹å¾åˆ†æ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç é£æ ¼` `ç¼–ç¨‹å®è·µ` `å‘½åè§„èŒƒ` `å¤æ‚æ€§åˆ†æ` `å¯ç»´æŠ¤æ€§` `GitHubæ•°æ®åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä»£ç é£æ ¼çš„å½±å“ç¼ºä¹ç³»ç»Ÿæ€§åˆ†æï¼Œéš¾ä»¥é‡åŒ–å…¶å˜åŒ–ã€‚
2. æœ¬æ–‡é€šè¿‡åˆ†æå¤§é‡GitHubä»£ç åº“ï¼Œæå‡ºäº†ä¸€ç§é‡åŒ–LLMså¯¹ä»£ç é£æ ¼å½±å“çš„æ–¹æ³•ï¼Œå…³æ³¨å‘½åè§„èŒƒã€å¤æ‚æ€§ã€å¯ç»´æŠ¤æ€§ç­‰æ–¹é¢ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsçš„ä½¿ç”¨ä¸ä»£ç é£æ ¼çš„æ¼”å˜å­˜åœ¨æ˜¾è‘—å…³è”ï¼Œä¾‹å¦‚å˜é‡å‘½åé£æ ¼çš„å˜åŒ–è¶‹åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¼–ç¨‹æ˜¯äººæœºäº¤äº’çš„åŸºæœ¬æ–¹å¼ä¹‹ä¸€ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä»£ç ç”Ÿæˆèƒ½åŠ›å¼€å§‹æ˜¾è‘—æ”¹å˜ç¼–ç¨‹å®è·µã€‚æœ¬æ–‡æ¢è®¨äº†LLMsæ˜¯å¦æ”¹å˜äº†ä»£ç é£æ ¼ï¼Œå¹¶å¦‚ä½•è¡¨å¾è¿™ç§å˜åŒ–ã€‚é€šè¿‡åˆ†æ2020è‡³2025å¹´é—´ä¸arXivè®ºæ–‡ç›¸å…³çš„19,000å¤šä¸ªGitHubä»£ç åº“ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºä¸LLMç”Ÿæˆä»£ç ç‰¹å¾ç›¸ä¸€è‡´çš„å¯æµ‹é‡çš„ç¼–ç é£æ ¼æ¼”å˜è¶‹åŠ¿ã€‚ä¾‹å¦‚ï¼ŒPythonä»£ç ä¸­snake_caseå˜é‡åçš„æ¯”ä¾‹ä»2023å¹´ç¬¬ä¸€å­£åº¦çš„47%å¢åŠ åˆ°2025å¹´ç¬¬ä¸€å­£åº¦çš„51%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†LLMsåœ¨ç®—æ³•é—®é¢˜ä¸Šçš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœæä¾›äº†LLMså½±å“ç°å®ç¼–ç¨‹é£æ ¼çš„é¦–ä¸ªå¤§è§„æ¨¡å®è¯è¯æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä»£ç é£æ ¼å½±å“çš„é‡åŒ–åˆ†æé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ä»£ç é£æ ¼å˜åŒ–çš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œéš¾ä»¥è¯„ä¼°LLMsçš„å®é™…å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†æä¸arXivè®ºæ–‡ç›¸å…³çš„GitHubä»£ç åº“ï¼Œè¯†åˆ«å‡ºä»£ç é£æ ¼çš„æ¼”å˜è¶‹åŠ¿ï¼Œç‰¹åˆ«æ˜¯å‘½åè§„èŒƒå’Œå¤æ‚æ€§ç­‰æ–¹é¢çš„å˜åŒ–ï¼Œä»¥æ­¤æ¥è¡¨å¾LLMsçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨æ•°æ®æŒ–æ˜å’Œç»Ÿè®¡åˆ†æçš„æ–¹æ³•ï¼Œé¦–å…ˆæ”¶é›†ç›¸å…³ä»£ç åº“æ•°æ®ï¼Œç„¶åè¿›è¡Œå‘½åè§„èŒƒã€å¤æ‚æ€§å’Œå¯ç»´æŠ¤æ€§ç­‰ç‰¹å¾çš„é‡åŒ–åˆ†æï¼Œæœ€åé€šè¿‡å¯¹æ¯”åˆ†æå¾—å‡ºç»“è®ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºé¦–æ¬¡å¤§è§„æ¨¡å®è¯ç ”ç©¶LLMså¯¹ä»£ç é£æ ¼çš„å½±å“ï¼Œæä¾›äº†å¯é‡åŒ–çš„è¶‹åŠ¿æ•°æ®ï¼Œå¡«è¡¥äº†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ”¶é›†é˜¶æ®µï¼Œé€‰æ‹©äº†2020è‡³2025å¹´é—´çš„19,000å¤šä¸ªGitHubä»£ç åº“ï¼Œåˆ†æäº†ä¸åŒæ—¶é—´æ®µå†…çš„å˜é‡å‘½åé£æ ¼å˜åŒ–ï¼Œä½¿ç”¨ç»Ÿè®¡æ–¹æ³•è¯„ä¼°äº†ä»£ç å¤æ‚æ€§å’Œå¯ç»´æŠ¤æ€§ç­‰æŒ‡æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPythonä»£ç ä¸­snake_caseå˜é‡åçš„æ¯”ä¾‹ä»2023å¹´ç¬¬ä¸€å­£åº¦çš„47%å¢åŠ åˆ°2025å¹´ç¬¬ä¸€å­£åº¦çš„51%ã€‚è¿™ä¸€å˜åŒ–è¡¨æ˜ï¼ŒLLMsçš„ä½¿ç”¨ä¸ä»£ç é£æ ¼çš„æ¼”å˜å­˜åœ¨æ˜¾è‘—å…³è”ï¼Œä¸ºç†è§£ç°ä»£ç¼–ç¨‹å®è·µæä¾›äº†é‡è¦çš„å®è¯ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€ä»£ç å®¡æŸ¥å’Œæ•™è‚²ç­‰ã€‚é€šè¿‡ç†è§£LLMså¯¹ä»£ç é£æ ¼çš„å½±å“ï¼Œå¼€å‘è€…å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨è¿™äº›æ¨¡å‹æé«˜ä»£ç è´¨é‡å’Œå¯ç»´æŠ¤æ€§ã€‚æ­¤å¤–ï¼Œæ•™è‚²æœºæ„å¯ä»¥æ ¹æ®è¿™äº›è¶‹åŠ¿è°ƒæ•´ç¼–ç¨‹æ•™å­¦å†…å®¹ï¼Œä»¥é€‚åº”æ–°çš„ç¼–ç¨‹å®è·µã€‚æœªæ¥ï¼Œéšç€LLMsçš„è¿›ä¸€æ­¥å‘å±•ï¼Œç›¸å…³ç ”ç©¶å°†ç»§ç»­æ¨åŠ¨ç¼–ç¨‹é¢†åŸŸçš„åˆ›æ–°ä¸å˜é©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 19,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake\_case variable names in Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Given the diversity of LLMs and usage scenarios, among other factors, it is difficult or even impossible to precisely estimate the proportion of code generated or assisted by LLMs. Our experimental results provide the first large-scale empirical evidence that LLMs affect real-world programming style.

