---
layout: default
title: Efficient Long-Context LLM Inference via KV Cache Clustering
---

# Efficient Long-Context LLM Inference via KV Cache Clustering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11418" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11418v1</a>
  <a href="https://arxiv.org/pdf/2506.11418.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11418v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11418v1', 'Efficient Long-Context LLM Inference via KV Cache Clustering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jie Hu, Shengnan Wang, Yutong He, Ping Gong, Jiawei Yi, Juncheng Zhang, Youhui Bai, Renhai Chen, Gong Zhang, Cheng Li, Kun Yuan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºChelseaæ¡†æ¶ä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†ä¸­çš„KVç¼“å­˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡LLM` `KVç¼“å­˜` `åœ¨çº¿èšç±»` `å—çŠ¶è½¯åŒ¹é…` `æ¨ç†åŠ é€Ÿ` `å†…å­˜ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿ä¸Šä¸‹æ–‡LLMçš„KVç¼“å­˜éœ€æ±‚åºå¤§ï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆä¸¢å¤±å…³é”®ä¿¡æ¯ï¼Œè¦ä¹ˆæ•ˆç‡æå‡æœ‰é™ã€‚
2. æå‡ºChelseaæ¡†æ¶ï¼Œé€šè¿‡å—çŠ¶è½¯åŒ¹é…å®ç°KVç¼“å­˜çš„é«˜æ•ˆèšç±»ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChelseaåœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒKVç¼“å­˜ä½¿ç”¨å‡å°‘80%ï¼Œæ¨ç†é€Ÿåº¦æå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œé•¿ä¸Šä¸‹æ–‡LLMæ‰€éœ€çš„åºå¤§KVç¼“å­˜å¸¦æ¥äº†æ˜¾è‘—çš„éƒ¨ç½²æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¼šä¸¢å¼ƒæœªæ¥ç”Ÿæˆæ‰€éœ€çš„é‡è¦ä¿¡æ¯ï¼Œæˆ–å› è®¡ç®—å¼€é”€å¤§è€Œæ•ˆç‡æå‡æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†Chelseaï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„åœ¨çº¿KVç¼“å­˜èšç±»æ¡†æ¶ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå…³é”®çŠ¶æ€åœ¨åºåˆ—ç»´åº¦ä¸Šè¡¨ç°å‡ºé«˜åº¦ç›¸ä¼¼æ€§ã€‚ä¸ºå®ç°é«˜æ•ˆèšç±»ï¼Œæˆ‘ä»¬å°†åºåˆ—åˆ’åˆ†ä¸ºå—ï¼Œå¹¶æå‡ºäº†å—çŠ¶è½¯åŒ¹é…æ–¹æ³•ï¼Œé€šè¿‡äº¤æ›¿åˆ’åˆ†ç­–ç•¥è¯†åˆ«ç›¸ä¼¼æ€§èšç±»ã€‚Chelseaå°†æ¯ä¸ªèšç±»ä¸­çš„KVç¼“å­˜åˆå¹¶ä¸ºä¸€ä¸ªè´¨å¿ƒã€‚å®éªŒè¡¨æ˜ï¼ŒChelseaåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼ŒKVç¼“å­˜å†…å­˜ä½¿ç”¨å‡å°‘é«˜è¾¾80%ï¼Œæ¨ç†è§£ç é˜¶æ®µåŠ é€Ÿæœ€é«˜å¯è¾¾3.19å€ï¼Œç«¯åˆ°ç«¯å»¶è¿Ÿå‡å°‘æœ€é«˜å¯è¾¾2.72å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†ä¸­KVç¼“å­˜çš„é«˜å†…å­˜éœ€æ±‚å’Œè®¡ç®—å¼€é”€é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¼šä¸¢å¤±é‡è¦ä¿¡æ¯æˆ–æ•ˆç‡æå‡æœ‰é™ï¼Œå¯¼è‡´å®é™…åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºChelseaæ¡†æ¶ï¼Œåˆ©ç”¨å…³é”®çŠ¶æ€åœ¨åºåˆ—ç»´åº¦ä¸Šçš„ç›¸ä¼¼æ€§ï¼Œé€šè¿‡å—çŠ¶åˆ’åˆ†å’Œè½¯åŒ¹é…ç­–ç•¥å®ç°KVç¼“å­˜çš„é«˜æ•ˆèšç±»ï¼Œä»è€Œå‡å°‘å†…å­˜ä½¿ç”¨å¹¶åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šChelseaçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åºåˆ—åˆ’åˆ†ã€å—çŠ¶è½¯åŒ¹é…ã€èšç±»è¯†åˆ«å’ŒKVç¼“å­˜åˆå¹¶å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œå°†è¾“å…¥åºåˆ—åˆ’åˆ†ä¸ºå¤šä¸ªå—ï¼Œç„¶ååœ¨æ¯ä¸ªå—å†…è¿›è¡Œè½¯åŒ¹é…ä»¥è¯†åˆ«ç›¸ä¼¼æ€§ï¼Œæœ€åå°†ç›¸ä¼¼çš„KVç¼“å­˜åˆå¹¶ä¸ºè´¨å¿ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šChelseaçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå—çŠ¶è½¯åŒ¹é…ç­–ç•¥å’Œèšç±»åˆå¹¶æœºåˆ¶ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ç›´æ¥ä¸¢å¼ƒæˆ–ç®€å•åˆå¹¶ç­–ç•¥æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ç•™å…³é”®ä¿¡æ¯å¹¶æé«˜æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†äº¤æ›¿åˆ’åˆ†ç­–ç•¥æ¥ä¼˜åŒ–å—å†…çš„ç›¸ä¼¼æ€§è¯†åˆ«ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æéªŒè¯äº†è®¡ç®—å¤æ‚åº¦å’Œå†…éƒ¨åˆ’åˆ†ç­–ç•¥çš„æœ€ä¼˜æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒChelseaåœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾80%çš„KVç¼“å­˜å†…å­˜ä½¿ç”¨å‡å°‘ï¼ŒåŒæ—¶åœ¨æ¨ç†è§£ç é˜¶æ®µåŠ é€Ÿæœ€é«˜å¯è¾¾3.19å€ï¼Œç«¯åˆ°ç«¯å»¶è¿Ÿå‡å°‘æœ€é«˜å¯è¾¾2.72å€ï¼Œå±•ç°äº†å…¶åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Chelseaæ¡†æ¶åœ¨é•¿ä¸Šä¸‹æ–‡LLMçš„æ¨ç†ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦å¤„ç†å¤§é‡ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚å…¶æ˜¾è‘—çš„å†…å­˜å’Œé€Ÿåº¦ä¼˜åŒ–å°†æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²å’Œæ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) with extended context windows have become increasingly prevalent for tackling complex tasks. However, the substantial Key-Value (KV) cache required for long-context LLMs poses significant deployment challenges. Existing approaches either discard potentially critical information needed for future generations or offer limited efficiency gains due to high computational overhead. In this paper, we introduce Chelsea, a simple yet effective framework for online KV cache clustering. Our approach is based on the observation that key states exhibit high similarity along the sequence dimension. To enable efficient clustering, we divide the sequence into chunks and propose Chunked Soft Matching, which employs an alternating partition strategy within each chunk and identifies clusters based on similarity. Chelsea then merges the KV cache within each cluster into a single centroid. Additionally, we provide a theoretical analysis of the computational complexity and the optimality of the intra-chunk partitioning strategy. Extensive experiments across various models and long-context benchmarks demonstrate that Chelsea achieves up to 80% reduction in KV cache memory usage while maintaining comparable model performance. Moreover, with minimal computational overhead, Chelsea accelerates the decoding stage of inference by up to 3.19$\times$ and reduces end-to-end latency by up to 2.72$\times$.

