---
layout: default
title: Instruction Tuning and CoT Prompting for Contextual Medical QA with LLMs
---

# Instruction Tuning and CoT Prompting for Contextual Medical QA with LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12182" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12182v1</a>
  <a href="https://arxiv.org/pdf/2506.12182.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12182v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12182v1', 'Instruction Tuning and CoT Prompting for Contextual Medical QA with LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenqian Le, Ziheng Gong, Chihang Wang, Haowei Ni, Panfeng Li, Xupeng Chen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: Accepted by 2025 International Conference on Artificial Intelligence, Human-Computer Interaction and Natural Language Processing

**æœŸåˆŠ**: Proceedings of the 2025 International Conference on Artificial Intelligence, Human-Computer Interaction and Natural Language Processing (ICAHN), 2025, pp. 43-46

**DOI**: [10.1109/ICAHN67688.2025.00016](https://doi.org/10.1109/ICAHN67688.2025.00016)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæŒ‡ä»¤è°ƒä¼˜ä¸CoTæç¤ºä»¥æå‡åŒ»å­¦é—®ç­”æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»å­¦é—®ç­”` `å¤§å‹è¯­è¨€æ¨¡å‹` `æŒ‡ä»¤è°ƒä¼˜` `é“¾å¼æ€ç»´` `ç”Ÿç‰©åŒ»å­¦æ¨ç†` `è½»é‡çº§å¾®è°ƒ` `æç¤ºè®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦é—®ç­”é¢†åŸŸçš„é€‚åº”æ€§å—åˆ°é¢†åŸŸå¤æ‚æ€§å’Œç›‘ç£ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. æœ¬ç ”ç©¶æå‡ºé€šè¿‡æ ‡å‡†æŒ‡ä»¤æç¤ºå’Œé“¾å¼æ€ç»´æç¤ºç»“åˆQLoRAè¿›è¡Œé«˜æ•ˆçš„æŒ‡ä»¤è°ƒä¼˜ï¼Œä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoTæç¤ºåœ¨é›¶-shotæ¡ä»¶ä¸‹æå‡æ¨ç†èƒ½åŠ›ï¼Œè€ŒæŒ‡ä»¤è°ƒä¼˜åˆ™æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†å¯¹æŸäº›æ¨¡å‹å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦é—®ç­”ï¼ˆMedQAï¼‰ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºé¢†åŸŸç‰¹å®šçš„å¤æ‚æ€§å’Œæœ‰é™çš„ç›‘ç£ï¼Œé€‚åº”è¿™äº›æ¨¡å‹è¿›è¡Œç”Ÿç‰©åŒ»å­¦æ¨ç†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æç¤ºè®¾è®¡å’Œè½»é‡çº§å¾®è°ƒå¯¹å¼€æºLLMsåœ¨PubMedQAåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å½±å“ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨ä¸¤ç§å¹¿æ³›ä½¿ç”¨çš„æç¤ºç­–ç•¥â€”â€”æ ‡å‡†æŒ‡ä»¤æç¤ºå’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºï¼Œå¹¶åº”ç”¨QLoRAè¿›è¡Œå‚æ•°é«˜æ•ˆçš„æŒ‡ä»¤è°ƒä¼˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoTæç¤ºåœ¨é›¶-shotè®¾ç½®ä¸­èƒ½å¤Ÿæ”¹å–„æ¨ç†ï¼Œè€ŒæŒ‡ä»¤è°ƒä¼˜æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œé’ˆå¯¹CoTæç¤ºçš„å¾®è°ƒå¹¶ä¸æ™®éæå‡æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½å¯¹æŸäº›è¾ƒå¤§æ¨¡å‹é€ æˆæ€§èƒ½ä¸‹é™ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ¨ç†æ„ŸçŸ¥æç¤ºæ˜¯æœ‰ç”¨çš„ï¼Œä½†å…¶æ•ˆæœä¾èµ–äºæ¨¡å‹å’Œè§„æ¨¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºåŒ»å­¦é—®ç­”åº”ç”¨ä¸­ç»“åˆæç¤ºå·¥ç¨‹ä¸é«˜æ•ˆå¾®è°ƒæä¾›äº†å®ç”¨è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦é—®ç­”ä¸­é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿç‰©åŒ»å­¦æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´é¢†åŸŸç‰¹å®šå¤æ‚æ€§å’Œç›‘ç£æ•°æ®æœ‰é™çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è®¾è®¡æœ‰æ•ˆçš„æç¤ºç­–ç•¥ï¼ˆæ ‡å‡†æŒ‡ä»¤æç¤ºå’Œé“¾å¼æ€ç»´æç¤ºï¼‰ç»“åˆè½»é‡çº§å¾®è°ƒï¼ˆQLoRAï¼‰ï¼Œä»¥æé«˜æ¨¡å‹åœ¨åŒ»å­¦é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å……åˆ†åˆ©ç”¨æ¨¡å‹çš„æ½œåŠ›ï¼ŒåŒæ—¶é™ä½å¾®è°ƒçš„è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæç¤ºè®¾è®¡å’Œå¾®è°ƒç­–ç•¥ã€‚é¦–å…ˆï¼Œé€šè¿‡ä¸åŒçš„æç¤ºç­–ç•¥å¼•å¯¼æ¨¡å‹è¿›è¡Œæ¨ç†ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨QLoRAè¿›è¡Œé«˜æ•ˆçš„æŒ‡ä»¤è°ƒä¼˜ï¼Œä»¥æå‡æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆäº†ä¸¤ç§æç¤ºç­–ç•¥ä¸é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œå‘ç°CoTæç¤ºåœ¨é›¶-shotæ¡ä»¶ä¸‹æœ‰æ•ˆæå‡æ¨ç†èƒ½åŠ›ï¼Œè€ŒæŒ‡ä»¤è°ƒä¼˜åˆ™åœ¨å‡†ç¡®æ€§ä¸Šè¡¨ç°çªå‡ºã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€æç¤ºæˆ–å¾®è°ƒç­–ç•¥å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨QLoRAè¿›è¡Œè½»é‡çº§å¾®è°ƒï¼Œç¡®ä¿åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶å‡å°‘è®¡ç®—èµ„æºæ¶ˆè€—ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥é€‚åº”åŒ»å­¦é—®ç­”çš„ç‰¹å®šéœ€æ±‚ã€‚å…·ä½“ç»†èŠ‚åŒ…æ‹¬å¯¹ä¸åŒæ¨¡å‹è§„æ¨¡çš„é€‚åº”æ€§è°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨CoTæç¤ºåœ¨é›¶-shotè®¾ç½®ä¸‹æ¨ç†èƒ½åŠ›æå‡æ˜¾è‘—ï¼Œè€ŒæŒ‡ä»¤è°ƒä¼˜åˆ™åœ¨å‡†ç¡®æ€§ä¸Šæé«˜äº†XX%ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æŸäº›è¾ƒå¤§æ¨¡å‹çš„CoTæç¤ºå¾®è°ƒå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹å’Œè§„æ¨¡ä¾èµ–æ€§ã€‚æ•´ä½“ä¸Šï¼Œç ”ç©¶ä¸ºåŒ»å­¦é—®ç­”æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦é—®ç­”ç³»ç»Ÿã€ä¸´åºŠå†³ç­–æ”¯æŒå·¥å…·å’Œç”Ÿç‰©åŒ»å­¦ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºåŒ»ç”Ÿå’Œæ‚£è€…æä¾›æ›´å‡†ç¡®çš„ä¿¡æ¯ï¼Œè¿›è€Œæ”¹å–„åŒ»ç–—æœåŠ¡è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶çš„æˆæœå¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½åŒ»ç–—åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have shown great potential in medical question answering (MedQA), yet adapting them to biomedical reasoning remains challenging due to domain-specific complexity and limited supervision. In this work, we study how prompt design and lightweight fine-tuning affect the performance of open-source LLMs on PubMedQA, a benchmark for multiple-choice biomedical questions. We focus on two widely used prompting strategies - standard instruction prompts and Chain-of-Thought (CoT) prompts - and apply QLoRA for parameter-efficient instruction tuning. Across multiple model families and sizes, our experiments show that CoT prompting alone can improve reasoning in zero-shot settings, while instruction tuning significantly boosts accuracy. However, fine-tuning on CoT prompts does not universally enhance performance and may even degrade it for certain larger models. These findings suggest that reasoning-aware prompts are useful, but their benefits are model- and scale-dependent. Our study offers practical insights into combining prompt engineering with efficient finetuning for medical QA applications.

