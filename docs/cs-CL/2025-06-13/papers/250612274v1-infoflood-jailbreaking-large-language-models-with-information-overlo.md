---
layout: default
title: InfoFlood: Jailbreaking Large Language Models with Information Overload
---

# InfoFlood: Jailbreaking Large Language Models with Information Overload

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12274" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12274v1</a>
  <a href="https://arxiv.org/pdf/2506.12274.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12274v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12274v1', 'InfoFlood: Jailbreaking Large Language Models with Information Overload')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Advait Yadav, Haibo Jin, Man Luo, Jun Zhuang, Haohan Wang

**åˆ†ç±»**: cs.CR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInfoFloodä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ¼æ´é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¿¡æ¯è¿‡è½½` `è¶Šç‹±æ”»å‡»` `å¤§å‹è¯­è¨€æ¨¡å‹` `å®‰å…¨æœºåˆ¶` `è¯­è¨€è½¬æ¢` `ç½‘ç»œå®‰å…¨` `äººå·¥æ™ºèƒ½ä¼¦ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¶Šç‹±æ–¹æ³•ä¾èµ–äºæ·»åŠ å‰ç¼€æˆ–åç¼€ï¼Œå­˜åœ¨ä¸€å®šçš„å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹ä¿¡æ¯è¿‡è½½å¸¦æ¥çš„æ–°å‹æ”»å‡»ã€‚
2. InfoFloodé€šè¿‡è¯­è¨€è½¬æ¢å°†æ¶æ„æŸ¥è¯¢è½¬åŒ–ä¸ºå¤æ‚æŸ¥è¯¢ï¼Œç›´æ¥åˆ©ç”¨è¯­è¨€å¤æ‚æ€§æ¥ç»•è¿‡å®‰å…¨æœºåˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒInfoFloodåœ¨å››ç§ä¸»æµLLMsä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸç‡æ¯”ä¼ ç»Ÿæ–¹æ³•é«˜å‡ºä¸‰å€ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶ç”Ÿæˆæœ‰å®³å“åº”çš„æ½œåŠ›å¼•å‘äº†ç¤¾ä¼šå’Œç›‘ç®¡æ–¹é¢çš„é‡å¤§æ‹…å¿§ã€‚ç°æœ‰çš„è¶Šç‹±æ–¹æ³•é€šå¸¸é€šè¿‡åœ¨æ¶æ„æç¤ºä¸­é™„åŠ ç²¾å¿ƒè®¾è®¡çš„å‰ç¼€æˆ–åç¼€æ¥ç»•è¿‡æ¨¡å‹çš„å†…ç½®å®‰å…¨æœºåˆ¶ã€‚æœ¬æ–‡è¯†åˆ«å‡ºä¸€ç§æ–°æ¼æ´ï¼Œå³è¿‡åº¦çš„è¯­è¨€å¤æ‚æ€§å¯ä»¥åœ¨ä¸éœ€è¦ä»»ä½•é™„åŠ å‰ç¼€æˆ–åç¼€çš„æƒ…å†µä¸‹ç ´åå†…ç½®å®‰å…¨æœºåˆ¶ï¼Œä»è€Œå…è®¸æ”»å‡»è€…ç›´æ¥å¼•å‘æœ‰å®³è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºäº†InfoFloodï¼Œä¸€ç§è¶Šç‹±æ”»å‡»ï¼Œèƒ½å¤Ÿå°†æ¶æ„æŸ¥è¯¢è½¬åŒ–ä¸ºå¤æ‚çš„ä¿¡æ¯è¿‡è½½æŸ¥è¯¢ï¼Œä»è€Œç»•è¿‡å†…ç½®å®‰å…¨æœºåˆ¶ã€‚é€šè¿‡å¯¹å››ç§å¹¿æ³›ä½¿ç”¨çš„LLMsè¿›è¡Œå®è¯éªŒè¯ï¼ŒInfoFloodçš„æˆåŠŸç‡æ˜¾è‘—é«˜äºåŸºçº¿æ”»å‡»ï¼Œæœ€é«˜å¯è¾¾ä¸‰å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ä¿¡æ¯è¿‡è½½æ—¶çš„å®‰å…¨æ¼æ´ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåº”å¯¹è¿™ç§æ–°å‹æ”»å‡»ï¼Œå¯¼è‡´æ¨¡å‹æ˜“å—æ“æ§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šInfoFloodçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¯­è¨€å¤æ‚æ€§è€Œéä¼ ç»Ÿçš„å‰ç¼€æˆ–åç¼€ï¼Œç›´æ¥ç”Ÿæˆå¤æ‚çš„æ¶æ„æŸ¥è¯¢ï¼Œä»è€Œç»•è¿‡æ¨¡å‹çš„å®‰å…¨æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šInfoFloodçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) è¯­è¨€è½¬æ¢æ¨¡å—ï¼Œç”¨äºé‡è¿°æ¶æ„æŸ¥è¯¢ï¼›2) å¤±è´¥åŸå› è¯†åˆ«æ¨¡å—ï¼Œåˆ†ææœªæˆåŠŸçš„å°è¯•ï¼›3) ç»“æ„ä¼˜åŒ–æ¨¡å—ï¼Œè°ƒæ•´æŸ¥è¯¢ç»“æ„ä»¥ä¿æŒæ¶æ„æ„å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šInfoFloodçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶ä¸ä¾èµ–äºé™„åŠ çš„å‰ç¼€æˆ–åç¼€ï¼Œè€Œæ˜¯é€šè¿‡ä¿¡æ¯è¿‡è½½ç›´æ¥å½±å“æ¨¡å‹çš„è¾“å‡ºï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»Ÿçš„è¶Šç‹±æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒInfoFloodé‡‡ç”¨äº†å¤šç§è¯­è¨€è½¬æ¢æŠ€æœ¯ï¼Œç¡®ä¿ç”Ÿæˆçš„æŸ¥è¯¢åœ¨å¤æ‚æ€§ä¸Šè¶…å‡ºæ¨¡å‹çš„å¤„ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶æ¶æ„æ„å›¾ä¸å˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInfoFloodåœ¨å››ç§ä¸»æµå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„è¶Šç‹±æˆåŠŸç‡ï¼Œæœ€é«˜å¯è¾¾ä¸‰å€äºåŸºçº¿æ”»å‡»ï¼Œä¸”å¸¸ç”¨çš„åå¤„ç†é˜²å¾¡æªæ–½å¦‚OpenAIçš„Moderation APIæœªèƒ½æœ‰æ•ˆæŠµå¾¡æ­¤ç±»æ”»å‡»ï¼Œæ­ç¤ºäº†ä¼ ç»ŸAIå®‰å…¨é˜²æŠ¤çš„é‡å¤§ç¼ºé™·ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç½‘ç»œå®‰å…¨ã€å†…å®¹å®¡æŸ¥å’Œäººå·¥æ™ºèƒ½ä¼¦ç†ç­‰ã€‚é€šè¿‡è¯†åˆ«å’Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¼æ´ï¼Œç ”ç©¶è€…å’Œå¼€å‘è€…å¯ä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å±€é™æ€§ï¼Œä»è€Œè®¾è®¡å‡ºæ›´æœ‰æ•ˆçš„å®‰å…¨é˜²æŠ¤æªæ–½ï¼Œæå‡AIç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. However, their potential to generate harmful responses has raised significant societal and regulatory concerns, especially when manipulated by adversarial techniques known as "jailbreak" attacks. Existing jailbreak methods typically involve appending carefully crafted prefixes or suffixes to malicious prompts in order to bypass the built-in safety mechanisms of these models.
>   In this work, we identify a new vulnerability in which excessive linguistic complexity can disrupt built-in safety mechanisms-without the need for any added prefixes or suffixes-allowing attackers to elicit harmful outputs directly. We refer to this phenomenon as Information Overload.
>   To automatically exploit this vulnerability, we propose InfoFlood, a jailbreak attack that transforms malicious queries into complex, information-overloaded queries capable of bypassing built-in safety mechanisms. Specifically, InfoFlood: (1) uses linguistic transformations to rephrase malicious queries, (2) identifies the root cause of failure when an attempt is unsuccessful, and (3) refines the prompt's linguistic structure to address the failure while preserving its malicious intent.
>   We empirically validate the effectiveness of InfoFlood on four widely used LLMs-GPT-4o, GPT-3.5-turbo, Gemini 2.0, and LLaMA 3.1-by measuring their jailbreak success rates. InfoFlood consistently outperforms baseline attacks, achieving up to 3 times higher success rates across multiple jailbreak benchmarks. Furthermore, we demonstrate that commonly adopted post-processing defenses, including OpenAI's Moderation API, Perspective API, and SmoothLLM, fail to mitigate these attacks. This highlights a critical weakness in traditional AI safety guardrails when confronted with information overload-based jailbreaks.

