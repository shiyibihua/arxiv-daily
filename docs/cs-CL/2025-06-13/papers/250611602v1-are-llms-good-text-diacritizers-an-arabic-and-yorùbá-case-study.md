---
layout: default
title: Are LLMs Good Text Diacritizers? An Arabic and YorÃ¹bÃ¡ Case Study
---

# Are LLMs Good Text Diacritizers? An Arabic and YorÃ¹bÃ¡ Case Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11602" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11602v1</a>
  <a href="https://arxiv.org/pdf/2506.11602.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11602v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11602v1', 'Are LLMs Good Text Diacritizers? An Arabic and YorÃ¹bÃ¡ Case Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hawau Olamide Toyin, Samar M. Magdy, Hanan Aldarmaki

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMultiDiacæ•°æ®é›†ä»¥æå‡é˜¿æ‹‰ä¼¯è¯­å’Œçº¦é²å·´è¯­çš„æ–‡æœ¬åŠ éŸ³æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬åŠ éŸ³` `é˜¿æ‹‰ä¼¯è¯­` `çº¦é²å·´è¯­` `å¤šè¯­è¨€æ•°æ®é›†` `LoRAå¾®è°ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡æœ¬åŠ éŸ³æ–¹æ³•åœ¨å¤„ç†é˜¿æ‹‰ä¼¯è¯­å’Œçº¦é²å·´è¯­æ—¶å­˜åœ¨å‡†ç¡®æ€§ä¸è¶³å’Œæ­§ä¹‰å¤„ç†ä¸ä½³çš„é—®é¢˜ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥MultiDiacæ•°æ®é›†å’Œè¯„ä¼°å¤šç§LLMsï¼Œæ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åŠ éŸ³ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè®¸å¤šç°æˆçš„LLMsåœ¨åŠ éŸ³æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¸“é—¨æ¨¡å‹ï¼Œä¸”å¾®è°ƒå¯æ˜¾è‘—æ”¹å–„å°æ¨¡å‹çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é˜¿æ‹‰ä¼¯è¯­å’Œçº¦é²å·´è¯­æ–‡æœ¬åŠ éŸ³ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºè¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„å¤šè¯­è¨€æ•°æ®é›†MultiDiacï¼Œæ¶µç›–äº†å¤šç§åŠ éŸ³æ­§ä¹‰æ ·æœ¬ã€‚æˆ‘ä»¬è¯„ä¼°äº†14ä¸ªä¸åŒè§„æ¨¡ã€å¯è®¿é—®æ€§å’Œè¯­è¨€è¦†ç›–èŒƒå›´çš„LLMsï¼Œå¹¶å°†å…¶ä¸6ä¸ªä¸“é—¨çš„åŠ éŸ³æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨LoRAå¯¹å››ä¸ªå°å‹å¼€æºæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œè®¸å¤šç°æˆçš„LLMsåœ¨é˜¿æ‹‰ä¼¯è¯­å’Œçº¦é²å·´è¯­çš„è¡¨ç°ä¼˜äºä¸“é—¨çš„åŠ éŸ³æ¨¡å‹ï¼Œä½†è¾ƒå°çš„æ¨¡å‹å­˜åœ¨å¹»è§‰ç°è±¡ã€‚å¯¹å°æ•°æ®é›†çš„å¾®è°ƒæœ‰åŠ©äºæå‡åŠ éŸ³æ€§èƒ½å¹¶é™ä½å¹»è§‰ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³é˜¿æ‹‰ä¼¯è¯­å’Œçº¦é²å·´è¯­æ–‡æœ¬åŠ éŸ³çš„å‡†ç¡®æ€§å’Œæ­§ä¹‰å¤„ç†ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„ä¸“é—¨åŠ éŸ³æ¨¡å‹åœ¨è¿™ä¸¤ç§è¯­è¨€ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„åŠ éŸ³æ­§ä¹‰æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ–‡æœ¬åŠ éŸ³ï¼Œå¹¶é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€æ•°æ®é›†MultiDiacæ¥è¯„ä¼°å…¶æ•ˆæœã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨åˆ©ç”¨LLMsçš„å¼ºå¤§è¯­è¨€ç†è§£èƒ½åŠ›æ¥æ”¹å–„åŠ éŸ³çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹é€‰æ‹©ã€åŸºå‡†æµ‹è¯•å’Œå¾®è°ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ„å»ºMultiDiacæ•°æ®é›†ï¼Œç„¶åé€‰æ‹©14ä¸ªä¸åŒçš„LLMsè¿›è¡Œè¯„ä¼°ï¼Œæœ€åä¸6ä¸ªä¸“é—¨æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†MultiDiacæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸“é—¨è®¾è®¡ç”¨äºæ•æ‰åŠ éŸ³æ­§ä¹‰ï¼Œæä¾›äº†ä¸°å¯Œçš„æ ·æœ¬ç”¨äºè¯„ä¼°LLMsçš„æ€§èƒ½ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºæ•°æ®é›†çš„å¤šæ ·æ€§å’Œé’ˆå¯¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨LoRAå¯¹å››ä¸ªå°å‹å¼€æºæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œè®¾ç½®äº†ç‰¹å®šçš„è¶…å‚æ•°ä»¥ä¼˜åŒ–åŠ éŸ³æ€§èƒ½ã€‚æŸå¤±å‡½æ•°çš„é€‰æ‹©å’Œæ¨¡å‹æ¶æ„çš„è°ƒæ•´ä¹Ÿæ˜¯å…³é”®è®¾è®¡å› ç´ ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å°æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè®¸å¤šç°æˆçš„LLMsåœ¨é˜¿æ‹‰ä¼¯è¯­å’Œçº¦é²å·´è¯­çš„åŠ éŸ³ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¸“é—¨çš„åŠ éŸ³æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„åŠ éŸ³æ­§ä¹‰æ—¶ã€‚å¾®è°ƒåçš„æ¨¡å‹åœ¨å°æ•°æ®é›†ä¸Šçš„åŠ éŸ³å‡†ç¡®ç‡æå‡æ˜¾è‘—ï¼Œå¹»è§‰ç‡ä¹Ÿæœ‰æ‰€é™ä½ï¼Œè¡¨æ˜å¾®è°ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æå‡é˜¿æ‹‰ä¼¯è¯­å’Œçº¦é²å·´è¯­çš„æ–‡æœ¬åŠ éŸ³æ•ˆæœï¼Œå¯ä»¥æ”¹å–„è¯­è¨€å­¦ä¹ å·¥å…·çš„å‡†ç¡®æ€§ï¼Œä¿ƒè¿›è¿™ä¸¤ç§è¯­è¨€çš„æ•°å­—åŒ–å’Œä¿¡æ¯è·å–ã€‚æ­¤å¤–ï¼Œç ”ç©¶æˆæœä¹Ÿå¯ä¸ºå…¶ä»–è¯­è¨€çš„æ–‡æœ¬å¤„ç†æä¾›å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate the effectiveness of large language models (LLMs) for text diacritization in two typologically distinct languages: Arabic and Yoruba. To enable a rigorous evaluation, we introduce a novel multilingual dataset MultiDiac, with diverse samples that capture a range of diacritic ambiguities. We evaluate 14 LLMs varying in size, accessibility, and language coverage, and benchmark them against 6 specialized diacritization models. Additionally, we fine-tune four small open-source models using LoRA for Yoruba. Our results show that many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yoruba, but smaller models suffer from hallucinations. Fine-tuning on a small dataset can help improve diacritization performance and reduce hallucination rates.

