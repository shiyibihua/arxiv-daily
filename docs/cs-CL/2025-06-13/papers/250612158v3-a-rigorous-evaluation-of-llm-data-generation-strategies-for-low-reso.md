---
layout: default
title: A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages
---

# A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12158" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12158v3</a>
  <a href="https://arxiv.org/pdf/2506.12158.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12158v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12158v3', 'A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tatiana Anikina, Jan Cegin, Jakub Simko, Simon Ostermann

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-09-19)

**å¤‡æ³¨**: Accepted to EMNLP 2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿè¯„ä¼°ä½èµ„æºè¯­è¨€çš„LLMæ•°æ®ç”Ÿæˆç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½èµ„æºè¯­è¨€` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®ç”Ÿæˆ` `è‡ªç„¶è¯­è¨€å¤„ç†` `åˆæˆæ•°æ®` `æ¨¡å‹è¯„ä¼°` `æ™ºèƒ½æç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç”Ÿæˆç­–ç•¥åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å°šä¸æ˜ç¡®ï¼Œç¼ºä¹ç³»ç»Ÿæ¯”è¾ƒã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ç³»ç»Ÿè¯„ä¼°å¤šç§ç”Ÿæˆç­–ç•¥åŠå…¶ç»„åˆï¼Œç‰¹åˆ«å…³æ³¨ç›®æ ‡è¯­è¨€æ¼”ç¤ºä¸LLMä¿®è®¢çš„ç»“åˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ˜ç•¥ç»„åˆèƒ½æ˜¾è‘—æé«˜ç”Ÿæˆæ•°æ®çš„è´¨é‡ï¼Œç¼©å°ä¸çœŸå®æ•°æ®çš„æ€§èƒ½å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°ç”¨äºç”Ÿæˆåˆæˆæ–‡æœ¬æ•°æ®ï¼Œä»¥è®­ç»ƒæ›´å°çš„ä¸“ä¸šæ¨¡å‹ã€‚ç„¶è€Œï¼Œé’ˆå¯¹ä½èµ„æºè¯­è¨€ç¯å¢ƒçš„å„ç§ç”Ÿæˆç­–ç•¥çš„æ¯”è¾ƒä»ç„¶ç¼ºä¹ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†11ç§ç±»å‹å¤šæ ·è¯­è¨€ä¸­ä¸åŒç”Ÿæˆç­–ç•¥åŠå…¶ç»„åˆçš„æ€§èƒ½ï¼Œä½¿ç”¨ä¸‰é¡¹è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å’Œå››ä¸ªå¼€æºLLMï¼Œè¯„ä¼°ç”Ÿæˆæ•°æ®ä¸çœŸå®æ•°æ®çš„ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯ç›®æ ‡è¯­è¨€æ¼”ç¤ºä¸LLMä¿®è®¢çš„æˆ˜ç•¥ç»„åˆï¼Œèƒ½æ˜¾è‘—ç¼©å°ä¸çœŸå®æ•°æ®çš„å·®è·ï¼ŒæŸäº›æƒ…å†µä¸‹ä»…ä¸º5%ã€‚åŒæ—¶ï¼Œæ™ºèƒ½æç¤ºæŠ€æœ¯å¯ä»¥å‡å°‘å¤§å‹LLMçš„ä¼˜åŠ¿ï¼Œçªæ˜¾äº†åœ¨ä½èµ„æºåœºæ™¯ä¸­ä½¿ç”¨å°å‹æ¨¡å‹çš„é«˜æ•ˆç”Ÿæˆç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä½èµ„æºè¯­è¨€ç”Ÿæˆç­–ç•¥çš„æœ‰æ•ˆæ€§ç¼ºä¹æ¯”è¾ƒçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸åŒè¯­è¨€ç¯å¢ƒä¸­çš„è¡¨ç°ä¸ä¸€ï¼Œéš¾ä»¥é€‰æ‹©æœ€ä½³ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿè¯„ä¼°å¤šç§ç”Ÿæˆç­–ç•¥åŠå…¶ç»„åˆï¼Œç‰¹åˆ«æ˜¯ç›®æ ‡è¯­è¨€çš„æ¼”ç¤ºä¸LLMçš„è‡ªæˆ‘ä¿®è®¢ï¼Œæ¥æå‡ä½èµ„æºè¯­è¨€çš„åˆæˆæ•°æ®è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å››ä¸ªå¼€æºLLMï¼Œç»“åˆä¸‰é¡¹NLPä»»åŠ¡ï¼Œè¯„ä¼°ç”Ÿæˆæ•°æ®ä¸çœŸå®æ•°æ®åœ¨ä¸‹æ¸¸æ¨¡å‹ä¸­çš„è¡¨ç°ï¼Œæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®ç”Ÿæˆã€æ¨¡å‹è®­ç»ƒä¸æ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ç›®æ ‡è¯­è¨€æ¼”ç¤ºä¸LLMä¿®è®¢çš„ç»„åˆç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†ä½èµ„æºè¯­è¨€çš„ç”Ÿæˆæ•ˆæœï¼ŒåŒºåˆ«äºä¼ ç»Ÿå•ä¸€ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æç¤ºç­–ç•¥å’Œç»„åˆï¼Œå…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªè¯¦ç»†æŠ«éœ²ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç›®æ ‡è¯­è¨€æ¼”ç¤ºä¸LLMä¿®è®¢çš„ç»„åˆç­–ç•¥åœ¨æŸäº›æƒ…å†µä¸‹å°†ç”Ÿæˆæ•°æ®çš„æ€§èƒ½ä¸çœŸå®æ•°æ®çš„å·®è·ç¼©å°è‡³5%ã€‚æ­¤å¤–ï¼Œæ™ºèƒ½æç¤ºæŠ€æœ¯çš„åº”ç”¨æœ‰æ•ˆå‡å°‘äº†å¤§å‹LLMçš„ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†åœ¨ä½èµ„æºåœºæ™¯ä¸­å°å‹æ¨¡å‹çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä½èµ„æºè¯­è¨€çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆå’Œä¿¡æ¯æå–ç­‰ã€‚é€šè¿‡æé«˜åˆæˆæ•°æ®çš„è´¨é‡ï¼Œå¯ä»¥å¸®åŠ©å°å‹æ¨¡å‹åœ¨ä½èµ„æºç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°å­¦ä¹ ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly used to generate synthetic textual data for training smaller specialized models. However, a comparison of various generation strategies for low-resource language settings is lacking. While various prompting strategies have been proposed, such as demonstrations, label-based summaries, and self-revision, their comparative effectiveness remains unclear, especially for low-resource languages. In this paper, we systematically evaluate the performance of these generation strategies and their combinations across 11 typologically diverse languages, including several extremely low-resource ones. Using three NLP tasks and four open-source LLMs, we assess downstream model performance on generated versus gold-standard data. Our results show that strategic combinations of generation methods, particularly target-language demonstrations with LLM-based revisions, yield strong performance, narrowing the gap with real data to as little as 5% in some settings. We also find that smart prompting techniques can reduce the advantage of larger LLMs, highlighting efficient generation strategies for synthetic data generation in low-resource scenarios with smaller models.

