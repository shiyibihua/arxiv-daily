---
layout: default
title: Lag-Relative Sparse Attention In Long Context Training
---

# Lag-Relative Sparse Attention In Long Context Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11498" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11498v1</a>
  <a href="https://arxiv.org/pdf/2506.11498.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11498v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11498v1', 'Lag-Relative Sparse Attention In Long Context Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Manlai Liang, Wanyi Huang, Mandi Liu, Huaijun Li, Jinlong Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLag-Relative Sparse Attentionä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡è®­ç»ƒä¸­çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡å¤„ç†` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨¡å‹å‹ç¼©` `è‡ªç„¶è¯­è¨€å¤„ç†` `é—®ç­”ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶é¢ä¸´è®¡ç®—å¤æ‚æ€§å’Œå†…å­˜å ç”¨çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºLag-Relative Sparse Attentionï¼ˆLRSAï¼‰ï¼Œé€šè¿‡é€å—é¢„å¡«å……é€‰æ‹©æœ€ç›¸å…³çš„é”®å€¼å¯¹ï¼Œä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLRSAåœ¨é”®å€¼å‹ç¼©ä¸‹æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œå¹¶åœ¨é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡è¾“å…¥æ—¶ä»å—åˆ°æ³¨æ„åŠ›è®¡ç®—çš„å¹³æ–¹å¤æ‚æ€§å’Œçº¿æ€§å¢åŠ çš„é”®å€¼å†…å­˜å ç”¨çš„é™åˆ¶ã€‚ä¸ºé™ä½è®¡ç®—æˆæœ¬å’Œå†…å­˜ï¼Œæ¨ç†æ—¶å¸¸ç”¨é”®å€¼ç¼“å­˜å‹ç¼©æŠ€æœ¯ï¼Œä½†è¿™å¾€å¾€å¯¼è‡´æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œå› ä¸ºæ¨¡å‹æœªç»è¿‡å‹ç¼©ä¸Šä¸‹æ–‡çš„è®­ç»ƒã€‚å°½ç®¡å­˜åœ¨æ›´å¤æ‚çš„å‹ç¼©æ–¹æ³•ï¼Œä½†ç”±äºä¸åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ä¸å…¼å®¹æˆ–è®¡ç®—å¼€é”€é«˜ï¼Œé€šå¸¸ä¸é€‚åˆåè®­ç»ƒã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†Lag-Relative Sparse Attentionï¼ˆLRSAï¼‰ï¼ŒåŸºäºLagKVå‹ç¼©æ–¹æ³•è¿›è¡Œé•¿ä¸Šä¸‹æ–‡åè®­ç»ƒã€‚è¯¥æ–¹æ³•é‡‡ç”¨é€å—é¢„å¡«å……ï¼Œé€‰æ‹©å›ºå®šå¤§å°æ»åçª—å£ä¸­çš„å‰Kä¸ªæœ€ç›¸å…³çš„é”®å€¼å¯¹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºæ˜¾è‘—çš„å†å²ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶ä¿æŒæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†LLMåœ¨é”®å€¼å‹ç¼©ä¸‹çš„é²æ£’æ€§ï¼Œå¹¶åœ¨é—®ç­”è°ƒä¼˜ä»»åŠ¡ä¸­å–å¾—äº†æ›´å¥½çš„å¾®è°ƒç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡è®­ç»ƒä¸­é¢ä¸´çš„è®¡ç®—å¤æ‚æ€§å’Œå†…å­˜å ç”¨é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨ç†æ—¶ä½¿ç”¨çš„é”®å€¼ç¼“å­˜å‹ç¼©æŠ€æœ¯ï¼Œè™½ç„¶å¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†å¾€å¾€å¯¼è‡´æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå› ä¸ºæ¨¡å‹æœªç»è¿‡å‹ç¼©ä¸Šä¸‹æ–‡çš„è®­ç»ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Lag-Relative Sparse Attentionï¼ˆLRSAï¼‰æ–¹æ³•ï¼Œé€šè¿‡é€å—é¢„å¡«å……çš„æ–¹å¼ï¼Œé€‰æ‹©å›ºå®šå¤§å°æ»åçª—å£ä¸­çš„å‰Kä¸ªæœ€ç›¸å…³çš„é”®å€¼å¯¹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºé‡è¦çš„å†å²ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLRSAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€é”®å€¼é€‰æ‹©å’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œå¯¹è¾“å…¥æ•°æ®è¿›è¡Œåˆ†å—å¤„ç†ï¼›ç„¶åï¼Œåœ¨æ¯ä¸ªå—ä¸­é€‰æ‹©æœ€ç›¸å…³çš„é”®å€¼å¯¹ï¼›æœ€åï¼Œåˆ©ç”¨è¿™äº›é€‰æ‹©çš„é”®å€¼å¯¹è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šLRSAçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†LagKVå‹ç¼©æ–¹æ³•ï¼Œå…è®¸æ¨¡å‹åœ¨ä¸å¢åŠ é¢å¤–å‚æ•°å’Œè®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡çš„å¤„ç†èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å‹ç¼©æŠ€æœ¯ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ¢¯åº¦ä¼˜åŒ–è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LRSAä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ»åçª—å£çš„å¤§å°å’Œé€‰æ‹©çš„Kå€¼ï¼Œè¿™äº›å‚æ•°çš„è®¾ç½®ç›´æ¥å½±å“æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿè€ƒè™‘äº†å‹ç¼©ä¸Šä¸‹æ–‡çš„å½±å“ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLRSAåœ¨ä½¿ç”¨é”®å€¼å‹ç¼©çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œå¹¶åœ¨é—®ç­”è°ƒä¼˜ä»»åŠ¡ä¸­å–å¾—äº†æ›´å¥½çš„å¾®è°ƒç»“æœï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„æ•ˆç‡å’Œé²æ£’æ€§ï¼ŒLRSAå¯ä»¥ä¸ºå®é™…åº”ç”¨æä¾›æ›´é«˜è´¨é‡çš„æ–‡æœ¬ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨é—®ç­”ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have made significant strides in natural language processing and generation, yet their ability to handle long-context input remains constrained by the quadratic complexity of attention computation and linear-increasing key-value memory footprint. To reduce computational costs and memory, key-value cache compression techniques are commonly applied at inference time, but this often leads to severe performance degradation, as models are not trained to handle compressed context. Although there are more sophisticated compression methods, they are typically unsuitable for post-training because of their incompatibility with gradient-based optimization or high computation overhead. To fill this gap with no additional parameter and little computation overhead, we propose Lag-Relative Sparse Attention(LRSA) anchored by the LagKV compression method for long context post-training. Our method performs chunk-by-chunk prefilling, which selects the top K most relevant key-value pairs in a fixed-size lagging window, allowing the model to focus on salient historical context while maintaining efficiency. Experimental results show that our approach significantly enhances the robustness of the LLM with key-value compression and achieves better fine-tuned results in the question-answer tuning task.

