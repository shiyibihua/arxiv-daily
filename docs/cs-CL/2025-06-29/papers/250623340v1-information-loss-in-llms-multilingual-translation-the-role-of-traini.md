---
layout: default
title: Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family
---

# Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23340" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23340v1</a>
  <a href="https://arxiv.org/pdf/2506.23340.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23340v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23340v1', 'Information Loss in LLMs\' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yumeng Lin, Xufeng Duan, David Haslett, Yige Chen, Zhenguang G. Cai

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è®­ç»ƒæ•°æ®ä¸è¯­è¨€ç‰¹æ€§å¯¹å¤šè¯­è¨€ç¿»è¯‘ä¿¡æ¯æŸå¤±çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€ç¿»è¯‘` `ä¿¡æ¯æŸå¤±` `è®­ç»ƒæ•°æ®` `è¯­è¨€æ¥è¿‘æ€§` `è¯­è¨€å®¶æ—` `æ¨¡å‹è¯„ä¼°` `BLEUåˆ†æ•°` `BERTç›¸ä¼¼åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æŸäº›è¯­è¨€å¯¹æ—¶ï¼Œå°¤å…¶æ˜¯ä½èµ„æºè¯­è¨€æ—¶ï¼Œç¿»è¯‘è´¨é‡ä»ç„¶ä¸è¶³ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿåˆ†æè®­ç»ƒæ•°æ®ã€è¯­è¨€æ¥è¿‘æ€§å’Œè¯­è¨€å®¶æ—å¯¹ç¿»è¯‘è´¨é‡çš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè®­ç»ƒæ•°æ®é‡ä¸è¯­è¨€è·ç¦»çš„äº’åŠ¨æ˜¾è‘—å½±å“ç¿»è¯‘è´¨é‡ï¼Œå°¤å…¶åœ¨ä½èµ„æºæ¡ä»¶ä¸‹ï¼Œç»“æ„æ¥è¿‘çš„è¯­è¨€è¡¨ç°æ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¿»è¯‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æŸäº›è¯­è¨€å¯¹ä¸Šä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯è®­ç»ƒæ•°æ®æœ‰é™æˆ–ä¸è‹±è¯­å­˜åœ¨æ˜¾è‘—è¯­è¨€å·®å¼‚çš„æƒ…å†µã€‚æœ¬ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†è®­ç»ƒæ•°æ®ã€è¯­è¨€æ¥è¿‘æ€§å’Œè¯­è¨€å®¶æ—å¦‚ä½•å½±å“å¤šè¯­è¨€ç¿»è¯‘ä¸­çš„ä¿¡æ¯æŸå¤±ã€‚é€šè¿‡å¯¹GPT-4å’ŒLlama 2è¿›è¡Œå¾€è¿”ç¿»è¯‘è¯„ä¼°ï¼Œä½¿ç”¨BLEUåˆ†æ•°å’ŒBERTç›¸ä¼¼åº¦æŒ‡æ ‡è¯„ä¼°ç¿»è¯‘è´¨é‡ã€‚ç»“æœè¡¨æ˜ï¼Œè®­ç»ƒæ•°æ®é‡ä¸è¯­è¨€è·ç¦»ä¹‹é—´å­˜åœ¨æ˜¾è‘—äº’åŠ¨ï¼šä¸°å¯Œçš„è®­ç»ƒæ•°æ®å¯ä»¥ç¼“è§£è¯­è¨€å·®å¼‚çš„å½±å“ï¼Œè€Œä¸è‹±è¯­ç»“æ„ä¸Šæ›´æ¥è¿‘çš„è¯­è¨€åœ¨ä½èµ„æºæ¡ä»¶ä¸‹ç¿»è¯‘è´¨é‡æ›´é«˜ã€‚å¤šç§è·ç¦»åº¦é‡ä¸­ï¼Œæ­£å­—æ³•ã€ç³»ç»Ÿå‘è‚²ã€å¥æ³•å’Œåœ°ç†è·ç¦»æ˜¯ç¿»è¯‘æ€§èƒ½çš„å¼ºé¢„æµ‹å› å­ã€‚è¯­è¨€å®¶æ—ä¹Ÿç‹¬ç«‹å½±å“ç¿»è¯‘è´¨é‡ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæ·±å…¥ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¤šè¯­è¨€ç¿»è¯‘çš„è¯­è¨€çº¦æŸï¼Œå¼ºè°ƒç¿»è¯‘è´¨é‡ä¸ä»…å—æ•°æ®é‡å½±å“ï¼Œè¿˜å—è¯­è¨€ä¹‹é—´çš„ç»“æ„å’Œç±»å‹å…³ç³»å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¿»è¯‘ä¸­é¢ä¸´çš„ä¿¡æ¯æŸå¤±é—®é¢˜ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹è®­ç»ƒæ•°æ®æœ‰é™æˆ–è¯­è¨€å·®å¼‚æ˜¾è‘—çš„è¯­è¨€å¯¹ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™äº›æƒ…å†µä¸‹çš„ç¿»è¯‘è´¨é‡æ™®éè¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿæ€§åœ°åˆ†æè®­ç»ƒæ•°æ®çš„è§„æ¨¡ã€è¯­è¨€ä¹‹é—´çš„æ¥è¿‘æ€§åŠè¯­è¨€å®¶æ—çš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ­ç¤ºè¿™äº›å› ç´ å¦‚ä½•å…±åŒå½±å“ç¿»è¯‘è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¾€è¿”ç¿»è¯‘çš„æ–¹æ³•ï¼Œè¯„ä¼°äº†GPT-4å’ŒLlama 2æ¨¡å‹çš„ç¿»è¯‘æ€§èƒ½ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€ç¿»è¯‘æ‰§è¡Œå’Œè´¨é‡è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæ­ç¤ºäº†è®­ç»ƒæ•°æ®é‡ä¸è¯­è¨€è·ç¦»ä¹‹é—´çš„äº’åŠ¨å…³ç³»ï¼Œå¼ºè°ƒäº†è¯­è¨€ç»“æ„å’Œç±»å‹å¯¹ç¿»è¯‘è´¨é‡çš„ç‹¬ç«‹å½±å“ï¼Œè¿™åœ¨ç°æœ‰æ–‡çŒ®ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†BLEUåˆ†æ•°å’ŒBERTç›¸ä¼¼åº¦ä½œä¸ºç¿»è¯‘è´¨é‡çš„è¯„ä¼°æŒ‡æ ‡ï¼Œé‡‡ç”¨äº†å¤šç§è·ç¦»åº¦é‡ï¼ˆå¦‚æ­£å­—æ³•ã€ç³»ç»Ÿå‘è‚²ã€å¥æ³•å’Œåœ°ç†è·ç¦»ï¼‰æ¥åˆ†æç¿»è¯‘æ€§èƒ½çš„å½±å“å› ç´ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè®­ç»ƒæ•°æ®çš„ä¸°å¯Œæ€§ä¸è¯­è¨€è·ç¦»çš„äº’åŠ¨æ˜¾è‘—å½±å“ç¿»è¯‘è´¨é‡ã€‚åœ¨ä½èµ„æºæ¡ä»¶ä¸‹ï¼Œä¸è‹±è¯­ç»“æ„æ¥è¿‘çš„è¯­è¨€çš„ç¿»è¯‘è´¨é‡æé«˜äº†20%ä»¥ä¸Šï¼Œä¸”æ­£å­—æ³•å’Œå¥æ³•è·ç¦»æ˜¯ç¿»è¯‘æ€§èƒ½çš„å¼ºé¢„æµ‹å› å­ã€‚è¿™ä¸€å‘ç°ä¸ºå¤šè¯­è¨€ç¿»è¯‘çš„ç ”ç©¶æä¾›äº†æ–°çš„ç†è®ºåŸºç¡€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„ç»“æœå¯å¹¿æ³›åº”ç”¨äºå¤šè¯­è¨€ç¿»è¯‘ç³»ç»Ÿçš„ä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„è¯­è¨€å¯¹ä¸­ã€‚é€šè¿‡ç†è§£è¯­è¨€ä¹‹é—´çš„ç»“æ„å…³ç³»ï¼Œå¼€å‘è€…å¯ä»¥æ›´æœ‰æ•ˆåœ°é€‰æ‹©è®­ç»ƒæ•°æ®å’Œæ¨¡å‹æ¶æ„ï¼Œä»è€Œæå‡ç¿»è¯‘è´¨é‡ã€‚æ­¤å¤–ï¼Œè¿™äº›å‘ç°ä¹Ÿä¸ºæœªæ¥çš„è¯­è¨€æ¨¡å‹ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ï¼Œå¯èƒ½å½±å“å¤šè¯­è¨€å¤„ç†çš„ç›¸å…³æŠ€æœ¯å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models have achieved impressive progress in multilingual translation, yet they continue to face challenges with certain language pairs-particularly those with limited training data or significant linguistic divergence from English. This study systematically investigates how training data, language proximity, and language family affect information loss in multilingual translation. We evaluate two large language models, GPT-4 and Llama 2, by performing round-trip translations. Translation quality was assessed using BLEU scores and BERT similarity metrics. Our results reveal a robust interaction between training data size and language distance: while abundant training data can mitigate the effects of linguistic divergence, languages structurally closer to English consistently yield higher translation quality in low-resource conditions. Among various distance metrics, orthographic, phylogenetic, syntactic, and geographical distances emerge as strong predictors of translation performance. Language family also exerts an independent influence. These findings contribute to a deeper understanding of the linguistic constraints shaping multilingual translation in large language models, emphasizing that translation quality is shaped not only by data volume but also by structural and typological relationships between languages.

