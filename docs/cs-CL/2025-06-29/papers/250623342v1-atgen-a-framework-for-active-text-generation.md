---
layout: default
title: ATGen: A Framework for Active Text Generation
---

# ATGen: A Framework for Active Text Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23342" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23342v1</a>
  <a href="https://arxiv.org/pdf/2506.23342.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23342v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23342v1', 'ATGen: A Framework for Active Text Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Akim Tsvigun, Daniil Vasilev, Ivan Tsvigun, Ivan Lysenko, Talgat Bektleuov, Aleksandr Medvedev, Uliana Vinogradova, Nikita Severin, Mikhail Mozikov, Andrey Savchenko, Rostislav Grigorev, Ramil Kuleev, Fedor Zhdanov, Artem Shelmanov, Ilya Makarov

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

**å¤‡æ³¨**: Accepted at ACL 2025 System Demonstrations

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºATGenæ¡†æ¶ä»¥è§£å†³è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­çš„ä¸»åŠ¨å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸»åŠ¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€ç”Ÿæˆ` `æ–‡æœ¬ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨æ ‡æ³¨` `æ ‡æ³¨æ•ˆç‡` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä¸»åŠ¨å­¦ä¹ çš„åº”ç”¨ä»ç„¶æœ‰é™ï¼Œå¯¼è‡´æ ‡æ³¨å·¥ä½œé‡å¤§ä¸”æ•ˆç‡ä½ä¸‹ã€‚
2. ATGenæ¡†æ¶é€šè¿‡ç»“åˆä¸»åŠ¨å­¦ä¹ ä¸æ–‡æœ¬ç”Ÿæˆï¼Œç®€åŒ–äº†NLGä»»åŠ¡ä¸­çš„æ ‡æ³¨è¿‡ç¨‹ï¼Œæ”¯æŒäººç±»å’Œè‡ªåŠ¨æ ‡æ³¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒATGenæ˜¾è‘—é™ä½äº†äººç±»æ ‡æ³¨è€…çš„å·¥ä½œé‡å’ŒAPIè°ƒç”¨æˆæœ¬ï¼Œæå‡äº†æ ‡æ³¨æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰åœ¨å‡å°‘æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ ‡æ³¨å·¥ä½œé‡æ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°½ç®¡è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰ä»»åŠ¡è¿‘å¹´æ¥å¤‡å—å…³æ³¨ï¼ŒALåœ¨NLGä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸»åŠ¨æ–‡æœ¬ç”Ÿæˆï¼ˆATGenï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å°†ALä¸æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ç»“åˆèµ·æ¥ï¼Œä½¿æœ€å…ˆè¿›çš„ALç­–ç•¥èƒ½å¤Ÿåº”ç”¨äºNLGã€‚è¯¥æ¡†æ¶ç®€åŒ–äº†NLGä»»åŠ¡ä¸­åŸºäºäººç±»æ ‡æ³¨è€…å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨æ ‡æ³¨ä»£ç†çš„ALèµ‹èƒ½æ ‡æ³¨ã€‚ATGenæ”¯æŒä½œä¸ºæœåŠ¡éƒ¨ç½²çš„LLMsï¼ˆå¦‚ChatGPTå’ŒClaudeï¼‰æˆ–æœ¬åœ°æ“ä½œçš„LLMsã€‚æ­¤å¤–ï¼ŒATGenæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„å¹³å°ï¼Œç”¨äºå¹³æ»‘å®æ–½å’ŒåŸºå‡†æµ‹è¯•é’ˆå¯¹NLGä»»åŠ¡çš„æ–°å‹ALç­–ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å¤šç§è®¾ç½®å’Œå¤šä¸ªæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæœ€å…ˆè¿›çš„ALç­–ç•¥çš„è¯„ä¼°ç»“æœï¼Œè¡¨æ˜ATGenå‡å°‘äº†äººç±»æ ‡æ³¨è€…çš„å·¥ä½œé‡å’Œä¸LLMåŸºäºæ ‡æ³¨ä»£ç†çš„APIè°ƒç”¨ç›¸å…³çš„æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­ä¸»åŠ¨å­¦ä¹ åº”ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ ‡æ³¨æ•ˆç‡å’Œæˆæœ¬ä¸Šå­˜åœ¨æ˜æ˜¾çŸ­æ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šATGenæ¡†æ¶é€šè¿‡å°†ä¸»åŠ¨å­¦ä¹ ä¸æ–‡æœ¬ç”Ÿæˆç»“åˆï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œé«˜æ•ˆçš„è‡ªåŠ¨æ ‡æ³¨ï¼Œä»è€Œå‡å°‘äººç±»æ ‡æ³¨è€…çš„è´Ÿæ‹…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šATGençš„æ•´ä½“æ¶æ„åŒ…æ‹¬äººç±»æ ‡æ³¨è€…ä¸LLMsè‡ªåŠ¨æ ‡æ³¨ä»£ç†çš„ååŒå·¥ä½œï¼Œæ”¯æŒå¤šç§LLMæœåŠ¡çš„æ¥å…¥ï¼Œæä¾›ç»Ÿä¸€çš„å®æ–½å’ŒåŸºå‡†æµ‹è¯•å¹³å°ã€‚

**å…³é”®åˆ›æ–°**ï¼šATGençš„ä¸»è¦åˆ›æ–°åœ¨äºå°†ä¸»åŠ¨å­¦ä¹ ç­–ç•¥æœ‰æ•ˆåœ°åº”ç”¨äºNLGä»»åŠ¡ï¼Œæ˜¾è‘—æå‡äº†æ ‡æ³¨æ•ˆç‡ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€æ ‡æ³¨æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¡†æ¶ä¸­è®¾è®¡äº†å¤šç§å‚æ•°è®¾ç½®ä»¥ä¼˜åŒ–æ ‡æ³¨è¿‡ç¨‹ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°æ¥å¹³è¡¡äººç±»ä¸è‡ªåŠ¨æ ‡æ³¨çš„è´¡çŒ®ï¼Œç¡®ä¿ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ä¸å¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒATGenåœ¨å¤šä¸ªæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¾ƒä¼ ç»Ÿæ–¹æ³•å‡å°‘äº†äººç±»æ ‡æ³¨è€…çš„å·¥ä½œé‡å’ŒAPIè°ƒç”¨æˆæœ¬ï¼Œæå‡äº†æ ‡æ³¨æ•ˆç‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ä¸å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ATGenæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œé€‚ç”¨äºéœ€è¦å¤§é‡æ–‡æœ¬ç”Ÿæˆå’Œæ ‡æ³¨çš„é¢†åŸŸï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€å†…å®¹åˆ›ä½œå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚å…¶é«˜æ•ˆçš„æ ‡æ³¨æœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—é™ä½äººåŠ›æˆæœ¬ï¼Œæé«˜æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Active learning (AL) has demonstrated remarkable potential in reducing the annotation effort required for training machine learning models. However, despite the surging popularity of natural language generation (NLG) tasks in recent years, the application of AL to NLG has been limited. In this paper, we introduce Active Text Generation (ATGen) - a comprehensive framework that bridges AL with text generation tasks, enabling the application of state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered annotation in NLG tasks using both human annotators and automatic annotation agents based on large language models (LLMs). The framework supports LLMs deployed as services, such as ChatGPT and Claude, or operated on-premises. Furthermore, ATGen provides a unified platform for smooth implementation and benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present evaluation results for state-of-the-art AL strategies across diverse settings and multiple text generation tasks. We show that ATGen reduces both the effort of human annotators and costs associated with API calls to LLM-based annotation agents. The code of the framework is available on GitHub under the MIT license. The video presentation is available at http://atgen-video.nlpresearch.group

