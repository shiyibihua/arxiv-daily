---
layout: default
title: Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions
---

# Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23146" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23146v3</a>
  <a href="https://arxiv.org/pdf/2506.23146.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23146v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23146v3', 'Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dingzriui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29 (æ›´æ–°: 2025-07-13)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå­¦ä¹ ä¸Šä¸‹æ–‡æ–œç‡ä»¥è§£å†³ICLè¯„ä¼°å¯é æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€§èƒ½è¯„ä¼°` `å­¦ä¹ å¢ç›Š` `ä¸Šä¸‹æ–‡ç›¸å…³æ€§` `åˆæˆè¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ICLè¯„ä¼°æ–¹æ³•ä¾èµ–äºæ€§èƒ½å˜åŒ–ï¼Œå­˜åœ¨å¯é æ€§ä½å’Œå½’å› ä¸æ¸…ç­‰é—®é¢˜ã€‚
2. æå‡ºå­¦ä¹ ä¸Šä¸‹æ–‡æ–œç‡ï¼ˆLCSï¼‰ä½œä¸ºæ–°åº¦é‡ï¼Œé€šè¿‡å»ºæ¨¡å­¦ä¹ å¢ç›Šä¸ä¸Šä¸‹æ–‡ç›¸å…³æ€§ä¹‹é—´çš„æ–œç‡æ¥é‡åŒ–ICLæœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºLCSä¸æ€§èƒ½æå‡é«˜åº¦ç›¸å…³ï¼Œå¹¶åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹æœ‰æ•ˆåæ˜ ICLçš„çœŸå®æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å·²æˆä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ€§èƒ½çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œå…¶æœ‰æ•ˆæ€§åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­å·®å¼‚æ˜¾è‘—ï¼Œç»™å®è·µè€…å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¾èµ–äºICLåº”ç”¨åçš„æ€§èƒ½å˜åŒ–ï¼Œå­˜åœ¨å¯é æ€§ä½ã€å½’å› å·®å’Œåœ¨æ•°æ®ä¸è¶³åœºæ™¯ä¸‹ä¸åˆ‡å®é™…ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åº¦é‡æ ‡å‡†â€”â€”å­¦ä¹ ä¸Šä¸‹æ–‡æ–œç‡ï¼ˆLCSï¼‰ï¼Œé€šè¿‡å»ºæ¨¡å­¦ä¹ å¢ç›Šä¸ä¸Šä¸‹æ–‡ç›¸å…³æ€§ä¹‹é—´çš„æ–œç‡æ¥é‡åŒ–ICLçš„æœ‰æ•ˆæ€§ã€‚LCSå…‹æœäº†åŸºäºæ€§èƒ½çš„åº¦é‡çš„å…³é”®å±€é™æ€§ï¼Œèƒ½å¤Ÿåœ¨è¾“å‡ºä¸æ­£ç¡®æ—¶æ•æ‰è¿ç»­çš„æŸå¤±å˜åŒ–ï¼Œæ”¹å–„äº†å¯é æ€§ï¼Œå¹¶é€šè¿‡åˆæˆè¯„ä¼°æœ€å°åŒ–å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLCSä¸æ ‡æ³¨è®¾ç½®ä¸­çš„æ€§èƒ½æå‡é«˜åº¦ç›¸å…³ï¼Œå¹¶åœ¨åå·®æˆ–æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸­å¯é åœ°åæ˜ çœŸå®æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ICLè¯„ä¼°æ–¹æ³•çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å…¶åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å·®å¼‚ï¼Œä»¥åŠå¯¹æ€§èƒ½å˜åŒ–çš„ä½å¯é æ€§å’Œå½’å› ä¸æ¸…çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå­¦ä¹ ä¸Šä¸‹æ–‡æ–œç‡ï¼ˆLCSï¼‰ï¼Œé€šè¿‡å»ºæ¨¡å­¦ä¹ å¢ç›Šä¸ä¸Šä¸‹æ–‡ç›¸å…³æ€§ä¹‹é—´çš„æ–œç‡ï¼Œæ¥é‡åŒ–ICLçš„æœ‰æ•ˆæ€§ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿåœ¨è¾“å‡ºä¸æ­£ç¡®çš„æƒ…å†µä¸‹ä»ç„¶æ•æ‰åˆ°æŸå¤±çš„è¿ç»­å˜åŒ–ï¼Œä»è€Œæé«˜è¯„ä¼°çš„å¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLCSçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œè®¡ç®—å­¦ä¹ å¢ç›Šï¼Œå³é€šè¿‡ç¤ºä¾‹å‡å°‘çš„æŸå¤±ï¼›å…¶æ¬¡ï¼Œè¯„ä¼°ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Œå³ç¤ºä¾‹ä¸è¾“å…¥ä¹‹é—´çš„ç›¸å…³æ€§ï¼›æœ€åï¼Œé€šè¿‡è¿™ä¸¤ä¸ªæŒ‡æ ‡è®¡ç®—LCSã€‚

**å…³é”®åˆ›æ–°**ï¼šLCSçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨è¾“å‡ºé”™è¯¯çš„æƒ…å†µä¸‹ä»ç„¶æœ‰æ•ˆæ•æ‰æŸå¤±å˜åŒ–ï¼Œå¹¶ä¸”èƒ½å¤Ÿå°†ICLå¤±è´¥å½’å› äºä¸Šä¸‹æ–‡å¯¹é½ä¸è¶³æˆ–è¾“å‡ºæ ¡å‡†è¿‡å¼ºï¼Œè¿™ä¸ä¼ ç»Ÿçš„åŸºäºæ€§èƒ½çš„è¯„ä¼°æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šLCSçš„è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å­¦ä¹ å¢ç›Šå’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„è®¡ç®—æ–¹å¼ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠåˆæˆè¯„ä¼°çš„ç­–ç•¥ï¼Œè¿™äº›è®¾è®¡ç¡®ä¿äº†LCSåœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLCSä¸æ ‡æ³¨è®¾ç½®ä¸­çš„æ€§èƒ½æå‡é«˜åº¦ç›¸å…³ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºæˆ–åå·®åœºæ™¯ä¸­ï¼ŒLCSèƒ½å¤Ÿå¯é åœ°åæ˜ ICLçš„çœŸå®æœ‰æ•ˆæ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒLCSåœ¨å¤šä¸ªä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç›¸å…³æ€§ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶ä½œä¸ºè¯„ä¼°å·¥å…·çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æä¾›æ›´å¯é çš„ICLè¯„ä¼°æ–¹æ³•ï¼Œç ”ç©¶è€…å’Œå¼€å‘è€…å¯ä»¥æ›´æœ‰æ•ˆåœ°ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œè¿›è€Œæå‡å®é™…åº”ç”¨ä¸­çš„ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ•ˆç‡ã€‚æœªæ¥ï¼ŒLCSå¯èƒ½ä¼šæˆä¸ºè¯„ä¼°ICLæœ‰æ•ˆæ€§çš„æ ‡å‡†å·¥å…·ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context learning (ICL) has emerged as an effective approach to enhance the performance of large language models (LLMs). However, its effectiveness varies significantly across models and tasks, posing challenges for practitioners to determine when ICL reliably improves performance. Current evaluation approaches, reliant on performance change after applying ICL, suffer from low reliability, poor attribution, and impracticality in data-insufficient scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that quantifies ICL effectiveness by modeling the slope between learning gain (loss decrease from demonstrations) and contextual relevance (demonstration-input relevance). LCS addresses key limitations of performance-based metrics: (1) it captures continuous loss changes even when outputs are incorrect, improving reliability; (2) its formulation attributes ICL failures to weak contextual alignment (inability to adapt inputs to demonstrations) or strong output calibration (self-verification of correctness); and (3) it minimizes reliance on labeled data via synthetic evaluation. Extensive experiments demonstrate that LCS strongly correlates with performance improvements in labeled settings and reliably reflects true effectiveness in biased or data-scarce scenarios. Further analysis reveals actionable thresholds for LCS and identifies model capabilities critical to ICL success.

