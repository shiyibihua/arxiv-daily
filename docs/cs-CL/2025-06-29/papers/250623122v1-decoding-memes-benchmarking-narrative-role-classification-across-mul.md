---
layout: default
title: Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models
---

# Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23122" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23122v1</a>
  <a href="https://arxiv.org/pdf/2506.23122.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23122v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23122v1', 'Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shivam Sharma, Tanmoy Chakraborty

**åˆ†ç±»**: cs.CL, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

**å¤‡æ³¨**: This work has been submitted to the IEEE for possible publication

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¯­è¨€å¤šæ¨¡æ€æ¨¡å‹ä»¥è§£å†³äº’è”ç½‘è¡¨æƒ…åŒ…å™äº‹è§’è‰²åˆ†ç±»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å™äº‹è§’è‰²åˆ†ç±»` `å¤šè¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `æç¤ºè®¾è®¡` `æ–‡åŒ–èƒŒæ™¯` `è¡¨æƒ…åŒ…åˆ†æ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ–¹æ³•åœ¨è¯†åˆ«äº’è”ç½‘è¡¨æƒ…åŒ…ä¸­çš„å™äº‹è§’è‰²æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡åŒ–å’Œè¯­è¨€æ··åˆå†…å®¹çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šè¯­è¨€å’Œå¤šæ¨¡æ€æ¨¡å‹çš„è§’è‰²åˆ†ç±»æ–¹æ³•ï¼Œåˆ©ç”¨æ›´å¹³è¡¡çš„æ ‡æ³¨æ•°æ®é›†å’Œæç¤ºè®¾è®¡ç­–ç•¥ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå®éªŒç»“æœæ˜¾ç¤ºï¼Œè¾ƒå¤§çš„æ¨¡å‹åœ¨è§’è‰²è¯†åˆ«ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œä½†åœ¨'å—å®³è€…'ç±»çš„è¯†åˆ«ä¸Šä»å­˜åœ¨å›°éš¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨äº’è”ç½‘è¡¨æƒ…åŒ…ä¸­è¯†åˆ«å™äº‹è§’è‰²ï¼ˆè‹±é›„ã€åæ´¾ã€å—å®³è€…åŠå…¶ä»–ï¼‰çš„æŒ‘æˆ˜ï¼Œæ¶µç›–è‹±è¯­åŠæ··åˆè¯­è¨€ï¼ˆè‹±è¯­-å°åœ°è¯­ï¼‰çš„ä¸‰ä¸ªæµ‹è¯•é›†ã€‚åŸºäºæœ€åˆåå‘'å…¶ä»–'ç±»çš„æ ‡æ³¨æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ›´å¹³è¡¡ä¸”è¯­è¨€å¤šæ ·çš„æ‰©å±•æ•°æ®é›†ã€‚é€šè¿‡å…¨é¢çš„è¯æ±‡å’Œç»“æ„åˆ†æï¼Œæ­ç¤ºäº†çœŸå®è¡¨æƒ…åŒ…ä¸­ç»†è…»ã€æ–‡åŒ–ç‰¹å®šä¸”ä¸°å¯Œçš„è¯­è¨€ä½¿ç”¨ï¼Œä¸åˆæˆçš„ä»‡æ¨å†…å®¹ç›¸æ¯”ï¼Œåè€…åˆ™è¡¨ç°å‡ºæ˜æ˜¾ä¸”é‡å¤çš„è¯æ±‡ç‰¹å¾ã€‚ä¸ºäº†åŸºå‡†åŒ–è§’è‰²æ£€æµ‹ä»»åŠ¡ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šç§æ¨¡å‹ï¼ŒåŒ…æ‹¬å¾®è°ƒçš„å¤šè¯­è¨€å˜æ¢å™¨ã€æƒ…æ„Ÿå’Œæ»¥ç”¨æ„è¯†åˆ†ç±»å™¨ã€æŒ‡ä»¤è°ƒä¼˜çš„LLMåŠå¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚å°½ç®¡è¾ƒå¤§çš„æ¨¡å‹å¦‚DeBERTa-v3å’ŒQwen2.5-VLè¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œä½†åœ¨å¯é è¯†åˆ«'å—å®³è€…'ç±»åŠè·¨æ–‡åŒ–å’Œæ··åˆå†…å®¹çš„æ³›åŒ–æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº†æç¤ºè®¾è®¡ç­–ç•¥ï¼Œå‘ç°ç»“åˆç»“æ„åŒ–æŒ‡ä»¤å’Œè§’è‰²å®šä¹‰çš„æ··åˆæç¤ºæä¾›äº†è¾¹é™…ä½†ä¸€è‡´çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†æ–‡åŒ–åŸºç¡€ã€æç¤ºå·¥ç¨‹å’Œå¤šæ¨¡æ€æ¨ç†åœ¨å»ºæ¨¡è§†è§‰-æ–‡æœ¬å†…å®¹ä¸­å¾®å¦™å™äº‹æ¡†æ¶çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åœ¨äº’è”ç½‘è¡¨æƒ…åŒ…ä¸­è¯†åˆ«å™äº‹è§’è‰²ï¼ˆå¦‚è‹±é›„ã€åæ´¾ã€å—å®³è€…ç­‰ï¼‰çš„å…·ä½“é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ–‡åŒ–å’Œè¯­è¨€æ··åˆå†…å®¹æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´è§’è‰²è¯†åˆ«çš„å‡†ç¡®æ€§ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªæ›´å¹³è¡¡ä¸”å¤šæ ·åŒ–çš„æ ‡æ³¨æ•°æ®é›†ï¼Œç»“åˆå¤šè¯­è¨€å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼Œæ¥æé«˜å™äº‹è§’è‰²çš„è¯†åˆ«èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥æç¤ºè®¾è®¡ç­–ç•¥ï¼ŒæŒ‡å¯¼æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œåˆ†ç±»è§’è‰²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹é€‰æ‹©ä¸è®­ç»ƒã€æç¤ºè®¾è®¡å’Œæ€§èƒ½è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ„å»ºå¤šè¯­è¨€å’Œå¤šæ¨¡æ€çš„æ ‡æ³¨æ•°æ®é›†ï¼›å…¶æ¬¡ï¼Œé€‰æ‹©å¤šç§æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬å¾®è°ƒçš„å˜æ¢å™¨å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼›ç„¶åï¼Œè®¾è®¡é€‚å½“çš„æç¤ºä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼›æœ€åï¼Œé€šè¿‡ç²¾åº¦ã€å¬å›ç‡å’ŒF1æŒ‡æ ‡è¯„ä¼°æ¨¡å‹è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§ç»“åˆæ–‡åŒ–èƒŒæ™¯çš„æç¤ºè®¾è®¡ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤šæ¨¡æ€æ¨¡å‹åœ¨å™äº‹è§’è‰²åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„å•ä¸€è¯­è¨€æˆ–å•ä¸€æ¨¡æ€æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æŸå¤±å‡½æ•°ä»¥é€‚åº”ä¸åŒè§’è‰²çš„åˆ†ç±»éœ€æ±‚ï¼Œå¹¶å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œäº†ç»†è‡´è°ƒä¼˜ã€‚æ­¤å¤–ï¼Œæç¤ºè®¾è®¡ä¸­ç»“åˆäº†ç»“æ„åŒ–æŒ‡ä»¤å’Œè§’è‰²å®šä¹‰ï¼Œä»¥å¼•å¯¼æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DeBERTa-v3å’ŒQwen2.5-VLç­‰å¤§å‹æ¨¡å‹åœ¨è§’è‰²è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ç²¾åº¦å’ŒF1åˆ†æ•°ä¸Šã€‚ç„¶è€Œï¼Œ'å—å®³è€…'ç±»çš„è¯†åˆ«ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œæç¤ºè®¾è®¡ç­–ç•¥çš„å¼•å…¥è™½ç„¶å¸¦æ¥äº†è¾¹é™…æ”¹è¿›ï¼Œä½†åœ¨æ•´ä½“æ€§èƒ½ä¸Šè¡¨ç°ä¸€è‡´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æã€åœ¨çº¿ç¤¾åŒºç®¡ç†å’Œè‡ªåŠ¨åŒ–å†…å®¹å®¡æ ¸ç­‰ã€‚é€šè¿‡æé«˜å¯¹è¡¨æƒ…åŒ…ä¸­å™äº‹è§’è‰²çš„è¯†åˆ«èƒ½åŠ›ï¼Œå¯ä»¥å¸®åŠ©å¹³å°æ›´å¥½åœ°ç†è§£ç”¨æˆ·ç”Ÿæˆå†…å®¹çš„æƒ…æ„Ÿå’Œæ„å›¾ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œå†…å®¹ç®¡ç†çš„æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„å¤šæ¨¡æ€å†…å®¹åˆ†æä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work investigates the challenging task of identifying narrative roles - Hero, Villain, Victim, and Other - in Internet memes, across three diverse test sets spanning English and code-mixed (English-Hindi) languages. Building on an annotated dataset originally skewed toward the 'Other' class, we explore a more balanced and linguistically diverse extension, originally introduced as part of the CLEF 2024 shared task. Comprehensive lexical and structural analyses highlight the nuanced, culture-specific, and context-rich language used in real memes, in contrast to synthetically curated hateful content, which exhibits explicit and repetitive lexical markers. To benchmark the role detection task, we evaluate a wide spectrum of models, including fine-tuned multilingual transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs, and multimodal vision-language models. Performance is assessed under zero-shot settings using precision, recall, and F1 metrics. While larger models like DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent challenges in reliably identifying the 'Victim' class and generalising across cultural and code-mixed content. We also explore prompt design strategies to guide multimodal models and find that hybrid prompts incorporating structured instructions and role definitions offer marginal yet consistent improvements. Our findings underscore the importance of cultural grounding, prompt engineering, and multimodal reasoning in modelling subtle narrative framings in visual-textual content.

