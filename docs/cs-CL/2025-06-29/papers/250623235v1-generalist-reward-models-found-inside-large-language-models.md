---
layout: default
title: Generalist Reward Models: Found Inside Large Language Models
---

# Generalist Reward Models: Found Inside Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23235" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23235v1</a>
  <a href="https://arxiv.org/pdf/2506.23235.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23235v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23235v1', 'Generalist Reward Models: Found Inside Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi-Chen Li, Tian Xu, Yang Yu, Xuqin Zhang, Xiong-Hui Chen, Zhongxiang Ling, Ningjing Chao, Lei Yuan, Zhi-Hua Zhou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨å¥–åŠ±æ¨¡å‹ä»¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¥–åŠ±æ¨¡å‹` `é€†å¼ºåŒ–å­¦ä¹ ` `å†…ç”Ÿå¥–åŠ±` `å¯¹é½ä¼˜åŒ–` `å¤šæ¨¡æ€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººç±»åå¥½æ•°æ®æ¥è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç¼ºä¹æœ‰æ•ˆçš„ç†è®ºåŸºç¡€ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç›´æ¥å¼•å‡ºå†…ç”Ÿå¥–åŠ±çš„æ–¹æ³•ï¼Œé¿å…äº†é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¥–åŠ±æ¨¡å‹å’ŒLLMè¯„åˆ¤è€…æ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹é½ä¾èµ–äºåŸºäºäººç±»åå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œè€Œè¿™äº›æ¨¡å‹çš„è®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚å°½ç®¡è¿‘æœŸç ”ç©¶å°è¯•é€šè¿‡AIåé¦ˆæ¥é™ä½æˆæœ¬ï¼Œä½†ç¼ºä¹ä¸¥è°¨çš„ç†è®ºåŸºç¡€ã€‚æœ¬æ–‡å‘ç°ï¼Œä»»ä½•é€šè¿‡æ ‡å‡†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è®­ç»ƒçš„LLMä¸­éƒ½æ½œåœ¨å­˜åœ¨ä¸€ä¸ªå¼ºå¤§çš„é€šç”¨å¥–åŠ±æ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜äº†è¿™ç§å†…ç”Ÿå¥–åŠ±å¹¶éå¯å‘å¼ï¼Œè€Œæ˜¯ä¸é€šè¿‡ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ å­¦ä¹ çš„å¥–åŠ±å‡½æ•°åœ¨ç†è®ºä¸Šç­‰ä»·ã€‚è¿™ä¸€è”ç³»ä½¿æˆ‘ä»¬èƒ½å¤Ÿç›´æ¥ä»åŸºç¡€æ¨¡å‹ä¸­å¼•å‡ºé«˜è´¨é‡çš„å¥–åŠ±ä¿¡å·ï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œä½¿ç”¨è¿™ç§å†…ç”Ÿå¥–åŠ±è¿›è¡Œåç»­å¼ºåŒ–å­¦ä¹ ä¼šå¯¼è‡´å…·æœ‰å¯è¯æ˜ä¼˜è¶Šè¯¯å·®ç•Œé™çš„ç­–ç•¥ã€‚å®éªŒéªŒè¯äº†è¿™ä¸€ç†è®ºï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¼˜äºç°æœ‰çš„LLMä½œä¸ºè¯„åˆ¤è€…çš„æ–¹æ³•ï¼Œè¿˜èƒ½è¶…è¶Šæ˜¾å¼è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¥–åŠ±å»ºæ¨¡é˜¶æ®µå¯ä»¥è¢«æ›´æœ‰æ•ˆçš„çŸ¥è¯†å¼•å‡ºæ–¹æ³•æ‰€æ›¿ä»£ï¼Œä¸ºLLMsåŠå¤šæ¨¡æ€æ¨¡å‹çš„å¯¹é½æä¾›äº†æ›´é«˜æ•ˆã€å¼ºå¤§å’Œå¯æ‰©å±•çš„èŒƒå¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½è¿‡ç¨‹ä¸­å¯¹äººç±»åå¥½æ•°æ®çš„ä¾èµ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•çš„é«˜æˆæœ¬å’Œç†è®ºåŸºç¡€è–„å¼±æ˜¯ä¸»è¦ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºé€šè¿‡ä»æ ‡å‡†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è®­ç»ƒçš„LLMä¸­å¼•å‡ºå†…ç”Ÿå¥–åŠ±ï¼Œè¯æ˜å…¶ä¸é€†å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°ç­‰ä»·ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å¥–åŠ±ä¿¡å·è·å–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) ä»é¢„è®­ç»ƒæˆ–ç›‘ç£å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ä¸­æå–å†…ç”Ÿå¥–åŠ±ï¼›2) ä½¿ç”¨è¯¥å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼›3) è¯„ä¼°ç­–ç•¥çš„è¯¯å·®ç•Œé™ã€‚

**å…³é”®åˆ›æ–°**ï¼šé¦–æ¬¡ç†è®ºè¯æ˜äº†å†…ç”Ÿå¥–åŠ±çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºé«˜æ•ˆçš„å¥–åŠ±å»ºæ¨¡æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†æ ‡å‡†çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿äº†ä»åŸºç¡€æ¨¡å‹ä¸­æå–å¥–åŠ±ä¿¡å·çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å†…ç”Ÿå¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥åœ¨è¯¯å·®ç•Œé™ä¸Šæ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä¸”åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¥–åŠ±æ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¥–åŠ±æ¨¡å‹çš„å¯¹é½è¿‡ç¨‹ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼Œæ¨åŠ¨AIæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The alignment of Large Language Models (LLMs) is critically dependent on reward models trained on costly human preference data. While recent work explores bypassing this cost with AI feedback, these methods often lack a rigorous theoretical foundation. In this paper, we discover that a powerful generalist reward model is already latently present within any LLM trained via standard next-token prediction. We prove that this endogenous reward is not a heuristic, but is theoretically equivalent to a reward function learned through offline inverse reinforcement learning. This connection allows us to directly elicit a high-quality reward signal from a base (pre-trained or supervised fine-tuned) model without any further training. Critically, we also prove that subsequent reinforcement learning using this endogenous reward leads to a policy with a provably superior error bound compared to the base model. To our best knowledge, this is the first theoretical proof of the effectiveness of reinforcement learning for LLMs. Our experiments validate this theory, demonstrating that our method not only outperforms existing LLM-as-a-judge approaches but can also surpass explicitly trained reward models. These findings suggest that the reward modeling stage can be replaced by a principled method of eliciting the knowledge already captured during pre-training, heralding a more efficient, powerful, and scalable paradigm for LLMs alignment as well as multi-modal models.

