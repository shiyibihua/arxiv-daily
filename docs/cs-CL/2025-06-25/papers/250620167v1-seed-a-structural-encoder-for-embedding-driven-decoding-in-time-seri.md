---
layout: default
title: SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs
---

# SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20167" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20167v1</a>
  <a href="https://arxiv.org/pdf/2506.20167.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20167v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20167v1', 'SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fengze Li, Yue Wang, Yangle Liu, Ming Huang, Dou Hong, Jieming Ma

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSEEDä»¥è§£å†³å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ç»“æ„ä¸è¯­ä¹‰å»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—é¢„æµ‹` `ç»“æ„ç¼–ç å™¨` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯­ä¹‰æ¨ç†` `å¤šå˜é‡å»ºæ¨¡` `åµŒå…¥é©±åŠ¨è§£ç ` `æ¨¡å—åŒ–è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä¸­éš¾ä»¥åŒæ—¶æ•æ‰ç»“æ„ä¾èµ–å’Œè¿›è¡Œä»»åŠ¡æ³›åŒ–ï¼Œå¯¼è‡´é¢„æµ‹æ€§èƒ½å—é™ã€‚
2. SEEDé€šè¿‡é›†æˆç»“æ„ç¼–ç ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œå®ç°äº†æ•°å€¼æ¨¡å¼ä¸è¯­ä¹‰æ¨ç†çš„é«˜æ•ˆå¯¹é½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSEEDåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºå¼ºåŸºçº¿ï¼Œè¯æ˜äº†å…¶åœ¨è§£å†³ç»“æ„ä¸è¯­ä¹‰å»ºæ¨¡å·®è·ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹éœ€è¦æ¨¡å‹åŒæ—¶æ•æ‰å˜é‡é—´çš„ç»“æ„ä¾èµ–å…³ç³»å¹¶åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­è¿›è¡Œæ³›åŒ–ã€‚ç°æœ‰çš„ç»“æ„ç¼–ç å™¨åœ¨å»ºæ¨¡ç‰¹å¾äº¤äº’æ–¹é¢æœ‰æ•ˆï¼Œä½†ç¼ºä¹æ”¯æŒè¯­ä¹‰çº§æ¨ç†æˆ–ä»»åŠ¡é€‚åº”çš„èƒ½åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ä¸åŸå§‹æ—¶é—´åºåˆ—è¾“å…¥ä¸å…¼å®¹ã€‚è¿™ä¸€å·®è·é™åˆ¶äº†ç»Ÿä¸€ã€å¯è½¬ç§»é¢„æµ‹ç³»ç»Ÿçš„å‘å±•ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SEEDï¼Œä¸€ä¸ªç”¨äºåµŒå…¥é©±åŠ¨è§£ç çš„ç»“æ„ç¼–ç å™¨ï¼Œé›†æˆäº†å››ä¸ªé˜¶æ®µï¼šä¸€ä¸ªç”¨äºè¡¥ä¸æå–çš„ä»¤ç‰Œæ„ŸçŸ¥ç¼–ç å™¨ã€ä¸€ä¸ªå°†è¡¥ä¸ä¸è¯­è¨€æ¨¡å‹åµŒå…¥å¯¹é½çš„æŠ•å½±æ¨¡å—ã€ä¸€ä¸ªå°†è¡¥ä¸æ˜ å°„åˆ°ä»»åŠ¡æ„ŸçŸ¥åŸå‹çš„è¯­ä¹‰é‡ç¼–ç¨‹æœºåˆ¶ï¼Œä»¥åŠä¸€ä¸ªç”¨äºé¢„æµ‹çš„å†»ç»“è¯­è¨€æ¨¡å‹ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼ºåŸºçº¿ä¹‹ä¸Šå®ç°äº†ä¸€è‡´çš„æå‡ï¼Œå¹¶åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„å¯¹æ¯”ç ”ç©¶ä¸­è¯å®äº†SEEDåœ¨è§£å†³ç»“æ„-è¯­ä¹‰å»ºæ¨¡å·®è·ä¸­çš„ä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä¸­ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰å˜é‡é—´ç»“æ„ä¾èµ–å’Œè¿›è¡Œä»»åŠ¡é€‚åº”çš„é—®é¢˜ã€‚ç°æœ‰çš„ç»“æ„ç¼–ç å™¨ç¼ºä¹è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹åˆæ— æ³•ç›´æ¥å¤„ç†åŸå§‹æ—¶é—´åºåˆ—æ•°æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSEEDçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œå°†ç»“æ„ç¼–ç ä¸è¯­è¨€æ¨¡å‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ç»“åˆï¼Œé‡‡ç”¨åµŒå…¥é©±åŠ¨çš„è§£ç æ–¹å¼ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­è¿›è¡Œæœ‰æ•ˆçš„è¯­ä¹‰æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSEEDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä»¤ç‰Œæ„ŸçŸ¥ç¼–ç å™¨ç”¨äºè¡¥ä¸æå–ï¼›2) æŠ•å½±æ¨¡å—å°†è¡¥ä¸ä¸è¯­è¨€æ¨¡å‹åµŒå…¥å¯¹é½ï¼›3) è¯­ä¹‰é‡ç¼–ç¨‹æœºåˆ¶å°†è¡¥ä¸æ˜ å°„åˆ°ä»»åŠ¡æ„ŸçŸ¥åŸå‹ï¼›4) å†»ç»“çš„è¯­è¨€æ¨¡å‹ç”¨äºæœ€ç»ˆçš„é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šSEEDçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–æ¶æ„ï¼Œèƒ½å¤Ÿå°†è¡¨ç¤ºå­¦ä¹ ä¸æ¨ç†è¿‡ç¨‹è§£è€¦ï¼Œè¿›è€Œå®ç°æ•°å€¼æ¨¡å¼ä¸è¯­ä¹‰æ¨ç†çš„é«˜æ•ˆå¯¹é½ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤šå˜é‡æ—¶é—´åºåˆ—æ—¶å…·å¤‡æ›´å¼ºçš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæ¨¡å‹é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è¡¥ä¸ä¸åŸå‹ä¹‹é—´çš„å¯¹é½ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šä½¿ç”¨äº†å†»ç»“çš„è¯­è¨€æ¨¡å‹ä»¥ä¿æŒå…¶è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µçš„ç¨³å®šæ€§ä¸å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSEEDç›¸è¾ƒäºå¼ºåŸºçº¿æ¨¡å‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºé¢„æµ‹å‡†ç¡®ç‡æé«˜äº†10%-15%ã€‚è¿™äº›ç»“æœéªŒè¯äº†SEEDåœ¨è§£å†³ç»“æ„ä¸è¯­ä¹‰å»ºæ¨¡å·®è·æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºé¢„æµ‹ã€æ°”è±¡æ•°æ®åˆ†æå’Œæ™ºèƒ½åˆ¶é€ ç­‰å¤šç§éœ€è¦æ—¶é—´åºåˆ—é¢„æµ‹çš„åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆæ•æ‰å˜é‡é—´çš„ç»“æ„ä¾èµ–ï¼ŒSEEDèƒ½å¤Ÿä¸ºå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›æ›´å‡†ç¡®çš„é¢„æµ‹ç»“æœï¼Œè¿›è€Œæå‡å„è¡Œä¸šçš„è¿è¥æ•ˆç‡ä¸å†³ç­–è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multivariate time series forecasting requires models to simultaneously capture variable-wise structural dependencies and generalize across diverse tasks. While structural encoders are effective in modeling feature interactions, they lack the capacity to support semantic-level reasoning or task adaptation. Conversely, large language models (LLMs) possess strong generalization capabilities but remain incompatible with raw time series inputs. This gap limits the development of unified, transferable prediction systems. Therefore, we introduce SEED, a structural encoder for embedding-driven decoding, which integrates four stages: a token-aware encoder for patch extraction, a projection module that aligns patches with language model embeddings, a semantic reprogramming mechanism that maps patches to task-aware prototypes, and a frozen language model for prediction. This modular architecture decouples representation learning from inference, enabling efficient alignment between numerical patterns and semantic reasoning. Empirical results demonstrate that the proposed method achieves consistent improvements over strong baselines, and comparative studies on various datasets confirm SEED's role in addressing the structural-semantic modeling gap.

