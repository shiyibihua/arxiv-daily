---
layout: default
title: MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering
---

# MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20821" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20821v1</a>
  <a href="https://arxiv.org/pdf/2506.20821.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20821v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20821v1', 'MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chinmay Gondhalekar, Urjitkumar Patel, Fang-Chun Yeh

**åˆ†ç±»**: cs.CL, cs.AI, cs.CE

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

**å¤‡æ³¨**: Preprint Copy

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMultiFinRAGä»¥è§£å†³é‡‘èé—®ç­”ä¸­çš„å¤šæ¨¡æ€æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `é‡‘èé—®ç­”` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `è·¨æ¨¡æ€æ¨ç†` `è½»é‡åŒ–æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é‡‘èæ–‡æ¡£æ—¶é¢ä¸´å¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç”±äºä»¤ç‰Œé™åˆ¶å’Œå¸ƒå±€ä¸¢å¤±ï¼Œå¯¼è‡´ä¿¡æ¯ç¢ç‰‡åŒ–ã€‚
2. MultiFinRAGé€šè¿‡è½»é‡åŒ–çš„å¤šæ¨¡æ€LLMè¿›è¡Œå¤šæ¨¡æ€æå–ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚å›é€€ç­–ç•¥ï¼Œå®ç°è·¨æ¨¡æ€æ¨ç†ã€‚
3. åœ¨å¤æ‚çš„é‡‘èé—®ç­”ä»»åŠ¡ä¸­ï¼ŒMultiFinRAGçš„å‡†ç¡®ç‡æ¯”åŸºçº¿æ¨¡å‹æé«˜äº†19ä¸ªç™¾åˆ†ç‚¹ï¼Œè¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é‡‘èæ–‡æ¡£å¦‚10-Kã€10-Qå’ŒæŠ•èµ„è€…æ¼”ç¤ºæ–‡ç¨¿é€šå¸¸åŒ…å«æ•°ç™¾é¡µå†…å®¹ï¼Œç»“åˆäº†å¯†é›†çš„å™è¿°æ–‡æœ¬ã€ç»“æ„åŒ–è¡¨æ ¼å’Œå¤æ‚å›¾å½¢ã€‚å›ç­”è¿™äº›å†…å®¹çš„é—®é¢˜å¸¸å¸¸éœ€è¦è·¨æ¨¡æ€çš„è”åˆæ¨ç†ï¼Œè¿™å¯¹ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“é€ æˆäº†æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†MultiFinRAGï¼Œä¸€ä¸ªä¸“ä¸ºé‡‘èé—®ç­”è®¾è®¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è¡¨æ ¼å’Œå›¾åƒåˆ†ç»„å¹¶å‘é€è‡³è½»é‡åŒ–çš„å¤šæ¨¡æ€LLMè¿›è¡Œæå–ï¼Œç”Ÿæˆç»“æ„åŒ–çš„JSONè¾“å‡ºå’Œç®€æ´çš„æ–‡æœ¬æ‘˜è¦ã€‚éšåï¼Œè¿™äº›è¾“å‡ºä¸å™è¿°æ–‡æœ¬ä¸€èµ·è¿›è¡ŒåµŒå…¥å’Œç´¢å¼•ï¼Œä»¥å®ç°ç²¾ç¡®æ£€ç´¢ã€‚æœ€ç»ˆï¼ŒMultiFinRAGåœ¨å¤æ‚çš„é‡‘èé—®ç­”ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡æ¯”ChatGPT-4oï¼ˆå…è´¹ç‰ˆï¼‰æé«˜äº†19ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨é‡‘èæ–‡æ¡£ä¸­è¿›è¡Œé—®ç­”æ—¶ï¼Œä¼ ç»Ÿæ–¹æ³•é¢ä¸´çš„å¤šæ¨¡æ€æ¨ç†å›°éš¾ï¼Œå°¤å…¶æ˜¯ä¿¡æ¯çš„ä»¤ç‰Œé™åˆ¶å’Œå¸ƒå±€ä¸¢å¤±é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMultiFinRAGçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è½»é‡åŒ–çš„å¤šæ¨¡æ€LLMè¿›è¡Œä¿¡æ¯æå–ï¼Œå¹¶ç»“åˆåˆ†å±‚å›é€€ç­–ç•¥ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶åŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡ï¼Œå¢å¼ºè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆè¿›è¡Œå¤šæ¨¡æ€æå–ï¼Œå°†è¡¨æ ¼å’Œå›¾åƒåˆ†æ‰¹å¤„ç†ï¼›ç„¶åç”Ÿæˆç»“æ„åŒ–çš„JSONè¾“å‡ºå’Œæ–‡æœ¬æ‘˜è¦ï¼›æ¥ç€è¿›è¡ŒåµŒå…¥å’Œç´¢å¼•ï¼›æœ€åå®æ–½åˆ†å±‚å›é€€ç­–ç•¥ä»¥ä¼˜åŒ–æ£€ç´¢è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMultiFinRAGçš„åˆ›æ–°åœ¨äºå…¶ä¸“ä¸ºé‡‘èé—®ç­”è®¾è®¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶åœ¨åŠ¨æ€ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¨ç†ï¼Œæ˜¾è‘—æå‡äº†é—®ç­”å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ¡†æ¶é‡‡ç”¨äº†é‡åŒ–çš„è½»é‡åŒ–å¤šæ¨¡æ€LLMï¼Œè®¾ç½®äº†æ¨¡æ€æ„ŸçŸ¥çš„ç›¸ä¼¼æ€§é˜ˆå€¼ï¼Œå¹¶è®¾è®¡äº†åˆ†å±‚å›é€€ç­–ç•¥ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸‹çš„æœ‰æ•ˆä¿¡æ¯æ£€ç´¢ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤æ‚çš„é‡‘èé—®ç­”ä»»åŠ¡ä¸­ï¼ŒMultiFinRAGçš„å‡†ç¡®ç‡æ¯”ChatGPT-4oï¼ˆå…è´¹ç‰ˆï¼‰é«˜å‡º19ä¸ªç™¾åˆ†ç‚¹ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ—¶çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒMultiFinRAGåœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MultiFinRAGåœ¨é‡‘èé¢†åŸŸçš„æ½œåœ¨åº”ç”¨å¹¿æ³›ï¼ŒåŒ…æ‹¬æŠ•èµ„åˆ†æã€è´¢åŠ¡æŠ¥å‘Šè§£è¯»å’Œé£é™©è¯„ä¼°ç­‰ã€‚å…¶é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›èƒ½å¤Ÿå¸®åŠ©é‡‘èä¸“ä¸šäººå£«å¿«é€Ÿè·å–å…³é”®ä¿¡æ¯ï¼Œæé«˜å†³ç­–æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†çš„é¢†åŸŸï¼Œå¦‚æ³•å¾‹æ–‡æ¡£åˆ†æå’ŒåŒ»ç–—è®°å½•è§£è¯»ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span hundreds of pages and combine diverse modalities, including dense narrative text, structured tables, and complex figures. Answering questions over such content often requires joint reasoning across modalities, which strains traditional large language models (LLMs) and retrieval-augmented generation (RAG) pipelines due to token limitations, layout loss, and fragmented cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation framework purpose-built for financial QA. MultiFinRAG first performs multimodal extraction by grouping table and figure images into batches and sending them to a lightweight, quantized open-source multimodal LLM, which produces both structured JSON outputs and concise textual summaries. These outputs, along with narrative text, are embedded and indexed with modality-aware similarity thresholds for precise retrieval. A tiered fallback strategy then dynamically escalates from text-only to text+table+image contexts when necessary, enabling cross-modal reasoning while reducing irrelevant context. Despite running on commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy than ChatGPT-4o (free-tier) on complex financial QA tasks involving text, tables, images, and combined multimodal reasoning.

