---
layout: default
title: ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models
---

# ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.02919" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.02919v1</a>
  <a href="https://arxiv.org/pdf/2507.02919.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.02919v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.02919v1', 'ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dai Li, Linzhuo Li, Huilian Sophie Qiu

**åˆ†ç±»**: cs.CL, cs.CY, cs.ET

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‡†ç¡®æ€§ä¼˜åŒ–å‡è®¾ä»¥è§£å†³LLMåœ¨æ°‘æ„è°ƒæŸ¥ä¸­çš„ä»£è¡¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ°‘æ„è°ƒæŸ¥` `ç»“æ„ä¸€è‡´æ€§` `åŒè´¨åŒ–` `å‡†ç¡®æ€§ä¼˜åŒ–å‡è®¾` `ç¤¾ä¼šç§‘å­¦ç ”ç©¶` `æ”¿ç­–åˆ¶å®š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨¡æ‹Ÿäººç±»æ„è§æ—¶å­˜åœ¨ç»“æ„ä¸€è‡´æ€§ç¼ºå¤±å’Œå°‘æ•°æ„è§åŒè´¨åŒ–çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºäº†å‡†ç¡®æ€§ä¼˜åŒ–å‡è®¾ï¼Œè®¤ä¸ºåŒè´¨åŒ–æ˜¯ç”±äºä¼˜å…ˆè€ƒè™‘æ¨¡æ€å“åº”æ‰€è‡´ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMçš„å“åº”åœ¨ç»“æ„ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ä¸Šæ˜¾è‘—ä½äºäººç±»æ•°æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTå’ŒLlamaè¢«æè®®ä½œä¸ºæ¨¡æ‹Ÿäººç±»æ„è§çš„â€œç¡…æ ·æœ¬â€ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è¿™ä¸€è§‚ç‚¹ï¼Œè®¤ä¸ºLLMså¯èƒ½ä¼šè¯¯è¡¨è¾¾äººå£å±‚é¢çš„æ„è§ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šç»“æ„ä¸€è‡´æ€§ç¼ºå¤±ï¼Œå³å“åº”å‡†ç¡®æ€§åœ¨ä¸åŒäººå£èšåˆå±‚æ¬¡ä¸Šä¸ä¸€è‡´ï¼Œä»¥åŠåŒè´¨åŒ–ï¼Œå³å°‘æ•°æ„è§çš„ä»£è¡¨æ€§ä¸è¶³ã€‚é€šè¿‡å¯¹ChatGPTï¼ˆGPT-4ï¼‰å’ŒMetaçš„Llama 3.1ç³»åˆ—è¿›è¡Œè°ƒæŸ¥ï¼Œæˆ‘ä»¬å‘ç°LLMçš„å“åº”ä¸äººç±»æ•°æ®ç›¸æ¯”å­˜åœ¨æ˜¾è‘—çš„ç»“æ„ä¸ä¸€è‡´æ€§å’Œä¸¥é‡çš„åŒè´¨åŒ–é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†â€œå‡†ç¡®æ€§ä¼˜åŒ–å‡è®¾â€ï¼Œè®¤ä¸ºåŒè´¨åŒ–æºäºå¯¹æ¨¡æ€å“åº”çš„ä¼˜å…ˆè€ƒè™‘ã€‚è¿™äº›é—®é¢˜æŒ‘æˆ˜äº†å°†LLMsä½œä¸ºäººç±»è°ƒæŸ¥æ•°æ®ç›´æ¥æ›¿ä»£å“çš„æœ‰æ•ˆæ€§ï¼Œå¯èƒ½ä¼šå¼ºåŒ–åˆ»æ¿å°è±¡å¹¶è¯¯å¯¼æ”¿ç­–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨¡æ‹Ÿäººç±»æ„è§æ—¶çš„ä»£è¡¨æ€§å’Œç»“æ„ä¸€è‡´æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¸åŒäººå£èšåˆå±‚æ¬¡ä¸Šå“åº”å‡†ç¡®æ€§ä¸ä¸€è‡´ï¼Œä¸”å°‘æ•°ç¾¤ä½“çš„æ„è§è¢«ä¸¥é‡ä½ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºâ€œå‡†ç¡®æ€§ä¼˜åŒ–å‡è®¾â€ï¼Œè®¤ä¸ºLLMsçš„åŒè´¨åŒ–ç°è±¡æºäºå¯¹æ¨¡æ€å“åº”çš„ä¼˜å…ˆè€ƒè™‘ã€‚è¿™ä¸€å‡è®¾ä¸ºç†è§£LLMsçš„å±€é™æ€§æä¾›äº†æ–°çš„è§†è§’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é€šè¿‡å¯¹ChatGPTï¼ˆGPT-4ï¼‰å’ŒMetaçš„Llama 3.1ç³»åˆ—è¿›è¡Œå®éªŒï¼Œä½¿ç”¨ç¾å›½å›½å®¶é€‰ä¸¾ç ”ç©¶ï¼ˆANESï¼‰2020çš„æ•°æ®ï¼Œåˆ†æå…¶å¯¹å •èƒå’Œéæ³•ç§»æ°‘é—®é¢˜çš„å“åº”ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹å“åº”ç”Ÿæˆå’Œç»“æœåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†LLMsåœ¨æ¨¡æ‹Ÿäººç±»æ„è§æ—¶çš„ç»“æ„ä¸ä¸€è‡´æ€§å’ŒåŒè´¨åŒ–é—®é¢˜ï¼Œæå‡ºäº†å‡†ç¡®æ€§ä¼˜åŒ–å‡è®¾ï¼Œå¼ºè°ƒäº†å¯¹æ¨¡æ€å“åº”çš„åå¥½å¯¹ç»“æœçš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨äº†ä¸åŒè§„æ¨¡çš„Llamaæ¨¡å‹ï¼ˆ8Bã€70Bã€405Bï¼‰ï¼Œå¹¶å¯¹å…¶å“åº”è¿›è¡Œå®šé‡åˆ†æï¼Œæ¯”è¾ƒå…¶ä¸äººç±»æ•°æ®çš„å·®å¼‚ï¼Œé‡ç‚¹å…³æ³¨å“åº”çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMçš„å“åº”åœ¨ç»“æ„ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ä¸Šæ˜¾è‘—ä½äºäººç±»æ•°æ®ï¼Œå°¤å…¶åœ¨å •èƒå’Œéæ³•ç§»æ°‘é—®é¢˜ä¸Šï¼Œå­˜åœ¨ä¸¥é‡çš„åŒè´¨åŒ–ç°è±¡ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†LLMsä½œä¸ºäººç±»è°ƒæŸ¥æ•°æ®æ›¿ä»£å“çš„å±€é™æ€§ï¼Œå¯èƒ½å¯¼è‡´æ”¿ç­–åˆ¶å®šä¸­çš„åˆ»æ¿å°è±¡å’Œè¯¯å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ”¿ç­–åˆ¶å®šã€ç¤¾ä¼šè°ƒæŸ¥å’Œå¸‚åœºç ”ç©¶ç­‰ã€‚é€šè¿‡æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨¡æ‹Ÿäººç±»æ„è§æ—¶çš„å±€é™æ€§ï¼Œç ”ç©¶ä¸ºå¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨LLMsæä¾›äº†é‡è¦çš„æŒ‡å¯¼ï¼Œé¿å…åœ¨å†³ç­–è¿‡ç¨‹ä¸­äº§ç”Ÿè¯¯å¯¼ã€‚æœªæ¥ï¼Œæ”¹è¿›LLMsçš„ä»£è¡¨æ€§å’Œä¸€è‡´æ€§å°†å¯¹ç¤¾ä¼šç§‘å­¦ç ”ç©¶äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) in the form of chatbots like ChatGPT and Llama are increasingly proposed as "silicon samples" for simulating human opinions. This study examines this notion, arguing that LLMs may misrepresent population-level opinions. We identify two fundamental challenges: a failure in structural consistency, where response accuracy doesn't hold across demographic aggregation levels, and homogenization, an underrepresentation of minority opinions. To investigate these, we prompted ChatGPT (GPT-4) and Meta's Llama 3.1 series (8B, 70B, 405B) with questions on abortion and unauthorized immigration from the American National Election Studies (ANES) 2020. Our findings reveal significant structural inconsistencies and severe homogenization in LLM responses compared to human data. We propose an "accuracy-optimization hypothesis," suggesting homogenization stems from prioritizing modal responses. These issues challenge the validity of using LLMs, especially chatbots AI, as direct substitutes for human survey data, potentially reinforcing stereotypes and misinforming policy.

