---
layout: default
title: When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs
---

# When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20544" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20544v1</a>
  <a href="https://arxiv.org/pdf/2506.20544.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20544v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20544v1', 'When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ammar Khairi, Daniel D'souza, Ye Shen, Julia Kreutzer, Sara Hooker

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¯­è¨€LLMæ¨ç†è®¡ç®—æ‰©å±•ç­–ç•¥ä»¥æå‡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æ¨¡å‹` `æ¨ç†è®¡ç®—` `é‡‡æ ·ç­–ç•¥` `é€‰æ‹©ç­–ç•¥` `å¼€æ”¾å¼ä»»åŠ¡` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†è®¡ç®—æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è‹±è¯­å’Œç‰¹å®šé¢†åŸŸï¼Œç¼ºä¹å¯¹å¤šè¯­è¨€å’Œå¼€æ”¾å¼ä»»åŠ¡çš„é€‚åº”æ€§ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†æ–°çš„é‡‡æ ·å’Œé€‰æ‹©ç­–ç•¥ï¼Œæ—¨åœ¨é’ˆå¯¹å¤šè¯­è¨€å’Œå¤šä»»åŠ¡æ¨ç†åœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ–°ç­–ç•¥åï¼Œæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­èƒœç‡æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•ä½¿å¾—æ¨ç†æ—¶è®¡ç®—çš„æ‰©å±•æˆä¸ºç ”ç©¶é‡ç‚¹ï¼Œä»è€Œåœ¨ä¸é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹æå‡æ€§èƒ½ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­åŠå°‘æ•°é¢†åŸŸï¼Œè€Œæœ¬ç ”ç©¶å…³æ³¨äºå¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡çš„å¤šè¯­è¨€ã€å¤šä»»åŠ¡è®¾ç½®ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºæ¸©åº¦å˜åŒ–çš„é‡‡æ ·ç­–ç•¥å’Œé€‰æ‹©ç­–ç•¥éœ€é€‚åº”å¤šæ ·åŒ–çš„é¢†åŸŸå’Œè¯­è¨€ç¯å¢ƒã€‚æˆ‘ä»¬æå‡ºäº†é’ˆå¯¹å¤šè¯­è¨€å’Œå¤šä»»åŠ¡æ¨ç†åœºæ™¯çš„æ–°å‹é‡‡æ ·å’Œé€‰æ‹©ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†ä¸åŒè¯­è¨€å’Œä»»åŠ¡çš„è¡¨ç°ï¼Œå°¤å…¶åœ¨m-ArenaHard-v2.0åŸºå‡†ä¸Šï¼Œ8Bæ¨¡å‹çš„èƒœç‡æå‡äº†6.8ä¸ªç™¾åˆ†ç‚¹ï¼Œ111Bæ¨¡å‹çš„èƒœç‡æå‡äº†9.0ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾ç¤ºå‡ºåœ¨ä½æˆæœ¬ä¸‹å®ç°æ˜¾è‘—æ€§èƒ½æå‡çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰å¤šè¯­è¨€LLMæ¨ç†è®¡ç®—æ–¹æ³•åœ¨å¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ä¸­çš„é€‚åº”æ€§ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒè¯­è¨€å’Œé¢†åŸŸçš„è¡¨ç°ä¸å‡è¡¡é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è°ƒæ•´é‡‡æ ·ç­–ç•¥å’Œé€‰æ‹©ç­–ç•¥ï¼Œé’ˆå¯¹å¤šè¯­è¨€å’Œå¤šä»»åŠ¡ç¯å¢ƒè¿›è¡Œä¼˜åŒ–ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ä¸åŒè¯­è¨€å’Œä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é‡‡æ ·é˜¶æ®µå’Œé€‰æ‹©é˜¶æ®µï¼Œé¦–å…ˆè¿›è¡Œå¤šæ ·åŒ–çš„è¾“å‡ºé‡‡æ ·ï¼Œç„¶åæ ¹æ®ç‰¹å®šç­–ç•¥é€‰æ‹©æœ€ä½³è¾“å‡ºï¼Œç¡®ä¿é€‚åº”ä¸åŒè¯­è¨€å’Œä»»åŠ¡çš„éœ€æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†é’ˆå¯¹å¤šè¯­è¨€å’Œå¤šä»»åŠ¡çš„ä¸“ç”¨é‡‡æ ·å’Œé€‰æ‹©ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨éè‹±è¯­ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†æ¸©åº¦å˜åŒ–çš„é‡‡æ ·ç­–ç•¥ï¼Œå¹¶åœ¨é€‰æ‹©é˜¶æ®µå¼•å…¥äº†å¤šæ ·åŒ–çš„é€‰æ‹©æ ‡å‡†ï¼Œä»¥é€‚åº”ä¸åŒçš„è¯­è¨€å’Œä»»åŠ¡ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ–°æå‡ºçš„é‡‡æ ·å’Œé€‰æ‹©ç­–ç•¥åï¼Œ8Bæ¨¡å‹åœ¨m-ArenaHard-v2.0åŸºå‡†æµ‹è¯•ä¸­èƒœç‡æå‡äº†6.8ä¸ªç™¾åˆ†ç‚¹ï¼Œè€Œ111Bæ¨¡å‹åœ¨ç›¸åŒåŸºå‡†ä¸Šèƒœç‡æå‡äº†9.0ä¸ªç™¾åˆ†ç‚¹ï¼Œè¡¨æ˜åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€ç¿»è¯‘ã€è·¨æ–‡åŒ–å†…å®¹ç”Ÿæˆå’Œå¤šä»»åŠ¡å­¦ä¹ ç­‰ã€‚é€šè¿‡æå‡å¤šè¯­è¨€LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æœåŠ¡äºå…¨çƒç”¨æˆ·ï¼Œä¿ƒè¿›ä¸åŒè¯­è¨€ä¹‹é—´çš„äº¤æµä¸ç†è§£ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œå•†ä¸šæ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.
>   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages.

