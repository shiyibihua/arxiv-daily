---
layout: default
title: Multi-lingual Functional Evaluation for Large Language Models
---

# Multi-lingual Functional Evaluation for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20793" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20793v1</a>
  <a href="https://arxiv.org/pdf/2506.20793.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20793v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20793v1', 'Multi-lingual Functional Evaluation for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Victor Ojewale, Inioluwa Deborah Raji, Suresh Venkatasubramanian

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¯­è¨€åŠŸèƒ½è¯„ä¼°åŸºå‡†ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€è¯„ä¼°` `å¤§è¯­è¨€æ¨¡å‹` `åŠŸèƒ½åŸºå‡†` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹é²æ£’æ€§` `è·¨è¯­è¨€èƒ½åŠ›` `æ•™è‚²æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šè¯­è¨€è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–é™æ€æ•°æ®åŸºå‡†ï¼Œæ— æ³•å…¨é¢åæ˜ æ¨¡å‹çš„å®é™…è¡¨ç°å’Œé²æ£’æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†è·¨è¯­è¨€åŠŸèƒ½åŸºå‡†ï¼Œé€šè¿‡ç¿»è¯‘ç°æœ‰çš„åŠŸèƒ½åŸºå‡†æ¨¡æ¿ï¼Œæ¶µç›–å¤šç§è¯­è¨€ä»¥è¯„ä¼°æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒæŸäº›å¤šè¯­è¨€åŸºå‡†åœ¨æ•æ‰æ¨¡å‹æ€§èƒ½æ–¹é¢è¡¨ç°ä¼˜äºå…¶ä»–åŸºå‡†ï¼Œæ€§èƒ½ä¸‹é™å¹…åº¦åœ¨15%è‡³24%ä¹‹é—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›é€šå¸¸é€šè¿‡é™æ€æ•°æ®åŸºå‡†è¿›è¡Œè¯„ä¼°ï¼Œå¦‚Belebeleã€M-MMLUå’ŒM-GSMã€‚ç„¶è€Œï¼Œè¿™äº›è¯„ä¼°æœªèƒ½å……åˆ†ç†è§£æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„å®é™…è¡¨ç°å’Œé²æ£’æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡åˆ›å»ºäº†å¤šè¯­è¨€åŠŸèƒ½åŸºå‡†â€”â€”è·¨è¯­è¨€å°å­¦æ•°å­¦ç¬¦å·ï¼ˆCL-GSM Symbolicï¼‰å’Œè·¨è¯­è¨€æŒ‡ä»¤è·Ÿéšè¯„ä¼°ï¼ˆCL-IFEvalï¼‰ï¼Œé€šè¿‡å°†ç°æœ‰åŠŸèƒ½åŸºå‡†æ¨¡æ¿ä»è‹±è¯­ç¿»è¯‘æˆæ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€å°åœ°è¯­ã€é˜¿æ‹‰ä¼¯è¯­å’Œçº¦é²å·´è¯­ç­‰äº”ç§è¯­è¨€ã€‚ç»“æœæ˜¾ç¤ºï¼ŒæŸäº›é™æ€å¤šè¯­è¨€åŸºå‡†æ¯”å…¶ä»–åŸºå‡†æ›´èƒ½å‡†ç¡®æ•æ‰åŠŸèƒ½è¡¨ç°ï¼Œæ¨¡å‹åœ¨ä¸åŒè¯­è¨€é—´çš„é²æ£’æ€§å·®å¼‚æ˜¾è‘—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šè¯­è¨€è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯é™æ€åŸºå‡†æ— æ³•å‡†ç¡®åæ˜ æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„å®é™…è¡¨ç°å’Œé²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ›å»ºè·¨è¯­è¨€åŠŸèƒ½åŸºå‡†ï¼Œç¿»è¯‘ç°æœ‰çš„åŠŸèƒ½åŸºå‡†æ¨¡æ¿ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šç§è¯­è¨€ä¸­è¿›è¡Œæœ‰æ•ˆè¯„ä¼°ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè·¨è¯­è¨€å°å­¦æ•°å­¦ç¬¦å·ï¼ˆCL-GSM Symbolicï¼‰å’Œè·¨è¯­è¨€æŒ‡ä»¤è·Ÿéšè¯„ä¼°ï¼ˆCL-IFEvalï¼‰ï¼Œæ¯ä¸ªæ¨¡å—å‡åŸºäºç¿»è¯‘çš„åŠŸèƒ½åŸºå‡†æ¨¡æ¿è¿›è¡Œè®¾è®¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ç¿»è¯‘ç°æœ‰åŸºå‡†æ¨¡æ¿ï¼Œåˆ›å»ºäº†æ–°çš„å¤šè¯­è¨€åŠŸèƒ½è¯„ä¼°å·¥å…·ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œå…³æ³¨äº†ç¿»è¯‘çš„å‡†ç¡®æ€§å’ŒåŠŸèƒ½åŸºå‡†çš„é€‚ç”¨æ€§ï¼Œç¡®ä¿ä¸åŒè¯­è¨€çš„è¯„ä¼°ç»“æœå…·æœ‰å¯æ¯”æ€§ï¼Œä¸”èƒ½å¤Ÿåæ˜ æ¨¡å‹çš„çœŸå®æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCL-GSM Symbolicåœ¨è‹±è¯­ã€æ³•è¯­å’Œè¥¿ç­ç‰™è¯­ä¸­çš„æ€§èƒ½ä¸‹é™å¹…åº¦åˆ†åˆ«ä¸º24%ã€17%å’Œ18%ã€‚åŒæ—¶ï¼ŒBelebeleä¸CL-IFEvalä¹‹é—´çš„æ€§èƒ½ä¸‹é™å¹…åº¦ä¸º15%è‡³24%ï¼Œè€ŒM-MMLUä¸CL-IFEvalä¹‹é—´çš„ä¸‹é™å¹…åº¦ä»…ä¸º0.5%è‡³3%ã€‚è¿™äº›ç»“æœè¡¨æ˜ä¸åŒåŸºå‡†åœ¨æ•æ‰æ¨¡å‹æ€§èƒ½æ–¹é¢çš„å·®å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€è·¨è¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šè¯­è¨€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ã€‚é€šè¿‡æ›´å‡†ç¡®çš„è¯„ä¼°åŸºå‡†ï¼Œç ”ç©¶è€…å’Œå¼€å‘è€…å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œæå‡å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-lingual competence in large language models is often evaluated via static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these evaluations often fail to provide an adequate understanding of the practical performance and robustness of models across multi-lingual settings. In response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following Eval (CL-IFEval)-- by translating existing functional benchmark templates from English to five additional languages that span the range of resources available for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that some static multi-lingual benchmarks capture functional performance much more closely than others (i.e. across models, there is a 24%, 17% and 18% decrease in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish respectively; similarly there's a 15 - 24% performance drop across languages between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between M-MMLU and CL-IFEval). Similarly, we find that model robustness across languages varies significantly, with certain languages (eg. Arabic, English) being the most consistently well performing across evaluation iterations.

