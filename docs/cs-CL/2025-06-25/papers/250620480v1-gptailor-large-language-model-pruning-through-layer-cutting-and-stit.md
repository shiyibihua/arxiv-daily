---
layout: default
title: GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching
---

# GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20480" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20480v1</a>
  <a href="https://arxiv.org/pdf/2506.20480.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20480v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20480v1', 'GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guinan Su, Li Shen, Lu Yin, Shiwei Liu, Yanwu Yang, Jonas Geiping

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Guinan-Su/auto-merge-llm)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGPTailorä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‰ªæ` `å±‚åˆå¹¶` `ä¼˜åŒ–ç®—æ³•` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å‹ç¼©` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡å‹å‰ªææ–¹æ³•ä¸»è¦é›†ä¸­äºå•ä¸€æ¨¡å‹ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„ä¼˜åŠ¿ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å±‚çš„ç»„åˆå’Œåˆå¹¶æ¥å‹ç¼©æ¨¡å‹çš„æ–°ç­–ç•¥ï¼Œæ—¨åœ¨ä¿ç•™ä¸åŒå¾®è°ƒæ¨¡å‹çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å‹ç¼©æ¨¡å‹æ—¶èƒ½å¤Ÿä¿æŒé«˜è¾¾97.3%çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘çº¦25%çš„å‚æ•°é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶åºå¤§çš„æ¨¡å‹è§„æ¨¡åœ¨éƒ¨ç½²å’Œæ¨ç†æ—¶å¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ã€‚è™½ç„¶ç»“æ„åŒ–å‰ªææ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆé™ä½è®¡ç®—æˆæœ¬ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºå•ä¸€æ¨¡å‹çš„å‰ªæã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ç­–ç•¥ï¼Œé€šè¿‡ä»å¾®è°ƒåçš„æ¨¡å‹å˜ä½“ä¸­æˆ˜ç•¥æ€§åœ°ç»„åˆæˆ–åˆå¹¶å±‚ï¼Œæ¥å‹ç¼©æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†è¿™äº›LLMsçš„æœ€ä¼˜å®šåˆ¶è§†ä¸ºä¸€ä¸ªé›¶é˜¶ä¼˜åŒ–é—®é¢˜ï¼Œé‡‡ç”¨æ”¯æŒå±‚ç§»é™¤ã€å±‚é€‰æ‹©å’Œå±‚åˆå¹¶çš„æœç´¢ç©ºé—´ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‹ç¼©æ¨¡å‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚ï¼Œé’ˆå¯¹Llama2-13Bæ¨¡å‹ç³»åˆ—ï¼Œæˆ‘ä»¬çš„å‹ç¼©æ¨¡å‹åœ¨ç§»é™¤çº¦25%å‚æ•°çš„åŒæ—¶ï¼Œä¿æŒäº†çº¦97.3%çš„åŸå§‹æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨éƒ¨ç½²æ—¶é¢ä¸´çš„è®¡ç®—æˆæœ¬é«˜å’Œæ¨¡å‹è§„æ¨¡åºå¤§çš„é—®é¢˜ã€‚ç°æœ‰çš„å‰ªææ–¹æ³•å¤§å¤šé›†ä¸­äºå•ä¸€æ¨¡å‹ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æˆ˜ç•¥æ€§åœ°ç»„åˆå’Œåˆå¹¶ä¸åŒå¾®è°ƒæ¨¡å‹çš„å±‚ï¼Œå½¢æˆä¸€ä¸ªæ–°çš„å‹ç¼©æ¨¡å‹ï¼Œä»è€Œä¿ç•™åŸå§‹æ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘æ¨¡å‹å‚æ•°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå±‚ç§»é™¤ã€å±‚é€‰æ‹©å’Œå±‚åˆå¹¶ã€‚é¦–å…ˆï¼Œè¯†åˆ«éœ€è¦ç§»é™¤çš„å±‚ï¼›å…¶æ¬¡ï¼Œä»ä¸åŒå€™é€‰æ¨¡å‹ä¸­é€‰æ‹©åˆé€‚çš„å±‚ï¼›æœ€åï¼Œå°†é€‰å®šçš„å±‚è¿›è¡Œåˆå¹¶ï¼Œå½¢æˆæ–°çš„æ¨¡å‹ç»“æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†æ¨¡å‹å‹ç¼©é—®é¢˜è§†ä¸ºä¸€ä¸ªé›¶é˜¶ä¼˜åŒ–é—®é¢˜ï¼Œå…è®¸åœ¨ä¸€ä¸ªç»Ÿä¸€çš„æœç´¢ç©ºé—´ä¸­è¿›è¡Œå¤šç§æ“ä½œã€‚è¿™ç§æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹å‰ªææ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°åˆ©ç”¨å¤šä¸ªæ¨¡å‹çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæœ¬æ–‡è®¾è®¡äº†é€‚åº”ä¸åŒæ¨¡å‹ç‰¹æ€§çš„æŸå¤±å‡½æ•°ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šé‡‡ç”¨äº†çµæ´»çš„å±‚ç»„åˆç­–ç•¥ï¼Œä»¥ç¡®ä¿å‹ç¼©åçš„æ¨¡å‹èƒ½å¤Ÿä¿æŒé«˜æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé’ˆå¯¹Llama2-13Bæ¨¡å‹ç³»åˆ—ï¼Œæ‰€æå‹ç¼©æ¨¡å‹åœ¨ç§»é™¤çº¦25%å‚æ•°çš„æƒ…å†µä¸‹ï¼Œä»èƒ½ä¿æŒçº¦97.3%çš„åŸå§‹æ€§èƒ½ã€‚è¿™ä¸€ç»“æœæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ¨¡å‹å‹ç¼©é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½æ¨¡å‹çš„è®¡ç®—èµ„æºéœ€æ±‚ï¼Œæå‡éƒ¨ç½²æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ›´å¹¿æ³›çš„å•†ä¸šåŒ–å’Œå®ç”¨åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in deployment and inference. While structured pruning of model parameters offers a promising way to reduce computational costs at deployment time, current methods primarily focus on single model pruning. In this work, we develop a novel strategy to compress models by strategically combining or merging layers from finetuned model variants, which preserves the original model's abilities by aggregating capabilities accentuated in different finetunes. We pose the optimal tailoring of these LLMs as a zero-order optimization problem, adopting a search space that supports three different operations: (1) Layer removal, (2) Layer selection from different candidate models, and (3) Layer merging. Our experiments demonstrate that this approach leads to competitive model pruning, for example, for the Llama2-13B model families, our compressed models maintain approximately 97.3\% of the original performance while removing $\sim25\%$ of parameters, significantly outperforming previous state-of-the-art methods. The code is available at https://github.com/Guinan-Su/auto-merge-llm.

