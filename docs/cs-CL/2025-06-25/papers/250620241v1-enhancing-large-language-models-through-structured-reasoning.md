---
layout: default
title: Enhancing Large Language Models through Structured Reasoning
---

# Enhancing Large Language Models through Structured Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20241" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20241v1</a>
  <a href="https://arxiv.org/pdf/2506.20241.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20241v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20241v1', 'Enhancing Large Language Models through Structured Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yubo Dong, Hehe Fan

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

**å¤‡æ³¨**: Preprint. Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡ç»“æ„åŒ–æ¨ç†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç»“æ„åŒ–æ¨ç†` `ç›‘ç£å¾®è°ƒ` `ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–` `æœ€å¤§æµç®—æ³•` `æœ€é•¿å…¬å…±å­åºåˆ—` `è‡ªç„¶è¯­è¨€å¤„ç†` `è‡ªåŠ¨å†³ç­–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºç¼ºä¹ç»“æ„åŒ–çŸ¥è¯†è¡¨ç¤ºï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›å—é™ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ˜¾å¼æ³¨é‡Šæ¨ç†æ­¥éª¤å°†éç»“æ„åŒ–æ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–æ ¼å¼ï¼Œå¹¶åˆ©ç”¨ç›‘ç£å¾®è°ƒè®­ç»ƒLLMsï¼Œå¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨æ¨ç†æ•ˆæœå’Œè®¡ç®—å¤æ‚åº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†ç»“æ„åŒ–æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè‡ªåŠ¨å†³ç­–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ä»é¢ä¸´å›°éš¾ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºéšå¼ç»Ÿè®¡å…³ç³»è€Œç¼ºä¹ç»“æ„åŒ–çŸ¥è¯†è¡¨ç¤ºã€‚å—è®¤çŸ¥ç§‘å­¦å’Œç¥ç»ç¬¦å·äººå·¥æ™ºèƒ½çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ˜¾å¼ç»“æ„åŒ–æ¨ç†å¢å¼ºLLMsçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆå°†éç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸ºç»“æ„åŒ–æ ¼å¼ï¼Œé€šè¿‡æ˜¾å¼æ³¨é‡Šæ¨ç†æ­¥éª¤æ¥å®ç°ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™ä¸€ç»“æ„åŒ–æ•°æ®é›†é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒLLMsã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å¼•å…¥ä¸¤ç§åˆ›æ–°ç®—æ³•â€”â€”æœ€å¤§æµï¼ˆMAX-Flowï¼‰å’Œæœ€é•¿å…¬å…±å­åºåˆ—ï¼ˆLCSï¼‰ï¼Œä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥å¢å¼ºLLMsçš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„DeepSeek-R1-Distill-Qwen-1.5Bæ¨¡å‹åœ¨å„ç§åœºæ™¯ä¸‹è¡¨ç°å‡ºç®€æ´çš„æ¨ç†èƒ½åŠ›å’Œå¼ºå¤§çš„æ€§èƒ½ï¼ŒéªŒè¯äº†ç»“æ„åŒ–æ¨ç†åœ¨LLMsä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–éšå¼ç»Ÿè®¡å…³ç³»ï¼Œç¼ºä¹æœ‰æ•ˆçš„ç»“æ„åŒ–çŸ¥è¯†è¡¨ç¤ºï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ˜¾å¼ç»“æ„åŒ–æ¨ç†æ¥å¢å¼ºLLMsçš„èƒ½åŠ›ï¼Œå…·ä½“æ–¹æ³•æ˜¯å°†éç»“æ„åŒ–æ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–æ ¼å¼ï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒè¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç»“æ„åŒ–æ•°æ®é›†æ„å»ºã€ç›‘ç£å¾®è°ƒè®­ç»ƒå’Œç»“æ„åŒ–æ¨ç†èƒ½åŠ›å¢å¼ºå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œè¿›è¡Œæ•°æ®çš„ç»“æ„åŒ–å¤„ç†ï¼Œç„¶ååˆ©ç”¨æ„å»ºçš„ç»“æ„åŒ–æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæœ€åé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–è¿›ä¸€æ­¥æå‡æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†MAX-Flowå’ŒLCSç®—æ³•ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥å¢å¼ºæ¨¡å‹çš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›ï¼Œè¿™ä¸€æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†çš„æœ‰æ•ˆæ€§å¹¶é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨ç†æ­¥éª¤çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”ç»“æ„åŒ–æ•°æ®çš„å¤„ç†éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„DeepSeek-R1-Distill-Qwen-1.5Bæ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œæ¨ç†æ•ˆæœæ›´åŠ ç®€æ´ï¼Œä¸”åœ¨å¤šç§åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†ç»“æ„åŒ–æ¨ç†çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨å†³ç­–æ”¯æŒã€å¤æ‚é—®é¢˜æ±‚è§£ç­‰ã€‚é€šè¿‡å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ›´å¹¿æ³›çš„åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–å¤„ç†ï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œæœªæ¥å¯èƒ½å¯¹æ•™è‚²ã€åŒ»ç–—ã€é‡‘èç­‰è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent Large Language Models (LLMs) have significantly advanced natural language processing and automated decision-making. However, these models still encounter difficulties when performing complex reasoning tasks involving logical deduction and systematic planning, primarily due to their reliance on implicit statistical relationships without structured knowledge representation.Inspired by cognitive science and neurosymbolic AI, we introduce a novel approach to enhance LLMs through explicit structured reasoning. First, we convert unstructured data into structured formats by explicitly annotating reasoning steps. We then employ this structured dataset to train LLMs through Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning capabilities of LLMs using Group Relative Policy Optimization (GRPO), incorporating two innovative algorithms--MAX-Flow and Longest Common Subsequence (LCS)--which notably improve reasoning effectiveness and reduce computational complexity. Experimental results from fine-tuning a DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust performance across various scenarios, and improved compatibility with optimization techniques, validating the efficacy of structured reasoning integration in LLMs.

