---
layout: default
title: AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control
---

# AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20160" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20160v2</a>
  <a href="https://arxiv.org/pdf/2506.20160.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20160v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20160v2', 'AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruosen Li, Ziming Luo, Quan Zhang, Ruochen Li, Ben Zhou, Ali Payani, Xinya Du

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25 (æ›´æ–°: 2025-08-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAALCä»¥è§£å†³å¤§å‹æ¨ç†æ¨¡å‹æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹æ¨ç†æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€é•¿åº¦æ§åˆ¶` `å‡†ç¡®æ€§ä¼˜åŒ–` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹æ¨ç†æ¨¡å‹åœ¨ç”Ÿæˆå†—é•¿æ€ç»´é“¾æ—¶ï¼Œå¯¼è‡´é«˜å»¶è¿Ÿå’Œæˆæœ¬ï¼Œä¸”å‡†ç¡®æ€§æå‡æœ‰é™ã€‚
2. AALCé€šè¿‡å¼•å…¥åŸºäºå‡†ç¡®æ€§çš„é•¿åº¦å¥–åŠ±ï¼ŒåŠ¨æ€è°ƒæ•´è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­£ç¡®æ€§ä¸ç®€æ´æ€§ï¼Œä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAALCåœ¨ä¿æŒæˆ–æå‡å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå“åº”é•¿åº¦å‡å°‘è¶…è¿‡50%ï¼Œæœ‰æ•ˆæŠ‘åˆ¶å†—ä½™æ¨ç†æ¨¡å¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡ç”Ÿæˆå†—é•¿çš„æ€ç»´é“¾æ¥å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™ç§â€œè¿‡åº¦æ€è€ƒâ€å¯¼è‡´é«˜å»¶è¿Ÿå’Œæˆæœ¬ï¼Œè€Œå‡†ç¡®æ€§æå‡å´ä¸æ˜æ˜¾ã€‚æœ¬æ–‡æå‡ºAALCï¼Œä¸€ç§è½»é‡çº§çš„ã€åŸºäºå‡†ç¡®æ€§çš„é•¿åº¦å¥–åŠ±ï¼Œé›†æˆäºå¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒåŠ¨æ€å¹³è¡¡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­£ç¡®æ€§ä¸ç®€æ´æ€§ã€‚é€šè¿‡å°†éªŒè¯å‡†ç¡®æ€§çº³å…¥å¥–åŠ±ï¼Œå¹¶é‡‡ç”¨å¹³æ»‘çš„åŠ¨æ€è°ƒåº¦é•¿åº¦æƒ©ç½šï¼ŒAALCåœ¨ç›®æ ‡æ€§èƒ½è¾¾åˆ°ä¹‹å‰å»¶è¿Ÿé•¿åº¦æƒ©ç½šã€‚é€šè¿‡åœ¨æ ‡å‡†å’Œåˆ†å¸ƒå¤–æ•°å­¦åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ä¿æŒæˆ–æå‡åŸå§‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå“åº”é•¿åº¦å‡å°‘è¶…è¿‡50%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹åœ¨ç”Ÿæˆå†—é•¿æ€ç»´é“¾æ—¶çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®æ€§æå‡ä¸å“åº”æ—¶é—´ä¹‹é—´å­˜åœ¨çŸ›ç›¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAALCé€šè¿‡å¼•å…¥åŠ¨æ€é•¿åº¦å¥–åŠ±æœºåˆ¶ï¼Œç»“åˆéªŒè¯å‡†ç¡®æ€§ï¼Œä¼˜åŒ–æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨ç†æ•ˆç‡ï¼Œå‡å°‘å†—ä½™æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAALCçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¥–åŠ±æœºåˆ¶ã€é•¿åº¦æƒ©ç½šè°ƒåº¦å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ï¼ŒåŠ¨æ€è°ƒæ•´æ¨¡å‹çš„è¾“å‡ºé•¿åº¦ä¸å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šAALCçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†é•¿åº¦æƒ©ç½šå»¶è¿Ÿåˆ°ç›®æ ‡æ€§èƒ½è¾¾åˆ°åï¼Œé¿å…äº†ç®€å•çš„è¾“å‡ºæˆªæ–­ï¼Œä¿ƒè¿›äº†æ›´é«˜æ•ˆçš„æ¨ç†è·¯å¾„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒAALCé‡‡ç”¨å¹³æ»‘çš„åŠ¨æ€è°ƒåº¦é•¿åº¦æƒ©ç½šï¼Œå¹¶å°†éªŒè¯å‡†ç¡®æ€§ä½œä¸ºå¥–åŠ±çš„ä¸€éƒ¨åˆ†ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå¹³è¡¡å‡†ç¡®æ€§ä¸ç®€æ´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAALCåœ¨æ ‡å‡†å’Œåˆ†å¸ƒå¤–æ•°å­¦åŸºå‡†ä¸Šï¼Œå“åº”é•¿åº¦å‡å°‘è¶…è¿‡50%ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡äº†åŸå§‹å‡†ç¡®æ€§ã€‚è¿™è¡¨æ˜AALCåœ¨æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å®ç°äº†æœ‰æ•ˆçš„å¹³è¡¡ï¼Œå…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–æ¨ç†ç­‰ã€‚é€šè¿‡æé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„æ•ˆç‡ï¼ŒAALCèƒ½å¤Ÿåœ¨å®æ—¶åº”ç”¨ä¸­æ˜¾è‘—é™ä½å“åº”æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒé«˜å‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large reasoning models (LRMs) achieve impressive reasoning capabilities by generating lengthy chain-of-thoughts, but this "overthinking" incurs high latency and cost without commensurate accuracy gains. In this work, we introduce AALC, a lightweight, accuracy-aware length reward integrated into reinforcement learning that dynamically balances correctness and brevity during training. By incorporating validation accuracy into the reward and employing a smooth, dynamically scheduled length penalty, AALC delays length penalty until target performance is met. Through extensive experiments across standard and out-of-distribution math benchmarks, we show that our approach reduces response length by over 50% while maintaining or even improving the original accuracy. Furthermore, qualitative analysis reveals that our method curbs redundant reasoning patterns such as excessive subgoal setting and verification, leading to structurally refined outputs rather than naive truncation. We also identify that efficiency gains are accompanied by reduced interpretability: models trained with AALC omit some narrative framing and explanatory context. These findings highlight the potential of reward-based strategies to guide LRMs toward more efficient, generalizable reasoning paths.

