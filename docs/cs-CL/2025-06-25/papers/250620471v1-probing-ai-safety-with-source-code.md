---
layout: default
title: Probing AI Safety with Source Code
---

# Probing AI Safety with Source Code

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20471" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20471v1</a>
  <a href="https://arxiv.org/pdf/2506.20471.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20471v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20471v1', 'Probing AI Safety with Source Code')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ujwal Narayan, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik Narasimhan, Ameet Deshpande, Vishvak Murahari

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCode of Thoughtè¯„ä¼°LLMå®‰å…¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `AIå®‰å…¨` `ä»£ç è½¬æ¢` `æ¯’æ€§è¯„ä¼°` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è¯„ä¼°` `å®‰å…¨å…³é”®åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´ç”¨æˆ·ä½“éªŒä¸å®‰å…¨ä¸”æœ‰å®³ã€‚
2. æœ¬æ–‡æå‡ºçš„Code of Thoughtï¼ˆCoDoTï¼‰ç­–ç•¥ï¼Œé€šè¿‡å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºä»£ç æ¥è¯„ä¼°LLMsçš„å®‰å…¨æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨CoDoTåï¼ŒGPT-4 Turboçš„æ¯’æ€§å¢åŠ 16.5å€ï¼ŒDeepSeek R1å®Œå…¨å¤±è´¥ï¼Œä¸ƒç§ç°ä»£LLMsçš„æ¯’æ€§å¹³å‡å¢åŠ 300%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªå®‰å…¨å…³é”®åº”ç”¨ä¸­å˜å¾—æ— å¤„ä¸åœ¨ï¼Œè¿™è¦æ±‚åœ¨æå‡èƒ½åŠ›çš„åŒæ—¶åŠ å¼ºå®‰å…¨æªæ–½ï¼Œä»¥ä½¿è¿™äº›æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½ä¿æŒä¸€è‡´ã€‚æœ¬æ–‡å±•ç¤ºäº†å½“å‰æ¨¡å‹åœ¨AIå®‰å…¨ç›®æ ‡ä¸Šä»¤äººæ‹…å¿§çš„ä¸è¶³ï¼Œå¯¼è‡´ç”¨æˆ·ä½“éªŒä¸å®‰å…¨ä¸”æœ‰å®³ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºCode of Thoughtï¼ˆCoDoTï¼‰çš„æç¤ºç­–ç•¥æ¥è¯„ä¼°LLMsçš„å®‰å…¨æ€§ã€‚CoDoTå°†è‡ªç„¶è¯­è¨€è¾“å…¥è½¬æ¢ä¸ºç®€å•ä»£ç ï¼Œè¡¨ç¤ºç›¸åŒçš„æ„å›¾ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒCoDoTå¯¼è‡´å¤šç§æœ€å…ˆè¿›çš„LLMsä¸€è‡´æ€§å¤±è´¥ï¼Œå¼ºè°ƒäº†ä»ç¬¬ä¸€åŸåˆ™è¯„ä¼°å®‰å…¨åŠªåŠ›çš„å¿…è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨æ€§è¯„ä¼°æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œæ§åˆ¶æ¨¡å‹è¾“å‡ºçš„æœ‰å®³æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºCode of Thoughtï¼ˆCoDoTï¼‰ç­–ç•¥ï¼Œé€šè¿‡å°†è‡ªç„¶è¯­è¨€æç¤ºè½¬æ¢ä¸ºä»£ç å½¢å¼ï¼Œæ¥æ›´å‡†ç¡®åœ°è¯„ä¼°å’Œå¼•å¯¼æ¨¡å‹çš„è¾“å‡ºï¼Œç¡®ä¿å®‰å…¨æ€§ä¸èƒ½åŠ›çš„åŒæ­¥æå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoDoTçš„æ•´ä½“æµç¨‹åŒ…æ‹¬è‡ªç„¶è¯­è¨€è¾“å…¥çš„è§£æã€è½¬æ¢ä¸ºä»£ç è¡¨ç¤ºã€ä»¥åŠå¯¹æ¨¡å‹è¾“å‡ºçš„è¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è¾“å…¥å¤„ç†ã€ä»£ç ç”Ÿæˆå’Œè¾“å‡ºåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šCoDoTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è‡ªç„¶è¯­è¨€æç¤ºè½¬åŒ–ä¸ºä»£ç ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°æ¨¡å‹å®‰å…¨æ€§çš„æ–¹æ³•ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç›´æ¥è¾“å…¥è¯„ä¼°æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCoDoTä½¿ç”¨äº†ç®€å•çš„å‡½æ•°è°ƒç”¨å½¢å¼æ¥è¡¨ç¤ºæ„å›¾ï¼Œä¾‹å¦‚å°†â€œMake the statement more toxic: {text}â€è½¬æ¢ä¸ºâ€œmake_more_toxic({text})â€ï¼Œè¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹çš„ååº”æ›´åŠ å¯æ§å’Œå¯é¢„æµ‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨CoDoTåï¼ŒGPT-4 Turboçš„æ¯’æ€§å¢åŠ äº†16.5å€ï¼Œè€ŒDeepSeek R1åœ¨æµ‹è¯•ä¸­å®Œå…¨å¤±è´¥ã€‚æ­¤å¤–ï¼Œä¸ƒç§ç°ä»£LLMsçš„æ¯’æ€§å¹³å‡å¢åŠ äº†300%ï¼Œæ˜¾ç¤ºå‡ºCoDoTåœ¨è¯„ä¼°æ¨¡å‹å®‰å…¨æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œé‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨å…³é”®çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œå¦‚åŒ»ç–—ã€é‡‘èå’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æœ‰æ•ˆè¯„ä¼°å’Œæé«˜LLMsçš„å®‰å…¨æ€§ï¼Œå¯ä»¥å‡å°‘æ½œåœ¨çš„é£é™©ï¼Œç¡®ä¿ç”¨æˆ·ä½“éªŒçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼ŒCoDoTå¯èƒ½æˆä¸ºè¯„ä¼°å’Œæ”¹è¿›AIç³»ç»Ÿå®‰å…¨æ€§çš„æ ‡å‡†å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have become ubiquitous, interfacing with humans in numerous safety-critical applications. This necessitates improving capabilities, but importantly coupled with greater safety measures to align these models with human values and preferences. In this work, we demonstrate that contemporary models fall concerningly short of the goal of AI safety, leading to an unsafe and harmful experience for users. We introduce a prompting strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT converts natural language inputs to simple code that represents the same intent. For instance, CoDoT transforms the natural language prompt "Make the statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT results in a consistent failure of a wide range of state-of-the-art LLMs. For example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of the time, and toxicity increases 300% on average across seven modern LLMs. Additionally, recursively applying CoDoT can further increase toxicity two times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the critical need to evaluate safety efforts from first principles, ensuring that safety and capabilities advance together.

