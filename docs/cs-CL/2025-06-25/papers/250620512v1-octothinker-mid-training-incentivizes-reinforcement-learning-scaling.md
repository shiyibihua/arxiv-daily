---
layout: default
title: OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling
---

# OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20512" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20512v1</a>
  <a href="https://arxiv.org/pdf/2506.20512.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20512v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20512v1', 'OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zengzhi Wang, Fan Zhou, Xuefeng Li, Pengfei Liu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

**å¤‡æ³¨**: 26 pages; The first three authors contribute to this work equally

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOctoThinkerä»¥æå‡å¼ºåŒ–å­¦ä¹ æ¨¡å‹çš„å¯æ‰©å±•æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `åŸºç¡€è¯­è¨€æ¨¡å‹` `ä¸­æœŸè®­ç»ƒ` `æ¨ç†èƒ½åŠ›` `æ•°å­¦è¯­æ–™åº“` `é•¿é“¾æ¨ç†` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºç¡€è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­è¡¨ç°ä¸ä¸€ï¼Œå°¤å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼ŒäºŸéœ€æ·±å…¥ç†è§£å…¶é€‚ç”¨æ€§ã€‚
2. æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„ä¸­æœŸè®­ç»ƒç­–ç•¥ï¼Œå…ˆç”¨æ’å®šå­¦ä¹ ç‡è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå†åœ¨å¤šä¸ªé“¾æ¨ç†åˆ†æ”¯ä¸Šè¿›è¡Œå­¦ä¹ ç‡è¡°å‡è®­ç»ƒã€‚
3. é€šè¿‡å®éªŒéªŒè¯ï¼ŒOctoThinkeræ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç¼©å°äº†ä¸æ›´å…·å¼ºåŒ–å­¦ä¹ å‹å¥½çš„æ¨¡å‹å®¶æ—çš„æ€§èƒ½å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸åŒåŸºç¡€è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­è¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šã€‚æœ¬æ–‡æ¢è®¨äº†ä¸­æœŸè®­ç»ƒç­–ç•¥å¦‚ä½•å½±å“å¼ºåŒ–å­¦ä¹ åŠ¨æ€ï¼Œé‡ç‚¹ç ”ç©¶äº†Qwenå’ŒLlamaä¸¤ç§æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œé«˜è´¨é‡çš„æ•°å­¦è¯­æ–™åº“æ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ï¼Œè€Œç°æœ‰æ›¿ä»£å“æ•ˆæœä¸ä½³ï¼›æ·»åŠ QAé£æ ¼æ•°æ®ï¼Œå°¤å…¶æ˜¯é•¿é“¾æ¨ç†ç¤ºä¾‹ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å¼ºåŒ–å­¦ä¹ ç»“æœï¼›é•¿é“¾æ¨ç†è™½ç„¶æå‡äº†æ¨ç†æ·±åº¦ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´æ¨¡å‹å“åº”å†—é•¿å’Œè®­ç»ƒä¸ç¨³å®šï¼Œå¼ºè°ƒäº†æ•°æ®æ ¼å¼çš„é‡è¦æ€§ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæå‡ºäº†ç¨³å®š-å†è¡°å‡çš„ä¸¤é˜¶æ®µä¸­æœŸè®­ç»ƒç­–ç•¥ï¼Œæ„å»ºäº†OctoThinkeræ¨¡å‹ç³»åˆ—ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å¼ºåŒ–å­¦ä¹ å…¼å®¹æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºç¡€è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­è¡¨ç°ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæå‡æ¨ç†å¯†é›†å‹ä»»åŠ¡çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥é«˜è´¨é‡æ•°å­¦è¯­æ–™å’ŒQAé£æ ¼æ•°æ®ï¼Œå°¤å…¶æ˜¯é•¿é“¾æ¨ç†ç¤ºä¾‹ï¼Œæ¥ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹ï¼Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤é˜¶æ®µçš„ä¸­æœŸè®­ç»ƒï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨200Bæ ‡è®°çš„æ’å®šå­¦ä¹ ç‡è¿›è¡ŒåŸºç¡€æ¨¡å‹è®­ç»ƒï¼Œç¬¬äºŒé˜¶æ®µåœ¨ä¸‰ä¸ªé“¾æ¨ç†é‡ç‚¹åˆ†æ”¯ä¸Šä½¿ç”¨20Bæ ‡è®°è¿›è¡Œå­¦ä¹ ç‡è¡°å‡è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºçš„Stable-then-Decayç­–ç•¥æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå‡æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨é«˜è´¨é‡çš„æ•°å­¦è¯­æ–™åº“MegaMath-Web-Proï¼Œå¹¶ç‰¹åˆ«å…³æ³¨æ•°æ®æ ¼å¼ï¼Œä»¥é¿å…é•¿é“¾æ¨ç†å¯¼è‡´çš„å†—é•¿å’Œä¸ç¨³å®šæ€§ã€‚æ¨¡å‹è®¾è®¡ä¸­åŒ…å«äº†å¤šæ¡é“¾æ¨ç†åˆ†æ”¯ï¼Œä»¥å¢å¼ºæ¨ç†æ·±åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOctoThinkeræ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†æ·±åº¦å’Œç¨³å®šæ€§æ–¹é¢ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°æœªçŸ¥ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å¼ºåŒ–å­¦ä¹ å…¼å®¹æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç§‘å­¦è®¡ç®—å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿä¸ºæ¨ç†å¯†é›†å‹ä»»åŠ¡æä¾›æ›´å¼ºå¤§çš„æ”¯æŒã€‚éšç€å¼ºåŒ–å­¦ä¹ æŠ€æœ¯çš„å‘å±•ï¼ŒOctoThinkeræ¨¡å‹æœ‰æœ›åœ¨å¤šç§å®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨åŸºç¡€æ¨¡å‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max).

