---
layout: default
title: "Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning"
---

# Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.15062" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.15062v3</a>
  <a href="https://arxiv.org/pdf/2505.15062.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.15062v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.15062v3', 'Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jiashu He, Jinxuan Fan, Bowen Jiang, Ignacio Houine, Dan Roth, Alejandro Ribeiro

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-21 (Êõ¥Êñ∞: 2025-10-03)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Self-GIVE‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜ‰∏≠ÁöÑÂÖ≥ËÅîÊÄùÁª¥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÂÖ≥ËÅîÊÄùÁª¥` `Âº∫ÂåñÂ≠¶‰π†` `Áü•ËØÜÂõæË∞±` `ÁîüÁâ©ÂåªÂ≠¶ÈóÆÁ≠î` `Êé®ÁêÜËÉΩÂäõ` `‰ø°ÊÅØÊèêÂèñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÁßëÂ≠¶ÈóÆÈ¢òÊó∂ÔºåÂ∏∏Â∏∏Èù¢‰∏¥Áü•ËØÜÊ£ÄÁ¥¢‰∏çË∂≥ÂíåÊé®ÁêÜÊïàÁéá‰Ωé‰∏ãÁöÑÊåëÊàò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Self-GIVEÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂÆûÁé∞Ëá™Âä®ÂåñÁöÑÂÖ≥ËÅîÊÄùÁª¥ÔºåÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSelf-GIVEÊòæËëóÊèêÈ´ò‰∫ÜQwen2.5Ê®°ÂûãÂú®ÁîüÁâ©ÂåªÂ≠¶ÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÂáèÂ∞ë‰∫Ütoken‰ΩøÁî®„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®Èù¢ÂØπÂ§çÊùÇÈóÆÈ¢òÊó∂Ôºå‰∫∫Á±ªÂ∏∏Â∏∏ÈÄöËøáÂ∞ÜÈóÆÈ¢ò‰∏éÂ∑≤ÊúâÁü•ËØÜÂÖ≥ËÅîÊù•Êé®ÂØºÂêàÁêÜÁ≠îÊ°à„ÄÇÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÁßëÂ≠¶ÈóÆÈ¢òÊó∂‰πüÈúÄË¶ÅËøôÁßçÂÖ≥ËÅîÊÄùÁª¥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê£ÄÁ¥¢Áü•ËØÜ‰∏çË∂≥‰ª•Áõ¥Êé•ÂõûÁ≠îÈóÆÈ¢òÊó∂„ÄÇGraph Inspired Veracity Extrapolation (GIVE)ÈÄöËøáÁü•ËØÜÂõæË∞±Êù•Êé®Êñ≠ÁªìÊûÑÂåñÁü•ËØÜÔºå‰ΩÜÂÖ∂ÊïàÁéáÂíåÈÄöÁî®ÊÄßÂèóÂà∞ÈôêÂà∂„ÄÇÊú¨ÊñáÊèêÂá∫Self-GIVEÔºå‰∏Ä‰∏™Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáËá™Âä®ÂåñÂÖ≥ËÅîÊÄùÁª¥Êù•Â¢ûÂº∫LLMs„ÄÇSelf-GIVEÊèêÂèñÁªìÊûÑÂåñ‰ø°ÊÅØÂíåÂÆû‰ΩìÈõÜÔºåÂ∏ÆÂä©Ê®°Âûã‰∏éÊü•ËØ¢Ê¶ÇÂøµÂª∫Á´ãËÅîÁ≥ª„ÄÇÁªèËøáÂú®135ËäÇÁÇπÁöÑUMLSÁü•ËØÜÂõæË∞±‰∏äÂæÆË∞ÉÔºåSelf-GIVEÊòæËëóÊèêÂçá‰∫ÜQwen2.5 3BÂíå7BÊ®°ÂûãÂú®ÁîüÁâ©ÂåªÂ≠¶ÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÁâπÂà´ÊòØ7BÊ®°ÂûãÂú®Êüê‰∫õ‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜGPT3.5 turboÔºåÂêåÊó∂ÂáèÂ∞ë‰∫Ü90%‰ª•‰∏äÁöÑtoken‰ΩøÁî®„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÁßëÂ≠¶Êé®ÁêÜ‰∏≠Áº∫‰πèÊúâÊïàÂÖ≥ËÅîÊÄùÁª¥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑGIVEÊñπÊ≥ïÂú®Áü•ËØÜÊé®Êñ≠Êó∂ÈúÄË¶ÅÊûÑÂª∫Âíå‰øÆÂâ™Â§ßÈáèÂÅáËÆæ‰∏âÂÖÉÁªÑÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ãÂíåÈÄöÁî®ÊÄß‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSelf-GIVEÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåËá™Âä®ÊèêÂèñÁªìÊûÑÂåñ‰ø°ÊÅØÂíåÂÆû‰ΩìÈõÜÔºåÂ∏ÆÂä©Ê®°Âûã‰∏éÊü•ËØ¢Ê¶ÇÂøµÂª∫Á´ãÊõ¥ÊúâÊïàÁöÑËÅîÁ≥ªÔºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÊïàÁöÑÊé®ÁêÜ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSelf-GIVEÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰ø°ÊÅØÊèêÂèñÊ®°Âùó„ÄÅÂÖ≥ËÅîÊÄùÁª¥Ê®°ÂùóÂíåÊé®ÁêÜÊ®°Âùó„ÄÇ‰ø°ÊÅØÊèêÂèñÊ®°ÂùóË¥üË¥£‰ªéÁü•ËØÜÂõæË∞±‰∏≠Ëé∑ÂèñÁõ∏ÂÖ≥‰ø°ÊÅØÔºåÂÖ≥ËÅîÊÄùÁª¥Ê®°ÂùóÈÄöËøáÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ãÔºåÊé®ÁêÜÊ®°ÂùóÂàôÁîüÊàêÊúÄÁªàÁ≠îÊ°à„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSelf-GIVEÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜËá™Âä®ÂåñÁöÑÂÖ≥ËÅîÊÄùÁª¥ÂºïÂÖ•Âà∞Â§ßËØ≠Ë®ÄÊ®°Âûã‰∏≠ÔºåÂÖãÊúç‰∫ÜGIVEÊñπÊ≥ïÂú®Áü•ËØÜÊé®Êñ≠Êó∂ÁöÑÈ´òÂºÄÈîÄÂíåÂ§çÊùÇÊÄßÔºå‰ΩøÂæóÂ∞èÂûãÊ®°Âûã‰πüËÉΩÊúâÊïàÂ∫îÁî®„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåSelf-GIVE‰ΩøÁî®‰∫Ü135ËäÇÁÇπÁöÑUMLSÁü•ËØÜÂõæË∞±ÔºåÂπ∂ÈÄöËøáÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÁ°Æ‰øùÂú®ÂáèÂ∞ëtoken‰ΩøÁî®ÁöÑÂêåÊó∂ÊèêÂçáÊé®ÁêÜÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSelf-GIVEÂú®Qwen2.5 3BÂíå7BÊ®°Âûã‰∏äÂàÜÂà´ÊèêÂçá‰∫Ü28.5%Ëá≥71.4%Âíå78.6%Ëá≥90.5%ÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØ7BÊ®°ÂûãÂú®Êüê‰∫õ‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜGPT3.5 turboÔºåÂêåÊó∂token‰ΩøÁî®ÂáèÂ∞ëË∂ÖËøá90%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Self-GIVEÁöÑÁ†îÁ©∂ÊàêÊûúÂú®ÁîüÁâ©ÂåªÂ≠¶ÈóÆÁ≠î„ÄÅÁßëÂ≠¶Á†îÁ©∂ÂíåÁü•ËØÜÊ£ÄÁ¥¢Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂ∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÊõ¥ÊúâÊïàÂú∞Ëé∑ÂèñÂíåÂà©Áî®Áü•ËØÜÔºåÊé®Âä®ÁßëÂ≠¶ÂèëÁé∞ÂíåÊäÄÊúØËøõÊ≠•„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> When addressing complex questions that require new information, people often associate the question with existing knowledge to derive a sensible answer. For instance, when evaluating whether melatonin aids insomnia, one might associate "hormones helping mental disorders" with "melatonin being a hormone and insomnia a mental disorder" to complete the reasoning. Large Language Models (LLMs) also require such associative thinking, particularly in resolving scientific inquiries when retrieved knowledge is insufficient and does not directly answer the question. Graph Inspired Veracity Extrapolation (GIVE) addresses this by using a knowledge graph (KG) to extrapolate structured knowledge. However, it involves the construction and pruning of many hypothetical triplets, which limits efficiency and generalizability. We propose Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic associative thinking through reinforcement learning. Self-GIVE extracts structured information and entity sets to assist the model in linking to the queried concepts. We address GIVE's key limitations: (1) extensive LLM calls and token overhead for knowledge extrapolation, (2) difficulty in deploying on smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B models by up to $\textbf{28.5%$\rightarrow$71.4% }$ and $\textbf{78.6$\rightarrow$90.5% }$ in samples $\textbf{unseen}$ in challenging biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90%. Self-GIVE enhances the scalable integration of structured retrieval and reasoning with associative thinking.

