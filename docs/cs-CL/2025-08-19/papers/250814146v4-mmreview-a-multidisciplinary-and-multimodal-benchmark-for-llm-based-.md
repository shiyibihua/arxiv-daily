---
layout: default
title: MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation
---

# MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14146" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14146v4</a>
  <a href="https://arxiv.org/pdf/2508.14146.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14146v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14146v4', 'MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xian Gao, Jiacheng Ruan, Zongyun Zhang, Jingsheng Gao, Ting Liu, Yuzhuo Fu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-10-08)

**å¤‡æ³¨**: Work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMReviewä»¥è§£å†³å­¦æœ¯åŒè¡Œè¯„å®¡è‡ªåŠ¨åŒ–çš„è¯„ä¼°æ ‡å‡†ç¼ºå¤±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒè¡Œè¯„å®¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å†…å®¹` `è¯„ä¼°åŸºå‡†` `è‡ªåŠ¨åŒ–ç³»ç»Ÿ` `è·¨å­¦ç§‘ç ”ç©¶` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰LLMåœ¨åŒè¡Œè¯„å®¡ä¸­çš„åº”ç”¨ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ ‡å‡†ï¼Œéš¾ä»¥å…¨é¢è¯„ä¼°å…¶ç”Ÿæˆè¯„å®¡çš„èƒ½åŠ›ã€‚
2. MMReviewé€šè¿‡è®¾è®¡å¤šæ¨¡æ€å†…å®¹å’Œä¸“å®¶è¯„è®ºï¼Œæä¾›äº†ä¸€ä¸ªè·¨å­¦ç§‘çš„ç»¼åˆè¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–å¤šç§è¯„å®¡ä»»åŠ¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMMReviewåœ¨è¯„ä¼°LLMså’ŒMLLMsçš„æ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œæ¨åŠ¨äº†è‡ªåŠ¨åŒ–è¯„å®¡ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å­¦æœ¯å‡ºç‰ˆç‰©çš„å¿«é€Ÿå¢é•¿ï¼ŒåŒè¡Œè¯„å®¡å·²æˆä¸ºç ”ç©¶ç¤¾åŒºä¸­ä¸€é¡¹é‡è¦ä½†è€—æ—¶çš„è´£ä»»ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºç”Ÿæˆè¯„å®¡è¯„è®ºï¼Œä½†å½“å‰çš„LLMè¯„å®¡ä»»åŠ¡ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°åŸºå‡†ï¼Œæ— æ³•ä¸¥æ ¼è¯„ä¼°æ¨¡å‹åœ¨ç”Ÿæˆå…¨é¢ã€å‡†ç¡®ä¸”ç¬¦åˆäººç±»åå¥½çš„è¯„ä¼°æ–¹é¢çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå›¾è¡¨ç­‰å¤šæ¨¡æ€å†…å®¹çš„åœºæ™¯ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MMReviewï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¤šä¸ªå­¦ç§‘å’Œæ¨¡æ€çš„ç»¼åˆåŸºå‡†ã€‚MMReviewåŒ…å«240ç¯‡è®ºæ–‡çš„å¤šæ¨¡æ€å†…å®¹å’Œä¸“å®¶æ’°å†™çš„è¯„å®¡è¯„è®ºï¼Œæ¶µç›–äººå·¥æ™ºèƒ½ã€è‡ªç„¶ç§‘å­¦ã€å·¥ç¨‹ç§‘å­¦å’Œç¤¾ä¼šç§‘å­¦å››ä¸ªä¸»è¦å­¦ç§‘çš„17ä¸ªç ”ç©¶é¢†åŸŸã€‚æˆ‘ä»¬è®¾è®¡äº†13ä¸ªä»»åŠ¡ï¼Œåˆ†ä¸ºå››ä¸ªæ ¸å¿ƒç±»åˆ«ï¼Œæ—¨åœ¨è¯„ä¼°LLMså’Œå¤šæ¨¡æ€LLMsï¼ˆMLLMsï¼‰åœ¨é€æ­¥ç”Ÿæˆè¯„å®¡ã€ç»“æœå½¢æˆã€äººç±»åå¥½å¯¹é½å’Œå¯¹æŠ—è¾“å…¥æ“æ§çš„é²æ£’æ€§æ–¹é¢çš„è¡¨ç°ã€‚å¯¹16ä¸ªå¼€æºæ¨¡å‹å’Œ5ä¸ªå…ˆè¿›é—­æºæ¨¡å‹è¿›è¡Œçš„å¹¿æ³›å®éªŒå±•ç¤ºäº†åŸºå‡†çš„å…¨é¢æ€§ã€‚æˆ‘ä»¬å¸Œæœ›MMReviewæˆä¸ºå»ºç«‹è‡ªåŠ¨åŒ–åŒè¡Œè¯„å®¡ç³»ç»Ÿæ ‡å‡†åŒ–åŸºç¡€çš„é‡è¦ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„å…·ä½“é—®é¢˜æ˜¯ç°æœ‰LLMåœ¨åŒè¡Œè¯„å®¡ä¸­çš„åº”ç”¨ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°åŸºå‡†ï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆè¯„ä¼°å…¶ç”Ÿæˆè¯„å®¡çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼ˆå¦‚å›¾è¡¨ï¼‰æ—¶è¡¨ç°ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºMMReviewä½œä¸ºä¸€ä¸ªç»¼åˆåŸºå‡†ï¼Œæ¶µç›–å¤šå­¦ç§‘å’Œå¤šæ¨¡æ€å†…å®¹ï¼Œè®¾è®¡äº†å¤šé¡¹ä»»åŠ¡ä»¥è¯„ä¼°LLMså’ŒMLLMsçš„è¯„å®¡ç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ä¸“å®¶æ’°å†™çš„è¯„è®ºï¼Œå¢å¼ºäº†è¯„ä¼°çš„æƒå¨æ€§å’Œå®ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMMReviewçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªæ ¸å¿ƒç±»åˆ«çš„ä»»åŠ¡ï¼Œå…±13ä¸ªä»»åŠ¡ï¼Œåˆ†åˆ«è¯„ä¼°é€æ­¥ç”Ÿæˆè¯„å®¡ã€ç»“æœå½¢æˆã€äººç±»åå¥½å¯¹é½å’Œå¯¹æŠ—è¾“å…¥çš„é²æ£’æ€§ã€‚æ¯ä¸ªä»»åŠ¡éƒ½é’ˆå¯¹ç‰¹å®šçš„è¯„ä¼°ç›®æ ‡ï¼Œç¡®ä¿å…¨é¢æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šMMReviewçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è·¨å­¦ç§‘å’Œå¤šæ¨¡æ€çš„è®¾è®¡ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°åŸºå‡†çš„ç©ºç™½ï¼Œä½¿å¾—è¯„ä¼°æ›´åŠ å…¨é¢å’Œå‡†ç¡®ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMMReviewèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€è¾“å…¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMMReviewé‡‡ç”¨äº†ä¸“å®¶æ’°å†™çš„è¯„è®ºä½œä¸ºåŸºå‡†ï¼Œç¡®ä¿äº†è¯„ä¼°çš„é«˜æ ‡å‡†ã€‚åŒæ—¶ï¼Œä»»åŠ¡è®¾è®¡è€ƒè™‘äº†ä¸åŒå­¦ç§‘çš„ç‰¹ç‚¹ï¼Œç¡®ä¿äº†è¯„ä¼°çš„é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMMReviewåœ¨è¯„ä¼°LLMså’ŒMLLMsçš„æ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹16ä¸ªå¼€æºæ¨¡å‹å’Œ5ä¸ªé—­æºæ¨¡å‹çš„æµ‹è¯•ï¼ŒMMReviewèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†æ¨¡å‹åœ¨ç”Ÿæˆè¯„å®¡æ—¶çš„è¡¨ç°ï¼Œæ¨åŠ¨äº†è‡ªåŠ¨åŒ–è¯„å®¡ç³»ç»Ÿçš„æ ‡å‡†åŒ–è¿›ç¨‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å­¦æœ¯å‡ºç‰ˆã€ç§‘ç ”è¯„å®¡å’Œè‡ªåŠ¨åŒ–è¯„å®¡ç³»ç»Ÿçš„å¼€å‘ã€‚MMReviewä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°å·¥å…·ï¼Œä¿ƒè¿›äº†åŒè¡Œè¯„å®¡çš„è‡ªåŠ¨åŒ–è¿›ç¨‹ï¼Œæå‡äº†è¯„å®¡çš„æ•ˆç‡å’Œè´¨é‡ï¼Œæœªæ¥å¯èƒ½å¯¹å­¦æœ¯ç•Œäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid growth of academic publications, peer review has become an essential yet time-consuming responsibility within the research community. Large Language Models (LLMs) have increasingly been adopted to assist in the generation of review comments; however, current LLM-based review tasks lack a unified evaluation benchmark to rigorously assess the models' ability to produce comprehensive, accurate, and human-aligned assessments, particularly in scenarios involving multimodal content such as figures and tables. To address this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans multiple disciplines and modalities. MMReview includes multimodal content and expert-written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and Social Sciences. We design a total of 13 tasks grouped into four core categories, aimed at evaluating the performance of LLMs and Multimodal LLMs (MLLMs) in step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark. We envision MMReview as a critical step toward establishing a standardized foundation for the development of automated peer review systems.

