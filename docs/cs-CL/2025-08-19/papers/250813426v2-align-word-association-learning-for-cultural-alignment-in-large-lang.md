---
layout: default
title: ALIGN: Word Association Learning for Cultural Alignment in Large Language Models
---

# ALIGN: Word Association Learning for Cultural Alignment in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13426" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13426v2</a>
  <a href="https://arxiv.org/pdf/2508.13426.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13426v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13426v2', 'ALIGN: Word Association Learning for Cultural Alignment in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chunhua Liu, Kabir Manandhar Shrestha, Sukai Huang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-12-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºALIGNæ–¹æ³•ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡åŒ–åè§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡åŒ–å¯¹é½` `è¯æ±‡è”æƒ³` `å¤§å‹è¯­è¨€æ¨¡å‹` `è®¤çŸ¥å¿ƒç†å­¦` `å¾®è°ƒæ–¹æ³•` `è·¨æ–‡åŒ–äº¤æµ` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨æ–‡åŒ–åè§ï¼Œä¸”ç¼ºä¹æœ‰æ•ˆçš„æ–‡åŒ–å¯¹é½å­¦ä¹ æ–¹æ³•ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ¯è¯­è€…çš„è¯æ±‡è”æƒ³è§„èŒƒå¯¹LLMsè¿›è¡Œå¾®è°ƒï¼Œä»¥æ•æ‰æ–‡åŒ–çŸ¥è¯†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨è¯æ±‡å¯¹é½å’Œæ–‡åŒ–ä»·å€¼å¯¹é½ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ä¸ä¸­å›½ä»·å€¼è§‚çš„å¯¹é½ä¸Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®­ç»ƒæ•°æ®ä¸­è¡¨ç°å‡ºæ–‡åŒ–åè§ï¼Œä¸”ç”±äºæ–‡åŒ–çŸ¥è¯†çš„å±€é™æ€§ï¼Œæ–‡åŒ–å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜ä¸”åŸºäºè®¤çŸ¥çš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨æ¯è¯­è€…çš„è¯æ±‡è”æƒ³è§„èŒƒæ¥å¾®è°ƒLLMsï¼Œåˆ©ç”¨è®¤çŸ¥å¿ƒç†å­¦çš„å‘ç°ï¼Œè¿™äº›è”æƒ³èƒ½å¤Ÿæ•æ‰æ–‡åŒ–çŸ¥è¯†ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªç¾å›½ï¼ˆè‹±è¯­ï¼‰å’Œä¸­å›½ï¼ˆæ™®é€šè¯ï¼‰çš„è¯æ±‡è”æƒ³æ•°æ®é›†ï¼Œå¯¹Llama-3.1-8Bå’ŒQwen-2.5-7Bè¿›è¡Œç›‘ç£å¾®è°ƒå’Œåå¥½ä¼˜åŒ–ã€‚é€šè¿‡æ¶µç›–è¯æ±‡è”æƒ³å’Œæ–‡åŒ–ä»·å€¼å¯¹é½çš„åŒå±‚è¯„ä¼°æ¡†æ¶ï¼Œæˆ‘ä»¬è¯„ä¼°æ¨¡å‹çš„æ–‡åŒ–å¯¹é½ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯æ±‡å¯¹é½æ˜¾è‘—æ”¹å–„ï¼ˆè‹±è¯­æå‡16-20%ï¼Œæ™®é€šè¯æå‡43-165%ï¼‰ï¼Œå¹¶ä¸”æ–‡åŒ–ä»·å€¼å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ã€‚ç»è¿‡å¾®è°ƒçš„Qwenåœ¨ä¸ä¸­å›½ä»·å€¼è§‚çš„å“åº”å¯¹é½ä¸Šå‡ ä¹ç¿»å€ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæœªæ¥ç ”ç©¶çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–åè§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ–‡åŒ–å¯¹é½æ–¹é¢å­˜åœ¨çŸ¥è¯†ä¸è¶³å’Œå­¦ä¹ æ–¹æ³•ä¸å¤Ÿæœ‰æ•ˆçš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¾®è°ƒLLMsï¼Œåˆ©ç”¨æ¯è¯­è€…çš„è¯æ±‡è”æƒ³è§„èŒƒæ¥æ•æ‰æ–‡åŒ–çŸ¥è¯†ï¼ŒåŸºäºè®¤çŸ¥å¿ƒç†å­¦çš„ç†è®ºè®¾è®¡è¿™ä¸€æ–¹æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹å¾®è°ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼Œé¦–å…ˆæ”¶é›†æ¥è‡ªç¾å›½å’Œä¸­å›½çš„è¯æ±‡è”æƒ³æ•°æ®ï¼Œç„¶åå¯¹Llama-3.1-8Bå’ŒQwen-2.5-7Bè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œæœ€åé€šè¿‡åŒå±‚è¯„ä¼°æ¡†æ¶è¿›è¡Œæ•ˆæœè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨æ–‡åŒ–èƒŒæ™¯çš„è¯æ±‡è”æƒ³è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ–‡åŒ–å¯¹é½èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå‡å°‘äº†å¯¹æ˜‚è´µé‡è®­ç»ƒçš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ–‡åŒ–ç›¸å…³çš„è¯æ±‡è”æƒ³ï¼Œå…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„Qwenæ¨¡å‹åœ¨ä¸ä¸­å›½ä»·å€¼è§‚çš„å“åº”å¯¹é½ä¸Šå‡ ä¹ç¿»å€ï¼Œä»13æå‡è‡³25ï¼Œä¸”åœ¨è¯æ±‡å¯¹é½ä¸Šï¼Œè‹±è¯­æå‡16-20%ï¼Œæ™®é€šè¯æå‡43-165%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡å¾®è°ƒçš„7-8Bæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸70BåŸºçº¿æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ï¼Œæ˜¾ç¤ºå‡ºæ–‡åŒ–åŸºç¡€è”æƒ³çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è·¨æ–‡åŒ–äº¤æµã€å›½é™…åŒ–çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ä»¥åŠå¤šè¯­è¨€ç¿»è¯‘ç³»ç»Ÿã€‚é€šè¿‡æ”¹å–„æ¨¡å‹çš„æ–‡åŒ–å¯¹é½èƒ½åŠ›ï¼Œå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œå‡å°‘æ–‡åŒ–è¯¯è§£ï¼Œä¿ƒè¿›ä¸åŒæ–‡åŒ–ä¹‹é—´çš„ç†è§£ä¸äº¤æµã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¸ºå…¶ä»–é¢†åŸŸçš„AIæ¨¡å‹æä¾›å€Ÿé‰´ï¼Œæ¨åŠ¨æ–‡åŒ–æ•æ„Ÿæ€§çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) exhibit cultural bias from overrepresented viewpoints in training data, yet cultural alignment remains a challenge due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient and cognitively grounded method: fine-tuning LLMs on native speakers' word-association norms, leveraging cognitive psychology findings that such associations capture cultural knowledge. Using word association datasets from native speakers in the US (English) and China (Mandarin), we train Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning and preference optimization. We evaluate models' cultural alignment through a two-tier evaluation framework that spans lexical associations and cultural value alignment using the World Values Survey. Results show significant improvements in lexical alignment (16-20% English, 43-165% Mandarin on Precision@5) and high-level cultural value shifts. On a subset of 50 questions where US and Chinese respondents diverge most, fine-tuned Qwen nearly doubles its response alignment with Chinese values (13 to 25). Remarkably, our trained 7-8B models match or exceed vanilla 70B baselines, demonstrating that a few million of culture-grounded associations achieve value alignment without expensive retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.

