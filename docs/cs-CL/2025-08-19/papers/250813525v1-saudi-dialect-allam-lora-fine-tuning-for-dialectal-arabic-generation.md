---
layout: default
title: Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation
---

# Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13525" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13525v1</a>
  <a href="https://arxiv.org/pdf/2508.13525.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13525v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13525v1', 'Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hassan Barmandah

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: 7 pages, 6 figures, 2 tables. Code: https://github.com/HasanBGIt/Saudi-Dialect-ALLaM . Dataset and trained weights/adapters are not released. Primary category: cs.CL

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoRAå¾®è°ƒæ–¹æ³•ä»¥è§£å†³é˜¿æ‹‰ä¼¯æ–¹è¨€ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é˜¿æ‹‰ä¼¯è¯­ç”Ÿæˆ` `æ–¹è¨€å¤„ç†` `LoRAå¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ–‡æœ¬ä¿çœŸåº¦` `æ–¹è¨€åˆ†ç±»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é˜¿æ‹‰ä¼¯è¯­å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦ä»¥ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ä¸ºä¸»ï¼Œç¼ºä¹å¯¹æ²™ç‰¹æ–¹è¨€çš„æœ‰æ•ˆæ”¯æŒï¼Œé™åˆ¶äº†å…¶åœ¨æ–¹è¨€ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†LoRAå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨æ²™ç‰¹æ–¹è¨€æŒ‡ä»¤æ•°æ®é›†ï¼Œæ¢ç´¢äº†å¸¦æ ‡ç­¾å’Œä¸å¸¦æ ‡ç­¾çš„è®­ç»ƒæ–¹å¼ï¼Œä»¥æé«˜æ–¹è¨€ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDialect-Tokenæ¨¡å‹åœ¨æ–¹è¨€æ§åˆ¶å’Œæ–‡æœ¬ä¿çœŸåº¦ä¸Šå‡ä¼˜äºå¤šç§å¼ºå¤§çš„é€šç”¨æŒ‡ä»¤æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†æ–¹è¨€ç”Ÿæˆçš„è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é˜¿æ‹‰ä¼¯è¯­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ï¼ˆMSAï¼‰ï¼Œå¯¹æ²™ç‰¹æ–¹è¨€ï¼ˆå¦‚Najdiå’ŒHijaziï¼‰çš„æ”¯æŒæœ‰é™ï¼Œå½±å“äº†å…¶æ•æ‰çœŸå®æ–¹è¨€å˜å¼‚çš„èƒ½åŠ›ã€‚æœ¬æ–‡ä½¿ç”¨ç§æœ‰çš„æ²™ç‰¹æ–¹è¨€æŒ‡ä»¤æ•°æ®é›†ï¼ˆåŒ…å«5466å¯¹åˆæˆæŒ‡ä»¤-å“åº”å¯¹ï¼ŒæŒ‰50/50åˆ†å‰²ï¼‰ï¼Œå¯¹æ²™ç‰¹é¦–ä¸ªåŸºç¡€æ¨¡å‹ALLaM-7B-Instruct-previewè¿›è¡ŒLoRAå¾®è°ƒï¼Œä»¥å®ç°æ²™ç‰¹æ–¹è¨€ç”Ÿæˆã€‚ç ”ç©¶äº†ä¸¤ç§å˜ä½“ï¼šä¸€ç§æ˜¯åœ¨æŒ‡ä»¤å‰æ·»åŠ æ˜¾å¼æ–¹è¨€æ ‡ç­¾çš„Dialect-Tokenè®­ç»ƒï¼Œå¦ä¸€ç§åˆ™çœç•¥æ ‡ç­¾çš„No-Tokenè®­ç»ƒã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒDialect-Tokenæ¨¡å‹åœ¨æ–¹è¨€æ§åˆ¶ä¸Šè¡¨ç°æœ€ä½³ï¼Œæ²™ç‰¹æ–¹è¨€ç”Ÿæˆç‡ä»47.97%æå‡è‡³84.21%ï¼ŒåŒæ—¶MSAæ³„æ¼ç‡ä»32.63%é™è‡³6.21%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é˜¿æ‹‰ä¼¯è¯­å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ²™ç‰¹æ–¹è¨€ï¼ˆNajdiå’ŒHijaziï¼‰çš„ç”Ÿæˆèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ï¼Œå¯¼è‡´æ–¹è¨€å˜å¼‚çš„æ•æ‰èƒ½åŠ›è¾ƒå¼±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ä½¿ç”¨ç§æœ‰çš„æ²™ç‰¹æ–¹è¨€æŒ‡ä»¤æ•°æ®é›†ï¼Œé‡‡ç”¨LoRAå¾®è°ƒæŠ€æœ¯ï¼Œæ¢ç´¢åœ¨æŒ‡ä»¤ä¸­æ·»åŠ æ–¹è¨€æ ‡ç­¾çš„è®­ç»ƒæ–¹å¼ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹æ–¹è¨€çš„ç”Ÿæˆèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹å¾®è°ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é›†åŒ…å«5466å¯¹åˆæˆæŒ‡ä»¤-å“åº”å¯¹ï¼Œæ¨¡å‹å¾®è°ƒä½¿ç”¨LoRAæŠ€æœ¯ï¼Œè¯„ä¼°åˆ™ç»“åˆå¤–éƒ¨æ–¹è¨€åˆ†ç±»å™¨å’Œæ–‡æœ¬ä¿çœŸåº¦æŒ‡æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºDialect-Tokenè®­ç»ƒæ–¹å¼ï¼Œé€šè¿‡åœ¨æŒ‡ä»¤å‰æ·»åŠ æ–¹è¨€æ ‡ç­¾ï¼Œæ˜¾è‘—æé«˜äº†æ–¹è¨€ç”Ÿæˆçš„æ§åˆ¶èƒ½åŠ›å’Œå‡†ç¡®æ€§ï¼Œé¿å…äº†ç°æœ‰æ¨¡å‹å¸¸è§çš„å…ƒæ•°æ®æ ‡ç­¾å›å£°é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œä»¥ä¼˜åŒ–æ–¹è¨€ç”Ÿæˆçš„è´¨é‡ã€‚å®éªŒä¸­å¯¹æ¯”äº†ä¸åŒè®­ç»ƒæ–¹å¼çš„æ•ˆæœï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨æ–¹è¨€ç”Ÿæˆä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDialect-Tokenæ¨¡å‹åœ¨æ–¹è¨€ç”Ÿæˆæ§åˆ¶ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ²™ç‰¹æ–¹è¨€ç”Ÿæˆç‡ä»47.97%æå‡è‡³84.21%ï¼ŒMSAæ³„æ¼ç‡ä»32.63%é™è‡³6.21%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ–‡æœ¬ä¿çœŸåº¦æŒ‡æ ‡chrF++å’ŒBERTScoreä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ï¼Œåˆ†åˆ«æé«˜äº†3.53å’Œ0.059ï¼Œè¶…è¶Šäº†å¤šç§å¼ºå¤§çš„é€šç”¨æŒ‡ä»¤æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹ç”Ÿæˆã€å®¢æˆ·æœåŠ¡å¯¹è¯ç³»ç»Ÿä»¥åŠæ•™è‚²é¢†åŸŸçš„æ–¹è¨€å­¦ä¹ å·¥å…·ã€‚é€šè¿‡æå‡æ–¹è¨€ç”Ÿæˆçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³æ²™ç‰¹åœ°åŒºç”¨æˆ·çš„éœ€æ±‚ï¼Œå¢å¼ºäººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæµç•…æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šå½±å“æ›´å¹¿æ³›çš„é˜¿æ‹‰ä¼¯è¯­å¤„ç†ä»»åŠ¡ï¼Œä¿ƒè¿›æ–¹è¨€ä¸æ ‡å‡†è¯­ä¹‹é—´çš„æœ‰æ•ˆè½¬æ¢ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) for Arabic are still dominated by Modern Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi and Hijazi. This underrepresentation hinders their ability to capture authentic dialectal variation. Using a privately curated Saudi Dialect Instruction dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50 split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model developed in Saudi Arabia, for Saudi dialect generation. We investigate two variants: (i) Dialect-Token training, which prepends an explicit dialect tag to the instruction, and (ii) No-Token training, which omits the tag at formatting time. Evaluation on a held-out test set combines an external dialect classifier with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The Dialect-Token model achieves the best control, raising the Saudi rate from 47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and fidelity, while avoiding metadata-tag echoing that these baselines frequently exhibit. We do not release the dataset or any model weights/adapters; instead, we release training/evaluation/inference code and a detailed datasheet (schema and aggregate statistics) to support independent verification.

