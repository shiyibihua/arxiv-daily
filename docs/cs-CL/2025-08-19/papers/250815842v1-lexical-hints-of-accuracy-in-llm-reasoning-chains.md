---
layout: default
title: Lexical Hints of Accuracy in LLM Reasoning Chains
---

# Lexical Hints of Accuracy in LLM Reasoning Chains

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15842" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15842v1</a>
  <a href="https://arxiv.org/pdf/2508.15842.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15842v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15842v1', 'Lexical Hints of Accuracy in LLM Reasoning Chains')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Arne Vanhoyweghen, Brecht Verbeken, Andres Algaba, Vincent Ginis

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: 21 pages, 7 figures, 6 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯æ±‡æç¤ºä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†é“¾çš„å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†é“¾` `è‡ªä¿¡åº¦æ ¡å‡†` `ä¸ç¡®å®šæ€§æŒ‡æ ‡` `æƒ…æ„Ÿåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä½å‡†ç¡®ç‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸”è‡ªä¿¡åº¦æ ¡å‡†ä¸è¶³ï¼Œå¯¼è‡´é”™è¯¯é¢„æµ‹é¢‘ç¹ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡åˆ†ææ¨ç†é“¾ä¸­çš„ç‰¹å¾ï¼Œå¦‚é•¿åº¦å’Œæƒ…æ„Ÿæ³¢åŠ¨ï¼Œæ¥è¯„ä¼°LLMçš„å†…éƒ¨ä¿¡å¿ƒï¼Œä»è€Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç¡®å®šæ€§è¯æ±‡æ˜¯é”™è¯¯å“åº”çš„å¼ºæŒ‡ç¤ºï¼Œè€Œæ¨ç†é“¾é•¿åº¦åœ¨ä¸­ç­‰éš¾åº¦åŸºå‡†ä¸­æœ‰æ•ˆï¼Œæå‡äº†æ¨¡å‹çš„å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œåœ¨å›ç­”å‰ç”Ÿæˆæ˜ç¡®çš„æ¨ç†é“¾ï¼ˆCoTï¼‰ï¼Œèƒ½æ˜¾è‘—æå‡å…¶åœ¨ä»£ç ã€æ•°å­¦å’Œå¸¸è¯†åŸºå‡†æµ‹è¯•ä¸­çš„æ•´ä½“è¡¨ç°ã€‚ç„¶è€Œï¼Œåœ¨ä¸€äº›å‡†ç¡®ç‡è¾ƒä½çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¦‚äººç±»æœ€åè€ƒè¯•ï¼ˆHLEï¼‰ï¼ŒLLMså¸¸å¸¸æŠ¥å‘Šé«˜è‡ªä¿¡åº¦ï¼Œåæ˜ å‡ºå…¶æ ¡å‡†ä¸ä½³ã€‚æœ¬æ–‡æµ‹è¯•äº†æ¨ç†é“¾çš„å¯æµ‹é‡ç‰¹æ€§æ˜¯å¦èƒ½å¯é åœ°åæ˜ LLMå¯¹ç­”æ¡ˆçš„å†…éƒ¨ä¿¡å¿ƒã€‚æˆ‘ä»¬åˆ†æäº†ä¸‰ç±»ç‰¹å¾ï¼šæ¨ç†é“¾é•¿åº¦ã€æ¨ç†é“¾å†…éƒ¨æƒ…æ„Ÿæ³¢åŠ¨å’Œè¯æ±‡æç¤ºï¼ŒåŒ…æ‹¬æ¨¡ç³Šè¯ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨ç†é“¾ä¸­çš„ä¸ç¡®å®šæ€§è¯æ±‡æ˜¯é”™è¯¯å“åº”çš„å¼ºæŒ‡ç¤ºï¼Œè€Œæƒ…æ„Ÿå˜åŒ–æä¾›äº†è¾ƒå¼±ä½†äº’è¡¥çš„ä¿¡å·ã€‚æ¨ç†é“¾é•¿åº¦åœ¨ä¸­ç­‰éš¾åº¦åŸºå‡†ä¸­æœ‰é¢„æµ‹èƒ½åŠ›ï¼Œä½†åœ¨HLEä¸­åˆ™æ— æ•ˆï¼Œè¡¨æ˜æ¨ç†é“¾é•¿åº¦ä»…åœ¨æ¨¡å‹èƒ½åŠ›èŒƒå›´å†…æœ‰æ•ˆã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æ¨ç†é“¾ä¸­çš„ä¸ç¡®å®šæ€§æŒ‡æ ‡æ¯”é«˜è‡ªä¿¡åº¦æ ‡è®°æ›´æ˜¾è‘—ï¼Œä½¿å¾—é”™è¯¯é¢„æµ‹æ›´ä¸ºå®¹æ˜“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½å‡†ç¡®ç‡åŸºå‡†æµ‹è¯•ä¸­è‡ªä¿¡åº¦æ ¡å‡†ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåæ˜ æ¨¡å‹çš„çœŸå®ä¿¡å¿ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†ææ¨ç†é“¾ä¸­çš„ç‰¹å¾ï¼ˆå¦‚é•¿åº¦ã€æƒ…æ„Ÿæ³¢åŠ¨å’Œè¯æ±‡æç¤ºï¼‰ï¼Œæä¾›ä¸€ç§å¯é çš„ä¿¡å¿ƒè¯„ä¼°ä¿¡å·ï¼Œä»¥è¾…åŠ©æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç‰¹å¾æå–å’Œæ¨¡å‹è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†æ¨ç†é“¾æ•°æ®ï¼Œç„¶åæå–ç›¸å…³ç‰¹å¾ï¼Œæœ€åé€šè¿‡å®éªŒéªŒè¯ç‰¹å¾ä¸æ¨¡å‹è¡¨ç°çš„å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºè¯†åˆ«æ¨ç†é“¾ä¸­çš„ä¸ç¡®å®šæ€§è¯æ±‡ä½œä¸ºé”™è¯¯é¢„æµ‹çš„å¼ºæŒ‡ç¤ºï¼Œè¿™ä¸€å‘ç°ä¸ç°æœ‰æ–¹æ³•çš„è‡ªä¿¡åº¦è¯„ä¼°æœºåˆ¶å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç‰¹å¾æå–ä¸­ï¼Œé‡ç‚¹å…³æ³¨æ¨ç†é“¾é•¿åº¦ã€æƒ…æ„Ÿæ³¢åŠ¨å’Œæ¨¡ç³Šè¯çš„ä½¿ç”¨ï¼Œé‡‡ç”¨é€‚å½“çš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒéš¾åº¦åŸºå‡†ä¸Šçš„é€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨ç†é“¾ä¸­çš„ä¸ç¡®å®šæ€§è¯æ±‡ï¼ˆå¦‚'guess'ã€'stuck'ã€'hard'ï¼‰æ˜¯é”™è¯¯å“åº”çš„æœ€å¼ºæŒ‡ç¤ºï¼Œè€Œæ¨ç†é“¾é•¿åº¦åœ¨ä¸­ç­‰éš¾åº¦åŸºå‡†ï¼ˆOmni-MATHï¼‰ä¸­æœ‰æ•ˆï¼Œå‡†ç¡®ç‡è¾¾åˆ°çº¦70%ã€‚åœ¨æ›´éš¾çš„åŸºå‡†ï¼ˆHLEï¼‰ä¸­ï¼Œå‡†ç¡®ç‡ä»…ä¸ºçº¦9%ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¸åŒéš¾åº¦ä¸‹çš„è¡¨ç°å·®å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²è¯„ä¼°ã€è‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§å’Œä¿¡å¿ƒæ ¡å‡†ï¼Œå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­å‡å°‘é”™è¯¯å“åº”ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œä¿¡ä»»åº¦ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å®‰å…¨çš„LLMéƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é£é™©åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning Large Language Models (LLMs) with reinforcement learning to produce an explicit Chain-of-Thought (CoT) before answering produces models that consistently raise overall performance on code, math, and general-knowledge benchmarks. However, on benchmarks where LLMs currently achieve low accuracy, such as Humanity's Last Exam (HLE), they often report high self-confidence, reflecting poor calibration. Here, we test whether measurable properties of the CoT provide reliable signals of an LLM's internal confidence in its answers. We analyze three feature classes: (i) CoT length, (ii) intra-CoT sentiment volatility, and (iii) lexicographic hints, including hedging words. Using DeepSeek-R1 and Claude 3.7 Sonnet on both Humanity's Last Exam (HLE), a frontier benchmark with very low accuracy, and Omni-MATH, a saturated benchmark of moderate difficulty, we find that lexical markers of uncertainty (e.g., $\textit{guess}$, $\textit{stuck}$, $\textit{hard}$) in the CoT are the strongest indicators of an incorrect response, while shifts in the CoT sentiment provide a weaker but complementary signal. CoT length is informative only on Omni-MATH, where accuracy is already high ($\approx 70\%$), and carries no signal on the harder HLE ($\approx 9\%$), indicating that CoT length predicts correctness only in the intermediate-difficulty benchmarks, i.e., inside the model's demonstrated capability, but still below saturation. Finally, we find that uncertainty indicators in the CoT are consistently more salient than high-confidence markers, making errors easier to predict than correct responses. Our findings support a lightweight post-hoc calibration signal that complements unreliable self-reported probabilities and supports safer deployment of LLMs.

