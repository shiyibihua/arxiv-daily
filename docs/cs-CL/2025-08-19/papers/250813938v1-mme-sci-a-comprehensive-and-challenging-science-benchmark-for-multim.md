---
layout: default
title: MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models
---

# MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13938" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13938v1</a>
  <a href="https://arxiv.org/pdf/2508.13938.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13938v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13938v1', 'MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiacheng Ruan, Dan Jiang, Xian Gao, Ting Liu, Yuzhuo Fu, Yangyang Kang

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: 9 pages, 6 figures, work in progress

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/JCruan519/MME-SCI)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMME-SCIä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­çš„å…³é”®æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç§‘å­¦è¯„ä¼°` `æ¨ç†èƒ½åŠ›` `å¤šè¯­è¨€æ”¯æŒ` `çŸ¥è¯†ç‚¹æ³¨é‡Š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç§‘å­¦é¢†åŸŸçš„è¯„ä¼°åŸºå‡†åœ¨å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ã€æ¨¡æ€è¦†ç›–å’ŒçŸ¥è¯†ç‚¹æ³¨é‡Šæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºMME-SCIåŸºå‡†ï¼Œé€šè¿‡æ”¶é›†é«˜è´¨é‡é—®ç­”å¯¹ï¼Œè¦†ç›–å¤šä¸ªå­¦ç§‘å’Œè¯­è¨€ï¼Œæå‡è¯„ä¼°çš„å…¨é¢æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚
3. åœ¨å¯¹16ä¸ªå¼€æºæ¨¡å‹å’Œ4ä¸ªé—­æºæ¨¡å‹çš„å®éªŒä¸­ï¼ŒMME-SCIæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æŒ‘æˆ˜æ€§ï¼Œæ¨¡å‹åœ¨å„å­¦ç§‘çš„å‡†ç¡®ç‡æ™®éè¾ƒä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç›¸åº”çš„è¯„ä¼°åŸºå‡†ä¹Ÿåœ¨ä¸æ–­å®Œå–„ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†åœ¨ç§‘å­¦é¢†åŸŸè¯„ä¼°æ¨¡å‹æ¨ç†èƒ½åŠ›æ—¶é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š1ï¼‰å¤šè¯­è¨€åœºæ™¯ä¸‹æ¨¡å‹æ¨ç†èƒ½åŠ›è¯„ä¼°ä¸è¶³ï¼›2ï¼‰å¯¹MLLMsçš„ç»¼åˆæ¨¡æ€è¦†ç›–è¯„ä¼°ä¸å¤Ÿï¼›3ï¼‰ç§‘å­¦çŸ¥è¯†ç‚¹ç¼ºä¹ç»†ç²’åº¦æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MME-SCIï¼Œä¸€ä¸ªå…¨é¢ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼Œæ”¶é›†äº†1,019å¯¹é«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œæ¶µç›–æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©å››ä¸ªå­¦ç§‘ï¼Œå¹¶æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€æ³•æ–‡ã€è¥¿ç­ç‰™æ–‡å’Œæ—¥æ–‡äº”ç§è¯­è¨€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMME-SCIå¯¹ç°æœ‰MLLMså…·æœ‰å¹¿æ³›çš„æŒ‘æˆ˜æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°åŸºå‡†åœ¨å¤šè¯­è¨€æ¨ç†ã€æ¨¡æ€è¦†ç›–å’Œç§‘å­¦çŸ¥è¯†ç»†ç²’åº¦æ³¨é‡Šæ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒçŸ¥è¯†æŒæ¡æƒ…å†µã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMME-SCIåŸºå‡†ï¼Œé€šè¿‡ç²¾å¿ƒæ”¶é›†å¤šå­¦ç§‘çš„é«˜è´¨é‡é—®ç­”å¯¹ï¼Œæ¶µç›–å¤šç§è¯­è¨€ï¼Œæ—¨åœ¨æä¾›ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥å…¨é¢è¯„ä¼°MLLMsçš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMME-SCIåŸºå‡†åŒ…å«ä¸‰ä¸ªè¯„ä¼°æ¨¡å¼ï¼Œæ¶‰åŠæ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©å››ä¸ªå­¦ç§‘ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€æ³•æ–‡ã€è¥¿ç­ç‰™æ–‡å’Œæ—¥æ–‡äº”ç§è¯­è¨€ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€é—®ç­”å¯¹æ„å»ºå’Œæ¨¡å‹è¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šMME-SCIçš„åˆ›æ–°åœ¨äºå…¶å¤šè¯­è¨€æ”¯æŒå’Œç»†ç²’åº¦çŸ¥è¯†ç‚¹æ³¨é‡Šï¼Œä½¿å¾—è¯„ä¼°æ›´å…·æ·±åº¦å’Œå¹¿åº¦ï¼Œèƒ½å¤Ÿæ­ç¤ºæ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„å¼±ç‚¹ã€‚ä¸ç°æœ‰åŸºå‡†ç›¸æ¯”ï¼ŒMME-SCIæä¾›äº†æ›´é«˜çš„æŒ‘æˆ˜æ€§å’Œæ›´å…¨é¢çš„è¯„ä¼°è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é—®ç­”å¯¹çš„æ„å»ºä¸­ï¼Œé‡‡ç”¨äº†é«˜è´¨é‡çš„ç§‘å­¦çŸ¥è¯†ç‚¹ï¼Œå¹¶ç¡®ä¿è¦†ç›–ä¸åŒè¯­è¨€çš„è¡¨è¾¾æ–¹å¼ã€‚è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†å¤šç§æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚å®éªŒä¸­è¿˜è€ƒè™‘äº†ä¸åŒå­¦ç§‘çš„éš¾åº¦å·®å¼‚ï¼Œç¡®ä¿è¯„ä¼°çš„å…¬å¹³æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¯¹16ä¸ªå¼€æºæ¨¡å‹å’Œ4ä¸ªé—­æºæ¨¡å‹çš„å®éªŒä¸­ï¼ŒMME-SCIæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æŒ‘æˆ˜æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒä»…è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œo4-miniåœ¨æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©çš„å‡†ç¡®ç‡åˆ†åˆ«ä»…ä¸º52.11%ã€24.73%ã€36.57%å’Œ29.80%ï¼Œæ˜¾ç¤ºå‡ºç›¸æ¯”ç°æœ‰åŸºå‡†æ›´é«˜çš„éš¾åº¦æ°´å¹³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MME-SCIåŸºå‡†çš„æå‡ºä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„è¯„ä¼°å·¥å…·ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒçŸ¥è¯†æŒæ¡æƒ…å†µã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†æœ‰æœ›æ¨åŠ¨ç§‘å­¦æ•™è‚²ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, multimodal large language models (MLLMs) have achieved significant advancements across various domains, and corresponding evaluation benchmarks have been continuously refined and improved. In this process, benchmarks in the scientific domain have played an important role in assessing the reasoning capabilities of MLLMs. However, existing benchmarks still face three key challenges: 1) Insufficient evaluation of models' reasoning abilities in multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive modality coverage; 3) Lack of fine-grained annotation of scientific knowledge points. To address these gaps, we propose MME-SCI, a comprehensive and challenging benchmark. We carefully collected 1,019 high-quality question-answer pairs, which involve 3 distinct evaluation modes. These pairs cover four subjects, namely mathematics, physics, chemistry, and biology, and support five languages: Chinese, English, French, Spanish, and Japanese. We conducted extensive experiments on 16 open-source models and 4 closed-source models, and the results demonstrate that MME-SCI is widely challenging for existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics, physics, chemistry, and biology, respectively, indicating a significantly higher difficulty level compared to existing benchmarks. More importantly, using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed existing models' performance in depth and identified their weaknesses in specific domains. The Data and Evaluation Code are available at https://github.com/JCruan519/MME-SCI.

