---
layout: default
title: DPad: Efficient Diffusion Language Models with Suffix Dropout
---

# DPad: Efficient Diffusion Language Models with Suffix Dropout

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14148" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14148v2</a>
  <a href="https://arxiv.org/pdf/2508.14148.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14148v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14148v2', 'DPad: Efficient Diffusion Language Models with Suffix Dropout')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai "Helen" Li, Yiran Chen

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-08-23)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Crys-Chen/DPad)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDPadä»¥è§£å†³æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è®¡ç®—æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£è¯­è¨€æ¨¡å‹` `æ–‡æœ¬ç”Ÿæˆ` `è®¡ç®—æ•ˆç‡` `æ³¨æ„åŠ›æœºåˆ¶` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨æ¯ä¸€æ­¥éƒ½éœ€é¢„æµ‹æ‰€æœ‰åç¼€æ ‡è®°ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€é«˜ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. DPadé€šè¿‡é™åˆ¶æ³¨æ„åŠ›èŒƒå›´åˆ°é™„è¿‘åç¼€æ ‡è®°ï¼Œç»“åˆæ»‘åŠ¨çª—å£å’Œè·ç¦»è¡°å‡ä¸¢å¼ƒç­–ç•¥ï¼Œä¼˜åŒ–äº†è®¡ç®—è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDPadåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾61.4å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£åŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰é€šè¿‡å°†è§£ç è¿‡ç¨‹è§†ä¸ºå»å™ªè¿‡ç¨‹æ¥å®ç°æ–‡æœ¬ç”Ÿæˆçš„å¹¶è¡ŒåŒ–ï¼Œä½†ç”±äºåœ¨æ¯ä¸€æ­¥éƒ½é¢„æµ‹æ‰€æœ‰æœªæ¥åç¼€æ ‡è®°è€Œå¯¼è‡´é«˜è®¡ç®—å¼€é”€ã€‚æœ¬æ–‡æå‡ºäº†Diffusion Scratchpadï¼ˆDPadï¼‰ï¼Œä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œé™åˆ¶æ³¨æ„åŠ›é›†ä¸­åœ¨ä¸€å°éƒ¨åˆ†é™„è¿‘çš„åç¼€æ ‡è®°ä¸Šï¼Œæ—¢ä¿æŒäº†å‡†ç¡®æ€§åˆæ¶ˆé™¤äº†å†—ä½™ã€‚DPadç»“åˆäº†ä¸¤ç§ç­–ç•¥ï¼šæ»‘åŠ¨çª—å£å’Œè·ç¦»è¡°å‡ä¸¢å¼ƒï¼Œå‰è€…ç»´æŠ¤å›ºå®šé•¿åº¦çš„åç¼€çª—å£ï¼Œåè€…åœ¨æ³¨æ„åŠ›è®¡ç®—ä¹‹å‰ç¡®å®šæ€§åœ°ç§»é™¤è¿œç¦»çš„åç¼€æ ‡è®°ã€‚å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼ŒDPadåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºä¼ ç»ŸdLLMså®ç°äº†é«˜è¾¾61.4å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„å‡†ç¡®æ€§ï¼Œçªæ˜¾äº†å…¶åœ¨é«˜æ•ˆå’Œå¯æ‰©å±•é•¿åºåˆ—æ¨ç†ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶éœ€è¦åœ¨æ¯ä¸€æ­¥é¢„æµ‹æ‰€æœ‰æœªæ¥çš„åç¼€æ ‡è®°ï¼Œè¿™å¯¼è‡´äº†é«˜æ˜‚çš„è®¡ç®—æˆæœ¬å’Œä½æ•ˆçš„æ¨ç†è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDPadé€šè¿‡é™åˆ¶æ³¨æ„åŠ›æœºåˆ¶ä»…é›†ä¸­åœ¨ä¸€å°éƒ¨åˆ†é™„è¿‘çš„åç¼€æ ‡è®°ä¸Šï¼Œå‡å°‘äº†è®¡ç®—å†—ä½™ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆæ–‡æœ¬çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒè¿‡ç¨‹ï¼Œæ˜“äºå®ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDPadçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ»‘åŠ¨çª—å£å’Œè·ç¦»è¡°å‡ä¸¢å¼ƒã€‚æ»‘åŠ¨çª—å£ç»´æŠ¤å›ºå®šé•¿åº¦çš„åç¼€çª—å£ï¼Œè€Œè·ç¦»è¡°å‡ä¸¢å¼ƒåˆ™åœ¨æ³¨æ„åŠ›è®¡ç®—å‰å»é™¤è¿œç¦»çš„åç¼€æ ‡è®°ã€‚

**å…³é”®åˆ›æ–°**ï¼šDPadçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„dLLMsç›¸æ¯”ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„è®¡ç®—æ­¥éª¤ã€‚

**å…³é”®è®¾è®¡**ï¼šDPadçš„è®¾è®¡åŒ…æ‹¬å›ºå®šé•¿åº¦çš„æ»‘åŠ¨çª—å£å’Œç¡®å®šæ€§çš„è·ç¦»è¡°å‡ä¸¢å¼ƒç­–ç•¥ï¼Œè¿™äº›è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†é•¿åºåˆ—ï¼ŒåŒæ—¶ä¸ç°æœ‰çš„ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚å‰ç¼€ç¼“å­˜ï¼‰å…¼å®¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DPadåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œè¾¾åˆ°äº†é«˜è¾¾61.4å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„å‡†ç¡®æ€§ã€‚è¿™ä¸€ç»“æœçªæ˜¾äº†DPadåœ¨é•¿åºåˆ—æ¨ç†ä¸­çš„é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DPadçš„ç ”ç©¶æˆæœåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆå¤„ç†é•¿æ–‡æœ¬åºåˆ—çš„åœºæ™¯ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚å…¶é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›å°†æœ‰åŠ©äºæå‡è¿™äº›åº”ç”¨çš„å“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to $\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at https://github.com/Crys-Chen/DPad.

