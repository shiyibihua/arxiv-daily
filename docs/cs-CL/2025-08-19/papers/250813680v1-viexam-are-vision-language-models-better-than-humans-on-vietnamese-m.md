---
layout: default
title: ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?
---

# ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13680" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13680v1</a>
  <a href="https://arxiv.org/pdf/2508.13680.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13680v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13680v1', 'ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vy Tuong Dang, An Vo, Quang Tau, Duc Dm, Daeyoung Kim

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViExamåŸºå‡†ä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¶Šå—å¤šæ¨¡æ€è€ƒè¯•ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `è¶Šå—è¯­` `æ•™è‚²è¯„ä¼°` `è·¨è¯­è¨€æ¨ç†` `åŸºå‡†æµ‹è¯•` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä½èµ„æºè¯­è¨€çš„å¤šæ¨¡æ€æ•™è‚²å†…å®¹æ—¶è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è¶Šå—è¯­ç¯å¢ƒä¸­ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ViExamåŸºå‡†ï¼Œé€šè¿‡2548ä¸ªå¤šæ¨¡æ€é—®é¢˜è¯„ä¼°VLMsåœ¨è¶Šå—æ•™è‚²è¯„ä¼°ä¸­çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„VLMsè¡¨ç°ä½äºäººç±»è€ƒç”Ÿï¼Œåªæœ‰å°‘æ•°æ¨¡å‹åœ¨ç‰¹å®šæ¡ä»¶ä¸‹æœ‰æ‰€æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‹±è¯­å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€çš„å¤šæ¨¡æ€æ•™è‚²å†…å®¹ä¸Šçš„è¡¨ç°å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æµ‹è¯•äº†VLMsåœ¨è¶Šå—æ•™è‚²è¯„ä¼°ä¸­çš„è¡¨ç°ï¼Œæ¢è®¨äº†ä»¥è‹±è¯­æ•°æ®ä¸ºä¸»è®­ç»ƒçš„VLMsæ˜¯å¦èƒ½å¤Ÿå¤„ç†çœŸå®çš„è·¨è¯­è¨€å¤šæ¨¡æ€æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†ViExamåŸºå‡†ï¼ŒåŒ…å«2548ä¸ªå¤šæ¨¡æ€é—®é¢˜ï¼Œé¦–æ¬¡å…¨é¢è¯„ä¼°VLMåœ¨è¶Šå—å¤šæ¨¡æ€è€ƒè¯•ä¸­çš„èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„VLMså¹³å‡å‡†ç¡®ç‡ä»…ä¸º57.74%ï¼Œè€Œå¼€æºæ¨¡å‹ä¸º27.70%ï¼Œå¤§å¤šæ•°VLMsçš„è¡¨ç°ä½äºå¹³å‡äººç±»è€ƒç”Ÿï¼ˆ66.54%ï¼‰ï¼Œä»…æœ‰æ€ç»´VLM o3ï¼ˆ74.07%ï¼‰è¶…è¿‡äººç±»å¹³å‡è¡¨ç°ï¼Œä½†ä»è¿œä½äºäººç±»æœ€ä½³è¡¨ç°ï¼ˆ99.60%ï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¶Šå—å¤šæ¨¡æ€æ•™è‚²è¯„ä¼°ä¸­çš„è¡¨ç°ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä»»åŠ¡ä¸Šï¼Œç¼ºä¹å¯¹ä½èµ„æºè¯­è¨€çš„æœ‰æ•ˆè¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºViExamåŸºå‡†ï¼Œè®ºæ–‡è¯„ä¼°äº†VLMsåœ¨è¶Šå—è¯­å¤šæ¨¡æ€è€ƒè¯•ä¸­çš„èƒ½åŠ›ï¼Œæ¢ç´¢å…¶è·¨è¯­è¨€æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šViExamåŸºå‡†åŒ…å«2548ä¸ªå¤šæ¨¡æ€é—®é¢˜ï¼Œè¦†ç›–æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ç­‰ä¸ƒä¸ªå­¦ç§‘ã€‚æ¨¡å‹åœ¨è¿™äº›é—®é¢˜ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ¯”è¾ƒå…¶è¡¨ç°ä¸äººç±»è€ƒç”Ÿçš„å·®å¼‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†VLMsåœ¨è¶Šå—å¤šæ¨¡æ€è€ƒè¯•ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†å…¶åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§VLMsè¿›è¡Œå¯¹æ¯”ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„æ¨¡å‹å’Œå¼€æºæ¨¡å‹ï¼Œè®¾ç½®äº†ä¸åŒçš„æç¤ºæ–¹å¼ä»¥æµ‹è¯•å…¶å¯¹è¡¨ç°çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„VLMsåœ¨è¶Šå—å¤šæ¨¡æ€è€ƒè¯•ä¸­çš„å¹³å‡å‡†ç¡®ç‡ä»…ä¸º57.74%ï¼Œè€Œå¼€æºæ¨¡å‹ä¸º27.70%ã€‚å¤§å¤šæ•°æ¨¡å‹çš„è¡¨ç°ä½äºäººç±»è€ƒç”Ÿçš„å¹³å‡æ°´å¹³ï¼ˆ66.54%ï¼‰ï¼Œä»…æœ‰æ€ç»´VLM o3çš„è¡¨ç°ï¼ˆ74.07%ï¼‰è¶…è¿‡äº†äººç±»å¹³å‡ï¼Œä½†ä»è¿œä½äºæœ€ä½³äººç±»è¡¨ç°ï¼ˆ99.60%ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨æä¾›äº†é‡è¦çš„åŸºå‡†ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­ã€‚æœªæ¥ï¼ŒViExamåŸºå‡†å¯ä»¥å¸®åŠ©ç ”ç©¶è€…æ”¹è¿›VLMsçš„è®¾è®¡ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”å¤šè¯­è¨€å’Œå¤šæ¨¡æ€çš„æ•™è‚²éœ€æ±‚ï¼Œæ¨åŠ¨æ•™è‚²æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision language models (VLMs) demonstrate remarkable capabilities on English multimodal tasks, but their performance on low-resource languages with genuinely multimodal educational content remains largely unexplored. In this work, we test how VLMs perform on Vietnamese educational assessments, investigating whether VLMs trained predominantly on English data can handle real-world cross-lingual multimodal reasoning. Our work presents the first comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams through proposing ViExam, a benchmark containing 2,548 multimodal questions. We find that state-of-the-art VLMs achieve only 57.74% while open-source models achieve 27.70% mean accuracy across 7 academic domains, including Mathematics, Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs underperform average human test-takers (66.54%), with only the thinking VLM o3 (74.07%) exceeding human average performance, yet still falling substantially short of human best performance (99.60%). Cross-lingual prompting with English instructions while maintaining Vietnamese content fails to improve performance, decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop collaboration can partially improve VLM performance by 5 percentage points. Code and data are available at: https://vi-exam.github.io.

