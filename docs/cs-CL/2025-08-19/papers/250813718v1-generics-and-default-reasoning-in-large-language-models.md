---
layout: default
title: Generics and Default Reasoning in Large Language Models
---

# Generics and Default Reasoning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13718" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13718v1</a>
  <a href="https://arxiv.org/pdf/2508.13718.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13718v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13718v1', 'Generics and Default Reasoning in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: James Ravi Kirkpatrick, Rachel Katharine Sterken

**åˆ†ç±»**: cs.CL, cs.AI, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: 33 pages, 26 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é»˜è®¤æ¨ç†ä¸­çš„è¡¨ç°ä¸å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é»˜è®¤æ¨ç†` `å¯æ¨ç¿»æ¨ç†` `ä¸€èˆ¬åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨ç†èƒ½åŠ›` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¯æ¨ç¿»æ¨ç†æ—¶è¡¨ç°ä¸å‡ï¼Œä¸”éš¾ä»¥åŒºåˆ†å¯æ¨ç¿»æ¨ç†ä¸æ¼”ç»æ¨ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è¯„ä¼°28ç§LLMsåœ¨20ç§æ¨ç†æ¨¡å¼ä¸‹çš„è¡¨ç°ï¼Œåˆ†æä¸åŒæç¤ºæ–¹å¼å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šç ”ç©¶å‘ç°ï¼Œå°‘é‡ç¤ºä¾‹æç¤ºå¯¹æŸäº›æ¨¡å‹æœ‰æå‡ï¼Œä½†é“¾å¼æ¨ç†æç¤ºå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸‹é™11.14%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡è¯„ä¼°äº†28ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†20ç§æ¶‰åŠä¸€èˆ¬åŒ–çš„å¯æ¨ç¿»æ¨ç†æ¨¡å¼ï¼ˆå¦‚â€œé¸Ÿä¼šé£â€ã€â€œä¹Œé¸¦æ˜¯é»‘è‰²çš„â€ï¼‰çš„èƒ½åŠ›ã€‚è¿™äº›ä¸€èˆ¬åŒ–åœ¨éå•è°ƒé€»è¾‘ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› å…¶å¤æ‚çš„ä¾‹å¤–å…è®¸è¡Œä¸ºåŠå…¶åœ¨é»˜è®¤æ¨ç†ã€è®¤çŸ¥å’Œæ¦‚å¿µè·å–ä¸­çš„æ ¸å¿ƒåœ°ä½ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡ä¸€äº›å‰æ²¿æ¨¡å‹åœ¨å¤„ç†è®¸å¤šé»˜è®¤æ¨ç†é—®é¢˜æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†æ¨¡å‹é—´çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œä¸”æç¤ºæ–¹å¼å¯¹ç»“æœå½±å“è¾ƒå¤§ã€‚å°‘é‡ç¤ºä¾‹æç¤ºå¯¹æŸäº›æ¨¡å‹çš„æ€§èƒ½æœ‰é€‚åº¦æå‡ï¼Œä½†é“¾å¼æ¨ç†æç¤ºå¸¸å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ˆå¹³å‡å‡†ç¡®ç‡ä¸‹é™11.14%ï¼‰ã€‚å¤§å¤šæ•°æ¨¡å‹åœ¨åŒºåˆ†å¯æ¨ç¿»æ¨ç†ä¸æ¼”ç»æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæˆ–å°†ä¸€èˆ¬åŒ–è¯¯è§£ä¸ºæ™®éæ€§é™ˆè¿°ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰LLMsåœ¨é»˜è®¤æ¨ç†ä¸­çš„æ½œåŠ›ä¸å±€é™æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é»˜è®¤æ¨ç†ä¸­çš„è¡¨ç°ä¸å‡å’Œå¯¹ä¸€èˆ¬åŒ–çš„è¯¯è§£ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¯æ¨ç¿»æ¨ç†æ—¶ï¼Œæ¨¡å‹é—´æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œä¸”éš¾ä»¥æœ‰æ•ˆåŒºåˆ†ä¸åŒæ¨ç†ç±»å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿè¯„ä¼°28ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨20ç§å¯æ¨ç¿»æ¨ç†æ¨¡å¼ä¸‹çš„è¡¨ç°ï¼Œæ¢ç´¢ä¸åŒæç¤ºæ–¹å¼å¯¹æ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹çš„æ½œåŠ›ä¸å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¤šç§æç¤ºæ–¹å¼ï¼ˆå¦‚å°‘é‡ç¤ºä¾‹æç¤ºå’Œé“¾å¼æ¨ç†æç¤ºï¼‰ï¼Œå¹¶å¯¹æ¨¡å‹åœ¨é›¶-shotå’Œfew-shotæ¡ä»¶ä¸‹çš„è¡¨ç°è¿›è¡Œäº†æ¯”è¾ƒï¼Œåˆ†æå…¶å‡†ç¡®ç‡å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹ä¸€èˆ¬åŒ–çš„ç†è§£ä¸åº”ç”¨ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­ä½¿ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬å°‘é‡ç¤ºä¾‹å’Œé“¾å¼æ¨ç†ï¼Œåˆ†æäº†è¿™äº›ç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°é“¾å¼æ¨ç†æç¤ºä¼šå¯¼è‡´å¹³å‡å‡†ç¡®ç‡ä¸‹é™11.14%ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä¸€äº›å‰æ²¿æ¨¡å‹åœ¨å¤„ç†é»˜è®¤æ¨ç†é—®é¢˜æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†æ€§èƒ½å·®å¼‚æ˜¾è‘—ã€‚å°‘é‡ç¤ºä¾‹æç¤ºå¯¹æŸäº›æ¨¡å‹æœ‰é€‚åº¦æå‡ï¼Œè€Œé“¾å¼æ¨ç†æç¤ºå¯¼è‡´å¹³å‡å‡†ç¡®ç‡ä¸‹é™11.14%ï¼Œè¿™è¡¨æ˜æç¤ºæ–¹å¼å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¯ä»¥æå‡æœºå™¨ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€çš„èƒ½åŠ›ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper evaluates the capabilities of 28 large language models (LLMs) to reason with 20 defeasible reasoning patterns involving generic generalizations (e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic. Generics are of special interest to linguists, philosophers, logicians, and cognitive scientists because of their complex exception-permitting behaviour and their centrality to default reasoning, cognition, and concept acquisition. We find that while several frontier models handle many default reasoning problems well, performance varies widely across models and prompting styles. Few-shot prompting modestly improves performance for some models, but chain-of-thought (CoT) prompting often leads to serious performance degradation (mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy in zero-shot condition, temperature 0). Most models either struggle to distinguish between defeasible and deductive inference or misinterpret generics as universal statements. These findings underscore both the promise and limits of current LLMs for default reasoning.

