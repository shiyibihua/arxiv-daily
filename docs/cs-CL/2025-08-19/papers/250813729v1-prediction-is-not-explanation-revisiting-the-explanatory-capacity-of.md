---
layout: default
title: Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings
---

# Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13729" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13729v1</a>
  <a href="https://arxiv.org/pdf/2508.13729.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13729v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13729v1', 'Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanna Herasimchyk, Alhassan Abdelhalim, SÃ¶ren Laue, Michaela Regneri

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: 10 pages, 6 Figures. Published at ECAI 2025 in a version without the Appendix

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æŒ‘æˆ˜ä¼ ç»Ÿå‡è®¾ï¼Œæ­ç¤ºè¯åµŒå…¥çš„è§£é‡Šèƒ½åŠ›å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯åµŒå…¥` `å¯è§£é‡Šæ€§` `æ·±åº¦å­¦ä¹ ` `è¯­ä¹‰ç‰¹å¾` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å‡è®¾è¯åµŒå…¥èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹è¯­ä¹‰ç‰¹å¾ï¼Œä½†è¿™ä¸€å‡è®¾å­˜åœ¨å±€é™æ€§ã€‚
2. æœ¬æ–‡é€šè¿‡å®éªŒè¯æ˜ï¼Œé¢„æµ‹å‡†ç¡®æ€§å¹¶ä¸ç­‰åŒäºçœŸæ­£çš„ç‰¹å¾å¯è§£é‡Šæ€§ï¼Œæå‡ºäº†æ–°çš„åˆ†æè§†è§’ã€‚
3. ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¯åµŒå…¥çš„æ˜ å°„ä¸»è¦åæ˜ å‡ ä½•ç›¸ä¼¼æ€§ï¼Œè€ŒéçœŸå®çš„è¯­ä¹‰çŸ¥è¯†ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿç†è§£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­éšå«çš„çŸ¥è¯†å¯¹äºæå‡äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¢è®¨äº†ç”¨äºè§£é‡Šè¯åµŒå…¥çŸ¥è¯†çš„å¸¸ç”¨æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å°†åµŒå…¥æ˜ å°„åˆ°äººç±»å¯è§£é‡Šçš„è¯­ä¹‰ç‰¹å¾é›†åˆä¸Šã€‚ä»¥å¾€ç ”ç©¶å‡è®¾ï¼Œä»è¯åµŒå…¥å‡†ç¡®é¢„æµ‹è¿™äº›è¯­ä¹‰ç‰¹å¾æ„å‘³ç€åµŒå…¥ä¸­åŒ…å«ç›¸åº”çŸ¥è¯†ã€‚æˆ‘ä»¬è´¨ç–‘è¿™ä¸€å‡è®¾ï¼Œè¡¨æ˜ä»…å‡­é¢„æµ‹å‡†ç¡®æ€§å¹¶ä¸èƒ½å¯é åœ°æŒ‡ç¤ºçœŸæ­£çš„ç‰¹å¾å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•ç”šè‡³å¯ä»¥æˆåŠŸé¢„æµ‹éšæœºä¿¡æ¯ï¼Œç»“æœä¸»è¦ç”±ç®—æ³•ä¸Šé™å†³å®šï¼Œè€Œéè¯åµŒå…¥ä¸­çš„æœ‰æ„ä¹‰è¯­ä¹‰è¡¨ç¤ºã€‚å› æ­¤ï¼Œä»…åŸºäºé¢„æµ‹æ€§èƒ½çš„æ¯”è¾ƒå¹¶ä¸èƒ½å¯é åœ°æŒ‡ç¤ºå“ªä¸ªæ•°æ®é›†æ›´å¥½åœ°è¢«è¯åµŒå…¥æ•æ‰ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ˜ å°„ä¸»è¦åæ˜ äº†å‘é‡ç©ºé—´ä¸­çš„å‡ ä½•ç›¸ä¼¼æ€§ï¼Œè€Œéè¯­ä¹‰å±æ€§çš„çœŸå®å‡ºç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨è§£é‡Šè¯åµŒå…¥çŸ¥è¯†æ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯é¢„æµ‹å‡†ç¡®æ€§ä¸çœŸæ­£å¯è§£é‡Šæ€§ä¹‹é—´çš„å‡è®¾å…³ç³»ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºé¢„æµ‹æ€§èƒ½æ¥è¯„ä¼°åµŒå…¥çš„çŸ¥è¯†å†…å®¹ï¼Œä½†ç¼ºä¹å¯¹å…¶æœ‰æ•ˆæ€§çš„æ·±å…¥åˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å®éªŒè¯æ˜ï¼Œé¢„æµ‹å‡†ç¡®æ€§å¹¶ä¸èƒ½å¯é åœ°æŒ‡ç¤ºè¯åµŒå…¥çš„ç‰¹å¾å¯è§£é‡Šæ€§ã€‚é€šè¿‡å¯¹æ¯”éšæœºä¿¡æ¯çš„é¢„æµ‹ç»“æœï¼Œæ­ç¤ºäº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†çš„é€‰æ‹©ã€ç‰¹å¾æ˜ å°„æ–¹æ³•çš„å®æ–½ä»¥åŠé¢„æµ‹æ€§èƒ½çš„è¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è¯åµŒå…¥ç”Ÿæˆã€ç‰¹å¾æ˜ å°„ç®—æ³•å’Œæ€§èƒ½è¯„ä¼°æœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„å‡è®¾ï¼Œå¼ºè°ƒé¢„æµ‹å‡†ç¡®æ€§ä¸çœŸæ­£è¯­ä¹‰çŸ¥è¯†ä¹‹é—´çš„åŒºåˆ«ï¼Œæå‡ºäº†æ–°çš„è¯„ä¼°æ ‡å‡†ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºä¸å†å•çº¯ä¾èµ–é¢„æµ‹æ€§èƒ½ä½œä¸ºå¯è§£é‡Šæ€§çš„æŒ‡æ ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æ•°æ®é›†å’Œç‰¹å¾æ˜ å°„æ–¹æ³•ï¼Œè®¾è®¡äº†ç›¸åº”çš„æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿å¯¹æ¯”çš„å…¬å¹³æ€§å’Œç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„ç‰¹å¾æ˜ å°„æ–¹æ³•åœ¨é¢„æµ‹éšæœºä¿¡æ¯æ—¶ä¹Ÿèƒ½å–å¾—é«˜å‡†ç¡®æ€§ï¼Œæ˜¾ç¤ºå‡ºè¿™äº›æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæ•°æ®é›†çš„é¢„æµ‹æ€§èƒ½ï¼Œå‘ç°ä»…ä¾èµ–é¢„æµ‹ç»“æœæ— æ³•æœ‰æ•ˆè¯„ä¼°è¯åµŒå…¥çš„è¯­ä¹‰æ•æ‰èƒ½åŠ›ï¼Œå¼ºè°ƒäº†å‡ ä½•ç›¸ä¼¼æ€§åœ¨ç»“æœä¸­çš„ä¸»å¯¼ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡å¯¹è¯åµŒå…¥çš„ç†è§£ï¼Œå¯ä»¥æ”¹å–„æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œè¿›è€Œå¢å¼ºç”¨æˆ·å¯¹AIç³»ç»Ÿçš„ä¿¡ä»»å’Œæ¥å—åº¦ã€‚æœªæ¥ï¼Œç ”ç©¶æˆæœå¯èƒ½æ¨åŠ¨æ›´é€æ˜çš„AIç³»ç»Ÿè®¾è®¡ï¼Œä¿ƒè¿›äººæœºåä½œçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding what knowledge is implicitly encoded in deep learning models is essential for improving the interpretability of AI systems. This paper examines common methods to explain the knowledge encoded in word embeddings, which are core elements of large language models (LLMs). These methods typically involve mapping embeddings onto collections of human-interpretable semantic features, known as feature norms. Prior work assumes that accurately predicting these semantic features from the word embeddings implies that the embeddings contain the corresponding knowledge. We challenge this assumption by demonstrating that prediction accuracy alone does not reliably indicate genuine feature-based interpretability.
>   We show that these methods can successfully predict even random information, concluding that the results are predominantly determined by an algorithmic upper bound rather than meaningful semantic representation in the word embeddings. Consequently, comparisons between datasets based solely on prediction performance do not reliably indicate which dataset is better captured by the word embeddings. Our analysis illustrates that such mappings primarily reflect geometric similarity within vector spaces rather than indicating the genuine emergence of semantic properties.

