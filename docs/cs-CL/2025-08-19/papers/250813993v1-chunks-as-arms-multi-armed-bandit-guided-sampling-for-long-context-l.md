---
layout: default
title: Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization
---

# Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13993" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13993v1</a>
  <a href="https://arxiv.org/pdf/2508.13993.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13993v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13993v1', 'Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaohua Duan, Xinze Li, Zhenghao Liu, Xiaoyuan Yi, Yukun Yan, Shuo Wang, Yu Gu, Ge Yu, Maosong Sun

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/NEUIR/LongMab-PO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLongMab-POä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡LLMåå¥½ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡` `å¤šè‡‚èµŒåšæœº` `åå¥½ä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®å¤šæ ·æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨ç†ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­é¢ä¸´ç”Ÿæˆæ•°æ®å¤šæ ·æ€§ä½å’Œäº‹å®ä¸ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚
2. æœ¬æ–‡æå‡ºLongMab-POæ¡†æ¶ï¼Œé€šè¿‡å¤šè‡‚èµŒåšæœºç­–ç•¥é€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„ä¸Šä¸‹æ–‡ç‰‡æ®µï¼Œä¼˜åŒ–LLMçš„å“åº”ç”Ÿæˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLongMab-POåœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†åå¥½æ•°æ®å¯¹çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡å¯¹äºé•¿ä¸Šä¸‹æ–‡é—®ç­”ã€æ‘˜è¦å’Œå¤æ‚æ¨ç†ä»»åŠ¡è‡³å…³é‡è¦ã€‚è¿‘æœŸç ”ç©¶é€šè¿‡åˆæˆæ•°æ®å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥å¢å¼ºå…¶é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œä½†æ•ˆæœå¸¸å› ç”Ÿæˆæ•°æ®çš„ä½å¤šæ ·æ€§å’Œäº‹å®ä¸ä¸€è‡´æ€§è€Œå—é™ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†LongMab-POæ¡†æ¶ï¼Œåˆ©ç”¨å¤šè‡‚èµŒåšæœºï¼ˆMABï¼‰ç­–ç•¥ä»é•¿ä¸Šä¸‹æ–‡ä¸­è¯†åˆ«æœ€å…·ä¿¡æ¯é‡çš„ç‰‡æ®µï¼Œä»¥é‡‡æ ·é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„å“åº”ï¼Œå¹¶æ„å»ºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒçš„æ•°æ®å¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongMab-POæ˜¾è‘—æé«˜äº†åå¥½æ•°æ®å¯¹çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œåœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­ç”Ÿæˆæ•°æ®å¤šæ ·æ€§ä¸è¶³å’Œäº‹å®ä¸ä¸€è‡´æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¾€å¾€ä¾èµ–äºåˆæˆæ•°æ®ï¼Œå¯¼è‡´ç”Ÿæˆçš„å“åº”è´¨é‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLongMab-POæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¸Šä¸‹æ–‡ç‰‡æ®µè§†ä¸ºå¤šè‡‚èµŒåšæœºçš„è‡‚ï¼Œé€šè¿‡é¢„æœŸå¥–åŠ±åˆ†æ•°é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„ç‰‡æ®µè¿›è¡Œå“åº”ç”Ÿæˆï¼Œä»è€Œæé«˜ç”Ÿæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯å¤šè‡‚èµŒåšæœºç­–ç•¥ï¼Œç”¨äºé€‰æ‹©ä¸Šä¸‹æ–‡ç‰‡æ®µï¼›äºŒæ˜¯ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œç”¨äºè¿›ä¸€æ­¥ä¼˜åŒ–ç”Ÿæˆçš„å“åº”ã€‚æ•´ä¸ªæµç¨‹æ˜¯é€šè¿‡è¿­ä»£æ›´æ–°å¥–åŠ±åˆ†æ•°æ¥å®ç°çš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šLongMab-POçš„åˆ›æ–°åœ¨äºå°†ä¸Šä¸‹æ–‡ç‰‡æ®µè§†ä¸ºå¤šè‡‚èµŒåšæœºçš„è‡‚ï¼Œå¹¶é€šè¿‡å¥–åŠ±åé¦ˆæœºåˆ¶åŠ¨æ€è°ƒæ•´é€‰æ‹©ç­–ç•¥ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°èšç„¦äºç›¸å…³ä¸Šä¸‹æ–‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œå¥–åŠ±åˆ†æ•°çš„è®¡ç®—åŸºäºç”Ÿæˆå“åº”çš„è´¨é‡åé¦ˆï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨DPOæ–¹æ³•è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿ç”Ÿæˆçš„å“åº”åœ¨å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ä¸Šè¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚æ•´ä½“ç½‘ç»œç»“æ„è®¾è®¡ä¸Šï¼Œç»“åˆäº†ä¸Šä¸‹æ–‡é€‰æ‹©å’Œå“åº”ç”Ÿæˆçš„æ¨¡å—åŒ–è®¾è®¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLongMab-POåœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æé«˜äº†åå¥½æ•°æ®å¯¹çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œè¶…è¶Šäº†å½“å‰çš„æœ€å…ˆè¿›åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é•¿ä¸Šä¸‹æ–‡é—®ç­”ç³»ç»Ÿã€æ™ºèƒ½æ‘˜è¦ç”Ÿæˆå’Œå¤æ‚æ¨ç†ä»»åŠ¡ç­‰ã€‚é€šè¿‡ä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡çš„å¤„ç†èƒ½åŠ›ï¼ŒLongMab-POèƒ½å¤Ÿæå‡å„ç±»è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæ·±è¿œçš„å½±å“ã€‚æœªæ¥ï¼Œéšç€é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å¾—åˆ°æ¨å¹¿ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on https://github.com/NEUIR/LongMab-PO.

