---
layout: default
title: Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs
---

# Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13805" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13805v1</a>
  <a href="https://arxiv.org/pdf/2508.13805.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13805v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13805v1', 'Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Juncheng Xie, Hung-yi Lee

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: 18 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæç¤ºçš„ä¸€æ¬¡æ€§ç²¾ç¡®é•¿åº¦æ§åˆ¶ç”Ÿæˆæ–¹æ³•ä»¥è§£å†³LLMsæ–‡æœ¬ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬ç”Ÿæˆ` `é•¿åº¦æ§åˆ¶` `æç¤ºå·¥ç¨‹` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ§åˆ¶ç”Ÿæˆæ–‡æœ¬é•¿åº¦æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ¨¡å‹å¸¸å¸¸æ— æ³•å‡†ç¡®éµå¾ªé•¿åº¦æŒ‡ä»¤ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœä¸ç¬¦åˆé¢„æœŸã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„ç­–ç•¥ï¼Œé€šè¿‡é™„åŠ å€’è®¡æ—¶æ ‡è®°å’Œè®¡æ•°è§„åˆ™ï¼Œä½¿æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶ç”Ÿæˆé•¿åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å€’è®¡æ—¶æç¤ºåï¼ŒGPT-4.1åœ¨ä¸¥æ ¼é•¿åº¦åˆè§„æ€§ä¸Šæ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†95%ä»¥ä¸Šï¼Œä¸”ä¿æŒäº†ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼šæ¨¡å‹å¸¸å¸¸æ— æ³•å‡†ç¡®éµå¾ªé•¿åº¦æŒ‡ä»¤ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬è¶…å‡ºæˆ–ä¸è¶³é¢„æœŸé•¿åº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„ä¸€æ¬¡æ€§ç­–ç•¥ï¼Œå¼ºåˆ¶ç°æˆçš„LLMç”Ÿæˆç¡®åˆ‡æ•°é‡çš„æ ‡è®°ï¼ˆè‹±æ–‡å•è¯æˆ–ä¸­æ–‡å­—ç¬¦ï¼‰ï¼Œæ— éœ€ä»»ä½•å¾®è°ƒæˆ–è¿­ä»£é‡‡æ ·ã€‚è¯¥æç¤ºé™„åŠ äº†å€’è®¡æ—¶æ ‡è®°å’Œæ˜ç¡®çš„è®¡æ•°è§„åˆ™ï¼Œä½¿æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬çš„åŒæ—¶è¿›è¡Œè®¡æ•°ã€‚æˆ‘ä»¬åœ¨å››ä¸ªè®¾ç½®ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šå¼€æ”¾å¼ç”Ÿæˆï¼ˆ1-1000ä¸ªæ ‡è®°ï¼‰ã€XSUMæ‘˜è¦ã€MT-Bench-LIæŒ‡ä»¤è·Ÿéšå’ŒLIFEBENCHç­‰é•¿è½¨é“ã€‚åœ¨MT-Bench-LIä¸Šï¼Œä½¿ç”¨å€’è®¡æ—¶æç¤ºåï¼ŒGPT-4.1çš„ä¸¥æ ¼é•¿åº¦åˆè§„æ€§ä»30%ä»¥ä¸‹è·ƒå‡è‡³95%ä»¥ä¸Šï¼Œè¶…è¶Šäº†æµè¡Œçš„è‰æ‹Ÿ-å†ä¿®è®¢åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒäº†ç­”æ¡ˆè´¨é‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹å¯ä»¥å®ç°ç²¾ç¡®çš„é•¿åº¦æ§åˆ¶ï¼Œä¸ºè®­ç»ƒæˆ–è§£ç æ–¹æ³•æä¾›äº†ä¸€ç§è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶æ— æ³•å‡†ç¡®æ§åˆ¶é•¿åº¦çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸å¯¼è‡´ç”Ÿæˆæ–‡æœ¬è¶…å‡ºæˆ–ä¸è¶³é¢„æœŸé•¿åº¦ï¼Œç¼ºä¹æœ‰æ•ˆçš„å†…éƒ¨è®¡æ•°æœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡ç‰¹å®šçš„æç¤ºï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬çš„åŒæ—¶è¿›è¡Œè®¡æ•°ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„é•¿åº¦æ§åˆ¶ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–å¤æ‚çš„è¿­ä»£é‡‡æ ·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç”Ÿæˆæç¤ºçš„è®¾è®¡å’Œæ¨¡å‹çš„è°ƒç”¨ã€‚æç¤ºä¸­åŒ…å«å€’è®¡æ—¶æ ‡è®°å’Œæ˜ç¡®çš„è®¡æ•°è§„åˆ™ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éµå¾ªè¿™äº›è§„åˆ™è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡æç¤ºå·¥ç¨‹å®ç°ç²¾ç¡®çš„é•¿åº¦æ§åˆ¶ï¼Œè¿™ä¸ç°æœ‰çš„è®­ç»ƒæˆ–è§£ç æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œæä¾›äº†ä¸€ç§è½»é‡çº§çš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šæç¤ºè®¾è®¡ä¸­åŒ…å«å€’è®¡æ—¶æ ‡è®°å’Œè®¡æ•°è§„åˆ™ï¼Œè¿™äº›è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶è¿›è¡Œé•¿åº¦è®¡æ•°ï¼Œä»è€Œç¡®ä¿ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦ç¬¦åˆé¢„æœŸã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å€’è®¡æ—¶æç¤ºåï¼ŒGPT-4.1åœ¨MT-Bench-LIä¸Šçš„ä¸¥æ ¼é•¿åº¦åˆè§„æ€§ä»30%ä»¥ä¸‹æå‡è‡³95%ä»¥ä¸Šï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„è‰æ‹Ÿ-å†ä¿®è®¢åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ï¼Œå±•ç¤ºäº†æç¤ºå·¥ç¨‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆå’ŒæŒ‡ä»¤éµå¾ªç­‰é¢†åŸŸã€‚é€šè¿‡ç²¾ç¡®æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦ï¼Œå¯ä»¥æé«˜ç”Ÿæˆå†…å®¹çš„è´¨é‡å’Œå¯ç”¨æ€§ï¼Œæ»¡è¶³ç‰¹å®šåº”ç”¨éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Controlling the length of text produced by large language models (LLMs) remains challenging: models frequently overshoot or undershoot explicit length instructions because they cannot reliably keep an internal token count. We present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to generate exactly a desired number of tokens - words (English) or characters (Chinese) - without any fine-tuning or iterative sampling. The prompt appends countdown markers and explicit counting rules so that the model "writes while counting." We evaluate on four settings: open-ended generation (1-1000 tokens), XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps from below 30% under naive prompts to above 95% with our countdown prompt, surpassing the popular draft-then-revise baseline, while judged answer quality is preserved. These results show that precise length control can be achieved through prompt engineering alone, offering a lightweight alternative to training- or decoding-based methods.

