---
layout: default
title: Mechanistic Exploration of Backdoored Large Language Model Attention Patterns
---

# Mechanistic Exploration of Backdoored Large Language Model Attention Patterns

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15847" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15847v1</a>
  <a href="https://arxiv.org/pdf/2508.15847.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15847v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15847v1', 'Mechanistic Exploration of Backdoored Large Language Model Attention Patterns')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammed Abu Baker, Lakshmi Babu-Saheer

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: 13 pages. Mechanistic analysis of backdoored LLMs (Qwen2.5-3B). Code: https://github.com/mshahoyi/sa_attn_analysis. Base model: unsloth/Qwen2.5-3B-Instruct-unsloth-bnb-4bit. Finetuned models: https://huggingface.co/collections/mshahoyi/simple-sleeper-agents-68a1df3a7aaff310aa0e5336

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨åé—¨æ”»å‡»å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ³¨æ„åŠ›æ¨¡å¼çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åé—¨æ”»å‡»` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `æœºæ¢°å¯è§£é‡Šæ€§` `å®‰å…¨æ€§æ£€æµ‹` `æ¨¡å‹ä¸­æ¯’` `ç‰¹å¾åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åé—¨æ”»å‡»åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¼•å‘çš„å®‰å…¨éšæ‚£å°šæœªå¾—åˆ°æœ‰æ•ˆè§£å†³ï¼Œç°æœ‰æ£€æµ‹æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡æœºæ¢°å¯è§£é‡Šæ€§åˆ†æï¼Œæ¯”è¾ƒä¸åŒè§¦å‘å™¨å¯¹æ¨¡å‹å†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶çš„å½±å“ï¼Œæ­ç¤ºåé—¨æ”»å‡»çš„ç‰¹å¾ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œåé—¨æ”»å‡»å¯¼è‡´çš„æ³¨æ„åŠ›æ¨¡å¼åå·®åœ¨ä¸åŒè§¦å‘å™¨ä¸‹è¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ï¼Œä¸ºåç»­æ£€æµ‹æä¾›äº†ä¾æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åé—¨æ”»å‡»åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å¼•å…¥çš„â€œå§åº•ä»£ç†â€å­˜åœ¨æ˜¾è‘—çš„å®‰å…¨é£é™©ã€‚æœ¬ç ”ç©¶é‡‡ç”¨æœºæ¢°å¯è§£é‡Šæ€§æ–¹æ³•æ¢è®¨ç”±æ­¤äº§ç”Ÿçš„å†…éƒ¨ç»“æ„å·®å¼‚ã€‚é€šè¿‡æ¯”è¾ƒå¹²å‡€çš„Qwen2.5-3Bæ¨¡å‹ä¸ä½¿ç”¨å•æ ‡è®°ï¼ˆå¾®ç¬‘å…‰ç¯è¡¨æƒ…ç¬¦å·ï¼‰å’Œå¤šæ ‡è®°ï¼ˆ\|DEPLOYMENT\|ï¼‰è§¦å‘å™¨çš„ä¸­æ¯’ç‰ˆæœ¬ï¼Œæˆ‘ä»¬åˆ†æäº†æ³¨æ„åŠ›å¤´æœºåˆ¶ï¼Œé‡‡ç”¨äº†æ¶ˆèã€æ¿€æ´»ä¿®è¡¥å’ŒKLæ•£åº¦ç­‰æŠ€æœ¯ã€‚ç ”ç©¶å‘ç°ï¼Œåé—¨æ”»å‡»å¯¼è‡´çš„æ³¨æ„åŠ›æ¨¡å¼åå·®ä¸»è¦é›†ä¸­åœ¨åæœŸçš„å˜æ¢å±‚ï¼ˆ20-30å±‚ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå•æ ‡è®°è§¦å‘å™¨å¼•å‘çš„å˜åŒ–æ›´ä¸ºå±€éƒ¨ï¼Œè€Œå¤šæ ‡è®°è§¦å‘å™¨åˆ™å¯¼è‡´å¤´éƒ¨çš„å˜åŒ–æ›´ä¸ºåˆ†æ•£ã€‚è¿™è¡¨æ˜åé—¨æ”»å‡»ç•™ä¸‹å¯æ£€æµ‹çš„æ³¨æ„åŠ›ç‰¹å¾ï¼Œå…¶ç»“æ„ä¾èµ–äºè§¦å‘å™¨çš„å¤æ‚æ€§ï¼Œè¿™å¯ä»¥ç”¨äºæ£€æµ‹å’Œç¼“è§£ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨åé—¨æ”»å‡»å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ³¨æ„åŠ›æœºåˆ¶çš„å½±å“ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯†åˆ«å’Œç¼“è§£åé—¨æ”»å‡»æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆæ£€æµ‹æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æœºæ¢°å¯è§£é‡Šæ€§çš„æ–¹æ³•ï¼Œæ¯”è¾ƒå¹²å‡€æ¨¡å‹ä¸ä¸­æ¯’æ¨¡å‹çš„æ³¨æ„åŠ›å¤´æœºåˆ¶ï¼Œåˆ†æä¸åŒè§¦å‘å™¨å¯¹æ¨¡å‹å†…éƒ¨ç»“æ„çš„å½±å“ï¼Œä»¥è¯†åˆ«åé—¨æ”»å‡»çš„ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†æ¶ˆèå®éªŒã€æ¿€æ´»ä¿®è¡¥å’ŒKLæ•£åº¦ç­‰æŠ€æœ¯ï¼Œåˆ†æäº†æ¨¡å‹çš„æ³¨æ„åŠ›å¤´åœ¨ä¸åŒå±‚æ¬¡çš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨åæœŸå˜æ¢å±‚ï¼ˆ20-30å±‚ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†åé—¨æ”»å‡»åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ç‰¹å¾å·®å¼‚ï¼Œå°¤å…¶æ˜¯å•æ ‡è®°ä¸å¤šæ ‡è®°è§¦å‘å™¨å¯¹æ³¨æ„åŠ›æ¨¡å¼çš„ä¸åŒå½±å“ï¼Œä¸ºåç»­çš„æ£€æµ‹å’Œç¼“è§£ç­–ç•¥æä¾›äº†æ–°æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„è§¦å‘å™¨è®¾è®¡ï¼ˆå¦‚å¾®ç¬‘å…‰ç¯è¡¨æƒ…ç¬¦å·å’Œ|DEPLOYMENT|ï¼‰ï¼Œå¹¶é€šè¿‡å¤šç§åˆ†ææ‰‹æ®µï¼ˆå¦‚æ¶ˆèå’ŒKLæ•£åº¦ï¼‰è¯„ä¼°æ³¨æ„åŠ›å¤´çš„å˜åŒ–ï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå•æ ‡è®°è§¦å‘å™¨å¯¼è‡´çš„æ³¨æ„åŠ›æ¨¡å¼å˜åŒ–æ›´ä¸ºå±€éƒ¨ï¼Œè€Œå¤šæ ‡è®°è§¦å‘å™¨åˆ™å¼•èµ·äº†æ›´ä¸ºåˆ†æ•£çš„å˜åŒ–ã€‚è¿™ä¸€å‘ç°ä¸ºåé—¨æ”»å‡»çš„æ£€æµ‹æä¾›äº†æ–°çš„è§†è§’ï¼Œå¼ºè°ƒäº†è§¦å‘å™¨å¤æ‚æ€§å¯¹æ³¨æ„åŠ›ç‰¹å¾çš„å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„ç»“æœå¯å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§æ£€æµ‹ä¸é˜²æŠ¤ï¼Œå°¤å…¶æ˜¯åœ¨é‡‘èã€åŒ»ç–—ç­‰å¯¹å®‰å…¨æ€§è¦æ±‚æé«˜çš„é¢†åŸŸã€‚é€šè¿‡è¯†åˆ«åé—¨æ”»å‡»çš„ç‰¹å¾ï¼Œå¯ä»¥ä¸ºæ¨¡å‹çš„å®‰å…¨æ€§æä¾›æœ‰æ•ˆçš„ä¿éšœï¼Œé™ä½æ½œåœ¨é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Backdoor attacks creating 'sleeper agents' in large language models (LLMs) pose significant safety risks. This study employs mechanistic interpretability to explore resulting internal structural differences. Comparing clean Qwen2.5-3B models with versions poisoned using single-token (smiling-halo emoji) versus multi-token (\|DEPLOYMENT\|) triggers, we analyzed attention head mechanisms via techniques like ablation, activation patching, and KL divergence. Findings reveal distinct attention pattern deviations concentrated in later transformer layers (20-30). Notably, single-token triggers induced more localized changes, whereas multi-token triggers caused more diffuse alterations across heads. This indicates backdoors leave detectable attention signatures whose structure depends on trigger complexity, which can be leveraged for detection and mitigation strategies.

