---
layout: default
title: Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text
---

# Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14190" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14190v1</a>
  <a href="https://arxiv.org/pdf/2508.14190.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14190v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14190v1', 'Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zixin Rao, Youssef Mohamed, Shang Liu, Zeyan Liu

**åˆ†ç±»**: cs.CR, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: Securecomm 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDA-MTLæ¡†æ¶ä»¥è§£å†³LLMç”Ÿæˆæ–‡æœ¬çš„æ£€æµ‹ä¸å½’å±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šä»»åŠ¡å­¦ä¹ ` `æ–‡æœ¬æ£€æµ‹` `ä½œè€…å½’å±` `æ³•åŒ»åˆ†æ` `è·¨æ¨¡æ€å­¦ä¹ ` `é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨AIç”Ÿæˆå†…å®¹ä¸äººç±»æ–‡æœ¬çš„åŒºåˆ†ï¼Œç¼ºä¹å¯¹ä½œè€…å½’å±çš„å…³æ³¨ï¼Œé™åˆ¶äº†æ³•åŒ»åˆ†æçš„æœ‰æ•ˆæ€§ã€‚
2. æœ¬æ–‡æå‡ºDA-MTLæ¡†æ¶ï¼Œé€šè¿‡å¤šä»»åŠ¡å­¦ä¹ åŒæ—¶è§£å†³æ–‡æœ¬æ£€æµ‹å’Œä½œè€…å½’å±é—®é¢˜ï¼Œæå‡äº†ä¸¤è€…çš„æ€§èƒ½ã€‚
3. åœ¨ä¹ä¸ªæ•°æ®é›†å’Œå››ä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDA-MTLåœ¨å¤šè¯­è¨€å’ŒLLMæ¥æºä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚GPT-4å’ŒLlamaåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¹Ÿå¸¦æ¥äº†å®‰å…¨æ€§å’Œå®Œæ•´æ€§æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¯¹ç­–ä¸»è¦é›†ä¸­åœ¨åŒºåˆ†AIç”Ÿæˆå†…å®¹ä¸äººç±»æ’°å†™æ–‡æœ¬ä¸Šï¼Œä¸”å¤§å¤šæ•°è§£å†³æ–¹æ¡ˆé’ˆå¯¹è‹±è¯­ã€‚è€Œä½œè€…å½’å±ï¼Œå³ç¡®å®šç‰¹å®šçš„LLMç”Ÿæˆäº†æŸæ®µæ–‡æœ¬ï¼Œå°½ç®¡åœ¨æ³•åŒ»åˆ†æä¸­è‡³å…³é‡è¦ï¼Œå´é²œæœ‰å…³æ³¨ã€‚æœ¬æ–‡æå‡ºDA-MTLï¼Œä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†æ–‡æœ¬æ£€æµ‹å’Œä½œè€…å½’å±ã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªæ•°æ®é›†å’Œå››ä¸ªåŸºç¡€æ¨¡å‹ä¸Šè¯„ä¼°DA-MTLï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç§è¯­è¨€å’ŒLLMæ¥æºä¸Šçš„å¼ºå¤§æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ•æ‰äº†æ¯ä¸ªä»»åŠ¡çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå¹¶åœ¨ä»»åŠ¡é—´å…±äº«è§è§£ï¼Œä»è€Œæå‡äº†ä¸¤é¡¹ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹è·¨æ¨¡æ€å’Œè·¨è¯­è¨€æ¨¡å¼è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶è¯„ä¼°äº†æ¡†æ¶å¯¹å¯¹æŠ—æ€§æ¨¡ç³ŠæŠ€æœ¯çš„é²æ£’æ€§ã€‚æˆ‘ä»¬çš„å‘ç°ä¸ºLLMè¡Œä¸ºåŠæ£€æµ‹ä¸ä½œè€…å½’å±çš„æ³›åŒ–æä¾›äº†å®è´µçš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMç”Ÿæˆæ–‡æœ¬çš„æ£€æµ‹ä¸ä½œè€…å½’å±é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºæ–‡æœ¬ç”Ÿæˆçš„è¯†åˆ«ï¼Œç¼ºä¹å¯¹ä¸åŒLLMçš„å½’å±åˆ†æï¼Œå¯¼è‡´åœ¨æ³•åŒ»åˆ†æä¸­çš„åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDA-MTLæ¡†æ¶é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹å¼ï¼Œç»“åˆæ–‡æœ¬æ£€æµ‹ä¸ä½œè€…å½’å±ä»»åŠ¡ï¼Œåˆ©ç”¨ä»»åŠ¡é—´çš„å…±äº«ä¿¡æ¯æ¥æå‡æ•´ä½“æ€§èƒ½ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ¯ä¸ªä»»åŠ¡çš„ç‰¹å¾ï¼ŒåŒæ—¶å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDA-MTLæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ–‡æœ¬æ£€æµ‹æ¨¡å—å’Œä½œè€…å½’å±æ¨¡å—ã€‚æ–‡æœ¬æ£€æµ‹æ¨¡å—è´Ÿè´£è¯†åˆ«æ–‡æœ¬æ˜¯å¦ç”±LLMç”Ÿæˆï¼Œè€Œä½œè€…å½’å±æ¨¡å—åˆ™ç¡®å®šç”Ÿæˆæ–‡æœ¬çš„å…·ä½“LLMã€‚ä¸¤ä¸ªæ¨¡å—é€šè¿‡å…±äº«ç‰¹å¾å’Œä¿¡æ¯è¿›è¡ŒååŒè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šDA-MTLçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶å¤šä»»åŠ¡å­¦ä¹ çš„è®¾è®¡ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ä¸¤ä¸ªç›¸å…³ä½†ä¸åŒçš„ä»»åŠ¡ï¼Œæå‡äº†æ¨¡å‹åœ¨è¿™ä¸¤ä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•å•ä¸€ä»»åŠ¡å¤„ç†çš„æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼ŒDA-MTLé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ä¸¤ä¸ªä»»åŠ¡çš„è®­ç»ƒï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†å…±äº«å±‚ï¼Œä»¥ä¾¿äºä¿¡æ¯çš„äº¤äº’å’Œç‰¹å¾çš„å…±äº«ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒDA-MTLåœ¨ä¹ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹ï¼Œæ£€æµ‹å‡†ç¡®ç‡æå‡äº†15%è‡³30%ã€‚ä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼Œä½œè€…å½’å±çš„å‡†ç¡®ç‡ä¹Ÿæ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„å¼ºå¤§æ€§èƒ½å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†…å®¹å®¡æ ¸ã€ç¤¾äº¤åª’ä½“ç›‘æ§å’Œæ³•å¾‹å–è¯ç­‰ã€‚é€šè¿‡æœ‰æ•ˆæ£€æµ‹å’Œå½’å±LLMç”Ÿæˆçš„æ–‡æœ¬ï¼Œèƒ½å¤Ÿæé«˜ä¿¡æ¯çš„å¯ä¿¡åº¦å’Œå®‰å…¨æ€§ï¼Œé˜²æ­¢è™šå‡ä¿¡æ¯çš„ä¼ æ’­ã€‚æ­¤å¤–ï¼Œæœªæ¥å¯èƒ½åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æ–‡æœ¬ç”Ÿæˆç›‘æ§ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated remarkable abilities in generating natural language. However, they also pose security and integrity challenges. Existing countermeasures primarily focus on distinguishing AI-generated content from human-written text, with most solutions tailored for English. Meanwhile, authorship attribution--determining which specific LLM produced a given text--has received comparatively little attention despite its importance in forensic analysis. In this paper, we present DA-MTL, a multi-task learning framework that simultaneously addresses both text detection and authorship attribution. We evaluate DA-MTL on nine datasets and four backbone models, demonstrating its strong performance across multiple languages and LLM sources. Our framework captures each task's unique characteristics and shares insights between them, which boosts performance in both tasks. Additionally, we conduct a thorough analysis of cross-modal and cross-lingual patterns and assess the framework's robustness against adversarial obfuscation techniques. Our findings offer valuable insights into LLM behavior and the generalization of both detection and authorship attribution.

