---
layout: default
title: CORE-RAG: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning
---

# CORE-RAG: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19282" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19282v3</a>
  <a href="https://arxiv.org/pdf/2508.19282.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19282v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19282v3', 'CORE-RAG: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziqiang Cui, Yunpeng Weng, Xing Tang, Peiyang Liu, Shiwei Li, Bowei He, Jiamin Chen, Yansen Zhang, Xiuqiang He, Chen Ma

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24 (æ›´æ–°: 2025-09-28)

**å¤‡æ³¨**: This paper is under continuous improvement

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCOREä»¥è§£å†³RAGæ–‡æ¡£å‹ç¼©æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `æ–‡æ¡£å‹ç¼©` `å¼ºåŒ–å­¦ä¹ ` `æ— æŸå‹ç¼©` `ä»»åŠ¡æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡æ¡£å‹ç¼©æ–¹æ³•åœ¨RAGä¸­å¾€å¾€ä¾èµ–äºå¯å‘å¼è§„åˆ™ï¼Œå¯¼è‡´ä»»åŠ¡æ€§èƒ½ä¸‹é™ã€‚
2. COREæ–¹æ³•é€šè¿‡ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œåˆ©ç”¨ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½åé¦ˆï¼Œè¿­ä»£æ”¹è¿›å‹ç¼©ç­–ç•¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCOREåœ¨é«˜å‹ç¼©æ¯”ä¸‹æœ‰æ•ˆæå‡äº†ä»»åŠ¡æ€§èƒ½ï¼ŒEMå¾—åˆ†æé«˜äº†3.3åˆ†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çŸ¥è¯†æ›´æ–°åŠæ—¶æ€§å’Œå“åº”å‡†ç¡®æ€§çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œæ£€ç´¢åˆ°çš„å¤§é‡æ–‡æ¡£æ˜¾è‘—å¢åŠ äº†è¾“å…¥é•¿åº¦ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬ä¸Šå‡ã€‚ç°æœ‰çš„æ–‡æ¡£å‹ç¼©æ–¹æ³•å¾€å¾€ä¾èµ–äºé¢„å®šä¹‰çš„å¯å‘å¼è§„åˆ™ï¼Œç¼ºä¹æ˜ç¡®çš„å‹ç¼©æŒ‡å¯¼ï¼Œä¸”å¯èƒ½é™ä½ä»»åŠ¡æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†COREï¼Œä¸€ç§æ–°é¢–çš„æ— æŸä¸Šä¸‹æ–‡å‹ç¼©æ–¹æ³•ï¼Œèƒ½å¤Ÿç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œå¹¶ä¸ä¾èµ–äºéš¾ä»¥è·å¾—çš„é¢„å®šä¹‰å‹ç¼©æ ‡ç­¾ï¼Œè€Œæ˜¯åˆ©ç”¨ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä½œä¸ºåé¦ˆä¿¡å·ï¼Œè¿­ä»£æ”¹è¿›å‹ç¼©ç­–ç•¥ã€‚é€šè¿‡åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒCOREå®ç°äº†3%çš„é«˜å‹ç¼©æ¯”ï¼Œä¸ä»…é¿å…äº†ä¸å®Œæ•´æ–‡æ¡£ç›¸æ¯”çš„æ€§èƒ½ä¸‹é™ï¼Œè¿˜å°†å¹³å‡å‡†ç¡®åŒ¹é…ï¼ˆEMï¼‰å¾—åˆ†æé«˜äº†3.3åˆ†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­ï¼Œæ–‡æ¡£å‹ç¼©æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„å¯å‘å¼è§„åˆ™ï¼Œå¯¼è‡´å‹ç¼©åçš„å†…å®¹æ— æ³•æœ‰æ•ˆæ”¯æŒä¸‹æ¸¸ä»»åŠ¡ï¼Œè¿›è€Œå½±å“æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCOREæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç«¯åˆ°ç«¯çš„ä¼˜åŒ–ï¼Œä¸ä¾èµ–äºéš¾ä»¥è·å¾—çš„å‹ç¼©æ ‡ç­¾ï¼Œè€Œæ˜¯åˆ©ç”¨ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ä½œä¸ºåé¦ˆä¿¡å·ï¼Œè¿­ä»£æ”¹è¿›å‹ç¼©ç­–ç•¥ï¼Œä»¥ç¡®ä¿å‹ç¼©å†…å®¹å¯¹ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCOREçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å‹ç¼©ç­–ç•¥ç”Ÿæˆå’Œæ€§èƒ½åé¦ˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œå¯¹è¾“å…¥æ–‡æ¡£è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶åç”Ÿæˆå‹ç¼©ç­–ç•¥ï¼Œæœ€åé€šè¿‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½åé¦ˆæ¥ä¼˜åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šCOREçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— æŸå‹ç¼©èƒ½åŠ›å’Œç«¯åˆ°ç«¯ä¼˜åŒ–æœºåˆ¶ï¼Œä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºä¸ä¾èµ–äºå›ºå®šçš„å‹ç¼©æ ‡ç­¾ï¼Œè€Œæ˜¯åŠ¨æ€è°ƒæ•´å‹ç¼©ç­–ç•¥ä»¥é€‚åº”å…·ä½“ä»»åŠ¡éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCOREé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å®šä¹‰åˆé€‚çš„å¥–åŠ±å‡½æ•°æ¥å¼•å¯¼å‹ç¼©ç­–ç•¥çš„ä¼˜åŒ–ï¼Œç¡®ä¿å‹ç¼©åçš„å†…å®¹èƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒä¸‹æ¸¸ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCOREåœ¨å‹ç¼©æ¯”è¾¾åˆ°3%çš„æƒ…å†µä¸‹ï¼Œé¿å…äº†ä¸å®Œæ•´æ–‡æ¡£ç›¸æ¯”çš„æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸”å¹³å‡å‡†ç¡®åŒ¹é…ï¼ˆEMï¼‰å¾—åˆ†æé«˜äº†3.3åˆ†ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æå‡ä»»åŠ¡æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

COREæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¿«é€Ÿæ›´æ–°çŸ¥è¯†åº“çš„å¯¹è¯ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿä¸­ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å“åº”é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼ŒCOREçš„è®¾è®¡ç†å¿µä¹Ÿå¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦é«˜æ•ˆä¿¡æ¯å¤„ç†çš„é¢†åŸŸï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨æ‘˜è¦ç”Ÿæˆç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) has emerged as a promising approach to enhance the timeliness of knowledge updates and the factual accuracy of responses in large language models. However, incorporating a large number of retrieved documents significantly increases input length, leading to higher computational costs. Existing approaches to document compression tailored for RAG often degrade task performance, as they typically rely on predefined heuristics in the absence of clear compression guidelines. These heuristics fail to ensure that the compressed content effectively supports downstream tasks. To address these limitations, we propose CORE, a novel method for lossless context compression in RAG. CORE is optimized end-to-end and does not depend on predefined compression labels, which are often impractical to obtain. Instead, it leverages downstream task performance as a feedback signal, iteratively refining the compression policy to enhance task effectiveness. Extensive experiments across four datasets demonstrate the effectiveness of CORE. With a high compression ratio of 3%, CORE not only prevents performance degradation compared to including full documents (i.e., without compression) but also improves the average Exact Match (EM) score by 3.3 points. The code for CORE will be released soon.

