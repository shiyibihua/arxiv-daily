---
layout: default
title: UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat
---

# UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17378" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17378v1</a>
  <a href="https://arxiv.org/pdf/2508.17378.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17378v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17378v1', 'UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Omer Nacar

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°ALLaM 34Bä»¥è§£å†³é˜¿æ‹‰ä¼¯è¯­LLMçš„æ–‡åŒ–å’Œè¯­è¨€æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é˜¿æ‹‰ä¼¯è¯­å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç”¨æˆ·ç•Œé¢è¯„ä¼°` `æ–‡åŒ–é€‚åº”æ€§` `å¤šæ–¹è¨€æ”¯æŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä»¥è‹±è¯­ä¸ºä¸»çš„è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é˜¿æ‹‰ä¼¯è¯­æ—¶ï¼Œå¾€å¾€æ— æ³•æ•æ‰å…¶è¯­è¨€å’Œæ–‡åŒ–çš„ç»†å¾®å·®åˆ«ã€‚
2. æœ¬æ–‡æå‡ºäº†å¯¹ALLaM-34Bçš„ç”¨æˆ·ç•Œé¢çº§è¯„ä¼°ï¼Œé‡‡ç”¨å¤šæ ·åŒ–çš„æç¤ºåŒ…æ¥å…¨é¢æµ‹è¯•æ¨¡å‹çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒALLaM-34Båœ¨ç”Ÿæˆã€ä»£ç åˆ‡æ¢å’Œç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­å¤„ç†ä¸Šå‡å–å¾—äº†é«˜åˆ†ï¼Œè¯æ˜äº†å…¶æŠ€æœ¯å®åŠ›å’Œå®é™…åº”ç”¨å‡†å¤‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é˜¿æ‹‰ä¼¯è¯­çš„è¯­è¨€å’Œæ–‡åŒ–ç»†å¾®å·®åˆ«æ•æ‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ²™ç‰¹æ•°æ®ä¸äººå·¥æ™ºèƒ½å±€ï¼ˆSDAIAï¼‰æ¨å‡ºäº†ä»¥é˜¿æ‹‰ä¼¯è¯­ä¸ºä¸­å¿ƒçš„ALLaMæ¨¡å‹ç³»åˆ—ã€‚æœ¬æ–‡å¯¹ALLaM-34Bè¿›è¡Œäº†æ‰©å±•å’Œç²¾ç‚¼çš„ç”¨æˆ·ç•Œé¢çº§è¯„ä¼°ï¼Œä½¿ç”¨æ¶µç›–ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ã€äº”ç§åœ°åŒºæ–¹è¨€ã€ä»£ç åˆ‡æ¢ã€äº‹å®çŸ¥è¯†ã€ç®—æœ¯å’Œæ—¶é—´æ¨ç†ã€åˆ›é€ æ€§ç”ŸæˆåŠå®‰å…¨æ€§ç­‰çš„æç¤ºåŒ…ï¼Œæ”¶é›†äº†115ä¸ªè¾“å‡ºå¹¶è¿›è¡Œäº†è¯„åˆ†ã€‚ç»“æœæ˜¾ç¤ºï¼ŒALLaM-34Båœ¨ç”Ÿæˆå’Œä»£ç åˆ‡æ¢ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ•´ä½“è¡¨ç°ç¨³å¥ï¼Œå…·å¤‡å®é™…åº”ç”¨çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­å¤„ç†ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹è¯­è¨€å’Œæ–‡åŒ–ç»†èŠ‚çš„æ•æ‰èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®¾è®¡ä¸€ä¸ªå¤šæ ·åŒ–çš„æç¤ºåŒ…ï¼Œå…¨é¢è¯„ä¼°ALLaM-34Båœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä»¥ç¡®ä¿å…¶åœ¨é˜¿æ‹‰ä¼¯è¯­ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯„ä¼°æµç¨‹åŒ…æ‹¬æ”¶é›†115ä¸ªè¾“å‡ºï¼Œä½¿ç”¨ä¸‰ä½å‰æ²¿LLMè¯„å®¡ï¼ˆGPT-5ã€Gemini 2.5 Proã€Claude Sonnet-4ï¼‰è¿›è¡Œè¯„åˆ†ï¼Œåˆ†æå¾—åˆ†åˆ†å¸ƒå¹¶å¯è§†åŒ–æ–¹è¨€æŒ‡æ ‡çƒ­å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šé€šè¿‡å¼•å…¥å¤šç§ä»»åŠ¡ç±»å‹å’Œæ–¹è¨€ï¼Œæœ¬æ–‡çš„è¯„ä¼°æ–¹æ³•æ¯”ä»¥å¾€æ›´å…¨é¢ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯„ä¼°ä¸­ï¼Œä½¿ç”¨äº†95%çš„ç½®ä¿¡åŒºé—´è®¡ç®—ç±»åˆ«å¹³å‡å€¼ï¼Œç¡®ä¿ç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œå¹¶é€šè¿‡çƒ­å›¾å±•ç¤ºä¸åŒæ–¹è¨€çš„è¡¨ç°å·®å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆå’Œä»£ç åˆ‡æ¢ä»»åŠ¡çš„å¹³å‡å¾—åˆ†ä¸º4.92/5ï¼Œç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­å¤„ç†å¾—åˆ†ä¸º4.74/5ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒALLaM-34Båœ¨ç”Ÿæˆå’Œä»£ç åˆ‡æ¢ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡å¾—åˆ†è¾¾åˆ°4.92/5ï¼Œç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­å¤„ç†å¾—åˆ†ä¸º4.74/5ï¼Œå®‰å…¨æ€§ç›¸å…³æç¤ºçš„è¡¨ç°ä¹Ÿç¨³å®šåœ¨4.54/5ï¼Œæ•´ä½“è¡¨ç°æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æŠ€æœ¯å®åŠ›å’Œå®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é˜¿æ‹‰ä¼¯è¯­æ•™è‚²ã€å®¢æˆ·æœåŠ¡ã€ç¤¾äº¤åª’ä½“äº’åŠ¨ç­‰ã€‚ALLaM-34Bçš„é«˜æ€§èƒ½ä½¿å…¶èƒ½å¤Ÿåœ¨å¤šç§å®é™…åœºæ™¯ä¸­æä¾›æ›´ä¸ºè‡ªç„¶å’Œæ–‡åŒ–é€‚åº”çš„å¯¹è¯ä½“éªŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨é˜¿æ‹‰ä¼¯è¯­å¤„ç†æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) trained primarily on English corpora often struggle to capture the linguistic and cultural nuances of Arabic. To address this gap, the Saudi Data and AI Authority (SDAIA) introduced the $ALLaM$ family of Arabic-focused models. The most capable of these available to the public, $ALLaM-34B$, was subsequently adopted by HUMAIN, who developed and deployed HUMAIN Chat, a closed conversational web service built on this model. This paper presents an expanded and refined UI-level evaluation of $ALLaM-34B$. Using a prompt pack spanning modern standard Arabic, five regional dialects, code-switching, factual knowledge, arithmetic and temporal reasoning, creative generation, and adversarial safety, we collected 115 outputs (23 prompts times 5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro, Claude Sonnet-4). We compute category-level means with 95\% confidence intervals, analyze score distributions, and visualize dialect-wise metric heat maps. The updated analysis reveals consistently high performance on generation and code-switching tasks (both averaging 4.92/5), alongside strong results in MSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect fidelity (4.21/5). Safety-related prompts show stable, reliable performance of (4.54/5). Taken together, these results position $ALLaM-34B$ as a robust and culturally grounded Arabic LLM, demonstrating both technical strength and practical readiness for real-world deployment.

