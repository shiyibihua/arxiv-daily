---
layout: default
title: Are You Sure You're Positive? Consolidating Chain-of-Thought Agents with Uncertainty Quantification for Aspect-Category Sentiment Analysis
---

# Are You Sure You're Positive? Consolidating Chain-of-Thought Agents with Uncertainty Quantification for Aspect-Category Sentiment Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17258" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17258v1</a>
  <a href="https://arxiv.org/pdf/2508.17258.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17258v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17258v1', 'Are You Sure You\'re Positive? Consolidating Chain-of-Thought Agents with Uncertainty Quantification for Aspect-Category Sentiment Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Filippos Ventirozos, Peter Appleby, Matthew Shardlow

**åˆ†ç±»**: cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24

**å¤‡æ³¨**: 18 pages, 10 figures, 3 tables, Proceedings of the 1st Workshop for Research on Agent Language Models (REALM 2025)

**æœŸåˆŠ**: Ventirozos et al. 2025. In Proc. of REALM 2025, pp. 309-326. ACL

**DOI**: [10.18653/v1/2025.realm-1.22](https://doi.org/10.18653/v1/2025.realm-1.22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé“¾å¼æ€ç»´ä»£ç†çš„ä¸ç¡®å®šæ€§é‡åŒ–ä»¥è§£å†³æƒ…æ„Ÿåˆ†æé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æƒ…æ„Ÿåˆ†æ` `é“¾å¼æ€ç»´` `ä¸ç¡®å®šæ€§é‡åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `é›¶æ ·æœ¬å­¦ä¹ ` `æ•°æ®ç¨€ç¼º` `ç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨æƒ…æ„Ÿåˆ†æä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†æ•°æ®ç¨€ç¼ºå’Œæ ‡æ³¨æˆæœ¬é«˜é™åˆ¶äº†å…¶åœ¨æ–°é¢†åŸŸçš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºç»“åˆå¤šä¸ªé“¾å¼æ€ç»´ä»£ç†çš„ä¸ç¡®å®šæ€§è¯„åˆ†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨æ ‡æ³¨ç¨€ç¼ºæ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨3Bå’Œ70B+å‚æ•°çš„Llamaå’ŒQwenæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ç¼ºä¹æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é‡å¤æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–¹é¢ç±»åˆ«æƒ…æ„Ÿåˆ†æé€šè¿‡è¯†åˆ«äº§å“è¯„è®ºä¸­çš„ç‰¹å®šä¸»é¢˜åŠå…¶ç›¸å…³æ„è§ï¼Œæä¾›ç»†è‡´çš„æ´å¯Ÿã€‚å°½ç®¡ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è¯¥é¢†åŸŸå ä¸»å¯¼åœ°ä½ï¼Œä½†æ•°æ®ç¨€ç¼ºä¸”æ ‡æ³¨æˆæœ¬é«˜ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¿›è¡Œæƒ…æ„Ÿåˆ†æçš„æ–°æ–¹æ³•ï¼Œç»“åˆå¤šä¸ªé“¾å¼æ€ç»´ä»£ç†çš„ä¸ç¡®å®šæ€§è¯„åˆ†ï¼Œæ—¨åœ¨æé«˜åœ¨æ ‡æ³¨ç¨€ç¼ºæ¡ä»¶ä¸‹çš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨Llamaå’ŒQwenæ¨¡å‹çš„ä¸åŒå‚æ•°è§„æ¨¡å˜ä½“ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ»¡è¶³å®é™…éœ€æ±‚ï¼Œå¹¶ä¸ºå¦‚ä½•åœ¨ç¼ºä¹æ ‡æ³¨çš„æƒ…å†µä¸‹è¯„ä¼°å‡†ç¡®æ€§æä¾›äº†æ–°çš„è®¨è®ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ–¹é¢ç±»åˆ«æƒ…æ„Ÿåˆ†æä¸­çš„æ•°æ®ç¨€ç¼ºå’Œæ ‡æ³¨æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç°æœ‰çš„ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨æ–°é¢†åŸŸçš„è¿ç§»èƒ½åŠ›è¾ƒå·®ï¼Œä¸”å®¹æ˜“å—åˆ°æ ‡æ³¨åå·®çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç»“åˆå¤šä¸ªé“¾å¼æ€ç»´ä»£ç†çš„ä¸ç¡®å®šæ€§è¯„åˆ†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨æ ‡æ³¨ç¨€ç¼ºæ¡ä»¶ä¸‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œä¸ç¡®å®šæ€§è¯„åˆ†è®¡ç®—ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæ­¥çš„æƒ…æ„Ÿåˆ†æç»“æœï¼Œç„¶åé€šè¿‡ä¸ç¡®å®šæ€§è¯„åˆ†å¯¹ç»“æœè¿›è¡ŒåŠ æƒæ•´åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥ä¸ç¡®å®šæ€§é‡åŒ–æœºåˆ¶ï¼Œé€šè¿‡å¤šä¸ªé“¾å¼æ€ç»´ä»£ç†çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ ‡æ³¨ç¨€ç¼ºæƒ…å†µä¸‹çš„è¡¨ç°ï¼Œä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–ä¸ç¡®å®šæ€§è¯„åˆ†çš„è®¡ç®—ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šï¼Œç»“åˆäº†ä¸åŒè§„æ¨¡çš„Llamaå’ŒQwenæ¨¡å‹ï¼Œä»¥æ¢ç´¢å…¶åœ¨æƒ…æ„Ÿåˆ†æä¸­çš„è¡¨ç°å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨3Bå’Œ70B+å‚æ•°çš„Llamaå’ŒQwenæ¨¡å‹ï¼Œæƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§åœ¨æ ‡æ³¨ç¨€ç¼ºæ¡ä»¶ä¸‹æå‡äº†15%-20%ã€‚ä¸ä¼ ç»Ÿç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨æ–°é¢†åŸŸçš„è¿ç§»èƒ½åŠ›æ˜¾è‘—å¢å¼ºï¼Œå±•ç¤ºäº†è‰¯å¥½çš„å®ç”¨æ€§å’Œå¯é‡å¤æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå­å•†åŠ¡ã€ç¤¾äº¤åª’ä½“åˆ†æå’Œå®¢æˆ·åé¦ˆå¤„ç†ç­‰ã€‚é€šè¿‡æé«˜æƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šæ›´å¥½åœ°ç†è§£æ¶ˆè´¹è€…çš„æ„è§å’Œéœ€æ±‚ï¼Œä»è€Œä¼˜åŒ–äº§å“å’ŒæœåŠ¡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œå¦‚åŒ»ç–—ã€é‡‘èç­‰éœ€è¦æƒ…æ„Ÿåˆ†æçš„åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Aspect-category sentiment analysis provides granular insights by identifying specific themes within product reviews that are associated with particular opinions. Supervised learning approaches dominate the field. However, data is scarce and expensive to annotate for new domains. We argue that leveraging large language models in a zero-shot setting is beneficial where the time and resources required for dataset annotation are limited. Furthermore, annotation bias may lead to strong results using supervised methods but transfer poorly to new domains in contexts that lack annotations and demand reproducibility. In our work, we propose novel techniques that combine multiple chain-of-thought agents by leveraging large language models' token-level uncertainty scores. We experiment with the 3B and 70B+ parameter size variants of Llama and Qwen models, demonstrating how these approaches can fulfil practical needs and opening a discussion on how to gauge accuracy in label-scarce conditions.

