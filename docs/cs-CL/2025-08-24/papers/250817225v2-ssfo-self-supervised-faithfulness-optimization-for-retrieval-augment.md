---
layout: default
title: SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation
---

# SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17225" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17225v2</a>
  <a href="https://arxiv.org/pdf/2508.17225.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17225v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17225v2', 'SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaqiang Tang, Yi Wang, Keyu Hu, Rui Xu, Chuang Li, Weigao Sun, Jian Li, Sihong Xie

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24 (æ›´æ–°: 2025-10-04)

**å¤‡æ³¨**: Working in progress

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/chkwy/SSFO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç›‘ç£ä¿¡åº¦ä¼˜åŒ–æ–¹æ³•ä»¥è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„ä¿¡åº¦é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `ä¿¡åº¦ä¼˜åŒ–` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `ç›´æ¥åå¥½ä¼˜åŒ–` `è·¨è¯­è¨€èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RAGç³»ç»Ÿåœ¨ç”Ÿæˆå“åº”æ—¶é¢ä¸´ä¿¡åº¦å¹»è§‰çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹ä¸æ£€ç´¢ä¸Šä¸‹æ–‡ä¸ä¸€è‡´ã€‚
2. æœ¬æ–‡æå‡ºçš„SSFOæ–¹æ³•é€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ„å»ºåå¥½æ•°æ®å¯¹ï¼Œåˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–å¯¹æ¨¡å‹ä¿¡åº¦è¿›è¡Œå¯¹é½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSSFOåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†ä¿¡åº¦è¡¨ç°ï¼Œå¹¶ä¸”åœ¨è·¨è¯­è¨€ä¿¡åº¦å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸Šè¡¨ç°å‡ºå¼ºæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿè¦æ±‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆä¸æ£€ç´¢ä¸Šä¸‹æ–‡ä¸€è‡´çš„å“åº”ã€‚ç„¶è€Œï¼Œä¿¡åº¦å¹»è§‰ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦æ˜‚è´µçš„ç›‘ç£å’Œåè®­ç»ƒæˆ–æ˜¾è‘—çš„æ¨ç†è´Ÿæ‹…ã€‚ä¸ºå…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªç›‘ç£ä¿¡åº¦ä¼˜åŒ–ï¼ˆSSFOï¼‰ï¼Œè¿™æ˜¯å¢å¼ºRAGä¿¡åº¦çš„é¦–ä¸ªè‡ªç›‘ç£å¯¹é½æ–¹æ³•ã€‚SSFOé€šè¿‡å¯¹æ¯”æ¨¡å‹åœ¨æœ‰æ— ä¸Šä¸‹æ–‡æƒ…å†µä¸‹ç”Ÿæˆçš„è¾“å‡ºæ„å»ºåå¥½æ•°æ®å¯¹ã€‚åˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼ŒSSFOåœ¨ä¸äº§ç”Ÿæ ‡æ³¨æˆæœ¬æˆ–é¢å¤–æ¨ç†è´Ÿæ‹…çš„æƒ…å†µä¸‹å¯¹é½æ¨¡å‹ä¿¡åº¦ã€‚æˆ‘ä»¬ç†è®ºå’Œå®è¯è¯æ˜SSFOåˆ©ç”¨äº†ä¸€ç§è‰¯æ€§çš„ä¼¼ç„¶ä½ç§»å½¢å¼ï¼Œå°†æ¦‚ç‡è´¨é‡ä»åŸºäºå‚æ•°çš„æ ‡è®°è½¬ç§»åˆ°ä¸Šä¸‹æ–‡å¯¹é½çš„æ ‡è®°ä¸Šã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¿®æ”¹åçš„DPOæŸå¤±å‡½æ•°ä»¥é¼“åŠ±ä¼¼ç„¶ä½ç§»ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒSSFOåœ¨å¤šä¸ªåŸºäºä¸Šä¸‹æ–‡çš„é—®é¢˜å›ç­”æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ä¿¡åº¦æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­ä¿¡åº¦å¹»è§‰çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–æ˜‚è´µçš„ç›‘ç£å’Œåè®­ç»ƒï¼Œå¯¼è‡´æ¨ç†è´Ÿæ‹…åŠ é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSSFOé€šè¿‡è‡ªç›‘ç£æ–¹å¼æ„å»ºåå¥½æ•°æ®å¯¹ï¼Œæ¯”è¾ƒæ¨¡å‹åœ¨æœ‰æ— ä¸Šä¸‹æ–‡æƒ…å†µä¸‹çš„è¾“å‡ºï¼Œä»è€Œä¼˜åŒ–ä¿¡åº¦ã€‚åˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•ï¼ŒSSFOèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ ‡æ³¨æˆæœ¬çš„æƒ…å†µä¸‹å®ç°ä¿¡åº¦å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSSFOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å¯¹æ„å»ºã€åå¥½å­¦ä¹ å’Œä¿¡åº¦ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹æ¯”ç”Ÿæˆçš„è¾“å‡ºæ„å»ºåå¥½æ•°æ®å¯¹ï¼›ç„¶åï¼Œåˆ©ç”¨DPOè¿›è¡Œä¿¡åº¦ä¼˜åŒ–ï¼›æœ€åï¼Œè¯„ä¼°æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šSSFOçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸€ç§è‡ªç›‘ç£çš„ä¿¡åº¦ä¼˜åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨ä¼¼ç„¶ä½ç§»çš„æ¦‚å¿µï¼Œå°†æ¦‚ç‡è´¨é‡ä»ä¸ç›¸å…³çš„æ ‡è®°è½¬ç§»åˆ°ä¸ä¸Šä¸‹æ–‡å¯¹é½çš„æ ‡è®°ä¸Šï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼ŒSSFOæå‡ºäº†ä¿®æ”¹åçš„DPOæŸå¤±å‡½æ•°ï¼Œä»¥é¼“åŠ±æ¨¡å‹åœ¨ç”Ÿæˆæ—¶å®ç°ä¼¼ç„¶ä½ç§»ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç»“æ„å’Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSSFOåœ¨å¤šä¸ªåŸºäºä¸Šä¸‹æ–‡çš„é—®é¢˜å›ç­”æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ä¿¡åº¦æ°´å¹³ã€‚å…·ä½“è€Œè¨€ï¼ŒSSFOåœ¨ä¿¡åº¦è¯„ä¼°æŒ‡æ ‡ä¸Šæå‡å¹…åº¦è¶…è¿‡äº†X%ï¼Œå¹¶åœ¨è·¨è¯­è¨€ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜ç”Ÿæˆå†…å®¹çš„ä¿¡åº¦ï¼ŒSSFOå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œå‡å°‘è¯¯å¯¼ä¿¡æ¯çš„ç”Ÿæˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) systems require Large Language Models (LLMs) to generate responses that are faithful to the retrieved context. However, faithfulness hallucination remains a critical challenge, as existing methods often require costly supervision and post-training or significant inference burdens. To overcome these limitations, we introduce Self-Supervised Faithfulness Optimization (SSFO), the first self-supervised alignment approach for enhancing RAG faithfulness. SSFO constructs preference data pairs by contrasting the model's outputs generated with and without the context. Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness without incurring labeling costs or additional inference burden. We theoretically and empirically demonstrate that SSFO leverages a benign form of \emph{likelihood displacement}, transferring probability mass from parametric-based tokens to context-aligned tokens. Based on this insight, we propose a modified DPO loss function to encourage likelihood displacement. Comprehensive evaluations show that SSFO significantly outperforms existing methods, achieving state-of-the-art faithfulness on multiple context-based question-answering datasets. Notably, SSFO exhibits strong generalization, improving cross-lingual faithfulness and preserving general instruction-following capabilities. We release our code and model at the anonymous link: https://github.com/chkwy/SSFO

