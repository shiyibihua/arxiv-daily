---
layout: default
title: LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions
---

# LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18321" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18321v3</a>
  <a href="https://arxiv.org/pdf/2508.18321.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18321v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18321v3', 'LLMs Can\'t Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maojia Song, Tej Deep Pala, Ruiwen Zhou, Weisheng Jin, Amir Zadeh, Chuan Li, Dorien Herremans, Soujanya Poria

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24 (æ›´æ–°: 2025-12-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKAIROSåŸºå‡†ä»¥è§£å†³LLMsåœ¨å¤šæ™ºèƒ½ä½“ç¤¾äº¤äº’åŠ¨ä¸­çš„è„†å¼±æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `ç¤¾äº¤äº’åŠ¨` `å†³ç­–èƒ½åŠ›` `ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–` `èæ´½å…³ç³»` `ä¿¡æ¯æ•´åˆ` `ä»ä¼—åè§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºä»ä¼—åè§ï¼Œæœªèƒ½å…¨é¢æ¢è®¨LLMsåœ¨å¤šæ™ºèƒ½ä½“ç¤¾äº¤äº’åŠ¨ä¸­çš„è¡¨ç°å’Œè„†å¼±æ€§ã€‚
2. è®ºæ–‡æå‡ºKAIROSåŸºå‡†ï¼Œé€šè¿‡æ¨¡æ‹Ÿæµ‹éªŒå¼åä½œï¼Œç³»ç»Ÿåˆ†æèæ´½å…³ç³»å’ŒåŒä¼´è¡Œä¸ºå¯¹å†³ç­–çš„å½±å“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹è§„æ¨¡å½±å“ç¤¾ä¼šå½±å“çš„æ˜“æ„Ÿæ€§ï¼Œè¾ƒå¤§æ¨¡å‹æ›´å…·éŸ§æ€§ï¼Œè€Œå°æ¨¡å‹éœ€é€šè¿‡GRPOè®­ç»ƒæå‡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«é›†æˆåˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä¸­ï¼Œå…¶ä¸­åŒä¼´äº’åŠ¨å½±å“ä¸ªä½“å†³ç­–ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨ä»ä¼—åè§ï¼Œä½†æˆ‘ä»¬æ‰©å±•äº†è§†è§’ï¼Œæ¢è®¨LLMså¦‚ä½•ä»å…ˆå‰çš„äº’åŠ¨ä¸­å»ºç«‹èæ´½å…³ç³»ã€è¯†åˆ«å’Œæ•´åˆé«˜è´¨é‡çš„åŒä¼´ä¿¡æ¯ï¼Œä»¥åŠæŠµæŠ—è¯¯å¯¼æ€§è¾“å…¥çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†KAIROSï¼Œä¸€ä¸ªæ¨¡æ‹Ÿæµ‹éªŒå¼åä½œçš„åŸºå‡†ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶åŒä¼´ä»£ç†çš„èæ´½æ°´å¹³å’Œè¡Œä¸ºã€‚è¿™ä¸€ç»Ÿä¸€çš„è®¾ç½®ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿç³»ç»Ÿåˆ†æèæ´½å…³ç³»ã€åŒä¼´è¡Œä¸ºå’Œæ¨¡å‹è‡ªä¿¡å¿ƒå¦‚ä½•å…±åŒå½±å“å†³ç­–ã€‚ä½¿ç”¨KAIROSï¼Œæˆ‘ä»¬è¯„ä¼°äº†æç¤ºã€ç›‘ç£å¾®è°ƒå’Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹è§„æ¨¡æ˜¯è°ƒèŠ‚ç¤¾ä¼šå½±å“æ˜“æ„Ÿæ€§çš„ä¸»è¦å› ç´ ï¼šè¾ƒå¤§çš„æ¨¡å‹æ›´å…·éŸ§æ€§ï¼Œå¹¶ä»åŸºäºæç¤ºçš„ç¼“è§£ä¸­å—ç›Šï¼Œè€Œè¾ƒå°çš„æ¨¡å‹åˆ™ä»ç„¶è„†å¼±ã€‚åªæœ‰ç»è¿‡ç²¾å¿ƒé…ç½®çš„GRPOè®­ç»ƒæ‰èƒ½ä¸ºå°æ¨¡å‹å¸¦æ¥ä¸€è‡´çš„é²æ£’æ€§å’Œæ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“ç¤¾äº¤äº’åŠ¨ä¸­è„†å¼±æ€§çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä»ä¼—åè§ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘èæ´½å…³ç³»å’Œä¿¡æ¯æ•´åˆçš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥KAIROSåŸºå‡†ï¼Œæ¨¡æ‹ŸåŒä¼´ä»£ç†çš„äº’åŠ¨ï¼Œç ”ç©¶å¦‚ä½•é€šè¿‡å†å²äº’åŠ¨å»ºç«‹èæ´½å…³ç³»ä»¥åŠå¦‚ä½•æŠµæŠ—è¯¯å¯¼æ€§è¾“å…¥ï¼Œä»è€Œæå‡LLMsåœ¨å¤æ‚ç¤¾äº¤åŠ¨æ€ä¸‹çš„å†³ç­–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKAIROSåŸºå‡†åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯åŒä¼´ä»£ç†çš„è¡Œä¸ºæ§åˆ¶ï¼Œå…¶æ¬¡æ˜¯èæ´½å…³ç³»çš„å»ºç«‹ï¼Œæœ€åæ˜¯å†³ç­–è¿‡ç¨‹çš„åˆ†æã€‚æ¨¡å‹é€šè¿‡ä¸åŒçš„è®­ç»ƒæ–¹å¼ï¼ˆæç¤ºã€å¾®è°ƒã€GRPOï¼‰è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥KAIROSåŸºå‡†æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„å®éªŒç¯å¢ƒï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°åˆ†æèæ´½å…³ç³»ã€åŒä¼´è¡Œä¸ºå’Œæ¨¡å‹è‡ªä¿¡å¿ƒå¯¹å†³ç­–çš„å½±å“ï¼Œè¿™åœ¨ç°æœ‰ç ”ç©¶ä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GRPOè®­ç»ƒä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„å‚æ•°é…ç½®ï¼Œä»¥ç¡®ä¿å°æ¨¡å‹åœ¨é¢å¯¹ç¤¾äº¤å½±å“æ—¶èƒ½å¤Ÿè·å¾—ä¸€è‡´çš„é²æ£’æ€§å’Œæ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹è§„æ¨¡æ˜¯å½±å“ç¤¾ä¼šå½±å“æ˜“æ„Ÿæ€§çš„å…³é”®å› ç´ ã€‚è¾ƒå¤§æ¨¡å‹åœ¨é¢å¯¹ç¤¾äº¤å½±å“æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„éŸ§æ€§ï¼Œå¹¶ä¸”é€šè¿‡æç¤ºæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡å…¶æ€§èƒ½ï¼Œè€Œå°æ¨¡å‹åˆ™éœ€è¦ç»è¿‡ç²¾å¿ƒé…ç½®çš„GRPOè®­ç»ƒæ‰èƒ½è·å¾—ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤æœºå™¨äººã€æ™ºèƒ½åŠ©æ‰‹å’Œå¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡LLMsåœ¨ç¤¾äº¤äº’åŠ¨ä¸­çš„å†³ç­–èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºå…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ï¼Œæ¨åŠ¨äººæœºåä½œçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are increasingly integrated into multi-agent systems (MAS), where peer interactions shape individual decisions. While prior work has mainly examined conformity bias, we broaden the view to include how LLMs build rapport from prior interactions, discern and integrate high-quality peer information, and resist misleading inputs-abilities essential for achieving collective intelligence under complex social dynamics. We introduce KAIROS, a benchmark that simulates quiz-style collaboration with peer agents whose rapport levels and behaviours can be precisely controlled in both historical interactions and the current round. This unified setup enables systematic analysis of how rapport, peer actions, and the model's self-confidence jointly influence decision-making. Using KAIROS, we evaluate prompting, supervised fine-tuning, and reinforcement learning via Group Relative Policy Optimisation (GRPO). Results show that model scale is a primary factor moderating susceptibility to social influence: larger models are more resilient and benefit from prompting-based mitigation, whereas smaller models remain vulnerable. Only carefully configured GRPO training yields consistent robustness and performance gains for small models.

