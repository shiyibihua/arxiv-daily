---
layout: default
title: DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning
---

# DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17337" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17337v1</a>
  <a href="https://arxiv.org/pdf/2508.17337.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17337v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17337v1', 'DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haojie Zhang

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24

**å¤‡æ³¨**: 8 pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TayeeChang/DropLoRA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDropLoRAä»¥è§£å†³ä½ç§©é€‚åº”æ–¹æ³•æ€§èƒ½ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©é€‚åº”` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `åŠ¨æ€å­ç©ºé—´å­¦ä¹ ` `å‰ªææ–¹æ³•` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LoRAæ–¹æ³•åœ¨ä½ç§©æ›´æ–°æ–¹é¢å­˜åœ¨æ€§èƒ½ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡æ•ˆæœä¸ä½³ã€‚
2. DropLoRAé€šè¿‡åœ¨ä½ç§©çŸ©é˜µä¹‹é—´å¼•å…¥å‰ªææ¨¡å—ï¼Œå®ç°åŠ¨æ€å­ç©ºé—´å­¦ä¹ ï¼Œä»è€Œå…‹æœä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDropLoRAåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†LoRAï¼Œæå‡å¹…åº¦æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºLoRAçš„å¤§å‹æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ä½¿ç”¨ä½ç§©åˆ†è§£æ¥è¿‘ä¼¼æ¨¡å‹å‚æ•°çš„æ›´æ–°ã€‚ç„¶è€Œï¼Œä¸å…¨å‚æ•°å¾®è°ƒç›¸æ¯”ï¼Œä½ç§©æ›´æ–°åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å¾€å¾€å¯¼è‡´æ€§èƒ½å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DropLoRAï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºå‰ªæçš„æ–¹æ³•ï¼Œä¸“æ³¨äºå‰ªæç§©ç»´åº¦ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒDropLoRAåœ¨LoRAçš„ä¸¤ä¸ªä½ç§©çŸ©é˜µä¹‹é—´åˆ›æ–°æ€§åœ°é›†æˆäº†å‰ªææ¨¡å—ï¼Œä»¥æ¨¡æ‹ŸåŠ¨æ€å­ç©ºé—´å­¦ä¹ ã€‚è¿™ç§åŠ¨æ€ä½ç§©å­ç©ºé—´å­¦ä¹ ä½¿DropLoRAèƒ½å¤Ÿå…‹æœä¼ ç»ŸLoRAåœ¨é™æ€å­ç©ºé—´å†…çš„å±€é™æ€§ã€‚é€šè¿‡æŒç»­é€‚åº”å­¦ä¹ å­ç©ºé—´ï¼ŒDropLoRAæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè€Œæ²¡æœ‰å¢åŠ é¢å¤–çš„è®­ç»ƒæˆ–æ¨ç†æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDropLoRAåœ¨å¾®è°ƒLLaMAç³»åˆ—æ¨¡å‹æ—¶ï¼Œåœ¨å¸¸è¯†æ¨ç†ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’ŒæŒ‡ä»¤è·Ÿéšç­‰å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå§‹ç»ˆä¼˜äºLoRAã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰LoRAæ–¹æ³•åœ¨ä½ç§©é€‚åº”ä¸­å¯¼è‡´çš„æ€§èƒ½ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°å·®è·ã€‚ä¼ ç»Ÿçš„ä½ç§©æ›´æ–°æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰æ¨¡å‹å‚æ•°çš„åŠ¨æ€å˜åŒ–ï¼Œé™åˆ¶äº†æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDropLoRAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨ä¸¤ä¸ªä½ç§©çŸ©é˜µä¹‹é—´å¼•å…¥å‰ªææ¨¡å—ï¼Œæ¨¡æ‹ŸåŠ¨æ€å­ç©ºé—´å­¦ä¹ ã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¸æ–­è°ƒæ•´å­¦ä¹ å­ç©ºé—´ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDropLoRAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå‰ªææ¨¡å—å’Œä¸¤ä¸ªä½ç§©çŸ©é˜µã€‚å‰ªææ¨¡å—è´Ÿè´£åŠ¨æ€è°ƒæ•´ä½ç§©çŸ©é˜µçš„ç§©ç»´åº¦ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬åˆå§‹ä½ç§©çŸ©é˜µçš„ç”Ÿæˆã€å‰ªææ¨¡å—çš„åº”ç”¨ä»¥åŠæœ€ç»ˆæ¨¡å‹çš„å¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šDropLoRAçš„æœ€é‡è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åŠ¨æ€å­ç©ºé—´å­¦ä¹ æœºåˆ¶ï¼Œé€šè¿‡å‰ªææ¨¡å—å®ç°å¯¹ä½ç§©çŸ©é˜µçš„åŠ¨æ€è°ƒæ•´ã€‚è¿™ä¸€åˆ›æ–°ä¸ä¼ ç»Ÿçš„é™æ€ä½ç§©æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´çµæ´»åœ°é€‚åº”ä¸åŒçš„ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DropLoRAä¸­ï¼Œå‰ªææ¨¡å—çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œå…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿå½±å“æ¨¡å‹çš„å¾®è°ƒæ•ˆæœã€‚è®ºæ–‡ä¸­è¯¦ç»†æè¿°äº†å‰ªæç­–ç•¥å’Œä½ç§©çŸ©é˜µçš„æ„å»ºæ–¹æ³•ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒé«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDropLoRAåœ¨å¾®è°ƒLLaMAç³»åˆ—æ¨¡å‹æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„LoRAæ–¹æ³•ã€‚åœ¨å¸¸è¯†æ¨ç†ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’ŒæŒ‡ä»¤è·Ÿéšç­‰ä»»åŠ¡ä¸­ï¼ŒDropLoRAçš„è¡¨ç°æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œå…·ä½“æ€§èƒ½æ•°æ®åœ¨è®ºæ–‡ä¸­è¯¦ç»†åˆ—å‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DropLoRAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚å…¶åŠ¨æ€å­ç©ºé—´å­¦ä¹ æœºåˆ¶èƒ½å¤Ÿæé«˜æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„é€‚åº”æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šé«˜æ•ˆçš„æ¨¡å‹å¾®è°ƒæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> LoRA-based large model parameter-efficient fine-tuning (PEFT) methods use low-rank de- composition to approximate updates to model parameters. However, compared to full- parameter fine-tuning, low-rank updates often lead to a performance gap in downstream tasks. To address this, we introduce DropLoRA, a novel pruning-based approach that focuses on pruning the rank dimension. Unlike conven- tional methods that attempt to overcome the low-rank bottleneck, DropLoRA innovatively integrates a pruning module between the two low-rank matrices in LoRA to simulate dy- namic subspace learning. This dynamic low- rank subspace learning allows DropLoRA to overcome the limitations of traditional LoRA, which operates within a static subspace. By continuously adapting the learning subspace, DropLoRA significantly boosts performance without incurring additional training or infer- ence costs. Our experimental results demon- strate that DropLoRA consistently outperforms LoRA in fine-tuning the LLaMA series across a wide range of large language model gener- ation tasks, including commonsense reason- ing, mathematical reasoning, code generation, and instruction-following. Our code is avail- able at https://github.com/TayeeChang/DropLoRA.

