---
layout: default
title: Speed Always Wins: A Survey on Efficient Architectures for Large Language Models
---

# Speed Always Wins: A Survey on Efficient Architectures for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09834" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09834v1</a>
  <a href="https://arxiv.org/pdf/2508.09834.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09834v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09834v1', 'Speed Always Wins: A Survey on Efficient Architectures for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

**å¤‡æ³¨**: Survey, 82 pages, GitHub: https://github.com/weigao266/Awesome-Efficient-Arch

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆæ¶æ„ä»¥è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®¡ç®—ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `Transformer` `è®¡ç®—æ•ˆç‡` `ç¨€ç–å»ºæ¨¡` `æ··åˆä¸“å®¶` `å¤šæ¨¡æ€AI` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Transformeræ¶æ„åœ¨è®¡ç®—æ•ˆç‡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œåº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç³»åˆ—åˆ›æ–°çš„LLMæ¶æ„ï¼Œé‡‡ç”¨çº¿æ€§å’Œç¨€ç–å»ºæ¨¡æ–¹æ³•ç­‰æŠ€æœ¯æ¥æå‡è®¡ç®—æ•ˆç‡ã€‚
3. é€šè¿‡å¯¹æ¯”å®éªŒï¼Œå±•ç¤ºäº†æ–°æ¶æ„åœ¨èµ„æºåˆ©ç”¨ç‡å’Œè®­ç»ƒé€Ÿåº¦ä¸Šçš„æ˜¾è‘—æå‡ï¼Œæ¨åŠ¨äº†å¯æ‰©å±•åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç†è§£ã€ç”Ÿæˆå’Œæ¨ç†ç­‰æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä¼ ç»Ÿçš„Transformeræ¶æ„åœ¨è®¡ç®—ä¸Šéœ€æ±‚å·¨å¤§ï¼Œé™åˆ¶äº†å…¶å¤§è§„æ¨¡è®­ç»ƒå’Œå®é™…éƒ¨ç½²çš„èƒ½åŠ›ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°å®¡è§†äº†åˆ›æ–°çš„LLMæ¶æ„ï¼Œæ—¨åœ¨è§£å†³Transformerå›ºæœ‰çš„å±€é™æ€§å¹¶æå‡æ•ˆç‡ã€‚å†…å®¹æ¶µç›–çº¿æ€§å’Œç¨€ç–åºåˆ—å»ºæ¨¡æ–¹æ³•ã€é«˜æ•ˆå…¨æ³¨æ„åŠ›å˜ä½“ã€ç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹ä»¥åŠæ–°å…´çš„æ‰©æ•£LLMsç­‰æŠ€æœ¯ã€‚é€šè¿‡å¯¹è¿™äº›æŠ€æœ¯çš„åˆ†ç±»è®¨è®ºï¼Œæœ¬æ–‡ä¸ºç°ä»£é«˜æ•ˆLLMæ¶æ„æä¾›äº†è“å›¾ï¼ŒæœŸæœ›èƒ½æ¿€åŠ±æœªæ¥åœ¨æ›´é«˜æ•ˆã€å¤šæ ·åŒ–çš„AIç³»ç»Ÿæ–¹é¢çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸTransformeræ¶æ„åœ¨è®¡ç®—æ•ˆç‡ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡è®­ç»ƒå’Œå®é™…åº”ç”¨ä¸­çš„é«˜è®¡ç®—éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥çº¿æ€§å’Œç¨€ç–åºåˆ—å»ºæ¨¡æ–¹æ³•ï¼Œä»¥åŠé«˜æ•ˆçš„å…¨æ³¨æ„åŠ›å˜ä½“ï¼Œæå‡LLMçš„è®¡ç®—æ•ˆç‡ï¼Œé™ä½èµ„æºæ¶ˆè€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬çº¿æ€§åºåˆ—å»ºæ¨¡ã€ç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹å’Œæ–°å…´çš„æ‰©æ•£LLMsç­‰æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªé«˜æ•ˆçš„LLMä½“ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†å¤šç§é«˜æ•ˆå»ºæ¨¡æŠ€æœ¯ï¼Œå½¢æˆäº†ä¸€ä¸ªå¤šå±‚æ¬¡çš„æ¶æ„è®¾è®¡ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å’Œæ··åˆä¸“å®¶ç­–ç•¥ï¼Œä¼˜åŒ–äº†å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥å®ç°æ›´é«˜çš„è®­ç»ƒæ•ˆç‡å’Œæ›´ä½çš„è®¡ç®—æˆæœ¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°æ¶æ„çš„æ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡ä¸Šç›¸æ¯”ä¼ ç»ŸTransformeræå‡äº†50%ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šä¿æŒäº†ç›¸ä¼¼çš„æ€§èƒ½æ°´å¹³ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œå¤šæ¨¡æ€AIç­‰ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹å®ç°é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.

