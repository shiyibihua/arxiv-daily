---
layout: default
title: Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs
---

# Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10142" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10142v3</a>
  <a href="https://arxiv.org/pdf/2508.10142.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10142v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10142v3', 'Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kartikeya Badola, Jonathan Simon, Arian Hosseini, Sara Marie Mc Carthy, Tsendsuren Munkhdalai, Abhimanyu Goyal, TomÃ¡Å¡ KoÄiskÃ½, Shyam Upadhyay, Bahare Fatemi, Mehran Kazemi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-08-24)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè½®ä»»åŠ¡åŸºå‡†ä»¥è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¸å¯¹è¯èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¤šè½®å¯¹è¯` `æ¨ç†èƒ½åŠ›` `ä¿¡æ¯è·å–` `ä»»åŠ¡åŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚å’Œäº¤äº’æ€§ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å’Œä¿¡æ¯è·å–æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šè½®ä»»åŠ¡åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMsçš„æ¨ç†å’Œå¯¹è¯èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šçš„è¡¨ç°å­˜åœ¨æ˜¾è‘—æå‡ç©ºé—´ï¼Œä¸»è¦é”™è¯¯æ¥æºäºæŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ¸…æ™°å®Œæ•´çš„é—®é¢˜æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤æ‚çš„äº¤äº’ä»»åŠ¡ä¸­å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºå‡†ï¼ŒåŒ…å«ä¸€ç³»åˆ—å¤šè½®ä»»åŠ¡ï¼Œæ—¨åœ¨æµ‹è¯•æ¨¡å‹çš„æ¨ç†ã€äº’åŠ¨å¯¹è¯å’Œä¿¡æ¯è·å–èƒ½åŠ›ã€‚è¿™äº›ä»»åŠ¡é‡‡ç”¨ç¡®å®šæ€§çš„è¯„åˆ†æœºåˆ¶ï¼Œé¿å…äº†äººå·¥å¹²é¢„çš„éœ€æ±‚ã€‚å¯¹å‰æ²¿æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æå‡ç©ºé—´ï¼Œåˆ†æè¡¨æ˜å¤§å¤šæ•°é”™è¯¯æºäºæŒ‡ä»¤éµå¾ªä¸å½“ã€æ¨ç†å¤±è´¥å’Œè§„åˆ’ä¸è¶³ã€‚è¯¥åŸºå‡†ä¸ºå½“å‰LLMsåœ¨å¤„ç†å¤æ‚äº¤äº’åœºæ™¯ä¸­çš„ä¼˜ç¼ºç‚¹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†åšå®çš„å¹³å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸­çš„æ¨ç†å’Œå¯¹è¯èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šè½®å¯¹è¯å’Œä¿¡æ¯è·å–æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ¨ç†é”™è¯¯å’Œä¿¡æ¯é—æ¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€å¥—å¤šè½®ä»»åŠ¡åŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæµ‹è¯•æ¨¡å‹åœ¨æ¨ç†ã€äº’åŠ¨å¯¹è¯å’Œä¿¡æ¯è·å–æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ç¡®å®šæ€§çš„è¯„åˆ†æœºåˆ¶ï¼Œå‡å°‘äº†äººå·¥å¹²é¢„çš„éœ€æ±‚ï¼Œä»è€Œæé«˜è¯„ä¼°çš„å®¢è§‚æ€§å’Œä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªå¤šè½®ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡é’ˆå¯¹ç‰¹å®šçš„æ¨ç†æˆ–å¯¹è¯èƒ½åŠ›è¿›è¡Œè®¾è®¡ã€‚æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œåˆ†æå…¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè®¾è®¡äº†ä¸€å¥—ç³»ç»ŸåŒ–çš„å¤šè½®ä»»åŠ¡åŸºå‡†ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°LLMsåœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸­çš„è¡¨ç°ã€‚è¿™ç§æ–¹æ³•ä¸ç°æœ‰çš„å•ä¸€ä»»åŠ¡è¯„ä¼°æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œæä¾›äº†æ›´æ·±å…¥çš„åˆ†æè§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šä»»åŠ¡è®¾è®¡ä¸­é‡‡ç”¨äº†ç¡®å®šæ€§çš„è¯„åˆ†æœºåˆ¶ï¼Œç¡®ä¿è¯„ä¼°çš„ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œä»»åŠ¡å†…å®¹æ¶µç›–äº†å¤šç§æ¨ç†å’Œå¯¹è¯åœºæ™¯ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒæƒ…å†µä¸‹çš„èƒ½åŠ›å¾—åˆ°å…¨é¢æµ‹è¯•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰å‰æ²¿æ¨¡å‹åœ¨å¤šè½®ä»»åŠ¡åŸºå‡†ä¸Šçš„è¡¨ç°å­˜åœ¨æ˜¾è‘—æå‡ç©ºé—´ï¼Œä¸»è¦é”™è¯¯é›†ä¸­åœ¨æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›æ–¹é¢ã€‚å…·ä½“æ•°æ®æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æå‡å¹…åº¦å¯è¾¾20%ï¼Œæ˜¾ç¤ºå‡ºè¯¥åŸºå‡†çš„æœ‰æ•ˆæ€§å’ŒæŒ‘æˆ˜æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¯¹è¯ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†ä¹Ÿå¯ä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›å‚è€ƒï¼Œä¿ƒè¿›æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) excel at solving problems with clear and complete statements, but often struggle with nuanced environments or interactive tasks which are common in most real-world scenarios. This highlights the critical need for developing LLMs that can effectively engage in logically consistent multi-turn dialogue, seek information and reason with incomplete data. To this end, we introduce a novel benchmark comprising a suite of multi-turn tasks each designed to test specific reasoning, interactive dialogue, and information-seeking abilities. These tasks have deterministic scoring mechanisms, thus eliminating the need for human intervention. Evaluating frontier models on our benchmark reveals significant headroom. Our analysis shows that most errors emerge from poor instruction following, reasoning failures, and poor planning. This benchmark provides valuable insights into the strengths and weaknesses of current LLMs in handling complex, interactive scenarios and offers a robust platform for future research aimed at improving these critical capabilities.

