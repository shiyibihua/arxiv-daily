---
layout: default
title: Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation
---

# Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09666" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.09666v2</a>
  <a href="https://arxiv.org/pdf/2508.09666.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09666v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09666v2', 'Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ziyang Ma, Qingyue Yuan, Linhai Zhang, Deyu Zhou

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-13 (Êõ¥Êñ∞: 2025-08-15)

**Â§áÊ≥®**: Preprint

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SlowED‰ª•Ëß£ÂÜ≥Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂÆâÂÖ®ÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â∞èÂûãËØ≠Ë®ÄÊ®°Âûã` `ÈìæÂºèÊÄùÁª¥` `Ëí∏È¶èËÆ≠ÁªÉ` `Ê®°ÂûãÂÆâÂÖ®ÊÄß` `Êé®ÁêÜËÉΩÂäõ` `‰ΩéÁÜµÂ±èËîΩ` `ÊÖ¢Ë∞É‰ºò`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈìæÂºèÊÄùÁª¥Ëí∏È¶èÊñπÊ≥ïÂú®ÊèêÂçáÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂêåÊó∂ÔºåÂøΩËßÜ‰∫ÜËÆ≠ÁªÉÂØπÊ®°ÂûãÂÆâÂÖ®ÊÄßÁöÑË¥üÈù¢ÂΩ±Âìç„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑSLowEDÊñπÊ≥ïÈÄöËøáSlow TuningÂíåLow-Entropy MaskingÊ®°ÂùóÔºå‰ºòÂåñÊ®°ÂûãÊùÉÈáçÂπ∂Â±èËîΩ‰∏çÂøÖË¶ÅÁöÑÂ≠¶‰π†ÁõÆÊ†áÔºå‰ª•Á°Æ‰øùÊ®°ÂûãÂÆâÂÖ®ÊÄß„ÄÇ
3. Âú®ÂØπ‰∏âÁßçÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆûÈ™å‰∏≠ÔºåSLowEDÂú®Êé®ÁêÜÂü∫ÂáÜÂíåÂÆâÂÖ®ËØÑ‰º∞‰∏äÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÂÆâÂÖ®ÊÄßÂæó‰ª•‰øùÊåÅ‰∏îÊé®ÁêÜËÉΩÂäõÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑÈìæÂºèÊÄùÁª¥Ëí∏È¶èÊñπÊ≥ï‰∏ªË¶ÅÈÄöËøáÂà©Áî®Âº∫Â§ßÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÈ´òË¥®ÈáèÁöÑÊé®ÁêÜÊù•Â¢ûÂº∫Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂØπÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂÆâÂÖ®ÊÄßÈÄ†ÊàêÁöÑË¥üÈù¢ÂΩ±ÂìçÈ≤úÊúâÁ†îÁ©∂ÂÖ≥Ê≥®„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÆâÂÖ®Ëí∏È¶èÊñπÊ≥ïSlow TuningÂíåLow-Entropy Masking DistillationÔºàSLowEDÔºâÔºåÈÄöËøáÂáèÂ∞èÊ®°ÂûãÊùÉÈáçÂèòÂåñÂπÖÂ∫¶ÂíåÂ±èËîΩ‰ΩéÁÜµÊ†áËÆ∞Ôºå‰øùÊåÅÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSLowEDÂú®Êé®ÁêÜËÉΩÂäõ‰∏äÁõ∏ËæÉ‰∫éÁé∞ÊúâËí∏È¶èÊñπÊ≥ïÊúâÊòæËëóÊèêÂçáÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÈìæÂºèÊÄùÁª¥Ëí∏È¶èËøáÁ®ã‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑÂÆâÂÖ®ÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÂêåÊó∂ÔºåÂèØËÉΩÂØºËá¥Ê®°ÂûãÂØπÊúâÂÆ≥ËæìÂÖ•ÁöÑËÑÜÂº±ÊÄßÂ¢ûÂä†„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSLowEDÊñπÊ≥ïÈÄöËøáÂáèÂ∞èÊ®°ÂûãÊùÉÈáçÁöÑÂèòÂåñÂπÖÂ∫¶ÔºàSlow TuningÔºâÂíåÂ±èËîΩ‰ΩéÁÜµÊ†áËÆ∞ÔºàLow-Entropy MaskingÔºâÔºå‰ª•‰ºòÂåñÊ®°ÂûãÁöÑÂ≠¶‰π†ËøáÁ®ãÔºåÁ°Æ‰øùÊ®°ÂûãÂú®Êé®ÁêÜËÉΩÂäõÊèêÂçáÁöÑÂêåÊó∂‰øùÊåÅÂÆâÂÖ®ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSLowEDÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöSlow TuningÈÄöËøáÈôêÂà∂ÊùÉÈáçÂèòÂåñÊù•‰ºòÂåñÊ®°ÂûãÔºåLow-Entropy MaskingÂàôÈÄöËøáÂ±èËîΩ‰ΩéÁÜµÊ†áËÆ∞Êù•ÊéíÈô§‰∏çÂøÖË¶ÅÁöÑÂ≠¶‰π†ÁõÆÊ†á„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÂÖàËøõË°åSlow TuningÔºåÂÜçÂ∫îÁî®Low-Entropy MaskingËøõË°åÁªÜÂåñËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSLowEDÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫ÜSlow TuningÂíåLow-Entropy Masking‰∏§ÁßçÁ≠ñÁï•ÔºåÂâçËÄÖÁ°Æ‰øùÊ®°ÂûãÂú®ÂàùÂßãÊùÉÈáçÈôÑËøëËøõË°å‰ºòÂåñÔºåÂêéËÄÖÂàôÊúâÊïàÊéíÈô§‰ΩéÁÜµÊ†áËÆ∞ÔºåÈÅøÂÖç‰∏çÂøÖË¶ÅÁöÑÂ≠¶‰π†Âπ≤Êâ∞„ÄÇËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÁõ¥Êé•ÊùÉÈáçË∞ÉÊï¥ÂíåÂÖ®Ê†áËÆ∞Â≠¶‰π†ÊñπÂºèÂΩ¢Êàê‰∫ÜÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Slow Tuning‰∏≠ÔºåÊ®°ÂûãÊùÉÈáçÂèòÂåñÁöÑÂπÖÂ∫¶Ë¢´‰∏•Ê†ºÊéßÂà∂Ôºå‰ª•‰øùÊåÅÊ®°ÂûãÁöÑÁ®≥ÂÆöÊÄßÔºõÂú®Low-Entropy Masking‰∏≠ÔºåËÆæÂÆö‰∫Ü‰ΩéÁÜµÊ†áËÆ∞ÁöÑÈòàÂÄºÔºå‰ª•ÂÜ≥ÂÆöÂì™‰∫õÊ†áËÆ∞Â∫îË¢´Â±èËîΩ„ÄÇÂÆûÈ™å‰∏≠‰ΩøÁî®ÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÂùáÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Á°Æ‰øùËÆ≠ÁªÉËøáÁ®ãÁöÑÊúâÊïàÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSLowEDÂú®Êé®ÁêÜÂü∫ÂáÜÔºàÂ¶ÇBBH„ÄÅBB-Sub„ÄÅARC„ÄÅAGIEvalÔºâÂíåÂÆâÂÖ®ËØÑ‰º∞ÔºàAdvBenchÔºâ‰∏äÂùáË°®Áé∞‰ºòÂºÇÔºå‰øùÊåÅ‰∫ÜÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÔºåÂπ∂Âú®Êé®ÁêÜËÉΩÂäõ‰∏äÁõ∏ËæÉ‰∫éÁé∞ÊúâËí∏È¶èÊñπÊ≥ïÊúâÊòæËëóÊèêÂçáÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶Êú™Áü•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏≠ÁöÑÂØπËØùÁ≥ªÁªü„ÄÅÊñáÊú¨ÁîüÊàêÂíåÈóÆÁ≠îÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊèêÂçáÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂπ∂Á°Æ‰øùÂÖ∂ÂÆâÂÖ®ÊÄßÔºåSLowEDÂèØ‰ª•Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂáèÂ∞ëÊ®°ÂûãÂØπÊúâÂÆ≥ËæìÂÖ•ÁöÑËÑÜÂº±ÊÄßÔºå‰ªéËÄåÊèêÈ´òÁî®Êà∑‰ø°‰ªªÂ∫¶ÂíåÁ≥ªÁªüÁöÑÂèØÈù†ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Previous chain-of-thought (CoT) distillation methods primarily focused on enhancing the reasoning capabilities of Small Language Models (SLMs) by utilizing high-quality rationales generated by powerful Large Language Models (LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM safety brought by the training, which are revealed in this study. Although there are works on safety alignment that fine-tune language models or manipulate model weights to defend against harmful inputs, they require extra computation or annotated data, and probably impact the reasoning ability of SLMs. In this paper, we investigate how to maintain the safety of SLMs during the CoT distillation process. Specifically, we propose a safe distillation method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the magnitude of model weight changes to optimize the model weights in the neighboring space near the initial weight distribution. Low-Entropy Masking masks low-entropy tokens, which are regarded as unnecessary learning targets, to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B, Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC, AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety of SLMs and comparably improves their reasoning capability compared to existing distillation methods. Furthermore, our ablation study presents the effectiveness of Slow Tuning and Low-Entropy Masking, with the former maintaining the model's safety in the early stage and the latter prolonging the safe training epochs.

