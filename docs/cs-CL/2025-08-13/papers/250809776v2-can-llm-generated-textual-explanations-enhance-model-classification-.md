---
layout: default
title: Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study
---

# Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09776" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09776v2</a>
  <a href="https://arxiv.org/pdf/2508.09776.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09776v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09776v2', 'Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mahdi Dhaini, Juraj Vladika, Ege Erdogan, Zineb Attaoui, Gjergji Kasneci

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-11-11)

**å¤‡æ³¨**: Accepted to the 34th International Conference on Artificial Neural Networks (ICANN 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªåŠ¨åŒ–æ¡†æ¶ä»¥ç”Ÿæˆæ–‡æœ¬è§£é‡Šæå‡æ¨¡å‹åˆ†ç±»æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯è§£é‡Šæ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬ç”Ÿæˆ` `è‡ªåŠ¨åŒ–æ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥æ ‡æ³¨æ–‡æœ¬è§£é‡Šï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ï¼Œé™åˆ¶äº†å¯è§£é‡Šæ€§ç ”ç©¶çš„è§„æ¨¡å’Œæ•ˆç‡ã€‚
2. æœ¬æ–‡æå‡ºä¸€ç§è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å¤šç§å…ˆè¿›çš„LLMç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬è§£é‡Šï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè‡ªåŠ¨ç”Ÿæˆçš„è§£é‡Šåœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­ï¼Œä¸äººå·¥æ ‡æ³¨çš„è§£é‡Šç›¸æ¯”ï¼Œèƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¿«é€Ÿå‘å±•çš„å¯è§£é‡Šè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œæ–‡æœ¬è§£é‡Šï¼ˆå³äººç±»èˆ¬çš„æ¨ç†ï¼‰å¯¹äºè§£é‡Šæ¨¡å‹é¢„æµ‹å’Œä¸°å¯Œå¯è§£é‡Šæ ‡ç­¾çš„æ•°æ®é›†è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äººå·¥æ ‡æ³¨ï¼Œæˆæœ¬é«˜ã€åŠ³åŠ¨å¯†é›†ä¸”éš¾ä»¥æ‰©å±•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å¤šç§å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬è§£é‡Šã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰æŒ‡æ ‡ä¸¥æ ¼è¯„ä¼°è¿™äº›LLMç”Ÿæˆçš„è§£é‡Šè´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†è¿™äº›è§£é‡Šå¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰å’ŒLLMsåœ¨è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè‡ªåŠ¨ç”Ÿæˆçš„è§£é‡Šåœ¨æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢ä¸äººå·¥æ ‡æ³¨çš„è§£é‡Šå…·æœ‰é«˜åº¦ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å‘ç°å¼ºè°ƒäº†åŸºäºLLMçš„å¯æ‰©å±•è‡ªåŠ¨åŒ–æ–‡æœ¬è§£é‡Šç”Ÿæˆçš„å‰æ™¯ï¼Œä»¥æ‰©å±•NLPæ•°æ®é›†å¹¶å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬è§£é‡Šï¼Œä»¥æ›¿ä»£ä¼ ç»Ÿçš„äººå·¥æ ‡æ³¨æ–¹æ³•ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºäººå·¥æ ‡æ³¨ä¸ä»…æˆæœ¬é«˜ï¼Œè€Œä¸”éš¾ä»¥æ»¡è¶³å¤§è§„æ¨¡æ•°æ®é›†çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šç§å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬è§£é‡Šï¼Œæ—¨åœ¨æé«˜è§£é‡Šçš„è´¨é‡å’Œæ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶è€…å¸Œæœ›å®ç°å¯æ‰©å±•çš„è§£é‡Šç”Ÿæˆï¼Œé™ä½äººå·¥æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€LLMé€‰æ‹©ã€æ–‡æœ¬ç”Ÿæˆã€è´¨é‡è¯„ä¼°å’Œæ€§èƒ½è¯„ä¼°å‡ ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œè¾“å…¥æ•°æ®é€šè¿‡é€‰æ‹©åˆé€‚çš„LLMè¿›è¡Œæ–‡æœ¬è§£é‡Šç”Ÿæˆï¼Œç„¶åä½¿ç”¨è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡è¯„ä¼°ç”Ÿæˆçš„è§£é‡Šè´¨é‡ï¼Œæœ€ååˆ†æè¿™äº›è§£é‡Šå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¤šç§LLMç»“åˆä½¿ç”¨ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬è§£é‡Šï¼Œå¹¶é€šè¿‡ç³»ç»Ÿçš„è¯„ä¼°æ–¹æ³•éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚è¿™ä¸ä¼ ç»Ÿä¾èµ–äººå·¥æ ‡æ³¨çš„æ–¹å¼æœ¬è´¨ä¸Šä¸åŒï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼Œè®ºæ–‡è¯¦ç»†æè¿°äº†LLMçš„é€‰æ‹©æ ‡å‡†ã€ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥åŠå¦‚ä½•å°†ç”Ÿæˆçš„è§£é‡Šåº”ç”¨äºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ€§èƒ½æå‡ä¸­ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè‡ªåŠ¨ç”Ÿæˆçš„æ–‡æœ¬è§£é‡Šåœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­ï¼Œä¸äººå·¥æ ‡æ³¨çš„è§£é‡Šç›¸æ¯”ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®åŸæ–‡è¡¥å……ï¼‰ã€‚è¿™ä¸€å‘ç°ä¸ºå¯æ‰©å±•çš„æ–‡æœ¬è§£é‡Šç”Ÿæˆæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ¨¡å‹è§£é‡Šã€æ•°æ®é›†æ‰©å±•ä»¥åŠæœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯è§£é‡Šæ€§æå‡ã€‚é€šè¿‡è‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬è§£é‡Šï¼Œç ”ç©¶è€…å’Œå¼€å‘è€…å¯ä»¥æ›´é«˜æ•ˆåœ°æ„å»ºå’Œä¼˜åŒ–æ¨¡å‹ï¼Œæ¨åŠ¨å¯è§£é‡ŠAIçš„å‘å±•ï¼Œæå‡ç”¨æˆ·å¯¹æ¨¡å‹å†³ç­–çš„ä¿¡ä»»åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance.

