---
layout: default
title: Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models
---

# Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10192" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.10192v1</a>
  <a href="https://arxiv.org/pdf/2508.10192.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10192v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10192v1', 'Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Igor Halperin

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG, q-fin.CP

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-13

**Â§áÊ≥®**: 24 pages, 3 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ËØ≠‰πâÂÅèÂ∑ÆÂ∫¶Èáè‰ª•Ê£ÄÊµãÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËôöÂÅáÁîüÊàêÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ËôöÂÅáÁîüÊàê` `ËØ≠‰πâÂÅèÂ∑Æ` `‰ø°ÊÅØËÆ∫Â∫¶Èáè` `ÂØπËØùÁ≥ªÁªü` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂ¶ÇËØ≠‰πâÁÜµ‰ªÖÈÄöËøáÊµãÈáèÂõ∫ÂÆöÊèêÁ§∫ÁöÑÂ§öÊ†∑ÊÄßÊù•Ê£ÄÊµãÈöèÊú∫ÊÄßÔºåÊó†Ê≥ïÊ∑±ÂÖ•ÂàÜÊûêÂìçÂ∫îÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑSDMÊ°ÜÊû∂ÈÄöËøáËÅîÂêàËÅöÁ±ªÂíå‰ø°ÊÅØËÆ∫Â∫¶ÈáèÔºåÂ¢ûÂº∫‰∫ÜÂØπÊèêÁ§∫ÂíåÂìçÂ∫î‰πãÈó¥ËØ≠‰πâÂÅèÂ∑ÆÁöÑÊ£ÄÊµãËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSDMÊ°ÜÊû∂Âú®Ê£ÄÊµã‰ø°ÂÆûÊÄßËôöÂÅáÁîüÊàêÊñπÈù¢Ë°®Áé∞‰ºòË∂äÔºåËÉΩÂ§üÊúâÊïàÂå∫ÂàÜ‰∏çÂêåÁöÑÁîüÊàêË°å‰∏∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂπøÊ≥õÂ∫îÁî®ÔºåËôöÂÅáÁîüÊàêÔºàhallucinationsÔºâÊàê‰∏∫ÂÖ∂‰∏ªË¶ÅÊåëÊàò‰πã‰∏ÄÔºåÊåáÊ®°ÂûãÁîüÊàê‰∏çÁúüÂÆûÊàñ‰∏çÁ¨¶Âêà‰∏ä‰∏ãÊñáÁöÑÊñáÊú¨„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËΩªÈáèÁ∫ßÊ°ÜÊû∂‚Äî‚ÄîËØ≠‰πâÂÅèÂ∑ÆÂ∫¶ÈáèÔºàSDMÔºâÔºåÁî®‰∫éÊ£ÄÊµã‰ø°ÂÆûÊÄßËôöÂÅáÁîüÊàêÔºåÁâπÂà´ÊòØÂØπÁî®Êà∑Êü•ËØ¢ÁöÑËØ≠‰πâÂÅèÁ¶ª„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåSDMÊ°ÜÊû∂ÈÄöËøáÊµãÈáèÂ§ö‰∏™ËØ≠‰πâÁ≠â‰ª∑ÁöÑÊèêÁ§∫ÁöÑÂìçÂ∫î‰∏ÄËá¥ÊÄßÔºåÊèê‰æõ‰∫ÜÊõ¥Ê∑±Â±ÇÊ¨°ÁöÑÈöèÊú∫ÊÄßÊµãËØï„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂà©Áî®Âè•Â≠êÂµåÂÖ•ÁöÑËÅîÂêàËÅöÁ±ªÔºåÂàõÂª∫ÊèêÁ§∫ÂíåÁ≠îÊ°àÁöÑÂÖ±‰∫´‰∏ªÈ¢òÁ©∫Èó¥ÔºåÂπ∂ÈÄöËøá‰ø°ÊÅØËÆ∫Â∫¶ÈáèËÆ°ÁÆóÊèêÁ§∫‰∏éÂìçÂ∫î‰πãÈó¥ÁöÑËØ≠‰πâÂÅèÂ∑ÆÔºåÊúÄÁªàÂΩ¢Êàê‰∏Ä‰∏™ËØäÊñ≠Ê°ÜÊû∂‰ª•ÂàÜÁ±ªLLMÁöÑÂìçÂ∫îÁ±ªÂûã„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁöÑËôöÂÅáÊñáÊú¨ÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Ê£ÄÊµãÂìçÂ∫îÁöÑ‰∏ÄËá¥ÊÄßÂíåËØ≠‰πâÂÅèÂ∑ÆÊñπÈù¢Â≠òÂú®‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSDMÊ°ÜÊû∂ÈÄöËøáÊµãÈáèÂ§ö‰∏™ËØ≠‰πâÁ≠â‰ª∑ÊèêÁ§∫ÁöÑÂìçÂ∫î‰∏ÄËá¥ÊÄßÔºåÊèê‰æõÊõ¥Ê∑±Â±ÇÊ¨°ÁöÑÈöèÊú∫ÊÄßÂàÜÊûêÔºåÂ¢ûÂº∫ÂØπ‰ø°ÂÆûÊÄßËôöÂÅáÁîüÊàêÁöÑÊ£ÄÊµãËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÊã¨Âè•Â≠êÂµåÂÖ•ÁöÑËÅîÂêàËÅöÁ±ª„ÄÅ‰∏ªÈ¢òÁ©∫Èó¥ÊûÑÂª∫Âíå‰ø°ÊÅØËÆ∫Â∫¶ÈáèËÆ°ÁÆóÁ≠â‰∏ªË¶ÅÊ®°ÂùóÔºåÂΩ¢Êàê‰∏Ä‰∏™ÁªºÂêàÁöÑÂàÜÊûêÊµÅÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSDMÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂ÂØπÊèêÁ§∫ÂíåÂìçÂ∫î‰πãÈó¥ÁöÑËØ≠‰πâÂÅèÂ∑ÆËøõË°åÈáèÂåñÔºåÁªìÂêà‰∫ÜJensen-ShannonÊï£Â∫¶ÂíåWassersteinË∑ùÁ¶ªÔºåÊèê‰æõ‰∫ÜÊõ¥ÂÖ®Èù¢ÁöÑÊ£ÄÊµãÊâãÊÆµ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊäÄÊúØÁªÜËäÇ‰∏äÔºåSDMÊ°ÜÊû∂‰ΩøÁî®‰∫ÜÂè•Â≠êÂµåÂÖ•ËøõË°åËÅöÁ±ªÔºåÂπ∂ÈÄöËøáÁÉ≠ÂõæÂèØËßÜÂåñÊèêÁ§∫‰∏éÂìçÂ∫îÁöÑ‰∏ªÈ¢òÂÖ±Áé∞ÔºåËÆæËÆ°‰∫ÜÂ§öÁßç‰ø°ÊÅØËÆ∫Â∫¶ÈáèÊù•ÈáèÂåñËØ≠‰πâÂÅèÂ∑Æ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSDMÊ°ÜÊû∂Âú®‰ø°ÂÆûÊÄßËôöÂÅáÁîüÊàêÊ£ÄÊµã‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂ∞§ÂÖ∂Âú®ÂØπÊØîÂü∫Á∫ø‰∏≠ÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏≠ÁöÑÂØπËØùÁ≥ªÁªü„ÄÅÂÜÖÂÆπÁîüÊàêÂíå‰ø°ÊÅØÊ£ÄÁ¥¢Á≠â„ÄÇÈÄöËøáÂáÜÁ°ÆÊ£ÄÊµãËôöÂÅáÁîüÊàêÔºåËÉΩÂ§üÊèêÈ´òÁî®Êà∑‰ΩìÈ™åÂíåÁ≥ªÁªüÁöÑÂèØÈù†ÊÄßÔºåÊú™Êù•ÂèØËÉΩÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÂíåÂèØ‰ø°ÊÄß‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The proliferation of Large Language Models (LLMs) is challenged by hallucinations, critical failure modes where models generate non-factual, nonsensical or unfaithful text. This paper introduces Semantic Divergence Metrics (SDM), a novel lightweight framework for detecting Faithfulness Hallucinations -- events of severe deviations of LLMs responses from input contexts. We focus on a specific implementation of these LLM errors, {confabulations, defined as responses that are arbitrary and semantically misaligned with the user's query. Existing methods like Semantic Entropy test for arbitrariness by measuring the diversity of answers to a single, fixed prompt. Our SDM framework improves upon this by being more prompt-aware: we test for a deeper form of arbitrariness by measuring response consistency not only across multiple answers but also across multiple, semantically-equivalent paraphrases of the original prompt. Methodologically, our approach uses joint clustering on sentence embeddings to create a shared topic space for prompts and answers. A heatmap of topic co-occurances between prompts and responses can be viewed as a quantified two-dimensional visualization of the user-machine dialogue. We then compute a suite of information-theoretic metrics to measure the semantic divergence between prompts and responses. Our practical score, $\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein distance to quantify this divergence, with a high score indicating a Faithfulness hallucination. Furthermore, we identify the KL divergence KL(Answer $\|\|$ Prompt) as a powerful indicator of \textbf{Semantic Exploration}, a key signal for distinguishing different generative behaviors. These metrics are further combined into the Semantic Box, a diagnostic framework for classifying LLM response types, including the dangerous, confident confabulation.

