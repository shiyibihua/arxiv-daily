---
layout: default
title: Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models
---

# Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09874" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09874v2</a>
  <a href="https://arxiv.org/pdf/2508.09874.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09874v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09874v2', 'Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaqi Cao, Jiarui Wang, Rubin Wei, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-10-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMemory Decoderä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸé€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é¢†åŸŸé€‚åº”` `Memory Decoder` `éå‚æ•°æ£€ç´¢` `å˜æ¢å™¨è§£ç å™¨` `ç”Ÿç‰©åŒ»å­¦` `é‡‘è` `æ³•å¾‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¦‚DAPTåœ¨é¢†åŸŸé€‚åº”ä¸­éœ€è¦æ˜‚è´µçš„å…¨å‚æ•°è®­ç»ƒï¼Œå¹¶ä¸”å®¹æ˜“å¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€‚
2. Memory Decoderæ˜¯ä¸€ç§å¯æ’æ‹”çš„é¢„è®­ç»ƒå†…å­˜ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹åŸå§‹æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„é¢†åŸŸé€‚åº”ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMemory Decoderåœ¨ç”Ÿç‰©åŒ»å­¦ã€é‡‘èå’Œæ³•å¾‹é¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œå¹³å‡é™ä½äº†6.17åˆ†çš„å›°æƒ‘åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šç”¨è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•å¦‚é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼ˆDAPTï¼‰éœ€è¦æ˜‚è´µçš„å…¨å‚æ•°è®­ç»ƒï¼Œå¹¶ä¸”å®¹æ˜“å‡ºç°ç¾éš¾æ€§é—å¿˜ã€‚åŒæ—¶ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç”±äºæ˜‚è´µçš„æœ€è¿‘é‚»æœç´¢å’Œæ›´é•¿çš„ä¸Šä¸‹æ–‡å¼•å…¥äº†æ˜¾è‘—çš„æ¨ç†å»¶è¿Ÿã€‚æœ¬æ–‡æå‡ºMemory Decoderï¼Œè¿™æ˜¯ä¸€ç§å¯æ’æ‹”çš„é¢„è®­ç»ƒå†…å­˜ï¼Œèƒ½å¤Ÿåœ¨ä¸æ”¹å˜åŸå§‹æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„é¢†åŸŸé€‚åº”ã€‚Memory Decoderä½¿ç”¨ä¸€ä¸ªå°å‹çš„å˜æ¢å™¨è§£ç å™¨ï¼Œå­¦ä¹ æ¨¡ä»¿å¤–éƒ¨éå‚æ•°æ£€ç´¢å™¨çš„è¡Œä¸ºã€‚ç»è¿‡è®­ç»ƒåï¼ŒMemory Decoderå¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•å…±äº«ç›¸åŒåˆ†è¯å™¨çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­ï¼Œæ— éœ€ç‰¹å®šæ¨¡å‹çš„ä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMemory Decoderæœ‰æ•ˆåœ°å°†å¤šç§Qwenå’ŒLlamaæ¨¡å‹é€‚åº”äºç”Ÿç‰©åŒ»å­¦ã€é‡‘èå’Œæ³•å¾‹ä¸‰ä¸ªç‰¹å®šé¢†åŸŸï¼Œå¹³å‡é™ä½å›°æƒ‘åº¦6.17åˆ†ã€‚æ€»ä½“è€Œè¨€ï¼ŒMemory Decoderå¼•å…¥äº†ä¸€ç§ä»¥ç‰¹å®šé¢„è®­ç»ƒå†…å­˜ç»„ä»¶ä¸ºä¸­å¿ƒçš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨å®ç°é¢†åŸŸç‰¹å®šçš„é€‚åº”ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸé€‚åº”ä¸­çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¦‚DAPTå’ŒRAGå­˜åœ¨é«˜æˆæœ¬å’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMemory Decoderé€šè¿‡å¼•å…¥ä¸€ä¸ªå°å‹å˜æ¢å™¨è§£ç å™¨ï¼Œæ¨¡ä»¿å¤–éƒ¨éå‚æ•°æ£€ç´¢å™¨çš„è¡Œä¸ºï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é¢†åŸŸé€‚åº”ï¼Œè€Œæ— éœ€ä¿®æ”¹åŸå§‹æ¨¡å‹çš„å‚æ•°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMemory Decoderçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå°å‹å˜æ¢å™¨è§£ç å™¨å’Œä¸€ä¸ªå¤–éƒ¨æ£€ç´¢å™¨ã€‚è§£ç å™¨å­¦ä¹ å¦‚ä½•ä»æ£€ç´¢å™¨ä¸­è·å–ä¿¡æ¯ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šMemory Decoderçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¯æ’æ‹”çš„å†…å­˜æ¶æ„ï¼Œå…è®¸ä¸ä»»ä½•å…±äº«ç›¸åŒåˆ†è¯å™¨çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ— ç¼é›†æˆï¼Œè¿™ä¸ä¼ ç»Ÿçš„å…¨å‚æ•°è®­ç»ƒæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMemory Decoderçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿å…¶èƒ½å¤Ÿæœ‰æ•ˆæ¨¡ä»¿å¤–éƒ¨æ£€ç´¢å™¨çš„è¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„æ¨ç†å»¶è¿Ÿã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥åœ¨å®éªŒä¸­è¿›è¡Œäº†ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMemory Decoderåœ¨å°†Qwenå’ŒLlamaæ¨¡å‹é€‚åº”äºç”Ÿç‰©åŒ»å­¦ã€é‡‘èå’Œæ³•å¾‹é¢†åŸŸæ—¶ï¼Œå¹³å‡é™ä½äº†6.17åˆ†çš„å›°æƒ‘åº¦ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ï¼Œå±•ç¤ºäº†å…¶åœ¨é¢†åŸŸé€‚åº”ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Memory Decoderçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”Ÿç‰©åŒ»å­¦ã€é‡‘èå’Œæ³•å¾‹ç­‰ä¸“ä¸šé¢†åŸŸï¼Œèƒ½å¤Ÿå¸®åŠ©å¤§è¯­è¨€æ¨¡å‹æ›´å¥½åœ°é€‚åº”ç‰¹å®šé¢†åŸŸçš„éœ€æ±‚ï¼Œæå‡å…¶åœ¨ä¸“ä¸šä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šåœ¨æ›´å¤šé¢†åŸŸä¸­æ¨å¹¿åº”ç”¨ï¼Œæ¨åŠ¨é¢†åŸŸç‰¹å®šæ¨¡å‹çš„å¿«é€Ÿå¼€å‘ä¸éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers from catastrophic forgetting. Meanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context. This paper introduces Memory Decoder, a plug-and-play pretrained memory that enables efficient domain adaptation without changing the original model's parameters. Memory Decoder employs a small transformer decoder that learns to imitate the behavior of an external non-parametric retriever. Once trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications. Experimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points. Overall, Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain.

