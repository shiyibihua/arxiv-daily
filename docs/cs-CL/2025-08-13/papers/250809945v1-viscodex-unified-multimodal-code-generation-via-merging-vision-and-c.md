---
layout: default
title: VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models
---

# VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09945" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09945v1</a>
  <a href="https://arxiv.org/pdf/2508.09945.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09945v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09945v1', 'VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingjie Jiang, Shaohan Huang, Xun Wu, Yixia Li, Dongdong Zhang, Furu Wei

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVisCodexä»¥è§£å†³å¤šæ¨¡æ€ä»£ç ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ä»£ç ç”Ÿæˆ` `è§†è§‰ç†è§£` `ç¼–ç è¯­è¨€æ¨¡å‹` `æ¨¡å‹åˆå¹¶` `æ•°æ®é›†æ„å»º` `ç¼–ç¨‹æ•™è‚²` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆä»£ç æ–¹é¢èƒ½åŠ›æœ‰é™ï¼Œæ— æ³•æ»¡è¶³å¤æ‚ç¼–ç¨‹éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºVisCodexï¼Œé€šè¿‡å°†è§†è§‰å’Œç¼–ç è¯­è¨€æ¨¡å‹æ— ç¼èåˆï¼Œæå‡å¤šæ¨¡æ€ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œé‡‡ç”¨ä»»åŠ¡å‘é‡æ¨¡å‹åˆå¹¶æŠ€æœ¯ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisCodexåœ¨å¼€æºMLLMsä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ¥è¿‘ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æ¨¡å‹åˆå¹¶ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ä¸æ–‡æœ¬ç†è§£çš„æ•´åˆä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä»å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆä»£ç çš„èƒ½åŠ›ä¸Šä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†VisCodexï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡æ— ç¼èåˆè§†è§‰å’Œç¼–ç è¯­è¨€æ¨¡å‹ï¼Œèµ‹äºˆMLLMså¼ºå¤§çš„å¤šæ¨¡æ€ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºä»»åŠ¡å‘é‡çš„æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œå°†æœ€å…ˆè¿›çš„ç¼–ç LLMé›†æˆåˆ°å¼ºå¤§çš„è§†è§‰-è¯­è¨€éª¨å¹²ä¸­ï¼ŒåŒæ—¶ä¿ç•™è§†è§‰ç†è§£å’Œé«˜çº§ç¼–ç æŠ€èƒ½ã€‚ä¸ºæ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡æ€ç¼–ç æ•°æ®é›†ï¼ˆMCDï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«598kæ ·æœ¬çš„å¤§è§„æ¨¡å¤šæ ·åŒ–é›†åˆï¼Œæ¶µç›–é«˜è´¨é‡HTMLä»£ç ã€å›¾è¡¨å›¾åƒ-ä»£ç å¯¹ã€å›¾åƒå¢å¼ºçš„StackOverflowé—®ç­”å’Œç®—æ³•é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†InfiBench-Vï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨éœ€è¦ç»†è‡´ç†è§£æ–‡æœ¬å’Œè§†è§‰ä¸Šä¸‹æ–‡çš„è§†è§‰ä¸°å¯Œçš„ç°å®ç¼–ç¨‹é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVisCodexåœ¨å¼€æºMLLMsä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ¥è¿‘äºGPT-4oç­‰ä¸“æœ‰æ¨¡å‹ï¼Œçªæ˜¾äº†æˆ‘ä»¬çš„æ¨¡å‹åˆå¹¶ç­–ç•¥å’Œæ–°æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆä»£ç æ—¶çš„èƒ½åŠ›ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„è§†è§‰å’Œæ–‡æœ¬è¾“å…¥æ—¶çš„å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆç»“åˆè§†è§‰ç†è§£ä¸ç¼–ç¨‹èƒ½åŠ›ï¼Œå¯¼è‡´ç”Ÿæˆçš„ä»£ç è´¨é‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVisCodexçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä»»åŠ¡å‘é‡æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œå°†å…ˆè¿›çš„ç¼–ç è¯­è¨€æ¨¡å‹ä¸å¼ºå¤§çš„è§†è§‰-è¯­è¨€éª¨å¹²æ— ç¼ç»“åˆï¼Œä»è€Œæå‡æ¨¡å‹åœ¨å¤šæ¨¡æ€è¾“å…¥ä¸‹çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨åŒæ—¶ä¿ç•™è§†è§‰ç†è§£å’Œç¼–ç æŠ€èƒ½ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„ç¼–ç¨‹ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVisCodexçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§‰ç†è§£æ¨¡å—ã€ç¼–ç ç”Ÿæˆæ¨¡å—å’Œä»»åŠ¡å‘é‡åˆå¹¶æ¨¡å—ã€‚è§†è§‰ç†è§£æ¨¡å—è´Ÿè´£æå–è¾“å…¥å›¾åƒçš„ç‰¹å¾ï¼Œç¼–ç ç”Ÿæˆæ¨¡å—åˆ™åŸºäºæå–çš„ç‰¹å¾å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆä»£ç ï¼Œä»»åŠ¡å‘é‡åˆå¹¶æ¨¡å—åˆ™å®ç°ä¸¤è€…çš„æœ‰æ•ˆèåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹åˆå¹¶ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆè§†è§‰å’Œç¼–ç æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVisCodexåœ¨å¤šæ¨¡æ€ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡è§†è§‰ç†è§£ä¸ä»£ç ç”Ÿæˆçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿åœ¨å¤„ç†å¤æ‚è¾“å…¥æ—¶çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVisCodexåœ¨å¤šæ¨¡æ€ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ä¸å¼€æºMLLMsçš„å¯¹æ¯”ä¸­ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼Œæ¥è¿‘äºGPT-4oç­‰ä¸“æœ‰æ¨¡å‹ï¼ŒéªŒè¯äº†æ¨¡å‹åˆå¹¶ç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œæ–°æ•°æ®é›†çš„è´¡çŒ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VisCodexçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•™è‚²ã€è½¯ä»¶å¼€å‘å’Œè‡ªåŠ¨åŒ–ç¼–ç¨‹ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤šæ¨¡æ€ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œè¯¥æ¨¡å‹å¯ä»¥å¸®åŠ©å¼€å‘è€…æ›´é«˜æ•ˆåœ°ç¼–å†™ä»£ç ï¼Œç”šè‡³åœ¨æ•™è‚²åœºæ™¯ä¸­è¾…åŠ©å­¦ç”Ÿå­¦ä¹ ç¼–ç¨‹ã€‚æ­¤å¤–ï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½åŠ©æ‰‹å’Œç¼–ç¨‹å·¥å…·ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨ç¼–ç¨‹çš„è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets.

