---
layout: default
title: Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks
---

# Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15797" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15797v1</a>
  <a href="https://arxiv.org/pdf/2508.15797.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15797v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15797v1', 'Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nouar AlDahoul, Yasir Zaki

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

**å¤‡æ³¨**: 5 pages, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯åŒ»ç–—ä»»åŠ¡ä¸­çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é˜¿æ‹‰ä¼¯åŒ»ç–—NLP` `åŸºå‡†æµ‹è¯•` `å¤šé¡¹é€‰æ‹©é¢˜` `å¼€æ”¾å¼é—®é¢˜` `è¯­ä¹‰å¯¹é½` `åŒ»ç–—æ™ºèƒ½é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯åŒ»ç–—NLPé¢†åŸŸçš„æœ‰æ•ˆæ€§ç ”ç©¶è¾ƒå°‘ï¼Œç¼ºä¹ç³»ç»Ÿè¯„ä¼°ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡åŸºå‡†æµ‹è¯•è¯„ä¼°å¤šç§LLMsåœ¨é˜¿æ‹‰ä¼¯åŒ»ç–—ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæå‡ºäº†åŸºäºå¤šæ•°æŠ•ç¥¨çš„è§£å†³æ–¹æ¡ˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•åœ¨å¤šé¡¹é€‰æ‹©é¢˜ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å¼€æ”¾å¼é—®é¢˜ä¸­å®ç°äº†é«˜è¯­ä¹‰å¯¹é½åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é˜¿æ‹‰ä¼¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨ä¸­å±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é˜¿æ‹‰ä¼¯åŒ»ç–—NLPé¢†åŸŸçš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨é˜¿æ‹‰ä¼¯åŒ»ç–—ä»»åŠ¡ä¸­çš„çŸ¥è¯†è¡¨è¾¾èƒ½åŠ›ï¼ŒåŸºäºAraHealthQAæŒ‘æˆ˜ä¸­çš„åŒ»ç–—æ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼é—®é¢˜çš„å›ç­”å‡†ç¡®æ€§ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæå‡ºçš„å¤šæ•°æŠ•ç¥¨è§£å†³æ–¹æ¡ˆåœ¨å¤šé¡¹é€‰æ‹©é¢˜ä»»åŠ¡ä¸­è¾¾åˆ°äº†77%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨Arahealthqa 2025æŒ‘æˆ˜ä¸­è·å¾—ç¬¬ä¸€åã€‚å¯¹äºå¼€æ”¾å¼é—®é¢˜ï¼Œå¤šä¸ªLLMsåœ¨è¯­ä¹‰å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæœ€é«˜BERTScoreè¾¾åˆ°86.44%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯åŒ»ç–—NLPä»»åŠ¡ä¸­çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›ï¼Œç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åŸºå‡†æµ‹è¯•å¤šç§LLMsï¼Œç‰¹åˆ«æ˜¯é‡‡ç”¨å¤šæ•°æŠ•ç¥¨æœºåˆ¶æ¥æé«˜å¤šé¡¹é€‰æ‹©é¢˜çš„å›ç­”å‡†ç¡®æ€§ï¼Œæ—¨åœ¨å……åˆ†æŒ–æ˜æ¨¡å‹çš„æ½œåŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†å‡†å¤‡ã€æ¨¡å‹é€‰æ‹©ã€è¯„ä¼°æŒ‡æ ‡è®¾å®šåŠç»“æœåˆ†æï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼é—®é¢˜çš„å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºçš„å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆç»“åˆäº†ä¸‰ç§åŸºç¡€æ¨¡å‹ï¼ˆGemini Flash 2.5ã€Gemini Pro 2.5å’ŒGPT o3ï¼‰ï¼Œåœ¨å¤šé¡¹é€‰æ‹©é¢˜ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†å‡†ç¡®ç‡ï¼Œä½“ç°äº†æ¨¡å‹ç»„åˆçš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼é—®é¢˜çš„è¯„ä¼°æ ‡å‡†ï¼Œé‡‡ç”¨BERTScoreä½œä¸ºè¯­ä¹‰å¯¹é½çš„è¡¡é‡æŒ‡æ ‡ï¼Œç¡®ä¿ç»“æœçš„ç§‘å­¦æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆåœ¨å¤šé¡¹é€‰æ‹©é¢˜ä»»åŠ¡ä¸­è¾¾åˆ°äº†77%çš„å‡†ç¡®ç‡ï¼Œä½åˆ—Arahealthqa 2025æŒ‘æˆ˜ç¬¬ä¸€åã€‚åŒæ—¶ï¼Œåœ¨å¼€æ”¾å¼é—®é¢˜ä»»åŠ¡ä¸­ï¼Œå¤šä¸ªLLMsçš„æœ€é«˜BERTScoreè¾¾åˆ°äº†86.44%ï¼Œå±•ç°äº†è‰¯å¥½çš„è¯­ä¹‰å¯¹é½èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æˆæœå¯å¹¿æ³›åº”ç”¨äºé˜¿æ‹‰ä¼¯åœ°åŒºçš„åŒ»ç–—ä¿¡æ¯ç³»ç»Ÿã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿä»¥åŠåŒ»ç–—æ•™è‚²ç­‰é¢†åŸŸï¼Œæå‡åŒ»ç–—æœåŠ¡çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒLLMsåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å°†æ›´åŠ æ·±å…¥ï¼Œæ¨åŠ¨ä¸ªæ€§åŒ–åŒ»ç–—å’Œæ™ºèƒ½è¯Šæ–­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent progress in large language models (LLMs) has showcased impressive proficiency in numerous Arabic natural language processing (NLP) applications. Nevertheless, their effectiveness in Arabic medical NLP domains has received limited investigation. This research examines the degree to which state-of-the-art LLMs demonstrate and articulate healthcare knowledge in Arabic, assessing their capabilities across a varied array of Arabic medical tasks. We benchmark several LLMs using a medical dataset proposed in the Arabic NLP AraHealthQA challenge in MedArabiQ2025 track. Various base LLMs were assessed on their ability to accurately provide correct answers from existing choices in multiple-choice questions (MCQs) and fill-in-the-blank scenarios. Additionally, we evaluated the capacity of LLMs in answering open-ended questions aligned with expert answers. Our results reveal significant variations in correct answer prediction accuracy and low variations in semantic alignment of generated answers, highlighting both the potential and limitations of current LLMs in Arabic clinical contexts. Our analysis shows that for MCQs task, the proposed majority voting solution, leveraging three base models (Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms others, achieving up to 77% accuracy and securing first place overall in the Arahealthqa 2025 shared task-track 2 (sub-task 1) challenge. Moreover, for the open-ended questions task, several LLMs were able to demonstrate excellent performance in terms of semantic alignment and achieve a maximum BERTScore of 86.44%.

