---
layout: default
title: LaajMeter: A Framework for LaaJ Evaluation
---

# LaajMeter: A Framework for LaaJ Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10161" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.10161v2</a>
  <a href="https://arxiv.org/pdf/2508.10161.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10161v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10161v2', 'LaajMeter: A Framework for LaaJ Evaluation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Samuel Ackerman, Gal Amram, Ora Nova Fandina, Eitan Farchi, Shmulik Froimovich, Raviv Gal, Wesam Ibraheem, Avi Ziv

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-13 (Êõ¥Êñ∞: 2025-11-25)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LaaJMeterÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥LaaJËØÑ‰º∞‰∏≠ÁöÑÊåëÊàò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÂÖÉËØÑ‰º∞` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `ÂêàÊàêÊï∞ÊçÆ` `ËØÑ‰º∞ÊåáÊ†á`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑLaaJËØÑ‰º∞ÊñπÊ≥ïÂú®ÁâπÂÆöÈ¢ÜÂüü‰∏≠Èù¢‰∏¥Êï∞ÊçÆÁ®ÄÁº∫ÂíåËØÑ‰º∞ÊàêÊú¨È´òÁöÑÈóÆÈ¢òÔºåÂØºËá¥ËØÑ‰º∞ÊåáÊ†áÁöÑÊúâÊïàÊÄßÈöæ‰ª•È™åËØÅ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫LaaJMeterÊ°ÜÊû∂ÔºåÈÄöËøáÁîüÊàêÂêàÊàêÊï∞ÊçÆÊù•ËøõË°åÁ≥ªÁªüÁöÑÂÖÉËØÑ‰º∞ÔºåÂ∏ÆÂä©È™åËØÅLaaJÂú®ÁâπÂÆö‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ
3. Âú®‰ª£Á†ÅÁøªËØë‰ªªÂä°‰∏≠ÔºåLaaJMeterÂ±ïÁ§∫‰∫Ü‰∏çÂêåËØÑ‰º∞ÊåáÊ†áÂØπËØÑ‰º∞ËÄÖË¥®ÈáèÁöÑÊïèÊÑüÊÄßÔºåÂº∫Ë∞É‰∫ÜÈÄâÊã©ÂêàÈÄÇÊåáÊ†áÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠‰Ωú‰∏∫ËØÑ‰º∞ËÄÖÁöÑ‰ΩøÁî®Êó•ÁõäÊôÆÂèäÔºåËøôÁßçËåÉÂºèË¢´Áß∞‰∏∫LaaJÔºàLaaJ-as-a-JudgeÔºâ„ÄÇÁÑ∂ËÄåÔºåÂú®ÁâπÂÆöÈ¢ÜÂüü‰∏≠ÔºåÂÖÉËØÑ‰º∞Èù¢‰∏¥ÁùÄÊï∞ÊçÆÁ®ÄÁº∫Âíå‰∏ìÂÆ∂ËØÑ‰º∞ÊàêÊú¨È´òÊòÇÁöÑÊåëÊàò„ÄÇÁé∞ÊúâÁöÑËØÑ‰º∞ÊåáÊ†áÂæÄÂæÄÊú™ÁªèËøáÁâπÂÆöÈ¢ÜÂüüÁöÑÈ™åËØÅÔºåÂØºËá¥Èöæ‰ª•Âà§Êñ≠Âì™‰∫õÊåáÊ†áÊúâÊïàËØÜÂà´LaaJË¥®Èáè„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜLaaJMeterÔºå‰∏Ä‰∏™Âü∫‰∫é‰ªøÁúüÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÂØπLaaJËøõË°åÂèóÊéßÁöÑÂÖÉËØÑ‰º∞„ÄÇLaaJMeterÂÖÅËÆ∏Â∑•Á®ãÂ∏àÁîüÊàê‰ª£Ë°®ËôöÊãüÊ®°ÂûãÂíåËØÑ‰º∞ËÄÖÁöÑÂêàÊàêÊï∞ÊçÆÔºå‰ªéËÄåÂú®Áé∞ÂÆûÊù°‰ª∂‰∏ãÁ≥ªÁªüÂàÜÊûêËØÑ‰º∞ÊåáÊ†á„ÄÇÈÄöËøáÂú®‰ª£Á†ÅÁøªËØë‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ÔºåÂ±ïÁ§∫‰∫Ü‰∏çÂêåÊåáÊ†áÂØπËØÑ‰º∞ËÄÖË¥®ÈáèÁöÑÊïèÊÑüÊÄßÔºåÂº∫Ë∞É‰∫ÜÊåáÊ†áÈÄâÊã©ÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥LaaJËØÑ‰º∞‰∏≠ÁöÑÂÖÉËØÑ‰º∞ÊåëÊàòÔºåÁâπÂà´ÊòØÂú®Êï∞ÊçÆÁ®ÄÁº∫Âíå‰∏ìÂÆ∂ËØÑ‰º∞ÊàêÊú¨È´òÁöÑÁâπÂÆöÈ¢ÜÂüü‰∏≠ÔºåÁé∞ÊúâËØÑ‰º∞ÊåáÊ†áÁöÑÊúâÊïàÊÄßÊú™ÂæóÂà∞È™åËØÅ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLaaJMeterÊ°ÜÊû∂ÈÄöËøáÁîüÊàêÂêàÊàêÊï∞ÊçÆÔºåÊ®°ÊãüËôöÊãüÊ®°ÂûãÂíåËØÑ‰º∞ËÄÖÔºåÊèê‰æõ‰∏Ä‰∏™ÂèóÊéßÁéØÂ¢ÉÊù•Á≥ªÁªüÂàÜÊûêËØÑ‰º∞ÊåáÊ†áÁöÑË°®Áé∞Ôºå‰ªéËÄåÂ∏ÆÂä©È™åËØÅLaaJÁöÑË¥®Èáè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLaaJMeterÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÁîüÊàêÊ®°Âùó„ÄÅËØÑ‰º∞ÊåáÊ†áÂàÜÊûêÊ®°ÂùóÂíåÁªìÊûúÈ™åËØÅÊ®°Âùó„ÄÇÊï∞ÊçÆÁîüÊàêÊ®°ÂùóÂàõÂª∫ÂêàÊàêÊï∞ÊçÆÔºåËØÑ‰º∞ÊåáÊ†áÂàÜÊûêÊ®°ÂùóËØÑ‰º∞‰∏çÂêåÊåáÊ†áÁöÑË°®Áé∞ÔºåÁªìÊûúÈ™åËØÅÊ®°ÂùóÊèê‰æõÂèçÈ¶àÂíåÊîπËøõÂª∫ËÆÆ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLaaJMeterÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂‰ªøÁúüÂü∫Á°ÄÁöÑÂÖÉËØÑ‰º∞ÊñπÊ≥ïÔºåÂÖÅËÆ∏Âú®‰ΩéËµÑÊ∫êÁéØÂ¢É‰∏≠ËøõË°åÊúâÊïàÁöÑLaaJËØÑ‰º∞Ôºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõ‰∫ÜÊõ¥È´òÁöÑÁÅµÊ¥ªÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåLaaJMeter‰ΩøÁî®‰∫ÜÂ§öÁßçÂêàÊàêÊï∞ÊçÆÁîüÊàêÁ≠ñÁï•ÔºåÁ°Æ‰øùÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄßÂíå‰ª£Ë°®ÊÄßÔºåÂêåÊó∂ÈááÁî®‰∫ÜÂ§öÁßçËØÑ‰º∞ÊåáÊ†áËøõË°åÊØîËæÉÔºåÁ°Æ‰øùËØÑ‰º∞ÁªìÊûúÁöÑÂÖ®Èù¢ÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®‰ª£Á†ÅÁøªËØë‰ªªÂä°‰∏≠ÔºåLaaJMeterÂ±ïÁ§∫‰∫Ü‰∏çÂêåËØÑ‰º∞ÊåáÊ†áÂØπËØÑ‰º∞ËÄÖË¥®ÈáèÁöÑÊïèÊÑüÊÄßÔºåÁªìÊûúË°®ÊòéÊüê‰∫õÂ∏∏Áî®ÊåáÊ†áÂú®ÁâπÂÆö‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂº∫Ë∞É‰∫ÜÈÄâÊã©ÂêàÈÄÇËØÑ‰º∞ÊåáÊ†áÁöÑÈáçË¶ÅÊÄßÔºåÊèêÂçá‰∫ÜLaaJËØÑ‰º∞ÁöÑÂáÜÁ°ÆÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LaaJMeterÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ΩéËµÑÊ∫êÁéØÂ¢É‰∏ãÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÂíåÂ∑•Á®ãÂ∏àÈ™åËØÅÂíå‰ºòÂåñLaaJÁöÑËØÑ‰º∞Ë¥®ÈáèÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÂèØÈù†ÊÄßÂíåÂèØÈáçÂ§çÊÄßÔºåÊé®Âä®NLPÈ¢ÜÂüüÁöÑËøõÊ≠•„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) are increasingly used as evaluators in natural language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). The analysis of a LaaJ software, commonly refereed to as meta-evaluation, pose significant challenges in domain-specific contexts. In such domains, in contrast to general domains, annotated data is scarce and expert evaluation is costly. As a result, meta-evaluation is often performed using metrics that have not been validated for the specific domain in which they are applied. Therefore, it becomes difficult to determine which metrics effectively identify LaaJ quality, and further, what threshold indicates sufficient evaluator performance. In this work, we introduce LaaJMeter, a simulation-based framework for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to generate synthetic data representing virtual models and judges, allowing systematic analysis of evaluation metrics under realistic conditions. This helps practitioners validate LaaJs for specific tasks: they can test whether their metrics correctly distinguish between high and low quality (virtual) LaaJs, and estimate appropriate thresholds for evaluator adequacy. We demonstrate the utility of LaaJMeter in a code translation task involving a legacy programming language, showing how different metrics vary in sensitivity to evaluator quality. Our results highlight the limitations of common metrics and the importance of principled metric selection. LaaJMeter provides a scalable and extensible solution for assessing LaaJs in low-resource settings, contributing to the broader effort to ensure trustworthy and reproducible evaluation in NLP.

