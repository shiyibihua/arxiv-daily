---
layout: default
title: Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models
---

# Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15798" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15798v1</a>
  <a href="https://arxiv.org/pdf/2508.15798.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15798v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15798v1', 'Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Saumya Roy

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯´æœåŠ›ä¸åè§å¼ºåŒ–æ¡†æ¶ä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯´æœåŠ›` `åè§å¼ºåŒ–` `ç¤¾ä¼šå½±å“` `AIå®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å¯èƒ½ä¼ æ’­é”™è¯¯ä¿¡æ¯å’Œç¤¾ä¼šåè§ï¼Œå¯¼è‡´æ½œåœ¨çš„æ»¥ç”¨é£é™©ã€‚
2. è®ºæ–‡æå‡ºäº†è¯´æœè€…-æ€€ç–‘è€…æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿè§’è‰²æ¥è¯„ä¼°æ¨¡å‹çš„è¯´æœåŠ›å’Œåè§å¼ºåŒ–æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsèƒ½å¤Ÿæœ‰æ•ˆå¡‘é€ å™äº‹ï¼Œä½†ä¹Ÿå¯èƒ½æ— æ„ä¸­å¼ºåŒ–åè§ï¼Œéœ€åŠ å¼ºç›‘ç®¡å’Œæ”¿ç­–æ”¯æŒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨è¯´æœåŠ›å’Œåè§æ”¾å¤§æ–¹é¢çš„æ½œåœ¨æ»¥ç”¨ï¼Œæ‰€æœ‰å®éªŒå‡ç”¨äºå®‰å…¨è¯„ä¼°ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”Ÿæˆä»¤äººä¿¡æœçš„äººç±»æ–‡æœ¬ï¼Œå¹¿æ³›åº”ç”¨äºå†…å®¹åˆ›ä½œã€å†³ç­–æ”¯æŒå’Œç”¨æˆ·äº’åŠ¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿä¹Ÿå¯èƒ½åœ¨å¤§è§„æ¨¡ä¼ æ’­ä¿¡æ¯æˆ–é”™è¯¯ä¿¡æ¯ï¼Œå¹¶åæ˜ å‡ºæ•°æ®ã€æ¶æ„æˆ–è®­ç»ƒé€‰æ‹©æ‰€å¯¼è‡´çš„ç¤¾ä¼šåè§ã€‚æœ¬æ–‡ç ”ç©¶äº†è¯´æœåŠ›ä¸åè§åœ¨LLMsä¸­çš„ç›¸äº’ä½œç”¨ï¼Œé‡ç‚¹å…³æ³¨ä¸å®Œç¾æˆ–åé¢‡çš„è¾“å‡ºå¦‚ä½•å½±å“è¯´æœæ•ˆæœã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯´æœè€…-æ€€ç–‘è€…æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹ŸçœŸå®æ€åº¦çš„è§’è‰²æ¥è¿›è¡Œå®éªŒï¼Œæ¯”è¾ƒæ€€ç–‘è€…åœ¨æ¥è§¦è¯´æœè€…æ¨¡å‹çš„è®ºç‚¹å‰åçš„ä¿¡å¿µå˜åŒ–ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨å¡‘é€ å™äº‹å’Œé€‚åº”è¯­æ°”æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†åŒæ ·çš„èƒ½åŠ›ä¹Ÿå¯èƒ½è¢«æ»¥ç”¨ä»¥è‡ªåŠ¨åŒ–é”™è¯¯ä¿¡æ¯ä¼ æ’­ï¼Œå¼ºåŒ–åˆ»æ¿å°è±¡å¹¶æ‰©å¤§ä¸å¹³ç­‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯´æœåŠ›å’Œåè§æ”¾å¤§æ–¹é¢çš„å½±å“ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ä¼ æ’­ä¿¡æ¯æ—¶çš„æ½œåœ¨é£é™©å’Œåè§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è¯´æœè€…-æ€€ç–‘è€…æ¡†æ¶ï¼Œæ¨¡æ‹Ÿä¸åŒè§’è‰²çš„ä¿¡å¿µå˜åŒ–ï¼Œé‡åŒ–è¯´æœåŠ›å’Œåè§å¼ºåŒ–çš„ç¨‹åº¦ï¼Œä»¥è¯„ä¼°LLMsçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¯´æœè€…æ¨¡å‹å’Œæ€€ç–‘è€…æ¨¡å‹ã€‚è¯´æœè€…æ¨¡å‹ç”Ÿæˆè®ºç‚¹ï¼Œæ€€ç–‘è€…æ¨¡å‹ä½œä¸ºäººç±»ä»£ç†ï¼Œæ¯”è¾ƒå…¶ä¿¡å¿µå˜åŒ–ã€‚ä½¿ç”¨Jensen-Shannonæ•£åº¦é‡åŒ–è¯´æœæ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡è§’è‰²æ¨¡æ‹Ÿæ¥è¯„ä¼°è¯´æœåŠ›å’Œåè§å¼ºåŒ–ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§†è§’æ¥ç†è§£LLMsçš„ç¤¾ä¼šå½±å“ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ›´åŠ æ³¨é‡æ¨¡å‹è¾“å‡ºçš„ç¤¾ä¼šåæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ä½¿ç”¨äº†å¯¹æ¯”æ¨¡å‹å’Œé™„åŠ çš„å¯¹æŠ—æ€§æç¤ºï¼Œä»¥æ¢æµ‹åè§çš„å­˜åœ¨ï¼Œè®¾è®¡äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è¯´æœåŠ›çš„è¯„ä¼°è¿‡ç¨‹ã€‚å®éªŒä¸­è¿˜è€ƒè™‘äº†ç§æ—ã€æ€§åˆ«å’Œå®—æ•™ç­‰å¤šç»´åº¦çš„åè§åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨è¯´æœåŠ›æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå½±å“æ€€ç–‘è€…çš„ä¿¡å¿µå˜åŒ–ã€‚é€šè¿‡Jensen-Shannonæ•£åº¦é‡åŒ–ï¼Œå‘ç°å¼ºè¯´æœè€…åœ¨åè§å¼ºåŒ–æ–¹é¢çš„å½±å“åŠ›æ˜¾è‘—ï¼Œæç¤ºäº†æ½œåœ¨çš„ç¤¾ä¼šé£é™©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¿ƒç†å­¦ã€å¸‚åœºè¥é”€å’Œæ³•å¾‹æ´åŠ©ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©è®¾è®¡æ›´å…·ä»·å€¼æ•æ„Ÿæ€§çš„AIç³»ç»Ÿï¼Œå‡å°‘åè§ä¼ æ’­çš„é£é™©ã€‚æœªæ¥ï¼Œç ”ç©¶æˆæœå¯ä¸ºæ”¿ç­–åˆ¶å®šæä¾›ä¾æ®ï¼Œä¿ƒè¿›å¯ä¿¡èµ–çš„AIéƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Warning: This research studies AI persuasion and bias amplification that could be misused; all experiments are for safety evaluation. Large Language Models (LLMs) now generate convincing, human-like text and are widely used in content creation, decision support, and user interactions. Yet the same systems can spread information or misinformation at scale and reflect social biases that arise from data, architecture, or training choices. This work examines how persuasion and bias interact in LLMs, focusing on how imperfect or skewed outputs affect persuasive impact. Specifically, we test whether persona-based models can persuade with fact-based claims while also, unintentionally, promoting misinformation or biased narratives.
>   We introduce a convincer-skeptic framework: LLMs adopt personas to simulate realistic attitudes. Skeptic models serve as human proxies; we compare their beliefs before and after exposure to arguments from convincer models. Persuasion is quantified with Jensen-Shannon divergence over belief distributions. We then ask how much persuaded entities go on to reinforce and amplify biased beliefs across race, gender, and religion. Strong persuaders are further probed for bias using sycophantic adversarial prompts and judged with additional models.
>   Our findings show both promise and risk. LLMs can shape narratives, adapt tone, and mirror audience values across domains such as psychology, marketing, and legal assistance. But the same capacity can be weaponized to automate misinformation or craft messages that exploit cognitive biases, reinforcing stereotypes and widening inequities. The core danger lies in misuse more than in occasional model mistakes. By measuring persuasive power and bias reinforcement, we argue for guardrails and policies that penalize deceptive use and support alignment, value-sensitive design, and trustworthy deployment.

