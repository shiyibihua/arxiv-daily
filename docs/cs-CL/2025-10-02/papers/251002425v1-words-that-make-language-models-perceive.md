---
layout: default
title: Words That Make Language Models Perceive
---

# Words That Make Language Models Perceive

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02425" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02425v1</a>
  <a href="https://arxiv.org/pdf/2510.02425.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02425v1" onclick="toggleFavorite(this, '2510.02425v1', 'Words That Make Language Models Perceive')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sophie L. Wang, Phillip Isola, Brian Cheung

**åˆ†ç±»**: cs.CL, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ„Ÿå®˜æç¤ºæ¿€æ´»çº¯æ–‡æœ¬è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåœ¨å¤šæ¨¡æ€è¡¨å¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `æ„Ÿå®˜æç¤º` `è¡¨å¾å­¦ä¹ ` `æ–‡æœ¬è§†è§‰å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶ä»…é€šè¿‡æ–‡æœ¬è®­ç»ƒï¼Œä½†å…¶å†…éƒ¨éšå«ç€å¤šæ¨¡æ€ä¿¡æ¯ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ„Ÿå®˜æç¤ºï¼ˆå¦‚â€œçœ‹â€ã€â€œå¬â€ï¼‰æ¥æ¿€æ´»LLMä¸­æ½œåœ¨çš„è§†è§‰å’Œå¬è§‰è¡¨å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç®€å•çš„æç¤ºå·¥ç¨‹å¯ä»¥æœ‰æ•ˆåœ°åœ¨çº¯æ–‡æœ¬LLMä¸­æ¿€æ´»ç›¸åº”çš„æ¨¡æ€è¡¨å¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»…é€šè¿‡æ–‡æœ¬è®­ç»ƒï¼Œè¡¨é¢ä¸Šç¼ºä¹ä»»ä½•ç›´æ¥çš„æ„ŸçŸ¥ç»éªŒï¼Œä½†å…¶å†…éƒ¨è¡¨å¾å—åˆ°è¯­è¨€ä¸­ç¼–ç çš„å¤šæ¨¡æ€è§„å¾‹çš„éšå¼å½±å“ã€‚æœ¬æ–‡éªŒè¯äº†æ˜¾å¼æ„Ÿå®˜æç¤ºå¯ä»¥æ­ç¤ºè¿™ç§æ½œåœ¨ç»“æ„çš„å‡è®¾ï¼Œä½¿çº¯æ–‡æœ¬LLMåœ¨è¡¨å¾ä¸Šæ›´æ¥è¿‘ä¸“ä¸šçš„è§†è§‰å’ŒéŸ³é¢‘ç¼–ç å™¨ã€‚å½“æ„Ÿå®˜æç¤ºå‘Šè¯‰æ¨¡å‹â€œçœ‹â€æˆ–â€œå¬â€æ—¶ï¼Œå®ƒä¼šæç¤ºæ¨¡å‹è§£æå…¶ä¸‹ä¸€ä¸ªtokené¢„æµ‹ï¼Œå°±å¥½åƒå®ƒä»¬æ˜¯ä»¥ä»æœªå®é™…æä¾›çš„æ½œåœ¨è§†è§‰æˆ–å¬è§‰è¯æ®ä¸ºæ¡ä»¶çš„ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè½»é‡çº§çš„æç¤ºå·¥ç¨‹å¯ä»¥å¯é åœ°æ¿€æ´»çº¯æ–‡æœ¬è®­ç»ƒçš„LLMä¸­é€‚åˆæ¨¡æ€çš„è¡¨å¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦åŸºäºæ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç¼ºä¹ç›´æ¥çš„æ„ŸçŸ¥ç»éªŒã€‚è™½ç„¶è¯­è¨€ä¸­è•´å«ç€å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä½†å¦‚ä½•æœ‰æ•ˆåœ°ä»çº¯æ–‡æœ¬LLMä¸­æå–å’Œåˆ©ç”¨è¿™äº›ä¿¡æ¯æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„å¤šæ¨¡æ€æ•°æ®æˆ–å¤æ‚çš„è®­ç»ƒç­–ç•¥ï¼Œè€Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢ä¸€ç§æ›´ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„Ÿå®˜æç¤ºæ¥å¼•å¯¼LLMæ¿€æ´»å…¶å†…éƒ¨æ½œåœ¨çš„å¤šæ¨¡æ€è¡¨å¾ã€‚ä½œè€…è®¤ä¸ºï¼ŒLLMåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å·²ç»å­¦ä¹ åˆ°äº†æ–‡æœ¬ä¸è§†è§‰ã€å¬è§‰ç­‰æ„Ÿå®˜ä¿¡æ¯ä¹‹é—´çš„å…³è”ï¼Œè€Œæ˜¾å¼çš„æ„Ÿå®˜æç¤ºå¯ä»¥ä½œä¸ºä¸€ç§â€œè§¦å‘å™¨â€ï¼Œä¿ƒä½¿æ¨¡å‹å°†ä¸‹ä¸€ä¸ªtokençš„é¢„æµ‹ä¸ç›¸åº”çš„æ„Ÿå®˜ä¿¡æ¯è”ç³»èµ·æ¥ã€‚è¿™ç§æ–¹æ³•æ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–å¤æ‚çš„æ¨¡å‹ç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) é€‰æ‹©ä¸€ä¸ªé¢„è®­ç»ƒçš„çº¯æ–‡æœ¬LLMï¼›2) è®¾è®¡æ„Ÿå®˜æç¤ºï¼Œä¾‹å¦‚â€œçœ‹â€æˆ–â€œå¬â€ï¼›3) å°†æ„Ÿå®˜æç¤ºæ·»åŠ åˆ°è¾“å…¥æ–‡æœ¬ä¸­ï¼Œä¾‹å¦‚â€œä¸€å¼ å›¾ç‰‡ï¼Œæˆ‘çœ‹åˆ°çš„æ˜¯â€¦â€æˆ–â€œä¸€æ®µå£°éŸ³ï¼Œæˆ‘å¬åˆ°çš„æ˜¯â€¦â€ï¼›4) ä½¿ç”¨LLMç”Ÿæˆä¸‹ä¸€ä¸ªtokenï¼›5) åˆ†æLLMç”Ÿæˆçš„tokençš„è¡¨å¾ï¼Œå¹¶å°†å…¶ä¸è§†è§‰æˆ–å¬è§‰ç¼–ç å™¨çš„è¡¨å¾è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°æ„Ÿå®˜æç¤ºæ˜¯å¦æˆåŠŸæ¿€æ´»äº†ç›¸åº”çš„æ¨¡æ€è¡¨å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§è½»é‡çº§çš„æç¤ºå·¥ç¨‹æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ¿€æ´»çº¯æ–‡æœ¬LLMä¸­çš„æ½œåœ¨å¤šæ¨¡æ€è¡¨å¾ã€‚ä¸éœ€è¦é¢å¤–è®­ç»ƒæ•°æ®æˆ–å¤æ‚æ¨¡å‹ç»“æ„çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ ç®€å•ã€é«˜æ•ˆï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿæ­ç¤ºäº†çº¯æ–‡æœ¬LLMä¸­è•´å«ç€ä¸°å¯Œçš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä¸ºæœªæ¥çš„å¤šæ¨¡æ€LLMç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬æ„Ÿå®˜æç¤ºçš„é€‰æ‹©å’Œè¡¨å¾çš„æ¯”è¾ƒæ–¹æ³•ã€‚æ„Ÿå®˜æç¤ºéœ€è¦è¶³å¤Ÿæ˜ç¡®ï¼Œèƒ½å¤Ÿå¼•å¯¼LLMæ¿€æ´»ç›¸åº”çš„æ¨¡æ€è¡¨å¾ã€‚è¡¨å¾çš„æ¯”è¾ƒæ–¹æ³•éœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¡é‡LLMç”Ÿæˆçš„tokençš„è¡¨å¾ä¸è§†è§‰æˆ–å¬è§‰ç¼–ç å™¨çš„è¡¨å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºæ‰€ä½¿ç”¨çš„LLMå’Œè§†è§‰/å¬è§‰ç¼–ç å™¨ï¼Œè®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†ä½™å¼¦ç›¸ä¼¼åº¦ç­‰æ–¹æ³•æ¥æ¯”è¾ƒè¡¨å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç®€å•çš„æ„Ÿå®˜æç¤ºï¼Œå¯ä»¥æ˜¾è‘—æé«˜çº¯æ–‡æœ¬LLMåœ¨è§†è§‰å’Œå¬è§‰ç›¸å…³ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”Ÿæˆå›¾åƒæè¿°æ—¶ï¼Œä½¿ç”¨â€œçœ‹â€çš„æç¤ºå¯ä»¥ä½¿LLMç”Ÿæˆæ›´å‡†ç¡®ã€æ›´ä¸°å¯Œçš„æè¿°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè¯†åˆ«éŸ³é¢‘ä¸­çš„äº‹ä»¶ï¼Œä¾‹å¦‚é€šè¿‡â€œå¬â€çš„æç¤ºæ¥è¯†åˆ«é¸Ÿé¸£æˆ–æ±½è½¦é¸£ç¬›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ã€å›¾åƒ/éŸ³é¢‘æè¿°ç”Ÿæˆã€ä»¥åŠæå‡è¯­è¨€æ¨¡å‹çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ç­‰é¢†åŸŸã€‚é€šè¿‡æ¿€æ´»LLMä¸­çš„æ½œåœ¨å¤šæ¨¡æ€è¡¨å¾ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°ç†è§£å’Œå¤„ç†ä¸è§†è§‰ã€å¬è§‰ç­‰æ„Ÿå®˜ä¿¡æ¯ç›¸å…³çš„ä»»åŠ¡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€äººæœºäº¤äº’ç­‰æ›´å¹¿æ³›çš„é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) trained purely on text ostensibly lack any direct perceptual experience, yet their internal representations are implicitly shaped by multimodal regularities encoded in language. We test the hypothesis that explicit sensory prompting can surface this latent structure, bringing a text-only LLM into closer representational alignment with specialist vision and audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it cues the model to resolve its next-token predictions as if they were conditioned on latent visual or auditory evidence that is never actually supplied. Our findings reveal that lightweight prompt engineering can reliably activate modality-appropriate representations in purely text-trained LLMs.

