---
layout: default
title: Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models
---

# Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01845" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01845v1</a>
  <a href="https://arxiv.org/pdf/2510.01845.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01845v1" onclick="toggleFavorite(this, '2510.01845v1', 'Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ece Takmaz, Lisa Bylinina, Jakub Dotlacil

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**å¤‡æ³¨**: Accepted to the EMNLP 2025 workshop BabyLM: Accelerating language modeling research with cognitively plausible datasets

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨¡å‹èåˆæ–¹æ³•ï¼Œæå‡å¤šæ¨¡æ€æ¨¡å‹åœ¨è¯­è¨€ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æ¨¡å‹èåˆ` `è¯­è¨€æ¨¡å‹` `BabyLM` `ä½èµ„æºå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€æ¨¡å‹åœ¨çº¯è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™å’Œæ¨¡æ‹Ÿå„¿ç«¥å­¦ä¹ çš„åœºæ™¯ä¸‹ã€‚
2. è®ºæ–‡æå‡ºæ¨¡å‹èåˆæ–¹æ³•ï¼Œé€šè¿‡èåˆå¤šæ¨¡æ€æ¨¡å‹å’Œçº¯è¯­è¨€æ¨¡å‹çš„å‚æ•°ï¼Œæå‡å¤šæ¨¡æ€æ¨¡å‹åœ¨è¯­è¨€ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹èåˆèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤šæ¨¡æ€æ¨¡å‹åœ¨çº¯è¯­è¨€ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒå…¶å¤šæ¨¡æ€æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹BabyLMæŒ‘æˆ˜èµ›çš„å¤šæ¨¡æ€èµ›é“ï¼Œæå‡ºäº†ä¸€ç§åœ¨ä½èµ„æºç¯å¢ƒä¸‹æ„å»ºç¬¦åˆå„¿ç«¥å‘å±•è§„å¾‹çš„å¤šæ¨¡æ€æ¨¡å‹çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ¨¡å‹åœ¨çº¯è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨åŠ æƒçº¿æ€§æ’å€¼èåˆå¤šæ¨¡æ€æ¨¡å‹å’Œçº¯è¯­è¨€æ¨¡å‹çš„å‚æ•°ï¼Œå³æ¨¡å‹èåˆï¼Œæ¥æå‡å¤šæ¨¡æ€æ¨¡å‹åœ¨è¯­è¨€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨ä¾§é‡è¯­æ³•çš„çº¯è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ç¡®å®è¾ƒå·®ï¼Œè€Œä¸çº¯æ–‡æœ¬æ¨¡å‹è¿›è¡Œæ¨¡å‹èåˆå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£è¿™ä¸ªé—®é¢˜ï¼ŒåŒæ—¶ä¿æŒå¤šæ¨¡æ€æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹å‚æ•°ä¼—å¤šï¼Œè®­ç»ƒæ•°æ®é‡å·¨å¤§ï¼Œè¿œè¶…å„¿ç«¥è¯­è¨€ä¹ å¾—è¿‡ç¨‹ä¸­æ¥è§¦åˆ°çš„è¯­è¨€æ•°æ®é‡ã€‚æ­¤å¤–ï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨çº¯è¯­è¨€ä»»åŠ¡ä¸Šçš„è¡¨ç°å¾€å¾€ä¸å¦‚çº¯è¯­è¨€æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨è¯­æ³•ç›¸å…³çš„ä»»åŠ¡ä¸Šã€‚å› æ­¤ï¼Œè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œæ„å»ºæ—¢èƒ½å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œåˆèƒ½ä¿æŒè‰¯å¥½è¯­è¨€èƒ½åŠ›çš„æ¨¡å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¨¡å‹èåˆï¼Œå°†å¤šæ¨¡æ€æ¨¡å‹å’Œçº¯è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ç»“åˆèµ·æ¥ã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯å°†è®­ç»ƒå¥½çš„å¤šæ¨¡æ€æ¨¡å‹å’Œçº¯è¯­è¨€æ¨¡å‹çš„å‚æ•°è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œä»è€Œä½¿èåˆåçš„æ¨¡å‹æ—¢èƒ½åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œåˆèƒ½ä¿æŒè¾ƒå¼ºçš„è¯­è¨€èƒ½åŠ›ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†ç¼“è§£å¤šæ¨¡æ€æ¨¡å‹åœ¨çº¯è¯­è¨€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨çš„æ¨¡å‹èåˆæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) åˆ†åˆ«è®­ç»ƒä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹å’Œä¸€ä¸ªçº¯è¯­è¨€æ¨¡å‹ï¼›2) ä½¿ç”¨åŠ æƒçº¿æ€§æ’å€¼æ–¹æ³•ï¼Œå°†ä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°è¿›è¡Œèåˆã€‚èåˆåçš„æ¨¡å‹å¯ä»¥åŒæ—¶å¤„ç†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œå¹¶ä¸”åœ¨çº¯è¯­è¨€ä»»åŠ¡ä¸Šå…·æœ‰è¾ƒå¥½çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ¨¡å‹èåˆæŠ€æœ¯åº”ç”¨äºå¤šæ¨¡æ€å­¦ä¹ é¢†åŸŸï¼Œå¹¶éªŒè¯äº†å…¶åœ¨æå‡å¤šæ¨¡æ€æ¨¡å‹è¯­è¨€èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä¸ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œæ¨¡å‹èåˆèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å·²æœ‰çš„çº¯è¯­è¨€æ¨¡å‹ï¼Œä»è€Œåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡é‡‡ç”¨åŠ æƒçº¿æ€§æ’å€¼ä½œä¸ºæ¨¡å‹èåˆçš„å…·ä½“æ–¹æ³•ã€‚å…·ä½“å…¬å¼ä¸ºï¼šÎ¸_merged = Î± * Î¸_multimodal + (1 - Î±) * Î¸_languageï¼Œå…¶ä¸­Î¸_mergedæ˜¯èåˆåçš„æ¨¡å‹å‚æ•°ï¼ŒÎ¸_multimodalæ˜¯å¤šæ¨¡æ€æ¨¡å‹å‚æ•°ï¼ŒÎ¸_languageæ˜¯çº¯è¯­è¨€æ¨¡å‹å‚æ•°ï¼ŒÎ±æ˜¯æƒé‡ç³»æ•°ï¼Œæ§åˆ¶ç€å¤šæ¨¡æ€æ¨¡å‹å’Œçº¯è¯­è¨€æ¨¡å‹åœ¨èåˆåçš„æ¨¡å‹ä¸­çš„è´¡çŒ®æ¯”ä¾‹ã€‚Î±çš„é€‰æ‹©æ˜¯ä¸€ä¸ªå…³é”®çš„è®¾è®¡å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨çº¯è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ç¡®å®è¾ƒå·®ï¼Œå°¤å…¶æ˜¯åœ¨ä¾§é‡è¯­æ³•çš„ä»»åŠ¡ä¸Šã€‚é€šè¿‡ä¸çº¯æ–‡æœ¬æ¨¡å‹è¿›è¡Œæ¨¡å‹èåˆï¼Œå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£è¿™ä¸ªé—®é¢˜ï¼ŒåŒæ—¶ä¿æŒå¤šæ¨¡æ€æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦å–å†³äºæƒé‡ç³»æ•°Î±çš„é€‰æ‹©ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´ç¬¦åˆå„¿ç«¥è®¤çŸ¥å‘å±•è§„å¾‹çš„å¤šæ¨¡æ€å­¦ä¹ ç³»ç»Ÿï¼Œä¾‹å¦‚å„¿ç«¥æ•™è‚²æœºå™¨äººã€æ™ºèƒ½ç©å…·ç­‰ã€‚é€šè¿‡æ¨¡å‹èåˆï¼Œå¯ä»¥æå‡è¿™äº›ç³»ç»Ÿåœ¨ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ–¹é¢çš„èƒ½åŠ›ï¼Œä»è€Œæ›´å¥½åœ°ä¸å„¿ç«¥è¿›è¡Œäº’åŠ¨å’Œäº¤æµã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–èµ„æºå—é™çš„å¤šæ¨¡æ€å­¦ä¹ åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> State-of-the-art vision-and-language models consist of many parameters and learn from enormous datasets, surpassing the amounts of linguistic data that children are exposed to as they acquire a language. This paper presents our approach to the multimodal track of the BabyLM challenge addressing this discrepancy. We develop language-only and multimodal models in low-resource settings using developmentally plausible datasets, with our multimodal models outperforming previous BabyLM baselines. One finding in the multimodal language model literature is that these models tend to underperform in \textit{language-only} tasks. Therefore, we focus on maintaining language-only abilities in multimodal models. To this end, we experiment with \textit{model merging}, where we fuse the parameters of multimodal models with those of language-only models using weighted linear interpolation. Our results corroborate the findings that multimodal models underperform in language-only benchmarks that focus on grammar, and model merging with text-only models can help alleviate this problem to some extent, while maintaining multimodal performance.

