---
layout: default
title: DCPO: Dynamic Clipping Policy Optimization
---

# DCPO: Dynamic Clipping Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02333" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.02333v2</a>
  <a href="https://arxiv.org/pdf/2509.02333.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02333v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02333v2', 'DCPO: Dynamic Clipping Policy Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, Rihui Xin

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-02 (Êõ¥Êñ∞: 2025-09-08)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**DCPOÔºöÂä®ÊÄÅË£ÅÂâ™Á≠ñÁï•‰ºòÂåñÔºåÊèêÂçáLLMÂú®ÂèØÈ™åËØÅÂ•ñÂä±‰∏ãÁöÑÊé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Á≠ñÁï•‰ºòÂåñ` `Âä®ÊÄÅË£ÅÂâ™` `ÂèØÈ™åËØÅÂ•ñÂä±`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâRLVRÊñπÊ≥ïÔºàÂ¶ÇGRPOÔºâÂú®ËÆ≠ÁªÉLLMÊó∂Â≠òÂú®Ê¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢òÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÊÄßËÉΩ„ÄÇ
2. DCPOÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥Ë£ÅÂâ™ËæπÁïåÂíå‰ΩøÁî®Âπ≥Êªë‰ºòÂäøÊ†áÂáÜÂåñÔºåÂ¢ûÂº∫tokenÁ∫ßÂà´Êé¢Á¥¢ÂíåÂìçÂ∫îÂà©Áî®Áéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDCPOÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂äÁé∞ÊúâÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜLLMÁöÑÊé®ÁêÜËÉΩÂäõÂíåËÆ≠ÁªÉÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫Âä®ÊÄÅË£ÅÂâ™Á≠ñÁï•‰ºòÂåñÔºàDCPOÔºâÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂèØÈ™åËØÅÂ•ñÂä±‰∏ãÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLVRÔºâÊ°ÜÊû∂‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇGRPOÂ∏∏Èù¢‰∏¥Ê¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢òÔºå‰∏ªË¶ÅÂéüÂõ†ÊòØtokenÁ∫ßÂà´Ê¶ÇÁéáÊØîÁéáÁöÑÂõ∫ÂÆöË£ÅÂâ™ËæπÁïå‰ª•ÂèäÁõ∏ÂêåÂ•ñÂä±ÁöÑÊ†áÂáÜÂåñÔºåÂØºËá¥Ê¢ØÂ∫¶Êõ¥Êñ∞ÊïàÁéá‰Ωé‰∏ãÔºåÁîüÊàêÂìçÂ∫îÁöÑÂà©Áî®Áéá‰∏çË∂≥„ÄÇDCPOÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂä®ÊÄÅË£ÅÂâ™Á≠ñÁï•ÔºåÂü∫‰∫étokenÁâπÂÆöÁöÑÂÖàÈ™åÊ¶ÇÁéáËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥Ë£ÅÂâ™ËæπÁïåÔºå‰ª•Â¢ûÂº∫tokenÁ∫ßÂà´ÁöÑÊé¢Á¥¢„ÄÇÂêåÊó∂ÔºåÈááÁî®Âπ≥Êªë‰ºòÂäøÊ†áÂáÜÂåñÊäÄÊúØÔºåÂú®Á¥ØÁßØËÆ≠ÁªÉÊ≠•È™§‰∏≠Ê†áÂáÜÂåñÂ•ñÂä±Ôºå‰ª•ÊèêÈ´òÂìçÂ∫îÁ∫ßÂà´ÁöÑÁîüÊàêÂìçÂ∫îÊúâÊïàÂà©Áî®Áéá„ÄÇÂú®Âõõ‰∏™Âü∫ÂáÜÊµãËØïÂíåÂõõ‰∏™‰∏çÂêåÊ®°Âûã‰∏äÔºåDCPOÂùáÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®Qwen2.5-Math-7BÊ®°Âûã‰∏äÔºåAIME24Âü∫ÂáÜÊµãËØï‰∏≠ÔºåË¥™Â©™Ëß£Á†Å‰∏ãAvg@1ËææÂà∞46.7Ôºå32Ê¨°ÈááÊ†∑‰∏ãAvg@32ËææÂà∞38.8ÔºåË∂ÖËøá‰∫ÜDAPO„ÄÅGRPOÂíåGSPO„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂ¶ÇGRPOÔºåÂú®ËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊó∂ÔºåÁî±‰∫étokenÁ∫ßÂà´Ê¶ÇÁéáÊØîÁéáÁöÑÂõ∫ÂÆöË£ÅÂâ™ËæπÁïåÂíåÁõ∏ÂêåÂ•ñÂä±ÁöÑÊ†áÂáÜÂåñÔºåÂØºËá¥Ê¢ØÂ∫¶Ê∂àÂ§±Ôºå‰ΩøÂæóÊ®°ÂûãÊó†Ê≥ïÊúâÊïàÂ≠¶‰π†ÂíåÂà©Áî®ÁîüÊàêÁöÑÊï∞ÊçÆÔºåÊúÄÁªàÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDCPOÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥Ë£ÅÂâ™ËæπÁïåÊù•Â¢ûÂº∫tokenÁ∫ßÂà´ÁöÑÊé¢Á¥¢ÔºåÂπ∂‰ΩøÁî®Âπ≥Êªë‰ºòÂäøÊ†áÂáÜÂåñÊù•ÊèêÈ´òÂìçÂ∫îÁ∫ßÂà´ÁöÑÁîüÊàêÂìçÂ∫îÂà©Áî®Áéá„ÄÇÂä®ÊÄÅË£ÅÂâ™ËæπÁïåÂÖÅËÆ∏Ê®°ÂûãÂú®ËÆ≠ÁªÉÂàùÊúüËøõË°åÊõ¥ÂπøÊ≥õÁöÑÊé¢Á¥¢ÔºåÈÅøÂÖçËøáÊó©Êî∂ÊïõÂà∞Â±ÄÈÉ®ÊúÄ‰ºòËß£„ÄÇÂπ≥Êªë‰ºòÂäøÊ†áÂáÜÂåñÂàôÁ°Æ‰øùÂ•ñÂä±‰ø°Âè∑Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Êõ¥Âä†Á®≥ÂÆöÂíåÊúâÊïà„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDCPOÁöÑÊï¥‰ΩìÊ°ÜÊû∂‰ªçÁÑ∂ÊòØÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÁ≠ñÁï•‰ºòÂåñÔºå‰ΩÜÂÖ∂ÂÖ≥ÈîÆÂú®‰∫é‰∏§‰∏™Ê†∏ÂøÉÊ®°ÂùóÁöÑÊîπËøõÔºö‰∏ÄÊòØÂä®ÊÄÅË£ÅÂâ™Ê®°ÂùóÔºåÁî®‰∫éËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥tokenÁ∫ßÂà´Ê¶ÇÁéáÊØîÁéáÁöÑË£ÅÂâ™ËæπÁïåÔºõ‰∫åÊòØÂπ≥Êªë‰ºòÂäøÊ†áÂáÜÂåñÊ®°ÂùóÔºåÁî®‰∫éÂú®Á¥ØÁßØËÆ≠ÁªÉÊ≠•È™§‰∏≠Ê†áÂáÜÂåñÂ•ñÂä±„ÄÇÊï¥‰∏™ÊµÅÁ®ãÂåÖÊã¨ÔºöÊ®°ÂûãÁîüÊàêÂìçÂ∫î„ÄÅËÆ°ÁÆóÂ•ñÂä±„ÄÅÂä®ÊÄÅË£ÅÂâ™Ê¶ÇÁéáÊØîÁéá„ÄÅÊ†áÂáÜÂåñ‰ºòÂäøÂáΩÊï∞„ÄÅÊõ¥Êñ∞Á≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDCPOÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂä®ÊÄÅË£ÅÂâ™Á≠ñÁï•„ÄÇ‰∏éÂõ∫ÂÆöË£ÅÂâ™ËæπÁïå‰∏çÂêåÔºåDCPOÊ†πÊçÆtokenÁâπÂÆöÁöÑÂÖàÈ™åÊ¶ÇÁéáËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥Ë£ÅÂâ™ËæπÁïå„ÄÇËøôÊÑèÂë≥ÁùÄÂØπ‰∫éÈÇ£‰∫õÊ®°Âûã‰∏çÂ§™Á°ÆÂÆöÁöÑtokenÔºåË£ÅÂâ™ËæπÁïå‰ºöÊõ¥ÂÆΩÊùæÔºåÂÖÅËÆ∏Ê®°ÂûãËøõË°åÊõ¥Â§öÁöÑÊé¢Á¥¢„ÄÇËøôÁßçÂä®ÊÄÅË∞ÉÊï¥Êú∫Âà∂ËÉΩÂ§üÊúâÊïàÂú∞ÈÅøÂÖçÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢òÔºåÂπ∂ÊèêÈ´òÊ®°ÂûãÁöÑÂ≠¶‰π†ÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂä®ÊÄÅË£ÅÂâ™ËæπÁïåÁöÑËÆ°ÁÆóÊñπÂºèÊòØÂü∫‰∫étokenÁöÑÂÖàÈ™åÊ¶ÇÁéá„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåË£ÅÂâ™ËæπÁïåÂèØ‰ª•ËÆæÁΩÆ‰∏∫ÂÖàÈ™åÊ¶ÇÁéáÁöÑÂáΩÊï∞Ôºå‰æãÂ¶ÇÔºåË£ÅÂâ™ËåÉÂõ¥ÂèØ‰ª•‰∏éÂÖàÈ™åÊ¶ÇÁéáÊàêÂèçÊØî„ÄÇÂπ≥Êªë‰ºòÂäøÊ†áÂáÜÂåñÂàôÈÄöËøáÂú®Á¥ØÁßØËÆ≠ÁªÉÊ≠•È™§‰∏≠ËÆ°ÁÆóÁßªÂä®Âπ≥ÂùáÂíåÊ†áÂáÜÂ∑ÆÊù•ÂÆûÁé∞„ÄÇÊçüÂ§±ÂáΩÊï∞‰ªçÁÑ∂ÊòØÂü∫‰∫éÁ≠ñÁï•Ê¢ØÂ∫¶ÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩÜÂä†ÂÖ•‰∫ÜÂä®ÊÄÅË£ÅÂâ™Âíå‰ºòÂäøÊ†áÂáÜÂåñÂêéÁöÑÊ¢ØÂ∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DCPOÂú®AIME24Âü∫ÂáÜÊµãËØï‰∏≠ÔºåQwen2.5-Math-7BÊ®°Âûã‰∏äÔºåË¥™Â©™Ëß£Á†Å‰∏ãAvg@1ËææÂà∞46.7Ôºå32Ê¨°ÈááÊ†∑‰∏ãAvg@32ËææÂà∞38.8ÔºåÊòæËëóË∂ÖË∂äDAPO (36.7/31.6)„ÄÅGRPO (36.7/32.1)ÂíåGSPO (40.0/34.9)„ÄÇÂú®AIME25Âü∫ÂáÜÊµãËØï‰∏≠ÔºåQwen2.5-14BÊ®°Âûã‰∏äÔºåDCPOËææÂà∞(23.3/19.0)ÔºåË∂ÖËøáGRPO (13.3/10.5)„ÄÅDAPO (20.0/15.3)ÂíåGSPO (16.7/9.9)„ÄÇDCPOÂú®Âõõ‰∏™Ê®°Âûã‰∏äÁöÑÈùûÈõ∂‰ºòÂäøÂπ≥ÂùáÊèêÂçá‰∫Ü28%ÔºåËÆ≠ÁªÉÊïàÁéáÊòØDAPOÁöÑ‰∏§ÂÄçÔºåtokenË£ÅÂâ™Áéá‰πüÊòæËëóÈôç‰Ωé„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DCPOÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêÑÁßçÈúÄË¶ÅÊé®ÁêÜÂíåÂÜ≥Á≠ñÁöÑ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÔºå‰æãÂ¶ÇÊï∞Â≠¶ÈóÆÈ¢òÊ±ÇËß£„ÄÅ‰ª£Á†ÅÁîüÊàê„ÄÅÁü•ËØÜÈóÆÁ≠îÁ≠â„ÄÇÈÄöËøáÊõ¥ÊúâÊïàÂú∞Âà©Áî®ÁîüÊàêÊï∞ÊçÆËøõË°åÂº∫ÂåñÂ≠¶‰π†ÔºåDCPOÂèØ‰ª•Â∏ÆÂä©Ê®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊéåÊè°Â§çÊùÇ‰ªªÂä°ÁöÑÈÄªËæëÂíåËßÑÂàôÔºå‰ªéËÄåÊèêÈ´òÂÖ∂Ëß£ÂÜ≥ÈóÆÈ¢òÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éÂÖ∂‰ªñÂ∫èÂàóÁîüÊàê‰ªªÂä°Ôºå‰æãÂ¶ÇÊú∫Âô®ÁøªËØëÂíåÊñáÊú¨ÊëòË¶Å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization(DCPO), which introduces a dynamic clipping strategy that adaptively adjusts clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing DAPO (36.7/31.6), GRPO (36.7/32.1) and GSPO (40.0/34.9) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5), DAPO (20.0/15.3) and GSPO (16.7/9.9). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.

