---
layout: default
title: Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR
---

# Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02522" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.02522v1</a>
  <a href="https://arxiv.org/pdf/2509.02522.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02522v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02522v1', 'Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang

**ÂàÜÁ±ª**: cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-02

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/ritzz-ai/PACS)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PACSÊ°ÜÊû∂ÔºåÈÄöËøáÁõëÁù£Â≠¶‰π†ÈöêÂºèËÄ¶ÂêàActor-CriticÔºåÊèêÂçáRLVR‰∏≠LLMÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `ÂèØÈ™åËØÅÂ•ñÂä±` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÁõëÁù£Â≠¶‰π†` `Á≠ñÁï•Ê¢ØÂ∫¶` `Actor-Critic` `Êï∞Â≠¶Êé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâRLVRÊñπÊ≥ïÂú®Â•ñÂä±Á®ÄÁñèÂíåÁ≠ñÁï•Ê¢ØÂ∫¶Êõ¥Êñ∞‰∏çÁ®≥ÂÆöÊñπÈù¢Â≠òÂú®ÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊñπÊ≥ï‰∏≠„ÄÇ
2. PACSÊ°ÜÊû∂Â∞ÜRLVRÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÁõëÁù£Â≠¶‰π†‰ªªÂä°ÔºåÈÄöËøáÈ¢ÑÊµãÂ•ñÂä±Ê†áÁ≠æÈöêÂºèËÄ¶ÂêàActorÂíåCriticÔºåÂÆûÁé∞Êõ¥Á®≥ÂÆöÁöÑËÆ≠ÁªÉ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåPACSÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏ä‰ºò‰∫éPPOÂíåGRPOÁ≠âÂü∫Á∫øÔºåÊòæËëóÊèêÂçá‰∫ÜLLMÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫PACSÁöÑÊñ∞ÂûãRLVRÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâRLVRÊñπÊ≥ï‰∏≠Â•ñÂä±Á®ÄÁñèÂíåÁ≠ñÁï•Ê¢ØÂ∫¶Êõ¥Êñ∞‰∏çÁ®≥ÂÆöÁ≠âÈóÆÈ¢ò„ÄÇPACSÈÄöËøáÂ∞ÜÁªìÊûúÂ•ñÂä±ËßÜ‰∏∫ÂèØÈ¢ÑÊµãÁöÑÊ†áÁ≠æÔºåÂ∞ÜRLVRÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™ÁõëÁù£Â≠¶‰π†‰ªªÂä°ÔºåËØ•‰ªªÂä°Âü∫‰∫éÁî±Á≠ñÁï•Ê®°ÂûãÂèÇÊï∞ÂåñÁöÑËØÑÂàÜÂáΩÊï∞ÔºåÂπ∂‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ËøõË°å‰ºòÂåñ„ÄÇËØ¶ÁªÜÁöÑÊ¢ØÂ∫¶ÂàÜÊûêË°®ÊòéÔºåËøôÁßçÁõëÁù£Â≠¶‰π†ÂΩ¢ÂºèÂú®Êú¨Ë¥®‰∏äÊÅ¢Â§ç‰∫ÜÁªèÂÖ∏ÁöÑÁ≠ñÁï•Ê¢ØÂ∫¶Êõ¥Êñ∞ÔºåÂêåÊó∂ÈöêÂºèÂú∞ËÄ¶Âêà‰∫ÜactorÂíåcriticÁöÑËßíËâ≤Ôºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊõ¥Á®≥ÂÆöÂíåÈ´òÊïàÁöÑËÆ≠ÁªÉ„ÄÇÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏äÁöÑÂü∫ÂáÜÊµãËØïË°®ÊòéÔºåPACS‰ºò‰∫éÂº∫Â§ßÁöÑRLVRÂü∫Á∫øÊñπÊ≥ïÔºå‰æãÂ¶ÇPPOÂíåGRPOÔºåÂÆûÁé∞‰∫ÜÂçìË∂äÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåPACSÂú®AIME 2025‰∏äÂÆûÁé∞‰∫Ü59.78%ÁöÑpass@256ÔºåÊØîPPOÂíåGRPOÂàÜÂà´ÊèêÈ´ò‰∫Ü13.32Âíå14.36‰∏™ÁôæÂàÜÁÇπ„ÄÇËøôÁßçÁÆÄÂçïËÄåÂº∫Â§ßÁöÑÊ°ÜÊû∂‰∏∫LLMÂü∫‰∫éÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂêéËÆ≠ÁªÉÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÊúâÂ∏åÊúõÁöÑÈÄîÂæÑ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥RLVRÔºàReinforcement Learning with Verifiable RewardsÔºâÊ°ÜÊû∂‰∏ãÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°Êó∂ÔºåÁî±‰∫éÂ•ñÂä±‰ø°Âè∑Á®ÄÁñèÂíåÁ≠ñÁï•Ê¢ØÂ∫¶Êõ¥Êñ∞‰∏çÁ®≥ÂÆöËÄåÂØºËá¥ÁöÑËÆ≠ÁªÉÂõ∞ÈöæÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑPPOÂíåGRPOÔºåÈöæ‰ª•ÊúâÊïàÂà©Áî®ÂèØÈ™åËØÅÁöÑÂ•ñÂä±Êù•ÊåáÂØºÁ≠ñÁï•‰ºòÂåñÔºåÈôêÂà∂‰∫ÜLLMsÂú®Êï∞Â≠¶ÂíåÁºñÁ®ãÁ≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöPACSÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜRLVRÈóÆÈ¢òÈáçÊñ∞Ë°®Ëø∞‰∏∫‰∏Ä‰∏™ÁõëÁù£Â≠¶‰π†ÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉÂ∞ÜÂèØÈ™åËØÅÁöÑÁªìÊûúÂ•ñÂä±ËßÜ‰∏∫‰∏Ä‰∏™ÂèØÈ¢ÑÊµãÁöÑÊ†áÁ≠æÔºåÁÑ∂ÂêéËÆ≠ÁªÉ‰∏Ä‰∏™ËØÑÂàÜÂáΩÊï∞ÔºàÁî±Á≠ñÁï•Ê®°ÂûãÂèÇÊï∞ÂåñÔºâÊù•È¢ÑÊµãËøô‰∏™Ê†áÁ≠æ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåPACSÂ∞ÜÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÁ≠ñÁï•‰ºòÂåñÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™ÁõëÁù£Â≠¶‰π†‰∏≠ÁöÑÂàÜÁ±ªÈóÆÈ¢òÔºå‰ªéËÄåÂèØ‰ª•Âà©Áî®‰∫§ÂèâÁÜµÊçüÂ§±Á≠âÊàêÁÜüÁöÑÁõëÁù£Â≠¶‰π†ÊäÄÊúØËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPACSÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) ÁªôÂÆö‰∏Ä‰∏™ËæìÂÖ•ÈóÆÈ¢òÔºåLLMÁîüÊàê‰∏Ä‰∏™ÂÄôÈÄâÁ≠îÊ°à„ÄÇ2) ‰ΩøÁî®ÂèØÈ™åËØÅÁöÑÂ•ñÂä±ÂáΩÊï∞ËØÑ‰º∞ËØ•Á≠îÊ°àÔºåÂæóÂà∞‰∏Ä‰∏™Â•ñÂä±ÂÄº„ÄÇ3) Â∞ÜËØ•Â•ñÂä±ÂÄºËßÜ‰∏∫‰∏Ä‰∏™ÁõëÁù£Â≠¶‰π†ÁöÑÊ†áÁ≠æ„ÄÇ4) ‰ΩøÁî®Á≠ñÁï•Ê®°ÂûãÂèÇÊï∞ÂåñÁöÑËØÑÂàÜÂáΩÊï∞È¢ÑÊµãËØ•Ê†áÁ≠æ„ÄÇ5) ‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞‰ºòÂåñËØÑÂàÜÂáΩÊï∞Ôºå‰ªéËÄåÈöêÂºèÂú∞Êõ¥Êñ∞Á≠ñÁï•Ê®°Âûã„ÄÇËøô‰∏™ËøáÁ®ãËø≠‰ª£ËøõË°åÔºåÁõ¥Âà∞Á≠ñÁï•Ê®°ÂûãÊî∂Êïõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPACSÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂÆÉÈÄöËøáÁõëÁù£Â≠¶‰π†Ê°ÜÊû∂ÂÆûÁé∞‰∫ÜActor-CriticÁöÑÈöêÂºèËÄ¶Âêà„ÄÇ‰º†ÁªüÁöÑActor-CriticÊñπÊ≥ïÈúÄË¶ÅÂàÜÂà´ËÆ≠ÁªÉActorÔºàÁ≠ñÁï•Ê®°ÂûãÔºâÂíåCriticÔºà‰ª∑ÂÄºÂáΩÊï∞ÔºâÔºåËÄåPACSÈÄöËøáÂ∞ÜÂ•ñÂä±È¢ÑÊµãÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÁõëÁù£Â≠¶‰π†ÈóÆÈ¢òÔºå‰ΩøÂæóActorÂíåCriticÁöÑËßíËâ≤Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ëá™ÁÑ∂Âú∞ÂçèÂêåÂ∑•‰Ωú„ÄÇËøôÁßçÈöêÂºèËÄ¶ÂêàÈÅøÂÖç‰∫Ü‰º†ÁªüActor-CriticÊñπÊ≥ï‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöPACSÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Á≠ñÁï•Ê®°ÂûãÁõ¥Êé•‰Ωú‰∏∫ËØÑÂàÜÂáΩÊï∞ÔºåÈÅøÂÖç‰∫ÜÂºïÂÖ•È¢ùÂ§ñÁöÑÁΩëÁªúÁªìÊûÑ„ÄÇ2) ‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞‰Ωú‰∏∫‰ºòÂåñÁõÆÊ†áÔºåÁÆÄÂåñ‰∫ÜËÆ≠ÁªÉËøáÁ®ã„ÄÇ3) ÈÄöËøáÊ¢ØÂ∫¶ÂàÜÊûêËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÂú®Êú¨Ë¥®‰∏äÁ≠â‰ª∑‰∫é‰º†ÁªüÁöÑÁ≠ñÁï•Ê¢ØÂ∫¶Êõ¥Êñ∞Ôºå‰øùËØÅ‰∫ÜÁÆóÊ≥ïÁöÑÊî∂ÊïõÊÄß„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁöÑÈÄâÊã©ÂèñÂÜ≥‰∫éÂÖ∑‰ΩìÁöÑLLMÂíå‰ªªÂä°„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

PACSÂú®AIME 2025Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåpass@256ÊåáÊ†áËææÂà∞59.78%ÔºåÁõ∏ÊØî‰∫éPPOÂíåGRPOÂü∫Á∫øÂàÜÂà´ÊèêÈ´ò‰∫Ü13.32Âíå14.36‰∏™ÁôæÂàÜÁÇπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPACSÊ°ÜÊû∂ËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®ÂèØÈ™åËØÅÁöÑÂ•ñÂä±‰ø°Âè∑ÔºåÊèêÂçáLLMsÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂπ∂Âú®Â§çÊùÇ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PACSÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÊèêÂçáLLMsÂú®ÈúÄË¶ÅÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞Ôºå‰æãÂ¶ÇÊï∞Â≠¶ÈóÆÈ¢òÊ±ÇËß£„ÄÅ‰ª£Á†ÅÁîüÊàê„ÄÅÈÄªËæëÊé®ÁêÜÁ≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂà©Áî®Â§ñÈÉ®ÂèçÈ¶àÊù•ÊåáÂØºLLMsÁöÑËÆ≠ÁªÉÔºåÊèêÈ´òÂÖ∂ËæìÂá∫ÁªìÊûúÁöÑÂèØÈù†ÊÄßÂíåÂáÜÁ°ÆÊÄßÔºå‰ªéËÄåÂú®ÊïôËÇ≤„ÄÅÁßëÁ†î„ÄÅËΩØ‰ª∂ÂºÄÂèëÁ≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor $\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.

