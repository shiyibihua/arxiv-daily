---
layout: default
title: Scaling behavior of large language models in emotional safety classification across sizes and tasks
---

# Scaling behavior of large language models in emotional safety classification across sizes and tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04512" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04512v1</a>
  <a href="https://arxiv.org/pdf/2509.04512.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04512v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04512v1', 'Scaling behavior of large language models in emotional safety classification across sizes and tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Edoardo Pinzuti, Oliver TÃ¼scher, AndrÃ© Ferreira Castro

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶LLMåœ¨æƒ…æ„Ÿå®‰å…¨åˆ†ç±»ä¸­çš„è§„æ¨¡æ•ˆåº”ï¼Œæ¢ç´¢è½»é‡çº§æ¨¡å‹åœ¨å¿ƒç†å¥åº·é¢†åŸŸçš„åº”ç”¨æ½œåŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æƒ…æ„Ÿå®‰å…¨åˆ†ç±»` `å¿ƒç†å¥åº·` `è§„æ¨¡æ•ˆåº”` `è½»é‡çº§å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨å¤„ç†æƒ…æ„Ÿå®‰å…¨å†…å®¹æ—¶ï¼Œç¼ºä¹å¯¹å…¶è§„æ¨¡æ•ˆåº”çš„æ·±å…¥ç†è§£ï¼Œé™åˆ¶äº†å…¶åœ¨å¿ƒç†å¥åº·é¢†åŸŸçš„åº”ç”¨ã€‚
2. é€šè¿‡æ„å»ºæ–°çš„æ•°æ®é›†å¹¶è¯„ä¼°ä¸åŒè§„æ¨¡çš„LLaMAæ¨¡å‹ï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹è§„æ¨¡ä¸æƒ…æ„Ÿå®‰å…¨åˆ†ç±»æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè½»é‡çº§å¾®è°ƒå¯ä»¥ä½¿å°å‹æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¾¾åˆ°ä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œé™ä½äº†éƒ¨ç½²æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†æƒ…æ„Ÿæ•æ„Ÿå†…å®¹æ—¶çš„è§„æ¨¡æ•ˆåº”ï¼Œè¿™å¯¹äºæ„å»ºå®‰å…¨å¯é çš„ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨å¿ƒç†å¥åº·é¢†åŸŸã€‚ç ”ç©¶èšç„¦äºä¸¤ä¸ªå…³é”®ä»»åŠ¡ï¼šæƒ…æ„Ÿå®‰å…¨çš„ä¸‰å…ƒåˆ†ç±»ï¼ˆå®‰å…¨ã€ä¸å®‰å…¨ã€ä¸´ç•Œï¼‰å’Œä½¿ç”¨å…­ç±»å®‰å…¨é£é™©åˆ†ç±»çš„å¤šæ ‡ç­¾åˆ†ç±»ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œé€šè¿‡åˆå¹¶å¤šä¸ªäººå·¥æ’°å†™çš„å¿ƒç†å¥åº·æ•°æ®é›†ï¼ˆ>15Kæ ·æœ¬ï¼‰ï¼Œå¹¶ä½¿ç”¨ChatGPTç”Ÿæˆçš„æƒ…æ„Ÿé‡æ–°è§£é‡Šæç¤ºè¿›è¡Œæ‰©å……ã€‚è¯„ä¼°äº†å››ä¸ªLLaMAæ¨¡å‹ï¼ˆ1Bã€3Bã€8Bã€70Bï¼‰åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸‹çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œæ›´å¤§çš„LLMåœ¨å¤šæ ‡ç­¾åˆ†ç±»å’Œé›¶æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°æ›´å¼ºã€‚ç„¶è€Œï¼Œè½»é‡çº§å¾®è°ƒä½¿å¾—1Bæ¨¡å‹åœ¨å¤šä¸ªé«˜æ•°æ®ç±»åˆ«ä¸­è¾¾åˆ°äº†ä¸æ›´å¤§æ¨¡å‹å’ŒBERTç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ¨ç†æ—¶ä»…éœ€<2GB VRAMã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ›´å°çš„ã€è®¾å¤‡ä¸Šçš„æ¨¡å‹å¯ä»¥ä½œä¸ºæ•æ„Ÿåº”ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæä¾›è§£é‡Šæƒ…æ„Ÿä¸Šä¸‹æ–‡å’Œç»´æŒå®‰å…¨å¯¹è¯è¾¹ç•Œçš„èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†æ²»ç–—æ€§LLMåº”ç”¨å’Œå®‰å…¨å…³é”®ç³»ç»Ÿçš„å¯æ‰©å±•å¯¹é½çš„å…³é”®æ„ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æƒ…æ„Ÿå®‰å…¨åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¿ƒç†å¥åº·é¢†åŸŸã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§å‹æ¨¡å‹ï¼Œä½†ç¼ºä¹å¯¹æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´å…³ç³»çš„æ·±å…¥ç†è§£ï¼Œå¹¶ä¸”å¿½ç•¥äº†å°å‹æ¨¡å‹åœ¨éšç§ä¿æŠ¤å’Œèµ„æºå—é™ç¯å¢ƒä¸‹çš„åº”ç”¨æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°ä¸åŒè§„æ¨¡çš„LLaMAæ¨¡å‹åœ¨æƒ…æ„Ÿå®‰å…¨åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¥æ­ç¤ºLLMçš„è§„æ¨¡æ•ˆåº”ã€‚åŒæ—¶ï¼Œæ¢ç´¢è½»é‡çº§å¾®è°ƒæ–¹æ³•ï¼Œä»¥ä½¿å°å‹æ¨¡å‹èƒ½å¤Ÿåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¾¾åˆ°ä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œä»è€Œé™ä½éƒ¨ç½²æˆæœ¬å¹¶æé«˜éšç§æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®æ„å»ºã€æ¨¡å‹é€‰æ‹©ã€å®éªŒè®¾ç½®å’Œç»“æœåˆ†æå››ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ„å»ºä¸€ä¸ªåŒ…å«æƒ…æ„Ÿå®‰å…¨æ ‡ç­¾çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚ç„¶åï¼Œé€‰æ‹©ä¸åŒè§„æ¨¡çš„LLaMAæ¨¡å‹ï¼ˆ1Bã€3Bã€8Bã€70Bï¼‰ä½œä¸ºç ”ç©¶å¯¹è±¡ã€‚æ¥ç€ï¼Œåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸‹ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨æƒ…æ„Ÿå®‰å…¨ä¸‰å…ƒåˆ†ç±»å’Œå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœ€åï¼Œåˆ†æå®éªŒç»“æœï¼Œæ­ç¤ºæ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è¯„ä¼°è½»é‡çº§å¾®è°ƒæ–¹æ³•çš„æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ„å»ºäº†ä¸€ä¸ªæ–°çš„ã€åŒ…å«æƒ…æ„Ÿå®‰å…¨æ ‡ç­¾çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°LLMï¼›2) ç³»ç»Ÿåœ°ç ”ç©¶äº†ä¸åŒè§„æ¨¡çš„LLaMAæ¨¡å‹åœ¨æƒ…æ„Ÿå®‰å…¨åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†LLMçš„è§„æ¨¡æ•ˆåº”ï¼›3) æ¢ç´¢äº†è½»é‡çº§å¾®è°ƒæ–¹æ³•ï¼Œä»¥ä½¿å°å‹æ¨¡å‹èƒ½å¤Ÿåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¾¾åˆ°ä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ„å»ºæ–¹é¢ï¼Œè®ºæ–‡åˆå¹¶äº†å¤šä¸ªäººå·¥æ’°å†™çš„å¿ƒç†å¥åº·æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ChatGPTç”Ÿæˆçš„æƒ…æ„Ÿé‡æ–°è§£é‡Šæç¤ºè¿›è¡Œæ‰©å……ï¼Œä»¥æé«˜æ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚åœ¨æ¨¡å‹è®­ç»ƒæ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå¹¶ä½¿ç”¨AdamWä¼˜åŒ–å™¨è¿›è¡Œä¼˜åŒ–ã€‚åœ¨å®éªŒè®¾ç½®æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒä¸‰ç§è®¾ç½®ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ›´å¤§çš„LLMåœ¨å¤šæ ‡ç­¾åˆ†ç±»å’Œé›¶æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°æ›´å¼ºã€‚ç„¶è€Œï¼Œé€šè¿‡è½»é‡çº§å¾®è°ƒï¼Œ1Bæ¨¡å‹åœ¨å¤šä¸ªé«˜æ•°æ®ç±»åˆ«ä¸­è¾¾åˆ°äº†ä¸æ›´å¤§æ¨¡å‹å’ŒBERTç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ¨ç†æ—¶ä»…éœ€<2GB VRAMã€‚è¿™è¡¨æ˜å°å‹æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”å¯ä»¥é™ä½éƒ¨ç½²æˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘éšç§ä¿æŠ¤çš„å¿ƒç†å¥åº·è¾…åŠ©å·¥å…·ï¼Œä¾‹å¦‚æƒ…æ„Ÿæ”¯æŒèŠå¤©æœºå™¨äººã€å¿ƒç†å¥åº·é£é™©è¯„ä¼°ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼Œå¯ä»¥åœ¨è®¾å¤‡æœ¬åœ°è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œé¿å…å°†æ•æ„Ÿæ•°æ®ä¸Šä¼ åˆ°äº‘ç«¯ï¼Œä»è€Œä¿æŠ¤ç”¨æˆ·éšç§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¸ºå¼€å‘å®‰å…¨å¯é çš„æ²»ç–—æ€§LLMåº”ç”¨æä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding how large language models (LLMs) process emotionally sensitive content is critical for building safe and reliable systems, particularly in mental health contexts. We investigate the scaling behavior of LLMs on two key tasks: trinary classification of emotional safety (safe vs. unsafe vs. borderline) and multi-label classification using a six-category safety risk taxonomy. To support this, we construct a novel dataset by merging several human-authored mental health datasets (> 15K samples) and augmenting them with emotion re-interpretation prompts generated via ChatGPT. We evaluate four LLaMA models (1B, 3B, 8B, 70B) across zero-shot, few-shot, and fine-tuning settings. Our results show that larger LLMs achieve stronger average performance, particularly in nuanced multi-label classification and in zero-shot settings. However, lightweight fine-tuning allowed the 1B model to achieve performance comparable to larger models and BERT in several high-data categories, while requiring <2GB VRAM at inference. These findings suggest that smaller, on-device models can serve as viable, privacy-preserving alternatives for sensitive applications, offering the ability to interpret emotional context and maintain safe conversational boundaries. This work highlights key implications for therapeutic LLM applications and the scalable alignment of safety-critical systems.

