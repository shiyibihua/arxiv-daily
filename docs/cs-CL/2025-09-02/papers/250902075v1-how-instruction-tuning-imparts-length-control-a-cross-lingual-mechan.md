---
layout: default
title: How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis
---

# How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02075" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02075v1</a>
  <a href="https://arxiv.org/pdf/2509.02075.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02075v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02075v1', 'How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Elisabetta Rocchetti, Alfio Ferrara

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶æŒ‡ä»¤è°ƒä¼˜å¦‚ä½•èµ‹äºˆå¤§è¯­è¨€æ¨¡å‹é•¿åº¦æ§åˆ¶èƒ½åŠ›ï¼šä¸€ç§è·¨è¯­è¨€çš„æœºåˆ¶åˆ†æ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æŒ‡ä»¤è°ƒä¼˜` `é•¿åº¦æ§åˆ¶` `æœºåˆ¶åˆ†æ` `è·¨è¯­è¨€ç ”ç©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç‰¹å®šé•¿åº¦çš„æ–‡æœ¬æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥ç²¾ç¡®æ§åˆ¶è¾“å‡ºé•¿åº¦ã€‚
2. è¯¥è®ºæ–‡é€šè¿‡åˆ†ææ¨¡å‹å†…éƒ¨ç»„ä»¶çš„è´¡çŒ®ï¼Œæ­ç¤ºæŒ‡ä»¤è°ƒä¼˜å¦‚ä½•æå‡é•¿åº¦æ§åˆ¶èƒ½åŠ›ã€‚
3. ç ”ç©¶å‘ç°æŒ‡ä»¤è°ƒä¼˜ä½¿æ¨¡å‹æ›´æ·±å±‚ç»„ä»¶ä¸“é—¨åŒ–ï¼Œä¸åŒè¯­è¨€å¯èƒ½é‡‡ç”¨ä¸åŒçš„è¡¥å¿æœºåˆ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éµå¾ªæ˜¾å¼é•¿åº¦çº¦æŸæ–¹é¢ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç”Ÿæˆå…·æœ‰ç²¾ç¡®å­—æ•°çš„æ–‡æœ¬ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è°ƒæŸ¥åŸºç¡€æ¨¡å‹åŠå…¶æŒ‡ä»¤è°ƒä¼˜åçš„æ¨¡å‹åœ¨è‹±è¯­å’Œæ„å¤§åˆ©è¯­ä¸­è¿›è¡Œé•¿åº¦æ§åˆ¶æ–‡æœ¬ç”Ÿæˆæ—¶çš„å·®å¼‚ã€‚æˆ‘ä»¬ä½¿ç”¨ç´¯ç§¯åŠ æƒå½’å› ï¼ˆCumulative Weighted Attributionï¼‰ï¼Œä¸€ç§æºè‡ªç›´æ¥Logitå½’å› çš„æŒ‡æ ‡ï¼Œåˆ†æäº†æ€§èƒ½å’Œå†…éƒ¨ç»„ä»¶çš„è´¡çŒ®ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒæŒ‡ä»¤è°ƒä¼˜ä¸»è¦é€šè¿‡ä¸“é—¨åŒ–æ¨¡å‹æ›´æ·±å±‚çš„ç»„ä»¶æ¥æ˜¾è‘—æé«˜é•¿åº¦æ§åˆ¶èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è‹±è¯­ä¸­ï¼ŒITæ¨¡å‹åæœŸå±‚çš„æ³¨æ„åŠ›å¤´æ˜¾ç¤ºå‡ºè¶Šæ¥è¶Šç§¯æçš„è´¡çŒ®ã€‚åœ¨æ„å¤§åˆ©è¯­ä¸­ï¼Œè™½ç„¶æ³¨æ„åŠ›è´¡çŒ®æœ‰æ‰€å‡å¼±ï¼Œä½†æœ€åä¸€å±‚çš„MLPè¡¨ç°å‡ºæ›´å¼ºçš„ç§¯æä½œç”¨ï¼Œè¡¨æ˜å­˜åœ¨ä¸€ç§è¡¥å¿æœºåˆ¶ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒæŒ‡ä»¤è°ƒä¼˜é‡æ–°é…ç½®äº†åæœŸå±‚ä»¥é€‚åº”ä»»åŠ¡ï¼Œç»„ä»¶çº§åˆ«çš„ç­–ç•¥å¯èƒ½ä¼šé€‚åº”è¯­è¨€ç¯å¢ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­éš¾ä»¥ç²¾ç¡®æ§åˆ¶è¾“å‡ºé•¿åº¦çš„é—®é¢˜ã€‚ç°æœ‰çš„åŸºç¡€æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œå¾€å¾€æ— æ³•å¾ˆå¥½åœ°æ»¡è¶³ç”¨æˆ·å¯¹æ–‡æœ¬é•¿åº¦çš„æ˜ç¡®è¦æ±‚ï¼Œä¾‹å¦‚ç”Ÿæˆå›ºå®šå­—æ•°çš„æ‘˜è¦æˆ–å›ç­”ã€‚è¿™é™åˆ¶äº†LLMsåœ¨å®é™…åº”ç”¨ä¸­çš„å¯ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æ¯”åˆ†æåŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒä¼˜åçš„æ¨¡å‹ï¼Œç ”ç©¶æŒ‡ä»¤è°ƒä¼˜å¦‚ä½•æ”¹å˜æ¨¡å‹çš„å†…éƒ¨è¿ä½œæœºåˆ¶ï¼Œä»è€Œèµ‹äºˆæ¨¡å‹é•¿åº¦æ§åˆ¶èƒ½åŠ›ã€‚é€šè¿‡åˆ†ææ¨¡å‹å†…éƒ¨ç»„ä»¶ï¼ˆå¦‚æ³¨æ„åŠ›å¤´å’ŒMLPå±‚ï¼‰çš„è´¡çŒ®ï¼Œæ­ç¤ºå“ªäº›ç»„ä»¶åœ¨é•¿åº¦æ§åˆ¶ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä»¥åŠä¸åŒè¯­è¨€ä¹‹é—´æ˜¯å¦å­˜åœ¨å·®å¼‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºå½’å› åˆ†æçš„æ–¹æ³•ï¼Œç§°ä¸ºç´¯ç§¯åŠ æƒå½’å› ï¼ˆCumulative Weighted Attributionï¼‰ï¼Œæ¥è¡¡é‡æ¨¡å‹å†…éƒ¨ç»„ä»¶å¯¹é•¿åº¦æ§åˆ¶çš„è´¡çŒ®ã€‚è¯¥æ–¹æ³•åŸºäºç›´æ¥Logitå½’å› ï¼Œé€šè¿‡è®¡ç®—æ¯ä¸ªç»„ä»¶å¯¹æœ€ç»ˆè¾“å‡ºLogitå€¼çš„è´¡çŒ®ï¼Œæ¥è¯„ä¼°å…¶åœ¨é•¿åº¦æ§åˆ¶ä¸­çš„ä½œç”¨ã€‚ç ”ç©¶åˆ†åˆ«åœ¨è‹±è¯­å’Œæ„å¤§åˆ©è¯­ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¯¹æ¯”äº†åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒä¼˜åçš„æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨ç´¯ç§¯åŠ æƒå½’å› æ–¹æ³•ï¼Œæ·±å…¥åˆ†æäº†æŒ‡ä»¤è°ƒä¼˜å¦‚ä½•æ”¹å˜LLMsçš„å†…éƒ¨è¿ä½œæœºåˆ¶ï¼Œä»è€Œèµ‹äºˆæ¨¡å‹é•¿åº¦æ§åˆ¶èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å‘ç°ä¸åŒè¯­è¨€å¯èƒ½é‡‡ç”¨ä¸åŒçš„è¡¥å¿æœºåˆ¶æ¥å®ç°é•¿åº¦æ§åˆ¶ï¼Œä¾‹å¦‚è‹±è¯­æ›´ä¾èµ–æ³¨æ„åŠ›å¤´ï¼Œè€Œæ„å¤§åˆ©è¯­æ›´ä¾èµ–MLPå±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†Transformeræ¶æ„çš„LLMsï¼Œå¹¶å¯¹æ¯”äº†åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒä¼˜åçš„æ¨¡å‹ã€‚ç´¯ç§¯åŠ æƒå½’å› æ–¹æ³•é€šè¿‡è®¡ç®—æ¯ä¸ªæ³¨æ„åŠ›å¤´å’ŒMLPå±‚å¯¹æœ€ç»ˆè¾“å‡ºLogitå€¼çš„è´¡çŒ®ï¼Œæ¥è¯„ä¼°å…¶åœ¨é•¿åº¦æ§åˆ¶ä¸­çš„ä½œç”¨ã€‚è®ºæ–‡è¿˜åˆ†æäº†ä¸åŒå±‚ä¹‹é—´çš„è´¡çŒ®å·®å¼‚ï¼Œä»¥åŠä¸åŒè¯­è¨€ä¹‹é—´çš„å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶å‘ç°ï¼ŒæŒ‡ä»¤è°ƒä¼˜æ˜¾è‘—æå‡äº†LLMsçš„é•¿åº¦æ§åˆ¶èƒ½åŠ›ã€‚åœ¨è‹±è¯­ä¸­ï¼ŒæŒ‡ä»¤è°ƒä¼˜åçš„æ¨¡å‹åæœŸå±‚çš„æ³¨æ„åŠ›å¤´æ˜¾ç¤ºå‡ºæ›´ç§¯æçš„è´¡çŒ®ã€‚åœ¨æ„å¤§åˆ©è¯­ä¸­ï¼Œè™½ç„¶æ³¨æ„åŠ›è´¡çŒ®å‡å¼±ï¼Œä½†æœ€åä¸€å±‚çš„MLPè¡¨ç°å‡ºæ›´å¼ºçš„ç§¯æä½œç”¨ï¼Œè¡¨æ˜å­˜åœ¨è¡¥å¿æœºåˆ¶ã€‚è¿™äº›ç»“æœè¡¨æ˜æŒ‡ä»¤è°ƒä¼˜é‡æ–°é…ç½®äº†åæœŸå±‚ä»¥é€‚åº”ä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦ç²¾ç¡®é•¿åº¦æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªåŠ¨æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ã€å†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡ç†è§£æŒ‡ä»¤è°ƒä¼˜å¦‚ä½•å½±å“æ¨¡å‹çš„å†…éƒ¨è¿ä½œæœºåˆ¶ï¼Œå¯ä»¥æ›´å¥½åœ°è®¾è®¡å’Œä¼˜åŒ–LLMsï¼Œä½¿å…¶æ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ï¼Œæé«˜æ–‡æœ¬ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•åˆ©ç”¨è¿™äº›å‘ç°æ¥è®¾è®¡æ›´æœ‰æ•ˆçš„é•¿åº¦æ§åˆ¶æ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Adhering to explicit length constraints, such as generating text with a precise word count, remains a significant challenge for Large Language Models (LLMs). This study aims at investigating the differences between foundation models and their instruction-tuned counterparts, on length-controlled text generation in English and Italian. We analyze both performance and internal component contributions using Cumulative Weighted Attribution, a metric derived from Direct Logit Attribution. Our findings reveal that instruction-tuning substantially improves length control, primarily by specializing components in deeper model layers. Specifically, attention heads in later layers of IT models show increasingly positive contributions, particularly in English. In Italian, while attention contributions are more attenuated, final-layer MLPs exhibit a stronger positive role, suggesting a compensatory mechanism. These results indicate that instruction-tuning reconfigures later layers for task adherence, with component-level strategies potentially adapting to linguistic context.

