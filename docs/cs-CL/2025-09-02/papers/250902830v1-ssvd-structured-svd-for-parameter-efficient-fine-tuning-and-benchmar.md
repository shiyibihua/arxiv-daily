---
layout: default
title: SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR
---

# SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02830" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02830v1</a>
  <a href="https://arxiv.org/pdf/2509.02830.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02830v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02830v1', 'SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pu Wang, Shinji Watanabe, Hugo Van hamme

**åˆ†ç±»**: cs.CL, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

**å¤‡æ³¨**: Accepted by IEEE ASRU 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“æ„åŒ–SVDå¼•å¯¼çš„å¾®è°ƒæ–¹æ³•SSVDï¼Œæå‡ASRé¢†åŸŸè¿ç§»æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³è¯†åˆ«` `é¢†åŸŸè‡ªé€‚åº”` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¥‡å¼‚å€¼åˆ†è§£` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•åœ¨è¯­éŸ³é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨é¢†åŸŸè¿ç§»åœºæ™¯ä¸‹çš„è¡¨ç°éªŒè¯ä¸è¶³ã€‚
2. æå‡ºSSVDæ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–çš„SVDåˆ†è§£ï¼Œé€‰æ‹©æ€§æ—‹è½¬è¾“å…¥ç›¸å…³çš„å¥‡å¼‚å‘é‡ï¼Œä¿æŒè¾“å‡ºå‘é‡ä¸å˜ã€‚
3. åœ¨é¢†åŸŸè½¬ç§»çš„è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šï¼ŒSSVDæ–¹æ³•åœ¨å¤šç§æ¨¡å‹è§„æ¨¡ä¸‹å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)å·²æˆä¸ºé€‚åº”å¤§å‹åŸºç¡€æ¨¡å‹çš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶ä½ç§©é€‚åº”(LoRA)åœ¨è¯­éŸ³åº”ç”¨ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†å…¶æœ€å…ˆè¿›çš„å˜ä½“ï¼Œå¦‚VeRAã€DoRAã€PiSSAå’ŒSVFTï¼Œä¸»è¦ä¸ºè¯­è¨€å’Œè§†è§‰ä»»åŠ¡å¼€å‘ï¼Œåœ¨è¯­éŸ³æ–¹é¢çš„éªŒè¯æœ‰é™ã€‚æœ¬æ–‡é¦–æ¬¡åœ¨ESPnetä¸­å…¨é¢é›†æˆå’ŒåŸºå‡†æµ‹è¯•äº†è¿™äº›PEFTæ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ç»“æ„åŒ–SVDå¼•å¯¼(SSVD)å¾®è°ƒï¼Œå®ƒé€‰æ‹©æ€§åœ°æ—‹è½¬è¾“å…¥ç›¸å…³çš„å³å¥‡å¼‚å‘é‡ï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºç›¸å…³çš„å‘é‡å›ºå®šï¼Œä»¥ä¿ç•™è¯­ä¹‰æ˜ å°„ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿä»¥æœ€å°‘çš„è®­ç»ƒå‚æ•°å’Œæ›´é«˜çš„æ•ˆç‡å®ç°é²æ£’çš„é¢†åŸŸè‡ªé€‚åº”ã€‚æˆ‘ä»¬åœ¨é¢†åŸŸè½¬ç§»çš„è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šè¯„ä¼°äº†æ‰€æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬å„¿ç«¥è¯­éŸ³å’Œæ–¹è¨€å˜å¼‚ï¼Œæ¨¡å‹è§„æ¨¡ä»0.1Båˆ°2Bã€‚æ‰€æœ‰å®ç°éƒ½åœ¨ESPnetä¸­å‘å¸ƒï¼Œä»¥æ”¯æŒå¯é‡å¤æ€§å’Œæœªæ¥çš„å·¥ä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­ï¼Œå½“è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®å­˜åœ¨é¢†åŸŸå·®å¼‚ï¼ˆdomain shiftï¼‰æ—¶ï¼Œå¦‚ä½•é«˜æ•ˆåœ°å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶é€‚åº”æ–°çš„é¢†åŸŸã€‚ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œå¦‚LoRAåŠå…¶å˜ä½“ï¼Œä¸»è¦é’ˆå¯¹è¯­è¨€å’Œè§†è§‰ä»»åŠ¡è®¾è®¡ï¼Œåœ¨è¯­éŸ³é¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯åœ¨é¢†åŸŸè¿ç§»åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œç¼ºä¹å……åˆ†çš„éªŒè¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç»“æ„åŒ–çš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼Œæœ‰é€‰æ‹©æ€§åœ°è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä»¥é€‚åº”æ–°çš„é¢†åŸŸã€‚å…·ä½“æ¥è¯´ï¼ŒSSVDæ–¹æ³•æ—¨åœ¨ä¿ç•™æ¨¡å‹ä¸­ä¸è¾“å‡ºç›¸å…³çš„è¯­ä¹‰æ˜ å°„ï¼ŒåŒæ—¶è°ƒæ•´ä¸è¾“å…¥ç›¸å…³çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é¢†åŸŸè‡ªé€‚åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSSVDæ–¹æ³•åœ¨ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼ˆå¦‚LoRAï¼‰çš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚å…¶æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œå¯¹æ¨¡å‹çš„æƒé‡çŸ©é˜µè¿›è¡ŒSVDåˆ†è§£ï¼›ç„¶åï¼Œé€‰æ‹©æ€§åœ°æ—‹è½¬ä¸è¾“å…¥ç›¸å…³çš„å³å¥‡å¼‚å‘é‡ï¼Œè€Œä¿æŒä¸è¾“å‡ºç›¸å…³çš„å·¦å¥‡å¼‚å‘é‡å›ºå®šï¼›æœ€åï¼Œä½¿ç”¨é¢†åŸŸç›¸å…³çš„æ•°æ®å¯¹è°ƒæ•´åçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¯¥æ–¹æ³•å¯ä»¥é›†æˆåˆ°ESPnetç­‰è¯­éŸ³è¯†åˆ«å·¥å…·åŒ…ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šSSVDæ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»“æ„åŒ–çš„SVDåˆ†è§£å’Œé€‰æ‹©æ€§æ—‹è½¬ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„ä½ç§©åˆ†è§£æ–¹æ³•ä¸åŒï¼ŒSSVDæ–¹æ³•ä¸æ˜¯ç®€å•åœ°å­¦ä¹ ä½ç§©çŸ©é˜µï¼Œè€Œæ˜¯é€šè¿‡SVDåˆ†è§£å°†æƒé‡çŸ©é˜µåˆ†è§£ä¸ºè¾“å…¥å’Œè¾“å‡ºç›¸å…³çš„éƒ¨åˆ†ï¼Œå¹¶åªè°ƒæ•´è¾“å…¥ç›¸å…³çš„éƒ¨åˆ†ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¿ç•™æ¨¡å‹çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶æé«˜é¢†åŸŸè‡ªé€‚åº”çš„æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šSSVDæ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨SVDåˆ†è§£å°†æƒé‡çŸ©é˜µåˆ†è§£ä¸ºè¾“å…¥å’Œè¾“å‡ºç›¸å…³çš„éƒ¨åˆ†ï¼›2) è®¾è®¡äº†ä¸€ç§é€‰æ‹©æ€§æ—‹è½¬ç­–ç•¥ï¼Œåªè°ƒæ•´ä¸è¾“å…¥ç›¸å…³çš„å³å¥‡å¼‚å‘é‡ï¼Œè€Œä¿æŒä¸è¾“å‡ºç›¸å…³çš„å·¦å¥‡å¼‚å‘é‡å›ºå®šï¼›3) å°†SSVDæ–¹æ³•é›†æˆåˆ°ESPnetå·¥å…·åŒ…ä¸­ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜ä½¿ç”¨å’Œæ‰©å±•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSSVDæ–¹æ³•åœ¨é¢†åŸŸè½¬ç§»çš„è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å„¿ç«¥è¯­éŸ³è¯†åˆ«å’Œæ–¹è¨€å˜å¼‚ç­‰ä»»åŠ¡ä¸Šï¼ŒSSVDæ–¹æ³•åœ¨å¤šç§æ¨¡å‹è§„æ¨¡ä¸‹å‡ä¼˜äºç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œå¦‚LoRAã€VeRAç­‰ã€‚å®éªŒç»“æœè¿˜è¡¨æ˜ï¼ŒSSVDæ–¹æ³•èƒ½å¤Ÿä»¥æ›´å°‘çš„è®­ç»ƒå‚æ•°å®ç°æ›´å¥½çš„é¢†åŸŸè‡ªé€‚åº”æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è¯­éŸ³è¯†åˆ«åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®åˆ†å¸ƒå­˜åœ¨å·®å¼‚çš„æƒ…å†µä¸‹ï¼Œä¾‹å¦‚å£éŸ³è¿ç§»ã€å„¿ç«¥è¯­éŸ³è¯†åˆ«ç­‰ã€‚é€šè¿‡SSVDæ–¹æ³•ï¼Œå¯ä»¥é«˜æ•ˆåœ°å°†é¢„è®­ç»ƒçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹è¿ç§»åˆ°æ–°çš„é¢†åŸŸï¼Œé™ä½æ¨¡å‹è®­ç»ƒæˆæœ¬ï¼Œæé«˜è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è¿˜æœ‰æ½œåŠ›åº”ç”¨äºå…¶ä»–è¯­éŸ³å¤„ç†ä»»åŠ¡ï¼Œå¦‚è¯­éŸ³åˆæˆã€è¯­éŸ³å¢å¼ºç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for adapting large foundation models. While low-rank adaptation (LoRA) is widely used in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA, PiSSA, and SVFT, are developed mainly for language and vision tasks, with limited validation in speech. This work presents the first comprehensive integration and benchmarking of these PEFT methods within ESPnet. We further introduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates input-associated right singular vectors while keeping output-associated vectors fixed to preserve semantic mappings. This design enables robust domain adaptation with minimal trainable parameters and improved efficiency. We evaluate all methods on domain-shifted speech recognition tasks, including child speech and dialectal variation, across model scales from 0.1B to 2B. All implementations are released in ESPnet to support reproducibility and future work.

