---
layout: default
title: LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue
---

# LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02292" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02292v1</a>
  <a href="https://arxiv.org/pdf/2509.02292.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02292v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02292v1', 'LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Katharine Kowalyshyn, Matthias Scheutz

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLLMçš„ä¸¤æ­¥æ¡†æ¶ï¼Œè¯„ä¼°å›¢é˜Ÿå¯¹è¯ä¸­å…±äº«å¿ƒæ™ºæ¨¡å‹çš„åå·®ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å…±äº«å¿ƒæ™ºæ¨¡å‹` `å›¢é˜Ÿå¯¹è¯` `åå·®æ£€æµ‹` `è‡ªç„¶è¯­è¨€ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè¯„ä¼°å›¢é˜Ÿå¯¹è¯ä¸­å…±äº«å¿ƒæ™ºæ¨¡å‹çš„åå·®ï¼Œé˜»ç¢äº†å›¢é˜Ÿåä½œæ•ˆç‡çš„æå‡ã€‚
2. åˆ©ç”¨LLMä½œä¸ºæ ‡æ³¨è€…å’Œåå·®æ£€æµ‹å™¨ï¼Œæ„å»ºä¸¤æ­¥æ¡†æ¶ï¼Œè‡ªåŠ¨è¯†åˆ«å’Œè¯„ä¼°å›¢é˜Ÿæˆå‘˜é—´å¿ƒç†çŠ¶æ€çš„å·®å¼‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMåœ¨è‡ªç„¶è¯­è¨€æ³¨é‡Šä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œæ­§ä¹‰æ¶ˆé™¤æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§è¯¯å·®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤æ­¥æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºäººç±»é£æ ¼çš„æ ‡æ³¨è€…ï¼Œåˆ†æå›¢é˜Ÿå¯¹è¯ä»¥è¿½è¸ªå›¢é˜Ÿçš„å…±äº«å¿ƒæ™ºæ¨¡å‹ï¼ˆSMMï¼‰ï¼Œå¹¶ä½œä¸ºè‡ªåŠ¨åå·®æ£€æµ‹å™¨ï¼Œè¯†åˆ«ä¸ªä½“å¿ƒç†çŠ¶æ€ä¹‹é—´çš„å·®å¼‚ã€‚ç¬¬ä¸€æ­¥ï¼ŒLLMé€šè¿‡è¯†åˆ«æ¥è‡ªåˆä½œè¿œç¨‹æœç´¢ä»»åŠ¡ï¼ˆCReSTï¼‰è¯­æ–™åº“ä¸­é¢å‘ä»»åŠ¡çš„å¯¹è¯ä¸­çš„SMMå…ƒç´ æ¥ç”Ÿæˆæ³¨é‡Šã€‚ç„¶åï¼Œç¬¬äºŒä¸ªLLMå°†è¿™äº›LLMè¡ç”Ÿçš„æ³¨é‡Šå’Œäººå·¥æ³¨é‡Šä¸é‡‘æ ‡å‡†æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ£€æµ‹å’Œè¡¨å¾å·®å¼‚ã€‚æœ¬æ–‡ä¸ºè¯¥ç”¨ä¾‹å®šä¹‰äº†ä¸€ä¸ªSMMä¸€è‡´æ€§è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å°†å…¶åº”ç”¨äºå…­ä¸ªCReSTå¯¹è¯ï¼Œæœ€ç»ˆç”Ÿæˆï¼šï¼ˆ1ï¼‰äººç±»å’ŒLLMæ³¨é‡Šçš„æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰SMMä¸€è‡´æ€§çš„å¯é‡å¤è¯„ä¼°æ¡†æ¶ï¼›ï¼ˆ3ï¼‰åŸºäºLLMçš„å·®å¼‚æ£€æµ‹çš„å®è¯è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMåœ¨ç®€å•çš„è‡ªç„¶è¯­è¨€æ³¨é‡Šä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„ä¸€è‡´æ€§ï¼Œä½†åœ¨éœ€è¦ç©ºé—´æ¨ç†æˆ–æ¶ˆé™¤éŸµå¾‹çº¿ç´¢æ­§ä¹‰çš„åœºæ™¯ä¸­ï¼Œå®ƒä»¬ä¼šç³»ç»Ÿæ€§åœ°å‡ºé”™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆè¯„ä¼°å›¢é˜Ÿå¯¹è¯ä¸­å…±äº«å¿ƒæ™ºæ¨¡å‹ï¼ˆSMMï¼‰çš„ä¸€è‡´æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥å¤§è§„æ¨¡åº”ç”¨ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†éœ€è¦å¤æ‚æ¨ç†ï¼ˆå¦‚ç©ºé—´æ¨ç†å’ŒéŸµå¾‹æ¶ˆæ­§ï¼‰çš„å¯¹è¯æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œæ¨¡æ‹Ÿäººç±»æ ‡æ³¨è€…çš„è¡Œä¸ºï¼Œè‡ªåŠ¨è¯†åˆ«å’Œè¯„ä¼°å›¢é˜Ÿå¯¹è¯ä¸­çš„SMMå…ƒç´ ã€‚é€šè¿‡æ¯”è¾ƒLLMå’Œäººç±»æ ‡æ³¨çš„ç»“æœï¼Œå¯ä»¥å‘ç°LLMåœ¨ç†è§£SMMæ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæ”¹è¿›LLMçš„æ¨ç†èƒ½åŠ›æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š1) LLMæ ‡æ³¨ï¼šä½¿ç”¨ä¸€ä¸ªLLMå¯¹CReSTè¯­æ–™åº“ä¸­çš„å›¢é˜Ÿå¯¹è¯è¿›è¡Œæ ‡æ³¨ï¼Œè¯†åˆ«å¯¹è¯ä¸­ä¸SMMç›¸å…³çš„å…ƒç´ ã€‚2) åå·®æ£€æµ‹ï¼šä½¿ç”¨å¦ä¸€ä¸ªLLMæ¯”è¾ƒLLMç”Ÿæˆçš„æ ‡æ³¨å’Œäººå·¥æ ‡æ³¨ï¼Œä¸é‡‘æ ‡å‡†æ ‡ç­¾è¿›è¡Œå¯¹æ¯”ï¼Œä»¥æ£€æµ‹å’Œè¡¨å¾å·®å¼‚ã€‚è¯¥æ¡†æ¶è¿˜å®šä¹‰äº†ä¸€ä¸ªSMMä¸€è‡´æ€§è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMæ ‡æ³¨çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åŸºäºLLMçš„ä¸¤æ­¥æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨è¯„ä¼°å›¢é˜Ÿå¯¹è¯ä¸­SMMçš„ä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿçš„äººå·¥æ ‡æ³¨æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å…·æœ‰æˆæœ¬ä½ã€æ•ˆç‡é«˜çš„ä¼˜ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥ç”¨äºè¯†åˆ«LLMåœ¨ç†è§£SMMæ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæ”¹è¿›LLMçš„æ¨ç†èƒ½åŠ›æä¾›æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†CReSTè¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“åŒ…å«é¢å‘ä»»åŠ¡çš„å›¢é˜Ÿå¯¹è¯ã€‚åœ¨LLMæ ‡æ³¨é˜¶æ®µï¼Œä½¿ç”¨äº†æç¤ºå·¥ç¨‹ï¼ˆprompt engineeringï¼‰æŠ€æœ¯ï¼Œå¼•å¯¼LLMè¯†åˆ«å¯¹è¯ä¸­ä¸SMMç›¸å…³çš„å…ƒç´ ã€‚åœ¨åå·®æ£€æµ‹é˜¶æ®µï¼Œä½¿ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®ç‡ã€å¬å›ç‡å’ŒF1å€¼ï¼Œè¯„ä¼°LLMæ ‡æ³¨çš„è´¨é‡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°ã€ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMåœ¨ç®€å•çš„è‡ªç„¶è¯­è¨€æ³¨é‡Šä»»åŠ¡ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„ä¸€è‡´æ€§ï¼Œä½†åœ¨éœ€è¦ç©ºé—´æ¨ç†æˆ–æ¶ˆé™¤éŸµå¾‹çº¿ç´¢æ­§ä¹‰çš„åœºæ™¯ä¸­ï¼ŒLLMä¼šç³»ç»Ÿæ€§åœ°å‡ºé”™ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†LLMåœ¨ç†è§£å¤æ‚å¿ƒç†çŠ¶æ€æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦å¯ç¤ºã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å›¢é˜Ÿåä½œæ•ˆç‡ã€æ”¹è¿›äººæœºåä½œç³»ç»Ÿã€ä»¥åŠå¼€å‘æ›´æ™ºèƒ½çš„å¯¹è¯ä»£ç†ã€‚é€šè¿‡è‡ªåŠ¨è¯„ä¼°å›¢é˜Ÿæˆå‘˜é—´çš„å…±äº«å¿ƒæ™ºæ¨¡å‹ï¼Œå¯ä»¥åŠæ—¶å‘ç°å’Œçº æ­£æ²Ÿé€šåå·®ï¼Œä»è€Œæé«˜å›¢é˜Ÿçš„æ•´ä½“è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ç”¨äºè®­ç»ƒæ›´å¼ºå¤§çš„LLMï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»çš„å¿ƒç†çŠ¶æ€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> What if large language models could not only infer human mindsets but also expose every blind spot in team dialogue such as discrepancies in the team members' joint understanding? We present a novel, two-step framework that leverages large language models (LLMs) both as human-style annotators of team dialogues to track the team's shared mental models (SMMs) and as automated discrepancy detectors among individuals' mental states. In the first step, an LLM generates annotations by identifying SMM elements within task-oriented dialogues from the Cooperative Remote Search Task (CReST) corpus. Then, a secondary LLM compares these LLM-derived annotations and human annotations against gold-standard labels to detect and characterize divergences. We define an SMM coherence evaluation framework for this use case and apply it to six CReST dialogues, ultimately producing: (1) a dataset of human and LLM annotations; (2) a reproducible evaluation framework for SMM coherence; and (3) an empirical assessment of LLM-based discrepancy detection. Our results reveal that, although LLMs exhibit apparent coherence on straightforward natural-language annotation tasks, they systematically err in scenarios requiring spatial reasoning or disambiguation of prosodic cues.

