---
layout: default
title: ProST: Progressive Sub-task Training for Pareto-Optimal Multi-agent Systems Using Small Language Models
---

# ProST: Progressive Sub-task Training for Pareto-Optimal Multi-agent Systems Using Small Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04508" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04508v2</a>
  <a href="https://arxiv.org/pdf/2509.04508.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04508v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04508v2', 'ProST: Progressive Sub-task Training for Pareto-Optimal Multi-agent Systems Using Small Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Biddut Sarker Bijoy, Mohammad Saqib Hasan, Pegah Alipoormolabashi, Avirup Sil, Aruna Balasubramanian, Niranjan Balasubramanian

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02 (æ›´æ–°: 2025-11-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProSTæ¸è¿›å¼å­ä»»åŠ¡è®­ç»ƒæ–¹æ³•ï¼Œæå‡å°å‹è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ•ˆç‡å’Œæ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å°å‹è¯­è¨€æ¨¡å‹` `æ¸è¿›å¼è®­ç»ƒ` `è¯¾ç¨‹å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `å­ä»»åŠ¡åˆ†è§£` `AppWorldç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å°å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­é¢ä¸´é•¿è½¨è¿¹å­¦ä¹ å›°éš¾ï¼Œå¯¼è‡´å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½å—é™ã€‚
2. æå‡ºæ¸è¿›å¼å­ä»»åŠ¡è®­ç»ƒï¼ˆProSTï¼‰ç­–ç•¥ï¼Œç±»ä¼¼äºè¯¾ç¨‹å­¦ä¹ ï¼Œé€æ­¥å¼•å…¥å­ä»»åŠ¡ä»¥æå‡å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒè¡¨æ˜ï¼ŒProSTèƒ½æœ‰æ•ˆæå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½ï¼Œå®ç°æ›´å¥½çš„æœ‰æ•ˆæ€§-æ•ˆç‡æƒè¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å•æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡å¯¹æ¯”ã€‚ç ”ç©¶å‘ç°ï¼ŒSLMåœ¨é•¿è½¨è¿¹å­¦ä¹ æ–¹é¢çš„å›°éš¾é™åˆ¶äº†å…¶æ€§èƒ½ï¼Œå³ä½¿ç»è¿‡ä¸“é—¨çš„è§’è‰²è®­ç»ƒï¼ŒSLMä¹Ÿæ— æ³•æœ‰æ•ˆåœ°å­¦ä¹ æ‰€æœ‰å­ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„æ¸è¿›å¼å­ä»»åŠ¡è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸä¸­é€æ­¥å¼•å…¥æ–°çš„å­ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç±»ä¼¼äºå®ä¾‹çº§åˆ«è¯¾ç¨‹å­¦ä¹ çš„ç­–ç•¥ï¼Œèƒ½å¤ŸæŒç»­æé«˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å„ç§é…ç½®ä¸‹çš„æœ‰æ•ˆæ€§ã€‚å¸•ç´¯æ‰˜åˆ†æè¡¨æ˜ï¼Œå¾®è°ƒåçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¤Ÿå®ç°æ›´å¥½çš„æœ‰æ•ˆæ€§-æ•ˆç‡æƒè¡¡ã€‚é¢å¤–çš„æ¶ˆèå®éªŒå’Œåˆ†æè¡¨æ˜äº†æ¸è¿›å¼è®­ç»ƒç­–ç•¥çš„é‡è¦æ€§åŠå…¶é™ä½å­ä»»åŠ¡é”™è¯¯ç‡çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨å¤æ‚å¤šæ™ºèƒ½ä½“ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç›´æ¥è®­ç»ƒSLMå®Œæˆæ‰€æœ‰å­ä»»åŠ¡ï¼Œä½†ç”±äºSLMçš„å®¹é‡é™åˆ¶å’Œé•¿è½¨è¿¹å­¦ä¹ çš„å›°éš¾ï¼Œå¯¼è‡´å…¶æ— æ³•æœ‰æ•ˆå­¦ä¹ æ‰€æœ‰å­ä»»åŠ¡ï¼Œä»è€Œé™åˆ¶äº†æ•´ä½“æ€§èƒ½ã€‚å°¤å…¶æ˜¯åœ¨AppWorldè¿™ç§éœ€è¦é•¿æœŸè§„åˆ’å’Œå¤šæ­¥éª¤äº¤äº’çš„ç¯å¢ƒä¸­ï¼Œé—®é¢˜æ›´åŠ çªå‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨æ¸è¿›å¼å­ä»»åŠ¡è®­ç»ƒï¼ˆProgressive Sub-task Training, ProSTï¼‰ã€‚ç±»ä¼¼äºè¯¾ç¨‹å­¦ä¹ ï¼ŒProSTä»ç®€å•çš„å­ä»»åŠ¡å¼€å§‹è®­ç»ƒSLMï¼Œç„¶åé€æ­¥å¼•å…¥æ›´å¤æ‚çš„å­ä»»åŠ¡ã€‚è¿™ç§æ–¹å¼æœ‰åŠ©äºSLMé€æ­¥æŒæ¡å„ä¸ªå­ä»»åŠ¡ï¼Œé¿å…ä¸€å¼€å§‹å°±é¢ä¸´è¿‡äºå¤æ‚çš„å­¦ä¹ ç›®æ ‡ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šProSTçš„æ•´ä½“æ¡†æ¶æ˜¯åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ªç¯ä¸­ï¼Œå¯¹æ¯ä¸ªè®­ç»ƒepochçš„è®­ç»ƒæ•°æ®è¿›è¡Œè°ƒæ•´ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯ä¸ªepochå¼€å§‹æ—¶ï¼Œæ ¹æ®é¢„å®šä¹‰çš„å­ä»»åŠ¡éš¾åº¦é¡ºåºï¼Œé€‰æ‹©ä¸€éƒ¨åˆ†å­ä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚éšç€epochçš„è¿›è¡Œï¼Œé€æ­¥å¢åŠ è®­ç»ƒçš„å­ä»»åŠ¡æ•°é‡ï¼Œç›´åˆ°æ‰€æœ‰å­ä»»åŠ¡éƒ½è¢«åŒ…å«åœ¨è®­ç»ƒé›†ä¸­ã€‚è¿™ç§æ¸è¿›å¼çš„è®­ç»ƒæ–¹å¼ä½¿å¾—SLMèƒ½å¤Ÿé€æ­¥é€‚åº”å¤æ‚ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šProSTçš„å…³é”®åˆ›æ–°åœ¨äºå°†è¯¾ç¨‹å­¦ä¹ çš„æ€æƒ³åº”ç”¨åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å­ä»»åŠ¡è®­ç»ƒä¸­ã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥è®­ç»ƒæ‰€æœ‰å­ä»»åŠ¡çš„æ–¹æ³•ç›¸æ¯”ï¼ŒProSTèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨SLMçš„æœ‰é™å®¹é‡ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å„ä¸ªå­ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒProSTçš„å®ç°æ–¹å¼ç®€å•ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰çš„å¤šæ™ºèƒ½ä½“è®­ç»ƒæ¡†æ¶ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šProSTçš„å…³é”®è®¾è®¡åŒ…æ‹¬å­ä»»åŠ¡çš„åˆ’åˆ†å’Œéš¾åº¦æ’åºã€‚è®ºæ–‡ä¸­ï¼Œå­ä»»åŠ¡çš„åˆ’åˆ†åŸºäºAppWorldç¯å¢ƒä¸­çš„ä¸åŒæ“ä½œå’Œç›®æ ‡ã€‚éš¾åº¦æ’åºå¯ä»¥æ ¹æ®ç»éªŒæˆ–é€šè¿‡å®éªŒç¡®å®šã€‚æ­¤å¤–ï¼ŒProSTè¿˜éœ€è¦ç¡®å®šæ¯ä¸ªepochä¸­å¼•å…¥æ–°å­ä»»åŠ¡çš„ç­–ç•¥ï¼Œä¾‹å¦‚çº¿æ€§å¢åŠ æˆ–æŒ‡æ•°å¢åŠ ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ä¸åº•å±‚ä½¿ç”¨çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸å…³ï¼ŒProSTæœ¬èº«å¹¶ä¸å¼•å…¥æ–°çš„æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒProSTèƒ½å¤Ÿæ˜¾è‘—æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ€§èƒ½ã€‚åœ¨AppWorldç¯å¢ƒä¸­ï¼Œä½¿ç”¨ProSTè®­ç»ƒçš„SLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å„ç§é…ç½®ä¸‹éƒ½å–å¾—äº†æ›´å¥½çš„æ•ˆæœï¼Œå¹¶ä¸”åœ¨æœ‰æ•ˆæ€§-æ•ˆç‡æƒè¡¡æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ¸è¿›å¼è®­ç»ƒç­–ç•¥å¯¹äºé™ä½å­ä»»åŠ¡é”™è¯¯ç‡è‡³å…³é‡è¦ã€‚å¸•ç´¯æ‰˜åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†ProSTåœ¨æå‡ç³»ç»Ÿæ•´ä½“æ€§èƒ½æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤šæ™ºèƒ½ä½“åä½œå®Œæˆå¤æ‚ä»»åŠ¡çš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººåä½œã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½äº¤é€šç®¡ç†ã€ä»¥åŠåˆ†å¸ƒå¼è®¡ç®—ç­‰ã€‚é€šè¿‡ä½¿ç”¨å°å‹è¯­è¨€æ¨¡å‹å’Œæ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé™ä½éƒ¨ç½²æˆæœ¬å’Œè®¡ç®—éœ€æ±‚ï¼Œå¹¶ä¸ºè¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸Šçš„å¤æ‚ä»»åŠ¡æä¾›è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-agent systems with smaller language models (SLMs) present a viable alternative to single agent systems powered by large language models (LLMs) for addressing complex problems. In this work, we study how these alternatives compare in terms of both effectiveness and efficiency. To study this trade-off, we instantiate single and multi-agent systems for the complex problems in the AppWorld environment using different sized language models.
>   We find that difficulties with long-trajectory learning in smaller language models (SLMs) limit their performance. Even when trained for specialized roles, SLMs fail to learn all subtasks effectively. To address this issue, we introduce a simple progressive sub-task training strategy, which introduces new sub-tasks progressively in each training epoch. We find that this novel strategy, analogous to instance level curriculum learning, consistently improves the effectiveness of multi-agents at all configurations. Our Pareto analysis shows that fine-tuned multi-agent systems yield better effectiveness-efficiency trade-offs. Additional ablations and analyses shows the importance of our progressive training strategy and its ability to reduce subtask error rates.

