---
layout: default
title: Avoidance Decoding for Diverse Multi-Branch Story Generation
---

# Avoidance Decoding for Diverse Multi-Branch Story Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02170" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02170v2</a>
  <a href="https://arxiv.org/pdf/2509.02170.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02170v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02170v2', 'Avoidance Decoding for Diverse Multi-Branch Story Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kyeongman Park, Nakyeong Yang, Kyomin Jung

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02 (æ›´æ–°: 2025-09-03)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAvoidance Decodingï¼Œè§£å†³LLMæ•…äº‹ç”Ÿæˆä¸­å¤šæ ·æ€§ä¸è¶³å’Œé‡å¤æ€§é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•…äº‹ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§£ç ç­–ç•¥` `å¤šæ ·æ€§` `é¿å…é‡å¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•…äº‹ç”Ÿæˆä¸­å­˜åœ¨é‡å¤æ€§é«˜ã€å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åˆ›é€ æ½œåŠ›ã€‚
2. Avoidance Decodingé€šè¿‡æƒ©ç½šä¸å†å²è¾“å‡ºçš„ç›¸ä¼¼æ€§ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ›´å¤šæ ·åŒ–çš„æ•…äº‹åˆ†æ”¯ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ•…äº‹ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œé™ä½äº†é‡å¤ç‡ï¼Œå¹¶æœ‰æ•ˆé¿å…äº†æ–‡æœ¬é€€åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ•…äº‹ç”Ÿæˆç­‰ä»»åŠ¡ä¸­ï¼Œç”±äºè¾“å…¥æç¤ºç›¸åŒï¼Œå¾€å¾€äº§ç”Ÿé‡å¤å’Œå•è°ƒçš„è¾“å‡ºï¼Œç¼ºä¹åˆ›é€ æ€§å¤šæ ·æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§£ç ç­–ç•¥ï¼Œå³Avoidance Decodingï¼Œé€šè¿‡æƒ©ç½šä¸å…ˆå‰ç”Ÿæˆè¾“å‡ºçš„ç›¸ä¼¼æ€§æ¥ä¿®æ”¹token logitsï¼Œä»è€Œé¼“åŠ±æ›´å¤šæ ·åŒ–çš„å¤šåˆ†æ”¯æ•…äº‹ã€‚è¿™ç§æƒ©ç½šè‡ªé€‚åº”åœ°å¹³è¡¡äº†ä¸¤ç§ç›¸ä¼¼æ€§åº¦é‡ï¼š(1)æ¦‚å¿µçº§ç›¸ä¼¼æ€§æƒ©ç½šï¼Œä¼˜å…ˆåœ¨æ—©æœŸé˜¶æ®µä½¿ç”¨ï¼Œä»¥å®ç°åˆå§‹æ•…äº‹æ¦‚å¿µçš„å¤šæ ·åŒ–ï¼›(2)å™äº‹çº§ç›¸ä¼¼æ€§æƒ©ç½šï¼Œåœ¨åæœŸé˜¶æ®µé€æ¸å¼ºè°ƒï¼Œä»¥ç¡®ä¿è‡ªç„¶è€Œå¤šæ ·çš„æƒ…èŠ‚å‘å±•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸å¼ºå¤§çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†é«˜è¾¾2.6å€çš„è¾“å‡ºå¤šæ ·æ€§ï¼Œå¹¶å°†é‡å¤æ€§å¹³å‡é™ä½äº†30%ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ç¼“è§£äº†æ–‡æœ¬é€€åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ–¹æ³•æ¿€æ´»äº†æ›´å¹¿æ³›çš„ç¥ç»å…ƒï¼Œè¡¨æ˜å®ƒåˆ©ç”¨äº†æ¨¡å‹å›ºæœ‰çš„åˆ›é€ åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿›è¡Œæ•…äº‹ç”Ÿæˆæ—¶ï¼Œå®¹æ˜“äº§ç”Ÿé‡å¤å’Œå•è°ƒçš„å†…å®¹ï¼Œç¼ºä¹æ–°é¢–æ€§å’Œå¤šæ ·æ€§ã€‚è¿™æ˜¯å› ä¸ºåœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å€¾å‘äºé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenåºåˆ—ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ•…äº‹ç¼ºä¹åˆ›é€ æ€§ã€‚ç°æœ‰çš„æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°å¹³è¡¡æ•…äº‹çš„è¿è´¯æ€§å’Œå¤šæ ·æ€§ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAvoidance Decodingçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œå¯¹ä¸å·²ç”Ÿæˆå†…å®¹ç›¸ä¼¼çš„tokenè¿›è¡Œæƒ©ç½šï¼Œä»è€Œé¼“åŠ±æ¨¡å‹æ¢ç´¢ä¸åŒçš„æ•…äº‹èµ°å‘ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´æƒ©ç½šåŠ›åº¦ï¼Œåœ¨æ•…äº‹ç”Ÿæˆçš„ä¸åŒé˜¶æ®µä¾§é‡ä¸åŒçš„ç›¸ä¼¼æ€§åº¦é‡ï¼Œä»¥å®ç°æ›´å¥½çš„å¤šæ ·æ€§å’Œè¿è´¯æ€§å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨LLMè¿›è¡Œæ•…äº‹ç”Ÿæˆï¼›2) è®¡ç®—å½“å‰tokenä¸å·²ç”Ÿæˆæ•…äº‹çš„æ¦‚å¿µçº§ç›¸ä¼¼åº¦å’Œå™äº‹çº§ç›¸ä¼¼åº¦ï¼›3) æ ¹æ®ç›¸ä¼¼åº¦è®¡ç®—æƒ©ç½šé¡¹ï¼Œå¹¶è°ƒæ•´tokençš„logitsï¼›4) æ ¹æ®è°ƒæ•´åçš„logitsé€‰æ‹©ä¸‹ä¸€ä¸ªtokenï¼Œé‡å¤ä¸Šè¿°è¿‡ç¨‹ç›´åˆ°æ•…äº‹ç”Ÿæˆå®Œæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªé€‚åº”çš„ç›¸ä¼¼æ€§æƒ©ç½šæœºåˆ¶ã€‚å®ƒåŒºåˆ†äº†æ¦‚å¿µçº§ç›¸ä¼¼æ€§å’Œå™äº‹çº§ç›¸ä¼¼æ€§ï¼Œå¹¶åœ¨æ•…äº‹ç”Ÿæˆçš„ä¸åŒé˜¶æ®µåŠ¨æ€è°ƒæ•´ä¸¤ç§ç›¸ä¼¼æ€§åº¦é‡çš„æƒé‡ã€‚åœ¨æ•…äº‹ç”Ÿæˆçš„æ—©æœŸé˜¶æ®µï¼Œä¾§é‡äºæ¦‚å¿µçº§ç›¸ä¼¼æ€§æƒ©ç½šï¼Œä»¥é¼“åŠ±æ¨¡å‹æ¢ç´¢ä¸åŒçš„æ•…äº‹ä¸»é¢˜ï¼›åœ¨æ•…äº‹ç”Ÿæˆçš„åæœŸé˜¶æ®µï¼Œä¾§é‡äºå™äº‹çº§ç›¸ä¼¼æ€§æƒ©ç½šï¼Œä»¥ä¿è¯æ•…äº‹çš„è¿è´¯æ€§å’Œæµç•…æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¦‚å¿µçº§ç›¸ä¼¼æ€§é€šè¿‡è®¡ç®—å½“å‰tokençš„embeddingä¸å·²ç”Ÿæˆæ•…äº‹çš„embeddingçš„ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¡¡é‡ã€‚å™äº‹çº§ç›¸ä¼¼æ€§åˆ™é€šè¿‡è®¡ç®—å½“å‰tokenä¸å·²ç”Ÿæˆæ•…äº‹çš„n-gram overlapæ¥è¡¡é‡ã€‚æƒ©ç½šé¡¹çš„è®¡ç®—å…¬å¼ä¸ºï¼špenalty = Î»_concept * concept_similarity + Î»_narrative * narrative_similarityï¼Œå…¶ä¸­Î»_conceptå’ŒÎ»_narrativeæ˜¯åŠ¨æ€è°ƒæ•´çš„æƒé‡å‚æ•°ï¼Œç”¨äºæ§åˆ¶ä¸¤ç§ç›¸ä¼¼æ€§æƒ©ç½šçš„åŠ›åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAvoidance Decodingåœ¨æ•…äº‹ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œæœ€é«˜å¯è¾¾2.6å€ï¼ŒåŒæ—¶å°†é‡å¤ç‡å¹³å‡é™ä½äº†30%ã€‚ä¸ç°æœ‰çš„å¼ºåŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ ·æ€§å’Œè¿è´¯æ€§ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œå¹¶æœ‰æ•ˆç¼“è§£äº†æ–‡æœ¬é€€åŒ–é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°è¯¥æ–¹æ³•èƒ½å¤Ÿæ¿€æ´»æ¨¡å‹ä¸­æ›´å¹¿æ³›çš„ç¥ç»å…ƒï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æ¨¡å‹çš„å†…åœ¨åˆ›é€ åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ç”Ÿæˆå¤šæ ·åŒ–æ–‡æœ¬å†…å®¹çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•…äº‹åˆ›ä½œã€å‰§æœ¬ç”Ÿæˆã€æ¸¸æˆå‰§æƒ…è®¾è®¡ç­‰ã€‚é€šè¿‡æå‡ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§å’Œåˆ›é€ æ€§ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´ä¸°å¯Œçš„ä½“éªŒï¼Œå¹¶æ¿€å‘æ›´å¤šçš„åˆ›ä½œçµæ„Ÿã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå¯¹è¯ç”Ÿæˆã€æ–‡æœ¬æ‘˜è¦ç­‰ä»»åŠ¡ï¼Œä»¥æé«˜ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often generate repetitive and monotonous outputs, especially in tasks like story generation, due to limited creative diversity when given the same input prompt. To address this challenge, we propose a novel decoding strategy, Avoidance Decoding, that modifies token logits by penalizing similarity to previously generated outputs, thereby encouraging more diverse multi-branch stories. This penalty adaptively balances two similarity measures: (1) Concept-level Similarity Penalty, which is prioritized in early stages to diversify initial story concepts, and (2) Narrative-level Similarity Penalty, which is increasingly emphasized later to ensure natural yet diverse plot development. Notably, our method achieves up to 2.6 times higher output diversity and reduces repetition by an average of 30% compared to strong baselines, while effectively mitigating text degeneration. Furthermore, we reveal that our method activates a broader range of neurons, demonstrating that it leverages the model's intrinsic creativity.

