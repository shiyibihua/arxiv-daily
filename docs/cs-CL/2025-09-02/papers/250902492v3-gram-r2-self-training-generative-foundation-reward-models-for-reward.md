---
layout: default
title: GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning
---

# GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02492" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02492v3</a>
  <a href="https://arxiv.org/pdf/2509.02492.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02492v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02492v3', 'GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenglong Wang, Yongyu Mu, Hang Zhou, Yifu Huo, Ziming Zhu, Jiali Zeng, Murun Yang, Bei Li, Xiaoyang Hao, Chunliang Zhang, Fandong Meng, Jingbo Zhu, Tong Xiao

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02 (æ›´æ–°: 2025-11-16)

**å¤‡æ³¨**: Accepted by AAAI 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRAM-R$^2$ï¼Œé€šè¿‡è‡ªè®­ç»ƒç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹å®ç°å¥–åŠ±æ¨ç†ï¼Œæå‡ä»»åŠ¡æ³›åŒ–æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±æ¨¡å‹` `å¥–åŠ±æ¨ç†` `è‡ªè®­ç»ƒ` `ç”Ÿæˆå¼æ¨¡å‹` `Transformer` `äººæœºåé¦ˆå¼ºåŒ–å­¦ä¹ ` `å“åº”æ’åº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¥–åŠ±æ¨¡å‹ä¸¥é‡ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨åå¥½æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ³›åŒ–æ€§å—é™ã€‚
2. æå‡ºGRAM-R$^2$ï¼Œé€šè¿‡è‡ªè®­ç»ƒç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼Œåˆ©ç”¨æœªæ ‡æ³¨æ•°æ®è¿›è¡Œå¥–åŠ±æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGRAM-R$^2$åœ¨å“åº”æ’åºã€ä»»åŠ¡é€‚åº”å’Œäººæœºåé¦ˆå¼ºåŒ–å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¥–åŠ±å»ºæ¨¡çš„æ˜¾è‘—è¿›å±•å¾—ç›Šäºä»ä»»åŠ¡ç‰¹å®šè®¾è®¡å‘é€šç”¨å¥–åŠ±æ¨¡å‹çš„èŒƒå¼è½¬å˜ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¼€å‘æœ‰æ•ˆçš„å¥–åŠ±æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæ ¹æœ¬æ€§çš„æŒ‘æˆ˜ï¼šä¸¥é‡ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨çš„åå¥½æ•°æ®ã€‚åœ¨å¤§é‡æœªæ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒæä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ï¼Œä½†ç°æœ‰æ–¹æ³•æœªèƒ½å°†æ˜¾å¼æ¨ç†èå…¥å¥–åŠ±æ¨¡å‹ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªè®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®æ¥æ¿€å‘å¥–åŠ±æ¨¡å‹ä¸­çš„å¥–åŠ±æ¨ç†ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†GRAM-R$^2$ï¼Œä¸€ä¸ªç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼Œç»è¿‡è®­ç»ƒä¸ä»…å¯ä»¥ç”Ÿæˆåå¥½æ ‡ç­¾ï¼Œè¿˜å¯ä»¥ç”Ÿæˆä¼´éšçš„å¥–åŠ±ç†ç”±ã€‚GRAM-R$^2$å¯ä»¥ä½œä¸ºå¥–åŠ±æ¨ç†çš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶å¯ä»¥åº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œåªéœ€æå°‘æˆ–æ— éœ€é¢å¤–çš„å¾®è°ƒã€‚å®ƒå¯ä»¥æ”¯æŒä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚å“åº”æ’åºå’Œä»»åŠ¡ç‰¹å®šå¥–åŠ±è°ƒæ•´ã€‚åœ¨å“åº”æ’åºã€ä»»åŠ¡é€‚åº”å’ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒGRAM-R$^2$å§‹ç»ˆæä¾›å¼ºå¤§çš„æ€§èƒ½ï¼Œä¼˜äºå‡ ä¸ªå¼ºå¤§çš„åˆ¤åˆ«å¼å’Œç”Ÿæˆå¼åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¥–åŠ±æ¨¡å‹ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨çš„åå¥½æ•°æ®ï¼Œè·å–æˆæœ¬é«˜æ˜‚ï¼Œä¸”æ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°æ–°çš„ä»»åŠ¡å’Œåœºæ™¯ã€‚ç¼ºä¹æ˜¾å¼çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å¾—æ¨¡å‹éš¾ä»¥ç†è§£å¥–åŠ±èƒŒåçš„åŸå› ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨æœªæ ‡æ³¨æ•°æ®è¿›è¡Œè‡ªè®­ç»ƒï¼Œé€šè¿‡ç”Ÿæˆå¥–åŠ±ç†ç”±æ¥å¢å¼ºå¥–åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯è®©æ¨¡å‹ä¸ä»…é¢„æµ‹åå¥½æ ‡ç­¾ï¼Œè¿˜è¦è§£é‡Šä¸ºä»€ä¹ˆåšå‡ºè¿™æ ·çš„åå¥½é€‰æ‹©ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´æ·±å±‚æ¬¡çš„å¥–åŠ±æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRAM-R$^2$æ˜¯ä¸€ä¸ªç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼Œå…¶è®­ç»ƒè¿‡ç¨‹åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨æœªæ ‡æ³¨æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œæ¨¡å‹å­¦ä¹ ç”Ÿæˆåå¥½æ ‡ç­¾å’Œå¯¹åº”çš„å¥–åŠ±ç†ç”±ï¼›2) ä½¿ç”¨å°‘é‡æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼›3) å°†è®­ç»ƒå¥½çš„æ¨¡å‹åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚å“åº”æ’åºå’Œä»»åŠ¡ç‰¹å®šå¥–åŠ±è°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†å¥–åŠ±ç†ç”±ç”Ÿæˆæœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ˜¾å¼çš„å¥–åŠ±æ¨ç†ã€‚ä¸ä¼ ç»Ÿçš„åˆ¤åˆ«å¼å¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼ŒGRAM-R$^2$èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¥–åŠ±èƒŒåçš„åŸå› ï¼Œä»è€Œå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚ä¸ç°æœ‰çš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼ŒGRAM-R$^2$æ›´åŠ æ³¨é‡å¥–åŠ±æ¨ç†èƒ½åŠ›çš„åŸ¹å…»ã€‚

**å…³é”®è®¾è®¡**ï¼šGRAM-R$^2$é‡‡ç”¨Transformeræ¶æ„ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ç”Ÿæˆåå¥½æ ‡ç­¾å’Œå¥–åŠ±ç†ç”±ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æŠ€å·§æ¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨teacher forcingæ¥åŠ é€Ÿè®­ç»ƒï¼Œä½¿ç”¨beam searchæ¥ç”Ÿæˆé«˜è´¨é‡çš„å¥–åŠ±ç†ç”±ã€‚å¥–åŠ±ç†ç”±çš„é•¿åº¦å’Œå†…å®¹ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨è¾¾å¥–åŠ±èƒŒåçš„åŸå› ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGRAM-R$^2$åœ¨å“åº”æ’åºã€ä»»åŠ¡é€‚åº”å’ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å“åº”æ’åºä»»åŠ¡ä¸Šï¼ŒGRAM-R$^2$çš„æ€§èƒ½ä¼˜äºå¤šä¸ªå¼ºå¤§çš„åˆ¤åˆ«å¼å’Œç”Ÿæˆå¼åŸºçº¿ã€‚åœ¨ä»»åŠ¡é€‚åº”ä»»åŠ¡ä¸Šï¼ŒGRAM-R$^2$ä»…éœ€å°‘é‡å¾®è°ƒå³å¯è¾¾åˆ°ä¸ä¸“é—¨è®­ç»ƒçš„æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GRAM-R$^2$å¯å¹¿æ³›åº”ç”¨äºå¯¹è¯ç³»ç»Ÿã€æ¨èç³»ç»Ÿã€æœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸã€‚é€šè¿‡å­¦ä¹ äººç±»çš„åå¥½å’Œå¥–åŠ±æœºåˆ¶ï¼Œå¯ä»¥æå‡ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚ä¾‹å¦‚ï¼Œåœ¨å¯¹è¯ç³»ç»Ÿä¸­ï¼Œå¯ä»¥åˆ©ç”¨GRAM-R$^2$æ¥è¯„ä¼°ä¸åŒå›å¤çš„è´¨é‡ï¼Œé€‰æ‹©æœ€ç¬¦åˆç”¨æˆ·æ„å›¾çš„å›å¤ã€‚åœ¨æœºå™¨äººæ§åˆ¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨GRAM-R$^2$æ¥å­¦ä¹ äººç±»çš„é©¾é©¶ä¹ æƒ¯ï¼Œä»è€Œå®ç°è‡ªåŠ¨é©¾é©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Significant progress in reward modeling over recent years has been driven by a paradigm shift from task-specific designs towards generalist reward models. Despite this trend, developing effective reward models remains a fundamental challenge: the heavy reliance on large-scale labeled preference data. Pre-training on abundant unlabeled data offers a promising direction, but existing approaches fall short of instilling explicit reasoning into reward models. To bridge this gap, we propose a self-training approach that leverages unlabeled data to elicit reward reasoning in reward models. Based on this approach, we develop GRAM-R$^2$, a generative reward model trained to produce not only preference labels but also accompanying reward rationales. GRAM-R$^2$ can serve as a foundation model for reward reasoning and can be applied to a wide range of tasks with minimal or no additional fine-tuning. It can support downstream applications such as response ranking and task-specific reward tuning. Experiments on response ranking, task adaptation, and reinforcement learning from human feedback demonstrate that GRAM-R$^2$ consistently delivers strong performance, outperforming several strong discriminative and generative baselines.

