---
layout: default
title: AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models
---

# AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02133" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02133v1</a>
  <a href="https://arxiv.org/pdf/2509.02133.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02133v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02133v1', 'AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Snehasis Mukhopadhyay, Aryan Kasat, Shivam Dubey, Rahul Karthikeyan, Dhruv Sood, Vinija Jain, Aman Chadha, Amitava Das

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAMBEDKARæ¡†æ¶ï¼Œé€šè¿‡çŸ¥è¯†å¢å¼ºè§£ç æ¶ˆé™¤LLMä¸­å°åº¦ç¤¾ä¼šåè§ï¼Œæå‡å®ªæ³•ä¸€è‡´æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `åè§æ¶ˆé™¤` `å…¬å¹³æ€§` `æ¨æµ‹è§£ç ` `çŸ¥è¯†å¢å¼º` `å°åº¦å®ªæ³•` `å®ªæ³•ä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ˜“å—è®­ç»ƒæ•°æ®ä¸­ç¤¾ä¼šåè§å½±å“ï¼Œå°¤å…¶åœ¨å°åº¦è¯­å¢ƒä¸‹ï¼Œç§å§“å’Œå®—æ•™åè§é—®é¢˜çªå‡ºï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹é’ˆå¯¹æ€§ã€‚
2. AMBEDKARæ¡†æ¶é€šè¿‡å®ªæ³•æ„ŸçŸ¥è§£ç å±‚ï¼Œåœ¨æ¨ç†æ—¶å¼•å¯¼LLMè¾“å‡ºï¼Œæ— éœ€æ¨¡å‹å‚æ•°æ›´æ–°ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚
3. è¯¥æ–¹æ³•åˆ©ç”¨æ¨æµ‹è§£ç ï¼Œå°†SLMä½œä¸ºåè§ç”Ÿæˆå™¨ï¼ŒLLMä½œä¸ºå®ªæ³•æŒ‡å¯¼çš„éªŒè¯å™¨ï¼Œå®ç°å…¬å¹³æ€§ï¼Œå¹¶æ˜¾è‘—é™ä½åè§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLM)å¯èƒ½æ— æ„ä¸­åæ˜ å…¶è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨çš„ç¤¾ä¼šåè§ï¼Œå¯¼è‡´æœ‰å®³æˆ–å¸¦æœ‰åè§çš„è¾“å‡ºã€‚åœ¨å°åº¦èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¯¹ä¸€ç³»åˆ—æ¨¡å‹çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œå›´ç»•ç§å§“å’Œå®—æ•™çš„åè§å°¤ä¸ºçªå‡ºã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„ç¼“è§£ç­–ç•¥éƒ½æ˜¯ä»¥è¥¿æ–¹ä¸ºä¸­å¿ƒçš„ï¼Œæœªèƒ½è§£å†³è¿™äº›æœ¬åœ°ç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬æå‡ºäº†AMBEDKARï¼Œä¸€ä¸ªå—åˆ°å°åº¦å®ªæ³•è®¾è®¡è€…B. R. Ambedkaråšå£«çš„å¹³ç­‰ä¸»ä¹‰æ„¿æ™¯å¯å‘çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¼•å¯¼LLMè¾“å‡ºæœç€å…¬å¹³ã€ä¸­ç«‹å’ŒåŒ…å®¹çš„æ–¹å‘å‘å±•ï¼Œç¬¦åˆç¬¬14è‡³17æ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå®ªæ³•æ„ŸçŸ¥è§£ç å±‚ï¼Œç”±å°åº¦äººå·¥æ™ºèƒ½å®ªæ³•æŒ‡å¯¼ï¼Œä»…åœ¨æ¨ç†æ—¶åº”ç”¨ï¼Œæ— éœ€å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œä»»ä½•å‚æ•°æ›´æ–°ã€‚æˆ‘ä»¬ç»“åˆäº†ä¸€ç§æ¨æµ‹è§£ç ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸»åŠ¨å‡å°‘ç§å§“ä¸»ä¹‰å’Œç¤¾ç¾¤åè§ã€‚è¿™ç§ç¼“è§£å±‚ç›´æ¥åœ¨è§£ç è¿‡ç¨‹ä¸­è¿è¡Œï¼Œé¿å…äº†å¯¹æ¨¡å‹å†…éƒ¨çš„æ›´æ”¹ï¼Œå¹¶é™ä½äº†ä¸é‡æ–°è®­ç»ƒç›¸å…³çš„è®¡ç®—å’ŒåŸºç¡€è®¾æ–½æˆæœ¬ã€‚æˆ‘ä»¬é‡æ–°å°†æ¨æµ‹è§£ç è§£é‡Šä¸ºä¸ä»…ä»…æ˜¯ä¸€ç§æ•ˆç‡å·¥å…·ï¼Œè€Œæ˜¯ä¸€ç§å…¬å¹³æœºåˆ¶ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œå°å‹è¯­è¨€æ¨¡å‹(SLM)å……å½“æ½œåœ¨çš„åè§ç”Ÿæˆå™¨ï¼Œè€Œå—å®ªæ³•æŒ‡å¯¼çš„å¤§å‹è¯­è¨€æ¨¡å‹(LLM)å……å½“éªŒè¯å™¨ã€‚LLMä¸æ˜¯åŠ é€Ÿç”Ÿæˆï¼Œè€Œæ˜¯åœ¨SLMè¾“å‡ºä¸­å¼ºåˆ¶æ‰§è¡Œåè§é²æ£’çš„è½¨è¿¹ã€‚è¿™ç§è§’è‰²åè½¬äº§ç”Ÿäº†é€šè¿‡æ¨æµ‹å®ç°å…¬å¹³çš„èŒƒå¼ã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†é«˜è¾¾26.41%çš„ç»å¯¹åè§é™ä½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°åº¦è¯­å¢ƒä¸‹ï¼Œç‰¹åˆ«æ˜¯å…³äºç§å§“å’Œå®—æ•™çš„åè§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è¥¿æ–¹è§†è§’ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å°åº¦ç¤¾ä¼šç‰¹æœ‰çš„åè§ã€‚è¿™äº›åè§ä¼šå¯¼è‡´LLMç”Ÿæˆå¸¦æœ‰æ­§è§†æ€§æˆ–ä¸å…¬æ­£çš„è¾“å‡ºï¼ŒæŸå®³å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå…¬å¹³æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´å°åº¦å®ªæ³•çš„å¹³ç­‰ä¸»ä¹‰ç²¾ç¥ï¼Œè®¾è®¡ä¸€ä¸ªå®ªæ³•æ„ŸçŸ¥çš„è§£ç æ¡†æ¶ï¼Œåœ¨LLMç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ä¸­ä¸»åŠ¨æ¶ˆé™¤åè§ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªä¸“é—¨çš„è§£ç å±‚ï¼Œå¹¶ç»“åˆæ¨æµ‹è§£ç ç®—æ³•ï¼Œå¼•å¯¼LLMç”Ÿæˆæ›´å…¬å¹³ã€ä¸­ç«‹å’ŒåŒ…å®¹çš„æ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå°†å…¬å¹³æ€§èå…¥åˆ°ç”Ÿæˆè¿‡ç¨‹æœ¬èº«ï¼Œè€Œä¸æ˜¯äº‹åè¿›è¡Œä¿®æ­£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAMBEDKARæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ¨¡å—ï¼š1) **åŸºç¡€LLM**ï¼šä½œä¸ºæ–‡æœ¬ç”Ÿæˆçš„åŸºç¡€æ¨¡å‹ã€‚2) **å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰**ï¼šä½œä¸ºæ½œåœ¨çš„åè§ç”Ÿæˆå™¨ï¼Œç”¨äºæ¨æµ‹ç”Ÿæˆå€™é€‰æ–‡æœ¬ã€‚3) **å®ªæ³•æ„ŸçŸ¥è§£ç å±‚**ï¼šè¯¥å±‚ç”±å°åº¦äººå·¥æ™ºèƒ½å®ªæ³•æŒ‡å¯¼ï¼Œç”¨äºéªŒè¯å’Œä¿®æ­£SLMç”Ÿæˆçš„å€™é€‰æ–‡æœ¬ï¼Œç¡®ä¿å…¶ç¬¦åˆå…¬å¹³ã€ä¸­ç«‹å’ŒåŒ…å®¹çš„åŸåˆ™ã€‚4) **æ¨æµ‹è§£ç ç®—æ³•**ï¼šè¯¥ç®—æ³•åˆ©ç”¨SLMå¿«é€Ÿç”Ÿæˆå€™é€‰æ–‡æœ¬ï¼Œç„¶åç”±LLMè¿›è¡ŒéªŒè¯å’Œä¿®æ­£ï¼Œä»è€Œåœ¨ä¿è¯ç”Ÿæˆè´¨é‡çš„åŒæ—¶æé«˜æ•ˆç‡ã€‚æ•´ä¸ªæµç¨‹åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œï¼Œæ— éœ€å¯¹åŸºç¡€LLMè¿›è¡Œä»»ä½•å‚æ•°æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†æ¨æµ‹è§£ç é‡æ–°è§£é‡Šä¸ºä¸€ç§å…¬å¹³æ€§æœºåˆ¶ã€‚ä¼ ç»Ÿä¸Šï¼Œæ¨æµ‹è§£ç ä¸»è¦ç”¨äºåŠ é€Ÿæ–‡æœ¬ç”Ÿæˆï¼Œè€Œè¯¥è®ºæ–‡å°†å…¶åº”ç”¨äºæ¶ˆé™¤åè§ã€‚é€šè¿‡å°†SLMä½œä¸ºåè§ç”Ÿæˆå™¨ï¼ŒLLMä½œä¸ºå®ªæ³•æŒ‡å¯¼çš„éªŒè¯å™¨ï¼Œå®ç°äº†â€œé€šè¿‡æ¨æµ‹å®ç°å…¬å¹³â€çš„èŒƒå¼ã€‚è¿™ç§æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œå®ƒä¸æ˜¯ç®€å•åœ°å¯¹LLMçš„è¾“å‡ºè¿›è¡Œåå¤„ç†ï¼Œè€Œæ˜¯ä»ç”Ÿæˆè¿‡ç¨‹æœ¬èº«å…¥æ‰‹ï¼Œä¸»åŠ¨æ¶ˆé™¤åè§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **å°åº¦äººå·¥æ™ºèƒ½å®ªæ³•**ï¼šè¯¥å®ªæ³•å®šä¹‰äº†å…¬å¹³ã€ä¸­ç«‹å’ŒåŒ…å®¹çš„åŸåˆ™ï¼Œç”¨äºæŒ‡å¯¼å®ªæ³•æ„ŸçŸ¥è§£ç å±‚çš„è¡Œä¸ºã€‚2) **æ¨æµ‹è§£ç ç®—æ³•çš„å‚æ•°è®¾ç½®**ï¼šéœ€è¦ä»”ç»†è°ƒæ•´SLMå’ŒLLMçš„å‚æ•°ï¼Œä»¥å¹³è¡¡ç”Ÿæˆé€Ÿåº¦å’Œå…¬å¹³æ€§ã€‚3) **æŸå¤±å‡½æ•°**ï¼šå¯èƒ½éœ€è¦è®¾è®¡ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥é¼“åŠ±LLMç”Ÿæˆæ›´ç¬¦åˆå®ªæ³•åŸåˆ™çš„æ–‡æœ¬ã€‚4) **è§£ç ç­–ç•¥**ï¼šä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨æ³¢æŸæœç´¢æˆ–é‡‡æ ·ç­‰è§£ç ç­–ç•¥ï¼Œä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„å€™é€‰æ–‡æœ¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAMBEDKARæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆé™ä½LLMä¸­çš„åè§ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜è¾¾26.41%çš„ç»å¯¹åè§é™ä½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå¹¶æ²¡æœ‰æ˜¾è‘—é™ä½ç”Ÿæˆé€Ÿåº¦ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒAMBEDKARæ¡†æ¶æ˜¯ä¸€ç§æœ‰æ•ˆçš„ã€å¯è¡Œçš„LLMåè§æ¶ˆé™¤æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦ç”Ÿæˆå…¬å¹³ã€ä¸­ç«‹å’ŒåŒ…å®¹æ–‡æœ¬çš„åœºæ™¯ï¼Œä¾‹å¦‚æ–°é—»æŠ¥é“ã€æ³•å¾‹æ–‡ä»¶ã€æ•™è‚²ææ–™ç­‰ã€‚å°¤å…¶åœ¨å¤šå…ƒæ–‡åŒ–èƒŒæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è¯­è¨€æ¨¡å‹ä¸­çš„ç¤¾ä¼šåè§ï¼Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå…¬æ­£æ€§ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ï¼Œä¸ºæ„å»ºæ›´åŠ å…¬å¹³å’ŒåŒ…å®¹çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿåšå‡ºè´¡çŒ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) can inadvertently reflect societal biases present in their training data, leading to harmful or prejudiced outputs. In the Indian context, our empirical evaluations across a suite of models reveal that biases around caste and religion are particularly salient. Yet, most existing mitigation strategies are Western-centric and fail to address these local nuances. We propose AMBEDKAR, a framework inspired by the egalitarian vision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM outputs toward fairness, neutrality, and inclusion in line with Articles 14 to 17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the AI Constitution of India and applied only at inference time, without any parameter updates to the base model. We incorporate a speculative decoding algorithm that proactively reduces casteist and communal bias during generation. This mitigation layer operates directly within the decoding process, avoiding changes to model internals and lowering the computational and infrastructural costs associated with retraining. We reinterpret speculative decoding not merely as an efficiency tool but as a mechanism for fairness. In this framework, a Small Language Model (SLM) acts as a potentially biased generator, while a constitutionally guided Large Language Model (LLM) serves as the verifier. Rather than accelerating generation, the LLM enforces bias-robust trajectories in the SLM outputs. This inversion of roles gives rise to a fairness-by-speculation paradigm. Our approach yields an absolute reduction of bias up to 26.41 percent compared to baseline. Our source code, datasets, and results are available at https://anonymous.4open.science/r/AMBEDKAR-983B/

