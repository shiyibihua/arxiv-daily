---
layout: default
title: E-THER: A Multimodal Dataset for Empathic AI -- Towards Emotional Mismatch Awareness
---

# E-THER: A Multimodal Dataset for Empathic AI -- Towards Emotional Mismatch Awareness

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02100" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02100v2</a>
  <a href="https://arxiv.org/pdf/2509.02100.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02100v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02100v2', 'E-THER: A Multimodal Dataset for Empathic AI -- Towards Emotional Mismatch Awareness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sharjeel Tahir, Judith Johnson, Jumana Abu-Khalaf, Syed Afaq Ali Shah

**åˆ†ç±»**: cs.HC, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02 (æ›´æ–°: 2025-09-08)

**å¤‡æ³¨**: 15 pages, 4 figures. Preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºE-THERå¤šæ¨¡æ€æ•°æ®é›†ï¼Œç”¨äºæå‡AIåœ¨è¯†åˆ«è¨€è¯­-è§†è§‰æƒ…æ„Ÿä¸ä¸€è‡´æ–¹é¢çš„èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…±æƒ…AI` `å¤šæ¨¡æ€æ•°æ®é›†` `æƒ…æ„Ÿä¸ä¸€è‡´` `äººæœ¬ä¸»ä¹‰æ²»ç–—` `è§†è§‰-è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…±æƒ…AIç³»ç»Ÿç¼ºä¹è¯†åˆ«è¨€è¯­ä¸æƒ…æ„ŸçŠ¶æ€ä¸ä¸€è‡´çš„èƒ½åŠ›ï¼Œé˜»ç¢äº†å…¶çœŸæ­£å…±æƒ…èƒ½åŠ›çš„æå‡ã€‚
2. E-THERæ•°æ®é›†é€šè¿‡æä¾›è¨€è¯­-è§†è§‰ä¸ä¸€è‡´çš„å¤šç»´æ ‡æ³¨ï¼Œæ—¨åœ¨è®­ç»ƒAIè¯†åˆ«å’Œç†è§£æ·±å±‚æƒ…æ„Ÿã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨E-THERè®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å…±æƒ…å¯¹è¯è´¨é‡å’Œæ²»ç–—å‚ä¸åº¦æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰å…±æƒ…AIç³»ç»Ÿçš„ä¸€ä¸ªæ™®éç¼ºé™·æ˜¯æ— æ³•è¯†åˆ«è¨€è¯­è¡¨è¾¾å¯èƒ½æ— æ³•å®Œå…¨åæ˜ æ½œåœ¨æƒ…æ„ŸçŠ¶æ€çš„æƒ…å†µã€‚è¿™æ˜¯å› ä¸ºç°æœ‰æ•°æ®é›†ä¾§é‡äºè¡¨é¢å±‚é¢çš„æƒ…æ„Ÿè¯†åˆ«ï¼Œè€Œæ²¡æœ‰è§£å†³å¯¹å…±æƒ…ç†è§£æœ‰ç”¨çš„å¤æ‚è¨€è¯­-è§†è§‰ä¸ä¸€è‡´ï¼ˆä¸åŒ¹é…ï¼‰æ¨¡å¼ã€‚æœ¬æ–‡æå‡ºäº†E-THERï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºä»¥äººä¸ºä¸­å¿ƒçš„æ²»ç–—çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå…·æœ‰ç”¨äºè¨€è¯­-è§†è§‰ä¸ä¸€è‡´æ£€æµ‹çš„å¤šç»´æ³¨é‡Šï¼Œä»è€Œèƒ½å¤Ÿè®­ç»ƒAIç³»ç»Ÿï¼Œä½¿å…¶å‘å±•çœŸæ­£çš„è€Œéè¡¨æ¼”æ€§çš„å…±æƒ…èƒ½åŠ›ã€‚æ•°æ®é›†ä¸­åŒ…å«çš„æ³¨é‡Šæ¥è‡ªäººæœ¬ä¸»ä¹‰æ–¹æ³•ï¼Œå³è¯†åˆ«å®¢æˆ·-å’¨è¯¢å¸ˆäº’åŠ¨ä¸­çš„è¨€è¯­-è§†è§‰æƒ…æ„Ÿé”™ä½ï¼Œä»è€Œå½¢æˆè®­ç»ƒå’Œè¯„ä¼°AIåœ¨å…±æƒ…ä»»åŠ¡ä¸Šçš„æ¡†æ¶ã€‚é¢å¤–çš„å‚ä¸åº¦è¯„åˆ†æä¾›äº†ç”¨äºç ”ç©¶åº”ç”¨çš„ behavioral æ³¨é‡Šã€‚åœ¨ä½¿ç”¨åŸºäºå…±æƒ…å’Œæ²»ç–—åŸåˆ™çš„è¯„ä¼°æŒ‡æ ‡æ—¶ï¼Œåœ¨æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­ï¼Œä¾‹å¦‚IDEFICSå’ŒVideoLLAVAï¼Œè§‚å¯Ÿåˆ°å…±æƒ…å’Œæ²»ç–—å¯¹è¯è´¨é‡çš„æ˜¾ç€æé«˜ã€‚å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬ç»è¿‡ä¸ä¸€è‡´è®­ç»ƒçš„æ¨¡å‹åœ¨å…³é”®ç‰¹å¾æ–¹é¢ä¼˜äºé€šç”¨æ¨¡å‹ï¼Œä¾‹å¦‚ç»´æŒæ²»ç–—å‚ä¸åº¦ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘äººä¸ºæˆ–å¤¸å¼ çš„è¯­è¨€æ¨¡å¼ï¼Œä»¥åŠä¿æŒå¯¹PCTç†è®ºæ¡†æ¶çš„å¿ å®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å…±æƒ…AIç³»ç»Ÿä¸»è¦ä¾èµ–äºè¡¨é¢å±‚é¢çš„æƒ…æ„Ÿè¯†åˆ«ï¼Œå¿½ç•¥äº†è¨€è¯­è¡¨è¾¾ä¸å®é™…æƒ…æ„ŸçŠ¶æ€å¯èƒ½å­˜åœ¨çš„ä¸ä¸€è‡´æ€§ã€‚è¿™ç§ä¸ä¸€è‡´æ€§æ˜¯äººç±»å…±æƒ…ç†è§£çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰å’Œåˆ©ç”¨ï¼Œå¯¼è‡´AIç³»ç»Ÿéš¾ä»¥äº§ç”ŸçœŸæ­£çš„å…±æƒ…èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šE-THERæ•°æ®é›†çš„æ ¸å¿ƒæ€è·¯æ˜¯æä¾›åŒ…å«è¨€è¯­å’Œè§†è§‰ä¿¡æ¯çš„å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶å¯¹å…¶ä¸­çš„æƒ…æ„Ÿä¸ä¸€è‡´æ€§è¿›è¡Œæ ‡æ³¨ã€‚é€šè¿‡è®­ç»ƒAIæ¨¡å‹è¯†åˆ«å’Œç†è§£è¿™äº›ä¸ä¸€è‡´æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ¨æ–­ä¸ªä½“çš„æƒ…æ„ŸçŠ¶æ€ï¼Œä»è€Œæå‡å…±æƒ…èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•å€Ÿé‰´äº†ä»¥äººä¸ºä¸­å¿ƒçš„æ²»ç–—ï¼ˆPerson-Centered Therapy, PCTï¼‰çš„ç†è®ºæ¡†æ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šE-THERæ•°æ®é›†çš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å¤šç»´æ ‡æ³¨å’Œæ¨¡å‹è®­ç»ƒè¯„ä¼°ä¸‰ä¸ªé˜¶æ®µã€‚æ•°æ®æ”¶é›†é˜¶æ®µä¸»è¦æ”¶é›†å®¢æˆ·-å’¨è¯¢å¸ˆçš„äº’åŠ¨è§†é¢‘å’Œæ–‡æœ¬è®°å½•ã€‚å¤šç»´æ ‡æ³¨é˜¶æ®µåˆ™ç”±ä¸“ä¸šäººå‘˜å¯¹æ•°æ®è¿›è¡Œæ ‡æ³¨ï¼Œæ ‡æ³¨å†…å®¹åŒ…æ‹¬æƒ…æ„ŸçŠ¶æ€ã€è¨€è¯­è¡¨è¾¾ã€è§†è§‰è¡¨ç°ä»¥åŠè¨€è¯­-è§†è§‰æƒ…æ„Ÿä¸ä¸€è‡´æ€§ã€‚æ¨¡å‹è®­ç»ƒè¯„ä¼°é˜¶æ®µåˆ™ä½¿ç”¨æ ‡æ³¨å¥½çš„æ•°æ®è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨åŸºäºå…±æƒ…å’Œæ²»ç–—åŸåˆ™çš„è¯„ä¼°æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šE-THERæ•°æ®é›†çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¯¹è¨€è¯­-è§†è§‰æƒ…æ„Ÿä¸ä¸€è‡´æ€§çš„æ ‡æ³¨ã€‚è¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹è¿™ä¸€é—®é¢˜æ„å»ºçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’å’Œå·¥å…·ï¼Œç”¨äºå¼€å‘æ›´å…·å…±æƒ…èƒ½åŠ›çš„AIç³»ç»Ÿã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜é‡‡ç”¨äº†ä»¥äººä¸ºä¸­å¿ƒçš„æ²»ç–—ç†è®ºæ¡†æ¶ï¼Œä¸ºAIå…±æƒ…èƒ½åŠ›çš„å»ºæ¨¡æä¾›äº†ç†è®ºåŸºç¡€ã€‚

**å…³é”®è®¾è®¡**ï¼šE-THERæ•°æ®é›†çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é‡‡ç”¨å¤šæ¨¡æ€æ•°æ®ï¼ŒåŒ…å«è¨€è¯­å’Œè§†è§‰ä¿¡æ¯ï¼›2) å¯¹è¨€è¯­-è§†è§‰æƒ…æ„Ÿä¸ä¸€è‡´æ€§è¿›è¡Œå¤šç»´æ ‡æ³¨ï¼›3) åŸºäºä»¥äººä¸ºä¸­å¿ƒçš„æ²»ç–—ç†è®ºæ¡†æ¶è¿›è¡Œæ ‡æ³¨ï¼›4) æä¾›é¢å¤–çš„å‚ä¸åº¦è¯„åˆ†ï¼Œç”¨äºè¡Œä¸ºåˆ†æï¼›5) ä½¿ç”¨åŸºäºå…±æƒ…å’Œæ²»ç–—åŸåˆ™çš„è¯„ä¼°æŒ‡æ ‡è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚å…·ä½“å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©å–å†³äºæ‰€ä½¿ç”¨çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚IDEFICSå’ŒVideoLLAVAã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ä½¿ç”¨E-THERæ•°æ®é›†è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¦‚IDEFICSå’ŒVideoLLAVAï¼‰åœ¨å…±æƒ…å’Œæ²»ç–—å¯¹è¯è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡ä¸ä¸€è‡´è®­ç»ƒçš„æ¨¡å‹åœ¨ç»´æŒæ²»ç–—å‚ä¸åº¦ã€å‡å°‘äººä¸ºè¯­è¨€æ¨¡å¼ä»¥åŠä¿æŒå¯¹PCTç†è®ºæ¡†æ¶çš„å¿ å®æ€§æ–¹é¢ä¼˜äºé€šç”¨æ¨¡å‹ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†æ•´ä½“æå‡è¶‹åŠ¿æ˜æ˜¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

E-THERæ•°æ®é›†åŠå…¶è®­ç»ƒæ–¹æ³•å¯åº”ç”¨äºå¿ƒç†å’¨è¯¢ã€å®¢æˆ·æœåŠ¡ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡AIçš„å…±æƒ…èƒ½åŠ›ï¼Œå¯ä»¥æ”¹å–„å¿ƒç†å’¨è¯¢çš„æ•ˆæœï¼Œæé«˜å®¢æˆ·æœåŠ¡çš„æ»¡æ„åº¦ï¼Œå¹¶ä½¿äººæœºäº¤äº’æ›´åŠ è‡ªç„¶å’Œæµç•…ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æƒ…æ„Ÿç›¸å…³çš„é¢†åŸŸï¼Œä¾‹å¦‚æ•™è‚²å’ŒåŒ»ç–—ä¿å¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A prevalent shortfall among current empathic AI systems is their inability to recognize when verbal expressions may not fully reflect underlying emotional states. This is because the existing datasets, used for the training of these systems, focus on surface-level emotion recognition without addressing the complex verbal-visual incongruence (mismatch) patterns useful for empathic understanding. In this paper, we present E-THER, the first Person-Centered Therapy-grounded multimodal dataset with multidimensional annotations for verbal-visual incongruence detection, enabling training of AI systems that develop genuine rather than performative empathic capabilities. The annotations included in the dataset are drawn from humanistic approach, i.e., identifying verbal-visual emotional misalignment in client-counsellor interactions - forming a framework for training and evaluating AI on empathy tasks. Additional engagement scores provide behavioral annotations for research applications. Notable gains in empathic and therapeutic conversational qualities are observed in state-of-the-art vision-language models (VLMs), such as IDEFICS and VideoLLAVA, using evaluation metrics grounded in empathic and therapeutic principles. Empirical findings indicate that our incongruence-trained models outperform general-purpose models in critical traits, such as sustaining therapeutic engagement, minimizing artificial or exaggerated linguistic patterns, and maintaining fidelity to PCT theoretical framework.

