---
layout: default
title: "Implicit Reasoning in Large Language Models: A Comprehensive Survey"
---

# Implicit Reasoning in Large Language Models: A Comprehensive Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02350" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02350v1</a>
  <a href="https://arxiv.org/pdf/2509.02350.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02350v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02350v1', 'Implicit Reasoning in Large Language Models: A Comprehensive Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jindong Li, Yali Fu, Li Fan, Jiahong Liu, Yao Shu, Chengwei Qin, Menglin Yang, Irwin King, Rex Ying

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/digailab/awesome-llm-implicit-reasoning)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„éšå¼æ¨ç†ç ”ç©¶è¿›å±•ä¸æ‰§è¡ŒèŒƒå¼åˆ†æ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `éšå¼æ¨ç†` `æ€ç»´é“¾` `æ‰§è¡ŒèŒƒå¼` `æ½œåœ¨ä¼˜åŒ–` `ä¿¡å·å¼•å¯¼æ§åˆ¶` `å±‚é€’å½’æ‰§è¡Œ` `æ¨ç†è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–æ˜¾å¼æ€ç»´é“¾æç¤ºï¼Œæˆæœ¬é«˜ã€é€Ÿåº¦æ…¢ï¼Œä¸”ä¸æ¨¡å‹å†…éƒ¨è®¡ç®—ä¸å®Œå…¨ä¸€è‡´ï¼Œé™åˆ¶äº†æ¨ç†æ•ˆç‡ã€‚
2. è®ºæ–‡æå‡ºåŸºäºæ‰§è¡ŒèŒƒå¼çš„éšå¼æ¨ç†åˆ†ç±»ï¼Œå…³æ³¨LLMå†…éƒ¨è®¡ç®—ç­–ç•¥ï¼ŒåŒ…æ‹¬æ½œåœ¨ä¼˜åŒ–ã€ä¿¡å·å¼•å¯¼æ§åˆ¶å’Œå±‚é€’å½’æ‰§è¡Œã€‚
3. ç»¼è¿°åˆ†æäº†éšå¼æ¨ç†çš„ç»“æ„ã€è¡Œä¸ºå’Œè¡¨ç¤ºè¯æ®ï¼Œå¹¶æ•´ç†äº†è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›å‚è€ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚åˆ©ç”¨LLMè¿›è¡Œæ¨ç†æ˜¯è§£å†³å¤šæ­¥éª¤é—®é¢˜å’Œå¤æ‚å†³ç­–çš„å…³é”®ã€‚ä¸ºäº†æ”¯æŒé«˜æ•ˆæ¨ç†ï¼Œæœ€è¿‘çš„ç ”ç©¶å·²å°†æ³¨æ„åŠ›ä»æ˜¾å¼çš„æ€ç»´é“¾æç¤ºè½¬å‘éšå¼æ¨ç†ï¼Œåœ¨è¿™ç§æ¨ç†ä¸­ï¼Œæ¨ç†é€šè¿‡æ½œåœ¨ç»“æ„åœ¨å†…éƒ¨é™é»˜è¿›è¡Œï¼Œè€Œæ— éœ€å‘å‡ºä¸­é—´æ–‡æœ¬æ­¥éª¤ã€‚éšå¼æ¨ç†å…·æœ‰ç”Ÿæˆæˆæœ¬æ›´ä½ã€æ¨ç†é€Ÿåº¦æ›´å¿«ä»¥åŠä¸å†…éƒ¨è®¡ç®—æ›´å¥½åœ°å¯¹é½ç­‰ä¼˜ç‚¹ã€‚è™½ç„¶ä¹‹å‰çš„ç»¼è¿°å·²ç»è®¨è®ºäº†æ¨ç†èƒŒæ™¯ä¸‹çš„æ½œåœ¨è¡¨ç¤ºï¼Œä½†ä»ç„¶ç¼ºä¹å¯¹LLMå†…éƒ¨æ¨ç†å¦‚ä½•å±•å¼€çš„ä¸“é—¨å’Œæœºåˆ¶å±‚é¢çš„è€ƒå¯Ÿã€‚æœ¬ç»¼è¿°é€šè¿‡å¼•å…¥ä»¥æ‰§è¡ŒèŒƒå¼ä¸ºä¸­å¿ƒçš„åˆ†ç±»æ³•æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œå°†é‡ç‚¹ä»è¡¨ç¤ºå½¢å¼è½¬ç§»åˆ°è®¡ç®—ç­–ç•¥ã€‚æˆ‘ä»¬æ ¹æ®	extbf{	extit{å†…éƒ¨è®¡ç®—å¦‚ä½•ä»¥åŠåœ¨å“ªé‡Œå±•å¼€} }å°†ç°æœ‰æ–¹æ³•ç»„ç»‡ä¸ºä¸‰ç§æ‰§è¡ŒèŒƒå¼ï¼šæ½œåœ¨ä¼˜åŒ–ã€ä¿¡å·å¼•å¯¼æ§åˆ¶å’Œå±‚é€’å½’æ‰§è¡Œã€‚æˆ‘ä»¬è¿˜å›é¡¾äº†æ”¯æŒLLMä¸­å­˜åœ¨éšå¼æ¨ç†çš„ç»“æ„ã€è¡Œä¸ºå’ŒåŸºäºè¡¨ç¤ºçš„è¯æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹ç°æœ‰å·¥ä½œä¸­ç”¨äºè¯„ä¼°éšå¼æ¨ç†æœ‰æ•ˆæ€§å’Œå¯é æ€§çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†è¿›è¡Œäº†ç»“æ„åŒ–æ¦‚è¿°ã€‚æˆ‘ä»¬åœ¨https://github.com/digailab/awesome-llm-implicit-reasoningç»´æŠ¤ä¸€ä¸ªæŒç»­æ›´æ–°çš„é¡¹ç›®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ˜¾å¼æ€ç»´é“¾çš„æ–¹æ³•ï¼Œå­˜åœ¨ç”Ÿæˆæˆæœ¬é«˜ã€æ¨ç†é€Ÿåº¦æ…¢ã€ä¸æ¨¡å‹å†…éƒ¨è®¡ç®—ä¸å®Œå…¨å¯¹é½ç­‰é—®é¢˜ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†LLMåœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚å› æ­¤ï¼Œéœ€è¦ç ”ç©¶æ›´é«˜æ•ˆã€æ›´è‡ªç„¶çš„æ¨ç†æ–¹å¼ï¼Œå³éšå¼æ¨ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMçš„éšå¼æ¨ç†è¿‡ç¨‹è§†ä¸ºä¸€ç§è®¡ç®—ç­–ç•¥ï¼Œå¹¶æ ¹æ®å†…éƒ¨è®¡ç®—çš„æ‰§è¡Œæ–¹å¼è¿›è¡Œåˆ†ç±»ã€‚é€šè¿‡åˆ†æä¸åŒçš„æ‰§è¡ŒèŒƒå¼ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£LLMå¦‚ä½•åˆ©ç”¨å…¶å†…éƒ¨ç»“æ„å’Œå‚æ•°è¿›è¡Œæ¨ç†ï¼Œä»è€Œä¸ºæ”¹è¿›æ¨ç†æ–¹æ³•æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡å°†ç°æœ‰çš„éšå¼æ¨ç†æ–¹æ³•åˆ†ä¸ºä¸‰ç§æ‰§è¡ŒèŒƒå¼ï¼š
1. **æ½œåœ¨ä¼˜åŒ–**ï¼šé€šè¿‡ä¼˜åŒ–æ½œåœ¨ç©ºé—´ä¸­çš„è¡¨ç¤ºæ¥è¿›è¡Œæ¨ç†ã€‚
2. **ä¿¡å·å¼•å¯¼æ§åˆ¶**ï¼šåˆ©ç”¨å¤–éƒ¨ä¿¡å·ï¼ˆå¦‚æç¤ºæˆ–å¥–åŠ±ï¼‰æ¥å¼•å¯¼LLMçš„æ¨ç†è¿‡ç¨‹ã€‚
3. **å±‚é€’å½’æ‰§è¡Œ**ï¼šé€šè¿‡åœ¨LLMçš„ä¸åŒå±‚ä¹‹é—´è¿›è¡Œé€’å½’è®¡ç®—æ¥è¿›è¡Œæ¨ç†ã€‚
è®ºæ–‡å¯¹æ¯ç§èŒƒå¼ä¸‹çš„ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†è¯¦ç»†çš„åˆ†æå’Œæ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºäºæ‰§è¡ŒèŒƒå¼çš„éšå¼æ¨ç†åˆ†ç±»æ–¹æ³•ã€‚è¿™ç§åˆ†ç±»æ–¹æ³•å°†ç ”ç©¶é‡ç‚¹ä»è¡¨ç¤ºå½¢å¼è½¬ç§»åˆ°è®¡ç®—ç­–ç•¥ï¼Œä¸ºç†è§£å’Œæ”¹è¿›LLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ã€‚ä¸ä»¥å¾€å…³æ³¨è¡¨å±‚ç°è±¡çš„ç ”ç©¶ä¸åŒï¼Œè¯¥ç»¼è¿°æ·±å…¥æ¢è®¨äº†LLMå†…éƒ¨çš„è®¡ç®—æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æ²¡æœ‰æå‡ºæ–°çš„ç®—æ³•æˆ–æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªç»¼è¿°æ€§çš„å·¥ä½œã€‚å…³é”®åœ¨äºå¯¹ç°æœ‰æ–¹æ³•çš„åˆ†ç±»å’Œåˆ†æï¼Œä»¥åŠå¯¹è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†çš„æ•´ç†ã€‚è®ºæ–‡è¯¦ç»†æè¿°äº†æ¯ç§æ‰§è¡ŒèŒƒå¼çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ï¼Œå¹¶å¯¹ä¸åŒæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¨è®ºäº†éšå¼æ¨ç†çš„è¯„ä¼°æ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ä¹‹å¤„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°ç³»ç»Ÿæ€§åœ°æ•´ç†å’Œåˆ†æäº†LLMä¸­éšå¼æ¨ç†çš„ç ”ç©¶è¿›å±•ï¼Œæå‡ºäº†åŸºäºæ‰§è¡ŒèŒƒå¼çš„åˆ†ç±»æ–¹æ³•ï¼Œå¹¶å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†è¯¦ç»†çš„æ¯”è¾ƒå’Œåˆ†æã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜æ•´ç†äº†è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚é€šè¿‡githubé¡¹ç›®æŒç»­æ›´æ–°ï¼Œä¿æŒäº†ç ”ç©¶çš„åŠæ—¶æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½é—®ç­”ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡ç†è§£å’Œåˆ©ç”¨LLMçš„éšå¼æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜æ¨ç†é€Ÿåº¦ï¼Œå¹¶æ”¹å–„æ¨¡å‹ä¸äººç±»æ€ç»´çš„å¯¹é½ç¨‹åº¦ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œåº”ç”¨æ•ˆæœã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥åŸºäºæ­¤ç»¼è¿°ï¼Œæ¢ç´¢æ›´æœ‰æ•ˆçš„éšå¼æ¨ç†æ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ›´å¹¿æ³›çš„é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated strong generalization across a wide range of tasks. Reasoning with LLMs is central to solving multi-step problems and complex decision-making. To support efficient reasoning, recent studies have shifted attention from explicit chain-of-thought prompting toward implicit reasoning, where reasoning occurs silently via latent structures without emitting intermediate textual steps. Implicit reasoning brings advantages such as lower generation cost, faster inference, and better alignment with internal computation. Although prior surveys have discussed latent representations in the context of reasoning, a dedicated and mechanism-level examination of how reasoning unfolds internally within LLMs remains absent. This survey fills that gap by introducing a taxonomy centered on execution paradigms, shifting the focus from representational forms to computational strategies. We organize existing methods into three execution paradigms based on \textbf{\textit{how and where internal computation unfolds} }: latent optimization, signal-guided control, and layer-recurrent execution. We also review structural, behavioral and representation-based evidence that supports the presence of implicit reasoning in LLMs. We further provide a structured overview of the evaluation metrics and benchmarks used in existing works to assess the effectiveness and reliability of implicit reasoning. We maintain a continuously updated project at: https://github.com/digailab/awesome-llm-implicit-reasoning.

