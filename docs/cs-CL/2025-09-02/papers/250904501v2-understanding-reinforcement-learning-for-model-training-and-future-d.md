---
layout: default
title: Understanding Reinforcement Learning for Model Training, and future directions with GRAPE
---

# Understanding Reinforcement Learning for Model Training, and future directions with GRAPE

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04501" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04501v2</a>
  <a href="https://arxiv.org/pdf/2509.04501.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04501v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04501v2', 'Understanding Reinforcement Learning for Model Training, and future directions with GRAPE')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rohit Patel

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02 (æ›´æ–°: 2025-10-21)

**å¤‡æ³¨**: 35 pages, 1 figure

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ·±å…¥å‰–ææŒ‡ä»¤è°ƒä¼˜å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶æå‡ºGRAPEæ–°æ–¹å‘**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æŒ‡ä»¤è°ƒä¼˜` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç­–ç•¥ä¼˜åŒ–` `TRPO` `PPO` `GRAPE`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æŒ‡ä»¤è°ƒä¼˜ç®—æ³•è§£é‡Šé€šå¸¸é¢„è®¾çŸ¥è¯†èƒŒæ™¯ï¼Œç¼ºä¹ç»†èŠ‚æˆ–è¿‡äºå¤æ‚ï¼Œéš¾ä»¥ç†è§£ã€‚
2. è®ºæ–‡é€šè¿‡ç®€åŒ–ç¬¦å·å’Œèšç„¦LLMï¼Œé€æ­¥è§£æSFTã€REINFORCEç­‰ç®—æ³•ï¼ŒåŠ›æ±‚æ¸…æ™°ç›´è§‚ã€‚
3. è®ºæ–‡ä¸ä»…å›é¡¾äº†ç°æœ‰æŠ€æœ¯ï¼Œè¿˜æå‡ºäº†GRAPEè¿™ä¸€æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œæ¢ç´¢æœªæ¥å¯èƒ½æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»é›¶å¼€å§‹ï¼Œå…¨é¢é˜è¿°äº†æ¨¡å‹æŒ‡ä»¤è°ƒä¼˜çš„å…³é”®ç®—æ³•ï¼šSFTã€Rejection Samplingã€REINFORCEã€TRPOã€PPOã€GRPOå’ŒDPOã€‚ç°æœ‰ç®—æ³•çš„è§£é‡Šé€šå¸¸å‡è®¾è¯»è€…å…·å¤‡å…ˆéªŒçŸ¥è¯†ï¼Œç¼ºä¹å…³é”®ç»†èŠ‚ï¼Œæˆ–è€…è¿‡äºæ³›åŒ–å’Œå¤æ‚ã€‚æœ¬æ–‡é’ˆå¯¹LLMï¼Œé‡‡ç”¨ç®€åŒ–çš„æ˜¾å¼ç¬¦å·ï¼Œé€æ­¥è®¨è®ºå’Œå¼€å‘æ¯ç§æ–¹æ³•ï¼Œæ—¨åœ¨æ¶ˆé™¤æ­§ä¹‰ï¼Œæä¾›å¯¹æ¦‚å¿µçš„æ¸…æ™°ç›´è§‚ç†è§£ã€‚é€šè¿‡å‡å°‘å¯¹æ›´å¹¿æ³›çš„å¼ºåŒ–å­¦ä¹ æ–‡çŒ®çš„ç»•è¡Œï¼Œå¹¶å°†æ¦‚å¿µä¸LLMè”ç³»èµ·æ¥ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†å¤šä½™çš„æŠ½è±¡ï¼Œé™ä½äº†è®¤çŸ¥å¼€é”€ã€‚åœ¨é˜è¿°ä¹‹åï¼Œæˆ‘ä»¬å¯¹è¶…å‡ºè¯¦ç»†æè¿°çš„æ–°æŠ€æœ¯å’Œæ–¹æ³•è¿›è¡Œäº†æ–‡çŒ®ç»¼è¿°ã€‚æœ€åï¼Œæå‡ºäº†GRAPEï¼ˆå¹¿ä¹‰ç›¸å¯¹ä¼˜åŠ¿ç­–ç•¥æ¼”åŒ–ï¼‰å½¢å¼çš„ç ”ç©¶å’Œæ¢ç´¢çš„æ–°æ€è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æŒ‡ä»¤è°ƒä¼˜ç®—æ³•çš„è§£é‡Šå¾€å¾€å‡è®¾è¯»è€…å·²ç»å…·å¤‡ä¸€å®šçš„å¼ºåŒ–å­¦ä¹ åŸºç¡€ï¼Œå¹¶ä¸”åœ¨æè¿°ä¸Šå­˜åœ¨ä¸å¤Ÿå…·ä½“ã€è¿‡äºæŠ½è±¡çš„é—®é¢˜ï¼Œè¿™ä½¿å¾—åˆå­¦è€…éš¾ä»¥ç†è§£è¿™äº›ç®—æ³•çš„æœ¬è´¨å’Œåº”ç”¨åœºæ™¯ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œå¯èƒ½å­˜åœ¨æ•ˆç‡æˆ–æ•ˆæœä¸Šçš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä»é›¶å¼€å§‹ã€é€æ­¥æ¨å¯¼çš„æ–¹å¼ï¼Œè¯¦ç»†è§£é‡Šæ¯ç§ç®—æ³•çš„åŸç†å’Œå®ç°ç»†èŠ‚ã€‚åŒæ—¶ï¼Œå°†è¿™äº›ç®—æ³•ä¸LLMçš„å…·ä½“åº”ç”¨åœºæ™¯ç›¸ç»“åˆï¼Œé¿å…ä¸å¿…è¦çš„æŠ½è±¡å’Œæ³›åŒ–ï¼Œä»è€Œé™ä½å­¦ä¹ é—¨æ§›ï¼Œæé«˜ç†è§£æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†GRAPEï¼Œæ—¨åœ¨æ¢ç´¢æ›´æœ‰æ•ˆçš„ç­–ç•¥æ¼”åŒ–æ–¹æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡é¦–å…ˆå›é¡¾äº†SFTã€Rejection Samplingã€REINFORCEã€TRPOã€PPOã€GRPOå’ŒDPOç­‰ç»å…¸ç®—æ³•ã€‚ç„¶åï¼Œé’ˆå¯¹æ¯ç§ç®—æ³•ï¼Œé‡‡ç”¨ç®€åŒ–çš„ç¬¦å·å’Œæ˜¾å¼çš„å…¬å¼ï¼Œé€æ­¥æ¨å¯¼å…¶åŸç†å’Œå®ç°æ­¥éª¤ã€‚æœ€åï¼Œæå‡ºäº†GRAPEï¼Œå¹¶æ¢è®¨äº†å…¶æ½œåœ¨çš„ç ”ç©¶æ–¹å‘ã€‚æ•´ä½“æ¡†æ¶æ˜¯ï¼šç®—æ³•å›é¡¾ -> è¯¦ç»†è§£æ -> æ–°æ–¹æ³•æå‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä¸¤ä¸ªæ–¹é¢ï¼šä¸€æ˜¯æä¾›äº†ä¸€ç§æ¸…æ™°ã€æ˜“æ‡‚çš„æŒ‡ä»¤è°ƒä¼˜ç®—æ³•è§£é‡Šæ–¹æ³•ï¼Œé™ä½äº†å­¦ä¹ é—¨æ§›ï¼›äºŒæ˜¯æå‡ºäº†GRAPEï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚GRAPEçš„æ ¸å¿ƒæ€æƒ³æ˜¯å¹¿ä¹‰ç›¸å¯¹ä¼˜åŠ¿ç­–ç•¥æ¼”åŒ–ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°æ¢ç´¢å’Œä¼˜åŒ–ç­–ç•¥ç©ºé—´ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡åœ¨ç®—æ³•è§£ææ–¹é¢ï¼Œå…³é”®åœ¨äºç¬¦å·çš„ç®€åŒ–å’Œå…¬å¼çš„æ˜¾å¼åŒ–ï¼Œé¿å…ä½¿ç”¨è¿‡äºæŠ½è±¡çš„æ•°å­¦ç¬¦å·ï¼Œè€Œæ˜¯é‡‡ç”¨æ›´ç›´è§‚ã€æ›´æ˜“äºç†è§£çš„è¡¨ç¤ºæ–¹å¼ã€‚åœ¨GRAPEçš„è®¾è®¡æ–¹é¢ï¼Œå…·ä½“çš„æŠ€æœ¯ç»†èŠ‚æœªçŸ¥ï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³æ˜¯åŸºäºç›¸å¯¹ä¼˜åŠ¿çš„ç­–ç•¥æ¼”åŒ–ï¼Œå¯èƒ½æ¶‰åŠåˆ°æ–°çš„å¥–åŠ±å‡½æ•°è®¾è®¡ã€ç­–ç•¥æ¢¯åº¦ä¼°è®¡æ–¹æ³•æˆ–ä¼˜åŒ–ç®—æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç”±äºæ˜¯ç»¼è¿°å’Œæ–¹æ³•æå‡ºï¼Œæ²¡æœ‰å…·ä½“çš„å®éªŒç»“æœã€‚äº®ç‚¹åœ¨äºå¯¹ç°æœ‰æŒ‡ä»¤è°ƒä¼˜ç®—æ³•è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„æ¢³ç†å’Œæ¸…æ™°çš„è§£é‡Šï¼Œå¹¶æå‡ºäº†GRAPEè¿™ä¸€æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æ›´æ¸…æ™°åœ°ç†è§£å’Œåº”ç”¨è¿™äº›ç®—æ³•ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚GRAPEçš„æå‡ºä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ï¼Œæœ‰æœ›æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper provides a self-contained, from-scratch, exposition of key algorithms for instruction tuning of models: SFT, Rejection Sampling, REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct Preference Optimization (DPO). Explanations of these algorithms often assume prior knowledge, lack critical details, and/or are overly generalized and complex. Here, each method is discussed and developed step by step using simplified and explicit notation focused on LLMs, aiming to eliminate ambiguity and provide a clear and intuitive understanding of the concepts. By minimizing detours into the broader RL literature and connecting concepts to LLMs, we eliminate superfluous abstractions and reduce cognitive overhead. Following this exposition, we provide a literature review of new techniques and approaches beyond those detailed. Finally, new ideas for research and exploration in the form of GRAPE (Generalized Relative Advantage Policy Evolution) are presented.

