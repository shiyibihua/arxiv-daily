---
layout: default
title: TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding
---

# TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14671" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14671v1</a>
  <a href="https://arxiv.org/pdf/2509.14671.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14671v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14671v1', 'TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaobo Xing, Wei Yuan, Tong Chen, Quoc Viet Hung Nguyen, Xiangliang Zhang, Hongzhi Yin

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TableDARTï¼šæå‡ºåŠ¨æ€è‡ªé€‚åº”å¤šæ¨¡æ€è·¯ç”±æ¡†æ¶ï¼Œç”¨äºè¡¨æ ¼ç†è§£ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¡¨æ ¼ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `åŠ¨æ€è·¯ç”±` `çŸ¥è¯†é›†æˆ` `é¢„è®­ç»ƒæ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¡¨æ ¼ç†è§£æ–¹æ³•åœ¨å¤„ç†è¡¨æ ¼æ•°æ®çš„è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯æ—¶å­˜åœ¨ä¸è¶³ï¼Œå¦‚Table-as-Textä¸¢å¤±ç»“æ„ä¿¡æ¯ï¼ŒTable-as-Imageè¯­ä¹‰ç†è§£ä¸è¶³ã€‚
2. TableDARTé€šè¿‡åŠ¨æ€é€‰æ‹©æ–‡æœ¬ã€å›¾åƒæˆ–èåˆè·¯å¾„ï¼Œå¹¶åˆ©ç”¨agentè¿›è¡Œè·¨æ¨¡æ€çŸ¥è¯†é›†æˆï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTableDARTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„state-of-the-artæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¡¨æ ¼æ•°æ®çš„è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯å»ºæ¨¡æ˜¯æœ‰æ•ˆè¡¨æ ¼ç†è§£çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ç°æœ‰çš„Table-as-Textæ–¹æ³•å°†è¡¨æ ¼æ‰å¹³åŒ–ä»¥ä¾›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½¿ç”¨ï¼Œä½†ä¸¢å¤±äº†å…³é”®çš„ç»“æ„çº¿ç´¢ï¼›Table-as-Imageæ–¹æ³•ä¿ç•™äº†ç»“æ„ï¼Œä½†åœ¨ç»†ç²’åº¦è¯­ä¹‰æ–¹é¢è¡¨ç°ä¸ä½³ã€‚æœ€è¿‘çš„Table-as-Multimodalityç­–ç•¥è¯•å›¾ç»“åˆæ–‡æœ¬å’Œè§†è§‰è§†å›¾ï¼Œä½†å®ƒä»¬ï¼ˆ1ï¼‰é™æ€åœ°å¤„ç†æ¯ä¸ªæŸ¥è¯¢-è¡¨æ ¼å¯¹çš„ä¸¤ç§æ¨¡æ€ï¼Œä¸å¯é¿å…åœ°å¼•å…¥å†—ä½™ç”šè‡³å†²çªï¼Œå¹¶ä¸”ï¼ˆ2ï¼‰ä¾èµ–äºå¯¹MLLMè¿›è¡Œæ˜‚è´µçš„å¾®è°ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TableDARTï¼Œä¸€ä¸ªè®­ç»ƒé«˜æ•ˆçš„æ¡†æ¶ï¼Œé€šè¿‡é‡ç”¨é¢„è®­ç»ƒçš„å•æ¨¡æ€æ¨¡å‹æ¥é›†æˆå¤šæ¨¡æ€è§†å›¾ã€‚TableDARTå¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„259ä¸‡å‚æ•°MLPé—¨æ§ç½‘ç»œï¼Œå¯ä»¥ä¸ºæ¯ä¸ªè¡¨æ ¼-æŸ¥è¯¢å¯¹åŠ¨æ€é€‰æ‹©æœ€ä½³è·¯å¾„ï¼ˆä»…æ–‡æœ¬ã€ä»…å›¾åƒæˆ–èåˆï¼‰ï¼Œä»è€Œæœ‰æ•ˆåœ°å‡å°‘æ¥è‡ªä¸¤ç§æ¨¡æ€çš„å†—ä½™å’Œå†²çªã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„agentï¼Œé€šè¿‡åˆ†æåŸºäºæ–‡æœ¬å’Œå›¾åƒçš„æ¨¡å‹çš„è¾“å‡ºï¼Œæ¥åè°ƒè·¨æ¨¡æ€çŸ¥è¯†é›†æˆï¼Œå¯ä»¥é€‰æ‹©æœ€ä½³ç»“æœæˆ–é€šè¿‡æ¨ç†åˆæˆæ–°çš„ç­”æ¡ˆã€‚è¿™ç§è®¾è®¡é¿å…äº†å®Œå…¨MLLMå¾®è°ƒçš„è¿‡é«˜æˆæœ¬ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTableDARTåœ¨å¼€æºæ¨¡å‹ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹³å‡è¶…è¿‡æœ€å¼ºçš„åŸºçº¿4.02%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¡¨æ ¼ç†è§£ä»»åŠ¡ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆèåˆè¡¨æ ¼çš„æ–‡æœ¬å’Œå›¾åƒä¿¡æ¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆæŸå¤±ç»“æ„ä¿¡æ¯ï¼ˆTable-as-Textï¼‰ï¼Œè¦ä¹ˆéš¾ä»¥æ•æ‰ç»†ç²’åº¦è¯­ä¹‰ï¼ˆTable-as-Imageï¼‰ï¼Œè€Œå¤šæ¨¡æ€æ–¹æ³•åˆå­˜åœ¨è®¡ç®—å†—ä½™å’Œéœ€è¦å¤§é‡å¾®è°ƒçš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åŠ¨æ€åœ°ã€è‡ªé€‚åº”åœ°é€‰æ‹©æœ€é€‚åˆå½“å‰è¡¨æ ¼-æŸ¥è¯¢å¯¹çš„æ¨¡æ€è·¯å¾„ï¼ˆæ–‡æœ¬ã€å›¾åƒæˆ–èåˆï¼‰ï¼Œå¹¶åˆ©ç”¨ä¸€ä¸ªagentæ¥åè°ƒè·¨æ¨¡æ€çŸ¥è¯†çš„é›†æˆã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å‡å°‘å†—ä½™è®¡ç®—ï¼Œé¿å…æ¨¡æ€å†²çªï¼Œå¹¶é™ä½å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œæ˜‚è´µå¾®è°ƒçš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTableDARTæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) æ–‡æœ¬ç¼–ç å™¨ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ç¼–ç è¡¨æ ¼æ–‡æœ¬ï¼›2) å›¾åƒç¼–ç å™¨ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ç¼–ç è¡¨æ ¼å›¾åƒï¼›3) é—¨æ§ç½‘ç»œï¼šä¸€ä¸ªè½»é‡çº§çš„MLPï¼Œæ ¹æ®è¡¨æ ¼-æŸ¥è¯¢å¯¹çš„ç‰¹å¾åŠ¨æ€é€‰æ‹©æœ€ä½³æ¨¡æ€è·¯å¾„ï¼›4) è·¨æ¨¡æ€çŸ¥è¯†é›†æˆAgentï¼šåˆ†ææ–‡æœ¬å’Œå›¾åƒæ¨¡å‹çš„è¾“å‡ºï¼Œé€‰æ‹©æœ€ä½³ç»“æœæˆ–åˆæˆæ–°çš„ç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šTableDARTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŠ¨æ€è‡ªé€‚åº”çš„å¤šæ¨¡æ€è·¯ç”±æœºåˆ¶å’Œè·¨æ¨¡æ€çŸ¥è¯†é›†æˆAgentã€‚åŠ¨æ€è·¯ç”±å…è®¸æ¨¡å‹æ ¹æ®è¾“å…¥è‡ªé€‚åº”åœ°é€‰æ‹©æœ€åˆé€‚çš„æ¨¡æ€ï¼Œé¿å…äº†é™æ€å¤„ç†æ‰€æœ‰æ¨¡æ€çš„å†—ä½™å’Œå†²çªã€‚è·¨æ¨¡æ€çŸ¥è¯†é›†æˆAgentåˆ™èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆæ¥è‡ªä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼Œæå‡æœ€ç»ˆçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šé—¨æ§ç½‘ç»œæ˜¯ä¸€ä¸ª2.59Må‚æ•°çš„MLPï¼Œè¾“å…¥æ˜¯è¡¨æ ¼å’ŒæŸ¥è¯¢çš„ç‰¹å¾å‘é‡ï¼Œè¾“å‡ºæ˜¯é€‰æ‹©ä¸åŒæ¨¡æ€è·¯å¾„çš„æ¦‚ç‡ã€‚è·¨æ¨¡æ€çŸ¥è¯†é›†æˆAgentä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–é¢„æµ‹å‡†ç¡®ç‡ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬äº¤å‰ç†µæŸå¤±å’Œå¼ºåŒ–å­¦ä¹ å¥–åŠ±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TableDARTåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„state-of-the-artæ€§èƒ½ï¼Œå¹³å‡è¶…è¿‡æœ€å¼ºçš„åŸºçº¿4.02%ã€‚ å®éªŒç»“æœè¡¨æ˜ï¼ŒTableDARTèƒ½å¤Ÿæœ‰æ•ˆåœ°èåˆè¡¨æ ¼çš„æ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ï¼Œå¹¶æ˜¾è‘—æå‡è¡¨æ ¼ç†è§£çš„å‡†ç¡®æ€§ã€‚ æ­¤å¤–ï¼ŒTableDARTçš„è®­ç»ƒæ•ˆç‡é«˜ï¼Œé¿å…äº†å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œæ˜‚è´µçš„å¾®è°ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TableDARTå¯åº”ç”¨äºå„ç§éœ€è¦ç†è§£è¡¨æ ¼æ•°æ®çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½é—®ç­”ã€æ•°æ®åˆ†æã€æŠ¥å‘Šç”Ÿæˆç­‰ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæå‡æœºå™¨å¯¹è¡¨æ ¼æ•°æ®çš„ç†è§£èƒ½åŠ›ï¼Œä»è€Œåœ¨é‡‘èã€åŒ»ç–—ã€æ•™è‚²ç­‰é¢†åŸŸå®ç°æ›´æ™ºèƒ½åŒ–çš„åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤„ç†æ›´å¤æ‚çš„è¡¨æ ¼ç»“æ„å’Œæ›´å¤šæ¨¡æ€çš„æ•°æ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modeling semantic and structural information from tabular data remains a core challenge for effective table understanding. Existing Table-as-Text approaches flatten tables for large language models (LLMs), but lose crucial structural cues, while Table-as-Image methods preserve structure yet struggle with fine-grained semantics. Recent Table-as-Multimodality strategies attempt to combine textual and visual views, but they (1) statically process both modalities for every query-table pair within a large multimodal LLMs (MLLMs), inevitably introducing redundancy and even conflicts, and (2) depend on costly fine-tuning of MLLMs. In light of this, we propose TableDART, a training-efficient framework that integrates multimodal views by reusing pretrained single-modality models. TableDART introduces a lightweight 2.59M-parameter MLP gating network that dynamically selects the optimal path (either Text-only, Image-only, or Fusion) for each table-query pair, effectively reducing redundancy and conflicts from both modalities. In addition, we propose a novel agent to mediate cross-modal knowledge integration by analyzing outputs from text- and image-based models, either selecting the best result or synthesizing a new answer through reasoning. This design avoids the prohibitive costs of full MLLM fine-tuning. Extensive experiments on seven benchmarks show that TableDART establishes new state-of-the-art performance among open-source models, surpassing the strongest baseline by an average of 4.02%. The code is available at: https://anonymous.4open.science/r/TableDART-C52B

