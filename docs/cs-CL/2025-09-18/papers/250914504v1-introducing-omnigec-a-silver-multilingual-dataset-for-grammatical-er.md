---
layout: default
title: Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction
---

# Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14504" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14504v1</a>
  <a href="https://arxiv.org/pdf/2509.14504.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14504v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14504v1', 'Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Roman Kovalchuk, Mariana Romanyshyn, Petro Ivaniuk

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**æœŸåˆŠ**: Proceedings of the Fourth Ukrainian Natural Language Processing Workshop (UNLP 2025)

**DOI**: [10.18653/v1/2025.unlp-1](https://doi.org/10.18653/v1/2025.unlp-1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniGECï¼šæå‡ºå¤šè¯­è¨€è¯­æ³•çº é”™çš„é“¶æ ‡å‡†æ•°æ®é›†ï¼Œä¿ƒè¿›è·¨è¯­è¨€GECæ¨¡å‹å‘å±•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­æ³•çº é”™` `å¤šè¯­è¨€` `æ•°æ®é›†` `å¤§å‹è¯­è¨€æ¨¡å‹` `é“¶æ ‡å‡†æ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­æ³•çº é”™ï¼ˆGECï¼‰æ–¹æ³•åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹é¢ä¸´æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œé˜»ç¢äº†è·¨è¯­è¨€GECæ¨¡å‹çš„å‘å±•ã€‚
2. OmniGECé€šè¿‡æ”¶é›†å’Œå¤„ç†å¤šç§æ¥æºçš„å¤šè¯­è¨€æ•°æ®ï¼Œæ„å»ºäº†ä¸€ä¸ªé“¶æ ‡å‡†æ•°æ®é›†ï¼Œä¸ºå¤šè¯­è¨€GECæä¾›äº†å®è´µèµ„æºã€‚
3. é€šè¿‡åœ¨OmniGECä¸Šå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒAya-Expanseå’ŒGemma-3åœ¨æ®µè½çº§å¤šè¯­è¨€GECä»»åŠ¡ä¸Šå–å¾—äº†SOTAç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†OmniGECï¼Œä¸€ä¸ªç”¨äºè¯­æ³•çº é”™ï¼ˆGECï¼‰çš„å¤šè¯­è¨€é“¶æ ‡å‡†æ•°æ®é›†é›†åˆï¼Œæ¶µç›–11ç§è¯­è¨€ï¼šæ·å…‹è¯­ã€è‹±è¯­ã€çˆ±æ²™å°¼äºšè¯­ã€å¾·è¯­ã€å¸Œè…Šè¯­ã€å†°å²›è¯­ã€æ„å¤§åˆ©è¯­ã€æ‹‰è„±ç»´äºšè¯­ã€æ–¯æ´›æ–‡å°¼äºšè¯­ã€ç‘å…¸è¯­å’Œä¹Œå…‹å…°è¯­ã€‚è¿™äº›æ•°æ®é›†æœ‰åŠ©äºå¼€å‘å¤šè¯­è¨€GECè§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼¥åˆäº†å°†è‹±è¯­GECè§£å†³æ–¹æ¡ˆé€‚é…åˆ°å¤šè¯­è¨€GECä¸­çš„æ•°æ®ç¼ºå£ã€‚æ•°æ®é›†ä¸­çš„æ–‡æœ¬æ¥æºäºä¸‰ä¸ªæ–¹é¢ï¼š11ç§ç›®æ ‡è¯­è¨€çš„ç»´åŸºç™¾ç§‘ç¼–è¾‘ã€Redditçš„å­ç‰ˆå—ä»¥åŠä»…é™ä¹Œå…‹å…°è¯­çš„UberText 2.0ç¤¾äº¤åª’ä½“è¯­æ–™åº“ã€‚ç»´åŸºç™¾ç§‘ç¼–è¾‘æºäºäººå·¥æ ¡æ­£ï¼Œè€ŒRedditå’ŒUberText 2.0æ•°æ®åˆ™ä½¿ç”¨GPT-4o-miniæ¨¡å‹è‡ªåŠ¨æ ¡æ­£ã€‚å¯¹æ•°æ®é›†ä¸­æ ¡æ­£çš„è´¨é‡è¿›è¡Œäº†è‡ªåŠ¨å’Œæ‰‹åŠ¨è¯„ä¼°ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å¤šè¯­è¨€OmniGECè¯­æ–™åº“ä¸Šå¾®è°ƒäº†ä¸¤ä¸ªå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹â€”â€”Aya-Expanse (8B) å’Œ Gemma-3 (12B)ï¼Œå¹¶åœ¨æ®µè½çº§å¤šè¯­è¨€GECä¸­å–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„ç»“æœã€‚æ•°æ®é›†é›†åˆå’Œæ€§èƒ½æœ€ä½³çš„æ¨¡å‹å¯åœ¨Hugging Faceä¸Šè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€è¯­æ³•çº é”™ï¼ˆGECï¼‰é¢†åŸŸæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ç°æœ‰çš„GECç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä¸Šï¼Œç¼ºä¹é«˜è´¨é‡çš„å¤šè¯­è¨€æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†è·¨è¯­è¨€GECæ¨¡å‹çš„å‘å±•å’Œåº”ç”¨ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥ç›´æ¥è¿ç§»åˆ°å…¶ä»–è¯­è¨€ï¼Œä¸”åœ¨å¤„ç†ä¸åŒè¯­è¨€çš„è¯­æ³•é”™è¯¯æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤šè¯­è¨€é“¶æ ‡å‡†æ•°æ®é›†ï¼Œé€šè¿‡ç»“åˆäººå·¥æ ¡æ­£å’Œè‡ªåŠ¨æ ¡æ­£çš„æ–¹æ³•ï¼Œç”ŸæˆåŒ…å«å¤šç§è¯­è¨€çš„GECè®­ç»ƒæ•°æ®ã€‚åˆ©ç”¨è¿™äº›æ•°æ®ï¼Œå¯ä»¥è®­ç»ƒå’Œå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶å…·å¤‡å¤šè¯­è¨€GECèƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å¼¥åˆæ•°æ®ç¼ºå£ï¼Œä¿ƒè¿›è·¨è¯­è¨€GECæŠ€æœ¯çš„å‘å±•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniGECçš„æ„å»ºæµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ï¼šä»ç»´åŸºç™¾ç§‘ç¼–è¾‘ã€Redditå­ç‰ˆå—å’ŒUberText 2.0ç¤¾äº¤åª’ä½“è¯­æ–™åº“ä¸­æ”¶é›†11ç§è¯­è¨€çš„æ–‡æœ¬æ•°æ®ã€‚2) æ•°æ®æ ¡æ­£ï¼šå¯¹äºç»´åŸºç™¾ç§‘ç¼–è¾‘ï¼Œåˆ©ç”¨äººå·¥æ ¡æ­£æ•°æ®ï¼›å¯¹äºRedditå’ŒUberText 2.0æ•°æ®ï¼Œä½¿ç”¨GPT-4o-miniæ¨¡å‹è¿›è¡Œè‡ªåŠ¨æ ¡æ­£ã€‚3) æ•°æ®è¯„ä¼°ï¼šé€šè¿‡è‡ªåŠ¨æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°ï¼Œè¯„ä¼°æ ¡æ­£æ•°æ®çš„è´¨é‡ã€‚4) æ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨OmniGECæ•°æ®é›†å¾®è°ƒAya-Expanse (8B) å’Œ Gemma-3 (12B) ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªå¤šè¯­è¨€é“¶æ ‡å‡†GECæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº†11ç§è¯­è¨€ï¼Œå¹¶ç»“åˆäº†äººå·¥æ ¡æ­£å’Œè‡ªåŠ¨æ ¡æ­£çš„æ–¹æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOmniGECæä¾›äº†æ›´ä¸°å¯Œã€æ›´å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ï¼Œæœ‰åŠ©äºæé«˜å¤šè¯­è¨€GECæ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é€šè¿‡å®éªŒéªŒè¯äº†åœ¨OmniGECä¸Šå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥å–å¾—SOTAç»“æœã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•°æ®æ¥æºçš„é€‰æ‹©ï¼šé€‰æ‹©ç»´åŸºç™¾ç§‘ã€Redditå’ŒUberText 2.0ç­‰ä¸åŒæ¥æºçš„æ•°æ®ï¼Œä»¥å¢åŠ æ•°æ®çš„å¤šæ ·æ€§ã€‚2) è‡ªåŠ¨æ ¡æ­£æ¨¡å‹ï¼šä½¿ç”¨GPT-4o-miniæ¨¡å‹è¿›è¡Œè‡ªåŠ¨æ ¡æ­£ï¼Œè¯¥æ¨¡å‹å…·æœ‰è¾ƒå¼ºçš„æ–‡æœ¬ç”Ÿæˆå’Œçº é”™èƒ½åŠ›ã€‚3) æ¨¡å‹å¾®è°ƒï¼šä½¿ç”¨Aya-Expanse (8B) å’Œ Gemma-3 (12B) ç­‰å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚4) è¯„ä¼°æŒ‡æ ‡ï¼šé‡‡ç”¨è‡ªåŠ¨æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œå…¨é¢è¯„ä¼°æ ¡æ­£æ•°æ®çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨OmniGECæ•°æ®é›†ä¸Šå¾®è°ƒçš„Aya-Expanse (8B) å’Œ Gemma-3 (12B) æ¨¡å‹åœ¨æ®µè½çº§å¤šè¯­è¨€GECä»»åŠ¡ä¸Šå–å¾—äº†SOTAç»“æœã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œä½†å¼ºè°ƒäº†ç›¸å¯¹äºç°æœ‰æ–¹æ³•çš„æ˜¾è‘—æå‡ã€‚æ•°æ®é›†å’Œæ¨¡å‹å·²åœ¨Hugging Faceä¸Šå‘å¸ƒï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤šè¯­è¨€æ–‡æœ¬å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€è¯­è¨€å­¦ä¹ ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºæé«˜æœºå™¨ç¿»è¯‘çš„è´¨é‡ï¼Œè¾…åŠ©è¯­è¨€å­¦ä¹ è€…è¿›è¡Œè¯­æ³•çº é”™ï¼Œä»¥åŠæå‡å¤šè¯­è¨€ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸çš„å‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥æ•°æ®é›†å¯ä»¥æ‰©å±•åˆ°æ›´å¤šè¯­è¨€ï¼Œå¹¶ä¸å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æ¨åŠ¨å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we introduce OmniGEC, a collection of multilingual silver-standard datasets for the task of Grammatical Error Correction (GEC), covering eleven languages: Czech, English, Estonian, German, Greek, Icelandic, Italian, Latvian, Slovene, Swedish, and Ukrainian. These datasets facilitate the development of multilingual GEC solutions and help bridge the data gap in adapting English GEC solutions to multilingual GEC. The texts in the datasets originate from three sources: Wikipedia edits for the eleven target languages, subreddits from Reddit in the eleven target languages, and the Ukrainian-only UberText 2.0 social media corpus. While Wikipedia edits were derived from human-made corrections, the Reddit and UberText 2.0 data were automatically corrected with the GPT-4o-mini model. The quality of the corrections in the datasets was evaluated both automatically and manually. Finally, we fine-tune two open-source large language models - Aya-Expanse (8B) and Gemma-3 (12B) - on the multilingual OmniGEC corpora and achieve state-of-the-art (SOTA) results for paragraph-level multilingual GEC. The dataset collection and the best-performing models are available on Hugging Face.

