---
layout: default
title: LLM-Assisted Topic Reduction for BERTopic on Social Media Data
---

# LLM-Assisted Topic Reduction for BERTopic on Social Media Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19365" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19365v1</a>
  <a href="https://arxiv.org/pdf/2509.19365.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19365v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19365v1', 'LLM-Assisted Topic Reduction for BERTopic on Social Media Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wannes Janssens, Matthias Bogaert, Dirk Van den Poel

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: 13 pages, 8 figures. To be published in the Post-Workshop proceedings of the ECML PKDD 2025 Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMè¾…åŠ©çš„BERTopicä¸»é¢˜é™ç»´æ–¹æ³•ï¼Œæå‡ç¤¾äº¤åª’ä½“æ•°æ®ä¸»é¢˜å»ºæ¨¡æ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `BERTopic` `ä¸»é¢˜å»ºæ¨¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸»é¢˜é™ç»´` `ç¤¾äº¤åª’ä½“æ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¤¾äº¤åª’ä½“æ•°æ®å™ªå£°å¤§ã€ç¨€ç–ï¼Œå¯¼è‡´BERTopicä¸»é¢˜å»ºæ¨¡äº§ç”Ÿè¿‡å¤šé‡å ä¸»é¢˜ï¼Œæ•ˆæœä¸ä½³ã€‚
2. åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸»é¢˜é™ç»´ï¼Œåˆå¹¶è¯­ä¹‰ç›¸ä¼¼çš„ä¸»é¢˜ï¼Œæå‡ä¸»é¢˜å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Twitter/Xæ•°æ®é›†ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä½†å—æ•°æ®é›†ç‰¹å¾å’Œå‚æ•°å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

BERTopicæ¡†æ¶åˆ©ç”¨TransformeråµŒå…¥å’Œå±‚æ¬¡èšç±»ä»éç»“æ„åŒ–æ–‡æœ¬è¯­æ–™åº“ä¸­æå–æ½œåœ¨ä¸»é¢˜ã€‚ç„¶è€Œï¼Œå®ƒåœ¨å¤„ç†ç¤¾äº¤åª’ä½“æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºç¤¾äº¤åª’ä½“æ•°æ®é€šå¸¸å™ªå£°å¤§ä¸”ç¨€ç–ï¼Œå¯¼è‡´äº§ç”Ÿè¿‡å¤šçš„é‡å ä¸»é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯ä¸»é¢˜å»ºæ¨¡ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—å¼€é”€ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§æ•°æ®ç¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆBERTopicè¿›è¡Œä¸»é¢˜ç”Ÿæˆå’Œå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸»é¢˜é™ç»´çš„æ¡†æ¶ã€‚è¯¥æ–¹æ³•é¦–å…ˆç”Ÿæˆä¸€ç»„åˆå§‹ä¸»é¢˜ï¼Œå¹¶æ„å»ºæ¯ä¸ªä¸»é¢˜çš„è¡¨ç¤ºã€‚ç„¶åï¼Œå°†è¿™äº›è¡¨ç¤ºä½œä¸ºè¾“å…¥æä¾›ç»™è¯­è¨€æ¨¡å‹ï¼Œè¯­è¨€æ¨¡å‹è¿­ä»£åœ°è¯†åˆ«å’Œåˆå¹¶è¯­ä¹‰ç›¸ä¼¼çš„ä¸»é¢˜ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªTwitter/Xæ•°æ®é›†å’Œå››ä¸ªä¸åŒçš„è¯­è¨€æ¨¡å‹ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜ä¸»é¢˜å¤šæ ·æ€§æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨è®¸å¤šæƒ…å†µä¸‹ä¹Ÿæé«˜äº†ä¸»é¢˜ä¸€è‡´æ€§ï¼Œä½†å¯¹æ•°æ®é›†ç‰¹å¾å’Œåˆå§‹å‚æ•°é€‰æ‹©å…·æœ‰ä¸€å®šçš„æ•æ„Ÿæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šBERTopicåœ¨å¤„ç†ç¤¾äº¤åª’ä½“æ•°æ®æ—¶ï¼Œç”±äºæ•°æ®æœ¬èº«çš„å™ªå£°å’Œç¨€ç–æ€§ï¼Œå®¹æ˜“äº§ç”Ÿå¤§é‡å†—ä½™å’Œé‡å çš„ä¸»é¢˜ã€‚è¿™äº›ä¸»é¢˜ä¸ä»…éš¾ä»¥è§£é‡Šï¼Œä¹Ÿé™ä½äº†ä¸»é¢˜å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯ä¸»é¢˜å»ºæ¨¡ï¼Œè®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œéš¾ä»¥åº”ç”¨äºå¤§è§„æ¨¡ç¤¾äº¤åª’ä½“æ•°æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†BERTopicå’Œå¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆèµ·æ¥ã€‚é¦–å…ˆä½¿ç”¨BERTopicç”Ÿæˆåˆå§‹ä¸»é¢˜ï¼Œç„¶ååˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œè¯†åˆ«å¹¶åˆå¹¶è¯­ä¹‰ç›¸ä¼¼çš„ä¸»é¢˜ï¼Œä»è€Œå‡å°‘ä¸»é¢˜æ•°é‡ï¼Œæé«˜ä¸»é¢˜çš„å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨åœ¨è®¡ç®—æ•ˆç‡å’Œä¸»é¢˜è´¨é‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šä¸»é¢˜ç”Ÿæˆå’Œä¸»é¢˜é™ç»´ã€‚åœ¨ä¸»é¢˜ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨BERTopicä»æ–‡æœ¬æ•°æ®ä¸­æå–åˆå§‹ä¸»é¢˜ã€‚åœ¨ä¸»é¢˜é™ç»´é˜¶æ®µï¼Œé¦–å…ˆä¸ºæ¯ä¸ªä¸»é¢˜æ„å»ºè¡¨ç¤ºï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ä¸»é¢˜å…³é”®è¯çš„åµŒå…¥å‘é‡ï¼‰ï¼Œç„¶åå°†è¿™äº›è¡¨ç¤ºè¾“å…¥åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡è¿­ä»£åœ°è¯†åˆ«å’Œåˆå¹¶è¯­ä¹‰ç›¸ä¼¼çš„ä¸»é¢˜ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ç»„æ›´ç²¾ç®€ã€æ›´å…·ä»£è¡¨æ€§çš„ä¸»é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸»é¢˜é™ç»´ï¼Œè€Œä¸æ˜¯ç›´æ¥è¿›è¡Œç«¯åˆ°ç«¯çš„ä¸»é¢˜å»ºæ¨¡ã€‚è¿™ç§æ–¹æ³•å……åˆ†åˆ©ç”¨äº†BERTopicåœ¨ä¸»é¢˜ç”Ÿæˆæ–¹é¢çš„æ•ˆç‡ï¼ŒåŒæ—¶å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›æ¥æé«˜ä¸»é¢˜è´¨é‡ã€‚é€šè¿‡è§£è€¦ä¸»é¢˜ç”Ÿæˆå’Œé™ç»´è¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•å¯ä»¥çµæ´»åœ°é€‰æ‹©ä¸åŒçš„BERTopicå‚æ•°å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥é€‚åº”ä¸åŒçš„æ•°æ®é›†å’Œè®¡ç®—èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¸»é¢˜é™ç»´é˜¶æ®µï¼Œå…³é”®çš„è®¾è®¡åŒ…æ‹¬å¦‚ä½•æ„å»ºä¸»é¢˜çš„è¡¨ç¤ºï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯†åˆ«å’Œåˆå¹¶ç›¸ä¼¼çš„ä¸»é¢˜ã€‚è®ºæ–‡å¯èƒ½æ¢ç´¢äº†ä¸åŒçš„ä¸»é¢˜è¡¨ç¤ºæ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºå…³é”®è¯åµŒå…¥çš„å¹³å‡æˆ–åŠ æƒå¹³å‡ï¼‰ï¼Œä»¥åŠä¸åŒçš„ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ã€‚æ­¤å¤–ï¼Œå¦‚ä½•è®¾ç½®åˆå¹¶é˜ˆå€¼ï¼Œä»¥åŠå¦‚ä½•è¿­ä»£åœ°è¿›è¡Œä¸»é¢˜åˆå¹¶ï¼Œä¹Ÿæ˜¯å½±å“æœ€ç»ˆç»“æœçš„å…³é”®å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªTwitter/Xæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ä¸åŒçš„è¯­è¨€æ¨¡å‹ï¼Œå‡èƒ½æœ‰æ•ˆæé«˜ä¸»é¢˜å¤šæ ·æ€§ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸»é¢˜ä¸€è‡´æ€§ä¹Ÿå¾—åˆ°äº†æå‡ã€‚ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ›´å°‘ä½†æ›´å…·ä»£è¡¨æ€§çš„ä¸»é¢˜ï¼Œä»è€Œæ›´å¥½åœ°æ¦‚æ‹¬ç¤¾äº¤åª’ä½“æ•°æ®çš„æ ¸å¿ƒå†…å®¹ã€‚å…·ä½“æå‡å¹…åº¦å–å†³äºæ•°æ®é›†çš„ç‰¹å¾å’Œåˆå§‹å‚æ•°çš„é€‰æ‹©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç¤¾äº¤åª’ä½“èˆ†æƒ…åˆ†æã€ç”¨æˆ·å…´è¶£æŒ–æ˜ã€æ–°é—»ä¸»é¢˜åˆ†ç±»ç­‰é¢†åŸŸã€‚é€šè¿‡æ›´å‡†ç¡®ã€æ›´ç®€æ´çš„ä¸»é¢˜å»ºæ¨¡ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£ç¤¾äº¤åª’ä½“ä¸Šçš„ä¿¡æ¯ä¼ æ’­æ¨¡å¼ã€ç”¨æˆ·è¡Œä¸ºå’Œçƒ­ç‚¹è¯é¢˜ï¼Œä¸ºä¼ä¸šå†³ç­–ã€æ”¿ç­–åˆ¶å®šå’Œèˆ†è®ºå¼•å¯¼æä¾›æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ–‡æœ¬æ•°æ®ï¼Œå¦‚å®¢æˆ·è¯„è®ºã€åœ¨çº¿è®ºå›ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The BERTopic framework leverages transformer embeddings and hierarchical clustering to extract latent topics from unstructured text corpora. While effective, it often struggles with social media data, which tends to be noisy and sparse, resulting in an excessive number of overlapping topics. Recent work explored the use of large language models for end-to-end topic modelling. However, these approaches typically require significant computational overhead, limiting their scalability in big data contexts. In this work, we propose a framework that combines BERTopic for topic generation with large language models for topic reduction. The method first generates an initial set of topics and constructs a representation for each. These representations are then provided as input to the language model, which iteratively identifies and merges semantically similar topics. We evaluate the approach across three Twitter/X datasets and four different language models. Our method outperforms the baseline approach in enhancing topic diversity and, in many cases, coherence, with some sensitivity to dataset characteristics and initial parameter selection.

