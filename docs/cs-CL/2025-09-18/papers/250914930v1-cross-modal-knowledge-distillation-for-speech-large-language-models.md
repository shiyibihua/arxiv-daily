---
layout: default
title: Cross-Modal Knowledge Distillation for Speech Large Language Models
---

# Cross-Modal Knowledge Distillation for Speech Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14930" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14930v1</a>
  <a href="https://arxiv.org/pdf/2509.14930.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14930v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14930v1', 'Cross-Modal Knowledge Distillation for Speech Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Enzhi Wang, Qicheng Li, Zhiyuan Tang, Yuhang Jia

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œè§£å†³è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç¾éš¾æ€§é—å¿˜å’Œæ¨¡æ€ä¸ç­‰ä»·é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹` `è·¨æ¨¡æ€å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `ç¾éš¾æ€§é—å¿˜` `æ¨¡æ€ä¸ç­‰ä»·` `è¯­éŸ³ç†è§£` `æ–‡æœ¬ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹åœ¨å¼•å…¥è¯­éŸ³èƒ½åŠ›åï¼Œå³ä½¿æ˜¯æ–‡æœ¬è¾“å…¥ï¼Œå…¶çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¹Ÿä¼šä¸‹é™ï¼Œè¯­éŸ³æŸ¥è¯¢æ—¶æ€§èƒ½æ›´å·®ã€‚
2. æå‡ºè·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°æ–‡æœ¬å’Œè¯­éŸ³åˆ°æ–‡æœ¬é€šé“ï¼Œå°†çŸ¥è¯†ä»æ–‡æœ¬æ•™å¸ˆæ¨¡å‹è¿ç§»åˆ°è¯­éŸ³LLMã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆä¿ç•™æ–‡æœ¬çŸ¥è¯†ï¼Œæ”¹å–„è·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶æå‡è¯­éŸ³äº¤äº’ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„ç¾éš¾æ€§é—å¿˜å’Œæ¨¡æ€ä¸ç­‰ä»·é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¼•å…¥è¯­éŸ³èƒ½åŠ›ä¼šé™ä½æ¨¡å‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå³ä½¿è¾“å…¥ä»ç„¶æ˜¯æ–‡æœ¬å½¢å¼ï¼Œå¹¶ä¸”å½“ä½¿ç”¨è¯­éŸ³æŸ¥è¯¢æ—¶ï¼Œæ€§èƒ½ä¼šè¿›ä¸€æ­¥ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ–‡æœ¬åˆ°æ–‡æœ¬å’Œè¯­éŸ³åˆ°æ–‡æœ¬ä¸¤ç§é€šé“ï¼Œå°†çŸ¥è¯†ä»åŸºäºæ–‡æœ¬çš„æ•™å¸ˆæ¨¡å‹è¿ç§»åˆ°è¯­éŸ³LLMã€‚åœ¨å¯¹è¯å’ŒéŸ³é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨ä¿ç•™æ–‡æœ¬çŸ¥è¯†ã€æ”¹å–„è·¨æ¨¡æ€å¯¹é½ä»¥åŠå¢å¼ºåŸºäºè¯­éŸ³äº¤äº’çš„æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè¯­éŸ³å¤§è¯­è¨€æ¨¡å‹åœ¨èåˆè¯­éŸ³èƒ½åŠ›æ—¶ï¼Œé¢ä¸´ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå³åœ¨å­¦ä¹ è¯­éŸ³ç›¸å…³ä»»åŠ¡åï¼Œæ¨¡å‹åœ¨åŸæœ‰æ–‡æœ¬ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œè¿˜å­˜åœ¨æ¨¡æ€ä¸ç­‰ä»·é—®é¢˜ï¼Œå³å¯¹äºç›¸åŒè¯­ä¹‰çš„æ–‡æœ¬å’Œè¯­éŸ³è¾“å…¥ï¼Œæ¨¡å‹è¡¨ç°å‡ºä¸ä¸€è‡´çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ï¼Œå¯¼è‡´è¯­éŸ³LLMçš„å®ç”¨æ€§å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çŸ¥è¯†è’¸é¦ï¼Œå°†ä¸€ä¸ªé¢„è®­ç»ƒçš„ã€å…·æœ‰å¼ºå¤§æ–‡æœ¬ç†è§£èƒ½åŠ›çš„æ•™å¸ˆæ¨¡å‹ä¸­çš„çŸ¥è¯†è¿ç§»åˆ°è¯­éŸ³LLMä¸­ã€‚é€šè¿‡è·¨æ¨¡æ€çš„çŸ¥è¯†è¿ç§»ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶æå‡è¯­éŸ³LLMçš„è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªæ–‡æœ¬æ•™å¸ˆæ¨¡å‹å’Œä¸€ä¸ªè¯­éŸ³å­¦ç”Ÿæ¨¡å‹ã€‚æ•™å¸ˆæ¨¡å‹æ¥æ”¶æ–‡æœ¬è¾“å…¥ï¼Œå­¦ç”Ÿæ¨¡å‹åŒæ—¶æ¥æ”¶æ–‡æœ¬å’Œè¯­éŸ³è¾“å…¥ã€‚æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦çš„è’¸é¦é€šé“ï¼šæ–‡æœ¬åˆ°æ–‡æœ¬è’¸é¦å’Œè¯­éŸ³åˆ°æ–‡æœ¬è’¸é¦ã€‚æ–‡æœ¬åˆ°æ–‡æœ¬è’¸é¦ç”¨äºä¿ç•™å­¦ç”Ÿæ¨¡å‹åœ¨æ–‡æœ¬ä»»åŠ¡ä¸Šçš„çŸ¥è¯†ï¼Œè¯­éŸ³åˆ°æ–‡æœ¬è’¸é¦ç”¨äºæå‡å­¦ç”Ÿæ¨¡å‹å¯¹è¯­éŸ³è¾“å…¥çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è·¨æ¨¡æ€çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œå°†æ–‡æœ¬æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†åŒæ—¶é€šè¿‡æ–‡æœ¬å’Œè¯­éŸ³ä¸¤ç§æ¨¡æ€è¿ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¯ä»¥ç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼Œè¿˜å¯ä»¥æå‡è¯­éŸ³LLMçš„è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ï¼Œä½¿å…¶æ›´å¥½åœ°ç†è§£å’Œå¤„ç†è¯­éŸ³è¾“å…¥ã€‚ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•å……åˆ†åˆ©ç”¨äº†è¯­éŸ³æ¨¡æ€çš„ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°æå‡äº†è¯­éŸ³LLMçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬è’¸é¦ä¸­ï¼Œä½¿ç”¨KLæ•£åº¦æŸå¤±æ¥è¡¡é‡æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹åœ¨æ–‡æœ¬è¾“å‡ºä¸Šçš„å·®å¼‚ã€‚åœ¨è¯­éŸ³åˆ°æ–‡æœ¬è’¸é¦ä¸­ï¼Œé¦–å…ˆå°†è¯­éŸ³è¾“å…¥è½¬æ¢ä¸ºæ–‡æœ¬è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨KLæ•£åº¦æŸå¤±æ¥è¡¡é‡æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹åœ¨æ–‡æœ¬è¾“å‡ºä¸Šçš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œç”¨äºè¿›ä¸€æ­¥æå‡å­¦ç”Ÿæ¨¡å‹å¯¹è¯­éŸ³è¾“å…¥çš„ç†è§£èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æå‡è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨å¯¹è¯å’ŒéŸ³é¢‘ç†è§£ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™æ–‡æœ¬çŸ¥è¯†ã€æ”¹å–„è·¨æ¨¡æ€å¯¹é½ä»¥åŠå¢å¼ºåŸºäºè¯­éŸ³äº¤äº’çš„æ¨ç†èƒ½åŠ›æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€è¯­éŸ³æœç´¢ã€è¯­éŸ³å¯¹è¯ç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶ã€æ›´é«˜æ•ˆçš„äººæœºäº¤äº’ï¼Œä¸ºç”¨æˆ·æä¾›æ›´æ™ºèƒ½åŒ–çš„æœåŠ¡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ•™è‚²ã€åŒ»ç–—ã€é‡‘èç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.

