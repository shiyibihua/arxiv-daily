---
layout: default
title: Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding
---

# Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15476" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15476v1</a>
  <a href="https://arxiv.org/pdf/2509.15476.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15476v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15476v1', 'Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhu Li, Xiyuan Gao, Yuqing Zhang, Shekhar Nayak, Matt Coler

**åˆ†ç±»**: cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å£è¯­è®½åˆºç†è§£ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è®½åˆºæ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `è·¨è¯­è¨€ç†è§£` `éŸ³é¢‘åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è®½åˆºæ£€æµ‹ä¾èµ–æ–‡æœ¬ã€è¯­éŸ³ã€è§†è§‰ç­‰å¤šæ¨¡æ€ä¿¡æ¯ï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºæ–‡æœ¬æˆ–è§†è§‰-æ–‡æœ¬ï¼Œç¼ºä¹å¯¹éŸ³é¢‘-è§†è§‰-æ–‡æœ¬è®½åˆºçš„å…¨é¢ç†è§£ã€‚
2. è®ºæ–‡æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€LLMåœ¨è·¨è¯­è¨€è®½åˆºæ£€æµ‹ä¸­çš„åº”ç”¨ï¼Œå¹¶é‡‡ç”¨åä½œé—¨æ§èåˆæ¨¡å—æ•´åˆå¤šæ¨¡æ€ç‰¹å¾ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒéŸ³é¢‘æ¨¡å‹å•æ¨¡æ€æ€§èƒ½æœ€ä½³ï¼Œæ–‡æœ¬-éŸ³é¢‘å’ŒéŸ³é¢‘-è§†è§‰ç»„åˆä¼˜äºå…¶ä»–æ¨¡æ€ç»„åˆï¼ŒQwen-Omniç­‰MLLMè¡¨ç°å‡ºç«äº‰åŠ›çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®½åˆºæ£€æµ‹æ˜¯è‡ªç„¶è¯­è¨€ç†è§£ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè®½åˆºæ„å›¾é€šå¸¸ä¾èµ–äºè·¨è¶Šæ–‡æœ¬ã€è¯­éŸ³å’Œè§†è§‰çš„ç»†å¾®è·¨æ¨¡æ€çº¿ç´¢ã€‚ä»¥å¾€çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æˆ–è§†è§‰-æ–‡æœ¬è®½åˆºä¸Šï¼Œè€Œå¯¹å…¨é¢çš„éŸ³é¢‘-è§†è§‰-æ–‡æœ¬è®½åˆºç†è§£çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€LLMåœ¨è‹±è¯­ï¼ˆMUStARD++ï¼‰å’Œä¸­æ–‡ï¼ˆMCSD 1.0ï¼‰è®½åˆºæ£€æµ‹ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’ŒLoRAå¾®è°ƒè®¾ç½®ã€‚é™¤äº†ç›´æ¥åˆ†ç±»ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†å°†æ¨¡å‹ä½œä¸ºç‰¹å¾ç¼–ç å™¨ï¼Œå¹¶é€šè¿‡åä½œé—¨æ§èåˆæ¨¡å—æ•´åˆå®ƒä»¬çš„è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºéŸ³é¢‘çš„æ¨¡å‹å®ç°äº†æœ€å¼ºçš„å•æ¨¡æ€æ€§èƒ½ï¼Œè€Œæ–‡æœ¬-éŸ³é¢‘å’ŒéŸ³é¢‘-è§†è§‰ç»„åˆä¼˜äºå•æ¨¡æ€å’Œä¸‰æ¨¡æ€æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒåƒQwen-Omniè¿™æ ·çš„MLLMæ˜¾ç¤ºå‡ºå…·æœ‰ç«äº‰åŠ›çš„é›¶æ ·æœ¬å’Œå¾®è°ƒæ€§èƒ½ã€‚æˆ‘ä»¬çš„å‘ç°çªå‡ºäº†MLLMåœ¨è·¨è¯­è¨€ã€éŸ³é¢‘-è§†è§‰-æ–‡æœ¬è®½åˆºç†è§£æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€è®½åˆºæ£€æµ‹é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯éŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯èåˆçš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æˆ–è§†è§‰-æ–‡æœ¬è®½åˆºæ£€æµ‹ï¼Œå¿½ç•¥äº†éŸ³é¢‘ä¿¡æ¯çš„é‡è¦æ€§ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹è·¨è¯­è¨€è®½åˆºçš„æœ‰æ•ˆå¤„ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰å¼ºå¤§çš„è¡¨å¾å­¦ä¹ èƒ½åŠ›ï¼Œç»“åˆåä½œé—¨æ§èåˆæ¨¡å—ï¼Œæœ‰æ•ˆåœ°æ•´åˆæ¥è‡ªæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜è®½åˆºæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡è·¨è¯­è¨€æ•°æ®é›†çš„å®éªŒï¼ŒéªŒè¯æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨LLM/MLLMä½œä¸ºç‰¹å¾ç¼–ç å™¨ï¼Œåˆ†åˆ«æå–æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„ç‰¹å¾è¡¨ç¤ºï¼›2) ä½¿ç”¨åä½œé—¨æ§èåˆæ¨¡å—ï¼Œè‡ªé€‚åº”åœ°èåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼Œçªå‡ºé‡è¦æ¨¡æ€çš„ä¿¡æ¯ï¼ŒæŠ‘åˆ¶å™ªå£°æ¨¡æ€çš„å½±å“ï¼›3) å°†èåˆåçš„ç‰¹å¾è¾“å…¥åˆ°åˆ†ç±»å™¨ä¸­ï¼Œè¿›è¡Œè®½åˆºæ£€æµ‹ã€‚è®ºæ–‡è¿˜æ¢ç´¢äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’ŒLoRAå¾®è°ƒç­‰ä¸åŒçš„è®­ç»ƒç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) é¦–æ¬¡ç³»ç»Ÿåœ°è¯„ä¼°äº†MLLMåœ¨éŸ³é¢‘-è§†è§‰-æ–‡æœ¬è®½åˆºæ£€æµ‹ä¸­çš„è¡¨ç°ï¼›2) æå‡ºäº†åä½œé—¨æ§èåˆæ¨¡å—ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°èåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼›3) æ¢ç´¢äº†è·¨è¯­è¨€è®½åˆºæ£€æµ‹ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåä½œé—¨æ§èåˆæ¨¡å—çš„è®¾è®¡æ˜¯å…³é”®ã€‚è¯¥æ¨¡å—é€šè¿‡å­¦ä¹ æ¯ä¸ªæ¨¡æ€çš„æƒé‡ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´ä¸åŒæ¨¡æ€çš„è´¡çŒ®ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å—ä½¿ç”¨ä¸€ä¸ªé—¨æ§æœºåˆ¶ï¼Œæ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€åœ°ç”Ÿæˆæ¯ä¸ªæ¨¡æ€çš„æƒé‡ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚LoRAå¾®è°ƒé‡‡ç”¨è¾ƒä½çš„ç§©æ¥æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä»è€Œå‡å°‘è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºéŸ³é¢‘çš„æ¨¡å‹åœ¨å•æ¨¡æ€è®½åˆºæ£€æµ‹ä¸­è¡¨ç°æœ€ä½³ï¼Œæ–‡æœ¬-éŸ³é¢‘å’ŒéŸ³é¢‘-è§†è§‰ç»„åˆä¼˜äºå•æ¨¡æ€å’Œä¸‰æ¨¡æ€æ¨¡å‹ã€‚Qwen-Omniç­‰MLLMåœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸‹å‡è¡¨ç°å‡ºå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¯æ˜äº†MLLMåœ¨è·¨è¯­è¨€ã€éŸ³é¢‘-è§†è§‰-æ–‡æœ¬è®½åˆºç†è§£æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæƒ…æ„Ÿåˆ†æã€èˆ†æƒ…ç›‘æ§ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚é€šè¿‡å‡†ç¡®è¯†åˆ«è®½åˆºè¨€è®ºï¼Œå¯ä»¥æå‡æœºå™¨ç†è§£äººç±»çœŸå®æ„å›¾çš„èƒ½åŠ›ï¼Œä»è€Œæ”¹è¿›äººæœºäº¤äº’ä½“éªŒï¼Œå¹¶ä¸ºä¼ä¸šæä¾›æ›´ç²¾å‡†çš„å¸‚åœºåˆ†æå’Œé£é™©é¢„è­¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sarcasm detection remains a challenge in natural language understanding, as sarcastic intent often relies on subtle cross-modal cues spanning text, speech, and vision. While prior work has primarily focused on textual or visual-textual sarcasm, comprehensive audio-visual-textual sarcasm understanding remains underexplored. In this paper, we systematically evaluate large language models (LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and Chinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In addition to direct classification, we explore models as feature encoders, integrating their representations through a collaborative gating fusion module. Experimental results show that audio-based models achieve the strongest unimodal performance, while text-audio and audio-vision combinations outperform unimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show competitive zero-shot and fine-tuned performance. Our findings highlight the potential of MLLMs for cross-lingual, audio-visual-textual sarcasm understanding.

