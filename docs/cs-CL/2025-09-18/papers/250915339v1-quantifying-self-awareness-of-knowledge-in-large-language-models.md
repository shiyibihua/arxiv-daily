---
layout: default
title: Quantifying Self-Awareness of Knowledge in Large Language Models
---

# Quantifying Self-Awareness of Knowledge in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15339" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15339v1</a>
  <a href="https://arxiv.org/pdf/2509.15339.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15339v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15339v1', 'Quantifying Self-Awareness of Knowledge in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yeongbin Seo, Dongha Lee, Jinyoung Yeo

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAQEæ–¹æ³•ä»¥é‡åŒ–å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†è‡ªæ„ŸçŸ¥ä¸­çš„é—®é¢˜ä¾§å½±å“ï¼Œå¹¶æå‡ºSCAOæ–¹æ³•å¢å¼ºæ¨¡å‹ä¾§ä¿¡å·ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è‡ªæ„ŸçŸ¥` `å¹»è§‰é¢„æµ‹` `é—®é¢˜ä¾§æ•ˆåº”` `è¯­ä¹‰å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹å¹»è§‰é¢„æµ‹ç ”ç©¶å¯èƒ½é«˜ä¼°äº†æ¨¡å‹çš„è‡ªæ„ŸçŸ¥èƒ½åŠ›ï¼Œå¿½ç•¥äº†é—®é¢˜ä¾§æ·å¾„çš„å½±å“ã€‚
2. è®ºæ–‡æå‡ºè¿‘ä¼¼é—®é¢˜ä¾§æ•ˆåº”ï¼ˆAQEï¼‰æ¥é‡åŒ–é—®é¢˜ä¾§ä¿¡æ¯å¯¹å¹»è§‰é¢„æµ‹çš„å½±å“ï¼Œä»è€Œæ›´å‡†ç¡®è¯„ä¼°æ¨¡å‹è‡ªæ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å¼•å…¥è¯­ä¹‰å‹ç¼©æ–¹æ³•SCAOï¼Œé€šè¿‡å•å­—å›ç­”å‹ç¼©è¯­ä¹‰ï¼Œå¢å¼ºæ¨¡å‹ä¾§ä¿¡å·ï¼Œå®éªŒè¡¨æ˜SCAOèƒ½æœ‰æ•ˆæå‡æ¨¡å‹è‡ªæ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¹»è§‰é¢„æµ‹é€šå¸¸è¢«è§£é‡Šä¸ºè‡ªæ„ŸçŸ¥èƒ½åŠ›çš„ä½“ç°ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§æ€§èƒ½å¯èƒ½æºäºé—®é¢˜ä¾§çš„æ·å¾„ï¼Œè€ŒéçœŸæ­£çš„æ¨¡å‹ä¾§å†…çœã€‚ä¸ºäº† disentangle è¿™äº›å› ç´ ï¼Œæˆ‘ä»¬æå‡ºäº†è¿‘ä¼¼é—®é¢˜ä¾§æ•ˆåº”ï¼ˆAQEï¼‰ï¼Œç”¨äºé‡åŒ–é—®é¢˜æ„ŸçŸ¥çš„è´¡çŒ®ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªæ•°æ®é›†çš„åˆ†æè¡¨æ˜ï¼Œè®¸å¤šå·²æŠ¥é“çš„æˆåŠŸæºäºåˆ©ç”¨é—®é¢˜ä¸­çš„è¡¨é¢æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† SCAOï¼ˆé€šè¿‡å•å­—å›ç­”è¿›è¡Œè¯­ä¹‰å‹ç¼©ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºæ¨¡å‹ä¾§ä¿¡å·åˆ©ç”¨çš„æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒSCAO å®ç°äº†å¼ºå¤§ä¸”ä¸€è‡´çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é—®é¢˜ä¾§çº¿ç´¢å‡å°‘çš„è®¾ç½®ä¸­ï¼Œçªæ˜¾äº†å…¶åœ¨åŸ¹å…» LLM ä¸­çœŸæ­£è‡ªæ„ŸçŸ¥èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çŸ¥è¯†è‡ªæ„ŸçŸ¥èƒ½åŠ›è¯„ä¼°ä¸­å­˜åœ¨çš„åå·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°LLMsçš„å¹»è§‰é¢„æµ‹èƒ½åŠ›æ—¶ï¼Œå¾€å¾€éš¾ä»¥åŒºåˆ†æ¨¡å‹çœŸæ­£çš„çŸ¥è¯†å†…çœèƒ½åŠ›å’Œä»…ä»…åˆ©ç”¨é—®é¢˜ä¾§çš„è¡¨é¢æ¨¡å¼è¿›è¡Œé¢„æµ‹çš„èƒ½åŠ›ã€‚è¿™ç§æ··æ·†å¯¼è‡´å¯¹LLMsè‡ªæ„ŸçŸ¥èƒ½åŠ›çš„è¿‡é«˜ä¼°è®¡ã€‚å› æ­¤ï¼Œå¦‚ä½•å‡†ç¡®é‡åŒ–é—®é¢˜ä¾§ä¿¡æ¯çš„å½±å“ï¼Œå¹¶è®¾è®¡æ–¹æ³•å¢å¼ºæ¨¡å‹ä¾§çš„çŸ¥è¯†ä¿¡å·ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ disentangle é—®é¢˜ä¾§å’Œæ¨¡å‹ä¾§çš„ä¿¡æ¯ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°LLMsçš„è‡ªæ„ŸçŸ¥èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆæå‡ºè¿‘ä¼¼é—®é¢˜ä¾§æ•ˆåº”ï¼ˆAQEï¼‰æ¥é‡åŒ–é—®é¢˜ä¾§ä¿¡æ¯å¯¹å¹»è§‰é¢„æµ‹çš„è´¡çŒ®ã€‚ç„¶åï¼Œé€šè¿‡è¯­ä¹‰å‹ç¼©æ–¹æ³•SCAOï¼Œå‡å°‘é—®é¢˜ä¾§çš„å¹²æ‰°ï¼Œå¹¶å¢å¼ºæ¨¡å‹ä¾§çš„çŸ¥è¯†ä¿¡å·ï¼Œä»è€Œæé«˜æ¨¡å‹çœŸæ­£çš„è‡ªæ„ŸçŸ¥èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼šä¸€æ˜¯è¿‘ä¼¼é—®é¢˜ä¾§æ•ˆåº”ï¼ˆAQEï¼‰çš„è®¡ç®—æ–¹æ³•ï¼Œç”¨äºé‡åŒ–é—®é¢˜ä¾§ä¿¡æ¯çš„å½±å“ï¼›äºŒæ˜¯è¯­ä¹‰å‹ç¼©æ–¹æ³•SCAOï¼Œç”¨äºå¢å¼ºæ¨¡å‹ä¾§çš„çŸ¥è¯†ä¿¡å·ã€‚AQEé€šè¿‡åˆ†ææ¨¡å‹åœ¨ä¸åŒé—®é¢˜å˜ä½“ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œæ¥ä¼°è®¡é—®é¢˜ä¾§ä¿¡æ¯å¯¹é¢„æµ‹ç»“æœçš„å½±å“ã€‚SCAOåˆ™é€šè¿‡å°†é—®é¢˜å‹ç¼©ä¸ºå•å­—å›ç­”ï¼Œå‡å°‘é—®é¢˜ä¾§çš„è¡¨é¢æ¨¡å¼ï¼Œä»è€Œè¿«ä½¿æ¨¡å‹æ›´å¤šåœ°ä¾èµ–è‡ªèº«çš„çŸ¥è¯†è¿›è¡Œé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†AQEå’ŒSCAOä¸¤ç§æ–¹æ³•ã€‚AQEæä¾›äº†ä¸€ç§é‡åŒ–é—®é¢˜ä¾§ä¿¡æ¯å½±å“çš„æ‰‹æ®µï¼Œä½¿å¾—ç ”ç©¶è€…å¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°LLMsçš„è‡ªæ„ŸçŸ¥èƒ½åŠ›ã€‚SCAOåˆ™é€šè¿‡è¯­ä¹‰å‹ç¼©ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†é—®é¢˜ä¾§çš„å¹²æ‰°ï¼Œå¹¶å¢å¼ºäº†æ¨¡å‹ä¾§çš„çŸ¥è¯†ä¿¡å·ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çœŸæ­£çš„è‡ªæ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•æ›´åŠ æ³¨é‡åŒºåˆ†é—®é¢˜ä¾§å’Œæ¨¡å‹ä¾§çš„ä¿¡æ¯ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°LLMsçš„è‡ªæ„ŸçŸ¥èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šAQEçš„å…·ä½“è®¡ç®—æ–¹æ³•æœªçŸ¥ï¼Œéœ€è¦å‚è€ƒè®ºæ–‡ç»†èŠ‚ã€‚SCAOçš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°å°†é—®é¢˜å‹ç¼©ä¸ºå•å­—å›ç­”ï¼Œå¹¶ä¿è¯å‹ç¼©åçš„ä¿¡æ¯ä»ç„¶èƒ½å¤Ÿä¿ç•™é—®é¢˜çš„æ ¸å¿ƒè¯­ä¹‰ã€‚å…·ä½“çš„å‹ç¼©ç®—æ³•å’ŒæŸå¤±å‡½æ•°æœªçŸ¥ï¼Œéœ€è¦å‚è€ƒè®ºæ–‡ç»†èŠ‚ã€‚æ­¤å¤–ï¼ŒSCAOåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½éœ€è¦è°ƒæ•´æ¨¡å‹çš„å‚æ•°ï¼Œä»¥é€‚åº”å•å­—å›ç­”çš„è¾“å…¥å½¢å¼ã€‚è¿™äº›å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„éœ€è¦å‚è€ƒè®ºæ–‡çš„è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAQEåˆ†ææ­ç¤ºäº†ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°LLMsè‡ªæ„ŸçŸ¥èƒ½åŠ›æ—¶å­˜åœ¨é«˜ä¼°ç°è±¡ã€‚SCAOæ–¹æ³•åœ¨å‡å°‘é—®é¢˜ä¾§çº¿ç´¢çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è‡ªæ„ŸçŸ¥èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ›´æ·±å±‚æ¬¡çŸ¥è¯†æ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œéœ€è¦å‚è€ƒè®ºæ–‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦å’Œå¯é æ€§ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—è¯Šæ–­ã€æ³•å¾‹å’¨è¯¢ç­‰å¯¹å‡†ç¡®æ€§è¦æ±‚é«˜çš„é¢†åŸŸã€‚é€šè¿‡æ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹çš„è‡ªæ„ŸçŸ¥èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°è¯†åˆ«å’Œé¿å…æ¨¡å‹äº§ç”Ÿå¹»è§‰ï¼Œä»è€Œæé«˜æ¨¡å‹çš„åº”ç”¨ä»·å€¼ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›å¯¹å¤§è¯­è¨€æ¨¡å‹å†…éƒ¨æœºåˆ¶çš„ç†è§£ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡æä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hallucination prediction in large language models (LLMs) is often interpreted as a sign of self-awareness. However, we argue that such performance can arise from question-side shortcuts rather than true model-side introspection. To disentangle these factors, we propose the Approximate Question-side Effect (AQE), which quantifies the contribution of question-awareness. Our analysis across multiple datasets reveals that much of the reported success stems from exploiting superficial patterns in questions. We further introduce SCAO (Semantic Compression by Answering in One word), a method that enhances the use of model-side signals. Experiments show that SCAO achieves strong and consistent performance, particularly in settings with reduced question-side cues, highlighting its effectiveness in fostering genuine self-awareness in LLMs.

