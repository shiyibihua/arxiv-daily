---
layout: default
title: Delta Knowledge Distillation for Large Language Models
---

# Delta Knowledge Distillation for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14526" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14526v1</a>
  <a href="https://arxiv.org/pdf/2509.14526.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14526v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14526v1', 'Delta Knowledge Distillation for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yihan Cao, Yanbin Kang, Zhengming Xing, Ruijie Jiang

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: 8 pages, 3 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDelta-KDï¼Œé€šè¿‡ä¿ç•™åˆ†å¸ƒåç§»é‡æå‡å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†è’¸é¦æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `åˆ†å¸ƒåç§»` `ç›‘ç£å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰tokençº§åˆ«çŸ¥è¯†è’¸é¦æ–¹æ³•å‡è®¾å¸ˆç”Ÿæ¨¡å‹å…±äº«æœ€ä¼˜è¡¨ç¤ºç©ºé—´ï¼Œè¿™åœ¨å®é™…ä¸­å¾€å¾€ä¸æˆç«‹ã€‚
2. Delta-KDé€šè¿‡æ˜¾å¼ä¿ç•™æ•™å¸ˆæ¨¡å‹åœ¨SFTæœŸé—´å¼•å…¥çš„åˆ†å¸ƒåç§»é‡ï¼Œå¼•å¯¼å­¦ç”Ÿæ¨¡å‹é€¼è¿‘æœ€ä¼˜è¡¨ç¤ºç©ºé—´ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDelta-KDåœ¨ROUGEæŒ‡æ ‡ä¸Šæ˜¾è‘—æå‡å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ï¼Œå¹¶èƒ½æ›´å¥½åœ°ä¿ç•™æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†è’¸é¦(KD)æ˜¯ä¸€ç§å¹¿æ³›é‡‡ç”¨çš„æ–¹æ³•ï¼Œé€šè¿‡å°†çŸ¥è¯†ä»å¤§å‹æ•™å¸ˆæ¨¡å‹è½¬ç§»åˆ°è¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹æ¥å‹ç¼©å¤§å‹ç¥ç»ç½‘ç»œã€‚åœ¨å¤§è¯­è¨€æ¨¡å‹çš„èƒŒæ™¯ä¸‹ï¼Œtokençº§åˆ«çš„KDï¼Œé€šå¸¸æ˜¯æœ€å°åŒ–å­¦ç”Ÿè¾“å‡ºåˆ†å¸ƒå’Œæ•™å¸ˆè¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ï¼Œå·²ç»æ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç»éªŒæ€§èƒ½ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œå‡è®¾å­¦ç”Ÿè¾“å‡ºåˆ†å¸ƒå’Œæ•™å¸ˆè¾“å‡ºåˆ†å¸ƒå…±äº«ç›¸åŒçš„æœ€ä¼˜è¡¨ç¤ºç©ºé—´ï¼Œè¿™ä¸€å‰æåœ¨è®¸å¤šæƒ…å†µä¸‹å¯èƒ½ä¸æˆç«‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeltaçŸ¥è¯†è’¸é¦(Delta-KD)ï¼Œè¿™æ˜¯tokençº§åˆ«KDçš„ä¸€ç§æ–°çš„æ‰©å±•ï¼Œå®ƒé¼“åŠ±å­¦ç”Ÿé€šè¿‡æ˜¾å¼åœ°ä¿ç•™æ•™å¸ˆçš„ç›‘ç£å¾®è°ƒ(SFT)æœŸé—´å¼•å…¥çš„åˆ†å¸ƒåç§»é‡Deltaæ¥é€¼è¿‘æœ€ä¼˜è¡¨ç¤ºç©ºé—´ã€‚ROUGEæŒ‡æ ‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDelta KDåœ¨ä¿ç•™æ›´å¤šæ•™å¸ˆçŸ¥è¯†çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰tokençº§åˆ«çš„çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨å‹ç¼©å¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œé€šå¸¸å‡è®¾å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹å…·æœ‰ç›¸åŒçš„æœ€ä¼˜è¡¨ç¤ºç©ºé—´ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹å®¹é‡çš„å·®å¼‚ä»¥åŠè®­ç»ƒæ–¹å¼çš„ä¸åŒï¼Œè¿™ä¸€å‡è®¾å¾€å¾€ä¸æˆç«‹ï¼Œå¯¼è‡´å­¦ç”Ÿæ¨¡å‹æ— æ³•å……åˆ†å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚å› æ­¤ï¼Œå¦‚ä½•å¼¥åˆå¸ˆç”Ÿæ¨¡å‹è¡¨ç¤ºç©ºé—´å·®å¼‚ï¼Œæå‡çŸ¥è¯†è’¸é¦æ•ˆæœï¼Œæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDelta-KDçš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œåœ¨çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­ï¼Œä¸ä»…è¦è®©å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒï¼Œè¿˜è¦å­¦ä¹ æ•™å¸ˆæ¨¡å‹åœ¨ç›‘ç£å¾®è°ƒ(SFT)è¿‡ç¨‹ä¸­å¼•å…¥çš„åˆ†å¸ƒåç§»é‡(Delta)ã€‚è¿™ä¸ªDeltaå¯ä»¥çœ‹ä½œæ˜¯æ•™å¸ˆæ¨¡å‹ä»é¢„è®­ç»ƒçŠ¶æ€åˆ°å¾®è°ƒçŠ¶æ€çš„çŸ¥è¯†å¢é‡ï¼ŒåŒ…å«äº†ä»»åŠ¡ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚é€šè¿‡æ˜¾å¼åœ°ä¿ç•™è¿™ä¸ªDeltaï¼Œå¯ä»¥å¼•å¯¼å­¦ç”Ÿæ¨¡å‹æ›´å¿«åœ°é€¼è¿‘æ•™å¸ˆæ¨¡å‹çš„æœ€ä¼˜è¡¨ç¤ºç©ºé—´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDelta-KDå»ºç«‹åœ¨tokençº§åˆ«çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ä¹‹ä¸Šã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œåˆ©ç”¨æ•™å¸ˆæ¨¡å‹åœ¨SFTå‰åçš„è¾“å‡ºåˆ†å¸ƒè®¡ç®—åˆ†å¸ƒåç§»é‡Deltaã€‚ç„¶åï¼Œåœ¨è®­ç»ƒå­¦ç”Ÿæ¨¡å‹æ—¶ï¼Œé™¤äº†æœ€å°åŒ–å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦å¤–ï¼Œè¿˜å¢åŠ ä¸€ä¸ªé¢å¤–çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºçº¦æŸå­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„åˆ†å¸ƒåç§»é‡Deltaã€‚æ•´ä½“æ¶æ„ä»ç„¶æ˜¯æ ‡å‡†çš„çŸ¥è¯†è’¸é¦æµç¨‹ï¼Œä½†æŸå¤±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ã€‚

**å…³é”®åˆ›æ–°**ï¼šDelta-KDæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œå®ƒæ˜¾å¼åœ°è€ƒè™‘äº†æ•™å¸ˆæ¨¡å‹åœ¨SFTè¿‡ç¨‹ä¸­å¼•å…¥çš„åˆ†å¸ƒåç§»é‡ï¼Œå¹¶å°†å…¶ä½œä¸ºä¸€ç§çŸ¥è¯†ä¼ é€’ç»™å­¦ç”Ÿæ¨¡å‹ã€‚è¿™ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•åªå…³æ³¨æ•™å¸ˆæ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºåˆ†å¸ƒä¸åŒï¼Œæ›´å…¨é¢åœ°è€ƒè™‘äº†æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¼¥åˆå¸ˆç”Ÿæ¨¡å‹ä¹‹é—´çš„è¡¨ç¤ºç©ºé—´å·®å¼‚ï¼Œæå‡çŸ¥è¯†è’¸é¦æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šDelta-KDçš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•è®¡ç®—å’Œåˆ©ç”¨åˆ†å¸ƒåç§»é‡Deltaã€‚è®ºæ–‡ä¸­ï¼ŒDeltaè¢«å®šä¹‰ä¸ºæ•™å¸ˆæ¨¡å‹åœ¨SFTå‰åçš„è¾“å‡ºåˆ†å¸ƒä¹‹å·®ã€‚åœ¨è®­ç»ƒå­¦ç”Ÿæ¨¡å‹æ—¶ï¼Œå¢åŠ äº†ä¸€ä¸ªé¢å¤–çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºæœ€å°åŒ–å­¦ç”Ÿæ¨¡å‹é¢„æµ‹çš„Deltaå’Œæ•™å¸ˆæ¨¡å‹Deltaä¹‹é—´çš„å·®å¼‚ã€‚å…·ä½“æ¥è¯´ï¼Œå¯ä»¥ä½¿ç”¨KLæ•£åº¦æˆ–è€…å‡æ–¹è¯¯å·®ç­‰æŸå¤±å‡½æ•°æ¥è¡¡é‡è¿™ä¸¤ä¸ªDeltaä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œå¦‚ä½•å¹³è¡¡KLæ•£åº¦æŸå¤±å’ŒDeltaæŸå¤±çš„æƒé‡ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDelta-KDåœ¨ROUGEæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„tokençº§åˆ«çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªå…·ä½“ä»»åŠ¡ä¸Šï¼ŒDelta-KDå¯ä»¥å°†å­¦ç”Ÿæ¨¡å‹çš„ROUGE-LæŒ‡æ ‡æå‡è¶…è¿‡2ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼ŒDelta-KDèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ï¼Œä½¿å¾—å­¦ç”Ÿæ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°æ›´æ¥è¿‘æ•™å¸ˆæ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Delta-KDå¯åº”ç”¨äºå„ç§éœ€è¦å‹ç¼©å¤§è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡éƒ¨ç½²ã€è¾¹ç¼˜è®¡ç®—ã€ä½èµ„æºè®¾å¤‡åº”ç”¨ç­‰ã€‚é€šè¿‡çŸ¥è¯†è’¸é¦ï¼Œå¯ä»¥å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›è¿ç§»åˆ°å°å‹æ¨¡å‹ä¸Šï¼Œä»è€Œåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„è‡ªç„¶è¯­è¨€å¤„ç†ã€‚è¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºæå‡æ¨¡å‹çš„æ¨ç†é€Ÿåº¦å’Œé™ä½è®¡ç®—æˆæœ¬ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge distillation (KD) is a widely adopted approach for compressing large neural networks by transferring knowledge from a large teacher model to a smaller student model. In the context of large language models, token level KD, typically minimizing the KL divergence between student output distribution and teacher output distribution, has shown strong empirical performance. However, prior work assumes student output distribution and teacher output distribution share the same optimal representation space, a premise that may not hold in many cases. To solve this problem, we propose Delta Knowledge Distillation (Delta-KD), a novel extension of token level KD that encourages the student to approximate an optimal representation space by explicitly preserving the distributional shift Delta introduced during the teacher's supervised finetuning (SFT). Empirical results on ROUGE metrics demonstrate that Delta KD substantially improves student performance while preserving more of the teacher's knowledge.

