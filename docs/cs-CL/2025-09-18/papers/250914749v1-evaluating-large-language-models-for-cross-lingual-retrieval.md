---
layout: default
title: Evaluating Large Language Models for Cross-Lingual Retrieval
---

# Evaluating Large Language Models for Cross-Lingual Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14749" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14749v1</a>
  <a href="https://arxiv.org/pdf/2509.14749.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14749v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14749v1', 'Evaluating Large Language Models for Cross-Lingual Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Longfei Zuo, Pingjun Hong, Oliver Kraus, Barbara Plank, Robert Litschko

**åˆ†ç±»**: cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: Accepted at EMNLP 2025 (Findings)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨è·¨è¯­è¨€æ£€ç´¢ä¸­çš„åº”ç”¨ï¼Œæ­ç¤ºæ£€ç´¢å™¨ä¸é‡æ’åºå™¨é—´çš„äº¤äº’å½±å“ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨è¯­è¨€æ£€ç´¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šè¯­è¨€åŒç¼–ç å™¨` `ä¿¡æ¯æ£€ç´¢` `é‡æ’åº` `æœºå™¨ç¿»è¯‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰CLIRæ–¹æ³•ä¾èµ–æœºå™¨ç¿»è¯‘è¿›è¡Œç¬¬ä¸€é˜¶æ®µæ£€ç´¢ï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ˜“å¼•å…¥è¯¯å·®ï¼Œé™åˆ¶äº†æ€§èƒ½ã€‚
2. è®ºæ–‡æå‡ºä½¿ç”¨å¤šè¯­è¨€åŒç¼–ç å™¨ä½œä¸ºç¬¬ä¸€é˜¶æ®µæ£€ç´¢å™¨ï¼Œå¹¶ç ”ç©¶å…¶ä¸LLMé‡æ’åºå™¨çš„äº¤äº’ä½œç”¨ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå¤šè¯­è¨€åŒç¼–ç å™¨èƒ½æå‡CLIRæ€§èƒ½ï¼Œä¸”æ›´å¼ºçš„é‡æ’åºæ¨¡å‹èƒ½é™ä½å¯¹ç¿»è¯‘çš„ä¾èµ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šé˜¶æ®µä¿¡æ¯æ£€ç´¢(IR)å·²æˆä¸ºæœç´¢é¢†åŸŸå¹¿æ³›é‡‡ç”¨çš„èŒƒå¼ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä½œä¸ºå•è¯­IRçš„ç¬¬äºŒé˜¶æ®µé‡æ’åºæ¨¡å‹å·²è¢«å¹¿æ³›è¯„ä¼°ï¼Œä½†å¯¹äºè·¨è¯­è¨€IR(CLIR)çš„ç³»ç»Ÿæ€§å¤§è§„æ¨¡æ¯”è¾ƒä»ç„¶ç¼ºä¹ã€‚æ­¤å¤–ï¼Œå…ˆå‰çš„å·¥ä½œè¡¨æ˜ï¼ŒåŸºäºLLMçš„é‡æ’åºå™¨å¯ä»¥æé«˜CLIRçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„è¯„ä¼°è®¾ç½®ä¾èµ–äºä½¿ç”¨æœºå™¨ç¿»è¯‘(MT)è¿›è¡Œç¬¬ä¸€é˜¶æ®µçš„è¯æ±‡æ£€ç´¢ã€‚è¿™ä¸ä»…æˆæœ¬é«˜æ˜‚ï¼Œè€Œä¸”å®¹æ˜“å¯¼è‡´è·¨é˜¶æ®µçš„è¯¯å·®ä¼ æ’­ã€‚æˆ‘ä»¬å¯¹æ®µè½çº§å’Œæ–‡æ¡£çº§CLIRçš„è¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨å¤šè¯­è¨€åŒç¼–ç å™¨ä½œä¸ºç¬¬ä¸€é˜¶æ®µæ£€ç´¢å™¨å¯ä»¥å®ç°è¿›ä¸€æ­¥çš„æå‡ï¼Œå¹¶ä¸”ç¿»è¯‘çš„å¥½å¤„éšç€æ›´å¼ºçš„é‡æ’åºæ¨¡å‹è€Œå‡å°‘ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒåŸºäºæŒ‡ä»¤è°ƒæ•´LLMçš„æˆå¯¹é‡æ’åºå™¨ä¸åˆ—è¡¨å¼é‡æ’åºå™¨ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªç ”ç©¶LLMåœ¨ä¸¤é˜¶æ®µCLIRä¸­æ£€ç´¢å™¨å’Œé‡æ’åºå™¨ä¹‹é—´äº¤äº’çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ²¡æœ‰MTçš„æƒ…å†µä¸‹ï¼Œå½“å‰æœ€å…ˆè¿›çš„é‡æ’åºå™¨ç›´æ¥åº”ç”¨äºCLIRæ—¶ä¼šä¸¥é‡ä¸è¶³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ï¼ˆCLIRï¼‰ä¸­ï¼Œç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰è¿›è¡Œç¬¬ä¸€é˜¶æ®µæ£€ç´¢ï¼Œå¯¼è‡´æˆæœ¬é«˜æ˜‚å’Œè¯¯å·®ä¼ æ’­çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œç¬¬ä¸€é˜¶æ®µæ£€ç´¢çš„è´¨é‡ç›´æ¥å½±å“åç»­é‡æ’åºçš„æ•ˆæœï¼Œè€ŒMTå¼•å…¥çš„å™ªå£°ä¼šé™ä½æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ¢ç´¢ä½¿ç”¨å¤šè¯­è¨€åŒç¼–ç å™¨ä½œä¸ºç¬¬ä¸€é˜¶æ®µæ£€ç´¢å™¨ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„åŸºäºè¯æ±‡çš„æ£€ç´¢æ–¹æ³•ç»“åˆæœºå™¨ç¿»è¯‘çš„æ–¹æ¡ˆã€‚åŒæ—¶ï¼Œç ”ç©¶ä¸åŒç±»å‹çš„LLMé‡æ’åºå™¨ä¸å¤šè¯­è¨€åŒç¼–ç å™¨æ£€ç´¢å™¨ä¹‹é—´çš„äº¤äº’ä½œç”¨ï¼Œåˆ†æåœ¨CLIRä»»åŠ¡ä¸­æ˜¯å¦å¯ä»¥å‡å°‘å¯¹æœºå™¨ç¿»è¯‘çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨ä¸¤é˜¶æ®µæ£€ç´¢æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤šè¯­è¨€åŒç¼–ç å™¨æ£€ç´¢å™¨ï¼Œä»è·¨è¯­è¨€æ–‡æ¡£é›†ä¸­æ£€ç´¢å€™é€‰æ–‡æ¡£ã€‚ç¬¬äºŒé˜¶æ®µä½¿ç”¨LLMé‡æ’åºå™¨ï¼Œå¯¹ç¬¬ä¸€é˜¶æ®µæ£€ç´¢åˆ°çš„å€™é€‰æ–‡æ¡£è¿›è¡Œæ’åºï¼Œé€‰æ‹©æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚è®ºæ–‡æ¯”è¾ƒäº†ä¸åŒçš„LLMé‡æ’åºå™¨ï¼ŒåŒ…æ‹¬æˆå¯¹é‡æ’åºå™¨å’Œåˆ—è¡¨å¼é‡æ’åºå™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†å¤šè¯­è¨€åŒç¼–ç å™¨æ£€ç´¢å™¨ä¸LLMé‡æ’åºå™¨åœ¨ä¸¤é˜¶æ®µCLIRä¸­çš„äº¤äº’ä½œç”¨ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨MTè¿›è¡Œç¬¬ä¸€é˜¶æ®µæ£€ç´¢ï¼Œè€Œå¿½ç•¥äº†æ£€ç´¢å™¨æœ¬èº«å¯¹CLIRæ€§èƒ½çš„å½±å“ã€‚è®ºæ–‡é¦–æ¬¡è¯æ˜äº†ä½¿ç”¨å¤šè¯­è¨€åŒç¼–ç å™¨å¯ä»¥æœ‰æ•ˆæå‡CLIRæ€§èƒ½ï¼Œå¹¶å‡å°‘å¯¹MTçš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€åŒç¼–ç å™¨æ¨¡å‹ä½œä¸ºç¬¬ä¸€é˜¶æ®µæ£€ç´¢å™¨ï¼Œä¾‹å¦‚mBERTæˆ–XLM-RoBERTaã€‚2) é‡‡ç”¨ä¸åŒçš„LLMé‡æ’åºå™¨ï¼ŒåŒ…æ‹¬åŸºäºæŒ‡ä»¤è°ƒæ•´çš„æˆå¯¹é‡æ’åºå™¨å’Œåˆ—è¡¨å¼é‡æ’åºå™¨ï¼Œä¾‹å¦‚RankT5ã€‚3) ä½¿ç”¨æ ‡å‡†CLIRæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œä¾‹å¦‚MLDocå’ŒMIRACLã€‚4) è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬Recall@Kå’ŒNDCG@Kï¼Œç”¨äºè¡¡é‡æ£€ç´¢å’Œæ’åºçš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¤šè¯­è¨€åŒç¼–ç å™¨ä½œä¸ºç¬¬ä¸€é˜¶æ®µæ£€ç´¢å™¨ï¼Œå¯ä»¥æ˜¾è‘—æå‡CLIRæ€§èƒ½ã€‚ä¸ä¾èµ–æœºå™¨ç¿»è¯‘çš„ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ®µè½çº§å’Œæ–‡æ¡£çº§CLIRä»»åŠ¡ä¸Šå‡å–å¾—äº†æ›´å¥½çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œæ›´å¼ºçš„LLMé‡æ’åºå™¨å¯ä»¥è¿›ä¸€æ­¥é™ä½å¯¹æœºå™¨ç¿»è¯‘çš„ä¾èµ–ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥å®Œå…¨æ¶ˆé™¤å¯¹æœºå™¨ç¿»è¯‘çš„éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè·¨è¯­è¨€æœç´¢å¼•æ“ã€å¤šè¯­è¨€é—®ç­”ç³»ç»Ÿã€å›½é™…æ–°é—»èšåˆç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæœ‰åŠ©äºæ‰“ç ´è¯­è¨€å£å’ï¼Œä¿ƒè¿›å…¨çƒèŒƒå›´å†…çš„ä¿¡æ¯å…±äº«å’ŒçŸ¥è¯†ä¼ æ’­ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æ–¹å‘æœ‰æœ›æ¨åŠ¨æ„å»ºæ›´åŠ æ™ºèƒ½å’Œä¾¿æ·çš„è·¨è¯­è¨€ä¿¡æ¯æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-stage information retrieval (IR) has become a widely-adopted paradigm in search. While Large Language Models (LLMs) have been extensively evaluated as second-stage reranking models for monolingual IR, a systematic large-scale comparison is still lacking for cross-lingual IR (CLIR). Moreover, while prior work shows that LLM-based rerankers improve CLIR performance, their evaluation setup relies on lexical retrieval with machine translation (MT) for the first stage. This is not only prohibitively expensive but also prone to error propagation across stages. Our evaluation on passage-level and document-level CLIR reveals that further gains can be achieved with multilingual bi-encoders as first-stage retrievers and that the benefits of translation diminishes with stronger reranking models. We further show that pairwise rerankers based on instruction-tuned LLMs perform competitively with listwise rerankers. To the best of our knowledge, we are the first to study the interaction between retrievers and rerankers in two-stage CLIR with LLMs. Our findings reveal that, without MT, current state-of-the-art rerankers fall severely short when directly applied in CLIR.

