---
layout: default
title: SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models
---

# SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15174" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15174v2</a>
  <a href="https://arxiv.org/pdf/2509.15174.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15174v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15174v2', 'SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huy Nghiem, Advik Sachdeva, Hal DaumÃ©

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-10-08)

**å¤‡æ³¨**: NLP, Hate speech detection, explanation, LLM. Version 2: updated experiments and analysis

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SMARTERï¼šåˆ©ç”¨è‡ªå¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œé«˜æ•ˆæå‡æ¯’æ€§æ£€æµ‹èƒ½åŠ›å¹¶æä¾›å¯è§£é‡Šæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¯’æ€§æ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `è‡ªå¢å¼ºå­¦ä¹ ` `å†…å®¹å®¡æ ¸`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¯’æ€§æ£€æµ‹æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä¸”ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œé™åˆ¶äº†å…¶åœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„åº”ç”¨ã€‚
2. SMARTERæ¡†æ¶åˆ©ç”¨LLMçš„è‡ªå¢å¼ºèƒ½åŠ›ï¼Œç”Ÿæˆåˆæˆè§£é‡Šå¹¶è¿›è¡Œè·¨æ¨¡å‹è®­ç»ƒï¼Œä»è€Œåœ¨æ•°æ®é‡æœ‰é™çš„æƒ…å†µä¸‹æå‡æ¯’æ€§æ£€æµ‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSMARTERåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå®å¹³å‡F1å€¼æœ€é«˜æå‡äº†13.5%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºè§£å†³ç¤¾äº¤åª’ä½“ä¸Šæ¶æ„å†…å®¹æ³›æ»¥çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SMARTERï¼Œä¸€ä¸ªæ•°æ®é«˜æ•ˆçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¯è§£é‡Šçš„å†…å®¹å®¡æ ¸ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨LLMsè‡ªèº«çš„è¾“å‡ºæ¥ç”Ÿæˆåˆæˆè§£é‡Šï¼Œç”¨äºçº æ­£æ­£ç¡®å’Œä¸æ­£ç¡®çš„æ ‡ç­¾ï¼Œä»è€Œé€šè¿‡åå¥½ä¼˜åŒ–å®ç°å¯¹é½ï¼Œä¸”åªéœ€æå°‘çš„äººå·¥ç›‘ç£ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡è·¨æ¨¡å‹è®­ç»ƒæ¥æ”¹è¿›è§£é‡Šè´¨é‡ï¼Œä½¿è¾ƒå¼±çš„æ¨¡å‹åœ¨é£æ ¼å’Œè¯­ä¹‰ä¸Šä¸è¾ƒå¼ºçš„æ¨¡å‹å¯¹é½ã€‚åœ¨HateXplainã€Latent Hateå’ŒImplicit Hateä¸‰ä¸ªåŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSMARTERä½¿LLMsèƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ¯”æ ‡å‡†çš„å°æ ·æœ¬åŸºçº¿æé«˜é«˜è¾¾13.5%çš„å®å¹³å‡F1å€¼ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡åˆ©ç”¨LLMsçš„è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›è¿›è¡Œåˆ†ç±»å’Œè§£é‡Šï¼Œä¸ºä½èµ„æºç¯å¢ƒæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ¯’æ€§å†…å®¹æ£€æµ‹ä¸­æ•°æ®æ•ˆç‡ä½å’Œç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œè¿™åœ¨ä½èµ„æºåœºæ™¯ä¸‹æ˜¯ä¸å¯è¡Œçš„ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ¨¡å‹ç¼ºä¹å¯¹å†³ç­–è¿‡ç¨‹çš„è§£é‡Šï¼Œéš¾ä»¥è¿›è¡Œè°ƒè¯•å’Œä¿¡ä»»è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªå¢å¼ºèƒ½åŠ›ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆè§£é‡Šæ¥æå‡æ¯’æ€§æ£€æµ‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡è®©LLMè§£é‡Šå…¶è‡ªèº«çš„é¢„æµ‹ç»“æœï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨LLMçš„çŸ¥è¯†ï¼Œå¹¶å‡å°‘å¯¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSMARTERæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯åˆ©ç”¨LLMsç”Ÿæˆåˆæˆè§£é‡Šï¼Œå¹¶ä½¿ç”¨åå¥½ä¼˜åŒ–è¿›è¡Œå¯¹é½ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯é€šè¿‡è·¨æ¨¡å‹è®­ç»ƒæ¥æå‡è§£é‡Šçš„è´¨é‡ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š
1. **é˜¶æ®µä¸€ï¼šè‡ªå¢å¼ºè§£é‡Šç”Ÿæˆ**ï¼šä½¿ç”¨LLMå¯¹æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¹¶è¦æ±‚LLMè§£é‡Šå…¶é¢„æµ‹ç»“æœã€‚å¯¹äºæ­£ç¡®å’Œé”™è¯¯çš„é¢„æµ‹ï¼Œéƒ½ç”Ÿæˆç›¸åº”çš„è§£é‡Šã€‚
2. **é˜¶æ®µäºŒï¼šåå¥½ä¼˜åŒ–å¯¹é½**ï¼šä½¿ç”¨ç”Ÿæˆçš„è§£é‡Šä½œä¸ºç›‘ç£ä¿¡å·ï¼Œé€šè¿‡åå¥½ä¼˜åŒ–æ¥å¯¹é½LLMçš„é¢„æµ‹å’Œè§£é‡Šã€‚
3. **é˜¶æ®µä¸‰ï¼šè·¨æ¨¡å‹è®­ç»ƒ**ï¼šä½¿ç”¨æ›´å¼ºå¤§çš„LLMç”Ÿæˆçš„è§£é‡Šä½œä¸ºç›®æ ‡ï¼Œè®­ç»ƒè¾ƒå¼±çš„LLMï¼Œä½¿å…¶åœ¨é£æ ¼å’Œè¯­ä¹‰ä¸Šä¸æ›´å¼ºå¤§çš„æ¨¡å‹å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨LLMsçš„è‡ªå¢å¼ºèƒ½åŠ›æ¥ç”Ÿæˆåˆæˆè§£é‡Šï¼Œå¹¶å°†å…¶ç”¨äºæå‡æ¯’æ€§æ£€æµ‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—å‡å°‘å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡è·¨æ¨¡å‹è®­ç»ƒï¼Œå¯ä»¥å°†æ›´å¼ºå¤§çš„LLMçš„çŸ¥è¯†è¿ç§»åˆ°è¾ƒå¼±çš„LLMï¼Œä»è€Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨æç¤ºå·¥ç¨‹æ¥å¼•å¯¼LLMç”Ÿæˆé«˜è´¨é‡çš„è§£é‡Šã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚Direct Preference Optimization, DPOï¼‰æ¥å¯¹é½LLMçš„é¢„æµ‹å’Œè§£é‡Šã€‚åœ¨ç¬¬ä¸‰é˜¶æ®µï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒè¾ƒå¼±çš„LLMï¼Œä½¿å…¶æ¨¡ä»¿æ›´å¼ºå¤§çš„LLMçš„é¢„æµ‹å’Œè§£é‡Šã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSMARTERæ¡†æ¶åœ¨HateXplainã€Latent Hateå’ŒImplicit Hateä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨HateXplainæ•°æ®é›†ä¸Šï¼ŒSMARTERæ¡†æ¶æ¯”æ ‡å‡†çš„å°æ ·æœ¬åŸºçº¿æé«˜äº†é«˜è¾¾13.5%çš„å®å¹³å‡F1å€¼ã€‚æ­¤å¤–ï¼ŒSMARTERæ¡†æ¶åœ¨æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°ä¸ä½¿ç”¨å¤§é‡æ•°æ®è®­ç»ƒçš„æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SMARTERæ¡†æ¶å¯åº”ç”¨äºç¤¾äº¤åª’ä½“å¹³å°ã€åœ¨çº¿è®ºå›ç­‰åœºæ™¯ï¼Œç”¨äºè‡ªåŠ¨æ£€æµ‹å’Œè¿‡æ»¤æœ‰å®³å†…å®¹ï¼Œæå‡å†…å®¹å®¡æ ¸æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºä½èµ„æºè¯­è¨€æˆ–æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ï¼Œæœ‰åŠ©äºæ„å»ºæ›´å®‰å…¨ã€æ›´å¥åº·çš„ç½‘ç»œç¯å¢ƒã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯æ‰©å±•åˆ°å…¶ä»–æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œå¹¶ä¸å…¶ä»–æŠ€æœ¯ï¼ˆå¦‚ä¸»åŠ¨å­¦ä¹ ï¼‰ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> WARNING: This paper contains examples of offensive materials. To address the proliferation of toxic content on social media, we introduce SMARTER, we introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.

