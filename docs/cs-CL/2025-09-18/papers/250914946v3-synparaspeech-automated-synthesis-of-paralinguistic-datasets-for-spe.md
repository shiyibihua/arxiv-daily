---
layout: default
title: SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding
---

# SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14946" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14946v3</a>
  <a href="https://arxiv.org/pdf/2509.14946.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14946v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14946v3', 'SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bingsong Bai, Qihang Lu, Wenbing Yang, Zihan Sun, Yueran Hou, Peilei Jia, Songbai Pu, Ruibo Fu, Yingming Gao, Ya Li, Jun Gao

**åˆ†ç±»**: eess.AS, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-09-28)

**å¤‡æ³¨**: Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ShawnPi233/SynParaSpeech)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSynParaSpeechæ¡†æ¶ï¼Œè‡ªåŠ¨åˆæˆå¤§è§„æ¨¡å‰¯è¯­è¨€æ•°æ®é›†ï¼Œæå‡è¯­éŸ³ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‰¯è¯­è¨€` `è¯­éŸ³åˆæˆ` `è¯­éŸ³ç†è§£` `æ•°æ®é›†æ„å»º` `è‡ªåŠ¨åŒ–æ ‡æ³¨` `è‡ªç„¶å¯¹è¯è¯­éŸ³`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å‰¯è¯­è¨€è¯­éŸ³åˆæˆæ–¹æ³•ä¾èµ–ç§æœ‰æ•°æ®é›†ï¼Œå…¬å¼€æ•°æ®é›†å­˜åœ¨è¯­éŸ³ä¸å®Œæ•´ã€æ—¶é—´æˆ³ä¸å‡†ç¡®ç­‰é—®é¢˜ã€‚
2. æå‡ºè‡ªåŠ¨åŒ–æ¡†æ¶SynParaSpeechï¼Œä»è‡ªç„¶å¯¹è¯è¯­éŸ³ä¸­ç”Ÿæˆå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å‰¯è¯­è¨€æ•°æ®é›†ã€‚
3. æ„å»ºåŒ…å«6ç§å‰¯è¯­è¨€ç±»åˆ«ã€æ—¶é•¿118.75å°æ—¶çš„SynParaSpeechæ•°æ®é›†ï¼Œæå‡è¯­éŸ³ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤§è§„æ¨¡çš„å‰¯è¯­è¨€æ•°æ®ï¼Œå¹¶åˆ©ç”¨è¯¥æ¡†æ¶æ„å»ºäº†SynParaSpeechæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«6ç§å‰¯è¯­è¨€ç±»åˆ«ï¼Œæ€»æ—¶é•¿è¾¾118.75å°æ—¶ï¼Œå¹¶å…·æœ‰ç²¾ç¡®çš„æ—¶é—´æˆ³ï¼Œæ‰€æœ‰æ•°æ®å‡æ¥æºäºè‡ªç„¶å¯¹è¯è¯­éŸ³ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åœ¨äºæå‡ºäº†é¦–ä¸ªç”¨äºæ„å»ºå¤§è§„æ¨¡å‰¯è¯­è¨€æ•°æ®é›†çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œå¹¶å‘å¸ƒäº†SynParaSpeechè¯­æ–™åº“ã€‚è¯¥è¯­æ–™åº“é€šè¿‡æ›´è‡ªç„¶çš„å‰¯è¯­è¨€åˆæˆæ¥æ”¹è¿›è¯­éŸ³ç”Ÿæˆï¼Œå¹¶é€šè¿‡æé«˜å‰¯è¯­è¨€äº‹ä»¶æ£€æµ‹æ¥å¢å¼ºè¯­éŸ³ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å‰¯è¯­è¨€è¯­éŸ³åˆæˆå’Œç†è§£æ–¹æ³•é¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚å…¬å¼€æ•°æ®é›†é€šå¸¸è´¨é‡ä¸é«˜ï¼Œå­˜åœ¨è¯­éŸ³ä¸å®Œæ•´ã€æ—¶é—´æˆ³ä¸å‡†ç¡®æˆ–ç¼ºå¤±ç­‰é—®é¢˜ï¼Œä¸”çœŸå®åœºæ™¯ç›¸å…³æ€§æœ‰é™ã€‚ç§æœ‰æ•°æ®é›†è™½ç„¶è´¨é‡è¾ƒé«˜ï¼Œä½†éš¾ä»¥è·å–å’Œå¤ç°ï¼Œé˜»ç¢äº†ç›¸å…³ç ”ç©¶çš„è¿›å±•ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å…·æœ‰ç²¾ç¡®æ—¶é—´æˆ³çš„å‰¯è¯­è¨€æ•°æ®é›†çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªåŠ¨åŒ–æµç¨‹ï¼Œä»å¤§é‡çš„è‡ªç„¶å¯¹è¯è¯­éŸ³ä¸­æå–å¹¶æ ‡æ³¨å‰¯è¯­è¨€äº‹ä»¶ï¼Œä»è€Œæ„å»ºå¤§è§„æ¨¡çš„å‰¯è¯­è¨€æ•°æ®é›†ã€‚é€šè¿‡è‡ªåŠ¨åŒ–ï¼Œå¯ä»¥é™ä½äººå·¥æ ‡æ³¨çš„æˆæœ¬ï¼Œæé«˜æ•°æ®ç”Ÿæˆçš„æ•ˆç‡ï¼Œå¹¶ä¿è¯æ•°æ®çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹ç§æœ‰æ•°æ®é›†çš„ä¾èµ–ï¼Œä¸ºå‰¯è¯­è¨€è¯­éŸ³åˆæˆå’Œç†è§£ç ”ç©¶æä¾›äº†å¯é çš„æ•°æ®åŸºç¡€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSynParaSpeechæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è¯­éŸ³æ•°æ®æ”¶é›†ï¼šæ”¶é›†å¤§é‡çš„è‡ªç„¶å¯¹è¯è¯­éŸ³æ•°æ®ã€‚2) å‰¯è¯­è¨€äº‹ä»¶æ£€æµ‹ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„å‰¯è¯­è¨€äº‹ä»¶æ£€æµ‹æ¨¡å‹ï¼Œè‡ªåŠ¨æ£€æµ‹è¯­éŸ³æ•°æ®ä¸­çš„å‰¯è¯­è¨€äº‹ä»¶ã€‚3) æ—¶é—´æˆ³æ ¡æ­£ï¼šå¯¹æ£€æµ‹åˆ°çš„å‰¯è¯­è¨€äº‹ä»¶çš„æ—¶é—´æˆ³è¿›è¡Œæ ¡æ­£ï¼Œæé«˜æ—¶é—´æˆ³çš„å‡†ç¡®æ€§ã€‚4) æ•°æ®è¿‡æ»¤ï¼šå¯¹æ£€æµ‹åˆ°çš„å‰¯è¯­è¨€äº‹ä»¶è¿›è¡Œè¿‡æ»¤ï¼Œå»é™¤è´¨é‡è¾ƒå·®çš„æ•°æ®ã€‚5) æ•°æ®é›†æ„å»ºï¼šå°†è¿‡æ»¤åçš„æ•°æ®æ•´ç†æˆæ•°æ®é›†ï¼Œå¹¶æä¾›ç²¾ç¡®çš„æ—¶é—´æˆ³ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†é¦–ä¸ªç”¨äºæ„å»ºå¤§è§„æ¨¡å‰¯è¯­è¨€æ•°æ®é›†çš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹å’Œæ ‡æ³¨å‰¯è¯­è¨€äº‹ä»¶ï¼Œå¹¶ç”Ÿæˆå…·æœ‰ç²¾ç¡®æ—¶é—´æˆ³çš„æ•°æ®é›†ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€äººå·¥æ ‡æ³¨ï¼Œå¯ä»¥å¤§å¤§é™ä½æ•°æ®ç”Ÿæˆçš„æˆæœ¬ï¼Œå¹¶æé«˜æ•°æ®ç”Ÿæˆçš„æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‰¯è¯­è¨€äº‹ä»¶æ£€æµ‹æ¨¡å—ä¸­ï¼Œå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¾‹å¦‚åŸºäºTransformerçš„æ¨¡å‹ã€‚æ—¶é—´æˆ³æ ¡æ­£æ¨¡å—å¯ä»¥ä½¿ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰ç­‰æŠ€æœ¯ï¼Œå¯¹æ—¶é—´æˆ³è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚æ•°æ®è¿‡æ»¤æ¨¡å—å¯ä»¥æ ¹æ®è¯­éŸ³è´¨é‡ã€ä¿¡å™ªæ¯”ç­‰æŒ‡æ ‡ï¼Œå»é™¤è´¨é‡è¾ƒå·®çš„æ•°æ®ã€‚æ•°æ®é›†æ„å»ºæ¨¡å—éœ€è¦è®¾è®¡åˆç†çš„æ•°æ®æ ¼å¼ï¼Œæ–¹ä¾¿åç»­ç ”ç©¶ä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SynParaSpeechæ•°æ®é›†åŒ…å«6ç§å‰¯è¯­è¨€ç±»åˆ«ï¼Œæ€»æ—¶é•¿è¾¾118.75å°æ—¶ï¼Œæ˜¯ç›®å‰æœ€å¤§çš„å…¬å¼€å‰¯è¯­è¨€æ•°æ®é›†ä¹‹ä¸€ã€‚è¯¥æ•°æ®é›†å…·æœ‰ç²¾ç¡®çš„æ—¶é—´æˆ³ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒé«˜ç²¾åº¦çš„å‰¯è¯­è¨€äº‹ä»¶æ£€æµ‹æ¨¡å‹ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼Œåˆ©ç”¨SynParaSpeechæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨å‰¯è¯­è¨€äº‹ä»¶æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè¯­éŸ³åˆæˆã€è¯­éŸ³è¯†åˆ«ã€æƒ…æ„Ÿè®¡ç®—ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è¯­éŸ³åˆæˆä¸­ï¼Œå¯ä»¥åˆ©ç”¨SynParaSpeechæ•°æ®é›†è®­ç»ƒæ¨¡å‹ï¼Œç”Ÿæˆæ›´è‡ªç„¶ã€æ›´å¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚åœ¨è¯­éŸ³è¯†åˆ«ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ•°æ®é›†æé«˜æ¨¡å‹å¯¹å‰¯è¯­è¨€äº‹ä»¶çš„è¯†åˆ«èƒ½åŠ›ï¼Œä»è€Œæé«˜è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®ç‡ã€‚åœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ•°æ®é›†æ„å»ºæ›´æ™ºèƒ½ã€æ›´äººæ€§åŒ–çš„å¯¹è¯ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing more realistic and engaging speech. However, existing methods typically depend on proprietary datasets, while publicly available resources often suffer from incomplete speech, inaccurate or missing timestamps, and limited real-world relevance. To address these problems, we propose an automated framework for generating large-scale paralinguistic data and apply it to construct the SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with 118.75 hours of data and precise timestamps, all derived from natural conversational speech. Our contributions lie in introducing the first automated method for constructing large-scale paralinguistic datasets and releasing the SynParaSpeech corpus, which advances speech generation through more natural paralinguistic synthesis and enhances speech understanding by improving paralinguistic event detection. The dataset and audio samples are available at https://github.com/ShawnPi233/SynParaSpeech.

