---
layout: default
title: Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models
---

# Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15216" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15216v2</a>
  <a href="https://arxiv.org/pdf/2509.15216.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15216v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15216v2', 'Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sreejato Chatterjee, Linh Tran, Quoc Duy Nguyen, Roni Kirson, Drue Hamlin, Harvest Aquino, Hanjia Lyu, Jiebo Luo, Timothy Dye

**åˆ†ç±»**: cs.CL, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-11-23)

**å¤‡æ³¨**: To appear in the 2025 IEEE International Conference on Big Data (IEEE BigData 2025)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/chattergpt/HSO-Bench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨è§„åˆ™å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°å…¨çƒå†å²ç»“æ„æ€§å‹è¿«**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å†å²ç»“æ„æ€§å‹è¿«` `è§„åˆ™å¼•å¯¼` `ç¤¾ä¼šå…¬å¹³` `è·¨æ–‡åŒ–ç ”ç©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¡¡é‡è·¨å›½å†å²ç»“æ„æ€§å‹è¿«æ—¶ï¼Œç¼ºä¹å¯¹å„å›½ç‹¬ç‰¹å†å²èƒŒæ™¯çš„è€ƒè™‘ï¼Œä¸”è¿‡åº¦ä¾èµ–ç‰©è´¨èµ„æºæŒ‡æ ‡ã€‚
2. è¯¥ç ”ç©¶æå‡ºåˆ©ç”¨è§„åˆ™å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æå–ä¸Šä¸‹æ–‡ç›¸å…³çš„å‹è¿«ä¿¡æ¯ï¼Œç”Ÿæˆå¯è§£é‡Šçš„å‹è¿«è¯„ä¼°åˆ†æ•°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡è§„åˆ™å¼•å¯¼ï¼Œå¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰åˆ°å›½å®¶å†…éƒ¨åŸºäºèº«ä»½çš„å†å²å‹è¿«ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è·¨æ–‡åŒ–è§†è§’ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿä¸Šè¡¡é‡å†å²ç»“æ„æ€§å‹è¿«çš„æ–¹æ³•ï¼Œç”±äºå„å›½ç‹¬ç‰¹çš„æ’æ–¥ã€æ®–æ°‘å’Œç¤¾ä¼šåœ°ä½å†å²ï¼Œåœ¨è·¨å›½æœ‰æ•ˆæ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”é€šå¸¸ä¾èµ–äºä¼˜å…ˆè€ƒè™‘ç‰©è´¨èµ„æºçš„ç»“æ„åŒ–æŒ‡æ ‡ï¼Œè€Œå¿½ç•¥äº†åŸºäºèº«ä»½çš„ç”Ÿæ´»ä½“éªŒå¼æ’æ–¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å‹è¿«æµ‹é‡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè·¨ä¸åŒåœ°ç¼˜æ”¿æ²»èƒŒæ™¯ä¸‹ç”Ÿæ´»ä½“éªŒå¼å†å²åŠ£åŠ¿çš„ä¸Šä¸‹æ–‡æ•æ„Ÿåˆ†æ•°ã€‚é€šè¿‡ä½¿ç”¨æ¥è‡ªå¤šè¯­è¨€COVID-19å…¨çƒç ”ç©¶çš„éç»“æ„åŒ–è‡ªæˆ‘è®¤åŒçš„ç§æ—è¯­æ–™ï¼Œæˆ‘ä»¬è®¾è®¡äº†è§„åˆ™å¼•å¯¼çš„æç¤ºç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆå¯è§£é‡Šçš„ã€ç†è®ºä¸Šåˆç†çš„å‹è¿«ä¼°è®¡ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†å¤šä¸ªæœ€å…ˆè¿›çš„LLMä¸Šçš„è¿™äº›ç­–ç•¥ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ˜ç¡®è§„åˆ™çš„æŒ‡å¯¼ä¸‹ï¼ŒLLMå¯ä»¥æ•æ‰åˆ°å›½å®¶å†…éƒ¨åŸºäºèº«ä»½çš„å†å²å‹è¿«çš„ç»†å¾®å½¢å¼ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ç§è¡¥å……æµ‹é‡å·¥å…·ï¼Œçªå‡ºäº†ç³»ç»Ÿæ€§æ’æ–¥çš„ç»´åº¦ï¼Œä¸ºç†è§£å‹è¿«å¦‚ä½•åœ¨æ•°æ®é©±åŠ¨çš„ç ”ç©¶å’Œå…¬å…±å«ç”ŸèƒŒæ™¯ä¸‹è¡¨ç°æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„è·¨æ–‡åŒ–è§†è§’ã€‚ä¸ºäº†æ”¯æŒå¯é‡å¤çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªå¼€æºåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°LLMåœ¨å‹è¿«æµ‹é‡æ–¹é¢çš„èƒ½åŠ›ï¼ˆhttps://github.com/chattergpt/HSO-Benchï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨è·¨å›½è¯„ä¼°å†å²ç»“æ„æ€§å‹è¿«æ—¶å­˜åœ¨çš„å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç»“æ„åŒ–çš„æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡å¯èƒ½æ— æ³•æ•æ‰åˆ°å„å›½ç‹¬ç‰¹çš„å†å²èƒŒæ™¯å’Œç¤¾ä¼šæ–‡åŒ–å·®å¼‚ï¼Œå¹¶ä¸”å®¹æ˜“å¿½ç•¥åŸºäºèº«ä»½è®¤åŒçš„ç”Ÿæ´»ä½“éªŒå¼æ’æ–¥ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæ›´çµæ´»ã€æ›´å…·ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„æ–¹æ³•æ¥è¡¡é‡å†å²ç»“æ„æ€§å‹è¿«ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡è§„åˆ™å¼•å¯¼çš„æç¤ºç­–ç•¥ï¼Œä½¿LLMèƒ½å¤Ÿä»éç»“æ„åŒ–çš„æ–‡æœ¬æ•°æ®ä¸­æå–ä¸å‹è¿«ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆå¯è§£é‡Šçš„å‹è¿«è¯„ä¼°åˆ†æ•°ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå°†LLMè§†ä¸ºä¸€ç§çŸ¥è¯†åº“å’Œæ¨ç†å¼•æ“ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œå¼•å¯¼å…¶è¾“å‡ºç¬¦åˆç†è®ºåŸºç¡€çš„å‹è¿«è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ï¼šä»å¤šè¯­è¨€COVID-19å…¨çƒç ”ç©¶ä¸­æ”¶é›†åŒ…å«è‡ªæˆ‘è®¤åŒçš„ç§æ—ä¿¡æ¯çš„éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ã€‚2) è§„åˆ™è®¾è®¡ï¼šè®¾è®¡ä¸€ç³»åˆ—è§„åˆ™ï¼Œç”¨äºæŒ‡å¯¼LLMè¿›è¡Œå‹è¿«è¯„ä¼°ï¼Œè¿™äº›è§„åˆ™åŸºäºå†å²å’Œç¤¾ä¼šå­¦ç†è®ºï¼Œä¾‹å¦‚ï¼ŒæŸäº›ç§æ—ç¾¤ä½“åœ¨ç‰¹å®šå›½å®¶å†å²ä¸Šé­å—äº†ç³»ç»Ÿæ€§çš„æ­§è§†å’Œå‹è¿«ã€‚3) æç¤ºå·¥ç¨‹ï¼šæ ¹æ®è®¾è®¡çš„è§„åˆ™ï¼Œæ„å»ºé’ˆå¯¹LLMçš„æç¤ºï¼Œä¾‹å¦‚ï¼Œâ€œåœ¨[å›½å®¶]ä¸­ï¼Œ[ç§æ—]ç¾¤ä½“åœ¨å†å²ä¸Šæ˜¯å¦é­å—è¿‡å‹è¿«ï¼Ÿè¯·ç»™å‡ºè§£é‡Šã€‚â€4) æ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨ä¸åŒçš„LLMï¼ˆä¾‹å¦‚ï¼ŒGPT-3, PaLMï¼‰å¯¹æç¤ºè¿›è¡Œè¯„ä¼°ï¼Œå¹¶åˆ†æå…¶è¾“å‡ºç»“æœçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚5) åŸºå‡†æ•°æ®é›†æ„å»ºï¼šæ„å»ºä¸€ä¸ªå¼€æºåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°LLMåœ¨å‹è¿«æµ‹é‡æ–¹é¢çš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå°†LLMåº”ç”¨äºå†å²ç»“æ„æ€§å‹è¿«çš„è¯„ä¼°ï¼Œå¹¶æå‡ºäº†ä¸€ç§è§„åˆ™å¼•å¯¼çš„æç¤ºç­–ç•¥ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š1) èƒ½å¤Ÿå¤„ç†éç»“æ„åŒ–çš„æ–‡æœ¬æ•°æ®ï¼Œä»è€Œå¯ä»¥åˆ©ç”¨æ›´å¹¿æ³›çš„æ•°æ®æ¥æºã€‚2) å…·æœ‰æ›´å¼ºçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œèƒ½å¤Ÿè€ƒè™‘å„å›½ç‹¬ç‰¹çš„å†å²èƒŒæ™¯å’Œç¤¾ä¼šæ–‡åŒ–å·®å¼‚ã€‚3) å…·æœ‰æ›´å¥½çš„å¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿæä¾›å‹è¿«è¯„ä¼°çš„ç†ç”±å’Œä¾æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è§„åˆ™çš„è®¾è®¡ï¼šè§„åˆ™éœ€è¦åŸºäºæ‰å®çš„ç†è®ºåŸºç¡€ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ¸…æ™°åœ°æŒ‡å¯¼LLMè¿›è¡Œå‹è¿«è¯„ä¼°ã€‚2) æç¤ºçš„è®¾è®¡ï¼šæç¤ºéœ€è¦ç®€æ´æ˜äº†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼LLMè¾“å‡ºæ‰€éœ€çš„ä¿¡æ¯ã€‚3) æ¨¡å‹é€‰æ‹©ï¼šé€‰æ‹©å…·æœ‰è¾ƒå¼ºè¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„LLMï¼Œä¾‹å¦‚ï¼ŒGPT-3, PaLMã€‚4) è¯„ä¼°æŒ‡æ ‡ï¼šä½¿ç”¨å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°LLMçš„è¾“å‡ºç»“æœï¼Œä¾‹å¦‚ï¼Œå‡†ç¡®ç‡ã€ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç ”ç©¶é€šè¿‡å®éªŒéªŒè¯äº†è§„åˆ™å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹åœ¨è¯„ä¼°å†å²ç»“æ„æ€§å‹è¿«æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é€‚å½“çš„è§„åˆ™å¼•å¯¼ä¸‹ï¼ŒLLMèƒ½å¤Ÿæ•æ‰åˆ°å›½å®¶å†…éƒ¨åŸºäºèº«ä»½çš„å†å²å‹è¿«ï¼Œå¹¶ç”Ÿæˆå¯è§£é‡Šçš„å‹è¿«è¯„ä¼°åˆ†æ•°ã€‚è¯¥ç ”ç©¶è¿˜å‘å¸ƒäº†ä¸€ä¸ªå¼€æºåŸºå‡†æ•°æ®é›†ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†ä¾¿åˆ©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå…¬å…±å«ç”Ÿã€ç¤¾ä¼šç§‘å­¦ã€äººæƒç ”ç©¶ç­‰é¢†åŸŸã€‚é€šè¿‡é‡åŒ–å†å²ç»“æ„æ€§å‹è¿«ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£ç¤¾ä¼šä¸å¹³ç­‰ç°è±¡çš„æ ¹æºï¼Œä¸ºåˆ¶å®šæ›´æœ‰æ•ˆçš„æ”¿ç­–å¹²é¢„æªæ–½æä¾›ä¾æ®ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºç›‘æµ‹ç¤¾ä¼šæ­§è§†å’Œåè§ï¼Œä¸ºä¿ƒè¿›ç¤¾ä¼šå…¬å¹³å’Œæ­£ä¹‰åšå‡ºè´¡çŒ®ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å½¢å¼çš„å‹è¿«ï¼Œä¾‹å¦‚æ€§åˆ«æ­§è§†ã€å®—æ•™æ­§è§†ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (https://github.com/chattergpt/HSO-Bench).

