---
layout: default
title: Explicit vs. Implicit Biographies: Evaluating and Adapting LLM Information Extraction on Wikidata-Derived Texts
---

# Explicit vs. Implicit Biographies: Evaluating and Adapting LLM Information Extraction on Wikidata-Derived Texts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14943" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14943v1</a>
  <a href="https://arxiv.org/pdf/2509.14943.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14943v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14943v1', 'Explicit vs. Implicit Biographies: Evaluating and Adapting LLM Information Extraction on Wikidata-Derived Texts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alessandra Stramiglio, Andrea Schimmenti, Valentina Pasqual, Marieke van Erp, Francesco Sovrano, Fabio Vitali

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡LoRAå¾®è°ƒæå‡LLMåœ¨Wikidataæ–‡æœ¬ä¿¡æ¯æŠ½å–ä¸­å¤„ç†éšå¼ä¿¡æ¯çš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¿¡æ¯æŠ½å–` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬éšå¼æ€§` `LoRAå¾®è°ƒ` `Wikidata` `çŸ¥è¯†å›¾è°±` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸNLPæ–¹æ³•ä¾èµ–æ˜¾å¼å£°æ˜è¯†åˆ«å®ä½“åŠå…¶å…³ç³»ï¼Œè€Œæ–‡æœ¬éšå¼æ€§å¯¹ä¿¡æ¯æŠ½å–æ„æˆæŒ‘æˆ˜ï¼Œä¾‹å¦‚ç†è§£â€œZuhdiæ¯å‘¨æ—¥å»æ•™å ‚â€ä¸­Zuhdiä¸åŸºç£æ•™çš„å…³ç³»ã€‚
2. è®ºæ–‡æå‡ºä½¿ç”¨LoRAå¾®è°ƒLLMï¼Œä½¿å…¶æ›´å¥½åœ°ç†è§£å’ŒæŠ½å–éšå¼æ–‡æœ¬ä¸­çš„ä¿¡æ¯ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨éšå¼æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡åœ¨åŒ…å«éšå¼ä¿¡æ¯çš„åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒLLMï¼Œå¯ä»¥æœ‰æ•ˆæé«˜æ¨¡å‹åœ¨ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­å¤„ç†éšå¼ä¿¡æ¯çš„èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æ–‡æœ¬éšå¼æ€§å¯¹é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¿¡æ¯æŠ½å–ï¼ˆIEï¼‰ä»»åŠ¡çš„å½±å“ï¼Œé€‰ç”¨çš„æ¨¡å‹åŒ…æ‹¬LLaMA 2.3ã€DeepSeekV1å’ŒPhi1.5ã€‚ä½œè€…æ„å»ºäº†ä¸¤ä¸ªåŒ…å«1ä¸‡æ¡éšå¼å’Œæ˜¾å¼ä¼ è®°ä¿¡æ¯çš„åˆæˆæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬éšå¼æ€§å¯¹LLMæ€§èƒ½çš„å½±å“ï¼Œå¹¶åˆ†æäº†ä½¿ç”¨éšå¼æ•°æ®è¿›è¡Œå¾®è°ƒæ˜¯å¦èƒ½æé«˜æ¨¡å‹åœ¨éšå¼æ¨ç†ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶é€šè¿‡å®éªŒæ¢ç´¢äº†LLMåœ¨IEä¸­å¤„ç†éšå¼å’Œæ˜¾å¼ä¸Šä¸‹æ–‡çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LoRAï¼ˆä½ç§©é€‚åº”ï¼‰å¾®è°ƒLLMæ¨¡å‹å¯ä»¥æé«˜å…¶ä»éšå¼æ–‡æœ¬ä¸­æå–ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä»è€Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­ï¼Œå¤„ç†æ–‡æœ¬éšå¼æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éœ€è¦æ¨ç†æ‰èƒ½è·å¾—çš„éšå«å…³ç³»æ—¶è¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚ä»â€œæŸäººç»å¸¸å»æ•™å ‚â€æ¨æ–­å…¶å®—æ•™ä¿¡ä»°ã€‚è¿™ç§éšå¼ä¿¡æ¯çš„ç¼ºå¤±ä¼šä¸¥é‡å½±å“ä¿¡æ¯æŠ½å–çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒLLMï¼Œä½¿å…¶æ›´å¥½åœ°ç†è§£å’Œå¤„ç†éšå¼æ–‡æœ¬ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡æ„å»ºåŒ…å«å¤§é‡éšå¼ä¿¡æ¯çš„åˆæˆæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨LoRAï¼ˆLow-Rank Adaptationï¼‰æŠ€æœ¯å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨éšå¼æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯è®©æ¨¡å‹å­¦ä¹ åˆ°ä»ä¸Šä¸‹æ–‡æ¨æ–­éšå«å…³ç³»çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ„å»ºåŒ…å«æ˜¾å¼å’Œéšå¼ä¿¡æ¯çš„åˆæˆæ•°æ®é›†ï¼Œæ•°æ®é›†æ¥æºäºWikidataï¼›2) é€‰æ‹©é¢„è®­ç»ƒLLMï¼ŒåŒ…æ‹¬LLaMA 2.3ã€DeepSeekV1å’ŒPhi1.5ï¼›3) ä½¿ç”¨LoRAæŠ€æœ¯åœ¨åˆæˆæ•°æ®é›†ä¸Šå¯¹LLMè¿›è¡Œå¾®è°ƒï¼›4) è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹åœ¨ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œé‡ç‚¹å…³æ³¨å¤„ç†éšå¼ä¿¡æ¯çš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªè¯„ä¼°LLMå¤„ç†éšå¼ä¿¡æ¯èƒ½åŠ›çš„å®éªŒæ¡†æ¶ï¼›2) ä½¿ç”¨åˆæˆæ•°æ®é›†å’ŒLoRAå¾®è°ƒæ–¹æ³•ï¼Œæœ‰æ•ˆæé«˜äº†LLMåœ¨éšå¼ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´æ³¨é‡æå‡æ¨¡å‹å¯¹æ–‡æœ¬æ·±å±‚å«ä¹‰çš„ç†è§£èƒ½åŠ›ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¾èµ–äºæ˜¾å¼è¡¨è¾¾ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åˆæˆæ•°æ®é›†çš„æ„å»ºæ–¹å¼ï¼Œç¡®ä¿åŒ…å«è¶³å¤Ÿæ•°é‡çš„éšå¼ä¿¡æ¯ï¼›2) LoRAå¾®è°ƒçš„å‚æ•°è®¾ç½®ï¼Œä¾‹å¦‚LoRAç§©çš„å¤§å°ã€å­¦ä¹ ç‡ç­‰ï¼›3) è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹åœ¨å¤„ç†éšå¼ä¿¡æ¯æ—¶çš„å‡†ç¡®ç‡å’Œå¬å›ç‡ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰æ›´è¯¦ç»†çš„æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LoRAå¾®è°ƒLLMå¯ä»¥æ˜¾è‘—æé«˜å…¶åœ¨éšå¼ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒåï¼ŒLLMåœ¨å¤„ç†éšå¼ä¿¡æ¯æ—¶çš„å‡†ç¡®ç‡å’Œå¬å›ç‡å‡å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚è®ºæ–‡å¯¹æ¯”äº†ä¸åŒLLMçš„æ€§èƒ½ï¼Œå¹¶åˆ†æäº†LoRAå¾®è°ƒå¯¹æ¨¡å‹å†…éƒ¨æ¨ç†è¿‡ç¨‹çš„å½±å“ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰æ›´è¯¦ç»†çš„æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºçŸ¥è¯†å›¾è°±æ„å»ºã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€èˆ†æƒ…åˆ†æç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜LLMå¯¹éšå¼ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ä»æ–‡æœ¬ä¸­æå–ä¿¡æ¯ï¼Œä»è€Œæå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»ç–—é¢†åŸŸï¼Œå¯ä»¥ä»ç—…å†ä¸­æå–æ‚£è€…çš„æ½œåœ¨ç–¾ç—…é£é™©ï¼›åœ¨é‡‘èé¢†åŸŸï¼Œå¯ä»¥ä»æ–°é—»æŠ¥é“ä¸­è¯†åˆ«å…¬å¸çš„æ½œåœ¨é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text Implicitness has always been challenging in Natural Language Processing (NLP), with traditional methods relying on explicit statements to identify entities and their relationships. From the sentence "Zuhdi attends church every Sunday", the relationship between Zuhdi and Christianity is evident for a human reader, but it presents a challenge when it must be inferred automatically. Large language models (LLMs) have proven effective in NLP downstream tasks such as text comprehension and information extraction (IE).
>   This study examines how textual implicitness affects IE tasks in pre-trained LLMs: LLaMA 2.3, DeepSeekV1, and Phi1.5. We generate two synthetic datasets of 10k implicit and explicit verbalization of biographic information to measure the impact on LLM performance and analyze whether fine-tuning implicit data improves their ability to generalize in implicit reasoning tasks.
>   This research presents an experiment on the internal reasoning processes of LLMs in IE, particularly in dealing with implicit and explicit contexts. The results demonstrate that fine-tuning LLM models with LoRA (low-rank adaptation) improves their performance in extracting information from implicit texts, contributing to better model interpretability and reliability.

