---
layout: default
title: Fair-GPTQ: Bias-Aware Quantization for Large Language Models
---

# Fair-GPTQ: Bias-Aware Quantization for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15206" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15206v1</a>
  <a href="https://arxiv.org/pdf/2509.15206.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15206v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15206v1', 'Fair-GPTQ: Bias-Aware Quantization for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Irina Proskurina, Guillaume Metzler, Julien Velcin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Fair-GPTQï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„åè§æ„ŸçŸ¥é‡åŒ–æ–¹æ³•ï¼Œæå‡å…¬å¹³æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é‡åŒ–` `å…¬å¹³æ€§` `åè§ç¼“è§£` `ç¾¤ä½“å…¬å¹³æ€§` `GPTQ` `æ¨¡å‹éƒ¨ç½²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰GPTQé‡åŒ–æ–¹æ³•åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå¯èƒ½å¢åŠ å¤§è¯­è¨€æ¨¡å‹çš„åè§è¾“å‡ºï¼ŒæŸå®³å…¬å¹³æ€§ã€‚
2. Fair-GPTQé€šè¿‡åœ¨é‡åŒ–ç›®æ ‡ä¸­åŠ å…¥ç¾¤ä½“å…¬å¹³æ€§çº¦æŸï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´å…¬å¹³çš„èˆå…¥æ“ä½œï¼Œå‡å°‘åè§ç”Ÿæˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFair-GPTQåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆé™ä½äº†æ€§åˆ«ã€ç§æ—å’Œå®—æ•™ç­‰æ–¹é¢çš„åˆ»æ¿å°è±¡åè§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹å¯¹å†…å­˜çš„é«˜éœ€æ±‚ä¿ƒä½¿äººä»¬å…³æ³¨é‡åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡å°†æ¨¡å‹æƒé‡æ˜ å°„åˆ°è¾ƒä½ç²¾åº¦çš„æ•´æ•°æ¥é™ä½è®¡ç®—æˆæœ¬ã€å†…å­˜ä½¿ç”¨å’Œå»¶è¿Ÿã€‚GPTQç­‰æ–¹æ³•æœ‰æ•ˆåœ°æœ€å°åŒ–äº†é‡åŒ–è¿‡ç¨‹ä¸­çš„è¾“å…¥-æƒé‡ä¹˜ç§¯è¯¯å·®ï¼›ç„¶è€Œï¼Œæœ€è¿‘çš„å®è¯ç ”ç©¶è¡¨æ˜ï¼Œå®ƒä»¬å¯èƒ½ä¼šå¢åŠ æœ‰åè¾“å‡ºï¼Œå¹¶é™ä½åœ¨å…¬å¹³æ€§åŸºå‡†ä¸Šçš„æ€§èƒ½ï¼Œä½†ç›®å‰å°šä¸æ¸…æ¥šå“ªäº›ç‰¹å®šæƒé‡å¯¼è‡´äº†è¿™ä¸ªé—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†æ˜¾å¼çš„ç¾¤ä½“å…¬å¹³æ€§çº¦æŸæ·»åŠ åˆ°é‡åŒ–ç›®æ ‡ä¸­ï¼Œä»è€Œåœ¨é‡åŒ–å’Œæ¨¡å‹å…¬å¹³æ€§ä¹‹é—´å»ºç«‹äº†æ–°çš„è”ç³»ï¼Œå¹¶å¼•å…¥äº†Fair-GPTQï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ˜ç¡®æ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸å…¬å¹³æ€§çš„é‡åŒ–æ–¹æ³•ã€‚æ·»åŠ çš„çº¦æŸå¼•å¯¼èˆå…¥æ“ä½œçš„å­¦ä¹ ï¼Œä»è€Œå‡å°‘å—ä¿æŠ¤ç¾¤ä½“çš„æœ‰åæ–‡æœ¬ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å…³æ³¨æ¶‰åŠèŒä¸šåè§å’Œæ­§è§†æ€§è¯­è¨€ï¼ˆåŒ…æ‹¬æ€§åˆ«ã€ç§æ—å’Œå®—æ•™ï¼‰çš„åˆ»æ¿å°è±¡ç”Ÿæˆã€‚Fair-GPTQå¯¹æ€§èƒ½çš„å½±å“æœ€å°ï¼Œåœ¨zero-shotåŸºå‡†ä¸Šè‡³å°‘ä¿ç•™äº†90%çš„åŸºçº¿å‡†ç¡®ç‡ï¼Œç›¸å¯¹äºåŠç²¾åº¦æ¨¡å‹é™ä½äº†ä¸å…¬å¹³æ€§ï¼Œå¹¶ä¿ç•™äº†4ä½é‡åŒ–çš„å†…å­˜å’Œé€Ÿåº¦ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¿˜å°†Fair-GPTQçš„æ€§èƒ½ä¸ç°æœ‰çš„å»åæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œå‘ç°å®ƒåœ¨ç§æ—åˆ»æ¿å°è±¡åŸºå‡†ä¸Šå®ç°äº†ä¸è¿­ä»£é›¶ç©ºé—´æŠ•å½±å»åæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼Œç»“æœéªŒè¯äº†æˆ‘ä»¬å¸¦æœ‰ç¾¤ä½“åè§é¡¹çš„é‡åŒ–é—®é¢˜çš„ç†è®ºè§£å†³æ–¹æ¡ˆï¼Œçªå‡ºäº†å…¶åœ¨ç”Ÿæˆæ¨¡å‹é‡åŒ–æ—¶å‡å°‘ç¾¤ä½“åè§çš„åº”ç”¨æ€§ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥ç”¨äºåˆ†æé€šé“å’Œæƒé‡çº§åˆ«å¯¹é‡åŒ–è¿‡ç¨‹ä¸­å…¬å¹³æ€§çš„è´¡çŒ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹é‡åŒ–è¿‡ç¨‹ä¸­å¼•å…¥çš„åè§é—®é¢˜ã€‚ç°æœ‰çš„GPTQç­‰é‡åŒ–æ–¹æ³•è™½ç„¶èƒ½æœ‰æ•ˆé™ä½æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä½†ä¼šåŠ å‰§æ¨¡å‹åœ¨æ€§åˆ«ã€ç§æ—ã€å®—æ•™ç­‰æ–¹é¢çš„åè§ï¼Œå¯¼è‡´ä¸å…¬å¹³çš„è¾“å‡ºç»“æœã€‚è¿™äº›æ–¹æ³•æ²¡æœ‰è€ƒè™‘é‡åŒ–å¯¹ä¸åŒç¾¤ä½“çš„å½±å“ï¼Œå¯¼è‡´æŸäº›ç‰¹å®šç¾¤ä½“çš„åˆ©ç›Šå—åˆ°æŸå®³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨é‡åŒ–è¿‡ç¨‹ä¸­å¼•å…¥ç¾¤ä½“å…¬å¹³æ€§çº¦æŸï¼Œä»è€Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´å…¬å¹³çš„é‡åŒ–ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡åœ¨é‡åŒ–ç›®æ ‡å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œæƒ©ç½šé‚£äº›å¯¼è‡´æ¨¡å‹å¯¹ä¸åŒç¾¤ä½“äº§ç”Ÿå·®å¼‚æ€§è¾“å‡ºçš„é‡åŒ–æ–¹æ¡ˆã€‚è¿™æ ·ï¼Œæ¨¡å‹åœ¨è¿½æ±‚é‡åŒ–æ•ˆç‡çš„åŒæ—¶ï¼Œä¹Ÿä¼šå…¼é¡¾å…¬å¹³æ€§ï¼Œä»è€Œå‡å°‘åè§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFair-GPTQçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) ä½¿ç”¨GPTQç­‰æ–¹æ³•è¿›è¡Œåˆæ­¥çš„é‡åŒ–ï¼›2) å®šä¹‰ä¸€ä¸ªç¾¤ä½“å…¬å¹³æ€§åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æ¨¡å‹å¯¹ä¸åŒç¾¤ä½“çš„åè§ç¨‹åº¦ï¼›3) åœ¨é‡åŒ–ç›®æ ‡å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œè¯¥æ­£åˆ™åŒ–é¡¹ä¸ç¾¤ä½“å…¬å¹³æ€§åº¦é‡æŒ‡æ ‡ç›¸å…³ï¼›4) ä½¿ç”¨ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚æ¢¯åº¦ä¸‹é™ï¼‰æ¥è°ƒæ•´é‡åŒ–å‚æ•°ï¼Œä»è€Œæœ€å°åŒ–é‡åŒ–è¯¯å·®å’Œç¾¤ä½“åè§ã€‚

**å…³é”®åˆ›æ–°**ï¼šFair-GPTQçš„å…³é”®åˆ›æ–°åœ¨äºå°†ç¾¤ä½“å…¬å¹³æ€§çº¦æŸæ˜¾å¼åœ°å¼•å…¥åˆ°é‡åŒ–è¿‡ç¨‹ä¸­ã€‚ä¸ç°æœ‰çš„é‡åŒ–æ–¹æ³•ä¸åŒï¼ŒFair-GPTQä¸ä»…å…³æ³¨é‡åŒ–è¯¯å·®ï¼Œè¿˜å…³æ³¨é‡åŒ–å¯¹æ¨¡å‹å…¬å¹³æ€§çš„å½±å“ã€‚é€šè¿‡åœ¨é‡åŒ–ç›®æ ‡å‡½æ•°ä¸­åŠ å…¥æ­£åˆ™åŒ–é¡¹ï¼ŒFair-GPTQèƒ½å¤Ÿå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´å…¬å¹³çš„é‡åŒ–ç­–ç•¥ï¼Œä»è€Œå‡å°‘åè§ã€‚

**å…³é”®è®¾è®¡**ï¼šFair-GPTQçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç¾¤ä½“å…¬å¹³æ€§åº¦é‡æŒ‡æ ‡çš„é€‰æ‹©ï¼šè®ºæ–‡å¯èƒ½ä½¿ç”¨äº†å¤šç§ç¾¤ä½“å…¬å¹³æ€§åº¦é‡æŒ‡æ ‡ï¼Œå¦‚ç»Ÿè®¡å‡ç­‰ã€æœºä¼šå‡ç­‰å’Œé¢„æµ‹å‡ç­‰ï¼›2) æ­£åˆ™åŒ–é¡¹çš„è®¾è®¡ï¼šæ­£åˆ™åŒ–é¡¹çš„è®¾è®¡éœ€è¦å¹³è¡¡é‡åŒ–è¯¯å·®å’Œç¾¤ä½“åè§ä¹‹é—´çš„æƒè¡¡ï¼›3) ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©ï¼šè®ºæ–‡å¯èƒ½ä½¿ç”¨äº†ä¸åŒçš„ä¼˜åŒ–ç®—æ³•æ¥è°ƒæ•´é‡åŒ–å‚æ•°ï¼Œå¦‚æ¢¯åº¦ä¸‹é™ã€Adamç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Fair-GPTQåœ¨zero-shotåŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†è‡³å°‘90%çš„åŸºçº¿å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨¡å‹åœ¨æ€§åˆ«ã€ç§æ—å’Œå®—æ•™ç­‰æ–¹é¢çš„åˆ»æ¿å°è±¡åè§ã€‚ä¸åŠç²¾åº¦æ¨¡å‹ç›¸æ¯”ï¼ŒFair-GPTQå‡å°‘äº†ä¸å…¬å¹³æ€§ï¼Œå¹¶ä¿ç•™äº†4ä½é‡åŒ–çš„å†…å­˜å’Œé€Ÿåº¦ä¼˜åŠ¿ã€‚åœ¨ç§æ—åˆ»æ¿å°è±¡åŸºå‡†ä¸Šï¼ŒFair-GPTQçš„æ€§èƒ½ä¸ç°æœ‰çš„è¿­ä»£é›¶ç©ºé—´æŠ•å½±å»åæ–¹æ³•ç›¸å½“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Fair-GPTQå¯åº”ç”¨äºå„ç§éœ€è¦éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨å…¬å¹³æ€§è‡³å…³é‡è¦çš„é¢†åŸŸï¼Œå¦‚æ‹›è˜ã€ä¿¡è´·è¯„ä¼°ã€æ³•å¾‹å’¨è¯¢ç­‰ã€‚é€šè¿‡å‡å°‘æ¨¡å‹åè§ï¼ŒFair-GPTQå¯ä»¥æé«˜å†³ç­–çš„å…¬æ­£æ€§å’Œé€æ˜åº¦ï¼Œé¿å…å¯¹ç‰¹å®šç¾¤ä½“é€ æˆæ­§è§†ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨AIæŠ€æœ¯çš„å…¬å¹³æ€§å’Œå¯ä¿¡èµ–æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> High memory demands of generative language models have drawn attention to quantization, which reduces computational cost, memory usage, and latency by mapping model weights to lower-precision integers. Approaches such as GPTQ effectively minimize input-weight product errors during quantization; however, recent empirical studies show that they can increase biased outputs and degrade performance on fairness benchmarks, and it remains unclear which specific weights cause this issue. In this work, we draw new links between quantization and model fairness by adding explicit group-fairness constraints to the quantization objective and introduce Fair-GPTQ, the first quantization method explicitly designed to reduce unfairness in large language models. The added constraints guide the learning of the rounding operation toward less-biased text generation for protected groups. Specifically, we focus on stereotype generation involving occupational bias and discriminatory language spanning gender, race, and religion. Fair-GPTQ has minimal impact on performance, preserving at least 90% of baseline accuracy on zero-shot benchmarks, reduces unfairness relative to a half-precision model, and retains the memory and speed benefits of 4-bit quantization. We also compare the performance of Fair-GPTQ with existing debiasing methods and find that it achieves performance on par with the iterative null-space projection debiasing approach on racial-stereotype benchmarks. Overall, the results validate our theoretical solution to the quantization problem with a group-bias term, highlight its applicability for reducing group bias at quantization time in generative models, and demonstrate that our approach can further be used to analyze channel- and weight-level contributions to fairness during quantization.

