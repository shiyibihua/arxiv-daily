---
layout: default
title: Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing
---

# Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15361" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15361v1</a>
  <a href="https://arxiv.org/pdf/2509.15361.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15361v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15361v1', 'Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zichen Wu, Hsiu-Yuan Huang, Yunfang Wu

**åˆ†ç±»**: cs.CL, cs.AI, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: Accepted by EMNLP 2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå› æœæ¨æ–­å’Œè‡ªé€‚åº”ä¸“å®¶è·¯ç”±çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å»åæ¡†æ¶ï¼Œæå‡å¤æ‚æ¨ç†ä»»åŠ¡çš„é²æ£’æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å› æœæ¨æ–­` `å»å` `åäº‹å®æ¨ç†` `æ··åˆä¸“å®¶æ¨¡å‹` `é²æ£’æ€§` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ˜“å—è™šå‡ç›¸å…³æ€§å½±å“ï¼Œå¯¼è‡´åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚
2. åˆ©ç”¨å› æœæ¨æ–­åŒºåˆ†æ ¸å¿ƒè¯­ä¹‰å’Œè™šå‡ä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡æ··åˆä¸“å®¶æ¨¡å‹è¿›è¡Œè‡ªé€‚åº”å»åã€‚
3. åœ¨è®½åˆºæ£€æµ‹å’Œæƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šï¼Œè¯¥æ¡†æ¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ç»å¸¸ä¾èµ–äºè™šå‡ç›¸å…³æ€§ï¼Œä»è€Œå‰Šå¼±äº†å…¶åœ¨å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡é€šè¿‡ä¸€ç§æ–°é¢–çš„åŸºäºå› æœä¸­ä»‹çš„å»åæ¡†æ¶ï¼Œè§£å†³äº†MLLMsä¸­è¡¨é¢ç›¸å…³æ€§åå·®çš„å…³é”®æŒ‘æˆ˜ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é€šè¿‡åäº‹å®ç¤ºä¾‹åŒºåˆ†æ ¸å¿ƒè¯­ä¹‰ä¸è™šå‡çš„æ–‡æœ¬å’Œè§†è§‰ä¸Šä¸‹æ–‡ï¼Œä»¥æ¿€æ´»è®­ç»ƒé˜¶æ®µçš„å»åï¼Œå¹¶é‡‡ç”¨å…·æœ‰åŠ¨æ€è·¯ç”±çš„æ··åˆä¸“å®¶(MoE)æ¶æ„æ¥é€‰æ‹©æ€§åœ°å¯ç”¨ç‰¹å®šæ¨¡æ€çš„å»åä¸“å®¶ã€‚åœ¨å¤šæ¨¡æ€è®½åˆºæ£€æµ‹å’Œæƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ˜¾è‘—ä¼˜äºå•æ¨¡æ€å»åç­–ç•¥å’Œç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®æ—¶ï¼Œå®¹æ˜“å—åˆ°è™šå‡ç›¸å…³æ€§çš„å½±å“ï¼Œå³æ¨¡å‹å­¦ä¹ åˆ°çš„æ˜¯æ•°æ®é›†ä¸­å­˜åœ¨çš„è¡¨é¢ä¸Šçš„ç»Ÿè®¡è§„å¾‹ï¼Œè€Œä¸æ˜¯çœŸå®çš„è¯­ä¹‰å…³ç³»ã€‚è¿™å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹æ–°çš„ã€åˆ†å¸ƒå¤–çš„æ•°æ®æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¾§é‡äºå•æ¨¡æ€çš„å»åï¼Œå¿½ç•¥äº†æ¨¡æ€ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œæ— æ³•æœ‰æ•ˆåœ°è§£å†³å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„è™šå‡ç›¸å…³æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å› æœæ¨æ–­æ¥è¯†åˆ«å’Œæ¶ˆé™¤è™šå‡ç›¸å…³æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æ„å»ºåäº‹å®æ ·æœ¬ï¼Œæ¥åŒºåˆ†æ ¸å¿ƒè¯­ä¹‰å’Œè™šå‡çš„æ–‡æœ¬/è§†è§‰ä¸Šä¸‹æ–‡ã€‚åäº‹å®æ ·æœ¬æ˜¯æŒ‡åœ¨ä¿æŒå…¶ä»–å› ç´ ä¸å˜çš„æƒ…å†µä¸‹ï¼Œæ”¹å˜æŸä¸ªå› ç´ æ‰€äº§ç”Ÿçš„æ ·æœ¬ã€‚é€šè¿‡æ¯”è¾ƒåŸå§‹æ ·æœ¬å’Œåäº‹å®æ ·æœ¬çš„é¢„æµ‹ç»“æœï¼Œå¯ä»¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦ä¾èµ–äºè™šå‡ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œå¹¶æ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹ç‚¹ï¼ŒåŠ¨æ€åœ°é€‰æ‹©ä¸åŒçš„ä¸“å®¶è¿›è¡Œå¤„ç†ï¼Œä»è€Œå®ç°è‡ªé€‚åº”çš„å»åã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) åäº‹å®æ ·æœ¬ç”Ÿæˆæ¨¡å—ï¼šç”¨äºç”Ÿæˆä¸åŸå§‹æ ·æœ¬ç›¸å¯¹åº”çš„åäº‹å®æ ·æœ¬ï¼Œé€šè¿‡æ”¹å˜æ–‡æœ¬æˆ–è§†è§‰ä¸Šä¸‹æ–‡ï¼Œæ¥æ­ç¤ºæ¨¡å‹å¯¹è™šå‡ç›¸å…³æ€§çš„ä¾èµ–ã€‚2) å»åè®­ç»ƒæ¨¡å—ï¼šåˆ©ç”¨åŸå§‹æ ·æœ¬å’Œåäº‹å®æ ·æœ¬ï¼Œè®­ç»ƒæ¨¡å‹å­¦ä¹ åŒºåˆ†æ ¸å¿ƒè¯­ä¹‰å’Œè™šå‡ä¸Šä¸‹æ–‡ã€‚3) æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å—ï¼šåŒ…å«å¤šä¸ªç‰¹å®šæ¨¡æ€çš„å»åä¸“å®¶ï¼Œæ¯ä¸ªä¸“å®¶è´Ÿè´£å¤„ç†ç‰¹å®šç±»å‹çš„è™šå‡ç›¸å…³æ€§ã€‚4) åŠ¨æ€è·¯ç”±æ¨¡å—ï¼šæ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹ç‚¹ï¼ŒåŠ¨æ€åœ°é€‰æ‹©åˆé€‚çš„ä¸“å®¶è¿›è¡Œå¤„ç†ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆé€šè¿‡åäº‹å®æ ·æœ¬ç”Ÿæˆæ¨¡å—ç”Ÿæˆåäº‹å®æ ·æœ¬ï¼Œç„¶ååˆ©ç”¨åŸå§‹æ ·æœ¬å’Œåäº‹å®æ ·æœ¬è®­ç»ƒå»åæ¨¡å‹ï¼Œåœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡åŠ¨æ€è·¯ç”±æ¨¡å—é€‰æ‹©åˆé€‚çš„ä¸“å®¶è¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ç‚¹ï¼š1) æå‡ºäº†åŸºäºå› æœä¸­ä»‹çš„å»åæ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«å’Œæ¶ˆé™¤å¤šæ¨¡æ€æ•°æ®ä¸­çš„è™šå‡ç›¸å…³æ€§ã€‚2) é‡‡ç”¨äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œå¹¶æ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹ç‚¹ï¼ŒåŠ¨æ€åœ°é€‰æ‹©ä¸åŒçš„ä¸“å®¶è¿›è¡Œå¤„ç†ï¼Œä»è€Œå®ç°è‡ªé€‚åº”çš„å»åã€‚3) é€šè¿‡åäº‹å®æ ·æœ¬ç”Ÿæˆæ¨¡å—ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ­ç¤ºæ¨¡å‹å¯¹è™šå‡ç›¸å…³æ€§çš„ä¾èµ–ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œè¯¥æ–¹æ³•ä¸ä»…è€ƒè™‘äº†å•æ¨¡æ€çš„å»åï¼Œè¿˜è€ƒè™‘äº†æ¨¡æ€ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£å†³å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„è™šå‡ç›¸å…³æ€§é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åäº‹å®æ ·æœ¬ç”Ÿæˆæ¨¡å—ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§ç­–ç•¥æ¥ç”Ÿæˆåäº‹å®æ ·æœ¬ï¼Œä¾‹å¦‚ï¼Œæ›¿æ¢æ–‡æœ¬ä¸­çš„å…³é”®è¯ã€æ”¹å˜å›¾åƒçš„èƒŒæ™¯ç­‰ã€‚åœ¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å—ä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒçš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°æ¥è®­ç»ƒä¸åŒçš„ä¸“å®¶ã€‚åœ¨åŠ¨æ€è·¯ç”±æ¨¡å—ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è·¯ç”±ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹ç‚¹ï¼ŒåŠ¨æ€åœ°é€‰æ‹©åˆé€‚çš„ä¸“å®¶è¿›è¡Œå¤„ç†ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°é€‰æ‹©å–å†³äºå…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šæ¨¡æ€è®½åˆºæ£€æµ‹å’Œæƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨è®½åˆºæ£€æµ‹ä»»åŠ¡ä¸Šï¼Œè¯¥æ¡†æ¶çš„å‡†ç¡®ç‡æ¯”æœ€å…ˆè¿›çš„æ¨¡å‹æé«˜äº†5%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¯æ˜äº†è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¶ˆé™¤æ¨¡å‹å¯¹è™šå‡ç›¸å…³æ€§çš„ä¾èµ–ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚ï¼šè‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ã€æ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡æ¶ˆé™¤æ¨¡å‹å¯¹è™šå‡ç›¸å…³æ€§çš„ä¾èµ–ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ¨å¹¿åˆ°æ›´å¤šçš„å¤šæ¨¡æ€ä»»åŠ¡å’Œé¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have shown substantial capabilities in integrating visual and textual information, yet frequently rely on spurious correlations, undermining their robustness and generalization in complex multimodal reasoning tasks. This paper addresses the critical challenge of superficial correlation bias in MLLMs through a novel causal mediation-based debiasing framework. Specially, we distinguishing core semantics from spurious textual and visual contexts via counterfactual examples to activate training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture with dynamic routing to selectively engages modality-specific debiasing experts. Empirical evaluation on multimodal sarcasm detection and sentiment analysis tasks demonstrates that our framework significantly surpasses unimodal debiasing strategies and existing state-of-the-art models.

