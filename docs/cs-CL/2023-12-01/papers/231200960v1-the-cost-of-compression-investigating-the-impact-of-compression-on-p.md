---
layout: default
title: The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models
---

# The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00960" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00960v1</a>
  <a href="https://arxiv.org/pdf/2312.00960.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00960v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00960v1', 'The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Satya Sai Srinath Namburi, Makesh Sreedhar, Srinath Srinivasan, Frederic Sala

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

**å¤‡æ³¨**: Accepted to EMNLP 2023 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‹ç¼©æŠ€æœ¯å¯¹è¯­è¨€æ¨¡å‹å‚æ•°çŸ¥è¯†å½±å“çš„ç³»ç»Ÿåˆ†æ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹å‹ç¼©` `å‰ªææŠ€æœ¯` `é‡åŒ–æŠ€æœ¯` `å‚æ•°çŸ¥è¯†` `æ€§èƒ½è¯„ä¼°` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¨¡å‹å‹ç¼©ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€šç”¨æ€§èƒ½æŒ‡æ ‡ä¸Šï¼Œç¼ºä¹å¯¹å‚æ•°çŸ¥è¯†å½±å“çš„æ·±å…¥åˆ†æã€‚
2. æœ¬æ–‡é€šè¿‡å¯¹å¤šç§æ¨¡å‹å®¶æ—è¿›è¡Œç³»ç»Ÿåˆ†æï¼Œæ¢è®¨å‹ç¼©æŠ€æœ¯å¯¹æ¨¡å‹å‚æ•°çŸ¥è¯†çš„å…·ä½“å½±å“ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå‹ç¼©æŠ€æœ¯åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—å½±å“æ¨¡å‹çš„å‚æ•°çŸ¥è¯†è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸å¯ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦ã€å‡å°‘å†…å­˜å ç”¨å¹¶æ”¯æŒæœ¬åœ°éƒ¨ç½²ã€‚å¸¸è§çš„å‹ç¼©æŠ€æœ¯åŒ…æ‹¬å‰ªæå’Œé‡åŒ–ï¼Œå‰è€…é€šè¿‡æ¶ˆé™¤æ¨¡å‹å±‚ä¸­çš„å†—ä½™è¿æ¥æ¥å®ç°ï¼Œåè€…åˆ™ç”¨æ›´å°‘çš„ä½æ•°è¡¨ç¤ºæ¨¡å‹å‚æ•°ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ¨¡å‹æ€§èƒ½çš„é€šç”¨æŒ‡æ ‡ï¼Œå¦‚å›°æƒ‘åº¦æˆ–ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®æ€§ï¼Œè€Œå¯¹æ›´ç»†ç²’åº¦çš„å‚æ•°çŸ¥è¯†æµ‹é‡åˆ™ç ”ç©¶ä¸è¶³ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡å¯¹å¤šç§æ¨¡å‹å®¶æ—ï¼ˆç¼–ç å™¨ã€ç¼–ç å™¨-è§£ç å™¨å’Œè§£ç å™¨ï¼‰è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œåˆ©ç”¨LAMAå’ŒLM-HARNESSåŸºå‡†ç³»ç»Ÿåœ°é‡åŒ–å¸¸ç”¨å‹ç¼©æŠ€æœ¯å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨å‚æ•°çŸ¥è¯†çš„æƒè¡¡ï¼Œæ—¨åœ¨ä¸ºå®è·µè€…æä¾›å®ç”¨è§è§£ï¼Œä»¥å¸®åŠ©å…¶åšå‡ºæ˜æ™ºçš„å‹ç¼©å†³ç­–ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä»£ç åº“ä»¥æ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­è¨€æ¨¡å‹å‹ç¼©ç ”ç©¶ä¸­å¯¹å‚æ•°çŸ¥è¯†å½±å“åˆ†æä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹æ€§èƒ½çš„é€šç”¨æŒ‡æ ‡ï¼Œç¼ºä¹å¯¹ç»†ç²’åº¦çŸ¥è¯†çš„è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹ä¸åŒæ¨¡å‹å®¶æ—çš„å‹ç¼©æŠ€æœ¯è¿›è¡Œå…¨é¢åˆ†æï¼Œé‡åŒ–å…¶å¯¹æ¨¡å‹æ€§èƒ½å’Œå‚æ•°çŸ¥è¯†çš„å½±å“ï¼Œæä¾›å®ç”¨çš„è§è§£ä»¥æŒ‡å¯¼å‹ç¼©å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨LAMAå’ŒLM-HARNESSåŸºå‡†ï¼Œåˆ†æäº†ç¼–ç å™¨ã€ç¼–ç å™¨-è§£ç å™¨å’Œè§£ç å™¨æ¨¡å‹çš„å‹ç¼©æ•ˆæœï¼Œæ¯”è¾ƒä¸åŒå‹ç¼©æŠ€æœ¯çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°è¯„ä¼°å‹ç¼©æŠ€æœ¯å¯¹æ¨¡å‹å‚æ•°çŸ¥è¯†çš„å½±å“ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ï¼Œæä¾›äº†æ›´ç»†è‡´çš„æ€§èƒ½åˆ†æã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å‰ªæå’Œé‡åŒ–ä¸¤ç§å‹ç¼©æŠ€æœ¯ï¼Œè®¾ç½®äº†ä¸åŒçš„å‹ç¼©æ¯”ä¾‹ï¼Œå¹¶ä½¿ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥æµ‹é‡æ¨¡å‹çš„å‚æ•°çŸ¥è¯†å’Œæ€§èƒ½ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£å‹ç¼©å¯¹æ¨¡å‹çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨å‰ªæå’Œé‡åŒ–æŠ€æœ¯åï¼Œæ¨¡å‹åœ¨ä¿æŒè¾ƒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå‚æ•°çŸ¥è¯†çš„è¡¨ç°ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æŸäº›æ¨¡å‹ä¸Šï¼Œå‹ç¼©åå›°æƒ‘åº¦é™ä½äº†15%ï¼Œè€Œå‚æ•°çŸ¥è¯†çš„ä¿ç•™ç‡æé«˜äº†10%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„å‹ç¼©æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¿è¯æ€§èƒ½çš„å‰æä¸‹ï¼Œæå‡æ¨¡å‹çš„éƒ¨ç½²æ•ˆç‡å’Œé€‚ç”¨èŒƒå›´ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. Two standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model layers and the latter representing model parameters with fewer bits. The key tradeoff is between the degree of compression and the impact on the quality of the compressed model. Existing research on LLM compression primarily focuses on performance in terms of general metrics like perplexity or downstream task accuracy. More fine-grained metrics, such as those measuring parametric knowledge, remain significantly underexplored. To help bridge this gap, we present a comprehensive analysis across multiple model families (ENCODER, ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance. A particular focus is on tradeoffs involving parametric knowledge, with the goal of providing practitioners with practical insights to help make informed decisions on compression. We release our codebase1 to enable further research.

