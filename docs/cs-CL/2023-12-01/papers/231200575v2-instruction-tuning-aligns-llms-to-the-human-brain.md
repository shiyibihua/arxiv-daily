---
layout: default
title: Instruction-tuning Aligns LLMs to the Human Brain
---

# Instruction-tuning Aligns LLMs to the Human Brain

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00575" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00575v2</a>
  <a href="https://arxiv.org/pdf/2312.00575.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00575v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00575v2', 'Instruction-tuning Aligns LLMs to the Human Brain')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, Antoine Bosselut

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2024-08-09)

**å¤‡æ³¨**: COLM 2024

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶æŒ‡ä»¤å¾®è°ƒä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººè„‘çš„å¯¹é½æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒ‡ä»¤å¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è„‘å¯¹é½` `è¡Œä¸ºå¯¹é½` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¸–ç•ŒçŸ¥è¯†ç†è§£` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•æœªèƒ½è¯æ˜æŒ‡ä»¤å¾®è°ƒèƒ½æœ‰æ•ˆæ•™ä¼šLLMsä»¥äººç±»æ–¹å¼å¤„ç†è¯­è¨€ï¼Œç¼ºä¹å®è¯æ”¯æŒã€‚
2. è®ºæ–‡é€šè¿‡åˆ†æLLMå†…éƒ¨è¡¨å¾ä¸äººç±»ç¥ç»æ´»åŠ¨çš„ç›¸ä¼¼æ€§ï¼Œæå‡ºäº†è„‘å¯¹é½å’Œè¡Œä¸ºå¯¹é½çš„è¯„ä¼°æ–¹æ³•ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒæŒ‡ä»¤å¾®è°ƒæå‡äº†è„‘å¯¹é½çº¦6%ï¼Œä½†å¯¹è¡Œä¸ºå¯¹é½æ— æ˜¾è‘—å½±å“ï¼Œä¸”æ¨¡å‹å¤§å°ä¸è„‘å¯¹é½é«˜åº¦ç›¸å…³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æŒ‡ä»¤å¾®è°ƒæ˜¯ä¸€ç§å¹¿æ³›é‡‡ç”¨çš„å¾®è°ƒæ–¹æ³•ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆæ›´æ¥è¿‘äººç±»ååº”çš„è¾“å‡ºã€‚ç„¶è€Œï¼Œå°šæ— ç ”ç©¶è¡¨æ˜æŒ‡ä»¤å¾®è°ƒç¡®å®æ•™ä¼šLLMsä»¥ç±»ä¼¼äººç±»çš„æ–¹å¼å¤„ç†è¯­è¨€ã€‚æœ¬æ–‡é€šè¿‡è„‘å¯¹é½å’Œè¡Œä¸ºå¯¹é½ä¸¤ç§æ–¹å¼ï¼Œæ¢è®¨æŒ‡ä»¤å¾®è°ƒå¯¹LLMä¸äººç±»è¯­è¨€å¤„ç†æœºåˆ¶çš„å¯¹é½æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼ŒæŒ‡ä»¤å¾®è°ƒé€šå¸¸æå‡äº†è„‘å¯¹é½ï¼ˆçº¦6%ï¼‰ï¼Œä½†å¯¹è¡Œä¸ºå¯¹é½æ²¡æœ‰æ˜¾è‘—å½±å“ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¤§å°å’Œä¸–ç•ŒçŸ¥è¯†ç†è§£ä¸è„‘å¯¹é½ä¹‹é—´å­˜åœ¨å¼ºæ­£ç›¸å…³ï¼Œè¡¨æ˜æŒ‡ä»¤å¾®è°ƒä¸ä»…æ”¹å–„äº†ä¸–ç•ŒçŸ¥è¯†çš„è¡¨å¾ï¼Œä¹Ÿå¢å¼ºäº†ä¸äººè„‘çš„å¯¹é½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æŒ‡ä»¤å¾®è°ƒæ˜¯å¦èƒ½æœ‰æ•ˆæ•™ä¼šå¤§å‹è¯­è¨€æ¨¡å‹ä»¥ç±»ä¼¼äººç±»çš„æ–¹å¼å¤„ç†è¯­è¨€çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹å¾®è°ƒæ•ˆæœçš„å®è¯åˆ†æï¼Œå°¤å…¶æ˜¯åœ¨ä¸äººç±»è¯­è¨€å¤„ç†æœºåˆ¶çš„å¯¹é½æ–¹é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šç ”ç©¶é€šè¿‡è¯„ä¼°LLMå†…éƒ¨è¡¨å¾ä¸äººç±»è¯­è¨€ç³»ç»Ÿç¥ç»æ´»åŠ¨çš„ç›¸ä¼¼æ€§ï¼Œæ¢è®¨æŒ‡ä»¤å¾®è°ƒå¯¹è„‘å¯¹é½å’Œè¡Œä¸ºå¯¹é½çš„å½±å“ã€‚é€šè¿‡æ¯”è¾ƒ25ä¸ªåŸå§‹å’ŒæŒ‡ä»¤å¾®è°ƒçš„LLMsï¼Œåˆ†æå…¶åœ¨é˜…è¯»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè„‘å¯¹é½è¯„ä¼°å’Œè¡Œä¸ºå¯¹é½è¯„ä¼°ã€‚è„‘å¯¹é½é€šè¿‡è®¡ç®—LLMå†…éƒ¨è¡¨å¾ä¸äººç±»ç¥ç»æ´»åŠ¨çš„ç›¸ä¼¼æ€§æ¥å®ç°ï¼Œè¡Œä¸ºå¯¹é½åˆ™é€šè¿‡æ¯”è¾ƒLLMå’Œäººç±»åœ¨é˜…è¯»ä»»åŠ¡ä¸­çš„è¡¨ç°æ¥è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¢è®¨æŒ‡ä»¤å¾®è°ƒå¯¹LLMä¸äººç±»è¯­è¨€å¤„ç†æœºåˆ¶å¯¹é½çš„å½±å“ï¼Œæ­ç¤ºäº†æ¨¡å‹å¤§å°å’Œä¸–ç•ŒçŸ¥è¯†ç†è§£ä¸è„‘å¯¹é½ä¹‹é—´çš„å¼ºç›¸å…³æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­ä½¿ç”¨äº†å¤šä¸ªæ•°æ®é›†ï¼Œè¯„ä¼°äº†ä¸åŒæ¨¡å‹çš„è¡¨ç°ï¼Œç‰¹åˆ«å…³æ³¨æ¨¡å‹å¤§å°ã€é—®é¢˜è§£å†³èƒ½åŠ›å’Œä¸–ç•ŒçŸ¥è¯†ç†è§£ç­‰å› ç´ å¯¹è„‘å¯¹é½çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒæŒ‡ä»¤å¾®è°ƒæ˜¾è‘—æå‡äº†è„‘å¯¹é½çº¦6%ï¼Œè€Œåœ¨è¡Œä¸ºå¯¹é½æ–¹é¢æœªè§æ˜¾è‘—å˜åŒ–ã€‚æ¨¡å‹å¤§å°ä¸è„‘å¯¹é½ä¹‹é—´çš„ç›¸å…³æ€§é«˜è¾¾0.95ï¼Œä¸–ç•ŒçŸ¥è¯†ç†è§£çš„ç›¸å…³æ€§ä¸º0.81ï¼Œæ˜¾ç¤ºå‡ºæŒ‡ä»¤å¾®è°ƒåœ¨æå‡ä¸–ç•ŒçŸ¥è¯†è¡¨å¾æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæ–¹æ³•æä¾›äº†æ–°çš„è§†è§’ï¼Œå°¤å…¶æ˜¯åœ¨æå‡æ¨¡å‹ä¸äººç±»è¯­è¨€å¤„ç†æœºåˆ¶å¯¹é½æ–¹é¢ã€‚å…¶ç»“æœå¯åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€æ•™è‚²æŠ€æœ¯å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸï¼Œæ¨åŠ¨æ›´æ™ºèƒ½çš„è¯­è¨€ç†è§£ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Instruction-tuning is a widely adopted finetuning method that enables large language models (LLMs) to generate output that more closely resembles human responses. However, no studies have shown that instruction-tuning actually teaches LLMs to process language in a similar manner as humans. We investigate the effect of instruction-tuning on aligning LLM and human language processing mechanisms in two ways: (1) brain alignment, the similarity of LLM internal representations to neural activity in the human language system, and (2) behavioral alignment, the similarity of LLM and human behavior on a reading task. We assess 25 vanilla and instruction-tuned LLMs on three datasets involving humans reading naturalistic stories and sentences, and find that instruction-tuning generally enhances brain alignment (~6%), but has no similar effect on behavioral alignment. To identify factors underlying this improvement in brain alignment, we compute correlations between brain alignment and various LLM properties, such as model size, problem-solving, and world knowledge understanding. Notably, we find a strong positive correlation between brain alignment and model size (r = 0.95), as well as performance on tasks requiring world knowledge (r = 0.81). Our results demonstrate that instruction-tuning LLMs improves both world knowledge representations and brain alignment, suggesting that the mechanisms that encode world knowledge in LLMs also improve representational alignment to the human brain.

