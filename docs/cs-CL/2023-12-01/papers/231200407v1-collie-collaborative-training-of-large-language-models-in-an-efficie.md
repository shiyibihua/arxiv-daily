---
layout: default
title: CoLLiE: Collaborative Training of Large Language Models in an Efficient Way
---

# CoLLiE: Collaborative Training of Large Language Models in an Efficient Way

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00407" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00407v1</a>
  <a href="https://arxiv.org/pdf/2312.00407.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00407v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00407v1', 'CoLLiE: Collaborative Training of Large Language Models in an Efficient Way')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kai Lv, Shuo Zhang, Tianle Gu, Shuhao Xing, Jiawei Hong, Keyu Chen, Xiaoran Liu, Yuqing Yang, Honglin Guo, Tengxiao Liu, Yu Sun, Qipeng Guo, Hang Yan, Xipeng Qiu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

**å¤‡æ³¨**: To appear at EMNLP 2023 Demo; Code is available at https://github.com/OpenLMLab/collie

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/OpenLMLab/collie)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoLLiEä»¥é«˜æ•ˆåä½œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åä½œè®­ç»ƒ` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `3Då¹¶è¡Œ` `ä¼˜åŒ–å™¨` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®­ç»ƒæ•ˆç‡` `å¼€æºå·¥å…·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•èµ„æºæ¶ˆè€—å·¨å¤§ï¼Œæ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥æ»¡è¶³å¿«é€Ÿå‘å±•çš„åº”ç”¨éœ€æ±‚ã€‚
2. CoLLiEé€šè¿‡3Då¹¶è¡Œå’Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œç»“åˆå¤šç§ä¼˜åŒ–å™¨ï¼Œå®ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆåä½œè®­ç»ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLLiEåœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒåœºæ™¯ä¸­å±•ç°äº†æ˜¾è‘—çš„è®­ç»ƒæ•ˆç‡æå‡ï¼Œä¼˜äºç°æœ‰ä¸»æµè§£å†³æ–¹æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ„ˆå‘é‡è¦ã€‚å¾—ç›Šäºå¼€æºç¤¾åŒºçš„æ”¯æŒï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„è·å–ä½¿å¾—é’ˆå¯¹ç‰¹å®šåº”ç”¨çš„é€‚é…æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™äº›æ¨¡å‹æ‰€éœ€çš„å·¨å¤§èµ„æºè¦æ±‚é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†CoLLiEï¼Œä¸€ä¸ªé«˜æ•ˆçš„åº“ï¼Œåˆ©ç”¨3Då¹¶è¡Œã€å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åŠå¤šç§ä¼˜åŒ–å™¨ï¼ˆå¦‚Lionã€Adanã€Sophiaã€LOMOå’ŒAdaLomoï¼‰æ¥ä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„åä½œè®­ç»ƒã€‚CoLLiEä»¥å…¶æ¨¡å—åŒ–è®¾è®¡å’Œå…¨é¢åŠŸèƒ½ï¼Œæä¾›äº†æ•ˆç‡ã€æ˜“ç”¨æ€§å’Œå®šåˆ¶åŒ–çš„å¹³è¡¡ã€‚ä¸ç°æœ‰çš„é¢„è®­ç»ƒå’Œå¾®è°ƒæ–¹æ¡ˆç›¸æ¯”ï¼ŒCoLLiEå±•ç°äº†æ›´ä¼˜çš„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶å¯¹ä¸åŒä¼˜åŒ–æ–¹æ³•ä¸‹æ¨¡å‹å¤§å°ä¸GPUå†…å­˜æ¶ˆè€—çš„ç›¸å…³æ€§è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œæ—¶é—´ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ä¸Šå­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ¨å¹¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoLLiEçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡3Då¹¶è¡Œå’Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œç»“åˆå¤šç§ä¼˜åŒ–å™¨ï¼Œæ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Œé™ä½èµ„æºæ¶ˆè€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoLLiEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œçš„3Då¹¶è¡Œç­–ç•¥ï¼Œå…¶æ¬¡æ˜¯é›†æˆå¤šç§ä¼˜åŒ–å™¨çš„çµæ´»é…ç½®ï¼Œæœ€åæ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•çš„åº”ç”¨ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„é«˜æ•ˆæ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoLLiEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–è®¾è®¡å’Œå¤šç§ä¼˜åŒ–å™¨çš„é›†æˆï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚çµæ´»é€‰æ‹©ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCoLLiEé‡‡ç”¨äº†å¤šç§ä¼˜åŒ–å™¨ï¼ˆå¦‚Lionã€Adanç­‰ï¼‰ï¼Œå¹¶é€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•æ¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoLLiEåœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒåœºæ™¯ä¸­ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•ï¼Œè®­ç»ƒæ•ˆç‡æå‡äº†30%ä»¥ä¸Šï¼Œä¸”åœ¨GPUå†…å­˜æ¶ˆè€—æ–¹é¢è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨å’ŒPEFTæ–¹æ³•ï¼ŒCoLLiEå±•ç°äº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoLLiEçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¿«é€Ÿé€‚åº”ç‰¹å®šä»»åŠ¡çš„åœºæ™¯ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œæƒ…æ„Ÿåˆ†æç­‰ã€‚å…¶é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•å°†æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™®åŠå’Œåº”ç”¨ï¼Œé™ä½ä¼ä¸šå’Œç ”ç©¶æœºæ„çš„èµ„æºæŠ•å…¥ã€‚æœªæ¥ï¼ŒCoLLiEæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„AIåº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie.

