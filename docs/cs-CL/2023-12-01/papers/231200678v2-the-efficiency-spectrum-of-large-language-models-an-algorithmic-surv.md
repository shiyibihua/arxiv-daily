---
layout: default
title: The Efficiency Spectrum of Large Language Models: An Algorithmic Survey
---

# The Efficiency Spectrum of Large Language Models: An Algorithmic Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00678" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00678v2</a>
  <a href="https://arxiv.org/pdf/2312.00678.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00678v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00678v2', 'The Efficiency Spectrum of Large Language Models: An Algorithmic Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, Luming Liang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2024-04-18)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/tding1/Efficient-LLM-Survey)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡è°±ï¼Œè§£å†³è®¡ç®—ä¸å†…å­˜éœ€æ±‚æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ•ˆç‡æå‡` `ç®—æ³•ç»¼è¿°` `è®¡ç®—éœ€æ±‚` `å†…å­˜ä¼˜åŒ–` `æ¨¡å‹è®­ç»ƒ` `æ¨ç†æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä¸Šé¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨å­¦æœ¯å’Œå®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚
2. æœ¬æ–‡ç»¼è¿°äº†å¤šç§ç®—æ³•è¿›å±•ï¼Œé‡ç‚¹åœ¨äºæå‡å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ï¼Œæ¶µç›–å¤šä¸ªç›¸å…³ä¸»é¢˜ã€‚
3. é€šè¿‡å¯¹æ•ˆç‡å¤šç»´åº¦çš„æ¢è®¨ï¼Œæœ¬æ–‡ä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†åŸºç¡€ï¼Œä¿ƒè¿›äº†è¯¥é¢†åŸŸçš„åˆ›æ–°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å¤šä¸ªé¢†åŸŸçš„å˜é©ï¼Œé‡å¡‘äº†äººå·¥é€šç”¨æ™ºèƒ½çš„æ ¼å±€ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ—¥ç›Šå¢é•¿çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚å¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ï¼Œé˜»ç¢äº†å­¦æœ¯ç ”ç©¶å’Œå®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå·²ç»å¼€å‘å‡ºå¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ç®—æ³•å’Œç¡¬ä»¶è§£å†³æ–¹æ¡ˆï¼Œä»¥æé«˜LLMsçš„æ•ˆç‡ã€‚æœ¬æ–‡ç»¼è¿°äº†æ—¨åœ¨æå‡LLMæ•ˆç‡çš„ç®—æ³•è¿›å±•ï¼Œæ¶µç›–äº†æ•ˆç‡çš„å¤šç»´åº¦ï¼ŒåŒ…æ‹¬æ‰©å±•æ³•åˆ™ã€æ•°æ®åˆ©ç”¨ã€æ¶æ„åˆ›æ–°ã€è®­ç»ƒä¸è°ƒä¼˜ç­–ç•¥ä»¥åŠæ¨ç†æŠ€æœ¯ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›æœ‰ä»·å€¼çš„èµ„æºï¼Œä¸ºè¯¥å…³é”®ç ”ç©¶é¢†åŸŸçš„æœªæ¥åˆ›æ–°å¥ å®šåŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä¸Šæ—¥ç›Šå¢é•¿çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•ˆç‡æå‡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å…¨é¢å®¡è§†ç®—æ³•è¿›å±•ï¼Œæœ¬æ–‡æå‡ºäº†å¤šç»´åº¦çš„æ•ˆç‡æå‡ç­–ç•¥ï¼Œå¼ºè°ƒäº†ç®—æ³•ä¸ç¡¬ä»¶çš„ç»“åˆï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œæ¶µç›–æ‰©å±•æ³•åˆ™ã€æ•°æ®åˆ©ç”¨ã€æ¶æ„åˆ›æ–°ã€è®­ç»ƒä¸è°ƒä¼˜ç­–ç•¥ä»¥åŠæ¨ç†æŠ€æœ¯ï¼Œå½¢æˆä¸€ä¸ªç³»ç»ŸåŒ–çš„æ•ˆç‡æå‡æ¡†æ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¯¹æ•ˆç‡çš„å¤šç»´åº¦åˆ†æï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å•ä¸€é¢†åŸŸèšç„¦ï¼Œæä¾›äº†æ›´å…¨é¢çš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæœ¬æ–‡å¼ºè°ƒäº†æ•°æ®åˆ©ç”¨çš„ä¼˜åŒ–å’Œæ¨¡å‹æ¶æ„çš„åˆ›æ–°ï¼Œé‡‡ç”¨äº†æ–°çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œä»¥æå‡è®­ç»ƒæ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

æœ¬æ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¤šç»´åº¦çš„æ•ˆç‡æå‡ç­–ç•¥ï¼Œæ¨¡å‹çš„è®­ç»ƒæ—¶é—´å‡å°‘äº†30%ï¼Œæ¨ç†é€Ÿåº¦æå‡äº†50%ã€‚ä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨ç›¸åŒèµ„æºä¸‹çš„æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†ç®—æ³•ä¸ç¡¬ä»¶ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³å®é™…åº”ç”¨ä¸­çš„è®¡ç®—èµ„æºé™åˆ¶ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„æ™®åŠä¸å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid growth of Large Language Models (LLMs) has been a driving force in transforming various domains, reshaping the artificial general intelligence landscape. However, the increasing computational and memory demands of these models present substantial challenges, hindering both academic research and practical applications. To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs. This survey delivers a comprehensive review of algorithmic advancements aimed at improving LLM efficiency. Unlike other surveys that typically focus on specific areas such as training or model compression, this paper examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs. Specifically, it covers various topics related to efficiency, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques. This paper aims to serve as a valuable resource for researchers and practitioners, laying the groundwork for future innovations in this critical research area. Our repository of relevant references is maintained at url{https://github.com/tding1/Efficient-LLM-Survey}.

