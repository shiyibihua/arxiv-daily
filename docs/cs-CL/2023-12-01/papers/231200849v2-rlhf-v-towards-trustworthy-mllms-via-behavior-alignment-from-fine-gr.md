---
layout: default
title: RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback
---

# RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00849" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00849v2</a>
  <a href="https://arxiv.org/pdf/2312.00849.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00849v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00849v2', 'RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, Tat-Seng Chua

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2024-03-08)

**å¤‡æ³¨**: Accepted by CVPR 2024

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/RLHF-V/RLHF-V)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRLHF-Vä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¹»è§‰é—®é¢˜` `äººç±»åé¦ˆ` `è¡Œä¸ºå¯¹é½` `å¯ä¿¡åº¦æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ™®éå­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œç”Ÿæˆçš„æ–‡æœ¬ç¼ºä¹ä¸å›¾åƒçš„äº‹å®åŸºç¡€ï¼Œå¯¼è‡´å…¶åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„ä¸å¯ä¿¡æ€§ã€‚
2. RLHF-Vé€šè¿‡ç»†ç²’åº¦çš„çº æ­£æ€§äººç±»åé¦ˆæ”¶é›†äººç±»åå¥½ï¼Œå¹¶è¿›è¡Œå¯†é›†çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¯ä¿¡åº¦ã€‚
3. åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRLHF-Væ˜¾è‘—é™ä½äº†åŸºçº¿æ¨¡å‹çš„å¹»è§‰ç‡34.8%ï¼Œå¹¶åœ¨å¯ä¿¡åº¦æ–¹é¢è¾¾åˆ°äº†å¼€æºMLLMçš„æœ€æ–°æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œäº¤äº’æ–¹é¢å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MLLMsæ™®éå­˜åœ¨ä¸¥é‡çš„å¹»è§‰é—®é¢˜ï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¸ç›¸å…³å›¾åƒç¼ºä¹äº‹å®åŸºç¡€ï¼Œå¯¼è‡´å…¶åœ¨ç°å®ä¸–ç•Œï¼ˆå°¤å…¶æ˜¯é«˜é£é™©ï¼‰åº”ç”¨ä¸­ä¸å¯ä¿¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†RLHF-Vï¼Œé€šè¿‡ç»†ç²’åº¦çš„çº æ­£æ€§äººç±»åé¦ˆè¿›è¡Œè¡Œä¸ºå¯¹é½ï¼Œä»è€Œå¢å¼ºMLLMçš„å¯ä¿¡åº¦ã€‚RLHF-Vä»¥æ®µè½çº§åˆ«çš„çº æ­£å½¢å¼æ”¶é›†äººç±»åå¥½ï¼Œå¹¶å¯¹äººç±»åé¦ˆè¿›è¡Œå¯†é›†çš„ç›´æ¥åå¥½ä¼˜åŒ–ã€‚ç»¼åˆåœ¨äº”ä¸ªåŸºå‡†ä¸Šçš„è‡ªåŠ¨å’Œäººå·¥è¯„ä¼°å®éªŒè¡¨æ˜ï¼ŒRLHF-Væ˜¾è‘—æé«˜äº†MLLMçš„å¯ä¿¡åº¦ï¼Œå¹¶åœ¨æ•°æ®å’Œè®¡ç®—æ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºç°çš„å¹»è§‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™ä¸€é—®é¢˜æ—¶æ•ˆæœä¸ä½³ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹ä¸å¯ä¿¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRLHF-Vçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»†ç²’åº¦çš„çº æ­£æ€§äººç±»åé¦ˆæ¥è¿›è¡Œè¡Œä¸ºå¯¹é½ï¼Œæ”¶é›†äººç±»å¯¹å¹»è§‰å†…å®¹çš„åå¥½ï¼Œå¹¶è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æå‡æ¨¡å‹çš„å¯ä¿¡åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRLHF-Vçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€åå¥½ä¼˜åŒ–å’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†äººç±»åé¦ˆï¼Œç„¶ååŸºäºè¿™äº›åé¦ˆè¿›è¡Œåå¥½ä¼˜åŒ–ï¼Œæœ€åæ›´æ–°æ¨¡å‹ä»¥æé«˜å…¶ç”Ÿæˆæ–‡æœ¬çš„å¯ä¿¡åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šRLHF-Vçš„å…³é”®åˆ›æ–°åœ¨äºé€šè¿‡æ®µè½çº§åˆ«çš„çº æ­£æ€§åé¦ˆè¿›è¡Œå¯†é›†ä¼˜åŒ–ï¼Œè¿™ç§æ–¹æ³•ä¸ä¼ ç»Ÿçš„å…¨å±€ä¼˜åŒ–æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å¯¹æŠ—å¹»è§‰é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒRLHF-Vä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥é‡åŒ–äººç±»åé¦ˆçš„åå¥½ï¼Œå¹¶åœ¨æ¨¡å‹è®­ç»ƒä¸­å¼•å…¥äº†æ–°çš„å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLHF-Våœ¨ä½¿ç”¨1400ä¸ªæ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½äº†åŸºçº¿æ¨¡å‹çš„å¹»è§‰ç‡34.8%ã€‚æ­¤å¤–ï¼ŒRLHF-Våœ¨å¯ä¿¡åº¦æ–¹é¢è¶…è¶Šäº†ä½¿ç”¨10000ä¸ªæ ‡æ³¨æ ·æœ¬çš„LLaVA-RLHFï¼Œä¸”åœ¨é˜²æ­¢å› è¿‡åº¦æ³›åŒ–å¼•èµ·çš„å¹»è§‰æ–¹é¢è¡¨ç°å‡ºæ¯”GPT-4Væ›´å¥½çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå’Œæ™ºèƒ½å®¢æœç­‰é«˜é£é™©åœºæ™¯ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ¨¡å‹çš„å¯ä¿¡åº¦è‡³å…³é‡è¦ï¼ŒRLHF-Vçš„æå‡ºæœ‰åŠ©äºæé«˜å¤šæ¨¡æ€ç³»ç»Ÿçš„å¯é æ€§å’Œå®ç”¨æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation efficiency. Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the hallucination rate of the base MLLM by 34.8%, outperforming the concurrent LLaVA-RLHF trained on 10k annotated data. The final model achieves state-of-the-art performance in trustworthiness among open-source MLLMs, and shows better robustness than GPT-4V in preventing hallucinations aroused from over-generalization. We open-source our code, model, and data at https://github.com/RLHF-V/RLHF-V.

