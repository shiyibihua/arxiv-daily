---
layout: default
title: SeaLLMs -- Large Language Models for Southeast Asia
---

# SeaLLMs -- Large Language Models for Southeast Asia

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00738" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00738v2</a>
  <a href="https://arxiv.org/pdf/2312.00738.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00738v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00738v2', 'SeaLLMs -- Large Language Models for Southeast Asia')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Zhiqiang Hu, Chenhui Shen, Yew Ken Chia, Xingxuan Li, Jianyu Wang, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, Lidong Bing

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2024-07-01)

**å¤‡æ³¨**: Technical report, ACL 2024 DEMO TRACK

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSeaLLMsä»¥è§£å†³ä¸œå—äºšè¯­è¨€èµ„æºä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸œå—äºšè¯­è¨€` `è¯­è¨€åè§` `æ¨¡å‹è°ƒä¼˜` `æ–‡åŒ–é€‚åº”æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä½èµ„æºè¯­è¨€` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä½èµ„æºå’ŒåŒºåŸŸè¯­è¨€æ—¶å­˜åœ¨æ˜¾è‘—çš„è¯­è¨€åè§ï¼Œå¯¼è‡´è¿™äº›è¯­è¨€çš„ä½¿ç”¨å—åˆ°é™åˆ¶ã€‚
2. è®ºæ–‡æå‡ºçš„SeaLLMsç³»åˆ—æ¨¡å‹ä¸“æ³¨äºä¸œå—äºšè¯­è¨€ï¼Œé€šè¿‡æ‰©å±•è¯æ±‡å’Œä¸“é—¨çš„è°ƒä¼˜æ–¹æ³•ï¼Œå¢å¼ºäº†å¯¹åŒºåŸŸè¯­è¨€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSeaLLM-13båœ¨å¤šç§è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨éæ‹‰ä¸è¯­è¨€ä¸Šè¶…è¶Šäº†ChatGPT-3.5ï¼Œå…·æœ‰æ›´å¥½çš„å®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†ä»å­˜åœ¨å¯¹é«˜èµ„æºè¯­è¨€çš„åè§ï¼Œä½èµ„æºå’ŒåŒºåŸŸè¯­è¨€å¸¸å¸¸è¢«å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SeaLLMsï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä¸“æ³¨äºä¸œå—äºšè¯­è¨€çš„åˆ›æ–°è¯­è¨€æ¨¡å‹ã€‚SeaLLMsåŸºäºLlama-2æ¨¡å‹ï¼Œç»è¿‡æ‰©å±•è¯æ±‡çš„æŒç»­é¢„è®­ç»ƒã€ä¸“é—¨çš„æŒ‡ä»¤å’Œå¯¹é½è°ƒä¼˜ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åŒºåŸŸè¯­è¨€çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒSeaLLM-13bæ¨¡å‹åœ¨å¤šç§è¯­è¨€ä»»åŠ¡å’ŒåŠ©æ‰‹é£æ ¼çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ä¸Šï¼Œç›¸è¾ƒäºå¯æ¯”çš„å¼€æºæ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æ³°è¯­ã€æŸ¬åŸ”å¯¨è¯­ã€è€æŒè¯­å’Œç¼…ç”¸è¯­ç­‰éæ‹‰ä¸è¯­è¨€ä¸­ï¼Œæ€§èƒ½å¤§å¹…è¶…è¶ŠChatGPT-3.5ï¼ŒåŒæ—¶ä¿æŒè½»é‡å’Œæˆæœ¬æ•ˆç›Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„å…·ä½“é—®é¢˜æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºå’ŒåŒºåŸŸè¯­è¨€ä¸Šçš„è¡¨ç°ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€åå‘é«˜èµ„æºè¯­è¨€ï¼Œå¯¼è‡´ä¸œå—äºšè¯­è¨€çš„ä½¿ç”¨å—åˆ°é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯æ„å»ºSeaLLMsç³»åˆ—æ¨¡å‹ï¼Œä¸“æ³¨äºä¸œå—äºšè¯­è¨€ï¼Œé€šè¿‡æ‰©å±•è¯æ±‡å’Œé’ˆå¯¹æ€§çš„è°ƒä¼˜ï¼Œæå‡æ¨¡å‹å¯¹è¿™äº›è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSeaLLMsçš„æ•´ä½“æ¶æ„åŸºäºLlama-2æ¨¡å‹ï¼Œç»è¿‡æŒç»­çš„é¢„è®­ç»ƒå’Œå¯¹é½è°ƒä¼˜ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ‰©å±•è¯æ±‡ã€æŒ‡ä»¤è°ƒä¼˜å’Œæ–‡åŒ–é€‚åº”æ€§è°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé’ˆå¯¹ä¸œå—äºšè¯­è¨€çš„ä¸“é—¨è®¾è®¡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ å½“åœ°æ–‡åŒ–ã€ä¹ ä¿—å’Œè¯­è¨€ç‰¹å¾ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„é€šç”¨æ€§è®¾è®¡å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒSeaLLMsé‡‡ç”¨äº†æ‰©å±•çš„è¯æ±‡è¡¨ï¼Œç»“åˆç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¤„ç†åŒºåŸŸè¯­è¨€æ—¶çš„å‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œè°ƒä¼˜ç­–ç•¥ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥é€‚åº”ä¸åŒè¯­è¨€çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSeaLLM-13bæ¨¡å‹åœ¨å¤šç§è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨éæ‹‰ä¸è¯­è¨€ï¼ˆå¦‚æ³°è¯­ã€æŸ¬åŸ”å¯¨è¯­ã€è€æŒè¯­å’Œç¼…ç”¸è¯­ï¼‰ä¸Šï¼Œç›¸è¾ƒäºChatGPT-3.5ï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼Œå±•ç¤ºäº†å…¶åœ¨ä½èµ„æºè¯­è¨€å¤„ç†ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç¿»è¯‘ã€æ–‡åŒ–ä¼ æ’­å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚SeaLLMsèƒ½å¤Ÿä¸ºä¸œå—äºšåœ°åŒºçš„ç”¨æˆ·æä¾›æ›´ä¸ºç²¾å‡†å’Œè‡ªç„¶çš„è¯­è¨€æœåŠ¡ï¼Œä¿ƒè¿›å½“åœ°è¯­è¨€çš„æ•°å­—åŒ–å’Œåº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon the Llama-2 model and further advanced through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM-13b models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.

