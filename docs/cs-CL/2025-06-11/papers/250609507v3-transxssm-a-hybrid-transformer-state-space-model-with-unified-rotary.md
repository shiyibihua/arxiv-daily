---
layout: default
title: TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding
---

# TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09507" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09507v3</a>
  <a href="https://arxiv.org/pdf/2506.09507.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09507v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09507v3', 'TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bingheng Wu, Jingze Shi, Yifan Wu, Nan Tang, Yuyu Luo

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-06-18)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTransXSSMä»¥è§£å†³Transformerä¸çŠ¶æ€ç©ºé—´æ¨¡å‹çš„ç¼–ç ä¸å…¼å®¹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é•¿åºåˆ—å»ºæ¨¡` `ä½ç½®ç¼–ç ` `Transformer` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `æ··åˆæ¶æ„` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Transformerå’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨ä½ç½®ç¼–ç æœºåˆ¶ä¸Šå­˜åœ¨æ ¹æœ¬ä¸ä¸€è‡´ï¼Œå¯¼è‡´æ•´åˆæ—¶æ€§èƒ½ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºç»Ÿä¸€çš„æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆUnified RoPEï¼‰ï¼Œä¸ºTransformerå’ŒSSMæä¾›ä¸€è‡´çš„ä½ç½®ç¼–ç æ¡†æ¶ã€‚
3. TransXSSMåœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä¸Šåˆ†åˆ«æ¯”æ ‡å‡†Transformerå¿«42.3%å’Œ29.5%ï¼Œå¹¶åœ¨å‡†ç¡®æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Transformersåœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åˆ™æ”¯æŒçº¿æ€§æ—¶é—´åºåˆ—å»ºæ¨¡ã€‚å°½ç®¡è¿™ä¸¤ç§æ¶æ„å…·æœ‰ååŒæ½œåŠ›ï¼Œä½†ç”±äºå…¶ä½ç½®ç¼–ç æœºåˆ¶çš„æ ¹æœ¬ä¸ä¸€è‡´ï¼ŒäºŒè€…çš„æ•´åˆé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚Transformersä¾èµ–æ˜¾å¼çš„æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ï¼Œè€ŒSSMsåˆ™é€šè¿‡å·ç§¯åˆ©ç”¨éšå¼ä½ç½®è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆUnified RoPEï¼‰æ–¹æ³•ï¼Œä»è€Œä¸ºè‡ªæ³¨æ„åŠ›å’ŒçŠ¶æ€ç©ºé—´ç»„ä»¶å»ºç«‹äº†ä¸€è‡´çš„ä½ç½®ç¼–ç æ¡†æ¶ã€‚åŸºäºUnified RoPEï¼Œæœ¬æ–‡å¼•å…¥äº†TransXSSMï¼Œä¸€ç§åœ¨ç»Ÿä¸€ä½ç½®ç¼–ç æ–¹æ¡ˆä¸‹æœ‰æ•ˆæ•´åˆTransformerå’ŒSSMå±‚çš„æ··åˆæ¶æ„ã€‚å®éªŒè¡¨æ˜ï¼ŒTransXSSMåœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä¸Šåˆ†åˆ«æ¯”æ ‡å‡†Transformeræ¨¡å‹å¿«42.3%å’Œ29.5%ï¼Œå¹¶åœ¨è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šè¶…è¶ŠTransformeråŸºçº¿è¶…è¿‡4%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³Transformerä¸çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨ä½ç½®ç¼–ç ä¸Šçš„ä¸å…¼å®¹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•´åˆæ—¶å¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™å’Œä¸è¿ç»­æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç»Ÿä¸€çš„æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆUnified RoPEï¼‰ï¼Œä½¿å¾—è‡ªæ³¨æ„åŠ›å’ŒçŠ¶æ€ç©ºé—´ç»„ä»¶èƒ½å¤Ÿå…±äº«ä¸€è‡´çš„ä½ç½®ç¼–ç ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTransXSSMæ¶æ„åŒ…æ‹¬Transformerå±‚å’ŒçŠ¶æ€ç©ºé—´å±‚ï¼ŒäºŒè€…é€šè¿‡Unified RoPEè¿›è¡Œè¿æ¥ï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„ç¼–ç æ¡†æ¶ï¼Œæ”¯æŒé«˜æ•ˆçš„é•¿åºåˆ—å»ºæ¨¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šUnified RoPEæ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒè§£å†³äº†ä¼ ç»ŸTransformerå’ŒSSMåœ¨ä½ç½®ç¼–ç ä¸Šçš„ä¸ä¸€è‡´æ€§ï¼Œä½¿å¾—æ··åˆæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰é•¿è·ç¦»ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒTransXSSMçš„å‚æ•°è®¾ç½®ç»è¿‡ä¼˜åŒ–ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±ï¼Œç½‘ç»œç»“æ„ç»“åˆäº†Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒSSMçš„çŠ¶æ€ç©ºé—´è¡¨ç¤ºï¼Œç¡®ä¿äº†é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TransXSSMåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šæ¯”æ ‡å‡†Transformerå¿«42.3%ï¼Œæ¨ç†é€Ÿåº¦å¿«29.5%ã€‚åœ¨è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶å‡†ç¡®æ€§è¶…è¶ŠTransformeråŸºçº¿è¶…è¿‡4%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ—¶é—´åºåˆ—é¢„æµ‹å’Œå…¶ä»–éœ€è¦é•¿åºåˆ—å»ºæ¨¡çš„ä»»åŠ¡ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ï¼ŒTransXSSMèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformers exhibit proficiency in capturing long-range dependencies, whereas State Space Models (SSMs) facilitate linear-time sequence modeling. Notwithstanding their synergistic potential, the integration of these architectures presents a significant challenge, primarily attributable to a fundamental incongr inuity their respective positional encoding mechanisms: Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs leverage implicit positional representations via convolutions. This divergence often precipitates discontinuities and suboptimal performance.To address this impediment, we propose a unified rotary position embedding (Unified RoPE) methodology, thereby establishing a consistent positional encoding framework for both self-attention and state-space components. Using this Unified RoPE, we introduce TransXSSM, a hybrid architecture that coherently integrates the Transformer and SSM layers under this unified positional encoding scheme. At a 4 sequenceK length, TransXSSM exhibits training and inference speeds that are 42.3% and 29.5% faster, respectively, relative to standard Transformer models. It also delivers higher accuracy: under comparable settings, it surpasses a Transformer baseline by over 4% on language modeling benchmarks.TransXSSM furthermore scales more effectively: TransXSSM-1.3B gains 7.22% in average accuracy over its 320M version (versus about 6% gains for equivalent Transformers or SSMs). Our results show that unified positional encoding resolves positional incompatibility in hybrid models, enabling efficient, high-performance long-context modeling.

