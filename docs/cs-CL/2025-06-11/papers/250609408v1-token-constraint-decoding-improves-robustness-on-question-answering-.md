---
layout: default
title: Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models
---

# Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09408" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09408v1</a>
  <a href="https://arxiv.org/pdf/2506.09408.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09408v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09408v1', 'Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jui-Ming Yao, Hao-Yuan Chen, Zi-Xian Tang, Bing-Jia Tan, Sheng-Wei Peng, Bing-Cheng Xie, Shun-Feng Su

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTokençº¦æŸè§£ç ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„é—®ç­”é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é—®ç­”ç³»ç»Ÿ` `é²æ£’æ€§` `Tokençº¦æŸè§£ç ` `è¾“å…¥å™ªå£°` `æ¨¡å‹æ— å…³` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹è¾“å…¥å™ªå£°æ—¶è¡¨ç°å‡ºé«˜åº¦è„†å¼±æ€§ï¼Œå¯¼è‡´é—®ç­”æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºTokençº¦æŸè§£ç ï¼ˆTCDï¼‰ï¼Œé€šè¿‡å¼ºåˆ¶å¯¹é½tokençº§é¢„æµ‹æ¥æå‡æ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTCDä¸æç¤ºå·¥ç¨‹ç»“åˆä½¿ç”¨æ—¶ï¼Œèƒ½å¤Ÿä¸ºè¾ƒå¼±æ¨¡å‹å¸¦æ¥é«˜è¾¾39%çš„æ€§èƒ½æå‡ï¼Œæ˜¾è‘—æ”¹å–„äº†é—®ç­”æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šé€‰é¢˜é—®ç­”ï¼ˆMCQAï¼‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹è¾“å…¥å¾®å°æ‰°åŠ¨é«˜åº¦æ•æ„Ÿã€‚æœ¬æ–‡æå‡ºå¹¶è¯„ä¼°äº†Tokençº¦æŸè§£ç ï¼ˆTCDï¼‰ï¼Œè¿™ä¸€ç®€å•è€Œæœ‰æ•ˆçš„æ¨ç†æ—¶ç®—æ³•é€šè¿‡å¼ºåˆ¶å¯¹é½tokençº§é¢„æµ‹æ¥å¢å¼ºåœ¨å™ªå£°ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚é€šè¿‡åœ¨CommonsenseQAã€MMLUå’ŒMMLU-Proä¸Šçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†TCDï¼Œå°¤å…¶æ˜¯ä¸æç¤ºå·¥ç¨‹ï¼ˆPEï¼‰ç»“åˆæ—¶ï¼Œæ˜¾è‘—æ¢å¤äº†å› è¾“å…¥å™ªå£°è€Œä¸‹é™çš„æ€§èƒ½ï¼Œä¸ºè¾ƒå¼±æ¨¡å‹å¦‚Gemma3 1Bå¸¦æ¥äº†é«˜è¾¾39%çš„ç»å¯¹å¢ç›Šã€‚æƒ©ç½šæ‰«æ åˆ†æè¿›ä¸€æ­¥æ­ç¤ºTCDéšå¼æ­£åˆ™åŒ–äº†è¿‡äºè‡ªä¿¡çš„è¾“å‡ºï¼Œä¸åŒæ¨¡å‹éœ€è¦ä¸åŒçš„æƒ©ç½šè°ƒåº¦ä»¥æœ€å¤§åŒ–é²æ£’æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç¡®ç«‹äº†TCDä½œä¸ºä¸€ç§å®ç”¨çš„ã€æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œä»¥æé«˜åœ¨ç°å®ä¸–ç•Œç¼ºé™·ä¸‹çš„æ¨ç†ç¨³å®šæ€§ï¼Œä¸ºLLMsåœ¨å®‰å…¨å…³é”®æˆ–ç”¨æˆ·é¢å¯¹çš„åº”ç”¨ä¸­çš„å¯é éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šé€‰é¢˜é—®ç­”ä¸­å¯¹è¾“å…¥å¾®å°æ‰°åŠ¨çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•åœ¨å™ªå£°ç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºTokençº¦æŸè§£ç ï¼ˆTCDï¼‰ï¼Œé€šè¿‡åœ¨æ¨ç†é˜¶æ®µå¼ºåˆ¶å¯¹é½tokençº§é¢„æµ‹ï¼Œä»è€Œå¢å¼ºæ¨¡å‹åœ¨å™ªå£°æ¡ä»¶ä¸‹çš„é²æ£’æ€§ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å‡å°‘æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨çš„æ•æ„Ÿæ€§ï¼Œæé«˜é—®ç­”çš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTCDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å¤„ç†ã€tokené¢„æµ‹å¯¹é½å’Œè¾“å‡ºç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚åœ¨è¾“å…¥å¤„ç†é˜¶æ®µï¼Œæ¨¡å‹æ¥æ”¶ç»è¿‡æ‰°åŠ¨çš„è¾“å…¥ï¼›åœ¨tokené¢„æµ‹å¯¹é½é˜¶æ®µï¼ŒTCDé€šè¿‡çº¦æŸæœºåˆ¶ç¡®ä¿ç”Ÿæˆçš„tokenä¹‹é—´çš„ä¸€è‡´æ€§ï¼›æœ€åï¼Œåœ¨è¾“å‡ºç”Ÿæˆé˜¶æ®µï¼Œæ¨¡å‹åŸºäºå¯¹é½åçš„tokenç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šTCDçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨¡å‹æ— å…³æ€§å’Œç®€å•æ€§ï¼Œèƒ½å¤Ÿé€‚ç”¨äºä¸åŒçš„è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”é€šè¿‡å¯¹tokençº§åˆ«çš„çº¦æŸæ¥æå‡é²æ£’æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¤„ç†æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨TCDä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æƒ©ç½šè°ƒåº¦ç­–ç•¥ï¼Œä»¥é€‚åº”ä¸åŒæ¨¡å‹çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å¯¹è¿‡äºè‡ªä¿¡è¾“å‡ºçš„æ­£åˆ™åŒ–ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ•´ä½“è¡¨ç°ã€‚é€šè¿‡æƒ©ç½šæœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨é¢å¯¹å™ªå£°æ—¶ä¿æŒæ›´å¥½çš„è¾“å‡ºç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTCDåœ¨CommonsenseQAã€MMLUå’ŒMMLU-Proä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å¯¹äºè¾ƒå¼±æ¨¡å‹Gemma3 1Bï¼Œæ€§èƒ½æå‡é«˜è¾¾39%ã€‚æ­¤å¤–ï¼Œæƒ©ç½šæ‰«æ åˆ†æè¡¨æ˜ï¼ŒTCDæœ‰æ•ˆåœ°æ­£åˆ™åŒ–äº†è¿‡äºè‡ªä¿¡çš„è¾“å‡ºï¼Œå¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬å®‰å…¨å…³é”®çš„é—®ç­”ç³»ç»Ÿã€ç”¨æˆ·äº¤äº’å¼åº”ç”¨ä»¥åŠéœ€è¦é«˜é²æ£’æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„ç¨³å®šæ€§ï¼ŒTCDèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´å¯é çš„æ”¯æŒï¼Œå°¤å…¶æ˜¯åœ¨ç”¨æˆ·ä½“éªŒå’Œå®‰å…¨æ€§è‡³å…³é‡è¦çš„é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated impressive performance on multiple-choice question answering (MCQA) benchmarks, yet they remain highly vulnerable to minor input perturbations. In this paper, we introduce and evaluate Token Constraint Decoding (TCD). This simple yet effective inference-time algorithm enforces alignment between token-level predictions to enhance robustness in noisy settings. Through extensive experiments on CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired with prompt engineering (PE) fixes, significantly restores performance degraded by input noise, yielding up to +39\% absolute gains for weaker models like Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly regularizes overconfident outputs, with different models requiring distinct penalty schedules to maximize resilience. Our findings establish TCD as a practical, model-agnostic approach for improving reasoning stability under real-world imperfections and pave the way for more reliable deployment of LLMs in safety-critical or user-facing applications.

