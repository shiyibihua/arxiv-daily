---
layout: default
title: DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts
---

# DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09351" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09351v1</a>
  <a href="https://arxiv.org/pdf/2506.09351.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09351v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09351v1', 'DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuchen Feng, Bowen Shen, Naibin Gu, Jiaxuan Zhao, Peng Fu, Zheng Lin, Weiping Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDIVEæ–¹æ³•ä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ ·æ€§é‡å»º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹é‡å»º` `å¤šæ ·æ€§å¢å¼º` `è®­ç»ƒæ•ˆç‡` `å‰ªææŠ€æœ¯` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡å»ºæ–¹æ³•åœ¨ä¸“å®¶å¤šæ ·æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯èƒ½å¯¼è‡´å†—ä½™ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºDIVEæ–¹æ³•ï¼Œé€šè¿‡é¢†åŸŸäº²å’Œæ€§æŒ–æ˜å’Œå‰ªæé‡å»ºä¸“å®¶ï¼Œå¢å¼ºæ¨¡å‹çš„å¤šæ ·æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDIVEåœ¨è®­ç»ƒæ•ˆç‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œä¸”å‡†ç¡®æ€§æŸå¤±æœ€å°ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œé€šè¿‡é€‰æ‹©æ€§æ¿€æ´»éƒ¨åˆ†å‚æ•°å®ç°é«˜æ•ˆæ€§ã€‚å°½ç®¡MoE LLMåœ¨æ¨ç†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ä»å¤´è®­ç»ƒå¤§é‡ä¸“å®¶çš„æˆæœ¬é«˜æ˜‚ï¼Œè€Œå°†ç¨ å¯†LLMé‡å»ºä¸ºMoE LLMæ˜¾è‘—é™ä½äº†è®­ç»ƒé¢„ç®—ã€‚ç„¶è€Œï¼Œç°æœ‰é‡å»ºæ–¹æ³•å¾€å¾€å¿½è§†ä¸“å®¶ä¹‹é—´çš„å¤šæ ·æ€§ï¼Œå¯¼è‡´æ½œåœ¨å†—ä½™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDIVEçš„å¤šæ ·æ€§å¢å¼ºé‡å»ºæ–¹æ³•ï¼ŒåŸºäºå¯¹ä¸åŒæ ¡å‡†æ•°æ®é›†è¿›è¡Œå‰ªæåè§‚å¯Ÿåˆ°çš„LLMæ˜¾è‘—å¤šæ ·æ€§ã€‚DIVEæ–¹æ³•åŒ…æ‹¬é¢†åŸŸäº²å’Œæ€§æŒ–æ˜ã€åŸºäºå‰ªæçš„ä¸“å®¶é‡å»ºå’Œé«˜æ•ˆå†è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒDIVEåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶å®ç°äº†è®­ç»ƒæ•ˆç‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å‰ªæå’ŒMoEé‡å»ºæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰é‡å»ºæ–¹æ³•å¿½è§†ä¸“å®¶å¤šæ ·æ€§çš„é—®é¢˜ï¼Œå¯¼è‡´å†—ä½™å’Œæ€§èƒ½ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•åœ¨ä»ç¨ å¯†LLMé‡å»ºä¸ºMoE LLMæ—¶ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨ä¸åŒæ ¡å‡†æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDIVEæ–¹æ³•çš„æ ¸å¿ƒåœ¨äºé€šè¿‡é¢†åŸŸäº²å’Œæ€§æŒ–æ˜å’Œå‰ªæé‡å»ºä¸“å®¶ï¼Œå¢å¼ºæ¨¡å‹çš„å¤šæ ·æ€§ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å¹¶é™ä½å†—ä½™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDIVEçš„æ•´ä½“æµç¨‹åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¢†åŸŸäº²å’Œæ€§æŒ–æ˜ã€å‰ªæåŸºç¡€ä¸Šçš„ä¸“å®¶é‡å»ºå’Œé«˜æ•ˆå†è®­ç»ƒã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ†æä¸åŒæ•°æ®é›†çš„æ ¡å‡†ç»“æœï¼ŒæŒ–æ˜é¢†åŸŸé—´çš„äº²å’Œæ€§ï¼›ç„¶åï¼Œè¿›è¡Œå‰ªæå’Œé‡ç»„FFNæ¨¡å—ï¼›æœ€åï¼Œå¯¹è·¯ç”±å™¨ã€ä¸“å®¶å’Œå½’ä¸€åŒ–æ¨¡å—è¿›è¡Œé«˜æ•ˆå†è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šDIVEçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šæ ·æ€§å¢å¼ºçš„é‡å»ºç­–ç•¥ï¼Œå¼ºè°ƒäº†ä¸åŒæ ¡å‡†æ•°æ®é›†å¯¹ä¸“å®¶å¤šæ ·æ€§çš„å½±å“ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€å‰ªæç­–ç•¥æœ¬è´¨ä¸Šæœ‰æ‰€åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DIVEä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬å‰ªæç­–ç•¥çš„é€‰æ‹©ã€æŸå¤±å‡½æ•°çš„ä¼˜åŒ–ä»¥åŠFFNæ¨¡å—çš„é‡ç»„æ–¹å¼ï¼Œç¡®ä¿åœ¨é‡å»ºè¿‡ç¨‹ä¸­æœ€å¤§é™åº¦åœ°ä¿ç•™æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDIVEåœ¨è®­ç»ƒæ•ˆç‡ä¸Šæ˜¾è‘—æå‡ï¼Œå‡†ç¡®æ€§æŸå¤±æœ€å°ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å‰ªæå’ŒMoEé‡å»ºæ–¹æ³•ï¼Œä¸”åœ¨ç›¸åŒæ¿€æ´»å‚æ•°æ•°é‡ä¸‹ï¼ŒDIVEçš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DIVEæ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦é«˜æ•ˆæ¨ç†å’Œä½æˆæœ¬è®­ç»ƒçš„åœºæ™¯ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚æœªæ¥ï¼ŒDIVEçš„æ€æƒ³å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œæ¨åŠ¨æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œå¤šæ ·æ€§æå‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture achieve high cost-efficiency by selectively activating a subset of the parameters. Despite the inference efficiency of MoE LLMs, the training of extensive experts from scratch incurs substantial overhead, whereas reconstructing a dense LLM into an MoE LLM significantly reduces the training budget. However, existing reconstruction methods often overlook the diversity among experts, leading to potential redundancy. In this paper, we come up with the observation that a specific LLM exhibits notable diversity after being pruned on different calibration datasets, based on which we present a Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE includes domain affinity mining, pruning-based expert reconstruction, and efficient retraining. Specifically, the reconstruction includes pruning and reassembly of the feed-forward network (FFN) module. After reconstruction, we efficiently retrain the model on routers, experts and normalization modules. We implement DIVE on Llama-style LLMs with open-source training corpora. Experiments show that DIVE achieves training efficiency with minimal accuracy trade-offs, outperforming existing pruning and MoE reconstruction methods with the same number of activated parameters.

