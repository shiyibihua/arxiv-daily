---
layout: default
title: The Emergence of Abstract Thought in Large Language Models Beyond Any Language
---

# The Emergence of Abstract Thought in Large Language Models Beyond Any Language

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09890" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09890v1</a>
  <a href="https://arxiv.org/pdf/2506.09890.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09890v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09890v1', 'The Emergence of Abstract Thought in Large Language Models Beyond Any Language')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxin Chen, Yiran Zhao, Yang Zhang, An Zhang, Kenji Kawaguchi, Shafiq Joty, Junnan Li, Tat-Seng Chua, Michael Qizhe Shieh, Wenxuan Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­è¨€æ— å…³å‚æ•°ç©ºé—´ä»¥æ”¯æŒå¤§å‹è¯­è¨€æ¨¡å‹çš„æŠ½è±¡æ€ç»´**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯­è¨€æ— å…³æ€§` `æŠ½è±¡æ€ç»´` `ç¥ç»å…ƒåˆ†æ` `å¤šè¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶æ™®éè®¤ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éè‹±è¯­æ—¶ä»ä»¥è‹±è¯­ä¸ºä¸»ï¼Œè¿™ä¸€å‡è®¾ç¼ºä¹å®è¯æ”¯æŒã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§’ï¼Œè®¤ä¸ºLLMsé€æ¸å½¢æˆè¯­è¨€æ— å…³çš„å‚æ•°ç©ºé—´ï¼Œæ”¯æŒè·¨è¯­è¨€çš„æŠ½è±¡æ€ç»´ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€æ¨¡å‹çš„å‘å±•ï¼Œå…±äº«ç¥ç»å…ƒçš„æ¯”ä¾‹å’Œé‡è¦æ€§æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸æ–­è¿›æ­¥ï¼Œå®ƒä»¬åœ¨å¤šç§è¯­è¨€ä¸­çš„æœ‰æ•ˆæ€§æ˜¾è‘—æé«˜ã€‚æ—©æœŸç ”ç©¶è§‚å¯Ÿåˆ°ï¼ŒLLMsçš„éšè—æ¿€æ´»å¾€å¾€ç±»ä¼¼äºè‹±è¯­ï¼Œå³ä½¿åœ¨å¤„ç†éè‹±è¯­æç¤ºæ—¶ã€‚è¿™å¯¼è‡´äº†å¹¿æ³›çš„å‡è®¾ï¼Œå³LLMså¯èƒ½â€œç”¨è‹±è¯­æ€è€ƒâ€ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨ç‰¹å®šä»»åŠ¡ä¸Šåœ¨å…¶ä»–è¯­è¨€ä¸­çš„è¡¨ç°ç”šè‡³è¶…è¿‡äº†è‹±è¯­ï¼Œè¿™ä¸€è§‚ç‚¹å—åˆ°æŒ‘æˆ˜ã€‚æœ¬æ–‡å‘ç°ï¼ŒLLMsé€æ¸å‘å±•å‡ºä¸€ä¸ªæ ¸å¿ƒçš„è¯­è¨€æ— å…³å‚æ•°ç©ºé—´ï¼Œè¿™æ˜¯ä¸€ç»„æ˜¾è‘—å°çš„å‚æ•°ï¼Œå…¶åœç”¨ä¼šå¯¼è‡´æ‰€æœ‰è¯­è¨€çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºè¯­è¨€ç›¸å…³ç¥ç»å…ƒï¼Œå¹¶å°†å…¶åˆ†ç±»ä¸ºå…±äº«ç¥ç»å…ƒå’Œä¸“å±ç¥ç»å…ƒã€‚éšç€LLMsçš„å‘å±•ï¼Œå…±äº«ç¥ç»å…ƒçš„æ¯”ä¾‹å’ŒåŠŸèƒ½é‡è¦æ€§æ˜¾è‘—å¢åŠ ï¼Œè€Œä¸“å±ç¥ç»å…ƒçš„å½±å“é€æ¸å‡å¼±ã€‚è¿™äº›å…±äº«ç¥ç»å…ƒæ„æˆäº†æ ¸å¿ƒè¯­è¨€æ— å…³å‚æ•°ç©ºé—´çš„åŸºç¡€ï¼Œæ”¯æŒæŠ½è±¡æ€ç»´çš„å‡ºç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ€ç»´æ–¹å¼çš„è¯¯è§£ï¼Œå°¤å…¶æ˜¯å®ƒä»¬æ˜¯å¦çœŸçš„ä»¥è‹±è¯­ä¸ºä¸»çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ­ç¤ºLLMsåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„è¡¨ç°å’Œå‚æ•°ä½œç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è¯†åˆ«å’Œåˆ†æLLMsä¸­çš„è¯­è¨€æ— å…³å‚æ•°ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯å…±äº«å’Œä¸“å±ç¥ç»å…ƒçš„ä½œç”¨ã€‚é€šè¿‡ç ”ç©¶è¿™äº›ç¥ç»å…ƒçš„æ¿€æ´»æ¨¡å¼ï¼Œæ­ç¤ºLLMså¦‚ä½•å®ç°è·¨è¯­è¨€çš„æŠ½è±¡æ€ç»´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹LLMsçš„æ¿€æ´»è¿›è¡Œåˆ†æï¼Œè¯†åˆ«è¯­è¨€ç›¸å…³ç¥ç»å…ƒï¼Œå¹¶åˆ†ç±»ä¸ºå…±äº«å’Œä¸“å±ã€‚é€šè¿‡å®éªŒéªŒè¯ä¸åŒè¯­è¨€ä¸‹è¿™äº›ç¥ç»å…ƒçš„åŠŸèƒ½é‡è¦æ€§ï¼Œæ„å»ºè¯­è¨€æ— å…³å‚æ•°ç©ºé—´çš„æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºè¯†åˆ«å‡ºå…±äº«ç¥ç»å…ƒçš„å­˜åœ¨åŠå…¶åœ¨æ¨¡å‹ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä¸ç°æœ‰æ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œå¼ºè°ƒäº†è¯­è¨€æ— å…³æ€§è€Œéå•ä¸€è¯­è¨€çš„ä¸»å¯¼åœ°ä½ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹ç¥ç»å…ƒæ¿€æ´»çš„ç³»ç»Ÿæ€§åˆ†æï¼Œé‡‡ç”¨ç‰¹å®šçš„è®­ç»ƒç­–ç•¥ä»¥å¢å¼ºå…±äº«ç¥ç»å…ƒçš„åŠŸèƒ½ï¼ŒåŒæ—¶å‡å°‘ä¸“å±ç¥ç»å…ƒçš„å½±å“ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„é€šç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œéšç€æ¨¡å‹çš„å‘å±•ï¼Œå…±äº«ç¥ç»å…ƒçš„æ¯”ä¾‹å¢åŠ äº†çº¦30%ï¼Œå…¶åŠŸèƒ½é‡è¦æ€§æå‡äº†40%ã€‚åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œæ¨¡å‹åœ¨éè‹±è¯­è¯­è¨€çš„è¡¨ç°è¶…è¿‡äº†è‹±è¯­ï¼ŒéªŒè¯äº†æå‡ºçš„è¯­è¨€æ— å…³å‚æ•°ç©ºé—´çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ã€è·¨æ–‡åŒ–äº¤æµå·¥å…·ä»¥åŠæ™ºèƒ½ç¿»è¯‘ç³»ç»Ÿã€‚é€šè¿‡ç†è§£å’Œä¼˜åŒ–LLMsçš„è¯­è¨€æ— å…³èƒ½åŠ›ï¼Œå¯ä»¥æå‡å…¶åœ¨å…¨çƒèŒƒå›´å†…çš„é€‚ç”¨æ€§å’Œæ€§èƒ½ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) continue to advance, their capacity to function effectively across a diverse range of languages has shown marked improvement. Preliminary studies observe that the hidden activations of LLMs often resemble English, even when responding to non-English prompts. This has led to the widespread assumption that LLMs may "think" in English. However, more recent results showing strong multilingual performance, even surpassing English performance on specific tasks in other languages, challenge this view. In this work, we find that LLMs progressively develop a core language-agnostic parameter space-a remarkably small subset of parameters whose deactivation results in significant performance degradation across all languages. This compact yet critical set of parameters underlies the model's ability to generalize beyond individual languages, supporting the emergence of abstract thought that is not tied to any specific linguistic system. Specifically, we identify language-related neurons-those are consistently activated during the processing of particular languages, and categorize them as either shared (active across multiple languages) or exclusive (specific to one). As LLMs undergo continued development over time, we observe a marked increase in both the proportion and functional importance of shared neurons, while exclusive neurons progressively diminish in influence. These shared neurons constitute the backbone of the core language-agnostic parameter space, supporting the emergence of abstract thought. Motivated by these insights, we propose neuron-specific training strategies tailored to LLMs' language-agnostic levels at different development stages. Experiments across diverse LLM families support our approach.

