---
layout: default
title: RePO: Replay-Enhanced Policy Optimization
---

# RePO: Replay-Enhanced Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09340" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09340v1</a>
  <a href="https://arxiv.org/pdf/2506.09340.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09340v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09340v1', 'RePO: Replay-Enhanced Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siheng Li, Zhanhui Zhou, Wai Lam, Chao Yang, Chaochao Lu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: Project Page: https://github.com/SihengLi99/RePO

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/SihengLi99/RePO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReplay-Enhanced Policy Optimizationä»¥è§£å†³RLæ•°æ®æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç­–ç•¥ä¼˜åŒ–` `é‡æ”¾ç­–ç•¥` `æ•°æ®æ•ˆç‡` `æ•°å­¦æ¨ç†` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œæ•°æ®åˆ©ç”¨ç‡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„RePOæ–¹æ³•é€šè¿‡é‡æ”¾ç­–ç•¥ä»é‡æ”¾ç¼“å†²åŒºä¸­è·å–ç¦»çº¿æ ·æœ¬ï¼Œä¼˜åŒ–ç­–ç•¥æ—¶èƒ½å¤Ÿåˆ©ç”¨æ›´ä¸°å¯Œçš„æ ·æœ¬ä¿¡æ¯ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRePOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼ŒåŒæ—¶å¢åŠ äº†è®¡ç®—æˆæœ¬å’Œæœ‰æ•ˆä¼˜åŒ–æ­¥éª¤çš„æ•°é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­è‡³å…³é‡è¦ã€‚è¿‘æœŸçš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•é€šè¿‡æ¯ä¸ªæç¤ºä¼°è®¡å¤šä¸ªåœ¨çº¿ç­–ç•¥è¾“å‡ºçš„ä¼˜åŠ¿ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ä¸”æ•°æ®æ•ˆç‡ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Replay-Enhanced Policy Optimizationï¼ˆRePOï¼‰ï¼Œåˆ©ç”¨å¤šæ ·åŒ–çš„é‡æ”¾ç­–ç•¥ä»é‡æ”¾ç¼“å†²åŒºä¸­æ£€ç´¢ç¦»çº¿æ ·æœ¬ï¼Œä½¿å¾—æ¯ä¸ªæç¤ºçš„ç­–ç•¥ä¼˜åŒ–èƒ½å¤ŸåŸºäºæ›´å¹¿æ³›å’Œå¤šæ ·çš„æ ·æœ¬é›†è¿›è¡Œã€‚å®éªŒè¡¨æ˜ï¼ŒRePOåœ¨äº”ä¸ªLLMsçš„ä¸ƒä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šï¼Œç›¸è¾ƒäºGRPOï¼ŒQwen2.5-Math-1.5Bå’ŒQwen3-1.7Bçš„å¹³å‡æ€§èƒ½åˆ†åˆ«æå‡äº†18.4å’Œ4.1åˆ†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•åœ¨æ¯ä¸ªæç¤ºä¸Šéœ€è¦å¤šä¸ªåœ¨çº¿è¾“å‡ºä»¥ä¼°è®¡ä¼˜åŠ¿ï¼Œè¿™å¯¼è‡´äº†é«˜è®¡ç®—æˆæœ¬å’Œä½æ•°æ®æ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRePOé€šè¿‡å¼•å…¥å¤šæ ·åŒ–çš„é‡æ”¾ç­–ç•¥ï¼Œä»é‡æ”¾ç¼“å†²åŒºä¸­æå–ç¦»çº¿æ ·æœ¬ï¼Œä½¿å¾—ç­–ç•¥ä¼˜åŒ–èƒ½å¤ŸåŸºäºæ›´å¹¿æ³›çš„æ ·æœ¬é›†è¿›è¡Œï¼Œä»è€Œæé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRePOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é‡æ”¾ç¼“å†²åŒºçš„ç®¡ç†ã€æ ·æœ¬çš„å¤šæ ·åŒ–é€‰æ‹©ä»¥åŠç­–ç•¥ä¼˜åŒ–çš„å®æ–½ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ ·æœ¬é‡‡é›†ã€ä¼˜åŠ¿ä¼°è®¡å’Œç­–ç•¥æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šRePOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨ç¦»çº¿æ ·æœ¬è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œè¿™ä¸ä¼ ç»Ÿçš„åœ¨çº¿ç­–ç•¥ä¼˜åŒ–æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬çš„å¤šæ ·æ€§å’Œåˆ©ç”¨ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼ŒRePOè®¾ç½®äº†åœ¨çº¿å’Œç¦»çº¿æ ·æœ¬æ•°é‡å‡ä¸º8ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•æ¥æå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRePOåœ¨Qwen2.5-Math-1.5Bå’ŒQwen3-1.7Bæ¨¡å‹ä¸Šåˆ†åˆ«å®ç°äº†18.4å’Œ4.1çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºGRPOï¼Œè®¡ç®—æˆæœ¬ä»…å¢åŠ äº†15%ï¼Œè€Œæœ‰æ•ˆä¼˜åŒ–æ­¥éª¤å¢åŠ äº†48%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–æ•ˆç‡ï¼ŒRePOèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡æ¨¡å‹çš„å“åº”è´¨é‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) is vital for optimizing large language models (LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages using multiple on-policy outputs per prompt, leading to high computational costs and low data efficiency. To address this, we introduce Replay-Enhanced Policy Optimization (RePO), which leverages diverse replay strategies to retrieve off-policy samples from a replay buffer, allowing policy optimization based on a broader and more diverse set of samples for each prompt. Experiments on five LLMs across seven mathematical reasoning benchmarks demonstrate that RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further analysis indicates that RePO increases computational cost by $15\%$ while raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B, with both on-policy and off-policy sample numbers set to $8$. The repository can be accessed at https://github.com/SihengLi99/RePO.

