---
layout: default
title: Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting
---

# Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09428" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09428v2</a>
  <a href="https://arxiv.org/pdf/2506.09428.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09428v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09428v2', 'Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fei Ding, Baiqiao Wang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-06-28)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ”¹è¿›çš„ç›‘ç£å¾®è°ƒæ–¹æ³•ä»¥ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç›‘ç£å¾®è°ƒ` `ç¾éš¾æ€§é—å¿˜` `æ•°æ®åˆæˆ` `é¢†åŸŸé€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æå‡ç‰¹å®šä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå¾€å¾€å¯¼è‡´æ¨¡å‹åœ¨ä¸€èˆ¬èƒ½åŠ›ä¸Šçš„æ˜¾è‘—ä¸‹é™ï¼Œå½¢æˆç¾éš¾æ€§é—å¿˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„SFTæ–¹æ³•ï¼Œé€šè¿‡é‡æ„åŸºç¡€æ¨¡å‹çš„æŒ‡ä»¤åˆ†å¸ƒï¼Œç»“åˆåˆæˆæ•°æ®ä¸é¢†åŸŸç‰¹å®šæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæœ‰æ•ˆç¼“è§£é—å¿˜é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹é€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼Œç‰¹å®šä»»åŠ¡æ€§èƒ½ä¹Ÿå¾—åˆ°äº†æå‡ï¼Œè¶…è¶Šäº†ä¼ ç»ŸåŸºçº¿æ¨¡å‹çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜¯æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æŒ‡ä»¤è·Ÿéšèƒ½åŠ›å’Œé€‚åº”ç‰¹å®šé¢†åŸŸçš„é‡è¦æ­¥éª¤ã€‚ç„¶è€Œï¼ŒSFTå¸¸å¸¸å¯¼è‡´æ¨¡å‹ä¸€èˆ¬èƒ½åŠ›çš„ä¸‹é™ï¼Œè¿™ä¸€ç°è±¡è¢«ç§°ä¸ºç¾éš¾æ€§é—å¿˜ã€‚å½“ç¬¬ä¸‰æ–¹å®è·µè€…å¾®è°ƒå¼€æºæ¨¡å‹æ—¶ï¼ŒåŸå§‹SFTæ•°æ®é€šå¸¸ä¸å¯ç”¨ï¼Œä»è€ŒåŠ å‰§äº†è¿™ä¸€é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”å…·æœ‰æˆæœ¬æ•ˆç›Šçš„SFTæ–¹æ³•ï¼Œæœ‰æ•ˆç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼Œè€Œæ— éœ€è®¿é—®åŸå§‹SFTæ•°æ®ã€‚è¯¥æ–¹æ³•é¦–å…ˆé‡æ„åŸºç¡€æ¨¡å‹çš„å¯èƒ½æŒ‡ä»¤åˆ†å¸ƒï¼Œç„¶åé‡‡ç”¨å¤šæ¨¡å‹ç”Ÿæˆå’Œè¿‡æ»¤ç®¡é“åˆæˆé«˜è´¨é‡çš„é€šç”¨æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…ä¿ç•™äº†æ¨¡å‹åœ¨ä¸€èˆ¬é¢†åŸŸçš„èƒ½åŠ›ï¼Œè¿˜æé«˜äº†ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä½¿ç”¨å…¬å¼€SFTæ•°æ®é›†çš„åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç›‘ç£å¾®è°ƒè¿‡ç¨‹ä¸­å‡ºç°çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒæ—¶ï¼Œå¾€å¾€æ— æ³•é¿å…æ¨¡å‹åœ¨ä¸€èˆ¬èƒ½åŠ›ä¸Šçš„ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹åŸå§‹SFTæ•°æ®çš„æƒ…å†µä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„SFTæ–¹æ³•ï¼Œé¦–å…ˆé‡æ„åŸºç¡€æ¨¡å‹çš„æŒ‡ä»¤åˆ†å¸ƒï¼Œç„¶åé€šè¿‡å¤šæ¨¡å‹ç”Ÿæˆå’Œè¿‡æ»¤ç®¡é“åˆæˆé«˜è´¨é‡çš„é€šç”¨æ•°æ®é›†ï¼Œæœ€åä¸é¢†åŸŸç‰¹å®šæ•°æ®æ··åˆè¿›è¡Œå¾®è°ƒï¼Œä»¥æ­¤æ¥ç¼“è§£ç¾éš¾æ€§é—å¿˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆé‡æ„åŸºç¡€æ¨¡å‹çš„æŒ‡ä»¤åˆ†å¸ƒï¼›å…¶æ¬¡ç”Ÿæˆé«˜è´¨é‡çš„é€šç”¨æ•°æ®é›†ï¼›æœ€åå°†åˆæˆæ•°æ®ä¸æ–°é¢†åŸŸæ•°æ®æ··åˆè¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ— éœ€è®¿é—®åŸå§‹SFTæ•°æ®ï¼Œé€šè¿‡é‡æ„æŒ‡ä»¤åˆ†å¸ƒå’Œåˆæˆæ•°æ®çš„æ–¹å¼ï¼Œæœ‰æ•ˆè§£å†³äº†ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–åŸå§‹æ•°æ®çš„å¾®è°ƒæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬åˆæˆæ•°æ®çš„ç”Ÿæˆç­–ç•¥å’Œè¿‡æ»¤æ ‡å‡†ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆä¿ç•™åŸæœ‰èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹é€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼Œç‰¹å®šä»»åŠ¡æ€§èƒ½æå‡æ˜¾è‘—ã€‚ä¸ä½¿ç”¨å…¬å¼€SFTæ•°æ®é›†çš„åŸºçº¿ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°æé«˜äº†X%ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å›¾è°±æ„å»ºç­‰ã€‚é€šè¿‡æœ‰æ•ˆç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´ä¸ºç¨³å¥å’Œé«˜æ•ˆçš„è¯­è¨€æ¨¡å‹ï¼Œæå‡å…¶åœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Supervised Fine-Tuning (SFT) is a critical step for enhancing the instruction-following capabilities of Large Language Models (LLMs) and adapting them to specialized domains. However, SFT often leads to a degradation of the model's general abilities, a phenomenon known as catastrophic forgetting. This problem is exacerbated when third-party practitioners fine-tune open-source models, as the original SFT data is typically not available. To address this challenge, we propose a novel and cost-effective SFT method that effectively mitigates catastrophic forgetting without requiring access to the original SFT data. Our approach first reconstructs the likely instruction distribution of the base model. It then employs a multi-model generation and filtering pipeline to synthesize a high-quality general-purpose dataset. This synthetic dataset is mixed with new, domain-specific data for fine-tuning. Experimental results show that our method not only preserves the model's capabilities in general domains but also improves task-specific performance, outperforming baselines that use publicly available SFT datasets.

