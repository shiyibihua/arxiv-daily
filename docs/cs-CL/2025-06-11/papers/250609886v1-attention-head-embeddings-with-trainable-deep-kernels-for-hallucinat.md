---
layout: default
title: Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs
---

# Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09886" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09886v1</a>
  <a href="https://arxiv.org/pdf/2506.09886.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09886v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09886v1', 'Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rodion Oblovatny, Alexandra Bazarova, Alexey Zaytsev

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¯è®­ç»ƒæ·±åº¦æ ¸çš„æ³¨æ„åŠ›å¤´åµŒå…¥ä»¥æ£€æµ‹LLMä¸­çš„å¹»è§‰**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¹»è§‰æ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ ` `æ¦‚ç‡åˆ†å¸ƒ` `æ¨¡å‹å†…åœ¨æ–¹æ³•` `åˆ†å¸ƒè·ç¦»` `å¯å­¦ä¹ æ ¸` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰æ—¶ï¼Œå¾€å¾€ä¾èµ–å¤–éƒ¨çŸ¥è¯†æˆ–è¾…åŠ©æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§å’Œçµæ´»æ€§ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒè·ç¦»çš„æ¨¡å‹å†…åœ¨æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨éšè—çŠ¶æ€åˆ†å¸ƒçš„æ¦‚ç‡å‘æ•£æ¥è¯†åˆ«å¹»è§‰å“åº”ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ£€æµ‹åŸºçº¿ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææç¤ºä¸å“åº”çš„éšè—çŠ¶æ€åˆ†å¸ƒä¹‹é—´çš„æ¦‚ç‡å‘æ•£æ¥æ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¹»è§‰ã€‚å‡ºäººæ„æ–™çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å¹»è§‰å“åº”ä¸å…¶æç¤ºä¹‹é—´çš„åå·®å°äºåŸºäºäº‹å®çš„å“åº”ï¼Œè¿™è¡¨æ˜å¹»è§‰é€šå¸¸æºäºè¡¨é¢çš„é‡è¿°è€Œéå®è´¨æ€§çš„æ¨ç†ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡å‹å†…åœ¨çš„æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨åˆ†å¸ƒè·ç¦»ä½œä¸ºåŸåˆ™æ€§çš„å¹»è§‰è¯„åˆ†ï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨çŸ¥è¯†æˆ–è¾…åŠ©æ¨¡å‹çš„éœ€æ±‚ã€‚ä¸ºäº†å¢å¼ºæ•æ„Ÿæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ·±åº¦å¯å­¦ä¹ æ ¸ï¼Œèƒ½å¤Ÿè‡ªåŠ¨é€‚åº”ä»¥æ•æ‰åˆ†å¸ƒä¹‹é—´çš„ç»†å¾®å‡ ä½•å·®å¼‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨æ²¡æœ‰æ ¸è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»ç„¶æä¾›äº†ä¸€ä¸ªç¨³å¥ã€å¯æ‰©å±•çš„å¹»è§‰æ£€æµ‹è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å¹»è§‰æ£€æµ‹çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤–éƒ¨çŸ¥è¯†æˆ–è¾…åŠ©æ¨¡å‹ï¼Œå¯¼è‡´çµæ´»æ€§ä¸è¶³å’Œé€‚ç”¨èŒƒå›´æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒè·ç¦»çš„æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨æç¤ºä¸å“åº”çš„éšè—çŠ¶æ€åˆ†å¸ƒä¹‹é—´çš„æ¦‚ç‡å‘æ•£æ¥è¯†åˆ«å¹»è§‰ã€‚ç ”ç©¶å‘ç°ï¼Œå¹»è§‰å“åº”ä¸æç¤ºä¹‹é—´çš„åå·®è¾ƒå°ï¼Œè¡¨æ˜å…¶æºäºè¡¨é¢é‡è¿°è€Œéæ·±å±‚æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€éšè—çŠ¶æ€æå–ã€åˆ†å¸ƒè·ç¦»è®¡ç®—å’Œå¹»è§‰è¯„åˆ†ç”Ÿæˆå››ä¸ªä¸»è¦æ¨¡å—ã€‚é€šè¿‡æ·±åº¦å¯å­¦ä¹ æ ¸ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨é€‚åº”å¹¶æ•æ‰åˆ†å¸ƒä¹‹é—´çš„ç»†å¾®å‡ ä½•å·®å¼‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ¨¡å‹å†…åœ¨çš„å¹»è§‰æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨åˆ†å¸ƒè·ç¦»ä½œä¸ºå¹»è§‰è¯„åˆ†ï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨çŸ¥è¯†çš„ä¾èµ–ã€‚è¿™ä¸€æ–¹æ³•åœ¨æ£€æµ‹å‡†ç¡®æ€§å’Œçµæ´»æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ·±åº¦å­¦ä¹ æ¡†æ¶æ¥å®ç°å¯å­¦ä¹ æ ¸ï¼Œè®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆ†å¸ƒè·ç¦»çš„è®¡ç®—ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰åˆ°ä¸åŒå“åº”ä¹‹é—´çš„å¾®å°å·®å¼‚ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæ¨¡å‹åœ¨æ²¡æœ‰æ ¸è®­ç»ƒçš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒç«äº‰åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚å³ä½¿åœ¨æ²¡æœ‰è¿›è¡Œæ ¸è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ–¹æ³•ä¾ç„¶ä¿æŒäº†è¾ƒé«˜çš„æ£€æµ‹å‡†ç¡®æ€§ï¼Œå±•ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§è¯„ä¼°ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿçš„è´¨é‡æ§åˆ¶ä»¥åŠè‡ªåŠ¨å†…å®¹ç”Ÿæˆçš„å¯é æ€§æ£€æµ‹ã€‚é€šè¿‡æœ‰æ•ˆè¯†åˆ«å¹»è§‰ï¼Œèƒ½å¤Ÿæé«˜è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯ä¿¡åº¦å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a novel approach for detecting hallucinations in large language models (LLMs) by analyzing the probabilistic divergence between prompt and response hidden-state distributions. Counterintuitively, we find that hallucinated responses exhibit smaller deviations from their prompts compared to grounded responses, suggesting that hallucinations often arise from superficial rephrasing rather than substantive reasoning. Leveraging this insight, we propose a model-intrinsic detection method that uses distributional distances as principled hallucination scores, eliminating the need for external knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable kernels that automatically adapt to capture nuanced geometric differences between distributions. Our approach outperforms existing baselines, demonstrating state-of-the-art performance on several benchmarks. The method remains competitive even without kernel training, offering a robust, scalable solution for hallucination detection.

