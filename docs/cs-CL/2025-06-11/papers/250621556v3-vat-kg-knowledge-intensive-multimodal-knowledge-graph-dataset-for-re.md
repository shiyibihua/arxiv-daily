---
layout: default
title: VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation
---

# VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21556" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21556v3</a>
  <a href="https://arxiv.org/pdf/2506.21556.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21556v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21556v3', 'VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hyeongcheol Park, Jiyoung Seo, MinHyuk Jang, Hogun Park, Ha Dam Baek, Gyusam Chang, Hyeonsoo Im, Sangpil Kim

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-09-26)

**å¤‡æ³¨**: Project Page: https://vatkg.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVAT-KGä»¥è§£å†³å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±çš„çŸ¥è¯†è¦†ç›–ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±` `çŸ¥è¯†å¢å¼ºç”Ÿæˆ` `è·¨æ¨¡æ€å¯¹é½` `è§†è§‰-éŸ³é¢‘-æ–‡æœ¬` `æ™ºèƒ½é—®ç­”` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±é€šå¸¸çŸ¥è¯†è¦†ç›–é¢æœ‰é™ï¼Œä¸”å¤šæ¨¡æ€æ”¯æŒä¸è¶³ï¼Œæ— æ³•æ»¡è¶³æœ€æ–°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºçš„VAT-KGæ˜¯é¦–ä¸ªæ¶µç›–è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯çš„çŸ¥è¯†å¯†é›†å‹å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼Œæ”¯æŒè·¨æ¨¡æ€çŸ¥è¯†å¯¹é½å’Œè‡ªåŠ¨ç”Ÿæˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVAT-KGåœ¨å¤šæ¨¡æ€é—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€çŸ¥è¯†ç»Ÿä¸€ä¸åˆ©ç”¨æ–¹é¢çš„å®é™…ä»·å€¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆMMKGsï¼‰åœ¨è¡¥å……å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„éšæ€§çŸ¥è¯†æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MMKGsé€šå¸¸é€šè¿‡å¢å¼ºå·²æœ‰çŸ¥è¯†å›¾è°±æ„å»ºï¼Œå¯¼è‡´çŸ¥è¯†è¦†ç›–é¢æœ‰é™ä¸”å¤šæ¨¡æ€æ”¯æŒä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è§†è§‰-éŸ³é¢‘-æ–‡æœ¬çŸ¥è¯†å›¾è°±ï¼ˆVAT-KGï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªæ¦‚å¿µä¸­å¿ƒçš„çŸ¥è¯†å¯†é›†å‹å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼Œæ¶µç›–è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ã€‚é€šè¿‡ä¸¥æ ¼çš„è¿‡æ»¤å’Œå¯¹é½æ­¥éª¤ï¼Œç¡®ä¿è·¨æ¨¡æ€çŸ¥è¯†çš„å¯¹é½ï¼Œå¹¶æ”¯æŒä»ä»»æ„å¤šæ¨¡æ€æ•°æ®é›†ä¸­è‡ªåŠ¨ç”ŸæˆMMKGsã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»æ„æ¨¡æ€çš„æŸ¥è¯¢æ£€ç´¢è¯¦ç»†çš„æ¦‚å¿µçº§çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVAT-KGåœ¨å¤šæ¨¡æ€é—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†MLLMsçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±åœ¨çŸ¥è¯†è¦†ç›–å’Œå¤šæ¨¡æ€æ”¯æŒæ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è§†é¢‘å’ŒéŸ³é¢‘ç­‰æ–°å…´æ¨¡æ€æ—¶çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºVAT-KGï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªæ¦‚å¿µä¸­å¿ƒçš„çŸ¥è¯†å›¾è°±ï¼Œæ•´åˆè§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œç¡®ä¿è·¨æ¨¡æ€çŸ¥è¯†çš„å¯¹é½å’Œä¸°å¯Œçš„æ¦‚å¿µæè¿°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ä¸¥æ ¼çš„è¿‡æ»¤ä¸å¯¹é½æ­¥éª¤ï¼Œä»¥åŠè‡ªåŠ¨ç”ŸæˆMMKGsçš„æµç¨‹ï¼Œç¡®ä¿çŸ¥è¯†çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šVAT-KGçš„åˆ›æ–°åœ¨äºå…¶çŸ¥è¯†å¯†é›†å‹å’Œå¤šæ¨¡æ€çš„è®¾è®¡ï¼Œé¦–æ¬¡å®ç°äº†å¯¹è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯çš„å…¨é¢è¦†ç›–ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†çŸ¥è¯†çš„ä¸°å¯Œæ€§å’Œé€‚ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ„å»ºè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šå±‚æ¬¡çš„è¿‡æ»¤æœºåˆ¶å’Œå¯¹é½ç®—æ³•ï¼Œç¡®ä¿ä¸åŒæ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡äº†é€‚åº”å¤šæ¨¡æ€æ•°æ®çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVAT-KGåœ¨å¤šæ¨¡æ€é—®ç­”ä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†çº¦20%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶åœ¨æ”¯æŒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VAT-KGçš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è·¨æ¨¡æ€æ£€ç´¢ã€æ•™è‚²å’ŒåŸ¹è®­ç­‰é¢†åŸŸã€‚å…¶ä¸°å¯Œçš„å¤šæ¨¡æ€çŸ¥è¯†è¦†ç›–èƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge across multiple modalities, play a pivotal role by complementing the implicit knowledge of Multimodal Large Language Models (MLLMs) and enabling more grounded reasoning via Retrieval Augmented Generation (RAG). However, existing MMKGs are generally limited in scope: they are often constructed by augmenting pre-existing knowledge graphs, which restricts their knowledge, resulting in outdated or incomplete knowledge coverage, and they often support only a narrow range of modalities, such as text and visual information. These limitations restrict applicability to multimodal tasks, particularly as recent MLLMs adopt richer modalities like video and audio. Therefore, we propose the Visual-Audio-Text Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive multimodal knowledge graph that covers visual, audio, and text information, where each triplet is linked to multimodal data and enriched with detailed descriptions of concepts. Specifically, our construction pipeline ensures cross-modal knowledge alignment between multimodal data and fine-grained semantics through a series of stringent filtering and alignment steps, enabling the automatic generation of MMKGs from any multimodal dataset. We further introduce a novel multimodal RAG framework that retrieves detailed concept-level knowledge in response to queries from arbitrary modalities. Experiments on question answering tasks across various modalities demonstrate the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical value in unifying and leveraging multimodal knowledge.

