---
layout: default
title: Reinforcement learning fine-tuning of language model for instruction following and math reasoning
---

# Reinforcement learning fine-tuning of language model for instruction following and math reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21560" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21560v2</a>
  <a href="https://arxiv.org/pdf/2506.21560.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21560v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21560v2', 'Reinforcement learning fine-tuning of language model for instruction following and math reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifu Han, Geo Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-07-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒè¯­è¨€æ¨¡å‹ä»¥æå‡æŒ‡ä»¤è·Ÿéšå’Œæ•°å­¦æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è¯­è¨€æ¨¡å‹` `æŒ‡ä»¤è·Ÿéš` `æ•°å­¦æ¨ç†` `å¾®è°ƒæŠ€æœ¯` `æ•°æ®å¢å¼º` `å¥–åŠ±æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æŒ‡ä»¤è·Ÿéšå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹çš„å¯¹é½æ€§å’Œå‡†ç¡®æ€§æ–¹é¢ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒæŠ€æœ¯ï¼Œç»“åˆä¸åŒçš„ä¼˜åŒ–ç­–ç•¥æ¥æå‡å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLOOæ–¹æ³•åœ¨å¯¹é½æ€§ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒDPOåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæä¾›äº†ç¨³å®šçš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒæŠ€æœ¯åœ¨ç´§å‡‘å‹è¯­è¨€æ¨¡å‹ï¼ˆQwen2.5-0.5B Baseï¼‰ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œé’ˆå¯¹æŒ‡ä»¤è·Ÿéšå’Œæ•°å­¦æ¨ç†è¿™ä¸¤é¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ä½¿ç”¨åå¥½æ ‡æ³¨æ•°æ®çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œå¸¦æœ‰å¥–åŠ±æ¨¡å‹çš„å¼ºåŒ–ç•™ä¸€æ³•ï¼ˆRLOOï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DeBERTaå¥–åŠ±å»ºæ¨¡çš„RLOOåœ¨å¯¹é½æ€§ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒDPOåˆ™æä¾›äº†å¼ºå¤§ä¸”ä¸€è‡´çš„ç»“æœã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œåˆæˆæ•°æ®å¢å¼ºå’Œå¤–éƒ¨éªŒè¯å™¨çš„æœ€ä½³Né‡‡æ ·æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å¾®è°ƒä¸æ¨ç†æ—¶å·¥å…·ç»“åˆçš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†è®­ç»ƒè½»é‡çº§ã€ä»»åŠ¡å¯¹é½çš„å°è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å…³é”®æƒè¡¡å’Œå®ç”¨ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéšå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¯¹é½æ€§å’Œå‡†ç¡®æ€§ä¸Šå­˜åœ¨æ˜æ˜¾çŸ­æ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒæŠ€æœ¯ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œåå¥½ä¼˜åŒ–ç­–ç•¥ï¼Œæå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯é€šè¿‡å¥–åŠ±æ¨¡å‹æ¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®å‡†å¤‡é˜¶æ®µæ¶‰åŠåå¥½æ ‡æ³¨æ•°æ®çš„æ”¶é›†ï¼Œæ¨¡å‹è®­ç»ƒé˜¶æ®µåˆ™åº”ç”¨ä¸åŒçš„å¾®è°ƒç­–ç•¥ï¼Œæœ€åé€šè¿‡è¯„ä¼°é˜¶æ®µéªŒè¯æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†RLOOæ–¹æ³•ä¸DeBERTaå¥–åŠ±æ¨¡å‹çš„ç»“åˆï¼Œè¿™ç§æ–¹æ³•åœ¨å¯¹é½æ€§ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åˆæˆæ•°æ®å¢å¼ºå’Œæœ€ä½³Né‡‡æ ·ç­–ç•¥ï¼Œç»“åˆå¤–éƒ¨éªŒè¯å™¨ä»¥æé«˜æ•°å­¦æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡ç‚¹è€ƒè™‘äº†å¥–åŠ±ä¿¡å·çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨DeBERTaå¥–åŠ±æ¨¡å‹çš„RLOOæ–¹æ³•åœ¨å¯¹é½æ€§ä¸Šè¾¾åˆ°äº†æœ€ä½³æ•ˆæœï¼Œè€ŒDPOæ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§ä¸”ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚æ•°å­¦æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§é€šè¿‡åˆæˆæ•°æ®å¢å¼ºå’Œæœ€ä½³Né‡‡æ ·ç­–ç•¥æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†å¾®è°ƒä¸æ¨ç†å·¥å…·ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéšå’Œæ•°å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´ä¸ºç²¾å‡†å’Œæ™ºèƒ½çš„äº¤äº’ä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨å¤šç§å®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This study investigates the effectiveness of reinforcement learning (RL) fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two challenging tasks: instruction following and mathematical reasoning. We compare supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models. Our experiments show that RLOO with DeBERTa reward modeling achieves the best alignment, while DPO provides strong and consistent results. For math reasoing tasks, synthetic data augmentation and best-of-N sampling with an external verifier significantly improve accuracy, showing the potential of combining fine-tuning with inference-time tools. This study highlights key trade-offs and practical strategies for training lightweight, task-aligned small-scale language models.

