---
layout: default
title: Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms
---

# Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09457" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09457v2</a>
  <a href="https://arxiv.org/pdf/2506.09457.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09457v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09457v2', 'Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeguan Xiao, Yun Chen, Guanhua Chen, Ke Tang

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-08-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPOETä»¥è§£å†³ç›´æ¥å¯¹é½ç®—æ³•ä¸­çš„å¥–åŠ±ç”Ÿæˆå·®è·é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›´æ¥å¯¹é½ç®—æ³•` `å¥–åŠ±ç”Ÿæˆ` `å‰ç¼€å¯¼å‘è®­ç»ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›´æ¥å¯¹é½ç®—æ³•åœ¨è®­ç»ƒä¼˜åŒ–ç›®æ ‡ä¸æ¨ç†ç”Ÿæˆæ€§èƒ½ä¹‹é—´å­˜åœ¨å¥–åŠ±ç”Ÿæˆå·®è·ï¼Œå½±å“äº†æ¨¡å‹çš„å®é™…è¡¨ç°ã€‚
2. è®ºæ–‡æå‡ºäº†å‰ç¼€å¯¼å‘ç­‰é•¿è®­ç»ƒï¼ˆPOETï¼‰æ–¹æ³•ï¼Œé€šè¿‡æˆªæ–­å“åº”é•¿åº¦æ¥è§£å†³å¥–åŠ±ç”Ÿæˆå·®è·é—®é¢˜ï¼Œå¢å¼ºäº†å¯¹å‰ç¼€æ ‡è®°çš„å…³æ³¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPOETåœ¨DPOå’ŒSimPOçš„æ ‡å‡†å®ç°ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨AlpacaEval 2ä¸­æé«˜äº†15.6åˆ†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›´æ¥å¯¹é½ç®—æ³•ï¼ˆDAAsï¼‰ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç®€å•åå¥½ä¼˜åŒ–ï¼ˆSimPOï¼‰ï¼Œä½œä¸ºå¯¹äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ç®—æ³•çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œé¢ä¸´å¥–åŠ±ç”Ÿæˆå·®è·çš„æ ¹æœ¬é™åˆ¶ã€‚æœ¬æ–‡è¯†åˆ«äº†è¿™ä¸€å·®è·çš„åŸå› ï¼Œä¸»è¦æ˜¯å‰ç¼€æ ‡è®°åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é‡è¦æ€§ä¸DAAséšå«å¥–åŠ±å‡½æ•°ä¹‹é—´çš„é”™ä½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå‰ç¼€å¯¼å‘ç­‰é•¿è®­ç»ƒï¼ˆPOETï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å°†åå¥½å’Œä¸åå¥½çš„å“åº”æˆªæ–­ä¸ºç›¸åŒé•¿åº¦ï¼Œä¼˜åŒ–DAAsçš„ç›®æ ‡ï¼Œä½¿å…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´å…³æ³¨å‰ç¼€æ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPOETåœ¨AlpacaEval 2ä¸­æå‡äº†å¤šè¾¾15.6åˆ†ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­æ•´ä½“æ”¹å–„äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„å…·ä½“é—®é¢˜æ˜¯ç›´æ¥å¯¹é½ç®—æ³•ï¼ˆDAAsï¼‰åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µä¹‹é—´çš„å¥–åŠ±ç”Ÿæˆå·®è·ï¼Œå¯¼è‡´æ¨¡å‹ç”Ÿæˆçš„å“åº”ä¸äººç±»åå¥½ä¸ä¸€è‡´ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè€ƒè™‘å‰ç¼€æ ‡è®°çš„é‡è¦æ€§ï¼Œé€ æˆä¼˜åŒ–ç›®æ ‡ä¸ç”Ÿæˆæ€§èƒ½çš„é”™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é‡‡ç”¨å‰ç¼€å¯¼å‘ç­‰é•¿è®­ç»ƒï¼ˆPOETï¼‰ï¼Œé€šè¿‡å°†åå¥½å’Œä¸åå¥½çš„å“åº”æˆªæ–­ä¸ºç›¸åŒé•¿åº¦ï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹æ›´å…³æ³¨å‰ç¼€æ ‡è®°ï¼Œä»è€Œç¼©å°å¥–åŠ±ç”Ÿæˆå·®è·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œæ”¶é›†åŒ…å«åå¥½å’Œä¸åå¥½å“åº”çš„è®­ç»ƒæ ·æœ¬ï¼›å…¶æ¬¡ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†è¿™äº›å“åº”æˆªæ–­ä¸ºç›¸åŒé•¿åº¦ï¼Œä»¥ä¾¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¿æŒä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†å‰ç¼€å¯¼å‘çš„è®­ç»ƒç­–ç•¥ï¼Œä½¿å¾—DAAsåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¯¹å‰ç¼€æ ‡è®°è¿›è¡Œå»ºæ¨¡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒPOETæ–¹æ³•é€šè¿‡åŠ¨æ€æˆªæ–­å“åº”é•¿åº¦æ¥å®ç°å¤šæ ·åŒ–çš„è®­ç»ƒæ ·æœ¬é•¿åº¦ï¼Œç¡®ä¿ä¼˜åŒ–ç›®æ ‡åœ¨æ‰€æœ‰æ—¶é—´æ­¥ä¸Šæ”¶æ•›ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼ºè°ƒå‰ç¼€æ ‡è®°çš„é‡è¦æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPOETåœ¨AlpacaEval 2ä¸­æå‡äº†å¤šè¾¾15.6åˆ†ï¼Œç›¸è¾ƒäºDPOå’ŒSimPOçš„æ ‡å‡†å®ç°ï¼Œæ•´ä½“æ€§èƒ½åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å‡æœ‰æ˜¾è‘—æ”¹å–„ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æ”¹å–„æ¨¡å‹å¯¹äººç±»åå¥½çš„å¯¹é½èƒ½åŠ›ï¼ŒPOETæ–¹æ³•èƒ½å¤Ÿæå‡ç”Ÿæˆæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO), have emerged as efficient alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms for aligning large language models (LLMs) with human preferences. However, DAAs suffer from a fundamental limitation we identify as the "reward-generation gap" -- a misalignment between optimization objectives during training and actual generation performance during inference. In this paper, we find a contributor to the reward-generation gap is the mismatch between the inherent importance of prefix tokens during the LLM generation process and how this importance is reflected in the implicit reward functions of DAAs. To bridge the gap, we adopt a token-level MDP perspective of DAAs to analyze its limitations and introduce a simple yet effective approach called Prefix-Oriented Equal-length Training (POET), which truncates both preferred and dispreferred responses to match the shorter one's length. Training with \mname, where both responses in each sample are truncated to equal length, resulting in diverse truncated lengths across samples, the optimization of DAAs objective is implicitly constrained to converge across all timesteps of token-level MDP, thus paying more attention to prefix tokens than the standard DAAs. We conduct experiments with DPO and SimPO, two representative DAAs, demonstrating that POET improves over their standard implementations, achieving up to 15.6 points in AlpacaEval 2 and overall improvements across downstream tasks. Our results highlight the importance of addressing the misalignment between reward optimization and generation performance in DAAs.

