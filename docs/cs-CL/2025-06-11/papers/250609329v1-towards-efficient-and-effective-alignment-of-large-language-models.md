---
layout: default
title: Towards Efficient and Effective Alignment of Large Language Models
---

# Towards Efficient and Effective Alignment of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09329" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.09329v1</a>
  <a href="https://arxiv.org/pdf/2506.09329.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09329v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09329v1', 'Towards Efficient and Effective Alignment of Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yuxin Jiang

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-11

**Â§áÊ≥®**: PhD thesis

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LionÂíåWebR‰ª•ÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂØπÈΩêÊïàÁéá‰∏éÊïàÊûú**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÂØπÈΩêÊäÄÊúØ` `Êï∞ÊçÆÂêàÊàê` `ÂØπÊäóËí∏È¶è` `Ëá™Âä®ÂåñÊ°ÜÊû∂` `Áü•ËØÜÊï¥Âêà` `ËØÑ‰º∞Âü∫ÂáÜ` `ÂÖÉÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÂØπÈΩêÊï∞ÊçÆÊî∂ÈõÜ‰∏ä‰æùËµñÊâãÂä®Êï∞ÊçÆÈõÜÊàñ‰∏ìÊúâÊ®°ÂûãÔºåÈôêÂà∂‰∫ÜÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫LionÂíåWebRÊ°ÜÊû∂ÔºåÈÄöËøáÂØπÊäóËí∏È¶èÂíåËá™Âä®ÂåñÊï∞ÊçÆÂêàÊàêÊù•‰ºòÂåñÂØπÈΩêÊï∞ÊçÆÊî∂ÈõÜËøáÁ®ã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊñ∞ÁöÑÊñπÊ≥ïÂú®Èõ∂-shotÊé®ÁêÜÂíåÁ∫¶ÊùüÈÅµÂæ™ËÉΩÂäõ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊ®°ÂûãÔºåÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÊîπËøõÊñπÂêë„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öÁßç‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫ÂçìË∂äÁöÑËÉΩÂäõÔºå‰ΩÜÂ¶Ç‰ΩïÈ´òÊïà‰∏îÊúâÊïàÂú∞‰ΩøÂÖ∂‰∏é‰∫∫Á±ªÊúüÊúõÂØπÈΩê‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàò„ÄÇÊú¨ËÆ∫ÊñáÈÄöËøáÂú®Êï∞ÊçÆÊî∂ÈõÜ„ÄÅËÆ≠ÁªÉÂíåËØÑ‰º∞ÊñπÈù¢ÂºïÂÖ•Êñ∞ÊñπÊ≥ïÔºåÊé®Âä®‰∫ÜLLMÁöÑÂØπÈΩêÁ†îÁ©∂„ÄÇÈ¶ñÂÖàÔºåÈíàÂØπÂØπÈΩêÊï∞ÊçÆÊî∂ÈõÜÔºåÊèêÂá∫‰∫ÜLionÔºå‰∏Ä‰∏™ÂØπÊäóËí∏È¶èÊ°ÜÊû∂ÔºåÈÄöËøáËØÜÂà´ÂíåÁîüÊàêÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊåá‰ª§Êù•Ëø≠‰ª£‰ºòÂåñËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÊ≠§Â§ñÔºåWebÈáçÊûÑÔºàWebRÔºâÊòØ‰∏Ä‰∏™ÂÖ®Ëá™Âä®Ê°ÜÊû∂ÔºåÁõ¥Êé•‰ªéÂéüÂßãÁΩëÈ°µÊñáÊ°£ÂêàÊàêÊåá‰ª§Ë∞É‰ºòÊï∞ÊçÆÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇÂú®ËÆ≠ÁªÉÊñπÈù¢ÔºåÂºÄÂèë‰∫ÜÂ≠¶‰π†ÁºñËæëÔºàLTEÔºâÊ°ÜÊû∂ÔºåËÉΩÂ§üÈ´òÊïàÊï¥ÂêàÊñ∞Áü•ËØÜÂπ∂‰øùÊåÅÁé∞Êúâ‰ø°ÊÅØ„ÄÇÊúÄÂêéÔºåÊèêÂá∫FollowBenchÔºå‰∏Ä‰∏™Â§öÂ±ÇÊ¨°„ÄÅÁªÜÁ≤íÂ∫¶ÁöÑÂü∫ÂáÜÔºåËØÑ‰º∞LLMsÂú®ÈÅµÂæ™Â§çÊùÇÁ∫¶ÊùüÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°Âûã‰∏é‰∫∫Á±ªÊúüÊúõÂØπÈΩêÁöÑÊïàÁéáÂíåÊïàÊûúÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÂØπÈΩêÊï∞ÊçÆÊî∂ÈõÜÂíåËÆ≠ÁªÉËøáÁ®ã‰∏≠Â≠òÂú®‰æùËµñÊâãÂä®Êï∞ÊçÆÈõÜ„ÄÅÁº∫‰πèÂ§öÊ†∑ÊÄßÁ≠âÁóõÁÇπ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫ÜLionÂíåWebRÊ°ÜÊû∂ÔºåÈÄöËøáÂØπÊäóËí∏È¶èÂíåËá™Âä®ÂåñÊï∞ÊçÆÂêàÊàêÊù•ÊèêÂçáÂØπÈΩêÊï∞ÊçÆÁöÑË¥®ÈáèÂíåÂ§öÊ†∑ÊÄßÔºåÂêåÊó∂ÂºïÂÖ•Â≠¶‰π†ÁºñËæëÔºàLTEÔºâÊ°ÜÊû∂Êù•‰ºòÂåñÁü•ËØÜÊï¥ÂêàËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) LionÊ°ÜÊû∂Áî®‰∫éÂØπÊäóÊÄßÊï∞ÊçÆÊî∂ÈõÜÔºõ2) WebRÊ°ÜÊû∂Áî®‰∫é‰ªéÁΩëÈ°µÊñáÊ°£Ëá™Âä®ÂêàÊàêÊåá‰ª§Êï∞ÊçÆÔºõ3) LTEÊ°ÜÊû∂Áî®‰∫éÈ´òÊïàÊï¥ÂêàÊñ∞Áü•ËØÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éLionÂíåWebRÁöÑÁªìÂêà‰ΩøÁî®ÔºåÂâçËÄÖÈÄöËøáÁîüÊàêÊåëÊàòÊÄßÊåá‰ª§‰ºòÂåñÊï∞ÊçÆÈõÜÔºåÂêéËÄÖÂàôÂÆûÁé∞‰∫ÜÊï∞ÊçÆÂêàÊàêÁöÑËá™Âä®ÂåñÔºåÊòæËëóÊèêÂçá‰∫ÜÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Lion‰∏≠ÔºåÈááÁî®ÂØπÊäóËÆ≠ÁªÉÁ≠ñÁï•Êù•ËØÜÂà´ÂíåÁîüÊàêÂõ∞ÈöæÊåá‰ª§ÔºõÂú®WebR‰∏≠ÔºåËÆæËÆ°‰∫ÜËá™Âä®ÂåñÊï∞ÊçÆÂêàÊàêÊµÅÁ®ãÔºõLTEÊ°ÜÊû∂‰∏≠ËøêÁî®‰∫ÜÂÖÉÂ≠¶‰π†ÊäÄÊúØ‰ª•ÊîØÊåÅÂÆûÊó∂ÂíåÊâπÈáèÁü•ËØÜÊõ¥Êñ∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰ΩøÁî®LionÂíåWebRÊ°ÜÊû∂ÂêéÔºåÊ®°ÂûãÂú®Èõ∂-shotÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÊòæËëóÊèêÂçáÔºåÁâπÂà´ÊòØÂú®ÈÅµÂæ™Â§çÊùÇÁ∫¶ÊùüÊñπÈù¢ÔºåFollowBenchÂü∫ÂáÜÊµãËØïÊè≠Á§∫‰∫ÜÂΩìÂâçÊ®°ÂûãÂú®Á∫¶ÊùüÈÅµÂæ™ËÉΩÂäõ‰∏äÁöÑÂÖ≥ÈîÆÂº±ÁÇπÔºå‰∏∫Êú™Êù•ÁöÑÊîπËøõÊèê‰æõ‰∫ÜÈáçË¶ÅËßÅËß£„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂä©Êâã„ÄÅÊïôËÇ≤ÊäÄÊúØÂíåËá™Âä®ÂåñÂÜÖÂÆπÁîüÊàêÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂØπÈΩêËÉΩÂäõÔºåÂèØ‰ª•Êõ¥Â•ΩÂú∞Êª°Ë∂≥Áî®Êà∑ÈúÄÊ±ÇÔºåÊèêÂçá‰∫∫Êú∫‰∫§‰∫íÁöÑËá™ÁÑ∂ÊÄßÂíåÊúâÊïàÊÄßÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) exhibit remarkable capabilities across diverse tasks, yet aligning them efficiently and effectively with human expectations remains a critical challenge. This thesis advances LLM alignment by introducing novel methodologies in data collection, training, and evaluation. We first address alignment data collection. Existing approaches rely heavily on manually curated datasets or proprietary models. To overcome these limitations, we propose Lion, an adversarial distillation framework that iteratively refines training data by identifying and generating challenging instructions, enabling state-of-the-art zero-shot reasoning. Additionally, we introduce Web Reconstruction (WebR), a fully automated framework that synthesizes instruction-tuning data directly from raw web documents, significantly improving data diversity and scalability over existing synthetic data methods. Next, we enhance alignment training through novel optimization techniques. We develop Learning to Edit (LTE), a framework that enables LLMs to efficiently integrate new knowledge while preserving existing information. LTE leverages meta-learning to improve both real-time and batch knowledge updates. Furthermore, we introduce Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) that explicitly captures token-level correlations in preference data, leading to superior alignment across QA and mathematical reasoning tasks. Finally, we tackle the challenge of evaluating alignment. Existing benchmarks emphasize response quality but overlook adherence to specific constraints. To bridge this gap, we introduce FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to follow complex constraints across diverse instruction types. Our results expose key weaknesses in current models' constraint adherence, offering insights for future improvements.

