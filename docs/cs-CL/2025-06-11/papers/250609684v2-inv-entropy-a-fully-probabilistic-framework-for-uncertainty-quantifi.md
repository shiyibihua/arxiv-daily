---
layout: default
title: Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models
---

# Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09684" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09684v2</a>
  <a href="https://arxiv.org/pdf/2506.09684.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09684v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09684v2', 'Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoyi Song, Ruihan Ji, Naichen Shi, Fan Lai, Raed Al Kontar

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-11-05)

**æœŸåˆŠ**: NeurIPS, 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInv-Entropyæ¡†æ¶ä»¥é‡åŒ–è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ç¡®å®šæ€§é‡åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¦‚ç‡æ¡†æ¶` `é€†æ¨¡å‹` `é—ä¼ ç®—æ³•` `è¯­ä¹‰ç›¸ä¼¼æ€§` `æ‰°åŠ¨ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•å¤šä¸ºå¯å‘å¼ï¼Œç¼ºä¹ç³»ç»Ÿçš„æ¦‚ç‡è§£é‡Šï¼Œé™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€†æ¨¡å‹çš„å®Œå…¨æ¦‚ç‡æ¡†æ¶ï¼Œé€šè¿‡æ‰°åŠ¨è¾“å…¥ç©ºé—´æ¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œå®šä¹‰äº†æ–°çš„ä¸ç¡®å®šæ€§åº¦é‡Inv-Entropyã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInv-Entropyåœ¨ä¸ç¡®å®šæ€§é‡åŒ–æ–¹é¢ä¼˜äºç°æœ‰çš„è¯­ä¹‰UQæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¼•å‘å˜é©ï¼Œä½†å…¶å¯é éƒ¨ç½²éœ€è¦æœ‰æ•ˆçš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰ã€‚ç°æœ‰UQæ–¹æ³•å¾€å¾€æ˜¯å¯å‘å¼çš„ï¼Œç¼ºä¹æ¦‚ç‡è§£é‡Šã€‚æœ¬æ–‡é¦–å…ˆæä¾›äº†æ‰°åŠ¨åœ¨LLMsä¸ç¡®å®šæ€§é‡åŒ–ä¸­çš„ç†è®ºä¾æ®ï¼Œéšåå¼•å…¥åŒéšæœºæ¸¸èµ°è§†è§’ï¼Œå°†è¾“å…¥-è¾“å‡ºå¯¹å»ºæ¨¡ä¸ºä¸¤ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œå¹¶é€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§å®šä¹‰è½¬ç§»æ¦‚ç‡ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€†æ¨¡å‹çš„å®Œå…¨æ¦‚ç‡æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿæ€§æ‰°åŠ¨è¯„ä¼°ç»™å®šè¾“å‡ºæ¡ä»¶ä¸‹è¾“å…¥ç©ºé—´çš„å¤šæ ·æ€§æ¥é‡åŒ–ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ç§æ–°çš„ä¸ç¡®å®šæ€§åº¦é‡Inv-Entropyï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºé—ä¼ ç®—æ³•çš„æ‰°åŠ¨ç®—æ³•GAAPï¼Œä»¥å¢å¼ºé‡‡æ ·è¾“å…¥çš„å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒInv-Entropyåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„è¯­ä¹‰UQæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹ç³»ç»Ÿæ€§å’Œæ¦‚ç‡è§£é‡Šï¼Œå¯¼è‡´ä¸å¯é çš„ç»“æœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºé€†æ¨¡å‹çš„å®Œå…¨æ¦‚ç‡æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿæ€§æ‰°åŠ¨è¾“å…¥ç©ºé—´æ¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œåˆ©ç”¨åŒéšæœºæ¸¸èµ°è§†è§’å»ºæ¨¡è¾“å…¥-è¾“å‡ºå¯¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§çš„é©¬å°”å¯å¤«é“¾å»ºæ¨¡è¾“å…¥-è¾“å‡ºå¯¹ï¼Œå…¶æ¬¡æ˜¯é€šè¿‡å®šä¹‰çš„Inv-Entropyåº¦é‡ä¸ç¡®å®šæ€§ï¼Œå¹¶ç»“åˆé—ä¼ ç®—æ³•GAAPå¢å¼ºè¾“å…¥å¤šæ ·æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†Inv-Entropyä½œä¸ºæ–°çš„ä¸ç¡®å®šæ€§åº¦é‡ï¼Œå¹¶é€šè¿‡åŒéšæœºæ¸¸èµ°è§†è§’æä¾›äº†æ–°çš„å»ºæ¨¡æ€è·¯ï¼Œæ˜¾è‘—åŒºåˆ«äºç°æœ‰çš„å¯å‘å¼æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§ä¸ç¡®å®šæ€§åº¦é‡ã€åµŒå…¥æ–¹å¼å’Œæ‰°åŠ¨ç­–ç•¥ï¼ŒGAAPç®—æ³•é€šè¿‡é—ä¼ ç®—æ³•ä¼˜åŒ–è¾“å…¥æ‰°åŠ¨ï¼Œæå‡äº†é‡‡æ ·çš„å¤šæ ·æ€§ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„å…·ä½“ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒInv-Entropyåœ¨ä¸ç¡®å®šæ€§é‡åŒ–æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰çš„è¯­ä¹‰UQæ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æœ‰æ•ˆçš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„æ™ºèƒ½ç³»ç»Ÿéƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have transformed natural language processing, but their reliable deployment requires effective uncertainty quantification (UQ). Existing UQ methods are often heuristic and lack a probabilistic interpretation. This paper begins by providing a theoretical justification for the role of perturbations in UQ for LLMs. We then introduce a dual random walk perspective, modeling input-output pairs as two Markov chains with transition probabilities defined by semantic similarity. Building on this, we propose a fully probabilistic framework based on an inverse model, which quantifies uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. Within this framework, we define a new uncertainty measure, Inv-Entropy. A key strength of our framework is its flexibility: it supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. We also propose GAAP, a perturbation algorithm based on genetic algorithms, which enhances the diversity of sampled inputs. In addition, we introduce a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), which directly assesses uncertainty without relying on correctness as a proxy. Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

