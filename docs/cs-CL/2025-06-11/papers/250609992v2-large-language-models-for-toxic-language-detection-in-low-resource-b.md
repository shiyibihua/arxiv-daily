---
layout: default
title: Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages
---

# Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09992" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09992v2</a>
  <a href="https://arxiv.org/pdf/2506.09992.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09992v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09992v2', 'Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amel Muminovic, Amela Kadric Muminovic

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-06-13)

**å¤‡æ³¨**: 8 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ£€æµ‹ä½èµ„æºå·´å°”å¹²è¯­è¨€ä¸­çš„æœ‰æ¯’è¯­è¨€**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœ‰æ¯’è¯­è¨€æ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä½èµ„æºè¯­è¨€` `ä¸Šä¸‹æ–‡å¢å¼º` `ç¤¾äº¤åª’ä½“è¯„è®º` `æ€§èƒ½è¯„ä¼°` `å·´å°”å¹²è¯­è¨€` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­ç¼ºä¹æœ‰æ•ˆçš„æœ‰æ¯’è¯­è¨€æ£€æµ‹å·¥å…·ï¼Œå¯¼è‡´åœ¨çº¿æœ‰æ¯’è¯­è¨€çš„ç›‘ç®¡ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æå‡å¯¹æœ‰æ¯’è¯„è®ºçš„æ£€æµ‹èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGeminiæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å¼ä¸‹è¾¾åˆ°0.82çš„F1åˆ†æ•°ï¼Œä¸”é›¶æ ·æœ¬çš„GPT-4.1åœ¨ç²¾ç¡®åº¦ä¸Šè¡¨ç°æœ€ä½³ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨çº¿æœ‰æ¯’è¯­è¨€å¯¹ç¤¾ä¼šé€ æˆäº†å®é™…ä¼¤å®³ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹æœ‰æ•ˆç›‘ç®¡å·¥å…·çš„åœ°åŒºã€‚æœ¬æ–‡è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¡å°”ç»´äºšè¯­ã€å…‹ç½—åœ°äºšè¯­å’Œæ³¢æ–¯å°¼äºšè¯­ä¸­å¤„ç†æœ‰æ¯’è¯„è®ºçš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºå¹¶æ‰‹åŠ¨æ ‡æ³¨äº†ä¸€ä¸ªåŒ…å«4500æ¡æ¥è‡ªYouTubeå’ŒTikTokè¯„è®ºçš„æ•°æ®é›†ï¼Œæ¶µç›–éŸ³ä¹ã€æ”¿æ²»ã€ä½“è‚²ç­‰å¤šç§ç±»åˆ«ã€‚æµ‹è¯•äº†å››ç§æ¨¡å‹ï¼ˆGPT-3.5 Turboã€GPT-4.1ã€Gemini 1.5 Proå’ŒClaude 3 Opusï¼‰ï¼Œå¹¶åœ¨é›¶æ ·æœ¬å’Œä¸Šä¸‹æ–‡å¢å¼ºä¸¤ç§æ¨¡å¼ä¸‹è¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ çŸ­ä¸Šä¸‹æ–‡ç‰‡æ®µå¹³å‡æé«˜äº†0.12çš„å¬å›ç‡ï¼Œå¹¶ä½¿F1åˆ†æ•°æé«˜äº†æœ€é«˜0.10ï¼Œå°½ç®¡æœ‰æ—¶ä¼šå¢åŠ è¯¯æŠ¥ã€‚Geminiåœ¨ä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å¼ä¸‹è¡¨ç°æœ€ä½³ï¼ŒF1åˆ†æ•°å’Œå‡†ç¡®ç‡å‡è¾¾åˆ°0.82ï¼Œè€Œé›¶æ ·æœ¬çš„GPT-4.1åœ¨ç²¾ç¡®åº¦ä¸Šé¢†å…ˆï¼Œä¸”è¯¯æŠ¥ç‡æœ€ä½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæœ€å°ä¸Šä¸‹æ–‡çš„æ·»åŠ å¯ä»¥æ”¹å–„ä½èµ„æºç¯å¢ƒä¸­çš„æœ‰æ¯’è¯­è¨€æ£€æµ‹ï¼Œå¹¶å»ºè®®æ”¹è¿›æç¤ºè®¾è®¡å’Œé˜ˆå€¼æ ¡å‡†ç­‰å®é™…ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ä½èµ„æºå·´å°”å¹²è¯­è¨€ï¼ˆå¦‚å¡å°”ç»´äºšè¯­ã€å…‹ç½—åœ°äºšè¯­å’Œæ³¢æ–¯å°¼äºšè¯­ï¼‰ä¸­ï¼Œæœ‰æ¯’è¯­è¨€æ£€æµ‹çš„æœ‰æ•ˆæ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥å‡†ç¡®è¯†åˆ«æœ‰æ¯’è¯„è®ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œé€šè¿‡æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æå‡æœ‰æ¯’è¯­è¨€çš„æ£€æµ‹æ•ˆæœã€‚è¿™ç§è®¾è®¡æ—¨åœ¨åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æé«˜æ¨¡å‹çš„å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹é€‰æ‹©ä¸è®­ç»ƒã€æ€§èƒ½è¯„ä¼°ç­‰å¤šä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œæ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ï¼Œç„¶ååœ¨ä¸åŒçš„æ¨¡å‹ï¼ˆå¦‚GPT-3.5 Turboã€GPT-4.1ç­‰ï¼‰ä¸Šè¿›è¡Œé›¶æ ·æœ¬å’Œä¸Šä¸‹æ–‡å¢å¼ºçš„æµ‹è¯•ï¼Œæœ€åè¯„ä¼°æ¨¡å‹çš„ç²¾ç¡®åº¦ã€å¬å›ç‡ç­‰æŒ‡æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡ä¸Šä¸‹æ–‡å¢å¼ºçš„æ–¹å¼æ˜¾è‘—æé«˜äº†æœ‰æ¯’è¯­è¨€æ£€æµ‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚è¿™ä¸ä¼ ç»Ÿçš„ä»…ä¾èµ–äºæ¨¡å‹æœ¬èº«çš„æ£€æµ‹æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œå…³é”®çš„å‚æ•°è®¾ç½®åŒ…æ‹¬ä¸Šä¸‹æ–‡ç‰‡æ®µçš„é•¿åº¦ã€æ¨¡å‹çš„é€‰æ‹©ä»¥åŠè¯„ä¼°æŒ‡æ ‡çš„å®šä¹‰ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„å…·ä½“ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGeminiæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å¼ä¸‹è¾¾åˆ°äº†0.82çš„F1åˆ†æ•°å’Œå‡†ç¡®ç‡ï¼Œè€Œé›¶æ ·æœ¬çš„GPT-4.1åœ¨ç²¾ç¡®åº¦ä¸Šè¡¨ç°æœ€ä½³ï¼Œä¸”è¯¯æŠ¥ç‡æœ€ä½ã€‚è¿™è¡¨æ˜ï¼Œé€šè¿‡ä¼˜åŒ–æç¤ºè®¾è®¡å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä½¿ç”¨ï¼Œå¯ä»¥æ˜¾è‘—æå‡æœ‰æ¯’è¯­è¨€æ£€æµ‹çš„æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å¹³å°ã€åœ¨çº¿è¯„è®ºç³»ç»Ÿå’Œå†…å®¹å®¡æ ¸å·¥å…·ï¼Œå°¤å…¶æ˜¯åœ¨å·´å°”å¹²åœ°åŒºçš„ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­ã€‚é€šè¿‡æœ‰æ•ˆæ£€æµ‹æœ‰æ¯’è¯­è¨€ï¼Œå¯ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œä¿ƒè¿›å¥åº·çš„åœ¨çº¿äº¤æµç¯å¢ƒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ä½èµ„æºè¯­è¨€çš„æœ‰æ¯’è¯­è¨€æ£€æµ‹ä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„ç¤¾ä¼šä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Online toxic language causes real harm, especially in regions with limited moderation tools. In this study, we evaluate how large language models handle toxic comments in Serbian, Croatian, and Bosnian, languages with limited labeled data. We built and manually labeled a dataset of 4,500 YouTube and TikTok comments drawn from videos across diverse categories, including music, politics, sports, modeling, influencer content, discussions of sexism, and general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus) were tested in two modes: zero-shot and context-augmented. We measured precision, recall, F1 score, accuracy and false positive rates. Including a short context snippet raised recall by about 0.12 on average and improved F1 score by up to 0.10, though it sometimes increased false positives. The best balance came from Gemini in context-augmented mode, reaching an F1 score of 0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the lowest false alarms. We show how adding minimal context can improve toxic language detection in low-resource settings and suggest practical strategies such as improved prompt design and threshold calibration. These results show that prompt design alone can yield meaningful gains in toxicity detection for underserved Balkan language communities.

