---
layout: default
title: GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations
---

# GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21559" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21559v1</a>
  <a href="https://arxiv.org/pdf/2506.21559.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21559v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21559v1', 'GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junze Chen, Cheng Yang, Shujie Li, Zhiqiang Zhang, Yawen Li, Junping Du, Chuan Shi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGraphLAMAä»¥è§£å†³å›¾è¯­è¨€æ¨¡å‹é€‚åº”æ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾è¯­è¨€æ¨¡å‹` `å‚æ•°é€‚åº”` `å›¾ç¥ç»ç½‘ç»œ` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æŒ‡ä»¤è°ƒä¼˜` `å°‘æ ·æœ¬å­¦ä¹ ` `æ¨ç†é€Ÿåº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æœªè§ä»»åŠ¡æ—¶ï¼Œå­˜åœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒæŒ‡ä»¤è°ƒä¼˜æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºGraphLAMAæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å‚æ•°é€‚åº”é˜¶æ®µï¼Œä»…éœ€å°‘é‡æ ‡æ³¨ç¤ºä¾‹å³å¯é«˜æ•ˆè°ƒæ•´å›¾è¯­è¨€æ¨¡å‹ï¼Œæå‡é¢„æµ‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGraphLAMAåœ¨èŠ‚ç‚¹åˆ†ç±»å’Œæ‘˜è¦ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†4.91%çš„å‡†ç¡®ç‡æå‡ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†10å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œæœ€è¿‘è¢«æ•´åˆç”¨äºå›¾åˆ†æï¼Œå½¢æˆå›¾è¯­è¨€æ¨¡å‹ï¼ˆGLMsï¼‰ã€‚ä¸€äº›GLMsèƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°è§£é‡Šæœªè§ä»»åŠ¡ï¼Œå¹¶é€šè¿‡å°‘é‡ç¤ºä¾‹è¿›è¡Œå­¦ä¹ ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚ç„¶è€Œï¼ŒICLåœ¨å›¾ä¸Šçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡å­˜åœ¨é—®é¢˜ï¼Œè€ŒæŒ‡ä»¤è°ƒä¼˜éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œéš¾ä»¥è·å–ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºGraphLAMAæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥é¢å¤–çš„å‚æ•°é€‚åº”é˜¶æ®µï¼Œä»…éœ€å°‘é‡æ ‡æ³¨ç¤ºä¾‹å³å¯é«˜æ•ˆè°ƒæ•´GLMsï¼Œä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§å’Œæ¨ç†é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphLAMAåœ¨å°‘/é›¶æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»å’Œæ‘˜è¦ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡æå‡4.91%ï¼Œæ¨ç†é€Ÿåº¦åœ¨5-shotè®¾ç½®ä¸‹å¯æé«˜10å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å›¾è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹æœªè§ä»»åŠ¡æ—¶çš„é€‚åº”æ€§ä¸è¶³å’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å‚æ•°å›ºå®šï¼Œä¸”æŒ‡ä»¤è°ƒä¼˜éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œéš¾ä»¥åœ¨å®é™…åœºæ™¯ä¸­åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGraphLAMAé€šè¿‡å¼•å…¥é¢å¤–çš„å‚æ•°é€‚åº”é˜¶æ®µï¼Œå…è®¸æ¨¡å‹åœ¨ä»…æœ‰å°‘é‡æ ‡æ³¨ç¤ºä¾‹çš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ•ˆè°ƒæ•´ï¼Œä»è€Œæé«˜é¢„æµ‹å‡†ç¡®æ€§å’Œæ¨ç†é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGraphLAMAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒé˜¶æ®µå’Œé€‚åº”é˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œé™¤äº†LLMçš„å‚æ•°å¤–ï¼Œæ¨¡å‹çš„å…¶ä»–å‚æ•°ä¼šé’ˆå¯¹ä¸åŒä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œä»¥æ•æ‰é€šç”¨çŸ¥è¯†ã€‚åœ¨é€‚åº”é˜¶æ®µï¼Œä»…æ›´æ–°å°‘é‡é¢„è®­ç»ƒå‚æ•°ï¼ŒåŸºäºå°‘é‡ç¤ºä¾‹è¿›è¡Œè°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šGraphLAMAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å‚æ•°é€‚åº”æœºåˆ¶ï¼Œä½¿å¾—å›¾è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹é«˜æ•ˆé€‚åº”æ–°ä»»åŠ¡ï¼Œä¸ä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒæŒ‡ä»¤è°ƒä¼˜æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œè®¾è®¡äº†å¤šä¸ªç»„ä»¶å°†èŠ‚ç‚¹è½¬æ¢ä¸ºLLMæ ‡è®°çš„è¡¨ç¤ºç©ºé—´ã€‚ä»»åŠ¡æŒ‡ä»¤åˆ™é€šè¿‡èŠ‚ç‚¹å’Œè¯­è¨€æ ‡è®°çš„æ··åˆè¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒGraphLAMAåœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»åŠæ‘˜è¦ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æå‡4.91%ã€‚ä¸ä¸Šä¸‹æ–‡å­¦ä¹ ç›¸æ¯”ï¼Œå…¶æ¨ç†é€Ÿåº¦åœ¨5-shotè®¾ç½®ä¸‹æé«˜äº†10å€ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GraphLAMAçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå›¾æ•°æ®åˆ†æã€ç¤¾äº¤ç½‘ç»œåˆ†æã€çŸ¥è¯†å›¾è°±æ„å»ºç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„é€‚åº”èƒ½åŠ›å’Œå¿«é€Ÿæ¨ç†é€Ÿåº¦ä½¿å¾—åœ¨å®é™…åº”ç”¨ä¸­èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†åŠ¨æ€å˜åŒ–çš„ä»»åŠ¡éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated their strong capabilities in various domains, and have been recently integrated for graph analysis as graph language models (GLMs). With LLMs as the predictor, some GLMs can interpret unseen tasks described by natural language, and learn from a few examples in the prompts without parameter tuning, known as in-context learning (ICL). Another subset of GLMs utilizes abundant training labels to enhance model performance, known as instruction tuning. However, we argue that ICL on graphs has effectiveness issues due to fixed parameters and efficiency issues due to long context. Meanwhile, the large amount of labeled data required for instruction tuning can be difficult to obtain in real-world scenarios. To this end, we aim to introduce an extra parameter adaptation stage that can efficiently tailor GLMs to an unseen graph and task with only a few labeled examples, in exchange for better prediction accuracy and faster inference speed. For implementation, in this paper we propose GraphLAMA method, with its model backbone and learning schemes specialized for efficient tuning and inference. Specifically, for model backbone, we use a graph neural network (GNN) with several well-designed components to transform nodes into the representation space of LLM tokens. Task instructions can then be represented as a mixture of node and language tokens. In the pre-training stage, model parameters except the LLM will be trained with different tasks to capture general knowledge. In the adaptation stage, only a few pre-trained parameters will be updated based on few-shot examples. Extensive experiments on few/zero-shot node classification and summary generation show that our proposed GraphLAMA achieves state-of-the-art performance with 4.91% absolution improvement in accuracy. Compared with ICL, our inference speed can be 10 times faster under 5-shot setting.

