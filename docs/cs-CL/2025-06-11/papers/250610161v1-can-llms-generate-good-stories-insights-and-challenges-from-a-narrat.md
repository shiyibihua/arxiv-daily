---
layout: default
title: Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective
---

# Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10161" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10161v1</a>
  <a href="https://arxiv.org/pdf/2506.10161.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10161v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10161v1', 'Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Wang, Max Kreminski

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: In 2025 IEEE Conference on Games (CoG)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå™äº‹è§„åˆ’çš„åŸºå‡†ä»¥è¯„ä¼°LLMsçš„æ•…äº‹ç”Ÿæˆèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•…äº‹ç”Ÿæˆ` `å™äº‹è§„åˆ’` `è®¡ç®—å™äº‹å­¦` `å› æœåˆç†æ€§` `è§’è‰²æ„å›¾` `æˆå‰§å†²çª` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•…äº‹ç”Ÿæˆæ–¹æ³•åœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¯¼è‡´å¯¹LLMsç”Ÿæˆé«˜è´¨é‡æ•…äº‹èƒ½åŠ›çš„ç†è§£ä¸è¶³ã€‚
2. è®ºæ–‡é€šè¿‡å¼•å…¥å™äº‹è§„åˆ’é—®é¢˜ï¼Œåˆ©ç”¨è®¡ç®—å™äº‹å­¦çš„è§è§£ï¼Œæå‡ºäº†ä¸€ä¸ªè¯„ä¼°LLMsæ•…äº‹ç”Ÿæˆèƒ½åŠ›çš„åŸºå‡†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4çº§åˆ«çš„LLMsåœ¨å°è§„æ¨¡ä¸Šèƒ½å¤Ÿç”Ÿæˆå› æœåˆç†çš„æ•…äº‹ï¼Œä½†åœ¨è§’è‰²æ„å›¾å’Œæˆå‰§å†²çªæ–¹é¢ä»éœ€æ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•…äº‹ç”Ÿæˆæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸€ä¸ªé‡è¦åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºè‡ªåŠ¨è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ä»¥åŠäººå·¥è¯„ä¼°çš„é«˜æˆæœ¬å’Œä¸»è§‚æ€§ï¼Œç†è§£LLMsç”Ÿæˆé«˜è´¨é‡æ•…äº‹çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚è®¡ç®—å™äº‹å­¦æä¾›äº†å…³äºå¥½æ•…äº‹æ„æˆçš„å®è´µè§è§£ï¼Œå·²åº”ç”¨äºç¬¦å·å™äº‹è§„åˆ’æ–¹æ³•ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡è§£å†³å™äº‹è§„åˆ’é—®é¢˜æ¥åŠ æ·±å¯¹LLMsæ•…äº‹ç”Ÿæˆèƒ½åŠ›çš„ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ–‡çŒ®ç¤ºä¾‹çš„å™äº‹è§„åˆ’è¯„ä¼°åŸºå‡†ï¼Œé‡ç‚¹å…³æ³¨å› æœåˆç†æ€§ã€è§’è‰²æ„å›¾å’Œæˆå‰§å†²çªã€‚å®éªŒè¡¨æ˜ï¼ŒGPT-4çº§åˆ«çš„LLMsèƒ½å¤Ÿåœ¨å°è§„æ¨¡ä¸Šç”Ÿæˆå› æœåˆç†çš„æ•…äº‹ï¼Œä½†åœ¨è§’è‰²æ„å›¾å’Œæˆå‰§å†²çªçš„è§„åˆ’ä¸Šä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„LLMsè¿›è¡Œå¤æ‚æ¨ç†ã€‚ç ”ç©¶ç»“æœæä¾›äº†LLMsåœ¨ä¸åŒæ–¹é¢ç”Ÿæˆé«˜è´¨é‡æ•…äº‹çš„è§„æ¨¡æ´å¯Ÿï¼ŒåŒæ—¶æ­ç¤ºäº†æœ‰è¶£çš„é—®é¢˜è§£å†³è¡Œä¸ºï¼Œå¹¶ä¸ºåœ¨æ¸¸æˆç¯å¢ƒä¸­åº”ç”¨LLMå™äº‹è§„åˆ’æä¾›äº†æŒ‘æˆ˜å’Œè€ƒè™‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMsåœ¨æ•…äº‹ç”Ÿæˆä¸­çš„è¯„ä¼°é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è‡ªåŠ¨å’Œäººå·¥è¯„ä¼°ä¸­å­˜åœ¨ä¸»è§‚æ€§å’Œé«˜æˆæœ¬çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å™äº‹è§„åˆ’çš„æ¡†æ¶ï¼Œåˆ©ç”¨è®¡ç®—å™äº‹å­¦çš„ç†è®ºï¼Œå»ºç«‹ä¸€ä¸ªåŸºå‡†æ¥è¯„ä¼°LLMsåœ¨æ•…äº‹ç”Ÿæˆä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«å…³æ³¨å› æœåˆç†æ€§ã€è§’è‰²æ„å›¾å’Œæˆå‰§å†²çªã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å™äº‹è§„åˆ’é—®é¢˜çš„å®šä¹‰ã€LLMsçš„è®­ç»ƒä¸è¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ–‡çŒ®ç¤ºä¾‹çš„é€‰æ‹©ã€è¯„ä¼°æŒ‡æ ‡çš„è®¾è®¡å’Œå®éªŒç»“æœçš„åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œç»“åˆäº†å™äº‹è§„åˆ’çš„ç†è®ºï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°LLMsåœ¨æ•…äº‹ç”Ÿæˆä¸­çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§’è‰²æ„å›¾å’Œæˆå‰§å†²çªæ–¹é¢çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒLLMsä»¥æé«˜å…¶å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œè®¾ç½®äº†é’ˆå¯¹å› æœåˆç†æ€§å’Œè§’è‰²æ„å›¾çš„ç‰¹å®šæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–ç”Ÿæˆæ•…äº‹çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4çº§åˆ«çš„LLMsåœ¨å°è§„æ¨¡æ•…äº‹ç”Ÿæˆä¸­èƒ½å¤Ÿå®ç°å› æœåˆç†æ€§ï¼Œç„¶è€Œåœ¨è§’è‰²æ„å›¾å’Œæˆå‰§å†²çªçš„ç”Ÿæˆä¸Šä»å­˜åœ¨æŒ‘æˆ˜ã€‚å…·ä½“è€Œè¨€ï¼Œç”Ÿæˆçš„æ•…äº‹åœ¨å› æœå…³ç³»ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚çš„è§’è‰²äº’åŠ¨å’Œå†²çªæƒ…èŠ‚ä¸Šéœ€è¦è¿›ä¸€æ­¥çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¸¸æˆè®¾è®¡ã€äº’åŠ¨å™äº‹å’Œæ•™è‚²ç­‰ã€‚é€šè¿‡æ”¹è¿›LLMsçš„æ•…äº‹ç”Ÿæˆèƒ½åŠ›ï¼Œå¯ä»¥ä¸ºæ¸¸æˆå¼€å‘è€…æä¾›æ›´ä¸°å¯Œçš„å™äº‹å·¥å…·ï¼Œæå‡ç©å®¶çš„æ²‰æµ¸æ„Ÿå’Œå‚ä¸åº¦ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºæ•™è‚²é¢†åŸŸçš„åˆ›æ„å†™ä½œæä¾›äº†æ–°çš„æ€è·¯ï¼Œå¸®åŠ©å­¦ç”Ÿæ›´å¥½åœ°ç†è§£æ•…äº‹ç»“æ„å’Œå™äº‹æŠ€å·§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Story generation has been a prominent application of Large Language Models (LLMs). However, understanding LLMs' ability to produce high-quality stories remains limited due to challenges in automatic evaluation methods and the high cost and subjectivity of manual evaluation. Computational narratology offers valuable insights into what constitutes a good story, which has been applied in the symbolic narrative planning approach to story generation. This work aims to deepen the understanding of LLMs' story generation capabilities by using them to solve narrative planning problems. We present a benchmark for evaluating LLMs on narrative planning based on literature examples, focusing on causal soundness, character intentionality, and dramatic conflict. Our experiments show that GPT-4 tier LLMs can generate causally sound stories at small scales, but planning with character intentionality and dramatic conflict remains challenging, requiring LLMs trained with reinforcement learning for complex reasoning. The results offer insights on the scale of stories that LLMs can generate while maintaining quality from different aspects. Our findings also highlight interesting problem solving behaviors and shed lights on challenges and considerations for applying LLM narrative planning in game environments.

