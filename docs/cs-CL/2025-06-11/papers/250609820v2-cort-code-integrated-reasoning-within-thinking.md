---
layout: default
title: CoRT: Code-integrated Reasoning within Thinking
---

# CoRT: Code-integrated Reasoning within Thinking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09820" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09820v2</a>
  <a href="https://arxiv.org/pdf/2506.09820.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09820v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09820v2', 'CoRT: Code-integrated Reasoning within Thinking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-06-12)

**å¤‡æ³¨**: work in progress

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ChengpengLi1003/CoRT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoRTæ¡†æ¶ä»¥æå‡å¤§è§„æ¨¡æ¨ç†æ¨¡å‹çš„æ•°å­¦è¿ç®—èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹æ¨ç†æ¨¡å‹` `æ•°å­¦æ¨ç†` `ä»£ç é›†æˆ` `Hint-Engineering` `åè®­ç»ƒæ¡†æ¶` `æ€§èƒ½æå‡` `æ•ˆç‡ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ•°å­¦è¿ç®—æ—¶æ•ˆç‡ä½ä¸‹ï¼Œä¸”å‡†ç¡®æ€§ä¸è¶³ï¼ŒäºŸéœ€æ”¹è¿›ã€‚
2. æœ¬æ–‡æå‡ºCoRTæ¡†æ¶ï¼Œé€šè¿‡Hint-Engineeringåˆæˆæ•°æ®ï¼Œä¼˜åŒ–å¤§å‹æ¨ç†æ¨¡å‹ä¸ä»£ç è§£é‡Šå™¨çš„äº¤äº’ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Hint-Engineeringçš„æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šåˆ†åˆ«æå‡äº†4%å’Œ8%çš„æ€§èƒ½ï¼Œå¹¶å‡å°‘äº†tokenä½¿ç”¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚o1å’ŒDeepSeek-R1åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚æ•°å­¦è¿ç®—æ—¶ä»ç„¶æ•ˆç‡ä½ä¸‹æˆ–ä¸å‡†ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CoRTï¼Œä¸€ä¸ªåè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨ä»£ç è§£é‡Šå™¨ï¼ˆCIï¼‰ã€‚é€šè¿‡Hint-EngineeringæŠ€æœ¯åˆæˆä»£ç é›†æˆæ¨ç†æ•°æ®ï¼Œä¼˜åŒ–LRMä¸CIçš„äº¤äº’ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨Hint-Engineeringçš„æ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šå®ç°äº†4%å’Œ8%çš„ç»å¯¹æå‡ï¼ŒåŒæ—¶åœ¨tokenä½¿ç”¨ä¸Šä¹Ÿæ˜¾è‘—å‡å°‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ•°å­¦è¿ç®—ä¸­çš„ä½æ•ˆç‡å’Œä½å‡†ç¡®æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¸å¤–éƒ¨çŸ¥è¯†ï¼ˆå¦‚ä»£ç è§£é‡Šå™¨ï¼‰ç»“åˆæ—¶ï¼Œæ•ˆç‡ä¸é«˜ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºCoRTæ¡†æ¶ï¼Œé€šè¿‡Hint-EngineeringæŠ€æœ¯åˆæˆä»£ç é›†æˆæ¨ç†æ•°æ®ï¼Œä¼˜åŒ–æ¨¡å‹ä¸ä»£ç è§£é‡Šå™¨çš„äº¤äº’ï¼Œä»è€Œæå‡æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoRTæ¡†æ¶åŒ…æ‹¬æ•°æ®åˆæˆã€åè®­ç»ƒå’Œå¤šç§å¾®è°ƒç­–ç•¥ã€‚é¦–å…ˆï¼Œé€šè¿‡Hint-Engineeringåˆæˆé«˜è´¨é‡çš„ä»£ç é›†æˆæ¨ç†æ•°æ®ï¼Œç„¶åå¯¹ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒã€æ‹’ç»å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºHint-Engineeringï¼Œé€šè¿‡åœ¨é€‚å½“ä½ç½®æ’å…¥ä¸åŒæç¤ºï¼Œä¼˜åŒ–äº†LRMä¸CIçš„äº¤äº’ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œæ‰‹åŠ¨åˆ›å»ºäº†30ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œæ¨¡å‹å‚æ•°èŒƒå›´ä»1.5Båˆ°32Bï¼Œé‡‡ç”¨äº†å¤šç§å¾®è°ƒç­–ç•¥ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œç¡®ä¿äº†å®éªŒçš„ä¸¥è°¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨Hint-Engineeringçš„æ¨¡å‹åœ¨DeepSeek-R1-Distill-Qwen-32Bå’ŒDeepSeek-R1-Distill-Qwen-1.5Bä¸Šåˆ†åˆ«å®ç°äº†4%å’Œ8%çš„ç»å¯¹æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œ32Bæ¨¡å‹çš„tokenä½¿ç”¨é‡å‡å°‘çº¦30%ï¼Œ1.5Bæ¨¡å‹å‡å°‘çº¦50%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆç‡æ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èå’Œç§‘å­¦è®¡ç®—ç­‰éœ€è¦å¤æ‚æ•°å­¦æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æå‡å¤§å‹æ¨ç†æ¨¡å‹çš„æ•°å­¦è¿ç®—èƒ½åŠ›ï¼ŒCoRTæ¡†æ¶å¯ä»¥ä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´é«˜æ•ˆçš„æ™ºèƒ½å†³ç­–æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable progress in natural language reasoning with long chain-of-thought (CoT), yet they remain inefficient or inaccurate when handling complex mathematical operations. Addressing these limitations through computational tools (e.g., computation libraries and symbolic solvers) is promising, but it introduces a technical challenge: Code Interpreter (CI) brings external knowledge beyond the model's internal text representations, thus the direct combination is not efficient. This paper introduces CoRT, a post-training framework for teaching LRMs to leverage CI effectively and efficiently. As a first step, we address the data scarcity issue by synthesizing code-integrated reasoning data through Hint-Engineering, which strategically inserts different hints at appropriate positions to optimize LRM-CI interaction. We manually create 30 high-quality samples, upon which we post-train models ranging from 1.5B to 32B parameters, with supervised fine-tuning, rejection fine-tuning and reinforcement learning. Our experimental results demonstrate that Hint-Engineering models achieve 4\% and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging mathematical reasoning datasets. Furthermore, Hint-Engineering models use about 30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model compared with the natural language models. The models and code are available at https://github.com/ChengpengLi1003/CoRT.

