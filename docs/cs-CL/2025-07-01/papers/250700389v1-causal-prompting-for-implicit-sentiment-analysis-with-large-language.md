---
layout: default
title: Causal Prompting for Implicit Sentiment Analysis with Large Language Models
---

# Causal Prompting for Implicit Sentiment Analysis with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00389" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00389v1</a>
  <a href="https://arxiv.org/pdf/2507.00389.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00389v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00389v1', 'Causal Prompting for Implicit Sentiment Analysis with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jing Ren, Wenhao Zhou, Bowen Li, Mujie Liu, Nguyen Linh Dan Le, Jiade Cen, Liping Chen, Ziqi Xu, Xiwei Xu, Xiaodong Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-07-01

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/whZ62/CAPITAL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAPITALæ¡†æ¶ä»¥è§£å†³éšå«æƒ…æ„Ÿåˆ†æä¸­çš„å› æœæ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éšå«æƒ…æ„Ÿåˆ†æ` `å› æœæ¨ç†` `é“¾å¼æ¨ç†` `å¤§è¯­è¨€æ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `é²æ£’æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„éšå«æƒ…æ„Ÿåˆ†ææ–¹æ³•ä¾èµ–äºé“¾å¼æ¨ç†çš„å¤šæ•°æŠ•ç¥¨ï¼Œæœªè€ƒè™‘å› æœæœ‰æ•ˆæ€§ï¼Œå¯¼è‡´åè§å’Œè™šå‡ç›¸å…³æ€§é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºCAPITALæ¡†æ¶ï¼Œé€šè¿‡å‰é—¨è°ƒæ•´å°†å› æœæ¨ç†å¼•å…¥é“¾å¼æ¨ç†ï¼Œåˆ†è§£å› æœæ•ˆåº”ä»¥æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†ISAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCAPITALåœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæç¤ºæ–¹æ³•ï¼Œå°¤å…¶åœ¨å¯¹æŠ—æ€§æ¡ä»¶ä¸‹è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšå«æƒ…æ„Ÿåˆ†æï¼ˆISAï¼‰æ—¨åœ¨æ¨æ–­éšå«è€Œéæ˜ç¡®è¡¨è¾¾çš„æƒ…æ„Ÿï¼Œè¿™è¦æ±‚æ¨¡å‹åœ¨å¾®å¦™çš„ä¸Šä¸‹æ–‡çº¿ç´¢ä¸Šè¿›è¡Œæ›´æ·±å±‚æ¬¡çš„æ¨ç†ã€‚å°½ç®¡è¿‘æœŸåŸºäºæç¤ºçš„æ–¹æ³•åœ¨ISAä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€ä¾èµ–äºé“¾å¼æ¨ç†è·¯å¾„çš„å¤šæ•°æŠ•ç¥¨ï¼Œè€Œæœªè¯„ä¼°å…¶å› æœæœ‰æ•ˆæ€§ï¼Œå¯¼è‡´æ¨¡å‹æ˜“å—å†…éƒ¨åè§å’Œè™šå‡ç›¸å…³æ€§çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†CAPITALï¼Œä¸€ä¸ªå°†å‰é—¨è°ƒæ•´èå…¥é“¾å¼æ¨ç†çš„å› æœæç¤ºæ¡†æ¶ã€‚CAPITALå°†æ•´ä½“å› æœæ•ˆåº”åˆ†è§£ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šè¾“å…¥æç¤ºå¯¹æ¨ç†é“¾çš„å½±å“ï¼Œä»¥åŠè¿™äº›é“¾å¯¹æœ€ç»ˆè¾“å‡ºçš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAPITALåœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºå¼ºåŸºçº¿ï¼Œå°¤å…¶åœ¨å¯¹æŠ—æ¡ä»¶ä¸‹è¡¨ç°çªå‡ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šéšå«æƒ…æ„Ÿåˆ†æï¼ˆISAï¼‰éœ€è¦æ¨æ–­æœªæ˜ç¡®è¡¨è¾¾çš„æƒ…æ„Ÿï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–é“¾å¼æ¨ç†çš„å¤šæ•°æŠ•ç¥¨ï¼Œæœªè€ƒè™‘å› æœå…³ç³»çš„æœ‰æ•ˆæ€§ï¼Œå¯¼è‡´æ¨¡å‹æ˜“å—åè§å’Œè™šå‡ç›¸å…³æ€§å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„CAPITALæ¡†æ¶é€šè¿‡å‰é—¨è°ƒæ•´å°†å› æœæ¨ç†å¼•å…¥é“¾å¼æ¨ç†ï¼Œæ—¨åœ¨åˆ†è§£å› æœæ•ˆåº”ï¼Œæ˜ç¡®è¾“å…¥æç¤ºå¯¹æ¨ç†é“¾å’Œæœ€ç»ˆè¾“å‡ºçš„å½±å“ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCAPITALæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šä¸€æ˜¯é€šè¿‡ç¼–ç å™¨è¿›è¡Œèšç±»ä»¥ä¼°è®¡æ¨ç†é“¾çš„å½±å“ï¼ŒäºŒæ˜¯ä½¿ç”¨NWGMè¿‘ä¼¼æ¥è¯„ä¼°æœ€ç»ˆè¾“å‡ºçš„å½±å“ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ ç›®æ ‡æ¥ä¼˜åŒ–ç¼–ç å™¨çš„è¡¨ç¤ºä¸LLMçš„æ¨ç†ç©ºé—´çš„å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šCAPITALçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å› æœæ¨ç†ä¸é“¾å¼æ¨ç†ç»“åˆï¼Œæ˜ç¡®äº†æ¨ç†è¿‡ç¨‹ä¸­çš„å› æœå…³ç³»ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„åè§å’Œè™šå‡ç›¸å…³æ€§é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒCAPITALé‡‡ç”¨äº†ç¼–ç å™¨èšç±»å’ŒNWGMè¿‘ä¼¼æ¥ä¼°è®¡å› æœæ•ˆåº”ï¼Œå¹¶å¼•å…¥å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ä»¥å¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œç¡®ä¿æ¨ç†è¿‡ç¨‹çš„å› æœæœ‰æ•ˆæ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†ISAæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCAPITALæ¡†æ¶åœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æç¤ºæ–¹æ³•ï¼Œå°¤å…¶åœ¨å¯¹æŠ—æ€§æ¡ä»¶ä¸‹ï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼Œå±•ç°äº†å…¶åœ¨å¤„ç†å¤æ‚æƒ…æ„Ÿæ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æã€å¸‚åœºæƒ…æ„Ÿç›‘æµ‹å’Œç”¨æˆ·åé¦ˆåˆ†æç­‰ã€‚é€šè¿‡æé«˜éšå«æƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼ŒCAPITALæ¡†æ¶èƒ½å¤Ÿä¸ºä¼ä¸šå’Œç ”ç©¶æœºæ„æä¾›æ›´å¯é çš„æƒ…æ„Ÿæ´å¯Ÿï¼Œå¸®åŠ©å…¶æ›´å¥½åœ°ç†è§£ç”¨æˆ·æƒ…æ„Ÿå’Œè¡Œä¸ºã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨å› æœæ¨ç†åœ¨AIé¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied rather than explicitly stated, requiring models to perform deeper reasoning over subtle contextual cues. While recent prompting-based methods using Large Language Models (LLMs) have shown promise in ISA, they often rely on majority voting over chain-of-thought (CoT) reasoning paths without evaluating their causal validity, making them susceptible to internal biases and spurious correlations. To address this challenge, we propose CAPITAL, a causal prompting framework that incorporates front-door adjustment into CoT reasoning. CAPITAL decomposes the overall causal effect into two components: the influence of the input prompt on the reasoning chains, and the impact of those chains on the final output. These components are estimated using encoder-based clustering and the NWGM approximation, with a contrastive learning objective used to better align the encoder's representation with the LLM's reasoning space. Experiments on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently outperforms strong prompting baselines in both accuracy and robustness, particularly under adversarial conditions. This work offers a principled approach to integrating causal inference into LLM prompting and highlights its benefits for bias-aware sentiment reasoning. The source code and case study are available at: https://github.com/whZ62/CAPITAL.

