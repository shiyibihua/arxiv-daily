---
layout: default
title: NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching
---

# NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13721" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13721v2</a>
  <a href="https://arxiv.org/pdf/2510.13721.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13721v2" onclick="toggleFavorite(this, '2510.13721v2', 'NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15 (æ›´æ–°: 2025-10-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**NExT-OMNIï¼šåŸºäºç¦»æ•£æµåŒ¹é…çš„ä»»æ„åˆ°ä»»æ„å…¨æ¨¡æ€ç»Ÿä¸€å»ºæ¨¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å…¨æ¨¡æ€æ¨¡å‹` `ç¦»æ•£æµåŒ¹é…` `è·¨æ¨¡æ€æ£€ç´¢` `å¤šè½®äº¤äº’` `ç»Ÿä¸€å»ºæ¨¡` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹å—é™äºè‡ªå›å½’æ¶æ„ï¼Œéš¾ä»¥å¹³è¡¡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œæ··åˆå’Œè§£è€¦ç­–ç•¥è®¾è®¡å†—ä½™ï¼Œé™åˆ¶äº†å…¶åœ¨è·¨æ¨¡æ€æ£€ç´¢ç­‰æ›´å¹¿æ³›åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. NExT-OMNIåˆ©ç”¨ç¦»æ•£æµèŒƒå¼ï¼Œé€šè¿‡åº¦é‡è¯±å¯¼çš„æ¦‚ç‡è·¯å¾„å’ŒåŠ¨åŠ›å­¦æœ€ä¼˜é€Ÿåº¦ï¼Œå®ç°ä»»æ„æ¨¡æ€åˆ°ä»»æ„æ¨¡æ€çš„ç»Ÿä¸€å»ºæ¨¡ï¼Œæå‡å“åº”æ•ˆç‡ã€‚
3. NExT-OMNIåœ¨å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨å¤šè½®å¤šæ¨¡æ€äº¤äº’å’Œè·¨æ¨¡æ€æ£€ç´¢æ–¹é¢è¶…è¶Šäº†ä»¥å¾€çš„ç»Ÿä¸€æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†NExT-OMNIï¼Œä¸€ä¸ªå¼€æºçš„å…¨æ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ç¦»æ•£æµèŒƒå¼å®ç°ç»Ÿä¸€å»ºæ¨¡ï¼Œä»è€Œæ”¯æŒä»»æ„æ¨¡æ€åˆ°ä»»æ„æ¨¡æ€çš„ç†è§£å’Œç”Ÿæˆï¼Œå¹¶æé«˜å“åº”æ•ˆç‡ã€‚NExT-OMNIåˆ©ç”¨åº¦é‡è¯±å¯¼çš„æ¦‚ç‡è·¯å¾„å’ŒåŠ¨åŠ›å­¦æœ€ä¼˜é€Ÿåº¦ï¼Œé€šè¿‡ç®€æ´çš„ç»Ÿä¸€è¡¨ç¤ºè€Œéä»»åŠ¡è§£è€¦è®¾è®¡ï¼Œå®ç°æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚è·¨æ¨¡æ€æ£€ç´¢ã€‚è¯¥æ¨¡å‹åœ¨å¤§è§„æ¨¡äº¤é”™çš„æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šè½®å¤šæ¨¡æ€äº¤äº’å’Œè·¨æ¨¡æ€æ£€ç´¢æ–¹é¢ä¼˜äºä»¥å¾€çš„ç»Ÿä¸€æ¨¡å‹ï¼Œçªæ˜¾äº†å…¶ä½œä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„æ¶æ„ä¼˜åŠ¿ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œä½œè€…å‘å¸ƒäº†è®­ç»ƒç»†èŠ‚ã€æ•°æ®åè®®ä»¥åŠä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§éƒ¨åˆ†å¤šæ¨¡æ€æ¨¡å‹ä¾èµ–äºè‡ªå›å½’æ¶æ„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¸Šçš„å¹³è¡¡ã€‚æ­¤å¤–ï¼Œè™½ç„¶ä¸€äº›æ··åˆå’Œè§£è€¦ç­–ç•¥è¢«ç”¨äºç»Ÿä¸€æ¡†æ¶ä¸­ï¼Œä½†å®ƒä»¬çš„è®¾è®¡è¾ƒä¸ºå†—ä½™ï¼Œæ— æ³•å¾ˆå¥½åœ°åº”ç”¨äºæ›´å¹¿æ³›çš„åœºæ™¯ï¼Œä¾‹å¦‚è·¨æ¨¡æ€æ£€ç´¢ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªæ›´åŠ ç»Ÿä¸€å’Œé«˜æ•ˆçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæ”¯æŒä»»æ„æ¨¡æ€ä¹‹é—´çš„è½¬æ¢å’Œäº¤äº’ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNExT-OMNIçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¦»æ•£æµåŒ¹é…ï¼ˆDiscrete Flow Matchingï¼‰èŒƒå¼ï¼Œå°†ä¸åŒæ¨¡æ€çš„æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ä¸­ã€‚é€šè¿‡å­¦ä¹ åº¦é‡è¯±å¯¼çš„æ¦‚ç‡è·¯å¾„å’ŒåŠ¨åŠ›å­¦æœ€ä¼˜é€Ÿåº¦ï¼Œæ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´è¿›è¡Œè½¬æ¢ï¼Œä»è€Œå®ç°ä»»æ„æ¨¡æ€åˆ°ä»»æ„æ¨¡æ€çš„ç†è§£å’Œç”Ÿæˆã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¼ ç»Ÿè‡ªå›å½’æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§æ›´åŠ ç®€æ´å’Œç»Ÿä¸€çš„å»ºæ¨¡æ–¹å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNExT-OMNIçš„æ•´ä½“æ¶æ„åŸºäºç¦»æ•£æµåŒ¹é…ã€‚é¦–å…ˆï¼Œä¸åŒæ¨¡æ€çš„æ•°æ®é€šè¿‡å„è‡ªçš„ç¼–ç å™¨æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ã€‚ç„¶åï¼Œæ¨¡å‹å­¦ä¹ ä¸€ä¸ªæµåœºï¼Œè¯¥æµåœºå®šä¹‰äº†ä»ä¸€ä¸ªæ¨¡æ€åˆ°å¦ä¸€ä¸ªæ¨¡æ€çš„æ¦‚ç‡è·¯å¾„ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æ²¿ç€è¿™ä¸ªæµåœºè¿›è¡Œé‡‡æ ·ï¼Œä»è€Œç”Ÿæˆç›®æ ‡æ¨¡æ€çš„æ•°æ®ã€‚è¯¥æ¡†æ¶åŒ…å«æ¨¡æ€ç¼–ç å™¨ã€æµåœºå­¦ä¹ æ¨¡å—å’Œæ¨¡æ€è§£ç å™¨ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šNExT-OMNIçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨ç¦»æ•£æµåŒ¹é…èŒƒå¼è¿›è¡Œå¤šæ¨¡æ€ç»Ÿä¸€å»ºæ¨¡ã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹ç›¸æ¯”ï¼Œç¦»æ•£æµåŒ¹é…èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æ”¯æŒä»»æ„æ¨¡æ€ä¹‹é—´çš„è½¬æ¢ã€‚æ­¤å¤–ï¼ŒNExT-OMNIé€šè¿‡å­¦ä¹ åº¦é‡è¯±å¯¼çš„æ¦‚ç‡è·¯å¾„å’ŒåŠ¨åŠ›å­¦æœ€ä¼˜é€Ÿåº¦ï¼Œè¿›ä¸€æ­¥æé«˜äº†ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šNExT-OMNIçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformerä½œä¸ºæ¨¡æ€ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œä»¥æ•æ‰ä¸åŒæ¨¡æ€æ•°æ®çš„å¤æ‚ç‰¹å¾ï¼›2) è®¾è®¡äº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºå­¦ä¹ æµåœºï¼Œè¯¥æŸå¤±å‡½æ•°åŒæ—¶è€ƒè™‘äº†ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ï¼›3) é‡‡ç”¨äº†ä¸€ç§è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜ç”Ÿæˆçš„å¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

NExT-OMNIåœ¨å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨å¤šè½®å¤šæ¨¡æ€äº¤äº’å’Œè·¨æ¨¡æ€æ£€ç´¢æ–¹é¢ï¼ŒNExT-OMNIæ˜¾è‘—ä¼˜äºä»¥å¾€çš„ç»Ÿä¸€æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æ¶æ„çš„ä¼˜è¶Šæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œè¡¨æ˜NExT-OMNIåœ¨å¤šä¸ªä»»åŠ¡ä¸Šéƒ½å–å¾—äº†SOTAæˆ–æ¥è¿‘SOTAçš„ç»“æœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

NExT-OMNIå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿã€è·¨æ¨¡æ€å†…å®¹ç”Ÿæˆã€æ™ºèƒ½åŠ©æ‰‹ã€æ•™è‚²å¨±ä¹ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆå›¾åƒæè¿°ã€è§†é¢‘æ‘˜è¦ã€éŸ³é¢‘è½¬å½•ç­‰ï¼Œè¿˜å¯ä»¥ç”¨äºå®ç°æ›´è‡ªç„¶å’Œæ™ºèƒ½çš„äººæœºäº¤äº’ã€‚æœªæ¥ï¼ŒNExT-OMNIæœ‰æœ›æˆä¸ºé€šç”¨äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Next-generation multimodal foundation models capable of any-to-any cross-modal generation and multi-turn interaction will serve as core components of artificial general intelligence systems, playing a pivotal role in human-machine interaction. However, most existing multimodal models remain constrained by autoregressive architectures, whose inherent limitations prevent a balanced integration of understanding and generation capabilities. Although hybrid and decoupling strategies have been explored to address these tasks within unified frameworks separately, their redundant, non-integrated designs limit their applicability to broader scenarios, such as cross-modal retrieval. In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling through discrete flow paradigms. By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency, while enabling broader application scenarios through concise unified representations rather than task-decoupled designs. Trained on large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers competitive performance on multimodal generation and understanding benchmarks, while outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval, highlighting its architectural advantages as a next-generation multimodal foundation model. To advance further research, we release training details, data protocols, and open-source both the code and model checkpoints.

