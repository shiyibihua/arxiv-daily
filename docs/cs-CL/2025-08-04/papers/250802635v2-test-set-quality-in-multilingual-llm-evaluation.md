---
layout: default
title: Test Set Quality in Multilingual LLM Evaluation
---

# Test Set Quality in Multilingual LLM Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02635" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02635v2</a>
  <a href="https://arxiv.org/pdf/2508.02635.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02635v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02635v2', 'Test Set Quality in Multilingual LLM Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chalamalasetti Kranti, Gabriel Bernier-Colborne, Yvan Gauthier, Sowmya Vajjala

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-11-13)

**å¤‡æ³¨**: to appear in the proceedings of Eval4NLP workshop at AACL 2025. Camera ready version

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¯­è¨€LLMè¯„ä¼°æ•°æ®é›†è´¨é‡åˆ†ææ–¹æ³•ä»¥æå‡è¯„ä¼°å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€è¯„ä¼°` `æ•°æ®é›†è´¨é‡` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ³•è¯­` `æ³°å¢å›ºè¯­` `æ€§èƒ½æ¯”è¾ƒ` `é”™è¯¯è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šè¯­è¨€è¯„ä¼°æ•°æ®é›†çš„è´¨é‡æœªå—åˆ°è¶³å¤Ÿé‡è§†ï¼Œå¯èƒ½å½±å“å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ç»“æœã€‚
2. é€šè¿‡æ‰‹åŠ¨åˆ†ææ³•è¯­å’Œæ³°å¢å›ºè¯­çš„è¯„ä¼°é›†ï¼Œè¯†åˆ«å¹¶ä¿®æ­£æ•°æ®é›†ä¸­çš„é”™è¯¯ï¼Œä»¥æé«˜è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¿®è®¢åçš„æ•°æ®é›†åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„æ€§èƒ½æå‡å¯è¾¾è¿‘10%ï¼Œå¼ºè°ƒäº†æ•°æ®é›†è´¨é‡çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šä¸ªå¤šè¯­è¨€åŸºå‡†æ•°æ®é›†ä»¥åŠè‡ªåŠ¨æ–¹å¼å¼€å‘ï¼Œç”¨äºè¡¡é‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°½ç®¡å·²æœ‰ç ”ç©¶æŒ‡å‡ºå®Œå…¨äººå·¥æ ‡æ³¨æµ‹è¯•é›†ä¸­çš„é”™è¯¯ï¼Œæ•°æ®é›†è´¨é‡é—®é¢˜ä»æœªå—åˆ°è¶³å¤Ÿé‡è§†ã€‚æœ¬æ–‡æ‰‹åŠ¨åˆ†æäº†æ³•è¯­å’Œæ³°å¢å›ºè¯­çš„å¤šè¯­è¨€è¯„ä¼°é›†ï¼Œè¯†åˆ«å‡ºå¤šä¸ªé”™è¯¯ï¼Œå¹¶æ¯”è¾ƒäº†ä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸå§‹å’Œä¿®è®¢ç‰ˆæœ¬æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå‘ç°ä¸¤ç§è¯­è¨€çš„æ€§èƒ½å·®å¼‚å¯è¾¾è¿‘10%ã€‚åŸºäºè¿™äº›ç»“æœï¼Œä½œè€…ä¸»å¼ æµ‹è¯•é›†åº”è¢«è§†ä¸ºå¯å˜çš„ï¼Œéœ€å®šæœŸæ£€æŸ¥å’Œä¿®è®¢ã€‚æœ€åï¼Œæå‡ºäº†é’ˆå¯¹æ•°æ®é›†åˆ›å»ºè€…å’Œä½¿ç”¨è€…çš„è´¨é‡æ”¹è¿›å»ºè®®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€è¯„ä¼°æ•°æ®é›†è´¨é‡ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œä¿®æ­£æ•°æ®é›†ä¸­çš„é”™è¯¯ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ‰‹åŠ¨åˆ†ææ³•è¯­å’Œæ³°å¢å›ºè¯­çš„å¤šè¯­è¨€è¯„ä¼°é›†ï¼Œè¯†åˆ«æ•°æ®é›†ä¸­çš„é”™è¯¯ï¼Œå¹¶æ¯”è¾ƒä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸå§‹å’Œä¿®è®¢ç‰ˆæœ¬æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œä»¥éªŒè¯æ•°æ®é›†è´¨é‡å¯¹è¯„ä¼°ç»“æœçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶æµç¨‹åŒ…æ‹¬æ•°æ®é›†çš„æ‰‹åŠ¨åˆ†æã€é”™è¯¯è¯†åˆ«ã€æ•°æ®é›†ä¿®è®¢ä»¥åŠæ€§èƒ½æ¯”è¾ƒã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é›†åˆ†æå·¥å…·ã€é”™è¯¯åˆ†ç±»æ ‡å‡†å’Œæ¨¡å‹è¯„ä¼°æŒ‡æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¯¹å¤šè¯­è¨€è¯„ä¼°æ•°æ®é›†è¿›è¡Œç³»ç»Ÿæ€§è´¨é‡æ£€æŸ¥çš„æ–¹æ³•ï¼Œå¼ºè°ƒäº†æ•°æ®é›†çš„å¯å˜æ€§å’Œå®šæœŸä¿®è®¢çš„é‡è¦æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºå…¨é¢çš„è¯„ä¼°è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†åˆ†æä¸­ï¼Œé‡‡ç”¨äº†å¤šç§é”™è¯¯è¯†åˆ«æ ‡å‡†ï¼Œå¹¶è®¾è®¡äº†ä¿®è®¢æµç¨‹ï¼Œç¡®ä¿ä¿®è®¢åçš„æ•°æ®é›†èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¿®è®¢åçš„æ•°æ®é›†åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼ŒæŸäº›æƒ…å†µä¸‹æ€§èƒ½å·®å¼‚æ¥è¿‘10%ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æ•°æ®é›†è´¨é‡å¯¹æ¨¡å‹è¯„ä¼°çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„æ•°æ®é›†æ„å»ºæä¾›äº†å®è¯ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œè·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡è¯„ä¼°æ•°æ®é›†çš„è´¨é‡ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œæ¨åŠ¨å¤šè¯­è¨€æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½å½±å“æ•°æ®é›†åˆ›å»ºå’Œä½¿ç”¨çš„æ ‡å‡†ï¼Œä¿ƒè¿›æ›´é«˜è´¨é‡çš„å¤šè¯­è¨€æ¨¡å‹ç ”ç©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues.

