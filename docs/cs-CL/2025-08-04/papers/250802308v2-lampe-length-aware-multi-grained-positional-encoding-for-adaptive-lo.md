---
layout: default
title: LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive Long-context Scaling Without Training
---

# LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive Long-context Scaling Without Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02308" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.02308v2</a>
  <a href="https://arxiv.org/pdf/2508.02308.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02308v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02308v2', 'LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive Long-context Scaling Without Training')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Sikui Zhang, Guangze Gao, Ziyun Gan, Chunfeng Yuan, Zefeng Lin, Houwen Peng, Bing Li, Weiming Hu

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-04 (Êõ¥Êñ∞: 2025-08-05)

**Â§áÊ≥®**: 13 pages, 9 figures

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/scar-on/LaMPE)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LaMPE‰ª•Ëß£ÂÜ≥ÈïøÊñáÊú¨ËæìÂÖ•ÁöÑÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÊñáÊú¨Â§ÑÁêÜ` `‰ΩçÁΩÆÁºñÁ†Å` `Ëá™ÈÄÇÂ∫îÊ®°Âûã` `Â§öÁ≤íÂ∫¶Ê≥®ÊÑèÂäõ` `Êó†ËÆ≠ÁªÉÊñπÊ≥ï`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜË∂ÖËøáÈ¢ÑËÆ≠ÁªÉ‰∏ä‰∏ãÊñáÁ™óÂè£ÁöÑËæìÂÖ•Êó∂ÔºåÊÄßËÉΩÊòæËëó‰∏ãÈôçÔºåÊú™ËÉΩÊúâÊïàÂà©Áî®Ê®°ÂûãÁöÑ‰∏ä‰∏ãÊñáËÉΩÂäõ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑLaMPEÈÄöËøáÂä®ÊÄÅÊò†Â∞ÑËæìÂÖ•ÈïøÂ∫¶‰∏é‰ΩçÁΩÆÁºñÁ†ÅÔºå‰ºòÂåñ‰∫ÜÈïøÊñáÊú¨ÁöÑÂ§ÑÁêÜËÉΩÂäõÔºåÈÅøÂÖç‰∫ÜÂõ∫ÂÆöÊò†Â∞ÑÁöÑÂ±ÄÈôêÊÄß„ÄÇ
3. Âú®Â§ö‰∏™Èïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÊµãËØï‰∏≠ÔºåLaMPEÁõ∏ËæÉ‰∫éÁé∞ÊúâÊñπÊ≥ïÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÈÄÇÁî®ÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËæìÂÖ•Ë∂ÖËøáÈ¢ÑËÆ≠ÁªÉ‰∏ä‰∏ãÊñáÁ™óÂè£Êó∂ÔºåÊÄßËÉΩÊòæËëó‰∏ãÈôçÔºå‰∏ªË¶ÅÁî±‰∫éÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÁöÑÂàÜÂ∏ÉÂ§ñÔºàOODÔºâË°å‰∏∫„ÄÇÁé∞ÊúâÁ†îÁ©∂ÈÄöËøáÂõ∫ÂÆöÊò†Â∞ÑÁ≠ñÁï•ÁºìËß£Ê≠§ÈóÆÈ¢òÔºå‰ΩÜÂøΩÁï•‰∫ÜËæìÂÖ•ÈïøÂ∫¶‰∏éÊ®°ÂûãÊúâÊïà‰∏ä‰∏ãÊñáÁ™óÂè£‰πãÈó¥ÁöÑÂä®ÊÄÅÂÖ≥Á≥ª„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÈïøÂ∫¶ÊÑüÁü•Â§öÁ≤íÂ∫¶‰ΩçÁΩÆÁºñÁ†ÅÔºàLaMPEÔºâÔºåÂÖÖÂàÜÂà©Áî®Ê®°ÂûãÁöÑÊúâÊïà‰∏ä‰∏ãÊñáÁ™óÂè£ÔºåÂÆûÁé∞LLMsÁöÑËá™ÈÄÇÂ∫îÈïø‰∏ä‰∏ãÊñáÊâ©Â±ï„ÄÇLaMPEÈÄöËøáÂèÇÊï∞ÂåñÁöÑÁº©ÊîæsigmoidÂáΩÊï∞Âª∫Á´ãÊò†Â∞ÑÈïøÂ∫¶‰∏éËæìÂÖ•ÈïøÂ∫¶‰πãÈó¥ÁöÑÂä®ÊÄÅÂÖ≥Á≥ªÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ§öÁ≤íÂ∫¶Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊàòÁï•ÊÄßÂú∞Âú®‰∏çÂêåÂ∫èÂàóÂå∫ÂüüÂàÜÈÖç‰ΩçÁΩÆÂàÜËæ®ÁéáÔºå‰ª•ÊçïÊçâÁªÜÁ≤íÂ∫¶Â±ÄÈÉ®ÊÄßÂíåÈïøË∑ùÁ¶ª‰æùËµñ„ÄÇÂÆûÈ™åË°®ÊòéÔºåLaMPEÂú®‰∫î‰∏™‰∏ªÊµÅÈïø‰∏ä‰∏ãÊñáÂü∫ÂáÜ‰∏äÊòæËëóÊèêÂçá‰∫Ü‰∏âÁßç‰ª£Ë°®ÊÄßLLMsÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜË∂ÖÂá∫È¢ÑËÆ≠ÁªÉ‰∏ä‰∏ãÊñáÁ™óÂè£ÁöÑËæìÂÖ•Êó∂ÔºåÂõ†ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÂØºËá¥ÁöÑÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöËøáÂõ∫ÂÆöÊò†Â∞ÑÁ≠ñÁï•Â§ÑÁêÜÂàÜÂ∏ÉÂ§ñÔºàOODÔºâ‰ΩçÁΩÆÔºå‰ΩÜÊú™ËÉΩËÄÉËôëËæìÂÖ•ÈïøÂ∫¶‰∏éÊ®°ÂûãÊúâÊïà‰∏ä‰∏ãÊñáÁ™óÂè£‰πãÈó¥ÁöÑÂä®ÊÄÅÂÖ≥Á≥ª„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLaMPEÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂèÇÊï∞ÂåñÁöÑÁº©ÊîæsigmoidÂáΩÊï∞ÔºåÂª∫Á´ãËæìÂÖ•ÈïøÂ∫¶‰∏é‰ΩçÁΩÆÁºñÁ†Å‰πãÈó¥ÁöÑÂä®ÊÄÅÊò†Â∞ÑÂÖ≥Á≥ªÔºå‰ªéËÄåËá™ÈÄÇÂ∫îÂú∞ÂàÜÈÖç‰ΩçÁΩÆÁºñÁ†ÅÁöÑËÉΩÂäõÔºå‰ºòÂåñÈïøÊñáÊú¨ÁöÑÂ§ÑÁêÜ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLaMPEÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö‰∏ÄÊòØÈïøÂ∫¶ÊÑüÁü•ÁöÑÂ§öÁ≤íÂ∫¶‰ΩçÁΩÆÁºñÁ†ÅÔºå‰∫åÊòØÂ§öÁ≤íÂ∫¶Ê≥®ÊÑèÂäõÊú∫Âà∂„ÄÇÂâçËÄÖË¥üË¥£Âä®ÊÄÅË∞ÉÊï¥‰ΩçÁΩÆÁºñÁ†ÅÔºåÂêéËÄÖÂàôÂú®‰∏çÂêåÂ∫èÂàóÂå∫ÂüüÂàÜÈÖç‰ΩçÁΩÆÂàÜËæ®ÁéáÔºå‰ª•ÊçïÊçâÁªÜÁ≤íÂ∫¶ÂíåÈïøË∑ùÁ¶ª‰æùËµñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLaMPEÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ËÆ≠ÁªÉÊó†ÂÖ≥ÊÄßÂíåÂä®ÊÄÅÊò†Â∞ÑËÉΩÂäõÔºåËÉΩÂ§üÊ†πÊçÆËæìÂÖ•ÈïøÂ∫¶Ëá™ÈÄÇÂ∫îË∞ÉÊï¥‰ΩçÁΩÆÁºñÁ†ÅÔºåÂå∫Âà´‰∫é‰º†ÁªüÁöÑÂõ∫ÂÆöÊò†Â∞ÑÁ≠ñÁï•ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÂ§ÑÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåLaMPEÈááÁî®‰∫ÜÂèÇÊï∞ÂåñÁöÑsigmoidÂáΩÊï∞Êù•ÂÆûÁé∞Âä®ÊÄÅÊò†Â∞ÑÔºåÂπ∂ÁªìÂêàÂ§öÁ≤íÂ∫¶Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁ°Æ‰øùÂú®‰∏çÂêåÂ∫èÂàóÂå∫ÂüüÂÜÖÊúâÊïàÊçïÊçâÂ±ÄÈÉ®ÂíåÂÖ®Â±Ä‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂØπÊØîÂÆûÈ™å‰∏≠ÔºåLaMPEÂú®‰∫î‰∏™‰∏ªÊµÅÈïø‰∏ä‰∏ãÊñáÂü∫ÂáÜ‰∏äÁõ∏ËæÉ‰∫éÁé∞ÊúâÈïøÂ∫¶Â§ñÊé®ÊñπÊ≥ïÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÂæÖË°•ÂÖÖÔºâÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§ÑÁêÜÈïøÊñáÊú¨ËæìÂÖ•Êó∂ÁöÑÊúâÊïàÊÄßÂíå‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LaMPEÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÂ§ÑÁêÜÈïøÊñáÊú¨ÁöÑ‰ªªÂä°‰∏≠ÔºåÂ¶ÇÊñáÊ°£ÊëòË¶Å„ÄÅÈïøÁØáÂØπËØùÁîüÊàêÁ≠â„ÄÇÂÖ∂Êó†ËÆ≠ÁªÉÁöÑÁâπÊÄß‰ΩøÂæóËØ•ÊñπÊ≥ïËÉΩÂ§üÂø´ÈÄüÈõÜÊàêÂà∞Áé∞ÊúâÁöÑRoPEÂü∫Á°ÄÊ®°Âûã‰∏≠ÔºåÊèêÂçáÊ®°ÂûãÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÁÅµÊ¥ªÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) experience significant performance degradation when the input exceeds the pretraining context window, primarily due to the out-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent studies mitigate this problem by remapping OOD positions into the in-distribution range with fixed mapping strategies, ignoring the dynamic relationship between input length and the model's effective context window. To this end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a training-free method that fully utilizes the model's effective context window for adaptive long-context scaling in LLMs. Motivated by the left-skewed frequency distribution of relative positions, LaMPE establishes a dynamic relationship between mapping length and input length through a parametric scaled sigmoid function to adaptively allocate positional capacity across varying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention mechanism that strategically allocates positional resolution across different sequence regions to capture both fine-grained locality and long-range dependencies. Our method can be seamlessly applied to a wide range of RoPE-based LLMs without training. Extensive experiments on three representative LLMs across five mainstream long-context benchmarks demonstrate that LaMPE achieves significant performance improvements compared to existing length extrapolation methods. The code will be released at https://github.com/scar-on/LaMPE.

