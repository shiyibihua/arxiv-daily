---
layout: default
title: Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time
---

# Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02037" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02037v2</a>
  <a href="https://arxiv.org/pdf/2508.02037.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02037v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02037v2', 'Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huihan Li, You Chen, Siyuan Wang, Yixin He, Ninareh Mehrabi, Rahul Gupta, Xiang Ren

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-08-20)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTIMæ¡†æ¶ä»¥è¯Šæ–­é“¾å¼æ€ç»´æ¨ç†ä¸­çš„è®°å¿†åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é“¾å¼æ€ç»´` `è®°å¿†åŒ–` `æ¨ç†èƒ½åŠ›` `ç»Ÿè®¡åˆ†æ` `è‡ªç„¶è¯­è¨€å¤„ç†` `é”™è¯¯è¯Šæ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¾“å…¥å¾®å°å˜åŒ–æ—¶å¸¸å¸¸å‡ºç°é”™è¯¯ï¼Œè¡¨æ˜å…¶æ¨ç†èƒ½åŠ›å¯èƒ½ä¾èµ–äºè®°å¿†åŒ–ã€‚
2. æœ¬æ–‡æå‡ºSTIMæ¡†æ¶ï¼Œé€šè¿‡æºæ„ŸçŸ¥çš„æ–¹å¼å¯¹æ¨ç†é“¾ä¸­çš„æ¯ä¸ªæ ‡è®°è¿›è¡Œè®°å¿†åŒ–æ¥æºçš„è¯†åˆ«ï¼Œå¸®åŠ©åˆ†ææ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤æ‚ä»»åŠ¡ä¸­æ¨¡å‹å¯¹è®°å¿†åŒ–çš„ä¾èµ–æ€§å¢å¼ºï¼Œå±€éƒ¨è®°å¿†åŒ–å¯¼è‡´çš„é”™è¯¯å æ¯”é«˜è¾¾67%ï¼ŒSTIMæœ‰æ•ˆé¢„æµ‹é”™è¯¯æ ‡è®°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¾“å…¥ç¨æœ‰å˜åŒ–æ—¶å¸¸å¸¸å¤±è´¥ï¼Œè¿™å¼•å‘äº†å¯¹å…¶æˆåŠŸæ˜¯å¦ä¾èµ–äºè®°å¿†åŒ–çš„æ‹…å¿§ã€‚ç‰¹åˆ«æ˜¯åœ¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ä¸­ï¼Œè™šå‡çš„è®°å¿†æ¨¡å¼å¯èƒ½å¯¼è‡´ä¸­é—´é”™è¯¯ï¼Œè¿›è€Œå½±å“æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶STIMï¼Œç”¨äºæºæ„ŸçŸ¥çš„è®°å¿†åŒ–æ ‡è¯†ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸é¢„è®­ç»ƒè¯­æ–™åº“ä¸­æ ‡è®°çš„ç»Ÿè®¡å…±ç°å…³ç³»ï¼Œå°†æ¨ç†é“¾ä¸­çš„æ¯ä¸ªæ ‡è®°å½’å› äºå¤šä¸ªè®°å¿†æºã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹åœ¨å¤æ‚æˆ–é•¿å°¾æ¡ˆä¾‹ä¸­æ›´ä¾èµ–è®°å¿†åŒ–ï¼Œè€Œå±€éƒ¨è®°å¿†åŒ–é€šå¸¸æ˜¯é”™è¯¯çš„ä¸»è¦é©±åŠ¨å› ç´ ï¼Œå¯¼è‡´å¤šè¾¾67%çš„é”™è¯¯æ ‡è®°ã€‚STIMçš„è®°å¿†åŒ–è¯„åˆ†åœ¨é¢„æµ‹é”™è¯¯æ¨ç†æ­¥éª¤ä¸­çš„é”™è¯¯æ ‡è®°æ–¹é¢ä¹Ÿè¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é“¾å¼æ€ç»´æ¨ç†ä¸­å› è®°å¿†åŒ–å¯¼è‡´çš„é”™è¯¯é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œåˆ†æè®°å¿†åŒ–å¯¹æ¨ç†ç»“æœçš„å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨è¾“å…¥å˜åŒ–æ—¶çš„è¡¨ç°ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSTIMæ¡†æ¶é€šè¿‡åˆ†ææ¨ç†é“¾ä¸­æ¯ä¸ªæ ‡è®°çš„æ¥æºï¼Œè¯†åˆ«å…¶ä¸é¢„è®­ç»ƒè¯­æ–™åº“çš„ç»Ÿè®¡å…³ç³»ï¼Œä»è€Œåˆ¤æ–­è®°å¿†åŒ–çš„å½±å“ã€‚æ­¤è®¾è®¡æ—¨åœ¨æä¾›æ›´ç»†ç²’åº¦çš„é”™è¯¯åˆ†æï¼Œå¸®åŠ©æ”¹è¿›æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSTIMæ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ ‡è®°å½’å› ã€ç»Ÿè®¡åˆ†æå’Œç»“æœå¯è§†åŒ–ç­‰æ¨¡å—ã€‚é¦–å…ˆï¼Œå¯¹æ¨ç†é“¾è¿›è¡Œæ ‡è®°åˆ†æï¼Œç„¶åæ ¹æ®ç»Ÿè®¡å…±ç°å…³ç³»å½’å› äºä¸åŒçš„è®°å¿†æºï¼Œæœ€åè¾“å‡ºå¯è§†åŒ–ç»“æœä»¥ä¾¿äºç†è§£å’Œæ”¹è¿›ã€‚

**å…³é”®åˆ›æ–°**ï¼šSTIMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æºæ„ŸçŸ¥çš„è®°å¿†åŒ–æ ‡è¯†èƒ½åŠ›ï¼Œèƒ½å¤Ÿå°†æ¯ä¸ªæ ‡è®°çš„é”™è¯¯å½’å› äºå±€éƒ¨ã€ä¸­ç¨‹æˆ–è¿œç¨‹è®°å¿†æºï¼Œè¿™ä¸€æ–¹æ³•åœ¨ç°æœ‰æ–‡çŒ®ä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼ŒSTIMé‡‡ç”¨äº†ç‰¹å®šçš„ç»Ÿè®¡æ–¹æ³•æ¥è®¡ç®—æ ‡è®°çš„å…±ç°é¢‘ç‡ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSTIMèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å‡ºå¤šè¾¾67%çš„é”™è¯¯æ ‡è®°ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æˆ–é•¿å°¾ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å¯¹è®°å¿†åŒ–çš„ä¾èµ–æ€§æ˜¾è‘—å¢å¼ºã€‚STIMçš„è®°å¿†åŒ–è¯„åˆ†åœ¨é¢„æµ‹é”™è¯¯æ¨ç†æ­¥éª¤ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨ç†è¯Šæ–­ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ¨ç†ä»»åŠ¡ã€å¯¹è¯ç³»ç»Ÿå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æ”¹å–„æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒSTIMå¯ä»¥å¸®åŠ©å¼€å‘æ›´æ™ºèƒ½çš„AIåŠ©æ‰‹ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶çš„é€šç”¨æ€§ä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºå…¶ä»–ç»“æ„åŒ–ç”Ÿæˆä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) perform well on reasoning benchmarks but often fail when inputs alter slightly, raising concerns about the extent to which their success relies on memorization. This issue is especially acute in Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger intermediate errors that cascade into incorrect final answers. We introduce STIM, a novel framework for Source-aware Token-level Identification of Memorization, which attributes each token in a reasoning chain to one of multiple memorization sources - local, mid-range, or long-range - based on their statistical co-occurrence with the token in the pretraining corpus. Our token-level analysis across tasks and distributional settings reveals that models rely more on memorization in complex or long-tail cases, and that local memorization is often the dominant driver of errors, leading to up to 67% of wrong tokens. We also show that memorization scores from STIM can be effective in predicting the wrong tokens in the wrong reasoning step. STIM offers a powerful tool for diagnosing and improving model reasoning and can generalize to other structured step-wise generation tasks.

