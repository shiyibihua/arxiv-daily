---
layout: default
title: Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation
---

# Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02618" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02618v2</a>
  <a href="https://arxiv.org/pdf/2508.02618.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02618v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02618v2', 'Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianxiang Zang, Meiling Ning, Shihan Dou, Jiazheng Zhang, Tao Gui, Qi Zhang, Xuanjing Huang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-09-17)

**å¤‡æ³¨**: This paper is not suitable for this topic, we need to adjust the context

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº¤äº’è’¸é¦ä»¥è§£å†³åå¥½å¥–åŠ±å»ºæ¨¡ä¸­çš„æ³¨æ„åŠ›åŠ«æŒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±æ¨¡å‹` `åå¥½å»ºæ¨¡` `äº¤äº’è’¸é¦` `æ³¨æ„åŠ›æœºåˆ¶` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åå¥½å»ºæ¨¡æ–¹æ³•åœ¨ä»¤ç‰Œçº§äº¤äº’ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´åˆ¤æ–­ä¿¡å·å®¹æ˜“å—åˆ°é”™è¯¯æ³¨æ„åŠ›çš„å½±å“ã€‚
2. æœ¬æ–‡æå‡ºçš„â€œäº¤äº’è’¸é¦â€æ¡†æ¶é€šè¿‡å¼•å…¥åŸºäºäº¤äº’çš„æ•™å¸ˆæ¨¡å‹ï¼Œä¼˜åŒ–æ³¨æ„åŠ›çº§åˆ«ä»¥æ”¹å–„åå¥½å»ºæ¨¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œäº¤äº’è’¸é¦åœ¨å¥–åŠ±ä¿¡å·çš„ç¨³å®šæ€§å’Œå¯æ³›åŒ–æ€§ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›RMä¼˜åŒ–æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä¸ºç”Ÿæˆçš„å“åº”æä¾›å¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åå¥½å»ºæ¨¡åœ¨ä»¤ç‰Œçº§äº¤äº’æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä½¿å¾—å…¶åˆ¤æ–­ä¿¡å·å®¹æ˜“å—åˆ°ä¸Šä¸‹æ–‡ä¸­é”™è¯¯æ³¨æ„åŠ›çš„å½±å“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒæ¡†æ¶â€œäº¤äº’è’¸é¦â€ï¼Œé€šè¿‡æ³¨æ„åŠ›çº§ä¼˜åŒ–æ¥å®ç°æ›´å……åˆ†çš„åå¥½å»ºæ¨¡ã€‚è¯¥æ–¹æ³•å¼•å…¥åŸºäºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œé€šè¿‡å…¨é¢çš„æ³¨æ„åŠ›æä¾›å¤æ‚çš„ä»¤ç‰Œäº¤äº’æ¨¡å¼ï¼Œå¹¶å¼•å¯¼åå¥½å»ºæ¨¡æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹çš„äº¤äº’æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼Œäº¤äº’è’¸é¦åœ¨æä¾›æ›´ç¨³å®šå’Œå¯æ³›åŒ–çš„å¥–åŠ±ä¿¡å·æ–¹é¢ä¼˜äºç°æœ‰çš„RMä¼˜åŒ–æ–¹æ³•ï¼Œçªæ˜¾äº†æ³¨æ„åŠ›åŠ«æŒåœ¨RMä¸­çš„æ ¹æœ¬æ€§é™åˆ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰å¥–åŠ±æ¨¡å‹åœ¨åå¥½å»ºæ¨¡ä¸­ç”±äºæ³¨æ„åŠ›æœºåˆ¶ä¸è¶³è€Œå¯¼è‡´çš„ä¿¡å·è„†å¼±é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä»¤ç‰Œçº§äº¤äº’çš„ç¼ºå¤±ä½¿å¾—åˆ¤æ–­ä¿¡å·æ˜“å—å¹²æ‰°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºâ€œäº¤äº’è’¸é¦â€æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªåŸºäºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œåˆ©ç”¨å…¶å¤æ‚çš„æ³¨æ„åŠ›æ¨¡å¼æ¥æŒ‡å¯¼åå¥½å»ºæ¨¡ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„åˆ¤æ–­èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ï¼Œæ•™å¸ˆæ¨¡å‹è´Ÿè´£ç”Ÿæˆå¤æ‚çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡æ³¨æ„åŠ›å¯¹é½ç›®æ ‡æ¥æ¨¡æ‹Ÿæ•™å¸ˆçš„äº¤äº’æ¨¡å¼ï¼Œæ•´ä¸ªè¿‡ç¨‹é€šè¿‡è’¸é¦è®­ç»ƒå®ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†äº¤äº’è’¸é¦çš„æ¦‚å¿µï¼Œé€šè¿‡ä¼˜åŒ–æ³¨æ„åŠ›æœºåˆ¶æ¥è§£å†³ä¼ ç»Ÿåå¥½å»ºæ¨¡ä¸­çš„æ³¨æ„åŠ›åŠ«æŒé—®é¢˜ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ç‹¬ç«‹ç¼–ç æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ³¨æ„åŠ›å¯¹é½æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å¼ï¼ŒåŒæ—¶åœ¨å‚æ•°è®¾ç½®ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œäº¤äº’è’¸é¦æ–¹æ³•åœ¨å¥–åŠ±ä¿¡å·çš„ç¨³å®šæ€§å’Œå¯æ³›åŒ–æ€§ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„RMä¼˜åŒ–æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æå‡äº†å¥–åŠ±ä¿¡å·çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä»äººç±»åé¦ˆä¸­å­¦ä¹ çš„åœºæ™¯ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€å†…å®¹ç”Ÿæˆå’Œä¸ªæ€§åŒ–æ¨èç­‰ã€‚é€šè¿‡æä¾›æ›´ç¨³å®šçš„å¥–åŠ±ä¿¡å·ï¼Œäº¤äº’è’¸é¦æœ‰åŠ©äºæå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨å¤šä¸ªAIåº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this "attention hacking", we propose "Interaction Distillation", a novel training framework for more adequate preference modeling through attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the preference modeling to simulate teacher model's interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in RM.

