---
layout: default
title: Learning Dynamics of Meta-Learning in Small Model Pretraining
---

# Learning Dynamics of Meta-Learning in Small Model Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02189" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02189v2</a>
  <a href="https://arxiv.org/pdf/2508.02189.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02189v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02189v2', 'Learning Dynamics of Meta-Learning in Small Model Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: David Demitri Africa, Yuval Weiss, Paula Buttery, Richard Diehl Martinez

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-11-07)

**å¤‡æ³¨**: Accepted (oral) to Student Research Workshop at IJCNLP-AACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå…ƒå­¦ä¹ åŠ¨æ€ä»¥ä¼˜åŒ–å°æ¨¡å‹é¢„è®­ç»ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…ƒå­¦ä¹ ` `å°æ¨¡å‹` `é¢„è®­ç»ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¤šè¯­è¨€è¯†åˆ«` `å¯è§£é‡Šæ€§` `è®­ç»ƒåŠ¨æ€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶æ€§èƒ½ä¼˜è¶Šï¼Œä½†è®­ç»ƒæˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶åœ¨å°å‹æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºå°†ä¸€é˜¶MAMLä¸å­é›†æ©è”½è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒç›¸ç»“åˆï¼Œæ—¨åœ¨æå‡å°æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨æŸå¤±ä¸Šæœ€å¤šæå‰1.6å€è¾¾åˆ°ç›¸åŒæ°´å¹³ï¼Œå¹¶åœ¨å¤šè¯­è¨€NERä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„F1æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶å¼ºå¤§ï¼Œä½†è®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æ¢è®¨å…ƒå­¦ä¹ æ˜¯å¦èƒ½ä½¿å°å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒä¸ä»…æ›´ä¼˜ï¼Œè¿˜æ›´å…·å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬å°†ä¸€é˜¶MAMLä¸å­é›†æ©è”½è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒç›¸ç»“åˆï¼Œæ„å»ºäº†å››ä¸ªLLamaé£æ ¼çš„è§£ç å™¨æ¨¡å‹ï¼ˆå‚æ•°é‡ä»11Måˆ°570Mï¼‰ï¼Œå¹¶åœ¨å¤šä¸ªè®¾ç½®å’Œå®é™…åº”ç”¨ä¸­è¯„ä¼°å…¶åœ¨åŸºç¡€NLPä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿè®­ç»ƒç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æŸå¤±ä¸Šæœ€å¤šæå‰1.6å€è¾¾åˆ°ç›¸åŒæ°´å¹³ï¼Œä¸”åœ¨ç›¸åŒè®¡ç®—æ¡ä»¶ä¸‹æå‡äº†å¤šè¯­è¨€é€šç”¨å‘½åå®ä½“è¯†åˆ«çš„F1åˆ†æ•°ï¼ŒåŒæ—¶è®­ç»ƒåŠ¨æ€æ›´æ˜“äºç†è§£ï¼Œè¡¨ç°ä¸ºç½‘ç»œè¡¨ç¤ºçš„å¤šæ ·åŒ–å’Œåç»­çš„å‹ç¼©è¿‡ç¨‹ã€‚è¿™ç§ä¸¤é˜¶æ®µçš„å˜åŒ–åœ¨æœ‰æ•ˆç§©æ›²çº¿å’Œæ³¨æ„åŠ›å¤´ç†µä¸­å‡æœ‰ä½“ç°ï¼Œæ¸…æ™°æ ‡è¯†å‡ºå„å±‚çš„ä¸“ä¸šåŒ–å’Œå†æ”¶æ•›è¿‡ç¨‹ã€‚ä»£ç ã€æ£€æŸ¥ç‚¹å’ŒWandBæ—¥å¿—å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ•ˆç‡å’Œå¯è§£é‡Šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸è®­ç»ƒæˆæœ¬é«˜ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ¨å¹¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†ä¸€é˜¶MAMLä¸å­é›†æ©è”½è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒç›¸ç»“åˆï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨åŠ é€Ÿæ¨¡å‹æ”¶æ•›å¹¶æé«˜å…¶åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯ç½‘ç»œè¡¨ç¤ºçš„å¤šæ ·åŒ–é˜¶æ®µï¼Œå…¶æ¬¡æ˜¯å‹ç¼©é˜¶æ®µã€‚æ¨¡å‹åœ¨è¿™ä¸¤ä¸ªé˜¶æ®µä¸­è¡¨ç°å‡ºä¸åŒçš„åŠ¨æ€ç‰¹å¾ï¼Œä¾¿äºç†è§£å’Œåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å…ƒå­¦ä¹ åŠ¨æ€å¼•å…¥å°æ¨¡å‹çš„é¢„è®­ç»ƒä¸­ï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹ä¸ä»…æ›´å¿«ï¼Œè€Œä¸”æ›´å…·å¯è§£é‡Šæ€§ã€‚è¿™ä¸ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ‰æ•ˆç§©æ›²çº¿å’Œæ³¨æ„åŠ›å¤´ç†µï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡ä¸Šèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œä¸“ä¸šåŒ–å’Œå†æ”¶æ•›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨æŸå¤±ä¸Šæœ€å¤šæå‰1.6å€è¾¾åˆ°ç›¸åŒæ°´å¹³ï¼Œå¹¶åœ¨å¤šè¯­è¨€é€šç”¨å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œåœ¨ç›¸åŒè®¡ç®—æ¡ä»¶ä¸‹å®ç°äº†F1åˆ†æ•°çš„æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¤šè¯­è¨€ä¿¡æ¯æå–ç­‰ã€‚é€šè¿‡æå‡å°å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œå¯è§£é‡Šæ€§ï¼Œç ”ç©¶æˆæœå¯ä»¥ä¿ƒè¿›å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models are powerful but costly. We ask whether meta-learning can make the pretraining of small language models not only better but also more interpretable. We integrate first-order MAML with subset-masked LM pretraining, producing four LLama-style decoder-only models (11M-570M params), and evaluate it on a fundamental NLP task with many settings and real-world applications. Compared with vanilla training, our model (i) reaches the same loss up to 1.6x sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and (iii) makes the training dynamics easy to read: first the network's representations fan out ("diversify") and later they collapse into a smaller, shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall in both effective-rank curves and attention-head entropy. The same curves pinpoint which layers specialise earliest and which later reconverge, giving a compact, interpretable signature of meta-adaptation. Code, checkpoints and WandB logs are released.

