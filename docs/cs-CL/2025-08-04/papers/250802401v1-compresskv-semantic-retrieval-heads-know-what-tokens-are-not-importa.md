---
layout: default
title: CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation
---

# CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02401" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02401v1</a>
  <a href="https://arxiv.org/pdf/2508.02401.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02401v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02401v1', 'CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TUDa-HWAI/CompressKV.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCompressKVä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„KVç¼“å­˜æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡å¤„ç†` `é”®å€¼ç¼“å­˜` `æ³¨æ„åŠ›æœºåˆ¶` `å†…å­˜ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹æ€§èƒ½æå‡` `å±‚è‡ªé€‚åº”ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•æœªèƒ½æœ‰æ•ˆåŒºåˆ†æ³¨æ„åŠ›å¤´çš„åŠŸèƒ½ï¼Œå¯¼è‡´é‡è¦æ ‡è®°è¢«é”™è¯¯é©±é€ï¼Œå½±å“LLMsæ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è¯†åˆ«æ¯å±‚ä¸­èƒ½å¤Ÿæ£€ç´¢é‡è¦æ ‡è®°çš„æ³¨æ„åŠ›å¤´ï¼Œä¼˜åŒ–KVç¼“å­˜çš„ä¿ç•™ç­–ç•¥ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCompressKVåœ¨LongBenchå’ŒNeedle-in-a-HaystackåŸºå‡†æµ‹è¯•ä¸­ï¼Œå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†å†…å­˜åˆ©ç”¨ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œéšç€é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°çš„å¢åŠ ï¼Œå†…å­˜å’Œæ‰§è¡Œæ•ˆç‡é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•ä¾èµ–äºä½¿ç”¨Grouped Query Attentionï¼ˆGQAï¼‰ä¸­çš„æ‰€æœ‰æ³¨æ„åŠ›å¤´è¿›è¡Œå¯å‘å¼çš„æ ‡è®°é©±é€ï¼Œè¿™ç§æ–¹æ³•å¿½è§†äº†æ³¨æ„åŠ›å¤´çš„ä¸åŒåŠŸèƒ½ï¼Œå¯¼è‡´å…³é”®æ ‡è®°è¢«é©±é€ï¼Œä»è€Œé™ä½äº†LLMsçš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•CompressKVï¼Œé€šè¿‡è¯†åˆ«æ¯å±‚ä¸­èƒ½å¤Ÿæ£€ç´¢é‡è¦æ ‡è®°çš„æ³¨æ„åŠ›å¤´ï¼Œæ¥ä¼˜åŒ–KVç¼“å­˜çš„ä¿ç•™ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCompressKVåœ¨ä¸åŒå†…å­˜é¢„ç®—ä¸‹çš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„KVç¼“å­˜æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ‰€æœ‰æ³¨æ„åŠ›å¤´è¿›è¡Œæ ‡è®°é©±é€ï¼Œå¯¼è‡´é‡è¦æ ‡è®°çš„ä¸¢å¤±ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è¯†åˆ«æ¯å±‚ä¸­èƒ½å¤Ÿæœ‰æ•ˆæ£€ç´¢é‡è¦æ ‡è®°çš„æ³¨æ„åŠ›å¤´ï¼Œè€Œéç®€å•ä¾èµ–æ‰€æœ‰å¤´ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´ç²¾å‡†åœ°ä¿ç•™å…³é”®æ ‡è®°ï¼Œæé«˜æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œè¯†åˆ«æ¯å±‚ä¸­é‡è¦çš„æ³¨æ„åŠ›å¤´ï¼›å…¶æ¬¡ï¼ŒåŸºäºè¿™äº›å¤´ç¡®å®šéœ€è¦ä¿ç•™çš„KVç¼“å­˜å¯¹ï¼›æœ€åï¼Œå®æ–½å±‚è‡ªé€‚åº”çš„KVç¼“å­˜åˆ†é…ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡åˆ†ææ¯å±‚çš„ç¼“å­˜é©±é€é”™è¯¯ï¼Œæå‡ºäº†å±‚è‡ªé€‚åº”çš„KVç¼“å­˜åˆ†é…ç­–ç•¥ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å…¨å±€é©±é€ç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¿ç•™é‡è¦ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæœ¬æ–‡å¯¹æ¯å±‚çš„æ³¨æ„åŠ›å¤´è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œç¡®å®šäº†å…¶åœ¨æ£€ç´¢åˆå§‹å’Œæœ€ç»ˆæ ‡è®°åŠå…¶è¯­ä¹‰ä¸Šä¸‹æ–‡ä¸­çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨äº†æ–°çš„KVç¼“å­˜åˆ†é…ç­–ç•¥ï¼Œä»¥é€‚åº”ä¸åŒå±‚çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCompressKVåœ¨LongBenchå’ŒNeedle-in-a-HaystackåŸºå‡†æµ‹è¯•ä¸­ï¼Œå‡åœ¨ä¸åŒå†…å­˜é¢„ç®—ä¸‹è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨KVç¼“å­˜ä¼˜åŒ–æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„æ•ˆç‡ï¼ŒCompressKVèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡æ¨¡å‹çš„å“åº”é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å•†ä¸šä»·å€¼å’Œç ”ç©¶æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.
>   To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.

