---
layout: default
title: VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo
---

# VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02317" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02317v3</a>
  <a href="https://arxiv.org/pdf/2508.02317.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02317v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02317v3', 'VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu

**åˆ†ç±»**: cs.CL, cs.AI, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-08-07)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVeOmniä»¥è§£å†³å¤šæ¨¡æ€å¤§æ¨¡å‹è®­ç»ƒæ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§æ¨¡å‹è®­ç»ƒ` `åˆ†å¸ƒå¼è®¡ç®—` `æ¨¡å‹ä¸­å¿ƒè®¾è®¡` `é«˜æ•ˆå¹¶è¡Œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§æ¨¡å‹è®­ç»ƒæ–¹æ³•åœ¨å¯æ‰©å±•æ€§å’Œå·¥ç¨‹æ•ˆç‡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å¤§è§„æ¨¡è®­ç»ƒçš„éœ€æ±‚ã€‚
2. VeOmnié€šè¿‡æ¨¡å—åŒ–è®¾è®¡å’Œæ¨¡å‹ä¸­å¿ƒçš„åˆ†å¸ƒå¼é…æ–¹ï¼Œå°†é€šä¿¡ä¸è®¡ç®—è§£è€¦ï¼Œå®ç°äº†é«˜æ•ˆçš„3Då¹¶è¡Œè®­ç»ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVeOmnièƒ½å¤Ÿä»¥è¶…è¿‡2800ä¸ªtoken/ç§’çš„é€Ÿåº¦è®­ç»ƒ300äº¿å‚æ•°çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¹¶æ”¯æŒæ‰©å±•åˆ°16ä¸‡çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•æ¨åŠ¨äº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºå¤„ç†å¤šæ ·åŒ–æ¨¡æ€æ‰€éœ€çš„å¼‚æ„æ¨¡å‹æ¶æ„ï¼Œè®­ç»ƒå¤šæ¨¡æ€LLMsä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ¡†æ¶é€šå¸¸å°†æ¨¡å‹å®šä¹‰ä¸å¹¶è¡Œé€»è¾‘çº ç¼ åœ¨ä¸€èµ·ï¼Œå¯¼è‡´å¯æ‰©å±•æ€§æœ‰é™å’Œå·¥ç¨‹å¼€é”€å·¨å¤§ã€‚æœ¬æ–‡æå‡ºäº†VeOmniï¼Œä¸€ä¸ªæ¨¡å—åŒ–å’Œé«˜æ•ˆçš„è®­ç»ƒæ¡†æ¶ï¼Œä»¥åŠ é€Ÿå¤šæ¨¡æ€LLMsçš„å¼€å‘ã€‚VeOmniå¼•å…¥äº†æ¨¡å‹ä¸­å¿ƒçš„åˆ†å¸ƒå¼é…æ–¹ï¼Œå°†é€šä¿¡ä¸è®¡ç®—è§£è€¦ï¼Œå®ç°äº†å¤šæ¨¡æ€LLMsçš„é«˜æ•ˆ3Då¹¶è¡Œã€‚VeOmniè¿˜å…·æœ‰çµæ´»çš„é…ç½®æ¥å£ï¼Œæ”¯æŒä»¥æœ€å°çš„ä»£ç æ›´æ”¹æ— ç¼é›†æˆæ–°æ¨¡æ€ã€‚ä½¿ç”¨VeOmniï¼Œä¸€ä¸ªå…·æœ‰300äº¿å‚æ•°çš„å¤šæ¨¡æ€ä¸“å®¶æ¨¡å‹å¯ä»¥ä»¥æ¯GPUè¶…è¿‡2800ä¸ªtoken/ç§’çš„ååé‡è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡128ä¸ªGPUæ‰©å±•åˆ°16ä¸‡çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå±•ç¤ºäº†å…¶åœ¨è®­ç»ƒå¤§å‹å¤šæ¨¡æ€LLMsæ–¹é¢çš„å“è¶Šæ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§æ¨¡å‹è®­ç»ƒä¸­çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†æ¨¡å‹å®šä¹‰ä¸å¹¶è¡Œé€»è¾‘æ··åˆï¼Œå¯¼è‡´å·¥ç¨‹å¼€é”€å¤§ä¸”éš¾ä»¥æ‰©å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVeOmniçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¨¡å—åŒ–è®¾è®¡å’Œæ¨¡å‹ä¸­å¿ƒçš„åˆ†å¸ƒå¼é…æ–¹ï¼Œå°†é€šä¿¡ä¸è®¡ç®—è§£è€¦ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„3Då¹¶è¡Œè®­ç»ƒã€‚è¿™ç§è®¾è®¡ä½¿å¾—ä¸åŒæ¨¡æ€çš„é›†æˆå˜å¾—æ›´åŠ çµæ´»å’Œé«˜æ•ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVeOmniçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œä¸»è¦åŒ…æ‹¬æ¨¡å‹å®šä¹‰æ¨¡å—ã€é€šä¿¡æ¨¡å—å’Œè®¡ç®—æ¨¡å—ã€‚é€šè¿‡å°†è¿™äº›æ¨¡å—åˆ†å¼€ï¼ŒVeOmnièƒ½å¤Ÿåœ¨ä¸åŒçš„ç¡¬ä»¶èµ„æºä¸Šå®ç°é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šVeOmniçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ¨¡å‹ä¸­å¿ƒçš„åˆ†å¸ƒå¼é…æ–¹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£è€¦é€šä¿¡ä¸è®¡ç®—ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„ç´§è€¦åˆè®¾è®¡å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨VeOmniä¸­ï¼Œå…³é”®çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡ä½¿å¾—åœ¨ä¸åŒGPUä¹‹é—´çš„è´Ÿè½½å‡è¡¡å¾—ä»¥ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œçµæ´»çš„é…ç½®æ¥å£æ”¯æŒå¿«é€Ÿé›†æˆæ–°æ¨¡æ€ï¼Œå‡å°‘äº†ä»£ç æ›´æ”¹çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVeOmnièƒ½å¤Ÿä»¥è¶…è¿‡2800ä¸ªtoken/ç§’çš„é€Ÿåº¦è®­ç»ƒä¸€ä¸ª300äº¿å‚æ•°çš„å¤šæ¨¡æ€ä¸“å®¶æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨128ä¸ªGPUä¸Šæ‰©å±•åˆ°16ä¸‡çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚è¿™ä¸€æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„è®­ç»ƒæ¡†æ¶ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§è§„æ¨¡å¤šæ¨¡æ€è®­ç»ƒä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VeOmniçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€äº¤äº’ç­‰ã€‚å…¶é«˜æ•ˆçš„è®­ç»ƒæ¡†æ¶èƒ½å¤ŸåŠ é€Ÿå¤šæ¨¡æ€æ¨¡å‹çš„å¼€å‘ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°æå‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. We present VeOmni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. VeOmni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. Using VeOmni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.

