---
layout: default
title: The SMeL Test: A simple benchmark for media literacy in language models
---

# The SMeL Test: A simple benchmark for media literacy in language models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02074" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02074v2</a>
  <a href="https://arxiv.org/pdf/2508.02074.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02074v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02074v2', 'The SMeL Test: A simple benchmark for media literacy in language models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gustaf Ahdritz, Anat Kleiman

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-08-07)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSMeLæµ‹è¯•ä»¥è¯„ä¼°è¯­è¨€æ¨¡å‹çš„åª’ä½“ç´ å…»èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åª’ä½“ç´ å…»` `è¯­è¨€æ¨¡å‹` `ä¿¡æ¯è¿‡æ»¤` `åˆæˆæµ‹è¯•` `å¹»è§‰ç‡` `åŸºå‡†è¯„ä¼°` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰è¯­è¨€æ¨¡å‹åœ¨è¿‡æ»¤ä¸å¯ä¿¡ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›å°šä¸æ˜ç¡®ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè§£å†³è¿™ä¸€é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä¿¡æ¯ç¯å¢ƒä¸­çš„è¿‡æ»¤èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰æµ‹è¯•çš„æ¨¡å‹éƒ½æœªèƒ½æŒç»­æˆåŠŸï¼Œæœ€ä½³æ¨¡å‹çš„å¹»è§‰ç‡é«˜è¾¾70%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº’è”ç½‘å……æ–¥ç€æœªæ ‡æ³¨ã€æ•…æ„è¯¯å¯¼æˆ–ä¸å¯ä¿¡çš„ä¿¡æ¯ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸è¢«ç”¨äºè‡ªä¸»ç½‘ç»œæµè§ˆï¼Œä½†å®ƒä»¬åœ¨è¿‡æ»¤ä¸å¯ä¿¡ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡æå‡ºäº†åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„åŸºå‡†ï¼Œæ—¨åœ¨æµ‹è¯•è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­ä¸»åŠ¨è¿‡æ»¤ä¸å¯ä¿¡ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å¤šç§å¸¸ç”¨çš„æŒ‡ä»¤è°ƒä¼˜LLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°æ²¡æœ‰æ¨¡å‹èƒ½å¤ŸæŒç»­æˆåŠŸï¼›å°½ç®¡æ¨ç†èƒ½åŠ›ä¸æ›´é«˜çš„å¾—åˆ†ç›¸å…³ï¼Œä½†å³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„APIæ¨¡å‹ä¹Ÿæœ‰é«˜è¾¾70%çš„å¹»è§‰ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¾ƒå¤§ä¸”æ›´å¼ºå¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šä¼˜äºè¾ƒå°çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¸ºè¿™ä¸€é‡è¦çš„å¹»è§‰å½¢å¼æä¾›æ›´å¤šè§è§£ï¼Œå¹¶æŒ‡å¯¼æ–°æ–¹æ³•çš„å¼€å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ä¸å¯ä¿¡ä¿¡æ¯æ—¶çš„è¿‡æ»¤èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯„ä¼°å’Œæå‡è¿™ä¸€èƒ½åŠ›ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½äº§ç”Ÿè¯¯å¯¼æ€§ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„SMeLæµ‹è¯•é€šè¿‡è®¾è®¡ä¸€ç³»åˆ—åŸºå‡†ä»»åŠ¡ï¼Œè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä¿¡æ¯ç¯å¢ƒä¸­çš„åª’ä½“ç´ å…»èƒ½åŠ›ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹çš„å±€é™æ€§å¹¶æ¨åŠ¨æ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSMeLæµ‹è¯•åŒ…æ‹¬å¤šä¸ªé˜¶æ®µï¼Œé¦–å…ˆç”Ÿæˆåˆæˆçš„åª’ä½“å†…å®¹ï¼Œç„¶åé€šè¿‡ä¸åŒçš„è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œæœ€åå¯¹æ¨¡å‹çš„è¡¨ç°è¿›è¡Œé‡åŒ–åˆ†æã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬å†…å®¹ç”Ÿæˆã€æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šSMeLæµ‹è¯•çš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶é’ˆå¯¹åª’ä½“ç´ å…»çš„ä¸“é—¨è®¾è®¡ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ç©ºç™½ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹åœ¨å¤„ç†ä¸å¯ä¿¡ä¿¡æ¯æ—¶çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šæµ‹è¯•ä¸­é‡‡ç”¨äº†å¤šç§å†…å®¹ç”Ÿæˆç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„ä¿¡æ¯å…·æœ‰å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚åŒæ—¶ï¼Œè¯„ä¼°è¿‡ç¨‹ä¸­è®¾ç½®äº†æ˜ç¡®çš„è¯„åˆ†æ ‡å‡†ï¼Œä»¥é‡åŒ–æ¨¡å‹çš„è¿‡æ»¤èƒ½åŠ›å’Œå¹»è§‰ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰æµ‹è¯•çš„è¯­è¨€æ¨¡å‹åœ¨è¿‡æ»¤ä¸å¯ä¿¡ä¿¡æ¯æ–¹é¢å‡æœªèƒ½æŒç»­æˆåŠŸï¼Œæœ€ä½³æ¨¡å‹çš„å¹»è§‰ç‡é«˜è¾¾70%ã€‚å°½ç®¡æ¨ç†èƒ½åŠ›ä¸å¾—åˆ†ç›¸å…³ï¼Œä½†æ›´å¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šè¡¨ç°æ›´å¥½ï¼Œè¿™ä¸€å‘ç°ä¸ºæ¨¡å‹è®¾è®¡æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ä¿¡æ¯æ£€ç´¢å’Œç¤¾äº¤åª’ä½“ç­‰ã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹çš„åª’ä½“ç´ å…»èƒ½åŠ›ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æ›´æœ‰æ•ˆåœ°è¯†åˆ«å’Œè¿‡æ»¤ä¸å¯ä¿¡ä¿¡æ¯ï¼Œä»è€Œæé«˜ä¿¡æ¯æ¶ˆè´¹çš„è´¨é‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æµ‹è¯•è¿˜å¯èƒ½æ¨åŠ¨æ–°ä¸€ä»£è¯­è¨€æ¨¡å‹çš„å¼€å‘ï¼Œä½¿å…¶åœ¨å¤æ‚ä¿¡æ¯ç¯å¢ƒä¸­è¡¨ç°æ›´ä½³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently succeeds; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it.

