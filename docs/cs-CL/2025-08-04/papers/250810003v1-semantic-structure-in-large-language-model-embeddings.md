---
layout: default
title: Semantic Structure in Large Language Model Embeddings
---

# Semantic Structure in Large Language Model Embeddings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10003" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10003v1</a>
  <a href="https://arxiv.org/pdf/2508.10003.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10003v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10003v1', 'Semantic Structure in Large Language Model Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Austin C. Kozlowski, Callin Dai, Andrei Boutyline

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åµŒå…¥ä¸­çš„è¯­ä¹‰ç»“æ„ä»¥ä¼˜åŒ–ç‰¹å¾å¼•å¯¼**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯­ä¹‰ç»“æ„` `åµŒå…¥è¡¨ç¤º` `åä¹‰è¯å¯¹` `ç‰¹å¾å¼•å¯¼` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¿ƒç†å­¦ç ”ç©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚è¯­ä¹‰æ—¶ï¼Œå¾€å¾€éš¾ä»¥æœ‰æ•ˆæ•æ‰ä½ç»´ç»“æ„ï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±å’Œè¯¯å¯¼æ€§ç»“æœã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡åˆ†æåä¹‰è¯å¯¹çš„è¯­ä¹‰æ–¹å‘ï¼Œæ­ç¤ºLLMsåµŒå…¥ä¸­çš„ä½ç»´è¯­ä¹‰ç»“æ„ï¼Œæä¾›äº†ä¸€ç§æ–°çš„ç†è§£æ–¹å¼ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯æ±‡çš„è¯­ä¹‰æŠ•å½±ä¸äººç±»è¯„åˆ†é«˜åº¦ç›¸å…³ï¼Œå¹¶ä¸”æœ‰æ•ˆç®€åŒ–ä¸ºä¸‰ç»´å­ç©ºé—´ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œåº”ç”¨ä»·å€¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¿ƒç†å­¦ç ”ç©¶å‘ç°ï¼Œäººç±»å¯¹è¯æ±‡çš„è¯­ä¹‰è¯„åˆ†å¯ä»¥åœ¨ä¿¡æ¯æŸå¤±è¾ƒå°çš„æƒ…å†µä¸‹ç®€åŒ–ä¸ºä½ç»´å½¢å¼ã€‚æœ¬æ–‡å‘ç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åµŒå…¥çŸ©é˜µæ‰€ç¼–ç çš„è¯­ä¹‰å…³è”ä¹Ÿå±•ç°å‡ºç±»ä¼¼çš„ç»“æ„ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯æ±‡åœ¨ç”±åä¹‰è¯å¯¹å®šä¹‰çš„è¯­ä¹‰æ–¹å‘ä¸Šçš„æŠ•å½±ä¸äººç±»è¯„åˆ†é«˜åº¦ç›¸å…³ï¼Œå¹¶ä¸”è¿™äº›æŠ•å½±æœ‰æ•ˆåœ°ç®€åŒ–ä¸ºLLMåµŒå…¥ä¸­çš„ä¸‰ç»´å­ç©ºé—´ï¼Œç±»ä¼¼äºäººç±»è°ƒæŸ¥å“åº”çš„æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæ²¿æŸä¸€è¯­ä¹‰æ–¹å‘ç§»åŠ¨æ ‡è®°ä¼šå¯¹å‡ ä½•å¯¹é½ç‰¹å¾äº§ç”Ÿä¸ä½™å¼¦ç›¸ä¼¼åº¦æˆæ¯”ä¾‹çš„æ„å¤–å½±å“ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒLLMsä¸­çš„è¯­ä¹‰ç‰¹å¾ä¸äººç±»è¯­è¨€ä¸­çš„ç›¸äº’å…³è”æ€§ç›¸ä¼¼ï¼Œå°½ç®¡è¡¨é¢å¤æ‚ï¼Œè¯­ä¹‰ä¿¡æ¯å´å‡ºå¥‡åœ°ä½ç»´ã€‚ç†è§£è¿™ç§è¯­ä¹‰ç»“æ„å¯¹äºé¿å…åœ¨ç‰¹å¾å¼•å¯¼æ—¶äº§ç”Ÿæ„å¤–åæœè‡³å…³é‡è¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­ä¹‰è¡¨ç¤ºä¸­çš„ä½ç»´ç»“æ„æ•æ‰ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è¯­ä¹‰å…³è”æ€§ï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±å’Œè¯¯å¯¼æ€§ç»“æœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†æåä¹‰è¯å¯¹å®šä¹‰çš„è¯­ä¹‰æ–¹å‘ï¼Œæœ¬æ–‡æ­ç¤ºäº†LLMsåµŒå…¥ä¸­çš„ä½ç»´è¯­ä¹‰ç»“æ„ï¼Œè¡¨æ˜è¯­ä¹‰ç‰¹å¾åœ¨æ¨¡å‹ä¸­æ˜¯ç›¸äº’äº¤ç»‡çš„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŸºäºåä¹‰è¯å¯¹çš„æŠ•å½±æ¨¡å‹ï¼Œç„¶åé€šè¿‡å¯¹æ¯”äººç±»è¯„åˆ†å’Œæ¨¡å‹è¾“å‡ºï¼ŒéªŒè¯äº†ä½ç»´ç»“æ„çš„æœ‰æ•ˆæ€§ï¼Œæœ€ç»ˆæå‡ºäº†ç‰¹å¾å¼•å¯¼çš„ä¼˜åŒ–æ–¹æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå‘ç°LLMsåµŒå…¥ä¸­çš„è¯­ä¹‰æŠ•å½±å¯ä»¥æœ‰æ•ˆç®€åŒ–ä¸ºä¸‰ç»´å­ç©ºé—´ï¼Œè¿™ä¸€å‘ç°ä¸ä¼ ç»Ÿçš„é«˜ç»´è¡¨ç¤ºæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„ç‰¹å¾å¼•å¯¼æ–¹æ³•ï¼Œå¹¶å¯¹æŠ•å½±æ–¹å‘è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨è¯­ä¹‰å¼•å¯¼æ—¶çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯æ±‡åœ¨è¯­ä¹‰æ–¹å‘ä¸Šçš„æŠ•å½±ä¸äººç±»è¯„åˆ†çš„ç›¸å…³æ€§è¶…è¿‡90%ï¼Œå¹¶ä¸”æœ‰æ•ˆç®€åŒ–ä¸ºä¸‰ç»´å­ç©ºé—´ã€‚è¿™ä¸€å‘ç°æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå±•ç¤ºäº†ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•çš„æ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æƒ…æ„Ÿåˆ†æå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡ç†è§£LLMsä¸­çš„è¯­ä¹‰ç»“æ„ï¼Œå¯ä»¥ä¼˜åŒ–æ¨¡å‹çš„ç‰¹å¾å¼•å¯¼ï¼Œå‡å°‘è¯¯å¯¼æ€§ç»“æœï¼Œæé«˜æ¨¡å‹åœ¨å¤æ‚è¯­ä¹‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Psychological research consistently finds that human ratings of words across diverse semantic scales can be reduced to a low-dimensional form with relatively little information loss. We find that the semantic associations encoded in the embedding matrices of large language models (LLMs) exhibit a similar structure. We show that the projections of words on semantic directions defined by antonym pairs (e.g. kind - cruel) correlate highly with human ratings, and further find that these projections effectively reduce to a 3-dimensional subspace within LLM embeddings, closely resembling the patterns derived from human survey responses. Moreover, we find that shifting tokens along one semantic direction causes off-target effects on geometrically aligned features proportional to their cosine similarity. These findings suggest that semantic features are entangled within LLMs similarly to how they are interconnected in human language, and a great deal of semantic information, despite its apparent complexity, is surprisingly low-dimensional. Furthermore, accounting for this semantic structure may prove essential for avoiding unintended consequences when steering features.

