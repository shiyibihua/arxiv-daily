---
layout: default
title: Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions
---

# Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08287" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08287v1</a>
  <a href="https://arxiv.org/pdf/2508.08287.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08287v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08287v1', 'Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Farah Atif, Nursultan Askarbekuly, Kareem Darwish, Monojit Choudhury

**åˆ†ç±»**: cs.CL, cs.AI, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04

**å¤‡æ³¨**: 8th AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFiqhQAåŸºå‡†ä»¥è¯„ä¼°LLMåœ¨å®—æ•™é—®é¢˜ä¸Šçš„å¯é æ€§ä¸å›é¿è¡Œä¸º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å®—æ•™é—®é¢˜` `ä¼Šæ–¯å…°æ•™æ³•` `æ¨¡å‹è¯„ä¼°` `å›é¿è¡Œä¸º` `å¤šè¯­è¨€å¤„ç†` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶æœªèƒ½å……åˆ†è¯„ä¼°LLMsåœ¨å®—æ•™é¢†åŸŸçš„å¯é æ€§ï¼Œå°¤å…¶æ˜¯å¯¹ä¸åŒå®—æ•™å­¦æ´¾çš„åŒºåˆ†å’Œå›é¿è¡Œä¸ºçš„è€ƒé‡ã€‚
2. æœ¬æ–‡æå‡ºFiqhQAåŸºå‡†ï¼Œä¸“æ³¨äºç”Ÿæˆä¼Šæ–¯å…°æ•™æ³•åˆ¤å†³ï¼Œå¹¶è¯„ä¼°LLMsåœ¨å‡†ç¡®æ€§å’Œå›é¿èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4oåœ¨å‡†ç¡®æ€§ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè€ŒGeminiå’ŒFanaråœ¨å›é¿è¡Œä¸ºä¸Šè¡¨ç°æ›´ä½³ï¼Œå°¤å…¶åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­å­˜åœ¨æ€§èƒ½ä¸‹é™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸçš„é—®ç­”ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶åœ¨å®—æ•™é¢†åŸŸçš„å¯é æ€§å’Œå‡†ç¡®æ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°åŸºå‡†FiqhQAï¼Œä¸“æ³¨äºç”±å››å¤§ä¸»è¦é€Šå°¼æ´¾å­¦æ´¾æ˜ç¡®åˆ†ç±»çš„ä¼Šæ–¯å…°æ•™æ³•åˆ¤å†³ï¼Œæ¶µç›–é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­ã€‚ä¸ä»¥å¾€ç ”ç©¶ä¸åŒï¼Œæœ¬æ–‡ä¸ä»…è¯„ä¼°LLMsçš„å‡†ç¡®æ€§ï¼Œè¿˜è€ƒå¯Ÿå…¶è¯†åˆ«ä½•æ—¶ä¸å›ç­”çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„é›¶-shotå’Œå›é¿å®éªŒæ˜¾ç¤ºï¼ŒLLMsåœ¨å‡†ç¡®æ€§å’Œå›é¿è¡Œä¸ºä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå°¤å…¶åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­è¡¨ç°å‡ºæ€§èƒ½ä¸‹é™ï¼Œå¼ºè°ƒäº†åœ¨éè‹±è¯­è¯­è¨€ä¸­è¿›è¡Œå®—æ•™æ¨ç†çš„å±€é™æ€§ã€‚æ­¤ç ”ç©¶é¦–æ¬¡å¯¹LLMsåœ¨ç‰¹å®šä¼Šæ–¯å…°å­¦æ´¾åˆ¤å†³ç”Ÿæˆçš„æœ‰æ•ˆæ€§è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯„ä¼°å…¶åœ¨ä¼Šæ–¯å…°æ³•å­¦æŸ¥è¯¢ä¸­çš„å›é¿èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®—æ•™é—®é¢˜å›ç­”ä¸­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒå®—æ•™å­¦æ´¾çš„åŒºåˆ†å’Œå›é¿è¡Œä¸ºçš„è¯„ä¼°ä¸Šå­˜åœ¨çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥FiqhQAåŸºå‡†ï¼Œä¸“æ³¨äºä¼Šæ–¯å…°æ•™æ³•åˆ¤å†³çš„ç”Ÿæˆï¼Œè¯„ä¼°LLMsåœ¨å‡†ç¡®æ€§å’Œå›é¿èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œä»¥å¡«è¡¥ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨é›¶-shotå­¦ä¹ å’Œå›é¿å®éªŒï¼Œæ¯”è¾ƒä¸åŒLLMsåœ¨é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­ä¸­çš„è¡¨ç°ï¼Œåˆ†æå…¶åœ¨å››å¤§é€Šå°¼æ´¾å­¦æ´¾ä¸­çš„å‡†ç¡®æ€§å’Œå›é¿èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶é¦–æ¬¡é’ˆå¯¹ç‰¹å®šä¼Šæ–¯å…°å­¦æ´¾çš„åˆ¤å†³ç”Ÿæˆè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯„ä¼°LLMsåœ¨å®—æ•™æ³•å­¦æŸ¥è¯¢ä¸­çš„å›é¿èƒ½åŠ›ï¼Œå¼ºè°ƒäº†ä»»åŠ¡ç‰¹å®šè¯„ä¼°çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨äº†å¤šç§LLMsï¼ŒåŒ…æ‹¬GPT-4oã€Geminiå’ŒFanarï¼Œè®¾ç½®äº†ä¸åŒçš„è¯„ä¼°æ ‡å‡†ï¼Œå…³æ³¨æ¨¡å‹åœ¨ä¸åŒè¯­è¨€å’Œå­¦æ´¾ä¸‹çš„è¡¨ç°å·®å¼‚ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®æœªè¯¦ç»†æŠ«éœ²ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4oåœ¨å‡†ç¡®æ€§ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè€ŒGeminiå’ŒFanaråœ¨å›é¿è¡Œä¸ºä¸Šè¡¨ç°æ›´ä½³ï¼Œå°¤å…¶åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½å‡æœ‰æ‰€ä¸‹é™ï¼Œå¼ºè°ƒäº†åœ¨éè‹±è¯­ç¯å¢ƒä¸­è¿›è¡Œå®—æ•™æ¨ç†çš„æŒ‘æˆ˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®—æ•™æ•™è‚²ã€æ³•å¾‹å’¨è¯¢å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜LLMsåœ¨å®—æ•™é—®é¢˜ä¸Šçš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´ä¸ºå¯ä¿¡çš„ç­”æ¡ˆï¼Œå‡å°‘é”™è¯¯ä¿¡æ¯çš„ä¼ æ’­ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœä¹Ÿä¸ºæœªæ¥åœ¨å…¶ä»–é¢†åŸŸçš„æ¨¡å‹è¯„ä¼°æä¾›äº†å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the increasing usage of Large Language Models (LLMs) in answering questions in a variety of domains, their reliability and accuracy remain unexamined for a plethora of domains including the religious domains. In this paper, we introduce a novel benchmark FiqhQA focused on the LLM generated Islamic rulings explicitly categorized by the four major Sunni schools of thought, in both Arabic and English. Unlike prior work, which either overlooks the distinctions between religious school of thought or fails to evaluate abstention behavior, we assess LLMs not only on their accuracy but also on their ability to recognize when not to answer. Our zero-shot and abstention experiments reveal significant variation across LLMs, languages, and legal schools of thought. While GPT-4o outperforms all other models in accuracy, Gemini and Fanar demonstrate superior abstention behavior critical for minimizing confident incorrect answers. Notably, all models exhibit a performance drop in Arabic, highlighting the limitations in religious reasoning for languages other than English. To the best of our knowledge, this is the first study to benchmark the efficacy of LLMs for fine-grained Islamic school of thought specific ruling generation and to evaluate abstention for Islamic jurisprudence queries. Our findings underscore the need for task-specific evaluation and cautious deployment of LLMs in religious applications.

