---
layout: default
title: CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis
---

# CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02322" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02322v4</a>
  <a href="https://arxiv.org/pdf/2508.02322.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02322v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02322v4', 'CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-11-26)

**å¤‡æ³¨**: Accepted in AAAI 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAMERAæ¡†æ¶ä»¥è§£å†³MoEæ¨¡å‹çš„å†—ä½™å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `æ¨¡å‹å‹ç¼©` `å¾®ä¸“å®¶` `å‰ªæ` `é‡åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MoEæ¨¡å‹åœ¨å‚æ•°å¢é•¿æ—¶æ€§èƒ½æå‡ä¸æˆæ¯”ä¾‹ï¼Œä¸”é¢ä¸´è®¡ç®—å’Œå­˜å‚¨å¼€é”€å¤§ç­‰æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºå¾®ä¸“å®¶ä½œä¸ºæ›´ç»†ç²’åº¦çš„å‹ç¼©å•å…ƒï¼Œå»ºç«‹CAMERAæ¡†æ¶ä»¥è¯†åˆ«å¾®ä¸“å®¶å†—ä½™ï¼Œæå‡æ¨¡å‹æ•ˆç‡ã€‚
3. åœ¨ä¹ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒCAMERA-Påœ¨å‰ªææ¯”20%è‡³60%ä¸‹å‡ä¼˜äºåŸºçº¿ï¼ŒCAMERA-Qåœ¨2ä½é‡åŒ–ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œè™½ç„¶åœ¨å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†é¢ä¸´æ˜¾è‘—çš„è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¸“å®¶çº§åˆ«çš„å‰ªæã€åˆå¹¶æˆ–åˆ†è§£ä¸Šå­˜åœ¨æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºCAMERAæ¡†æ¶ï¼Œé€šè¿‡å¾®ä¸“å®¶å†—ä½™åˆ†æï¼Œè¯†åˆ«å¾®ä¸“å®¶çš„è´¡çŒ®å·®å¼‚ï¼Œè¿›è€Œæå‡ºCAMERA-På’ŒCAMERA-Qï¼Œåˆ†åˆ«ç”¨äºç»“æ„åŒ–å‰ªæå’Œæ··åˆç²¾åº¦é‡åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒCAMERA-Påœ¨20%åˆ°60%çš„å‰ªææ¯”ä¸‹ä¼˜äºå¼ºåŸºçº¿ï¼Œè€ŒCAMERA-Qåœ¨2ä½é‡åŒ–ä¸‹ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³MoEæ¨¡å‹åœ¨å‚æ•°å¢é•¿æ—¶æ€§èƒ½æå‡ä¸æˆæ¯”ä¾‹çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å‰ªæå’Œåˆå¹¶æ–¹é¢å­˜åœ¨æ€§èƒ½å’Œæ•ˆç‡çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¾®ä¸“å®¶ä½œä¸ºå‹ç¼©å•å…ƒï¼ŒCAMERAæ¡†æ¶èƒ½å¤Ÿæ›´ç»†è‡´åœ°åˆ†æå’Œè¯†åˆ«å†—ä½™ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹çš„è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCAMERAæ¡†æ¶åŒ…æ‹¬å¾®ä¸“å®¶å†—ä½™åˆ†æã€CAMERA-Pç»“æ„åŒ–å‰ªæå’ŒCAMERA-Qæ··åˆç²¾åº¦é‡åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œæ•´ä½“æµç¨‹ä¸ºï¼šé¦–å…ˆè¿›è¡Œå¾®ä¸“å®¶è¯†åˆ«ï¼Œç„¶åå®æ–½å‰ªæå’Œé‡åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šCAMERAæ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¾®ä¸“å®¶çš„å¼•å…¥ï¼Œä½¿å¾—å†—ä½™åˆ†ææ›´åŠ ç»†è‡´ï¼Œèƒ½å¤Ÿåœ¨æ›´é«˜çš„å‹ç¼©æ¯”ä¸‹ä¿æŒæ¨¡å‹æ€§èƒ½ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„ä¸“å®¶çº§åˆ«å‰ªææ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨CAMERA-Pä¸­ï¼Œå‰ªææ¯”ç‡è®¾ç½®åœ¨20%è‡³60%ä¹‹é—´ï¼ŒCAMERA-Qåˆ™é‡‡ç”¨2ä½é‡åŒ–ç­–ç•¥ï¼Œç¡®ä¿åœ¨å‹ç¼©çš„åŒæ—¶ä¿æŒæ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCAMERA-Påœ¨å‰ªææ¯”20%è‡³60%ä¸‹å‡ä¼˜äºå¼ºåŸºçº¿ï¼Œä¸”CAMERA-Qåœ¨2ä½é‡åŒ–ä¸‹çš„æ€§èƒ½è¶…è¶Šç°æœ‰çš„çŸ©é˜µå’Œé€šé“çº§æ–¹æ³•ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚å…·ä½“è€Œè¨€ï¼ŒCAMERAæ–¹æ³•åœ¨å•ä¸ªNVIDIA A100-40GB GPUä¸Šå¯¹Qwen2-57B-A14Bçš„å¾®ä¸“å®¶åˆ†æè€—æ—¶ä¸è¶³5åˆ†é’Ÿã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ï¼Œæé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯ç”¨æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼ŒCAMERAæ¡†æ¶æœ‰æœ›æ¨åŠ¨æ›´å¤šåŸºäºMoEæ¶æ„çš„æ¨¡å‹ä¼˜åŒ–ç ”ç©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level pruning, merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under pruning ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.

