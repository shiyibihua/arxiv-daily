---
layout: default
title: Modular Arithmetic: Language Models Solve Math Digit by Digit
---

# Modular Arithmetic: Language Models Solve Math Digit by Digit

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02513" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02513v1</a>
  <a href="https://arxiv.org/pdf/2508.02513.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02513v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02513v1', 'Modular Arithmetic: Language Models Solve Math Digit by Digit')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tanja Baeumel, Daniil Gurgurov, Yusser al Ghussin, Josef van Genabith, Simon Ostermann

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°å­—ä½ç½®ç‰¹å®šç”µè·¯ä»¥è§£å†³è¯­è¨€æ¨¡å‹çš„ç®—æœ¯é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç®—æœ¯è¿ç®—` `æ•°å­—è¡¨ç¤º` `å› æœå¹²é¢„` `ç‰¹å¾é‡è¦æ€§` `å¯è§£é‡Šæ€§` `ç”µè·¯è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç®—æœ¯ä»»åŠ¡ä¸­çš„å†…éƒ¨æœºåˆ¶ç¼ºä¹ç»Ÿä¸€ç†è§£ï¼Œå°¤å…¶æ˜¯æ•°å­—è¡¨ç¤ºæ–¹å¼å’Œè¿ç®—ç­–ç•¥ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§’ï¼Œè®¤ä¸ºLLMsé€šè¿‡æ•°å­—ä½ç½®ç‰¹å®šç”µè·¯é€ä½å¤„ç†æ•°å­—ï¼Œä»è€Œæ‰§è¡Œç®—æœ¯è¿ç®—ã€‚
3. é€šè¿‡å®éªŒéªŒè¯ï¼Œå‘ç°è¿™äº›ç”µè·¯åœ¨ä¸åŒæ¨¡å‹å’Œæ ‡è®°ç­–ç•¥ä¸‹å‡å­˜åœ¨ï¼Œä¸”èƒ½æœ‰æ•ˆæå‡ç®—æœ¯ä»»åŠ¡çš„è§£å†³èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡è¿‘æœŸç ”ç©¶å¼€å§‹æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç®€å•ç®—æœ¯ä»»åŠ¡ä¸­æ‰€é‡‡ç”¨çš„å†…éƒ¨ç­–ç•¥ï¼Œä½†å¯¹å…¶åŸºæœ¬æœºåˆ¶çš„ç»Ÿä¸€ç†è§£ä»ç„¶ç¼ºä¹ã€‚æœ¬æ–‡æ‰©å±•äº†è¿‘æœŸå‘ç°ï¼Œè¡¨æ˜LLMsä»¥é€ä½æ–¹å¼è¡¨ç¤ºæ•°å­—ï¼Œå¹¶æä¾›äº†æ•°å­—ä½ç½®ç‰¹å®šç”µè·¯çš„è¯æ®ï¼Œè¿™äº›ç”µè·¯ä½¿LLMsèƒ½å¤Ÿæ‰§è¡Œç®€å•çš„ç®—æœ¯ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡ç‰¹å¾é‡è¦æ€§å’Œå› æœå¹²é¢„çš„æ–¹æ³•è¯†åˆ«å¹¶éªŒè¯äº†è¿™äº›ç”µè·¯ï¼Œæ­ç¤ºäº†LLMsåœ¨è§£å†³ç®—æœ¯é—®é¢˜æ—¶æ‰€ä¾èµ–çš„å¯ç»„åˆå’Œå¯è§£é‡Šçš„ç»“æ„ã€‚æˆ‘ä»¬çš„å¹²é¢„é€‰æ‹©æ€§åœ°æ”¹å˜äº†æ¨¡å‹åœ¨ç‰¹å®šæ•°å­—ä½ç½®çš„é¢„æµ‹ï¼Œå±•ç¤ºäº†æ•°å­—ä½ç½®ç”µè·¯åœ¨è§£å†³ç®—æœ¯ä»»åŠ¡ä¸­çš„å› æœä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰§è¡Œç®€å•ç®—æœ¯ä»»åŠ¡æ—¶çš„å†…éƒ¨æœºåˆ¶ä¸æ˜ç¡®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ­ç¤ºæ¨¡å‹å¦‚ä½•é€ä½å¤„ç†æ•°å­—åŠå…¶è¿ç®—ç­–ç•¥çš„ç»†èŠ‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºLLMsé€šè¿‡æ•°å­—ä½ç½®ç‰¹å®šç”µè·¯é€ä½å¤„ç†æ•°å­—ï¼Œè¿™äº›ç”µè·¯åœ¨ä¸åŒçš„æ•°å­—ä½ç½®ï¼ˆå¦‚å•ä½ã€åä½ã€ç™¾ä½ï¼‰ç‹¬ç«‹æ“ä½œï¼Œä»è€Œæé«˜ç®—æœ¯è¿ç®—çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾é‡è¦æ€§åˆ†æå’Œå› æœå¹²é¢„ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡ç‰¹å¾é‡è¦æ€§è¯†åˆ«æ•°å­—ä½ç½®ç‰¹å®šç”µè·¯ï¼Œç„¶åé€šè¿‡å› æœå¹²é¢„éªŒè¯è¿™äº›ç”µè·¯çš„ä½œç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºè¯†åˆ«å’ŒéªŒè¯äº†æ•°å­—ä½ç½®ç‰¹å®šç”µè·¯çš„å­˜åœ¨ï¼Œè¿™ä¸€å‘ç°ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºæä¾›äº†å¯è§£é‡Šçš„ç®—æœ¯è¿ç®—æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†é’ˆå¯¹ç‰¹å®šæ•°å­—ä½ç½®çš„å¹²é¢„ç­–ç•¥ï¼Œæ”¹å˜æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ŒéªŒè¯äº†ç”µè·¯çš„å› æœä½œç”¨ã€‚æ¨¡å‹çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡å‡è€ƒè™‘äº†ä¸åŒæ•°å­—ä½ç½®çš„ç‹¬ç«‹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ•°å­—ä½ç½®ç‰¹å®šç”µè·¯åœ¨ä¸åŒæ¨¡å‹å’Œæ ‡è®°ç­–ç•¥ä¸‹å‡æœ‰æ•ˆå­˜åœ¨ã€‚é€šè¿‡å¹²é¢„å®éªŒï¼Œæ¨¡å‹åœ¨ç‰¹å®šæ•°å­—ä½ç½®çš„é¢„æµ‹å‡†ç¡®æ€§æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è¿™äº›ç”µè·¯åœ¨ç®—æœ¯ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–è®¡ç®—å·¥å…·ç­‰ã€‚é€šè¿‡ç†è§£LLMsçš„ç®—æœ¯å¤„ç†æœºåˆ¶ï¼Œå¯ä»¥æå‡å…¶åœ¨æ•°å­¦æ•™è‚²å’Œè®¡ç®—ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While recent work has begun to uncover the internal strategies that Large Language Models (LLMs) employ for simple arithmetic tasks, a unified understanding of their underlying mechanisms is still lacking. We extend recent findings showing that LLMs represent numbers in a digit-wise manner and present evidence for the existence of digit-position-specific circuits that LLMs use to perform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that operate independently on different digit positions (units, tens, hundreds). Notably, such circuits exist independently of model size and of tokenization strategy, i.e. both for models that encode longer numbers digit-by-digit and as one token. Using Feature Importance and Causal Interventions, we identify and validate the digit-position-specific circuits, revealing a compositional and interpretable structure underlying the solving of arithmetic problems in LLMs. Our interventions selectively alter the model's prediction at targeted digit positions, demonstrating the causal role of digit-position circuits in solving arithmetic tasks.

