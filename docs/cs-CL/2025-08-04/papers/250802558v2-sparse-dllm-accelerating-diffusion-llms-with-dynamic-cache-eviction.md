---
layout: default
title: Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction
---

# Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02558" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02558v2</a>
  <a href="https://arxiv.org/pdf/2508.02558.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02558v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02558v2', 'Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-11-05)

**å¤‡æ³¨**: 12 pages, 7 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/OpenMOSS/Sparse-dLLM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSparse-dLLMä»¥è§£å†³æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹` `åŠ¨æ€ç¼“å­˜` `ç¨€ç–æ³¨æ„åŠ›` `è®¡ç®—å¤æ‚æ€§` `å†…å­˜ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `è§£ç æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶é¢ä¸´äºŒæ¬¡è®¡ç®—å¤æ‚æ€§å’Œå†…å­˜å¼€é”€é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„åº”ç”¨ã€‚
2. Sparse-dLLMé€šè¿‡åŠ¨æ€ç¼“å­˜é©±é€å’Œç¨€ç–æ³¨æ„åŠ›çš„ç»“åˆï¼Œé€‰æ‹©æ€§åœ°ä¿ç•™é‡è¦tokenå¹¶é©±é€ä¸é‡è¦çš„tokenï¼Œä»è€Œæé«˜è§£ç æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSparse-dLLMåœ¨LLaDAå’ŒDreamç³»åˆ—ä¸Šå®ç°äº†é«˜è¾¾10å€çš„ååé‡æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸ä¼ ç»ŸdLLMsç›¸ä¼¼çš„æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰åœ¨æ¨ç†å’Œå¹¶è¡Œè§£ç æ–¹é¢å–å¾—äº†çªç ´ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´ç€å·¨å¤§çš„äºŒæ¬¡è®¡ç®—å¤æ‚æ€§å’Œå†…å­˜å¼€é”€ã€‚ç°æœ‰çš„ç¼“å­˜æŠ€æœ¯é€šè¿‡å­˜å‚¨å…¨å±‚çŠ¶æ€æ¥åŠ é€Ÿè§£ç ï¼Œä½†å¯¼è‡´äº†æ˜¾è‘—çš„å†…å­˜ä½¿ç”¨ï¼Œé™åˆ¶äº†é•¿ä¸Šä¸‹æ–‡çš„åº”ç”¨ã€‚æœ¬æ–‡åˆ†æäº†dLLMsä¸­çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œå‘ç°è·¨å±‚ç¨€ç–æ€§æŒç»­å­˜åœ¨ï¼Œå…³é”®tokenåœ¨è§£ç æ­¥éª¤ä¸­ä¿æŒæ˜¾è‘—æ€§ï¼Œè€Œä½ç›¸å…³tokenåˆ™ä¿æŒä¸é‡è¦ï¼Œä»è€Œæ¿€åŠ±äº†é€‰æ‹©æ€§ç¼“å­˜é©±é€ã€‚æˆ‘ä»¬æå‡ºäº†Sparse-dLLMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†åŠ¨æ€ç¼“å­˜é©±é€ä¸ç¨€ç–æ³¨æ„åŠ›ç»“åˆçš„æ— è®­ç»ƒæ¡†æ¶ã€‚é€šè¿‡åˆ©ç”¨tokenæ˜¾è‘—æ€§åœ¨æ­¥éª¤ä¸­çš„ç¨³å®šæ€§ï¼ŒSparse-dLLMä¿ç•™å…³é”®tokenï¼Œå¹¶ä½¿ç”¨æ³¨æ„åŠ›å¼•å¯¼ç­–ç•¥åŠ¨æ€é©±é€ä¸é‡è¦çš„å‰ç¼€/åç¼€æ¡ç›®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSparse-dLLMåœ¨LLaDAå’ŒDreamç³»åˆ—ä¸Šå®ç°äº†æ¯”ä¼ ç»ŸdLLMsé«˜å‡º10å€çš„ååé‡ï¼ŒåŒæ—¶æ€§èƒ½ç›¸å½“ä¸”å³°å€¼å†…å­˜æˆæœ¬ç›¸ä¼¼ï¼Œè¶…è¶Šäº†ä»¥å¾€æ–¹æ³•çš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´çš„é«˜è®¡ç®—å¤æ‚æ€§å’Œå†…å­˜å¼€é”€é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å…¨å±‚çŠ¶æ€ç¼“å­˜åŠ é€Ÿè§£ç ï¼Œä½†å¯¼è‡´å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œé™åˆ¶äº†é•¿ä¸Šä¸‹æ–‡çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSparse-dLLMçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ³¨æ„åŠ›æ¨¡å¼ä¸­çš„ç¨€ç–æ€§ï¼Œé€šè¿‡åŠ¨æ€ç¼“å­˜é©±é€ç­–ç•¥é€‰æ‹©æ€§åœ°ä¿ç•™é‡è¦tokenï¼Œé©±é€ä¸é‡è¦çš„tokenï¼Œä»è€Œé™ä½å†…å­˜ä½¿ç”¨å¹¶æé«˜è§£ç æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSparse-dLLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åŠ¨æ€ç¼“å­˜ç®¡ç†æ¨¡å—å’Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚åŠ¨æ€ç¼“å­˜ç®¡ç†æ¨¡å—è´Ÿè´£æ ¹æ®tokençš„æ˜¾è‘—æ€§åŠ¨æ€è°ƒæ•´ç¼“å­˜å†…å®¹ï¼Œè€Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åˆ™ä¼˜åŒ–äº†è®¡ç®—è¿‡ç¨‹ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„è®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šSparse-dLLMçš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡æå‡ºäº†æ— è®­ç»ƒçš„åŠ¨æ€ç¼“å­˜é©±é€ä¸ç¨€ç–æ³¨æ„åŠ›ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†è§£ç æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSparse-dLLMé‡‡ç”¨äº†å»¶è¿ŸåŒå‘ç¨€ç–ç¼“å­˜ç­–ç•¥ï¼Œåˆ©ç”¨tokenæ˜¾è‘—æ€§åœ¨è§£ç æ­¥éª¤ä¸­çš„ç¨³å®šæ€§ï¼Œç¡®ä¿å…³é”®tokençš„ä¿ç•™ï¼ŒåŒæ—¶åŠ¨æ€é©±é€ä¸é‡è¦çš„å‰ç¼€å’Œåç¼€æ¡ç›®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Sparse-dLLMåœ¨LLaDAå’ŒDreamç³»åˆ—ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶ååé‡æ¯”ä¼ ç»ŸdLLMsé«˜å‡º10å€ï¼ŒåŒæ—¶åœ¨æ€§èƒ½å’Œå³°å€¼å†…å­˜æˆæœ¬ä¸Šä¿æŒç›¸ä¼¼ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œè¶…è¶Šäº†ä»¥å¾€çš„ç›¸å…³æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Sparse-dLLMçš„ç ”ç©¶æˆæœåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œé•¿æ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜è§£ç æ•ˆç‡å’Œé™ä½å†…å­˜å¼€é”€ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ”¯æŒæ›´å¤æ‚çš„åº”ç”¨åœºæ™¯ï¼Œæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠå’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness. The code is available at https://github.com/OpenMOSS/Sparse-dLLM.

