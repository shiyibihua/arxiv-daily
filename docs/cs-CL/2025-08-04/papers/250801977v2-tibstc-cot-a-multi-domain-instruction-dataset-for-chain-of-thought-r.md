---
layout: default
title: TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models
---

# TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01977" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01977v2</a>
  <a href="https://arxiv.org/pdf/2508.01977.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01977v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01977v2', 'TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fan Gao, Cheng Huang, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-12-16)

**å¤‡æ³¨**: We will merge this paper with arXiv:2503.18288

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Vicentvankor/sun-shine)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTIBSTC-CoTä»¥è§£å†³è—è¯­æ•°æ®ç¨€ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è—è¯­å¤„ç†` `ä½èµ„æºè¯­è¨€` `æ•°æ®é›†æ„å»º` `é“¾å¼æ€ç»´` `è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ™ºèƒ½ç¿»è¯‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è—è¯­ä½œä¸ºä¸€ç§ä½èµ„æºè¯­è¨€ï¼Œé¢ä¸´ä¸¥é‡çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ»¡è¶³å…¶è¯­è¨€å¤„ç†éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºäº†TIBSTC-CoTæ•°æ®é›†ï¼Œé€šè¿‡é“¾å¼æ€ç»´æç¤ºè‡ªåŠ¨æ„å»ºï¼Œæä¾›äº†å¤šé¢†åŸŸçš„è—è¯­æ•°æ®ï¼Œæ”¯æŒè¯­è¨€ç†è§£ä¸ç”Ÿæˆã€‚
3. åŸºäºTIBSTC-CoTè®­ç»ƒçš„Sunshine-thinking LLMåœ¨æ¨ç†å’Œç”Ÿæˆæ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå·²æ¥è¿‘æœ€å…ˆè¿›çš„å¤šè¯­è¨€æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è§£å†³è—è¯­è¿™ä¸€ä½èµ„æºè¯­è¨€çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TIBSTC-CoTï¼Œä¸€ä¸ªé€šè¿‡é“¾å¼æ€ç»´æç¤ºè‡ªåŠ¨æ„å»ºçš„å¤§è§„æ¨¡å¤šé¢†åŸŸè—è¯­æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•å’Œå¯é‡å¤çš„æ•°æ®é›†åˆ›å»ºæ¡†æ¶ï¼Œæ¶µç›–äº†è¯­è¨€ç†è§£å’Œç”Ÿæˆæ‰€éœ€çš„å¤šæ ·åŒ–é¢†åŸŸå’Œæ¨ç†æ¨¡å¼ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†Sunshine-thinking LLMç³»åˆ—ï¼Œè¿™äº›è—è¯­ä¸­å¿ƒçš„è¯­è¨€æ¨¡å‹å…·å¤‡é“¾å¼æ€ç»´èƒ½åŠ›ï¼Œç»è¿‡è®­ç»ƒååœ¨æ¨ç†å’Œç”Ÿæˆæ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå·²æ¥è¿‘å½“å‰æœ€å…ˆè¿›çš„å¤šè¯­è¨€LLMã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå®ç°åŒ…å®¹æ€§äººå·¥æ™ºèƒ½è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä¿ƒè¿›äº†é«˜è´¨é‡è—è¯­å¤„ç†çš„å®ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è—è¯­æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€å¤„ç†ä¸Šæ•ˆæœä¸ä½³ï¼Œç¼ºä¹è¶³å¤Ÿçš„é«˜è´¨é‡æ•°æ®æ”¯æŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡é“¾å¼æ€ç»´æç¤ºï¼Œè‡ªåŠ¨ç”Ÿæˆå¤šé¢†åŸŸçš„è—è¯­æ•°æ®é›†TIBSTC-CoTï¼Œæä¾›ä¸°å¯Œçš„æ¨ç†æ¨¡å¼å’Œè¯­è¨€ç†è§£èƒ½åŠ›ï¼Œè¿›è€Œè®­ç»ƒå‡ºé«˜æ•ˆçš„è—è¯­è¯­è¨€æ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æ„å»ºé€šè¿‡å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„è—è¯­æ–‡æœ¬ï¼Œæ¨¡å‹è®­ç»ƒåˆ™åŸºäºç”Ÿæˆçš„æ•°æ®é›†è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šTIBSTC-CoTæ•°æ®é›†çš„è‡ªåŠ¨æ„å»ºæ–¹æ³•æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œæ˜¾è‘—æé«˜äº†ä½èµ„æºè¯­è¨€çš„æ•°æ®è·å–æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿæ‰‹åŠ¨æ ‡æ³¨æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„å¯æ‰©å±•æ€§å’Œå¯é‡å¤æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–é“¾å¼æ€ç»´èƒ½åŠ›çš„å­¦ä¹ ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°è¾¾åˆ°æœ€ä¼˜ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„è®¾è®¡ä¹Ÿé’ˆå¯¹è—è¯­ç‰¹æ€§è¿›è¡Œäº†è°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºTIBSTC-CoTè®­ç»ƒçš„Sunshine-thinking LLMåœ¨æ¨ç†å’Œç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½ä¸å½“å‰æœ€å…ˆè¿›çš„å¤šè¯­è¨€LLMç›¸å½“ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æå‡å¹…åº¦ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è—è¯­çš„è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„è—è¯­æ•°æ®å’Œæ¨¡å‹ï¼Œèƒ½å¤Ÿä¿ƒè¿›è—è¯­ç›¸å…³æŠ€æœ¯çš„å‘å±•ï¼Œæå‡è—è¯­ç”¨æˆ·çš„æ•°å­—ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šå’Œæ–‡åŒ–ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: https://github.com/Vicentvankor/sun-shine.

