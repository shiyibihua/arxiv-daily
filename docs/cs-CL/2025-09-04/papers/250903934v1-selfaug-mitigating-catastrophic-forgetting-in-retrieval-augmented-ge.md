---
layout: default
title: SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment
---

# SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03934" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03934v1</a>
  <a href="https://arxiv.org/pdf/2509.03934.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03934v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03934v1', 'SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqing Huang, Rongyang Zhang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Xuyang Zhi, Guiquan Liu, Xin Li, Hao Wang, Enhong Chen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/USTC-StarTeam/SelfAug)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSelfAugï¼Œé€šè¿‡è‡ªå¯¹é½åˆ†å¸ƒç¼“è§£RAGä¸­ç¾éš¾æ€§é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `ç¾éš¾æ€§é—å¿˜` `è‡ªåˆ†å¸ƒå¯¹é½` `å¤§å‹è¯­è¨€æ¨¡å‹` `æŒç»­å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. RAGå¾®è°ƒè™½èƒ½æå‡ç‰¹å®šä»»åŠ¡æ€§èƒ½ï¼Œä½†æ˜“å¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œæ¨¡å‹ä¸§å¤±åŸæœ‰çŸ¥è¯†å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. SelfAugé€šè¿‡è‡ªå¯¹é½è¾“å…¥åºåˆ—logitsï¼Œä¿æŒæ¨¡å‹è¯­ä¹‰åˆ†å¸ƒï¼Œä»è€Œç¼“è§£ç¾éš¾æ€§é—å¿˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSelfAugåœ¨ä¸‹æ¸¸ä»»åŠ¡å­¦ä¹ å’Œé€šç”¨èƒ½åŠ›ä¿æŒé—´å–å¾—å¹³è¡¡ï¼Œå¹¶æ­ç¤ºåˆ†å¸ƒåç§»ä¸ç¾éš¾æ€§é—å¿˜çš„ç›¸å…³æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°è¿›å±•é€šè¿‡å…¶åœ¨ç†è§£å’Œæ‰§è¡Œå„ç§ä»»åŠ¡æ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ã€‚è™½ç„¶æœ‰ç›‘ç£çš„å¾®è°ƒï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœºæ™¯ä¸­ï¼Œæœ‰æ•ˆåœ°æé«˜äº†ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼Œä½†å®ƒé€šå¸¸ä¼šå¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œå³æ¨¡å‹ä¼šä¸¢å¤±å…ˆå‰è·å¾—çš„çŸ¥è¯†å’Œé€šç”¨èƒ½åŠ›ã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆè¦ä¹ˆéœ€è¦è®¿é—®é€šç”¨æŒ‡ä»¤æ•°æ®ï¼Œè¦ä¹ˆåœ¨ä¿æŒæ¨¡å‹åŸå§‹åˆ†å¸ƒæ–¹é¢é¢ä¸´é™åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºSelfAugï¼Œä¸€ç§è‡ªåˆ†å¸ƒå¯¹é½æ–¹æ³•ï¼Œå®ƒå¯¹é½è¾“å…¥åºåˆ—logitsä»¥ä¿æŒæ¨¡å‹çš„è¯­ä¹‰åˆ†å¸ƒï¼Œä»è€Œå‡è½»ç¾éš¾æ€§é—å¿˜å¹¶æé«˜ä¸‹æ¸¸æ€§èƒ½ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒSelfAugåœ¨ä¸‹æ¸¸å­¦ä¹ å’Œé€šç”¨èƒ½åŠ›ä¿æŒä¹‹é—´å–å¾—äº†å“è¶Šçš„å¹³è¡¡ã€‚æˆ‘ä»¬å…¨é¢çš„å®è¯åˆ†ææ­ç¤ºäº†RAGåœºæ™¯ä¸­åˆ†å¸ƒåç§»ä¸ç¾éš¾æ€§é—å¿˜ä¸¥é‡ç¨‹åº¦ä¹‹é—´çš„ç›´æ¥ç›¸å…³æ€§ï¼Œçªå‡ºäº†é€šç”¨æŒ‡ä»¤è°ƒæ•´ä¸­ç¼ºä¹RAGèƒ½åŠ›å¦‚ä½•å¯¼è‡´å¾®è°ƒæœŸé—´çš„æ˜¾ç€åˆ†å¸ƒåç§»ã€‚æˆ‘ä»¬çš„å‘ç°ä¸ä»…åŠ æ·±äº†å¯¹RAGä¸Šä¸‹æ–‡ä¸­ç¾éš¾æ€§é—å¿˜çš„ç†è§£ï¼Œè€Œä¸”è¿˜æä¾›äº†ä¸€ç§é€‚ç”¨äºå„ç§å¾®è°ƒåœºæ™¯çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœºæ™¯ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»è¿‡æœ‰ç›‘ç£å¾®è°ƒåå‡ºç°çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–äºé¢å¤–çš„é€šç”¨æŒ‡ä»¤æ•°æ®ï¼Œå¢åŠ äº†æ•°æ®è·å–æˆæœ¬å’Œå¤„ç†å¤æ‚åº¦ï¼Œè¦ä¹ˆéš¾ä»¥æœ‰æ•ˆä¿æŒæ¨¡å‹çš„åŸå§‹åˆ†å¸ƒï¼Œå¯¼è‡´æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æå‡çš„åŒæ—¶ï¼Œä¸§å¤±äº†åŸæœ‰çš„é€šç”¨èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSelfAugçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªåˆ†å¸ƒå¯¹é½æ¥ç¼“è§£ç¾éš¾æ€§é—å¿˜ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡å¯¹é½å¾®è°ƒå‰åæ¨¡å‹åœ¨è¾“å…¥åºåˆ—ä¸Šçš„logitsåˆ†å¸ƒï¼Œä»è€Œä¿æŒæ¨¡å‹çš„è¯­ä¹‰åˆ†å¸ƒï¼Œé¿å…æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¿‡åº¦æ‹Ÿåˆç‰¹å®šä»»åŠ¡çš„æ•°æ®ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™åŸæœ‰çš„çŸ¥è¯†å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ— éœ€é¢å¤–çš„é€šç”¨æŒ‡ä»¤æ•°æ®ï¼Œè€Œæ˜¯åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„è¾“å‡ºæ¥è¿›è¡Œå¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSelfAugçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1. ä½¿ç”¨RAGæ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚2. åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œè®¡ç®—æ¨¡å‹åœ¨åŸå§‹è¾“å…¥åºåˆ—ä¸Šçš„logitsåˆ†å¸ƒã€‚3. ä½¿ç”¨è‡ªåˆ†å¸ƒå¯¹é½æŸå¤±å‡½æ•°ï¼Œå°†å¾®è°ƒåçš„logitsåˆ†å¸ƒä¸åŸå§‹logitsåˆ†å¸ƒè¿›è¡Œå¯¹é½ã€‚4. ä½¿ç”¨å¯¹é½åçš„æ¨¡å‹è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šSelfAugçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªåˆ†å¸ƒå¯¹é½çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºç¼“è§£RAGä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSelfAugä¸éœ€è¦é¢å¤–çš„é€šç”¨æŒ‡ä»¤æ•°æ®ï¼Œè€Œæ˜¯åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„è¾“å‡ºæ¥è¿›è¡Œå¯¹é½ï¼Œä»è€Œæ›´åŠ é«˜æ•ˆå’Œä¾¿æ·ã€‚æ­¤å¤–ï¼ŒSelfAugé€šè¿‡ç›´æ¥å¯¹é½logitsåˆ†å¸ƒï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¿æŒæ¨¡å‹çš„è¯­ä¹‰åˆ†å¸ƒï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™åŸæœ‰çš„çŸ¥è¯†å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šSelfAugçš„å…³é”®è®¾è®¡åœ¨äºè‡ªåˆ†å¸ƒå¯¹é½æŸå¤±å‡½æ•°ã€‚è¯¥æŸå¤±å‡½æ•°ç”¨äºè¡¡é‡å¾®è°ƒå‰åæ¨¡å‹åœ¨è¾“å…¥åºåˆ—ä¸Šçš„logitsåˆ†å¸ƒçš„å·®å¼‚ã€‚å…·ä½“æ¥è¯´ï¼Œå¯ä»¥ä½¿ç”¨KLæ•£åº¦æˆ–äº¤å‰ç†µç­‰æ–¹æ³•æ¥è®¡ç®—logitsåˆ†å¸ƒçš„å·®å¼‚ï¼Œå¹¶å°†è¯¥å·®å¼‚ä½œä¸ºæŸå¤±å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡è°ƒæ•´æŸå¤±å‡½æ•°çš„æƒé‡æ¥æ§åˆ¶è‡ªåˆ†å¸ƒå¯¹é½çš„å¼ºåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSelfAugåœ¨å¤šä¸ªRAGåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒSelfAugèƒ½å¤Ÿå°†æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜5%ä»¥ä¸Šï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ç¼“è§£äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ä¸ç°æœ‰çš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒSelfAugåœ¨ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å’Œé€šç”¨èƒ½åŠ›ä¿æŒä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SelfAugå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦åˆ©ç”¨RAGè¿›è¡ŒçŸ¥è¯†å¢å¼ºçš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½é—®ç­”ã€æ–‡æ¡£æ‘˜è¦ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡ç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼ŒSelfAugèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå…¶åŸæœ‰çš„é€šç”¨èƒ½åŠ›ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“å®ç”¨æ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼ŒSelfAugæœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„å¾®è°ƒåœºæ™¯ï¼Œä¾‹å¦‚æŒç»­å­¦ä¹ å’Œé¢†åŸŸè‡ªé€‚åº”ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in large language models (LLMs) have revolutionized natural language processing through their remarkable capabilities in understanding and executing diverse tasks. While supervised fine-tuning, particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively enhances task-specific performance, it often leads to catastrophic forgetting, where models lose their previously acquired knowledge and general capabilities. Existing solutions either require access to general instruction data or face limitations in preserving the model's original distribution. To overcome these limitations, we propose SelfAug, a self-distribution alignment method that aligns input sequence logits to preserve the model's semantic distribution, thereby mitigating catastrophic forgetting and improving downstream performance. Extensive experiments demonstrate that SelfAug achieves a superior balance between downstream learning and general capability retention. Our comprehensive empirical analysis reveals a direct correlation between distribution shifts and the severity of catastrophic forgetting in RAG scenarios, highlighting how the absence of RAG capabilities in general instruction tuning leads to significant distribution shifts during fine-tuning. Our findings not only advance the understanding of catastrophic forgetting in RAG contexts but also provide a practical solution applicable across diverse fine-tuning scenarios. Our code is publicly available at https://github.com/USTC-StarTeam/SelfAug.

