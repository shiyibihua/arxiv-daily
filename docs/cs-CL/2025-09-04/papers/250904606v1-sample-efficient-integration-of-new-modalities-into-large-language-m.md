---
layout: default
title: Sample-efficient Integration of New Modalities into Large Language Models
---

# Sample-efficient Integration of New Modalities into Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04606" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04606v1</a>
  <a href="https://arxiv.org/pdf/2509.04606.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04606v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04606v1', 'Sample-efficient Integration of New Modalities into Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Osman Batur Ä°nce, AndrÃ© F. T. Martins, Oisin Mac Aodha, Edoardo M. Ponti

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: Pre-print

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSEMIæ–¹æ³•ï¼Œé«˜æ•ˆåœ°å°†æ–°æ¨¡æ€é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡æ€é›†æˆ` `è¶…ç½‘ç»œ` `æ ·æœ¬é«˜æ•ˆå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å°†æ–°æ¨¡æ€é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œéœ€è¦å¤§é‡é…å¯¹æ•°æ®ï¼Œè¿™å¯¹äºä½èµ„æºæ¨¡æ€æ˜¯å·¨å¤§çš„æŒ‘æˆ˜ã€‚
2. SEMIæ–¹æ³•é€šè¿‡è®­ç»ƒä¸€ä¸ªè¶…ç½‘ç»œæ¥é€‚é…å…±äº«æŠ•å½±å™¨ï¼Œä»…éœ€å°‘é‡æ ·æœ¬å³å¯å°†æ–°æ¨¡æ€é›†æˆåˆ°LLMä¸­ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSEMIåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ–°æ¨¡æ€é›†æˆçš„æ ·æœ¬æ•ˆç‡ï¼Œä¾‹å¦‚å«æ˜Ÿå›¾åƒå’Œå¤©æ–‡å›¾åƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šç§æ¨¡æ€ã€‚ç„¶è€Œï¼Œç”±äºå¯èƒ½çš„æ¨¡æ€ç©ºé—´å·¨å¤§ä¸”ä¸æ–­å‘å±•ï¼Œä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥åŒ…å«æ‰€æœ‰æ¨¡æ€æ˜¯ä¸å¯è¡Œçš„ã€‚æ­¤å¤–ï¼Œç›®å‰å°†æ¨¡æ€é›†æˆåˆ°é¢„å…ˆå­˜åœ¨çš„åŸºç¡€æ¨¡å‹ä¸­éœ€è¦å¤§é‡çš„é…å¯¹æ•°æ®ï¼Œè€Œå¯¹äºä½èµ„æºæ¨¡æ€æ¥è¯´ï¼Œè¿™äº›æ•°æ®é€šå¸¸ä¸å¯ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå°†æ–°æ¨¡æ€é«˜æ•ˆé›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ–¹æ³•ï¼Œç§°ä¸ºæ ·æœ¬é«˜æ•ˆæ¨¡æ€é›†æˆï¼ˆSEMIï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¶…ç½‘ç»œï¼Œå®ƒå¯ä»¥å°†å…±äº«æŠ•å½±å™¨ï¼ˆä½äºæ¨¡æ€ç‰¹å®šç¼–ç å™¨å’ŒLLMä¹‹é—´ï¼‰é€‚é…åˆ°ä»»ä½•æ¨¡æ€ã€‚è¯¥è¶…ç½‘ç»œåœ¨é«˜èµ„æºæ¨¡æ€ï¼ˆå³æ–‡æœ¬ã€è¯­éŸ³ã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¨ç†æ—¶ä»¥ä»»æ„æ¨¡æ€çš„å°‘é‡æ ·æœ¬ä¸ºæ¡ä»¶ï¼Œä»¥ç”Ÿæˆåˆé€‚çš„é€‚é…å™¨ã€‚ä¸ºäº†å¢åŠ è®­ç»ƒæ¨¡æ€çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬é€šè¿‡ç­‰è·å˜æ¢äººä¸ºåœ°å¢åŠ ç¼–ç å™¨çš„æ•°é‡ã€‚æˆ‘ä»¬å‘ç°ï¼ŒSEMIåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹é›†æˆæ–°æ¨¡æ€ï¼ˆå³å«æ˜Ÿå›¾åƒã€å¤©æ–‡å›¾åƒã€æƒ¯æ€§æµ‹é‡ã€åˆ†å­ï¼‰æ—¶ï¼Œå®ç°äº†æ ·æœ¬æ•ˆç‡çš„æ˜¾è‘—æå‡ï¼Œä¸”é€‚ç”¨äºä»»æ„åµŒå…¥ç»´åº¦çš„ç¼–ç å™¨ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†è¾¾åˆ°ä¸32-shot SEMIç›¸åŒçš„ç²¾åº¦ï¼Œä»å¤´å¼€å§‹è®­ç»ƒæŠ•å½±å™¨éœ€è¦å¤š64å€çš„æ•°æ®ã€‚å› æ­¤ï¼ŒSEMIæœ‰æœ›æ‰©å±•åŸºç¡€æ¨¡å‹çš„æ¨¡æ€è¦†ç›–èŒƒå›´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å°†æ–°çš„ã€ä½èµ„æºæ¨¡æ€é«˜æ•ˆåœ°é›†æˆåˆ°é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡é…å¯¹æ•°æ®æ¥è®­ç»ƒæ¨¡æ€ç‰¹å®šçš„æŠ•å½±å±‚ï¼Œè¿™å¯¹äºæ•°æ®ç¨€ç¼ºçš„æ¨¡æ€æ¥è¯´æ˜¯ä¸å¯è¡Œçš„ã€‚æ­¤å¤–ï¼Œä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªåŒ…å«æ‰€æœ‰æ¨¡æ€çš„åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—ä¸Šæ˜¯æå…¶æ˜‚è´µçš„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªè¶…ç½‘ç»œï¼ˆhypernetworkï¼‰æ¥ç”Ÿæˆæ¨¡æ€ç‰¹å®šçš„é€‚é…å™¨ï¼Œä»è€Œå°†æ¨¡æ€ç¼–ç å™¨çš„è¾“å‡ºæ˜ å°„åˆ°LLMçš„è¾“å…¥ç©ºé—´ã€‚è¯¥è¶…ç½‘ç»œåœ¨é«˜èµ„æºæ¨¡æ€ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ å¦‚ä½•æ ¹æ®å°‘é‡æ–°æ¨¡æ€çš„æ ·æœ¬æ¥ç”Ÿæˆåˆé€‚çš„é€‚é…å™¨ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¸ºæ¯ç§æ–°æ¨¡æ€ä»å¤´å¼€å§‹è®­ç»ƒæŠ•å½±å±‚ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ¨¡æ€ç‰¹å®šçš„ç¼–ç å™¨ï¼Œç”¨äºå°†ä¸åŒæ¨¡æ€çš„æ•°æ®è½¬æ¢ä¸ºåµŒå…¥å‘é‡ï¼›2) ä¸€ä¸ªå…±äº«çš„æŠ•å½±å™¨ï¼Œç”¨äºå°†ç¼–ç å™¨çš„è¾“å‡ºæ˜ å°„åˆ°LLMçš„è¾“å…¥ç©ºé—´ï¼›3) ä¸€ä¸ªè¶…ç½‘ç»œï¼Œç”¨äºç”Ÿæˆæ¨¡æ€ç‰¹å®šçš„é€‚é…å™¨ï¼Œè¯¥é€‚é…å™¨ä½œç”¨äºå…±äº«æŠ•å½±å™¨ï¼›4) ä¸€ä¸ªé¢„è®­ç»ƒçš„LLMï¼Œç”¨äºå¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ã€‚è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œåœ¨é«˜èµ„æºæ¨¡æ€ä¸Šè®­ç»ƒè¶…ç½‘ç»œï¼›ç„¶åï¼Œåœ¨å°‘é‡æ–°æ¨¡æ€æ ·æœ¬ä¸Šå¾®è°ƒè¶…ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨è¶…ç½‘ç»œæ¥ç”Ÿæˆæ¨¡æ€ç‰¹å®šçš„é€‚é…å™¨ã€‚ä¸ç›´æ¥è®­ç»ƒæ¨¡æ€ç‰¹å®šçš„æŠ•å½±å±‚ç›¸æ¯”ï¼Œè¶…ç½‘ç»œèƒ½å¤Ÿåˆ©ç”¨åœ¨é«˜èµ„æºæ¨¡æ€ä¸Šå­¦ä¹ åˆ°çš„çŸ¥è¯†ï¼Œä»è€Œåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹å®ç°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºä½¿ç”¨ç­‰è·å˜æ¢æ¥å¢åŠ è®­ç»ƒæ¨¡æ€çš„å¤šæ ·æ€§ï¼Œè¿›ä¸€æ­¥æå‡äº†è¶…ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè¶…ç½‘ç»œä»¥å°‘é‡æ–°æ¨¡æ€çš„æ ·æœ¬ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆé€‚é…å™¨çš„æƒé‡ã€‚é€‚é…å™¨å¯ä»¥æ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚æˆ–æ›´å¤æ‚çš„ç¥ç»ç½‘ç»œã€‚æŸå¤±å‡½æ•°é€šå¸¸åŒ…æ‹¬ä¸€ä¸ªé‡æ„æŸå¤±ï¼Œç”¨äºç¡®ä¿é€‚é…å™¨èƒ½å¤Ÿå‡†ç¡®åœ°å°†æ¨¡æ€ç¼–ç å™¨çš„è¾“å‡ºæ˜ å°„åˆ°LLMçš„è¾“å…¥ç©ºé—´ï¼Œä»¥åŠä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚ç­‰è·å˜æ¢é€šè¿‡æ—‹è½¬æˆ–åå°„ç¼–ç å™¨çš„è¾“å‡ºå‘é‡æ¥ç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬ï¼Œä»è€Œå¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSEMIæ–¹æ³•åœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ–°æ¨¡æ€é›†æˆçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†è¾¾åˆ°ä¸32-shot SEMIç›¸åŒçš„ç²¾åº¦ï¼Œä»å¤´å¼€å§‹è®­ç»ƒæŠ•å½±å™¨éœ€è¦å¤š64å€çš„æ•°æ®ã€‚SEMIåœ¨å«æ˜Ÿå›¾åƒã€å¤©æ–‡å›¾åƒã€æƒ¯æ€§æµ‹é‡å’Œåˆ†å­ç­‰å¤šç§æ¨¡æ€ä¸Šéƒ½å–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨ä»·å€¼ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†ä»»æ„åµŒå…¥ç»´åº¦çš„ç¼–ç å™¨ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å…¶çµæ´»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€ä¿¡æ¯å¤„ç†é¢†åŸŸï¼Œä¾‹å¦‚åŒ»å­¦å½±åƒåˆ†æã€é¥æ„Ÿå›¾åƒè§£è¯‘ã€æœºå™¨äººæ„ŸçŸ¥ç­‰ã€‚é€šè¿‡SEMIæ–¹æ³•ï¼Œå¯ä»¥å¿«é€Ÿå°†æ–°çš„ä¼ æ„Ÿå™¨æ•°æ®æˆ–æ•°æ®æ¨¡æ€é›†æˆåˆ°ç°æœ‰çš„LLMä¸­ï¼Œä»è€Œæ‰©å±•LLMçš„åº”ç”¨èŒƒå›´ï¼Œå¹¶é™ä½å¼€å‘æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´çµæ´»çš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal foundation models can process several modalities. However, since the space of possible modalities is large and evolving over time, training a model from scratch to encompass all modalities is unfeasible. Moreover, integrating a modality into a pre-existing foundation model currently requires a significant amount of paired data, which is often not available for low-resource modalities. In this paper, we introduce a method for sample-efficient modality integration (SEMI) into Large Language Models (LLMs). To this end, we devise a hypernetwork that can adapt a shared projector -- placed between modality-specific encoders and an LLM -- to any modality. The hypernetwork, trained on high-resource modalities (i.e., text, speech, audio, video), is conditioned on a few samples from any arbitrary modality at inference time to generate a suitable adapter. To increase the diversity of training modalities, we artificially multiply the number of encoders through isometric transformations. We find that SEMI achieves a significant boost in sample efficiency during few-shot integration of new modalities (i.e., satellite images, astronomical images, inertial measurements, and molecules) with encoders of arbitrary embedding dimensionality. For instance, to reach the same accuracy as 32-shot SEMI, training the projector from scratch needs 64$\times$ more data. As a result, SEMI holds promise to extend the modality coverage of foundation models.

