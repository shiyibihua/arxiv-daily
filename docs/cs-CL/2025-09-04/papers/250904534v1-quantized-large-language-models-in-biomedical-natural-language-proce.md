---
layout: default
title: Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation
---

# Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04534" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04534v1</a>
  <a href="https://arxiv.org/pdf/2509.04534.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04534v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04534v1', 'Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zaifu Zhan, Shuang Zhou, Min Zeng, Kai Yu, Meijia Song, Xiaoyi Chen, Jun Wang, Yu Hou, Rui Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: 11 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é‡åŒ–LLMå®ç°ç”Ÿç‰©åŒ»å­¦NLPï¼šè¯„ä¼°ä¸æ¨èï¼Œé™ä½éƒ¨ç½²æˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å‹ç¼©` `æœ¬åœ°éƒ¨ç½²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹è®¡ç®—éœ€æ±‚é«˜ï¼Œéš¾ä»¥åœ¨æ•°æ®éšç§æ•æ„Ÿçš„åŒ»ç–—ç¯å¢ƒä¸­éƒ¨ç½²ã€‚
2. é€šè¿‡é‡åŒ–æŠ€æœ¯å‹ç¼©æ¨¡å‹ï¼Œåœ¨é™ä½èµ„æºéœ€æ±‚çš„åŒæ—¶ï¼Œå°½é‡ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œé‡åŒ–èƒ½æ˜¾è‘—é™ä½GPUå†…å­˜éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶è§„æ¨¡å’Œè®¡ç®—éœ€æ±‚çš„å¿«é€Ÿå¢é•¿å¯¹åŒ»ç–—ç¯å¢ƒä¸­çš„åº”ç”¨æ„æˆäº†ä¸»è¦éšœç¢ï¼Œå› ä¸ºæ•°æ®éšç§é™åˆ¶äº†äº‘éƒ¨ç½²ï¼Œä¸”èµ„æºæœ‰é™ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†é‡åŒ–å¯¹12ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å½±å“ï¼ŒåŒ…æ‹¬é€šç”¨æ¨¡å‹å’Œç”Ÿç‰©åŒ»å­¦ä¸“ç”¨æ¨¡å‹ï¼Œæ¶µç›–å‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–ã€å¤šæ ‡ç­¾åˆ†ç±»å’Œé—®ç­”å››ä¸ªå…³é”®ä»»åŠ¡çš„å…«ä¸ªåŸºå‡†æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œé‡åŒ–æ˜¾è‘—é™ä½äº†GPUå†…å­˜éœ€æ±‚ï¼ˆé«˜è¾¾75%ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä½¿å¾—åœ¨40GBæ¶ˆè´¹çº§GPUä¸Šéƒ¨ç½²70Bå‚æ•°æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚æ­¤å¤–ï¼Œé¢†åŸŸç‰¹å®šçŸ¥è¯†å’Œå¯¹é«˜çº§æç¤ºæ–¹æ³•çš„å“åº”æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¾—ä»¥ä¿ç•™ã€‚è¿™äº›å‘ç°æä¾›äº†é‡è¦çš„å®è·µå’ŒæŒ‡å¯¼ä»·å€¼ï¼Œçªå‡ºäº†é‡åŒ–ä½œä¸ºä¸€ç§å®ç”¨ä¸”æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¯ä»¥åœ¨ç”Ÿç‰©åŒ»å­¦ç¯å¢ƒä¸­å®‰å…¨åœ°æœ¬åœ°éƒ¨ç½²å¤§å‹ä¸”é«˜å®¹é‡çš„è¯­è¨€æ¨¡å‹ï¼Œä»è€Œå¼¥åˆäº†äººå·¥æ™ºèƒ½æŠ€æœ¯è¿›æ­¥ä¸ç°å®ä¸´åºŠè½¬åŒ–ä¹‹é—´çš„å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆBioNLPï¼‰é¢†åŸŸéƒ¨ç½²å›°éš¾çš„é—®é¢˜ã€‚ç°æœ‰LLMè™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†å…¶å·¨å¤§çš„æ¨¡å‹ä½“ç§¯å’Œè®¡ç®—èµ„æºéœ€æ±‚ï¼Œä½¿å¾—å®ƒä»¬éš¾ä»¥åœ¨å¯¹æ•°æ®éšç§æœ‰ä¸¥æ ¼è¦æ±‚çš„åŒ»ç–—ç¯å¢ƒä¸­åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸‹ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–äº‘éƒ¨ç½²ï¼Œè¦ä¹ˆéœ€è¦æ˜‚è´µçš„ç¡¬ä»¶ï¼Œæ— æ³•æ»¡è¶³æœ¬åœ°åŒ–éƒ¨ç½²çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œåœ¨å°½å¯èƒ½ä¸æŸå¤±æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½LLMçš„å­˜å‚¨ç©ºé—´å’Œè®¡ç®—å¤æ‚åº¦ã€‚é€šè¿‡é‡åŒ–ï¼Œå¯ä»¥å°†æ¨¡å‹å‚æ•°ä»é«˜ç²¾åº¦æµ®ç‚¹æ•°è½¬æ¢ä¸ºä½ç²¾åº¦æ•´æ•°ï¼Œä»è€Œå‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†æ—¶é—´ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) é€‰æ‹©12ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„LLMï¼ŒåŒ…æ‹¬é€šç”¨æ¨¡å‹å’Œç”Ÿç‰©åŒ»å­¦ä¸“ç”¨æ¨¡å‹ï¼›2) åœ¨8ä¸ªBioNLPåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°è¿™äº›æ¨¡å‹çš„æ€§èƒ½ï¼Œæ¶µç›–å‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–ã€å¤šæ ‡ç­¾åˆ†ç±»å’Œé—®ç­”å››ä¸ªä»»åŠ¡ï¼›3) å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œä¸åŒç¨‹åº¦çš„é‡åŒ–ï¼ˆä¾‹å¦‚ï¼Œ4-bité‡åŒ–ï¼‰ï¼›4) è¯„ä¼°é‡åŒ–åæ¨¡å‹åœ¨å„ä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ä¸åŸå§‹æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼›5) åˆ†æé‡åŒ–å¯¹æ¨¡å‹é¢†åŸŸçŸ¥è¯†å’Œæç¤ºå­¦ä¹ èƒ½åŠ›çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†é‡åŒ–æŠ€æœ¯åœ¨BioNLPé¢†åŸŸLLMä¸Šçš„æœ‰æ•ˆæ€§ã€‚ä»¥å¾€çš„ç ”ç©¶å¯èƒ½åªå…³æ³¨å•ä¸ªæ¨¡å‹æˆ–ä»»åŠ¡ï¼Œè€Œè¯¥ç ”ç©¶è¦†ç›–äº†å¤šä¸ªæ¨¡å‹ã€å¤šä¸ªä»»åŠ¡å’Œå¤šä¸ªæ•°æ®é›†ï¼Œä»è€Œå¾—å‡ºäº†æ›´å…·æ™®é€‚æ€§çš„ç»“è®ºã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å…³æ³¨äº†é‡åŒ–å¯¹æ¨¡å‹é¢†åŸŸçŸ¥è¯†å’Œæç¤ºå­¦ä¹ èƒ½åŠ›çš„å½±å“ï¼Œè¿™å¯¹äºBioNLPåº”ç”¨è‡³å…³é‡è¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©äº†å…·æœ‰ä»£è¡¨æ€§çš„LLMï¼ŒåŒ…æ‹¬BERTã€RoBERTaã€BioBERTç­‰ï¼›2) é€‰æ‹©äº†æ¶µç›–ä¸åŒBioNLPä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†ï¼Œä¾‹å¦‚BC5CDRã€ChemProtç­‰ï¼›3) é‡‡ç”¨äº†æ ‡å‡†çš„é‡åŒ–æ–¹æ³•ï¼Œä¾‹å¦‚Post-Training Quantization (PTQ)ï¼›4) ä½¿ç”¨äº†å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚F1-scoreã€å‡†ç¡®ç‡ç­‰ï¼›5) è¯¦ç»†åˆ†æäº†é‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶ç»™å‡ºäº†å®é™…éƒ¨ç½²çš„å»ºè®®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡é‡åŒ–ï¼Œå¯ä»¥å°†LLMçš„GPUå†…å­˜éœ€æ±‚é™ä½é«˜è¾¾75%ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åœ¨BioNLPä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œ70Bå‚æ•°çš„æ¨¡å‹å¯ä»¥åœ¨40GBæ¶ˆè´¹çº§GPUä¸Šè¿è¡Œã€‚æ­¤å¤–ï¼Œé‡åŒ–å¯¹æ¨¡å‹çš„é¢†åŸŸçŸ¥è¯†å’Œæç¤ºå­¦ä¹ èƒ½åŠ›çš„å½±å“è¾ƒå°ï¼Œè¿™æ„å‘³ç€é‡åŒ–åçš„æ¨¡å‹ä»ç„¶å¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºå¤æ‚çš„BioNLPä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºåŒ»ç–—å¥åº·é¢†åŸŸï¼Œä¾‹å¦‚è¾…åŠ©è¯Šæ–­ã€è¯ç‰©ç ”å‘ã€ç”µå­ç—…å†åˆ†æç­‰ã€‚é€šè¿‡é‡åŒ–æŠ€æœ¯ï¼Œå¯ä»¥åœ¨æœ¬åœ°éƒ¨ç½²é«˜æ€§èƒ½çš„LLMï¼Œä»è€Œä¿æŠ¤æ‚£è€…éšç§ï¼Œå¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚è¿™æœ‰åŠ©äºæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„æ™®åŠå’Œåº”ç”¨ï¼Œæé«˜åŒ»ç–—æœåŠ¡çš„è´¨é‡å’Œæ•ˆç‡ï¼Œå¹¶åŠ é€Ÿç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„è¿›å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models have demonstrated remarkable capabilities in biomedical natural language processing, yet their rapid growth in size and computational requirements present a major barrier to adoption in healthcare settings where data privacy precludes cloud deployment and resources are limited. In this study, we systematically evaluated the impact of quantization on 12 state-of-the-art large language models, including both general-purpose and biomedical-specific models, across eight benchmark datasets covering four key tasks: named entity recognition, relation extraction, multi-label classification, and question answering. We show that quantization substantially reduces GPU memory requirements-by up to 75%-while preserving model performance across diverse tasks, enabling the deployment of 70B-parameter models on 40GB consumer-grade GPUs. In addition, domain-specific knowledge and responsiveness to advanced prompting methods are largely maintained. These findings provide significant practical and guiding value, highlighting quantization as a practical and effective strategy for enabling the secure, local deployment of large yet high-capacity language models in biomedical contexts, bridging the gap between technical advances in AI and real-world clinical translation.

