---
layout: default
title: A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models
---

# A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03871" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03871v1</a>
  <a href="https://arxiv.org/pdf/2509.03871.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03871v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03871v1', 'A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanbo Wang, Yongcan Yu, Jian Liang, Ran He

**åˆ†ç±»**: cs.CL, cs.AI, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: 38 pages. This survey considers papers published up to June 30, 2025. Work in progress

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ybwang119/Awesome-reasoning-safety)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°æ€§ç ”ç©¶ï¼šå…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„å¯ä¿¡åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯ä¿¡æ¨ç†` `é•¿é“¾æ€ç»´` `äººå·¥æ™ºèƒ½å®‰å…¨` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç†è§£CoTæ¨ç†å¦‚ä½•å½±å“å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹å…¨é¢è¯„ä¼°ã€‚
2. è¯¥è®ºæ–‡å¯¹æ¨ç†æ¨¡å‹å’ŒCoTæŠ€æœ¯è¿›è¡Œäº†ç»¼è¿°ï¼Œèšç„¦çœŸå®æ€§ã€å®‰å…¨æ€§ã€é²æ£’æ€§ã€å…¬å¹³æ€§å’Œéšç§äº”ä¸ªå¯ä¿¡æ¨ç†ç»´åº¦ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†æŠ€æœ¯åœ¨å¢å¼ºæ¨¡å‹å¯ä¿¡åº¦æ–¹é¢æœ‰æ½œåŠ›ï¼Œä½†æ¨ç†æ¨¡å‹æœ¬èº«ä¹Ÿå¯èƒ½å­˜åœ¨å®‰å…¨ã€é²æ£’æ€§å’Œéšç§æ¼æ´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿é“¾æ€ç»´(Long-CoT)æ¨ç†çš„å‘å±•æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€ç†è§£ã€å¤æ‚é—®é¢˜è§£å†³å’Œä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¿™ç§èŒƒå¼ä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œä½†å¯¹äºåŸºäºCoTçš„æ¨ç†å¦‚ä½•å½±å“è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦çš„å…¨é¢ç†è§£ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡ç»¼è¿°äº†æœ€è¿‘å…³äºæ¨ç†æ¨¡å‹å’ŒCoTæŠ€æœ¯çš„ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨å¯ä¿¡æ¨ç†çš„äº”ä¸ªæ ¸å¿ƒç»´åº¦ï¼šçœŸå®æ€§ã€å®‰å…¨æ€§ã€é²æ£’æ€§ã€å…¬å¹³æ€§å’Œéšç§ã€‚å¯¹äºæ¯ä¸ªæ–¹é¢ï¼Œæˆ‘ä»¬æŒ‰æ—¶é—´é¡ºåºå¯¹æœ€è¿‘çš„ç ”ç©¶è¿›è¡Œäº†æ¸…æ™°è€Œç»“æ„åŒ–çš„æ¦‚è¿°ï¼Œå¹¶è¯¦ç»†åˆ†æäº†å®ƒä»¬çš„æ–¹æ³•ã€å‘ç°å’Œå±€é™æ€§ã€‚æœ€åè¿˜é™„ä¸Šäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥ä¾›å‚è€ƒå’Œè®¨è®ºã€‚æ€»çš„æ¥è¯´ï¼Œè™½ç„¶æ¨ç†æŠ€æœ¯æœ‰æœ›é€šè¿‡å‡å°‘å¹»è§‰ã€æ£€æµ‹æœ‰å®³å†…å®¹å’Œæé«˜é²æ£’æ€§æ¥å¢å¼ºæ¨¡å‹çš„å¯ä¿¡åº¦ï¼Œä½†å‰æ²¿çš„æ¨ç†æ¨¡å‹æœ¬èº«åœ¨å®‰å…¨æ€§ã€é²æ£’æ€§å’Œéšç§æ–¹é¢å¸¸å¸¸å­˜åœ¨ç›¸å½“ç”šè‡³æ›´å¤§çš„æ¼æ´ã€‚é€šè¿‡ç»¼åˆè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿæˆä¸ºäººå·¥æ™ºèƒ½å®‰å…¨ç¤¾åŒºåŠæ—¶äº†è§£æ¨ç†å¯ä¿¡åº¦æœ€æ–°è¿›å±•çš„å®è´µèµ„æºã€‚ç›¸å…³è®ºæ–‡çš„å®Œæ•´åˆ—è¡¨å¯åœ¨https://github.com/ybwang119/Awesome-reasoning-safetyæ‰¾åˆ°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯ä¿¡åº¦è¯„ä¼°ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹CoTæ¨ç†å¦‚ä½•å½±å“LLMçš„çœŸå®æ€§ã€å®‰å…¨æ€§ã€é²æ£’æ€§ã€å…¬å¹³æ€§å’Œéšç§ç­‰å…³é”®ç»´åº¦çš„å…¨é¢ç†è§£ã€‚ç°æœ‰ç ”ç©¶å¾€å¾€åªå…³æ³¨å•ä¸€ç»´åº¦ï¼Œç¼ºä¹ç³»ç»Ÿæ€§çš„åˆ†æå’Œæ•´åˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¯¹ç°æœ‰å…³äºLLMæ¨ç†å¯ä¿¡åº¦çš„ç ”ç©¶è¿›è¡Œç³»ç»Ÿæ€§çš„æ¢³ç†å’Œå½’çº³ï¼Œæ„å»ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œä»çœŸå®æ€§ã€å®‰å…¨æ€§ã€é²æ£’æ€§ã€å…¬å¹³æ€§å’Œéšç§äº”ä¸ªç»´åº¦æ¥è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åˆ†æç°æœ‰æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥è®ºæ–‡é‡‡ç”¨ç»¼è¿°ç ”ç©¶çš„æ–¹æ³•ï¼Œæ²¡æœ‰æå‡ºæ–°çš„æŠ€æœ¯æ¡†æ¶ã€‚å…¶ä¸»è¦å·¥ä½œæ˜¯ï¼š
1.  æ”¶é›†å’Œæ•´ç†è¿‘å¹´æ¥å…³äºLLMæ¨ç†å’ŒCoTæŠ€æœ¯çš„ç ”ç©¶è®ºæ–‡ã€‚
2.  å°†è¿™äº›è®ºæ–‡æŒ‰ç…§çœŸå®æ€§ã€å®‰å…¨æ€§ã€é²æ£’æ€§ã€å…¬å¹³æ€§å’Œéšç§äº”ä¸ªç»´åº¦è¿›è¡Œåˆ†ç±»ã€‚
3.  å¯¹æ¯ä¸ªç»´åº¦ä¸‹çš„ç ”ç©¶è¿›è¡Œè¯¦ç»†åˆ†æï¼ŒåŒ…æ‹¬å…¶æ–¹æ³•ã€å‘ç°å’Œå±€é™æ€§ã€‚
4.  æ€»ç»“ç°æœ‰ç ”ç©¶çš„ä¸è¶³ï¼Œå¹¶æå‡ºæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç³»ç»Ÿæ€§çš„è§†è§’å’Œå…¨é¢çš„åˆ†ææ¡†æ¶ã€‚å®ƒé¦–æ¬¡å°†LLMæ¨ç†çš„å¯ä¿¡åº¦åˆ†è§£ä¸ºäº”ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå¹¶å¯¹æ¯ä¸ªç»´åº¦ä¸‹çš„ç ”ç©¶è¿›è¡Œäº†æ·±å…¥çš„åˆ†æå’Œæ¯”è¾ƒã€‚è¿™ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„è“å›¾ï¼Œå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°ç†è§£LLMæ¨ç†çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥è®ºæ–‡æ²¡æœ‰æ¶‰åŠå…·ä½“çš„æŠ€æœ¯è®¾è®¡ï¼Œè€Œæ˜¯åœ¨ç»¼è¿°çš„åŸºç¡€ä¸Šï¼Œå¯¹æ¯ä¸ªç»´åº¦ä¸‹çš„ç ”ç©¶æ–¹æ³•è¿›è¡Œäº†æ€»ç»“å’Œåˆ†ç±»ã€‚ä¾‹å¦‚ï¼Œåœ¨çœŸå®æ€§æ–¹é¢ï¼Œè®ºæ–‡åˆ†æäº†å„ç§å‡å°‘å¹»è§‰çš„æ–¹æ³•ï¼›åœ¨å®‰å…¨æ€§æ–¹é¢ï¼Œè®ºæ–‡è®¨è®ºäº†å¦‚ä½•æ£€æµ‹å’Œé˜²æ­¢LLMç”Ÿæˆæœ‰å®³å†…å®¹ã€‚è¿™äº›æ€»ç»“å’Œåˆ†ç±»ä¸ºç ”ç©¶äººå‘˜æä¾›äº†å®è´µçš„å‚è€ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°è®ºæ–‡ç³»ç»Ÿåœ°åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„äº”ä¸ªå…³é”®å¯ä¿¡åº¦ç»´åº¦ï¼šçœŸå®æ€§ã€å®‰å…¨æ€§ã€é²æ£’æ€§ã€å…¬å¹³æ€§å’Œéšç§ã€‚ç ”ç©¶æ­ç¤ºäº†ç°æœ‰æ¨ç†æ¨¡å‹åœ¨æå‡æŸäº›ç»´åº¦å¯ä¿¡åº¦çš„åŒæ—¶ï¼Œå¯èƒ½åœ¨å…¶ä»–ç»´åº¦ä¸Šå­˜åœ¨æ›´å¤§çš„æ¼æ´ã€‚ä¾‹å¦‚ï¼ŒCoTæŠ€æœ¯è™½ç„¶èƒ½æé«˜å‡†ç¡®æ€§ï¼Œä½†ä¹Ÿå¯èƒ½æ”¾å¤§å®‰å…¨å’Œéšç§é£é™©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºäººå·¥æ™ºèƒ½å®‰å…¨è¯„ä¼°ã€æ¨¡å‹é£é™©ç®¡ç†ã€ä»¥åŠæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ä¸ªé¢†åŸŸçš„å¯é æ€§ã€‚é€šè¿‡æ›´å…¨é¢åœ°ç†è§£å’Œè¯„ä¼°LLMæ¨ç†çš„å¯ä¿¡åº¦ï¼Œå¯ä»¥ä¿ƒè¿›å…¶åœ¨åŒ»ç–—ã€é‡‘èã€æ³•å¾‹ç­‰é«˜é£é™©é¢†åŸŸçš„å®‰å…¨åº”ç”¨ï¼Œå¹¶ä¸ºæœªæ¥çš„æ¨¡å‹æ”¹è¿›æä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.

