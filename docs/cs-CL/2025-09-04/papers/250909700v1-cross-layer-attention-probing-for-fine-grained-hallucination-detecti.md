---
layout: default
title: Cross-Layer Attention Probing for Fine-Grained Hallucination Detection
---

# Cross-Layer Attention Probing for Fine-Grained Hallucination Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09700" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09700v1</a>
  <a href="https://arxiv.org/pdf/2509.09700.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09700v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09700v1', 'Cross-Layer Attention Probing for Fine-Grained Hallucination Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Malavika Suresh, Rahaf Aljundi, Ikechukwu Nkisi-Orji, Nirmalie Wiratunga

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: To be published at the TRUST-AI workshop, ECAI 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨å±‚æ³¨æ„åŠ›æ¢æµ‹(CLAP)æŠ€æœ¯ï¼Œç”¨äºç»†ç²’åº¦åœ°æ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰ç°è±¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¹»è§‰æ£€æµ‹` `è·¨å±‚æ³¨æ„åŠ›` `æ¿€æ´»æ¢æµ‹` `å¯é æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œé™ä½äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥è¿›è¡Œç»†ç²’åº¦çš„å¹»è§‰æ£€æµ‹ã€‚
2. è®ºæ–‡æå‡ºè·¨å±‚æ³¨æ„åŠ›æ¢æµ‹(CLAP)æŠ€æœ¯ï¼Œå°†æ•´ä¸ªæ®‹å·®æµçš„æ¿€æ´»è§†ä¸ºè”åˆåºåˆ—ï¼Œä»è€Œæ›´å…¨é¢åœ°æ•æ‰å¹»è§‰ç‰¹å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCLAPåœ¨ä¸åŒè§£ç ç­–ç•¥å’Œåˆ†å¸ƒå¤–æ•°æ®ä¸Šå‡èƒ½æœ‰æ•ˆæ£€æµ‹å¹»è§‰ï¼Œå¹¶ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œæå‡äº†LLMçš„å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å„ç§åº”ç”¨ä¸­çš„å¤§è§„æ¨¡é‡‡ç”¨ï¼Œç”±äºå®ƒä»¬å€¾å‘äºç”Ÿæˆä¸å‡†ç¡®çš„æ–‡æœ¬ï¼Œå³å¹»è§‰ï¼Œå› æ­¤å¯é æ€§é—®é¢˜æ—¥ç›Šä¸¥é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¿€æ´»æ¢æµ‹æŠ€æœ¯â€”â€”è·¨å±‚æ³¨æ„åŠ›æ¢æµ‹(CLAP)ï¼Œç”¨äºå¹»è§‰æ£€æµ‹ï¼Œå®ƒå°†æ•´ä¸ªæ®‹å·®æµä¸­çš„LLMæ¿€æ´»ä½œä¸ºè”åˆåºåˆ—è¿›è¡Œå¤„ç†ã€‚ä½¿ç”¨äº”ä¸ªLLMå’Œä¸‰ä¸ªä»»åŠ¡çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒCLAPæ”¹è¿›äº†å¹»è§‰æ£€æµ‹ï¼Œæ— è®ºæ˜¯å¯¹äºè´ªå©ªè§£ç çš„å“åº”è¿˜æ˜¯åœ¨è¾ƒé«˜æ¸©åº¦ä¸‹é‡‡æ ·çš„å“åº”ï¼Œä»è€Œå®ç°äº†ç»†ç²’åº¦çš„æ£€æµ‹ï¼Œå³åŒºåˆ†ç»™å®šæç¤ºçš„ä¸åŒé‡‡æ ·å“åº”ä¸­çš„å¹»è§‰å’Œéå¹»è§‰çš„èƒ½åŠ›ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæå‡ºä¸€ç§ä½¿ç”¨CLAPçš„æ£€æµ‹-ç„¶å-ç¼“è§£ç­–ç•¥ï¼Œä»¥å‡å°‘å¹»è§‰å¹¶æé«˜LLMçš„å¯é æ€§ï¼Œä¼˜äºç›´æ¥ç¼“è§£æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œå³ä½¿åœ¨åˆ†å¸ƒå¤–åº”ç”¨æ—¶ï¼ŒCLAPä¹Ÿèƒ½ä¿æŒè¾ƒé«˜çš„å¯é æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œç»å¸¸ä¼šäº§ç”Ÿä¸äº‹å®ä¸ç¬¦æˆ–æ— æ„ä¹‰çš„å†…å®¹ï¼Œå³â€œå¹»è§‰â€ã€‚ç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•é€šå¸¸ä¸å¤Ÿç²¾ç¡®ï¼Œéš¾ä»¥åŒºåˆ†åŒä¸€æç¤ºä¸‹ä¸åŒç”Ÿæˆç»“æœä¸­çš„å¹»è§‰å’Œéå¹»è§‰éƒ¨åˆ†ï¼Œä¹Ÿéš¾ä»¥åœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMå†…éƒ¨çš„è·¨å±‚æ³¨æ„åŠ›ä¿¡æ¯æ¥æ¢æµ‹å¹»è§‰ã€‚ä½œè€…è®¤ä¸ºï¼ŒLLMåœ¨ç”Ÿæˆå¹»è§‰æ—¶ï¼Œå…¶å†…éƒ¨çš„æ³¨æ„åŠ›æ¨¡å¼ä¼šä¸ç”ŸæˆçœŸå®ä¿¡æ¯æ—¶æœ‰æ‰€ä¸åŒã€‚é€šè¿‡åˆ†ææ•´ä¸ªæ®‹å·®æµä¸­çš„æ¿€æ´»ï¼Œå¯ä»¥æ›´å…¨é¢åœ°æ•æ‰åˆ°è¿™ç§å·®å¼‚ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„å¹»è§‰æ£€æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLAP (Cross-Layer Attention Probing) çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š1) ç»™å®šä¸€ä¸ªæç¤ºå’ŒLLMç”Ÿæˆçš„å¤šä¸ªå“åº”ï¼›2) æå–LLMæ¯ä¸€å±‚çš„æ¿€æ´»å€¼ï¼Œå½¢æˆä¸€ä¸ªè·¨å±‚çš„æ¿€æ´»åºåˆ—ï¼›3) ä½¿ç”¨ä¸€ä¸ªåˆ†ç±»å™¨ï¼ˆä¾‹å¦‚çº¿æ€§å±‚æˆ–MLPï¼‰å¯¹è¯¥æ¿€æ´»åºåˆ—è¿›è¡Œå¤„ç†ï¼Œé¢„æµ‹æ¯ä¸ªå“åº”æ˜¯å¦åŒ…å«å¹»è§‰ï¼›4) å¯ä»¥åˆ©ç”¨æ£€æµ‹ç»“æœï¼Œé€‰æ‹©æˆ–ä¿®æ”¹å“åº”ï¼Œä»è€Œç¼“è§£å¹»è§‰é—®é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šCLAPçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è·¨å±‚æ³¨æ„åŠ›æ¢æµ‹çš„æ–¹æ³•ã€‚ä¸ä»¥å¾€åªå…³æ³¨æœ€åä¸€å±‚æˆ–å°‘æ•°å‡ å±‚çš„æ¿€æ´»ä¸åŒï¼ŒCLAPå……åˆ†åˆ©ç”¨äº†æ•´ä¸ªæ®‹å·®æµçš„ä¿¡æ¯ï¼Œä»è€Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•æ‰åˆ°LLMå†…éƒ¨çš„å¹»è§‰ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒCLAPè¿˜èƒ½å¤Ÿè¿›è¡Œç»†ç²’åº¦çš„å¹»è§‰æ£€æµ‹ï¼ŒåŒºåˆ†åŒä¸€æç¤ºä¸‹ä¸åŒç”Ÿæˆç»“æœä¸­çš„å¹»è§‰å’Œéå¹»è§‰éƒ¨åˆ†ã€‚

**å…³é”®è®¾è®¡**ï¼šCLAPçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•æœ‰æ•ˆåœ°æå–å’Œè¡¨ç¤ºè·¨å±‚æ¿€æ´»ä¿¡æ¯ï¼›2) å¦‚ä½•è®­ç»ƒåˆ†ç±»å™¨ä»¥åŒºåˆ†å¹»è§‰å’Œéå¹»è§‰å“åº”ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†ç®€å•çš„çº¿æ€§å±‚æˆ–MLPä½œä¸ºåˆ†ç±»å™¨ï¼Œå¹¶é€šè¿‡äº¤å‰ç†µæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å¯èƒ½éœ€è¦æ ¹æ®ä¸åŒçš„LLMå’Œä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCLAPåœ¨äº”ä¸ªLLMå’Œä¸‰ä¸ªä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨é«˜æ¸©åº¦é‡‡æ ·çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ£€æµ‹å¹»è§‰ã€‚æ­¤å¤–ï¼ŒCLAPåœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šä¹Ÿèƒ½ä¿æŒè¾ƒé«˜çš„å¯é æ€§ï¼Œè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç›´æ¥ç¼“è§£æ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨CLAPçš„æ£€æµ‹-ç„¶å-ç¼“è§£ç­–ç•¥èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å‡å°‘å¹»è§‰å¹¶æé«˜LLMçš„å¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦LLMç”Ÿæˆå¯é æ–‡æœ¬çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€å†…å®¹åˆ›ä½œã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡æ£€æµ‹å’Œç¼“è§£LLMçš„å¹»è§‰ï¼Œå¯ä»¥æé«˜ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å’Œå¯ä¿¡åº¦ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œåº”ç”¨ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the large-scale adoption of Large Language Models (LLMs) in various applications, there is a growing reliability concern due to their tendency to generate inaccurate text, i.e. hallucinations. In this work, we propose Cross-Layer Attention Probing (CLAP), a novel activation probing technique for hallucination detection, which processes the LLM activations across the entire residual stream as a joint sequence. Our empirical evaluations using five LLMs and three tasks show that CLAP improves hallucination detection compared to baselines on both greedy decoded responses as well as responses sampled at higher temperatures, thus enabling fine-grained detection, i.e. the ability to disambiguate hallucinations and non-hallucinations among different sampled responses to a given prompt. This allows us to propose a detect-then-mitigate strategy using CLAP to reduce hallucinations and improve LLM reliability compared to direct mitigation approaches. Finally, we show that CLAP maintains high reliability even when applied out-of-distribution.

