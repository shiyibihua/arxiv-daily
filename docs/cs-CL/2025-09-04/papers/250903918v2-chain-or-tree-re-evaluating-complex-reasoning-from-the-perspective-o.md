---
layout: default
title: Chain or tree? Re-evaluating complex reasoning from the perspective of a matrix of thought
---

# Chain or tree? Re-evaluating complex reasoning from the perspective of a matrix of thought

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03918" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.03918v2</a>
  <a href="https://arxiv.org/pdf/2509.03918.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03918v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03918v2', 'Chain or tree? Re-evaluating complex reasoning from the perspective of a matrix of thought')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Fengxiao Tang, Yufeng Li, Zongzong Wu, Ming Zhao

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-04 (Êõ¥Êñ∞: 2025-09-26)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/lyfiter/mtqa)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Áü©ÈòµÊÄùÁª¥ÔºàMoTÔºâÊ°ÜÊû∂ÔºåÊèêÂçáLLMÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Â§çÊùÇÊé®ÁêÜ` `Áü©ÈòµÊÄùÁª¥` `Áü•ËØÜÂõæË∞±` `Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊÄùÁª¥ÈìæÂíåÊÄùÁª¥Ê†ëÊñπÊ≥ïÂú®Â§çÊùÇÊé®ÁêÜ‰∏≠Â≠òÂú®ÂÜó‰ΩôÂíåË∑ØÂæÑÂçï‰∏ÄÈóÆÈ¢òÔºå‰∏îÊòìÂèóÊ£ÄÁ¥¢Âà∞ÁöÑÈîôËØØÁü•ËØÜËØØÂØº„ÄÇ
2. ÊèêÂá∫Áü©ÈòµÊÄùÁª¥ÔºàMoTÔºâÊ°ÜÊû∂ÔºåÈÄöËøáÂàó-ÂçïÂÖÉÈÄö‰ø°Êú∫Âà∂ËøõË°åÂ§öÁ≠ñÁï•Ê∑±Â∫¶ÊÄùËÄÉÔºåÂπ∂ÁªìÂêà‰∫ãÂÆûÁ∫†Ê≠£Êú∫Âà∂„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMoTÂú®24ÁÇπÊ∏∏Êàè„ÄÅÈóÆÈ¢òÂõûÁ≠îÂíåÂëΩÈ¢òÂÜô‰Ωú‰ªªÂä°‰∏≠‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊé®ÁêÜÊó∂Èó¥ÊòæËëóÈôç‰Ωé„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÂ§çÊùÇÂíåÊäΩË±°‰ªªÂä°Êó∂ÔºåÁî±‰∫éÊé®ÁêÜËÉΩÂäõ‰∏çË∂≥ÔºåÂáÜÁ°ÆÊÄß‰ºöÊòæËëó‰∏ãÈôç„ÄÇÊÄùÁª¥ÈìæÔºàCoTÔºâÂíåÊÄùÁª¥Ê†ëÔºàToTÔºâÁ≠âÊÄùÁª¥ÁªìÊûÑÊó®Âú®Â¢ûÂº∫LLMsÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÂ≠òÂú®Âõ∫ÊúâÁº∫Èô∑ÔºåÂ¶ÇÊ†ëÁªìÊûÑ‰∏≠Âêå‰∏ÄÂ±ÇÂÜÖÁöÑÂÜó‰ΩôÂíåÈìæÁªìÊûÑ‰∏≠Ë∑ØÂæÑÁöÑÂçï‰∏ÄÊÄß„ÄÇ‰∏Ä‰∫õÁ†îÁ©∂Âà©Áî®Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊñπÊ≥ïÊù•Â¢ûÂº∫CoTÂíåToTÔºå‰ª•ÂáèËΩªLLMs‰∏≠ÁöÑÂπªËßâÔºå‰ΩÜÊÄùÁª¥ÁªìÊûÑÁöÑÂü∫Êú¨Áº∫ÁÇπ‰ªçÁÑ∂Â≠òÂú®„ÄÇÊ≠§Â§ñÔºåÂú®Â§ÑÁêÜÂ§öÂÆû‰ΩìÂíåÂ§öË∑≥‰ø°ÊÅØÊó∂ÔºåÊ£ÄÁ¥¢Âà∞ÁöÑÈ™åËØÅÁü•ËØÜÈÄöÂ∏∏ÂåÖÂê´Â§ßÈáèÁ¢éÁâáÂåñ„ÄÅË°®Èù¢ÂåñÁîöËá≥ÈîôËØØÁöÑÊï∞ÊçÆÔºåËØØÂØºLLMsÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñËÄåÈ´òÊïàÁöÑLLMsÊÄùÁª¥ÁªìÊûÑ‚Äî‚ÄîÁü©ÈòµÊÄùÁª¥ÔºàMoTÔºâ„ÄÇMoTÈÄöËøá‚ÄúÂàó-ÂçïÂÖÉÈÄö‰ø°‚ÄùÊú∫Âà∂Âú®Ê∞¥Âπ≥ÂíåÂûÇÁõ¥Áª¥Â∫¶‰∏äÊé¢Á¥¢ÈóÆÈ¢òÔºå‰ΩøLLMsËÉΩÂ§üÁßØÊûÅÂèÇ‰∏éÂ§öÁ≠ñÁï•ÂíåÊ∑±Â∫¶ÊÄùËÄÉÔºåÂêåÊó∂ÂáèÂ∞ëÂàóÂçïÂÖÉÂÜÖÊÄùÁª¥ËäÇÁÇπÁöÑÂÜó‰ΩôÔºå‰ªéËÄåÂ¢ûÂº∫LLMsÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÈÄöËøá‰∫ãÂÆûÁ∫†Ê≠£Êú∫Âà∂ÔºåÂÆÉÂà©Áî®RAGÊ£ÄÁ¥¢Âà∞ÁöÑÁü•ËØÜÂõæË∞±‰∏âÂÖÉÁªÑÂíåÂéüÂßãÊñáÊú¨Êù•ÊûÑÂª∫Áü•ËØÜÂçïÂÖÉÂπ∂Á∫†Ê≠£ÈîôËØØÁ≠îÊ°à„ÄÇ‰∏∫‰∫ÜÈ™åËØÅËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊàë‰ª¨Âú®24ÁÇπÊ∏∏Êàè„ÄÅÈóÆÈ¢òÂõûÁ≠îËØÑ‰º∞ÂíåÂëΩÈ¢òÂÜô‰Ωú‰∏â‰∏™‰ªªÂä°‰∏≠ËøõË°å‰∫ÜÂ§ßÈáèÂÆûÈ™å„ÄÇÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÊé®ÁêÜÊó∂Èó¥‰ªÖ‰∏∫Âü∫Á∫øÊñπÊ≥ïÁöÑ14.4ÔºÖÔºåËØÅÊòé‰∫ÜÂÖ∂ÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÂáÜÁ°ÆÊÄß‰∏ãÈôçÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÊÄùÁª¥ÈìæÔºàCoTÔºâÂíåÊÄùÁª¥Ê†ëÔºàToTÔºâÔºåÂ≠òÂú®ÂÜó‰Ωô„ÄÅË∑ØÂæÑÂçï‰∏Ä‰ª•ÂèäÊòìÂèóÈîôËØØÊ£ÄÁ¥¢Áü•ËØÜËØØÂØºÁ≠âÁóõÁÇπ„ÄÇËøô‰∫õÈóÆÈ¢òÈôêÂà∂‰∫ÜLLMÂú®ÈúÄË¶ÅÂ§öÊ≠•Êé®ÁêÜÂíåÂ§ñÈÉ®Áü•ËØÜÈ™åËØÅÁöÑ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Áü©ÈòµÂºèÁöÑÊÄùÁª¥ÁªìÊûÑÔºåÂÖÅËÆ∏LLMÂú®Ê∞¥Âπ≥ÂíåÂûÇÁõ¥ÊñπÂêë‰∏äÊé¢Á¥¢ÈóÆÈ¢ò„ÄÇÈÄöËøá‚ÄúÂàó-ÂçïÂÖÉÈÄö‰ø°‚ÄùÊú∫Âà∂ÔºåÈºìÂä±LLMËøõË°åÂ§öÁ≠ñÁï•ÂíåÊ∑±Â∫¶ÊÄùËÄÉÔºåÂêåÊó∂ÂáèÂ∞ëÂÜó‰Ωô„ÄÇÊ≠§Â§ñÔºåÂà©Áî®Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÂíå‰∫ãÂÆûÁ∫†Ê≠£Êú∫Âà∂ÔºåÊèêÈ´òÁü•ËØÜÁöÑÂáÜÁ°ÆÊÄßÔºåÈÅøÂÖçLLMÂèóÂà∞ÈîôËØØ‰ø°ÊÅØÁöÑËØØÂØº„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMoTÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ÈóÆÈ¢òÂàÜËß£ÔºöÂ∞ÜÂ§çÊùÇÈóÆÈ¢òÂàÜËß£‰∏∫Â§ö‰∏™Â≠êÈóÆÈ¢ò„ÄÇ2) Áü©ÈòµÊûÑÂª∫ÔºöÊûÑÂª∫‰∏Ä‰∏™Áü©ÈòµÔºåÂÖ∂‰∏≠ÊØè‰∏ÄÂàó‰ª£Ë°®‰∏ÄÁßçÊé®ÁêÜÁ≠ñÁï•ÔºåÊØè‰∏ÄË°å‰ª£Ë°®Êé®ÁêÜÁöÑ‰∏≠Èó¥Ê≠•È™§„ÄÇ3) Âàó-ÂçïÂÖÉÈÄö‰ø°ÔºöLLMÂú®Áü©ÈòµÁöÑÂçïÂÖÉÊ†º‰∏≠ËøõË°åÊé®ÁêÜÔºåÂπ∂ÈÄöËøáÂàó‰πãÈó¥ÁöÑÈÄö‰ø°ÂÖ±‰∫´‰ø°ÊÅØ„ÄÇ4) Áü•ËØÜÊ£ÄÁ¥¢‰∏é‰∫ãÂÆûÁ∫†Ê≠£ÔºöÂà©Áî®RAGÊ£ÄÁ¥¢Áõ∏ÂÖ≥Áü•ËØÜÔºåÂπ∂‰ΩøÁî®‰∫ãÂÆûÁ∫†Ê≠£Êú∫Âà∂Á∫†Ê≠£ÈîôËØØ‰ø°ÊÅØ„ÄÇ5) Á≠îÊ°àÁîüÊàêÔºöÂü∫‰∫éÁü©Èòµ‰∏≠ÁöÑÊé®ÁêÜÁªìÊûúÁîüÊàêÊúÄÁªàÁ≠îÊ°à„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMoTÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Áü©ÈòµÂºèÁöÑÊÄùÁª¥ÁªìÊûÑÂíåÂàó-ÂçïÂÖÉÈÄö‰ø°Êú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑÈìæÂºèÊàñÊ†ëÁä∂ÁªìÊûÑÁõ∏ÊØîÔºåMoTËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞Êé¢Á¥¢ÈóÆÈ¢òÁ©∫Èó¥ÔºåÂπ∂ÂáèÂ∞ëÂÜó‰Ωô„ÄÇÊ≠§Â§ñÔºå‰∫ãÂÆûÁ∫†Ê≠£Êú∫Âà∂ËÉΩÂ§üÊúâÊïàÊèêÈ´òÁü•ËØÜÁöÑÂáÜÁ°ÆÊÄßÔºåÈÅøÂÖçLLMÂèóÂà∞ÈîôËØØ‰ø°ÊÅØÁöÑËØØÂØº„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMoTÊ°ÜÊû∂‰∏≠ÔºåÂàóÁöÑÊï∞Èáè‰ª£Ë°®Êé®ÁêÜÁ≠ñÁï•ÁöÑÂ§öÊ†∑ÊÄßÔºåË°åÁöÑÊï∞Èáè‰ª£Ë°®Êé®ÁêÜÁöÑÊ∑±Â∫¶„ÄÇÂàó-ÂçïÂÖÉÈÄö‰ø°Êú∫Âà∂ÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÊñπÂºèÊú™Áü•ÔºåÂèØËÉΩÊ∂âÂèäÊ≥®ÊÑèÂäõÊú∫Âà∂ÊàñÊ∂àÊÅØ‰º†ÈÄíÊú∫Âà∂„ÄÇ‰∫ãÂÆûÁ∫†Ê≠£Êú∫Âà∂ÁöÑËÆæËÆ°ÁªÜËäÇ‰πüÊú™Áü•ÔºåÂèØËÉΩÊ∂âÂèäÁü•ËØÜÂõæË∞±Êé®ÁêÜÊàñÊñáÊú¨Ëï¥Âê´ËØÜÂà´Á≠âÊäÄÊúØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMoTÊ°ÜÊû∂Âú®24ÁÇπÊ∏∏Êàè„ÄÅÈóÆÈ¢òÂõûÁ≠îËØÑ‰º∞ÂíåÂëΩÈ¢òÂÜô‰Ωú‰∏â‰∏™‰ªªÂä°‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÂ∞§ÂÖ∂ÊòØÂú®Êé®ÁêÜÊó∂Èó¥ÊñπÈù¢ÔºåMoT‰ªÖ‰∏∫Âü∫Á∫øÊñπÊ≥ïÁöÑ14.4%ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÊïàÁéá„ÄÇËøôË°®ÊòéMoTÂú®‰øùËØÅÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂Ôºå‰πüÂÖ∑ÊúâÂæàÈ´òÁöÑÂÆûÁî®‰ª∑ÂÄº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÈúÄË¶ÅÂ§çÊùÇÊé®ÁêÜÂíåÁü•ËØÜÈ™åËØÅÁöÑÈ¢ÜÂüüÔºåÂ¶ÇÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅËá™Âä®ÂëΩÈ¢òÁîüÊàê„ÄÅÊ∏∏ÊàèAIÁ≠â„ÄÇÈÄöËøáÊèêÈ´òLLMÁöÑÊé®ÁêÜËÉΩÂäõÂíåÁü•ËØÜÂáÜÁ°ÆÊÄßÔºåÂèØ‰ª•ÊèêÂçáËøô‰∫õÂ∫îÁî®ÁöÑÁî®Êà∑‰ΩìÈ™åÂíåÊÄßËÉΩÔºåÂπ∂ÊúâÊúõÂú®ÊïôËÇ≤„ÄÅÁßëÁ†îÁ≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) face significant accuracy degradation due to insufficient reasoning ability when dealing with complex and abstract tasks. Thought structures such as Chain of Thought (CoT) and Tree of Thought (ToT) focus on enhancing the reasoning capability of LLMs. However, they suffer from inherent drawbacks such as redundancy within the same layer of the tree structure and the singularity of the paths in the chain structure. Some studies have utilized Retrieval-Augmented Generation (RAG) methods to enhance CoT and ToT in mitigating hallucinations in LLMs, yet the fundamental shortcomings of the thought structures still persist. Furthermore, when dealing with multi-entity and multi-hop information, the retrieved verification knowledge often contains large amounts of fragmented, superficial, or even erroneous data, misleading the reasoning process of LLMs. To address these issues, we propose the Matrix of Thought (MoT), a novel and efficient thought structure for LLMs. MoT explores problems in both horizontal and vertical dimensions through a "column-cell communication" mechanism, enabling LLMs to actively engage in multi-strategy and deep thinking while reducing redundancy in the thought nodes within the column cells, thereby enhancing the reasoning capability of LLMs. Additionally, through a fact-correction mechanism, it leverages the knowledge graph triples retrieved by RAG and the original text to construct knowledge units and correct erroneous answers. To validate the effectiveness of this method, we conducted extensive experiments in three tasks: 24-point game, question answering evaluation, and proposition writing.The results demonstrate that our framework outperforms state-of-the-art methods, with reasoning time only 14.4\% of that of the baseline method, proving its efficiency and accuracy. The code for framework is available at https://github.com/lyfiter/mtqa.

