---
layout: default
title: Measuring and Guiding Monosemanticity
---

# Measuring and Guiding Monosemanticity

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19382" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19382v2</a>
  <a href="https://arxiv.org/pdf/2506.19382.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19382v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19382v2', 'Measuring and Guiding Monosemanticity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruben HÃ¤rle, Felix Friedrich, Manuel Brack, Stephan WÃ¤ldchen, BjÃ¶rn Deiseroth, Patrick Schramowski, Kristian Kersting

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24 (æ›´æ–°: 2025-12-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç‰¹å¾å•ä¹‰æ€§è¯„åˆ†ä»¥è§£å†³ç‰¹å¾è¡¨ç¤ºæ“æ§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç‰¹å¾å•ä¹‰æ€§` `ç¨€ç–è‡ªç¼–ç å™¨` `æœºæ¢°è§£é‡Šæ€§` `å¯æ§æ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ½œåœ¨è¡¨ç¤º` `æ¯’æ€§æ£€æµ‹` `å†™ä½œé£æ ¼è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç‰¹å¾è¡¨ç¤ºçš„å®šä½å’Œæ“æ§ä¸Šå­˜åœ¨å¯é æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´ç†è§£å’Œæ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨åŠ¨æ€å˜å¾—å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºç‰¹å¾å•ä¹‰æ€§è¯„åˆ†ï¼ˆFMSï¼‰ä½œä¸ºé‡åŒ–ç‰¹å¾å•ä¹‰æ€§çš„æŒ‡æ ‡ï¼Œå¹¶å¼•å…¥å¼•å¯¼ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆG-SAEï¼‰ï¼Œé€šè¿‡æ ‡è®°æ¦‚å¿µæ¡ä»¶åŒ–æ½œåœ¨è¡¨ç¤ºã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒG-SAEåœ¨å¤šä¸ªä»»åŠ¡ä¸­æå‡äº†å•ä¹‰æ€§ï¼Œå¢å¼ºäº†å¯¹ç›®æ ‡æ¦‚å¿µçš„å®šä½å’Œæ“æ§èƒ½åŠ›ï¼Œä¸”è´¨é‡ä¸‹é™å¹…åº¦è¾ƒå°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¯¹æœºæ¢°è§£é‡Šæ€§å’Œå¯æ§æ€§çš„å…³æ³¨å¢åŠ ï¼Œç ”ç©¶è€…å¸Œæœ›æ›´å¥½åœ°ç†è§£å’Œå½±å“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†…éƒ¨åŠ¨æ€ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¯é å®šä½å’Œæ“æ§ç‰¹å¾è¡¨ç¤ºæ–¹é¢é¢ä¸´æ ¹æœ¬æ€§æŒ‘æˆ˜ã€‚ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰ä½œä¸ºä¸€ç§ç‰¹å¾æå–çš„æ–°æ–¹å‘ï¼Œè™½ç„¶æœ‰æ½œåŠ›ï¼Œä½†åœ¨ç‰¹å¾éš”ç¦»å’Œå•ä¹‰æ€§æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºç‰¹å¾å•ä¹‰æ€§è¯„åˆ†ï¼ˆFMSï¼‰ï¼Œé‡åŒ–æ½œåœ¨è¡¨ç¤ºä¸­çš„ç‰¹å¾å•ä¹‰æ€§ï¼Œå¹¶æå‡ºå¼•å¯¼ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆG-SAEï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®æ ‡è®°æ¦‚å¿µæ¡ä»¶åŒ–æ½œåœ¨è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒG-SAEåœ¨æ¯’æ€§æ£€æµ‹ã€å†™ä½œé£æ ¼è¯†åˆ«å’Œéšç§å±æ€§è¯†åˆ«ä¸­æ˜¾è‘—æå‡äº†å•ä¹‰æ€§å’Œæ“æ§æ•ˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç‰¹å¾è¡¨ç¤ºæ–¹æ³•åœ¨å®šä½å’Œæ“æ§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¨€ç–è‡ªç¼–ç å™¨åœ¨ç‰¹å¾éš”ç¦»å’Œå•ä¹‰æ€§æ–¹é¢çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ç‰¹å¾å•ä¹‰æ€§è¯„åˆ†ï¼ˆFMSï¼‰ï¼Œç³»ç»Ÿé‡åŒ–ç‰¹å¾å•ä¹‰æ€§ï¼Œå¹¶æå‡ºå¼•å¯¼ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆG-SAEï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆæ ‡è®°æ¦‚å¿µä»¥æ”¹å–„æ½œåœ¨è¡¨ç¤ºçš„å¯è§£é‡Šæ€§å’Œæ“æ§æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šG-SAEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾å•ä¹‰æ€§è¯„åˆ†çš„è®¡ç®—æ¨¡å—å’Œæ¡ä»¶åŒ–è®­ç»ƒæ¨¡å—ã€‚å‰è€…ç”¨äºè¯„ä¼°ç‰¹å¾çš„å•ä¹‰æ€§ï¼Œåè€…åˆ™é€šè¿‡æ ‡è®°æ¦‚å¿µå¼•å¯¼æ½œåœ¨è¡¨ç¤ºçš„å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥FMSä½œä¸ºé‡åŒ–å·¥å…·ï¼Œå¹¶é€šè¿‡G-SAEå®ç°æ½œåœ¨è¡¨ç¤ºçš„æ¡ä»¶åŒ–è®­ç»ƒï¼Œä»è€Œæ˜¾è‘—æå‡äº†ç‰¹å¾çš„å•ä¹‰æ€§å’Œæ“æ§èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨G-SAEä¸­ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç‰¹å¾å•ä¹‰æ€§ï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­ä¿æŒé«˜æ•ˆçš„ç‰¹å¾æå–å’Œæ“æ§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒæµç¨‹ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æ”¯æŒæ¡ä»¶åŒ–å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒG-SAEåœ¨æ¯’æ€§æ£€æµ‹ä»»åŠ¡ä¸­ç›¸æ¯”åŸºçº¿æ–¹æ³•æå‡äº†ç‰¹å¾å•ä¹‰æ€§å¾—åˆ†20%ï¼Œåœ¨å†™ä½œé£æ ¼è¯†åˆ«ä¸­æå‡äº†å‡†ç¡®ç‡15%ã€‚æ­¤å¤–ï¼Œéšç§å±æ€§è¯†åˆ«çš„æ“æ§æ•ˆæœä¹Ÿæ˜¾è‘—å¢å¼ºï¼Œè´¨é‡ä¸‹é™å¹…åº¦å°äº5%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬ç”Ÿæˆå’Œé£æ ¼è¿ç§»ç­‰ä»»åŠ¡ã€‚é€šè¿‡æå‡ç‰¹å¾å•ä¹‰æ€§å’Œæ“æ§èƒ½åŠ›ï¼Œç ”ç©¶æˆæœèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´å…·å¯è§£é‡Šæ€§å’Œå¯æ§æ€§çš„è¯­è¨€æ¨¡å‹ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> There is growing interest in leveraging mechanistic interpretability and controllability to better understand and influence the internal dynamics of large language models (LLMs). However, current methods face fundamental challenges in reliably localizing and manipulating feature representations. Sparse Autoencoders (SAEs) have recently emerged as a promising direction for feature extraction at scale, yet they, too, are limited by incomplete feature isolation and unreliable monosemanticity. To systematically quantify these limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric to quantify feature monosemanticity in latent representation. Building on these insights, we propose Guided Sparse Autoencoders (G-SAE), a method that conditions latent representations on labeled concepts during training. We demonstrate that reliable localization and disentanglement of target concepts within the latent space improve interpretability, detection of behavior, and control. Specifically, our evaluations on toxicity detection, writing style identification, and privacy attribute recognition show that G-SAE not only enhances monosemanticity but also enables more effective and fine-grained steering with less quality degradation. Our findings provide actionable guidelines for measuring and advancing mechanistic interpretability and control of LLMs.

