---
layout: default
title: Automated Detection of Pre-training Text in Black-box LLMs
---

# Automated Detection of Pre-training Text in Black-box LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19399" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19399v1</a>
  <a href="https://arxiv.org/pdf/2506.19399.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19399v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19399v1', 'Automated Detection of Pre-training Text in Black-box LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruihan Hu, Yu-Ming Shang, Jiankun Peng, Wei Luo, Yazhe Wang, Xi Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

**å¤‡æ³¨**: 13 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVeilProbeä»¥è§£å†³é»‘ç®±LLMé¢„è®­ç»ƒæ–‡æœ¬æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é»‘ç®±æ¨¡å‹` `é¢„è®­ç»ƒæ–‡æœ¬` `æ•°æ®éšç§` `è‡ªåŠ¨åŒ–æ£€æµ‹` `æœºå™¨å­¦ä¹ ` `åºåˆ—åˆ°åºåˆ—æ¨¡å‹` `ç‰ˆæƒä¿æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äºæ¨¡å‹çš„éšè—ä¿¡æ¯ï¼Œæ— æ³•åœ¨é»‘ç®±ç¯å¢ƒä¸­æœ‰æ•ˆæ£€æµ‹é¢„è®­ç»ƒæ–‡æœ¬ï¼Œä¸”éœ€è¦å¤§é‡äººå·¥è®¾è®¡é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºVeilProbeæ¡†æ¶ï¼Œåˆ©ç”¨åºåˆ—åˆ°åºåˆ—æ˜ å°„æ¨¡å‹è‡ªåŠ¨æ¨æ–­è¾“å…¥ä¸è¾“å‡ºä¹‹é—´çš„æ˜ å°„ç‰¹å¾ï¼Œå‡å°‘äººå·¥å¹²é¢„ã€‚
3. åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒVeilProbeåœ¨é»‘ç®±ç¯å¢ƒä¸­è¡¨ç°ä¼˜è¶Šï¼ŒæˆåŠŸç¼“è§£äº†è¿‡æ‹Ÿåˆé—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€æµ‹ç»™å®šæ–‡æœ¬æ˜¯å¦å±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é¢„è®­ç»ƒæ•°æ®å¯¹äºç¡®ä¿æ•°æ®éšç§å’Œç‰ˆæƒä¿æŠ¤è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ¨¡å‹çš„éšè—ä¿¡æ¯ï¼Œæ— æ³•åœ¨é»‘ç®±ç¯å¢ƒä¸­æœ‰æ•ˆå·¥ä½œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†VeilProbeï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨é»‘ç®±ç¯å¢ƒä¸­æ— éœ€äººå·¥å¹²é¢„è‡ªåŠ¨æ£€æµ‹LLMsé¢„è®­ç»ƒæ–‡æœ¬çš„æ¡†æ¶ã€‚VeilProbeåˆ©ç”¨åºåˆ—åˆ°åºåˆ—æ˜ å°„æ¨¡å‹æ¨æ–­è¾“å…¥æ–‡æœ¬ä¸LLMç”Ÿæˆçš„è¾“å‡ºåç¼€ä¹‹é—´çš„æ½œåœ¨æ˜ å°„ç‰¹å¾ï¼Œå¹¶é€šè¿‡å…³é”®tokenæ‰°åŠ¨è·å¾—æ›´å…·åŒºåˆ†æ€§çš„æˆå‘˜ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°çœŸå®åœºæ™¯ä¸­è®­ç»ƒæ–‡æœ¬æ ·æœ¬æœ‰é™ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŸºäºåŸå‹çš„æˆå‘˜åˆ†ç±»å™¨ä»¥ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ã€‚å¯¹ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†è¿›è¡Œçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é»‘ç®±ç¯å¢ƒä¸­æœ‰æ•ˆä¸”ä¼˜è¶Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨é»‘ç®±ç¯å¢ƒä¸­æ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–‡æœ¬çš„å…·ä½“é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ¨¡å‹çš„éšè—ä¿¡æ¯ï¼Œæ— æ³•åœ¨ä»…æœ‰è¾“å…¥å’Œè¾“å‡ºæ–‡æœ¬çš„æƒ…å†µä¸‹æœ‰æ•ˆå·¥ä½œï¼Œä¸”å¾€å¾€éœ€è¦å¤§é‡äººå·¥è®¾è®¡çš„é—®é¢˜å’ŒæŒ‡ä»¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVeilProbeæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åºåˆ—åˆ°åºåˆ—æ˜ å°„æ¨¡å‹è‡ªåŠ¨æ¨æ–­è¾“å…¥æ–‡æœ¬ä¸LLMç”Ÿæˆçš„è¾“å‡ºåç¼€ä¹‹é—´çš„æ½œåœ¨æ˜ å°„ç‰¹å¾ï¼Œä»è€Œå®ç°æ— éœ€äººå·¥å¹²é¢„çš„é¢„è®­ç»ƒæ–‡æœ¬æ£€æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVeilProbeçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥æ–‡æœ¬å¤„ç†ã€åºåˆ—åˆ°åºåˆ—æ˜ å°„æ¨¡å‹æ¨æ–­ã€å…³é”®tokenæ‰°åŠ¨å’ŒåŸºäºåŸå‹çš„æˆå‘˜åˆ†ç±»å™¨å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œè¾“å…¥æ–‡æœ¬é€šè¿‡æ¨¡å‹ç”Ÿæˆè¾“å‡ºåç¼€ï¼Œç„¶åæ¨æ–­æ½œåœ¨æ˜ å°„ç‰¹å¾ï¼Œæ¥ç€è¿›è¡Œtokenæ‰°åŠ¨ä»¥å¢å¼ºç‰¹å¾çš„å¯åŒºåˆ†æ€§ï¼Œæœ€åä½¿ç”¨åŸå‹åˆ†ç±»å™¨è¿›è¡Œæˆå‘˜èµ„æ ¼åˆ¤æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šVeilProbeçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶åœ¨é»‘ç®±ç¯å¢ƒä¸­å®ç°äº†è‡ªåŠ¨åŒ–çš„é¢„è®­ç»ƒæ–‡æœ¬æ£€æµ‹ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•å¯¹éšè—ä¿¡æ¯çš„ä¾èµ–ï¼Œå¹¶ä¸”å‡å°‘äº†äººå·¥å¹²é¢„çš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒVeilProbeé‡‡ç”¨äº†åºåˆ—åˆ°åºåˆ—æ¨¡å‹è¿›è¡Œç‰¹å¾æ¨æ–­ï¼Œå¹¶é€šè¿‡å…³é”®tokenæ‰°åŠ¨æ¥å¢å¼ºç‰¹å¾çš„åŒºåˆ†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒåŸºäºåŸå‹çš„æˆå‘˜åˆ†ç±»å™¨è¢«å¼•å…¥ä»¥åº”å¯¹è®­ç»ƒæ ·æœ¬æœ‰é™å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVeilProbeæ¡†æ¶åœ¨é»‘ç®±ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒæˆåŠŸæé«˜äº†é¢„è®­ç»ƒæ–‡æœ¬æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®å±•ç¤ºäº†åœ¨ä¸åŒæ•°æ®é›†ä¸Šæ£€æµ‹å‡†ç¡®ç‡çš„æå‡å¹…åº¦ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®éšç§ä¿æŠ¤ã€ç‰ˆæƒç®¡ç†å’Œæ¨¡å‹å®‰å…¨æ€§è¯„ä¼°ç­‰ã€‚é€šè¿‡è‡ªåŠ¨åŒ–æ£€æµ‹LLMçš„é¢„è®­ç»ƒæ–‡æœ¬ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢æ•°æ®æ³„éœ²å’Œç‰ˆæƒä¾µæƒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šåœ¨æ›´å¤šçš„é»‘ç®±æ¨¡å‹æ£€æµ‹å’Œæ•°æ®å®‰å…¨é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Detecting whether a given text is a member of the pre-training data of Large Language Models (LLMs) is crucial for ensuring data privacy and copyright protection. Most existing methods rely on the LLM's hidden information (e.g., model parameters or token probabilities), making them ineffective in the black-box setting, where only input and output texts are accessible. Although some methods have been proposed for the black-box setting, they rely on massive manual efforts such as designing complicated questions or instructions. To address these issues, we propose VeilProbe, the first framework for automatically detecting LLMs' pre-training texts in a black-box setting without human intervention. VeilProbe utilizes a sequence-to-sequence mapping model to infer the latent mapping feature between the input text and the corresponding output suffix generated by the LLM. Then it performs the key token perturbations to obtain more distinguishable membership features. Additionally, considering real-world scenarios where the ground-truth training text samples are limited, a prototype-based membership classifier is introduced to alleviate the overfitting issue. Extensive evaluations on three widely used datasets demonstrate that our framework is effective and superior in the black-box setting.

