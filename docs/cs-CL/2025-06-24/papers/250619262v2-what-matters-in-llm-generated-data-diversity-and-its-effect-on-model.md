---
layout: default
title: What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning
---

# What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19262" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19262v2</a>
  <a href="https://arxiv.org/pdf/2506.19262.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19262v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19262v2', 'What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuchang Zhu, Huazhen Zhong, Qunshu Lin, Haotong Wei, Xiaolong Sun, Zixuan Yu, Minghao Liu, Zibin Zheng, Liang Chen

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24 (æ›´æ–°: 2025-06-25)

**å¤‡æ³¨**: Ongoing work

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨LLMç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å¯¹æ¨¡å‹å¾®è°ƒçš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®å¤šæ ·æ€§` `æ¨¡å‹å¾®è°ƒ` `åˆæˆæ•°æ®` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶æœªèƒ½å……åˆ†è€ƒè™‘LLMç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚
2. æœ¬æ–‡é€šè¿‡å®éªŒæ¢è®¨ä¸åŒå¤šæ ·æ€§æ°´å¹³çš„LLMç”Ÿæˆæ•°æ®å¯¹ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæå‡ºäº†æ··åˆæ¯”ä¾‹çš„åˆæˆæ•°æ®è®­ç»ƒæ–¹æ³•ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€‚åº¦å¤šæ ·æ€§çš„LLMç”Ÿæˆæ•°æ®åœ¨æ ‡æ³¨æ•°æ®ä¸è¶³æ—¶èƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œè€Œé«˜åº¦å¤šæ ·åŒ–çš„æ•°æ®åˆ™ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆèƒ½åŠ›çš„æ˜¾è‘—æå‡ï¼Œåˆ©ç”¨LLMç”Ÿæˆçš„æ•°æ®è®­ç»ƒä¸‹æ¸¸æ¨¡å‹æˆä¸ºç¼“è§£ç‰¹å®šé¢†åŸŸæ•°æ®ç¨€ç¼ºå’Œå‡å°‘è€—æ—¶æ ‡æ³¨çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿‘æœŸç ”ç©¶æŒ‡å‡ºï¼ŒåŸºäºè‡ªç”Ÿæˆæ•°æ®çš„è¿­ä»£è®­ç»ƒå¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶å…³æ³¨LLMç”Ÿæˆæ•°æ®çš„å½±å“ï¼Œä½†å¾€å¾€å¿½è§†äº†æ•°æ®å¤šæ ·æ€§è¿™ä¸€å…³é”®å› ç´ ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨LLMç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å¯¹ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œé€‚åº¦å¤šæ ·æ€§çš„LLMç”Ÿæˆæ•°æ®åœ¨æ ‡æ³¨æ•°æ®ä¸è¶³çš„æƒ…å†µä¸‹èƒ½æå‡æ¨¡å‹æ€§èƒ½ï¼Œè€Œé«˜åº¦å¤šæ ·åŒ–çš„æ•°æ®åˆ™å¯èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚å¸Œæœ›æˆ‘ä»¬çš„å®è¯å‘ç°èƒ½ä¸ºæœªæ¥LLMä½œä¸ºæ•°æ®ç”Ÿæˆå™¨çš„ç ”ç©¶æä¾›æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯LLMç”Ÿæˆæ•°æ®åœ¨è¿­ä»£è®­ç»ƒä¸­å¯¼è‡´çš„æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯æ•°æ®å¤šæ ·æ€§ä¸è¶³çš„æƒ…å†µä¸‹ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ•°æ®å¤šæ ·æ€§è¿™ä¸€å…³é”®å› ç´ ï¼Œå¯¼è‡´æ¨¡å‹å´©æºƒç°è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†æLLMç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å¯¹ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæ¢ç´¢ä¸åŒå¤šæ ·æ€§æ°´å¹³çš„æ•°æ®åœ¨æ¨¡å‹è®­ç»ƒä¸­çš„ä½œç”¨ï¼Œæ—¨åœ¨æ‰¾åˆ°æœ€ä½³çš„æ•°æ®ç»„åˆç­–ç•¥ä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆã€æ•°æ®å¤šæ ·æ€§è¯„ä¼°å’Œä¸‹æ¸¸æ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆç”Ÿæˆä¸åŒå¤šæ ·æ€§æ°´å¹³çš„LLMæ•°æ®ï¼Œç„¶åè¯„ä¼°å…¶å¤šæ ·æ€§ï¼Œæœ€åå°†è¿™äº›æ•°æ®ç”¨äºè®­ç»ƒä¸‹æ¸¸æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°åˆ†æäº†LLMç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæå‡ºäº†æ··åˆæ¯”ä¾‹çš„åˆæˆæ•°æ®è®­ç»ƒæ–¹æ³•ï¼Œä¸ä¼ ç»Ÿçš„å•ä¸€æ•°æ®æºè®­ç»ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„å¤šæ ·æ€§æ¯”ä¾‹ï¼Œå¹¶é‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒæ•°æ®ç»„åˆä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€‚åº¦å¤šæ ·æ€§çš„LLMç”Ÿæˆæ•°æ®åœ¨æ ‡æ³¨æ•°æ®ä¸è¶³çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€§èƒ½æå‡å¹…åº¦å¯è¾¾15%ï¼Œè€Œé«˜åº¦å¤šæ ·åŒ–çš„æ•°æ®åˆ™å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œé™ä½å¹…åº¦å¯è¾¾10%ã€‚è¿™äº›ç»“æœä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å®è¯ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„ç‰¹å®šåº”ç”¨åœºæ™¯ä¸­ã€‚é€šè¿‡ä¼˜åŒ–LLMç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§ï¼Œå¯ä»¥æœ‰æ•ˆæå‡ä¸‹æ¸¸æ¨¡å‹çš„æ€§èƒ½ï¼Œé™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the remarkable generative capabilities of large language models (LLMs), using LLM-generated data to train downstream models has emerged as a promising approach to mitigate data scarcity in specific domains and reduce time-consuming annotations. However, recent studies have highlighted a critical issue: iterative training on self-generated data results in model collapse, where model performance degrades over time. Despite extensive research on the implications of LLM-generated data, these works often neglect the importance of data diversity, a key factor in data quality. In this work, we aim to understand the implications of the diversity of LLM-generated data on downstream model performance. Specifically, we explore how varying levels of diversity in LLM-generated data affect downstream model performance. Additionally, we investigate the performance of models trained on data that mixes different proportions of LLM-generated data, which we refer to as synthetic data. Our experimental results show that, with minimal distribution shift, moderately diverse LLM-generated data can enhance model performance in scenarios with insufficient labeled data, whereas highly diverse generated data has a negative impact. We hope our empirical findings will offer valuable guidance for future studies on LLMs as data generators.

