---
layout: default
title: SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning
---

# SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19767" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19767v1</a>
  <a href="https://arxiv.org/pdf/2506.19767.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19767v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19767v1', 'SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, Dongbin Zhao

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSRFTæ–¹æ³•ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç›‘ç£å¾®è°ƒ` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `ç†µæ„ŸçŸ¥æœºåˆ¶` `å•é˜¶æ®µä¼˜åŒ–` `æ•°å­¦æ¨ç†` `åˆ†å¸ƒå¤–æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ•´åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æ—¶å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œæ•ˆæœä¸ä½³çš„é—®é¢˜ã€‚
2. SRFTæ–¹æ³•é€šè¿‡ç†µæ„ŸçŸ¥åŠ æƒæœºåˆ¶ï¼Œå°†SFTå’ŒRLç»Ÿä¸€ä¸ºå•é˜¶æ®µä¼˜åŒ–ï¼Œæå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. SRFTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†å’Œåˆ†å¸ƒå¤–æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¦‚ä½•æœ€ä½³åœ°æ•´åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹æ ‡è®°åˆ†å¸ƒã€å­¦ä¹ åŠ¨æ€å’Œé›†æˆæœºåˆ¶çš„å…¨é¢åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¿™ä¸¤ç§èŒƒå¼ä¹‹é—´çš„å…³é”®å·®å¼‚ï¼šSFTå¼•å…¥äº†ç²—ç²’åº¦çš„å…¨å±€å˜åŒ–ï¼Œè€ŒRLåˆ™è¿›è¡Œç»†ç²’åº¦çš„é€‰æ‹©æ€§ä¼˜åŒ–ï¼Œç†µä½œä¸ºè®­ç»ƒæœ‰æ•ˆæ€§çš„å…³é”®æŒ‡æ ‡ã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ç›‘ç£å¼ºåŒ–å¾®è°ƒï¼ˆSRFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç†µæ„ŸçŸ¥åŠ æƒæœºåˆ¶ç»Ÿä¸€ä¸¤ç§å¾®è°ƒèŒƒå¼çš„å•é˜¶æ®µæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåŒæ—¶åº”ç”¨SFTå’ŒRLï¼Œç›´æ¥ä¼˜åŒ–LLMï¼Œå®éªŒè¡¨æ˜SRFTåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°59.1%ï¼Œæ¯”é›¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•æé«˜äº†9.0%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ•´åˆä¸ä½³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€é‡‡ç”¨ä¸¤é˜¶æ®µçš„é¡ºåºä¼˜åŒ–ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„SRFTæ–¹æ³•é€šè¿‡ç†µæ„ŸçŸ¥åŠ æƒæœºåˆ¶ï¼Œå°†SFTå’ŒRLç»“åˆä¸ºå•é˜¶æ®µä¼˜åŒ–ï¼Œèƒ½å¤ŸåŒæ—¶åˆ©ç”¨ç¤ºä¾‹å’Œè‡ªæˆ‘æ¢ç´¢çš„å›åˆè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSRFTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€ç†µè®¡ç®—ã€SFTå’ŒRLçš„è”åˆä¼˜åŒ–æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡ç†µè®¡ç®—è¯„ä¼°å½“å‰æ¨¡å‹çš„å­¦ä¹ çŠ¶æ€ï¼Œç„¶åæ ¹æ®ç†µå€¼åŠ¨æ€è°ƒæ•´SFTå’ŒRLçš„æƒé‡ï¼Œæœ€åè¿›è¡Œè”åˆè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šSRFTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç†µæ„ŸçŸ¥åŠ æƒæœºåˆ¶ï¼Œè¿™ä¸€æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°è°ƒæ•´SFTå’ŒRLçš„å½±å“åŠ›ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å­¦ä¹ ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSRFTé¿å…äº†ä¸¤é˜¶æ®µè®­ç»ƒçš„å¤æ‚æ€§ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SRFTä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡SFTå’ŒRLçš„å½±å“ï¼Œå¹¶é‡‡ç”¨äº†åŠ¨æ€å­¦ä¹ ç‡ç­–ç•¥æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç»“æ„ä¸Šé‡‡ç”¨äº†å¤šå±‚æ¬¡çš„ç¥ç»ç½‘ç»œï¼Œä»¥å¢å¼ºå…¶è¡¨è¾¾èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SRFTåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°59.1%ï¼Œæ¯”é›¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•æé«˜äº†9.0%ã€‚åœ¨ä¸‰ä¸ªåˆ†å¸ƒå¤–åŸºå‡†ä¸Šï¼ŒSRFTçš„è¡¨ç°ä¹Ÿæ˜¾è‘—æå‡ï¼Œå‡†ç¡®ç‡æé«˜äº†10.9%ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èå’ŒåŒ»ç–—ç­‰éœ€è¦å¤æ‚æ¨ç†çš„åœºæ™¯ã€‚SRFTæ–¹æ³•èƒ½å¤Ÿæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨è¿™äº›é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œä¸ºå†³ç­–æ”¯æŒå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰åº”ç”¨æä¾›æ›´ä¸ºå‡†ç¡®çš„ç»“æœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks.

