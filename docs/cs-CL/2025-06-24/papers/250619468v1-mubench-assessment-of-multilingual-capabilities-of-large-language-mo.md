---
layout: default
title: MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages
---

# MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19468" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19468v1</a>
  <a href="https://arxiv.org/pdf/2506.19468.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19468v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19468v1', 'MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhan Han, Yifan Zhang, Zhixun Chen, Binbin Liu, Haobin Lin, Bingni Zhang, Taifeng Wang, Mykola Pechenizkiy, Meng Fang, Yin Zheng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMuBenchä»¥è¯„ä¼°å¤šè¯­è¨€å¤§æ¨¡å‹çš„èƒ½åŠ›å·®å¼‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æ¨¡å‹` `è¯„ä¼°åŸºå‡†` `è·¨è¯­è¨€å¯¹é½` `æ€§èƒ½åˆ†æ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨ç¿»è¯‘` `ä½èµ„æºè¯­è¨€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šè¯­è¨€å¤§æ¨¡å‹è¯„ä¼°æ–¹æ³•å­˜åœ¨æ•°æ®é›†æœ‰é™å’Œç¼ºä¹è·¨è¯­è¨€å¯¹é½çš„é—®é¢˜ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å…¨é¢ã€‚
2. æœ¬æ–‡æå‡ºMuBenchåŸºå‡†ï¼Œæ¶µç›–61ç§è¯­è¨€ï¼Œè¯„ä¼°å¤šç§èƒ½åŠ›ï¼Œå¹¶å¼•å…¥å¤šè¯­è¨€ä¸€è‡´æ€§ï¼ˆMLCï¼‰ä½œä¸ºæ€§èƒ½åˆ†æçš„æ–°æŒ‡æ ‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰å¤šè¯­è¨€æ¨¡å‹åœ¨å£°ç§°çš„è¯­è¨€è¦†ç›–ä¸å®é™…è¡¨ç°ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå°¤å…¶æ˜¯ä½èµ„æºè¯­è¨€è¡¨ç°ä¸ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ–°çš„æ¨¡å‹ä¸æ–­å£°ç§°æ”¯æŒè¶Šæ¥è¶Šå¤šçš„è¯­è¨€ã€‚ç„¶è€Œï¼Œç°æœ‰è¯„ä¼°æ•°æ®é›†æœ‰é™ï¼Œç¼ºä¹è·¨è¯­è¨€å¯¹é½ï¼Œå¯¼è‡´å¤šè¯­è¨€èƒ½åŠ›çš„è¯„ä¼°åœ¨è¯­è¨€å’ŒæŠ€èƒ½è¦†ç›–ä¸Šå­˜åœ¨ç¢ç‰‡åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MuBenchï¼Œä¸€ä¸ªæ¶µç›–61ç§è¯­è¨€å¹¶è¯„ä¼°å¹¿æ³›èƒ½åŠ›çš„åŸºå‡†ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ç§æœ€å…ˆè¿›çš„å¤šè¯­è¨€LLMsï¼Œå‘ç°å£°ç§°çš„è¯­è¨€è¦†ç›–ä¸å®é™…è¡¨ç°ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå°¤å…¶æ˜¯è‹±è¯­ä¸ä½èµ„æºè¯­è¨€ä¹‹é—´çš„æŒç»­æ€§èƒ½å·®å¼‚ã€‚åŸºäºMuBenchçš„å¯¹é½ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šè¯­è¨€ä¸€è‡´æ€§ï¼ˆMLCï¼‰ä½œä¸ºåˆ†ææ€§èƒ½ç“¶é¢ˆå’ŒæŒ‡å¯¼æ¨¡å‹æ”¹è¿›çš„è¡¥å……æŒ‡æ ‡ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨è‹±è¯­å’Œä¸­æ–‡ä¸Šé¢„è®­ç»ƒäº†ä¸€å¥—12äº¿å‚æ•°çš„æ¨¡å‹ï¼Œä½¿ç”¨500Bä¸ªæ ‡è®°ï¼Œå˜åŒ–è¯­è¨€æ¯”ä¾‹å’Œå¹¶è¡Œæ•°æ®æ¯”ä¾‹ï¼Œä»¥ç ”ç©¶è·¨è¯­è¨€è¿ç§»åŠ¨æ€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šè¯­è¨€å¤§æ¨¡å‹è¯„ä¼°æ–¹æ³•ä¸­æ•°æ®é›†ä¸è¶³å’Œç¼ºä¹è·¨è¯­è¨€å¯¹é½çš„é—®é¢˜ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„ç‰‡é¢æ€§å’Œä¸å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥MuBenchåŸºå‡†ï¼Œæ¶µç›–61ç§è¯­è¨€å¹¶è¯„ä¼°å¤šç§èƒ½åŠ›ï¼Œæä¾›å…¨é¢çš„å¤šè¯­è¨€èƒ½åŠ›è¯„ä¼°ï¼Œå¹¶æå‡ºå¤šè¯­è¨€ä¸€è‡´æ€§ï¼ˆMLCï¼‰ä½œä¸ºè¡¥å……æŒ‡æ ‡ï¼Œå¸®åŠ©åˆ†ææ¨¡å‹æ€§èƒ½ç“¶é¢ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMuBenchåŸºå‡†åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯æ•°æ®é›†çš„æ„å»ºï¼Œç¡®ä¿è·¨è¯­è¨€å¯¹é½ï¼›å…¶æ¬¡æ˜¯èƒ½åŠ›è¯„ä¼°ï¼Œæ¶µç›–è¯­è¨€ç†è§£ã€ç”Ÿæˆç­‰å¤šç§ä»»åŠ¡ï¼›æœ€åæ˜¯æ€§èƒ½åˆ†æï¼Œåˆ©ç”¨MLCæŒ‡æ ‡è¿›è¡Œæ·±å…¥åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šMuBenchçš„æå‡ºæ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€çš„è¯„ä¼°ä¸Šï¼Œæä¾›äº†æ›´ä¸ºå‡†ç¡®çš„æ€§èƒ½åˆ†æå·¥å…·ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†12äº¿å‚æ•°çš„æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®ä¸º500Bä¸ªæ ‡è®°ï¼Œè®¾è®¡äº†ä¸åŒçš„è¯­è¨€æ¯”ä¾‹å’Œå¹¶è¡Œæ•°æ®æ¯”ä¾‹ï¼Œä»¥ç ”ç©¶è·¨è¯­è¨€è¿ç§»çš„åŠ¨æ€ç‰¹æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰å¤šè¯­è¨€æ¨¡å‹åœ¨å£°ç§°çš„è¯­è¨€è¦†ç›–ä¸å®é™…è¡¨ç°ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸Šï¼Œæ€§èƒ½å·®å¼‚å¯è¾¾30%ã€‚é€šè¿‡å¼•å…¥å¤šè¯­è¨€ä¸€è‡´æ€§ï¼ˆMLCï¼‰æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œä¸ºåç»­æ”¹è¿›æä¾›æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œè·¨æ–‡åŒ–ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æä¾›å…¨é¢çš„è¯„ä¼°åŸºå‡†ï¼ŒMuBenchå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´å¥½åœ°ç†è§£å’Œæ”¹è¿›å¤šè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œæ¨åŠ¨å¤šè¯­è¨€æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multilingual large language models (LLMs) are advancing rapidly, with new models frequently claiming support for an increasing number of languages. However, existing evaluation datasets are limited and lack cross-lingual alignment, leaving assessments of multilingual capabilities fragmented in both language and skill coverage. To address this, we introduce MuBench, a benchmark covering 61 languages and evaluating a broad range of capabilities. We evaluate several state-of-the-art multilingual LLMs and find notable gaps between claimed and actual language coverage, particularly a persistent performance disparity between English and low-resource languages. Leveraging MuBench's alignment, we propose Multilingual Consistency (MLC) as a complementary metric to accuracy for analyzing performance bottlenecks and guiding model improvement. Finally, we pretrain a suite of 1.2B-parameter models on English and Chinese with 500B tokens, varying language ratios and parallel data proportions to investigate cross-lingual transfer dynamics.

