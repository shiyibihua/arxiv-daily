---
layout: default
title: AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models
---

# AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19505" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19505v2</a>
  <a href="https://arxiv.org/pdf/2506.19505.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19505v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19505v2', 'AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeyu Li, Chuanfu Xiao, Yang Wang, Xiang Liu, Zhenheng Tang, Baotong Lu, Mao Yang, Xinyu Chen, Xiaowen Chu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24 (æ›´æ–°: 2025-10-18)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAnTKVä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹KVç¼“å­˜é‡åŒ–ç²¾åº¦ä¸‹é™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æŠ€æœ¯` `å¤§è¯­è¨€æ¨¡å‹` `KVç¼“å­˜` `é”šå¾—åˆ†` `å‘é‡é‡åŒ–` `æ¨¡å‹å‹ç¼©` `æ€§èƒ½ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„KVç¼“å­˜é‡åŒ–æ–¹æ³•åœ¨è¶…ä½æ¯”ç‰¹é‡åŒ–æ—¶ï¼Œç²¾åº¦ä¸‹é™ä¸å‡åŒ€ï¼Œå¯¼è‡´æ•´ä½“æ€§èƒ½å—æŸã€‚
2. AnTKVé€šè¿‡å¼•å…¥é”šå¾—åˆ†æ¥è¯†åˆ«å¯¹é‡åŒ–æ•æ„Ÿçš„tokenï¼Œå¹¶ä¿ç•™è¿™äº›tokenä»¥å‡è½»ç²¾åº¦æŸå¤±ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAnTKVåœ¨4æ¯”ç‰¹ä¸‹ä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼Œåœ¨1æ¯”ç‰¹ä¸‹æ˜¾è‘—é™ä½å›°æƒ‘åº¦ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é‡åŒ–å·²æˆä¸ºå‡å°‘å¤§è¯­è¨€æ¨¡å‹KVç¼“å­˜å†…å­˜å ç”¨çš„æœ‰æ•ˆè½»é‡çº§è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¶…ä½æ¯”ç‰¹KVç¼“å­˜é‡åŒ–å¯¼è‡´çš„ç²¾åº¦ä¸‹é™ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ ‡é‡é‡åŒ–å—é™äº1æ¯”ç‰¹ï¼Œè€Œå‘é‡é‡åŒ–åˆ™åˆ©ç”¨äº†å‘é‡å†…çš„ç›¸å…³æ€§ï¼Œå…è®¸åœ¨è¶…ä½æ¯”ç‰¹èŒƒå›´å†…è¿›è¡Œé‡åŒ–ã€‚ä¸ºè¿›ä¸€æ­¥å‡è½»é‡åŒ–å¼•èµ·çš„ç²¾åº¦ä¸‹é™ï¼Œæœ¬æ–‡å¼•å…¥äº†é”šå¾—åˆ†æ¥è¡¡é‡æ¯ä¸ªtokenå¯¹é‡åŒ–çš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬çš„åˆ†æå’Œå®éªŒè¡¨æ˜ï¼Œä¿ç•™1%å…·æœ‰æœ€é«˜é”šå¾—åˆ†çš„tokenå¯ä»¥æ˜¾è‘—å‡è½»æ¿€è¿›é‡åŒ–ä¸‹çš„ç²¾åº¦æŸå¤±ã€‚æˆ‘ä»¬æå‡ºäº†AnTKVï¼Œä¸€ä¸ªåŒé˜¶æ®µæ¡†æ¶ï¼Œåˆ©ç”¨é”štokenæ„ŸçŸ¥å‘é‡é‡åŒ–æ¥å‹ç¼©KVç¼“å­˜ï¼Œç»“åˆç¦»çº¿tokenæ„ŸçŸ¥ä¸­å¿ƒå­¦ä¹ å’Œåœ¨çº¿é”štokené€‰æ‹©ï¼Œä»¥å¹³è¡¡å‹ç¼©å’Œç²¾åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹KVç¼“å­˜çš„è¶…ä½æ¯”ç‰¹é‡åŒ–å¯¼è‡´çš„ç²¾åº¦ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é‡åŒ–è¿‡ç¨‹ä¸­æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œä¿ç•™å¯¹ç²¾åº¦å½±å“è¾ƒå¤§çš„tokenï¼Œå¯¼è‡´æ•´ä½“æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡é”šå¾—åˆ†æ¥è¯„ä¼°æ¯ä¸ªtokenå¯¹é‡åŒ–çš„æ•æ„Ÿæ€§ï¼Œä¿ç•™å°‘é‡é«˜æ•æ„Ÿæ€§tokenï¼Œä»è€Œå‡è½»é‡åŒ–å¸¦æ¥çš„ç²¾åº¦æŸå¤±ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç¦»çº¿å­¦ä¹ å’Œåœ¨çº¿é€‰æ‹©ï¼Œä»¥å®ç°æ›´å¥½çš„å‹ç¼©ä¸ç²¾åº¦å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAnTKVæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºç¦»çº¿tokenæ„ŸçŸ¥ä¸­å¿ƒå­¦ä¹ ï¼Œç¬¬äºŒé˜¶æ®µä¸ºåœ¨çº¿é”štokené€‰æ‹©ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†ä¸FlashAttentionå…¼å®¹çš„åœ¨çº¿é€‰æ‹©å†…æ ¸ï¼Œä»¥æé«˜éƒ¨ç½²æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šAnTKVçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥é”šå¾—åˆ†æœºåˆ¶ï¼Œè¯†åˆ«å‡ºå¯¹é‡åŒ–æ•æ„Ÿçš„tokenï¼Œå¹¶é€šè¿‡ä¿ç•™è¿™äº›tokenæ¥æ˜¾è‘—å‡è½»ç²¾åº¦æŸå¤±ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„é‡åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨tokené—´çš„ç›¸å…³æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒAnTKVé‡‡ç”¨äº†é«˜æ•ˆçš„åœ¨çº¿é”štokené€‰æ‹©å†…æ ¸ï¼Œæ”¯æŒåœ¨å•ä¸ª80GB A100ä¸Šæ‰©å±•åˆ°840K tokensï¼ŒåŒæ—¶åœ¨è§£ç ååé‡ä¸Šæ¯”FP16åŸºçº¿æé«˜äº†3.5å€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAnTKVåœ¨4æ¯”ç‰¹é‡åŒ–ä¸‹çš„æ€§èƒ½ä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼Œè€Œåœ¨1æ¯”ç‰¹é‡åŒ–ä¸‹ï¼Œå›°æƒ‘åº¦æ˜¾è‘—é™ä½è‡³6.32ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼ŒCQä¸º7.25ï¼ŒKVQuantä¸º15.36ï¼Œå±•ç¤ºäº†å…¶åœ¨è¶…ä½æ¯”ç‰¹é‡åŒ–ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AnTKVçš„ç ”ç©¶æˆæœåœ¨å¤§è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆå†…å­˜ç®¡ç†å’Œå¿«é€Ÿæ¨ç†çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œå®æ—¶ç¿»è¯‘ç­‰ã€‚é€šè¿‡æœ‰æ•ˆå‹ç¼©KVç¼“å­˜ï¼ŒAnTKVèƒ½å¤Ÿåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ï¼Œæ¨åŠ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Quantization has emerged as an effective and lightweight solution to reduce the memory footprint of the KV cache in Large Language Models. Nevertheless, minimizing the accuracy degradation caused by ultra-low-bit KV cache quantization remains a significant challenge. While scalar quantization is constrained by 1-bit bound, vector quantization exploits intra-vector correlations and enables sub-bit regimes, making it more suitable for ultra-low-bit quantization. To further mitigate quantization-induced degradation, we reveal that the degradation is highly uneven across tokens in attention quality. To investigate this unevenness, we introduce anchor score to measure each token's sensitivity to quantization. Our analysis and experiments show that preserving a small subset (1\%) of tokens with the highest Anchor Score significantly mitigates accuracy loss under aggressive quantization.
>   We propose AnTKV, a dual-stage framework that leverages anchor token-aware vector quantization to compress the KV cache. It combines offline token-aware centroids learning and online anchor token selection to balance compression and accuracy. To enable efficient deployment, we design an online anchor token selection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale to 840K tokens on a single 80GB A100, while delivering up to $3.5\times$ higher decoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV matches or surpasses prior methods at 4-bit, and significantly reduce perplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on Mistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant.

