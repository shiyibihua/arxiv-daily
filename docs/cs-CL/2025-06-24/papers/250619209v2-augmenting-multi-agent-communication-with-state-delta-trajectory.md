---
layout: default
title: Augmenting Multi-Agent Communication with State Delta Trajectory
---

# Augmenting Multi-Agent Communication with State Delta Trajectory

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19209" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19209v2</a>
  <a href="https://arxiv.org/pdf/2506.19209.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19209v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19209v2', 'Augmenting Multi-Agent Communication with State Delta Trajectory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yichen Tang, Weihang Su, Yujia Zhou, Yiqun Liu, Min Zhang, Shaoping Ma, Qingyao Ai

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24 (æ›´æ–°: 2025-09-24)

**å¤‡æ³¨**: 22 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçŠ¶æ€å¢é‡è½¨è¿¹ä»¥å¢å¼ºå¤šæ™ºèƒ½ä½“é€šä¿¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `çŠ¶æ€å¢é‡ç¼–ç ` `æ¨ç†é€»è¾‘` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¿¡æ¯ä¼ é€’` `å¤æ‚ä»»åŠ¡` `é€šä¿¡åè®®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸»è¦ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œé€šä¿¡ï¼Œå¯¼è‡´ä¿¡æ¯åœ¨ä¼ é€’è¿‡ç¨‹ä¸­ä¸å¯é¿å…åœ°æŸå¤±ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é€šä¿¡åè®®ï¼Œç»“åˆè‡ªç„¶è¯­è¨€æ ‡è®°å’ŒçŠ¶æ€è½¬ç§»è½¨è¿¹ï¼Œä»¥æ›´å…¨é¢åœ°ä¼ é€’ä¿¡æ¯ï¼Œè§£å†³äº†ä¿¡æ¯æŸå¤±çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨çŠ¶æ€å¢é‡ç¼–ç ï¼ˆSDEï¼‰çš„æ–¹æ³•åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å®ç°äº†SOTAæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ™ºèƒ½ä½“æŠ€æœ¯å¦‚è§’è‰²æ‰®æ¼”æˆ–å¤šè½®è¾©è®ºå·²è¢«è¯æ˜èƒ½æœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸»è¦ä¾èµ–è‡ªç„¶è¯­è¨€è¿›è¡Œé€šä¿¡ï¼Œè¿™åœ¨ç®€æ´æ€§å’Œå¯è§£é‡Šæ€§ä¸Šæœ‰ä¼˜åŠ¿ï¼Œä½†ä¹Ÿå¯¼è‡´ä¿¡æ¯æŸå¤±ï¼Œå°¤å…¶æ˜¯åœ¨ä¼ é€’æ¨ç†é€»è¾‘æˆ–æŠ½è±¡æ€æƒ³æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é€šä¿¡åè®®ï¼Œèƒ½å¤ŸåŒæ—¶ä¼ é€’è‡ªç„¶è¯­è¨€æ ‡è®°å’Œé€æ ‡è®°çŠ¶æ€è½¬ç§»è½¨è¿¹ã€‚æˆ‘ä»¬å‘ç°ï¼Œç›¸è¾ƒäºå®é™…çŠ¶æ€å€¼ï¼ŒLLMsåœ¨ç”Ÿæˆæ¯ä¸ªæ ‡è®°åçš„çŠ¶æ€å˜åŒ–åºåˆ—èƒ½æ›´å¥½åœ°åæ˜ æ¨ç†è¿‡ç¨‹ä¸­çš„éšå«ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨çŠ¶æ€å¢é‡ç¼–ç ï¼ˆSDEï¼‰æ–¹æ³•çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†SOTAæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å› ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œé€šä¿¡è€Œå¯¼è‡´çš„ä¿¡æ¯æŸå¤±é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†é€»è¾‘å’ŒæŠ½è±¡æ€æƒ³çš„ä¼ é€’ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„é€šä¿¡åè®®ï¼Œå…è®¸æ™ºèƒ½ä½“åœ¨ä¼ é€’ä¿¡æ¯æ—¶åŒæ—¶å‘é€è‡ªç„¶è¯­è¨€æ ‡è®°å’ŒçŠ¶æ€è½¬ç§»è½¨è¿¹ï¼Œä»è€Œæ›´å…¨é¢åœ°åæ˜ æ¨ç†è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯çŠ¶æ€å¢é‡ç¼–ç ï¼ˆSDEï¼‰æ¨¡å—ï¼Œç”¨äºè¡¨ç¤ºçŠ¶æ€è½¬ç§»è½¨è¿¹ï¼›äºŒæ˜¯é€šä¿¡æ¨¡å—ï¼Œè´Ÿè´£åœ¨æ™ºèƒ½ä½“ä¹‹é—´ä¼ é€’è‡ªç„¶è¯­è¨€å’ŒçŠ¶æ€ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥çŠ¶æ€å¢é‡ç¼–ç ï¼ˆSDEï¼‰ï¼Œé€šè¿‡æ•æ‰çŠ¶æ€å˜åŒ–åºåˆ—æ¥å¢å¼ºä¿¡æ¯ä¼ é€’çš„æœ‰æ•ˆæ€§ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•ä¸€è‡ªç„¶è¯­è¨€ä¼ é€’æ–¹å¼æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSDEæ¨¡å—é‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿çŠ¶æ€è½¬ç§»çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°ä¸­è€ƒè™‘äº†ä¿¡æ¯çš„å®Œæ•´æ€§å’Œæ¨ç†çš„è¿è´¯æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨çŠ¶æ€å¢é‡ç¼–ç ï¼ˆSDEï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œç›¸è¾ƒäºå…¶ä»–é€šä¿¡åè®®ï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼Œå…·ä½“æ•°æ®æœªæä¾›ï¼Œä½†è¡¨æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€è‡ªåŠ¨åŒ–æ¨ç†å’Œåä½œæœºå™¨äººç­‰ã€‚é€šè¿‡å¢å¼ºå¤šæ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡èƒ½åŠ›ï¼Œå¯ä»¥æå‡ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-agent techniques such as role playing or multi-turn debates have been shown to be effective in improving the performance of large language models (LLMs) in downstream tasks. Despite their differences in workflows, existing multi-agent systems constructed from a single base LLM mostly use natural language for agent communication. While this is appealing for its simplicity and interpretability, it also introduces inevitable information loss as one model must down sample its continuous state vectors to discrete tokens before transferring them to the other model. Such losses are particularly significant when the information to transfer is not simple facts, but reasoning logics or abstractive thoughts. To tackle this problem, we propose a new communication protocol that transfers both natural language tokens and token-wise state transition trajectory from one agent to another. Particularly, compared to the actual state value, we find that the sequence of state changes in LLMs after generating each token can better reflect the information hidden behind the inference process. We propose a State Delta Encoding (SDE) method to represent state transition trajectories. The experimental results show that multi-agent systems with SDE achieve SOTA performance compared to other communication protocols, particularly in tasks that involve complex reasoning.

