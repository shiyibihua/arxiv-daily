---
layout: default
title: Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study
---

# Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19794" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19794v5</a>
  <a href="https://arxiv.org/pdf/2506.19794.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19794v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19794v5', 'Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Ningyu Zhang, Huajun Chen

**åˆ†ç±»**: cs.CL, cs.AI, cs.IR, cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24 (æ›´æ–°: 2025-11-13)

**å¤‡æ³¨**: AAAI 2026 (oral)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zjunlp/DataMind)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®åˆæˆæ–¹æ³•ä»¥æå‡å¼€æºLLMçš„æ•°æ®åˆ†æèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®åˆ†æ` `æ¨ç†èƒ½åŠ›` `æ•°æ®åˆæˆ` `å¼€æºæ¨¡å‹` `æˆ˜ç•¥è§„åˆ’` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æºLLMåœ¨æ•°æ®åˆ†æä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ•°æ®åˆæˆæ–¹æ³•ï¼Œé€šè¿‡å¤šæ ·åŒ–çš„çœŸå®åœºæ™¯æ•°æ®é›†æ¥æå‡æ¨¡å‹çš„æ•°æ®åˆ†æèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ”¹è¿›åçš„æ¨¡å‹åœ¨æ•°æ®ç†è§£å’Œæ¨ç†èƒ½åŠ›ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æˆ˜ç•¥è§„åˆ’æ–¹é¢è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–æ•°æ®åˆ†æä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†å¼€æºæ¨¡å‹åœ¨æ¨ç†å¯†é›†åœºæ™¯ä¸­é¢ä¸´æ˜¾è‘—é™åˆ¶ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¢å¼ºå¼€æºLLMæ•°æ®åˆ†æèƒ½åŠ›çš„ç­–ç•¥ã€‚é€šè¿‡ç­–åˆ’å¤šæ ·åŒ–çš„çœŸå®åœºæ™¯ç§å­æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ¨¡å‹åœ¨æ•°æ®ç†è§£ã€ä»£ç ç”Ÿæˆå’Œæˆ˜ç•¥è§„åˆ’ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ä¸Šçš„è¡¨ç°ã€‚åˆ†æç»“æœæ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šæˆ˜ç•¥è§„åˆ’è´¨é‡æ˜¯æ¨¡å‹æ€§èƒ½çš„ä¸»è¦å†³å®šå› ç´ ï¼›äº¤äº’è®¾è®¡å’Œä»»åŠ¡å¤æ‚æ€§æ˜¾è‘—å½±å“æ¨ç†èƒ½åŠ›ï¼›æ•°æ®è´¨é‡å¯¹å®ç°æœ€ä½³æ€§èƒ½çš„å½±å“å¤§äºå¤šæ ·æ€§ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ•°æ®åˆæˆæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¼€æºLLMçš„åˆ†ææ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®åˆ†æä»»åŠ¡ä¸­æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ•°æ®åˆ†æåœºæ™¯æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆç†è§£æ•°æ®å’Œç”Ÿæˆç›¸åº”ä»£ç ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ„å»ºå¤šæ ·åŒ–çš„çœŸå®åœºæ™¯æ•°æ®é›†ï¼Œæ¥æå‡æ¨¡å‹åœ¨æ•°æ®ç†è§£ã€ä»£ç ç”Ÿæˆå’Œæˆ˜ç•¥è§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨è¿™äº›ç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œæ‰¾å‡ºå½±å“æ€§èƒ½çš„å…³é”®å› ç´ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è¯„ä¼°å’Œæ€§èƒ½æå‡ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œç­–åˆ’å¤šæ ·åŒ–çš„åœºæ™¯æ•°æ®é›†ï¼›å…¶æ¬¡ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸‹çš„è¡¨ç°ï¼›æœ€åï¼ŒåŸºäºè¯„ä¼°ç»“æœä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†æ•°æ®åˆæˆæ–¹æ³•ï¼Œå¼ºè°ƒæ•°æ®è´¨é‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨æˆ˜ç•¥è§„åˆ’æ–¹é¢çš„ä½œç”¨ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•ä¾§é‡äºæ¨¡å‹æ¶æ„ä¼˜åŒ–çš„æ€è·¯æœ‰æ‰€ä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®åˆæˆè¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†å¤šæ ·åŒ–çš„åœºæ™¯å‚æ•°ï¼Œç¡®ä¿æ•°æ®é›†çš„çœŸå®æ€§å’Œå¤æ‚æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨äº†é’ˆå¯¹æ€§æŸå¤±å‡½æ•°ï¼Œä»¥æå‡æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ”¹è¿›åçš„å¼€æºLLMåœ¨æ•°æ®ç†è§£å’Œæ¨ç†èƒ½åŠ›ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æˆ˜ç•¥è§„åˆ’è´¨é‡ä¸Šï¼Œæ¨¡å‹æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®ç§‘å­¦ã€å•†ä¸šæ™ºèƒ½å’Œè‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒç­‰ã€‚é€šè¿‡æå‡å¼€æºLLMåœ¨æ•°æ®åˆ†æä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¯ä»¥ä¸ºä¼ä¸šå’Œç ”ç©¶æœºæ„æä¾›æ›´é«˜æ•ˆçš„åˆ†æå·¥å…·ï¼Œæ¨åŠ¨æ•°æ®é©±åŠ¨å†³ç­–çš„æ™®åŠä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.

