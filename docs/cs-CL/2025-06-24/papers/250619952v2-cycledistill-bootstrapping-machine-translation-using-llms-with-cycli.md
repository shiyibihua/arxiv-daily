---
layout: default
title: CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation
---

# CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19952" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19952v2</a>
  <a href="https://arxiv.org/pdf/2506.19952.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19952v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19952v2', 'CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Deepon Halder, Thanmay Jayakumar, Raj Dabre

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24 (æ›´æ–°: 2025-08-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCycleDistillä»¥è§£å†³ä½èµ„æºè¯­è¨€æœºå™¨ç¿»è¯‘é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨ç¿»è¯‘` `ä½èµ„æºè¯­è¨€` `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆæˆå¹³è¡Œè¯­æ–™` `è’¸é¦è®­ç»ƒ` `å°‘æ ·æœ¬å­¦ä¹ ` `ç¿»è¯‘è´¨é‡æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿåœ¨ä½èµ„æºè¯­è¨€ä¸Šç¼ºä¹è¶³å¤Ÿçš„å¹³è¡Œè¯­æ–™ï¼Œå¯¼è‡´ç¿»è¯‘è´¨é‡ä¸é«˜ã€‚
2. CycleDistillé€šè¿‡è¿­ä»£ç”Ÿæˆåˆæˆå¹³è¡Œè¯­æ–™åº“ï¼Œåˆ©ç”¨LLMså’Œå°‘é‡æ ·æœ¬ç¿»è¯‘æ¥æå‡æœºå™¨ç¿»è¯‘è´¨é‡ã€‚
3. åœ¨å®éªŒä¸­ï¼ŒCycleDistillåœ¨ä¸‰ç§å°åº¦è¯­è¨€ä¸Šå®ç°äº†20-30 chrFç‚¹çš„æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å°‘é‡æ ·æœ¬æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é«˜è´¨é‡æœºå™¨ç¿»è¯‘ä¸Šä»ä¸åŠä¸“é—¨è®­ç»ƒçš„MTç³»ç»Ÿï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­ã€‚æœ¬æ–‡æå‡ºCycleDistillï¼Œä¸€ç§åˆ©ç”¨LLMså’Œå°‘é‡æ ·æœ¬ç¿»è¯‘çš„å¼•å¯¼æ–¹æ³•ï¼Œé€šè¿‡é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬MTä»å•è¯­è¯­æ–™åº“è¿­ä»£ç”Ÿæˆåˆæˆå¹³è¡Œè¯­æ–™åº“ï¼Œè¿›è€Œå¾®è°ƒæ¨¡å‹ã€‚CycleDistillä»…éœ€1è‡³4ä¸ªå°‘é‡æ ·æœ¬ç¤ºä¾‹ï¼Œä¾é å•è¯­è¯­æ–™åº“å³å¯å®ç°é«˜è´¨é‡æœºå™¨ç¿»è¯‘ã€‚åœ¨é’ˆå¯¹ä¸‰ç§å°åº¦è¯­è¨€çš„å®éªŒä¸­ï¼ŒCycleDistillåœ¨é¦–æ¬¡è¿­ä»£ä¸­å¹³å‡æé«˜äº†20-30ä¸ªchrFç‚¹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†åœ¨è’¸é¦è¿‡ç¨‹ä¸­åˆ©ç”¨softmaxæ¿€æ´»çš„æ•ˆæœï¼Œè§‚å¯Ÿåˆ°ç¿»è¯‘è´¨é‡çš„è½»å¾®æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä½èµ„æºè¯­è¨€æœºå™¨ç¿»è¯‘ä¸­å¹³è¡Œè¯­æ–™ç¨€ç¼ºçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤§é‡å¹³è¡Œè¯­æ–™ï¼Œå¯¼è‡´åœ¨ä½èµ„æºè¯­è¨€ä¸Šè¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCycleDistillçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¿­ä»£ç”Ÿæˆåˆæˆå¹³è¡Œè¯­æ–™åº“ï¼Œåˆ©ç”¨LLMsè¿›è¡Œé›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬ç¿»è¯‘ï¼Œä»è€Œå‡å°‘å¯¹å¹³è¡Œè¯­æ–™çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‡ ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä»å•è¯­è¯­æ–™åº“ç”Ÿæˆåˆæˆå¹³è¡Œè¯­æ–™ï¼›ç„¶åï¼Œä½¿ç”¨ç”Ÿæˆçš„å¹³è¡Œè¯­æ–™å¾®è°ƒç¿»è¯‘æ¨¡å‹ï¼›æœ€åï¼Œé‡å¤è¿™ä¸€è¿‡ç¨‹ä»¥è¿›ä¸€æ­¥æå‡ç¿»è¯‘è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCycleDistillçš„åˆ›æ–°åœ¨äºå…¶è¿­ä»£ç”Ÿæˆåˆæˆå¹³è¡Œè¯­æ–™çš„èƒ½åŠ›ï¼Œä½¿å¾—åœ¨ç¼ºä¹å¹³è¡Œè¯­æ–™çš„æƒ…å†µä¸‹ä»èƒ½å®ç°é«˜è´¨é‡ç¿»è¯‘ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–å¤§é‡å¹³è¡Œè¯­æ–™çš„æ–¹å¼æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼ŒCycleDistillä»…éœ€1è‡³4ä¸ªå°‘é‡æ ·æœ¬ç¤ºä¾‹ï¼Œå¹¶ä¸”åœ¨è’¸é¦è¿‡ç¨‹ä¸­å¼•å…¥softmaxæ¿€æ´»ä»¥æå‡ç¿»è¯‘è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨é’ˆå¯¹ä¸‰ç§å°åº¦è¯­è¨€çš„å®éªŒä¸­ï¼ŒCycleDistillåœ¨é¦–æ¬¡è¿­ä»£ä¸­å¹³å‡æé«˜äº†20-30ä¸ªchrFç‚¹ï¼Œç›¸è¾ƒäºå°‘æ ·æœ¬åŸºçº¿æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„ç¿»è¯‘è´¨é‡æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CycleDistillçš„ç ”ç©¶æˆæœåœ¨ä½èµ„æºè¯­è¨€çš„æœºå™¨ç¿»è¯‘é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©æå‡è¿™äº›è¯­è¨€çš„ç¿»è¯‘è´¨é‡ï¼Œä¿ƒè¿›è·¨è¯­è¨€äº¤æµå’Œä¿¡æ¯è·å–ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„çµæ´»æ€§ä½¿å…¶é€‚ç”¨äºå¤šç§è¯­è¨€å¯¹çš„ç¿»è¯‘ä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs), despite their ability to perform few-shot machine translation (MT), often lag behind dedicated MT systems trained on parallel corpora, which are crucial for high quality machine translation (MT). However, parallel corpora are often scarce or non-existent for low-resource languages. In this paper, we propose CycleDistill, a bootstrapping approach leveraging LLMs and few-shot translation to obtain high-quality MT systems. CycleDistill involves iteratively generating synthetic parallel corpora from monolingual corpora via zero- or few-shot MT, which is then used to fine-tune the model that was used for generating said data for MT. CycleDistill does not need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments focusing on three Indian languages, by relying solely on monolingual corpora, it can achieve high-quality machine translation, improving upon a few-shot baseline model by over 20-30 chrF points on average in the first iteration. We also study the effect of leveraging softmax activations during the distillation process and observe mild improvements in translation quality.

