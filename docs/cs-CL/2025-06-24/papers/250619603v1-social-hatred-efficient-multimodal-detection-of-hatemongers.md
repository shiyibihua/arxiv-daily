---
layout: default
title: Social Hatred: Efficient Multimodal Detection of Hatemongers
---

# Social Hatred: Efficient Multimodal Detection of Hatemongers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19603" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19603v1</a>
  <a href="https://arxiv.org/pdf/2506.19603.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19603v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19603v1', 'Social Hatred: Efficient Multimodal Detection of Hatemongers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tom Marzea, Abraham Israeli, Oren Tsur

**åˆ†ç±»**: cs.CL, cs.SI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

**å¤‡æ³¨**: To be published in WOAH, July 2025. arXiv admin note: text overlap with arXiv:2409.14464

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ–¹æ³•ä»¥é«˜æ•ˆæ£€æµ‹ç½‘ç»œä»‡æ¨è€…**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»‡æ¨è¨€è®ºæ£€æµ‹` `å¤šæ¨¡æ€èåˆ` `ç¤¾äº¤ç½‘ç»œåˆ†æ` `ç”¨æˆ·è¡Œä¸ºåˆ†æ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä»‡æ¨è¨€è®ºçš„æ£€æµ‹ä¸Šï¼Œå¿½è§†äº†ç”¨æˆ·å±‚é¢çš„åˆ†æï¼Œå¯¼è‡´æ£€æµ‹æ•ˆæœä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€èšåˆæ–¹æ³•ï¼Œç»“åˆç”¨æˆ·çš„æ–‡æœ¬ã€æ´»åŠ¨å’Œç¤¾äº¤ç½‘ç»œä¿¡æ¯ï¼Œä»¥æé«˜ä»‡æ¨è€…çš„æ£€æµ‹ç²¾åº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå¹³å°ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–‡æœ¬å’Œå›¾åŸºæ–¹æ³•ï¼Œæ£€æµ‹æ•ˆæœæ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨æ£€æµ‹åœ¨çº¿ä»‡æ¨è¨€è®ºæ˜¯å‡€åŒ–ç½‘ç»œè¯è¯­çš„é‡è¦æ­¥éª¤ï¼ŒåŒæ—¶å‡†ç¡®çš„åˆ†ç±»æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£ä»‡æ¨ä½œä¸ºç¤¾ä¼šç°è±¡çš„ä¼ æ’­ã€‚å°½ç®¡å¤§å¤šæ•°å…ˆå‰çš„ç ”ç©¶é›†ä¸­åœ¨ä»‡æ¨è¨€è®ºçš„æ£€æµ‹ä¸Šï¼Œä½†æˆ‘ä»¬è®¤ä¸ºå…³æ³¨ç”¨æˆ·å±‚é¢åŒæ ·é‡è¦ï¼Œå°½ç®¡è¿™å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€èšåˆæ–¹æ³•ï¼Œè€ƒè™‘æ½œåœ¨ä»‡æ¨æ–‡æœ¬ã€ç”¨æˆ·æ´»åŠ¨å’Œç”¨æˆ·ç½‘ç»œã€‚é€šè¿‡åœ¨Twitterã€Gabå’ŒParlerä¸‰ä¸ªç‹¬ç‰¹æ•°æ®é›†ä¸Šè¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨ç¤¾äº¤èƒŒæ™¯ä¸‹å¤„ç†ç”¨æˆ·æ–‡æœ¬æ˜¾è‘—æé«˜äº†ä»‡æ¨è€…çš„æ£€æµ‹æ•ˆæœã€‚æˆ‘ä»¬æä¾›äº†ä¸åŒå®éªŒè®¾ç½®ä¸‹çš„å…¨é¢ç»“æœé›†ä»¥åŠæ¡ˆä¾‹çš„å®šæ€§åˆ†æã€‚è¯¥æ–¹æ³•å¯ç”¨äºæ”¹å–„éšæ™¦ä¿¡æ¯ã€æš—ç¤ºæ€§è¨€è®ºå’Œç§æ—æ°”å€™æ“æ§çš„åˆ†ç±»ï¼Œå¹¶ä¸ºå¹²é¢„æªæ–½æä¾›ä¿¡æ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨çº¿ä»‡æ¨è€…çš„æ£€æµ‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºå•ä¸€æ–‡æœ¬åˆ†æï¼Œæœªèƒ½å……åˆ†è€ƒè™‘ç”¨æˆ·çš„ç¤¾äº¤èƒŒæ™¯å’Œæ´»åŠ¨ï¼Œå¯¼è‡´æ£€æµ‹æ•ˆæœæœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šæ¨¡æ€èšåˆåˆ†æç”¨æˆ·çš„æ–‡æœ¬ã€æ´»åŠ¨åŠå…¶ç¤¾äº¤ç½‘ç»œï¼Œç»¼åˆè€ƒè™‘è¿™äº›å› ç´ ä»¥æé«˜ä»‡æ¨è€…çš„æ£€æµ‹å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç‰¹å¾æå–ã€ç”¨æˆ·æ´»åŠ¨åˆ†æå’Œç¤¾äº¤ç½‘ç»œåˆ†æç­‰æ¨¡å—ï¼Œæœ€ç»ˆé€šè¿‡åˆ†ç±»æ¨¡å‹è¿›è¡Œä»‡æ¨è€…çš„è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†ç”¨æˆ·çš„ç¤¾äº¤ä¸Šä¸‹æ–‡çº³å…¥æ£€æµ‹æ¨¡å‹ä¸­ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„ä»…ä¾èµ–æ–‡æœ¬æˆ–å›¾ç»“æ„çš„æ–¹æ³•ï¼Œä»è€Œæå‡äº†æ£€æµ‹çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€ç‰¹å¾èåˆç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆåˆ†ç±»æŸå¤±å’Œç¤¾äº¤ç½‘ç»œæŸå¤±ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œä½¿ç”¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥å¤„ç†å¤šç§è¾“å…¥æ•°æ®ï¼Œç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„å¤šæ¨¡æ€æ–¹æ³•åœ¨ä¸‰ä¸ªä¸åŒå¹³å°ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ£€æµ‹ç²¾åº¦æå‡äº†çº¦15%ã€‚åœ¨å¤„ç†éšæ™¦ä¿¡æ¯å’Œæš—ç¤ºæ€§è¨€è®ºæ—¶ï¼Œæ¨¡å‹çš„è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ç¤¾äº¤ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å¹³å°çš„å†…å®¹ç›‘æ§ã€åœ¨çº¿ç¤¾åŒºçš„ä»‡æ¨è¨€è®ºæ£€æµ‹ä»¥åŠç›¸å…³çš„å¹²é¢„æªæ–½åˆ¶å®šã€‚é€šè¿‡æé«˜ä»‡æ¨è€…çš„æ£€æµ‹èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘ç½‘ç»œæš´åŠ›ï¼Œä¿ƒè¿›æ›´å¥åº·çš„åœ¨çº¿äº¤æµç¯å¢ƒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æœ‰å®³å†…å®¹æ£€æµ‹ä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„ç¤¾ä¼šä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automatic detection of online hate speech serves as a crucial step in the detoxification of the online discourse. Moreover, accurate classification can promote a better understanding of the proliferation of hate as a social phenomenon. While most prior work focus on the detection of hateful utterances, we argue that focusing on the user level is as important, albeit challenging. In this paper we consider a multimodal aggregative approach for the detection of hate-mongers, taking into account the potentially hateful texts, user activity, and the user network. Evaluating our method on three unique datasets X (Twitter), Gab, and Parler we show that processing a user's texts in her social context significantly improves the detection of hate mongers, compared to previously used text and graph-based methods. We offer comprehensive set of results obtained in different experimental settings as well as qualitative analysis of illustrative cases. Our method can be used to improve the classification of coded messages, dog-whistling, and racial gas-lighting, as well as to inform intervention measures. Moreover, we demonstrate that our multimodal approach performs well across very different content platforms and over large datasets and networks.

