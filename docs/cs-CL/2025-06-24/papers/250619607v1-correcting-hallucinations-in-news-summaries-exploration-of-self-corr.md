---
layout: default
title: Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge
---

# Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19607" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19607v1</a>
  <a href="https://arxiv.org/pdf/2506.19607.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19607v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19607v1', 'Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Juraj Vladika, Ihsan Soydemir, Florian Matthes

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

**å¤‡æ³¨**: Accepted to FEVER @ ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæˆ‘çº æ­£æ–¹æ³•ä»¥è§£å†³æ–°é—»æ‘˜è¦ä¸­çš„è™šå‡ä¿¡æ¯é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘çº æ­£` `è™šå‡ä¿¡æ¯` `æ–°é—»æ‘˜è¦` `å¤–éƒ¨çŸ¥è¯†` `å¤šè½®äº¤äº’` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–°é—»æ‘˜è¦æ—¶å¸¸å‡ºç°è™šå‡ä¿¡æ¯ï¼Œå½±å“ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚
2. æœ¬æ–‡æå‡ºè‡ªæˆ‘çº æ­£æ–¹æ³•ï¼Œé€šè¿‡å¤šè½®ç”ŸæˆéªŒè¯é—®é¢˜å¹¶åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†è¿›è¡Œä¿®æ­£ï¼Œä»è€Œæé«˜æ‘˜è¦çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨çº æ­£è™šå‡æ‘˜è¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”ä¸äººå·¥è¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆè¿è´¯æ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´è™šå‡ä¿¡æ¯çš„é—®é¢˜ï¼Œå³äº‹å®ä¸å‡†ç¡®çš„é™ˆè¿°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æ¢è®¨äº†è‡ªæˆ‘çº æ­£æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åˆ©ç”¨LLMsçš„å¤šè½®ç‰¹æ€§ï¼Œè¿­ä»£ç”ŸæˆéªŒè¯é—®é¢˜ä»¥è¯¢é—®é¢å¤–è¯æ®ï¼Œå¹¶ç”¨å†…éƒ¨æˆ–å¤–éƒ¨çŸ¥è¯†å›ç­”è¿™äº›é—®é¢˜ï¼Œä»è€Œä¿®æ­£åŸå§‹å“åº”ã€‚æˆ‘ä»¬å°†ä¸¤ç§æœ€å…ˆè¿›çš„è‡ªæˆ‘çº æ­£ç³»ç»Ÿåº”ç”¨äºçº æ­£è™šå‡æ–°é—»æ‘˜è¦ï¼Œå¹¶åˆ†æç»“æœï¼Œæ­ç¤ºæœç´¢å¼•æ“ç‰‡æ®µå’Œå°‘é‡ç¤ºä¾‹çš„å®é™…ç›Šå¤„ï¼Œä»¥åŠG-Evalä¸äººå·¥è¯„ä¼°çš„é«˜åº¦ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–°é—»æ‘˜è¦æ—¶äº§ç”Ÿçš„è™šå‡ä¿¡æ¯é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆéªŒè¯ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§ï¼Œå¯¼è‡´ä¿¡æ¯å¤±çœŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„è‡ªæˆ‘çº æ­£æ–¹æ³•é€šè¿‡å¤šè½®äº¤äº’ç”ŸæˆéªŒè¯é—®é¢˜ï¼Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æºå¯¹ç”Ÿæˆçš„æ‘˜è¦è¿›è¡Œæ ¡æ­£ï¼Œä»è€Œæé«˜ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œç”Ÿæˆåˆæ­¥æ‘˜è¦ï¼›å…¶æ¬¡ï¼ŒåŸºäºåˆæ­¥æ‘˜è¦ç”ŸæˆéªŒè¯é—®é¢˜ï¼›æœ€åï¼Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“å›ç­”è¿™äº›é—®é¢˜å¹¶ä¿®æ­£æ‘˜è¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è‡ªæˆ‘çº æ­£æœºåˆ¶ä¸å¤–éƒ¨çŸ¥è¯†ç»“åˆï¼Œå½¢æˆä¸€ä¸ªè¿­ä»£çš„åé¦ˆå¾ªç¯ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ‘˜è¦çš„å‡†ç¡®æ€§ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šè½®é—®ç­”æœºåˆ¶ï¼Œç»“åˆäº†å¤šä¸ªæœç´¢å¼•æ“çš„çŸ¥è¯†ç‰‡æ®µï¼Œå¹¶é€šè¿‡å°‘é‡ç¤ºä¾‹æç¤ºæ¥ä¼˜åŒ–æ¨¡å‹çš„å“åº”è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„è‡ªæˆ‘çº æ­£æ–¹æ³•åœ¨çº æ­£è™šå‡æ‘˜è¦æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒG-Evalè¯„åˆ†ä¸äººå·¥è¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–°é—»åª’ä½“ã€ä¿¡æ¯æ£€ç´¢å’Œå†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜æ–°é—»æ‘˜è¦çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿå¢å¼ºå…¬ä¼—å¯¹ä¿¡æ¯çš„ä¿¡ä»»åº¦ï¼Œå‡å°‘è™šå‡ä¿¡æ¯çš„ä¼ æ’­ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œå®é™…å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While large language models (LLMs) have shown remarkable capabilities to generate coherent text, they suffer from the issue of hallucinations -- factually inaccurate statements. Among numerous approaches to tackle hallucinations, especially promising are the self-correcting methods. They leverage the multi-turn nature of LLMs to iteratively generate verification questions inquiring additional evidence, answer them with internal or external knowledge, and use that to refine the original response with the new corrections. These methods have been explored for encyclopedic generation, but less so for domains like news summarization. In this work, we investigate two state-of-the-art self-correcting systems by applying them to correct hallucinated summaries using evidence from three search engines. We analyze the results and provide insights into systems' performance, revealing interesting practical findings on the benefits of search engine snippets and few-shot prompts, as well as high alignment of G-Eval and human evaluation.

