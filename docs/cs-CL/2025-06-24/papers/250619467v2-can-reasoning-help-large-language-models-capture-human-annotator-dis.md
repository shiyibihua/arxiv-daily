---
layout: default
title: Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?
---

# Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19467" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19467v2</a>
  <a href="https://arxiv.org/pdf/2506.19467.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19467v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19467v2', 'Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingwei Ni, Yu Fan, VilÃ©m Zouhar, Donya Rooein, Alexander Hoyle, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Elliott Ash

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24 (æ›´æ–°: 2025-08-04)

**å¤‡æ³¨**: Preprint Under Review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¡¨æ˜æ¨ç†æ–¹æ³•å¯¹å¤§è¯­è¨€æ¨¡å‹æ•æ‰äººç±»æ ‡æ³¨è€…åˆ†æ­§çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `äººç±»æ ‡æ³¨` `æ¨ç†æ–¹æ³•` `åˆ†æ­§å»ºæ¨¡` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ•æ‰äººç±»æ ‡æ³¨è€…çš„åˆ†æ­§æ—¶å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯RLVRæ¨ç†æœªèƒ½æœ‰æ•ˆå»ºæ¨¡è¿™ç§å˜å¼‚ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸åŒæ¨ç†è®¾ç½®ï¼Œæ¢ç´¢å…¶å¯¹LLMåˆ†æ­§å»ºæ¨¡çš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨ç®€å•æ€ç»´é“¾æ¨ç†çš„æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRLVRæ¨ç†é™ä½äº†åˆ†æ­§å»ºæ¨¡çš„æ€§èƒ½ï¼Œè€Œç®€å•æ€ç»´é“¾æ¨ç†æ˜¾è‘—æå‡äº†åŸºäºäººç±»åé¦ˆçš„LLMçš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»æ ‡æ³¨ä¸­çš„å˜å¼‚ï¼ˆå³åˆ†æ­§ï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­å¾ˆå¸¸è§ï¼Œé€šå¸¸åæ˜ äº†ä»»åŠ¡çš„ä¸»è§‚æ€§å’Œæ ·æœ¬çš„æ¨¡ç³Šæ€§ã€‚å»ºæ¨¡è¿™ç§å˜å¼‚å¯¹æ•æ„Ÿäºæ­¤ä¿¡æ¯çš„åº”ç”¨è‡³å…³é‡è¦ã€‚å°½ç®¡RLVRé£æ ¼çš„æ¨ç†ï¼ˆå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šæå‡äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œä½†å°šä¸æ¸…æ¥šè¿™ç§æ¨ç†æ˜¯å¦èƒ½å¸®åŠ©LLMæ•æ‰äººç±»æ ‡æ³¨çš„æœ‰æ„ä¹‰å˜å¼‚ã€‚æœ¬æ–‡è¯„ä¼°äº†ä¸åŒæ¨ç†è®¾ç½®å¯¹LLMåˆ†æ­§å»ºæ¨¡çš„å½±å“ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†æ¨¡å‹è§„æ¨¡ã€åˆ†å¸ƒè¡¨è¾¾æ–¹æ³•å’Œå¼•å¯¼æ–¹æ³•ä¸‹çš„æ¯ç§æ¨ç†è®¾ç½®ï¼Œç»“æœæ˜¾ç¤ºRLVRé£æ ¼çš„æ¨ç†åœ¨åˆ†æ­§å»ºæ¨¡ä¸­é™ä½äº†æ€§èƒ½ï¼Œè€Œç®€å•çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†åˆ™æå‡äº†åŸºäºäººç±»åé¦ˆçš„RLHF LLMçš„æ€§èƒ½ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨åˆ†æ­§é‡è¦æ—¶ç”¨æ¨ç†LLMæ›¿ä»£äººç±»æ ‡æ³¨è€…çš„æ½œåœ¨é£é™©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ•æ‰äººç±»æ ‡æ³¨è€…åˆ†æ­§æ—¶çš„ä¸è¶³ï¼Œç°æœ‰çš„RLVRæ¨ç†æ–¹æ³•æœªèƒ½æœ‰æ•ˆå»ºæ¨¡è¿™ç§å˜å¼‚ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸åŒçš„æ¨ç†è®¾ç½®ï¼Œç‰¹åˆ«æ˜¯æ¯”è¾ƒRLVRæ¨ç†ä¸ç®€å•æ€ç»´é“¾æ¨ç†ï¼ˆCoTï¼‰ï¼Œä»¥æ¢è®¨å…¶å¯¹LLMåˆ†æ­§å»ºæ¨¡çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶è®¾è®¡äº†60ä¸ªå®éªŒè®¾ç½®ï¼Œæ¶µç›–3ä¸ªä»»åŠ¡ï¼Œè¯„ä¼°äº†ä¸åŒæ¨¡å‹è§„æ¨¡ã€åˆ†å¸ƒè¡¨è¾¾æ–¹æ³•å’Œå¼•å¯¼æ–¹æ³•ä¸‹çš„æ¨ç†æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå‘ç°RLVRæ¨ç†åœ¨åˆ†æ­§å»ºæ¨¡ä¸­åè€Œé™ä½äº†æ€§èƒ½ï¼Œè€Œç®€å•æ€ç»´é“¾æ¨ç†åˆ™èƒ½æœ‰æ•ˆæå‡åŸºäºäººç±»åé¦ˆçš„LLMçš„è¡¨ç°ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„é¢„æœŸæ•ˆæœæˆªç„¶ç›¸åã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­å¯¹æ¨¡å‹è§„æ¨¡ã€æ¨ç†æ–¹æ³•å’Œå¼•å¯¼ç­–ç•¥è¿›è¡Œäº†ç»†è‡´çš„å‚æ•°è®¾ç½®ï¼Œç¡®ä¿äº†è¯„ä¼°çš„å…¨é¢æ€§å’Œç»“æœçš„å¯é æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLVRæ¨ç†åœ¨åˆ†æ­§å»ºæ¨¡ä¸­å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè€Œç®€å•æ€ç»´é“¾æ¨ç†ä½¿åŸºäºäººç±»åé¦ˆçš„LLMæ€§èƒ½æå‡ï¼Œå…·ä½“æå‡å¹…åº¦æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œä½†è¿™ä¸€å‘ç°å¯¹æ¨ç†æ–¹æ³•çš„é€‰æ‹©å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ ‡æ³¨ä»»åŠ¡ã€æƒ…æ„Ÿåˆ†æå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æ›´å¥½åœ°ç†è§£äººç±»æ ‡æ³¨è€…çš„åˆ†æ­§ï¼ŒLLMå¯ä»¥åœ¨å¤„ç†ä¸»è§‚æ€§å’Œæ¨¡ç³Šæ€§è¾ƒé«˜çš„ä»»åŠ¡æ—¶æä¾›æ›´å‡†ç¡®çš„ç»“æœï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„è‡ªåŠ¨æ ‡æ³¨ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Variation in human annotation (i.e., disagreements) is common in NLP, often reflecting important information like task subjectivity and sample ambiguity. Modeling this variation is important for applications that are sensitive to such information. Although RLVR-style reasoning (Reinforcement Learning with Verifiable Rewards) has improved Large Language Model (LLM) performance on many tasks, it remains unclear whether such reasoning enables LLMs to capture informative variation in human annotation. In this work, we evaluate the influence of different reasoning settings on LLM disagreement modeling. We systematically evaluate each reasoning setting across model sizes, distribution expression methods, and steering methods, resulting in 60 experimental setups across 3 tasks. Surprisingly, our results show that RLVR-style reasoning degrades performance in disagreement modeling, while naive Chain-of-Thought (CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback). These findings underscore the potential risk of replacing human annotators with reasoning LLMs, especially when disagreements are important.

