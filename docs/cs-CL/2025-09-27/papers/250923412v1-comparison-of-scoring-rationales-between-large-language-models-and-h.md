---
layout: default
title: Comparison of Scoring Rationales Between Large Language Models and Human Raters
---

# Comparison of Scoring Rationales Between Large Language Models and Human Raters

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23412" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23412v1</a>
  <a href="https://arxiv.org/pdf/2509.23412.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23412v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23412v1', 'Comparison of Scoring Rationales Between Large Language Models and Human Raters')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haowei Hua, Hong Jiao, Dan Song

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: 23 Pages, 4 Tables, 13 Figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¯¹æ¯”å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»è¯„åˆ†è€…çš„è¯„åˆ†ç†ç”±ï¼Œæ¢ç©¶è‡ªåŠ¨è¯„åˆ†ä¸€è‡´æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨è¯„åˆ†` `è¯„åˆ†ç†ç”±` `ä¸€è‡´æ€§åˆ†æ` `æ•™è‚²è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªåŠ¨è¯„åˆ†æ–¹æ³•ç¼ºä¹å¯¹è¯„åˆ†ç†ç”±çš„æ·±å…¥ç†è§£ï¼Œå¯¼è‡´è¯„åˆ†ä¸€è‡´æ€§éš¾ä»¥ä¿è¯ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„ä¸»è§‚é¢˜è¯„åˆ†ä¸­ã€‚
2. æœ¬ç ”ç©¶å¯¹æ¯”äººç±»å’ŒLLMçš„è¯„åˆ†ç†ç”±ï¼Œé€šè¿‡ç›¸ä¼¼åº¦åˆ†æå’Œèšç±»åˆ†æï¼Œæ­ç¤ºLLMè¯„åˆ†çš„å†…åœ¨é€»è¾‘ï¼Œä»è€Œæé«˜è¯„åˆ†ä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŒLLMåœ¨è¯„åˆ†å‡†ç¡®æ€§ä¸Šå­˜åœ¨å·®å¼‚ï¼Œä¸”å…¶è¯„åˆ†ç†ç”±ä¸äººç±»è¯„åˆ†è€…å­˜åœ¨å·®å¼‚ï¼Œä¸ºæ”¹è¿›LLMè‡ªåŠ¨è¯„åˆ†æä¾›äº†ä¾æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨è¯„åˆ†çš„è¿›æ­¥ä¸æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„è¿›æ­¥å¯†åˆ‡ç›¸å…³ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼ŒChatGPTã€Geminiã€Claudeç­‰ç”Ÿæˆå¼äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººå·²è¢«ç”¨äºè‡ªåŠ¨è¯„åˆ†ã€‚é‰´äºLLMå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå®ƒä»¬è¿˜å¯ä»¥ç”Ÿæˆè¯„åˆ†ç†ç”±æ¥æ”¯æŒå…¶ç»™å‡ºçš„åˆ†æ•°ã€‚å› æ­¤ï¼Œè¯„ä¼°äººç±»å’ŒLLMè¯„åˆ†è€…æä¾›çš„ç†ç”±æœ‰åŠ©äºç†è§£ä¸¤è€…åœ¨è¯„åˆ†æ—¶åº”ç”¨çš„æ¨ç†æ–¹å¼ï¼Œä»è€Œå‘ç°è¯„åˆ†ä¸ä¸€è‡´çš„æ½œåœ¨åŸå› ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†äººç±»å’ŒLLMè¯„åˆ†è€…çš„ç†ç”±ï¼Œä»¥è¯†åˆ«è¯„åˆ†ä¸ä¸€è‡´çš„æ½œåœ¨åŸå› ã€‚ä½¿ç”¨æ¥è‡ªå¤§è§„æ¨¡æµ‹è¯•çš„ä½œæ–‡ï¼ŒåŸºäºäºŒæ¬¡åŠ æƒkappaå’Œå½’ä¸€åŒ–äº’ä¿¡æ¯ï¼Œæ£€éªŒäº†GPT-4oã€Geminiå’Œå…¶ä»–LLMçš„è¯„åˆ†å‡†ç¡®æ€§ã€‚ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è¯„ä¼°æ‰€æä¾›ç†ç”±çš„ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼ŒåŸºäºç†ç”±åµŒå…¥çš„ä¸»æˆåˆ†åˆ†æï¼Œæ¢ç´¢äº†ç†ç”±ä¸­çš„èšç±»æ¨¡å¼ã€‚æœ¬ç ”ç©¶çš„å‘ç°æ·±å…¥äº†è§£äº†LLMåœ¨è‡ªåŠ¨è¯„åˆ†ä¸­çš„å‡†ç¡®æ€§å’Œâ€œæ€è€ƒâ€æ–¹å¼ï¼Œæœ‰åŠ©äºæ›´å¥½åœ°ç†è§£äººç±»è¯„åˆ†å’ŒåŸºäºLLMçš„è‡ªåŠ¨è¯„åˆ†èƒŒåçš„ç†ç”±ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨è¯„åˆ†é¢†åŸŸä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„åˆ†ç»“æœä¸äººç±»è¯„åˆ†ç»“æœä¸ä¸€è‡´çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹LLMè¯„åˆ†ç†ç”±çš„æ·±å…¥åˆ†æï¼Œéš¾ä»¥ç†è§£LLMçš„è¯„åˆ†é€»è¾‘ï¼Œä»è€Œå¯¼è‡´è¯„åˆ†ç»“æœéš¾ä»¥è§£é‡Šå’Œä¿¡ä»»ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•ä¹Ÿç¼ºä¹å¯¹ä¸åŒLLMä¹‹é—´è¯„åˆ†å·®å¼‚çš„ç³»ç»Ÿæ€§æ¯”è¾ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æ¯”LLMå’Œäººç±»è¯„åˆ†è€…çš„è¯„åˆ†ç†ç”±ï¼Œæ¥ç†è§£LLMçš„è¯„åˆ†é€»è¾‘ï¼Œå¹¶æ‰¾å‡ºå¯¼è‡´è¯„åˆ†ä¸ä¸€è‡´çš„åŸå› ã€‚å…·ä½“è€Œè¨€ï¼Œè®ºæ–‡ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¡¡é‡è¯„åˆ†ç†ç”±çš„ç›¸ä¼¼æ€§ï¼Œå¹¶ä½¿ç”¨ä¸»æˆåˆ†åˆ†æå’Œèšç±»åˆ†ææ¥æ¢ç´¢è¯„åˆ†ç†ç”±ä¸­çš„æ½œåœ¨æ¨¡å¼ã€‚é€šè¿‡è¿™äº›åˆ†æï¼Œå¯ä»¥æ·±å…¥äº†è§£LLMçš„â€œæ€è€ƒâ€æ–¹å¼ï¼Œå¹¶ä¸ºæ”¹è¿›LLMè‡ªåŠ¨è¯„åˆ†æä¾›ä¾æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) æ•°æ®æ”¶é›†ï¼šæ”¶é›†å¤§è§„æ¨¡æµ‹è¯•ä¸­çš„ä½œæ–‡åŠå…¶äººç±»è¯„åˆ†å’ŒLLMè¯„åˆ†ï¼›2) è¯„åˆ†ç†ç”±ç”Ÿæˆï¼šè¦æ±‚äººç±»è¯„åˆ†è€…å’ŒLLMæä¾›è¯„åˆ†ç†ç”±ï¼›3) è¯„åˆ†å‡†ç¡®æ€§è¯„ä¼°ï¼šä½¿ç”¨äºŒæ¬¡åŠ æƒkappaå’Œå½’ä¸€åŒ–äº’ä¿¡æ¯è¯„ä¼°LLMçš„è¯„åˆ†å‡†ç¡®æ€§ï¼›4) è¯„åˆ†ç†ç”±ç›¸ä¼¼æ€§åˆ†æï¼šä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è¯„ä¼°äººç±»å’ŒLLMè¯„åˆ†ç†ç”±çš„ç›¸ä¼¼æ€§ï¼›5) è¯„åˆ†ç†ç”±èšç±»åˆ†æï¼šä½¿ç”¨ä¸»æˆåˆ†åˆ†æå’Œèšç±»åˆ†ææ¢ç´¢è¯„åˆ†ç†ç”±ä¸­çš„æ½œåœ¨æ¨¡å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ç³»ç»Ÿæ€§åœ°å¯¹æ¯”äº†äººç±»å’ŒLLMçš„è¯„åˆ†ç†ç”±ï¼Œæ­ç¤ºäº†LLMè¯„åˆ†çš„å†…åœ¨é€»è¾‘ï¼›2) ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å’Œèšç±»åˆ†æç­‰æ–¹æ³•ï¼Œæ·±å…¥åˆ†æäº†è¯„åˆ†ç†ç”±çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ï¼›3) è¯„ä¼°äº†ä¸åŒLLMåœ¨è‡ªåŠ¨è¯„åˆ†ä¸­çš„è¡¨ç°ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬çš„è¯„åˆ†ç†ç”±ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨äºŒæ¬¡åŠ æƒkappaå’Œå½’ä¸€åŒ–äº’ä¿¡æ¯æ¥è¯„ä¼°è¯„åˆ†å‡†ç¡®æ€§ï¼Œè¿™ä¸¤ç§æŒ‡æ ‡èƒ½å¤Ÿæœ‰æ•ˆè¡¡é‡è¯„åˆ†è€…ä¹‹é—´çš„ä¸€è‡´æ€§ï¼›2) ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¡¡é‡è¯„åˆ†ç†ç”±çš„ç›¸ä¼¼æ€§ï¼Œè¿™æ˜¯ä¸€ç§å¸¸ç”¨çš„æ–‡æœ¬ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼›3) ä½¿ç”¨ä¸»æˆåˆ†åˆ†æå’Œèšç±»åˆ†ææ¥æ¢ç´¢è¯„åˆ†ç†ç”±ä¸­çš„æ½œåœ¨æ¨¡å¼ï¼Œè¿™ä¸¤ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°é™ç»´å’Œå‘ç°æ•°æ®ä¸­çš„ç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4oåœ¨è‡ªåŠ¨è¯„åˆ†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä½†å…¶è¯„åˆ†ç†ç”±ä¸äººç±»è¯„åˆ†è€…å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä½™å¼¦ç›¸ä¼¼åº¦åˆ†ææ˜¾ç¤ºï¼Œä¸åŒLLMä¹‹é—´çš„è¯„åˆ†ç†ç”±ç›¸ä¼¼åº¦è¾ƒé«˜ï¼Œä½†ä¸äººç±»è¯„åˆ†è€…çš„ç›¸ä¼¼åº¦è¾ƒä½ã€‚èšç±»åˆ†ææ­ç¤ºäº†LLMè¯„åˆ†ç†ç”±ä¸­å­˜åœ¨çš„æ½œåœ¨æ¨¡å¼ï¼Œä¸ºæ”¹è¿›LLMè‡ªåŠ¨è¯„åˆ†æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ•™è‚²è¯„ä¼°ã€å†…å®¹å®¡æ ¸ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚é€šè¿‡æ·±å…¥ç†è§£LLMçš„è¯„åˆ†é€»è¾‘ï¼Œå¯ä»¥æé«˜è‡ªåŠ¨è¯„åˆ†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå‡å°‘äººå·¥å¹²é¢„ï¼Œæé«˜æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥å¸®åŠ©å¼€å‘æ›´æ™ºèƒ½çš„æ•™è‚²è¾…åŠ©å·¥å…·ï¼Œä¸ºå­¦ç”Ÿæä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ åé¦ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Advances in automated scoring are closely aligned with advances in machine-learning and natural-language-processing techniques. With recent progress in large language models (LLMs), the use of ChatGPT, Gemini, Claude, and other generative-AI chatbots for automated scoring has been explored. Given their strong reasoning capabilities, LLMs can also produce rationales to support the scores they assign. Thus, evaluating the rationales provided by both human and LLM raters can help improve the understanding of the reasoning that each type of rater applies when assigning a score. This study investigates the rationales of human and LLM raters to identify potential causes of scoring inconsistency. Using essays from a large-scale test, the scoring accuracy of GPT-4o, Gemini, and other LLMs is examined based on quadratic weighted kappa and normalized mutual information. Cosine similarity is used to evaluate the similarity of the rationales provided. In addition, clustering patterns in rationales are explored using principal component analysis based on the embeddings of the rationales. The findings of this study provide insights into the accuracy and ``thinking'' of LLMs in automated scoring, helping to improve the understanding of the rationales behind both human scoring and LLM-based automated scoring.

