---
layout: default
title: The Impact of Role Design in In-Context Learning for Large Language Models
---

# The Impact of Role Design in In-Context Learning for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23501" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23501v1</a>
  <a href="https://arxiv.org/pdf/2509.23501.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23501v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23501v1', 'The Impact of Role Design in In-Context Learning for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hamidreza Rouzegar, Masoud Makrehchi

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: Code is available at https://github.com/hrouzegar/Role_Based-In-Context-Learning

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è§’è‰²è®¾è®¡å¯¹å¤§è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡å­¦ä¹ çš„å½±å“ï¼Œæå‡æ¨¡å‹åœ¨å¤šä»»åŠ¡ä¸Šçš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `è§’è‰²è®¾è®¡` `æç¤ºå·¥ç¨‹` `å¤§è¯­è¨€æ¨¡å‹` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ç¼ºä¹å¯¹æç¤ºä¸­è§’è‰²è®¾è®¡çš„æ·±å…¥ç ”ç©¶ï¼Œå¯èƒ½é™åˆ¶äº†å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚
2. æœ¬æ–‡æ¢ç´¢äº†åœ¨æç¤ºä¸­å¼•å…¥è§’è‰²ä¿¡æ¯ï¼Œä»¥å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œä»»åŠ¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç²¾å¿ƒè®¾è®¡çš„è§’è‰²æç¤ºèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†è§’è‰²è®¾è®¡å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„å½±å“ã€‚ä¸Šä¸‹æ–‡å­¦ä¹ ä½¿LLMèƒ½å¤Ÿåœ¨æ²¡æœ‰é¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒåŸºäºæç¤ºç”Ÿæˆé¢„æµ‹ã€‚è™½ç„¶æç¤ºå·¥ç¨‹å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†æç¤ºä¸­è§’è‰²è®¾è®¡çš„å½±å“ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ä½¿ç”¨OpenAIçš„GPT-3.5å’ŒGPT-4oä»¥åŠMetaçš„Llama2-7bå’ŒLlama2-13bï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­æ£€éªŒäº†è§’è‰²é…ç½®çš„å½±å“ã€‚æˆ‘ä»¬è¯„ä¼°äº†æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œé‡ç‚¹å…³æ³¨æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»ã€é—®ç­”å’Œæ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºè§’è‰²çš„æç¤ºç»“æ„å…·æœ‰å¢å¼ºLLMæ€§èƒ½çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ï¼Œæç¤ºå·¥ç¨‹å¯¹æ¨¡å‹æ€§èƒ½å½±å“ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶å…³æ³¨è§’è‰²è®¾è®¡åœ¨æç¤ºä¸­çš„ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æç¤ºè¯çš„é€‰æ‹©å’Œæ’åºï¼Œå¿½ç•¥äº†è§’è‰²æ‰®æ¼”å¯¹æ¨¡å‹ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„å½±å“ã€‚è¿™ç§å¿½ç•¥å¯èƒ½å¯¼è‡´æ¨¡å‹æ— æ³•å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå½±å“ä»»åŠ¡å®Œæˆçš„è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨æç¤ºä¸­æ˜ç¡®å®šä¹‰æ¨¡å‹çš„è§’è‰²ï¼Œå¼•å¯¼æ¨¡å‹ä»¥ç‰¹å®šèº«ä»½å’Œè§†è§’æ¥ç†è§£å’Œè§£å†³é—®é¢˜ã€‚è¿™ç§è§’è‰²æ‰®æ¼”èƒ½å¤Ÿå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç»„ç»‡çŸ¥è¯†ï¼Œæé«˜æ¨ç†èƒ½åŠ›ï¼Œå¹¶ç”Ÿæˆæ›´ç¬¦åˆé¢„æœŸçš„ç»“æœã€‚é€šè¿‡è§’è‰²è®¾è®¡ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºï¼Œä½¿å…¶æ›´ä¸“æ³¨äºä»»åŠ¡ç›®æ ‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨å®éªŒç ”ç©¶çš„æ–¹æ³•ï¼Œä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) é€‰æ‹©ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»ã€é—®ç­”å’Œæ•°å­¦æ¨ç†ï¼›2) è®¾è®¡åŒ…å«ä¸åŒè§’è‰²ä¿¡æ¯çš„æç¤ºï¼Œä¾‹å¦‚â€œä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°å­¦å®¶â€ï¼›3) ä½¿ç”¨ä¸åŒçš„å¼€æºå’Œé—­æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆGPT-3.5, GPT-4o, Llama2-7b, Llama2-13bï¼‰è¿›è¡Œå®éªŒï¼›4) è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè§’è‰²æç¤ºä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶è¿›è¡Œå¯¹æ¯”åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¼ºè°ƒäº†è§’è‰²è®¾è®¡åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Œå¹¶éªŒè¯äº†å…¶å¯¹å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„ç§¯æå½±å“ã€‚ä¸ä»¥å¾€ä¸»è¦å…³æ³¨æç¤ºè¯æœ¬èº«çš„ç ”ç©¶ä¸åŒï¼Œæœ¬æ–‡å°†è§’è‰²æ‰®æ¼”å¼•å…¥æç¤ºå·¥ç¨‹ï¼Œä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è§’è‰²ä¿¡æ¯çš„é€‰æ‹©ï¼Œéœ€è¦æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹è¿›è¡Œè®¾è®¡ï¼Œä¾‹å¦‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­é€‰æ‹©â€œæ•°å­¦å®¶â€è§’è‰²ï¼›2) æç¤ºçš„ç»“æ„ï¼Œéœ€è¦å°†è§’è‰²ä¿¡æ¯æ¸…æ™°åœ°èå…¥æç¤ºä¸­ï¼Œä¾‹å¦‚â€œä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°å­¦å®¶ï¼Œè¯·è§£å†³ä»¥ä¸‹é—®é¢˜â€ï¼›3) å®éªŒçš„è¯„ä¼°æŒ‡æ ‡ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä¾‹å¦‚å‡†ç¡®ç‡ã€F1å€¼ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸Šï¼Œé€šè¿‡å¼•å…¥è§’è‰²è®¾è®¡ï¼Œå¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨â€œæ•°å­¦å®¶â€è§’è‰²æç¤ºçš„æ¨¡å‹ç›¸æ¯”æ²¡æœ‰è§’è‰²æç¤ºçš„æ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡äº†XX%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œä¸åŒæ¨¡å‹å¯¹è§’è‰²æç¤ºçš„æ•æ„Ÿåº¦ä¸åŒï¼Œè¿™ä¸ºæœªæ¥çš„æ¨¡å‹ä¼˜åŒ–æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†å’Œç”Ÿæˆçš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼ã€å†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è§’è‰²æç¤ºï¼Œå¯ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šæ€§å’Œå¯é æ€§ï¼Œä½¿å…¶æ›´å¥½åœ°æœåŠ¡äºå®é™…åº”ç”¨ã€‚æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ›´å¤æ‚çš„è§’è‰²è®¾è®¡æ–¹æ³•ï¼Œä»¥é€‚åº”æ›´å¹¿æ³›çš„åº”ç”¨éœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context learning (ICL) enables Large Language Models (LLMs) to generate predictions based on prompts without additional fine-tuning. While prompt engineering has been widely studied, the impact of role design within prompts remains underexplored. This study examines the influence of role configurations in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models' performance across datasets, focusing on tasks like sentiment analysis, text classification, question answering, and math reasoning. Our findings suggest the potential of role-based prompt structuring to enhance LLM performance.

