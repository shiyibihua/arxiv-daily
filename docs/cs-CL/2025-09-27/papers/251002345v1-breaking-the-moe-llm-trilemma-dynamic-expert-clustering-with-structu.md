---
layout: default
title: Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression
---

# Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02345" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02345v1</a>
  <a href="https://arxiv.org/pdf/2510.02345.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02345v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02345v1', 'Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peijun Zhu, Ning Yang, Jiayu Wei, Jinghang Wu, Haijun Zhang

**åˆ†ç±»**: cs.CL, cs.AI, cs.DC, cs.LG, cs.NE

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: 12 pages, 2 figures, 3 tables. Under review as a conference paper at ICLR 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåŠ¨æ€ä¸“å®¶èšç±»ä¸ç»“æ„åŒ–å‹ç¼©çš„MoE LLMä¼˜åŒ–æ¡†æ¶ï¼Œè§£å†³è´Ÿè½½ä¸å‡ã€å‚æ•°å†—ä½™å’Œé€šä¿¡å¼€é”€é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `MoE` `åŠ¨æ€èšç±»` `ç»“æ„åŒ–å‹ç¼©` `ä½ç§©åˆ†è§£` `æ¨¡å‹ä¼˜åŒ–` `å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MoE LLMé¢ä¸´è´Ÿè½½ä¸å‡è¡¡ã€å‚æ•°å†—ä½™å’Œé€šä¿¡å¼€é”€çš„ä¸‰é‡å›°å¢ƒï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åŒæ—¶æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºåŠ¨æ€ä¸“å®¶èšç±»å’Œç»“æ„åŒ–å‹ç¼©çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡åœ¨çº¿èšç±»å’Œæƒé‡åˆ†è§£ï¼Œä¼˜åŒ–æ¨¡å‹ç»“æ„å’Œå‚æ•°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒæ¨¡å‹è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†å‚æ•°é‡ã€æé«˜äº†ååé‡ï¼Œå¹¶é™ä½äº†è´Ÿè½½æ–¹å·®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºåŠ¨æ€ä¸“å®¶èšç±»å’Œç»“æ„åŒ–å‹ç¼©ï¼Œæ—¨åœ¨è§£å†³MoE LLMä¸­è´Ÿè½½ä¸å¹³è¡¡ã€å‚æ•°å†—ä½™å’Œé€šä¿¡å¼€é”€è¿™ä¸‰å¤§éš¾é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åœ¨çº¿èšç±»ç¨‹åºï¼Œå®šæœŸä½¿ç”¨å‚æ•°å’Œæ¿€æ´»ç›¸ä¼¼åº¦çš„èåˆæŒ‡æ ‡å¯¹ä¸“å®¶è¿›è¡Œé‡ç»„ï¼Œä»è€Œç¨³å®šä¸“å®¶åˆ©ç”¨ç‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ‰¹åˆ©ç”¨è·¯ç”±å™¨çš„è¯­ä¹‰åµŒå…¥èƒ½åŠ›åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€é‡æ„æ¨¡å‹æ¶æ„ä»¥å®ç°æ˜¾è‘—æ•ˆç‡æå‡çš„æ¡†æ¶ä¹‹ä¸€ã€‚åœ¨æ¯ä¸ªé›†ç¾¤ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“å®¶æƒé‡åˆ†è§£ä¸ºå…±äº«çš„åŸºç¡€çŸ©é˜µå’Œæä½ç§©çš„æ®‹å·®é€‚é…å™¨ï¼Œä»è€Œåœ¨æ¯ä¸ªç»„ä¸­å®ç°é«˜è¾¾äº”å€çš„å‚æ•°ç¼©å‡ï¼ŒåŒæ—¶ä¿æŒä¸“ä¸šåŒ–ã€‚è¿™ç§ç»“æ„æ”¯æŒä¸¤é˜¶æ®µåˆ†å±‚è·¯ç”±ç­–ç•¥ï¼štokené¦–å…ˆè¢«åˆ†é…åˆ°ä¸€ä¸ªé›†ç¾¤ï¼Œç„¶ååˆ†é…åˆ°è¯¥é›†ç¾¤å†…çš„ç‰¹å®šä¸“å®¶ï¼Œä»è€Œå¤§å¤§å‡å°‘äº†è·¯ç”±æœç´¢ç©ºé—´å’Œå…¨äº’è¿é€šä¿¡é‡ã€‚æ­¤å¤–ï¼Œå¼‚æ„ç²¾åº¦æ–¹æ¡ˆï¼ˆå°†å…±äº«åŸºç¡€å­˜å‚¨åœ¨FP16ä¸­ï¼Œå°†æ®‹å·®å› å­å­˜å‚¨åœ¨INT4ä¸­ï¼‰ä¸éæ´»åŠ¨é›†ç¾¤çš„åŠ¨æ€å¸è½½ç›¸ç»“åˆï¼Œå°†å³°å€¼å†…å­˜æ¶ˆè€—é™ä½åˆ°ä¸å¯†é›†æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚åœ¨GLUEå’ŒWikiText-103ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨åŒ¹é…æ ‡å‡†MoEæ¨¡å‹è´¨é‡çš„åŒæ—¶ï¼Œå°†æ€»å‚æ•°å‡å°‘äº†çº¦80%ï¼Œå°†ååé‡æé«˜äº†10%åˆ°20%ï¼Œå¹¶å°†ä¸“å®¶è´Ÿè½½æ–¹å·®é™ä½äº†ä¸‰å€ä»¥ä¸Šã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œç»“æ„é‡ç»„æ˜¯å®ç°å¯æ‰©å±•ã€é«˜æ•ˆå’Œå†…å­˜é«˜æ•ˆçš„MoE LLMçš„ä¸€æ¡å¯è¡Œè·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šMoE LLMåœ¨æ‰©å±•æ¨¡å‹å®¹é‡çš„åŒæ—¶ï¼Œé¢ä¸´ç€è´Ÿè½½ä¸å‡è¡¡ã€å‚æ•°å†—ä½™å’Œé€šä¿¡å¼€é”€ä¸‰å¤§æŒ‘æˆ˜ã€‚è´Ÿè½½ä¸å‡è¡¡å¯¼è‡´éƒ¨åˆ†ä¸“å®¶åˆ©ç”¨ç‡ä½ï¼Œå‚æ•°å†—ä½™å¢åŠ äº†å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼Œè€Œå…¨äº’è¿é€šä¿¡åˆ™å¸¦æ¥äº†å·¨å¤§çš„é€šä¿¡å¼€é”€ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åªèƒ½è§£å†³å…¶ä¸­ä¸€ä¸ªæˆ–ä¸¤ä¸ªé—®é¢˜ï¼Œéš¾ä»¥å®ç°æ•´ä½“ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŠ¨æ€è°ƒæ•´ä¸“å®¶ç»“æ„å’Œå‹ç¼©ä¸“å®¶æƒé‡ï¼Œå®ç°è´Ÿè½½å‡è¡¡ã€å‚æ•°é«˜æ•ˆå’Œé€šä¿¡ä¼˜åŒ–çš„ç»Ÿä¸€ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆé€šè¿‡åœ¨çº¿èšç±»ç®—æ³•å°†ä¸“å®¶åŠ¨æ€åˆ†ç»„ï¼Œä½¿å¾—ç›¸ä¼¼çš„ä¸“å®¶èšé›†åœ¨ä¸€èµ·ï¼Œä»è€Œæé«˜ä¸“å®¶åˆ©ç”¨ç‡ã€‚ç„¶åï¼Œå¯¹æ¯ä¸ªä¸“å®¶ç»„å†…çš„ä¸“å®¶æƒé‡è¿›è¡Œç»“æ„åŒ–å‹ç¼©ï¼Œå‡å°‘å‚æ•°å†—ä½™ã€‚æœ€åï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µåˆ†å±‚è·¯ç”±ç­–ç•¥ï¼Œé™ä½é€šä¿¡å¼€é”€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šåŠ¨æ€ä¸“å®¶èšç±»ã€ç»“æ„åŒ–æƒé‡å‹ç¼©å’Œåˆ†å±‚è·¯ç”±ã€‚åŠ¨æ€ä¸“å®¶èšç±»æ¨¡å—ä½¿ç”¨åœ¨çº¿èšç±»ç®—æ³•ï¼Œå®šæœŸæ ¹æ®å‚æ•°å’Œæ¿€æ´»ç›¸ä¼¼åº¦å¯¹ä¸“å®¶è¿›è¡Œé‡ç»„ã€‚ç»“æ„åŒ–æƒé‡å‹ç¼©æ¨¡å—å°†ä¸“å®¶æƒé‡åˆ†è§£ä¸ºå…±äº«çš„åŸºç¡€çŸ©é˜µå’Œä½ç§©æ®‹å·®é€‚é…å™¨ã€‚åˆ†å±‚è·¯ç”±æ¨¡å—é¦–å…ˆå°†tokenåˆ†é…åˆ°é›†ç¾¤ï¼Œç„¶ååˆ†é…åˆ°é›†ç¾¤å†…çš„ç‰¹å®šä¸“å®¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶è§£å†³MoE LLMçš„è´Ÿè½½ä¸å‡è¡¡ã€å‚æ•°å†—ä½™å’Œé€šä¿¡å¼€é”€ä¸‰å¤§é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜åˆ©ç”¨äº†è·¯ç”±å™¨çš„è¯­ä¹‰åµŒå…¥èƒ½åŠ›ï¼Œåœ¨è®­ç»ƒæœŸé—´åŠ¨æ€é‡æ„æ¨¡å‹æ¶æ„ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€ä¸“å®¶èšç±»é‡‡ç”¨åœ¨çº¿K-meansç®—æ³•ï¼Œä½¿ç”¨å‚æ•°å’Œæ¿€æ´»ç›¸ä¼¼åº¦çš„åŠ æƒèåˆä½œä¸ºè·ç¦»åº¦é‡ã€‚ç»“æ„åŒ–æƒé‡å‹ç¼©å°†ä¸“å®¶æƒé‡åˆ†è§£ä¸ºå…±äº«åŸºç¡€çŸ©é˜µï¼ˆFP16ï¼‰å’Œä½ç§©æ®‹å·®é€‚é…å™¨ï¼ˆINT4ï¼‰ï¼Œå¹¶é‡‡ç”¨å¼‚æ„ç²¾åº¦å­˜å‚¨ã€‚åˆ†å±‚è·¯ç”±é‡‡ç”¨ä¸¤é˜¶æ®µè·¯ç”±ç­–ç•¥ï¼Œå¹¶åŠ¨æ€å¸è½½éæ´»è·ƒé›†ç¾¤ä»¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨GLUEå’ŒWikiText-103æ•°æ®é›†ä¸Šï¼Œåœ¨åŒ¹é…æ ‡å‡†MoEæ¨¡å‹è´¨é‡çš„åŒæ—¶ï¼Œå°†æ€»å‚æ•°å‡å°‘äº†çº¦80%ï¼Œå°†ååé‡æé«˜äº†10%åˆ°20%ï¼Œå¹¶å°†ä¸“å®¶è´Ÿè½½æ–¹å·®é™ä½äº†ä¸‰å€ä»¥ä¸Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æé«˜MoEæ¨¡å‹çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡é™ä½MoEæ¨¡å‹çš„å‚æ•°é‡å’Œè®¡ç®—æˆæœ¬ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨MoE LLMåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„éƒ¨ç½²å’Œåº”ç”¨ï¼Œå¹¶åŠ é€ŸAIæŠ€æœ¯çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an online clustering procedure that periodically regroups experts using a fused metric of parameter and activation similarity, which stabilizes expert utilization. To our knowledge, this is one of the first frameworks to leverage the semantic embedding capability of the router to dynamically reconfigure the model's architecture during training for substantial efficiency gains. Within each cluster, we decompose expert weights into a shared base matrix and extremely low-rank residual adapters, achieving up to fivefold parameter reduction per group while preserving specialization. This structure enables a two-stage hierarchical routing strategy: tokens are first assigned to a cluster, then to specific experts within it, drastically reducing the routing search space and the volume of all-to-all communication. Furthermore, a heterogeneous precision scheme, which stores shared bases in FP16 and residual factors in INT4, coupled with dynamic offloading of inactive clusters, reduces peak memory consumption to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our framework matches the quality of standard MoE models while reducing total parameters by approximately 80%, improving throughput by 10% to 20%, and lowering expert load variance by a factor of over three. Our work demonstrates that structural reorganization is a principled path toward scalable, efficient, and memory-effective MoE LLMs.

