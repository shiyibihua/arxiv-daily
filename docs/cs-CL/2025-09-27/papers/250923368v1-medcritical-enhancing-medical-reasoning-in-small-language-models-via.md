---
layout: default
title: MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction
---

# MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23368" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23368v1</a>
  <a href="https://arxiv.org/pdf/2509.23368.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23368v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23368v1', 'MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xinchun Su, Chunxu Luo, Yixuan Li, Weidong Yang, Lipeng Ma

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**MedCriticalÔºöÈÄöËøáËá™Âçè‰ΩúÊ†°Ê≠£Â¢ûÂº∫Â∞èËØ≠Ë®ÄÊ®°ÂûãÂú®ÂåªÁñóÊé®ÁêÜ‰∏≠ÁöÑËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂåªÁñóÊé®ÁêÜ` `Â∞èËØ≠Ë®ÄÊ®°Âûã` `Áü•ËØÜËí∏È¶è` `Ëá™Âçè‰ΩúÂ≠¶‰π†` `Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ` `ÈïøÈìæÊÄùÁª¥` `Ê®°ÂûãËá™ÂçöÂºà`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â∞èËØ≠Ë®ÄÊ®°ÂûãÂú®ÂåªÁñóÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏çË∂≥ÔºåÁé∞ÊúâÁü•ËØÜËí∏È¶èÊñπÊ≥ï‰æùËµñÂ§ßÂûãÊ®°Âûã‰Ωú‰∏∫ÊïôÂ∏àÔºåÊàêÊú¨È´òÊòÇ„ÄÇ
2. MedCriticalÊ°ÜÊû∂Âà©Áî®Â∞èËØ≠Ë®ÄÊ®°ÂûãËá™ÂçöÂºàÔºåÈÄöËøáÈïøÈìæÊÄùÁª¥Ê®°ÊùøÂºïÂØºÂíåËá™Ëø≠‰ª£DPO‰ºòÂåñÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMedCritical 7BÊ®°ÂûãÂú®CMExamÂü∫ÂáÜ‰∏äË∂ÖË∂äÂêåÁ∫ßÂà´Ê®°ÂûãÔºåÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®ÂåªÂ≠¶È¢ÜÂüüÔºå‰∏¥Â∫äËØäÊñ≠„ÄÅÊ≤ªÁñóËÆ°ÂàíÂíåÂåªÂ≠¶Áü•ËØÜÊï¥ÂêàÁ≠âÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°ÊûÅÂÖ∑ÊåëÊàòÊÄßÔºåÂ∞èËØ≠Ë®ÄÊ®°ÂûãÂú®ËøôÊñπÈù¢ÁöÑË°®Áé∞ÈÄöÂ∏∏‰∏çÂ¶ÇGPT-4ÂíåDeepseekÁ≠âÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇÊúÄËøëÂü∫‰∫éÁü•ËØÜËí∏È¶èÁöÑÊñπÊ≥ïÊó®Âú®ÈÄöËøáÊïôÂ∏àÂºïÂØºÁöÑÈîôËØØÁ∫†Ê≠£Êù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩÜËøôÁßçÂ∞ÜLLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑÊñπÊ≥ïÂú®ÊàêÊú¨„ÄÅÊó∂Èó¥ÂíåÊïàÁéáÊñπÈù¢‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇ‰∏∫‰∫ÜËßÑÈÅøËøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰∏§Èò∂ÊÆµÊ°ÜÊû∂MedCriticalÔºåÂÆÉ‰ΩøÁî®Áî±Â§ßÂûãÊïôÂ∏àÊ®°ÂûãÂæÆË∞ÉÁöÑÂ∞èÂûãËØ≠Ë®ÄÊ®°Âûã‰∏éËá™Ë∫´ËøõË°åÂçöÂºà„ÄÇÂú®Á¨¨‰∏ÄÈò∂ÊÆµÔºåÊàë‰ª¨‰ªéÊïôÂ∏àÊ®°Âûã‰∏≠ÊèêÂèñÈ´òÂ±ÇÊ¨°ÂíåËØ¶ÁªÜÁöÑÈïøÈìæÊÄùÁª¥Ê®°ÊùøÔºå‰ª•ÊåáÂØºÂ≠¶ÁîüÊ®°ÂûãÁîüÊàêÊõ¥Â§çÊùÇÁöÑÊé®ÁêÜÊÄùÁª¥„ÄÇÂú®Á¨¨‰∫åÈò∂ÊÆµÔºåÊàë‰ª¨ÈÄöËøáÊ®°ÂûãËá™Ëø≠‰ª£Âçè‰ΩúÂºïÂÖ•Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÔºåÈÄöËøáÂú®ËÆ≠ÁªÉÊúüÈó¥‰∏éÂæÆË∞ÉÊ®°ÂûãÁöÑÊ†°Ê≠£ËΩ®ËøπËøõË°åÂçöÂºàÊù•Â¢ûÂº∫Â≠¶ÁîüÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËøôÁßçÊ®°ÂûãËá™Â≠¶‰π†DPOÊñπÊ≥ïÊïôÂØºÂ≠¶ÁîüÊ®°ÂûãÂà©Áî®Ëá™Ë∫´ÈîôËØØÈ©±Âä®ÁöÑËßÅËß£Êù•Â∑©Âõ∫ÂÖ∂ÊäÄËÉΩÂíåÁü•ËØÜÔºå‰ªéËÄåËß£ÂÜ≥Â§çÊùÇÈóÆÈ¢òÔºåÂπ∂‰ª•ËæÉ‰ΩéÁöÑÊàêÊú¨ÂÆûÁé∞‰∫Ü‰∏é‰ΩøÁî®ÊïôÂ∏àÊ®°ÂûãÁöÑ‰º†ÁªüÁü•ËØÜËí∏È¶èÊñπÊ≥ïÁõ∏ÂΩìÁöÑÁªìÊûú„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàë‰ª¨ÁöÑMedCritical 7BÊ®°ÂûãÂú®CMExamÂü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫éTaiyiÂíåHuatuo-o1-7BÊ®°ÂûãÔºåÂàÜÂà´ÊèêÈ´ò‰∫Ü3.04ÔºÖÂíå10.12ÔºÖÔºåÂú®7BÁ∫ßÂà´ÁöÑÂ∞èÂûãÊ®°Âûã‰∏≠ÂÆûÁé∞‰∫ÜÊñ∞ÁöÑSOTAÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â∞èËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÂåªÁñóÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÂü∫‰∫éÁü•ËØÜËí∏È¶èÁöÑÊñπÊ≥ï‰æùËµñÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫ÊïôÂ∏àÔºåÂ≠òÂú®ÊàêÊú¨È´ò„ÄÅÊïàÁéá‰ΩéÁ≠âÈóÆÈ¢òÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ËµÑÊ∫êÂèóÈôêÂú∫ÊôØ‰∏ãÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â∞èËØ≠Ë®ÄÊ®°ÂûãËá™Ë∫´ËøõË°åÂ≠¶‰π†ÂíåÊèêÂçáÔºåÈÅøÂÖçÂØπÂ§ßÂûãÊïôÂ∏àÊ®°ÂûãÁöÑ‰æùËµñ„ÄÇÈÄöËøáËÆ©Ê®°Âûã‰∏éËá™Ë∫´ÁîüÊàêÁöÑÊ†°Ê≠£ËΩ®ËøπËøõË°åÂçöÂºàÔºåÊ®°ÊãüÊïôÂ∏àÊåáÂØºÁöÑËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMedCriticalÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) **ÈïøÈìæÊÄùÁª¥Ê®°ÊùøÂºïÂØº**Ôºö‰ªéÂ§ßÂûãÊïôÂ∏àÊ®°Âûã‰∏≠ÊèêÂèñÈ´òÂ±ÇÊ¨°ÂíåËØ¶ÁªÜÁöÑÊé®ÁêÜÈìæÊ®°ÊùøÔºåÁî®‰∫éÊåáÂØºÂ≠¶ÁîüÊ®°ÂûãÁîüÊàêÊõ¥Â§çÊùÇÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ2) **Ëá™Ëø≠‰ª£Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâ**ÔºöÈÄöËøáËÆ©Â≠¶ÁîüÊ®°Âûã‰∏éËá™Ë∫´ÂæÆË∞ÉÂêéÁöÑÊ†°Ê≠£ËΩ®ËøπËøõË°åÂçöÂºàÔºåÂà©Áî®DPOÁÆóÊ≥ï‰ºòÂåñÊ®°ÂûãÁöÑÂÅèÂ•ΩÔºå‰ΩøÂÖ∂Êõ¥ÂÄæÂêë‰∫éÊ≠£Á°ÆÁöÑÊé®ÁêÜË∑ØÂæÑ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂà©Áî®Ê®°ÂûãËá™Ë∫´ËøõË°åÂ≠¶‰π†Âíå‰ºòÂåñÔºåÈÅøÂÖç‰∫ÜÂØπÂ§ßÂûãÊïôÂ∏àÊ®°ÂûãÁöÑ‰æùËµñ„ÄÇÈÄöËøáËá™ÂçöÂºàÂíåDPOÁÆóÊ≥ïÔºåÂÆûÁé∞‰∫ÜÁü•ËØÜÁöÑËá™ÊàëÊèêÁÇºÂíåËÉΩÂäõÁöÑÊèêÂçáÔºåÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨ÔºåÊèêÈ´ò‰∫ÜÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÈïøÈìæÊÄùÁª¥Ê®°ÊùøÁöÑËÆæËÆ°ÈúÄË¶Å‰ªîÁªÜËÄÉËôëÔºå‰ª•Á°Æ‰øùËÉΩÂ§üÊúâÊïàÂú∞ÂºïÂØºÂ≠¶ÁîüÊ®°ÂûãËøõË°åÊé®ÁêÜ„ÄÇDPOÁÆóÊ≥ï‰∏≠ÁöÑÂ•ñÂä±ÂáΩÊï∞ÈúÄË¶ÅÂêàÁêÜËÆæËÆ°Ôºå‰ª•Âå∫ÂàÜÊ≠£Á°ÆÁöÑÊé®ÁêÜË∑ØÂæÑÂíåÈîôËØØÁöÑÊé®ÁêÜË∑ØÂæÑ„ÄÇÊ≠§Â§ñÔºåËá™Ëø≠‰ª£ÁöÑÊ¨°Êï∞‰πüÈúÄË¶ÅËøõË°åË∞ÉÊï¥Ôºå‰ª•ËææÂà∞ÊúÄ‰Ω≥ÁöÑËÆ≠ÁªÉÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MedCritical 7BÊ®°ÂûãÂú®CMExamÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåË∂ÖË∂ä‰∫ÜTaiyiÂíåHuatuo-o1-7BÊ®°ÂûãÔºåÂàÜÂà´ÊèêÈ´ò‰∫Ü3.04%Âíå10.12%ÔºåÂú®7BÁ∫ßÂà´ÁöÑÂ∞èÂûãÊ®°Âûã‰∏≠ÂÆûÁé∞‰∫ÜÊñ∞ÁöÑSOTAÊÄßËÉΩ„ÄÇËøôË°®ÊòéËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞ÊèêÂçáÂ∞èËØ≠Ë®ÄÊ®°ÂûãÂú®ÂåªÁñóÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MedCriticalÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éËæÖÂä©‰∏¥Â∫äÂÜ≥Á≠ñ„ÄÅÂåªÂ≠¶Áü•ËØÜÈóÆÁ≠î„ÄÅÊô∫ËÉΩËØäÊñ≠Á≠âÈ¢ÜÂüü„ÄÇËØ•ÊñπÊ≥ïÈôç‰Ωé‰∫ÜÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰æùËµñÔºå‰ΩøÂæóÂ∞èËØ≠Ë®ÄÊ®°Âûã‰πüËÉΩÂú®ÂåªÁñóÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®ÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éËµÑÊ∫êÂèóÈôêÁöÑÂåªÁñóÊú∫ÊûÑÂíåÁßªÂä®ÂåªÁñóÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In the field of medicine, complex reasoning tasks such as clinical diagnosis, treatment planning, and medical knowledge integration pose significant challenges, where small language models often underperform compared to large language models like GPT-4 and Deepseek. Recent knowledge distillation-based methods aim to address these issues through teacher-guided error correction, but this LLM as judge approach remains challenging in terms of cost, time, and efficiency. To circumvent this issue, we propose a novel two-stage framework, MedCritical, which uses a small language model fine-tuned by a large teacher model to play against itself. In the first stage, we extract high-level and detailed long-chain thought templates from the teacher model to guide the student model to generate more complex reasoning thoughts. In the second stage, we introduce direct preference optimization (DPO) through model self-iteration collaboration to enhance the reasoning ability of the student model by playing against the correction trajectory of the fine-tuned model during training. This model self-learning DPO approach teaches the student model to use its own error-driven insights to consolidate its skills and knowledge to solve complex problems, and achieves comparable results to traditional knowledge distillation methods using teacher models at a lower cost. Notably, our MedCritical 7B model outperforms the Taiyi and Huatuo-o1-7B models by 3.04\% and 10.12\% respectively on the CMExam benchmark, achieving new SOTA performance among 7B-class small models.

