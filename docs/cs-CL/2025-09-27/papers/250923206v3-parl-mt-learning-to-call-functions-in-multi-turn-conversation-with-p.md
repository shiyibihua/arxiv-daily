---
layout: default
title: PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness
---

# PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23206" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23206v3</a>
  <a href="https://arxiv.org/pdf/2509.23206.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23206v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23206v3', 'PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huacan Chai, Zijie Cao, Maolin Ran, Yingxuan Yang, Jianghao Lin, Xin Peng, Hairui Wang, Renjie Ding, Ziyu Wan, Muning Wen, Weiwen Liu, Weinan Zhang, Fei Huang, Ying Wen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27 (æ›´æ–°: 2025-10-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PARL-MTï¼šé€šè¿‡è¿›åº¦æ„ŸçŸ¥å­¦ä¹ åœ¨å¤šè½®å¯¹è¯ä¸­è°ƒç”¨å‡½æ•°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè½®å¯¹è¯` `å‡½æ•°è°ƒç”¨` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¿›åº¦æ„ŸçŸ¥` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šè½®å¯¹è¯å‡½æ•°è°ƒç”¨ä¸­ç¼ºä¹ä»»åŠ¡çº§è§„åˆ’ï¼Œæˆ–ä½¿ç”¨ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ å­˜åœ¨å†—ä½™å’Œç¼ºä¹è¿›åº¦æ„ŸçŸ¥ã€‚
2. PARL-MTæ¡†æ¶é€šè¿‡è¿›åº¦æ„ŸçŸ¥ç”Ÿæˆ(PAG)å’Œè¿›åº¦æ„ŸçŸ¥å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ (PAG-RL)å°†è¿›åº¦æ„ŸçŸ¥èå…¥LLMè®­ç»ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPARL-MTåœ¨ä¸¤ä¸ªå…¬å…±åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†è¿›åº¦æ„ŸçŸ¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å•è½®å‡½æ•°è°ƒç”¨ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†æ—…è¡Œè§„åˆ’æˆ–å¤šé˜¶æ®µæ•°æ®åˆ†æç­‰å®é™…åº”ç”¨é€šå¸¸å‘ç”Ÿåœ¨å¤šè½®å¯¹è¯ä¸­ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼ŒLLMsä¸ä»…å¿…é¡»åœ¨æ¯ä¸ªæ­¥éª¤ä¸­å‘å‡ºå‡†ç¡®çš„å‡½æ•°è°ƒç”¨ï¼Œè¿˜å¿…é¡»ä¿æŒè¿›åº¦æ„ŸçŸ¥ï¼Œå³æ€»ç»“è¿‡å»äº¤äº’å¹¶è§„åˆ’æœªæ¥è¡ŒåŠ¨ä»¥ç¡®ä¿è¿è´¯çš„ã€é•¿æœŸçš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆå°†å¤šè½®è®­ç»ƒç®€åŒ–ä¸ºå­¤ç«‹çš„å•è½®æ ·æœ¬ï¼Œå¿½ç•¥äº†ä»»åŠ¡çº§åˆ«çš„è§„åˆ’ï¼Œè¦ä¹ˆé‡‡ç”¨ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ (RL)ï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨å†—ä½™é—®é¢˜ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹è¿›åº¦æ„ŸçŸ¥çš„æ˜¾å¼é›†æˆã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†PARL-MTï¼Œè¿™æ˜¯ä¸€ä¸ªå°†è¿›åº¦æ„ŸçŸ¥æ˜¾å¼åœ°èå…¥åˆ°LLMå¤šè½®å‡½æ•°è°ƒç”¨è®­ç»ƒä¸­çš„æ¡†æ¶ã€‚PARL-MTç»“åˆäº†(i)ä¸€ä¸ªè¿›åº¦æ„ŸçŸ¥ç”Ÿæˆ(PAG)ç®¡é“ï¼Œè¯¥ç®¡é“è‡ªåŠ¨æ„å»ºå°†å¯¹è¯æ‘˜è¦ä¸æœªæ¥ä»»åŠ¡è§„åˆ’ç›¸ç»“åˆçš„æ•°æ®é›†ï¼Œä»¥åŠ(ii)ä¸€ä¸ªè¿›åº¦æ„ŸçŸ¥å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ (PAG-RL)ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†è¿›åº¦æ„ŸçŸ¥é›†æˆåˆ°RLè®­ç»ƒä¸­ï¼Œä»¥å‡å°‘ä¸Šä¸‹æ–‡å†—ä½™å¹¶æé«˜å±€éƒ¨åŠ¨ä½œä¸å…¨å±€ä»»åŠ¡å®Œæˆä¹‹é—´çš„ä¸€è‡´æ€§ã€‚åœ¨ä¸¤ä¸ªå…¬å…±åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPARL-MTæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªå‡ºäº†è¿›åº¦æ„ŸçŸ¥åœ¨å®ç°é²æ£’å’Œé«˜æ•ˆçš„å¤šè½®å‡½æ•°è°ƒç”¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯åœºæ™¯ä¸‹è¿›è¡Œå‡½æ•°è°ƒç”¨æ—¶ï¼Œç¼ºä¹å¯¹å¯¹è¯å†å²çš„æ€»ç»“å’Œæœªæ¥ä»»åŠ¡è§„åˆ’èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå°†å¤šè½®å¯¹è¯ç®€åŒ–ä¸ºå•è½®å¯¹è¯ï¼Œå¿½ç•¥äº†ä»»åŠ¡çº§åˆ«çš„è§„åˆ’ï¼›è¦ä¹ˆé‡‡ç”¨ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ï¼Œä½†è¿™ç§æ–¹æ³•å®¹æ˜“äº§ç”Ÿå†—ä½™ï¼Œå¹¶ä¸”æ²¡æœ‰æ˜¾å¼åœ°æ•´åˆè¿›åº¦æ„ŸçŸ¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ˜¾å¼åœ°å°†è¿›åº¦æ„ŸçŸ¥èå…¥åˆ°LLMçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¯¹è¯å†å²ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€è§„åˆ’æœªæ¥çš„åŠ¨ä½œã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŒ…å«è¿›åº¦æ„ŸçŸ¥ç”Ÿæˆ(PAG)å’Œè¿›åº¦æ„ŸçŸ¥å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ (PAG-RL)çš„æ¡†æ¶ï¼Œä»è€Œæå‡LLMåœ¨å¤šè½®å¯¹è¯ä¸­å‡½æ•°è°ƒç”¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPARL-MTæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š(1) è¿›åº¦æ„ŸçŸ¥ç”Ÿæˆ(PAG)ï¼šè¯¥é˜¶æ®µè‡ªåŠ¨æ„å»ºæ•°æ®é›†ï¼Œå°†å¯¹è¯æ‘˜è¦ä¸æœªæ¥ä»»åŠ¡è§„åˆ’ç›¸ç»“åˆï¼Œä¸ºåç»­çš„å¼ºåŒ–å­¦ä¹ æä¾›è®­ç»ƒæ•°æ®ã€‚(2) è¿›åº¦æ„ŸçŸ¥å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ (PAG-RL)ï¼šè¯¥é˜¶æ®µå°†è¿›åº¦æ„ŸçŸ¥é›†æˆåˆ°å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ï¼Œä»¥å‡å°‘ä¸Šä¸‹æ–‡å†—ä½™ï¼Œå¹¶æé«˜å±€éƒ¨åŠ¨ä½œä¸å…¨å±€ä»»åŠ¡å®Œæˆä¹‹é—´çš„ä¸€è‡´æ€§ã€‚PAG-RLåˆ©ç”¨PAGç”Ÿæˆçš„æ•°æ®æ¥æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œä»è€Œä½¿LLMèƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ å¦‚ä½•åœ¨å¤šè½®å¯¹è¯ä¸­è¿›è¡Œå‡½æ•°è°ƒç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ˜¾å¼åœ°å°†è¿›åº¦æ„ŸçŸ¥èå…¥åˆ°LLMçš„å¤šè½®å¯¹è¯å‡½æ•°è°ƒç”¨è®­ç»ƒä¸­ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPARL-MTèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¯¹è¯å†å²ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€è§„åˆ’æœªæ¥çš„åŠ¨ä½œï¼Œä»è€Œæé«˜äº†å‡½æ•°è°ƒç”¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚PAGå’ŒPAG-RLçš„ç»“åˆæ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„å…³é”®ã€‚

**å…³é”®è®¾è®¡**ï¼šPAGç®¡é“çš„å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ï¼Œä½†å…¶æ ¸å¿ƒç›®æ ‡æ˜¯ç”ŸæˆåŒ…å«å¯¹è¯æ‘˜è¦å’Œæœªæ¥ä»»åŠ¡è§„åˆ’çš„æ•°æ®é›†ã€‚PAG-RLç®—æ³•çš„å…³é”®åœ¨äºå¦‚ä½•å°†è¿›åº¦æ„ŸçŸ¥ä¿¡æ¯æœ‰æ•ˆåœ°èå…¥åˆ°å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°å’ŒçŠ¶æ€è¡¨ç¤ºä¸­ã€‚å…·ä½“çš„å¥–åŠ±å‡½æ•°è®¾è®¡å’ŒçŠ¶æ€è¡¨ç¤ºæ–¹å¼æœªçŸ¥ï¼Œä½†éœ€è¦èƒ½å¤Ÿåæ˜ å¯¹è¯çš„è¿›åº¦å’Œæœªæ¥çš„ä»»åŠ¡ç›®æ ‡ã€‚æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©å’Œè¶…å‚æ•°çš„è°ƒæ•´ä¹Ÿæ˜¯å½±å“æ€§èƒ½çš„å…³é”®å› ç´ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PARL-MTåœ¨ä¸¤ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†è¿›åº¦æ„ŸçŸ¥åœ¨å¤šè½®å‡½æ•°è°ƒç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­æ˜ç¡®æŒ‡å‡ºPARL-MTå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ˜¾å¼åœ°å°†è¿›åº¦æ„ŸçŸ¥èå…¥åˆ°LLMçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æé«˜å…¶åœ¨å¤šè½®å¯¹è¯ä¸­å‡½æ•°è°ƒç”¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤šè½®å¯¹è¯å’Œå‡½æ•°è°ƒç”¨çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€æ—…è¡Œè§„åˆ’ã€æ•°æ®åˆ†æç­‰ã€‚é€šè¿‡æé«˜LLMåœ¨å¤šè½®å¯¹è¯ä¸­å‡½æ•°è°ƒç”¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶é™ä½äººå·¥å¹²é¢„çš„éœ€æ±‚ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„é¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨åŒ–æµç¨‹ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved impressive success in single-turn function calling, yet real-world applications such as travel planning or multi-stage data analysis typically unfold across multi-turn conversations. In these settings, LLMs must not only issue accurate function calls at each step but also maintain progress awareness, the ability to summarize past interactions and plan future actions to ensure coherent, long-horizon task execution. Existing approaches, however, either reduce multi-turn training to isolated single-turn samples, which neglects task-level planning, or employ end-to-end reinforcement learning (RL) that struggles with redundancy and lacks explicit integration of progress awareness. To overcome these limitations, we introduce PARL-MT, a framework that explicitly incorporates progress awareness into LLM training for multi-turn function calling. PARL-MT combines (i) a Progress Awareness Generation (PAG) pipeline, which automatically constructs datasets coupling conversation summaries with future task planning, and (ii) a Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which integrates progress awareness into RL training to reduce contextual redundancy and improve alignment between local actions and global task completion. Empirical results on two public benchmarks demonstrate that PARL-MT significantly outperforms existing methods, highlighting the effectiveness of progress awareness in enabling robust and efficient multi-turn function calling.

