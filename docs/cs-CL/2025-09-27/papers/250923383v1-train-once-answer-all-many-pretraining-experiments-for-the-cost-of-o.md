---
layout: default
title: Train Once, Answer All: Many Pretraining Experiments for the Cost of One
---

# Train Once, Answer All: Many Pretraining Experiments for the Cost of One

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23383" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23383v1</a>
  <a href="https://arxiv.org/pdf/2509.23383.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23383v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23383v1', 'Train Once, Answer All: Many Pretraining Experiments for the Cost of One')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sebastian Bordt, Martin Pawelczyk

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå•æ¬¡è®­ç»ƒå¤šé‡å®éªŒæ–¹æ³•ï¼Œé™ä½å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå®éªŒæˆæœ¬ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é¢„è®­ç»ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å®éªŒæ–¹æ³•` `è®¡ç®—æ•ˆç‡` `çŸ¥è¯†è·å–` `æ•°æ®æ±¡æŸ“` `æ¨¡å‹å®‰å…¨æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å¯¹æ¨¡å‹å­¦ä¹ æœºåˆ¶çš„æ·±å…¥ç ”ç©¶ã€‚
2. æå‡ºä¸€ç§å•æ¬¡è®­ç»ƒè¿è¡Œä¸­åŒæ—¶è¿›è¡Œå¤šä¸ªé¢„è®­ç»ƒå®éªŒçš„æ–¹æ³•ï¼Œé™ä½å®éªŒæˆæœ¬ã€‚
3. é€šè¿‡1.5Bå‚æ•°æ¨¡å‹ä¸Šçš„å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ï¼Œå¹¶å¤ç°äº†å¤šä¸ªå·²æœ‰ç ”ç©¶ç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸçš„ç ”ç©¶è¡¨æ˜ï¼Œå¯æ§çš„é¢„è®­ç»ƒå®éªŒæ˜¯ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­¦ä¹ ã€æ¨ç†å’Œè®°å¿†çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒçš„è®¡ç®—æˆæœ¬æ„æˆäº†ä¸€ä¸ªæ˜¾è‘—çš„é™åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºåœ¨å•æ¬¡è®­ç»ƒè¿è¡Œä¸­åŒæ—¶è¿›è¡Œå¤šä¸ªé¢„è®­ç»ƒå®éªŒã€‚æˆ‘ä»¬é€šè¿‡åœ¨210B tokensä¸Šè®­ç»ƒä¸€ä¸ª15äº¿å‚æ•°çš„æ¨¡å‹ï¼Œè¿›è¡Œäº†åä¸ªå®éªŒï¼Œè¯æ˜äº†è¿™ç§æ–¹æ³•çš„å¯è¡Œæ€§ã€‚è™½ç„¶æˆ‘ä»¬åªè®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œä½†æˆ‘ä»¬å¯ä»¥å¤ç°ä¹‹å‰å¤šé¡¹å…³äºæ•°æ®æ±¡æŸ“ã€æŠ•æ¯’å’Œè®°å¿†çš„ç ”ç©¶ç»“æœã€‚æˆ‘ä»¬è¿˜å¯¹çŸ¥è¯†è·å–ã€æ•°å­¦æ¨ç†å’Œæ°´å°æŠ€æœ¯è¿›è¡Œäº†æ–°çš„ç ”ç©¶ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åŠ¨æ€æ›´æ–°è®­ç»ƒæ•°æ®ï¼Œç›´åˆ°æ¨¡å‹è·å¾—ç‰¹å®šçš„çŸ¥è¯†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™åä¸ªå®éªŒå¯¹æ¨¡å‹çš„è®­ç»ƒåŠ¨æ€å’Œæ•´ä½“æ€§èƒ½çš„å½±å“æ˜¯æœ€å°çš„ã€‚ç„¶è€Œï¼Œä¸åŒå®éªŒä¹‹é—´çš„ç›¸äº’ä½œç”¨å¯èƒ½æˆä¸ºæˆ‘ä»¬æ–¹æ³•ä¸­æ½œåœ¨çš„æ··æ·†å› ç´ ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡æŒç»­çš„é¢„è®­ç»ƒå®éªŒæ¥æµ‹è¯•ç›¸äº’ä½œç”¨ï¼Œå‘ç°å®ƒä»¬åœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œåœ¨å•æ¬¡è®­ç»ƒè¿è¡Œä¸­è¿›è¡Œå¤šä¸ªé¢„è®­ç»ƒå®éªŒï¼Œå¯ä»¥åœ¨è®¡ç®—é¢„ç®—å†…å¯¹å¤§å‹æ¨¡å‹è¿›è¡Œä¸¥æ ¼çš„ç§‘å­¦å®éªŒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç ”ç©¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å­¦ä¹ ã€æ¨ç†å’Œè®°å¿†æœºåˆ¶çš„æ¢ç´¢å—é™äºé¢„è®­ç»ƒçš„é«˜æ˜‚è®¡ç®—æˆæœ¬ã€‚æ¯æ¬¡è¿›è¡Œæ–°çš„å®éªŒï¼Œéƒ½éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè€—æ—¶è€—åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•é™ä½é¢„è®­ç»ƒå®éªŒçš„æˆæœ¬ï¼Œæˆä¸ºä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨å•æ¬¡æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶è¿›è¡Œå¤šä¸ªä¸åŒçš„é¢„è®­ç»ƒå®éªŒã€‚é€šè¿‡å·§å¦™åœ°è®¾è®¡å®éªŒæ–¹æ¡ˆï¼Œä½¿å¾—å¤šä¸ªå®éªŒèƒ½å¤Ÿå…±äº«åŒä¸€ä¸ªæ¨¡å‹ï¼Œä»è€Œå¤§å¹…é™ä½è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚è¿™æ ·ï¼Œç ”ç©¶äººå‘˜å¯ä»¥ç”¨æ›´ä½çš„æˆæœ¬ï¼Œæ¢ç´¢æ›´å¤šå…³äºæ¨¡å‹å­¦ä¹ æœºåˆ¶çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºè®¾è®¡å¤šä¸ªå¹¶è¡Œçš„é¢„è®­ç»ƒå®éªŒï¼Œè¿™äº›å®éªŒå…±äº«åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªå®éªŒéƒ½æŒ‰ç…§å…¶ç‰¹å®šçš„æ•°æ®å’Œç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä½†æ‰€æœ‰å®éªŒéƒ½ä½œç”¨äºåŒä¸€ä¸ªæ¨¡å‹å‚æ•°ã€‚ä¸ºäº†è¯„ä¼°å®éªŒä¹‹é—´çš„ç›¸äº’å½±å“ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†æŒç»­é¢„è®­ç»ƒå®éªŒï¼Œä»¥æ£€éªŒä¸åŒå®éªŒä¹‹é—´çš„å¹²æ‰°ç¨‹åº¦ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1ï¼‰å®šä¹‰å¤šä¸ªé¢„è®­ç»ƒå®éªŒï¼›2ï¼‰ä½¿ç”¨å…±äº«æ¨¡å‹è¿›è¡Œå¹¶è¡Œè®­ç»ƒï¼›3ï¼‰è¯„ä¼°å®éªŒç»“æœå¹¶åˆ†æç›¸äº’å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†â€œå•æ¬¡è®­ç»ƒï¼Œå›ç­”æ‰€æœ‰é—®é¢˜â€çš„é¢„è®­ç»ƒèŒƒå¼ã€‚ä¸ä¼ ç»Ÿçš„æ¯æ¬¡å®éªŒéƒ½é‡æ–°è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡å…±äº«æ¨¡å‹çš„æ–¹å¼ï¼Œå¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹è¿›è¡Œæ›´å¤šçš„å®éªŒã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰ç²¾å¿ƒè®¾è®¡çš„å®éªŒæ–¹æ¡ˆï¼Œç¡®ä¿å„ä¸ªå®éªŒä¹‹é—´ä¸ä¼šäº§ç”Ÿè¿‡å¤§çš„å¹²æ‰°ï¼›2ï¼‰ä½¿ç”¨æŒç»­é¢„è®­ç»ƒå®éªŒæ¥è¯„ä¼°å®éªŒä¹‹é—´çš„ç›¸äº’å½±å“ï¼›3ï¼‰åŠ¨æ€æ›´æ–°è®­ç»ƒæ•°æ®ï¼Œä»¥å®ç°å¯¹æ¨¡å‹çŸ¥è¯†è·å–è¿‡ç¨‹çš„ç²¾ç¡®æ§åˆ¶ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ä¸å„ä¸ªå®éªŒçš„å…·ä½“ç›®æ ‡ç›¸å…³ï¼Œè®ºæ–‡ä¸­ç»™å‡ºäº†è¯¦ç»†çš„æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡åœ¨1.5Bå‚æ•°æ¨¡å‹ä¸Šè¿›è¡Œ10ä¸ªå¹¶è¡Œé¢„è®­ç»ƒå®éªŒï¼ŒæˆåŠŸå¤ç°äº†å¤šä¸ªå·²æœ‰ç ”ç©¶çš„ç»“æœï¼ŒåŒ…æ‹¬æ•°æ®æ±¡æŸ“ã€æŠ•æ¯’å’Œè®°å¿†ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œå„ç§é¢„è®­ç»ƒå®éªŒï¼Œå¹¶ä¸”ä¸åŒå®éªŒä¹‹é—´çš„ç›¸äº’å½±å“å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¿›è¡Œäº†çŸ¥è¯†è·å–ã€æ•°å­¦æ¨ç†å’Œæ°´å°æŠ€æœ¯ç­‰æ–¹é¢çš„åˆ›æ–°æ€§ç ”ç©¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç§‘å­¦ç ”ç©¶ï¼Œä¾‹å¦‚æ¨¡å‹çš„å¯è§£é‡Šæ€§ç ”ç©¶ã€çŸ¥è¯†è·å–æœºåˆ¶ç ”ç©¶ã€ä»¥åŠæ¨¡å‹å®‰å…¨æ€§ç ”ç©¶ç­‰ã€‚é€šè¿‡é™ä½é¢„è®­ç»ƒå®éªŒçš„æˆæœ¬ï¼Œå¯ä»¥åŠ é€Ÿç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œå¹¶ä¿ƒè¿›æ›´å®‰å…¨ã€æ›´å¯é çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿé€‚ç”¨äºå…¶ä»–éœ€è¦å¤§é‡è®¡ç®—èµ„æºçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå’Œå®éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent work has demonstrated that controlled pretraining experiments are a powerful tool for understanding learning, reasoning, and memorization in large language models (LLMs). However, the computational cost of pretraining presents a significant constraint. To overcome this constraint, we propose to conduct multiple pretraining experiments simultaneously during a single training run. We demonstrate the feasibility of this approach by conducting ten experiments during the training of a 1.5B parameter model on 210B tokens. Although we only train a single model, we can replicate the results from multiple previous works on data contamination, poisoning, and memorization. We also conduct novel investigations into knowledge acquisition, mathematical reasoning, and watermarking. For example, we dynamically update the training data until the model acquires a particular piece of knowledge. Remarkably, the influence of the ten experiments on the model's training dynamics and overall performance is minimal. However, interactions between different experiments may act as a potential confounder in our approach. We propose to test for interactions with continual pretraining experiments, finding them to be negligible in our setup. Overall, our findings suggest that performing multiple pretraining experiments in a single training run can enable rigorous scientific experimentation with large models on a compute budget.

