---
layout: default
title: Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs
---

# Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02351" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02351v1</a>
  <a href="https://arxiv.org/pdf/2510.02351.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02351v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02351v1', 'Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dzmitry Pihulski, Jan KocoÅ„

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: To appear in the Proceedings of the IEEE International Conference on Data Mining Workshops (ICDMW)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨æ¨ç†LLMä¸ªæ€§åŒ–æ”¿æ²»æ¨æ–‡å†’çŠ¯æ€§æ£€æµ‹ï¼Œè€ƒè™‘è¯­è¨€ã€æ–‡åŒ–å’Œæ„è¯†å½¢æ€å› ç´ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å†’çŠ¯æ€§æ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ”¿æ²»æ¨æ–‡` `ä¸ªæ€§åŒ–` `æ¨ç†èƒ½åŠ›` `æ–‡åŒ–å·®å¼‚` `æ„è¯†å½¢æ€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ”¿æ²»æ¨æ–‡å†’çŠ¯æ€§æ£€æµ‹ä¸­ï¼Œéš¾ä»¥å…¼é¡¾è¯­è¨€ã€æ–‡åŒ–å’Œæ„è¯†å½¢æ€çš„å·®å¼‚æ€§ï¼Œå¯¼è‡´åˆ¤æ–­ç»“æœç¼ºä¹ä¸ªæ€§åŒ–å’Œå¯è§£é‡Šæ€§ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨å…·å¤‡æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ¨¡æ‹Ÿä¸åŒæ”¿æ²»å’Œæ–‡åŒ–è§†è§’ï¼Œå®ç°å¯¹æ”¿æ²»æ¨æ–‡å†’çŠ¯æ€§çš„ä¸ªæ€§åŒ–è¯„ä¼°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå…·æœ‰æ¨ç†èƒ½åŠ›çš„è¾ƒå¤§æ¨¡å‹åœ¨æ•æ‰æ„è¯†å½¢æ€å’Œæ–‡åŒ–å·®å¼‚æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œæ˜¾è‘—æå‡äº†å†’çŠ¯æ€§åˆ¤æ–­çš„ä¸ªæ€§åŒ–å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¢«è¦æ±‚é‡‡çº³ç‰¹å®šçš„æ”¿æ²»å’Œæ–‡åŒ–è§†è§’æ—¶ï¼Œå¦‚ä½•è¯„ä¼°æ”¿æ²»è¨€è®ºä¸­çš„å†’çŠ¯æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨MD-Agreementæ•°æ®é›†çš„å¤šè¯­ç§å­é›†ï¼Œè¯¥æ•°æ®é›†ä»¥2020å¹´ç¾å›½å¤§é€‰çš„æ¨æ–‡ä¸ºä¸­å¿ƒï¼Œè¯„ä¼°äº†å‡ ç§æœ€æ–°çš„LLMâ€”â€”åŒ…æ‹¬DeepSeek-R1ã€o4-miniã€GPT-4.1-miniã€Qwen3ã€Gemmaå’ŒMistralâ€”â€”çš„ä»»åŠ¡æ˜¯ä»ä¸åŒæ”¿æ²»è§’è‰²ï¼ˆæå³ã€ä¿å®ˆæ´¾ã€ä¸­é—´æ´¾ã€è¿›æ­¥æ´¾ï¼‰çš„è§†è§’åˆ¤æ–­æ¨æ–‡æ˜¯å¦å…·æœ‰å†’çŠ¯æ€§ï¼Œæ¶µç›–è‹±è¯­ã€æ³¢å…°è¯­å’Œä¿„è¯­ç¯å¢ƒã€‚ç»“æœè¡¨æ˜ï¼Œå…·æœ‰æ˜¾å¼æ¨ç†èƒ½åŠ›çš„è¾ƒå¤§æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒDeepSeek-R1ã€o4-miniï¼‰å¯¹æ„è¯†å½¢æ€å’Œæ–‡åŒ–å·®å¼‚æ›´åŠ ä¸€è‡´å’Œæ•æ„Ÿï¼Œè€Œè¾ƒå°çš„æ¨¡å‹é€šå¸¸æ— æ³•æ•æ‰åˆ°ç»†å¾®çš„å·®åˆ«ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜äº†å†’çŠ¯æ€§åˆ¤æ–­çš„ä¸ªæ€§åŒ–å’Œå¯è§£é‡Šæ€§ï¼Œè¡¨æ˜è¿™ç§æœºåˆ¶æ˜¯ä½¿LLMé€‚åº”è·¨è¯­è¨€å’Œæ„è¯†å½¢æ€çš„ç»†è‡´çš„ç¤¾ä¼šæ”¿æ²»æ–‡æœ¬åˆ†ç±»çš„å…³é”®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ”¿æ²»æ¨æ–‡å†’çŠ¯æ€§æ£€æµ‹æ–¹æ³•é€šå¸¸å¿½ç•¥äº†è¯­è¨€ã€æ–‡åŒ–å’Œæ„è¯†å½¢æ€èƒŒæ™¯å¯¹å†’çŠ¯æ€§åˆ¤æ–­çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥å‡†ç¡®æ•æ‰ä¸åŒäººç¾¤å¯¹åŒä¸€æ¨æ–‡çš„ä¸åŒæ„Ÿå—ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ä¸ªæ€§åŒ–å’Œå¯è§£é‡Šæ€§ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡promptingçš„æ–¹å¼ï¼Œè®©LLMæ¨¡æ‹Ÿä¸åŒæ”¿æ²»å’Œæ–‡åŒ–èƒŒæ™¯çš„è§’è‰²ï¼Œä»è€Œå¯¹æ”¿æ²»æ¨æ–‡çš„å†’çŠ¯æ€§è¿›è¡Œä¸ªæ€§åŒ–è¯„ä¼°ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£ä¸åŒè§†è§’ä¸‹çš„å†’çŠ¯æ€§ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„åˆ¤æ–­ä¾æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) æ•°æ®é›†å‡†å¤‡ï¼šä½¿ç”¨MD-Agreementæ•°æ®é›†çš„å¤šè¯­ç§å­é›†ï¼ŒåŒ…å«æ¥è‡ª2020å¹´ç¾å›½å¤§é€‰çš„æ¨æ–‡ï¼Œæ¶µç›–è‹±è¯­ã€æ³¢å…°è¯­å’Œä¿„è¯­ã€‚2) æ¨¡å‹é€‰æ‹©ï¼šé€‰æ‹©å¤šä¸ªLLMè¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬DeepSeek-R1ã€o4-miniã€GPT-4.1-miniã€Qwen3ã€Gemmaå’ŒMistralã€‚3) Promptè®¾è®¡ï¼šè®¾è®¡promptï¼ŒæŒ‡ç¤ºLLMæ‰®æ¼”ä¸åŒçš„æ”¿æ²»è§’è‰²ï¼ˆæå³ã€ä¿å®ˆæ´¾ã€ä¸­é—´æ´¾ã€è¿›æ­¥æ´¾ï¼‰ï¼Œå¹¶è¦æ±‚å…¶åˆ¤æ–­æ¨æ–‡æ˜¯å¦å…·æœ‰å†’çŠ¯æ€§ã€‚4) è¯„ä¼°æŒ‡æ ‡ï¼šè¯„ä¼°LLMåœ¨ä¸åŒæ”¿æ²»è§’è‰²ä¸‹çš„åˆ¤æ–­ä¸€è‡´æ€§å’Œå¯¹æ–‡åŒ–å·®å¼‚çš„æ•æ„Ÿæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›è¿›è¡Œä¸ªæ€§åŒ–çš„å†’çŠ¯æ€§æ£€æµ‹ã€‚é€šè¿‡promptingçš„æ–¹å¼ï¼Œè®©LLMæ¨¡æ‹Ÿä¸åŒæ”¿æ²»å’Œæ–‡åŒ–èƒŒæ™¯çš„è§’è‰²ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£ä¸åŒè§†è§’ä¸‹çš„å†’çŠ¯æ€§ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†å†’çŠ¯æ€§åˆ¤æ–­çš„å‡†ç¡®æ€§ï¼Œè¿˜æä¾›äº†å¯è§£é‡Šçš„åˆ¤æ–­ä¾æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) Promptçš„è®¾è®¡ï¼Œéœ€è¦æ¸…æ™°åœ°å®šä¹‰æ”¿æ²»è§’è‰²çš„ç«‹åœºå’Œä»·å€¼è§‚ã€‚2) æ¨¡å‹é€‰æ‹©ï¼Œéœ€è¦é€‰æ‹©å…·æœ‰è¶³å¤Ÿæ¨ç†èƒ½åŠ›çš„LLMã€‚3) è¯„ä¼°æŒ‡æ ‡ï¼Œéœ€è¦èƒ½å¤Ÿè¡¡é‡æ¨¡å‹åœ¨ä¸åŒæ”¿æ²»è§’è‰²ä¸‹çš„åˆ¤æ–­ä¸€è‡´æ€§å’Œå¯¹æ–‡åŒ–å·®å¼‚çš„æ•æ„Ÿæ€§ã€‚è®ºæ–‡æ²¡æœ‰æ˜ç¡®æåŠæŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ç­‰ç»†èŠ‚ï¼Œå¯èƒ½ä½¿ç”¨äº†LLMè‡ªå¸¦çš„æŸå¤±å‡½æ•°å’Œé¢„è®­ç»ƒå¥½çš„ç½‘ç»œç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå…·æœ‰æ˜¾å¼æ¨ç†èƒ½åŠ›çš„è¾ƒå¤§æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒDeepSeek-R1ã€o4-miniï¼‰å¯¹æ„è¯†å½¢æ€å’Œæ–‡åŒ–å·®å¼‚æ›´åŠ ä¸€è‡´å’Œæ•æ„Ÿï¼Œè€Œè¾ƒå°çš„æ¨¡å‹é€šå¸¸æ— æ³•æ•æ‰åˆ°ç»†å¾®çš„å·®åˆ«ã€‚æ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜äº†å†’çŠ¯æ€§åˆ¤æ–­çš„ä¸ªæ€§åŒ–å’Œå¯è§£é‡Šæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ¨ç†æœºåˆ¶æ˜¯ä½¿LLMé€‚åº”è·¨è¯­è¨€å’Œæ„è¯†å½¢æ€çš„ç»†è‡´çš„ç¤¾ä¼šæ”¿æ²»æ–‡æœ¬åˆ†ç±»çš„å…³é”®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç¤¾äº¤åª’ä½“å¹³å°çš„å†…å®¹å®¡æ ¸ã€èˆ†æƒ…åˆ†æå’Œä¸ªæ€§åŒ–æ¨èç­‰é¢†åŸŸã€‚é€šè¿‡ç†è§£ä¸åŒæ”¿æ²»å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹ç”¨æˆ·å¯¹å†…å®¹çš„æ„Ÿå—ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°è¿‡æ»¤æœ‰å®³ä¿¡æ¯ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¿ƒè¿›å¥åº·çš„åœ¨çº¿è®¨è®ºç¯å¢ƒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ–‡æœ¬å’Œæ›´å¹¿æ³›çš„æ–‡åŒ–èƒŒæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We explore how large language models (LLMs) assess offensiveness in political discourse when prompted to adopt specific political and cultural perspectives. Using a multilingual subset of the MD-Agreement dataset centered on tweets from the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1, o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets as offensive or non-offensive from the viewpoints of varied political personas (far-right, conservative, centrist, progressive) across English, Polish, and Russian contexts. Our results show that larger models with explicit reasoning abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to ideological and cultural variation, while smaller models often fail to capture subtle distinctions. We find that reasoning capabilities significantly improve both the personalization and interpretability of offensiveness judgments, suggesting that such mechanisms are key to adapting LLMs for nuanced sociopolitical text classification across languages and ideologies.

