---
layout: default
title: Learning to Reason in Structured In-context Environments with Reinforcement Learning
---

# Learning to Reason in Structured In-context Environments with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23330" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23330v1</a>
  <a href="https://arxiv.org/pdf/2509.23330.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23330v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23330v1', 'Learning to Reason in Structured In-context Environments with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peng Yu, Zeyuan Zhao, Shao Zhang, Luoyi Fu, Xinbing Wang, Ying Wen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSIEæ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡LLMåœ¨ç»“æ„åŒ–ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç»“æ„åŒ–æ•°æ®` `æ¨ç†ç¯å¢ƒ` `çŸ¥è¯†å›¾è°±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ¨ç†ç¯å¢ƒä¾èµ–ä¸“å®¶æ ‡æ³¨ï¼Œéš¾ä»¥æ‰©å±•ï¼Œä¸”æ¸¸æˆç¯å¢ƒå­¦ä¹ çš„æŠ€èƒ½æ³›åŒ–æ€§å·®ã€‚
2. æå‡ºç»“æ„åŒ–ä¸Šä¸‹æ–‡ç¯å¢ƒï¼ˆSIEï¼‰æ¡†æ¶ï¼Œä»å¤§è§„æ¨¡ç»“æ„åŒ–æ•°æ®è‡ªåŠ¨æ„å»ºæ¨ç†ç¯å¢ƒï¼Œæ”¯æŒå¯æ³›åŒ–æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSIEæ¡†æ¶æ˜¾è‘—æå‡äº†LLMåœ¨ç»“æ„åŒ–æ¨ç†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶èƒ½æ³›åŒ–åˆ°æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œç¯å¢ƒæ¢ç´¢åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç”±äºç¯å¢ƒçš„å†…åœ¨å±æ€§å†³å®šäº†LLMså¯ä»¥å­¦ä¹ çš„èƒ½åŠ›ï¼Œå› æ­¤ç¯å¢ƒåœ¨RLå¾®è°ƒè¿‡ç¨‹ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚ç†æƒ³çš„LLMæ¨ç†ç¯å¢ƒåº”å…·å¤‡ä¸‰ä¸ªæ ¸å¿ƒç‰¹å¾ï¼šå¯æ‰©å±•æ€§ã€å¯æ³›åŒ–çš„æ¨ç†èƒ½åŠ›å’Œå¯éªŒè¯æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°å­¦å’Œç¼–ç ç¯å¢ƒç”±äºä¸¥é‡ä¾èµ–ä¸“å®¶æ ‡æ³¨è€Œéš¾ä»¥æ‰©å±•ï¼Œè€ŒåŸºäºæ¸¸æˆçš„å­¦ä¹ æŠ€èƒ½è¿‡äºä¸“ä¸šåŒ–è€Œéš¾ä»¥æ³›åŒ–ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»“æ„åŒ–ä¸Šä¸‹æ–‡ç¯å¢ƒï¼ˆSIEï¼‰æ¡†æ¶ã€‚SIEé€šè¿‡ä»å¤§è§„æ¨¡ç»“æ„åŒ–æ•°æ®ä¸­è‡ªåŠ¨æ„å»ºæ¨ç†ç¯å¢ƒæ¥å®ç°å¯æ‰©å±•æ€§ï¼Œå…¶ä¸­ä¸°å¯Œçš„ç»„åˆæ¨¡å¼è‡ªç„¶æ”¯æŒå¯æ³›åŒ–çš„æ¨ç†ã€‚æ­¤å¤–ï¼Œç»“æ„åŒ–æ•°æ®ä¸­æ˜¾å¼çš„æ¨¡å¼å’Œæ¨ç†é“¾ä¸ºåŸºäºè§„åˆ™çš„å¯éªŒè¯æ€§æä¾›äº†åŸºç¡€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSIEæ¡†æ¶ä¸ä»…åœ¨é¢†åŸŸå†…çš„ç»“æ„åŒ–æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œè€Œä¸”ä½¿å­¦ä¹ åˆ°çš„ç»„åˆæ¨ç†æŠ€èƒ½èƒ½å¤Ÿæœ‰æ•ˆåœ°æ³›åŒ–åˆ°é¢†åŸŸå¤–çš„æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†åœ¨ä¿¡æ¯æœ‰é™çš„å±€éƒ¨SIEä¸­è¿›è¡Œå­¦ä¹ ï¼Œå‘ç°LLMså¯ä»¥é€šè¿‡æ¢ç´¢ç¯å¢ƒæ¥æ¨æ–­ç¼ºå¤±çš„ä¿¡æ¯ï¼Œä»è€Œå®ç°ç¨³å¥çš„æ¨ç†æ”¹è¿›å’Œæ³›åŒ–æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ç¯å¢ƒå­˜åœ¨å¯æ‰©å±•æ€§ã€é€šç”¨æ¨ç†èƒ½åŠ›å’Œå¯éªŒè¯æ€§ä¸‰ä¸ªæ–¹é¢çš„ä¸è¶³ã€‚æ•°å­¦å’Œç¼–ç ç¯å¢ƒä¾èµ–äºä¸“å®¶æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æ‰©å±•ã€‚æ¸¸æˆç¯å¢ƒè™½ç„¶å¯ä»¥æä¾›ä¸€å®šçš„æ¨ç†è®­ç»ƒï¼Œä½†å­¦ä¹ åˆ°çš„æŠ€èƒ½è¿‡äºä¸“ä¸šåŒ–ï¼Œéš¾ä»¥æ³›åŒ–åˆ°å…¶ä»–é¢†åŸŸã€‚å› æ­¤ï¼Œå¦‚ä½•æ„å»ºä¸€ä¸ªå¯æ‰©å±•ã€å¯æ³›åŒ–ä¸”å¯éªŒè¯çš„LLMæ¨ç†ç¯å¢ƒæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è§„æ¨¡çš„ç»“æ„åŒ–æ•°æ®è‡ªåŠ¨æ„å»ºæ¨ç†ç¯å¢ƒã€‚ç»“æ„åŒ–æ•°æ®å…·æœ‰ä¸°å¯Œçš„ç»„åˆæ¨¡å¼ï¼Œå¯ä»¥æ”¯æŒé€šç”¨æ¨ç†ã€‚åŒæ—¶ï¼Œç»“æ„åŒ–æ•°æ®ä¸­æ˜¾å¼çš„æ¨¡å¼å’Œæ¨ç†é“¾ä¸ºè§„åˆ™éªŒè¯æä¾›äº†åŸºç¡€ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œè®©LLMåœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­è¿›è¡Œæ¢ç´¢å’Œå­¦ä¹ ï¼Œä»è€Œæå‡å…¶æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSIEæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **æ•°æ®æ„å»º**ï¼šä»å¤§è§„æ¨¡ç»“æ„åŒ–æ•°æ®æºï¼ˆä¾‹å¦‚çŸ¥è¯†å›¾è°±ã€æ•°æ®åº“ç­‰ï¼‰ä¸­æå–æ•°æ®ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºé€‚åˆLLMå¤„ç†çš„æ ¼å¼ã€‚2) **ç¯å¢ƒæ„å»º**ï¼šåŸºäºæå–çš„æ•°æ®æ„å»ºæ¨ç†ç¯å¢ƒï¼ŒåŒ…æ‹¬çŠ¶æ€ç©ºé—´ã€åŠ¨ä½œç©ºé—´å’Œå¥–åŠ±å‡½æ•°ã€‚çŠ¶æ€ç©ºé—´è¡¨ç¤ºå½“å‰æ¨ç†çš„çŠ¶æ€ï¼ŒåŠ¨ä½œç©ºé—´è¡¨ç¤ºLLMå¯ä»¥é‡‡å–çš„åŠ¨ä½œï¼Œå¥–åŠ±å‡½æ•°ç”¨äºè¯„ä¼°LLMçš„æ¨ç†ç»“æœã€‚3) **å¼ºåŒ–å­¦ä¹ è®­ç»ƒ**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚PPOã€DQNç­‰ï¼‰è®­ç»ƒLLMï¼Œä½¿å…¶å­¦ä¼šåœ¨ç¯å¢ƒä¸­è¿›è¡Œæ¨ç†ã€‚LLMé€šè¿‡æ¢ç´¢ç¯å¢ƒï¼Œé€‰æ‹©åŠ¨ä½œï¼Œå¹¶æ ¹æ®å¥–åŠ±å‡½æ•°è°ƒæ•´ç­–ç•¥ï¼Œæœ€ç»ˆè¾¾åˆ°æœ€ä¼˜çš„æ¨ç†æ€§èƒ½ã€‚4) **æ³›åŒ–è¯„ä¼°**ï¼šåœ¨é¢†åŸŸå¤–çš„æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¯„ä¼°LLMçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ç»“æ„åŒ–ä¸Šä¸‹æ–‡ç¯å¢ƒï¼ˆSIEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨ä»å¤§è§„æ¨¡ç»“æ„åŒ–æ•°æ®ä¸­æ„å»ºæ¨ç†ç¯å¢ƒï¼Œä»è€Œè§£å†³äº†ç°æœ‰æ¨ç†ç¯å¢ƒå¯æ‰©å±•æ€§å·®çš„é—®é¢˜ã€‚æ­¤å¤–ï¼ŒSIEæ¡†æ¶åˆ©ç”¨ç»“æ„åŒ–æ•°æ®çš„ç»„åˆæ¨¡å¼å’Œæ˜¾å¼æ¨ç†é“¾ï¼Œæé«˜äº†LLMçš„é€šç”¨æ¨ç†èƒ½åŠ›å’Œå¯éªŒè¯æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç¯å¢ƒæ„å»ºæ–¹é¢ï¼ŒçŠ¶æ€ç©ºé—´çš„è®¾è®¡éœ€è¦èƒ½å¤Ÿå……åˆ†è¡¨ç¤ºå½“å‰æ¨ç†çš„çŠ¶æ€ï¼ŒåŠ¨ä½œç©ºé—´çš„è®¾è®¡éœ€è¦èƒ½å¤Ÿè¦†ç›–æ‰€æœ‰å¯èƒ½çš„æ¨ç†æ­¥éª¤ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦èƒ½å¤Ÿå‡†ç¡®è¯„ä¼°LLMçš„æ¨ç†ç»“æœï¼Œå¹¶å¼•å¯¼LLMæœç€æ­£ç¡®çš„æ–¹å‘è¿›è¡Œå­¦ä¹ ã€‚åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹é¢ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œè¶…å‚æ•°ï¼Œä»¥ä¿è¯LLMèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ åˆ°æ¨ç†ç­–ç•¥ã€‚è®ºæ–‡ä¸­è¿˜æ¢ç´¢äº†åœ¨ä¿¡æ¯æœ‰é™çš„å±€éƒ¨SIEä¸­è¿›è¡Œå­¦ä¹ ï¼Œé€šè¿‡é¼“åŠ±LLMæ¢ç´¢ç¯å¢ƒæ¥æ¨æ–­ç¼ºå¤±çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜å…¶é²æ£’æ€§å’Œæ³›åŒ–æ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰æ›´è¯¦ç»†çš„æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSIEæ¡†æ¶åœ¨é¢†åŸŸå†…çš„ç»“æ„åŒ–æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹¶ä¸”å­¦ä¹ åˆ°çš„ç»„åˆæ¨ç†æŠ€èƒ½èƒ½å¤Ÿæœ‰æ•ˆåœ°æ³›åŒ–åˆ°é¢†åŸŸå¤–çš„æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ï¼Œä½†æ€»ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢éƒ½è¡¨ç°å‡ºäº†ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½é—®ç­”ã€çŸ¥è¯†å›¾è°±æ¨ç†ã€æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡LLMåœ¨ç»“æ„åŒ–æ•°æ®ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ„å»ºæ›´æ™ºèƒ½ã€æ›´å¯é çš„AIç³»ç»Ÿã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„ç»“æ„åŒ–ç¯å¢ƒï¼Œä¾‹å¦‚ä»£ç ç”Ÿæˆã€ç¨‹åºè°ƒè¯•ç­‰ï¼Œä»è€Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved significant advancements in reasoning capabilities through reinforcement learning (RL) via environmental exploration. As the intrinsic properties of the environment determine the abilities that LLMs can learn, the environment plays a important role in the RL finetuning process. An ideal LLM reasoning environment should possess three core characteristics: scalability, generalizable reasoning, and verifiability. However, existing mathematical and coding environments are difficult to scale due to heavy reliance on expert annotation, while the skills learned in game-based environments are too specialized to generalize. To bridge this gap, we introduce the \textbf{S}tructured \textbf{I}n-context \textbf{E}nvironment (SIE) framework. SIE achieves scalability by automatically constructing reasoning environments from large-scale structured data, where the rich compositional patterns naturally support generalizable reasoning. Moreover, the explicit schemas and reasoning chains in structured data provide a foundation for rule-based verifiability. Experimental results show that SIE framework not only achieves substantial improvements in in-domain structured reasoning, but also enables the learned compositional reasoning skills to generalize effectively to out-of-domain mathematical and logical reasoning tasks. We further explored learning in information-limited partial SIEs and found that LLMs can infer the missing information through exploring the environment, leading to robust reasoning improvements and generalization performance.

