---
layout: default
title: No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization
---

# No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23387" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23387v1</a>
  <a href="https://arxiv.org/pdf/2509.23387.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23387v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23387v1', 'No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Wenhang Shi, Yiren Chen, Shuqing Bian, Xinyi Zhang, Kai Tang, Pengfei Hu, Zhe Zhao, Wei Lu, Xiaoyong Du

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

**Â§áÊ≥®**: 10 pages for main content

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Eric8932/GRACE)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫GRACEÊ°ÜÊû∂ÔºåÈÄöËøáÈó®Êéß‰ºòÂåñÂíåËá™ÈÄÇÂ∫îÂéãÁº©ÊèêÂçáPrompt‰ºòÂåñÊïàÁéá‰∏éÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Prompt‰ºòÂåñ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Èó®ÊéßÊú∫Âà∂` `Ëá™ÈÄÇÂ∫îÂéãÁº©` `Â±ÄÈÉ®ÊúÄ‰ºò` `È´òÊïà‰ºòÂåñ` `ÂèçÈ¶àË∞ÉËäÇ` `Êõ¥Êñ∞ÊãíÁªù`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâPrompt‰ºòÂåñÊñπÊ≥ïÈöæ‰ª•Á®≥ÂÆöÁîüÊàêÊîπËøõPromptÔºåÊïàÁéá‰Ωé‰∏îÊòìÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºò„ÄÇ
2. GRACEÊ°ÜÊû∂ÈÄöËøáÈó®Êéß‰ºòÂåñÁ®≥ÂÆöPromptÊõ¥Êñ∞ÔºåËá™ÈÄÇÂ∫îÂéãÁº©Ë∑≥Âá∫Â±ÄÈÉ®ÊúÄ‰ºòÔºåÊèêÂçá‰ºòÂåñÊïàÁéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåGRACEÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºå‰∏î‰ªÖÈúÄ25%ÁöÑËÆ°ÁÆóËµÑÊ∫ê„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

PromptÂ∑•Á®ãÂØπ‰∫éÂÖÖÂàÜÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËá≥ÂÖ≥ÈáçË¶Å„ÄÇËá™Âä®prompt‰ºòÂåñ‰∏∫‰ª£‰ª∑È´òÊòÇÁöÑÊâãÂä®ËÆæËÆ°Êèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÊõø‰ª£ÊñπÊ°àÔºå‰ΩÜÁîüÊàêÊúâÊïàÁöÑprompt‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Èöæ‰ª•Á®≥ÂÆöÂú∞ÁîüÊàêÊîπËøõÁöÑpromptÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ãÔºåÂπ∂‰∏îÂøΩÁï•‰∫Üprompt‰ºòÂåñÂÆπÊòìÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºòÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜGRACEÔºå‰∏Ä‰∏™ÈõÜÊàê‰∫Ü‰∏§ÁßçÂçèÂêåÁ≠ñÁï•ÁöÑÊ°ÜÊû∂ÔºöÈó®Êéß‰ºòÂåñÂíåËá™ÈÄÇÂ∫îÂéãÁº©Ôºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑprompt‰ºòÂåñ„ÄÇÈó®Êéß‰ºòÂåñÁ≠ñÁï•ÂºïÂÖ•‰∫ÜÂèçÈ¶àË∞ÉËäÇÈó®ÂíåÊõ¥Êñ∞ÊãíÁªùÈó®ÔºåÂÆÉ‰ª¨‰ºòÂåñÊõ¥Êñ∞‰ø°Âè∑‰ª•‰∫ßÁîüÁ®≥ÂÆöÊúâÊïàÁöÑpromptÊîπËøõ„ÄÇÂΩì‰ºòÂåñÂÅúÊªûÊó∂ÔºåËá™ÈÄÇÂ∫îÂéãÁº©Á≠ñÁï•ÊèêÁÇºpromptÁöÑÊ†∏ÂøÉÊ¶ÇÂøµÔºåÈáçÊûÑ‰ºòÂåñËΩ®ËøπÂπ∂ÂºÄËæüÊñ∞ÁöÑË∑ØÂæÑ„ÄÇÈÄöËøáÁ≠ñÁï•ÊÄßÂú∞ÂºïÂÖ•‰ø°ÊÅØÊçüÂ§±ÔºåGRACEÂú®ÊÄßËÉΩÂíåÊïàÁéáÊñπÈù¢ÈÉΩÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊèêÂçá„ÄÇÂú®Ë∑®‰∏â‰∏™ÂÆûÈôÖÈ¢ÜÂüüÁöÑ11‰∏™‰ªªÂä°ÔºàÂåÖÊã¨BIG-Bench Hard (BBH)„ÄÅÁâπÂÆöÈ¢ÜÂüüÂíåÈÄöÁî®NLP‰ªªÂä°ÔºâÁöÑÂπøÊ≥õÂÆûÈ™å‰∏≠ÔºåGRACEÁõ∏ÂØπ‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÂàÜÂà´ÂÆûÁé∞‰∫Ü4.7%„ÄÅ4.4%Âíå2.7%ÁöÑÊòæËëóÂπ≥ÂùáÁõ∏ÂØπÊÄßËÉΩÊèêÂçá„ÄÇËøõ‰∏ÄÊ≠•ÁöÑÂàÜÊûêË°®ÊòéÔºåGRACE‰ªÖ‰ΩøÁî®ÂÖàÂâçÊñπÊ≥ïÊâÄÈúÄÁöÑ25%ÁöÑpromptÁîüÊàêÈ¢ÑÁÆóÂ∞±ÂÆûÁé∞‰∫ÜËøô‰∫õÊî∂ÁõäÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂È´ò‰ºòÂåñÊïàÁéáÂíå‰ΩéËÆ°ÁÆóÂºÄÈîÄ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Ëá™Âä®Prompt‰ºòÂåñ‰∏≠ÊïàÁéá‰Ωé„ÄÅÊòìÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºòÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•Á®≥ÂÆöÁîüÊàêÊîπËøõÁöÑPromptÔºåÂØºËá¥‰ºòÂåñËøáÁ®ãÊïàÁéá‰Ωé‰∏ãÔºå‰∏îÂÆπÊòìÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºòËß£ÔºåÈöæ‰ª•ÊâæÂà∞ÂÖ®Â±ÄÊúÄ‰ºòÁöÑPrompt„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•Èó®ÊéßÊú∫Âà∂Êù•Á®≥ÂÆöPromptÁöÑÊõ¥Êñ∞ËøáÁ®ãÔºåÂπ∂Âà©Áî®Ëá™ÈÄÇÂ∫îÂéãÁº©Á≠ñÁï•Êù•Ë∑≥Âá∫Â±ÄÈÉ®ÊúÄ‰ºò„ÄÇÈó®ÊéßÊú∫Âà∂ÂèØ‰ª•ËøáÊª§ÊéâÊó†ÊïàÊàñÊúâÂÆ≥ÁöÑÊõ¥Êñ∞‰ø°Âè∑Ôºå‰ªéËÄå‰øùËØÅPromptÁöÑÁ®≥ÂÆöÊîπËøõ„ÄÇËá™ÈÄÇÂ∫îÂéãÁº©ÂàôÂèØ‰ª•Âú®‰ºòÂåñÂÅúÊªûÊó∂ÔºåÊèêÁÇºPromptÁöÑÊ†∏ÂøÉ‰ø°ÊÅØÔºåÈáçÊûÑ‰ºòÂåñËΩ®ËøπÔºå‰ªéËÄåÂºÄËæüÊñ∞ÁöÑ‰ºòÂåñË∑ØÂæÑ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGRACEÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈó®Êéß‰ºòÂåñÔºàGated RefinementÔºâÂíåËá™ÈÄÇÂ∫îÂéãÁº©ÔºàAdaptive CompressionÔºâ„ÄÇÈó®Êéß‰ºòÂåñÊ®°ÂùóÈÄöËøáÂèçÈ¶àË∞ÉËäÇÈó®ÂíåÊõ¥Êñ∞ÊãíÁªùÈó®Êù•ÊéßÂà∂PromptÁöÑÊõ¥Êñ∞ËøáÁ®ãÔºåÁ°Æ‰øùÊõ¥Êñ∞ÁöÑÊúâÊïàÊÄßÂíåÁ®≥ÂÆöÊÄß„ÄÇËá™ÈÄÇÂ∫îÂéãÁº©Ê®°ÂùóÂàôÂú®‰ºòÂåñÂÅúÊªûÊó∂ÔºåÂØπPromptËøõË°åÂéãÁº©ÔºåÊèêÂèñÊ†∏ÂøÉ‰ø°ÊÅØÔºåÂπ∂Âü∫‰∫éÂéãÁº©ÂêéÁöÑPromptÈáçÊñ∞ÂºÄÂßã‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöGRACEÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÈó®ÊéßÊú∫Âà∂ÂíåËá™ÈÄÇÂ∫îÂéãÁº©Á≠ñÁï•ÁªìÂêàËµ∑Êù•ÔºåÂÖ±Âêå‰ΩúÁî®‰∫éPrompt‰ºòÂåñËøáÁ®ã„ÄÇÈó®ÊéßÊú∫Âà∂‰øùËØÅ‰∫ÜPromptÊõ¥Êñ∞ÁöÑÁ®≥ÂÆöÊÄßÔºåÈÅøÂÖç‰∫ÜÊó†ÊïàÊàñÊúâÂÆ≥ÁöÑÊõ¥Êñ∞ÔºåËÄåËá™ÈÄÇÂ∫îÂéãÁº©ÂàôÊèê‰æõ‰∫ÜË∑≥Âá∫Â±ÄÈÉ®ÊúÄ‰ºòÁöÑÊâãÊÆµÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜPrompt‰ºòÂåñÁöÑÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåGRACEËÉΩÂ§üÊõ¥Á®≥ÂÆö„ÄÅÊõ¥È´òÊïàÂú∞ÁîüÊàêÈ´òË¥®ÈáèÁöÑPrompt„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÈó®Êéß‰ºòÂåñÊ®°Âùó‰∏≠ÁöÑÂèçÈ¶àË∞ÉËäÇÈó®ÂíåÊõ¥Êñ∞ÊãíÁªùÈó®ÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆ„ÄÇÂèçÈ¶àË∞ÉËäÇÈó®Ê†πÊçÆPromptÁöÑÊÄßËÉΩÂèçÈ¶àÊù•Ë∞ÉÊï¥Êõ¥Êñ∞‰ø°Âè∑ÁöÑÂº∫Â∫¶ÔºåËÄåÊõ¥Êñ∞ÊãíÁªùÈó®ÂàôÊ†πÊçÆÊõ¥Êñ∞‰ø°Âè∑ÁöÑË¥®ÈáèÊù•ÂÜ≥ÂÆöÊòØÂê¶Êé•ÂèóÊõ¥Êñ∞„ÄÇËá™ÈÄÇÂ∫îÂéãÁº©Ê®°ÂùóÂàôÈúÄË¶ÅËÆæËÆ°ÂêàÈÄÇÁöÑÂéãÁº©ÁÆóÊ≥ïÔºå‰ª•‰øùËØÅÂéãÁº©ÂêéÁöÑPromptËÉΩÂ§ü‰øùÁïôÊ†∏ÂøÉ‰ø°ÊÅØÔºåÂπ∂ËÉΩÂ§üÊúâÊïàÂú∞ÂºïÂØºÂêéÁª≠ÁöÑ‰ºòÂåñËøáÁ®ã„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ÈÄâÊã©ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑ‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

GRACEÂú®11‰∏™‰ªªÂä°‰∏äËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÂåÖÊã¨BIG-Bench Hard (BBH)„ÄÅÁâπÂÆöÈ¢ÜÂüüÂíåÈÄöÁî®NLP‰ªªÂä°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGRACEÁõ∏ÂØπ‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÂàÜÂà´ÂÆûÁé∞‰∫Ü4.7%„ÄÅ4.4%Âíå2.7%ÁöÑÊòæËëóÂπ≥ÂùáÁõ∏ÂØπÊÄßËÉΩÊèêÂçá„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåGRACE‰ªÖ‰ΩøÁî®ÂÖàÂâçÊñπÊ≥ïÊâÄÈúÄÁöÑ25%ÁöÑPromptÁîüÊàêÈ¢ÑÁÆóÂ∞±ÂÆûÁé∞‰∫ÜËøô‰∫õÊî∂ÁõäÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂È´ò‰ºòÂåñÊïàÁéáÂíå‰ΩéËÆ°ÁÆóÂºÄÈîÄ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

GRACEÊ°ÜÊû∂ÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅPromptÂ∑•Á®ãÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÊñáÊú¨ÁîüÊàê„ÄÅÈóÆÁ≠îÁ≥ªÁªü„ÄÅÊú∫Âô®ÁøªËØëÁ≠â„ÄÇÈÄöËøáËá™Âä®‰ºòÂåñPromptÔºåÂèØ‰ª•ÊòæËëóÊèêÂçáËøô‰∫õÂ∫îÁî®ÁöÑÊïàÊûúÔºåÂπ∂Èôç‰Ωé‰∫∫Â∑•ËÆæËÆ°ÁöÑÊàêÊú¨„ÄÇËØ•Á†îÁ©∂ÂØπ‰∫éÊé®Âä®LLMÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊôÆÂèäÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Prompt engineering is crucial for leveraging the full potential of large language models (LLMs). While automatic prompt optimization offers a scalable alternative to costly manual design, generating effective prompts remains challenging. Existing methods often struggle to stably generate improved prompts, leading to low efficiency, and overlook that prompt optimization easily gets trapped in local optima. Addressing this, we propose GRACE, a framework that integrates two synergistic strategies: Gated Refinement and Adaptive Compression, achieving Efficient prompt optimization. The gated refinement strategy introduces a feedback regulation gate and an update rejection gate, which refine update signals to produce stable and effective prompt improvements. When optimization stagnates, the adaptive compression strategy distills the prompt's core concepts, restructuring the optimization trace and opening new paths. By strategically introducing information loss through refinement and compression, GRACE delivers substantial gains in performance and efficiency. In extensive experiments on 11 tasks across three practical domains, including BIG-Bench Hard (BBH), domain-specific, and general NLP tasks, GRACE achieves significant average relative performance improvements of 4.7%, 4.4% and 2.7% over state-of-the-art methods, respectively. Further analysis shows that GRACE achieves these gains using only 25% of the prompt generation budget required by prior methods, highlighting its high optimization efficiency and low computational overhead. Our code is available at https://github.com/Eric8932/GRACE.

