---
layout: default
title: Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models
---

# Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23233" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23233v1</a>
  <a href="https://arxiv.org/pdf/2509.23233.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23233v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23233v1', 'Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sina J. Semnani, Jirayu Burapacheep, Arpandeep Khatua, Thanawan Atchariyachanvanit, Zheng Wang, Monica S. Lam

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: EMNLP 2025 (Main Conference)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLAIREç³»ç»Ÿï¼Œç”¨äºæ£€æµ‹ç»´åŸºç™¾ç§‘è¯­æ–™åº“çº§åˆ«çŸ¥è¯†ä¸ä¸€è‡´æ€§ï¼Œæå‡ç¼–è¾‘æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†ä¸ä¸€è‡´æ€§æ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç»´åŸºç™¾ç§‘` `ä¿¡æ¯æ£€ç´¢` `äººæœºåä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç»´åŸºç™¾ç§‘ä½œä¸ºçŸ¥è¯†åº“å­˜åœ¨äº‹å®ä¸ä¸€è‡´é—®é¢˜ï¼Œå½±å“ä¸‹æ¸¸åº”ç”¨ï¼ŒäºŸéœ€æœ‰æ•ˆæ£€æµ‹æ–¹æ³•ã€‚
2. æå‡ºCLAIREç³»ç»Ÿï¼Œåˆ©ç”¨LLMæ¨ç†å’Œæ£€ç´¢ï¼Œè¾…åŠ©äººå·¥è¯†åˆ«ç»´åŸºç™¾ç§‘ä¸­çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCLAIREèƒ½æ˜¾è‘—æå‡ç»´åŸºç™¾ç§‘ç¼–è¾‘çš„æ•ˆç‡å’Œä¿¡å¿ƒï¼Œå¹¶æ„å»ºäº†WIKICOLLIDEåŸºå‡†æ•°æ®é›†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»´åŸºç™¾ç§‘æ˜¯å…¨çƒæœ€å¤§çš„å¼€æ”¾çŸ¥è¯†è¯­æ–™åº“ï¼Œè¢«å¹¿æ³›ä½¿ç”¨ï¼Œå¹¶ä¸”æ˜¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„å…³é”®èµ„æºã€‚ç¡®ä¿å…¶å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚æœ¬æ–‡å…³æ³¨ä¸ä¸€è‡´æ€§ï¼Œä¸€ç§ç‰¹æ®Šçš„factual inaccuracyï¼Œå¹¶æå‡ºäº†è¯­æ–™åº“çº§åˆ«ä¸ä¸€è‡´æ€§æ£€æµ‹çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†CLAIREï¼Œä¸€ä¸ªagenticç³»ç»Ÿï¼Œå®ƒç»“åˆäº†LLMæ¨ç†å’Œæ£€ç´¢ï¼Œä»¥å‘ˆç°æ½œåœ¨çš„ä¸ä¸€è‡´æ€§å£°æ˜ä»¥åŠä¸Šä¸‹æ–‡è¯æ®ï¼Œä¾›äººå·¥å®¡æŸ¥ã€‚åœ¨ä¸€é¡¹ä¸ç»éªŒä¸°å¯Œçš„ç»´åŸºç™¾ç§‘ç¼–è¾‘çš„ç”¨æˆ·ç ”ç©¶ä¸­ï¼Œ87.5%çš„äººæŠ¥å‘Šåœ¨ä½¿ç”¨CLAIREæ—¶ä¿¡å¿ƒæ›´é«˜ï¼Œå¹¶ä¸”å‚ä¸è€…åœ¨ç›¸åŒçš„æ—¶é—´å†…è¯†åˆ«å‡º64.7%æ›´å¤šçš„ä¸ä¸€è‡´æ€§ã€‚ç»“åˆCLAIREå’Œäººå·¥æ ‡æ³¨ï¼Œæˆ‘ä»¬è´¡çŒ®äº†WIKICOLLIDEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªçœŸå®çš„ç»´åŸºç™¾ç§‘ä¸ä¸€è‡´æ€§åŸºå‡†ã€‚é€šè¿‡ä½¿ç”¨CLAIREè¾…åŠ©åˆ†æçš„éšæœºæŠ½æ ·ï¼Œæˆ‘ä»¬å‘ç°è‡³å°‘3.3%çš„è‹±è¯­ç»´åŸºç™¾ç§‘äº‹å®ä¸å¦ä¸€ä¸ªäº‹å®ç›¸çŸ›ç›¾ï¼Œå¹¶ä¸”ä¸ä¸€è‡´æ€§ä¼ æ’­åˆ°7.3%çš„FEVEROUSå’Œ4.0%çš„AmbigQAç¤ºä¾‹ä¸­ã€‚åœ¨æ­¤æ•°æ®é›†ä¸Šå¯¹å¼ºå¤§çš„åŸºçº¿è¿›è¡ŒåŸºå‡†æµ‹è¯•æ˜¾ç¤ºå‡ºå·¨å¤§çš„æå‡ç©ºé—´ï¼šæœ€ä½³çš„å®Œå…¨è‡ªåŠ¨åŒ–ç³»ç»Ÿä»…è¾¾åˆ°75.1%çš„AUROCã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç»´åŸºç™¾ç§‘ä¸­è¯­æ–™åº“çº§åˆ«çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äººå·¥å®¡æ ¸ï¼Œæ•ˆç‡ä½ä¸‹ä¸”éš¾ä»¥å‘ç°éšè—çš„ä¸ä¸€è‡´æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶å…·å¤‡ä¸€å®šçš„çŸ¥è¯†æ¨ç†èƒ½åŠ›ï¼Œä½†ç›´æ¥åº”ç”¨äºç»´åŸºç™¾ç§‘çš„è§„æ¨¡åŒ–ä¸ä¸€è‡´æ€§æ£€æµ‹ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç¼ºä¹é’ˆå¯¹æ€§çš„å·¥å…·å’ŒåŸºå‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œä¿¡æ¯æ£€ç´¢æŠ€æœ¯ï¼Œæ„å»ºä¸€ä¸ªæ™ºèƒ½ä»£ç†ç³»ç»Ÿï¼ˆCLAIREï¼‰ï¼Œè¾…åŠ©äººå·¥ç¼–è¾‘å‘ç°å¹¶éªŒè¯ç»´åŸºç™¾ç§‘ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡LLMè¿›è¡Œåˆæ­¥çš„çŸ›ç›¾è¯†åˆ«å’Œè¯æ®æ£€ç´¢ï¼Œç„¶åå°†ç»“æœå‘ˆç°ç»™äººå·¥ç¼–è¾‘è¿›è¡Œæœ€ç»ˆç¡®è®¤ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLAIREç³»ç»Ÿçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å£°æ˜æå–ï¼šä»ç»´åŸºç™¾ç§‘æ–‡ç« ä¸­æå–äº‹å®æ€§å£°æ˜ã€‚2) çŸ›ç›¾æ£€æµ‹ï¼šåˆ©ç”¨LLMå¯¹æå–çš„å£°æ˜è¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨çŸ›ç›¾ã€‚3) è¯æ®æ£€ç´¢ï¼šå¯¹äºæ½œåœ¨çš„çŸ›ç›¾å£°æ˜ï¼Œä»ç»´åŸºç™¾ç§‘ä¸­æ£€ç´¢ç›¸å…³çš„ä¸Šä¸‹æ–‡è¯æ®ã€‚4) äººå·¥å®¡æ ¸ï¼šå°†çŸ›ç›¾å£°æ˜å’Œè¯æ®å‘ˆç°ç»™äººå·¥ç¼–è¾‘ï¼Œè¿›è¡Œæœ€ç»ˆç¡®è®¤å’Œä¿®æ­£ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†LLMçš„æ¨ç†èƒ½åŠ›ä¸ä¿¡æ¯æ£€ç´¢æŠ€æœ¯ç›¸ç»“åˆï¼Œæ„å»ºäº†ä¸€ä¸ªåŠè‡ªåŠ¨åŒ–çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§æ£€æµ‹ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ„å»ºäº†WIKICOLLIDEæ•°æ®é›†ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†åŸºå‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šCLAIREç³»ç»Ÿä½¿ç”¨äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå…·ä½“æ¨¡å‹æœªçŸ¥ï¼‰è¿›è¡ŒçŸ›ç›¾æ£€æµ‹å’Œè¯æ®æ£€ç´¢ã€‚åœ¨çŸ›ç›¾æ£€æµ‹é˜¶æ®µï¼Œé‡‡ç”¨äº†åŸºäºpromptingçš„æ–¹æ³•ï¼Œå¼•å¯¼LLMåˆ¤æ–­ä¸¤ä¸ªå£°æ˜æ˜¯å¦çŸ›ç›¾ã€‚è¯æ®æ£€ç´¢é˜¶æ®µï¼Œä½¿ç”¨äº†åŸºäºå…³é”®è¯çš„æ£€ç´¢æ–¹æ³•ï¼Œä»ç»´åŸºç™¾ç§‘ä¸­æ£€ç´¢ä¸å£°æ˜ç›¸å…³çš„æ–‡ç« æ®µè½ã€‚äººå·¥å®¡æ ¸ç•Œé¢è®¾è®¡ç®€æ´æ˜äº†ï¼Œæ–¹ä¾¿ç¼–è¾‘å¿«é€Ÿæµè§ˆå£°æ˜å’Œè¯æ®ï¼Œå¹¶åšå‡ºåˆ¤æ–­ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨CLAIREåï¼Œç»´åŸºç™¾ç§‘ç¼–è¾‘çš„ä¿¡å¿ƒæå‡äº†87.5%ï¼Œå¹¶ä¸”åœ¨ç›¸åŒæ—¶é—´å†…è¯†åˆ«å‡ºçš„ä¸ä¸€è‡´æ€§å¢åŠ äº†64.7%ã€‚é€šè¿‡CLAIREè¾…åŠ©åˆ†æï¼Œå‘ç°è‡³å°‘3.3%çš„è‹±è¯­ç»´åŸºç™¾ç§‘äº‹å®å­˜åœ¨çŸ›ç›¾ã€‚åœ¨WIKICOLLIDEæ•°æ®é›†ä¸Šï¼Œæœ€ä½³çš„è‡ªåŠ¨åŒ–ç³»ç»ŸAUROCä»…ä¸º75.1%ï¼Œè¡¨æ˜ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤§è§„æ¨¡çŸ¥è¯†åº“çš„è´¨é‡æ§åˆ¶ï¼Œä¾‹å¦‚ç»´åŸºç™¾ç§‘ã€DBpediaç­‰ã€‚é€šè¿‡è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤çŸ¥è¯†ä¸ä¸€è‡´æ€§ï¼Œå¯ä»¥æé«˜çŸ¥è¯†åº“çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä»è€Œæå‡ä¸‹æ¸¸åº”ç”¨ï¼ˆå¦‚é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±ï¼‰çš„æ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„çŸ¥è¯†åº“å’Œè¯­è¨€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Wikipedia is the largest open knowledge corpus, widely used worldwide and serving as a key resource for training large language models (LLMs) and retrieval-augmented generation (RAG) systems. Ensuring its accuracy is therefore critical. But how accurate is Wikipedia, and how can we improve it?
>   We focus on inconsistencies, a specific type of factual inaccuracy, and introduce the task of corpus-level inconsistency detection. We present CLAIRE, an agentic system that combines LLM reasoning with retrieval to surface potentially inconsistent claims along with contextual evidence for human review. In a user study with experienced Wikipedia editors, 87.5% reported higher confidence when using CLAIRE, and participants identified 64.7% more inconsistencies in the same amount of time.
>   Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first benchmark of real Wikipedia inconsistencies. Using random sampling with CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset reveals substantial headroom: the best fully automated system achieves an AUROC of only 75.1%.
>   Our results show that contradictions are a measurable component of Wikipedia and that LLM-based systems like CLAIRE can provide a practical tool to help editors improve knowledge consistency at scale.

