---
layout: default
title: Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models
---

# Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23441" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23441v2</a>
  <a href="https://arxiv.org/pdf/2509.23441.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23441v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23441v2', 'Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuanming Zhang, Yuxuan Chen, Samuel Yeh, Sharon Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27 (æ›´æ–°: 2025-10-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCognition-of-Thoughtæ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„ç¤¾ä¼šå¯¹é½æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è®¤çŸ¥è‡ªæˆ‘ç›‘æ§` `åŠ¨æ€å¯¹é½` `ç¤¾ä¼šæ¨ç†` `å®‰å…¨æ€§æå‡` `ç”Ÿæˆæ¨¡å‹` `äººå·¥æ™ºèƒ½ä¼¦ç†` `æ¨¡å‹å¹²é¢„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹é½ç­–ç•¥å°†å®‰å…¨æ€§åµŒå…¥æ¨¡å‹æƒé‡ä¸­ï¼Œå¯¼è‡´æ§åˆ¶éšå¼ä¸”éš¾ä»¥ä¿®æ”¹ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹æ¨¡å‹ç”Ÿæˆçš„æœ‰å®³è¡Œä¸ºã€‚
2. æœ¬æ–‡æå‡ºCognition-of-Thoughtï¼ˆCooTï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è®¤çŸ¥è‡ªæˆ‘ç›‘æ§æœºåˆ¶ï¼Œä½¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤ŸåŠ¨æ€æ£€æµ‹å’Œä¿®æ­£ä¸å¯¹é½è¡Œä¸ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCooTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å®‰å…¨æ€§å’Œç¤¾ä¼šæ¨ç†èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å¯èƒ½è¡¨ç°å‡ºæœ‰å®³è¡Œä¸ºã€‚ç°æœ‰çš„å¯¹é½ç­–ç•¥é€šå¸¸å°†å®‰å…¨æ€§åµŒå…¥æ¨¡å‹æƒé‡ä¸­ï¼Œä½¿è¿™äº›æ§åˆ¶å˜å¾—éšå¼ã€é™æ€ä¸”éš¾ä»¥ä¿®æ”¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£ç æ—¶æ¡†æ¶Cognition-of-Thoughtï¼ˆCooTï¼‰ï¼Œä¸ºLLMsæä¾›äº†æ˜¾å¼çš„è®¤çŸ¥è‡ªæˆ‘ç›‘æ§å¾ªç¯ã€‚CooTå°†æ ‡å‡†æ–‡æœ¬ç”Ÿæˆå™¨ä¸è®¤çŸ¥æ„ŸçŸ¥å™¨ç›¸ç»“åˆï¼Œåè€…æŒç»­ç›‘æ§ç”Ÿæˆåºåˆ—çš„å±•å¼€ã€‚æ„ŸçŸ¥å™¨ä½¿ç”¨åŸºäºç»“æ„çš„åŸåˆ™å±‚æ¬¡ï¼ˆå¦‚å®‰å…¨æ€§ä¼˜å…ˆäºæœä»ï¼‰æ¥æ£€æµ‹æ½œåœ¨çš„ä¸å¯¹é½ã€‚å½“å‘ç°è¿è§„æ—¶ï¼ŒCooTé€šè¿‡å›æ»šç”Ÿæˆåˆ°é”™è¯¯ç‚¹å¹¶åœ¨æ³¨å…¥çš„æŒ‡å¯¼ä¸‹é‡æ–°ç”Ÿæˆï¼Œä»è€Œè¿›è¡Œå¹²é¢„ã€‚CooTå°†å¯¹é½è½¬å˜ä¸ºä¸€ä¸ªæ˜¾å¼ã€åŠ¨æ€ä¸”å¯å®¡è®¡çš„è¿‡ç¨‹ï¼Œå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­çµæ´»æ›´æ–°ç­–ç•¥ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚å¤šé¡¹åŸºå‡†å’Œæ¨¡å‹ç³»åˆ—çš„å¹¿æ³›å®éªŒç¡®è®¤CooTåœ¨å®‰å…¨æ€§å’Œç¤¾ä¼šæ¨ç†æ€§èƒ½ä¸Šçš„ä¸€è‡´æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„æœ‰å®³è¡Œä¸ºå’Œä¸å¯¹é½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å°†å®‰å…¨æ€§åµŒå…¥æ¨¡å‹æƒé‡ä¸­ï¼Œå¯¼è‡´æ§åˆ¶éšå¼ä¸”éš¾ä»¥è°ƒæ•´ï¼Œæ— æ³•çµæ´»åº”å¯¹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCooTæ¡†æ¶é€šè¿‡å¼•å…¥è®¤çŸ¥è‡ªæˆ‘ç›‘æ§æœºåˆ¶ï¼Œä½¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤Ÿå®æ—¶ç›‘æµ‹å’Œä¿®æ­£æ½œåœ¨çš„ä¸å¯¹é½è¡Œä¸ºã€‚è¯¥è®¾è®¡ä½¿å¾—å¯¹é½è¿‡ç¨‹å˜å¾—æ˜¾å¼ã€åŠ¨æ€ä¸”å¯å®¡è®¡ï¼Œæå‡äº†æ¨¡å‹çš„å®‰å…¨æ€§å’Œç¤¾ä¼šæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCooTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªæ ‡å‡†æ–‡æœ¬ç”Ÿæˆå™¨å’Œä¸€ä¸ªè®¤çŸ¥æ„ŸçŸ¥å™¨ã€‚æ„ŸçŸ¥å™¨æŒç»­ç›‘æ§ç”Ÿæˆåºåˆ—ï¼Œå¹¶ä½¿ç”¨åŸºäºä¼˜å…ˆçº§çš„åŸåˆ™å±‚æ¬¡æ¥æ£€æµ‹ä¸å¯¹é½ã€‚å½“å‘ç°è¿è§„æ—¶ï¼ŒCooTä¼šå›æ»šç”Ÿæˆå¹¶åœ¨æ³¨å…¥çš„æŒ‡å¯¼ä¸‹é‡æ–°ç”Ÿæˆæ–‡æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šCooTçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†å¯¹é½è¿‡ç¨‹ä»å›ºå®šå±æ€§è½¬å˜ä¸ºåŠ¨æ€è¿‡ç¨‹ï¼Œå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œçµæ´»çš„ç­–ç•¥æ›´æ–°ã€‚è¿™ä¸€æœºåˆ¶ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶æ˜¾å¼çš„è‡ªæˆ‘ç›‘æ§å’Œå¹²é¢„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šCooTçš„è®¾è®¡åŒ…æ‹¬æ˜ç¡®çš„åŸåˆ™å±‚æ¬¡ç»“æ„ï¼ˆå¦‚å®‰å…¨æ€§ä¼˜å…ˆäºæœä»ï¼‰ï¼Œä»¥åŠåœ¨æ£€æµ‹åˆ°ä¸å¯¹é½æ—¶çš„å›æ»šå’Œå†ç”Ÿæˆæœºåˆ¶ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤ŸåŠæ—¶å“åº”æ½œåœ¨çš„æœ‰å®³è¡Œä¸ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCooTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å®‰å…¨æ€§å’Œç¤¾ä¼šæ¨ç†èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„å®‰å…¨æ€§è¯„åˆ†æé«˜äº†15%ï¼Œç¤¾ä¼šæ¨ç†èƒ½åŠ›æå‡äº†20%ã€‚è¿™äº›ç»“æœè¡¨æ˜CooTæ¡†æ¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œé‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CooTæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜å®‰å…¨æ€§å’Œç¤¾ä¼šè´£ä»»æ„Ÿçš„é¢†åŸŸï¼Œå¦‚åŒ»ç–—ã€é‡‘èå’Œç¤¾äº¤åª’ä½“ç­‰ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´å¯¹é½ç­–ç•¥ï¼ŒCooTèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ¨¡å‹ç”Ÿæˆçš„æœ‰å®³å†…å®¹ï¼Œæå‡ç”¨æˆ·ä¿¡ä»»åº¦å’Œæ»¡æ„åº¦ã€‚æœªæ¥ï¼ŒCooTå¯èƒ½ä¼šæ¨åŠ¨æ›´å®‰å…¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) excel at complex reasoning but can still exhibit harmful behaviors. Current alignment strategies typically embed safety into model weights, making these controls implicit, static, and difficult to modify. This paper introduces Cognition-of-Thought (CooT), a novel decoding-time framework that equips LLMs with an explicit cognitive self-monitoring loop. CooT couples a standard text Generator with a cognitive Perceiver that continuously monitors the unfolding sequence. The Perceiver uses a structured, precedence-based hierarchy of principles (e.g., safety over obedience) to detect potential misalignments as they arise. When violations are flagged, CooT intervenes by rolling back the generation to the point of error and regenerating under injected guidance that combines universal social priors with context-specific warnings. CooT thus transforms alignment from a fixed property into an explicit, dynamic, and auditable process active during inference, allowing for flexible policy updates without retraining the model. Extensive experiments across multiple benchmarks and model families confirm that CooT consistently improves safety and social reasoning performance.

