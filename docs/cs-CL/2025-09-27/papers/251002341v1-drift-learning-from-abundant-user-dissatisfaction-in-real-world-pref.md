---
layout: default
title: DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning
---

# DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02341" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02341v1</a>
  <a href="https://arxiv.org/pdf/2510.02341.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02341v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02341v1', 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Wang, Bolian Li, Junlin Wu, Zhaoxuan Tan, Zheli Liu, Ruqi Zhang, Ananth Grama, Qingkai Zeng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/cacayaya/DRIFT.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRIFTä»¥è§£å†³ç°å®ä¸–ç•Œåå¥½å­¦ä¹ ä¸­çš„ç”¨æˆ·ä¸æ»¡ä¿¡å·é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”¨æˆ·ä¸æ»¡ä¿¡å·` `åå¥½å­¦ä¹ ` `åŠ¨æ€æŠ½æ ·` `æ¨¡å‹è®­ç»ƒ` `å¯¹è¯ç³»ç»Ÿ` `æ¨èç³»ç»Ÿ` `ä»£ç ç”Ÿæˆ` `ç”¨æˆ·åé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åå¥½å­¦ä¹ æ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººç±»æ ‡æ³¨ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ç”¨æˆ·ä¸æ»¡ä¿¡å·ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒä¸å……åˆ†ã€‚
2. DRIFTé€šè¿‡åˆ©ç”¨çœŸå®çš„ç”¨æˆ·ä¸æ»¡ä¿¡å·è¿›è¡Œè®­ç»ƒï¼Œå¹¶åŠ¨æ€æŠ½æ ·æ­£é¢åé¦ˆï¼Œæå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDRIFTåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°å®ä¸–ç•Œçš„å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²ï¼ˆå¦‚å¯¹è¯AIç³»ç»Ÿã€ä»£ç ç”ŸæˆåŠ©æ‰‹ï¼‰è‡ªç„¶äº§ç”Ÿå¤§é‡éšå«çš„ç”¨æˆ·ä¸æ»¡ï¼ˆDSATï¼‰ä¿¡å·ï¼Œç”¨æˆ·é€šè¿‡è¿­ä»£æ”¹è¿›ã€çº æ­£å’Œè¡¨è¾¾åå¥½æ¥å¯»æ±‚æ›´å¥½çš„ç­”æ¡ˆï¼Œè€Œæ˜ç¡®çš„æ»¡æ„åº¦ï¼ˆSATï¼‰åé¦ˆåˆ™ç¨€ç¼ºã€‚ç°æœ‰çš„åå¥½å­¦ä¹ æ–¹æ³•ä¸è¿™ç§æ•°æ®ç‰¹å¾ä¸åŒ¹é…ï¼Œä¾èµ–äºæ˜‚è´µçš„äººç±»æ ‡æ³¨æˆ–å‡è®¾å¤§é‡æ­£é¢åé¦ˆã€‚æœ¬æ–‡æå‡ºäº†DRIFTï¼ˆDissatisfaction-Refined Iterative preference Trainingï¼‰ï¼Œè¯¥æ–¹æ³•ä»¥çœŸå®çš„DSATä¿¡å·ä¸ºåŸºç¡€è¿›è¡Œè®­ç»ƒï¼Œå¹¶åŠ¨æ€ä»ä¸æ–­æ¼”å˜çš„ç­–ç•¥ä¸­æŠ½æ ·æ­£é¢åé¦ˆã€‚å®éªŒè¯æ˜ï¼ŒåŸºäºçœŸå®çš„WildFeedbackæ•°æ®é›†å’Œåˆæˆçš„UltraFeedbackæ•°æ®é›†è®­ç»ƒçš„DRIFTæ¨¡å‹åœ¨WildBenchä»»åŠ¡å¾—åˆ†ä¸Šæå‡äº†6.23%ï¼ˆ7Bï¼‰/ 7.61%ï¼ˆ14Bï¼‰ï¼Œåœ¨AlpacaEval2èƒœç‡ä¸Šæå‡äº†8.95%ï¼ˆ7Bï¼‰/ 12.29%ï¼ˆ14Bï¼‰ï¼Œè¶…è¶Šäº†å¼ºåŸºçº¿æ–¹æ³•å¦‚è¿­ä»£DPOå’ŒSPINã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°å®ä¸–ç•Œåå¥½å­¦ä¹ ä¸­ç”¨æˆ·ä¸æ»¡ä¿¡å·çš„æœ‰æ•ˆåˆ©ç”¨é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºæ˜‚è´µçš„äººç±»æ ‡æ³¨ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨éšå«çš„ç”¨æˆ·åé¦ˆï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDRIFTçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è®­ç»ƒé”šå®šåœ¨çœŸå®çš„ç”¨æˆ·ä¸æ»¡ä¿¡å·ä¸Šï¼Œå¹¶ä»ä¸æ–­æ¼”å˜çš„ç­–ç•¥ä¸­åŠ¨æ€æŠ½æ ·æ­£é¢åé¦ˆã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ›´å¥½åœ°æ•æ‰ç”¨æˆ·çš„çœŸå®åå¥½å’Œéœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDRIFTçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ä¿¡å·å¤„ç†ã€åŠ¨æ€æŠ½æ ·å’Œæ¨¡å‹è®­ç»ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†ç”¨æˆ·çš„DSATä¿¡å·ï¼Œç„¶åé€šè¿‡ç‰¹å®šç®—æ³•å¤„ç†è¿™äº›ä¿¡å·ï¼Œæ¥ç€åŠ¨æ€æŠ½æ ·æ­£é¢åé¦ˆï¼Œæœ€åè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šDRIFTçš„æœ€é‡è¦æŠ€æœ¯åˆ›æ–°åœ¨äºå…¶åˆ©ç”¨ç”¨æˆ·ä¸æ»¡ä¿¡å·è¿›è¡Œè®­ç»ƒçš„èƒ½åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºæ­£é¢åé¦ˆçš„æ–¹å¼æœ¬è´¨ä¸Šä¸åŒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒDRIFTèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç”¨æˆ·çš„çœŸå®åå¥½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DRIFTä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬æŸå¤±å‡½æ•°çš„é€‰æ‹©å’ŒåŠ¨æ€æŠ½æ ·ç­–ç•¥çš„å®ç°ã€‚æŸå¤±å‡½æ•°æ—¨åœ¨æœ€å¤§åŒ–ç”¨æˆ·æ»¡æ„åº¦çš„è¾¹é™…ï¼ŒåŒæ—¶é¿å…æ¢¯åº¦é€€åŒ–ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒæ¢ç´¢èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDRIFTåœ¨WildBenchä»»åŠ¡å¾—åˆ†ä¸Šæå‡äº†6.23%ï¼ˆ7Bï¼‰å’Œ7.61%ï¼ˆ14Bï¼‰ï¼Œåœ¨AlpacaEval2èƒœç‡ä¸Šæå‡äº†8.95%ï¼ˆ7Bï¼‰å’Œ12.29%ï¼ˆ14Bï¼‰ã€‚åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­ï¼Œ14Bæ¨¡å‹è®­ç»ƒåçš„DRIFTè¶…è¶Šäº†GPT-4o-miniï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DRIFTçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨å¯¹è¯ç³»ç»Ÿã€æ¨èç³»ç»Ÿå’Œä»£ç ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨ç”¨æˆ·ä¸æ»¡ä¿¡å·ï¼ŒDRIFTèƒ½å¤Ÿæå‡æ¨¡å‹çš„å“åº”è´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚æœªæ¥ï¼ŒDRIFTå¯èƒ½ä¼šå½±å“æ›´å¤šåŸºäºç”¨æˆ·åé¦ˆçš„æ™ºèƒ½åº”ç”¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce \textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) / +7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.

