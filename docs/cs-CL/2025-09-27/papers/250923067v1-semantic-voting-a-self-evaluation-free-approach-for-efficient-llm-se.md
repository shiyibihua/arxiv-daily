---
layout: default
title: Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks
---

# Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23067" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23067v1</a>
  <a href="https://arxiv.org/pdf/2509.23067.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23067v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23067v1', 'Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chunyang Jiang, Yonggang Zhang, Yiyang Cai, Chi-Min Chan, Yulong Liu, Mingming Chen, Wei Xue, Yike Guo

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰æŠ•ç¥¨æ–¹æ³•ï¼Œæ— éœ€è‡ªè¯„ä¼°å³å¯é«˜æ•ˆæå‡LLMåœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæå‡` `è¯­ä¹‰æŠ•ç¥¨` `å¥å­åµŒå…¥` `å¼€æ”¾å¼ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè‡ªæå‡æ–¹æ³•ä¾èµ–è‡ªè¯„ä¼°ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å­˜åœ¨å›ºæœ‰åå·®å¯¼è‡´è¿‡åº¦è‡ªä¿¡ã€‚
2. æå‡ºè¯­ä¹‰æŠ•ç¥¨æ–¹æ³•ï¼Œé€šè¿‡å¥å­åµŒå…¥æ¨¡å‹è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œå®ç°è½¯åŒ¹é…ï¼Œé¿å…è‡ªè¯„ä¼°ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½ä¸Šå‡ä¼˜äºè‡ªè¯„ä¼°æ–¹æ³•ï¼Œé€‚ç”¨äºå¤šç§æ¨¡å‹å’Œä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›‘ç£æ•°æ®çš„è·å–æˆæœ¬æ—¥ç›Šå¢åŠ ï¼Œæ¨åŠ¨äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªæå‡çš„å¹¿æ³›å…³æ³¨ã€‚åƒå¤šæ•°æŠ•ç¥¨è¿™æ ·ç®€å•çš„æ— ç›‘ç£ä¿¡å·å·²è¢«è¯æ˜åœ¨å¯éªŒè¯ä»»åŠ¡ä¸­ç”Ÿæˆä¼ªæ ‡ç­¾æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ç”±äºå“åº”çš„å¼€æ”¾æ€§ï¼Œå®ƒä»¬åœ¨ä¸å¯éªŒè¯ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œç¿»è¯‘ï¼‰ä¸­çš„é€‚ç”¨æ€§å—åˆ°é™åˆ¶ã€‚å› æ­¤ï¼Œè‡ªè¯„ä¼°æœºåˆ¶ï¼ˆä¾‹å¦‚ï¼Œè‡ªåˆ¤æ–­å’Œç†µæœ€å°åŒ–ï¼‰ä¸»è¦ç”¨äºå¯¼å‡ºä¼ªæ ‡ç­¾ã€‚ç„¶è€Œï¼Œä¾èµ–äºLLMçš„è‡ªè¯„ä¼°é€šå¸¸ä¼šäº§ç”Ÿå¾ˆé«˜çš„è®¡ç®—å¼€é”€ï¼Œå¹¶ç”±äºå†…åœ¨åå·®è€Œå¼•å…¥è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºä¸å¯éªŒè¯ä»»åŠ¡çš„æ–°å‹æ— è‡ªè¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°è½»é‡çº§ä½†æœ‰æ•ˆçš„è‡ªæå‡ã€‚å—åˆ°å¯éªŒè¯ä»»åŠ¡ä¸­å¸¸ç”¨çš„å¤šæ•°æŠ•ç¥¨çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰æŠ•ç¥¨ä½œä¸ºä¸€ç§æ–°é¢–çš„æœºåˆ¶ï¼Œå®ƒå°†ç¡¬åŒ¹é…ï¼ˆå³ç²¾ç¡®åŒ¹é…ï¼‰çš„åŸåˆ™æ”¾å®½ä¸ºè½¯åŒ¹é…ï¼ˆå³è¯­ä¹‰ç›¸ä¼¼æ€§ï¼‰ã€‚é€šè¿‡åˆ©ç”¨è½»é‡çº§å¥å­åµŒå…¥æ¨¡å‹æ¥é‡åŒ–è¯­ä¹‰ç›¸ä¼¼æ€§æ¥å®ç°è½¯åŒ¹é…ï¼Œä»è€Œå‡è½»äº†è¿‡åº¦çš„è®¡ç®—è´Ÿæ‹…å’Œä¸è‡ªè¯„ä¼°ç›¸å…³çš„å†…åœ¨åå·®é™åˆ¶ã€‚å…¨é¢çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢å–å¾—äº†æ˜¾ç€æé«˜ï¼Œå¹¶ä¸”åœ¨å„ç§æ¨¡å‹æ¶æ„å’Œä»»åŠ¡ä¸­ï¼Œæ€»ä½“æ€§èƒ½ä¼˜äºè‡ªè¯„ä¼°æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸å¯éªŒè¯çš„å¼€æ”¾å¼ä»»åŠ¡ï¼ˆå¦‚ç¿»è¯‘ï¼‰ä¸­è¿›è¡Œè‡ªæå‡æ—¶ï¼Œä¼ ç»Ÿè‡ªè¯„ä¼°æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”æ˜“å—å›ºæœ‰åå·®å½±å“çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–LLMè‡ªèº«è¿›è¡Œè¯„ä¼°ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæ¶ˆè€—å¤§ï¼Œå¹¶ä¸”ç”±äºLLMçš„åè§ï¼Œå®¹æ˜“äº§ç”Ÿè¿‡åº¦è‡ªä¿¡ï¼Œä»è€Œå½±å“è‡ªæå‡çš„æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´å¯éªŒè¯ä»»åŠ¡ä¸­å¸¸ç”¨çš„å¤šæ•°æŠ•ç¥¨æœºåˆ¶ï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°ä¸å¯éªŒè¯ä»»åŠ¡ã€‚ä¸åŒäºå¤šæ•°æŠ•ç¥¨çš„ç¡¬åŒ¹é…ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰ï¼Œè®ºæ–‡æå‡ºä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œè½¯åŒ¹é…ã€‚é€šè¿‡è®¡ç®—ä¸åŒLLMç”Ÿæˆç»“æœä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œé€‰æ‹©è¯­ä¹‰ä¸Šæœ€ä¸€è‡´çš„ç»“æœä½œä¸ºä¼ªæ ‡ç­¾ï¼Œä»è€Œå®ç°è‡ªæå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) ä½¿ç”¨å¤šä¸ªLLMå¯¹åŒä¸€è¾“å…¥ç”Ÿæˆå¤šä¸ªå€™é€‰è¾“å‡ºã€‚2) ä½¿ç”¨è½»é‡çº§çš„å¥å­åµŒå…¥æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒSentence-BERTï¼‰è®¡ç®—æ‰€æœ‰å€™é€‰è¾“å‡ºä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚3) åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡ŒæŠ•ç¥¨ï¼Œé€‰æ‹©ä¸å…¶ä»–å€™é€‰è¾“å‡ºè¯­ä¹‰æœ€ç›¸ä¼¼çš„è¾“å‡ºä½œä¸ºä¼ªæ ‡ç­¾ã€‚4) ä½¿ç”¨ç”Ÿæˆçš„ä¼ªæ ‡ç­¾å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œå®ç°è‡ªæå‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨è¯­ä¹‰æŠ•ç¥¨ä»£æ›¿ä¼ ç»Ÿçš„è‡ªè¯„ä¼°æ–¹æ³•ã€‚ä¸è‡ªè¯„ä¼°ç›¸æ¯”ï¼Œè¯­ä¹‰æŠ•ç¥¨é¿å…äº†LLMè‡ªèº«çš„è¯„ä¼°è¿‡ç¨‹ï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶å‡å°‘äº†å› LLMå›ºæœ‰åå·®å¯¼è‡´çš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è½»é‡çº§çš„å¥å­åµŒå…¥æ¨¡å‹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ï¼Œè¿›ä¸€æ­¥é™ä½äº†è®¡ç®—è´Ÿæ‹…ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„å¥å­åµŒå…¥æ¨¡å‹ï¼Œéœ€è¦åœ¨è®¡ç®—æ•ˆç‡å’Œè¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚2) è®¾è®¡åˆé€‚çš„æŠ•ç¥¨ç­–ç•¥ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦æˆ–åŠ æƒç›¸ä¼¼åº¦æ¥ç¡®å®šæœ€ç»ˆçš„ä¼ªæ ‡ç­¾ã€‚3) æ¢ç´¢ä¸åŒçš„å¾®è°ƒç­–ç•¥ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡æˆ–æ­£åˆ™åŒ–æ–¹æ³•æ¥ä¼˜åŒ–LLMçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰æŠ•ç¥¨æ–¹æ³•åœ¨å¤šç§æ¨¡å‹æ¶æ„å’Œä»»åŠ¡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„è‡ªè¯„ä¼°æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨è¯­ä¹‰æŠ•ç¥¨æ–¹æ³•è®­ç»ƒçš„LLMåœ¨BLEUè¯„åˆ†ä¸Šå¹³å‡æå‡äº†2-3ä¸ªç‚¹ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬é™ä½äº†çº¦50%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬æ‘˜è¦å’Œå¯¹è¯ç”Ÿæˆä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦LLMè‡ªæå‡çš„å¼€æ”¾å¼ä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€å¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—æˆæœ¬å’Œå‡å°‘åå·®ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´é«˜æ•ˆåœ°è®­ç»ƒå’Œä¼˜åŒ–LLMï¼Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„ä»»åŠ¡å’Œæ¨¡å‹ï¼Œä¸ºLLMçš„è‡ªå­¦ä¹ ç ”ç©¶æä¾›æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rising cost of acquiring supervised data has driven significant interest in self-improvement for large language models (LLMs). Straightforward unsupervised signals like majority voting have proven effective in generating pseudo-labels for verifiable tasks, while their applicability to unverifiable tasks (e.g., translation) is limited by the open-ended character of responses. As a result, self-evaluation mechanisms (e.g., self-judging and entropy minimization) are predominantly used to derive pseudo-labels. However, self-evaluation relying on LLMs typically incurs high computational overhead and introduces overconfidence issues due to intrinsic biases. To address these challenges, we propose a novel self-evaluation-free approach for unverifiable tasks, designed for lightweight yet effective self-improvement. Inspired by majority voting commonly employed in verifiable tasks, we propose semantic voting as a novel mechanism that relaxes the principle of hard matching (i.e., exact matching) toward soft matching (i.e., semantic similarity). Soft matching is achieved by leveraging a lightweight sentence embedding model to quantify semantic similarity, thereby mitigating excessive computational burden and intrinsic bias-associated limitations of self-evaluation. Comprehensive experiments demonstrate that our method achieves substantial gains in computational efficiency and overall better performance than self-evaluation methods across diverse model architectures and tasks.

