---
layout: default
title: d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching
---

# d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23094" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23094v1</a>
  <a href="https://arxiv.org/pdf/2509.23094.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23094v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23094v1', 'd$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuchu Jiang, Yue Cai, Xiangzhong Luo, Jiale Fu, Jiarui Wang, Chonghan Liu, Xu Yang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Kamichanw/d2Cache)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºd$^2$Cacheï¼Œé€šè¿‡åŒé‡è‡ªé€‚åº”ç¼“å­˜åŠ é€Ÿæ‰©æ•£æ¨¡å‹LLMçš„æ¨ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `KVç¼“å­˜` `æ¨ç†åŠ é€Ÿ` `è‡ªé€‚åº”ç¼“å­˜` `åŒå‘æ³¨æ„åŠ›` `æ–‡æœ¬ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£æ¨¡å‹LLMæ¨ç†æ•ˆç‡ä½ï¼Œå› å…¶åŒå‘æ³¨æ„åŠ›æœºåˆ¶æ— æ³•ç›´æ¥åˆ©ç”¨KVç¼“å­˜ã€‚
2. d$^2$Cacheé€šè¿‡åŒé‡è‡ªé€‚åº”ç¼“å­˜ï¼Œé€‰æ‹©æ€§æ›´æ–°å’Œé‡ç”¨tokençš„KVçŠ¶æ€ï¼ŒåŠ é€Ÿæ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œd$^2$Cacheåœ¨åŠ é€Ÿæ¨ç†çš„åŒæ—¶ï¼Œè¿˜èƒ½æå‡ç”Ÿæˆè´¨é‡ï¼Œå¹¶æä¾›æ›´å¯é çš„è§£ç æ–¹å¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£æ¨¡å‹å¤§å‹è¯­è¨€æ¨¡å‹(dLLMs)è™½ç„¶æ€§èƒ½ä¼˜å¼‚ï¼Œä½†æ¨ç†æ•ˆç‡ä»ç„¶è¾ƒä½ã€‚è¿™æ˜¯å› ä¸ºdLLMsä¾èµ–äºåŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ— æ³•åƒè‡ªå›å½’æ¨¡å‹(ARMs)é‚£æ ·ç›´æ¥å—ç›Šäºæ ‡å‡†çš„é”®å€¼(KV)ç¼“å­˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒé‡è‡ªé€‚åº”ç¼“å­˜(d$^2$Cache)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„è¿‘ä¼¼KVç¼“å­˜æ¡†æ¶ï¼Œç”¨äºåŠ é€ŸdLLMæ¨ç†ã€‚d$^2$Cacheé‡‡ç”¨ä¸¤é˜¶æ®µç»†ç²’åº¦é€‰æ‹©ç­–ç•¥ï¼Œä»¥è¯†åˆ«tokenå¹¶åœ¨æ¯ä¸ªè§£ç æ­¥éª¤è‡ªé€‚åº”åœ°æ›´æ–°å…¶KVçŠ¶æ€ï¼ŒåŒæ—¶ç¼“å­˜å‰©ä½™tokençš„KVçŠ¶æ€ä»¥ä¾›é‡ç”¨ã€‚æ­¤å¤–ï¼Œd$^2$Cacheè‡ªç„¶åœ°æä¾›äº†ä¸€ç§æ›´å¯é çš„è§£ç æ›¿ä»£æ–¹æ¡ˆï¼Œå¯ä»¥å®ç°å‡†ä»å·¦åˆ°å³çš„ç”Ÿæˆï¼Œå¹¶å‡è½»åºåˆ—æœ«å°¾tokençš„è¿‡æ—©è¿‡åº¦è‡ªä¿¡ã€‚åœ¨ä¸¤ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„dLLMï¼ˆå³LLaDAå’ŒDreamï¼‰ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œd$^2$Cacheä¸ä»…å®ç°äº†æ˜¾è‘—çš„æ¨ç†åŠ é€Ÿï¼Œè€Œä¸”åœ¨ç”Ÿæˆè´¨é‡æ–¹é¢ä¹Ÿäº§ç”Ÿäº†ä¸€è‡´çš„æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ‰©æ•£æ¨¡å‹LLMï¼ˆdLLMsï¼‰çš„æ¨ç†æ•ˆç‡ä½ä¸‹æ˜¯ä¸»è¦ç“¶é¢ˆã€‚ä¸è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰ä¸åŒï¼ŒdLLMsä¾èµ–äºåŒå‘æ³¨æ„åŠ›ï¼Œå› æ­¤æ— æ³•ç›´æ¥åˆ©ç”¨æ ‡å‡†çš„KVç¼“å­˜æœºåˆ¶æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚ç°æœ‰çš„æ–¹æ³•éš¾ä»¥åœ¨ä¿è¯ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡dLLMsçš„æ¨ç†é€Ÿåº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šd$^2$Cacheçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¿‘ä¼¼KVç¼“å­˜æ¥åŠ é€ŸdLLMçš„æ¨ç†ã€‚å®ƒä¸æ˜¯ç®€å•åœ°ç¼“å­˜æ‰€æœ‰tokençš„KVçŠ¶æ€ï¼Œè€Œæ˜¯é‡‡ç”¨ä¸€ç§ç»†ç²’åº¦çš„é€‰æ‹©ç­–ç•¥ï¼Œè‡ªé€‚åº”åœ°é€‰æ‹©ä¸€éƒ¨åˆ†tokenè¿›è¡ŒKVçŠ¶æ€çš„æ›´æ–°ï¼ŒåŒæ—¶ç¼“å­˜å…¶ä½™tokençš„KVçŠ¶æ€ä»¥ä¾›åç»­æ­¥éª¤é‡ç”¨ã€‚è¿™ç§é€‰æ‹©æ€§æ›´æ–°å’Œé‡ç”¨èƒ½å¤Ÿåœ¨å‡å°‘è®¡ç®—é‡çš„åŒæ—¶ï¼Œå°½é‡ä¿æŒç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šd$^2$Cacheæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼štokené€‰æ‹©å’ŒKVçŠ¶æ€æ›´æ–°/ç¼“å­˜ã€‚åœ¨tokené€‰æ‹©é˜¶æ®µï¼Œæ¡†æ¶é¦–å…ˆä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„é€‰æ‹©å™¨æ¥è¯„ä¼°æ¯ä¸ªtokençš„é‡è¦æ€§ï¼Œç„¶åæ ¹æ®è¯„ä¼°ç»“æœé€‰æ‹©ä¸€éƒ¨åˆ†tokenè¿›è¡ŒKVçŠ¶æ€çš„æ›´æ–°ã€‚å¯¹äºæœªè¢«é€‰æ‹©çš„tokenï¼Œå…¶KVçŠ¶æ€å°†è¢«ç¼“å­˜èµ·æ¥ï¼Œå¹¶åœ¨åç»­çš„è§£ç æ­¥éª¤ä¸­ç›´æ¥é‡ç”¨ã€‚åœ¨KVçŠ¶æ€æ›´æ–°é˜¶æ®µï¼Œè¢«é€‰æ‹©çš„tokençš„KVçŠ¶æ€ä¼šæ ¹æ®å½“å‰è§£ç æ­¥éª¤çš„è¾“å…¥è¿›è¡Œæ›´æ–°ï¼Œä»¥ä¿è¯å…¶å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šd$^2$Cacheçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŒé‡è‡ªé€‚åº”ç¼“å­˜æœºåˆ¶ã€‚é¦–å…ˆï¼Œtokençš„é€‰æ‹©æ˜¯è‡ªé€‚åº”çš„ï¼Œå³æ ¹æ®æ¯ä¸ªtokençš„é‡è¦æ€§åŠ¨æ€åœ°å†³å®šæ˜¯å¦æ›´æ–°å…¶KVçŠ¶æ€ã€‚å…¶æ¬¡ï¼ŒKVçŠ¶æ€çš„æ›´æ–°ä¹Ÿæ˜¯è‡ªé€‚åº”çš„ï¼Œå³æ ¹æ®å½“å‰è§£ç æ­¥éª¤çš„è¾“å…¥æ¥æ›´æ–°è¢«é€‰æ‹©çš„tokençš„KVçŠ¶æ€ã€‚è¿™ç§åŒé‡è‡ªé€‚åº”æœºåˆ¶èƒ½å¤Ÿåœ¨å‡å°‘è®¡ç®—é‡çš„åŒæ—¶ï¼Œå°½é‡ä¿æŒç”Ÿæˆè´¨é‡ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œd$^2$Cacheä¸æ˜¯ç®€å•åœ°ç¼“å­˜æ‰€æœ‰tokençš„KVçŠ¶æ€ï¼Œè€Œæ˜¯é‡‡ç”¨ä¸€ç§ç»†ç²’åº¦çš„é€‰æ‹©ç­–ç•¥ï¼Œåªæ›´æ–°ä¸€éƒ¨åˆ†tokençš„KVçŠ¶æ€ã€‚

**å…³é”®è®¾è®¡**ï¼šd$^2$Cacheçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è½»é‡çº§tokené€‰æ‹©å™¨çš„è®¾è®¡ï¼Œéœ€è¦é«˜æ•ˆåœ°è¯„ä¼°æ¯ä¸ªtokençš„é‡è¦æ€§ï¼›2) KVçŠ¶æ€æ›´æ–°ç­–ç•¥çš„è®¾è®¡ï¼Œéœ€è¦åœ¨ä¿è¯å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå°½é‡å‡å°‘è®¡ç®—é‡ï¼›3) ç¼“å­˜ç®¡ç†ç­–ç•¥çš„è®¾è®¡ï¼Œéœ€è¦æœ‰æ•ˆåœ°ç®¡ç†ç¼“å­˜ç©ºé—´ï¼Œå¹¶ä¿è¯ç¼“å­˜çš„å‘½ä¸­ç‡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œd$^2$Cacheåœ¨LLaDAå’ŒDreamç­‰dLLMä¸Šå®ç°äº†æ˜¾è‘—çš„æ¨ç†åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡äº†ç”Ÿæˆè´¨é‡ã€‚å…·ä½“è€Œè¨€ï¼Œd$^2$Cacheåœ¨æ¨ç†é€Ÿåº¦ä¸Šå–å¾—äº†é«˜è¾¾Xå€çš„åŠ é€Ÿï¼ˆå…·ä½“æ•°å€¼è¯·å‚è€ƒè®ºæ–‡åŸæ–‡ï¼‰ï¼Œå¹¶ä¸”åœ¨BLEUã€ROUGEç­‰æŒ‡æ ‡ä¸Šå–å¾—äº†Y%çš„æå‡ï¼ˆå…·ä½“æ•°å€¼è¯·å‚è€ƒè®ºæ–‡åŸæ–‡ï¼‰ã€‚è¿™äº›ç»“æœè¯æ˜äº†d$^2$Cacheçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

d$^2$Cacheå¯å¹¿æ³›åº”ç”¨äºå„ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡åŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥é™ä½éƒ¨ç½²æˆæœ¬ï¼Œæé«˜ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¿ƒè¿›dLLMsåœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚è¯¥æŠ€æœ¯è¿˜æœ‰åŠ©äºæ¨åŠ¨AIåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ï¼Œå®ç°æ›´é«˜æ•ˆçš„æœ¬åœ°åŒ–æ¨ç†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache.

