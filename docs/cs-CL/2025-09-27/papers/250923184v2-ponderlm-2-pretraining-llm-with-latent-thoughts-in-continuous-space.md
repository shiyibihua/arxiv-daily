---
layout: default
title: PonderLM-2: Pretraining LLM with Latent Thoughts in Continuous Space
---

# PonderLM-2: Pretraining LLM with Latent Thoughts in Continuous Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23184" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23184v2</a>
  <a href="https://arxiv.org/pdf/2509.23184.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23184v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23184v2', 'PonderLM-2: Pretraining LLM with Latent Thoughts in Continuous Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Boyi Zeng, He Li, Shixiang Song, Yixuan Wang, Ziwei He, Xinbing Wang, Zhouhan Lin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27 (æ›´æ–°: 2025-10-24)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PonderLM-2ï¼šé€šè¿‡åœ¨è¿ç»­ç©ºé—´ä¸­é¢„è®­ç»ƒå…·æœ‰æ½œåœ¨æ€æƒ³çš„LLMï¼Œæå‡å•tokenç”Ÿæˆè´¨é‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ` `æ½œåœ¨æ€æƒ³` `è¿ç»­ç©ºé—´æ¨ç†` `Chain-of-Thought` `Transformer` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Chain-of-Thought (CoT)é€šè¿‡å¢åŠ æ¨ç†æ­¥éª¤æå‡æ€§èƒ½ï¼Œä½†é¢„è®­ç»ƒé˜¶æ®µè®¡ç®—æ­¥éª¤çš„åˆ©ç”¨ä¸è¶³ã€‚
2. PonderLM-2é€šè¿‡åœ¨é¢„è®­ç»ƒé˜¶æ®µå¼•å…¥æ½œåœ¨æ€æƒ³ç”Ÿæˆæ­¥éª¤ï¼Œä½¿æ¨¡å‹åœ¨è¿ç»­ç©ºé—´ä¸­ä¼˜åŒ–tokené¢„æµ‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPonderLM-2åœ¨ç›¸åŒæ¨ç†æˆæœ¬ä¸‹ä¼˜äºæ›´å¤§å‚æ•°é‡çš„æ ‡å‡†æ¨¡å‹ï¼Œä¸”å¢åŠ æ½œåœ¨æ€æƒ³æ•°é‡å¯è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ–¹æ³•ï¼šå…·æœ‰æ½œåœ¨æ€æƒ³çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒï¼ˆPonderLM-2ï¼‰ã€‚è¯¥æ–¹æ³•é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ï¼Œä½¿å…¶é¦–å…ˆç”Ÿæˆä¸€ä¸ªä¸­é—´çš„æ½œåœ¨æ€æƒ³â€”â€”å½“å‰ä½ç½®çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€â€”â€”ç„¶åå°†å…¶ç”¨ä½œè¾“å…¥æ¥é¢„æµ‹å®é™…çš„åç»­tokenã€‚è¿™ç§é¢å¤–çš„è®¡ç®—æ­¥éª¤ä½¿LMèƒ½å¤Ÿåœ¨ä¸å—çº¦æŸçš„è¿ç»­ç©ºé—´ä¸­æ”¹è¿›å…¶é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„æ¨ç†æˆæœ¬ä¸‹ï¼Œæ¯ä¸ªtokenç”Ÿæˆä¸€ä¸ªé¢å¤–æ½œåœ¨æ€æƒ³çš„LMä¼˜äºå‚æ•°é‡æ˜¯å…¶ä¸¤å€çš„æ ‡å‡†æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„PonderLM-2-Pythia-1.4Båœ¨Pileæ•°æ®é›†ä¸Šé¢„è®­ç»ƒäº†300Bä¸ªtokenï¼Œåœ¨è¯­è¨€å»ºæ¨¡å’Œä¸€ç³»åˆ—é€šç”¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåœ¨ç›¸åŒæ•°æ®ä¸Šè®­ç»ƒçš„vanilla Pythia-2.8Bã€‚æ­¤å¤–ï¼Œå¢åŠ åœ¨æ¯ä¸ªå®é™…tokenä¹‹å‰ç”Ÿæˆçš„æ½œåœ¨æ€æƒ³çš„æ•°é‡â€”â€”å½¢æˆç±»ä¼¼äºCoTçš„é“¾â€”â€”å§‹ç»ˆæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•é€šå¸¸ç›´æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œç¼ºä¹å¯¹ä¸­é—´æ¨ç†è¿‡ç¨‹çš„å»ºæ¨¡ã€‚Chain-of-Thought (CoT) è™½ç„¶åœ¨æ¨ç†é˜¶æ®µæœ‰æ•ˆï¼Œä½†é¢„è®­ç»ƒé˜¶æ®µçš„è®¡ç®—æ­¥éª¤åˆ©ç”¨ç‡è¾ƒä½ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒé˜¶æ®µçš„è®¡ç®—èµ„æºï¼Œæå‡æ¨¡å‹ç”Ÿæˆæ¯ä¸ªtokençš„è´¨é‡ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPonderLM-2çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨é¢„è®­ç»ƒé˜¶æ®µæ¨¡æ‹ŸCoTçš„æ¨ç†è¿‡ç¨‹ï¼Œè®©æ¨¡å‹åœ¨é¢„æµ‹æ¯ä¸ªtokenä¹‹å‰ï¼Œå…ˆç”Ÿæˆä¸€ä¸ªâ€œæ½œåœ¨æ€æƒ³â€ï¼ˆlatent thoughtï¼‰ã€‚è¿™ä¸ªæ½œåœ¨æ€æƒ³æ˜¯å½“å‰ä½ç½®çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ï¼Œå¯ä»¥çœ‹ä½œæ˜¯æ¨¡å‹å¯¹å½“å‰ä¸Šä¸‹æ–‡çš„ç†è§£å’Œæ€è€ƒã€‚é€šè¿‡å°†è¿™ä¸ªæ½œåœ¨æ€æƒ³ä½œä¸ºè¾“å…¥ï¼Œæ¨¡å‹å¯ä»¥è¿›ä¸€æ­¥ refine å…¶é¢„æµ‹ï¼Œä»è€Œæé«˜ç”Ÿæˆtokençš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPonderLM-2çš„æ•´ä½“æ¡†æ¶å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œè¾“å…¥tokenåºåˆ—ç»è¿‡æ ‡å‡†çš„Transformerç¼–ç å™¨ï¼Œå¾—åˆ°æ¯ä¸ªä½ç½®çš„éšè—çŠ¶æ€ã€‚ç„¶åï¼Œå¯¹äºæ¯ä¸ªä½ç½®ï¼Œæ¨¡å‹ç”Ÿæˆä¸€ä¸ªæ½œåœ¨æ€æƒ³ï¼Œå³è¯¥ä½ç½®çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚æ¥ä¸‹æ¥ï¼Œå°†è¿™ä¸ªæ½œåœ¨æ€æƒ³è¾“å…¥åˆ°ä¸€ä¸ªé¢å¤–çš„çº¿æ€§å±‚æˆ–å°å‹ç¥ç»ç½‘ç»œä¸­ï¼Œå¾—åˆ°ä¸€ä¸ª refined çš„è¡¨ç¤ºã€‚æœ€åï¼Œä½¿ç”¨è¿™ä¸ª refined çš„è¡¨ç¤ºæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥é‡å¤å¤šæ¬¡ï¼Œå½¢æˆä¸€ä¸ªç±»ä¼¼äºCoTçš„é“¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šPonderLM-2æœ€é‡è¦çš„åˆ›æ–°ç‚¹æ˜¯åœ¨é¢„è®­ç»ƒé˜¶æ®µå¼•å…¥äº†æ½œåœ¨æ€æƒ³ç”Ÿæˆæ­¥éª¤ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è¿ç»­ç©ºé—´ä¸­è¿›è¡Œæ¨ç†å’Œä¼˜åŒ–ã€‚ä¸ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒPonderLM-2 èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œæé«˜ç”Ÿæˆtokençš„è´¨é‡ã€‚æ­¤å¤–ï¼ŒPonderLM-2 è¿˜èƒ½å¤Ÿé€šè¿‡å¢åŠ æ½œåœ¨æ€æƒ³çš„æ•°é‡ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œè¿™ä¸ CoT çš„æ€æƒ³æ˜¯ä¸€è‡´çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šPonderLM-2çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ½œåœ¨æ€æƒ³çš„è¡¨ç¤ºå½¢å¼ï¼šè®ºæ–‡ä¸­ä½¿ç”¨æœ€åä¸€ä¸ªéšè—çŠ¶æ€ä½œä¸ºæ½œåœ¨æ€æƒ³çš„è¡¨ç¤ºï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚2) Refinementæ¨¡å—ï¼šè®ºæ–‡ä¸­ä½¿ç”¨çº¿æ€§å±‚æˆ–å°å‹ç¥ç»ç½‘ç»œæ¥ refine æ½œåœ¨æ€æƒ³ï¼Œè¿™å¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚3) æŸå¤±å‡½æ•°ï¼šè®ºæ–‡ä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥è€ƒè™‘å¼•å…¥å…¶ä»–çš„æ­£åˆ™åŒ–é¡¹ï¼Œä¾‹å¦‚é¼“åŠ±æ½œåœ¨æ€æƒ³çš„å¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PonderLM-2-Pythia-1.4Båœ¨Pileæ•°æ®é›†ä¸Šé¢„è®­ç»ƒäº†300Bä¸ªtokenï¼Œåœ¨è¯­è¨€å»ºæ¨¡å’Œä¸€ç³»åˆ—é€šç”¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåœ¨ç›¸åŒæ•°æ®ä¸Šè®­ç»ƒçš„vanilla Pythia-2.8Bã€‚è¿™æ„å‘³ç€åœ¨ç›¸åŒçš„æ¨ç†æˆæœ¬ä¸‹ï¼ŒPonderLM-2èƒ½å¤Ÿè¾¾åˆ°æ›´é«˜çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å¢åŠ æ½œåœ¨æ€æƒ³çš„æ•°é‡è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒPonderLM-2çš„æ€§èƒ½æå‡è¶…è¿‡äº†10%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PonderLM-2å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼ŒPonderLM-2å¯ä»¥æå‡è¿™äº›ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒPonderLM-2è¿˜å¯ä»¥ç”¨äºå¼€å‘æ›´æ™ºèƒ½çš„å¯¹è¯ç³»ç»Ÿå’Œè™šæ‹ŸåŠ©æ‰‹ï¼Œä¸ºç”¨æˆ·æä¾›æ›´è‡ªç„¶ã€æ›´æµç•…çš„äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼ŒPonderLM-2æœ‰æœ›æˆä¸ºä¸‹ä¸€ä»£è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ ‡å‡†æ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The remarkable success of Chain-of-Thought (CoT), which enhances performance by scaling generation steps at test-time, inspires us to ask: can we leverage a similar scaling of computational steps during pretraining to improve the generation of each individual token? To address this, we propose a novel pre-training methodology: Pretraining Language Models with Latent Thoughts (PonderLM-2). Our approach pretrains a language model (LM) to first generate an intermediate latent thought-the last hidden state of the current position-which is then used as input to predict the actual subsequent token. This additional computational step enables the LM to refine its prediction within unconstrained continuous space. Our experiments demonstrate that, at an identical inference cost, a LM that generates one additional latent thought per token outperforms a standard model with double the parameters. For instance, our PonderLM-2-Pythia-1.4B, pretrained on 300B tokens from the Pile, significantly surpasses the vanilla Pythia-2.8B trained on the same data on both language modeling and a range of general downstream tasks. Furthermore, increasing the number of latent thoughts generated before each actual token-forming a chain analogous to CoT-consistently improves the model's performance.

