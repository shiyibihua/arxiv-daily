---
layout: default
title: Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization
---

# Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23371" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23371v1</a>
  <a href="https://arxiv.org/pdf/2509.23371.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23371v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23371v1', 'Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junming Yang, Ning Xu, Biao Liu, Shiqi Qiao, Xin Geng

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMetaAPOï¼Œé€šè¿‡å…ƒåŠ æƒåœ¨çº¿é‡‡æ ·å¼¥åˆæ•°æ®ç”Ÿæˆä¸åå¥½ä¼˜åŒ–ä¹‹é—´çš„å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åå¥½ä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½` `å…ƒå­¦ä¹ ` `åœ¨çº¿é‡‡æ ·` `æ•°æ®åˆ†å¸ƒåŒ¹é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åå¥½ä¼˜åŒ–æ–¹æ³•éš¾ä»¥é€‚åº”æ¨¡å‹åŠ¨æ€å­¦ä¹ çŠ¶æ€ï¼Œå¯¼è‡´ç¦»çº¿æ•°æ®ä¸åœ¨çº¿ç­–ç•¥ä¹‹é—´å­˜åœ¨åˆ†å¸ƒä¸åŒ¹é…ã€‚
2. MetaAPOé€šè¿‡å¼•å…¥å…ƒå­¦ä¹ å™¨åŠ¨æ€è¯„ä¼°åœ¨çº¿é‡‡æ ·çš„æ”¶ç›Šï¼ŒæŒ‡å¯¼æ•°æ®ç”Ÿæˆå¹¶åˆ†é…æ ·æœ¬æƒé‡ï¼Œå¹³è¡¡æ•°æ®è´¨é‡ä¸åˆ†å¸ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMetaAPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—é™ä½äº†åœ¨çº¿æ ‡æ³¨æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åå¥½ä¼˜åŒ–å¯¹äºä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œæ„å›¾å¯¹é½è‡³å…³é‡è¦ã€‚æ­¤è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜æ˜¯é¢„å…ˆæ”¶é›†çš„ç¦»çº¿åå¥½æ•°æ®ä¸ä¸æ–­æ¼”å˜çš„æ¨¡å‹ç­–ç•¥ä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾ä½¿ç”¨é™æ€å¯å‘å¼æ–¹æ³•æˆ–è§£è€¦çš„åœ¨çº¿é‡‡æ ·ç­–ç•¥æ¥ç¼©å°è¿™ç§å·®è·ï¼Œä½†å®ƒä»¬é€šå¸¸æ— æ³•é€‚åº”æ¨¡å‹çš„åŠ¨æ€å­¦ä¹ çŠ¶æ€ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶MetaåŠ æƒè‡ªé€‚åº”åå¥½ä¼˜åŒ–ï¼ˆMetaAPOï¼‰ï¼Œè¯¥æ¡†æ¶åŠ¨æ€åœ°å°†æ•°æ®ç”Ÿæˆä¸æ¨¡å‹è®­ç»ƒç›¸ç»“åˆã€‚MetaAPOé‡‡ç”¨è½»é‡çº§å…ƒå­¦ä¹ å™¨ï¼Œä½œä¸ºâ€œå¯¹é½å·®è·ä¼°è®¡å™¨â€ï¼Œä»¥è¯„ä¼°åœ¨çº¿ç­–ç•¥é‡‡æ ·ç›¸å¯¹äºç¦»çº¿æ•°æ®çš„æ½œåœ¨ç›Šå¤„ã€‚è¿™æŒ‡å¯¼æœ‰é’ˆå¯¹æ€§çš„åœ¨çº¿ç”Ÿæˆï¼Œå¹¶å°†æ ·æœ¬çº§å…ƒæƒé‡åˆ†é…ç»™ä¼˜åŒ–ç›®æ ‡ï¼Œä»è€ŒåŠ¨æ€å¹³è¡¡åœ¨çº¿å’Œç¦»çº¿æ•°æ®çš„è´¨é‡å’Œåˆ†å¸ƒã€‚åœ¨AlpacaEval 2ã€Arena-Hardå’ŒMT-Benchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMetaAPOåœ¨å„ç§è®¾ç½®ä¸‹å§‹ç»ˆä¼˜äºç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒæ—¶é™ä½äº†42%çš„åœ¨çº¿æ ‡æ³¨æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åå¥½ä¼˜åŒ–æ–¹æ³•åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œé¢ä¸´ç¦»çº¿åå¥½æ•°æ®ä¸æ¨¡å‹ç­–ç•¥æ¼”è¿›ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚é—®é¢˜ã€‚é™æ€å¯å‘å¼æˆ–è§£è€¦çš„åœ¨çº¿é‡‡æ ·ç­–ç•¥æ— æ³•æœ‰æ•ˆé€‚åº”æ¨¡å‹å­¦ä¹ çŠ¶æ€ï¼Œå¯¼è‡´å¯¹é½æ•ˆæœå—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMetaAPOçš„æ ¸å¿ƒåœ¨äºåŠ¨æ€è€¦åˆæ•°æ®ç”Ÿæˆä¸æ¨¡å‹è®­ç»ƒï¼Œé€šè¿‡å…ƒå­¦ä¹ å™¨ä¼°è®¡åœ¨çº¿é‡‡æ ·çš„ä»·å€¼ï¼Œä»è€ŒæŒ‡å¯¼æ•°æ®ç”Ÿæˆå¹¶åŠ¨æ€è°ƒæ•´åœ¨çº¿å’Œç¦»çº¿æ•°æ®çš„æƒé‡ã€‚è¿™ç§è‡ªé€‚åº”æ–¹æ³•æ—¨åœ¨å¼¥åˆæ•°æ®åˆ†å¸ƒå·®å¼‚ï¼Œæå‡å¯¹é½æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMetaAPOæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) åå¥½æ•°æ®æ”¶é›†æ¨¡å—ï¼ŒåŒ…æ‹¬ç¦»çº¿æ•°æ®å’Œåœ¨çº¿é‡‡æ ·æ•°æ®ï¼›2) å…ƒå­¦ä¹ å™¨ï¼Œç”¨äºä¼°è®¡åœ¨çº¿é‡‡æ ·çš„æ”¶ç›Šï¼›3) æƒé‡åˆ†é…æ¨¡å—ï¼Œæ ¹æ®å…ƒå­¦ä¹ å™¨çš„è¾“å‡ºï¼Œä¸ºåœ¨çº¿å’Œç¦»çº¿æ•°æ®åˆ†é…æ ·æœ¬çº§æƒé‡ï¼›4) åå¥½ä¼˜åŒ–æ¨¡å—ï¼Œä½¿ç”¨åŠ æƒåçš„æ•°æ®è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå…ƒå­¦ä¹ å™¨åŠ¨æ€è¯„ä¼°åœ¨çº¿é‡‡æ ·çš„ä»·å€¼ï¼ŒæŒ‡å¯¼åœ¨çº¿æ•°æ®ç”Ÿæˆï¼Œå¹¶è°ƒæ•´ä¼˜åŒ–ç›®æ ‡ä¸­åœ¨çº¿å’Œç¦»çº¿æ•°æ®çš„æƒé‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šMetaAPOçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†å…ƒå­¦ä¹ å™¨ä½œä¸ºâ€œå¯¹é½å·®è·ä¼°è®¡å™¨â€ï¼ŒåŠ¨æ€è¯„ä¼°åœ¨çº¿é‡‡æ ·çš„ä»·å€¼ã€‚ä¸ç°æœ‰æ–¹æ³•é‡‡ç”¨é™æ€ç­–ç•¥æˆ–è§£è€¦æ–¹å¼ä¸åŒï¼ŒMetaAPOå®ç°äº†æ•°æ®ç”Ÿæˆä¸æ¨¡å‹è®­ç»ƒçš„åŠ¨æ€è€¦åˆï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”æ¨¡å‹å­¦ä¹ çŠ¶æ€ã€‚

**å…³é”®è®¾è®¡**ï¼šMetaå­¦ä¹ å™¨æ˜¯ä¸€ä¸ªè½»é‡çº§ç½‘ç»œï¼Œè¾“å…¥æ˜¯å½“å‰æ¨¡å‹çš„ç­–ç•¥å’Œç¦»çº¿æ•°æ®ï¼Œè¾“å‡ºæ˜¯åœ¨çº¿é‡‡æ ·çš„é¢„æœŸæ”¶ç›Šã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸ºæœ€å¤§åŒ–åœ¨çº¿é‡‡æ ·çš„æ”¶ç›Šï¼ŒåŒæ—¶æœ€å°åŒ–ä¸ç¦»çº¿æ•°æ®çš„å·®å¼‚ã€‚æ ·æœ¬çº§æƒé‡æ ¹æ®å…ƒå­¦ä¹ å™¨çš„è¾“å‡ºè¿›è¡Œè°ƒæ•´ï¼Œé«˜æ”¶ç›Šçš„åœ¨çº¿æ ·æœ¬è·å¾—æ›´é«˜çš„æƒé‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMetaAPOåœ¨AlpacaEval 2ã€Arena-Hardå’ŒMT-Benchç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå§‹ç»ˆä¼˜äºç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒMetaAPOåœ¨æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œè¿˜é™ä½äº†42%çš„åœ¨çº¿æ ‡æ³¨æˆæœ¬ï¼Œè¡¨æ˜å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œç»æµæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MetaAPOå¯åº”ç”¨äºå„ç§éœ€è¦å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å’Œæ„å›¾çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€å†…å®¹ç”Ÿæˆã€æ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡é™ä½åœ¨çº¿æ ‡æ³¨æˆæœ¬å¹¶æå‡å¯¹é½æ•ˆæœï¼ŒMetaAPOæœ‰åŠ©äºå¼€å‘æ›´å®‰å…¨ã€æ›´å¯é ã€æ›´ç¬¦åˆäººç±»æœŸæœ›çš„AIç³»ç»Ÿã€‚è¯¥æ–¹æ³•ä¹Ÿä¸ºå…¶ä»–å¼ºåŒ–å­¦ä¹ å¯¹é½ä»»åŠ¡æä¾›äº†å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. A significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the model's dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework that dynamically couples data generation with model training. MetaAPO employs a lightweight meta-learner, as an "alignment gap estimator", to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs.

