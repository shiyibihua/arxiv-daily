---
layout: default
title: Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning
---

# Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23140" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23140v1</a>
  <a href="https://arxiv.org/pdf/2509.23140.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23140v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23140v1', 'Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Song Jin, Juntian Zhang, Yong Liu, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei Lin, Rui Yan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TagPRï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ€ç»´æ ‡æ³¨æå‡LLMçš„ä¸ªæ€§åŒ–æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ€ç»´æ ‡æ³¨` `ç”¨æˆ·åå¥½` `å¥–åŠ±æ¨¡å‹` `ç›‘ç£å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨é€šç”¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åˆ†æç”¨æˆ·å†å²ã€æ¨æ–­åå¥½å¹¶ç”Ÿæˆå®šåˆ¶åŒ–å“åº”çš„ä¸ªæ€§åŒ–æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. TagPRæ¡†æ¶é€šè¿‡æ•°æ®é©±åŠ¨çš„æ¨ç†é“¾ç”Ÿæˆä¸æ ‡æ³¨ï¼Œä»¥åŠç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„ååŒè®­ç»ƒç­–ç•¥ï¼Œæå‡LLMçš„ä¸ªæ€§åŒ–æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTagPRåœ¨LaMPåŸºå‡†å’Œè‡ªå»ºæ•°æ®é›†ä¸Šå‡å–å¾—äº†SOTAç»“æœï¼Œç›¸æ¯”åŸºçº¿æ¨¡å‹å¹³å‡æå‡äº†32.65%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTagPRçš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸ªæ€§åŒ–æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡â€œæ ‡æ³¨æ€ç»´â€çš„æ–¹æ³•ï¼Œé¦–å…ˆå¼€å‘ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„æµç¨‹ï¼Œè‡ªåŠ¨ç”Ÿæˆå¹¶è¯­ä¹‰æ ‡æ³¨æ¨ç†é“¾ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªç»“æ„åŒ–çš„æ•°æ®é›†ï¼Œä¿ƒè¿›å¯è§£é‡Šçš„æ¨ç†ã€‚ç„¶åï¼Œæå‡ºä¸€ç§ååŒè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆåœ¨æ­¤æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä»¥å»ºç«‹åŸºæœ¬çš„æ¨ç†æ¨¡å¼ï¼Œç„¶åè¿›è¡Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿‡ç¨‹ã€‚æ­¤RLé˜¶æ®µç”±ç‹¬ç‰¹çš„å¤åˆå¥–åŠ±ä¿¡å·å¼•å¯¼ï¼Œè¯¥ä¿¡å·é›†æˆäº†åŸºäºæ ‡ç­¾çš„çº¦æŸå’Œä¸€ä¸ªæ–°é¢–çš„å¸¦æœ‰ç”¨æˆ·åµŒå…¥çš„ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹ï¼ˆPRMUï¼‰ï¼Œä»¥å®ç°ä¸ç”¨æˆ·ç‰¹å®šé€»è¾‘çš„ç»†ç²’åº¦å¯¹é½ã€‚åœ¨å…¬å…±LaMPåŸºå‡†å’Œä¸€ä¸ªè‡ªæ„å»ºæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ï¼Œç›¸å¯¹äºåŸºç¡€æ¨¡å‹å¹³å‡æé«˜äº†32.65%ã€‚è¿™é¡¹å·¥ä½œéªŒè¯äº†ç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¨ç†æ˜¯é‡Šæ”¾LLMä¸­çœŸæ­£ä¸ªæ€§åŒ–èƒ½åŠ›çš„ä¸€ç§éå¸¸æœ‰æ•ˆçš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸ªæ€§åŒ–æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰LLMè™½ç„¶å…·å¤‡å¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨ç†è§£ç”¨æˆ·ç‰¹å®šåå¥½ã€å¹¶æ ¹æ®è¿™äº›åå¥½ç”Ÿæˆå®šåˆ¶åŒ–å“åº”æ–¹é¢è¡¨ç°æ¬ ä½³ã€‚ç°æœ‰çš„æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ç”¨æˆ·å†å²ä¿¡æ¯ï¼Œç¼ºä¹å¯¹æ¨ç†è¿‡ç¨‹çš„ç»†ç²’åº¦æ§åˆ¶å’Œå¯è§£é‡Šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡â€œæ ‡æ³¨æ€ç»´â€ï¼ˆTagging the Thoughtï¼‰çš„æ–¹å¼ï¼Œæ˜¾å¼åœ°å»ºæ¨¡å’ŒæŒ‡å¯¼LLMçš„æ¨ç†è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆæ„å»ºç»“æ„åŒ–çš„æ¨ç†é“¾æ•°æ®ï¼Œå¹¶å¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤è¿›è¡Œè¯­ä¹‰æ ‡æ³¨ï¼Œç„¶ååˆ©ç”¨è¿™äº›æ ‡æ³¨ä¿¡æ¯æ¥è®­ç»ƒLLMï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨ç”¨æˆ·åå¥½ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–LLMçš„æ¨ç†ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„ä¸ªæ€§åŒ–å“åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTagPRæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **æ•°æ®ç”Ÿæˆä¸æ ‡æ³¨**ï¼šåˆ©ç”¨æ•°æ®é©±åŠ¨çš„æµç¨‹è‡ªåŠ¨ç”Ÿæˆæ¨ç†é“¾ï¼Œå¹¶è¿›è¡Œè¯­ä¹‰æ ‡æ³¨ã€‚2) **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼šåœ¨æ ‡æ³¨æ•°æ®ä¸Šå¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶å­¦ä¹ åŸºæœ¬çš„æ¨ç†æ¨¡å¼ã€‚3) **å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**ï¼šåˆ©ç”¨å¤åˆå¥–åŠ±ä¿¡å·ï¼Œå¯¹LLMè¿›è¡Œè¿›ä¸€æ­¥çš„ä¼˜åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç”¨æˆ·ç‰¹å®šåå¥½ã€‚å¤åˆå¥–åŠ±ä¿¡å·åŒ…æ‹¬åŸºäºæ ‡ç­¾çš„çº¦æŸå’Œä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹ï¼ˆPRMUï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **â€œæ ‡æ³¨æ€ç»´â€æ–¹æ³•**ï¼šé€šè¿‡æ˜¾å¼åœ°æ ‡æ³¨æ¨ç†é“¾ï¼Œæé«˜äº†LLMæ¨ç†è¿‡ç¨‹çš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚2) **å¤åˆå¥–åŠ±ä¿¡å·**ï¼šç»“åˆäº†åŸºäºæ ‡ç­¾çš„çº¦æŸå’Œä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹ï¼Œå®ç°äº†å¯¹LLMæ¨ç†ç­–ç•¥çš„ç»†ç²’åº¦ä¼˜åŒ–ã€‚3) **ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹ï¼ˆPRMUï¼‰**ï¼šåˆ©ç”¨ç”¨æˆ·åµŒå…¥æ¥å»ºæ¨¡ç”¨æˆ·ç‰¹å®šåå¥½ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°LLMç”Ÿæˆçš„å“åº”çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªå¤åˆå¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šåŸºäºæ ‡ç­¾çš„çº¦æŸå’Œä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹ï¼ˆPRMUï¼‰ã€‚åŸºäºæ ‡ç­¾çš„çº¦æŸç”¨äºç¡®ä¿LLMç”Ÿæˆçš„æ¨ç†é“¾ç¬¦åˆé¢„å®šä¹‰çš„è¯­ä¹‰è§„åˆ™ã€‚PRMUåˆ™åˆ©ç”¨ç”¨æˆ·åµŒå…¥æ¥é¢„æµ‹ç”¨æˆ·å¯¹LLMç”Ÿæˆçš„å“åº”çš„åå¥½ã€‚ç”¨æˆ·åµŒå…¥å¯ä»¥é€šè¿‡ç”¨æˆ·å†å²è¡Œä¸ºæ•°æ®å­¦ä¹ å¾—åˆ°ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨æœ€å¤§åŒ–å¤åˆå¥–åŠ±ï¼Œä»è€Œä¼˜åŒ–LLMçš„æ¨ç†ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTagPRåœ¨LaMPåŸºå‡†æµ‹è¯•å’Œè‡ªå»ºæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ï¼ŒTagPRç›¸å¯¹äºåŸºçº¿æ¨¡å‹å¹³å‡æé«˜äº†32.65%ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒéªŒè¯äº†å„ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚ï¼ŒåŸºäºæ ‡ç­¾çš„çº¦æŸå’Œä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹ï¼ˆPRMUï¼‰éƒ½å¯¹æœ€ç»ˆæ€§èƒ½æœ‰é‡è¦è´¡çŒ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä¸ªæ€§åŒ–æ¨èç³»ç»Ÿã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€ä»¥åŠå…¶ä»–éœ€è¦ç†è§£ç”¨æˆ·åå¥½å¹¶ç”Ÿæˆå®šåˆ¶åŒ–å†…å®¹çš„åœºæ™¯ã€‚é€šè¿‡æå‡LLMçš„ä¸ªæ€§åŒ–æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæé«˜ç³»ç»Ÿçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements have endowed Large Language Models (LLMs) with impressive general reasoning capabilities, yet they often struggle with personalization reasoning - the crucial ability to analyze user history, infer unique preferences, and generate tailored responses. To address this limitation, we introduce TagPR, a novel training framework that significantly enhances an LLM's intrinsic capacity for personalization reasoning through a tagging the thought approach. Our method first develops a data-driven pipeline to automatically generate and semantically label reasoning chains, creating a structured dataset that fosters interpretable reasoning. We then propose a synergistic training strategy that begins with Supervised Fine-Tuning (SFT) on this tagged data to establish foundational reasoning patterns, followed by a multi-stage reinforcement learning (RL) process. This RL phase is guided by a unique composite reward signal, which integrates tag-based constraints and a novel Personalization Reward Model with User Embeddings (PRMU) to achieve fine-grained alignment with user-specific logic. Extensive experiments on the public LaMP benchmark and a self-constructed dataset demonstrate that our approach achieves state-of-the-art results, delivering an average improvement of 32.65% over the base model across all tasks. Our work validates that structured, interpretable reasoning is a highly effective pathway to unlocking genuine personalization capabilities in LLMs.

