---
layout: default
title: A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks
---

# A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23208" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23208v1</a>
  <a href="https://arxiv.org/pdf/2509.23208.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23208v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23208v1', 'A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haorui Yu, Ramon Ruiz-Dolz, Qiufeng Yi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: EMNLP 2025 submission, 10 pages, 6 figures, 5 tables

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yha9806/VULCA-EMNLP2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“æ„åŒ–æ¡†æ¶ï¼Œè¯„ä¼°å¹¶æå‡å¤šæ¨¡æ€LLMåœ¨ä¸­å›½æ–‡åŒ–æƒ…å¢ƒä¸‹çš„ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ–‡åŒ–æƒ…å¢ƒç†è§£` `è‰ºæœ¯è¯„è®ºç”Ÿæˆ` `é›¶æ ·æœ¬åˆ†ç±»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMåœ¨æ–‡åŒ–æƒ…å¢ƒä¸‹çš„ç†è§£èƒ½åŠ›ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è‰ºæœ¯è¯„è®ºç­‰éœ€è¦æ·±åº¦è¯­ä¹‰ç†è§£çš„ä»»åŠ¡ä¸­ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§ç»“æ„åŒ–æ¡†æ¶ï¼Œé€šè¿‡é‡åŒ–ä¸“å®¶è¯„è®ºç‰¹å¾ï¼Œå®šä¹‰è¯„è®ºè§’è‰²ï¼Œå¼•å¯¼VLMç”Ÿæˆå¤šè§’åº¦è¯„è®ºã€‚
3. å®éªŒè¯„ä¼°äº†Llamaã€Qwenã€Geminiç­‰VLMåœ¨ç”Ÿæˆä¸­å›½ç»˜ç”»è¯„è®ºæ–¹é¢çš„è¡¨ç°ï¼Œæ­ç¤ºäº†å…¶ä¼˜åŠ¿ä¸ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ—¨åœ¨æµ‹è¯•å’Œè¯„ä¼°å½“å‰ä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç”Ÿæˆä¸­å›½ä¼ ç»Ÿç»˜ç”»è¯„è®ºæ–¹é¢çš„èƒ½åŠ›å’Œç‰¹ç‚¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªç”¨äºä¸­å›½ç»˜ç”»è¯„è®ºçš„é‡åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä½¿ç”¨é›¶æ ·æœ¬åˆ†ç±»æ¨¡å‹ï¼Œä»äººç±»ä¸“å®¶è¯„è®ºä¸­æå–å¤šç»´è¯„ä¼°ç‰¹å¾æ„å»ºè€Œæˆï¼Œè¿™äº›ç‰¹å¾æ¶µç›–è¯„ä¼°ç«‹åœºã€ç‰¹å¾å…³æ³¨ç‚¹å’Œè¯„è®ºè´¨é‡ã€‚åŸºäºè¿™äº›ç‰¹å¾ï¼Œå®šä¹‰å¹¶é‡åŒ–äº†å‡ ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„è¯„è®ºè§’è‰²ã€‚ç„¶åï¼Œè¯¥æ¡†æ¶è¢«ç”¨äºè¯„ä¼°é€‰å®šçš„VLMï¼Œå¦‚Llamaã€Qwenæˆ–Geminiã€‚å®éªŒè®¾è®¡åŒ…æ‹¬è§’è‰²å¼•å¯¼æç¤ºï¼Œä»¥è¯„ä¼°VLMä»ä¸åŒè§’åº¦ç”Ÿæˆè¯„è®ºçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†VLMåœ¨è‰ºæœ¯è¯„è®ºé¢†åŸŸçš„å½“å‰æ€§èƒ½æ°´å¹³ã€ä¼˜åŠ¿å’Œæ”¹è¿›é¢†åŸŸï¼Œä»è€Œæ·±å…¥äº†è§£å®ƒä»¬åœ¨å¤æ‚è¯­ä¹‰ç†è§£å’Œå†…å®¹ç”Ÿæˆä»»åŠ¡ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚å®éªŒä»£ç å¯åœ¨https://github.com/yha9806/VULCA-EMNLP2025å…¬å¼€è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç†è§£å’Œç”Ÿæˆå…·æœ‰æ–‡åŒ–èƒŒæ™¯çš„è¯„è®ºæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­å›½ä¼ ç»Ÿç»˜ç”»ç­‰é¢†åŸŸã€‚ç°æœ‰çš„æ–¹æ³•éš¾ä»¥æ•æ‰ä¸“å®¶è¯„è®ºçš„å¤šç»´åº¦ç‰¹å¾ï¼Œæ— æ³•æœ‰æ•ˆå¼•å¯¼VLMç”Ÿæˆé«˜è´¨é‡ã€å¤šè§†è§’çš„è¯„è®ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªé‡åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»ä¸“å®¶è¯„è®ºä¸­æå–å…³é”®ç‰¹å¾ï¼Œå¹¶å®šä¹‰ä¸åŒçš„è¯„è®ºè§’è‰²ã€‚é€šè¿‡è§’è‰²å¼•å¯¼æç¤ºï¼Œå¯ä»¥è¯„ä¼°VLMåœ¨ä¸åŒè§†è§’ä¸‹çš„è¯„è®ºç”Ÿæˆèƒ½åŠ›ï¼Œä»è€Œæ­ç¤ºå…¶ä¼˜åŠ¿å’Œä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **ä¸“å®¶è¯„è®ºæ•°æ®æ”¶é›†**ï¼šæ”¶é›†ä¸­å›½ç»˜ç”»ä¸“å®¶çš„è¯„è®ºæ•°æ®ã€‚2) **ç‰¹å¾æå–**ï¼šä½¿ç”¨é›¶æ ·æœ¬åˆ†ç±»æ¨¡å‹ä»ä¸“å®¶è¯„è®ºä¸­æå–å¤šç»´è¯„ä¼°ç‰¹å¾ï¼ŒåŒ…æ‹¬è¯„ä¼°ç«‹åœºã€ç‰¹å¾å…³æ³¨ç‚¹å’Œè¯„è®ºè´¨é‡ã€‚3) **è¯„è®ºè§’è‰²å®šä¹‰**ï¼šåŸºäºæå–çš„ç‰¹å¾ï¼Œå®šä¹‰å¹¶é‡åŒ–å…·æœ‰ä»£è¡¨æ€§çš„è¯„è®ºè§’è‰²ã€‚4) **VLMè¯„ä¼°**ï¼šä½¿ç”¨è§’è‰²å¼•å¯¼æç¤ºï¼Œè¯„ä¼°é€‰å®šçš„VLMï¼ˆå¦‚Llamaã€Qwenã€Geminiï¼‰ç”Ÿæˆè¯„è®ºçš„èƒ½åŠ›ã€‚5) **ç»“æœåˆ†æ**ï¼šåˆ†æVLMç”Ÿæˆçš„è¯„è®ºï¼Œè¯„ä¼°å…¶æ€§èƒ½æ°´å¹³ã€ä¼˜åŠ¿å’Œæ”¹è¿›é¢†åŸŸã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„é‡åŒ–æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œæå‡VLMåœ¨æ–‡åŒ–æƒ…å¢ƒä¸‹çš„ç†è§£èƒ½åŠ›ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä»ä¸“å®¶è¯„è®ºä¸­æå–å¤šç»´ç‰¹å¾ï¼Œå¹¶å®šä¹‰è¯„è®ºè§’è‰²ï¼Œä»è€Œä¸ºVLMæä¾›æ›´æ˜ç¡®çš„æŒ‡å¯¼ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´å…·æ·±åº¦å’Œå¹¿åº¦çš„è¯„è®ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç‰¹å¾æå–é˜¶æ®µï¼Œä½¿ç”¨äº†é›¶æ ·æœ¬åˆ†ç±»æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç›´æ¥å¯¹è¯„è®ºæ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®ã€‚åœ¨è§’è‰²å¼•å¯¼æç¤ºé˜¶æ®µï¼Œè®¾è®¡äº†ä¸åŒçš„æç¤ºè¯­ï¼Œä»¥å¼•å¯¼VLMä»ä¸åŒçš„è§’åº¦ç”Ÿæˆè¯„è®ºã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°VLMåœ¨ç”Ÿæˆä¸­å›½ç»˜ç”»è¯„è®ºæ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡è§’è‰²å¼•å¯¼æç¤ºï¼ŒVLMèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸åŒè§†è§’çš„è¯„è®ºï¼Œä½†ä»å­˜åœ¨æ”¹è¿›ç©ºé—´ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦è¯­ä¹‰ç†è§£å’Œæ–‡åŒ–èƒŒæ™¯çŸ¥è¯†æ–¹é¢ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨æ‘˜è¦ä¸­æœªæåŠï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‰ºæœ¯è¯„è®ºç”Ÿæˆã€æ–‡åŒ–é—äº§ä¿æŠ¤ã€è·¨æ–‡åŒ–äº¤æµç­‰é¢†åŸŸã€‚é€šè¿‡æå‡VLMåœ¨æ–‡åŒ–æƒ…å¢ƒä¸‹çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥å¸®åŠ©äººä»¬æ›´å¥½åœ°ç†è§£å’Œæ¬£èµä¸åŒæ–‡åŒ–çš„è‰ºæœ¯ä½œå“ï¼Œä¿ƒè¿›æ–‡åŒ–äº¤æµå’Œç†è§£ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–æ–‡åŒ–é¢†åŸŸï¼Œä¾‹å¦‚æ–‡å­¦ã€éŸ³ä¹ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This study aims to test and evaluate the capabilities and characteristics of current mainstream Visual Language Models (VLMs) in generating critiques for traditional Chinese painting. To achieve this, we first developed a quantitative framework for Chinese painting critique. This framework was constructed by extracting multi-dimensional evaluative features covering evaluative stance, feature focus, and commentary quality from human expert critiques using a zero-shot classification model. Based on these features, several representative critic personas were defined and quantified. This framework was then employed to evaluate selected VLMs such as Llama, Qwen, or Gemini. The experimental design involved persona-guided prompting to assess the VLM's ability to generate critiques from diverse perspectives. Our findings reveal the current performance levels, strengths, and areas for improvement of VLMs in the domain of art critique, offering insights into their potential and limitations in complex semantic understanding and content generation tasks. The code used for our experiments can be publicly accessed at: https://github.com/yha9806/VULCA-EMNLP2025.

