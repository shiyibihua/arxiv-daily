---
layout: default
title: Dual-Space Smoothness for Robust and Balanced LLM Unlearning
---

# Dual-Space Smoothness for Robust and Balanced LLM Unlearning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23362" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23362v1</a>
  <a href="https://arxiv.org/pdf/2509.23362.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23362v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23362v1', 'Dual-Space Smoothness for Robust and Balanced LLM Unlearning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Han Yan, Zheyuan Liu, Meng Jiang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: A unified framework that enforces dual-space smoothness in representation and parameter spaces to improve robustness and balance unlearning metrics

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PRISMï¼šé€šè¿‡åŒç©ºé—´å¹³æ»‘å®ç°é²æ£’ä¸”å‡è¡¡çš„LLMä¸å¯å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨ä¸å¯å­¦ä¹ ` `éšç§ä¿æŠ¤` `é²æ£’æ€§` `åŒç©ºé—´å¹³æ»‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMä¸å¯å­¦ä¹ æ–¹æ³•é¢ä¸´ç¾éš¾æ€§é—å¿˜å’ŒæŒ‡æ ‡ä¸å¹³è¡¡é—®é¢˜ï¼Œéš¾ä»¥å…¼é¡¾æœ‰æ•ˆæ€§ã€æ•ˆç”¨ä¿æŒå’Œéšç§ä¿æŠ¤ã€‚
2. PRISMæ¡†æ¶é€šè¿‡åœ¨è¡¨ç¤ºç©ºé—´å’Œå‚æ•°ç©ºé—´ä¸­å®æ–½åŒç©ºé—´å¹³æ»‘ï¼Œæé«˜æ¨¡å‹é²æ£’æ€§ï¼Œå¹³è¡¡ä¸å¯å­¦ä¹ çš„å„é¡¹æŒ‡æ ‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPRISMåœ¨å¤šç§æ”»å‡»ä¸‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å…³é”®æŒ‡æ ‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæœºå™¨ä¸å¯å­¦ä¹ æŠ€æœ¯åº”è¿è€Œç”Ÿï¼Œä»¥è§£å†³ç”¨æˆ·éšç§ã€ç‰ˆæƒä¾µçŠ¯å’Œæ•´ä½“å®‰å…¨æ€§ç­‰æ—¥ç›Šå¢é•¿çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œç›®å‰æœ€å…ˆè¿›çš„ä¸å¯å­¦ä¹ æ–¹æ³•é€šå¸¸ä¼šé­å—ç¾éš¾æ€§é—å¿˜å’ŒæŒ‡æ ‡ä¸å¹³è¡¡çš„å›°æ‰°ï¼Œä¾‹å¦‚ï¼Œä»¥ç‰ºç‰²å…¶ä»–ç›®æ ‡ï¼ˆå¦‚ä¸å¯å­¦ä¹ æœ‰æ•ˆæ€§ã€æ•ˆç”¨ä¿æŒæˆ–éšç§ä¿æŠ¤ï¼‰ä¸ºä»£ä»·è¿‡åº¦ä¼˜åŒ–ä¸€ä¸ªç›®æ ‡ã€‚æ­¤å¤–ï¼Œè¡¨ç¤ºæˆ–å‚æ•°ç©ºé—´ä¸­çš„å¾®å°æ‰°åŠ¨å¯èƒ½è¢«é‡æ–°å­¦ä¹ å’Œè¶Šç‹±æ”»å‡»åˆ©ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PRISMï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå®ƒåœ¨è¡¨ç¤ºå’Œå‚æ•°ç©ºé—´ä¸­å¼ºåˆ¶æ‰§è¡ŒåŒç©ºé—´å¹³æ»‘ï¼Œä»¥æé«˜é²æ£’æ€§å¹¶å¹³è¡¡ä¸å¯å­¦ä¹ æŒ‡æ ‡ã€‚PRISMåŒ…å«ä¸¤ä¸ªå¹³æ»‘ä¼˜åŒ–é˜¶æ®µï¼šï¼ˆiï¼‰è¡¨ç¤ºç©ºé—´é˜¶æ®µï¼Œè¯¥é˜¶æ®µé‡‡ç”¨ç»è¿‡é²æ£’è®­ç»ƒçš„æ¢é’ˆæ¥é˜²å¾¡è¶Šç‹±æ”»å‡»ï¼Œä»¥åŠï¼ˆiiï¼‰å‚æ•°ç©ºé—´é˜¶æ®µï¼Œè¯¥é˜¶æ®µè§£è€¦ä¿ç•™-é—å¿˜æ¢¯åº¦å†²çªï¼Œå‡å°‘ä¸å¹³è¡¡ï¼Œå¹¶å¹³æ»‘å‚æ•°ç©ºé—´ä»¥å‡è½»é‡æ–°å­¦ä¹ æ”»å‡»ã€‚åœ¨WMDPå’ŒMUSEä¸Šï¼Œè·¨è¶Šå¯¹è¯å’Œè¿ç»­æ–‡æœ¬è®¾ç½®çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPRISMåœ¨å¤šç§æ”»å‡»ä¸‹ä¼˜äºSOTAåŸºçº¿ï¼ŒåŒæ—¶åœ¨å…³é”®æŒ‡æ ‡ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¯å­¦ä¹ æ–¹æ³•ï¼Œåœ¨å°è¯•åˆ é™¤ç‰¹å®šä¿¡æ¯æ—¶ï¼Œå®¹æ˜“å¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œå³æ¨¡å‹å¿˜è®°äº†å¤§é‡æœ‰ç”¨çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€åœ¨ä¸å¯å­¦ä¹ æœ‰æ•ˆæ€§ã€æ•ˆç”¨ä¿æŒå’Œéšç§ä¿æŠ¤ç­‰æŒ‡æ ‡ä¸Šè¡¨ç°ä¸å¹³è¡¡ï¼Œè¿‡åº¦ä¼˜åŒ–ä¸€ä¸ªæŒ‡æ ‡è€Œç‰ºç‰²å…¶ä»–æŒ‡æ ‡ã€‚åŒæ—¶ï¼Œæ¨¡å‹å®¹æ˜“å—åˆ°é‡æ–°å­¦ä¹ æ”»å‡»å’Œè¶Šç‹±æ”»å‡»ï¼Œé²æ£’æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPRISMçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨è¡¨ç¤ºç©ºé—´å’Œå‚æ•°ç©ºé—´ä¸­å¼•å…¥å¹³æ»‘æ€§çº¦æŸï¼Œæ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå¹³è¡¡æ€§ã€‚è¡¨ç¤ºç©ºé—´å¹³æ»‘æ—¨åœ¨é˜²å¾¡è¶Šç‹±æ”»å‡»ï¼Œå‚æ•°ç©ºé—´å¹³æ»‘æ—¨åœ¨å‡è½»é‡æ–°å­¦ä¹ æ”»å‡»ï¼Œå¹¶è§£è€¦retain-forgetæ¢¯åº¦å†²çªï¼Œä»è€Œå‡å°‘æŒ‡æ ‡ä¸å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPRISMæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦çš„ä¼˜åŒ–é˜¶æ®µï¼š1) è¡¨ç¤ºç©ºé—´å¹³æ»‘é˜¶æ®µï¼šåˆ©ç”¨ä¸€ä¸ªç»è¿‡é²æ£’è®­ç»ƒçš„æ¢é’ˆç½‘ç»œï¼Œå¯¹æ¨¡å‹çš„è¡¨ç¤ºç©ºé—´è¿›è¡Œå¹³æ»‘ï¼Œä»¥é˜²å¾¡è¶Šç‹±æ”»å‡»ã€‚2) å‚æ•°ç©ºé—´å¹³æ»‘é˜¶æ®µï¼šé€šè¿‡è§£è€¦retain-forgetæ¢¯åº¦ï¼Œå‡å°‘æŒ‡æ ‡ä¸å¹³è¡¡ï¼Œå¹¶å¯¹å‚æ•°ç©ºé—´è¿›è¡Œå¹³æ»‘ï¼Œä»¥å‡è½»é‡æ–°å­¦ä¹ æ”»å‡»ã€‚è¿™ä¸¤ä¸ªé˜¶æ®µå…±åŒä½œç”¨ï¼Œå®ç°é²æ£’ä¸”å‡è¡¡çš„ä¸å¯å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šPRISMçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŒç©ºé—´å¹³æ»‘çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºLLMçš„ä¸å¯å­¦ä¹ ä»»åŠ¡ä¸­ã€‚é€šè¿‡åœ¨è¡¨ç¤ºç©ºé—´å’Œå‚æ•°ç©ºé—´åŒæ—¶è¿›è¡Œå¹³æ»‘ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå¹³è¡¡æ€§ï¼Œä»è€Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¡¨ç¤ºç©ºé—´å¹³æ»‘é˜¶æ®µï¼Œä½¿ç”¨å¯¹æŠ—è®­ç»ƒæ¥è®­ç»ƒæ¢é’ˆç½‘ç»œï¼Œä½¿å…¶å¯¹è¾“å…¥æ‰°åŠ¨å…·æœ‰é²æ£’æ€§ã€‚åœ¨å‚æ•°ç©ºé—´å¹³æ»‘é˜¶æ®µï¼Œè®¾è®¡äº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºè§£è€¦retain-forgetæ¢¯åº¦ï¼Œå¹¶å¼•å…¥æ­£åˆ™åŒ–é¡¹æ¥å¹³æ»‘å‚æ•°ç©ºé—´ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°å½¢å¼åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä½†æ­¤å¤„æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PRISMåœ¨WMDPå’ŒMUSEæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒPRISMåœ¨å¤šç§æ”»å‡»ä¸‹ä¼˜äºSOTAåŸºçº¿ã€‚å…·ä½“æ¥è¯´ï¼ŒPRISMåœ¨ä¸å¯å­¦ä¹ æœ‰æ•ˆæ€§ã€æ•ˆç”¨ä¿æŒå’Œéšç§ä¿æŠ¤ç­‰æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå¹¶ä¸”åœ¨è¿™äº›æŒ‡æ ‡ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªçŸ¥ï¼Œéœ€è¦æŸ¥é˜…è®ºæ–‡åŸæ–‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PRISMæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§ã€ç‰ˆæƒå’Œæ¨¡å‹å®‰å…¨çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šåœ¨çº¿æ•™è‚²ã€æ™ºèƒ½å®¢æœã€é‡‘èé£æ§ç­‰ã€‚é€šè¿‡è¯¥æŠ€æœ¯ï¼Œå¯ä»¥å®‰å…¨åœ°åˆ é™¤æ¨¡å‹ä¸­çš„æ•æ„Ÿä¿¡æ¯ï¼Œé˜²æ­¢æ¨¡å‹è¢«ç”¨äºéæ³•ç›®çš„ï¼Œå¹¶æé«˜æ¨¡å‹çš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æˆä¸ºLLMå®‰å…¨é¢†åŸŸçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid advancement of large language models, Machine Unlearning has emerged to address growing concerns around user privacy, copyright infringement, and overall safety. Yet state-of-the-art (SOTA) unlearning methods often suffer from catastrophic forgetting and metric imbalance, for example by over-optimizing one objective (e.g., unlearning effectiveness, utility preservation, or privacy protection) at the expense of others. In addition, small perturbations in the representation or parameter space can be exploited by relearn and jailbreak attacks. To address these challenges, we propose PRISM, a unified framework that enforces dual-space smoothness in representation and parameter spaces to improve robustness and balance unlearning metrics. PRISM consists of two smoothness optimization stages: (i) a representation space stage that employs a robustly trained probe to defend against jailbreak attacks, and (ii) a parameter-space stage that decouples retain-forget gradient conflicts, reduces imbalance, and smooths the parameter space to mitigate relearning attacks. Extensive experiments on WMDP and MUSE, across conversational-dialogue and continuous-text settings, show that PRISM outperforms SOTA baselines under multiple attacks while achieving a better balance among key metrics.

