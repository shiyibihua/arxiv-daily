---
layout: default
title: Bias after Prompting: Persistent Discrimination in Large Language Models
---

# Bias after Prompting: Persistent Discrimination in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08146" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08146v2</a>
  <a href="https://arxiv.org/pdf/2509.08146.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08146v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08146v2', 'Bias after Prompting: Persistent Discrimination in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nivedha Sivakumar, Natalie Mackraz, Samira Khorshidi, Krishna Patel, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09 (æ›´æ–°: 2025-11-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºPromptingåçš„å¤§è¯­è¨€æ¨¡å‹åè§ï¼šæŒç»­å­˜åœ¨çš„æ­§è§†ç°è±¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `åè§è½¬ç§»` `Prompting` `å…¬å¹³æ€§` `å› æœæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä½ä¼°äº†Promptingæ–¹æ³•å¼•å…¥çš„åè§é£é™©ï¼Œè®¤ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„åè§ä¸ä¼šè½¬ç§»åˆ°è°ƒæ•´åçš„æ¨¡å‹ã€‚
2. è¯¥ç ”ç©¶é€šè¿‡åˆ†æå› æœæ¨¡å‹ä¸­Promptè°ƒæ•´åçš„åè§è½¬ç§»ç°è±¡ï¼Œæ­ç¤ºäº†Promptingæ–¹æ³•å¯èƒ½å¼•å…¥å¹¶æ”¾å¤§åè§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå³ä½¿é‡‡ç”¨æµè¡Œçš„Promptingå»åè§ç­–ç•¥ï¼Œä¹Ÿæ— æ³•å§‹ç»ˆå¦‚ä¸€åœ°å‡å°‘è·¨æ¨¡å‹ã€ä»»åŠ¡å’Œäººå£ç»Ÿè®¡çš„åè§è½¬ç§»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…ˆå‰å…³äºåè§è½¬ç§»å‡è®¾ï¼ˆBTHï¼‰çš„ç ”ç©¶å¯èƒ½å­˜åœ¨ä¸€ä¸ªå±é™©çš„å‡è®¾ï¼Œå³é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åè§ä¸ä¼šè½¬ç§»åˆ°ç»è¿‡è°ƒæ•´çš„æ¨¡å‹ä¸­ã€‚æœ¬æ–‡é€šè¿‡åœ¨å› æœæ¨¡å‹ä¸­ç ”ç©¶promptè°ƒæ•´ä¸‹çš„BTHï¼Œä»è€Œå¦å®šäº†è¿™ä¸€å‡è®¾ï¼Œå› ä¸ºpromptingæ˜¯å®é™…åº”ç”¨ä¸­ä¸€ç§éå¸¸æµè¡Œå’Œæ˜“äºä½¿ç”¨çš„è°ƒæ•´ç­–ç•¥ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸åï¼Œæˆ‘ä»¬å‘ç°åè§å¯ä»¥é€šè¿‡promptingè½¬ç§»ï¼Œå¹¶ä¸”æµè¡Œçš„åŸºäºpromptçš„ç¼“è§£æ–¹æ³•å¹¶ä¸èƒ½å§‹ç»ˆå¦‚ä¸€åœ°é˜²æ­¢åè§è½¬ç§»ã€‚å…·ä½“è€Œè¨€ï¼Œå†…åœ¨åè§ä¸promptè°ƒæ•´åçš„åè§ä¹‹é—´çš„ç›¸å…³æ€§åœ¨ä¸åŒçš„äººå£ç»Ÿè®¡å’Œä»»åŠ¡ä¸­ä¿æŒä¸­ç­‰åˆ°å¼ºçƒˆçš„æ°´å¹³â€”â€”ä¾‹å¦‚ï¼Œåœ¨å…±æŒ‡æ¶ˆè§£ä¸­æ€§åˆ«ï¼ˆrho >= 0.94ï¼‰ï¼Œåœ¨é—®ç­”ä¸­å¹´é¾„ï¼ˆrho >= 0.98ï¼‰å’Œå®—æ•™ï¼ˆrho >= 0.69ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å½“æ”¹å˜å°‘é‡æ ·æœ¬ç»„æˆå‚æ•°ï¼ˆå¦‚æ ·æœ¬å¤§å°ã€åˆ»æ¿å°è±¡å†…å®¹ã€èŒä¸šåˆ†å¸ƒå’Œä»£è¡¨æ€§å¹³è¡¡ï¼‰æ—¶ï¼Œåè§ä»ç„¶é«˜åº¦ç›¸å…³ï¼ˆrho >= 0.90ï¼‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ç§åŸºäºpromptçš„å»åè§ç­–ç•¥ï¼Œå‘ç°ä¸åŒçš„æ–¹æ³•æœ‰ä¸åŒçš„ä¼˜åŠ¿ï¼Œä½†æ²¡æœ‰ä¸€ç§æ–¹æ³•èƒ½å¤Ÿå§‹ç»ˆå¦‚ä¸€åœ°å‡å°‘è·¨æ¨¡å‹ã€ä»»åŠ¡æˆ–äººå£ç»Ÿè®¡çš„åè§è½¬ç§»ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œçº æ­£å†…åœ¨æ¨¡å‹ä¸­çš„åè§ï¼Œå¹¶å¯èƒ½æé«˜æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥é˜²æ­¢åè§ä¼ æ’­åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç ”ç©¶æœªèƒ½å……åˆ†è®¤è¯†åˆ°Promptingæ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¼•å…¥å’Œä¼ æ’­åè§çš„é£é™©ã€‚å°½ç®¡Promptingæ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æ¨¡å‹è°ƒæ•´ç­–ç•¥ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶å¯èƒ½é”™è¯¯åœ°è®¤ä¸ºé¢„è®­ç»ƒæ¨¡å‹ä¸­çš„åè§ä¸ä¼šè½¬ç§»åˆ°é€šè¿‡Promptingè°ƒæ•´åçš„æ¨¡å‹ä¸­ã€‚è¿™ç§å‡è®¾å¿½ç•¥äº†Promptingå¯èƒ½æ”¾å¤§æˆ–æ”¹å˜åŸæœ‰åè§çš„å¯èƒ½æ€§ï¼Œå¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡ä¸­å‡ºç°ä¸å…¬å¹³æˆ–æ­§è§†æ€§çš„ç»“æœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å®è¯ç ”ç©¶æ¥éªŒè¯Promptingæ˜¯å¦ä¼šå¯¼è‡´åè§è½¬ç§»ï¼Œå¹¶è¯„ä¼°ç°æœ‰Promptingå»åè§ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶äººå‘˜é€šè¿‡è®¾è®¡ä¸€ç³»åˆ—å®éªŒï¼Œç³»ç»Ÿåœ°åˆ†æäº†å†…åœ¨åè§ä¸Promptè°ƒæ•´åçš„åè§ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥åŠä¸åŒPromptingå‚æ•°å¯¹åè§è½¬ç§»çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸‹æ¸¸ä»»åŠ¡ï¼›2) è®¾è®¡ä¸åŒçš„Promptingç­–ç•¥ï¼ŒåŒ…æ‹¬å°‘é‡æ ·æœ¬å­¦ä¹ å’Œä¸åŒçš„Promptç»„æˆæ–¹å¼ï¼›3) ä½¿ç”¨é¢„å®šä¹‰çš„åè§è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹åœ¨ä¸åŒPromptä¸‹çš„åè§ç¨‹åº¦ï¼›4) åˆ†æå†…åœ¨åè§ä¸Promptè°ƒæ•´åçš„åè§ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥åŠä¸åŒPromptingå‚æ•°å¯¹åè§è½¬ç§»çš„å½±å“ï¼›5) è¯„ä¼°å‡ ç§æµè¡Œçš„Promptingå»åè§ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) éªŒè¯äº†Promptingä¼šå¯¼è‡´åè§è½¬ç§»ï¼Œå¦å®šäº†ä¹‹å‰ç ”ç©¶ä¸­å…³äºåè§ä¸ä¼šè½¬ç§»çš„å‡è®¾ï¼›2) ç³»ç»Ÿåœ°åˆ†æäº†ä¸åŒPromptingå‚æ•°å¯¹åè§è½¬ç§»çš„å½±å“ï¼Œæ­ç¤ºäº†Promptingç­–ç•¥çš„å¤æ‚æ€§ï¼›3) è¯„ä¼°äº†ç°æœ‰Promptingå»åè§ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå‘ç°è¿™äº›ç­–ç•¥å¹¶ä¸èƒ½å§‹ç»ˆå¦‚ä¸€åœ°å‡å°‘åè§è½¬ç§»ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©äº†å…·æœ‰ä»£è¡¨æ€§çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚å…±æŒ‡æ¶ˆè§£å’Œé—®ç­”ï¼Œä»¥è¯„ä¼°ä¸åŒä»»åŠ¡ä¸‹çš„åè§è½¬ç§»æƒ…å†µï¼›2) è®¾è®¡äº†ä¸åŒçš„Promptingç­–ç•¥ï¼ŒåŒ…æ‹¬æ”¹å˜æ ·æœ¬å¤§å°ã€åˆ»æ¿å°è±¡å†…å®¹ã€èŒä¸šåˆ†å¸ƒå’Œä»£è¡¨æ€§å¹³è¡¡ç­‰å‚æ•°ï¼Œä»¥åˆ†æä¸åŒPromptingå‚æ•°å¯¹åè§è½¬ç§»çš„å½±å“ï¼›3) ä½¿ç”¨äº†é¢„å®šä¹‰çš„åè§è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚æ€§åˆ«ã€å¹´é¾„å’Œå®—æ•™åè§ï¼Œä»¥é‡åŒ–æ¨¡å‹åœ¨ä¸åŒPromptä¸‹çš„åè§ç¨‹åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå†…åœ¨åè§ä¸Promptè°ƒæ•´åçš„åè§ä¹‹é—´å­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œä¾‹å¦‚åœ¨å…±æŒ‡æ¶ˆè§£ä¸­æ€§åˆ«ï¼ˆrho >= 0.94ï¼‰ï¼Œåœ¨é—®ç­”ä¸­å¹´é¾„ï¼ˆrho >= 0.98ï¼‰å’Œå®—æ•™ï¼ˆrho >= 0.69ï¼‰ã€‚å³ä½¿æ”¹å˜å°‘é‡æ ·æœ¬ç»„æˆå‚æ•°ï¼Œåè§ä»ç„¶é«˜åº¦ç›¸å…³ï¼ˆrho >= 0.90ï¼‰ã€‚ç°æœ‰çš„Promptingå»åè§ç­–ç•¥å¹¶ä¸èƒ½å§‹ç»ˆå¦‚ä¸€åœ°å‡å°‘åè§è½¬ç§»ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å…¬å¹³æ€§å’Œå¯é æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚å®ƒå¯ä»¥åº”ç”¨äºå¼€å‘æ›´å…¬å¹³ã€æ›´è´Ÿè´£ä»»çš„AIç³»ç»Ÿï¼Œä¾‹å¦‚åœ¨æ‹›è˜ã€ä¿¡è´·è¯„ä¼°å’Œæ³•å¾‹å†³ç­–ç­‰é¢†åŸŸã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´æœ‰æ•ˆçš„å»åè§ç­–ç•¥ï¼Œå¹¶å¼€å‘èƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹å’Œç¼“è§£Promptingå¼•å…¥çš„åè§çš„å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A dangerous assumption that can be made from prior work on the bias transfer hypothesis (BTH) is that biases do not transfer from pre-trained large language models (LLMs) to adapted models. We invalidate this assumption by studying the BTH in causal models under prompt adaptations, as prompting is an extremely popular and accessible adaptation strategy used in real-world applications. In contrast to prior work, we find that biases can transfer through prompting and that popular prompt-based mitigation methods do not consistently prevent biases from transferring. Specifically, the correlation between intrinsic biases and those after prompt adaptation remain moderate to strong across demographics and tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age (rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we find that biases remain strongly correlated when varying few-shot composition parameters, such as sample size, stereotypical content, occupational distribution and representational balance (rho >= 0.90). We evaluate several prompt-based debiasing strategies and find that different approaches have distinct strengths, but none consistently reduce bias transfer across models, tasks or demographics. These results demonstrate that correcting bias, and potentially improving reasoning ability, in intrinsic models may prevent propagation of biases to downstream tasks.

