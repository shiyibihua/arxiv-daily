---
layout: default
title: Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations
---

# Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07311" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07311v1</a>
  <a href="https://arxiv.org/pdf/2509.07311.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07311v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07311v1', 'Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sihyun Park

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKAMIRæ–¹æ³•ï¼Œé€šè¿‡æ¨¡å‹å†…éƒ¨è¡¨å¾åˆ†æè¿›è¡Œé«˜æ•ˆè®­ç»ƒæ•°æ®é€‰æ‹©ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®é€‰æ‹©` `æ¨¡å‹å†…éƒ¨è¡¨å¾` `çŸ¥è¯†åˆ†æ` `ç›‘ç£å¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨é˜…è¯»ç†è§£` `æ–‡æœ¬æ‘˜è¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰SFTè®­ç»ƒæ•°æ®é€‰æ‹©ç¼ºä¹æœ‰æ•ˆæ–¹æ³•ï¼Œç®€å•å¢åŠ æ•°æ®é‡æˆ–ä¾èµ–æç¤ºå·¥ç¨‹çš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚
2. KAMIRé€šè¿‡åˆ†ææ¨¡å‹å†…éƒ¨è¡¨å¾ï¼Œè¯„ä¼°æ•°æ®ä¸æ¨¡å‹çŸ¥è¯†çš„å…³è”åº¦ï¼Œé€‰æ‹©å¯¹æ¨¡å‹è€Œè¨€â€œä¸ç†Ÿæ‚‰â€ä½†æœ‰ä»·å€¼çš„æ•°æ®ã€‚
3. å®éªŒè¯æ˜ï¼Œä½¿ç”¨KAMIRé€‰æ‹©çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨æœºå™¨é˜…è¯»ç†è§£å’Œæ‘˜è¦ç­‰ä»»åŠ¡ä¸Šçš„æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å¾—ç›Šäºé¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¯¹é½è°ƒæ•´ã€‚å…¶ä¸­ï¼ŒSFTåœ¨å°†æ¨¡å‹çš„é€šç”¨çŸ¥è¯†è½¬åŒ–ä¸ºé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç»“æ„åŒ–å“åº”æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œç›®å‰è¿˜æ²¡æœ‰æ˜ç¡®æœ‰æ•ˆçš„æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚ç®€å•åœ°å¢åŠ æ•°æ®é‡å¹¶ä¸èƒ½ä¿è¯æ€§èƒ½çš„æå‡ï¼Œè€Œé¢„å¤„ç†ã€æŠ½æ ·å’ŒéªŒè¯åˆ™éœ€è¦å¤§é‡çš„æ—¶é—´å’Œæˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå·²ç»æå‡ºäº†å„ç§æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚å…¶ä¸­ï¼ŒåŸºäºçŸ¥è¯†çš„é€‰æ‹©æ–¹æ³•é€šè¿‡åˆ†ææ¨¡å‹çš„å“åº”æ¥è¯†åˆ«åˆé€‚çš„è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºæç¤ºå·¥ç¨‹ï¼Œä½¿å…¶å¯¹å˜åŒ–æ•æ„Ÿï¼Œå¹¶äº§ç”Ÿé¢å¤–çš„æç¤ºè®¾è®¡æˆæœ¬ã€‚æœ¬ç ”ç©¶æå‡ºäº†çŸ¥è¯†åˆ†æé€šè¿‡æ¨¡å‹å†…éƒ¨è¡¨å¾ï¼ˆKAMIRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ¨¡å‹å†…éƒ¨è¡¨å¾çš„æ•°æ®æ¥å…‹æœè¿™äº›é™åˆ¶ã€‚KAMIRè®¡ç®—æ¯ä¸€å±‚ï¼ˆå—ï¼‰çš„éšè—çŠ¶æ€ä¸ç»™å®šè¾“å…¥çš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥è¯„ä¼°æ•°æ®ã€‚ä¸ä¸»è¦å±€é™äºå¤šé¡¹é€‰æ‹©ä»»åŠ¡çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒKAMIRå¯ä»¥åº”ç”¨äºå¹¿æ³›çš„ä»»åŠ¡ï¼Œå¦‚æœºå™¨é˜…è¯»ç†è§£å’Œæ‘˜è¦ã€‚æ­¤å¤–ï¼Œå³ä½¿ä½¿ç”¨å°å‹æ•°æ®é›†å’Œç®€å•çš„åˆ†ç±»å™¨æ¶æ„ï¼Œå®ƒä¹Ÿèƒ½åŸºäºæ¨¡å‹å¯¹è¾“å…¥çš„ç†Ÿæ‚‰ç¨‹åº¦é€‰æ‹©å¯¹è®­ç»ƒæœ‰ç”¨çš„æ•°æ®ã€‚è·¨ä¸åŒä»»åŠ¡æ•°æ®é›†çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ä¸å¤ªç†Ÿæ‚‰çš„æ•°æ®è¿›è¡Œè®­ç»ƒå¯ä»¥å¸¦æ¥æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µï¼Œå¦‚ä½•é«˜æ•ˆé€‰æ‹©è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç®€å•å¢åŠ æ•°æ®é‡æˆ–ä¾èµ–æç¤ºå·¥ç¨‹çš„æ–¹æ³•ï¼Œè¦ä¹ˆæ•ˆç‡ä½ä¸‹ï¼Œè¦ä¹ˆå¯¹æç¤ºå˜åŒ–æ•æ„Ÿï¼Œä¸”æˆæœ¬è¾ƒé«˜ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥å‡†ç¡®è¯„ä¼°æ•°æ®å¯¹æ¨¡å‹å­¦ä¹ çš„ä»·å€¼ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œé€šè¿‡åˆ†ææ¨¡å‹å†…éƒ¨çš„éšè—å±‚è¡¨å¾ï¼Œæ¥è¯„ä¼°æ•°æ®ä¸æ¨¡å‹å·²æœ‰çŸ¥è¯†çš„å…³è”ç¨‹åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœä¸€ä¸ªæ•°æ®æ ·æœ¬åœ¨æ¨¡å‹çš„å†…éƒ¨è¡¨å¾ä¸Šä¸æœ€ç»ˆè¡¨å¾çš„ç›¸ä¼¼åº¦è¾ƒä½ï¼Œåˆ™è®¤ä¸ºè¯¥æ ·æœ¬å¯¹æ¨¡å‹è€Œè¨€æ˜¯â€œä¸ç†Ÿæ‚‰â€çš„ï¼Œå¯èƒ½åŒ…å«æ›´å¤šæœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œä»è€Œæ›´æœ‰åŠ©äºæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKAMIRæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) å¯¹äºç»™å®šçš„è¾“å…¥æ•°æ®ï¼Œé€šè¿‡æ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œè·å–æ¯ä¸€å±‚ï¼ˆblockï¼‰çš„éšè—çŠ¶æ€ã€‚2) è®¡ç®—æ¯ä¸€å±‚çš„éšè—çŠ¶æ€ä¸æœ€ç»ˆéšè—çŠ¶æ€ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚3) åŸºäºè®¡ç®—å¾—åˆ°çš„ç›¸ä¼¼åº¦ï¼Œå¯¹æ•°æ®è¿›è¡Œæ’åºæˆ–ç­›é€‰ï¼Œé€‰æ‹©ç›¸ä¼¼åº¦è¾ƒä½çš„æ•°æ®ä½œä¸ºè®­ç»ƒé›†ã€‚4) ä½¿ç”¨é€‰æ‹©å‡ºçš„æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šKAMIRçš„å…³é”®åˆ›æ–°åœ¨äºï¼Œå®ƒåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„å†…éƒ¨è¡¨å¾æ¥è¯„ä¼°æ•°æ®çš„ä»·å€¼ï¼Œé¿å…äº†å¯¹å¤–éƒ¨çŸ¥è¯†åº“æˆ–æç¤ºå·¥ç¨‹çš„ä¾èµ–ã€‚ä¸ä»¥å¾€ä¸»è¦é’ˆå¯¹å¤šé¡¹é€‰æ‹©é¢˜çš„æ–¹æ³•ä¸åŒï¼ŒKAMIRå¯ä»¥åº”ç”¨äºæ›´å¹¿æ³›çš„ä»»åŠ¡ï¼Œå¦‚æœºå™¨é˜…è¯»ç†è§£å’Œæ‘˜è¦ã€‚æ­¤å¤–ï¼ŒKAMIRæ–¹æ³•åªéœ€è¦å°‘é‡æ•°æ®å’Œä¸€ä¸ªç®€å•çš„åˆ†ç±»å™¨æ¶æ„ï¼Œå°±èƒ½æœ‰æ•ˆåœ°é€‰æ‹©è®­ç»ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šKAMIRçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) éšè—çŠ¶æ€ç›¸ä¼¼åº¦çš„è®¡ç®—æ–¹å¼ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–æ¬§æ°è·ç¦»ã€‚2) å¦‚ä½•ç»¼åˆä¸åŒå±‚çš„ç›¸ä¼¼åº¦ä¿¡æ¯ï¼Œä¾‹å¦‚å¯ä»¥å¯¹ä¸åŒå±‚çš„ç›¸ä¼¼åº¦è¿›è¡ŒåŠ æƒå¹³å‡ã€‚3) å¦‚ä½•è®¾å®šç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä»¥é€‰æ‹©åˆé€‚çš„è®­ç»ƒæ•°æ®ã€‚è®ºæ–‡ä¸­å¯èƒ½è¿˜æ¶‰åŠäº†ç‰¹å®šä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„è°ƒæ•´ï¼Œä½†æ‘˜è¦ä¸­æœªæåŠå…·ä½“ç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨KAMIRæ–¹æ³•é€‰æ‹©çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨æœºå™¨é˜…è¯»ç†è§£å’Œæ‘˜è¦ç­‰ä»»åŠ¡ä¸Šçš„æ³›åŒ–æ€§èƒ½ã€‚å³ä½¿åœ¨å°æ•°æ®é›†å’Œç®€å•åˆ†ç±»å™¨æ¶æ„ä¸‹ï¼ŒKAMIRä¹Ÿèƒ½æœ‰æ•ˆé€‰æ‹©è®­ç»ƒæ•°æ®ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡æ­£æ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

KAMIRæ–¹æ³•å¯åº”ç”¨äºå„ç§éœ€è¦æ•°æ®é€‰æ‹©çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡é€‰æ‹©æ›´æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼ŒåŠ é€Ÿæ¨¡å‹å¼€å‘å‘¨æœŸã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºæ•°æ®èµ„æºæœ‰é™æˆ–æ•°æ®è´¨é‡å‚å·®ä¸é½çš„åœºæ™¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) have been driven by pretraining, supervised fine tuning (SFT), and alignment tuning. Among these, SFT plays a crucial role in transforming a model 's general knowledge into structured responses tailored to specific tasks. However, there is no clearly established methodology for effective training data selection. Simply increasing the volume of data does not guarantee performance improvements, while preprocessing, sampling, and validation require substantial time and cost.
>   To address this issue, a variety of data selection methods have been proposed. Among them, knowledge based selection approaches identify suitable training data by analyzing the model 's responses. Nevertheless, these methods typically rely on prompt engineering, making them sensitive to variations and incurring additional costs for prompt design.
>   In this study, we propose Knowledge Analysis via Model Internal Representations (KAMIR), a novel approach that overcomes these limitations by analyzing data based on the model 's internal representations. KAMIR computes similarities between the hidden states of each layer (block) and the final hidden states for a given input to assess the data. Unlike prior methods that were largely limited to multiple choice tasks, KAMIR can be applied to a wide range of tasks such as machine reading comprehension and summarization. Moreover, it selects data useful for training based on the model 's familiarity with the input, even with a small dataset and a simple classifier architecture. Experiments across diverse task datasets demonstrate that training with less familiar data leads to better generalization performance.

