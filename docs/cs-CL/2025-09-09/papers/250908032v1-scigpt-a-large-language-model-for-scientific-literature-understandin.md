---
layout: default
title: SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery
---

# SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08032" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08032v1</a>
  <a href="https://arxiv.org/pdf/2509.08032.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08032v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08032v1', 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fengyu She, Nan Wang, Hongfei Wu, Ziyi Wan, Jingmian Wang, Chang Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SciGPTï¼šé¢å‘ç§‘å­¦æ–‡çŒ®ç†è§£å’ŒçŸ¥è¯†å‘ç°çš„å¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ç§‘å­¦æ–‡çŒ®ç†è§£` `çŸ¥è¯†å‘ç°` `é¢†åŸŸè‡ªé€‚åº”` `ç¨€ç–æ··åˆä¸“å®¶` `é¢†åŸŸè’¸é¦` `çŸ¥è¯†å›¾è°±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é€šç”¨å¤§è¯­è¨€æ¨¡å‹éš¾ä»¥æ•æ‰ç§‘å­¦é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­å’Œæ–¹æ³•è®ºä¸¥è°¨æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨è·¨å­¦ç§‘ç ”ç©¶ä¸­çš„åº”ç”¨ã€‚
2. SciGPTé€šè¿‡ä½æˆæœ¬çš„é¢†åŸŸè’¸é¦ã€ç¨€ç–æ··åˆä¸“å®¶æ³¨æ„åŠ›æœºåˆ¶å’ŒçŸ¥è¯†æ„ŸçŸ¥é€‚é…ï¼Œæå‡äº†æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸçš„æ€§èƒ½ã€‚
3. åœ¨ScienceBenchä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSciGPTåœ¨åºåˆ—æ ‡æ³¨ã€ç”Ÿæˆå’Œæ¨ç†ç­‰æ ¸å¿ƒç§‘å­¦ä»»åŠ¡ä¸Šä¼˜äºGPT-4oï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç§‘å­¦æ–‡çŒ®å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œè¿™ç»™ç ”ç©¶äººå‘˜é«˜æ•ˆåœ°ç»¼åˆçŸ¥è¯†å¸¦æ¥äº†ç“¶é¢ˆã€‚é€šç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ–‡æœ¬å¤„ç†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¸¸å¸¸æ— æ³•æ•æ‰ç§‘å­¦é¢†åŸŸç‰¹æœ‰çš„ç»†å¾®å·®åˆ«(ä¾‹å¦‚ï¼ŒæŠ€æœ¯æœ¯è¯­ã€æ–¹æ³•è®ºçš„ä¸¥è°¨æ€§)ï¼Œå¹¶ä¸”éš¾ä»¥å¤„ç†å¤æ‚çš„ç§‘å­¦ä»»åŠ¡ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨è·¨å­¦ç§‘ç ”ç©¶ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†SciGPTï¼Œä¸€ä¸ªä¸ºç§‘å­¦æ–‡çŒ®ç†è§£è€Œè®¾è®¡çš„é¢†åŸŸè‡ªé€‚åº”åŸºç¡€æ¨¡å‹ï¼Œä»¥åŠScienceBenchï¼Œä¸€ä¸ªä¸ºè¯„ä¼°ç§‘å­¦LLMsé‡èº«å®šåˆ¶çš„å¼€æºåŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰ç§‘å­¦æ–‡çŒ®æ•°é‡çˆ†ç‚¸å¼å¢é•¿ï¼Œç ”ç©¶äººå‘˜éš¾ä»¥é«˜æ•ˆåœ°ä»ä¸­æå–å’Œç»¼åˆçŸ¥è¯†ã€‚é€šç”¨å¤§è¯­è¨€æ¨¡å‹è™½ç„¶å…·å¤‡ä¸€å®šçš„æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼Œä½†åœ¨ç†è§£ç§‘å­¦é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­ã€æ–¹æ³•è®ºä»¥åŠå¤„ç†å¤æ‚ç§‘å­¦ä»»åŠ¡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•å¾ˆå¥½åœ°æ”¯æŒè·¨å­¦ç§‘ç ”ç©¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSciGPTçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªé¢†åŸŸè‡ªé€‚åº”çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†ç§‘å­¦æ–‡çŒ®ã€‚é€šè¿‡é¢†åŸŸè’¸é¦ã€ç¨€ç–æ··åˆä¸“å®¶æ³¨æ„åŠ›æœºåˆ¶å’ŒçŸ¥è¯†æ„ŸçŸ¥é€‚é…ç­‰æŠ€æœ¯ï¼Œæå‡æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸçš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSciGPTæ„å»ºäºQwen3æ¶æ„ä¹‹ä¸Šï¼Œä¸»è¦åŒ…å«ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼š1) ä½æˆæœ¬é¢†åŸŸè’¸é¦æ¨¡å—ï¼Œç”¨äºå°†é€šç”¨è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°ç§‘å­¦é¢†åŸŸï¼›2) ç¨€ç–æ··åˆä¸“å®¶(SMoE)æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå¤„ç†é•¿æ–‡æ¡£å¹¶é™ä½å†…å­˜æ¶ˆè€—ï¼›3) çŸ¥è¯†æ„ŸçŸ¥é€‚é…æ¨¡å—ï¼Œç”¨äºæ•´åˆé¢†åŸŸæœ¬ä½“çŸ¥è¯†ï¼Œå¼¥åˆè·¨å­¦ç§‘çŸ¥è¯†çš„å·®è·ã€‚

**å…³é”®åˆ›æ–°**ï¼šSciGPTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é¢†åŸŸè‡ªé€‚åº”ç­–ç•¥ï¼Œå…·ä½“åŒ…æ‹¬ï¼š1) é‡‡ç”¨ä¸¤é˜¶æ®µè’¸é¦æµç¨‹ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ï¼›2) å¼•å…¥SMoEæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¾è‘—é™ä½é•¿æ–‡æ¡£å¤„ç†çš„å†…å­˜æ¶ˆè€—ï¼›3) é€šè¿‡çŸ¥è¯†æ„ŸçŸ¥é€‚é…ï¼Œå°†é¢†åŸŸæœ¬ä½“çŸ¥è¯†èå…¥æ¨¡å‹ï¼Œæå‡è·¨å­¦ç§‘ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šSciGPTçš„é¢†åŸŸè’¸é¦é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤§è§„æ¨¡ç§‘å­¦æ–‡æœ¬è¿›è¡Œé¢„è®­ç»ƒï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨é«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒã€‚SMoEæ³¨æ„åŠ›æœºåˆ¶é€šè¿‡ç¨€ç–æ¿€æ´»ä¸åŒçš„ä¸“å®¶ç½‘ç»œï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚çŸ¥è¯†æ„ŸçŸ¥é€‚é…æ¨¡å—åˆ©ç”¨é¢†åŸŸæœ¬ä½“æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œå¹¶å°†çŸ¥è¯†å›¾è°±çš„ä¿¡æ¯èå…¥åˆ°æ¨¡å‹çš„è¡¨ç¤ºå­¦ä¹ ä¸­ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SciGPTåœ¨ScienceBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨åºåˆ—æ ‡æ³¨ã€ç”Ÿæˆå’Œæ¨ç†ç­‰æ ¸å¿ƒç§‘å­¦ä»»åŠ¡ä¸Šä¼˜äºGPT-4oã€‚æ­¤å¤–ï¼ŒSciGPTåœ¨æœªè§è¿‡çš„ç§‘å­¦ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œè¯æ˜äº†å…¶åœ¨AIè¾…åŠ©ç§‘å­¦å‘ç°æ–¹é¢çš„æ½œåŠ›ã€‚SMoEæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿå°†å¤„ç†32000 tokené•¿æ–‡æ¡£çš„å†…å­˜æ¶ˆè€—é™ä½55%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SciGPTå¯åº”ç”¨äºå¤šä¸ªç§‘å­¦é¢†åŸŸï¼Œä¾‹å¦‚ç”Ÿç‰©åŒ»å­¦ã€åŒ–å­¦ã€ææ–™ç§‘å­¦ç­‰ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å¿«é€Ÿç†è§£å’Œç»¼åˆæ–‡çŒ®çŸ¥è¯†ï¼ŒåŠ é€Ÿç§‘å­¦å‘ç°è¿‡ç¨‹ã€‚å®ƒè¿˜å¯ä»¥ç”¨äºæ„å»ºæ™ºèƒ½ç§‘ç ”åŠ©æ‰‹ï¼Œè¾…åŠ©ç§‘ç ”äººå‘˜è¿›è¡Œæ–‡çŒ®æ£€ç´¢ã€çŸ¥è¯†æŠ½å–ã€å‡è®¾ç”Ÿæˆç­‰ä»»åŠ¡ï¼Œæå‡ç§‘ç ”æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Scientific literature is growing exponentially, creating a critical bottleneck for researchers to efficiently synthesize knowledge. While general-purpose Large Language Models (LLMs) show potential in text processing, they often fail to capture scientific domain-specific nuances (e.g., technical jargon, methodological rigor) and struggle with complex scientific tasks, limiting their utility for interdisciplinary research. To address these gaps, this paper presents SciGPT, a domain-adapted foundation model for scientific literature understanding and ScienceBench, an open source benchmark tailored to evaluate scientific LLMs.
>   Built on the Qwen3 architecture, SciGPT incorporates three key innovations: (1) low-cost domain distillation via a two-stage pipeline to balance performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention mechanism that cuts memory consumption by 55\% for 32,000-token long-document reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to bridge interdisciplinary knowledge gaps.
>   Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in core scientific tasks including sequence labeling, generation, and inference. It also exhibits strong robustness in unseen scientific tasks, validating its potential to facilitate AI-augmented scientific discovery.

