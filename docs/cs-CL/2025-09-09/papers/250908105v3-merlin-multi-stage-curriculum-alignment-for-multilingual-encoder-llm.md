---
layout: default
title: MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder-LLM Integration in Cross-Lingual Reasoning
---

# MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder-LLM Integration in Cross-Lingual Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08105" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08105v3</a>
  <a href="https://arxiv.org/pdf/2509.08105.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08105v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08105v3', 'MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder-LLM Integration in Cross-Lingual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kosei Uemura, David GuzmÃ¡n, Quang Phuoc Nguyen, Jesujoba Oluwadara Alabi, En-shiun Annie Lee, David Ifeoluwa Adelani

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09 (æ›´æ–°: 2025-11-10)

**å¤‡æ³¨**: under submission

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MERLINï¼šå¤šé˜¶æ®µè¯¾ç¨‹å¯¹é½æ¡†æ¶ï¼Œæå‡è·¨è¯­è¨€æ¨ç†ä¸­å¤šè¯­è¨€ç¼–ç å™¨-LLMé›†æˆæ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨è¯­è¨€æ¨ç†` `ä½èµ„æºè¯­è¨€` `è¯¾ç¨‹å­¦ä¹ ` `æ¨¡å‹å †å ` `DoRA` `å¤šè¯­è¨€ç¼–ç å™¨` `LLMé›†æˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¼–ç å™¨-è§£ç å™¨æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å¤šè¯­è¨€LLMçš„åº”ç”¨ã€‚
2. MERLINé‡‡ç”¨ä¸¤é˜¶æ®µæ¨¡å‹å †å å’Œè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä»é€šç”¨åŒè¯­æ•°æ®åˆ°ç‰¹å®šä»»åŠ¡æ•°æ®é€æ­¥è®­ç»ƒï¼Œå¹¶ä»…å¾®è°ƒå°‘é‡DoRAæƒé‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMERLINåœ¨AfriMGSMä¸Šæ˜¾è‘—ä¼˜äºMindMergerå’ŒGPT-4o-miniï¼Œå¹¶åœ¨MGSMå’ŒMSVAMPä¸Šå–å¾—ä¸€è‡´æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‹±è¯­æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è®¸å¤šä½èµ„æºè¯­è¨€ï¼ˆLRLï¼‰ä¸­è¿›è¡Œå¤æ‚çš„æ¨ç†ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„ç¼–ç å™¨-è§£ç å™¨æ–¹æ³•ï¼Œå¦‚LangBridgeå’ŒMindMergerï¼Œæé«˜äº†ä¸­é«˜èµ„æºè¯­è¨€çš„å‡†ç¡®æ€§ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€ä¸Šä»ç„¶å­˜åœ¨å¾ˆå¤§çš„å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†MERLINï¼Œä¸€ä¸ªä¸¤é˜¶æ®µæ¨¡å‹å †å æ¡†æ¶ï¼Œå®ƒåº”ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥â€”â€”ä»ä¸€èˆ¬çš„åŒè¯­å¹³è¡Œè¯­æ–™åˆ°ç‰¹å®šä»»åŠ¡çš„æ•°æ®â€”â€”å¹¶ä¸”åªè°ƒæ•´ä¸€å°éƒ¨åˆ†DoRAæƒé‡ã€‚åœ¨AfriMGSMåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMERLINçš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡æ¯”MindMergeræé«˜äº†+12.9ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ä¸”ä¼˜äºGPT-4o-miniã€‚å®ƒè¿˜åœ¨MGSMå’ŒMSVAMPä¸Šäº§ç”Ÿäº†æŒç»­çš„æ”¶ç›Šï¼ˆ+0.9å’Œ+2.8ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œè¯æ˜äº†å…¶åœ¨ä½èµ„æºå’Œé«˜èµ„æºç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä½èµ„æºè¯­è¨€ï¼ˆLRLsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œç°æœ‰ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚LangBridgeå’ŒMindMergerè™½ç„¶åœ¨ä¸­é«˜èµ„æºè¯­è¨€ä¸Šæœ‰æ‰€æå‡ï¼Œä½†åœ¨LRLsä¸Šä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMERLINçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä»æ˜“åˆ°éš¾åœ°è®­ç»ƒæ¨¡å‹ï¼Œé¦–å…ˆåˆ©ç”¨é€šç”¨åŒè¯­æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€æ­¥è¿‡æ¸¡åˆ°ç‰¹å®šä»»åŠ¡çš„æ•°æ®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°å­¦ä¹ è·¨è¯­è¨€çš„çŸ¥è¯†è¡¨ç¤ºï¼Œå¹¶é€‚åº”ç‰¹å®šä»»åŠ¡çš„éœ€æ±‚ã€‚åŒæ—¶ï¼Œé‡‡ç”¨DoRAï¼ˆDecomposed Low-Rank Adaptationï¼‰æ–¹æ³•ï¼Œä»…å¾®è°ƒå°‘é‡æƒé‡ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶é¿å…äº†è¿‡æ‹Ÿåˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMERLINæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¨¡å‹å †å æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨åŒè¯­å¹³è¡Œè¯­æ–™è®­ç»ƒä¸€ä¸ªè·¨è¯­è¨€ç¼–ç å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿå°†ä¸åŒè¯­è¨€çš„è¾“å…¥æ˜ å°„åˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ã€‚ç¬¬äºŒé˜¶æ®µï¼Œå°†ç¼–ç å™¨çš„è¾“å‡ºä½œä¸ºLLMçš„è¾“å…¥ï¼Œåˆ©ç”¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®è¿›è¡Œå¾®è°ƒã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥å¢åŠ è®­ç»ƒæ•°æ®çš„éš¾åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šMERLINçš„å…³é”®åˆ›æ–°åœ¨äºå¤šé˜¶æ®µè¯¾ç¨‹å¯¹é½å’ŒDoRAæƒé‡è°ƒæ•´ã€‚è¯¾ç¨‹å¯¹é½ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»é€šç”¨çŸ¥è¯†é€æ­¥å­¦ä¹ åˆ°ç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚DoRAæ–¹æ³•é€šè¿‡åˆ†è§£æƒé‡çŸ©é˜µï¼Œä»…å¾®è°ƒå°‘é‡ä½ç§©çŸ©é˜µï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶é¿å…äº†è¿‡æ‹Ÿåˆã€‚

**å…³é”®è®¾è®¡**ï¼šMERLINçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¤§è§„æ¨¡åŒè¯­å¹³è¡Œè¯­æ–™è¿›è¡Œé¢„è®­ç»ƒï¼›2) é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥å¢åŠ è®­ç»ƒæ•°æ®çš„éš¾åº¦ï¼›3) ä½¿ç”¨DoRAæ–¹æ³•ï¼Œä»…å¾®è°ƒå°‘é‡æƒé‡ï¼›4) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„è·¨è¯­è¨€è¡¨ç¤ºèƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MERLINåœ¨AfriMGSMåŸºå‡†æµ‹è¯•ä¸­ï¼Œç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡æ¯”MindMergeræé«˜äº†+12.9ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ä¸”ä¼˜äºGPT-4o-miniã€‚æ­¤å¤–ï¼Œåœ¨MGSMå’ŒMSVAMPæ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†æŒç»­çš„æ”¶ç›Šï¼ˆ+0.9å’Œ+2.8ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œè¯æ˜äº†å…¶åœ¨ä½èµ„æºå’Œé«˜èµ„æºç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒMERLINåœ¨è·¨è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MERLINæ¡†æ¶å¯åº”ç”¨äºå„ç§è·¨è¯­è¨€æ¨ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€è·¨è¯­è¨€é—®ç­”ã€å¤šè¯­è¨€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæå‡ä½èµ„æºè¯­è¨€çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ï¼Œä¿ƒè¿›å…¨çƒèŒƒå›´å†…çš„ä¿¡æ¯äº¤æµå’ŒçŸ¥è¯†å…±äº«ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ›´å¤šè¯­è¨€å’Œä»»åŠ¡ï¼Œå¹¶ä¸å…¶ä»–æŠ€æœ¯ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡è·¨è¯­è¨€æ¨ç†çš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models excel in English but still struggle with complex reasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder methods such as LangBridge and MindMerger raise accuracy on mid and high-resource languages, yet they leave a large gap on LRLs. We present MERLIN, a two-stage model-stacking framework that applies a curriculum learning strategy -- from general bilingual bitext to task-specific data -- and adapts only a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves exact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini. It also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp), demonstrating effectiveness across both low and high-resource settings.

