---
layout: default
title: Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs
---

# Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18113" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18113v1</a>
  <a href="https://arxiv.org/pdf/2509.18113.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18113v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18113v1', 'Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin Hu, Yue Kang, Guanzi Yao, Tianze Kang, Mengjie Wang, Heyao Liu

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€Promptèåˆæ¡†æ¶ï¼Œæå‡LLMåœ¨å¤šä»»åŠ¡å’Œè·¨é¢†åŸŸåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šä»»åŠ¡å­¦ä¹ ` `è·¨é¢†åŸŸé€‚åº”` `Promptå·¥ç¨‹` `åŠ¨æ€Promptèåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–å›ºå®šPromptæ¨¡æ¿ï¼Œéš¾ä»¥é€‚åº”å¤šä»»åŠ¡å’Œè·¨é¢†åŸŸåœºæ™¯ä¸‹çš„è¯­ä¹‰å·®å¼‚ã€‚
2. æå‡ºåŠ¨æ€Promptèåˆæ¡†æ¶ï¼Œé€šè¿‡Promptæ± å’Œä»»åŠ¡æ„ŸçŸ¥è°ƒåº¦ç­–ç•¥ï¼ŒåŠ¨æ€ç»„åˆå’Œå¯¹é½Promptã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨è¯­è¨€ç†è§£å’ŒçŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä»»åŠ¡å’Œè·¨é¢†åŸŸç¯å¢ƒä¸­å¸¸è§çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚ä¸ä¾èµ–å›ºå®šPromptæ¨¡æ¿çš„SPoTç­‰ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰åŠ¨æ€Promptè°ƒåº¦æœºåˆ¶ã€‚é€šè¿‡å¼•å…¥Promptæ± å’Œä»»åŠ¡æ„ŸçŸ¥è°ƒåº¦ç­–ç•¥ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŠ¨æ€åœ°ç»„åˆå’Œå¯¹é½ä¸åŒä»»åŠ¡çš„Promptï¼Œä»è€Œå¢å¼ºæ¨¡å‹æ•æ‰è·¨ä»»åŠ¡è¯­ä¹‰å·®å¼‚çš„èƒ½åŠ›ã€‚åœ¨Promptèåˆè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åˆ©ç”¨ä»»åŠ¡åµŒå…¥å’Œé—¨æ§æœºåˆ¶æ¥ç²¾ç»†åœ°æ§åˆ¶Promptä¿¡å·ï¼Œç¡®ä¿Promptå†…å®¹ä¸ä»»åŠ¡ç‰¹å®šéœ€æ±‚å¯¹é½ï¼Œå¹¶æ„å»ºè·¨ä»»åŠ¡çš„çµæ´»å…±äº«è·¯å¾„ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„ä¼˜åŒ–ç›®æ ‡ä¾§é‡äºè”åˆå¤šä»»åŠ¡å­¦ä¹ ï¼Œå¹¶ç»“åˆè‡ªåŠ¨å­¦ä¹ ç­–ç•¥æ¥è°ƒåº¦æƒé‡ï¼Œæœ‰æ•ˆç¼“è§£ä»»åŠ¡å¹²æ‰°å’Œè´Ÿè¿ç§»ã€‚é€šè¿‡ä¸€ç³»åˆ—æ•æ„Ÿæ€§å®éªŒï¼ŒéªŒè¯äº†è¯¥æœºåˆ¶åœ¨ä¿æŒæ¨¡å‹ç¨³å®šæ€§å’Œå¢å¼ºè¿ç§»èƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥Promptè°ƒåº¦æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å„ç§è¯­è¨€ç†è§£å’ŒçŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨ç»Ÿä¸€å¤šä»»åŠ¡å»ºæ¨¡å’Œè·¨é¢†åŸŸé€‚åº”æ–¹é¢çš„é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä»»åŠ¡å’Œè·¨é¢†åŸŸåœºæ™¯ä¸‹ï¼Œç”±äºä»»åŠ¡å·®å¼‚æ€§å’Œé¢†åŸŸçŸ¥è¯†çš„å·®å¼‚æ€§ï¼Œæ³›åŒ–èƒ½åŠ›å—åˆ°é™åˆ¶ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚SPoTï¼Œä¾èµ–äºå›ºå®šçš„Promptæ¨¡æ¿ï¼Œæ— æ³•æœ‰æ•ˆåœ°æ•æ‰å’Œåˆ©ç”¨ä¸åŒä»»åŠ¡ä¹‹é—´çš„è¯­ä¹‰å…³è”ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥åŠ¨æ€Promptèåˆæœºåˆ¶ï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªPromptæ± ï¼Œå¹¶æ ¹æ®ä»»åŠ¡çš„ç‰¹å®šéœ€æ±‚åŠ¨æ€åœ°é€‰æ‹©ã€ç»„åˆå’Œè°ƒæ•´Promptã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æ›´å¥½åœ°æ•æ‰ä»»åŠ¡é—´çš„è¯­ä¹‰å·®å¼‚ï¼Œå¹¶ä¿ƒè¿›çŸ¥è¯†çš„æœ‰æ•ˆè¿ç§»ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¤šä»»åŠ¡å’Œè·¨é¢†åŸŸç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) Promptæ± ï¼šå­˜å‚¨å¤šä¸ªPromptæ¨¡æ¿ï¼Œæ¯ä¸ªPromptæ¨¡æ¿å¯èƒ½é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡æˆ–é¢†åŸŸã€‚2) ä»»åŠ¡åµŒå…¥æ¨¡å—ï¼šå°†æ¯ä¸ªä»»åŠ¡æ˜ å°„åˆ°ä¸€ä¸ªä½ç»´å‘é‡ç©ºé—´ï¼Œç”¨äºè¡¨ç¤ºä»»åŠ¡çš„è¯­ä¹‰ä¿¡æ¯ã€‚3) Promptè°ƒåº¦æ¨¡å—ï¼šæ ¹æ®ä»»åŠ¡åµŒå…¥ï¼Œä»Promptæ± ä¸­é€‰æ‹©åˆé€‚çš„Promptï¼Œå¹¶åŠ¨æ€åœ°è°ƒæ•´Promptçš„æƒé‡ã€‚4) Promptèåˆæ¨¡å—ï¼šå°†é€‰æ‹©çš„Promptè¿›è¡Œèåˆï¼Œç”Ÿæˆæœ€ç»ˆçš„Promptï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹çš„å­¦ä¹ ã€‚5) å¤šä»»åŠ¡å­¦ä¹ æ¨¡å—ï¼šåˆ©ç”¨èåˆåçš„Promptï¼Œå¯¹å¤šä¸ªä»»åŠ¡è¿›è¡Œè”åˆè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºåŠ¨æ€Promptè°ƒåº¦æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šPromptæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡çš„ç‰¹å®šéœ€æ±‚åŠ¨æ€åœ°è°ƒæ•´Promptï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰ä»»åŠ¡é—´çš„è¯­ä¹‰å·®å¼‚ï¼Œå¹¶ä¿ƒè¿›çŸ¥è¯†çš„æœ‰æ•ˆè¿ç§»ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨å­¦ä¹ ç­–ç•¥æ¥è°ƒåº¦ä»»åŠ¡æƒé‡ï¼Œæœ‰æ•ˆç¼“è§£ä»»åŠ¡å¹²æ‰°å’Œè´Ÿè¿ç§»ã€‚

**å…³é”®è®¾è®¡**ï¼š1) Promptæ¸©åº¦å‚æ•°ï¼šç”¨äºæ§åˆ¶Prompté€‰æ‹©çš„å¤šæ ·æ€§ï¼Œè¾ƒé«˜çš„æ¸©åº¦å€¼ä¼šå¢åŠ é€‰æ‹©ä¸åŒPromptçš„æ¦‚ç‡ã€‚2) ä»»åŠ¡åµŒå…¥ï¼šä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰å¯¹ä»»åŠ¡æè¿°è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°ä»»åŠ¡åµŒå…¥ã€‚3) é—¨æ§æœºåˆ¶ï¼šç”¨äºæ§åˆ¶ä¸åŒPromptçš„æƒé‡ï¼Œæ ¹æ®ä»»åŠ¡åµŒå…¥åŠ¨æ€åœ°è°ƒæ•´æƒé‡ã€‚4) æŸå¤±å‡½æ•°ï¼šé‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ çš„æŸå¤±å‡½æ•°ï¼Œå¹¶ç»“åˆè‡ªåŠ¨å­¦ä¹ ç­–ç•¥æ¥è°ƒåº¦ä»»åŠ¡æƒé‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åŠ¨æ€Promptè°ƒåº¦æ–¹æ³•åœ¨å¤šä¸ªè¯­è¨€ç†è§£å’ŒçŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨XXXæ•°æ®é›†ä¸Šï¼Œç›¸æ¯”äºåŸºçº¿æ¨¡å‹SPoTï¼Œè¯¥æ–¹æ³•å–å¾—äº†X%çš„æ€§èƒ½æå‡ã€‚æ•æ„Ÿæ€§å®éªŒéªŒè¯äº†Promptæ¸©åº¦å‚æ•°å’Œä»»åŠ¡æ•°é‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤šä»»åŠ¡å’Œè·¨é¢†åŸŸçŸ¥è¯†è¿ç§»çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šæ™ºèƒ½å®¢æœã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€çŸ¥è¯†å›¾è°±é—®ç­”ç­‰ã€‚é€šè¿‡åŠ¨æ€Promptèåˆï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é€‚åº”æ€§å’Œæ€§èƒ½ï¼Œé™ä½æ¨¡å‹å¼€å‘å’Œç»´æŠ¤æˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„æœªæ¥å‘å±•å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This study addresses the generalization limitations commonly observed in large language models under multi-task and cross-domain settings. Unlike prior methods such as SPoT, which depends on fixed prompt templates, our study introduces a unified multi-task learning framework with dynamic prompt scheduling mechanism. By introducing a prompt pool and a task-aware scheduling strategy, the method dynamically combines and aligns prompts for different tasks. This enhances the model's ability to capture semantic differences across tasks. During prompt fusion, the model uses task embeddings and a gating mechanism to finely control the prompt signals. This ensures alignment between prompt content and task-specific demands. At the same time, it builds flexible sharing pathways across tasks. In addition, the proposed optimization objective centers on joint multi-task learning. It incorporates an automatic learning strategy for scheduling weights, which effectively mitigates task interference and negative transfer. To evaluate the effectiveness of the method, a series of sensitivity experiments were conducted. These experiments examined the impact of prompt temperature parameters and task number variation. The results confirm the advantages of the proposed mechanism in maintaining model stability and enhancing transferability. Experimental findings show that the prompt scheduling method significantly improves performance on a range of language understanding and knowledge reasoning tasks. These results fully demonstrate its applicability and effectiveness in unified multi-task modeling and cross-domain adaptation.

