---
layout: default
title: Evolution and compression in LLMs: On the emergence of human-aligned categorization
---

# Evolution and compression in LLMs: On the emergence of human-aligned categorization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08093" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08093v3</a>
  <a href="https://arxiv.org/pdf/2509.08093.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08093v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08093v3', 'Evolution and compression in LLMs: On the emergence of human-aligned categorization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nathaniel Imel, Noga Zaslavsky

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09 (æ›´æ–°: 2025-12-01)

**å¤‡æ³¨**: Accepted at CogInterp: Interpreting Cognition in Deep Learning Models Workshop at NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡è¿­ä»£å­¦ä¹ è¿›åŒ–å‡ºä¸äººç±»å¯¹é½çš„è¯­ä¹‰åˆ†ç±»ç³»ç»Ÿã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯­ä¹‰åˆ†ç±»` `ä¿¡æ¯ç“¶é¢ˆ` `æ–‡åŒ–æ¼”åŒ–` `è¿­ä»£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹æœªé’ˆå¯¹ä¿¡æ¯ç“¶é¢ˆä¼˜åŒ–ï¼Œå¯¼è‡´å…¶è¯­ä¹‰ç³»ç»Ÿä¸äººç±»å¯¹é½ç¨‹åº¦æœªçŸ¥ã€‚
2. è®ºæ–‡æå‡ºè¿­ä»£ä¸Šä¸‹æ–‡è¯­è¨€å­¦ä¹ (IICLL)æ–¹æ³•ï¼Œæ¨¡æ‹ŸLLMä¸­ä¼ªé¢œè‰²å‘½åç³»ç»Ÿçš„æ–‡åŒ–æ¼”å˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMå¯ä»¥é€šè¿‡IICLLè¿­ä»£åœ°ä¼˜åŒ–è¯­ä¹‰ç³»ç»Ÿï¼Œä½¿å…¶æ›´æ¥è¿‘äººç±»çš„IBæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§é‡è¯æ®è¡¨æ˜ï¼Œäººç±»çš„è¯­ä¹‰ç±»åˆ«ç³»ç»Ÿé€šè¿‡ä¿¡æ¯ç“¶é¢ˆ(IB)çš„å¤æ‚æ€§-å‡†ç¡®æ€§æƒè¡¡å®ç°äº†è¿‘ä¹æœ€ä¼˜çš„å‹ç¼©ã€‚å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„è®­ç»ƒç›®æ ‡å¹¶éå¦‚æ­¤ï¼Œè¿™å¼•å‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šLLMæ˜¯å¦èƒ½å¤Ÿè¿›åŒ–å‡ºé«˜æ•ˆçš„ã€ä¸äººç±»å¯¹é½çš„è¯­ä¹‰ç³»ç»Ÿï¼Ÿä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä»¥é¢œè‰²åˆ†ç±»ä¸ºé‡ç‚¹â€”â€”è¿™æ˜¯è®¤çŸ¥ç±»åˆ«ç†è®ºçš„å…³é”®è¯•éªŒå°ï¼Œæ‹¥æœ‰éå¸¸ä¸°å¯Œçš„äººç±»æ•°æ®â€”â€”å¹¶ä½¿ç”¨LLMå¤åˆ¶äº†ä¸¤é¡¹æœ‰å½±å“åŠ›çš„äººç±»ç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹è‹±è¯­é¢œè‰²å‘½åç ”ç©¶ï¼Œè¡¨æ˜LLMåœ¨å¤æ‚æ€§å’Œè‹±è¯­å¯¹é½æ–¹é¢å·®å¼‚å¾ˆå¤§ï¼Œå…¶ä¸­æ›´å¤§çš„æŒ‡ä»¤è°ƒæ•´æ¨¡å‹å®ç°äº†æ›´å¥½çš„å¯¹é½å’ŒIBæ•ˆç‡ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æµ‹è¯•è¿™äº›LLMæ˜¯å¦ä»…ä»…æ¨¡ä»¿å…¶è®­ç»ƒæ•°æ®ä¸­çš„æ¨¡å¼ï¼Œæˆ–è€…å®é™…ä¸Šè¡¨ç°å‡ºç±»ä¼¼äººç±»çš„ã€è¶‹å‘äºIBæ•ˆç‡çš„å½’çº³åç½®ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç§ç§°ä¸ºè¿­ä»£ä¸Šä¸‹æ–‡è¯­è¨€å­¦ä¹ (IICLL)çš„æ–¹æ³•ï¼Œåœ¨LLMä¸­æ¨¡æ‹Ÿäº†ä¼ªé¢œè‰²å‘½åç³»ç»Ÿçš„æ–‡åŒ–æ¼”å˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸äººç±»ç±»ä¼¼ï¼ŒLLMè¿­ä»£åœ°å°†æœ€åˆéšæœºçš„ç³»ç»Ÿé‡æ„ä¸ºæ›´é«˜çš„IBæ•ˆç‡ã€‚ç„¶è€Œï¼Œåªæœ‰å…·æœ‰æœ€å¼ºä¸Šä¸‹æ–‡èƒ½åŠ›çš„æ¨¡å‹(Gemini 2.0)èƒ½å¤Ÿé‡ç°äººç±»è§‚å¯Ÿåˆ°çš„å¹¿æ³›çš„è¿‘ä¹æœ€ä¼˜çš„IBæƒè¡¡ï¼Œè€Œå…¶ä»–æœ€å…ˆè¿›çš„æ¨¡å‹åˆ™æ”¶æ•›åˆ°ä½å¤æ‚åº¦çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œä¸äººç±»è¯­ä¹‰æ•ˆç‡çš„åŸºæœ¬åŸç†ç›¸åŒï¼Œä¸äººç±»å¯¹é½çš„è¯­ä¹‰ç±»åˆ«å¯ä»¥åœ¨LLMä¸­æ¶Œç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ˜¯å¦èƒ½å¤Ÿåƒäººç±»ä¸€æ ·ï¼Œé€šè¿‡å­¦ä¹ å’Œæ¼”åŒ–ï¼Œå½¢æˆé«˜æ•ˆä¸”ä¸äººç±»è®¤çŸ¥å¯¹é½çš„è¯­ä¹‰åˆ†ç±»ç³»ç»Ÿã€‚ç°æœ‰LLMçš„è®­ç»ƒç›®æ ‡å¹¶éç›´æ¥é’ˆå¯¹è¯­ä¹‰å‹ç¼©å’Œäººç±»å¯¹é½ï¼Œå› æ­¤å…¶è¯­ä¹‰ç³»ç»Ÿçš„æ•ˆç‡å’Œå¯¹é½ç¨‹åº¦æ˜¯æœªçŸ¥çš„ï¼Œéœ€è¦è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡æ‹Ÿäººç±»æ–‡åŒ–æ¼”åŒ–çš„è¿‡ç¨‹ï¼Œé€šè¿‡è¿­ä»£åœ°è®©LLMå­¦ä¹ å’Œæ”¹è¿›é¢œè‰²å‘½åç³»ç»Ÿï¼Œè§‚å¯Ÿå…¶æ˜¯å¦ä¼šè‡ªå‘åœ°è¶‹å‘äºä¿¡æ¯ç“¶é¢ˆ(IB)ç†è®ºæ‰€é¢„æµ‹çš„ä¼˜åŒ–çŠ¶æ€ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»è¯­è¨€çš„æ¼”åŒ–è¿‡ç¨‹ï¼Œå¯ä»¥æ­ç¤ºLLMæ˜¯å¦å…·æœ‰ç±»ä¼¼äººç±»çš„å½’çº³åç½®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š1) è‹±è¯­é¢œè‰²å‘½åç ”ç©¶ï¼šè¯„ä¼°LLMåœ¨è‹±è¯­é¢œè‰²å‘½åä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¡¡é‡å…¶å¤æ‚æ€§å’Œä¸äººç±»çš„å¯¹é½ç¨‹åº¦ã€‚2) è¿­ä»£ä¸Šä¸‹æ–‡è¯­è¨€å­¦ä¹ (IICLL)ï¼šé€šè¿‡è¿­ä»£åœ°è®©LLMå­¦ä¹ å’Œæ”¹è¿›ä¼ªé¢œè‰²å‘½åç³»ç»Ÿï¼Œæ¨¡æ‹Ÿæ–‡åŒ–æ¼”åŒ–è¿‡ç¨‹ã€‚IICLLåŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼ša) åˆå§‹åŒ–ä¸€ä¸ªéšæœºçš„é¢œè‰²å‘½åç³»ç»Ÿã€‚b) ä½¿ç”¨LLMæ ¹æ®ä¸Šä¸‹æ–‡å­¦ä¹ è¯¥ç³»ç»Ÿã€‚c) ä½¿ç”¨å­¦ä¹ åçš„ç³»ç»Ÿç”Ÿæˆæ–°çš„é¢œè‰²å‘½åæ•°æ®ã€‚d) é‡å¤æ­¥éª¤bå’Œcï¼Œè¿›è¡Œå¤šæ¬¡è¿­ä»£ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è¿­ä»£ä¸Šä¸‹æ–‡è¯­è¨€å­¦ä¹ (IICLL)æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡æ‹Ÿæ–‡åŒ–æ¼”åŒ–è¿‡ç¨‹çš„åˆ›æ–°æ–¹æ³•ï¼Œå¯ä»¥ç”¨äºç ”ç©¶LLMçš„è¯­ä¹‰å­¦ä¹ å’Œæ¼”åŒ–èƒ½åŠ›ã€‚é€šè¿‡IICLLï¼Œç ”ç©¶äººå‘˜å¯ä»¥è§‚å¯ŸLLMæ˜¯å¦å…·æœ‰ç±»ä¼¼äººç±»çš„å½’çº³åç½®ï¼Œä»¥åŠå…¶è¯­ä¹‰ç³»ç»Ÿæ˜¯å¦ä¼šè‡ªå‘åœ°è¶‹å‘äºä¼˜åŒ–çŠ¶æ€ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨IICLLä¸­ï¼Œå…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) é¢œè‰²ç©ºé—´çš„è¡¨ç¤ºï¼šä½¿ç”¨CIE Labé¢œè‰²ç©ºé—´æ¥è¡¨ç¤ºé¢œè‰²ã€‚2) é¢œè‰²å‘½åç³»ç»Ÿçš„è¡¨ç¤ºï¼šä½¿ç”¨ä¸€ä¸ªæ˜ å°„è¡¨ï¼Œå°†é¢œè‰²æ˜ å°„åˆ°åç§°ã€‚3) LLMçš„å­¦ä¹ æ–¹å¼ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œè®©LLMæ ¹æ®ä¸Šä¸‹æ–‡å­¦ä¹ é¢œè‰²å‘½åç³»ç»Ÿã€‚4) è¿­ä»£æ¬¡æ•°ï¼šè¿›è¡Œå¤šæ¬¡è¿­ä»£ï¼Œä»¥è§‚å¯ŸLLMçš„è¯­ä¹‰ç³»ç»Ÿæ˜¯å¦ä¼šè¶‹å‘äºä¼˜åŒ–çŠ¶æ€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¾ƒå¤§çš„æŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨è‹±è¯­é¢œè‰²å‘½åä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¥½çš„å¯¹é½å’ŒIBæ•ˆç‡ã€‚é€šè¿‡IICLLï¼ŒLLMèƒ½å¤Ÿè¿­ä»£åœ°ä¼˜åŒ–è¯­ä¹‰ç³»ç»Ÿï¼Œä½¿å…¶æ›´æ¥è¿‘äººç±»çš„IBæ•ˆç‡ã€‚Gemini 2.0æ¨¡å‹èƒ½å¤Ÿé‡ç°äººç±»è§‚å¯Ÿåˆ°çš„å¹¿æ³›çš„è¿‘ä¹æœ€ä¼˜çš„IBæƒè¡¡ï¼Œè€Œå…¶ä»–æ¨¡å‹åˆ™æ”¶æ•›åˆ°ä½å¤æ‚åº¦çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä½¿å…¶æ›´ç¬¦åˆäººç±»è®¤çŸ¥ä¹ æƒ¯ã€‚é€šè¿‡ä¼˜åŒ–LLMçš„è¯­ä¹‰è¡¨ç¤ºï¼Œå¯ä»¥æé«˜å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æ£€ç´¢å’Œæœºå™¨ç¿»è¯‘ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºç†è§£äººç±»è¯­è¨€çš„æ¼”åŒ–æœºåˆ¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Converging evidence suggests that human systems of semantic categories achieve near-optimal compression via the Information Bottleneck (IB) complexity-accuracy tradeoff. Large language models (LLMs) are not trained for this objective, which raises the question: are LLMs capable of evolving efficient human-aligned semantic systems? To address this question, we focus on color categorization -- a key testbed of cognitive theories of categorization with uniquely rich human data -- and replicate with LLMs two influential human studies. First, we conduct an English color-naming study, showing that LLMs vary widely in their complexity and English-alignment, with larger instruction-tuned models achieving better alignment and IB-efficiency. Second, to test whether these LLMs simply mimic patterns in their training data or actually exhibit a human-like inductive bias toward IB-efficiency, we simulate cultural evolution of pseudo color-naming systems in LLMs via a method we refer to as Iterated in-Context Language Learning (IICLL). We find that akin to humans, LLMs iteratively restructure initially random systems towards greater IB-efficiency. However, only a model with strongest in-context capabilities (Gemini 2.0) is able to recapitulate the wide range of near-optimal IB-tradeoffs observed in humans, while other state-of-the-art models converge to low-complexity solutions. These findings demonstrate how human-aligned semantic categories can emerge in LLMs via the same fundamental principle that underlies semantic efficiency in humans.

