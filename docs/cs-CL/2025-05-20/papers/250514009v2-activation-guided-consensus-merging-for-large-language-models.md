---
layout: default
title: Activation-Guided Consensus Merging for Large Language Models
---

# Activation-Guided Consensus Merging for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14009" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14009v2</a>
  <a href="https://arxiv.org/pdf/2505.14009.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14009v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14009v2', 'Activation-Guided Consensus Merging for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxuan Yao, Shuqi Liu, Zehua Liu, Qintong Li, Mingyang Liu, Xiongwei Han, Zhijiang Guo, Han Wu, Linqi Song

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-11-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¿€æ´»å¼•å¯¼å…±è¯†åˆå¹¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸ç¨³å®šæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹åˆå¹¶` `æ¿€æ´»å¼•å¯¼` `äº’ä¿¡æ¯` `æ¨ç†èƒ½åŠ›` `æ•ˆç‡æå‡` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è®­ç»ƒå’Œæç¤ºæ–¹æ³•åœ¨æ•ˆç‡å’Œç¨³å®šæ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œæ— æ³•æœ‰æ•ˆæ•´åˆä¸åŒå¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¿€æ´»å¼•å¯¼å…±è¯†åˆå¹¶ï¼ˆACMï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ¿€æ´»ä¹‹é—´çš„äº’ä¿¡æ¯æ¥ç¡®å®šå±‚ç‰¹å®šçš„åˆå¹¶ç³»æ•°ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒACMåœ¨Qwen-7Bæ¨¡å‹ä¸­å®ç°äº†55.3%çš„å“åº”é•¿åº¦å‡å°‘ï¼ŒåŒæ—¶æ¨ç†å‡†ç¡®æ€§æé«˜äº†1.3ä¸ªç™¾åˆ†ç‚¹ï¼Œè¡¨ç°ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œç ”ç©¶è€…è¶Šæ¥è¶Šå…³æ³¨å°†ç³»ç»Ÿ2çš„æ¨ç†èƒ½åŠ›ä¸ç³»ç»Ÿ1çš„æ•ˆç‡ç›¸ç»“åˆã€‚ç°æœ‰çš„åŸºäºè®­ç»ƒå’Œæç¤ºçš„æ–¹æ³•åœ¨æ•ˆç‡å’Œç¨³å®šæ€§æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè€Œæ¨¡å‹åˆå¹¶ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„ç­–ç•¥ï¼Œå¯ä»¥å°†ä¸åŒå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ ·åŒ–èƒ½åŠ›æ•´åˆä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ¨¡å‹åˆå¹¶æ–¹æ³•å¾€å¾€å‡è®¾å„å±‚çš„é‡è¦æ€§å‡åŒ€ï¼Œå¿½è§†äº†ç¥ç»å…ƒç»„ä»¶å›ºæœ‰çš„åŠŸèƒ½å¼‚è´¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†æ¿€æ´»å¼•å¯¼å…±è¯†åˆå¹¶ï¼ˆACMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„åˆå¹¶æ¡†æ¶ï¼Œé€šè¿‡é¢„è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹æ¿€æ´»ä¹‹é—´çš„äº’ä¿¡æ¯æ¥ç¡®å®šå±‚ç‰¹å®šçš„åˆå¹¶ç³»æ•°ã€‚ACMæœ‰æ•ˆåœ°ä¿ç•™äº†ä»»åŠ¡ç‰¹å®šçš„èƒ½åŠ›ï¼Œè€Œæ— éœ€æ¢¯åº¦è®¡ç®—æˆ–é¢å¤–è®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒACMåœ¨é•¿çŸ­åˆå¹¶ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åˆå¹¶æ–¹æ³•åœ¨å±‚é‡è¦æ€§å‡è®¾ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¿½è§†äº†ç¥ç»ç½‘ç»œå„å±‚åŠŸèƒ½çš„å¼‚è´¨æ€§ï¼Œå¯¼è‡´åˆå¹¶æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæ¿€æ´»å¼•å¯¼å…±è¯†åˆå¹¶ï¼ˆACMï¼‰æ¡†æ¶ï¼Œé€šè¿‡åˆ†æé¢„è®­ç»ƒä¸å¾®è°ƒæ¨¡å‹æ¿€æ´»ä¹‹é—´çš„äº’ä¿¡æ¯ï¼ŒåŠ¨æ€ç¡®å®šæ¯å±‚çš„åˆå¹¶ç³»æ•°ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ•´åˆæ¨¡å‹èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šACMæ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¿€æ´»æå–ã€äº’ä¿¡æ¯è®¡ç®—å’Œå±‚ç‰¹å®šåˆå¹¶ç³»æ•°ç”Ÿæˆç­‰ä¸»è¦æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„åˆå¹¶æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šACMçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶å±‚ç‰¹å®šçš„åˆå¹¶ç³»æ•°è®¡ç®—æ–¹æ³•ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„å‡åŒ€å‡è®¾ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™ä»»åŠ¡ç‰¹å®šèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒACMä¸éœ€è¦é¢å¤–çš„æ¢¯åº¦è®¡ç®—æˆ–è®­ç»ƒï¼Œåˆ©ç”¨äº’ä¿¡æ¯ä½œä¸ºåˆå¹¶ç³»æ•°çš„ä¾æ®ï¼Œç¡®ä¿äº†åˆå¹¶è¿‡ç¨‹çš„é«˜æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ACMçš„TIES-Mergingåœ¨Qwen-7Bæ¨¡å‹ä¸­å®ç°äº†55.3%çš„å“åº”é•¿åº¦å‡å°‘ï¼ŒåŒæ—¶æ¨ç†å‡†ç¡®æ€§æé«˜äº†1.3ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†ACMçš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œå“åº”æ•ˆç‡ã€‚æœªæ¥ï¼ŒACMæ¡†æ¶æœ‰æœ›åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œè·¨é¢†åŸŸæ¨¡å‹åˆå¹¶ä¸­å‘æŒ¥æ›´å¤§ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent research has increasingly focused on reconciling the reasoning capabilities of System 2 with the efficiency of System 1. While existing training-based and prompt-based approaches face significant challenges in terms of efficiency and stability, model merging emerges as a promising strategy to integrate the diverse capabilities of different Large Language Models (LLMs) into a unified model. However, conventional model merging methods often assume uniform importance across layers, overlooking the functional heterogeneity inherent in neural components. To address this limitation, we propose \textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}), a plug-and-play merging framework that determines layer-specific merging coefficients based on mutual information between activations of pre-trained and fine-tuned models. ACM effectively preserves task-specific capabilities without requiring gradient computations or additional training. Extensive experiments on Long-to-Short (L2S) and general merging tasks demonstrate that ACM consistently outperforms all baseline methods. For instance, in the case of Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\% } reduction in response length while simultaneously improving reasoning accuracy by \textbf{1.3} points.

