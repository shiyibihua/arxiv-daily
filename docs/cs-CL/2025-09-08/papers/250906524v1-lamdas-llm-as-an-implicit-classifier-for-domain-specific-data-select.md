---
layout: default
title: LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection
---

# LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.06524" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.06524v1</a>
  <a href="https://arxiv.org/pdf/2509.06524.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.06524v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.06524v1', 'LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jian Wu, Hang Yu, Bingchang Liu, Wenjie Yang, Peng Di, Jianguo Li, Yue Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LAMDASï¼šåˆ©ç”¨LLMä½œä¸ºéšå¼åˆ†ç±»å™¨è¿›è¡Œé¢†åŸŸæ•°æ®é€‰æ‹©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®é€‰æ‹©` `é¢†åŸŸè‡ªé€‚åº”` `éšå¼åˆ†ç±»` `å•ç±»åˆ†ç±»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é¢†åŸŸæ•°æ®ç¨€ç¼ºæ˜¯LLMé¢†åŸŸé€‚é…çš„å…³é”®ç“¶é¢ˆï¼Œç›´æ¥ä½¿ç”¨å¤§é‡æœªæ¸…æ´—æ•°æ®ä¼šå¼•å…¥å™ªå£°ã€‚
2. LAMDASå°†LLMä½œä¸ºéšå¼åˆ†ç±»å™¨ï¼ŒæŠŠæ•°æ®é€‰æ‹©è½¬åŒ–ä¸ºå•ç±»åˆ†ç±»é—®é¢˜ï¼Œæ— éœ€æ˜¾å¼ç‰¹å¾å·¥ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLAMDASä½¿ç”¨å°‘é‡æ•°æ®è¶…è¶Šå…¨æ•°æ®è®­ç»ƒï¼Œå¹¶ä¼˜äºå¤šä¸ªSOTAåŸºçº¿ï¼Œå…¼é¡¾æ€§èƒ½ä¸æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åº”ç”¨äºç‰¹å®šé¢†åŸŸæ—¶ï¼Œå¸¸å¸¸é¢ä¸´é«˜è´¨é‡äººå·¥æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„ç“¶é¢ˆã€‚è™½ç„¶å¤§é‡æœªç»æ£€æŸ¥çš„æ•°æ®å”¾æ‰‹å¯å¾—ï¼Œä½†ç›²ç›®åœ°ä½¿ç”¨å®ƒä»¬è¿›è¡Œå¾®è°ƒå¯èƒ½ä¼šå¼•å…¥å™ªå£°å¹¶é™ä½æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ˜ç•¥æ€§çš„æ•°æ®é€‰æ‹©è‡³å…³é‡è¦ï¼Œè¿™éœ€è¦ä¸€ç§æ—¢å‡†ç¡®åˆé«˜æ•ˆçš„æ–¹æ³•ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¯åˆ†ä¸ºåŸºäºç›¸ä¼¼æ€§çš„æ–¹æ³•å’Œç›´æ¥ä¼˜åŒ–æ–¹æ³•ï¼Œéš¾ä»¥åŒæ—¶å®ç°è¿™äº›ç›®æ ‡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•LAMDASï¼ˆLLM As an iMplicit classifier for domain-specific DAta Selectionï¼‰ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„LLMæœ¬èº«ä½œä¸ºéšå¼åˆ†ç±»å™¨ï¼Œä»è€Œç»•è¿‡æ˜¾å¼çš„ç‰¹å¾å·¥ç¨‹å’Œè®¡ç®—å¯†é›†å‹çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚LAMDASå°†æ•°æ®é€‰æ‹©é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªå•ç±»åˆ†ç±»é—®é¢˜ï¼Œè¯†åˆ«å‡ºâ€œå±äºâ€ç”±å°å‹å‚è€ƒæ•°æ®é›†å®šä¹‰çš„ç›®æ ‡é¢†åŸŸçš„æ•°æ®ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLAMDASä¸ä»…ä½¿ç”¨ä¸€å°éƒ¨åˆ†æ•°æ®å°±è¶…è¿‡äº†å…¨æ•°æ®è®­ç»ƒçš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨å„ç§åœºæ™¯ä¸‹éƒ½ä¼˜äºä¹ä¸ªæœ€å…ˆè¿›(SOTA)çš„åŸºçº¿ã€‚æ­¤å¤–ï¼Œä¸æ‰€æœ‰è¯„ä¼°çš„åŸºçº¿ç›¸æ¯”ï¼ŒLAMDASåœ¨æ€§èƒ½æå‡å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å®ç°äº†æœ€å¼•äººæ³¨ç›®çš„å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç‰¹å®šé¢†åŸŸå†…ï¼Œé«˜è´¨é‡æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä»å¤§é‡æœªæ ‡æ³¨æ•°æ®ä¸­é«˜æ•ˆã€å‡†ç¡®åœ°é€‰æ‹©å‡ºé€‚åˆå¾®è°ƒLLMçš„æ•°æ®å­é›†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºç›¸ä¼¼åº¦çš„æ–¹æ³•å’Œç›´æ¥ä¼˜åŒ–æ–¹æ³•ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè¦ä¹ˆåœ¨å‡†ç¡®æ€§ä¸Šæœ‰æ‰€æ¬ ç¼ºï¼Œéš¾ä»¥åŒæ—¶æ»¡è¶³æ•ˆç‡å’Œæ€§èƒ½çš„è¦æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLAMDASçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é¢„è®­ç»ƒçš„LLMæœ¬èº«è§†ä¸ºä¸€ä¸ªéšå¼çš„é¢†åŸŸåˆ†ç±»å™¨ã€‚é€šè¿‡å°‘é‡çš„å‚è€ƒæ•°æ®é›†æ¥å¼•å¯¼LLMç†è§£ç›®æ ‡é¢†åŸŸï¼Œç„¶ååˆ©ç”¨LLMå¯¹å€™é€‰æ•°æ®è¿›è¡Œâ€œæ‰“åˆ†â€ï¼Œåˆ¤æ–­å…¶æ˜¯å¦å±äºç›®æ ‡é¢†åŸŸã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ˜¾å¼çš„ç‰¹å¾å·¥ç¨‹å’Œå¤æ‚çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»è€Œæé«˜äº†æ•°æ®é€‰æ‹©çš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLAMDASçš„æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1) **å‚è€ƒæ•°æ®é›†æ„å»º**ï¼šæ”¶é›†å°‘é‡é«˜è´¨é‡çš„é¢†åŸŸå†…æ•°æ®ä½œä¸ºå‚è€ƒé›†ã€‚2) **LLMæç¤ºå·¥ç¨‹**ï¼šè®¾è®¡åˆé€‚çš„æç¤ºè¯­ï¼Œå¼•å¯¼LLMç†è§£å‚è€ƒæ•°æ®é›†æ‰€ä»£è¡¨çš„é¢†åŸŸã€‚3) **å€™é€‰æ•°æ®è¯„åˆ†**ï¼šä½¿ç”¨LLMå¯¹å€™é€‰æ•°æ®è¿›è¡Œè¯„åˆ†ï¼Œåˆ¤æ–­å…¶ä¸ç›®æ ‡é¢†åŸŸçš„ç›¸ä¼¼åº¦æˆ–ç›¸å…³æ€§ã€‚4) **æ•°æ®é€‰æ‹©**ï¼šæ ¹æ®è¯„åˆ†ç»“æœï¼Œé€‰æ‹©æ’åé å‰çš„å€™é€‰æ•°æ®ç”¨äºLLMçš„å¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šLAMDASæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†LLMæœ¬èº«ä½œä¸ºéšå¼åˆ†ç±»å™¨ï¼Œé¿å…äº†ä¼ ç»Ÿæ•°æ®é€‰æ‹©æ–¹æ³•ä¸­å¤æ‚çš„ç‰¹å¾å·¥ç¨‹å’Œä¼˜åŒ–è¿‡ç¨‹ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLAMDASæ— éœ€è®­ç»ƒé¢å¤–çš„åˆ†ç±»å™¨æˆ–è®¡ç®—æ•°æ®ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œè€Œæ˜¯ç›´æ¥åˆ©ç”¨LLMçš„é¢„è®­ç»ƒçŸ¥è¯†æ¥åˆ¤æ–­æ•°æ®æ˜¯å¦å±äºç›®æ ‡é¢†åŸŸã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†æ•°æ®é€‰æ‹©çš„æ•ˆç‡ï¼Œè€Œä¸”èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨LLMçš„é¢†åŸŸçŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šLAMDASçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **æç¤ºè¯­çš„è®¾è®¡**ï¼šæç¤ºè¯­éœ€è¦èƒ½å¤Ÿæ¸…æ™°åœ°è¡¨è¾¾ç›®æ ‡é¢†åŸŸçš„ç‰¹å¾ï¼Œå¹¶å¼•å¯¼LLMè¿›è¡Œå‡†ç¡®çš„åˆ¤æ–­ã€‚2) **è¯„åˆ†å‡½æ•°çš„é€‰æ‹©**ï¼šè¯„åˆ†å‡½æ•°éœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¡é‡å€™é€‰æ•°æ®ä¸ç›®æ ‡é¢†åŸŸçš„ç›¸ä¼¼åº¦æˆ–ç›¸å…³æ€§ã€‚3) **æ•°æ®é€‰æ‹©ç­–ç•¥**ï¼šæ•°æ®é€‰æ‹©ç­–ç•¥éœ€è¦å¹³è¡¡æ•°æ®çš„è´¨é‡å’Œæ•°é‡ï¼Œä»¥è·å¾—æœ€ä½³çš„å¾®è°ƒæ•ˆæœã€‚å…·ä½“çš„æç¤ºè¯­è®¾è®¡ã€è¯„åˆ†å‡½æ•°å’Œæ•°æ®é€‰æ‹©ç­–ç•¥å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLAMDASåœ¨å¤šä¸ªé¢†åŸŸæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰SOTAæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šï¼ŒLAMDASä»…ä½¿ç”¨20%çš„æ•°æ®å°±è¶…è¿‡äº†å…¨æ•°æ®è®­ç»ƒçš„æ€§èƒ½ï¼Œå¹¶ä¸”ç›¸æ¯”å…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚è¿™äº›ç»“æœéªŒè¯äº†LAMDASåœ¨é¢†åŸŸæ•°æ®é€‰æ‹©æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LAMDASå¯å¹¿æ³›åº”ç”¨äºå„ç§é¢†åŸŸLLMçš„å®šåˆ¶åŒ–è®­ç»ƒï¼Œä¾‹å¦‚åŒ»ç–—ã€é‡‘èã€æ³•å¾‹ç­‰ã€‚é€šè¿‡é«˜æ•ˆçš„æ•°æ®é€‰æ‹©ï¼Œå¯ä»¥é™ä½äººå·¥æ ‡æ³¨æˆæœ¬ï¼ŒåŠ é€ŸLLMåœ¨ç‰¹å®šé¢†åŸŸçš„è½åœ°åº”ç”¨ã€‚è¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºæå‡LLMåœ¨æ•°æ®åŒ®ä¹é¢†åŸŸçš„æ€§èƒ½ï¼Œå¹¶é™ä½æ¨¡å‹è®­ç»ƒçš„è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Adapting large language models (LLMs) to specific domains often faces a critical bottleneck: the scarcity of high-quality, human-curated data. While large volumes of unchecked data are readily available, indiscriminately using them for fine-tuning risks introducing noise and degrading performance. Strategic data selection is thus crucial, requiring a method that is both accurate and efficient. Existing approaches, categorized as similarity-based and direct optimization methods, struggle to simultaneously achieve these goals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for domain-specific DAta Selection), a novel approach that leverages the pre-trained LLM itself as an implicit classifier, thereby bypassing explicit feature engineering and computationally intensive optimization process. LAMDAS reframes data selection as a one-class classification problem, identifying candidate data that "belongs" to the target domain defined by a small reference dataset. Extensive experimental results demonstrate that LAMDAS not only exceeds the performance of full-data training using a fraction of the data but also outperforms nine state-of-the-art (SOTA) baselines under various scenarios. Furthermore, LAMDAS achieves the most compelling balance between performance gains and computational efficiency compared to all evaluated baselines.

