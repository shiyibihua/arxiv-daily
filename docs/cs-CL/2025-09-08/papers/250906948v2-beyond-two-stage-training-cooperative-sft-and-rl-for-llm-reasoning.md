---
layout: default
title: Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning
---

# Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.06948" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.06948v2</a>
  <a href="https://arxiv.org/pdf/2509.06948.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.06948v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.06948v2', 'Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liang Chen, Xueting Han, Li Shen, Jing Bai, Kam-Fai Wong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-08 (æ›´æ–°: 2025-10-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºååŒSFTä¸RLè®­ç»ƒæ¡†æ¶ï¼Œè§£å†³LLMæ¨ç†ä¸­ç¾éš¾æ€§é—å¿˜ä¸æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç›‘ç£å¾®è°ƒ` `æ¨ç†èƒ½åŠ›` `åŒå±‚ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä¸¤é˜¶æ®µSFTå’ŒRLè®­ç»ƒLLMæ¨ç†èƒ½åŠ›çš„æ–¹æ³•å­˜åœ¨ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¯¼è‡´RLè®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§ååŒSFTå’ŒRLçš„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡åŒå±‚ä¼˜åŒ–ï¼Œä½¿SFTèƒ½å¤ŸæŒ‡å¯¼RLçš„ä¼˜åŒ–è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)å·²è¢«è¯æ˜èƒ½æœ‰æ•ˆæ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç”±äºå…¶è¯•é”™æ€§è´¨ï¼Œé¢ä¸´ä¸¥é‡çš„æ•ˆç‡æŒ‘æˆ˜ã€‚å¸¸è§çš„åšæ³•æ˜¯é‡‡ç”¨ç›‘ç£å¾®è°ƒ(SFT)ä½œä¸ºRLçš„é¢„çƒ­é˜¶æ®µï¼Œä½†è¿™ç§è§£è€¦çš„ä¸¤é˜¶æ®µæ–¹æ³•å­˜åœ¨ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼šç¬¬äºŒé˜¶æ®µçš„RLé€æ¸ä¸¢å¤±SFTä¹ å¾—çš„è¡Œä¸ºï¼Œå¹¶ä½æ•ˆåœ°æ¢ç´¢æ–°çš„æ¨¡å¼ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¨¡å‹å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨åŒå±‚ä¼˜åŒ–æ¥ä¿ƒè¿›è¿™äº›è®­ç»ƒèŒƒå¼ä¹‹é—´æ›´å¥½çš„åˆä½œã€‚é€šè¿‡å°†SFTç›®æ ‡å»ºç«‹åœ¨æœ€ä¼˜RLç­–ç•¥çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿SFTèƒ½å¤Ÿå…ƒå­¦ä¹ å¦‚ä½•æŒ‡å¯¼RLçš„ä¼˜åŒ–è¿‡ç¨‹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸‹å±‚æ‰§è¡ŒRLæ›´æ–°ï¼ŒåŒæ—¶æ¥å—SFTç›‘ç£ï¼Œä¸Šå±‚æ˜¾å¼åœ°æœ€å¤§åŒ–ååŒå¢ç›Šâ€”â€”è”åˆSFT-RLè®­ç»ƒç›¸å¯¹äºå•ç‹¬RLçš„æ€§èƒ½ä¼˜åŠ¿ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºåŸºçº¿ï¼Œå¹¶åœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå…ˆä½¿ç”¨SFTè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åä½¿ç”¨RLè¿›è¡Œå¾®è°ƒã€‚è¿™ç§æ–¹æ³•çš„ä¸»è¦é—®é¢˜åœ¨äºï¼ŒRLè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šé€æ¸é—å¿˜SFTé˜¶æ®µå­¦ä¹ åˆ°çš„çŸ¥è¯†ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡é™ä½ï¼Œæœ€ç»ˆæ€§èƒ½å—é™ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆç»“åˆSFTå’ŒRLä¼˜åŠ¿ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜çš„è®­ç»ƒæ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŒå±‚ä¼˜åŒ–ï¼Œå°†SFTå’ŒRLè®­ç»ƒè¿‡ç¨‹ç´§å¯†ç»“åˆã€‚å…·ä½“æ¥è¯´ï¼ŒSFTçš„ç›®æ ‡æ˜¯å¸®åŠ©RLæ›´å¥½åœ°è¿›è¡Œæ¢ç´¢ï¼Œè€ŒRLçš„ç›®æ ‡æ˜¯æå‡æ•´ä½“æ€§èƒ½ã€‚é€šè¿‡è¿™ç§ååŒè®­ç»ƒçš„æ–¹å¼ï¼ŒSFTå¯ä»¥å…ƒå­¦ä¹ å¦‚ä½•æŒ‡å¯¼RLçš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»è€Œé¿å…ç¾éš¾æ€§é—å¿˜ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é‡‡ç”¨åŒå±‚ä¼˜åŒ–æ¡†æ¶ã€‚ä¸‹å±‚ä¼˜åŒ–å™¨æ‰§è¡ŒRLæ›´æ–°ï¼ŒåŒæ—¶æ¥å—SFTçš„ç›‘ç£ã€‚ä¸Šå±‚ä¼˜åŒ–å™¨åˆ™æ˜¾å¼åœ°æœ€å¤§åŒ–ååŒå¢ç›Šï¼Œå³è”åˆSFT-RLè®­ç»ƒç›¸å¯¹äºå•ç‹¬RLè®­ç»ƒçš„æ€§èƒ½æå‡ã€‚è¿™ç§æ¡†æ¶å…è®¸SFTå’ŒRLç›¸äº’åä½œï¼Œå…±åŒæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†SFTå’ŒRLè®­ç»ƒè¿‡ç¨‹è§£è€¦ï¼Œé€šè¿‡åŒå±‚ä¼˜åŒ–æ¡†æ¶å°†äºŒè€…ç´§å¯†ç»“åˆï¼Œä½¿å¾—SFTèƒ½å¤ŸæŒ‡å¯¼RLçš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»è€Œé¿å…ç¾éš¾æ€§é—å¿˜ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚è¿™ç§ååŒè®­ç»ƒçš„æ–¹å¼æ˜¯ä¸ç°æœ‰ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æœ€æœ¬è´¨çš„åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å°†SFTç›®æ ‡å»ºç«‹åœ¨æœ€ä¼˜RLç­–ç•¥çš„åŸºç¡€ä¸Šï¼Œä½¿å¾—SFTèƒ½å¤Ÿå­¦ä¹ å¦‚ä½•æŒ‡å¯¼RLçš„ä¼˜åŒ–ï¼›2) ä¸Šå±‚ä¼˜åŒ–å™¨æ˜¾å¼åœ°æœ€å¤§åŒ–ååŒå¢ç›Šï¼Œé¼“åŠ±SFTå’ŒRLä¹‹é—´çš„åˆä½œï¼›3) ä½¿ç”¨åˆé€‚çš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡SFTå’ŒRLçš„è®­ç»ƒç›®æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨äº”ä¸ªæ¨ç†åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†è¯¥æ–¹æ³•åœ¨é¿å…ç¾éš¾æ€§é—å¿˜å’Œæé«˜è®­ç»ƒæ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½é—®ç­”ã€å¯¹è¯ç³»ç»Ÿã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡ï¼Œå¯ä»¥é™ä½æ¨¡å‹éƒ¨ç½²æˆæœ¬ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶æ¨åŠ¨ç›¸å…³äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has proven effective in incentivizing the reasoning abilities of large language models (LLMs), but suffers from severe efficiency challenges due to its trial-and-error nature. While the common practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this decoupled two-stage approach suffers from catastrophic forgetting: second-stage RL gradually loses SFT-acquired behaviors and inefficiently explores new patterns. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. By conditioning the SFT objective on the optimal RL policy, our approach enables SFT to meta-learn how to guide RL's optimization process. During training, the lower level performs RL updates while simultaneously receiving SFT supervision, and the upper level explicitly maximizes the cooperative gain-the performance advantage of joint SFT-RL training over RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency.

