---
layout: default
title: COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens
---

# COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.06836" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.06836v3</a>
  <a href="https://arxiv.org/pdf/2509.06836.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.06836v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.06836v3', 'COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eugene Kwek, Wenpeng Yin

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-08 (æ›´æ–°: 2025-10-10)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCOMPACTï¼Œé€šè¿‡è”åˆä¼˜åŒ–è¯è¡¨å’ŒFFNé€šé“å‰ªæï¼Œæå‡LLMå’ŒSLMçš„æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹å‰ªæ` `è¯­è¨€æ¨¡å‹å‹ç¼©` `Transformer` `è¯è¡¨å‰ªæ` `FFNé€šé“å‰ªæ` `ä½å»¶è¿Ÿæ¨ç†` `è¾¹ç¼˜éƒ¨ç½²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å‰ªææ–¹æ³•åœ¨ä¿æŒTransformerç»“æ„å’Œå¤„ç†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå®½åº¦å‰ªæç ´åæ ‡å‡†ç»“æ„ï¼Œæ·±åº¦å‰ªæå¯¼è‡´ç²¾åº¦éª¤é™ã€‚
2. COMPACTè”åˆå‰ªæç¨€æœ‰è¯æ±‡å’ŒFFNä¸­é—´é€šé“ï¼Œåˆ©ç”¨common-token-weighted activationså¯¹é½é‡è¦æ€§ï¼Œä¿æŒæ ‡å‡†Transformerç»“æ„ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCOMPACTåœ¨å¤šç§æ¨¡å‹ï¼ˆQwenã€LLaMAã€Gemmaï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ä¸‹æ¸¸æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—é™ä½äº†å‚æ•°é‡ã€å†…å­˜å ç”¨å’Œå»¶è¿Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å†…å­˜ã€å»¶è¿Ÿå’ŒæœåŠ¡æˆæœ¬æ–¹é¢çš„æ•ˆç‡ï¼Œä½¿å…¶æ›´é€‚ç”¨äºè¾¹ç¼˜éƒ¨ç½²ã€äº¤äº’å¼åº”ç”¨å’Œå¤§è§„æ¨¡å¯æŒç»­æ¨ç†ï¼Œæœ¬æ–‡æå‡ºäº†COMPACTæ–¹æ³•ã€‚è¯¥æ–¹æ³•è”åˆæ‰§è¡Œï¼šï¼ˆiï¼‰å‰ªæç¨€æœ‰è¯æ±‡ä»¥ç¼©å°åµŒå…¥/LMå¤´å±‚ï¼›ï¼ˆiiï¼‰ä½¿ç”¨common-token-weighted activationså‰ªæFFNä¸­é—´é€šé“ï¼Œä½¿é‡è¦æ€§ä¸å‰ªæåçš„tokenåˆ†å¸ƒå¯¹é½ã€‚COMPACTç»§æ‰¿äº†æ·±åº¦å’Œå®½åº¦å‰ªæçš„ä¼˜ç‚¹ï¼Œå¦‚éƒ¨ç½²å‹å¥½æ€§ï¼ˆä¿æŒæ ‡å‡†çš„Transformeræ¶æ„ï¼‰ã€å°ºåº¦é€‚åº”æ€§ï¼ˆæƒè¡¡è¯æ±‡è¡¨ä¸FFNå‰ªæï¼‰ã€å…·æœ‰ç«äº‰åŠ›çš„å‰ªææ—¶é—´å’Œæ˜¾è‘—çš„å†…å­˜èŠ‚çœä»¥åŠååé‡æå‡ã€‚åœ¨Qwenã€LLaMAå’ŒGemmaç³»åˆ—ï¼ˆ0.5B-70Bï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCOMPACTå®ç°äº†æœ€å…ˆè¿›çš„ä¸‹æ¸¸æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†å‚æ•°é‡ã€GPUå†…å­˜å’Œå»¶è¿Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å®½åº¦å‰ªææ–¹æ³•é€šå¸¸ä¼šç ´åTransformerçš„æ ‡å‡†ç»“æ„ï¼Œå¯¼è‡´éœ€è¦å®šåˆ¶åŒ–çš„æ¨ç†ä»£ç ã€‚æ·±åº¦å‰ªæè™½ç„¶ä¿æŒäº†ç»“æ„ï¼Œä½†å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„ç²¾åº¦ä¸‹é™ã€‚æ­¤å¤–ï¼Œè®¸å¤šå‰ªææ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä¸Šéš¾ä»¥ç»´æŒæ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§æ—¢èƒ½ä¿æŒæ ‡å‡†Transformerç»“æ„ï¼Œåˆèƒ½æœ‰æ•ˆåº”ç”¨äºä¸åŒè§„æ¨¡è¯­è¨€æ¨¡å‹çš„å‰ªææ–¹æ³•æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCOMPACTçš„æ ¸å¿ƒæ€è·¯æ˜¯è”åˆä¼˜åŒ–è¯è¡¨å‰ªæå’ŒFFNé€šé“å‰ªæã€‚é€šè¿‡å‰ªæä½é¢‘è¯æ±‡æ¥å‡å°embeddingå±‚å’ŒLM headçš„å¤§å°ï¼ŒåŒæ—¶åˆ©ç”¨common-token-weighted activationsæ¥æŒ‡å¯¼FFNä¸­é—´å±‚çš„é€šé“å‰ªæã€‚è¿™ç§è”åˆä¼˜åŒ–ä½¿å¾—å‰ªæè¿‡ç¨‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å‰ªæåçš„tokenåˆ†å¸ƒï¼Œä»è€Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘å‚æ•°é‡å’Œè®¡ç®—é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCOMPACTæ–¹æ³•åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šè¯è¡¨å‰ªæå’ŒFFNé€šé“å‰ªæã€‚é¦–å…ˆï¼Œæ ¹æ®è¯æ±‡çš„é¢‘ç‡è¿›è¡Œå‰ªæï¼Œç§»é™¤ä½é¢‘è¯æ±‡ï¼Œä»è€Œå‡å°embeddingå±‚å’ŒLM headçš„å¤§å°ã€‚ç„¶åï¼Œåˆ©ç”¨common-token-weighted activationsæ¥è¯„ä¼°FFNä¸­é—´å±‚é€šé“çš„é‡è¦æ€§ï¼Œå¹¶å‰ªæä¸é‡è¦çš„é€šé“ã€‚æ•´ä¸ªè¿‡ç¨‹ä¿æŒäº†æ ‡å‡†çš„Transformeræ¶æ„ï¼Œæ— éœ€å®šåˆ¶åŒ–çš„æ¨ç†ä»£ç ã€‚

**å…³é”®åˆ›æ–°**ï¼šCOMPACTçš„å…³é”®åˆ›æ–°åœ¨äºè”åˆä¼˜åŒ–è¯è¡¨å‰ªæå’ŒFFNé€šé“å‰ªæï¼Œå¹¶åˆ©ç”¨common-token-weighted activationsæ¥æŒ‡å¯¼FFNé€šé“å‰ªæã€‚ä¸ä¼ ç»Ÿçš„ç‹¬ç«‹å‰ªææ–¹æ³•ç›¸æ¯”ï¼ŒCOMPACTèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å‰ªæåçš„tokenåˆ†å¸ƒï¼Œä»è€Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°æ›´é«˜çš„å‹ç¼©ç‡ã€‚æ­¤å¤–ï¼ŒCOMPACTæ–¹æ³•ä¿æŒäº†æ ‡å‡†çš„Transformeræ¶æ„ï¼Œé¿å…äº†å®šåˆ¶åŒ–æ¨ç†ä»£ç çš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯è¡¨å‰ªææ–¹é¢ï¼Œéœ€è¦ç¡®å®šä¸€ä¸ªåˆé€‚çš„è¯æ±‡é¢‘ç‡é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è¯æ±‡å°†è¢«ç§»é™¤ã€‚åœ¨FFNé€šé“å‰ªææ–¹é¢ï¼Œcommon-token-weighted activationsçš„è®¡ç®—æ–¹å¼æ˜¯å…³é”®ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªé€šé“ï¼Œè®¡ç®—å…¶å¯¹å¸¸è§tokençš„æ¿€æ´»å€¼çš„åŠ æƒå¹³å‡ï¼Œæƒé‡å¯ä»¥æ˜¯tokençš„é¢‘ç‡æˆ–å…¶ä»–é‡è¦æ€§æŒ‡æ ‡ã€‚ç„¶åï¼Œæ ¹æ®è¿™äº›åŠ æƒå¹³å‡å€¼å¯¹é€šé“è¿›è¡Œæ’åºï¼Œå¹¶å‰ªæä¸é‡è¦çš„é€šé“ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„è¯­è¨€æ¨¡å‹æŸå¤±å‡½æ•°ï¼Œå¹¶åœ¨å‰ªæè¿‡ç¨‹ä¸­åŠ å…¥æ­£åˆ™åŒ–é¡¹ï¼Œä»¥é¼“åŠ±ç¨€ç–æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCOMPACTåœ¨Qwenã€LLaMAå’ŒGemmaç­‰å¤šä¸ªæ¨¡å‹å®¶æ—ï¼ˆ0.5B-70Bï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ä¸‹æ¸¸æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¿æŒç›¸ä¼¼æ€§èƒ½çš„å‰æä¸‹ï¼ŒCOMPACTèƒ½å¤Ÿæ˜¾è‘—å‡å°‘å‚æ•°é‡ã€GPUå†…å­˜å ç”¨å’Œæ¨ç†å»¶è¿Ÿã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®åœ¨è®ºæ–‡ä¸­è¯¦ç»†å±•ç¤ºï¼Œå¹¶ä¸ç°æœ‰çš„å‰ªææ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

COMPACTæ–¹æ³•å¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆéƒ¨ç½²è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚è¾¹ç¼˜è®¾å¤‡ä¸Šçš„è‡ªç„¶è¯­è¨€å¤„ç†ã€ç§»åŠ¨åº”ç”¨ä¸­çš„æ™ºèƒ½åŠ©æ‰‹ã€ä»¥åŠå¤§è§„æ¨¡åœ¨çº¿æœåŠ¡çš„ä½å»¶è¿Ÿæ¨ç†ã€‚é€šè¿‡é™ä½æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼ŒCOMPACTèƒ½å¤Ÿæ˜¾è‘—é™ä½éƒ¨ç½²æˆæœ¬ï¼Œæé«˜ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¿ƒè¿›å¯æŒç»­çš„AIå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Making large language models (LLMs) more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a promising technique, but existing pruning methods are limited: width pruning often breaks the standard transformer layout, requiring custom inference code, while depth pruning can cause abrupt accuracy drops. Also, while many pruning approaches are effective against LLMs, they struggle to maintain performance on small language models (SLMs). In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/LM head layers and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT inherits strengths of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab. vs. FFN pruning), competitive pruning times, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream performance, with substantial reductions in parameters, GPU memory, and latency.

