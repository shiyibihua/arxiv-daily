---
layout: default
title: Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data
---

# Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07202" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07202v2</a>
  <a href="https://arxiv.org/pdf/2509.07202.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07202v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07202v2', 'Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Khushiyant

**åˆ†ç±»**: cs.HC, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-08 (æ›´æ–°: 2025-11-16)

**å¤‡æ³¨**: 15 pages, 10 figures, 5 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºRNNç¼–ç å™¨å’ŒGemma 2Bçš„åˆ†ç±»å™¨-LLMæ¶æ„ï¼Œç”¨äºè„‘ç”µä¿¡å·æ–‡æœ¬ç”Ÿæˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è„‘ç”µä¿¡å·` `æ–‡æœ¬ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¾ªç¯ç¥ç»ç½‘ç»œ` `è„‘æœºæ¥å£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŸºäºè„‘ç”µä¿¡å·çš„æ–‡æœ¬ç”Ÿæˆé¢ä¸´æ•°æ®é‡å’Œè®¡ç®—èµ„æºçš„éœ€æ±‚æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾æ•ˆç‡ä¸æ€§èƒ½ã€‚
2. è®ºæ–‡æå‡ºç»“åˆRNNç¼–ç å™¨å’ŒGemma 2Bçš„åˆ†ç±»å™¨-LLMæ¶æ„ï¼Œé™ä½æ•°æ®å’Œè®¡ç®—éœ€æ±‚ï¼Œæå‡ç”Ÿæˆæ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®å—é™æƒ…å†µä¸‹ä»è¡¨ç°å‡ºè‰²ï¼Œæ•´ä½“æ€§èƒ½è¾ƒç°æœ‰æ–¹æ³•æå‡10%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è„‘ç”µå›¾(EEG)æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†Gemma 2Bå¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä¸åˆ†ç±»å™¨-LLMæ¶æ„ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥äº†å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)ç¼–ç å™¨ã€‚è¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†æ•°æ®å’Œè®¡ç®—èµ„æºçš„éœ€æ±‚ï¼ŒåŒæ—¶å®ç°äº†æ¥è¿‘æœ€å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ•´ä½“æ€§èƒ½æå‡äº†10%ã€‚æ‰€æå‡ºçš„æ¶æ„å±•ç¤ºäº†è„‘ç”µä¿¡å·æ–‡æœ¬ç”Ÿæˆä¸­æœ‰æ•ˆè¿ç§»å­¦ä¹ çš„å¯èƒ½æ€§ï¼Œå³ä½¿åœ¨æ•°æ®å—é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒç¨³å¥å’ŒåŠŸèƒ½æ€§ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å°†LLMä¸è„‘ç”µè§£ç ç›¸ç»“åˆä»¥æ”¹è¿›è¾…åŠ©æŠ€æœ¯ï¼Œå¹¶æé«˜ä¸¥é‡è¿åŠ¨éšœç¢æ‚£è€…çš„ç‹¬ç«‹æ€§å’Œæ²Ÿé€šèƒ½åŠ›ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œè¯¥æ–¹æ³•çªç ´äº†ç°æœ‰èƒ½åŠ›çš„é™åˆ¶ï¼Œä¸ºè„‘æœºæ¥å£çš„ç ”ç©¶å’Œåº”ç”¨å¼€è¾Ÿäº†æ–°çš„é“è·¯ï¼Œä½¿åŸºäºè„‘ç”µä¿¡å·çš„æ–‡æœ¬ç”Ÿæˆæ›´æ˜“äºè®¿é—®å’Œé«˜æ•ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸºäºè„‘ç”µä¿¡å·ï¼ˆEEGï¼‰çš„æ–‡æœ¬ç”Ÿæˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†è„‘ç”µä¿¡å·è§£ç å¹¶è½¬åŒ–ä¸ºæœ‰æ„ä¹‰çš„æ–‡æœ¬ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ç»“åˆå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å¯¹è„‘ç”µä¿¡å·è¿›è¡Œæœ‰æ•ˆç¼–ç ã€‚é€šè¿‡åˆ†ç±»å™¨-LLMæ¶æ„ï¼Œå°†è„‘ç”µä¿¡å·çš„ç‰¹å¾æ˜ å°„åˆ°LLMçš„è¾“å…¥ç©ºé—´ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ–‡æœ¬ç”Ÿæˆã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨é™ä½å¯¹å¤§é‡è®­ç»ƒæ•°æ®çš„ä¾èµ–ï¼Œå¹¶æé«˜ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) RNNç¼–ç å™¨ï¼šç”¨äºæå–è„‘ç”µä¿¡å·çš„æ—¶åºç‰¹å¾ã€‚2) åˆ†ç±»å™¨ï¼šå°†RNNç¼–ç å™¨çš„è¾“å‡ºæ˜ å°„åˆ°LLMçš„è¾“å…¥ç©ºé—´ï¼Œå¯ä»¥ç†è§£ä¸ºå°†è„‘ç”µä¿¡å·åˆ†ç±»åˆ°ä¸åŒçš„æ–‡æœ¬ç±»åˆ«ã€‚3) Gemma 2B LLMï¼šåˆ©ç”¨åˆ†ç±»å™¨çš„è¾“å‡ºä½œä¸ºæç¤ºï¼Œç”Ÿæˆæœ€ç»ˆçš„æ–‡æœ¬ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼šè„‘ç”µä¿¡å· -> RNNç¼–ç å™¨ -> åˆ†ç±»å™¨ -> Gemma 2B LLM -> ç”Ÿæˆæ–‡æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†é¢„è®­ç»ƒçš„LLMä¸è„‘ç”µä¿¡å·è§£ç ç›¸ç»“åˆï¼Œåˆ©ç”¨LLMçš„å…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼æ–‡æœ¬ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯è„‘ç”µä¿¡å·æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†å¯¹è®­ç»ƒæ•°æ®çš„éœ€æ±‚ï¼Œå¹¶æé«˜äº†ç”Ÿæˆæ–‡æœ¬çš„æµç•…æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œåˆ†ç±»å™¨-LLMæ¶æ„çš„è®¾è®¡ä½¿å¾—æ¨¡å‹å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨è„‘ç”µä¿¡å·çš„ç‰¹å¾ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šRNNç¼–ç å™¨é‡‡ç”¨LSTMæˆ–GRUå•å…ƒï¼Œç”¨äºæ•æ‰è„‘ç”µä¿¡å·çš„æ—¶åºä¾èµ–å…³ç³»ã€‚åˆ†ç±»å™¨å¯ä»¥ä½¿ç”¨å…¨è¿æ¥ç½‘ç»œæˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼Œå°†RNNçš„è¾“å‡ºæ˜ å°„åˆ°LLMçš„è¯åµŒå…¥ç©ºé—´ã€‚Gemma 2B LLMé‡‡ç”¨æ ‡å‡†çš„Transformeræ¶æ„ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚å…³é”®å‚æ•°åŒ…æ‹¬RNNçš„éšè—å±‚å¤§å°ã€åˆ†ç±»å™¨çš„å±‚æ•°å’Œç¥ç»å…ƒæ•°é‡ã€LLMçš„å­¦ä¹ ç‡ç­‰ã€‚è®ºæ–‡å¯èƒ½è¿˜é‡‡ç”¨äº†æ•°æ®å¢å¼ºã€æ­£åˆ™åŒ–ç­‰æŠ€æœ¯æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç ”ç©¶çš„å…³é”®å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨è„‘ç”µä¿¡å·æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæ•´ä½“æ€§èƒ½æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†10%ã€‚å³ä½¿åœ¨æ•°æ®é‡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä»ç„¶è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°†é¢„è®­ç»ƒçš„LLMä¸è„‘ç”µä¿¡å·è§£ç ç›¸ç»“åˆæ˜¯ä¸€ç§å¾ˆæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¾…åŠ©æŠ€æœ¯é¢†åŸŸï¼Œä¾‹å¦‚å¸®åŠ©ä¸¥é‡è¿åŠ¨éšœç¢æ‚£è€…é€šè¿‡è„‘ç”µä¿¡å·è¿›è¡Œäº¤æµå’Œæ§åˆ¶è®¾å¤‡ï¼Œæé«˜ä»–ä»¬çš„ç‹¬ç«‹æ€§å’Œç”Ÿæ´»è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ç”¨äºè„‘æœºæ¥å£ã€ç¥ç»åº·å¤ç­‰é¢†åŸŸï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œå®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥å‘å±•ï¼Œå®ç°æ›´è‡ªç„¶ã€æ›´é«˜æ•ˆçš„è„‘ç”µä¿¡å·æ–‡æœ¬ç”Ÿæˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text generating capabilities have undergone a substantial transformation with the introduction of large language models (LLMs). Electroencephalography (EEG)-based text production is still difficult, though, because it requires a lot of data and processing power. This paper introduces a new method that combines the use of the Gemma 2B LLM with a classifier-LLM architecture to incorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically lowers the amount of data and compute power needed while achieving performance close to that of cutting-edge methods. Notably, compared to current methodologies, our methodology delivers an overall performance improvement of 10%. The suggested architecture demonstrates the possibility of effective transfer learning for EEG-based text production, remaining strong and functional even in the face of data limits. This work highlights the potential of integrating LLMs with EEG decoding to improve assistive technologies and improve independence and communication for those with severe motor limitations. Our method pushes the limits of present capabilities and opens new paths for research and application in brain-computer interfaces by efficiently using the strengths of pre-trained language models. This makes EEG-based text production more accessible and efficient.

