---
layout: default
title: HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models
---

# HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.06596" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.06596v1</a>
  <a href="https://arxiv.org/pdf/2509.06596.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.06596v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.06596v1', 'HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin Tong, Zhi Lin, Jingya Wang, Bo Jin

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HAVEï¼šé€šè¿‡å¤´è‡ªé€‚åº”é—¨æ§ä¸å€¼æ ¡å‡†ç¼“è§£å¤§è¯­è¨€æ¨¡å‹å¹»è§‰**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¹»è§‰ç¼“è§£` `æ³¨æ„åŠ›æœºåˆ¶` `å¤´è‡ªé€‚åº”é—¨æ§` `å€¼æ ¡å‡†` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¯ä¿¡èµ–ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­æ˜“äº§ç”Ÿå¹»è§‰ï¼Œä¸»è¦åŸå› æ˜¯æ³¨æ„åŠ›å¤´é‡è¦æ€§è¢«å¿½ç•¥ä¸”åŸå§‹æ³¨æ„åŠ›æƒé‡ä¸å‡†ç¡®ã€‚
2. HAVEé€šè¿‡å¤´è‡ªé€‚åº”é—¨æ§åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›å¤´æƒé‡ï¼Œå¹¶åˆ©ç”¨å€¼å‘é‡æ ¡å‡†æ³¨æ„åŠ›ï¼Œä»è€Œæ„å»ºæ›´å‡†ç¡®çš„tokençº§è¯æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHAVEåœ¨å¤šä¸ªQAåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—å‡å°‘äº†å¹»è§‰ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œä¸”æ— éœ€å¾®è°ƒï¼Œæ˜“äºé›†æˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€ç´¢å¢å¼ºæˆ–é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆä¸­ç»å¸¸äº§ç”Ÿå¹»è§‰ï¼Œå³ä½¿å­˜åœ¨ç›¸å…³è¯æ®ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™æºäºä¸¤ä¸ªé—®é¢˜ï¼šå¤´çš„é‡è¦æ€§è¢«è§†ä¸ºä¸è¾“å…¥æ— å…³ï¼Œå¹¶ä¸”åŸå§‹æ³¨æ„åŠ›æƒé‡ä¸èƒ½å¾ˆå¥½åœ°åæ˜ æ¯ä¸ªtokençš„çœŸå®è´¡çŒ®ã€‚æˆ‘ä»¬æå‡ºäº†HAVEï¼ˆå¤´è‡ªé€‚åº”é—¨æ§å’Œå€¼æ ¡å‡†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€å¾®è°ƒçš„è§£ç æ¡†æ¶ï¼Œç›´æ¥è§£å†³äº†è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚HAVEå¼•å…¥äº†å¤´è‡ªé€‚åº”é—¨æ§ï¼Œå®ƒæ‰§è¡Œæ³¨æ„åŠ›å¤´çš„å®ä¾‹çº§è½¯é‡åŠ æƒï¼Œä»¥åŠå€¼æ ¡å‡†ï¼Œå®ƒåˆ©ç”¨å€¼å‘é‡çš„å¤§å°æ¥å¢å¼ºæ³¨æ„åŠ›ï¼Œä»¥è¿‘ä¼¼å†™å›è´¡çŒ®ã€‚è¿™äº›æ¨¡å—å…±åŒæ„å»ºäº†ä¸æ¨¡å‹æ›´æ–°å¯¹é½çš„tokençº§è¯æ®ï¼Œå¹¶é€šè¿‡è½»é‡çº§çš„ä¸ç¡®å®šæ€§ç¼©æ”¾ç­–ç•¥å°†å…¶ä¸LMåˆ†å¸ƒèåˆã€‚HAVEæ— éœ€å¾®è°ƒï¼Œå¹¶ä¸”åœ¨å•ä¸ªå‰å‘ä¼ é€’ä¸­è¿è¡Œï¼Œä½¿å…¶é«˜æ•ˆä¸”é€‚ç”¨èŒƒå›´å¹¿æ³›ã€‚è·¨å¤šä¸ªQAåŸºå‡†å’ŒLLMç³»åˆ—çš„å®éªŒè¡¨æ˜ï¼ŒHAVEå§‹ç»ˆå¦‚ä¸€åœ°å‡å°‘å¹»è§‰ï¼Œå¹¶ä¸”ä¼˜äºåŒ…æ‹¬DAGCDåœ¨å†…çš„å¼ºå¤§åŸºçº¿ï¼Œä¸”å¼€é”€é€‚ä¸­ã€‚è¯¥æ¡†æ¶æ˜¯é€æ˜çš„ã€å¯å¤ç°çš„ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åœ°ä¸ç°æˆçš„LLMé›†æˆï¼Œä»è€Œåœ¨å®é™…ç¯å¢ƒä¸­æ¨è¿›å¯ä¿¡èµ–çš„ç”Ÿæˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆæˆ–é•¿æ–‡æœ¬ç”Ÿæˆä¸­å‡ºç°çš„å¹»è§‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†æ³¨æ„åŠ›å¤´çš„é‡è¦æ€§è§†ä¸ºä¸è¾“å…¥æ— å…³çš„ï¼Œå¹¶ä¸”åŸå§‹çš„æ³¨æ„åŠ›æƒé‡å¹¶ä¸èƒ½å‡†ç¡®åæ˜ æ¯ä¸ªtokenå¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æœ‰ç›¸å…³è¯æ®çš„æƒ…å†µä¸‹ä»ç„¶äº§ç”Ÿä¸å‡†ç¡®æˆ–è™šå‡çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›å¤´çš„é‡è¦æ€§ï¼Œå¹¶æ ¡å‡†æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå‡å°‘å¹»è§‰ã€‚å…·ä½“æ¥è¯´ï¼ŒHAVEæ¡†æ¶é€šè¿‡å¤´è‡ªé€‚åº”é—¨æ§æœºåˆ¶æ¥åŠ¨æ€è°ƒæ•´æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„é‡è¦æ€§ï¼Œå¹¶åˆ©ç”¨å€¼æ ¡å‡†æœºåˆ¶æ¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ¯ä¸ªtokençš„è´¡çŒ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHAVEæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šå¤´è‡ªé€‚åº”é—¨æ§ï¼ˆHead-Adaptive Gatingï¼‰å’Œå€¼æ ¡å‡†ï¼ˆValue Calibrationï¼‰ã€‚å¤´è‡ªé€‚åº”é—¨æ§æ¨¡å—æ ¹æ®è¾“å…¥å®ä¾‹åŠ¨æ€åœ°è°ƒæ•´æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„é‡è¦æ€§ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å…³æ³¨ä¸å½“å‰è¾“å…¥ç›¸å…³çš„æ³¨æ„åŠ›å¤´ã€‚å€¼æ ¡å‡†æ¨¡å—é€šè¿‡è€ƒè™‘å€¼å‘é‡çš„å¤§å°æ¥æ ¡å‡†æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°æ¯ä¸ªtokenå¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ã€‚è¿™ä¸¤ä¸ªæ¨¡å—å…±åŒæ„å»ºäº†tokençº§åˆ«çš„è¯æ®ï¼Œå¹¶å°†å…¶ä¸è¯­è¨€æ¨¡å‹çš„åˆ†å¸ƒèåˆï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„ç»“æœã€‚æ•´ä¸ªæ¡†æ¶æ— éœ€å¾®è°ƒï¼Œå¯ä»¥åœ¨å•ä¸ªå‰å‘ä¼ é€’ä¸­å®Œæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šHAVEçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å¤´è‡ªé€‚åº”é—¨æ§å’Œå€¼æ ¡å‡†è¿™ä¸¤ä¸ªæ¨¡å—ï¼Œè¿™ä¸¤ä¸ªæ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ç°æœ‰æ–¹æ³•ä¸­æ³¨æ„åŠ›å¤´é‡è¦æ€§è¢«å¿½ç•¥å’Œæ³¨æ„åŠ›æƒé‡ä¸å‡†ç¡®çš„é—®é¢˜ã€‚å¤´è‡ªé€‚åº”é—¨æ§èƒ½å¤ŸåŠ¨æ€åœ°è°ƒæ•´æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„é‡è¦æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å…³æ³¨ä¸å½“å‰è¾“å…¥ç›¸å…³çš„æ³¨æ„åŠ›å¤´ã€‚å€¼æ ¡å‡†èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°æ¯ä¸ªtokenå¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ã€‚

**å…³é”®è®¾è®¡**ï¼šå¤´è‡ªé€‚åº”é—¨æ§é€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„æƒé‡ï¼Œè¯¥ç½‘ç»œçš„è¾“å…¥æ˜¯å½“å‰è¾“å…¥å®ä¾‹çš„è¡¨ç¤ºã€‚å€¼æ ¡å‡†é€šè¿‡å°†æ³¨æ„åŠ›æƒé‡ä¸å€¼å‘é‡çš„å¤§å°ç›¸ä¹˜æ¥æ ¡å‡†æ³¨æ„åŠ›æƒé‡ã€‚HAVEæ¡†æ¶ä½¿ç”¨ä¸€ä¸ªä¸ç¡®å®šæ€§ç¼©æ”¾ç­–ç•¥å°†tokençº§åˆ«çš„è¯æ®ä¸è¯­è¨€æ¨¡å‹çš„åˆ†å¸ƒèåˆï¼Œè¯¥ç­–ç•¥æ ¹æ®æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ¥è°ƒæ•´è¯æ®çš„æƒé‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHAVEåœ¨å¤šä¸ªQAåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—å‡å°‘äº†å¹»è§‰ï¼Œå¹¶ä¸”ä¼˜äºåŒ…æ‹¬DAGCDåœ¨å†…çš„å¼ºå¤§åŸºçº¿ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHAVEèƒ½å¤Ÿå°†å¹»è§‰ç‡é™ä½10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒHAVEçš„è®¡ç®—å¼€é”€é€‚ä¸­ï¼Œå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­é«˜æ•ˆè¿è¡Œã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HAVEæ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºéœ€è¦å‡å°‘å¹»è§‰çš„å„ç§åœºæ™¯ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ã€å¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼ŒHAVEå¯ä»¥å¢å¼ºç”¨æˆ·å¯¹å¤§è¯­è¨€æ¨¡å‹çš„ä¿¡ä»»ï¼Œå¹¶ä¿ƒè¿›å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚è¯¥æ¡†æ¶çš„æ˜“é›†æˆæ€§å’Œæ— éœ€å¾®è°ƒçš„ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿåº”ç”¨äºå„ç§ç°æœ‰çš„LLMã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings.

