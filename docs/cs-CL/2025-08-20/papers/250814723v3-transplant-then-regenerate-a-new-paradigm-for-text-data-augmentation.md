---
layout: default
title: Transplant Then Regenerate: A New Paradigm for Text Data Augmentation
---

# Transplant Then Regenerate: A New Paradigm for Text Data Augmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14723" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14723v3</a>
  <a href="https://arxiv.org/pdf/2508.14723.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14723v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14723v3', 'Transplant Then Regenerate: A New Paradigm for Text Data Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guangzhan Wang, Hongyu Zhang, Beijun Shen, Xiaodong Gu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20 (æ›´æ–°: 2025-09-14)

**å¤‡æ³¨**: Accepted by EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLMTransplantä»¥è§£å†³æ–‡æœ¬æ•°æ®å¢å¼ºçš„å¤šæ ·æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬æ•°æ®å¢å¼º` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `å†…å®¹ç”Ÿæˆ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬å¢å¼ºæ–¹æ³•å¦‚åå‘ç¿»è¯‘ä¸»è¦é›†ä¸­åœ¨è¯æ±‡å±‚é¢çš„æ”¹å†™ï¼Œå¯¼è‡´ç”Ÿæˆçš„å˜ä½“ç¼ºä¹å¤šæ ·æ€§å’Œåˆ›é€ æ€§ã€‚
2. LMTransplanté€šè¿‡å°†ç§å­æ–‡æœ¬ä¸LLMæ‰©å±•çš„ä¸Šä¸‹æ–‡ç»“åˆï¼Œç”Ÿæˆæ›´ä¸°å¯Œçš„æ–‡æœ¬å˜ä½“ï¼Œå……åˆ†åˆ©ç”¨LLMä¸­åµŒå…¥çš„çŸ¥è¯†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLMTransplantåœ¨å¤šä¸ªæ–‡æœ¬ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿå¢å¼ºæ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®å¢å¼ºæ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„å…³é”®æŠ€æœ¯ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚åå‘ç¿»è¯‘ä¸»è¦å…³æ³¨è¯æ±‡å±‚é¢çš„æ”¹å†™ï¼Œé€šå¸¸åªèƒ½ç”Ÿæˆè¯­ä¹‰ç›¸åŒçš„å˜ä½“ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å…¶â€œçŸ¥è¯†æ¶Œç°â€èƒ½åŠ›å¢å¼ºäº†æ–‡æœ¬å¢å¼ºï¼Œä½†æ§åˆ¶è¾“å‡ºçš„é£æ ¼å’Œç»“æ„ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶éœ€è¦ç²¾ç»†çš„æç¤ºå·¥ç¨‹ã€‚æœ¬æ–‡æå‡ºäº†LMTransplantï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨LLMsçš„æ–°å‹æ–‡æœ¬å¢å¼ºèŒƒå¼ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯â€œç§»æ¤åå†ç”Ÿæˆâ€ï¼šå°†ç§å­æ–‡æœ¬èå…¥LLMæ‰©å±•çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå¹¶è¦æ±‚LLMåŸºäºæ‰©å±•çš„ä¸Šä¸‹æ–‡å†ç”Ÿæˆå˜ä½“ã€‚è¿™ä¸€ç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿåˆ›é€ å‡ºæ›´å…·å¤šæ ·æ€§å’Œåˆ›é€ æ€§çš„å†…å®¹å˜ä½“ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ–‡æœ¬çš„æ ¸å¿ƒå±æ€§ã€‚æˆ‘ä»¬åœ¨å„ç§æ–‡æœ¬ç›¸å…³ä»»åŠ¡ä¸Šè¯„ä¼°äº†LMTransplantï¼Œè¯æ˜å…¶åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–‡æœ¬å¢å¼ºæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å¢å¼ºæ•°æ®è§„æ¨¡å¢é•¿æ—¶è¡¨ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–‡æœ¬æ•°æ®å¢å¼ºæ–¹æ³•åœ¨å¤šæ ·æ€§å’Œåˆ›é€ æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ä¼ ç»Ÿæ–¹æ³•ç”Ÿæˆçš„å˜ä½“å¾€å¾€ç¼ºä¹æ–°æ„å’Œå˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLMTransplantçš„æ ¸å¿ƒæ€è·¯æ˜¯â€œç§»æ¤åå†ç”Ÿæˆâ€ï¼Œå³å°†ç§å­æ–‡æœ¬ä¸LLMç”Ÿæˆçš„æ‰©å±•ä¸Šä¸‹æ–‡ç»“åˆï¼Œåˆ©ç”¨LLMçš„çŸ¥è¯†ç”Ÿæˆå¤šæ ·åŒ–çš„æ–‡æœ¬å˜ä½“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œç”Ÿæˆæ‰©å±•ä¸Šä¸‹æ–‡ï¼›å…¶æ¬¡ï¼Œå°†ç§å­æ–‡æœ¬èå…¥è¯¥ä¸Šä¸‹æ–‡ï¼›æœ€åï¼Œåˆ©ç”¨LLMç”Ÿæˆæ–°çš„æ–‡æœ¬å˜ä½“ã€‚

**å…³é”®åˆ›æ–°**ï¼šLMTransplantçš„åˆ›æ–°åœ¨äºå…¶â€œç§»æ¤åå†ç”Ÿæˆâ€çš„ç­–ç•¥ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„ç®€å•é‡è¿°ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å…·åˆ›é€ æ€§å’Œå¤šæ ·æ€§çš„æ–‡æœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ç§å­æ–‡æœ¬çš„é€‰æ‹©å’Œä¸Šä¸‹æ–‡æ‰©å±•çš„æ–¹å¼ï¼ŒæŸå¤±å‡½æ•°åˆ™å…³æ³¨ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§å’Œä¸åŸå§‹æ–‡æœ¬çš„ç›¸ä¼¼æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLMTransplantåœ¨å¤šä¸ªæ–‡æœ¬ç›¸å…³ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„æ–‡æœ¬å¢å¼ºæ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå°¤å…¶åœ¨ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§å’Œåˆ›é€ æ€§æ–¹é¢è¡¨ç°çªå‡ºï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿã€å†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡æé«˜æ–‡æœ¬æ•°æ®çš„å¤šæ ·æ€§å’Œåˆ›é€ æ€§ï¼ŒLMTransplantå¯ä¸ºå„ç§åº”ç”¨æä¾›æ›´ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ï¼Œè¿›è€Œæå‡æ¨¡å‹çš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨ç”Ÿæˆå¼AIå’Œè‡ªåŠ¨åŒ–å†…å®¹åˆ›ä½œç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their "knowledge emergence" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.

