---
layout: default
title: Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models
---

# Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14427" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14427v1</a>
  <a href="https://arxiv.org/pdf/2508.14427.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14427v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14427v1', 'Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wuyang Zhang, Yexin Tian, Xiandong Meng, Mengjie Wang, Junliang Du

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçŸ¥è¯†å›¾è°±æ³¨å…¥å¾®è°ƒæ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†å›¾è°±` `å¾®è°ƒç®—æ³•` `å›¾ç¥ç»ç½‘ç»œ` `ç»“æ„åŒ–æ¨ç†` `è¯­ä¹‰ç†è§£` `è‡ªç„¶è¯­è¨€å¤„ç†` `å®ä½“è¯†åˆ«` `é—®ç­”ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–çŸ¥è¯†ä»»åŠ¡æ—¶ï¼Œå¸¸å‡ºç°æ¨ç†é“¾ç¼ºå¤±å’Œå®ä½“è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§çŸ¥è¯†å›¾è°±æ³¨å…¥çš„å¾®è°ƒç®—æ³•ï¼Œé€šè¿‡å›¾ç¥ç»ç½‘ç»œç¼–ç å®ä½“åŠå…¶å…³ç³»ï¼Œæ„å»ºå›¾åŸºè¯­ä¹‰è¡¨ç¤ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡é€»è¾‘å»ºæ¨¡èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦ç»“æ„åŒ–çŸ¥è¯†çš„ä»»åŠ¡æ—¶ï¼Œæ¨ç†é“¾ç¼ºå¤±å’Œå®ä½“çº§è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±æ³¨å…¥çš„å¾®è°ƒç®—æ³•æ¡†æ¶ã€‚è¯¥æ–¹æ³•åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥ç»“æ„åŒ–å›¾ä¿¡æ¯è¿›è¡Œè¾…åŠ©å­¦ä¹ ã€‚é€šè¿‡å›¾ç¥ç»ç½‘ç»œå¯¹å®ä½“åŠå…¶å…³ç³»è¿›è¡Œç¼–ç ï¼Œæ„å»ºå›¾åŸºè¯­ä¹‰è¡¨ç¤ºã€‚è®¾è®¡äº†ä¸€ç§èåˆæœºåˆ¶ï¼Œå°†çŸ¥è¯†å›¾è°±åµŒå…¥ä¸è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºå…±åŒå»ºæ¨¡ã€‚ä¸ºå¢å¼ºçŸ¥è¯†æ•´åˆçš„é²æ£’æ€§ï¼Œå¼•å…¥äº†é—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€å¹³è¡¡è¯­è¨€è¯­ä¹‰ä¸ç»“æ„çŸ¥è¯†çš„è´¡çŒ®ï¼Œæœ‰æ•ˆç¼“è§£ä¸åŒè¡¨ç¤ºç©ºé—´ä¹‹é—´çš„å†²çªã€‚è®­ç»ƒè¿‡ç¨‹ä¸­æ„å»ºäº†è”åˆæŸå¤±å‡½æ•°ï¼Œå…¼é¡¾ä»»åŠ¡æ€§èƒ½å’Œç»“æ„å¯¹é½ç›®æ ‡ï¼Œæå‡äº†å®ä½“é¢„æµ‹å’Œè¯­ä¹‰æ¨ç†çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¿˜è¿›è¡Œäº†ç³»ç»Ÿçš„æ•æ„Ÿæ€§å®éªŒï¼Œè¯„ä¼°å­¦ä¹ ç‡ã€å›¾è¦†ç›–ç‡å’Œç»“æ„æ‰°åŠ¨å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å®ä½“è¯†åˆ«ã€é—®ç­”å’Œè¯­è¨€ç”Ÿæˆç­‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç»“æ„åŒ–æ¨ç†ä»»åŠ¡ä¸­æ¨ç†é“¾ç¼ºå¤±å’Œå®ä½“çº§è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚è¯­ä¹‰æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆæ•´åˆç»“æ„åŒ–çŸ¥è¯†ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡çŸ¥è¯†å›¾è°±æ³¨å…¥çš„å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå¯¹çŸ¥è¯†å›¾è°±è¿›è¡Œç¼–ç ï¼Œä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œè¯­ä¹‰ç†è§£ã€‚é€šè¿‡å°†ç»“æ„åŒ–çŸ¥è¯†ä¸è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç»“åˆï¼Œæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€å›¾ç¥ç»ç½‘ç»œæ¨¡å—å’Œèåˆæœºåˆ¶ã€‚é¦–å…ˆï¼Œé€šè¿‡å›¾ç¥ç»ç½‘ç»œå¯¹çŸ¥è¯†å›¾è°±è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå›¾åŸºè¯­ä¹‰è¡¨ç¤ºï¼›ç„¶åï¼Œè®¾è®¡èåˆæœºåˆ¶å°†å›¾åµŒå…¥ä¸è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºç»“åˆï¼›æœ€åï¼Œä½¿ç”¨é—¨æ§æœºåˆ¶åŠ¨æ€è°ƒæ•´ä¸¤è€…çš„è´¡çŒ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œé€šè¿‡é—¨æ§æœºåˆ¶è§£å†³äº†è¯­è¨€è¯­ä¹‰ä¸ç»“æ„çŸ¥è¯†ä¹‹é—´çš„å†²çªï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ„å»ºäº†è”åˆæŸå¤±å‡½æ•°ï¼Œç»¼åˆè€ƒè™‘ä»»åŠ¡æ€§èƒ½å’Œç»“æ„å¯¹é½ç›®æ ‡ã€‚æ­¤å¤–ï¼Œè¿›è¡Œäº†æ•æ„Ÿæ€§å®éªŒï¼Œè¯„ä¼°äº†å­¦ä¹ ç‡ã€å›¾è¦†ç›–ç‡å’Œç»“æ„æ‰°åŠ¨ç­‰å‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç¡®ä¿äº†æ–¹æ³•çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„çŸ¥è¯†å›¾è°±æ³¨å…¥å¾®è°ƒæ¡†æ¶åœ¨å®ä½“è¯†åˆ«ã€é—®ç­”å’Œè¯­è¨€ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå°¤å…¶åœ¨è¯­ä¹‰ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡é€»è¾‘å»ºæ¨¡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢ã€å¯¹è¯ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨å¤æ‚è¯­ä¹‰ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šéœ€è¦ç»“æ„åŒ–çŸ¥è¯†çš„åº”ç”¨åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper addresses the problems of missing reasoning chains and insufficient entity-level semantic understanding in large language models when dealing with tasks that require structured knowledge. It proposes a fine-tuning algorithm framework based on knowledge graph injection. The method builds on pretrained language models and introduces structured graph information for auxiliary learning. A graph neural network is used to encode entities and their relations, constructing a graph-based semantic representation. A fusion mechanism is then designed to jointly model the knowledge graph embeddings with the contextual representations from the language model. To enhance the robustness of knowledge integration, a gating mechanism is introduced to dynamically balance the contributions of linguistic semantics and structural knowledge. This effectively mitigates conflicts between different representational spaces. During training, a joint loss function is constructed to account for both task performance and structural alignment objectives. This helps improve the accuracy of entity prediction and semantic reasoning. The study also includes a series of systematic sensitivity experiments. It evaluates the effects of learning rate, graph coverage, and structural perturbations on model performance. The results further validate the effectiveness and stability of the proposed method across tasks such as entity recognition, question answering, and language generation. Experimental findings show that the proposed structure-aware fine-tuning framework significantly enhances the model's ability to represent complex semantic units. It demonstrates better semantic consistency and contextual logic modeling in scenarios involving structural reasoning and entity extraction.

