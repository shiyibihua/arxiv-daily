---
layout: default
title: Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner
---

# Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15044" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15044v3</a>
  <a href="https://arxiv.org/pdf/2508.15044.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15044v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15044v3', 'Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bolian Li, Yanran Wu, Xinyu Luo, Ruqi Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20 (æ›´æ–°: 2025-09-23)

**å¤‡æ³¨**: EMNLP 2025 Main Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¥–åŠ±è½¬ç§»çš„æ¨æµ‹é‡‡æ ·ä»¥æé«˜æµ‹è¯•æ—¶å¯¹é½æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹å¯¹é½` `æ¨æµ‹é‡‡æ ·` `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æµ‹è¯•æ—¶å¯¹é½æŠ€æœ¯å¾€å¾€å¯¼è‡´æ˜¾è‘—çš„æ¨ç†æˆæœ¬ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„å¥–åŠ±è½¬ç§»çš„æ¨æµ‹é‡‡æ ·ç®—æ³•ï¼Œé€šè¿‡å¯¹é½è‰ç¨¿æ¨¡å‹ä¸äººç±»åå¥½ï¼Œæå‡äº†æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç®—æ³•åœ¨å¼±åˆ°å¼ºçš„å¯¹é½å®éªŒä¸­ï¼Œé‡‘å¥–å¾—åˆ†æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶æ¨ç†æˆæœ¬æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œä¸äººç±»åå¥½çš„å¯¹é½å·²æˆä¸ºå…¶å‘å±•çš„å…³é”®æ­¥éª¤ã€‚è¿‘å¹´æ¥ï¼Œæµ‹è¯•æ—¶å¯¹é½æŠ€æœ¯å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´æ˜¾è‘—çš„æ¨ç†æˆæœ¬ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ•ˆç‡ç“¶é¢ˆï¼Œæœ¬æ–‡æå‡ºäº†å¥–åŠ±è½¬ç§»çš„æ¨æµ‹é‡‡æ ·ï¼ˆSSSï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡å¯¹é½ä¸€ä¸ªå°å‹è‰ç¨¿æ¨¡å‹ä¸äººç±»åå¥½ï¼ŒåŒæ—¶ä¿æŒç›®æ ‡æ¨¡å‹ä¸å˜ï¼Œæ¥é«˜æ•ˆé¢„æµ‹æœªæ¥çš„æ ‡è®°ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†é€šè¿‡ä¿®æ”¹æ¥å—æ ‡å‡†å’Œå¥–åŠ±æ ‡è®°åˆ†å¸ƒï¼Œå¯ä»¥åˆ©ç”¨å¯¹é½è‰ç¨¿æ¨¡å‹ä¸æœªå¯¹é½ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„åˆ†å¸ƒè½¬ç§»ï¼Œæ¢å¤å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰çš„æœ€ä¼˜è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨æµ‹è¯•æ—¶å¼±åˆ°å¼ºçš„å¯¹é½å®éªŒä¸­å–å¾—äº†æ˜¾è‘—çš„é‡‘å¥–å¾—åˆ†ï¼ŒåŒæ—¶æ¨ç†æˆæœ¬å¤§å¹…é™ä½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æµ‹è¯•æ—¶å¯¹é½æŠ€æœ¯åœ¨æ¨ç†æˆæœ¬ä¸Šçš„ä¸è¶³ï¼Œå¯¼è‡´å…¶å®é™…åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¥–åŠ±è½¬ç§»çš„æ¨æµ‹é‡‡æ ·ï¼ˆSSSï¼‰ç®—æ³•ï¼Œé€šè¿‡å¯¹é½è‰ç¨¿æ¨¡å‹ä¸äººç±»åå¥½ï¼Œåˆ©ç”¨åˆ†å¸ƒè½¬ç§»æ¥ä¼˜åŒ–ç›®æ ‡æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è‰ç¨¿æ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼Œè‰ç¨¿æ¨¡å‹è´Ÿè´£é«˜æ•ˆé¢„æµ‹æœªæ¥æ ‡è®°ï¼Œè€Œç›®æ ‡æ¨¡å‹ä¿æŒä¸å˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡ä¿®æ”¹æ¥å—æ ‡å‡†å’Œå¥–åŠ±æ ‡è®°åˆ†å¸ƒï¼Œåˆ©ç”¨å¯¹é½è‰ç¨¿æ¨¡å‹ä¸æœªå¯¹é½ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„åˆ†å¸ƒè½¬ç§»ï¼Œæ¢å¤RLHFçš„æœ€ä¼˜è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬è‰ç¨¿æ¨¡å‹çš„å¯¹é½æ–¹å¼å’Œå¥–åŠ±æ ‡è®°çš„åˆ†å¸ƒç­–ç•¥ï¼Œè¿™äº›è®¾è®¡ç¡®ä¿äº†ç®—æ³•çš„é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¥–åŠ±è½¬ç§»çš„æ¨æµ‹é‡‡æ ·ç®—æ³•åœ¨æµ‹è¯•æ—¶å¼±åˆ°å¼ºçš„å¯¹é½å®éªŒä¸­ï¼Œé‡‘å¥–å¾—åˆ†æ˜¾è‘—æé«˜ï¼Œä¸”æ¨ç†æˆæœ¬é™ä½äº†çº¦30%ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œæ¨ç†èƒ½åŠ›æå‡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å®æ—¶åé¦ˆå’Œå¿«é€Ÿå“åº”çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆç­‰ã€‚æœªæ¥ï¼Œè¯¥ç®—æ³•å¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šé«˜æ•ˆå¯¹é½æŠ€æœ¯çš„å‘å±•ï¼Œæå‡äººæœºäº¤äº’çš„è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Aligning large language models (LLMs) with human preferences has become a critical step in their development. Recent research has increasingly focused on test-time alignment, where additional compute is allocated during inference to enhance LLM safety and reasoning capabilities. However, these test-time alignment techniques often incur substantial inference costs, limiting their practical application. We are inspired by the speculative sampling acceleration, which leverages a small draft model to efficiently predict future tokens, to address the efficiency bottleneck of test-time alignment. We introduce the reward-shifted speculative sampling (SSS) algorithm, in which the draft model is aligned with human preferences, while the target model remains unchanged. We theoretically demonstrate that the distributional shift between the aligned draft model and the unaligned target model can be exploited to recover the RLHF optimal solution without actually obtaining it, by modifying the acceptance criterion and bonus token distribution. Our algorithm achieves superior gold reward scores at a significantly reduced inference cost in test-time weak-to-strong alignment experiments, thereby validating both its effectiveness and efficiency.

