---
layout: default
title: DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections
---

# DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15851" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15851v1</a>
  <a href="https://arxiv.org/pdf/2508.15851.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15851v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15851v1', 'DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiwon Park, Seohyun Pyeon, Jinwoo Kim, Rina Carines Cabal, Yihao Ding, Soyeon Caren Han

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDocHop-QAä»¥è§£å†³å¤šæ–‡æ¡£å¤šæ¨¡æ€é—®ç­”ä¸­çš„æ¨ç†æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€é—®ç­”` `å¤šæ–‡æ¡£æ¨ç†` `å¼€æ”¾å¼æ¨ç†` `ç§‘å­¦æ–‡çŒ®` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é—®ç­”åŸºå‡†å¤šå±€é™äºå•æ–‡æ¡£ï¼Œæ— æ³•å¤„ç†å¤æ‚çš„å¤šæ–‡æ¡£ã€å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. DocHop-QAé€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«å¤šç§ä¿¡æ¯æ ¼å¼çš„å¤§è§„æ¨¡é—®ç­”æ•°æ®é›†ï¼Œæ”¯æŒåœ¨å¤šä¸ªæ–‡æ¡£ä¸­è¿›è¡Œå¼€æ”¾å¼æ¨ç†ã€‚
3. é€šè¿‡å››é¡¹ä»»åŠ¡çš„è¯„ä¼°ï¼ŒDocHop-QAå±•ç¤ºäº†å…¶åœ¨å¤æ‚å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œæ¨åŠ¨äº†é—®ç­”ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„é—®ç­”åŸºå‡†ä»å±€é™äºå•æ®µè½æˆ–å•æ–‡æ¡£è®¾ç½®ï¼Œæ— æ³•æ•æ‰ç°å®ä¸–ç•Œä¿¡æ¯æ£€ç´¢ä»»åŠ¡çš„å¤æ‚æ€§ã€‚å®é™…çš„é—®ç­”ä»»åŠ¡é€šå¸¸éœ€è¦åœ¨å¤šä¸ªæ–‡æ¡£ã€æ¨¡æ€å’Œç»“æ„æ ¼å¼ä¸­è¿›è¡Œå¤šè·³æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†DocHop-QAï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«11,379ä¸ªå¤šæ¨¡æ€ã€å¤šæ–‡æ¡£ã€å¤šè·³é—®ç­”å®ä¾‹çš„å¤§è§„æ¨¡åŸºå‡†ã€‚è¯¥æ•°æ®é›†ç”±æ¥è‡ªPubMedçš„å…¬å¼€ç§‘å­¦æ–‡çŒ®æ„å»ºï¼Œå…·æœ‰é¢†åŸŸæ— å…³æ€§ï¼Œå¹¶åŒ…å«æ–‡æœ¬æ®µè½ã€è¡¨æ ¼å’Œç»“æ„å¸ƒå±€çº¿ç´¢ç­‰å¤šæ ·åŒ–ä¿¡æ¯æ ¼å¼ã€‚ä¸ç°æœ‰æ•°æ®é›†ä¸åŒï¼ŒDocHop-QAä¸ä¾èµ–äºæ˜¾å¼è¶…é“¾æ¥æ–‡æ¡£ï¼Œè€Œæ˜¯é€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå¸ƒå±€æ„ŸçŸ¥çš„è¯æ®ç»¼åˆæ”¯æŒå¼€æ”¾å¼æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰é—®ç­”ç³»ç»Ÿåœ¨å¤„ç†å¤šæ–‡æ¡£å’Œå¤šæ¨¡æ€ä¿¡æ¯æ—¶çš„æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸€æ–‡æ¡£ï¼Œå¯¼è‡´æ¨ç†è·¯å¾„æµ…æ˜¾ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDocHop-QAçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šæ–‡æ¡£çš„é—®ç­”åŸºå‡†ï¼Œæ”¯æŒå¼€æ”¾å¼æ¨ç†ã€‚é€šè¿‡å¼•å…¥å¤šæ ·åŒ–çš„ä¿¡æ¯æ ¼å¼å’Œå¸ƒå±€æ„ŸçŸ¥ï¼Œæå‡é—®ç­”ç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€LLMé©±åŠ¨çš„é—®ç­”ç”Ÿæˆç®¡é“å’Œå¤šé¡¹ä»»åŠ¡è¯„ä¼°ã€‚æ•°æ®é›†ä»PubMedä¸­æå–ï¼Œæ¶µç›–æ–‡æœ¬ã€è¡¨æ ¼ç­‰å¤šç§ä¿¡æ¯æ ¼å¼ï¼Œç®¡é“åŸºäºé«˜é¢‘ç§‘å­¦é—®é¢˜æ¦‚å¿µè¿›è¡Œè®¾è®¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDocHop-QAçš„ä¸»è¦åˆ›æ–°åœ¨äºä¸ä¾èµ–äºè¶…é“¾æ¥æ–‡æ¡£ï¼Œè€Œæ˜¯é€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå¸ƒå±€ä¿¡æ¯è¿›è¡Œè¯æ®ç»¼åˆï¼Œæ”¯æŒæ›´å¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—é—®ç­”ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†æ›´çœŸå®çš„åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œé‡‡ç”¨äº†å¤šç§ä¿¡æ¯æ ¼å¼ï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼›åœ¨é—®ç­”ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œè®¾è®¡äº†åŸºäºé«˜é¢‘é—®é¢˜æ¦‚å¿µçš„LLMé©±åŠ¨ç®¡é“ï¼Œä»¥æé«˜ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å››é¡¹ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼ŒDocHop-QAå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„æ˜¾è‘—èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ç»“æ„åŒ–ç´¢å¼•é¢„æµ‹å’Œç”Ÿæˆé—®ç­”ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DocHop-QAçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºç§‘å­¦æ–‡çŒ®æ£€ç´¢ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿä»¥åŠä¿¡æ¯æ£€ç´¢é¢†åŸŸã€‚å…¶å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›å°†æ¨åŠ¨æ›´å¤æ‚çš„é—®ç­”ç³»ç»Ÿçš„å‘å±•ï¼Œæå‡ç”¨æˆ·åœ¨ä¿¡æ¯è·å–è¿‡ç¨‹ä¸­çš„ä½“éªŒå’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½ä¸ºè·¨é¢†åŸŸçŸ¥è¯†æ•´åˆå’Œè‡ªåŠ¨åŒ–ä¿¡æ¯å¤„ç†æä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite recent advances in large language models (LLMs), most QA benchmarks are still confined to single-paragraph or single-document settings, failing to capture the complexity of real-world information-seeking tasks. Practical QA often requires multi-hop reasoning over information distributed across multiple documents, modalities, and structural formats. Although prior datasets made progress in this area, they rely heavily on Wikipedia-based content and unimodal plain text, with shallow reasoning paths that typically produce brief phrase-level or single-sentence answers, thus limiting their realism and generalizability. We propose DocHop-QA, a large-scale benchmark comprising 11,379 QA instances for multimodal, multi-document, multi-hop question answering. Constructed from publicly available scientific documents sourced from PubMed, DocHop-QA is domain-agnostic and incorporates diverse information formats, including textual passages, tables, and structural layout cues. Unlike existing datasets, DocHop-QA does not rely on explicitly hyperlinked documents; instead, it supports open-ended reasoning through semantic similarity and layout-aware evidence synthesis. To scale realistic QA construction, we designed an LLM-driven pipeline grounded in 11 high-frequency scientific question concepts. We evaluated DocHop-QA through four tasks spanning structured index prediction, generative answering, and multimodal integration, reflecting both discriminative and generative paradigms. These tasks demonstrate DocHop-QA's capacity to support complex, multimodal reasoning across multiple documents.

