---
layout: default
title: Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion
---

# Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15848" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15848v1</a>
  <a href="https://arxiv.org/pdf/2508.15848.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15848v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15848v1', 'Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yinghan Zhou, Juan Wen, Wanli Peng, Zhengxian Wu, Ziwei Zhang, Yiming Xue

**åˆ†ç±»**: cs.CR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæˆ‘ä¼ªè£…æ”»å‡»ä»¥è§£å†³AIGTæ£€æµ‹è§„é¿é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæˆ‘ä¼ªè£…æ”»å‡»` `AIç”Ÿæˆæ–‡æœ¬` `æ£€æµ‹è§„é¿` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯¹æŠ—ç‰¹å¾æå–` `ä¸Šä¸‹æ–‡ä¼˜åŒ–` `æ–‡æœ¬ç”Ÿæˆ` `å†…å®¹å®¡æ ¸`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„AIGTæ£€æµ‹è§„é¿æ–¹æ³•å­˜åœ¨é«˜è®¡ç®—æˆæœ¬å’Œæ–‡æœ¬è´¨é‡ä¸‹é™çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„è‡ªæˆ‘ä¼ªè£…æ”»å‡»ï¼ˆSDAï¼‰é€šè¿‡å¯¹æŠ—ç‰¹å¾æå–å’Œä¸Šä¸‹æ–‡ç¤ºä¾‹ä¼˜åŒ–ï¼Œå¸®åŠ©LLMä¸»åŠ¨ä¼ªè£…è¾“å‡ºï¼Œé™ä½æ£€æµ‹æ¦‚ç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSDAæ˜¾è‘—é™ä½äº†å¤šç§AIGTæ£€æµ‹å™¨çš„æ£€æµ‹å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

AIç”Ÿæˆæ–‡æœ¬ï¼ˆAIGTï¼‰æ£€æµ‹è§„é¿æ—¨åœ¨é™ä½AIGTçš„æ£€æµ‹æ¦‚ç‡ï¼Œå¸®åŠ©è¯†åˆ«æ£€æµ‹å™¨çš„å¼±ç‚¹å¹¶å¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚å°½ç®¡ç°æœ‰çš„è§„é¿æ–¹æ³•è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œæ–‡æœ¬è´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªæˆ‘ä¼ªè£…æ”»å‡»ï¼ˆSDAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿä¸»åŠ¨ä¼ªè£…å…¶è¾“å‡ºï¼Œä»è€Œé™ä½è¢«åˆ†ç±»å™¨æ£€æµ‹çš„å¯èƒ½æ€§ã€‚SDAåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šå¯¹æŠ—ç‰¹å¾æå–å™¨å’ŒåŸºäºæ£€ç´¢çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¼˜åŒ–å™¨ã€‚å‰è€…ç”Ÿæˆä¼ªè£…ç‰¹å¾ï¼Œä½¿LLMèƒ½å¤Ÿç†è§£å¦‚ä½•ç”Ÿæˆæ›´äººæ€§åŒ–çš„æ–‡æœ¬ï¼›åè€…ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„ç¤ºä¾‹ä½œä¸ºä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œè¿›ä¸€æ­¥å¢å¼ºLLMçš„è‡ªæˆ‘ä¼ªè£…èƒ½åŠ›ï¼Œå¹¶å‡è½»ä¼ªè£…è¿‡ç¨‹å¯¹ç”Ÿæˆæ–‡æœ¬å¤šæ ·æ€§çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSDAæœ‰æ•ˆé™ä½äº†å¤šç§AIGTæ£€æµ‹å™¨å¯¹ä¸‰ç§ä¸åŒLLMç”Ÿæˆæ–‡æœ¬çš„å¹³å‡æ£€æµ‹å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒäº†AIGTçš„è´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³AIç”Ÿæˆæ–‡æœ¬ï¼ˆAIGTï¼‰åœ¨æ£€æµ‹æ—¶çš„è§„é¿é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œç”Ÿæˆæ–‡æœ¬è´¨é‡ä¸‹é™çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè‡ªæˆ‘ä¼ªè£…æ”»å‡»ï¼ˆSDAï¼‰é€šè¿‡å¼•å…¥å¯¹æŠ—ç‰¹å¾æå–å™¨å’ŒåŸºäºæ£€ç´¢çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¼˜åŒ–å™¨ï¼Œä½¿LLMèƒ½å¤Ÿä¸»åŠ¨ç”Ÿæˆæ›´å…·äººç±»ç‰¹å¾çš„æ–‡æœ¬ï¼Œä»è€Œé™ä½è¢«æ£€æµ‹çš„é£é™©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSDAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå¯¹æŠ—ç‰¹å¾æå–å™¨è´Ÿè´£ç”Ÿæˆä¼ªè£…ç‰¹å¾ï¼Œå¸®åŠ©LLMç†è§£äººç±»æ–‡æœ¬çš„ç‰¹å¾ï¼›è€Œä¸Šä¸‹æ–‡ç¤ºä¾‹ä¼˜åŒ–å™¨åˆ™ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ç¤ºä¾‹ï¼Œå¢å¼ºç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§å’Œè‡ªç„¶æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSDAçš„åˆ›æ–°åœ¨äºå…¶ä¸»åŠ¨ä¼ªè£…æœºåˆ¶ï¼Œå…è®¸LLMåœ¨ç”Ÿæˆæ–‡æœ¬æ—¶è‡ªæˆ‘è°ƒæ•´è¾“å‡ºç‰¹å¾ï¼Œä¸ä¼ ç»Ÿçš„è¢«åŠ¨è§„é¿æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSDAä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ä¼ªè£…ç‰¹å¾çš„ç”Ÿæˆï¼ŒåŒæ—¶é€šè¿‡æ£€ç´¢ç®—æ³•ç¡®ä¿ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„ç›¸å…³æ€§å’Œå¤šæ ·æ€§ï¼Œä»¥æå‡ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å’Œæ£€æµ‹è§„é¿èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSDAæ˜¾è‘—é™ä½äº†å¤šç§AIGTæ£€æµ‹å™¨çš„å¹³å‡æ£€æµ‹å‡†ç¡®ç‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ä¸‰ç§ä¸åŒLLMç”Ÿæˆçš„æ–‡æœ¬ä¸Šï¼Œæ£€æµ‹å‡†ç¡®ç‡é™ä½å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†æ–‡æœ¬çš„é«˜è´¨é‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†…å®¹ç”Ÿæˆã€ç¤¾äº¤åª’ä½“ç®¡ç†å’Œè‡ªåŠ¨åŒ–å†™ä½œç­‰ã€‚é€šè¿‡æé«˜AIGTçš„æ£€æµ‹è§„é¿èƒ½åŠ›ï¼ŒSDAå¯ä»¥å¸®åŠ©å¼€å‘æ›´ä¸ºæ™ºèƒ½çš„æ–‡æœ¬ç”Ÿæˆç³»ç»Ÿï¼Œå¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ï¼Œæœªæ¥å¯èƒ½å¯¹å†…å®¹å®¡æ ¸å’Œä¿¡æ¯ä¼ æ’­äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> AI-generated text (AIGT) detection evasion aims to reduce the detection probability of AIGT, helping to identify weaknesses in detectors and enhance their effectiveness and reliability in practical applications. Although existing evasion methods perform well, they suffer from high computational costs and text quality degradation. To address these challenges, we propose Self-Disguise Attack (SDA), a novel approach that enables Large Language Models (LLM) to actively disguise its output, reducing the likelihood of detection by classifiers. The SDA comprises two main components: the adversarial feature extractor and the retrieval-based context examples optimizer. The former generates disguise features that enable LLMs to understand how to produce more human-like text. The latter retrieves the most relevant examples from an external knowledge base as in-context examples, further enhancing the self-disguise ability of LLMs and mitigating the impact of the disguise process on the diversity of the generated text. The SDA directly employs prompts containing disguise features and optimized context examples to guide the LLM in generating detection-resistant text, thereby reducing resource consumption. Experimental results demonstrate that the SDA effectively reduces the average detection accuracy of various AIGT detectors across texts generated by three different LLMs, while maintaining the quality of AIGT.

