---
layout: default
title: Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset
---

# Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15096" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15096v1</a>
  <a href="https://arxiv.org/pdf/2508.15096.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15096v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15096v1', 'Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNemotron-CC-Mathä»¥è§£å†³æ•°å­¦æ•°æ®é›†è´¨é‡ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°å­¦æ•°æ®é›†` `é¢„è®­ç»ƒæ¨¡å‹` `ç§‘å­¦æ–‡æœ¬æå–` `æ·±åº¦å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `æ•°æ®è´¨é‡` `å¼€æº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°å­¦æ•°æ®é›†åœ¨è´¨é‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œä¸»è¦ç”±äºæå–ç®—æ³•ä¸ç¨³å®šå’ŒHTMLè½¬æ¢é—®é¢˜ï¼Œå¯¼è‡´æ•°å­¦ç»“æ„æ— æ³•å¯é ä¿ç•™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æå–ç®¡é“ï¼Œèƒ½å¤Ÿä»Common Crawlä¸­ç¨³å¥åœ°æå–æ•°å­¦å†…å®¹ï¼Œæ”¯æŒå¤šç§æ•°å­¦æ ¼å¼çš„æ¢å¤ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨Nemotron-CC-Mathé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨æ•°å­¦æ¨ç†å’Œé€šç”¨æ¨ç†ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é¢„è®­ç»ƒä¸­ï¼Œä½¿ç”¨é«˜è´¨é‡çš„ç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚æ•°å­¦å’Œä»£ç ï¼‰æ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°å­¦æ•°æ®é›†å› æå–ç®—æ³•è„†å¼±ã€HTMLåˆ°æ–‡æœ¬çš„è½¬æ¢æŸå¤±ä»¥åŠæ•°å­¦ç»“æ„çš„ä¿ç•™ä¸å¯é è€Œè´¨é‡ä¸‹é™ã€‚æœ¬æ–‡æå‡ºNemotron-CC-Mathï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºCommon Crawlæ„å»ºçš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°å­¦è¯­æ–™åº“ï¼Œé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„é¢†åŸŸæ— å…³çš„ç®¡é“ï¼Œä¸“é—¨è®¾è®¡ç”¨äºç¨³å¥çš„ç§‘å­¦æ–‡æœ¬æå–ã€‚è¯¥ç®¡é“é€šè¿‡å¸ƒå±€æ„ŸçŸ¥æ¸²æŸ“å’ŒåŸºäºLLMçš„æ¸…ç†é˜¶æ®µï¼Œæ¢å¤äº†å¤šç§æ ¼å¼çš„æ•°å­¦å†…å®¹ï¼Œä¿æŒäº†æ–¹ç¨‹å’Œä»£ç å—çš„ç»“æ„å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥è¯­æ–™åº“é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ•°å­¦æ•°æ®é›†åœ¨è´¨é‡å’Œç»“æ„ä¿ç•™æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç”±äºæå–ç®—æ³•è„†å¼±å’ŒHTMLè½¬æ¢æŸå¤±å¯¼è‡´çš„æ•°å­¦å†…å®¹ä¸¢å¤±é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„é¢†åŸŸæ— å…³çš„æå–ç®¡é“ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤å¤šç§æ ¼å¼çš„æ•°å­¦å†…å®¹ï¼Œå¹¶é€šè¿‡æ¸…ç†é˜¶æ®µç¡®ä¿å†…å®¹çš„ç»“æ„å®Œæ•´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¸ƒå±€æ„ŸçŸ¥æ¸²æŸ“æ¨¡å—å’ŒåŸºäºLLMçš„æ¸…ç†é˜¶æ®µï¼Œå‰è€…è´Ÿè´£ä»ç½‘é¡µä¸­æå–æ•°å­¦å†…å®¹ï¼Œåè€…åˆ™å¯¹æå–çš„å†…å®¹è¿›è¡Œæ ‡å‡†åŒ–å’Œä¸€è‡´æ€§ä¿®æ­£ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç®¡é“çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶èƒ½å¤Ÿå¤„ç†å¤šç§æ•°å­¦æ ¼å¼ï¼ˆå¦‚MathJaxã€KaTeXã€MathMLï¼‰ï¼Œå¹¶é€šè¿‡å¸ƒå±€æ„ŸçŸ¥æŠ€æœ¯æå‡æå–çš„å‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é’ˆå¯¹ä¸åŒæ•°å­¦æ ¼å¼çš„ç‰¹å®šå¤„ç†ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡ä¿ç•™ç»“æ„å®Œæ•´æ€§ï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†æ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿçš„æ–‡æœ¬å¤„ç†æŠ€æœ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨Nemotron-CC-Mathé¢„è®­ç»ƒçš„Nemotron-T 8Bæ¨¡å‹åœ¨MATHå’ŒMBPP+åŸºå‡†ä¸Šåˆ†åˆ«è·å¾—äº†+4.8è‡³+12.6å’Œ+4.6è‡³+14.3çš„æ€§èƒ½æå‡ï¼Œä¸”åœ¨MMLUå’ŒMMLU-Stemçš„é€šç”¨é¢†åŸŸè¡¨ç°ä¹Ÿæœ‰æ‰€æ”¹å–„ï¼Œæ ‡å¿—ç€åœ¨å¼€æ”¾æ•°å­¦é¢„è®­ç»ƒè¯­æ–™åº“ä¸­è®¾ç«‹äº†æ–°çš„æ€§èƒ½æ ‡å‡†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç§‘ç ”å’Œè‡ªåŠ¨åŒ–ç¼–ç¨‹ç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„æ•°å­¦æ•°æ®é›†ï¼Œå¯ä»¥ä¸ºæ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡æä¾›æ›´å¼ºçš„æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶ä¸åº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pretraining large language models (LLMs) on high-quality, structured data such as mathematics and code substantially enhances reasoning capabilities. However, existing math-focused datasets built from Common Crawl suffer from degraded quality due to brittle extraction heuristics, lossy HTML-to-text conversion, and the failure to reliably preserve mathematical structure. In this work, we introduce Nemotron-CC-Math, a large-scale, high-quality mathematical corpus constructed from Common Crawl using a novel, domain-agnostic pipeline specifically designed for robust scientific text extraction.
>   Unlike previous efforts, our pipeline recovers math across various formats (e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx and a targeted LLM-based cleaning stage. This approach preserves the structural integrity of equations and code blocks while removing boilerplate, standardizing notation into LaTeX representation, and correcting inconsistencies.
>   We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+ (133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably, Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including MegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens than FineMath-4+, which was previously the highest-quality math pretraining dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to +12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines, while also improving general-domain performance on MMLU and MMLU-Stem.
>   We present the first pipeline to reliably extract scientific content--including math--from noisy web-scale data, yielding measurable gains in math, code, and general reasoning, and setting a new state of the art among open math pretraining corpora. To support open-source efforts, we release our code and datasets.

