---
layout: default
title: NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model
---

# NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14444" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14444v4</a>
  <a href="https://arxiv.org/pdf/2508.14444.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14444v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14444v4', 'NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: NVIDIA, :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haifeng Qian, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jian Zhang, Jiaqi Zeng, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Tugrul Konuk, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20 (æ›´æ–°: 2025-09-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNemotron-Nano-9B-v2ä»¥æå‡æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§ä¸æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ··åˆæ¨¡å‹` `æ¨ç†ä»»åŠ¡` `Mamba-Transformer` `æ¨¡å‹å‹ç¼©` `é«˜ååé‡` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†æ¨¡å‹åœ¨å¤„ç†é•¿è¾“å…¥å’Œè¾“å‡ºæ—¶é¢ä¸´ååé‡ä¸è¶³å’Œå‡†ç¡®æ€§ä¸é«˜çš„æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºçš„Nemotron-Nano-9B-v2æ¨¡å‹é€šè¿‡å¼•å…¥Mamba-2å±‚æ›¿ä»£ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›å±‚ï¼Œæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒNemotron-Nano-9B-v2åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸åŒç±»æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ¨ç†ååé‡æé«˜äº†6å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†Nemotron-Nano-9B-v2ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆMamba-Transformerè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡ï¼ŒåŒæ—¶åœ¨ä¸åŒç±»æ¨¡å‹ç›¸æ¯”æ—¶å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚Nemotron-Nano-9B-v2åŸºäºNemotron-Hæ¶æ„ï¼Œæ›¿æ¢äº†å¤§éƒ¨åˆ†è‡ªæ³¨æ„åŠ›å±‚ï¼Œä»¥æé«˜ç”Ÿæˆæ¨ç†æ‰€éœ€çš„é•¿æ€ç»´è½¨è¿¹æ—¶çš„æ¨ç†é€Ÿåº¦ã€‚é€šè¿‡åœ¨20ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šé¢„è®­ç»ƒä¸€ä¸ª120äº¿å‚æ•°çš„æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨Minitronç­–ç•¥è¿›è¡Œå‹ç¼©å’Œè’¸é¦ï¼ŒNemotron-Nano-9B-v2åœ¨æ¨ç†è®¾ç½®ä¸‹çš„ååé‡æé«˜äº†å¤šè¾¾6å€ï¼ŒåŒæ—¶åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸ç°æœ‰åŒç±»æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å°†åœ¨Hugging Faceä¸Šå‘å¸ƒNemotron-Nano-9B-v2åŠå…¶ç›¸å…³æ£€æŸ¥ç‚¹å’Œæ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨ç†æ¨¡å‹åœ¨å¤„ç†é•¿è¾“å…¥å’Œè¾“å‡ºæ—¶çš„ååé‡ä¸è¶³å’Œå‡†ç¡®æ€§ä¸é«˜çš„é—®é¢˜ã€‚ç°æœ‰çš„Transformeræ¶æ„åœ¨æ¨ç†é€Ÿåº¦å’Œæ•ˆç‡ä¸Šå­˜åœ¨æ˜æ˜¾çš„ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNemotron-Nano-9B-v2é€šè¿‡å°†å¤§éƒ¨åˆ†è‡ªæ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºMamba-2å±‚ï¼Œæ—¨åœ¨æé«˜æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ç”Ÿæˆæ¨ç†æ‰€éœ€çš„é•¿æ€ç»´è½¨è¿¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŸºäºNemotron-Hï¼Œé¦–å…ˆé¢„è®­ç»ƒä¸€ä¸ª120äº¿å‚æ•°çš„åŸºç¡€æ¨¡å‹ï¼Œç„¶åé€šè¿‡Minitronç­–ç•¥è¿›è¡Œå‹ç¼©å’Œè’¸é¦ï¼Œä»¥ä¾¿åœ¨å•ä¸ªNVIDIA A10G GPUä¸Šå®ç°é«˜è¾¾128kçš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šNemotron-Nano-9B-v2çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†Mamba-2å±‚ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ï¼Œå¹¶åœ¨å‡†ç¡®æ€§ä¸Šä¸ç°æœ‰åŒç±»æ¨¡å‹æŒå¹³æˆ–æ›´ä¼˜ã€‚ä¸ä¼ ç»ŸTransformeræ¨¡å‹ç›¸æ¯”ï¼Œè¿™ç§æ··åˆæ¶æ„åœ¨æ¨ç†æ•ˆç‡ä¸Šå…·æœ‰æœ¬è´¨çš„åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†FP8è®­ç»ƒç­–ç•¥ï¼Œå¹¶åœ¨20ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚æ¨¡å‹çš„å‹ç¼©å’Œè’¸é¦è¿‡ç¨‹é€šè¿‡Minitronç­–ç•¥å®ç°ï¼Œç¡®ä¿åœ¨æ¨ç†æ—¶èƒ½å¤Ÿåœ¨è¾ƒä½çš„å†…å­˜å ç”¨ä¸‹å¤„ç†æ›´é•¿çš„è¾“å…¥å’Œè¾“å‡ºã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†é˜è¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNemotron-Nano-9B-v2åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†ä¸Qwen3-8Bç­‰åŒç±»æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨8kè¾“å…¥å’Œ16kè¾“å‡ºçš„æ¨ç†è®¾ç½®ä¸‹ï¼Œååé‡æå‡é«˜è¾¾6å€ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Nemotron-Nano-9B-v2æ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œå¤æ‚æ¨ç†ä»»åŠ¡ã€‚å…¶é«˜ååé‡å’Œå‡†ç¡®æ€§ä½¿å…¶é€‚ç”¨äºå®æ—¶åº”ç”¨åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹è¿˜å¯èƒ½åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œå¤§è§„æ¨¡æ•°æ®å¤„ç†ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.

