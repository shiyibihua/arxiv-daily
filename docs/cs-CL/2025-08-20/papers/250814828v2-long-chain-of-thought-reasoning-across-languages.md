---
layout: default
title: Long Chain-of-Thought Reasoning Across Languages
---

# Long Chain-of-Thought Reasoning Across Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14828" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14828v2</a>
  <a href="https://arxiv.org/pdf/2508.14828.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14828v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14828v2', 'Long Chain-of-Thought Reasoning Across Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Josh Barua, Seun Eisape, Kayo Yin, Alane Suhr

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20 (æ›´æ–°: 2025-10-09)

**å¤‡æ³¨**: v1 is a workshop version accepted to SCALR @ COLM 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶å¤šè¯­è¨€é•¿é“¾æ¨ç†èƒ½åŠ›çš„è¿ç§»ä¸æå‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿é“¾æ¨ç†` `å¤šè¯­è¨€å¤„ç†` `æ¨¡å‹æ‰©å±•` `é¢„è®­ç»ƒ` `åˆæˆæ•°æ®` `æ¨ç†æ•ˆç‡` `è·¨è¯­è¨€è¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†æ¨¡å‹åœ¨è‹±è¯­ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶é•¿é“¾æ¨ç†èƒ½åŠ›åœ¨å…¶ä»–è¯­è¨€ä¸­çš„è¿ç§»æ€§å°šä¸æ˜ç¡®ã€‚
2. æœ¬æ–‡é€šè¿‡æ¯”è¾ƒEn-CoTå’ŒTarget-CoTä¸¤ç§æ¨ç†è®¾ç½®ï¼Œæ¢è®¨æ¨¡å‹æ‰©å±•ã€é¢„è®­ç»ƒç­‰å¯¹å¤šè¯­è¨€æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹è§„æ¨¡çš„æ‰©å¤§å¯¹En-CoTæœ‰åˆ©ï¼Œä½†Target-CoTçš„æ€§èƒ½æå‡æœ‰é™ï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¸ºæ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹æ¨ç†æ¨¡å‹åœ¨è‹±è¯­ä¸­å±•ç°äº†ç”Ÿæˆé•¿é“¾æ¨ç†ï¼ˆCoTsï¼‰çš„å“è¶Šèƒ½åŠ›ï¼Œä½†æˆ‘ä»¬ä»ç„¶ç¼ºä¹å¯¹è¿™äº›èƒ½åŠ›å¦‚ä½•è¿ç§»åˆ°å…¶ä»–è¯­è¨€çš„ç†è§£ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†æ¨¡å‹å¼€å‘çš„å››ä¸ªå…³é”®é˜¶æ®µâ€”â€”æ‰©å±•ã€é¢„è®­ç»ƒã€åè®­ç»ƒå’Œæ¨ç†ï¼Œæ¢è®¨é•¿é“¾æ¨ç†èƒ½åŠ›åœ¨ä¹ç§éè‹±è¯­ç›®æ ‡è¯­è¨€ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹è§„æ¨¡çš„æ‰©å¤§æå‡äº†En-CoTçš„å¤šè¯­è¨€ä»»åŠ¡æ€§èƒ½ï¼Œä½†Target-CoTçš„è¡¨ç°ä»ç„¶æ»åï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é•¿å¤šæ­¥æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†åˆæˆæ•°æ®çš„åè®­ç»ƒæ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†è‡ªåŠ¨ç¿»è¯‘çš„æ¨ç†è½¨è¿¹åœ¨Fine-tuningä¸­çš„ä¼˜åŠ¿ã€‚æœ€åï¼ŒæŠ¥å‘Šäº†ä¸åŒè¯­è¨€æ¨ç†æ•ˆç‡çš„å·®å¼‚åŠå…¶ç‰¹å®šå¤±è´¥æ¨¡å¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„é•¿é“¾æ¨ç†èƒ½åŠ›è¿ç§»é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨éè‹±è¯­è¯­è¨€ä¸­çš„è¡¨ç°ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿæ€§ç ”ç©¶æ¨¡å‹çš„æ‰©å±•ã€é¢„è®­ç»ƒã€åè®­ç»ƒå’Œæ¨ç†é˜¶æ®µï¼Œåˆ†æä¸åŒè®¾ç½®å¯¹å¤šè¯­è¨€æ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œæå‡ºåˆæˆæ•°æ®çš„åè®­ç»ƒæ–¹æ³•ä»¥æå‡Target-CoTæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åˆ†ä¸ºå››ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ‰©å±•æ¨¡å‹è§„æ¨¡ï¼›2) è¿›è¡Œå¤šè¯­è¨€é¢„è®­ç»ƒï¼›3) é‡‡ç”¨åˆæˆæ•°æ®è¿›è¡Œåè®­ç»ƒï¼›4) è¿›è¡Œæ¨ç†æ•ˆç‡åˆ†æã€‚æ¯ä¸ªé˜¶æ®µéƒ½é’ˆå¯¹ä¸åŒè¯­è¨€çš„æ¨ç†èƒ½åŠ›è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­è¿›è¡Œé•¿é“¾æ¨ç†çš„ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å‘ç°äº†åœ¨Target-CoTä¸­æ€§èƒ½æ»åçš„ç°è±¡ï¼Œå¼ºè°ƒäº†åˆæˆæ•°æ®åœ¨åè®­ç»ƒä¸­çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œé‡‡ç”¨äº†å¹¿æ³›çš„å¤šè¯­è¨€æ•°æ®é›†ï¼Œè€Œåœ¨åè®­ç»ƒé˜¶æ®µåˆ™ä½¿ç”¨äº†è‡ªåŠ¨ç¿»è¯‘çš„æ¨ç†è½¨è¿¹è¿›è¡ŒFine-tuningï¼Œä»¥æå‡æ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€ä¸­çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹è§„æ¨¡çš„æ‰©å¤§åœ¨En-CoTä¸­æå‡äº†å¤šè¯­è¨€ä»»åŠ¡æ€§èƒ½ï¼Œä½†åœ¨Target-CoTä¸­è¡¨ç°æ»åï¼Œå°¤å…¶åœ¨æ•°å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½å·®è·æ˜æ˜¾ã€‚æ­¤å¤–ï¼ŒFine-tuningä½¿ç”¨è‡ªåŠ¨ç¿»è¯‘çš„æ¨ç†è½¨è¿¹ç›¸è¾ƒäºç›®æ ‡è¯­è¨€è½¨è¿¹è¡¨ç°æ›´ä½³ï¼Œæ˜¾ç¤ºå‡ºåˆæˆæ•°æ®çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬å¤šè¯­è¨€æ™ºèƒ½åŠ©æ‰‹ã€è·¨è¯­è¨€æ•™è‚²å·¥å…·ä»¥åŠå…¨çƒåŒ–çš„è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æœåŠ¡äºä¸åŒè¯­è¨€ç”¨æˆ·çš„éœ€æ±‚ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½çš„æ™®åŠä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While large reasoning models have shown remarkable ability to generate long chains-of-thought (CoTs) in English, we still lack understanding of how these long-form reasoning abilities transfer to the vast majority of the world's languages. In this work, we systematically investigate four key stages of model development--scaling, pretraining, post-training, and inference--to understand how long CoT capabilities extend beyond English. We compare two reasoning settings across nine non-English target languages: En-CoT, where models process target-language inputs, but reason in English; and Target-CoT, where models both process inputs and generate long CoTs in the target language. We find that scaling reasoning model size improves multilingual task performance in En-CoT, but Target-CoT performance lags behind. This gap widens for tasks requiring long, multi-step CoTs such as mathematical reasoning. Shifting to pretraining, we find that adding a specialized reasoning stage enhances En-CoT performance but degrades Target-CoT, whereas broad multilingual pretraining improves both modes simultaneously. Given the scarcity of high-quality reasoning traces in languages other than English, we explore synthetic data curation approaches for post-training. We demonstrate that fine-tuning on reasoning traces automatically translated from gold English traces outperforms fine-tuning on target-language traces distilled from large reasoning models. Finally, we report disparities in inference efficiency between languages and uncover language-specific failure modes in CoTs. We release models, datasets, and code to foster further research.

