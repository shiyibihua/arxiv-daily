---
layout: default
title: Scaled Signed Averaging Improves In-Context and Early Learning Benchmark Performance in Small Transformers
---

# Scaled Signed Averaging Improves In-Context and Early Learning Benchmark Performance in Small Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14685" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14685v2</a>
  <a href="https://arxiv.org/pdf/2508.14685.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14685v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14685v2', 'Scaled Signed Averaging Improves In-Context and Early Learning Benchmark Performance in Small Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Omar Naim, Swarnadeep Bhar, JÃ©rÃ´me Bolte, Nicholas Asher

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20 (æ›´æ–°: 2025-10-07)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¼©æ”¾ç­¾åå¹³å‡æ³•ä»¥è§£å†³å°å‹å˜æ¢å™¨çš„å­¦ä¹ é™åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `é‡è¯ç†è§£` `çº¿æ€§å‡½æ•°` `å˜æ¢å™¨æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç¼©æ”¾ç­¾åå¹³å‡æ³•` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†é‡è¯å’Œçº¿æ€§å‡½æ•°ä»»åŠ¡æ—¶å­˜åœ¨æ€§èƒ½ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨Softmaxä½œä¸ºè¯„åˆ†å‡½æ•°æ—¶ã€‚
2. è®ºæ–‡æå‡ºçš„ç¼©æ”¾ç­¾åå¹³å‡æ³•ï¼ˆSSAï¼‰ä½œä¸ºSoftmaxçš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ—¨åœ¨æ”¹å–„æ¨¡å‹åœ¨è¯­ä¹‰ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSSAåœ¨ICLä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ—©æœŸå­¦ä¹ åŸºå‡†ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„Softmaxå˜æ¢å™¨æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹é¢çš„èƒ½åŠ›å¤‡å—å…³æ³¨ï¼Œä½†æˆ‘ä»¬ç ”ç©¶äº†å…¶åœ¨æ¶‰åŠé‡è¯ï¼ˆå¦‚â€œæ‰€æœ‰â€å’Œâ€œä¸€äº›â€ï¼‰åŠçº¿æ€§å‡½æ•°çš„è¯­ä¹‰ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è¯„åˆ†å‡½æ•°Softmaxæ˜¯å¯¼è‡´è¿™äº›å±€é™æ€§çš„ä¸€ä¸ªå› ç´ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ›¿ä»£æ–¹æ¡ˆâ€”â€”ç¼©æ”¾ç­¾åå¹³å‡æ³•ï¼ˆSSAï¼‰ï¼Œä»¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSSAåœ¨æˆ‘ä»¬çš„ICLä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ—©æœŸå­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†åŸºå‡†å’Œé›¶æ ·æœ¬åŠå°‘æ ·æœ¬çš„è¯­è¨€æ¢æµ‹ä»»åŠ¡ä¸­è¶…è¶Šäº†ä½¿ç”¨Softmaxçš„å˜æ¢å™¨æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å¯¹é‡è¯å’Œçº¿æ€§å‡½æ•°ä»»åŠ¡çš„è¡¨ç°ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–Softmaxè¯„åˆ†å‡½æ•°ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç¼©æ”¾ç­¾åå¹³å‡æ³•ï¼ˆSSAï¼‰ä½œä¸ºSoftmaxçš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡æ”¹å˜è¯„åˆ†æœºåˆ¶æ¥æ”¹å–„æ¨¡å‹å¯¹å¤æ‚è¯­ä¹‰ä»»åŠ¡çš„ç†è§£èƒ½åŠ›ï¼Œè¿›è€Œæé«˜æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSSAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥åµŒå…¥ã€æ³¨æ„åŠ›æœºåˆ¶å’Œè¾“å‡ºç”Ÿæˆæ¨¡å—ï¼Œå…¶ä¸­æ³¨æ„åŠ›æœºåˆ¶é‡‡ç”¨SSAæ›¿ä»£ä¼ ç»Ÿçš„Softmaxï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹è¯­ä¹‰ä¿¡æ¯çš„æ•æ‰èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†ç¼©æ”¾ç­¾åå¹³å‡æ³•ï¼Œè¿™ä¸€æ–¹æ³•é€šè¿‡è°ƒæ•´è¯„åˆ†å‡½æ•°çš„è®¡ç®—æ–¹å¼ï¼Œæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä¸ä¼ ç»ŸSoftmaxæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSSAçš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­å‡èƒ½å‘æŒ¥æœ€ä½³æ•ˆæœï¼ŒåŒæ—¶æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ä¹Ÿè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”æ–°çš„è¯„åˆ†æœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç¼©æ”¾ç­¾åå¹³å‡æ³•ï¼ˆSSAï¼‰åœ¨å¤šä¸ªä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é‡è¯å’Œçº¿æ€§å‡½æ•°ä»»åŠ¡æ—¶ï¼Œç›¸è¾ƒäºä½¿ç”¨Softmaxçš„æ¨¡å‹ï¼ŒSSAåœ¨æ—©æœŸå­¦ä¹ åŸºå‡†ä¸Šæå‡å¹…åº¦è¾¾åˆ°äº†XX%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€è¯­ä¹‰ç†è§£å’Œæ—©æœŸå­¦ä¹ ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚è¯­ä¹‰ç»“æ„æ—¶ï¼ŒSSAèƒ½å¤Ÿæä¾›æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼ŒSSAå¯èƒ½ä¼šå½±å“æ›´å¤šåŸºäºå˜æ¢å™¨çš„æ¨¡å‹è®¾è®¡ï¼Œæ¨åŠ¨è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°æå‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Large Language models' abilities for in-context learning (ICL) have drawn much attention, we examine some of its limitations on semantic tasks involving quantifiers like "all" and "some", as well as on tasks with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these limitations. We propose scaled signed averaging (SSA), a novel alternative to Softmax to mitigate these problems. We show that SSA significantly improves performance on our ICL tasks. In addition, SSA outperforms transformer models with Softmax on several early learning NLP benchmarks and linguistic probing tasks on zero and few-shot settings.

