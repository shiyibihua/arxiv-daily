---
layout: default
title: DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement
---

# DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14391" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14391v1</a>
  <a href="https://arxiv.org/pdf/2508.14391.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14391v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14391v1', 'DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yupei Yang, Fan Feng, Lin Yang, Wanxi Deng, Lin Qu, Biwei Huang, Shikui Tu, Lei Xu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDEPTHæ¡†æ¶ä»¥è§£å†³å…³ç³»æå–ä¸­çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…³ç³»æå–` `çŸ¥è¯†å›¾è°±` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¾èµ–æ„ŸçŸ¥` `å¼ºåŒ–å­¦ä¹ ` `å¥å­ç®€åŒ–` `ç»“æ„åŒ–çŸ¥è¯†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…³ç³»æå–æ–¹æ³•åœ¨å¤æ‚å¥å­ç»“æ„ä¸‹éš¾ä»¥å¯é åˆ¤æ–­å…³ç³»çš„å­˜åœ¨ï¼Œå¯¼è‡´å¹»è§‰ç°è±¡é¢‘å‘ã€‚
2. DEPTHæ¡†æ¶é€šè¿‡ä¾èµ–æ„ŸçŸ¥å¥å­ç®€åŒ–å’ŒåŒå±‚æ¬¡ç²¾ç‚¼ï¼Œå‡å°‘è¯­æ³•å™ªå£°å¹¶æå‡å…³ç³»æå–çš„å‡†ç¡®æ€§ã€‚
3. åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDEPTHå°†å¹»è§‰ç‡é™ä½è‡³7.0%ï¼ŒF1åˆ†æ•°è¾ƒæœ€å…ˆè¿›åŸºçº¿æå‡17.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…³ç³»æå–æ˜¯æ„å»ºç»“æ„åŒ–çŸ¥è¯†çš„é‡è¦ç¯èŠ‚ï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­¤é¢†åŸŸå±•ç°å‡ºè‰¯å¥½æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºå…³ç³»åˆ†ç±»ï¼Œéš¾ä»¥å¯é åˆ¤æ–­å…³ç³»çš„å­˜åœ¨ï¼Œå°¤å…¶åœ¨å¤æ‚å¥å­ç»“æ„ä¸‹ï¼Œå®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå¯¼è‡´çŸ¥è¯†å›¾è°±ä¸­çš„å™ªå£°è¾¹ç¼˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºDEPTHæ¡†æ¶ï¼Œç»“åˆä¾èµ–æ„ŸçŸ¥å¥å­ç®€åŒ–å’ŒåŒå±‚æ¬¡å±‚æ¬¡åŒ–ç²¾ç‚¼ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆé€šè¿‡æœ€çŸ­ä¾èµ–è·¯å¾„æå–å…³ç³»ï¼Œç®€åŒ–å¥å­ä»¥å‡å°‘è¯­æ³•å™ªå£°ï¼›å…¶æ¬¡èšåˆå±€éƒ¨é¢„æµ‹å¹¶åŸºäºæ•´ä½“ç†è§£è¿›è¡Œä¿®æ­£ã€‚å®éªŒè¡¨æ˜ï¼ŒDEPTHå°†å¹»è§‰ç‡é™ä½è‡³7.0%ï¼ŒF1åˆ†æ•°æå‡17.2%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å…³ç³»æå–ä¸­å­˜åœ¨çš„å¹»è§‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚å¥å­ç»“æ„ä¸‹éš¾ä»¥å‡†ç¡®åˆ¤æ–­å…³ç³»çš„å­˜åœ¨ï¼Œå¯¼è‡´çŸ¥è¯†å›¾è°±ä¸­å‡ºç°å™ªå£°è¾¹ç¼˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDEPTHæ¡†æ¶é€šè¿‡ä¾èµ–æ„ŸçŸ¥å¥å­ç®€åŒ–å’ŒåŒå±‚æ¬¡ç²¾ç‚¼ï¼Œæ—¨åœ¨å‡å°‘è¯­æ³•å™ªå£°å¹¶ä¿ç•™å…³é”®è¯­ä¹‰ï¼Œä»è€Œæé«˜å…³ç³»æå–çš„å¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDEPTHæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç¬¬ä¸€é˜¶æ®µæ˜¯Groundingæ¨¡å—ï¼Œé€šè¿‡æœ€çŸ­ä¾èµ–è·¯å¾„æå–å…³ç³»ï¼Œç®€åŒ–å¥å­ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯Refinementæ¨¡å—ï¼Œèšåˆå±€éƒ¨é¢„æµ‹å¹¶è¿›è¡Œæ•´ä½“ä¿®æ­£ã€‚

**å…³é”®åˆ›æ–°**ï¼šDEPTHçš„åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¾èµ–æ„ŸçŸ¥çš„å¥å­ç®€åŒ–å’ŒåŒå±‚æ¬¡çš„ç²¾ç‚¼è¿‡ç¨‹ï¼Œæ˜¾è‘—é™ä½äº†å¹»è§‰ç°è±¡ï¼Œæå‡äº†å…³ç³»æå–çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºå› æœå…³ç³»çš„å¥–åŠ±æ¨¡å‹ï¼Œä»¥å‡å°‘å¥–åŠ±é»‘å®¢ç°è±¡ï¼Œå¹¶é€šè¿‡äººç±»åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„ç¨³å¥å¾®è°ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DEPTHåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¹»è§‰ç‡é™ä½è‡³7.0%ï¼ŒåŒæ—¶F1åˆ†æ•°è¾ƒæœ€å…ˆè¿›åŸºçº¿æå‡17.2%ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œè¯æ˜äº†å…¶åœ¨å…³ç³»æå–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨çŸ¥è¯†å›¾è°±æ„å»ºã€ä¿¡æ¯æŠ½å–å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜å…³ç³»æå–çš„å‡†ç¡®æ€§ï¼ŒDEPTHèƒ½å¤Ÿä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›æ›´å¯é çš„ç»“æ„åŒ–çŸ¥è¯†ï¼Œè¿›è€Œæå‡æ™ºèƒ½ç³»ç»Ÿçš„å†³ç­–èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Relation extraction enables the construction of structured knowledge for many downstream applications. While large language models (LLMs) have shown great promise in this domain, most existing methods concentrate on relation classification, which predicts the semantic relation type between a related entity pair. However, we observe that LLMs often struggle to reliably determine whether a relation exists, especially in cases involving complex sentence structures or intricate semantics, which leads to spurious predictions. Such hallucinations can introduce noisy edges in knowledge graphs, compromising the integrity of structured knowledge and downstream reliability. To address these challenges, we propose DEPTH, a framework that integrates Dependency-aware sEntence simPlification and Two-tiered Hierarchical refinement into the relation extraction pipeline. Given a sentence and its candidate entity pairs, DEPTH operates in two stages: (1) the Grounding module extracts relations for each pair by leveraging their shortest dependency path, distilling the sentence into a minimal yet coherent relational context that reduces syntactic noise while preserving key semantics; (2) the Refinement module aggregates all local predictions and revises them based on a holistic understanding of the sentence, correcting omissions and inconsistencies. We further introduce a causality-driven reward model that mitigates reward hacking by disentangling spurious correlations, enabling robust fine-tuning via reinforcement learning with human feedback. Experiments on six benchmarks demonstrate that DEPTH reduces the average hallucination rate to 7.0\% while achieving a 17.2\% improvement in average F1 score over state-of-the-art baselines.

