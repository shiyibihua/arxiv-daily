---
layout: default
title: Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs
---

# Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14896" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.14896v2</a>
  <a href="https://arxiv.org/pdf/2508.14896.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14896v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14896v2', 'Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-20 (Êõ¥Êñ∞: 2025-10-15)

**Â§áÊ≥®**: Technical Report, Work in Progress

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/FelixMessi/QDLM)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Á≥ªÁªüÁ†îÁ©∂ÂêéËÆ≠ÁªÉÈáèÂåñ‰ª•‰ºòÂåñÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÉ®ÁΩ≤**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÂêéËÆ≠ÁªÉÈáèÂåñ` `Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàê` `ËæπÁºòËÆ°ÁÆó` `Ê®°ÂûãÂéãÁº©` `ÊøÄÊ¥ªÂºÇÂ∏∏ÂÄº` `Â§ö‰ªªÂä°ËØÑ‰º∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËæπÁºòËÆæÂ§á‰∏äÁöÑÈÉ®ÁΩ≤Èù¢‰∏¥ÂèÇÊï∞ËßÑÊ®°Â§ßÂíåËµÑÊ∫êÈúÄÊ±ÇÈ´òÁöÑÊåëÊàò„ÄÇ
2. Êú¨ÊñáÈ¶ñÊ¨°Á≥ªÁªüÁ†îÁ©∂‰∫ÜÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÔºåËØÜÂà´Âá∫ÊøÄÊ¥ªÂºÇÂ∏∏ÂÄºÂØπ‰Ωé‰ΩçÈáèÂåñÁöÑÂΩ±Âìç„ÄÇ
3. ÈÄöËøáÂ§öÁª¥Â∫¶ËØÑ‰º∞ÔºåÊàë‰ª¨Êèê‰æõ‰∫Ü‰∏çÂêåÈÖçÁΩÆ‰∏ãdLLMsÈáèÂåñË°å‰∏∫ÁöÑÂÆûÁî®ËßÅËß£ÔºåÊé®Âä®‰∫ÜÊú™Êù•Á†îÁ©∂ÊñπÂêë„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàdLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàê‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫‰ºò‰∫éËá™ÂõûÂΩíÔºàARÔºâÊ®°ÂûãÁöÑÊΩúÂäõÔºå‰ΩÜÁî±‰∫éÂÖ∂Â∫ûÂ§ßÁöÑÂèÇÊï∞ËßÑÊ®°ÂíåÈ´òËµÑÊ∫êÈúÄÊ±ÇÔºåÈÉ®ÁΩ≤Âú®ËæπÁºòËÆæÂ§á‰∏ä‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÂ∞ΩÁÆ°ÂêéËÆ≠ÁªÉÈáèÂåñÔºàPTQÔºâÂ∑≤Ë¢´ÂπøÊ≥õÂ∫îÁî®‰∫éAR LLMsÔºå‰ΩÜÂÖ∂Âú®dLLMs‰∏≠ÁöÑÈÄÇÁî®ÊÄßÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢ËÆ®„ÄÇÊú¨ÊñáÈ¶ñÊ¨°Á≥ªÁªüÁ†îÁ©∂‰∫ÜÂü∫‰∫éÊâ©Êï£ÁöÑËØ≠Ë®ÄÊ®°ÂûãÁöÑÈáèÂåñÔºåËØÜÂà´Âá∫ÂºÇÂ∏∏Â§ßÁöÑÊøÄÊ¥ªÂÄº‰Ωú‰∏∫‰Ωé‰ΩçÈáèÂåñÁöÑ‰∏ªË¶ÅÊåëÊàò„ÄÇÊàë‰ª¨ÂÆûÁé∞‰∫ÜÂÖàËøõÁöÑPTQÊñπÊ≥ïÔºåÂπ∂Âú®Â§öÁßç‰ªªÂä°Á±ªÂûãÂíåÊ®°ÂûãÂèò‰Ωì‰∏äËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåÊèê‰æõ‰∫Ü‰∏çÂêåÈÖçÁΩÆ‰∏ãdLLMsÈáèÂåñË°å‰∏∫ÁöÑÂÆûÁî®ËßÅËß£„ÄÇÂ∏åÊúõÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏∫Êú™Êù•È´òÊïàÁöÑdLLMÈÉ®ÁΩ≤Â•†ÂÆöÂü∫Á°Ä„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËæπÁºòËÆæÂ§á‰∏äÈÉ®ÁΩ≤Êó∂ÁöÑÈáèÂåñÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÊøÄÊ¥ªÂºÇÂ∏∏ÂÄºÊó∂Èöæ‰ª•‰øùÊåÅÁ≤æÂ∫¶ÔºåÂΩ±Âìç‰∫Ü‰Ωé‰ΩçÈáèÂåñÁöÑÊïàÊûú„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÈÄöËøáËØÜÂà´ÊøÄÊ¥ªÂºÇÂ∏∏ÂÄºÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ≥ªÁªüÂåñÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÈáèÂåñÂêéÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÁ≤æÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂ÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÊøÄÊ¥ªÂÄºÂàÜÊûê„ÄÅÈáèÂåñÊñπÊ≥ïÂÆûÁé∞ÂíåÂ§ö‰ªªÂä°ËØÑ‰º∞‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºåÁ°Æ‰øùÂú®‰∏çÂêå‰ªªÂä°ÂíåÊ®°ÂûãÂèò‰Ωì‰∏ãÁöÑÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨Á†îÁ©∂ÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÈ¶ñÊ¨°Â∞ÜÂêéËÆ≠ÁªÉÈáèÂåñÂ∫îÁî®‰∫éÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÂπ∂Á≥ªÁªüËØÑ‰º∞ÂÖ∂Âú®‰∏çÂêåÈÖçÁΩÆ‰∏ãÁöÑË°®Áé∞ÔºåÂ°´Ë°•‰∫ÜËØ•È¢ÜÂüüÁöÑÁ†îÁ©∂Á©∫ÁôΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊàë‰ª¨Âú®ÈáèÂåñËøáÁ®ã‰∏≠ËÆæÁΩÆ‰∫Ü‰∏çÂêåÁöÑ‰ΩçÂÆΩÂíåÈáèÂåñÊñπÊ≥ïÔºåÂπ∂ËÆæËÆ°‰∫ÜÈÄÇÂ∫îÂ§öÁßç‰ªªÂä°Á±ªÂûãÁöÑËØÑ‰º∞Ê†áÂáÜÔºå‰ª•Á°Æ‰øùÈáèÂåñÂêéÁöÑÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈááÁî®Êñ∞ÊèêÂá∫ÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÊñπÊ≥ïÂêéÔºåÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ö‰∏™‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÊèêÂçáÊòæËëóÔºåÂ∞§ÂÖ∂ÊòØÂú®‰Ωé‰ΩçÈáèÂåñÊÉÖÂÜµ‰∏ãÔºåÊ®°ÂûãÁöÑÁ≤æÂ∫¶ÊçüÂ§±Èôç‰ΩéËá≥5%‰ª•ÂÜÖÔºåÁõ∏ËæÉ‰∫é‰º†ÁªüÊñπÊ≥ïÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Êú¨Á†îÁ©∂ÁöÑÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÈúÄË¶ÅÂú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠ÈÉ®ÁΩ≤ÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°ÔºåÂ¶ÇÁßªÂä®ËÆæÂ§á‰∏äÁöÑÊô∫ËÉΩÂä©Êâã„ÄÅËæπÁºòËÆ°ÁÆó‰∏≠ÁöÑÊñáÊú¨ÁîüÊàêÁ≠â„ÄÇÈÄöËøá‰ºòÂåñÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈáèÂåñÊñπÊ≥ïÔºåÂèØ‰ª•ÊòæËëóÊèêÂçáÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊïàÁéáÂíåÂìçÂ∫îÈÄüÂ∫¶ÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÊôÆÂèä‰∏éÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. Our code is publicly available at https://github.com/FelixMessi/QDLM.

