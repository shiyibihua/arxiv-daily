---
layout: default
title: Credence Calibration Game? Calibrating Large Language Models through Structured Play
---

# Credence Calibration Game? Calibrating Large Language Models through Structured Play

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14390" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14390v1</a>
  <a href="https://arxiv.org/pdf/2508.14390.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14390v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14390v1', 'Credence Calibration Game? Calibrating Large Language Models through Structured Play')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ke Fang, Tianyi Zhao, Lu Cheng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ¸¸æˆç»“æ„çš„æ ¡å‡†æ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„ä¿¡å¿ƒä¼°è®¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ä¿¡å¿ƒæ ¡å‡†` `æ¸¸æˆåŒ–å­¦ä¹ ` `åŠ¨æ€åé¦ˆ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ ¡å‡†æ–¹æ³•å¤šä¾èµ–äºäº‹åè°ƒæ•´æˆ–è¾…åŠ©è®­ç»ƒï¼Œç¼ºä¹æœ‰æ•ˆçš„åŠ¨æ€åé¦ˆæœºåˆ¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¿¡å¿µæ ¡å‡†æ¸¸æˆçš„æç¤ºé©±åŠ¨æ ¡å‡†æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–äº¤äº’å¾ªç¯æ”¹å–„æ¨¡å‹ä¿¡å¿ƒä¼°è®¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹å’Œé…ç½®ä¸‹å‡æ˜¾è‘—æå‡äº†æ ¡å‡†æ•ˆæœï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å†³ç­–å…³é”®é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œç¡®ä¿å…¶ä¿¡å¿ƒä¼°è®¡ä¸å®é™…æ­£ç¡®æ€§ç›¸ç¬¦å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ ¡å‡†æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨äº‹åè°ƒæ•´æˆ–è¾…åŠ©æ¨¡å‹è®­ç»ƒï¼Œä½†è®¸å¤šæ–¹æ³•éœ€è¦é¢å¤–çš„ç›‘ç£æˆ–å‚æ•°æ›´æ–°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæç¤ºçš„æ ¡å‡†æ¡†æ¶ï¼Œçµæ„Ÿæ¥æºäºä¿¡å¿µæ ¡å‡†æ¸¸æˆã€‚è¯¥æ–¹æ³•å»ºç«‹äº†ä¸€ä¸ªç»“æ„åŒ–çš„äº¤äº’å¾ªç¯ï¼ŒLLMsæ ¹æ®å…¶é¢„æµ‹ä¿¡å¿ƒä¸æ­£ç¡®æ€§çš„ä¸€è‡´æ€§è·å¾—åé¦ˆã€‚é€šè¿‡åé¦ˆé©±åŠ¨çš„æç¤ºå’Œå¯¹å…ˆå‰è¡¨ç°çš„è‡ªç„¶è¯­è¨€æ€»ç»“ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åŠ¨æ€æ”¹å–„æ¨¡å‹æ ¡å‡†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸åŒæ¨¡å‹å’Œæ¸¸æˆé…ç½®ä¸‹ï¼Œè¯„ä¼°æŒ‡æ ‡å‡æœ‰ä¸€è‡´æå‡ï¼Œå±•ç¤ºäº†åŸºäºæ¸¸æˆçš„æç¤ºä½œä¸ºæœ‰æ•ˆæ ¡å‡†ç­–ç•¥çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å†³ç­–å…³é”®é¢†åŸŸä¸­ä¿¡å¿ƒä¼°è®¡ä¸å®é™…æ­£ç¡®æ€§ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºäº‹åè°ƒæ•´ï¼Œç¼ºä¹æœ‰æ•ˆçš„åŠ¨æ€åé¦ˆæœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹æ ¡å‡†æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¡å‡†æ¡†æ¶é€šè¿‡å¼•å…¥åŸºäºæ¸¸æˆçš„ç»“æ„åŒ–äº¤äº’ï¼Œåˆ©ç”¨åé¦ˆæœºåˆ¶åŠ¨æ€è°ƒæ•´æ¨¡å‹çš„ä¿¡å¿ƒä¼°è®¡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸ç¯å¢ƒçš„äº¤äº’ä¸­ä¸æ–­ä¼˜åŒ–å…¶ä¿¡å¿ƒé¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åé¦ˆé©±åŠ¨çš„æç¤ºç”Ÿæˆæ¨¡å—ã€è‡ªç„¶è¯­è¨€æ€»ç»“æ¨¡å—å’Œæ ¡å‡†åé¦ˆæ¨¡å—ã€‚æ¨¡å‹åœ¨æ¯æ¬¡é¢„æµ‹åï¼Œæ ¹æ®å…¶ä¿¡å¿ƒä¸å®é™…ç»“æœçš„å¯¹æ¯”ï¼Œè·å¾—ç›¸åº”çš„åé¦ˆå¹¶è¿›è¡Œè°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ¸¸æˆåŒ–çš„ç»“æ„åŒ–äº¤äº’æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­è‡ªæˆ‘æ ¡å‡†ï¼Œè€Œä¸æ˜¯ä¾èµ–äºé™æ€çš„åå¤„ç†æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ä¸ç°æœ‰çš„æ ¡å‡†æŠ€æœ¯æœ¬è´¨ä¸Šä¸åŒï¼Œå¼ºè°ƒäº†äº¤äº’åé¦ˆçš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºè‡ªç„¶è¯­è¨€çš„æç¤ºç”Ÿæˆç­–ç•¥ï¼Œç»“åˆå†å²è¡¨ç°çš„æ€»ç»“ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹åé¦ˆçš„ç†è§£å’Œå“åº”èƒ½åŠ›ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†ä¿¡å¿ƒä¸æ­£ç¡®æ€§çš„å¯¹é½ç¨‹åº¦ï¼Œä»¥ç¡®ä¿æ ¡å‡†æ•ˆæœçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ ¡å‡†æ¡†æ¶åœ¨å¤šä¸ªæ¨¡å‹å’Œé…ç½®ä¸‹å‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›é…ç½®ä¸‹ï¼Œæ¨¡å‹çš„æ ¡å‡†è¯¯å·®é™ä½äº†20%ä»¥ä¸Šï¼ŒéªŒè¯äº†åŸºäºæ¸¸æˆçš„æç¤ºæ–¹æ³•åœ¨æ¨¡å‹æ ¡å‡†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å†³ç­–æ”¯æŒã€é‡‘èé£é™©è¯„ä¼°å’Œè‡ªåŠ¨åŒ–å®¢æœç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„ä¿¡å¿ƒæ ¡å‡†èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å…¶åœ¨å…³é”®å†³ç­–åœºæ™¯ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œæœªæ¥å¯èƒ½å¯¹è¡Œä¸šæ ‡å‡†å’Œå®è·µäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As Large Language Models (LLMs) are increasingly deployed in decision-critical domains, it becomes essential to ensure that their confidence estimates faithfully correspond to their actual correctness. Existing calibration methods have primarily focused on post-hoc adjustments or auxiliary model training; however, many of these approaches necessitate additional supervision or parameter updates. In this work, we propose a novel prompt-based calibration framework inspired by the Credence Calibration Game. Our method establishes a structured interaction loop wherein LLMs receive feedback based on the alignment of their predicted confidence with correctness. Through feedback-driven prompting and natural language summaries of prior performance, our framework dynamically improves model calibration. Extensive experiments across models and game configurations demonstrate consistent improvements in evaluation metrics. Our results highlight the potential of game-based prompting as an effective strategy for LLM calibration. Code and data are available at https://anonymous.4open.science/r/LLM-Calibration/.

