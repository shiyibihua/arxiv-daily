---
layout: default
title: Improving LLMs for Machine Translation Using Synthetic Preference Data
---

# Improving LLMs for Machine Translation Using Synthetic Preference Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14951" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14951v1</a>
  <a href="https://arxiv.org/pdf/2508.14951.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14951v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14951v1', 'Improving LLMs for Machine Translation Using Synthetic Preference Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dario Vajda, Domen VreÅ¡, Marko Robnik-Å ikonja

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

**å¤‡æ³¨**: Paper with individual presentation at LUHME workshop at ECAI 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡åˆæˆåå¥½æ•°æ®æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æœºå™¨ç¿»è¯‘èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨ç¿»è¯‘` `ç›´æ¥åå¥½ä¼˜åŒ–` `åˆæˆæ•°æ®` `è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†ä»å­˜åœ¨ç¿»è¯‘è´¨é‡ä¸ç¨³å®šå’Œé”™è¯¯é¢‘å‘çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒï¼Œåˆ©ç”¨åˆæˆçš„è´¨é‡æ’åæ•°æ®æ¥æå‡æœºå™¨ç¿»è¯‘æ¨¡å‹çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨ç¿»è¯‘ç»´åŸºç™¾ç§‘æ–‡ç« æ—¶ï¼ŒCOMETå¾—åˆ†æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”å‡å°‘äº†è¯­è¨€å’Œæ ¼å¼é”™è¯¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹å·²æˆä¸ºæœ‰æ•ˆçš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿã€‚æœ¬æ–‡æ¢è®¨å¦‚ä½•åˆ©ç”¨ç›¸å¯¹è¾ƒå°‘ä¸”æ˜“äºç”Ÿæˆçš„æ•°æ®èµ„æºï¼Œæ”¹å–„é€šç”¨æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„è¡¨ç°ã€‚ä»¥æ–¯æ´›æ–‡å°¼äºšè¯­ä¸ºæ¡ˆä¾‹ï¼Œé‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒå¯¹GaMS-9B-Instructæ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼Œä½¿ç”¨ç¨‹åºåŒ–ç­–åˆ’å’Œå¢å¼ºçš„å…¬å…±æ•°æ®é›†å­é›†ã€‚DPOéœ€è¦è´¨é‡æ’åå®ä¾‹å¯¹ï¼Œæˆ‘ä»¬é€šè¿‡ç¿»è¯‘è‹±æ–‡ç»´åŸºç™¾ç§‘æ–‡ç« ç”Ÿæˆè®­ç»ƒæ•°æ®é›†ï¼Œå¹¶åŸºäºå¯å‘å¼æ–¹æ³•å’Œè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚COMETï¼‰å¯¹ç¿»è¯‘ç»“æœè¿›è¡Œæ’åã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨ç¿»è¯‘ç»´åŸºç™¾ç§‘æ–‡ç« æ—¶ï¼Œè¶…è¶Šäº†å‚ä¸æ•°æ®é›†ç”Ÿæˆçš„ä¸¤ä¸ªæ¨¡å‹ï¼ŒCOMETå¾—åˆ†åˆ†åˆ«æå‡çº¦0.04å’Œ0.02ï¼Œå¹¶ä¸”æ›´ä¸€è‡´åœ°é¿å…äº†è¯­è¨€å’Œæ ¼å¼é”™è¯¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä¸­å­˜åœ¨çš„ç¿»è¯‘è´¨é‡ä¸ç¨³å®šå’Œé”™è¯¯é¢‘å‘çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡ç¿»è¯‘æ—¶ï¼Œå¾€å¾€ä¾èµ–äºå¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜ä¸”æ•ˆç‡ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆçš„è´¨é‡æ’åæ•°æ®æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡ç¨‹åºåŒ–ç”Ÿæˆæ•°æ®ï¼Œé™ä½äº†å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œä»è€Œæé«˜äº†æ•°æ®ç”Ÿæˆçš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®ç”Ÿæˆã€è´¨é‡æ’åå’Œæ¨¡å‹å¾®è°ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œä½¿ç”¨ä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGaMS-9B-Instructå’ŒEuroLLM-9B-Instructï¼‰ç¿»è¯‘è‹±æ–‡ç»´åŸºç™¾ç§‘æ–‡ç« ï¼Œç„¶ååŸºäºå¯å‘å¼æ–¹æ³•å’Œè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å¯¹ç¿»è¯‘ç»“æœè¿›è¡Œæ’åï¼Œæœ€ååˆ©ç”¨DPOå¯¹GaMS-9B-Instructæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡ç¨‹åºåŒ–ç”Ÿæˆçš„åˆæˆåå¥½æ•°æ®æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚è¿™ç§æ–¹æ³•ä¸ä¼ ç»Ÿçš„ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨çš„è®­ç»ƒæ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DPOè®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹çš„ç¿»è¯‘è´¨é‡ï¼Œå¹¶é€šè¿‡COMETç­‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°ç¿»è¯‘ç»“æœçš„è´¨é‡ã€‚æ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹æ³¨é‡é¿å…è¯­è¨€å’Œæ ¼å¼é”™è¯¯ï¼Œç¡®ä¿ç”Ÿæˆçš„ç¿»è¯‘æ›´åŠ è‡ªç„¶æµç•…ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨ç¿»è¯‘ç»´åŸºç™¾ç§‘æ–‡ç« æ—¶ï¼ŒCOMETå¾—åˆ†åˆ†åˆ«æå‡çº¦0.04å’Œ0.02ï¼Œè¶…è¶Šäº†å‚ä¸æ•°æ®é›†ç”Ÿæˆçš„ä¸¤ä¸ªåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨è¯­è¨€å’Œæ ¼å¼é”™è¯¯æ–¹é¢è¡¨ç°æ›´ä¸ºä¸€è‡´ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„ç¿»è¯‘è´¨é‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€ç¿»è¯‘ç³»ç»Ÿã€è·¨æ–‡åŒ–äº¤æµå¹³å°ä»¥åŠå›½é™…åŒ–è½¯ä»¶å¼€å‘ç­‰ã€‚é€šè¿‡æå‡æœºå™¨ç¿»è¯‘çš„è´¨é‡å’Œä¸€è‡´æ€§ï¼Œå¯ä»¥å¤§å¹…æé«˜ç”¨æˆ·ä½“éªŒï¼Œä¿ƒè¿›ä¸åŒè¯­è¨€ç”¨æˆ·ä¹‹é—´çš„æ²Ÿé€šä¸ç†è§£ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šè¯­è¨€å¯¹çš„ç¿»è¯‘ä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨æœºå™¨ç¿»è¯‘æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models have emerged as effective machine translation systems. In this paper, we explore how a general instruction-tuned large language model can be improved for machine translation using relatively few easily produced data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct model using Direct Preference Optimization (DPO) training on a programmatically curated and enhanced subset of a public dataset. As DPO requires pairs of quality-ranked instances, we generated its training dataset by translating English Wikipedia articles using two LLMs, GaMS-9B-Instruct and EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics coupled with automatic evaluation metrics such as COMET. The evaluation shows that our fine-tuned model outperforms both models involved in the dataset generation. In comparison to the baseline models, the fine-tuned model achieved a COMET score gain of around 0.04 and 0.02, respectively, on translating Wikipedia articles. It also more consistently avoids language and formatting errors.

