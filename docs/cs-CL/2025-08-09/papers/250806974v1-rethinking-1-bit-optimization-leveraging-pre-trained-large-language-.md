---
layout: default
title: Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models
---

# Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06974" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06974v1</a>
  <a href="https://arxiv.org/pdf/2508.06974.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06974v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06974v1', 'Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhijun Tu, Hanting Chen, Siqi Liu, Chuanjian Liu, Jian Li, Jie Hu, Yunhe Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-09

**å¤‡æ³¨**: 16 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¸è¿›å¼è®­ç»ƒæ–¹æ³•ä»¥ä¼˜åŒ–1-bitå¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `1-bité‡åŒ–` `å¤§è¯­è¨€æ¨¡å‹` `æ¸è¿›å¼è®­ç»ƒ` `é¢„è®­ç»ƒæ¨¡å‹` `äºŒè¿›åˆ¶æ„ŸçŸ¥åˆå§‹åŒ–` `åŒé‡ç¼©æ”¾è¡¥å¿` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰1-bit LLMè®­ç»ƒæ–¹æ³•é€šå¸¸ä»é›¶å¼€å§‹ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´è®­ç»ƒæˆæœ¬é«˜å’Œå‡†ç¡®æ€§ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å¹³æ»‘è½¬æ¢æµ®ç‚¹æƒé‡ä¸ºäºŒè¿›åˆ¶æƒé‡ï¼Œç»“åˆäºŒè¿›åˆ¶æ„ŸçŸ¥åˆå§‹åŒ–å’ŒåŒé‡ç¼©æ”¾è¡¥å¿ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨ä¸åŒè§„æ¨¡çš„LLMä¸Šå‡è¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå‡å°‘äº†è®­ç»ƒæˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

1-bit LLMé‡åŒ–åœ¨å‡å°‘å­˜å‚¨å’Œè®¡ç®—æˆæœ¬æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä»å¤´å¼€å§‹è®­ç»ƒ1-bit LLMï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬å’Œæ˜¾è‘—çš„å‡†ç¡®æ€§ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸€è‡´çš„æ¸è¿›å¼è®­ç»ƒæ–¹æ³•ï¼Œå¹³æ»‘åœ°å°†æµ®ç‚¹æƒé‡è½¬æ¢ä¸ºäºŒè¿›åˆ¶æƒé‡ï¼Œå¹¶ç»“åˆäºŒè¿›åˆ¶æ„ŸçŸ¥åˆå§‹åŒ–å’ŒåŒé‡ç¼©æ”¾è¡¥å¿ï¼Œä»¥é™ä½æ¸è¿›è®­ç»ƒçš„éš¾åº¦å¹¶æå‡æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒè§„æ¨¡çš„LLMä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥å®ç°é«˜æ€§èƒ½çš„1-bit LLMï¼Œæ¶ˆé™¤äº†ä»å¤´è®­ç»ƒçš„é«˜æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰1-bit LLMè®­ç»ƒæ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„é—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒæˆæœ¬é«˜å’Œå‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ¸è¿›å¼è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡é€æ­¥å°†æµ®ç‚¹æƒé‡è½¬åŒ–ä¸ºäºŒè¿›åˆ¶æƒé‡ï¼Œé™ä½äº†ç›´æ¥é€‚åº”çš„éš¾åº¦ï¼ŒåŒæ—¶å¼•å…¥äº†äºŒè¿›åˆ¶æ„ŸçŸ¥åˆå§‹åŒ–å’ŒåŒé‡ç¼©æ”¾è¡¥å¿ä»¥æå‡è®­ç»ƒæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬åˆå§‹åŒ–é˜¶æ®µã€æ¸è¿›è®­ç»ƒé˜¶æ®µå’Œæ€§èƒ½è¯„ä¼°é˜¶æ®µã€‚åˆå§‹åŒ–é˜¶æ®µä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæƒé‡åˆå§‹åŒ–ï¼Œæ¸è¿›è®­ç»ƒé˜¶æ®µåˆ™é€šè¿‡é€æ­¥è½¬æ¢æƒé‡å®ç°äºŒè¿›åˆ¶åŒ–ï¼Œæœ€åè¿›è¡Œæ€§èƒ½è¯„ä¼°ä»¥éªŒè¯æ¨¡å‹æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ä¸€è‡´çš„æ¸è¿›å¼è®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å…¨ç²¾åº¦ä¸1-bitè¡¨ç¤ºä¹‹é—´çš„å·®è·ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†äºŒè¿›åˆ¶æ„ŸçŸ¥åˆå§‹åŒ–ç­–ç•¥ï¼Œç¡®ä¿åˆå§‹æƒé‡é€‚åº”äºŒè¿›åˆ¶åŒ–éœ€æ±‚ï¼ŒåŒæ—¶å¼•å…¥åŒé‡ç¼©æ”¾è¡¥å¿æœºåˆ¶ï¼Œä»¥å¹³è¡¡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°å’Œæ¨¡å‹è¡¨ç°ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸åŒè§„æ¨¡çš„LLMä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†é«˜æ€§èƒ½1-bit LLMçš„å¯è¡Œæ€§ï¼Œä¸”æ— éœ€ä»å¤´è®­ç»ƒï¼Œæå¤§é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡ä¼˜åŒ–1-bit LLMçš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—é™ä½æ¨¡å‹çš„å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼Œä½¿å¾—å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å¾—ä»¥åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 1-bit LLM quantization offers significant advantages in reducing storage and computational costs. However, existing methods typically train 1-bit LLMs from scratch, failing to fully leverage pre-trained models. This results in high training costs and notable accuracy degradation. We identify that the large gap between full precision and 1-bit representations makes direct adaptation difficult. In this paper, we introduce a consistent progressive training for both forward and backward, smoothly converting the floating-point weights into the binarized ones. Additionally, we incorporate binary-aware initialization and dual-scaling compensation to reduce the difficulty of progressive training and improve the performance. Experimental results on LLMs of various sizes demonstrate that our method outperforms existing approaches. Our results show that high-performance 1-bit LLMs can be achieved using pre-trained models, eliminating the need for expensive training from scratch.

