---
layout: default
title: Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution
---

# Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07111" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07111v1</a>
  <a href="https://arxiv.org/pdf/2508.07111.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07111v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07111v1', 'Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Falaah Arif Khan, Nivedha Sivakumar, Yinong Oliver Wang, Katherine Metcalf, Cezanne Camacho, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWinoIdentityåŸºå‡†ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„äº¤å‰åè§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äº¤å‰åè§` `å¤§å‹è¯­è¨€æ¨¡å‹` `å…¬å¹³æ€§è¯„ä¼°` `WinoIdentity` `æ ¸å¿ƒæŒ‡ä»£ä¿¡å¿ƒå·®å¼‚` `ç¤¾ä¼šå±æ€§` `AIä¼¦ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†äº¤å‰åè§æ—¶å­˜åœ¨ä¸è¶³ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘å¤šé‡èº«ä»½äº¤å‰å¸¦æ¥çš„å¤æ‚æ€§ã€‚
2. æœ¬æ–‡æå‡ºWinoIdentityåŸºå‡†ï¼Œé€šè¿‡å¢åŠ å¤šè¾¾25ä¸ªç¤¾ä¼šå±æ€§ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒäº¤å‰èº«ä»½ä¸‹çš„ä¿¡å¿ƒå·®å¼‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨æŸäº›èº«ä»½ä¸‹çš„ä¿¡å¿ƒå·®å¼‚é«˜è¾¾40%ï¼Œå°¤å…¶å¯¹åŒé‡åŠ£åŠ¿èº«ä»½è¡¨ç°å‡ºè¾ƒä½çš„ä¿¡å¿ƒï¼Œåæ˜ å‡ºæ¨¡å‹çš„æ½œåœ¨åè§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œç„¶è€Œå®ƒä»¬å¯èƒ½åæ˜ å¹¶åŠ å‰§ç¤¾ä¼šåè§ï¼Œå°¤å…¶æ˜¯åœ¨æ‹›è˜å’Œæ‹›ç”Ÿç­‰å…³é”®ç¤¾ä¼šåœºæ™¯ä¸­ã€‚æœ¬æ–‡æ‰©å±•äº†å•ä¸€è½´çº¿çš„å…¬å¹³æ€§è¯„ä¼°ï¼Œæ¢è®¨äº¤å‰åè§ï¼Œæå‡ºäº†æ–°çš„åŸºå‡†WinoIdentityï¼Œé€šè¿‡å¢åŠ 25ä¸ªä¸æ€§åˆ«äº¤å‰çš„ç¤¾ä¼šå±æ€§ï¼Œç”Ÿæˆ245,700ä¸ªæç¤ºä»¥è¯„ä¼°50ç§åè§æ¨¡å¼ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨ä¸åŒäººå£å±æ€§ä¸‹çš„ä¿¡å¿ƒå·®å¼‚é«˜è¾¾40%ï¼Œå°¤å…¶åœ¨ååˆ»æ¿å°è±¡çš„ç¯å¢ƒä¸­ï¼Œå¯¹åŒé‡åŠ£åŠ¿èº«ä»½çš„ä¿¡å¿ƒæœ€ä½ã€‚è¿™è¡¨æ˜LLMsçš„ä¼˜å¼‚è¡¨ç°å¯èƒ½æºäºè®°å¿†è€Œéé€»è¾‘æ¨ç†ï¼Œæ­ç¤ºäº†ä»·å€¼å¯¹é½å’Œæœ‰æ•ˆæ€§æ–¹é¢çš„ç‹¬ç«‹å¤±è´¥ï¼Œå¯èƒ½å¯¼è‡´ç¤¾ä¼šä¼¤å®³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†äº¤å‰åè§æ—¶çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€è½´çº¿çš„å…¬å¹³æ€§ï¼Œæœªèƒ½è€ƒè™‘å¤šé‡èº«ä»½äº¤å‰æ‰€å¸¦æ¥çš„ç‹¬ç‰¹åŠ£åŠ¿æ¨¡å¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºWinoIdentityåŸºå‡†ï¼Œå¢åŠ å¤šä¸ªç¤¾ä¼šå±æ€§ä¸æ€§åˆ«çš„äº¤å‰ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒèº«ä»½ä¸‹çš„ä¿¡å¿ƒå·®å¼‚ï¼Œä»¥æ­ç¤ºæ½œåœ¨çš„äº¤å‰åè§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆæ‰©å±•äº†WinoBiasæ•°æ®é›†ï¼Œå¢åŠ äº†25ä¸ªä¸æ€§åˆ«äº¤å‰çš„å±æ€§ï¼Œç”Ÿæˆ245,700ä¸ªæç¤ºã€‚æ¥ç€ï¼Œé€šè¿‡æ ¸å¿ƒæŒ‡ä»£ä¿¡å¿ƒå·®å¼‚æŒ‡æ ‡è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒèº«ä»½ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†æ ¸å¿ƒæŒ‡ä»£ä¿¡å¿ƒå·®å¼‚è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿé‡åŒ–æ¨¡å‹å¯¹ä¸åŒäº¤å‰èº«ä»½çš„ä¿¡å¿ƒå·®å¼‚ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚èº«ä»½æ—¶çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§äººå£å±æ€§è¿›è¡Œäº¤å‰åˆ†æï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹åœ¨ååˆ»æ¿å°è±¡ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œè®¾è®¡äº†ç›¸åº”çš„è¯„ä¼°æµç¨‹å’ŒæŒ‡æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œäº”ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒäººå£å±æ€§ä¸‹çš„ä¿¡å¿ƒå·®å¼‚é«˜è¾¾40%ã€‚ç‰¹åˆ«æ˜¯åœ¨ååˆ»æ¿å°è±¡çš„ç¯å¢ƒä¸­ï¼Œæ¨¡å‹å¯¹åŒé‡åŠ£åŠ¿èº«ä»½çš„ä¿¡å¿ƒæœ€ä½ï¼Œè¡¨æ˜æ¨¡å‹åœ¨å¤„ç†å¤æ‚èº«ä»½æ—¶å­˜åœ¨æ˜¾è‘—çš„åè§ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ä»·å€¼å¯¹é½å’Œæœ‰æ•ˆæ€§æ–¹é¢çš„ç‹¬ç«‹å¤±è´¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ‹›è˜ã€æ‹›ç”Ÿã€æ³•å¾‹å’ŒåŒ»ç–—ç­‰ç¤¾ä¼šå†³ç­–åœºæ™¯ã€‚é€šè¿‡è¯†åˆ«å’Œé‡åŒ–äº¤å‰åè§ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´å…¬å¹³çš„AIç³»ç»Ÿï¼Œå‡å°‘ç¤¾ä¼šèº«ä»½åŸºç¡€çš„ä¼¤å®³ï¼Œæ¨åŠ¨ç¤¾ä¼šå…¬æ­£ã€‚æœªæ¥ï¼Œç ”ç©¶ç»“æœå¯ä¸ºæ”¿ç­–åˆ¶å®šå’ŒAIä¼¦ç†æä¾›é‡è¦å‚è€ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved impressive performance, leading to their widespread adoption as decision-support tools in resource-constrained contexts like hiring and admissions. There is, however, scientific consensus that AI systems can reflect and exacerbate societal biases, raising concerns about identity-based harm when used in critical social contexts. Prior work has laid a solid foundation for assessing bias in LLMs by evaluating demographic disparities in different language reasoning tasks. In this work, we extend single-axis fairness evaluations to examine intersectional bias, recognizing that when multiple axes of discrimination intersect, they create distinct patterns of disadvantage. We create a new benchmark called WinoIdentity by augmenting the WinoBias dataset with 25 demographic markers across 10 attributes, including age, nationality, and race, intersected with binary gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns. Focusing on harms of omission due to underrepresentation, we investigate bias through the lens of uncertainty and propose a group (un)fairness metric called Coreference Confidence Disparity which measures whether models are more or less confident for some intersectional identities than others. We evaluate five recently published LLMs and find confidence disparities as high as 40% along various demographic attributes including body type, sexual orientation and socio-economic status, with models being most uncertain about doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly, coreference confidence decreases even for hegemonic or privileged markers, indicating that the recent impressive performance of LLMs is more likely due to memorization than logical reasoning. Notably, these are two independent failures in value alignment and validity that can compound to cause social harm.

