---
layout: default
title: Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation
---

# Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12040" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12040v1</a>
  <a href="https://arxiv.org/pdf/2508.12040.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12040v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12040v1', 'Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinyi Han, Tingyun Li, Shisong Chen, Jie Shi, Xinyi Wang, Guanglei Yue, Jiaqing Liang, Xin Lin, Liqian Wen, Zulong Chen, Yanghua Xiao

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-16

**å¤‡æ³¨**: The initial versin was made in August 2024

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFineCEä»¥è§£å†³LLMç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç½®ä¿¡åº¦ä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç½®ä¿¡åº¦ä¼°è®¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬ç”Ÿæˆ` `ç›‘ç£å­¦ä¹ ` `æ¦‚ç‡åˆ†å¸ƒ` `åŠ¨æ€è°ƒæ•´` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç½®ä¿¡åº¦ä¼°è®¡æ–¹æ³•å­˜åœ¨ç²—ç²’åº¦è¯„åˆ†æœºåˆ¶ï¼Œæ— æ³•æä¾›ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç»†ç²’åº¦ç½®ä¿¡åº¦ä¼°è®¡ï¼Œå¯¼è‡´LLMè¾“å‡ºçš„å¯ä¿¡åº¦ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºFineCEï¼Œé€šè¿‡æ„å»ºè®­ç»ƒæ•°æ®ç®¡é“å’Œç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œå®ç°å¯¹ä»»æ„æ–‡æœ¬åºåˆ—çš„å‡†ç¡®ç½®ä¿¡åº¦è¯„åˆ†ï¼Œå¹¶å¼•å…¥BCIç­–ç•¥æå‡ä¼°è®¡æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFineCEåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šä¼ ç»Ÿç½®ä¿¡åº¦ä¼°è®¡æ–¹æ³•ï¼Œæå‡äº†LLMç”Ÿæˆå†…å®¹çš„å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ç¼ºä¹è‡ªæˆ‘æ„è¯†ï¼Œå¸¸å¸¸å¯¹é”™è¯¯é¢„æµ‹è¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡ã€‚å› æ­¤ï¼Œå‡†ç¡®çš„ç½®ä¿¡åº¦ä¼°è®¡å¯¹äºæé«˜LLMç”Ÿæˆè¾“å‡ºçš„å¯ä¿¡åº¦è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨ç²—ç²’åº¦è¯„åˆ†æœºåˆ¶ï¼Œæ— æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æä¾›ç»†ç²’åº¦ã€è¿ç»­çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FineCEï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç½®ä¿¡åº¦ä¼°è®¡æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­æä¾›å‡†ç¡®çš„ç»†ç²’åº¦ç½®ä¿¡åº¦è¯„åˆ†ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€æ¡å…¨é¢çš„æ•°æ®æ„å»ºç®¡é“ï¼Œä»¥æœ‰æ•ˆæ•æ‰LLMå“åº”çš„æ½œåœ¨æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶è®­ç»ƒæ¨¡å‹ä»¥ç›‘ç£æ–¹å¼é¢„æµ‹ä»»æ„æ–‡æœ¬åºåˆ—çš„ç½®ä¿¡åº¦è¯„åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‘åç½®ä¿¡åº¦é›†æˆï¼ˆBCIï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨åç»­æ–‡æœ¬çš„ä¿¡æ¯æ¥å¢å¼ºå½“å‰åºåˆ—çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFineCEåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„ç»å…¸ç½®ä¿¡åº¦ä¼°è®¡æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­ç½®ä¿¡åº¦ä¼°è®¡ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå…¶ç²—ç²’åº¦è¯„åˆ†æœºåˆ¶ï¼Œæ— æ³•æä¾›ç»†ç²’åº¦çš„ç½®ä¿¡åº¦ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹å¯¹é”™è¯¯é¢„æµ‹çš„è¿‡åº¦è‡ªä¿¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯æå‡ºFineCEæ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæœ‰æ•ˆçš„è®­ç»ƒæ•°æ®å’Œå¼•å…¥BCIç­–ç•¥ï¼Œæä¾›è¿ç»­çš„ã€ç»†ç²’åº¦çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è‡ªæˆ‘è¯„ä¼°èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFineCEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ„å»ºç®¡é“ã€ç½®ä¿¡åº¦é¢„æµ‹æ¨¡å‹å’ŒBCIç­–ç•¥ã€‚æ•°æ®æ„å»ºç®¡é“ç”¨äºæ•æ‰LLMå“åº”çš„æ¦‚ç‡åˆ†å¸ƒï¼Œç½®ä¿¡åº¦é¢„æµ‹æ¨¡å‹åˆ™é€šè¿‡ç›‘ç£å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼ŒBCIç­–ç•¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨åç»­æ–‡æœ¬ä¿¡æ¯å¢å¼ºå½“å‰åºåˆ—çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šFineCEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»†ç²’åº¦ç½®ä¿¡åº¦ä¼°è®¡èƒ½åŠ›å’ŒBCIç­–ç•¥çš„å¼•å…¥ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFineCEèƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´ç½®ä¿¡åº¦è¯„åˆ†ï¼Œæ˜¾è‘—æå‡äº†ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨FineCEä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬è®­ç»ƒæ•°æ®çš„æ„å»ºç­–ç•¥ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠæ¨¡å‹æ¶æ„çš„ä¼˜åŒ–ã€‚å…·ä½“ç»†èŠ‚åŒ…æ‹¬ä½¿ç”¨å¤šæ ·åŒ–çš„æ–‡æœ¬åºåˆ—è¿›è¡Œè®­ç»ƒï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç”Ÿæˆåœºæ™¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFineCEåœ¨ç½®ä¿¡åº¦ä¼°è®¡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºç½®ä¿¡åº¦è¯„åˆ†çš„å‡†ç¡®æ€§æé«˜äº†çº¦15%-20%ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒç”Ÿæˆä»»åŠ¡ä¸­å‡å±•ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§å’Œç¨³å®šæ€§ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜LLMç”Ÿæˆå†…å®¹çš„ç½®ä¿¡åº¦ä¼°è®¡ï¼ŒFineCEèƒ½å¤Ÿå¢å¼ºç”¨æˆ·å¯¹æ¨¡å‹è¾“å‡ºçš„ä¿¡ä»»ï¼Œæ¨åŠ¨æ›´å¹¿æ³›çš„å®é™…åº”ç”¨ã€‚æ­¤å¤–ï¼Œæœªæ¥å¯èƒ½åœ¨å…¶ä»–ç”Ÿæˆæ¨¡å‹ä¸­æ¨å¹¿æ­¤æ–¹æ³•ï¼Œæå‡å…¶å¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While large language models (LLMs) have demonstrated remarkable performance across diverse tasks, they fundamentally lack self-awareness and frequently exhibit overconfidence, assigning high confidence scores to incorrect predictions. Accurate confidence estimation is therefore critical for enhancing the trustworthiness and reliability of LLM-generated outputs. However, existing approaches suffer from coarse-grained scoring mechanisms that fail to provide fine-grained, continuous confidence estimates throughout the generation process. To address these limitations, we introduce FineCE, a novel confidence estimation method that delivers accurate, fine-grained confidence scores during text generation. Specifically, we first develop a comprehensive pipeline for constructing training data that effectively captures the underlying probabilistic distribution of LLM responses, and then train a model to predict confidence scores for arbitrary text sequences in a supervised manner. Furthermore, we propose a Backward Confidence Integration (BCI) strategy that leverages information from the subsequent text to enhance confidence estimation for the current sequence during inference. We also introduce three strategies for identifying optimal positions to perform confidence estimation within the generation process. Extensive experiments on multiple benchmark datasets demonstrate that FineCE consistently outperforms existing classical confidence estimation methods. Our code and all baselines used in the paper are available on GitHub.

