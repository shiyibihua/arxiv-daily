---
layout: default
title: Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases
---

# Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12031" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12031v1</a>
  <a href="https://arxiv.org/pdf/2508.12031.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12031v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12031v1', 'Learning Wisdom from Errors: Promoting LLM\'s Continual Relation Learning through Exploiting Error Cases')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaozhe Yin, Jinyu Guo, Kai Shuang, Xia Liu, Ruize Ou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæŒ‡ä»¤çš„å¯¹æ¯”è°ƒä¼˜æ–¹æ³•ä»¥è§£å†³æŒç»­å…³ç³»å­¦ä¹ ä¸­çš„é”™è¯¯æ¡ˆä¾‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒç»­å…³ç³»æå–` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `æŒ‡ä»¤è°ƒä¼˜` `è®¤çŸ¥åå·®çº æ­£` `é”™è¯¯æ¡ˆä¾‹åˆ©ç”¨` `åŒä»»åŠ¡å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æŒç»­å…³ç³»æå–æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨é”™è¯¯æ¡ˆä¾‹ï¼Œå¯¼è‡´æ¨¡å‹è®¤çŸ¥åå·®æœªå¾—åˆ°å……åˆ†çº æ­£ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæŒ‡ä»¤çš„å¯¹æ¯”è°ƒä¼˜æ–¹æ³•ï¼Œé€šè¿‡åŒä»»åŠ¡å¾®è°ƒåŒºåˆ†è®­ç»ƒå’Œè®°å¿†æ•°æ®ï¼Œæå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨TACREDå’ŒFewRelæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æŒç»­å…³ç³»æå–ï¼ˆCREï¼‰æ—¨åœ¨ä¸æ–­å­¦ä¹ æ–°å‡ºç°çš„å…³ç³»ï¼ŒåŒæ—¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚ç°æœ‰CREæ–¹æ³•ä¸»è¦ä¾èµ–è®°å¿†é‡æ”¾å’Œå¯¹æ¯”å­¦ä¹ æ¥å‡è½»ç¾éš¾æ€§é—å¿˜ï¼Œä½†æœªèƒ½é‡è§†èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºæ¨¡å‹è®¤çŸ¥åå·®çš„é”™è¯¯æ¡ˆä¾‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæŒ‡ä»¤çš„æŒç»­å¯¹æ¯”è°ƒä¼˜æ–¹æ³•ï¼Œé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨CREä¸­çš„åº”ç”¨ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•å°†æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒå’Œè®°å¿†æ•°æ®æ ¹æ®åˆå§‹å“åº”çš„æ­£ç¡®æ€§åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œå¹¶é€šè¿‡åŒä»»åŠ¡å¾®è°ƒè¿›è¡Œå·®å¼‚åŒ–å¤„ç†ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæŒ‡ä»¤çš„å¯¹æ¯”è°ƒä¼˜ç­–ç•¥ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æŒç»­çº æ­£å½“å‰çš„è®¤çŸ¥åå·®ï¼Œä»è€Œæ›´é€‚åˆLLMsåœ°ç¼©å°æ—§å…³ç³»ä¸æ–°å…³ç³»ä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨TACREDå’ŒFewRelæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›CREæ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ•ˆæœï¼Œè¯æ˜äº†ä¸“æ³¨äºåˆ©ç”¨é”™è¯¯æ¡ˆä¾‹çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æŒç»­å…³ç³»æå–ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨é”™è¯¯æ¡ˆä¾‹æ¥æ­ç¤ºæ¨¡å‹çš„è®¤çŸ¥åå·®ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŸºäºæŒ‡ä»¤çš„æŒç»­å¯¹æ¯”è°ƒä¼˜æ–¹æ³•ï¼Œé€šè¿‡å°†è®­ç»ƒå’Œè®°å¿†æ•°æ®åˆ†å¼€å¤„ç†ï¼Œåˆ©ç”¨æŒ‡ä»¤è°ƒä¼˜çš„æ–¹å¼æ¥æŒç»­çº æ­£æ¨¡å‹çš„è®¤çŸ¥åå·®ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®åˆ†å‰²æ¨¡å—ã€åŒä»»åŠ¡å¾®è°ƒæ¨¡å—å’ŒæŒ‡ä»¤å¯¹æ¯”è°ƒä¼˜æ¨¡å—ã€‚æ•°æ®åˆ†å‰²æ¨¡å—æ ¹æ®åˆå§‹å“åº”çš„æ­£ç¡®æ€§å°†æ•°æ®åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼ŒåŒä»»åŠ¡å¾®è°ƒæ¨¡å—åˆ†åˆ«å¤„ç†è¿™ä¸¤éƒ¨åˆ†æ•°æ®ï¼ŒæŒ‡ä»¤å¯¹æ¯”è°ƒä¼˜æ¨¡å—åˆ™åˆ©ç”¨å†å²æ•°æ®æŒ‡å¯¼å½“å‰å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†é”™è¯¯æ¡ˆä¾‹çº³å…¥å­¦ä¹ è¿‡ç¨‹ï¼Œé€šè¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ–¹å¼ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°çº æ­£è®¤çŸ¥åå·®ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ç»Ÿä¸€å¤„ç†æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ä¸åŒä»»åŠ¡çš„å­¦ä¹ æ•ˆæœï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™å¼•å…¥äº†æŒ‡ä»¤åµŒå…¥å±‚ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹æŒ‡ä»¤çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨TACREDå’ŒFewRelæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›CREæ€§èƒ½ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†åˆ©ç”¨é”™è¯¯æ¡ˆä¾‹çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³ç³»æå–ã€çŸ¥è¯†å›¾è°±æ„å»ºä»¥åŠæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹æ–°å…³ç³»çš„å­¦ä¹ èƒ½åŠ›å’Œå¯¹é”™è¯¯æ¡ˆä¾‹çš„å¤„ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›åº”ç”¨çš„æ™ºèƒ½åŒ–æ°´å¹³å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Continual Relation Extraction (CRE) aims to continually learn new emerging relations while avoiding catastrophic forgetting. Existing CRE methods mainly use memory replay and contrastive learning to mitigate catastrophic forgetting. However, these methods do not attach importance to the error cases that can reveal the model's cognitive biases more effectively. To address this issue, we propose an instruction-based continual contrastive tuning approach for Large Language Models (LLMs) in CRE. Different from existing CRE methods that typically handle the training and memory data in a unified manner, this approach splits the training and memory data of each task into two parts respectively based on the correctness of the initial responses and treats them differently through dual-task fine-tuning. In addition, leveraging the advantages of LLM's instruction-following ability, we propose a novel instruction-based contrastive tuning strategy for LLM to continuously correct current cognitive biases with the guidance of previous data in an instruction-tuning manner, which mitigates the gap between old and new relations in a more suitable way for LLMs. We experimentally evaluate our model on TACRED and FewRel, and the results show that our model achieves new state-of-the-art CRE performance with significant improvements, demonstrating the importance of specializing in exploiting error cases.

