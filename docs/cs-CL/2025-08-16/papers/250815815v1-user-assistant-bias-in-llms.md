---
layout: default
title: User-Assistant Bias in LLMs
---

# User-Assistant Bias in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15815" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15815v1</a>
  <a href="https://arxiv.org/pdf/2508.15815.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15815v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15815v1', 'User-Assistant Bias in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xu Pan, Jingxuan Fan, Zidi Xiong, Ely Hahami, Jorin Overwiening, Ziqian Xie

**åˆ†ç±»**: cs.CL, cs.AI, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç”¨æˆ·åŠ©æ‰‹åè§æ¨¡å‹ä»¥ä¼˜åŒ–å¤šè½®å¯¹è¯è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”¨æˆ·åŠ©æ‰‹åè§` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šè½®å¯¹è¯` `æ•°æ®é›†æ„å»º` `åå¥½ä¼˜åŒ–` `å¾®è°ƒå®éªŒ` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­å­˜åœ¨ç”¨æˆ·åŠ©æ‰‹åè§ï¼Œå¯¼è‡´æ¨¡å‹è¡¨ç°ä¸ä¸€è‡´ï¼Œå½±å“ç”¨æˆ·ä½“éªŒã€‚
2. è®ºæ–‡æå‡ºäº†UserAssistæ•°æ®é›†ï¼Œé€šè¿‡åŸºå‡†æµ‹è¯•å’Œå¾®è°ƒå®éªŒæ¥ç†è§£å’Œæ“æ§ç”¨æˆ·åŠ©æ‰‹åè§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå•†ä¸šæ¨¡å‹å’Œå¼€æºæ¨¡å‹åœ¨ç”¨æˆ·åè§ä¸Šè¡¨ç°ä¸åŒï¼Œä¸”é€šè¿‡DPOæ–¹æ³•å¯æœ‰æ•ˆè°ƒæ•´åè§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®å¯¹è¯ä¸­å¯èƒ½ä¼šåå‘äºä¾èµ–è‡ªèº«æˆ–ç”¨æˆ·çš„èŠå¤©è®°å½•ä¿¡æ¯ï¼Œå¯¼è‡´è¿‡äºå›ºæ‰§æˆ–è¿‡äºé¡ºä»çš„è¡Œä¸ºã€‚æœ¬æ–‡å°†è¿™ä¸€ç‰¹å¾å½¢å¼åŒ–ä¸ºç”¨æˆ·åŠ©æ‰‹åè§ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåŒ…å«8000ä¸ªå¤šè½®å¯¹è¯çš„æ•°æ®é›†UserAssistï¼Œç”¨äºåŸºå‡†æµ‹è¯•ã€ç†è§£å’Œæ“æ§å‰æ²¿LLMsä¸­çš„ç”¨æˆ·åŠ©æ‰‹åè§ã€‚é€šè¿‡UserAssist-testï¼Œæˆ‘ä»¬å¯¹26ä¸ªå•†ä¸šæ¨¡å‹å’Œ26ä¸ªå¼€æºæ¨¡å‹çš„ç”¨æˆ·åŠ©æ‰‹åè§è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å•†ä¸šæ¨¡å‹è¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„ç”¨æˆ·åè§ã€‚å¼€æºæ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹å­˜åœ¨æ˜¾è‘—çš„ç”¨æˆ·åè§ï¼Œè€Œæ¨ç†æ¨¡å‹çš„ç”¨æˆ·åè§è¾ƒå¼±ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å—æ§å¾®è°ƒå®éªŒï¼Œå‘ç°äººç±»åå¥½å¯¹é½ä¼šå¢åŠ ç”¨æˆ·åè§ï¼Œè€ŒåŸºäºæ€ç»´é“¾çš„è®­ç»ƒåˆ™ä¼šå‡å°‘åè§ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯ä»¥åŒå‘è°ƒæ•´ç”¨æˆ·åŠ©æ‰‹åè§ï¼Œå¹¶ä¸”åœ¨é¢†åŸŸå†…å¤–çš„å¯¹è¯ä¸­å‡è¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬çš„ç»“æœä¸ºLLMå¦‚ä½•æ•´åˆä¸åŒæ¥æºçš„ä¿¡æ¯æä¾›äº†æ´è§ï¼Œå¹¶ä¸ºæ£€æµ‹å’Œæ§åˆ¶æ¨¡å‹å¼‚å¸¸æä¾›äº†å¯è¡Œçš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­è¡¨ç°å‡ºçš„ç”¨æˆ·åŠ©æ‰‹åè§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œè°ƒæ•´è¿™ç§åè§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¯¹è¯ä¸­è¡¨ç°å‡ºå›ºæ‰§æˆ–è¿‡äºé¡ºä»çš„è¡Œä¸ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥UserAssistæ•°æ®é›†ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°å’Œæ“æ§ç”¨æˆ·åŠ©æ‰‹åè§ï¼Œåˆ©ç”¨å¾®è°ƒå’Œåå¥½ä¼˜åŒ–æŠ€æœ¯æ¥æ”¹å–„æ¨¡å‹çš„å¯¹è¯è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€åŸºå‡†æµ‹è¯•ã€å¾®è°ƒå®éªŒå’Œåå¥½ä¼˜åŒ–å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡UserAssistæ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç„¶åè¿›è¡Œå—æ§å¾®è°ƒï¼Œæœ€ååº”ç”¨DPOæŠ€æœ¯è¿›è¡Œåè§è°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°å®šä¹‰å’Œé‡åŒ–ç”¨æˆ·åŠ©æ‰‹åè§ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯ä¸åŒè®­ç»ƒç­–ç•¥å¯¹åè§çš„å½±å“ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§†è§’æ¥ç†è§£LLMsçš„è¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†äººç±»åå¥½å¯¹é½å’Œæ€ç»´é“¾è®­ç»ƒä½œä¸ºå…³é”®è®¾è®¡ï¼Œå‰è€…å¢åŠ äº†ç”¨æˆ·åè§ï¼Œè€Œåè€…åˆ™æœ‰æ•ˆå‡å°‘äº†åè§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå•†ä¸šæ¨¡å‹åœ¨ç”¨æˆ·åè§ä¸Šè¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„åå·®ï¼Œè€Œå¼€æºæ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹åˆ™æ˜¾è‘—å­˜åœ¨ç”¨æˆ·åè§ã€‚é€šè¿‡DPOæ–¹æ³•ï¼Œç”¨æˆ·åŠ©æ‰‹åè§å¯ä»¥æœ‰æ•ˆåœ°è¿›è¡ŒåŒå‘è°ƒæ•´ï¼Œä¸”åœ¨ä¸åŒå¯¹è¯åœºæ™¯ä¸­å‡è¡¨ç°è‰¯å¥½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹å’Œæ•™è‚²é¢†åŸŸç­‰ï¼Œèƒ½å¤Ÿæå‡ç”¨æˆ·ä¸æ¨¡å‹çš„äº¤äº’ä½“éªŒï¼Œå‡å°‘æ¨¡å‹åè§å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚æœªæ¥ï¼Œç ”ç©¶æˆæœå¯ä¸ºæ›´æ™ºèƒ½çš„å¯¹è¯ç³»ç»Ÿè®¾è®¡æä¾›ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) can bias towards relying on their own or the user's information in chat history, leading to overly stubborn or agreeable behaviors in multi-turn conversations. In this paper, we formalize this model characteristic as user-assistant bias and introduce an 8k multi-turn conversation dataset $\textbf{UserAssist}$, which we use to benchmark, understand and manipulate the user-assistant bias in frontier LLMs. Leveraging $\textbf{UserAssist-test}$, we first benchmark the user-assistant bias of 26 commercial and 26 open-weight models. Commercial models show various levels of user bias. Evaluation on open-weight models reveals significant user bias in the instruction-tuned models, and weak user bias in reasoning (or reasoning-distilled) models. We then perform controlled fine-tuning experiments to pinpoint the post-training recipe contributing to these bias shifts: human preference alignment increases user bias, while training on chain-of-thought reasoning traces decreases it. Finally, we demonstrate that user-assistant bias can be bidirectionally adjusted by performing direct preference optimization (DPO) on $\textbf{UserAssist-train}$, and generalizes well to both in-domain and out-of-domain conversations. Our results provide insights into how the LLM integrates information from different sources, and also a viable way to detect and control model abnormalities.

