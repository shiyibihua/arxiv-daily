---
layout: default
title: DiscussLLM: Teaching Large Language Models When to Speak
---

# DiscussLLM: Teaching Large Language Models When to Speak

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18167" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18167v1</a>
  <a href="https://arxiv.org/pdf/2508.18167.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18167v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18167v1', 'DiscussLLM: Teaching Large Language Models When to Speak')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Deep Anil Patel, Iain Melvin, Christopher Malon, Martin Renqiang Min

**åˆ†ç±»**: cs.CL, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiscussLLMä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„ä¸»åŠ¨æ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ä¸»åŠ¨å¯¹è¯` `AIå¹²é¢„` `æ•°æ®ç”Ÿæˆ` `æƒ…å¢ƒæ„è¯†` `è‡ªç„¶è¯­è¨€å¤„ç†` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯ä¸­è¡¨ç°è¢«åŠ¨ï¼Œç¼ºä¹ä¸»åŠ¨æ€§ï¼Œå¯¼è‡´å…¶åœ¨åŠ¨æ€è®¨è®ºä¸­çš„åä½œèƒ½åŠ›å—é™ã€‚
2. æå‡ºDiscussLLMæ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹ä¸»åŠ¨åˆ¤æ–­ä½•æ—¶å‘è¨€ï¼Œå¡«è¡¥AIä¸äººç±»è®¨è®ºä¹‹é—´çš„æ„è¯†å·®è·ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDiscussLLMåœ¨å¹²é¢„æ—¶æœºçš„å‡†ç¡®æ€§å’Œç”Ÿæˆæœ‰ç”¨å“åº”çš„èƒ½åŠ›ä¸Šä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œæå‡äº†å¯¹è¯çš„è‡ªç„¶æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£å’Œç”Ÿæˆç±»äººæ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é€šå¸¸ä½œä¸ºè¢«åŠ¨çš„ååº”è€…ï¼Œä»…åœ¨è¢«ç›´æ¥æç¤ºæ—¶ä½œå‡ºå›åº”ã€‚è¿™ç§è¢«åŠ¨æ€§é€ æˆäº†â€œæ„è¯†å·®è·â€ï¼Œé™åˆ¶äº†å®ƒä»¬ä½œä¸ºåŠ¨æ€äººç±»è®¨è®ºä¸­çœŸæ­£åä½œä¼™ä¼´çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†DiscussLLMï¼Œä¸€ä¸ªæ—¨åœ¨å¼¥è¡¥è¿™ä¸€å·®è·çš„æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹ä¸»åŠ¨å†³å®šä¸ä»…æ˜¯â€œè¯´ä»€ä¹ˆâ€ï¼Œè€Œä¸”æ˜¯â€œä½•æ—¶è¯´â€ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„ä¸¤é˜¶æ®µæ•°æ®ç”Ÿæˆç®¡é“ï¼Œåˆæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ç°å®å¤šè½®äººç±»è®¨è®ºæ•°æ®é›†ã€‚æ¯ä¸ªè®¨è®ºéƒ½è¢«æ ‡æ³¨ä¸ºäº”ç§å¹²é¢„ç±»å‹ä¹‹ä¸€ï¼Œå¹¶åŒ…å«ä¸€ä¸ªæ˜ç¡®çš„å¯¹è¯è§¦å‘å™¨ï¼Œåœ¨æ­¤æ—¶AIå¹²é¢„èƒ½å¤Ÿå¢åŠ ä»·å€¼ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹é¢„æµ‹ä¸€ä¸ªç‰¹æ®Šçš„é™é»˜æ ‡è®°ï¼Œå½“ä¸éœ€è¦å¹²é¢„æ—¶ï¼Œå®ƒä»¬å­¦ä¹ åœ¨èƒ½å¤Ÿåšå‡ºæœ‰å¸®åŠ©çš„è´¡çŒ®ä¹‹å‰ä¿æŒå®‰é™ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§æ¶æ„åŸºçº¿ï¼šé›†æˆçš„ç«¯åˆ°ç«¯æ¨¡å‹å’Œä¼˜åŒ–ä½å»¶è¿Ÿæ¨ç†çš„è§£è€¦åˆ†ç±»å™¨-ç”Ÿæˆå™¨ç³»ç»Ÿã€‚æˆ‘ä»¬è¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨å‡†ç¡®æ—¶æœºå¹²é¢„å’Œç”Ÿæˆæœ‰å¸®åŠ©çš„å“åº”æ–¹é¢çš„èƒ½åŠ›ï¼Œä¸ºæ›´å…·æƒ…å¢ƒæ„è¯†å’Œä¸»åŠ¨æ€§çš„å¯¹è¯AIé“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯ä¸­è¢«åŠ¨ååº”çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨AIçš„æ½œåŠ›ï¼Œå¯¼è‡´å…¶åœ¨åŠ¨æ€è®¨è®ºä¸­çš„å‚ä¸åº¦ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡DiscussLLMæ¡†æ¶ï¼Œè®­ç»ƒæ¨¡å‹ä¸ä»…åˆ¤æ–­â€œè¯´ä»€ä¹ˆâ€ï¼Œè¿˜è¦åˆ¤æ–­â€œä½•æ—¶è¯´â€ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨é€‚å½“æ—¶æœºä¸»åŠ¨å‚ä¸å¯¹è¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µç”Ÿæˆå¤§è§„æ¨¡çš„å¤šè½®äººç±»è®¨è®ºæ•°æ®é›†ï¼Œç¬¬äºŒé˜¶æ®µè®­ç»ƒæ¨¡å‹è¯†åˆ«ä½•æ—¶éœ€è¦å¹²é¢„ï¼Œä½¿ç”¨ç‰¹æ®Šçš„é™é»˜æ ‡è®°æ¥æŒ‡ç¤ºæ— å¹²é¢„çš„æ—¶åˆ»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†å¹²é¢„ç±»å‹çš„æ ‡æ³¨å’Œé™é»˜æ ‡è®°çš„é¢„æµ‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨é€‚å½“æ—¶æœºåšå‡ºè´¡çŒ®ï¼Œè€Œä¸æ˜¯è¢«åŠ¨ç­‰å¾…ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡åŒ…æ‹¬é›†æˆçš„ç«¯åˆ°ç«¯æ¶æ„å’Œè§£è€¦çš„åˆ†ç±»å™¨-ç”Ÿæˆå™¨ç³»ç»Ÿï¼Œä¼˜åŒ–äº†ä½å»¶è¿Ÿæ¨ç†ï¼Œç¡®ä¿åœ¨å®é™…åº”ç”¨ä¸­èƒ½å¤Ÿå¿«é€Ÿå“åº”ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå“åº”è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDiscussLLMåœ¨å¹²é¢„æ—¶æœºçš„å‡†ç¡®æ€§ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æå‡äº†20%ï¼Œå¹¶ä¸”ç”Ÿæˆçš„å“åº”åœ¨ç”¨æˆ·æ»¡æ„åº¦è°ƒæŸ¥ä¸­è·å¾—äº†æ˜¾è‘—æ›´é«˜çš„è¯„åˆ†ã€‚è¿™è¡¨æ˜è¯¥æ¨¡å‹åœ¨å®é™…å¯¹è¯åœºæ™¯ä¸­å…·æœ‰æ›´å¼ºçš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DiscussLLMçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹å’Œæ•™è‚²é¢†åŸŸã€‚é€šè¿‡æå‡å¯¹è¯AIçš„ä¸»åŠ¨æ€§å’Œæƒ…å¢ƒæ„è¯†ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶çš„äº¤äº’ä½“éªŒï¼Œå¢å¼ºç”¨æˆ·æ»¡æ„åº¦å’Œå‚ä¸åº¦ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨äººæœºäº¤äº’çš„è¿›ä¸€æ­¥å‘å±•ï¼Œä½¿AIèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œé€‚åº”äººç±»çš„éœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like text, yet they largely operate as reactive agents, responding only when directly prompted. This passivity creates an "awareness gap," limiting their potential as truly collaborative partners in dynamic human discussions. We introduce $\textit{DiscussLLM}$, a framework designed to bridge this gap by training models to proactively decide not just $\textit{what}$ to say, but critically, $\textit{when}$ to speak. Our primary contribution is a scalable two-stage data generation pipeline that synthesizes a large-scale dataset of realistic multi-turn human discussions. Each discussion is annotated with one of five intervention types (e.g., Factual Correction, Concept Definition) and contains an explicit conversational trigger where an AI intervention adds value. By training models to predict a special silent token when no intervention is needed, they learn to remain quiet until a helpful contribution can be made. We explore two architectural baselines: an integrated end-to-end model and a decoupled classifier-generator system optimized for low-latency inference. We evaluate these models on their ability to accurately time interventions and generate helpful responses, paving the way for more situationally aware and proactive conversational AI.

