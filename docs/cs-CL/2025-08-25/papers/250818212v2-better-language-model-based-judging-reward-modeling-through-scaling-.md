---
layout: default
title: Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries
---

# Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18212" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18212v2</a>
  <a href="https://arxiv.org/pdf/2508.18212.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18212v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18212v2', 'Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Meiling Ning, Zhongbao Zhang, Junda Ye, Jiabao Guo, Qingyuan Guan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25 (æ›´æ–°: 2025-11-15)

**å¤‡æ³¨**: After further internal discussion, our author team has decided to withdraw this submission due to the need for several important refinements to the manuscript. All co-authors have been informed and agree with this decision

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºESFP-RMæ¨¡å‹ä»¥æå‡è¯­è¨€æ¨¡å‹å¥–åŠ±å»ºæ¨¡æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `å¥–åŠ±å»ºæ¨¡` `è‡ªç„¶è¯­è¨€æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `æ¨¡å‹æ³›åŒ–` `ä¸Šä¸‹æ–‡ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸å¤Ÿç¨³å®šï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–çš„è¾“å…¥å’Œåé¦ˆã€‚
2. æœ¬æ–‡æå‡ºçš„ESFP-RMæ¨¡å‹é€šè¿‡å¼•å…¥è§£é‡Šæ€§æ§½æ¡†æ¶ï¼Œæ‰©å±•äº†æ¨¡å‹çš„ç†è§£è¾¹ç•Œï¼Œä»è€Œæå‡äº†å¥–åŠ±å»ºæ¨¡çš„æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒESFP-RMåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¸»æµè‡ªå›å½’æ¨¡å‹ï¼Œå°¤å…¶åœ¨RLHFå’ŒOODåœºæ™¯ä¸­å…·æœ‰æ›´å¥½çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€åŸºäºè¯­è¨€æ¨¡å‹çš„å¥–åŠ±å»ºæ¨¡ï¼ˆLM-based judging reward modelingï¼‰çš„å…´èµ·ï¼Œç”Ÿæˆå¥–åŠ±æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰ä¸­å±•ç°å‡ºé«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºè¿›ä¸€æ­¥æ¨åŠ¨è¿™ä¸€èŒƒå¼ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ ¸å¿ƒè§è§£ï¼šè¿™ç§å¥–åŠ±å»ºæ¨¡ä¸è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰åœ¨å½¢å¼ä¸Šå…·æœ‰åŸºæœ¬ä¸€è‡´æ€§ã€‚åŸºäºè¿™ä¸€è§†è§’ï¼Œæœ¬æ–‡æå‡ºäº†ESFP-RMï¼Œä¸€ä¸ªåˆ©ç”¨è§£é‡Šæ€§æ§½æ¡†æ¶è¿›è¡Œé¢„æµ‹çš„ä¸¤é˜¶æ®µè¯­è¨€æ¨¡å‹å¥–åŠ±æ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒESFP-RMåœ¨æ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸­ï¼Œæä¾›äº†æ¯”ç”Ÿæˆå¥–åŠ±æ¨¡å‹æ›´ç¨³å®šå’Œæ›´å…·æ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±ä¿¡å·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç”Ÿæˆå¥–åŠ±æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¸ç¨³å®šçš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤šæ ·åŒ–è¾“å…¥å’Œåé¦ˆæ—¶çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¥–åŠ±å»ºæ¨¡ä¸è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡æ‰©å±•æ¨¡å‹çš„ç†è§£è¾¹ç•Œæ¥æå‡å¥–åŠ±å»ºæ¨¡çš„æ•ˆæœã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šESFP-RMæ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µç»“æ„ï¼Œé¦–å…ˆé€šè¿‡è§£é‡Šæ€§æ§½æ¡†æ¶è¿›è¡Œé¢„æµ‹ï¼Œç„¶åç”Ÿæˆå¥–åŠ±ä¿¡å·ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ä¸Šä¸‹æ–‡è§£é‡Šç”Ÿæˆã€æ§½é¢„æµ‹å’Œå¥–åŠ±ä¿¡å·è®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†è§£é‡Šæ€§æ§½æ¡†æ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤æ‚çš„è¾“å…¥ï¼Œä»è€Œæå‡äº†å¥–åŠ±ä¿¡å·çš„è´¨é‡ã€‚è¿™ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é¢„æµ‹ç²¾åº¦ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†ä¸Šä¸‹æ–‡ä¿¡æ¯å¤„ç†æ¨¡å—ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒESFP-RMåœ¨å¤šä¸ªNLIä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¸»æµè‡ªå›å½’æ¨¡å‹ï¼Œå°¤å…¶åœ¨RLHFå’ŒOODåœºæ™¯ä¸­ï¼Œå¥–åŠ±ä¿¡å·çš„ç¨³å®šæ€§æå‡äº†çº¦20%ï¼Œæ³›åŒ–èƒ½åŠ›æ˜¾è‘—å¢å¼ºï¼Œå±•ç¤ºäº†è¯¥æ¨¡å‹çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¥–åŠ±å»ºæ¨¡çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒESFP-RMèƒ½å¤Ÿåœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸­æä¾›æ›´å¯é çš„åé¦ˆï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The emergence of LM-based judging reward modeling, represented by generative reward models, has successfully made reinforcement learning from AI feedback (RLAIF) efficient and scalable. To further advance this paradigm, we propose a core insight: this form of reward modeling shares fundamental formal consistency with natural language inference (NLI), a core task in natural language understanding. This reframed perspective points to a key path for building superior reward models: scaling the model's comprehension boundaries. Pursuing this path, exploratory experiments on NLI tasks demonstrate that the slot prediction masked language models (MLMs) incorporating contextual explanations achieve significantly better performance compared to mainstream autoregressive models. Based on this key finding, we propose ESFP-RM, a two-stage LM-based judging reward model that utilizes an explanation based slot framework for prediction to fully leverage the advantages of MLMs. Extensive experiments demonstrate that in both reinforcement learning from human feedback (RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers more stable and generalizable reward signals compared to generative reward models.

