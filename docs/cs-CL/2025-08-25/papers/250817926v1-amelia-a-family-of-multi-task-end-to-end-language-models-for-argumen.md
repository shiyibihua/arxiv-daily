---
layout: default
title: AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation
---

# AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17926" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17926v1</a>
  <a href="https://arxiv.org/pdf/2508.17926.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17926v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17926v1', 'AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Henri Savigny, Bruno Yun

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAMELIAä»¥è§£å†³å¤šä»»åŠ¡è®ºè¯æŒ–æ˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è®ºè¯æŒ–æ˜` `å¤šä»»åŠ¡å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `è¿ç§»å­¦ä¹ ` `æ¨¡å‹åˆå¹¶` `æ•°æ®é›†æ„å»º` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è®ºè¯æŒ–æ˜æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤šä¸ªç‹¬ç«‹çš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½ä¸ä¸€è‡´ã€‚
2. æœ¬æ–‡æå‡ºAMELIAï¼Œé€šè¿‡æ„å»ºç»Ÿä¸€çš„å¤šä»»åŠ¡æ•°æ®é›†å’Œæ¢ç´¢ä¸åŒçš„è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨æå‡è®ºè¯æŒ–æ˜çš„æ•ˆç‡å’Œæ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»»åŠ¡ç‰¹å®šå¾®è°ƒæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè€Œå¤šä»»åŠ¡å¾®è°ƒå’Œæ¨¡å‹åˆå¹¶åˆ™åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®ºè¯æŒ–æ˜æ˜¯è®ºè¯å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œæ—¨åœ¨ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­è‡ªåŠ¨æå–è®ºè¯ç»“æ„åŠå…¶å…³ç³»ã€‚æœ¬æ–‡ç ”ç©¶å¦‚ä½•åˆ©ç”¨å•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹æ‰§è¡Œä¸€ä¸ªæˆ–å¤šä¸ªè®ºè¯æŒ–æ˜ä»»åŠ¡ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸¤ä¸ªæ–¹é¢ï¼šé¦–å…ˆï¼Œé€šè¿‡è°ƒæŸ¥å’Œè½¬æ¢19ä¸ªå·²çŸ¥çš„è®ºè¯æŒ–æ˜æ•°æ®é›†ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šä»»åŠ¡æ•°æ®é›†ï¼›å…¶æ¬¡ï¼Œæ¢ç´¢äº†ä½¿ç”¨Meta AIçš„Llama-3.1-8B-Instructæ¨¡å‹çš„å¤šç§è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å•ä»»åŠ¡å¾®è°ƒã€å¤šä»»åŠ¡è”åˆå¾®è°ƒå’Œæ¨¡å‹åˆå¹¶ã€‚å®éªŒè¡¨æ˜ï¼Œä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒæ˜¾è‘—æé«˜äº†å„ä¸ªä»»åŠ¡çš„æ€§èƒ½ï¼Œè€Œå¤šä»»åŠ¡å¾®è°ƒåœ¨ä¿æŒå¼ºå¤§æ€§èƒ½çš„åŒæ—¶æ²¡æœ‰å‡ºç°æ€§èƒ½ä¸‹é™ï¼Œå±•ç¤ºäº†ç›¸å…³ä»»åŠ¡ä¹‹é—´æœ‰æ•ˆçš„è¿ç§»å­¦ä¹ ã€‚æœ€åï¼Œæ¨¡å‹åˆå¹¶æä¾›äº†ä¸€ç§å¯è¡Œçš„æŠ˜ä¸­æ–¹æ¡ˆï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°äº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å•ä¸€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šä»»åŠ¡è®ºè¯æŒ–æ˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ¨¡å‹æ•ˆç‡ä½ä¸‹å’Œä»»åŠ¡é—´æ€§èƒ½ä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡æ•°æ®é›†ï¼Œå¹¶æ¢ç´¢ä¸åŒçš„è®­ç»ƒç­–ç•¥ï¼Œæœ¬æ–‡å¸Œæœ›åœ¨æå‡ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šæ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ã€‚æ•°æ®é›†æ•´åˆäº†19ä¸ªç°æœ‰çš„è®ºè¯æŒ–æ˜æ•°æ®é›†ï¼Œæ¨¡å‹è®­ç»ƒåˆ™åŒ…æ‹¬å•ä»»åŠ¡å¾®è°ƒã€å¤šä»»åŠ¡è”åˆå¾®è°ƒå’Œæ¨¡å‹åˆå¹¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†æ¨¡å‹åˆå¹¶ç­–ç•¥ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¿æŒå¤šä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä¸ä¼ ç»Ÿçš„å…¨ä»»åŠ¡å¾®è°ƒæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒçš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ï¼Œä»¥é€‚åº”å„ä¸ªä»»åŠ¡çš„ç‰¹æ€§ã€‚åŒæ—¶ï¼Œæ¨¡å‹åˆå¹¶çš„å…·ä½“å®ç°ç»†èŠ‚ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨åˆå¹¶åä»èƒ½ä¿æŒè¾ƒé«˜çš„æ€§èƒ½æ°´å¹³ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä»»åŠ¡ç‰¹å®šå¾®è°ƒåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè€Œå¤šä»»åŠ¡å¾®è°ƒåœ¨ä¿æŒå¼ºå¤§æ€§èƒ½çš„åŒæ—¶æ²¡æœ‰å‡ºç°æ€§èƒ½ä¸‹é™ã€‚æ¨¡å‹åˆå¹¶ç­–ç•¥å®ç°äº†ä¸å…¨ä»»åŠ¡å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå±•ç¤ºäº†æœ‰æ•ˆçš„è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ³•å¾‹æ–‡æœ¬åˆ†æã€ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸å’Œæ•™è‚²é¢†åŸŸçš„è®ºè¯èƒ½åŠ›è¯„ä¼°ã€‚é€šè¿‡è‡ªåŠ¨åŒ–çš„è®ºè¯æŒ–æ˜ï¼Œèƒ½å¤Ÿæé«˜ä¿¡æ¯å¤„ç†çš„æ•ˆç‡ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œåˆ†æå¤æ‚çš„è®ºè¯ç»“æ„ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Argument mining is a subfield of argumentation that aims to automatically extract argumentative structures and their relations from natural language texts. This paper investigates how a single large language model can be leveraged to perform one or several argument mining tasks. Our contributions are two-fold. First, we construct a multi-task dataset by surveying and converting 19 well-known argument mining datasets from the literature into a unified format. Second, we explore various training strategies using Meta AI's Llama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2) fine-tuning jointly on multiple tasks, and (3) merging models fine-tuned separately on individual tasks. Our experiments show that task-specific fine-tuning significantly improves individual performance across all tasks. Moreover, multi-task fine-tuning maintains strong performance without degradation, suggesting effective transfer learning across related tasks. Finally, we demonstrate that model merging offers a viable compromise: it yields competitive performance while mitigating the computational costs associated with full multi-task fine-tuning.

