---
layout: default
title: From BERT to LLMs: Comparing and Understanding Chinese Classifier Prediction in Language Models
---

# From BERT to LLMs: Comparing and Understanding Chinese Classifier Prediction in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18253" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18253v2</a>
  <a href="https://arxiv.org/pdf/2508.18253.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18253v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18253v2', 'From BERT to LLMs: Comparing and Understanding Chinese Classifier Prediction in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziqi Zhang, Jianfei Ma, Emmanuele Chersoni, Jieshun You, Zhaoxin Feng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25 (æ›´æ–°: 2025-11-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒBERTä¸LLMsåœ¨ä¸­æ–‡åˆ†ç±»å™¨é¢„æµ‹ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸­æ–‡åˆ†ç±»å™¨` `å¤§å‹è¯­è¨€æ¨¡å‹` `BERT` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¾®è°ƒæŠ€æœ¯` `æ³¨æ„åŠ›æœºåˆ¶` `æ•™è‚²åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰ç ”ç©¶æœªå……åˆ†æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡åˆ†ç±»å™¨é¢„æµ‹ä¸­çš„èƒ½åŠ›ï¼Œå¯¼è‡´æ•™è‚²åº”ç”¨ä¸­çš„æ½œåœ¨é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¤šç§æ©è”½ç­–ç•¥è¯„ä¼°LLMsçš„é¢„æµ‹èƒ½åŠ›ï¼Œå¹¶æ¢ç´¢å¾®è°ƒä»¥æå‡å…¶åˆ†ç±»å™¨æ€§èƒ½ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šç ”ç©¶å‘ç°LLMsçš„è¡¨ç°ä¸å¦‚BERTï¼Œä¸”é¢„æµ‹æ•ˆæœä¾èµ–äºåç»­åè¯çš„ä¿¡æ¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆ†ç±»å™¨æ˜¯ä¸­æ–‡è¯­è¨€çš„é‡è¦ç‰¹å¾ï¼Œå…¶æ­£ç¡®é¢„æµ‹å¯¹æ•™è‚²åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡çŒ®ä¸­å¯¹æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸­æ–‡åˆ†ç±»å™¨çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›æ¢è®¨è¾ƒå°‘ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é‡‡ç”¨å¤šç§æ©è”½ç­–ç•¥è¯„ä¼°LLMsçš„å†…åœ¨èƒ½åŠ›ã€ä¸åŒå¥å­å…ƒç´ çš„è´¡çŒ®ä»¥åŠæ³¨æ„åŠ›æœºåˆ¶åœ¨é¢„æµ‹ä¸­çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢ç´¢äº†å¯¹LLMsè¿›è¡Œå¾®è°ƒä»¥æå‡åˆ†ç±»å™¨æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsçš„è¡¨ç°ä¸å¦‚BERTï¼Œå³ä½¿ç»è¿‡å¾®è°ƒï¼Œé¢„æµ‹æ•ˆæœä»ç„¶å—é™äºåç»­åè¯çš„ä¿¡æ¯ï¼Œè¿™ä¹Ÿè§£é‡Šäº†å…·æœ‰åŒå‘æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹ï¼ˆå¦‚BERTï¼‰çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸­æ–‡åˆ†ç±»å™¨é¢„æµ‹ä¸­çš„èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è¯„ä¼°LLMsåœ¨è¿™ä¸€é¢†åŸŸçš„è¡¨ç°ï¼Œå¯¼è‡´æ•™è‚²åº”ç”¨ä¸­çš„æ½œåœ¨æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¤šç§æ©è”½ç­–ç•¥ï¼Œè¯„ä¼°LLMsçš„å†…åœ¨èƒ½åŠ›åŠå…¶æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†ï¼ŒåŒæ—¶æ¢ç´¢å¾®è°ƒæ–¹æ³•ä»¥æå‡åˆ†ç±»å™¨æ€§èƒ½ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ­ç¤ºä¸åŒå¥å­å…ƒç´ å¯¹é¢„æµ‹çš„è´¡çŒ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¤šç§æ©è”½ç­–ç•¥ï¼Œåˆ†ä¸ºè¯„ä¼°LLMsçš„åŸºæœ¬èƒ½åŠ›ã€åˆ†æå¥å­å…ƒç´ çš„è´¡çŒ®å’Œå¾®è°ƒæ¨¡å‹ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ¯ä¸ªé˜¶æ®µéƒ½é€šè¿‡å®éªŒéªŒè¯æ¨¡å‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†BERTä¸LLMsåœ¨ä¸­æ–‡åˆ†ç±»å™¨é¢„æµ‹ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†åç»­åè¯ä¿¡æ¯å¯¹é¢„æµ‹çš„é‡è¦æ€§ï¼Œå¼ºè°ƒäº†åŒå‘æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æ©è”½ç­–ç•¥å’Œå¾®è°ƒæŠ€æœ¯ï¼Œå…³æ³¨ä¸åŒå¥å­å…ƒç´ çš„è´¡çŒ®ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒå’Œè¯„ä¼°é˜¶æ®µçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨ä¸­æ–‡åˆ†ç±»å™¨é¢„æµ‹ä¸­çš„è¡¨ç°ä¸å¦‚BERTï¼Œä¸”å³ä½¿ç»è¿‡å¾®è°ƒï¼Œæ€§èƒ½æå‡ä»æœ‰é™ã€‚å…·ä½“è€Œè¨€ï¼Œåç»­åè¯çš„ä¿¡æ¯å¯¹é¢„æµ‹æ•ˆæœæœ‰æ˜¾è‘—å½±å“ï¼Œå¼ºè°ƒäº†åŒå‘æ³¨æ„åŠ›æœºåˆ¶çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€è¯­è¨€å­¦ä¹ å·¥å…·å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚é€šè¿‡æå‡LLMsåœ¨ä¸­æ–‡åˆ†ç±»å™¨é¢„æµ‹ä¸­çš„è¡¨ç°ï¼Œå¯ä»¥ä¸ºæ•™è‚²åº”ç”¨æä¾›æ›´å‡†ç¡®çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä»è€Œæ”¹å–„å­¦ä¹ æ•ˆæœå’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Classifiers are an important and defining feature of the Chinese language, and their correct prediction is key to numerous educational applications. Yet, whether the most popular Large Language Models (LLMs) possess proper knowledge the Chinese classifiers is an issue that has largely remain unexplored in the Natural Language Processing (NLP) literature.
>   To address such a question, we employ various masking strategies to evaluate the LLMs' intrinsic ability, the contribution of different sentence elements, and the working of the attention mechanisms during prediction. Besides, we explore fine-tuning for LLMs to enhance the classifier performance.
>   Our findings reveal that LLMs perform worse than BERT, even with fine-tuning. The prediction, as expected, greatly benefits from the information about the following noun, which also explains the advantage of models with a bidirectional attention mechanism such as BERT.

