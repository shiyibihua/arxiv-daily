---
layout: default
title: Training Language Model Agents to Find Vulnerabilities with CTF-Dojo
---

# Training Language Model Agents to Find Vulnerabilities with CTF-Dojo

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18370" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18370v2</a>
  <a href="https://arxiv.org/pdf/2508.18370.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18370v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18370v2', 'Training Language Model Agents to Find Vulnerabilities with CTF-Dojo')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Terry Yue Zhuo, Dingmin Wang, Hantian Ding, Varun Kumar, Zijian Wang

**åˆ†ç±»**: cs.SE, cs.CL, cs.CR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25 (æ›´æ–°: 2025-09-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCTF-Dojoä»¥è§£å†³å¯æ‰©å±•æ‰§è¡Œç¯å¢ƒä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯æ‰§è¡Œç¯å¢ƒ` `è‡ªåŠ¨åŒ–ç®¡é“` `ç½‘ç»œå®‰å…¨` `æœºå™¨å­¦ä¹ ä»£ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç¼ºä¹å¯æ‰©å±•å’Œé€šç”¨çš„æ‰§è¡Œç¯å¢ƒï¼Œé™åˆ¶äº†é«˜æ€§èƒ½æœºå™¨å­¦ä¹ ä»£ç†çš„è®­ç»ƒã€‚
2. CTF-Dojoæä¾›äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¯æ‰§è¡Œè¿è¡Œæ—¶ç¯å¢ƒï¼Œç»“åˆCTFæŒ‘æˆ˜å’Œè‡ªåŠ¨åŒ–ç¯å¢ƒæ„å»ºå·¥å…·CTF-Forgeã€‚
3. åœ¨CTF-Dojoä¸Šè®­ç»ƒçš„LLMä»£ç†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯æ‰§è¡Œè¿è¡Œç¯å¢ƒä¸­è®­ç»ƒæ—¶è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­é€šè¿‡éªŒè¯åé¦ˆå¾ªç¯å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œç¼ºä¹å¯æ‰©å±•å’Œé€šç”¨çš„æ‰§è¡ŒåŸºç¡€ç¯å¢ƒé™åˆ¶äº†æ›´å¼ºå¤§æœºå™¨å­¦ä¹ ä»£ç†çš„è®­ç»ƒè¿›å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CTF-Dojoï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹LLMsè®­ç»ƒçš„å¯éªŒè¯åé¦ˆçš„å¤§è§„æ¨¡å¯æ‰§è¡Œè¿è¡Œæ—¶ï¼ŒåŒ…å«658ä¸ªåŠŸèƒ½é½å…¨çš„CTFé£æ ¼æŒ‘æˆ˜ï¼Œä¸”é€šè¿‡Dockerå®¹å™¨åŒ–ç¡®ä¿å¯é‡ç°æ€§ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†CTF-Forgeï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œèƒ½å¤Ÿåœ¨å‡ åˆ†é’Ÿå†…å°†å…¬å¼€å¯ç”¨çš„å·¥ä»¶è½¬åŒ–ä¸ºå¯ç”¨çš„æ‰§è¡Œç¯å¢ƒï¼Œæ¶ˆé™¤ä¼ ç»Ÿä¸Šéœ€è¦çš„æ•°å‘¨ä¸“å®¶é…ç½®ã€‚é€šè¿‡åœ¨CTF-Dojoä¸Šè®­ç»ƒåŸºäºLLMçš„ä»£ç†ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªç«äº‰åŸºå‡†ä¸Šå®ç°äº†é«˜è¾¾11.6%çš„ç»å¯¹æå‡ï¼Œå»ºç«‹äº†æ–°çš„å¼€æ”¾æƒé‡æœ€å…ˆè¿›æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨å­¦ä¹ ä»£ç†è®­ç»ƒä¸­ç¼ºä¹å¯æ‰©å±•å’Œé€šç”¨çš„æ‰§è¡Œç¯å¢ƒçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„ä¸“æœ‰ç³»ç»Ÿï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œå¯é‡å¤æ€§å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºCTF-Dojoä½œä¸ºä¸€ä¸ªå¤§è§„æ¨¡çš„å¯æ‰§è¡Œè¿è¡Œæ—¶ç¯å¢ƒï¼Œç»“åˆCTFé£æ ¼çš„æŒ‘æˆ˜å’Œè‡ªåŠ¨åŒ–çš„ç¯å¢ƒæ„å»ºå·¥å…·CTF-Forgeï¼Œä»¥å®ç°å¿«é€Ÿã€å¯é‡å¤çš„è®­ç»ƒè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCTF-Dojoçš„æ•´ä½“æ¶æ„åŒ…æ‹¬658ä¸ªCTFæŒ‘æˆ˜ï¼Œæ‰€æœ‰æŒ‘æˆ˜å‡é€šè¿‡Dockerå®¹å™¨åŒ–ï¼Œç¡®ä¿å¯é‡ç°æ€§ã€‚åŒæ—¶ï¼ŒCTF-Forgeè‡ªåŠ¨åŒ–ç®¡é“èƒ½å¤Ÿå°†å…¬å¼€å·¥ä»¶è½¬åŒ–ä¸ºå¯ç”¨ç¯å¢ƒï¼Œæå¤§åœ°å‡å°‘äº†é…ç½®æ—¶é—´ã€‚

**å…³é”®åˆ›æ–°**ï¼šCTF-Dojoçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æä¾›äº†å¯éªŒè¯åé¦ˆçš„æ‰§è¡Œç¯å¢ƒï¼Œå…è®¸LLMåœ¨çœŸå®çš„æ‰§è¡Œä¸Šä¸‹æ–‡ä¸­è¿›è¡Œè®­ç»ƒï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†486æ¡é«˜è´¨é‡çš„æ‰§è¡ŒéªŒè¯è½¨è¿¹ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨CTF-Dojoä¸Šè®­ç»ƒçš„32Bæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°31.9%çš„Pass@1ï¼Œè¾ƒå¼ºåŸºçº¿æå‡äº†11.6%ã€‚è¯¥æ¨¡å‹çš„æ€§èƒ½ä¸å‰æ²¿æ¨¡å‹å¦‚DeepSeek-V3-0324å’ŒGemini-2.5-Flashç›¸åª²ç¾ï¼Œå»ºç«‹äº†æ–°çš„å¼€æ”¾æƒé‡æœ€å…ˆè¿›æ°´å¹³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç½‘ç»œå®‰å…¨ã€è½¯ä»¶å·¥ç¨‹å’Œè‡ªåŠ¨åŒ–æµ‹è¯•ç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªå¯æ‰©å±•çš„è®­ç»ƒç¯å¢ƒï¼ŒCTF-Dojoèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´å¼ºå¤§çš„æœºå™¨å­¦ä¹ ä»£ç†ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ï¼Œé™ä½å¼€å‘æˆæœ¬ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.

