---
layout: default
title: ISACL: Internal State Analyzer for Copyrighted Training Data Leakage
---

# ISACL: Internal State Analyzer for Copyrighted Training Data Leakage

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17767" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17767v2</a>
  <a href="https://arxiv.org/pdf/2508.17767.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17767v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17767v2', 'ISACL: Internal State Analyzer for Copyrighted Training Data Leakage')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guangwei Zhang, Qisheng Su, Jiateng Liu, Cheng Qian, Yanzhou Pan, Yanjie Fu, Denghui Zhang

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25 (æ›´æ–°: 2025-09-13)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/changhu73/Internal_states_leakage)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºISACLä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ç‰ˆæƒæ•°æ®æ³„éœ²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç‰ˆæƒæ•°æ®æ³„éœ²` `å†…éƒ¨çŠ¶æ€åˆ†æ` `ä¸»åŠ¨é˜²æŠ¤` `æ•°æ®éšç§` `ä¼¦ç†æ ‡å‡†` `ç¥ç»ç½‘ç»œåˆ†ç±»å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å†…å®¹ç”Ÿæˆåæ‰å¤„ç†ç‰ˆæƒæ•°æ®æ³„éœ²ï¼Œå¯¼è‡´æ•æ„Ÿä¿¡æ¯å¯èƒ½è¢«æš´éœ²ï¼Œç¼ºä¹ä¸»åŠ¨é˜²æŠ¤æœºåˆ¶ã€‚
2. æœ¬ç ”ç©¶æå‡ºISACLï¼Œé€šè¿‡åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨çŠ¶æ€ï¼Œæå‰è¯†åˆ«æ½œåœ¨çš„ç‰ˆæƒæ•°æ®æ³„éœ²é£é™©ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒISACLæœ‰æ•ˆé™ä½äº†ç‰ˆæƒæ•°æ®æ³„éœ²é£é™©ï¼Œç¡®ä¿äº†ç”Ÿæˆæ–‡æœ¬çš„åˆè§„æ€§å’Œé«˜è´¨é‡ã€‚
4. method_zh

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå¼•å‘äº†é©å‘½ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ„å¤–æ³„éœ²ç‰ˆæƒæˆ–ä¸“æœ‰æ•°æ®çš„é£é™©ï¼Œå°¤å…¶æ˜¯åœ¨è¿™äº›æ•°æ®ç”¨äºè®­ç»ƒä½†å¹¶ä¸æ‰“ç®—åˆ†å‘æ—¶ã€‚ä¼ ç»Ÿæ–¹æ³•ä»…åœ¨å†…å®¹ç”Ÿæˆåå¤„ç†è¿™äº›æ³„éœ²ï¼Œå¯èƒ½å¯¼è‡´æ•æ„Ÿä¿¡æ¯çš„æš´éœ²ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸»åŠ¨çš„æ–¹æ³•ï¼šåœ¨æ–‡æœ¬ç”Ÿæˆä¹‹å‰æ£€æŸ¥LLMsçš„å†…éƒ¨çŠ¶æ€ä»¥æ£€æµ‹æ½œåœ¨æ³„éœ²ã€‚é€šè¿‡ä½¿ç”¨ç»è¿‡ç­–åˆ’çš„ç‰ˆæƒææ–™æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç¥ç»ç½‘ç»œåˆ†ç±»å™¨æ¥è¯†åˆ«é£é™©ï¼Œä»è€Œå…è®¸é€šè¿‡åœæ­¢ç”Ÿæˆè¿‡ç¨‹æˆ–æ”¹å˜è¾“å‡ºä»¥é˜²æ­¢æ³„éœ²è¿›è¡Œæ—©æœŸå¹²é¢„ã€‚ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿé›†æˆï¼Œè¯¥æ¡†æ¶ç¡®ä¿éµå®ˆç‰ˆæƒå’Œè®¸å¯è¦æ±‚ï¼ŒåŒæ—¶å¢å¼ºæ•°æ®éšç§å’Œä¼¦ç†æ ‡å‡†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåˆ†æå†…éƒ¨çŠ¶æ€æœ‰æ•ˆé™ä½äº†ç‰ˆæƒæ•°æ®æ³„éœ²çš„é£é™©ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œé¡ºåˆ©èå…¥AIå·¥ä½œæµç¨‹ï¼Œç¡®ä¿éµå®ˆç‰ˆæƒæ³•è§„ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆã€‚è¯¥å®ç°å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒISACLåœ¨è¯†åˆ«ç‰ˆæƒæ•°æ®æ³„éœ²æ–¹é¢çš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œé£é™©è¯†åˆ«ç‡æå‡äº†çº¦30%ï¼Œæœ‰æ•ˆé™ä½äº†æ•æ„Ÿä¿¡æ¯çš„æ³„éœ²é£é™©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†…å®¹ç”Ÿæˆã€æ™ºèƒ½å®¢æœå’Œæ•™è‚²ç­‰å¤šä¸ªé¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿æŠ¤ç‰ˆæƒæ•°æ®ï¼Œæå‡æ•°æ®éšç§å’Œä¼¦ç†æ ‡å‡†ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but pose risks of inadvertently exposing copyrighted or proprietary data, especially when such data is used for training but not intended for distribution. Traditional methods address these leaks only after content is generated, which can lead to the exposure of sensitive information. This study introduces a proactive approach: examining LLMs' internal states before text generation to detect potential leaks. By using a curated dataset of copyrighted materials, we trained a neural network classifier to identify risks, allowing for early intervention by stopping the generation process or altering outputs to prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG) system, this framework ensures adherence to copyright and licensing requirements while enhancing data privacy and ethical standards. Our results show that analyzing internal states effectively mitigates the risk of copyrighted data leakage, offering a scalable solution that fits smoothly into AI workflows, ensuring compliance with copyright regulations while maintaining high-quality text generation. The implementation is available on GitHub.\footnote{https://github.com/changhu73/Internal_states_leakage}

