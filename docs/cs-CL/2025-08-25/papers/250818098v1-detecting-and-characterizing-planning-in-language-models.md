---
layout: default
title: Detecting and Characterizing Planning in Language Models
---

# Detecting and Characterizing Planning in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18098" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18098v1</a>
  <a href="https://arxiv.org/pdf/2508.18098.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18098v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18098v1', 'Detecting and Characterizing Planning in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jatin Nainani, Sankaran Vaidyanathan, Connor Watts, Andre N. Assis, Alice Rigg

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**å¤‡æ³¨**: 9 pages, 4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå½¢å¼åŒ–æ ‡å‡†ä»¥æ£€æµ‹è¯­è¨€æ¨¡å‹ä¸­çš„è§„åˆ’è¡Œä¸º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `è§„åˆ’è¡Œä¸º` `å³å…´ç”Ÿæˆ` `ä»£ç ç”Ÿæˆ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æŒ‡ä»¤è°ƒä¼˜` `å¤šæ­¥éª¤æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å‡è®¾å›ºå®šçš„è§„åˆ’èŒƒå›´ï¼Œä¸”å¤šé›†ä¸­äºå•ä¸€æç¤ºï¼Œæœªèƒ½å…¨é¢æ¢è®¨è¯­è¨€æ¨¡å‹çš„è§„åˆ’èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºå½¢å¼åŒ–æ ‡å‡†ä»¥æ£€æµ‹è¯­è¨€æ¨¡å‹ä¸­çš„è§„åˆ’è¡Œä¸ºï¼Œå¹¶å¼€å‘åŠè‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“è¿›è¡ŒéªŒè¯ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGemma-2-2Bæ¨¡å‹åœ¨è¯—æ­Œç”Ÿæˆä»»åŠ¡ä¸­é€šè¿‡å³å…´ç”Ÿæˆè§£å†³é—®é¢˜ï¼Œå¹¶åœ¨MBPPä»»åŠ¡ä¸­è¡¨ç°å‡ºè§„åˆ’ä¸å³å…´çš„åˆ‡æ¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼ŒLLMså¯èƒ½ä¼šè¿›è¡Œè§„åˆ’ï¼Œå³æå‰é€‰æ‹©æœªæ¥ç›®æ ‡æ ‡è®°å¹¶ç”Ÿæˆä¸­é—´æ ‡è®°ä»¥å®ç°ç›®æ ‡ï¼Œè€Œä¸ä»…ä»…æ˜¯é€ä¸ªå³å…´ç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶é€šå¸¸å‡è®¾å›ºå®šçš„è§„åˆ’èŒƒå›´ï¼Œä¸”å¤šé›†ä¸­äºå•ä¸€æç¤ºæˆ–ç‹­çª„é¢†åŸŸã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†å½¢å¼åŒ–ä¸”å› æœåŸºç¡€çš„æ ‡å‡†ï¼Œä»¥åŒºåˆ†æ¨¡å‹å’Œä»»åŠ¡ä¸­çš„è§„åˆ’ä¸å³å…´ç”Ÿæˆï¼Œå¹¶å°†å…¶æ“ä½œåŒ–ä¸ºåŠè‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“ã€‚æˆ‘ä»¬å°†è¯¥ç®¡é“åº”ç”¨äºGemma-2-2Bæ¨¡å‹åœ¨MBPPä»£ç ç”ŸæˆåŸºå‡†å’Œè¯—æ­Œç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œè§„åˆ’å¹¶éæ™®éé€‚ç”¨ï¼ŒGemma-2-2Båœ¨ç›¸åŒä»»åŠ¡ä¸­é€šè¿‡å³å…´ç”Ÿæˆè§£å†³é—®é¢˜ï¼Œå¹¶åœ¨MBPPä»»åŠ¡ä¸­åœ¨è§„åˆ’ä¸å³å…´ä¹‹é—´åˆ‡æ¢ã€‚è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒæŒ‡ä»¤è°ƒä¼˜ç²¾ç‚¼äº†åŸºç¡€æ¨¡å‹ä¸­çš„è§„åˆ’è¡Œä¸ºï¼Œè€Œéä»é›¶å¼€å§‹åˆ›å»ºã€‚æ­¤ç ”ç©¶ä¸ºLLMsä¸­è§„åˆ’çš„æœºåˆ¶ç ”ç©¶æä¾›äº†å¯é‡å¤å’Œå¯æ‰©å±•çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆåŒºåˆ†è¯­è¨€æ¨¡å‹ä¸­çš„è§„åˆ’ä¸å³å…´ç”Ÿæˆè¡Œä¸ºçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å‡è®¾å›ºå®šçš„è§„åˆ’èŒƒå›´ï¼Œæœªèƒ½å…¨é¢æ¢è®¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­çš„è§„åˆ’èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€å¥—å½¢å¼åŒ–ä¸”å› æœåŸºç¡€çš„æ ‡å‡†ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°æ£€æµ‹å’Œè¡¨å¾è¯­è¨€æ¨¡å‹çš„è§„åˆ’è¡Œä¸ºï¼Œå¹¶å°†å…¶å®ç°ä¸ºä¸€ä¸ªåŠè‡ªåŠ¨åŒ–çš„æ³¨é‡Šç®¡é“ï¼Œä»¥ä¾¿äºåœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­è¿›è¡Œæ¯”è¾ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ ‡å‡†å®šä¹‰ã€æ³¨é‡Šç®¡é“çš„è®¾è®¡ä¸å®ç°ï¼Œä»¥åŠåœ¨Gemma-2-2Bå’ŒClaude 3.5 Haikuæ¨¡å‹ä¸Šçš„åº”ç”¨ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è§„åˆ’è¡Œä¸ºæ£€æµ‹å’Œç»“æœåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†å¯æ“ä½œçš„æ ‡å‡†å’Œæ³¨é‡Šç®¡é“ï¼Œä½¿å¾—è§„åˆ’è¡Œä¸ºçš„æ£€æµ‹æ›´åŠ ç³»ç»ŸåŒ–å’Œå¯é‡å¤ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ³¨é‡Šç®¡é“ä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬è§„åˆ’èŒƒå›´çš„åŠ¨æ€è°ƒæ•´å’Œå¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡åˆ™ä¾§é‡äºä¼˜åŒ–è§„åˆ’ä¸å³å…´ç”Ÿæˆçš„åŒºåˆ†åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGemma-2-2Bæ¨¡å‹åœ¨è¯—æ­Œç”Ÿæˆä»»åŠ¡ä¸­ä¸»è¦ä¾èµ–å³å…´ç”Ÿæˆï¼Œè€Œåœ¨MBPPä»»åŠ¡ä¸­åˆ™è¡¨ç°å‡ºè§„åˆ’ä¸å³å…´çš„åˆ‡æ¢ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œè§„åˆ’è¡Œä¸ºå¹¶éæ™®éé€‚ç”¨ï¼Œä¸”æŒ‡ä»¤è°ƒä¼˜èƒ½å¤Ÿæœ‰æ•ˆç²¾ç‚¼æ¨¡å‹çš„è§„åˆ’èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ä»£ç ç”Ÿæˆå’Œåˆ›æ„å†™ä½œç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£è¯­è¨€æ¨¡å‹çš„è§„åˆ’èƒ½åŠ›ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨ç¼–ç¨‹å’Œå†…å®¹åˆ›ä½œç­‰é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern large language models (LLMs) have demonstrated impressive performance across a wide range of multi-step reasoning tasks. Recent work suggests that LLMs may perform planning - selecting a future target token in advance and generating intermediate tokens that lead towards it - rather than merely improvising one token at a time. However, existing studies assume fixed planning horizons and often focus on single prompts or narrow domains. To distinguish planning from improvisation across models and tasks, we present formal and causally grounded criteria for detecting planning and operationalize them as a semi-automated annotation pipeline. We apply this pipeline to both base and instruction-tuned Gemma-2-2B models on the MBPP code generation benchmark and a poem generation task where Claude 3.5 Haiku was previously shown to plan. Our findings show that planning is not universal: unlike Haiku, Gemma-2-2B solves the same poem generation task through improvisation, and on MBPP it switches between planning and improvisation across similar tasks and even successive token predictions. We further show that instruction tuning refines existing planning behaviors in the base model rather than creating them from scratch. Together, these studies provide a reproducible and scalable foundation for mechanistic studies of planning in LLMs.

