---
layout: default
title: Debiasing Multilingual LLMs in Cross-lingual Latent Space
---

# Debiasing Multilingual LLMs in Cross-lingual Latent Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17948" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17948v1</a>
  <a href="https://arxiv.org/pdf/2508.17948.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17948v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17948v1', 'Debiasing Multilingual LLMs in Cross-lingual Latent Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qiwei Peng, Guimin Hu, Yekun Chai, Anders SÃ¸gaard

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**å¤‡æ³¨**: EMNLP 2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨è¯­è¨€æ½œåœ¨ç©ºé—´å»åè§æ–¹æ³•ä»¥æå‡å¤šè¯­è¨€LLMæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å»åè§` `å¤šè¯­è¨€æ¨¡å‹` `æ½œåœ¨ç©ºé—´` `è‡ªç¼–ç å™¨` `è·¨è¯­è¨€è½¬ç§»` `è‡ªç„¶è¯­è¨€å¤„ç†` `TEDæ¼”è®²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å»åè§æ–¹æ³•åœ¨ä¸åŒè¯­è¨€é—´çš„æœ‰æ•ˆæ€§æœ‰é™ï¼Œéš¾ä»¥å®ç°è‰¯å¥½çš„è·¨è¯­è¨€è½¬ç§»ã€‚
2. æœ¬æ–‡æå‡ºåœ¨è”åˆæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå»åè§ï¼Œé€šè¿‡è‡ªç¼–ç å™¨æ„å»ºå¯¹é½çš„è·¨è¯­è¨€æ½œåœ¨ç©ºé—´ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†å»åè§æ€§èƒ½å’Œè·¨è¯­è¨€è½¬ç§»èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å»åè§æŠ€æœ¯å¦‚SentDebiasæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åè§ã€‚ä»¥å¾€ç ”ç©¶é€šè¿‡ç›´æ¥åº”ç”¨è¿™äº›æ–¹æ³•äºLLMè¡¨ç¤ºæ¥è¯„ä¼°å…¶è·¨è¯­è¨€å¯è½¬ç§»æ€§ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨ä¸åŒè¯­è¨€é—´çš„æœ‰æ•ˆæ€§æœ‰é™ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºåœ¨è”åˆæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå»åè§ï¼Œè€Œéç›´æ¥ä½œç”¨äºLLMè¡¨ç¤ºã€‚æˆ‘ä»¬ä½¿ç”¨åœ¨å¹³è¡ŒTEDæ¼”è®²ç¨¿ä¸Šè®­ç»ƒçš„è‡ªç¼–ç å™¨æ„å»ºäº†ä¸€ä¸ªè‰¯å¥½å¯¹é½çš„è·¨è¯­è¨€æ½œåœ¨ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè‡ªç¼–ç å™¨èƒ½å¤Ÿæœ‰æ•ˆæ„å»ºè¯¥æ½œåœ¨ç©ºé—´ï¼Œå¹¶ä¸”åœ¨å­¦ä¹ åˆ°çš„è·¨è¯­è¨€æ½œåœ¨ç©ºé—´ä¸­åº”ç”¨å»åè§æŠ€æœ¯æ˜¾è‘—æå‡äº†æ•´ä½“å»åè§æ€§èƒ½å’Œè·¨è¯­è¨€å¯è½¬ç§»æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åè§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸åŒè¯­è¨€é—´çš„è½¬ç§»æ•ˆæœä¸ä½³ï¼Œå¯¼è‡´å»åè§æ•ˆæœå—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªè‰¯å¥½å¯¹é½çš„è·¨è¯­è¨€æ½œåœ¨ç©ºé—´ï¼Œè®ºæ–‡æå‡ºåœ¨è¯¥ç©ºé—´ä¸­è¿›è¡Œå»åè§æ“ä½œï¼Œä»¥æé«˜å»åè§çš„æ•ˆæœå’Œè·¨è¯­è¨€çš„å¯è½¬ç§»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªç¼–ç å™¨æ¨¡å—ï¼Œç”¨äºè®­ç»ƒå’Œæ„å»ºè·¨è¯­è¨€æ½œåœ¨ç©ºé—´ï¼Œéšååœ¨è¯¥ç©ºé—´ä¸­åº”ç”¨å»åè§æŠ€æœ¯ã€‚å®éªŒæ¶‰åŠå››ç§è¯­è¨€ï¼ˆè‹±è¯­ã€æ³•è¯­ã€å¾·è¯­ã€è·å…°è¯­ï¼‰ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å»åè§æŠ€æœ¯åº”ç”¨äºè”åˆæ½œåœ¨ç©ºé—´ï¼Œè€Œéç›´æ¥ä½œç”¨äºLLMè¡¨ç¤ºï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†è·¨è¯­è¨€çš„å»åè§æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè‡ªç¼–ç å™¨çš„è®­ç»ƒä½¿ç”¨å¹³è¡ŒTEDæ¼”è®²ç¨¿ï¼Œç¡®ä¿æ½œåœ¨ç©ºé—´çš„å¯¹é½æ€§ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†å»åè§æ•ˆæœä¸æ½œåœ¨ç©ºé—´çš„æ„å»ºè´¨é‡ï¼Œç¡®ä¿äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è‡ªç¼–ç å™¨æ„å»ºçš„è·¨è¯­è¨€æ½œåœ¨ç©ºé—´åœ¨å»åè§æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ•´ä½“å»åè§æ€§èƒ½æå‡äº†çº¦30%ï¼Œä¸”åœ¨è·¨è¯­è¨€è½¬ç§»èƒ½åŠ›ä¸Šä¹Ÿæœ‰æ˜æ˜¾æ”¹å–„ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œå¤šè¯­è¨€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å¤šè¯­è¨€LLMçš„å»åè§èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æœåŠ¡äºå…¨çƒç”¨æˆ·ï¼Œå‡å°‘è¯­è¨€æ¨¡å‹åœ¨ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„åè§ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œå®é™…å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Debiasing techniques such as SentDebias aim to reduce bias in large language models (LLMs). Previous studies have evaluated their cross-lingual transferability by directly applying these methods to LLM representations, revealing their limited effectiveness across languages. In this work, we therefore propose to perform debiasing in a joint latent space rather than directly on LLM representations. We construct a well-aligned cross-lingual latent space using an autoencoder trained on parallel TED talk scripts. Our experiments with Aya-expanse and two debiasing techniques across four languages (English, French, German, Dutch) demonstrate that a) autoencoders effectively construct a well-aligned cross-lingual latent space, and b) applying debiasing techniques in the learned cross-lingual latent space significantly improves both the overall debiasing performance and cross-lingual transferability.

