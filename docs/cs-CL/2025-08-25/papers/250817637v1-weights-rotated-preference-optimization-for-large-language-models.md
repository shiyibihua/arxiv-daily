---
layout: default
title: Weights-Rotated Preference Optimization for Large Language Models
---

# Weights-Rotated Preference Optimization for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17637" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17637v1</a>
  <a href="https://arxiv.org/pdf/2508.17637.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17637v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17637v1', 'Weights-Rotated Preference Optimization for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenxu Yang, Ruipeng Jia, Mingyu Zheng, Naibin Gu, Zheng Lin, Siyuan Chen, Weichong Yin, Hua Wu, Weiping Wang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**å¤‡æ³¨**: EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæƒé‡æ—‹è½¬åå¥½ä¼˜åŒ–ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„å¥–åŠ±é»‘å®¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ç›´æ¥åå¥½ä¼˜åŒ–` `å¥–åŠ±é»‘å®¢` `æƒé‡æ—‹è½¬ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `çŸ¥è¯†ä¿ç•™` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•åœ¨å¯¹é½å¤§è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹ç¼ºä¹å¤šæ ·æ€§å’ŒçŸ¥è¯†é—å¿˜ã€‚
2. æœ¬æ–‡æå‡ºçš„æƒé‡æ—‹è½¬åå¥½ä¼˜åŒ–ç®—æ³•é€šè¿‡çº¦æŸè¾“å‡ºå±‚å’Œä¸­é—´éšè—çŠ¶æ€ï¼Œé˜²æ­¢æ¨¡å‹åç¦»å‚è€ƒæ¨¡å‹ï¼Œä¿ç•™çŸ¥è¯†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRoPOåœ¨AlpacaEval 2ä¸Šæå‡3.27åˆ†ï¼Œåœ¨MT-Benchä¸Šè¶…è¶ŠåŸºçº¿6.2è‡³7.5åˆ†ï¼Œä¸”ä»…ä½¿ç”¨0.015%å¯è®­ç»ƒå‚æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨å¯¹é½å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ–¹é¢æœ‰æ•ˆï¼Œä½†å¥–åŠ±é»‘å®¢ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚è¯¥é—®é¢˜å‡ºç°åœ¨LLMsè¿‡åº¦é™ä½æ‹’ç»å®Œæˆçš„æ¦‚ç‡ä»¥è·å¾—é«˜å¥–åŠ±ï¼Œè€ŒæœªçœŸæ­£å®ç°å…¶é¢„æœŸç›®æ ‡ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹è¿‡äºå†—é•¿ä¸”ç¼ºä¹å¤šæ ·æ€§ï¼ŒåŒæ—¶é€ æˆçŸ¥è¯†çš„ç¾éš¾æ€§é—å¿˜ã€‚æˆ‘ä»¬æ¢è®¨äº†è¿™ä¸€é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼Œå³å‚æ•°ç©ºé—´ä¸­çš„ç¥ç»å…ƒå´©æºƒå¯¼è‡´çš„è¡¨ç¤ºå†—ä½™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æƒé‡æ—‹è½¬åå¥½ä¼˜åŒ–ï¼ˆRoPOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡KLæ•£åº¦éšå¼çº¦æŸè¾“å‡ºå±‚logitsï¼Œå¹¶é€šè¿‡åœ¨å¤šç²’åº¦æ­£äº¤çŸ©é˜µä¸Šè¿›è¡Œå¾®è°ƒæ˜¾å¼çº¦æŸä¸­é—´éšè—çŠ¶æ€ã€‚è¿™ä¸€è®¾è®¡é˜²æ­¢äº†ç­–ç•¥æ¨¡å‹è¿‡åº¦åç¦»å‚è€ƒæ¨¡å‹ï¼Œä»è€Œä¿ç•™äº†åœ¨é¢„è®­ç»ƒå’ŒSFTé˜¶æ®µè·å¾—çš„çŸ¥è¯†å’Œè¡¨è¾¾èƒ½åŠ›ã€‚æˆ‘ä»¬çš„RoPOåœ¨AlpacaEval 2ä¸Šå®ç°äº†æœ€é«˜3.27åˆ†çš„æå‡ï¼Œå¹¶åœ¨MT-Benchä¸Šè¶…è¶Šæœ€ä½³åŸºçº¿6.2è‡³7.5åˆ†ï¼Œä»…ä½¿ç”¨0.015%çš„å¯è®­ç»ƒå‚æ•°ï¼Œå±•ç¤ºäº†å…¶åœ¨ç¼“è§£DPOå¥–åŠ±é»‘å®¢é—®é¢˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç›´æ¥åå¥½ä¼˜åŒ–ä¸­å‡ºç°çš„å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¯¼è‡´æ¨¡å‹ç”Ÿæˆå†…å®¹å†—é•¿ä¸”ç¼ºä¹å¤šæ ·æ€§ï¼ŒåŒæ—¶é€ æˆçŸ¥è¯†çš„ç¾éš¾æ€§é—å¿˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæƒé‡æ—‹è½¬åå¥½ä¼˜åŒ–ï¼ˆRoPOï¼‰ç®—æ³•ï¼Œé€šè¿‡éšå¼å’Œæ˜¾å¼çº¦æŸï¼Œä¿æŒæ¨¡å‹çš„çŸ¥è¯†å’Œè¡¨è¾¾èƒ½åŠ›ï¼Œé˜²æ­¢å…¶åç¦»å‚è€ƒæ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoPOç®—æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯é€šè¿‡KLæ•£åº¦çº¦æŸè¾“å‡ºå±‚logitsï¼ŒäºŒæ˜¯é€šè¿‡å¤šç²’åº¦æ­£äº¤çŸ©é˜µå¾®è°ƒä¸­é—´éšè—çŠ¶æ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šRoPOçš„åˆ›æ–°åœ¨äºåŒæ—¶å¯¹è¾“å‡ºå±‚å’Œéšè—çŠ¶æ€è¿›è¡Œçº¦æŸï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­çš„è¡¨ç¤ºå†—ä½™é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šRoPOä½¿ç”¨çš„æŸå¤±å‡½æ•°ç»“åˆäº†KLæ•£åº¦å’Œæ­£äº¤çº¦æŸï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒå¯¹çŸ¥è¯†çš„æœ‰æ•ˆåˆ©ç”¨ï¼ŒåŒæ—¶å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RoPOç®—æ³•åœ¨AlpacaEval 2ä¸Šå®ç°äº†æœ€é«˜3.27åˆ†çš„æå‡ï¼Œå¹¶åœ¨MT-Benchä¸Šè¶…è¶Šæœ€ä½³åŸºçº¿6.2è‡³7.5åˆ†ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è§£å†³å¥–åŠ±é»‘å®¢é—®é¢˜æ–¹é¢çš„æ˜¾è‘—æ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•ä»…ä½¿ç”¨0.015%çš„å¯è®­ç»ƒå‚æ•°ï¼Œå±•ç°äº†å…¶é«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼ŒRoPOèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶åœ¨å¤šç§å®é™…åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„çŸ¥è¯†ä¼ é€’å’Œä¿¡æ¯ç”Ÿæˆã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½å¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œåº”ç”¨äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the efficacy of Direct Preference Optimization (DPO) in aligning Large Language Models (LLMs), reward hacking remains a pivotal challenge. This issue emerges when LLMs excessively reduce the probability of rejected completions to achieve high rewards, without genuinely meeting their intended goals. As a result, this leads to overly lengthy generation lacking diversity, as well as catastrophic forgetting of knowledge. We investigate the underlying reason behind this issue, which is representation redundancy caused by neuron collapse in the parameter space. Hence, we propose a novel Weights-Rotated Preference Optimization (RoPO) algorithm, which implicitly constrains the output layer logits with the KL divergence inherited from DPO and explicitly constrains the intermediate hidden states by fine-tuning on a multi-granularity orthogonal matrix. This design prevents the policy model from deviating too far from the reference model, thereby retaining the knowledge and expressive capabilities acquired during pre-training and SFT stages. Our RoPO achieves up to a 3.27-point improvement on AlpacaEval 2, and surpasses the best baseline by 6.2 to 7.5 points on MT-Bench with merely 0.015% of the trainable parameters, demonstrating its effectiveness in alleviating the reward hacking problem of DPO.

