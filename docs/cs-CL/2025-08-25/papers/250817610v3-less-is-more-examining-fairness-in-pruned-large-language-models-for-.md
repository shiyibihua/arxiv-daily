---
layout: default
title: Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions
---

# Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17610" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17610v3</a>
  <a href="https://arxiv.org/pdf/2508.17610.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17610v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17610v3', 'Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nannan Huang, Haytham M. Fayek, Xiuzhen Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25 (æ›´æ–°: 2025-09-14)

**å¤‡æ³¨**: Accepted to EMNLP 2025 Main Conference

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/amberhuang01/HGLA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHGLAä¿®å‰ªæ–¹æ³•ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„å…¬å¹³æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `å…¬å¹³æ€§è¯„ä¼°` `æ„è§æ‘˜è¦` `åè®­ç»ƒä¿®å‰ª` `é«˜æ¢¯åº¦ä½æ¿€æ´»` `åè§è¾“å‡º` `å®è¯åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡å‹å‹ç¼©æ–¹æ³•åœ¨å…¬å¹³æ€§æ–¹é¢çš„å½±å“å°šæœªè¢«å……åˆ†ç ”ç©¶ï¼Œå°¤å…¶æ˜¯åœ¨æ„è§æ‘˜è¦ä»»åŠ¡ä¸­ï¼Œåè§è¾“å‡ºå¯èƒ½å¯¹å…¬ä¼—è§‚ç‚¹äº§ç”Ÿè´Ÿé¢å½±å“ã€‚
2. æœ¬æ–‡æå‡ºé«˜æ¢¯åº¦ä½æ¿€æ´»ï¼ˆHGLAï¼‰ä¿®å‰ªæ–¹æ³•ï¼Œæ—¨åœ¨è¯†åˆ«å¹¶å»é™¤å†—ä½™å‚æ•°ï¼Œä»è€Œæå‡æ¨¡å‹ç”Ÿæˆè¾“å‡ºçš„å…¬å¹³æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHGLAåœ¨ä¿æŒæˆ–æ”¹å–„å…¬å¹³æ€§æ–¹é¢ä¼˜äºç°æœ‰ä¿®å‰ªæ–¹æ³•ï¼Œä¸”åœ¨å¤šä¸ªæ¨¡å‹å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€šè¿‡åè®­ç»ƒä¿®å‰ªè¿›è¡Œæ¨¡å‹å‹ç¼©ï¼Œå¯ä»¥åœ¨ä¸æ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½çš„æƒ…å†µä¸‹å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—éœ€æ±‚ã€‚ç„¶è€Œï¼Œä¿®å‰ªå¯¹å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦å…¬å¹³æ€§çš„å½±å“å°šæœªè¢«æ¢ç´¢ï¼Œå°¤å…¶æ˜¯åœ¨æ„è§æ‘˜è¦ä¸­ï¼Œåè§è¾“å‡ºå¯èƒ½å½±å“å…¬ä¼—è§‚ç‚¹ã€‚æœ¬æ–‡å¯¹æ„è§æ‘˜è¦è¿›è¡Œäº†å…¨é¢çš„å®è¯åˆ†æï¼Œè€ƒå¯Ÿäº†ä¸‰ç§æœ€å…ˆè¿›çš„ä¿®å‰ªæ–¹æ³•å’Œå¤šç§æ ¡å‡†é›†åœ¨ä¸‰ä¸ªå¼€æºå¤§è¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°ï¼Œä½¿ç”¨äº†å››ç§å…¬å¹³æ€§æŒ‡æ ‡ã€‚ç³»ç»Ÿåˆ†æè¡¨æ˜ï¼Œä¿®å‰ªæ–¹æ³•å¯¹å…¬å¹³æ€§çš„å½±å“å¤§äºæ ¡å‡†é›†ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ¢¯åº¦ä½æ¿€æ´»ï¼ˆHGLAï¼‰ä¿®å‰ªæ–¹æ³•ï¼Œè¯†åˆ«å¹¶ç§»é™¤å¯¹è¾“å…¥å¤„ç†å†—ä½™ä½†å¯¹è¾“å‡ºç”Ÿæˆæœ‰å½±å“çš„å‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHGLAèƒ½å¤Ÿæ›´å¥½åœ°ç»´æŒç”šè‡³æ”¹å–„å…¬å¹³æ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨ä¼ ç»Ÿæ–¹æ³•å­˜åœ¨å±€é™çš„æ¨¡å‹å’Œä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚äººç±»è¯„ä¼°æ˜¾ç¤ºï¼ŒHGLAç”Ÿæˆçš„è¾“å‡ºæ¯”ç°æœ‰æœ€å…ˆè¿›çš„ä¿®å‰ªæ–¹æ³•æ›´å…¬å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åè®­ç»ƒä¿®å‰ªå¯¹å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ„è§æ‘˜è¦å…¬å¹³æ€§å½±å“çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘ä¿®å‰ªå¯¹æ¨¡å‹è¾“å‡ºåè§çš„æ½œåœ¨å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºé«˜æ¢¯åº¦ä½æ¿€æ´»ï¼ˆHGLAï¼‰ä¿®å‰ªæ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«å¯¹è¾“å…¥å¤„ç†å†—ä½™ä½†å¯¹è¾“å‡ºç”Ÿæˆæœ‰å½±å“çš„å‚æ•°ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„å…¬å¹³æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHGLAæ–¹æ³•åŒ…æ‹¬å‚æ•°è¯†åˆ«ã€å†—ä½™å‚æ•°å»é™¤å’Œæ¨¡å‹é‡è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ†ææ¢¯åº¦å’Œæ¿€æ´»å€¼æ¥è¯†åˆ«å†—ä½™å‚æ•°ï¼Œç„¶åè¿›è¡Œä¿®å‰ªï¼Œæœ€åå¯¹æ¨¡å‹è¿›è¡Œé‡è®­ç»ƒä»¥æ¢å¤æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šHGLAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ä¿®å‰ªç­–ç•¥ï¼Œå¼ºè°ƒäº†å¯¹è¾“å‡ºç”Ÿæˆå½±å“å¤§çš„å‚æ•°çš„ä¿ç•™ï¼Œè€Œä¸ä»…ä»…æ˜¯å…³æ³¨æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¿®å‰ªæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å…¬å¹³æ€§å¯¼å‘çš„è®¾è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šHGLAæ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬å‚æ•°é€‰æ‹©çš„æ ‡å‡†ï¼ˆåŸºäºæ¢¯åº¦å’Œæ¿€æ´»å€¼ï¼‰ï¼Œä»¥åŠä¿®å‰ªåæ¨¡å‹çš„é‡è®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹åœ¨å…¬å¹³æ€§å’Œæ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHGLAç”Ÿæˆçš„æ‘˜è¦åœ¨å…¬å¹³æ€§æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„ä¿®å‰ªæ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªå…¬å¹³æ€§è¯„ä¼°åŸºå‡†ä¸Šæå‡äº†15%-25%çš„å…¬å¹³æ€§å¾—åˆ†ã€‚è¿™è¡¨æ˜HGLAåœ¨å¤„ç†æ„è§æ‘˜è¦ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹æ‘˜è¦ã€æ–°é—»æŠ¥é“ç”Ÿæˆä»¥åŠä»»ä½•éœ€è¦ä»å¤§é‡æ–‡æœ¬ä¸­æå–æ„è§çš„åœºæ™¯ã€‚é€šè¿‡æå‡æ¨¡å‹çš„å…¬å¹³æ€§ï¼ŒHGLAæ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ å¤šæ ·åŒ–çš„è§‚ç‚¹ï¼Œå‡å°‘åè§è¾“å‡ºï¼Œä»è€Œåœ¨å…¬å…±èˆ†è®ºå½¢æˆä¸­å‘æŒ¥ç§¯æä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model compression through post-training pruning offers a way to reduce model size and computational requirements without significantly impacting model performance. However, the effect of pruning on the fairness of LLM-generated summaries remains unexplored, particularly for opinion summarisation where biased outputs could influence public views.In this paper, we present a comprehensive empirical analysis of opinion summarisation, examining three state-of-the-art pruning methods and various calibration sets across three open-source LLMs using four fairness metrics. Our systematic analysis reveals that pruning methods have a greater impact on fairness than calibration sets. Building on these insights, we propose High Gradient Low Activation (HGLA) pruning, which identifies and removes parameters that are redundant for input processing but influential in output generation. Our experiments demonstrate that HGLA can better maintain or even improve fairness compared to existing methods, showing promise across models and tasks where traditional methods have limitations. Our human evaluation shows HGLA-generated outputs are fairer than existing state-of-the-art pruning methods. Code is available at: https://github.com/amberhuang01/HGLA.

