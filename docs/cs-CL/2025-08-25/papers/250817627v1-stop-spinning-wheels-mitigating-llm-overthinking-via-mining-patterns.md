---
layout: default
title: Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit
---

# Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17627" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17627v1</a>
  <a href="https://arxiv.org/pdf/2508.17627.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17627v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17627v1', 'Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihao Wei, Liang Pang, Jiahao Liu, Jingcheng Deng, Shicheng Xu, Zenghao Duan, Jingang Wang, Fei Sun, Xunliang Cai, Huawei Shen, Xueqi Cheng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°æ–¹æ³•ä»¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿‡åº¦æ€è€ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†ä¼˜åŒ–` `è¿‡åº¦æ€è€ƒ` `æ¨ç†å®Œæˆç‚¹` `æ¨¡å¼æŒ–æ˜` `èµ„æºæ¶ˆè€—` `å¯å‘å¼è§„åˆ™`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ—¶ï¼Œè¿‡åº¦æ€è€ƒå¯¼è‡´æ€§èƒ½ä¸‹é™å’Œèµ„æºæµªè´¹ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡è¯†åˆ«æ¨ç†å®Œæˆç‚¹ï¼ˆRCPï¼‰æ¥å‡è½»è¿‡åº¦æ€è€ƒï¼Œåˆ©ç”¨æ•æ„Ÿçš„RCPæ¨¡å¼å’Œè½»é‡çº§é˜ˆå€¼ç­–ç•¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡å°‘äº†ä»¤ç‰Œæ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ‰©å±•ä¸ªä½“æ€ç»´è¿‡ç¨‹æ¥å¢å¼ºå¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿‡åº¦æ€è€ƒå¯èƒ½ä¼šé™ä½æ•´ä½“æ€§èƒ½ã€‚æœ¬æ–‡å°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šæ¢ç´¢ä¸è¶³é˜¶æ®µã€è¡¥å¿æ¨ç†é˜¶æ®µå’Œæ¨ç†æ”¶æ•›é˜¶æ®µã€‚é€šå¸¸ï¼ŒLLMsåœ¨è¡¥å¿æ¨ç†é˜¶æ®µèƒ½å¤Ÿäº§ç”Ÿæ­£ç¡®ç­”æ¡ˆï¼Œè€Œæ¨ç†æ”¶æ•›é˜¶æ®µåˆ™å¸¸å¸¸å¯¼è‡´è¿‡åº¦æ€è€ƒï¼Œå¢åŠ èµ„æºæ¶ˆè€—ç”šè‡³å¯¼è‡´æ— é™å¾ªç¯ã€‚å› æ­¤ï¼Œå‡è½»è¿‡åº¦æ€è€ƒçš„å…³é”®åœ¨äºæ£€æµ‹è¡¥å¿æ¨ç†é˜¶æ®µçš„ç»“æŸï¼Œå³æ¨ç†å®Œæˆç‚¹ï¼ˆRCPï¼‰ã€‚æœ¬æ–‡é€šè¿‡æŒ–æ˜æ›´æ•æ„Ÿä¸”ä¸€è‡´çš„RCPæ¨¡å¼ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¯å‘å¼è§„åˆ™çš„è½»é‡çº§é˜ˆå€¼ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæˆ–æé«˜æ¨ç†å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†ä»¤ç‰Œæ¶ˆè€—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°çš„è¿‡åº¦æ€è€ƒé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ£€æµ‹æ¨ç†å®Œæˆç‚¹ï¼ˆRCPï¼‰æ—¶ç¼ºä¹é«˜æ•ˆå’Œç²¾ç¡®çš„å¹³è¡¡ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œæ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æŒ–æ˜æ¨ç†å®Œæˆç‚¹ï¼ˆRCPï¼‰çš„æ•æ„Ÿæ¨¡å¼ï¼Œç»“åˆå¯å‘å¼è§„åˆ™ï¼Œè®¾è®¡å‡ºä¸€ç§è½»é‡çº§çš„é˜ˆå€¼ç­–ç•¥ï¼Œä»¥ä¾¿åŠæ—¶ç»“æŸè¡¥å¿æ¨ç†é˜¶æ®µï¼Œé¿å…è¿›å…¥è¿‡åº¦æ€è€ƒé˜¶æ®µã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šæ¨ç†è¿‡ç¨‹ç›‘æ§ã€RCPæ¨¡å¼æŒ–æ˜å’Œé˜ˆå€¼åˆ¤æ–­ã€‚é¦–å…ˆç›‘æ§LLMçš„æ¨ç†è¿‡ç¨‹ï¼Œè¯†åˆ«æ½œåœ¨çš„RCPï¼Œç„¶åé€šè¿‡æŒ–æ˜å†å²æ¨ç†æ•°æ®ä¸­çš„æ¨¡å¼æ¥ç¡®å®šRCPï¼Œæœ€ååº”ç”¨é˜ˆå€¼ç­–ç•¥æ¥åˆ¤æ–­ä½•æ—¶ç»“æŸæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åŸºäºæ¨¡å¼æŒ–æ˜çš„RCPè¯†åˆ«æ–¹æ³•ï¼Œä¸ç°æœ‰çš„é€å¥æŸ¥è¯¢æˆ–ç›‘æ§ç»“æŸæ ‡è®°çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ›´åŠ é«˜æ•ˆå’Œå‡†ç¡®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å¯å‘å¼è§„åˆ™æ¥å®šä¹‰é˜ˆå€¼ï¼Œç¡®ä¿åœ¨ä¸åŒçš„æ¨ç†ä»»åŠ¡ä¸­éƒ½èƒ½æœ‰æ•ˆè¯†åˆ«RCPã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„èµ„æºä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨AIME24ã€AIME25å’ŒGPQA-Dç­‰åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œæ¶ˆè€—ï¼Œä¸”æ¨ç†å‡†ç¡®æ€§ä¿æŒä¸å˜æˆ–æœ‰æ‰€æå‡ï¼Œå±•ç¤ºäº†åœ¨èµ„æºåˆ©ç”¨å’Œæ¨ç†æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡å‡å°‘è¿‡åº¦æ€è€ƒï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œé™ä½èµ„æºæ¶ˆè€—ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) enhance complex reasoning tasks by scaling the individual thinking process. However, prior work shows that overthinking can degrade overall performance. Motivated by observed patterns in thinking length and content length, we categorize reasoning into three stages: insufficient exploration stage, compensatory reasoning stage, and reasoning convergence stage. Typically, LLMs produce correct answers in the compensatory reasoning stage, whereas reasoning convergence often triggers overthinking, causing increased resource usage or even infinite loops. Therefore, mitigating overthinking hinges on detecting the end of the compensatory reasoning stage, defined as the Reasoning Completion Point (RCP). RCP typically appears at the end of the first complete reasoning cycle and can be identified by querying the LLM sentence by sentence or monitoring the probability of an end-of-thinking token (e.g., \texttt{</think>}), though these methods lack an efficient and precise balance. To improve this, we mine more sensitive and consistent RCP patterns and develop a lightweight thresholding strategy based on heuristic rules. Experimental evaluations on benchmarks (AIME24, AIME25, GPQA-D) demonstrate that the proposed method reduces token consumption while preserving or enhancing reasoning accuracy.

