---
layout: default
title: How Reliable are LLMs for Reasoning on the Re-ranking task?
---

# How Reliable are LLMs for Reasoning on the Re-ranking task?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18444" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18444v1</a>
  <a href="https://arxiv.org/pdf/2508.18444.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18444v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18444v1', 'How Reliable are LLMs for Reasoning on the Re-ranking task?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nafis Tanveer Islam, Zhiming Zhao

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**å¤‡æ³¨**: Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks after the conference has published the paper

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æä¸åŒè®­ç»ƒæ–¹æ³•å¯¹LLMé‡æ’åºä»»åŠ¡çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é‡æ’åºä»»åŠ¡` `è¯­ä¹‰ç†è§£` `å¯è§£é‡Šæ€§` `è®­ç»ƒæ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMåœ¨é‡æ’åºä»»åŠ¡ä¸­é¢ä¸´é€æ˜æ€§ä¸è¶³å’Œç”¨æˆ·å‚ä¸åº¦ä½çš„é—®é¢˜ï¼Œå¯¼è‡´å†…å®¹é‡æ’åºçš„å‡†ç¡®æ€§å—åˆ°å½±å“ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡åˆ†æä¸åŒè®­ç»ƒæ–¹æ³•å¯¹LLMè¯­ä¹‰ç†è§£çš„å½±å“ï¼Œæ¥æå‡é‡æ’åºä»»åŠ¡çš„æ¨ç†èƒ½åŠ›å’Œé€æ˜æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒæŸäº›è®­ç»ƒæ–¹æ³•åœ¨å¯è§£é‡Šæ€§æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œæ­ç¤ºäº†LLMçš„å¯é æ€§é—®é¢˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯­ä¹‰ç†è§£èƒ½åŠ›çš„æå‡ï¼Œå®ƒä»¬åœ¨ä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ„è¯†ï¼Œä½†è¿™ä¹Ÿå¸¦æ¥äº†é€æ˜æ€§çš„é—®é¢˜ã€‚å°½ç®¡é€šè¿‡å®éªŒåˆ†æå–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†æ·±å…¥ç†è§£LLMçš„å†…éƒ¨å·¥ä½œæœºåˆ¶å¯¹äºç†è§£é‡æ’åºèƒŒåçš„æ¨ç†è‡³å…³é‡è¦ã€‚æœ¬æ–‡åˆ†æäº†ä¸åŒè®­ç»ƒæ–¹æ³•å¦‚ä½•å½±å“LLMåœ¨é‡æ’åºä»»åŠ¡ä¸­çš„è¯­ä¹‰ç†è§£ï¼Œå¹¶æ¢è®¨è¿™äº›æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿç”Ÿæˆæ›´å…·ä¿¡æ¯æ€§çš„æ–‡æœ¬æ¨ç†ï¼Œä»¥å…‹æœé€æ˜æ€§å’Œæœ‰é™è®­ç»ƒæ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMåœ¨é‡æ’åºä»»åŠ¡ä¸­é¢ä¸´çš„é€æ˜æ€§ä¸è¶³å’Œè®­ç»ƒæ•°æ®æœ‰é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†ç†è§£è¯­ä¹‰ï¼Œå¯¼è‡´é‡æ’åºæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†æä¸åŒè®­ç»ƒæ–¹æ³•å¯¹LLMè¯­ä¹‰ç†è§£çš„å½±å“ï¼Œæ¢è®¨å¦‚ä½•æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼Œä»è€Œæ”¹å–„é‡æ’åºçš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†æ¥è‡ªç¯å¢ƒå’Œåœ°çƒç§‘å­¦é¢†åŸŸçš„å°å‹æ’åæ•°æ®é›†ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œé‡æ’åºè¯„ä¼°çš„æ•´ä½“æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒä¸åŒè®­ç»ƒæ–¹æ³•å¯¹LLMé‡æ’åºä»»åŠ¡çš„å½±å“ï¼Œæ­ç¤ºäº†æŸäº›æ–¹æ³•åœ¨å¯è§£é‡Šæ€§æ–¹é¢çš„ä¼˜åŠ¿ï¼ŒæŒ‘æˆ˜äº†LLMçš„å¯é æ€§å‡è®¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯ä¸åŒæ–¹æ³•çš„æ•ˆæœã€‚å…·ä½“ç»†èŠ‚åŒ…æ‹¬è®­ç»ƒæ•°æ®çš„é€‰æ‹©ã€æ¨¡å‹æ¶æ„çš„è°ƒæ•´ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒæŸäº›è®­ç»ƒæ–¹æ³•åœ¨é‡æ’åºä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ã€‚è¿™ä¸€å‘ç°ä¸ºLLMçš„å¯é æ€§è¯„ä¼°æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¿¡æ¯æ£€ç´¢ã€æ¨èç³»ç»Ÿå’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚é€šè¿‡æå‡LLMåœ¨é‡æ’åºä»»åŠ¡ä¸­çš„é€æ˜æ€§å’Œå‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´å¯é çš„å†³ç­–æ”¯æŒï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the improving semantic understanding capability of Large Language Models (LLMs), they exhibit a greater awareness and alignment with human values, but this comes at the cost of transparency. Although promising results are achieved via experimental analysis, an in-depth understanding of the LLM's internal workings is unavoidable to comprehend the reasoning behind the re-ranking, which provides end users with an explanation that enables them to make an informed decision. Moreover, in newly developed systems with limited user engagement and insufficient ranking data, accurately re-ranking content remains a significant challenge. While various training methods affect the training of LLMs and generate inference, our analysis has found that some training methods exhibit better explainability than others, implying that an accurate semantic understanding has not been learned through all training methods; instead, abstract knowledge has been gained to optimize evaluation, which raises questions about the true reliability of LLMs. Therefore, in this work, we analyze how different training methods affect the semantic understanding of the re-ranking task in LLMs and investigate whether these models can generate more informed textual reasoning to overcome the challenges of transparency or LLMs and limited training data. To analyze the LLMs for re-ranking tasks, we utilize a relatively small ranking dataset from the environment and the Earth science domain to re-rank retrieved content. Furthermore, we also analyze the explainable information to see if the re-ranking can be reasoned using explainability.

