---
layout: default
title: Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs
---

# Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17863" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17863v1</a>
  <a href="https://arxiv.org/pdf/2508.17863.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17863v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17863v1', 'Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dingdong Wang, Junan Li, Mingyu Cui, Dongchao Yang, Xueyuan Chen, Helen Meng

**åˆ†ç±»**: cs.CL, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**å¤‡æ³¨**: Accepted to EMNLP 2025 Main Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒç¦»æ•£æ ‡è®°ä¸è¿ç»­ç‰¹å¾åœ¨è¯­éŸ³ç†è§£ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³ç†è§£` `è‡ªç›‘ç£å­¦ä¹ ` `ç¦»æ•£æ ‡è®°` `è¿ç»­ç‰¹å¾` `å¤§è¯­è¨€æ¨¡å‹` `æ€§èƒ½æ¯”è¾ƒ` `éŸ³é¢‘å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç¦»æ•£æ ‡è®°ä¸è¿ç»­ç‰¹å¾çš„æ€§èƒ½å·®è·ä¸Šç¼ºä¹æ·±å…¥ç ”ç©¶ï¼Œå½±å“äº†è¯­éŸ³ç†è§£çš„æ•ˆæœã€‚
2. æœ¬æ–‡é€šè¿‡å…¬å¹³æ¯”è¾ƒç¦»æ•£å’Œè¿ç»­ç‰¹å¾ï¼Œé‡‡ç”¨ç›¸åŒçš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œæ¢è®¨å…¶åœ¨è¯­éŸ³ç†è§£ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¿ç»­ç‰¹å¾åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç¦»æ•£æ ‡è®°ï¼Œæ­ç¤ºäº†ä¸¤è€…åœ¨å¤„ç†è¯­éŸ³ä¿¡æ¯æ—¶çš„ä¸åŒç‰¹æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSpeechLLMsï¼‰çš„å…´èµ·ï¼Œç¦»æ•£æ ‡è®°å’Œè¿ç»­ç‰¹å¾æˆä¸ºè¯­éŸ³å¤„ç†çš„ä¸¤ç§ä¸»è¦æ–¹æ³•ã€‚å°½ç®¡è¿™ä¸¤ç§æ–¹æ³•åœ¨éŸ³é¢‘ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä¹‹é—´çš„æ€§èƒ½å·®è·å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡åœ¨ç›¸åŒå®éªŒè®¾ç½®ä¸‹å¯¹åŸºäºè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„ç¦»æ•£å’Œè¿ç»­ç‰¹å¾è¿›è¡Œäº†å…¬å¹³æ¯”è¾ƒï¼Œè¯„ä¼°äº†å®ƒä»¬åœ¨å…­ä¸ªä¸è¯­éŸ³ç†è§£ç›¸å…³ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œè¿ç»­ç‰¹å¾åœ¨å¤šé¡¹ä»»åŠ¡ä¸­æ™®éä¼˜äºç¦»æ•£æ ‡è®°ï¼Œä¸”ä¸¤ç§æ–¹æ³•åœ¨å­¦ä¹ å’Œå¤„ç†è¯­éŸ³ä¿¡æ¯çš„æ–¹å¼ä¸Šå±•ç°å‡ºä¸åŒçš„ç‰¹å¾å’Œæ¨¡å¼ã€‚å¸Œæœ›æˆ‘ä»¬çš„ç»“æœèƒ½ä¸ºæ¨åŠ¨SpeechLLMsä¸­çš„è¯­éŸ³ç†è§£æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»æ•£æ ‡è®°ä¸è¿ç»­ç‰¹å¾åœ¨è¯­éŸ³ç†è§£ä¸­çš„æ€§èƒ½å·®è·é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ¯”è¾ƒè¿™ä¸¤ç§ç‰¹å¾çš„ä¼˜åŠ£ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­é€‰æ‹©å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åœ¨ç›¸åŒå®éªŒæ¡ä»¶ä¸‹å¯¹ç¦»æ•£å’Œè¿ç»­ç‰¹å¾è¿›è¡Œæ¯”è¾ƒï¼Œåˆ†æå®ƒä»¬åœ¨è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ä¸‹çš„è¡¨ç°ï¼Œæ­ç¤ºå„è‡ªçš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„æ¡†æ¶ï¼Œè¯„ä¼°äº†å°è§„æ¨¡å’Œå¤§è§„æ¨¡LLMï¼ˆå¦‚Qwen1.5-0.5Bå’ŒLlama3.1-8Bï¼‰åœ¨å…­ä¸ªè¯­éŸ³ç†è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…å«äº†æœ‰æ•ˆæ¯”è¾ƒã€SSLå±‚åˆ†æã€LLMå±‚åˆ†æå’Œé²æ£’æ€§æ¯”è¾ƒç­‰æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†ç¦»æ•£æ ‡è®°ä¸è¿ç»­ç‰¹å¾çš„æ€§èƒ½ï¼Œæ­ç¤ºäº†è¿ç»­ç‰¹å¾åœ¨å¤šé¡¹ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç»Ÿä¸€çš„è¯„ä¼°æ ‡å‡†å’Œæ•°æ®é›†ï¼Œè®¾ç½®äº†ç›¸åŒçš„è¶…å‚æ•°å’ŒæŸå¤±å‡½æ•°ï¼Œç¡®ä¿äº†æ¯”è¾ƒçš„å…¬å¹³æ€§å’Œç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿ç»­ç‰¹å¾åœ¨å…­ä¸ªè¯­éŸ³ç†è§£ä»»åŠ¡ä¸­æ™®éä¼˜äºç¦»æ•£æ ‡è®°ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡LLMä¸Šè¡¨ç°æ›´ä¸ºçªå‡ºï¼Œæå‡å¹…åº¦å¯è¾¾15%ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥çš„è¯­éŸ³å¤„ç†ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘å’Œä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¯­éŸ³ç†è§£çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚éšç€SpeechLLMsçš„å‘å±•ï¼Œæœ¬æ–‡çš„å‘ç°å°†ä¸ºç›¸å…³æŠ€æœ¯çš„ä¼˜åŒ–å’Œåº”ç”¨æä¾›é‡è¦å‚è€ƒï¼Œæ¨åŠ¨è¯­éŸ³å¤„ç†æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rise of Speech Large Language Models (SpeechLLMs), two dominant approaches have emerged for speech processing: discrete tokens and continuous features. Each approach has demonstrated strong capabilities in audio-related processing tasks. However, the performance gap between these two paradigms has not been thoroughly explored. To address this gap, we present a fair comparison of self-supervised learning (SSL)-based discrete and continuous features under the same experimental settings. We evaluate their performance across six spoken language understanding-related tasks using both small and large-scale LLMs (Qwen1.5-0.5B and Llama3.1-8B). We further conduct in-depth analyses, including efficient comparison, SSL layer analysis, LLM layer analysis, and robustness comparison. Our findings reveal that continuous features generally outperform discrete tokens in various tasks. Each speech processing method exhibits distinct characteristics and patterns in how it learns and processes speech information. We hope our results will provide valuable insights to advance spoken language understanding in SpeechLLMs.

