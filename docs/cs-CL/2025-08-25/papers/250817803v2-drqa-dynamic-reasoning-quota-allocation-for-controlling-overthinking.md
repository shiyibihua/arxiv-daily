---
layout: default
title: DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models
---

# DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17803" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.17803v2</a>
  <a href="https://arxiv.org/pdf/2508.17803.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17803v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17803v2', 'DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Kaiwen Yan, Xuanqing Shi, Hongcheng Guo, Wenxuan Wang, Zhuosheng Zhang, Chengwei Qin

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-25 (Êõ¥Êñ∞: 2025-11-07)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DRQA‰ª•Ëß£ÂÜ≥Êé®ÁêÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËøáÂ∫¶ÊÄùËÄÉÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êé®ÁêÜÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Âä®ÊÄÅÊé®ÁêÜÈÖçÈ¢ù` `ËøáÂ∫¶ÊÄùËÄÉ` `Âº∫ÂåñÂ≠¶‰π†` `ËµÑÊ∫êÂàÜÈÖç` `ËÆ°ÁÆóÊïàÁéá` `ÁßëÂ≠¶Êé®ÁêÜ` `Êï∞Â≠¶Êé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊé®ÁêÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÁÆÄÂçïÈóÆÈ¢òÊó∂Â∏∏Âá∫Áé∞ÂÜóÈïøÊé®ÁêÜÔºåÂØºËá¥ËÆ°ÁÆóËµÑÊ∫êÊµ™Ë¥π„ÄÇ
2. DRQAÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂíåÊâπÈáèÁîüÊàêÁöÑÂÅèÂ•ΩÊï∞ÊçÆÔºåÂä®ÊÄÅÂàÜÈÖçÊé®ÁêÜËµÑÊ∫ê‰ª•ÊèêÈ´òÊïàÁéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDRQAÂú®Â§ö‰∏™Êï∞Â≠¶ÂíåÁßëÂ≠¶Êé®ÁêÜÂü∫ÂáÜ‰∏äÊòæËëóÈôç‰Ωé‰∫Ütoken‰ΩøÁî®ÔºåÂêåÊó∂ÊèêÈ´ò‰∫ÜÁ≠îÊ°àÂáÜÁ°ÆÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êé®ÁêÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàRLLMsÔºâÂ¶ÇOpenAI-O3ÂíåDeepSeek-R1Âú®ÁªìÊûÑÂåñÂíåÂ§öÊ≠•È™§Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇÁÑ∂ËÄåÔºåÁ†îÁ©∂Ë°®ÊòéÔºåRLLMsÂ∏∏Â∏∏Âá∫Áé∞ËøáÂ∫¶ÊÄùËÄÉÁöÑÈóÆÈ¢òÔºåÂç≥Âú®ÁÆÄÂçïÈóÆÈ¢ò‰∏äÁîüÊàêÂÜóÈïøÁöÑÊé®ÁêÜÈìæÔºåÂØºËá¥ËøáÂ§öÁöÑtokenÊ∂àËÄóÂíåËÆ°ÁÆóÊïàÁéá‰Ωé‰∏ã„ÄÇÊú¨ÊñáÊèêÂá∫Âä®ÊÄÅÊé®ÁêÜÈÖçÈ¢ùÂàÜÈÖçÔºàDRQAÔºâÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂíåÊâπÈáèÁîüÊàêÁöÑÂÅèÂ•ΩÊï∞ÊçÆÔºåËÆ≠ÁªÉÊ®°ÂûãËá™ÈÄÇÂ∫îÂàÜÈÖçÊé®ÁêÜËµÑÊ∫êÔºå‰ªéËÄåÈºìÂä±Ê®°ÂûãÁîüÊàêÂáÜÁ°Æ‰∏îÁÆÄÊ¥ÅÁöÑÂõûÁ≠î„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDRQAÊòæËëóÂáèÂ∞ë‰∫Ütoken‰ΩøÁî®ÔºåÂêåÊó∂Âú®ËÆ∏Â§öÊÉÖÂÜµ‰∏ãÊèêÈ´ò‰∫ÜÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êé®ÁêÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÁÆÄÂçïÈóÆÈ¢òÊó∂ÁöÑËøáÂ∫¶ÊÄùËÄÉÁé∞Ë±°ÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ËµÑÊ∫ê‰ΩøÁî®‰∏äÊïàÁéá‰Ωé‰∏ãÔºåÂØºËá¥‰∏çÂøÖË¶ÅÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDRQAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂÄüÈâ¥ÊâπÈáèÂ§ÑÁêÜ‰∏≠ÁöÑËµÑÊ∫êÁ´û‰∫â‰ºòÂäøÔºåÂ∞ÜÂÖ∂Â∫îÁî®‰∫éÂçïÈóÆÈ¢òÊé®ÁêÜÔºåÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥Êé®ÁêÜÈÖçÈ¢ùÊù•‰ºòÂåñÊ®°ÂûãÁöÑÂõûÁ≠îË¥®ÈáèÂíåÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDRQAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÊî∂ÈõÜ„ÄÅÊ®°ÂûãËÆ≠ÁªÉÂíåÊé®ÁêÜÈò∂ÊÆµ„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÊâπÈáèÂ§ÑÁêÜÁîüÊàêÂÅèÂ•ΩÊï∞ÊçÆÔºåÁÑ∂ÂêéÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊ®°ÂûãËá™ÈÄÇÂ∫îÂàÜÈÖçÊé®ÁêÜËµÑÊ∫êÔºåÊúÄÂêéÂú®Êé®ÁêÜÈò∂ÊÆµÊ†πÊçÆÈóÆÈ¢òÂ§çÊùÇÂ∫¶Âä®ÊÄÅË∞ÉÊï¥Êé®ÁêÜÊ∑±Â∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDRQAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÊâπÈáèÂ§ÑÁêÜ‰∏≠ÁöÑËµÑÊ∫êÁ´û‰∫âÊú∫Âà∂ÂºïÂÖ•ÂçïÈóÆÈ¢òÊé®ÁêÜÔºåÊòæËëóÊîπÂñÑ‰∫ÜÊ®°ÂûãÂú®ÁÆÄÂçïÈóÆÈ¢ò‰∏äÁöÑË°®Áé∞ÔºåÂáèÂ∞ë‰∫Ü‰∏çÂøÖË¶ÅÁöÑÊé®ÁêÜÊ≠•È™§„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®DRQA‰∏≠Ôºå‰ΩøÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•Âπ≥Ë°°ÂáÜÁ°ÆÊÄß‰∏éÁÆÄÊ¥ÅÊÄßÔºåÂêåÊó∂ËÆæËÆ°‰∫ÜÈÄÇÂ∫îÊÄßÊé®ÁêÜÊ∑±Â∫¶ÁöÑÂèÇÊï∞ËÆæÁΩÆÔºå‰ª•Á°Æ‰øùÊ®°ÂûãÂú®‰∏çÂêåÈóÆÈ¢ò‰∏äËÉΩÂ§üÁÅµÊ¥ªË∞ÉÊï¥Êé®ÁêÜÁ≠ñÁï•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDRQAÂú®Â§ö‰∏™Êï∞Â≠¶ÂíåÁßëÂ≠¶Êé®ÁêÜÂü∫ÂáÜ‰∏äÔºåtoken‰ΩøÁî®ÈáèÂáèÂ∞ë‰∫ÜÁ∫¶30%ÔºåÂêåÊó∂Âú®Á≠îÊ°àÂáÜÁ°ÆÊÄß‰∏äÊèêÂçá‰∫Ü5%-10%„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåDRQAÂú®Â§ÑÁêÜÁÆÄÂçïÈóÆÈ¢òÊó∂ÁöÑÊïàÁéáÊòæËëóÊèêÈ´òÔºåÂ±ïÁé∞‰∫ÜËâØÂ•ΩÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DRQAÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂåÖÊã¨ÊïôËÇ≤„ÄÅÁßëÂ≠¶Á†îÁ©∂ÂíåÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÊé®ÁêÜÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÔºåDRQAËÉΩÂ§üÂ∏ÆÂä©Áî®Êà∑Êõ¥Âø´ÈÄüÂú∞Ëé∑Âèñ‰ø°ÊÅØÔºåÈôç‰ΩéËÆ°ÁÆóËµÑÊ∫êÁöÑÊ∂àËÄóÔºåÊé®Âä®Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØÊåÅÁª≠ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1, have recently demonstrated remarkable capabilities by performing structured and multi-step reasoning. However, recent studies reveal that RLLMs often suffer from overthinking, i.e., producing unnecessarily lengthy reasoning chains even for simple questions, leading to excessive token consumption and computational inefficiency. Interestingly, we observe that when processing multiple questions in batch mode, RLLMs exhibit more resource-efficient behavior by dynamically compressing reasoning steps for easier problems, due to implicit resource competition. Inspired by this, we propose Dynamic Reasoning Quota Allocation (DRQA), a novel method that transfers the benefits of resource competition from batch processing to single-question inference. Specifically, DRQA leverages batch-generated preference data and reinforcement learning to train the model to allocate reasoning resources adaptively. By encouraging the model to internalize a preference for responses that are both accurate and concise, DRQA enables it to generate concise answers for simple questions while retaining sufficient reasoning depth for more challenging ones. Extensive experiments on a wide range of mathematical and scientific reasoning benchmarks demonstrate that DRQA significantly reduces token usage while maintaining, and in many cases improving, answer accuracy. By effectively mitigating the overthinking problem, DRQA offers a promising direction for more efficient and scalable deployment of RLLMs, and we hope it inspires further exploration into fine-grained control of reasoning behaviors.

