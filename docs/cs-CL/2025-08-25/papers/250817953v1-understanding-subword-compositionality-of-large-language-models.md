---
layout: default
title: Understanding Subword Compositionality of Large Language Models
---

# Understanding Subword Compositionality of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17953" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17953v1</a>
  <a href="https://arxiv.org/pdf/2508.17953.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17953v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17953v1', 'Understanding Subword Compositionality of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qiwei Peng, Yekun Chai, Anders SÃ¸gaard

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**å¤‡æ³¨**: EMNLP 2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹å¤§è¯­è¨€æ¨¡å‹å­è¯ç»„åˆæ€§çš„æ·±å…¥ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å­è¯ç»„åˆ` `ç»“æ„ç›¸ä¼¼æ€§` `è¯­ä¹‰å¯åˆ†è§£æ€§` `å½¢å¼ä¿ç•™` `ç»„åˆç­–ç•¥` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨å­è¯ç»„åˆæ—¶çš„è¡¨ç°å·®å¼‚å°šæœªå¾—åˆ°å……åˆ†ç†è§£ï¼Œç¼ºä¹ç³»ç»Ÿçš„åˆ†ææ¡†æ¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®ºæ–‡é€šè¿‡å®éªŒåˆ†æå­è¯ç»„åˆçš„ç»“æ„ç›¸ä¼¼æ€§ã€è¯­ä¹‰å¯åˆ†è§£æ€§å’Œå½¢å¼ä¿ç•™ï¼Œæ­ç¤ºä¸åŒæ¨¡å‹çš„ç»„åˆç­–ç•¥ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šç ”ç©¶å‘ç°äº”ä¸ªLLMå®¶æ—å¯åˆ†ä¸ºä¸‰ç±»ï¼Œä¸”åœ¨ä¸åŒå±‚æ¬¡ä¸Šå¯¹å­è¯ç»„åˆçš„æ•æ„Ÿæ€§è¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥å­è¯åºåˆ—ä¸ºè¾“å…¥ï¼Œéœ€æœ‰æ•ˆç»„åˆå­è¯è¡¨ç¤ºä»¥å½¢æˆæœ‰æ„ä¹‰çš„è¯çº§è¡¨ç¤ºã€‚æœ¬æ–‡é€šè¿‡ä¸€ç³»åˆ—å®éªŒæ¢è®¨LLMså¦‚ä½•ç»„åˆå­è¯ä¿¡æ¯ï¼Œé‡ç‚¹å…³æ³¨ç»“æ„ç›¸ä¼¼æ€§ã€è¯­ä¹‰å¯åˆ†è§£æ€§å’Œå½¢å¼ä¿ç•™ä¸‰ä¸ªå…³é”®æ–¹é¢ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œäº”ä¸ªLLMå®¶æ—å¯åˆ†ä¸ºä¸‰ç±»ï¼Œåæ˜ å…¶åº•å±‚ç»„åˆç­–ç•¥çš„å·®å¼‚ã€‚å…·ä½“è§‚å¯Ÿåˆ°ï¼šå­è¯ç»„åˆä¸æ•´ä½“è¯è¡¨ç¤ºä¹‹é—´çš„ç»“æ„ç›¸ä¼¼æ€§åœ¨ä¸åŒå±‚æ¬¡ä¸Šå‘ˆç°ä¸‰ç§ä¸åŒæ¨¡å¼ï¼›é€å±‚æ¢æµ‹å…¶å¯¹è¯­ä¹‰å¯åˆ†è§£æ€§çš„æ•æ„Ÿæ€§è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼›åœ¨æ¢æµ‹å¯¹å½¢å¼ç‰¹å¾çš„æ•æ„Ÿæ€§æ—¶ï¼Œå‘ˆç°å‡ºä¸‰ç§ä¸åŒæ¨¡å¼ã€‚è¿™äº›å‘ç°ä¸ºLLMsçš„ç»„åˆåŠ¨æ€æä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶çªæ˜¾äº†LLMsåœ¨ç¼–ç å’Œæ•´åˆå­è¯ä¿¡æ¯æ—¶çš„ä¸åŒç»„åˆæ¨¡å¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å­è¯ç»„åˆæ—¶çš„è¡¨ç°å·®å¼‚é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½ç³»ç»Ÿåˆ†æå…¶ç»„åˆç­–ç•¥çš„å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®¾è®¡ä¸€ç³»åˆ—å®éªŒï¼Œæ¢è®¨å­è¯ç»„åˆçš„ç»“æ„ç›¸ä¼¼æ€§ã€è¯­ä¹‰å¯åˆ†è§£æ€§å’Œå½¢å¼ä¿ç•™ï¼Œæ­ç¤ºä¸åŒæ¨¡å‹åœ¨ç»„åˆå­è¯ä¿¡æ¯æ—¶çš„ç­–ç•¥å·®å¼‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç»“æ„ç›¸ä¼¼æ€§åˆ†æï¼›2) è¯­ä¹‰å¯åˆ†è§£æ€§æ¢æµ‹ï¼›3) å½¢å¼ç‰¹å¾æ•æ„Ÿæ€§è¯„ä¼°ã€‚æ¯ä¸ªæ¨¡å—é€šè¿‡é€å±‚åˆ†ææ¨¡å‹çš„è¾“å‡ºï¼Œæ¯”è¾ƒå­è¯ç»„åˆä¸æ•´ä½“è¯è¡¨ç¤ºçš„å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°åˆ†ç±»å’Œåˆ†æä¸åŒLLMå®¶æ—çš„ç»„åˆç­–ç•¥ï¼Œæ­ç¤ºäº†å­è¯ç»„åˆä¸æ•´ä½“è¯è¡¨ç¤ºä¹‹é—´çš„ä¸‰ç§æ¼”å˜æ¨¡å¼ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­é‡‡ç”¨é€å±‚æ¢æµ‹çš„æ–¹æ³•ï¼Œè®¾ç½®äº†å¤šç§å‚æ•°ä»¥è¯„ä¼°ä¸åŒå±‚æ¬¡çš„è¾“å‡ºï¼Œä½¿ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹å¯¹è¯­ä¹‰å’Œå½¢å¼ç‰¹å¾çš„æ•æ„Ÿæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œäº”ä¸ªå¤§è¯­è¨€æ¨¡å‹å®¶æ—åœ¨å­è¯ç»„åˆçš„ç»“æ„ç›¸ä¼¼æ€§å’Œè¯­ä¹‰å¯åˆ†è§£æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ï¼Œå°¤å…¶åœ¨é€å±‚åˆ†æä¸­ï¼ŒæŸäº›æ¨¡å‹åœ¨ç‰¹å®šå±‚æ¬¡ä¸Šå¯¹è¯­ä¹‰ç‰¹å¾çš„æ•æ„Ÿæ€§æå‡äº†20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶ç»„åˆç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„å­è¯ç»„åˆæ€§ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨å¤æ‚è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) take sequences of subwords as input, requiring them to effective compose subword representations into meaningful word-level representations. In this paper, we present a comprehensive set of experiments to probe how LLMs compose subword information, focusing on three key aspects: structural similarity, semantic decomposability, and form retention. Our analysis of the experiments suggests that these five LLM families can be classified into three distinct groups, likely reflecting difference in their underlying composition strategies. Specifically, we observe (i) three distinct patterns in the evolution of structural similarity between subword compositions and whole-word representations across layers; (ii) great performance when probing layer by layer their sensitivity to semantic decompositionality; and (iii) three distinct patterns when probing sensitivity to formal features, e.g., character sequence length. These findings provide valuable insights into the compositional dynamics of LLMs and highlight different compositional pattens in how LLMs encode and integrate subword information.

