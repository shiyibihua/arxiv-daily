---
layout: default
title: Enhancing Speech Large Language Models through Reinforced Behavior Alignment
---

# Enhancing Speech Large Language Models through Reinforced Behavior Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03526" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03526v1</a>
  <a href="https://arxiv.org/pdf/2509.03526.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03526v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03526v1', 'Enhancing Speech Large Language Models through Reinforced Behavior Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yansong Liu, Jiateng Li, Yuan Liu

**åˆ†ç±»**: cs.CL, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼ºåŒ–è¡Œä¸ºå¯¹é½æ¡†æ¶ä»¥æå‡è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `è¡Œä¸ºå¯¹é½` `è‡ªæˆ‘åˆæˆ` `å¤šæ¨¡æ€ä»»åŠ¡` `æŒ‡ä»¤è·Ÿéš` `è¯­éŸ³å¤„ç†` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéšæ–¹é¢è¡¨ç°ä¸å¦‚æ–‡æœ¬æ¨¡å‹ï¼Œå°¤å…¶åœ¨å¤„ç†åŠ¨æ€ç”¨æˆ·è¯­éŸ³æ—¶å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚
2. æœ¬æ–‡æå‡ºçš„å¼ºåŒ–è¡Œä¸ºå¯¹é½ï¼ˆRBAï¼‰æ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘åˆæˆç”Ÿæˆé«˜è´¨é‡å¯¹é½æ•°æ®ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¯¹SpeechLMsè¿›è¡Œè¡Œä¸ºå¯¹é½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRBAæ–¹æ³•åœ¨æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ä¸Šè¶…è¶Šä¼ ç»Ÿè’¸é¦åŸºçº¿ï¼Œå¹¶åœ¨å£è¯­é—®ç­”å’Œè¯­éŸ³è½¬æ–‡æœ¬ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•å¼•å‘äº†å°†å…¶è¯­è¨€èƒ½åŠ›æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€çš„ç ”ç©¶å…´è¶£ï¼Œå‚¬ç”Ÿäº†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMsï¼‰ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡æ€é—´çš„å·®å¼‚ï¼Œè¿™äº›æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéšæ–¹é¢ä»å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¼ºåŒ–è¡Œä¸ºå¯¹é½ï¼ˆRBAï¼‰æ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘åˆæˆç”Ÿæˆé«˜ä¿çœŸå¯¹é½æ•°æ®ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯¹SpeechLMsè¿›è¡Œè¡Œä¸ºå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæå‡äº†SpeechLMsçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿè’¸é¦åŸºçº¿ï¼Œå¹¶å¯æ‰©å±•è‡³å£è¯­é—®ç­”å’Œè¯­éŸ³è½¬æ–‡æœ¬ç­‰ä»»åŠ¡ï¼Œå–å¾—äº†åœ¨å¼€æ”¾åŸºå‡†ä¸Šçš„é¢†å…ˆæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­ç”±äºæ¨¡æ€å·®å¼‚å¯¼è‡´çš„æ€§èƒ½ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥æ ‡æ³¨è¿›è¡Œå¾®è°ƒï¼Œæ•ˆç‡ä½ä¸”æ•ˆæœæœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¼ºåŒ–è¡Œä¸ºå¯¹é½ï¼ˆRBAï¼‰æ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘åˆæˆç”Ÿæˆé«˜ä¿çœŸå¯¹é½æ•°æ®ï¼Œé¿å…äº†å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¯¹æ¨¡å‹è¡Œä¸ºè¿›è¡Œå¯¹é½ï¼Œä»è€Œæå‡æ¨¡å‹çš„è¯­è¨€ç”Ÿæˆèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRBAæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯é€šè¿‡å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ç”Ÿæˆå¯¹é½æ•°æ®ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹SpeechLMsè¿›è¡Œè¡Œä¸ºå¯¹é½ï¼Œç¡®ä¿å…¶è¾“å‡ºä¸æ•™å¸ˆæ¨¡å‹ä¸€è‡´ã€‚

**å…³é”®åˆ›æ–°**ï¼šRBAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºè‡ªæˆ‘åˆæˆæ•°æ®ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ å¯¹é½ç­–ç•¥çš„ç»“åˆï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è¡¡é‡æ¨¡å‹è¾“å‡ºä¸æ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§å­¦ä¹ ç‡ç­–ç•¥ä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œç¡®ä¿æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­èƒ½å¤Ÿå¿«é€Ÿé€‚åº”ç”¨æˆ·çš„è¯­éŸ³è¾“å…¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRBAæ–¹æ³•åœ¨æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿè’¸é¦åŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°äº†XX%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å£è¯­é—®ç­”å’Œè¯­éŸ³è½¬æ–‡æœ¬ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå’Œå¤šæ¨¡æ€äº¤äº’å¹³å°ç­‰ã€‚é€šè¿‡æå‡è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯èƒ½æ‰©å±•åˆ°æ›´å¤šå¤æ‚çš„è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œå½±å“åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The recent advancements of Large Language Models (LLMs) have spurred considerable research interest in extending their linguistic capabilities beyond text to other modalities, which leads to emergence of speech-based LLMs (SpeechLMs) with capability of processing user request in either speech or textual formats. However, owing to inter-modal discrepancies, these SpeechLMs still exhibit a significant performance gap compared to their text-based LLM counterparts in instruction-following, particularly when confronted with the dynamic and variable nature of user speech. To address this challenge, this paper introduces a framework termed Reinforced Behavior Alignment (RBA), designed to bolster the language generation proficiency of SpeechLMs. Instead of relying on supervised fine-tuning from human annotations, RBA employs a self-synthesis methodology to generate extensive, high-fidelity alignment data by a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of a teacher using a reinforcement learning-based approach. Experimental results demonstrate that this method effectively enhances the instruction-following capabilities of SpeechLMs that outperform conventional distillation baselines. Crucially, we demonstrate that RBA can be seamlessly extended to tasks such including spoken question answering and speech-to-text translation, attaining state-of-the-art performance on open benchmarks with only self-generated data.

