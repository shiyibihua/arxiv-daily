---
layout: default
title: ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models
---

# ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17892" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17892v2</a>
  <a href="https://arxiv.org/pdf/2508.17892.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17892v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17892v2', 'ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25 (æ›´æ–°: 2025-09-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºILReä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡å¤„ç†` `ä¸­é—´å±‚æ£€ç´¢` `ä¸Šä¸‹æ–‡å‹ç¼©` `å¤§å‹è¯­è¨€æ¨¡å‹` `è®¡ç®—æ•ˆç‡` `å†…å­˜ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶é¢ä¸´çŸ­æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦å’Œé«˜è®¡ç®—å¤æ‚åº¦ç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æœ¬æ–‡æå‡ºä¸­é—´å±‚æ£€ç´¢ï¼ˆILReï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç¦»çº¿é€‰æ‹©ä¸­é—´è§£ç å±‚å¹¶ä¼˜åŒ–ä¸Šä¸‹æ–‡ç¼–ç è¿‡ç¨‹ï¼Œæœ‰æ•ˆé™ä½è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒILReåœ¨å¤„ç†å•ä¸ª1Mä»¤ç‰Œè¯·æ±‚æ—¶é€Ÿåº¦æå‡çº¦180å€ï¼Œå¹¶åœ¨RULER-1MåŸºå‡†æµ‹è¯•ä¸­å–å¾—çº¦79.8çš„å¾—åˆ†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸­ä»ç„¶å­˜åœ¨çŸ­æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ã€è®¡ç®—å¤æ‚åº¦é«˜å’Œå†…å­˜å¼€é”€å¤§çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸Šä¸‹æ–‡å‹ç¼©ç®¡é“â€”â€”ä¸­é—´å±‚æ£€ç´¢ï¼ˆILReï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç¦»çº¿ç¡®å®šä¸€ä¸ªä¸­é—´è§£ç å™¨å±‚ï¼Œä»…å¯¹è¯¥å±‚è¿›è¡Œä¸Šä¸‹æ–‡ç¼–ç ï¼Œå¹¶é€šè¿‡è¾“å…¥æŸ¥è¯¢ä¸å®Œæ•´é”®ç¼“å­˜ä¹‹é—´çš„æ³¨æ„åŠ›å¾—åˆ†æ¥å›å¿†ä»¤ç‰Œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¤šæ± æ ¸åˆ†é…ç­–ç•¥ï¼Œä»¥ä¿æŒè¯­ä¹‰çš„å®Œæ•´æ€§ã€‚è¯¥æ–¹æ³•å°†é¢„å¡«å……å¤æ‚åº¦ä»O(LÂ²)é™ä½åˆ°O(L)ï¼Œå¹¶å°†å†…å­˜å ç”¨å‡å°‘åˆ°å…¨ä¸Šä¸‹æ–‡æ‰€éœ€çš„å‡ ååˆ†ä¹‹ä¸€ï¼ŒåŒæ—¶åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸­è¡¨ç°å‡ºä¸å…¨ä¸Šä¸‹æ–‡è®¾ç½®ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿è¾“å…¥æ—¶é¢ä¸´çŸ­æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ã€è®¡ç®—å¤æ‚åº¦ä¸ºO(LÂ²)å’Œé«˜å†…å­˜å¼€é”€ç­‰ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šILReé€šè¿‡ç¦»çº¿é€‰æ‹©ä¸€ä¸ªä¸­é—´è§£ç å™¨å±‚ï¼Œä»…å¯¹è¯¥å±‚è¿›è¡Œä¸Šä¸‹æ–‡ç¼–ç ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›å¾—åˆ†è¿›è¡Œä»¤ç‰Œå›å¿†ï¼Œä»è€Œæœ‰æ•ˆé™ä½è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜å ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šILReçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç¦»çº¿ä¸­é—´å±‚é€‰æ‹©ã€æµå¼åˆ†å—é¢„å¡«å……å’ŒåŸºäºæ³¨æ„åŠ›å¾—åˆ†çš„ä»¤ç‰Œå›å¿†ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªé«˜æ•ˆçš„ä¸Šä¸‹æ–‡å‹ç¼©ç®¡é“ã€‚

**å…³é”®åˆ›æ–°**ï¼šILReçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šæ± æ ¸åˆ†é…ç­–ç•¥ï¼Œä»¥ä¿æŒè¯­ä¹‰çš„å®Œæ•´æ€§ï¼ŒåŒæ—¶å°†é¢„å¡«å……å¤æ‚åº¦ä»O(LÂ²)é™ä½åˆ°O(L)ï¼Œæ˜¾è‘—æå‡äº†å¤„ç†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä»¤ç‰Œå›å¿†è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ± æ ¸åˆ†é…ç­–ç•¥ï¼Œç¡®ä¿åœ¨å‡å°‘å†…å­˜å ç”¨çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä¿ç•™è¯­ä¹‰ä¿¡æ¯ï¼Œæ­¤å¤–ï¼ŒILReæ— éœ€é¢å¤–çš„åæœŸè®­ç»ƒæˆ–æ“ä½œå¼€å‘ï¼Œç›´æ¥æé«˜äº†å¤„ç†é€Ÿåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ILReåœ¨å¤„ç†å•ä¸ª1Mä»¤ç‰Œè¯·æ±‚æ—¶ï¼Œé€Ÿåº¦æå‡çº¦180å€ï¼Œå¤„ç†æ—¶é—´ä¸è¶³åŠåˆ†é’Ÿã€‚æ­¤å¤–ï¼Œåœ¨RULER-1MåŸºå‡†æµ‹è¯•ä¸­ï¼ŒILReå–å¾—äº†çº¦79.8çš„å¾—åˆ†ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å…¨ä¸Šä¸‹æ–‡è®¾ç½®ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦å¤„ç†é•¿æ–‡æœ¬çš„åœºæ™¯ï¼Œå¦‚æ³•å¾‹æ–‡ä¹¦åˆ†æã€é•¿ç¯‡æ–‡ç« æ‘˜è¦ç­‰ã€‚ILReçš„é«˜æ•ˆæ€§å’Œä½å†…å­˜å ç”¨å°†æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate decoder layer offline, encodes context by streaming chunked prefill only up to that layer, and recalls tokens by the attention scores between the input query and full key cache in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the prefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to a few tenths of that required for the full context, but also delivers performance comparable to or superior to the full-context setup in long-context scenarios. Without additional post training or operator development, ILRe can process a single $1M$ tokens request in less than half a minute (speedup $\approx 180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.

