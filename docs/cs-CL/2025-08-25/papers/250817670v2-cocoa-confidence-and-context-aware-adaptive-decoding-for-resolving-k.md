---
layout: default
title: CoCoA: Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models
---

# CoCoA: Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17670" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.17670v2</a>
  <a href="https://arxiv.org/pdf/2508.17670.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17670v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17670v2', 'CoCoA: Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Anant Khandelwal, Manish Gupta, Puneet Agrawal

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-25 (Êõ¥Êñ∞: 2025-08-27)

**Â§áÊ≥®**: Accepted to EMNLP'25, Main. 21 pages, 17 tables, 3 Figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CoCoA‰ª•Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÁü•ËØÜÂÜ≤Á™ÅÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Áü•ËØÜÂÜ≤Á™Å` `Ëá™ÈÄÇÂ∫îËß£Á†Å` `‰ø°ÂøÉÊÑüÁü•` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÈóÆÁ≠îÁ≥ªÁªü` `Ëá™Âä®ÊëòË¶Å` `ÈïøÊñáÊú¨Â§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂØπÊØîËß£Á†ÅÊñπÊ≥ïÂú®Â§ÑÁêÜÁü•ËØÜÂÜ≤Á™ÅÊó∂Áº∫‰πèÈÄÇÂ∫îÊÄßÔºåÂ∞§ÂÖ∂Âú®‰ΩéÂÜ≤Á™ÅÁéØÂ¢É‰∏≠ÊÄßËÉΩÂèØËÉΩ‰∏ãÈôç„ÄÇ
2. CoCoAÈÄöËøá‰ø°ÂøÉÊÑüÁü•Â∫¶ÈáèÂíåÂπø‰πâÊï£Â∫¶Êù•ÂÆûÁé∞ÂÜ≤Á™ÅËß£ÂÜ≥Ôºå‰øùÊåÅÂú®‰ΩéÂÜ≤Á™ÅÊÉÖÂÜµ‰∏ãÁöÑÂº∫ÊÄßËÉΩ„ÄÇ
3. Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåCoCoAÁöÑÈóÆÁ≠îÂáÜÁ°ÆÁéáÂπ≥ÂùáÊèêÂçá9.2ÂàÜÔºåÊëòË¶ÅÂíåÈïøÊñáÊú¨ÈóÆÁ≠îÁöÑ‰∫ãÂÆûÊÄßÊèêÂçá2.5ÂàÜ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁîüÊàêÂø†ÂÆûÂÜÖÂÆπÊó∂Èù¢‰∏¥Êù•Ëá™ÂèÇÊï∞ËÆ∞ÂøÜ‰∏éÂ§ñÈÉ®‰∏ä‰∏ãÊñá‰πãÈó¥ÁöÑÁü•ËØÜÂÜ≤Á™Å„ÄÇÁé∞ÊúâÁöÑÂØπÊØîËß£Á†ÅÊñπÊ≥ïËôΩÁÑ∂ÈíàÂØπÂÜ≤Á™ÅËøõË°å‰∫ÜË∞É‰ºòÔºå‰ΩÜÂú®‰ΩéÂÜ≤Á™ÅÁéØÂ¢É‰∏ãÁöÑÈÄÇÂ∫îÊÄß‰∏çË∂≥ÔºåÂèØËÉΩÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫é‰ø°ÂøÉÂíå‰∏ä‰∏ãÊñáÁöÑËá™ÈÄÇÂ∫îËß£Á†ÅÁÆóÊ≥ïCoCoAÔºåÊó®Âú®ÂÆûÁé∞ÂéüÂàôÊÄßÁöÑÂÜ≤Á™ÅËß£ÂÜ≥ÂíåÂ¢ûÂº∫ÁöÑÂø†ÂÆûÊÄß„ÄÇCoCoAÈÄöËøáÂà©Áî®‰ø°ÂøÉÊÑüÁü•Â∫¶ÈáèÔºàÁÜµÂ∑ÆÂíå‰∏ä‰∏ãÊñáÂ≥∞ÂÄºÔºâ‰ª•ÂèäÂèÇÊï∞Âíå‰∏ä‰∏ãÊñáÂàÜÂ∏É‰πãÈó¥ÁöÑÂπø‰πâÊï£Â∫¶Êù•Ëß£ÂÜ≥ÂÜ≤Á™Å„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåCoCoAÂú®Â§ö‰∏™ÈóÆÁ≠î„ÄÅÊëòË¶ÅÂíåÈïøÊñáÊú¨ÈóÆÁ≠îÂü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåQAÂáÜÁ°ÆÁéáÂπ≥ÂùáÊèêÂçá9.2ÂàÜÔºåÊëòË¶ÅÂíåÈïøÊñáÊú¨ÈóÆÁ≠îÁöÑ‰∫ãÂÆûÊÄßÂπ≥ÂùáÊèêÂçá2.5ÂàÜÔºåÂπ∂ÂØπÂÜ≤Á™ÅÂèòÂåñË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÊïèÊÑüÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÂÜÖÂÆπÊó∂ÁöÑÁü•ËØÜÂÜ≤Á™ÅÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®‰ΩéÂÜ≤Á™ÅÁéØÂ¢É‰∏ãÁöÑÈÄÇÂ∫îÊÄß‰∏çË∂≥ÔºåÂèØËÉΩÂØºËá¥ÁîüÊàêÂÜÖÂÆπÁöÑÂáÜÁ°ÆÊÄßÂíåÂø†ÂÆûÊÄß‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCoCoAÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÁªìÂêà‰ø°ÂøÉÊÑüÁü•Â∫¶ÈáèÔºàÂ¶ÇÁÜµÂ∑ÆÂíå‰∏ä‰∏ãÊñáÂ≥∞ÂÄºÔºâ‰∏éÂèÇÊï∞Âíå‰∏ä‰∏ãÊñáÂàÜÂ∏É‰πãÈó¥ÁöÑÂπø‰πâÊï£Â∫¶Ôºå‰ª•ÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÂÜ≤Á™ÅËß£ÂÜ≥„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåCoCoAËÉΩÂ§üÂú®‰∏çÂêåÂÜ≤Á™ÅÁ®ãÂ∫¶‰∏ã‰øùÊåÅËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCoCoAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰ø°ÂøÉÊÑüÁü•Ê®°ÂùóÂíåÂÜ≤Á™ÅËß£ÂÜ≥Ê®°Âùó„ÄÇ‰ø°ÂøÉÊÑüÁü•Ê®°ÂùóËØÑ‰º∞ÁîüÊàêÂÜÖÂÆπÁöÑÂèØ‰ø°Â∫¶ÔºåËÄåÂÜ≤Á™ÅËß£ÂÜ≥Ê®°ÂùóÂàôÊ†πÊçÆËØÑ‰º∞ÁªìÊûúË∞ÉÊï¥ÁîüÊàêÁ≠ñÁï•Ôºå‰ª•‰ºòÂåñËæìÂá∫Ë¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCoCoAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Ëá™ÈÄÇÂ∫îËß£Á†ÅÊú∫Âà∂ÔºåËÉΩÂ§üÊ†πÊçÆ‰∏ä‰∏ãÊñáÂíå‰ø°ÂøÉÂä®ÊÄÅË∞ÉÊï¥ÁîüÊàêÁ≠ñÁï•Ôºå‰∏é‰º†ÁªüÁöÑÈùôÊÄÅËß£Á†ÅÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∑ÊúâÊõ¥È´òÁöÑÁÅµÊ¥ªÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåCoCoAÂºïÂÖ•‰∫ÜÁÜµÂ∑ÆÂíå‰∏ä‰∏ãÊñáÂ≥∞ÂÄº‰Ωú‰∏∫ÂÖ≥ÈîÆÂ∫¶ÈáèÊåáÊ†áÔºåÂπ∂ËÆæËÆ°‰∫ÜÁõ∏Â∫îÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•Á°Æ‰øùÁîüÊàêÂÜÖÂÆπÁöÑÂø†ÂÆûÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂÆûÈ™å‰∏≠ÔºåCoCoAÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÈóÆÁ≠îÂáÜÁ°ÆÁéáÂπ≥ÂùáÊèêÂçá9.2ÂàÜÔºåÁõ∏ËæÉ‰∫éÂº∫Âü∫Á∫øAdaCADÔºåÊëòË¶ÅÂíåÈïøÊñáÊú¨ÈóÆÁ≠îÁöÑ‰∫ãÂÆûÊÄßÂπ≥ÂùáÊèêÂçá2.5ÂàÜÔºåÂ±ïÁé∞Âá∫ÂØπÂÜ≤Á™ÅÂèòÂåñÁöÑ‰ºòË∂äÊïèÊÑüÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CoCoAÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÊΩúÂú®Â∫îÁî®‰ª∑ÂÄºÔºåÂåÖÊã¨ÈóÆÁ≠îÁ≥ªÁªü„ÄÅËá™Âä®ÊëòË¶ÅÁîüÊàêÂíåÈïøÊñáÊú¨Â§ÑÁêÜÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÁîüÊàêÂÜÖÂÆπÁöÑÂáÜÁ°ÆÊÄßÂíåÂø†ÂÆûÊÄßÔºåCoCoAËÉΩÂ§ü‰∏∫Áî®Êà∑Êèê‰æõÊõ¥ÂèØÈù†ÁöÑ‰ø°ÊÅØÊúçÂä°ÔºåÊé®Âä®Êô∫ËÉΩÂä©ÊâãÂíåÂÜÖÂÆπÁîüÊàêÂ∑•ÂÖ∑ÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Faithful generation in large language models (LLMs) is challenged by knowledge conflicts between parametric memory and external context. Existing contrastive decoding methods tuned specifically to handle conflict often lack adaptability and can degrade performance in low conflict settings. We introduce CoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-level algorithm for principled conflict resolution and enhanced faithfulness. CoCoA resolves conflict by utilizing confidence-aware measures (entropy gap and contextual peakedness) and the generalized divergence between the parametric and contextual distributions. Crucially, CoCoA maintains strong performance even in low conflict settings. Extensive experiments across multiple LLMs on diverse Question Answering (QA), Summarization, and Long-Form Question Answering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance over strong baselines like AdaCAD. It yields significant gains in QA accuracy, up to 9.2 points on average compared to the strong baseline AdaCAD, and improves factuality in summarization and LFQA by up to 2.5 points on average across key benchmarks. Additionally, it demonstrates superior sensitivity to conflict variations. CoCoA enables more informed, context-aware, and ultimately more faithful token generation.

