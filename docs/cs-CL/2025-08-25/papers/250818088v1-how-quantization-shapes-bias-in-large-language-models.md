---
layout: default
title: How Quantization Shapes Bias in Large Language Models
---

# How Quantization Shapes Bias in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18088" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18088v1</a>
  <a href="https://arxiv.org/pdf/2508.18088.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18088v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18088v1', 'How Quantization Shapes Bias in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Federico Marcuzzi, Xuefei Ning, Roy Schwartz, Iryna Gurevych

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°é‡åŒ–å¯¹å¤§è¯­è¨€æ¨¡å‹åè§çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æŠ€æœ¯` `æ¨¡å‹åè§` `å¤§è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¼¦ç†è€ƒé‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é‡åŒ–è¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†è€ƒè™‘å¯¹æ¨¡å‹åè§çš„å½±å“ï¼Œå°¤å…¶æ˜¯å¯¹ä¸åŒäººå£å­ç¾¤ä½“çš„å½±å“ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»¼åˆè¯„ä¼°é‡åŒ–å¯¹æ¨¡å‹åè§å½±å“çš„æ–¹æ³•ï¼Œé‡ç‚¹åˆ†ææƒé‡å’Œæ¿€æ´»é‡åŒ–ç­–ç•¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡åŒ–å¯ä»¥é™ä½æ¨¡å‹æ¯’æ€§ï¼Œä½†åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å¯èƒ½å¢åŠ åˆ»æ¿å°è±¡å’Œä¸å…¬å¹³æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ¿€è¿›å‹ç¼©ä¸‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶å…¨é¢è¯„ä¼°äº†é‡åŒ–å¦‚ä½•å½±å“æ¨¡å‹åè§ï¼Œç‰¹åˆ«å…³æ³¨å…¶å¯¹ä¸ªä½“äººå£å­ç¾¤ä½“çš„å½±å“ã€‚æˆ‘ä»¬èšç„¦äºæƒé‡å’Œæ¿€æ´»é‡åŒ–ç­–ç•¥ï¼Œè€ƒå¯Ÿå…¶åœ¨å¤šç§åè§ç±»å‹ï¼ˆåŒ…æ‹¬åˆ»æ¿å°è±¡ã€æ¯’æ€§ã€æƒ…æ„Ÿå’Œå…¬å¹³æ€§ï¼‰ä¸Šçš„æ•ˆæœã€‚é€šè¿‡åœ¨ä¹ä¸ªåŸºå‡†ä¸Šä½¿ç”¨æ¦‚ç‡å’Œç”Ÿæˆæ–‡æœ¬æŒ‡æ ‡ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸åŒæ¶æ„å’Œæ¨ç†èƒ½åŠ›çš„æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œé‡åŒ–å¯¹åè§çš„å½±å“å¤æ‚ï¼šè™½ç„¶å®ƒå¯ä»¥å‡å°‘æ¨¡å‹çš„æ¯’æ€§ä¸”å¯¹æƒ…æ„Ÿå½±å“ä¸å¤§ï¼Œä½†åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨æ¿€è¿›å‹ç¼©ä¸‹ï¼Œå¾€å¾€ä¼šç•¥å¾®å¢åŠ åˆ»æ¿å°è±¡å’Œä¸å…¬å¹³æ€§ã€‚è¿™äº›è¶‹åŠ¿åœ¨ä¸åŒäººå£ç±»åˆ«å’Œæ¨¡å‹ç±»å‹ä¸­æ™®éä¸€è‡´ï¼Œå°½ç®¡å…¶å¹…åº¦ä¾èµ–äºå…·ä½“è®¾ç½®ã€‚æ€»ä½“è€Œè¨€ï¼Œç»“æœå¼ºè°ƒäº†åœ¨å®é™…åº”ç”¨ä¸­å¹³è¡¡æ•ˆç‡ä¸ä¼¦ç†è€ƒé‡çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³é‡åŒ–å¯¹å¤§è¯­è¨€æ¨¡å‹åè§å½±å“çš„è¯„ä¼°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†é‡åŒ–å¯¹ä¸åŒäººå£å­ç¾¤ä½“çš„æ½œåœ¨å½±å“ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½äº§ç”Ÿä¸å…¬å¹³ç»“æœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç³»ç»Ÿè¯„ä¼°é‡åŒ–ç­–ç•¥å¯¹æ¨¡å‹åè§çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯æƒé‡å’Œæ¿€æ´»é‡åŒ–ï¼Œåˆ†æå…¶åœ¨ä¸åŒåè§ç±»å‹ä¸Šçš„è¡¨ç°ï¼Œä»¥ä¾¿ä¸ºæ¨¡å‹ä¼˜åŒ–æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ¦‚ç‡å’Œç”Ÿæˆæ–‡æœ¬æŒ‡æ ‡ï¼Œæ¶µç›–ä¹ä¸ªåŸºå‡†æµ‹è¯•ã€‚æ¨¡å‹çš„æ¶æ„å’Œæ¨ç†èƒ½åŠ›ä¹Ÿè¢«çº³å…¥è€ƒé‡ï¼Œä»¥å…¨é¢è¯„ä¼°é‡åŒ–çš„æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°åˆ†æäº†é‡åŒ–å¯¹æ¨¡å‹åè§çš„å¤æ‚å½±å“ï¼Œæ­ç¤ºäº†é‡åŒ–åœ¨é™ä½æ¯’æ€§ä¸å¢åŠ åˆ»æ¿å°è±¡ä¹‹é—´çš„æƒè¡¡ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒçš„é‡åŒ–ç­–ç•¥ï¼Œå¹¶å¯¹æ¨¡å‹çš„æ¶æ„å’Œæ¨ç†èƒ½åŠ›è¿›è¡Œäº†åˆ†ç±»ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒè®¾ç½®ä¸‹çš„æ¨¡å‹è¡¨ç°ï¼Œç ”ç©¶æ­ç¤ºäº†é‡åŒ–å¯¹åè§çš„å½±å“å¹…åº¦å’Œæ–¹å‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡åŒ–èƒ½å¤Ÿæ˜¾è‘—é™ä½æ¨¡å‹çš„æ¯’æ€§ï¼Œä½†åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨æ¿€è¿›å‹ç¼©ä¸‹ï¼Œåˆ»æ¿å°è±¡å’Œä¸å…¬å¹³æ€§ç•¥æœ‰ä¸Šå‡ã€‚ä¸åŒæ¨¡å‹æ¶æ„å’Œæ¨ç†èƒ½åŠ›çš„æ¯”è¾ƒè¡¨æ˜ï¼Œè¿™äº›è¶‹åŠ¿åœ¨å„äººå£ç±»åˆ«ä¸­æ™®éå­˜åœ¨ï¼Œå¼ºè°ƒäº†é‡åŒ–ç­–ç•¥é€‰æ‹©çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸å’Œäººå·¥æ™ºèƒ½ä¼¦ç†ç­‰ã€‚é€šè¿‡ç†è§£é‡åŒ–å¯¹æ¨¡å‹åè§çš„å½±å“ï¼Œç ”ç©¶å¯ä»¥å¸®åŠ©å¼€å‘æ›´å…¬å¹³å’Œé«˜æ•ˆçš„è¯­è¨€æ¨¡å‹ï¼Œä¿ƒè¿›æŠ€æœ¯åœ¨ç¤¾ä¼šä¸­çš„è´Ÿè´£ä»»ä½¿ç”¨ã€‚æœªæ¥ï¼Œéšç€é‡åŒ–æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ï¼Œè¯¥ç ”ç©¶çš„å‘ç°å°†å¯¹æ¨¡å‹è®¾è®¡å’Œè¯„ä¼°æ ‡å‡†äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work presents a comprehensive evaluation of how quantization affects model bias, with particular attention to its impact on individual demographic subgroups. We focus on weight and activation quantization strategies and examine their effects across a broad range of bias types, including stereotypes, toxicity, sentiment, and fairness. We employ both probabilistic and generated text-based metrics across nine benchmarks and evaluate models varying in architecture family and reasoning ability. Our findings show that quantization has a nuanced impact on bias: while it can reduce model toxicity and does not significantly impact sentiment, it tends to slightly increase stereotypes and unfairness in generative tasks, especially under aggressive compression. These trends are generally consistent across demographic categories and model types, although their magnitude depends on the specific setting. Overall, our results highlight the importance of carefully balancing efficiency and ethical considerations when applying quantization in practice.

