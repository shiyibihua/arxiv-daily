---
layout: default
title: SentiMM: A Multimodal Multi-Agent Framework for Sentiment Analysis in Social Media
---

# SentiMM: A Multimodal Multi-Agent Framework for Sentiment Analysis in Social Media

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18108" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18108v1</a>
  <a href="https://arxiv.org/pdf/2508.18108.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18108v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18108v1', 'SentiMM: A Multimodal Multi-Agent Framework for Sentiment Analysis in Social Media')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xilai Xu, Zilin Zhao, Chengye Song, Zining Wang, Jinhe Qiang, Jiongrui Yan, Yuhuai Lin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSentiMMæ¡†æ¶ä»¥è§£å†³ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æä¸­çš„å¤šæ¨¡æ€æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åˆ†æ` `æƒ…æ„Ÿåˆ†ç±»` `ç¤¾äº¤åª’ä½“` `çŸ¥è¯†æ£€ç´¢` `è·¨æ¨¡æ€èåˆ` `å¤šä»£ç†æ¡†æ¶` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æƒ…æ„Ÿåˆ†ææ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€ç¤¾äº¤åª’ä½“å†…å®¹æ—¶ï¼Œå¸¸å¸¸é¢ä¸´è·¨æ¨¡æ€èåˆä¸è¶³å’Œå¤–éƒ¨çŸ¥è¯†æ•´åˆç¼ºå¤±çš„é—®é¢˜ã€‚
2. SentiMMæ¡†æ¶é€šè¿‡ä¸“é—¨çš„ä»£ç†å¤„ç†æ–‡æœ¬å’Œè§†è§‰æ•°æ®ï¼Œèåˆå¤šæ¨¡æ€ç‰¹å¾å¹¶åˆ©ç”¨çŸ¥è¯†æ£€ç´¢å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSentiMMåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ç¤¾äº¤åª’ä½“ä¸Šå¤šæ¨¡æ€å†…å®¹çš„æ—¥ç›Šæ™®åŠï¼Œæƒ…æ„Ÿåˆ†æåœ¨å¤„ç†å¼‚æ„æ•°æ®å’Œè¯†åˆ«å¤šæ ‡ç­¾æƒ…æ„Ÿæ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹æœ‰æ•ˆçš„è·¨æ¨¡æ€èåˆå’Œå¤–éƒ¨çŸ¥è¯†æ•´åˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SentiMMï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚SentiMMé€šè¿‡ä¸“é—¨çš„ä»£ç†å¤„ç†æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ï¼Œèåˆå¤šæ¨¡æ€ç‰¹å¾ï¼Œé€šè¿‡çŸ¥è¯†æ£€ç´¢ä¸°å¯Œä¸Šä¸‹æ–‡ï¼Œå¹¶èšåˆç»“æœè¿›è¡Œæœ€ç»ˆæƒ…æ„Ÿåˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†SentiMMDï¼Œä¸€ä¸ªåŒ…å«ä¸ƒä¸ªç»†ç²’åº¦æƒ…æ„Ÿç±»åˆ«çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSentiMMåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ï¼ŒéªŒè¯äº†æˆ‘ä»¬ç»“æ„åŒ–æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç¤¾äº¤åª’ä½“ä¸­å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¼‚æ„æ•°æ®å’Œå¤šæ ‡ç­¾æƒ…æ„Ÿè¯†åˆ«æ—¶å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è·¨æ¨¡æ€ä¿¡æ¯èåˆå’Œå¤–éƒ¨çŸ¥è¯†æ•´åˆæ–¹é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSentiMMæ¡†æ¶é€šè¿‡å¼•å…¥å¤šä¸ªä¸“é—¨çš„ä»£ç†æ¥å¤„ç†ä¸åŒæ¨¡æ€çš„æ•°æ®ï¼Œæ—¨åœ¨å®ç°æ›´æœ‰æ•ˆçš„ç‰¹å¾èåˆå’Œä¸Šä¸‹æ–‡ä¸°å¯Œï¼Œä»è€Œæé«˜æƒ…æ„Ÿåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSentiMMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ–‡æœ¬å’Œè§†è§‰è¾“å…¥çš„å¤„ç†ä»£ç†ã€è·¨æ¨¡æ€ç‰¹å¾èåˆæ¨¡å—ã€çŸ¥è¯†æ£€ç´¢æ¨¡å—ä»¥åŠæœ€ç»ˆçš„æƒ…æ„Ÿåˆ†ç±»èšåˆæ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æå‡æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSentiMMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šä»£ç†è®¾è®¡å’ŒçŸ¥è¯†æ£€ç´¢çš„ç»“åˆï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€æ¨¡æ€å¤„ç†å’Œç¼ºä¹å¤–éƒ¨çŸ¥è¯†çš„æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒSentiMMé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¤šæ¨¡æ€ç‰¹å¾çš„èåˆæ•ˆæœï¼Œå¹¶è®¾è®¡äº†çµæ´»çš„ç½‘ç»œç»“æ„ä»¥é€‚åº”ä¸åŒæ¨¡æ€æ•°æ®çš„è¾“å…¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒSentiMMåœ¨å¤šä¸ªæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“ç›‘æµ‹ã€å“ç‰Œæƒ…æ„Ÿåˆ†æå’Œç”¨æˆ·åé¦ˆå¤„ç†ç­‰ã€‚é€šè¿‡æé«˜æƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§ï¼ŒSentiMMèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šæ›´å¥½åœ°ç†è§£ç”¨æˆ·æƒ…æ„Ÿï¼Œä¼˜åŒ–å¸‚åœºç­–ç•¥ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯æ‰©å±•è‡³å…¶ä»–å¤šæ¨¡æ€æ•°æ®åˆ†æé¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œå½±å“åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the increasing prevalence of multimodal content on social media, sentiment analysis faces significant challenges in effectively processing heterogeneous data and recognizing multi-label emotions. Existing methods often lack effective cross-modal fusion and external knowledge integration. We propose SentiMM, a novel multi-agent framework designed to systematically address these challenges. SentiMM processes text and visual inputs through specialized agents, fuses multimodal features, enriches context via knowledge retrieval, and aggregates results for final sentiment classification. We also introduce SentiMMD, a large-scale multimodal dataset with seven fine-grained sentiment categories. Extensive experiments demonstrate that SentiMM achieves superior performance compared to state-of-the-art baselines, validating the effectiveness of our structured approach.

