---
layout: default
title: Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining
---

# Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15556" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15556v1</a>
  <a href="https://arxiv.org/pdf/2509.15556.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15556v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15556v1', 'Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ping Guo, Yubing Ren, Binbin Liu, Fengze Liu, Haobin Lin, Yifan Zhang, Bingni Zhang, Taifeng Wang, Yin Zheng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºClimbæ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–å¤šè¯­è¨€æ•°æ®åˆ†é…æå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®åˆ†é…` `è·¨è¯­è¨€äº¤äº’` `é¢„è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šè¯­è¨€LLMè®­ç»ƒä¸­éš¾ä»¥ç¡®å®šæœ€ä½³è¯­è¨€æ¯”ä¾‹ï¼Œå¿½ç•¥äº†è·¨è¯­è¨€äº¤äº’å’Œæ•°æ®é›†è§„æ¨¡çš„å½±å“ã€‚
2. Climbæ¡†æ¶é€šè¿‡å¼•å…¥è·¨è¯­è¨€äº¤äº’æ„ŸçŸ¥è¯­è¨€æ¯”ä¾‹ï¼Œé‡åŒ–è¯­è¨€é—´çš„ä¾èµ–å…³ç³»ï¼Œä¼˜åŒ–å¤šè¯­è¨€æ•°æ®åˆ†é…ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒClimbèƒ½å‡†ç¡®è¡¡é‡è·¨è¯­è¨€äº¤äº’ï¼Œæå‡LLMå¤šè¯­è¨€æ€§èƒ½ï¼Œç”šè‡³åª²ç¾ä½¿ç”¨æ›´å¤štokensè®­ç»ƒçš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºå…¨çƒèŒƒå›´å†…å„ç§åº”ç”¨ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ï¼Œä»è€Œæ¨åŠ¨äº†å¯¹æœ‰æ•ˆå¤šè¯­è¨€èƒ½åŠ›çš„ç©ºå‰å…¨çƒéœ€æ±‚ã€‚å®ç°å¼ºå¤§çš„å¤šè¯­è¨€æ€§èƒ½çš„å…³é”®åœ¨äºè®­ç»ƒè¯­æ–™åº“ä¸­è¯­è¨€æ¯”ä¾‹çš„æˆ˜ç•¥åˆ†é…ã€‚ç„¶è€Œï¼Œç”±äºå¤æ‚çš„è·¨è¯­è¨€äº¤äº’å’Œå¯¹æ•°æ®é›†è§„æ¨¡çš„æ•æ„Ÿæ€§ï¼Œç¡®å®šæœ€ä½³è¯­è¨€æ¯”ä¾‹æå…·æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºClimbï¼ˆè·¨è¯­è¨€äº¤äº’æ„ŸçŸ¥å¤šè¯­è¨€å¹³è¡¡ï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°ä¼˜åŒ–å¤šè¯­è¨€æ•°æ®åˆ†é…ã€‚Climbçš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†ä¸€ç§è·¨è¯­è¨€äº¤äº’æ„ŸçŸ¥è¯­è¨€æ¯”ä¾‹ï¼Œé€šè¿‡æ•è·è¯­è¨€é—´çš„ä¾èµ–å…³ç³»æ¥æ˜¾å¼é‡åŒ–æ¯ç§è¯­è¨€çš„æœ‰æ•ˆåˆ†é…ã€‚åˆ©ç”¨è¯¥æ¯”ä¾‹ï¼ŒClimbæå‡ºäº†ä¸€ç§åŸåˆ™æ€§çš„ä¸¤æ­¥ä¼˜åŒ–ç¨‹åºâ€”â€”é¦–å…ˆå‡è¡¡å„ç§è¯­è¨€çš„è¾¹é™…æ”¶ç›Šï¼Œç„¶åæœ€å¤§åŒ–æ‰€å¾—è¯­è¨€åˆ†é…å‘é‡çš„å¤§å°â€”â€”ä»è€Œæ˜¾è‘—ç®€åŒ–äº†å›ºæœ‰å¤šè¯­è¨€ä¼˜åŒ–é—®é¢˜ã€‚å¤§é‡å®éªŒè¯å®ï¼ŒClimbå¯ä»¥å‡†ç¡®è¡¡é‡å„ç§å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„è·¨è¯­è¨€äº¤äº’ã€‚ä½¿ç”¨Climbå¯¼å‡ºçš„æ¯”ä¾‹è®­ç»ƒçš„LLMå§‹ç»ˆå¦‚ä¸€åœ°å®ç°äº†æœ€å…ˆè¿›çš„å¤šè¯­è¨€æ€§èƒ½ï¼Œç”šè‡³å®ç°äº†ä¸ä½¿ç”¨æ›´å¤štokensè®­ç»ƒçš„å¼€æºLLMç›¸åª²ç¾çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤šè¯­è¨€é¢„è®­ç»ƒä¸­ï¼Œå¦‚ä½•ç¡®å®šå„ç§è¯­è¨€çš„æœ€ä½³æ•°æ®åˆ†é…æ¯”ä¾‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥å¤„ç†å¤æ‚çš„è·¨è¯­è¨€äº¤äº’ï¼Œå¹¶ä¸”å¯¹æ•°æ®é›†è§„æ¨¡çš„æ•æ„Ÿæ€§è¾ƒé«˜ï¼Œå¯¼è‡´è®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šClimbæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥â€œè·¨è¯­è¨€äº¤äº’æ„ŸçŸ¥è¯­è¨€æ¯”ä¾‹â€è¿™ä¸€æ¦‚å¿µï¼Œé€šè¿‡é‡åŒ–ä¸åŒè¯­è¨€ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæ›´å‡†ç¡®åœ°è¯„ä¼°æ¯ç§è¯­è¨€å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ã€‚åŸºäºæ­¤ï¼ŒClimbæ—¨åœ¨ä¼˜åŒ–è¯­è¨€åˆ†é…ï¼Œä½¿å¾—æ¯ç§è¯­è¨€çš„è¾¹é™…æ”¶ç›Šç›¸ç­‰ï¼Œå¹¶æœ€å¤§åŒ–æ•´ä½“çš„è¯­è¨€åˆ†é…å‘é‡ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šClimbæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š1) **è·¨è¯­è¨€äº¤äº’æ„ŸçŸ¥è¯­è¨€æ¯”ä¾‹è®¡ç®—**ï¼šè¯¥æ­¥éª¤æ—¨åœ¨é‡åŒ–æ¯ç§è¯­è¨€çš„æœ‰æ•ˆåˆ†é…ï¼Œè€ƒè™‘äº†è¯­è¨€é—´çš„ä¾èµ–å…³ç³»ã€‚å…·ä½“æ–¹æ³•æœªçŸ¥ã€‚2) **ä¸¤æ­¥ä¼˜åŒ–è¿‡ç¨‹**ï¼šé¦–å…ˆï¼Œå‡è¡¡å„ç§è¯­è¨€çš„è¾¹é™…æ”¶ç›Šï¼Œç¡®ä¿æ¯ç§è¯­è¨€å¯¹æ¨¡å‹æ€§èƒ½çš„æå‡è´¡çŒ®ç›¸å½“ï¼›ç„¶åï¼Œæœ€å¤§åŒ–æ‰€å¾—è¯­è¨€åˆ†é…å‘é‡çš„å¤§å°ï¼Œä»¥å……åˆ†åˆ©ç”¨æ‰€æœ‰è¯­è¨€çš„æ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šClimbçš„å…³é”®åˆ›æ–°åœ¨äºå…¶â€œè·¨è¯­è¨€äº¤äº’æ„ŸçŸ¥è¯­è¨€æ¯”ä¾‹â€çš„æ¦‚å¿µï¼Œå®ƒèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¡¡é‡ä¸åŒè¯­è¨€ä¹‹é—´çš„ç›¸äº’å½±å“ï¼Œä»è€Œä¸ºå¤šè¯­è¨€æ•°æ®åˆ†é…æä¾›æ›´åˆç†çš„ä¾æ®ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒClimbèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¯­è¨€é—´çš„å¤æ‚å…³ç³»ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¤šè¯­è¨€æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å¹¶æœªè¯¦ç»†æè¿°è·¨è¯­è¨€äº¤äº’æ„ŸçŸ¥è¯­è¨€æ¯”ä¾‹çš„å…·ä½“è®¡ç®—æ–¹æ³•ï¼Œä»¥åŠä¸¤æ­¥ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ã€‚è¿™äº›ç»†èŠ‚å¯èƒ½åŒ…å«åœ¨è¡¥å……ææ–™æˆ–åç»­ç ”ç©¶ä¸­ã€‚å…·ä½“ç½‘ç»œç»“æ„æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Climbæ¡†æ¶è®­ç»ƒçš„LLMåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒClimbè®­ç»ƒçš„æ¨¡å‹ç”šè‡³å¯ä»¥ä¸ä½¿ç”¨æ›´å¤štokensè®­ç»ƒçš„å¼€æºLLMç›¸åª²ç¾ï¼Œè¿™è¡¨æ˜Climbåœ¨æå‡æ¨¡å‹æ•ˆç‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Climbæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦å¤šè¯­è¨€èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒåœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ã€å¤šè¯­è¨€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤šè¯­è¨€æ•°æ®åˆ†é…ï¼ŒClimbèƒ½å¤Ÿæå‡LLMåœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä»è€Œä¸ºå…¨çƒç”¨æˆ·æä¾›æ›´ä¼˜è´¨çš„æœåŠ¡ã€‚è¯¥ç ”ç©¶çš„æˆæœæœ‰åŠ©äºæ¨åŠ¨å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have become integral to a wide range of applications worldwide, driving an unprecedented global demand for effective multilingual capabilities. Central to achieving robust multilingual performance is the strategic allocation of language proportions within training corpora. However, determining optimal language ratios is highly challenging due to intricate cross-lingual interactions and sensitivity to dataset scale. This paper introduces Climb (Cross-Lingual Interaction-aware Multilingual Balancing), a novel framework designed to systematically optimize multilingual data allocation. At its core, Climb introduces a cross-lingual interaction-aware language ratio, explicitly quantifying each language's effective allocation by capturing inter-language dependencies. Leveraging this ratio, Climb proposes a principled two-step optimization procedure--first equalizing marginal benefits across languages, then maximizing the magnitude of the resulting language allocation vectors--significantly simplifying the inherently complex multilingual optimization problem. Extensive experiments confirm that Climb can accurately measure cross-lingual interactions across various multilingual settings. LLMs trained with Climb-derived proportions consistently achieve state-of-the-art multilingual performance, even achieving competitive performance with open-sourced LLMs trained with more tokens.

