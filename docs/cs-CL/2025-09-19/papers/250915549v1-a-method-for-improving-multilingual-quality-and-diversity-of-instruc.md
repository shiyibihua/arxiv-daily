---
layout: default
title: A method for improving multilingual quality and diversity of instruction fine-tuning datasets
---

# A method for improving multilingual quality and diversity of instruction fine-tuning datasets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15549" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15549v1</a>
  <a href="https://arxiv.org/pdf/2509.15549.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15549v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15549v1', 'A method for improving multilingual quality and diversity of instruction fine-tuning datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chunguang Zhao, Yilun Liu, Pufan Zeng, Yuanchang Luo, Shimin Tao, Minggui He, Weibin Meng, Song Xu, Ziang Chen, Chen Liu, Hongxia Ma, Li Zhang, Boxing Chen, Daimeng Wei

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM-DaQæ–¹æ³•ï¼Œæå‡å¤šè¯­è¨€æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œå¢å¼ºLLMçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€å­¦ä¹ ` `æŒ‡ä»¤å¾®è°ƒ` `æ•°æ®é€‰æ‹©` `æ•°æ®è´¨é‡` `æ•°æ®å¤šæ ·æ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `è·¨è¯­è¨€æ³›åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹é€‰æ‹©æŒ‡ä»¤å¾®è°ƒæ•°æ®æ—¶ï¼Œä¾èµ–ç®€å•å¯å‘å¼æˆ–ç‰¹å®šè¯­è¨€å‡è®¾ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. M-DaQæ–¹æ³•é€šè¿‡é€‰æ‹©é«˜è´¨é‡å’Œè¯­ä¹‰å¤šæ ·åŒ–çš„å¤šè¯­è¨€æŒ‡ä»¤å¾®è°ƒæ ·æœ¬ï¼Œæå‡LLMçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨M-DaQå¾®è°ƒçš„æ¨¡å‹åœ¨18ç§è¯­è¨€ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼Œèƒœç‡è¶…è¿‡60%ï¼Œæ–‡åŒ–ç›¸å…³æ€§æ›´å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šè¯­è¨€æŒ‡ä»¤å¾®è°ƒ(IFT)å¯¹äºä½¿å¤§å‹è¯­è¨€æ¨¡å‹(LLM)èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°ä¸åŒçš„è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡å¤šè¯­è¨€è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºä»¥åŠç›¸åº”çš„æ„å»ºæ–¹æ³•ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®ç“¶é¢ˆã€‚è™½ç„¶æ•°æ®é€‰æ‹©åœ¨è‹±è¯­ç¯å¢ƒä¸­æ˜¾ç¤ºå‡ºäº†å¸Œæœ›ï¼Œä½†ç”±äºä¾èµ–äºç®€å•çš„å¯å‘å¼æ–¹æ³•æˆ–ç‰¹å®šäºè¯­è¨€çš„å‡è®¾ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸æ— æ³•è·¨è¯­è¨€æ¨å¹¿ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å¤šè¯­è¨€æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§(M-DaQ)æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©é«˜è´¨é‡å’Œè¯­ä¹‰å¤šæ ·çš„å¤šè¯­è¨€IFTæ ·æœ¬æ¥æé«˜LLMçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¯¹å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨é¢å¯¹é½å‡è®¾(SAH)è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿç ”ç©¶ã€‚åœ¨18ç§è¯­è¨€ä¸Šçš„ç»éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨M-DaQæ–¹æ³•å¾®è°ƒçš„æ¨¡å‹æ¯”vanillaåŸºçº¿å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œèƒœç‡è¶…è¿‡60%ã€‚äººå·¥è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†è¿™äº›æ”¶ç›Šï¼Œçªå‡ºäº†å“åº”ä¸­æ–‡åŒ–ç‚¹çš„å¢åŠ ã€‚æˆ‘ä»¬å‘å¸ƒäº†M-DaQä»£ç ï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰å¤šè¯­è¨€æŒ‡ä»¤å¾®è°ƒé¢ä¸´é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•éš¾ä»¥è·¨è¯­è¨€æ³›åŒ–ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ä¸åŒè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¡¨ç°ä¸ä½³ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºç®€å•çš„å¯å‘å¼è§„åˆ™æˆ–é’ˆå¯¹ç‰¹å®šè¯­è¨€çš„å‡è®¾ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å¤šè¯­è¨€æ•°æ®çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šM-DaQçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é€‰æ‹©é«˜è´¨é‡å’Œè¯­ä¹‰å¤šæ ·åŒ–çš„å¤šè¯­è¨€æŒ‡ä»¤å¾®è°ƒæ ·æœ¬ï¼Œä»è€Œæå‡LLMçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚è¯¥æ–¹æ³•æ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•åœ¨è·¨è¯­è¨€æ³›åŒ–æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æé«˜æ¨¡å‹åœ¨ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„é€‚åº”æ€§ã€‚é€šè¿‡ç²¾å¿ƒé€‰æ‹©è®­ç»ƒæ•°æ®ï¼ŒM-DaQåŠ›æ±‚ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆå„ç§è¯­è¨€çš„æ–‡æœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM-DaQæ–¹æ³•åŒ…å«æ•°æ®è´¨é‡è¯„ä¼°å’Œæ•°æ®å¤šæ ·æ€§é€‰æ‹©ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œå¯¹å€™é€‰å¤šè¯­è¨€æŒ‡ä»¤å¾®è°ƒæ•°æ®è¿›è¡Œè´¨é‡è¯„ä¼°ï¼Œç­›é€‰å‡ºé«˜è´¨é‡çš„æ ·æœ¬ã€‚ç„¶åï¼Œåœ¨é«˜è´¨é‡æ ·æœ¬ä¸­ï¼Œé€šè¿‡æŸç§ç­–ç•¥ï¼ˆå…·ä½“ç­–ç•¥æœªçŸ¥ï¼Œè®ºæ–‡æœªè¯¦ç»†æè¿°ï¼‰é€‰æ‹©è¯­ä¹‰ä¸Šå¤šæ ·åŒ–çš„æ ·æœ¬ï¼Œä»¥ç¡®ä¿è®­ç»ƒæ•°æ®çš„è¦†ç›–èŒƒå›´å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œä½¿ç”¨é€‰æ‹©å‡ºçš„é«˜è´¨é‡å’Œå¤šæ ·åŒ–æ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šM-DaQçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»¼åˆè€ƒè™‘äº†å¤šè¯­è¨€æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æœ‰æ•ˆçš„æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒM-DaQæ›´åŠ æ³¨é‡è·¨è¯­è¨€çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¯­è¨€è¡¨è¾¾ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é¦–æ¬¡ç³»ç»Ÿåœ°ç ”ç©¶äº†è¡¨é¢å¯¹é½å‡è®¾(SAH)åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„é€‚ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†æè¿°æ•°æ®è´¨é‡è¯„ä¼°å’Œå¤šæ ·æ€§é€‰æ‹©çš„å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¾‹å¦‚ä½¿ç”¨çš„å…·ä½“æŒ‡æ ‡ã€ç®—æ³•æˆ–æ¨¡å‹ã€‚å…³äºæŸå¤±å‡½æ•°ã€ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ä¹ŸæœªçŸ¥ã€‚ä½†å¯ä»¥æ¨æµ‹ï¼Œæ•°æ®è´¨é‡è¯„ä¼°å¯èƒ½æ¶‰åŠä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¯¹æ•°æ®çš„æµç•…æ€§ã€è¯­æ³•æ­£ç¡®æ€§ã€è¯­ä¹‰å®Œæ•´æ€§ç­‰æ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚å¤šæ ·æ€§é€‰æ‹©å¯èƒ½æ¶‰åŠä½¿ç”¨èšç±»ç®—æ³•æˆ–åŸºäºåµŒå…¥çš„æ–¹æ³•æ¥é€‰æ‹©è¯­ä¹‰ä¸Šä¸åŒçš„æ ·æœ¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨M-DaQæ–¹æ³•å¾®è°ƒçš„æ¨¡å‹åœ¨18ç§è¯­è¨€ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œèƒœç‡è¶…è¿‡vanillaåŸºçº¿60%ä»¥ä¸Šã€‚äººå·¥è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†è¿™äº›æ”¶ç›Šï¼Œè¡¨æ˜ä½¿ç”¨M-DaQå¾®è°ƒçš„æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ–‡åŒ–ç›¸å…³æ€§ï¼Œä»è€Œäº§ç”Ÿæ›´ç¬¦åˆå½“åœ°æ–‡åŒ–ä¹ æƒ¯çš„å“åº”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

M-DaQæ–¹æ³•å¯åº”ç”¨äºå„ç§éœ€è¦å¤šè¯­è¨€æ”¯æŒçš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚å¤šè¯­è¨€èŠå¤©æœºå™¨äººã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ã€å¤šè¯­è¨€å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡LLMçš„å¤šè¯­è¨€èƒ½åŠ›ï¼ŒM-DaQå¯ä»¥å¸®åŠ©æ„å»ºæ›´åŠ æ™ºèƒ½ã€é«˜æ•ˆå’Œæ˜“äºä½¿ç”¨çš„å¤šè¯­è¨€AIç³»ç»Ÿï¼Œä¿ƒè¿›ä¸åŒè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹çš„äº¤æµä¸åˆä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large language models (LLMs) to generalize effectively across diverse linguistic and cultural contexts. However, the scarcity of high-quality multilingual training data and corresponding building method remains a critical bottleneck. While data selection has shown promise in English settings, existing methods often fail to generalize across languages due to reliance on simplistic heuristics or language-specific assumptions. In this work, we introduce Multilingual Data Quality and Diversity (M-DaQ), a novel method for improving LLMs multilinguality, by selecting high-quality and semantically diverse multilingual IFT samples. We further conduct the first systematic investigation of the Superficial Alignment Hypothesis (SAH) in multilingual setting. Empirical results across 18 languages demonstrate that models fine-tuned with M-DaQ method achieve significant performance gains over vanilla baselines over 60% win rate. Human evaluations further validate these gains, highlighting the increment of cultural points in the response. We release the M-DaQ code to support future research.

