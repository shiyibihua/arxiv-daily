---
layout: default
title: Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models
---

# Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15631" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15631v1</a>
  <a href="https://arxiv.org/pdf/2509.15631.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15631v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15631v1', 'Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tomoya Yamashita, Akira Ito, Yuuki Yamanaka, Masanori Yamada, Takayuki Miura, Toshiki Shibahara

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç¨€ç–è‡ªç¼–ç å™¨çš„å†…éƒ¨è¡¨å¾è§£å­¦ä¹ æ–¹æ³•ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯é—å¿˜æ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ä¿¡æ¯é—å¿˜` `å†…éƒ¨è¡¨å¾` `ç¨€ç–è‡ªç¼–ç å™¨` `éšç§ä¿æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMé—å¿˜æ–¹æ³•ä¸»è¦é€šè¿‡æŠ‘åˆ¶ä¸è‰¯è¾“å‡ºæ¥å®ç°ï¼Œæ— æ³•çœŸæ­£æ¶ˆé™¤æ¨¡å‹å†…éƒ¨çš„çŸ¥è¯†è¡¨å¾ï¼Œä¸”æ˜“å¯¼è‡´æ¨¡å‹å´©æºƒã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ–°é¢–çš„é—å¿˜æ–¹æ³•ï¼Œé€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨å°†é—å¿˜ç›®æ ‡çš„å†…éƒ¨æ¿€æ´»ä¸â€œæœªçŸ¥â€å®ä½“çš„æ¿€æ´»å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå¯¹é½å†…éƒ¨æ¿€æ´»ï¼Œå‡å°‘æ¨¡å‹å¯¹ç›®æ ‡çŸ¥è¯†çš„å›å¿†ï¼ŒåŒæ—¶é¿å…å¯¹éç›®æ ‡çŸ¥è¯†çš„æ˜¾è‘—æŸå®³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§åº”ç”¨ä¸­çš„æ—¥ç›Šæ™®åŠï¼Œéšç§å’Œç‰ˆæƒé—®é¢˜æ—¥ç›Šçªå‡ºï¼Œå¯¹æ›´æœ‰æ•ˆçš„LLMä¿¡æ¯é—å¿˜æŠ€æœ¯çš„éœ€æ±‚ä¹Ÿéšä¹‹å¢åŠ ã€‚è®¸å¤šç°æœ‰çš„é—å¿˜æ–¹æ³•æ—¨åœ¨é€šè¿‡é¢å¤–çš„è®­ç»ƒï¼ˆä¾‹å¦‚ï¼Œæ¢¯åº¦ä¸Šå‡ï¼‰æ¥æŠ‘åˆ¶ä¸è‰¯è¾“å‡ºï¼Œä»è€Œé™ä½ç”Ÿæˆæ­¤ç±»è¾“å‡ºçš„æ¦‚ç‡ã€‚è™½ç„¶è¿™ç§åŸºäºæŠ‘åˆ¶çš„æ–¹æ³•å¯ä»¥æ§åˆ¶æ¨¡å‹è¾“å‡ºï¼Œä½†å®ƒä»¬å¯èƒ½æ— æ³•æ¶ˆé™¤åµŒå…¥åœ¨æ¨¡å‹å†…éƒ¨æ¿€æ´»ä¸­çš„åº•å±‚çŸ¥è¯†ï¼›æŠ‘åˆ¶å“åº”å¹¶ä¸ç­‰åŒäºå¿˜è®°å®ƒã€‚æ­¤å¤–ï¼Œè¿™ç§åŸºäºæŠ‘åˆ¶çš„æ–¹æ³•é€šå¸¸ä¼šé­å—æ¨¡å‹å´©æºƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é—å¿˜æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥å¹²é¢„æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»ã€‚åœ¨æˆ‘ä»¬çš„å…¬å¼ä¸­ï¼Œé—å¿˜è¢«å®šä¹‰ä¸ºä¸€ç§çŠ¶æ€ï¼Œå…¶ä¸­é—å¿˜ç›®æ ‡çš„æ¿€æ´»ä¸â€œæœªçŸ¥â€å®ä½“çš„æ¿€æ´»æ— æ³•åŒºåˆ†ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªé—å¿˜ç›®æ ‡ï¼Œè¯¥ç›®æ ‡å°†ç›®æ ‡å®ä½“çš„æ¿€æ´»ä»å·²çŸ¥å®ä½“çš„æ¿€æ´»è½¬ç§»åˆ°ç¨€ç–è‡ªç¼–ç å™¨æ½œåœ¨ç©ºé—´ä¸­çš„æœªçŸ¥å®ä½“çš„æ¿€æ´»ã€‚é€šè¿‡å°†ç›®æ ‡çš„å†…éƒ¨æ¿€æ´»ä¸æœªçŸ¥å®ä½“çš„å†…éƒ¨æ¿€æ´»å¯¹é½ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å¯¹ç›®æ ‡å®ä½“çš„è¯†åˆ«ä»â€œå·²çŸ¥â€è½¬å˜ä¸ºâ€œæœªçŸ¥â€ï¼Œä»è€Œå®ç°çœŸæ­£çš„é—å¿˜ï¼ŒåŒæ—¶é¿å…è¿‡åº¦æŠ‘åˆ¶å’Œæ¨¡å‹å´©æºƒã€‚ç»éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å¯¹é½äº†é—å¿˜ç›®æ ‡çš„å†…éƒ¨æ¿€æ´»ï¼Œè¿™æ˜¯åŸºäºæŠ‘åˆ¶çš„æ–¹æ³•æ— æ³•å¯é å®ç°çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†æ¨¡å‹åœ¨é—®ç­”ä»»åŠ¡ä¸­å¯¹ç›®æ ‡çŸ¥è¯†çš„å›å¿†ï¼Œè€Œä¸ä¼šå¯¹éç›®æ ‡çŸ¥è¯†é€ æˆé‡å¤§æŸå®³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é—å¿˜æ–¹æ³•ï¼Œä¾‹å¦‚åŸºäºæ¢¯åº¦ä¸Šå‡çš„æŠ‘åˆ¶æ–¹æ³•ï¼Œä¸»è¦é€šè¿‡é™ä½æ¨¡å‹ç”Ÿæˆç‰¹å®šè¾“å‡ºçš„æ¦‚ç‡æ¥å®ç°â€œé—å¿˜â€ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¹¶ä¸èƒ½çœŸæ­£æ¶ˆé™¤æ¨¡å‹å†…éƒ¨çš„çŸ¥è¯†è¡¨å¾ï¼Œä»…ä»…æ˜¯æŠ‘åˆ¶äº†è¾“å‡ºï¼Œè€Œä¸”å®¹æ˜“å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ç”šè‡³å´©æºƒã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤ŸçœŸæ­£ä»æ¨¡å‹å†…éƒ¨æ¶ˆé™¤ç‰¹å®šçŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ•´ä½“æ€§èƒ½çš„é—å¿˜æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†â€œé—å¿˜â€å®šä¹‰ä¸ºä¸€ç§çŠ¶æ€ï¼Œå³æ¨¡å‹å¯¹é—å¿˜ç›®æ ‡çš„å†…éƒ¨æ¿€æ´»ä¸å¯¹â€œæœªçŸ¥â€å®ä½“çš„å†…éƒ¨æ¿€æ´»æ— æ³•åŒºåˆ†ã€‚é€šè¿‡å°†é—å¿˜ç›®æ ‡çš„å†…éƒ¨æ¿€æ´»å‘â€œæœªçŸ¥â€å®ä½“çš„æ¿€æ´»æ–¹å‘è°ƒæ•´ï¼Œä½¿æ¨¡å‹ä¸å†èƒ½å¤Ÿè¯†åˆ«æˆ–å›å¿†èµ·è¯¥ç›®æ ‡çš„ç›¸å…³çŸ¥è¯†ï¼Œä»è€Œå®ç°çœŸæ­£çš„é—å¿˜ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç®€å•æŠ‘åˆ¶è¾“å‡ºå¯èƒ½å¸¦æ¥çš„å‰¯ä½œç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1. ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨å­¦ä¹ LLMå†…éƒ¨æ¿€æ´»çš„æ½œåœ¨ç©ºé—´è¡¨å¾ã€‚2. ç¡®å®šéœ€è¦é—å¿˜çš„ç›®æ ‡å®ä½“ã€‚3. æ”¶é›†â€œå·²çŸ¥â€å®ä½“å’Œâ€œæœªçŸ¥â€å®ä½“çš„æ¿€æ´»æ•°æ®ã€‚4. å®šä¹‰ä¸€ä¸ªé—å¿˜ç›®æ ‡ï¼Œè¯¥ç›®æ ‡æ—¨åœ¨å°†é—å¿˜ç›®æ ‡çš„æ¿€æ´»åœ¨ç¨€ç–è‡ªç¼–ç å™¨æ½œåœ¨ç©ºé—´ä¸­ï¼Œä»â€œå·²çŸ¥â€å®ä½“æ¿€æ´»åŒºåŸŸç§»åŠ¨åˆ°â€œæœªçŸ¥â€å®ä½“æ¿€æ´»åŒºåŸŸã€‚5. é€šè¿‡ä¼˜åŒ–è¯¥é—å¿˜ç›®æ ‡ï¼Œè°ƒæ•´LLMçš„å‚æ•°ï¼Œä»è€Œå®ç°å¯¹ç›®æ ‡çŸ¥è¯†çš„é—å¿˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå…¶å¯¹â€œé—å¿˜â€çš„å®šä¹‰ï¼Œä»¥åŠé€šè¿‡ç›´æ¥å¹²é¢„æ¨¡å‹å†…éƒ¨æ¿€æ´»æ¥å®ç°é—å¿˜çš„æ€è·¯ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæŠ‘åˆ¶çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•æ—¨åœ¨æ¶ˆé™¤æ¨¡å‹å†…éƒ¨çš„çŸ¥è¯†è¡¨å¾ï¼Œè€Œä¸æ˜¯ä»…ä»…æŠ‘åˆ¶è¾“å‡ºã€‚æ­¤å¤–ï¼Œä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨å­¦ä¹ æ¿€æ´»çš„æ½œåœ¨ç©ºé—´è¡¨å¾ï¼Œæœ‰åŠ©äºæ›´æœ‰æ•ˆåœ°è°ƒæ•´æ¿€æ´»ï¼Œå¹¶é¿å…å¯¹æ¨¡å‹æ•´ä½“æ€§èƒ½é€ æˆè¿‡å¤§çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. ç¨€ç–è‡ªç¼–ç å™¨çš„ç»“æ„å’Œè®­ç»ƒæ–¹å¼ï¼Œéœ€è¦ä¿è¯èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ LLMå†…éƒ¨æ¿€æ´»çš„æ½œåœ¨ç©ºé—´è¡¨å¾ã€‚2. é—å¿˜ç›®æ ‡çš„å®šä¹‰ï¼Œéœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°å°†é—å¿˜ç›®æ ‡çš„æ¿€æ´»å‘â€œæœªçŸ¥â€å®ä½“æ¿€æ´»åŒºåŸŸç§»åŠ¨ï¼ŒåŒæ—¶é¿å…å¯¹å…¶ä»–çŸ¥è¯†é€ æˆæŸå®³ã€‚3. ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¼˜åŒ–é—å¿˜ç›®æ ‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹é½é—å¿˜ç›®æ ‡çš„å†…éƒ¨æ¿€æ´»ï¼Œä½¿å…¶ä¸â€œæœªçŸ¥â€å®ä½“çš„æ¿€æ´»æ›´åŠ æ¥è¿‘ï¼Œè€Œä¼ ç»Ÿçš„æŠ‘åˆ¶æ–¹æ³•éš¾ä»¥è¾¾åˆ°è¿™ä¸€æ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨é—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—é™ä½äº†æ¨¡å‹å¯¹ç›®æ ‡çŸ¥è¯†çš„å›å¿†ç‡ï¼ŒåŒæ—¶å¯¹éç›®æ ‡çŸ¥è¯†çš„å½±å“è¾ƒå°ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å®ç°æœ‰æ•ˆé—å¿˜çš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§æˆ–éµå®ˆç‰ˆæƒæ³•è§„çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šä»LLMä¸­ç§»é™¤ä¸ªäººèº«ä»½ä¿¡æ¯ã€æ¶ˆé™¤ä¸å½“æˆ–æœ‰å®³å†…å®¹ã€é˜²æ­¢æ¨¡å‹æ³„éœ²å•†ä¸šæœºå¯†ç­‰ã€‚é€šè¿‡è¯¥æ–¹æ³•ï¼Œå¯ä»¥æ›´å®‰å…¨ã€å¯é åœ°éƒ¨ç½²LLMï¼Œå¹¶é™ä½æ½œåœ¨çš„æ³•å¾‹å’Œä¼¦ç†é£é™©ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æˆä¸ºLLMå®‰å…¨å’Œåˆè§„çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) are increasingly deployed across various applications, privacy and copyright concerns have heightened the need for more effective LLM unlearning techniques. Many existing unlearning methods aim to suppress undesirable outputs through additional training (e.g., gradient ascent), which reduces the probability of generating such outputs. While such suppression-based approaches can control model outputs, they may not eliminate the underlying knowledge embedded in the model's internal activations; muting a response is not the same as forgetting it. Moreover, such suppression-based methods often suffer from model collapse. To address these issues, we propose a novel unlearning method that directly intervenes in the model's internal activations. In our formulation, forgetting is defined as a state in which the activation of a forgotten target is indistinguishable from that of ``unknown'' entities. Our method introduces an unlearning objective that modifies the activation of the target entity away from those of known entities and toward those of unknown entities in a sparse autoencoder latent space. By aligning the target's internal activation with those of unknown entities, we shift the model's recognition of the target entity from ``known'' to ``unknown'', achieving genuine forgetting while avoiding over-suppression and model collapse. Empirically, we show that our method effectively aligns the internal activations of the forgotten target, a result that the suppression-based approaches do not reliably achieve. Additionally, our method effectively reduces the model's recall of target knowledge in question-answering tasks without significant damage to the non-target knowledge.

