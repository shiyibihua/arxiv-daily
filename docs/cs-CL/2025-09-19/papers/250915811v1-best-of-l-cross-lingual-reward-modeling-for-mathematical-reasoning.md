---
layout: default
title: Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning
---

# Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15811" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15811v1</a>
  <a href="https://arxiv.org/pdf/2509.15811.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15811v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15811v1', 'Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sara Rajaee, Rochelle Choenni, Ekaterina Shutova, Christof Monz

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBest-of-Lè·¨è¯­è¨€å¥–åŠ±æ¨¡å‹ï¼Œæå‡å¤šè¯­è¨€LLMåœ¨æ•°å­¦æ¨ç†ä¸­çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨è¯­è¨€å­¦ä¹ ` `å¥–åŠ±æ¨¡å‹` `æ•°å­¦æ¨ç†` `å¤šè¯­è¨€LLM` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šè¯­è¨€LLMçš„æ¨ç†èƒ½åŠ›åœ¨ä¸åŒè¯­è¨€é—´å­˜åœ¨å·®å¼‚ï¼Œä¸”ä¸åŒè¯­è¨€çš„æ¨ç†è·¯å¾„å¯èƒ½äº’è¡¥ï¼Œä½†ç°æœ‰ç ”ç©¶å¯¹æ­¤å…³æ³¨ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºè·¨è¯­è¨€å¥–åŠ±æ¨¡å‹Best-of-Lï¼Œé€šè¿‡å¯¹è·¨è¯­è¨€ç”Ÿæˆçš„ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œé€‰å‡ºæœ€ä½³ç­”æ¡ˆï¼Œæå‡æ¨ç†æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—æå‡äº†æ•°å­¦æ¨ç†æ€§èƒ½ï¼Œå³ä½¿å¯¹äºé«˜èµ„æºè¯­è¨€å¦‚è‹±è¯­ï¼Œåœ¨ä½é‡‡æ ·é¢„ç®—ä¸‹ä¹Ÿèƒ½å—ç›Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†èƒ½åŠ›ä¸æ–­æå‡ï¼Œä½†å¤šè¯­è¨€LLMsåœ¨ä¸åŒè¯­è¨€ä¸­çš„æ¨ç†èƒ½åŠ›å·®å¼‚ä»¥åŠä¸åŒè¯­è¨€æ˜¯å¦èƒ½äº§ç”Ÿäº’è¡¥çš„æ¨ç†è·¯å¾„ä»ä¸æ˜ç¡®ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œç”¨äºå¯¹ç»™å®šé—®é¢˜çš„è·¨è¯­è¨€ç”Ÿæˆå“åº”è¿›è¡Œæ’åºã€‚ç»“æœè¡¨æ˜ï¼Œä¸åœ¨å•ä¸€è¯­è¨€å†…ä½¿ç”¨å¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è·¨è¯­è¨€å¥–åŠ±æ¨¡å‹æ˜¾è‘—æé«˜äº†æ•°å­¦æ¨ç†æ€§èƒ½ï¼Œç”šè‡³å¯¹é«˜èµ„æºè¯­è¨€ä¹Ÿæœ‰ç›Šå¤„ã€‚è™½ç„¶è‹±è¯­åœ¨å¤šè¯­è¨€æ¨¡å‹ä¸­é€šå¸¸è¡¨ç°å‡ºæœ€é«˜çš„æ€§èƒ½ï¼Œä½†æˆ‘ä»¬å‘ç°ï¼Œåœ¨ä½é‡‡æ ·é¢„ç®—ä¸‹ï¼Œè·¨è¯­è¨€é‡‡æ ·å°¤å…¶æœ‰åˆ©äºè‹±è¯­ã€‚æˆ‘ä»¬çš„å‘ç°æ­ç¤ºäº†é€šè¿‡åˆ©ç”¨ä¸åŒè¯­è¨€çš„äº’è¡¥ä¼˜åŠ¿æ¥æé«˜å¤šè¯­è¨€æ¨ç†çš„æ–°æœºä¼šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œä¸åŒè¯­è¨€ä¹‹é—´æ¨ç†èƒ½åŠ›å·®å¼‚çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨å•ä¸€è¯­è¨€å†…çš„å¥–åŠ±å»ºæ¨¡ï¼Œå¿½ç•¥äº†ä¸åŒè¯­è¨€å¯èƒ½æä¾›çš„äº’è¡¥ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è·¨è¯­è¨€çš„å¥–åŠ±å»ºæ¨¡ï¼Œå³è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿè¯„ä¼°ä¸åŒè¯­è¨€ç”Ÿæˆçš„ç­”æ¡ˆè´¨é‡çš„å¥–åŠ±æ¨¡å‹ã€‚é€šè¿‡å¯¹å¤šç§è¯­è¨€ç”Ÿæˆçš„ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œé€‰æ‹©æœ€ä½³ç­”æ¡ˆï¼Œä»è€Œæå‡æ•´ä½“çš„æ¨ç†æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå……åˆ†åˆ©ç”¨ä¸åŒè¯­è¨€çš„ä¼˜åŠ¿ï¼Œå¼¥è¡¥å•ä¸€è¯­è¨€æ¨ç†çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨å¤šè¯­è¨€LLMå¯¹ç»™å®šçš„æ•°å­¦é—®é¢˜ç”Ÿæˆå¤šç§è¯­è¨€çš„ç­”æ¡ˆï¼›2) ä½¿ç”¨è·¨è¯­è¨€å¥–åŠ±æ¨¡å‹å¯¹è¿™äº›ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œè¯„ä¼°å…¶è´¨é‡ï¼›3) é€‰æ‹©å¥–åŠ±æœ€é«˜çš„ç­”æ¡ˆä½œä¸ºæœ€ç»ˆç»“æœã€‚è¯¥æ¡†æ¶çš„å…³é”®åœ¨äºè·¨è¯­è¨€å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒï¼Œè¯¥æ¨¡å‹éœ€è¦èƒ½å¤Ÿç†è§£å’Œæ¯”è¾ƒä¸åŒè¯­è¨€çš„ç­”æ¡ˆï¼Œå¹¶ç»™å‡ºåˆç†çš„è¯„åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºè·¨è¯­è¨€å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•ã€‚è¯¥æ¨¡å‹ä¸ä»…éœ€è¦å­¦ä¹ è¯„ä¼°å•ä¸€è¯­è¨€ç­”æ¡ˆçš„è´¨é‡ï¼Œè¿˜éœ€è¦å­¦ä¹ å¦‚ä½•æ¯”è¾ƒå’Œæ’åºä¸åŒè¯­è¨€çš„ç­”æ¡ˆã€‚è¿™éœ€è¦æ¨¡å‹å…·å¤‡ä¸€å®šçš„è·¨è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¯†åˆ«ä¸åŒè¯­è¨€è¡¨è¾¾çš„ç›¸åŒå«ä¹‰ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å¯èƒ½æ¶‰åŠçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¥–åŠ±æ¨¡å‹çš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚ä½¿ç”¨Transformeræ¶æ„ï¼›2) è®­ç»ƒæ•°æ®çš„æ„å»ºæ–¹å¼ï¼Œä¾‹å¦‚ä½¿ç”¨äººå·¥æ ‡æ³¨æˆ–è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®ï¼›3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œä¾‹å¦‚ä½¿ç”¨æ’åºæŸå¤±æˆ–å›å½’æŸå¤±ï¼›4) é‡‡æ ·ç­–ç•¥ï¼Œä¾‹å¦‚åœ¨ä½é‡‡æ ·é¢„ç®—ä¸‹å¦‚ä½•é€‰æ‹©æ›´å…·ä»£è¡¨æ€§çš„è¯­è¨€è¿›è¡Œé‡‡æ ·ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è·¨è¯­è¨€å¥–åŠ±æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†å•ä¸€è¯­è¨€å¥–åŠ±æ¨¡å‹ã€‚å°¤å…¶æ˜¯åœ¨ä½é‡‡æ ·é¢„ç®—ä¸‹ï¼Œè‹±è¯­çš„æ€§èƒ½ä¹Ÿå¾—åˆ°äº†æå‡ï¼Œè¡¨æ˜è·¨è¯­è¨€é‡‡æ ·èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ä¸åŒè¯­è¨€çš„äº’è¡¥ä¼˜åŠ¿ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šè¯­è¨€æ™ºèƒ½åŠ©æ‰‹ã€è·¨è¯­è¨€æ•™è‚²å¹³å°ã€ä»¥åŠéœ€è¦å¤„ç†å¤šè¯­è¨€ä¿¡æ¯çš„é‡‘èã€æ³•å¾‹ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤šè¯­è¨€LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æœåŠ¡äºå…¨çƒç”¨æˆ·ï¼Œä¿ƒè¿›è·¨æ–‡åŒ–äº¤æµä¸åˆä½œï¼Œå¹¶ä¸ºè§£å†³å¤æ‚é—®é¢˜æä¾›æ›´å…¨é¢çš„è§†è§’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While the reasoning abilities of large language models (LLMs) continue to advance, it remains unclear how such ability varies across languages in multilingual LLMs and whether different languages produce reasoning paths that complement each other. To investigate this question, we train a reward model to rank generated responses for a given question across languages. Our results show that our cross-lingual reward model substantially improves mathematical reasoning performance compared to using reward modeling within a single language, benefiting even high-resource languages. While English often exhibits the highest performance in multilingual models, we find that cross-lingual sampling particularly benefits English under low sampling budgets. Our findings reveal new opportunities to improve multilingual reasoning by leveraging the complementary strengths of diverse languages.

