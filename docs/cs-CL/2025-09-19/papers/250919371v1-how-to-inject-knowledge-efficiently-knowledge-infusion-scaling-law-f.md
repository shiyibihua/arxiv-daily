---
layout: default
title: How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models
---

# How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19371" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19371v1</a>
  <a href="https://arxiv.org/pdf/2509.19371.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19371v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19371v1', 'How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kangtao Lv, Haibin Chen, Yujin Yuan, Langming Liu, Shilei Liu, Yongwei Wang, Wenbo Su, Bo Zheng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçŸ¥è¯†æ³¨å…¥ç¼©æ”¾å¾‹ï¼Œé«˜æ•ˆæŒ‡å¯¼å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„é¢†åŸŸçŸ¥è¯†æ³¨å…¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é¢„è®­ç»ƒ` `çŸ¥è¯†æ³¨å…¥` `ç¼©æ”¾å¾‹` `é¢†åŸŸçŸ¥è¯†` `ç¾éš¾æ€§é—å¿˜` `çŸ¥è¯†å´©æºƒç‚¹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ç¼ºä¹é¢†åŸŸçŸ¥è¯†ä¼˜åŒ–ï¼Œåœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°ä¸ä½³ï¼Œç”šè‡³äº§ç”Ÿå¹»è§‰ã€‚
2. è®ºæ–‡æå‡ºçŸ¥è¯†æ³¨å…¥ç¼©æ”¾å¾‹ï¼Œé€šè¿‡åˆ†æå°æ¨¡å‹æ¥é¢„æµ‹å¤§æ¨¡å‹æœ€ä½³é¢†åŸŸçŸ¥è¯†æ³¨å…¥é‡ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜ã€‚
3. å®éªŒéªŒè¯äº†è¯¥ç¼©æ”¾å¾‹çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ¨¡å‹å¤§å°å’Œé¢„è®­ç»ƒtokené¢„ç®—ä¸‹å·¥ä½œã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºçš„å“è¶Šé€šç”¨èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ä¼˜åŒ–æ—¶ï¼Œå®ƒä»¬åœ¨ä¸“ä¸šçŸ¥è¯†åŸºå‡†æµ‹è¯•ä¸­é€šå¸¸è¡¨ç°ä¸ä½³ï¼Œç”šè‡³äº§ç”Ÿå¹»è§‰ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨é¢„è®­ç»ƒæœŸé—´ç­–ç•¥æ€§åœ°æ³¨å…¥é¢†åŸŸçŸ¥è¯†å¯ä»¥æ˜¾è‘—æé«˜ä¸‹æ¸¸æ€§èƒ½ã€‚ä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜åœ¨äºå¹³è¡¡è¿™ç§æ³¨å…¥çš„æƒè¡¡ï¼šæ³¨å…¥è¿‡å°‘çš„é¢†åŸŸç‰¹å®šæ•°æ®ä¼šå¯¼è‡´ä¸“ä¸šåŒ–ä¸è¶³ï¼Œè€Œè¿‡åº¦æ³¨å…¥ä¼šå¼•å‘å¯¹å…ˆå‰è·å¾—çŸ¥è¯†çš„ç¾éš¾æ€§é—å¿˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å…³æ³¨ç”±è¿‡åº¦æ³¨å…¥å¼•èµ·çš„è®°å¿†å´©æºƒç°è±¡ã€‚é€šè¿‡ç³»ç»Ÿçš„å®éªŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®è§‚å¯Ÿç»“æœï¼Œå³1ï¼‰ä¸´ç•Œå´©æºƒç‚¹ï¼šæ¯ä¸ªæ¨¡å‹éƒ½è¡¨ç°å‡ºä¸€ä¸ªé˜ˆå€¼ï¼Œè¶…è¿‡è¯¥é˜ˆå€¼å…¶çŸ¥è¯†ä¿ç•™èƒ½åŠ›ä¼šæ€¥å‰§ä¸‹é™ï¼›2ï¼‰è§„æ¨¡ç›¸å…³æ€§ï¼šè¿™äº›å´©æºƒç‚¹ä¸æ¨¡å‹çš„å¤§å°ä¸€è‡´åœ°ç¼©æ”¾ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çŸ¥è¯†æ³¨å…¥ç¼©æ”¾å¾‹ï¼Œé€šè¿‡åˆ†æè¾ƒå°çš„æ¨¡å‹æ¥é¢„æµ‹è¦æ³¨å…¥åˆ°å¤§å‹LLMä¸­çš„æœ€ä½³é¢†åŸŸçŸ¥è¯†é‡ã€‚è·¨ä¸åŒæ¨¡å‹å¤§å°å’Œé¢„è®­ç»ƒtokené¢„ç®—çš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ç¼©æ”¾å¾‹çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçŸ¥è¯†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç›´æ¥åº”ç”¨ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ç”šè‡³äº§ç”Ÿå¹»è§‰ã€‚ä¸ºäº†æå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ï¼Œéœ€è¦åœ¨é¢„è®­ç»ƒé˜¶æ®µæ³¨å…¥é¢†åŸŸçŸ¥è¯†ã€‚ç„¶è€Œï¼Œå¦‚ä½•ç¡®å®šæœ€ä½³çš„çŸ¥è¯†æ³¨å…¥é‡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ³¨å…¥è¿‡å°‘æ— æ³•æœ‰æ•ˆæå‡æ€§èƒ½ï¼Œè€Œæ³¨å…¥è¿‡å¤šåˆ™ä¼šå¯¼è‡´æ¨¡å‹é—å¿˜å…ˆå‰å­¦ä¹ åˆ°çš„é€šç”¨çŸ¥è¯†ï¼Œå³å‘ç”Ÿç¾éš¾æ€§é—å¿˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å‘ç°çŸ¥è¯†æ³¨å…¥é‡ä¸æ¨¡å‹è§„æ¨¡ä¹‹é—´çš„å…³ç³»ï¼Œå³çŸ¥è¯†æ³¨å…¥ç¼©æ”¾å¾‹ã€‚é€šè¿‡è§‚å¯Ÿå°æ¨¡å‹çš„çŸ¥è¯†å´©æºƒç‚¹ï¼Œå¹¶å°†å…¶ä¸æ¨¡å‹è§„æ¨¡è”ç³»èµ·æ¥ï¼Œä»è€Œé¢„æµ‹å¤§æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µåº”è¯¥æ³¨å…¥çš„æœ€ä½³é¢†åŸŸçŸ¥è¯†é‡ã€‚è¿™ç§æ–¹æ³•é¿å…äº†åœ¨å¤§æ¨¡å‹ä¸Šè¿›è¡Œå¤§é‡çš„å®éªŒæœç´¢ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1ï¼‰ç³»ç»Ÿæ€§åœ°åœ¨ä¸åŒå¤§å°çš„æ¨¡å‹ä¸Šè¿›è¡Œå®éªŒï¼Œæ¢ç´¢çŸ¥è¯†æ³¨å…¥é‡ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼›2ï¼‰è§‚å¯Ÿå¹¶è®°å½•æ¯ä¸ªæ¨¡å‹çš„çŸ¥è¯†å´©æºƒç‚¹ï¼Œå³æ¨¡å‹æ€§èƒ½å¼€å§‹æ˜¾è‘—ä¸‹é™çš„çŸ¥è¯†æ³¨å…¥é‡ï¼›3ï¼‰åˆ†æçŸ¥è¯†å´©æºƒç‚¹ä¸æ¨¡å‹è§„æ¨¡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå»ºç«‹çŸ¥è¯†æ³¨å…¥ç¼©æ”¾å¾‹ï¼›4ï¼‰åœ¨å¤§æ¨¡å‹ä¸ŠéªŒè¯è¯¥ç¼©æ”¾å¾‹çš„æœ‰æ•ˆæ€§ï¼Œå³ä½¿ç”¨è¯¥ç¼©æ”¾å¾‹é¢„æµ‹çš„çŸ¥è¯†æ³¨å…¥é‡è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶è¯„ä¼°æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†çŸ¥è¯†æ³¨å…¥ç¼©æ”¾å¾‹ã€‚è¯¥ç¼©æ”¾å¾‹èƒ½å¤Ÿæ ¹æ®æ¨¡å‹è§„æ¨¡é¢„æµ‹æœ€ä½³çš„é¢†åŸŸçŸ¥è¯†æ³¨å…¥é‡ï¼Œä»è€Œé¿å…äº†åœ¨å¤§æ¨¡å‹ä¸Šè¿›è¡Œå¤§é‡çš„å®éªŒæœç´¢ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å‘ç°äº†çŸ¥è¯†å´©æºƒç‚¹è¿™ä¸€ç°è±¡ï¼Œå¹¶å°†å…¶ä¸æ¨¡å‹è§„æ¨¡è”ç³»èµ·æ¥ï¼Œä¸ºç†è§£å¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å­¦ä¹ å’Œé—å¿˜æœºåˆ¶æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨ä¸åŒå¤§å°çš„æ¨¡å‹è¿›è¡Œå®éªŒï¼Œä»¥è§‚å¯ŸçŸ¥è¯†æ³¨å…¥é‡ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼›2ï¼‰å®šä¹‰äº†çŸ¥è¯†å´©æºƒç‚¹ï¼Œå¹¶ä½¿ç”¨ç‰¹å®šçš„æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹çš„çŸ¥è¯†ä¿ç•™èƒ½åŠ›ï¼›3ï¼‰ä½¿ç”¨å›å½’åˆ†æç­‰æ–¹æ³•æ¥å»ºç«‹çŸ¥è¯†å´©æºƒç‚¹ä¸æ¨¡å‹è§„æ¨¡ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå¾—åˆ°çŸ¥è¯†æ³¨å…¥ç¼©æ”¾å¾‹ï¼›4ï¼‰åœ¨ä¸åŒé¢†åŸŸå’Œæ•°æ®é›†ä¸ŠéªŒè¯è¯¥ç¼©æ”¾å¾‹çš„æ³›åŒ–æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç ”ç©¶é€šè¿‡å®éªŒå‘ç°ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½å­˜åœ¨ä¸€ä¸ªçŸ¥è¯†å´©æºƒç‚¹ï¼Œè¶…è¿‡è¯¥ç‚¹å…¶çŸ¥è¯†ä¿ç•™èƒ½åŠ›ä¼šæ€¥å‰§ä¸‹é™ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™äº›å´©æºƒç‚¹ä¸æ¨¡å‹çš„å¤§å°å‘ˆç°ä¸€è‡´çš„ç¼©æ”¾å…³ç³»ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡æå‡ºçš„çŸ¥è¯†æ³¨å…¥ç¼©æ”¾å¾‹èƒ½å¤Ÿæœ‰æ•ˆåœ°é¢„æµ‹å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­æœ€ä½³çš„é¢†åŸŸçŸ¥è¯†æ³¨å…¥é‡ï¼Œå¹¶åœ¨ä¸åŒæ¨¡å‹å¤§å°å’Œé¢„è®­ç»ƒtokené¢„ç®—ä¸‹å¾—åˆ°éªŒè¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é¢†åŸŸçŸ¥è¯†å¢å¼ºçš„å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒåœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—ã€é‡‘èã€æ³•å¾‹ç­‰é¢†åŸŸã€‚é€šè¿‡çŸ¥è¯†æ³¨å…¥ç¼©æ”¾å¾‹ï¼Œå¯ä»¥é«˜æ•ˆåœ°å°†é¢†åŸŸçŸ¥è¯†æ³¨å…¥åˆ°å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œæå‡å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ï¼Œå¹¶å‡å°‘ç¾éš¾æ€§é—å¿˜çš„é£é™©ã€‚è¿™æœ‰åŠ©äºæ„å»ºæ›´åŠ ä¸“ä¸šåŒ–å’Œå¯é çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæ›´å¥½åœ°æœåŠ¡äºå„ä¸ªè¡Œä¸šã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.

