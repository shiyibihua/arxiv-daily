---
layout: default
title: Distribution-Aligned Decoding for Efficient LLM Task Adaptation
---

# Distribution-Aligned Decoding for Efficient LLM Task Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15888" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15888v3</a>
  <a href="https://arxiv.org/pdf/2509.15888.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15888v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15888v3', 'Distribution-Aligned Decoding for Efficient LLM Task Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Senkang Hu, Xudong Han, Jinqi Jiang, Yihang Tao, Zihan Fang, Yong Dai, Sam Tak Wu Kwong, Yuguang Fang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19 (æ›´æ–°: 2025-10-12)

**å¤‡æ³¨**: Accepted by NeurIPS'25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSVDecodeï¼Œé€šè¿‡åˆ†å¸ƒå¯¹é½è§£ç é«˜æ•ˆé€‚åº”LLMä¸‹æ¸¸ä»»åŠ¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `åˆ†å¸ƒå¯¹é½` `è§£ç å¼•å¯¼` `è¿ç§»å­¦ä¹ ` `ä»»åŠ¡é€‚é…` `Steering Vector`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰PEFTæ–¹æ³•åœ¨LLMä»»åŠ¡é€‚é…ä¸­ä»é¢ä¸´æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„æ–¹æ¡ˆã€‚
2. SVDecodeé€šè¿‡æå–steering vectorï¼Œåœ¨è§£ç é˜¶æ®µç›´æ¥å¯¹é½è¾“å‡ºåˆ†å¸ƒå’Œä»»åŠ¡åˆ†å¸ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSVDecodeèƒ½æ˜¾è‘—æå‡å¤šé¡¹é€‰æ‹©é¢˜å‡†ç¡®ç‡å’Œå¼€æ”¾å¼é—®é¢˜çœŸå®æ€§ï¼Œä¸”æ— éœ€é¢å¤–å¯è®­ç»ƒå‚æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å³ä½¿é‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ï¼Œå°†æ•°åäº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹é€‚é…åˆ°ä¸‹æ¸¸ä»»åŠ¡ä»ç„¶æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡å°†ä»»åŠ¡é€‚é…é‡æ–°å®šä¹‰ä¸ºè¾“å‡ºåˆ†å¸ƒå¯¹é½ï¼šç›®æ ‡æ˜¯åœ¨è§£ç è¿‡ç¨‹ä¸­ç›´æ¥å°†è¾“å‡ºåˆ†å¸ƒå¼•å¯¼è‡³ä»»åŠ¡åˆ†å¸ƒï¼Œè€Œä¸æ˜¯é€šè¿‡æƒé‡æ›´æ–°é—´æ¥å®ç°ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Steering Vector Decoding (SVDecode)ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§ã€å…¼å®¹PEFTä¸”å…·æœ‰ç†è®ºåŸºç¡€çš„æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œä¸€ä¸ªç®€çŸ­çš„é¢„çƒ­å¾®è°ƒï¼Œå¹¶ä»é¢„çƒ­å¾®è°ƒæ¨¡å‹å’Œé¢„è®­ç»ƒæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„Kullback-Leibler (KL)æ•£åº¦æ¢¯åº¦ä¸­æå–ä»»åŠ¡ç›¸å…³çš„steering vectorã€‚ç„¶åï¼Œè¯¥steering vectorç”¨äºæŒ‡å¯¼è§£ç è¿‡ç¨‹ï¼Œä»¥å°†æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒå¼•å¯¼è‡³ä»»åŠ¡åˆ†å¸ƒã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†SVDecodeä¸å®Œå…¨å¾®è°ƒçš„æ¢¯åº¦æ­¥é•¿ä¸€é˜¶ç­‰ä»·ï¼Œå¹¶æ¨å¯¼å‡ºäº†steering vectorå¼ºåº¦çš„å…¨å±€æœ€ä¼˜è§£ã€‚åœ¨ä¸‰ä¸ªä»»åŠ¡å’Œä¹ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSVDecodeä¸å››ç§æ ‡å‡†PEFTæ–¹æ³•ç›¸ç»“åˆï¼Œå°†å¤šé¡¹é€‰æ‹©é¢˜çš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾5ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶å°†å¼€æ”¾å¼é—®é¢˜çš„çœŸå®æ€§æé«˜äº†2ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨å¸¸è¯†æ•°æ®é›†ä¸Šè·å¾—äº†ç±»ä¼¼çš„æ”¶ç›Šï¼ˆ1-2ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œè€Œæ— éœ€åœ¨PEFTé€‚é…å™¨ä¹‹å¤–æ·»åŠ å¯è®­ç»ƒå‚æ•°ã€‚å› æ­¤ï¼ŒSVDecodeä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†æ›´å¼ºå¤§çš„ä»»åŠ¡é€‚é…çš„è½»é‡çº§ã€ç†è®ºåŸºç¡€åšå®çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„é«˜æ•ˆé€‚é…é—®é¢˜ã€‚å³ä½¿ä½¿ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå¾®è°ƒæ•°åäº¿å‚æ•°çš„LLMä»ç„¶éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œå®ƒä»¬é€šå¸¸é€šè¿‡æ›´æ–°æ¨¡å‹æƒé‡æ¥é—´æ¥è°ƒæ•´è¾“å‡ºåˆ†å¸ƒï¼Œæ•ˆç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä»»åŠ¡é€‚é…é—®é¢˜è½¬åŒ–ä¸ºè¾“å‡ºåˆ†å¸ƒå¯¹é½é—®é¢˜ã€‚é€šè¿‡ç›´æ¥åœ¨è§£ç é˜¶æ®µå¼•å¯¼æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒå‘ç›®æ ‡ä»»åŠ¡çš„åˆ†å¸ƒé æ‹¢ï¼Œé¿å…äº†é€šè¿‡æƒé‡æ›´æ–°çš„é—´æ¥è°ƒæ•´ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æ›´ç›´æ¥ã€æ›´é«˜æ•ˆåœ°å®ç°ä»»åŠ¡é€‚é…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSVDecodeæ–¹æ³•åŒ…å«ä»¥ä¸‹ä¸»è¦é˜¶æ®µï¼š1) **é¢„çƒ­å¾®è°ƒï¼ˆWarm-start Fine-tuneï¼‰**ï¼šä½¿ç”¨å°‘é‡æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¿«é€Ÿå¾®è°ƒï¼Œä½¿å…¶åˆæ­¥é€‚åº”ç›®æ ‡ä»»åŠ¡ã€‚2) **Steering Vectoræå–**ï¼šè®¡ç®—é¢„çƒ­å¾®è°ƒæ¨¡å‹å’Œé¢„è®­ç»ƒæ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦æ¢¯åº¦ï¼Œå¹¶ä»ä¸­æå–ä»»åŠ¡ç›¸å…³çš„steering vectorã€‚3) **è§£ç å¼•å¯¼**ï¼šåœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨æå–çš„steering vectorå¼•å¯¼æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒå‘ç›®æ ‡ä»»åŠ¡åˆ†å¸ƒé æ‹¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šSVDecodeçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç›´æ¥å¯¹é½è¾“å‡ºåˆ†å¸ƒçš„æ€è·¯ï¼Œä»¥åŠsteering vectorçš„æå–å’Œåº”ç”¨æ–¹å¼ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒSVDecodeé¿å…äº†å¯¹æ¨¡å‹æƒé‡çš„ç›´æ¥ä¿®æ”¹ï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ä»ç†è®ºä¸Šè¯æ˜äº†SVDecodeä¸å®Œå…¨å¾®è°ƒçš„ä¸€é˜¶ç­‰ä»·æ€§ï¼Œå¹¶æ¨å¯¼å‡ºäº†steering vectorå¼ºåº¦çš„å…¨å±€æœ€ä¼˜è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨KLæ•£åº¦æ¢¯åº¦ä½œä¸ºsteering vectorçš„æ¥æºï¼Œä»¥æ•æ‰é¢„è®­ç»ƒæ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚2) æ¨å¯¼steering vectorå¼ºåº¦çš„å…¨å±€æœ€ä¼˜è§£ï¼Œä»¥ç¡®ä¿è§£ç è¿‡ç¨‹çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚3) å°†SVDecodeä¸ç°æœ‰çš„PEFTæ–¹æ³•ç›¸ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥æé«˜ä»»åŠ¡é€‚é…çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSVDecodeä¸å››ç§æ ‡å‡†PEFTæ–¹æ³•ç›¸ç»“åˆï¼Œåœ¨ä¸‰ä¸ªä»»åŠ¡å’Œä¹ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå°†å¤šé¡¹é€‰æ‹©é¢˜çš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾5ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶å°†å¼€æ”¾å¼é—®é¢˜çš„çœŸå®æ€§æé«˜äº†2ä¸ªç™¾åˆ†ç‚¹ã€‚åœ¨å¸¸è¯†æ•°æ®é›†ä¸Šï¼ŒSVDecodeä¹Ÿè·å¾—äº†1-2ä¸ªç™¾åˆ†ç‚¹çš„æå‡ï¼Œä¸”æ— éœ€åœ¨PEFTé€‚é…å™¨ä¹‹å¤–æ·»åŠ å¯è®­ç»ƒå‚æ•°ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSVDecodeæ˜¯ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„LLMä»»åŠ¡é€‚é…æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SVDecodeå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå„ç§éœ€è¦å¿«é€Ÿã€é«˜æ•ˆåœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹é€‚é…åˆ°ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„åœºæ™¯ï¼Œå¦‚é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒï¼Œä¾‹å¦‚è¾¹ç¼˜è®¡ç®—è®¾å¤‡æˆ–ç§»åŠ¨è®¾å¤‡ã€‚æ­¤å¤–ï¼ŒSVDecodeè¿˜å¯ä»¥ç”¨äºä¸ªæ€§åŒ–è¯­è¨€æ¨¡å‹ï¼Œæ ¹æ®ç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚è°ƒæ•´æ¨¡å‹çš„è¾“å‡ºé£æ ¼å’Œå†…å®¹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Adapting billion-parameter language models to a downstream task is still costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task adaptation as output-distribution alignment: the objective is to steer the output distribution toward the task distribution directly during decoding rather than indirectly through weight updates. Building on this view, we introduce Steering Vector Decoding (SVDecode), a lightweight, PEFT-compatible, and theoretically grounded method. We start with a short warm-start fine-tune and extract a task-aware steering vector from the Kullback-Leibler (KL) divergence gradient between the output distribution of the warm-started and pre-trained models. This steering vector is then used to guide the decoding process to steer the model's output distribution towards the task distribution. We theoretically prove that SVDecode is first-order equivalent to the gradient step of full fine-tuning and derive a globally optimal solution for the strength of the steering vector. Across three tasks and nine benchmarks, SVDecode paired with four standard PEFT methods improves multiple-choice accuracy by up to 5 percentage points and open-ended truthfulness by 2 percentage points, with similar gains (1-2 percentage points) on commonsense datasets without adding trainable parameters beyond the PEFT adapter. SVDecode thus offers a lightweight, theoretically grounded path to stronger task adaptation for large language models.

