---
layout: default
title: UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression
---

# UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15763" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15763v1</a>
  <a href="https://arxiv.org/pdf/2509.15763.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15763v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15763v1', 'UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenlong Deng, Zhisong Zhang, Kelong Mao, Shuaiyi Li, Tianqing Fang, Hongming Zhang, Haitao Mi, Dong Yu, Zhicheng Dou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: 15 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniGistï¼šé¢å‘é€šç”¨å’Œç¡¬ä»¶å¯¹é½çš„åºåˆ—çº§é•¿ä¸Šä¸‹æ–‡å‹ç¼©æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡å‹ç¼©` `åºåˆ—çº§å‹ç¼©` `é”®å€¼ç¼“å­˜` `å¤§å‹è¯­è¨€æ¨¡å‹` `gist token` `GPUä¼˜åŒ–` `é•¿ç¨‹ä¾èµ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿æ–‡æœ¬å¤„ç†å¯¹LLMçš„KVç¼“å­˜é€ æˆå·¨å¤§å‹åŠ›ï¼Œåºåˆ—çº§å‹ç¼©ä¼šæŸå¤±é‡è¦ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
2. UniGisté€šè¿‡å¼•å…¥gist tokenæ›¿æ¢åŸå§‹tokenï¼Œç»†ç²’åº¦åœ°ä¿ç•™ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
3. UniGisté‡‡ç”¨æ— chunkè®­ç»ƒç­–ç•¥å’Œgistç§»ä½æŠ€å·§ï¼Œä¼˜åŒ–GPUè®­ç»ƒå¹¶æ”¯æŒçµæ´»æ¨ç†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿ä¸Šä¸‹æ–‡è¾“å…¥çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œä½†é”®å€¼(KV)ç¼“å­˜çš„å†…å­˜å¼€é”€ä»ç„¶æ˜¯é€šç”¨éƒ¨ç½²çš„ä¸»è¦ç“¶é¢ˆã€‚å°½ç®¡å·²ç»æ¢ç´¢äº†å„ç§å‹ç¼©ç­–ç•¥ï¼Œä½†åºåˆ—çº§å‹ç¼©ï¼ˆå³åˆ é™¤æŸäº›tokençš„å®Œæ•´KVç¼“å­˜ï¼‰å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒå¯èƒ½å¯¼è‡´é‡è¦ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä¸¢å¤±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniGistï¼Œè¿™æ˜¯ä¸€ä¸ªåºåˆ—çº§é•¿ä¸Šä¸‹æ–‡å‹ç¼©æ¡†æ¶ï¼Œé€šè¿‡ä»¥ç»†ç²’åº¦çš„æ–¹å¼ç”¨ç‰¹æ®Šçš„å‹ç¼©tokenï¼ˆgistï¼‰æ›¿æ¢åŸå§‹tokenï¼Œä»è€Œæœ‰æ•ˆåœ°ä¿ç•™ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ— chunkçš„è®­ç»ƒç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå¸¦æœ‰gistç§»ä½æŠ€å·§çš„é«˜æ•ˆå†…æ ¸ï¼Œä»è€Œå®ç°äº†ä¼˜åŒ–çš„GPUè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆè¿˜æ”¯æŒçµæ´»çš„æ¨ç†ï¼Œå…è®¸å®é™…åˆ é™¤å‹ç¼©çš„tokenï¼Œä»è€Œå®ç°å®æ—¶çš„å†…å­˜èŠ‚çœã€‚åœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUniGistæ˜¾è‘—æé«˜äº†å‹ç¼©è´¨é‡ï¼Œå°¤å…¶æ˜¯åœ¨ç»†èŠ‚å›å¿†ä»»åŠ¡å’Œé•¿ç¨‹ä¾èµ–å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ï¼Œå…¶KVç¼“å­˜ä¼šæ¶ˆè€—å¤§é‡å†…å­˜ï¼Œæˆä¸ºéƒ¨ç½²çš„ç“¶é¢ˆã€‚åºåˆ—çº§å‹ç¼©è™½ç„¶å¯ä»¥å‡å°‘å†…å­˜å ç”¨ï¼Œä½†ç›´æ¥ä¸¢å¼ƒæŸäº›tokençš„KVç¼“å­˜ä¼šå¯¼è‡´å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä¸¢å¤±ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨å‹ç¼©KVç¼“å­˜çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä¿ç•™é‡è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniGistçš„æ ¸å¿ƒæ€è·¯æ˜¯ç”¨ç‰¹æ®Šçš„å‹ç¼©tokenï¼ˆgistï¼‰æ¥æ›¿æ¢åŸå§‹tokenï¼Œè€Œä¸æ˜¯ç›´æ¥åˆ é™¤ã€‚è¿™äº›gist tokenåŒ…å«äº†åŸå§‹tokençš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œåœ¨å‹ç¼©çš„åŒæ—¶ä¿ç•™äº†é‡è¦çš„è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡ç»†ç²’åº¦çš„æ›¿æ¢ç­–ç•¥ï¼ŒUniGistå¯ä»¥æ›´ç²¾ç¡®åœ°æ§åˆ¶ä¿¡æ¯çš„æŸå¤±ï¼Œæé«˜å‹ç¼©è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniGistçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªè®­ç»ƒé˜¶æ®µå’Œä¸€ä¸ªæ¨ç†é˜¶æ®µã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ å¦‚ä½•å°†åŸå§‹tokenæ›¿æ¢ä¸ºgist tokenï¼Œå¹¶ä¼˜åŒ–gist tokençš„è¡¨ç¤ºï¼Œä½¿å…¶èƒ½å¤Ÿå°½å¯èƒ½åœ°ä¿ç•™åŸå§‹tokençš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå¯ä»¥é€‰æ‹©ç›´æ¥åˆ é™¤gist tokenä»¥èŠ‚çœå†…å­˜ï¼Œæˆ–è€…ä¿ç•™gist tokenä»¥æé«˜æ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ— chunkçš„è®­ç»ƒç­–ç•¥ï¼Œé¿å…äº†å¯¹è¾“å…¥åºåˆ—è¿›è¡Œåˆ†å—å¤„ç†ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šUniGistçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†gist tokençš„æ¦‚å¿µï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ç»†ç²’åº¦çš„æ›¿æ¢ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„åºåˆ—çº§å‹ç¼©æ–¹æ³•ç›¸æ¯”ï¼ŒUniGistä¸æ˜¯ç®€å•åœ°åˆ é™¤tokenï¼Œè€Œæ˜¯ç”¨åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„gist tokenæ¥ä»£æ›¿ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™äº†è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒUniGistè¿˜è®¾è®¡äº†ä¸€ä¸ªå¸¦æœ‰gistç§»ä½æŠ€å·§çš„é«˜æ•ˆå†…æ ¸ï¼Œä¼˜åŒ–äº†GPUè®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šUniGistçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) Gist tokençš„è¡¨ç¤ºå­¦ä¹ ï¼šé€šè¿‡è®­ç»ƒæ¨¡å‹å­¦ä¹ å¦‚ä½•å°†åŸå§‹tokenæ˜ å°„åˆ°gist tokenï¼Œå¹¶ä¼˜åŒ–gist tokençš„è¡¨ç¤ºï¼Œä½¿å…¶èƒ½å¤Ÿå°½å¯èƒ½åœ°ä¿ç•™åŸå§‹tokençš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚2) ç»†ç²’åº¦çš„æ›¿æ¢ç­–ç•¥ï¼šæ ¹æ®tokençš„é‡è¦æ€§ï¼Œé€‰æ‹©æ€§åœ°å°†åŸå§‹tokenæ›¿æ¢ä¸ºgist tokenï¼Œä»è€Œæ›´å¥½åœ°æ§åˆ¶ä¿¡æ¯çš„æŸå¤±ã€‚3) Gistç§»ä½æŠ€å·§ï¼šé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹gist tokenè¿›è¡Œç§»ä½æ“ä½œï¼Œå¢å¼ºæ¨¡å‹å¯¹é•¿ç¨‹ä¾èµ–å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ã€‚4) æ— chunkè®­ç»ƒç­–ç•¥ï¼šé¿å…å¯¹è¾“å…¥åºåˆ—è¿›è¡Œåˆ†å—å¤„ç†ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒUniGiståœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†å‹ç¼©è´¨é‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»†èŠ‚å›å¿†ä»»åŠ¡ä¸­ï¼ŒUniGistçš„æ€§èƒ½ä¼˜äºå…¶ä»–å‹ç¼©æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒUniGiståœ¨é•¿ç¨‹ä¾èµ–å»ºæ¨¡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰é•¿è·ç¦»çš„è¯­ä¹‰å…³ç³»ã€‚å…·ä½“è€Œè¨€ï¼ŒUniGiståœ¨æŸäº›ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾20%çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniGistå¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„åœºæ™¯ï¼Œä¾‹å¦‚é•¿æ–‡æ¡£æ‘˜è¦ã€ä»£ç ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡é™ä½KVç¼“å­˜çš„å†…å­˜å ç”¨ï¼ŒUniGistå¯ä»¥ä½¿å¤§å‹è¯­è¨€æ¨¡å‹æ›´å®¹æ˜“éƒ¨ç½²åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡ã€‚æ­¤å¤–ï¼ŒUniGistè¿˜å¯ä»¥æé«˜æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ï¼Œä»è€Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models are increasingly capable of handling long-context inputs, but the memory overhead of key-value (KV) cache remains a major bottleneck for general-purpose deployment. While various compression strategies have been explored, sequence-level compression, which drops the full KV caches for certain tokens, is particularly challenging as it can lead to the loss of important contextual information. To address this, we introduce UniGist, a sequence-level long-context compression framework that efficiently preserves context information by replacing raw tokens with special compression tokens (gists) in a fine-grained manner. We adopt a chunk-free training strategy and design an efficient kernel with a gist shift trick, enabling optimized GPU training. Our scheme also supports flexible inference by allowing the actual removal of compressed tokens, resulting in real-time memory savings. Experiments across multiple long-context tasks demonstrate that UniGist significantly improves compression quality, with especially strong performance in detail-recalling tasks and long-range dependency modeling.

