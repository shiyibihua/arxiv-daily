---
layout: default
title: Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment
---

# Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15701" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15701v1</a>
  <a href="https://arxiv.org/pdf/2509.15701.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15701v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15701v1', 'Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ke Wang, Wenning Wei, Yan Deng, Lei He, Sheng Zhao

**åˆ†ç±»**: cs.CL, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: submitted to ICASSP2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¾®è°ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç”¨äºè‡ªåŠ¨å‘éŸ³è¯„ä¼°ï¼Œæå‡ç»†ç²’åº¦è¯„ä¼°èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨å‘éŸ³è¯„ä¼°` `å¤šæ¨¡æ€æ¨¡å‹` `å¾®è°ƒ` `è¯­éŸ³è¯†åˆ«` `è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªåŠ¨å‘éŸ³è¯„ä¼°ç³»ç»Ÿåœ¨ç»†ç²’åº¦ï¼ˆå¦‚éŸ³ç´ çº§åˆ«ï¼‰è¯„ä¼°æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥å‡†ç¡®æ•æ‰å‘éŸ³ç»†èŠ‚ã€‚
2. é€šè¿‡å¾®è°ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„è¡¨å¾èƒ½åŠ›ï¼Œæå‡æ¨¡å‹åœ¨ç»†ç²’åº¦å‘éŸ³è¯„ä¼°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨å•è¯å’Œå¥å­çº§åˆ«è¯„ä¼°ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éŸ³ç´ çº§åˆ«ä»æœ‰æå‡ç©ºé—´ï¼ŒSCCæŒ‡æ ‡æ›´é€‚åˆè¯„ä¼°æ’åºä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨å‘éŸ³è¯„ä¼°(APA)å¯¹äºè®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ (CALL)è‡³å…³é‡è¦ï¼Œå®ƒéœ€è¦åœ¨å¤šä¸ªç²’åº¦å’Œæ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚å¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMMs)ä¸ºAPAæä¾›äº†æ–°çš„æœºä¼šï¼Œä½†å®ƒä»¬åœ¨ç»†ç²’åº¦è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ä»ä¸ç¡®å®šã€‚æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨Speechocean762æ•°æ®é›†å’Œä¸€ä¸ªç§æœ‰è¯­æ–™åº“å¯¹LMMsè¿›è¡Œå¾®è°ƒä»¥ç”¨äºAPAã€‚å¾®è°ƒæ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬è®¾ç½®ï¼Œå¹¶ä¸”åœ¨å•ç²’åº¦ä»»åŠ¡ä¸Šå®ç°äº†ä¸å…¬å…±å’Œå•†ä¸šç³»ç»Ÿç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚è¯¥æ¨¡å‹åœ¨å•è¯å’Œå¥å­çº§åˆ«è¡¨ç°è‰¯å¥½ï¼Œè€ŒéŸ³ç´ çº§åˆ«çš„è¯„ä¼°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°Pearsonç›¸å…³ç³»æ•°(PCC)è¾¾åˆ°0.9ï¼Œè€ŒSpearmanç­‰çº§ç›¸å…³ç³»æ•°(SCC)ä¿æŒåœ¨0.6å·¦å³ï¼Œè¡¨æ˜SCCæ›´å¥½åœ°åæ˜ äº†åºæ•°ä¸€è‡´æ€§ã€‚è¿™äº›å‘ç°çªå‡ºäº†LMMsåœ¨APAä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥åœ¨ç»†ç²’åº¦å»ºæ¨¡å’Œæ’åºæ„ŸçŸ¥è¯„ä¼°æ–¹é¢çš„å·¥ä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨å‘éŸ³è¯„ä¼°ï¼ˆAPAï¼‰ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨ç»†ç²’åº¦å±‚é¢ï¼ˆç‰¹åˆ«æ˜¯éŸ³ç´ çº§åˆ«ï¼‰è¯„ä¼°å‡†ç¡®æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„APAç³»ç»Ÿéš¾ä»¥å……åˆ†åˆ©ç”¨è¯­éŸ³å’Œæ–‡æœ¬ä¿¡æ¯ä¹‹é—´çš„å…³è”ï¼Œå¯¼è‡´å¯¹å‘éŸ³ç»†èŠ‚çš„æ•æ‰èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å¼ºå¤§çš„è¡¨å¾å­¦ä¹ èƒ½åŠ›ï¼Œé€šè¿‡å¾®è°ƒçš„æ–¹å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è¯­éŸ³å’Œæ–‡æœ¬ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œä»è€Œæå‡åœ¨ç»†ç²’åº¦å‘éŸ³è¯„ä¼°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚LMMs é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ åˆ°çš„é€šç”¨çŸ¥è¯†å¯ä»¥è¿ç§»åˆ° APA ä»»åŠ¡ä¸­ï¼Œå‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) é€‰æ‹©åˆé€‚çš„LMMä½œä¸ºåŸºç¡€æ¨¡å‹ï¼›2) æ„å»ºåŒ…å«è¯­éŸ³å’Œæ–‡æœ¬ä¿¡æ¯çš„è®­ç»ƒæ•°æ®é›†ï¼ˆSpeechocean762å’Œç§æœ‰è¯­æ–™åº“ï¼‰ï¼›3) ä½¿ç”¨è®­ç»ƒæ•°æ®å¯¹LMMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”APAä»»åŠ¡ï¼›4) åœ¨ä¸åŒç²’åº¦çº§åˆ«ï¼ˆå•è¯ã€å¥å­ã€éŸ³ç´ ï¼‰è¯„ä¼°å¾®è°ƒåæ¨¡å‹çš„æ€§èƒ½ï¼›5) åˆ†æä¸åŒè¯„ä¼°æŒ‡æ ‡ï¼ˆPCCå’ŒSCCï¼‰çš„é€‚ç”¨æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åº”ç”¨äºè‡ªåŠ¨å‘éŸ³è¯„ä¼°ä»»åŠ¡ï¼Œå¹¶æ¢ç´¢äº†å¾®è°ƒLMMsåœ¨æå‡ç»†ç²’åº¦è¯„ä¼°æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚ä¸ä¼ ç»Ÿçš„APAç³»ç»Ÿç›¸æ¯”ï¼ŒLMMsèƒ½å¤Ÿæ›´å¥½åœ°èåˆè¯­éŸ³å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°å‘éŸ³è´¨é‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†ä¸åŒè¯„ä¼°æŒ‡æ ‡åœ¨APAä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å‚è€ƒã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„LMMæ¶æ„ï¼ˆå…·ä½“æ¨¡å‹æœªçŸ¥ï¼‰ï¼›2) è®¾è®¡åˆé€‚çš„è¾“å…¥è¡¨ç¤ºï¼Œå°†è¯­éŸ³å’Œæ–‡æœ¬ä¿¡æ¯è¾“å…¥åˆ°LMMä¸­ï¼ˆå…·ä½“æ–¹æ³•æœªçŸ¥ï¼‰ï¼›3) é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°è¿›è¡Œå¾®è°ƒï¼Œä¾‹å¦‚å‡æ–¹è¯¯å·®æˆ–äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆå…·ä½“é€‰æ‹©æœªçŸ¥ï¼‰ï¼›4) é’ˆå¯¹ä¸åŒç²’åº¦çº§åˆ«çš„è¯„ä¼°ä»»åŠ¡ï¼Œè®¾è®¡ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1å€¼ï¼ˆå…·ä½“æŒ‡æ ‡æœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„LMMåœ¨å•è¯å’Œå¥å­çº§åˆ«çš„å‘éŸ³è¯„ä¼°ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬è®¾ç½®ã€‚Pearsonç›¸å…³ç³»æ•°(PCC)è¾¾åˆ°0.9ï¼Œè¡¨æ˜æ¨¡å‹åœ¨é¢„æµ‹å‘éŸ³è´¨é‡æ–¹é¢å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼ŒSpearmanç­‰çº§ç›¸å…³ç³»æ•°(SCC)ä»…ä¸º0.6å·¦å³ï¼Œè¡¨æ˜æ¨¡å‹åœ¨æ’åºä¸€è‡´æ€§æ–¹é¢ä»æœ‰æå‡ç©ºé—´ã€‚éŸ³ç´ çº§åˆ«çš„è¯„ä¼°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ (CALL)ç³»ç»Ÿï¼Œä¸ºå­¦ä¹ è€…æä¾›æ›´å‡†ç¡®ã€ç»†è‡´çš„å‘éŸ³åé¦ˆã€‚é€šè¿‡è‡ªåŠ¨è¯„ä¼°å­¦ä¹ è€…çš„å‘éŸ³æ°´å¹³ï¼Œå¸®åŠ©ä»–ä»¬å‘ç°å‘éŸ³é—®é¢˜å¹¶è¿›è¡Œçº æ­£ï¼Œä»è€Œæé«˜è¯­è¨€å­¦ä¹ æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯åº”ç”¨äºè¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆç­‰é¢†åŸŸï¼Œæå‡è¯­éŸ³å¤„ç†ç³»ç»Ÿçš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted Language Learning (CALL), requiring evaluation across multiple granularities and aspects. Large Multimodal Models (LMMs) present new opportunities for APA, but their effectiveness in fine-grained assessment remains uncertain. This work investigates fine-tuning LMMs for APA using the Speechocean762 dataset and a private corpus. Fine-tuning significantly outperforms zero-shot settings and achieves competitive results on single-granularity tasks compared to public and commercial systems. The model performs well at word and sentence levels, while phoneme-level assessment remains challenging. We also observe that the Pearson Correlation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation Coefficient (SCC) remains around 0.6, suggesting that SCC better reflects ordinal consistency. These findings highlight both the promise and limitations of LMMs for APA and point to future work on fine-grained modeling and rank-aware evaluation.

