---
layout: default
title: Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets
---

# Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15621" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15621v1</a>
  <a href="https://arxiv.org/pdf/2509.15621.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15621v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15621v1', 'Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tomoya Yamashita, Yuuki Yamanaka, Masanori Yamada, Takayuki Miura, Toshiki Shibahara, Tomoharu Iwata

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè‡ªæ„å»ºçŸ¥è¯†ä¸‰å…ƒç»„çš„å¤§è¯­è¨€æ¨¡å‹æ¦‚å¿µé—å¿˜æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¦‚å¿µé—å¿˜` `å¤§è¯­è¨€æ¨¡å‹` `çŸ¥è¯†å›¾è°±` `æœºå™¨é—å¿˜` `çŸ¥è¯†ä¸‰å…ƒç»„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨é—å¿˜æ–¹æ³•ä¾èµ–äºç‰¹å®šå¥å­ï¼Œæ— æ³•æœ‰æ•ˆç§»é™¤LLMä¸­æ›´å¹¿æ³›çš„æ¦‚å¿µï¼Œå¦‚äººç‰©æˆ–äº‹ä»¶ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ–°çš„æ¦‚å¿µé—å¿˜æ–¹æ³•ï¼Œé€šè¿‡çŸ¥è¯†å›¾è°±è¡¨ç¤ºLLMå†…éƒ¨çŸ¥è¯†ï¼Œå¹¶ç§»é™¤ç›®æ ‡èŠ‚ç‚¹åŠå…¶å…³è”è¾¹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®ç°æ¦‚å¿µçº§åˆ«é—å¿˜çš„åŒæ—¶ï¼Œèƒ½å¤Ÿè¾ƒå¥½åœ°ä¿ç•™LLMä¸­ä¸ç›¸å…³çš„çŸ¥è¯†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¦‚å¿µé—å¿˜ï¼ˆCUï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³LLMä¸­å­˜åœ¨çš„éšç§å’Œç‰ˆæƒé—®é¢˜ã€‚ç°æœ‰æœºå™¨é—å¿˜ï¼ˆMUï¼‰æ–¹æ³•ä¾èµ–äºæ˜¾å¼çš„ç›®æ ‡å¥å­ï¼Œæ— æ³•ç§»é™¤æ›´å¹¿æ³›çš„æ¦‚å¿µï¼Œå¦‚äººç‰©æˆ–äº‹ä»¶ã€‚æœ¬æ–‡å°†CUå®šä¹‰ä¸ºç§»é™¤çŸ¥è¯†å›¾è°±ä¸­ä»£è¡¨LLMå†…éƒ¨çŸ¥è¯†çš„é—å¿˜ç›®æ ‡èŠ‚ç‚¹åŠå…¶ç›¸å…³è¾¹ã€‚è¯¥æ–¹æ³•é¦–å…ˆæç¤ºLLMç”Ÿæˆå…³äºé—å¿˜ç›®æ ‡çš„çŸ¥è¯†ä¸‰å…ƒç»„å’Œè§£é‡Šæ€§å¥å­ï¼Œç„¶åå¯¹è¿™äº›è¡¨ç¤ºè¿›è¡Œé—å¿˜å¤„ç†ã€‚é€šè¿‡å°†é—å¿˜è¿‡ç¨‹ä¸LLMçš„å†…éƒ¨çŸ¥è¯†è¡¨ç¤ºå¯¹é½ï¼Œå®ç°æ›´ç²¾ç¡®å’Œå…¨é¢çš„æ¦‚å¿µç§»é™¤ï¼ŒåŒæ—¶ä¿ç•™ä¸ç›¸å…³çš„çŸ¥è¯†ã€‚åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å®ç°æ¦‚å¿µçº§åˆ«çš„é—å¿˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹é—å¿˜æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åˆ é™¤ç‰¹å®šçš„å¥å­æˆ–æ–‡æ¡£ï¼Œè€Œå¿½ç•¥äº†æ›´é«˜çº§åˆ«çš„æ¦‚å¿µé—å¿˜ã€‚ä¾‹å¦‚ï¼Œå¦‚æœéœ€è¦è®©æ¨¡å‹å¿˜è®°æŸä¸ªç‰¹å®šäººç‰©ï¼Œç°æœ‰çš„æ–¹æ³•å¯èƒ½éœ€è¦åˆ é™¤æ‰€æœ‰åŒ…å«è¯¥äººç‰©åå­—çš„å¥å­ï¼Œè¿™ä¸ä»…æ•ˆç‡ä½ä¸‹ï¼Œè€Œä¸”å®¹æ˜“é—æ¼ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦æ˜¾å¼çš„ç›®æ ‡å¥å­ï¼Œéš¾ä»¥å¤„ç†éšå¼çŸ¥è¯†çš„é—å¿˜ã€‚å› æ­¤ï¼Œå¦‚ä½•å®ç°å¯¹LLMä¸­æ¦‚å¿µçº§åˆ«çš„ç²¾ç¡®é—å¿˜ï¼ŒåŒæ—¶é¿å…å¯¹æ— å…³çŸ¥è¯†é€ æˆæŸå®³ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMçš„å†…éƒ¨çŸ¥è¯†è¡¨ç¤ºä¸ºçŸ¥è¯†å›¾è°±ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨æ¦‚å¿µï¼Œè¾¹ä»£è¡¨æ¦‚å¿µä¹‹é—´çš„å…³ç³»ã€‚æ¦‚å¿µé—å¿˜çš„ç›®æ ‡è¢«å®šä¹‰ä¸ºä»è¯¥çŸ¥è¯†å›¾è°±ä¸­ç§»é™¤ç‰¹å®šçš„ç›®æ ‡èŠ‚ç‚¹åŠå…¶ç›¸å…³çš„è¾¹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œè®ºæ–‡é¦–å…ˆåˆ©ç”¨LLMè‡ªèº«çš„èƒ½åŠ›æ¥æ„å»ºå…³äºé—å¿˜ç›®æ ‡çš„çŸ¥è¯†ä¸‰å…ƒç»„å’Œè§£é‡Šæ€§å¥å­ï¼Œç„¶åå¯¹è¿™äº›è¡¨ç¤ºè¿›è¡Œé—å¿˜å¤„ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **çŸ¥è¯†ä¸‰å…ƒç»„ç”Ÿæˆ**ï¼šåˆ©ç”¨æç¤ºå·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰æŠ€æœ¯ï¼Œå¼•å¯¼LLMç”Ÿæˆå…³äºé—å¿˜ç›®æ ‡çš„çŸ¥è¯†ä¸‰å…ƒç»„ï¼ˆä¾‹å¦‚ï¼Œ(äººç‰©, èŒä¸š, æ¼”å‘˜)ï¼‰å’Œè§£é‡Šæ€§å¥å­ã€‚2) **çŸ¥è¯†è¡¨ç¤ºå¯¹é½**ï¼šå°†ç”Ÿæˆçš„çŸ¥è¯†ä¸‰å…ƒç»„å’Œè§£é‡Šæ€§å¥å­ä¸LLMçš„å†…éƒ¨çŸ¥è¯†è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œç¡®ä¿é—å¿˜è¿‡ç¨‹èƒ½å¤Ÿä½œç”¨äºLLMçš„å†…éƒ¨çŸ¥è¯†ã€‚3) **é—å¿˜è¿‡ç¨‹**ï¼šé‡‡ç”¨ç‰¹å®šçš„é—å¿˜ç®—æ³•ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•ï¼‰æ¥æ›´æ–°LLMçš„å‚æ•°ï¼Œä»è€Œç§»é™¤ç›®æ ‡èŠ‚ç‚¹åŠå…¶ç›¸å…³çš„è¾¹ã€‚4) **çŸ¥è¯†ä¿ç•™**ï¼šåœ¨é—å¿˜è¿‡ç¨‹ä¸­ï¼Œéœ€è¦é‡‡å–æªæ–½æ¥ä¿æŠ¤LLMä¸­ä¸ç›¸å…³çš„çŸ¥è¯†ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡æ­£åˆ™åŒ–é¡¹æ¥çº¦æŸå‚æ•°çš„æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ¦‚å¿µé—å¿˜é—®é¢˜è½¬åŒ–ä¸ºçŸ¥è¯†å›¾è°±ä¸Šçš„èŠ‚ç‚¹å’Œè¾¹ç§»é™¤é—®é¢˜ï¼Œå¹¶åˆ©ç”¨LLMè‡ªèº«çš„èƒ½åŠ›æ¥æ„å»ºå…³äºé—å¿˜ç›®æ ‡çš„çŸ¥è¯†è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•ä¸ä»…èƒ½å¤Ÿå®ç°å¯¹æ¦‚å¿µçº§åˆ«çš„ç²¾ç¡®é—å¿˜ï¼Œè€Œä¸”èƒ½å¤Ÿæ›´å¥½åœ°ä¿æŠ¤LLMä¸­ä¸ç›¸å…³çš„çŸ¥è¯†ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦æ˜¾å¼çš„ç›®æ ‡å¥å­ï¼Œèƒ½å¤Ÿå¤„ç†éšå¼çŸ¥è¯†çš„é—å¿˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨çŸ¥è¯†ä¸‰å…ƒç»„ç”Ÿæˆé˜¶æ®µï¼Œè®ºæ–‡é‡‡ç”¨äº†ç‰¹å®šçš„æç¤ºæ¨¡æ¿æ¥å¼•å¯¼LLMç”Ÿæˆé«˜è´¨é‡çš„çŸ¥è¯†ä¸‰å…ƒç»„å’Œè§£é‡Šæ€§å¥å­ã€‚åœ¨é—å¿˜è¿‡ç¨‹é˜¶æ®µï¼Œè®ºæ–‡é‡‡ç”¨äº†åŸºäºæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•æ¥æ›´æ–°LLMçš„å‚æ•°ï¼Œå¹¶å¼•å…¥äº†æ­£åˆ™åŒ–é¡¹æ¥çº¦æŸå‚æ•°çš„æ›´æ–°ï¼Œä»è€Œä¿æŠ¤LLMä¸­ä¸ç›¸å…³çš„çŸ¥è¯†ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å®ç°æ¦‚å¿µçº§åˆ«çš„é—å¿˜ï¼ŒåŒæ—¶ä¿ç•™LLMä¸­ä¸ç›¸å…³çš„çŸ¥è¯†ã€‚åœ¨çœŸå®æ•°æ®é›†å’Œåˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœéƒ½è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é—å¿˜ç›®æ ‡æ¦‚å¿µçš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒLLMåœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä¿æŠ¤LLMä¸­çš„éšç§æ•°æ®ã€ç§»é™¤ä¸å‡†ç¡®æˆ–æœ‰å®³ä¿¡æ¯ã€ä»¥åŠåº”å¯¹ç‰ˆæƒé—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºç§»é™¤LLMä¸­å…³äºç‰¹å®šäººç‰©çš„æ•æ„Ÿä¿¡æ¯ï¼Œæˆ–è€…ç§»é™¤LLMä¸­å…³äºè™šå‡äº‹ä»¶çš„é”™è¯¯ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºå®šåˆ¶åŒ–LLMï¼Œä½¿å…¶åªåŒ…å«ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼Œä»è€Œæé«˜LLMçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Machine Unlearning (MU) has recently attracted considerable attention as a solution to privacy and copyright issues in large language models (LLMs). Existing MU methods aim to remove specific target sentences from an LLM while minimizing damage to unrelated knowledge. However, these approaches require explicit target sentences and do not support removing broader concepts, such as persons or events. To address this limitation, we introduce Concept Unlearning (CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to represent the LLM's internal knowledge and define CU as removing the forgetting target nodes and associated edges. This graph-based formulation enables a more intuitive unlearning and facilitates the design of more effective methods. We propose a novel method that prompts the LLM to generate knowledge triplets and explanatory sentences about the forgetting target and applies the unlearning process to these representations. Our approach enables more precise and comprehensive concept removal by aligning the unlearning process with the LLM's internal knowledge representations. Experiments on real-world and synthetic datasets demonstrate that our method effectively achieves concept-level unlearning while preserving unrelated knowledge.

