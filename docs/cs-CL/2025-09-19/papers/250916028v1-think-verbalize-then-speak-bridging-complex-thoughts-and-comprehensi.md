---
layout: default
title: Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech
---

# Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16028" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16028v1</a>
  <a href="https://arxiv.org/pdf/2509.16028.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16028v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16028v1', 'Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sang Hoon Woo, Sehun Lee, Kang-wook Kim, Gunhee Kim

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: EMNLP 2025 Main. Project page: https://yhytoto12.github.io/TVS-ReVerT

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://yhytoto12.github.io/TVS-ReVerT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºThink-Verbalize-Speakæ¡†æ¶ï¼Œè§£è€¦æ¨ç†ä¸å£è¯­è¡¨è¾¾ï¼Œæå‡å£è¯­å¯¹è¯ç³»ç»Ÿæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å£è¯­å¯¹è¯ç³»ç»Ÿ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†ä¸è¡¨è¾¾è§£è€¦` `è¯­éŸ³è‡ªç„¶åº¦` `æ–‡æœ¬æ‘˜è¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å£è¯­å¯¹è¯ç³»ç»Ÿç›´æ¥åº”ç”¨LLMï¼Œå¯¼è‡´æ–‡æœ¬ä¼˜åŒ–ä¸å£è¯­è¡¨è¾¾ä¸åŒ¹é…ï¼Œå½±å“è¯­éŸ³è‡ªç„¶åº¦å’Œæ¨ç†èƒ½åŠ›ã€‚
2. Think-Verbalize-Speakæ¡†æ¶é€šè¿‡ä¸­é—´æ­¥éª¤â€œverbalizingâ€å°†æ¨ç†ä¸å£è¯­è¡¨è¾¾è§£è€¦ï¼Œä¿ç•™LLMçš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸æ˜¾è‘—å½±å“æ¨ç†æ€§èƒ½çš„å‰æä¸‹ï¼Œæå‡äº†è¯­éŸ³çš„è‡ªç„¶æ€§å’Œç®€æ´æ€§ï¼Œå¹¶æå‡ºäº†ReVerTåŠ é€Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å£è¯­å¯¹è¯ç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å‘æŒ¥å…¶å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥å°†LLMsåº”ç”¨äºå£è¯­äº¤æµé€šå¸¸ä¼šäº§ç”Ÿæ¬¡ä¼˜çš„ç»“æœï¼Œå› ä¸ºæœ€ä½³æ–‡æœ¬å’Œå£å¤´è¡¨è¾¾ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ã€‚è™½ç„¶ç°æœ‰çš„æ–¹æ³•è°ƒæ•´LLMsä»¥äº§ç”Ÿé€‚åˆè¯­éŸ³è¾“å‡ºï¼Œä½†å®ƒä»¬å¯¹æ¨ç†æ€§èƒ½çš„å½±å“ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Think-Verbalize-Speakï¼Œä¸€ä¸ªå°†æ¨ç†ä¸å£è¯­è¡¨è¾¾åˆ†ç¦»çš„æ¡†æ¶ï¼Œä»¥ä¿æŒLLMsçš„å®Œæ•´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ–¹æ³•çš„ä¸­å¿ƒæ˜¯verbalizingï¼Œä¸€ä¸ªä¸­é—´æ­¥éª¤ï¼Œå°†æ€æƒ³è½¬åŒ–ä¸ºè‡ªç„¶çš„ã€é€‚åˆè¯­éŸ³çš„æ–‡æœ¬ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ReVerTï¼Œä¸€ä¸ªåŸºäºå¢é‡å’Œå¼‚æ­¥æ‘˜è¦çš„ã€å»¶è¿Ÿæ•ˆç‡é«˜çš„verbalizerã€‚è·¨å¤šä¸ªåŸºå‡†çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯¹æ¨ç†å½±å“æœ€å°çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†è¯­éŸ³çš„è‡ªç„¶æ€§å’Œç®€æ´æ€§ã€‚åŒ…å«æ•°æ®é›†å’Œæºä»£ç çš„é¡¹ç›®é¡µé¢å¯åœ¨https://yhytoto12.github.io/TVS-ReVerT æ‰¾åˆ°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å£è¯­å¯¹è¯ç³»ç»Ÿç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆè¯­éŸ³ï¼Œä½†LLMsé€šå¸¸é’ˆå¯¹æ–‡æœ¬ä¼˜åŒ–ï¼Œç”Ÿæˆçš„æ–‡æœ¬å†—é•¿ã€ä¸è‡ªç„¶ï¼Œä¸é€‚åˆç›´æ¥ç”¨äºå£è¯­äº¤æµã€‚æ­¤å¤–ï¼Œä¸ºäº†é€‚åº”è¯­éŸ³è¾“å‡ºï¼Œå¯¹LLMsè¿›è¡Œè°ƒæ•´å¯èƒ½ä¼šæŸå®³å…¶åŸæœ‰çš„æ¨ç†èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•å¹³è¡¡è¯­éŸ³çš„è‡ªç„¶æ€§å’Œæ¨ç†èƒ½åŠ›æ˜¯å½“å‰å£è¯­å¯¹è¯ç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMçš„æ¨ç†è¿‡ç¨‹ä¸æœ€ç»ˆçš„å£è¯­è¡¨è¾¾è§£è€¦ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆè®©LLMè¿›è¡Œå……åˆ†çš„æ€è€ƒï¼ˆThinkï¼‰ï¼Œç„¶åå°†æ€è€ƒçš„ç»“æœè½¬åŒ–ä¸ºè‡ªç„¶ã€ç®€æ´ã€é€‚åˆå£è¯­è¡¨è¾¾çš„æ–‡æœ¬ï¼ˆVerbalizeï¼‰ï¼Œæœ€åå†å°†æ–‡æœ¬è½¬åŒ–ä¸ºè¯­éŸ³ï¼ˆSpeakï¼‰ã€‚é€šè¿‡å¼•å…¥ä¸­é—´çš„â€œVerbalizeâ€æ­¥éª¤ï¼Œå¯ä»¥æ›´å¥½åœ°æ§åˆ¶è¯­éŸ³çš„è´¨é‡ï¼ŒåŒæ—¶é¿å…å¯¹LLMçš„æ¨ç†èƒ½åŠ›é€ æˆæŸå®³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šThink-Verbalize-Speakæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) **Think**ï¼šä½¿ç”¨LLMè¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆè¯¦ç»†çš„æ€è€ƒè¿‡ç¨‹å’Œç»“è®ºã€‚2) **Verbalize**ï¼šå°†LLMçš„æ€è€ƒç»“æœè½¬åŒ–ä¸ºé€‚åˆå£è¯­è¡¨è¾¾çš„æ–‡æœ¬ã€‚è¯¥é˜¶æ®µä½¿ç”¨ReVerTæ¨¡å‹ï¼Œå®ƒåŸºäºå¢é‡å’Œå¼‚æ­¥æ‘˜è¦ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°ç”Ÿæˆç®€æ´è‡ªç„¶çš„å£è¯­æ–‡æœ¬ã€‚3) **Speak**ï¼šå°†Verbalizeé˜¶æ®µç”Ÿæˆçš„æ–‡æœ¬è½¬åŒ–ä¸ºè¯­éŸ³ï¼Œå¯ä»¥ä½¿ç”¨ç°æœ‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†Think-Verbalize-Speakæ¡†æ¶ï¼Œå°†æ¨ç†ä¸å£è¯­è¡¨è¾¾è§£è€¦ã€‚æ­¤å¤–ï¼ŒReVerTæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ï¼Œå®ƒèƒ½å¤Ÿé«˜æ•ˆåœ°å°†LLMçš„æ€è€ƒç»“æœè½¬åŒ–ä¸ºé€‚åˆå£è¯­è¡¨è¾¾çš„æ–‡æœ¬ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿è¯æ¨ç†èƒ½åŠ›çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜è¯­éŸ³çš„è‡ªç„¶æ€§å’Œç®€æ´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šReVerTæ¨¡å‹é‡‡ç”¨å¢é‡å’Œå¼‚æ­¥æ‘˜è¦çš„æ–¹å¼ï¼Œé€æ­¥ç”Ÿæˆå£è¯­æ–‡æœ¬ã€‚å…·ä½“æ¥è¯´ï¼ŒReVerTæ¨¡å‹é¦–å…ˆå°†LLMçš„æ€è€ƒç»“æœåˆ†å‰²æˆå¤šä¸ªç‰‡æ®µï¼Œç„¶åå¯¹æ¯ä¸ªç‰‡æ®µè¿›è¡Œæ‘˜è¦ï¼Œæœ€åå°†æ‘˜è¦ç»“æœæ‹¼æ¥èµ·æ¥ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼ŒReVerTæ¨¡å‹é‡‡ç”¨å¼‚æ­¥çš„æ–¹å¼è¿›è¡Œæ‘˜è¦ï¼Œå³åœ¨ç”Ÿæˆä¸€ä¸ªç‰‡æ®µçš„æ‘˜è¦çš„åŒæ—¶ï¼Œå¯ä»¥å¼€å§‹å¤„ç†ä¸‹ä¸€ä¸ªç‰‡æ®µã€‚æ­¤å¤–ï¼ŒReVerTæ¨¡å‹è¿˜ä½¿ç”¨äº†ä¸€äº›æŠ€å·§æ¥æé«˜æ‘˜è¦çš„è´¨é‡ï¼Œä¾‹å¦‚ä½¿ç”¨å…³é”®è¯æå–å’Œå¥å­å‹ç¼©ç­‰æŠ€æœ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒThink-Verbalize-Speakæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è¯­éŸ³è‡ªç„¶åº¦æ–¹é¢ï¼Œè¯¥æ–¹æ³•æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†çº¦10%ã€‚åœ¨è¯­éŸ³ç®€æ´æ€§æ–¹é¢ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦ç¼©çŸ­äº†çº¦20%ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯¹æ¨ç†æ€§èƒ½çš„å½±å“éå¸¸å°ï¼Œå‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚ReVerTæ¨¡å‹ä¹Ÿè¡¨ç°å‡ºå¾ˆé«˜çš„æ•ˆç‡ï¼Œèƒ½å¤Ÿæ»¡è¶³å®æ—¶å£è¯­å¯¹è¯çš„éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§å£è¯­å¯¹è¯ç³»ç»Ÿï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€èŠå¤©æœºå™¨äººã€è¯­éŸ³å®¢æœç­‰ã€‚é€šè¿‡æé«˜è¯­éŸ³çš„è‡ªç„¶æ€§å’Œç®€æ´æ€§ï¼Œå¯ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæé«˜å¯¹è¯æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæ•™è‚²é¢†åŸŸï¼Œä¾‹å¦‚è¾…åŠ©è¯­è¨€å­¦ä¹ ã€è‡ªåŠ¨ç”Ÿæˆæ•™å­¦ææ–™ç­‰ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è¯­éŸ³ç¿»è¯‘ã€è¯­éŸ³æœç´¢ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose Think-Verbalize-Speak, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is verbalizing, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce ReVerT, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at https://yhytoto12.github.io/TVS-ReVerT

