---
layout: default
title: Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning
---

# Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16025" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16025v1</a>
  <a href="https://arxiv.org/pdf/2509.16025.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16025v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16025v1', 'Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hong-Yun Lin, Jhen-Ke Lin, Chung-Chun Wang, Hao-Chien Lu, Berlin Chen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: Copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å’Œå¤šç›®æ ‡å­¦ä¹ çš„ä¼šè¯çº§å£è¯­è¯„ä¼°æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å£è¯­è¯„ä¼°` `å¤šæ¨¡æ€å­¦ä¹ ` `åŸºç¡€æ¨¡å‹` `å¤šç›®æ ‡å­¦ä¹ ` `è¯­éŸ³è¯†åˆ«` `è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å£è¯­è¯„ä¼°æ–¹æ³•ä¾èµ–çº§è”pipelineæˆ–çŸ­éŸ³é¢‘çª—å£ï¼Œå­˜åœ¨è¯¯å·®ä¼ æ’­å’Œå¿½ç•¥è¯­ç¯‡ä¿¡æ¯çš„ä¸è¶³ã€‚
2. æå‡ºä¸€ç§åŸºäºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„æ–¹æ³•ï¼Œç»“åˆå¤šç›®æ ‡å­¦ä¹ å’Œè¯­éŸ³å…ˆéªŒï¼Œå®ç°ä¼šè¯çº§æ•´ä½“è¯„ä¼°ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºè®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å£è¯­è¯„ä¼°ï¼ˆSLAï¼‰æ—¨åœ¨è¯„ä¼°å­¦ä¹ è€…åœ¨è‡ªç„¶è¯­éŸ³ä¸­çš„å£è¯­èƒ½åŠ›ã€‚éšç€ç¬¬äºŒè¯­è¨€ä¸ºè‹±è¯­çš„å­¦ä¹ è€…æ•°é‡ä¸æ–­å¢é•¿ï¼Œå¯¹å¯é SLAçš„éœ€æ±‚ä¹Ÿæ—¥ç›Šå¢åŠ ï¼Œè¿™å¯¹äºè®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ï¼ˆCALLï¼‰è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºçº§è”pipelineï¼Œå®¹æ˜“äº§ç”Ÿè¯¯å·®ä¼ æ’­ï¼Œæˆ–è€…ä½¿ç”¨çŸ­éŸ³é¢‘çª—å£çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå¯èƒ½å¿½ç•¥è¯­ç¯‡å±‚é¢çš„è¯æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹æ–¹æ³•ï¼Œå¯ä»¥åœ¨å•æ¬¡å¤„ç†ä¸­æ‰§è¡Œä¼šè¯çº§åˆ«çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¤šç›®æ ‡å­¦ä¹ ä¸åŸºäºå†»ç»“çš„Whisper ASRæ¨¡å‹çš„è¯­éŸ³å…ˆéªŒç›¸ç»“åˆï¼Œç”¨äºå£°å­¦æ„ŸçŸ¥æ ¡å‡†ï¼Œä»è€Œå¯ä»¥åœ¨ä¸ä¾èµ–æ‰‹å·¥ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œè”åˆå­¦ä¹ SLAçš„æ•´ä½“å’Œç‰¹å¾çº§åˆ«ç›®æ ‡ã€‚é€šè¿‡è¿è´¯åœ°å¤„ç†L2å­¦ä¹ è€…çš„æ•´ä¸ªå›ç­”ä¼šè¯ï¼Œè¯¥æ¨¡å‹æ“…é•¿é¢„æµ‹æ•´ä½“å£è¯­èƒ½åŠ›ã€‚åœ¨Speak & ImproveåŸºå‡†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›çš„çº§è”ç³»ç»Ÿï¼Œå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„è·¨éƒ¨åˆ†æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ä¸ªç´§å‡‘çš„å¯éƒ¨ç½²çš„è¯„åˆ†å™¨ï¼Œä¸“ä¸ºCALLåº”ç”¨è€Œè®¾è®¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å£è¯­è¯„ä¼°æ–¹æ³•ä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ã€‚ä¸€æ˜¯ä¾èµ–äºçº§è”çš„pipelineï¼Œä¾‹å¦‚å…ˆè¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œå†æå–è¯­è¨€ç‰¹å¾ï¼Œæœ€åè¿›è¡Œè¯„åˆ†ã€‚è¿™ç§æ–¹å¼å®¹æ˜“å¯¼è‡´è¯¯å·®åœ¨å„ä¸ªé˜¶æ®µä¹‹é—´ä¼ æ’­ï¼Œé™ä½æ•´ä½“è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚äºŒæ˜¯ç°æœ‰çš„ç«¯åˆ°ç«¯æ¨¡å‹é€šå¸¸åªå¤„ç†çŸ­éŸ³é¢‘çª—å£ï¼Œæ— æ³•æ•æ‰åˆ°ä¼šè¯çº§åˆ«çš„è¯­ç¯‡ä¿¡æ¯ï¼Œä¾‹å¦‚è¿è´¯æ€§ã€é€»è¾‘æ€§ç­‰ï¼Œè¿™äº›ä¿¡æ¯å¯¹äºè¯„ä¼°å£è¯­èƒ½åŠ›è‡³å…³é‡è¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œç›´æ¥å¯¹æ•´ä¸ªä¼šè¯è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œé¿å…è¯¯å·®ä¼ æ’­å’Œæ•æ‰è¯­ç¯‡ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œä½œè€…åˆ©ç”¨é¢„è®­ç»ƒçš„Whisper ASRæ¨¡å‹ä½œä¸ºè¯­éŸ³å…ˆéªŒï¼Œå¹¶ç»“åˆå¤šç›®æ ‡å­¦ä¹ ï¼ŒåŒæ—¶é¢„æµ‹æ•´ä½“å£è¯­èƒ½åŠ›å’Œå„ä¸ªç»†ç²’åº¦çš„è¯­è¨€ç‰¹å¾ã€‚è¿™æ ·å¯ä»¥å……åˆ†åˆ©ç”¨è¯­éŸ³ä¿¡æ¯ï¼Œå¹¶å­¦ä¹ åˆ°æ›´é²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) Whisper ASRæ¨¡å‹ï¼šç”¨äºæå–è¯­éŸ³ç‰¹å¾ï¼Œå¹¶è¿›è¡Œå£°å­¦æ„ŸçŸ¥æ ¡å‡†ã€‚2) å¤šç›®æ ‡å­¦ä¹ æ¨¡å—ï¼šç”¨äºåŒæ—¶é¢„æµ‹æ•´ä½“å£è¯­èƒ½åŠ›å’Œå„ä¸ªç»†ç²’åº¦çš„è¯­è¨€ç‰¹å¾ã€‚3) èåˆæ¨¡å—ï¼šç”¨äºå°†è¯­éŸ³ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œèåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚æ•´ä¸ªæµç¨‹æ˜¯ç«¯åˆ°ç«¯çš„ï¼Œå¯ä»¥ç›´æ¥å¯¹æ•´ä¸ªä¼šè¯è¿›è¡Œå»ºæ¨¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å’Œå¤šç›®æ ‡å­¦ä¹ ç›¸ç»“åˆï¼Œç”¨äºä¼šè¯çº§åˆ«çš„å£è¯­è¯„ä¼°ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¯ä»¥é¿å…è¯¯å·®ä¼ æ’­ï¼Œæ•æ‰è¯­ç¯‡ä¿¡æ¯ï¼Œå¹¶å­¦ä¹ åˆ°æ›´é²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„Whisper ASRæ¨¡å‹ä½œä¸ºè¯­éŸ³å…ˆéªŒï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨è¯­éŸ³ä¿¡æ¯ï¼Œæé«˜è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œä½œè€…ä½¿ç”¨äº†å†»ç»“çš„Whisper ASRæ¨¡å‹ï¼Œä»¥ä¿è¯è¯­éŸ³ç‰¹å¾çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šç›®æ ‡å­¦ä¹ æ–¹é¢ï¼Œä½œè€…ä½¿ç”¨äº†ä¸åŒçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ•´ä½“å£è¯­èƒ½åŠ›å’Œå„ä¸ªç»†ç²’åº¦çš„è¯­è¨€ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜è®¾è®¡äº†ä¸€ä¸ªèåˆæ¨¡å—ï¼Œç”¨äºå°†è¯­éŸ³ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œèåˆã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨Speak & ImproveåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œè¶…è¶Šäº†å…ˆå‰çš„æœ€å…ˆè¿›çš„çº§è”ç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…åœ¨æ•´ä½“å£è¯­èƒ½åŠ›è¯„ä¼°æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œä¸”å…·æœ‰å¾ˆå¼ºçš„è·¨éƒ¨åˆ†æ³›åŒ–èƒ½åŠ›ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰åˆ°å£è¯­èƒ½åŠ›çš„æ ¸å¿ƒç‰¹å¾ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è¯­æ–™å’Œåœºæ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ï¼ˆCALLï¼‰é¢†åŸŸï¼Œä¸ºL2è‹±è¯­å­¦ä¹ è€…æä¾›è‡ªåŠ¨åŒ–çš„å£è¯­è¯„ä¼°æœåŠ¡ã€‚å®ƒå¯ä»¥ç”¨äºåœ¨çº¿å£è¯­ç»ƒä¹ ã€æ¨¡æ‹Ÿè€ƒè¯•ã€ä»¥åŠä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„æ¨èç­‰æ–¹é¢ï¼Œå¸®åŠ©å­¦ä¹ è€…æ›´æœ‰æ•ˆåœ°æé«˜å£è¯­èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºæ‹›è˜é¢è¯•ã€è¯­è¨€èƒ½åŠ›è®¤è¯ç­‰åœºæ™¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„å¸‚åœºå‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spoken Language Assessment (SLA) estimates a learner's oral proficiency from spontaneous speech. The growing population of L2 English speakers has intensified the demand for reliable SLA, a critical component of Computer Assisted Language Learning (CALL). Existing efforts often rely on cascaded pipelines, which are prone to error propagation, or end-to-end models that often operate on a short audio window, which might miss discourse-level evidence. This paper introduces a novel multimodal foundation model approach that performs session-level evaluation in a single pass. Our approach couples multi-target learning with a frozen, Whisper ASR model-based speech prior for acoustic-aware calibration, allowing for jointly learning holistic and trait-level objectives of SLA without resorting to handcrafted features. By coherently processing the entire response session of an L2 speaker, the model excels at predicting holistic oral proficiency. Experiments conducted on the Speak & Improve benchmark demonstrate that our proposed approach outperforms the previous state-of-the-art cascaded system and exhibits robust cross-part generalization, producing a compact deployable grader that is tailored for CALL applications.

