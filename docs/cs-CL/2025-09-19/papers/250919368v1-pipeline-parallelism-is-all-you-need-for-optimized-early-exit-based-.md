---
layout: default
title: Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding
---

# Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19368" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.19368v1</a>
  <a href="https://arxiv.org/pdf/2509.19368.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19368v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19368v1', 'Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ruanjun Li, Ziheng Liu, Yuanming Shi, Jiawei Shao, Chi Zhang, Xuelong Li

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-19

**Â§áÊ≥®**: 17 pages, 7 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÊµÅÊ∞¥Á∫øÂπ∂Ë°åËá™Êé®ÊµãËß£Á†ÅPPSDÔºå‰ºòÂåñÂü∫‰∫éÊó©ÈÄÄÂá∫ÁöÑLLMÊé®ÁêÜ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Ëá™Êé®ÊµãËß£Á†Å` `Êó©ÈÄÄÂá∫` `ÊµÅÊ∞¥Á∫øÂπ∂Ë°å` `Ê®°ÂûãÊé®ÁêÜÂä†ÈÄü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÊó©ÈÄÄÂá∫ÁöÑËá™Êé®ÊµãËß£Á†ÅÊñπÊ≥ïÔºåÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Âä†ÈÄüÊïàÊûú‰∏ç‰Ω≥ÔºåÂ∏∏Âõ†draftÊàêÊú¨ËøáÈ´òÂØºËá¥Ë¥üÂä†ÈÄü„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÊµÅÊ∞¥Á∫øÂπ∂Ë°åËá™Êé®ÊµãËß£Á†ÅÔºàPPSDÔºâÔºåÈÄöËøáÊµÅÊ∞¥Á∫øÂåñdraftÂíåÈ™åËØÅËøáÁ®ãÔºåÈÅøÂÖçÊµ™Ë¥πËÆ°ÁÆóËµÑÊ∫ê„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPPSDÂú®Â§öÁßçÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫Ü2.01x~3.81xÁöÑÂä†ÈÄüÊØîÔºåÊé•ËøëÊúÄ‰ºòÂä†ÈÄüÊïàÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂÖ∑ÊúâÂá∫Ëâ≤ÁöÑÁîüÊàêË¥®ÈáèÔºå‰ΩÜÁî±‰∫éÊØè‰∏™ËæìÂá∫tokenÈÉΩÈúÄË¶ÅÈÄöËøáÊâÄÊúâÊ®°ÂûãÂ±ÇËá™ÂõûÂΩíÁîüÊàêÔºåÂõ†Ê≠§Êé®ÁêÜÊàêÊú¨ÈùûÂ∏∏È´ò„ÄÇÂü∫‰∫éÊó©ÈÄÄÂá∫ÁöÑËá™Êé®ÊµãËß£Á†ÅÔºàEESDÔºâÊó®Âú®Èôç‰ΩéËøô‰∏ÄÊàêÊú¨„ÄÇÁÑ∂ËÄåÔºåÂú®ÂÆûË∑µ‰∏≠ÔºåÂç≥‰ΩøÂÖ∑ÊúâËâØÂ•ΩÂØπÈΩêÁöÑÊó©ÈÄÄÂá∫Â§¥ÂíåÈÄâÊã©ÁöÑÈÄÄÂá∫‰ΩçÁΩÆÔºåËÆ∏Â§öÊñπÊ≥ï‰πüÈöæ‰ª•Âú®ËøôÁßçdraft-then-verifyËåÉÂºè‰∏≠ÂÆûÁé∞È¢ÑÊúüÁöÑÂä†ÈÄü„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêË°®ÊòéÔºåÂè™ÊúâÂΩìÁªùÂ§ßÂ§öÊï∞draft tokenË¢´LLMÊé•ÂèóÊó∂ÔºåEESDÊâçËÉΩËé∑ÂæóÊî∂Áõä„ÄÇÂê¶ÂàôÔºådraftÊàêÊú¨ÂèØËÉΩ‰ºöË∂ÖËøáÂä†ÈÄüÂ¢ûÁõäÔºåÂØºËá¥Ë¥üÂä†ÈÄü„ÄÇ‰∏∫‰∫ÜÁºìËß£Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊµÅÊ∞¥Á∫øÂπ∂Ë°åËá™Êé®ÊµãËß£Á†ÅÔºàPPSDÔºâÔºåÂÆÉÂÆåÂÖ®ÊµÅÊ∞¥Á∫øÂåñdraftÂíåÈ™åËØÅÂ∑•‰ΩúÔºå‰ªéËÄåÈÅøÂÖçÂú®Â§±Ë¥•ÁöÑÈ¢ÑÊµã‰∏äÊµ™Ë¥πÁ≤æÂäõ„ÄÇÂÆÉÂÖ∑Êúâ‰∏§‰∏™ÂÖ≥ÈîÆÂàõÊñ∞„ÄÇÊàë‰ª¨Â∞ÜÊ®°ÂûãÂ±ÇÈÖçÁΩÆ‰∏∫ÊµÅÊ∞¥Á∫øÔºåÂÖ∂‰∏≠Êó©ÈÄÄÂá∫ÔºàdraftÔºâËÆ°ÁÆóÂíåÂâ©‰ΩôÂ±ÇÔºàÈ™åËØÅÔºâËÆ°ÁÆóÈáçÂè†„ÄÇÊàë‰ª¨‰∫§ÈîôËøõË°åÊØè‰∏™tokenÁöÑdraftÂíåÈ™åËØÅ„ÄÇÂΩìLLMÂú®ÂÖ∂ÊúÄÂêéÂá†Â±Ç‰∏≠È™åËØÅÂΩìÂâçtokenÊó∂ÔºåÊó©ÈÄÄÂá∫Ë∑ØÂæÑÂêåÊó∂draft‰∏ã‰∏Ä‰∏™token„ÄÇËøôÁßçverify-while-draftÊñπÊ°à‰ΩøÊâÄÊúâÂçïÂÖÉ‰øùÊåÅÂøôÁ¢åÔºåÂπ∂Á±ª‰ºº‰∫éÊµÅÊ∞¥Á∫øÂåñÊé®ÊµãÂíåÈ™åËØÅÈò∂ÊÆµÔºå‰ªéËÄåÂä®ÊÄÅÂú∞È™åËØÅtoken„ÄÇÁªèÈ™åÁªìÊûúËØÅÂÆûÔºåPPSDÂú®Ëá™Êé®ÊµãLLMÊé®ÁêÜ‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÂä†ÈÄü„ÄÇÂú®‰∏çÂêåÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÔºåPPSDÂÆûÁé∞‰∫Ü2.01x~3.81xËåÉÂõ¥ÂÜÖÁöÑÂä†ÈÄüÊØîÔºåÂú®Âõ∫ÂÆöÁöÑÊé•ÂèóÁéáÂíåÈÄÄÂá∫‰ΩçÁΩÆ‰∏äÂá†‰πéËé∑Âæó‰∫ÜÊúÄ‰Ω≥Âä†ÈÄüÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Êèê‰æõÈ´òÊïàËá™Êé®ÊµãÊñπÈù¢ÁöÑËøõÊ≠•„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜÊàêÊú¨È´òÊòÇÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®Âü∫‰∫éÊó©ÈÄÄÂá∫ÁöÑËá™Êé®ÊµãËß£Á†ÅÔºàEESDÔºâÊñπÊ≥ï‰∏≠ÔºåÁî±‰∫édraft tokenÁöÑÊé•ÂèóÁéá‰∏çÈ´òÔºåÂØºËá¥draftÈò∂ÊÆµÁöÑËÆ°ÁÆóÂºÄÈîÄÊäµÊ∂à‰∫ÜÂä†ÈÄüÊïàÊûúÔºåÁîöËá≥Âá∫Áé∞Ë¥üÂä†ÈÄü„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫édraftÂíåÈ™åËØÅÈò∂ÊÆµÁöÑÊïàÁéá‰∏çÈ´òÔºåÂ≠òÂú®ËµÑÊ∫êÊµ™Ë¥π„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈááÁî®ÊµÅÊ∞¥Á∫øÂπ∂Ë°åÁöÑÊñπÂºèÔºåÂ∞ÜdraftÂíåÈ™åËØÅËøáÁ®ãÂπ∂Ë°åÂåñÔºå‰ΩøÂæóÂú®È™åËØÅÂΩìÂâçtokenÁöÑÂêåÊó∂ÔºåÂèØ‰ª•Âπ∂Ë°åÂú∞draft‰∏ã‰∏Ä‰∏™token„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂèØ‰ª•ÂÖÖÂàÜÂà©Áî®ËÆ°ÁÆóËµÑÊ∫êÔºåÈÅøÂÖçÂú®Êú™Ë¢´Êé•ÂèóÁöÑdraft token‰∏äÊµ™Ë¥πËÆ°ÁÆó„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPPSDÁöÑÊäÄÊúØÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™ÈÉ®ÂàÜÔºöÈ¶ñÂÖàÔºåÂ∞ÜÊ®°ÂûãÂ±ÇÈÖçÁΩÆ‰∏∫ÊµÅÊ∞¥Á∫øÔºå‰ΩøÂæóÊó©ÈÄÄÂá∫ÔºàdraftÔºâËÆ°ÁÆóÂíåÂâ©‰ΩôÂ±ÇÔºàÈ™åËØÅÔºâËÆ°ÁÆóÂèØ‰ª•ÈáçÂè†ËøõË°å„ÄÇÂÖ∂Ê¨°Ôºå‰∫§ÈîôËøõË°åÊØè‰∏™tokenÁöÑdraftÂíåÈ™åËØÅÔºåÂç≥Âú®LLMÈ™åËØÅÂΩìÂâçtokenÊó∂ÔºåÊó©ÈÄÄÂá∫Ë∑ØÂæÑÂêåÊó∂draft‰∏ã‰∏Ä‰∏™token„ÄÇËøôÁßçverify-while-draftÁöÑÊñπÊ°à‰øùËØÅ‰∫ÜÊâÄÊúâËÆ°ÁÆóÂçïÂÖÉÁöÑÂøôÁ¢åÁä∂ÊÄÅ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPPSDÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂÖ∂ÊµÅÊ∞¥Á∫øÂπ∂Ë°åÁöÑdraftÂíåÈ™åËØÅÊú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑEESDÊñπÊ≥ïÁõ∏ÊØîÔºåPPSDËÉΩÂ§üÂÖÖÂàÜÂà©Áî®ËÆ°ÁÆóËµÑÊ∫êÔºåÈÅøÂÖçÂú®Êú™Ë¢´Êé•ÂèóÁöÑdraft token‰∏äÊµ™Ë¥πËÆ°ÁÆóÔºå‰ªéËÄåÊòæËëóÊèêÈ´òÊé®ÁêÜÊïàÁéá„ÄÇÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåPPSDÂ∞ÜdraftÂíåÈ™åËØÅËøáÁ®ãÂπ∂Ë°åÂåñÔºåËÄå‰º†ÁªüÊñπÊ≥ïÊòØ‰∏≤Ë°åÊâßË°å„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöPPSDÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂ¶Ç‰ΩïÂ∞ÜÊ®°ÂûãÂ±ÇÈÖçÁΩÆ‰∏∫ÊµÅÊ∞¥Á∫øÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂÆûÁé∞draftÂíåÈ™åËØÅÁöÑ‰∫§ÈîôÊâßË°å„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÂèØËÉΩÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑLLMËøõË°åË∞ÉÊï¥Ôºå‰ΩÜÊ†∏ÂøÉÊÄùÊÉ≥ÊòØ‰øùËØÅdraftÂíåÈ™åËØÅËøáÁ®ãËÉΩÂ§üÂ∞ΩÂèØËÉΩÂú∞Âπ∂Ë°åËøõË°åÔºå‰ªéËÄåÊúÄÂ§ßÂåñËÆ°ÁÆóËµÑÊ∫êÁöÑÂà©Áî®Áéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPPSDÂú®Â§öÁßçÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÂä†ÈÄüÊïàÊûúÔºåÂä†ÈÄüÊØîÂú®2.01x~3.81x‰πãÈó¥„ÄÇËøô‰∏ÄÁªìÊûúË°®ÊòéÔºåPPSDËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òLLMÁöÑÊé®ÁêÜÊïàÁéáÔºåÂπ∂‰∏îÂú®Âõ∫ÂÆöÁöÑÊé•ÂèóÁéáÂíåÈÄÄÂá∫‰ΩçÁΩÆ‰∏äÔºåPPSDÂá†‰πéËé∑Âæó‰∫ÜÊúÄ‰Ω≥ÁöÑÂä†ÈÄüÊïàÊûúÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®È´òÊïàËá™Êé®ÊµãÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PPSDÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÂä†ÈÄüÂêÑÁßçÂü∫‰∫éLLMÁöÑÂ∫îÁî®ÔºåÂ¶ÇÊú∫Âô®ÁøªËØë„ÄÅÊñáÊú¨ÊëòË¶Å„ÄÅÂØπËØùÁîüÊàêÁ≠â„ÄÇÈÄöËøáÈôç‰ΩéLLMÁöÑÊé®ÁêÜÊàêÊú¨ÔºåPPSDÂèØ‰ª•‰ΩøÂæóËøô‰∫õÂ∫îÁî®Âú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠‰πüËÉΩÈ´òÊïàËøêË°åÔºåÂπ∂‰øÉËøõLLMÂú®Êõ¥Â§öÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) deliver impressive generation quality, but incur very high inference cost because each output token is generated auto-regressively through all model layers. Early-exit based self-speculative decoding (EESD) has emerged to mitigate this cost. However, in practice, many approaches struggle to achieve the expected acceleration in such draft-then-verify paradigm even with a well-aligned early-exit head and selected exit position. Our analysis reveals that EESD only pays off when the vast majority of draft tokens are accepted by the LLM. Otherwise, the draft cost may overcome the acceleration gain and lead to a negative speedup. To mitigate this, we propose Pipeline-Parallel Self-Speculative Decoding (PPSD) that fully pipelines the draft and verification work so that no effort is wasted on failed predictions. It has two key innovations. We configure the model layers as a pipeline in which early-exit (draft) computations and remaining-layer (verification) computations overlap. We interleave drafting and verification per token. While the LLM is verifying the current token in its final layers, the early-exit path simultaneously drafts the next token. Such a verify-while-draft scheme keeps all units busy and validates tokens on-the-fly analogous to pipelining the speculation and verification stages. Empirical results confirm that PPSD achieves state-of-the-art acceleration in self-speculative LLM inference. On diverse benchmarks, PPSD achieves speedup ratios in the range of 2.01x~3.81x, which gains almost the optimal acceleration at the fixed acceptance rate and exit position, showcasing its advancement in providing efficient self-speculation.

