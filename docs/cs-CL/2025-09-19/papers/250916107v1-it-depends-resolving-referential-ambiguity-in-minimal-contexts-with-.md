---
layout: default
title: It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge
---

# It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16107" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16107v1</a>
  <a href="https://arxiv.org/pdf/2509.16107.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16107v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16107v1', 'It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lukas Ellinger, Georg Groh

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: Accepted by UncertaiNLP workshop @ EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ€å°ä¸Šä¸‹æ–‡ä¸­åˆ©ç”¨å¸¸è¯†çŸ¥è¯†è§£å†³æŒ‡ä»£æ¶ˆè§£æ­§ä¹‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒ‡ä»£æ¶ˆè§£` `æ­§ä¹‰å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¸¸è¯†æ¨ç†` `å¤šè½®å¯¹è¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æŒ‡ä»£æ¶ˆè§£æ­§ä¹‰æ—¶ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨å¸¸è¯†çŸ¥è¯†å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´ç†è§£åå·®ã€‚
2. è¯¥ç ”ç©¶é€šè¿‡æ„å»ºå¤šè¯­è¨€æ•°æ®é›†ï¼Œå¹¶åˆ†æLLMåœ¨ä¸åŒç®€åŒ–ç¨‹åº¦æç¤ºä¸‹çš„è¡¨ç°ï¼Œæ­ç¤ºäº†å…¶åœ¨æ­§ä¹‰å¤„ç†ä¸Šçš„å±€é™æ€§ã€‚
3. é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–å¾®è°ƒLlama-3.1-8Bæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å…¶åœ¨å„ç§è¯·æ±‚ç±»å‹ä¸‹çš„æ­§ä¹‰æ¶ˆè§£èƒ½åŠ›ï¼ŒéªŒè¯äº†å¾®è°ƒçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ­§ä¹‰è¯æˆ–æ¬ æ˜ç¡®çš„æŒ‡ä»£éœ€è¦å¯¹è¯è€…é€šè¿‡ä¾èµ–å…±äº«ä¸Šä¸‹æ–‡å’Œå¸¸è¯†çŸ¥è¯†æ¥è§£å†³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦å¯ä»¥åˆ©ç”¨å¸¸è¯†æ¥è§£å†³å¤šè½®å¯¹è¯ä¸­çš„æŒ‡ä»£æ­§ä¹‰ï¼Œå¹¶åˆ†æäº†å½“æ­§ä¹‰æŒç»­å­˜åœ¨æ—¶å®ƒä»¬çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç®€åŒ–è¯­è¨€çš„è¯·æ±‚å¦‚ä½•å½±å“è¿™ç§èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç§æ–°çš„å¤šè¯­è¨€è¯„ä¼°æ•°æ®é›†ï¼Œé€šè¿‡LLM-as-Judgeå’Œäººå·¥æ ‡æ³¨æ¥æµ‹è¯•DeepSeek v3ã€GPT-4oã€Qwen3-32Bã€GPT-4o-miniå’ŒLlama-3.1-8Bã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„LLMéš¾ä»¥æœ‰æ•ˆåœ°è§£å†³æ­§ä¹‰ï¼šå®ƒä»¬å€¾å‘äºåšæŒå•ä¸€çš„è§£é‡Šæˆ–æ¶µç›–æ‰€æœ‰å¯èƒ½çš„æŒ‡ä»£ï¼Œè€Œä¸æ˜¯å›é¿æˆ–å¯»æ±‚æ¾„æ¸…ã€‚è¿™ç§å±€é™æ€§åœ¨ç®€åŒ–æç¤ºä¸‹å˜å¾—æ›´åŠ æ˜æ˜¾ï¼Œè¿™å¤§å¤§å‡å°‘äº†å¸¸è¯†æ¨ç†å’Œå¤šæ ·åŒ–å“åº”ç­–ç•¥çš„ä½¿ç”¨ã€‚ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–å¯¹Llama-3.1-8Bè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ‰€æœ‰è¯·æ±‚ç±»å‹çš„æ­§ä¹‰æ¶ˆè§£èƒ½åŠ›ã€‚è¿™äº›ç»“æœå¼ºè°ƒéœ€è¦è¿›è¡Œé«˜çº§å¾®è°ƒï¼Œä»¥æé«˜LLMå¤„ç†æ­§ä¹‰çš„èƒ½åŠ›ï¼Œå¹¶ç¡®ä¿åœ¨ä¸åŒçš„æ²Ÿé€šé£æ ¼ä¸­å…·æœ‰å¼ºå¤§çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ€å°ä¸Šä¸‹æ–‡ä¸­ï¼Œç”±äºç¼ºä¹å¸¸è¯†çŸ¥è¯†è€Œæ— æ³•æœ‰æ•ˆè§£å†³æŒ‡ä»£æ¶ˆè§£æ­§ä¹‰çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ­§ä¹‰æ—¶ï¼Œè¦ä¹ˆå€¾å‘äºå•ä¸€è§£é‡Šï¼Œè¦ä¹ˆè¦†ç›–æ‰€æœ‰å¯èƒ½æŒ‡ä»£ï¼Œç¼ºä¹çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç®€åŒ–è¯­è¨€çš„è¯·æ±‚ä¼šè¿›ä¸€æ­¥é™ä½æ¨¡å‹çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ï¼ŒåŠ å‰§æ­§ä¹‰å¤„ç†çš„å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç³»ç»Ÿæ€§çš„å®éªŒè¯„ä¼°ï¼Œæ­ç¤ºç°æœ‰LLMåœ¨æŒ‡ä»£æ¶ˆè§£æ­§ä¹‰æ–¹é¢çš„ä¸è¶³ï¼Œå¹¶æ¢ç´¢é€šè¿‡å¾®è°ƒæ¥æå‡å…¶æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡æ„å»ºå¤šè¯­è¨€æ•°æ®é›†ï¼Œæ¨¡æ‹ŸçœŸå®å¯¹è¯åœºæ™¯ä¸­çš„æ­§ä¹‰æƒ…å†µï¼Œå¹¶åˆ†æLLMåœ¨ä¸åŒæç¤ºä¸‹çš„å“åº”ç­–ç•¥ã€‚ç„¶åï¼Œåˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimization, DPOï¼‰å¯¹Llama-3.1-8Bæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”æ­§ä¹‰æ¶ˆè§£ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) æ„å»ºå¤šè¯­è¨€è¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…å«å…·æœ‰æŒ‡ä»£æ­§ä¹‰çš„å¯¹è¯ç‰‡æ®µï¼›2) é€‰æ‹©å¤šä¸ªä»£è¡¨æ€§çš„LLMï¼ˆå¦‚DeepSeek v3ã€GPT-4oã€Qwen3-32Bã€GPT-4o-miniå’ŒLlama-3.1-8Bï¼‰è¿›è¡Œæµ‹è¯•ï¼›3) ä½¿ç”¨LLM-as-Judgeå’Œäººå·¥æ ‡æ³¨ä¸¤ç§æ–¹å¼è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼›4) å¯¹Llama-3.1-8Bæ¨¡å‹è¿›è¡ŒDPOå¾®è°ƒï¼›5) æ¯”è¾ƒå¾®è°ƒå‰åæ¨¡å‹åœ¨æ­§ä¹‰æ¶ˆè§£ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†ç°æœ‰LLMåœ¨æŒ‡ä»£æ¶ˆè§£æ­§ä¹‰æ–¹é¢çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å…¶åœ¨å¸¸è¯†æ¨ç†å’Œä¸Šä¸‹æ–‡ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼›2) æå‡ºäº†é€šè¿‡DPOå¾®è°ƒæ¥æå‡LLMæ­§ä¹‰æ¶ˆè§£èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ï¼›3) æ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€è¯„ä¼°æ•°æ®é›†ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†åŸºå‡†ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶æ›´æ³¨é‡å¯¹LLMåœ¨çœŸå®å¯¹è¯åœºæ™¯ä¸­æ­§ä¹‰å¤„ç†èƒ½åŠ›çš„è¯„ä¼°å’Œæå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒè®¾è®¡æ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨äº†ä¸åŒç±»å‹çš„æç¤ºï¼ˆåŒ…æ‹¬ç®€åŒ–è¯­è¨€çš„è¯·æ±‚ï¼‰æ¥æµ‹è¯•LLMçš„é²æ£’æ€§ã€‚åœ¨DPOå¾®è°ƒæ–¹é¢ï¼Œè®ºæ–‡å¯èƒ½éœ€è¦ä»”ç»†è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œä»¥å¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´å‡†ç¡®çš„æŒ‡ä»£æ¶ˆè§£ç­–ç•¥ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ­¤å¤„æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰LLMåœ¨å¤„ç†æŒ‡ä»£æ¶ˆè§£æ­§ä¹‰æ—¶è¡¨ç°ä¸ä½³ï¼Œå€¾å‘äºé€‰æ‹©å•ä¸€è§£é‡Šæˆ–è¦†ç›–æ‰€æœ‰å¯èƒ½æŒ‡ä»£ã€‚åœ¨ç®€åŒ–æç¤ºä¸‹ï¼Œæ¨¡å‹æ€§èƒ½è¿›ä¸€æ­¥ä¸‹é™ã€‚ç„¶è€Œï¼Œé€šè¿‡DPOå¾®è°ƒLlama-3.1-8Bæ¨¡å‹ï¼Œåœ¨æ‰€æœ‰è¯·æ±‚ç±»å‹ä¸‹ï¼Œæ­§ä¹‰æ¶ˆè§£èƒ½åŠ›å¾—åˆ°æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å¾®è°ƒçš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¢æœã€å¯¹è¯ç³»ç»Ÿã€æœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸï¼Œæå‡æœºå™¨åœ¨å¤æ‚è¯­å¢ƒä¸‹çš„ç†è§£èƒ½åŠ›ï¼Œå‡å°‘å› æ­§ä¹‰å¯¼è‡´çš„é”™è¯¯ã€‚é€šè¿‡æé«˜LLMçš„æ­§ä¹‰æ¶ˆè§£èƒ½åŠ›ï¼Œå¯ä»¥æ”¹å–„äººæœºäº¤äº’ä½“éªŒï¼Œä½¿æœºå™¨æ›´è‡ªç„¶ã€å‡†ç¡®åœ°ç†è§£äººç±»æ„å›¾ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½åŒ–çš„æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Ambiguous words or underspecified references require interlocutors to resolve them, often by relying on shared context and commonsense knowledge. Therefore, we systematically investigate whether Large Language Models (LLMs) can leverage commonsense to resolve referential ambiguity in multi-turn conversations and analyze their behavior when ambiguity persists. Further, we study how requests for simplified language affect this capacity. Using a novel multilingual evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that current LLMs struggle to resolve ambiguity effectively: they tend to commit to a single interpretation or cover all possible references, rather than hedging or seeking clarification. This limitation becomes more pronounced under simplification prompts, which drastically reduce the use of commonsense reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct Preference Optimization substantially improves ambiguity resolution across all request types. These results underscore the need for advanced fine-tuning to improve LLMs' handling of ambiguity and to ensure robust performance across diverse communication styles.

