---
layout: default
title: BEFT: Bias-Efficient Fine-Tuning of Language Models
---

# BEFT: Bias-Efficient Fine-Tuning of Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15974" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15974v1</a>
  <a href="https://arxiv.org/pdf/2509.15974.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15974v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15974v1', 'BEFT: Bias-Efficient Fine-Tuning of Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Baichuan Huang, Ananth Balashankar, Amir Aminifar

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BEFTï¼šä¸€ç§é«˜æ•ˆåç½®é¡¹å¾®è°ƒæ–¹æ³•ï¼Œæå‡è¯­è¨€æ¨¡å‹åœ¨ä½æ•°æ®åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `åç½®é¡¹å¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹å¾®è°ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åç½®é¡¹å¾®è°ƒæ–¹æ³•ç¼ºä¹æœ‰æ•ˆæŒ‡å¯¼ï¼Œéš¾ä»¥é€‰æ‹©åˆé€‚çš„åç½®é¡¹ä»¥ä¼˜åŒ–ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚
2. BEFTé€šè¿‡é€‰æ‹©åˆé€‚çš„åç½®é¡¹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæé«˜å‚æ•°æ•ˆç‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒBEFTåœ¨å¤šç§LLMå’Œä¸‹æ¸¸ä»»åŠ¡ä¸­ä¼˜äºå…¶ä»–åç½®é€‰æ‹©æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åç½®é¡¹å¾®è°ƒæ–¹æ³•ï¼ˆBEFTï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ä¸­ï¼Œç‰¹åˆ«æ˜¯åç½®é¡¹å¾®è°ƒæ—¶ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„åç½®é¡¹ä»¥è·å¾—æœ€ä½³ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„é—®é¢˜ã€‚å°½ç®¡å…¨åç½®é¡¹å¾®è°ƒå› å…¶æ˜“ç”¨æ€§å’Œç«äº‰åŠ›è€Œå¤‡å—å…³æ³¨ï¼Œä½†ä¸åŒåç½®é¡¹ï¼ˆå¦‚æŸ¥è¯¢ã€é”®æˆ–å€¼æŠ•å½±ä¸­çš„åç½®é¡¹ï¼‰çš„å¾®è°ƒä¸ä¸‹æ¸¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»å°šä¸æ˜ç¡®ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºåç½®å˜åŒ–å¹…åº¦æˆ–ç»éªŒFisherä¿¡æ¯çš„æ–¹æ³•ï¼Œåœ¨é€‰æ‹©æœ‰æ•ˆå¾®è°ƒçš„ç‰¹å®šåç½®é¡¹æ–¹é¢æä¾›çš„æŒ‡å¯¼æœ‰é™ã€‚æœ¬æ–‡é€šè¿‡é€‰æ‹©åˆé€‚çš„åç½®é¡¹è¿›è¡Œå¾®è°ƒï¼Œæ„å»ºäº†BEFTçš„åŸºç¡€ã€‚åœ¨åŒ…å«ç¼–ç å™¨å’Œè§£ç å™¨æ¶æ„ã€å‚æ•°è§„æ¨¡ä»1.1äº¿åˆ°67äº¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šï¼Œæˆ‘ä»¬å¹¿æ³›è¯„ä¼°äº†BEFTä¸å…¶ä»–åç½®é€‰æ‹©æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼ŒBEFTåœ¨åˆ†ç±»ã€å¤šé¡¹é€‰æ‹©å’Œç”Ÿæˆç­‰å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ä¸­ï¼Œå…¨åç½®é¡¹å¾®è°ƒè™½ç„¶æ˜“ç”¨ä¸”æ€§èƒ½æœ‰ç«äº‰åŠ›ï¼Œä½†ä¸åŒåç½®é¡¹å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“å°šä¸æ˜ç¡®ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚åŸºäºåç½®å˜åŒ–å¹…åº¦æˆ–Fisherä¿¡æ¯ï¼‰åœ¨é€‰æ‹©ç”¨äºå¾®è°ƒçš„ç‰¹å®šåç½®é¡¹æ—¶æŒ‡å¯¼æ€§ä¸è¶³ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBEFTçš„æ ¸å¿ƒåœ¨äºæå‡ºä¸€ç§é€‰æ‹©ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®æŸç§æŒ‡æ ‡æˆ–ç®—æ³•ï¼Œç¡®å®šå“ªäº›åç½®é¡¹çš„å¾®è°ƒèƒ½å¤Ÿå¸¦æ¥æœ€å¤§çš„æ€§èƒ½æå‡ã€‚é€šè¿‡åªå¾®è°ƒè¿™äº›å…³é”®åç½®é¡¹ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å‚æ•°æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBEFTæ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š1ï¼‰åç½®é¡¹é€‰æ‹©é˜¶æ®µï¼šä½¿ç”¨æå‡ºçš„é€‰æ‹©ç­–ç•¥ï¼Œè¯„ä¼°ä¸åŒåç½®é¡¹çš„é‡è¦æ€§ï¼Œå¹¶é€‰æ‹©ä¸€éƒ¨åˆ†è¿›è¡Œå¾®è°ƒã€‚2ï¼‰å¾®è°ƒé˜¶æ®µï¼šä»…å¯¹é€‰æ‹©çš„åç½®é¡¹è¿›è¡Œå¾®è°ƒï¼Œä¼˜åŒ–æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ•´ä½“æ¡†æ¶ç®€æ´é«˜æ•ˆï¼Œæ˜“äºå®ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šBEFTçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æœ‰æ•ˆçš„åç½®é¡¹é€‰æ‹©ç­–ç•¥ã€‚è¯¥ç­–ç•¥å¯èƒ½åŸºäºæŸç§æ¢¯åº¦ä¿¡æ¯ã€æ¿€æ´»ç»Ÿè®¡æˆ–å…¶ä»–ä¸ä»»åŠ¡ç›¸å…³çš„æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ä¸åŒåç½®é¡¹å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ã€‚é€šè¿‡è¿™ç§é€‰æ‹©æœºåˆ¶ï¼ŒBEFTèƒ½å¤Ÿé¿å…å¯¹æ‰€æœ‰åç½®é¡¹è¿›è¡Œç›²ç›®å¾®è°ƒï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„åç½®é¡¹é€‰æ‹©ç­–ç•¥æ˜¯BEFTçš„å…³é”®è®¾è®¡ã€‚è®ºæ–‡ä¸­å¯èƒ½è¯¦ç»†æè¿°äº†ç”¨äºè¯„ä¼°åç½®é¡¹é‡è¦æ€§çš„æŒ‡æ ‡ï¼Œä»¥åŠé€‰æ‹©åç½®é¡¹çš„å…·ä½“ç®—æ³•ã€‚æ­¤å¤–ï¼Œå¾®è°ƒé˜¶æ®µçš„å­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨ç­‰è¶…å‚æ•°è®¾ç½®ä¹Ÿå¯èƒ½å¯¹æœ€ç»ˆæ€§èƒ½äº§ç”Ÿå½±å“ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å–å†³äºä¸‹æ¸¸ä»»åŠ¡çš„ç±»å‹ï¼ˆåˆ†ç±»ã€ç”Ÿæˆç­‰ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBEFTåœ¨å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå‚æ•°è§„æ¨¡ä»1.1äº¿åˆ°67äº¿ï¼‰å’Œä¸‹æ¸¸ä»»åŠ¡ï¼ˆåˆ†ç±»ã€å¤šé¡¹é€‰æ‹©å’Œç”Ÿæˆï¼‰ä¸­å‡ä¼˜äºå…¶ä»–åç½®é€‰æ‹©æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦å¼ºè°ƒäº†BEFTçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BEFTå¯åº”ç”¨äºå„ç§å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™æˆ–æ•°æ®é‡è¾ƒå°‘çš„åœºæ™¯ä¸‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜å¾®è°ƒæ•ˆç‡ï¼Œé™ä½æˆæœ¬ï¼Œå¹¶æå‡æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning all-bias-terms stands out among various parameter-efficient fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and competitive performance, especially in low-data regimes. Bias-only fine-tuning has the potential for unprecedented parameter efficiency. However, the link between fine-tuning different bias terms (i.e., bias terms in the query, key, or value projections) and downstream performance remains unclear. The existing approaches, e.g., based on the magnitude of bias change or empirical Fisher information, provide limited guidance for selecting the particular bias term for effective fine-tuning. In this paper, we propose an approach for selecting the bias term to be fine-tuned, forming the foundation of our bias-efficient fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against other bias-selection approaches, across a wide range of large language models (LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B parameters. Our results demonstrate the effectiveness and superiority of our bias-efficient approach on diverse downstream tasks, including classification, multiple-choice, and generation tasks.

