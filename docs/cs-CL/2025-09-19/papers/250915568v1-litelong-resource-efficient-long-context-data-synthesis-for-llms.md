---
layout: default
title: LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs
---

# LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15568" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15568v1</a>
  <a href="https://arxiv.org/pdf/2509.15568.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15568v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15568v1', 'LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junlong Jia, Xing Wu, Chaochen Gao, Ziyang Chen, Zijia Lin, Zhongzhi Li, Weinong Wang, Haotian Xu, Donghui Jin, Debing Zhang, Binghui Guo

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LiteLongï¼šä¸€ç§èµ„æºé«˜æ•ˆçš„é•¿æ–‡æœ¬æ•°æ®åˆæˆæ–¹æ³•ï¼Œç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æœ¬æ•°æ®åˆæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ™ºèƒ½ä½“è¾©è®º` `ä¸»é¢˜ç»„ç»‡` `BM25æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é•¿æ–‡æœ¬æ•°æ®åˆæˆæ–¹æ³•è®¡ç®—æ•ˆç‡ä½ï¼Œéš¾ä»¥æ»¡è¶³å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒçš„éœ€æ±‚ã€‚
2. LiteLongé€šè¿‡ç»“æ„åŒ–ä¸»é¢˜ç»„ç»‡å’Œå¤šæ™ºèƒ½ä½“è¾©è®ºï¼Œé«˜æ•ˆåˆæˆé«˜è´¨é‡é•¿æ–‡æœ¬æ•°æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLiteLongåœ¨é•¿æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”æ˜“äºä¸å…¶ä»–é•¿ä¾èµ–å¢å¼ºæ–¹æ³•é›†æˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜è´¨é‡çš„é•¿æ–‡æœ¬æ•°æ®å¯¹äºè®­ç»ƒèƒ½å¤Ÿå¤„ç†å¤§é‡æ–‡æ¡£çš„å¤§å‹è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºç›¸å…³æ€§èšåˆçš„åˆæˆæ–¹æ³•é¢ä¸´è®¡ç®—æ•ˆç‡çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†LiteLongï¼Œä¸€ç§èµ„æºé«˜æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–çš„ä¸»é¢˜ç»„ç»‡å’Œå¤šæ™ºèƒ½ä½“è¾©è®ºæ¥åˆæˆé•¿æ–‡æœ¬æ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨BISACå›¾ä¹¦åˆ†ç±»ç³»ç»Ÿæä¾›å…¨é¢çš„åˆ†å±‚ä¸»é¢˜ç»„ç»‡ï¼Œç„¶åé‡‡ç”¨å¤šLLMè¾©è®ºæœºåˆ¶ï¼Œåœ¨è¯¥ç»“æ„å†…ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„ä¸»é¢˜ã€‚å¯¹äºæ¯ä¸ªä¸»é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨è½»é‡çº§çš„BM25æ£€ç´¢æ¥è·å–ç›¸å…³æ–‡æ¡£ï¼Œå¹¶å°†å®ƒä»¬è¿æ¥æˆ128K tokençš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨HELMETå’ŒRuleråŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLiteLongå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„é•¿æ–‡æœ¬æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥ä¸å…¶ä»–é•¿ä¾èµ–å¢å¼ºæ–¹æ³•æ— ç¼é›†æˆã€‚LiteLongé€šè¿‡é™ä½è®¡ç®—å’Œæ•°æ®å·¥ç¨‹æˆæœ¬ï¼Œä½¿é«˜è´¨é‡çš„é•¿æ–‡æœ¬æ•°æ®åˆæˆæ›´æ˜“äºè®¿é—®ï¼Œä»è€Œä¿ƒè¿›é•¿æ–‡æœ¬è¯­è¨€è®­ç»ƒçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ï¼Œé«˜è´¨é‡é•¿æ–‡æœ¬æ•°æ®åˆæˆæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºç›¸å…³æ€§èšåˆçš„æ–¹æ³•ï¼Œåœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œä½¿å¾—é•¿æ–‡æœ¬æ•°æ®çš„è·å–æˆæœ¬å¾ˆé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLiteLongçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“æ„åŒ–çš„ä¸»é¢˜ç»„ç»‡å’Œå¤šæ™ºèƒ½ä½“è¾©è®ºæ¥æé«˜é•¿æ–‡æœ¬æ•°æ®åˆæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚é¦–å…ˆï¼Œåˆ©ç”¨BISACå›¾ä¹¦åˆ†ç±»ç³»ç»Ÿæ„å»ºåˆ†å±‚ä¸»é¢˜ç»“æ„ï¼Œç„¶ååˆ©ç”¨å¤šä¸ªLLMè¿›è¡Œè¾©è®ºï¼Œç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä¸»é¢˜ï¼Œæœ€ååŸºäºè¿™äº›ä¸»é¢˜æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶æ‹¼æ¥æˆé•¿æ–‡æœ¬æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLiteLongçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **ä¸»é¢˜ç»„ç»‡**ï¼šåˆ©ç”¨BISACå›¾ä¹¦åˆ†ç±»ç³»ç»Ÿæ„å»ºåˆ†å±‚ä¸»é¢˜ç»“æ„ã€‚2) **ä¸»é¢˜ç”Ÿæˆ**ï¼šä½¿ç”¨å¤šä¸ªLLMè¿›è¡Œè¾©è®ºï¼Œåœ¨ä¸»é¢˜ç»“æ„å†…ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä¸»é¢˜ã€‚3) **æ–‡æ¡£æ£€ç´¢**ï¼šå¯¹äºæ¯ä¸ªç”Ÿæˆçš„ä¸»é¢˜ï¼Œä½¿ç”¨è½»é‡çº§çš„BM25æ£€ç´¢ç®—æ³•è·å–ç›¸å…³æ–‡æ¡£ã€‚4) **æ•°æ®åˆæˆ**ï¼šå°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ‹¼æ¥æˆ128K tokençš„é•¿æ–‡æœ¬è®­ç»ƒæ ·æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šLiteLongçš„å…³é”®åˆ›æ–°åœ¨äºå…¶èµ„æºé«˜æ•ˆçš„é•¿æ–‡æœ¬æ•°æ®åˆæˆæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºç›¸å…³æ€§èšåˆçš„æ–¹æ³•ç›¸æ¯”ï¼ŒLiteLongé€šè¿‡ç»“æ„åŒ–çš„ä¸»é¢˜ç»„ç»‡å’Œå¤šæ™ºèƒ½ä½“è¾©è®ºï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬å’Œæ•°æ®å·¥ç¨‹æˆæœ¬ï¼ŒåŒæ—¶ä¿è¯äº†ç”Ÿæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šLiteLongçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨BISACå›¾ä¹¦åˆ†ç±»ç³»ç»Ÿä½œä¸ºä¸»é¢˜ç»„ç»‡çš„å…ˆéªŒçŸ¥è¯†ï¼Œé¿å…äº†ä»é›¶å¼€å§‹æ„å»ºä¸»é¢˜ç»“æ„çš„å¤æ‚æ€§ã€‚2) é‡‡ç”¨å¤šLLMè¾©è®ºæœºåˆ¶ï¼Œé¼“åŠ±ç”Ÿæˆå¤šæ ·åŒ–çš„ä¸»é¢˜ï¼Œé¿å…äº†å•ä¸€LLMç”Ÿæˆä¸»é¢˜çš„åå·®ã€‚3) ä½¿ç”¨è½»é‡çº§çš„BM25æ£€ç´¢ç®—æ³•ï¼Œé™ä½äº†æ–‡æ¡£æ£€ç´¢çš„è®¡ç®—æˆæœ¬ã€‚4) å°†é•¿æ–‡æœ¬æ ·æœ¬å›ºå®šä¸º128K tokenï¼Œæ–¹ä¾¿åç»­çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LiteLongåœ¨HELMETå’ŒRuleråŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„é•¿æ–‡æœ¬æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä¸å…¶ä»–é•¿ä¾èµ–å¢å¼ºæ–¹æ³•æ— ç¼é›†æˆï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚LiteLongé™ä½äº†è®¡ç®—å’Œæ•°æ®å·¥ç¨‹æˆæœ¬ï¼Œä½¿å¾—é«˜è´¨é‡é•¿æ–‡æœ¬æ•°æ®åˆæˆæ›´æ˜“äºè®¿é—®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LiteLongå¯åº”ç”¨äºå„ç§éœ€è¦é•¿æ–‡æœ¬ç†è§£çš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒåœºæ™¯ï¼Œä¾‹å¦‚é•¿æ–‡æ¡£æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚è¯¥æ–¹æ³•é™ä½äº†é•¿æ–‡æœ¬æ•°æ®åˆæˆçš„æˆæœ¬ï¼Œä½¿å¾—æ›´å¤šç ”ç©¶è€…å’Œå¼€å‘è€…èƒ½å¤Ÿè®­ç»ƒå‡ºå…·æœ‰ä¼˜ç§€é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›çš„æ¨¡å‹ï¼Œä»è€Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> High-quality long-context data is essential for training large language models (LLMs) capable of processing extensive documents, yet existing synthesis approaches using relevance-based aggregation face challenges of computational efficiency. We present LiteLong, a resource-efficient method for synthesizing long-context data through structured topic organization and multi-agent debate. Our approach leverages the BISAC book classification system to provide a comprehensive hierarchical topic organization, and then employs a debate mechanism with multiple LLMs to generate diverse, high-quality topics within this structure. For each topic, we use lightweight BM25 retrieval to obtain relevant documents and concatenate them into 128K-token training samples. Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves competitive long-context performance and can seamlessly integrate with other long-dependency enhancement methods. LiteLong makes high-quality long-context data synthesis more accessible by reducing both computational and data engineering costs, facilitating further research in long-context language training.

