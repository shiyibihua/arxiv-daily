---
layout: default
title: Making Large Language Models Efficient Dense Retrievers
---

# Making Large Language Models Efficient Dense Retrievers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20612" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20612v1</a>
  <a href="https://arxiv.org/pdf/2512.20612.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20612v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20612v1', 'Making Large Language Models Efficient Dense Retrievers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yibin Lei, Shwai He, Ang Li, Andrew Yates

**åˆ†ç±»**: cs.IR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEffiRæ¡†æ¶ï¼Œé€šè¿‡MLPå‹ç¼©æå‡LLMå¯†é›†æ£€ç´¢å™¨çš„æ•ˆç‡ï¼Œä¿æŒæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯†é›†æ£€ç´¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `å±‚å†—ä½™` `MLPå‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„å¯†é›†æ£€ç´¢å™¨å‚æ•°é‡å·¨å¤§ï¼Œè®¡ç®—æ•ˆç‡ä½ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. EffiRæ¡†æ¶é€šè¿‡åˆ†æLLMå±‚å†—ä½™ï¼Œé‡ç‚¹å‹ç¼©MLPå±‚ï¼Œä¿ç•™å…³é”®çš„æ³¨æ„åŠ›å±‚ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒEffiRåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å¤§å°å’Œæ¨ç†æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç›´æ¥å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºå¯†é›†æ£€ç´¢å¯ä»¥è·å¾—å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å…¶åºå¤§çš„å‚æ•°é‡å¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶æ­ç¤ºäº†LLMåœ¨ç”Ÿæˆä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—çš„å±‚å†—ä½™ï¼Œä½†å½“è¿™äº›æ¨¡å‹è¢«ç”¨äºæ£€ç´¢ä»»åŠ¡æ—¶ï¼Œæ˜¯å¦å­˜åœ¨ç±»ä¼¼çš„å†—ä½™ä»ç„¶ä¸æ¸…æ¥šï¼Œå› ä¸ºæ£€ç´¢ä»»åŠ¡éœ€è¦å°†æ•´ä¸ªåºåˆ—ç¼–ç æˆå›ºå®šçš„è¡¨ç¤ºï¼Œè€Œä¸æ˜¯è¿­ä»£åœ°ç”Ÿæˆtokenã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹åŸºäºLLMçš„å¯†é›†æ£€ç´¢å™¨ä¸­çš„å±‚å†—ä½™è¿›è¡Œäº†å…¨é¢çš„åˆ†æã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸ç”Ÿæˆè®¾ç½®ç›¸æ¯”ï¼ŒMLPå±‚æ›´æ˜“äºä¿®å‰ªï¼Œè€Œæ³¨æ„åŠ›å±‚å¯¹äºè¯­ä¹‰èšåˆä»ç„¶è‡³å…³é‡è¦ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†EffiRï¼Œä¸€ä¸ªç”¨äºå¼€å‘é«˜æ•ˆæ£€ç´¢å™¨çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç²—åˆ°ç²¾çš„ç­–ç•¥ï¼ˆç²—ç²’åº¦çš„æ·±åº¦ç¼©å‡ï¼Œç„¶åæ˜¯ç»†ç²’åº¦çš„å®½åº¦ç¼©å‡ï¼‰æ‰§è¡Œå¤§è§„æ¨¡çš„MLPå‹ç¼©ï¼Œå¹¶ç»“åˆç‰¹å®šäºæ£€ç´¢çš„å¾®è°ƒã€‚åœ¨ä¸åŒçš„BEIRæ•°æ®é›†å’ŒLLMéª¨å¹²ç½‘ç»œä¸Šï¼ŒEffiRåœ¨ä¿æŒå…¨å°ºå¯¸æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å¤§å°å’Œæ¨ç†æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå¯†é›†æ£€ç´¢å™¨æ—¶è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç›´æ¥å¾®è°ƒæ•´ä¸ªLLMï¼Œå‚æ•°é‡å·¨å¤§ï¼Œæ¨ç†æˆæœ¬é«˜æ˜‚ã€‚è™½ç„¶LLMåœ¨ç”Ÿæˆä»»åŠ¡ä¸­å­˜åœ¨å±‚å†—ä½™å·²è¢«å‘ç°ï¼Œä½†æ£€ç´¢ä»»åŠ¡çš„å±‚å†—ä½™æƒ…å†µå°šä¸æ˜ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å‘ç°å¹¶åˆ©ç”¨LLMåœ¨æ£€ç´¢ä»»åŠ¡ä¸­çš„å±‚å†—ä½™ï¼Œç‰¹åˆ«æ˜¯MLPå±‚çš„å†—ä½™ï¼Œé€šè¿‡å‹ç¼©MLPå±‚æ¥é™ä½æ¨¡å‹å¤§å°å’Œæ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™æ³¨æ„åŠ›å±‚ä»¥ç»´æŒè¯­ä¹‰èšåˆèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEffiRæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦é˜¶æ®µï¼š1) å±‚å†—ä½™åˆ†æï¼šåˆ†æLLMä¸­ä¸åŒå±‚çš„å¯ä¿®å‰ªæ€§ï¼Œå‘ç°MLPå±‚æ›´æ˜“äºä¿®å‰ªã€‚2) ç²—ç²’åº¦æ·±åº¦ç¼©å‡ï¼šç§»é™¤éƒ¨åˆ†MLPå±‚ï¼Œé™ä½æ¨¡å‹æ·±åº¦ã€‚3) ç»†ç²’åº¦å®½åº¦ç¼©å‡ï¼šå¯¹å‰©ä½™çš„MLPå±‚è¿›è¡Œæƒé‡å‰ªæï¼Œè¿›ä¸€æ­¥é™ä½æ¨¡å‹å®½åº¦ã€‚4) æ£€ç´¢ç‰¹å®šå¾®è°ƒï¼šå¯¹å‹ç¼©åçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ¢å¤æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå‘ç°äº†LLMåœ¨æ£€ç´¢ä»»åŠ¡ä¸­MLPå±‚å’Œæ³¨æ„åŠ›å±‚ä¸åŒçš„é‡è¦æ€§ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†é’ˆå¯¹æ€§çš„å‹ç¼©ç­–ç•¥ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒEffiRä¸æ˜¯å‡åŒ€åœ°å‹ç¼©æ‰€æœ‰å±‚ï¼Œè€Œæ˜¯é‡ç‚¹å‹ç¼©å†—ä½™çš„MLPå±‚ï¼Œä¿ç•™å…³é”®çš„æ³¨æ„åŠ›å±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šEffiRé‡‡ç”¨äº†ç²—åˆ°ç²¾çš„å‹ç¼©ç­–ç•¥ï¼Œé¦–å…ˆè¿›è¡Œæ·±åº¦ç¼©å‡ï¼Œç„¶åè¿›è¡Œå®½åº¦ç¼©å‡ã€‚æ·±åº¦ç¼©å‡é€šè¿‡ç§»é™¤æ•´ä¸ªMLPå±‚æ¥å®ç°ï¼Œå®½åº¦ç¼©å‡é€šè¿‡æƒé‡å‰ªææ¥å®ç°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ä½¿ç”¨äº†æ£€ç´¢ç‰¹å®šçš„å¾®è°ƒæ–¹æ³•ï¼Œä¾‹å¦‚å¯¹æ¯”å­¦ä¹ ï¼Œä»¥ä¼˜åŒ–å‹ç¼©åçš„æ¨¡å‹ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20612v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20612v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20612v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEffiRåœ¨å¤šä¸ªBEIRæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ä¸åŒçš„LLMéª¨å¹²ç½‘ç»œï¼Œéƒ½èƒ½åœ¨ä¿æŒå…¨å°ºå¯¸æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½æ¨¡å‹å¤§å°å’Œæ¨ç†æˆæœ¬ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§ä¿¡æ¯æ£€ç´¢åœºæ™¯ï¼Œä¾‹å¦‚æœç´¢å¼•æ“ã€é—®ç­”ç³»ç»Ÿã€æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡é™ä½LLMå¯†é›†æ£€ç´¢å™¨çš„è®¡ç®—æˆæœ¬ï¼Œå¯ä»¥ä½¿å…¶æ›´å®¹æ˜“éƒ¨ç½²åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¹¶æé«˜æ£€ç´¢æ•ˆç‡ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.

