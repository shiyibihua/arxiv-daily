---
layout: default
title: M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation
---

# M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20136" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20136v1</a>
  <a href="https://arxiv.org/pdf/2512.20136.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20136v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20136v1', 'M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hyeongcheol Park, Jiyoung Seo, Jaewon Mun, Hogun Park, Wonmin Byeon, Sung June Kim, Hyeonsoo Im, JeungSub Lee, Sangpil Kim

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM$^3$KG-RAGï¼Œé€šè¿‡å¤šè·³å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±å¢å¼ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæå‡MLLMåœ¨è§†å¬é¢†åŸŸçš„æ¨ç†å’Œ grounding èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `çŸ¥è¯†å›¾è°±` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¤šè·³æ¨ç†` `è§†å¬ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MMKGåœ¨è§†å¬é¢†åŸŸå­˜åœ¨æ¨¡æ€è¦†ç›–ä¸è¶³å’Œå¤šè·³è¿æ¥æ€§æœ‰é™çš„é—®é¢˜ï¼Œé™åˆ¶äº†å¤šæ¨¡æ€RAGçš„æ€§èƒ½ã€‚
2. M$^3$KG-RAGé€šè¿‡æ„å»ºå¤šè·³MMKGå¹¶å¼•å…¥GRASPæœºåˆ¶ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„çŸ¥è¯†æ£€ç´¢å’Œå†—ä½™ä¿¡æ¯çš„è¿‡æ»¤ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒM$^3$KG-RAGæ˜¾è‘—æå‡äº†MLLMåœ¨å¤šæ¨¡æ€æ¨ç†å’Œ grounding æ–¹é¢çš„èƒ½åŠ›ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æœ€è¿‘æ‰©å±•åˆ°å¤šæ¨¡æ€è®¾ç½®ï¼Œå°†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)ä¸æµ·é‡çš„å¤–éƒ¨çŸ¥è¯†è¯­æ–™åº“ï¼ˆå¦‚å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±(MMKG)ï¼‰è¿æ¥èµ·æ¥ã€‚å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†è§†å¬é¢†åŸŸçš„å¤šæ¨¡æ€RAGä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºï¼š1)ç°æœ‰MMKGçš„æ¨¡æ€è¦†ç›–èŒƒå›´å’Œå¤šè·³è¿æ¥æ€§æœ‰é™ï¼›2)ä»…åŸºäºå…±äº«å¤šæ¨¡æ€åµŒå…¥ç©ºé—´ä¸­çš„ç›¸ä¼¼æ€§è¿›è¡Œæ£€ç´¢ï¼Œæ— æ³•è¿‡æ»¤æ‰ç¦»é¢˜æˆ–å†—ä½™çš„çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†M$^3$KG-RAGï¼Œä¸€ç§å¤šè·³å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±å¢å¼ºçš„RAGï¼Œå®ƒä»MMKGä¸­æ£€ç´¢ä¸æŸ¥è¯¢å¯¹é½çš„è§†å¬çŸ¥è¯†ï¼Œä»è€Œæé«˜MLLMçš„æ¨ç†æ·±åº¦å’Œç­”æ¡ˆçš„å¿ å®æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„å¤šæ™ºèƒ½ä½“æµæ°´çº¿æ¥æ„å»ºå¤šè·³MMKG (M$^3$KG)ï¼Œå…¶ä¸­åŒ…å«ä¸Šä¸‹æ–‡ä¸°å¯Œçš„å¤šæ¨¡æ€å®ä½“ä¸‰å…ƒç»„ï¼Œä»è€Œèƒ½å¤ŸåŸºäºè¾“å…¥æŸ¥è¯¢è¿›è¡Œæ¨¡æ€æ£€ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†GRASP (Grounded Retrieval And Selective Pruning)ï¼Œå®ƒç¡®ä¿äº†å¯¹æŸ¥è¯¢çš„ç²¾ç¡®å®ä½“ groundingï¼Œè¯„ä¼°äº†ç­”æ¡ˆæ”¯æŒçš„ç›¸å…³æ€§ï¼Œå¹¶ä¿®å‰ªå†—ä½™ä¸Šä¸‹æ–‡ï¼Œåªä¿ç•™ç”Ÿæˆå“åº”æ‰€éœ€çš„çŸ¥è¯†ã€‚åœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒM$^3$KG-RAGæ˜¾è‘—å¢å¼ºäº†MLLMçš„å¤šæ¨¡æ€æ¨ç†å’Œ grounding èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€RAGåœ¨è§†å¬é¢†åŸŸé¢ä¸´çš„çŸ¥è¯†è¦†ç›–ä¸è¶³å’Œæ£€ç´¢ç²¾åº¦ä¸é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæœ‰é™çš„MMKGï¼Œå¹¶ä¸”æ£€ç´¢è¿‡ç¨‹å®¹æ˜“å¼•å…¥æ— å…³æˆ–å†—ä½™ä¿¡æ¯ï¼Œå¯¼è‡´MLLMçš„æ¨ç†èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºæ›´å…¨é¢ã€è¿æ¥æ€§æ›´å¼ºçš„å¤šè·³MMKGï¼Œå¹¶ç»“åˆç²¾ç¡®çš„å®ä½“ grounding å’Œé€‰æ‹©æ€§å‰ªæç­–ç•¥ï¼Œæ¥æå‡æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œä»è€Œå¢å¼ºMLLMçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM$^3$KG-RAGåŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šè·³MMKG (M$^3$KG) æ„å»ºæ¨¡å—ï¼Œé‡‡ç”¨å¤šæ™ºèƒ½ä½“æµæ°´çº¿ï¼Œä»å¤šæºæ•°æ®ä¸­æå–ä¸Šä¸‹æ–‡ä¸°å¯Œçš„å¤šæ¨¡æ€å®ä½“ä¸‰å…ƒç»„ï¼›2) GRASP (Grounded Retrieval And Selective Pruning) æ¨¡å—ï¼Œç”¨äºå¯¹æŸ¥è¯¢è¿›è¡Œå®ä½“ groundingï¼Œè¯„ä¼°çŸ¥è¯†çš„ç›¸å…³æ€§ï¼Œå¹¶å‰ªæå†—ä½™ä¿¡æ¯ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼šè¾“å…¥æŸ¥è¯¢ -> M$^3$KGæ£€ç´¢ -> GRASPå¤„ç† -> MLLMç”Ÿæˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†è½»é‡çº§çš„å¤šæ™ºèƒ½ä½“æµæ°´çº¿ï¼Œç”¨äºæ„å»ºå¤šè·³MMKGï¼Œæ‰©å±•äº†çŸ¥è¯†è¦†ç›–èŒƒå›´å’Œè¿æ¥æ€§ï¼›2) å¼•å…¥äº†GRASPæœºåˆ¶ï¼Œé€šè¿‡å®ä½“ grounding å’Œé€‰æ‹©æ€§å‰ªæï¼Œæé«˜äº†æ£€ç´¢çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒM$^3$KG-RAGèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ï¼Œæå‡MLLMçš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå¤šæ™ºèƒ½ä½“æµæ°´çº¿åŒ…å«å¤šä¸ªagentï¼Œåˆ†åˆ«è´Ÿè´£ä»ä¸åŒæ¨¡æ€çš„æ•°æ®ä¸­æå–å®ä½“å’Œå…³ç³»ã€‚GRASPæ¨¡å—ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå®ä½“ groundingï¼Œå¹¶è®¾è®¡äº†ç›¸å…³æ€§è¯„ä¼°å‡½æ•°æ¥è¡¡é‡çŸ¥è¯†ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚å‰ªæç­–ç•¥åŸºäºç›¸å…³æ€§å¾—åˆ†ï¼Œç§»é™¤å†—ä½™æˆ–æ— å…³çš„çŸ¥è¯†ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20136v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20136v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20136v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒM$^3$KG-RAGåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨XXXæ•°æ®é›†ä¸Šï¼ŒM$^3$KG-RAGçš„å‡†ç¡®ç‡æé«˜äº†XX%ï¼ŒF1å€¼æé«˜äº†YY%ã€‚è¿™äº›ç»“æœè¯æ˜äº†M$^3$KG-RAGåœ¨å¤šæ¨¡æ€æ¨ç†å’Œ grounding æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è§†å¬å†…å®¹ç†è§£ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¢æœä¸­ï¼Œå¯ä»¥åˆ©ç”¨M$^3$KG-RAGä»æµ·é‡çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®ã€æ›´å…¨é¢çš„ç­”æ¡ˆã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£å‘¨å›´ç¯å¢ƒï¼Œå¹¶åšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.

