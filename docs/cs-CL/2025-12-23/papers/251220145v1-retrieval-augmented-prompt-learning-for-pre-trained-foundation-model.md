---
layout: default
title: Retrieval-augmented Prompt Learning for Pre-trained Foundation Models
---

# Retrieval-augmented Prompt Learning for Pre-trained Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20145" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20145v1</a>
  <a href="https://arxiv.org/pdf/2512.20145.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20145v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20145v1', 'Retrieval-augmented Prompt Learning for Pre-trained Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV, cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

**å¤‡æ³¨**: IEEE/ACM Transactions on Audio, Speech and Language Processing

**DOI**: [10.1109/TASLPRO.2025.3608936](https://doi.org/10.1109/TASLPRO.2025.3608936)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRetroPromptï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºæç¤ºå­¦ä¹ æå‡é¢„è®­ç»ƒæ¨¡å‹æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºå­¦ä¹ ` `æç¤ºå­¦ä¹ ` `é¢„è®­ç»ƒæ¨¡å‹` `å°‘æ ·æœ¬å­¦ä¹ ` `çŸ¥è¯†åº“` `æ³›åŒ–èƒ½åŠ›` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿæç¤ºå­¦ä¹ æ–¹æ³•åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­å­˜åœ¨è¿‡åº¦ä¾èµ–è®°å¿†ã€æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é‡æœ‰é™çš„æƒ…å†µä¸‹ã€‚
2. RetroPromptçš„æ ¸å¿ƒæ€æƒ³æ˜¯è§£è€¦çŸ¥è¯†ä¸è®°å¿†ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºçš„æ–¹å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“æå‡æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRetroPromptåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹ï¼Œåœ¨NLPå’ŒCVä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†å¯¹æ­»è®°ç¡¬èƒŒçš„ä¾èµ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹(PFMs)å·²æˆä¸ºä¿ƒè¿›å¤§è§„æ¨¡å¤šæ¨¡æ€å­¦ä¹ çš„å…³é”®ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æç¤ºå­¦ä¹ æœ‰æ•ˆåœ°é‡‡ç”¨äº†â€œé¢„è®­ç»ƒã€æç¤ºå’Œé¢„æµ‹â€èŒƒå¼ï¼Œä»¥æé«˜å°‘æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼ŒPFMçš„æç¤ºå­¦ä¹ æ–¹æ³•ä»ç„¶éµå¾ªå‚æ•°å­¦ä¹ èŒƒå¼ï¼Œè¿™å¯èƒ½ä¼šæŸå®³è®°å¿†å’Œæ­»è®°ç¡¬èƒŒçš„æ³›åŒ–ç¨³å®šæ€§ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œä¼ ç»Ÿçš„æç¤ºå­¦ä¹ å¯èƒ½éš¾ä»¥å……åˆ†åˆ©ç”¨éå…¸å‹å®ä¾‹ï¼Œå¹¶é¿å…åœ¨å®Œå…¨ç›‘ç£è®­ç»ƒè¿‡ç¨‹ä¸­è¿‡åº¦æ‹Ÿåˆæœ‰é™æ•°æ®çš„æµ…å±‚æ¨¡å¼ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†RetroPromptæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°†çŸ¥è¯†ä¸å•çº¯çš„è®°å¿†åˆ†ç¦»æ¥å®ç°è®°å¿†å’Œæ³›åŒ–ä¹‹é—´çš„å¹³è¡¡ã€‚ä¸ä¼ ç»Ÿçš„æç¤ºæ–¹æ³•ä¸åŒï¼ŒRetroPromptåˆ©ç”¨ä»è®­ç»ƒæ•°æ®ç”Ÿæˆçš„å…¬å¼€çŸ¥è¯†åº“ï¼Œå¹¶åœ¨è¾“å…¥ã€è®­ç»ƒå’Œæ¨ç†é˜¶æ®µç»“åˆæ£€ç´¢æœºåˆ¶ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨ä»è¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºå¯ç”¨çš„çº¿ç´¢ã€‚æˆ‘ä»¬åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å„ç§æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬æå‡ºçš„æ–¹æ³•RetroPromptåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚é€šè¿‡å¯¹è®°å¿†æ¨¡å¼çš„è¯¦ç»†åˆ†æï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°RetroPromptæœ‰æ•ˆåœ°å‡å°‘äº†å¯¹æ­»è®°ç¡¬èƒŒçš„ä¾èµ–ï¼Œä»è€Œå¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œè¿‡åº¦ä¾èµ–å‚æ•°è®°å¿†ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ä¸­çš„æµ…å±‚æ¨¡å¼ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚æ¨¡å‹éš¾ä»¥å……åˆ†åˆ©ç”¨éå…¸å‹å®ä¾‹ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRetroPromptçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ£€ç´¢å¢å¼ºæœºåˆ¶ï¼Œå°†çŸ¥è¯†ä¸å•çº¯çš„è®°å¿†è§£è€¦ã€‚æ¨¡å‹ä¸å†ä»…ä»…ä¾èµ–è‡ªèº«å‚æ•°è®°å¿†ï¼Œè€Œæ˜¯èƒ½å¤Ÿä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å¹³è¡¡è®°å¿†å’Œæ³›åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRetroPromptçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **çŸ¥è¯†åº“æ„å»º**ï¼šåˆ©ç”¨è®­ç»ƒæ•°æ®æ„å»ºä¸€ä¸ªå…¬å¼€å¯è®¿é—®çš„çŸ¥è¯†åº“ã€‚2) **æ£€ç´¢æ¨¡å—**ï¼šåœ¨è¾“å…¥ã€è®­ç»ƒå’Œæ¨ç†é˜¶æ®µï¼Œæ¨¡å‹ä½¿ç”¨æ£€ç´¢æ¨¡å—ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚3) **æç¤ºå­¦ä¹ **ï¼šå°†æ£€ç´¢åˆ°çš„ä¿¡æ¯èå…¥åˆ°æç¤ºä¸­ï¼Œå¼•å¯¼æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚æ•´ä¸ªæµç¨‹æ—¨åœ¨åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æ¥å¢å¼ºæ¨¡å‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šRetroPromptçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ£€ç´¢æœºåˆ¶èå…¥åˆ°æç¤ºå­¦ä¹ ä¸­ï¼Œå®ç°äº†çŸ¥è¯†ä¸è®°å¿†çš„è§£è€¦ã€‚ä¸ä¼ ç»Ÿçš„æç¤ºå­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒRetroPromptä¸å†ä»…ä»…ä¾èµ–æ¨¡å‹è‡ªèº«çš„å‚æ•°è®°å¿†ï¼Œè€Œæ˜¯èƒ½å¤Ÿä¸»åŠ¨ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†å¯¹æ­»è®°ç¡¬èƒŒçš„ä¾èµ–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šRetroPromptçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **çŸ¥è¯†åº“çš„æ„å»ºæ–¹å¼**ï¼šçŸ¥è¯†åº“çš„æ„å»ºæ–¹å¼ä¼šå½±å“æ£€ç´¢çš„æ•ˆç‡å’Œè´¨é‡ã€‚è®ºæ–‡å¯èƒ½é‡‡ç”¨äº†ç‰¹å®šçš„æ•°æ®ç»“æ„æˆ–ç´¢å¼•æ–¹æ³•æ¥ä¼˜åŒ–çŸ¥è¯†åº“ã€‚2) **æ£€ç´¢æ¨¡å—çš„è®¾è®¡**ï¼šæ£€ç´¢æ¨¡å—éœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°æ‰¾åˆ°ä¸è¾“å…¥ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è®ºæ–‡å¯èƒ½é‡‡ç”¨äº†ç‰¹å®šçš„ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•æˆ–æ£€ç´¢ç®—æ³•ã€‚3) **æç¤ºèåˆæ–¹å¼**ï¼šå¦‚ä½•å°†æ£€ç´¢åˆ°çš„ä¿¡æ¯æœ‰æ•ˆåœ°èå…¥åˆ°æç¤ºä¸­ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå…³é”®çš„è®¾è®¡ç‚¹ã€‚è®ºæ–‡å¯èƒ½é‡‡ç”¨äº†ç‰¹å®šçš„èåˆç­–ç•¥ï¼Œä¾‹å¦‚æ³¨æ„åŠ›æœºåˆ¶æˆ–æ‹¼æ¥æ“ä½œã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20145v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20145v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20145v1/figs/cv_results.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

RetroPromptåœ¨å¤šä¸ªNLPå’ŒCVæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜å…¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é€šè¿‡å¯¹è®°å¿†æ¨¡å¼çš„åˆ†æï¼Œå‘ç°RetroPromptæœ‰æ•ˆå‡å°‘äº†å¯¹æ­»è®°ç¡¬èƒŒçš„ä¾èµ–ï¼Œä»è€Œå¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†æ•´ä½“è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RetroPromptæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ã€å›¾åƒè¯†åˆ«ã€æœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ä¸­ï¼Œå¯ä»¥åˆ©ç”¨RetroPromptæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºå°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.

