---
layout: default
title: Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles
---

# Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20324" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20324v1</a>
  <a href="https://arxiv.org/pdf/2512.20324.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20324v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20324v1', 'Can LLMs Solve My Grandma\'s Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nurul Labib Sayeedi, Md. Faiyaz Abdullah Sayeedi, Khushnur Binte Jahangir, Swakkhar Shatabda, Sarah Masud Preum

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Labib1610/BanglaRiddleEval)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BanglaRiddleEvalï¼šè¯„ä¼°å¤šè¯­è¨€å¤§æ¨¡å‹åœ¨å­ŸåŠ æ‹‰è¯­è°œè¯­æ¨ç†ä¸Šçš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å­ŸåŠ æ‹‰è¯­` `è°œè¯­æ¨ç†` `ä½èµ„æºè¯­è¨€` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨å½¢è±¡åŒ–ã€æ–‡åŒ–èƒŒæ™¯å’Œä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›æœ‰å¾…æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å­ŸåŠ æ‹‰è¯­ç­‰ä½èµ„æºè¯­è¨€ä¸­ã€‚
2. è®ºæ–‡æ„å»ºäº†BanglaRiddleEvalåŸºå‡†ï¼ŒåŒ…å«å­ŸåŠ æ‹‰è¯­ä¼ ç»Ÿè°œè¯­ï¼Œç”¨äºè¯„ä¼°LLMåœ¨æ¨ç†ã€æ­§ä¹‰æ¶ˆè§£ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰LLMåœ¨å­ŸåŠ æ‹‰è¯­è°œè¯­æ¨ç†æ–¹é¢ä¸äººç±»æ°´å¹³å­˜åœ¨å·®è·ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šNLPåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨å½¢è±¡åŒ–ã€å…·æœ‰æ–‡åŒ–åŸºç¡€å’Œä½èµ„æºç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥BanglaRiddleEvalæ¥è§£å†³å­ŸåŠ æ‹‰è¯­çš„è¿™ä¸€å·®è·ï¼ŒBanglaRiddleEvalæ˜¯ä¸€ä¸ªåŒ…å«1,244ä¸ªä¼ ç»Ÿå­ŸåŠ æ‹‰è¯­è°œè¯­çš„åŸºå‡†ï¼Œè¿™äº›è°œè¯­è¢«å®ä¾‹åŒ–ä¸ºå››ä¸ªä»»åŠ¡ï¼ˆæ€»å…±4,976ä¸ªè°œè¯­-ä»»åŠ¡ç»„åˆï¼‰ã€‚ä½¿ç”¨åŸºäºLLMçš„pipelineï¼Œæˆ‘ä»¬ç”Ÿæˆäº†æ€ç»´é“¾è§£é‡Šã€è¯­ä¹‰è¿è´¯çš„å¹²æ‰°é¡¹å’Œç»†ç²’åº¦çš„æ­§ä¹‰æ³¨é‡Šï¼Œå¹¶åœ¨ä¸åŒçš„promptç­–ç•¥ä¸‹è¯„ä¼°äº†ä¸€ç³»åˆ—å¼€æºå’Œé—­æºæ¨¡å‹ã€‚æ¨¡å‹åœ¨ç”Ÿæˆå¼é—®ç­”ä¸­å®ç°äº†é€‚åº¦çš„è¯­ä¹‰é‡å ï¼Œä½†æ­£ç¡®ç‡è¾ƒä½ï¼›å¤šé¡¹é€‰æ‹©é¢˜çš„å‡†ç¡®ç‡å³°å€¼ä»…ä¸ºçº¦56%ï¼Œè€Œäººç±»åŸºçº¿ä¸º83%ï¼›æ­§ä¹‰æ¶ˆè§£çš„èŒƒå›´ä»å¤§çº¦26%åˆ°68%ï¼Œé«˜è´¨é‡çš„è§£é‡Šä»…é™äºæœ€å¼ºçš„æ¨¡å‹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„LLMæ•è·äº†ä¸€äº›å­ŸåŠ æ‹‰è¯­è°œè¯­æ¨ç†æ‰€éœ€çš„çº¿ç´¢ï¼Œä½†ä¸äººç±»æ°´å¹³çš„æ€§èƒ½ç›¸å·®ç”šè¿œï¼Œä»è€Œå°†BanglaRiddleEvalç¡®ç«‹ä¸ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä½èµ„æºå½¢è±¡æ¨ç†æ–°åŸºå‡†ã€‚æ‰€æœ‰æ•°æ®ã€ä»£ç å’Œè¯„ä¼°è„šæœ¬éƒ½å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ï¼šhttps://github.com/Labib1610/BanglaRiddleEvalã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£å’Œè§£å†³ä¼ ç»Ÿå­ŸåŠ æ‹‰è¯­è°œè¯­æ–¹é¢çš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•åœ¨ä½èµ„æºã€æ–‡åŒ–ç›¸å…³çš„è¯­è¨€æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•å……åˆ†æ•æ‰è°œè¯­ä¸­çš„éšå–»ã€æ­§ä¹‰å’Œæ–‡åŒ–èƒŒæ™¯ã€‚è¿™å¯¼è‡´LLMsåœ¨å¤„ç†æ­¤ç±»ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°åŸºå‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨é’ˆå¯¹å­ŸåŠ æ‹‰è¯­è°œè¯­çš„è¯„ä¼°åŸºå‡†ï¼ˆBanglaRiddleEvalï¼‰ï¼Œå¹¶åˆ©ç”¨è¯¥åŸºå‡†æ¥ç³»ç»Ÿåœ°è¯„ä¼°å„ç§LLMsçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åˆ†æLLMsåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºå…¶åœ¨å¤„ç†ä½èµ„æºã€å½¢è±¡åŒ–è¯­è¨€æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›æ–¹å‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®é›†æ„å»ºï¼šæ”¶é›†å¹¶æ•´ç†1244ä¸ªä¼ ç»Ÿå­ŸåŠ æ‹‰è¯­è°œè¯­ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå››ä¸ªä¸åŒçš„ä»»åŠ¡ï¼ˆå¦‚ç”Ÿæˆå¼é—®ç­”ã€å¤šé¡¹é€‰æ‹©é¢˜ã€æ­§ä¹‰æ¶ˆè§£ç­‰ï¼‰ã€‚2) æ•°æ®å¢å¼ºï¼šåˆ©ç”¨LLMç”Ÿæˆæ€ç»´é“¾è§£é‡Šã€è¯­ä¹‰è¿è´¯çš„å¹²æ‰°é¡¹å’Œç»†ç²’åº¦çš„æ­§ä¹‰æ³¨é‡Šï¼Œä»¥æ‰©å……æ•°æ®é›†ã€‚3) æ¨¡å‹è¯„ä¼°ï¼šé€‰æ‹©ä¸€ç³»åˆ—å¼€æºå’Œé—­æºLLMsï¼Œå¹¶åœ¨BanglaRiddleEvalåŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œé‡‡ç”¨ä¸åŒçš„promptç­–ç•¥ã€‚4) ç»“æœåˆ†æï¼šåˆ†æLLMsåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶ä¸äººç±»åŸºçº¿è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°å…¶æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†BanglaRiddleEvalï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å­ŸåŠ æ‹‰è¯­è°œè¯­æ¨ç†çš„åŸºå‡†ï¼Œå¡«è¡¥äº†ä½èµ„æºè¯­è¨€æ¨ç†è¯„ä¼°çš„ç©ºç™½ã€‚2) åˆ©ç”¨LLMç”Ÿæˆæ€ç»´é“¾è§£é‡Šå’Œæ­§ä¹‰æ³¨é‡Šï¼Œä¸ºè°œè¯­æ¨ç†æä¾›äº†æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚3) ç³»ç»Ÿåœ°è¯„ä¼°äº†å„ç§LLMsåœ¨å­ŸåŠ æ‹‰è¯­è°œè¯­æ¨ç†ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºäº†å…¶åœ¨å¤„ç†ä½èµ„æºã€å½¢è±¡åŒ–è¯­è¨€æ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºæ–¹é¢ï¼Œè®ºæ–‡å°†æ¯ä¸ªè°œè¯­è½¬åŒ–ä¸ºå››ä¸ªä»»åŠ¡ï¼šç”Ÿæˆå¼é—®ç­”ï¼ˆGenerative QAï¼‰ã€å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQï¼‰ã€æ­§ä¹‰æ¶ˆè§£ï¼ˆAmbiguity Resolutionï¼‰å’Œè§£é‡Šç”Ÿæˆï¼ˆExplanation Generationï¼‰ã€‚åœ¨æ¨¡å‹è¯„ä¼°æ–¹é¢ï¼Œé‡‡ç”¨äº†ä¸åŒçš„promptç­–ç•¥ï¼Œä¾‹å¦‚zero-shotã€few-shotå’Œchain-of-thought promptingã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ç”Ÿæˆå¼é—®ç­”çš„è¯­ä¹‰é‡å åº¦ã€å¤šé¡¹é€‰æ‹©é¢˜çš„å‡†ç¡®ç‡å’Œæ­§ä¹‰æ¶ˆè§£çš„å‡†ç¡®ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰LLMåœ¨BanglaRiddleEvalåŸºå‡†ä¸Šçš„è¡¨ç°è¿œä½äºäººç±»æ°´å¹³ã€‚å¤šé¡¹é€‰æ‹©é¢˜çš„å‡†ç¡®ç‡å³°å€¼ä»…ä¸º56%ï¼Œè€Œäººç±»åŸºçº¿ä¸º83%ã€‚æ­§ä¹‰æ¶ˆè§£çš„å‡†ç¡®ç‡èŒƒå›´ä¸º26%è‡³68%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMè™½ç„¶èƒ½æ•æ‰åˆ°ä¸€äº›çº¿ç´¢ï¼Œä½†åœ¨å­ŸåŠ æ‹‰è¯­è°œè¯­æ¨ç†æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡LLMåœ¨ä½èµ„æºè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼šå¼€å‘æ›´æ™ºèƒ½çš„å­ŸåŠ æ‹‰è¯­èŠå¤©æœºå™¨äººã€æ•™è‚²è¾…åŠ©å·¥å…·å’Œæ–‡åŒ–é—äº§ä¿æŠ¤åº”ç”¨ã€‚æ­¤å¤–ï¼ŒBanglaRiddleEvalåŸºå‡†å¯ä»¥ä¿ƒè¿›å¯¹LLMåœ¨å½¢è±¡åŒ–è¯­è¨€ç†è§£æ–¹é¢çš„ç ”ç©¶ï¼Œå¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.

