---
layout: default
title: Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents
---

# Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20092" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20092v1</a>
  <a href="https://arxiv.org/pdf/2512.20092.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20092v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20092v1', 'Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiming Du, Baojun Wang, Yifan Xiang, Zhaowei Wang, Wenyu Huang, Boyang Xue, Bin Liang, Xingshan Zeng, Fei Mi, Haoli Bai, Lifeng Shang, Jeff Z. Pan, Yuxin Jiang, Kam-Fai Wong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Elvin-Yiming-Du/Memory-T1/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMemory-T1æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è§£å†³å¤šè½®å¯¹è¯Agentä¸­çš„æ—¶åºæ¨ç†éš¾é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ—¶åºæ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šè½®å¯¹è¯` `é•¿æ–‡æœ¬å»ºæ¨¡` `å¯¹è¯Agent`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é•¿æ–‡æœ¬æ¨¡å‹åœ¨å¤„ç†é•¿ç¨‹å¤šè½®å¯¹è¯æ—¶ï¼Œéš¾ä»¥å‡†ç¡®è¯†åˆ«æ—¶åºä¿¡æ¯ï¼Œå¯¼è‡´æ—¶åºæ¨ç†æ€§èƒ½ä¸‹é™ã€‚
2. Memory-T1æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œå­¦ä¹ æ—¶é—´æ„ŸçŸ¥çš„è®°å¿†é€‰æ‹©ç­–ç•¥ï¼Œä»å¯¹è¯å†å²ä¸­é€‰æ‹©æœ€ç›¸å…³çš„è¯æ®ä¼šè¯ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMemory-T1åœ¨Time-DialogåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå¹¶åœ¨é•¿æ–‡æœ¬è¾“å…¥ä¸‹ä¿æŒäº†é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºMemory-T1æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼šè¯Agentåœ¨é•¿ç¨‹å¤šè½®å¯¹è¯ä¸­è¿›è¡Œæ—¶åºæ¨ç†æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å†—é•¿ä¸”åŒ…å«å™ªå£°çš„å¯¹è¯å†å²æ—¶ï¼Œéš¾ä»¥å‡†ç¡®è¯†åˆ«æ—¶åºç›¸å…³ä¿¡æ¯ï¼Œä¸¥é‡å½±å“æ¨ç†æ€§èƒ½ã€‚Memory-T1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ (RL)æ–¹æ³•å­¦ä¹ æ—¶é—´æ„ŸçŸ¥çš„è®°å¿†é€‰æ‹©ç­–ç•¥ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ç”±ç²—åˆ°ç²¾çš„ç­–ç•¥ï¼Œé¦–å…ˆé€šè¿‡æ—¶é—´å’Œç›¸å…³æ€§è¿‡æ»¤å™¨å°†å¯¹è¯å†å²ä¿®å‰ªä¸ºå€™é€‰é›†ï¼Œç„¶åç”±RL Agenté€‰æ‹©ç²¾ç¡®çš„è¯æ®ä¼šè¯ã€‚RLè®­ç»ƒç”±å¤šçº§å¥–åŠ±å‡½æ•°æŒ‡å¯¼ï¼Œä¼˜åŒ–(i)ç­”æ¡ˆå‡†ç¡®æ€§ï¼Œ(ii)è¯æ®åŸºç¡€ï¼Œä»¥åŠ(iii)æ—¶é—´ä¸€è‡´æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ—¶é—´ä¸€è‡´æ€§å¥–åŠ±é€šè¿‡è¯„ä¼°ä¼šè¯çº§åˆ«ï¼ˆæ—¶é—´é‚»è¿‘åº¦ï¼‰å’Œè¯è¯­çº§åˆ«ï¼ˆæ—¶é—´ä¿çœŸåº¦ï¼‰ä¸æŸ¥è¯¢æ—¶é—´èŒƒå›´çš„å¯¹é½æƒ…å†µï¼Œæä¾›å¯†é›†ä¿¡å·ï¼Œä½¿Agentèƒ½å¤Ÿè§£å†³ç»†å¾®çš„æ—¶é—´æ­§ä¹‰ã€‚åœ¨Time-DialogåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMemory-T1å°†7Bæ¨¡å‹çš„æ•´ä½“å¾—åˆ†æé«˜åˆ°67.0%ï¼Œä¸ºå¼€æºæ¨¡å‹å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶ä¸”ä¼˜äº14BåŸºçº¿æ¨¡å‹10.2%ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæ—¶é—´ä¸€è‡´æ€§å’Œè¯æ®åŸºç¡€å¥–åŠ±å…±åŒè´¡çŒ®äº†15.0%çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼ŒMemory-T1åœ¨é«˜è¾¾128k tokensçš„æƒ…å†µä¸‹ä¿æŒäº†é²æ£’æ€§ï¼Œè€ŒåŸºçº¿æ¨¡å‹åˆ™å´©æºƒï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤§é‡å¯¹è¯å†å²ä¸­çš„å™ªå£°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¯¹è¯Agentåœ¨å¤„ç†é•¿ç¨‹å¤šè½®å¯¹è¯æ—¶ï¼Œç”±äºå¯¹è¯å†å²å†—é•¿ä¸”åŒ…å«å™ªå£°ï¼Œéš¾ä»¥å‡†ç¡®è¯†åˆ«ä¸æ—¶é—´ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æ—¶åºæ¨ç†èƒ½åŠ›ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåŒºåˆ†ä¸åŒæ—¶é—´æ®µçš„ä¿¡æ¯ï¼Œå®¹æ˜“å—åˆ°æ— å…³ä¿¡æ¯çš„å¹²æ‰°ï¼Œä»è€Œå½±å“æœ€ç»ˆçš„æ¨ç†ç»“æœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMemory-T1çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®æ—¶é—´ä¿¡æ¯é€‰æ‹©ç›¸å…³å¯¹è¯ä¼šè¯çš„Agentã€‚é€šè¿‡å­¦ä¹ æ—¶é—´æ„ŸçŸ¥çš„è®°å¿†é€‰æ‹©ç­–ç•¥ï¼ŒAgentå¯ä»¥ä»å†—é•¿çš„å¯¹è¯å†å²ä¸­æå–å‡ºä¸å½“å‰é—®é¢˜æœ€ç›¸å…³çš„è¯æ®ï¼Œä»è€Œæé«˜æ—¶åºæ¨ç†çš„å‡†ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»åœ¨å›å¿†ä¿¡æ¯æ—¶ï¼Œä¼šæ ¹æ®æ—¶é—´çº¿ç´¢è¿›è¡Œç­›é€‰çš„è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMemory-T1æ¡†æ¶é‡‡ç”¨ç”±ç²—åˆ°ç²¾çš„ç­–ç•¥ã€‚é¦–å…ˆï¼Œåˆ©ç”¨æ—¶é—´å’Œç›¸å…³æ€§è¿‡æ»¤å™¨å¯¹å¯¹è¯å†å²è¿›è¡Œåˆæ­¥ç­›é€‰ï¼Œå¾—åˆ°å€™é€‰ä¼šè¯é›†åˆã€‚ç„¶åï¼Œå¼ºåŒ–å­¦ä¹ Agentä»å€™é€‰é›†ä¸­é€‰æ‹©æœ€ç»ˆçš„è¯æ®ä¼šè¯ã€‚Agentçš„çŠ¶æ€åŒ…æ‹¬å¯¹è¯å†å²ã€å½“å‰é—®é¢˜å’Œå·²é€‰æ‹©çš„ä¼šè¯ã€‚Agentçš„åŠ¨ä½œæ˜¯é€‰æ‹©ä¸‹ä¸€ä¸ªä¼šè¯ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡å¤šçº§å¥–åŠ±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬ç­”æ¡ˆå‡†ç¡®æ€§å¥–åŠ±ã€è¯æ®åŸºç¡€å¥–åŠ±å’Œæ—¶é—´ä¸€è‡´æ€§å¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šMemory-T1çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†æ—¶é—´ä¸€è‡´æ€§å¥–åŠ±ï¼Œè¯¥å¥–åŠ±ä»ä¼šè¯çº§åˆ«ï¼ˆæ—¶é—´é‚»è¿‘åº¦ï¼‰å’Œè¯è¯­çº§åˆ«ï¼ˆæ—¶é—´ä¿çœŸåº¦ï¼‰è¯„ä¼°é€‰æ‹©çš„ä¼šè¯ä¸æŸ¥è¯¢æ—¶é—´èŒƒå›´çš„å¯¹é½æƒ…å†µã€‚è¿™ç§æ—¶é—´ä¸€è‡´æ€§å¥–åŠ±èƒ½å¤Ÿæä¾›å¯†é›†çš„åé¦ˆä¿¡å·ï¼Œå¸®åŠ©Agentå­¦ä¹ åŒºåˆ†ç»†å¾®çš„æ—¶é—´å·®å¼‚ï¼Œä»è€Œæ›´å‡†ç¡®åœ°é€‰æ‹©ç›¸å…³çš„è¯æ®ä¼šè¯ã€‚

**å…³é”®è®¾è®¡**ï¼šæ—¶é—´ä¸€è‡´æ€§å¥–åŠ±æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ï¼Œå®ƒåŒ…æ‹¬ä¼šè¯çº§åˆ«çš„æ—¶é—´é‚»è¿‘åº¦è¯„ä¼°å’Œè¯è¯­çº§åˆ«çš„æ—¶é—´ä¿çœŸåº¦è¯„ä¼°ã€‚æ—¶é—´é‚»è¿‘åº¦è¯„ä¼°ä¼šè¯çš„æ—¶é—´æˆ³ä¸æŸ¥è¯¢æ—¶é—´èŒƒå›´çš„æ¥è¿‘ç¨‹åº¦ã€‚æ—¶é—´ä¿çœŸåº¦è¯„ä¼°ä¼šè¯ä¸­è¯è¯­çš„æ—¶é—´é¡ºåºæ˜¯å¦ä¸æŸ¥è¯¢æ—¶é—´èŒƒå›´ä¸€è‡´ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜ä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æ¥èåˆé€‰æ‹©çš„è¯æ®ä¼šè¯ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç­”æ¡ˆå‡†ç¡®æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20092v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20092v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20092v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

Memory-T1åœ¨Time-DialogåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°†7Bæ¨¡å‹çš„æ•´ä½“å¾—åˆ†æé«˜åˆ°67.0%ï¼Œè¶…è¶Šäº†14BåŸºçº¿æ¨¡å‹10.2%ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæ—¶é—´ä¸€è‡´æ€§å’Œè¯æ®åŸºç¡€å¥–åŠ±å…±åŒè´¡çŒ®äº†15.0%çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼ŒMemory-T1åœ¨å¤„ç†é«˜è¾¾128k tokensçš„é•¿æ–‡æœ¬è¾“å…¥æ—¶ï¼Œä»ç„¶ä¿æŒäº†é²æ£’æ€§ï¼Œè€ŒåŸºçº¿æ¨¡å‹åˆ™å´©æºƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Memory-T1æ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦æ—¶åºæ¨ç†çš„å¯¹è¯Agentï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹ã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæé«˜Agentåœ¨å¤„ç†å¤æ‚å¯¹è¯åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®ã€æ›´ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤„ç†é•¿ç¨‹ä¾èµ–å…³ç³»çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/

