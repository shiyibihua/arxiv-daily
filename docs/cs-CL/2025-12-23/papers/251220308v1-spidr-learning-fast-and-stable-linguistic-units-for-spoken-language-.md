---
layout: default
title: SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision
---

# SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20308" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20308v1</a>
  <a href="https://arxiv.org/pdf/2512.20308.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20308v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20308v1', 'SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maxime Poli, Mahi Luthra, Youssef Benchekroun, Yosuke Higuchi, Martin Gleize, Jiayi Shen, Robin Algayres, Yu-An Chung, Mido Assran, Juan Pino, Emmanuel Dupoux

**åˆ†ç±»**: cs.CL, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

**å¤‡æ³¨**: 30 pages, 16 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/facebookresearch/spidr)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SpidRï¼šæ— éœ€ç›‘ç£ï¼Œå­¦ä¹ å¿«é€Ÿç¨³å®šçš„è¯­éŸ³å•å…ƒç”¨äºè¯­éŸ³è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `è¯­éŸ³è¡¨å¾å­¦ä¹ ` `è¯­éŸ³è¯­è¨€æ¨¡å‹` `è‡ªè’¸é¦` `åœ¨çº¿èšç±»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­éŸ³è¯­è¨€æ¨¡å‹ä¾èµ–æ–‡æœ¬ä¸­é—´æ­¥éª¤ï¼Œé™åˆ¶äº†å…¶æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒSpidRæ—¨åœ¨ç›´æ¥ä»è¯­éŸ³ä¸­å­¦ä¹ è¯­è¨€ã€‚
2. SpidRé€šè¿‡æ©ç é¢„æµ‹ã€è‡ªè’¸é¦å’Œåœ¨çº¿èšç±»ï¼Œå­¦ä¹ å…·æœ‰é«˜åº¦å¯è®¿é—®è¯­éŸ³ä¿¡æ¯çš„è¯­éŸ³è¡¨å¾ï¼Œæå‡ç æœ¬è´¨é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSpidRåœ¨å¤šä¸ªè¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†é¢„è®­ç»ƒæ—¶é—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­éŸ³è¯­è¨€å»ºæ¨¡å’Œè¯­éŸ³è¡¨å¾å­¦ä¹ çš„å¹¶è¡Œå‘å±•ï¼Œä½¿å¾—ç›´æ¥ä»è¯­éŸ³ä¸­å­¦ä¹ è¯­è¨€è€Œæ— éœ€æ–‡æœ¬ä¸­é—´æ­¥éª¤æˆä¸ºå¯èƒ½ã€‚æœ¬æ–‡æå‡ºäº†SpidRï¼Œä¸€ç§è‡ªç›‘ç£è¯­éŸ³è¡¨å¾æ¨¡å‹ï¼Œå®ƒèƒ½é«˜æ•ˆåœ°å­¦ä¹ å…·æœ‰é«˜åº¦å¯è®¿é—®è¯­éŸ³ä¿¡æ¯çš„è¡¨å¾ï¼Œç‰¹åˆ«é€‚åˆäºæ— æ–‡æœ¬è¯­éŸ³è¯­è¨€å»ºæ¨¡ã€‚SpidRä½¿ç”¨æ©ç é¢„æµ‹ç›®æ ‡ã€è‡ªè’¸é¦å’Œåœ¨çº¿èšç±»åœ¨åŸå§‹æ³¢å½¢ä¸Šè¿›è¡Œè®­ç»ƒã€‚å­¦ç”Ÿæ¨¡å‹çš„ä¸­é—´å±‚å­¦ä¹ é¢„æµ‹æ¥è‡ªæ•™å¸ˆæ¨¡å‹ä¸­é—´å±‚çš„åˆ†é…ã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§å­¦ä¹ ç›®æ ‡ç¨³å®šäº†åœ¨çº¿èšç±»è¿‡ç¨‹ï¼Œä»è€Œäº§ç”Ÿæ›´é«˜è´¨é‡çš„ç æœ¬ã€‚åœ¨ä¸‹æ¸¸è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ï¼ˆsWUGGYã€sBLIMPã€tSCï¼‰ä¸­ï¼ŒSpidRä¼˜äºwav2vec 2.0ã€HuBERTã€WavLMå’ŒDinoSRã€‚æ­¤å¤–ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†è¯­éŸ³å•å…ƒè´¨é‡ï¼ˆABXã€PNMIï¼‰ä¸è¯­è¨€å»ºæ¨¡æ€§èƒ½ä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒéªŒè¯äº†è¿™äº›æŒ‡æ ‡ä½œä¸ºå¯é ä»£ç†ã€‚æœ€åï¼Œä¸HuBERTç›¸æ¯”ï¼ŒSpidRæ˜¾è‘—å‡å°‘äº†é¢„è®­ç»ƒæ—¶é—´ï¼Œä»…éœ€åœ¨16ä¸ªGPUä¸Šé¢„è®­ç»ƒä¸€å¤©ï¼Œè€Œä¸æ˜¯ä¸€å‘¨ã€‚è¿™ç§åŠ é€Ÿå¾—ç›Šäºé¢„è®­ç»ƒæ–¹æ³•å’Œé«˜æ•ˆçš„ä»£ç åº“ï¼Œä»è€Œå¯ä»¥æ›´å¿«åœ°è¿­ä»£å’Œæ›´å®¹æ˜“åœ°è¿›è¡Œå®éªŒã€‚ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å·²å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•é«˜æ•ˆåœ°ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­å­¦ä¹ é«˜è´¨é‡çš„è¯­éŸ³è¡¨å¾ï¼Œç”¨äºæ— æ–‡æœ¬è¯­éŸ³è¯­è¨€å»ºæ¨¡ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚HuBERTï¼Œé¢„è®­ç»ƒæ—¶é—´é•¿ï¼Œä¸”åœ¨çº¿èšç±»è¿‡ç¨‹ä¸å¤Ÿç¨³å®šï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„è¯­éŸ³å•å…ƒè´¨é‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSpidRçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“åˆæ©ç é¢„æµ‹ã€è‡ªè’¸é¦å’Œåœ¨çº¿èšç±»ï¼Œç¨³å®šåœ¨çº¿èšç±»è¿‡ç¨‹ï¼Œä»è€Œå­¦ä¹ åˆ°å…·æœ‰é«˜åº¦å¯è®¿é—®è¯­éŸ³ä¿¡æ¯çš„è¯­éŸ³è¡¨å¾ã€‚è‡ªè’¸é¦è¿‡ç¨‹ä½¿å¾—å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å’Œç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpidRçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹å’Œä¸€ä¸ªæ•™å¸ˆæ¨¡å‹ã€‚é¦–å…ˆï¼Œå¯¹åŸå§‹è¯­éŸ³æ³¢å½¢è¿›è¡Œæ©ç å¤„ç†ã€‚ç„¶åï¼Œå­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹åˆ†åˆ«å¯¹æ©ç åçš„è¯­éŸ³è¿›è¡Œç¼–ç ã€‚å­¦ç”Ÿæ¨¡å‹çš„ä¸­é—´å±‚å­¦ä¹ é¢„æµ‹æ¥è‡ªæ•™å¸ˆæ¨¡å‹ä¸­é—´å±‚çš„èšç±»åˆ†é…ã€‚æœ€åï¼Œé€šè¿‡æ©ç é¢„æµ‹æŸå¤±å’Œè‡ªè’¸é¦æŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šSpidRçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨è‡ªè’¸é¦æ¥ç¨³å®šåœ¨çº¿èšç±»è¿‡ç¨‹ã€‚ä¼ ç»Ÿçš„åœ¨çº¿èšç±»æ–¹æ³•å®¹æ˜“å—åˆ°å™ªå£°å’Œåˆå§‹åŒ–å½±å“ï¼Œå¯¼è‡´èšç±»ç»“æœä¸ç¨³å®šã€‚é€šè¿‡è‡ªè’¸é¦ï¼Œå­¦ç”Ÿæ¨¡å‹å¯ä»¥å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ï¼Œä»è€Œæé«˜èšç±»ç»“æœçš„ç¨³å®šæ€§å’Œè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šSpidRçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ©ç é¢„æµ‹ç›®æ ‡æ¥å­¦ä¹ è¯­éŸ³è¡¨å¾ï¼›2) ä½¿ç”¨è‡ªè’¸é¦æ¥ç¨³å®šåœ¨çº¿èšç±»è¿‡ç¨‹ï¼›3) ä½¿ç”¨åœ¨çº¿èšç±»æ¥ç”Ÿæˆè¯­éŸ³å•å…ƒçš„ç æœ¬ï¼›4) ä½¿ç”¨ABXå’ŒPNMIæŒ‡æ ‡æ¥è¯„ä¼°è¯­éŸ³å•å…ƒçš„è´¨é‡ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°åŒ…æ‹¬æ©ç é¢„æµ‹æŸå¤±å’Œè‡ªè’¸é¦æŸå¤±ã€‚ç½‘ç»œç»“æ„åŸºäºTransformeræ¶æ„ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20308v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20308v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20308v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

SpidRåœ¨sWUGGYã€sBLIMPå’ŒtSCç­‰ä¸‹æ¸¸è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šä¼˜äºwav2vec 2.0ã€HuBERTã€WavLMå’ŒDinoSRç­‰åŸºçº¿æ¨¡å‹ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒSpidRæ˜¾è‘—å‡å°‘äº†é¢„è®­ç»ƒæ—¶é—´ï¼Œä»…éœ€åœ¨16ä¸ªGPUä¸Šè®­ç»ƒä¸€å¤©ï¼Œè€ŒHuBERTéœ€è¦ä¸€å‘¨ã€‚è¿™ä½¿å¾—SpidRæ›´æ˜“äºè®­ç»ƒå’Œéƒ¨ç½²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SpidRåœ¨è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆã€è¯­éŸ³ç¿»è¯‘ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºæ„å»ºç«¯åˆ°ç«¯çš„è¯­éŸ³å¤„ç†ç³»ç»Ÿï¼Œæ— éœ€æ–‡æœ¬æ ‡æ³¨æ•°æ®ï¼Œé™ä½äº†æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„æˆæœ¬ã€‚æ­¤å¤–ï¼ŒSpidRè¿˜å¯ä»¥ç”¨äºè·¨è¯­è¨€è¯­éŸ³å¤„ç†ï¼Œä¾‹å¦‚ï¼Œåœ¨ä¸€ç§è¯­è¨€ä¸Šè®­ç»ƒçš„æ¨¡å‹å¯ä»¥ç”¨äºå¦ä¸€ç§è¯­è¨€çš„è¯­éŸ³å¤„ç†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.

