---
layout: default
title: DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding
---

# DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21287" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21287v1</a>
  <a href="https://arxiv.org/pdf/2509.21287.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21287v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21287v1', 'DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kin Ian Lo, Hala Hawashin, Mina Abbaszadeh, Tilen Limback-Stokin, Hadi Wazni, Mehrnoosh Sadrzadeh

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DisCoCLIPï¼šä¸€ç§ç”¨äºè§†è§‰-è¯­è¨€ç†è§£çš„åˆ†å¸ƒç»„åˆå¼ é‡ç½‘ç»œç¼–ç å™¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€ç†è§£` `ç»„åˆæ¨ç†` `å¼ é‡ç½‘ç»œ` `å¥æ³•ç»“æ„` `CLIP` `è‡ªç›‘ç£å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹å¿½ç•¥äº†è¯­è¨€çš„ç»„åˆç»“æ„ï¼Œå¯¼è‡´åœ¨ç†è§£è¯åºå’Œè¯­ä¹‰å…³ç³»çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚
2. DisCoCLIPç»“åˆCLIPè§†è§‰Transformerå’Œå¼ é‡ç½‘ç»œæ–‡æœ¬ç¼–ç å™¨ï¼Œæ˜¾å¼ç¼–ç å¥å­çš„å¥æ³•ç»“æ„ï¼Œæå‡ç»„åˆæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDisCoCLIPåœ¨SVO-Probesã€AROç­‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹æ“…é•¿å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œä½†å¸¸å¸¸å¿½ç•¥è¯­è¨€çš„ç»„åˆç»“æ„ï¼Œå¯¼è‡´åœ¨ä¾èµ–è¯åºå’Œè°“è¯-è®ºå…ƒç»“æ„çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†DisCoCLIPï¼Œä¸€ç§å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œå®ƒç»“åˆäº†å†»ç»“çš„CLIPè§†è§‰Transformerå’Œä¸€ä¸ªæ–°é¢–çš„å¼ é‡ç½‘ç»œæ–‡æœ¬ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨æ˜¾å¼åœ°ç¼–ç äº†å¥æ³•ç»“æ„ã€‚å¥å­é€šè¿‡ç»„åˆèŒƒç•´è¯­æ³•è§£æå™¨è¿›è¡Œè§£æï¼Œäº§ç”Ÿåˆ†å¸ƒå¼çš„è¯å¼ é‡ï¼Œå…¶æ”¶ç¼©åæ˜ äº†å¥å­çš„è¯­æ³•æ¨å¯¼ã€‚ä¸ºäº†ä¿æŒæ¨¡å‹çš„æ•ˆç‡ï¼Œé«˜é˜¶å¼ é‡é€šè¿‡å¼ é‡åˆ†è§£è¿›è¡Œåˆ†è§£ï¼Œå°†å‚æ•°æ•°é‡ä»æ•°åƒä¸‡å‡å°‘åˆ°ä¸€ç™¾ä¸‡ä»¥ä¸‹ã€‚é€šè¿‡è‡ªç›‘ç£å¯¹æ¯”æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼ŒDisCoCLIPæ˜¾è‘—æé«˜äº†å¯¹åŠ¨è¯è¯­ä¹‰å’Œè¯åºçš„æ•æ„Ÿæ€§ï¼šå®ƒå°†CLIPçš„SVO-ProbesåŠ¨è¯å‡†ç¡®ç‡ä»77.6%æé«˜åˆ°82.4%ï¼Œå°†AROå±æ€§å’Œå…³ç³»åˆ†æ•°æé«˜äº†9%ä»¥ä¸Šå’Œ4%ï¼Œå¹¶åœ¨æ–°å¼•å…¥çš„SVO-SwapåŸºå‡†ä¸Šå®ç°äº†93.7%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¼ é‡ç½‘ç»œåµŒå…¥æ˜¾å¼çš„è¯­è¨€ç»“æ„å¯ä»¥äº§ç”Ÿå¯è§£é‡Šçš„ã€å‚æ•°é«˜æ•ˆçš„è¡¨ç¤ºï¼Œä»è€Œæ˜¾è‘—æé«˜è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­çš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¦‚CLIPï¼Œåœ¨å›¾åƒ-æ–‡æœ¬å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç†è§£è¯­è¨€çš„ç»„åˆæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚å®ƒä»¬éš¾ä»¥æ•æ‰è¯åºå’Œè°“è¯-è®ºå…ƒç»“æ„ç­‰å…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹è¯­è¨€ç»“æ„çš„æ˜¾å¼å»ºæ¨¡ï¼Œæ˜¯å…¶ç—›ç‚¹æ‰€åœ¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDisCoCLIPçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¥å­çš„å¥æ³•ç»“æ„æ˜¾å¼åœ°ç¼–ç åˆ°æ–‡æœ¬è¡¨ç¤ºä¸­ã€‚é€šè¿‡ä½¿ç”¨ç»„åˆèŒƒç•´è¯­æ³•ï¼ˆCCGï¼‰è§£æå™¨è§£æå¥å­ï¼Œå¹¶å°†æ¯ä¸ªè¯è¡¨ç¤ºä¸ºåˆ†å¸ƒå¼çš„å¼ é‡ï¼Œå¼ é‡çš„æ”¶ç¼©è¿‡ç¨‹æ¨¡æ‹Ÿäº†å¥å­çš„è¯­æ³•æ¨å¯¼è¿‡ç¨‹ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è¯ä¸è¯ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜ç»„åˆæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDisCoCLIPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå†»ç»“çš„CLIPè§†è§‰Transformerå’Œä¸€ä¸ªå¼ é‡ç½‘ç»œæ–‡æœ¬ç¼–ç å™¨ã€‚é¦–å…ˆï¼Œå›¾åƒé€šè¿‡CLIPè§†è§‰Transformeræå–è§†è§‰ç‰¹å¾ã€‚ç„¶åï¼Œå¥å­é€šè¿‡CCGè§£æå™¨è¿›è¡Œè§£æï¼Œç”Ÿæˆå¥æ³•æ ‘ã€‚æ¯ä¸ªè¯è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªå¼ é‡ï¼Œå¹¶é€šè¿‡å¼ é‡æ”¶ç¼©æ“ä½œå°†å¥æ³•æ ‘çš„ä¿¡æ¯ç¼–ç åˆ°å¥å­çš„è¡¨ç¤ºä¸­ã€‚æœ€åï¼Œä½¿ç”¨è‡ªç›‘ç£å¯¹æ¯”æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œä½¿è§†è§‰å’Œè¯­è¨€è¡¨ç¤ºå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šDisCoCLIPæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨å¼ é‡ç½‘ç»œæ¥æ˜¾å¼åœ°ç¼–ç å¥å­çš„å¥æ³•ç»“æ„ã€‚ä¸ä¼ ç»Ÿçš„æ–‡æœ¬ç¼–ç å™¨ç›¸æ¯”ï¼Œå¼ é‡ç½‘ç»œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¯ä¸è¯ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜ç»„åˆæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜æ¨¡å‹çš„æ•ˆç‡ï¼ŒDisCoCLIPä½¿ç”¨äº†å¼ é‡åˆ†è§£æŠ€æœ¯ï¼Œå‡å°‘äº†å‚æ•°æ•°é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šDisCoCLIPçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨CCGè§£æå™¨ç”Ÿæˆå¥æ³•æ ‘ï¼›2) å°†æ¯ä¸ªè¯è¡¨ç¤ºä¸ºä¸€ä¸ªå¼ é‡ï¼Œå¼ é‡çš„ç»´åº¦ä¸è¯çš„è¯­ä¹‰å’Œå¥æ³•è§’è‰²ç›¸å…³ï¼›3) ä½¿ç”¨å¼ é‡æ”¶ç¼©æ“ä½œå°†å¥æ³•æ ‘çš„ä¿¡æ¯ç¼–ç åˆ°å¥å­çš„è¡¨ç¤ºä¸­ï¼›4) ä½¿ç”¨å¼ é‡åˆ†è§£æŠ€æœ¯å‡å°‘å‚æ•°æ•°é‡ï¼›5) ä½¿ç”¨è‡ªç›‘ç£å¯¹æ¯”æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DisCoCLIPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨SVO-ProbesåŠ¨è¯å‡†ç¡®ç‡ä¸Šï¼ŒDisCoCLIPå°†CLIPçš„æ€§èƒ½ä»77.6%æé«˜åˆ°82.4%ã€‚åœ¨AROå±æ€§å’Œå…³ç³»åˆ†æ•°ä¸Šï¼ŒDisCoCLIPåˆ†åˆ«æå‡äº†9%ä»¥ä¸Šå’Œ4%ã€‚æ­¤å¤–ï¼ŒDisCoCLIPåœ¨SVO-SwapåŸºå‡†ä¸Šå®ç°äº†93.7%çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜å…¶å¯¹è¯åºçš„æ•æ„Ÿæ€§æ˜¾è‘—æé«˜ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDisCoCLIPèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜è§†è§‰-è¯­è¨€æ¨¡å‹çš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DisCoCLIPçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ã€ä»¥åŠéœ€è¦ç†è§£å¤æ‚è¯­è¨€ç»“æ„çš„æœºå™¨äººæ§åˆ¶ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹è¯­è¨€ç»„åˆæ€§çš„ç†è§£ï¼Œå¯ä»¥ä½¿æœºå™¨æ›´å¥½åœ°ç†è§£äººç±»çš„æ„å›¾ï¼Œä»è€Œå®ç°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„AIåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent vision-language models excel at large-scale image-text alignment but often neglect the compositional structure of language, leading to failures on tasks that hinge on word order and predicate-argument structure. We introduce DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer with a novel tensor network text encoder that explicitly encodes syntactic structure. Sentences are parsed with a Combinatory Categorial Grammar parser to yield distributional word tensors whose contractions mirror the sentence's grammatical derivation. To keep the model efficient, high-order tensors are factorized with tensor decompositions, reducing parameter count from tens of millions to under one million. Trained end-to-end with a self-supervised contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%, boosts ARO attribution and relation scores by over 9% and 4%, and achieves 93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that embedding explicit linguistic structure via tensor networks yields interpretable, parameter-efficient representations that substantially improve compositional reasoning in vision-language tasks.

