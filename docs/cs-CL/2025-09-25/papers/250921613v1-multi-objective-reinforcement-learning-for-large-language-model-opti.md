---
layout: default
title: Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective
---

# Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21613" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21613v1</a>
  <a href="https://arxiv.org/pdf/2509.21613.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21613v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21613v1', 'Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingxiao Kong, Cong Yang, Oya Deniz Beyan, Zeyd Boukhers

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**å¤‡æ³¨**: 3 pages, 1 figure, accepted by ECAI MODeM 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–ï¼Œæå‡ºå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ çš„è¿œæ™¯æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å…ƒç­–ç•¥å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–` `åŸºå‡†æµ‹è¯•æ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MORLæ–¹æ³•åœ¨åº”ç”¨äºLLMä¼˜åŒ–æ—¶ï¼Œç¼ºä¹æ•ˆç‡å’Œçµæ´»æ€§ï¼Œéš¾ä»¥é€‚åº”LLMçš„å¤æ‚æ€§å’Œä¸ªæ€§åŒ–éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºå…ƒç­–ç•¥çš„MORLæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŒå±‚å­¦ä¹ èŒƒå¼æå‡LLMä¼˜åŒ–è¿‡ç¨‹çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚
3. è®ºæ–‡æå‡ºäº†ä¸€ä¸ªMORLåŸºå‡†æµ‹è¯•æ¡†æ¶çš„æ„¿æ™¯ï¼Œç”¨äºè¯„ä¼°ä¸åŒMORLæ–¹æ³•å¯¹ä¸åŒç›®æ ‡å…³ç³»çš„å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ (MORL)ä¸ºä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸­çš„å¤šä¸ªç›®æ ‡å¸¦æ¥äº†æ˜¾è‘—çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§MORLåˆ†ç±»æ³•ï¼Œå¹¶ç ”ç©¶äº†å„ç§MORLæ–¹æ³•åº”ç”¨äºLLMä¼˜åŒ–æ—¶çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¼ºè°ƒäº†å¯¹é«˜æ•ˆã€çµæ´»ä¸”èƒ½é€‚åº”ä¸ªæ€§åŒ–åŠŸèƒ½ä»¥åŠLLMå’ŒRLå›ºæœ‰å¤æ‚æ€§çš„æ–¹æ³•çš„éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªMORLåŸºå‡†æµ‹è¯•æ¡†æ¶çš„æ„¿æ™¯ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ä¸åŒæ–¹æ³•å¯¹å¤šæ ·åŒ–ç›®æ ‡å…³ç³»çš„å½±å“ã€‚ä½œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå…ƒç­–ç•¥MORLçš„å¼€å‘ï¼Œå®ƒå¯ä»¥é€šè¿‡å…¶åŒå±‚å­¦ä¹ èŒƒå¼æ¥æé«˜æ•ˆç‡å’Œçµæ´»æ€§ï¼ŒåŒæ—¶å¼ºè°ƒäº†æ”¹è¿›LLMæ€§èƒ½çš„å…³é”®ç ”ç©¶é—®é¢˜å’Œæ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ–¹æ³•é€šå¸¸åªå…³æ³¨å•ä¸€ç›®æ ‡ï¼Œä¾‹å¦‚ç”Ÿæˆæ–‡æœ¬çš„æµç•…æ€§æˆ–å‡†ç¡®æ€§ï¼Œè€Œå¿½ç•¥äº†å…¶ä»–é‡è¦çš„ç›®æ ‡ï¼Œå¦‚å®‰å…¨æ€§ã€å…¬å¹³æ€§å’Œä¸ªæ€§åŒ–ã€‚ç°æœ‰çš„å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œé¢ä¸´ç€æ•ˆç‡ä½ä¸‹ã€éš¾ä»¥å¤„ç†é«˜ç»´çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ç­‰é—®é¢˜ï¼Œå¹¶ä¸”ç¼ºä¹è¶³å¤Ÿçš„çµæ´»æ€§æ¥é€‚åº”ä¸åŒç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å…ƒç­–ç•¥å­¦ä¹ æ¥æå‡å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ åœ¨ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æ—¶çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚å…ƒç­–ç•¥å­¦ä¹ é€šè¿‡å­¦ä¹ ä¸€ä¸ªç­–ç•¥çš„ç­–ç•¥ï¼Œå¯ä»¥å¿«é€Ÿé€‚åº”æ–°çš„ä»»åŠ¡å’Œç›®æ ‡ï¼Œä»è€Œé¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒçš„éœ€è¦ã€‚æ­¤å¤–ï¼Œå…ƒç­–ç•¥å­¦ä¹ è¿˜å¯ä»¥é€šè¿‡å­¦ä¹ ä¸åŒç›®æ ‡ä¹‹é—´çš„å…³ç³»ï¼Œæ¥æ›´å¥½åœ°å¹³è¡¡ä¸åŒç›®æ ‡ä¹‹é—´çš„å†²çªã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå…ƒç­–ç•¥çš„MORLæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç¯å¢ƒæ¨¡æ‹Ÿå™¨ï¼šç”¨äºæ¨¡æ‹Ÿå¤§è¯­è¨€æ¨¡å‹çš„äº¤äº’ç¯å¢ƒï¼ŒåŒ…æ‹¬ç”¨æˆ·è¾“å…¥ã€æ¨¡å‹è¾“å‡ºå’Œå¥–åŠ±ä¿¡å·ã€‚2) å…ƒç­–ç•¥å­¦ä¹ å™¨ï¼šç”¨äºå­¦ä¹ ä¸€ä¸ªå¯ä»¥å¿«é€Ÿé€‚åº”ä¸åŒç›®æ ‡çš„å…ƒç­–ç•¥ã€‚3) ç­–ç•¥ä¼˜åŒ–å™¨ï¼šç”¨äºæ ¹æ®å…ƒç­–ç•¥å’Œç¯å¢ƒåé¦ˆï¼Œä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„ç­–ç•¥ã€‚4) ç›®æ ‡å…³ç³»å»ºæ¨¡å™¨ï¼šç”¨äºå­¦ä¹ ä¸åŒç›®æ ‡ä¹‹é—´çš„å…³ç³»ï¼Œä¾‹å¦‚æµç•…æ€§å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å…ƒç­–ç•¥å­¦ä¹ å¼•å…¥åˆ°å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä»è€Œæå‡äº†LLMä¼˜åŒ–è¿‡ç¨‹çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚ä¼ ç»Ÿçš„MORLæ–¹æ³•éœ€è¦ä¸ºæ¯ä¸ªç›®æ ‡ç»„åˆå•ç‹¬è®­ç»ƒä¸€ä¸ªç­–ç•¥ï¼Œè€Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åªéœ€è¦è®­ç»ƒä¸€ä¸ªå…ƒç­–ç•¥ï¼Œå°±å¯ä»¥å¿«é€Ÿé€‚åº”ä¸åŒçš„ç›®æ ‡ç»„åˆã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ä¸ªç›®æ ‡å…³ç³»å»ºæ¨¡å™¨ï¼Œå¯ä»¥å­¦ä¹ ä¸åŒç›®æ ‡ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæ›´å¥½åœ°å¹³è¡¡ä¸åŒç›®æ ‡ä¹‹é—´çš„å†²çªã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) å…ƒç­–ç•¥å­¦ä¹ å™¨çš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚ä½¿ç”¨Transformerç½‘ç»œæ¥å­¦ä¹ å…ƒç­–ç•¥ã€‚2) å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼Œéœ€è¦èƒ½å¤Ÿåæ˜ ä¸åŒç›®æ ‡çš„ä¼˜å…ˆçº§å’Œé‡è¦æ€§ã€‚3) ç›®æ ‡å…³ç³»å»ºæ¨¡å™¨çš„è®¾è®¡ï¼Œä¾‹å¦‚ä½¿ç”¨è´å¶æ–¯ç½‘ç»œæ¥å»ºæ¨¡ä¸åŒç›®æ ‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚4) æ¢ç´¢ç­–ç•¥çš„è®¾è®¡ï¼Œéœ€è¦åœ¨æ¢ç´¢æ–°çš„ç›®æ ‡ç»„åˆå’Œåˆ©ç”¨å·²çŸ¥çš„ç›®æ ‡ç»„åˆä¹‹é—´è¿›è¡Œå¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æå‡ºäº†ä¸€ä¸ªMORLåŸºå‡†æµ‹è¯•æ¡†æ¶çš„æ„¿æ™¯ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒMORLæ–¹æ³•å¯¹ä¸åŒç›®æ ‡å…³ç³»çš„å½±å“ã€‚è™½ç„¶æ²¡æœ‰ç»™å‡ºå…·ä½“çš„å®éªŒç»“æœï¼Œä½†è¯¥æ¡†æ¶çš„æå‡ºä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„è¯„ä¼°å·¥å…·ã€‚æœªæ¥çš„å·¥ä½œå¯ä»¥åŸºäºæ­¤æ¡†æ¶ï¼Œå¯¹æ¯”ä¸åŒMORLæ–¹æ³•åœ¨LLMä¼˜åŒ–ä¸­çš„æ€§èƒ½ï¼Œå¹¶åˆ†æä¸åŒæ–¹æ³•å¯¹ä¸åŒç›®æ ‡å…³ç³»çš„å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä¼˜åŒ–å¤šä¸ªç›®æ ‡çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡å¹³è¡¡ä¸åŒç›®æ ‡ï¼Œå¯ä»¥æå‡LLMçš„æ•´ä½“æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒï¼Œä¾‹å¦‚åœ¨ä¿è¯ç”Ÿæˆæ–‡æœ¬æµç•…æ€§çš„åŒæ—¶ï¼Œæé«˜å…¶å‡†ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›LLMçš„ä¸ªæ€§åŒ–å®šåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-Objective Reinforcement Learning (MORL) presents significant challenges and opportunities for optimizing multiple objectives in Large Language Models (LLMs). We introduce a MORL taxonomy and examine the advantages and limitations of various MORL methods when applied to LLM optimization, identifying the need for efficient and flexible approaches that accommodate personalization functionality and inherent complexities in LLMs and RL. We propose a vision for a MORL benchmarking framework that addresses the effects of different methods on diverse objective relationships. As future research directions, we focus on meta-policy MORL development that can improve efficiency and flexibility through its bi-level learning paradigm, highlighting key research questions and potential solutions for improving LLM performance.

