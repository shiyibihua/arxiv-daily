---
layout: default
title: Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution
---

# Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21557" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21557v1</a>
  <a href="https://arxiv.org/pdf/2509.21557.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21557v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21557v1', 'Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yash Saxena, Raviteja Bommireddy, Ankur Padia, Manas Gaur

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**å¤‡æ³¨**: Accepted at NeurIPS 2025 LLM Evaluation Workshop

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¯¹æ¯”ç”Ÿæˆæ—¶å’Œåç½®å¼•ç”¨ï¼Œå…¨é¢è¯„ä¼°LLMçš„å½’å› èƒ½åŠ›ï¼Œä¸ºé«˜é£é™©åœºæ™¯æä¾›é€‰æ‹©ä¾æ®ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼•ç”¨ç”Ÿæˆ` `ç”Ÿæˆæ—¶å¼•ç”¨` `åç½®å¼•ç”¨` `æ£€ç´¢å¢å¼º` `å½’å› è¯„ä¼°` `é«˜é£é™©åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨ç”Ÿæˆå¼•ç”¨æ—¶é¢ä¸´è¦†ç›–ç‡å’Œæ­£ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œéš¾ä»¥æ»¡è¶³é«˜é£é™©åœºæ™¯çš„éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºç”Ÿæˆæ—¶å¼•ç”¨(G-Cite)å’Œåç½®å¼•ç”¨(P-Cite)ä¸¤ç§èŒƒå¼ï¼Œå¹¶åˆ†æå…¶ä¼˜ç¼ºç‚¹ã€‚
3. å®éªŒè¡¨æ˜æ£€ç´¢å¢å¼ºæ˜¯æå‡å½’å› è´¨é‡çš„å…³é”®ï¼ŒP-Citeåœ¨è¦†ç›–ç‡å’Œæ­£ç¡®æ€§ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†ä½¿å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨åŒ»ç–—ã€æ³•å¾‹ã€å­¦æœ¯å’Œé‡‘èç­‰é«˜é£é™©é¢†åŸŸæ›´å€¼å¾—ä¿¡èµ–ï¼Œå¿…é¡»å¼•ç”¨å¯éªŒè¯çš„æ¥æºã€‚å®è·µè€…å’Œç ”ç©¶äººå‘˜é¢ä¸´ä¸€ä¸ªé€‰æ‹©ï¼šè®©æ¨¡å‹åœ¨è§£ç è¿‡ç¨‹ä¸­ç”Ÿæˆå¼•ç”¨ï¼Œæˆ–è€…å…ˆè®©æ¨¡å‹èµ·è‰ç­”æ¡ˆï¼Œç„¶åå†é™„åŠ é€‚å½“çš„å¼•ç”¨ã€‚ä¸ºäº†æ˜ç¡®è¿™ç§é€‰æ‹©ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§èŒƒå¼ï¼šç”Ÿæˆæ—¶å¼•ç”¨(G-Cite)ï¼Œå®ƒä¸€æ¬¡æ€§ç”Ÿæˆç­”æ¡ˆå’Œå¼•ç”¨ï¼›ä»¥åŠåç½®å¼•ç”¨(P-Cite)ï¼Œå®ƒåœ¨èµ·è‰åæ·»åŠ æˆ–éªŒè¯å¼•ç”¨ã€‚æˆ‘ä»¬å¯¹å››ç§æµè¡Œçš„å½’å› æ•°æ®é›†è¿›è¡Œäº†ä»é›¶æ ·æœ¬åˆ°é«˜çº§æ£€ç´¢å¢å¼ºæ–¹æ³•çš„å…¨é¢è¯„ä¼°ï¼Œå¹¶æä¾›äº†åŸºäºè¯æ®çš„å»ºè®®ï¼Œæƒè¡¡äº†å„ç§ç”¨ä¾‹ä¸­çš„åˆ©å¼Šã€‚ç»“æœè¡¨æ˜ï¼Œè¦†ç›–ç‡å’Œå¼•ç”¨æ­£ç¡®æ€§ä¹‹é—´å­˜åœ¨ä¸€è‡´çš„æƒè¡¡ï¼Œæ£€ç´¢æ˜¯ä¸¤ç§èŒƒå¼ä¸­å½’å› è´¨é‡çš„ä¸»è¦é©±åŠ¨å› ç´ ã€‚P-Citeæ–¹æ³•ä»¥å…·æœ‰ç«äº‰åŠ›çš„æ­£ç¡®æ€§å’Œé€‚åº¦çš„å»¶è¿Ÿå®ç°äº†é«˜è¦†ç›–ç‡ï¼Œè€ŒG-Citeæ–¹æ³•åˆ™ä»¥ç‰ºç‰²è¦†ç›–ç‡å’Œé€Ÿåº¦ä¸ºä»£ä»·ä¼˜å…ˆè€ƒè™‘ç²¾åº¦ã€‚æˆ‘ä»¬å»ºè®®åœ¨é«˜é£é™©åº”ç”¨ä¸­é‡‡ç”¨ä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„P-Citeä¼˜å…ˆæ–¹æ³•ï¼Œè€Œå°†G-Citeä¿ç•™ç»™ä¸¥æ ¼çš„å£°æ˜éªŒè¯ç­‰å¯¹ç²¾åº¦è¦æ±‚ä¸¥æ ¼çš„è®¾ç½®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå†…å®¹æ—¶å¦‚ä½•è¿›è¡Œæœ‰æ•ˆå’Œå¯é çš„å¼•ç”¨çš„é—®é¢˜ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆåœ¨ç”Ÿæˆå†…å®¹çš„åŒæ—¶ç”Ÿæˆå¼•ç”¨ï¼ˆGeneration-Time Citation, G-Citeï¼‰ï¼Œè¦ä¹ˆåœ¨ç”Ÿæˆå†…å®¹ä¹‹åå†æ·»åŠ æˆ–éªŒè¯å¼•ç”¨ï¼ˆPost-hoc Citation, P-Citeï¼‰ã€‚è¿™ä¸¤ç§æ–¹æ³•å„æœ‰ä¼˜ç¼ºç‚¹ï¼Œä½†åœ¨é«˜é£é™©é¢†åŸŸï¼Œå¦‚åŒ»ç–—ã€æ³•å¾‹ç­‰ï¼Œéœ€è¦ä»”ç»†æƒè¡¡ã€‚ç°æœ‰çš„ç—›ç‚¹åœ¨äºå¦‚ä½•åœ¨è¦†ç›–ç‡ï¼ˆå¼•ç”¨å°½å¯èƒ½å¤šçš„ç›¸å…³ä¿¡æ¯ï¼‰å’Œæ­£ç¡®æ€§ï¼ˆå¼•ç”¨å‡†ç¡®çš„æ¥æºï¼‰ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œä»¥åŠå¦‚ä½•é™ä½å»¶è¿Ÿã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼•ç”¨ç”Ÿæˆè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦èŒƒå¼ï¼Œå³G-Citeå’ŒP-Citeï¼Œå¹¶å¯¹è¿™ä¸¤ç§èŒƒå¼è¿›è¡Œå…¨é¢çš„è¯„ä¼°å’Œæ¯”è¾ƒã€‚é€šè¿‡å®éªŒåˆ†æï¼Œæ‰¾å‡ºæ¯ç§èŒƒå¼çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œå¹¶ä¸ºä¸åŒçš„åº”ç”¨åœºæ™¯æä¾›é€‰æ‹©å»ºè®®ã€‚è®ºæ–‡å¼ºè°ƒæ£€ç´¢å¢å¼ºåœ¨æé«˜å¼•ç”¨è´¨é‡ä¸­çš„ä½œç”¨ï¼Œå¹¶å»ºè®®åœ¨é«˜é£é™©åœºæ™¯ä¸­ä¼˜å…ˆè€ƒè™‘ä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„P-Citeæ–¹æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š
1.  **æ•°æ®å‡†å¤‡**ï¼šä½¿ç”¨å››ä¸ªæµè¡Œçš„å½’å› æ•°æ®é›†è¿›è¡Œå®éªŒã€‚
2.  **æ¨¡å‹é€‰æ‹©**ï¼šé€‰æ‹©ä¸åŒçš„LLMæ¨¡å‹ï¼Œå¹¶ç»“åˆé›¶æ ·æœ¬å­¦ä¹ å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ã€‚
3.  **èŒƒå¼å®ç°**ï¼šåˆ†åˆ«å®ç°G-Citeå’ŒP-Citeä¸¤ç§å¼•ç”¨ç”ŸæˆèŒƒå¼ã€‚
4.  **è¯„ä¼°æŒ‡æ ‡**ï¼šä½¿ç”¨è¦†ç›–ç‡ã€æ­£ç¡®æ€§å’Œå»¶è¿Ÿç­‰æŒ‡æ ‡å¯¹ä¸åŒæ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚
5.  **å®éªŒåˆ†æ**ï¼šå¯¹å®éªŒç»“æœè¿›è¡Œæ·±å…¥åˆ†æï¼Œæ‰¾å‡ºä¸åŒæ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹çš„ä¼˜åŠ£ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1.  **ä¸¤ç§å¼•ç”¨èŒƒå¼çš„æ˜ç¡®åˆ’åˆ†**ï¼šæ˜ç¡®å®šä¹‰äº†G-Citeå’ŒP-Citeä¸¤ç§å¼•ç”¨ç”ŸæˆèŒƒå¼ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ¸…æ™°çš„æ¡†æ¶ã€‚
2.  **å…¨é¢çš„è¯„ä¼°å’Œæ¯”è¾ƒ**ï¼šå¯¹ä¸¤ç§èŒƒå¼è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°å’Œæ¯”è¾ƒï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è¦†ç›–ç‡ã€æ­£ç¡®æ€§å’Œå»¶è¿Ÿç­‰æ–¹é¢çš„æƒè¡¡ã€‚
3.  **æ£€ç´¢å¢å¼ºçš„é‡è¦æ€§**ï¼šå¼ºè°ƒäº†æ£€ç´¢å¢å¼ºåœ¨æé«˜å¼•ç”¨è´¨é‡ä¸­çš„é‡è¦ä½œç”¨ã€‚
4.  **åº”ç”¨åœºæ™¯å»ºè®®**ï¼šä¸ºä¸åŒçš„åº”ç”¨åœºæ™¯æä¾›äº†é€‰æ‹©å»ºè®®ï¼Œå¸®åŠ©å®è·µè€…é€‰æ‹©åˆé€‚çš„å¼•ç”¨ç”Ÿæˆæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š
1.  **æ£€ç´¢å¢å¼ºæ¨¡å—**ï¼šä½¿ç”¨æ£€ç´¢æ¨¡å‹ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œä»¥æé«˜å¼•ç”¨è´¨é‡ã€‚
2.  **è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©**ï¼šé€‰æ‹©è¦†ç›–ç‡ã€æ­£ç¡®æ€§å’Œå»¶è¿Ÿç­‰æŒ‡æ ‡ï¼Œå…¨é¢è¯„ä¼°ä¸åŒæ–¹æ³•çš„æ€§èƒ½ã€‚
3.  **å®éªŒè®¾ç½®**ï¼šè®¾è®¡äº†ä»é›¶æ ·æœ¬åˆ°é«˜çº§æ£€ç´¢å¢å¼ºæ–¹æ³•çš„å®éªŒï¼Œä»¥å…¨é¢è¯„ä¼°ä¸åŒæ–¹æ³•çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒP-Citeæ–¹æ³•åœ¨è¦†ç›–ç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„æ­£ç¡®æ€§ï¼Œä½†å»¶è¿Ÿè¾ƒé«˜ã€‚G-Citeæ–¹æ³•åˆ™ä»¥ç‰ºç‰²è¦†ç›–ç‡å’Œé€Ÿåº¦ä¸ºä»£ä»·ï¼Œä¼˜å…ˆè€ƒè™‘ç²¾åº¦ã€‚æ£€ç´¢å¢å¼ºæ˜¯æé«˜ä¸¤ç§èŒƒå¼ä¸­å½’å› è´¨é‡çš„å…³é”®é©±åŠ¨å› ç´ ã€‚åœ¨é«˜é£é™©åº”ç”¨ä¸­ï¼Œå»ºè®®é‡‡ç”¨ä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„P-Citeä¼˜å…ˆæ–¹æ³•ï¼Œè€Œå°†G-Citeä¿ç•™ç»™ä¸¥æ ¼çš„å£°æ˜éªŒè¯ç­‰å¯¹ç²¾åº¦è¦æ±‚ä¸¥æ ¼çš„è®¾ç½®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦é«˜åº¦å¯ä¿¡åº¦çš„å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€æ³•å¾‹å’¨è¯¢ã€é‡‘èåˆ†æå’Œå­¦æœ¯ç ”ç©¶ç­‰ã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„å¼•ç”¨ç”ŸæˆèŒƒå¼ï¼Œå¯ä»¥æé«˜LLMç”Ÿæˆå†…å®¹çš„å¯é æ€§å’Œå¯ä¿¡åº¦ï¼Œå‡å°‘é”™è¯¯ä¿¡æ¯çš„ä¼ æ’­ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®å’Œå¯é çš„ä¿¡æ¯æœåŠ¡ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•ç»“åˆG-Citeå’ŒP-Citeçš„ä¼˜ç‚¹ï¼Œå¼€å‘æ›´é«˜æ•ˆå’Œå¯é çš„å¼•ç”¨ç”Ÿæˆæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Trustworthy Large Language Models (LLMs) must cite human-verifiable sources in high-stakes domains such as healthcare, law, academia, and finance, where even small errors can have severe consequences. Practitioners and researchers face a choice: let models generate citations during decoding, or let models draft answers first and then attach appropriate citations. To clarify this choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which produces the answer and citations in one pass, and Post-hoc Citation (P-Cite), which adds or verifies citations after drafting. We conduct a comprehensive evaluation from zero-shot to advanced retrieval-augmented methods across four popular attribution datasets and provide evidence-based recommendations that weigh trade-offs across use cases. Our results show a consistent trade-off between coverage and citation correctness, with retrieval as the main driver of attribution quality in both paradigms. P-Cite methods achieve high coverage with competitive correctness and moderate latency, whereas G-Cite methods prioritize precision at the cost of coverage and speed. We recommend a retrieval-centric, P-Cite-first approach for high-stakes applications, reserving G-Cite for precision-critical settings such as strict claim verification. Our codes and human evaluation results are available at https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

