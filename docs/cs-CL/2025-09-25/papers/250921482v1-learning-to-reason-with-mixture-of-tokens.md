---
layout: default
title: Learning to Reason with Mixture of Tokens
---

# Learning to Reason with Mixture of Tokens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21482" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21482v1</a>
  <a href="https://arxiv.org/pdf/2509.21482.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21482v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21482v1', 'Learning to Reason with Mixture of Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adit Jain, Brendan Rappazzo

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**å¤‡æ³¨**: 30 page

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆTokenç”Ÿæˆæ–¹æ³•ï¼Œæå‡LLMåœ¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æ··åˆTokenç”Ÿæˆ` `å¯éªŒè¯å¥–åŠ±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ çš„LLMæ¨ç†æ–¹æ³•å¿½ç•¥äº†æ¨¡å‹æ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸°å¯Œä¿¡æ¯ï¼Œé™åˆ¶äº†æ¨ç†æœç´¢ç©ºé—´ã€‚
2. æå‡ºæ··åˆTokenç”Ÿæˆï¼ˆMoT-Gï¼‰æ–¹æ³•ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­åˆ©ç”¨tokençš„æ··åˆè¡¨ç¤ºï¼Œæ‰©å±•äº†æ¨ç†çš„æœç´¢ç©ºé—´ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMoT-Gæ–¹æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œå‡å°‘äº†æ‰€éœ€çš„è½¨è¿¹æ•°é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†åœ¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¸­åˆ©ç”¨æ··åˆTokenç”Ÿæˆï¼ˆMoT-Gï¼‰æ–¹æ³•æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åŸºäºGroup Relative Policy Optimizationï¼Œé€šè¿‡é‡‡æ ·å¤šä¸ªæ¨ç†è¿‡ç¨‹ï¼Œç›¸äº’è¯„åˆ†å¹¶è°ƒæ•´ç­–ç•¥ã€‚ä½†è¿™äº›æ–¹æ³•åœ¨æ¯ä¸€æ­¥æ¨ç†ä¸­éƒ½é‡‡æ ·ç¦»æ•£çš„tokenï¼Œå¿½ç•¥äº†æ¨¡å‹æ¦‚ç‡åˆ†å¸ƒä¸­ä¸°å¯Œçš„åˆ†å¸ƒä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ¨å¹¿äº†ç°æœ‰çš„MoT-Gæ–¹æ³•ï¼Œå¹¶æ‰©å±•äº†RLVRï¼Œä½¿å…¶å¯ä»¥ç›´æ¥åœ¨è¿ç»­æ··åˆç©ºé—´ä¸­ç”Ÿæˆæ€ç»´é“¾ã€‚åœ¨Reasoning-Gymä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMoT-Gæ–¹æ³•åœ¨10ä¸ªä»»åŠ¡ä¸­çš„7ä¸ªä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ˆ5-35%çš„å¢ç›Šï¼‰ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€åŠçš„è½¨è¿¹æ•°é‡å°±è¾¾åˆ°äº†ä¸æ ‡å‡†è§£ç ç›¸å½“çš„ç²¾åº¦ï¼Œè¡¨æ˜è®­ç»ƒæ•ˆç‡å¾—åˆ°äº†æé«˜ã€‚é€šè¿‡å…¨é¢çš„éšè—çŠ¶æ€å’Œtokençº§åˆ«åˆ†æï¼Œè¯æ˜MoT-Gçš„ä¼˜åŠ¿å¯èƒ½æºäºå…¶åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒæ›´é«˜çš„éšè—çŠ¶æ€ç†µå’Œä¿ƒè¿›tokenç©ºé—´æ¢ç´¢çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰çš„æ–¹æ³•ï¼Œå¦‚Group Relative Policy Optimizationï¼Œåœ¨ç”Ÿæˆæ¨ç†é“¾æ—¶ï¼Œæ¯ä¸€æ­¥éƒ½åªé‡‡æ ·ä¸€ä¸ªç¦»æ•£çš„tokenã€‚è¿™ç§åšæ³•å¿½ç•¥äº†æ¨¡å‹åœ¨æ¯ä¸ªtokenä¸Šçš„æ¦‚ç‡åˆ†å¸ƒä¿¡æ¯ï¼Œå¯¼è‡´æœç´¢ç©ºé—´å—é™ï¼Œå¯èƒ½é”™è¿‡æ›´ä¼˜çš„æ¨ç†è·¯å¾„ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•æœ‰æ•ˆåˆ©ç”¨æ¨¡å‹æä¾›çš„å…¨éƒ¨ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ··åˆTokenç”Ÿæˆï¼ˆMoT-Gï¼‰æ–¹æ³•ï¼Œä¸å†å±€é™äºé‡‡æ ·å•ä¸ªtokenï¼Œè€Œæ˜¯å°†å¤šä¸ªtokençš„embeddingè¿›è¡ŒåŠ æƒæ··åˆï¼Œå½¢æˆä¸€ä¸ªè¿ç»­çš„æ··åˆè¡¨ç¤ºã€‚è¿™æ ·å¯ä»¥ä¿ç•™æ¨¡å‹åœ¨æ¯ä¸ªtokenä¸Šçš„æ¦‚ç‡åˆ†å¸ƒä¿¡æ¯ï¼Œæ‰©å±•æœç´¢ç©ºé—´ï¼Œä»è€Œæ‰¾åˆ°æ›´ä¼˜çš„æ¨ç†è·¯å¾„ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†å…‹æœç°æœ‰æ–¹æ³•çš„ä¿¡æ¯æŸå¤±é—®é¢˜ï¼Œæ›´å……åˆ†åœ°åˆ©ç”¨æ¨¡å‹çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„MoT-Gæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ¦‚æ‹¬ç°æœ‰çš„MoT-Gæ–¹æ³•ï¼ŒåŒ…æ‹¬é‚£äº›æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å°†æ··åˆåµŒå…¥æ„å»ºä¸ºtokenåµŒå…¥çš„åŠ æƒå’Œã€‚è¯¥æ¡†æ¶æ‰©å±•äº†RLVRï¼Œä½¿å…¶å¯ä»¥ç›´æ¥åœ¨è¿ç»­æ··åˆç©ºé—´ä¸­ç”Ÿæˆæ€ç»´é“¾ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨LLMç”Ÿæˆtokençš„æ¦‚ç‡åˆ†å¸ƒï¼›2ï¼‰æ ¹æ®æ¦‚ç‡åˆ†å¸ƒå¯¹tokençš„embeddingè¿›è¡ŒåŠ æƒæ··åˆï¼Œç”Ÿæˆæ··åˆtokenè¡¨ç¤ºï¼›3ï¼‰ä½¿ç”¨æ··åˆtokenè¡¨ç¤ºè¿›è¡Œæ¨ç†ï¼›4ï¼‰ä½¿ç”¨RLVRå¯¹ç­–ç•¥è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†æ··åˆtokenç”Ÿæˆæ–¹æ³•å¼•å…¥åˆ°å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ä¸­ã€‚ä¸ç°æœ‰æ–¹æ³•åªé‡‡æ ·å•ä¸ªtokenä¸åŒï¼ŒMoT-Gæ–¹æ³•åˆ©ç”¨äº†tokençš„æ¦‚ç‡åˆ†å¸ƒä¿¡æ¯ï¼Œç”Ÿæˆæ··åˆtokenè¡¨ç¤ºï¼Œä»è€Œæ‰©å±•äº†æœç´¢ç©ºé—´ï¼Œæé«˜äº†æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œç°æœ‰æ–¹æ³•æ˜¯ç¦»æ•£çš„tokené‡‡æ ·ï¼Œè€ŒMoT-Gæ–¹æ³•æ˜¯è¿ç»­çš„æ··åˆè¡¨ç¤ºã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰æ··åˆæƒé‡è®¡ç®—æ–¹æ³•ï¼šå¯ä»¥ä½¿ç”¨æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒä½œä¸ºæƒé‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–æ–¹æ³•è®¡ç®—æƒé‡ã€‚2ï¼‰å¥–åŠ±å‡½æ•°è®¾è®¡ï¼šéœ€è¦è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°æ¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ å¦‚ä½•ç”Ÿæˆæ›´æœ‰æ•ˆçš„æ··åˆtokenè¡¨ç¤ºã€‚3ï¼‰æ¨¡å‹æ¶æ„ï¼šå¯ä»¥ä½¿ç”¨ç°æœ‰çš„LLMæ¨¡å‹ï¼Œåªéœ€è¦ä¿®æ”¹tokenç”Ÿæˆéƒ¨åˆ†ï¼Œä½¿å…¶æ”¯æŒæ··åˆtokenç”Ÿæˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMoT-Gæ–¹æ³•åœ¨Reasoning-Gymçš„10ä¸ªä»»åŠ¡ä¸­çš„7ä¸ªä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ˆ5-35%çš„å¢ç›Šï¼‰ï¼Œä¸æ ‡å‡†è§£ç ç›¸æ¯”ï¼Œä½¿ç”¨ä¸€åŠçš„è½¨è¿¹æ•°é‡å°±è¾¾åˆ°äº†ç›¸å½“çš„ç²¾åº¦ï¼Œè¡¨æ˜è®­ç»ƒæ•ˆç‡å¾—åˆ°äº†æé«˜ã€‚è¿™è¡¨æ˜MoT-Gæ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ¨¡å‹çš„ä¿¡æ¯ï¼Œæé«˜æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ¨ç†ç­‰ã€‚é€šè¿‡æå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œè§£å†³å¤æ‚é—®é¢˜ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰ï¼Œæå‡æ™ºèƒ½ç³»ç»Ÿçš„å†³ç­–èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning with verifiable rewards (RLVR) has become a leading approach for improving large language model (LLM) reasoning capabilities. Most current methods follow variants of Group Relative Policy Optimization, which samples multiple reasoning completions, scores them relative to each other, and adjusts the policy accordingly. However, these approaches invariably sample discrete tokens at each reasoning step, discarding the rich distributional information in the model's probability distribution over candidate tokens. While preserving and utilizing this distributional information has proven beneficial in non-RL settings, current RLVR methods seem to be unnecessarily constraining the reasoning search space by not using this information. To address this limitation, we investigate mixture-of-token generation (MoT-G) in RLVR. We present a unified framework that generalizes existing MoT-G approaches, including existing training-free methods that construct mixture embeddings as weighted sums over token embeddings, and extend RLVR to operate directly in this continuous mixture space for generating chain-of-thought. Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive language tasks, we find that MoT--G methods achieve substantial improvements (5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of trajectories, suggesting improved training efficiency. Through comprehensive hidden-state and token-level analyses, we provide evidence that MoT--G's benefits may stem from its ability to maintain higher hidden-state entropy throughout the reasoning process and promote exploration in token space.

