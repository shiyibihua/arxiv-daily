---
layout: default
title: RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards
---

# RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21319" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21319v2</a>
  <a href="https://arxiv.org/pdf/2509.21319.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21319v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21319v2', 'RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25 (æ›´æ–°: 2025-10-30)

**å¤‡æ³¨**: Added link to access models: https://huggingface.co/collections/nvidia/reward-models-10-2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/collections/nvidia)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRLBFFï¼Œç»“åˆäººç±»åé¦ˆå’Œå¯éªŒè¯å¥–åŠ±ï¼Œæå‡LLMå¯¹é½æ•ˆæœå¹¶æ”¯æŒæ¨ç†æ—¶è‡ªå®šä¹‰åŸåˆ™ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `å¥–åŠ±æ¨¡å‹` `å¤§è¯­è¨€æ¨¡å‹å¯¹é½` `äºŒå…ƒåé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. RLHFä¾èµ–äººç±»åé¦ˆï¼Œç¼ºä¹æ˜ç¡®æ ‡å‡†ï¼Œå¯¼è‡´å¯è§£é‡Šæ€§å·®å’Œæ˜“å—å¥–åŠ±æ“çºµï¼›RLVRåˆ™å—é™äºåŸºäºæ­£ç¡®æ€§çš„éªŒè¯å™¨ï¼Œé€‚ç”¨èŒƒå›´æœ‰é™ã€‚
2. RLBFFä»äººç±»åé¦ˆä¸­æå–äºŒå…ƒåŸåˆ™ï¼Œå°†å¥–åŠ±æ¨¡å‹è®­ç»ƒè½¬åŒ–ä¸ºè•´å«ä»»åŠ¡ï¼Œä»è€Œç»“åˆäº†äººç±»åé¦ˆçš„çµæ´»æ€§å’Œè§„åˆ™éªŒè¯çš„ç²¾ç¡®æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRLBFFè®­ç»ƒçš„å¥–åŠ±æ¨¡å‹åœ¨RM-Benchå’ŒJudgeBenchä¸Šå–å¾—äº†é¢†å…ˆæ€§èƒ½ï¼Œå¹¶èƒ½ä»¥ä½æˆæœ¬å¯¹é½Qwen3-32Bï¼Œåª²ç¾æ›´å¤§å‹çš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼šåŸºäºäºŒå…ƒçµæ´»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLBFFï¼‰ï¼Œæ—¨åœ¨ç»“åˆäººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„ä¼˜åŠ¿ã€‚RLHFä¾èµ–äººç±»åˆ¤æ–­ï¼Œç¼ºä¹å¯è§£é‡Šæ€§å’Œæ˜“å—å¥–åŠ±æ“çºµï¼Œè€ŒRLVRåˆ™å—é™äºåŸºäºæ­£ç¡®æ€§çš„éªŒè¯å™¨ã€‚RLBFFä»è‡ªç„¶è¯­è¨€åé¦ˆä¸­æå–å¯ä»¥ç”¨äºŒå…ƒæ–¹å¼å›ç­”çš„åŸåˆ™ï¼ˆä¾‹å¦‚ï¼Œä¿¡æ¯å‡†ç¡®æ€§ï¼šæ˜¯/å¦ï¼Œä»£ç å¯è¯»æ€§ï¼šæ˜¯/å¦ï¼‰ã€‚è¿™äº›åŸåˆ™å¯ç”¨äºå°†å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒè½¬åŒ–ä¸ºè•´å«ä»»åŠ¡ï¼ˆresponseæ˜¯å¦æ»¡è¶³ç‰¹å®šåŸåˆ™ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è¿™ç§æ–¹å¼ä¸‹è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹åœ¨æ•°æ®åŒ¹é…æ—¶ä¼˜äºBradley-Terryæ¨¡å‹ï¼Œå¹¶åœ¨RM-Benchï¼ˆ86.2%ï¼‰å’ŒJudgeBenchï¼ˆ81.4%ï¼Œæˆªè‡³2025å¹´9æœˆ24æ—¥æ’åç¬¬ä¸€ï¼‰ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¨ç†æ—¶æŒ‡å®šæ„Ÿå…´è¶£çš„åŸåˆ™ï¼Œä»è€Œå®šåˆ¶å¥–åŠ±æ¨¡å‹çš„å…³æ³¨ç‚¹ã€‚æœ€åï¼Œæœ¬æ–‡æä¾›äº†ä¸€ä¸ªå®Œå…¨å¼€æºçš„æ–¹æ¡ˆï¼ˆåŒ…æ‹¬æ•°æ®ï¼‰ï¼Œä½¿ç”¨RLBFFå’Œæˆ‘ä»¬çš„å¥–åŠ±æ¨¡å‹å¯¹Qwen3-32Bè¿›è¡Œå¯¹é½ï¼Œåœ¨MT-Benchã€WildBenchå’ŒArena Hard v2ç­‰é€šç”¨å¯¹é½åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¾¾åˆ°æˆ–è¶…è¿‡o3-miniå’ŒDeepSeek R1çš„æ€§èƒ½ï¼ˆä¸”æ¨ç†æˆæœ¬ä½äº5%ï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•ï¼Œå¦‚RLHFå’ŒRLVRï¼Œå„æœ‰ä¸è¶³ã€‚RLHFä¾èµ–äºä¸»è§‚çš„äººç±»åé¦ˆï¼Œç¼ºä¹æ˜ç¡®çš„è¯„åˆ¤æ ‡å‡†ï¼Œå¯¼è‡´å¥–åŠ±æ¨¡å‹éš¾ä»¥è§£é‡Šï¼Œå®¹æ˜“è¢«æ¨¡å‹â€œå¥–åŠ±æ”»å‡»â€ã€‚RLVRè™½ç„¶æœ‰æ˜ç¡®çš„è§„åˆ™ï¼Œä½†åªèƒ½é’ˆå¯¹ç‰¹å®šç±»å‹çš„ä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆï¼‰è¿›è¡ŒéªŒè¯ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æ›´å¹¿æ³›çš„åœºæ™¯ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ—¢èƒ½åˆ©ç”¨äººç±»åé¦ˆçš„çµæ´»æ€§ï¼Œåˆèƒ½ä¿è¯å¥–åŠ±ä¿¡å·å¯è§£é‡Šæ€§çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRLBFFçš„æ ¸å¿ƒæ€è·¯æ˜¯ä»è‡ªç„¶è¯­è¨€å½¢å¼çš„äººç±»åé¦ˆä¸­æå–å‡ºå¯ä»¥ç”¨äºŒå…ƒå½¢å¼ï¼ˆæ˜¯/å¦ï¼‰å›ç­”çš„åŸåˆ™ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥æå–å‡ºâ€œä¿¡æ¯æ˜¯å¦å‡†ç¡®â€ã€â€œè¯­è¨€æ˜¯å¦æµç•…â€ç­‰åŸåˆ™ã€‚ç„¶åï¼Œå°†å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒè½¬åŒ–ä¸ºä¸€ä¸ªè•´å«ä»»åŠ¡ï¼šåˆ¤æ–­ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦æ»¡è¶³è¿™äº›åŸåˆ™ã€‚è¿™æ ·ï¼Œå¥–åŠ±æ¨¡å‹ä¸ä»…å¯ä»¥å­¦ä¹ äººç±»çš„åå¥½ï¼Œè¿˜å¯ä»¥å­¦ä¹ åˆ°æ˜ç¡®çš„è¯„åˆ¤æ ‡å‡†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRLBFFçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) æ”¶é›†åŒ…å«è‡ªç„¶è¯­è¨€åé¦ˆçš„æ•°æ®é›†ï¼›2) ä»è‡ªç„¶è¯­è¨€åé¦ˆä¸­æå–äºŒå…ƒåŸåˆ™ï¼›3) ä½¿ç”¨æå–çš„åŸåˆ™è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œå°†è®­ç»ƒç›®æ ‡è®¾å®šä¸ºåˆ¤æ–­ç”Ÿæˆæ–‡æœ¬æ˜¯å¦æ»¡è¶³è¿™äº›åŸåˆ™çš„è•´å«ä»»åŠ¡ï¼›4) ä½¿ç”¨è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šRLBFFæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†è‡ªç„¶è¯­è¨€åé¦ˆè½¬åŒ–ä¸ºäºŒå…ƒåŸåˆ™ï¼Œä»è€Œå°†å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒè½¬åŒ–ä¸ºä¸€ä¸ªè•´å«ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•æ—¢åˆ©ç”¨äº†äººç±»åé¦ˆçš„çµæ´»æ€§ï¼Œåˆä¿è¯äº†å¥–åŠ±ä¿¡å·çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼ŒRLBFFè¿˜å…è®¸ç”¨æˆ·åœ¨æ¨ç†æ—¶æŒ‡å®šæ„Ÿå…´è¶£çš„åŸåˆ™ï¼Œä»è€Œå®šåˆ¶å¥–åŠ±æ¨¡å‹çš„è¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šRLBFFçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•ä»è‡ªç„¶è¯­è¨€åé¦ˆä¸­æœ‰æ•ˆåœ°æå–äºŒå…ƒåŸåˆ™ï¼ˆå…·ä½“æ–¹æ³•æœªçŸ¥ï¼Œè®ºæ–‡ä¸­å¯èƒ½æœªè¯¦ç»†è¯´æ˜ï¼‰ï¼›2) å¦‚ä½•è®¾è®¡è•´å«ä»»åŠ¡çš„æŸå¤±å‡½æ•°ï¼Œä½¿å¾—å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°åˆ¤æ–­ç”Ÿæˆæ–‡æœ¬æ˜¯å¦æ»¡è¶³ç‰¹å®šåŸåˆ™ï¼›3) å¦‚ä½•å°†å¥–åŠ±æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹é½ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ­¤å¤„æ— æ³•è¯¦ç»†å±•å¼€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRLBFFè®­ç»ƒçš„å¥–åŠ±æ¨¡å‹åœ¨RM-Benchä¸Šè¾¾åˆ°äº†86.2%çš„å‡†ç¡®ç‡ï¼Œåœ¨JudgeBenchä¸Šè¾¾åˆ°äº†81.4%çš„å‡†ç¡®ç‡ï¼ˆæˆªè‡³2025å¹´9æœˆ24æ—¥æ’åç¬¬ä¸€ï¼‰ã€‚æ­¤å¤–ï¼Œä½¿ç”¨RLBFFå¯¹é½çš„Qwen3-32Bæ¨¡å‹åœ¨MT-Benchã€WildBenchå’ŒArena Hard v2ç­‰é€šç”¨å¯¹é½åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¾¾åˆ°äº†æˆ–è¶…è¿‡äº†o3-miniå’ŒDeepSeek R1çš„æ€§èƒ½ï¼Œä¸”æ¨ç†æˆæœ¬ä½äº5%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRLBFFæ˜¯ä¸€ç§æœ‰æ•ˆçš„æ¨¡å‹å¯¹é½æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RLBFFå¯åº”ç”¨äºå„ç§éœ€è¦äººç±»åé¦ˆçš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½åœºæ™¯ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆã€ä»£ç ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå–äºŒå…ƒåŸåˆ™ï¼ŒRLBFFå¯ä»¥æé«˜å¥–åŠ±æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œé²æ£’æ€§ï¼Œé™ä½å¥–åŠ±æ”»å‡»çš„é£é™©ã€‚æ­¤å¤–ï¼ŒRLBFFè¿˜æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰åŸåˆ™ï¼Œä»è€Œå®ç°ä¸ªæ€§åŒ–çš„æ¨¡å‹å¯¹é½ã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models: https://huggingface.co/collections/nvidia/reward-models-10-2025

