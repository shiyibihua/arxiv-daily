---
layout: default
title: Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs
---

# Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21080" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21080v1</a>
  <a href="https://arxiv.org/pdf/2509.21080.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21080v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21080v1', 'Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yixin Wan, Xingrun Chen, Kai-Wei Chang

**åˆ†ç±»**: cs.CL, cs.AI, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºLLMæ–‡åŒ–å®šä½åå·®å¹¶æå‡ºåŸºäºAgentçš„åè§ç¼“è§£æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡åŒ–å®šä½åå·®` `å…¬å¹³æ€§` `ä»£ç†` `åè§ç¼“è§£` `ç”Ÿæˆå¼æ¨¡å‹` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶ï¼Œå­˜åœ¨ä»¥ä¸»æµæ–‡åŒ–è§†è§’çœ‹å¾…ä¸–ç•Œçš„æ–‡åŒ–å®šä½åå·®é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ„å»ºå…¬å¹³ä»£ç†ï¼Œè¿›è¡Œè‡ªæˆ‘åæ€å’Œé‡å†™ï¼Œæˆ–é€šè¿‡å¤šä»£ç†åä½œæ¥ç¼“è§£æ–‡åŒ–å®šä½åå·®ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒåŸºäºä»£ç†çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£LLMåœ¨ç”Ÿæˆå†…å®¹æ—¶è¡¨ç°å‡ºçš„æ–‡åŒ–åè§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå¼åº”ç”¨ä¸­å±•ç°äº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œæœ¬æ–‡å‘ç°LLMå­˜åœ¨æ–‡åŒ–ç›¸å…³çš„å¾®å¦™å…¬å¹³æ€§é—®é¢˜ï¼Œå…¶ç”Ÿæˆå†…å®¹å€¾å‘äºä¸»æµç¾å›½æ–‡åŒ–çš„è§†è§’ï¼Œå¯¹éä¸»æµæ–‡åŒ–è¡¨ç°å‡ºæ˜¾è‘—çš„å¤–éƒ¨æ€§ã€‚æœ¬æ–‡è¯†åˆ«å¹¶ç³»ç»Ÿç ”ç©¶äº†è¿™ç§æ–°å‹çš„æ–‡åŒ–å®šä½åå·®ï¼Œå³LLMçš„é»˜è®¤ç”Ÿæˆç«‹åœºä¸ä¸»æµè§‚ç‚¹ä¸€è‡´ï¼Œå¹¶å°†å…¶ä»–æ–‡åŒ–è§†ä¸ºå±€å¤–äººã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†CultureLensåŸºå‡†ï¼ŒåŒ…å«4000ä¸ªç”Ÿæˆæç¤ºå’Œ3ä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œé€šè¿‡æ–‡åŒ–æƒ…å¢ƒä¸‹çš„è®¿è°ˆè„šæœ¬ç”Ÿæˆä»»åŠ¡æ¥é‡åŒ–è¿™ç§åå·®ï¼Œå…¶ä¸­LLMæ‰®æ¼”è®°è€…é‡‡è®¿10ç§ä¸åŒæ–‡åŒ–çš„å½“åœ°äººã€‚å¯¹5ä¸ªå…ˆè¿›LLMçš„è¯„ä¼°æ­ç¤ºäº†ä¸€ä¸ªæ˜æ˜¾çš„æ¨¡å¼ï¼šæ¨¡å‹åœ¨ç¾å›½è¯­å¢ƒä¸‹å¹³å‡é‡‡ç”¨è¶…è¿‡88%çš„å†…éƒ¨äººå£«è¯­æ°”ï¼Œä½†åœ¨è¾ƒä¸å ä¸»å¯¼åœ°ä½çš„æ–‡åŒ–ä¸­ï¼Œä¸æˆæ¯”ä¾‹åœ°é‡‡ç”¨å¤–éƒ¨äººå£«ç«‹åœºã€‚ä¸ºäº†è§£å†³è¿™äº›åå·®ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§æ¨ç†æ—¶ç¼“è§£æ–¹æ³•ï¼šåŸºäºæç¤ºçš„å…¬å¹³å¹²é¢„æ”¯æŸ±ï¼ˆFIPï¼‰åŸºçº¿æ–¹æ³•ï¼Œä»¥åŠä¸€ä¸ªç»“æ„åŒ–çš„é€šè¿‡å…¬å¹³ä»£ç†ç¼“è§£ï¼ˆMFAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªæµç¨‹ï¼šï¼ˆ1ï¼‰MFA-SAï¼ˆå•ä»£ç†ï¼‰å¼•å…¥äº†ä¸€ä¸ªåŸºäºå…¬å¹³æ€§æŒ‡å—çš„è‡ªæˆ‘åæ€å’Œé‡å†™å¾ªç¯ã€‚ï¼ˆ2ï¼‰MFA-MAï¼ˆå¤šä»£ç†ï¼‰å°†è¯¥è¿‡ç¨‹æ„å»ºæˆä¸€ä¸ªä¸“ä¸šä»£ç†çš„å±‚çº§ç»“æ„ï¼šè§„åˆ’ä»£ç†ï¼ˆåˆå§‹è„šæœ¬ç”Ÿæˆï¼‰ã€è¯„è®ºä»£ç†ï¼ˆæ ¹æ®å…¬å¹³æ€§æ”¯æŸ±è¯„ä¼°åˆå§‹è„šæœ¬ï¼‰å’Œæ”¹è¿›ä»£ç†ï¼ˆæ•´åˆåé¦ˆä»¥ç”Ÿæˆæ¶¦è‰²åçš„æ— åè„šæœ¬ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºä»£ç†çš„æ–¹æ³•æ˜¯ç¼“è§£ç”Ÿæˆå¼LLMä¸­åè§çš„ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå†…å®¹æ—¶å­˜åœ¨çš„æ–‡åŒ–å®šä½åå·®é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼ŒLLMå€¾å‘äºä»¥ä¸»æµæ–‡åŒ–ï¼ˆå¦‚ç¾å›½æ–‡åŒ–ï¼‰çš„è§†è§’è¿›è¡Œç”Ÿæˆï¼Œè€Œå°†å…¶ä»–æ–‡åŒ–è§†ä¸ºâ€œå±€å¤–äººâ€ï¼Œä»è€Œå¯¼è‡´ä¸å…¬å¹³çš„ç°è±¡ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹è¿™ç§åå·®çš„æœ‰æ•ˆè¯†åˆ«å’Œç¼“è§£æœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä»£ç†ï¼ˆAgentï¼‰æ¥æ¨¡æ‹Ÿäººç±»çš„è‡ªæˆ‘åæ€å’Œåä½œè¿‡ç¨‹ï¼Œä»è€Œå¼•å¯¼LLMç”Ÿæˆæ›´åŠ å…¬å¹³å’Œå®¢è§‚çš„å†…å®¹ã€‚é€šè¿‡è®©ä»£ç†æ ¹æ®é¢„å®šä¹‰çš„å…¬å¹³æ€§åŸåˆ™å¯¹ç”Ÿæˆå†…å®¹è¿›è¡Œè¯„ä¼°å’Œæ”¹è¿›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å‡å°‘æ–‡åŒ–å®šä½åå·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºäº†ä¸¤ç§åŸºäºä»£ç†çš„ç¼“è§£æ–¹æ³•ï¼šMFA-SAï¼ˆå•ä»£ç†ï¼‰å’ŒMFA-MAï¼ˆå¤šä»£ç†ï¼‰ã€‚MFA-SAé‡‡ç”¨å•ä»£ç†çš„è‡ªæˆ‘åæ€å’Œé‡å†™å¾ªç¯ï¼Œä»£ç†æ ¹æ®å…¬å¹³æ€§æŒ‡å—è¯„ä¼°åˆå§‹ç”Ÿæˆè„šæœ¬ï¼Œå¹¶è¿›è¡Œè¿­ä»£æ”¹è¿›ã€‚MFA-MAåˆ™æ„å»ºäº†ä¸€ä¸ªå¤šä»£ç†å±‚çº§ç»“æ„ï¼ŒåŒ…æ‹¬è§„åˆ’ä»£ç†ï¼ˆè´Ÿè´£åˆå§‹è„šæœ¬ç”Ÿæˆï¼‰ã€è¯„è®ºä»£ç†ï¼ˆè´Ÿè´£æ ¹æ®å…¬å¹³æ€§æ”¯æŸ±è¯„ä¼°è„šæœ¬ï¼‰å’Œæ”¹è¿›ä»£ç†ï¼ˆè´Ÿè´£æ•´åˆåé¦ˆå¹¶ç”Ÿæˆæœ€ç»ˆè„šæœ¬ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†ä»£ç†çš„æ¦‚å¿µå¼•å…¥åˆ°LLMçš„åè§ç¼“è§£ä¸­ï¼Œå¹¶æå‡ºäº†ä¸¤ç§ä¸åŒçš„ä»£ç†æ¶æ„ï¼ˆå•ä»£ç†å’Œå¤šä»£ç†ï¼‰ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæç¤ºçš„æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºä»£ç†çš„æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰å’Œçº æ­£LLMä¸­çš„æ–‡åŒ–å®šä½åå·®ã€‚å¤šä»£ç†æ¶æ„é€šè¿‡æ˜ç¡®åˆ†å·¥ï¼Œä½¿å¾—åè§æ£€æµ‹å’Œç¼“è§£è¿‡ç¨‹æ›´åŠ ç»“æ„åŒ–å’Œå¯æ§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MFA-SAä¸­ï¼Œå…³é”®åœ¨äºå…¬å¹³æ€§æŒ‡å—çš„è®¾è®¡ï¼Œå®ƒæŒ‡å¯¼ä»£ç†è¿›è¡Œè‡ªæˆ‘è¯„ä¼°å’Œé‡å†™ã€‚åœ¨MFA-MAä¸­ï¼Œå…³é”®åœ¨äºå„ä¸ªä»£ç†ä¹‹é—´çš„åä½œæœºåˆ¶ï¼Œä»¥åŠè¯„è®ºä»£ç†æ‰€ä½¿ç”¨çš„å…¬å¹³æ€§æ”¯æŸ±ã€‚è®ºæ–‡æ²¡æœ‰è¯¦ç»†è¯´æ˜å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ï¼Œè¿™äº›å¯èƒ½ä¾èµ–äºåº•å±‚LLMçš„å®ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºä»£ç†çš„æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½LLMä¸­çš„æ–‡åŒ–å®šä½åå·®ã€‚ä¾‹å¦‚ï¼Œåœ¨CultureLensåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMFA-MAæ–¹æ³•åœ¨å¤šä¸ªæ–‡åŒ–èƒŒæ™¯ä¸‹éƒ½å–å¾—äº†ä¼˜äºåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨éä¸»æµæ–‡åŒ–èƒŒæ™¯ä¸‹ï¼Œå…¶æ”¹è¿›æ•ˆæœæ›´ä¸ºæ˜æ˜¾ã€‚ä¸FIPåŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒMFAæ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æé«˜ç”Ÿæˆå†…å®¹çš„å…¬å¹³æ€§å’Œå®¢è§‚æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ç”Ÿæˆæ–‡åŒ–æ•æ„Ÿå†…å®¹çš„åœºæ™¯ï¼Œä¾‹å¦‚è·¨æ–‡åŒ–äº¤æµã€æ•™è‚²ã€æ–°é—»æŠ¥é“ç­‰ã€‚é€šè¿‡ç¼“è§£LLMä¸­çš„æ–‡åŒ–å®šä½åå·®ï¼Œå¯ä»¥æé«˜ç”Ÿæˆå†…å®¹çš„å…¬å¹³æ€§å’Œå®¢è§‚æ€§ï¼Œé¿å…å¯¹ç‰¹å®šæ–‡åŒ–ç¾¤ä½“çš„æ­§è§†æˆ–åè§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„åè§ç¼“è§£ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æ€§åˆ«åè§ã€ç§æ—åè§ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have unlocked a wide range of downstream generative applications. However, we found that they also risk perpetuating subtle fairness issues tied to culture, positioning their generations from the perspectives of the mainstream US culture while demonstrating salient externality towards non-mainstream ones. In this work, we identify and systematically investigate this novel culture positioning bias, in which an LLM's default generative stance aligns with a mainstream view and treats other cultures as outsiders. We propose the CultureLens benchmark with 4000 generation prompts and 3 evaluation metrics for quantifying this bias through the lens of a culturally situated interview script generation task, in which an LLM is positioned as an onsite reporter interviewing local people across 10 diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a stark pattern: while models adopt insider tones in over 88 percent of US-contexted scripts on average, they disproportionately adopt mainly outsider stances for less dominant cultures. To resolve these biases, we propose 2 inference-time mitigation methods: a baseline prompt-based Fairness Intervention Pillars (FIP) method, and a structured Mitigation via Fairness Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent) introduces a self-reflection and rewriting loop based on fairness guidelines. (2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized agents: a Planner Agent(initial script generation), a Critique Agent (evaluates initial script against fairness pillars), and a Refinement Agent (incorporates feedback to produce a polished, unbiased script). Empirical results showcase the effectiveness of agent-based methods as a promising direction for mitigating biases in generative LLMs.

