---
layout: default
title: Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond
---

# Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21284" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21284v1</a>
  <a href="https://arxiv.org/pdf/2509.21284.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21284v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21284v1', 'Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç†è®ºåˆ†æCoTæ¨ç†çš„é²æ£’æ€§è¾¹ç•Œï¼Œæ­ç¤ºæ¨ç†æ­¥æ•°å’ŒåµŒå…¥èŒƒæ•°çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ€ç»´é“¾` `é²æ£’æ€§` `è¾“å…¥æ‰°åŠ¨` `ç†è®ºåˆ†æ` `çº¿æ€§è‡ªæ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰CoTæ–¹æ³•æ˜“å—è¾“å…¥æ‰°åŠ¨å½±å“ï¼Œç¼ºä¹å¯¹æ‰°åŠ¨å¦‚ä½•å½±å“æ¨ç†è¿‡ç¨‹çš„ç†è®ºè§£é‡Šã€‚
2. è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æï¼Œæ¨å¯¼äº†CoTè¾“å‡ºæ³¢åŠ¨åœ¨å¯æ¥å—èŒƒå›´å†…è¾“å…¥æ‰°åŠ¨çš„ä¸Šé™ã€‚
3. å®éªŒéªŒè¯äº†ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ï¼Œè¡¨æ˜æ¨ç†æ­¥æ•°å’ŒåµŒå…¥èŒƒæ•°ä¸CoTé²æ£’æ€§ç›¸å…³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œæ€ç»´é“¾ï¼ˆCoTï¼‰çš„è¾“å‡ºå®¹æ˜“å—åˆ°è¾“å…¥æ‰°åŠ¨çš„å½±å“ã€‚å°½ç®¡è®¸å¤šæ–¹æ³•è¯•å›¾é€šè¿‡ä¼˜åŒ–æç¤ºæ¥ç¼“è§£è¿™ç§å½±å“ï¼Œä½†å¯¹äºè¿™äº›æ‰°åŠ¨å¦‚ä½•å½±å“CoTè¾“å‡ºçš„ç†è®ºè§£é‡Šä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é¢†åŸŸã€‚è¿™ä¸€ç©ºç™½é™åˆ¶äº†æˆ‘ä»¬å¯¹è¾“å…¥æ‰°åŠ¨å¦‚ä½•åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼ æ’­çš„æ·±å…¥ç†è§£ï¼Œå¹¶é˜»ç¢äº†æç¤ºä¼˜åŒ–æ–¹æ³•çš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä»ç†è®ºä¸Šåˆ†æäº†è¾“å…¥æ‰°åŠ¨å¯¹CoTè¾“å‡ºæ³¢åŠ¨çš„å½±å“ã€‚æˆ‘ä»¬é¦–å…ˆæ¨å¯¼äº†åœ¨è¾“å‡ºæ³¢åŠ¨åœ¨å¯æ¥å—èŒƒå›´å†…çš„æ¡ä»¶ä¸‹ï¼Œè¾“å…¥æ‰°åŠ¨çš„ä¸Šé™ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¯æ˜ï¼šï¼ˆiï¼‰è¯¥ä¸Šé™ä¸CoTä¸­çš„æ¨ç†æ­¥æ•°å‘ˆæ­£ç›¸å…³ï¼›ï¼ˆiiï¼‰å³ä½¿æ˜¯æ— é™é•¿çš„æ¨ç†è¿‡ç¨‹ä¹Ÿæ— æ³•æ¶ˆé™¤è¾“å…¥æ‰°åŠ¨çš„å½±å“ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›ç»“è®ºåº”ç”¨äºçº¿æ€§è‡ªæ³¨æ„åŠ›ï¼ˆLSAï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥çœ‹ä½œæ˜¯Transformerçš„ç®€åŒ–ç‰ˆæœ¬ã€‚å¯¹äºLSAæ¨¡å‹ï¼Œæˆ‘ä»¬è¯æ˜äº†è¾“å…¥æ‰°åŠ¨çš„ä¸Šé™ä¸è¾“å…¥åµŒå…¥å’Œéšè—çŠ¶æ€å‘é‡çš„èŒƒæ•°å‘ˆè´Ÿç›¸å…³ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç†è®ºåˆ†æï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸»æµæ•°æ®é›†å’Œå››ä¸ªä¸»æµæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒã€‚å®éªŒç»“æœä¸æˆ‘ä»¬çš„ç†è®ºåˆ†æä¸€è‡´ï¼Œä»ç»éªŒä¸Šè¯æ˜äº†æˆ‘ä»¬å‘ç°çš„æ­£ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰CoTæ–¹æ³•å¯¹è¾“å…¥æ‰°åŠ¨æ•æ„Ÿï¼Œå¯¼è‡´è¾“å‡ºä¸ç¨³å®šã€‚ç¼ºä¹å¯¹CoTæ¨ç†è¿‡ç¨‹é²æ£’æ€§çš„ç†è®ºç†è§£ï¼Œé˜»ç¢äº†æ›´æœ‰æ•ˆçš„æç¤ºä¼˜åŒ–æ–¹æ³•çš„å‘å±•ã€‚è®ºæ–‡æ—¨åœ¨ä»ç†è®ºä¸Šåˆ†æè¾“å…¥æ‰°åŠ¨å¦‚ä½•å½±å“CoTè¾“å‡ºï¼Œå¹¶é‡åŒ–è¿™ç§å½±å“çš„è¾¹ç•Œã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨å¯¼ä¸€ä¸ªè¾“å…¥æ‰°åŠ¨çš„ä¸Šé™ï¼Œä½¿å¾—åœ¨æ‰°åŠ¨å°äºè¯¥ä¸Šé™æ—¶ï¼ŒCoTè¾“å‡ºçš„æ³¢åŠ¨åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚é€šè¿‡åˆ†æè¯¥ä¸Šé™ä¸CoTæ¨ç†æ­¥æ•°ã€åµŒå…¥èŒƒæ•°ç­‰å› ç´ çš„å…³ç³»ï¼Œæ­ç¤ºå½±å“CoTé²æ£’æ€§çš„å…³é”®å› ç´ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªé€šç”¨çš„CoTæ¨ç†æ¨¡å‹ï¼Œå¹¶å®šä¹‰äº†è¾“å…¥æ‰°åŠ¨å’Œè¾“å‡ºæ³¢åŠ¨ã€‚ç„¶åï¼Œåˆ©ç”¨æ•°å­¦å·¥å…·æ¨å¯¼äº†è¾“å…¥æ‰°åŠ¨ä¸Šé™çš„è¡¨è¾¾å¼ã€‚æ¥ç€ï¼Œå°†è¯¥ç†è®ºæ¡†æ¶åº”ç”¨äºLSAæ¨¡å‹ï¼Œå¹¶è¿›ä¸€æ­¥åˆ†æäº†LSAæ¨¡å‹ä¸­è¾“å…¥æ‰°åŠ¨ä¸Šé™ä¸åµŒå…¥èŒƒæ•°çš„å…³ç³»ã€‚æœ€åï¼Œé€šè¿‡å®éªŒéªŒè¯äº†ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„åˆ›æ–°åœ¨äºä»ç†è®ºä¸Šåˆ†æäº†CoTæ¨ç†çš„é²æ£’æ€§è¾¹ç•Œï¼Œå¹¶é‡åŒ–äº†è¾“å…¥æ‰°åŠ¨å¯¹CoTè¾“å‡ºçš„å½±å“ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè®ºæ–‡ä¸æ˜¯ç®€å•åœ°é€šè¿‡ä¼˜åŒ–æç¤ºæ¥æé«˜CoTçš„é²æ£’æ€§ï¼Œè€Œæ˜¯ä»æ ¹æœ¬ä¸Šæ­ç¤ºäº†å½±å“CoTé²æ£’æ€§çš„å†…åœ¨æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ•°å­¦å·¥å…·æ¨å¯¼è¾“å…¥æ‰°åŠ¨ä¸Šé™çš„è¡¨è¾¾å¼ï¼›2) å°†ç†è®ºæ¡†æ¶åº”ç”¨äºLSAæ¨¡å‹ï¼Œä»¥ä¾¿è¿›è¡Œæ›´å…·ä½“çš„åˆ†æï¼›3) é€šè¿‡å®éªŒéªŒè¯ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ã€‚è®ºæ–‡æ²¡æœ‰æ¶‰åŠå…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„çš„è®¾è®¡ï¼Œè€Œæ˜¯ä¾§é‡äºç†è®ºåˆ†æå’ŒéªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCoTçš„é²æ£’æ€§ä¸æ¨ç†æ­¥æ•°å‘ˆè´Ÿç›¸å…³ï¼Œä¸åµŒå…¥èŒƒæ•°å‘ˆæ­£ç›¸å…³ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ã€‚åœ¨ä¸‰ä¸ªä¸»æµæ•°æ®é›†å’Œå››ä¸ªä¸»æµæ¨¡å‹ä¸Šçš„å®éªŒç»“æœå‡ä¸ç†è®ºåˆ†æä¸€è‡´ï¼Œè¯æ˜äº†è¯¥ç†è®ºçš„æ™®é€‚æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡CoTæ¨¡å‹çš„é²æ£’æ€§ï¼Œä¾‹å¦‚ï¼Œåœ¨å¯¹å®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥ç†è®ºæŒ‡å¯¼æç¤ºè®¾è®¡ï¼Œé™ä½æ¨¡å‹å—åˆ°æ¶æ„è¾“å…¥æ”»å‡»çš„é£é™©ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¸ºå¼€å‘æ›´é²æ£’çš„æ¨ç†æ¨¡å‹æä¾›ç†è®ºåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing research indicates that the output of Chain-of-Thought (CoT) is significantly affected by input perturbations. Although many methods aim to mitigate such impact by optimizing prompts, a theoretical explanation of how these perturbations influence CoT outputs remains an open area of research. This gap limits our in-depth understanding of how input perturbations propagate during the reasoning process and hinders further improvements in prompt optimization methods. Therefore, in this paper, we theoretically analyze the effect of input perturbations on the fluctuation of CoT outputs. We first derive an upper bound for input perturbations under the condition that the output fluctuation is within an acceptable range, based on which we prove that: (i) This upper bound is positively correlated with the number of reasoning steps in the CoT; (ii) Even an infinitely long reasoning process cannot eliminate the impact of input perturbations. We then apply these conclusions to the Linear Self-Attention (LSA) model, which can be viewed as a simplified version of the Transformer. For the LSA model, we prove that the upper bound for input perturbation is negatively correlated with the norms of the input embedding and hidden state vectors. To validate this theoretical analysis, we conduct experiments on three mainstream datasets and four mainstream models. The experimental results align with our theoretical analysis, empirically demonstrating the correctness of our findings.

