---
layout: default
title: Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning
---

# Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21487" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21487v2</a>
  <a href="https://arxiv.org/pdf/2509.21487.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21487v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21487v2', 'Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jillian Xu, Dylan Zhou, Vinay Shukla, Yang Yang, Junrui Ruan, Shuhuai Lin, Wenfei Zou, Yinxiao Liu, Karthik Lakshmanan

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25 (æ›´æ–°: 2025-09-29)

**å¤‡æ³¨**: 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Efficient Reasoning Workshop

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒå¤´æ¨ç†è’¸é¦(DHRD)ï¼Œåœ¨ä¸ç‰ºç‰²æ¨ç†é€Ÿåº¦çš„å‰æä¸‹æå‡åˆ†ç±»å™¨ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒå¤´æ¨ç†è’¸é¦` `Chain-of-Thought` `çŸ¥è¯†è’¸é¦` `è¯­è¨€æ¨¡å‹` `åˆ†ç±»ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Chain-of-Thought (CoT) promptingè™½ç„¶èƒ½æé«˜åˆ†ç±»ç²¾åº¦ï¼Œä½†æ¨ç†è¿‡ç¨‹çš„ç”Ÿæˆæ˜¾è‘—é™ä½äº†ååé‡ã€‚
2. DHRDæ–¹æ³•é€šè¿‡å¼•å…¥ä»…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¨ç†å¤´ï¼Œå¹¶åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDHRDåœ¨SuperGLUEä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„æ¨ç†é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŒå¤´æ¨ç†è’¸é¦(DHRD)çš„ç®€å•è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºdecoder-onlyè¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•æ·»åŠ äº†ä¸¤ä¸ªå¤´éƒ¨ï¼š(i)ä¸€ä¸ªåœ¨è®­ç»ƒå’Œæ¨ç†æœŸé—´ä½¿ç”¨çš„æ± åŒ–åˆ†ç±»å¤´ï¼Œä»¥åŠ(ii)ä¸€ä¸ªä»…åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ã€ç”±æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹ç›‘ç£çš„æ¨ç†å¤´ã€‚è®­ç»ƒæ—¶ï¼Œä½¿ç”¨æ ‡ç­¾äº¤å‰ç†µå’Œè¾“å…¥åŠ æ¨ç†è¿‡ç¨‹åºåˆ—ä¸Šçš„tokençº§è¯­è¨€æ¨¡å‹æŸå¤±çš„åŠ æƒå’Œä½œä¸ºæŸå¤±å‡½æ•°ã€‚åœ¨ä¸ƒä¸ªSuperGLUEä»»åŠ¡ä¸Šï¼ŒDHRDç›¸å¯¹äºæ± åŒ–åŸºçº¿äº§ç”Ÿäº†0.65-5.47%çš„ç›¸å¯¹å¢ç›Šï¼Œåœ¨è•´å«/å› æœä»»åŠ¡ä¸Šçš„å¢ç›Šå°¤ä¸ºæ˜¾è‘—ã€‚ç”±äºåœ¨æµ‹è¯•æ—¶ç¦ç”¨äº†æ¨ç†å¤´ï¼Œå› æ­¤æ¨ç†ååé‡ä¸æ± åŒ–åˆ†ç±»å™¨ç›¸åŒ¹é…ï¼Œå¹¶ä¸”åœ¨ç›¸åŒçš„backboneä¸Šï¼ŒQPSæ¯”CoTè§£ç é«˜96-142å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³Chain-of-Thought (CoT) promptingåœ¨æå‡åˆ†ç±»ç²¾åº¦æ—¶å¸¦æ¥çš„æ¨ç†é€Ÿåº¦ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨CoTè¿›è¡Œæ¨ç†ï¼Œè™½ç„¶ç²¾åº¦é«˜ï¼Œä½†ç”±äºéœ€è¦ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´ååé‡æ˜¾è‘—é™ä½ã€‚è€Œç®€å•çš„æ± åŒ–åˆ†ç±»å™¨è™½ç„¶é€Ÿåº¦å¿«ï¼Œä½†ç²¾åº¦ä¸å¦‚CoTã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è’¸é¦å­¦ä¹ ï¼Œåœ¨è®­ç»ƒé˜¶æ®µå¼•å…¥ä¸€ä¸ªæ¨ç†å¤´ï¼Œå¹¶ä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œä»è€Œè®©å­¦ç”Ÿæ¨¡å‹å­¦ä¹ åˆ°CoTçš„æ¨ç†èƒ½åŠ›ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œåªä½¿ç”¨æ± åŒ–åˆ†ç±»å¤´ï¼Œä»è€Œä¿è¯æ¨ç†é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDHRDæ–¹æ³•çš„æ ¸å¿ƒæ˜¯è®­ç»ƒä¸€ä¸ªå¸¦æœ‰ä¸¤ä¸ªheadçš„decoder-onlyè¯­è¨€æ¨¡å‹ã€‚ä¸€ä¸ªheadæ˜¯æ± åŒ–åˆ†ç±»å¤´ï¼Œç”¨äºæœ€ç»ˆçš„åˆ†ç±»ä»»åŠ¡ï¼›å¦ä¸€ä¸ªheadæ˜¯æ¨ç†å¤´ï¼Œä»…åœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨ï¼Œç”¨äºå­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªæŸå¤±å‡½æ•°ï¼šä¸€ä¸ªæ˜¯æ ‡å‡†çš„æ ‡ç­¾äº¤å‰ç†µæŸå¤±ï¼Œç”¨äºç›‘ç£åˆ†ç±»å¤´çš„å­¦ä¹ ï¼›å¦ä¸€ä¸ªæ˜¯tokençº§åˆ«çš„è¯­è¨€æ¨¡å‹æŸå¤±ï¼Œç”¨äºç›‘ç£æ¨ç†å¤´çš„å­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨¡ä»¿æ•™å¸ˆæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šDHRDçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ¨ç†è¿‡ç¨‹çš„å­¦ä¹ ä¸æœ€ç»ˆçš„åˆ†ç±»ä»»åŠ¡è§£è€¦ã€‚é€šè¿‡åœ¨è®­ç»ƒé˜¶æ®µå¼•å…¥æ¨ç†å¤´ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°CoTçš„æ¨ç†èƒ½åŠ›ï¼Œè€Œåœ¨æ¨ç†é˜¶æ®µåªä½¿ç”¨æ± åŒ–åˆ†ç±»å¤´ï¼Œä»è€Œé¿å…äº†æ¨ç†è¿‡ç¨‹çš„ç”Ÿæˆï¼Œä¿è¯äº†æ¨ç†é€Ÿåº¦ã€‚è¿™ç§train-time-only reasoningçš„æ–¹å¼ï¼Œåœ¨ä¸ç‰ºç‰²æ¨ç†é€Ÿåº¦çš„å‰æä¸‹ï¼Œæå‡äº†åˆ†ç±»ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šDHRDçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) ä½¿ç”¨ä¸¤ä¸ªheadï¼Œä¸€ä¸ªç”¨äºåˆ†ç±»ï¼Œä¸€ä¸ªç”¨äºæ¨ç†ï¼›(2) ä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä½œä¸ºç›‘ç£ä¿¡å·ï¼›(3) ä½¿ç”¨æ ‡ç­¾äº¤å‰ç†µå’Œtokençº§åˆ«è¯­è¨€æ¨¡å‹æŸå¤±çš„åŠ æƒå’Œä½œä¸ºæŸå¤±å‡½æ•°ï¼›(4) åœ¨æ¨ç†é˜¶æ®µç¦ç”¨æ¨ç†å¤´ï¼Œåªä½¿ç”¨æ± åŒ–åˆ†ç±»å¤´ã€‚æŸå¤±å‡½æ•°çš„æƒé‡éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œä»¥å¹³è¡¡åˆ†ç±»ç²¾åº¦å’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DHRDåœ¨ä¸ƒä¸ªSuperGLUEä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸å¯¹äºæ± åŒ–åŸºçº¿ï¼Œç›¸å¯¹å¢ç›Šè¾¾åˆ°äº†0.65-5.47%ã€‚å°¤å…¶åœ¨è•´å«/å› æœä»»åŠ¡ä¸Šï¼Œå¢ç›Šæ›´ä¸ºæ˜¾è‘—ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒDHRDåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶ï¼Œæ¨ç†é€Ÿåº¦ä¸æ± åŒ–åˆ†ç±»å™¨ç›¸å½“ï¼Œå¹¶ä¸”åœ¨ç›¸åŒçš„backboneä¸Šï¼ŒQPSæ¯”CoTè§£ç é«˜96-142å€ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DHRDæ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§éœ€è¦é«˜ç²¾åº¦å’Œé«˜æ•ˆç‡çš„åˆ†ç±»ä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€ç†è§£ã€æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æç­‰ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¡ç®—ç¯å¢ƒï¼Œåœ¨è¿™äº›åœºæ™¯ä¸‹ï¼Œæ¨ç†é€Ÿåº¦è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼ŒDHRDè¿˜å¯ä»¥ä½œä¸ºä¸€ç§é€šç”¨çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæå‡å„ç§decoder-onlyè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-Thought (CoT) prompting often improves classification accuracy, but it introduces a significant throughput penalty with rationale generation (Wei et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for decoder-only language models (LMs) that adds (i) a pooled classification head used during training and inference and (ii) a reasoning head supervised by teacher rationales used only in training. We train with a loss function that is a weighted sum of label cross-entropy and token-level LM loss over input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative gains of 0.65-5.47% over pooled baselines, with notably larger gains on entailment/causal tasks. Since we disable the reasoning head at test time, inference throughput matches pooled classifiers and exceeds CoT decoding on the same backbones by 96-142 times in QPS.

