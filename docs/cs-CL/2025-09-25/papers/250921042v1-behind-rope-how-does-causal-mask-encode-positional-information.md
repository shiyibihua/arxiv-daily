---
layout: default
title: Behind RoPE: How Does Causal Mask Encode Positional Information?
---

# Behind RoPE: How Does Causal Mask Encode Positional Information?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21042" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21042v1</a>
  <a href="https://arxiv.org/pdf/2509.21042.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21042v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21042v1', 'Behind RoPE: How Does Causal Mask Encode Positional Information?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junu Kim, Xiao Liu, Zhenghao Lin, Lei Ji, Yeyun Gong, Edward Choi

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**å¤‡æ³¨**: Codes available at: https://github.com/starmpcc/causal_mask_encodes_positional

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºRoPEèƒŒåæœºåˆ¶ï¼šå› æœæ©ç å¦‚ä½•ç¼–ç ä½ç½®ä¿¡æ¯**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Transformer` `ä½ç½®ç¼–ç ` `å› æœæ©ç ` `æ³¨æ„åŠ›æœºåˆ¶` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformerè§£ç å™¨ä¾èµ–ä½ç½®ç¼–ç ï¼Œä½†å› æœæ©ç çš„ä½œç”¨è¢«å¿½è§†ï¼Œå…¶å½±å“æœ‰å¾…æ·±å…¥ç ”ç©¶ã€‚
2. è®ºæ–‡è¯æ˜å› æœæ©ç æœ¬èº«èƒ½åœ¨æ³¨æ„åŠ›åˆ†æ•°ä¸­å¼•å…¥ä½ç½®ä¾èµ–æ¨¡å¼ï¼Œæ— éœ€å‚æ•°æˆ–å› æœå…³ç³»ã€‚
3. å®éªŒéªŒè¯äº†ç†è®ºåˆ†æï¼Œå‘ç°å› æœæ©ç ä¸RoPEçš„äº¤äº’ä¼šæ‰­æ›²RoPEçš„ç›¸å¯¹ä½ç½®ç¼–ç ç‰¹æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Transformerè§£ç å™¨ä¸­ï¼ŒRoPEç­‰æ˜¾å¼ä½ç½®ç¼–ç æ˜¯ä½ç½®ä¿¡æ¯çš„ä¸»è¦æ¥æºï¼Œä½†å› æœæ©ç ä¹Ÿæä¾›äº†ä½ç½®ä¿¡æ¯ã€‚æœ¬æ–‡è¯æ˜ï¼Œå³ä½¿åœ¨æ²¡æœ‰å‚æ•°æˆ–è¾“å…¥ä¸­çš„å› æœä¾èµ–å…³ç³»çš„æƒ…å†µä¸‹ï¼Œå› æœæ©ç ä¹Ÿèƒ½åœ¨æ³¨æ„åŠ›åˆ†æ•°ä¸­è¯±å¯¼å‡ºä½ç½®ç›¸å…³çš„æ¨¡å¼ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿™ç§è¯±å¯¼çš„æ³¨æ„åŠ›æ¨¡å¼å€¾å‘äºåçˆ±é™„è¿‘çš„æŸ¥è¯¢-é”®å¯¹ï¼Œè¿™ä¸å¸¸è§çš„ä½ç½®ç¼–ç çš„è¡Œä¸ºç›¸ä¼¼ã€‚å®è¯åˆ†æè¯å®ï¼Œè®­ç»ƒåçš„æ¨¡å‹è¡¨ç°å‡ºç›¸åŒçš„è¡Œä¸ºï¼Œå¹¶ä¸”å­¦ä¹ åˆ°çš„å‚æ•°è¿›ä¸€æ­¥æ”¾å¤§äº†è¿™äº›æ¨¡å¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å› æœæ©ç å’ŒRoPEçš„ç›¸äº’ä½œç”¨ä¼šå°†RoPEçš„ç›¸å¯¹æ³¨æ„åŠ›åˆ†æ•°æ¨¡å¼æ‰­æ›²ä¸ºéç›¸å¯¹æ¨¡å¼ã€‚æˆ‘ä»¬åœ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸€è‡´åœ°è§‚å¯Ÿåˆ°è¿™ç§æ•ˆåº”ï¼Œè¿™è¡¨æ˜åœ¨æ˜¾å¼ä½ç½®ç¼–ç ä¹‹å¤–ï¼Œè€ƒè™‘å› æœæ©ç ä½œä¸ºä½ç½®ä¿¡æ¯æ¥æºçš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Transformerè§£ç å™¨ä¸»è¦ä¾èµ–RoPEç­‰æ˜¾å¼ä½ç½®ç¼–ç æ¥æä¾›ä½ç½®ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†å› æœæ©ç æœ¬èº«æ‰€è•´å«çš„ä½ç½®ä¿¡æ¯ã€‚ç°æœ‰ç ”ç©¶å¯¹å› æœæ©ç å¦‚ä½•å½±å“æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥åŠå®ƒä¸æ˜¾å¼ä½ç½®ç¼–ç çš„äº¤äº’ä½œç”¨ç¼ºä¹æ·±å…¥ç†è§£ã€‚è¿™å¯èƒ½å¯¼è‡´å¯¹æ¨¡å‹è¡Œä¸ºçš„è¯¯åˆ¤ï¼Œä»¥åŠå¯¹ä½ç½®ç¼–ç æ–¹å¼çš„ä¸åˆç†è®¾è®¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è¯æ˜å¹¶åˆ†æå› æœæ©ç æœ¬èº«å¯ä»¥è¯±å¯¼å‡ºä½ç½®ç›¸å…³çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚å³ä½¿åœ¨æ²¡æœ‰å‚æ•°æˆ–è¾“å…¥å› æœä¾èµ–çš„æƒ…å†µä¸‹ï¼Œå› æœæ©ç ä¹Ÿèƒ½ä½¿æ¨¡å‹å€¾å‘äºå…³æ³¨é™„è¿‘çš„query-keyå¯¹ï¼Œä»è€Œèµ·åˆ°ç±»ä¼¼ä½ç½®ç¼–ç çš„ä½œç”¨ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæ­ç¤ºäº†å› æœæ©ç åœ¨Transformerä¸­çš„éšå¼ä½ç½®ç¼–ç èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é¦–å…ˆè¿›è¡Œäº†ç†è®ºåˆ†æï¼Œæ¨å¯¼äº†å› æœæ©ç å¦‚ä½•å½±å“æ³¨æ„åŠ›åˆ†æ•°çš„æ•°å­¦å…¬å¼ã€‚ç„¶åï¼Œé€šè¿‡å®éªŒéªŒè¯äº†ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ï¼ŒåŒ…æ‹¬åœ¨ä¸åŒæ¨¡å‹æ¶æ„å’Œæ•°æ®é›†ä¸Šè§‚å¯Ÿå› æœæ©ç å¯¹æ³¨æ„åŠ›æ¨¡å¼çš„å½±å“ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†å› æœæ©ç ä¸RoPEç­‰æ˜¾å¼ä½ç½®ç¼–ç çš„äº¤äº’ä½œç”¨ï¼Œæ­ç¤ºäº†å®ƒä»¬ä¹‹é—´çš„ç›¸äº’å½±å“ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šç†è®ºæ¨å¯¼ -> å®éªŒéªŒè¯ -> äº¤äº’åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ­ç¤ºäº†å› æœæ©ç çš„éšå¼ä½ç½®ç¼–ç èƒ½åŠ›ã€‚ä»¥å¾€ç ”ç©¶ä¸»è¦å…³æ³¨æ˜¾å¼ä½ç½®ç¼–ç çš„ä½œç”¨ï¼Œè€Œå¿½ç•¥äº†å› æœæ©ç æœ¬èº«æ‰€è•´å«çš„ä½ç½®ä¿¡æ¯ã€‚è®ºæ–‡é¦–æ¬¡è¯æ˜äº†å› æœæ©ç å¯ä»¥åœ¨æ²¡æœ‰å‚æ•°æˆ–å› æœä¾èµ–çš„æƒ…å†µä¸‹ï¼Œè¯±å¯¼å‡ºä½ç½®ç›¸å…³çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¿™ä¸ºç†è§£Transformerçš„è¡Œä¸ºæä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è®¾è®¡äº†ç†è®ºåˆ†ææ¡†æ¶ï¼Œæ¨å¯¼äº†å› æœæ©ç å¯¹æ³¨æ„åŠ›åˆ†æ•°çš„æ•°å­¦å½±å“ï¼›2) è®¾è®¡äº†å®éªŒæ–¹æ¡ˆï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ï¼Œå¹¶åˆ†æäº†å› æœæ©ç ä¸RoPEçš„äº¤äº’ä½œç”¨ï¼›3) é‡‡ç”¨äº†å¤šç§æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†ï¼Œä»¥ç¡®ä¿ç»“è®ºçš„æ™®é€‚æ€§ã€‚è®ºæ–‡æ²¡æœ‰æ¶‰åŠç‰¹åˆ«å¤æ‚çš„å‚æ•°è®¾ç½®æˆ–æŸå¤±å‡½æ•°ï¼Œé‡ç‚¹åœ¨äºæ­ç¤ºå› æœæ©ç çš„å†…åœ¨æœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæ­ç¤ºäº†å› æœæ©ç åœ¨Transformerè§£ç å™¨ä¸­çš„éšå¼ä½ç½®ç¼–ç èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè®­ç»ƒåçš„æ¨¡å‹è¡¨ç°å‡ºä¸ç†è®ºåˆ†æä¸€è‡´çš„è¡Œä¸ºï¼Œå¹¶ä¸”å› æœæ©ç ä¸RoPEçš„äº¤äº’ä¼šæ‰­æ›²RoPEçš„ç›¸å¯¹ä½ç½®ç¼–ç ç‰¹æ€§ã€‚è¿™äº›å‘ç°å¯¹ç†è§£å’Œä¼˜åŒ–Transformeræ¨¡å‹å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–è®¾è®¡ï¼Œä¾‹å¦‚ï¼Œåœ¨è®¾è®¡ä½ç½®ç¼–ç æ–¹æ¡ˆæ—¶ï¼Œå¯ä»¥è€ƒè™‘å› æœæ©ç çš„å½±å“ï¼Œé¿å…å†—ä½™æˆ–å†²çªã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥å¸®åŠ©æ›´å¥½åœ°ç†è§£Transformeræ¨¡å‹çš„è¡Œä¸ºï¼Œä¸ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ç ”ç©¶æä¾›æ–°çš„æ€è·¯ã€‚æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•åˆ©ç”¨å› æœæ©ç çš„éšå¼ä½ç½®ç¼–ç èƒ½åŠ›ï¼Œè®¾è®¡æ›´é«˜æ•ˆçš„Transformeræ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While explicit positional encodings such as RoPE are a primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPE's relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as a source of positional information alongside explicit positional encodings.

