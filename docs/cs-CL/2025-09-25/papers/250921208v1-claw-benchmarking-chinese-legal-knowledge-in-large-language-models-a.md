---
layout: default
title: CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis
---

# CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21208" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21208v1</a>
  <a href="https://arxiv.org/pdf/2509.21208.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21208v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21208v1', 'CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinzhe Xu, Liang Zhao, Hongshen Xu, Chen Chen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CLawï¼šæ„å»ºä¸­æ–‡æ³•å¾‹çŸ¥è¯†åŸºå‡†ï¼Œè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æ³•å¾‹æ¨ç†ä¸­çš„èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ³•å¾‹çŸ¥è¯†` `ä¸­æ–‡æ³•å¾‹` `åŸºå‡†æµ‹è¯•` `æ³•å¾‹æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸåº”ç”¨ä¸­ï¼Œç¼ºä¹å¯¹æ³•å¾‹çŸ¥è¯†çš„ä¸“é—¨è®­ç»ƒï¼Œå¯¼è‡´å…¶æ³•å¾‹çŸ¥è¯†æ·±åº¦ä¸è¶³ï¼Œå½±å“äº†æ¨ç†çš„å¯é æ€§ã€‚
2. CLawåŸºå‡†é€šè¿‡æ„å»ºç»†ç²’åº¦çš„æ³•å¾‹è¯­æ–™åº“å’Œæ¡ˆä¾‹æ¨ç†å®ä¾‹ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡æ³•å¾‹çŸ¥è¯†å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨å‡†ç¡®å†ç°æ³•å¾‹æ¡æ–‡æ–¹é¢å­˜åœ¨æ˜¾è‘—å›°éš¾ï¼Œè¿™ä¸¥é‡å½±å“äº†å…¶æ³•å¾‹æ¨ç†çš„å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºåˆ†ææ³•å¾‹æ–‡æœ¬å’Œå¼•ç”¨ç›¸å…³æ³•è§„ï¼Œä½†ç”±äºé€šç”¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹å¯¹æ³•å¾‹é¢†åŸŸçš„ä¸“é—¨å…³æ³¨ï¼Œå…¶å¯é æ€§å¸¸å¸¸å—åˆ°å½±å“ï¼Œæ©ç›–äº†å…¶æ³•å¾‹çŸ¥è¯†çš„çœŸæ­£æ·±åº¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCLawçš„æ–°åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°LLMsåœ¨ä¸­æ–‡æ³•å¾‹çŸ¥è¯†åŠå…¶åœ¨æ¨ç†ä¸­çš„åº”ç”¨ã€‚CLawåŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªå…¨é¢çš„ã€ç»†ç²’åº¦çš„è¯­æ–™åº“ï¼ŒåŒ…å«æ‰€æœ‰306éƒ¨ä¸­å›½å›½å®¶æ³•è§„ï¼Œåˆ†å‰²åˆ°å­æ¡æ¬¾çº§åˆ«ï¼Œå¹¶ç»“åˆç²¾ç¡®çš„å†å²ä¿®è®¢æ—¶é—´æˆ³ï¼Œç”¨äºä¸¥æ ¼çš„å¬å›è¯„ä¼°ï¼ˆ64,849æ¡ç›®ï¼‰ï¼›ï¼ˆ2ï¼‰ä¸€ç»„å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€åŸºäºæ¡ˆä¾‹çš„æ¨ç†å®ä¾‹ï¼ˆ254ä¸ªï¼‰ï¼Œè¿™äº›å®ä¾‹æ¥è‡ªä¸­å›½æœ€é«˜æ³•é™¢ç­–åˆ’çš„ææ–™ï¼Œç”¨äºè¯„ä¼°æ³•å¾‹çŸ¥è¯†çš„å®é™…åº”ç”¨ã€‚æˆ‘ä»¬çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œå¤§å¤šæ•°å½“ä»£LLMsåœ¨å¿ å®åœ°å†ç°æ³•å¾‹æ¡æ–‡æ–¹é¢éƒ½å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚ç”±äºå‡†ç¡®æ£€ç´¢å’Œå¼•ç”¨æ³•å¾‹æ¡æ–‡æ˜¯æ³•å¾‹æ¨ç†çš„åŸºç¡€ï¼Œå› æ­¤è¿™ç§ç¼ºé™·ä¸¥é‡æŸå®³äº†å…¶å“åº”çš„å¯é æ€§ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¦åœ¨LLMsä¸­å®ç°å¯ä¿¡çš„æ³•å¾‹æ¨ç†ï¼Œéœ€è¦å‡†ç¡®çš„çŸ¥è¯†æ£€ç´¢ï¼ˆå¯èƒ½é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¥å¢å¼ºï¼‰å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›çš„å¼ºå¤§ååŒä½œç”¨ã€‚è¿™é¡¹å·¥ä½œä¸ºæ¨è¿›ç‰¹å®šé¢†åŸŸçš„LLMæ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„æ³•å¾‹é¢†åŸŸï¼Œæä¾›äº†é‡è¦çš„åŸºå‡†å’Œå…³é”®è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ³•å¾‹æ–‡æœ¬æ—¶ï¼Œç”±äºç¼ºä¹ä¸“é—¨çš„æ³•å¾‹çŸ¥è¯†è®­ç»ƒï¼Œæ— æ³•å‡†ç¡®ç†è§£å’Œåº”ç”¨æ³•å¾‹æ¡æ–‡ï¼Œå¯¼è‡´åœ¨æ³•å¾‹æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥è¯„ä¼°æ¨¡å‹åœ¨ç»†ç²’åº¦æ³•å¾‹çŸ¥è¯†ä¸Šçš„æŒæ¡ç¨‹åº¦ï¼Œä¹Ÿæ— æ³•æœ‰æ•ˆè¡¡é‡æ¨¡å‹åœ¨å®é™…æ¡ˆä¾‹ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCLawçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªé«˜è´¨é‡ã€ç»†ç²’åº¦çš„ä¸­æ–‡æ³•å¾‹çŸ¥è¯†åŸºå‡†ï¼ŒåŒ…æ‹¬å…¨é¢çš„æ³•å¾‹è¯­æ–™åº“å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ˆä¾‹æ¨ç†å®ä¾‹ï¼Œä»è€Œå…¨é¢è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„çŸ¥è¯†æŒæ¡å’Œæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç²¾ç¡®çš„å†å²ä¿®è®¢æ—¶é—´æˆ³ï¼Œå¯ä»¥è¿›è¡Œæ›´ä¸¥æ ¼çš„å¬å›è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLawåŸºå‡†ä¸»è¦åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼šä¸€æ˜¯ç»†ç²’åº¦çš„ä¸­æ–‡æ³•å¾‹è¯­æ–™åº“ï¼ŒåŒ…å«æ‰€æœ‰306éƒ¨ä¸­å›½å›½å®¶æ³•è§„ï¼Œåˆ†å‰²åˆ°å­æ¡æ¬¾çº§åˆ«ï¼Œå¹¶åŒ…å«å†å²ä¿®è®¢æ—¶é—´æˆ³ã€‚äºŒæ˜¯æ¡ˆä¾‹æ¨ç†å®ä¾‹ï¼ŒåŒ…å«254ä¸ªæ¥è‡ªä¸­å›½æœ€é«˜æ³•é™¢çš„æ¡ˆä¾‹ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å®é™…æ¡ˆä¾‹ä¸­çš„æ³•å¾‹æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°è¿‡ç¨‹åŒ…æ‹¬çŸ¥è¯†å¬å›å’Œæ¡ˆä¾‹æ¨ç†ä¸¤ä¸ªé˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šCLawçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»†ç²’åº¦çš„æ³•å¾‹è¯­æ–™åº“å’Œæ¡ˆä¾‹æ¨ç†å®ä¾‹ã€‚è¯­æ–™åº“çš„ç»†ç²’åº¦åˆ†å‰²å’Œå†å²ä¿®è®¢æ—¶é—´æˆ³ä½¿å¾—å¯ä»¥è¿›è¡Œæ›´ç²¾ç¡®çš„çŸ¥è¯†å¬å›è¯„ä¼°ã€‚æ¡ˆä¾‹æ¨ç†å®ä¾‹åˆ™æ¨¡æ‹Ÿäº†å®é™…çš„æ³•å¾‹åº”ç”¨åœºæ™¯ï¼Œæ›´çœŸå®åœ°åæ˜ äº†æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯­æ–™åº“çš„æ„å»ºé‡‡ç”¨äº†äººå·¥æ ‡æ³¨å’Œè‡ªåŠ¨åˆ†å‰²ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œç¡®ä¿äº†è¯­æ–™åº“çš„è´¨é‡å’Œè¦†ç›–èŒƒå›´ã€‚æ¡ˆä¾‹æ¨ç†å®ä¾‹çš„è®¾è®¡å‚è€ƒäº†ä¸­å›½æœ€é«˜æ³•é™¢çš„æ¡ˆä¾‹ï¼Œå¹¶è¿›è¡Œäº†é€‚å½“çš„ç®€åŒ–å’ŒæŠ½è±¡ï¼Œä»¥ä¾¿äºæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬çŸ¥è¯†å¬å›ç‡å’Œæ¡ˆä¾‹æ¨ç†å‡†ç¡®ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨CLawåŸºå‡†ä¸Šè¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨çŸ¥è¯†å¬å›æ–¹é¢ã€‚å¤§å¤šæ•°æ¨¡å‹æ— æ³•å‡†ç¡®åœ°æ£€ç´¢å’Œå¼•ç”¨ç›¸å…³çš„æ³•å¾‹æ¡æ–‡ï¼Œè¿™ä¸¥é‡å½±å“äº†å…¶åœ¨æ¡ˆä¾‹æ¨ç†ä¸­çš„è¡¨ç°ã€‚ä¾‹å¦‚ï¼Œåœ¨çŸ¥è¯†å¬å›ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡ä½äº50%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„çŸ¥è¯†æŒæ¡å’Œæ¨ç†èƒ½åŠ›ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CLawåŸºå‡†çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ³•å¾‹å’¨è¯¢ã€æ™ºèƒ½åˆåŒå®¡æŸ¥ã€æ³•å¾‹æ–‡ä¹¦ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„çŸ¥è¯†æŒæ¡å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºæ³•å¾‹ä»ä¸šè€…æä¾›æ›´é«˜æ•ˆã€æ›´å‡†ç¡®çš„è¾…åŠ©å·¥å…·ï¼Œä»è€Œæå‡æ³•å¾‹æœåŠ¡çš„è´¨é‡å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å¯ä»¥ç”¨äºå¼€å‘æ›´æ™ºèƒ½çš„æ³•å¾‹AIç³»ç»Ÿï¼Œæ¨åŠ¨æ³•å¾‹è¡Œä¸šçš„æ™ºèƒ½åŒ–è½¬å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly tasked with analyzing legal texts and citing relevant statutes, yet their reliability is often compromised by general pre-training that ingests legal texts without specialized focus, obscuring the true depth of their legal knowledge. This paper introduces CLaw, a novel benchmark specifically engineered to meticulously evaluate LLMs on Chinese legal knowledge and its application in reasoning. CLaw comprises two key components: (1) a comprehensive, fine-grained corpus of all 306 Chinese national statutes, segmented to the subparagraph level and incorporating precise historical revision timesteps for rigorous recall evaluation (64,849 entries), and (2) a challenging set of 254 case-based reasoning instances derived from China Supreme Court curated materials to assess the practical application of legal knowledge. Our empirical evaluation reveals that most contemporary LLMs significantly struggle to faithfully reproduce legal provisions. As accurate retrieval and citation of legal provisions form the basis of legal reasoning, this deficiency critically undermines the reliability of their responses. We contend that achieving trustworthy legal reasoning in LLMs requires a robust synergy of accurate knowledge retrieval--potentially enhanced through supervised fine-tuning (SFT) or retrieval-augmented generation (RAG)--and strong general reasoning capabilities. This work provides an essential benchmark and critical insights for advancing domain-specific LLM reasoning, particularly within the complex legal sphere.

