---
layout: default
title: OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule
---

# OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21623" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.21623v1</a>
  <a href="https://arxiv.org/pdf/2509.21623.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21623v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21623v1', 'OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja\'s Rule')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yuxuan Zhu, David H. Yang, Mohammad Mohammadi Amiri, Keerthiram Murugesan, Tejaswini Pedapati, Pin-Yu Chen

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-25

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**OjaKVÔºöÂà©Áî®OjaËßÑÂàôËøõË°å‰∏ä‰∏ãÊñáÊÑüÁü•Âú®Á∫ø‰ΩéÁß©KVÁºìÂ≠òÂéãÁº©ÔºåÊèêÂçáÈïøÊñáÊú¨Â§ÑÁêÜÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `KVÁºìÂ≠òÂéãÁº©` `‰ΩéÁß©Ëøë‰ºº` `Âú®Á∫øÂ≠¶‰π†` `ÈïøÊñáÊú¨Â§ÑÁêÜ` `OjaËßÑÂàô` `‰∏ä‰∏ãÊñáÊÑüÁü•` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâKVÁºìÂ≠òÂéãÁº©ÊñπÊ≥ï‰æùËµñÈùôÊÄÅÁ¶ªÁ∫øÂ≠¶‰π†ÁöÑÂ≠êÁ©∫Èó¥ÔºåÊó†Ê≥ïÈÄÇÂ∫îÈïøÊñáÊú¨‰∏≠Âä®ÊÄÅÂèòÂåñÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. OjaKVÈááÁî®Ê∑∑ÂêàÂ≠òÂÇ®Á≠ñÁï•ÂíåÂú®Á∫øÂ≠êÁ©∫Èó¥Ëá™ÈÄÇÂ∫îÔºåÂØπÈáçË¶ÅtokenÂÖ®Áß©‰øùÁïôÔºå‰∏≠Èó¥tokenËøõË°å‰ΩéÁß©ÂéãÁº©ÔºåÂπ∂‰ΩøÁî®OjaÁÆóÊ≥ïÂä®ÊÄÅË∞ÉÊï¥ÊäïÂΩ±Âü∫„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåOjaKVÂú®È´òÂéãÁº©ÊØî‰∏ã‰øùÊåÅÁîöËá≥ÊèêÈ´ò‰∫Üzero-shotÂáÜÁ°ÆÁéáÔºåÂ∞§ÂÖ∂Âú®ÈïøÊñáÊú¨Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊó†ÈúÄÊ®°ÂûãÂæÆË∞É„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏çÊñ≠Êâ©Â±ïÁöÑÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜËÉΩÂäõÂèóÂà∞ÊòæËëóÁöÑÂÜÖÂ≠òÁì∂È¢àÈôêÂà∂ÔºöËá™ÂõûÂΩíÁîüÊàêÊâÄÈúÄÁöÑÈîÆÂÄº(KV)ÁºìÂ≠ò„ÄÇËøôÁßçÁì∂È¢àÈùûÂ∏∏‰∏•ÈáçÔºõ‰æãÂ¶ÇÔºå‰∏Ä‰∏™Llama-3.1-8BÊ®°ÂûãÂ§ÑÁêÜ‰∏Ä‰∏™32K tokenÁöÑpromptÔºåbatch size‰∏∫4Êó∂ÔºåÂÖ∂KVÁºìÂ≠òÈúÄË¶ÅÂ§ßÁ∫¶16GBÔºåË∂ÖËøá‰∫ÜÊ®°ÂûãÊú¨Ë∫´ÁöÑÊùÉÈáç„ÄÇËôΩÁÑ∂ÈÄöËøá‰ΩéÁß©ÊäïÂΩ±ËøõË°åKVÁºìÂ≠òÂéãÁº©ÊòØ‰∏Ä‰∏™ÊúâÂ∏åÊúõÁöÑÊñπÂêëÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÈùôÊÄÅÁöÑ„ÄÅÁ¶ªÁ∫øÂ≠¶‰π†ÁöÑÂ≠êÁ©∫Èó¥ÔºåÂú®Êï∞ÊçÆÂàÜÂ∏ÉÂèëÁîüÂèòÂåñÊó∂Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜOjaKVÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂÆÉÂ∞ÜÊàòÁï•ÊÄßÁöÑÊ∑∑ÂêàÂ≠òÂÇ®Á≠ñÁï•‰∏éÂú®Á∫øÂ≠êÁ©∫Èó¥Ëá™ÈÄÇÂ∫îÁõ∏ÁªìÂêà„ÄÇÈ¶ñÂÖàÔºåOjaKVËÆ§ËØÜÂà∞Âπ∂ÈùûÊâÄÊúâtokenÂØπ‰∫éÂéãÁº©ÈÉΩÂêåÁ≠âÈáçË¶ÅÔºõÂÆÉ‰ª•ÂÖ®Áß©‰øùÁïôÂÖ≥ÈîÆÁöÑÁ¨¨‰∏Ä‰∏™ÂíåÊúÄËøëÁöÑtokenÔºå‰∏∫Ê≥®ÊÑèÂäõÊú∫Âà∂Áª¥Êä§È´ò‰øùÁúüÂ∫¶ÁöÑÈîöÁÇπ„ÄÇÂÖ∂Ê¨°ÔºåÂØπ‰∫éÁªùÂ§ßÂ§öÊï∞‰∏≠Èó¥tokenÔºåÂÆÉÈÄöËøá‰ΩøÁî®OjaÁÆóÊ≥ïËøõË°åÂú®Á∫ø‰∏ªÊàêÂàÜÂàÜÊûêÊù•Â¢ûÈáèÂú∞Ë∞ÉÊï¥ÊäïÂΩ±Âü∫Ôºå‰ªéËÄåÂ∫îÁî®‰ΩéÁß©ÂéãÁº©„ÄÇËøôÁßçËá™ÈÄÇÂ∫îÂåÖÊã¨Âú®promptÈ¢ÑÂ°´ÂÖÖÊúüÈó¥ÁöÑÂÖ®Èù¢Êõ¥Êñ∞ÂíåÂú®Ëß£Á†ÅÊúüÈó¥ÁöÑËΩªÈáèÁ∫ßÂë®ÊúüÊÄßÊõ¥Êñ∞ÔºåÁ°Æ‰øùÂ≠êÁ©∫Èó¥‰∏é‰∏çÊñ≠ÂèòÂåñÁöÑ‰∏ä‰∏ãÊñá‰øùÊåÅ‰∏ÄËá¥„ÄÇËá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòØÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂‰∏éÁé∞‰ª£Ê≥®ÊÑèÂäõÊ®°ÂùóÔºàÂ¶ÇFlashAttentionÔºâÂÆåÂÖ®ÂÖºÂÆπ„ÄÇÂÆûÈ™åË°®ÊòéÔºåOjaKVÂú®È´òÂéãÁº©ÊØî‰∏ã‰øùÊåÅÁîöËá≥ÊèêÈ´ò‰∫Üzero-shotÂáÜÁ°ÆÁéá„ÄÇÁâπÂà´ÊòØÔºåOjaKVÂú®ÈúÄË¶ÅÂ§çÊùÇÊé®ÁêÜÁöÑË∂ÖÈïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÊµãËØï‰∏≠Ëé∑Âæó‰∫ÜÊúÄÊòæËëóÁöÑÊî∂ÁõäÔºåÁ™ÅÂá∫‰∫ÜÂú®Á∫øÂ≠êÁ©∫Èó¥Ëá™ÈÄÇÂ∫îÂú®Âä®ÊÄÅË∑üË∏™‰∏ä‰∏ãÊñáÂèòÂåñ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ∑∑ÂêàÊ°ÜÊû∂ÊòØ‰∏ÄÁßçÂÆûÁî®ÁöÑ„ÄÅÂç≥ÊèíÂç≥Áî®ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÁî®‰∫éÂÜÖÂ≠òÈ´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÔºåËÄåÊó†ÈúÄÊ®°ÂûãÂæÆË∞É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜÈïøÊñáÊú¨Êó∂ÔºåKVÁºìÂ≠òÂç†Áî®Â§ßÈáèÂÜÖÂ≠òÔºåÊàê‰∏∫ÊÄßËÉΩÁì∂È¢à„ÄÇÁé∞ÊúâÁöÑ‰ΩéÁß©ÂéãÁº©ÊñπÊ≥ï‰æùËµñÈùôÊÄÅÂ≠êÁ©∫Èó¥ÔºåÊó†Ê≥ïÈÄÇÂ∫îÈïøÊñáÊú¨‰∏≠‰∏ä‰∏ãÊñáÁöÑÂä®ÊÄÅÂèòÂåñÔºåÂØºËá¥ÂéãÁº©ÊÄßËÉΩ‰∏ãÈôçÔºåÂΩ±ÂìçÊ®°ÂûãÂáÜÁ°ÆÁéá„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöOjaKVÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÁªìÂêàÊ∑∑ÂêàÂ≠òÂÇ®Á≠ñÁï•ÂíåÂú®Á∫øÂ≠êÁ©∫Èó¥Ëá™ÈÄÇÂ∫î„ÄÇÂÆÉËÆ§‰∏∫Âπ∂ÈùûÊâÄÊúâtokenÈÉΩÂêåÁ≠âÈáçË¶ÅÔºåÂõ†Ê≠§ÂØπÂÖ≥ÈîÆtokenÔºàÂ¶ÇÈ¶ñ‰∏™ÂíåÊúÄËøëÁöÑtokenÔºâËøõË°åÂÖ®Áß©‰øùÁïôÔºå‰øùËØÅÈáçË¶Å‰ø°ÊÅØÁöÑÂÆåÊï¥ÊÄß„ÄÇÂØπ‰∫é‰∏≠Èó¥tokenÔºåÂàôÈááÁî®‰ΩéÁß©ÂéãÁº©ÔºåÂπ∂Âà©Áî®OjaÁÆóÊ≥ïËøõË°åÂú®Á∫øÊõ¥Êñ∞Ôºå‰ΩøÂéãÁº©Â≠êÁ©∫Èó¥ËÉΩÂ§üÂä®ÊÄÅÈÄÇÂ∫î‰∏ä‰∏ãÊñáÁöÑÂèòÂåñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöOjaKVÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) Ê∑∑ÂêàÂ≠òÂÇ®Á≠ñÁï•ÔºöÂå∫ÂàÜÈáçË¶ÅtokenÂíå‰∏≠Èó¥tokenÔºåÂàÜÂà´ÈááÁî®ÂÖ®Áß©Â≠òÂÇ®Âíå‰ΩéÁß©ÂéãÁº©„ÄÇ2) Âú®Á∫øÂ≠êÁ©∫Èó¥Ëá™ÈÄÇÂ∫îÔºö‰ΩøÁî®OjaÁÆóÊ≥ïÂØπ‰ΩéÁß©ÂéãÁº©ÁöÑÊäïÂΩ±Âü∫ËøõË°åÂú®Á∫øÊõ¥Êñ∞ÔºåÂåÖÊã¨promptÈ¢ÑÂ°´ÂÖÖÊúüÈó¥ÁöÑÂÖ®Èù¢Êõ¥Êñ∞ÂíåËß£Á†ÅÊúüÈó¥ÁöÑËΩªÈáèÁ∫ßÂë®ÊúüÊÄßÊõ¥Êñ∞„ÄÇ3) ÂÖºÂÆπÊÄßËÆæËÆ°Ôºö‰∏éÁé∞‰ª£Ê≥®ÊÑèÂäõÊ®°ÂùóÔºàÂ¶ÇFlashAttentionÔºâÂÆåÂÖ®ÂÖºÂÆπÔºåÊòì‰∫éÈõÜÊàêÂà∞Áé∞ÊúâÁ≥ªÁªü‰∏≠„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöOjaKVÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Âú®Á∫øÂ≠êÁ©∫Èó¥Ëá™ÈÄÇÂ∫îËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÈùôÊÄÅÂ≠êÁ©∫Èó¥ÊñπÊ≥ï‰∏çÂêåÔºåOjaKVËÉΩÂ§üÊ†πÊçÆ‰∏ä‰∏ãÊñáÁöÑÂä®ÊÄÅÂèòÂåñÔºåÂÆûÊó∂Ë∞ÉÊï¥ÂéãÁº©Â≠êÁ©∫Èó¥Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÊçïÊçâÈïøÊñáÊú¨‰∏≠ÁöÑ‰ø°ÊÅØ„ÄÇËøôÁßçÂú®Á∫øËá™ÈÄÇÂ∫îËÉΩÂäõÊòØOjaKVÂú®ÈïøÊñáÊú¨Êé®ÁêÜ‰ªªÂä°‰∏≠ÂèñÂæó‰ºòÂºÇÊÄßËÉΩÁöÑÂÖ≥ÈîÆ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöOjaKVÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®OjaÁÆóÊ≥ïËøõË°åÂú®Á∫ø‰∏ªÊàêÂàÜÂàÜÊûêÔºå‰ª•Â¢ûÈáèÊñπÂºèÊõ¥Êñ∞ÊäïÂΩ±Âü∫„ÄÇ2) ÈááÁî®Ê∑∑ÂêàÂ≠òÂÇ®Á≠ñÁï•ÔºåÂπ≥Ë°°ÂéãÁº©ÁéáÂíå‰ø°ÊÅØÊçüÂ§±„ÄÇ3) ËÆæËÆ°ËΩªÈáèÁ∫ßÁöÑÂë®ÊúüÊÄßÊõ¥Êñ∞Êú∫Âà∂ÔºåÈôç‰ΩéËÆ°ÁÆóÂºÄÈîÄÔºå‰øùËØÅÂÆûÊó∂ÊÄß„ÄÇ4) Ê°ÜÊû∂ËÆæËÆ°‰øùËØÅ‰∏éFlashAttentionÁ≠âÁé∞ÊúâÂä†ÈÄüÊäÄÊúØÁöÑÂÖºÂÆπÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

OjaKVÂú®ÈïøÊñáÊú¨Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®ÈúÄË¶ÅÂ§çÊùÇÊé®ÁêÜÁöÑ‰ªªÂä°‰∏≠Ëé∑Âæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇÂÆûÈ™åË°®ÊòéÔºåOjaKVÂú®È´òÂéãÁº©ÊØî‰∏ã‰øùÊåÅÁîöËá≥ÊèêÈ´ò‰∫Üzero-shotÂáÜÁ°ÆÁéáÔºåËØÅÊòé‰∫ÜÂú®Á∫øÂ≠êÁ©∫Èó¥Ëá™ÈÄÇÂ∫îÂú®Âä®ÊÄÅË∑üË∏™‰∏ä‰∏ãÊñáÂèòÂåñ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇËØ•ÊñπÊ≥ïÊó†ÈúÄÊ®°ÂûãÂæÆË∞ÉÔºåÂç≥ÂèØ‰Ωú‰∏∫Âç≥ÊèíÂç≥Áî®ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂÆûÁé∞ÂÜÖÂ≠òÈ´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

OjaKVÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂ§ÑÁêÜÈïøÊñáÊú¨ÁöÑÂú∫ÊôØÔºåÂ¶ÇÈïøÊñáÊ°£ÊëòË¶Å„ÄÅÈïøÁØáÂ∞èËØ¥ÁîüÊàê„ÄÅÂ§çÊùÇÈóÆÁ≠îÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÈôç‰ΩéKVÁºìÂ≠òÁöÑÂÜÖÂ≠òÂç†Áî®ÔºåOjaKVËÉΩÂ§üÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äÁöÑÈÉ®ÁΩ≤ËÉΩÂäõÔºåÂπ∂Âä†ÈÄüÈïøÊñáÊú¨Êé®ÁêÜËøáÁ®ãÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The expanding long-context capabilities of large language models are constrained by a significant memory bottleneck: the key-value (KV) cache required for autoregressive generation. This bottleneck is substantial; for instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of 4 requires approximately 16GB for its KV cache, a size exceeding the model's weights. While KV-cache compression via low-rank projection is a promising direction, existing methods rely on a static, offline-learned subspace that performs poorly under data distribution shifts. To overcome these limitations, we introduce OjaKV, a novel framework that integrates a strategic hybrid storage policy with online subspace adaptation. First, OjaKV recognizes that not all tokens are equally important for compression; it preserves the crucial first and most recent tokens in full-rank, maintaining high-fidelity anchors for attention. Second, for the vast majority of intermediate tokens, it applies low-rank compression by incrementally adapting the projection basis using Oja's algorithm for online principal component analysis. This adaptation involves a comprehensive update during prompt prefilling and lightweight periodic updates during decoding, ensuring the subspace remains aligned with the evolving context. Crucially, our framework is fully compatible with modern attention modules like FlashAttention. Experiments demonstrate that OjaKV maintains or even improves zero-shot accuracy at high compression ratios. In particular, OjaKV achieves its strongest gains on very long-context benchmarks that require complex reasoning, highlighting the importance of online subspace adaptation in dynamically tracking context shifts. These results establish our hybrid framework as a practical, plug-and-play solution for memory-efficient long-context inference without requiring model fine-tuning.

