---
layout: default
title: UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG
---

# UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03663" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03663v2</a>
  <a href="https://arxiv.org/pdf/2510.03663.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03663v2" onclick="toggleFavorite(this, '2510.03663v2', 'UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiangyu Peng, Can Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, Chien-Sheng Wu

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-04 (æ›´æ–°: 2025-10-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniDoc-Benchï¼Œç”¨äºè¯„ä¼°æ–‡æ¡£å‹å¤šæ¨¡æ€RAGç³»ç»Ÿçš„ç»Ÿä¸€åŸºå‡†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€RAG` `æ–‡æ¡£ç†è§£` `åŸºå‡†æµ‹è¯•` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MM-RAGè¯„ä¼°ç¼ºä¹ç»Ÿä¸€æ€§ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°æ–‡æ¡£å‹å¤šæ¨¡æ€åœºæ™¯ï¼Œé˜»ç¢äº†LLMåœ¨å®é™…çŸ¥è¯†åº“ä¸­çš„åº”ç”¨ã€‚
2. UniDoc-Benchæ„å»ºå¤§è§„æ¨¡çœŸå®æ–‡æ¡£æ•°æ®é›†ï¼Œå¹¶è®¾è®¡ç»Ÿä¸€è¯„ä¼°åè®®ï¼Œæ”¯æŒå¤šç§æ¨¡æ€å’Œæ£€ç´¢èŒƒå¼çš„å…¬å¹³æ¯”è¾ƒã€‚
3. å®éªŒè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ–‡æœ¬-å›¾åƒèåˆRAGä¼˜äºå•æ¨¡æ€å’Œè”åˆå¤šæ¨¡æ€æ£€ç´¢ï¼Œæ­ç¤ºäº†ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMM-RAGï¼‰æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ™ºèƒ½ä½“åº”ç”¨äºç°å®ä¸–ç•ŒçŸ¥è¯†åº“çš„å…³é”®æ–¹æ³•ã€‚ç„¶è€Œï¼Œç›®å‰çš„è¯„ä¼°æ˜¯åˆ†æ•£çš„ï¼Œè¦ä¹ˆå­¤ç«‹åœ°å…³æ³¨æ–‡æœ¬æˆ–å›¾åƒï¼Œè¦ä¹ˆå…³æ³¨ç®€åŒ–çš„å¤šæ¨¡æ€è®¾ç½®ï¼Œæ— æ³•æ•æ‰ä»¥æ–‡æ¡£ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€ç”¨ä¾‹ã€‚æœ¬æ–‡æå‡ºäº†UniDoc-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡ã€çœŸå®çš„MM-RAGåŸºå‡†ï¼Œç”±æ¥è‡ªå…«ä¸ªé¢†åŸŸçš„7ä¸‡ä¸ªçœŸå®PDFé¡µé¢æ„å»ºè€Œæˆã€‚æˆ‘ä»¬çš„æµç¨‹æå–å¹¶é“¾æ¥æ¥è‡ªæ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾è¡¨çš„è¯æ®ï¼Œç„¶åç”Ÿæˆ1600ä¸ªå¤šæ¨¡æ€é—®ç­”å¯¹ï¼Œæ¶µç›–äº‹å®æ£€ç´¢ã€æ¯”è¾ƒã€æ€»ç»“å’Œé€»è¾‘æ¨ç†æŸ¥è¯¢ã€‚ä¸ºäº†ç¡®ä¿å¯é æ€§ï¼Œ20%çš„é—®ç­”å¯¹ç»è¿‡å¤šä½æ ‡æ³¨è€…å’Œä¸“å®¶ä»²è£çš„éªŒè¯ã€‚UniDoc-Benchæ”¯æŒå››ç§èŒƒå¼çš„åŒç±»æ¯”è¾ƒï¼šï¼ˆ1ï¼‰çº¯æ–‡æœ¬ï¼Œï¼ˆ2ï¼‰çº¯å›¾åƒï¼Œï¼ˆ3ï¼‰å¤šæ¨¡æ€æ–‡æœ¬-å›¾åƒèåˆï¼Œä»¥åŠï¼ˆ4ï¼‰å¤šæ¨¡æ€è”åˆæ£€ç´¢â€”â€”åœ¨ç»Ÿä¸€çš„åè®®ä¸‹ï¼Œå…·æœ‰æ ‡å‡†åŒ–çš„å€™é€‰æ± ã€æç¤ºå’Œè¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ–‡æœ¬-å›¾åƒèåˆRAGç³»ç»Ÿå§‹ç»ˆä¼˜äºå•æ¨¡æ€å’Œè”åˆå¤šæ¨¡æ€åŸºäºåµŒå…¥çš„æ£€ç´¢ï¼Œè¡¨æ˜å•ç‹¬çš„æ–‡æœ¬æˆ–å›¾åƒéƒ½ä¸è¶³å¤Ÿï¼Œå¹¶ä¸”å½“å‰çš„å¤šæ¨¡æ€åµŒå…¥ä»ç„¶ä¸è¶³ã€‚é™¤äº†åŸºå‡†æµ‹è¯•ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¿˜æ­ç¤ºäº†è§†è§‰ä¸Šä¸‹æ–‡ä½•æ—¶ä»¥åŠå¦‚ä½•è¡¥å……æ–‡æœ¬è¯æ®ï¼Œæ­ç¤ºäº†ç³»ç»Ÿæ€§çš„å¤±è´¥æ¨¡å¼ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¼ºå¤§çš„MM-RAGæµç¨‹æä¾›äº†å¯æ“ä½œçš„æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MM-RAGè¯„ä¼°ä½“ç³»å­˜åœ¨ç¢ç‰‡åŒ–é—®é¢˜ï¼Œç¼ºä¹ä¸€ä¸ªç»Ÿä¸€ã€å¤§è§„æ¨¡ã€çœŸå®çš„åŸºå‡†æ¥è¯„ä¼°æ–‡æ¡£å‹å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„RAGç³»ç»Ÿã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆåªå…³æ³¨æ–‡æœ¬æˆ–å›¾åƒï¼Œè¦ä¹ˆé‡‡ç”¨ç®€åŒ–çš„å¤šæ¨¡æ€è®¾ç½®ï¼Œæ— æ³•å……åˆ†è¯„ä¼°LLMåœ¨å¤„ç†å¤æ‚æ–‡æ¡£æ—¶çš„èƒ½åŠ›ã€‚è¿™ä½¿å¾—ä¸åŒMM-RAGç³»ç»Ÿçš„æ€§èƒ½éš¾ä»¥æ¯”è¾ƒï¼Œä¹Ÿé˜»ç¢äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniDoc-Benchçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€çœŸå®çš„æ–‡æ¡£æ•°æ®é›†ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°åè®®ï¼Œä»¥æ”¯æŒä¸åŒMM-RAGç³»ç»Ÿçš„å…¬å¹³æ¯”è¾ƒã€‚é€šè¿‡æå–å’Œé“¾æ¥æ–‡æ¡£ä¸­çš„æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾è¡¨ç­‰ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆæ¶µç›–å¤šç§æŸ¥è¯¢ç±»å‹çš„å¤šæ¨¡æ€é—®ç­”å¯¹ï¼ŒUniDoc-Benchèƒ½å¤Ÿå…¨é¢è¯„ä¼°MM-RAGç³»ç»Ÿåœ¨å¤„ç†å¤æ‚æ–‡æ¡£æ—¶çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniDoc-Benchçš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ï¼šä»å…«ä¸ªé¢†åŸŸæ”¶é›†7ä¸‡ä¸ªçœŸå®çš„PDFé¡µé¢ã€‚2) ä¿¡æ¯æå–ï¼šä»PDFé¡µé¢ä¸­æå–æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾è¡¨ç­‰ä¿¡æ¯ï¼Œå¹¶å»ºç«‹å®ƒä»¬ä¹‹é—´çš„é“¾æ¥ã€‚3) é—®ç­”å¯¹ç”Ÿæˆï¼šåŸºäºæå–çš„ä¿¡æ¯ï¼Œç”Ÿæˆ1600ä¸ªå¤šæ¨¡æ€é—®ç­”å¯¹ï¼Œæ¶µç›–äº‹å®æ£€ç´¢ã€æ¯”è¾ƒã€æ€»ç»“å’Œé€»è¾‘æ¨ç†ç­‰æŸ¥è¯¢ç±»å‹ã€‚4) è´¨é‡æ§åˆ¶ï¼š20%çš„é—®ç­”å¯¹ç»è¿‡å¤šä½æ ‡æ³¨è€…å’Œä¸“å®¶ä»²è£çš„éªŒè¯ï¼Œä»¥ç¡®ä¿æ•°æ®çš„è´¨é‡ã€‚5) è¯„ä¼°åè®®ï¼šè®¾è®¡ç»Ÿä¸€çš„è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬æ ‡å‡†åŒ–çš„å€™é€‰æ± ã€æç¤ºå’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æ”¯æŒä¸åŒMM-RAGç³»ç»Ÿçš„å…¬å¹³æ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šUniDoc-Benchçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å®ƒæ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡ã€çœŸå®çš„æ–‡æ¡£å‹å¤šæ¨¡æ€RAGåŸºå‡†ã€‚2) å®ƒæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°åè®®ï¼Œæ”¯æŒå¤šç§æ¨¡æ€å’Œæ£€ç´¢èŒƒå¼çš„å…¬å¹³æ¯”è¾ƒã€‚3) å®ƒæ­ç¤ºäº†ç°æœ‰MM-RAGç³»ç»Ÿåœ¨å¤„ç†å¤æ‚æ–‡æ¡£æ—¶çš„ä¸è¶³ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šUniDoc-Benchçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•°æ®é›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§ï¼š7ä¸‡ä¸ªPDFé¡µé¢æ¶µç›–äº†å…«ä¸ªä¸åŒçš„é¢†åŸŸï¼Œç¡®ä¿äº†æ•°æ®é›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§ã€‚2) é—®ç­”å¯¹çš„ç±»å‹ï¼š1600ä¸ªé—®ç­”å¯¹æ¶µç›–äº†å¤šç§æŸ¥è¯¢ç±»å‹ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°MM-RAGç³»ç»Ÿçš„èƒ½åŠ›ã€‚3) è¯„ä¼°æŒ‡æ ‡ï¼šé‡‡ç”¨æ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®ç‡ã€å¬å›ç‡å’ŒF1å€¼ï¼Œä»¥æ”¯æŒä¸åŒç³»ç»Ÿçš„å…¬å¹³æ¯”è¾ƒã€‚4) å€™é€‰æ± æ„å»ºï¼šæ„å»ºæ ‡å‡†åŒ–çš„å€™é€‰æ± ï¼Œç¡®ä¿æ‰€æœ‰ç³»ç»Ÿéƒ½åŸºäºç›¸åŒçš„å€™é€‰æ–‡æ¡£è¿›è¡Œæ£€ç´¢ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ–‡æœ¬-å›¾åƒèåˆRAGç³»ç»Ÿå§‹ç»ˆä¼˜äºå•æ¨¡æ€ï¼ˆçº¯æ–‡æœ¬æˆ–çº¯å›¾åƒï¼‰å’Œè”åˆå¤šæ¨¡æ€åŸºäºåµŒå…¥çš„æ£€ç´¢æ–¹æ³•ã€‚è¿™è¡¨æ˜å•ç‹¬çš„æ–‡æœ¬æˆ–å›¾åƒä¿¡æ¯ä¸è¶³ä»¥æœ‰æ•ˆè§£å†³æ–‡æ¡£å‹å¤šæ¨¡æ€RAGé—®é¢˜ï¼Œå¹¶ä¸”å½“å‰çš„å¤šæ¨¡æ€åµŒå…¥æ–¹æ³•ä»æœ‰å¾…æ”¹è¿›ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniDoc-Benchå¯ç”¨äºè¯„ä¼°å’Œæ”¹è¿›å„ç§æ–‡æ¡£å‹å¤šæ¨¡æ€RAGç³»ç»Ÿï¼Œä¾‹å¦‚æ™ºèƒ½æ–‡æ¡£åŠ©æ‰‹ã€çŸ¥è¯†å›¾è°±æ„å»ºã€æ™ºèƒ½å®¢æœç­‰ã€‚è¯¥åŸºå‡†èƒ½å¤Ÿæ¨åŠ¨LLMåœ¨å¤„ç†å¤æ‚æ–‡æ¡£æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶ä¿ƒè¿›å…¶åœ¨é‡‘èã€æ³•å¾‹ã€åŒ»ç–—ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚æœªæ¥ï¼Œå¯ä»¥æ‰©å±•UniDoc-Benchçš„æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æ›´å¥½åœ°åæ˜ å®é™…åº”ç”¨åœºæ™¯çš„éœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying large language models (LLMs) and agents to real-world knowledge bases, yet current evaluations are fragmented, focusing on either text or images in isolation or on simplified multimodal setups that fail to capture document-centric multimodal use cases. In this paper, we introduce UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from 70k real-world PDF pages across eight domains. Our pipeline extracts and links evidence from text, tables, and figures, then generates 1,600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries. To ensure reliability, 20% of QA pairs are validated by multiple annotators and expert adjudication. UniDoc-Bench supports apples-to-apples comparison across four paradigms: (1) text-only, (2) image-only, (3) multimodal text-image fusion, and (4) multimodal joint retrieval -- under a unified protocol with standardized candidate pools, prompts, and evaluation metrics. Our experiments show that multimodal text-image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding-based retrieval, indicating that neither text nor images alone are sufficient and that current multimodal embeddings remain inadequate. Beyond benchmarking, our analysis reveals when and how visual context complements textual evidence, uncovers systematic failure modes, and offers actionable guidance for developing more robust MM-RAG pipelines.

