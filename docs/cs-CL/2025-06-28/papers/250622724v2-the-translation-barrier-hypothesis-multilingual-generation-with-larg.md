---
layout: default
title: The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure
---

# The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22724" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22724v2</a>
  <a href="https://arxiv.org/pdf/2506.22724.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22724v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22724v2', 'The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Niyati Bafna, Tianjian Li, Kenton Murray, David R. Mortensen, David Yarowsky, Hale Sirin, Daniel Khashabi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-28 (æ›´æ–°: 2025-10-20)

**å¤‡æ³¨**: 28 pages, incl. appendix

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¿»è¯‘éšœç¢å‡è¯´ä»¥è§£å†³å¤šè¯­è¨€ç”Ÿæˆè´¨é‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€ç”Ÿæˆ` `ç¿»è¯‘éšœç¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä½èµ„æºè¯­è¨€` `ä»»åŠ¡è§£å†³` `éšå«å¤±è´¥` `è¯­è¨€å¯¹æ¯”åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç”Ÿæˆä¸­å¯¹ä¸­ä½èµ„æºè¯­è¨€çš„æ”¯æŒä¸è¶³ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºç¿»è¯‘éšœç¢å‡è¯´ï¼Œè®¤ä¸ºä»»åŠ¡è§£å†³æˆåŠŸä½†ç¿»è¯‘å¤±è´¥æ˜¯å¯¼è‡´ä½è´¨é‡è¾“å‡ºçš„ä¸»è¦åŸå› ã€‚
3. é€šè¿‡å¯¹108å¯¹è¯­è¨€çš„å®éªŒï¼Œå‘ç°ç¿»è¯‘éšœç¢åœ¨å¤§å¤šæ•°è¯­è¨€å¯¹çš„é”™è¯¯ä¸­å ä¸»å¯¼åœ°ä½ï¼Œå°¤å…¶åœ¨ä½èµ„æºè¯­è¨€ä¸­æ›´ä¸ºä¸¥é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šè¯­è¨€ç”Ÿæˆåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œå°¤å…¶æ˜¯ä¸­ä½èµ„æºè¯­è¨€çš„ç”Ÿæˆè´¨é‡å¾€å¾€è¾ƒå·®ï¼Œä½†å…¶åŸå› å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡é¦–å…ˆå±•ç¤ºäº†éšå«çš„ä»»åŠ¡è§£å†³ä¸ç¿»è¯‘ç®¡é“ï¼Œæ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€æ— å…³çš„æ–¹å¼ä¸‹è§£å†³ä»»åŠ¡ï¼Œç„¶åå°†ç­”æ¡ˆæ¦‚å¿µç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€ã€‚æˆ‘ä»¬å‡è®¾ç¿»è¯‘é˜¶æ®µçš„å¤±è´¥æ˜¯å¯¼è‡´æœ€ç»ˆè¾“å‡ºè´¨é‡ä½ä¸‹çš„é‡è¦åŸå› ï¼Œå¹¶å°†å…¶å½¢å¼åŒ–ä¸ºç¿»è¯‘éšœç¢å‡è¯´ã€‚é€šè¿‡å¯¹108å¯¹è¯­è¨€çš„è¯æ±‡ç¿»è¯‘ä»»åŠ¡è¿›è¡Œé‡åŒ–åˆ†æï¼Œæˆ‘ä»¬å‘ç°ç¿»è¯‘éšœç¢åœ¨å¤§å¤šæ•°è¯­è¨€å¯¹çš„é”™è¯¯ä¸­å ä¸»å¯¼åœ°ä½ï¼Œå°¤å…¶åœ¨ä½èµ„æºç›®æ ‡è¯­è¨€ä¸­è¡¨ç°å°¤ä¸ºä¸¥é‡ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†ç«¯åˆ°ç«¯å¤šè¯­è¨€ç”Ÿæˆä¸­çš„ä¸€ä¸ªé‡è¦ç“¶é¢ˆï¼Œä¸ºæœªæ¥æå‡LLMsçš„å¤šè¯­è¨€èƒ½åŠ›æä¾›äº†å‚è€ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç”Ÿæˆä¸­ï¼Œå°¤å…¶æ˜¯ä¸­ä½èµ„æºè¯­è¨€çš„ç”Ÿæˆè´¨é‡ä½ä¸‹ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«ä»»åŠ¡è§£å†³ä¸ç¿»è¯‘ä¹‹é—´çš„å…³ç³»ï¼Œå¯¼è‡´è¾“å‡ºè´¨é‡ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºç¿»è¯‘éšœç¢å‡è¯´ï¼Œè®¤ä¸ºæ¨¡å‹åœ¨è§£å†³ä»»åŠ¡æ—¶è™½ç„¶æˆåŠŸï¼Œä½†åœ¨ç¿»è¯‘é˜¶æ®µå‡ºç°éšå«å¤±è´¥ï¼Œä»è€Œå½±å“æœ€ç»ˆè¾“å‡ºè´¨é‡ã€‚é€šè¿‡é‡åŒ–åˆ†æï¼Œæ˜ç¡®å„é˜¶æ®µå¯¹æœ€ç»ˆç»“æœçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºä»»åŠ¡è§£å†³ï¼Œæ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€æ— å…³çš„æƒ…å†µä¸‹ç”Ÿæˆç­”æ¡ˆï¼›ç¬¬äºŒé˜¶æ®µä¸ºç¿»è¯‘ï¼Œå°†ç”Ÿæˆçš„ç­”æ¡ˆæ¦‚å¿µç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ç¿»è¯‘éšœç¢å‡è¯´ï¼Œæ˜ç¡®äº†ä»»åŠ¡è§£å†³ä¸ç¿»è¯‘ä¹‹é—´çš„éšå«å…³ç³»ï¼Œå¹¶é‡åŒ–äº†å…¶å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºå…³æ³¨ç¿»è¯‘é˜¶æ®µçš„å¤±è´¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†108å¯¹è¯­è¨€è¿›è¡Œè¯æ±‡ç¿»è¯‘ä»»åŠ¡ï¼Œè®¾è®¡äº†ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥é‡åŒ–ç¿»è¯‘éšœç¢å¯¹é”™è¯¯çš„è´¡çŒ®ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒå®Œæ•´è®ºæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç¿»è¯‘éšœç¢åœ¨å¤§å¤šæ•°è¯­è¨€å¯¹çš„é”™è¯¯ä¸­å ä¸»å¯¼åœ°ä½ï¼Œå°¤å…¶åœ¨ä½èµ„æºè¯­è¨€ä¸­ï¼Œç¿»è¯‘å¤±è´¥çš„å½±å“å°¤ä¸ºæ˜¾è‘—ã€‚è¿™ä¸€å‘ç°ä¸ºæœªæ¥çš„å¤šè¯­è¨€ç”Ÿæˆç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘å’Œæ”¹è¿›ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€ç¿»è¯‘ç³»ç»Ÿã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œå¤šè¯­è¨€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡è¯†åˆ«å’Œè§£å†³ç¿»è¯‘éšœç¢ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç”Ÿæˆä¸­çš„è¡¨ç°ï¼Œä¿ƒè¿›ä¸åŒè¯­è¨€ä¹‹é—´çš„äº¤æµä¸ç†è§£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multilingual generation with large language models (LLMs) is often of poor quality for mid- to low-resource languages, but the causes for this are not well-understood. We first demonstrate the existence of an implicit task-solving-->translation pipeline for generation, whereby the model first solves the required task in a largely target-language-agnostic manner, and subsequently translates answer concepts into the intended target language. We hypothesize that the failure of the translation stage, despite task-solving success, is an important culprit for the observed low quality of final outputs, and formalize this as the translation barrier hypothesis. We quantify the extent to which either stage in the pipeline is responsible for final failure for a word translation task across 108 language pairs, and find that the translation barrier explains a dominant portion of error for a majority of language pairs, and is especially severe for low-resource target languages. Our results highlight an important bottleneck for end-to-end multilingual generation, relevant for future work seeking to improve multilinguality in LLMs.

