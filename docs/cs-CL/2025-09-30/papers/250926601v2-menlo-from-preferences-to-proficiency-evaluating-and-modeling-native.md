---
layout: default
title: MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages
---

# MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26601" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26601v2</a>
  <a href="https://arxiv.org/pdf/2509.26601.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26601v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26601v2', 'MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenxi Whitehouse, Sebastian Ruder, Tony Lin, Oksana Kurylo, Haruka Takagi, Janice Lam, NicolÃ² Busetto, Denise Diaz, Francisco GuzmÃ¡n

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-11-11)

**å¤‡æ³¨**: 10 pages, 23 tables, 17 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MENLOï¼šæå‡ºå¤šè¯­è¨€åŸç”Ÿè´¨é‡è¯„ä¼°æ¡†æ¶ï¼Œæå‡LLMåœ¨47ç§è¯­è¨€ä¸Šçš„è¡¨ç°ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€è¯„ä¼°` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±æ¨¡å‹` `åå¥½å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨å¤šç§è¯­è¨€ä¸­ç”Ÿæˆé«˜è´¨é‡ã€ç±»æ¯è¯­æ°´å¹³çš„å“åº”é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ã€‚
2. MENLOæ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿå—ä¼—è®¾è®¡ï¼Œæ„å»ºäº†å¤šè¯­è¨€è¯„ä¼°æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒLLMè¯„åˆ¤å™¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMENLOæ¡†æ¶èƒ½æœ‰æ•ˆæå‡LLMçš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œä½†ä¸äººç±»åˆ¤æ–­ä»å­˜åœ¨å·®è·ï¼Œæœªæ¥æœ‰æå‡ç©ºé—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†MENLOæ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è¯­è¨€ä¸­ç”Ÿæˆå“åº”çš„ç±»æ¯è¯­è´¨é‡ã€‚MENLOåŸºäºå—ä¼—è®¾è®¡å¯å‘çš„æœºåˆ¶ï¼Œå®ç°äº†å¯¹åŸç”Ÿè´¨é‡çš„è¯„ä¼°ã€‚åˆ©ç”¨MENLOï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«6423ä¸ªäººå·¥æ ‡æ³¨çš„æç¤º-å“åº”åå¥½å¯¹çš„æ•°æ®é›†ï¼Œè¦†ç›–47ç§è¯­è¨€çš„å››ä¸ªè´¨é‡ç»´åº¦ï¼Œå¹¶å…·æœ‰è¾ƒé«˜çš„äººå·¥æ ‡æ³¨ä¸€è‡´æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œé›¶æ ·æœ¬LLMè¯„åˆ¤å™¨åœ¨æˆå¯¹è¯„ä¼°å’Œç»“æ„åŒ–æ ‡æ³¨è§„åˆ™çš„å¸®åŠ©ä¸‹è¡¨ç°æ˜¾è‘—æå‡ï¼Œä½†ä»ä¸åŠäººç±»æ ‡æ³¨è€…ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ã€å¥–åŠ±å¡‘é€ å’Œå¤šä»»åŠ¡å­¦ä¹ ç­‰æ–¹æ³•è¿›è¡Œå¾®è°ƒï¼Œæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è¯„åˆ¤å™¨å¯ä»¥ä½œä¸ºç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹æ¥æé«˜LLMçš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œä½†ä¸äººç±»åˆ¤æ–­ä»å­˜åœ¨å·®å¼‚ã€‚ç ”ç©¶ç»“æœä¸ºå¯æ‰©å±•çš„å¤šè¯­è¨€è¯„ä¼°å’Œåå¥½å¯¹é½æä¾›äº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚ä½œè€…å‘å¸ƒäº†æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œä»¥æ”¯æŒå¤šè¯­è¨€LLMè¯„ä¼°çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è¯­è¨€ç¯å¢ƒä¸‹ï¼Œç”Ÿæˆé«˜è´¨é‡ã€ç±»æ¯è¯­æ°´å¹³å“åº”çš„è¯„ä¼°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ç»Ÿä¸€ã€å¯æ‰©å±•çš„è¯„ä¼°æ¡†æ¶ï¼Œéš¾ä»¥å‡†ç¡®è¡¡é‡LLMåœ¨ä¸åŒè¯­è¨€ä¸­çš„è¡¨ç°ï¼Œå¹¶ä¸”ä¾èµ–äººå·¥è¯„ä¼°æˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŸºäºå—ä¼—è®¾è®¡çš„è¯„ä¼°æ¡†æ¶MENLOï¼Œé€šè¿‡æ¨¡æ‹ŸçœŸå®ç”¨æˆ·åœºæ™¯ï¼Œè®©äººå·¥æ ‡æ³¨è€…å¯¹LLMç”Ÿæˆçš„å“åº”è¿›è¡Œåå¥½æ’åºï¼Œä»è€Œé‡åŒ–LLMçš„ç±»æ¯è¯­è´¨é‡ã€‚åŒæ—¶ï¼Œåˆ©ç”¨è¿™äº›äººå·¥æ ‡æ³¨æ•°æ®è®­ç»ƒLLMè¯„åˆ¤å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªåŠ¨è¯„ä¼°LLMçš„ç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMENLOæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ï¼šæ”¶é›†æ¶µç›–47ç§è¯­è¨€çš„æç¤º-å“åº”å¯¹ï¼Œå¹¶ç”±äººå·¥æ ‡æ³¨è€…æ ¹æ®å››ä¸ªè´¨é‡ç»´åº¦ï¼ˆæµç•…æ€§ã€ç›¸å…³æ€§ã€å‡†ç¡®æ€§ã€è‡ªç„¶æ€§ï¼‰è¿›è¡Œåå¥½æ’åºã€‚2) è¯„åˆ¤å™¨è®­ç»ƒï¼šåˆ©ç”¨äººå·¥æ ‡æ³¨æ•°æ®ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ã€å¥–åŠ±å¡‘é€ å’Œå¤šä»»åŠ¡å­¦ä¹ ç­‰æ–¹æ³•ï¼Œè®­ç»ƒLLMè¯„åˆ¤å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªåŠ¨è¯„ä¼°LLMçš„ç”Ÿæˆè´¨é‡ã€‚3) LLMä¼˜åŒ–ï¼šå°†è®­ç»ƒå¥½çš„LLMè¯„åˆ¤å™¨ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œç”¨äºä¼˜åŒ–LLMçš„ç”Ÿæˆç­–ç•¥ï¼Œæé«˜å…¶å¤šè¯­è¨€èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†åŸºäºå—ä¼—è®¾è®¡çš„MENLOè¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¡¡é‡LLMçš„ç±»æ¯è¯­è´¨é‡ã€‚2) æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€è¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–47ç§è¯­è¨€ï¼Œä¸ºå¤šè¯­è¨€LLMè¯„ä¼°æä¾›äº†å®è´µèµ„æºã€‚3) åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒLLMè¯„åˆ¤å™¨ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–çš„å¤šè¯­è¨€è¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯„åˆ¤å™¨è®­ç»ƒæ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶ç»“åˆå¥–åŠ±å¡‘é€ æŠ€æœ¯ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä½¿ç”¨äº†PPOç®—æ³•ï¼Œå¹¶è®¾è®¡äº†åŸºäºäººå·¥æ ‡æ³¨åå¥½æ•°æ®çš„å¥–åŠ±å‡½æ•°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å°è¯•äº†å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œå°†ä¸åŒè¯­è¨€çš„è¯„ä¼°ä»»åŠ¡è”åˆè®­ç»ƒï¼Œä»¥æé«˜è¯„åˆ¤å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé›¶æ ·æœ¬LLMè¯„åˆ¤å™¨åœ¨æˆå¯¹è¯„ä¼°å’Œç»“æ„åŒ–æ ‡æ³¨è§„åˆ™çš„å¸®åŠ©ä¸‹è¡¨ç°æ˜¾è‘—æå‡ï¼Œä½†ä»ä¸åŠäººç±»æ ‡æ³¨è€…ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ã€å¥–åŠ±å¡‘é€ å’Œå¤šä»»åŠ¡å­¦ä¹ ç­‰æ–¹æ³•è¿›è¡Œå¾®è°ƒï¼Œæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚ç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è¯„åˆ¤å™¨å¯ä»¥ä½œä¸ºç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹æ¥æé«˜LLMçš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œä½†ä¸äººç±»åˆ¤æ–­ä»å­˜åœ¨å·®å¼‚ã€‚å…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šè¯­è¨€LLMçš„å¼€å‘å’Œè¯„ä¼°ï¼Œå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°äº†è§£LLMåœ¨ä¸åŒè¯­è¨€ä¸­çš„è¡¨ç°ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ç”¨äºè¯„ä¼°æœºå™¨ç¿»è¯‘ç³»ç»Ÿçš„è´¨é‡ï¼Œä»¥åŠå…¶ä»–éœ€è¦ç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬çš„åº”ç”¨åœºæ™¯ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å‘å±•ï¼Œä¿ƒè¿›è·¨è¯­è¨€äº¤æµã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs' multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.

