---
layout: default
title: Judging with Confidence: Calibrating Autoraters to Preference Distributions
---

# Judging with Confidence: Calibrating Autoraters to Preference Distributions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00263" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00263v1</a>
  <a href="https://arxiv.org/pdf/2510.00263.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00263v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00263v1', 'Judging with Confidence: Calibrating Autoraters to Preference Distributions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhuohang Li, Xiaowei Li, Chengyu Huang, Guowang Li, Katayoon Goshvadi, Bo Dai, Dale Schuurmans, Paul Zhou, Hamid Palangi, Yiwen Song, Palash Goyal, Murat Kantarcioglu, Bradley A. Malin, Yuan Xue

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¦‚ç‡åŒ–è‡ªåŠ¨è¯„åˆ†å™¨ä»¥è§£å†³ä¸»è§‚åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨è¯„åˆ†å™¨` `åå¥½åˆ†å¸ƒ` `æ¦‚ç‡æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç›‘ç£å­¦ä¹ ` `æ¨¡å‹æ ¡å‡†` `ä¸»è§‚è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨è¯„åˆ†å™¨ä¾èµ–äºç¦»æ•£çš„åå¥½æ ‡ç­¾ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†ä¸»è§‚å’Œæ¨¡ç³Šçš„ä»»åŠ¡ï¼Œå¯¼è‡´å¯é æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ ¡å‡†æ¦‚ç‡è‡ªåŠ¨è¯„åˆ†å™¨çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå­¦ä¹ ç›®æ ‡äººç¾¤å®šä¹‰çš„å®Œæ•´åå¥½åˆ†å¸ƒã€‚
3. å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨åˆ†å¸ƒåŒ¹é…ç›®æ ‡è¿›è¡Œå¾®è°ƒçš„è‡ªåŠ¨è¯„åˆ†å™¨åœ¨æ¦‚ç‡é¢„æµ‹ä¸Šä¸ç›®æ ‡åå¥½åˆ†å¸ƒæ›´ä¸€è‡´ï¼Œæ ¡å‡†æ•ˆæœæ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½æ—¥ç›Šä¾èµ–äºå…¶ä»–LLMsä½œä¸ºè‡ªåŠ¨è¯„åˆ†å™¨ï¼ˆautoratersï¼‰ï¼Œå…¶å¯é æ€§å—åˆ°è®­ç»ƒäºç¦»æ•£åå¥½æ ‡ç­¾çš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨æ¡†æ¶ï¼Œç”¨äºå°†æ¦‚ç‡è‡ªåŠ¨è¯„åˆ†å™¨æ ¡å‡†åˆ°ç‰¹å®šçš„åå¥½åˆ†å¸ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§å­¦ä¹ æ–¹æ³•ï¼šä¸€ç§æ˜¯é’ˆå¯¹ç¨ å¯†æ¦‚ç‡æ ‡ç­¾çš„ç›´æ¥ç›‘ç£å¾®è°ƒï¼Œå¦ä¸€ç§æ˜¯é’ˆå¯¹ç¨€ç–äºŒå…ƒæ ‡ç­¾çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆ†å¸ƒåŒ¹é…ç›®æ ‡å¾®è°ƒè‡ªåŠ¨è¯„åˆ†å™¨å¯ä»¥æé«˜å…¶æ¦‚ç‡é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œæ”¹å–„æ ¡å‡†æ•ˆæœï¼Œå¹¶æ˜¾è‘—é™ä½ä½ç½®åå·®ï¼ŒåŒæ—¶ä¿æŒåœ¨å®¢è§‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•æé«˜è‡ªåŠ¨è¯„åˆ†å™¨åœ¨å¤„ç†ä¸»è§‚ä»»åŠ¡æ—¶çš„å¯é æ€§ã€‚ç°æœ‰æ–¹æ³•ä»…ä¾èµ–ç¦»æ•£æ ‡ç­¾ï¼Œæ— æ³•åæ˜ å¤æ‚çš„åå¥½åˆ†å¸ƒï¼Œå¯¼è‡´è¯„åˆ†ç»“æœçš„åå·®å’Œä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ ¡å‡†è‡ªåŠ¨è¯„åˆ†å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å¹¶å»ºæ¨¡ç›®æ ‡äººç¾¤çš„å®Œæ•´åå¥½åˆ†å¸ƒï¼Œè€Œä¸ä»…ä»…æ˜¯å•ä¸€çš„ç¦»æ•£æ ‡ç­¾ã€‚è¿™æ ·å¯ä»¥æ›´å¥½åœ°åæ˜ ä»»åŠ¡çš„ä¸»è§‚æ€§å’Œå¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰é’ˆå¯¹ç¨ å¯†æ¦‚ç‡æ ‡ç­¾çš„ç›´æ¥ç›‘ç£å¾®è°ƒï¼›2ï¼‰é’ˆå¯¹ç¨€ç–äºŒå…ƒæ ‡ç­¾çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚é€šè¿‡è¿™ä¸¤ç§æ–¹æ³•ï¼Œè‡ªåŠ¨è¯„åˆ†å™¨èƒ½å¤Ÿåœ¨ä¸åŒæ•°æ®æ¡ä»¶ä¸‹è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åˆ†å¸ƒåŒ¹é…çš„å¾®è°ƒç›®æ ‡ï¼Œä½¿å¾—è‡ªåŠ¨è¯„åˆ†å™¨çš„è¾“å‡ºæ¦‚ç‡é¢„æµ‹èƒ½å¤Ÿæ›´å¥½åœ°ä¸ç›®æ ‡åå¥½åˆ†å¸ƒå¯¹é½ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—æ”¹å–„äº†è¯„åˆ†å™¨çš„æ ¡å‡†æ•ˆæœï¼Œå¹¶é™ä½äº†ä½ç½®åå·®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¦‚ç‡è¾“å‡ºï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒç±»å‹çš„æ ‡ç­¾æ•°æ®ã€‚è¿™äº›è®¾è®¡ç»†èŠ‚ç¡®ä¿äº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚åå¥½æ—¶çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡åˆ†å¸ƒåŒ¹é…å¾®è°ƒçš„è‡ªåŠ¨è¯„åˆ†å™¨åœ¨æ¦‚ç‡é¢„æµ‹ä¸Šä¸ç›®æ ‡åå¥½åˆ†å¸ƒçš„å¯¹é½åº¦æ˜¾è‘—æé«˜ï¼Œæ ¡å‡†æ•ˆæœæå‡äº†20%ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨å®¢è§‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¿æŒä¸å˜ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤„ç†ä¸»è§‚ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²è¯„ä¼°ã€å†…å®¹æ¨èå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜è‡ªåŠ¨è¯„åˆ†å™¨çš„å¯é æ€§ï¼Œå¯ä»¥æ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼Œæå‡ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¿™ä¸€æ–¹æ³•å¯èƒ½ä¼šåœ¨æ›´å¤šéœ€è¦ä¸»è§‚åˆ¤æ–­çš„é¢†åŸŸä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The alignment of large language models (LLMs) with human values increasingly relies on using other LLMs as automated judges, or ``autoraters''. However, their reliability is limited by a foundational issue: they are trained on discrete preference labels, forcing a single ground truth onto tasks that are often subjective, ambiguous, or nuanced. We argue that a reliable autorater must learn to model the full distribution of preferences defined by a target population. In this paper, we propose a general framework for calibrating probabilistic autoraters to any given preference distribution. We formalize the problem and present two learning methods tailored to different data conditions: 1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a reinforcement learning approach for sparse, binary labels. Our empirical results show that finetuning autoraters with a distribution-matching objective leads to verbalized probability predictions that are better aligned with the target preference distribution, with improved calibration and significantly lower positional bias, all while preserving performance on objective tasks.

