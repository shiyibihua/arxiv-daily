---
layout: default
title: Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning
---

# Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00125" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00125v1</a>
  <a href="https://arxiv.org/pdf/2510.00125.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00125v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00125v1', 'Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hong kyu Lee, Ruixuan Liu, Li Xiong

**åˆ†ç±»**: cs.CL, cs.AI, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç›´æ¥Tokenä¼˜åŒ–ï¼ˆDTOï¼‰æ–¹æ³•ï¼Œå®ç°å¤§è¯­è¨€æ¨¡å‹è‡ªåŒ…å«å¼é—å¿˜å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é—å¿˜å­¦ä¹ ` `éšç§ä¿æŠ¤` `å†…å®¹å®¡æ ¸` `è‡ªåŒ…å«` `Tokenä¼˜åŒ–` `æ¨¡å‹æ ¡æ­£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹é—å¿˜å­¦ä¹ æ–¹æ³•ä¾èµ–å¤–éƒ¨èµ„æºï¼Œå¦‚è¾…åŠ©æ¨¡å‹æˆ–æ•°æ®é›†ï¼Œå­˜åœ¨éšç§é£é™©ä¸”ä¸å®ç”¨ã€‚
2. DTOæ–¹æ³•é€šè¿‡ç›´æ¥ä¼˜åŒ–tokençº§åˆ«ç›®æ ‡ï¼ŒåŒºåˆ†ç›®æ ‡tokenå’Œéç›®æ ‡tokenï¼Œå®ç°é«˜æ•ˆçš„è‡ªåŒ…å«å¼é—å¿˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDTOåœ¨é—å¿˜è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ€é«˜æå‡16.8å€ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹æ•ˆç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŒ…å«å¼é—å¿˜å­¦ä¹ æ–¹æ³•â€”â€”ç›´æ¥Tokenä¼˜åŒ–ï¼ˆDTOï¼‰ã€‚é—å¿˜å­¦ä¹ æ˜¯ä¸€ç§æ–°å…´æŠ€æœ¯ï¼Œæ—¨åœ¨ä»æ¨¡å‹ä¸­ç§»é™¤éƒ¨åˆ†è®­ç»ƒæ•°æ®ï¼ˆé—å¿˜é›†ï¼‰çš„å½±å“ï¼Œè€Œæ— éœ€å®Œå…¨é‡æ–°è®­ç»ƒï¼Œå…¶åº”ç”¨åŒ…æ‹¬éšç§ä¿æŠ¤ã€å†…å®¹å®¡æ ¸å’Œæ¨¡å‹æ ¡æ­£ã€‚å…³é”®æŒ‘æˆ˜åœ¨äºç¡®ä¿æ¨¡å‹å®Œå…¨å¿˜è®°é—å¿˜é›†çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¸æŸå®³å…¶æ•´ä½“æ•ˆç”¨ã€‚ç°æœ‰LLMé—å¿˜å­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–è¾…åŠ©è¯­è¨€æ¨¡å‹ã€ä¿ç•™æ•°æ®é›†ï¼Œç”šè‡³å•†ä¸šAIæœåŠ¡ï¼Œä½†è¿™å¹¶ä¸å®ç”¨ï¼Œå¹¶å¯èƒ½å¼•å…¥é¢å¤–çš„éšç§é£é™©ã€‚DTOç›´æ¥ä¼˜åŒ–tokençº§åˆ«çš„ç›®æ ‡ï¼Œæ— éœ€å¤–éƒ¨èµ„æºã€‚å¯¹äºéœ€è¦é—å¿˜çš„åºåˆ—ï¼Œæˆ‘ä»¬è¯†åˆ«ä¸¤ç§tokenï¼šç›®æ ‡tokenï¼ˆæ•è·é—å¿˜çš„å…³é”®çŸ¥è¯†ï¼‰å’Œéç›®æ ‡tokenï¼ˆç»´æŠ¤æ¨¡å‹æ•ˆç”¨ï¼‰ã€‚å‰è€…ç”¨äºä¼˜åŒ–é—å¿˜ç›®æ ‡ï¼Œåè€…ç”¨äºä¿æŒæ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒDTOåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾16.8å€çš„é—å¿˜è´¨é‡æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„æ¨¡å‹æ•ˆç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é—å¿˜å­¦ä¹ é—®é¢˜ï¼Œå³å¦‚ä½•ä»æ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šè®­ç»ƒæ•°æ®ï¼ˆé—å¿˜é›†ï¼‰çš„å½±å“ï¼Œè€Œæ— éœ€å®Œå…¨é‡æ–°è®­ç»ƒã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºä¾èµ–å¤–éƒ¨èµ„æºï¼ˆå¦‚è¾…åŠ©è¯­è¨€æ¨¡å‹ã€ä¿ç•™æ•°æ®é›†æˆ–å•†ä¸šAIæœåŠ¡ï¼‰ï¼Œè¿™ä¸ä»…å¢åŠ äº†è®¡ç®—æˆæœ¬ï¼Œè¿˜å¯èƒ½å¼•å…¥é¢å¤–çš„éšç§é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥åœ¨tokençº§åˆ«è¿›è¡Œä¼˜åŒ–ï¼ŒåŒºåˆ†éœ€è¦é—å¿˜çš„å…³é”®çŸ¥è¯†ï¼ˆç›®æ ‡tokenï¼‰å’Œä¿æŒæ¨¡å‹æ•ˆç”¨çš„éç›®æ ‡tokenã€‚é€šè¿‡å·®å¼‚åŒ–å¤„ç†è¿™ä¸¤ç±»tokenï¼Œå¯ä»¥åœ¨å®ç°æœ‰æ•ˆé—å¿˜çš„åŒæ—¶ï¼Œå°½å¯èƒ½åœ°ä¿ç•™æ¨¡å‹çš„åŸæœ‰èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDTOæ–¹æ³•çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) **åºåˆ—è¯†åˆ«**ï¼šç¡®å®šéœ€è¦é—å¿˜çš„æ–‡æœ¬åºåˆ—ã€‚2) **Tokenåˆ†ç±»**ï¼šå°†åºåˆ—ä¸­çš„tokenåˆ†ä¸ºç›®æ ‡tokenå’Œéç›®æ ‡tokenã€‚ç›®æ ‡tokenæ˜¯åŒ…å«éœ€è¦é—å¿˜çŸ¥è¯†çš„tokenï¼Œéç›®æ ‡tokenæ˜¯ç”¨äºä¿æŒæ¨¡å‹é€šç”¨èƒ½åŠ›çš„tokenã€‚3) **ç›®æ ‡ä¼˜åŒ–**ï¼šé’ˆå¯¹ç›®æ ‡tokenï¼Œä¼˜åŒ–é—å¿˜ç›®æ ‡ï¼Œä¾‹å¦‚é™ä½æ¨¡å‹å¯¹è¿™äº›tokençš„é¢„æµ‹æ¦‚ç‡ã€‚4) **æ•ˆç”¨ä¿æŒ**ï¼šé’ˆå¯¹éç›®æ ‡tokenï¼Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œä¾‹å¦‚æé«˜æ¨¡å‹å¯¹è¿™äº›tokençš„é¢„æµ‹æ¦‚ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDTOæ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶è‡ªåŒ…å«æ€§ï¼Œå³æ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨èµ„æºå³å¯å®ç°é—å¿˜å­¦ä¹ ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•å½¢æˆäº†æœ¬è´¨åŒºåˆ«ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦è¾…åŠ©æ¨¡å‹æˆ–æ•°æ®é›†æ¥æŒ‡å¯¼é—å¿˜è¿‡ç¨‹ã€‚æ­¤å¤–ï¼ŒDTOæ–¹æ³•é€šè¿‡åŒºåˆ†ç›®æ ‡tokenå’Œéç›®æ ‡tokenï¼Œå®ç°äº†æ›´ç²¾ç»†åŒ–çš„é—å¿˜æ§åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šDTOæ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **ç›®æ ‡tokençš„é€‰æ‹©ç­–ç•¥**ï¼šå¦‚ä½•å‡†ç¡®è¯†åˆ«åŒ…å«éœ€è¦é—å¿˜çŸ¥è¯†çš„tokenã€‚2) **é—å¿˜æŸå¤±å‡½æ•°**ï¼šå¦‚ä½•è®¾è®¡æŸå¤±å‡½æ•°æ¥æœ‰æ•ˆé™ä½æ¨¡å‹å¯¹ç›®æ ‡tokençš„é¢„æµ‹æ¦‚ç‡ã€‚3) **æ•ˆç”¨ä¿æŒæŸå¤±å‡½æ•°**ï¼šå¦‚ä½•è®¾è®¡æŸå¤±å‡½æ•°æ¥ä¿æŒæ¨¡å‹å¯¹éç›®æ ‡tokençš„é¢„æµ‹èƒ½åŠ›ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å½¢å¼å’Œå‚æ•°è®¾ç½®éœ€è¦åœ¨å®éªŒä¸­è¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–ï¼Œä»¥è¾¾åˆ°æœ€ä½³çš„é—å¿˜æ•ˆæœå’Œæ¨¡å‹æ•ˆç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDTOæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒDTOçš„é—å¿˜è´¨é‡æ¯”æœ€æ–°çš„åŸºçº¿æ–¹æ³•æé«˜äº†é«˜è¾¾16.8å€ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸºçº¿æ–¹æ³•ç›¸å½“çš„æ¨¡å‹æ•ˆç”¨ã€‚è¿™äº›ç»“æœè¯æ˜äº†DTOæ–¹æ³•åœ¨å®ç°é«˜æ•ˆä¸”å®ç”¨çš„é—å¿˜å­¦ä¹ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DTOæ–¹æ³•å¯åº”ç”¨äºå¤šç§åœºæ™¯ï¼ŒåŒ…æ‹¬ï¼š1) éšç§ä¿æŠ¤ï¼šä»æ¨¡å‹ä¸­ç§»é™¤åŒ…å«ä¸ªäººéšç§çš„æ•°æ®ã€‚2) å†…å®¹å®¡æ ¸ï¼šåˆ é™¤æ¨¡å‹ä¸­ä¸å½“æˆ–æœ‰å®³çš„å†…å®¹ã€‚3) æ¨¡å‹æ ¡æ­£ï¼šä¿®æ­£æ¨¡å‹ä¸­çš„é”™è¯¯çŸ¥è¯†æˆ–åè§ã€‚è¯¥æ–¹æ³•å…·æœ‰è‡ªåŒ…å«æ€§ï¼Œæ˜“äºéƒ¨ç½²å’Œä½¿ç”¨ï¼Œæœ‰æœ›æ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å’Œå¯é æ€§æ–¹é¢çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Machine unlearning is an emerging technique that removes the influence of a subset of training data (forget set) from a model without full retraining, with applications including privacy protection, content moderation, and model correction. The key challenge lies in ensuring that the model completely forgets the knowledge of the forget set without compromising its overall utility. Existing unlearning methods for large language models (LLMs) often utilize auxiliary language models, retain datasets, or even commercial AI services for effective unlearning and maintaining the model utility. However, dependence on these external resources is often impractical and could potentially introduce additional privacy risks. In this work, we propose direct token optimization (DTO), a novel self-contained unlearning approach for LLMs that directly optimizes the token level objectives and eliminates the need for external resources. Given a sequence to unlearn, we identify two categories of tokens: target tokens, which capture critical knowledge for unlearning, and the remaining non-target tokens, which are crucial for maintaining the model utility. The former are used to optimize the unlearning objective, while the latter serve to preserve the model's performance. The experimental results show that the proposed DTO achieves up to 16.8$\times$ improvement in forget quality on several benchmark datasets than the latest baselines while maintaining a comparable level of model utility.

