---
layout: default
title: Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation
---

# Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.05125" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.05125v1</a>
  <a href="https://arxiv.org/pdf/2510.05125.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.05125v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.05125v1', 'Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Reza Shirkavand, Xiaokai Wei, Chen Wang, Zheng Hui, Heng Huang, Michelle Gong

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIDIOMoEï¼Œé€šè¿‡Item-IDæ–¹è¨€å’ŒMoEç»“æ„ï¼Œå¢å¼ºLLMåœ¨æ¨èç³»ç»Ÿä¸­çš„ååŒè¿‡æ»¤èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨èç³»ç»Ÿ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ååŒè¿‡æ»¤` `æ··åˆä¸“å®¶æ¨¡å‹` `Item-ID` `è‡ªç„¶è¯­è¨€ç†è§£` `ä¸ªæ€§åŒ–æ¨è`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨èç³»ç»Ÿéš¾ä»¥åŒæ—¶å…¼é¡¾ååŒè¿‡æ»¤çš„æ•ˆç‡å’ŒLLMçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œæ— æ³•æ»¡è¶³ç”¨æˆ·å¯¹è‡ªç„¶è¯­è¨€äº¤äº’çš„éœ€æ±‚ã€‚
2. IDIOMoEå°†itemäº¤äº’å†å²è§†ä¸ºä¸€ç§â€œItem-IDæ–¹è¨€â€ï¼Œé€šè¿‡MoEç»“æ„ä½¿LLMèƒ½å¤ŸåŒæ—¶ç†è§£æ–‡æœ¬å’ŒååŒä¿¡å·ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒIDIOMoEåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜ç§€çš„æ¨èæ€§èƒ½ï¼Œå¹¶ä¿ç•™äº†é¢„è®­ç»ƒLLMçš„æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ååŒè¿‡æ»¤åœ¨æ¨èç³»ç»Ÿä¸­å…·æœ‰é¢„æµ‹å‡†ç¡®æ€§å’Œæ•ˆç‡ä¼˜åŠ¿ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹(LLM)åˆ™å…·å¤‡è¡¨è¾¾æ€§å’Œæ³›åŒ–æ¨ç†èƒ½åŠ›ã€‚ç°ä»£æ¨èç³»ç»Ÿéœ€è¦ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ã€‚ç„¶è€Œï¼Œç”¨æˆ·å¯¹è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œé€æ˜è§£é‡Šç­‰æ—¥ç›Šå¢é•¿çš„æœŸæœ›ï¼Œè¿›ä¸€æ­¥çªæ˜¾äº†ç»Ÿä¸€æ–¹æ³•çš„éœ€æ±‚ã€‚å®ç°è¿™ä¸€ç›®æ ‡å¹¶éæ˜“äº‹ã€‚ååŒä¿¡å·é€šå¸¸å…·æœ‰tokenæ•ˆç‡ï¼Œä½†åœ¨è¯­ä¹‰ä¸Šä¸é€æ˜ï¼Œè€ŒLLMåœ¨è¯­ä¹‰ä¸Šä¸°å¯Œï¼Œä½†å¦‚æœä»…åœ¨æ–‡æœ¬è¾“å…¥ä¸Šè®­ç»ƒï¼Œåˆ™éš¾ä»¥å»ºæ¨¡éšå¼ç”¨æˆ·åå¥½ã€‚æœ¬æ–‡ä»‹ç»äº†Item-ID + Oral-language Mixture-of-Experts Language Model (IDIOMoE)ï¼Œå®ƒå°†itemäº¤äº’å†å²è§†ä¸ºè¯­è¨€ç©ºé—´ä¸­çš„ä¸€ç§åŸç”Ÿæ–¹è¨€ï¼Œä½¿ååŒä¿¡å·èƒ½å¤Ÿä»¥ä¸è‡ªç„¶è¯­è¨€ç›¸åŒçš„æ–¹å¼è¢«ç†è§£ã€‚é€šè¿‡å°†é¢„è®­ç»ƒLLMçš„æ¯ä¸ªblockçš„Feed Forward Networkæ‹†åˆ†ä¸ºä¸€ä¸ªå•ç‹¬çš„æ–‡æœ¬ä¸“å®¶å’Œä¸€ä¸ªitemä¸“å®¶ï¼Œå¹¶ä½¿ç”¨token-type gatingï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¿å…äº†æ–‡æœ¬å’Œç›®å½•æ¨¡æ€ä¹‹é—´çš„ç ´åæ€§å¹²æ‰°ã€‚IDIOMoEåœ¨å…¬å…±å’Œä¸“æœ‰æ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºå¼ºå¤§çš„æ¨èæ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†é¢„è®­ç»ƒæ¨¡å‹çš„æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ¨èç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°èåˆååŒè¿‡æ»¤å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŠ¿ã€‚ååŒè¿‡æ»¤è™½ç„¶é«˜æ•ˆï¼Œä½†è¯­ä¹‰ä¿¡æ¯ä¸è¶³ï¼›LLMè™½ç„¶è¯­ä¹‰ä¸°å¯Œï¼Œä½†åœ¨å»ºæ¨¡éšå¼ç”¨æˆ·åå¥½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æ­¤å¤–ï¼Œç”¨æˆ·æœŸæœ›æ¨èç³»ç»Ÿèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€æŸ¥è¯¢å¹¶æä¾›é€æ˜çš„è§£é‡Šï¼Œè¿™è¿›ä¸€æ­¥å¢åŠ äº†èåˆçš„éš¾åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šIDIOMoEçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†itemäº¤äº’å†å²è§†ä¸ºä¸€ç§â€œItem-IDæ–¹è¨€â€ï¼Œä½¿LLMèƒ½å¤Ÿåƒç†è§£è‡ªç„¶è¯­è¨€ä¸€æ ·ç†è§£ååŒä¿¡å·ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒååŒè¿‡æ»¤çš„æ•ˆç‡å’ŒLLMçš„è¯­ä¹‰ç†è§£èƒ½åŠ›å¯ä»¥æ›´å¥½åœ°ç»“åˆã€‚åŒæ—¶ï¼Œä¸ºäº†é¿å…æ–‡æœ¬å’Œitemä¿¡æ¯ä¹‹é—´çš„å¹²æ‰°ï¼Œé‡‡ç”¨äº†MoEç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIDIOMoEåŸºäºé¢„è®­ç»ƒçš„LLMæ„å»ºã€‚å…¶ä¸»è¦æ¶æ„åŒ…æ‹¬ï¼š1) å°†LLMçš„æ¯ä¸ªblockä¸­çš„Feed Forward Network (FFN) åˆ†è£‚ä¸ºä¸¤ä¸ªä¸“å®¶ï¼šä¸€ä¸ªæ–‡æœ¬ä¸“å®¶å’Œä¸€ä¸ªitemä¸“å®¶ã€‚2) ä½¿ç”¨token-type gatingæœºåˆ¶ï¼Œæ ¹æ®è¾“å…¥tokençš„ç±»å‹ï¼ˆæ–‡æœ¬æˆ–item IDï¼‰æ¥åŠ¨æ€åœ°é€‰æ‹©ä½¿ç”¨å“ªä¸ªä¸“å®¶ã€‚3) é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ–‡æœ¬å’Œitemä¿¡æ¯å¯ä»¥å¹¶è¡Œå¤„ç†ï¼Œé¿å…äº†ç›¸äº’å¹²æ‰°ã€‚

**å…³é”®åˆ›æ–°**ï¼šIDIOMoEçš„å…³é”®åˆ›æ–°åœ¨äºå°†itemäº¤äº’å†å²è§†ä¸ºä¸€ç§è¯­è¨€ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§MoEç»“æ„æ¥å¤„ç†æ–‡æœ¬å’Œitemä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—LLMèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ååŒä¿¡å·ï¼Œä»è€Œæé«˜æ¨èæ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒIDIOMoEé¿å…äº†ç›´æ¥å°†item IDåµŒå…¥åˆ°æ–‡æœ¬åºåˆ—ä¸­ï¼Œä»è€Œå‡å°‘äº†æ–‡æœ¬å’Œitemä¿¡æ¯ä¹‹é—´çš„è€¦åˆã€‚

**å…³é”®è®¾è®¡**ï¼šIDIOMoEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) Token-type gatingæœºåˆ¶ï¼šæ ¹æ®è¾“å…¥tokençš„ç±»å‹ï¼ˆæ–‡æœ¬æˆ–item IDï¼‰æ¥åŠ¨æ€åœ°é€‰æ‹©ä½¿ç”¨å“ªä¸ªä¸“å®¶ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„é—¨æ§å‘é‡æ¥æ§åˆ¶æ–‡æœ¬ä¸“å®¶å’Œitemä¸“å®¶çš„æ¿€æ´»ç¨‹åº¦ã€‚2) æŸå¤±å‡½æ•°ï¼šIDIOMoEä½¿ç”¨æ ‡å‡†çš„è¯­è¨€æ¨¡å‹æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶å¯ä»¥æ·»åŠ é¢å¤–çš„æ¨èç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚pairwise ranking lossã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

IDIOMoEåœ¨å…¬å…±å’Œä¸“æœ‰æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ¨èæ€§èƒ½æå‡ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒIDIOMoEçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰çš„ååŒè¿‡æ»¤å’ŒLLM-basedæ¨èæ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒIDIOMoEåœ¨æé«˜æ¨èæ€§èƒ½çš„åŒæ—¶ï¼Œä¿ç•™äº†é¢„è®­ç»ƒLLMçš„æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

IDIOMoEå¯åº”ç”¨äºå„ç§æ¨èç³»ç»Ÿï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç†è§£è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œæä¾›é€æ˜è§£é‡Šçš„åœºæ™¯ä¸­ã€‚ä¾‹å¦‚ï¼Œç”µå•†å¹³å°å¯ä»¥ä½¿ç”¨IDIOMoEæ¥ç†è§£ç”¨æˆ·çš„æœç´¢æ„å›¾ï¼Œå¹¶æ ¹æ®ç”¨æˆ·çš„å†å²è¡Œä¸ºå’Œåå¥½æ¨èç›¸å…³çš„å•†å“ã€‚æ­¤å¤–ï¼ŒIDIOMoEè¿˜å¯ä»¥ç”¨äºä¸ªæ€§åŒ–æ–°é—»æ¨èã€éŸ³ä¹æ¨èç­‰é¢†åŸŸï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While collaborative filtering delivers predictive accuracy and efficiency, and Large Language Models (LLMs) enable expressive and generalizable reasoning, modern recommendation systems must bring these strengths together. Growing user expectations, such as natural-language queries and transparent explanations, further highlight the need for a unified approach. However, doing so is nontrivial. Collaborative signals are often token-efficient but semantically opaque, while LLMs are semantically rich but struggle to model implicit user preferences when trained only on textual inputs. This paper introduces Item-ID + Oral-language Mixture-of-Experts Language Model (IDIOMoE), which treats item interaction histories as a native dialect within the language space, enabling collaborative signals to be understood in the same way as natural language. By splitting the Feed Forward Network of each block of a pretrained LLM into a separate text expert and an item expert with token-type gating, our method avoids destructive interference between text and catalog modalities. IDIOMoE demonstrates strong recommendation performance across both public and proprietary datasets, while preserving the text understanding of the pretrained model.

