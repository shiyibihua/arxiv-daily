---
layout: default
title: Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization
---

# Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26520" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26520v1</a>
  <a href="https://arxiv.org/pdf/2509.26520.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26520v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26520v1', 'Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yaoxiang Wang, Qingguo Hu, Yucheng Ding, Ruizhe Wang, Yeyun Gong, Jian Jiao, Yelong Shen, Peng Cheng, Jinsong Su

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMatryoshka MoEï¼Œå®ç°MoEæ¨¡å‹åœ¨æ¨ç†æ—¶ä¸“å®¶åˆ©ç”¨çš„å¼¹æ€§è°ƒæ•´**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `å¼¹æ€§æ¨ç†` `æ¨¡å‹è®­ç»ƒ` `æ¨¡å‹å‹ç¼©` `å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MoEæ¨¡å‹é‡‡ç”¨Top-Kè·¯ç”±ï¼Œæ¨ç†æ—¶ä¸“å®¶æ•°é‡è°ƒæ•´ä¼šå¯¼è‡´æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œç¼ºä¹å¼¹æ€§ã€‚
2. M-MoEé€šè¿‡è®­ç»ƒæ—¶ä¸“å®¶æ•°é‡éšæœºåŒ–ï¼Œä½¿æ¨¡å‹å­¦ä¹ ç²—åˆ°ç»†çš„ä¸“å®¶æ’åºï¼Œå®ç°å¼¹æ€§æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒM-MoEæ¨¡å‹åœ¨ä¸åŒä¸“å®¶æ•°é‡ä¸‹æ€§èƒ½æ¥è¿‘ä¸“é—¨è®­ç»ƒçš„æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶æ¨¡å‹(MoE)å·²æˆä¸ºæœ‰æ•ˆæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸€ç§æœ‰å‰æ™¯çš„èŒƒä¾‹ï¼Œå®ƒæ— éœ€æˆæ¯”ä¾‹åœ°å¢åŠ è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼ŒTop-Kè·¯ç”±å™¨çš„æ ‡å‡†è®­ç»ƒç­–ç•¥é˜»ç¢äº†MoEæ¨¡å‹å……åˆ†å‘æŒ¥å…¶å¼¹æ€§æ¨ç†çš„æ½œåŠ›ã€‚å½“åœ¨æ¨ç†æ—¶æ”¹å˜æ¿€æ´»ä¸“å®¶çš„æ•°é‡æ—¶ï¼Œè¿™äº›æ¨¡å‹ä¼šè¡¨ç°å‡ºæ€¥å‰§çš„æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡ä»‹ç»Matryoshka MoE (M-MoE)ï¼Œä¸€ä¸ªè®­ç»ƒæ¡†æ¶ï¼Œå°†ç²—åˆ°ç»†çš„ç»“æ„ç›´æ¥çŒè¾“åˆ°ä¸“å®¶é›†æˆä¸­ã€‚é€šè¿‡åœ¨è®­ç»ƒæœŸé—´ç³»ç»Ÿåœ°æ”¹å˜æ¿€æ´»ä¸“å®¶çš„æ•°é‡ï¼ŒM-MoEè¿«ä½¿æ¨¡å‹å­¦ä¹ æœ‰æ„ä¹‰çš„æ’åºï¼šæ’åé å‰çš„ä¸“å®¶ååŒæä¾›å¿…è¦çš„ã€ç²—ç²’åº¦çš„èƒ½åŠ›ï¼Œè€Œéšåçš„ä¸“å®¶é€æ­¥æ·»åŠ æ›´ç»†ç²’åº¦çš„ç»†èŠ‚ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªç²’åº¦ä¸Šæ¢ç´¢äº†è¿™ä¸€åŸåˆ™ï¼Œç¡®å®šäº†ä¸€ç§é€å±‚éšæœºåŒ–ç­–ç•¥æ˜¯æœ€æœ‰æ•ˆçš„ã€‚å®éªŒè¡¨æ˜ï¼Œå•ä¸ªM-MoEæ¨¡å‹å®ç°äº†å“è¶Šçš„å¼¹æ€§ï¼Œå…¶åœ¨å„ç§ä¸“å®¶æ•°é‡ä¸‹çš„æ€§èƒ½ä¸æ•´ä¸ªä¸“å®¶æ¨¡å‹å¥—ä»¶çš„æ€§èƒ½éå¸¸åŒ¹é…ï¼Œä½†ä»…éœ€æ€»è®­ç»ƒæˆæœ¬çš„ä¸€å°éƒ¨åˆ†ã€‚è¿™ç§çµæ´»æ€§ä¸ä»…è§£é”äº†å¼¹æ€§æ¨ç†ï¼Œè€Œä¸”è¿˜èƒ½å¤Ÿé€šè¿‡ä¸ºä¸åŒçš„æ¨¡å‹å±‚åˆ†é…ä¸åŒçš„è®¡ç®—é¢„ç®—æ¥ä¼˜åŒ–æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤§è§„æ¨¡MoEæ¨¡å‹æ›´å®ç”¨å’Œæ›´å…·é€‚åº”æ€§çš„éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MoEæ¨¡å‹åœ¨è®­ç»ƒæ—¶é€šå¸¸é‡‡ç”¨å›ºå®šçš„Top-Kè·¯ç”±ç­–ç•¥ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦ä¾èµ–äºç‰¹å®šæ•°é‡çš„ä¸“å®¶ç»„åˆã€‚å½“åœ¨æ¨ç†é˜¶æ®µéœ€è¦è°ƒæ•´æ¿€æ´»çš„ä¸“å®¶æ•°é‡æ—¶ï¼ˆä¾‹å¦‚ï¼Œä¸ºäº†é€‚åº”ä¸åŒçš„è®¡ç®—èµ„æºé™åˆ¶ï¼‰ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼Œæ— æ³•å®ç°çœŸæ­£çš„å¼¹æ€§æ¨ç†ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºç¼ºä¹å¯¹ä¸“å®¶é‡è¦æ€§çš„æ’åºå’Œå¯¹ä¸åŒä¸“å®¶ç»„åˆçš„é€‚åº”èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šM-MoEçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥ä¸“å®¶æ•°é‡çš„éšæœºæ€§ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ ä¸€ç§ç²—åˆ°ç»†çš„ä¸“å®¶ç»“æ„ã€‚æ’åé å‰çš„ä¸“å®¶æä¾›æ ¸å¿ƒåŠŸèƒ½ï¼Œåç»­ä¸“å®¶é€æ­¥æ·»åŠ ç»†èŠ‚ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒæ•°é‡çš„æ¿€æ´»ä¸“å®¶ï¼Œå¹¶åœ¨å„ç§è®¡ç®—é¢„ç®—ä¸‹ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚è¿™ç§è®¾è®¡å€Ÿé‰´äº†ä¿„ç½—æ–¯å¥—å¨ƒï¼ˆMatryoshkaï¼‰çš„æ¦‚å¿µï¼Œæ¯ä¸€å±‚éƒ½åŒ…å«æ›´ç²¾ç»†çš„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM-MoEçš„æ•´ä½“æ¡†æ¶ä¸æ ‡å‡†çš„MoEæ¨¡å‹ç±»ä¼¼ï¼Œä¸»è¦åŒºåˆ«åœ¨äºè®­ç»ƒé˜¶æ®µã€‚åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼ŒM-MoEéšæœºé€‰æ‹©ä¸€ä¸ªæ¿€æ´»ä¸“å®¶æ•°é‡Kï¼Œç„¶åä½¿ç”¨Top-Kè·¯ç”±é€‰æ‹©Kä¸ªä¸“å®¶è¿›è¡Œè®¡ç®—ã€‚é€šè¿‡åœ¨ä¸åŒçš„å±‚ä½¿ç”¨ä¸åŒçš„Kå€¼ï¼Œå¯ä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„å¼¹æ€§ã€‚æ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†ï¼Œå¦‚è·¯ç”±ç½‘ç»œå’Œä¸“å®¶ç½‘ç»œï¼Œå¯ä»¥é‡‡ç”¨ç°æœ‰çš„MoEæ¶æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šM-MoEæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå…¶è®­ç»ƒç­–ç•¥ï¼Œå³åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºæ”¹å˜æ¿€æ´»ä¸“å®¶çš„æ•°é‡ã€‚è¿™ç§ç­–ç•¥è¿«ä½¿æ¨¡å‹å­¦ä¹ ä¸€ç§ä¸“å®¶é‡è¦æ€§çš„æ’åºï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è®¡ç®—é¢„ç®—ï¼Œå¹¶åœ¨å„ç§ä¸“å®¶æ•°é‡ä¸‹ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒM-MoEä¸éœ€è¦ä¸ºæ¯ç§ä¸“å®¶æ•°é‡è®­ç»ƒå•ç‹¬çš„æ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šM-MoEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€å±‚éšæœºåŒ–ç­–ç•¥ï¼Œå³åœ¨ä¸åŒçš„æ¨¡å‹å±‚ä½¿ç”¨ä¸åŒçš„æ¿€æ´»ä¸“å®¶æ•°é‡ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„å¼¹æ€§ï¼›2) æ¿€æ´»ä¸“å®¶æ•°é‡çš„é‡‡æ ·ç­–ç•¥ï¼Œä¾‹å¦‚å‡åŒ€é‡‡æ ·æˆ–åŸºäºæŸç§åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼›3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œéœ€è¦ç¡®ä¿æ¨¡å‹åœ¨å„ç§ä¸“å®¶æ•°é‡ä¸‹éƒ½èƒ½å­¦ä¹ åˆ°æœ‰æ•ˆçš„è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå•ä¸ªM-MoEæ¨¡å‹åœ¨ä¸åŒä¸“å®¶æ•°é‡ä¸‹çš„æ€§èƒ½ä¸ä¸“é—¨è®­ç»ƒçš„æ¨¡å‹å¥—ä»¶çš„æ€§èƒ½éå¸¸æ¥è¿‘ï¼Œä½†è®­ç»ƒæˆæœ¬ä»…ä¸ºåè€…çš„å‡ åˆ†ä¹‹ä¸€ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªå…·ä½“ä»»åŠ¡ä¸Šï¼ŒM-MoEæ¨¡å‹åœ¨æ¿€æ´»ä¸åŒæ•°é‡çš„ä¸“å®¶æ—¶ï¼Œæ€§èƒ½ä¸‹é™å¹…åº¦è¿œå°äºä¼ ç»Ÿçš„MoEæ¨¡å‹ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¾¾åˆ°ä¸ä¸“é—¨è®­ç»ƒçš„æ¨¡å‹ç›¸å½“çš„ç²¾åº¦ã€‚è¿™è¯æ˜äº†M-MoEåœ¨å®ç°å¼¹æ€§æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

M-MoEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ã€æ ¹æ®ç”¨æˆ·éœ€æ±‚åŠ¨æ€è°ƒæ•´æ¨¡å‹è®¡ç®—é‡ã€ä»¥åŠåœ¨äº‘ç«¯æä¾›å¼¹æ€§æ¨ç†æœåŠ¡ã€‚è¯¥æŠ€æœ¯å¯ä»¥æ˜¾è‘—é™ä½MoEæ¨¡å‹çš„éƒ¨ç½²æˆæœ¬ï¼Œå¹¶æé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚æœªæ¥ï¼ŒM-MoEå¯ä»¥ä¸å…¶ä»–æ¨¡å‹å‹ç¼©æŠ€æœ¯ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently scaling large language models without a proportional increase in computational cost. However, the standard training strategy of Top-K router prevents MoE models from realizing their full potential for elastic inference. When the number of activated experts is altered at inference time, these models exhibit precipitous performance degradation. In this work, we introduce Matryoshka MoE (M-MoE), a training framework that instills a coarse-to-fine structure directly into the expert ensemble. By systematically varying the number of activated experts during training, M-MoE compels the model to learn a meaningful ranking: top-ranked experts collaborate to provide essential, coarse-grained capabilities, while subsequent experts add progressively finer-grained detail. We explore this principle at multiple granularities, identifying a layer-wise randomization strategy as the most effective. Our experiments demonstrate that a single M-MoE model achieves remarkable elasticity, with its performance at various expert counts closely matching that of an entire suite of specialist models, but at only a fraction of the total training cost. This flexibility not only unlocks elastic inference but also enables optimizing performance by allocating different computational budgets to different model layers. Our work paves the way for more practical and adaptable deployments of large-scale MoE models.

