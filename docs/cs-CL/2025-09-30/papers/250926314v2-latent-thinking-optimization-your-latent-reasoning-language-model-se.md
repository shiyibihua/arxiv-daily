---
layout: default
title: Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in Its Latent Thoughts
---

# Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in Its Latent Thoughts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26314" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26314v2</a>
  <a href="https://arxiv.org/pdf/2509.26314.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26314v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26314v2', 'Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in Its Latent Thoughts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanwen Du, Yuxin Dong, Xia Ning

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLatent Thinking Optimizationï¼Œé€šè¿‡éšç©ºé—´å¥–åŠ±å»ºæ¨¡æå‡LLMæ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ½œåœ¨æ€ç»´ä¼˜åŒ–` `å¥–åŠ±å»ºæ¨¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `éšç©ºé—´è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMä¾èµ–è‡ªç„¶è¯­è¨€çš„æ€ç»´é“¾è¿›è¡Œæ¨ç†ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ï¼Œæ˜“è¿‡åº¦æ€è€ƒï¼Œä¸”ç¼ºä¹å¯¹ä¸­é—´æ¨ç†æ­¥éª¤çš„æœ‰æ•ˆç›‘ç£ã€‚
2. è®ºæ–‡æå‡ºLatent Thinking Optimization (LTO)ï¼Œåˆ©ç”¨æ½œåœ¨å¥–åŠ±æ¨¡å‹(LRM)ä¼˜åŒ–LLMçš„æ½œåœ¨æ€ç»´è¿‡ç¨‹ï¼Œæ— éœ€æ˜¾å¼è¯­è¨€ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLRMèƒ½æœ‰æ•ˆæ£€æµ‹é”™è¯¯æ¨ç†æ¨¡å¼ï¼ŒLTOæ˜¾è‘—æå‡LLMåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶å…·å¤‡è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ“…é•¿é€šè¿‡ç”Ÿæˆè‡ªç„¶è¯­è¨€çš„æ€ç»´é“¾æ¥è§£å†³é—®é¢˜ï¼Œä½†è¿™ç§å£å¤´æ€è€ƒè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å®¹æ˜“è¿‡åº¦æ€è€ƒã€‚æœ€è¿‘çš„å·¥ä½œæå‡ºäº†ä¸€ç§æ½œåœ¨æ€ç»´æ¶æ„Huginn-3.5Bï¼Œå®ƒå°†ä¸­é—´æ¨ç†æ­¥éª¤è¡¨ç¤ºä¸ºæ½œåœ¨è¡¨ç¤ºåºåˆ—ã€‚ç„¶è€Œï¼Œæ½œåœ¨æ€ç»´ç¼ºä¹å¯è§£é‡Šæ€§ä¸”éš¾ä»¥ç›‘ç£ï¼Œå¼•å‘äº†å¯¹å…¶æ½œåœ¨æ€ç»´è¿‡ç¨‹çš„æ­£ç¡®æ€§å’Œå¯é æ€§çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†Huginn-3.5Bå¦‚ä½•åœ¨æ½œåœ¨ç©ºé—´ä¸­æ€è€ƒï¼Œä»¥åŠå¤–éƒ¨ç›‘ç£ä¿¡å·å¦‚ä½•æ”¹å–„å…¶æ½œåœ¨æ€ç»´è¿‡ç¨‹ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¯¼è‡´æ­£ç¡®ç­”æ¡ˆä¸é”™è¯¯ç­”æ¡ˆçš„æ½œåœ¨æ€ç»´è¡¨ç°å‡ºé«˜åº¦å¯åŒºåˆ†çš„æ¨¡å¼ï¼Œå¹¶ä¸”æ½œåœ¨åˆ†ç±»å™¨å¯ä»¥ç›´æ¥ä»æ½œåœ¨æ€ç»´ä¸­å¯é åœ°é¢„æµ‹ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚åˆ©ç”¨è¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¦‚ç‡ç®—æ³•Latent Thinking Optimization (LTO)ï¼Œè¯¥ç®—æ³•é‡‡ç”¨æ½œåœ¨åˆ†ç±»å™¨ä½œä¸ºæ½œåœ¨å¥–åŠ±æ¨¡å‹(LRM)æ¥ä¼˜åŒ–æ½œåœ¨æ€ç»´è¿‡ç¨‹ã€‚åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLRMåœ¨æ£€æµ‹ä¸æ­£ç¡®çš„æ½œåœ¨æ€ç»´æ¨¡å¼æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œå¹¶ä¸”LTOå¯ä»¥æ˜¾ç€æ”¹å–„æ½œåœ¨æ€ç»´è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜LRMå¯ä»¥è·¨ä¸åŒé¢†åŸŸæ³›åŒ–ï¼Œå¹¶ä¸”LTOå¯ä»¥æ— ç¼åœ°åº”ç”¨äºé€šç”¨LLMä»¥æ”¹å–„å…¶æ€ç»´è¿‡ç¨‹ã€‚ä¸å£å¤´æ€è€ƒç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼Œå¥–åŠ±å»ºæ¨¡å’Œä½¿ç”¨ç›‘ç£ç¼©æ”¾æµ‹è¯•æ—¶æ€è€ƒå¯ä»¥ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œï¼Œçªå‡ºäº†å…¶ä½œä¸ºä¸€ç§é€šç”¨ã€é«˜æ•ˆä¸”é¢†åŸŸæ— å…³çš„æ–¹æ³•æ¥æ”¹å–„LLMæ€ç»´è¿‡ç¨‹çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶ï¼Œé€šå¸¸ä¾èµ–äºç”Ÿæˆè‡ªç„¶è¯­è¨€çš„æ€ç»´é“¾ã€‚è¿™ç§æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†å­˜åœ¨å‡ ä¸ªæ˜æ˜¾çš„ç—›ç‚¹ï¼šä¸€æ˜¯è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå› ä¸ºéœ€è¦å¤„ç†å’Œç”Ÿæˆå¤§é‡çš„æ–‡æœ¬ï¼›äºŒæ˜¯å®¹æ˜“å‡ºç°è¿‡åº¦æ€è€ƒï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼›ä¸‰æ˜¯ç¼ºä¹å¯¹ä¸­é—´æ¨ç†æ­¥éª¤çš„æœ‰æ•ˆç›‘ç£ï¼Œéš¾ä»¥ä¿è¯æ¨ç†è¿‡ç¨‹çš„æ­£ç¡®æ€§å’Œå¯é æ€§ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆä¸”å¯æ§åœ°æå‡LLMçš„æ¨ç†èƒ½åŠ›æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMçš„æ½œåœ¨ç©ºé—´è¿›è¡Œæ¨ç†ï¼Œå¹¶å¼•å…¥å¥–åŠ±æ¨¡å‹æ¥æŒ‡å¯¼å’Œä¼˜åŒ–æ½œåœ¨æ€ç»´è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡è®¤ä¸ºLLMåœ¨è¿›è¡Œæ¨ç†æ—¶ï¼Œå…¶å†…éƒ¨çš„æ½œåœ¨è¡¨ç¤ºå·²ç»åŒ…å«äº†ä¸°å¯Œçš„æ¨ç†ä¿¡æ¯ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªæ½œåœ¨å¥–åŠ±æ¨¡å‹(LRM)ï¼Œå¯ä»¥è¯„ä¼°è¿™äº›æ½œåœ¨è¡¨ç¤ºçš„è´¨é‡ï¼Œå¹¶åˆ©ç”¨è¿™ä¸ªå¥–åŠ±ä¿¡å·æ¥ä¼˜åŒ–LLMçš„æ½œåœ¨æ€ç»´è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ˜¾å¼çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶æé«˜äº†æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLTOçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ½œåœ¨æ€ç»´æ¨¡å‹ï¼šä½¿ç”¨å¦‚Huginn-3.5Bç­‰æ¨¡å‹ï¼Œå°†è¾“å…¥é—®é¢˜ç¼–ç ä¸ºä¸€ç³»åˆ—æ½œåœ¨è¡¨ç¤ºï¼Œä½œä¸ºæ¨ç†è¿‡ç¨‹çš„ä¸­é—´æ­¥éª¤ã€‚2) æ½œåœ¨å¥–åŠ±æ¨¡å‹(LRM)ï¼šè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œç”¨äºé¢„æµ‹æ½œåœ¨è¡¨ç¤ºåºåˆ—æ˜¯å¦ä¼šå¯¼è‡´æ­£ç¡®çš„ç­”æ¡ˆã€‚LRMçš„è¾“å…¥æ˜¯æ½œåœ¨è¡¨ç¤ºåºåˆ—ï¼Œè¾“å‡ºæ˜¯å¥–åŠ±å€¼ï¼Œè¡¨ç¤ºè¯¥åºåˆ—çš„è´¨é‡ã€‚3) ä¼˜åŒ–ç®—æ³•ï¼šä½¿ç”¨ä¸€ç§æ¦‚ç‡ç®—æ³•ï¼ˆå¦‚å¼ºåŒ–å­¦ä¹ æˆ–è¿›åŒ–ç­–ç•¥ï¼‰ï¼Œæ ¹æ®LRMçš„å¥–åŠ±ä¿¡å·ï¼Œä¼˜åŒ–æ½œåœ¨æ€ç»´æ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆæ›´é«˜è´¨é‡çš„æ½œåœ¨è¡¨ç¤ºåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¥–åŠ±å»ºæ¨¡å’Œä¼˜åŒ–ç›´æ¥åº”ç”¨äºLLMçš„æ½œåœ¨ç©ºé—´ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè‡ªç„¶è¯­è¨€çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æ›´åŠ é«˜æ•ˆï¼Œå¹¶ä¸”å¯ä»¥é¿å…è‡ªç„¶è¯­è¨€å¸¦æ¥çš„å™ªå£°å’Œæ­§ä¹‰ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜è¯æ˜äº†LRMå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥è·¨ä¸åŒçš„é¢†åŸŸå’Œä»»åŠ¡è¿›è¡Œåº”ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šLRMçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è¾“å…¥è¡¨ç¤ºï¼šå°†æ½œåœ¨è¡¨ç¤ºåºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¯ä»¥ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)æˆ–Transformerç­‰æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚2) è¾“å‡ºè¡¨ç¤ºï¼šè¾“å‡ºä¸€ä¸ªæ ‡é‡å¥–åŠ±å€¼ï¼Œè¡¨ç¤ºæ½œåœ¨è¡¨ç¤ºåºåˆ—çš„è´¨é‡ã€‚3) æŸå¤±å‡½æ•°ï¼šå¯ä»¥ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æˆ–å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°ï¼Œæ ¹æ®é¢„æµ‹çš„å¥–åŠ±å€¼ä¸å®é™…ç­”æ¡ˆçš„æ­£ç¡®æ€§è¿›è¡Œè®­ç»ƒã€‚4) ä¼˜åŒ–ç®—æ³•ï¼šå¯ä»¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOï¼‰æˆ–è¿›åŒ–ç­–ç•¥ç®—æ³•ï¼ˆå¦‚CMA-ESï¼‰æ¥ä¼˜åŒ–æ½œåœ¨æ€ç»´æ¨¡å‹ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLTOåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒLTOå¯ä»¥å°†LLMçš„å‡†ç¡®ç‡æé«˜10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¯æ˜äº†LRMå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥è·¨ä¸åŒçš„é¢†åŸŸå’Œä»»åŠ¡è¿›è¡Œåº”ç”¨ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè‡ªç„¶è¯­è¨€çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ç›¸æ¯”ï¼ŒLTOåœ¨è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½æ–¹é¢å‡å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ã€é‡‘èåˆ†æç­‰ã€‚é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ¨ç†å’Œä¼˜åŒ–ï¼Œå¯ä»¥æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜å“åº”é€Ÿåº¦ï¼Œå¹¶æå‡LLMçš„å†³ç­–è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æˆä¸ºæå‡é€šç”¨LLMæ¨ç†èƒ½åŠ›çš„ä¸€ç§é‡è¦æ‰‹æ®µï¼Œå¹¶æ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) excel at problem solving by generating chain of thoughts in natural language, but such verbal thinking is computationally costly and prone to overthinking. Recent work instead proposes a latent thinking architecture Huginn-3.5B, which represents intermediate reasoning steps as sequence of latent representations. However, latent thoughts lack interpretability and are difficult to supervise, raising concerns about the correctness and reliability of its latent thinking processes. In this paper, we provide a systematic study of how Huginn-3.5B thinks in the latent space and how external supervision signals can improve its latent thinking processes. We show that latent thoughts leading to correct versus incorrect answers exhibit highly distinguishable patterns, and that a latent classifier can reliably predict answer correctness directly from latent thoughts. Leveraging these insights, we propose Latent Thinking Optimization (LTO), a probabilistic algorithm that employs the latent classifier as a Latent Reward Model (LRM) to optimize the latent thinking processes. Extensive experiments across diverse reasoning tasks demonstrate that LRM is highly effective in detecting incorrect latent thinking patterns, and LTO can significantly improve the latent thinking processes. Furthermore, we show that LRM can generalize across diverse domains, and LTO can be seamlessly applied to general LLMs to improve their thinking processes. In contrast to verbal thinking, our method demonstrates that reward modeling and scaling test-time thinking with supervision can be performed directly in the latent space, highlighting its potential as a general, efficient, and domain-agnostic approach to improving the thinking processes of LLMs.

