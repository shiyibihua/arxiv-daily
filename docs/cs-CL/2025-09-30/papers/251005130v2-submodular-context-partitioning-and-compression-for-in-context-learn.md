---
layout: default
title: Submodular Context Partitioning and Compression for In-Context Learning
---

# Submodular Context Partitioning and Compression for In-Context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.05130" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.05130v2</a>
  <a href="https://arxiv.org/pdf/2510.05130.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.05130v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.05130v2', 'Submodular Context Partitioning and Compression for In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaoyi Zheng, Canyu Zhang, Tianyi Zhou, Shengjie Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSub-CPæ¡†æ¶ï¼Œåˆ©ç”¨å­æ¨¡ç›®æ ‡è¿›è¡Œä¸Šä¸‹æ–‡åˆ†å—å’Œå‹ç¼©ï¼Œæå‡ICLæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å­æ¨¡ä¼˜åŒ–` `ä¿¡æ¯å‹ç¼©` `Transformer` `è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformerçš„äºŒæ¬¡å¤æ‚åº¦é™åˆ¶äº†ICLä¸­exemplarçš„æ•°é‡ï¼Œç°æœ‰ä¸Šä¸‹æ–‡åˆ’åˆ†æ–¹æ³•å¿½ç•¥äº†ä¿¡æ¯å†—ä½™å’Œè¡¨ç¤ºä¸è¶³ã€‚
2. Sub-CPæ¡†æ¶åˆ©ç”¨å­æ¨¡ç›®æ ‡æ§åˆ¶å—çš„å¤šæ ·æ€§ï¼Œå®ç°å¯¹è¯­ä¹‰ç»“æ„çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œå¹¶æ”¯æŒé¢„è®¡ç®—ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSub-CPåœ¨å¤šä¸ªæ•°æ®é›†å’Œä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹å‡èƒ½ç¨³å®šæå‡ICLæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºSub-CPï¼Œä¸€ä¸ªå—æ„ŸçŸ¥çš„ä¸Šä¸‹æ–‡é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­ç”±äºTransformerçš„äºŒæ¬¡å¤æ‚åº¦å¯¼è‡´çš„exemplaræ•°é‡é™åˆ¶é—®é¢˜ã€‚ç°æœ‰ICLæ–¹æ³•é€šå¸¸å°†ä¸Šä¸‹æ–‡åˆ’åˆ†ä¸ºå—è¿›è¡Œå¤„ç†ï¼Œä½†å¿½ç•¥äº†ä¸åŒåˆ’åˆ†ç­–ç•¥å¯¼è‡´çš„ä¿¡æ¯å†—ä½™æˆ–è¡¨ç¤ºä¸è¶³ã€‚Sub-CPåˆ©ç”¨å­æ¨¡ç›®æ ‡æ¥æ§åˆ¶å—çš„å¤šæ ·æ€§ï¼Œæ”¯æŒçµæ´»çš„é€‰æ‹©ç­–ç•¥ï¼Œå…è®¸æ¯ä¸ªå—ä»å…¨å±€å¤šæ ·åˆ°å±€éƒ¨è¿è´¯å˜åŒ–ï¼Œä»è€Œå®ç°å¯¹è¯­ä¹‰ç»“æ„çš„ç»†ç²’åº¦æ§åˆ¶å¹¶æ”¯æŒé¢„è®¡ç®—ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSub-CPåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹éƒ½èƒ½æŒç»­æå‡æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³In-Context Learning (ICL) ä¸­ï¼Œç”±äºTransformeræ¶æ„çš„äºŒæ¬¡å¤æ‚åº¦ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡é•¿åº¦å—é™ï¼Œè¿›è€Œé™åˆ¶äº†å¯ä»¥ä½¿ç”¨çš„ç¤ºä¾‹æ•°é‡çš„é—®é¢˜ã€‚ç°æœ‰çš„ä¸Šä¸‹æ–‡åˆ’åˆ†æ–¹æ³•ï¼Œå¦‚é›†æˆã€å‹ç¼©å’Œäº¤å‰æ³¨æ„åŠ›ç­‰ï¼Œåœ¨åˆ’åˆ†ä¸Šä¸‹æ–‡æ—¶ï¼Œå¾€å¾€å¿½ç•¥äº†ä¸åŒåˆ’åˆ†ç­–ç•¥æ‰€å¯¼è‡´çš„ä¿¡æ¯å†—ä½™æˆ–è¡¨ç¤ºä¸è¶³ï¼Œä»è€Œå½±å“äº†ICLçš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å­æ¨¡å‡½æ•°æ¥é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„ä¸Šä¸‹æ–‡å—ï¼Œä»è€Œåœ¨å‹ç¼©ä¸Šä¸‹æ–‡çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä¿ç•™é‡è¦çš„ä¿¡æ¯ã€‚å­æ¨¡å‡½æ•°èƒ½å¤Ÿè¡¡é‡é›†åˆçš„å¤šæ ·æ€§ï¼Œå› æ­¤å¯ä»¥ç”¨äºé€‰æ‹©æ—¢åŒ…å«å…¨å±€ä¿¡æ¯ï¼Œåˆå…·æœ‰å±€éƒ¨è¿è´¯æ€§çš„ä¸Šä¸‹æ–‡å—ã€‚é€šè¿‡æ§åˆ¶å—çš„å¤šæ ·æ€§ï¼Œå¯ä»¥æ›´å¥½åœ°æ•æ‰ä¸Šä¸‹æ–‡çš„è¯­ä¹‰ç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSub-CPæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) å°†ä¸Šä¸‹æ–‡åˆ’åˆ†ä¸ºå¤šä¸ªå—ï¼›2) ä½¿ç”¨å­æ¨¡å‡½æ•°å¯¹æ¯ä¸ªå—çš„é‡è¦æ€§è¿›è¡Œè¯„åˆ†ï¼Œè¯„åˆ†æ ‡å‡†è€ƒè™‘äº†å—çš„å…¨å±€å¤šæ ·æ€§å’Œå±€éƒ¨è¿è´¯æ€§ï¼›3) é€‰æ‹©å¾—åˆ†æœ€é«˜çš„è‹¥å¹²ä¸ªå—ï¼Œç»„æˆå‹ç¼©åçš„ä¸Šä¸‹æ–‡ï¼›4) å°†å‹ç¼©åçš„ä¸Šä¸‹æ–‡è¾“å…¥åˆ°LLMä¸­è¿›è¡Œæ¨ç†ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§é€‰æ‹©ç­–ç•¥ï¼Œå…è®¸æ¯ä¸ªå—åœ¨å…¨å±€å¤šæ ·æ€§å’Œå±€éƒ¨è¿è´¯æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šSub-CPçš„å…³é”®åˆ›æ–°åœ¨äºå°†å­æ¨¡å‡½æ•°åº”ç”¨äºä¸Šä¸‹æ–‡é€‰æ‹©ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ§åˆ¶å—çš„å¤šæ ·æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSub-CPèƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡å…¨å±€ä¿¡æ¯å’Œå±€éƒ¨ä¿¡æ¯ï¼Œä»è€Œæé«˜ICLçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSub-CPæ¡†æ¶å…·æœ‰çµæ´»æ€§ï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„ä»»åŠ¡å’Œæ•°æ®é›†é€‰æ‹©ä¸åŒçš„å­æ¨¡å‡½æ•°å’Œé€‰æ‹©ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†å¤šç§å­æ¨¡å‡½æ•°ï¼Œä¾‹å¦‚Facility Locationå‡½æ•°å’ŒGraph Cutå‡½æ•°ï¼Œä»¥è¡¡é‡å—çš„å¤šæ ·æ€§å’Œè¿è´¯æ€§ã€‚é€‰æ‹©ç­–ç•¥åŒ…æ‹¬å…¨å±€å¤šæ ·æ€§ä¼˜å…ˆã€å±€éƒ¨è¿è´¯æ€§ä¼˜å…ˆä»¥åŠäºŒè€…ä¹‹é—´çš„å¹³è¡¡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®ä¸åŒçš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚è®ºæ–‡è¿˜æåˆ°Sub-CPæ”¯æŒé¢„è®¡ç®—ï¼Œå¯ä»¥æå‰è®¡ç®—å¥½æ¯ä¸ªå—çš„å¾—åˆ†ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSub-CPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒSub-CPç›¸æ¯”äºåŸºçº¿æ–¹æ³•æå‡äº†5%ä»¥ä¸Šçš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒSub-CPåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¿˜éªŒè¯äº†Sub-CPçš„é¢„è®¡ç®—èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Sub-CPæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ç³»ç»Ÿã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡å‹ç¼©ä¸Šä¸‹æ–‡ï¼ŒSub-CPå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜æ¨ç†æ•ˆç‡ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²ã€‚è¯¥ç ”ç©¶å¯¹äºæå‡ICLçš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context learning (ICL) enables efficient few-shot learning in large language models (LLMs) without training, but suffers from the quadratic input complexity of transformers, limiting the maximum number of exemplars. While various efficient ICL approaches partition the context into blocks to process (e.g., ensembling, compression, cross-attention), they often ignore the information redundancy or under-representation caused by different partition strategies, leading to suboptimal performance. To tackle this problem, we propose Sub-CP, a block-aware context selection framework that leverages submodular objectives to control block diversity. Sub-CP supports a flexible spectrum of selection strategies, allowing each block to range from globally diverse to locally coherent. This allows fine-grained control over semantic structure while enabling precomputation. Extensive experiments across diverse tasks on multiple datasets show that Sub-CP consistently improves performance across model scales.

