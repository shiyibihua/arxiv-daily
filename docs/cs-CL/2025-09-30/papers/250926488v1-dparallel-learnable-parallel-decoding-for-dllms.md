---
layout: default
title: dParallel: Learnable Parallel Decoding for dLLMs
---

# dParallel: Learnable Parallel Decoding for dLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26488" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26488v1</a>
  <a href="https://arxiv.org/pdf/2509.26488.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26488v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26488v1', 'dParallel: Learnable Parallel Decoding for dLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: Working in progress, code base: https://github.com/czg1225/dParallel

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/czg1225/dParallel)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**dParallelï¼šé¢å‘æ‰©æ•£è¯­è¨€å¤§æ¨¡å‹çš„å¹¶è¡Œå¯å­¦ä¹ è§£ç æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£è¯­è¨€æ¨¡å‹` `å¹¶è¡Œè§£ç ` `çŸ¥è¯†è’¸é¦` `ç¡®å®šæ€§å¼ºåˆ¶` `æ¨ç†åŠ é€Ÿ` `è‡ªç„¶è¯­è¨€ç”Ÿæˆ` `å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰©æ•£è¯­è¨€æ¨¡å‹(dLLMs)çš„å¹¶è¡Œè§£ç æ½œåŠ›æœªè¢«å……åˆ†æŒ–æ˜ï¼Œæ¨ç†æ•ˆç‡æå‡å—é™ï¼Œéœ€è¦å‡å°‘è§£ç æ­¥éª¤ã€‚
2. dParallelé€šè¿‡ç¡®å®šæ€§å¼ºåˆ¶è’¸é¦ï¼ŒåŠ é€Ÿmasked tokensçš„ç¡®å®šæ€§æ”¶æ•›ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„å¹¶è¡Œè§£ç ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒdParallelåœ¨GSM8Kå’ŒMBPPç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—å‡å°‘è§£ç æ­¥éª¤ï¼Œå®ç°é«˜è¾¾10.5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£è¯­è¨€å¤§æ¨¡å‹(dLLMs)ä½œä¸ºè‡ªå›å½’ç”Ÿæˆçš„ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› å…¶å¹¶è¡Œtokené¢„æµ‹å’Œæ›´ä½çš„æ¨ç†å»¶è¿Ÿè€Œå¤‡å—ç ”ç©¶ç•Œå…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å¹¶è¡Œè§£ç æ½œåŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œå› ä¸ºç°æœ‰çš„å¼€æºæ¨¡å‹ä»ç„¶éœ€è¦æ¥è¿‘tokené•¿åº¦çš„è§£ç æ­¥éª¤æ‰èƒ½ç¡®ä¿æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†dParallelï¼Œä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥é‡Šæ”¾dLLMsçš„å›ºæœ‰å¹¶è¡Œæ€§ä»¥å®ç°å¿«é€Ÿé‡‡æ ·ã€‚æˆ‘ä»¬å‘ç°å¹¶è¡Œè§£ç çš„å…³é”®ç“¶é¢ˆæ¥è‡ªäºmasked tokensçš„é¡ºåºç¡®å®šæ€§æ”¶æ•›ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒï¼šç¡®å®šæ€§å¼ºåˆ¶è’¸é¦ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è®­ç»ƒç­–ç•¥ï¼Œå®ƒè’¸é¦æ¨¡å‹ä»¥éµå¾ªå…¶åŸå§‹é‡‡æ ·è½¨è¿¹ï¼ŒåŒæ—¶å¼ºåˆ¶å®ƒæ›´å¿«ã€æ›´å¹¶è¡Œåœ°åœ¨masked tokensä¸Šå®ç°é«˜ç¡®å®šæ€§ã€‚åœ¨å„ç§åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ˜¾è‘—å‡å°‘è§£ç æ­¥éª¤çš„æ•°é‡ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚å½“åº”ç”¨äºLLaDA-8B-Instructæ¨¡å‹æ—¶ï¼ŒdParallelå°†GSM8Kä¸Šçš„è§£ç æ­¥éª¤ä»256å‡å°‘åˆ°30ï¼Œå®ç°äº†8.5å€çš„åŠ é€Ÿè€Œæ²¡æœ‰æ€§èƒ½ä¸‹é™ã€‚åœ¨MBPPåŸºå‡†ä¸Šï¼Œå®ƒå°†è§£ç æ­¥éª¤ä»256å‡å°‘åˆ°24ï¼Œä»è€Œåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶å®ç°äº†10.5å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨https://github.com/czg1225/dParallel è·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ‰©æ•£è¯­è¨€æ¨¡å‹(dLLMs)å¹¶è¡Œè§£ç æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰dLLMsè™½ç„¶ç†è®ºä¸Šæ”¯æŒå¹¶è¡Œç”Ÿæˆtokenï¼Œä½†å®é™…åº”ç”¨ä¸­ä»éœ€å¤§é‡çš„è§£ç æ­¥éª¤ï¼Œé™åˆ¶äº†æ¨ç†é€Ÿåº¦ã€‚è¿™æ˜¯å› ä¸ºmasked tokensçš„ç¡®å®šæ€§æ”¶æ•›æ˜¯é¡ºåºè¿›è¡Œçš„ï¼Œå¯¼è‡´æ— æ³•å……åˆ†åˆ©ç”¨å¹¶è¡Œè®¡ç®—çš„ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®­ç»ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¿«åœ°å¯¹masked tokensäº§ç”Ÿé«˜ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼Œä»è€Œå‡å°‘æ‰€éœ€çš„è§£ç æ­¥éª¤ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡â€œç¡®å®šæ€§å¼ºåˆ¶è’¸é¦â€æ–¹æ³•ï¼Œè®©æ¨¡å‹å­¦ä¹ åœ¨æ›´å°‘çš„æ­¥éª¤å†…è¾¾åˆ°ä¸åŸå§‹æ¨¡å‹ç›¸ä¼¼çš„é¢„æµ‹ç»“æœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šdParallelçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨åŸå§‹çš„dLLMè¿›è¡Œé‡‡æ ·ï¼Œå¾—åˆ°tokenç”Ÿæˆè½¨è¿¹ã€‚ç„¶åï¼Œä½¿ç”¨â€œç¡®å®šæ€§å¼ºåˆ¶è’¸é¦â€æ–¹æ³•è®­ç»ƒæ–°çš„æ¨¡å‹ï¼Œä½¿å…¶æ¨¡ä»¿åŸå§‹æ¨¡å‹çš„ç”Ÿæˆè½¨è¿¹ï¼Œå¹¶åœ¨æ›´å°‘çš„æ­¥éª¤å†…è¾¾åˆ°é«˜ç½®ä¿¡åº¦ã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥çœ‹ä½œæ˜¯çŸ¥è¯†è’¸é¦çš„ä¸€ç§å½¢å¼ï¼Œå…¶ä¸­åŸå§‹æ¨¡å‹æ˜¯æ•™å¸ˆæ¨¡å‹ï¼Œæ–°çš„æ¨¡å‹æ˜¯å­¦ç”Ÿæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†â€œç¡®å®šæ€§å¼ºåˆ¶è’¸é¦â€è¿™ä¸€è®­ç»ƒç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸ä»…å…³æ³¨æœ€ç»ˆçš„é¢„æµ‹ç»“æœï¼Œè¿˜å…³æ³¨ä¸­é—´çš„ç”Ÿæˆè½¨è¿¹ï¼Œä»è€Œä¿è¯äº†åœ¨å‡å°‘è§£ç æ­¥éª¤çš„åŒæ—¶ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¸ä¼šä¸‹é™ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¼ºåˆ¶æ¨¡å‹æ›´å¿«åœ°å¯¹masked tokensäº§ç”Ÿé«˜ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼Œä»è€ŒåŠ é€Ÿäº†å¹¶è¡Œè§£ç çš„è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨â€œç¡®å®šæ€§å¼ºåˆ¶è’¸é¦â€ä¸­ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ã€‚è®ºæ–‡å¯èƒ½é‡‡ç”¨äº†å¤šç§æŸå¤±å‡½æ•°çš„ç»„åˆï¼Œä¾‹å¦‚ï¼š1) æ¨¡ä»¿æŸå¤±ï¼Œç”¨äºè¡¡é‡å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ç”Ÿæˆè½¨è¿¹çš„ç›¸ä¼¼åº¦ï¼›2) ç¡®å®šæ€§æŸå¤±ï¼Œç”¨äºé¼“åŠ±å­¦ç”Ÿæ¨¡å‹æ›´å¿«åœ°å¯¹masked tokensäº§ç”Ÿé«˜ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼›3) æ ‡å‡†çš„è¯­è¨€æ¨¡å‹æŸå¤±ï¼Œç”¨äºä¿æŒæ¨¡å‹çš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚éœ€è¦åœ¨è®ºæ–‡åŸæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

dParallelåœ¨LLaDA-8B-Instructæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨GSM8KåŸºå‡†æµ‹è¯•ä¸­ï¼Œè§£ç æ­¥éª¤ä»256å‡å°‘åˆ°30ï¼Œå®ç°äº†8.5å€çš„åŠ é€Ÿï¼Œè€Œæ€§èƒ½æ²¡æœ‰ä¸‹é™ã€‚åœ¨MBPPåŸºå‡†æµ‹è¯•ä¸­ï¼Œè§£ç æ­¥éª¤ä»256å‡å°‘åˆ°24ï¼Œå®ç°äº†10.5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒdParallelèƒ½å¤Ÿæ˜¾è‘—æé«˜dLLMsçš„æ¨ç†æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

dParallelå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºåŠ é€Ÿå„ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡å‡å°‘æ¨ç†å»¶è¿Ÿï¼ŒdParallelå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶é™ä½éƒ¨ç½²æˆæœ¬ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹å»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨åœºæ™¯ä¸­ï¼Œä¾‹å¦‚åœ¨çº¿å¯¹è¯ç³»ç»Ÿå’Œå®æ—¶å†…å®¹ç”Ÿæˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel

