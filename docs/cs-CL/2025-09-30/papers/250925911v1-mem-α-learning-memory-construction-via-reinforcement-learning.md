---
layout: default
title: Mem-Î±: Learning Memory Construction via Reinforcement Learning
---

# Mem-Î±: Learning Memory Construction via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25911" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25911v1</a>
  <a href="https://arxiv.org/pdf/2509.25911.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25911v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25911v1', 'Mem-Î±: Learning Memory Construction via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian McAuley, Xiaojian Wu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMem-Î±ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒLLM Agentæœ‰æ•ˆç®¡ç†å¤æ‚è®°å¿†ç³»ç»Ÿï¼Œè§£å†³é•¿ç¨‹ä¿¡æ¯ç†è§£é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è®°å¿†å¢å¼ºAgent` `é•¿ç¨‹ä¿¡æ¯ç†è§£` `è¯­è¨€æ¨¡å‹` `è®°å¿†ç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è®°å¿†å¢å¼ºAgentä¾èµ–é¢„å®šä¹‰æŒ‡ä»¤æ›´æ–°è®°å¿†ï¼Œç¼ºä¹è‡ªä¸»åˆ¤æ–­ä¿¡æ¯é‡è¦æ€§å’Œç»„ç»‡æ–¹å¼çš„èƒ½åŠ›ï¼Œå¯¼è‡´è®°å¿†æ„å»ºæ¬¡ä¼˜ã€‚
2. Mem-Î±åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡ä¸ç¯å¢ƒäº¤äº’å’Œä¸‹æ¸¸ä»»åŠ¡åé¦ˆï¼Œè®­ç»ƒAgentè‡ªä¸»ç®¡ç†å¤æ‚è®°å¿†ç³»ç»Ÿï¼Œä¼˜åŒ–ä¿¡æ¯å­˜å‚¨å’Œæ›´æ–°ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMem-Î±åœ¨é•¿åºåˆ—ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†è¿œè¶…è®­ç»ƒé•¿åº¦çš„åºåˆ—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰Agentå—åˆ°æœ‰é™ä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶ï¼Œéœ€è¦å¤–éƒ¨è®°å¿†ç³»ç»Ÿæ¥è¿›è¡Œé•¿æœŸä¿¡æ¯ç†è§£ã€‚ç°æœ‰çš„è®°å¿†å¢å¼ºAgenté€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„æŒ‡ä»¤å’Œå·¥å…·è¿›è¡Œè®°å¿†æ›´æ–°ã€‚ç„¶è€Œï¼Œè¯­è¨€æ¨¡å‹å¯èƒ½ç¼ºä¹ç¡®å®šå­˜å‚¨å“ªäº›ä¿¡æ¯ã€å¦‚ä½•æ„å»ºä¿¡æ¯ä»¥åŠä½•æ—¶æ›´æ–°ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è®°å¿†ç³»ç»Ÿå˜å¾—æ›´åŠ å¤æ‚æ—¶ï¼Œè¿™ä¼šå¯¼è‡´æ¬¡ä¼˜çš„è®°å¿†æ„å»ºå’Œä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Mem-Î±ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡äº¤äº’å’Œåé¦ˆæ¥è®­ç»ƒAgentæœ‰æ•ˆåœ°ç®¡ç†å¤æ‚çš„è®°å¿†ç³»ç»Ÿã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªä¸“é—¨çš„è®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„å¤šè½®äº¤äº’æ¨¡å¼ï¼Œå¹¶é…æœ‰å…¨é¢çš„è¯„ä¼°é—®é¢˜ï¼Œæ—¨åœ¨æ•™æˆæœ‰æ•ˆçš„è®°å¿†ç®¡ç†ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒAgentå¤„ç†é¡ºåºä¿¡æ¯å—ï¼Œå­¦ä¹ æå–å’Œå­˜å‚¨ç›¸å…³å†…å®¹ï¼Œç„¶åæ›´æ–°è®°å¿†ç³»ç»Ÿã€‚å¥–åŠ±ä¿¡å·æ¥è‡ªä¸‹æ¸¸é—®é¢˜å›ç­”åœ¨å®Œæ•´äº¤äº’å†å²ä¸Šçš„å‡†ç¡®æ€§ï¼Œç›´æ¥ä¼˜åŒ–è®°å¿†æ„å»ºã€‚ä¸ºäº†è¯´æ˜æˆ‘ä»¬è®­ç»ƒæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒ…å«æ ¸å¿ƒã€æƒ…æ™¯å’Œè¯­ä¹‰ç»„ä»¶çš„è®°å¿†æ¶æ„ï¼Œé…å¤‡äº†å¤šä¸ªç”¨äºè®°å¿†æ“ä½œçš„å·¥å…·ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMem-Î±ç›¸å¯¹äºç°æœ‰çš„è®°å¿†å¢å¼ºAgentåŸºçº¿å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å°½ç®¡ä»…åœ¨æœ€å¤§é•¿åº¦ä¸º30k tokensçš„å®ä¾‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†æˆ‘ä»¬çš„Agentè¡¨ç°å‡ºå¯¹è¶…è¿‡400k tokensåºåˆ—çš„æ˜¾è‘—æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¿‡è®­ç»ƒé•¿åº¦çš„13å€ï¼Œçªå‡ºäº†Mem-Î±çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹Agentå—é™äºä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†é•¿ç¨‹ä¿¡æ¯ã€‚è™½ç„¶å¯ä»¥é€šè¿‡å¤–éƒ¨è®°å¿†ç³»ç»Ÿæ¥æ‰©å±•Agentçš„è®°å¿†èƒ½åŠ›ï¼Œä½†æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°ç®¡ç†è¿™äº›è®°å¿†ï¼ŒåŒ…æ‹¬é€‰æ‹©å“ªäº›ä¿¡æ¯å­˜å‚¨ã€å¦‚ä½•ç»„ç»‡ä¿¡æ¯ä»¥åŠä½•æ—¶æ›´æ–°ä¿¡æ¯ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„è§„åˆ™å’Œå·¥å…·ï¼Œç¼ºä¹è‡ªä¸»å­¦ä¹ å’Œä¼˜åŒ–è®°å¿†ç®¡ç†ç­–ç•¥çš„èƒ½åŠ›ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±å’Œæ¬¡ä¼˜æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMem-Î±çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒAgentè‡ªä¸»å­¦ä¹ è®°å¿†ç®¡ç†ç­–ç•¥ã€‚é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å’Œä¸‹æ¸¸ä»»åŠ¡çš„åé¦ˆï¼ŒAgentå¯ä»¥å­¦ä¹ åˆ°å“ªäº›ä¿¡æ¯æ˜¯é‡è¦çš„ï¼Œåº”è¯¥å¦‚ä½•å­˜å‚¨å’Œç»„ç»‡è¿™äº›ä¿¡æ¯ï¼Œä»¥åŠä½•æ—¶åº”è¯¥æ›´æ–°è®°å¿†ã€‚è¿™ç§æ–¹æ³•å…è®¸Agentæ ¹æ®å®é™…ä»»åŠ¡çš„éœ€æ±‚åŠ¨æ€åœ°è°ƒæ•´å…¶è®°å¿†ç®¡ç†ç­–ç•¥ï¼Œä»è€Œæé«˜å…¶åœ¨é•¿ç¨‹ä¿¡æ¯å¤„ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMem-Î±çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªAgentã€ä¸€ä¸ªè®°å¿†ç³»ç»Ÿå’Œä¸€ä¸ªç¯å¢ƒã€‚Agentè´Ÿè´£æ¥æ”¶ç¯å¢ƒçš„è¾“å…¥ï¼Œå¹¶æ ¹æ®å½“å‰çš„è®°å¿†çŠ¶æ€å’Œç­–ç•¥é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚åŠ¨ä½œå¯ä»¥æ˜¯å­˜å‚¨ä¿¡æ¯ã€æ£€ç´¢ä¿¡æ¯æˆ–æ›´æ–°è®°å¿†ç­‰æ“ä½œã€‚è®°å¿†ç³»ç»Ÿè´Ÿè´£å­˜å‚¨å’Œç®¡ç†Agentçš„è®°å¿†ã€‚ç¯å¢ƒè´Ÿè´£æä¾›è¾“å…¥å’Œåé¦ˆï¼Œå¹¶æ ¹æ®Agentçš„åŠ¨ä½œæ›´æ–°çŠ¶æ€ã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œï¼ŒAgentçš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸‹æ¸¸ä»»åŠ¡çš„å¥–åŠ±ï¼Œä¾‹å¦‚é—®é¢˜å›ç­”çš„å‡†ç¡®ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šMem-Î±çš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒAgentè‡ªä¸»ç®¡ç†è®°å¿†ç³»ç»Ÿã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•ç›¸æ¯”ï¼ŒMem-Î±å¯ä»¥æ ¹æ®å®é™…ä»»åŠ¡çš„éœ€æ±‚åŠ¨æ€åœ°è°ƒæ•´å…¶è®°å¿†ç®¡ç†ç­–ç•¥ï¼Œä»è€Œæé«˜å…¶åœ¨é•¿ç¨‹ä¿¡æ¯å¤„ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒMem-Î±è¿˜è®¾è®¡äº†ä¸€ä¸ªåŒ…å«æ ¸å¿ƒã€æƒ…æ™¯å’Œè¯­ä¹‰ç»„ä»¶çš„è®°å¿†æ¶æ„ï¼Œé…å¤‡äº†å¤šä¸ªç”¨äºè®°å¿†æ“ä½œçš„å·¥å…·ï¼Œè¿›ä¸€æ­¥æé«˜äº†è®°å¿†ç³»ç»Ÿçš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šMem-Î±çš„å…³é”®è®¾è®¡åŒ…æ‹¬å¥–åŠ±å‡½æ•°çš„è®¾è®¡ã€è®°å¿†æ¶æ„çš„è®¾è®¡å’ŒåŠ¨ä½œç©ºé—´çš„è®¾è®¡ã€‚å¥–åŠ±å‡½æ•°åŸºäºä¸‹æ¸¸ä»»åŠ¡çš„å‡†ç¡®ç‡ï¼Œé¼“åŠ±Agentå­¦ä¹ æœ‰æ•ˆçš„è®°å¿†ç®¡ç†ç­–ç•¥ã€‚è®°å¿†æ¶æ„åŒ…å«æ ¸å¿ƒã€æƒ…æ™¯å’Œè¯­ä¹‰ç»„ä»¶ï¼Œåˆ†åˆ«ç”¨äºå­˜å‚¨ä¸åŒç±»å‹çš„ä¿¡æ¯ã€‚åŠ¨ä½œç©ºé—´åŒ…æ‹¬å­˜å‚¨ä¿¡æ¯ã€æ£€ç´¢ä¿¡æ¯å’Œæ›´æ–°è®°å¿†ç­‰æ“ä½œï¼Œå…è®¸Agentçµæ´»åœ°ç®¡ç†è®°å¿†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Mem-Î±åœ¨å®éªŒä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¶…è¿‡äº†ç°æœ‰çš„è®°å¿†å¢å¼ºAgentåŸºçº¿ã€‚å°¤å…¶å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMem-Î±åœ¨ä»…ä½¿ç”¨æœ€å¤§é•¿åº¦ä¸º30k tokensçš„å®ä¾‹è¿›è¡Œè®­ç»ƒåï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°è¶…è¿‡400k tokensçš„åºåˆ—ï¼Œè¿™è¡¨æ˜Mem-Î±å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†é•¿ç¨‹ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Mem-Î±å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨æ™ºèƒ½å®¢æœã€å¯¹è¯ç³»ç»Ÿã€é•¿æ–‡æœ¬ç†è§£ã€çŸ¥è¯†å›¾è°±æ„å»ºç­‰é¢†åŸŸã€‚é€šè¿‡å­¦ä¹ æœ‰æ•ˆçš„è®°å¿†ç®¡ç†ç­–ç•¥ï¼ŒAgentå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨é•¿ç¨‹ä¿¡æ¯ï¼Œä»è€Œæé«˜å…¶åœ¨è¿™äº›é¢†åŸŸçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒMem-Î±è¿˜å¯ä»¥åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€æ¸¸æˆAIç­‰éœ€è¦é•¿æœŸè®°å¿†çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it, especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, we propose Mem-alpha, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. We also construct a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract and store relevant content, then update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of our training framework, we design a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens, over 13x the training length, highlighting the robustness of Mem-alpha.

