---
layout: default
title: ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations
---

# ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25868" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25868v2</a>
  <a href="https://arxiv.org/pdf/2509.25868.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25868v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25868v2', 'ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yindong Wang, Martin PreiÃŸ, Margarita BugueÃ±o, Jan Vincent Hoffbauer, Abdullatif Ghajar, Tolga Buz, Gerard de Melo

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-01)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ddz5431/ReFACT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ReFACTï¼šæå‡ºä¸€ä¸ªç§‘å­¦çŸ¥è¯†æé€ æ£€æµ‹åŸºå‡†ï¼ŒåŒ…å«ä½ç½®é”™è¯¯æ ‡æ³¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç§‘å­¦çŸ¥è¯†` `æé€ æ£€æµ‹` `åŸºå‡†æµ‹è¯•` `é”™è¯¯å®šä½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸå®¹æ˜“æé€ äº‹å®ï¼Œç¼ºä¹ç»†ç²’åº¦çš„è¯„ä¼°å’Œçº æ­£æœºåˆ¶ã€‚
2. ReFACTåŸºå‡†æä¾›ä¸“å®¶æ ‡æ³¨çš„ç§‘å­¦é—®ç­”å¯¹ï¼ŒåŒ…å«æ­£ç¡®ç­”æ¡ˆå’Œé”™è¯¯ç­”æ¡ˆï¼Œå¹¶æ ‡æ³¨é”™è¯¯ä½ç½®å’Œç±»å‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯GPT-4oç­‰å…ˆè¿›æ¨¡å‹åœ¨ReFACTä¸Šçš„è¡¨ç°ä¹Ÿæœ‰é™ï¼Œå‡¸æ˜¾äº†åŸºå‡†çš„ä»·å€¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»å¸¸æé€ ç§‘å­¦äº‹å®ï¼Œä¸¥é‡æŸå®³äº†å…¶å¯ä¿¡åº¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œéœ€è¦è¶…è¶ŠäºŒå…ƒäº‹å®æ€§åˆ¤æ–­çš„åŸºå‡†ï¼Œå¹¶å®ç°ç»†ç²’åº¦çš„è¯„ä¼°ã€‚æˆ‘ä»¬æ¨å‡ºäº†ReFACTï¼ˆReddit False And Correct Textsï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1001ä¸ªç”±ä¸“å®¶æ ‡æ³¨çš„é—®ç­”å¯¹çš„åŸºå‡†ï¼Œæ¶µç›–äº†ä¸åŒçš„ç§‘å­¦é¢†åŸŸï¼Œç”¨äºæ£€æµ‹ç§‘å­¦çŸ¥è¯†æé€ ã€‚æ¯ä¸ªå®ä¾‹éƒ½åŒ…å«ä¸€ä¸ªç§‘å­¦ä¸Šæ­£ç¡®çš„ç­”æ¡ˆå’Œä¸€ä¸ªéäº‹å®çš„å¯¹åº”ç­”æ¡ˆï¼Œå¹¶æ ‡æ³¨äº†ç²¾ç¡®çš„é”™è¯¯èŒƒå›´å’Œé”™è¯¯ç±»å‹ã€‚ReFACTæ”¯æŒå¤šé˜¶æ®µè¯„ä¼°ï¼šï¼ˆ1ï¼‰æé€ æ£€æµ‹ï¼Œï¼ˆ2ï¼‰ç»†ç²’åº¦çš„é”™è¯¯å®šä½ï¼Œä»¥åŠï¼ˆ3ï¼‰çº æ­£ã€‚æˆ‘ä»¬å¯¹9ä¸ªæœ€å…ˆè¿›çš„LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°æ€§èƒ½æœ‰é™ï¼ˆçº¦50%çš„å‡†ç¡®ç‡ï¼‰ã€‚å³ä½¿æ˜¯GPT-4oç­‰é¡¶çº§æ¨¡å‹ä¹Ÿæ— æ³•åŒºåˆ†äº‹å®æ€§å’Œæé€ çš„ç§‘å­¦ç­”æ¡ˆï¼Œè¿™å¼•å‘äº†å¯¹LLMä½œä¸ºè¯„åˆ¤æ ‡å‡†çš„å¯é æ€§çš„æ‹…å¿§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒï¼Œéœ€è¦ç»†ç²’åº¦çš„ã€ç»è¿‡äººå·¥éªŒè¯çš„åŸºå‡†æ¥æ£€æµ‹å’Œçº æ­£ç‰¹å®šé¢†åŸŸèƒŒæ™¯ä¸‹çš„ç§‘å­¦çŸ¥è¯†æé€ ã€‚è¯¥æ•°æ®é›†å¯åœ¨https://github.com/ddz5431/ReFACT è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦é¢†åŸŸä¸­æé€ äº‹å®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨äºŒå…ƒçš„äº‹å®æ€§åˆ¤æ–­ï¼ˆæ­£ç¡®æˆ–é”™è¯¯ï¼‰ï¼Œç¼ºä¹å¯¹é”™è¯¯ä½ç½®å’Œç±»å‹çš„ç»†ç²’åº¦åˆ†æï¼Œéš¾ä»¥æœ‰æ•ˆè¯„ä¼°å’Œçº æ­£LLMsçš„ç§‘å­¦çŸ¥è¯†æé€ è¡Œä¸ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªé«˜è´¨é‡çš„ã€ç»†ç²’åº¦æ ‡æ³¨çš„ç§‘å­¦çŸ¥è¯†æé€ æ£€æµ‹åŸºå‡†ReFACTã€‚é€šè¿‡æä¾›åŒ…å«é”™è¯¯ä½ç½®å’Œç±»å‹çš„æ ‡æ³¨æ•°æ®ï¼ŒReFACTèƒ½å¤Ÿæ”¯æŒå¤šé˜¶æ®µçš„è¯„ä¼°ï¼ŒåŒ…æ‹¬æé€ æ£€æµ‹ã€é”™è¯¯å®šä½å’Œçº æ­£ã€‚è¿™ç§ç»†ç²’åº¦çš„è¯„ä¼°æ–¹å¼èƒ½å¤Ÿæ›´å…¨é¢åœ°äº†è§£LLMsåœ¨ç§‘å­¦é¢†åŸŸçš„çŸ¥è¯†ç¼ºé™·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReFACTåŸºå‡†çš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ï¼šä»Redditç­‰å¹³å°æ”¶é›†ç§‘å­¦ç›¸å…³çš„é—®ç­”å¯¹ï¼›2) ä¸“å®¶æ ‡æ³¨ï¼šç”±é¢†åŸŸä¸“å®¶å¯¹æ¯ä¸ªé—®ç­”å¯¹è¿›è¡Œæ ‡æ³¨ï¼ŒåŒ…æ‹¬æä¾›ä¸€ä¸ªç§‘å­¦ä¸Šæ­£ç¡®çš„ç­”æ¡ˆå’Œä¸€ä¸ªéäº‹å®çš„å¯¹åº”ç­”æ¡ˆï¼Œå¹¶æ ‡æ³¨éäº‹å®ç­”æ¡ˆä¸­çš„é”™è¯¯èŒƒå›´å’Œé”™è¯¯ç±»å‹ï¼›3) æ•°æ®é›†å‘å¸ƒï¼šå°†æ ‡æ³¨å¥½çš„æ•°æ®æ•´ç†æˆReFACTåŸºå‡†ï¼Œå¹¶å…¬å¼€å‘å¸ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šReFACTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»†ç²’åº¦çš„é”™è¯¯æ ‡æ³¨ã€‚ä¸ä»¥å¾€çš„äºŒå…ƒäº‹å®æ€§åˆ¤æ–­åŸºå‡†ä¸åŒï¼ŒReFACTä¸ä»…æ ‡æ³¨äº†ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼Œè¿˜æ ‡æ³¨äº†é”™è¯¯ç­”æ¡ˆä¸­çš„é”™è¯¯ä½ç½®å’Œé”™è¯¯ç±»å‹ã€‚è¿™ç§ç»†ç²’åº¦çš„æ ‡æ³¨æ–¹å¼ä½¿å¾—ReFACTèƒ½å¤Ÿæ”¯æŒæ›´æ·±å…¥çš„åˆ†æå’Œè¯„ä¼°ï¼Œä¾‹å¦‚é”™è¯¯å®šä½å’Œçº æ­£ã€‚

**å…³é”®è®¾è®¡**ï¼šReFACTåŸºå‡†åŒ…å«1001ä¸ªä¸“å®¶æ ‡æ³¨çš„é—®ç­”å¯¹ï¼Œæ¶µç›–äº†ä¸åŒçš„ç§‘å­¦é¢†åŸŸã€‚æ¯ä¸ªå®ä¾‹éƒ½åŒ…å«ä¸€ä¸ªç§‘å­¦ä¸Šæ­£ç¡®çš„ç­”æ¡ˆå’Œä¸€ä¸ªéäº‹å®çš„å¯¹åº”ç­”æ¡ˆï¼Œå¹¶æ ‡æ³¨äº†ç²¾ç¡®çš„é”™è¯¯èŒƒå›´å’Œé”™è¯¯ç±»å‹ã€‚é”™è¯¯ç±»å‹åŒ…æ‹¬ä½†ä¸é™äºæ¦‚å¿µé”™è¯¯ã€æ•°å€¼é”™è¯¯å’Œå…³ç³»é”™è¯¯ã€‚è®ºæ–‡æ²¡æœ‰æ¶‰åŠç‰¹å®šçš„æ¨¡å‹ç»“æ„æˆ–æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œè€Œæ˜¯ä¾§é‡äºåŸºå‡†çš„æ„å»ºå’Œè¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å¯¹9ä¸ªæœ€å…ˆè¿›çš„LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯GPT-4oç­‰é¡¶çº§æ¨¡å‹åœ¨ReFACTä¸Šçš„å‡†ç¡®ç‡ä¹Ÿä»…ä¸º50%å·¦å³ã€‚è¿™è¡¨æ˜ï¼Œç°æœ‰LLMåœ¨ç§‘å­¦çŸ¥è¯†æé€ æ£€æµ‹æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå‡¸æ˜¾äº†ReFACTåŸºå‡†çš„ä»·å€¼å’Œæ„ä¹‰ã€‚è¯¥ç»“æœä¹Ÿå¯¹LLMä½œä¸ºè¯„åˆ¤æ ‡å‡†çš„å¯é æ€§æå‡ºäº†è´¨ç–‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ReFACTåŸºå‡†å¯ç”¨äºè¯„ä¼°å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸçš„çŸ¥è¯†å‡†ç¡®æ€§ã€‚é€šè¿‡ReFACTï¼Œç ”ç©¶äººå‘˜å¯ä»¥å¼€å‘æ›´æœ‰æ•ˆçš„ç§‘å­¦çŸ¥è¯†æé€ æ£€æµ‹å’Œçº æ­£æ–¹æ³•ï¼Œæé«˜LLMsåœ¨ç§‘å­¦ç ”ç©¶ã€æ•™è‚²å’Œä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸçš„å¯é æ€§ã€‚æœªæ¥ï¼ŒReFACTå¯ä»¥æ‰©å±•åˆ°æ›´å¤šé¢†åŸŸï¼Œå¹¶ä¸å…¶ä»–çŸ¥è¯†åº“ç›¸ç»“åˆï¼Œæ„å»ºæ›´å¼ºå¤§çš„ç§‘å­¦çŸ¥è¯†æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) frequently confabulate scientific facts, severely undermining their trustworthiness. Addressing this challenge requires benchmarks that go beyond binary factuality and enable fine-grained evaluation. We introduce ReFACT (Reddit False And Correct Texts), a benchmark of 1,001 expert-annotated question-answer pairs spanning diverse scientific domains for the detection of scientific confabulation. Each instance includes both a scientifically correct answer and a non-factual counterpart annotated with precise error spans and error types. ReFACT enables multi-stage evaluation: (1) confabulation detection, (2) fine-grained error localization, and (3) correction. We benchmark 9 state-of-the-art LLMs, revealing limited performance (about 50 percent accuracy). Even top models such as GPT-4o fail to distinguish factual from confabulated scientific answers, raising concerns about the reliability of LLM-as-judge evaluation paradigms. Our findings highlight the need for fine-grained, human-validated benchmarks to detect and correct scientific confabulation in domain-specific contexts. The dataset is available at: https://github.com/ddz5431/ReFACT

