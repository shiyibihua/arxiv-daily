---
layout: default
title: TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture
---

# TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01279" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01279v1</a>
  <a href="https://arxiv.org/pdf/2510.01279.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01279v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01279v1', 'TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongchao Chen, Jiefeng Chen, Rui Meng, Ji Yin, Na Li, Chuchu Fan, Chi Wang, Tomas Pfister, Jinsung Yoon

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 27 pages, 13 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TUMIXï¼šåŸºäºå·¥å…·ä½¿ç”¨æ··åˆçš„å¤šAgentæµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å·¥å…·ä½¿ç”¨` `å¤šAgentç³»ç»Ÿ` `é›†æˆå­¦ä¹ ` `æµ‹è¯•æ—¶æ‰©å±•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨æ–¹é¢ç¼ºä¹æœ‰æ•ˆæŒ‡å¯¼ï¼Œéš¾ä»¥æœ‰æ•ˆç»“åˆæ–‡æœ¬æ¨ç†ã€ç¼–ç å’Œæœç´¢ã€‚
2. TUMIXé€šè¿‡å¹¶è¡Œè¿è¡Œå¤šä¸ªå…·æœ‰ä¸åŒå·¥å…·ä½¿ç”¨ç­–ç•¥çš„agentï¼Œå¹¶è¿­ä»£å…±äº«å’Œæ”¹è¿›ç­”æ¡ˆæ¥è§£å†³æ­¤é—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTUMIXåœ¨æ¨ç†å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æå‰åœæ­¢æ¥é™ä½æ¨ç†æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTool-Use Mixture (TUMIX)çš„é›†æˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å·¥å…·ä½¿ç”¨æ–¹é¢çš„ä¼˜åŒ–é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç»“åˆæ–‡æœ¬æ¨ç†ã€ç¼–ç å’Œæœç´¢ä»¥åº”å¯¹å¤šæ ·åŒ–é—®é¢˜æ—¶ã€‚TUMIXå¹¶è¡Œè¿è¡Œå¤šä¸ªagentï¼Œæ¯ä¸ªagenté‡‡ç”¨ä¸åŒçš„å·¥å…·ä½¿ç”¨ç­–ç•¥å’Œç­”æ¡ˆè·¯å¾„ã€‚è¿™äº›agentåŸºäºé—®é¢˜å’Œä¹‹å‰çš„ç­”æ¡ˆè¿­ä»£åœ°å…±äº«å’Œæ”¹è¿›å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTUMIXåœ¨å…³é”®æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸å¯¹äºæœ€å…ˆè¿›çš„å·¥å…·å¢å¼ºå’Œæµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ï¼Œåœ¨Gemini-2.5-Proå’ŒGemini-2.5-Flashä¸Šå®ç°äº†å¹³å‡é«˜è¾¾3.55%çš„å‡†ç¡®ç‡æå‡ï¼Œä¸”æ¨ç†æˆæœ¬å‡ ä¹ç›¸åŒã€‚Agentçš„å¤šæ ·æ€§å’Œè´¨é‡è‡³å…³é‡è¦ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨LLMè‡ªåŠ¨ä¼˜åŒ–agentè®¾è®¡æ¥å¢å¼ºã€‚æ­¤å¤–ï¼ŒTUMIXå¯ä»¥åœ¨è¾¾åˆ°è¶³å¤Ÿçš„ç½®ä¿¡åº¦æ—¶åœæ­¢æ”¹è¿›ï¼Œä»è€Œåœ¨ä»…49%çš„æ¨ç†æˆæœ¬ä¸‹ä¿æŒæ€§èƒ½ã€‚è¿›ä¸€æ­¥æ‰©å±•å¯ä»¥å®ç°æ›´é«˜çš„æ€§èƒ½ï¼Œä½†æˆæœ¬ä¹Ÿä¼šå¢åŠ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚ChatGPT Agentå’ŒGemini-Proï¼Œé›†æˆäº†ä»£ç è§£é‡Šå™¨å’Œæœç´¢ç­‰å·¥å…·ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›å·¥å…·ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•å°†æ–‡æœ¬æ¨ç†ã€ç¼–ç å’Œæœç´¢ç»“åˆèµ·æ¥è§£å†³å„ç§é—®é¢˜ï¼Œä»ç„¶ç¼ºä¹æœ‰æ•ˆçš„æŒ‡å¯¼ã€‚ç°æœ‰çš„æ–¹æ³•åœ¨å·¥å…·ä½¿ç”¨ç­–ç•¥ä¸Šä¸å¤Ÿçµæ´»ï¼Œéš¾ä»¥é€‚åº”ä¸åŒç±»å‹çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTUMIXçš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨ä¸€ç§é›†æˆå­¦ä¹ çš„æ–¹æ³•ï¼Œå¹¶è¡Œè¿è¡Œå¤šä¸ªagentï¼Œæ¯ä¸ªagentéƒ½é‡‡ç”¨ä¸åŒçš„å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚é€šè¿‡è®©è¿™äº›agentç›¸äº’åä½œã€å…±äº«ä¿¡æ¯å’Œæ”¹è¿›ç­”æ¡ˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ¢ç´¢ä¸åŒçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æœ€ç»ˆå¾—åˆ°æ›´å‡†ç¡®ã€æ›´å¯é çš„ç­”æ¡ˆã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºé›†æ€å¹¿ç›Šï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨ä¸åŒagentçš„ä¼˜åŠ¿ï¼Œé¿å…å•ä¸€ç­–ç•¥çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTUMIXçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **Agentæ± **ï¼šåŒ…å«å¤šä¸ªagentï¼Œæ¯ä¸ªagentéƒ½é…å¤‡ä¸åŒçš„å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚2) **å¹¶è¡Œæ‰§è¡Œ**ï¼šæ‰€æœ‰agentå¹¶è¡Œæ‰§è¡Œï¼Œé’ˆå¯¹ç»™å®šçš„é—®é¢˜ç”Ÿæˆåˆæ­¥ç­”æ¡ˆã€‚3) **ä¿¡æ¯å…±äº«**ï¼šagentä¹‹é—´å…±äº«ç­”æ¡ˆå’Œæ¨ç†è¿‡ç¨‹ï¼Œä»¥ä¾¿ç›¸äº’å­¦ä¹ å’Œæ”¹è¿›ã€‚4) **è¿­ä»£æ”¹è¿›**ï¼šåŸºäºå…±äº«çš„ä¿¡æ¯ï¼Œagentè¿­ä»£åœ°æ”¹è¿›è‡ªå·±çš„ç­”æ¡ˆã€‚5) **ç½®ä¿¡åº¦è¯„ä¼°**ï¼šè¯„ä¼°æ¯ä¸ªagentç­”æ¡ˆçš„ç½®ä¿¡åº¦ï¼Œå¹¶æ ¹æ®ç½®ä¿¡åº¦å†³å®šæ˜¯å¦åœæ­¢è¿­ä»£ã€‚6) **ç­”æ¡ˆèåˆ**ï¼šå°†æ‰€æœ‰agentçš„ç­”æ¡ˆèåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„ç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šTUMIXçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šagenté›†æˆå­¦ä¹ çš„æ¡†æ¶ï¼Œä»¥åŠåŸºäºç½®ä¿¡åº¦çš„æå‰åœæ­¢æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„å•ä¸€agentæ–¹æ³•ç›¸æ¯”ï¼ŒTUMIXå¯ä»¥æ›´æœ‰æ•ˆåœ°æ¢ç´¢ä¸åŒçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æå‰åœæ­¢æœºåˆ¶å¯ä»¥åœ¨ä¿è¯æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½æ¨ç†æˆæœ¬ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMè‡ªåŠ¨ä¼˜åŒ–agentè®¾è®¡ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨TUMIXä¸­ï¼Œagentçš„å¤šæ ·æ€§è‡³å…³é‡è¦ï¼Œå¯ä»¥é€šè¿‡è®¾è®¡ä¸åŒçš„å·¥å…·ä½¿ç”¨ç­–ç•¥æ¥å®ç°ã€‚ä¾‹å¦‚ï¼Œä¸€äº›agentå¯ä»¥ä¾§é‡äºæ–‡æœ¬æ¨ç†ï¼Œå¦ä¸€äº›agentå¯ä»¥ä¾§é‡äºç¼–ç ï¼Œè¿˜æœ‰ä¸€äº›agentå¯ä»¥ä¾§é‡äºæœç´¢ã€‚ç½®ä¿¡åº¦è¯„ä¼°å¯ä»¥ä½¿ç”¨LLMæ¥åˆ¤æ–­ç­”æ¡ˆçš„åˆç†æ€§å’Œä¸€è‡´æ€§ã€‚ç­”æ¡ˆèåˆå¯ä»¥ä½¿ç”¨åŠ æƒå¹³å‡æˆ–å…¶ä»–é›†æˆå­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå¦‚ä½•åˆ©ç”¨LLMè‡ªåŠ¨ä¼˜åŒ–agentè®¾è®¡ï¼Œä¾‹å¦‚è‡ªåŠ¨ç”Ÿæˆå·¥å…·ä½¿ç”¨ç­–ç•¥ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTUMIXåœ¨Gemini-2.5-Proå’ŒGemini-2.5-Flashä¸Šï¼Œç›¸å¯¹äºæœ€å…ˆè¿›çš„å·¥å…·å¢å¼ºå’Œæµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ï¼Œå®ç°äº†å¹³å‡é«˜è¾¾3.55%çš„å‡†ç¡®ç‡æå‡ã€‚æ­¤å¤–ï¼ŒTUMIXå¯ä»¥åœ¨ä»…49%çš„æ¨ç†æˆæœ¬ä¸‹ä¿æŒæ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡è¿›ä¸€æ­¥æ‰©å±•æ¥è·å¾—æ›´é«˜çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒTUMIXæ˜¯ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„å·¥å…·ä½¿ç”¨æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TUMIXå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨ç¼–ç¨‹ã€ç§‘å­¦ç ”ç©¶ç­‰ã€‚é€šè¿‡æé«˜LLMçš„æ¨ç†èƒ½åŠ›å’Œå·¥å…·ä½¿ç”¨æ•ˆç‡ï¼ŒTUMIXå¯ä»¥å¸®åŠ©äººä»¬æ›´é«˜æ•ˆåœ°è§£å†³å„ç§é—®é¢˜ï¼Œå¹¶æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚æœªæ¥ï¼ŒTUMIXå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤šçš„é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While integrating tools like Code Interpreter and Search has significantly enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and Gemini-Pro, practical guidance on optimal tool use is lacking. The core challenge is effectively combining textual reasoning, coding, and search for diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an ensemble framework that runs multiple agents in parallel, each employing distinct tool-use strategies and answer paths. Agents in TUMIX iteratively share and refine responses based on the question and previous answers. In experiments, TUMIX achieves significant gains over state-of-the-art tool-augmented and test-time scaling methods, delivering an average accuracy improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference costs. We find that agent diversity and quality are crucial and can be enhanced by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt refinement upon reaching sufficient confidence, preserving performance at only 49% of the inference cost. Further scaling can achieve higher performance, albeit at a greater cost.

