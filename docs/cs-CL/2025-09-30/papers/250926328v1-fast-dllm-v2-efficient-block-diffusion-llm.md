---
layout: default
title: Fast-dLLM v2: Efficient Block-Diffusion LLM
---

# Fast-dLLM v2: Efficient Block-Diffusion LLM

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26328" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26328v1</a>
  <a href="https://arxiv.org/pdf/2509.26328.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26328v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26328v1', 'Fast-dLLM v2: Efficient Block-Diffusion LLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, Enze Xie

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Fast-dLLM v2ï¼šé«˜æ•ˆå—æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ŒåŠ é€Ÿå¹¶è¡Œæ–‡æœ¬ç”Ÿæˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å—æ‰©æ•£è¯­è¨€æ¨¡å‹` `å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆ` `é«˜æ•ˆæ¨ç†` `åˆ†å±‚ç¼“å­˜` `è‡ªå›å½’æ¨¡å‹` `äº’è¡¥æ³¨æ„åŠ›æ©ç ` `ä½èµ„æºå¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è‡ªå›å½’LLMæ¨ç†æ•ˆç‡å—é™äºé¡ºåºè§£ç ï¼Œæˆä¸ºå®é™…åº”ç”¨çš„ç“¶é¢ˆã€‚
2. Fast-dLLM v2é€šè¿‡å—æ‰©æ•£æœºåˆ¶å’Œäº’è¡¥æ³¨æ„åŠ›æ©ç ï¼Œå°†é¢„è®­ç»ƒARæ¨¡å‹è½¬åŒ–ä¸ºæ”¯æŒå¹¶è¡Œç”Ÿæˆçš„dLLMã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFast-dLLM v2åœ¨ä¿æŒæˆ–è¶…è¿‡ARæ¨¡å‹å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾2.5å€çš„æ¨ç†åŠ é€Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªå›å½’ï¼ˆARï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å…¶å›ºæœ‰çš„é¡ºåºè§£ç é™åˆ¶äº†æ¨ç†æ•ˆç‡ã€‚æœ¬æ–‡æå‡ºäº†Fast-dLLM v2ï¼Œä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å—æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°å°†é¢„è®­ç»ƒçš„ARæ¨¡å‹é€‚é…ä¸ºdLLMï¼Œç”¨äºå¹¶è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œä»…éœ€çº¦10äº¿tokençš„å¾®è°ƒã€‚ä¸Dreamç­‰å…¨æ³¨æ„åŠ›æ‰©æ•£LLMï¼ˆ5800äº¿tokenï¼‰ç›¸æ¯”ï¼Œè¿™å‡å°‘äº†500å€çš„è®­ç»ƒæ•°æ®ï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ¡ˆï¼Œå°†å—æ‰©æ•£æœºåˆ¶ä¸äº’è¡¥æ³¨æ„åŠ›æ©ç ç›¸ç»“åˆï¼Œä»è€Œå®ç°å—çŠ¶åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œè€Œä¸ä¼šç‰ºç‰²ARè®­ç»ƒç›®æ ‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥åŠ é€Ÿè§£ç ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ†å±‚ç¼“å­˜æœºåˆ¶ï¼šå—çº§ç¼“å­˜ï¼Œç”¨äºå­˜å‚¨è·¨å—çš„å†å²ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼›å­å—ç¼“å­˜ï¼Œç”¨äºåœ¨éƒ¨åˆ†è§£ç çš„å—å†…å®ç°é«˜æ•ˆçš„å¹¶è¡Œç”Ÿæˆã€‚ç»“åˆæˆ‘ä»¬çš„å¹¶è¡Œè§£ç æµæ°´çº¿ï¼ŒFast-dLLM v2åœ¨ä¸å½±å“ç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œå®ç°äº†æ¯”æ ‡å‡†ARè§£ç é«˜è¾¾2.5å€çš„åŠ é€Ÿã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFast-dLLM v2åœ¨å‡†ç¡®æ€§æ–¹é¢ä¸ARåŸºçº¿ç›¸åŒ¹é…æˆ–è¶…è¿‡ï¼ŒåŒæ—¶åœ¨dLLMä¸­æä¾›äº†æœ€å…ˆè¿›çš„æ•ˆç‡â€”â€”æ ‡å¿—ç€æœç€å¿«é€Ÿå‡†ç¡®çš„LLMçš„å®é™…éƒ¨ç½²è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è‡ªå›å½’ï¼ˆARï¼‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶éœ€è¦é¡ºåºè§£ç ï¼Œæ•ˆç‡è¾ƒä½ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚å…¨æ³¨æ„åŠ›æ‰©æ•£è¯­è¨€æ¨¡å‹è™½ç„¶æ”¯æŒå¹¶è¡Œç”Ÿæˆï¼Œä½†éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨é™ä½è®­ç»ƒæˆæœ¬çš„åŒæ—¶ï¼Œå®ç°LLMçš„å¹¶è¡Œé«˜æ•ˆç”Ÿæˆæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFast-dLLM v2çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é¢„è®­ç»ƒçš„ARæ¨¡å‹è½¬åŒ–ä¸ºå—æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰ï¼Œåˆ©ç”¨å—æ‰©æ•£æœºåˆ¶å®ç°å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆã€‚é€šè¿‡ç»“åˆå—æ‰©æ•£æœºåˆ¶å’Œäº’è¡¥æ³¨æ„åŠ›æ©ç ï¼Œæ¨¡å‹å¯ä»¥åœ¨å—çº§åˆ«è¿›è¡ŒåŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼ŒåŒæ—¶ä¿ç•™ARæ¨¡å‹çš„è®­ç»ƒç›®æ ‡ï¼Œä»è€Œå‡å°‘äº†å¯¹å¤§é‡è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFast-dLLM v2çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å—æ‰©æ•£æœºåˆ¶ï¼šå°†æ–‡æœ¬åˆ†æˆå—ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£è¿‡ç¨‹å¹¶è¡Œç”Ÿæˆè¿™äº›å—ã€‚2) äº’è¡¥æ³¨æ„åŠ›æ©ç ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº’è¡¥æ³¨æ„åŠ›æ©ç æ¥ä¿ç•™ARæ¨¡å‹çš„è®­ç»ƒç›®æ ‡ï¼ŒåŒæ—¶å…è®¸å—çº§åˆ«çš„åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚3) åˆ†å±‚ç¼“å­˜æœºåˆ¶ï¼šåŒ…æ‹¬å—çº§ç¼“å­˜å’Œå­å—ç¼“å­˜ï¼Œç”¨äºå­˜å‚¨å†å²ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼ŒåŠ é€Ÿè§£ç è¿‡ç¨‹ã€‚4) å¹¶è¡Œè§£ç æµæ°´çº¿ï¼šåˆ©ç”¨å¹¶è¡Œè®¡ç®—èµ„æºï¼ŒåŠ é€Ÿå—çš„ç”Ÿæˆå’Œè§£ç ã€‚

**å…³é”®åˆ›æ–°**ï¼šFast-dLLM v2çš„å…³é”®åˆ›æ–°åœ¨äºå…¶é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•å’Œåˆ†å±‚ç¼“å­˜æœºåˆ¶ã€‚ä¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„å…¨æ³¨æ„åŠ›æ‰©æ•£LLMç›¸æ¯”ï¼ŒFast-dLLM v2ä»…éœ€å°‘é‡å¾®è°ƒå³å¯å®ç°å¹¶è¡Œç”Ÿæˆï¼Œå¤§å¤§é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚åˆ†å±‚ç¼“å­˜æœºåˆ¶è¿›ä¸€æ­¥åŠ é€Ÿäº†è§£ç è¿‡ç¨‹ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†å—æ‰©æ•£æŸå¤±å’ŒARæŸå¤±çš„åŠ æƒç»„åˆï¼Œä»¥å¹³è¡¡å¹¶è¡Œç”Ÿæˆå’Œåºåˆ—å»ºæ¨¡çš„èƒ½åŠ›ã€‚äº’è¡¥æ³¨æ„åŠ›æ©ç çš„è®¾è®¡å…è®¸æ¨¡å‹åœ¨å—å†…è¿›è¡ŒåŒå‘æ³¨æ„åŠ›è®¡ç®—ï¼ŒåŒæ—¶é™åˆ¶å—é—´çš„æ³¨æ„åŠ›ï¼Œä»¥ä¿ç•™ARæ¨¡å‹çš„å› æœå…³ç³»ã€‚åˆ†å±‚ç¼“å­˜æœºåˆ¶ä¸­çš„å—å¤§å°å’Œå­å—å¤§å°æ˜¯é‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ¨¡å‹å¤§å°è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Fast-dLLM v2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨å‡†ç¡®æ€§æ–¹é¢ä¸ARåŸºçº¿ç›¸åŒ¹é…æˆ–è¶…è¿‡ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒå®ç°äº†é«˜è¾¾2.5å€çš„æ¨ç†åŠ é€Ÿï¼ŒåŒæ—¶ä»…éœ€çº¦10äº¿tokençš„å¾®è°ƒï¼Œä¸å…¨æ³¨æ„åŠ›æ‰©æ•£LLMç›¸æ¯”ï¼Œè®­ç»ƒæ•°æ®å‡å°‘äº†500å€ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFast-dLLM v2åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œæ˜¯dLLMé¢†åŸŸçš„ä¸€é¡¹é‡è¦è¿›å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Fast-dLLM v2å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼šå®æ—¶å¯¹è¯ç³»ç»Ÿã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€ä»£ç ç”Ÿæˆç­‰ã€‚å…¶é«˜æ•ˆçš„å¹¶è¡Œç”Ÿæˆèƒ½åŠ›å¯ä»¥æ˜¾è‘—é™ä½å»¶è¿Ÿï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œç”±äºå…¶è®­ç»ƒæˆæœ¬è¾ƒä½ï¼Œå¯ä»¥æ›´å®¹æ˜“åœ°éƒ¨ç½²åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—å¹³å°ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨LLMåœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.

