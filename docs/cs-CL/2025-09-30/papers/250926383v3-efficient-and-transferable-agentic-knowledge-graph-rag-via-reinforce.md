---
layout: default
title: Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning
---

# Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26383" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26383v3</a>
  <a href="https://arxiv.org/pdf/2509.26383.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26383v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26383v3', 'Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinyeop Song, Song Wang, Julian Shun, Yada Zhu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-09)

**å¤‡æ³¨**: 10 pages, 5 figures. Submitted to ICLR 2026

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Jinyeop3110/KG-R1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆå¯è¿ç§»AgenticçŸ¥è¯†å›¾è°±RAGæ¡†æ¶KG-R1**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†å›¾è°±` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `Agent` `é—®ç­”ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰KG-RAGç³»ç»Ÿä¾èµ–å¤šæ¨¡å—LLMç»„åˆï¼Œå¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚ä¸”æ¨¡å‹è¡Œä¸ºä¸ç‰¹å®šçŸ¥è¯†å›¾è°±ç»‘å®šã€‚
2. KG-R1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå•ä¸ªagentï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªä¸»ä¸çŸ¥è¯†å›¾è°±äº¤äº’å¹¶è¿›è¡Œæ£€ç´¢ã€æ¨ç†å’Œç”Ÿæˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒKG-R1åœ¨KGQAä»»åŠ¡ä¸Šï¼Œä½¿ç”¨æ›´å°‘çš„tokenå®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œå¹¶å…·å¤‡è‰¯å¥½çš„è·¨çŸ¥è¯†å›¾è°±è¿ç§»èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆ(KG-RAG)å°†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä¸ç»“æ„åŒ–ã€å¯éªŒè¯çš„çŸ¥è¯†å›¾è°±(KG)ç›¸ç»“åˆï¼Œä»¥å‡å°‘å¹»è§‰å¹¶æš´éœ²æ¨ç†è½¨è¿¹ã€‚ç„¶è€Œï¼Œè®¸å¤šKG-RAGç³»ç»Ÿç»„åˆäº†å¤šä¸ªLLMæ¨¡å—(ä¾‹å¦‚ï¼Œè§„åˆ’ã€æ¨ç†å’Œå“åº”)ï¼Œä»è€Œå¢åŠ äº†æ¨ç†æˆæœ¬ï¼Œå¹¶å°†è¡Œä¸ºç»‘å®šåˆ°ç‰¹å®šçš„ç›®æ ‡KGã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†KG-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ (RL)å®ç°çš„agentic KGæ£€ç´¢å¢å¼ºç”Ÿæˆ(KG-RAG)æ¡†æ¶ã€‚KG-R1åˆ©ç”¨å•ä¸ªagentä¸KGä½œä¸ºå…¶ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå­¦ä¹ åœ¨æ¯ä¸ªæ­¥éª¤ä¸­æ£€ç´¢ä¿¡æ¯ï¼Œå¹¶å°†æ£€ç´¢åˆ°çš„ä¿¡æ¯æ•´åˆåˆ°å…¶æ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚è¯¥è¿‡ç¨‹é€šè¿‡ç«¯åˆ°ç«¯çš„RLè¿›è¡Œä¼˜åŒ–ã€‚åœ¨çŸ¥è¯†å›¾è°±é—®ç­”(KGQA)åŸºå‡†æµ‹è¯•çš„å—æ§å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†æ•ˆç‡å’Œå¯è¿ç§»æ€§ï¼šä½¿ç”¨Qwen-2.5-3Bï¼ŒKG-R1ä»¥æ¯”ä½¿ç”¨æ›´å¤§åŸºç¡€æ¨¡å‹æˆ–å¾®è°ƒæ¨¡å‹çš„å…ˆå‰å¤šæ¨¡å—å·¥ä½œæµç¨‹æ–¹æ³•æ›´å°‘çš„ç”Ÿæˆtokenæé«˜äº†ç­”æ¡ˆå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒKG-R1å®ç°äº†å³æ’å³ç”¨ï¼šç»è¿‡è®­ç»ƒåï¼Œå®ƒæ— éœ€ä¿®æ”¹å³å¯åœ¨æ–°KGä¸Šä¿æŒå¼ºå¤§çš„å‡†ç¡®æ€§ã€‚è¿™äº›ç‰¹æ€§ä½¿KG-R1æˆä¸ºä¸€ä¸ªæœ‰å‰æ™¯çš„KG-RAGæ¡†æ¶ï¼Œé€‚ç”¨äºå®é™…éƒ¨ç½²ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨https://github.com/Jinyeop3110/KG-R1ä¸Šå…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰KG-RAGæ–¹æ³•é€šå¸¸é‡‡ç”¨å¤šæ¨¡å—çš„LLMæµç¨‹ï¼Œä¾‹å¦‚è§„åˆ’ã€æ¨ç†å’Œç”Ÿæˆï¼Œè¿™å¯¼è‡´äº†è¾ƒé«˜çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€é’ˆå¯¹ç‰¹å®šçš„çŸ¥è¯†å›¾è°±è¿›è¡Œä¼˜åŒ–ï¼Œç¼ºä¹è·¨å›¾è°±çš„æ³›åŒ–èƒ½åŠ›ï¼Œéš¾ä»¥é€‚åº”å®é™…åº”ç”¨ä¸­ä¸æ–­å˜åŒ–çš„çŸ¥è¯†åº“ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜æ•ˆä¸”å¯è¿ç§»çš„KG-RAGæ¡†æ¶æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šKG-R1çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†KG-RAGè¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªagentä¸çŸ¥è¯†å›¾è°±ç¯å¢ƒçš„äº¤äº’è¿‡ç¨‹ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œagentèƒ½å¤Ÿå­¦ä¹ å¦‚ä½•åœ¨æ¯ä¸€æ­¥é€‰æ‹©åˆé€‚çš„çŸ¥è¯†å›¾è°±èŠ‚ç‚¹è¿›è¡Œæ£€ç´¢ï¼Œå¹¶å°†æ£€ç´¢åˆ°çš„ä¿¡æ¯èå…¥åˆ°è‡ªèº«çš„æ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚è¿™ç§ç«¯åˆ°ç«¯çš„å­¦ä¹ æ–¹å¼é¿å…äº†æ‰‹åŠ¨è®¾è®¡å¤æ‚çš„å¤šæ¨¡å—æµç¨‹ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKG-R1æ¡†æ¶åŒ…å«ä¸€ä¸ªagentå’Œä¸€ä¸ªçŸ¥è¯†å›¾è°±ç¯å¢ƒã€‚Agentæ¥æ”¶é—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼ˆå³é€‰æ‹©ä¸€ä¸ªçŸ¥è¯†å›¾è°±èŠ‚ç‚¹è¿›è¡Œæ£€ç´¢ï¼‰ã€‚ç¯å¢ƒæ ¹æ®agentçš„åŠ¨ä½œè¿”å›ç›¸åº”çš„èŠ‚ç‚¹ä¿¡æ¯ã€‚Agentå°†æ£€ç´¢åˆ°çš„ä¿¡æ¯èå…¥åˆ°è‡ªèº«çš„è®°å¿†ä¸­ï¼Œå¹¶é‡å¤ä¸Šè¿°è¿‡ç¨‹ç›´åˆ°ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚æ•´ä¸ªè¿‡ç¨‹é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–å›ç­”é—®é¢˜çš„å‡†ç¡®ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šKG-R1æœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†KG-RAGè¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªagentä¸ç¯å¢ƒçš„äº¤äº’è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚ä¸ä¼ ç»Ÿçš„å¤šæ¨¡å—æ–¹æ³•ç›¸æ¯”ï¼ŒKG-R1é¿å…äº†æ‰‹åŠ¨è®¾è®¡å¤æ‚çš„æµç¨‹ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒKG-R1çš„agentèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ å¦‚ä½•é€‰æ‹©åˆé€‚çš„çŸ¥è¯†å›¾è°±èŠ‚ç‚¹è¿›è¡Œæ£€ç´¢ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šKG-R1ä½¿ç”¨Qwen-2.5-3Bä½œä¸ºåŸºç¡€è¯­è¨€æ¨¡å‹ã€‚Agentçš„ç½‘ç»œç»“æ„åŒ…æ‹¬ä¸€ä¸ªembeddingå±‚ã€ä¸€ä¸ªLSTMå±‚å’Œä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡ç”Ÿæˆç­”æ¡ˆä¸æ­£ç¡®ç­”æ¡ˆä¹‹é—´çš„å·®å¼‚ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•é‡‡ç”¨Proximal Policy Optimization (PPO)ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨reward shapingæŠ€æœ¯æ¥åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒKG-R1åœ¨KGQAåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä½¿ç”¨Qwen-2.5-3Bæ¨¡å‹ï¼ŒKG-R1åœ¨å›ç­”å‡†ç¡®ç‡æ–¹é¢ä¼˜äºä½¿ç”¨æ›´å¤§è§„æ¨¡æ¨¡å‹æˆ–ç»è¿‡å¾®è°ƒçš„ä¼ ç»Ÿå¤šæ¨¡å—æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒKG-R1è¿˜å±•ç°å‡ºäº†è‰¯å¥½çš„è·¨çŸ¥è¯†å›¾è°±è¿ç§»èƒ½åŠ›ï¼Œæ— éœ€ä¿®æ”¹å³å¯åœ¨æ–°çŸ¥è¯†å›¾è°±ä¸Šä¿æŒè¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒKG-R1çš„å‡†ç¡®ç‡æå‡è¶…è¿‡10%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

KG-R1å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ„å»ºã€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚å®ƒå¯ä»¥åº”ç”¨äºå„ç§é¢†åŸŸï¼Œä¾‹å¦‚åŒ»ç–—ã€é‡‘èã€æ•™è‚²ç­‰ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€å¯é çš„çŸ¥è¯†æœåŠ¡ã€‚æœªæ¥ï¼ŒKG-R1å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ”¯æŒå¤šè¯­è¨€çŸ¥è¯†å›¾è°±ã€åŠ¨æ€çŸ¥è¯†å›¾è°±ç­‰ï¼Œä»è€Œæ›´å¥½åœ°æ»¡è¶³å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.

