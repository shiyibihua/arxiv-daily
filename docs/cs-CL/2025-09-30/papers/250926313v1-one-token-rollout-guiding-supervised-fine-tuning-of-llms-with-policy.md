---
layout: default
title: One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient
---

# One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26313" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26313v1</a>
  <a href="https://arxiv.org/pdf/2509.26313.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26313v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26313v1', 'One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rui Ming, Haoyuan Wu, Shoubo Hu, Zhuolun He, Bei Yu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOne-Token Rolloutç®—æ³•ï¼Œåˆ©ç”¨ç­–ç•¥æ¢¯åº¦æŒ‡å¯¼LLMçš„ç›‘ç£å¾®è°ƒï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç›‘ç£å¾®è°ƒ` `ç­–ç•¥æ¢¯åº¦` `å¼ºåŒ–å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `On-Policyå­¦ä¹ ` `One-Token Rollout`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç›‘ç£å¾®è°ƒSFTåœ¨LLMæ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå…¶ä¸»è¦åŸå› æ˜¯SFTå­¦ä¹ çš„æ˜¯å›ºå®šçš„off-policyæ•°æ®ã€‚
2. One-Token Rollout (OTR)ç®—æ³•å°†æ¯ä¸ªtokenç”Ÿæˆè§†ä¸ºå•æ­¥å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨ç­–ç•¥æ¢¯åº¦å°†off-policyæ•°æ®è½¬åŒ–ä¸ºon-policyä¿¡å·ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒOTRåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œé€šç”¨é¢†åŸŸæ¨ç†ç­‰ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºæ ‡å‡†SFTï¼ŒéªŒè¯äº†on-policyæ•°æ®å¯¹æ³›åŒ–çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›‘ç£å¾®è°ƒ(SFT)æ˜¯è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„ä¸»è¦æ–¹æ³•ï¼Œä½†ä¸å¼ºåŒ–å­¦ä¹ (RL)ç›¸æ¯”ï¼Œå®ƒåœ¨æ³›åŒ–æ–¹é¢å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œè¿™ç§æ€§èƒ½å·®å¼‚ä¸ä»…æºäºæŸå¤±å‡½æ•°ï¼Œæ›´æºäºä¸€ä¸ªæ›´æ ¹æœ¬çš„åŒºåˆ«ï¼šSFTä»å›ºå®šçš„ã€é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†ä¸­å­¦ä¹ ï¼Œè€ŒRLåˆ©ç”¨ä»å½“å‰ç­–ç•¥ä¸­é‡‡æ ·çš„on-policyæ•°æ®ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†one-token rollout (OTR)ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¾®è°ƒç®—æ³•ï¼Œå®ƒä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•æŒ‡å¯¼SFTã€‚OTRé€šè¿‡å°†æ¯ä¸ªtokenç”Ÿæˆè§†ä¸ºå•æ­¥å¼ºåŒ–å­¦ä¹ è½¨è¿¹æ¥é‡æ„è‡ªå›å½’å­¦ä¹ è¿‡ç¨‹ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œå®ƒé€šè¿‡ä»å½“å‰ç­–ç•¥çš„åˆ†å¸ƒä¸­é‡‡æ ·å¤šä¸ªå€™é€‰tokenæ¥æ‰§è¡Œè’™ç‰¹å¡ç½—â€œrolloutâ€ã€‚ç„¶åï¼Œä½¿ç”¨æ¥è‡ªç›‘ç£æ•°æ®çš„ground-truth tokenä¸ºè¿™äº›æ ·æœ¬æä¾›å¥–åŠ±ä¿¡å·ã€‚åœ¨ç­–ç•¥æ¢¯åº¦çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬çš„ç®—æ³•å°†é™æ€çš„ã€off-policyçš„ç›‘ç£æ•°æ®è½¬åŒ–ä¸ºtokençº§åˆ«çš„åŠ¨æ€çš„ã€on-policyä¿¡å·ï¼Œä»è€Œè·å¾—on-policyå­¦ä¹ çš„æ³›åŒ–ä¼˜åŠ¿ï¼ŒåŒæ—¶é¿å…äº†å®Œæ•´å¥å­ç”Ÿæˆçš„é«˜æ˜‚å¼€é”€ã€‚é€šè¿‡åœ¨æ¶µç›–æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œé€šç”¨é¢†åŸŸæ¨ç†ç­‰å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†OTRå§‹ç»ˆä¼˜äºæ ‡å‡†SFTã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒOTRæ˜¯ä¸€ç§å¼ºå¤§è€Œå®ç”¨çš„LLMå¾®è°ƒæ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†ä»¤äººä¿¡æœçš„è¯æ®ï¼Œè¡¨æ˜æ•°æ®çš„on-policyæ€§è´¨æ˜¯æ³›åŒ–çš„å…³é”®é©±åŠ¨å› ç´ ï¼Œä¸ºå¾®è°ƒLLMæä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–°æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜¯å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¸¸ç”¨æ–¹æ³•ï¼Œä½†å…¶æ³›åŒ–èƒ½åŠ›ä¸å¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚SFTä½¿ç”¨é¢„å…ˆæ”¶é›†çš„é™æ€æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†æ¨¡å‹æ¢ç´¢å’Œé€‚åº”æ–°æ•°æ®çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•æé«˜SFTçš„æ³›åŒ–èƒ½åŠ›æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†SFTè¿‡ç¨‹è½¬åŒ–ä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œå…·ä½“æ¥è¯´ï¼Œå°†æ¯ä¸ªtokençš„ç”Ÿæˆè§†ä¸ºä¸€ä¸ªå•æ­¥å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥ç­–ç•¥æ¢¯åº¦ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®å½“å‰ç­–ç•¥ç”Ÿæˆtokenï¼Œå¹¶æ ¹æ®ground truth tokenè·å¾—å¥–åŠ±ï¼Œä»è€Œå®ç°on-policyå­¦ä¹ ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨åˆ©ç”¨on-policyæ•°æ®çš„ä¼˜åŠ¿ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOTRç®—æ³•çš„æ ¸å¿ƒæµç¨‹å¦‚ä¸‹ï¼š1. å¯¹äºæ¯ä¸ªtokenç”Ÿæˆæ­¥éª¤ï¼Œä»å½“å‰LLMç­–ç•¥ä¸­é‡‡æ ·å¤šä¸ªå€™é€‰tokenã€‚2. ä½¿ç”¨ç›‘ç£æ•°æ®ä¸­çš„ground-truth tokenä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œè¯„ä¼°æ¯ä¸ªå€™é€‰tokençš„è´¨é‡ã€‚3. ä½¿ç”¨ç­–ç•¥æ¢¯åº¦ç®—æ³•æ›´æ–°LLMçš„å‚æ•°ï¼Œä½¿å¾—æ¨¡å‹æ›´å€¾å‘äºç”Ÿæˆé«˜è´¨é‡çš„tokenã€‚è¿™ä¸ªè¿‡ç¨‹åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šè¿­ä»£è¿›è¡Œï¼Œç›´åˆ°æ¨¡å‹æ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šOTRçš„å…³é”®åˆ›æ–°åœ¨äºå°†é™æ€çš„ã€off-policyçš„ç›‘ç£æ•°æ®è½¬åŒ–ä¸ºåŠ¨æ€çš„ã€on-policyä¿¡å·ã€‚é€šè¿‡åœ¨tokençº§åˆ«è¿›è¡Œrolloutå’Œå¥–åŠ±ï¼ŒOTRèƒ½å¤Ÿæ¨¡æ‹Ÿå¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢è¿‡ç¨‹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„SFTæ–¹æ³•ç›¸æ¯”ï¼ŒOTRä¸éœ€è¦é¢å¤–çš„å¼ºåŒ–å­¦ä¹ æ•°æ®æˆ–å¤æ‚çš„å¥–åŠ±å‡½æ•°è®¾è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šOTRçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. Rolloutç­–ç•¥ï¼šä»å½“å‰LLMç­–ç•¥ä¸­é‡‡æ ·å¤šä¸ªå€™é€‰tokenã€‚å¯ä»¥ä½¿ç”¨ä¸åŒçš„é‡‡æ ·æ–¹æ³•ï¼Œå¦‚top-ké‡‡æ ·æˆ–nucleusé‡‡æ ·ã€‚2. å¥–åŠ±å‡½æ•°ï¼šä½¿ç”¨ç›‘ç£æ•°æ®ä¸­çš„ground-truth tokenä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚å¦‚æœç”Ÿæˆçš„tokenä¸ground-truth tokenç›¸åŒï¼Œåˆ™å¥–åŠ±ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚3. ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼šå¯ä»¥ä½¿ç”¨ä¸åŒçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œå¦‚REINFORCEæˆ–PPOã€‚è®ºæ–‡ä¸­å…·ä½“ä½¿ç”¨çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOTRåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œé€šç”¨é¢†åŸŸæ¨ç†ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºæ ‡å‡†çš„SFTæ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒOTRåœ¨å„ç§ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºä¸€è‡´çš„ä¼˜è¶Šæ€§ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¾®è°ƒLLMçš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜LLMçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°é€‚åº”æ–°çš„ä»»åŠ¡å’Œæ•°æ®ï¼Œä»è€Œæé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å’Œæ•ˆæœã€‚è¯¥æ–¹æ³•åœ¨æ•™è‚²ã€å®¢æœã€å†…å®¹åˆ›ä½œç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), yet it often struggles with generalization compared to reinforcement learning (RL). In this work, we posit that this performance disparity stems not just from the loss function, but from a more fundamental difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. Building on this hypothesis, we introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides SFT with the policy gradient method. OTR reframes the autoregressive learning process by treating each token generation as a single-step reinforcement learning trajectory. At each step, it performs a Monte Carlo ``rollout'' by sampling multiple candidate tokens from the current policy's distribution. The ground-truth token from the supervised data is then used to provide a reward signal to these samples. Guided by policy gradient, our algorithm repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level, capturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation. Through extensive experiments on a diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning, we demonstrate that OTR consistently outperforms standard SFT. Our findings establish OTR as a powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is a critical driver of generalization, offering a promising new direction for fine-tuning LLMs.

