---
layout: default
title: Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis
---

# Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26074" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26074v2</a>
  <a href="https://arxiv.org/pdf/2509.26074.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26074v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26074v2', 'Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Leitian Tao, Xuefeng Du, Sharon Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-14)

**å¤‡æ³¨**: Accepted by NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/deeplearning-wisc/lens)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LENSï¼šé€šè¿‡æ½œåœ¨ç©ºé—´åˆæˆæå‡æœ‰é™åå¥½æ•°æ®ä¸‹çš„å¥–åŠ±æ¨¡å‹å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±å»ºæ¨¡` `åå¥½å­¦ä¹ ` `æ•°æ®å¢å¼º` `æ½œåœ¨ç©ºé—´` `å˜åˆ†è‡ªç¼–ç å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¥–åŠ±å»ºæ¨¡ä¾èµ–å¤§é‡åå¥½æ•°æ®ï¼Œä½†æ•°æ®æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œç°æœ‰æ–‡æœ¬åˆæˆæ–¹æ³•è®¡ç®—å¼€é”€å¤§ã€‚
2. LENSåœ¨LLMæ½œåœ¨ç©ºé—´ä¸­åˆæˆåå¥½æ•°æ®ï¼Œåˆ©ç”¨VAEå­¦ä¹ ç»“æ„åŒ–è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æ‰°åŠ¨ç”Ÿæˆå¤šæ ·æ ·æœ¬ã€‚
3. å®éªŒè¡¨æ˜LENSä¼˜äºæ–‡æœ¬å¢å¼ºæ–¹æ³•ï¼Œç”Ÿæˆé€Ÿåº¦æå‡18å€ï¼Œæ¨¡å‹è§„æ¨¡ç¼©å°16000å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥–åŠ±å»ºæ¨¡å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½è‡³å…³é‡è¦ï¼Œä½†å¸¸å¸¸å—é™äºåå¥½æ•°æ®çš„é«˜æ˜‚æˆæœ¬ã€‚ç°æœ‰çš„æ–‡æœ¬æ•°æ®åˆæˆæ–¹æ³•è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºLENSçš„æ–°æ¡†æ¶ï¼Œç”¨äºç›´æ¥åœ¨LLMçš„æ½œåœ¨åµŒå…¥ç©ºé—´ä¸­åˆæˆåå¥½æ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¥å­¦ä¹ å“åº”åµŒå…¥çš„ç»“æ„åŒ–æ½œåœ¨è¡¨ç¤ºã€‚é€šè¿‡åœ¨æ­¤æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œå—æ§æ‰°åŠ¨å¹¶è§£ç å›åµŒå…¥ç©ºé—´ï¼Œæˆ‘ä»¬é«˜æ•ˆåœ°ç”Ÿæˆå¤šæ ·ä¸”è¯­ä¹‰ä¸€è‡´çš„åˆæˆåå¥½å¯¹ï¼Œç»•è¿‡äº†æ˜‚è´µçš„æ–‡æœ¬ç”Ÿæˆå’Œæ ‡æ³¨ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºä¿è¯ï¼Œè¡¨æ˜æˆ‘ä»¬åˆæˆçš„åå¥½å¯¹è¿‘ä¼¼ä¿ç•™äº†åŸå§‹åå¥½æ’åºï¼Œå¹¶æé«˜äº†å¥–åŠ±æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ½œåœ¨ç©ºé—´åˆæˆåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºäºæ–‡æœ¬çš„å¢å¼ºæ–¹æ³•ï¼Œåœ¨ç”Ÿæˆé€Ÿåº¦ä¸Šå¿«18å€ï¼Œæ¨¡å‹è§„æ¨¡å°16000å€çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å“è¶Šçš„ç»“æœã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºé€šè¿‡é«˜æ•ˆçš„æ•°æ®å¢å¼ºæ¥å¢å¼ºå¥–åŠ±å»ºæ¨¡æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¥–åŠ±å»ºæ¨¡æ—¨åœ¨è®­ç»ƒä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹äººç±»å¯¹ä¸åŒLLMè¾“å‡ºçš„åå¥½ã€‚ç„¶è€Œï¼Œè·å–è¶³å¤Ÿæ•°é‡çš„äººå·¥æ ‡æ³¨åå¥½æ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œæˆä¸ºè®­ç»ƒé«˜è´¨é‡å¥–åŠ±æ¨¡å‹çš„ç“¶é¢ˆã€‚ç°æœ‰çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä¾‹å¦‚åŸºäºæ–‡æœ¬çš„ç”Ÿæˆå¼æ–¹æ³•ï¼Œè®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLENSçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨LLMçš„æ½œåœ¨åµŒå…¥ç©ºé—´ä¸­ç›´æ¥åˆæˆåå¥½æ•°æ®ï¼Œè€Œä¸æ˜¯åœ¨æ–‡æœ¬ç©ºé—´ä¸­è¿›è¡Œç”Ÿæˆã€‚é€šè¿‡å­¦ä¹ å“åº”åµŒå…¥çš„ç»“æ„åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œå¯ä»¥åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå—æ§æ‰°åŠ¨ï¼Œç”Ÿæˆè¯­ä¹‰ä¸€è‡´çš„åˆæˆåå¥½å¯¹ã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ˜‚è´µçš„æ–‡æœ¬ç”Ÿæˆå’Œæ ‡æ³¨è¿‡ç¨‹ï¼Œä»è€Œæé«˜äº†æ•°æ®å¢å¼ºçš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLENSæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **åµŒå…¥æ¨¡å—**ï¼šå°†LLMçš„å“åº”æ–‡æœ¬åµŒå…¥åˆ°é«˜ç»´å‘é‡ç©ºé—´ä¸­ã€‚2) **VAEæ¨¡å—**ï¼šä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨å­¦ä¹ å“åº”åµŒå…¥çš„ç»“æ„åŒ–æ½œåœ¨è¡¨ç¤ºã€‚VAEç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œç¼–ç å™¨å°†åµŒå…¥æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ï¼Œè§£ç å™¨å°†æ½œåœ¨å‘é‡é‡æ„ä¸ºåµŒå…¥ã€‚3) **æ‰°åŠ¨æ¨¡å—**ï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­å¯¹æ½œåœ¨å‘é‡è¿›è¡Œå—æ§æ‰°åŠ¨ï¼Œç”Ÿæˆæ–°çš„æ½œåœ¨å‘é‡ï¼Œä»£è¡¨ä¸åŒçš„åå¥½ç¨‹åº¦ã€‚4) **è§£ç æ¨¡å—**ï¼šå°†æ‰°åŠ¨åçš„æ½œåœ¨å‘é‡è§£ç å›åµŒå…¥ç©ºé—´ï¼Œå¾—åˆ°åˆæˆçš„å“åº”åµŒå…¥ã€‚5) **åå¥½å¯¹æ„å»ºæ¨¡å—**ï¼šå°†åŸå§‹å“åº”åµŒå…¥å’Œåˆæˆçš„å“åº”åµŒå…¥é…å¯¹ï¼Œæ„å»ºåˆæˆçš„åå¥½æ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šLENSçš„å…³é”®åˆ›æ–°åœ¨äºç›´æ¥åœ¨LLMçš„æ½œåœ¨åµŒå…¥ç©ºé—´ä¸­è¿›è¡Œæ•°æ®åˆæˆï¼Œé¿å…äº†æ–‡æœ¬ç”Ÿæˆå’Œæ ‡æ³¨çš„æˆæœ¬ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLENSå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼ŒLENSé€šè¿‡ç†è®ºåˆ†æè¯æ˜äº†åˆæˆçš„åå¥½å¯¹è¿‘ä¼¼ä¿ç•™äº†åŸå§‹åå¥½æ’åºï¼Œå¹¶æé«˜äº†å¥–åŠ±æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šVAEçš„æ½œåœ¨ç©ºé—´ç»´åº¦ã€æ‰°åŠ¨ç­–ç•¥ï¼ˆä¾‹å¦‚ï¼Œæ·»åŠ é«˜æ–¯å™ªå£°æˆ–è¿›è¡Œçº¿æ€§æ’å€¼ï¼‰ã€ä»¥åŠæŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œé‡æ„æŸå¤±å’ŒKLæ•£åº¦ï¼‰æ˜¯LENSçš„å…³é”®è®¾è®¡å‚æ•°ã€‚è®ºæ–‡å¯èƒ½é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ç¡®ä¿æ½œåœ¨ç©ºé—´çš„å¹³æ»‘æ€§å’Œè¿ç»­æ€§ï¼Œå¹¶ä½¿ç”¨ç‰¹å®šçš„æ‰°åŠ¨ç­–ç•¥æ¥æ§åˆ¶åˆæˆæ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œè¶…å‚æ•°è®¾ç½®éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LENSåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºäºæ–‡æœ¬çš„å¢å¼ºæ–¹æ³•ï¼Œåœ¨ç”Ÿæˆé€Ÿåº¦ä¸Šå¿«18å€ï¼Œæ¨¡å‹è§„æ¨¡å°16000å€çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å“è¶Šçš„ç»“æœã€‚è¿™è¡¨æ˜LENSæ˜¯ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LENSå¯åº”ç”¨äºå„ç§éœ€è¦å¥–åŠ±å»ºæ¨¡çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡é«˜æ•ˆåœ°åˆæˆåå¥½æ•°æ®ï¼ŒLENSå¯ä»¥é™ä½è®­ç»ƒé«˜è´¨é‡å¥–åŠ±æ¨¡å‹çš„æˆæœ¬ï¼Œä»è€Œæå‡LLMåœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºæ•°æ®ç¨€ç¼ºçš„åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼ŒåŠ é€ŸLLMçš„å¯¹é½è¿‡ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reward modeling, crucial for aligning large language models (LLMs) with human preferences, is often bottlenecked by the high cost of preference data. Existing textual data synthesis methods are computationally expensive. We propose a novel framework LENS for synthesizing preference data directly in the LLM's latent embedding space. Our method employs a Variational Autoencoder (VAE) to learn a structured latent representation of response embeddings. By performing controlled perturbations in this latent space and decoding back to the embedding space, we efficiently generate diverse, semantically consistent synthetic preference pairs, bypassing costly text generation and annotation. We provide theoretical guarantees that our synthesized pairs approximately preserve original preference ordering and improve reward model generalization. Empirically, our latent-space synthesis significantly outperforms text-based augmentation on standard benchmarks, achieving superior results while being 18x faster in generation and using a 16,000x smaller model. Our work offers a scalable and effective alternative for enhancing reward modeling through efficient data augmentation. Code is publicly available at https://github.com/deeplearning-wisc/lens

