---
layout: default
title: Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel
---

# Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25913" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25913v2</a>
  <a href="https://arxiv.org/pdf/2509.25913.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25913v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25913v2', 'Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chuanyang Zheng, Jiankai Sun, Yihang Gao, Enze Xie, Yuehao Wang, Peihao Wang, Ting Xu, Matthew Chang, Liliang Ren, Jingyao Li, Jing Xiong, Kashif Rasul, Mac Schwager, Anderson Schneider, Zhangyang Wang, Yuriy Nevmyvaka

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-14)

**å¤‡æ³¨**: Tech Report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKERNä»¥æ›¿ä»£Softmaxè§£å†³MoEæ¨¡å‹ä¸­çš„è·¯ç”±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `Nadaraya-Watsonå›å½’` `è·¯ç”±å‡½æ•°` `æ·±åº¦å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è®¡ç®—æ•ˆç‡` `ReLUæ¿€æ´»` `â„“2å½’ä¸€åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„MoEæ¨¡å‹æ™®éä¾èµ–Softmaxä½œä¸ºè·¯ç”±å‡½æ•°ï¼Œä½†è¿™ä¸€é€‰æ‹©ç¼ºä¹ç†è®ºæ”¯æŒï¼Œå¯èƒ½é™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„KERNè·¯ç”±å‡½æ•°ï¼ŒåŸºäºNadaraya-Watsonå›å½’çš„æ•°å­¦æ¡†æ¶ï¼Œæä¾›äº†ä¸€ç§æ›´çµæ´»çš„è·¯ç”±æœºåˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒKERNåœ¨å¤šä¸ªMoEå’ŒLLMä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å·²æˆä¸ºæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºçŸ³ã€‚ä¼ ç»Ÿä¸Šï¼ŒMoEä¾èµ–äºSoftmaxä½œä¸ºè·¯ç”±è¯„åˆ†å‡½æ•°æ¥èšåˆä¸“å®¶è¾“å‡ºï¼Œä½†è¿™ä¸€é€‰æ‹©å¹¶æœªç»è¿‡ä¸¥æ ¼çš„ç†è®ºéªŒè¯ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†ç»å…¸çš„Nadaraya-Watsonå›å½’ï¼Œå‘ç°MoEä¸å…¶å…·æœ‰ç›¸åŒçš„æ•°å­¦å½¢å¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é›¶é¢å¤–æˆæœ¬çš„KERNè·¯ç”±å‡½æ•°ï¼Œä½œä¸ºSoftmaxçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨MoEå’ŒLLMä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰MoEæ¨¡å‹ä¸­Softmaxè·¯ç”±å‡½æ•°çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å…¶ç¼ºä¹ç†è®ºåŸºç¡€çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬é€šè¿‡é‡æ–°å®¡è§†Nadaraya-Watsonå›å½’ï¼Œæå‡ºKERNè·¯ç”±å‡½æ•°ï¼Œåˆ©ç”¨å…¶æ•°å­¦ç‰¹æ€§æ¥æ›¿ä»£Softmaxï¼Œä»è€Œå®ç°æ›´çµæ´»çš„ä¸“å®¶é€‰æ‹©æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKERNè·¯ç”±å‡½æ•°é‡‡ç”¨FFNé£æ ¼çš„ç»“æ„ï¼Œç»“åˆReLUæ¿€æ´»å’Œâ„“2å½’ä¸€åŒ–ï¼Œæ•´ä½“æµç¨‹åŒ…æ‹¬è¾“å…¥ç‰¹å¾çš„å¤„ç†ã€è·¯ç”±æƒé‡çš„è®¡ç®—å’Œä¸“å®¶è¾“å‡ºçš„èšåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šKERNçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é›¶é¢å¤–æˆæœ¬çš„è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹ï¼Œæä¾›æ¯”Softmaxæ›´ä¼˜çš„è·¯ç”±é€‰æ‹©èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šKERNè·¯ç”±å‡½æ•°çš„è®¾è®¡åŒ…æ‹¬ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°å’Œâ„“2å½’ä¸€åŒ–ï¼Œä»¥ç¡®ä¿è·¯ç”±æƒé‡çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ï¼ŒåŒæ—¶ä¿æŒä¸Nadaraya-Watsonå›å½’çš„ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKERNåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºä¼ ç»ŸSoftmaxè·¯ç”±å‡½æ•°ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°5%-10%ã€‚æ­¤å¤–ï¼ŒKERNåœ¨è®¡ç®—æ•ˆç‡ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºMoEæ¨¡å‹æ–°è·¯ç”±å‡½æ•°çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰éœ€è¦å¤§è§„æ¨¡æ¨¡å‹çš„ä»»åŠ¡ã€‚é€šè¿‡æ”¹è¿›è·¯ç”±æœºåˆ¶ï¼ŒKERNæœ‰æœ›æå‡æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) has become a cornerstone in recent state-of-the-art large language models (LLMs). Traditionally, MoE relies on $\mathrm{Softmax}$ as the router score function to aggregate expert output, a designed choice that has persisted from the earliest MoE models to modern LLMs, and is now widely regarded as standard practice. However, the necessity of using $\mathrm{Softmax}$ to project router weights into a probability simplex remains an unchallenged assumption rather than a principled design choice. In this work, we first revisit the classical Nadaraya-Watson regression and observe that MoE shares the same mathematical formulation as Nadaraya-Watson regression. Furthermore, we show that both feed-forward neural network (FFN) and MoE can be interpreted as a special case of Nadaraya-Watson regression, where the kernel function corresponds to the input neurons of the output layer. Motivated by these insights, we propose the \textbf{zero-additional-cost} Kernel Inspired Router with Normalization (KERN), an FFN-style router function, as an alternative to $\mathrm{Softmax}$. We demonstrate that this router generalizes both $\mathrm{Sigmoid}$- and $\mathrm{Softmax}$-based routers. \textbf{Based on empirical observations and established practices in FFN implementation, we recommend the use of $\mathrm{ReLU}$ activation and $\ell_2$-normalization in $\mathrm{KERN}$ router function.} Comprehensive experiments in MoE and LLM validate the effectiveness of the proposed FFN-style router function \methodNorm.

