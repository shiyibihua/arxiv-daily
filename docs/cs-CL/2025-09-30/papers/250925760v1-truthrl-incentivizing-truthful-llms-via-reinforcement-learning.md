---
layout: default
title: TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning
---

# TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25760" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.25760v1</a>
  <a href="https://arxiv.org/pdf/2509.25760.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25760v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25760v1', 'TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhepei Wei, Xiao Yang, Kai Sun, Jiaqi Wang, Rulin Shao, Sean Chen, Mohammad Kachuee, Teja Gollapudi, Tony Liao, Nicolas Scheffer, Rakesh Wanga, Anuj Kumar, Yu Meng, Wen-tau Yih, Xin Luna Dong

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**TruthRLÔºöÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊøÄÂä±Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÊõ¥ÁúüÂÆûÂèØÈù†ÁöÑÁ≠îÊ°à**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `ÁúüÂÆûÊÄß` `ÂπªËßâ` `Áü•ËØÜÂØÜÈõÜÂûãÈóÆÁ≠î` `‰∏âÂÖÉÂ•ñÂä±` `GRPO`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Áü•ËØÜÂØÜÈõÜÂûã‰ªªÂä°‰∏≠Êòì‰∫ßÁîüÂπªËßâÔºå‰∏îÈöæ‰ª•Âπ≥Ë°°ÂáÜÁ°ÆÊÄßÂíåËØÜÂà´‰∏çÁ°ÆÂÆöÊÄß„ÄÇ
2. TruthRLÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰ΩøÁî®‰∏âÂÖÉÂ•ñÂä±ÔºàÊ≠£Á°Æ„ÄÅÂπªËßâ„ÄÅÂºÉÊùÉÔºâÁõ¥Êé•‰ºòÂåñÊ®°ÂûãÁöÑÁúüÂÆûÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåTruthRLÊòæËëóÈôç‰Ωé‰∫ÜÂπªËßâÁéáÔºà28.9%ÔºâÂπ∂ÊèêÂçá‰∫ÜÁúüÂÆûÊÄßÔºà21.1%ÔºâÔºå‰ºò‰∫é‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰∫ãÂÆûÊÄßÈóÆÁ≠îÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆπÊòì‰∫ßÁîüÂπªËßâÂíå‰∏çÁúüÂÆûÁöÑÂõûÁ≠îÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅË∂ÖÂá∫ÂÖ∂ÂèÇÊï∞Áü•ËØÜÁöÑ‰ø°ÊÅØÊó∂„ÄÇÁúüÂÆûÊÄß‰∏ç‰ªÖ‰ªÖÊòØÂáÜÁ°ÆÊÄßÔºåÊ®°ÂûãËøòÂøÖÈ°ªËØÜÂà´‰∏çÁ°ÆÂÆöÊÄßÂπ∂Âú®‰∏çÁ°ÆÂÆöÊó∂ÈÄâÊã©ÂºÉÊùÉ‰ª•ÈÅøÂÖçÂπªËßâ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈù¢‰∏¥Ê†πÊú¨ÊåëÊàòÔºö‰ºòÂåñÂáÜÁ°ÆÊÄßÁöÑÊñπÊ≥ïÈÄöÂ∏∏‰ºöÊîæÂ§ßÂπªËßâÔºåËÄåÈºìÂä±ÂºÉÊùÉÁöÑÊñπÊ≥ïÂèØËÉΩËøá‰∫é‰øùÂÆàÔºåÁâ∫Áâ≤‰∫ÜÊ≠£Á°ÆÁöÑÁ≠îÊ°à„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜTruthRLÔºå‰∏Ä‰∏™ÈÄöÁî®ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊ°ÜÊû∂ÔºåÁõ¥Êé•‰ºòÂåñLLMÁöÑÁúüÂÆûÊÄß„ÄÇTruthRL‰ΩøÁî®GRPOÂÆûÁé∞ÔºåÈááÁî®ÁÆÄÂçïËÄåÊúâÊïàÁöÑ‰∏âÂÖÉÂ•ñÂä±ÔºåÂå∫ÂàÜÊ≠£Á°ÆÁ≠îÊ°à„ÄÅÂπªËßâÂíåÂºÉÊùÉ„ÄÇÂÆÉÊøÄÂä±Ê®°ÂûãÈÄöËøáÊèê‰æõÊ≠£Á°ÆÁ≠îÊ°àÂíåÂú®‰∏çÁ°ÆÂÆöÊó∂ÈÄâÊã©ÂºÉÊùÉÊù•ÂáèÂ∞ëÂπªËßâÔºå‰ªéËÄåÊèêÈ´òÁúüÂÆûÊÄß„ÄÇÂú®Âõõ‰∏™Áü•ËØÜÂØÜÈõÜÂûãÂü∫ÂáÜÊµãËØï‰∏≠Ôºå‰∏évanilla RLÁõ∏ÊØîÔºåTruthRLÊòæËëóÂáèÂ∞ë‰∫Ü28.9%ÁöÑÂπªËßâÔºåÊèêÈ´ò‰∫Ü21.1%ÁöÑÁúüÂÆûÊÄßÔºåÂú®Ê£ÄÁ¥¢ÂíåÈùûÊ£ÄÁ¥¢ËÆæÁΩÆ‰∏ãÔºåÂêÑÁßçÈ™®Âπ≤Ê®°ÂûãÔºà‰æãÂ¶ÇÔºåQwen„ÄÅLlamaÔºâÈÉΩËé∑Âæó‰∫ÜÊåÅÁª≠ÁöÑÊî∂Áõä„ÄÇÊ∑±ÂÖ•ÁöÑÊ∂àËûçÁ†îÁ©∂Ë°®ÊòéÔºåvanillaÁöÑ‰ª•ÂáÜÁ°ÆÊÄßÈ©±Âä®ÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇÁõëÁù£ÂæÆË∞ÉÊàñÂÖ∑Êúâ‰∫åÂÖÉÂ•ñÂä±ÁöÑRLÔºåÈöæ‰ª•Âπ≥Ë°°‰∫ãÂÆûÊ≠£Á°ÆÊÄßÂíå‰∏çÁ°ÆÂÆöÊÄß„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑ‰ª•ÁúüÂÆûÊÄßÈ©±Âä®ÁöÑTruthRLÂú®ÂáÜÁ°ÆÊÄßÂíåÁúüÂÆûÊÄßÊñπÈù¢ÈÉΩÂèñÂæó‰∫ÜÂá∫Ëâ≤ÁöÑÊÄßËÉΩÔºåÁ™ÅÂá∫‰∫ÜÂ≠¶‰π†ÁõÆÊ†áËÆæËÆ°ÂØπ‰∫éÂºÄÂèëÁúüÂÆûÁöÑLLMÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Áü•ËØÜÂØÜÈõÜÂûãÈóÆÁ≠î‰ªªÂä°‰∏≠Â≠òÂú®ÁöÑÂπªËßâÈóÆÈ¢òÔºåÂç≥Ê®°ÂûãÂú®Áº∫‰πèË∂≥Â§ü‰ø°ÊÅØÁöÑÊÉÖÂÜµ‰∏ãÔºåÁîüÊàê‰∏çÁúüÂÆûÊàñ‰∏çÂáÜÁ°ÆÁöÑÁ≠îÊ°à„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÁõëÁù£ÂæÆË∞ÉÊàñ‰ΩøÁî®‰∫åÂÖÉÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÈöæ‰ª•Âú®ËøΩÊ±ÇÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÔºåÊúâÊïàËØÜÂà´ÂíåÂ§ÑÁêÜËá™Ë∫´ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåÂØºËá¥Ê®°ÂûãË¶Å‰πàËøáÂ∫¶Ëá™‰ø°Âú∞‰∫ßÁîüÂπªËßâÔºåË¶Å‰πàËøá‰∫é‰øùÂÆàÂú∞ÊîæÂºÉÂõûÁ≠î„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTruthRLÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Áõ¥Êé•‰ºòÂåñÊ®°ÂûãÁöÑÁúüÂÆûÊÄßÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂáÜÁ°ÆÊÄß„ÄÇÂÆÉÈÄöËøáÂºïÂÖ•‰∏Ä‰∏™‰∏âÂÖÉÂ•ñÂä±ÂáΩÊï∞ÔºåÊòéÁ°ÆÂå∫ÂàÜÊ≠£Á°ÆÁ≠îÊ°à„ÄÅÂπªËßâÂíåÂºÉÊùÉÔºå‰ªéËÄåÊøÄÂä±Ê®°ÂûãÂú®‰∏çÁ°ÆÂÆöÊó∂ÈÄâÊã©ÂºÉÊùÉÔºåÈÅøÂÖç‰∫ßÁîüÂπªËßâ„ÄÇËøôÁßçÊñπÊ≥ïÊó®Âú®‰ΩøÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Ê†°ÂáÜÂÖ∂ÁΩÆ‰ø°Â∫¶ÔºåÂπ∂Âú®Áü•ËØÜËæπÁïå‰πãÂ§ñË°®Áé∞Âá∫ÈÄÇÂΩìÁöÑË∞¶ÈÄä„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTruthRL‰ΩøÁî®GRPOÔºàGeneralized Proximal Policy OptimizationÔºâ‰Ωú‰∏∫Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö1) ‰ΩøÁî®LLMÁîüÊàêÁ≠îÊ°àÔºõ2) Ê†πÊçÆÁ≠îÊ°àÁöÑ‰∫ãÂÆûÊÄßÁªô‰∫à‰∏âÂÖÉÂ•ñÂä±ÔºàÊ≠£Á°Æ„ÄÅÂπªËßâ„ÄÅÂºÉÊùÉÔºâÔºõ3) ‰ΩøÁî®GRPOÊõ¥Êñ∞LLMÁöÑÁ≠ñÁï•Ôºå‰ΩøÂÖ∂Êõ¥ÂÄæÂêë‰∫éÁîüÊàêÁúüÂÆûÁ≠îÊ°àÊàñÂú®‰∏çÁ°ÆÂÆöÊó∂ÈÄâÊã©ÂºÉÊùÉ„ÄÇËØ•Ê°ÜÊû∂ÂèØ‰ª•Â∫îÁî®‰∫éÂêÑÁßçLLMÔºåÂπ∂‰∏îÂèØ‰ª•‰∏éÊ£ÄÁ¥¢Â¢ûÂº∫ÊñπÊ≥ïÁªìÂêà‰ΩøÁî®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTruthRLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂‰∏âÂÖÉÂ•ñÂä±ÂáΩÊï∞ÔºåÂÆÉÊòéÁ°ÆÂå∫ÂàÜ‰∫ÜÊ≠£Á°ÆÁ≠îÊ°à„ÄÅÂπªËßâÂíåÂºÉÊùÉ„ÄÇ‰∏é‰º†ÁªüÁöÑ‰∫åÂÖÉÂ•ñÂä±ÔºàÊ≠£Á°Æ/ÈîôËØØÔºâÁõ∏ÊØîÔºå‰∏âÂÖÉÂ•ñÂä±ËÉΩÂ§üÊõ¥ÁªÜËá¥Âú∞ÊåáÂØºÊ®°ÂûãÁöÑÂ≠¶‰π†Ôºå‰ΩøÂÖ∂‰∏ç‰ªÖÂÖ≥Ê≥®ÁîüÊàêÊ≠£Á°ÆÁ≠îÊ°àÔºåËøòÂÖ≥Ê≥®ËØÜÂà´ÂíåÈÅøÂÖçÂπªËßâ„ÄÇËøôÁßçÂ•ñÂä±Êú∫Âà∂ÈºìÂä±Ê®°ÂûãÂú®‰∏çÁ°ÆÂÆöÊó∂ÈÄâÊã©ÂºÉÊùÉÔºå‰ªéËÄåÊèêÈ´òÊï¥‰ΩìÁöÑÁúüÂÆûÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöTruthRL‰ΩøÁî®GRPOÁÆóÊ≥ïËøõË°åÁ≠ñÁï•‰ºòÂåñ„ÄÇ‰∏âÂÖÉÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆÔºåÂÖ∑‰ΩìÊï∞ÂÄºÂèØ‰ª•Ê†πÊçÆ‰ªªÂä°ÂíåÊ®°ÂûãËøõË°åË∞ÉÊï¥„ÄÇ‰æãÂ¶ÇÔºåÊ≠£Á°ÆÁ≠îÊ°àÂ•ñÂä±‰∏∫+1ÔºåÂπªËßâÂ•ñÂä±‰∏∫-1ÔºåÂºÉÊùÉÂ•ñÂä±‰∏∫0„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÁ†îÁ©∂‰∫Ü‰∏çÂêåÁöÑÂºÉÊùÉÁ≠ñÁï•Ôºå‰æãÂ¶ÇÂü∫‰∫éÊ®°ÂûãÁΩÆ‰ø°Â∫¶ÁöÑÂºÉÊùÉ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂèñÂÜ≥‰∫éÊâÄ‰ΩøÁî®ÁöÑLLM„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTruthRLÂú®Âõõ‰∏™Áü•ËØÜÂØÜÈõÜÂûãÂü∫ÂáÜÊµãËØï‰∏≠Ôºå‰∏évanilla RLÁõ∏ÊØîÔºåÊòæËëóÂáèÂ∞ë‰∫Ü28.9%ÁöÑÂπªËßâÔºåÊèêÈ´ò‰∫Ü21.1%ÁöÑÁúüÂÆûÊÄß„ÄÇÂú®QwenÂíåLlamaÁ≠â‰∏çÂêåÈ™®Âπ≤Ê®°Âûã‰∏äÔºå‰ª•ÂèäÂú®Ê£ÄÁ¥¢ÂíåÈùûÊ£ÄÁ¥¢ËÆæÁΩÆ‰∏ãÔºåTruthRLÈÉΩË°®Áé∞Âá∫‰∏ÄËá¥ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÊ∂àËûçÁ†îÁ©∂Ë°®ÊòéÔºå‰∏âÂÖÉÂ•ñÂä±ÂáΩÊï∞ÊòØTruthRLÊàêÂäüÁöÑÂÖ≥ÈîÆÔºå‰ºò‰∫é‰º†ÁªüÁöÑ‰∫åÂÖÉÂ•ñÂä±ÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

TruthRLÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈ´òÂ∫¶ÂèØÈù†ÊÄßÁöÑÁü•ËØÜÂØÜÈõÜÂûãÈóÆÁ≠îÂú∫ÊôØÔºå‰æãÂ¶ÇÂåªÁñóËØäÊñ≠„ÄÅÈáëËûçÂàÜÊûê„ÄÅÊ≥ïÂæãÂí®ËØ¢Á≠â„ÄÇÈÄöËøáÊèêÈ´òLLMÁöÑÁúüÂÆûÊÄßÔºåÂèØ‰ª•ÂáèÂ∞ëÈîôËØØ‰ø°ÊÅØÁöÑ‰º†Êí≠ÔºåÂ¢ûÂº∫Áî®Êà∑ÂØπAIÁ≥ªÁªüÁöÑ‰ø°‰ªªÔºåÂπ∂‰øÉËøõAIÂú®ÂÖ≥ÈîÆÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÁ±ªÂûãÁöÑÁîüÊàê‰ªªÂä°Ôºå‰æãÂ¶ÇÊñáÊú¨ÊëòË¶ÅÂíåÊú∫Âô®ÁøªËØë„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.

