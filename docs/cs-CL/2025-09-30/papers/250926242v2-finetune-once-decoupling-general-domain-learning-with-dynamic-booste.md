---
layout: default
title: Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing
---

# Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26242" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26242v2</a>
  <a href="https://arxiv.org/pdf/2509.26242.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26242v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26242v2', 'Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Tang, Ruijie Liu, Yifan Wang, Shiyu Li, Xi Chen

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-17)

**å¤‡æ³¨**: 9 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€å¢å¼ºé€€ç«(DBA)æ–¹æ³•ï¼Œè§£è€¦é€šç”¨å’Œé¢†åŸŸå­¦ä¹ ï¼Œé«˜æ•ˆå¾®è°ƒLLMã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `é¢†åŸŸè‡ªé€‚åº”` `æ¢¯åº¦å¢å¼º` `åŠ¨æ€æ­¥é•¿` `é€€ç«å­¦ä¹ ` `é«˜æ•ˆè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸLLMå¾®è°ƒä¾èµ–å¤æ‚æ•°æ®æ··åˆå’Œé‡å¤å®éªŒï¼Œæ³›åŒ–æ€§èƒ½éš¾ä»¥ä¿è¯ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. DBAæ–¹æ³•é€šè¿‡é€šç”¨æ•°æ®ä¸Šçš„é›¶å­¦ä¹ ç‡è®­ç»ƒè·å–å…¨å±€æ¢¯åº¦ï¼Œç”¨äºé¢†åŸŸè®­ç»ƒçš„æ¢¯åº¦å¢å¼ºå’Œæ­¥é•¿æ ¡æ­£ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDBAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ¯”ä¼ ç»Ÿå¾®è°ƒå¹³å‡æå‡5.8%çš„è”åˆæ€§èƒ½ï¼Œå¹¶å‡å°‘91%çš„GPUæ—¶é—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„å¾®è°ƒå±•ç°å‡ºå“è¶Šçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•é€šå¸¸éœ€è¦å¤æ‚çš„æ•°æ®æ··åˆå’Œé‡å¤å®éªŒæ‰èƒ½è·å¾—æœ€ä½³çš„æ³›åŒ–æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜å¹¶ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå³åŠ¨æ€å¢å¼ºé€€ç«(DBA)ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡åœ¨é€šç”¨æ•°æ®ä¸Šè¿›è¡Œé›¶å­¦ä¹ ç‡è®­ç»ƒè·å¾—å…¨å±€æ¢¯åº¦ï¼Œç„¶åå°†å…¶ç”¨äºé¢†åŸŸè®­ç»ƒæœŸé—´çš„æ¢¯åº¦å¢å¼ºå’ŒåŠ¨æ€è®­ç»ƒæ­¥é•¿æ ¡æ­£ã€‚ç»“åˆé€€ç«å­¦ä¹ ï¼Œæˆ‘ä»¬æœ€ç»ˆå»ºç«‹äº†ä¸€ä¸ªä»…ä¾èµ–äºé¢†åŸŸæ•°æ®è€Œä¸ä¼šå´©æºƒçš„å¾®è°ƒæµç¨‹ã€‚é€šè¿‡åœ¨å¤šä¸ªæµè¡ŒåŸºç¡€æ¨¡å‹ä¸Šè¯„ä¼°é€šç”¨å’Œé¢†åŸŸç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼ŒDBAåœ¨è”åˆæ€§èƒ½æ–¹é¢æ¯”ä¼ ç»Ÿå¾®è°ƒå¹³å‡æé«˜äº†5.8%ã€‚æ­¤å¤–ï¼Œç”±äºé€šç”¨æ•°æ®ä¸å†å‚ä¸é€€ç«è¿‡ç¨‹ï¼Œå› æ­¤ä¹Ÿæ¶ˆé™¤äº†ç”±æ•°æ®æ··åˆå¯¼è‡´çš„é‡å¤å®éªŒã€‚æ ¹æ®æˆ‘ä»¬çš„æµ‹è¯•ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒDBAæ–¹æ³•å¯ä»¥å‡å°‘91.0%çš„GPUä½¿ç”¨æ—¶é—´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œä¸ºäº†è¾¾åˆ°è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œé€šå¸¸éœ€è¦ç²¾ç»†çš„æ•°æ®æ··åˆç­–ç•¥å’Œå¤§é‡çš„é‡å¤å®éªŒã€‚è¿™å¯¼è‡´äº†è®­ç»ƒè¿‡ç¨‹çš„å¤æ‚æ€§å’Œè®¡ç®—èµ„æºçš„æµªè´¹ï¼ŒåŒæ—¶ä¹Ÿå¢åŠ äº†æ¨¡å‹è®­ç»ƒçš„ä¸ç¡®å®šæ€§ã€‚å¦‚ä½•é«˜æ•ˆä¸”ç¨³å®šåœ°å¾®è°ƒLLMï¼Œé¿å…å¤æ‚çš„æ•°æ®æ··åˆå’Œé‡å¤å®éªŒï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é€šç”¨çŸ¥è¯†çš„å­¦ä¹ å’Œé¢†åŸŸçŸ¥è¯†çš„å­¦ä¹ è§£è€¦ã€‚é¦–å…ˆï¼Œåˆ©ç”¨é€šç”¨æ•°æ®æå–å…¨å±€æ¢¯åº¦ä¿¡æ¯ï¼Œç„¶åå°†å…¶ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼ŒæŒ‡å¯¼é¢†åŸŸæ•°æ®çš„å¾®è°ƒè¿‡ç¨‹ã€‚é€šè¿‡æ¢¯åº¦å¢å¼ºå’ŒåŠ¨æ€æ­¥é•¿æ ¡æ­£ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¿«ã€æ›´ç¨³å®šåœ°é€‚åº”é¢†åŸŸæ•°æ®ï¼ŒåŒæ—¶é¿å…äº†ç¾éš¾æ€§é—å¿˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDBAæ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š1) **å…¨å±€æ¢¯åº¦æå–é˜¶æ®µ**ï¼šåœ¨é€šç”¨æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨é›¶å­¦ä¹ ç‡è¿›è¡Œè®­ç»ƒï¼Œæå–æ¨¡å‹çš„å…¨å±€æ¢¯åº¦ä¿¡æ¯ã€‚è¿™ä¸ªæ¢¯åº¦ä»£è¡¨äº†æ¨¡å‹å¯¹é€šç”¨çŸ¥è¯†çš„ç†è§£ã€‚2) **é¢†åŸŸå¾®è°ƒé˜¶æ®µ**ï¼šåœ¨é¢†åŸŸæ•°æ®é›†ä¸Šï¼Œåˆ©ç”¨æå–çš„å…¨å±€æ¢¯åº¦è¿›è¡Œæ¢¯åº¦å¢å¼ºï¼Œå¹¶åŠ¨æ€è°ƒæ•´è®­ç»ƒæ­¥é•¿ã€‚åŒæ—¶ï¼Œç»“åˆé€€ç«å­¦ä¹ ç‡ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€æ­¥é€‚åº”é¢†åŸŸæ•°æ®ï¼Œæœ€ç»ˆè¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šDBAæ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºè§£è€¦äº†é€šç”¨çŸ¥è¯†å­¦ä¹ å’Œé¢†åŸŸçŸ¥è¯†å­¦ä¹ ï¼Œå¹¶åˆ©ç”¨å…¨å±€æ¢¯åº¦ä½œä¸ºæ¡¥æ¢ï¼Œå°†ä¸¤è€…è”ç³»èµ·æ¥ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒDBAæ–¹æ³•ä¸éœ€è¦å¤æ‚çš„æ•°æ®æ··åˆï¼Œé¿å…äº†é‡å¤å®éªŒï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€æ­¥é•¿æ ¡æ­£æœºåˆ¶èƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶è®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…æ¨¡å‹å´©æºƒã€‚

**å…³é”®è®¾è®¡**ï¼šDBAæ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **é›¶å­¦ä¹ ç‡è®­ç»ƒ**ï¼šä½¿ç”¨é›¶å­¦ä¹ ç‡è®­ç»ƒé€šç”¨æ•°æ®ï¼Œæ˜¯ä¸ºäº†æå–æ¨¡å‹çš„å…¨å±€æ¢¯åº¦ä¿¡æ¯ï¼Œè€Œä¸æ˜¯è®©æ¨¡å‹è®°ä½é€šç”¨æ•°æ®ã€‚2) **æ¢¯åº¦å¢å¼º**ï¼šå°†å…¨å±€æ¢¯åº¦ä¸é¢†åŸŸæ•°æ®çš„æ¢¯åº¦è¿›è¡Œèåˆï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨é€šç”¨çŸ¥è¯†ã€‚3) **åŠ¨æ€æ­¥é•¿æ ¡æ­£**ï¼šæ ¹æ®å…¨å±€æ¢¯åº¦å’Œé¢†åŸŸæ•°æ®çš„æ¢¯åº¦ä¹‹é—´çš„å·®å¼‚ï¼ŒåŠ¨æ€è°ƒæ•´è®­ç»ƒæ­¥é•¿ï¼Œä»¥ä¿è¯è®­ç»ƒçš„ç¨³å®šæ€§ã€‚4) **é€€ç«å­¦ä¹ ç‡**ï¼šé‡‡ç”¨é€€ç«å­¦ä¹ ç‡ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€æ­¥é€‚åº”é¢†åŸŸæ•°æ®ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDBAæ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œå¹³å‡æå‡äº†5.8%çš„è”åˆæ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒDBAæ–¹æ³•èƒ½å¤Ÿå‡å°‘91.0%çš„GPUä½¿ç”¨æ—¶é—´ï¼Œæå¤§åœ°æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚è¿™äº›ç»“æœå……åˆ†è¯æ˜äº†DBAæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DBAæ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ç³»ç»Ÿã€æƒ…æ„Ÿåˆ†æç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½å¾®è°ƒæˆæœ¬ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ï¼ŒåŠ é€ŸLLMåœ¨å„è¡Œä¸šçš„è½åœ°åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæ½œåœ¨çš„å•†ä¸šå‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) fine-tuning shows excellent implications. However, vanilla fine-tuning methods often require intricate data mixture and repeated experiments for optimal generalization. To address these challenges and streamline the training process, we propose an efficient and universal solution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through zero-learning-rate training on general data, which is subsequently employed for gradient boosting and dynamic training step correction during domain training. In conjunction with annealing learning, we end up establishing a fine-tuning pipeline that relies solely on domain data without collapse. By evaluating both general and domain-specific performance across multiple tasks on several popular base models, DBA achieves an average improvement of 5.8% in joint performance over vanilla fine-tuning. Furthermore, since general data is no longer involved in annealing, repeated experiments led by data mixture are also eliminated. According to our tests, the DBA method can reduce GPU hours by 91.0% compared to the vanilla method.

