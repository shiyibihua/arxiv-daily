---
layout: default
title: TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding
---

# TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00161" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00161v1</a>
  <a href="https://arxiv.org/pdf/2510.00161.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00161v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00161v1', 'TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kimihiro Hasegawa, Wiradee Imrattanatrai, Masaki Asada, Ken Fukuda, Teruko Mitamura

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 21 pages. Code: https://github.com/kimihiroh/tama

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTAMAï¼šå·¥å…·å¢å¼ºçš„å¤šæ¨¡æ€Agentï¼Œç”¨äºç¨‹åºæ€§æ´»åŠ¨ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€Agent` `ç¨‹åºæ€§æ´»åŠ¨ç†è§£` `å·¥å…·å¢å¼º` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤šåª’ä½“æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¨‹åºæ€§æ´»åŠ¨åŠ©æ‰‹åœ¨æ—¥å¸¸å’Œä¸“ä¸šåœºæ™¯ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†é’ˆå¯¹æ­¤ç±»åŠ©æ‰‹çš„ç³»ç»Ÿå¼€å‘ä»æœ‰å¾…æ¢ç´¢ã€‚
2. TAMAæ¡†æ¶åˆ©ç”¨å¤šåª’ä½“è¿”å›å·¥å…·ï¼Œå®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„äº¤é”™æ¨ç†ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTAMAèƒ½æœ‰æ•ˆæå‡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç¨‹åºæ€§é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶éªŒè¯äº†å…³é”®è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTAMAï¼ˆTool-Augmented Multimodal Agentï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºç¨‹åºæ€§æ´»åŠ¨ç†è§£ã€‚TAMAé€šè¿‡åˆ©ç”¨å¤šåª’ä½“è¿”å›å·¥å…·ï¼Œå®ç°äº†å…è®­ç»ƒè®¾ç½®ä¸‹çš„äº¤é”™å¤šæ¨¡æ€æ¨ç†ã€‚åœ¨å¤šæ¨¡æ€ç¨‹åºæ€§é—®ç­”æ•°æ®é›†ProMQA-Assemblyä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯GPT-5å’ŒMiMo-VLã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶ä¸ºè¯¥æ¡†æ¶çš„ä¸¤ä¸ªå…³é”®ç‰¹å¾æä¾›äº†ç»éªŒæ”¯æŒï¼Œå³å¤šåª’ä½“è¿”å›å·¥å…·å’ŒAgentçµæ´»çš„å·¥å…·é€‰æ‹©ã€‚æˆ‘ä»¬ç›¸ä¿¡æå‡ºçš„æ¡†æ¶å’Œå®éªŒç»“æœå°†ä¿ƒè¿›è§†é¢‘å’Œå¤šæ¨¡æ€ä»»åŠ¡çš„å›¾åƒæ€è€ƒèŒƒå¼ï¼Œå¹¶æ¨åŠ¨ç¨‹åºæ€§æ´»åŠ¨åŠ©æ‰‹çš„å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¨‹åºæ€§æ´»åŠ¨ç†è§£é—®é¢˜ï¼Œå³å¦‚ä½•è®©AIç³»ç»Ÿç†è§£å¹¶è¾…åŠ©äººç±»å®Œæˆä¸€ç³»åˆ—æ­¥éª¤åŒ–çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ç»„è£…å®¶å…·æˆ–è¿›è¡Œå®éªŒã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæˆ–è€…éš¾ä»¥æœ‰æ•ˆåœ°æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚è§†é¢‘å’Œæ–‡æœ¬ï¼‰ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨â€œå·¥å…·â€æ¥å¢å¼ºå¤šæ¨¡æ€Agentçš„èƒ½åŠ›ã€‚è¿™é‡Œçš„â€œå·¥å…·â€æŒ‡çš„æ˜¯èƒ½å¤Ÿè¿”å›å¤šåª’ä½“ä¿¡æ¯ï¼ˆä¾‹å¦‚å›¾åƒã€è§†é¢‘ï¼‰çš„å¤–éƒ¨æ¨¡å—ã€‚é€šè¿‡è®©Agentåœ¨æ¨ç†è¿‡ç¨‹ä¸­çµæ´»åœ°é€‰æ‹©å’Œä½¿ç”¨è¿™äº›å·¥å…·ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è·å–å’Œæ•´åˆç›¸å…³ä¿¡æ¯ï¼Œä»è€Œæé«˜ç¨‹åºæ€§æ´»åŠ¨ç†è§£çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¿™ç§è®¾è®¡æ¨¡ä»¿äº†äººç±»åœ¨è§£å†³é—®é¢˜æ—¶ä¼šæŸ¥é˜…èµ„æ–™ã€ä½¿ç”¨å·¥å…·çš„ä¹ æƒ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTAMAæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **å¤šæ¨¡æ€è¾“å…¥**ï¼šæ¥æ”¶è§†é¢‘ã€æ–‡æœ¬ç­‰å¤šç§æ¨¡æ€çš„è¾“å…¥ä¿¡æ¯ã€‚2) **Agent**ï¼šè´Ÿè´£æ ¹æ®å½“å‰çŠ¶æ€å’Œç›®æ ‡ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚3) **å·¥å…·é›†**ï¼šåŒ…å«å„ç§å¤šåª’ä½“è¿”å›å·¥å…·ï¼Œä¾‹å¦‚å›¾åƒæ£€ç´¢ã€è§†é¢‘åˆ†æç­‰ã€‚4) **æ¨ç†å¼•æ“**ï¼šåˆ©ç”¨Agenté€‰æ‹©çš„å·¥å…·ï¼Œå¯¹è¾“å…¥ä¿¡æ¯è¿›è¡Œå¤„ç†å’Œæ¨ç†ï¼Œç”Ÿæˆæœ€ç»ˆçš„ç­”æ¡ˆæˆ–è¡ŒåŠ¨æŒ‡ä»¤ã€‚æ•´ä¸ªæµç¨‹æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼ŒAgentä¼šæ ¹æ®æ¨ç†ç»“æœä¸æ–­è°ƒæ•´å·¥å…·é€‰æ‹©ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šTAMAçš„å…³é”®åˆ›æ–°åœ¨äºå°†â€œå·¥å…·â€çš„æ¦‚å¿µå¼•å…¥åˆ°å¤šæ¨¡æ€Agentä¸­ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§çµæ´»çš„å·¥å…·é€‰æ‹©æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯æ¨¡å‹ç›¸æ¯”ï¼ŒTAMAå…·æœ‰æ›´å¼ºçš„å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§ã€‚é€šè¿‡å¢åŠ æ–°çš„å·¥å…·ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•TAMAçš„èƒ½åŠ›ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒTAMAçš„å…è®­ç»ƒç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°çš„ä»»åŠ¡å’Œç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šTAMAæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **å·¥å…·çš„å®šä¹‰**ï¼šæ¯ä¸ªå·¥å…·éƒ½å®šä¹‰äº†è¾“å…¥å’Œè¾“å‡ºçš„æ ¼å¼ï¼Œä»¥åŠå…¶åŠŸèƒ½æè¿°ã€‚2) **Agentçš„å†³ç­–ç­–ç•¥**ï¼šAgentéœ€è¦æ ¹æ®å½“å‰çŠ¶æ€å’Œç›®æ ‡ï¼Œé€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ã€‚è¿™å¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ æˆ–å…¶ä»–å†³ç­–ç®—æ³•æ¥å®ç°ã€‚3) **å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆ**ï¼šTAMAéœ€è¦æœ‰æ•ˆåœ°èåˆæ¥è‡ªä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼Œä¾‹å¦‚è§†é¢‘å¸§ã€æ–‡æœ¬æè¿°å’Œå·¥å…·è¿”å›çš„å¤šåª’ä½“ä¿¡æ¯ã€‚è¿™å¯ä»¥é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æˆ–å…¶ä»–èåˆæ–¹æ³•æ¥å®ç°ã€‚è®ºæ–‡ä¸­å…·ä½“ä½¿ç”¨çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ProMQA-Assemblyæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTAMAæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒTAMAèƒ½å¤Ÿæå‡GPT-5å’ŒMiMo-VLç­‰æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å¤šåª’ä½“è¿”å›å·¥å…·å’ŒAgentçµæ´»çš„å·¥å…·é€‰æ‹©æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒTAMAæ˜¯ä¸€ç§æœ‰æ•ˆçš„ç¨‹åºæ€§æ´»åŠ¨ç†è§£æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TAMAæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼šæ™ºèƒ½å®¶å±…åŠ©æ‰‹ï¼ˆè¾…åŠ©ç”¨æˆ·å®Œæˆçƒ¹é¥ªã€ç»´ä¿®ç­‰ä»»åŠ¡ï¼‰ã€å·¥ä¸šæœºå™¨äººï¼ˆæŒ‡å¯¼å·¥äººè¿›è¡Œè£…é…ã€ç»´æŠ¤ç­‰æ“ä½œï¼‰ã€åŒ»ç–—è¾…åŠ©ç³»ç»Ÿï¼ˆå¸®åŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­ã€æ‰‹æœ¯ç­‰ï¼‰ã€‚é€šè¿‡ä¸å„ç§å·¥å…·çš„é›†æˆï¼ŒTAMAå¯ä»¥é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚æœªæ¥ï¼ŒTAMAæœ‰æœ›æˆä¸ºäººæœºåä½œçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Procedural activity assistants potentially support humans in a variety of settings, from our daily lives, e.g., cooking or assembling flat-pack furniture, to professional situations, e.g., manufacturing or biological experiments. Despite its potential use cases, the system development tailored for such an assistant is still underexplored. In this paper, we propose a novel framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural activity understanding. TAMA enables interleaved multimodal reasoning by making use of multimedia-returning tools in a training-free setting. Our experimental result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our approach can improve the performance of vision-language models, especially GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support for the effectiveness of two features that characterize our framework, multimedia-returning tools and agentic flexible tool selection. We believe our proposed framework and experimental results facilitate the thinking with images paradigm for video and multimodal tasks, let alone the development of procedural activity assistants.

