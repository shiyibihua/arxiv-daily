---
layout: default
title: Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities
---

# Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25725" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25725v1</a>
  <a href="https://arxiv.org/pdf/2509.25725.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25725v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25725v1', 'Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiayi Kuang, Haojing Huang, Yinghui Li, Xinnian Liang, Zhikun Xu, Yangning Li, Xiaoyu Tan, Chao Qu, Meishan Zhang, Ying Shen, Philip S. Yu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°å­¦åŸå­èƒ½åŠ›è§£è€¦æ–¹æ³•ï¼Œæ¢ç´¢å¤§è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›çš„æœ¬è´¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ•°å­¦æ¨ç†` `åŸå­èƒ½åŠ›` `è§£è€¦` `è®¤çŸ¥å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ•°å­¦æ¨ç†ä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œéš¾ä»¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦çœŸæ­£ç†è§£æ•°å­¦æ¦‚å¿µï¼Œå­˜åœ¨è¿‡æ‹Ÿåˆé£é™©ã€‚
2. å—äººç±»è§£å†³é—®é¢˜æ–¹å¼å¯å‘ï¼Œå°†æ•°å­¦èƒ½åŠ›è§£è€¦ä¸ºåŸå­èƒ½åŠ›ï¼Œä»é¢†åŸŸå’Œé€»è¾‘ä¸¤ä¸ªç»´åº¦è¿›è¡Œåˆ’åˆ†ã€‚
3. æ„å»ºåŸå­èƒ½åŠ›æ•°æ®é›†ï¼Œå®éªŒåˆ†æä¸åŒåŸå­èƒ½åŠ›é—´çš„ç›¸äº’å½±å“ï¼ŒæŒ‡å¯¼æ›´æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºç›®å‰çš„å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ä¸»è¦ä¾èµ–äºé€šè¿‡åŒ…å«å„ç§æ•°å­¦é—®é¢˜å’Œé•¿æ€è€ƒé“¾çš„è®­ç»ƒæ•°æ®é›†è¿›è¡Œæ‰©å±•ï¼Œè¿™å¼•å‘äº†å…³äºLLMsæ˜¯å¦çœŸæ­£è·å¾—äº†æ•°å­¦æ¦‚å¿µå’Œæ¨ç†åŸåˆ™ï¼Œè¿˜æ˜¯ä»…ä»…è®°ä½äº†è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å€¾å‘äºå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªåŸºæœ¬åŸå­èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼æ¥è¯„ä¼°æ•°å­¦åŸå­èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œå°†åŸå­èƒ½åŠ›åˆ†ä¸ºä¸¤ä¸ªç»´åº¦ï¼š(1)è·¨è¶Šå››ä¸ªä¸»è¦æ•°å­¦é¢†åŸŸ(ä»£æ•°ã€å‡ ä½•ã€åˆ†æå’Œæ‹“æ‰‘)çš„ç‰¹å®šé¢†åŸŸèƒ½åŠ›ï¼Œä»¥åŠ(2)ä¸åŒçº§åˆ«çš„é€»è¾‘èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¦‚å¿µç†è§£ã€ä½¿ç”¨å½¢å¼æ•°å­¦è¯­è¨€çš„å‰å‘å¤šæ­¥æ¨ç†ä»¥åŠåä¾‹é©±åŠ¨çš„åå‘æ¨ç†ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªåŸå­èƒ½åŠ›å•å…ƒæå‡ºäº†ç›¸åº”çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç ”ç©¶ä¸åŒçš„åŸå­èƒ½åŠ›å¦‚ä½•ç›¸äº’å½±å“ï¼Œä»¥æ¢ç´¢å¼•å‘æ‰€éœ€ç‰¹å®šåŸå­èƒ½åŠ›çš„ç­–ç•¥ã€‚å¯¹é«˜çº§æ¨¡å‹çš„è¯„ä¼°å’Œå®éªŒç»“æœæ˜¾ç¤ºäº†å…³äºæ¨¡å‹åœ¨å„ç§åŸå­èƒ½åŠ›ä¸Šçš„ä¸åŒè¡¨ç°ä»¥åŠåŸå­èƒ½åŠ›ä¹‹é—´ç›¸äº’ä½œç”¨çš„è®¸å¤šæœ‰è¶£çš„å‘ç°å’Œå¯å‘ã€‚æˆ‘ä»¬çš„å‘ç°å¼ºè°ƒäº†å°†æ•°å­¦æ™ºèƒ½è§£è€¦ä¸ºåŸå­ç»„æˆéƒ¨åˆ†çš„é‡è¦æ€§ï¼Œä¸ºæ¨¡å‹è®¤çŸ¥æä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶æŒ‡å¯¼è®­ç»ƒç­–ç•¥æœç€æ›´é«˜æ•ˆã€å¯è½¬ç§»å’Œè®¤çŸ¥åŸºç¡€çš„â€œåŸå­æ€ç»´â€èŒƒå¼å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¤§è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒã€‚è¿™ç§æ–¹æ³•çš„ä¸»è¦ç—›ç‚¹åœ¨äºï¼Œæˆ‘ä»¬æ— æ³•ç¡®å®šæ¨¡å‹æ˜¯å¦çœŸæ­£ç†è§£äº†æ•°å­¦æ¦‚å¿µå’Œæ¨ç†åŸåˆ™ï¼Œè¿˜æ˜¯ä»…ä»…è®°ä½äº†è®­ç»ƒæ•°æ®ã€‚è¿™å¯¼è‡´æ¨¡å‹å¯èƒ½åœ¨è®­ç»ƒæ•°æ®åˆ†å¸ƒä¹‹å¤–è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤æ‚çš„æ•°å­¦æ¨ç†èƒ½åŠ›åˆ†è§£ä¸ºæ›´å°çš„ã€æ›´åŸºæœ¬çš„â€œåŸå­èƒ½åŠ›â€ã€‚è¿™ç§åˆ†è§£æ¨¡ä»¿äº†äººç±»è§£å†³æ•°å­¦é—®é¢˜çš„æ–¹å¼ï¼Œå³é¦–å…ˆç†è§£åŸºæœ¬æ¦‚å¿µï¼Œç„¶åé€æ­¥åº”ç”¨è¿™äº›æ¦‚å¿µè¿›è¡Œæ¨ç†ã€‚é€šè¿‡å…³æ³¨è¿™äº›åŸå­èƒ½åŠ›ï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´ç²¾ç¡®åœ°è¯„ä¼°å’Œæå‡æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) **åŸå­èƒ½åŠ›åˆ†ç±»**ï¼šå°†æ•°å­¦èƒ½åŠ›åˆ†ä¸ºä¸¤ä¸ªç»´åº¦ï¼šé¢†åŸŸç‰¹å®šèƒ½åŠ›ï¼ˆä»£æ•°ã€å‡ ä½•ã€åˆ†æã€æ‹“æ‰‘ï¼‰å’Œé€»è¾‘èƒ½åŠ›ï¼ˆæ¦‚å¿µç†è§£ã€å‰å‘å¤šæ­¥æ¨ç†ã€åä¾‹é©±åŠ¨çš„åå‘æ¨ç†ï¼‰ã€‚2) **æ•°æ®é›†æ„å»º**ï¼šä¸ºæ¯ä¸ªåŸå­èƒ½åŠ›å•å…ƒæ„å»ºç›¸åº”çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†æ—¨åœ¨æµ‹è¯•æ¨¡å‹åœ¨ç‰¹å®šåŸå­èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚3) **å®éªŒè¯„ä¼°**ï¼šé€šè¿‡å®éªŒè¯„ä¼°ä¸åŒåŸå­èƒ½åŠ›ä¹‹é—´çš„ç›¸äº’å½±å“ï¼Œå¹¶æ¢ç´¢å¦‚ä½•æœ‰æ•ˆåœ°æ¿€å‘æ‰€éœ€çš„ç‰¹å®šåŸå­èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†â€œåŸå­æ€ç»´â€çš„èŒƒå¼ï¼Œå³å°†å¤æ‚çš„æ•°å­¦æ¨ç†èƒ½åŠ›è§£è€¦ä¸ºåŸå­èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•ï¼ˆä¸»è¦ä¾èµ–äºå¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒï¼‰çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œå®ƒæ›´åŠ å…³æ³¨æ¨¡å‹å¯¹åŸºæœ¬æ•°å­¦æ¦‚å¿µçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„åŸ¹å…»ï¼Œè€Œä¸æ˜¯ç®€å•åœ°è®°å¿†è®­ç»ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **åŸå­èƒ½åŠ›çš„ç»†ç²’åº¦åˆ’åˆ†**ï¼šç¡®ä¿æ¯ä¸ªåŸå­èƒ½åŠ›å•å…ƒè¶³å¤Ÿå°ï¼Œä»¥ä¾¿èƒ½å¤Ÿç²¾ç¡®è¯„ä¼°æ¨¡å‹åœ¨è¯¥èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚2) **æ•°æ®é›†çš„å¤šæ ·æ€§**ï¼šä¸ºæ¯ä¸ªåŸå­èƒ½åŠ›å•å…ƒæ„å»ºå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œä»¥é¿å…æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç‰¹å®šç±»å‹çš„é—®é¢˜ã€‚3) **å®éªŒè®¾è®¡çš„ä¸¥è°¨æ€§**ï¼šé€šè¿‡æ§åˆ¶å˜é‡ç­‰æ–¹æ³•ï¼Œç¡®ä¿å®éªŒç»“æœèƒ½å¤Ÿå‡†ç¡®åæ˜ ä¸åŒåŸå­èƒ½åŠ›ä¹‹é—´çš„ç›¸äº’å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç ”ç©¶é€šè¿‡å®éªŒå‘ç°ï¼Œä¸åŒåŸå­èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¤æ‚çš„ç›¸äº’ä½œç”¨ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹åœ¨æ¦‚å¿µç†è§£æ–¹é¢çš„èƒ½åŠ›ä¼šå½±å“å…¶å‰å‘æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œé’ˆå¯¹ç‰¹å®šåŸå­èƒ½åŠ›çš„è®­ç»ƒå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨è¯¥èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œå¹¶å¯èƒ½å¯¹å…¶ä»–ç›¸å…³èƒ½åŠ›äº§ç”Ÿç§¯æå½±å“ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†å®éªŒç»“æœè¡¨æ˜åŸå­èƒ½åŠ›è§£è€¦çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦ã€ç§‘å­¦ã€å·¥ç¨‹ç­‰é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åŸå­èƒ½åŠ›çš„è§£è€¦å’Œé’ˆå¯¹æ€§è®­ç»ƒï¼Œå¯ä»¥å¼€å‘å‡ºæ›´å¯é ã€æ›´å…·æ³›åŒ–èƒ½åŠ›çš„AIç³»ç»Ÿï¼Œåº”ç”¨äºè‡ªåŠ¨åŒ–å®šç†è¯æ˜ã€ç§‘å­¦å‘ç°ã€æ™ºèƒ½æ•™è‚²ç­‰åœºæ™¯ï¼Œå¹¶ä¿ƒè¿›é€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated outstanding performance in mathematical reasoning capabilities. However, we argue that current large-scale reasoning models primarily rely on scaling up training datasets with diverse mathematical problems and long thinking chains, which raises questions about whether LLMs genuinely acquire mathematical concepts and reasoning principles or merely remember the training data. In contrast, humans tend to break down complex problems into multiple fundamental atomic capabilities. Inspired by this, we propose a new paradigm for evaluating mathematical atomic capabilities. Our work categorizes atomic abilities into two dimensions: (1) field-specific abilities across four major mathematical fields, algebra, geometry, analysis, and topology, and (2) logical abilities at different levels, including conceptual understanding, forward multi-step reasoning with formal math language, and counterexample-driven backward reasoning. We propose corresponding training and evaluation datasets for each atomic capability unit, and conduct extensive experiments about how different atomic capabilities influence others, to explore the strategies to elicit the required specific atomic capability. Evaluation and experimental results on advanced models show many interesting discoveries and inspirations about the different performances of models on various atomic capabilities and the interactions between atomic capabilities. Our findings highlight the importance of decoupling mathematical intelligence into atomic components, providing new insights into model cognition and guiding the development of training strategies toward a more efficient, transferable, and cognitively grounded paradigm of "atomic thinking".

