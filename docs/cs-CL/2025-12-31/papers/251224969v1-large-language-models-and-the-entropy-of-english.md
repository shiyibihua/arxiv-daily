---
layout: default
title: Large language models and the entropy of English
---

# Large language models and the entropy of English

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24969" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24969v1</a>
  <a href="https://arxiv.org/pdf/2512.24969.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24969v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24969v1', 'Large language models and the entropy of English')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Colin Scheibner, Lindsay M. Smith, William Bialek

**åˆ†ç±»**: cond-mat.stat-mech, cs.CL, physics.bio-ph, q-bio.NC

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

**å¤‡æ³¨**: 8 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ­ç¤ºè‹±è¯­æ–‡æœ¬çš„é•¿ç¨‹ç»“æ„**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¡ä»¶ç†µ` `é•¿ç¨‹ä¾èµ–` `æ–‡æœ¬åˆ†æ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œéš¾ä»¥æ•æ‰å­—ç¬¦ä¹‹é—´çš„é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡å¤§è¯­è¨€æ¨¡å‹åˆ†ææ–‡æœ¬çš„æ¡ä»¶ç†µï¼Œæ­ç¤ºé•¿ç¨‹ç»“æ„çš„å­˜åœ¨åŠå…¶å­¦ä¹ è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œç¼–ç é•¿åº¦æŒç»­å‡å°‘ï¼Œæ˜¾ç¤ºå‡ºå­—ç¬¦é—´çš„æ˜¾è‘—ç›¸å…³æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­ç¤ºæ¥è‡ªå¤šç§æ¥æºçš„è‹±è¯­æ–‡æœ¬ä¸­çš„é•¿ç¨‹ç»“æ„ã€‚æ¡ä»¶ç†µæˆ–ç¼–ç é•¿åº¦åœ¨è®¸å¤šæƒ…å†µä¸‹éšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ è€ŒæŒç»­å‡å°‘ï¼Œè‡³å°‘è¾¾åˆ°$N	ext{âˆ¼}10^4$ä¸ªå­—ç¬¦ï¼Œè¿™è¡¨æ˜åœ¨è¿™äº›è·ç¦»ä¸Šå­˜åœ¨ç›´æ¥çš„ä¾èµ–å…³ç³»æˆ–äº¤äº’ã€‚æˆ‘ä»¬ä»æ•°æ®ä¸­ç‹¬ç«‹äºæ¨¡å‹å±•ç¤ºäº†å­—ç¬¦ä¹‹é—´çš„å°ä½†æ˜¾è‘—çš„ç›¸å…³æ€§ã€‚ç¼–ç é•¿åº¦çš„åˆ†å¸ƒæ­ç¤ºäº†å¯¹å¤§é‡å­—ç¬¦çš„é€æ¸å¢å¼ºçš„ç¡®å®šæ€§ã€‚æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é•¿çŸ­ä¸Šä¸‹æ–‡é•¿åº¦çš„åŠ¨æ€å·®å¼‚ï¼Œè¡¨æ˜é•¿ç¨‹ç»“æ„çš„å­¦ä¹ æ˜¯é€æ­¥è¿›è¡Œçš„ã€‚æˆ‘ä»¬çš„ç»“æœä¸ºæ„å»ºLLMsæˆ–è¯­è¨€æœ¬èº«çš„ç»Ÿè®¡ç‰©ç†æ¨¡å‹æä¾›äº†çº¦æŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨æ­ç¤ºè‹±è¯­æ–‡æœ¬ä¸­çš„é•¿ç¨‹ç»“æ„ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶éš¾ä»¥æ•æ‰å­—ç¬¦ä¹‹é—´çš„é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹åˆ†ææ–‡æœ¬çš„æ¡ä»¶ç†µï¼Œç ”ç©¶å­—ç¬¦é—´çš„ä¾èµ–å…³ç³»ï¼Œå±•ç¤ºé•¿ç¨‹ç»“æ„çš„å­˜åœ¨åŠå…¶å­¦ä¹ è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒå’Œæ¡ä»¶ç†µè®¡ç®—ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆæ”¶é›†å¤šç§æ¥æºçš„è‹±è¯­æ–‡æœ¬æ•°æ®ï¼Œç„¶åè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œæœ€åè®¡ç®—ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦ä¸‹çš„æ¡ä»¶ç†µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡æ¡ä»¶ç†µçš„åˆ†ææ­ç¤ºäº†é•¿ç¨‹ä¾èµ–å…³ç³»çš„å­˜åœ¨ï¼Œä¸”å±•ç¤ºäº†é•¿ç¨‹ç»“æ„çš„å­¦ä¹ æ˜¯ä¸€ä¸ªæ¸è¿›çš„è¿‡ç¨‹ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„çŸ­æœŸä¾èµ–å‡è®¾å½¢æˆå¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†é€‚å½“çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼Œå¹¶é€šè¿‡è°ƒæ•´å­¦ä¹ ç‡å’ŒæŸå¤±å‡½æ•°ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œä»¥ç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é•¿ç¨‹ä¾èµ–ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24969v1/figures/C4_models.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24969v1/figures/GernreFigureV3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24969v1/figures/histogramsV3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œæ¡ä»¶ç†µæŒç»­é™ä½ï¼Œè¡¨æ˜é•¿ç¨‹ä¾èµ–å…³ç³»çš„å­˜åœ¨ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨$N	ext{âˆ¼}10^4$å­—ç¬¦çš„æƒ…å†µä¸‹ï¼Œç¼–ç é•¿åº¦æ˜¾è‘—å‡å°‘ï¼Œå±•ç¤ºäº†å­—ç¬¦é—´çš„å°ä½†æ˜¾è‘—çš„ç›¸å…³æ€§ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£è¯­è¨€æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡æ­ç¤ºé•¿ç¨‹ç»“æ„çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¯ä»¥ä¸ºæ”¹è¿›ç°æœ‰è¯­è¨€æ¨¡å‹æä¾›ç†è®ºä¾æ®ï¼Œè¿›è€Œæå‡æ¨¡å‹åœ¨å¤æ‚æ–‡æœ¬å¤„ç†ä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼Œç ”ç©¶ç»“æœå¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹è®¾è®¡å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.

