---
layout: default
title: Do Large Language Models Know What They Are Capable Of?
---

# Do Large Language Models Know What They Are Capable Of?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24661" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24661v1</a>
  <a href="https://arxiv.org/pdf/2512.24661.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24661v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24661v1', 'Do Large Language Models Know What They Are Capable Of?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Casey O. Barkan, Sid Black, Oliver Sourbut

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

**å¤‡æ³¨**: 23 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘èƒ½åŠ›è®¤çŸ¥ä¸å†³ç­–æ”¹è¿›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘è®¤çŸ¥` `å†³ç­–æ”¹è¿›` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å¤šæ­¥éª¤ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»»åŠ¡æˆåŠŸé¢„æµ‹ä¸Šæ™®éè¿‡åº¦è‡ªä¿¡ï¼Œç¼ºä¹å¯¹è‡ªèº«èƒ½åŠ›çš„å‡†ç¡®è¯„ä¼°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç ”ç©¶é€šè¿‡å¤šæ­¥éª¤ä»»åŠ¡å’Œä¸Šä¸‹æ–‡ç»éªŒï¼Œè¯„ä¼°LLMsçš„è‡ªæˆ‘è®¤çŸ¥ä¸å†³ç­–èƒ½åŠ›ï¼Œæ¢ç´¢æ”¹è¿›ç­–ç•¥ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå‘ç°éƒ¨åˆ†LLMsåœ¨ç»å†å¤±è´¥åèƒ½å¤Ÿæ”¹å–„å†³ç­–ï¼Œä½†æ•´ä½“ä»è¡¨ç°å‡ºè¿‡åº¦ä¹è§‚çš„å€¾å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦èƒ½å¤Ÿé¢„æµ‹å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ï¼Œä»¥åŠåœ¨å¤šæ­¥éª¤ä»»åŠ¡ä¸­éšç€è¿›å±•å…¶é¢„æµ‹èƒ½åŠ›æ˜¯å¦æœ‰æ‰€æå‡ã€‚ç ”ç©¶å‘ç°ï¼Œæ‰€æœ‰æµ‹è¯•çš„LLMsæ™®éè¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹åœ¨æˆåŠŸé¢„æµ‹ä¸Šå…·æœ‰ä¼˜äºéšæœºçš„åˆ¤åˆ«èƒ½åŠ›ã€‚å°½ç®¡è¾ƒæ–°å’Œè¾ƒå¤§çš„LLMsé€šå¸¸æ²¡æœ‰æ›´å¼ºçš„åˆ¤åˆ«èƒ½åŠ›ï¼Œä½†Claudeæ¨¡å‹æ˜¾ç¤ºå‡ºè¿™ä¸€è¶‹åŠ¿ã€‚åœ¨å¤šæ­¥éª¤ä»»åŠ¡ä¸­ï¼Œéƒ¨åˆ†å‰æ²¿LLMsçš„è¿‡åº¦è‡ªä¿¡éšç€ä»»åŠ¡è¿›å±•è€ŒåŠ å‰§ï¼Œè€Œæ¨ç†å‹LLMsçš„è¡¨ç°ä¸éæ¨ç†å‹LLMsç›¸å½“æˆ–æ›´å·®ã€‚éƒ¨åˆ†LLMsåœ¨ç»å†å¤±è´¥çš„ä¸Šä¸‹æ–‡ç»éªŒåèƒ½å¤Ÿå‡å°‘è¿‡åº¦è‡ªä¿¡ï¼Œä»è€Œæ˜¾è‘—æ”¹å–„å†³ç­–ï¼Œè€Œå…¶ä»–æ¨¡å‹åˆ™æœªèƒ½åšåˆ°ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„LLMä»£ç†å—é™äºå¯¹è‡ªèº«èƒ½åŠ›çš„ç¼ºä¹è®¤çŸ¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»»åŠ¡æˆåŠŸé¢„æµ‹ä¸­çš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯„ä¼°æ¨¡å‹çš„è‡ªæˆ‘èƒ½åŠ›ï¼Œå¯¼è‡´å†³ç­–å¤±è¯¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡åˆ†æLLMsåœ¨å¤šæ­¥éª¤ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¢è®¨å…¶è‡ªæˆ‘è®¤çŸ¥èƒ½åŠ›åŠå¦‚ä½•é€šè¿‡ä¸Šä¸‹æ–‡ç»éªŒæ”¹å–„å†³ç­–ã€‚è®¾è®¡çš„æ ¸å¿ƒåœ¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é˜¶æ®µçš„è‡ªä¿¡ç¨‹åº¦ä¸å®é™…è¡¨ç°ä¹‹é—´çš„å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å®éªŒè®¾è®¡ï¼Œåˆ†ä¸ºä»»åŠ¡é¢„æµ‹ã€ä¸Šä¸‹æ–‡ç»éªŒå­¦ä¹ å’Œå†³ç­–è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—é€šè¿‡ä¸åŒçš„ä»»åŠ¡è®¾ç½®å’Œè¯„ä¼°æ ‡å‡†è¿›è¡ŒéªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ­ç¤ºäº†LLMsåœ¨å¤šæ­¥éª¤ä»»åŠ¡ä¸­çš„è¿‡åº¦è‡ªä¿¡ç°è±¡åŠå…¶å¯¹å†³ç­–çš„å½±å“ï¼Œå°¤å…¶æ˜¯é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ æ¥è°ƒæ•´è‡ªä¿¡ç¨‹åº¦çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­é‡‡ç”¨äº†å¤šç§ä»»åŠ¡è®¾ç½®ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¡¨ç°ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ä»»åŠ¡å¤æ‚åº¦ã€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å¼•å…¥ç­‰ï¼Œç¡®ä¿äº†å®éªŒçš„å…¨é¢æ€§ä¸æœ‰æ•ˆæ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24661v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24661v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24661v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡æ‰€æœ‰LLMsæ™®éå­˜åœ¨è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹åœ¨æˆåŠŸé¢„æµ‹ä¸Šè¡¨ç°å‡ºä¼˜äºéšæœºçš„åˆ¤åˆ«èƒ½åŠ›ã€‚éƒ¨åˆ†LLMsåœ¨ç»å†å¤±è´¥åèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„å†³ç­–ï¼Œè¡¨æ˜ä¸Šä¸‹æ–‡å­¦ä¹ å¯¹å†³ç­–è´¨é‡çš„æå‡å…·æœ‰é‡è¦ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡LLMsçš„è‡ªæˆ‘è®¤çŸ¥èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘å†³ç­–å¤±è¯¯ï¼Œå¢å¼ºå…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ä»·å€¼ï¼Œæœªæ¥å¯èƒ½å¯¹AIçš„å®‰å…¨æ€§å’Œå¯é æ€§äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs' decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.

