---
layout: default
title: "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models"
---

# Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24618" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24618v1</a>
  <a href="https://arxiv.org/pdf/2512.24618.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24618v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24618v1', 'Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junru Lu, Jiarui Qin, Lingfeng Qiao, Yinghui Li, Xinyi Dai, Bo Ke, Jianfeng He, Ruizhi Qiao, Di Yin, Xing Sun, Yunsheng Wu, Yinsong Liu, Shuangyin Liu, Mingkong Tang, Haodong Lin, Jiayi Kuang, Fanxu Meng, Xiaojuan Tang, Yunjia Xi, Junjie Huang, Haotong Yang, Zhenyi Shen, Yangning Li, Qianwen Zhang, Yifei Yu, Siyu An, Junnan Dong, Qiufeng Wang, Jie Wang, Keyu Chen, Wei Wen, Taian Guo, Zhifeng Shen, Daohai Yu, Jiahao Li, Ke Li, Zongyi Li, Xiaoyu Tan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

**å¤‡æ³¨**: 57 pages, 26 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºYoutu-LLMï¼Œä¸€ç§è½»é‡çº§ä¸”å…·å¤‡åŸç”ŸAgentèƒ½åŠ›çš„è¯­è¨€æ¨¡å‹ï¼Œæ€§èƒ½è¶…è¶ŠåŒè§„æ¨¡æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è½»é‡çº§è¯­è¨€æ¨¡å‹` `Agentæ™ºèƒ½` `é•¿ä¸Šä¸‹æ–‡æ¨ç†` `è¯¾ç¨‹å­¦ä¹ ` `å¤šæ½œåœ¨æ³¨æ„åŠ›` `ä»å¤´é¢„è®­ç»ƒ` `STEMæ•™è‚²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å°æ¨¡å‹ä¾èµ–è’¸é¦ï¼Œéš¾ä»¥å…¼é¡¾æ•ˆç‡ä¸Agentèƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. Youtu-LLMé€šè¿‡ä»å¤´é¢„è®­ç»ƒï¼Œç»“åˆMLAæ¶æ„å’ŒSTEMå¯¼å‘è¯æ±‡ï¼Œå®ç°é•¿ä¸Šä¸‹æ–‡æ¨ç†å’Œè§„åˆ’ã€‚
3. å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥å’ŒAgentä¸­æœŸè®­ç»ƒï¼Œä½¿æ¨¡å‹åœ¨Agentä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰SOTAåŸºçº¿ï¼Œé€šç”¨æ€§èƒ½ä¹Ÿå…·ç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»Youtu-LLMï¼Œä¸€ç§è½»é‡çº§ä½†åŠŸèƒ½å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒå…¼é¡¾äº†é«˜è®¡ç®—æ•ˆç‡å’ŒåŸç”ŸAgentæ™ºèƒ½ã€‚ä¸ä¾èµ–è’¸é¦çš„å…¸å‹å°å‹æ¨¡å‹ä¸åŒï¼ŒYoutu-LLM (1.96B) ä»å¤´å¼€å§‹é¢„è®­ç»ƒï¼Œä»¥ç³»ç»Ÿåœ°åŸ¹å…»æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚ä¸»è¦æŠ€æœ¯è¿›æ­¥å¦‚ä¸‹ï¼š(1) å…·æœ‰é•¿ä¸Šä¸‹æ–‡æ”¯æŒçš„ç´§å‡‘æ¶æ„ï¼šYoutu-LLM æ„å»ºåœ¨å…·æœ‰æ–°å‹é¢å‘ STEM è¯æ±‡çš„å¯†é›†å¤šæ½œåœ¨æ³¨æ„åŠ› (MLA) æ¶æ„ä¹‹ä¸Šï¼Œæ”¯æŒ 128k ä¸Šä¸‹æ–‡çª—å£ã€‚è¿™ç§è®¾è®¡åœ¨æœ€å°çš„å†…å­˜å ç”¨å†…å®ç°äº†å¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†å’ŒçŠ¶æ€è·Ÿè¸ªï¼Œä½¿å…¶æˆä¸ºé•¿ç¨‹ Agent å’Œæ¨ç†ä»»åŠ¡çš„ç†æƒ³é€‰æ‹©ã€‚(2) æœ‰åŸåˆ™çš„â€œå¸¸è¯†-STEM-Agentâ€è¯¾ç¨‹ï¼šæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå¤§çº¦ 11T tokens çš„å¤§å‹è¯­æ–™åº“ï¼Œå¹¶å®æ–½äº†å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚é€šè¿‡é€æ­¥å°†é¢„è®­ç»ƒæ•°æ®åˆ†å¸ƒä»ä¸€èˆ¬å¸¸è¯†è½¬ç§»åˆ°å¤æ‚çš„ STEM å’Œ Agent ä»»åŠ¡ï¼Œæˆ‘ä»¬ç¡®ä¿æ¨¡å‹è·å¾—æ·±åˆ»çš„è®¤çŸ¥èƒ½åŠ›ï¼Œè€Œä¸æ˜¯è¡¨é¢ä¸Šçš„å¯¹é½ã€‚(3) å¯æ‰©å±•çš„ Agent ä¸­æœŸè®­ç»ƒï¼šä¸“é—¨é’ˆå¯¹ Agent ä¸­æœŸè®­ç»ƒï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ ·åŒ–çš„æ•°æ®æ„å»ºæ–¹æ¡ˆæ¥åˆæˆè·¨æ•°å­¦ã€ç¼–ç å’Œå·¥å…·ä½¿ç”¨é¢†åŸŸçš„ä¸°å¯Œå¤šæ ·çš„è½¨è¿¹ã€‚è¿™ç§é«˜è´¨é‡çš„æ•°æ®ä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å†…åŒ–è§„åˆ’å’Œåæ€è¡Œä¸ºã€‚å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼ŒYoutu-LLM ä¸º 2B ä»¥ä¸‹çš„ LLM æ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚åœ¨é€šç”¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒå®ç°äº†ä¸æ›´å¤§æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè€Œåœ¨ç‰¹å®šäº Agent çš„ä»»åŠ¡ä¸­ï¼Œå®ƒæ˜¾ç€è¶…è¶Šäº†ç°æœ‰çš„ SOTA åŸºçº¿ï¼Œè¡¨æ˜è½»é‡çº§æ¨¡å‹å¯ä»¥æ‹¥æœ‰å¼ºå¤§çš„å†…åœ¨ Agent èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å°å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä¾èµ–äºçŸ¥è¯†è’¸é¦ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨æ¨ç†ã€è§„åˆ’å’ŒAgentä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¦‚ä½•åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ„å»ºä¸€ä¸ªæ—¢é«˜æ•ˆåˆæ™ºèƒ½çš„è½»é‡çº§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨æ¨¡å‹å¤§å°ã€ä¸Šä¸‹æ–‡é•¿åº¦å’ŒAgentèƒ½åŠ›ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šYoutu-LLMçš„æ ¸å¿ƒæ€è·¯æ˜¯ä»å¤´å¼€å§‹é¢„è®­ç»ƒä¸€ä¸ªè½»é‡çº§æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥æå‡æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ•°æ®ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„å‚æ•°ä¸‹å®ç°å¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†å’ŒAgentèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šYoutu-LLMçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) **ç´§å‡‘æ¶æ„**ï¼šé‡‡ç”¨å¤šæ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰æ¶æ„ï¼Œæ”¯æŒ128kä¸Šä¸‹æ–‡çª—å£ã€‚2) **STEMå¯¼å‘è¯æ±‡**ï¼šè®¾è®¡äº†ä¸€ç§æ–°å‹çš„é¢å‘STEMçš„è¯æ±‡è¡¨ã€‚3) **å¤šé˜¶æ®µè®­ç»ƒ**ï¼šé‡‡ç”¨â€œå¸¸è¯†-STEM-Agentâ€è¯¾ç¨‹ï¼Œé€æ­¥æå‡æ¨¡å‹èƒ½åŠ›ã€‚4) **Agentä¸­æœŸè®­ç»ƒ**ï¼šä½¿ç”¨å¤šæ ·åŒ–çš„æ•°æ®æ„å»ºæ–¹æ¡ˆï¼Œåˆæˆæ•°å­¦ã€ç¼–ç å’Œå·¥å…·ä½¿ç”¨ç­‰é¢†åŸŸçš„è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šYoutu-LLMçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **ä»å¤´é¢„è®­ç»ƒ**ï¼šé¿å…äº†çŸ¥è¯†è’¸é¦å¸¦æ¥çš„æ€§èƒ½ç“¶é¢ˆã€‚2) **MLAæ¶æ„**ï¼šåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘äº†æ¨¡å‹å‚æ•°é‡ã€‚3) **è¯¾ç¨‹å­¦ä¹ **ï¼šé€šè¿‡é€æ­¥å¢åŠ è®­ç»ƒéš¾åº¦ï¼Œæå‡æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ã€‚4) **Agentä¸­æœŸè®­ç»ƒ**ï¼šä¸“é—¨é’ˆå¯¹Agentä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œæå‡æ¨¡å‹çš„è§„åˆ’å’Œåæ€èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹æ¶æ„æ–¹é¢ï¼ŒMLAæ¶æ„é€šè¿‡å¼•å…¥å¤šä¸ªæ½œåœ¨å˜é‡æ¥æ•æ‰è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒæ–¹é¢çš„ä¿¡æ¯ã€‚åœ¨è®­ç»ƒæ•°æ®æ–¹é¢ï¼Œâ€œå¸¸è¯†-STEM-Agentâ€è¯¾ç¨‹é€æ­¥å¢åŠ è®­ç»ƒéš¾åº¦ï¼Œä»ä¸€èˆ¬å¸¸è¯†åˆ°å¤æ‚çš„STEMå’ŒAgentä»»åŠ¡ã€‚åœ¨Agentä¸­æœŸè®­ç»ƒä¸­ï¼Œä½¿ç”¨å¤šæ ·åŒ–çš„æ•°æ®æ„å»ºæ–¹æ¡ˆï¼Œåˆæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24618v1/figures/1_intro/scatter.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24618v1/x1.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24618v1/x2.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

Youtu-LLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚åœ¨é€šç”¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒä¸æ›´å¤§çš„æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚åœ¨Agentç‰¹å®šä»»åŠ¡ä¸­ï¼Œå®ƒæ˜¾è‘—ä¼˜äºç°æœ‰çš„SOTAåŸºçº¿ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›Agentä»»åŠ¡ä¸Šï¼ŒYoutu-LLMçš„æ€§èƒ½æå‡è¶…è¿‡äº†10%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè½»é‡çº§æ¨¡å‹å¯ä»¥æ‹¥æœ‰å¼ºå¤§çš„å†…åœ¨Agentèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Youtu-LLMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥è¢«éƒ¨ç½²åœ¨ç§»åŠ¨è®¾å¤‡æˆ–åµŒå…¥å¼ç³»ç»Ÿä¸­ï¼Œç”¨äºæ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å®¢æœã€æ™ºèƒ½å®¶å±…ç­‰åº”ç”¨ã€‚å…¶å¼ºå¤§çš„Agentèƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿå¤„ç†å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªåŠ¨ä»£ç ç”Ÿæˆã€æ•°å­¦é—®é¢˜æ±‚è§£ã€å·¥å…·ä½¿ç”¨ç­‰ã€‚è¯¥ç ”ç©¶ä¸ºè½»é‡çº§è¯­è¨€æ¨¡å‹çš„å‘å±•å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled "Commonsense-STEM-Agent" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.

