---
layout: default
title: Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline
---

# Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24933" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24933v1</a>
  <a href="https://arxiv.org/pdf/2512.24933.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24933v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24933v1', 'Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minjun Zhao, Xinyu Zhang, Shuai Zhang, Deyang Li, Ruifeng Shi

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºADOPTæ¡†æ¶ï¼Œè‡ªé€‚åº”ä¼˜åŒ–å¤šæ­¥LLMæµæ°´çº¿ä¸­çš„æç¤ºï¼Œè§£å†³ä¾èµ–å»ºæ¨¡éš¾é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ­¥LLMæµæ°´çº¿` `æç¤ºä¼˜åŒ–` `ä¾èµ–å»ºæ¨¡` `æ–‡æœ¬æ¢¯åº¦ä¼°è®¡` `è‡ªé€‚åº”èµ„æºåˆ†é…` `Shapleyå€¼` `å› æœæ¨æ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ­¥LLMæµæ°´çº¿æ€§èƒ½å—å„æ­¥éª¤æç¤ºå½±å“ï¼Œä½†ç¼ºä¹æ­¥éª¤ç›‘ç£å’Œæ­¥éª¤é—´ä¾èµ–å¯¼è‡´è”åˆä¼˜åŒ–å›°éš¾ã€‚
2. ADOPTæ¡†æ¶æ˜¾å¼å»ºæ¨¡æ­¥éª¤ä¾èµ–ï¼Œå®ç°ç²¾ç¡®æ–‡æœ¬æ¢¯åº¦ä¼°è®¡ï¼Œè§£è€¦æ¢¯åº¦ä¼°è®¡ä¸æ›´æ–°ï¼Œç®€åŒ–ä¼˜åŒ–è¿‡ç¨‹ã€‚
3. ADOPTé‡‡ç”¨Shapleyå€¼è‡ªé€‚åº”åˆ†é…ä¼˜åŒ–èµ„æºï¼Œå®éªŒè¡¨æ˜å…¶ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ­¥LLMæµæ°´çº¿é€šè¿‡ç»“æ„åŒ–çš„åºåˆ—å¤šæ¬¡è°ƒç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³å¤æ‚ä»»åŠ¡ï¼Œä½†å…¶æ€§èƒ½ä¸¥é‡ä¾èµ–äºæ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨çš„æç¤ºã€‚ç”±äºç¼ºä¹æ­¥éª¤çº§åˆ«çš„ç›‘ç£å’Œæ­¥éª¤é—´çš„ä¾èµ–å…³ç³»ï¼Œè”åˆä¼˜åŒ–è¿™äº›æç¤ºéå¸¸å›°éš¾ã€‚ç°æœ‰çš„ç«¯åˆ°ç«¯æç¤ºä¼˜åŒ–æ–¹æ³•åœ¨è¿™ç§æ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”å¸¸å¸¸äº§ç”Ÿæ¬¡ä¼˜æˆ–ä¸ç¨³å®šçš„æ›´æ–°ã€‚æˆ‘ä»¬æå‡ºäº†ADOPTï¼Œä¸€ä¸ªç”¨äºå¤šæ­¥LLMæµæ°´çº¿çš„è‡ªé€‚åº”ä¾èµ–æ„ŸçŸ¥æç¤ºä¼˜åŒ–æ¡†æ¶ã€‚ADOPTæ˜¾å¼åœ°å»ºæ¨¡äº†æ¯ä¸ªLLMæ­¥éª¤å’Œæœ€ç»ˆä»»åŠ¡ç»“æœä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œèƒ½å¤Ÿè¿›è¡Œç²¾ç¡®çš„æ–‡æœ¬æ¢¯åº¦ä¼°è®¡ï¼Œç±»ä¼¼äºè®¡ç®—è§£æå¯¼æ•°ã€‚å®ƒå°†æ–‡æœ¬æ¢¯åº¦ä¼°è®¡ä¸æ¢¯åº¦æ›´æ–°è§£è€¦ï¼Œå°†å¤šæç¤ºä¼˜åŒ–ç®€åŒ–ä¸ºçµæ´»çš„å•æç¤ºä¼˜åŒ–æ­¥éª¤ï¼Œå¹¶é‡‡ç”¨åŸºäºShapleyå€¼çš„æœºåˆ¶æ¥é€‚åº”æ€§åœ°åˆ†é…ä¼˜åŒ–èµ„æºã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†å’Œå¤šæ ·åŒ–æµæ°´çº¿ç»“æ„ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒADOPTæ˜¯æœ‰æ•ˆä¸”é²æ£’çš„ï¼Œå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æç¤ºä¼˜åŒ–åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ­¥LLMæµæ°´çº¿çš„æ€§èƒ½é«˜åº¦ä¾èµ–äºæ¯ä¸ªæ­¥éª¤çš„æç¤ºè®¾è®¡ï¼Œä½†ç”±äºç¼ºä¹å¯¹ä¸­é—´æ­¥éª¤çš„ç›´æ¥ç›‘ç£ä¿¡å·ï¼Œä»¥åŠæ­¥éª¤ä¹‹é—´å¤æ‚çš„ä¾èµ–å…³ç³»ï¼Œè”åˆä¼˜åŒ–æ‰€æœ‰æ­¥éª¤çš„æç¤ºå˜å¾—å¼‚å¸¸å›°éš¾ã€‚ç°æœ‰çš„ç«¯åˆ°ç«¯æç¤ºä¼˜åŒ–æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†è¿™ç§å¤æ‚æ€§ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šADOPTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ˜¾å¼åœ°å»ºæ¨¡æ¯ä¸ªLLMæ­¥éª¤ä¸æœ€ç»ˆä»»åŠ¡ç»“æœä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæ¥è§£å†³å¤šæ­¥æç¤ºä¼˜åŒ–é—®é¢˜ã€‚ç±»ä¼¼äºè®¡ç®—è§£æå¯¼æ•°ï¼ŒADOPTèƒ½å¤Ÿç²¾ç¡®åœ°ä¼°è®¡æ–‡æœ¬æ¢¯åº¦ï¼Œä»è€Œæ›´å‡†ç¡®åœ°æŒ‡å¯¼æç¤ºçš„ä¼˜åŒ–æ–¹å‘ã€‚é€šè¿‡è§£è€¦æ–‡æœ¬æ¢¯åº¦ä¼°è®¡å’Œæ¢¯åº¦æ›´æ–°ï¼Œå°†å¤æ‚çš„å¤šæç¤ºä¼˜åŒ–é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªæ›´æ˜“äºå¤„ç†çš„å•æç¤ºä¼˜åŒ–é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šADOPTæ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š1) ä¾èµ–å…³ç³»å»ºæ¨¡ï¼šåˆ©ç”¨å› æœæ¨æ–­æˆ–æ³¨æ„åŠ›æœºåˆ¶ç­‰æ–¹æ³•ï¼Œæ˜¾å¼åœ°å»ºæ¨¡æ¯ä¸ªLLMæ­¥éª¤å¯¹æœ€ç»ˆç»“æœçš„å½±å“ã€‚2) æ–‡æœ¬æ¢¯åº¦ä¼°è®¡ï¼šåŸºäºå»ºæ¨¡çš„ä¾èµ–å…³ç³»ï¼Œè®¡ç®—æ¯ä¸ªæ­¥éª¤æç¤ºçš„æ–‡æœ¬æ¢¯åº¦ï¼Œåæ˜ å…¶å¯¹æœ€ç»ˆç»“æœçš„å½±å“ç¨‹åº¦ã€‚3) è‡ªé€‚åº”èµ„æºåˆ†é…ä¸ä¼˜åŒ–ï¼šä½¿ç”¨åŸºäºShapleyå€¼çš„æœºåˆ¶ï¼Œæ ¹æ®æ¯ä¸ªæ­¥éª¤çš„é‡è¦æ€§è‡ªé€‚åº”åœ°åˆ†é…ä¼˜åŒ–èµ„æºï¼Œå¹¶é‡‡ç”¨å•æç¤ºä¼˜åŒ–ç®—æ³•æ›´æ–°æç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šADOPTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä¾èµ–æ„ŸçŸ¥çš„æ–‡æœ¬æ¢¯åº¦ä¼°è®¡æ–¹æ³•å’Œè‡ªé€‚åº”èµ„æºåˆ†é…æœºåˆ¶ã€‚ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯ä¼˜åŒ–æ–¹æ³•å¿½ç•¥äº†æ­¥éª¤é—´çš„ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´æ¢¯åº¦ä¼°è®¡ä¸å‡†ç¡®ã€‚ADOPTé€šè¿‡æ˜¾å¼å»ºæ¨¡ä¾èµ–å…³ç³»ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„æ¢¯åº¦ä¼°è®¡ï¼Œä»è€Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–æç¤ºã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”èµ„æºåˆ†é…æœºåˆ¶èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªæ­¥éª¤çš„é‡è¦æ€§åŠ¨æ€è°ƒæ•´ä¼˜åŒ–åŠ›åº¦ï¼Œé¿å…äº†å¯¹æ‰€æœ‰æ­¥éª¤è¿›è¡Œå¹³å‡å¤„ç†ï¼Œæé«˜äº†ä¼˜åŒ–æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šADOPTä½¿ç”¨Shapleyå€¼æ¥è¡¡é‡æ¯ä¸ªæ­¥éª¤å¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ï¼Œå¹¶ä»¥æ­¤ä¸ºä¾æ®åˆ†é…ä¼˜åŒ–èµ„æºã€‚å…·ä½“è€Œè¨€ï¼ŒShapleyå€¼é€šè¿‡è®¡ç®—æ¯ä¸ªæ­¥éª¤åœ¨æ‰€æœ‰å¯èƒ½çš„æ­¥éª¤ç»„åˆä¸­çš„è¾¹é™…è´¡çŒ®æ¥è¯„ä¼°å…¶é‡è¦æ€§ã€‚åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼ŒADOPTå¯ä»¥é‡‡ç”¨å„ç§å•æç¤ºä¼˜åŒ–ç®—æ³•ï¼Œä¾‹å¦‚æ¢¯åº¦ä¸‹é™æˆ–è¿›åŒ–ç®—æ³•ï¼Œæ¥æ›´æ–°æ¯ä¸ªæ­¥éª¤çš„æç¤ºã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œé€šå¸¸åŒ…æ‹¬ä»»åŠ¡ç›¸å…³çš„æŸå¤±å’Œæ­£åˆ™åŒ–é¡¹ï¼Œä»¥é¿å…è¿‡æ‹Ÿåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒADOPTåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†å’Œä¸åŒçš„æµæ°´çº¿ç»“æ„ä¸Šå‡ä¼˜äºç°æœ‰çš„æç¤ºä¼˜åŒ–åŸºçº¿ã€‚ä¾‹å¦‚ï¼Œåœ¨çŸ¥è¯†å›¾è°±æ¨ç†ä»»åŠ¡ä¸Šï¼ŒADOPTç›¸æ¯”äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡äº†5%-10%ã€‚æ­¤å¤–ï¼ŒADOPTåœ¨é¢å¯¹å™ªå£°æ•°æ®å’Œå¯¹æŠ—æ€§æ”»å‡»æ—¶ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ADOPTæ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¤šæ­¥æ¨ç†æˆ–å†³ç­–çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚çŸ¥è¯†å›¾è°±æ¨ç†ã€å¯¹è¯ç³»ç»Ÿã€ä»£ç ç”Ÿæˆå’Œè§„åˆ’ç­‰ã€‚é€šè¿‡è‡ªåŠ¨ä¼˜åŒ–æ¯ä¸ªæ­¥éª¤çš„æç¤ºï¼Œå¯ä»¥æ˜¾è‘—æå‡LLMæµæ°´çº¿çš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Œé™ä½äººå·¥è®¾è®¡æç¤ºçš„æˆæœ¬ï¼Œå¹¶åŠ é€ŸLLMåœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter-step dependencies. Existing end-to-end prompt optimization methods struggle under these conditions and often yield suboptimal or unstable updates. We propose ADOPT, an Adaptive Dependency-aware Prompt Optimization framework for multi-step LLM pipelines. ADOPT explicitly models the dependency between each LLM step and the final task outcome, enabling precise text-gradient estimation analogous to computing analytical derivatives. It decouples textual gradient estimation from gradient updates, reducing multi-prompt optimization to flexible single-prompt optimization steps, and employs a Shapley-based mechanism to adaptively allocate optimization resources. Experiments on real-world datasets and diverse pipeline structures show that ADOPT is effective and robust, consistently outperforming state-of-the-art prompt optimization baselines.

