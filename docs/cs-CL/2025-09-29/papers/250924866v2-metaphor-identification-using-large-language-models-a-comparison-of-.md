---
layout: default
title: Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning
---

# Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24866" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24866v2</a>
  <a href="https://arxiv.org/pdf/2509.24866.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24866v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24866v2', 'Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Matteo Fuoli, Weihang Huang, Jeannette Littlemore, Sarah Turner, Ellen Wilding

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-10-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œéšå–»è¯†åˆ«ï¼šæ¯”è¾ƒRAGã€æç¤ºå·¥ç¨‹å’Œå¾®è°ƒæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éšå–»è¯†åˆ«` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `æç¤ºå·¥ç¨‹` `å¾®è°ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ–‡æœ¬åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éšå–»è¯†åˆ«ä¾èµ–äººå·¥æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥å¤§è§„æ¨¡åº”ç”¨ï¼Œé˜»ç¢äº†å¯¹è®¤çŸ¥ã€æƒ…æ„Ÿå’Œæ„è¯†å½¢æ€çš„æ·±å…¥åˆ†æã€‚
2. è®ºæ–‡æ¢ç´¢äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–éšå–»è¯†åˆ«ï¼Œæ¯”è¾ƒäº†æ£€ç´¢å¢å¼ºç”Ÿæˆã€æç¤ºå·¥ç¨‹å’Œå¾®è°ƒä¸‰ç§æ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„LLMåœ¨éšå–»è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸­å€¼F1å€¼è¾¾åˆ°0.79ï¼Œä¸ºè‡ªåŠ¨åŒ–éšå–»åˆ†ææä¾›äº†å¯èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšå–»æ˜¯è¯è¯­ä¸­æ™®éå­˜åœ¨çš„ç‰¹å¾ï¼Œä¹Ÿæ˜¯è€ƒå¯Ÿè®¤çŸ¥ã€æƒ…æ„Ÿå’Œæ„è¯†å½¢æ€çš„æœ‰åŠ›å·¥å…·ã€‚ç„¶è€Œï¼Œç”±äºéšå–»çš„ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§ï¼Œå¤§è§„æ¨¡åˆ†æä¸€ç›´å—åˆ°æ‰‹åŠ¨æ ‡æ³¨éœ€æ±‚çš„é™åˆ¶ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å…¨æ–‡éšå–»è¯†åˆ«æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§æ–¹æ³•ï¼šï¼ˆiï¼‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œå…¶ä¸­æ¨¡å‹è¢«æä¾›ä¸€ä¸ªä»£ç æœ¬ï¼Œå¹¶è¢«æŒ‡ç¤ºæ ¹æ®å…¶è§„åˆ™å’Œç¤ºä¾‹æ¥æ³¨é‡Šæ–‡æœ¬ï¼›ï¼ˆiiï¼‰æç¤ºå·¥ç¨‹ï¼Œæˆ‘ä»¬è®¾è®¡ç‰¹å®šäºä»»åŠ¡çš„å£å¤´æŒ‡ä»¤ï¼›ä»¥åŠï¼ˆiiiï¼‰å¾®è°ƒï¼Œå…¶ä¸­æ¨¡å‹åœ¨æ‰‹åŠ¨ç¼–ç çš„æ–‡æœ¬ä¸Šè¿›è¡Œè®­ç»ƒä»¥ä¼˜åŒ–æ€§èƒ½ã€‚åœ¨æç¤ºå·¥ç¨‹ä¸­ï¼Œæˆ‘ä»¬æµ‹è¯•äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ€ç»´é“¾ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„é—­æºLLMå¯ä»¥å®ç°é«˜ç²¾åº¦ï¼Œå…¶ä¸­å¾®è°ƒäº§ç”Ÿçš„ä¸­å€¼F1åˆ†æ•°ä¸º0.79ã€‚äººç±»å’ŒLLMè¾“å‡ºçš„æ¯”è¾ƒè¡¨æ˜ï¼Œå¤§å¤šæ•°å·®å¼‚æ˜¯ç³»ç»Ÿæ€§çš„ï¼Œåæ˜ äº†éšå–»ç†è®ºä¸­ä¼—æ‰€å‘¨çŸ¥çš„ç°è‰²åŒºåŸŸå’Œæ¦‚å¿µæŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒLLMå¯ä»¥ç”¨äºè‡³å°‘éƒ¨åˆ†åœ°è‡ªåŠ¨åŒ–éšå–»è¯†åˆ«ï¼Œå¹¶ä¸”å¯ä»¥ä½œä¸ºå¼€å‘å’Œæ”¹è¿›éšå–»è¯†åˆ«åè®®åŠå…¶åŸºç¡€ç†è®ºçš„è¯•éªŒå°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³éšå–»è¯†åˆ«çš„è‡ªåŠ¨åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äººå·¥æ ‡æ³¨ï¼Œè€—æ—¶è€—åŠ›ï¼Œéš¾ä»¥åº”ç”¨äºå¤§è§„æ¨¡æ–‡æœ¬åˆ†æã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬ä¸­çš„éšå–»æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡ä¸åŒçš„æ–¹æ³•ï¼ˆRAGã€æç¤ºå·¥ç¨‹ã€å¾®è°ƒï¼‰æ¥æŒ‡å¯¼æ¨¡å‹è¿›è¡Œéšå–»è¯†åˆ«ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒæ–¹æ³•çš„æ€§èƒ½ï¼Œæ¢ç´¢LLMåœ¨éšå–»è¯†åˆ«ä»»åŠ¡ä¸­çš„æœ€ä½³åº”ç”¨æ–¹å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æ¯”è¾ƒäº†ä¸‰ç§ä¸»è¦çš„æŠ€æœ¯æ¡†æ¶ï¼š
1. **æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)**ï¼šæ¨¡å‹é¦–å…ˆä»ä¸€ä¸ªåŒ…å«éšå–»è§„åˆ™å’Œç¤ºä¾‹çš„ä»£ç æœ¬ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç„¶ååŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆéšå–»æ ‡æ³¨ã€‚
2. **æç¤ºå·¥ç¨‹**ï¼šé€šè¿‡è®¾è®¡ä¸åŒçš„æç¤ºï¼ˆé›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€æ€ç»´é“¾ï¼‰ï¼Œå¼•å¯¼æ¨¡å‹ç†è§£ä»»åŠ¡å¹¶è¿›è¡Œéšå–»è¯†åˆ«ã€‚
3. **å¾®è°ƒ**ï¼šä½¿ç”¨äººå·¥æ ‡æ³¨çš„éšå–»æ•°æ®é›†å¯¹é¢„è®­ç»ƒçš„LLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”éšå–»è¯†åˆ«ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†RAGã€æç¤ºå·¥ç¨‹å’Œå¾®è°ƒä¸‰ç§æ–¹æ³•åœ¨éšå–»è¯†åˆ«ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†LLMä¸äººç±»æ ‡æ³¨ç»“æœçš„å·®å¼‚ï¼Œæ­ç¤ºäº†LLMåœ¨éšå–»è¯†åˆ«ä¸­å­˜åœ¨çš„æŒ‘æˆ˜å’Œæ½œåœ¨æ”¹è¿›æ–¹å‘ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æç¤ºå·¥ç¨‹ä¸­ï¼Œè®ºæ–‡å°è¯•äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ€ç»´é“¾ç­‰ä¸åŒçš„æç¤ºç­–ç•¥ï¼Œä»¥æ¢ç´¢æœ€ä½³çš„æç¤ºæ–¹å¼ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†äººå·¥æ ‡æ³¨çš„éšå–»æ•°æ®é›†ï¼Œå¹¶ä¼˜åŒ–äº†æ¨¡å‹çš„è®­ç»ƒå‚æ•°ï¼Œä»¥æé«˜æ¨¡å‹çš„éšå–»è¯†åˆ«ç²¾åº¦ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„LLMåœ¨éšå–»è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œä¸­å€¼F1åˆ†æ•°ä¸º0.79ã€‚è¿™è¡¨æ˜LLMåœ¨è‡ªåŠ¨åŒ–éšå–»è¯†åˆ«æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†LLMä¸äººç±»æ ‡æ³¨ç»“æœçš„å·®å¼‚ï¼Œå‘ç°å¤§å¤šæ•°å·®å¼‚æ˜¯ç³»ç»Ÿæ€§çš„ï¼Œåæ˜ äº†éšå–»ç†è®ºä¸­å­˜åœ¨çš„ç°è‰²åŒºåŸŸå’Œæ¦‚å¿µæŒ‘æˆ˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—ç¤¾ä¼šç§‘å­¦ã€å¿ƒç†å­¦ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºåˆ†ææ–°é—»æŠ¥é“ä¸­çš„éšå–»ï¼Œä»è€Œäº†è§£åª’ä½“çš„æ„è¯†å½¢æ€å€¾å‘ï¼›ä¹Ÿå¯ä»¥ç”¨äºç ”ç©¶æ–‡å­¦ä½œå“ä¸­çš„éšå–»ï¼Œä»è€Œæ·±å…¥ç†è§£ä½œè€…çš„åˆ›ä½œæ„å›¾ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºå¼€å‘æ™ºèƒ½æ–‡æœ¬åˆ†æå·¥å…·ï¼Œè¾…åŠ©ç ”ç©¶äººå‘˜è¿›è¡Œå¤§è§„æ¨¡æ–‡æœ¬æ•°æ®çš„åˆ†æã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Metaphor is a pervasive feature of discourse and a powerful lens for examining cognition, emotion, and ideology. Large-scale analysis, however, has been constrained by the need for manual annotation due to the context-sensitive nature of metaphor. This study investigates the potential of large language models (LLMs) to automate metaphor identification in full texts. We compare three methods: (i) retrieval-augmented generation (RAG), where the model is provided with a codebook and instructed to annotate texts based on its rules and examples; (ii) prompt engineering, where we design task-specific verbal instructions; and (iii) fine-tuning, where the model is trained on hand-coded texts to optimize performance. Within prompt engineering, we test zero-shot, few-shot, and chain-of-thought strategies. Our results show that state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning yielding a median F1 score of 0.79. A comparison of human and LLM outputs reveals that most discrepancies are systematic, reflecting well-known grey areas and conceptual challenges in metaphor theory. We propose that LLMs can be used to at least partly automate metaphor identification and can serve as a testbed for developing and refining metaphor identification protocols and the theory that underpins them.

