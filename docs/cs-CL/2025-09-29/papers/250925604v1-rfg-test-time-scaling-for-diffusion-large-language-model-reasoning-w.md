---
layout: default
title: RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance
---

# RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25604" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25604v1</a>
  <a href="https://arxiv.org/pdf/2509.25604.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25604v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25604v1', 'RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianlang Chen, Minkai Xu, Jure Leskovec, Stefano Ermon

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: 27 pages, 3 figures, 2 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRFGï¼šä¸€ç§å…å¥–åŠ±å¼•å¯¼çš„æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹æ¨ç†æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†å¼•å¯¼` `å…å¥–åŠ±å­¦ä¹ ` `æµ‹è¯•æ—¶ç¼©æ”¾` `æ•°å­¦æ¨ç†` `ä»£ç ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¾èµ–å¯†é›†æ ‡æ³¨çš„å¥–åŠ±æ¨¡å‹å¼•å¯¼æ¨ç†ï¼Œä½†æ‰©æ•£æ¨¡å‹ä¸­é—´çŠ¶æ€éƒ¨åˆ†æ©ç ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨ã€‚
2. RFGé€šè¿‡å¢å¼ºæ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„å¯¹æ•°ä¼¼ç„¶æ¯”å‚æ•°åŒ–è¿‡ç¨‹å¥–åŠ±ï¼Œæ— éœ€æ˜¾å¼å¥–åŠ±æ¨¡å‹å³å¯å¼•å¯¼æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRFGåœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†å„ç§æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œæœ€é«˜æå‡9.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹(dLLMs)åœ¨å¤§å‹è¯­è¨€å»ºæ¨¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œäººä»¬è¶Šæ¥è¶Šå…³æ³¨é€šè¿‡é€æ­¥å¼•å¯¼æ¨ç†è¿‡ç¨‹æ¥è¿›ä¸€æ­¥æé«˜å…¶è§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„å¸¸è§åšæ³•æ˜¯å­¦ä¹ ä¸€ä¸ªè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œå¹¶å¯¹æ¯ä¸ªä¸­é—´æ­¥éª¤è¿›è¡Œå¯†é›†æ ‡æ³¨ã€‚ç„¶è€Œï¼Œè¿™å¯¹äºdLLMsæ¥è¯´å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬çš„ç”Ÿæˆæ˜¯ä»¥ä»»æ„é¡ºåºè¿›è¡Œçš„ï¼Œå¹¶ä¸”ä¸­é—´çŠ¶æ€æ˜¯éƒ¨åˆ†æ©ç çš„å¥å­ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å…å¥–åŠ±å¼•å¯¼(RFG)æ–¹æ³•ï¼Œç”¨äºåœ¨æ²¡æœ‰æ˜¾å¼è¿‡ç¨‹å¥–åŠ±çš„æƒ…å†µä¸‹å¼•å¯¼dLLMsçš„æ¨ç†è½¨è¿¹ã€‚RFGçš„å…³é”®æ€æƒ³æ˜¯é€šè¿‡å¢å¼ºå’Œå‚è€ƒdLLMsçš„å¯¹æ•°ä¼¼ç„¶æ¯”æ¥å‚æ•°åŒ–è¿‡ç¨‹å¥–åŠ±ï¼Œå…¶ä¸­å¢å¼ºæ¨¡å‹å¯ä»¥é€šè¿‡ä»»ä½•ç°æˆçš„ã€ç»è¿‡å¼ºåŒ–å­¦ä¹ (RL)æˆ–ç›‘ç£å¾®è°ƒ(SFT)åè®­ç»ƒçš„dLLMè½»æ¾è·å¾—ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºè¯æ˜ï¼Œè¡¨æ˜RFGåœ¨æ²¡æœ‰é¢å¤–å¥–åŠ±çš„æƒ…å†µä¸‹è¯±å¯¼äº†å¥–åŠ±å¼•å¯¼çš„é‡‡æ ·åˆ†å¸ƒã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†ä¸Šï¼Œä½¿ç”¨å„ç§ç»è¿‡ä¸åŒåè®­ç»ƒæ–¹æ³•å¢å¼ºçš„dLLMsè¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚RFGåœ¨æ‰€æœ‰ä»»åŠ¡å’Œæ¨¡å‹ç±»å‹ä¸­å§‹ç»ˆäº§ç”Ÿæ˜¾è‘—çš„æ”¹è¿›ï¼Œå®ç°äº†é«˜è¾¾9.2%çš„å‡†ç¡®ç‡æå‡ã€‚è¿™äº›å‘ç°å°†RFGç¡®ç«‹ä¸ºä¸€ä¸ªé€šç”¨çš„ã€æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹çš„æƒ…å†µä¸‹æ‰©å±•æµ‹è¯•æ—¶æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆå¼•å¯¼å…¶æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è‡ªå›å½’æ¨¡å‹çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•ï¼Œä¾èµ–äºå¯¹æ¯ä¸ªä¸­é—´æ­¥éª¤çš„å¯†é›†æ ‡æ³¨ï¼Œè¿™åœ¨dLLMsä¸­ä¸å¯è¡Œï¼Œå› ä¸ºdLLMsçš„ç”Ÿæˆæ˜¯ä»»æ„é¡ºåºçš„ï¼Œä¸­é—´çŠ¶æ€æ˜¯éƒ¨åˆ†æ©ç çš„å¥å­ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§æ— éœ€æ˜¾å¼å¥–åŠ±æ¨¡å‹ï¼Œä¹Ÿèƒ½æœ‰æ•ˆå¼•å¯¼dLLMsæ¨ç†çš„æ–¹æ³•æ˜¯å…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªå¢å¼ºçš„dLLMå’Œä¸€ä¸ªå‚è€ƒdLLMçš„å¯¹æ•°ä¼¼ç„¶æ¯”æ¥å‚æ•°åŒ–è¿‡ç¨‹å¥–åŠ±ã€‚å¢å¼ºçš„dLLMå¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç­‰æ–¹æ³•è·å¾—ï¼Œä»£è¡¨äº†æœŸæœ›çš„æ¨ç†æ–¹å‘ã€‚é€šè¿‡æ¯”è¾ƒå¢å¼ºæ¨¡å‹å’Œå‚è€ƒæ¨¡å‹åœ¨æ¯ä¸ªæ­¥éª¤çš„ç”Ÿæˆæ¦‚ç‡ï¼Œå¯ä»¥éšå¼åœ°ä¼°è®¡å¥–åŠ±ï¼Œä»è€Œå¼•å¯¼dLLMæœç€æ›´æœ‰åˆ©çš„æ–¹å‘ç”Ÿæˆã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ˜¾å¼å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒå’Œæ ‡æ³¨ï¼Œç®€åŒ–äº†å¼•å¯¼è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRFGæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) å‡†å¤‡ä¸€ä¸ªå¢å¼ºçš„dLLMå’Œä¸€ä¸ªå‚è€ƒdLLMã€‚å¢å¼ºçš„dLLMé€šè¿‡RLæˆ–SFTç­‰æ–¹æ³•è¿›è¡Œåè®­ç»ƒï¼Œä½¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚å‚è€ƒdLLMå¯ä»¥æ˜¯åŸå§‹çš„dLLMï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªç»è¿‡ä¸åŒæ–¹å¼è®­ç»ƒçš„dLLMã€‚2) åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¯¹äºæ¯ä¸ªç”Ÿæˆæ­¥éª¤ï¼Œè®¡ç®—å¢å¼ºæ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ç”Ÿæˆå½“å‰tokençš„å¯¹æ•°ä¼¼ç„¶ã€‚3) ä½¿ç”¨å¯¹æ•°ä¼¼ç„¶æ¯”ä½œä¸ºéšå¼çš„è¿‡ç¨‹å¥–åŠ±ï¼Œå¹¶å°†å…¶ç”¨äºè°ƒæ•´dLLMçš„é‡‡æ ·åˆ†å¸ƒï¼Œä»è€Œå¼•å¯¼dLLMæœç€æ›´æœ‰åˆ©çš„æ–¹å‘ç”Ÿæˆã€‚4) é‡å¤æ­¥éª¤2å’Œ3ï¼Œç›´åˆ°ç”Ÿæˆå®Œæ•´çš„è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šRFGæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå®ƒæå‡ºäº†ä¸€ç§å…å¥–åŠ±çš„å¼•å¯¼æ–¹æ³•ï¼Œé¿å…äº†æ˜¾å¼å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒå’Œæ ‡æ³¨ã€‚é€šè¿‡åˆ©ç”¨å¢å¼ºæ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„å¯¹æ•°ä¼¼ç„¶æ¯”ï¼ŒRFGå¯ä»¥éšå¼åœ°ä¼°è®¡è¿‡ç¨‹å¥–åŠ±ï¼Œå¹¶å°†å…¶ç”¨äºå¼•å¯¼dLLMçš„æ¨ç†è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ä¸ä»…ç®€åŒ–äº†å¼•å¯¼è¿‡ç¨‹ï¼Œè¿˜æé«˜äº†å¼•å¯¼çš„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRFGä¸éœ€è¦é¢å¤–çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œå› æ­¤æ›´åŠ çµæ´»å’Œæ˜“äºä½¿ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šRFGçš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•é€‰æ‹©å¢å¼ºæ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨å¯¹æ•°ä¼¼ç„¶æ¯”æ¥è°ƒæ•´é‡‡æ ·åˆ†å¸ƒã€‚å¢å¼ºæ¨¡å‹çš„é€‰æ‹©å–å†³äºå…·ä½“çš„ä»»åŠ¡å’Œç›®æ ‡ï¼Œå¯ä»¥é€‰æ‹©ç»è¿‡RLæˆ–SFTç­‰æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹ã€‚å‚è€ƒæ¨¡å‹çš„é€‰æ‹©ä¹Ÿéœ€è¦ä»”ç»†è€ƒè™‘ï¼Œå¯ä»¥é€‰æ‹©åŸå§‹çš„dLLMï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªç»è¿‡ä¸åŒæ–¹å¼è®­ç»ƒçš„dLLMã€‚å¯¹æ•°ä¼¼ç„¶æ¯”çš„ç¼©æ”¾ç³»æ•°æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ¨¡å‹è¿›è¡Œè°ƒæ•´ã€‚è®ºæ–‡ä¸­å¹¶æ²¡æœ‰è¯¦ç»†è¯´æ˜å…·ä½“çš„æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ï¼Œå› ä¸ºRFGæ˜¯ä¸€ç§é€šç”¨çš„å¼•å¯¼æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äºå„ç§ä¸åŒçš„dLLMsã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRFGåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†ä¸Šï¼Œæ˜¾è‘—æå‡äº†å„ç§æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒRFGå®ç°äº†é«˜è¾¾9.2%çš„å‡†ç¡®ç‡æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRFGæ˜¯ä¸€ç§æœ‰æ•ˆçš„ã€é€šç”¨çš„ã€æ— éœ€è®­ç»ƒçš„å¼•å¯¼æ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ‰©æ•£æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RFGå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œå¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€é€»è¾‘æ¨ç†ç­‰ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œé™ä½äº†åº”ç”¨é—¨æ§›ï¼Œæœ‰åŠ©äºæå‡å„ç§æ‰©æ•£æ¨¡å‹çš„æ¨ç†æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion large language models (dLLMs) have shown great potential in large-scale language modeling, and there is an increasing interest in further improving the capacity to solve complex problems by guiding the reasoning process step by step. Common practice for autoregressive language models typically learns a process reward model with dense annotation for each intermediate step. However, this is challenging for dLLMs where the generation is in an any-order fashion and intermediate states are partially masked sentences. To this end, in this paper, we propose reward-free guidance (RFG), a principled method for guiding the reasoning trajectory of dLLMs without explicit process reward. The key idea of RFG is to parameterize the process reward by log-likelihood ratios of the enhanced and reference dLLMs, where the enhanced model can be easily obtained by any off-the-shelf dLLM that has been post-trained with reinforcement learning (RL) or supervised fine-tuning (SFT). We provide theoretical justification that RFG induces the reward-guided sampling distribution with no additional reward. We conduct comprehensive experiments on four challenging mathematical reasoning and code generation benchmarks using a diverse suite of dLLMs enhanced with various post-training methods. RFG consistently yields significant improvements across all tasks and model types, achieving accuracy gains of up to 9.2%. These findings establish RFG as a general training-free framework that scales test-time reasoning without reliance on external reward models.

