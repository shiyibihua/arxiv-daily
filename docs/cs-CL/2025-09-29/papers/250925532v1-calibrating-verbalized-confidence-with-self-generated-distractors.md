---
layout: default
title: Calibrating Verbalized Confidence with Self-Generated Distractors
---

# Calibrating Verbalized Confidence with Self-Generated Distractors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25532" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25532v1</a>
  <a href="https://arxiv.org/pdf/2509.25532.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25532v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25532v1', 'Calibrating Verbalized Confidence with Self-Generated Distractors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Victor Wang, Elias Stengel-Eskin

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: Code: https://github.com/dubai03nsr/dinco

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDINCOï¼Œé€šè¿‡è‡ªç”Ÿæˆå¹²æ‰°é¡¹æ ¡å‡†LLMçš„ç½®ä¿¡åº¦ï¼Œæå‡å¯é æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç½®ä¿¡åº¦æ ¡å‡†` `è‡ªç”Ÿæˆå¹²æ‰°é¡¹` `æš—ç¤ºæ€§åå·®` `ä¸€è‡´æ€§è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMçš„ç½®ä¿¡åº¦è¡¨è¾¾å­˜åœ¨æ ¡å‡†ä¸ä½³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä½å‡†ç¡®ç‡æƒ…å†µä¸‹è¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡ï¼Œå½±å“ç”¨æˆ·ä¿¡ä»»ã€‚
2. è®ºæ–‡æå‡ºDistractor-Normalized Coherence (DINCO)æ–¹æ³•ï¼Œé€šè¿‡è‡ªç”Ÿæˆå¹²æ‰°é¡¹æ¥ä¼°è®¡å’Œæ ¡æ­£LLMçš„æš—ç¤ºæ€§åå·®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDINCOèƒ½æä¾›æ›´å¯é çš„ç½®ä¿¡åº¦ä¼°è®¡ï¼Œä¸”åœ¨è¾ƒå°‘æ¨ç†æ¬¡æ•°ä¸‹ä¼˜äºè‡ªæ´½æ€§æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ ¡å‡†çš„ç½®ä¿¡åº¦ä¼°è®¡å¯¹äºäººç±»ç”¨æˆ·ä¿¡ä»»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºè‡³å…³é‡è¦ã€‚å°½ç®¡LLMå¯ä»¥ç”¨äººç±»å¯è§£é‡Šçš„æ–¹å¼è¡¨è¾¾å…¶ç½®ä¿¡åº¦ï¼Œä½†ç»éªŒè¡¨æ˜ï¼ŒLLMç”Ÿæˆçš„ç½®ä¿¡åº¦åˆ†æ•°æ ¡å‡†ä¸ä½³ï¼Œåœ¨ä½å‡†ç¡®ç‡çš„å®ä¾‹ä¸ŠæŠ¥å‘Šé«˜ç½®ä¿¡åº¦ï¼Œä»è€ŒæŸå®³ä¿¡ä»»å’Œå®‰å…¨æ€§ã€‚æˆ‘ä»¬å‡è®¾è¿™ç§è¿‡åº¦è‡ªä¿¡é€šå¸¸æºäºLLMåœ¨é¢å¯¹å®ƒç¼–ç ä¿¡æ¯è¾ƒå°‘çš„å£°æ˜æ—¶ï¼Œæ˜“å—æš—ç¤ºçš„å½±å“ï¼›æˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯äº†è¿™ä¸€å‡è®¾ï¼Œå‘ç°LLMåœ¨è¾ƒä½å‡†ç¡®ç‡çš„å£°æ˜ä¸Šæ›´å®¹æ˜“å—åˆ°æš—ç¤ºã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¹²æ‰°é¡¹å½’ä¸€åŒ–ä¸€è‡´æ€§ï¼ˆDINCOï¼‰ï¼Œå®ƒé€šè¿‡è®©æ¨¡å‹ç‹¬ç«‹åœ°åœ¨å‡ ä¸ªè‡ªç”Ÿæˆçš„å¹²æ‰°é¡¹ï¼ˆå³æ›¿ä»£å£°æ˜ï¼‰ä¸Šè¡¨è¾¾å…¶ç½®ä¿¡åº¦ï¼Œå¹¶æŒ‰æ€»ç½®ä¿¡åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ¥ä¼°è®¡å’Œè§£é‡ŠLLMçš„æš—ç¤ºæ€§åå·®ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ ¡å‡†ï¼Œæˆ‘ä»¬åˆ©ç”¨ç”Ÿæˆå™¨-éªŒè¯å™¨ä¸ä¸€è‡´æ€§ï¼Œç”¨åŸºäºä¸€è‡´æ€§çš„ç”Ÿæˆå™¨ç½®ä¿¡åº¦ä¼°è®¡æ¥å¢å¼ºå½’ä¸€åŒ–çš„éªŒè¯å™¨ç½®ä¿¡åº¦ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æµè¡Œçš„è‡ªæ´½æ–¹æ³•è§†ä¸ºåˆ©ç”¨è·¨é‡‡æ ·ç”Ÿæˆçš„ä¸€è‡´æ€§ï¼Œå¹¶å°†å½’ä¸€åŒ–çš„å£å¤´ç½®ä¿¡åº¦è§†ä¸ºåˆ©ç”¨è·¨ä¸å…¼å®¹å£°æ˜çš„éªŒè¯çš„ä¸€è‡´æ€§ï¼Œä»è€Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†è¿™äº›äº’è¡¥çš„ä¸€è‡´æ€§ç»´åº¦é›†æˆåˆ°DINCOä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒDINCOæä¾›äº†ä¸å¤ªé¥±å’Œâ€”â€”å› æ­¤æ›´å¯ç”¨â€”â€”çš„ç½®ä¿¡åº¦ä¼°è®¡ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥çš„é‡‡æ ·æœ¬èº«æ— æ³•ç¼©å°DINCOå’ŒåŸºçº¿ä¹‹é—´çš„å·®è·ï¼ŒDINCOåœ¨10æ¬¡æ¨ç†è°ƒç”¨ä¸­ä¼˜äº100æ¬¡çš„è‡ªæ´½æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆæ–‡æœ¬çš„åŒæ—¶ï¼Œä¹Ÿèƒ½å¤Ÿè¾“å‡ºç½®ä¿¡åº¦è¯„ä¼°ï¼Œä½†è¿™äº›ç½®ä¿¡åº¦å¾€å¾€ä¸å®é™…å‡†ç¡®ç‡ä¸åŒ¹é…ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹å¯¹æŸäº›é—®é¢˜äº†è§£è¾ƒå°‘æ—¶ï¼Œå®¹æ˜“å—åˆ°æš—ç¤ºè€Œç»™å‡ºè¿‡é«˜çš„ç½®ä¿¡åº¦ã€‚è¿™ç§ç½®ä¿¡åº¦æ ¡å‡†é—®é¢˜é™ä½äº†LLMè¾“å‡ºçš„å¯ä¿¡åº¦ï¼Œé™åˆ¶äº†å…¶åœ¨å®‰å…¨æ•æ„Ÿåœºæ™¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥â€œå¹²æ‰°é¡¹â€æ¥è¯„ä¼°LLMçš„â€œæš—ç¤ºæ€§â€ï¼Œå³æ¨¡å‹åœ¨é¢å¯¹ä¸ç¡®å®šä¿¡æ¯æ—¶ï¼Œæ˜¯å¦å®¹æ˜“å—åˆ°è¯¯å¯¼è€Œç»™å‡ºé«˜ç½®ä¿¡åº¦ã€‚é€šè¿‡è®©æ¨¡å‹å¯¹å¤šä¸ªè‡ªç”Ÿæˆçš„å¹²æ‰°é¡¹è¿›è¡Œç½®ä¿¡åº¦è¯„ä¼°ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»è€Œæ¶ˆé™¤æ¨¡å‹å›ºæœ‰çš„æš—ç¤ºæ€§åå·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDINCOæ–¹æ³•åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) **ç”Ÿæˆå¹²æ‰°é¡¹**ï¼šé’ˆå¯¹ç»™å®šçš„é—®é¢˜ï¼ŒLLMç”Ÿæˆå¤šä¸ªä¸åŒçš„ã€ç”šè‡³ç›¸äº’çŸ›ç›¾çš„ç­”æ¡ˆï¼ˆå¹²æ‰°é¡¹ï¼‰ã€‚2) **ç½®ä¿¡åº¦è¯„ä¼°**ï¼šLLMå¯¹åŸå§‹ç­”æ¡ˆå’Œæ‰€æœ‰å¹²æ‰°é¡¹åˆ†åˆ«è¿›è¡Œç½®ä¿¡åº¦è¯„ä¼°ï¼Œå¾—åˆ°ä¸€ç³»åˆ—ç½®ä¿¡åº¦åˆ†æ•°ã€‚3) **å½’ä¸€åŒ–å¤„ç†**ï¼šå°†åŸå§‹ç­”æ¡ˆçš„ç½®ä¿¡åº¦é™¤ä»¥æ‰€æœ‰ç­”æ¡ˆï¼ˆåŒ…æ‹¬åŸå§‹ç­”æ¡ˆå’Œå¹²æ‰°é¡¹ï¼‰çš„ç½®ä¿¡åº¦ä¹‹å’Œï¼Œå¾—åˆ°å½’ä¸€åŒ–åçš„ç½®ä¿¡åº¦ã€‚4) **ç”Ÿæˆå™¨-éªŒè¯å™¨ä¸€è‡´æ€§**ï¼šç»“åˆç”Ÿæˆå™¨å’ŒéªŒè¯å™¨ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼Œè¿›ä¸€æ­¥æ ¡å‡†ç½®ä¿¡åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šDINCOçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **åˆ©ç”¨è‡ªç”Ÿæˆå¹²æ‰°é¡¹è¯„ä¼°æš—ç¤ºæ€§åå·®**ï¼šé€šè¿‡è®©æ¨¡å‹å¯¹å¤šä¸ªä¸åŒçš„ç­”æ¡ˆè¿›è¡Œè¯„ä¼°ï¼Œä»è€Œé‡åŒ–æ¨¡å‹å¯¹ä¸ç¡®å®šä¿¡æ¯çš„æ•æ„Ÿç¨‹åº¦ã€‚2) **å½’ä¸€åŒ–å¤„ç†æ¶ˆé™¤åå·®**ï¼šé€šè¿‡å½’ä¸€åŒ–å¤„ç†ï¼Œé™ä½æ¨¡å‹åœ¨ä¸ç¡®å®šæƒ…å†µä¸‹ç»™å‡ºé«˜ç½®ä¿¡åº¦çš„å¯èƒ½æ€§ï¼Œä»è€Œæé«˜ç½®ä¿¡åº¦æ ¡å‡†çš„å‡†ç¡®æ€§ã€‚3) **æ•´åˆç”Ÿæˆå™¨-éªŒè¯å™¨ä¸ä¸€è‡´æ€§**ï¼šå°†ç”Ÿæˆå™¨å’ŒéªŒè¯å™¨çš„ä¿¡æ¯ç»“åˆèµ·æ¥ï¼Œè¿›ä¸€æ­¥æé«˜ç½®ä¿¡åº¦è¯„ä¼°çš„å¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼Œå¹²æ‰°é¡¹çš„æ•°é‡æ˜¯ä¸€ä¸ªé‡è¦çš„å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚æ­¤å¤–ï¼Œç”Ÿæˆå™¨-éªŒè¯å™¨ä¸€è‡´æ€§çš„å…·ä½“å®ç°æ–¹å¼ä¹Ÿä¼šå½±å“æœ€ç»ˆçš„æ€§èƒ½ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†å¸¸è§çš„è‡ªæ´½æ€§æ–¹æ³•ä½œä¸ºç”Ÿæˆå™¨ä¸€è‡´æ€§çš„åº¦é‡ï¼Œå¹¶å°†å…¶ä¸å½’ä¸€åŒ–çš„éªŒè¯å™¨ç½®ä¿¡åº¦ç›¸ç»“åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDINCOæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜LLMç½®ä¿¡åº¦æ ¡å‡†çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹å¯¹æŸäº›é—®é¢˜äº†è§£è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚DINCOåœ¨ä»…ä½¿ç”¨10æ¬¡æ¨ç†è°ƒç”¨æ—¶ï¼Œæ€§èƒ½å°±è¶…è¿‡äº†ä½¿ç”¨100æ¬¡æ¨ç†è°ƒç”¨çš„è‡ªæ´½æ€§æ–¹æ³•ï¼Œè¡¨æ˜DINCOå…·æœ‰æ›´é«˜çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒDINCOæä¾›çš„ç½®ä¿¡åº¦ä¼°è®¡æ›´åŠ å¯é ï¼Œä¸æ˜“é¥±å’Œï¼Œæ›´æ˜“äºç”¨æˆ·ç†è§£å’Œä½¿ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦LLMæä¾›å¯é ç½®ä¿¡åº¦è¯„ä¼°çš„åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èé£é™©è¯„ä¼°ã€æ³•å¾‹å’¨è¯¢ç­‰ã€‚é€šè¿‡æé«˜LLMç½®ä¿¡åº¦æ ¡å‡†çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·å¯¹LLMè¾“å‡ºçš„ä¿¡ä»»ï¼Œå¹¶é™ä½å› é”™è¯¯ä¿¡æ¯å¯¼è‡´çš„é£é™©ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ¨¡å‹å’Œä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.

