---
layout: default
title: Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model
---

# Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25543" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25543v1</a>
  <a href="https://arxiv.org/pdf/2509.25543.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25543v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25543v1', 'Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fahim Faisal, Kaiqiang Song, Song Wang, Simin Ma, Shujian Liu, Haoyun Deng, Sathish Reddy Indurthi

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPB-RLSVRæ¡†æ¶ï¼Œåˆ©ç”¨é«˜èµ„æºä¸“å®¶æ¨¡å‹æå‡å¤šè¯­è¨€LLMçš„æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `æ¢è½´å­¦ä¹ ` `è¯­ä¹‰ç›¸ä¼¼åº¦` `è·¨è¯­è¨€è¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•æå‡LLMæ¨ç†èƒ½åŠ›ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œå¯¼è‡´è·¨è¯­è¨€æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®è·ã€‚
2. æå‡ºPB-RLSVRæ¡†æ¶ï¼Œåˆ©ç”¨é«˜æ€§èƒ½è‹±è¯­LLMä½œä¸ºæ¢è½´ï¼Œé€šè¿‡è¯­ä¹‰ç­‰ä»·æ€§å¥–åŠ±è¿ç§»æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPB-RLSVRæ˜¾è‘—æå‡äº†Llama-3.1-8B-Instructå’ŒQwen3-32Bçš„å¤šè¯­è¨€æ¨ç†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŸºäºæ¢è½´çš„è¯­ä¹‰å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆPB-RLSVRï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œè§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä¸Šçš„æ€§èƒ½å·®è·é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é«˜æ€§èƒ½çš„è‹±è¯­LLMä½œä¸ºâ€œæ¢è½´â€æ¨¡å‹ï¼Œä¸ºæ¨ç†ä»»åŠ¡ç”Ÿæˆå‚è€ƒç­”æ¡ˆã€‚ç„¶åï¼ŒåŸºäºå¤šè¯­è¨€æ¨¡å‹å“åº”ä¸è‹±è¯­å‚è€ƒç­”æ¡ˆçš„è¯­ä¹‰ç­‰ä»·æ€§æ¥å¥–åŠ±è¯¥æ¨¡å‹ï¼Œä»è€Œæœ‰æ•ˆåœ°å°†æ¢è½´æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¿ç§»åˆ°å…¶ä»–è¯­è¨€ã€‚ç ”ç©¶äººå‘˜æ¢ç´¢äº†å‡ ç§è·¨è¯­è¨€è¯­ä¹‰å¥–åŠ±å‡½æ•°ï¼ŒåŒ…æ‹¬åŸºäºåµŒå…¥å’Œæœºå™¨ç¿»è¯‘çš„æ–¹æ³•ã€‚åœ¨å¤šè¯­è¨€æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPB-RLSVRæ˜¾è‘—ç¼©å°äº†è‹±è¯­å’Œå…¶ä»–è¯­è¨€ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå¤§å¹…ä¼˜äºä¼ ç»Ÿçš„PPOåŸºçº¿ã€‚å…·ä½“è€Œè¨€ï¼ŒPB-RLSVRæ¡†æ¶åˆ†åˆ«å°†Llama-3.1-8B-Instructå’ŒQwen3-32Bçš„å¹³å‡å¤šè¯­è¨€æ€§èƒ½æé«˜äº†16.41%å’Œ10.17%ï¼Œå±•ç¤ºäº†ä¸€ç§å¼ºå¤§ä¸”æ•°æ®é«˜æ•ˆçš„æ–¹æ³•æ¥æ„å»ºçœŸæ­£çš„å¤šè¯­è¨€æ¨ç†æ™ºèƒ½ä½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›è¿›å±•ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä¸Šã€‚å¯¹äºå…¶ä»–è¯­è¨€ï¼Œç”±äºç¼ºä¹é«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®ï¼ŒLLMçš„æ¨ç†èƒ½åŠ›ç›¸å¯¹è¾ƒå¼±ï¼Œå¯¼è‡´è·¨è¯­è¨€æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®è·ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°æå‡LLMåœ¨å¤šç§è¯­è¨€ä¸Šçš„æ¨ç†èƒ½åŠ›æ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªé«˜æ€§èƒ½çš„è‹±è¯­LLMä½œä¸ºâ€œæ¢è½´â€æ¨¡å‹ï¼Œå°†å…¶å¼ºå¤§çš„æ¨ç†èƒ½åŠ›è¿ç§»åˆ°å…¶ä»–è¯­è¨€ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆä½¿ç”¨è‹±è¯­LLMä¸ºæ¨ç†ä»»åŠ¡ç”Ÿæˆé«˜è´¨é‡çš„å‚è€ƒç­”æ¡ˆï¼Œç„¶åé€šè¿‡æ¯”è¾ƒå¤šè¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆä¸è‹±è¯­å‚è€ƒç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼åº¦æ¥è¯„ä¼°å¤šè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºå¥–åŠ±ä¿¡å·æ¥è®­ç»ƒå¤šè¯­è¨€æ¨¡å‹ã€‚è¿™æ ·ï¼Œå°±å¯ä»¥é¿å…ç›´æ¥åœ¨ç›®æ ‡è¯­è¨€ä¸Šæ ‡æ³¨å¤§é‡æ•°æ®çš„éœ€æ±‚ï¼Œä»è€Œå®ç°æ•°æ®é«˜æ•ˆçš„å¤šè¯­è¨€æ¨ç†èƒ½åŠ›æå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPB-RLSVRæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **æ¢è½´æ¨¡å‹ç”Ÿæˆå‚è€ƒç­”æ¡ˆ**ï¼šä½¿ç”¨é«˜æ€§èƒ½çš„è‹±è¯­LLMå¯¹ç»™å®šçš„æ¨ç†ä»»åŠ¡ç”Ÿæˆå‚è€ƒç­”æ¡ˆã€‚2) **å¤šè¯­è¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ**ï¼šä½¿ç”¨å¾…è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹å¯¹ç›¸åŒçš„æ¨ç†ä»»åŠ¡ç”Ÿæˆç­”æ¡ˆã€‚3) **è¯­ä¹‰å¥–åŠ±è®¡ç®—**ï¼šè®¡ç®—å¤šè¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆä¸è‹±è¯­å‚è€ƒç­”æ¡ˆä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚è¿™é‡Œå¯ä»¥ä½¿ç”¨å¤šç§è·¨è¯­è¨€è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼Œä¾‹å¦‚åŸºäºåµŒå…¥çš„æ–¹æ³•æˆ–åŸºäºæœºå™¨ç¿»è¯‘çš„æ–¹æ³•ã€‚4) **å¼ºåŒ–å­¦ä¹ è®­ç»ƒ**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚PPOï¼‰æ ¹æ®è¯­ä¹‰å¥–åŠ±ä¿¡å·æ¥è®­ç»ƒå¤šè¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆçš„ç­”æ¡ˆåœ¨è¯­ä¹‰ä¸Šæ›´æ¥è¿‘è‹±è¯­å‚è€ƒç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨äº†é«˜èµ„æºè¯­è¨€ï¼ˆè‹±è¯­ï¼‰çš„LLMæ¥æŒ‡å¯¼ä½èµ„æºè¯­è¨€çš„LLMè¿›è¡Œæ¨ç†èƒ½åŠ›å­¦ä¹ ï¼Œä»è€Œé¿å…äº†åœ¨ä½èµ„æºè¯­è¨€ä¸Šæ ‡æ³¨å¤§é‡æ•°æ®çš„éœ€æ±‚ã€‚è¿™ç§åŸºäºæ¢è½´æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†é«˜èµ„æºè¯­è¨€çš„çŸ¥è¯†è¿ç§»åˆ°ä½èµ„æºè¯­è¨€ï¼Œä»è€Œæå‡LLMåœ¨å¤šç§è¯­è¨€ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒPB-RLSVRä¸éœ€è¦äººå·¥æ ‡æ³¨çš„å¥–åŠ±ä¿¡å·ï¼Œè€Œæ˜¯é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦è‡ªåŠ¨ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œä»è€Œå¤§å¤§é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯­ä¹‰å¥–åŠ±è®¡ç®—æ–¹é¢ï¼Œè®ºæ–‡æ¢ç´¢äº†å‡ ç§ä¸åŒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºè·¨è¯­è¨€åµŒå…¥çš„æ–¹æ³•å’ŒåŸºäºæœºå™¨ç¿»è¯‘çš„æ–¹æ³•ã€‚åŸºäºåµŒå…¥çš„æ–¹æ³•ç›´æ¥è®¡ç®—å¤šè¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆå’Œè‹±è¯­å‚è€ƒç­”æ¡ˆçš„åµŒå…¥å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚åŸºäºæœºå™¨ç¿»è¯‘çš„æ–¹æ³•é¦–å…ˆå°†å¤šè¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆç¿»è¯‘æˆè‹±è¯­ï¼Œç„¶åå†è®¡ç®—ç¿»è¯‘åçš„ç­”æ¡ˆä¸è‹±è¯­å‚è€ƒç­”æ¡ˆä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨äº†PPOç®—æ³•ï¼Œå¹¶å¯¹å¥–åŠ±å‡½æ•°è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPB-RLSVRæ¡†æ¶æ˜¾è‘—æå‡äº†LLMçš„å¤šè¯­è¨€æ¨ç†æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼ŒPB-RLSVRåˆ†åˆ«å°†Llama-3.1-8B-Instructå’ŒQwen3-32Bçš„å¹³å‡å¤šè¯­è¨€æ€§èƒ½æé«˜äº†16.41%å’Œ10.17%ï¼Œå¤§å¹…ä¼˜äºä¼ ç»Ÿçš„PPOåŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒPB-RLSVRæ˜¯ä¸€ç§æœ‰æ•ˆä¸”æ•°æ®é«˜æ•ˆçš„å¤šè¯­è¨€æ¨ç†èƒ½åŠ›æå‡æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤šè¯­è¨€æ™ºèƒ½å®¢æœã€å¤šè¯­è¨€æœºå™¨ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡LLMçš„å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æœåŠ¡äºå…¨çƒç”¨æˆ·ï¼Œä¿ƒè¿›ä¸åŒè¯­è¨€æ–‡åŒ–ä¹‹é—´çš„äº¤æµä¸ç†è§£ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤šçš„è¯­è¨€å’Œä»»åŠ¡ï¼Œæ„å»ºæ›´åŠ é€šç”¨å’Œå¼ºå¤§çš„å¤šè¯­è¨€æ™ºèƒ½ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While reinforcement learning has advanced the reasoning abilities of Large Language Models (LLMs), these gains are largely confined to English, creating a significant performance disparity across languages. To address this, we introduce Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by circumventing the need for human-annotated data in target languages. Our approach employs a high-performing English LLM as a "pivot" model to generate reference responses for reasoning tasks. A multilingual model is then rewarded based on the semantic equivalence of its responses to the English reference, effectively transferring the pivot model's reasoning capabilities across languages. We investigate several cross-lingual semantic reward functions, including those based on embeddings and machine translation. Extensive experiments on a suite of multilingual reasoning benchmarks show that our method significantly narrows the performance gap between English and other languages, substantially outperforming traditional PPO baselines. Specifically, our PB-RLSVR framework improves the average multilingual performance of Llama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively, demonstrating a powerful and data-efficient approach to building truly multilingual reasoning agents.

