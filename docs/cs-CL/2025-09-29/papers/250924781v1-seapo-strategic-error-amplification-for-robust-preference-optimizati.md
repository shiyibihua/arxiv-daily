---
layout: default
title: SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models
---

# SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24781" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24781v1</a>
  <a href="https://arxiv.org/pdf/2509.24781.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24781v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24781v1', 'SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jun Rao, Yunjie Liao, Xuebo Liu, Zepeng Lin, Lian Lian, Dong Jin, Shengjun Cheng, Jun Yu, Min Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: EMNLP 2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SeaPOï¼šé€šè¿‡ç­–ç•¥æ€§è¯¯å·®æ”¾å¤§å¢å¼ºå¤§è¯­è¨€æ¨¡å‹åå¥½ä¼˜åŒ–çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `åå¥½ä¼˜åŒ–` `è¯¯å·®æ”¾å¤§` `é²æ£’æ€§` `å¯¹é½` `è´Ÿæ ·æœ¬æŒ–æ˜` `ç­–ç•¥æ€§å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åå¥½ä¼˜åŒ–æ–¹æ³•åœ¨æ­£è´Ÿæ ·æœ¬è´¨é‡æ¥è¿‘æ—¶é¢ä¸´ä¼˜åŒ–å›°éš¾ï¼Œé™åˆ¶äº†å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚
2. SeaPOé€šè¿‡ç­–ç•¥æ€§åœ°å¼•å…¥ç‰¹å®šç±»å‹çš„è¯¯å·®åˆ°è´Ÿæ ·æœ¬ä¸­ï¼Œç¡®ä¿è´Ÿæ ·æœ¬è´¨é‡ä½äºæ­£æ ·æœ¬ï¼Œä»è€Œæ”¹å–„åå¥½å­¦ä¹ ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSeaPOæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨çœŸå®æ€§ç­‰æ–¹é¢çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹å‡æœ‰æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„ç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åå¥½ä¼˜åŒ–çš„å¯¹é½æ–¹æ³•æ—¨åœ¨é€šè¿‡åˆ©ç”¨æ­£è´Ÿæ ·æœ¬å¯¹æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹åœ¨è¯„åˆ†æˆ–ç”Ÿæˆå“åº”æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œæ­£è´Ÿæ ·æœ¬çš„è´¨é‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½å˜å¾—ç›¸ä¼¼ï¼Œè¿™ä½¿å¾—åå¥½å­¦ä¹ çš„ä¼˜åŒ–å˜å¾—å¤æ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SeaPOï¼Œä¸€ç§ç­–ç•¥æ€§è¯¯å·®æ”¾å¤§æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨LLMsä¸­å¸¸è§çš„ä¸‰ä¸ªè¯¯å·®ç±»å‹ï¼Œå°†ç‰¹å®šçš„è¯¯å·®æ¨¡å¼å¼•å…¥åˆ°æ¨¡å‹åå¥½ä¼˜åŒ–ä¸­ã€‚è¯¥ç­–ç•¥ç¡®ä¿è´Ÿæ ·æœ¬æ¯”æ­£æ ·æœ¬æ›´å…·è¯¯å·®ï¼Œå¹¶é‡‡ç”¨åŸºäºåå¥½çš„è®­ç»ƒæ¥å‡è½»è¿™äº›è¯¯å·®çš„å‘ç”Ÿï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚åœ¨äº”ä¸ªèƒ½åŠ›ç»´åº¦å’Œä¸åŒæ¨¡å‹è§„æ¨¡ï¼ˆ1.5Båˆ°14Bï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œç”Ÿæˆçš„æ•°æ®æ˜¾è‘—æé«˜äº†æ•´ä½“æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨çœŸå®æ€§æ–¹é¢ï¼Œè§‚å¯Ÿåˆ°5-10ä¸ªç™¾åˆ†ç‚¹çš„æ”¹è¿›ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œä»»åŠ¡æ€§èƒ½éšå¼•å…¥çš„è¯¯å·®ç±»å‹è€Œå˜åŒ–ã€‚æ³¨å…¥æœ€å¸¸è§çš„è¯¯å·®ç±»å‹å¯ä»¥æé«˜ç›¸å…³ä»»åŠ¡çš„æ€§èƒ½ï¼Œè€Œæ··åˆè¯¯å·®ç±»å‹ä¼šå¯¼è‡´æ›´å¹¿æ³›çš„æ€§èƒ½æå‡ï¼šå¤§å¤šæ•°ä»»åŠ¡è¡¨ç°å‡ºç¨³å®šçš„æ”¹è¿›ï¼Œè€Œå°‘æ•°ä»»åŠ¡è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åå¥½ä¼˜åŒ–ä¸­ï¼Œç”±äºæ­£è´Ÿæ ·æœ¬è´¨é‡è¶‹åŒå¯¼è‡´çš„è®­ç»ƒå›°éš¾é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåŒºåˆ†æ­£è´Ÿæ ·æœ¬ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•å……åˆ†å­¦ä¹ åˆ°æ­£ç¡®çš„åå¥½ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSeaPOçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç­–ç•¥æ€§åœ°æ”¾å¤§è´Ÿæ ·æœ¬ä¸­çš„è¯¯å·®ï¼Œäººä¸ºåœ°æ‹‰å¼€æ­£è´Ÿæ ·æœ¬çš„å·®è·ï¼Œä»è€Œä½¿æ¨¡å‹æ›´å®¹æ˜“å­¦ä¹ åˆ°æ­£ç¡®çš„åå¥½ã€‚é€šè¿‡å¼•å…¥ç‰¹å®šç±»å‹çš„è¯¯å·®ï¼Œç¡®ä¿è´Ÿæ ·æœ¬åœ¨æŸäº›æ–¹é¢æ˜æ˜¾åŠ£äºæ­£æ ·æœ¬ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹æ›´å¥½åœ°è¿›è¡ŒåŒºåˆ†å’Œå­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSeaPOçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) è¯†åˆ«å¤§è¯­è¨€æ¨¡å‹ä¸­å¸¸è§çš„è¯¯å·®ç±»å‹ï¼›2) è®¾è®¡ç­–ç•¥ï¼Œå°†è¿™äº›è¯¯å·®ç±»å‹æ³¨å…¥åˆ°è´Ÿæ ·æœ¬ä¸­ï¼Œç”Ÿæˆå¸¦æœ‰ç‰¹å®šè¯¯å·®æ¨¡å¼çš„è´Ÿæ ·æœ¬ï¼›3) ä½¿ç”¨å¸¦æœ‰è¯¯å·®æ”¾å¤§çš„æ­£è´Ÿæ ·æœ¬å¯¹ï¼Œè¿›è¡ŒåŸºäºåå¥½çš„è®­ç»ƒï¼Œä¼˜åŒ–æ¨¡å‹ï¼›4) è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒéªŒè¯SeaPOçš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSeaPOçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç­–ç•¥æ€§è¯¯å·®æ”¾å¤§æ–¹æ³•ã€‚ä¸ä»¥å¾€ä»…ä»…ä¾èµ–äºåŸå§‹æ­£è´Ÿæ ·æœ¬è¿›è¡Œè®­ç»ƒçš„æ–¹æ³•ä¸åŒï¼ŒSeaPOä¸»åŠ¨å¹²é¢„æ ·æœ¬è´¨é‡ï¼Œé€šè¿‡å¼•å…¥ç‰¹å®šç±»å‹çš„è¯¯å·®æ¥å¢å¼ºè´Ÿæ ·æœ¬çš„åŒºåˆ†åº¦ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹å­¦ä¹ åå¥½ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå…‹æœæ­£è´Ÿæ ·æœ¬è´¨é‡è¶‹åŒå¸¦æ¥çš„è®­ç»ƒå›°éš¾ï¼Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šSeaPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è¯¯å·®ç±»å‹çš„é€‰æ‹©ï¼šé€‰æ‹©å¤§è¯­è¨€æ¨¡å‹ä¸­å¸¸è§çš„ã€æ˜“äºè¯†åˆ«å’Œæ³¨å…¥çš„è¯¯å·®ç±»å‹ï¼Œä¾‹å¦‚äº‹å®æ€§é”™è¯¯ã€é€»è¾‘é”™è¯¯å’Œé£æ ¼ä¸ä¸€è‡´ç­‰ï¼›2) è¯¯å·®æ³¨å…¥ç­–ç•¥ï¼šè®¾è®¡åˆç†çš„ç­–ç•¥ï¼Œç¡®ä¿è¯¯å·®æ³¨å…¥çš„ç¨‹åº¦é€‚ä¸­ï¼Œæ—¢èƒ½æ”¾å¤§æ­£è´Ÿæ ·æœ¬çš„å·®è·ï¼Œåˆä¸ä¼šä½¿è´Ÿæ ·æœ¬è¿‡äºç¦»è°±ï¼Œå¤±å»è®­ç»ƒä»·å€¼ï¼›3) åå¥½è®­ç»ƒæ–¹æ³•ï¼šé‡‡ç”¨åˆé€‚çš„åå¥½è®­ç»ƒæ–¹æ³•ï¼Œä¾‹å¦‚pairwise ranking lossæˆ–margin ranking lossï¼Œæ¥ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†æ­£è´Ÿæ ·æœ¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSeaPOåœ¨å¤šä¸ªèƒ½åŠ›ç»´åº¦ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨çœŸå®æ€§æ–¹é¢ï¼Œè§‚å¯Ÿåˆ°5-10ä¸ªç™¾åˆ†ç‚¹çš„æ”¹è¿›ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒè¯¯å·®ç±»å‹å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“ï¼Œå‘ç°æ³¨å…¥æœ€å¸¸è§çš„è¯¯å·®ç±»å‹å¯ä»¥æé«˜ç›¸å…³ä»»åŠ¡çš„æ€§èƒ½ï¼Œè€Œæ··åˆè¯¯å·®ç±»å‹ä¼šå¯¼è‡´æ›´å¹¿æ³›çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœéªŒè¯äº†SeaPOçš„æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SeaPOæ–¹æ³•å¯åº”ç”¨äºå„ç§éœ€è¦åå¥½ä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„åå¥½å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æœŸæœ›ã€æ›´å‡†ç¡®ã€æ›´å¯é çš„è¾“å‡ºã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯ç”¨æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ï¼Œå¹¶ä¸ºæœªæ¥çš„åå¥½ä¼˜åŒ–ç ”ç©¶æä¾›æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing alignment methods for preference optimization of large language models (LLMs) aim to enhance model performance by utilizing pairs of positive and negative samples. However, due to the limited capacity of models in scoring or generating responses, the quality of positive and negative samples may become similar during training, which complicates optimization for preference learning. To address this issue, we introduce SeaPO, a Strategic Error Amplification method that leverages three error types commonly occurring in LLMs to introduce specific error patterns into the model Preference Optimization. This strategy ensures that negative samples are more erroneous than positive samples and preference-based training is employed to mitigate the occurrence of these errors, thereby enhancing model performance. Evaluations across five capability dimensions and different model scales (1.5B to 14B) demonstrate that the generated data significantly improved overall model performance, particularly in terms of truthfulness, with improvements of 5-10 percentage points observed. Further analysis reveals that task performance varies depending on the error types introduced. Injecting the most common error types improves performance in related tasks, while a mix of error types leads to a broader performance enhancement: most tasks show stable improvements, while a few tasks exhibit significant gains.

