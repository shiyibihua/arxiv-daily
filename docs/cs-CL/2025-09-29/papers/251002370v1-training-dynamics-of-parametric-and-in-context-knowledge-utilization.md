---
layout: default
title: Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models
---

# Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02370" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02370v1</a>
  <a href="https://arxiv.org/pdf/2510.02370.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02370v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02370v1', 'Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minsung Kim, Dong-Kyum Kim, Jea Kwon, Nakyeong Yang, Kyomin Jung, Meeyoung Cha

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: 16 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è®­ç»ƒæ¡ä»¶å¯¹è¯­è¨€æ¨¡å‹å‚æ•°åŒ–çŸ¥è¯†å’Œä¸Šä¸‹æ–‡çŸ¥è¯†åˆ©ç”¨çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `çŸ¥è¯†ä»²è£` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å‚æ•°åŒ–çŸ¥è¯†` `æ£€ç´¢å¢å¼ºç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨åˆ©ç”¨ä¸Šä¸‹æ–‡çŸ¥è¯†å’Œå‚æ•°åŒ–çŸ¥è¯†æ—¶å­˜åœ¨å†²çªï¼Œç¼ºä¹å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­çŸ¥è¯†ä»²è£ç­–ç•¥çš„ç³»ç»Ÿç†è§£ã€‚
2. è¯¥ç ”ç©¶é€šè¿‡æ§åˆ¶è®­ç»ƒæ¡ä»¶ï¼Œç ”ç©¶äº†å…¶å¯¹æ¨¡å‹åˆ©ç”¨ä¸Šä¸‹æ–‡çŸ¥è¯†å’Œå‚æ•°åŒ–çŸ¥è¯†çš„å½±å“ï¼Œä»¥åŠæ¨¡å‹å¦‚ä½•åœ¨ä¸¤è€…ä¹‹é—´è¿›è¡Œä»²è£ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ–‡æ¡£å†…äº‹å®é‡å¤å’ŒåŒ…å«ä¸ä¸€è‡´ä¿¡æ¯çš„è¯­æ–™åº“è®­ç»ƒï¼Œæœ‰åŠ©äºæ¨¡å‹å‘å±•æ›´å¼ºå¤§çš„çŸ¥è¯†ä»²è£ç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶ç»å¸¸é‡åˆ°ä¸Šä¸‹æ–‡æ£€ç´¢çŸ¥è¯†ä¸é¢„è®­ç»ƒæœŸé—´è·å¾—çš„å‚æ•°åŒ–çŸ¥è¯†ä¹‹é—´çš„å†²çªã€‚ç›²ç›®æ¥å—å¤–éƒ¨çŸ¥è¯†çš„æ¨¡å‹å®¹æ˜“å—åˆ°é”™è¯¯ä¿¡æ¯çš„å½±å“ï¼Œè€Œä¸¥æ ¼éµå®ˆå‚æ•°åŒ–çŸ¥è¯†çš„æ¨¡å‹åˆ™æ— æ³•ä»æ£€ç´¢ä¸­è·ç›Šã€‚å°½ç®¡æ£€ç´¢å¢å¼ºç”Ÿæˆå·²è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†æˆ‘ä»¬ä»ç„¶ç¼ºä¹å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­çŸ¥è¯†ä»²è£ç­–ç•¥å½¢æˆå› ç´ çš„ç³»ç»Ÿç†è§£ã€‚è¿™ç§å·®è·å¯èƒ½å¯¼è‡´é¢„è®­ç»ƒæ¨¡å‹å…·æœ‰ä¸è‰¯çš„ä»²è£è¡Œä¸ºï¼Œä»è€Œæµªè´¹å¤§é‡çš„è®¡ç®—èµ„æºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–æ¬¡å¯¹è®­ç»ƒæ¡ä»¶å¦‚ä½•å½±å“æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çŸ¥è¯†å’Œå‚æ•°åŒ–çŸ¥è¯†çš„åˆ©ç”¨ä»¥åŠå®ƒä»¬å¦‚ä½•åœ¨ä¸¤è€…ä¹‹é—´è¿›è¡Œä»²è£è¿›è¡Œäº†å—æ§ç ”ç©¶ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªåˆæˆçš„ä¼ è®°è¯­æ–™åº“ä¸Šè®­ç»ƒåŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶ç³»ç»Ÿåœ°æ§åˆ¶å„ç§æ¡ä»¶ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ–‡æ¡£å†…äº‹å®çš„é‡å¤ä¿ƒè¿›äº†å‚æ•°åŒ–å’Œä¸Šä¸‹æ–‡èƒ½åŠ›çš„å‘å±•ã€‚æ­¤å¤–ï¼Œåœ¨åŒ…å«ä¸ä¸€è‡´ä¿¡æ¯æˆ–åˆ†å¸ƒåå·®çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé¼“åŠ±æ¨¡å‹å¼€å‘åˆ©ç”¨å‚æ•°åŒ–å’Œä¸Šä¸‹æ–‡çŸ¥è¯†çš„å¼ºå¤§ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™äº›éç†æƒ³å±æ€§å¯¹äºå­¦ä¹ é²æ£’çš„ä»²è£è‡³å…³é‡è¦ï¼Œè€Œä¸æ˜¯å°†å…¶è§†ä¸ºéœ€è¦æ¶ˆé™¤çš„ä¼ªå½±ã€‚è¿™äº›è§è§£ä¸ºé¢„è®­ç»ƒæ¨¡å‹æä¾›äº†å…·ä½“çš„ç»éªŒæŒ‡å¯¼ï¼Œä½¿å…¶èƒ½å¤Ÿå’Œè°åœ°æ•´åˆå‚æ•°åŒ–å’Œä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨ä»ä¸Šä¸‹æ–‡ä¸­æ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼Œå¹¶ä¸é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—çš„å‚æ•°åŒ–çŸ¥è¯†è¿›è¡Œèåˆå’Œä»²è£ï¼Œæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹å¯¹è®­ç»ƒè¿‡ç¨‹å¦‚ä½•å½±å“è¿™ç§çŸ¥è¯†ä»²è£ç­–ç•¥çš„ç³»ç»Ÿæ€§ç†è§£ï¼Œå¯¼è‡´æ¨¡å‹å¯èƒ½è¿‡åº¦ä¾èµ–æŸä¸€ç§çŸ¥è¯†æ¥æºï¼Œä»è€Œå½±å“æ¨¡å‹çš„æ€§èƒ½å’Œå¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªå¯æ§çš„å®éªŒç¯å¢ƒï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶ä¸åŒçš„è®­ç»ƒæ¡ä»¶å¦‚ä½•å½±å“æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çŸ¥è¯†å’Œå‚æ•°åŒ–çŸ¥è¯†çš„åˆ©ç”¨ã€‚é€šè¿‡åˆ†ææ¨¡å‹åœ¨ä¸åŒè®­ç»ƒæ¡ä»¶ä¸‹çš„è¡Œä¸ºï¼Œæ­ç¤ºå½±å“çŸ¥è¯†ä»²è£ç­–ç•¥çš„å…³é”®å› ç´ ï¼Œå¹¶ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜åŒ–æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶é‡‡ç”¨åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨ä¸€ä¸ªåˆæˆçš„ä¼ è®°è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥è¯­æ–™åº“åŒ…å«å¯æ§çš„äº‹å®é‡å¤ã€ä¸ä¸€è‡´ä¿¡æ¯å’Œåˆ†å¸ƒåå·®ç­‰ç‰¹å¾ï¼Œç”¨äºæ¨¡æ‹Ÿä¸åŒçš„è®­ç»ƒæ¡ä»¶ã€‚é€šè¿‡æ§åˆ¶è¿™äº›æ¡ä»¶ï¼Œç ”ç©¶äººå‘˜å¯ä»¥è§‚å¯Ÿæ¨¡å‹åœ¨åˆ©ç”¨ä¸Šä¸‹æ–‡çŸ¥è¯†å’Œå‚æ•°åŒ–çŸ¥è¯†æ—¶çš„è¡Œä¸ºå˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„åˆ›æ–°ä¹‹å¤„åœ¨äºï¼Œå®ƒé¦–æ¬¡å¯¹è®­ç»ƒæ¡ä»¶å¦‚ä½•å½±å“è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ä»²è£ç­–ç•¥è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„ç ”ç©¶ã€‚é€šè¿‡æ„å»ºå¯æ§çš„å®éªŒç¯å¢ƒï¼Œæ­ç¤ºäº†æ–‡æ¡£å†…äº‹å®é‡å¤å’ŒåŒ…å«ä¸ä¸€è‡´ä¿¡æ¯çš„è¯­æ–™åº“è®­ç»ƒå¯¹æ¨¡å‹çŸ¥è¯†ä»²è£èƒ½åŠ›çš„é‡è¦ä½œç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ï¼Œç ”ç©¶äººå‘˜ç²¾å¿ƒè®¾è®¡äº†åˆæˆä¼ è®°è¯­æ–™åº“ï¼Œæ§åˆ¶äº†äº‹å®é‡å¤çš„é¢‘ç‡ã€ä¸ä¸€è‡´ä¿¡æ¯çš„æ¯”ä¾‹ä»¥åŠæ•°æ®åˆ†å¸ƒçš„åå·®ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜è®¾è®¡äº†ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çŸ¥è¯†å’Œå‚æ•°åŒ–çŸ¥è¯†çš„åˆ©ç”¨ç¨‹åº¦ï¼Œä»¥åŠæ¨¡å‹åœ¨ä¸¤è€…ä¹‹é—´è¿›è¡Œä»²è£çš„èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ä¸æ ‡å‡†çš„Transformeræ¨¡å‹ä¿æŒä¸€è‡´ï¼Œé‡ç‚¹åœ¨äºè®­ç»ƒæ•°æ®çš„è®¾è®¡å’Œåˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ–‡æ¡£å†…äº‹å®çš„é‡å¤ä¿ƒè¿›äº†å‚æ•°åŒ–å’Œä¸Šä¸‹æ–‡èƒ½åŠ›çš„å‘å±•ã€‚æ­¤å¤–ï¼Œåœ¨åŒ…å«ä¸ä¸€è‡´ä¿¡æ¯æˆ–åˆ†å¸ƒåå·®çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé¼“åŠ±æ¨¡å‹å¼€å‘åˆ©ç”¨å‚æ•°åŒ–å’Œä¸Šä¸‹æ–‡çŸ¥è¯†çš„å¼ºå¤§ç­–ç•¥ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œéç†æƒ³çš„è®­ç»ƒæ•°æ®å¯¹äºå­¦ä¹ é²æ£’çš„çŸ¥è¯†ä»²è£è‡³å…³é‡è¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ä¸Šä¸‹æ–‡çŸ¥è¯†å’Œå‚æ•°åŒ–çŸ¥è¯†ï¼Œæé«˜æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œå¯é æ€§ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºæ”¹è¿›æ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œå¹¶é¿å…å—åˆ°é”™è¯¯ä¿¡æ¯çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¸ºæ„å»ºæ›´é²æ£’ã€æ›´å¯ä¿¡çš„AIç³»ç»Ÿæä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models often encounter conflicts between in-context knowledge retrieved at inference time and parametric knowledge acquired during pretraining. Models that accept external knowledge uncritically are vulnerable to misinformation, whereas models that adhere rigidly to parametric knowledge fail to benefit from retrieval. Despite the widespread adoption of retrieval-augmented generation, we still lack a systematic understanding of what shapes knowledge-arbitration strategies during training. This gap risks producing pretrained models with undesirable arbitration behaviors and, consequently, wasting substantial computational resources after the pretraining budget has already been spent. To address this problem, we present the first controlled study of how training conditions influence models' use of in-context and parametric knowledge, and how they arbitrate between them. We train transformer-based language models on a synthetic biographies corpus while systematically controlling various conditions. Our experiments reveal that intra-document repetition of facts fosters the development of both parametric and in-context capabilities. Moreover, training on a corpus that contains inconsistent information or distributional skew encourages models to develop robust strategies for leveraging parametric and in-context knowledge. Rather than viewing these non-ideal properties as artifacts to remove, our results indicate that they are important for learning robust arbitration. These insights offer concrete, empirical guidance for pretraining models that harmoniously integrate parametric and in-context knowledge.

