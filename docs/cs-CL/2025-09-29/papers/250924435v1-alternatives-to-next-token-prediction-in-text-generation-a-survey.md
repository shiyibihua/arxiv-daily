---
layout: default
title: Alternatives To Next Token Prediction In Text Generation -- A Survey
---

# Alternatives To Next Token Prediction In Text Generation -- A Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24435" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24435v1</a>
  <a href="https://arxiv.org/pdf/2509.24435.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24435v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24435v1', 'Alternatives To Next Token Prediction In Text Generation -- A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Charlie Wyatt, Aditya Joshi, Flora Salim

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°ï¼šæ¢ç´¢æ–‡æœ¬ç”Ÿæˆä¸­ä¸‹ä¸€è¯é¢„æµ‹çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåº”å¯¹LLMçš„å›ºæœ‰ç¼ºé™·ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸‹ä¸€è¯é¢„æµ‹` `å¤šè¯é¢„æµ‹` `æ½œåœ¨æ¨ç†` `è¿ç»­ç”Ÿæˆ` `éTransformeræ¶æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹ä¾èµ–çš„ä¸‹ä¸€è¯é¢„æµ‹ï¼ˆNTPï¼‰å­˜åœ¨é•¿æœŸè§„åˆ’ä¸è¶³ã€è¯¯å·®ç´¯ç§¯ç­‰é—®é¢˜ã€‚
2. è¯¥ç»¼è¿°æ¢ç´¢äº†äº”ç±»NTPæ›¿ä»£æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¤šè¯é¢„æµ‹ã€å…ˆè§„åˆ’åç”Ÿæˆã€æ½œåœ¨æ¨ç†ç­‰ã€‚
3. é€šè¿‡å¯¹è¿™äº›æ–¹æ³•çš„åˆ†ç±»å’Œç»¼åˆï¼Œæ—¨åœ¨æŒ‡å¯¼ç ”ç©¶äººå‘˜å¼€å‘æ›´å¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸‹ä¸€è¯é¢„æµ‹ï¼ˆNTPï¼‰èŒƒå¼æ¨åŠ¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç©ºå‰æˆåŠŸï¼Œä½†åŒæ—¶ä¹Ÿå¯¼è‡´äº†å…¶æœ€é¡½å›ºçš„å¼±ç‚¹ï¼Œå¦‚é•¿æœŸè§„åˆ’èƒ½åŠ›å·®ã€è¯¯å·®ç´¯ç§¯å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡å¯¹NTPçš„æ›¿ä»£æ–¹æ¡ˆè¿›è¡Œäº†ç»¼è¿°ï¼Œæè¿°äº†æ–°å…´çš„æ›¿ä»£NTPçš„ç”Ÿæ€ç³»ç»Ÿã€‚æˆ‘ä»¬å°†è¿™äº›æ–¹æ³•åˆ†ä¸ºäº”ä¸ªä¸»è¦ç±»åˆ«ï¼šï¼ˆ1ï¼‰å¤šè¯é¢„æµ‹ï¼Œå®ƒé’ˆå¯¹çš„æ˜¯æœªæ¥çš„ä¸€ç»„è¯ï¼Œè€Œä¸æ˜¯å•ä¸ªè¯ï¼›ï¼ˆ2ï¼‰å…ˆè§„åˆ’åç”Ÿæˆï¼Œé¢„å…ˆåˆ›å»ºä¸€ä¸ªå…¨å±€çš„ã€é«˜å±‚æ¬¡çš„è®¡åˆ’æ¥æŒ‡å¯¼è¯çº§åˆ«çš„è§£ç ï¼›ï¼ˆ3ï¼‰æ½œåœ¨æ¨ç†ï¼Œå°†è‡ªå›å½’è¿‡ç¨‹æœ¬èº«è½¬ç§»åˆ°è¿ç»­æ½œåœ¨ç©ºé—´ï¼›ï¼ˆ4ï¼‰è¿ç»­ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£ã€æµåŒ¹é…æˆ–åŸºäºèƒ½é‡çš„æ–¹æ³•ï¼Œç”¨è¿­ä»£çš„ã€å¹¶è¡Œçš„ç»†åŒ–ä»£æ›¿é¡ºåºç”Ÿæˆï¼›ï¼ˆ5ï¼‰éTransformeræ¶æ„ï¼Œé€šè¿‡å…¶å›ºæœ‰çš„æ¨¡å‹ç»“æ„æ¥è§„é¿NTPã€‚é€šè¿‡ç»¼åˆè¿™äº›æ–¹æ³•çš„è§è§£ï¼Œæœ¬ç»¼è¿°æä¾›äº†ä¸€ä¸ªåˆ†ç±»æ³•ï¼Œä»¥æŒ‡å¯¼ç ”ç©¶è§£å†³è¯çº§åˆ«ç”Ÿæˆçš„å·²çŸ¥å±€é™æ€§çš„æ¨¡å‹ï¼Œä»è€Œä¸ºè‡ªç„¶è¯­è¨€å¤„ç†å¼€å‘æ–°çš„å˜é©æ€§æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ç”±äºé‡‡ç”¨ä¸‹ä¸€è¯é¢„æµ‹ï¼ˆNTPï¼‰èŒƒå¼è€Œå¯¼è‡´çš„å›ºæœ‰ç¼ºé™·ï¼Œä¾‹å¦‚é•¿æœŸè§„åˆ’èƒ½åŠ›å·®ã€è¯¯å·®ç´¯ç§¯å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºé€ä¸ªtokençš„ç”Ÿæˆæ–¹å¼ï¼Œç¼ºä¹å…¨å±€è§„åˆ’å’Œé•¿æœŸä¾èµ–å»ºæ¨¡èƒ½åŠ›ï¼Œå®¹æ˜“äº§ç”Ÿè¯­ä¹‰ä¸è¿è´¯å’Œé€»è¾‘é”™è¯¯ç­‰é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è·³å‡ºä¼ ç»Ÿçš„NTPæ¡†æ¶ï¼Œæ¢ç´¢å¤šç§æ›¿ä»£æ–¹æ¡ˆï¼Œä»¥å…‹æœNTPçš„å±€é™æ€§ã€‚è¿™äº›æ›¿ä»£æ–¹æ¡ˆæ¶µç›–äº†ä»é¢„æµ‹ç›®æ ‡ã€ç”Ÿæˆæ–¹å¼åˆ°æ¨¡å‹æ¶æ„ç­‰å¤šä¸ªå±‚é¢ï¼Œæ—¨åœ¨æå‡LLMçš„ç”Ÿæˆè´¨é‡ã€æ•ˆç‡å’Œå¯æ§æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡å°†NTPçš„æ›¿ä»£æ–¹æ¡ˆåˆ†ä¸ºäº”å¤§ç±»ï¼šï¼ˆ1ï¼‰å¤šè¯é¢„æµ‹ï¼šä¸€æ¬¡é¢„æµ‹å¤šä¸ªtokenï¼Œå‡å°‘è¯¯å·®ç´¯ç§¯ï¼›ï¼ˆ2ï¼‰å…ˆè§„åˆ’åç”Ÿæˆï¼šå…ˆç”Ÿæˆå…¨å±€è®¡åˆ’ï¼Œå†æ ¹æ®è®¡åˆ’ç”Ÿæˆæ–‡æœ¬ï¼›ï¼ˆ3ï¼‰æ½œåœ¨æ¨ç†ï¼šåœ¨è¿ç»­æ½œåœ¨ç©ºé—´è¿›è¡Œæ¨ç†ï¼Œé¿å…ç¦»æ•£tokençš„é™åˆ¶ï¼›ï¼ˆ4ï¼‰è¿ç»­ç”Ÿæˆæ–¹æ³•ï¼šé‡‡ç”¨æ‰©æ•£æ¨¡å‹ç­‰æ–¹æ³•è¿›è¡Œå¹¶è¡Œç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–ï¼›ï¼ˆ5ï¼‰éTransformeræ¶æ„ï¼šä½¿ç”¨éTransformerç»“æ„ï¼Œé¿å…NTPèŒƒå¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹NTPæ›¿ä»£æ–¹æ¡ˆè¿›è¡Œäº†ç³»ç»Ÿæ€§çš„æ¢³ç†å’Œåˆ†ç±»ï¼Œå¹¶å¯¹æ¯ç§æ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæ–¹æ¡ˆçš„ç‰¹ç‚¹ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†é€‰æ‹©å’Œç»„åˆä¸åŒæ–¹æ³•çš„æŒ‡å¯¼ï¼Œä»è€Œä¿ƒè¿›äº†æ–‡æœ¬ç”Ÿæˆé¢†åŸŸçš„å‘å±•ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æ²¡æœ‰æ¶‰åŠå…·ä½“çš„æ¨¡å‹è®¾è®¡ç»†èŠ‚ï¼Œè€Œæ˜¯ä¾§é‡äºå¯¹ç°æœ‰æ–¹æ³•çš„åˆ†ç±»å’Œæ€»ç»“ã€‚å¯¹äºæ¯ç§æ›¿ä»£æ–¹æ¡ˆï¼Œè®ºæ–‡éƒ½è¯¦ç»†æè¿°äº†å…¶æ ¸å¿ƒæ€æƒ³ã€æŠ€æœ¯ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯ï¼Œå¹¶å¼•ç”¨äº†ç›¸å…³çš„ç ”ç©¶å·¥ä½œã€‚è®ºæ–‡è¿˜è®¨è®ºäº†ä¸åŒæ–¹æ¡ˆä¹‹é—´çš„è”ç³»å’ŒåŒºåˆ«ï¼Œä»¥åŠæœªæ¥å¯èƒ½çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°è®ºæ–‡ç³»ç»Ÿåœ°æ•´ç†äº†ä¸‹ä¸€è¯é¢„æµ‹ï¼ˆNTPï¼‰çš„å¤šç§æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶å°†å…¶å½’çº³ä¸ºäº”å¤§ç±»ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„è·¯çº¿å›¾ï¼Œæ–¹ä¾¿ä»–ä»¬é€‰æ‹©åˆé€‚çš„æ–¹æ³•æ¥è§£å†³LLMåœ¨æ–‡æœ¬ç”Ÿæˆä¸­é‡åˆ°çš„é—®é¢˜ã€‚è¯¥è®ºæ–‡ä¸ºæœªæ¥æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶æ–¹å‘æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆçš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€å¯¹è¯ç³»ç»Ÿã€å†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡é‡‡ç”¨NTPçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¯ä»¥æå‡ç”Ÿæˆæ–‡æœ¬çš„è¿è´¯æ€§ã€é€»è¾‘æ€§å’Œåˆ›é€ æ€§ï¼Œä»è€Œæ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¸ºç›¸å…³åº”ç”¨å¸¦æ¥æ–°çš„å‘å±•æœºé‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The paradigm of Next Token Prediction (NTP) has driven the unprecedented success of Large Language Models (LLMs), but is also the source of their most persistent weaknesses such as poor long-term planning, error accumulation, and computational inefficiency. Acknowledging the growing interest in exploring alternatives to NTP, the survey describes the emerging ecosystem of alternatives to NTP. We categorise these approaches into five main families: (1) Multi-Token Prediction, which targets a block of future tokens instead of a single one; (2) Plan-then-Generate, where a global, high-level plan is created upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the autoregressive process itself into a continuous latent space; (4) Continuous Generation Approaches, which replace sequential generation with iterative, parallel refinement through diffusion, flow matching, or energy-based methods; and (5) Non-Transformer Architectures, which sidestep NTP through their inherent model structure. By synthesizing insights across these methods, this survey offers a taxonomy to guide research into models that address the known limitations of token-level generation to develop new transformative models for natural language processing.

