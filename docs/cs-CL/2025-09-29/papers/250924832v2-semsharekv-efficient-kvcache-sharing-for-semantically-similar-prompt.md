---
layout: default
title: SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching
---

# SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24832" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.24832v2</a>
  <a href="https://arxiv.org/pdf/2509.24832.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24832v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24832v2', 'SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xinye Zhao, Spyridon Mastorakis

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-29 (Êõ¥Êñ∞: 2025-12-16)

**Â§áÊ≥®**: 11 figures, 14pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SemShareKVÔºöÈÄöËøáTokenÁ∫ßLSHÂåπÈÖç‰∏∫ËØ≠‰πâÁõ∏‰ººPromptÈ´òÊïàÂÖ±‰∫´KVCache**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `KVÁºìÂ≠òÂÖ±‰∫´` `Â±ÄÈÉ®ÊïèÊÑüÂìàÂ∏å` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜÂä†ÈÄü` `ËØ≠‰πâÁõ∏‰ººÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. LLMÊé®ÁêÜÊó∂KVÁºìÂ≠òÂç†Áî®Â§ßÈáèÂÜÖÂ≠òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ËØ≠‰πâÁõ∏‰ºº‰ΩÜËØçÊ±á‰∏çÂêåÁöÑprompt‰∏äÊïàÊûúÊúâÈôê„ÄÇ
2. SemShareKVÂà©Áî®tokenÂµåÂÖ•ÁöÑLSHÂåπÈÖçËøõË°åÊ®°Á≥ätokenÂåπÈÖçÔºåÂπ∂ÁªìÂêàRoPE‰øùÁïô‰ΩçÁΩÆ‰ø°ÊÅØÔºåÂÆûÁé∞KVÁºìÂ≠òÂÖ±‰∫´„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSemShareKVÂú®ÊëòË¶Å‰ªªÂä°‰∏äÊòæËëóÊèêÂçáÊé®ÁêÜÈÄüÂ∫¶Âπ∂Èôç‰ΩéÂÜÖÂ≠òÂç†Áî®ÔºåÂêåÊó∂‰øùËØÅËæìÂá∫Ë¥®Èáè„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËßÑÊ®°ÁöÑÊåÅÁª≠Êâ©Â§ßÔºåÊé®ÁêÜËøáÁ®ã‰∏≠ÈîÆÂÄºÔºàKVÔºâÁºìÂ≠òÁöÑÂÜÖÂ≠òÂç†Áî®Â∑≤Êàê‰∏∫‰∏Ä‰∏™ÈáçË¶ÅÁöÑÁì∂È¢à„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂéãÁº©Âçï‰∏™promptÂÜÖÁöÑKVÁºìÂ≠òÔºåÊàñÈáçÁî®Ë∑®promptÂÖ±‰∫´ÁöÑÂâçÁºÄÊàñÈ¢ëÁπÅÂá∫Áé∞ÁöÑÊñáÊú¨ÁâáÊÆµ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÁ≠ñÁï•Âú®promptËØ≠‰πâÁõ∏‰ºº‰ΩÜËØçÊ±á‰∏çÂêåÁöÑÂú∫ÊôØ‰∏≠ÂèóÂà∞ÈôêÂà∂ÔºåËøôÁßçÊÉÖÂÜµÂú®Â§öÊñáÊ°£ÊëòË¶ÅÂíåÂØπËØù‰ª£ÁêÜÁ≠â‰ªªÂä°‰∏≠ÁªèÂ∏∏ÂèëÁîü„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü	extit{SemShareKV}Ôºå‰∏Ä‰∏™KVÁºìÂ≠òÂÖ±‰∫´ÂíåÂéãÁº©Ê°ÜÊû∂ÔºåÈÄöËøáÈáçÁî®ËØ≠‰πâÁõ∏‰ººprompt‰∏≠ÁöÑKVCacheÊù•Âä†ÈÄüLLMÊé®ÁêÜ„ÄÇSemShareKV‰∏ç‰æùËµñ‰∫éÁ≤æÁ°ÆÁöÑtokenÂåπÈÖçÔºåËÄåÊòØ‰ΩøÁî®Â±ÄÈÉ®ÊïèÊÑüÂìàÂ∏åÔºàLSHÔºâÂú®tokenÂµåÂÖ•‰∏äÂ∫îÁî®Ê®°Á≥ätokenÂåπÈÖçÔºåÂπ∂ÁªìÂêàÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâ‰ª•Êõ¥Â•ΩÂú∞‰øùÁïô‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇÈÄöËøáÈÄâÊã©ÊÄßÂú∞ÈáçÁî®Êù•Ëá™ÂèÇËÄÉpromptÁºìÂ≠òÁöÑÁõ∏ÂÖ≥ÈîÆÂÄºÂØπÔºåSemShareKVÂáèÂ∞ë‰∫ÜÂÜó‰ΩôËÆ°ÁÆóÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËæìÂá∫Ë¥®Èáè„ÄÇÂú®‰∏çÂêåÁöÑÊëòË¶ÅÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÂú®5k tokenËæìÂÖ•ÁöÑÊÉÖÂÜµ‰∏ãÔºåÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü6.25ÂÄçÔºåGPUÂÜÖÂ≠ò‰ΩøÁî®ÈáèÈôç‰Ωé‰∫Ü42ÔºÖÔºåËÄåË¥®Èáè‰∏ãÈôçÂèØÂøΩÁï•‰∏çËÆ°„ÄÇËøô‰∫õÁªìÊûúÁ™ÅÂá∫‰∫ÜËØ≠‰πâÊÑüÁü•ÁºìÂ≠òÂÖ±‰∫´Âú®È´òÊïàLLMÊé®ÁêÜÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËøáÁ®ã‰∏≠ÔºåÁî±‰∫éKVÁºìÂ≠òÂç†Áî®Â§ßÈáèÂÜÖÂ≠òËÄåÂØºËá¥ÁöÑÊïàÁéáÁì∂È¢àÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÁ≤æÁ°ÆÁöÑtokenÂåπÈÖçÔºåÊó†Ê≥ïÊúâÊïàÂà©Áî®ËØ≠‰πâÁõ∏‰ºº‰ΩÜËØçÊ±á‰∏çÂêåÁöÑprompt‰πãÈó¥ÁöÑKVÁºìÂ≠òÔºåÂØºËá¥ÂÜó‰ΩôËÆ°ÁÆóÂíåÂÜÖÂ≠òÊµ™Ë¥π„ÄÇËøôÁßçÁé∞Ë±°Âú®Â§öÊñáÊ°£ÊëòË¶Å„ÄÅÂØπËØùÁ≥ªÁªüÁ≠âÂú∫ÊôØ‰∏≠Â∞§‰∏∫Á™ÅÂá∫„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSemShareKVÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËØ≠‰πâÁõ∏‰ººÊÄßÊù•ÂÖ±‰∫´KVÁºìÂ≠ò„ÄÇÂÆÉ‰∏çÂÜç‰æùËµñ‰∫éÁ≤æÁ°ÆÁöÑtokenÂåπÈÖçÔºåËÄåÊòØÈÄöËøáÊ®°Á≥äÂåπÈÖçÁöÑÊñπÂºèÔºåËØÜÂà´ËØ≠‰πâ‰∏äÁõ∏‰ººÁöÑtokenÔºåÂπ∂ÈáçÁî®Ëøô‰∫õtokenÂØπÂ∫îÁöÑKVÁºìÂ≠ò„ÄÇËøôÊ†∑ÂèØ‰ª•Âú®ÂáèÂ∞ëÂÜó‰ΩôËÆ°ÁÆóÁöÑÂêåÊó∂ÔºåÈôç‰ΩéÂÜÖÂ≠òÂç†Áî®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSemShareKVÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) **TokenÂµåÂÖ•**ÔºöÂ∞ÜËæìÂÖ•promptÂíåÂèÇËÄÉpromptÁöÑtokenËΩ¨Êç¢‰∏∫ÂµåÂÖ•ÂêëÈáè„ÄÇ2) **LSHÂåπÈÖç**Ôºö‰ΩøÁî®Â±ÄÈÉ®ÊïèÊÑüÂìàÂ∏åÔºàLSHÔºâÂú®tokenÂµåÂÖ•‰∏äËøõË°åÊ®°Á≥äÂåπÈÖçÔºåÊâæÂà∞ËØ≠‰πâÁõ∏‰ººÁöÑtokenÂØπ„ÄÇ3) **RoPEÁºñÁ†Å**ÔºöÁªìÂêàÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÊù•Êõ¥Â•ΩÂú∞‰øùÁïôtokenÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØÔºåÊèêÈ´òÂåπÈÖçÁöÑÂáÜÁ°ÆÊÄß„ÄÇ4) **KVÁºìÂ≠òÈáçÁî®**ÔºöÊ†πÊçÆLSHÂåπÈÖçÁöÑÁªìÊûúÔºåÈÄâÊã©ÊÄßÂú∞ÈáçÁî®ÂèÇËÄÉpromptÁöÑKVÁºìÂ≠òÔºåÂáèÂ∞ëÂÜó‰ΩôËÆ°ÁÆó„ÄÇ5) **LLMÊé®ÁêÜ**Ôºö‰ΩøÁî®ÈáçÁî®ÂêéÁöÑKVÁºìÂ≠òËøõË°åLLMÊé®ÁêÜÔºåÁîüÊàêÊúÄÁªàËæìÂá∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSemShareKVÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂü∫‰∫étokenÂµåÂÖ•ÁöÑÊ®°Á≥äÂåπÈÖçÊú∫Âà∂ÔºåÊâìÁ†¥‰∫Ü‰º†ÁªüÊñπÊ≥ïÂØπÁ≤æÁ°ÆtokenÂåπÈÖçÁöÑ‰æùËµñ„ÄÇÈÄöËøáLSHÂíåRoPEÁöÑÁªìÂêàÔºåSemShareKVËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ËØÜÂà´ËØ≠‰πâÁõ∏‰ººÁöÑtokenÔºåÂπ∂ÊúâÊïàÂú∞ÈáçÁî®KVÁºìÂ≠ò„ÄÇËøôÁßçÊñπÊ≥ïÂú®ËØ≠‰πâÁõ∏‰ºº‰ΩÜËØçÊ±á‰∏çÂêåÁöÑprompt‰∏äÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSemShareKVÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) **LSHÂìàÂ∏åÂáΩÊï∞ÁöÑÈÄâÊã©**ÔºöÈÄâÊã©ÂêàÈÄÇÁöÑLSHÂìàÂ∏åÂáΩÊï∞Ôºå‰ª•‰øùËØÅÂåπÈÖçÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ2) **RoPEÁöÑÂèÇÊï∞ËÆæÁΩÆ**ÔºöË∞ÉÊï¥RoPEÁöÑÂèÇÊï∞Ôºå‰ª•Êõ¥Â•ΩÂú∞‰øùÁïô‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇ3) **KVÁºìÂ≠òÈáçÁî®Á≠ñÁï•**ÔºöËÆæËÆ°ÂêàÁêÜÁöÑKVÁºìÂ≠òÈáçÁî®Á≠ñÁï•ÔºåÈÅøÂÖçÂºïÂÖ•Âô™Â£∞ÊàñÈôç‰ΩéËæìÂá∫Ë¥®Èáè„ÄÇ4) **Áõ∏‰ººÂ∫¶ÈòàÂÄº**ÔºöËÆæÁΩÆÂêàÈÄÇÁöÑÁõ∏‰ººÂ∫¶ÈòàÂÄºÔºåÊéßÂà∂ÂåπÈÖçÁöÑ‰∏•Ê†ºÁ®ãÂ∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SemShareKVÂú®Â§öÊñáÊ°£ÊëòË¶ÅÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®5k tokenËæìÂÖ•ÁöÑÊÉÖÂÜµ‰∏ãÔºåSemShareKVÂèØ‰ª•Â∞ÜÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò6.25ÂÄçÔºåÂêåÊó∂Â∞ÜGPUÂÜÖÂ≠ò‰ΩøÁî®ÈáèÈôç‰Ωé42ÔºÖÔºåËÄåËæìÂá∫Ë¥®ÈáèÁöÑ‰∏ãÈôçÂèØ‰ª•ÂøΩÁï•‰∏çËÆ°„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåSemShareKVÊòØ‰∏ÄÁßçÈ´òÊïà‰∏îÂÆûÁî®ÁöÑLLMÊé®ÁêÜÂä†ÈÄüÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SemShareKVÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÈúÄË¶ÅÂ§ÑÁêÜÂ§ßÈáèËØ≠‰πâÁõ∏‰ººpromptÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂ§öÊñáÊ°£ÊëòË¶Å„ÄÅÂØπËØùÁ≥ªÁªü„ÄÅ‰ª£Á†ÅÁîüÊàêÁ≠â„ÄÇÈÄöËøáÈôç‰ΩéÂÜÖÂ≠òÂç†Áî®ÂíåÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶ÔºåSemShareKVÂèØ‰ª•ÊòæËëóÊèêÂçáLLMÂú®Ëøô‰∫õÈ¢ÜÂüüÁöÑÂ∫îÁî®ÊïàÁéáÔºåÂπ∂Èôç‰ΩéÈÉ®ÁΩ≤ÊàêÊú¨„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõËøõ‰∏ÄÊ≠•Êé®ÂπøÂà∞ÂÖ∂‰ªñLLMÂ∫îÁî®È¢ÜÂüüÔºåÊé®Âä®LLMÁöÑÊôÆÂèäÂíåÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.

