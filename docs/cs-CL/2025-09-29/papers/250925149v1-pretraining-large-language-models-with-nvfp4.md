---
layout: default
title: Pretraining Large Language Models with NVFP4
---

# Pretraining Large Language Models with NVFP4

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25149" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25149v1</a>
  <a href="https://arxiv.org/pdf/2509.25149.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25149v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25149v1', 'Pretraining Large Language Models with NVFP4')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: NVIDIA, Felix Abecassis, Anjulie Agrusa, Dong Ahn, Jonah Alben, Stefania Alborghetti, Michael Andersch, Sivakumar Arayandi, Alexis Bjorlin, Aaron Blakeman, Evan Briones, Ian Buck, Bryan Catanzaro, Jinhang Choi, Mike Chrzanowski, Eric Chung, Victor Cui, Steve Dai, Bita Darvish Rouhani, Carlo del Mundo, Deena Donia, Burc Eryilmaz, Henry Estela, Abhinav Goel, Oleg Goncharov, Yugi Guvvala, Robert Hesse, Russell Hewett, Herbert Hum, Ujval Kapasi, Brucek Khailany, Mikail Khona, Nick Knight, Alex Kondratenko, Ronny Krashinsky, Ben Lanir, Simon Layton, Michael Lightstone, Daniel Lo, Paulius Micikevicius, Asit Mishra, Tim Moon, Deepak Narayanan, Chao Ni, Abhijit Paithankar, Satish Pasumarthi, Ankit Patel, Mostofa Patwary, Ashwin Poojary, Gargi Prasad, Sweta Priyadarshi, Yigong Qin, Xiaowei Ren, Oleg Rybakov, Charbel Sakr, Sanjeev Satheesh, Stas Sergienko, Pasha Shamis, Kirthi Shankar, Nishant Sharma, Mohammad Shoeybi, Michael Siu, Misha Smelyanskiy, Darko Stosic, Dusan Stosic, Bor-Yiing Su, Frank Sun, Nima Tajbakhsh, Shelby Thomas, Przemek Tredak, Evgeny Tsykunov, Gandhi Vaithilingam, Aditya Vavre, Rangharajan Venkatesan, Roger Waleffe, Qiyu Wan, Hexin Wang, Mengdi Wang, Lizzie Wei, Hao Wu, Evan Wu, Keith Wyss, Ning Xu, Jinze Xue, Charlene Yang, Yujia Zhai, Ruoxi Zhang, Jingyang Zhu, Zhongbo Zhu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNVFP4è®­ç»ƒæ–¹æ³•ï¼Œå®ç°4-bitç²¾åº¦ä¸‹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ç¨³å®šé«˜æ•ˆé¢„è®­ç»ƒã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `ä½ç²¾åº¦è®­ç»ƒ` `NVFP4` `é‡åŒ–` `éšæœºå“ˆè¾¾ç›å˜æ¢` `é¢„è®­ç»ƒ` `FP4`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒéœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºï¼Œæé«˜é¢„è®­ç»ƒæ•ˆç‡å¯¹äºæ„å»ºæ›´å¼ºå¤§çš„LLMè‡³å…³é‡è¦ã€‚
2. è®ºæ–‡æå‡ºNVFP4è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡éšæœºå“ˆè¾¾ç›å˜æ¢ã€äºŒç»´é‡åŒ–ã€éšæœºèˆå…¥å’Œé€‰æ‹©æ€§é«˜ç²¾åº¦å±‚ç­‰æŠ€æœ¯ï¼Œå®ç°ç¨³å®šè®­ç»ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨NVFP4è®­ç»ƒçš„120äº¿å‚æ•°æ¨¡å‹åœ¨è®­ç»ƒæŸå¤±å’Œä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®æ€§ä¸Šä¸FP8åŸºçº¿ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨NVFP4æ ¼å¼è¿›è¡Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¨³å®šä¸”ç²¾ç¡®è®­ç»ƒçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é›†æˆäº†éšæœºå“ˆè¾¾ç›å˜æ¢ï¼ˆRHTï¼‰ä»¥é™åˆ¶å—çº§å¼‚å¸¸å€¼ï¼Œé‡‡ç”¨äºŒç»´é‡åŒ–æ–¹æ¡ˆä»¥ç¡®ä¿å‰å‘å’Œåå‘ä¼ æ’­çš„ä¸€è‡´æ€§è¡¨ç¤ºï¼Œåˆ©ç”¨éšæœºèˆå…¥è¿›è¡Œæ— åæ¢¯åº¦ä¼°è®¡ï¼Œå¹¶ç»“åˆé€‰æ‹©æ€§çš„é«˜ç²¾åº¦å±‚ã€‚é€šè¿‡åœ¨10ä¸‡äº¿tokensä¸Šè®­ç»ƒä¸€ä¸ª120äº¿å‚æ•°çš„æ¨¡å‹éªŒè¯äº†è¯¥æ–¹æ³•ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢å…¬å¼€è®°å½•çš„æœ€é•¿çš„4-bitç²¾åº¦è®­ç»ƒè¿è¡Œã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åŸºäºNVFP4çš„é¢„è®­ç»ƒæŠ€æœ¯è®­ç»ƒçš„æ¨¡å‹ï¼Œå…¶è®­ç»ƒæŸå¤±å’Œä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®æ€§ä¸FP8åŸºçº¿ç›¸å½“ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒNVFP4ä¸æœ¬æ–‡æå‡ºçš„è®­ç»ƒæ–¹æ³•ç›¸ç»“åˆï¼Œä»£è¡¨äº†çª„ç²¾åº¦LLMè®­ç»ƒç®—æ³•çš„ä¸€ä¸ªé‡å¤§è¿›æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒéœ€è¦æ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºå’Œèƒ½æºã€‚è™½ç„¶8-bitæµ®ç‚¹ï¼ˆFP8ï¼‰è®­ç»ƒå·²è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†è¿›ä¸€æ­¥é™ä½ç²¾åº¦åˆ°4-bitæµ®ç‚¹ï¼ˆFP4ï¼‰å¯ä»¥æ˜¾è‘—æé«˜è®¡ç®—é€Ÿåº¦å’Œèµ„æºåˆ©ç”¨ç‡ã€‚ç„¶è€Œï¼Œè¿™ç§æä½çš„é‡åŒ–ç²¾åº¦ç»™è®­ç»ƒçš„ç¨³å®šæ€§ã€æ”¶æ•›æ€§å’Œå®ç°å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡æ¨¡å‹å’Œé•¿åºåˆ—è®­ç»ƒä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ç³»åˆ—æŠ€æœ¯æ‰‹æ®µæ¥å…‹æœ4-bité‡åŒ–å¸¦æ¥çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œä»è€Œå®ç°å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨NVFP4æ ¼å¼ä¸‹çš„ç¨³å®šè®­ç»ƒã€‚è¿™äº›æŠ€æœ¯åŒ…æ‹¬é™åˆ¶å¼‚å¸¸å€¼ã€ä¿æŒå‰åå‘ä¼ æ’­ä¸€è‡´æ€§ã€å‡å°‘æ¢¯åº¦ä¼°è®¡åå·®ä»¥åŠåœ¨å…³é”®å±‚ä¿æŒé«˜ç²¾åº¦ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **éšæœºå“ˆè¾¾ç›å˜æ¢ï¼ˆRHTï¼‰**ï¼šç”¨äºé™åˆ¶å—çº§å¼‚å¸¸å€¼ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚2) **äºŒç»´é‡åŒ–æ–¹æ¡ˆ**ï¼šç¡®ä¿å‰å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­è¡¨ç¤ºçš„ä¸€è‡´æ€§ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚3) **éšæœºèˆå…¥**ï¼šç”¨äºæ— åæ¢¯åº¦ä¼°è®¡ï¼Œå‡å°‘é‡åŒ–è¯¯å·®å¸¦æ¥çš„å½±å“ã€‚4) **é€‰æ‹©æ€§é«˜ç²¾åº¦å±‚**ï¼šåœ¨å¯¹ç²¾åº¦è¦æ±‚è¾ƒé«˜çš„å±‚ï¼ˆä¾‹å¦‚ï¼ŒæŸäº›æ³¨æ„åŠ›å±‚ï¼‰ä½¿ç”¨æ›´é«˜çš„ç²¾åº¦ï¼Œä»¥ä¿æŒæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†éšæœºå“ˆè¾¾ç›å˜æ¢ã€äºŒç»´é‡åŒ–ã€éšæœºèˆå…¥å’Œé€‰æ‹©æ€§é«˜ç²¾åº¦å±‚ç»“åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„NVFP4è®­ç»ƒæ–¹æ¡ˆã€‚è¿™ç§æ–¹æ¡ˆèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³4-bité‡åŒ–å¸¦æ¥çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œä½¿å¾—å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æä½çš„ç²¾åº¦ä¸‹è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼š1) **éšæœºå“ˆè¾¾ç›å˜æ¢çš„å‚æ•°è®¾ç½®**ï¼šéœ€è¦ä»”ç»†è°ƒæ•´å“ˆè¾¾ç›å˜æ¢çš„å‚æ•°ï¼Œä»¥ç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆåœ°é™åˆ¶å¼‚å¸¸å€¼ï¼ŒåŒæ—¶é¿å…è¿‡åº¦å¹³æ»‘ã€‚2) **äºŒç»´é‡åŒ–æ–¹æ¡ˆçš„å…·ä½“å®ç°**ï¼šéœ€è¦é€‰æ‹©åˆé€‚çš„é‡åŒ–èŒƒå›´å’Œé‡åŒ–æ­¥é•¿ï¼Œä»¥å¹³è¡¡é‡åŒ–è¯¯å·®å’Œè®¡ç®—æ•ˆç‡ã€‚3) **éšæœºèˆå…¥çš„å®ç°æ–¹å¼**ï¼šéœ€è¦ä¿è¯éšæœºèˆå…¥çš„æ¦‚ç‡åˆ†å¸ƒæ˜¯å‡åŒ€çš„ï¼Œä»¥å®ç°æ— åæ¢¯åº¦ä¼°è®¡ã€‚4) **é€‰æ‹©æ€§é«˜ç²¾åº¦å±‚çš„é€‰æ‹©ç­–ç•¥**ï¼šéœ€è¦æ ¹æ®æ¨¡å‹çš„å…·ä½“ç»“æ„å’Œä»»åŠ¡ç‰¹ç‚¹ï¼Œé€‰æ‹©å¯¹ç²¾åº¦è¦æ±‚è¾ƒé«˜çš„å±‚ä½¿ç”¨æ›´é«˜çš„ç²¾åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨NVFP4è®­ç»ƒçš„120äº¿å‚æ•°æ¨¡å‹åœ¨10ä¸‡äº¿tokensä¸Šè¿›è¡Œè®­ç»ƒåï¼Œå…¶è®­ç»ƒæŸå¤±å’Œä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®æ€§ä¸FP8åŸºçº¿æ¨¡å‹ç›¸å½“ã€‚è¿™è¯æ˜äº†NVFP4è®­ç»ƒæ–¹æ³•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä¸ºæ›´é«˜æ•ˆçš„LLMè®­ç»ƒæä¾›äº†æ–°çš„é€”å¾„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿæ¨¡å‹è¿­ä»£ã€‚å°¤å…¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ï¼Œå¦‚è¾¹ç¼˜è®¡ç®—è®¾å¤‡æˆ–å°å‹ç ”ç©¶å›¢é˜Ÿï¼ŒNVFP4è®­ç»ƒæ–¹æ³•èƒ½æœ‰æ•ˆæå‡LLMçš„è®­ç»ƒæ•ˆç‡å’Œå¯éƒ¨ç½²æ€§ï¼Œæ¨åŠ¨AIæŠ€æœ¯çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) today are powerful problem solvers across many domains, and they continue to get stronger as they scale in model size, training set size, and training set quality, as shown by extensive research and experimentation across the industry. Training a frontier model today requires on the order of tens to hundreds of yottaflops, which is a massive investment of time, compute, and energy. Improving pretraining efficiency is therefore essential to enable the next generation of even more capable LLMs. While 8-bit floating point (FP8) training is now widely adopted, transitioning to even narrower precision, such as 4-bit floating point (FP4), could unlock additional improvements in computational speed and resource utilization. However, quantization at this level poses challenges to training stability, convergence, and implementation, notably for large-scale models trained on long token horizons.
>   In this study, we introduce a novel approach for stable and accurate training of large language models (LLMs) using the NVFP4 format. Our method integrates Random Hadamard transforms (RHT) to bound block-level outliers, employs a two-dimensional quantization scheme for consistent representations across both the forward and backward passes, utilizes stochastic rounding for unbiased gradient estimation, and incorporates selective high-precision layers. We validate our approach by training a 12-billion-parameter model on 10 trillion tokens -- the longest publicly documented training run in 4-bit precision to date. Our results show that the model trained with our NVFP4-based pretraining technique achieves training loss and downstream task accuracies comparable to an FP8 baseline. These findings highlight that NVFP4, when combined with our training approach, represents a major step forward in narrow-precision LLM training algorithms.

