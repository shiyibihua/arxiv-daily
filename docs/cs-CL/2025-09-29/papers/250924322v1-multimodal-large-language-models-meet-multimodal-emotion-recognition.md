---
layout: default
title: Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey
---

# Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24322" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24322v1</a>
  <a href="https://arxiv.org/pdf/2509.24322.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24322v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24322v1', 'Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuntao Shou, Tao Meng, Wei Ai, Keqin Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: 35 pages, 10 figures, 8 tables

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yuntaoshou/Awesome-Emotion-Reasoning)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿè¯†åˆ«ä¸æ¨ç†ä¸­çš„åº”ç”¨ä¸æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æƒ…æ„Ÿè¯†åˆ«` `æƒ…æ„Ÿæ¨ç†` `è·¨æ¨¡æ€èåˆ` `äººå·¥æ™ºèƒ½` `ç³»ç»Ÿæ€§ç»¼è¿°` `æ¨¡å‹æ¶æ„` `æ€§èƒ½åŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸æ¨ç†ä¸­ç¼ºä¹ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œå¯¼è‡´ç ”ç©¶è€…éš¾ä»¥æŠŠæ¡æœ€æ–°è¿›å±•ä¸æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡é€šè¿‡å…¨é¢ç»¼è¿°LLMsä¸MLLMsåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸æ¨ç†ä¸­çš„åº”ç”¨ï¼Œæä¾›äº†æ¨¡å‹æ¶æ„ã€æ•°æ®é›†åŠæ€§èƒ½åŸºå‡†çš„æ•´åˆã€‚
3. ç ”ç©¶æŒ‡å‡ºäº†å…³é”®æŒ‘æˆ˜å¹¶æå‡ºæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶è€…æä¾›å®ç”¨çš„å‚è€ƒä¸è§è§£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œæ ‡å¿—ç€å‘äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚éšç€å¯¹æ›´é«˜å±‚æ¬¡è¯­ä¹‰å’Œè·¨æ¨¡æ€èåˆçš„éœ€æ±‚å¢åŠ ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åº”è¿è€Œç”Ÿï¼Œæ•´åˆæ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘ç­‰å¤šç§ä¿¡æ¯æºï¼Œä»¥å¢å¼ºå¤æ‚åœºæ™¯ä¸­çš„å»ºæ¨¡å’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨ç§‘å­¦äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†å·²æˆä¸ºå¿«é€Ÿå‘å±•çš„å‰æ²¿é¢†åŸŸã€‚å°½ç®¡LLMså’ŒMLLMsåœ¨è¿™ä¸€é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç¼ºä¹ç³»ç»Ÿæ€§çš„ç»¼è¿°æ¥æ•´åˆæœ€æ–°å‘å±•ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æä¾›äº†LLMså’ŒMLLMsåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸æ¨ç†æ–¹é¢çš„å…¨é¢ç»¼è¿°ï¼Œæ¶µç›–æ¨¡å‹æ¶æ„ã€æ•°æ®é›†å’Œæ€§èƒ½åŸºå‡†ï¼Œå¹¶å¼ºè°ƒå…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›æƒå¨å‚è€ƒå’Œå®ç”¨è§è§£ã€‚æ ¹æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬æ–‡æ˜¯é¦–æ¬¡å…¨é¢è°ƒæŸ¥MLLMsä¸å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†äº¤å‰é¢†åŸŸçš„å°è¯•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸æ¨ç†é¢†åŸŸç¼ºä¹ç³»ç»Ÿæ€§ç»¼è¿°çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•´åˆæœ€æ–°è¿›å±•ä¸æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å…¨é¢å›é¡¾LLMså’ŒMLLMsåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸æ¨ç†ä¸­çš„åº”ç”¨ï¼Œæ•´åˆæ¨¡å‹æ¶æ„ã€æ•°æ®é›†å’Œæ€§èƒ½åŸºå‡†ï¼Œæä¾›ç ”ç©¶è€…æ‰€éœ€çš„å‚è€ƒèµ„æ–™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ–‡çŒ®å›é¡¾ã€æ¨¡å‹åˆ†ç±»ã€æ•°æ®é›†åˆ†æå’Œæ€§èƒ½è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ï¼Œç³»ç»Ÿæ€§åœ°å‘ˆç°å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸æ¨ç†çš„ç°çŠ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡é¦–æ¬¡å…¨é¢è°ƒæŸ¥äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†çš„äº¤å‰é¢†åŸŸï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„æ–‡çŒ®ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç»¼è¿°è¿‡ç¨‹ä¸­ï¼Œé‡ç‚¹åˆ†æäº†ä¸åŒæ¨¡å‹çš„æ¶æ„è®¾è®¡ã€è®­ç»ƒç­–ç•¥åŠå…¶åœ¨æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæä¾›äº†è¯¦ç»†çš„æ€§èƒ½åŸºå‡†å¯¹æ¯”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

æœ¬æ–‡ç»¼è¿°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿè¯†åˆ«ä¸æ¨ç†ä¸­çš„åº”ç”¨ï¼ŒæŒ‡å‡ºäº†å½“å‰æ¨¡å‹åœ¨æ€§èƒ½å’Œåº”ç”¨åœºæ™¯ä¸­çš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚é€šè¿‡å¯¹æ¯”ç°æœ‰æ¨¡å‹ï¼Œå¼ºè°ƒäº†åœ¨å¤æ‚æƒ…æ„Ÿåœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€ç¤¾äº¤åª’ä½“ç›‘æµ‹ã€æ™ºèƒ½å®¢æœç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šå’Œç ”ç©¶æœºæ„æ›´å¥½åœ°ç†è§£å’Œå“åº”ç”¨æˆ·æƒ…æ„Ÿã€‚æœªæ¥ï¼Œéšç€å¤šæ¨¡æ€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œè¯¥é¢†åŸŸå¯èƒ½ä¼šå¯¹äººæœºäº¤äº’ã€æƒ…æ„Ÿè®¡ç®—ç­‰äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, large language models (LLMs) have driven major advances in language understanding, marking a significant step toward artificial general intelligence (AGI). With increasing demands for higher-level semantics and cross-modal fusion, multimodal large language models (MLLMs) have emerged, integrating diverse information sources (e.g., text, vision, and audio) to enhance modeling and reasoning in complex scenarios. In AI for Science, multimodal emotion recognition and reasoning has become a rapidly growing frontier. While LLMs and MLLMs have achieved notable progress in this area, the field still lacks a systematic review that consolidates recent developments. To address this gap, this paper provides a comprehensive survey of LLMs and MLLMs for emotion recognition and reasoning, covering model architectures, datasets, and performance benchmarks. We further highlight key challenges and outline future research directions, aiming to offer researchers both an authoritative reference and practical insights for advancing this domain. To the best of our knowledge, this paper is the first attempt to comprehensively survey the intersection of MLLMs with multimodal emotion recognition and reasoning. The summary of existing methods mentioned is in our Github: \href{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}.

