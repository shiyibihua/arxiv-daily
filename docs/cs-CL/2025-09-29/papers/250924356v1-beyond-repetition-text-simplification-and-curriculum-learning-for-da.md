---
layout: default
title: Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining
---

# Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24356" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24356v1</a>
  <a href="https://arxiv.org/pdf/2509.24356.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24356v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24356v1', 'Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Matthew Theodore Roque, Dan John Velasco

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: To be published in BabyLM Workshop at EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é’ˆå¯¹æ•°æ®å—é™çš„é¢„è®­ç»ƒï¼Œæå‡ºåŸºäºæ–‡æœ¬ç®€åŒ–å’Œè¯¾ç¨‹å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ–‡æœ¬ç®€åŒ–` `è¯¾ç¨‹å­¦ä¹ ` `é¢„è®­ç»ƒ` `æ•°æ®å¢å¼º` `è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒç ”ç©¶ä¸»è¦é›†ä¸­äºå¤§æ•°æ®é›†ï¼Œå¿½ç•¥äº†æ•°æ®å—é™åœºæ™¯ä¸‹çš„ä¼˜åŒ–é—®é¢˜ã€‚
2. è¯¥è®ºæ–‡æå‡ºåˆ©ç”¨æ–‡æœ¬ç®€åŒ–å’Œè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä¼˜åŒ–æ•°æ®å—é™åœºæ™¯ä¸‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ·»åŠ ç®€åŒ–æ•°æ®å¹¶é‡‡ç”¨åˆé€‚çš„å¤æ‚åº¦æ’åºç­–ç•¥ï¼Œèƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å¤šæ•°å…³äºè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„ç ”ç©¶éƒ½é›†ä¸­åœ¨å¤§å‹æ•°æ®é›†ä¸Šï¼Œè€Œæ•°æ®å—é™ç¯å¢ƒä¸‹çš„ä¼˜åŒ–é—®é¢˜ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™æ ·çš„ç¯å¢ƒä¸­ï¼Œè®­ç»ƒæ•°æ®é¡ºåºä»¥åŠåŒ…å«ç›¸åŒæ–‡æœ¬çš„ä¸åŒç‰ˆæœ¬çš„å½±å“ä»ç„¶æœªçŸ¥ã€‚æœ¬æ–‡é€šè¿‡ç ”ç©¶é¢„è®­ç»ƒä¸­çš„è¯¾ç¨‹å­¦ä¹ æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨æ–‡æœ¬å¤æ‚åº¦çš„æ’åºå’Œé€šè¿‡ç®€åŒ–è¿›è¡Œçš„æ•°æ®å¢å¼ºã€‚æˆ‘ä»¬æ¢ç©¶ï¼šï¼ˆ1ï¼‰ç®€åŒ–æ–‡æœ¬æ˜¯å¦æ¯”é‡ç”¨åŸå§‹æ•°æ®æ›´èƒ½æé«˜è¡¨ç¤ºè´¨é‡ï¼Ÿï¼ˆ2ï¼‰æŒ‰æ–‡æœ¬å¤æ‚åº¦æ’åºæ•°æ®æ˜¯å¦èƒ½äº§ç”Ÿæ›´å¥½çš„è¡¨ç¤ºï¼Ÿä¸ºäº†å›ç­”è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€å¯¹å¹¶è¡Œè¯­æ–™åº“ï¼Œå…¶ä¸­äººå·¥ç¼–å†™çš„æ®µè½ä¸LLMç®€åŒ–çš„å˜ä½“å¯¹é½ï¼Œå¹¶æµ‹è¯•äº†å››ç§æ•°æ®è°ƒåº¦æ–¹æ¡ˆï¼šé‡å¤æš´éœ²ã€ä½åˆ°é«˜å¤æ‚åº¦ã€é«˜åˆ°ä½å¤æ‚åº¦ä»¥åŠäº¤é”™ã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒä»æ ·æœ¬æ•ˆç‡çš„è§’åº¦åˆ†ææ¨¡å‹çš„è¡¨ç¤ºè´¨é‡ï¼Œä»¥åŠå…¶åœ¨è¯­è¨€çŸ¥è¯†ã€å®ä½“è·Ÿè¸ªã€ä¸–ç•ŒçŸ¥è¯†å’Œå¸¸è¯†æ¨ç†æ–¹é¢çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸é‡å¤æš´éœ²åŸºçº¿ç›¸æ¯”ï¼Œæ·»åŠ ç®€åŒ–æ•°æ®å¯ä»¥æé«˜å¾®è°ƒå’Œé›¶æ ·æœ¬æ€§èƒ½ï¼šè¾ƒå°çš„æ¨¡å‹å—ç›Šäºä½åˆ°é«˜å¤æ‚åº¦ï¼Œè€Œè¾ƒå¤§çš„æ¨¡å‹åœ¨äº¤é”™æ’åºä¸‹è¡¨ç°æ›´å¥½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ•°æ®é‡æœ‰é™æƒ…å†µä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆè¿›è¡Œè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•°æ®å—é™åœºæ™¯ä¸‹ï¼Œé€šå¸¸é‡‡ç”¨é‡å¤ä½¿ç”¨æ•°æ®çš„æ–¹å¼ï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œä¸”æœªå……åˆ†åˆ©ç”¨æ•°æ®ä¸­çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä¸åŒå¤æ‚åº¦çš„æ–‡æœ¬å¯¹æ¨¡å‹å­¦ä¹ çš„å½±å“ä¹Ÿæœªè¢«å……åˆ†ç ”ç©¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ–‡æœ¬ç®€åŒ–æŠ€æœ¯ç”ŸæˆåŸå§‹æ–‡æœ¬çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå¹¶ç»“åˆè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œæ§åˆ¶è®­ç»ƒæ•°æ®çš„å¤æ‚åº¦é¡ºåºã€‚é€šè¿‡å¼•å…¥ç®€åŒ–æ–‡æœ¬ï¼Œå¢åŠ æ•°æ®çš„å¤šæ ·æ€§ï¼Œé¿å…æ¨¡å‹è¿‡æ‹Ÿåˆã€‚åŒæ—¶ï¼Œé€šè¿‡è¯¾ç¨‹å­¦ä¹ ï¼Œè®©æ¨¡å‹å…ˆå­¦ä¹ ç®€å•çš„æ–‡æœ¬ï¼Œå†é€æ­¥å­¦ä¹ å¤æ‚çš„æ–‡æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹é¢„è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªé˜¶æ®µã€‚æ•°æ®å‡†å¤‡é˜¶æ®µï¼Œæ„å»ºåŒ…å«åŸå§‹æ–‡æœ¬å’ŒLLMç®€åŒ–æ–‡æœ¬çš„å¹¶è¡Œè¯­æ–™åº“ã€‚æ¨¡å‹é¢„è®­ç»ƒé˜¶æ®µï¼Œé‡‡ç”¨ä¸åŒçš„æ•°æ®è°ƒåº¦ç­–ç•¥ï¼ˆé‡å¤æš´éœ²ã€ä½åˆ°é«˜å¤æ‚åº¦ã€é«˜åˆ°ä½å¤æ‚åº¦ã€äº¤é”™ï¼‰è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚æ€§èƒ½è¯„ä¼°é˜¶æ®µï¼Œé€šè¿‡å¾®è°ƒå’Œé›¶æ ·æœ¬æµ‹è¯•ï¼Œè¯„ä¼°æ¨¡å‹çš„è¡¨ç¤ºè´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ–‡æœ¬ç®€åŒ–å’Œè¯¾ç¨‹å­¦ä¹ ç›¸ç»“åˆï¼Œåº”ç”¨äºæ•°æ®å—é™çš„é¢„è®­ç»ƒåœºæ™¯ã€‚ä¸ä¼ ç»Ÿçš„é‡å¤æ•°æ®æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ•°æ®ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç³»ç»Ÿåœ°ç ”ç©¶äº†ä¸åŒå¤æ‚åº¦æ’åºç­–ç•¥å¯¹æ¨¡å‹å­¦ä¹ çš„å½±å“ï¼Œä¸ºæ•°æ®å—é™åœºæ™¯ä¸‹çš„é¢„è®­ç»ƒæä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨LLMè¿›è¡Œæ–‡æœ¬ç®€åŒ–ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ç®€åŒ–æ–‡æœ¬ï¼›2) è®¾è®¡å››ç§ä¸åŒçš„æ•°æ®è°ƒåº¦ç­–ç•¥ï¼Œæ¢ç´¢ä¸åŒå¤æ‚åº¦æ’åºæ–¹å¼çš„å½±å“ï¼›3) é‡‡ç”¨å¾®è°ƒå’Œé›¶æ ·æœ¬æµ‹è¯•ä¸¤ç§è¯„ä¼°æ–¹å¼ï¼Œå…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼›4) é’ˆå¯¹ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼Œæ¢ç´¢æœ€ä½³çš„æ•°æ®è°ƒåº¦ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸é‡å¤æš´éœ²åŸºçº¿ç›¸æ¯”ï¼Œæ·»åŠ ç®€åŒ–æ•°æ®å¯ä»¥æé«˜å¾®è°ƒå’Œé›¶æ ·æœ¬æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œè¾ƒå°çš„æ¨¡å‹å—ç›Šäºä½åˆ°é«˜å¤æ‚åº¦çš„æ•°æ®è°ƒåº¦ç­–ç•¥ï¼Œè€Œè¾ƒå¤§çš„æ¨¡å‹åœ¨äº¤é”™æ’åºä¸‹è¡¨ç°æ›´å¥½ã€‚è¿™è¡¨æ˜ï¼Œé’ˆå¯¹ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼Œéœ€è¦é‡‡ç”¨ä¸åŒçš„æ•°æ®è°ƒåº¦ç­–ç•¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä½èµ„æºè¯­è¨€çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒã€ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬ç”Ÿæˆå’Œç†è§£ç­‰åœºæ™¯ã€‚é€šè¿‡æ–‡æœ¬ç®€åŒ–å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œå¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹åœ¨æ•°æ®å—é™ç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼Œé™ä½æ¨¡å‹è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿç›¸å…³æŠ€æœ¯çš„è½åœ°åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Most studies on language model pretraining focus on large datasets, leaving open questions about optimization in data-constrained settings. In such settings, the effects of training data order and of including alternative versions of the same text remain underexplored. We address this by studying curriculum learning in pretraining, focusing on text-complexity ordering and data augmentation via simplification. We ask: (1) Does simplifying texts enhance representation quality more than reusing the original data? and (2) Does ordering data by text complexity yield better representations? To answer, we build on a pair of parallel corpora where human-written paragraphs are aligned with LLM-simplified variants, and test four data schedules: repeated exposure, low-to-high complexity, high-to-low, and interleaved. We analyze models' representation quality from a sample efficiency perspective via fine-tuning, as well as its zero-shot performance on linguistic knowledge, entity tracking, world knowledge, and commonsense reasoning. Our findings show that adding simplified data improves fine-tuning and zero-shot performance over a repeated-exposure baseline: smaller models benefit from low-to-high complexity, while larger models perform better with interleaved ordering.

