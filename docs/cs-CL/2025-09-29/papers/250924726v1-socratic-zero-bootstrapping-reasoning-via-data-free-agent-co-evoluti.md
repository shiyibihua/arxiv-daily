---
layout: default
title: Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution
---

# Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24726" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24726v1</a>
  <a href="https://arxiv.org/pdf/2509.24726.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24726v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24726v1', 'Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaobo Wang, Zhengbo Jiao, Zifan Zhang, Yilang Peng, Xu Ze, Boyu Yang, Wei Wang, Hu Wei, Linfeng Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: 23 pages, 3 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Socratic-Zeroï¼šé€šè¿‡æ— æ•°æ®AgentååŒè¿›åŒ–å¼•å¯¼LLMæ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æ•°æ®åˆæˆ` `AgentååŒè¿›åŒ–` `æ— æ•°æ®å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ¨ç†ä¾èµ–å¤§è§„æ¨¡äººå·¥æ ‡æ³¨æ•°æ®é›†ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ï¼Œæ•°æ®åˆæˆæ–¹æ³•å­˜åœ¨è´¨é‡ä¸ç¨³å®šå’Œé€‚åº”æ€§å·®çš„é—®é¢˜ã€‚
2. Socratic-Zeroé€šè¿‡æ•™å¸ˆã€è§£é¢˜è€…å’Œç”Ÿæˆå™¨ä¸‰ä¸ªAgentçš„ååŒè¿›åŒ–ï¼Œä»å°‘é‡ç§å­æ•°æ®ä¸­è‡ªä¸»ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSocratic-Zeroåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ•°æ®åˆæˆæ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†éƒ¨åˆ†å•†ä¸šLLMã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„çªç ´ä¾èµ–äºå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸ç”±äººå·¥æ ‡æ³¨ï¼Œéš¾ä»¥æ‰©å±•ã€‚æ•°æ®åˆæˆæˆ–è’¸é¦æä¾›äº†ä¸€ç§æœ‰å¸Œæœ›çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ•°æ®è´¨é‡ä¸ä¸€è‡´ä»¥åŠæ— æ³•åŠ¨æ€é€‚åº”æ¨¡å‹ä¸æ–­å‘å±•çš„èƒ½åŠ›æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„è®­ç»ƒä¿¡å·ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†Socratic-Zeroï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨è‡ªä¸»çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªAgentï¼ˆæ•™å¸ˆã€è§£é¢˜è€…å’Œç”Ÿæˆå™¨ï¼‰çš„ååŒè¿›åŒ–ï¼Œä»æœ€å°‘çš„ç§å­ç¤ºä¾‹ä¸­ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚è§£é¢˜è€…é€šè¿‡å­¦ä¹ æˆåŠŸå’Œå¤±è´¥è½¨è¿¹ä¸Šçš„åå¥½åé¦ˆæ¥ä¸æ–­æ”¹è¿›å…¶æ¨ç†èƒ½åŠ›ï¼›æ•™å¸ˆæ ¹æ®è§£é¢˜è€…çš„å¼±ç‚¹è‡ªé€‚åº”åœ°è®¾è®¡è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼›ç”Ÿæˆå™¨æç‚¼æ•™å¸ˆçš„é—®é¢˜è®¾è®¡ç­–ç•¥ï¼Œä»¥å®ç°å¯æ‰©å±•ã€é«˜ä¿çœŸçš„è¯¾ç¨‹ç”Ÿæˆã€‚è¿™ä¸ªé—­ç¯ç³»ç»Ÿäº§ç”Ÿäº†ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„è¯¾ç¨‹ï¼Œä¸éœ€è¦é¢„å…ˆå­˜åœ¨çš„ä»»åŠ¡æˆ–æ ‡ç­¾ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä»ä»…100ä¸ªç§å­é—®é¢˜å¼€å§‹ï¼Œæˆ‘ä»¬çš„Socratic-Solver-8Båœ¨ä¸ƒä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆAMC23ã€AIME24-25ã€å¥¥æ—åŒ¹å…‹ã€MATH-500ã€Minervaå’ŒGSM8Kï¼‰ä¸Šï¼Œæ¯”ä¹‹å‰çš„æ•°æ®åˆæˆæ–¹æ³•å¹³å‡æé«˜äº†+20.2ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ä¸”åœ¨Qwen3å’ŒGLM4ç³»åˆ—æ¨¡å‹ä¸Šéƒ½è·å¾—äº†æŒç»­çš„æ”¶ç›Šã€‚æ›´ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ¥è‡ªSocratic-Generator-32Bçš„åˆæˆæ•°æ®ä½¿å­¦ç”ŸLLMèƒ½å¤Ÿåœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸Šå®ç°ä¼˜äºå…¶ä»–æœ€å…ˆè¿›ï¼ˆSOTAï¼‰å•†ä¸šLLMçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬Qwen3-235B-A22Bã€DeepSeek-V3.1-671Bã€GPT-5ã€Gemini-2.5-Proã€Grok-4å’ŒClaude-4.1-Opusã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å¯¹å¤§è§„æ¨¡é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ•°æ®åˆæˆæ–¹æ³•çš„ç—›ç‚¹åœ¨äºç”Ÿæˆçš„æ•°æ®è´¨é‡ä¸ç¨³å®šï¼Œéš¾ä»¥é€‚åº”æ¨¡å‹èƒ½åŠ›çš„åŠ¨æ€å˜åŒ–ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªè‡ªä¸»çš„AgentååŒè¿›åŒ–æ¡†æ¶ï¼Œæ¨¡æ‹Ÿè‹æ ¼æ‹‰åº•å¼æ•™å­¦ï¼Œè®©æ¨¡å‹åœ¨ä¸æ•™å¸ˆAgentçš„äº¤äº’ä¸­ä¸æ–­å­¦ä¹ å’Œæå‡æ¨ç†èƒ½åŠ›ã€‚æ•™å¸ˆAgentæ ¹æ®è§£é¢˜è€…Agentçš„å¼±ç‚¹ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œè§£é¢˜è€…Agenté€šè¿‡å­¦ä¹ æˆåŠŸå’Œå¤±è´¥çš„ç»éªŒæ¥æ”¹è¿›è‡ªèº«ï¼Œç”Ÿæˆå™¨Agentåˆ™è´Ÿè´£æç‚¼æ•™å¸ˆAgentçš„é—®é¢˜è®¾è®¡ç­–ç•¥ï¼Œä»è€Œå®ç°å¯æ‰©å±•çš„è¯¾ç¨‹ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSocratic-Zeroæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ•™å¸ˆAgentã€è§£é¢˜è€…Agentå’Œç”Ÿæˆå™¨Agentã€‚æ•™å¸ˆAgentè´Ÿè´£æ ¹æ®è§£é¢˜è€…Agentçš„è¡¨ç°ç”Ÿæˆæ–°çš„é—®é¢˜ï¼›è§£é¢˜è€…Agentè´Ÿè´£è§£å†³æ•™å¸ˆAgentæå‡ºçš„é—®é¢˜ï¼Œå¹¶æ ¹æ®åé¦ˆè°ƒæ•´ç­–ç•¥ï¼›ç”Ÿæˆå™¨Agentè´Ÿè´£å­¦ä¹ æ•™å¸ˆAgentçš„é—®é¢˜ç”Ÿæˆç­–ç•¥ï¼Œå¹¶ç”Ÿæˆå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®ã€‚æ•´ä¸ªæ¡†æ¶æ˜¯ä¸€ä¸ªé—­ç¯ç³»ç»Ÿï¼Œä¸‰ä¸ªAgentç›¸äº’åä½œï¼Œå…±åŒæå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šSocratic-Zeroçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å®Œå…¨è‡ªä¸»çš„AgentååŒè¿›åŒ–æœºåˆ¶ï¼Œæ— éœ€äººå·¥æ ‡æ³¨æ•°æ®ï¼Œå³å¯ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚é€šè¿‡æ•™å¸ˆAgentçš„è‡ªé€‚åº”é—®é¢˜è®¾è®¡å’Œè§£é¢˜è€…Agentçš„ç»éªŒå­¦ä¹ ï¼ŒSocratic-Zeroèƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼LLMå­¦ä¹ å¤æ‚çš„æ¨ç†æŠ€èƒ½ã€‚æ­¤å¤–ï¼Œç”Ÿæˆå™¨Agentçš„å¼•å…¥ä½¿å¾—è¯¾ç¨‹ç”Ÿæˆå…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯ä»¥ç”Ÿæˆå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºå…³é”®å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°ã€ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ã€‚ä½†å¯ä»¥æ¨æ–­ï¼Œæ•™å¸ˆAgentçš„é—®é¢˜ç”Ÿæˆç­–ç•¥å¯èƒ½æ¶‰åŠåˆ°åŸºäºè§„åˆ™æˆ–æ¨¡å‹çš„ç”Ÿæˆæ–¹æ³•ï¼Œè§£é¢˜è€…Agentçš„å­¦ä¹ å¯èƒ½é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æˆ–æ¨¡ä»¿å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œç”Ÿæˆå™¨Agentå¯èƒ½é‡‡ç”¨è’¸é¦å­¦ä¹ ç­‰æ–¹æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Socratic-Zeroåœ¨ä¸ƒä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¯”ä¹‹å‰çš„æ•°æ®åˆæˆæ–¹æ³•å¹³å‡æé«˜äº†+20.2ä¸ªç™¾åˆ†ç‚¹ã€‚ä½¿ç”¨Socratic-Generator-32Bç”Ÿæˆçš„åˆæˆæ•°æ®è®­ç»ƒçš„LLMï¼Œæ€§èƒ½ä¼˜äºQwen3-235B-A22Bã€DeepSeek-V3.1-671Bç­‰å•†ä¸šLLMï¼Œç”šè‡³å¯ä»¥åª²ç¾GPT-5ã€Gemini-2.5-Proç­‰æ¨¡å‹ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Socratic-Zeroå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºæå‡LLMåœ¨æ•°å­¦ã€ç§‘å­¦ã€é€»è¾‘ç­‰é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å¯ä»¥é™ä½LLMè®­ç»ƒå¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼Œå¹¶ä¿ƒè¿›LLMåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦è‡ªä¸»å­¦ä¹ å’ŒçŸ¥è¯†å‘ç°çš„é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on massive, high-quality datasets-typically human-annotated and thus difficult to scale. While data synthesis or distillation offers a promising alternative, existing methods struggle with inconsistent data quality and an inability to dynamically adapt to the evolving capabilities of the model, leading to suboptimal training signals. To address these limitations, we introduce Socratic-Zero, a fully autonomous framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: the Teacher, the Solver, and the Generator. The Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories; the Teacher adaptively crafts increasingly challenging questions based on the Solver's weaknesses; and the Generator distills the Teacher's question-design strategy to enable scalable, high-fidelity curriculum generation. This closed-loop system produces a self-improving curriculum-requiring no pre-existing tasks or labels. Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25, Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3 and GLM4 series models. Even more surprisingly, synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks, including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4, and Claude-4.1-Opus.

