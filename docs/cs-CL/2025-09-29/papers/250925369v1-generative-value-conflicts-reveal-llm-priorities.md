---
layout: default
title: Generative Value Conflicts Reveal LLM Priorities
---

# Generative Value Conflicts Reveal LLM Priorities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25369" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25369v1</a>
  <a href="https://arxiv.org/pdf/2509.25369.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25369v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25369v1', 'Generative Value Conflicts Reveal LLM Priorities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andy Liu, Kshitish Ghate, Mona Diab, Daniel Fried, Atoosa Kasirzadeh, Max Kleiman-Weiner

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ConflictScopeï¼šæ­ç¤ºLLMåœ¨ä»·å€¼å†²çªä¸‹çš„ä¼˜å…ˆçº§åå¥½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»·å€¼å¯¹é½` `ä»·å€¼å†²çª` `ä¼¦ç†å®‰å…¨` `ç³»ç»Ÿæç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå¯¹é½æ–¹æ³•ç¼ºä¹å¯¹ä»·å€¼å†²çªåœºæ™¯çš„æœ‰æ•ˆå¤„ç†ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­éš¾ä»¥åšå‡ºç¬¦åˆé¢„æœŸçš„æƒè¡¡ã€‚
2. ConflictScopeé€šè¿‡è‡ªåŠ¨ç”Ÿæˆä»·å€¼å†²çªåœºæ™¯ï¼Œå¹¶åˆ†æLLMçš„å“åº”ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹åœ¨ä¸åŒä»·å€¼ä¹‹é—´çš„ä¼˜å…ˆçº§æ’åºã€‚
3. å®éªŒè¡¨æ˜ï¼Œå¼€æ”¾å¼è¯„ä¼°ä¼šä½¿æ¨¡å‹åå‘ä¸ªäººä»·å€¼ï¼Œä½†é€šè¿‡ç³»ç»Ÿæç¤ºå¯ä»¥æ˜¾è‘—æ”¹å–„æ¨¡å‹åœ¨ä»·å€¼å†²çªä¸‹çš„å¯¹é½æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰å·¥ä½œè‡´åŠ›äºä½¿åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠ©æ‰‹ä¸ç›®æ ‡ä»·å€¼é›†åˆå¯¹é½ï¼Œä½†è¿™äº›åŠ©æ‰‹åœ¨éƒ¨ç½²æ—¶ç»å¸¸éœ€è¦åœ¨ä¸åŒä»·å€¼ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚é’ˆå¯¹ç°æœ‰å¯¹é½æ•°æ®é›†ä¸­ä»·å€¼å†²çªç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ConflictScopeï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æµç¨‹ï¼Œç”¨äºè¯„ä¼°LLMå¦‚ä½•å¯¹ä¸åŒçš„ä»·å€¼è¿›è¡Œä¼˜å…ˆçº§æ’åºã€‚ç»™å®šä¸€ä¸ªç”¨æˆ·å®šä¹‰çš„ä»·å€¼é›†åˆï¼ŒConflictScopeè‡ªåŠ¨ç”Ÿæˆè¯­è¨€æ¨¡å‹é¢ä¸´ä¸¤ä¸ªä»·å€¼å†²çªçš„åœºæ™¯ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨LLMç¼–å†™çš„â€œç”¨æˆ·æç¤ºâ€æ¥æç¤ºç›®æ ‡æ¨¡å‹ï¼Œå¹¶è¯„ä¼°å®ƒä»¬çš„è‡ªç”±æ–‡æœ¬å“åº”ï¼Œä»¥å¼•å‡ºä»·å€¼é›†åˆä¸­çš„ä»·å€¼æ’åºã€‚é€šè¿‡æ¯”è¾ƒå¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼è¯„ä¼°çš„ç»“æœï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨æ›´å¼€æ”¾çš„ä»·å€¼å†²çªç¯å¢ƒä¸­ï¼Œä¼šä»æ”¯æŒä¿æŠ¤æ€§ä»·å€¼ï¼ˆå¦‚æ— å®³æ€§ï¼‰è½¬å‘æ”¯æŒä¸ªäººä»·å€¼ï¼ˆå¦‚ç”¨æˆ·è‡ªä¸»æ€§ï¼‰ã€‚ç„¶è€Œï¼Œåœ¨æ¨¡å‹çš„ç³»ç»Ÿæç¤ºä¸­åŒ…å«è¯¦ç»†çš„ä»·å€¼æ’åºï¼Œå¯ä»¥å°†ä¸ç›®æ ‡æ’åºçš„å¯¹é½æé«˜14%ï¼Œè¡¨æ˜ç³»ç»Ÿæç¤ºå¯ä»¥åœ¨ä»·å€¼å†²çªä¸‹é€‚åº¦æˆåŠŸåœ°å¯¹é½LLMè¡Œä¸ºã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†è¯„ä¼°æ¨¡å‹ä¸­ä»·å€¼ä¼˜å…ˆçº§çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥å·¥ä½œå¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½ç ”ç©¶ä¸»è¦å…³æ³¨äºå°†æ¨¡å‹ä¸ä¸€ç»„é¢„å®šä¹‰çš„ç›®æ ‡ä»·å€¼å¯¹é½ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼ŒLLMç»å¸¸é¢ä¸´ä¸åŒä»·å€¼ä¹‹é—´çš„å†²çªï¼Œä¾‹å¦‚ç”¨æˆ·è‡ªä¸»æ€§ä¸æ¨¡å‹çš„æ— å®³æ€§ã€‚ç°æœ‰çš„å¯¹é½æ•°æ®é›†ç¼ºä¹è¶³å¤Ÿçš„ä»·å€¼å†²çªåœºæ™¯ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è¿™äº›åœºæ™¯ä¸‹çš„è¡Œä¸ºéš¾ä»¥é¢„æµ‹å’Œæ§åˆ¶ã€‚å› æ­¤ï¼Œå¦‚ä½•è¯„ä¼°å’Œæ”¹å–„LLMåœ¨ä»·å€¼å†²çªä¸‹çš„ä»·å€¼ä¼˜å…ˆçº§æ’åºæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æµç¨‹ï¼Œç”¨äºç”Ÿæˆä»·å€¼å†²çªåœºæ™¯ï¼Œå¹¶åˆ†æLLMåœ¨è¿™äº›åœºæ™¯ä¸‹çš„å“åº”ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹åœ¨ä¸åŒä»·å€¼ä¹‹é—´çš„ä¼˜å…ˆçº§æ’åºã€‚é€šè¿‡åˆ†ææ¨¡å‹çš„å“åº”ï¼Œå¯ä»¥äº†è§£æ¨¡å‹åœ¨é¢ä¸´ä»·å€¼å†²çªæ—¶æ›´å€¾å‘äºå“ªä¸ªä»·å€¼ï¼Œä»è€Œè¯„ä¼°æ¨¡å‹çš„ä»·å€¼å¯¹é½ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢ç´¢äº†é€šè¿‡ç³»ç»Ÿæç¤ºæ¥å¼•å¯¼æ¨¡å‹åœ¨ä»·å€¼å†²çªä¸‹åšå‡ºæ›´ç¬¦åˆé¢„æœŸçš„å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šConflictScopeåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **ä»·å€¼é›†åˆå®šä¹‰**ï¼šç”¨æˆ·å®šä¹‰ä¸€ç»„éœ€è¦è¯„ä¼°çš„ä»·å€¼ï¼Œä¾‹å¦‚â€œç”¨æˆ·è‡ªä¸»æ€§â€ã€â€œæ— å®³æ€§â€ç­‰ã€‚2) **å†²çªåœºæ™¯ç”Ÿæˆ**ï¼šè‡ªåŠ¨ç”ŸæˆLLMé¢ä¸´ä¸¤ä¸ªä»·å€¼å†²çªçš„åœºæ™¯ã€‚è¿™äº›åœºæ™¯é€šè¿‡LLMç¼–å†™çš„â€œç”¨æˆ·æç¤ºâ€æ¥å‘ˆç°ã€‚3) **æ¨¡å‹å“åº”ç”Ÿæˆ**ï¼šä½¿ç”¨ç›®æ ‡LLMå¯¹ç”Ÿæˆçš„å†²çªåœºæ™¯è¿›è¡Œå“åº”ã€‚4) **ä»·å€¼æ’åºè¯„ä¼°**ï¼šåˆ†æLLMçš„è‡ªç”±æ–‡æœ¬å“åº”ï¼Œæå–æ¨¡å‹åœ¨ä¸åŒä»·å€¼ä¹‹é—´çš„ä¼˜å…ˆçº§æ’åºã€‚ç ”ç©¶æ¯”è¾ƒäº†å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼è¯„ä¼°ä¸¤ç§æ–¹å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ConflictScopeï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–çš„ä»·å€¼å†²çªåœºæ™¯ç”Ÿæˆå’Œè¯„ä¼°æµç¨‹ã€‚è¯¥æµç¨‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ­ç¤ºLLMåœ¨ä»·å€¼å†²çªä¸‹çš„ä»·å€¼ä¼˜å…ˆçº§åå¥½ï¼Œå¹¶ä¸ºæ”¹å–„æ¨¡å‹çš„ä»·å€¼å¯¹é½æä¾›æŒ‡å¯¼ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒConflictScopeèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡Œä¸ºï¼Œå¹¶å‘ç°æ¨¡å‹æ½œåœ¨çš„ä»·å€¼åå·®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç³»ç»Ÿæç¤ºæ–¹é¢ï¼Œç ”ç©¶æ¢ç´¢äº†åœ¨ç³»ç»Ÿæç¤ºä¸­åŒ…å«è¯¦ç»†çš„ä»·å€¼æ’åºä¿¡æ¯ï¼Œä»¥å¼•å¯¼æ¨¡å‹åœ¨ä»·å€¼å†²çªä¸‹åšå‡ºæ›´ç¬¦åˆé¢„æœŸçš„å†³ç­–ã€‚å…·ä½“æ¥è¯´ï¼Œç ”ç©¶äººå‘˜åœ¨ç³»ç»Ÿæç¤ºä¸­æ˜ç¡®æŒ‡å®šäº†ä¸åŒä»·å€¼ä¹‹é—´çš„ä¼˜å…ˆçº§å…³ç³»ï¼Œä¾‹å¦‚â€œæ— å®³æ€§ > ç”¨æˆ·è‡ªä¸»æ€§â€ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶äººå‘˜è¯•å›¾å½±å“æ¨¡å‹åœ¨ä»·å€¼å†²çªä¸‹çš„å†³ç­–ï¼Œä½¿å…¶æ›´å€¾å‘äºä¼˜å…ˆçº§æ›´é«˜çš„ä»·å€¼ã€‚ç ”ç©¶è¿˜æ¯”è¾ƒäº†ä¸åŒè¯„ä¼°æ–¹å¼ï¼ˆå¤šé¡¹é€‰æ‹© vs. å¼€æ”¾å¼ï¼‰å¯¹æ¨¡å‹ä»·å€¼ä¼˜å…ˆçº§çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¼€æ”¾å¼è¯„ä¼°ä¸­ï¼ŒLLMæ›´å€¾å‘äºæ”¯æŒä¸ªäººä»·å€¼ï¼ˆå¦‚ç”¨æˆ·è‡ªä¸»æ€§ï¼‰ï¼Œè€Œå¿½ç•¥ä¿æŠ¤æ€§ä»·å€¼ï¼ˆå¦‚æ— å®³æ€§ï¼‰ã€‚ç„¶è€Œï¼Œé€šè¿‡åœ¨ç³»ç»Ÿæç¤ºä¸­åŒ…å«è¯¦ç»†çš„ä»·å€¼æ’åºä¿¡æ¯ï¼Œå¯ä»¥å°†æ¨¡å‹ä¸ç›®æ ‡æ’åºçš„å¯¹é½ç¨‹åº¦æé«˜14%ã€‚è¿™è¡¨æ˜ç³»ç»Ÿæç¤ºæ˜¯ä¸€ç§æœ‰æ•ˆçš„ä»·å€¼å¯¹é½æ–¹æ³•ï¼Œä½†ä»æœ‰æå‡ç©ºé—´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡LLMåœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§ä¸å¯é æ€§ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—ã€é‡‘èç­‰é«˜é£é™©é¢†åŸŸï¼Œç¡®ä¿LLMåœ¨é¢ä¸´ä¼¦ç†å›°å¢ƒæ—¶èƒ½å¤Ÿåšå‡ºç¬¦åˆä¼¦ç†è§„èŒƒçš„å†³ç­–ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒLLMçš„ä»·å€¼å¯¹é½ç¨‹åº¦ï¼Œä¸ºç”¨æˆ·é€‰æ‹©åˆé€‚çš„LLMæä¾›å‚è€ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Past work seeks to align large language model (LLM)-based assistants with a target set of values, but such assistants are frequently forced to make tradeoffs between values when deployed. In response to the scarcity of value conflict in existing alignment datasets, we introduce ConflictScope, an automatic pipeline to evaluate how LLMs prioritize different values. Given a user-defined value set, ConflictScope automatically generates scenarios in which a language model faces a conflict between two values sampled from the set. It then prompts target models with an LLM-written "user prompt" and evaluates their free-text responses to elicit a ranking over values in the value set. Comparing results between multiple-choice and open-ended evaluations, we find that models shift away from supporting protective values, such as harmlessness, and toward supporting personal values, such as user autonomy, in more open-ended value conflict settings. However, including detailed value orderings in models' system prompts improves alignment with a target ranking by 14%, showing that system prompting can achieve moderate success at aligning LLM behavior under value conflict. Our work demonstrates the importance of evaluating value prioritization in models and provides a foundation for future work in this area.

