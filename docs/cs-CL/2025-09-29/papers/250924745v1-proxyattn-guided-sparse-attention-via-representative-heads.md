---
layout: default
title: ProxyAttn: Guided Sparse Attention via Representative Heads
---

# ProxyAttn: Guided Sparse Attention via Representative Heads

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24745" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24745v1</a>
  <a href="https://arxiv.org/pdf/2509.24745.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24745v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24745v1', 'ProxyAttn: Guided Sparse Attention via Representative Heads')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yixuan Wang, Huang He, Siqi Bao, Hua Wu, Haifeng Wang, Qingfu Zhu, Wanxiang Che

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: 14pages, 5figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/wyxstriker/ProxyAttn)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ProxyAttnï¼šé€šè¿‡ä»£è¡¨æ€§æ³¨æ„åŠ›å¤´å¼•å¯¼çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ é€Ÿé•¿æ–‡æœ¬å¤„ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–æ³¨æ„åŠ›` `é•¿æ–‡æœ¬å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶åŠ é€Ÿ` `ä»£è¡¨æ€§æ³¨æ„åŠ›å¤´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å—ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­é¢ä¸´é«˜ç¨€ç–ç‡ä¸‹æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶å—é‡è¦æ€§ä¼°è®¡è¿‡äºç²—ç³™ã€‚
2. ProxyAttné€šè¿‡é€‰æ‹©ä»£è¡¨æ€§æ³¨æ„åŠ›å¤´ï¼Œå¹¶åˆ©ç”¨å…¶å¾—åˆ†è¿‘ä¼¼æ‰€æœ‰å¤´çš„å¾—åˆ†ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„å—é‡è¦æ€§è¯„ä¼°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒProxyAttnåœ¨å¤šç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½å’Œæ•ˆç‡ï¼Œå®ç°äº†é«˜è¾¾10.3å€çš„æ³¨æ„åŠ›åŠ é€Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦é™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šçš„æ•ˆç‡ã€‚æœ€è¿‘ï¼ŒåŠ¨æ€ä¼°è®¡å—é‡è¦æ€§çš„æ–¹æ³•å®ç°äº†é«˜æ•ˆçš„å—ç¨€ç–æ³¨æ„åŠ›ï¼Œæ˜¾è‘—åŠ é€Ÿäº†LLMçš„é•¿æ–‡æœ¬é¢„å¡«å……ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç²—ç²’åº¦çš„ä¼°è®¡ä¸å¯é¿å…åœ°å¯¼è‡´åœ¨é«˜ç¨€ç–ç‡ä¸‹çš„æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ProxyAttnï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„ç¨€ç–æ³¨æ„åŠ›ç®—æ³•ï¼Œé€šè¿‡å‹ç¼©æ³¨æ„åŠ›å¤´çš„ç»´åº¦æ¥å®ç°æ›´ç²¾ç¡®çš„å—ä¼°è®¡ã€‚åŸºäºå¯¹å¤šä¸ªæ³¨æ„åŠ›å¤´ä¹‹é—´ç›¸ä¼¼æ€§çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨æ± åŒ–çš„ä»£è¡¨æ€§æ³¨æ„åŠ›å¤´çš„åˆ†æ•°æ¥è¿‘ä¼¼æ‰€æœ‰å¤´çš„åˆ†æ•°ã€‚ä¸ºäº†è§£å†³ä¸åŒå¤´ä¹‹é—´çš„ä¸åŒç¨€ç–æ€§ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å—æ„ŸçŸ¥çš„åŠ¨æ€é¢„ç®—ä¼°è®¡æ–¹æ³•ã€‚é€šè¿‡å°†æ¥è‡ªä»£è¡¨æ€§ä»£ç†å¤´çš„åˆ†æ•°ä¸å¤šå¤´åŠ¨æ€é¢„ç®—ç›¸ç»“åˆï¼Œæˆ‘ä»¬ä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬å®ç°äº†æ›´ç»†ç²’åº¦çš„å—é‡è¦æ€§è¯„ä¼°ã€‚åœ¨å„ç§ä¸»æµæ¨¡å‹å’Œå¹¿æ³›åŸºå‡†ä¸Šçš„å®éªŒè¯å®äº†æ³¨æ„åŠ›å¤´ä¹‹é—´æ½œåœ¨çš„ç›¸ä¼¼æ€§ã€‚åˆ©ç”¨ç»†ç²’åº¦çš„ä¼°è®¡ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼ŒProxyAttnå¯ä»¥åœ¨ä¸æ˜¾è‘—æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå®ç°é«˜è¾¾10.3å€çš„æ³¨æ„åŠ›åŠ é€Ÿå’Œ2.4å€çš„é¢„å¡«å……åŠ é€Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œç”±äºæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦è€Œå¯¼è‡´çš„æ•ˆç‡ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰çš„å—ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•è™½ç„¶èƒ½å¤ŸåŠ é€Ÿè®¡ç®—ï¼Œä½†ç”±äºå…¶ç²—ç²’åº¦çš„å—é‡è¦æ€§ä¼°è®¡ï¼Œåœ¨é«˜ç¨€ç–ç‡ä¸‹ä¼šé€ æˆæ˜¾è‘—çš„æ€§èƒ½æŸå¤±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šä¸ªæ³¨æ„åŠ›å¤´ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œé€šè¿‡å°‘é‡â€œä»£è¡¨æ€§â€çš„æ³¨æ„åŠ›å¤´æ¥è¿‘ä¼¼æ‰€æœ‰æ³¨æ„åŠ›å¤´çš„è¡Œä¸ºã€‚å…·ä½“æ¥è¯´ï¼Œé€‰æ‹©ä¸€éƒ¨åˆ†æ³¨æ„åŠ›å¤´ä½œä¸ºä»£ç†ï¼ˆProxyï¼‰ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬çš„å¾—åˆ†æ¥ä¼°è®¡æ•´ä¸ªæ³¨æ„åŠ›å—çš„é‡è¦æ€§ã€‚è¿™æ ·å¯ä»¥åœ¨é™ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œä¿æŒè¾ƒé«˜çš„ä¼°è®¡ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šProxyAttnç®—æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **ä»£è¡¨æ€§æ³¨æ„åŠ›å¤´é€‰æ‹©**ï¼šé€‰æ‹©ä¸€éƒ¨åˆ†æ³¨æ„åŠ›å¤´ä½œä¸ºä»£ç†å¤´ã€‚é€‰æ‹©ç­–ç•¥æœªçŸ¥ï¼Œè®ºæ–‡ä¸­å¯èƒ½é»˜è®¤éšæœºé€‰æ‹©æˆ–ä½¿ç”¨æŸç§å¯å‘å¼æ–¹æ³•ã€‚2) **ä»£ç†å¤´å¾—åˆ†è®¡ç®—**ï¼šè®¡ç®—è¿™äº›ä»£ç†å¤´çš„æ³¨æ„åŠ›å¾—åˆ†ã€‚3) **å—é‡è¦æ€§ä¼°è®¡**ï¼šä½¿ç”¨ä»£ç†å¤´çš„å¾—åˆ†æ¥è¿‘ä¼¼æ•´ä¸ªæ³¨æ„åŠ›å—çš„é‡è¦æ€§ã€‚4) **åŠ¨æ€é¢„ç®—åˆ†é…**ï¼šæ ¹æ®å—çš„é‡è¦æ€§ï¼ŒåŠ¨æ€åœ°ä¸ºæ¯ä¸ªå—åˆ†é…è®¡ç®—èµ„æºã€‚5) **ç¨€ç–æ³¨æ„åŠ›è®¡ç®—**ï¼šåªå¯¹é‡è¦çš„å—è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œå®ç°åŠ é€Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šProxyAttnçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨æ³¨æ„åŠ›å¤´ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œé€šè¿‡ä»£ç†å¤´æ¥è¿‘ä¼¼æ•´ä¸ªæ³¨æ„åŠ›å—çš„é‡è¦æ€§ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨é™ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œä¿æŒè¾ƒé«˜çš„ä¼°è®¡ç²¾åº¦ï¼Œä»è€Œåœ¨é«˜ç¨€ç–ç‡ä¸‹ä¹Ÿèƒ½è·å¾—è¾ƒå¥½çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒProxyAttnçš„å—é‡è¦æ€§ä¼°è®¡æ›´åŠ ç²¾ç»†ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«é‡è¦çš„å—ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æå‡ºäº†å—æ„ŸçŸ¥çš„åŠ¨æ€é¢„ç®—ä¼°è®¡æ–¹æ³•ï¼Œä»¥è§£å†³ä¸åŒå¤´ä¹‹é—´çš„ä¸åŒç¨€ç–æ€§ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ï¼Œå¦‚ä»£è¡¨æ€§æ³¨æ„åŠ›å¤´çš„é€‰æ‹©ç­–ç•¥ã€ä»£ç†å¤´å¾—åˆ†çš„æ± åŒ–æ–¹æ³•ã€ä»¥åŠåŠ¨æ€é¢„ç®—åˆ†é…çš„å…·ä½“ç®—æ³•ï¼Œåœ¨è®ºæ–‡ä¸­å¯èƒ½æ²¡æœ‰è¯¦ç»†æè¿°ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ä»£ç æ‰èƒ½ç¡®å®šã€‚æŸå¤±å‡½æ•°æœªçŸ¥ï¼Œç½‘ç»œç»“æ„ä¸åŸå§‹Transformerä¿æŒä¸€è‡´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ProxyAttnåœ¨å¤šç§ä¸»æµæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProxyAttnå¯ä»¥åœ¨ä¸æ˜¾è‘—æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå®ç°é«˜è¾¾10.3å€çš„æ³¨æ„åŠ›åŠ é€Ÿå’Œ2.4å€çš„é¢„å¡«å……åŠ é€Ÿã€‚è¿™è¡¨æ˜ProxyAttnèƒ½å¤Ÿæœ‰æ•ˆåœ°é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶æå‡é•¿æ–‡æœ¬å¤„ç†çš„æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ProxyAttnå¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿æ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚æ–‡æ¡£æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—ï¼ŒProxyAttnèƒ½å¤Ÿæå‡è¿™äº›åº”ç”¨çš„å¤„ç†é€Ÿåº¦å’Œæ•ˆç‡ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„æ–‡æœ¬ï¼Œå¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•è¿˜æœ‰æ½œåŠ›åº”ç”¨äºå…¶ä»–ç±»å‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¾‹å¦‚è§†è§‰Transformerç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The quadratic complexity of attention mechanisms limits the efficiency of Large Language Models (LLMs) on long-text tasks. Recently, methods that dynamically estimate block importance have enabled efficient block sparse attention, leading to significant acceleration in long-text pre-filling of LLMs. However, their coarse-grained estimation inevitably leads to performance degradation at high sparsity rates. In this work, we propose ProxyAttn, a training-free sparse attention algorithm that achieves more precise block estimation by compressing the dimension of attention heads. Based on our observation of the similarity among multiple attention heads, we use the scores of pooled representative heads to approximate the scores for all heads. To account for the varying sparsity among heads, we also propose a block-aware dynamic budget estimation method. By combining the scores from representative proxy heads with multi-head dynamic budgets, we achieve a more fine-grained block importance evaluation at low computational cost. Experiments on a variety of mainstream models and extensive benchmarks confirm the underlying similarity among attention heads. Leveraging a fine-grained estimation, the proposed method achieves substantial gains in performance and efficiency compared to existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention acceleration and 2.4x prefilling acceleration without significant performance loss. Our code is available at https://github.com/wyxstriker/ProxyAttn.

