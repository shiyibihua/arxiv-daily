---
layout: default
title: Expanding Computation Spaces of LLMs at Inference Time
---

# Expanding Computation Spaces of LLMs at Inference Time

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24884" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24884v1</a>
  <a href="https://arxiv.org/pdf/2509.24884.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24884v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24884v1', 'Expanding Computation Spaces of LLMs at Inference Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yoonna Jang, Kisu Yang, Isabelle Augenstein

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ¨ç†æ—¶æ‰©å±•LLMè®¡ç®—ç©ºé—´çš„æ–¹æ³•ï¼Œæå‡é—®é¢˜è§£å†³èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†æ—¶æ‰©å±•` `è®¡ç®—ç©ºé—´` `å¡«å……token` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–è®­ç»ƒé¢å¤–çš„tokenæ¥æ‰©å±•è®¡ç®—ç©ºé—´ï¼Œå¢åŠ äº†è®­ç»ƒæˆæœ¬å’Œæ¨¡å‹å¤æ‚åº¦ã€‚
2. è¯¥ç ”ç©¶æ¢ç´¢åœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡æ’å…¥å¡«å……tokenåºåˆ—æ¥æ‰©å±•LLMçš„è®¡ç®—ç©ºé—´ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼Œé€‚å½“çš„å¡«å……tokenç±»å‹å’Œä½ç½®å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨é—®ç­”å’Œæ•°å­¦ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå°¤å…¶å¯¹å°æ¨¡å‹æå‡æ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ—¶ï¼Œèƒ½å¦é€šè¿‡äººå·¥æ’å…¥å¡«å……è¯åºåˆ—æ¥æ‰©å±•å…¶è®¡ç®—ç©ºé—´ã€‚ä¸åŒäºä»¥å¾€è®­ç»ƒå¡«å……è¯æˆ–ç‰¹æ®Štokenä½œä¸ºé¢å¤–è®¡ç®—ç©ºé—´çš„æ–¹æ³•ï¼Œæœ¬æ–‡æ¢ç´¢äº†ä»…åœ¨æ¨ç†é˜¶æ®µæ’å…¥å¡«å……è¯åºåˆ—çš„å¯èƒ½æ€§ã€‚ç ”ç©¶é¦–å…ˆç¡®å®šäº†æœ‰æ•ˆçš„tokenç±»å‹ã€æ•°é‡å’Œæ’å…¥ä½ç½®ï¼Œç„¶åè€ƒå¯Ÿäº†æ¨¡å‹åœ¨è®­ç»ƒçš„å“ªä¸ªé˜¶æ®µå¼€å§‹åˆ©ç”¨æ‰©å±•çš„è®¡ç®—ç©ºé—´ï¼Œæœ€åé€šè¿‡æ³¨æ„åŠ›å›¾åˆ†æäº†è¿™äº›ç©ºé—´å†…çš„åŠ¨æ€ã€‚åœ¨1.7Båˆ°32Bçš„æ¨¡å‹ä¸Šï¼Œé’ˆå¯¹å¼€æ”¾åŸŸé—®ç­”å’Œæ•°å­¦ä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼Œé€‚å½“çš„tokenç±»å‹å’Œæ•°é‡æœ‰æ‰€ä¸åŒï¼Œä½†å°†å¡«å……è¯ç›´æ¥æ”¾åœ¨æœ€ç»ˆçš„'Answer:' tokenä¹‹å‰æ˜¯æœ€æœ‰æ•ˆçš„ã€‚è¾ƒå°çš„æ¨¡å‹å—ç›Šæœ€å¤§ï¼Œåœ¨SmolLM2-1.7B-Instructä¸­æå‡é«˜è¾¾12.372ä¸ªç™¾åˆ†ç‚¹ï¼Œè¡¨æ˜è¿™äº›ç©ºé—´å……å½“äº†é¢å¤–çš„è®¡ç®—èƒ½åŠ›ï¼Œè€Œä¸æ˜¯å†—ä½™è¾“å…¥ã€‚æ³¨æ„åŠ›å›¾æ˜¾ç¤ºï¼Œæ‰©å±•çš„ç©ºé—´é€šå¸¸å»¶ç»­äº†åŸå§‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ—¶ä¼šå…³æ³¨é—®é¢˜æˆ–ç­”æ¡ˆé€‰é¡¹ï¼Œè¡¨æ˜å…¶å¯¹é—®é¢˜è§£å†³å…·æœ‰æœ‰æ„ä¹‰çš„è®¡ç®—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶ï¼Œè®¡ç®—èƒ½åŠ›å—é™äºæ¨¡å‹å‚æ•°å’Œè¾“å…¥é•¿åº¦ã€‚ä»¥å¾€æ‰©å±•è®¡ç®—ç©ºé—´çš„æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œä¾‹å¦‚è®­ç»ƒç‰¹å®šçš„å¡«å……tokenæˆ–ç‰¹æ®Štokenï¼Œè¿™å¢åŠ äº†è®­ç»ƒæˆæœ¬å’Œæ¨¡å‹å¤æ‚åº¦ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæ‰©å±•LLMçš„è®¡ç®—ç©ºé—´æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡åœ¨è¾“å…¥æ–‡æœ¬ä¸­äººå·¥æ’å…¥ä¸€æ®µå¡«å……tokenåºåˆ—ï¼Œæ¥æ‰©å±•LLMçš„è®¡ç®—ç©ºé—´ã€‚è¿™ç§æ–¹æ³•æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°æˆ–è¿›è¡Œé¢å¤–çš„è®­ç»ƒï¼Œåªéœ€è¦åœ¨æ¨ç†æ—¶å¯¹è¾“å…¥è¿›è¡Œç®€å•çš„å¤„ç†ã€‚é€šè¿‡è°ƒæ•´å¡«å……tokençš„ç±»å‹ã€æ•°é‡å’Œæ’å…¥ä½ç½®ï¼Œå¯ä»¥ä¼˜åŒ–æ‰©å±•è®¡ç®—ç©ºé—´çš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) é€‰æ‹©åˆé€‚çš„å¡«å……tokenç±»å‹ï¼Œä¾‹å¦‚å¸¸è§çš„tokenæˆ–éšæœºç”Ÿæˆçš„tokenï¼›2) ç¡®å®šå¡«å……tokençš„æ•°é‡ï¼Œé€šè¿‡å®éªŒæ‰¾åˆ°æœ€ä½³çš„æ•°é‡ï¼›3) ç¡®å®šå¡«å……tokençš„æ’å…¥ä½ç½®ï¼Œä¾‹å¦‚åœ¨é—®é¢˜ä¹‹åã€ç­”æ¡ˆä¹‹å‰ç­‰ï¼›4) å°†å¡«å……tokenåºåˆ—æ’å…¥åˆ°è¾“å…¥æ–‡æœ¬ä¸­ï¼Œç„¶åè¾“å…¥åˆ°LLMä¸­è¿›è¡Œæ¨ç†ï¼›5) åˆ†æLLMçš„è¾“å‡ºç»“æœï¼Œè¯„ä¼°å¡«å……tokenå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œå®ƒæå‡ºäº†ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒï¼Œä»…åœ¨æ¨ç†é˜¶æ®µå³å¯æ‰©å±•LLMè®¡ç®—ç©ºé—´çš„æ–¹æ³•ã€‚ä¸ä»¥å¾€éœ€è¦è®­ç»ƒå¡«å……tokenæˆ–ç‰¹æ®Štokençš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ ç®€å•ã€é«˜æ•ˆï¼Œå¹¶ä¸”å¯ä»¥çµæ´»åœ°åº”ç”¨äºä¸åŒçš„LLMå’Œä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) å¡«å……tokenç±»å‹çš„é€‰æ‹©ï¼Œå®éªŒå‘ç°ä¸åŒçš„tokenç±»å‹å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ä¸åŒï¼›2) å¡«å……tokenæ•°é‡çš„ç¡®å®šï¼Œå®éªŒè¡¨æ˜å­˜åœ¨ä¸€ä¸ªæœ€ä½³çš„tokenæ•°é‡ï¼Œè¿‡å¤šæˆ–è¿‡å°‘éƒ½ä¼šé™ä½æ¨¡å‹æ€§èƒ½ï¼›3) å¡«å……tokenæ’å…¥ä½ç½®çš„é€‰æ‹©ï¼Œå®éªŒå‘ç°å°†å¡«å……tokenæ’å…¥åˆ°ç­”æ¡ˆä¹‹å‰æ•ˆæœæœ€å¥½ï¼›4) ä½¿ç”¨æ³¨æ„åŠ›å›¾æ¥åˆ†æå¡«å……tokenå¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£æ‰©å±•è®¡ç®—ç©ºé—´çš„ä½œç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ¨ç†æ—¶æ’å…¥å¡«å……tokenå¯ä»¥æ˜¾è‘—æå‡LLMåœ¨å¼€æ”¾åŸŸé—®ç­”å’Œæ•°å­¦ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å¯¹äºè¾ƒå°çš„æ¨¡å‹ï¼Œä¾‹å¦‚SmolLM2-1.7B-Instructï¼Œæ€§èƒ½æå‡é«˜è¾¾12.372ä¸ªç™¾åˆ†ç‚¹ã€‚æ³¨æ„åŠ›å›¾åˆ†ææ˜¾ç¤ºï¼Œæ‰©å±•çš„è®¡ç®—ç©ºé—´èƒ½å¤Ÿå»¶ç»­åŸå§‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶å…³æ³¨é—®é¢˜æˆ–ç­”æ¡ˆé€‰é¡¹ï¼Œè¡¨æ˜å…¶å¯¹é—®é¢˜è§£å†³å…·æœ‰å®é™…æ„ä¹‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æ‰©å±•LLMè®¡ç®—èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚å¤æ‚é—®é¢˜æ±‚è§£ã€é•¿æ–‡æœ¬ç†è§£ã€çŸ¥è¯†åº“é—®ç­”ç­‰ã€‚é€šè¿‡åœ¨æ¨ç†æ—¶åŠ¨æ€æ‰©å±•è®¡ç®—ç©ºé—´ï¼Œå¯ä»¥æå‡LLMåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼Œå¹¶é™ä½éƒ¨ç½²æˆæœ¬ã€‚è¯¥æ–¹æ³•è¿˜å¯ç”¨äºæ¢ç´¢LLMçš„å†…éƒ¨è®¡ç®—æœºåˆ¶ï¼Œä¸ºæ¨¡å‹ä¼˜åŒ–å’Œæ”¹è¿›æä¾›æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-thought (CoT) rationale enables language models to use additional task-related text for problem-solving, benefiting not only from detailed reasoning steps but also from the expanded computational space of longer inputs. Prior work has trained filler or special tokens to serve as additional computation spaces. In this study, we investigate whether language models can leverage artificially inserted sequences of filler tokens solely at inference. We first identify effective token types, numbers, and insertion locations, then examine at what stage of training models begin to exploit the expanded computation space, and finally analyze dynamics within these spaces via attention maps. Experiments on models ranging from 1.7B to 32B across open-domain QA and math tasks show that appropriate token types and counts vary, but placing filler tokens directly before the final 'Answer:' token is most effective. Smaller models benefit most, up to 12.372 percentage points in SmolLM2-1.7B-Instruct, indicating that these spaces act as additional computational capacity rather than redundant input. Attention maps reveal that expanded spaces often continue the original attention mechanism and sometimes focus on questions or answer options, suggesting meaningful computation for problem-solving.

