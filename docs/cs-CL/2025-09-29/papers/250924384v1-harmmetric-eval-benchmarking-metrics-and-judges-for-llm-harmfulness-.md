---
layout: default
title: HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment
---

# HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24384" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24384v1</a>
  <a href="https://arxiv.org/pdf/2509.24384.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24384v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24384v1', 'HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Langqi Yang, Tianhang Zheng, Kedong Xiu, Yixuan Chen, Di Wang, Puning Zhao, Zhan Qin, Kui Ren

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/datasets/qusgo)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHarmMetric Evalï¼Œç”¨äºå…¨é¢è¯„ä¼°LLMæœ‰å®³æ€§è¯„ä¼°æŒ‡æ ‡ä¸åˆ¤åˆ«å™¨çš„è´¨é‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æœ‰å®³æ€§è¯„ä¼°` `è¶Šç‹±æ”»å‡»` `åŸºå‡†æµ‹è¯•` `å®‰å…¨å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯„ä¼°LLMæœ‰å®³æ€§çš„æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨ç¼ºä¹ç³»ç»Ÿæ€§çš„è´¨é‡è¯„ä¼°ï¼Œå½±å“äº†è¶Šç‹±æ”»å‡»æœ‰æ•ˆæ€§è¯„ä¼°çš„å¯ä¿¡åº¦ã€‚
2. HarmMetric Evalæ—¨åœ¨æä¾›ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºæ•´ä½“å’Œç»†ç²’åº¦åœ°è¯„ä¼°æœ‰å®³æ€§æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨çš„æœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä¼ ç»ŸæŒ‡æ ‡METEORå’ŒROUGE-1åœ¨è¯„ä¼°æ¨¡å‹å“åº”æœ‰å®³æ€§æ–¹é¢ä¼˜äºåŸºäºLLMçš„åˆ¤åˆ«å™¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½å¯¹å…¶å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œè¶Šç‹±æ”»å‡»ä¼šç ´åè¿™ç§å¯¹é½ï¼Œä»è€Œå¼•è¯±LLMäº§ç”Ÿæœ‰å®³è¾“å‡ºã€‚è¿‘å¹´æ¥ï¼Œæ¶Œç°äº†å¤§é‡çš„è¶Šç‹±æ”»å‡»ï¼ŒåŒæ—¶ä¹Ÿå‡ºç°äº†å„ç§ç”¨äºè¯„ä¼°LLMè¾“å‡ºæœ‰å®³æ€§çš„æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨ã€‚ç„¶è€Œï¼Œç¼ºä¹ä¸€ä¸ªç³»ç»Ÿçš„åŸºå‡†æ¥è¯„ä¼°è¿™äº›æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨çš„è´¨é‡å’Œæœ‰æ•ˆæ€§ï¼Œè¿™å‰Šå¼±äº†å·²æŠ¥å‘Šçš„è¶Šç‹±æœ‰æ•ˆæ€§å’Œå…¶ä»–é£é™©çš„å¯ä¿¡åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HarmMetric Evalï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œæ—¨åœ¨æ”¯æŒå¯¹æœ‰å®³æ€§æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨çš„æ•´ä½“å’Œç»†ç²’åº¦è¯„ä¼°ã€‚æˆ‘ä»¬çš„åŸºå‡†åŒ…æ‹¬ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å…·æœ‰ä»£è¡¨æ€§çš„æœ‰å®³æç¤ºä»¥åŠå„ç§æœ‰å®³å’Œéæœ‰å®³çš„æ¨¡å‹å“åº”ï¼Œä»¥åŠä¸€ä¸ªçµæ´»çš„è¯„åˆ†æœºåˆ¶ï¼Œä¸å„ç§æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨å…¼å®¹ã€‚é€šè¿‡HarmMetric Evalï¼Œæˆ‘ä»¬çš„å¤§é‡å®éªŒæ­ç¤ºäº†ä¸€ä¸ªä»¤äººæƒŠè®¶çš„ç»“æœï¼šä¸¤ä¸ªä¼ ç»Ÿçš„æŒ‡æ ‡â€”â€”METEORå’ŒROUGE-1â€”â€”åœ¨è¯„ä¼°æ¨¡å‹å“åº”çš„æœ‰å®³æ€§æ–¹é¢ä¼˜äºåŸºäºLLMçš„åˆ¤åˆ«å™¨ï¼Œè¿™æŒ‘æˆ˜äº†å…³äºLLMåœ¨è¯¥é¢†åŸŸä¼˜è¶Šæ€§çš„æ™®éçœ‹æ³•ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¯åœ¨https://huggingface.co/datasets/qusgo/HarmMetric_Evalå…¬å¼€è·å–ï¼Œä»£ç å¯åœ¨https://anonymous.4open.science/r/HarmMetric-Eval-4CBEè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•ç³»ç»Ÿæ€§åœ°è¯„ä¼°ç”¨äºè¡¡é‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰å®³æ€§çš„å„ç§æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨çš„è´¨é‡å’Œæœ‰æ•ˆæ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ ‡å‡†ï¼Œå¯¼è‡´å¯¹LLMå®‰å…¨æ€§çš„è¯„ä¼°ç»“æœéš¾ä»¥æ¯”è¾ƒå’ŒéªŒè¯ï¼ŒåŒæ—¶ä¹Ÿéš¾ä»¥ç¡®å®šå“ªäº›æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨çœŸæ­£æœ‰æ•ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªé«˜è´¨é‡çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«å„ç§æœ‰å®³æç¤ºä»¥åŠå¯¹åº”çš„æœ‰å®³å’Œéæœ‰å®³æ¨¡å‹å“åº”ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªçµæ´»çš„è¯„åˆ†æœºåˆ¶ï¼Œä½¿å¾—ä¸åŒçš„æœ‰å®³æ€§æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨å¯ä»¥åœ¨åŒä¸€åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°å’Œæ¯”è¾ƒã€‚é€šè¿‡å¯¹å„ç§æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°è¿›è¡Œåˆ†æï¼Œä»è€Œç¡®å®šå…¶ä¼˜åŠ£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHarmMetric EvalåŸºå‡†ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) é«˜è´¨é‡çš„æœ‰å®³æç¤ºæ•°æ®é›†ï¼›2) é’ˆå¯¹æ¯ä¸ªæç¤ºï¼Œæ”¶é›†LLMç”Ÿæˆçš„æœ‰å®³å’Œéæœ‰å®³å“åº”ï¼›3) çµæ´»çš„è¯„åˆ†æœºåˆ¶ï¼Œå…è®¸ä¸åŒçš„æœ‰å®³æ€§æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨å¯¹å“åº”è¿›è¡Œè¯„åˆ†ï¼›4) è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡ä¸åŒæŒ‡æ ‡å’Œåˆ¤åˆ«å™¨çš„æ€§èƒ½ï¼Œä¾‹å¦‚å‡†ç¡®ç‡ã€å¬å›ç‡ç­‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMæœ‰å®³æ€§è¯„ä¼°æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨çš„åŸºå‡†æ•°æ®é›†ã€‚ä¸ä»¥å¾€çš„ç ”ç©¶ä¸åŒï¼Œè¯¥ç ”ç©¶ä¸ä»…å…³æ³¨LLMæœ¬èº«çš„å®‰å…¨é—®é¢˜ï¼Œæ›´å…³æ³¨ç”¨äºè¯„ä¼°LLMå®‰å…¨æ€§çš„å·¥å…·çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¼ ç»Ÿçš„æ–‡æœ¬ç›¸ä¼¼åº¦æŒ‡æ ‡åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºåŸºäºLLMçš„åˆ¤åˆ«å™¨ï¼Œè¿™æŒ‘æˆ˜äº†ä»¥å¾€çš„è®¤çŸ¥ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†åŒ…å«äº†å¤šç§ç±»å‹çš„æœ‰å®³æç¤ºï¼Œä¾‹å¦‚æ¶‰åŠä»‡æ¨è¨€è®ºã€æš´åŠ›ã€æ­§è§†ç­‰ã€‚å¯¹äºæ¯ä¸ªæç¤ºï¼Œæ”¶é›†äº†å¤šä¸ªLLMç”Ÿæˆçš„å“åº”ï¼Œå¹¶äººå·¥æ ‡æ³¨äº†è¿™äº›å“åº”çš„æœ‰å®³ç¨‹åº¦ã€‚è¯„åˆ†æœºåˆ¶å…è®¸ä¸åŒçš„æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨å¯¹å“åº”è¿›è¡Œè¯„åˆ†ï¼Œå¹¶æ ¹æ®è¯„åˆ†ç»“æœè®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†METEORå’ŒROUGE-1ç­‰ä¼ ç»ŸæŒ‡æ ‡ï¼Œä»¥åŠåŸºäºLLMçš„åˆ¤åˆ«å™¨ï¼Œä¾‹å¦‚GPT-3å’ŒBERTç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¼ ç»Ÿçš„æ–‡æœ¬ç›¸ä¼¼åº¦æŒ‡æ ‡METEORå’ŒROUGE-1åœ¨è¯„ä¼°æ¨¡å‹å“åº”çš„æœ‰å®³æ€§æ–¹é¢ï¼Œè¡¨ç°ä¼˜äºåŸºäºLLMçš„åˆ¤åˆ«å™¨ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†å½“å‰æ™®éè®¤ä¸ºLLMåœ¨æœ‰å®³æ€§è¯„ä¼°æ–¹é¢å…·æœ‰ä¼˜åŠ¿çš„è§‚ç‚¹ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºLLMçš„å®‰å…¨è¯„ä¼°å’Œé£é™©ç®¡ç†ã€‚å¼€å‘è€…å¯ä»¥ä½¿ç”¨HarmMetric Evalæ¥è¯„ä¼°å’Œé€‰æ‹©åˆé€‚çš„æœ‰å®³æ€§è¯„ä¼°æŒ‡æ ‡å’Œåˆ¤åˆ«å™¨ï¼Œä»è€Œæé«˜LLMçš„å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†è¿˜å¯ä»¥ç”¨äºæ¯”è¾ƒä¸åŒLLMçš„å®‰å…¨æ€§ï¼Œå¹¶ä¿ƒè¿›LLMå®‰å…¨é¢†åŸŸçš„ç ”ç©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The alignment of large language models (LLMs) with human values is critical for their safe deployment, yet jailbreak attacks can subvert this alignment to elicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak attacks has emerged, accompanied by diverse metrics and judges to assess the harmfulness of the LLM outputs. However, the absence of a systematic benchmark to assess the quality and effectiveness of these metrics and judges undermines the credibility of the reported jailbreak effectiveness and other risks. To address this gap, we introduce HarmMetric Eval, a comprehensive benchmark designed to support both overall and fine-grained evaluation of harmfulness metrics and judges. Our benchmark includes a high-quality dataset of representative harmful prompts paired with diverse harmful and non-harmful model responses, alongside a flexible scoring mechanism compatible with various metrics and judges. With HarmMetric Eval, our extensive experiments uncover a surprising result: two conventional metrics--METEOR and ROUGE-1--outperform LLM-based judges in evaluating the harmfulness of model responses, challenging prevailing beliefs about LLMs' superiority in this domain. Our dataset is publicly available at https://huggingface.co/datasets/qusgo/HarmMetric_Eval, and the code is available at https://anonymous.4open.science/r/HarmMetric-Eval-4CBE.

