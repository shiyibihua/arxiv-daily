---
layout: default
title: DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models
---

# DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24296" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24296v1</a>
  <a href="https://arxiv.org/pdf/2509.24296.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24296v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24296v1', 'DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zherui Li, Zheng Nie, Zhenhong Zhou, Yufei Guo, Yue Liu, Yitong Zhang, Yu Cheng, Qingsong Wen, Kun Wang, Jiaheng Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/niez233/DiffuGuard)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DiffuGuardï¼šæ­ç¤ºå¹¶ä¿®å¤æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ä¸­å›ºæœ‰çš„å®‰å…¨æ¼æ´**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£è¯­è¨€æ¨¡å‹` `å®‰å…¨æ€§` `è¶Šç‹±æ”»å‡»` `é˜²å¾¡æ¡†æ¶` `éšæœºé€€ç«` `é£é™©æ£€æµ‹` `æ–‡æœ¬ç”Ÿæˆ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£è¯­è¨€æ¨¡å‹é¢ä¸´æ–°å‹å®‰å…¨å¨èƒï¼Œå…¶è¿­ä»£ç”Ÿæˆæ–¹å¼ä¸è‡ªå›å½’æ¨¡å‹ä¸åŒï¼Œæ˜“å—æ­¥å†…å’Œæ­¥é—´åŠ¨æ€æ”»å‡»ã€‚
2. DiffuGuardé€šè¿‡éšæœºé€€ç«é‡æ©ç å’Œå—çº§å®¡è®¡ä¿®å¤ï¼Œåœ¨è§£ç è¿‡ç¨‹ä¸­å¼•å…¥éšæœºæ€§å’Œé£é™©æ£€æµ‹ï¼Œæå‡æ¨¡å‹å®‰å…¨æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDiffuGuardèƒ½æ˜¾è‘—é™ä½è¶Šç‹±æ”»å‡»æˆåŠŸç‡ï¼Œä»47.9%é™è‡³14.7%ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½å’Œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹(dLLMs)çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„å®‰å…¨æ¼æ´ï¼Œè¿™äº›æ¼æ´ä¸è‡ªå›å½’LLMsæœ‰ç€æœ¬è´¨åŒºåˆ«ï¼Œæºäºå…¶è¿­ä»£å’Œå¹¶è¡Œçš„ç”Ÿæˆæœºåˆ¶ã€‚æœ¬æ–‡æ·±å…¥åˆ†æäº†dLLMåœ¨æ­¥å†…å’Œæ­¥é—´åŠ¨æ€æ–¹é¢çš„æ¼æ´ï¼Œä»¥åº”å¯¹è¶Šç‹±æ”»å‡»ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ ‡å‡†è´ªå©ªé‡æ©ç ç­–ç•¥ä¸­å›ºæœ‰çš„æœ‰å®³åå·®ï¼Œå¹¶è¯†åˆ«å‡ºä¸€ä¸ªå…³é”®ç°è±¡ï¼Œå³å»å™ªè·¯å¾„ä¾èµ–æ€§ï¼Œå…¶ä¸­æ—©æœŸtokençš„å®‰å…¨æ€§å†³å®šæ€§åœ°å½±å“æœ€ç»ˆè¾“å‡ºã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè™½ç„¶å½“å‰çš„è§£ç ç­–ç•¥æ„æˆäº†ä¸€ä¸ªé‡å¤§æ¼æ´ï¼Œä½†dLLMå…·æœ‰å·¨å¤§çš„å†…åœ¨å®‰å…¨æ½œåŠ›ã€‚ä¸ºäº†é‡Šæ”¾è¿™ç§æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†DiffuGuardï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„é˜²å¾¡æ¡†æ¶ï¼Œé€šè¿‡åŒé˜¶æ®µæ–¹æ³•è§£å†³æ¼æ´ï¼šéšæœºé€€ç«é‡æ©ç åŠ¨æ€å¼•å…¥å—æ§éšæœºæ€§ä»¥å‡è½»è´ªå©ªé€‰æ‹©åå·®ï¼Œè€Œå—çº§å®¡è®¡å’Œä¿®å¤åˆ©ç”¨å†…éƒ¨æ¨¡å‹è¡¨ç¤ºè¿›è¡Œè‡ªä¸»é£é™©æ£€æµ‹å’Œå¼•å¯¼æ ¡æ­£ã€‚åœ¨å››ä¸ªdLLMä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDiffuGuardå…·æœ‰å“è¶Šçš„æœ‰æ•ˆæ€§ï¼Œé’ˆå¯¹å…­ç§ä¸åŒçš„è¶Šç‹±æ–¹æ³•ï¼Œæ”»å‡»æˆåŠŸç‡ä»47.9%é™ä½åˆ°14.7%ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ•ˆç”¨å’Œæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä¸­å­˜åœ¨çš„å®‰å…¨æ¼æ´é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è¶Šç‹±æ”»å‡»çš„è„†å¼±æ€§ã€‚ç°æœ‰çš„dLLMsç”±äºå…¶ç‹¬ç‰¹çš„è¿­ä»£å’Œå¹¶è¡Œç”Ÿæˆæœºåˆ¶ï¼Œä¸è‡ªå›å½’æ¨¡å‹ç›¸æ¯”ï¼Œé¢ä¸´ç€ä¸åŒçš„å®‰å…¨æŒ‘æˆ˜ã€‚æ ‡å‡†çš„è´ªå©ªé‡æ©ç ç­–ç•¥å­˜åœ¨æœ‰å®³åå·®ï¼Œå¹¶ä¸”æ¨¡å‹å­˜åœ¨â€œå»å™ªè·¯å¾„ä¾èµ–æ€§â€ï¼Œå³æ—©æœŸtokençš„å®‰å…¨æ€§ä¼šæ˜¾è‘—å½±å“æœ€ç»ˆè¾“å‡ºçš„å®‰å…¨æ€§ã€‚è¿™äº›é—®é¢˜ä½¿å¾—dLLMså®¹æ˜“å—åˆ°æ¶æ„æ”»å‡»ï¼Œäº§ç”Ÿæœ‰å®³å†…å®¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDiffuGuardçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨è§£ç è¿‡ç¨‹ä¸­å¼•å…¥éšæœºæ€§å’Œé£é™©æ£€æµ‹æœºåˆ¶ï¼Œæ¥å¢å¼ºdLLMsçš„å®‰å…¨æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡éšæœºé€€ç«é‡æ©ç æ¥å‡è½»è´ªå©ªé€‰æ‹©åå·®ï¼Œå¹¶é€šè¿‡å—çº§å®¡è®¡å’Œä¿®å¤æ¥æ£€æµ‹å’Œçº æ­£æ½œåœ¨çš„é£é™©å†…å®¹ã€‚è¿™ç§åŒé˜¶æ®µçš„æ–¹æ³•æ—¨åœ¨åˆ©ç”¨dLLMså†…åœ¨çš„å®‰å…¨æ½œåŠ›ï¼ŒåŒæ—¶é¿å…å¯¹æ¨¡å‹è¿›è¡Œé‡æ–°è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDiffuGuardæ˜¯ä¸€ä¸ªåŒé˜¶æ®µçš„é˜²å¾¡æ¡†æ¶ï¼ŒåŒ…å«ä»¥ä¸‹ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š
1. **éšæœºé€€ç«é‡æ©ç ï¼ˆStochastic Annealing Remaskingï¼‰**ï¼šè¯¥æ¨¡å—é€šè¿‡åŠ¨æ€å¼•å…¥å—æ§çš„éšæœºæ€§æ¥ç¼“è§£è´ªå©ªé€‰æ‹©åå·®ã€‚å®ƒåœ¨é‡æ©ç è¿‡ç¨‹ä¸­ï¼Œå¹¶éæ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼Œè€Œæ˜¯ä»¥ä¸€å®šçš„æ¦‚ç‡é€‰æ‹©å…¶ä»–tokenï¼Œä»è€Œæ¢ç´¢æ›´å¤šçš„ç”Ÿæˆè·¯å¾„ã€‚
2. **å—çº§å®¡è®¡å’Œä¿®å¤ï¼ˆBlock-level Audit and Repairï¼‰**ï¼šè¯¥æ¨¡å—åˆ©ç”¨æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºæ¥æ£€æµ‹å’Œçº æ­£æ½œåœ¨çš„é£é™©å†…å®¹ã€‚å®ƒå°†ç”Ÿæˆçš„æ–‡æœ¬åˆ†æˆå—ï¼Œå¹¶å¯¹æ¯ä¸ªå—è¿›è¡Œé£é™©è¯„ä¼°ã€‚å¦‚æœæ£€æµ‹åˆ°é£é™©ï¼Œåˆ™ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨ä¿¡æ¯æ¥å¼•å¯¼ç”Ÿæˆæ›´å®‰å…¨çš„å†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šDiffuGuardçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ— éœ€è®­ç»ƒçš„é˜²å¾¡æœºåˆ¶ï¼Œä»¥åŠå…¶åŒé˜¶æ®µçš„è®¾è®¡ã€‚ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹çš„é˜²å¾¡æ–¹æ³•ä¸åŒï¼ŒDiffuGuardå¯ä»¥ç›´æ¥åº”ç”¨äºç°æœ‰çš„dLLMsï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œå…¶åŒé˜¶æ®µçš„è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³dLLMsä¸­å­˜åœ¨çš„ä¸¤ç§ä¸»è¦æ¼æ´ï¼šè´ªå©ªé€‰æ‹©åå·®å’Œå»å™ªè·¯å¾„ä¾èµ–æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼š
* **éšæœºé€€ç«é‡æ©ç **ï¼šå¼•å…¥é€€ç«å‚æ•°æ¥æ§åˆ¶éšæœºæ€§çš„å¼ºåº¦ã€‚é€€ç«å‚æ•°éšç€è§£ç çš„è¿›è¡Œè€Œé€æ¸é™ä½ï¼Œä»è€Œåœ¨æ—©æœŸé˜¶æ®µå…è®¸æ›´å¤šçš„æ¢ç´¢ï¼Œè€Œåœ¨åæœŸé˜¶æ®µåˆ™æ›´åŠ æ³¨é‡ç”Ÿæˆè´¨é‡ã€‚
* **å—çº§å®¡è®¡å’Œä¿®å¤**ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„é£é™©æ£€æµ‹æ¨¡å‹æ¥è¯„ä¼°æ–‡æœ¬å—çš„é£é™©ã€‚å¦‚æœæ£€æµ‹åˆ°é£é™©ï¼Œåˆ™ä½¿ç”¨æ¨¡å‹çš„æ¢¯åº¦ä¿¡æ¯æ¥å¼•å¯¼ç”Ÿæˆæ›´å®‰å…¨çš„å†…å®¹ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä¼šè®¡ç®—é£é™©æ£€æµ‹æ¨¡å‹å¯¹æ¯ä¸ªtokençš„æ¢¯åº¦ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¢¯åº¦æ¥è°ƒæ•´tokençš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œé™ä½ç”Ÿæˆé£é™©å†…å®¹çš„å¯èƒ½æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DiffuGuardåœ¨å››ä¸ªä¸åŒçš„dLLMsä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—é™ä½è¶Šç‹±æ”»å‡»çš„æˆåŠŸç‡ã€‚å…·ä½“æ¥è¯´ï¼Œé’ˆå¯¹å…­ç§ä¸åŒçš„è¶Šç‹±æ–¹æ³•ï¼ŒDiffuGuardå°†æ”»å‡»æˆåŠŸç‡ä»47.9%é™ä½åˆ°14.7%ã€‚åŒæ—¶ï¼ŒDiffuGuardè¿˜èƒ½å¤Ÿä¿æŒæ¨¡å‹çš„æ•ˆç”¨å’Œæ•ˆç‡ï¼Œä¸ä¼šå¯¹æ¨¡å‹çš„ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDiffuGuardæ˜¯ä¸€ç§æœ‰æ•ˆçš„dLLMsé˜²å¾¡æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DiffuGuardå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥ç”¨äºä¿æŠ¤å„ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆåº”ç”¨ï¼Œä¾‹å¦‚èŠå¤©æœºå™¨äººã€å†…å®¹åˆ›ä½œå·¥å…·å’Œä»£ç ç”Ÿæˆå™¨ã€‚é€šè¿‡æé«˜dLLMsçš„å®‰å…¨æ€§ï¼ŒDiffuGuardå¯ä»¥å‡å°‘æ¶æ„æ”»å‡»å’Œæœ‰å®³å†…å®¹çš„äº§ç”Ÿï¼Œä»è€Œä¿ƒè¿›è¿™äº›æŠ€æœ¯çš„å®‰å…¨å’Œå¯é åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¸ºæœªæ¥dLLMsçš„å®‰å…¨è®¾è®¡æä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid advancement of Diffusion Large Language Models (dLLMs) introduces unprecedented vulnerabilities that are fundamentally distinct from Autoregressive LLMs, stemming from their iterative and parallel generation mechanisms. In this paper, we conduct an in-depth analysis of dLLM vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step and inter-step dynamics. Experimental results reveal a harmful bias inherent in the standard greedy remasking strategy and identify a critical phenomenon we term Denoising-path Dependence, where the safety of early-stage tokens decisively influences the final output. These findings also indicate that while current decoding strategies constitute a significant vulnerability, dLLMs possess a substantial intrinsic safety potential. To unlock this potential, we propose DiffuGuard, a training-free defense framework that addresses vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking dynamically introduces controlled randomness to mitigate greedy selection bias, while Block-level Audit and Repair exploits internal model representations for autonomous risk detection and guided correction. Comprehensive experiments on four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while preserving model utility and efficiency. Our code is available at: https://github.com/niez233/DiffuGuard.

