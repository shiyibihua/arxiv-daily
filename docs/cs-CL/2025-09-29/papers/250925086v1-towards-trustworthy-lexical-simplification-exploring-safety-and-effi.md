---
layout: default
title: Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs
---

# Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25086" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25086v1</a>
  <a href="https://arxiv.org/pdf/2509.25086.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25086v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25086v1', 'Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Akio Hayakawa, Stefan Bott, Horacio Saggion

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºå°å‹LLMçš„å®‰å…¨é«˜æ•ˆè¯æ±‡ç®€åŒ–æ¡†æ¶ï¼Œå¹¶æ¢ç´¢å®‰å…¨è¿‡æ»¤ç­–ç•¥ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯æ±‡ç®€åŒ–` `å°å‹LLM` `çŸ¥è¯†è’¸é¦` `å®‰å…¨æ€§` `è¿‡æ»¤ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹LLMåœ¨è¯æ±‡ç®€åŒ–ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†éƒ¨ç½²åœ¨éšç§æ•æ„Ÿå’Œèµ„æºå—é™ç¯å¢ƒå­˜åœ¨æŒ‘æˆ˜ï¼Œä¸”å®‰å…¨æ€§éš¾ä»¥ä¿è¯ã€‚
2. åˆ©ç”¨å°å‹LLMï¼Œç»“åˆçŸ¥è¯†è’¸é¦å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ„å»ºé«˜æ•ˆçš„è¯æ±‡ç®€åŒ–æ¡†æ¶ï¼Œå¹¶æå‡ºåŸºäºè¾“å‡ºæ¦‚ç‡çš„è¿‡æ»¤ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒçŸ¥è¯†è’¸é¦æå‡äº†è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼Œä½†å¼•å…¥äº†å®‰å…¨é£é™©ï¼Œè€Œæå‡ºçš„è¿‡æ»¤ç­–ç•¥èƒ½æœ‰æ•ˆæŠ‘åˆ¶æœ‰å®³ç®€åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯æ±‡ç®€åŒ–ï¼ˆLSï¼‰çš„å®é™…åº”ç”¨ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éšç§æ•æ„Ÿå’Œèµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚æ­¤å¤–ï¼Œç”±äºå¼±åŠ¿ç”¨æˆ·ç¾¤ä½“ï¼ˆä¾‹å¦‚æ®‹ç–¾äººï¼‰æ˜¯è¯¥æŠ€æœ¯çš„ä¸»è¦ç›®æ ‡ç¾¤ä½“ä¹‹ä¸€ï¼Œå› æ­¤ç¡®ä¿LSç³»ç»Ÿè¾“å‡ºçš„å®‰å…¨æ€§å’Œæ­£ç¡®æ€§è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„LSç³»ç»Ÿæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¯åœ¨æœ¬åœ°ç¯å¢ƒä¸­éƒ¨ç½²çš„å°å‹LLMã€‚åœ¨è¯¥æ¡†æ¶å†…ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨åˆæˆæ•°æ®çš„çŸ¥è¯†è’¸é¦å’Œä¸Šä¸‹æ–‡å­¦ä¹ ä½œä¸ºåŸºçº¿ã€‚æˆ‘ä»¬åœ¨äº”ç§è¯­è¨€ä¸­çš„å®éªŒå¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œäº†è‡ªåŠ¨å’Œæ‰‹åŠ¨è¯„ä¼°ã€‚æˆ‘ä»¬çš„äººå·¥åˆ†æè¡¨æ˜ï¼Œè™½ç„¶çŸ¥è¯†è’¸é¦æé«˜äº†è‡ªåŠ¨æŒ‡æ ‡åˆ†æ•°ï¼Œä½†å®ƒä¹Ÿé€šè¿‡å¢åŠ æœ‰å®³ç®€åŒ–å¼•å…¥äº†å®‰å…¨æƒè¡¡ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹çš„è¾“å‡ºæ¦‚ç‡æ˜¯æ£€æµ‹æœ‰å®³ç®€åŒ–çš„æœ‰ç”¨ä¿¡å·ã€‚åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿‡æ»¤ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¿ç•™æœ‰ç›Šç®€åŒ–çš„åŒæ—¶ï¼ŒæŠ‘åˆ¶æœ‰å®³ç®€åŒ–ã€‚è¿™é¡¹å·¥ä½œä¸ºä½¿ç”¨å°å‹LLMè¿›è¡Œé«˜æ•ˆä¸”å®‰å…¨çš„LSå»ºç«‹äº†ä¸€ä¸ªåŸºå‡†ã€‚å®ƒçªå‡ºäº†æ€§èƒ½ã€æ•ˆç‡å’Œå®‰å…¨æ€§ä¹‹é—´çš„å…³é”®æƒè¡¡ï¼Œå¹¶å±•ç¤ºäº†ä¸€ç§ç”¨äºå®‰å…¨å®é™…éƒ¨ç½²çš„æœ‰å¸Œæœ›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¯æ±‡ç®€åŒ–ï¼ˆLSï¼‰ä»»åŠ¡ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èµ„æºå—é™å’Œéšç§æ•æ„Ÿåœºæ™¯ä¸‹éƒ¨ç½²å›°éš¾ï¼Œä»¥åŠè¾“å‡ºç»“æœå®‰å…¨æ€§éš¾ä»¥ä¿è¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§å‹æ¨¡å‹ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”å¯èƒ½ç”Ÿæˆä¸å®‰å…¨æˆ–ä¸åˆé€‚çš„ç®€åŒ–ç»“æœï¼Œå°¤å…¶æ˜¯åœ¨é¢å‘å¼±åŠ¿ç¾¤ä½“æ—¶ï¼Œå®‰å…¨æ€§é—®é¢˜å°¤ä¸ºé‡è¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å°å‹LLMï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ„å»ºä¸€ä¸ªé«˜æ•ˆä¸”å®‰å…¨çš„è¯æ±‡ç®€åŒ–ç³»ç»Ÿã€‚é€šè¿‡åˆ†ææ¨¡å‹è¾“å‡ºæ¦‚ç‡ï¼Œè¯†åˆ«å¹¶è¿‡æ»¤æ‰æ½œåœ¨çš„æœ‰å®³ç®€åŒ–ï¼Œä»è€Œåœ¨æ€§èƒ½å’Œå®‰å…¨æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶æé«˜ç®€åŒ–ç»“æœçš„å¯é æ€§å’Œé€‚ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) å°å‹LLMï¼šä½œä¸ºè¯æ±‡ç®€åŒ–çš„æ ¸å¿ƒæ¨¡å‹ã€‚2) çŸ¥è¯†è’¸é¦æ¨¡å—ï¼šä½¿ç”¨åˆæˆæ•°æ®ï¼Œå°†å¤§å‹æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°å‹æ¨¡å‹ã€‚3) ä¸Šä¸‹æ–‡å­¦ä¹ æ¨¡å—ï¼šåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æå‡ç®€åŒ–æ•ˆæœã€‚4) è¿‡æ»¤æ¨¡å—ï¼šåŸºäºæ¨¡å‹è¾“å‡ºæ¦‚ç‡ï¼Œè¯†åˆ«å¹¶è¿‡æ»¤æœ‰å®³ç®€åŒ–ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œè¾“å…¥å¤æ‚è¯æ±‡å’Œä¸Šä¸‹æ–‡ï¼Œå°å‹LLMç”Ÿæˆå€™é€‰ç®€åŒ–è¯æ±‡ï¼Œç„¶åé€šè¿‡è¿‡æ»¤æ¨¡å—ç­›é€‰ï¼Œæœ€ç»ˆè¾“å‡ºå®‰å…¨ä¸”åˆé€‚çš„ç®€åŒ–ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åŸºäºå°å‹LLMè¾“å‡ºæ¦‚ç‡çš„æœ‰å®³ç®€åŒ–è¿‡æ»¤ç­–ç•¥ã€‚ä¸åŒäºä»¥å¾€ä¾§é‡äºæå‡ç®€åŒ–æ€§èƒ½çš„æ–¹æ³•ï¼Œè¯¥ç ”ç©¶å…³æ³¨ç®€åŒ–ç»“æœçš„å®‰å…¨æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è¿‡æ»¤æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¿ç•™æœ‰ç›Šç®€åŒ–çš„åŒæ—¶ï¼ŒæŠ‘åˆ¶æœ‰å®³ç®€åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åˆæˆæ•°æ®çš„ç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºçŸ¥è¯†è’¸é¦ï¼Œä¿è¯æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚2) æ¨¡å‹è¾“å‡ºæ¦‚ç‡é˜ˆå€¼çš„è®¾å®šï¼Œç”¨äºè¿‡æ»¤æœ‰å®³ç®€åŒ–ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œå¯èƒ½åŒ…å«äº¤å‰ç†µæŸå¤±ã€å¯¹æ¯”æŸå¤±ç­‰ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒçŸ¥è¯†è’¸é¦å¯ä»¥æœ‰æ•ˆæå‡å°å‹LLMåœ¨è¯æ±‡ç®€åŒ–ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿå¢åŠ äº†æœ‰å®³ç®€åŒ–çš„é£é™©ã€‚é€šè¿‡æå‡ºçš„è¿‡æ»¤ç­–ç•¥ï¼Œå¯ä»¥åœ¨å¾ˆå¤§ç¨‹åº¦ä¸ŠæŠ‘åˆ¶æœ‰å®³ç®€åŒ–ï¼ŒåŒæ—¶ä¿ç•™æœ‰ç›Šç®€åŒ–ã€‚äººå·¥è¯„ä¼°ç»“æœéªŒè¯äº†è¯¥è¿‡æ»¤ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®‰å…¨é«˜æ•ˆçš„è¯æ±‡ç®€åŒ–æä¾›äº†ä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¾…åŠ©é˜…è¯»å·¥å…·ã€æ•™è‚²è½¯ä»¶ã€æ— éšœç¢ç½‘ç«™ç­‰é¢†åŸŸï¼Œå¸®åŠ©å¼±åŠ¿ç¾¤ä½“ï¼ˆå¦‚æ®‹ç–¾äººã€è¯­è¨€å­¦ä¹ è€…ï¼‰æ›´å¥½åœ°ç†è§£æ–‡æœ¬å†…å®¹ã€‚é€šè¿‡é™ä½è¯æ±‡éš¾åº¦ï¼Œæé«˜ä¿¡æ¯çš„å¯è®¿é—®æ€§ï¼Œä¿ƒè¿›ç¤¾ä¼šå…¬å¹³å’ŒåŒ…å®¹ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ï¼Œæå‡ç³»ç»Ÿçš„å¯ç”¨æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite their strong performance, large language models (LLMs) face challenges in real-world application of lexical simplification (LS), particularly in privacy-sensitive and resource-constrained environments. Moreover, since vulnerable user groups (e.g., people with disabilities) are one of the key target groups of this technology, it is crucial to ensure the safety and correctness of the output of LS systems. To address these issues, we propose an efficient framework for LS systems that utilizes small LLMs deployable in local environments. Within this framework, we explore knowledge distillation with synthesized data and in-context learning as baselines. Our experiments in five languages evaluate model outputs both automatically and manually. Our manual analysis reveals that while knowledge distillation boosts automatic metric scores, it also introduces a safety trade-off by increasing harmful simplifications. Importantly, we find that the model's output probability is a useful signal for detecting harmful simplifications. Leveraging this, we propose a filtering strategy that suppresses harmful simplifications while largely preserving beneficial ones. This work establishes a benchmark for efficient and safe LS with small LLMs. It highlights the key trade-offs between performance, efficiency, and safety, and demonstrates a promising approach for safe real-world deployment.

