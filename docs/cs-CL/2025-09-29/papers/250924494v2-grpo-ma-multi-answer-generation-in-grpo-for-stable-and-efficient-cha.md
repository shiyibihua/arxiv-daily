---
layout: default
title: GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training
---

# GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24494" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24494v2</a>
  <a href="https://arxiv.org/pdf/2509.24494.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24494v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24494v2', 'GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongcheng Wang, Yinuo Huang, Sukai Wang, Guanghui Ren, Hao Dong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-10-28)

**å¤‡æ³¨**: Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GRPO-MAï¼šé€šè¿‡å¤šç­”æ¡ˆç”Ÿæˆæå‡GRPOåœ¨CoTè®­ç»ƒä¸­çš„ç¨³å®šæ€§å’Œæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Chain-of-Thought` `å¼ºåŒ–å­¦ä¹ ` `å¤šç­”æ¡ˆç”Ÿæˆ` `æ¢¯åº¦ä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. GRPOåœ¨CoTè®­ç»ƒä¸­å­˜åœ¨æ¢¯åº¦è€¦åˆã€å¥–åŠ±ç¨€ç–å’Œä¼˜åŠ¿ä¼°è®¡ä¸ç¨³å®šç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½å’Œæ•ˆç‡ã€‚
2. GRPO-MAé€šè¿‡ä»æ¯ä¸ªæ€è€ƒè¿‡ç¨‹ä¸­ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œæ¥é™ä½æ¢¯åº¦æ–¹å·®ï¼Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGRPO-MAåœ¨æ•°å­¦ã€ä»£ç å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå¹¶éªŒè¯äº†å¤šç­”æ¡ˆç”Ÿæˆç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹GRPOç®—æ³•åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ—¶é¢ä¸´çš„æŒ‘æˆ˜è¿›è¡Œäº†ç ”ç©¶ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬ï¼šæ€æƒ³å’Œç­”æ¡ˆä¹‹é—´çš„æ¢¯åº¦è€¦åˆã€æœ‰é™å¹¶è¡Œé‡‡æ ·å¯¼è‡´çš„ç¨€ç–å¥–åŠ±ä¿¡å·ä»¥åŠä¸ç¨³å®šçš„ä¼˜åŠ¿ä¼°è®¡ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ä½†ç†è®ºä¸Šæœ‰ä¾æ®çš„æ–¹æ³•GRPO-MAï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ¯ä¸ªæ€è€ƒè¿‡ç¨‹ä¸­çš„å¤šç­”æ¡ˆç”Ÿæˆï¼Œä»è€Œå®ç°æ›´ç¨³å¥å’Œé«˜æ•ˆçš„ä¼˜åŒ–ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†æ€æƒ³ä¼˜åŠ¿çš„æ–¹å·®éšç€æ¯ä¸ªæ€æƒ³çš„ç­”æ¡ˆæ•°é‡çš„å¢åŠ è€Œå‡å°‘ã€‚å®éªŒä¸Šï¼Œæˆ‘ä»¬çš„æ¢¯åº¦åˆ†æè¯å®äº†è¿™ä¸€æ•ˆæœï¼Œè¡¨æ˜GRPO-MAé™ä½äº†æ¢¯åº¦å³°å€¼ã€‚åœ¨æ•°å­¦ã€ä»£ç å’Œå„ç§å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGRPO-MAæ˜¾è‘—æé«˜äº†æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼Œå¢åŠ æ¯ä¸ªæ€æƒ³çš„ç­”æ¡ˆæ•°é‡å§‹ç»ˆå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šGRPOç®—æ³•åœ¨è®­ç»ƒLLMs/VLMsè¿›è¡ŒCoTæ¨ç†æ—¶ï¼Œé¢ä¸´ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯æ€æƒ³ï¼ˆthoughtsï¼‰å’Œç­”æ¡ˆä¹‹é—´çš„æ¢¯åº¦è€¦åˆï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼›äºŒæ˜¯ç”±äºå¹¶è¡Œé‡‡æ ·æ•°é‡æœ‰é™ï¼Œå¥–åŠ±ä¿¡å·ç¨€ç–ï¼Œéš¾ä»¥æœ‰æ•ˆæŒ‡å¯¼è®­ç»ƒï¼›ä¸‰æ˜¯ä¸ç¨³å®šçš„ä¼˜åŠ¿ä¼°è®¡ï¼Œå½±å“ç­–ç•¥ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGRPO-MAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¯ä¸ªæ€è€ƒè¿‡ç¨‹ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œä»è€Œå¢åŠ å¥–åŠ±ä¿¡å·çš„å¯†åº¦ï¼Œé™ä½æ¢¯åº¦æ–¹å·®ï¼Œå¹¶ç¨³å®šä¼˜åŠ¿ä¼°è®¡ã€‚é€šè¿‡ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ€è€ƒè¿‡ç¨‹çš„è´¨é‡ï¼Œä»è€Œæ›´å‡†ç¡®åœ°æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRPO-MAæ²¿ç”¨äº†GRPOçš„æ•´ä½“æ¡†æ¶ï¼Œä¸»è¦åŒºåˆ«åœ¨äºç­”æ¡ˆç”Ÿæˆé˜¶æ®µã€‚åœ¨GRPOä¸­ï¼Œæ¯ä¸ªæ€è€ƒè¿‡ç¨‹åªç”Ÿæˆä¸€ä¸ªç­”æ¡ˆï¼Œè€Œåœ¨GRPO-MAä¸­ï¼Œæ¯ä¸ªæ€è€ƒè¿‡ç¨‹ç”Ÿæˆå¤šä¸ªç­”æ¡ˆã€‚ç„¶åï¼Œæ ¹æ®è¿™äº›ç­”æ¡ˆè®¡ç®—å¥–åŠ±ï¼Œå¹¶ç”¨äºæ›´æ–°ç­–ç•¥ç½‘ç»œã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1ï¼‰è¾“å…¥é—®é¢˜ï¼›2ï¼‰æ¨¡å‹ç”Ÿæˆæ€è€ƒè¿‡ç¨‹ï¼›3ï¼‰åŸºäºæ€è€ƒè¿‡ç¨‹ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼›4ï¼‰è®¡ç®—å¥–åŠ±ï¼›5ï¼‰æ›´æ–°ç­–ç•¥ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šGRPO-MAçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šç­”æ¡ˆç”Ÿæˆæœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„GRPOæ–¹æ³•ç›¸æ¯”ï¼ŒGRPO-MAèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„è®­ç»ƒæ•°æ®ï¼Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œå¤šç­”æ¡ˆç”Ÿæˆå¯ä»¥é™ä½æ€æƒ³ä¼˜åŠ¿çš„æ–¹å·®ï¼Œä»è€Œå‡å°‘æ¢¯åº¦æ›´æ–°çš„å™ªå£°ã€‚

**å…³é”®è®¾è®¡**ï¼šGRPO-MAçš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•ç”Ÿæˆå¤šä¸ªç­”æ¡ˆä»¥åŠå¦‚ä½•è®¡ç®—å¥–åŠ±ã€‚è®ºæ–‡ä¸­å¹¶æ²¡æœ‰è¯¦ç»†è¯´æ˜å…·ä½“çš„å¤šç­”æ¡ˆç”Ÿæˆæ–¹æ³•ï¼Œä½†å¯ä»¥é‡‡ç”¨å¤šç§ç­–ç•¥ï¼Œä¾‹å¦‚ï¼šä½¿ç”¨ä¸åŒçš„è§£ç ç­–ç•¥ï¼ˆå¦‚Top-k samplingã€nucleus samplingï¼‰ï¼Œæˆ–è€…å¯¹åŒä¸€ä¸ªæ€è€ƒè¿‡ç¨‹è¿›è¡Œå¤šæ¬¡é‡‡æ ·ã€‚å¥–åŠ±è®¡ç®—å¯ä»¥é‡‡ç”¨å¹³å‡å¥–åŠ±æˆ–è€…åŠ æƒå¹³å‡å¥–åŠ±ï¼Œå…·ä½“æƒé‡å¯ä»¥æ ¹æ®ç­”æ¡ˆçš„è´¨é‡è¿›è¡Œè°ƒæ•´ã€‚æ­¤å¤–ï¼Œç­”æ¡ˆæ•°é‡çš„é€‰æ‹©ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦åœ¨å®éªŒä¸­è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGRPO-MAåœ¨æ•°å­¦ã€ä»£ç å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒGRPO-MAçš„æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡äº†10%ã€‚æ¢¯åº¦åˆ†æè¡¨æ˜ï¼ŒGRPO-MAèƒ½å¤Ÿæœ‰æ•ˆé™ä½æ¢¯åº¦å³°å€¼ï¼Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å¢åŠ æ¯ä¸ªæ€è€ƒè¿‡ç¨‹çš„ç­”æ¡ˆæ•°é‡èƒ½å¤ŸæŒç»­æå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GRPO-MAå¯åº”ç”¨äºå„ç§éœ€è¦CoTæ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šæ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€è§†è§‰é—®ç­”ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ å¯é å’Œé«˜æ•ˆã€‚æœªæ¥ï¼ŒGRPO-MAæœ‰æœ›è¢«åº”ç”¨äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling between thoughts and answers, sparse reward signals caused by limited parallel sampling, and unstable advantage estimation. To mitigate these challenges, we propose GRPO-MA, a simple yet theoretically grounded method that leverages multi-answer generation from each thought process, enabling more robust and efficient optimization. Theoretically, we show that the variance of thought advantage decreases as the number of answers per thought increases. Empirically, our gradient analysis confirms this effect, showing that GRPO-MA reduces gradient spikes compared to GRPO. Experiments on math, code, and diverse multimodal tasks demonstrate that GRPO-MA substantially improves performance and training efficiency. Our ablation studies further reveal that increasing the number of answers per thought consistently enhances model performance.

