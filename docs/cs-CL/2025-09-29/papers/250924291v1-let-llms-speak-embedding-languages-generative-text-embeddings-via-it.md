---
layout: default
title: Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement
---

# Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24291" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24291v1</a>
  <a href="https://arxiv.org/pdf/2509.24291.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24291v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24291v1', 'Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu-Che Tsai, Kuan-Yu Chen, Yuan-Chi Li, Yuan-Hao Chen, Ching-Yu Tsai, Shou-De Lin

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGIRCSEï¼Œåˆ©ç”¨ç”Ÿæˆå¼LLMè¿­ä»£ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ï¼Œæ˜¾è‘—æå‡è¯­ä¹‰è¡¨å¾èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åµŒå…¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç”Ÿæˆå¼æ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `è‡ªå›å½’ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„æ–‡æœ¬åµŒå…¥æ–¹æ³•ä¸»è¦ä¾èµ–encoder-onlyæ¨¡å¼ï¼Œå¿½ç•¥äº†LLMå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œé™åˆ¶äº†è¡¨å¾å­¦ä¹ çš„æ½œåŠ›ã€‚
2. GIRCSEæ¡†æ¶é€šè¿‡è‡ªå›å½’ç”Ÿæˆè½¯tokenåºåˆ—ï¼Œå¹¶åˆ©ç”¨è¿­ä»£å¯¹æ¯”ä¼˜åŒ–ç›®æ ‡ï¼Œé€æ­¥æç‚¼è¯­ä¹‰è¡¨ç¤ºï¼Œæ•æ‰æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGIRCSEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„LLMåµŒå…¥æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºæµ‹è¯•æ—¶æ€§èƒ½éšç”Ÿæˆtokenæ•°é‡å¢åŠ è€Œæå‡çš„ç‰¹æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGIRCSEï¼ˆç”¨äºå¯¹æ¯”å¥å­åµŒå…¥çš„ç”Ÿæˆå¼è¿­ä»£ä¼˜åŒ–ï¼‰çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è‡ªå›å½’ç”Ÿæˆæ¥è¿­ä»£åœ°æ”¹è¿›è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œå‘æŒ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ç°æœ‰åŸºäºLLMçš„åµŒå…¥æ–¹æ³•é€šå¸¸é‡‡ç”¨çš„encoder-onlyèŒƒå¼ä¸åŒï¼ŒGIRCSEé€šè¿‡ç”Ÿæˆåœ¨å¯¹æ¯”ç›®æ ‡ä¸‹ä¼˜åŒ–çš„è½¯tokenåºåˆ—ï¼Œæ•æ‰äº†encoder-onlyæ–¹æ³•ç»å¸¸é—æ¼çš„æ½œåœ¨æ¦‚å¿µå’Œéšå¼è¯­ä¹‰ã€‚ä¸ºäº†æŒ‡å¯¼è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿­ä»£å¯¹æ¯”ä¼˜åŒ–ï¼ˆICRï¼‰ç›®æ ‡ï¼Œè¯¥ç›®æ ‡é¼“åŠ±æ¯ä¸ªä¼˜åŒ–æ­¥éª¤äº§ç”Ÿæ›´å¥½çš„è¡¨ç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGIRCSEåœ¨MTEBåŸºå‡†å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸Šä¼˜äºå¼ºå¤§çš„åŸºäºLLMçš„åµŒå…¥åŸºçº¿ã€‚æ­¤å¤–ï¼ŒGIRCSEè¡¨ç°å‡ºä¸€ç§æ¶Œç°çš„æµ‹è¯•æ—¶ç¼©æ”¾ç‰¹æ€§ï¼šåœ¨æ¨ç†æ—¶ç”Ÿæˆæ›´å¤šçš„tokenå¯ä»¥ç¨³å®šåœ°æé«˜åµŒå…¥è´¨é‡ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†ç”Ÿæˆå¼è¿­ä»£ä¼˜åŒ–ä½œä¸ºè¡¨å¾å­¦ä¹ çš„ä¸€ç§æ–°èŒƒå¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºLLMçš„æ–‡æœ¬åµŒå…¥æ–¹æ³•é€šå¸¸å°†LLMè§†ä¸ºé™æ€çš„ç‰¹å¾æå–å™¨ï¼Œé‡‡ç”¨encoder-onlyçš„æ¨¡å¼ã€‚è¿™ç§æ–¹æ³•å¿½ç•¥äº†LLMå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨LLMæ‰€è•´å«çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥å¯èƒ½æ— æ³•æ•æ‰åˆ°æ·±å±‚çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä¾‹å¦‚æ½œåœ¨æ¦‚å¿µå’Œéšå¼è¯­ä¹‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGIRCSEçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMçš„è‡ªå›å½’ç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡è¿­ä»£åœ°ç”Ÿæˆå’Œä¼˜åŒ–è½¯tokenåºåˆ—æ¥æ”¹è¿›æ–‡æœ¬çš„è¯­ä¹‰è¡¨ç¤ºã€‚é€šè¿‡åœ¨å¯¹æ¯”å­¦ä¹ çš„ç›®æ ‡ä¸‹ä¼˜åŒ–ç”Ÿæˆçš„tokenï¼ŒGIRCSEèƒ½å¤Ÿé€æ­¥æç‚¼æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œè·å¾—æ›´å…·è¡¨è¾¾åŠ›çš„æ–‡æœ¬åµŒå…¥ã€‚è¿™ç§æ–¹æ³•å°†LLMä»ä¸€ä¸ªé™æ€çš„ç‰¹å¾æå–å™¨è½¬å˜ä¸ºä¸€ä¸ªåŠ¨æ€çš„è¯­ä¹‰è¡¨ç¤ºç”Ÿæˆå™¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGIRCSEæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **åˆå§‹åŒ–**ï¼šä½¿ç”¨LLMå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°åˆå§‹çš„æ–‡æœ¬åµŒå…¥ã€‚2) **è¿­ä»£ç”Ÿæˆ**ï¼šåŸºäºåˆå§‹åµŒå…¥ï¼Œåˆ©ç”¨LLMçš„è‡ªå›å½’ç”Ÿæˆèƒ½åŠ›ç”Ÿæˆä¸€ç³»åˆ—è½¯tokenã€‚3) **å¯¹æ¯”ä¼˜åŒ–**ï¼šä½¿ç”¨è¿­ä»£å¯¹æ¯”ä¼˜åŒ–ï¼ˆICRï¼‰ç›®æ ‡å‡½æ•°ï¼Œé¼“åŠ±æ¯ä¸ªç”Ÿæˆæ­¥éª¤äº§ç”Ÿæ›´å¥½çš„æ–‡æœ¬è¡¨ç¤ºã€‚4) **åµŒå…¥æå–**ï¼šå°†æœ€ç»ˆç”Ÿæˆçš„è½¯tokenåºåˆ—ä½œä¸ºæ–‡æœ¬çš„åµŒå…¥è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šGIRCSEæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†LLMçš„ç”Ÿæˆèƒ½åŠ›å¼•å…¥åˆ°æ–‡æœ¬åµŒå…¥çš„å­¦ä¹ è¿‡ç¨‹ä¸­ã€‚ä¸ä¼ ç»Ÿçš„encoder-onlyæ–¹æ³•ä¸åŒï¼ŒGIRCSEé€šè¿‡è¿­ä»£ç”Ÿæˆå’Œä¼˜åŒ–è½¯tokenåºåˆ—ï¼Œèƒ½å¤Ÿæ•æ‰åˆ°æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆæ›´å…·è¡¨è¾¾åŠ›çš„æ–‡æœ¬åµŒå…¥ã€‚æ­¤å¤–ï¼ŒICRç›®æ ‡çš„å¼•å…¥ä¹Ÿä¿è¯äº†æ¯æ¬¡è¿­ä»£éƒ½èƒ½æå‡åµŒå…¥çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šGIRCSEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **è½¯tokenç”Ÿæˆ**ï¼šä½¿ç”¨LLMçš„è‡ªå›å½’ç”Ÿæˆèƒ½åŠ›ç”Ÿæˆè½¯tokenï¼Œè€Œä¸æ˜¯ç›´æ¥ç”Ÿæˆç¦»æ•£çš„tokenã€‚2) **è¿­ä»£å¯¹æ¯”ä¼˜åŒ–ï¼ˆICRï¼‰**ï¼šICRç›®æ ‡å‡½æ•°é¼“åŠ±æ¯ä¸ªç”Ÿæˆæ­¥éª¤äº§ç”Ÿçš„åµŒå…¥éƒ½æ¯”å‰ä¸€æ­¥çš„åµŒå…¥æ›´å¥½ï¼Œä»è€Œä¿è¯äº†åµŒå…¥è´¨é‡çš„é€æ­¥æå‡ã€‚ICRæŸå¤±å‡½æ•°é€šå¸¸åŸºäºå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œä¾‹å¦‚InfoNCEã€‚3) **æµ‹è¯•æ—¶ç¼©æ”¾**ï¼šåœ¨æ¨ç†é˜¶æ®µï¼Œå¯ä»¥é€šè¿‡å¢åŠ ç”Ÿæˆçš„tokenæ•°é‡æ¥è¿›ä¸€æ­¥æå‡åµŒå…¥çš„è´¨é‡ï¼Œå±•ç°å‡ºä¸€ç§æ¶Œç°çš„æµ‹è¯•æ—¶ç¼©æ”¾ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGIRCSEåœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºLLMçš„åµŒå…¥æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒGIRCSEçš„æ€§èƒ½æå‡è¶…è¿‡5%ã€‚æ­¤å¤–ï¼ŒGIRCSEè¿˜å±•ç°å‡ºä¸€ç§æ¶Œç°çš„æµ‹è¯•æ—¶ç¼©æ”¾ç‰¹æ€§ï¼Œå³åœ¨æ¨ç†æ—¶ç”Ÿæˆæ›´å¤šçš„tokenå¯ä»¥ç¨³å®šåœ°æé«˜åµŒå…¥è´¨é‡ã€‚è¿™è¡¨æ˜GIRCSEèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›æ¥å­¦ä¹ æ›´å…·è¡¨è¾¾åŠ›çš„æ–‡æœ¬åµŒå…¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GIRCSEç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥å¯ä»¥å¹¿æ³›åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬æ£€ç´¢ã€æ–‡æœ¬åˆ†ç±»ã€è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ã€èšç±»ç­‰ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºéœ€è¦æ•æ‰æ·±å±‚è¯­ä¹‰ä¿¡æ¯çš„åœºæ™¯ï¼Œä¾‹å¦‚çŸ¥è¯†å›¾è°±è¡¥å…¨ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚æœªæ¥ï¼ŒGIRCSEå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤šæ¨¡æ€é¢†åŸŸï¼Œä¾‹å¦‚å›¾åƒæ–‡æœ¬æ£€ç´¢ã€è§†é¢‘è¯­ä¹‰ç†è§£ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing large language model (LLM)-based embeddings typically adopt an encoder-only paradigm, treating LLMs as static feature extractors and overlooking their core generative strengths. We introduce GIRCSE (Generative Iterative Refinement for Contrastive Sentence Embeddings), a novel framework that leverages autoregressive generation to iteratively refine semantic representations. By producing sequences of soft tokens optimized under contrastive objective, GIRCSE captures latent concepts and implicit semantics that encoder-only methods often miss. To guide this process, we propose an Iterative Contrastive Refinement (ICR) objective that encourages each refinement step to yield better representations. Extensive experiments show that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an emergent test-time scaling property: generating more tokens at inference steadily improves embedding quality. Our results establish generative iterative refinement as a new paradigm for representation learning.

