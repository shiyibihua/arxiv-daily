---
layout: default
title: How Well Do LLMs Imitate Human Writing Style?
---

# How Well Do LLMs Imitate Human Writing Style?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24930" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24930v1</a>
  <a href="https://arxiv.org/pdf/2509.24930.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24930v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24930v1', 'How Well Do LLMs Imitate Human Writing Style?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rebira Jemama, Rajesh Kumar

**åˆ†ç±»**: cs.CL, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: IEEE UEMCON 2025, 11 pages, 4 figures, and 4 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¿«é€Ÿå…è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡ä»¿äººç±»å†™ä½œé£æ ¼çš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é£æ ¼æ¨¡ä»¿` `ä½œè€…èº«ä»½éªŒè¯` `å…è®­ç»ƒå­¦ä¹ ` `æ–‡æœ¬é£æ ¼åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥å¿«é€Ÿå‡†ç¡®åœ°è¯„ä¼°LLMæ¨¡ä»¿äººç±»å†™ä½œé£æ ¼çš„èƒ½åŠ›ï¼Œéœ€è¦è€—æ—¶çš„è®­ç»ƒå’Œè°ƒå‚ã€‚
2. æå‡ºä¸€ç§å…è®­ç»ƒæ¡†æ¶ï¼Œç»“åˆTF-IDFå’ŒTransformeråµŒå…¥ï¼Œé€šè¿‡ç»éªŒè·ç¦»åˆ†å¸ƒè¿›è¡Œé£æ ¼åŒ¹é…ï¼Œæ— éœ€ç›‘ç£å­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå°‘æ ·æœ¬å’Œè¡¥å…¨æç¤ºç­–ç•¥èƒ½æ˜¾è‘—æé«˜LLMçš„é£æ ¼æ¨¡ä»¿èƒ½åŠ›ï¼Œä½†é«˜ä¿çœŸä¸ä»£è¡¨äººç±»çš„ä¸å¯é¢„æµ‹æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿç”Ÿæˆæµç•…çš„æ–‡æœ¬ï¼Œä½†å®ƒä»¬å¤åˆ¶ç‰¹å®šäººç±»ä½œè€…ç‹¬ç‰¹é£æ ¼çš„èƒ½åŠ›ä»ä¸æ¸…æ¥šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿã€å…è®­ç»ƒçš„æ¡†æ¶ï¼Œç”¨äºä½œè€…èº«ä»½éªŒè¯å’Œé£æ ¼æ¨¡ä»¿åˆ†æã€‚è¯¥æ–¹æ³•å°†TF-IDFå­—ç¬¦n-gramä¸TransformeråµŒå…¥ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡ç»éªŒè·ç¦»åˆ†å¸ƒå¯¹æ–‡æœ¬å¯¹è¿›è¡Œåˆ†ç±»ï¼Œæ— éœ€ç›‘ç£è®­ç»ƒæˆ–é˜ˆå€¼è°ƒæ•´ã€‚åœ¨å­¦æœ¯è®ºæ–‡ä¸Šå®ç°äº†97.5%çš„å‡†ç¡®ç‡ï¼Œåœ¨è·¨é¢†åŸŸè¯„ä¼°ä¸­è¾¾åˆ°äº†94.5%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ç›¸å¯¹äºåŸºäºå‚æ•°çš„åŸºçº¿ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘äº†91.8%ï¼Œå†…å­˜ä½¿ç”¨é‡å‡å°‘äº†59%ã€‚ä½¿ç”¨è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ¥è‡ªä¸‰ä¸ªä¸åŒç³»åˆ—ï¼ˆLlamaã€Qwenã€Mixtralï¼‰çš„äº”ä¸ªLLMï¼Œé‡‡ç”¨äº†å››ç§æç¤ºç­–ç•¥â€”â€”é›¶æ ·æœ¬ã€å•æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ–‡æœ¬è¡¥å…¨ã€‚ç»“æœè¡¨æ˜ï¼Œæç¤ºç­–ç•¥å¯¹é£æ ¼ä¿çœŸåº¦çš„å½±å“æ¯”æ¨¡å‹å¤§å°æ›´å¤§ï¼šå°‘æ ·æœ¬æç¤ºäº§ç”Ÿçš„é£æ ¼åŒ¹é…å‡†ç¡®ç‡æ¯”é›¶æ ·æœ¬é«˜å‡º23.5å€ï¼Œè¡¥å…¨æç¤ºä¸åŸå§‹ä½œè€…çš„é£æ ¼ä¸€è‡´æ€§è¾¾åˆ°99.9%ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œé«˜ä¿çœŸæ¨¡ä»¿å¹¶ä¸æ„å‘³ç€åƒäººç±»ä¸€æ ·ä¸å¯é¢„æµ‹â€”â€”äººç±»æ–‡ç« çš„å¹³å‡å›°æƒ‘åº¦ä¸º29.5ï¼Œè€ŒåŒ¹é…çš„LLMè¾“å‡ºçš„å¹³å‡å›°æƒ‘åº¦ä»…ä¸º15.2ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œé£æ ¼ä¿çœŸåº¦å’Œç»Ÿè®¡å¯æ£€æµ‹æ€§æ˜¯å¯åˆ†ç¦»çš„ï¼Œä¸ºä½œè€…èº«ä»½å»ºæ¨¡ã€æ£€æµ‹å’Œèº«ä»½æ¡ä»¶ç”Ÿæˆæ–¹é¢çš„æœªæ¥å·¥ä½œå¥ å®šäº†å¯é‡å¤çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡ä»¿ç‰¹å®šäººç±»ä½œè€…å†™ä½œé£æ ¼çš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå¹¶ä¸”éœ€è¦é’ˆå¯¹ä¸åŒçš„ä½œè€…æˆ–é¢†åŸŸè¿›è¡Œè°ƒæ•´ï¼Œç¼ºä¹é€šç”¨æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åŒºåˆ†é£æ ¼ä¸Šçš„ç›¸ä¼¼æ€§å’Œç»Ÿè®¡ä¸Šçš„å¯åŒºåˆ†æ€§ï¼Œå³LLMå¯èƒ½åœ¨é£æ ¼ä¸Šæ¨¡ä»¿å¾—å¾ˆå¥½ï¼Œä½†åœ¨ç»Ÿè®¡ä¸Šä»ç„¶å®¹æ˜“è¢«æ£€æµ‹å‡ºæ¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æå‡ºä¸€ç§å…è®­ç»ƒçš„é£æ ¼æ¨¡ä»¿è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»“åˆTF-IDFå­—ç¬¦n-gramå’ŒTransformeråµŒå…¥ï¼Œæ•æ‰æ–‡æœ¬çš„é£æ ¼ç‰¹å¾ï¼Œå¹¶é€šè¿‡ç»éªŒè·ç¦»åˆ†å¸ƒæ¥è¡¡é‡æ–‡æœ¬ä¹‹é—´çš„é£æ ¼ç›¸ä¼¼åº¦ã€‚è¿™ç§æ–¹æ³•æ— éœ€ç›‘ç£è®­ç»ƒï¼Œé¿å…äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¿«é€Ÿè¯„ä¼°LLMçš„é£æ ¼æ¨¡ä»¿èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜å…³æ³¨äº†LLMç”Ÿæˆæ–‡æœ¬çš„ç»Ÿè®¡å¯åŒºåˆ†æ€§ï¼Œé€šè¿‡å›°æƒ‘åº¦ç­‰æŒ‡æ ‡æ¥è¡¡é‡LLMç”Ÿæˆæ–‡æœ¬ä¸äººç±»æ–‡æœ¬çš„å·®å¼‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ–‡æœ¬é¢„å¤„ç†ï¼šå¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œæ¸…æ´—å’Œæ ‡å‡†åŒ–å¤„ç†ã€‚2) ç‰¹å¾æå–ï¼šä½¿ç”¨TF-IDFå­—ç¬¦n-gramæå–æ–‡æœ¬çš„è¯æ±‡ç‰¹å¾ï¼Œä½¿ç”¨TransformeråµŒå…¥æå–æ–‡æœ¬çš„è¯­ä¹‰ç‰¹å¾ã€‚3) é£æ ¼ç›¸ä¼¼åº¦è®¡ç®—ï¼šé€šè¿‡è®¡ç®—æ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„è·ç¦»ï¼Œè¡¡é‡æ–‡æœ¬ä¹‹é—´çš„é£æ ¼ç›¸ä¼¼åº¦ã€‚4) ç»éªŒè·ç¦»åˆ†å¸ƒï¼šåˆ©ç”¨ç»éªŒè·ç¦»åˆ†å¸ƒå¯¹æ–‡æœ¬å¯¹è¿›è¡Œåˆ†ç±»ï¼Œåˆ¤æ–­LLMæ˜¯å¦æˆåŠŸæ¨¡ä»¿äº†äººç±»ä½œè€…çš„é£æ ¼ã€‚5) ç»Ÿè®¡å¯åŒºåˆ†æ€§è¯„ä¼°ï¼šä½¿ç”¨å›°æƒ‘åº¦ç­‰æŒ‡æ ‡è¯„ä¼°LLMç”Ÿæˆæ–‡æœ¬ä¸äººç±»æ–‡æœ¬çš„ç»Ÿè®¡å·®å¼‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§å…è®­ç»ƒçš„é£æ ¼æ¨¡ä»¿è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— éœ€ç›‘ç£è®­ç»ƒï¼Œèƒ½å¤Ÿå¿«é€Ÿå‡†ç¡®åœ°è¯„ä¼°LLMçš„é£æ ¼æ¨¡ä»¿èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å…³æ³¨äº†LLMç”Ÿæˆæ–‡æœ¬çš„ç»Ÿè®¡å¯åŒºåˆ†æ€§ï¼Œæå‡ºäº†è¯„ä¼°é£æ ¼ä¿çœŸåº¦å’Œç»Ÿè®¡å¯æ£€æµ‹æ€§çš„æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨TF-IDFå­—ç¬¦n-gramå’ŒTransformeråµŒå…¥ç›¸ç»“åˆçš„æ–¹å¼æå–æ–‡æœ¬ç‰¹å¾ï¼Œèƒ½å¤Ÿæ•æ‰æ–‡æœ¬çš„è¯æ±‡å’Œè¯­ä¹‰ä¿¡æ¯ã€‚2) ä½¿ç”¨ç»éªŒè·ç¦»åˆ†å¸ƒå¯¹æ–‡æœ¬å¯¹è¿›è¡Œåˆ†ç±»ï¼Œé¿å…äº†é˜ˆå€¼è°ƒæ•´çš„å›°éš¾ã€‚3) ä½¿ç”¨å›°æƒ‘åº¦ç­‰æŒ‡æ ‡è¯„ä¼°LLMç”Ÿæˆæ–‡æœ¬ä¸äººç±»æ–‡æœ¬çš„ç»Ÿè®¡å·®å¼‚ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°LLMçš„é£æ ¼æ¨¡ä»¿èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å­¦æœ¯è®ºæ–‡ä¸Šå®ç°äº†97.5%çš„å‡†ç¡®ç‡ï¼Œåœ¨è·¨é¢†åŸŸè¯„ä¼°ä¸­è¾¾åˆ°äº†94.5%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ç›¸å¯¹äºåŸºäºå‚æ•°çš„åŸºçº¿ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘äº†91.8%ï¼Œå†…å­˜ä½¿ç”¨é‡å‡å°‘äº†59%ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œå°‘æ ·æœ¬æç¤ºäº§ç”Ÿçš„é£æ ¼åŒ¹é…å‡†ç¡®ç‡æ¯”é›¶æ ·æœ¬é«˜å‡º23.5å€ï¼Œè¡¥å…¨æç¤ºä¸åŸå§‹ä½œè€…çš„é£æ ¼ä¸€è‡´æ€§è¾¾åˆ°99.9%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä½œè€…èº«ä»½éªŒè¯ã€æ–‡æœ¬é£æ ¼åˆ†æã€èº«ä»½æ¡ä»¶æ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºæ£€æµ‹AIç”Ÿæˆçš„æ–‡ç« æ˜¯å¦æ¨¡ä»¿äº†ç‰¹å®šä½œè€…çš„é£æ ¼ï¼Œæˆ–è€…ç”¨äºç”Ÿæˆå…·æœ‰ç‰¹å®šä½œè€…é£æ ¼çš„æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£LLMçš„é£æ ¼æ¨¡ä»¿èƒ½åŠ›ï¼Œå¹¶ä¸ºå¼€å‘æ›´å…·ä¸ªæ€§åŒ–çš„AIç³»ç»Ÿæä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) can generate fluent text, but their ability to replicate the distinctive style of a specific human author remains unclear. We present a fast, training-free framework for authorship verification and style imitation analysis. The method integrates TF-IDF character n-grams with transformer embeddings and classifies text pairs through empirical distance distributions, eliminating the need for supervised training or threshold tuning. It achieves 97.5\% accuracy on academic essays and 94.5\% in cross-domain evaluation, while reducing training time by 91.8\% and memory usage by 59\% relative to parameter-based baselines. Using this framework, we evaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across four prompting strategies - zero-shot, one-shot, few-shot, and text completion. Results show that the prompting strategy has a more substantial influence on style fidelity than model size: few-shot prompting yields up to 23.5x higher style-matching accuracy than zero-shot, and completion prompting reaches 99.9\% agreement with the original author's style. Crucially, high-fidelity imitation does not imply human-like unpredictability - human essays average a perplexity of 29.5, whereas matched LLM outputs average only 15.2. These findings demonstrate that stylistic fidelity and statistical detectability are separable, establishing a reproducible basis for future work in authorship modeling, detection, and identity-conditioned generation.

