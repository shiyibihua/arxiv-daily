---
layout: default
title: Reinforcement Mid-Training
---

# Reinforcement Mid-Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24375" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24375v1</a>
  <a href="https://arxiv.org/pdf/2509.24375.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24375v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24375v1', 'Reinforcement Mid-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yijun Tian, Shaoyu Chen, Zhichao Xu, Yawei Wang, Jinhe Bi, Peng Han, Wei Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼ºåŒ–ä¸­è®­ç»ƒï¼ˆRMTï¼‰æ¡†æ¶ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½å¹¶åŠ é€Ÿè®­ç»ƒã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `ä¸­é—´è®­ç»ƒ` `åŠ¨æ€Tokené¢„ç®—` `è‡ªé€‚åº”é‡‡æ ·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæµç¨‹ç¼ºä¹ä¸­é—´å¼ºåŒ–è®­ç»ƒé˜¶æ®µï¼Œå¯¼è‡´æ¨¡å‹æ¨ç†æ•ˆç‡ä½ã€tokenåˆ©ç”¨ä¸å……åˆ†ã€‚
2. è®ºæ–‡æå‡ºå¼ºåŒ–ä¸­è®­ç»ƒæ¡†æ¶RMTï¼Œé€šè¿‡åŠ¨æ€tokené¢„ç®—ã€è‡ªé€‚åº”é‡‡æ ·å’ŒåŒé‡è®­ç»ƒç­–ç•¥æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRMTåœ¨è¯­è¨€å»ºæ¨¡å’Œæ•°å­¦é¢†åŸŸå‡å–å¾—æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å¼ºåŒ–ä¸­è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æŒ‡å‡ºï¼Œåœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒä¹‹é—´å­˜åœ¨ä¸€ä¸ªå…·æœ‰å·¨å¤§æ€§èƒ½æå‡æ½œåŠ›çš„ä¸­é—´é˜¶æ®µï¼Œå³å¼ºåŒ–ä¸­è®­ç»ƒã€‚è®ºæ–‡æ­£å¼å®šä¹‰äº†è¯¥é—®é¢˜ï¼Œå¹¶è¯†åˆ«äº†ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šè¿‡åº¦æ¨ç†æ­¥éª¤å¯¼è‡´çš„è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€å¿½ç•¥ä¸å¹³è¡¡çš„tokenç†µåˆ†å¸ƒä»¥åŠtokenä¿¡æ¯çš„æœªå……åˆ†åˆ©ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†RMTï¼Œä¸€ä¸ªé«˜æ•ˆã€è‡ªé€‚åº”å’Œç»Ÿä¸€çš„å¼ºåŒ–ä¸­è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…å«å¤šç§åˆ›æ–°ç»„ä»¶ã€‚å…·ä½“è€Œè¨€ï¼Œé¦–å…ˆå¼•å…¥åŠ¨æ€tokené¢„ç®—æœºåˆ¶ï¼Œçº¦æŸä¸å¿…è¦çš„æ¨ç†æ­¥éª¤å¹¶ç¼“è§£æ¨¡å‹è¿‡åº¦æ€è€ƒã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºè¯¾ç¨‹çš„è‡ªé€‚åº”é‡‡æ ·æ–¹æ³•ï¼Œä¿ƒè¿›ä»æ˜“åˆ°éš¾çš„tokenæ¸è¿›å­¦ä¹ è½¨è¿¹ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œä¸‹ä¸€tokené¢„æµ‹çš„åŒé‡è®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿å¯¹å…³é”®tokençš„é’ˆå¯¹æ€§å­¦ä¹ å¹¶å……åˆ†åˆ©ç”¨æ‰€æœ‰tokenä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRMTä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨è¯­è¨€å»ºæ¨¡ä¸­å®ç°äº†é«˜è¾¾+64.91%çš„æ€§èƒ½æå‡ï¼Œè€Œæ¨ç†é•¿åº¦ä»…ä¸º21%ã€‚åŒæ—¶è¯æ˜ï¼Œå¼ºåŒ–ä¸­è®­ç»ƒåè·å¾—çš„checkpointå¯ä»¥ä¿ƒè¿›åç»­çš„åè®­ç»ƒï¼Œåœ¨æ•°å­¦é¢†åŸŸäº§ç”Ÿé«˜è¾¾+18.76%çš„æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒé€šå¸¸åˆ†ä¸ºé¢„è®­ç»ƒå’Œåè®­ç»ƒä¸¤ä¸ªé˜¶æ®µï¼Œå¿½ç•¥äº†ä¸­é—´é˜¶æ®µçš„æ½œåŠ›ã€‚ç›´æ¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ—¶ï¼Œæ¨¡å‹å¯èƒ½è¿›è¡Œè¿‡å¤šçš„ä¸å¿…è¦çš„æ¨ç†æ­¥éª¤ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¯¹ä¸åŒtokençš„å…³æ³¨åº¦ä¸åŒï¼Œtokenç†µåˆ†å¸ƒä¸å¹³è¡¡ï¼Œä¸”ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ‰€æœ‰tokençš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒä¹‹é—´å¼•å…¥ä¸€ä¸ªå¼ºåŒ–ä¸­è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚RMTæ¡†æ¶æ—¨åœ¨è§£å†³è¿‡åº¦æ¨ç†ã€tokenç†µä¸å¹³è¡¡å’Œtokenä¿¡æ¯æœªå……åˆ†åˆ©ç”¨çš„é—®é¢˜ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆã€è‡ªé€‚åº”å’Œç»Ÿä¸€çš„å¼ºåŒ–ä¸­è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRMTæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šåŠ¨æ€tokené¢„ç®—æœºåˆ¶ã€åŸºäºè¯¾ç¨‹çš„è‡ªé€‚åº”é‡‡æ ·æ–¹æ³•å’ŒåŒé‡è®­ç»ƒç­–ç•¥ã€‚åŠ¨æ€tokené¢„ç®—æœºåˆ¶é™åˆ¶ä¸å¿…è¦çš„æ¨ç†æ­¥éª¤ï¼Œé¿å…æ¨¡å‹è¿‡åº¦æ€è€ƒã€‚è‡ªé€‚åº”é‡‡æ ·æ–¹æ³•æ ¹æ®tokençš„éš¾æ˜“ç¨‹åº¦è¿›è¡Œé‡‡æ ·ï¼Œä¿ƒè¿›æ¨¡å‹ä»æ˜“åˆ°éš¾åœ°å­¦ä¹ ã€‚åŒé‡è®­ç»ƒç­–ç•¥ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œä¸‹ä¸€tokené¢„æµ‹ï¼Œç¡®ä¿æ¨¡å‹å¯¹å…³é”®tokenè¿›è¡Œé’ˆå¯¹æ€§å­¦ä¹ ï¼Œå¹¶å……åˆ†åˆ©ç”¨æ‰€æœ‰tokençš„ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šRMTçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å¼ºåŒ–ä¸­è®­ç»ƒçš„æ¦‚å¿µï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„æ¡†æ¶æ¥è§£å†³è¯¥é˜¶æ®µé¢ä¸´çš„æŒ‘æˆ˜ã€‚åŠ¨æ€tokené¢„ç®—æœºåˆ¶ã€è‡ªé€‚åº”é‡‡æ ·æ–¹æ³•å’ŒåŒé‡è®­ç»ƒç­–ç•¥éƒ½æ˜¯é’ˆå¯¹ç‰¹å®šé—®é¢˜æå‡ºçš„åˆ›æ–°è§£å†³æ–¹æ¡ˆï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRMTèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€tokené¢„ç®—æœºåˆ¶é€šè¿‡è®¾å®šä¸€ä¸ªåŠ¨æ€å˜åŒ–çš„tokenæ•°é‡ä¸Šé™æ¥çº¦æŸæ¨¡å‹çš„æ¨ç†é•¿åº¦ã€‚è‡ªé€‚åº”é‡‡æ ·æ–¹æ³•ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ çš„æ€æƒ³ï¼Œæ ¹æ®tokençš„ç†µå€¼åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¦‚ç‡ã€‚åŒé‡è®­ç»ƒç­–ç•¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¥–åŠ±æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„tokenï¼ŒåŒæ—¶ä½¿ç”¨ä¸‹ä¸€tokené¢„æµ‹æŸå¤±æ¥ä¿è¯æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ‰€æœ‰tokençš„ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°æ˜¯å¼ºåŒ–å­¦ä¹ å¥–åŠ±å’Œä¸‹ä¸€tokené¢„æµ‹æŸå¤±çš„åŠ æƒå’Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRMTåœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­å®ç°äº†é«˜è¾¾+64.91%çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ¨ç†é•¿åº¦ä»…ä¸ºç°æœ‰æ–¹æ³•çš„21%ã€‚æ­¤å¤–ï¼Œä½¿ç”¨RMTè®­ç»ƒå¾—åˆ°çš„checkpointå¯ä»¥æ˜¾è‘—æå‡åç»­åè®­ç»ƒçš„æ•ˆæœï¼Œåœ¨æ•°å­¦é¢†åŸŸå–å¾—äº†+18.76%çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœå……åˆ†éªŒè¯äº†RMTçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†å’Œç²¾ç¡®æ§åˆ¶çš„åœºæ™¯ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ç­‰ã€‚é€šè¿‡å¼ºåŒ–ä¸­è®­ç»ƒï¼Œå¯ä»¥æå‡æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼ŒåŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The development of state-of-the-art large language models is commonly understood as a two-stage process involving pre-training and post-training. We point out the need for an additional intermediate stage called reinforcement mid-training with potential for strong performance gains. In this paper, we formally define the problem and identify three key challenges: (1) inefficient training due to excessive reasoning steps, (2) disregard of the imbalanced token entropy distribution, and (3) underutilization of token information. To address these challenges, we propose RMT, a framework for efficient, adaptive, and unified reinforcement mid-training with various innovative components. In particular, we first introduce a dynamic token budget mechanism that constrains unnecessary reasoning steps and mitigates model overthinking. Next, we design a curriculum-based adaptive sampling method that fosters a progressive learning trajectory from easy to hard tokens. Finally, we present a dual training strategy that combines reinforcement learning with next-token prediction, ensuring targeted learning on key tokens and full exploitation of all token information. Extensive experiments demonstrate the superiority of RMT over state-of-the-art methods, achieving up to +64.91% performance improvement with only 21% of the reasoning length in language modeling. We also show that checkpoints obtained after reinforcement mid-training can benefit the subsequent post-training, yielding up to +18.76% improvement in the mathematical domain.

