---
layout: default
title: Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models
---

# Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24488" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24488v1</a>
  <a href="https://arxiv.org/pdf/2509.24488.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24488v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24488v1', 'Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenjie Fu, Huandong Wang, Junyao Gao, Guoan Wan, Tao Jiang

**åˆ†ç±»**: cs.CL, cs.CR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/wjfu99/LLM_Self_Sanitize)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSelf-Sanitizeæ¡†æ¶ï¼Œç¼“è§£å¤§è¯­è¨€æ¨¡å‹ä¸­çš„éšç§æ³„éœ²é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `éšç§æ³„éœ²` `å†…å®¹å®‰å…¨` `å®æ—¶ç›‘æ§` `è‡ªæˆ‘ä¿®å¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ç¼“è§£æœ‰å®³å†…å®¹ç”Ÿæˆçš„æ–¹æ³•ä¸»è¦ä¾èµ–äº‹åè¿‡æ»¤ï¼Œå¯¼è‡´é«˜å»¶è¿Ÿå’Œè®¡ç®—å¼€é”€ï¼Œä¸é€‚ç”¨äºæµå¼ç”Ÿæˆã€‚
2. Self-Sanitizeæ¡†æ¶æ¨¡æ‹Ÿäººç±»çš„è‡ªæˆ‘ç›‘æ§å’Œä¿®å¤è¡Œä¸ºï¼Œé€šè¿‡è½»é‡çº§æ¨¡å—åœ¨tokençº§åˆ«å®æ—¶æ£€æµ‹å’Œä¿®æ­£æœ‰å®³å†…å®¹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSelf-Sanitizeåœ¨éšç§æ³„éœ²åœºæ™¯ä¸‹ï¼Œä»¥æå°å¼€é”€æ˜¾è‘—æå‡ç¼“è§£æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ•ˆç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èŠå¤©æœºå™¨äººå’Œä»£ç åŠ©æ‰‹ç­‰å¹¿æ³›åº”ç”¨ä¸­å–å¾—æ˜¾è‘—æˆåŠŸï¼Œç”Ÿæˆæœ‰å®³å†…å®¹çš„é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚å°½ç®¡åœ¨ä½¿LLMsç¬¦åˆå®‰å…¨å’Œä¼¦ç†æ ‡å‡†æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å¯¹æŠ—æ€§æç¤ºä»ç„¶å¯ä»¥è¯±å¯¼å‡ºä¸è‰¯å“åº”ã€‚ç°æœ‰çš„ç¼“è§£ç­–ç•¥ä¸»è¦åŸºäºäº‹åè¿‡æ»¤ï¼Œè¿™ä¼šå¼•å…¥å¤§é‡çš„å»¶è¿Ÿæˆ–è®¡ç®—å¼€é”€ï¼Œå¹¶ä¸”ä¸tokençº§åˆ«çš„æµå¼ç”Ÿæˆä¸å…¼å®¹ã€‚æœ¬æ–‡ä»‹ç»Self-Sanitizeï¼Œè¿™æ˜¯ä¸€ä¸ªå—è®¤çŸ¥å¿ƒç†å­¦å¯å‘çš„æ–°å‹LLMé©±åŠ¨çš„ç¼“è§£æ¡†æ¶ï¼Œå®ƒæ¨¡æ‹Ÿäº†äººç±»åœ¨å¯¹è¯ä¸­çš„è‡ªæˆ‘ç›‘æ§å’Œè‡ªæˆ‘ä¿®å¤è¡Œä¸ºã€‚Self-SanitizeåŒ…å«ä¸€ä¸ªè½»é‡çº§çš„Self-Monitoræ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡è¡¨å¾å·¥ç¨‹åœ¨tokençº§åˆ«æŒç»­æ£€æŸ¥LLMä¸­çš„é«˜çº§æ„å›¾ï¼Œä»¥åŠä¸€ä¸ªSelf-Repairæ¨¡å—ï¼Œè¯¥æ¨¡å—æ‰§è¡Œæœ‰å®³å†…å®¹çš„å°±åœ°æ ¡æ­£ï¼Œè€Œæ— éœ€å¯åŠ¨å•ç‹¬çš„å®¡æŸ¥å¯¹è¯ã€‚è¿™ç§è®¾è®¡å…è®¸å®æ—¶æµå¼ç›‘æ§å’Œæ— ç¼ä¿®å¤ï¼Œå¯¹å»¶è¿Ÿå’Œèµ„æºåˆ©ç”¨ç‡çš„å½±å“å¯å¿½ç•¥ä¸è®¡ã€‚é‰´äºä»¥å¾€çš„ç ”ç©¶é€šå¸¸å¯¹ä¾µçŠ¯éšç§çš„å†…å®¹å…³æ³¨ä¸è¶³ï¼Œæˆ‘ä»¬å¯¹å››ä¸ªLLMåœ¨ä¸‰ä¸ªéšç§æ³„éœ²åœºæ™¯ä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒSelf-Sanitizeä»¥æœ€å°çš„å¼€é”€å’Œä¸é™ä½LLMæ•ˆç”¨çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„ç¼“è§£æ€§èƒ½ï¼Œä¸ºæ›´å®‰å…¨çš„LLMéƒ¨ç½²æä¾›äº†å®ç”¨è€Œå¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹é“¾æ¥è·å¾—ï¼šhttps://github.com/wjfu99/LLM_Self_Sanitize

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå†…å®¹æ—¶å¯èƒ½å­˜åœ¨çš„éšç§æ³„éœ²é—®é¢˜ã€‚ç°æœ‰çš„ç¼“è§£æ–¹æ³•ï¼Œå¦‚äº‹åè¿‡æ»¤ï¼Œä¼šå¼•å…¥æ˜¾è‘—çš„å»¶è¿Ÿå’Œè®¡ç®—å¼€é”€ï¼Œæ— æ³•æ»¡è¶³å®æ—¶æµå¼ç”Ÿæˆçš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œä»¥å¾€ç ”ç©¶å¯¹éšç§æ³„éœ²çš„å…³æ³¨åº¦ä¸è¶³ï¼Œç¼ºä¹æœ‰æ•ˆçš„é’ˆå¯¹æ€§è§£å†³æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡æ‹Ÿäººç±»çš„è‡ªæˆ‘ç›‘æ§å’Œè‡ªæˆ‘ä¿®å¤æœºåˆ¶ï¼Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿå®æ—¶æ£€æµ‹å’Œä¿®æ­£æœ‰å®³å†…å®¹çš„æ¡†æ¶ã€‚é€šè¿‡åœ¨LLMå†…éƒ¨ç½²è½»é‡çº§çš„ç›‘æ§å’Œä¿®å¤æ¨¡å—ï¼Œå®ç°å¯¹ç”Ÿæˆè¿‡ç¨‹çš„åŠ¨æ€å¹²é¢„ï¼Œä»è€Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„äº‹åå¤„ç†å¸¦æ¥çš„å»¶è¿Ÿé—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSelf-Sanitizeæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šSelf-Monitorå’ŒSelf-Repairã€‚Self-Monitoræ¨¡å—é€šè¿‡è¡¨å¾å·¥ç¨‹æŠ€æœ¯ï¼Œåœ¨tokençº§åˆ«æŒç»­ç›‘æ§LLMçš„å†…éƒ¨çŠ¶æ€ï¼Œæ£€æµ‹æ˜¯å¦å­˜åœ¨æ½œåœ¨çš„æœ‰å®³æ„å›¾ã€‚Self-Repairæ¨¡å—åˆ™åœ¨æ£€æµ‹åˆ°æœ‰å®³å†…å®¹åï¼Œç«‹å³è¿›è¡Œå°±åœ°æ ¡æ­£ï¼Œæ— éœ€å¯åŠ¨é¢å¤–çš„å®¡æŸ¥å¯¹è¯ã€‚æ•´ä¸ªè¿‡ç¨‹ä»¥æµå¼æ–¹å¼è¿›è¡Œï¼Œå¯¹LLMçš„ç”Ÿæˆè¿‡ç¨‹å‡ ä¹æ²¡æœ‰å»¶è¿Ÿå½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šSelf-Sanitizeçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ¨¡ä»¿äººç±»è®¤çŸ¥è¿‡ç¨‹çš„å®æ—¶ç›‘æ§å’Œä¿®å¤æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„äº‹åè¿‡æ»¤æ–¹æ³•ä¸åŒï¼ŒSelf-Sanitizeèƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€åœ°å¹²é¢„å’Œä¿®æ­£æœ‰å®³å†…å®¹ï¼Œä»è€Œé¿å…äº†å»¶è¿Ÿé—®é¢˜ï¼Œå¹¶æé«˜äº†ç¼“è§£æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶çš„è®¾è®¡è½»é‡çº§ï¼Œå¯¹LLMçš„æ€§èƒ½å½±å“æå°ã€‚

**å…³é”®è®¾è®¡**ï¼šSelf-Monitoræ¨¡å—çš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°æå–å’Œåˆ†æLLMçš„å†…éƒ¨è¡¨å¾ï¼Œä»¥å‡†ç¡®åˆ¤æ–­æ˜¯å¦å­˜åœ¨æœ‰å®³æ„å›¾ã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°å¯¹ç‰¹å®šå±‚çš„æ¿€æ´»å€¼è¿›è¡Œåˆ†æï¼Œæˆ–è€…ä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†ç±»å™¨æ¥è¯†åˆ«æœ‰å®³æ¨¡å¼ã€‚Self-Repairæ¨¡å—çš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•åœ¨ä¸å½±å“ç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œå¯¹æœ‰å®³å†…å®¹è¿›è¡Œæœ‰æ•ˆçš„ä¿®æ­£ã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°ä½¿ç”¨ç‰¹å®šçš„ç¼–è¾‘ç­–ç•¥ï¼Œæˆ–è€…åˆ©ç”¨LLMè‡ªèº«çš„ç”Ÿæˆèƒ½åŠ›æ¥ç”Ÿæˆæ›´å®‰å…¨çš„å†…å®¹ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æ²¡æœ‰è¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨å››ä¸ªLLMå’Œä¸‰ä¸ªéšç§æ³„éœ²åœºæ™¯ä¸‹è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜Self-Sanitizeæ¡†æ¶èƒ½å¤Ÿä»¥æå°çš„å¼€é”€å®ç°å“è¶Šçš„ç¼“è§£æ€§èƒ½ï¼Œä¸”ä¸ä¼šæ˜¾è‘—é™ä½LLMçš„æ•ˆç”¨ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­æœ‰æ‰€å±•ç¤ºï¼Œä½†å…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚å®éªŒç»“æœéªŒè¯äº†Self-Sanitizeæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Self-Sanitizeæ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦å®‰å…¨å†…å®¹ç”Ÿæˆçš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œå¦‚èŠå¤©æœºå™¨äººã€ä»£ç åŠ©æ‰‹ã€å†…å®¹åˆ›ä½œå¹³å°ç­‰ã€‚é€šè¿‡å®æ—¶ç›‘æ§å’Œä¿®å¤æœ‰å®³å†…å®¹ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆé™ä½éšç§æ³„éœ²é£é™©ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¸ºLLMçš„å•†ä¸šåŒ–éƒ¨ç½²æä¾›æ›´å¯é çš„å®‰å…¨ä¿éšœã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥å‘å±•ï¼Œåº”ç”¨äºæ›´å¤æ‚çš„å®‰å…¨åœºæ™¯ï¼Œä¾‹å¦‚å¯¹æŠ—æ€§æ”»å‡»é˜²å¾¡å’Œè™šå‡ä¿¡æ¯æ£€æµ‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As Large Language Models (LLMs) achieve remarkable success across a wide range of applications, such as chatbots and code copilots, concerns surrounding the generation of harmful content have come increasingly into focus. Despite significant advances in aligning LLMs with safety and ethical standards, adversarial prompts can still be crafted to elicit undesirable responses. Existing mitigation strategies are predominantly based on post-hoc filtering, which introduces substantial latency or computational overhead, and is incompatible with token-level streaming generation. In this work, we introduce Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive psychology, which emulates human self-monitor and self-repair behaviors during conversations. Self-Sanitize comprises a lightweight Self-Monitor module that continuously inspects high-level intentions within the LLM at the token level via representation engineering, and a Self-Repair module that performs in-place correction of harmful content without initiating separate review dialogues. This design allows for real-time streaming monitoring and seamless repair, with negligible impact on latency and resource utilization. Given that privacy-invasive content has often been insufficiently focused in previous studies, we perform extensive experiments on four LLMs across three privacy leakage scenarios. The results demonstrate that Self-Sanitize achieves superior mitigation performance with minimal overhead and without degrading the utility of LLMs, offering a practical and robust solution for safer LLM deployments. Our code is available at the following link: https://github.com/wjfu99/LLM_Self_Sanitize

