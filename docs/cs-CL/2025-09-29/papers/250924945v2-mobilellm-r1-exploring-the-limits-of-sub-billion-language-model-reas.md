---
layout: default
title: MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes
---

# MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24945" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24945v2</a>
  <a href="https://arxiv.org/pdf/2509.24945.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24945v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24945v2', 'MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changsheng Zhao, Ernie Chang, Zechun Liu, Chia-Jung Chang, Wei Wen, Chen Lai, Sheng Cao, Yuandong Tian, Raghuraman Krishnamoorthi, Yangyang Shi, Vikas Chandra

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-09-30)

**å¤‡æ³¨**: Model: https://huggingface.co/collections/facebook/mobilellm-r1-68c4597b104fac45f28f448e

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MobileLLM-R1ï¼šé€šè¿‡å¼€æ”¾è®­ç»ƒæ–¹æ¡ˆæ¢ç´¢åäº¿å‚æ•°ä»¥ä¸‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æé™**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æ•°æ®é‡é‡‡æ ·` `å¼€æºè®­ç»ƒ` `çŸ¥è¯†è’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä¾èµ–äºåºå¤§çš„æ•°æ®é›†ï¼ˆ>10T tokensï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥è·å¾—å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å°å‹æ¨¡å‹çš„å‘å±•ã€‚
2. MobileLLM-R1é€šè¿‡ç²¾å¿ƒç­–åˆ’å’Œé‡é‡‡æ ·å¼€æºæ•°æ®é›†ï¼Œè¯æ˜äº†ä»…éœ€çº¦2T tokensçš„é«˜è´¨é‡æ•°æ®å³å¯è®­ç»ƒå‡ºå…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„åäº¿å‚æ•°ä»¥ä¸‹æ¨¡å‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMobileLLM-R1-950Måœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¼˜äºæˆ–åŒ¹é…äº†ä½¿ç”¨æ›´å¤šæ•°æ®è®­ç»ƒçš„æ›´å¤§æ¨¡å‹ï¼Œå¦‚Qwen3-0.6Bã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èŒƒå¼è½¬å˜ï¼Œä»æœ¬èƒ½ååº”åˆ°æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œå¼•å‘äº†ä¸¤ä¸ªæ™®éå‡è®¾ï¼šï¼ˆ1ï¼‰æ¨ç†èƒ½åŠ›åªå‡ºç°åœ¨è¶³å¤Ÿå¤§çš„æ¨¡å‹ä¸­ï¼›ï¼ˆ2ï¼‰è¿™ç§èƒ½åŠ›éœ€è¦åœ¨æµ·é‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚è™½ç„¶ç¬¬ä¸€ä¸ªå‡è®¾å·²ç»å—åˆ°æœ€è¿‘çš„åäº¿å‚æ•°ä»¥ä¸‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚Qwen3-0.6Bå’ŒDeepSeekè’¸é¦å˜ä½“ï¼‰çš„æŒ‘æˆ˜ï¼Œä½†ç¬¬äºŒä¸ªå‡è®¾åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªå—åˆ°è´¨ç–‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†æ‰©å±•åˆ°æå…¶åºå¤§çš„è¯­æ–™åº“ï¼ˆ>10T tokensï¼‰å¯¹äºæ¨ç†èƒ½åŠ›å‡ºç°çš„å¿…è¦æ€§ã€‚é€šè¿‡ä»”ç»†ç­–åˆ’å’Œé‡æ–°é‡‡æ ·æˆ‘ä»¬è®¤ä¸ºåœ¨è®¾è®¡çš„æŒ‡æ ‡ä¸‹æœ‰ç›Šçš„å¼€æºæ•°æ®é›†ï¼Œæˆ‘ä»¬è¯æ˜äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å¯ä»¥ç”¨æ›´å°‘çš„æ•°æ®å‡ºç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œåªæœ‰çº¦2T tokensçš„é«˜è´¨é‡æ•°æ®å°±è¶³å¤Ÿäº†ï¼Œå¹¶ä¸”åœ¨ä»è¿™äº›çº¦2T tokensé‡æ–°é‡‡æ ·çš„æ•°æ®é›†ä¸Šè¿›è¡Œ4.2T tokensçš„é¢„è®­ç»ƒï¼Œç„¶åè¿›è¡Œæ—¢å®šçš„åè®­ç»ƒç¨‹åºï¼Œå°±å¯ä»¥å¼€å‘MobileLLM-R1ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åäº¿å‚æ•°ä»¥ä¸‹çš„æ¨ç†æ¨¡å‹ï¼Œå…¶æ€§èƒ½å¤§å¤§ä¼˜äºä¹‹å‰åœ¨å®Œå…¨å¼€æºæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼ŒMobileLLM-R1-950Mçš„AIMEå¾—åˆ†è¾¾åˆ°15.5ï¼Œè€ŒOLMo-2-1.48Bä»…ä¸º0.6ï¼ŒSmolLM-2-1.7Bä»…ä¸º0.3ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡ä¸Qwen3ç”¨äºé¢„è®­ç»ƒçš„36T-tokenä¸“æœ‰è¯­æ–™åº“ç›¸æ¯”ï¼ŒMobileLLM-R1-950Mä»…åœ¨11.7%çš„tokensä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œä½†å®ƒåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¸Qwen3-0.6Bç›¸åŒ¹é…æˆ–è¶…è¿‡äº†Qwen3-0.6Bã€‚ä¸ºäº†ä¿ƒè¿›è¿™æ–¹é¢çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†å®Œæ•´çš„è®­ç»ƒæ–¹æ¡ˆã€æ•°æ®æ¥æºã€æ•°æ®æ··åˆæ¯”ä¾‹å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä»¥åŠæ•´ä¸ªç ”ç©¶è¿‡ç¨‹ä¸­è·å¾—çš„å…³é”®è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸éœ€è¦ä½¿ç”¨è¶…è¿‡10T tokensçš„æ•°æ®è¿›è¡Œè®­ç»ƒæ‰èƒ½è·å¾—è¾ƒå¥½çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å¯¼è‡´è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”é™åˆ¶äº†å°å‹æ¨¡å‹çš„å‘å±•ã€‚å› æ­¤ï¼Œå¦‚ä½•ä½¿ç”¨æ›´å°‘çš„æ•°æ®è®­ç»ƒå‡ºå…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å°å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç²¾å¿ƒæŒ‘é€‰å’Œé‡é‡‡æ ·å¼€æºæ•°æ®é›†ï¼Œæ„å»ºä¸€ä¸ªé«˜è´¨é‡çš„å°è§„æ¨¡è®­ç»ƒæ•°æ®é›†ã€‚ä½œè€…è®¤ä¸ºï¼Œå¹¶éæ‰€æœ‰æ•°æ®éƒ½å¯¹æ¨ç†èƒ½åŠ›çš„æå‡æœ‰ç›Šï¼Œå› æ­¤éœ€è¦å¯¹æ•°æ®è¿›è¡Œç­›é€‰å’Œä¼˜åŒ–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åœ¨ä½¿ç”¨è¾ƒå°‘æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒå‡ºå…·æœ‰ç«äº‰åŠ›çš„æ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMobileLLM-R1çš„è®­ç»ƒæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®é›†æ„å»ºï¼šä»å¼€æºæ•°æ®é›†ä¸­é€‰æ‹©æœ‰ç›Šäºæ¨ç†èƒ½åŠ›æå‡çš„æ•°æ®ï¼Œå¹¶è¿›è¡Œé‡é‡‡æ ·ï¼Œæ„å»ºä¸€ä¸ªè§„æ¨¡çº¦ä¸º2T tokensçš„é«˜è´¨é‡æ•°æ®é›†ã€‚2) é¢„è®­ç»ƒï¼šä½¿ç”¨æ„å»ºçš„æ•°æ®é›†è¿›è¡Œ4.2T tokensçš„é¢„è®­ç»ƒã€‚3) åè®­ç»ƒï¼šé‡‡ç”¨æ—¢å®šçš„åè®­ç»ƒç¨‹åºï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„æ•°æ®é€‰æ‹©å’Œé‡é‡‡æ ·ç­–ç•¥ï¼Œè¯æ˜äº†é«˜è´¨é‡çš„å°è§„æ¨¡æ•°æ®é›†å¯ä»¥æ›¿ä»£å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶ä½¿å¾—å°å‹æ¨¡å‹ä¹Ÿèƒ½å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ æ³¨é‡æ•°æ®çš„è´¨é‡è€Œéæ•°é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡å…¬å¼€äº†å®Œæ•´çš„æ•°æ®é›†æ„å»ºç»†èŠ‚ï¼ŒåŒ…æ‹¬æ•°æ®æ¥æºã€æ•°æ®æ··åˆæ¯”ä¾‹ç­‰ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å…¬å¼€äº†æ¨¡å‹çš„è®­ç»ƒæ–¹æ¡ˆå’Œæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…è¿›è¡Œå¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚å¯èƒ½å‚è€ƒäº†å·²æœ‰çš„å·¥ä½œï¼Œè®ºæ–‡é‡ç‚¹åœ¨äºæ•°æ®å¤„ç†å’Œè®­ç»ƒæµç¨‹çš„ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MobileLLM-R1-950Måœ¨AIMEæµ‹è¯•ä¸­å–å¾—äº†15.5çš„å¾—åˆ†ï¼Œæ˜¾è‘—ä¼˜äºOLMo-2-1.48Bï¼ˆ0.6ï¼‰å’ŒSmolLM-2-1.7Bï¼ˆ0.3ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå°½ç®¡MobileLLM-R1-950Mä»…ä½¿ç”¨äº†Qwen3é¢„è®­ç»ƒæ•°æ®é‡çš„11.7%ï¼Œä½†åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½ä¸Qwen3-0.6Bç›¸åŒ¹é…ç”šè‡³è¶…è¶Šäº†Qwen3-0.6Bï¼Œè¯æ˜äº†é«˜è´¨é‡å°è§„æ¨¡æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MobileLLM-R1çš„ç ”ç©¶æˆæœå¯åº”ç”¨äºèµ„æºå—é™çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿç­‰ã€‚è¯¥ç ”ç©¶é™ä½äº†è®­ç»ƒå’Œéƒ¨ç½²è¯­è¨€æ¨¡å‹çš„æˆæœ¬ï¼Œä½¿å¾—æ›´å¤šç”¨æˆ·å¯ä»¥ä½¿ç”¨é«˜æ€§èƒ½çš„AIæœåŠ¡ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ¨åŠ¨å°å‹è¯­è¨€æ¨¡å‹åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é—®ç­”ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The paradigm shift in large language models (LLMs) from instinctive responses to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1) reasoning capabilities only emerge in sufficiently large models, and (2) such capabilities require training on massive datasets. While the first assumption has already been challenged by recent sub-billion-parameter reasoning models such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely unquestioned. In this work, we revisit the necessity of scaling to extremely large corpora (>10T tokens) for reasoning emergence. By carefully curating and resampling open-source datasets that we identify as beneficial under our designed metrics, we demonstrate that strong reasoning abilities can emerge with far less data. Specifically, we show that only ~2T tokens of high-quality data are sufficient, and pre-training with 4.2T tokens on the dataset resampled from these ~2T tokens, followed by a established post-training procedure, enables the development of MobileLLM-R1, a series of sub-billion-parameter reasoning models that substantially outperform prior models trained on fully open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of 15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B. Remarkably, despite being trained on only 11.7% of the tokens compared to Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate further research in this direction, we have released the complete training recipe, data sources, data mixing ratio, and model checkpoints, together with the key insights obtained throughout this study.

