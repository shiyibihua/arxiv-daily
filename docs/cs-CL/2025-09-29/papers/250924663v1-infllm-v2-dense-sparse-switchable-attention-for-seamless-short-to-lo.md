---
layout: default
title: InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation
---

# InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24663" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.24663v1</a>
  <a href="https://arxiv.org/pdf/2509.24663.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24663v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24663v1', 'InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Weilin Zhao, Zihan Zhou, Zhou Su, Chaojun Xiao, Yuxuan Li, Yanghao Li, Yudi Zhang, Weilun Zhao, Zhen Li, Yuxiang Huang, Ao Sun, Xu Han, Zhiyuan Liu

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-29

**üîó ‰ª£Á†Å/È°πÁõÆ**: [HUGGINGFACE](https://huggingface.co/openbmb/MiniCPM4.1-8B)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫InfLLM-V2Ôºö‰∏ÄÁßçÁ®†ÂØÜ-Á®ÄÁñèÂèØÂàáÊç¢Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂÆûÁé∞Ê®°Âûã‰ªéÁü≠Â∫èÂàóÂà∞ÈïøÂ∫èÂàóÁöÑÊó†ÁºùÈÄÇÂ∫î„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÂ∫èÂàóÂª∫Ê®°` `Á®ÄÁñèÊ≥®ÊÑèÂäõ` `Á®†ÂØÜÊ≥®ÊÑèÂäõ` `ÂèØÂàáÊç¢Ê≥®ÊÑèÂäõ` `ËØ≠Ë®ÄÊ®°Âûã` `ÈïøÊñáÊú¨ÁêÜËß£` `ÊÄùÁª¥ÈìæÊé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïÂºïÂÖ•ËøáÂ§öÂèÇÊï∞ÔºåÁ†¥Âùè‰∫Ü‚ÄúÁü≠È¢ÑËÆ≠ÁªÉÔºåÈïøÂæÆË∞É‚ÄùÊµÅÁ®ãÔºåÂØºËá¥Êî∂ÊïõÊÖ¢„ÄÅÈöæÂä†ÈÄü„ÄÇ
2. InfLLM-V2ÈÄöËøáÊó†ÂèÇÊï∞Êû∂ÊûÑ‰øÆÊîπÈáçÁî®Á®†ÂØÜÊ≥®ÊÑèÂäõÂèÇÊï∞Ôºå‰øùÊåÅÁü≠ÈïøÂ∫èÂàóÂ§ÑÁêÜ‰∏ÄËá¥ÊÄßÔºåÂÆûÁé∞Êó†ÁºùÈÄÇÂ∫î„ÄÇ
3. InfLLM-V2Âú®Áü≠Â∫èÂàó‰ΩøÁî®Á®†ÂØÜÊ≥®ÊÑèÂäõÔºåÈïøÂ∫èÂàó‰ΩøÁî®Á®ÄÁñèÊ≥®ÊÑèÂäõÔºåÂÖºÈ°æÊïàÁéá‰∏éÊÄßËÉΩÔºåÂä†ÈÄüÊïàÊûúÊòæËëó„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞‰ª£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂØπÈïøÂ∫èÂàóÂ§ÑÁêÜËÉΩÂäõËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÊ†áÂáÜTransformerÊû∂ÊûÑ‰∏≠ÁöÑËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂Èù¢‰∏¥‰∏•ÈáçÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òÁì∂È¢à„ÄÇÂèØËÆ≠ÁªÉÁöÑÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïÊèê‰æõ‰∫Ü‰∏ÄÁßçÊúâÂ∏åÊúõÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÔºàÂ¶ÇNSAÔºâÂºïÂÖ•‰∫ÜËøáÂ§öÁöÑÈ¢ùÂ§ñÂèÇÊï∞ÔºåÂπ∂Á†¥Âùè‰∫Ü‰º†ÁªüÁöÑ‚ÄúÁü≠Â∫èÂàóÈ¢ÑËÆ≠ÁªÉÔºåÈïøÂ∫èÂàóÂæÆË∞É‚ÄùÂ∑•‰ΩúÊµÅÁ®ãÔºåÂØºËá¥Êî∂ÊïõÁºìÊÖ¢ÂíåÈöæ‰ª•Âä†ÈÄü„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁ®†ÂØÜ-Á®ÄÁñèÂèØÂàáÊç¢Ê≥®ÊÑèÂäõÊ°ÜÊû∂ÔºåÁß∞‰∏∫InfLLM-V2„ÄÇInfLLM-V2ÊòØ‰∏ÄÁßçÂèØËÆ≠ÁªÉÁöÑÁ®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂèØ‰ª•Êó†ÁºùÂú∞Â∞ÜÊ®°Âûã‰ªéÁü≠Â∫èÂàóÈÄÇÂ∫îÂà∞ÈïøÂ∫èÂàó„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåInfLLM-V2ÈÄöËøáÊó†ÂèÇÊï∞ÁöÑÊû∂ÊûÑ‰øÆÊîπÈáçÁî®Á®†ÂØÜÊ≥®ÊÑèÂäõÂèÇÊï∞Ôºå‰øùÊåÅÁü≠Â∫èÂàóÂíåÈïøÂ∫èÂàóÂ§ÑÁêÜ‰πãÈó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåInfLLM-V2ÈÄöËøáÂØπÁü≠ËæìÂÖ•‰ΩøÁî®Á®†ÂØÜÊ≥®ÊÑèÂäõÔºåÂπ∂Âπ≥ÊªëËøáÊ∏°Âà∞ÂØπÈïøÂ∫èÂàó‰ΩøÁî®Á®ÄÁñèÊ≥®ÊÑèÂäõÔºåÁ°Æ‰øù‰∫ÜÊâÄÊúâÂ∫èÂàóÈïøÂ∫¶‰∏äÁöÑËÆ°ÁÆóÊïàÁéá„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞ÂÆûÈôÖÂä†ÈÄüÔºåÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÂºïÂÖ•‰∫ÜInfLLM-V2ÁöÑÈ´òÊïàÂÆûÁé∞ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂºÄÈîÄ„ÄÇÂú®Èïø‰∏ä‰∏ãÊñáÁêÜËß£ÂíåÊÄùÁª¥ÈìæÊé®ÁêÜÊñπÈù¢ÁöÑÂÆûÈ™åË°®ÊòéÔºåInfLLM-V2ÊØîÁ®†ÂØÜÊ≥®ÊÑèÂäõÂø´4ÂÄçÔºåÂêåÊó∂ÂàÜÂà´‰øùÁïô‰∫Ü98.1%Âíå99.7%ÁöÑÊÄßËÉΩ„ÄÇÂü∫‰∫éInfLLM-V2Ê°ÜÊû∂ÔºåÊàë‰ª¨ËÆ≠ÁªÉÂπ∂ÂºÄÊ∫ê‰∫ÜMiniCPM4.1Ôºàhttps://huggingface.co/openbmb/MiniCPM4.1-8BÔºâÔºåËøôÊòØ‰∏Ä‰∏™Ê∑∑ÂêàÊé®ÁêÜÊ®°ÂûãÔºå‰∏∫Á†îÁ©∂Á§æÂå∫Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂèØÂ§çÁé∞ÁöÑÂÆûÁé∞„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ÔºåÊ†áÂáÜÁöÑTransformerËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Èù¢‰∏¥ËÆ°ÁÆóÂíåÂÜÖÂ≠òÁì∂È¢à„ÄÇÂèØËÆ≠ÁªÉÁöÑÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïËôΩÁÑ∂ÊúâÊΩúÂäõÔºå‰ΩÜÂ¶ÇNSAÁ≠âÊñπÊ≥ïÂºïÂÖ•‰∫ÜËøáÂ§öÁöÑÈ¢ùÂ§ñÂèÇÊï∞ÔºåÁ†¥Âùè‰∫Ü‰º†ÁªüÁöÑ‚ÄúÁü≠Â∫èÂàóÈ¢ÑËÆ≠ÁªÉÔºåÈïøÂ∫èÂàóÂæÆË∞É‚ÄùÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂØºËá¥Êî∂ÊïõÈÄüÂ∫¶ÊÖ¢ÔºåÈöæ‰ª•Âä†ÈÄüÔºåÂπ∂‰∏îÂ¢ûÂä†‰∫ÜÊ®°ÂûãÈÉ®ÁΩ≤ÁöÑÊàêÊú¨„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöInfLLM-V2ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËÆæËÆ°‰∏ÄÁßçÁ®†ÂØÜ-Á®ÄÁñèÂèØÂàáÊç¢ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂú®Áü≠Â∫èÂàó‰∏äÂà©Áî®Á®†ÂØÜÊ≥®ÊÑèÂäõÁöÑ‰ºòÂäøÔºåÂπ∂Âú®ÈïøÂ∫èÂàó‰∏äÂπ≥ÊªëËøáÊ∏°Âà∞Á®ÄÁñèÊ≥®ÊÑèÂäõÔºå‰ªéËÄåÂú®‰øùËØÅÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÈôç‰ΩéËÆ°ÁÆóÂíåÂÜÖÂ≠òÂºÄÈîÄ„ÄÇÈÄöËøáÈáçÁî®Á®†ÂØÜÊ≥®ÊÑèÂäõÁöÑÂèÇÊï∞ÔºåÈÅøÂÖçÂºïÂÖ•È¢ùÂ§ñÁöÑÂèÇÊï∞Ôºå‰øùÊåÅ‰∫ÜÊ®°ÂûãÂú®Áü≠Â∫èÂàóÂíåÈïøÂ∫èÂàóÂ§ÑÁêÜ‰∏äÁöÑ‰∏ÄËá¥ÊÄßÔºå‰ªéËÄåËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÁü•ËØÜ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöInfLLM-V2Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´Á®†ÂØÜÊ≥®ÊÑèÂäõÊ®°ÂùóÂíåÁ®ÄÁñèÊ≥®ÊÑèÂäõÊ®°ÂùóÔºå‰ª•Âèä‰∏Ä‰∏™ÂàáÊç¢Êú∫Âà∂„ÄÇÂØπ‰∫éÁü≠Â∫èÂàóÔºåÊ®°Âûã‰ΩøÁî®Á®†ÂØÜÊ≥®ÊÑèÂäõËøõË°åÂ§ÑÁêÜÔºå‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇÈöèÁùÄÂ∫èÂàóÈïøÂ∫¶ÁöÑÂ¢ûÂä†ÔºåÊ®°ÂûãÈÄêÊ∏êÂàáÊç¢Âà∞Á®ÄÁñèÊ≥®ÊÑèÂäõÔºå‰ª•Èôç‰ΩéËÆ°ÁÆóÂíåÂÜÖÂ≠òÂºÄÈîÄ„ÄÇÂàáÊç¢Êú∫Âà∂Ê†πÊçÆÂ∫èÂàóÈïøÂ∫¶Âä®ÊÄÅË∞ÉÊï¥Á®†ÂØÜÂíåÁ®ÄÁñèÊ≥®ÊÑèÂäõÁöÑÊùÉÈáçÔºåÂÆûÁé∞Âπ≥ÊªëËøáÊ∏°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöInfLLM-V2ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Á®†ÂØÜ-Á®ÄÁñèÂèØÂàáÊç¢ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ª•ÂèäÂèÇÊï∞ÈáçÁî®Á≠ñÁï•„ÄÇÈÄöËøáÂèÇÊï∞ÈáçÁî®ÔºåÈÅøÂÖç‰∫ÜÂºïÂÖ•È¢ùÂ§ñÁöÑÂèÇÊï∞Ôºå‰øùÊåÅ‰∫ÜÊ®°ÂûãÂú®Áü≠Â∫èÂàóÂíåÈïøÂ∫èÂàóÂ§ÑÁêÜ‰∏äÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂèØÂàáÊç¢ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊ†πÊçÆÂ∫èÂàóÈïøÂ∫¶Âä®ÊÄÅË∞ÉÊï¥ËÆ°ÁÆóËµÑÊ∫êÔºå‰ªéËÄåÂú®‰øùËØÅÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÈôç‰ΩéËÆ°ÁÆóÂíåÂÜÖÂ≠òÂºÄÈîÄ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöInfLLM-V2ÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Êó†ÂèÇÊï∞ÁöÑÊû∂ÊûÑ‰øÆÊîπÔºå‰ª•ÈáçÁî®Á®†ÂØÜÊ≥®ÊÑèÂäõÂèÇÊï∞Ôºõ2) Âü∫‰∫éÂ∫èÂàóÈïøÂ∫¶ÁöÑÂä®ÊÄÅÂàáÊç¢Êú∫Âà∂ÔºåÁî®‰∫éÂπ≥ÊªëËøáÊ∏°Âà∞Á®ÄÁñèÊ≥®ÊÑèÂäõÔºõ3) È´òÊïàÁöÑÁ®ÄÁñèÊ≥®ÊÑèÂäõÂÆûÁé∞Ôºå‰ª•Èôç‰ΩéËÆ°ÁÆóÂºÄÈîÄ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÊú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜËØ¥ÊòéÔºåÈúÄË¶ÅÂèÇËÄÉËÆ∫ÊñáÂÖ®Êñá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåInfLLM-V2Âú®Èïø‰∏ä‰∏ãÊñáÁêÜËß£ÂíåÊÄùÁª¥ÈìæÊé®ÁêÜ‰ªªÂä°‰∏äÔºåÊØîÁ®†ÂØÜÊ≥®ÊÑèÂäõÂø´4ÂÄçÔºåÂêåÊó∂ÂàÜÂà´‰øùÁïô‰∫Ü98.1%Âíå99.7%ÁöÑÊÄßËÉΩ„ÄÇÂü∫‰∫éInfLLM-V2Ê°ÜÊû∂ËÆ≠ÁªÉÁöÑMiniCPM4.1Ê®°ÂûãÂ∑≤ÂºÄÊ∫êÔºå‰∏∫Á†îÁ©∂Á§æÂå∫Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂèØÂ§çÁé∞ÁöÑÂÆûÁé∞„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

InfLLM-V2ÈÄÇÁî®‰∫éÈúÄË¶ÅÂ§ÑÁêÜÈïøÂ∫èÂàóÁöÑÂêÑÁßçËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°Ôºå‰æãÂ¶ÇÈïøÊñáÊú¨ÊëòË¶Å„ÄÅÊñáÊ°£ÈóÆÁ≠î„ÄÅ‰ª£Á†ÅÁîüÊàêÂíåÈïøÂØπËØùÁ≠â„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•ÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂíåÂÜÖÂ≠òÂºÄÈîÄÔºå‰ΩøÂæóÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÈïøÂ∫èÂàóÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÂú®Ëøô‰∫õ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÔºå‰ΩøÂæóËøô‰∫õËÆæÂ§á‰πüËÉΩÂ§üËøêË°åÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Long-sequence processing is a critical capability for modern large language models. However, the self-attention mechanism in the standard Transformer architecture faces severe computational and memory bottlenecks when processing long sequences. While trainable sparse attention methods offer a promising solution, existing approaches such as NSA introduce excessive extra parameters and disrupt the conventional \textit{pretrain-on-short, finetune-on-long} workflow, resulting in slow convergence and difficulty in acceleration. To overcome these limitations, we introduce dense-sparse switchable attention framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2 reuses dense attention parameters through parameter-free architecture modification, maintaining consistency between short and long sequence processing. Additionally, InfLLM-V2 ensures computational efficiency across all sequence lengths, by using dense attention for short inputs and smoothly transitioning to sparse attention for long sequences. To achieve practical acceleration, we further introduce an efficient implementation of InfLLM-V2 that significantly reduces the computational overhead. Our experiments on long-context understanding and chain-of-thought reasoning demonstrate that InfLLM-V2 is 4$\times$ faster than dense attention while retaining 98.1% and 99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we have trained and open-sourced MiniCPM4.1 (https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model, providing a reproducible implementation for the research community.

