---
layout: default
title: Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection
---

# Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25138" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25138v1</a>
  <a href="https://arxiv.org/pdf/2509.25138.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25138v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25138v1', 'Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ivan Vykopal, Antonia Karamolegkou, Jaroslav KopÄan, Qiwei Peng, TomÃ¡Å¡ JavÅ¯rek, Michal Gregor, MariÃ¡n Å imko

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶å¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨è·¨è¯­è¨€äº‹å®æ ¸æŸ¥ä¸­çš„è¯­è¨€å’Œæ£€ç´¢åå·®**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æ¨¡å‹` `äº‹å®æ ¸æŸ¥` `è¯­è¨€åå·®` `æ£€ç´¢åå·®` `è·¨è¯­è¨€å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è·¨è¯­è¨€äº‹å®æ ¸æŸ¥æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€ä¸Šè¡¨ç°ä¸ä½³ï¼Œå­˜åœ¨æ˜¾è‘—çš„è¯­è¨€åå·®ã€‚
2. é€šè¿‡å¤šè¯­è¨€æç¤ºç­–ç•¥ï¼Œç ”ç©¶ä¸åŒLLMåœ¨å¤šç§è¯­è¨€ä¸Šçš„æ€§èƒ½å·®å¼‚ï¼Œæ­ç¤ºè¯­è¨€åå·®çš„æ¨¡å¼ã€‚
3. åˆ†æä¿¡æ¯æ£€ç´¢è¿‡ç¨‹ä¸­çš„æ£€ç´¢åå·®ï¼Œå‘ç°çƒ­é—¨å£°æ˜è¢«è¿‡åº¦æ£€ç´¢ï¼Œå½±å“äº‹å®æ ¸æŸ¥çš„å…¬å¹³æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸ºè·¨è¯­è¨€äº‹å®æ ¸æŸ¥æä¾›äº†å¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸è¡¨ç°å‡ºè¯­è¨€åå·®ï¼Œå³åœ¨é«˜èµ„æºè¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰ä¸Šçš„è¡¨ç°æ˜æ˜¾ä¼˜äºä½èµ„æºè¯­è¨€ã€‚æœ¬æ–‡è¿˜æå‡ºå¹¶ç ”ç©¶äº†ä¸€ä¸ªæ–°æ¦‚å¿µâ€”â€”æ£€ç´¢åå·®ï¼Œå³ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿå€¾å‘äºåè¢’æŸäº›ä¿¡æ¯ï¼Œå¯¼è‡´æ£€ç´¢è¿‡ç¨‹å‡ºç°åå·®ã€‚æœ¬æ–‡ç ”ç©¶äº†å…ˆå‰äº‹å®æ ¸æŸ¥å£°æ˜æ£€æµ‹(PFCD)ä¸­çš„è¯­è¨€å’Œæ£€ç´¢åå·®ã€‚æˆ‘ä»¬ä½¿ç”¨å®Œå…¨å¤šè¯­è¨€çš„æç¤ºç­–ç•¥ï¼Œåˆ©ç”¨AMC-16Kæ•°æ®é›†ï¼Œè¯„ä¼°äº†20ç§è¯­è¨€çš„å…­ä¸ªå¼€æºå¤šè¯­è¨€LLMã€‚é€šè¿‡å°†ä»»åŠ¡æç¤ºç¿»è¯‘æˆæ¯ç§è¯­è¨€ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å•è¯­å’Œè·¨è¯­æ€§èƒ½çš„å·®å¼‚ï¼Œå¹¶æ ¹æ®æ¨¡å‹ç³»åˆ—ã€å¤§å°å’Œæç¤ºç­–ç•¥ç¡®å®šäº†å…³é”®è¶‹åŠ¿ã€‚æˆ‘ä»¬çš„å‘ç°çªå‡ºäº†LLMè¡Œä¸ºä¸­æŒç»­å­˜åœ¨çš„åå·®ï¼Œå¹¶ä¸ºæé«˜å¤šè¯­è¨€äº‹å®æ ¸æŸ¥çš„å…¬å¹³æ€§æä¾›äº†å»ºè®®ã€‚ä¸ºäº†ç ”ç©¶æ£€ç´¢åå·®ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šè¯­è¨€åµŒå…¥æ¨¡å‹ï¼Œå¹¶ç ”ç©¶äº†æ£€ç´¢åˆ°çš„å£°æ˜çš„é¢‘ç‡ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒæŸäº›å£°æ˜åœ¨ä¸åŒçš„å¸–å­ä¸­è¢«ä¸æˆæ¯”ä¾‹åœ°æ£€ç´¢ï¼Œå¯¼è‡´çƒ­é—¨å£°æ˜çš„æ£€ç´¢æ€§èƒ½è™šé«˜ï¼Œè€Œä¸å¤ªå¸¸è§çš„å£°æ˜åˆ™è¢«ä½ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€ç¯å¢ƒä¸‹ï¼Œå…ˆå‰ç»è¿‡äº‹å®æ ¸æŸ¥çš„å£°æ˜æ£€æµ‹ä»»åŠ¡ä¸­å­˜åœ¨çš„è¯­è¨€åå·®å’Œæ£€ç´¢åå·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä½èµ„æºè¯­è¨€æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå¹¶ä¸”ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿå¯èƒ½å­˜åœ¨åå¥½ï¼Œå¯¼è‡´æŸäº›å£°æ˜è¢«è¿‡åº¦æ£€ç´¢ï¼Œè€Œå¦ä¸€äº›å£°æ˜åˆ™è¢«å¿½ç•¥ï¼Œä»è€Œå½±å“äº‹å®æ ¸æŸ¥çš„å‡†ç¡®æ€§å’Œå…¬å¹³æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šè¯­è¨€æç¤ºç­–ç•¥ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°ä¸åŒå¤šè¯­è¨€LLMåœ¨å¤šç§è¯­è¨€ä¸Šçš„æ€§èƒ½ï¼Œä»è€Œæ­ç¤ºè¯­è¨€åå·®çš„æ¨¡å¼ã€‚åŒæ—¶ï¼Œé€šè¿‡åˆ†ææ£€ç´¢åˆ°çš„å£°æ˜çš„é¢‘ç‡ï¼Œé‡åŒ–æ£€ç´¢åå·®çš„å½±å“ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£åå·®çš„æ¥æºï¼Œå¹¶ä¸ºæ”¹è¿›å¤šè¯­è¨€äº‹å®æ ¸æŸ¥ç³»ç»Ÿæä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®é›†å‡†å¤‡ï¼šä½¿ç”¨AMC-16Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤šç§è¯­è¨€çš„äº‹å®æ ¸æŸ¥å£°æ˜ã€‚2) æ¨¡å‹é€‰æ‹©ï¼šé€‰æ‹©å…­ä¸ªå¼€æºå¤šè¯­è¨€LLMè¿›è¡Œè¯„ä¼°ã€‚3) æç¤ºç­–ç•¥ï¼šé‡‡ç”¨å®Œå…¨å¤šè¯­è¨€çš„æç¤ºç­–ç•¥ï¼Œå°†ä»»åŠ¡æç¤ºç¿»è¯‘æˆæ¯ç§è¯­è¨€ã€‚4) æ€§èƒ½è¯„ä¼°ï¼šè¯„ä¼°æ¨¡å‹åœ¨å•è¯­å’Œè·¨è¯­ç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼Œå¹¶åˆ†æè¯­è¨€åå·®ã€‚5) æ£€ç´¢åå·®åˆ†æï¼šä½¿ç”¨å¤šè¯­è¨€åµŒå…¥æ¨¡å‹ï¼Œåˆ†ææ£€ç´¢åˆ°çš„å£°æ˜çš„é¢‘ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†å¤šè¯­è¨€LLMåœ¨äº‹å®æ ¸æŸ¥ä»»åŠ¡ä¸­çš„è¯­è¨€åå·®ï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸Šçš„æ€§èƒ½å·®å¼‚ã€‚2) æå‡ºäº†æ£€ç´¢åå·®çš„æ¦‚å¿µï¼Œå¹¶åˆ†æäº†å…¶å¯¹äº‹å®æ ¸æŸ¥å…¬å¹³æ€§çš„å½±å“ã€‚3) é‡‡ç”¨å®Œå…¨å¤šè¯­è¨€çš„æç¤ºç­–ç•¥ï¼Œé¿å…äº†è‹±è¯­ä¸­å¿ƒçš„æ–¹æ³•ï¼Œæ›´çœŸå®åœ°åæ˜ äº†æ¨¡å‹çš„è·¨è¯­è¨€èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒè®¾è®¡æ–¹é¢ï¼Œæœ¬æ–‡é‡‡ç”¨äº†ä»¥ä¸‹å…³é”®è®¾è®¡ï¼š1) ä½¿ç”¨AMC-16Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†è¦†ç›–äº†20ç§è¯­è¨€ï¼Œä¸ºå¤šè¯­è¨€è¯„ä¼°æä¾›äº†åŸºç¡€ã€‚2) é€‰æ‹©äº†å…­ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„å¼€æºå¤šè¯­è¨€LLMï¼ŒåŒ…æ‹¬ä¸åŒæ¨¡å‹ç³»åˆ—å’Œå¤§å°çš„æ¨¡å‹ã€‚3) é‡‡ç”¨å®Œå…¨å¤šè¯­è¨€çš„æç¤ºç­–ç•¥ï¼Œç¡®ä¿æ‰€æœ‰è¯­è¨€çš„æç¤ºéƒ½ç»è¿‡ç¿»è¯‘ï¼Œé¿å…äº†è‹±è¯­ä¸­å¿ƒå¸¦æ¥çš„åå·®ã€‚4) ä½¿ç”¨å¤šè¯­è¨€åµŒå…¥æ¨¡å‹è®¡ç®—å£°æ˜ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå¹¶åˆ†ææ£€ç´¢åˆ°çš„å£°æ˜çš„é¢‘ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šè¯­è¨€LLMåœ¨ä¸åŒè¯­è¨€ä¸Šçš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œé«˜èµ„æºè¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰çš„æ€§èƒ½æ˜æ˜¾ä¼˜äºä½èµ„æºè¯­è¨€ã€‚æ£€ç´¢åå·®åˆ†ææ˜¾ç¤ºï¼ŒæŸäº›å£°æ˜è¢«ä¸æˆæ¯”ä¾‹åœ°æ£€ç´¢ï¼Œå¯¼è‡´çƒ­é—¨å£°æ˜çš„æ£€ç´¢æ€§èƒ½è™šé«˜ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨å¤šè¯­è¨€äº‹å®æ ¸æŸ¥ä¸­è§£å†³è¯­è¨€å’Œæ£€ç´¢åå·®çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šè¯­è¨€æ–°é—»åª’ä½“ã€ç¤¾äº¤åª’ä½“å¹³å°å’Œäº‹å®æ ¸æŸ¥ç»„ç»‡ï¼Œä»¥æé«˜è·¨è¯­è¨€äº‹å®æ ¸æŸ¥çš„å‡†ç¡®æ€§å’Œå…¬å¹³æ€§ã€‚é€šè¿‡å‡å°‘è¯­è¨€å’Œæ£€ç´¢åå·®ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°è¯†åˆ«è™šå‡ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­ï¼Œä»è€Œä¿ƒè¿›ä¿¡æ¯çš„å¥åº·ä¼ æ’­å’Œå…¬ä¼—çš„çŸ¥æƒ…æƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multilingual Large Language Models (LLMs) offer powerful capabilities for cross-lingual fact-checking. However, these models often exhibit language bias, performing disproportionately better on high-resource languages such as English than on low-resource counterparts. We also present and inspect a novel concept - retrieval bias, when information retrieval systems tend to favor certain information over others, leaving the retrieval process skewed. In this paper, we study language and retrieval bias in the context of Previously Fact-Checked Claim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20 languages using a fully multilingual prompting strategy, leveraging the AMC-16K dataset. By translating task prompts into each language, we uncover disparities in monolingual and cross-lingual performance and identify key trends based on model family, size, and prompting strategy. Our findings highlight persistent bias in LLM behavior and offer recommendations for improving equity in multilingual fact-checking. To investigate retrieval bias, we employed multilingual embedding models and look into the frequency of retrieved claims. Our analysis reveals that certain claims are retrieved disproportionately across different posts, leading to inflated retrieval performance for popular claims while under-representing less common ones.

