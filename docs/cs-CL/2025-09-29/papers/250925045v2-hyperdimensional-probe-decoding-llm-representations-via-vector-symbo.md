---
layout: default
title: Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures
---

# Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25045" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25045v2</a>
  <a href="https://arxiv.org/pdf/2509.25045.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25045v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25045v2', 'Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-12-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¶…ç»´æ¢é’ˆï¼Œé€šè¿‡å‘é‡ç¬¦å·æ¶æ„è§£ç å¤§å‹è¯­è¨€æ¨¡å‹è¡¨å¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `å‘é‡ç¬¦å·æ¶æ„` `ç¥ç»æ¢æµ‹` `è¡¨å¾å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå¯è§£é‡Šæ€§æ–¹æ³•éš¾ä»¥åŒæ—¶å…¼é¡¾è¾“å…¥ç‰¹å¾æå–å’Œè¾“å‡ºåˆ†å¸ƒåˆ†æï¼Œé™åˆ¶äº†å¯¹æ¨¡å‹å†…éƒ¨è¡¨å¾çš„å…¨é¢ç†è§£ã€‚
2. æå‡ºè¶…ç»´æ¢é’ˆï¼Œç»“åˆå‘é‡ç¬¦å·æ¶æ„å’Œç¥ç»æ¢æµ‹ï¼Œç»Ÿä¸€äº†ç›‘ç£æ¢é’ˆã€ç¨€ç–è‡ªç¼–ç å™¨å’Œlogitåˆ†æç­‰æ–¹æ³•ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå–LLMä¸­çš„æ¦‚å¿µï¼Œæ­ç¤ºç±»æ¯”æ¨ç†å’ŒQAç”Ÿæˆä¸­çš„æ¦‚å¿µé©±åŠ¨æ¨¡å¼ï¼Œæå‡è¯­ä¹‰ç†è§£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›å¼ºå¤§ï¼Œä½†å…¶å†…éƒ¨è¡¨å¾ä»ç„¶ä¸é€æ˜ï¼Œç†è§£æœ‰é™ã€‚ç°æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•è¦ä¹ˆä¾§é‡äºè¾“å…¥å¯¼å‘çš„ç‰¹å¾æå–ï¼Œå¦‚ç›‘ç£æ¢é’ˆå’Œç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œè¦ä¹ˆä¾§é‡äºè¾“å‡ºåˆ†å¸ƒæ£€æŸ¥ï¼Œå¦‚logitå¯¼å‘çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¦å…¨é¢ç†è§£LLMå‘é‡ç©ºé—´ï¼Œéœ€è¦æ•´åˆè¿™ä¸¤ç§è§†è§’ï¼Œè€Œç°æœ‰æ–¹æ³•ç”±äºå¯¹æ½œåœ¨ç‰¹å¾å®šä¹‰çš„çº¦æŸè€Œéš¾ä»¥åšåˆ°ã€‚æˆ‘ä»¬å¼•å…¥äº†è¶…ç»´æ¢é’ˆï¼Œè¿™æ˜¯ä¸€ç§æ··åˆç›‘ç£æ¢é’ˆï¼Œå®ƒå°†ç¬¦å·è¡¨å¾ä¸ç¥ç»æ¢æµ‹ç›¸ç»“åˆã€‚åˆ©ç”¨å‘é‡ç¬¦å·æ¶æ„ï¼ˆVSAï¼‰å’Œè¶…å‘é‡ä»£æ•°ï¼Œå®ƒç»Ÿä¸€äº†å…ˆå‰çš„æ–¹æ³•ï¼šç›‘ç£æ¢é’ˆçš„è‡ªé¡¶å‘ä¸‹å¯è§£é‡Šæ€§ã€SAEçš„ç¨€ç–é©±åŠ¨ä»£ç†ç©ºé—´ä»¥åŠé¢å‘è¾“å‡ºçš„logitç ”ç©¶ã€‚è¿™ä½¿å¾—æ›´æ·±å…¥çš„ä»¥è¾“å…¥ä¸ºä¸­å¿ƒçš„ç‰¹å¾æå–æˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶æ”¯æŒé¢å‘è¾“å‡ºçš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆå¦‚ä¸€åœ°æå–è·¨LLMã€åµŒå…¥å¤§å°å’Œè®¾ç½®çš„æœ‰æ„ä¹‰çš„æ¦‚å¿µï¼Œæ­ç¤ºäº†é¢å‘ç±»æ¯”çš„æ¨ç†å’Œä»¥QAä¸ºä¸­å¿ƒçš„æ–‡æœ¬ç”Ÿæˆä¸­æ¦‚å¿µé©±åŠ¨çš„æ¨¡å¼ã€‚é€šè¿‡æ”¯æŒè”åˆè¾“å…¥è¾“å‡ºåˆ†æï¼Œè¿™é¡¹å·¥ä½œæé«˜äº†å¯¹ç¥ç»è¡¨å¾çš„è¯­ä¹‰ç†è§£ï¼ŒåŒæ—¶ç»Ÿä¸€äº†å…ˆå‰æ–¹æ³•çš„äº’è¡¥è§†è§’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯è§£é‡Šæ€§æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚ç›‘ç£æ¢é’ˆå’Œç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ä¾§é‡äºä»è¾“å…¥ä¸­æå–ç‰¹å¾ï¼Œè€ŒåŸºäºlogitçš„æ–¹æ³•åˆ™å…³æ³¨è¾“å‡ºåˆ†å¸ƒã€‚ç„¶è€Œï¼Œè¦çœŸæ­£ç†è§£LLMçš„å†…éƒ¨è¿ä½œæœºåˆ¶ï¼Œéœ€è¦åŒæ—¶è€ƒè™‘è¾“å…¥å’Œè¾“å‡ºï¼Œå¹¶å»ºç«‹å®ƒä»¬ä¹‹é—´çš„è”ç³»ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºå®ƒä»¬åœ¨å®šä¹‰æ½œåœ¨ç‰¹å¾æ—¶å­˜åœ¨çº¦æŸï¼Œæ— æ³•æä¾›å…¨é¢çš„è§†è§’ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å‘é‡ç¬¦å·æ¶æ„ï¼ˆVSAï¼‰å°†ç¬¦å·è¡¨å¾ä¸ç¥ç»æ¢æµ‹ç›¸ç»“åˆï¼Œä»è€Œå®ç°å¯¹LLMå†…éƒ¨è¡¨å¾çš„æ›´æ·±å…¥ç†è§£ã€‚VSAå…è®¸å°†æ¦‚å¿µè¡¨ç¤ºä¸ºé«˜ç»´å‘é‡ï¼ˆè¶…å‘é‡ï¼‰ï¼Œå¹¶ä½¿ç”¨ä»£æ•°è¿ç®—æ¥ç»„åˆå’Œæ“ä½œè¿™äº›æ¦‚å¿µã€‚é€šè¿‡å°†LLMçš„æ¿€æ´»å‘é‡ä¸VSAä¸­çš„ç¬¦å·è¡¨ç¤ºè”ç³»èµ·æ¥ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£LLMå¦‚ä½•å¤„ç†å’Œè¡¨ç¤ºä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¶…ç»´æ¢é’ˆçš„æŠ€æœ¯æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨VSAæ„å»ºæ¦‚å¿µçš„ç¬¦å·è¡¨ç¤ºï¼›2) ä½¿ç”¨ç›‘ç£å­¦ä¹ è®­ç»ƒä¸€ä¸ªæ¢é’ˆï¼Œå°†LLMçš„æ¿€æ´»å‘é‡æ˜ å°„åˆ°VSAç©ºé—´ä¸­çš„æ¦‚å¿µè¡¨ç¤ºï¼›3) ä½¿ç”¨è¶…å‘é‡ä»£æ•°åˆ†ææ¢é’ˆå­¦ä¹ åˆ°çš„æ¦‚å¿µè¡¨ç¤ºï¼Œä¾‹å¦‚ï¼Œé€šè¿‡è®¡ç®—æ¦‚å¿µä¹‹é—´çš„ç›¸ä¼¼åº¦æˆ–ç»„åˆæ¦‚å¿µæ¥æ¨æ–­æ–°çš„æ¦‚å¿µï¼›4) å°†åˆ†æç»“æœä¸LLMçš„è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œæ¯”è¾ƒï¼Œä»¥éªŒè¯æ¢é’ˆçš„æœ‰æ•ˆæ€§å¹¶æ­ç¤ºLLMçš„å†…éƒ¨è¿ä½œæœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¶…ç»´æ¢é’ˆçš„å…³é”®åˆ›æ–°åœ¨äºå®ƒå°†ç¬¦å·è¡¨å¾ä¸ç¥ç»æ¢æµ‹ç›¸ç»“åˆï¼Œä»è€Œå¼¥åˆäº†è¾“å…¥å¯¼å‘å’Œè¾“å‡ºå¯¼å‘çš„å¯è§£é‡Šæ€§æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£æ¢é’ˆç›¸æ¯”ï¼Œè¶…ç»´æ¢é’ˆä½¿ç”¨VSAä½œä¸ºä»£ç†ç©ºé—´ï¼Œå…è®¸æ›´çµæ´»å’Œå¯è§£é‡Šçš„ç‰¹å¾å®šä¹‰ã€‚ä¸SAEç›¸æ¯”ï¼Œè¶…ç»´æ¢é’ˆç›´æ¥å­¦ä¹ æ¦‚å¿µè¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¾èµ–äºç¨€ç–æ€§çº¦æŸã€‚

**å…³é”®è®¾è®¡**ï¼šè¶…ç»´æ¢é’ˆçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é«˜ç»´éšæœºå‘é‡æ¥è¡¨ç¤ºåŸå­æ¦‚å¿µï¼›2) ä½¿ç”¨å¾ªç¯å·ç§¯æ¥ç»„åˆæ¦‚å¿µï¼Œå½¢æˆæ›´å¤æ‚çš„æ¦‚å¿µè¡¨ç¤ºï¼›3) ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¡¡é‡æ¦‚å¿µä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼›4) ä½¿ç”¨çº¿æ€§å›å½’ä½œä¸ºæ¢é’ˆçš„æ¨¡å‹ï¼Œå°†LLMçš„æ¿€æ´»å‘é‡æ˜ å°„åˆ°VSAç©ºé—´ä¸­çš„æ¦‚å¿µè¡¨ç¤ºï¼›5) ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¢é’ˆï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹çš„æ¦‚å¿µè¡¨ç¤ºä¸çœŸå®çš„ç¬¦å·è¡¨ç¤ºä¹‹é—´çš„å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¶…ç»´æ¢é’ˆèƒ½å¤Ÿæœ‰æ•ˆåœ°æå–LLMä¸­çš„æ¦‚å¿µï¼Œå¹¶åœ¨ä¸åŒçš„LLMæ¶æ„ã€åµŒå…¥å¤§å°å’Œè®¾ç½®ä¸‹ä¿æŒä¸€è‡´æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ç±»æ¯”æ¨ç†ä»»åŠ¡ä¸­ï¼Œè¶…ç»´æ¢é’ˆèƒ½å¤Ÿæ­ç¤ºLLMå¦‚ä½•ä½¿ç”¨æ¦‚å¿µæ¥æ¨æ–­æ–°çš„å…³ç³»ã€‚åœ¨QAä»»åŠ¡ä¸­ï¼Œè¶…ç»´æ¢é’ˆèƒ½å¤Ÿè¯†åˆ«LLMåœ¨å›ç­”é—®é¢˜æ—¶æ‰€å…³æ³¨çš„å…³é”®æ¦‚å¿µã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡ç†è§£æ¨¡å‹å¦‚ä½•è¡¨ç¤ºå’Œå¤„ç†çŸ¥è¯†ï¼Œå¯ä»¥æ›´å¥½åœ°æ§åˆ¶æ¨¡å‹çš„ç”Ÿæˆè¡Œä¸ºï¼Œé¿å…ç”Ÿæˆä¸å‡†ç¡®æˆ–æœ‰å®³çš„å†…å®¹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè¯Šæ–­æ¨¡å‹çš„ç¼ºé™·ï¼Œä¾‹å¦‚ï¼Œè¯†åˆ«æ¨¡å‹åœ¨å“ªäº›æ¦‚å¿µä¸Šå­˜åœ¨ç†è§£åå·®ï¼Œä»è€Œæœ‰é’ˆå¯¹æ€§åœ°æ”¹è¿›æ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods either focus on input-oriented feature extraction, such as supervised probes and Sparse Autoencoders (SAEs), or on output distribution inspection, such as logit-oriented approaches. A full understanding of LLM vector spaces, however, requires integrating both perspectives, something existing approaches struggle with due to constraints on latent feature definitions. We introduce the Hyperdimensional Probe, a hybrid supervised probe that combines symbolic representations with neural probing. Leveraging Vector Symbolic Architectures (VSAs) and hypervector algebra, it unifies prior methods: the top-down interpretability of supervised probes, SAE's sparsity-driven proxy space, and output-oriented logit investigation. This allows deeper input-focused feature extraction while supporting output-oriented investigation. Our experiments show that our method consistently extracts meaningful concepts across LLMs, embedding sizes, and setups, uncovering concept-driven patterns in analogy-oriented inference and QA-focused text generation. By supporting joint input-output analysis, this work advances semantic understanding of neural representations while unifying the complementary perspectives of prior methods.

