---
layout: default
title: Quantifier Scope Interpretation in Language Learners and LLMs
---

# Quantifier Scope Interpretation in Language Learners and LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10860" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10860v1</a>
  <a href="https://arxiv.org/pdf/2509.10860.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10860v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10860v1', 'Quantifier Scope Interpretation in Language Learners and LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaohua Fang, Yue Li, Yan Cong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡è¯è¾–åŸŸè§£é‡Šä¸Šä¸äººç±»å­˜åœ¨ç›¸ä¼¼æ€§ï¼Œä½†å—é¢„è®­ç»ƒæ•°æ®å½±å“ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡è¯è¾–åŸŸè§£é‡Š` `å¤§å‹è¯­è¨€æ¨¡å‹` `è·¨è¯­è¨€ç ”ç©¶` `äººç±»ç›¸ä¼¼æ€§` `è¯­ä¹‰ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šé‡è¯è¯­å¥çš„æ­§ä¹‰æ€§ç»™è¯­è¨€å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†å¸¦æ¥æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å‡†ç¡®æ¨¡æ‹Ÿäººç±»çš„ç†è§£ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡æ¦‚ç‡è¯„ä¼°å’Œäººç±»ç›¸ä¼¼æ€§è¯„åˆ†ï¼Œåˆ†æLLMsåœ¨è‹±æ±‰ä¸¤ç§è¯­è¨€ä¸­é‡è¯è¾–åŸŸè§£é‡Šçš„è¡¨ç°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMsåœ¨è¡¨å±‚è¾–åŸŸè§£é‡Šä¸Šä¸äººç±»ç›¸ä¼¼ï¼Œä½†é€†è¾–åŸŸåå¥½å—é¢„è®­ç»ƒæ•°æ®å½±å“ï¼Œå­˜åœ¨æ¨¡å‹å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é‡‡ç”¨è·¨è¯­è¨€çš„æ–¹æ³•ï¼Œç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•å¤„ç†è‹±è¯­å’Œæ±‰è¯­ä¸­çš„é‡è¯è¾–åŸŸè§£é‡Šï¼Œä½¿ç”¨æ¦‚ç‡æ¥è¯„ä¼°è§£é‡Šçš„å¯èƒ½æ€§ã€‚ä½¿ç”¨äººç±»ç›¸ä¼¼æ€§ï¼ˆHSï¼‰åˆ†æ•°æ¥é‡åŒ–LLMsåœ¨ä¸åŒè¯­è¨€ç¾¤ä½“ä¸­æ¨¡æ‹Ÿäººç±»è¡¨ç°çš„ç¨‹åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°LLMså€¾å‘äºè¡¨å±‚è¾–åŸŸè§£é‡Šï¼Œè¿™ä¸äººç±»çš„å€¾å‘ä¸€è‡´ï¼Œè€Œåªæœ‰ä¸€äº›LLMsåœ¨é€†è¾–åŸŸåå¥½æ–¹é¢åŒºåˆ†è‹±è¯­å’Œæ±‰è¯­ï¼Œåæ˜ äº†ä¸äººç±»ç›¸ä¼¼çš„æ¨¡å¼ã€‚HSåˆ†æ•°çªå‡ºäº†LLMsåœ¨é€¼è¿‘äººç±»è¡Œä¸ºæ–¹é¢çš„å·®å¼‚ï¼Œä½†å®ƒä»¬ä¸äººç±»ä¿æŒä¸€è‡´çš„æ€»ä½“æ½œåŠ›æ˜¯å€¼å¾—æ³¨æ„çš„ã€‚æ¨¡å‹æ¶æ„ã€è§„æ¨¡ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®è¯­è¨€èƒŒæ™¯ï¼Œæ˜¾è‘—å½±å“LLMsé€¼è¿‘äººç±»é‡è¯è¾–åŸŸè§£é‡Šçš„ç¨‹åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†åŒ…å«å¤šä¸ªé‡è¯çš„å¥å­æ—¶ï¼Œå¦‚ä½•è¿›è¡Œè¾–åŸŸè§£é‡Šã€‚è¿™ç±»å¥å­é€šå¸¸å…·æœ‰æ­§ä¹‰æ€§ï¼Œä¸åŒçš„è¾–åŸŸè§£é‡Šä¼šäº§ç”Ÿä¸åŒçš„è¯­ä¹‰ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä¼ ç»Ÿçš„åŸºäºè§„åˆ™æˆ–ç»Ÿè®¡æ¨¡å‹çš„è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ï¼Œéš¾ä»¥å‡†ç¡®æ¨¡æ‹Ÿäººç±»åœ¨ä¸åŒè¯­è¨€ä¸­çš„è¾–åŸŸè§£é‡Šåå¥½ï¼Œå°¤å…¶æ˜¯åœ¨è·¨è¯­è¨€çš„åœºæ™¯ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMsçš„æ¦‚ç‡è¾“å‡ºæ¥è¯„ä¼°ä¸åŒè¾–åŸŸè§£é‡Šçš„å¯èƒ½æ€§ï¼Œå¹¶ä½¿ç”¨äººç±»ç›¸ä¼¼æ€§ï¼ˆHSï¼‰åˆ†æ•°æ¥é‡åŒ–LLMsçš„è§£é‡Šä¸äººç±»è§£é‡Šçš„ç›¸ä¼¼ç¨‹åº¦ã€‚é€šè¿‡æ¯”è¾ƒLLMsåœ¨è‹±è¯­å’Œæ±‰è¯­ä¸­çš„è¡¨ç°ï¼Œç ”ç©¶å…¶è·¨è¯­è¨€çš„è¾–åŸŸè§£é‡Šèƒ½åŠ›ï¼Œå¹¶åˆ†ææ¨¡å‹æ¶æ„ã€è§„æ¨¡å’Œé¢„è®­ç»ƒæ•°æ®å¯¹ç»“æœçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) æ„å»ºåŒ…å«å¤šé‡è¯æ­§ä¹‰å¥çš„è‹±è¯­å’Œæ±‰è¯­æµ‹è¯•é›†ï¼›2) ä½¿ç”¨ä¸åŒçš„LLMsï¼ˆä¾‹å¦‚ï¼Œä¸åŒæ¶æ„ã€è§„æ¨¡å’Œé¢„è®­ç»ƒæ•°æ®çš„æ¨¡å‹ï¼‰å¯¹æµ‹è¯•é›†ä¸­çš„å¥å­è¿›è¡Œå¤„ç†ï¼Œå¹¶è®°å½•LLMså¯¹ä¸åŒè¾–åŸŸè§£é‡Šçš„æ¦‚ç‡è¾“å‡ºï¼›3) è®¡ç®—LLMsçš„è¾–åŸŸè§£é‡Šåå¥½ï¼Œä¾‹å¦‚ï¼Œè¡¨å±‚è¾–åŸŸå’Œé€†è¾–åŸŸçš„åå¥½ï¼›4) ä½¿ç”¨äººç±»ç›¸ä¼¼æ€§ï¼ˆHSï¼‰åˆ†æ•°æ¥é‡åŒ–LLMsçš„è§£é‡Šä¸äººç±»è§£é‡Šçš„ç›¸ä¼¼ç¨‹åº¦ï¼›5) åˆ†ææ¨¡å‹æ¶æ„ã€è§„æ¨¡å’Œé¢„è®­ç»ƒæ•°æ®å¯¹LLMsè¾–åŸŸè§£é‡Šèƒ½åŠ›çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) é‡‡ç”¨äº†ä¸€ç§åŸºäºæ¦‚ç‡çš„é‡åŒ–æ–¹æ³•æ¥è¯„ä¼°LLMsçš„è¾–åŸŸè§£é‡Šèƒ½åŠ›ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¯¹è¾–åŸŸè§£é‡Šçš„ç¡¬æ€§åˆ¤æ–­ï¼›2) ä½¿ç”¨äººç±»ç›¸ä¼¼æ€§ï¼ˆHSï¼‰åˆ†æ•°æ¥é‡åŒ–LLMsçš„è§£é‡Šä¸äººç±»è§£é‡Šçš„ç›¸ä¼¼ç¨‹åº¦ï¼Œæä¾›äº†ä¸€ç§æ›´ç»†ç²’åº¦çš„è¯„ä¼°æŒ‡æ ‡ï¼›3) é€šè¿‡è·¨è¯­è¨€çš„æ¯”è¾ƒï¼Œæ­ç¤ºäº†LLMsåœ¨ä¸åŒè¯­è¨€ä¸­çš„è¾–åŸŸè§£é‡Šå·®å¼‚ï¼Œå¹¶åˆ†æäº†æ¨¡å‹æ¶æ„ã€è§„æ¨¡å’Œé¢„è®­ç»ƒæ•°æ®å¯¹ç»“æœçš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç²¾å¿ƒè®¾è®¡çš„åŒ…å«å¤šé‡è¯æ­§ä¹‰å¥çš„è‹±è¯­å’Œæ±‰è¯­æµ‹è¯•é›†ï¼Œç¡®ä¿æµ‹è¯•çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ï¼›2) é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„LLMsï¼Œä¾‹å¦‚ï¼Œä¸åŒæ¶æ„ï¼ˆTransformerã€RNNç­‰ï¼‰ã€è§„æ¨¡ï¼ˆå‚æ•°é‡ï¼‰å’Œé¢„è®­ç»ƒæ•°æ®ï¼ˆè‹±è¯­ã€æ±‰è¯­ã€å¤šè¯­è¨€ï¼‰çš„æ¨¡å‹ï¼›3) ä½¿ç”¨åˆé€‚çš„æ¦‚ç‡è®¡ç®—æ–¹æ³•æ¥è¯„ä¼°LLMså¯¹ä¸åŒè¾–åŸŸè§£é‡Šçš„åå¥½ï¼›4) ä½¿ç”¨æ ‡å‡†åŒ–çš„äººç±»ç›¸ä¼¼æ€§ï¼ˆHSï¼‰åˆ†æ•°æ¥é‡åŒ–LLMsçš„è§£é‡Šä¸äººç±»è§£é‡Šçš„ç›¸ä¼¼ç¨‹åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°LLMså€¾å‘äºè¡¨å±‚è¾–åŸŸè§£é‡Šï¼Œä¸äººç±»çš„å€¾å‘ä¸€è‡´ã€‚éƒ¨åˆ†LLMsåœ¨é€†è¾–åŸŸåå¥½æ–¹é¢åŒºåˆ†è‹±è¯­å’Œæ±‰è¯­ï¼Œåæ˜ äº†ä¸äººç±»ç›¸ä¼¼çš„æ¨¡å¼ã€‚äººç±»ç›¸ä¼¼æ€§ï¼ˆHSï¼‰åˆ†æ•°æ˜¾ç¤ºï¼ŒLLMsåœ¨é€¼è¿‘äººç±»è¡Œä¸ºæ–¹é¢å­˜åœ¨å·®å¼‚ï¼Œä½†æ€»ä½“æ½œåŠ›æ˜¾è‘—ã€‚æ¨¡å‹æ¶æ„ã€è§„æ¨¡å’Œé¢„è®­ç»ƒæ•°æ®è¯­è¨€èƒŒæ™¯æ˜¾è‘—å½±å“LLMsé€¼è¿‘äººç±»é‡è¯è¾–åŸŸè§£é‡Šçš„ç¨‹åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ­§ä¹‰è¯­å¥æ—¶ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºæ”¹è¿›æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·çš„æ„å›¾ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£LLMsçš„è¯­è¨€å­¦ä¹ æœºåˆ¶ï¼Œå¹¶ä¸ºLLMsçš„è®¾è®¡å’Œè®­ç»ƒæä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sentences with multiple quantifiers often lead to interpretive ambiguities, which can vary across languages. This study adopts a cross-linguistic approach to examine how large language models (LLMs) handle quantifier scope interpretation in English and Chinese, using probabilities to assess interpretive likelihood. Human similarity (HS) scores were used to quantify the extent to which LLMs emulate human performance across language groups. Results reveal that most LLMs prefer the surface scope interpretations, aligning with human tendencies, while only some differentiate between English and Chinese in the inverse scope preferences, reflecting human-similar patterns. HS scores highlight variability in LLMs' approximation of human behavior, but their overall potential to align with humans is notable. Differences in model architecture, scale, and particularly models' pre-training data language background, significantly influence how closely LLMs approximate human quantifier scope interpretations.

