---
layout: default
title: Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction
---

# Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10798" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.10798v1</a>
  <a href="https://arxiv.org/pdf/2509.10798.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10798v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10798v1', 'Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yijun Liu, Yixuan Wang, Yuzhuang Xu, Shiyu Ji, Yang Xu, Qingfu Zhu, Wanxiang Che

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-13

**Â§áÊ≥®**: preprint

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Judge QÔºöÈÄöËøáÂèØËÆ≠ÁªÉÊü•ËØ¢‰ºòÂåñKVÁºìÂ≠òÊ∑òÊ±∞‰∏≠ÁöÑ‰ø°ÊÅØ‰øùÁïô**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `KVÁºìÂ≠òÊ∑òÊ±∞` `ÈïøÂ∫èÂàóÂª∫Ê®°` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `ÂÖ®Â±Ä‰ø°ÊÅØ` `ËΩØToken`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâKVÁºìÂ≠òÊ∑òÊ±∞ÊñπÊ≥ïËøáÂ∫¶ÂÖ≥Ê≥®Â±ÄÈÉ®‰ø°ÊÅØÔºåÂøΩÁï•ÂÖ®Â±Ä‰ø°ÊÅØÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. Judge QÈÄöËøáÂºïÂÖ•ÂèØËÆ≠ÁªÉÁöÑËΩØtokenÂàóË°®Ôºå‰ΩøÊü•ËØ¢ËÉΩÂ§üÊçïËé∑ÂÖ®Â±Ä‰ø°ÊÅØÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞KVÁºìÂ≠òÁöÑÈáçË¶ÅÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåJudge QÂú®LongBenchÂíåRULERÁ≠âÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÊÄßËÉΩ‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºå‰∏îËÆ≠ÁªÉÂºÄÈîÄÊûÅÂ∞è„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂà©Áî®ÈîÆÂÄºÔºàKVÔºâÁºìÂ≠òÊù•Â≠òÂÇ®Â∫èÂàóÂ§ÑÁêÜËøáÁ®ã‰∏≠ÁöÑÂéÜÂè≤‰ø°ÊÅØ„ÄÇKVÁºìÂ≠òÁöÑÂ§ßÂ∞èÈöèÁùÄÂ∫èÂàóÈïøÂ∫¶ÁöÑÂ¢ûÂä†ËÄåÁ∫øÊÄßÂ¢ûÈïøÔºå‰∏•ÈáçÂΩ±ÂìçÂÜÖÂ≠ò‰ΩøÁî®ÂíåËß£Á†ÅÊïàÁéá„ÄÇÁé∞ÊúâÁöÑKVÁºìÂ≠òÊ∑òÊ±∞ÊñπÊ≥ïÈÄöÂ∏∏‰ΩøÁî®È¢ÑÂ°´ÂÖÖÈò∂ÊÆµÁöÑÊúÄÂêé‰∏Ä‰∏™Á™óÂè£‰Ωú‰∏∫Êü•ËØ¢Ôºå‰ª•ËÆ°ÁÆóKVÈáçË¶ÅÊÄßÂæóÂàÜËøõË°åÊ∑òÊ±∞„ÄÇËôΩÁÑ∂ËøôÁßçÊñπÊ°àÊòì‰∫éÂÆûÁé∞Ôºå‰ΩÜÂÆÉÂæÄÂæÄËøáÂ∫¶ÂÖ≥Ê≥®Â±ÄÈÉ®‰ø°ÊÅØÔºåÂèØËÉΩÂØºËá¥ÂøΩÁï•ÊàñÈÅóÊºèÂÖ≥ÈîÆÁöÑÂÖ®Â±Ä‰ø°ÊÅØ„ÄÇ‰∏∫‰∫ÜÁºìËß£Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÊñπÊ≥ïJudge QÔºåËØ•ÊñπÊ≥ïÁªìÂêà‰∫Ü‰∏Ä‰∏™ËΩØtokenÂàóË°®„ÄÇËØ•ÊñπÊ≥ï‰ªÖ‰ª•ËæÉ‰ΩéÁöÑËÆ≠ÁªÉÊàêÊú¨Ë∞ÉÊï¥Ê®°ÂûãÁöÑÂµåÂÖ•Â±Ç„ÄÇÈÄöËøáÂ∞ÜËΩØtokenÂàóË°®ËøûÊé•Âà∞ËæìÂÖ•Â∫èÂàóÁöÑÊú´Â∞æÔºåÊàë‰ª¨ËÆ≠ÁªÉËøô‰∫õtokenÁöÑÊ≥®ÊÑèÂäõÂõæ‰∏éÂéüÂßãËæìÂÖ•Â∫èÂàóÁöÑÊ≥®ÊÑèÂäõÂõæÂØπÈΩêÔºå‰ΩøÂÖ∂‰∏éÂÆûÈôÖËß£Á†ÅtokenÁöÑÊ≥®ÊÑèÂäõÂõæÂØπÈΩê„ÄÇËøôÊ†∑Ôºå‰∏éËΩØtokenÂØπÂ∫îÁöÑÊü•ËØ¢ÂèØ‰ª•ÊúâÊïàÂú∞ÊçïËé∑ÂÖ®Â±Ä‰ø°ÊÅØÔºåÂπ∂Êõ¥Â•ΩÂú∞ËØÑ‰º∞KVÁºìÂ≠ò‰∏≠ÈîÆÂíåÂÄºÁöÑÈáçË¶ÅÊÄßÔºå‰ªéËÄåÂú®KVÁºìÂ≠òË¢´Ê∑òÊ±∞Êó∂‰øùÊåÅËß£Á†ÅË¥®Èáè„ÄÇÂú®Áõ∏ÂêåÁöÑÊ∑òÊ±∞È¢ÑÁÆó‰∏ãÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÊØîÁé∞ÊúâÁöÑÊ∑òÊ±∞ÊñπÊ≥ïË°®Áé∞Âá∫Êõ¥Â∞ëÁöÑÊÄßËÉΩ‰∏ãÈôç„ÄÇÊàë‰ª¨ÈÄöËøáÂú®Llama-3.1-8B-InstructÂíåMistral-7B-Instruct-v0.3Á≠âÊ®°Âûã‰∏äËøõË°åÁöÑÂÆûÈ™åÈ™åËØÅ‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÔºå‰ΩøÁî®‰∫ÜÂåÖÊã¨LongBench„ÄÅRULERÂíåNeedle-in-a-HaystackÂú®ÂÜÖÁöÑÂü∫ÂáÜ„ÄÇÁªìÊûúË°®ÊòéÔºåLongBench‰∏äÁöÑÊîπËøõÁ∫¶‰∏∫1‰∏™ÁÇπÔºåRULER‰∏äÁöÑÊîπËøõË∂ÖËøá3‰∏™ÁÇπ„ÄÇËøôÁßçÊèêÂá∫ÁöÑÊñπÊ≥ïÂèØ‰ª•Êó†ÁºùÂú∞ÈõÜÊàêÂà∞Áé∞ÊúâÁöÑÂºÄÊ∫êÊ®°Âûã‰∏≠ÔºåÂè™ÈúÄÊûÅÂ∞ëÁöÑËÆ≠ÁªÉÂºÄÈîÄÔºå‰ªéËÄåÊèêÈ´òKVÁºìÂ≠òÊ∑òÊ±∞Âú∫ÊôØ‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ÔºåKVÁºìÂ≠ò‰ºöÁ∫øÊÄßÂ¢ûÈïøÔºåÂØºËá¥ÂÜÖÂ≠òÂç†Áî®ËøáÈ´òÂíåËß£Á†ÅÊïàÁéáÈôç‰Ωé„ÄÇÁé∞ÊúâÁöÑKVÁºìÂ≠òÊ∑òÊ±∞Á≠ñÁï•Ôºå‰æãÂ¶Ç‰ΩøÁî®È¢ÑÂ°´ÂÖÖÈò∂ÊÆµÁöÑÊúÄÂêéÁ™óÂè£‰Ωú‰∏∫Êü•ËØ¢ÔºåÂæÄÂæÄÂè™ÂÖ≥Ê≥®Â±ÄÈÉ®‰ø°ÊÅØÔºåÊó†Ê≥ïÊúâÊïàËØÜÂà´Âíå‰øùÁïôÂÖ®Â±ÄÈáçË¶Å‰ø°ÊÅØÔºå‰ªéËÄåÂΩ±ÂìçÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöJudge QÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËÆ≠ÁªÉ‰∏ÄÁªÑËΩØtokenÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊçïËé∑ËæìÂÖ•Â∫èÂàóÁöÑÂÖ®Â±Ä‰ø°ÊÅØ„ÄÇËøô‰∫õËΩØtokenË¢´Ê∑ªÂä†Âà∞ËæìÂÖ•Â∫èÂàóÁöÑÊú´Â∞æÔºåÂπ∂ÈÄöËøáËÆ≠ÁªÉÔºå‰ΩøÂÖ∂ÂØπÂéüÂßãËæìÂÖ•Â∫èÂàóÁöÑÊ≥®ÊÑèÂäõÂàÜÂ∏É‰∏éÂÆûÈôÖËß£Á†ÅtokenÁöÑÊ≥®ÊÑèÂäõÂàÜÂ∏ÉÂØπÈΩê„ÄÇËøôÊ†∑ÔºåËΩØtokenÂØπÂ∫îÁöÑÊü•ËØ¢Â∞±ËÉΩÊõ¥ÂÖ®Èù¢Âú∞ËØÑ‰º∞KVÁºìÂ≠ò‰∏≠ÈîÆÂíåÂÄºÁöÑÈáçË¶ÅÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöJudge QÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÊ≠•È™§Ôºö1) Âú®ÂéüÂßãËæìÂÖ•Â∫èÂàóÂêéÊ∑ªÂä†‰∏ÄÁªÑÂèØËÆ≠ÁªÉÁöÑËΩØtoken„ÄÇ2) ‰ΩøÁî®ËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜÂåÖÂê´ËΩØtokenÁöÑÂ∫èÂàó„ÄÇ3) ËÆ°ÁÆóËΩØtokenÂØπÂéüÂßãËæìÂÖ•Â∫èÂàóÁöÑÊ≥®ÊÑèÂäõÂõæ„ÄÇ4) ‰ΩøÁî®ÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩøËΩØtokenÁöÑÊ≥®ÊÑèÂäõÂõæ‰∏éÂÆûÈôÖËß£Á†ÅtokenÁöÑÊ≥®ÊÑèÂäõÂõæÂØπÈΩê„ÄÇ5) ‰ΩøÁî®ËÆ≠ÁªÉÂ•ΩÁöÑËΩØtoken‰Ωú‰∏∫Êü•ËØ¢ÔºåËØÑ‰º∞KVÁºìÂ≠ò‰∏≠ÈîÆÂíåÂÄºÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂ËøõË°åÊ∑òÊ±∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöJudge QÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂèØËÆ≠ÁªÉÁöÑËΩØtokenÂàóË°®ÔºåÂπ∂‰ΩøÁî®Ê≥®ÊÑèÂäõÂØπÈΩêÁöÑÊñπÂºèÔºå‰ΩøËøô‰∫õËΩØtokenËÉΩÂ§üÊçïËé∑ÂÖ®Â±Ä‰ø°ÊÅØ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåJudge Q‰∏çÂÜçÂ±ÄÈôê‰∫éÂ±ÄÈÉ®‰ø°ÊÅØÔºåËÄåÊòØËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞ËØÑ‰º∞KVÁºìÂ≠òÁöÑÈáçË¶ÅÊÄß„ÄÇÊ≠§Â§ñÔºåJudge QÂè™ÈúÄË¶ÅË∞ÉÊï¥Ê®°ÂûãÁöÑÂµåÂÖ•Â±ÇÔºåËÆ≠ÁªÉÊàêÊú¨ÈùûÂ∏∏‰Ωé„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöJudge QÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ËΩØtokenÁöÑÊï∞ÈáèÔºöÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ÂíåÊ®°ÂûãËøõË°åË∞ÉÊï¥„ÄÇ2) Ê≥®ÊÑèÂäõÂØπÈΩêÁöÑÊçüÂ§±ÂáΩÊï∞ÔºöÂèØ‰ª•‰ΩøÁî®KLÊï£Â∫¶Êàñ‰∫§ÂèâÁÜµÁ≠âÊçüÂ§±ÂáΩÊï∞ÔºåË°°ÈáèËΩØtokenÁöÑÊ≥®ÊÑèÂäõÂõæ‰∏éÂÆûÈôÖËß£Á†ÅtokenÁöÑÊ≥®ÊÑèÂäõÂõæ‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇ3) ËÆ≠ÁªÉÁ≠ñÁï•ÔºöÂèØ‰ª•‰ΩøÁî®AdamÁ≠â‰ºòÂåñÂô®ÔºåÂπ∂ËÆæÁΩÆÂêàÈÄÇÁöÑÂ≠¶‰π†ÁéáÂíåËÆ≠ÁªÉËΩÆÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåJudge QÂú®LongBench‰∏äÊèêÂçá‰∫ÜÁ∫¶1‰∏™ÁÇπÔºåÂú®RULER‰∏äÊèêÂçá‰∫ÜË∂ÖËøá3‰∏™ÁÇπ„ÄÇËøô‰∫õÊèêÂçáÊòØÂú®Áõ∏ÂêåÁöÑÊ∑òÊ±∞È¢ÑÁÆó‰∏ãÂÆûÁé∞ÁöÑÔºåË°®ÊòéJudge QËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®ÊúâÈôêÁöÑKVÁºìÂ≠òËµÑÊ∫ê„ÄÇÊ≠§Â§ñÔºåJudge QÁöÑËÆ≠ÁªÉÂºÄÈîÄÊûÅÂ∞èÔºåÂèØ‰ª•ËΩªÊùæÈõÜÊàêÂà∞Áé∞ÊúâÁöÑÂºÄÊ∫êÊ®°Âûã‰∏≠„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Judge QÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂ§ÑÁêÜÈïøÂ∫èÂàóÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰æãÂ¶ÇÊñáÊ°£ÊëòË¶Å„ÄÅÊú∫Âô®ÁøªËØë„ÄÅÂØπËØùÁîüÊàêÁ≠â„ÄÇÈÄöËøá‰ºòÂåñKVÁºìÂ≠òÊ∑òÊ±∞Á≠ñÁï•ÔºåJudge QÂèØ‰ª•ÊòæËëóÈôç‰ΩéÂÜÖÂ≠òÂç†Áî®ÔºåÊèêÈ´òËß£Á†ÅÊïàÁéáÔºå‰ªéËÄå‰ΩøËøô‰∫õÊ®°ÂûãËÉΩÂ§üÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ËøêË°åÔºåÂπ∂Â§ÑÁêÜÊõ¥ÈïøÁöÑÂ∫èÂàó„ÄÇËØ•ÊñπÊ≥ïÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÂíåÂÆûÈôÖ‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) utilize key-value (KV) cache to store historical information during sequence processing. The size of KV cache grows linearly as the length of the sequence extends, which seriously affects memory usage and decoding efficiency. Current methods for KV cache eviction typically utilize the last window from the pre-filling phase as queries to compute the KV importance scores for eviction. Although this scheme is simple to implement, it tends to overly focus on local information, potentially leading to the neglect or omission of crucial global information. To mitigate this issue, we propose Judge Q, a novel training method which incorporates a soft token list. This method only tunes the model's embedding layer at a low training cost. By concatenating the soft token list at the end of the input sequence, we train these tokens' attention map to the original input sequence to align with that of the actual decoded tokens. In this way, the queries corresponding to the soft tokens can effectively capture global information and better evaluate the importance of the keys and values within the KV cache, thus maintaining decoding quality when KV cache is evicted. Under the same eviction budget, our method exhibits less performance degradation compared to existing eviction approaches. We validate our approach through experiments conducted on models such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an improvement of approximately 1 point on the LongBench and over 3 points on RULER. This proposed methodology can be seamlessly integrated into existing open-source models with minimal training overhead, thereby enhancing performance in KV cache eviction scenarios.

