---
layout: default
title: RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning
---

# RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00974" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00974v5</a>
  <a href="https://arxiv.org/pdf/2509.00974.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00974v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00974v5', 'RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chia-Hsuan Hsu, Jun-En Ding, Hsin-Ling Hsu, Chih-Ho Hsu, Li-Hung Yao, Chun-Chieh Liao, Feng Liu, Fang-Ming Hung

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31 (æ›´æ–°: 2025-12-07)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRPROä»¥è§£å†³åŒ»ç–—é—®ç­”ä¸­çš„æ¨ç†å‡†ç¡®æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»ç–—é—®ç­”` `æ¨ç†ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ ` `ä¸´åºŠå†³ç­–` `å¤§å‹è¯­è¨€æ¨¡å‹` `åå¥½é©±åŠ¨` `ç»„æ’åä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é—®ç­”ä¸­ç”Ÿæˆçš„æ¨ç†é“¾ç¼ºä¹äº‹å®å‡†ç¡®æ€§å’Œä¸´åºŠå¯é æ€§ï¼Œå½±å“äº†å…¶åº”ç”¨æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºäº†RPROæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸åå¥½é©±åŠ¨çš„æ¨ç†ä¼˜åŒ–ï¼Œæå‡äº†ä¸´åºŠæ¨ç†é“¾çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRPROåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶æ˜¯æˆ‘ä»¬çš„2Bå‚æ•°æ¨¡å‹è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŒ»ç–—é—®ç­”éœ€è¦å°†é¢†åŸŸçŸ¥è¯†ä¸é€»è¾‘æ¨ç†ç›¸ç»“åˆçš„é«˜çº§æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å¸¸ç”Ÿæˆç¼ºä¹äº‹å®å‡†ç¡®æ€§å’Œä¸´åºŠå¯é æ€§çš„æ¨ç†é“¾ã€‚æœ¬æ–‡æå‡ºäº†æ’ååå¥½å¼ºåŒ–ä¼˜åŒ–ï¼ˆRPROï¼‰ï¼Œä¸€ä¸ªç»“åˆå¼ºåŒ–å­¦ä¹ ä¸åå¥½é©±åŠ¨æ¨ç†ä¼˜åŒ–çš„æ–°æ¡†æ¶ï¼Œä»¥æå‡ä¸´åºŠæ¨ç†é“¾çš„è¡¨ç°ã€‚RPROé€šè¿‡é‡‡ç”¨ä»»åŠ¡è‡ªé€‚åº”æ¨ç†æ¨¡æ¿å’Œæ¦‚ç‡è¯„ä¼°æœºåˆ¶ï¼Œä½¿æ¨¡å‹è¾“å‡ºä¸æ—¢å®šä¸´åºŠå·¥ä½œæµç¨‹å¯¹é½ï¼ŒåŒæ—¶è‡ªåŠ¨è¯†åˆ«å’Œçº æ­£ä½è´¨é‡æ¨ç†é“¾ã€‚ä¸ä¼ ç»Ÿçš„æˆå¯¹åå¥½æ–¹æ³•ä¸åŒï¼ŒRPROå¼•å…¥äº†åŸºäºBradley-Terryæ¨¡å‹çš„ç»„æ’åä¼˜åŒ–ï¼Œå¹¶ç»“åˆKLæ•£åº¦æ­£åˆ™åŒ–ä»¥å®ç°ç¨³å®šè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨PubMedQAã€MedQA-USMLEå’Œæ¥è‡ªè¿œä¸œçºªå¿µåŒ»é™¢çš„çœŸå®ä¸´åºŠæ•°æ®é›†ä¸Šï¼ŒRPROåœ¨å¼ºåŸºçº¿æ¨¡å‹ä¸Šå®ç°äº†ä¸€è‡´çš„æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„2Bå‚æ•°æ¨¡å‹è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„7B-20Bæ¨¡å‹ï¼ŒåŒ…æ‹¬åŒ»å­¦ä¸“ä¸šåŒ–å˜ä½“ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå°†åå¥½ä¼˜åŒ–ä¸è´¨é‡é©±åŠ¨çš„ä¼˜åŒ–ç›¸ç»“åˆï¼Œä¸ºæ„å»ºæ›´å¯é çš„åŒ»ç–—LLMsæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”ä¸´åºŠåŸºç¡€çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŒ»ç–—é—®ç­”ä¸­æ¨ç†é“¾çš„å‡†ç¡®æ€§å’Œå¯é æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸ç”Ÿæˆä½è´¨é‡çš„æ¨ç†é“¾ï¼Œå¯¼è‡´ä¸´åºŠåº”ç”¨æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRPROæ¡†æ¶é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸åå¥½é©±åŠ¨çš„æ¨ç†ä¼˜åŒ–ï¼Œåˆ©ç”¨ä»»åŠ¡è‡ªé€‚åº”æ¨ç†æ¨¡æ¿å’Œæ¦‚ç‡è¯„ä¼°æœºåˆ¶ï¼Œæå‡æ¨ç†é“¾çš„è´¨é‡ä¸å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRPROçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä»»åŠ¡è‡ªé€‚åº”æ¨ç†æ¨¡æ¿ã€æ¦‚ç‡è¯„ä¼°æœºåˆ¶å’Œç»„æ’åä¼˜åŒ–ã€‚é¦–å…ˆï¼Œæ¨¡å‹ç”Ÿæˆæ¨ç†é“¾ï¼Œç„¶åé€šè¿‡è¯„ä¼°æœºåˆ¶å¯¹å…¶è¿›è¡Œè¯„åˆ†ï¼Œæœ€åé€šè¿‡ç»„æ’åä¼˜åŒ–è¿›è¡Œè°ƒæ•´å’Œæ”¹è¿›ã€‚

**å…³é”®åˆ›æ–°**ï¼šRPROçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºBradley-Terryæ¨¡å‹çš„ç»„æ’åä¼˜åŒ–æ–¹æ³•ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„æˆå¯¹åå¥½æ–¹æ³•ï¼ŒåŒæ—¶ç»“åˆKLæ•£åº¦æ­£åˆ™åŒ–ä»¥å®ç°è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä»»åŠ¡è‡ªé€‚åº”çš„æ¨ç†æ¨¡æ¿ï¼ŒæŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†KLæ•£åº¦æ­£åˆ™åŒ–ï¼Œå¹¶é€šè¿‡ç»„æ’åä¼˜åŒ–æ¥æå‡æ¨ç†é“¾çš„æ•´ä½“è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRPROåœ¨PubMedQAå’ŒMedQA-USMLEç­‰æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶æ˜¯æˆ‘ä»¬çš„2Bå‚æ•°æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†7B-20Bçš„æ›´å¤§æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºRPROåœ¨åŒ»ç–—é—®ç­”ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—é—®ç­”ç³»ç»Ÿã€ä¸´åºŠå†³ç­–æ”¯æŒå·¥å…·å’ŒåŒ»å­¦æ•™è‚²ç­‰ã€‚é€šè¿‡æå‡åŒ»ç–—LLMsçš„æ¨ç†å‡†ç¡®æ€§å’Œå¯é æ€§ï¼ŒRPROèƒ½å¤Ÿä¸ºåŒ»ç”Ÿå’Œæ‚£è€…æä¾›æ›´ä¸ºç²¾å‡†çš„ä¿¡æ¯æ”¯æŒï¼Œè¿›è€Œæ”¹å–„åŒ»ç–—æœåŠ¡è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„åŒ»ç–—åœºæ™¯ä¸­æ¨å¹¿åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½åŒ»ç–—çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO distinguishes itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns model outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley--Terry model and incorporates KL-divergence regularization for stable training. Experiments on PubMedQA, MedQA-USMLE, and a real-world clinical dataset from Far Eastern Memorial Hospital (FEMH) demonstrate consistent improvements over strong baselines. Remarkably, our 2B-parameter model outperforms much larger 7B--20B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement provides a scalable and clinically grounded approach to building more reliable medical LLMs.

