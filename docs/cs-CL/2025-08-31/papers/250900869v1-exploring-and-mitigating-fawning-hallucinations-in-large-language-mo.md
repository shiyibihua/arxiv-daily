---
layout: default
title: Exploring and Mitigating Fawning Hallucinations in Large Language Models
---

# Exploring and Mitigating Fawning Hallucinations in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00869" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00869v1</a>
  <a href="https://arxiv.org/pdf/2509.00869.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00869v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00869v1', 'Exploring and Mitigating Fawning Hallucinations in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zixuan Shangguan, Yanjie Dong, Lanjun Wang, Xiaoyi Fan, Victor C. M. Leung, Xiping Hu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåä½œå¯¹æ¯”è§£ç ä»¥ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è°„åªšå¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è°„åªšå¹»è§‰` `å¯¹æ¯”è§£ç ` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¿¡æ¯å‡†ç¡®æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¯¯å¯¼æ€§æç¤ºæ—¶ï¼Œå®¹æ˜“äº§ç”Ÿè°„åªšå¹»è§‰ï¼Œå¯¼è‡´è¾“å‡ºåç¦»çœŸå®ä¿¡æ¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºåä½œå¯¹æ¯”è§£ç ï¼ˆCCDï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”è¯±å¯¼çš„è°„åªšå¹»è§‰ä¸ä¸­æ€§è¾“å…¥çš„è¾“å‡ºåˆ†å¸ƒï¼Œå‡å°‘å¯¹è¯¯å¯¼æ€§ä¿¡æ¯çš„ä¾èµ–ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå®éªŒç»“æœæ˜¾ç¤ºï¼ŒCCDåœ¨å¤šä¸ªä»»åŠ¡ä¸­æœ‰æ•ˆç¼“è§£äº†è°„åªšå¹»è§‰ï¼Œæé«˜äº†ç”Ÿæˆå“åº”çš„äº‹å®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å½“å…¶è¾“å‡ºä¸è¯¯å¯¼æ€§æç¤ºå¯¹é½æ—¶ï¼Œç”Ÿæˆçš„å“åº”å¯èƒ½åç¦»çœŸå®ä¿¡æ¯ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºè°„åªšå¹»è§‰ï¼Œæ¨¡å‹ä¼˜å…ˆè€ƒè™‘ä¸è¾“å…¥éšå«è§‚ç‚¹çš„ä¸€è‡´æ€§ï¼Œè€Œéå‡†ç¡®æ€§ã€‚æœ¬æ–‡åˆ†æäº†è°„åªšå¹»è§‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†ä¸€ç§é’ˆå¯¹è°„åªšå¹»è§‰ç¼“è§£çš„å¯¹æ¯”è§£ç æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œè®¾è®¡äº†ä¸¤ç§èŒƒå¼ä»¥ç”Ÿæˆç›¸åº”çš„è¯¯å¯¼æ€§è¾“å…¥ï¼Œä»è€Œä¸€è‡´æ€§åœ°è¯±å¯¼è°„åªšå¹»è§‰ã€‚é€šè¿‡å¯¹æ¯”è¯±å¯¼å’Œè½¬åŒ–åçš„ä¸­æ€§è¾“å…¥çš„è¾“å‡ºåˆ†å¸ƒåå·®ï¼Œæå‡ºçš„åä½œå¯¹æ¯”è§£ç ï¼ˆCCDï¼‰èƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å‡å°‘å¯¹è¯¯å¯¼æ€§ä¿¡æ¯çš„ä¾èµ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCCDèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£è°„åªšå¹»è§‰ï¼Œæé«˜ç”Ÿæˆå“åº”çš„äº‹å®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹è¯¯å¯¼æ€§æç¤ºæ—¶äº§ç”Ÿçš„è°„åªšå¹»è§‰ç°è±¡ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œç¼“è§£è¿™ç§ç°è±¡ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡å¯¹æ¯”è§£ç æ–¹æ³•ï¼Œåˆ©ç”¨è¯±å¯¼çš„è°„åªšå¹»è§‰ä¸ä¸­æ€§è¾“å…¥ä¹‹é—´çš„è¾“å‡ºåˆ†å¸ƒå·®å¼‚ï¼Œæ¥å‡å°‘æ¨¡å‹å¯¹è¯¯å¯¼æ€§ä¿¡æ¯çš„ä¾èµ–ï¼Œä»è€Œæé«˜ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆç”Ÿæˆç›¸åº”çš„è¯¯å¯¼æ€§è¾“å…¥ä»¥è¯±å¯¼è°„åªšå¹»è§‰ï¼Œå…¶æ¬¡åº”ç”¨åä½œå¯¹æ¯”è§£ç ï¼ˆCCDï¼‰æ–¹æ³•å¯¹æ¯”è¾“å‡ºåˆ†å¸ƒï¼Œè°ƒæ•´ç”Ÿæˆå†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åä½œå¯¹æ¯”è§£ç ï¼ˆCCDï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”ä¸åŒè¾“å…¥çš„è¾“å‡ºåˆ†å¸ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£è°„åªšå¹»è§‰ï¼Œè€Œæ— éœ€è¿›è¡Œé¢å¤–çš„æ¨¡å‹è®­ç»ƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬è¯±å¯¼è¾“å…¥çš„ç”Ÿæˆç­–ç•¥å’Œå¯¹æ¯”æŸå¤±å‡½æ•°çš„è®¾ç½®ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å¹¶è°ƒæ•´è¾“å‡ºï¼Œæå‡ç”Ÿæˆå†…å®¹çš„äº‹å®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåä½œå¯¹æ¯”è§£ç ï¼ˆCCDï¼‰æ–¹æ³•åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ˜¾è‘—é™ä½äº†è°„åªšå¹»è§‰çš„å‘ç”Ÿç‡ï¼Œç”Ÿæˆå“åº”çš„äº‹å®æ€§æé«˜äº†çº¦15%ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒCCDåœ¨å¤„ç†è¯¯å¯¼æ€§æç¤ºæ—¶çš„è¡¨ç°æœ‰äº†æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç”Ÿæˆå’Œå†…å®¹åˆ›ä½œç­‰åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆç¼“è§£è°„åªšå¹»è§‰ï¼Œæå‡ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´å¯ä¿¡çš„ä¿¡æ¯ï¼Œå¢å¼ºäººæœºäº¤äº’çš„è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated exceptional proficiency in language understanding. However, when LLMs align their outputs with deceptive and/or misleading prompts, the generated responses could deviate from the de facto information. Such observations are known as fawning hallucinations, where the model prioritizes alignment with the input's implied perspective over accuracy and truthfulness. In this work, we analyze fawning hallucinations in various natural language processing tasks and tailor the so-termed contrastive decoding method for fawning-hallucination mitigation. Specifically, we design two paradigms to generate corresponding deceptive and/or misleading inputs for the consistent fawning hallucinations induction. Then, we propose the collaborative contrastive decoding (CCD) to handle the fawning hallucinations across different tasks in LLMs. By contrasting the deviation in output distribution between induced and transformed neutral inputs, the proposed CCD can reduce reliance on deceptive and/or misleading information without requiring additional training. Extensive experiments demonstrate that the proposed CCD can effectively mitigate fawning hallucinations and improve the factuality of the generated responses over various tasks.

