---
layout: default
title: CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders
---

# CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00691" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00691v2</a>
  <a href="https://arxiv.org/pdf/2509.00691.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00691v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00691v2', 'CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alex Gulko, Yusen Peng, Sachin Kumar

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31 (æ›´æ–°: 2025-09-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCE-Benchä»¥è§£å†³ç¨€ç–è‡ªç¼–ç å™¨å¯è§£é‡Šæ€§è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `å¯è§£é‡Šæ€§è¯„ä¼°` `å¯¹æ¯”å­¦ä¹ ` `æœºå™¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¨€ç–è‡ªç¼–ç å™¨è¯„ä¼°æ–¹æ³•å¤§å¤šä¾èµ–å¤–éƒ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶ç‹¬ç«‹æ€§å’Œå¯é æ€§ã€‚
2. æœ¬æ–‡æå‡ºCE-Benchï¼Œä¸€ä¸ªåŸºäºå¯¹æ¯”æ•…äº‹å¯¹çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æä¾›è½»é‡ä¸”å¯é çš„ç¨€ç–è‡ªç¼–ç å™¨å¯è§£é‡Šæ€§è¯„ä¼°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCE-Benchåœ¨å¯è§£é‡Šæ€§è¯„ä¼°ä¸Šä¸ç°æœ‰åŸºå‡†é«˜åº¦ä¸€è‡´ï¼Œä¸”æ— éœ€å¤–éƒ¨è¯„åˆ¤ï¼Œæå‡äº†è¯„ä¼°çš„ç‹¬ç«‹æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œç”¨äºæ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¯è§£é‡Šç‰¹å¾ã€‚å°½ç®¡ç°æœ‰å¤šç§è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ï¼Œä½†å¤§å¤šæ•°ä¾èµ–äºå¤–éƒ¨LLMã€‚æœ¬æ–‡æå‡ºäº†CE-Benchï¼Œä¸€ä¸ªæ–°é¢–ä¸”è½»é‡çš„ç¨€ç–è‡ªç¼–ç å™¨å¯¹æ¯”è¯„ä¼°åŸºå‡†ï¼ŒåŸºäºç²¾å¿ƒç­–åˆ’çš„å¯¹æ¯”æ•…äº‹å¯¹æ•°æ®é›†ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ç ”ç©¶ï¼Œä»¥éªŒè¯æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒCE-Benchå¯é åœ°æµ‹é‡ç¨€ç–è‡ªç¼–ç å™¨çš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸ç°æœ‰åŸºå‡†è‰¯å¥½å¯¹é½ï¼Œæ— éœ€å¤–éƒ¨LLMè¯„åˆ¤ï¼Œä¸”ä¸SAEBenchçš„ç»“æœè¾¾åˆ°è¶…è¿‡70%çš„æ–¯çš®å°”æ›¼ç›¸å…³æ€§ã€‚å®˜æ–¹å®ç°å’Œè¯„ä¼°æ•°æ®é›†å·²å¼€æºå¹¶å…¬å¼€å¯ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¨€ç–è‡ªç¼–ç å™¨å¯è§£é‡Šæ€§è¯„ä¼°ä¸­å¯¹å¤–éƒ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºè¯„ä¼°çš„å¯é æ€§å’Œç‹¬ç«‹æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCE-Benché€šè¿‡æ„å»ºä¸€ä¸ªåŸºäºå¯¹æ¯”æ•…äº‹å¯¹çš„æ•°æ®é›†ï¼Œæä¾›äº†ä¸€ç§è½»é‡çº§çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ç‹¬ç«‹äºå¤–éƒ¨æ¨¡å‹è¿›è¡Œå¯è§£é‡Šæ€§è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCE-Benchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€å¯¹æ¯”è¯„ä¼°æ–¹æ³•å’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ„å»ºå¯¹æ¯”æ•…äº‹å¯¹ä»¥å½¢æˆè¯„ä¼°åŸºç¡€ï¼›å…¶æ¬¡ï¼Œè®¾è®¡è¯„ä¼°æ–¹æ³•ä»¥é‡åŒ–å¯è§£é‡Šæ€§ï¼›æœ€åï¼Œè¿›è¡Œç»“æœåˆ†æä»¥éªŒè¯è¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šCE-Benchçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ä¸ä¾èµ–å¤–éƒ¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°æ ‡å‡†ï¼Œä¸”ä¸ç°æœ‰åŸºå‡†ï¼ˆå¦‚SAEBenchï¼‰ç»“æœé«˜åº¦ä¸€è‡´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCE-Benché‡‡ç”¨äº†ç²¾å¿ƒç­–åˆ’çš„å¯¹æ¯”æ•…äº‹å¯¹ï¼Œå¹¶é€šè¿‡æ–¯çš®å°”æ›¼ç›¸å…³æ€§æ¥é‡åŒ–è¯„ä¼°ç»“æœï¼Œç¡®ä¿äº†è¯„ä¼°çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCE-Benchåœ¨å¯è§£é‡Šæ€§è¯„ä¼°ä¸Šä¸SAEBenchçš„ç»“æœè¾¾åˆ°äº†è¶…è¿‡70%çš„æ–¯çš®å°”æ›¼ç›¸å…³æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶è¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚è¯¥æ–¹æ³•çš„æå‡ºä¸ºç¨€ç–è‡ªç¼–ç å™¨çš„è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…·æœ‰é‡è¦çš„å­¦æœ¯ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯è§£é‡Šæ€§ç ”ç©¶ä»¥åŠç›¸å…³é¢†åŸŸçš„æ¨¡å‹è¯„ä¼°ã€‚CE-Benchçš„è®¾è®¡å¯ä»¥ä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€ç§æ–°çš„è¯„ä¼°å·¥å…·ï¼Œå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–ç¨€ç–è‡ªç¼–ç å™¨çš„å¯è§£é‡Šæ€§ï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse autoencoders (SAEs) are a promising approach for uncovering interpretable features in large language models (LLMs). While several automated evaluation methods exist for SAEs, most rely on external LLMs. In this work, we introduce CE-Bench, a novel and lightweight contrastive evaluation benchmark for sparse autoencoders, built on a curated dataset of contrastive story pairs. We conduct comprehensive evaluation studies to validate the effectiveness of our approach. Our results show that CE-Bench reliably measures the interpretability of sparse autoencoders and aligns well with existing benchmarks without requiring an external LLM judge, achieving over 70% Spearman correlation with results in SAEBench. The official implementation and evaluation dataset are open-sourced and publicly available.

