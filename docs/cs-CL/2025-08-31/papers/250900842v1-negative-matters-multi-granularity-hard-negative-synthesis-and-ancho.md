---
layout: default
title: Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings
---

# Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00842" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00842v1</a>
  <a href="https://arxiv.org/pdf/2509.00842.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00842v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00842v1', 'Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tengyu Pan, Zhichao Duan, Zhenyu Li, Bowen Dong, Ning Liu, Xiuxing Li, Jianyong Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31

**DOI**: [10.18653/v1/2025.acl-long.1501](https://doi.org/10.18653/v1/2025.acl-long.1501)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šç²’åº¦ç¡¬è´Ÿæ ·æœ¬åˆæˆä¸é”šæ ‡è®°æ„ŸçŸ¥æ± åŒ–ä»¥æå‡æ–‡æœ¬åµŒå…¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åµŒå…¥` `å¯¹æ¯”å­¦ä¹ ` `è´Ÿæ ·æœ¬ç”Ÿæˆ` `å¤šç²’åº¦åˆæˆ` `é”šæ ‡è®°æ„ŸçŸ¥æ± åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `è¯­ä¹‰è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨è´Ÿæ ·æœ¬ç”Ÿæˆæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆåŒºåˆ†ç»†å¾®çš„è¯­ä¹‰å·®å¼‚ã€‚
2. æå‡ºå¤šç²’åº¦ç¡¬è´Ÿæ ·æœ¬åˆæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–è´Ÿæ ·æœ¬ï¼Œå¹¶å¼•å…¥é”šæ ‡è®°æ„ŸçŸ¥æ± åŒ–æ–¹æ³•ä»¥æå‡åµŒå…¥å‡†ç¡®æ€§ã€‚
3. åœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€ææ–¹æ³•è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰åˆæˆç­–ç•¥ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†è¯­ä¹‰ä¿¡æ¯ç¼–ç ä¸ºç¨ å¯†çš„å‘é‡è¡¨ç¤ºã€‚ç°æœ‰æ¨¡å‹é€šå¸¸ä½¿ç”¨ï¼ˆæŸ¥è¯¢ã€æ­£æ ·æœ¬ã€è´Ÿæ ·æœ¬ï¼‰ä¸‰å…ƒç»„è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œå…¶ä¸­è´Ÿæ ·æœ¬åœ¨å¢å¼ºæ¨¡å‹åŒºåˆ†ç»†å¾®è¯­ä¹‰å·®å¼‚çš„èƒ½åŠ›æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šç²’åº¦ç¡¬è´Ÿæ ·æœ¬ï¼ˆMGHï¼‰åˆæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå…·æœ‰ä¸åŒç›¸ä¼¼åº¦çš„å¤šæ ·åŒ–è´Ÿæ ·æœ¬ï¼Œä¿ƒè¿›äº†ç›‘ç£è®­ç»ƒä¸­çš„ç²—åˆ°ç»†çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é”šæ ‡è®°æ„ŸçŸ¥ï¼ˆATAï¼‰æ± åŒ–æ–¹æ³•ï¼Œæ ¹æ®LLMsä¸­è§‚å¯Ÿåˆ°çš„èšåˆæ¨¡å¼ä¸ºé”šæ ‡è®°åˆ†é…æ›´é«˜çš„æƒé‡ï¼Œä»è€Œåœ¨ä¸å¢åŠ æ¨¡å‹å¤æ‚åº¦çš„æƒ…å†µä¸‹æé«˜æ–‡æœ¬åµŒå…¥çš„å‡†ç¡®æ€§ã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åˆæˆç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨å¯¹æ¯”å­¦ä¹ ä¸­è´Ÿæ ·æœ¬ç”Ÿæˆçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæ•æ‰ç»†å¾®çš„è¯­ä¹‰å·®å¼‚ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤šç²’åº¦ç¡¬è´Ÿæ ·æœ¬åˆæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸åŒç›¸ä¼¼åº¦çš„è´Ÿæ ·æœ¬ï¼Œç»“åˆé”šæ ‡è®°æ„ŸçŸ¥æ± åŒ–æ–¹æ³•ï¼Œæå‡æ¨¡å‹çš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå¤šç²’åº¦ç¡¬è´Ÿæ ·æœ¬åˆæˆæ¨¡å—å’Œé”šæ ‡è®°æ„ŸçŸ¥æ± åŒ–æ¨¡å—ã€‚å‰è€…è´Ÿè´£ç”Ÿæˆå¤šæ ·åŒ–çš„è´Ÿæ ·æœ¬ï¼Œåè€…åˆ™é€šè¿‡åŠ æƒèšåˆæå‡åµŒå…¥å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¤šç²’åº¦ç¡¬è´Ÿæ ·æœ¬åˆæˆæ¡†æ¶çš„æå‡ºï¼Œåˆ©ç”¨LLMsç”Ÿæˆçš„è´Ÿæ ·æœ¬å…·æœ‰æ›´é«˜çš„å¤šæ ·æ€§å’Œç›¸ä¼¼åº¦å±‚æ¬¡ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé”šæ ‡è®°çš„æƒé‡æ ¹æ®èšåˆæ¨¡å¼åŠ¨æ€è°ƒæ•´ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºå¯¹æ¯”æŸå¤±ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°ç»†å¾®çš„è¯­ä¹‰å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰åˆæˆç­–ç•¥ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œåœ¨åˆæˆæ•°æ®å’Œå…¬å…±æ£€ç´¢æ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æ£€ç´¢å’Œå¯¹è¯ç³»ç»Ÿç­‰è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡æå‡æ–‡æœ¬åµŒå…¥çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text embedding models are essential for various natural language processing tasks, enabling the effective encoding of semantic information into dense vector representations. These models are typically optimized using triplets of (query, positive, negative) data pairs for contrastive learning, where the negative samples play a critical role in enhancing the model's ability to discern subtle semantic distinctions. In this work, we introduce a Multi-Granularity Hard-negative (MGH) synthesis framework that leverages large language models (LLMs) to generate diverse negative samples with varying levels of similarity with the query. This approach facilitates a coarse-to-fine curriculum learning strategy during supervised training, allowing the embedding model to progressively learn more nuanced semantic representations. Meanwhile, we propose an Anchor Token Aware (ATA) pooling method that assigns higher weights to anchor tokens based on aggregation patterns observed in LLMs, improving text embedding accuracy without increasing model complexity. Comprehensive experiments on the MTEB benchmark demonstrate that our methods achieve state-of-the-art performance, surpassing existing synthesis strategies both with synthetic data and when combined with public retrieval datasets.

