---
layout: default
title: LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA
---

# LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00731" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00731v1</a>
  <a href="https://arxiv.org/pdf/2509.00731.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00731v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00731v1', 'LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Houji Jin, Negin Ashrafi, Armin Abdollahi, Wei Liu, Jian Wang, Ganyu Gui, Maryam Pishgar, Huanghao Feng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLoRAçš„è§£ç å™¨æ¨¡å‹ä»¥æé«˜ä¸­æ–‡AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸­æ–‡æ–‡æœ¬æ£€æµ‹` `AIç”Ÿæˆæ–‡æœ¬` `è§£ç å™¨æ¨¡å‹` `LoRA` `é²æ£’æ€§` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸­æ–‡AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹æ–¹æ³•åœ¨é¢å¯¹è¯­è¨€ç»†å¾®å·®å¼‚æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯ç¼–ç å™¨æ¨¡å‹åœ¨åˆ†å¸ƒå˜åŒ–ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºLoRAçš„è§£ç å™¨æ¨¡å‹ï¼Œé€šè¿‡è½»é‡çº§åˆ†ç±»å¤´å’ŒæŒ‡ä»¤æ ¼å¼è¾“å…¥è¿›è¡Œé€‚é…ï¼Œæå‡äº†æ¨¡å‹çš„æ£€æµ‹èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoRAé€‚é…çš„Qwen2.5-7Bæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†95.94%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—é«˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ï¼Œå±•ç°äº†æ›´å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå‡†ç¡®æ£€æµ‹AIç”Ÿæˆæ–‡æœ¬çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­æ–‡ç­‰è¯­è¨€ä¸­ï¼Œç»†å¾®çš„è¯­è¨€å·®å¼‚å¯¹ç°æœ‰æ–¹æ³•æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿæ¯”è¾ƒäº†åŸºäºç¼–ç å™¨çš„Transformeræ¨¡å‹ï¼ˆå¦‚ä¸­æ–‡BERT-largeå’ŒRoBERTa-wwm-ext-largeï¼‰ã€ä»…è§£ç çš„LLMï¼ˆé˜¿é‡Œå·´å·´çš„Qwen2.5-7Bï¼‰ä»¥åŠä½¿ç”¨NLPCC 2025ä¸­æ–‡AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹ä»»åŠ¡çš„FastTextåŸºçº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç¼–ç å™¨æ¨¡å‹å‡ ä¹è®°ä½äº†è®­ç»ƒæ•°æ®ï¼Œä½†åœ¨åˆ†å¸ƒå˜åŒ–ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ç›¸è¾ƒä¹‹ä¸‹ï¼ŒLoRAé€‚é…çš„Qwen2.5-7Båœ¨æµ‹è¯•ä¸­è¾¾åˆ°äº†95.94%çš„å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œå¯¹æ•°æ®é›†ç‰¹å®šä¼ªå½±çš„æŠµæŠ—åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä¸­æ–‡AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹ä¸­çš„é²æ£’æ€§é—®é¢˜ï¼Œç°æœ‰çš„ç¼–ç å™¨æ¨¡å‹åœ¨é¢å¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´å‡†ç¡®ç‡ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºä½¿ç”¨è§£ç å™¨æ¨¡å‹Qwen2.5-7Bï¼Œå¹¶é€šè¿‡LoRAè¿›è¡Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼Œä»¥æé«˜æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹é€‚é…å’Œåˆ†ç±»é˜¶æ®µã€‚ç¼–ç å™¨æ¨¡å‹é€šè¿‡æ–°é¢–çš„åŸºäºæç¤ºçš„æ©ç è¯­è¨€å»ºæ¨¡è¿›è¡Œå¾®è°ƒï¼Œè€Œè§£ç å™¨æ¨¡å‹åˆ™é‡‡ç”¨æŒ‡ä»¤æ ¼å¼è¾“å…¥å’Œè½»é‡çº§åˆ†ç±»å¤´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºä½¿ç”¨LoRAå¯¹è§£ç å™¨æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†å¯¹æ•°æ®é›†ç‰¹å®šä¼ªå½±çš„æŠµæŠ—åŠ›ï¼Œä¸ä¼ ç»Ÿçš„ç¼–ç å™¨æ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è½»é‡çº§åˆ†ç±»å¤´ï¼Œå¹¶é€šè¿‡LoRAè¿›è¡Œå‚æ•°é€‚é…ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoRAé€‚é…çš„Qwen2.5-7Bæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†95.94%çš„å‡†ç¡®ç‡ï¼Œå¹³è¡¡çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡æŒ‡æ ‡è¡¨æ˜å…¶åœ¨é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç¼–ç å™¨æ¨¡å‹ï¼ˆBERT: 79.3%ï¼ŒRoBERTa: 76.3%ï¼‰å’ŒFastTextï¼ˆ83.5%ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ã€æ•™è‚²é¢†åŸŸçš„æ–‡æœ¬ç”Ÿæˆæ£€æµ‹ä»¥åŠæ–°é—»æŠ¥é“çš„çœŸå®æ€§éªŒè¯ç­‰ã€‚é€šè¿‡æé«˜ä¸­æ–‡AIç”Ÿæˆæ–‡æœ¬çš„æ£€æµ‹èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆé˜²æ­¢è™šå‡ä¿¡æ¯çš„ä¼ æ’­ï¼Œæå‡ä¿¡æ¯çš„å¯ä¿¡åº¦å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid growth of large language models (LLMs) has heightened the demand for accurate detection of AI-generated text, particularly in languages like Chinese, where subtle linguistic nuances pose significant challenges to current methods. In this study, we conduct a systematic comparison of encoder-based Transformers (Chinese BERT-large and RoBERTa-wwm-ext-large), a decoder-only LLM (Alibaba's Qwen2.5-7B/DeepSeek-R1-Distill-Qwen-7B fine-tuned via Low-Rank Adaptation, LoRA), and a FastText baseline using the publicly available dataset from the NLPCC 2025 Chinese AI-Generated Text Detection Task. Encoder models were fine-tuned using a novel prompt-based masked language modeling approach, while Qwen2.5-7B was adapted for classification with an instruction-format input and a lightweight classification head trained via LoRA. Experiments reveal that although encoder models nearly memorize training data, they suffer significant performance degradation under distribution shifts (RoBERTa: 76.3% test accuracy; BERT: 79.3%). FastText demonstrates surprising lexical robustness (83.5% accuracy) yet lacks deeper semantic understanding. In contrast, the LoRA-adapted Qwen2.5-7B achieves 95.94% test accuracy with balanced precision-recall metrics, indicating superior generalization and resilience to dataset-specific artifacts. These findings underscore the efficacy of decoder-based LLMs with parameter-efficient fine-tuning for robust Chinese AI-generated text detection. Future work will explore next-generation Qwen3 models, distilled variants, and ensemble strategies to enhance cross-domain robustness further.

