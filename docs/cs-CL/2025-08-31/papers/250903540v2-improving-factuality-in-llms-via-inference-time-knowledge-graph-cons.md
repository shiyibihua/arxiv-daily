---
layout: default
title: Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction
---

# Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03540" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03540v2</a>
  <a href="https://arxiv.org/pdf/2509.03540.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03540v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03540v2', 'Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shanglin Wu, Lihui Liu, Jinho D. Choi, Kai Shu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31 (æ›´æ–°: 2025-10-07)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€æ„å»ºçŸ¥è¯†å›¾è°±ä»¥æå‡LLMçš„äº‹å®å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†å›¾è°±` `äº‹å®ä¸€è‡´æ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŠ¨æ€æ„å»º` `ä¿¡æ¯æ£€ç´¢` `æ¨ç†èƒ½åŠ›` `æ£€ç´¢å¢å¼ºç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMåœ¨ç”Ÿæˆäº‹å®ä¸€è‡´çš„å›ç­”æ—¶ï¼Œå› å‚æ•°è®°å¿†çš„å±€é™æ€§è€Œé¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´è¾“å‡ºçš„å‡†ç¡®æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€æ„å»ºçŸ¥è¯†å›¾è°±çš„æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆå†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ï¼Œæå‡LLMçš„äº‹å®å‡†ç¡®æ€§ã€‚
3. åœ¨ä¸‰ä¸ªäº‹å®é—®ç­”åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨äº‹å®å‡†ç¡®æ€§ä¸Šç›¸è¾ƒäºåŸºçº¿æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆäº‹å®ä¸€è‡´çš„ç­”æ¡ˆæ—¶å¸¸å¸¸é¢ä¸´å‚æ•°è®°å¿†çš„å±€é™æ€§ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•é€šè¿‡åœ¨æ¨ç†æ—¶å¼•å…¥å¤–éƒ¨çŸ¥è¯†æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†é€šå¸¸å°†çŸ¥è¯†è§†ä¸ºéç»“æ„åŒ–æ–‡æœ¬ï¼Œå¯¼è‡´æ£€ç´¢å‡†ç¡®æ€§é™ä½ï¼Œç»„åˆæ¨ç†å—é˜»ï¼Œå¹¶åŠ å‰§æ— å…³ä¿¡æ¯å¯¹LLMè¾“å‡ºäº‹å®ä¸€è‡´æ€§çš„å½±å“ã€‚ä¸ºå…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€æ„å»ºå’Œæ‰©å±•çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ï¼Œæ•´åˆLLMå†…éƒ¨æå–çš„çŸ¥è¯†å’Œå¤–éƒ¨æ£€ç´¢çš„çŸ¥è¯†ã€‚è¯¥æ–¹æ³•é€šè¿‡æç¤ºä»é—®é¢˜ä¸­æå–ç§å­KGï¼Œéšååˆ©ç”¨LLMçš„å†…éƒ¨çŸ¥è¯†è¿›è¡Œè¿­ä»£æ‰©å±•ï¼Œå¹¶é€šè¿‡å¤–éƒ¨æ£€ç´¢é€‰æ‹©æ€§åœ°ç²¾ç‚¼KGï¼Œä»è€Œå¢å¼ºäº‹å®è¦†ç›–ç‡å’Œçº æ­£ä¸å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¤šæ ·çš„äº‹å®é—®ç­”åŸºå‡†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºåœ¨äº‹å®å‡†ç¡®æ€§ä¸Šç›¸è¾ƒäºåŸºçº¿æœ‰ä¸€è‡´çš„æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå›ç­”æ—¶çš„äº‹å®ä¸€è‡´æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å› å¤„ç†çŸ¥è¯†çš„æ–¹å¼å¯¼è‡´å‡†ç¡®æ€§ä¸è¶³å’Œæ¨ç†èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºé€šè¿‡æ¨ç†æ—¶åŠ¨æ€æ„å»ºå’Œæ‰©å±•çŸ¥è¯†å›¾è°±ï¼Œç»“åˆLLMå†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨çŸ¥è¯†ï¼Œæå‡äº‹å®å‡†ç¡®æ€§å’Œè¦†ç›–ç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ä»é—®é¢˜æå–ç§å­KGï¼Œåˆ©ç”¨LLMå†…éƒ¨çŸ¥è¯†è¿›è¡Œè¿­ä»£æ‰©å±•ï¼Œå¹¶é€šè¿‡å¤–éƒ¨æ£€ç´¢ç²¾ç‚¼KGï¼Œå½¢æˆä¸€ä¸ªåŠ¨æ€æ›´æ–°çš„çŸ¥è¯†ç»“æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåŠ¨æ€æ„å»ºçŸ¥è¯†å›¾è°±çš„èƒ½åŠ›ï¼Œä½¿å¾—çŸ¥è¯†ä¸ä»…é™äºé™æ€æ–‡æœ¬ï¼Œè€Œæ˜¯é€šè¿‡æ¨ç†è¿‡ç¨‹ä¸æ–­æ›´æ–°å’Œä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†äº‹å®ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†è¿­ä»£æ‰©å±•å’Œé€‰æ‹©æ€§ç²¾ç‚¼çš„ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡äº‹å®è¦†ç›–ç‡å’Œå‡†ç¡®æ€§çš„å¹³è¡¡ï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†LLMçš„ç‰¹æ€§ä¸çŸ¥è¯†å›¾è°±çš„æ„å»ºéœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ä¸‰ä¸ªäº‹å®é—®ç­”åŸºå‡†ä¸Šå‡æ˜¾è‘—æå‡äº†äº‹å®å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œå‡†ç¡®æ€§æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†åŠ¨æ€çŸ¥è¯†å›¾è°±æ„å»ºçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿä»¥åŠä¿¡æ¯æ£€ç´¢ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç³»ç»Ÿçš„äº‹å®å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„çŸ¥è¯†ç®¡ç†å’Œä¿¡æ¯å¤„ç†æŠ€æœ¯çš„å‘å±•ï¼Œä¿ƒè¿›AIåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often struggle with producing factually consistent answers due to limitations in their parametric memory. Retrieval-Augmented Generation (RAG) paradigms mitigate this issue by incorporating external knowledge at inference time. However, such methods typically handle knowledge as unstructured text, which reduces retrieval accuracy, hinders compositional reasoning, and amplifies the influence of irrelevant information on the factual consistency of LLM outputs. To overcome these limitations, we propose a novel framework that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating both internal knowledge extracted from LLMs and external knowledge retrieved from external sources. Our method begins by extracting a seed KG from the question via prompting, followed by iterative expansion using the LLM's internal knowledge. The KG is then selectively refined through external retrieval, enhancing factual coverage and correcting inaccuracies. We evaluate our approach on three diverse Factual QA benchmarks, demonstrating consistent gains in factual accuracy over baselines. Our findings reveal that inference-time KG construction is a promising direction for enhancing LLM factuality in a structured, interpretable, and scalable manner.

