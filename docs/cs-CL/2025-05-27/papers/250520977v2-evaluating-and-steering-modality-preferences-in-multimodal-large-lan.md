---
layout: default
title: Evaluating and Steering Modality Preferences in Multimodal Large Language Model
---

# Evaluating and Steering Modality Preferences in Multimodal Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20977" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20977v2</a>
  <a href="https://arxiv.org/pdf/2505.20977.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20977v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20977v2', 'Evaluating and Steering Modality Preferences in Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, Min Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-09-29)

**å¤‡æ³¨**: Modality Preference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMCÂ²åŸºå‡†ä»¥è¯„ä¼°å’Œå¼•å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨¡æ€åå¥½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ¨¡æ€åå¥½` `MCÂ²åŸºå‡†` `è¡¨ç¤ºå·¥ç¨‹` `å¹»è§‰ç¼“è§£` `å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘` `æ½œåœ¨è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šæ¨¡æ€å†²çªè¯æ®æ—¶ï¼Œæ¨¡æ€åå¥½çš„è¡¨ç°å°šæœªå¾—åˆ°ç³»ç»Ÿè¯„ä¼°ï¼Œç¼ºä¹ç›¸å…³åŸºå‡†ã€‚
2. æœ¬æ–‡æå‡ºMCÂ²åŸºå‡†ï¼Œé€šè¿‡å—æ§è¯æ®å†²çªåœºæ™¯è¯„ä¼°æ¨¡æ€åå¥½ï¼Œå¹¶æå‡ºåŸºäºè¡¨ç¤ºå·¥ç¨‹çš„æ¢æµ‹å’Œå¼•å¯¼æ–¹æ³•ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æ¨¡æ€åå¥½ï¼Œæå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å¹»è§‰ç¼“è§£å’Œå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­æ˜¯å¦å­˜åœ¨æ¨¡æ€åå¥½ä»æœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ„å»ºäº†MCÂ²åŸºå‡†ï¼Œåœ¨å—æ§è¯æ®å†²çªåœºæ™¯ä¸‹ç³»ç»Ÿè¯„ä¼°æ¨¡æ€åå¥½ã€‚ç ”ç©¶å‘ç°ï¼Œ18ä¸ªæµ‹è¯•çš„MLLMsæ™®éè¡¨ç°å‡ºæ˜æ˜¾çš„æ¨¡æ€åè§ï¼Œä¸”æ¨¡æ€åå¥½å¯é€šè¿‡å¤–éƒ¨å¹²é¢„è¿›è¡Œå½±å“ã€‚æ·±å…¥åˆ†ææ˜¾ç¤ºï¼Œæ¨¡æ€åå¥½çš„æ–¹å‘å¯ä»¥åœ¨MLLMsçš„æ½œåœ¨è¡¨ç¤ºä¸­æ•æ‰åˆ°ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¡¨ç¤ºå·¥ç¨‹çš„æ¢æµ‹å’Œå¼•å¯¼æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œé¢å¤–å¾®è°ƒæˆ–ç²¾å¿ƒè®¾è®¡æç¤ºçš„æƒ…å†µä¸‹ï¼Œæ˜ç¡®æ§åˆ¶æ¨¡æ€åå¥½ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå¢å¼ºäº†æ¨¡æ€åå‘æ‰€éœ€æ–¹å‘ï¼Œå¹¶åœ¨å¹»è§‰ç¼“è§£å’Œå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½æ•ˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ—¶çš„æ¨¡æ€åå¥½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½ç³»ç»Ÿè¯„ä¼°æ¨¡æ€åå¥½ï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆæ§åˆ¶æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºMCÂ²åŸºå‡†ï¼Œé€šè¿‡å—æ§è¯æ®å†²çªåœºæ™¯è¯„ä¼°æ¨¡æ€åå¥½ï¼Œå¹¶åŸºäºæ½œåœ¨è¡¨ç¤ºè®¾è®¡æ¢æµ‹å’Œå¼•å¯¼æ–¹æ³•ï¼Œä»¥æ˜ç¡®æ§åˆ¶æ¨¡æ€åå¥½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡æ€åå¥½è¯„ä¼°ã€è¡¨ç¤ºæ¢æµ‹ä¸å¼•å¯¼æ¨¡å—ã€‚é¦–å…ˆæ„å»ºMCÂ²åŸºå‡†ï¼Œç„¶åé€šè¿‡åˆ†ææ½œåœ¨è¡¨ç¤ºæ•æ‰æ¨¡æ€åå¥½ï¼Œæœ€åå®æ–½å¼•å¯¼æ–¹æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†åŸºäºè¡¨ç¤ºå·¥ç¨‹çš„æ¢æµ‹å’Œå¼•å¯¼æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œé¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œç›´æ¥æ§åˆ¶æ¨¡æ€åå¥½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ¨¡æ€æƒé‡è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å’Œå¼•å¯¼æ¨¡æ€åå¥½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¹»è§‰ç¼“è§£å’Œå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ¨¡æ€åå¥½çš„æœ‰æ•ˆå¼•å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ã€ä¿¡æ¯æ£€ç´¢å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æœ‰æ•ˆæ§åˆ¶æ¨¡æ€åå¥½ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build a \textbf{MC\textsuperscript{2} } benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose a probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward a desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements.

