---
layout: default
title: LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures
---

# LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14252" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14252v2</a>
  <a href="https://arxiv.org/pdf/2509.14252.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14252v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14252v2', 'LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hai Huang, Yann LeCun, Randall Balestriero

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11 (æ›´æ–°: 2025-10-07)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/rbalestr-lab/llm-jepa)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLM-JEPAï¼Œå°†è”åˆåµŒå…¥é¢„æµ‹æ¶æ„åº”ç”¨äºLLMçš„é¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è”åˆåµŒå…¥é¢„æµ‹æ¶æ„` `é¢„è®­ç»ƒ` `å¾®è°ƒ` `åµŒå…¥ç©ºé—´å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè®­ç»ƒä¸»è¦ä¾èµ–è¾“å…¥ç©ºé—´é‡å»ºï¼Œè€Œè§†è§‰é¢†åŸŸçš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆJEPAï¼‰è¡¨ç°æ›´ä¼˜ï¼Œå­˜åœ¨é¢†åŸŸå·®å¼‚ã€‚
2. LLM-JEPAå°†JEPAçš„æ€æƒ³å¼•å…¥LLMè®­ç»ƒï¼Œé€šè¿‡åµŒå…¥ç©ºé—´é¢„æµ‹æå‡æ¨¡å‹æ€§èƒ½ï¼Œé€‚ç”¨äºé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM-JEPAåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†LLMè®­ç»ƒæ–¹æ³•ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒã€å¾®è°ƒå’Œè¯„ä¼°ä¾èµ–äºè¾“å…¥ç©ºé—´é‡å»ºå’Œç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨è§†è§‰é¢†åŸŸï¼ŒåŸºäºè”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆJEPAsï¼‰çš„åµŒå…¥ç©ºé—´è®­ç»ƒç›®æ ‡è¿œä¼˜äºå…¶è¾“å…¥ç©ºé—´å¯¹åº”æ–¹æ³•ã€‚è¯­è¨€å’Œè§†è§‰è®­ç»ƒæ–¹å¼çš„è¿™ç§ä¸åŒ¹é…å¼•å‡ºäº†ä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜ï¼šè¯­è¨€è®­ç»ƒæ–¹æ³•èƒ½å¦ä»è§†è§‰æ–¹æ³•ä¸­å­¦ä¹ ä¸€äº›æŠ€å·§ï¼Ÿç¼ºä¹JEPAé£æ ¼çš„LLMè¯æ˜äº†ä¸ºè¯­è¨€è®¾è®¡æ­¤ç±»ç›®æ ‡çš„æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æœè¿™ä¸ªæ–¹å‘è¿ˆå‡ºçš„ç¬¬ä¸€æ­¥ï¼Œå¼€å‘äº†LLM-JEPAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºJEPAçš„LLMè§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºå¾®è°ƒå’Œé¢„è®­ç»ƒã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒLLM-JEPAèƒ½å¤Ÿåœ¨å„ç§æ¨¡å‹ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†LLMè®­ç»ƒç›®æ ‡ï¼ŒåŒæ—¶å¯¹è¿‡æ‹Ÿåˆå…·æœ‰é²æ£’æ€§ã€‚è¿™äº›å‘ç°åœ¨ä¼—å¤šæ•°æ®é›†ï¼ˆNL-RXã€GSM8Kã€Spiderã€RottenTomatoesï¼‰å’Œæ¥è‡ªLlama3ã€OpenELMã€Gemma2å’ŒOlmoç³»åˆ—çš„å„ç§æ¨¡å‹ä¸­è§‚å¯Ÿåˆ°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰LLMçš„è®­ç»ƒèŒƒå¼ä¸»è¦ä¾èµ–äºè¾“å…¥ç©ºé—´çš„é‡å»ºå’Œç”Ÿæˆèƒ½åŠ›ï¼Œä¾‹å¦‚é¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹å¼å¯èƒ½ä¸å¤Ÿé«˜æ•ˆï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°è¿‡æ‹Ÿåˆçš„å½±å“ã€‚è§†è§‰é¢†åŸŸçš„JEPAsæ–¹æ³•åœ¨åµŒå…¥ç©ºé—´è¿›è¡Œè®­ç»ƒï¼Œå·²è¢«è¯æ˜æ›´åŠ æœ‰æ•ˆã€‚å› æ­¤ï¼Œå¦‚ä½•å°†JEPAsçš„æ€æƒ³å¼•å…¥åˆ°LLMçš„è®­ç»ƒä¸­ï¼Œå…‹æœä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLLM-JEPAçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨LLMçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸å†ç›´æ¥é¢„æµ‹è¾“å…¥ç©ºé—´çš„å†…å®¹ï¼ˆä¾‹å¦‚ä¸‹ä¸€ä¸ªtokenï¼‰ï¼Œè€Œæ˜¯åœ¨åµŒå…¥ç©ºé—´ä¸­è¿›è¡Œé¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹å­¦ä¹ é¢„æµ‹åŒä¸€è¾“å…¥çš„ä¸¤ä¸ªä¸åŒè§†è§’ï¼ˆä¾‹å¦‚ï¼Œç»è¿‡ä¸åŒå™ªå£°å¤„ç†çš„ç‰ˆæœ¬ï¼‰çš„åµŒå…¥è¡¨ç¤ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ›´åŠ é²æ£’å’Œæ³›åŒ–çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLLM-JEPAçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šç¼–ç å™¨å’Œé¢„æµ‹å™¨ã€‚ç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥è¡¨ç¤ºã€‚é¢„æµ‹å™¨æ¥æ”¶ç¼–ç å™¨è¾“å‡ºçš„åµŒå…¥è¡¨ç¤ºï¼Œå¹¶é¢„æµ‹åŒä¸€è¾“å…¥çš„å¦ä¸€ä¸ªè§†è§’çš„åµŒå…¥è¡¨ç¤ºã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æœ€å°åŒ–é¢„æµ‹çš„åµŒå…¥è¡¨ç¤ºä¸ç›®æ ‡åµŒå…¥è¡¨ç¤ºä¹‹é—´çš„å·®å¼‚æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚è¯¥æ¡†æ¶å¯ä»¥åº”ç”¨äºLLMçš„é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šLLM-JEPAæœ€é‡è¦çš„åˆ›æ–°ç‚¹æ˜¯å°†JEPAsçš„æ€æƒ³ä»è§†è§‰é¢†åŸŸå¼•å…¥åˆ°LLMé¢†åŸŸã€‚ä¸ä¼ ç»Ÿçš„è¾“å…¥ç©ºé—´é¢„æµ‹æ–¹æ³•ç›¸æ¯”ï¼ŒLLM-JEPAåœ¨åµŒå…¥ç©ºé—´è¿›è¡Œé¢„æµ‹ï¼Œå¯ä»¥å­¦ä¹ åˆ°æ›´åŠ é²æ£’å’Œæ³›åŒ–çš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼ŒLLM-JEPAçš„è®¾è®¡ä½¿å¾—å®ƒå¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºç°æœ‰çš„LLMæ¶æ„ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§è§„æ¨¡çš„ä¿®æ”¹ã€‚

**å…³é”®è®¾è®¡**ï¼šLLM-JEPAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•ç”ŸæˆåŒä¸€è¾“å…¥çš„ä¸¤ä¸ªä¸åŒè§†è§’ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡éšæœºmaskingã€token shufflingç­‰æ–¹å¼å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œå¤„ç†ã€‚2) å¦‚ä½•å®šä¹‰åµŒå…¥è¡¨ç¤ºä¹‹é—´çš„å·®å¼‚ã€‚å¯ä»¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ã€å‡æ–¹è¯¯å·®ç­‰æŒ‡æ ‡æ¥è¡¡é‡é¢„æµ‹çš„åµŒå…¥è¡¨ç¤ºä¸ç›®æ ‡åµŒå…¥è¡¨ç¤ºä¹‹é—´çš„å·®å¼‚ã€‚3) å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç¼–ç å™¨å’Œé¢„æµ‹å™¨æ¶æ„ã€‚å¯ä»¥ä½¿ç”¨Transformerã€MLPç­‰æ¶æ„ä½œä¸ºç¼–ç å™¨å’Œé¢„æµ‹å™¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM-JEPAåœ¨å¤šä¸ªæ•°æ®é›†ï¼ˆNL-RXã€GSM8Kã€Spiderã€RottenTomatoesï¼‰å’Œå„ç§æ¨¡å‹ï¼ˆLlama3ã€OpenELMã€Gemma2å’ŒOlmoï¼‰ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒLLM-JEPAçš„æ€§èƒ½æå‡è¶…è¿‡äº†5%ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼ŒLLM-JEPAå¯¹è¿‡æ‹Ÿåˆå…·æœ‰æ›´å¥½çš„é²æ£’æ€§ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„æ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LLM-JEPAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥æé«˜LLMçš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ å¯é ã€‚æ­¤å¤–ï¼ŒLLM-JEPAè¿˜å¯ä»¥ä¿ƒè¿›LLMçš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œä¾‹å¦‚æ¢ç´¢æ›´åŠ æœ‰æ•ˆçš„åµŒå…¥ç©ºé—´è®­ç»ƒæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: https://github.com/rbalestr-lab/llm-jepa.

