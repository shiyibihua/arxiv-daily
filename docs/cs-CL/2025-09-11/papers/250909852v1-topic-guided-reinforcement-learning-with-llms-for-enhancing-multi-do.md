---
layout: default
title: Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization
---

# Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09852" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09852v1</a>
  <a href="https://arxiv.org/pdf/2509.09852.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09852v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09852v1', 'Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chuyuan Li, Austin Xu, Shafiq Joty, Giuseppe Carenini

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸»é¢˜å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨LLMæå‡å¤šæ–‡æ¡£æ‘˜è¦ç”Ÿæˆè´¨é‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ–‡æ¡£æ‘˜è¦` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸»é¢˜å¼•å¯¼` `å†…å®¹é€‰æ‹©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ–‡æ¡£æ‘˜è¦é¢ä¸´æ•´åˆå¤šæºä¿¡æ¯å¹¶ä¿æŒä¸»é¢˜ä¸€è‡´æ€§çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾ä¿¡æ¯é‡å’Œç›¸å…³æ€§ã€‚
2. è®ºæ–‡æå‡ºä¸»é¢˜å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä¸»é¢˜æ ‡ç­¾æç¤ºå’Œä¸»é¢˜å¥–åŠ±æœºåˆ¶ï¼Œæå‡æ‘˜è¦ç”Ÿæˆçš„ä¿¡æ¯é‡å’Œä¸»é¢˜å¯¹é½åº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Multi-Newså’ŒMulti-XScienceæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰åŸºçº¿ï¼ŒéªŒè¯äº†ä¸»é¢˜å¼•å¯¼çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ–‡æ¡£æ‘˜è¦ï¼ˆMDSï¼‰çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°æ•´åˆæ¥è‡ªå¤šä¸ªæ¥æºçš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒè¿è´¯æ€§å’Œä¸»é¢˜ç›¸å…³æ€§ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•æ–‡æ¡£æ‘˜è¦æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å®ƒä»¬åœ¨MDSä¸Šçš„æ€§èƒ½ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸»é¢˜å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥æ”¹è¿›MDSä¸­çš„å†…å®¹é€‰æ‹©ã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜ï¼Œä½¿ç”¨ä¸»é¢˜æ ‡ç­¾æ˜¾å¼åœ°æç¤ºæ¨¡å‹å¯ä»¥å¢å¼ºç”Ÿæˆæ‘˜è¦çš„ä¿¡æ¯é‡ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œæˆ‘ä»¬åœ¨Group Relative Policy Optimization (GRPO) æ¡†æ¶å†…æå‡ºäº†ä¸€ç§æ–°çš„ä¸»é¢˜å¥–åŠ±ï¼Œä»¥è¡¡é‡ç”Ÿæˆæ‘˜è¦ä¸æºæ–‡æ¡£ä¹‹é—´çš„ä¸»é¢˜å¯¹é½ç¨‹åº¦ã€‚åœ¨Multi-Newså’ŒMulti-XScienceæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œçªå‡ºäº†åœ¨MDSä¸­åˆ©ç”¨ä¸»é¢˜çº¿ç´¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ–‡æ¡£æ‘˜è¦æ—¨åœ¨ä»å¤šä¸ªæ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´ã€è¿è´¯ä¸”ä¸»é¢˜ç›¸å…³çš„æ‘˜è¦ã€‚ç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œåœ¨å¤šæ–‡æ¡£æ‘˜è¦ä»»åŠ¡ä¸­ä»å­˜åœ¨ä¿¡æ¯æ•´åˆä¸è¶³ã€ä¸»é¢˜ä¸€è‡´æ€§è¾ƒå·®çš„é—®é¢˜ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨æ–‡æ¡£ä¸­çš„ä¸»é¢˜ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸»é¢˜å¼•å¯¼çš„æ–¹å¼ï¼Œæ˜¾å¼åœ°åˆ©ç”¨æ–‡æ¡£ä¸­çš„ä¸»é¢˜ä¿¡æ¯æ¥æŒ‡å¯¼æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆé€šè¿‡ä¸»é¢˜æ ‡ç­¾æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆæ›´å…·ä¿¡æ¯é‡çš„æ‘˜è¦ï¼›ç„¶åï¼Œè®¾è®¡ä¸»é¢˜å¥–åŠ±å‡½æ•°ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ‘˜è¦ç”Ÿæˆç­–ç•¥ï¼Œä½¿å…¶ä¸æºæ–‡æ¡£çš„ä¸»é¢˜æ›´åŠ å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŸºäºGroup Relative Policy Optimization (GRPO)ã€‚é¦–å…ˆï¼Œä½¿ç”¨ä¸»é¢˜æ ‡ç­¾æç¤ºLLMç”Ÿæˆåˆå§‹æ‘˜è¦ã€‚ç„¶åï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ä¸»é¢˜å¥–åŠ±å‡½æ•°ä¼˜åŒ–LLMçš„æ‘˜è¦ç”Ÿæˆç­–ç•¥ã€‚ä¸»é¢˜å¥–åŠ±å‡½æ•°è¡¡é‡ç”Ÿæˆæ‘˜è¦ä¸æºæ–‡æ¡£ä¹‹é—´çš„ä¸»é¢˜å¯¹é½ç¨‹åº¦ã€‚æ•´ä¸ªæ¡†æ¶åŒ…å«ä¸»é¢˜æ ‡ç­¾ç”Ÿæˆæ¨¡å—ã€æ‘˜è¦ç”Ÿæˆæ¨¡å—å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸»é¢˜å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå°†ä¸»é¢˜ä¿¡æ¯æ˜¾å¼åœ°èå…¥åˆ°æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸ä»…åˆ©ç”¨äº†LLMå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œè¿˜é€šè¿‡ä¸»é¢˜æç¤ºå’Œä¸»é¢˜å¥–åŠ±æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°æå‡äº†æ‘˜è¦çš„ä¿¡æ¯é‡å’Œä¸»é¢˜ç›¸å…³æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šä¸»é¢˜æ ‡ç­¾ç”Ÿæˆæ¨¡å—å¯ä»¥ä½¿ç”¨ç°æœ‰çš„ä¸»é¢˜æ¨¡å‹æˆ–å…³é”®è¯æå–ç®—æ³•ã€‚ä¸»é¢˜å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œå¯ä»¥é‡‡ç”¨åŸºäºä¸»é¢˜åˆ†å¸ƒçš„ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼Œä¾‹å¦‚è®¡ç®—ç”Ÿæˆæ‘˜è¦å’Œæºæ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒçš„KLæ•£åº¦æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•é‡‡ç”¨GRPOï¼Œå¥–åŠ±å‡½æ•°ç”±ä¿¡æ¯é‡å¥–åŠ±å’Œä¸»é¢˜å¥–åŠ±åŠ æƒç»„æˆã€‚å…·ä½“æƒé‡éœ€è¦æ ¹æ®å®éªŒè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Multi-Newså’ŒMulti-XScienceæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¼˜äºå¤šä¸ªå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ROUGEæŒ‡æ ‡ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹å–å¾—äº†1-2ä¸ªç™¾åˆ†ç‚¹çš„æå‡ï¼Œè¯æ˜äº†ä¸»é¢˜å¼•å¯¼çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ–°é—»æ‘˜è¦ã€ç§‘ç ”æ–‡çŒ®ç»¼è¿°ã€ä¼šè®®è®°å½•æ•´ç†ç­‰é¢†åŸŸï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿè·å–å¤šç¯‡æ–‡æ¡£çš„æ ¸å¿ƒä¿¡æ¯ï¼Œæé«˜ä¿¡æ¯å¤„ç†æ•ˆç‡ã€‚æœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢å°†è¯¥æ–¹æ³•åº”ç”¨äºå…¶ä»–è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚å¯¹è¯ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ç­‰ï¼Œæå‡ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å’Œç›¸å…³æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A key challenge in Multi-Document Summarization (MDS) is effectively integrating information from multiple sources while maintaining coherence and topical relevance. While Large Language Models have shown impressive results in single-document summarization, their performance on MDS still leaves room for improvement. In this paper, we propose a topic-guided reinforcement learning approach to improve content selection in MDS. We first show that explicitly prompting models with topic labels enhances the informativeness of the generated summaries. Building on this insight, we propose a novel topic reward within the Group Relative Policy Optimization (GRPO) framework to measure topic alignment between the generated summary and source documents. Experimental results on the Multi-News and Multi-XScience datasets demonstrate that our method consistently outperforms strong baselines, highlighting the effectiveness of leveraging topical cues in MDS.

