---
layout: default
title: LLM Architecture, Scaling Laws, and Economics: A Quick Summary
---

# LLM Architecture, Scaling Laws, and Economics: A Quick Summary

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.11572" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.11572v1</a>
  <a href="https://arxiv.org/pdf/2511.11572.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.11572v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.11572v1', 'LLM Architecture, Scaling Laws, and Economics: A Quick Summary')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: William H. Press

**åˆ†ç±»**: cs.GL, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: 9 pages, 3 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ€»ç»“LLMæ¶æ„ã€æ‰©å±•æ³•åˆ™ä¸ç»æµæ€§ï¼Œä¸ºå¿«é€Ÿç†è§£æä¾›å‚è€ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `Transformeræ¶æ„` `æ‰©å±•æ³•åˆ™` `è®¡ç®—æˆæœ¬` `å†…å­˜æˆæœ¬`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ¶æ„è™½ç„¶æœ‰æ•ˆï¼Œä½†ç¼ºä¹ä¸€ä¸ªç®€æ´çš„æ€»ç»“æ€§æ–‡æ¡£ï¼Œä¸åˆ©äºå¿«é€Ÿäº†è§£å…¶æ ¸å¿ƒåŸç†ã€‚
2. æœ¬æ–‡å¯¹LLMçš„Transformeræ¶æ„ã€æ‰©å±•æ³•åˆ™å’Œç»æµæˆæœ¬è¿›è¡Œäº†æ€»ç»“ï¼Œæ–¹ä¾¿è¯»è€…å¿«é€ŸæŒæ¡å…³é”®ä¿¡æ¯ã€‚
3. æ–‡ç« å¯¹è®¡ç®—å’Œå†…å­˜çš„æ‰©å±•æ³•åˆ™è¿›è¡Œäº†æ¢³ç†ï¼Œå¹¶å¯¹ä¸åŒè§„æ¨¡LLMçš„å‚æ•°æˆæœ¬è¿›è¡Œäº†ä¼°ç®—ï¼Œä¸ºLLMçš„éƒ¨ç½²å’Œä¼˜åŒ–æä¾›å‚è€ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç®€è¦æ€»ç»“äº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡å‡†æ¶æ„ï¼Œè¯¥æ¶æ„é‡‡ç”¨QKVè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬å…¸å‹Transformerçš„æ¶æ„ã€‚æ–‡ç« ç»™å‡ºäº†è®¡ç®—ï¼ˆflopsï¼‰å’Œå†…å­˜ï¼ˆå‚æ•°åŠ æ•°æ®ï¼‰çš„æ‰©å±•æ³•åˆ™ï¼Œä»¥åŠå½“å‰ï¼ˆ2025å¹´ï¼‰å„ç§è§„æ¨¡LLMå‚æ•°çš„ç²—ç•¥æˆæœ¬ä¼°ç®—ï¼Œå¹¶è®¨è®ºäº†æ˜¯å¦åº”å°†DeepSeekè§†ä¸ºä¸€ä¸ªç‰¹ä¾‹ã€‚æœ¬æ–‡æ²¡æœ‰æå‡ºä»»ä½•æ–°å†…å®¹ï¼Œä½†è¿™äº›ææ–™ä¼¼ä¹ä¸å®¹æ˜“ä»¥æ‘˜è¦å½¢å¼è·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç ”ç©¶æ¶‰åŠå¤šä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬æ¶æ„è®¾è®¡ã€æ‰©å±•æ³•åˆ™ä»¥åŠç»æµæˆæœ¬ã€‚ç„¶è€Œï¼Œè¿™äº›ä¿¡æ¯å¾€å¾€åˆ†æ•£åœ¨ä¸åŒçš„è®ºæ–‡å’ŒæŠ¥å‘Šä¸­ï¼Œç¼ºä¹ä¸€ä¸ªç®€æ´æ˜äº†çš„æ€»ç»“æ€§æ–‡æ¡£ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆéš¾ä»¥å¿«é€Ÿäº†è§£LLMçš„å…¨è²Œã€‚æ­¤å¤–ï¼Œå¯¹äºç‰¹å®šæ¨¡å‹ï¼ˆå¦‚DeepSeekï¼‰æ˜¯å¦å…·æœ‰ç‰¹æ®Šæ€§ï¼Œä¹Ÿç¼ºä¹æ·±å…¥çš„è®¨è®ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¯¹LLMçš„å…³é”®ç»„æˆéƒ¨åˆ†è¿›è¡Œæ¢³ç†å’Œæ€»ç»“ï¼ŒåŒ…æ‹¬å…¶æ¶æ„ã€æ‰©å±•æ³•åˆ™å’Œç»æµæˆæœ¬ã€‚é€šè¿‡å°†è¿™äº›ä¿¡æ¯æ•´åˆåˆ°ä¸€ä¸ªæ–‡æ¡£ä¸­ï¼Œå¯ä»¥ä¸ºè¯»è€…æä¾›ä¸€ä¸ªå¿«é€Ÿå…¥é—¨LLMçš„é€”å¾„ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¯¹DeepSeekæ¨¡å‹è¿›è¡Œäº†è®¨è®ºï¼Œè¯•å›¾åˆ†æå…¶æ˜¯å¦å…·æœ‰ç‰¹æ®Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡ä¸»è¦é‡‡ç”¨æ–‡çŒ®ç»¼è¿°çš„æ–¹æ³•ï¼Œå¯¹ç°æœ‰çš„LLMç›¸å…³ç ”ç©¶è¿›è¡Œæ¢³ç†å’Œæ€»ç»“ã€‚å…·ä½“æ¥è¯´ï¼Œæ–‡ç« é¦–å…ˆä»‹ç»äº†LLMçš„Transformeræ¶æ„ï¼ŒåŒ…æ‹¬QKVè‡ªæ³¨æ„åŠ›æœºåˆ¶ç­‰å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç„¶åï¼Œæ–‡ç« ç»™å‡ºäº†è®¡ç®—ï¼ˆflopsï¼‰å’Œå†…å­˜ï¼ˆå‚æ•°åŠ æ•°æ®ï¼‰çš„æ‰©å±•æ³•åˆ™ï¼Œå¹¶å¯¹ä¸åŒè§„æ¨¡LLMçš„å‚æ•°æˆæœ¬è¿›è¡Œäº†ä¼°ç®—ã€‚æœ€åï¼Œæ–‡ç« å¯¹DeepSeekæ¨¡å‹è¿›è¡Œäº†è®¨è®ºï¼Œè¯•å›¾åˆ†æå…¶æ˜¯å¦å…·æœ‰ç‰¹æ®Šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¯¹LLMç›¸å…³ä¿¡æ¯çš„æ•´åˆå’Œæ€»ç»“ã€‚è™½ç„¶æ–‡ç« æ²¡æœ‰æå‡ºä»»ä½•æ–°çš„æŠ€æœ¯æ–¹æ³•ï¼Œä½†å®ƒå°†åˆ†æ•£åœ¨ä¸åŒè®ºæ–‡å’ŒæŠ¥å‘Šä¸­çš„ä¿¡æ¯æ•´åˆåˆ°ä¸€ä¸ªæ–‡æ¡£ä¸­ï¼Œä¸ºè¯»è€…æä¾›äº†ä¸€ä¸ªå¿«é€Ÿå…¥é—¨LLMçš„é€”å¾„ã€‚è¿™ç§æ€»ç»“æ€§çš„å·¥ä½œå¯¹äºLLMé¢†åŸŸçš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚

**å…³é”®è®¾è®¡**ï¼šæœ¬æ–‡æ²¡æœ‰æ¶‰åŠå…·ä½“çš„æŠ€æœ¯è®¾è®¡ç»†èŠ‚ã€‚æ–‡ç« ä¸»è¦å…³æ³¨å¯¹ç°æœ‰LLMç›¸å…³ä¿¡æ¯çš„æ¢³ç†å’Œæ€»ç»“ï¼ŒåŒ…æ‹¬Transformeræ¶æ„ã€æ‰©å±•æ³•åˆ™å’Œç»æµæˆæœ¬ç­‰æ–¹é¢ã€‚å¯¹äºè¿™äº›æ–¹é¢ï¼Œæ–‡ç« éƒ½ç»™å‡ºäº†ç®€æ´æ˜äº†çš„æè¿°ï¼Œæ–¹ä¾¿è¯»è€…å¿«é€Ÿç†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

æœ¬æ–‡çš„ä¸»è¦äº®ç‚¹åœ¨äºå¯¹LLMæ¶æ„ã€æ‰©å±•æ³•åˆ™å’Œç»æµæˆæœ¬è¿›è¡Œäº†æ€»ç»“ï¼Œå¹¶å¯¹ä¸åŒè§„æ¨¡LLMçš„å‚æ•°æˆæœ¬è¿›è¡Œäº†ä¼°ç®—ã€‚è™½ç„¶æ²¡æœ‰æä¾›å…·ä½“çš„æ€§èƒ½æ•°æ®æˆ–å¯¹æ¯”åŸºçº¿ï¼Œä½†ä¸ºè¯»è€…æä¾›äº†ä¸€ä¸ªå¿«é€Ÿäº†è§£LLMå…¨è²Œçš„é€”å¾„ï¼Œå¹¶ä¸ºLLMçš„éƒ¨ç½²å’Œä¼˜åŒ–æä¾›äº†å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æ€»ç»“å¯åº”ç”¨äºAIç ”ç©¶äººå‘˜å¿«é€Ÿäº†è§£LLMæ¶æ„ã€æ‰©å±•æ³•åˆ™å’Œç»æµæ€§ï¼ŒæŒ‡å¯¼æ¨¡å‹é€‰æ‹©ã€è®­ç»ƒå’Œéƒ¨ç½²ã€‚åŒæ—¶ï¼Œå¯¹LLMçš„æˆæœ¬ä¼°ç®—æœ‰åŠ©äºä¼˜åŒ–èµ„æºåˆ†é…ï¼Œæ¨åŠ¨LLMåœ¨å„è¡Œä¸šçš„åº”ç”¨ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The current standard architecture of Large Language Models (LLMs) with QKV self-attention is briefly summarized, including the architecture of a typical Transformer. Scaling laws for compute (flops) and memory (parameters plus data) are given, along with their present (2025) rough cost estimates for the parameters of present LLMs of various scales, including discussion of whether DeepSeek should be viewed as a special case. Nothing here is new, but this material seems not otherwise readily available in summary form.

