---
layout: default
title: CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models
---

# CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09675" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.09675v1</a>
  <a href="https://arxiv.org/pdf/2509.09675.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09675v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09675v1', 'CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Runpeng Dai, Linfeng Song, Haolin Liu, Zhenwen Liang, Dian Yu, Haitao Mi, Zhaopeng Tu, Rui Liu, Tong Zheng, Hongtu Zhu, Dong Yu

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-11

**Â§áÊ≥®**: 21 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â•ΩÂ•áÂøÉÈ©±Âä®Êé¢Á¥¢(CDE)Ê°ÜÊû∂ÔºåÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÊé¢Á¥¢ÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `Â•ΩÂ•áÂøÉÈ©±Âä®Êé¢Á¥¢` `Êé¢Á¥¢Â•ñÂä±` `Á≠ñÁï•Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâRLVRÊñπÊ≥ïÂú®Â¢ûÂº∫LLMÊé®ÁêÜËÉΩÂäõÊó∂ÔºåÂ≠òÂú®Êé¢Á¥¢‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂØºËá¥Ê®°ÂûãËøáÊó©Êî∂ÊïõÂíåÁÜµÂ¥©Ê∫É„ÄÇ
2. CDEÊ°ÜÊû∂Âà©Áî®Ê®°ÂûãËá™Ë∫´ÁöÑÂ•ΩÂ•áÂøÉÊåáÂØºÊé¢Á¥¢ÔºåÈÄöËøáactorÁöÑÂõ∞ÊÉëÂ∫¶ÂíåcriticÁöÑ‰ª∑ÂÄº‰º∞ËÆ°ÊñπÂ∑Æ‰Ωú‰∏∫Êé¢Á¥¢Â•ñÂä±„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåCDEÂú®AIMEÂü∫ÂáÜÊµãËØï‰∏≠Áõ∏ÊØîÊ†áÂáÜRLVRÊñπÊ≥ïÊúâÊòæËëóÊèêÂçáÔºåÂπ∂Êè≠Á§∫‰∫ÜRLVR‰∏≠ÁöÑÊ†°ÂáÜÂ¥©Ê∫ÉÊú∫Âà∂„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â•ΩÂ•áÂøÉÈ©±Âä®Êé¢Á¥¢(CDE)ÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)Âú®Âü∫‰∫éÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†(RLVR)‰∏≠Êé¢Á¥¢‰∏çË∂≥ÔºåÂØºËá¥ËøáÊó©Êî∂ÊïõÂíåÁÜµÂ¥©Ê∫ÉÁöÑÈóÆÈ¢ò„ÄÇCDEÂà©Áî®Ê®°ÂûãËá™Ë∫´ÁöÑÂÜÖÂú®Â•ΩÂ•áÂøÉÊù•ÊåáÂØºÊé¢Á¥¢ÔºåÈÄöËøáactorÂíåcriticÁöÑ‰ø°Âè∑Êù•ÂΩ¢ÂºèÂåñÂ•ΩÂ•áÂøÉÔºöÂØπ‰∫éactorÔºå‰ΩøÁî®ÁîüÊàêÂìçÂ∫îÁöÑÂõ∞ÊÉëÂ∫¶ÔºõÂØπ‰∫écriticÔºå‰ΩøÁî®Â§öÂ§¥Êû∂ÊûÑ‰∏≠‰ª∑ÂÄº‰º∞ËÆ°ÁöÑÊñπÂ∑Æ„ÄÇËøô‰∫õ‰ø°Âè∑‰Ωú‰∏∫RLVRÊ°ÜÊû∂ÂÜÖÁöÑÊé¢Á¥¢Â•ñÂä±Êù•ÂºïÂØºÊ®°Âûã„ÄÇÁêÜËÆ∫ÂàÜÊûêË°®ÊòéÔºåactorÂ•ñÂä±ËÉΩÂ§üÊÉ©ÁΩöËøáÂ∫¶Ëá™‰ø°ÁöÑÈîôËØØÂπ∂‰øÉËøõÊ≠£Á°ÆÂìçÂ∫îÁöÑÂ§öÊ†∑ÊÄßÔºõcriticÂ•ñÂä±‰∏éÂº∫ÂåñÂ≠¶‰π†‰∏≠Âü∫‰∫éËÆ°Êï∞ÁöÑÊé¢Á¥¢Â•ñÂä±Áõ∏ÂÖ≥„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®AIMEÂü∫ÂáÜÊµãËØï‰∏≠ÔºåCDE‰ΩøÁî®GRPO/PPOÁÆóÊ≥ïÁõ∏ÊØîÊ†áÂáÜRLVRÊñπÊ≥ïÊèêÂçá‰∫ÜÁ∫¶3‰∏™ÁôæÂàÜÁÇπ„ÄÇËøõ‰∏ÄÊ≠•ÂàÜÊûêÊè≠Á§∫‰∫ÜRLVR‰∏≠ÁöÑÊ†°ÂáÜÂ¥©Ê∫ÉÊú∫Âà∂ÔºåÈòêÊòé‰∫ÜLLMÂ∏∏ËßÅÁöÑÂ§±Ë¥•Ê®°Âºè„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†(RLVR)ÊñπÊ≥ïÂú®ËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)Êó∂ÔºåÈù¢‰∏¥Êé¢Á¥¢ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇÊ®°ÂûãÂÆπÊòìÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºòÔºåÊó†Ê≥ïÂÖÖÂàÜÊé¢Á¥¢ÊΩúÂú®ÁöÑËß£Á©∫Èó¥ÔºåÂØºËá¥ÊÄßËÉΩÂèóÈôê„ÄÇÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÊúâÊïàÁöÑÊé¢Á¥¢Êú∫Âà∂ÔºåÈöæ‰ª•Âπ≥Ë°°Âà©Áî®(exploitation)ÂíåÊé¢Á¥¢(exploration)„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂºïÂÖ•Â•ΩÂ•áÂøÉÈ©±Âä®ÁöÑÊé¢Á¥¢(Curiosity-Driven Exploration, CDE)Êú∫Âà∂ÔºåÂà©Áî®Ê®°ÂûãËá™Ë∫´ÁöÑÂÜÖÂú®Â•ΩÂ•áÂøÉÊù•ÂºïÂØºÊé¢Á¥¢ËøáÁ®ã„ÄÇÈÄöËøáÂ•ñÂä±Ê®°ÂûãÊé¢Á¥¢Êñ∞ÁöÑ„ÄÅ‰∏çÁ°ÆÂÆöÁöÑÁä∂ÊÄÅÔºåÈºìÂä±Ê®°ÂûãË∑≥Âá∫Â±ÄÈÉ®ÊúÄ‰ºòÔºåÂèëÁé∞Êõ¥‰ºòÁöÑÁ≠ñÁï•„ÄÇËøôÁßçÊñπÊ≥ïÊó®Âú®ÊèêÈ´òÊé¢Á¥¢ÊïàÁéáÔºåÈÅøÂÖçËøáÊó©Êî∂Êïõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCDEÊ°ÜÊû∂Âú®Ê†áÂáÜÁöÑRLVRÊ°ÜÊû∂Âü∫Á°Ä‰∏äÔºåÂ¢ûÂä†‰∫ÜÂ•ΩÂ•áÂøÉÂ•ñÂä±È°π„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö1) LLM (Actor) ÁîüÊàêÂìçÂ∫îÔºõ2) CriticËØÑ‰º∞ÂìçÂ∫îÁöÑË¥®ÈáèÂπ∂ÁªôÂá∫Â•ñÂä±Ôºõ3) ActorÊ†πÊçÆÂ•ñÂä±Êõ¥Êñ∞Á≠ñÁï•Ôºõ4) ÂêåÊó∂ÔºåËÆ°ÁÆóActorÁöÑÂõ∞ÊÉëÂ∫¶ÂíåCriticÁöÑ‰ª∑ÂÄº‰º∞ËÆ°ÊñπÂ∑ÆÔºå‰Ωú‰∏∫Â•ΩÂ•áÂøÉÂ•ñÂä±Ôºõ5) Â∞ÜÂ•ΩÂ•áÂøÉÂ•ñÂä±‰∏éÂ§ñÈÉ®Â•ñÂä±ÁªìÂêàÔºåÂÖ±ÂêåÊåáÂØºActorÁöÑÁ≠ñÁï•Êõ¥Êñ∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCDEÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂà©Áî®ActorÁöÑÂõ∞ÊÉëÂ∫¶ÂíåCriticÁöÑ‰ª∑ÂÄº‰º∞ËÆ°ÊñπÂ∑ÆÊù•ÈáèÂåñÂ•ΩÂ•áÂøÉ„ÄÇActorÁöÑÂõ∞ÊÉëÂ∫¶ÂèçÊò†‰∫ÜÊ®°ÂûãÂØπÁîüÊàêÂìçÂ∫îÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåÂõ∞ÊÉëÂ∫¶Ë∂äÈ´òÔºåËØ¥ÊòéÊ®°ÂûãÂØπËØ•ÂìçÂ∫îË∂äÂ•ΩÂ•á„ÄÇCriticÁöÑ‰ª∑ÂÄº‰º∞ËÆ°ÊñπÂ∑ÆÂèçÊò†‰∫ÜÊ®°ÂûãÂØπÁä∂ÊÄÅ‰ª∑ÂÄºÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåÊñπÂ∑ÆË∂äÈ´òÔºåËØ¥ÊòéÊ®°ÂûãÂØπËØ•Áä∂ÊÄÅË∂äÂ•ΩÂ•á„ÄÇÂ∞ÜËøô‰∏§Áßç‰ø°Âè∑ÁªìÂêàËµ∑Êù•ÔºåÂèØ‰ª•Êõ¥ÂÖ®Èù¢Âú∞Ë°°ÈáèÊ®°ÂûãÁöÑÂ•ΩÂ•áÂøÉÔºåÂπ∂ÊåáÂØºÊé¢Á¥¢„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöActorÁöÑÂ•ΩÂ•áÂøÉÂ•ñÂä±ÈááÁî®ÁîüÊàêÂìçÂ∫îÁöÑÂõ∞ÊÉëÂ∫¶ÔºåÂõ∞ÊÉëÂ∫¶Ë∂äÈ´òÔºåÂ•ñÂä±Ë∂äÈ´ò„ÄÇCriticÁöÑÂ•ΩÂ•áÂøÉÂ•ñÂä±ÈááÁî®Â§öÂ§¥CriticÁΩëÁªú‰∏≠‰ª∑ÂÄº‰º∞ËÆ°ÁöÑÊñπÂ∑ÆÔºåÊñπÂ∑ÆË∂äÈ´òÔºåÂ•ñÂä±Ë∂äÈ´ò„ÄÇÊÄªÂ•ñÂä±ÊòØÂ§ñÈÉ®Â•ñÂä±ÂíåÂ•ΩÂ•áÂøÉÂ•ñÂä±ÁöÑÂä†ÊùÉÂíåÔºåÊùÉÈáçÁ≥ªÊï∞ÊòØÂèØË∞ÉËäÇÁöÑË∂ÖÂèÇÊï∞„ÄÇÊçüÂ§±ÂáΩÊï∞ÁªìÂêà‰∫ÜÁ≠ñÁï•Ê¢ØÂ∫¶ÊçüÂ§±Âíå‰ª∑ÂÄºÂáΩÊï∞ÊçüÂ§±ÔºåÂπ∂Âä†ÂÖ•‰∫ÜÊ≠£ÂàôÂåñÈ°πÔºå‰ª•Èò≤Ê≠¢ËøáÊãüÂêà„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®AIMEÂü∫ÂáÜÊµãËØï‰∏≠ÔºåCDE‰ΩøÁî®GRPO/PPOÁÆóÊ≥ïÁõ∏ÊØîÊ†áÂáÜRLVRÊñπÊ≥ïÊèêÂçá‰∫ÜÁ∫¶3‰∏™ÁôæÂàÜÁÇπ„ÄÇËøôË°®ÊòéCDEËÉΩÂ§üÊúâÊïàÊèêÈ´òLLMÁöÑÊé¢Á¥¢ÊïàÁéáÔºåÂπ∂Â≠¶‰π†Âà∞Êõ¥‰ºòÁöÑÁ≠ñÁï•„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂàÜÊûê‰∫ÜRLVR‰∏≠ÁöÑÊ†°ÂáÜÂ¥©Ê∫ÉÊú∫Âà∂Ôºå‰∏∫ÁêÜËß£LLMÁöÑÂ§±Ë¥•Ê®°ÂºèÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÜËßí„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CDEÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅLLMËøõË°åÁ≠ñÁï•Â≠¶‰π†ÂíåÂÜ≥Á≠ñÁöÑ‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÂØπËØùÁîüÊàê„ÄÅÊñáÊú¨ÊëòË¶Å„ÄÅ‰ª£Á†ÅÁîüÊàê„ÄÅÊ∏∏ÊàèAIÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÊé¢Á¥¢ÊïàÁéáÔºåCDEÂèØ‰ª•Â∏ÆÂä©LLMÊõ¥Âø´Âú∞Â≠¶‰π†Âà∞Êõ¥‰ºòÁöÑÁ≠ñÁï•ÔºåÊèêÂçá‰ªªÂä°ÊÄßËÉΩ„ÄÇËØ•Á†îÁ©∂ÂØπ‰∫éÊèêÂçáLLMÁöÑÈÄöÁî®ÊÄßÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâÔºåÂπ∂ÊúâÊúõÊé®Âä®LLMÂú®Êõ¥Â§öÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.

