---
layout: default
title: All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens
---

# All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09650" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09650v1</a>
  <a href="https://arxiv.org/pdf/2509.09650.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09650v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09650v1', 'All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siddarth Mamidanna, Daking Rai, Ziyu Yao, Yilun Zhou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: EMNLP 2025 Main Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAll-for-Oneå­å›¾ï¼Œä½¿LLMä»…ç”¨æœ«å°¾tokenè®¡ç®—è§£å†³å¿ƒç®—é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¿ƒç®—ä»»åŠ¡` `ä¿¡æ¯ä¼ é€’` `è®¡ç®—è·¯å¾„` `æ¨¡å‹å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¡ç®—ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å†…éƒ¨è®¡ç®—æœºåˆ¶å°šä¸æ˜ç¡®ï¼Œéœ€è¦æ·±å…¥ç ”ç©¶ã€‚
2. è®ºæ–‡æå‡ºAll-for-One (AF1)å­å›¾ï¼Œé€šè¿‡CAMAå’ŒABPæŠ€æœ¯ï¼Œé™åˆ¶è®¡ç®—å‘ç”Ÿåœ¨æœ€åä¸€ä¸ªtokenï¼Œç®€åŒ–è®¡ç®—è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜AF1å­å›¾å¯¹äºå¿ƒç®—ä»»åŠ¡çš„é«˜æ€§èƒ½è‡³å…³é‡è¦ï¼Œä¸”å…·æœ‰è·¨æ¨¡å‹å’Œè¾“å…¥é£æ ¼çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨ä¼—å¤šè®¡ç®—ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶å†…éƒ¨è¿ä½œæœºåˆ¶ä»ä¸æ¸…æ¥šã€‚ç†è®ºä¸Šï¼Œå› æœè‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå¤šå±‚æ„ŸçŸ¥æœºå±‚çš„ç»“åˆä½¿å¾—æ¯ä¸ªtokenéƒ½èƒ½è®¿é—®å¹¶è®¡ç®—åŸºäºæ‰€æœ‰å…ˆå‰tokençš„ä¿¡æ¯ã€‚å®é™…ä¸Šï¼Œè¿™ç§æ“ä½œåœ¨å¤šå¤§ç¨‹åº¦ä¸Šå­˜åœ¨ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œé’ˆå¯¹å¿ƒç®—ä»»åŠ¡ï¼ˆå³é€šè¿‡ä¸‹ä¸€ä¸ªtokené¢„æµ‹ç›´æ¥è¿›è¡Œæ•°å­¦è®¡ç®—ï¼Œè€Œæ— éœ€æ˜¾å¼æ¨ç†ï¼‰ï¼Œæˆ‘ä»¬åˆ†ä¸‰ä¸ªæ­¥éª¤ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼šæŠ‘åˆ¶åˆå§‹å±‚ä¸­ç‰¹å®šäºè¾“å…¥çš„tokenè®¡ç®—ï¼Œé™åˆ¶æ¥ä¸‹æ¥å‡ å±‚ä¸­è·¨tokenä½ç½®çš„ä¿¡æ¯ä¼ é€’è·¯å¾„ï¼Œå¹¶å¼ºåˆ¶æ‰€æœ‰è®¡ç®—åœ¨å‰©ä½™å±‚ä¸­çš„æœ€åä¸€ä¸ªtokenå¤„å‘ç”Ÿã€‚é€šè¿‡ä¸¤ç§æå‡ºçš„æŠ€æœ¯ï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¹³å‡æ¶ˆè(CAMA)å’ŒåŸºäºæ³¨æ„åŠ›çš„çª¥è§†(ABP)ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºä¸€ä¸ªAll-for-Oneå­å›¾(AF1)ï¼Œè¯¥å­å›¾åœ¨å„ç§å¿ƒç®—ä»»åŠ¡ä¸Šå…·æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§ï¼Œå…¶ä¸­æœ‰æ„ä¹‰çš„è®¡ç®—å‘ç”Ÿåœ¨éå¸¸æ™šçš„å±‚ï¼ˆå°±å±‚æ·±åº¦è€Œè¨€ï¼‰ï¼Œå¹¶ä¸”ä»…å‘ç”Ÿåœ¨æœ€åä¸€ä¸ªtokenå¤„ï¼Œè¯¥tokenæ¥æ”¶æ¥è‡ªç‰¹å®šä¸­é—´å±‚ä¸­å…¶ä»–tokençš„ä¿¡æ¯ã€‚åœ¨å„ç§æ¨¡å‹å’Œç®—æœ¯è¡¨è¾¾å¼ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥å­å›¾å¯¹äºé«˜æ¨¡å‹æ€§èƒ½æ˜¯å……åˆ†ä¸”å¿…è¦çš„ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ¨¡å‹ä¹‹é—´è½¬ç§»ï¼Œå¹¶ä¸”é€‚ç”¨äºå„ç§è¾“å…¥æ ·å¼ã€‚å¯¹ä¸åŒCAMAå’ŒABPæ›¿ä»£æ–¹æ¡ˆçš„æ¶ˆèå®éªŒæ­ç¤ºäº†å®ƒä»¬ç›¸å¯¹äºå…¶ä»–æ–¹æ³•çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œè¿™äº›ä¼˜åŠ¿å¯èƒ½å…·æœ‰ç‹¬ç«‹çš„æ„ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨æ¢ç©¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰§è¡Œå¿ƒç®—ä»»åŠ¡æ—¶ï¼Œä¿¡æ¯å¦‚ä½•åœ¨ä¸åŒtokenä¹‹é—´ä¼ é€’å’Œè®¡ç®—ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†LLMè§†ä¸ºé»‘ç›’ï¼Œç¼ºä¹å¯¹å†…éƒ¨è®¡ç®—è¿‡ç¨‹çš„ç»†ç²’åº¦ç†è§£ï¼Œéš¾ä»¥è§£é‡Šå…¶èƒ½åŠ›æ¥æºã€‚è®ºæ–‡å¸Œæœ›é€šè¿‡å®éªŒæ‰‹æ®µæ­ç¤ºLLMåœ¨å¿ƒç®—ä»»åŠ¡ä¸­çš„å…³é”®è®¡ç®—è·¯å¾„å’Œä¿¡æ¯ä¼ é€’æ¨¡å¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é€æ­¥é™åˆ¶LLMçš„è®¡ç®—èƒ½åŠ›ï¼Œæœ€ç»ˆå°†æ‰€æœ‰è®¡ç®—é›†ä¸­åœ¨æœ€åä¸€ä¸ªtokenä¸Šã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥è¯†åˆ«å‡ºå¯¹äºå¿ƒç®—ä»»åŠ¡è‡³å…³é‡è¦çš„ä¿¡æ¯ä¼ é€’è·¯å¾„å’Œè®¡ç®—æ¨¡å—ï¼Œä»è€Œæ­ç¤ºLLMè§£å†³å¿ƒç®—é—®é¢˜çš„å…³é”®æœºåˆ¶ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºâ€œå‰¥æ´‹è‘±â€ï¼Œé€æ­¥å»é™¤ä¸å¿…è¦çš„è®¡ç®—ï¼Œæœ€ç»ˆç•™ä¸‹æ ¸å¿ƒçš„è®¡ç®—è·¯å¾„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼š1) æŠ‘åˆ¶åˆå§‹å±‚ä¸­ç‰¹å®šäºè¾“å…¥çš„tokenè®¡ç®—ï¼›2) é™åˆ¶ä¸­é—´å±‚ä¸­è·¨tokenä½ç½®çš„ä¿¡æ¯ä¼ é€’è·¯å¾„ï¼›3) å¼ºåˆ¶æ‰€æœ‰è®¡ç®—åœ¨å‰©ä½™å±‚ä¸­çš„æœ€åä¸€ä¸ªtokenå¤„å‘ç”Ÿã€‚ä¸ºäº†å®ç°è¿™äº›ç›®æ ‡ï¼Œè®ºæ–‡æå‡ºäº†ä¸¤ç§å…³é”®æŠ€æœ¯ï¼šContext-Aware Mean Ablation (CAMA) å’Œ Attention-Based Peeking (ABP)ã€‚CAMAç”¨äºæŠ‘åˆ¶ä¸å¿…è¦çš„tokenè®¡ç®—ï¼ŒABPç”¨äºæ§åˆ¶ä¿¡æ¯åœ¨ä¸åŒtokenä¹‹é—´çš„ä¼ é€’ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†All-for-One (AF1)å­å›¾çš„æ¦‚å¿µï¼Œå¹¶è¯æ˜äº†è¯¥å­å›¾å¯¹äºå¿ƒç®—ä»»åŠ¡çš„é«˜æ€§èƒ½è‡³å…³é‡è¦ã€‚AF1å­å›¾è¡¨æ˜ï¼ŒLLMåœ¨è§£å†³å¿ƒç®—é—®é¢˜æ—¶ï¼Œå¹¶éæ‰€æœ‰tokenéƒ½å‚ä¸è®¡ç®—ï¼Œè€Œæ˜¯å°†å¤§éƒ¨åˆ†è®¡ç®—é›†ä¸­åœ¨æœ€åä¸€ä¸ªtokenä¸Šï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„ä¿¡æ¯ä¼ é€’è·¯å¾„å°†å…¶ä»–tokençš„ä¿¡æ¯ä¼ é€’ç»™æœ€åä¸€ä¸ªtokenã€‚è¿™ç§è®¡ç®—æ¨¡å¼ä¸ä¼ ç»Ÿçš„åˆ†å¸ƒå¼è®¡ç®—æ¨¡å¼ä¸åŒï¼Œæ­ç¤ºäº†LLMåœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„é«˜æ•ˆè®¡ç®—ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šCAMAé€šè¿‡è®¡ç®—æ¯ä¸ªtokenåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„å¹³å‡æ¿€æ´»å€¼ï¼Œå¹¶å°†å…¶ä½œä¸ºæ¶ˆèçš„åŸºå‡†ï¼Œä»è€Œå®ç°å¯¹ä¸å¿…è¦tokenè®¡ç®—çš„æŠ‘åˆ¶ã€‚ABPé€šè¿‡ä¿®æ”¹æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…è®¸æœ€åä¸€ä¸ªtokenâ€œçª¥è§†â€å…¶ä»–tokençš„ä¿¡æ¯ï¼Œä½†é™åˆ¶å…¶ä»–tokenä¹‹é—´çš„ä¿¡æ¯ä¼ é€’ã€‚è¿™äº›æŠ€æœ¯ç»†èŠ‚çš„è®¾è®¡æ—¨åœ¨ç²¾ç¡®æ§åˆ¶LLMçš„è®¡ç®—è¿‡ç¨‹ï¼Œä»è€Œè¯†åˆ«å‡ºAF1å­å›¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAll-for-One (AF1)å­å›¾å¯¹äºå¿ƒç®—ä»»åŠ¡çš„é«˜æ€§èƒ½æ˜¯å……åˆ†ä¸”å¿…è¦çš„ã€‚åœ¨å„ç§æ¨¡å‹å’Œç®—æœ¯è¡¨è¾¾å¼ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAF1å­å›¾å¯ä»¥åœ¨ä¸åŒçš„æ¨¡å‹ä¹‹é—´è½¬ç§»ï¼Œå¹¶ä¸”é€‚ç”¨äºå„ç§è¾“å…¥æ ·å¼ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒCAMAå’ŒABPæŠ€æœ¯ç›¸å¯¹äºå…¶ä»–æ–¹æ³•å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«å…³é”®è®¡ç®—è·¯å¾„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡LLMçš„è®¡ç®—æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡ç†è§£LLMåœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„å…³é”®è®¡ç®—è·¯å¾„ï¼Œå¯ä»¥è®¾è®¡æ›´é«˜æ•ˆçš„æ¨¡å‹ç»“æ„å’Œè®­ç»ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿæœ‰åŠ©äºå¼€å‘æ›´å¯é çš„LLMï¼Œé¿å…æ¨¡å‹è¿‡åº¦ä¾èµ–æŸäº›tokenæˆ–è®¡ç®—è·¯å¾„ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å¯¹äºå¼€å‘è½»é‡çº§ã€é«˜æ•ˆçš„LLMå…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) demonstrate proficiency across numerous computational tasks, yet their inner workings remain unclear. In theory, the combination of causal self-attention and multilayer perceptron layers allows every token to access and compute information based on all preceding tokens. In practice, to what extent are such operations present? In this paper, on mental math tasks (i.e., direct math calculation via next-token prediction without explicit reasoning), we investigate this question in three steps: inhibiting input-specific token computations in the initial layers, restricting the routes of information transfer across token positions in the next few layers, and forcing all computation to happen at the last token in the remaining layers. With two proposed techniques, Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with high accuracy on a wide variety of mental math tasks, where meaningful computation occurs very late (in terms of layer depth) and only at the last token, which receives information of other tokens in few specific middle layers. Experiments on a variety of models and arithmetic expressions show that this subgraph is sufficient and necessary for high model performance, transfers across different models, and works on a variety of input styles. Ablations on different CAMA and ABP alternatives reveal their unique advantages over other methods, which may be of independent interest.

