---
layout: default
title: Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing
---

# Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09160" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09160v1</a>
  <a href="https://arxiv.org/pdf/2509.09160.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09160v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09160v1', 'Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiyue Liu, Fanrong Ma, Xin Ling

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: Accepted by the IEEE International Conference on Multimedia and Expo (ICME 2025). Â© 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åäº‹å®å¢å¼ºå»åæ¡†æ¶ï¼Œç”¨äºè§£å†³ç›®æ ‡å¯¼å‘çš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ç±»ä¸­çš„åè§é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ç±»` `åäº‹å®æ¨ç†` `æ•°æ®å¢å¼º` `å¯¹æ¯”å­¦ä¹ ` `å»åå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç›®æ ‡å¯¼å‘å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ç±»æ–¹æ³•è¿‡åº¦ä¾èµ–æ–‡æœ¬å†…å®¹ï¼Œå¿½ç•¥äº†æ•°æ®é›†ä¸­å­˜åœ¨çš„è¯çº§åˆ«ä¸Šä¸‹æ–‡åå·®ã€‚
2. æå‡ºåäº‹å®å¢å¼ºå»åæ¡†æ¶ï¼Œé€šè¿‡åäº‹å®æ•°æ®å¢å¼ºå’Œè‡ªé€‚åº”å»åå¯¹æ¯”å­¦ä¹ ï¼Œå‡å°‘è™šå‡ç›¸å…³æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•ï¼Œæå‡äº†æƒ…æ„Ÿåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åäº‹å®å¢å¼ºå»åæ¡†æ¶ï¼Œç”¨äºè§£å†³ç›®æ ‡å¯¼å‘çš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ç±»é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸è¿‡åº¦ä¾èµ–æ–‡æœ¬å†…å®¹ï¼Œå¿½ç•¥æ•°æ®é›†åå·®ï¼Œç‰¹åˆ«æ˜¯è¯çº§åˆ«çš„ä¸Šä¸‹æ–‡åå·®ï¼Œå¯¼è‡´æ–‡æœ¬ç‰¹å¾å’Œè¾“å‡ºæ ‡ç­¾ä¹‹é—´äº§ç”Ÿè™šå‡ç›¸å…³æ€§ï¼Œä»è€Œå½±å“åˆ†ç±»å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åäº‹å®æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæœ€å°ç¨‹åº¦åœ°æ”¹å˜æƒ…æ„Ÿç›¸å…³çš„å› æœç‰¹å¾ï¼Œç”Ÿæˆç»†èŠ‚åŒ¹é…çš„å›¾åƒ-æ–‡æœ¬æ ·æœ¬ï¼Œä»¥å¼•å¯¼æ¨¡å‹å…³æ³¨ä¸æƒ…æ„Ÿç›¸å…³çš„å†…å®¹ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä»åäº‹å®æ•°æ®ä¸­å­¦ä¹ é²æ£’ç‰¹å¾å¹¶ä¿ƒè¿›æ¨¡å‹å†³ç­–ï¼Œå¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å»åå¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†æœ‰åè¯çš„å½±å“ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç›®æ ‡å¯¼å‘çš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ç±»æ—¨åœ¨æ ¹æ®ç»™å®šçš„å›¾åƒ-æ–‡æœ¬å¯¹é¢„æµ‹ç‰¹å®šç›®æ ‡çš„æƒ…æ„Ÿææ€§ã€‚ç°æœ‰æ–¹æ³•çš„ä¸€ä¸ªä¸»è¦ç—›ç‚¹æ˜¯å®ƒä»¬å®¹æ˜“å—åˆ°æ•°æ®é›†åå·®çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬ä¸­è¯çº§åˆ«çš„ä¸Šä¸‹æ–‡åå·®ã€‚è¿™ç§åå·®å¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°æ–‡æœ¬ç‰¹å¾å’Œæƒ…æ„Ÿæ ‡ç­¾ä¹‹é—´çš„è™šå‡ç›¸å…³æ€§ï¼Œé™ä½äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åäº‹å®æ¨ç†æ¥å‡å°‘æ¨¡å‹å¯¹åå·®ç‰¹å¾çš„ä¾èµ–ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡ç”Ÿæˆåäº‹å®æ ·æœ¬ï¼Œå³åœ¨ä¿æŒå›¾åƒå†…å®¹ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå¯¹æ–‡æœ¬è¿›è¡Œå¾®å°çš„æƒ…æ„Ÿç›¸å…³ä¿®æ”¹ï¼Œä»è€Œè¿«ä½¿æ¨¡å‹å…³æ³¨å›¾åƒä¸­çš„æƒ…æ„Ÿä¿¡æ¯ï¼Œå¹¶å­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚åŒæ—¶ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ è¿›ä¸€æ­¥åŒºåˆ†åŸå§‹æ ·æœ¬å’Œåäº‹å®æ ·æœ¬ï¼Œä»è€Œå®ç°å»åçš„ç›®çš„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šåäº‹å®æ•°æ®å¢å¼ºæ¨¡å—å’Œè‡ªé€‚åº”å»åå¯¹æ¯”å­¦ä¹ æ¨¡å—ã€‚é¦–å…ˆï¼Œåäº‹å®æ•°æ®å¢å¼ºæ¨¡å—é€šè¿‡æœ€å°åŒ–åœ°æ”¹å˜æƒ…æ„Ÿç›¸å…³çš„å› æœç‰¹å¾ï¼Œç”Ÿæˆç»†èŠ‚åŒ¹é…çš„å›¾åƒ-æ–‡æœ¬æ ·æœ¬ã€‚ç„¶åï¼Œè‡ªé€‚åº”å»åå¯¹æ¯”å­¦ä¹ æ¨¡å—åˆ©ç”¨è¿™äº›åäº‹å®æ ·æœ¬ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œä¿ƒä½¿æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾ï¼Œå¹¶å‡è½»æœ‰åè¯çš„å½±å“ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆè¿›è¡Œæ•°æ®å¢å¼ºï¼Œç„¶ååˆ©ç”¨å¢å¼ºåçš„æ•°æ®è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œæœ€ç»ˆæå‡æƒ…æ„Ÿåˆ†ç±»çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªåäº‹å®å¢å¼ºçš„å»åæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘ç›®æ ‡å¯¼å‘å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ç±»ä¸­çš„åå·®ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸ä»…è€ƒè™‘äº†æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„å…³ç³»ï¼Œè¿˜è€ƒè™‘äº†æ•°æ®é›†ä¸­å­˜åœ¨çš„åå·®ï¼Œå¹¶é€šè¿‡åäº‹å®æ¨ç†çš„æ–¹å¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ›´å…·ä¿¡æ¯é‡çš„æ ·æœ¬ï¼Œå¹¶ä¿ƒä½¿æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åäº‹å®æ•°æ®å¢å¼ºæ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†æœ€å°åŒ–ä¿®æ”¹æƒ…æ„Ÿç›¸å…³å› æœç‰¹å¾çš„ç­–ç•¥ï¼Œä»¥ä¿è¯ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚åœ¨è‡ªé€‚åº”å»åå¯¹æ¯”å­¦ä¹ æ–¹é¢ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªè‡ªé€‚åº”çš„æƒé‡æœºåˆ¶ï¼Œç”¨äºå¹³è¡¡åŸå§‹æ ·æœ¬å’Œåäº‹å®æ ·æœ¬ä¹‹é—´çš„è´¡çŒ®ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé‡‡ç”¨äº†å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œç”¨äºåŒºåˆ†åŸå§‹æ ·æœ¬å’Œåäº‹å®æ ·æœ¬ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†æè¿°ï¼Œéœ€è¦å‚è€ƒåŸæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€ä½³æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡æ­£æ–‡ä¸­æŸ¥æ‰¾ã€‚è¯¥ç ”ç©¶éªŒè¯äº†åäº‹å®å¢å¼ºå»åæ¡†æ¶åœ¨ç›®æ ‡å¯¼å‘å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæƒ…æ„Ÿåˆ†æã€èˆ†æƒ…ç›‘æ§ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨èˆ†æƒ…ç›‘æ§ä¸­ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯†åˆ«ç¤¾äº¤åª’ä½“ä¸Šç”¨æˆ·å¯¹ç‰¹å®šäº‹ä»¶æˆ–äº§å“çš„æƒ…æ„Ÿå€¾å‘ï¼Œä»è€Œä¸ºæ”¿åºœæˆ–ä¼ä¸šæä¾›å†³ç­–æ”¯æŒã€‚åœ¨æ™ºèƒ½å®¢æœä¸­ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„æƒ…æ„Ÿéœ€æ±‚ï¼Œæä¾›æ›´ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚è¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºæå‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨æ›´å¤æ‚çš„ç°å®åœºæ™¯ä¸­å‘æŒ¥ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Target-oriented multimodal sentiment classification seeks to predict sentiment polarity for specific targets from image-text pairs. While existing works achieve competitive performance, they often over-rely on textual content and fail to consider dataset biases, in particular word-level contextual biases. This leads to spurious correlations between text features and output labels, impairing classification accuracy. In this paper, we introduce a novel counterfactual-enhanced debiasing framework to reduce such spurious correlations. Our framework incorporates a counterfactual data augmentation strategy that minimally alters sentiment-related causal features, generating detail-matched image-text samples to guide the model's attention toward content tied to sentiment. Furthermore, for learning robust features from counterfactual data and prompting model decisions, we introduce an adaptive debiasing contrastive learning mechanism, which effectively mitigates the influence of biased words. Experimental results on several benchmark datasets show that our proposed method outperforms state-of-the-art baselines.

