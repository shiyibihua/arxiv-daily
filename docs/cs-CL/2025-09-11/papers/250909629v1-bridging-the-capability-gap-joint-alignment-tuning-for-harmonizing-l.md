---
layout: default
title: Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems
---

# Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09629" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09629v1</a>
  <a href="https://arxiv.org/pdf/2509.09629.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09629v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09629v1', 'Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minghang Zhu, Zhengliang Shi, Zhiwei Xu, Shiguang Wu, Lingjie Wang, Pengjie Ren, Zhaochun Ren, Zhumin Chen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: EMNLP 2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMOATæ¡†æ¶ï¼Œé€šè¿‡è”åˆå¯¹é½è°ƒæ•´æå‡LLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåä½œèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è”åˆå¯¹é½` `è¿­ä»£ä¼˜åŒ–` `å·¥å…·ä½¿ç”¨` `æ™ºèƒ½ä½“åä½œ` `è§„åˆ’æ™ºèƒ½ä½“` `æ‰§è¡Œæ™ºèƒ½ä½“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç‹¬ç«‹å¾®è°ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„å„ä¸ªæ™ºèƒ½ä½“ï¼Œå¯¼è‡´æ™ºèƒ½ä½“é—´èƒ½åŠ›ä¸åŒ¹é…ï¼Œåä½œæ•ˆæœå·®ã€‚
2. MOATæ¡†æ¶é€šè¿‡è¿­ä»£å¯¹é½è§„åˆ’æ™ºèƒ½ä½“å’Œæ‰§è¡Œæ™ºèƒ½ä½“ï¼Œä¼˜åŒ–å­ç›®æ ‡ç”Ÿæˆå’ŒåŠ¨ä½œæ‰§è¡Œï¼Œæå‡æ•´ä½“åä½œèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMOATåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å·²è§å’Œæœªè§ä»»åŠ¡ä¸Šå‡æœ‰æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ä½¿å¾—æ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæˆä¸ºå¯èƒ½ï¼Œé€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºä¸“é—¨çš„æ™ºèƒ½ä½“æ¥è§£å†³ï¼Œä¾‹å¦‚è§„åˆ’æ™ºèƒ½ä½“ç”Ÿæˆå­ç›®æ ‡ï¼Œæ‰§è¡Œæ™ºèƒ½ä½“æ‰§è¡Œå·¥å…·ä½¿ç”¨åŠ¨ä½œã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç‹¬ç«‹åœ°å¾®è°ƒè¿™äº›æ™ºèƒ½ä½“ï¼Œå¯¼è‡´æ™ºèƒ½ä½“ä¹‹é—´å­˜åœ¨èƒ½åŠ›å·®è·ï¼Œåè°ƒæ€§å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MOATï¼Œä¸€ä¸ªå¤šæ™ºèƒ½ä½“è”åˆå¯¹é½è°ƒæ•´æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£å¯¹é½æ¥æé«˜æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œã€‚MOATåœ¨ä¸¤ä¸ªå…³é”®é˜¶æ®µä¹‹é—´äº¤æ›¿è¿›è¡Œï¼šï¼ˆ1ï¼‰è§„åˆ’æ™ºèƒ½ä½“å¯¹é½ï¼Œä¼˜åŒ–è§„åˆ’æ™ºèƒ½ä½“ä»¥ç”Ÿæˆæ›´å¥½åœ°æŒ‡å¯¼æ‰§è¡Œæ™ºèƒ½ä½“çš„å­ç›®æ ‡åºåˆ—ï¼›ï¼ˆ2ï¼‰æ‰§è¡Œæ™ºèƒ½ä½“æ”¹è¿›ï¼Œä½¿ç”¨æ™ºèƒ½ä½“è‡ªèº«ç”Ÿæˆçš„å„ç§å­ç›®æ ‡-åŠ¨ä½œå¯¹æ¥å¾®è°ƒæ‰§è¡Œæ™ºèƒ½ä½“ï¼Œä»¥å¢å¼ºå…¶æ³›åŒ–èƒ½åŠ›ã€‚ç†è®ºåˆ†æè¯æ˜ï¼ŒMOATç¡®ä¿äº†ä¸€ä¸ªéé€’å‡å’Œé€æ­¥æ”¶æ•›çš„è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMOATä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œåœ¨å·²è§ä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†3.1%ï¼Œåœ¨æœªè§ä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†4.4%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œç”±äºå„ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹è®­ç»ƒå¯¼è‡´çš„åä½œèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åˆ†åˆ«å¾®è°ƒè§„åˆ’æ™ºèƒ½ä½“å’Œæ‰§è¡Œæ™ºèƒ½ä½“ï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´è§„åˆ’æ™ºèƒ½ä½“ç”Ÿæˆçš„å­ç›®æ ‡å¯èƒ½éš¾ä»¥è¢«æ‰§è¡Œæ™ºèƒ½ä½“ç†è§£æˆ–æ‰§è¡Œï¼Œä»è€Œå½±å“æ•´ä½“ä»»åŠ¡å®Œæˆæ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMOATçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è”åˆå¯¹é½è°ƒæ•´ï¼Œè¿­ä»£åœ°ä¼˜åŒ–è§„åˆ’æ™ºèƒ½ä½“å’Œæ‰§è¡Œæ™ºèƒ½ä½“ï¼Œä½¿å®ƒä»¬æ›´å¥½åœ°ååŒå·¥ä½œã€‚å…·ä½“æ¥è¯´ï¼Œè§„åˆ’æ™ºèƒ½ä½“è¢«è®­ç»ƒæˆç”Ÿæˆæ›´é€‚åˆæ‰§è¡Œæ™ºèƒ½ä½“æ‰§è¡Œçš„å­ç›®æ ‡ï¼Œè€Œæ‰§è¡Œæ™ºèƒ½ä½“åˆ™é€šè¿‡å­¦ä¹ æ›´å¤šæ ·åŒ–çš„å­ç›®æ ‡-åŠ¨ä½œå¯¹æ¥æé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§è¿­ä»£å¼çš„å¯¹é½è¿‡ç¨‹èƒ½å¤Ÿé€æ­¥ç¼©å°æ™ºèƒ½ä½“ä¹‹é—´çš„èƒ½åŠ›å·®è·ï¼Œæå‡æ•´ä½“ç³»ç»Ÿçš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMOATæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šè§„åˆ’æ™ºèƒ½ä½“å¯¹é½å’Œæ‰§è¡Œæ™ºèƒ½ä½“æ”¹è¿›ã€‚åœ¨è§„åˆ’æ™ºèƒ½ä½“å¯¹é½é˜¶æ®µï¼Œé€šè¿‡ä¼˜åŒ–è§„åˆ’æ™ºèƒ½ä½“ï¼Œä½¿å…¶ç”Ÿæˆçš„å­ç›®æ ‡åºåˆ—èƒ½å¤Ÿæ›´å¥½åœ°æŒ‡å¯¼æ‰§è¡Œæ™ºèƒ½ä½“ã€‚åœ¨æ‰§è¡Œæ™ºèƒ½ä½“æ”¹è¿›é˜¶æ®µï¼Œåˆ©ç”¨æ‰§è¡Œæ™ºèƒ½ä½“è‡ªèº«ç”Ÿæˆçš„å„ç§å­ç›®æ ‡-åŠ¨ä½œå¯¹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå¢å¼ºå…¶æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸¤ä¸ªé˜¶æ®µäº¤æ›¿è¿›è¡Œï¼Œç›´åˆ°ç³»ç»Ÿæ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šMOATçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è”åˆå¯¹é½è°ƒæ•´çš„ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„ç‹¬ç«‹è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒMOATè€ƒè™‘äº†æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°ååŒå·¥ä½œã€‚æ­¤å¤–ï¼ŒMOATè¿˜åˆ©ç”¨æ‰§è¡Œæ™ºèƒ½ä½“è‡ªèº«ç”Ÿæˆçš„æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæé«˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šMOATçš„å…·ä½“å®ç°ç»†èŠ‚åŒ…æ‹¬ï¼šè§„åˆ’æ™ºèƒ½ä½“çš„ä¼˜åŒ–ç›®æ ‡æ˜¯ç”Ÿæˆèƒ½å¤Ÿæœ€å¤§åŒ–æ‰§è¡Œæ™ºèƒ½ä½“å¥–åŠ±çš„å­ç›®æ ‡åºåˆ—ï¼›æ‰§è¡Œæ™ºèƒ½ä½“çš„å¾®è°ƒé‡‡ç”¨äº†ä¸€ç§åŸºäºè‡ªç”Ÿæˆæ•°æ®çš„ç­–ç•¥ï¼Œå³åˆ©ç”¨æ‰§è¡Œæ™ºèƒ½ä½“è‡ªèº«ç”Ÿæˆçš„å­ç›®æ ‡-åŠ¨ä½œå¯¹è¿›è¡Œè®­ç»ƒï¼›æ¡†æ¶çš„æ”¶æ•›æ€§é€šè¿‡ç†è®ºåˆ†æè¿›è¡Œäº†è¯æ˜ï¼Œä¿è¯äº†è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMOATåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚åœ¨å·²è§ä»»åŠ¡ä¸Šï¼ŒMOATå¹³å‡æé«˜äº†3.1%çš„æ€§èƒ½ï¼Œè€Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æœªè§ä»»åŠ¡ä¸Šï¼Œå¹³å‡æé«˜äº†4.4%ã€‚è¿™äº›ç»“æœéªŒè¯äº†MOATæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†è”åˆå¯¹é½è°ƒæ•´ç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MOATæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦å¤šæ™ºèƒ½ä½“åä½œçš„å¤æ‚ä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨åŒ–æµç¨‹è®¾è®¡ã€æ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡æå‡æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œèƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ä»»åŠ¡å®Œæˆæ•ˆç‡å’Œè´¨é‡ï¼Œé™ä½äººå·¥å¹²é¢„æˆæœ¬ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The advancement of large language models (LLMs) has enabled the construction of multi-agent systems to solve complex tasks by dividing responsibilities among specialized agents, such as a planning agent for subgoal generation and a grounding agent for executing tool-use actions. Most existing methods typically fine-tune these agents independently, leading to capability gaps among them with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint Alignment Tuning framework that improves agents collaboration through iterative alignment. MOAT alternates between two key stages: (1) Planning Agent Alignment, which optimizes the planning agent to generate subgoal sequences that better guide the grounding agent; and (2) Grounding Agent Improving, which fine-tunes the grounding agent using diverse subgoal-action pairs generated by the agent itself to enhance its generalization capablity. Theoretical analysis proves that MOAT ensures a non-decreasing and progressively convergent training process. Experiments across six benchmarks demonstrate that MOAT outperforms state-of-the-art baselines, achieving average improvements of 3.1% on held-in tasks and 4.4% on held-out tasks.

