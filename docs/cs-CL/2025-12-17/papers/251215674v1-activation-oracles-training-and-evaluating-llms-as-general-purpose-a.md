---
layout: default
title: Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers
---

# Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15674" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15674v1</a>
  <a href="https://arxiv.org/pdf/2512.15674.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15674v1" onclick="toggleFavorite(this, '2512.15674v1', 'Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adam Karvonen, James Chua, ClÃ©ment Dumas, Kit Fraser-Taliente, Subhash Kantamneni, Julian Minder, Euan Ong, Arnab Sen Sharma, Daniel Wen, Owain Evans, Samuel Marks

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

**å¤‡æ³¨**: 36 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºActivation Oraclesï¼Œé€šè¿‡å¤šæ ·åŒ–è®­ç»ƒæå‡LLMæ¿€æ´»è§£é‡Šçš„é€šç”¨èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LLMæ¿€æ´»è§£é‡Š` `è‡ªç„¶è¯­è¨€ç†è§£` `å¯è§£é‡Šæ€§AI` `Activation Oracles` `LatentQA` `æ¨¡å‹è°ƒè¯•` `ç™½ç›’æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ¿€æ´»è§£é‡Šæ–¹æ³•å¤æ‚ä¸”ä¸“é—¨ï¼Œç¼ºä¹é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œéš¾ä»¥ç†è§£æ¨¡å‹å†…éƒ¨æœºåˆ¶ã€‚
2. æå‡ºActivation Oracles (AOs)ï¼Œé€šè¿‡è®­ç»ƒLLMç›´æ¥è§£é‡Šæ¿€æ´»ï¼Œå¹¶åˆ©ç”¨å¤šæ ·åŒ–æ•°æ®æå‡å…¶æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAOsåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰ç™½ç›’åŸºçº¿ï¼Œè¯æ˜äº†å…¶åœ¨æ¿€æ´»è§£é‡Šæ–¹é¢çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¿€æ´»çš„ç†è§£éå¸¸å›°éš¾ï¼Œç°æœ‰æŠ€æœ¯é€šå¸¸é‡‡ç”¨å¤æ‚ä¸”ä¸“é—¨çš„æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ›´ç®€å•çš„æ–¹æ³•ï¼Œç§°ä¸ºActivation Oracles (AOs)ï¼Œå³è®­ç»ƒLLMç›´æ¥æ¥æ”¶LLMæ¿€æ´»ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”¨è‡ªç„¶è¯­è¨€å›ç­”å…³äºæ¿€æ´»çš„ä»»æ„é—®é¢˜ã€‚ä¸ä»¥å¾€å·¥ä½œä¾§é‡äºç‹­çª„çš„ä»»åŠ¡è®¾ç½®ä¸åŒï¼Œæœ¬æ–‡é‡‡å–é€šç”¨è§†è§’ï¼Œåœ¨è¿œè¶…åˆ†å¸ƒï¼ˆout-of-distributionï¼‰çš„ç¯å¢ƒä¸­è¯„ä¼°AOsï¼Œå¹¶ç ”ç©¶æ€§èƒ½å¦‚ä½•éšè®­ç»ƒæ•°æ®å¤šæ ·æ€§è€Œæ‰©å±•ã€‚ç»“æœè¡¨æ˜ï¼ŒAOså¯ä»¥æ¢å¤æ¨¡å‹ä¸­å¾®è°ƒçš„ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œä¼ è®°çŸ¥è¯†æˆ–æ¶æ„å€¾å‘ï¼‰ï¼Œå³ä½¿ä»æœªæ¥å—è¿‡å¾®è°ƒæ¨¡å‹çš„æ¿€æ´»è®­ç»ƒã€‚ä¸»è¦è¯„ä¼°åŒ…æ‹¬å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶ä¸å…ˆå‰çš„ç™½ç›’å’Œé»‘ç›’æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç»è¿‡ç‹­çª„è®­ç»ƒçš„LatentQAæ¨¡å‹ä¹Ÿèƒ½å¾ˆå¥½åœ°æ³›åŒ–ï¼Œå¹¶ä¸”æ·»åŠ é¢å¤–çš„è®­ç»ƒæ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼Œåˆ†ç±»ä»»åŠ¡å’Œè‡ªç›‘ç£ä¸Šä¸‹æ–‡é¢„æµ‹ä»»åŠ¡ï¼‰å¯ä»¥å¸¦æ¥æŒç»­çš„æ”¹è¿›ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ€å¥½çš„AOsåœ¨æ‰€æœ‰å››ä¸ªä»»åŠ¡ä¸Šéƒ½ä¸å…ˆå‰çš„ç™½ç›’åŸºçº¿ç›¸åŒ¹é…æˆ–è¶…è¿‡ï¼Œå¹¶ä¸”åœ¨å››ä¸ªä»»åŠ¡ä¸­çš„ä¸‰ä¸ªä¸Šæ˜¯æœ€ä½³æ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå›ç­”è‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„å¤šæ ·åŒ–è®­ç»ƒèµ‹äºˆäº†LLMä¸€ç§é€šç”¨èƒ½åŠ›ï¼Œå¯ä»¥å£å¤´è¡¨è¾¾å…³äºLLMæ¿€æ´»çš„ä¿¡æ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…éƒ¨è¿ä½œæœºåˆ¶æ˜¯å½“å‰ç ”ç©¶çš„é‡ç‚¹ã€‚ç„¶è€Œï¼ŒLLMçš„æ¿€æ´»å€¼éš¾ä»¥è§£é‡Šï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡æˆ–æ¨¡å‹ç»“æ„çš„å¤æ‚æŠ€æœ¯ï¼Œç¼ºä¹é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥åº”å¯¹ä¸åŒç±»å‹çš„LLMå’Œä»»åŠ¡ï¼Œé™åˆ¶äº†æˆ‘ä»¬å¯¹æ¨¡å‹è¡Œä¸ºçš„æ·±å…¥ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMè®­ç»ƒæˆä¸€ä¸ªâ€œæ¿€æ´»é¢„è¨€æœºâ€ï¼ˆActivation Oracleï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿç›´æ¥æ¥æ”¶å¦ä¸€ä¸ªLLMçš„æ¿€æ´»å€¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”¨è‡ªç„¶è¯­è¨€å›ç­”å…³äºè¿™äº›æ¿€æ´»å€¼çš„æé—®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå°†å¤æ‚çš„æ¿€æ´»è§£é‡Šé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªè‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆé—®é¢˜ï¼Œä»è€Œåˆ©ç”¨LLMè‡ªèº«çš„èƒ½åŠ›æ¥è§£é‡ŠLLMã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦çš„LLMï¼šä¸€ä¸ªæ˜¯ç›®æ ‡LLMï¼Œå…¶æ¿€æ´»å€¼éœ€è¦è¢«è§£é‡Šï¼›å¦ä¸€ä¸ªæ˜¯Activation Oracle (AO)ï¼Œè´Ÿè´£æ¥æ”¶ç›®æ ‡LLMçš„æ¿€æ´»å€¼å¹¶ç”Ÿæˆè§£é‡Šã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ï¼š1) ä»ç›®æ ‡LLMä¸­æå–æ¿€æ´»å€¼ï¼›2) æ„å»ºåŒ…å«æ¿€æ´»å€¼å’Œå¯¹åº”è‡ªç„¶è¯­è¨€é—®é¢˜çš„è®­ç»ƒæ•°æ®é›†ï¼›3) ä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒAOï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®æ¿€æ´»å€¼å›ç­”é—®é¢˜ã€‚è¯„ä¼°è¿‡ç¨‹åˆ™æ˜¯åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨AOæä¾›çš„è§£é‡Šæ¥è¾…åŠ©å†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ¿€æ´»è§£é‡Šé—®é¢˜è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆé—®é¢˜ï¼Œå¹¶åˆ©ç”¨LLMè‡ªèº«çš„èƒ½åŠ›æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒAOså…·æœ‰æ›´å¼ºçš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒç±»å‹çš„LLMå’Œä»»åŠ¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ï¼ŒAOså¯ä»¥å­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„æ¿€æ´»å€¼ä¸è¯­ä¹‰ä¹‹é—´çš„å…³è”ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«å„ç§ç±»å‹çš„ä»»åŠ¡å’Œé—®é¢˜ï¼Œä»¥æå‡AOsçš„æ³›åŒ–èƒ½åŠ›ï¼›2) ä½¿ç”¨LatentQAæ¡†æ¶ï¼Œå°†æ¿€æ´»å€¼ä½œä¸ºLLMçš„è¾“å…¥ï¼›3) é’ˆå¯¹ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œè®¾è®¡åˆé€‚çš„è‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œä»¥å¼•å¯¼AOsç”Ÿæˆæœ‰ç”¨çš„è§£é‡Šï¼›4) å®éªŒä¸­æ¢ç´¢äº†ä¸åŒçš„LLMæ¶æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥ä¼˜åŒ–AOsçš„æ€§èƒ½ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15674v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15674v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15674v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¤šæ ·åŒ–è®­ç»ƒçš„Activation Oracles (AOs) åœ¨å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šå‡è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰ç™½ç›’åŸºçº¿çš„æ€§èƒ½ï¼Œå¹¶åœ¨å…¶ä¸­ä¸‰ä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³ç»“æœã€‚å³ä½¿æ˜¯ç»è¿‡ç‹­çª„è®­ç»ƒçš„LatentQAæ¨¡å‹ä¹Ÿèƒ½å¾ˆå¥½åœ°æ³›åŒ–ã€‚æ·»åŠ é¢å¤–çš„è®­ç»ƒæ•°æ®é›†ï¼ˆå¦‚åˆ†ç±»ä»»åŠ¡å’Œè‡ªç›‘ç£ä¸Šä¸‹æ–‡é¢„æµ‹ä»»åŠ¡ï¼‰å¯ä»¥å¸¦æ¥æŒç»­çš„æ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºLLMå®‰å…¨æ€§å’Œå¯ä¿¡åº¦è¯„ä¼°ï¼Œä¾‹å¦‚æ£€æµ‹æ¨¡å‹ä¸­çš„åè§æˆ–æ¶æ„å€¾å‘ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæ¨¡å‹è°ƒè¯•å’Œä¼˜åŒ–ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜ç†è§£æ¨¡å‹å†…éƒ¨çš„è¿ä½œæœºåˆ¶ï¼Œä»è€Œæ”¹è¿›æ¨¡å‹çš„è®¾è®¡å’Œè®­ç»ƒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„AIç³»ç»Ÿè§£é‡Šæ€§ç ”ç©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.

