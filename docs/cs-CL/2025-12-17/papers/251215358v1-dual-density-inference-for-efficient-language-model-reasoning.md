---
layout: default
title: Dual-Density Inference for Efficient Language Model Reasoning
---

# Dual-Density Inference for Efficient Language Model Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15358" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15358v1</a>
  <a href="https://arxiv.org/pdf/2512.15358.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15358v1" onclick="toggleFavorite(this, '2512.15358v1', 'Dual-Density Inference for Efficient Language Model Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengyi Zhao, Shubo Zhang, Yuxi Zhang, Huimin Wang, Binyang Li, Kam-Fai Wong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDenseråŒå¯†åº¦æ¨ç†æ¡†æ¶ï¼Œæå‡LLMåœ¨å¤æ‚æ¨ç†é—®ç­”ä»»åŠ¡ä¸­çš„æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒå¯†åº¦æ¨ç†` `è¯­è¨€æ¨¡å‹` `é«˜æ•ˆæ¨ç†` `é—®ç­”ç³»ç»Ÿ` `å‹ç¼©æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ¨ç†æ–¹æ³•åœ¨ä¸­é—´æ­¥éª¤å’Œæœ€ç»ˆç­”æ¡ˆä¸­ä½¿ç”¨ç›¸åŒè¯­è¨€å¯†åº¦ï¼Œå¯¼è‡´è®¡ç®—å†—ä½™ã€‚
2. Denseræ¡†æ¶é€šè¿‡åŒºåˆ†æ¨ç†å’Œå›ç­”é˜¶æ®µï¼Œåˆ†åˆ«ä¼˜åŒ–ä¿¡æ¯å¯†åº¦ï¼Œå®ç°é«˜æ•ˆæ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDenseråœ¨ä¿æŒæˆ–æå‡å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†tokenæ¶ˆè€—ï¼Œæå‡æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ–¹æ³•å¯¹ä¸­é—´æ¨ç†å’Œæœ€ç»ˆç­”æ¡ˆéƒ½é‡‡ç”¨ç»Ÿä¸€çš„è¯­è¨€å¯†åº¦ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬çš„è§‚å¯Ÿå‘ç°ï¼Œæ¨ç†è¿‡ç¨‹ä¸ºæ¨¡å‹è‡ªèº«æœåŠ¡ï¼Œè€Œå›ç­”åˆ™ä¸ºäººç±»ç†è§£æœåŠ¡ã€‚è¿™ç§åŒºåˆ«ä½¿å¾—å¯ä»¥ä½¿ç”¨å‹ç¼©çš„ã€ç¬¦å·ä¸°å¯Œçš„è¯­è¨€è¿›è¡Œä¸­é—´è®¡ç®—ï¼ŒåŒæ—¶ä¿æŒäººç±»å¯è¯»çš„æœ€ç»ˆè§£é‡Šã€‚ä¸ºäº†è§£å†³è¿™ç§ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Denserï¼šåŒå¯†åº¦æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ†åˆ«ä¼˜åŒ–æ¨ç†å’Œå›ç­”é˜¶æ®µçš„ä¿¡æ¯å¯†åº¦ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ä¸‰ä¸ªç»„ä»¶å®ç°è¿™ä¸€ç‚¹ï¼šä¸€ä¸ªåˆ†æè¾“å…¥é—®é¢˜çš„æŸ¥è¯¢å¤„ç†æ¨¡å—ï¼Œä¸€ä¸ªç”¨äºé«˜æ•ˆä¸­é—´è®¡ç®—çš„é«˜å¯†åº¦å‹ç¼©æ¨ç†æœºåˆ¶ï¼Œä»¥åŠä¸€ä¸ªå°†å‹ç¼©æ¨ç†è½¬æ¢ä¸ºäººç±»å¯è¯»è§£å†³æ–¹æ¡ˆçš„ç­”æ¡ˆç”Ÿæˆç»„ä»¶ã€‚è·¨å¤šä¸ªæ¨ç†é—®ç­”åŸºå‡†çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸æ ‡å‡†çš„æ€ç»´é“¾æ–¹æ³•ç›¸æ¯”ï¼ŒDenseræœ€å¤šå¯å‡å°‘62%çš„tokenæ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜å‡†ç¡®æ€§ã€‚è¿™äº›æ•ˆç‡æå‡å¯¹äºä¼ ç»Ÿæ–¹æ³•ç”Ÿæˆå¤§é‡è§£é‡Šçš„å¤æ‚å¤šæ­¥éª¤æ¨ç†é—®é¢˜å°¤å…¶é‡è¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†é—®ç­”ä»»åŠ¡ä¸­è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚Chain-of-Thoughtï¼Œåœ¨æ¨ç†çš„ä¸­é—´æ­¥éª¤å’Œæœ€ç»ˆç­”æ¡ˆä¸­éƒ½ä½¿ç”¨ç›¸åŒçš„è¯­è¨€å¯†åº¦ï¼Œäº§ç”Ÿäº†å¤§é‡å†—ä½™çš„tokenï¼Œå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚è¿™ç§ç»Ÿä¸€çš„å¯†åº¦å¿½ç•¥äº†æ¨ç†è¿‡ç¨‹ä¸»è¦æ˜¯ä¸ºæ¨¡å‹è‡ªèº«æœåŠ¡ï¼Œè€Œå›ç­”æ‰æ˜¯ä¸ºäº†äººç±»ç†è§£æœåŠ¡çš„æœ¬è´¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDenserçš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨â€œåŒå¯†åº¦â€æ¨ç†ï¼Œå³å¯¹ä¸­é—´æ¨ç†è¿‡ç¨‹ä½¿ç”¨é«˜å¯†åº¦ã€å‹ç¼©çš„è¯­è¨€è¡¨ç¤ºï¼Œè€Œå¯¹æœ€ç»ˆç­”æ¡ˆåˆ™ä½¿ç”¨äººç±»å¯è¯»çš„è‡ªç„¶è¯­è¨€ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨å†…éƒ¨é«˜æ•ˆåœ°è¿›è¡Œè®¡ç®—å’Œæ¨ç†ï¼ŒåŒæ—¶ä»ç„¶èƒ½å¤Ÿæä¾›æ¸…æ™°æ˜“æ‡‚çš„è§£é‡Šã€‚è¿™ç§è®¾è®¡åŸºäºè§‚å¯Ÿï¼šæ¨ç†è¿‡ç¨‹æ˜¯æ¨¡å‹å†…éƒ¨çš„è®¡ç®—è¿‡ç¨‹ï¼Œå¯ä»¥ä½¿ç”¨æ›´ç´§å‡‘çš„è¡¨ç¤ºï¼Œè€Œæœ€ç»ˆç­”æ¡ˆéœ€è¦æ˜“äºäººç±»ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDenseræ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) **æŸ¥è¯¢å¤„ç†æ¨¡å—**ï¼šè´Ÿè´£åˆ†æè¾“å…¥é—®é¢˜ï¼Œç†è§£é—®é¢˜çš„éœ€æ±‚å’Œçº¦æŸã€‚2) **é«˜å¯†åº¦å‹ç¼©æ¨ç†æœºåˆ¶**ï¼šä½¿ç”¨å‹ç¼©çš„ã€ç¬¦å·ä¸°å¯Œçš„è¯­è¨€è¿›è¡Œä¸­é—´æ¨ç†è®¡ç®—ï¼Œå‡å°‘tokenæ•°é‡ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚3) **ç­”æ¡ˆç”Ÿæˆæ¨¡å—**ï¼šå°†å‹ç¼©çš„æ¨ç†ç»“æœç¿»è¯‘æˆäººç±»å¯è¯»çš„è‡ªç„¶è¯­è¨€ç­”æ¡ˆï¼Œç¡®ä¿æœ€ç»ˆè¾“å‡ºçš„å¯ç†è§£æ€§ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼šè¾“å…¥é—®é¢˜ -> æŸ¥è¯¢å¤„ç† -> å‹ç¼©æ¨ç† -> ç­”æ¡ˆç”Ÿæˆ -> è¾“å‡ºç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šDenseræœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå…¶â€œåŒå¯†åº¦â€æ¨ç†çš„æ€æƒ³ï¼Œå®ƒæ‰“ç ´äº†ä¼ ç»Ÿæ–¹æ³•ä¸­æ¨ç†å’Œå›ç­”ä½¿ç”¨ç»Ÿä¸€è¯­è¨€å¯†åº¦çš„é™åˆ¶ï¼Œæ ¹æ®ä¸åŒé˜¶æ®µçš„éœ€æ±‚é‡‡ç”¨ä¸åŒçš„è¡¨ç¤ºæ–¹å¼ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—å‡å°‘tokenæ¶ˆè€—ï¼Œæé«˜è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡å‡†ç¡®ç‡ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒDenseræ›´åŠ å…³æ³¨æ¨ç†è¿‡ç¨‹çš„è®¡ç®—æ•ˆç‡ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è¿›è¡Œäº†ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†è¯´æ˜å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ã€‚ä½†æ˜¯ï¼Œé«˜å¯†åº¦å‹ç¼©æ¨ç†æœºåˆ¶çš„è®¾è®¡æ˜¯å…³é”®ï¼Œå¯èƒ½æ¶‰åŠåˆ°ç‰¹å®šçš„ç¼–ç æ–¹å¼ã€çŸ¥è¯†è¡¨ç¤ºæ–¹æ³•æˆ–ç¬¦å·åŒ–æŠ€æœ¯ï¼Œä»¥å®ç°é«˜æ•ˆçš„æ¨ç†è®¡ç®—ã€‚ç­”æ¡ˆç”Ÿæˆæ¨¡å—å¯èƒ½éœ€è¦ä½¿ç”¨ä¸€äº›è‡ªç„¶è¯­è¨€ç”ŸæˆæŠ€æœ¯ï¼Œå°†å‹ç¼©çš„æ¨ç†ç»“æœè½¬æ¢æˆæµç•…ã€è‡ªç„¶çš„è¯­è¨€ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15358v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15358v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15358v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDenseråœ¨å¤šä¸ªæ¨ç†é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸æ ‡å‡†çš„Chain-of-Thoughtæ–¹æ³•ç›¸æ¯”ï¼Œæœ€å¤šå¯å‡å°‘62%çš„tokenæ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜å‡†ç¡®æ€§ã€‚å°¤å…¶æ˜¯åœ¨å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†é—®é¢˜ä¸­ï¼ŒDenserçš„æ•ˆç‡æå‡æ›´ä¸ºæ˜¾è‘—ã€‚è¿™äº›ç»“æœéªŒè¯äº†Denseræ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Denseræ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†çš„é—®ç­”ç³»ç»Ÿï¼Œä¾‹å¦‚ç§‘å­¦é—®ç­”ã€æ•°å­¦é—®é¢˜æ±‚è§£ã€å¸¸è¯†æ¨ç†ç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—æˆæœ¬ï¼ŒDenserå¯ä»¥ä½¿LLMåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ›´é«˜æ•ˆåœ°è¿è¡Œï¼Œå¹¶ä¿ƒè¿›LLMåœ¨ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿç­‰é¢†åŸŸçš„åº”ç”¨ã€‚æœªæ¥ï¼ŒDenserçš„æ€è·¯å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–NLPä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown impressive capabilities in complex reasoning tasks. However, current approaches employ uniform language density for both intermediate reasoning and final answers, leading to computational inefficiency. Our observation found that reasoning process serves a computational function for the model itself, while answering serves a communicative function for human understanding. This distinction enables the use of compressed, symbol-rich language for intermediate computations while maintaining human-readable final explanations. To address this inefficiency, we present Denser: \underline{D}ual-d\underline{ens}ity inf\underline{er}ence, a novel framework that optimizes information density separately for reasoning and answering phases. Our framework implements this through three components: a query processing module that analyzes input problems, a high-density compressed reasoning mechanism for efficient intermediate computations, and an answer generation component that translates compressed reasoning into human-readable solutions. Experimental evaluation across multiple reasoning question answering benchmarks demonstrates that Denser reduces token consumption by up to 62\% compared to standard Chain-of-Thought methods while preserving or improving accuracy. These efficiency gains are particularly significant for complex multi-step reasoning problems where traditional methods generate extensive explanations.

