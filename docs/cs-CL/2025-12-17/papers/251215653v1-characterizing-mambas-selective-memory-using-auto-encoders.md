---
layout: default
title: Characterizing Mamba's Selective Memory using Auto-Encoders
---

# Characterizing Mamba's Selective Memory using Auto-Encoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15653" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15653v1</a>
  <a href="https://arxiv.org/pdf/2512.15653.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15653v1" onclick="toggleFavorite(this, '2512.15653v1', 'Characterizing Mamba\'s Selective Memory using Auto-Encoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tamanna Hossain, Robert L. Logan, Ganesh Jagadeesan, Sameer Singh, Joel Tetreault, Alejandro Jaimes

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

**å¤‡æ³¨**: AACL 2025. Oral Presentation

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨è‡ªç¼–ç å™¨å‰–æMambaé€‰æ‹©æ€§è®°å¿†çš„é—å¿˜ç‰¹æ€§ï¼Œæ­ç¤ºå…¶åœ¨ç‰¹å®šç±»å‹ä¿¡æ¯ä¸Šçš„è®°å¿†çŸ­æ¿ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŠ¶æ€ç©ºé—´æ¨¡å‹` `Mamba` `é€‰æ‹©æ€§è®°å¿†` `è‡ªç¼–ç å™¨` `ä¿¡æ¯æŸå¤±` `è¯­è¨€å»ºæ¨¡` `é•¿åºåˆ—å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹SSMè¯­è¨€æ¨¡å‹é—å¿˜ä¿¡æ¯ç±»å‹çš„ç»†è‡´åˆ»ç”»ï¼Œé™åˆ¶äº†å¯¹æ¨¡å‹è®°å¿†æœºåˆ¶çš„æ·±å…¥ç†è§£ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨è‡ªç¼–ç å™¨é‡å»ºSSMéšè—çŠ¶æ€ä¸­çš„åºåˆ—ï¼Œé€šè¿‡æ¯”è¾ƒè¾“å…¥å’Œé‡å»ºç»“æœæ¥é‡åŒ–ä¿¡æ¯æŸå¤±ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMambaåœ¨æ•°å­¦ç›¸å…³tokensã€ç»„ç»‡å®ä½“æåŠå’Œéæ ‡å‡†ç¾å¼è‹±è¯­æ–¹è¨€ä¸Šæ›´å®¹æ˜“å‘ç”Ÿä¿¡æ¯æŸå¤±ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŠ¶æ€ç©ºé—´æ¨¡å‹(SSMs)å› å…¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨å›ºå®šå†…å­˜ï¼Œæˆä¸ºè¯­è¨€å»ºæ¨¡ä¸­Transformerçš„ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¿™ç§å›ºå®šçš„å†…å­˜ä½¿ç”¨æ–¹å¼éœ€è¦åœ¨å¤„ç†é•¿åºåˆ—æ—¶ï¼Œåœ¨éšè—çŠ¶æ€ä¸­æŸå¤±ä¸€äº›ä¿¡æ¯ã€‚è™½ç„¶ä¹‹å‰çš„å·¥ä½œå·²ç»ç ”ç©¶äº†å‘ç”Ÿä¿¡æ¯æŸå¤±çš„åºåˆ—é•¿åº¦ï¼Œä½†å¹¶æ²¡æœ‰æè¿°SSMè¯­è¨€æ¨¡å‹(LMs)å€¾å‘äºå¿˜è®°çš„ä¿¡æ¯ç±»å‹ã€‚æœ¬æ–‡é€šè¿‡è¯†åˆ«SSM LMsæ›´é¢‘ç¹å¿˜è®°çš„tokensç±»å‹ï¼ˆä¾‹å¦‚ï¼Œè¯æ€§ã€å‘½åå®ä½“ï¼‰å’Œåºåˆ—ç±»å‹ï¼ˆä¾‹å¦‚ï¼Œä»£ç ã€æ•°å­¦é—®é¢˜ï¼‰æ¥å¡«è¡¥è¿™ä¸€çŸ¥è¯†ç©ºç™½ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒä¸€ä¸ªè‡ªç¼–ç å™¨ä»SSMçš„éšè—çŠ¶æ€é‡å»ºåºåˆ—ï¼Œå¹¶é€šè¿‡æ¯”è¾ƒè¾“å…¥å’Œé‡å»ºç»“æœæ¥è¡¡é‡ä¿¡æ¯æŸå¤±ã€‚æˆ‘ä»¬ä½¿ç”¨Mambaç³»åˆ—çš„SSM LMsï¼ˆ1.3äº¿--14äº¿å‚æ•°ï¼‰åœ¨4--256ä¸ªtokensçš„åºåˆ—ä¸Šè¿›è¡Œå®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œä¸æ•°å­¦ç›¸å…³çš„tokensï¼ˆä¾‹å¦‚ï¼Œæ•°å­—ã€å˜é‡ï¼‰ã€ç»„ç»‡å®ä½“æåŠä»¥åŠæ ‡å‡†ç¾å¼è‹±è¯­çš„æ›¿ä»£æ–¹è¨€çš„ä¿¡æ¯æŸå¤±ç‡æ˜æ˜¾æ›´é«˜ã€‚ç„¶åï¼Œæˆ‘ä»¬æ£€æŸ¥è¿™äº›tokensåœ¨Mambaé¢„è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„é¢‘ç‡ï¼Œå‘ç°ä¸å¤ªå¸¸è§çš„tokenså¾€å¾€æ˜¯Mambaæœ€å®¹æ˜“å¿˜è®°çš„ã€‚é€šè¿‡è¯†åˆ«è¿™äº›æ¨¡å¼ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ˜ç¡®çš„æ–¹å‘ï¼Œä»¥å¼€å‘æ›´å¥½åœ°æ§åˆ¶Mambaä¿ç•™é‡è¦ä¿¡æ¯èƒ½åŠ›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œç‰¹åˆ«æ˜¯Mambaæ¨¡å‹ï¼Œåœ¨å¤„ç†é•¿åºåˆ—æ—¶ï¼Œç”±äºå›ºå®šå†…å­˜é™åˆ¶è€Œå¯¼è‡´çš„ä¿¡æ¯é—å¿˜é—®é¢˜ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ä¿¡æ¯æŸå¤±å‘ç”Ÿçš„åºåˆ—é•¿åº¦ï¼Œè€Œå¿½ç•¥äº†å¯¹é—å¿˜ä¿¡æ¯ç±»å‹çš„å…·ä½“åˆ†æã€‚è¿™é˜»ç¢äº†å¯¹SSMè®°å¿†æœºåˆ¶çš„æ·±å…¥ç†è§£å’Œæ”¹è¿›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªç¼–ç å™¨æ¥è¯„ä¼°Mambaæ¨¡å‹çš„ä¿¡æ¯ä¿ç•™èƒ½åŠ›ã€‚é€šè¿‡å°†Mambaçš„éšè—çŠ¶æ€ä½œä¸ºè‡ªç¼–ç å™¨çš„è¾“å…¥ï¼Œå¹¶è®­ç»ƒè‡ªç¼–ç å™¨é‡å»ºåŸå§‹è¾“å…¥åºåˆ—ï¼Œå¯ä»¥é‡åŒ–Mambaæ¨¡å‹çš„ä¿¡æ¯æŸå¤±ã€‚ä¿¡æ¯æŸå¤±è¶Šå¤§ï¼Œè¡¨æ˜Mambaæ¨¡å‹å¯¹è¯¥ç±»å‹ä¿¡æ¯çš„è®°å¿†èƒ½åŠ›è¶Šå¼±ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«Mambaæ¨¡å‹å®¹æ˜“é—å¿˜çš„ä¿¡æ¯ç±»å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨Mambaæ¨¡å‹å¤„ç†è¾“å…¥åºåˆ—ï¼Œè·å¾—éšè—çŠ¶æ€ï¼›2) å°†Mambaçš„éšè—çŠ¶æ€è¾“å…¥åˆ°è‡ªç¼–ç å™¨ä¸­ï¼›3) è®­ç»ƒè‡ªç¼–ç å™¨é‡å»ºåŸå§‹è¾“å…¥åºåˆ—ï¼›4) æ¯”è¾ƒåŸå§‹è¾“å…¥åºåˆ—å’Œè‡ªç¼–ç å™¨çš„é‡å»ºåºåˆ—ï¼Œè®¡ç®—ä¿¡æ¯æŸå¤±ã€‚ä¿¡æ¯æŸå¤±çš„è®¡ç®—æ–¹å¼å¯ä»¥æ˜¯å¤šç§ï¼Œä¾‹å¦‚è®¡ç®—é‡å»ºåºåˆ—å’ŒåŸå§‹åºåˆ—ä¹‹é—´çš„äº¤å‰ç†µæˆ–å‡æ–¹è¯¯å·®ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†è‡ªç¼–ç å™¨åº”ç”¨äºåˆ†æSSMè¯­è¨€æ¨¡å‹çš„è®°å¿†ç‰¹æ€§ã€‚é€šè¿‡è‡ªç¼–ç å™¨ï¼Œå¯ä»¥æœ‰æ•ˆåœ°é‡åŒ–SSMæ¨¡å‹çš„ä¿¡æ¯æŸå¤±ï¼Œå¹¶è¯†åˆ«æ¨¡å‹å®¹æ˜“é—å¿˜çš„ä¿¡æ¯ç±»å‹ã€‚è¿™ç§æ–¹æ³•ä¸ºç ”ç©¶SSMæ¨¡å‹çš„è®°å¿†æœºåˆ¶æä¾›äº†ä¸€ç§æ–°çš„è§†è§’å’Œå·¥å…·ã€‚

**å…³é”®è®¾è®¡**ï¼šè‡ªç¼–ç å™¨çš„å…·ä½“ç»“æ„å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œé€‰æ‹©ã€‚è®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†æ ‡å‡†çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œå¹¶é‡‡ç”¨äº†é€‚å½“çš„æ¿€æ´»å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ã€‚æŸå¤±å‡½æ•°é€šå¸¸é€‰æ‹©äº¤å‰ç†µæˆ–å‡æ–¹è¯¯å·®ï¼Œç”¨äºè¡¡é‡é‡å»ºåºåˆ—å’ŒåŸå§‹åºåˆ—ä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯èƒ½å¯¹Mambaæ¨¡å‹çš„éšè—çŠ¶æ€è¿›è¡Œäº†å½’ä¸€åŒ–æˆ–ç¼©æ”¾ç­‰é¢„å¤„ç†æ“ä½œï¼Œä»¥æé«˜è‡ªç¼–ç å™¨çš„é‡å»ºæ•ˆæœã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15653v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15653v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15653v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMambaæ¨¡å‹åœ¨å¤„ç†æ•°å­¦ç›¸å…³tokensï¼ˆå¦‚æ•°å­—ã€å˜é‡ï¼‰ã€ç»„ç»‡å®ä½“æåŠä»¥åŠéæ ‡å‡†ç¾å¼è‹±è¯­æ–¹è¨€æ—¶ï¼Œä¿¡æ¯æŸå¤±ç‡æ˜¾è‘—é«˜äºå…¶ä»–ç±»å‹çš„ä¿¡æ¯ã€‚è¿›ä¸€æ­¥åˆ†æå‘ç°ï¼ŒMambaæ¨¡å‹æ›´å®¹æ˜“å¿˜è®°åœ¨é¢„è®­ç»ƒæ•°æ®ä¸­å‡ºç°é¢‘ç‡è¾ƒä½çš„tokensã€‚è¿™äº›å‘ç°ä¸ºæ”¹è¿›Mambaæ¨¡å‹çš„è®°å¿†æœºåˆ¶æä¾›äº†é‡è¦çš„çº¿ç´¢ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡SSMè¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨æ•°å­¦ã€ä»£ç æˆ–ç‰¹å®šæ–¹è¨€æ–‡æœ¬å¤„ç†ä¸­ã€‚é€šè¿‡äº†è§£æ¨¡å‹çš„è®°å¿†çŸ­æ¿ï¼Œå¯ä»¥é’ˆå¯¹æ€§åœ°ä¼˜åŒ–æ¨¡å‹ç»“æ„æˆ–è®­ç»ƒæ•°æ®ï¼Œæé«˜æ¨¡å‹åœ¨è¿™äº›é¢†åŸŸçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯ç”¨äºè¯„ä¼°å…¶ä»–ç±»å‹çš„è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯ä¿ç•™èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> State space models (SSMs) are a promising alternative to transformers for language modeling because they use fixed memory during inference. However, this fixed memory usage requires some information loss in the hidden state when processing long sequences. While prior work has studied the sequence length at which this information loss occurs, it does not characterize the types of information SSM language models (LMs) tend to forget. In this paper, we address this knowledge gap by identifying the types of tokens (e.g., parts of speech, named entities) and sequences (e.g., code, math problems) that are more frequently forgotten by SSM LMs. We achieve this by training an auto-encoder to reconstruct sequences from the SSM's hidden state, and measure information loss by comparing inputs with their reconstructions. We perform experiments using the Mamba family of SSM LMs (130M--1.4B) on sequences ranging from 4--256 tokens. Our results show significantly higher rates of information loss on math-related tokens (e.g., numbers, variables), mentions of organization entities, and alternative dialects to Standard American English. We then examine the frequency that these tokens appear in Mamba's pretraining data and find that less prevalent tokens tend to be the ones Mamba is most likely to forget. By identifying these patterns, our work provides clear direction for future research to develop methods that better control Mamba's ability to retain important information.

