---
layout: default
title: Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings
---

# Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15001" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15001v1</a>
  <a href="https://arxiv.org/pdf/2506.15001.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15001v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15001v1', 'Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ignacio Sastre, Aiala RosÃ¡

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

**å¤‡æ³¨**: This paper will be presented at The First Workshop on Large Language Model Memorization (L2M2) at ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯é€†å¥å­åµŒå…¥ç”Ÿæˆæ–¹æ³•ä»¥æå‡æ–‡æœ¬é‡æ„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯é€†å¥å­åµŒå…¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬é‡æ„` `è®°å¿†æ ‡è®°` `ä¿¡æ¯æ£€ç´¢` `æ–‡æœ¬ç”Ÿæˆ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå¥å­åµŒå…¥æ—¶ï¼Œæ— æ³•å®ç°åŸå§‹æ–‡æœ¬çš„å‡†ç¡®é‡æ„ï¼Œé™åˆ¶äº†å…¶åœ¨æŸäº›åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¼•å…¥ç‰¹æ®Šçš„è®°å¿†æ ‡è®°ï¼Œä¼˜åŒ–å…¶åµŒå…¥ä»¥å®ç°å¯é€†å¥å­åµŒå…¥ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿé‡æ„åŸå§‹æ–‡æœ¬ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLlama 3.1 8Bæ¨¡å‹åœ¨å¤šç§è¯­è¨€å’Œé•¿åºåˆ—ä¸Šå‡èƒ½æˆåŠŸé‡æ„æ–‡æœ¬ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶è§‚å¯Ÿåˆ°ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šå¯ä»¥ç”Ÿæˆå¯é€†çš„å¥å­åµŒå…¥ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®é‡æ„åŸå§‹æ–‡æœ¬ã€‚è¿™æ˜¯é€šè¿‡å¼•å…¥ä¸€ç§ç‰¹æ®Šçš„è®°å¿†æ ‡è®°å®ç°çš„ï¼Œè¯¥æ ‡è®°çš„åµŒå…¥é€šè¿‡åœ¨å›ºå®šåºåˆ—ä¸Šè¿›è¡Œè®­ç»ƒè¿›è¡Œä¼˜åŒ–ã€‚å½“ä½¿ç”¨è¯¥åµŒå…¥è¿›è¡Œæç¤ºæ—¶ï¼Œæ¨¡å‹èƒ½å¤Ÿå‡†ç¡®é‡æ„è¯¥å›ºå®šåºåˆ—ã€‚æˆ‘ä»¬åœ¨è‹±è¯­å’Œè¥¿ç­ç‰™è¯­æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¿™ä¸€ç°è±¡ï¼Œåºåˆ—é•¿åº¦è¾¾åˆ°çº¦240ä¸ªæ ‡è®°ï¼Œæ¨¡å‹è§„æ¨¡ä»1äº¿åˆ°80äº¿å‚æ•°ä¸ç­‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLlama 3.1 8BæˆåŠŸé‡æ„äº†æ‰€æœ‰æµ‹è¯•åºåˆ—ã€‚æˆ‘ä»¬çš„å‘ç°çªæ˜¾äº†LLMçš„ä¸€ä¸ªæœ‰è¶£èƒ½åŠ›ï¼Œå¹¶æš—ç¤ºäº†åœ¨åŸºäºè®°å¿†çš„æ£€ç´¢ã€å‹ç¼©å’Œå—æ§æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„æ½œåœ¨åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå¥å­åµŒå…¥æ—¶æ— æ³•å‡†ç¡®é‡æ„åŸå§‹æ–‡æœ¬çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ–‡æœ¬é‡æ„èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå½±å“äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯å¼•å…¥ä¸€ç§ç‰¹æ®Šçš„è®°å¿†æ ‡è®°ï¼Œé€šè¿‡å¯¹è¯¥æ ‡è®°çš„åµŒå…¥è¿›è¡Œä¼˜åŒ–ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¥æ”¶åˆ°è¯¥åµŒå…¥æ—¶å‡†ç¡®é‡æ„å›ºå®šåºåˆ—ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ä¸æ”¹å˜æƒé‡çš„æƒ…å†µä¸‹ï¼Œå…·å¤‡äº†é‡æ„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¼•å…¥è®°å¿†æ ‡è®°çš„åµŒå…¥ä¼˜åŒ–è¿‡ç¨‹ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ—¶é€šè¿‡å›ºå®šåºåˆ—è¿›è¡Œå­¦ä¹ ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è®°å¿†æ ‡è®°çš„ç”Ÿæˆã€åµŒå…¥ä¼˜åŒ–å’Œé‡æ„è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡è®°å¿†æ ‡è®°å®ç°äº†å¯é€†å¥å­åµŒå…¥çš„ç”Ÿæˆï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•å‘åµŒå…¥ç”Ÿæˆå½¢æˆäº†æœ¬è´¨åŒºåˆ«ï¼Œæä¾›äº†æ–°çš„æ–‡æœ¬é‡æ„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œè®ºæ–‡å¯¹è®°å¿†æ ‡è®°çš„åµŒå…¥è¿›è¡Œäº†ä¸“é—¨çš„ä¼˜åŒ–ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ç¡®ä¿é‡æ„çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹è§„æ¨¡ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLlama 3.1 8Bæ¨¡å‹åœ¨æ‰€æœ‰æµ‹è¯•çš„åºåˆ—ä¸Šå‡æˆåŠŸé‡æ„ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†é•¿è¾¾240ä¸ªæ ‡è®°çš„æ–‡æœ¬æ—¶çš„å¼ºå¤§èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œæ¨¡å‹åœ¨å¤šç§è¯­è¨€å’Œä¸åŒè§„æ¨¡ä¸‹å‡å…·å¤‡è‰¯å¥½çš„é‡æ„æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŸºäºè®°å¿†çš„æ£€ç´¢ç³»ç»Ÿã€æ–‡æœ¬å‹ç¼©æŠ€æœ¯ä»¥åŠå—æ§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡å®ç°å¯é€†å¥å­åµŒå…¥ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨éœ€è¦ç²¾ç¡®é‡æ„æ–‡æœ¬çš„åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæå‡ä¿¡æ¯æ£€ç´¢å’Œç”Ÿæˆçš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we observe an interesting phenomenon: it is possible to generate reversible sentence embeddings that allow an LLM to reconstruct the original text exactly, without modifying the model's weights. This is achieved by introducing a special memory token, whose embedding is optimized through training on a fixed sequence. When prompted with this embedding, the model reconstructs the fixed sequence exactly. We evaluate this phenomenon across English and Spanish datasets, sequences of up to approximately 240 tokens, and model scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B successfully reconstructs all tested sequences. Our findings highlight an interesting capability of LLMs and suggest potential applications in memory-based retrieval, compression, and controlled text generation.

