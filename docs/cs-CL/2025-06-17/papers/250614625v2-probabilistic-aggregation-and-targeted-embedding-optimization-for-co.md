---
layout: default
title: Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models
---

# Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14625" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14625v2</a>
  <a href="https://arxiv.org/pdf/2506.14625.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14625v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14625v2', 'Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenchen Yuan, Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17 (æ›´æ–°: 2025-06-18)

**å¤‡æ³¨**: Accepted to ACL 2025 (Findings)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé›†ä½“é“å¾·æ¨ç†æ¡†æ¶ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„é“å¾·åˆ¤æ–­åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é“å¾·æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹èšåˆ` `åµŒå…¥ä¼˜åŒ–` `é“å¾·ä¸€è‡´æ€§` `äººå·¥æ™ºèƒ½ä¼¦ç†` `æ¦‚ç‡æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚é“å¾·å›°å¢ƒæ—¶è¡¨ç°å‡ºåå·®ï¼Œç¼ºä¹ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚
2. æå‡ºäº†ä¸€ç§èšåˆå¤šä¸ªæ¨¡å‹é“å¾·åˆ¤æ–­çš„æ¡†æ¶ï¼Œé€šè¿‡åŠ æƒå’Œä¼˜åŒ–å®ç°é›†ä½“é“å¾·åˆ¤æ–­ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é“å¾·åˆ¤æ–­ä¸€è‡´æ€§å’Œæ¨¡å‹å¿ å®åº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é“å¾·æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚çš„å¤šå› ç´ é“å¾·å›°å¢ƒä¸­å¸¸å¸¸å‡ºç°åå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®å¼‚ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡ç»¼åˆå¤šä¸ªLLMsçš„é“å¾·åˆ¤æ–­å½¢æˆé›†ä½“é“å¾·åˆ¤æ–­ï¼Œå¹¶å¯¹æ˜¾è‘—åç¦»å…±è¯†çš„æ¨¡å‹è¿›è¡Œé‡æ–°æ ¡å‡†ã€‚æˆ‘ä»¬çš„èšåˆæœºåˆ¶å°†è¿ç»­çš„é“å¾·å¯æ¥å—æ€§è¯„åˆ†èåˆä¸ºé›†ä½“æ¦‚ç‡ï¼Œå¹¶æ ¹æ®æ¨¡å‹çš„å¯é æ€§åŠ æƒè´¡çŒ®ã€‚å¯¹äºä¸ä¸€è‡´çš„æ¨¡å‹ï¼Œé‡‡ç”¨é’ˆå¯¹æ€§çš„åµŒå…¥ä¼˜åŒ–ç¨‹åºå¾®è°ƒé“å¾·å“²å­¦ç†è®ºçš„æ ‡è®°åµŒå…¥ï¼Œæœ€å°åŒ–ä¸å…±è¯†çš„JSæ•£åº¦ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡ç¤¾ä¼šé“å¾·å›°å¢ƒæ•°æ®é›†ä¸Šæ„å»ºäº†ç¨³å¥çš„å…±è¯†ï¼Œå¹¶æé«˜äº†å•ä¸ªæ¨¡å‹çš„å¿ å®åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é“å¾·å›°å¢ƒä¸­åˆ¤æ–­åå·®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå•ä¸€æ¨¡å‹çš„åˆ¤æ–­ï¼Œå¯¼è‡´ç»“æœä¸ä¸€è‡´ï¼Œç¼ºä¹å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç»¼åˆå¤šä¸ªæ¨¡å‹çš„é“å¾·åˆ¤æ–­ï¼Œå½¢æˆä¸€ä¸ªé›†ä½“é“å¾·åˆ¤æ–­ï¼Œå¹¶å¯¹åç¦»å…±è¯†çš„æ¨¡å‹è¿›è¡Œæ ¡å‡†ã€‚èšåˆæœºåˆ¶ä½¿ç”¨è¿ç»­çš„é“å¾·å¯æ¥å—æ€§è¯„åˆ†ï¼Œæå‡åˆ¤æ–­çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé“å¾·åˆ¤æ–­èšåˆæ¨¡å—å’Œé’ˆå¯¹æ€§åµŒå…¥ä¼˜åŒ–æ¨¡å—ã€‚å‰è€…è´Ÿè´£å°†å¤šä¸ªæ¨¡å‹çš„åˆ¤æ–­èåˆä¸ºé›†ä½“æ¦‚ç‡ï¼Œåè€…åˆ™å¾®è°ƒä¸ä¸€è‡´æ¨¡å‹çš„åµŒå…¥ä»¥å‡å°‘ä¸å…±è¯†çš„å·®å¼‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§åŸºäºæ¦‚ç‡çš„èšåˆæœºåˆ¶ï¼Œèƒ½å¤Ÿå¤„ç†è¿ç»­è¯„åˆ†è€Œéç®€å•çš„äºŒå…ƒæ ‡ç­¾ï¼ŒåŒæ—¶å¼•å…¥äº†é’ˆå¯¹æ€§çš„åµŒå…¥ä¼˜åŒ–ï¼Œç¡®ä¿è¯­ä¹‰çš„å®Œæ•´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨èšåˆè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹çš„è´¡çŒ®æ ¹æ®å…¶å¯é æ€§è¿›è¡ŒåŠ æƒï¼Œä¼˜åŒ–è¿‡ç¨‹ä¸­é‡‡ç”¨JSæ•£åº¦ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œç¡®ä¿æ¨¡å‹åœ¨é“å¾·å“²å­¦ç†è®ºä¸Šçš„ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤§è§„æ¨¡ç¤¾ä¼šé“å¾·å›°å¢ƒæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†é“å¾·åˆ¤æ–­çš„ä¸€è‡´æ€§ï¼Œæ¨¡å‹çš„å¿ å®åº¦æå‡å¹…åº¦è¶…è¿‡20%ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œé›†ä½“é“å¾·åˆ¤æ–­çš„å‡†ç¡®æ€§å’Œå¯é æ€§å‡æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é“å¾·å†³ç­–æ”¯æŒç³»ç»Ÿã€è‡ªåŠ¨åŒ–ä¼¦ç†å®¡æŸ¥å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡å®ç°å¤šä¸ªæ¨¡å‹çš„é“å¾·ä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿæå‡AIç³»ç»Ÿåœ¨å¤æ‚é“å¾·æƒ…å¢ƒä¸‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œæ¨åŠ¨æ›´ä¸ºä¸€è‡´çš„é“å¾·å†³ç­–ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„AIåº”ç”¨ä¸­å¾—åˆ°æ¨å¹¿ï¼Œä¿ƒè¿›äººæœºåä½œä¸­çš„é“å¾·è€ƒé‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.

