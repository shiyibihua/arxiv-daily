---
layout: default
title: Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs
---

# Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14731" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14731v2</a>
  <a href="https://arxiv.org/pdf/2506.14731.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14731v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14731v2', 'Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ling Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, Zujie Wen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17 (æ›´æ–°: 2025-06-18)

**å¤‡æ³¨**: Technical Report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRing-liteä»¥è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸“å®¶æ··åˆæ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `è’¸é¦è®­ç»ƒ` `æ¨ç†æ•ˆç‡` `å¤šé¢†åŸŸæ•°æ®é›†æˆ` `æ¨¡å‹ä¼˜åŒ–` `è®¡ç®—ç¨³å®šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ•ˆç‡å’Œç¨³å®šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡å‚æ•°æ¿€æ´»æ—¶ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆè’¸é¦ä¸å¼ºåŒ–å­¦ä¹ çš„è”åˆè®­ç»ƒæ–¹æ³•ï¼Œå¹¶å¼•å…¥C3POä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRing-liteåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ¿€æ´»å‚æ•°æ•°é‡æ˜¾è‘—ä½äºåŒç±»æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†Ring-liteï¼Œä¸€ç§åŸºäºä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰çš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–ä»¥å®ç°é«˜æ•ˆä¸”ç¨³å¥çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åŸºäºå…¬å¼€çš„Ling-liteæ¨¡å‹ï¼Œæ‹¥æœ‰168äº¿å‚æ•°ï¼Œå…¶ä¸­275äº¿å‚æ•°è¢«æ¿€æ´»ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ˆå¦‚AIMEã€LiveCodeBenchã€GPQA-Diamondï¼‰ä¸Šï¼Œä¸æœ€å…ˆè¿›çš„å°è§„æ¨¡æ¨ç†æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ï¼ŒåŒæ—¶ä»…æ¿€æ´»äº†å¯æ¯”æ¨¡å‹æ‰€éœ€å‚æ•°çš„ä¸‰åˆ†ä¹‹ä¸€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»“åˆè’¸é¦ä¸RLçš„è”åˆè®­ç»ƒæµç¨‹ï¼Œæ­ç¤ºäº†MoE RLè®­ç»ƒä¸­çš„æœªè®°å½•æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†çº¦æŸä¸Šä¸‹æ–‡è®¡ç®—ç­–ç•¥ä¼˜åŒ–ï¼ˆC3POï¼‰ï¼Œå¢å¼ºäº†è®­ç»ƒç¨³å®šæ€§å¹¶æé«˜äº†è®¡ç®—ååé‡ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œä»¥åè°ƒå¤šé¢†åŸŸæ•°æ®é›†æˆï¼Œè§£å†³æ··åˆæ•°æ®é›†è®­ç»ƒä¸­å‡ºç°çš„é¢†åŸŸå†²çªã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ•ˆç‡å’Œç¨³å®šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ï¼Œå¾€å¾€éœ€è¦æ¿€æ´»å¤§é‡å‚æ•°ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹å’Œè®­ç»ƒä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºRing-liteæ¨¡å‹ï¼Œé€šè¿‡ç»“åˆè’¸é¦ä¸å¼ºåŒ–å­¦ä¹ çš„è”åˆè®­ç»ƒæ–¹æ³•ï¼Œä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘æ¿€æ´»å‚æ•°çš„æ•°é‡ã€‚C3POç­–ç•¥çš„å¼•å…¥ï¼Œæ—¨åœ¨æå‡è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) è’¸é¦æ¨¡å—ï¼Œé€šè¿‡é€‰æ‹©åŸºäºç†µæŸå¤±çš„æ£€æŸ¥ç‚¹è¿›è¡Œè®­ç»ƒï¼›2) å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼Œåˆ©ç”¨C3POç­–ç•¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼›3) ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œåè°ƒå¤šé¢†åŸŸæ•°æ®é›†æˆï¼Œè§£å†³é¢†åŸŸå†²çªã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºC3POç­–ç•¥çš„æå‡ºï¼Œå®ƒé€šè¿‡ç®—æ³•ä¸ç³»ç»Ÿçš„ååŒè®¾è®¡ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚è¿™ä¸€æ–¹æ³•åœ¨MoE RLè®­ç»ƒä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºç†µæŸå¤±çš„æ£€æŸ¥ç‚¹é€‰æ‹©ç­–ç•¥ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µè·å¾—æ›´å¥½çš„æ€§èƒ½ä¸æ•ˆç‡å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRing-liteåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿä¸æœ€å…ˆè¿›çš„å°è§„æ¨¡æ¨ç†æ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶ä»…æ¿€æ´»ä¸‰åˆ†ä¹‹ä¸€çš„å‚æ•°ã€‚é€šè¿‡C3POç­–ç•¥ï¼Œè®­ç»ƒç¨³å®šæ€§æ˜¾è‘—æé«˜ï¼Œè®¡ç®—æ•ˆç‡ä¹Ÿå¾—åˆ°äº†ä¼˜åŒ–ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Ring-liteæ¨¡å‹çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿã€æ™ºèƒ½é—®ç­”ç­‰ã€‚å…¶é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›å’Œè¾ƒä½çš„å‚æ•°æ¿€æ´»éœ€æ±‚ï¼Œä½¿å…¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å…·æœ‰å®é™…ä»·å€¼ï¼Œèƒ½å¤Ÿæ¨åŠ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ä¸æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.

