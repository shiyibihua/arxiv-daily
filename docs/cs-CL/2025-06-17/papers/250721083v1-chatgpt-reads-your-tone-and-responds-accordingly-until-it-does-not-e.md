---
layout: default
title: ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs
---

# ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.21083" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.21083v1</a>
  <a href="https://arxiv.org/pdf/2507.21083.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.21083v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.21083v1', 'ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Franck Bardol

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/bardolfranck/llm-responses-viewer)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨æƒ…æ„Ÿæ¡†æ¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æƒ…æ„Ÿåˆ†æ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹åå·®` `AIå¯¹é½` `ç”¨æˆ·ä¿¡ä»»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿæ¡†æ¶ä¸‹çš„å“åº”åå·®å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œå¯èƒ½å½±å“æ¨¡å‹çš„å¯é æ€§å’Œä¿¡ä»»åº¦ã€‚
2. æœ¬æ–‡é€šè¿‡ç³»ç»Ÿå˜åŒ–æç¤ºçš„æƒ…æ„ŸåŸºè°ƒï¼Œæ¢è®¨å…¶å¯¹GPT-4å“åº”çš„å½±å“ï¼Œæå‡ºäº†æ–°çš„é‡åŒ–æ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4å¯¹è´Ÿé¢é—®é¢˜çš„å“åº”æ˜¾è‘—å‡å°‘ï¼Œå°¤å…¶åœ¨æ•æ„Ÿè¯é¢˜ä¸Šï¼Œæ¨¡å‹çš„æƒ…æ„Ÿå“åº”è¢«æŠ‘åˆ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰ä¸ä»…æ ¹æ®æé—®å†…å®¹è°ƒæ•´å“åº”ï¼Œè¿˜ä¼šå—åˆ°æƒ…æ„Ÿæªè¾çš„å½±å“ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°å˜åŒ–äº†156ä¸ªæç¤ºçš„æƒ…æ„ŸåŸºè°ƒï¼Œåˆ†æå…¶å¯¹æ¨¡å‹å“åº”çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼ŒGPT-4å¯¹è´Ÿé¢æ¡†æ¶é—®é¢˜çš„è´Ÿé¢å“åº”æ¦‚ç‡æ˜¯ä¸­æ€§é—®é¢˜çš„ä¸‰åˆ†ä¹‹ä¸€ï¼Œè¡¨æ˜æ¨¡å‹å­˜åœ¨â€œåå¼¹â€åå·®ï¼Œå¸¸å¸¸å‘ä¸­æ€§æˆ–ç§¯æå€¾æ–œã€‚åœ¨æ•æ„Ÿè¯é¢˜ä¸Šï¼Œè¿™ç§æ•ˆåº”æ›´ä¸ºæ˜æ˜¾ï¼Œæç¤ºçš„åŸºè°ƒå˜åŒ–è¢«æŠ‘åˆ¶ï¼Œæš—ç¤ºå¯¹é½è¦†ç›–ç°è±¡ã€‚æˆ‘ä»¬å¼•å…¥äº†â€œåŸºè°ƒä¸‹é™â€ç­‰æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨åŸºè°ƒ-æ•ˆä»·è½¬ç§»çŸ©é˜µé‡åŒ–è¡Œä¸ºï¼Œ1536ç»´åµŒå…¥çš„å¯è§†åŒ–ç»“æœç¡®è®¤äº†åŸºäºåŸºè°ƒçš„è¯­ä¹‰æ¼‚ç§»ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†æƒ…æ„Ÿæ¡†æ¶é©±åŠ¨çš„åå·®è¿™ä¸€æœªè¢«å……åˆ†æ¢è®¨çš„ç±»åˆ«ï¼Œå¯¹AIå¯¹é½å’Œä¿¡ä»»å…·æœ‰é‡è¦å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿæ¡†æ¶ä¸‹çš„å“åº”åå·®ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘æƒ…æ„Ÿæªè¾å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ•æ„Ÿè¯é¢˜ä¸Šçš„è¡¨ç°ä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿæ€§åœ°å˜åŒ–æç¤ºçš„æƒ…æ„ŸåŸºè°ƒï¼Œåˆ†æå…¶å¯¹æ¨¡å‹å“åº”çš„å½±å“ï¼Œæå‡ºâ€œåŸºè°ƒä¸‹é™â€ç­‰æ–°æ¦‚å¿µï¼Œä»¥é‡åŒ–æ¨¡å‹çš„æƒ…æ„Ÿå“åº”ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨156ä¸ªä¸åŒæƒ…æ„ŸåŸºè°ƒçš„æç¤ºï¼Œç»“åˆ1536ç»´åµŒå…¥è¿›è¡Œå¯è§†åŒ–åˆ†æï¼Œä½¿ç”¨åŸºè°ƒ-æ•ˆä»·è½¬ç§»çŸ©é˜µé‡åŒ–æ¨¡å‹çš„å“åº”å˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥äº†â€œåå¼¹â€åå·®å’Œâ€œåŸºè°ƒä¸‹é™â€æ¦‚å¿µï¼Œæ­ç¤ºäº†æƒ…æ„Ÿæ¡†æ¶å¯¹æ¨¡å‹è¾“å‡ºçš„æ·±è¿œå½±å“ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§æƒ…æ„ŸåŸºè°ƒçš„æç¤ºï¼Œå¹¶é€šè¿‡é‡åŒ–åˆ†æå’Œå¯è§†åŒ–æ‰‹æ®µï¼Œæ·±å…¥æ¢è®¨äº†æ¨¡å‹åœ¨ä¸åŒæƒ…æ„Ÿæ¡†æ¶ä¸‹çš„å“åº”ç‰¹å¾ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4å¯¹è´Ÿé¢æ¡†æ¶é—®é¢˜çš„è´Ÿé¢å“åº”æ¦‚ç‡æ˜¯ä¸­æ€§é—®é¢˜çš„ä¸‰åˆ†ä¹‹ä¸€ï¼Œè¡¨æ˜æ¨¡å‹å­˜åœ¨æ˜¾è‘—çš„â€œåå¼¹â€åå·®ã€‚åœ¨æ•æ„Ÿè¯é¢˜ä¸Šï¼Œè¿™ç§åå·®æ›´åŠ æ˜æ˜¾ï¼Œæç¤ºçš„æƒ…æ„ŸåŸºè°ƒå˜åŒ–è¢«æŠ‘åˆ¶ï¼Œå¼ºè°ƒäº†æƒ…æ„Ÿæ¡†æ¶å¯¹æ¨¡å‹è¾“å‡ºçš„æ·±è¿œå½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€ç”¨æˆ·äº¤äº’è®¾è®¡å’ŒAIå¯¹è¯ç³»ç»Ÿçš„ä¼˜åŒ–ã€‚é€šè¿‡ç†è§£æƒ…æ„Ÿæ¡†æ¶å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ï¼Œå¯ä»¥æå‡AIç³»ç»Ÿåœ¨æ•æ„Ÿè¯é¢˜ä¸Šçš„å“åº”è´¨é‡ï¼Œå¢å¼ºç”¨æˆ·ä¿¡ä»»ï¼Œæ¨åŠ¨æ›´å®‰å…¨å’Œå¯é çš„AIåº”ç”¨ã€‚æœªæ¥ï¼Œç ”ç©¶ç»“æœå¯èƒ½å¼•å¯¼æ›´å¥½çš„AIå¯¹é½ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šæ ·åŒ–æƒ…æ„Ÿè¡¨è¾¾ä¸‹çš„ç¨³å®šæ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a "rebound" bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the "tone floor" - a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: https://github.com/bardolfranck/llm-responses-viewer

