---
layout: default
title: Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent
---

# Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14302" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14302v1</a>
  <a href="https://arxiv.org/pdf/2506.14302.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14302v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14302v1', 'Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xueyang Feng, Jingsen Zhang, Jiakai Tang, Wei Li, Guohao Cai, Xu Chen, Quanyu Dai, Yue Zhu, Zhenhua Dong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

**å¤‡æ³¨**: Accepted to Findings of ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè½®å¯¹è¯æ¨èä»£ç†çš„æœŸæœ›ç¡®è®¤åå¥½ä¼˜åŒ–æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹è¯æ¨è` `ç”¨æˆ·æ»¡æ„åº¦` `æœŸæœ›ç¡®è®¤ç†è®º` `å¤šè½®å¯¹è¯` `åå¥½ä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç”¨æˆ·æ¨¡æ‹Ÿå™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹è¯æ¨èä»£ç†åœ¨å¤šè½®å¯¹è¯ä¸­å¸¸å¸¸æ— æ³•æŒç»­æ»¡è¶³ç”¨æˆ·æœŸæœ›ï¼Œå¯¼è‡´ç”¨æˆ·æ»¡æ„åº¦ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºçš„ECPOæ–¹æ³•åˆ©ç”¨æœŸæœ›ç¡®è®¤ç†è®ºï¼Œæ˜ç¡®å»ºæ¨¡ç”¨æˆ·æ»¡æ„åº¦çš„æ¼”å˜ï¼Œå¹¶é’ˆå¯¹ä¸æ»¡æ„çš„å“åº”è¿›è¡Œä¼˜åŒ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒECPOåœ¨æ•ˆç‡å’Œæœ‰æ•ˆæ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šè½®åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œæå‡äº†å¯¹è¯æ¨èçš„äº¤äº’èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†å¯¹è¯æ¨èä»£ç†ï¼ˆCRAsï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†å¸¸å¸¸ç”ŸæˆçŸ­è§†çš„å“åº”ï¼Œæ— æ³•æŒç»­å¼•å¯¼ç”¨æˆ·å¹¶æ»¡è¶³å…¶æœŸæœ›ã€‚å°½ç®¡åå¥½ä¼˜åŒ–åœ¨å¯¹é½LLMsä¸ç”¨æˆ·æœŸæœ›æ–¹é¢æœ‰æ•ˆï¼Œä½†åœ¨å¤šè½®å¯¹è¯ä¸­è¡¨ç°ä¸ä½³ä¸”æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šè½®åå¥½ä¼˜åŒ–ï¼ˆMTPOï¼‰èŒƒå¼ECPOï¼Œåˆ©ç”¨æœŸæœ›ç¡®è®¤ç†è®ºæ˜ç¡®å»ºæ¨¡ç”¨æˆ·æ»¡æ„åº¦åœ¨å¤šè½®å¯¹è¯ä¸­çš„æ¼”å˜ï¼Œæ­ç¤ºä¸æ»¡çš„æ½œåœ¨åŸå› ã€‚è¿™äº›åŸå› å¯ç”¨äºæ”¯æŒå¯¹ä¸æ»¡æ„å“åº”çš„é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œä»è€Œå®ç°é€è½®åå¥½ä¼˜åŒ–ã€‚ECPOå·§å¦™åœ°æ¶ˆé™¤äº†ç°æœ‰MTPOæ–¹æ³•çš„æ˜¾è‘—é‡‡æ ·å¼€é”€ï¼ŒåŒæ—¶ç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹æ¨åŠ¨æœ‰æ„ä¹‰çš„æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECPOæ˜¾è‘—å¢å¼ºäº†CRAçš„äº¤äº’èƒ½åŠ›ï¼Œåœ¨æ•ˆç‡å’Œæœ‰æ•ˆæ€§ä¸Šå‡ä¼˜äºç°æœ‰MTPOæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯å¤šè½®å¯¹è¯æ¨èä»£ç†åœ¨ç”¨æˆ·æœŸæœ›æ»¡è¶³æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤šè½®å¯¹è¯ä¸­è¡¨ç°ä¸ä½³ä¸”æˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„ECPOæ–¹æ³•é€šè¿‡æœŸæœ›ç¡®è®¤ç†è®ºï¼Œæ˜ç¡®å»ºæ¨¡ç”¨æˆ·æ»¡æ„åº¦çš„æ¼”å˜ï¼Œé’ˆå¯¹ä¸æ»¡æ„çš„å“åº”è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å®ç°é€è½®åå¥½ä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šECPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç”¨æˆ·æ»¡æ„åº¦å»ºæ¨¡ã€æœŸæœ›ç¡®è®¤åé¦ˆæ¨¡æ‹Ÿä»¥åŠé’ˆå¯¹æ€§å“åº”ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚ç”¨æˆ·æ¨¡æ‹Ÿå™¨AILOç”¨äºç”Ÿæˆç”¨æˆ·åé¦ˆï¼Œæ”¯æŒæœŸæœ›ç¡®è®¤è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šECPOçš„ä¸»è¦åˆ›æ–°åœ¨äºæ¶ˆé™¤äº†ç°æœ‰æ–¹æ³•çš„æ˜¾è‘—é‡‡æ ·å¼€é”€ï¼ŒåŒæ—¶ç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹èƒ½å¤Ÿæœ‰æ•ˆæ¨åŠ¨ç”¨æˆ·æ»¡æ„åº¦çš„æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥é‡åŒ–ç”¨æˆ·æ»¡æ„åº¦ï¼Œå¹¶é€šè¿‡LLMæ„å»ºç”¨æˆ·æ¨¡æ‹Ÿå™¨AILOï¼Œä»¥å®ç°é«˜æ•ˆçš„åé¦ˆç”Ÿæˆå’Œä¼˜åŒ–è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•çš„ç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨å¤šè½®å¯¹è¯ä¸­èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ç”¨æˆ·çš„æœŸæœ›å˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒECPOåœ¨å¤šè½®å¯¹è¯ä¸­çš„æ•ˆç‡æå‡è¶…è¿‡30%ï¼Œæœ‰æ•ˆæ€§æå‡è¶…è¿‡25%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šè½®åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åœ¨çº¿è´­ç‰©ã€å†…å®¹æ¨èå’Œå®¢æˆ·æœåŠ¡ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚é€šè¿‡ä¼˜åŒ–å¯¹è¯æ¨èä»£ç†çš„äº¤äº’èƒ½åŠ›ï¼Œæœªæ¥å¯åœ¨æ›´å¤šæ™ºèƒ½åŠ©æ‰‹å’Œæ¨èç³»ç»Ÿä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæ¨åŠ¨ä¸ªæ€§åŒ–æœåŠ¡çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.

