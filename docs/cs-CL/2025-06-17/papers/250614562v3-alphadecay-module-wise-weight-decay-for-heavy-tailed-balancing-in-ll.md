---
layout: default
title: AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs
---

# AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14562" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14562v3</a>
  <a href="https://arxiv.org/pdf/2506.14562.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14562v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14562v3', 'AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Di He, Songjun Tu, Ajay Jaiswal, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17 (æ›´æ–°: 2025-11-05)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hed-ucas/AlphaDecay)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAlphaDecayä»¥è§£å†³LLMsæ¨¡å—é—´æƒé‡è¡°å‡ä¸å‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æƒé‡è¡°å‡` `è‡ªé€‚åº”å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `é‡å°¾è‡ªæ­£åˆ™åŒ–` `è°±ç‰¹æ€§åˆ†æ` `æ¨¡å‹è®­ç»ƒ` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æƒé‡è¡°å‡æ–¹æ³•é€šå¸¸ä¸ºæ¯å±‚åˆ†é…ç»Ÿä¸€è¡°å‡ç‡ï¼Œæœªèƒ½è€ƒè™‘æ¨¡å—é—´çš„ç»“æ„å·®å¼‚å’Œè°±ç‰¹æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„AlphaDecayæ–¹æ³•é€šè¿‡é‡å°¾è‡ªæ­£åˆ™åŒ–ç†è®ºï¼Œè‡ªé€‚åº”åœ°ä¸ºæ¯ä¸ªæ¨¡å—åˆ†é…ä¸åŒçš„æƒé‡è¡°å‡å¼ºåº¦ï¼Œä»¥å¹³è¡¡æ¨¡å—é—´çš„å·®å¼‚ã€‚
3. åœ¨60Måˆ°1Bå‚æ•°è§„æ¨¡çš„å¤šé¡¹é¢„è®­ç»ƒä»»åŠ¡ä¸­ï¼ŒAlphaDecayåœ¨å›°æƒ‘åº¦å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„ç»Ÿä¸€è¡°å‡å’Œå…¶ä»–è‡ªé€‚åº”è¡°å‡æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æƒé‡è¡°å‡æ˜¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ ‡å‡†æ­£åˆ™åŒ–æŠ€æœ¯ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¸ºæ¯ä¸€å±‚åˆ†é…ç»Ÿä¸€çš„è¡°å‡ç‡ï¼Œä½†å¿½è§†äº†LLMsçš„ç»“æ„å¤šæ ·æ€§åŠæ¨¡å—é—´çš„è°±ç‰¹æ€§å·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†AlphaDecayï¼Œä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°ä¸ºLLMçš„æ¯ä¸ªæ¨¡å—åˆ†é…ä¸åŒçš„æƒé‡è¡°å‡å¼ºåº¦ã€‚è¯¥æ–¹æ³•åŸºäºé‡å°¾è‡ªæ­£åˆ™åŒ–ï¼ˆHT-SRï¼‰ç†è®ºï¼Œé€šè¿‡åˆ†ææƒé‡ç›¸å…³çŸ©é˜µçš„ç»éªŒè°±å¯†åº¦ï¼ˆESDï¼‰æ¥é‡åŒ–â€œé‡å°¾æ€§â€ã€‚è¡¨ç°å‡ºæ›´æ˜æ˜¾é‡å°¾ESDçš„æ¨¡å—è¢«åˆ†é…è¾ƒå¼±çš„è¡°å‡ï¼Œè€Œè°±è¾ƒè½»å°¾çš„æ¨¡å—åˆ™è·å¾—è¾ƒå¼ºçš„è¡°å‡ã€‚å®éªŒè¡¨æ˜ï¼ŒAlphaDecayåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„ç»Ÿä¸€è¡°å‡å’Œå…¶ä»–è‡ªé€‚åº”è¡°å‡åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æƒé‡è¡°å‡æ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æœªèƒ½è€ƒè™‘æ¨¡å—é—´ç»“æ„å¤šæ ·æ€§çš„é—®é¢˜ã€‚ç»Ÿä¸€çš„è¡°å‡ç‡å¯èƒ½å¯¼è‡´æŸäº›æ¨¡å—å­¦ä¹ æ•ˆæœä¸ä½³ï¼Œå½±å“æ•´ä½“æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAlphaDecayé€šè¿‡é‡å°¾è‡ªæ­£åˆ™åŒ–ç†è®ºï¼Œåˆ†ææƒé‡ç›¸å…³çŸ©é˜µçš„è°±ç‰¹æ€§ï¼Œè‡ªé€‚åº”åœ°ä¸ºæ¯ä¸ªæ¨¡å—åˆ†é…ä¸åŒçš„æƒé‡è¡°å‡å¼ºåº¦ã€‚è¿™æ ·å¯ä»¥é’ˆå¯¹ä¸åŒæ¨¡å—çš„ç‰¹æ€§è¿›è¡Œä¼˜åŒ–ï¼Œæå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é¦–å…ˆè®¡ç®—æ¯ä¸ªæ¨¡å—çš„ç»éªŒè°±å¯†åº¦ï¼ˆESDï¼‰ï¼Œç„¶åæ ¹æ®å…¶é‡å°¾æ€§åˆ†é…æƒé‡è¡°å‡å¼ºåº¦ã€‚å…·ä½“æµç¨‹åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è°±ç‰¹æ€§åˆ†æã€è¡°å‡å¼ºåº¦åˆ†é…å’Œæ¨¡å‹è®­ç»ƒç­‰é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šAlphaDecayçš„åˆ›æ–°åœ¨äºå…¶åŸºäºé‡å°¾æ€§åˆ†æçš„è‡ªé€‚åº”æƒé‡è¡°å‡åˆ†é…ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„ç»Ÿä¸€è¡°å‡æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ¨¡å—é—´çš„å­¦ä¹ å·®å¼‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸­ï¼Œè®ºæ–‡è¯¦ç»†æè¿°äº†å¦‚ä½•è®¡ç®—ç»éªŒè°±å¯†åº¦ã€å¦‚ä½•é‡åŒ–é‡å°¾æ€§ä»¥åŠå¦‚ä½•è®¾ç½®ä¸åŒæ¨¡å—çš„è¡°å‡å¼ºåº¦ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å„æ¨¡å—çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAlphaDecayåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå›°æƒ‘åº¦é™ä½ï¼Œæ³›åŒ–èƒ½åŠ›å¢å¼ºã€‚ä¸ä¼ ç»Ÿçš„ç»Ÿä¸€è¡°å‡æ–¹æ³•ç›¸æ¯”ï¼ŒAlphaDecayåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒAlphaDecayèƒ½å¤Ÿæå‡æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆå­¦ä¹ çš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines. Our code is available at https://github.com/hed-ucas/AlphaDecay.

