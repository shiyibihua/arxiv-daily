---
layout: default
title: Reasoning with Exploration: An Entropy Perspective
---

# Reasoning with Exploration: An Entropy Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14758" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14758v4</a>
  <a href="https://arxiv.org/pdf/2506.14758.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14758v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14758v4', 'Reasoning with Exploration: An Entropy Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, Furu Wei

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17 (æ›´æ–°: 2025-11-08)

**å¤‡æ³¨**: AAAI 2026 Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç†µçš„æ¢ç´¢æ–¹æ³•ä»¥æå‡å¼ºåŒ–å­¦ä¹ æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æ¢ç´¢ä¸åˆ©ç”¨` `ç†µ` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¤šåå‘åˆ©ç”¨ï¼Œå¯¼è‡´æ€§èƒ½åœæ»ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ¢ç´¢æœºåˆ¶ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡åœ¨ä¼˜åŠ¿å‡½æ•°ä¸­å¢åŠ ç†µé¡¹æ¥ä¿ƒè¿›æ›´é•¿æ›´æ·±çš„æ¨ç†é“¾ï¼Œä»è€Œå¢å¼ºæ¢ç´¢èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Pass@KæŒ‡æ ‡ä¸Šæ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨æå¤§Kå€¼ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨æ˜¯æ ¸å¿ƒç›®æ ‡ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æœ‰æ‰€æå‡ï¼Œä½†ç°æœ‰æ–¹æ³•å¤šåå‘åˆ©ç”¨ï¼Œå¯¼è‡´æ€§èƒ½åœæ»ã€‚æœ¬æ–‡é‡æ–°å®¡è§†ç†µè¿™ä¸€æ¢ç´¢ä¿¡å·ï¼Œæ¢è®¨å…¶ä¸LLMæ¢ç´¢æ€§æ¨ç†çš„å…³ç³»ã€‚é€šè¿‡å®è¯åˆ†æï¼Œå‘ç°é«˜ç†µåŒºåŸŸä¸ä¸‰ç§æ¢ç´¢æ€§æ¨ç†è¡Œä¸ºå­˜åœ¨æ­£ç›¸å…³ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåœ¨æ ‡å‡†å¼ºåŒ–å­¦ä¹ ä¸­ä»…éœ€ä¸€è¡Œä»£ç çš„æœ€å°ä¿®æ”¹ï¼šåœ¨ä¼˜åŠ¿å‡½æ•°ä¸­å¢åŠ åŸºäºç†µçš„é¡¹ã€‚ä¸ä¼ ç»Ÿæœ€å¤§ç†µæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬é€šè¿‡ä¿ƒè¿›æ›´é•¿æ›´æ·±çš„æ¨ç†é“¾æ¥é¼“åŠ±æ¢ç´¢ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨Pass@KæŒ‡æ ‡ä¸Šå–å¾—æ˜¾è‘—æå‡ï¼Œæ¨åŠ¨äº†LLMæ¨ç†çš„è¾¹ç•Œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€è¿‡äºä¾èµ–åˆ©ç”¨ï¼Œå¯¼è‡´æ€§èƒ½æå‡å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºé€šè¿‡åœ¨ä¼˜åŠ¿å‡½æ•°ä¸­å¢åŠ ç†µçš„é¡¹æ¥é¼“åŠ±æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯ä¿ƒè¿›æ›´é•¿å’Œæ›´æ·±çš„æ¨ç†é“¾ï¼Œè€Œéå•çº¯å¢åŠ ä¸ç¡®å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ ‡å‡†å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸»è¦æ¨¡å—ä¸ºä¼˜åŠ¿å‡½æ•°çš„ä¿®æ”¹ï¼Œå¢åŠ ç†µé¡¹åè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ç†µçš„å¼•å…¥ï¼Œæ”¹å˜äº†æ¢ç´¢çš„æ–¹å¼ï¼Œå¼ºè°ƒæ¨ç†é“¾çš„æ·±åº¦å’Œé•¿åº¦ï¼Œè€Œéä»…ä»…æ˜¯å¢åŠ éšæœºæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¼˜åŠ¿å‡½æ•°ä¸­æ·»åŠ ç†µé¡¹çš„å…·ä½“å®ç°ä¸ºä¸€è¡Œä»£ç ï¼Œç¡®ä¿äº†æ–¹æ³•çš„ç®€æ´æ€§å’Œæ˜“ç”¨æ€§ï¼ŒåŒæ—¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒäº†æ¨¡å‹çš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨Pass@KæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨Kå€¼æå¤§æ—¶ï¼Œæ€§èƒ½æå‡å¹…åº¦è¶…è¿‡äº†ä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†æ›´å¼ºçš„æ¨ç†èƒ½åŠ›å’Œæ¢ç´¢æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨æ¨ç†ç­‰ã€‚é€šè¿‡æå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing large language model (LLM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LLMs. Through empirical analysis, we uncover positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LLMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LLM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LLM reasoning.

