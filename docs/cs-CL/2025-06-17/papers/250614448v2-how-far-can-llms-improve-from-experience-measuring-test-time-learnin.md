---
layout: default
title: How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison
---

# How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14448" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14448v2</a>
  <a href="https://arxiv.org/pdf/2506.14448.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14448v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14448v2', 'How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiayin Wang, Zhiquang Guo, Weizhi Ma, Min Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17 (æ›´æ–°: 2025-08-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæµ‹è¯•æ—¶é—´å­¦ä¹ è¯„ä¼°æ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æµ‹è¯•æ—¶é—´å­¦ä¹ ` `è¯­ä¹‰æ¸¸æˆ` `äººå·¥æ™ºèƒ½è¯„ä¼°` `åŠ¨æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é™æ€çŸ¥è¯†çš„æµ‹è¯•ï¼Œå¿½è§†äº†æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è¯­ä¹‰æ¸¸æˆè¯„ä¼°æµ‹è¯•æ—¶é—´å­¦ä¹ ï¼Œæ—¨åœ¨è¡¡é‡æ¨¡å‹åœ¨ç»éªŒåŸºç¡€ä¸Šæå‡è¡¨ç°çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨æµ‹è¯•æ—¶é—´å­¦ä¹ æ–¹é¢å…·æœ‰å¯æµ‹é‡çš„èƒ½åŠ›ï¼Œä½†å…¶è¿›æ­¥é€Ÿåº¦å’Œç¨³å®šæ€§ä¸åŠäººç±»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯„ä¼°è®¾è®¡çš„ä¸æ–­æ·±å…¥ï¼Œå…¨é¢ä¸”å‰ç»æ€§çš„è¯„ä¼°æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚ç°æœ‰åŸºå‡†ä¸»è¦è¯„ä¼°é™æ€çŸ¥è¯†ï¼Œè€Œæ™ºèƒ½è¿˜åŒ…æ‹¬ä»ç»éªŒä¸­å¿«é€Ÿå­¦ä¹ çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å€¡å¯¼è¯„ä¼°æµ‹è¯•æ—¶é—´å­¦ä¹ ï¼Œå³åœ¨æµ‹è¯•æœŸé—´é€šè¿‡ç»éªŒæå‡åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºè¯­ä¹‰æ¸¸æˆä½œä¸ºæœ‰æ•ˆçš„æµ‹è¯•å¹³å°ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªå®¢è§‚çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¯”è¾ƒæ¨¡å‹åœ¨æœ‰é™å’Œç´¯ç§¯ç»éªŒä¸‹çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMså±•ç°å‡ºå¯æµ‹é‡çš„æµ‹è¯•æ—¶é—´å­¦ä¹ èƒ½åŠ›ï¼Œä½†åœ¨ç´¯ç§¯ç»éªŒä¸‹çš„æå‡ä¸å¦‚äººç±»ç¨³å®šä¸”è¿›å±•ç¼“æ…¢ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†LLMsä½œä¸ºé€šç”¨å­¦ä¹ æœºå™¨çš„æ½œåŠ›ï¼ŒåŒæ—¶æ­ç¤ºäº†æ¨¡å‹ä¸äººç±»ä¹‹é—´çš„æ˜¾è‘—æ™ºåŠ›å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•æ— æ³•æœ‰æ•ˆè¡¡é‡å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­å­¦ä¹ èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é™æ€çŸ¥è¯†ï¼Œæœªèƒ½åæ˜ æ¨¡å‹çš„é€‚åº”æ€§å’Œå­¦ä¹ èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è¯­ä¹‰æ¸¸æˆä½œä¸ºæµ‹è¯•å¹³å°ï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­çš„æµ‹è¯•æ—¶é—´å­¦ä¹ èƒ½åŠ›ï¼Œå¼ºè°ƒç»éªŒå¯¹æ¨¡å‹è¡¨ç°çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å››ç§ç»éªŒè¡¨ç¤ºå½¢å¼ï¼Œæ¯”è¾ƒæ¨¡å‹åœ¨æœ‰é™ç»éªŒå’Œç´¯ç§¯ç»éªŒä¸‹çš„è¡¨ç°ã€‚è¯„ä¼°æ¡†æ¶é€šè¿‡ä¸äººç±»å‚ä¸è€…çš„æ¯”è¾ƒæä¾›åŸºçº¿ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æµ‹è¯•æ—¶é—´å­¦ä¹ çš„æ¦‚å¿µï¼Œå¹¶é€šè¿‡è¯­ä¹‰æ¸¸æˆæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è¯„ä¼°å¹³å°ï¼Œçªç ´äº†ä¼ ç»Ÿé™æ€è¯„ä¼°çš„é™åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾è®¡äº†ä¸åŒçš„ç»éªŒè¡¨ç¤ºå½¢å¼ï¼Œå¹¶è®¾ç½®äº†ç›¸åº”çš„è¯„ä¼°æ ‡å‡†ï¼Œä»¥ç¡®ä¿å¯¹æ¨¡å‹å­¦ä¹ èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨æµ‹è¯•æ—¶é—´å­¦ä¹ ä¸­è¡¨ç°å‡ºå¯æµ‹é‡çš„èƒ½åŠ›ï¼Œä½†åœ¨ç´¯ç§¯ç»éªŒä¸‹çš„æå‡é€Ÿåº¦è¾ƒæ…¢ï¼Œä¸”ç¨³å®šæ€§ä¸å¦‚äººç±»ã€‚ä¸å…«åäººç±»å‚ä¸è€…çš„æ¯”è¾ƒç»“æœè¿›ä¸€æ­¥æ­ç¤ºäº†æ¨¡å‹ä¸äººç±»ä¹‹é—´çš„æ™ºåŠ›å·®è·ï¼Œå¼ºè°ƒäº†å¯¹æ¨¡å‹èƒ½åŠ›çš„æ·±å…¥ç†è§£çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ¸¸æˆè®¾è®¡å’Œäººæœºäº¤äº’ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´æ™ºèƒ½çš„ç³»ç»Ÿï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚é€šè¿‡è¯„ä¼°æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºæœªæ¥çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿè®¾è®¡æä¾›é‡è¦å‚è€ƒï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½å‘é€šç”¨æ™ºèƒ½çš„æ–¹å‘å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.

