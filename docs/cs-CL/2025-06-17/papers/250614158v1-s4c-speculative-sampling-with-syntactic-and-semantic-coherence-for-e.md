---
layout: default
title: S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models
---

# S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14158" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14158v1</a>
  <a href="https://arxiv.org/pdf/2506.14158.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14158v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14158v1', 'S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tao He, Guang Huang, Yu Yang, Tianshi Xu, Sicheng Zhao, Guiguang Ding, Pengyang Wang, Feng Tian

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºS$^4$Cä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†å»¶è¿Ÿé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç‡` `æ–‡æœ¬ç”Ÿæˆ` `å¤šå¤´è‰æ‹Ÿ` `éªŒè¯æ ‘` `è¯­æ³•è¿è´¯æ€§` `è¯­ä¹‰è¿è´¯æ€§` `å®æ—¶åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘æ–‡æœ¬ç”Ÿæˆä¸­çš„å†…åœ¨è¿è´¯æ€§ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. S$^4$Cæ¡†æ¶é€šè¿‡å¤šå¤´è‰æ‹ŸåŠ é€Ÿä»¤ç‰Œç”Ÿæˆï¼Œå¹¶åˆ©ç”¨è¿ç»­éªŒè¯æ ‘è¿›è¡Œé«˜æ•ˆå€™é€‰éªŒè¯å’Œç‰¹å¾é‡ç”¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒS$^4$Cåœ¨å¤šä¸ªä¸»æµä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæå‡äº†æ•ˆç‡å’Œå¹¶è¡Œæ€§ï¼Œå‡å°‘äº†è®¡ç®—èµ„æºæ¶ˆè€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶è‡ªå›å½’ç‰¹æ€§å¯¼è‡´æ˜¾è‘—çš„æ¨ç†å»¶è¿Ÿï¼Œç»™å®æ—¶åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºS$^4$Cçš„æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¤šå¤´è‰æ‹Ÿå’Œè¿ç»­éªŒè¯æ ‘ï¼Œæå‡äº†æ¨ç†æ•ˆç‡å’Œå¹¶è¡Œæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS$^4$Cåœ¨ä¸»æµä»»åŠ¡ä¸­è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æœ‰æ•ˆä»¤ç‰Œçš„ç”Ÿæˆé€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œåœ¨Spec-benchåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†2.26x-2.60xçš„åŠ é€Ÿæ¯”ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„å»¶è¿Ÿé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ–‡æœ¬çš„è¯­æ³•å’Œè¯­ä¹‰è¿è´¯æ€§ï¼Œå¯¼è‡´ç”Ÿæˆæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºS$^4$Cæ¡†æ¶ï¼Œé€šè¿‡å¤šå¤´è‰æ‹Ÿå¿«é€Ÿç”Ÿæˆä»¤ç‰Œï¼Œå¹¶ä½¿ç”¨è¿ç»­éªŒè¯æ ‘æ¥é«˜æ•ˆéªŒè¯å€™é€‰ä»¤ç‰Œï¼Œå……åˆ†åˆ©ç”¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è¯­æ³•å’Œè¯­ä¹‰ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šS$^4$Cæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šè‰æ‹Ÿé˜¶æ®µå’ŒéªŒè¯é˜¶æ®µã€‚åœ¨è‰æ‹Ÿé˜¶æ®µï¼Œé‡‡ç”¨å¤šå¤´æœºåˆ¶å¹¶è¡Œç”Ÿæˆå¤šä¸ªå€™é€‰ä»¤ç‰Œï¼›åœ¨éªŒè¯é˜¶æ®µï¼Œé€šè¿‡éªŒè¯æ ‘å¯¹ç”Ÿæˆçš„å€™é€‰è¿›è¡Œå¿«é€ŸéªŒè¯å’Œç‰¹å¾é‡ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šS$^4$Cçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šå¤´è‰æ‹Ÿå’Œè¿ç»­éªŒè¯æ ‘çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ•ˆç‡å’Œæœ‰æ•ˆä»¤ç‰Œçš„ç”Ÿæˆç‡ï¼Œä¸ä¼ ç»Ÿçš„å•ä¸€è‰æ‹Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ–‡æœ¬çš„è¿è´¯æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å€™é€‰ä»¤ç‰Œçš„ç”Ÿæˆè´¨é‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚è¯¥è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒè¯­æ³•å’Œè¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒS$^4$Cåœ¨Spec-benchåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†2.26x-2.60xçš„åŠ é€Ÿæ¯”ï¼Œæ˜æ˜¾ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¡¨æ˜å…¶åœ¨æ¨ç†æ•ˆç‡å’Œæœ‰æ•ˆä»¤ç‰Œç”Ÿæˆæ–¹é¢çš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®æ—¶å¯¹è¯ç³»ç»Ÿã€æ™ºèƒ½å®¢æœã€è‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼ŒS$^4$Cèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´å¿«é€Ÿçš„å“åº”ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½åœ¨å¤šæ¨¡æ€äº¤äº’å’Œå¤æ‚ä»»åŠ¡å¤„ç†ç­‰é¢†åŸŸå‘æŒ¥æ›´å¤§ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.

