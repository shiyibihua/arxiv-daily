---
layout: default
title: Re-Initialization Token Learning for Tool-Augmented Large Language Models
---

# Re-Initialization Token Learning for Tool-Augmented Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14248" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14248v1</a>
  <a href="https://arxiv.org/pdf/2506.14248.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14248v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14248v1', 'Re-Initialization Token Learning for Tool-Augmented Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenghao Li, Liu Liu, Baosheng Yu, Jiayan Qiu, Yibing Zhan

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå·¥å…·å¢å¼ºçš„å¤§è¯­è¨€æ¨¡å‹é‡åˆå§‹åŒ–ä»¤ç‰Œå­¦ä¹ æ–¹æ³•ä»¥è§£å†³å¤æ‚ä»»åŠ¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å·¥å…·å¢å¼º` `å¤§è¯­è¨€æ¨¡å‹` `ä»¤ç‰Œå­¦ä¹ ` `æ•°å€¼æ¨ç†` `çŸ¥è¯†é—®ç­”` `è®¡åˆ’ç”Ÿæˆ` `æ¨¡å‹é€‚åº”æ€§` `åµŒå…¥å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•æœªèƒ½è€ƒè™‘å·¥å…·ä»¤ç‰Œä¸è¯ä»¤ç‰Œä¹‹é—´çš„å…³ç³»ï¼Œé™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»¤ç‰Œå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å°†å·¥å…·ä»¤ç‰Œä¸è¯åµŒå…¥ç©ºé—´å¯¹é½ï¼Œå¢å¼ºæ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚
3. åœ¨æ•°å€¼æ¨ç†ã€çŸ¥è¯†é—®ç­”å’Œè®¡åˆ’ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ•°å€¼æ¨ç†å’Œè®¡åˆ’ç”Ÿæˆï¼‰æ—¶è¡¨ç°ä¸ä½³ã€‚å°†å¤–éƒ¨å·¥å…·ï¼ˆå¦‚è®¡ç®—å™¨å’Œæ•°æ®åº“ï¼‰é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¯¹äºæå‡é—®é¢˜è§£å†³èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¸ºæ¯ä¸ªå·¥å…·åˆ†é…å”¯ä¸€ä»¤ç‰Œï¼Œç„¶è€Œæœªèƒ½è€ƒè™‘å·¥å…·ä¸è¯ä»¤ç‰Œä¹‹é—´çš„å…³ç³»ï¼Œé™åˆ¶äº†é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»¤ç‰Œå­¦ä¹ æ–¹æ³•ï¼Œä»åˆå§‹åŒ–çš„è§’åº¦å°†å·¥å…·ä»¤ç‰Œä¸ç°æœ‰è¯åµŒå…¥ç©ºé—´å¯¹é½ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åŸºäºå·¥å…·çš„åç§°æˆ–æè¿°æ„å»ºå…ˆéªŒä»¤ç‰ŒåµŒå…¥ï¼Œç”¨äºåˆå§‹åŒ–å’Œæ­£åˆ™åŒ–å¯å­¦ä¹ çš„å·¥å…·ä»¤ç‰ŒåµŒå…¥ï¼Œç¡®ä¿å­¦ä¹ çš„åµŒå…¥ä¸è¯ä»¤ç‰Œç©ºé—´è‰¯å¥½å¯¹é½ï¼Œæé«˜å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„æ€§èƒ½ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•é€šè¿‡ç®€å•çš„ä»¤ç‰Œåˆ†é…æœªèƒ½æœ‰æ•ˆåˆ©ç”¨å·¥å…·ä¸è¯ä¹‹é—´çš„å…³ç³»ï¼Œå¯¼è‡´é€‚åº”æ€§å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§é‡åˆå§‹åŒ–ä»¤ç‰Œå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå·¥å…·çš„å…ˆéªŒä»¤ç‰ŒåµŒå…¥ï¼Œå°†å…¶ä¸ç°æœ‰è¯åµŒå…¥ç©ºé—´å¯¹é½ï¼Œä»è€Œæé«˜å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§å’Œæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å…ˆéªŒä»¤ç‰ŒåµŒå…¥çš„æ„å»ºã€å¯å­¦ä¹ å·¥å…·ä»¤ç‰ŒåµŒå…¥çš„åˆå§‹åŒ–ä¸æ­£åˆ™åŒ–ï¼Œä»¥åŠä¸è¯ä»¤ç‰Œç©ºé—´çš„å¯¹é½è¿‡ç¨‹ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬å·¥å…·æè¿°è§£æã€åµŒå…¥åˆå§‹åŒ–å’Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡å…ˆéªŒä»¤ç‰ŒåµŒå…¥çš„æ„å»ºä¸å¯¹é½ï¼Œä½¿å¾—å·¥å…·ä»¤ç‰Œèƒ½å¤Ÿåœ¨è¯åµŒå…¥ç©ºé—´ä¸­è·å¾—æ›´å¥½çš„ä½ç½®ï¼Œä»è€Œæå‡æ¨¡å‹çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŸºäºå·¥å…·åç§°çš„åµŒå…¥åˆå§‹åŒ–æ–¹æ³•ï¼Œå¹¶è®¾è®¡äº†æ­£åˆ™åŒ–ç­–ç•¥ä»¥ç¡®ä¿å­¦ä¹ è¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§ï¼ŒæŸå¤±å‡½æ•°åˆ™ç»“åˆäº†å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§ä¸è¯åµŒå…¥çš„ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨GSM8K-XLã€FuncQAã€KAMELå’ŒVirtualHomeæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºCoTã€REACTã€ICLå’ŒToolkenGPTç­‰åŸºçº¿ï¼Œæå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šé¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿä»¥åŠå¤æ‚ä»»åŠ¡çš„å†³ç­–æ”¯æŒç³»ç»Ÿã€‚é€šè¿‡å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ•™è‚²ã€åŒ»ç–—ã€é‡‘èç­‰å¤šä¸ªè¡Œä¸šä¸­å®ç°æ›´é«˜æ•ˆçš„é—®é¢˜è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.

