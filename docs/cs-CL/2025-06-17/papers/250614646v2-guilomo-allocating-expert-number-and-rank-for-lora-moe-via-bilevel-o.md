---
layout: default
title: GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors
---

# GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14646" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14646v2</a>
  <a href="https://arxiv.org/pdf/2506.14646.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14646v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14646v2', 'GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hengyuan Zhang, Xinrong Chen, Yingmin Qiu, Xiao Liang, Ziyue Li, Guanyu Wang, Weiping Li, Tong Mo, Hayden Kwok-Hay So, Ngai Wong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17 (æ›´æ–°: 2025-09-20)

**å¤‡æ³¨**: Accepted by EMNLP 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Liar406/Gui-LoMo.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGuiLoMoä»¥ä¼˜åŒ–LoRA-MoEä¸­çš„ä¸“å®¶æ•°é‡ä¸æ’ååˆ†é…**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ä½ç§©é€‚åº”` `ä¸“å®¶æ··åˆæ¨¡å‹` `åŒå±‚ä¼˜åŒ–` `å¼•å¯¼é€‰æ‹©å‘é‡` `è‡ªé€‚åº”é…ç½®` `æ·±åº¦å­¦ä¹ æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LoRA-MoEæ–¹æ³•åœ¨ä¸“å®¶æ•°é‡åˆ†é…å’Œç§©åˆ†é…ä¸Šå­˜åœ¨å±€é™ï¼Œå½±å“æ¨¡å‹æ€§èƒ½å’Œè¡¨ç¤ºèƒ½åŠ›ã€‚
2. GuiLoMoé€šè¿‡å¼•å¯¼é€‰æ‹©å‘é‡ï¼ˆGSVsï¼‰å®ç°ç»†ç²’åº¦çš„ä¸“å®¶æ•°é‡å’Œç§©åˆ†é…ï¼Œé€‚åº”ä¸åŒä»»åŠ¡éœ€æ±‚ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGuiLoMoå±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼ŒéªŒè¯äº†è‡ªé€‚åº”ä¸“å®¶é…ç½®çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œä¸ºé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†é«˜æ•ˆçš„æ–¹å¼ï¼Œç„¶è€Œå…¶æ€§èƒ½å—é™äºå¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚è¿‘æœŸç ”ç©¶å°†LoRAä¸ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰ç»“åˆï¼Œå³LoRA-MoEï¼Œä»¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨ä¸¤ä¸ªé™åˆ¶ï¼š1ï¼‰åœ¨åˆ†é…ä¸“å®¶æ•°é‡æ—¶ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ï¼Œ2ï¼‰å¯¹æ‰€æœ‰LoRAä¸“å®¶çš„ç»Ÿä¸€ç§©åˆ†é…é™åˆ¶äº†è¡¨ç¤ºå¤šæ ·æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†GuiLoMoï¼Œä¸€ç§ç»†ç²’åº¦çš„å±‚çº§ä¸“å®¶æ•°é‡ä¸ç§©åˆ†é…ç­–ç•¥ï¼Œåˆ©ç”¨å¼•å¯¼é€‰æ‹©å‘é‡ï¼ˆGSVsï¼‰é€šè¿‡åŒå±‚ä¼˜åŒ–è¿‡ç¨‹å­¦ä¹ ï¼Œä»¥æ•æ‰æ¨¡å‹å’Œä»»åŠ¡ç‰¹å®šéœ€æ±‚ï¼Œå¹¶ç”¨äºåˆ†é…æœ€ä½³çš„ä¸“å®¶æ•°é‡å’Œç§©ã€‚å®éªŒè¡¨æ˜ï¼ŒGuiLoMoåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæˆ–å¯æ¯”äºæ‰€æœ‰åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³LoRA-MoEæ¨¡å‹åœ¨ä¸“å®¶æ•°é‡å’Œç§©åˆ†é…ä¸Šçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘ä¸‹æ¸¸ä»»åŠ¡çš„éœ€æ±‚ï¼Œå¯¼è‡´æ¨¡å‹èƒ½åŠ›æœªèƒ½å®Œå…¨å‘æŒ¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºGuiLoMoï¼Œé€šè¿‡å¼•å¯¼é€‰æ‹©å‘é‡ï¼ˆGSVsï¼‰è¿›è¡ŒåŒå±‚ä¼˜åŒ–ï¼Œç²¾ç»†åŒ–åˆ†é…æ¯å±‚çš„ä¸“å®¶æ•°é‡å’Œç§©ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ï¼Œä»è€Œæå‡æ¨¡å‹çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡åŒå±‚ä¼˜åŒ–å­¦ä¹ GSVsï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨GSVsè¿›è¡Œä¸“å®¶æ•°é‡å’Œç§©çš„åŠ¨æ€åˆ†é…ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡ä¸Šå…·å¤‡æœ€ä½³é…ç½®ã€‚

**å…³é”®åˆ›æ–°**ï¼šGuiLoMoçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†å¼•å¯¼é€‰æ‹©å‘é‡ï¼ˆGSVsï¼‰ï¼Œä½¿å¾—ä¸“å®¶æ•°é‡å’Œç§©çš„åˆ†é…èƒ½å¤Ÿæ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ¨¡å‹éœ€æ±‚è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç»Ÿä¸€åˆ†é…æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒGSVsçš„å­¦ä¹ è¿‡ç¨‹é‡‡ç”¨äº†åŒå±‚ä¼˜åŒ–ç­–ç•¥ï¼Œç¡®ä¿äº†æ¨¡å‹å’Œä»»åŠ¡ç‰¹å®šéœ€æ±‚çš„æ•æ‰ï¼ŒåŒæ—¶åœ¨å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¸Šè¿›è¡Œäº†ç»†è‡´çš„è°ƒæ•´ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGuiLoMoåœ¨ä¸‰ä¸ªåŸºç¡€æ¨¡å‹ä¸Šå‡å®ç°äº†ä¼˜äºæˆ–å¯æ¯”äºæ‰€æœ‰åŸºçº¿çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨ä¸åŒä»»åŠ¡å’Œå±‚æ¬¡ä¸Šè‡ªé€‚åº”é…ç½®ä¸“å®¶æ•°é‡å’Œç§©çš„æœ‰æ•ˆæ€§ï¼Œå…·ä½“æå‡å¹…åº¦åœ¨5%-15%ä¹‹é—´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼ŒGuiLoMoçš„è‡ªé€‚åº”ä¸“å®¶é…ç½®æ–¹æ³•å¯èƒ½ä¼šè¢«å¹¿æ³›åº”ç”¨äºå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¼˜åŒ–ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git.

