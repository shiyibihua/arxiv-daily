---
layout: default
title: GRAM: A Generative Foundation Reward Model for Reward Generalization
---

# GRAM: A Generative Foundation Reward Model for Reward Generalization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14175" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14175v2</a>
  <a href="https://arxiv.org/pdf/2506.14175.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14175v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14175v2', 'GRAM: A Generative Foundation Reward Model for Reward Generalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Bei Li, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17 (æ›´æ–°: 2025-06-18)

**å¤‡æ³¨**: Accepted by ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRAMæ¨¡å‹ä»¥è§£å†³å¥–åŠ±æ¨¡å‹æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±æ¨¡å‹` `ç”Ÿæˆæ¨¡å‹` `æ— ç›‘ç£å­¦ä¹ ` `ç›‘ç£å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `è‡ªç„¶è¯­è¨€å¤„ç†` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¥–åŠ±æ¨¡å‹é€šå¸¸ä»…ä¾èµ–æ ‡æ³¨æ•°æ®ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›å’Œåº”ç”¨èŒƒå›´ã€‚
2. æœ¬æ–‡æå‡ºçš„GRAMæ¨¡å‹ç»“åˆäº†æ— ç›‘ç£å’Œç›‘ç£å­¦ä¹ ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„ä¼˜åŠ¿è¿›è¡Œå¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRAMåœ¨å“åº”æ’åã€åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå¤šä¸ªå¼ºåŸºçº¿æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿‡ç¨‹ä¸­ï¼Œå¥–åŠ±æ¨¡å‹å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–æ ‡æ³¨çš„äººç±»åå¥½æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨æœªæ ‡æ³¨å’Œæ ‡æ³¨æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œé¦–å…ˆé€šè¿‡å¤§è§„æ¨¡æ— ç›‘ç£å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œç„¶åé€šè¿‡ç›‘ç£å­¦ä¹ è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡ä½¿ç”¨æ ‡ç­¾å¹³æ»‘ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¼˜åŒ–äº†ä¸€ä¸ªæ­£åˆ™åŒ–çš„æˆå¯¹æ’åæŸå¤±ã€‚è¿™ä¸€ç»“æœä¸ºè®­ç»ƒå¥–åŠ±æ¨¡å‹æä¾›äº†æ–°çš„è§†è§’ï¼Œå°†ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹è”ç³»åœ¨åŒä¸€ç±»è®­ç»ƒç›®æ ‡ä¸‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¥–åŠ±æ¨¡å‹é€šå¸¸ä½œä¸ºåˆ¤åˆ«æ¨¡å‹è®­ç»ƒï¼Œä¾èµ–äºæ ‡æ³¨çš„äººç±»åå¥½æ•°æ®ï¼Œå¯¼è‡´å…¶åœ¨æ–°ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„GRAMæ¨¡å‹é€šè¿‡ç»“åˆæ— ç›‘ç£å’Œç›‘ç£å­¦ä¹ ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„ç‰¹æ€§ï¼Œå¢å¼ºå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œä»è€Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRAMæ¨¡å‹é¦–å…ˆè¿›è¡Œå¤§è§„æ¨¡æ— ç›‘ç£å­¦ä¹ ï¼Œæ„å»ºåŸºç¡€å¥–åŠ±æ¨¡å‹ï¼Œç„¶åé€šè¿‡ç›‘ç£å­¦ä¹ è¿›è¡Œå¾®è°ƒã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šGRAMæ¨¡å‹çš„åˆ›æ–°åœ¨äºå°†ç”Ÿæˆæ¨¡å‹ä¸åˆ¤åˆ«æ¨¡å‹çš„è®­ç»ƒç›®æ ‡ç»“åˆï¼Œæå‡ºäº†ä½¿ç”¨æ ‡ç­¾å¹³æ»‘ä¼˜åŒ–æ­£åˆ™åŒ–çš„æˆå¯¹æ’åæŸå¤±ï¼Œä»è€Œä¸ºå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨æ ‡ç­¾å¹³æ»‘æŠ€æœ¯ï¼Œè®¾ç½®äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§å¼ºçš„ç½‘ç»œç»“æ„ï¼Œä»¥ä¾¿åœ¨ä¸åŒä»»åŠ¡ä¸­å®ç°è‰¯å¥½çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGRAMæ¨¡å‹åœ¨å“åº”æ’åå’Œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ç›¸è¾ƒäºå¤šä¸ªå¼ºåŸºçº¿æ¨¡å‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚GRAMæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­å¿«é€Ÿé€‚åº”ï¼Œå‡å°‘äº†å¾®è°ƒçš„éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.

