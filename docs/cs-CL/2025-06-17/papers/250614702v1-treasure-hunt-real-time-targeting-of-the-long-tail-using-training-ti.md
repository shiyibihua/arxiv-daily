---
layout: default
title: Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers
---

# Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14702" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14702v1</a>
  <a href="https://arxiv.org/pdf/2506.14702.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14702v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14702v1', 'Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daniel D'souza, Julia Kreutzer, Adrien Morisot, Ahmet ÃœstÃ¼n, Sara Hooker

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè®­ç»ƒæ—¶æ ‡è®°ä¼˜åŒ–ä»¥æå‡é•¿å°¾ç‰¹å¾è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿å°¾ç‰¹å¾` `æ¨¡å‹å¾®è°ƒ` `ç”Ÿæˆæ§åˆ¶` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç¨€æœ‰ç‰¹å¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹åœ¨ç¨€æœ‰ç‰¹å¾ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥é€‚åº”è®­ç»ƒä¸­æœªå……åˆ†ä»£è¡¨çš„ç”¨ä¾‹ã€‚
2. é€šè¿‡ä¼˜åŒ–è®­ç»ƒåè®®ï¼Œåˆ›å»ºæ•°æ®ç‰¹å¾å’Œä»»åŠ¡æ¥æºçš„åˆ†ç±»æ³•ï¼Œæå‡æ¨¡å‹åœ¨æ¨ç†æ—¶çš„å¯æ§æ€§å’Œæ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æ ‡è®°åç”Ÿæˆè´¨é‡å¹³å‡æå‡5.7%ï¼Œåœ¨ä»£è¡¨æ€§ä¸è¶³çš„é¢†åŸŸæå‡è¶…è¿‡9.1%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£æœºå™¨å­¦ä¹ é¢ä¸´çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨ç¨€æœ‰å’Œä»£è¡¨æ€§ä¸è¶³çš„ç‰¹å¾ä¸Šè¡¨ç°è‰¯å¥½ã€‚ç°æœ‰çš„å¤§å‹é€šç”¨æ¨¡å‹åœ¨é«˜é¢‘ä½¿ç”¨åœºæ™¯ä¸­è¡¨ç°æœ€ä½³ï¼Œéš¾ä»¥é€‚åº”è®­ç»ƒè¯­æ–™ä¸­ä»£è¡¨æ€§ä¸è¶³çš„ç‰¹å®šç”¨ä¾‹ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•ä¼˜åŒ–è®­ç»ƒåè®®ï¼Œä»¥æé«˜æ¨ç†æ—¶å¯¹è¿™äº›ç”¨ä¾‹çš„å¯æ§æ€§å’Œæ€§èƒ½ã€‚é€šè¿‡åˆ›å»ºæ•°æ®ç‰¹å¾å’Œä»»åŠ¡æ¥æºçš„è¯¦ç»†åˆ†ç±»æ³•ï¼Œä½œè€…æå‡ºäº†ä¸€ç§çµæ´»çš„æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶è‡ªåŠ¨æ¨æ–­è¿™äº›æ ‡è®°ï¼Œä»è€Œæ˜¾è‘—æå‡äº†é•¿å°¾æ ·æœ¬çš„ç”Ÿæˆè´¨é‡ï¼Œå°¤å…¶åœ¨ä»£è¡¨æ€§ä¸è¶³çš„é¢†åŸŸä¸­å–å¾—äº†è¶…è¿‡9.1%çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°ä»£æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é•¿å°¾ç‰¹å¾ä¸Šçš„è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒè¯­æ–™ä¸­ä»£è¡¨æ€§ä¸è¶³çš„ç”¨ä¾‹ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæç¤ºå·¥ç¨‹æˆ–å°‘é‡ç¤ºä¾‹ï¼Œå¯¼è‡´æ¨¡å‹å¯¹å°å˜åŒ–æ•æ„Ÿï¼Œä¸”éš¾ä»¥ä¿æŒæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œåˆ›å»ºä¸€å¥—è¯¦ç»†çš„æ•°æ®ç‰¹å¾å’Œä»»åŠ¡æ¥æºåˆ†ç±»æ³•ï¼Œä½¿æ¨¡å‹åœ¨æ¨ç†æ—¶èƒ½å¤Ÿè‡ªåŠ¨æ¨æ–­æ ‡è®°ï¼Œä»è€Œæé«˜å¯¹é•¿å°¾ç‰¹å¾çš„å“åº”èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ ‡è®°çš„ç”Ÿæˆã€æ¨¡å‹çš„å¾®è°ƒå’Œæ¨ç†é˜¶æ®µã€‚æ¨¡å‹åœ¨è®­ç»ƒæ—¶å­¦ä¹ å¦‚ä½•è¯†åˆ«å’Œç”Ÿæˆè¿™äº›æ ‡è®°ï¼Œæ¨ç†æ—¶åˆ™å¯é€‰æ‹©æ€§åœ°ä½¿ç”¨è¿™äº›æ ‡è®°æ¥æ§åˆ¶ç”Ÿæˆç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç»“åˆï¼Œé€šè¿‡æ˜ç¡®çš„æ ‡è®°æ§åˆ¶ç”Ÿæˆå±æ€§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é•¿å°¾ç‰¹å¾ä¸Šçš„è¡¨ç°ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ä¾èµ–å›ºå®šæç¤ºçš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°æ ‡è®°çš„ç”Ÿæˆã€‚åŒæ—¶ï¼Œè®¾è®¡äº†çµæ´»çš„å‚æ•°è®¾ç½®ï¼Œä½¿å¾—æ ‡è®°åœ¨æ¨ç†æ—¶å¯ä»¥é€‰æ‹©æ€§ä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è®­ç»ƒæ—¶æ ‡è®°åï¼Œæ¨¡å‹åœ¨å¼€æ”¾å¼ç”Ÿæˆè´¨é‡ä¸Šå¹³å‡æå‡5.7%ï¼Œåœ¨ä»£è¡¨æ€§ä¸è¶³çš„é¢†åŸŸæå‡è¶…è¿‡9.1%ã€‚åœ¨ç‰¹å®šä»»åŠ¡å¦‚CodeRepairä¸Šï¼Œè¡¨ç°æå‡è¾¾åˆ°14.1%ï¼Œåœ¨é•¿åº¦æŒ‡ä»¤è·Ÿéšè¯„ä¼°ä¸­ç»å¯¹æå‡è¾¾åˆ°35.3%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ä»£ç ç”Ÿæˆå’Œå…¶ä»–éœ€è¦å¤„ç†ç¨€æœ‰ç‰¹å¾çš„ä»»åŠ¡ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨é•¿å°¾ç‰¹å¾ä¸Šçš„è¡¨ç°ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ç‰¹å®šè¡Œä¸šæˆ–é¢†åŸŸçš„éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.

