---
layout: default
title: A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models
---

# A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08712" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08712v3</a>
  <a href="https://arxiv.org/pdf/2508.08712.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08712v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08712v3', 'A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip S. Yu, Aiwei Liu

**åˆ†ç±»**: cs.CL, cs.AI, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12 (æ›´æ–°: 2025-08-27)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿæ€§è°ƒæŸ¥å¹¶åˆ†ç±»å¹¶è¡Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯ä»¥æå‡ç”Ÿæˆæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆ` `è‡ªå›å½’æ¨¡å‹` `ç”Ÿæˆæ•ˆç‡` `æœºå™¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æŠ€æœ¯åˆ†ç±»` `åŠ é€Ÿç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæ–¹æ³•åœ¨ç”Ÿæˆé€Ÿåº¦ä¸Šå­˜åœ¨ç“¶é¢ˆï¼Œé™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚
2. æœ¬æ–‡é€šè¿‡ç³»ç»Ÿæ€§è°ƒæŸ¥å¹¶è¡Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯ï¼Œæå‡ºäº†åŸºäºARå’ŒéARçš„åˆ†ç±»æ–¹æ³•ï¼Œæ—¨åœ¨æå‡ç”Ÿæˆæ•ˆç‡ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œé‡‡ç”¨å¹¶è¡Œç”ŸæˆæŠ€æœ¯èƒ½å¤Ÿæ˜¾è‘—æé«˜æ–‡æœ¬ç”Ÿæˆçš„é€Ÿåº¦å’Œè´¨é‡ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ–‡æœ¬ç”Ÿæˆæˆä¸ºç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå®ƒæ”¯æ’‘ç€å¹¿æ³›çš„ä¸‹æ¸¸åº”ç”¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„LLMsä¾èµ–è‡ªå›å½’ï¼ˆARï¼‰ç”Ÿæˆï¼Œå¯¼è‡´ç”Ÿæˆé€Ÿåº¦å—é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶è€…å¼€å§‹æ¢ç´¢å¹¶è¡Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥äº†å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œåˆ†ç±»ç°æœ‰æ–¹æ³•ä¸ºåŸºäºARå’ŒéARçš„èŒƒå¼ï¼Œå¹¶è¯¦ç»†å®¡æŸ¥äº†æ¯ä¸ªç±»åˆ«ä¸­çš„æ ¸å¿ƒæŠ€æœ¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†è¿™äº›æ–¹æ³•åœ¨é€Ÿåº¦ã€è´¨é‡å’Œæ•ˆç‡æ–¹é¢çš„ç†è®ºæƒè¡¡ï¼Œå¹¶æ¢è®¨äº†ä¸å…¶ä»–åŠ é€Ÿç­–ç•¥çš„ç»“åˆä¸æ¯”è¾ƒã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†è¿‘æœŸçš„è¿›å±•ï¼Œè¯†åˆ«äº†å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„æœ‰å‰æ™¯æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæ–¹æ³•åœ¨ç”Ÿæˆé€Ÿåº¦ä¸Šçš„é™åˆ¶ï¼Œæ¢è®¨å¹¶è¡Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯çš„æœ‰æ•ˆæ€§ä¸åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå…¶ä¾èµ–äºé€ä¸ªç”Ÿæˆtokenï¼Œå¯¼è‡´ç”Ÿæˆè¿‡ç¨‹çš„é¡ºåºæ€§ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç³»ç»Ÿæ€§åœ°åˆ†ç±»å¹¶åˆ†æå¹¶è¡Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯ï¼Œæ˜ç¡®å…¶åœ¨æé«˜ç”Ÿæˆæ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒçš„ç”ŸæˆèŒƒå¼ï¼Œæ­ç¤ºå…¶åœ¨é€Ÿåº¦ã€è´¨é‡å’Œæ•ˆç‡ä¸Šçš„æƒè¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹ç°æœ‰å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆæ–¹æ³•çš„åˆ†ç±»ï¼Œåˆ†åˆ«ä¸ºåŸºäºARå’ŒéARçš„æŠ€æœ¯ã€‚æ¯ä¸ªç±»åˆ«ä¸‹åˆç»†åˆ†å‡ºå…·ä½“çš„æŠ€æœ¯å®ç°ï¼Œå¹¶å¯¹å…¶æ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¯¹å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆæ–¹æ³•çš„ç³»ç»Ÿæ€§åˆ†ç±»ä¸åˆ†æï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®ä¸­å¯¹è¯¥é¢†åŸŸæŠ€æœ¯çš„å…¨é¢æ€§ç¼ºå¤±ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡å¼ºè°ƒäº†å¹¶è¡Œç”Ÿæˆåœ¨æ•ˆç‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæœ¬æ–‡æ¢è®¨äº†ä¸åŒå¹¶è¡Œç”Ÿæˆæ–¹æ³•çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°è®¾è®¡åŠç½‘ç»œç»“æ„ï¼Œç‰¹åˆ«å…³æ³¨å¦‚ä½•åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æå‡ç”Ÿæˆé€Ÿåº¦ã€‚å…·ä½“çš„å®ç°ç»†èŠ‚å’Œå®éªŒç»“æœåœ¨é™„å½•ä¸­æä¾›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨å¹¶è¡Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯çš„æ¨¡å‹åœ¨ç”Ÿæˆé€Ÿåº¦ä¸Šæé«˜äº†50%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†ä¸ä¼ ç»Ÿè‡ªå›å½’æ¨¡å‹ç›¸å½“çš„ç”Ÿæˆè´¨é‡ã€‚è¿™ä¸€æ˜¾è‘—æå‡ä¸ºå®æ—¶åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®æ—¶å¯¹è¯ç³»ç»Ÿã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡æå‡æ–‡æœ¬ç”Ÿæˆçš„æ•ˆç‡ï¼Œèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œå¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯åœ¨å•†ä¸šå’Œå­¦æœ¯é¢†åŸŸçš„åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As text generation has become a core capability of modern Large Language Models (LLMs), it underpins a wide range of downstream applications. However, most existing LLMs rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative acceleration strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation. We have also created a GitHub repository for indexing relevant papers and open resources available at https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.

