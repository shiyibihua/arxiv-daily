---
layout: default
title: P/D-Device: Disaggregated Large Language Model between Cloud and Devices
---

# P/D-Device: Disaggregated Large Language Model between Cloud and Devices

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09035" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09035v1</a>
  <a href="https://arxiv.org/pdf/2508.09035.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09035v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09035v1', 'P/D-Device: Disaggregated Large Language Model between Cloud and Devices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yibo Jin, Yixu Xu, Yue Chen, Chengbin Wang, Tao Wang, Jiaqi Huang, Rongfei Zhang, Yiming Dong, Yuting Yan, Ke Cheng, Yingjie Zhu, Shulan Wang, Qianqian Tang, Shuaishuai Meng, Guanxin Cheng, Ze Wang, Shuyan Miao, Ketao Wang, Wen Liu, Yifan Yang, Tong Zhang, Anran Wang, Chengzhou Lu, Tiantian Dong, Yongsheng Zhang, Zhe Wang, Hefei Guo, Hongjie Liu, Wei Lu, Zhengyong Zhang

**åˆ†ç±»**: cs.DC, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºP/D-Deviceä»¥è§£å†³äº‘ç«¯ä¸è®¾å¤‡é—´èµ„æºç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `äº‘è®¡ç®—` `è®¾å¤‡åä½œ` `å“åº”æ—¶é—´ä¼˜åŒ–` `ååé‡æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§£ç é˜¶æ®µç”Ÿæˆçš„ä»¤ç‰Œè¿‡å¤šï¼Œå¯¼è‡´äº‘ç«¯èµ„æºå ç”¨è¿‡é•¿ï¼Œæ— æ³•æé«˜ååé‡ã€‚
2. è®ºæ–‡æå‡ºå°†å¤§å‹è¯­è¨€æ¨¡å‹åˆ†ç¦»åˆ°äº‘ç«¯å’Œè®¾å¤‡ä¹‹é—´ï¼Œäº‘ç«¯è´Ÿè´£é¢„å¡«å……ï¼Œè®¾å¤‡åˆ™å¿«é€Ÿå“åº”ç”¨æˆ·è¯·æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¦–æ¬¡ä»¤ç‰Œæ—¶é—´å‡å°‘è‡³å°‘60%ï¼Œæœ€å¤§è¾“å‡ºä»¤ç‰Œæ—¶é—´ä¸ºæ•°åæ¯«ç§’ï¼Œäº‘ç«¯ååé‡æå‡è‡³15å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å·¥ä¸šå®è·µä¸­ï¼Œåˆ†æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹è¢«å¹¿æ³›åº”ç”¨ä»¥æå‡æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨è§£ç é˜¶æ®µç”Ÿæˆçš„è¿‡å¤šä»¤ç‰Œå ç”¨èµ„æºï¼Œå¯¼è‡´äº‘ç«¯æ— æ³•å®ç°æ›´é«˜çš„ååé‡ã€‚åŒæ—¶ï¼Œç”±äºè®¾å¤‡èµ„æºæœ‰é™ï¼Œéšç€æç¤ºé•¿åº¦çš„å¢åŠ ï¼Œé¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰æ˜¾è‘—å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™ä¸€èµ„æºç“¶é¢ˆï¼Œæœ¬æ–‡æå‡ºå°†å¤§å‹è¯­è¨€æ¨¡å‹åˆ†ç¦»åˆ°äº‘ç«¯å’Œè®¾å¤‡ä¹‹é—´ã€‚å…·ä½“è€Œè¨€ï¼Œäº‘ç«¯åœ¨é¢„å¡«å……é˜¶æ®µä¸ºæ¯ä¸ªè®¾å¤‡æä¾›éƒ¨åˆ†å†…å®¹ï¼Œè®¾å¤‡åœ¨æ¥æ”¶åˆ°ç¬¬ä¸€ä¸ªä»¤ç‰Œåç«‹å³å“åº”ç”¨æˆ·ï¼Œä»è€Œé™ä½TTFTã€‚åç»­ä»¤ç‰Œé€šè¿‡é€Ÿåº¦æ§åˆ¶å™¨å¹³æ»‘å‘ˆç°ï¼Œç›´åˆ°è®¾å¤‡èµ¶ä¸Šè¿›åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTFTå‡å°‘è‡³å°‘60%ï¼Œæœ€å¤§TPOTçº¦ä¸ºæ•°åæ¯«ç§’ï¼Œäº‘ç«¯ååé‡æé«˜è‡³15å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³äº‘ç«¯ä¸è®¾å¤‡é—´çš„èµ„æºç“¶é¢ˆé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è§£ç é˜¶æ®µç”Ÿæˆè¿‡å¤šä»¤ç‰Œï¼Œå¯¼è‡´äº‘ç«¯ååé‡ä½ä¸”è®¾å¤‡é¦–æ¬¡ä»¤ç‰Œæ—¶é—´æ˜¾è‘—å¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹åˆ†ç¦»åˆ°äº‘ç«¯å’Œè®¾å¤‡ä¹‹é—´ï¼Œäº‘ç«¯åœ¨é¢„å¡«å……é˜¶æ®µä¸ºè®¾å¤‡æä¾›éƒ¨åˆ†å†…å®¹ï¼Œè®¾å¤‡åœ¨æ¥æ”¶åˆ°ç¬¬ä¸€ä¸ªä»¤ç‰Œåç«‹å³å“åº”ç”¨æˆ·ï¼Œä»è€Œé™ä½TTFTã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬äº‘ç«¯é¢„å¡«å……å’Œè®¾å¤‡å“åº”ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚äº‘ç«¯ç”Ÿæˆåˆå§‹ä»¤ç‰Œåï¼Œè®¾å¤‡å¿«é€Ÿå“åº”ç”¨æˆ·è¯·æ±‚ï¼Œéšåé€šè¿‡é€Ÿåº¦æ§åˆ¶å™¨å¹³æ»‘å‘ˆç°åç»­ä»¤ç‰Œï¼Œç›´åˆ°è®¾å¤‡èµ¶ä¸Šè¿›åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤„ç†åˆ†æ•£åˆ°äº‘ç«¯å’Œè®¾å¤‡ä¹‹é—´ï¼Œæ˜¾è‘—é™ä½äº†è®¾å¤‡çš„å“åº”å»¶è¿Ÿï¼Œå¹¶æé«˜äº†äº‘ç«¯çš„èµ„æºåˆ©ç”¨ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨é€Ÿåº¦æ§åˆ¶å™¨æ¥å¹³æ»‘åç»­ä»¤ç‰Œçš„å‘ˆç°ï¼ŒåŒæ—¶åˆ©ç”¨äº‘ç«¯ç”Ÿæˆçš„ä¸­é—´æ•°æ®æ¥ä¼˜åŒ–è®¾å¤‡çš„æ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰å‡å°‘è‡³å°‘60%ï¼Œæœ€å¤§è¾“å‡ºä»¤ç‰Œæ—¶é—´ï¼ˆTPOTï¼‰çº¦ä¸ºæ•°åæ¯«ç§’ï¼Œäº‘ç«¯ååé‡æå‡è‡³15å€ï¼ŒéªŒè¯äº†P/D-Deviceæ–¹æ¡ˆçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€åœ¨çº¿å®¢æœå’Œå®æ—¶ç¿»è¯‘ç­‰åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–äº‘ç«¯ä¸è®¾å¤‡é—´çš„åä½œï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œé™ä½å“åº”å»¶è¿Ÿï¼Œæ»¡è¶³å¯¹å®æ—¶æ€§è¦æ±‚è¾ƒé«˜çš„åº”ç”¨éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Serving disaggregated large language models has been widely adopted in industrial practice for enhanced performance. However, too many tokens generated in decoding phase, i.e., occupying the resources for a long time, essentially hamper the cloud from achieving a higher throughput. Meanwhile, due to limited on-device resources, the time to first token (TTFT), i.e., the latency of prefill phase, increases dramatically with the growth on prompt length. In order to concur with such a bottleneck on resources, i.e., long occupation in cloud and limited on-device computing capacity, we propose to separate large language model between cloud and devices. That is, the cloud helps a portion of the content for each device, only in its prefill phase. Specifically, after receiving the first token from the cloud, decoupling with its own prefill, the device responds to the user immediately for a lower TTFT. Then, the following tokens from cloud are presented via a speed controller for smoothed TPOT (the time per output token), until the device catches up with the progress. On-device prefill is then amortized using received tokens while the resource usage in cloud is controlled. Moreover, during cloud prefill, the prompt can be refined, using those intermediate data already generated, to further speed up on-device inference. We implement such a scheme P/D-Device, and confirm its superiority over other alternatives. We further propose an algorithm to decide the best settings. Real-trace experiments show that TTFT decreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud throughput increases by up to 15x.

