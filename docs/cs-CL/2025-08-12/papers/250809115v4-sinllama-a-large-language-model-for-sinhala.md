---
layout: default
title: SinLlama -- A Large Language Model for Sinhala
---

# SinLlama -- A Large Language Model for Sinhala

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09115" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09115v4</a>
  <a href="https://arxiv.org/pdf/2508.09115.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09115v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09115v4', 'SinLlama -- A Large Language Model for Sinhala')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: H. W. K. Aravinda, Rashad Sirajudeen, Samith Karunathilake, Nisansa de Silva, Surangika Ranathunga, Rishemjit Kaur

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12 (æ›´æ–°: 2025-11-08)

**DOI**: [10.1109/MERCon67903.2025.11217094](https://doi.org/10.1109/MERCon67903.2025.11217094)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSinLlamaä»¥æå‡åƒ§ä¼½ç½—è¯­çš„è¯­è¨€æ¨¡å‹æ”¯æŒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åƒ§ä¼½ç½—è¯­` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä½èµ„æºè¯­è¨€` `æ–‡æœ¬åˆ†ç±»` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šè¯­è¨€LLMå¯¹ä½èµ„æºè¯­è¨€å¦‚åƒ§ä¼½ç½—è¯­çš„æ”¯æŒä¸è¶³ï¼Œå¯¼è‡´å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡æ‰©å±•Llama-3-8Bï¼Œå¢å¼ºå…¶åˆ†è¯å™¨å¹¶åœ¨å¤§é‡åƒ§ä¼½ç½—è¯­è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæå‡ºäº†SinLlamaæ¨¡å‹ã€‚
3. SinLlamaåœ¨ä¸‰é¡¹æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ç»è¿‡å¾®è°ƒåï¼Œæ˜¾è‘—è¶…è¶Šäº†Llama-3-8Bçš„åŸºç¡€å’ŒæŒ‡ä»¤ç‰ˆæœ¬ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½èµ„æºè¯­è¨€å¦‚åƒ§ä¼½ç½—è¯­å¸¸è¢«å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¿½è§†ã€‚æœ¬ç ”ç©¶æ‰©å±•äº†ç°æœ‰çš„å¤šè¯­è¨€LLMï¼ˆLlama-3-8Bï¼‰ï¼Œä»¥æ›´å¥½åœ°æœåŠ¡äºåƒ§ä¼½ç½—è¯­ã€‚æˆ‘ä»¬å¢å¼ºäº†LLMçš„åˆ†è¯å™¨ï¼ŒåŠ å…¥äº†ç‰¹å®šäºåƒ§ä¼½ç½—è¯­çš„è¯æ±‡ï¼Œå¹¶åœ¨æ¸…ç†åçš„1000ä¸‡æ¡åƒ§ä¼½ç½—è¯­è¯­æ–™ä¸Šè¿›è¡Œäº†æŒç»­é¢„è®­ç»ƒï¼Œæœ€ç»ˆå½¢æˆäº†SinLlamaæ¨¡å‹ã€‚è¿™æ˜¯é¦–ä¸ªæ˜ç¡®æ”¯æŒåƒ§ä¼½ç½—è¯­çš„è§£ç å™¨åŸºç¡€å¼€æºLLMã€‚åœ¨å¯¹SinLlamaè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒä»¥å®Œæˆä¸‰é¡¹æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æ—¶ï¼Œå…¶è¡¨ç°æ˜¾è‘—è¶…è¶Šäº†Llama-3-8Bçš„åŸºç¡€å’ŒæŒ‡ä»¤å˜ä½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä½èµ„æºè¯­è¨€åƒ§ä¼½ç½—è¯­åœ¨ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ”¯æŒä¸è¶³é—®é¢˜ã€‚ç°æœ‰çš„å¤šè¯­è¨€LLMæœªèƒ½å……åˆ†è€ƒè™‘åƒ§ä¼½ç½—è¯­çš„ç‰¹æ€§ï¼Œå¯¼è‡´å…¶åœ¨ç›¸å…³ä»»åŠ¡ä¸­çš„æ€§èƒ½è¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ‰©å±•Llama-3-8Bæ¨¡å‹ï¼Œå¢å¼ºå…¶å¯¹åƒ§ä¼½ç½—è¯­çš„æ”¯æŒã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶è€…å¯¹æ¨¡å‹çš„åˆ†è¯å™¨è¿›è¡Œäº†æ”¹è¿›ï¼ŒåŠ å…¥äº†ç‰¹å®šäºåƒ§ä¼½ç½—è¯­çš„è¯æ±‡ï¼Œå¹¶åœ¨æ¸…ç†åçš„1000ä¸‡æ¡åƒ§ä¼½ç½—è¯­è¯­æ–™ä¸Šè¿›è¡Œäº†æŒç»­é¢„è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹Llama-3-8Bæ¨¡å‹çš„åŸºç¡€æ¶æ„è¿›è¡Œè°ƒæ•´ï¼Œå¢å¼ºåˆ†è¯å™¨ï¼Œè¿›è¡Œè¯­æ–™æ¸…ç†å’Œé¢„è®­ç»ƒï¼Œæœ€åè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒä»¥é€‚åº”ç‰¹å®šçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šSinLlamaæ˜¯é¦–ä¸ªæ˜ç¡®æ”¯æŒåƒ§ä¼½ç½—è¯­çš„è§£ç å™¨åŸºç¡€å¼€æºLLMï¼Œå…¶åˆ›æ–°åœ¨äºé’ˆå¯¹ä½èµ„æºè¯­è¨€çš„ç‰¹å®šéœ€æ±‚è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€‚ç”¨æ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œç ”ç©¶è€…å¯¹åˆ†è¯å™¨è¿›è¡Œäº†ç‰¹å®šäºåƒ§ä¼½ç½—è¯­çš„è¯æ±‡æ‰©å±•ï¼Œç¡®ä¿äº†æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è¯¥è¯­è¨€çš„ç‹¬ç‰¹ç‰¹å¾ã€‚åŒæ—¶ï¼Œé‡‡ç”¨äº†é€‚åˆåƒ§ä¼½ç½—è¯­çš„æŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸‰é¡¹æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒSinLlamaç»è¿‡æŒ‡ä»¤å¾®è°ƒåï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºLlama-3-8Bçš„åŸºç¡€å’ŒæŒ‡ä»¤å˜ä½“ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤„ç†åƒ§ä¼½ç½—è¯­æ–‡æœ¬æ—¶çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SinLlamaæ¨¡å‹çš„æå‡ºä¸ºåƒ§ä¼½ç½—è¯­çš„è‡ªç„¶è¯­è¨€å¤„ç†æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ï¼Œæ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æç­‰ã€‚éšç€å¯¹ä½èµ„æºè¯­è¨€çš„å…³æ³¨å¢åŠ ï¼Œè¯¥æ¨¡å‹çš„å®é™…ä»·å€¼å°†ä¸æ–­æå‡ï¼Œä¿ƒè¿›ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Low-resource languages such as Sinhala are often overlooked by open-source Large Language Models (LLMs). In this research, we extend an existing multilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM tokenizer with Sinhala specific vocabulary and perform continual pre-training on a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This is the very first decoder-based open-source LLM with explicit Sinhala support. When SinLlama was instruction fine-tuned for three text classification tasks, it outperformed base and instruct variants of Llama-3-8B by a significant margin.

