---
layout: default
title: Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training
---

# Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14904" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14904v1</a>
  <a href="https://arxiv.org/pdf/2508.14904.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14904v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14904v1', 'Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianfeng Si, Lin Sun, Zhewen Tan, Xiangzheng Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: 12 pages,5 figures,4 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€å…±è®­ç»ƒæ¡†æ¶ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…å®¹å®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å†…å®¹å®‰å…¨` `å…±è®­ç»ƒ` `é­”æ³•ä»¤ç‰Œ` `å®‰å…¨å¯¹é½` `åŠ¨æ€åˆ‡æ¢` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å†…å®¹å®‰å…¨æ–¹æ³•ä¾èµ–å¤šé˜¶æ®µè®­ç»ƒï¼Œç¼ºä¹çµæ´»çš„åæœŸæ§åˆ¶ï¼Œå½±å“æ¨¡å‹çš„å®ç”¨æ€§ã€‚
2. æå‡ºçš„å…±è®­ç»ƒæ¡†æ¶é€šè¿‡é­”æ³•ä»¤ç‰Œå®ç°å¤šç§å®‰å…¨è¡Œä¸ºçš„åŠ¨æ€åˆ‡æ¢ï¼Œæå‡äº†æ¨¡å‹çš„çµæ´»æ€§å’Œå¯æ§æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œ8Bæ¨¡å‹åœ¨å®‰å…¨æ€§èƒ½ä¸Šè¶…è¶Š671Bçš„DeepSeek-R1ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒå’Œéƒ¨ç½²æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å†…å®¹å®‰å…¨æ–¹é¢çš„æ–¹æ³•ï¼Œå¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œé€šå¸¸ä¾èµ–äºå¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç¼ºä¹ç»†ç²’åº¦çš„åæœŸå¯æ§æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å…±è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€çš„SFTé˜¶æ®µä¸­é«˜æ•ˆæ•´åˆå¤šç§å®‰å…¨è¡Œä¸ºï¼šç§¯æï¼ˆåˆæ³•/äº²ç¤¾ä¼šï¼‰ã€æ¶ˆæï¼ˆæœªè¿‡æ»¤/é£é™©å€¾å‘ï¼‰å’Œæ‹’ç»ï¼ˆæ‹’ç»å¯¼å‘/ä¿å®ˆï¼‰ã€‚æ¯ç§è¡Œä¸ºé€šè¿‡ç®€å•çš„ç³»ç»Ÿçº§æŒ‡ä»¤æˆ–é­”æ³•ä»¤ç‰ŒåŠ¨æ€æ¿€æ´»ï¼Œå®ç°æ¨ç†æ—¶çš„éšç§˜å’Œé«˜æ•ˆè¡Œä¸ºåˆ‡æ¢ã€‚è¿™ç§çµæ´»æ€§æ”¯æŒå¤šç§éƒ¨ç½²åœºæ™¯ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å®‰å…¨å¯¹é½è´¨é‡ä¸Šä¸SFT+DPOç›¸å½“ï¼Œä¸”åœ¨å®‰å…¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶ŠDeepSeek-R1ï¼ŒåŒæ—¶å¤§å¹…é™ä½è®­ç»ƒå¤æ‚æ€§å’Œéƒ¨ç½²æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†…å®¹å®‰å…¨æ–¹é¢çš„è®­ç»ƒæ–¹æ³•å¤šä¾èµ–äºå¤æ‚çš„å¤šé˜¶æ®µæµç¨‹ï¼Œå¯¼è‡´åæœŸå¯æ§æ€§ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³ä¸åŒåœºæ™¯çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„å…±è®­ç»ƒæ¡†æ¶é€šè¿‡å¼•å…¥é­”æ³•ä»¤ç‰Œï¼Œå®ç°äº†åœ¨å•ä¸€SFTé˜¶æ®µä¸­åŠ¨æ€æ¿€æ´»å¤šç§å®‰å…¨è¡Œä¸ºï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„çµæ´»æ€§å’Œå¯æ§æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶æ•´åˆäº†ç§¯æã€æ¶ˆæå’Œæ‹’ç»ä¸‰ç§å®‰å…¨è¡Œä¸ºï¼Œä½¿ç”¨é­”æ³•ä»¤ç‰Œä½œä¸ºæŒ‡ä»¤ï¼Œæ”¯æŒåœ¨æ¨ç†æ—¶å¿«é€Ÿåˆ‡æ¢è¡Œä¸ºï¼Œé€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å®‰å…¨å¯¹é½è¾¹é™…çš„æ¦‚å¿µï¼Œé€šè¿‡æ˜ç¡®çš„å“åº”åˆ†å¸ƒåŒºåˆ†ä¸åŒçš„å®‰å…¨æ¨¡å¼ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯æ§æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å®‰å…¨è¡Œä¸ºçš„åˆ†å¸ƒï¼ŒåŒæ—¶è°ƒæ•´äº†æ¨¡å‹çš„ç½‘ç»œç»“æ„ä»¥æ”¯æŒé­”æ³•ä»¤ç‰Œçš„åŠ¨æ€æ¿€æ´»ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„8Bæ¨¡å‹åœ¨å®‰å…¨æ€§èƒ½ä¸Šè¶…è¶Šäº†DeepSeek-R1ï¼ˆ671Bï¼‰ï¼Œåœ¨å®‰å…¨å¯¹é½è´¨é‡ä¸Šä¸SFT+DPOç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®­ç»ƒå¤æ‚æ€§å’Œéƒ¨ç½²æˆæœ¬ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ã€åœ¨çº¿å®¢æœç³»ç»Ÿå’Œæ•™è‚²å¹³å°ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç”¨æˆ·äº¤äº’çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œéšç€æ¨¡å‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œè¯¥æ¡†æ¶æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„åœºæ™¯ä¸­åº”ç”¨ï¼Œæ¨åŠ¨å†…å®¹å®‰å…¨æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current methods for content safety in Large Language Models (LLMs), such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), often rely on multi-stage training pipelines and lack fine-grained, post-deployment controllability. To address these limitations, we propose a unified co-training framework that efficiently integrates multiple safety behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and rejective (refusal-oriented/conservative) within a single SFT stage. Notably, each behavior is dynamically activated via a simple system-level instruction, or magic token, enabling stealthy and efficient behavioral switching at inference time. This flexibility supports diverse deployment scenarios, such as positive for safe user interaction, negative for internal red-teaming, and rejective for context-aware refusals triggered by upstream moderation signals. This co-training strategy induces a distinct Safety Alignment Margin in the output space, characterized by well-separated response distributions corresponding to each safety mode. The existence of this margin provides empirical evidence for the model's safety robustness and enables unprecedented fine-grained control. Experiments show that our method matches the safety alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1 (671B) in safety performance, while significantly reducing both training complexity and deployment costs. This work presents a scalable, efficient, and highly controllable solution for LLM content safety.

