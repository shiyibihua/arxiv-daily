---
layout: default
title: AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators
---

# AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09101" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.09101v1</a>
  <a href="https://arxiv.org/pdf/2508.09101.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09101v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09101v1', 'AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jason Chou, Ao Liu, Yuchi Deng, Zhiying Zeng, Tao Zhang, Haotian Zhu, Jianwei Cai, Yue Mao, Chenchen Zhang, Lingyun Tan, Ziyan Xu, Bohui Zhai, Hengyi Liu, Speed Zhu, Wiggin Zhou, Fengzong Lian

**ÂàÜÁ±ª**: cs.CL, cs.SE

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-12

**Â§áÊ≥®**: Homepage: https://autocodebench.github.io/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫AutoCodeBench‰ª•Ëß£ÂÜ≥Áé∞Êúâ‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜÁöÑÂ±ÄÈôêÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰ª£Á†ÅÁîüÊàê` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Ëá™Âä®ÂåñÂü∫ÂáÜ` `Â§öËØ≠Ë®ÄÊîØÊåÅ` `Êï∞ÊçÆÈõÜÁîüÊàê` `Êú∫Âô®Â≠¶‰π†ËØÑ‰º∞` `ËΩØ‰ª∂ÂºÄÂèë`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜ‰æùËµñ‰∫∫Â∑•Ê†áÊ≥®ÔºåÈöæ‰ª•Êâ©Â±ïÔºå‰∏î‰∏ªË¶ÅÈõÜ‰∏≠Âú®PythonÔºåÁº∫‰πèÂ§öËØ≠Ë®ÄÂíåÂ§öÊ†∑ÊÄß„ÄÇ
2. ÊèêÂá∫AutoCodeGenÊñπÊ≥ïÔºåÈÄöËøáLLMsËá™Âä®ÁîüÊàêÈ´òÈöæÂ∫¶Â§öËØ≠Ë®ÄÊï∞ÊçÆÈõÜÔºåÁ°Æ‰øùÊµãËØïÁî®‰æãÁöÑÊ≠£Á°ÆÊÄßÂíåÂÆåÊï¥ÊÄß„ÄÇ
3. Âú®AutoCodeBench‰∏äËØÑ‰º∞‰∫Ü30Â§öÁßçÂºÄÊ∫êÂíå‰∏ìÊúâLLMsÔºåÁªìÊûúÊòæÁ§∫Âç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÊ®°Âûã‰πüÈöæ‰ª•Â∫îÂØπËøô‰∫õÂ§çÊùÇ‰ªªÂä°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ö‰∏™È¢ÜÂüüÂ±ïÁé∞‰∫ÜÂçìË∂äÁöÑËÉΩÂäõÔºåÂÖ∂‰∏≠‰ª£Á†ÅÁîüÊàêÊàê‰∏∫ÈáçÁÇπÂÖ≥Ê≥®ÁöÑÈ¢ÜÂüü„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑ‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜÂ≠òÂú®ÊòæËëóÁöÑÂ±ÄÈôêÊÄßÔºå‰∏ªË¶Å‰æùËµñ‰∫∫Â∑•Ê†áÊ≥®ÔºåÈöæ‰ª•Êâ©Â±ïÂà∞‰∏çÂêåÁºñÁ®ãËØ≠Ë®ÄÂíåÈóÆÈ¢òÂ§çÊùÇÂ∫¶„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∞Âü∫ÂáÜ‰∏ªË¶ÅÈõÜ‰∏≠Âú®PythonÔºåÁº∫‰πèÂ§öÊ†∑ÊÄßÂíåÈöæÂ∫¶„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜAutoCodeGenÔºå‰∏ÄÁßçËá™Âä®ÁîüÊàêÈ´òÈöæÂ∫¶Â§öËØ≠Ë®Ä‰ª£Á†ÅÁîüÊàêÊï∞ÊçÆÈõÜÁöÑÊñπÊ≥ïÔºåÈÅøÂÖç‰∫Ü‰∫∫Â∑•Ê†áÊ≥®ÁöÑÈúÄÊ±Ç„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜAutoCodeBenchÔºå‰∏Ä‰∏™ÂåÖÂê´3920‰∏™ÈóÆÈ¢òÁöÑÂ§ßËßÑÊ®°‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞LLMsÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÂíåÂ§öÊ†∑ÊÄßÁöÑÂ§öËØ≠Ë®Ä‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞Êúâ‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜÁöÑÂ±ÄÈôêÊÄßÔºåÁâπÂà´ÊòØ‰∫∫Â∑•Ê†áÊ≥®ÁöÑÈ´òÊàêÊú¨ÂíåÂ§öËØ≠Ë®ÄÊîØÊåÅ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫AutoCodeGenÔºåÈÄöËøáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËá™Âä®ÁîüÊàêÈ´òÈöæÂ∫¶ÁöÑÂ§öËØ≠Ë®Ä‰ª£Á†ÅÁîüÊàêÊï∞ÊçÆÈõÜÔºåÈÅøÂÖç‰∫Ü‰∫∫Â∑•Âπ≤È¢ÑÔºåÂêåÊó∂Á°Æ‰øùÊï∞ÊçÆÁöÑË¥®ÈáèÂíåÂ§öÊ†∑ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨‰ΩøÁî®LLMsÁîüÊàêÊµãËØïËæìÂÖ•ÔºåÈÄöËøáÂ§öËØ≠Ë®ÄÊ≤ôÁÆ±Ëé∑ÂèñÊµãËØïËæìÂá∫ÔºåÂπ∂ÈÄöËøáÈÄÜÂ∫èÈóÆÈ¢òÁîüÊàêÂíåÂ§öÈáçËøáÊª§Ê≠•È™§Á°Æ‰øùÊï∞ÊçÆË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAutoCodeGenÁöÑÊúÄÂ§ßÂàõÊñ∞Âú®‰∫éÂÖ∂ÂÆåÂÖ®Ëá™Âä®ÂåñÁöÑÁîüÊàêËøáÁ®ãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊï∞ÊçÆÈõÜÁöÑËßÑÊ®°ÂíåÂ§öÊ†∑ÊÄßÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÂ±ÄÈôê„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Êï∞ÊçÆÁîüÊàêËøáÁ®ã‰∏≠ÔºåÈááÁî®‰∫ÜÂ§öÈáçËøáÊª§Êú∫Âà∂ÂíåÈÄÜÂ∫èÈóÆÈ¢òÁîüÊàêÁ≠ñÁï•Ôºå‰ª•Á°Æ‰øùÁîüÊàêÈóÆÈ¢òÁöÑÈöæÂ∫¶ÂíåÂ§öÊ†∑ÊÄßÔºåÂêåÊó∂ËÆæËÆ°‰∫ÜAutoCodeBenchÂíåAutoCodeBench-Lite‰∏§‰∏™ÁâàÊú¨‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑËØÑ‰º∞ÈúÄÊ±Ç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåË∂ÖËøá30ÁßçLLMsÂú®AutoCodeBench‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇÊÄßÂíåÂ§öÊ†∑ÊÄßÊñπÈù¢ÔºåÊòæÁ§∫Âá∫ÂΩìÂâçÊ®°ÂûãÂú®Â§ÑÁêÜÂ§öËØ≠Ë®ÄÂíåÈ´òÈöæÂ∫¶‰ªªÂä°Êó∂ÁöÑÂ±ÄÈôêÊÄß„ÄÇËøô‰∏ÄÂèëÁé∞Âº∫Ë∞É‰∫ÜAutoCodeBenchÂú®Êé®Âä®Êõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑ‰ª£Á†ÅÁîüÊàêÁ†îÁ©∂‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËΩØ‰ª∂ÂºÄÂèë„ÄÅÊïôËÇ≤ÂíåËá™Âä®ÂåñÊµãËØïÁ≠â„ÄÇÈÄöËøáÊèê‰æõÈ´òË¥®ÈáèÁöÑÂ§öËØ≠Ë®Ä‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜÔºåAutoCodeBenchÂèØ‰ª•Â∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÂíåÂºÄÂèëËÄÖÊõ¥Â•ΩÂú∞ËØÑ‰º∞ÂíåÊîπËøõ‰ª£Á†ÅÁîüÊàêÊ®°ÂûãÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ïÂíåÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios.

