---
layout: default
title: Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments
---

# Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08791" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08791v2</a>
  <a href="https://arxiv.org/pdf/2508.08791.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08791v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08791v2', 'Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junjie Ye, Changhao Jiang, Zhengyin Du, Yufei Xu, Xuesong Yao, Zhiheng Xi, Xiaoran Fan, Qi Zhang, Tao Gui, Xuanjing Huang, Jiecao Chen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12 (æ›´æ–°: 2025-09-12)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªåŠ¨åŒ–ç¯å¢ƒæ„å»ºç®¡é“ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹å·¥å…·ä½¿ç”¨èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å·¥å…·ä½¿ç”¨` `å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨åŒ–ç¯å¢ƒ` `å¥–åŠ±æœºåˆ¶` `æ¨¡å‹è®­ç»ƒ` `æ™ºèƒ½ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å·¥å…·ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸Šå­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ç¨³å®šçš„è®­ç»ƒç¯å¢ƒå’Œå¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–ç¯å¢ƒæ„å»ºç®¡é“ï¼Œèƒ½å¤Ÿé«˜æ•ˆåˆ›å»ºè®­ç»ƒç¯å¢ƒå¹¶æä¾›è¯¦ç»†åé¦ˆï¼Œç»“åˆå¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒè§„æ¨¡çš„LLMsä¸Šæ˜¾è‘—æå‡äº†å·¥å…·ä½¿ç”¨æ€§èƒ½ï¼Œä¸”æœªé™ä½æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆçš„å·¥å…·ä½¿ç”¨å¯¹äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç¯å¢ƒçš„æœ‰æ„ä¹‰äº’åŠ¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ä¸“é—¨ä¸ºå·¥å…·ä½¿ç”¨è®¾è®¡çš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œè¿›å±•å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–ç¯å¢ƒæ„å»ºç®¡é“ï¼Œç»“åˆåœºæ™¯åˆ†è§£ã€æ–‡æ¡£ç”Ÿæˆã€åŠŸèƒ½é›†æˆã€å¤æ‚æ€§æ‰©å±•å’Œå±€éƒ¨éƒ¨ç½²ï¼Œèƒ½å¤Ÿåˆ›å»ºé«˜è´¨é‡çš„è®­ç»ƒç¯å¢ƒï¼Œæä¾›è¯¦ç»†ä¸”å¯æµ‹é‡çš„åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ï¼Œè¯„ä¼°å·¥å…·ä½¿ç”¨çš„ç²¾ç¡®æ€§å’Œä»»åŠ¡æ‰§è¡Œçš„å®Œæ•´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å·¥å…·ä½¿ç”¨æ€§èƒ½ï¼Œè€Œä¸æŸå®³å…¶ä¸€èˆ¬èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨ä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ„å»ºç¨³å®šè®­ç»ƒç¯å¢ƒå’Œè®¾è®¡æœ‰æ•ˆå¥–åŠ±æœºåˆ¶æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è‡ªåŠ¨åŒ–ç¯å¢ƒæ„å»ºç®¡é“ï¼Œç»“åˆå¤šç§æŠ€æœ¯æ‰‹æ®µï¼Œåˆ›å»ºé«˜è´¨é‡çš„è®­ç»ƒç¯å¢ƒï¼Œå¹¶å¼•å…¥å¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥æå‡æ¨¡å‹çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç¯å¢ƒæ„å»ºã€åé¦ˆç”Ÿæˆå’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚ç¯å¢ƒæ„å»ºæ¨¡å—è´Ÿè´£åœºæ™¯åˆ†è§£å’Œæ–‡æ¡£ç”Ÿæˆï¼Œåé¦ˆç”Ÿæˆæ¨¡å—æä¾›å¯æµ‹é‡çš„å¥–åŠ±ï¼Œæ¨¡å‹è®­ç»ƒæ¨¡å—åˆ™æ•´åˆæ ‡å‡†RLç®—æ³•è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè‡ªåŠ¨åŒ–ç¯å¢ƒæ„å»ºå’Œå¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„æ‰‹åŠ¨æ„å»ºå’Œæ¨¡ç³Šå¥–åŠ±æœºåˆ¶å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç¯å¢ƒæ„å»ºä¸­ï¼Œé‡‡ç”¨å¤æ‚æ€§æ‰©å±•å’Œå±€éƒ¨éƒ¨ç½²ç­–ç•¥ï¼Œç¡®ä¿è®­ç»ƒç¯å¢ƒçš„å¤šæ ·æ€§å’Œç¨³å®šæ€§ï¼›å¥–åŠ±æœºåˆ¶åˆ™é€šè¿‡è¯„ä¼°å·¥å…·ä½¿ç”¨çš„ç²¾ç¡®æ€§å’Œä»»åŠ¡æ‰§è¡Œçš„å®Œæ•´æ€§æ¥è¿›è¡Œè®¾è®¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¯¥æ–¹æ³•çš„æ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨æ€§èƒ½ä¸Šæ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œè€Œæ¨¡å‹çš„é€šç”¨èƒ½åŠ›ä¿æŒä¸å˜ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„é€‚åº”æ€§ä¸ç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–ç¼–ç¨‹ã€æœºå™¨äººæ§åˆ¶ç­‰ï¼Œèƒ½å¤Ÿæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…ä»»åŠ¡ä¸­çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ä¸å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨å¤šç§é¢†åŸŸä¸­å®ç°æ›´é«˜æ•ˆçš„ä»»åŠ¡æ‰§è¡Œä¸äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce a verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models' tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models.

