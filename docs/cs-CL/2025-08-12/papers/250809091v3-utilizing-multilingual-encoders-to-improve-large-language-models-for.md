---
layout: default
title: Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages
---

# Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09091" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.09091v3</a>
  <a href="https://arxiv.org/pdf/2508.09091.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09091v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09091v3', 'Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Imalsha Puranegedara, Themira Chathumina, Nisal Ranathunga, Nisansa de Silva, Surangika Ranathunga, Mokanarangan Thayaparan

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-12 (Êõ¥Êñ∞: 2025-11-08)

**DOI**: [10.1109/MERCon67903.2025.11216992](https://doi.org/10.1109/MERCon67903.2025.11216992)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â§öÂ±ÇËûçÂêàÁ≠ñÁï•‰ª•ÊèêÂçá‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑLLMÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰ΩéËµÑÊ∫êËØ≠Ë®Ä` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Â§öËØ≠Ë®ÄÂ§ÑÁêÜ` `‰∏≠Èó¥Â±ÇËûçÂêà` `TransformerÊ®°Âûã` `ËØ≠Ë®ÄÊ®°Âûã‰ºòÂåñ` `Êï∞ÊçÆÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏äÁöÑË°®Áé∞‰∏ç‰Ω≥Ôºå‰∏ªË¶ÅÁî±‰∫éËÆ≠ÁªÉÊï∞ÊçÆÁöÑËã±ËØ≠‰∏≠ÂøÉÂåñÔºåÂØºËá¥Ê®°ÂûãÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜÂ§öËØ≠Ë®ÄËæìÂÖ•„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊû∂ÊûÑÔºåÈÄöËøáËûçÂêàÊâÄÊúâ‰∏≠Èó¥Â±ÇÁöÑË°®Á§∫ÔºåÂ¢ûÂº∫‰∫Ü‰º†ÈÄíÁªôÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËØ≠Ë®Ä‰ø°ÊÅØ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêÊ®°ÂûãÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂùáÊòæËëó‰ºò‰∫éÂü∫Á∫øÔºåÂ∞§ÂÖ∂Âú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏äÂèñÂæó‰∫ÜÊòéÊòæÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëã±ËØ≠‰∏äË°®Áé∞‰ºòÂºÇÔºå‰ΩÜÂú®‰ΩéËµÑÊ∫êËØ≠Ë®ÄÔºàLRLsÔºâ‰∏äÊÄßËÉΩÊòæËëó‰∏ãÈôçÔºå‰∏ªË¶ÅÁî±‰∫éËÆ≠ÁªÉÊï∞ÊçÆÁöÑËã±ËØ≠‰∏≠ÂøÉÂåñ„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇLangBridge‰ªÖ‰ΩøÁî®ÊúÄÁªàÁºñÁ†ÅÂ±ÇÂØπLLMsËøõË°åÂØπÈΩê„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊû∂ÊûÑÔºåËûçÂêàÊâÄÊúâ‰∏≠Èó¥Â±ÇÔºå‰∏∞ÂØå‰º†ÈÄíÁªôLLMÁöÑËØ≠Ë®Ä‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂåÖÊã¨‰∏§ÁßçÁ≠ñÁï•ÔºöÂÖ®Â±ÄSoftmaxÂä†ÊùÉÂíåÂü∫‰∫éTransformerÁöÑSoftmaxÊ®°ÂûãÔºåÂêéËÄÖÂ≠¶‰π†ÁâπÂÆö‰∫étokenÁöÑÊùÉÈáç„ÄÇÈÄöËøáÂ∞ÜËûçÂêàË°®Á§∫Êò†Â∞ÑÂà∞LLMÁöÑÂµåÂÖ•Á©∫Èó¥Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÂ§ÑÁêÜÂ§öËØ≠Ë®ÄËæìÂÖ•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑTransformer SoftmaxÊ®°ÂûãÂú®XNLI„ÄÅIndicXNLI„ÄÅÂÉß‰ºΩÁΩóÊñ∞ÈóªÂàÜÁ±ªÂíå‰∫öÈ©¨ÈÄäËØÑËÆ∫‰∏äÊòæËëóË∂ÖË∂äLangBridgeÂü∫Á∫øÔºåÂ∞§ÂÖ∂Âú®LRLs‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂÉß‰ºΩÁΩóÂàÜÁ±ªÂáÜÁ°ÆÁéá‰ªé71.66%ÊèêÂçáËá≥75.86%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏äÁöÑÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂ¶ÇLangBridge‰ªÖÂà©Áî®ÊúÄÁªàÁºñÁ†ÅÂ±ÇÔºåÊú™ËÉΩÂÖÖÂàÜÂà©Áî®‰∏≠Èó¥Â±Ç‰ø°ÊÅØ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáËûçÂêàÊâÄÊúâ‰∏≠Èó¥Â±ÇÁöÑË°®Á§∫ÔºåÂ¢ûÂº∫‰º†ÈÄíÁªôLLMÁöÑËØ≠Ë®Ä‰ø°ÊÅØÔºå‰ªéËÄåÊèêÂçáÂÖ∂Âú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏äÁöÑË°®Áé∞„ÄÇËØ•ËÆæËÆ°Êó®Âú®ÂÖÖÂàÜÂà©Áî®Â§öÂ±ÇÊ¨°ÁöÑËØ≠Ë®ÄÁâπÂæÅ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂÖ®Â±ÄSoftmaxÂä†ÊùÉÁî®‰∫éËØÑ‰º∞ÂêÑÂ±ÇÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÂèäÂü∫‰∫éTransformerÁöÑSoftmaxÊ®°ÂûãÁî®‰∫éÂ≠¶‰π†tokenÁâπÂÆöÁöÑÊùÉÈáç„ÄÇËûçÂêàÂêéÁöÑË°®Á§∫Ë¢´Êò†Â∞ÑÂà∞LLMÁöÑÂµåÂÖ•Á©∫Èó¥„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éËûçÂêàÊâÄÊúâ‰∏≠Èó¥Â±ÇÁöÑË°®Á§∫ÔºåËÄåÈùû‰ªÖ‰æùËµñÊúÄÁªàÂ±Ç„ÄÇËøô‰∏ÄÊñπÊ≥ïÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂØπ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ®°ÂûãËÆ≠ÁªÉ‰ªÖ‰ΩøÁî®Ëã±ËØ≠Êï∞ÊçÆÔºåÊú™‰ΩøÁî®‰ªª‰ΩïÂπ≥Ë°åÊàñÂ§öËØ≠Ë®ÄÊï∞ÊçÆ„ÄÇÂÖ®Â±ÄSoftmaxÂíåTransformer SoftmaxÁöÑËÆæËÆ°Á°Æ‰øù‰∫ÜÂ±ÇÈó¥‰ø°ÊÅØÁöÑÊúâÊïàÊï¥Âêà„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Êú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜËØ¥ÊòéÔºåÈúÄÂèÇËÄÉÂéüÊñá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêTransformer SoftmaxÊ®°ÂûãÂú®XNLI„ÄÅIndicXNLI„ÄÅÂÉß‰ºΩÁΩóÊñ∞ÈóªÂàÜÁ±ªÂíå‰∫öÈ©¨ÈÄäËØÑËÆ∫‰∏äÊòæËëóË∂ÖË∂äLangBridgeÂü∫Á∫øÔºåÁâπÂà´ÊòØÂú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏äÔºåÂÉß‰ºΩÁΩóÂàÜÁ±ªÂáÜÁ°ÆÁéá‰ªé71.66%ÊèêÂçáËá≥75.86%ÔºåÊï¥‰ΩìXNLIÂáÜÁ°ÆÁéá‰ªé70.36%ÊèêÂçáËá≥71.50%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Â§öËØ≠Ë®ÄÁøªËØë„ÄÅË∑®ËØ≠Ë®Ä‰ø°ÊÅØÊ£ÄÁ¥¢Âíå‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°„ÄÇÈÄöËøáÊèêÂçá‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§ÑÁêÜËÉΩÂäõÔºåËØ•ÊñπÊ≥ïÊúâÂä©‰∫éÂÆûÁé∞Êõ¥ÂÖ¨Âπ≥ÁöÑËØ≠Ë®ÄÊäÄÊúØÂèëÂ±ïÔºå‰øÉËøõÂÖ®ÁêÉËØ≠Ë®ÄÁöÑÂπ≥Á≠â‰ΩøÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) excel in English, but their performance degrades significantly on low-resource languages (LRLs) due to English-centric training. While methods like LangBridge align LLMs with multilingual encoders such as the Massively Multilingual Text-to-Text Transfer Transformer (mT5), they typically use only the final encoder layer. We propose a novel architecture that fuses all intermediate layers, enriching the linguistic information passed to the LLM. Our approach features two strategies: (1) a Global Softmax weighting for overall layer importance, and (2) a Transformer Softmax model that learns token-specific weights. The fused representations are mapped into the LLM's embedding space, enabling it to process multilingual inputs. The model is trained only on English data, without using any parallel or multilingual data. Evaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews, our Transformer Softmax model significantly outperforms the LangBridge baseline. We observe strong performance gains in LRLs, improving Sinhala classification accuracy from 71.66% to 75.86% and achieving clear improvements across Indic languages such as Tamil, Bengali, and Malayalam. These specific gains contribute to an overall boost in average XNLI accuracy from 70.36% to 71.50%. This approach offers a scalable, data-efficient path toward more capable and equitable multilingual LLMs.

