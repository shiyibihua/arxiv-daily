---
layout: default
title: ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning
---

# ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09303" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09303v1</a>
  <a href="https://arxiv.org/pdf/2508.09303.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09303v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09303v1', 'ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shu Zhao, Tan Yu, Anbang Xu, Japinder Singh, Aaditya Shukla, Rama Akkiraju

**åˆ†ç±»**: cs.CL, cs.AI, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºParallelSearchä»¥è§£å†³æœç´¢æŸ¥è¯¢çš„å¹¶è¡Œå¤„ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¹¶è¡Œå¤„ç†` `å¼ºåŒ–å­¦ä¹ ` `ä¿¡æ¯æ£€ç´¢` `æŸ¥è¯¢åˆ†è§£` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æœç´¢æŸ¥è¯¢æ—¶å­˜åœ¨ä¸¥æ ¼çš„é¡ºåºå¤„ç†é™åˆ¶ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤šä¸ªå®ä½“æ¯”è¾ƒçš„æƒ…å†µä¸‹ã€‚
2. æœ¬æ–‡æå‡ºParallelSearchæ¡†æ¶ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¹¶è¡ŒåŒ–æŸ¥è¯¢ç»“æ„ï¼Œæ”¯æŒåŒæ—¶æ‰§è¡Œå¤šä¸ªæœç´¢æ“ä½œã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒParallelSearchåœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†ä¸Šå¹³å‡æå‡2.9%çš„æ€§èƒ½ï¼Œåœ¨å¯å¹¶è¡ŒåŒ–é—®é¢˜ä¸Šæå‡12.7%ï¼Œä¸”å‡å°‘äº†LLMè°ƒç”¨æ¬¡æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¢å¼ºæ¨ç†çš„æœç´¢ä»£ç†å¦‚Search-R1ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è®­ç»ƒï¼Œå±•ç°äº†ä»å¤–éƒ¨çŸ¥è¯†æºè¿›è¡Œå¤šæ­¥ä¿¡æ¯æ£€ç´¢çš„å“è¶Šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æœç´¢æŸ¥è¯¢æ—¶å­˜åœ¨ä¸¥æ ¼çš„é¡ºåºå¤„ç†é™åˆ¶ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ParallelSearchï¼Œä¸€ä¸ªæ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿè¯†åˆ«å¯å¹¶è¡ŒåŒ–çš„æŸ¥è¯¢ç»“æ„å¹¶åŒæ—¶æ‰§è¡Œå¤šä¸ªæœç´¢æ“ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒParallelSearchåœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†ä¸Šå¹³å‡æå‡äº†2.9%çš„æ€§èƒ½ï¼Œåœ¨å¯å¹¶è¡ŒåŒ–é—®é¢˜ä¸Šæå‡äº†12.7%ï¼Œä¸”ä»…éœ€69.6%çš„LLMè°ƒç”¨æ¬¡æ•°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœç´¢ä»£ç†åœ¨å¤„ç†æŸ¥è¯¢æ—¶çš„é¡ºåºå¤„ç†é™åˆ¶ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨æŸ¥è¯¢ä¸­çš„å¹¶è¡Œæ€§ï¼Œå½±å“äº†å¤šæ­¥ä¿¡æ¯æ£€ç´¢çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºParallelSearchæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿LLMsèƒ½å¤Ÿè¯†åˆ«å¹¶è¡ŒåŒ–çš„æŸ¥è¯¢ç»“æ„ï¼Œä»è€Œå®ç°å¤šä¸ªæœç´¢æ“ä½œçš„å¹¶è¡Œæ‰§è¡Œï¼Œæå‡ä¿¡æ¯æ£€ç´¢æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šParallelSearchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æŸ¥è¯¢åˆ†è§£æ¨¡å—ã€å¹¶è¡Œæœç´¢æ¨¡å—å’Œå¥–åŠ±è¯„ä¼°æ¨¡å—ã€‚æŸ¥è¯¢åˆ†è§£æ¨¡å—è´Ÿè´£è¯†åˆ«ç‹¬ç«‹çš„æŸ¥è¯¢æˆåˆ†ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™å¹¶è¡Œæœç´¢æ¨¡å—è¿›è¡Œå¤„ç†ï¼Œæœ€åé€šè¿‡å¥–åŠ±è¯„ä¼°æ¨¡å—å¯¹ç»“æœè¿›è¡Œè¯„ä¼°å’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸“é—¨çš„å¥–åŠ±å‡½æ•°ï¼Œæ¿€åŠ±æ¨¡å‹è¯†åˆ«ç‹¬ç«‹çš„æŸ¥è¯¢ç»„ä»¶ï¼ŒåŒæ—¶è€ƒè™‘ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€æŸ¥è¯¢åˆ†è§£è´¨é‡å’Œå¹¶è¡Œæ‰§è¡Œçš„å¥½å¤„ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæå‡å¤„ç†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œè®¾ç½®äº†é’ˆå¯¹å¹¶è¡ŒæŸ¥è¯¢çš„æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–äº†æŸ¥è¯¢åˆ†è§£çš„è´¨é‡ï¼Œå¹¶è®¾è®¡äº†é€‚åº”å¹¶è¡Œæ‰§è¡Œçš„ç½‘ç»œç»“æ„ï¼Œä»¥æ”¯æŒé«˜æ•ˆçš„ä¿¡æ¯æ£€ç´¢ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒParallelSearchåœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†ä¸Šå¹³å‡æå‡äº†2.9%çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¯å¹¶è¡ŒåŒ–é—®é¢˜ä¸Šæå‡äº†12.7%ã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºé¡ºåºæ–¹æ³•ï¼ŒParallelSearchä»…éœ€69.6%çš„LLMè°ƒç”¨æ¬¡æ•°ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœç´¢å¼•æ“ã€é—®ç­”ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢å¹³å°ã€‚é€šè¿‡æå‡æŸ¥è¯¢å¤„ç†æ•ˆç‡ï¼ŒParallelSearchèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡æ•°æ®ç¯å¢ƒä¸­æä¾›æ›´å¿«é€Ÿå’Œå‡†ç¡®çš„å“åº”ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning-augmented search agents such as Search-R1, trained via reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable capabilities in multi-step information retrieval from external knowledge sources. These agents address the limitations of their parametric memory by dynamically gathering relevant facts to address complex reasoning tasks. However, existing approaches suffer from a fundamental architectural limitation: they process search queries strictly sequentially, even when handling inherently parallelizable and logically independent comparisons. This sequential bottleneck significantly constrains computational efficiency, particularly for queries that require multiple entity comparisons. To address this critical limitation, we propose ParallelSearch, a novel reinforcement learning framework that empowers large language models (LLMs) to recognize parallelizable query structures and execute multiple search operations concurrently. Our approach introduces dedicated reward functions that incentivize the identification of independent query components while preserving answer accuracy through jointly considering correctness, query decomposition quality, and parallel execution benefits. Comprehensive experiments demonstrate that ParallelSearch outperforms state-of-the-art baselines by an average performance gain of 2.9% across seven question-answering benchmarks. Notably, on parallelizable questions, our method achieves a 12.7% performance improvement while requiring only 69.6% of the LLM calls compared to sequential approaches.

