---
layout: default
title: Train Long, Think Short: Curriculum Learning for Efficient Reasoning
---

# Train Long, Think Short: Curriculum Learning for Efficient Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08940" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08940v1</a>
  <a href="https://arxiv.org/pdf/2508.08940.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08940v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08940v1', 'Train Long, Think Short: Curriculum Learning for Efficient Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, Bernard Ghanem

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: Under Review

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hammoudhasan/curriculum_grpo)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ä»¥æé«˜é•¿è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯¾ç¨‹å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–` `é•¿åº¦æ§åˆ¶` `æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨å›ºå®šé•¿åº¦è®­ç»ƒé¢„ç®—ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å­¦ä¹ ä¸­çš„æ¢ç´¢ä¸å‹ç¼©è¿‡ç¨‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡é€æ­¥æ”¶ç´§æ ‡è®°é¢„ç®—ï¼Œä¿ƒè¿›æ¨¡å‹ä»æ¢ç´¢åˆ°æç‚¼æœ‰æ•ˆæ¨ç†ç­–ç•¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¾ç¨‹å­¦ä¹ è®­ç»ƒåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºå›ºå®šé¢„ç®—åŸºçº¿ï¼Œå‡†ç¡®æ€§å’Œæ ‡è®°æ•ˆç‡æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„ç ”ç©¶å¼•å…¥äº†æ˜¾å¼é•¿åº¦æ§åˆ¶ï¼Œä»¥åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–å›ºå®šé•¿åº¦çš„è®­ç»ƒé¢„ç®—ï¼Œæœªèƒ½åˆ©ç”¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸å‹ç¼©çš„è‡ªç„¶è¿›å±•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„é•¿åº¦æ§åˆ¶æ¨ç†ç­–ç•¥ï¼Œä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚è¯¥æ–¹æ³•ä»å®½æ¾çš„æ ‡è®°é¢„ç®—å¼€å§‹ï¼Œé€æ¸æ”¶ç´§ï¼Œé¼“åŠ±æ¨¡å‹é¦–å…ˆå‘ç°æœ‰æ•ˆçš„è§£å†³ç­–ç•¥ï¼Œç„¶åå°†å…¶æç‚¼ä¸ºæ›´ç®€æ´çš„æ¨ç†è½¨è¿¹ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒï¼Œè¯¾ç¨‹å­¦ä¹ è®­ç»ƒåœ¨ç›¸åŒçš„æœ€ç»ˆé¢„ç®—ä¸‹å§‹ç»ˆä¼˜äºå›ºå®šé¢„ç®—åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—çš„æ ‡è®°æ•ˆç‡æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡è¿‡ç¨‹ä¸­ï¼Œå›ºå®šé•¿åº¦è®­ç»ƒé¢„ç®—çš„å±€é™æ€§ï¼Œå¯¼è‡´æ¨¡å‹æœªèƒ½å……åˆ†åˆ©ç”¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸å‹ç¼©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„ç­–ç•¥ï¼ŒåˆæœŸä½¿ç”¨å®½æ¾çš„æ ‡è®°é¢„ç®—ï¼Œé€æ­¥æ”¶ç´§é¢„ç®—ï¼Œé¼“åŠ±æ¨¡å‹å…ˆæ¢ç´¢æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå†è¿›è¡Œç®€åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼ŒåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šåˆå§‹å®½æ¾çš„æ ‡è®°é¢„ç®—ã€é€æ­¥æ”¶ç´§çš„è®­ç»ƒè¿‡ç¨‹ã€ä»¥åŠå¹³è¡¡ä»»åŠ¡æ­£ç¡®æ€§ã€é•¿åº¦æ•ˆç‡å’Œæ ¼å¼éµå¾ªçš„å¥–åŠ±å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ ‡è®°é¢„ç®—ï¼Œä¿ƒè¿›æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€æ­¥ä¼˜åŒ–ï¼ŒåŒºåˆ«äºä¼ ç»Ÿå›ºå®šé¢„ç®—æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šè®¾è®¡äº†ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œç»¼åˆè€ƒè™‘ä»»åŠ¡æ­£ç¡®æ€§ã€é•¿åº¦æ•ˆç‡å’Œæ ¼å¼éµå¾ªï¼Œæ­¤å¤–è¿˜è¿›è¡Œäº†å¥–åŠ±æƒé‡å’Œè¡°å‡è°ƒåº¦çš„æ¶ˆèå®éªŒï¼Œä»¥éªŒè¯é€æ­¥çº¦æŸçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¾ç¨‹å­¦ä¹ è®­ç»ƒåœ¨GSM8Kã€MATH500ã€SVAMPç­‰æ•°æ®é›†ä¸Šå‡ä¼˜äºå›ºå®šé¢„ç®—åŸºçº¿ï¼Œå‡†ç¡®æ€§æå‡å¹…åº¦è¾¾åˆ°X%ï¼Œæ ‡è®°æ•ˆç‡æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨å®é™…åº”ç”¨ä¸­é™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæé«˜å“åº”é€Ÿåº¦ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„AIç³»ç»Ÿå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo.

