---
layout: default
title: Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents
---

# Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08645" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08645v1</a>
  <a href="https://arxiv.org/pdf/2508.08645.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08645v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08645v1', 'Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zheng Wu, Heyuan Huang, Yanjia Yang, Yuanyi Song, Xingyu Lou, Weiwen Liu, Weinan Zhang, Jun Wang, Zhuosheng Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MadeAgents/Quick-on-the-Uptake)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIFRAgentæ¡†æ¶ä»¥è§£å†³ä¸ªæ€§åŒ–ç§»åŠ¨ä»£ç†çš„æ„å›¾è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–ä»£ç†` `æ„å›¾è¯†åˆ«` `äººç±»æ¼”ç¤º` `å¤šæ¨¡æ€å­¦ä¹ ` `ç§»åŠ¨ä»»åŠ¡è‡ªåŠ¨åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨äººç±»çš„æ˜¾æ€§æ„å›¾æµï¼Œå¿½è§†äº†éšæ€§æ„å›¾æµï¼Œå¯¼è‡´ä¸ªæ€§åŒ–ç§»åŠ¨ä»£ç†çš„æ„å»ºå›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºIFRAgentæ¡†æ¶ï¼Œé€šè¿‡åˆ†æäººç±»æ¼”ç¤ºä¸­çš„æ˜¾æ€§å’Œéšæ€§æ„å›¾æµï¼Œæ„å»ºæ„å›¾å¯¹é½çš„æ ‡å‡†æ“ä½œç¨‹åºå’Œç”¨æˆ·ä¹ æƒ¯åº“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIFRAgentåœ¨æ„å›¾å¯¹é½ç‡ä¸Šå¹³å‡æå‡6.79%ï¼Œåœ¨æ­¥éª¤å®Œæˆç‡ä¸Šå¹³å‡æå‡5.30%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œç§»åŠ¨ä»»åŠ¡çš„è‡ªåŠ¨åŒ–å˜å¾—æ„ˆåŠ å¯è¡Œã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨äººç±»çš„æ˜¾æ€§æ„å›¾æµï¼Œè€Œå¿½è§†äº†éšæ€§æ„å›¾æµï¼Œå¯¼è‡´ä¸ªæ€§åŒ–ç§»åŠ¨ä»£ç†çš„æ„å»ºé¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†IFRAgentæ¡†æ¶ï¼Œé€šè¿‡åˆ†æäººç±»æ¼”ç¤ºä¸­çš„æ˜¾æ€§å’Œéšæ€§æ„å›¾æµï¼Œæ„å»ºæ ‡å‡†æ“ä½œç¨‹åºåº“å’Œç”¨æˆ·ä¹ æƒ¯åº“ï¼Œä»è€Œæå‡ç§»åŠ¨ä»£ç†ä¸äººç±»æ„å›¾çš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIFRAgentåœ¨æ„å›¾å¯¹é½ç‡ä¸Šå¹³å‡æå‡6.79%ï¼Œåœ¨æ­¥éª¤å®Œæˆç‡ä¸Šå¹³å‡æå‡5.30%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸ªæ€§åŒ–ç§»åŠ¨ä»£ç†åœ¨ç†è§£äººç±»æ„å›¾æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹éšæ€§æ„å›¾æµçš„å¿½è§†ï¼Œä½¿å¾—ä»£ç†éš¾ä»¥æ»¡è¶³ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šIFRAgentæ¡†æ¶é€šè¿‡åˆ†æäººç±»æ¼”ç¤ºä¸­çš„æ˜¾æ€§æ„å›¾æµå’Œéšæ€§æ„å›¾æµï¼Œæ„å»ºæ ‡å‡†æ“ä½œç¨‹åºåº“å’Œç”¨æˆ·ä¹ æƒ¯åº“ï¼Œä»è€Œæå‡ç§»åŠ¨ä»£ç†ä¸äººç±»æ„å›¾çš„å¯¹é½ç¨‹åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIFRAgentçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ„å›¾æµè¯†åˆ«ã€æ ‡å‡†æ“ä½œç¨‹åºæå–ã€æŸ¥è¯¢é‡å†™å’Œä¸ªæ€§åŒ–ç”Ÿæˆç­‰æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†MobileIARæ•°æ®é›†ï¼Œç„¶ååˆ†ææ˜¾æ€§å’Œéšæ€§æ„å›¾æµï¼Œæœ€åç”Ÿæˆä¸ªæ€§åŒ–çš„æŸ¥è¯¢å’Œæ ‡å‡†æ“ä½œç¨‹åºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºåŒæ—¶è€ƒè™‘æ˜¾æ€§å’Œéšæ€§æ„å›¾æµï¼Œæ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„æ„å›¾å¯¹é½è¯„ä¼°æ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†ç§»åŠ¨ä»£ç†çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒIFRAgentä½¿ç”¨äº†æŸ¥è¯¢çº§å‘é‡åº“æ¥å­˜å‚¨æ ‡å‡†æ“ä½œç¨‹åºï¼Œå¹¶ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆå’ŒæŸ¥è¯¢é‡å†™æŠ€æœ¯ï¼Œä»¥ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„ä¸ªæ€§åŒ–æŸ¥è¯¢å’Œæ“ä½œç¨‹åºã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡å°šæœªè¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIFRAgentåœ¨æ„å›¾å¯¹é½ç‡ä¸Šå¹³å‡æå‡6.79%ï¼Œç›¸å¯¹æå‡32.06%ï¼›åœ¨æ­¥éª¤å®Œæˆç‡ä¸Šå¹³å‡æå‡5.30%ï¼Œç›¸å¯¹æå‡26.34%ã€‚è¿™äº›ç»“æœæ˜¾ç¤ºäº†IFRAgentåœ¨ç†è§£å’Œå“åº”ç”¨æˆ·æ„å›¾æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æ‰‹æœºåŠ©æ‰‹ã€æ™ºèƒ½å®¶å±…æ§åˆ¶å’Œä¸ªæ€§åŒ–æœåŠ¡ç­‰ã€‚é€šè¿‡æå‡ç§»åŠ¨ä»£ç†å¯¹ç”¨æˆ·éšæ€§æ„å›¾çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹çš„å¹¿æ³›åº”ç”¨ã€‚æœªæ¥ï¼ŒIFRAgentæ¡†æ¶æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå®ç°ä¸ªæ€§åŒ–æœåŠ¡çš„è‡ªåŠ¨åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As multimodal large language models advance rapidly, the automation of mobile tasks has become increasingly feasible through the use of mobile-use agents that mimic human interactions from graphical user interface. To further enhance mobile-use agents, previous studies employ demonstration learning to improve mobile-use agents from human demonstrations. However, these methods focus solely on the explicit intention flows of humans (e.g., step sequences) while neglecting implicit intention flows (e.g., personal preferences), which makes it difficult to construct personalized mobile-use agents. In this work, to evaluate the \textbf{I}ntention \textbf{A}lignment \textbf{R}ate between mobile-use agents and humans, we first collect \textbf{MobileIAR}, a dataset containing human-intent-aligned actions and ground-truth actions. This enables a comprehensive assessment of the agents' understanding of human intent. Then we propose \textbf{IFRAgent}, a framework built upon \textbf{I}ntention \textbf{F}low \textbf{R}ecognition from human demonstrations. IFRAgent analyzes explicit intention flows from human demonstrations to construct a query-level vector library of standard operating procedures (SOP), and analyzes implicit intention flows to build a user-level habit repository. IFRAgent then leverages a SOP extractor combined with retrieval-augmented generation and a query rewriter to generate personalized query and SOP from a raw ambiguous query, enhancing the alignment between mobile-use agents and human intent. Experimental results demonstrate that IFRAgent outperforms baselines by an average of 6.79\% (32.06\% relative improvement) in human intention alignment rate and improves step completion rates by an average of 5.30\% (26.34\% relative improvement). The codes are available at https://github.com/MadeAgents/Quick-on-the-Uptake.

