---
layout: default
title: Steering Towards Fairness: Mitigating Political Bias in LLMs
---

# Steering Towards Fairness: Mitigating Political Bias in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08846" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08846v3</a>
  <a href="https://arxiv.org/pdf/2508.08846.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08846v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08846v3', 'Steering Towards Fairness: Mitigating Political Bias in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Afrozah Nadeem, Mark Dras, Usman Naseem

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12 (æ›´æ–°: 2025-09-20)

**å¤‡æ³¨**: Accepted at CASE@RANLP2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°æ–¹æ³•ä»¥ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ”¿æ²»åè§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ”¿æ²»åè§` `å»åè§æ–¹æ³•` `æ¿€æ´»æå–` `æ„è¯†å½¢æ€åˆ†æ` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹å…¬å¹³æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ”¿æ²»å’Œç»æµå†…å®¹æ—¶ï¼Œå¸¸å¸¸è¡¨ç°å‡ºæ˜æ˜¾çš„æ„è¯†å½¢æ€åè§ï¼Œå½±å“å…¶åº”ç”¨çš„å…¬å¹³æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ”¿æ²»åæ ‡æµ‹è¯•çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ†æéšè—å±‚æ¿€æ´»æ¥æ¢æµ‹å’Œç¼“è§£è§£ç å™¨LLMsä¸­çš„åè§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè§£ç å™¨LLMsåœ¨ä¸åŒå±‚æ¬¡ä¸Šç³»ç»Ÿæ€§åœ°ç¼–ç äº†åè§ï¼Œæå‡ºçš„ç¼“è§£æ–¹æ³•æœ‰æ•ˆé™ä½äº†è¿™äº›åè§çš„å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ä½¿å…¶åœ¨å„ç§å®é™…åº”ç”¨ä¸­å¾—åˆ°å¹¿æ³›ä½¿ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æ”¿æ²»å’Œç»æµç»´åº¦ä¸Šå¾€å¾€ä¼šç¼–ç å’Œå†ç°æ„è¯†å½¢æ€åè§ã€‚æœ¬æ–‡é€šè¿‡åˆ†æå†…éƒ¨æ¨¡å‹è¡¨ç¤ºï¼Œé‡‡ç”¨äº†ä¸€ç§æ¢æµ‹å’Œç¼“è§£è§£ç å™¨åŸºç¡€LLMsä¸­åè§çš„æ¡†æ¶ã€‚åŸºäºæ”¿æ²»åæ ‡æµ‹è¯•ï¼ˆPCTï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯¹æ¯”å¯¹æå–å’Œæ¯”è¾ƒMistralå’ŒDeepSeekç­‰æ¨¡å‹çš„éšè—å±‚æ¿€æ´»ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å…¨é¢çš„æ¿€æ´»æå–ç®¡é“ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ„è¯†å½¢æ€è½´ä¸Šè¿›è¡Œé€å±‚åˆ†æï¼Œæ­ç¤ºä¸æ”¿æ²»æ¡†æ¶ç›¸å…³çš„æ˜¾è‘—å·®å¼‚ã€‚ç»“æœè¡¨æ˜ï¼Œè§£ç å™¨LLMsåœ¨å„å±‚ä¸­ç³»ç»Ÿæ€§åœ°ç¼–ç äº†è¡¨å¾åè§ï¼Œè¿™å¯ä»¥ç”¨äºæœ‰æ•ˆçš„åŸºäºå¼•å¯¼å‘é‡çš„ç¼“è§£ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£LLMsä¸­æ”¿æ²»åè§çš„ç¼–ç æä¾›äº†æ–°è§è§£ï¼Œå¹¶æå‡ºäº†ä¸€ç§è¶…è¶Šè¡¨é¢è¾“å‡ºå¹²é¢„çš„å»åè§æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„æ”¿æ²»åè§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨æ¨¡å‹è¾“å‡ºçš„è¡¨é¢ï¼Œè€Œå¿½è§†äº†å†…éƒ¨è¡¨ç¤ºçš„åˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹æ¯”å¯¹æå–éšè—å±‚æ¿€æ´»ï¼Œç»“åˆæ”¿æ²»åæ ‡æµ‹è¯•ï¼Œæ·±å…¥åˆ†ææ¨¡å‹å†…éƒ¨çš„åè§è¡¨ç°ï¼Œä»è€Œæå‡ºæœ‰æ•ˆçš„å»åè§ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¿€æ´»æå–ç®¡é“ã€å¯¹æ¯”å¯¹ç”Ÿæˆå’Œå±‚çº§åˆ†ææ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ„è¯†å½¢æ€è½´ä¸Šè¿›è¡Œç³»ç»Ÿæ€§åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥äº†åŸºäºå¼•å¯¼å‘é‡çš„ç¼“è§£æ–¹æ³•ï¼Œèƒ½å¤Ÿé’ˆå¯¹ä¸åŒå±‚æ¬¡çš„åè§è¿›è¡Œæœ‰æ•ˆå¹²é¢„ï¼Œè¿™ä¸€æ–¹æ³•è¶…è¶Šäº†ä¼ ç»Ÿçš„è¾“å‡ºå¹²é¢„ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¿€æ´»æå–è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å±‚çº§åˆ†æå’Œå¯¹æ¯”å¯¹ç”Ÿæˆçš„æŠ€æœ¯ç»†èŠ‚ï¼Œç¡®ä¿äº†å¯¹ä¸åŒæ„è¯†å½¢æ€çš„å…¨é¢è¦†ç›–å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡ç²¾ç»†çš„å‚æ•°è®¾ç½®ï¼Œæå‡äº†æ¨¡å‹çš„å»åè§æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ç¼“è§£æ”¿æ²»åè§æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œè§£ç å™¨LLMsçš„åè§æ°´å¹³é™ä½äº†çº¦30%ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†æ¨¡å‹çš„å…¬å¹³æ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ã€æ–°é—»ç”Ÿæˆå’Œè‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜è¿™äº›ç³»ç»Ÿçš„å…¬å¹³æ€§å’Œå®¢è§‚æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„AIåº”ç”¨ä¸­æ¨å¹¿ï¼Œä¿ƒè¿›æŠ€æœ¯çš„ç¤¾ä¼šè´£ä»»æ„Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in large language models (LLMs) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases along political and economic dimensions. In this paper, we employ a framework for probing and mitigating such biases in decoder-based LLMs through analysis of internal model representations. Grounded in the Political Compass Test (PCT), this method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that decoder LLMs systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in LLMs and offers a principled approach to debiasing beyond surface-level output interventions.

