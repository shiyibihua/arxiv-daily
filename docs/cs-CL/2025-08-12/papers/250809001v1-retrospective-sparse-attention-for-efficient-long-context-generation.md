---
layout: default
title: Retrospective Sparse Attention for Efficient Long-Context Generation
---

# Retrospective Sparse Attention for Efficient Long-Context Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09001" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09001v1</a>
  <a href="https://arxiv.org/pdf/2508.09001.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09001v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09001v1', 'Retrospective Sparse Attention for Efficient Long-Context Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seonghwan Choi, Beomseok Kang, Dongwon Jo, Jae-Joon Kim

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRetroAttentionä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆä¸­çš„KVç¼“å­˜ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆ` `é”®å€¼ç¼“å­˜` `æ³¨æ„åŠ›æœºåˆ¶` `è¯­è¨€æ¨¡å‹` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„KVç¼“å­˜æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—çš„å»¶è¿Ÿå’Œå†…å­˜å ç”¨é—®é¢˜ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†ç´¯ç§¯çš„æ³¨æ„åŠ›è¯¯å·®ã€‚
2. æœ¬æ–‡æå‡ºRetroAttentionï¼Œé€šè¿‡è¿½æº¯æ€§ä¿®æ­£è¿‡å»çš„æ³¨æ„åŠ›è¾“å‡ºï¼Œåˆ©ç”¨æ–°åˆ°è¾¾çš„KVæ¡ç›®æ¥æé«˜ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRetroAttentionåœ¨é•¿ç”ŸæˆåŸºå‡†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ‰æ•ˆKVæš´éœ²æå‡1.6å€ï¼Œå‡†ç¡®ç‡æå‡21.9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œå¤šè½®å¯¹è¯ç­‰é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­è¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨ã€‚ç„¶è€Œï¼Œæ‰©å±•ä¸Šä¸‹æ–‡çš„æ¨ç†å—åˆ°é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜çš„ç“¶é¢ˆé™åˆ¶ï¼Œå…¶å†…å­˜å ç”¨éšç€åºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼Œå¹¶ä¸»å¯¼æ¯ä¸ªè§£ç æ­¥éª¤çš„å»¶è¿Ÿã€‚å°½ç®¡è¿‘æœŸçš„KVç¼“å­˜å‹ç¼©æ–¹æ³•è¯†åˆ«å¹¶åŠ è½½é‡è¦çš„æ ‡è®°ï¼Œä½†ä¸»è¦é›†ä¸­åœ¨è¾“å…¥ä¸Šä¸‹æ–‡ä¸Šï¼Œæœªèƒ½è§£å†³é•¿è§£ç è¿‡ç¨‹ä¸­ç´¯ç§¯çš„æ³¨æ„åŠ›è¯¯å·®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRetroAttentionçš„æ–°å‹KVç¼“å­˜æ›´æ–°æŠ€æœ¯ï¼Œé€šè¿‡ä½¿ç”¨åç»­è§£ç æ­¥éª¤ä¸­æ–°åˆ°è¾¾çš„KVæ¡ç›®ï¼Œè¿½æº¯æ€§åœ°ä¿®æ­£è¿‡å»çš„æ³¨æ„åŠ›è¾“å‡ºã€‚é€šè¿‡ç»´æŠ¤è½»é‡çº§çš„è¾“å‡ºç¼“å­˜ï¼ŒRetroAttentionä½¿å¾—è¿‡å»çš„æŸ¥è¯¢èƒ½å¤Ÿé«˜æ•ˆè®¿é—®æ›´ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶å¼•å…¥çš„å»¶è¿Ÿå¼€é”€æå°ã€‚è¿™æ‰“ç ´äº†å›ºå®šæ³¨æ„åŠ›è¾“å‡ºçš„èŒƒå¼ï¼Œå…è®¸å¯¹å…ˆå‰çš„è¿‘ä¼¼è¿›è¡ŒæŒç»­ä¿®æ­£ã€‚å¤§é‡åœ¨é•¿ç”ŸæˆåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRetroAttentionåœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„KVå‹ç¼©æ–¹æ³•ï¼Œæœ‰æ•ˆKVæš´éœ²æé«˜äº†æœ€å¤š1.6å€ï¼Œå‡†ç¡®ç‡æå‡äº†æœ€å¤š21.9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç”±äºKVç¼“å­˜å¯¼è‡´çš„å»¶è¿Ÿå’Œå†…å­˜å ç”¨é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¾“å…¥ä¸Šä¸‹æ–‡ï¼Œæœªèƒ½æœ‰æ•ˆå¤„ç†é•¿è§£ç è¿‡ç¨‹ä¸­çš„ç´¯ç§¯æ³¨æ„åŠ›è¯¯å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRetroAttentionçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¿½æº¯æ€§ä¿®æ­£è¿‡å»çš„æ³¨æ„åŠ›è¾“å‡ºï¼Œåˆ©ç”¨åç»­è§£ç æ­¥éª¤ä¸­æ–°åˆ°è¾¾çš„KVæ¡ç›®æ¥æé«˜ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§ã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸æ–­ä¿®æ­£å…ˆå‰çš„è¿‘ä¼¼ï¼Œæå‡ç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRetroAttentionçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè½»é‡çº§è¾“å‡ºç¼“å­˜å’ŒKVç¼“å­˜æ›´æ–°æœºåˆ¶ã€‚è¾“å‡ºç¼“å­˜å­˜å‚¨è¿‡å»çš„æ³¨æ„åŠ›è¾“å‡ºï¼Œè€ŒKVç¼“å­˜æ›´æ–°æœºåˆ¶åˆ™åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­å¼•å…¥æ–°çš„KVæ¡ç›®ä»¥ä¿®æ­£è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šRetroAttentionçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶è¿½æº¯æ€§ä¿®æ­£æœºåˆ¶ï¼Œè¿™ä¸ç°æœ‰çš„å›ºå®šæ³¨æ„åŠ›è¾“å‡ºæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚é€šè¿‡åŠ¨æ€æ›´æ–°æ³¨æ„åŠ›è¾“å‡ºï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆä»»åŠ¡çš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒRetroAttentioné‡‡ç”¨äº†è½»é‡çº§çš„è¾“å‡ºç¼“å­˜è®¾è®¡ï¼Œç¡®ä¿åœ¨å¼•å…¥æ–°KVæ¡ç›®æ—¶ï¼Œå»¶è¿Ÿå¼€é”€æœ€å°åŒ–ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ä¼˜åŒ–ï¼Œä»¥æå‡ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRetroAttentionåœ¨é•¿ç”ŸæˆåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„KVå‹ç¼©æ–¹æ³•ï¼Œæœ‰æ•ˆKVæš´éœ²æå‡äº†æœ€å¤š1.6å€ï¼Œå‡†ç¡®ç‡æå‡äº†æœ€å¤š21.9%ã€‚è¿™äº›ç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é•¿æ–‡æœ¬ç”Ÿæˆã€ä»£ç è‡ªåŠ¨ç”Ÿæˆå’Œå¤šè½®å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒRetroAttentionèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–ç¼–ç¨‹å·¥å…·çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value (KV) cache, whose memory footprint grows linearly with sequence length and dominates latency at each decoding step. While recent KV cache compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long decoding. In this paper, we introduce RetroAttention, a novel KV cache update technique that retrospectively revises past attention outputs using newly arrived KV entries from subsequent decoding steps. By maintaining a lightweight output cache, RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) KV compression methods, increasing effective KV exposure by up to 1.6$\times$ and accuracy by up to 21.9\%.

