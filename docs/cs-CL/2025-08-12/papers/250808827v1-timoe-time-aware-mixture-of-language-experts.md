---
layout: default
title: TiMoE: Time-Aware Mixture of Language Experts
---

# TiMoE: Time-Aware Mixture of Language Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08827" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08827v1</a>
  <a href="https://arxiv.org/pdf/2508.08827.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08827v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08827v1', 'TiMoE: Time-Aware Mixture of Language Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Robin Faro, Dongyang Fan, Tamar Alphaidze, Martin Jaggi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/epfml/TiMoE)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTiMoEä»¥è§£å†³è¯­è¨€æ¨¡å‹çš„æ—¶é—´çŸ¥è¯†è¿‡æ—¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´æ„ŸçŸ¥` `è¯­è¨€æ¨¡å‹` `ä¸“å®¶æ··åˆ` `å› æœæ¨ç†` `NLPä»»åŠ¡` `çŸ¥è¯†æ›´æ–°` `é¢„è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›ºå®šå¿«ç…§ä¸Šè®­ç»ƒï¼Œå¯¼è‡´çŸ¥è¯†è¿‡æ—¶å’Œæ—¶é—´æ³„æ¼é—®é¢˜ï¼Œå½±å“é¢„æµ‹å‡†ç¡®æ€§ã€‚
2. æœ¬æ–‡æå‡ºTiMoEï¼Œé€šè¿‡å¯¹ä¸åŒæ—¶é—´æ®µçš„ä¸“å®¶è¿›è¡Œé¢„è®­ç»ƒï¼Œç¡®ä¿æ¨ç†æ—¶çš„å› æœæœ‰æ•ˆæ€§ï¼Œé¿å…æœªæ¥ä¿¡æ¯å¹²æ‰°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTiMoEåœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡å°‘æœªæ¥çŸ¥è¯†é”™è¯¯è¾¾15%ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸åœ¨å›ºå®šçš„ç½‘ç»œå¿«ç…§ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯¼è‡´å…¶çŸ¥è¯†å¯èƒ½è¿‡æ—¶ï¼Œå¹¶ä¸”åœ¨é¢„æµ‹æ—¶å¯èƒ½å‡ºç°æ—¶é—´æ³„æ¼ï¼Œå³ä¾èµ–äºæŸ¥è¯¢æ—¶é—´ç‚¹ä¹‹åçš„ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ä»å¤´å¼€å§‹å¯¹ä¸€ç»„GPTé£æ ¼çš„ä¸“å®¶è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿ç”¨2013-2024å¹´è¯­æ–™åº“çš„ä¸åŒä¸¤å¹´åˆ‡ç‰‡ï¼Œå¹¶é€šè¿‡TiMoEï¼ˆæ—¶é—´æ„ŸçŸ¥è¯­è¨€ä¸“å®¶æ··åˆæ¨¡å‹ï¼‰å°†å®ƒä»¬ç»“åˆèµ·æ¥ã€‚åœ¨æ¨ç†æ—¶ï¼ŒTiMoEä¼šå±è”½æ‰€æœ‰è®­ç»ƒçª—å£åœ¨æŸ¥è¯¢æ—¶é—´æˆ³ä¹‹åç»“æŸçš„ä¸“å®¶ï¼Œå¹¶åˆå¹¶å‰©ä½™çš„å¯¹æ•°æ¦‚ç‡ï¼Œç¡®ä¿ä¸¥æ ¼çš„å› æœæœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¿ç•™å¤šæœŸçŸ¥è¯†çš„å¹¿åº¦ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†TSQAï¼Œä¸€ä¸ªåŒ…å«10,000ä¸ªé—®é¢˜çš„åŸºå‡†ï¼Œå…¶é€‰é¡¹è¢«æ˜ç¡®æ ‡è®°ä¸ºè¿‡å»ã€æœªæ¥æˆ–æ— å…³ï¼Œä»è€Œå…è®¸å¯¹æ—¶é—´å¹»è§‰è¿›è¡Œç»†è‡´çš„æµ‹é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å…±åŒé€‚åº”çš„TiMoEå˜ä½“åœ¨å…«ä¸ªæ ‡å‡†NLPä»»åŠ¡å’ŒTSQAä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ¹é…æˆ–è¶…è¿‡äº†æœ€ä½³å•æœŸä¸“å®¶ï¼Œå¹¶å°†æœªæ¥çŸ¥è¯†é”™è¯¯å‡å°‘äº†å¤šè¾¾15%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›ºå®šå¿«ç…§è®­ç»ƒä¸‹çš„çŸ¥è¯†è¿‡æ—¶å’Œæ—¶é—´æ³„æ¼é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†æ—¶é—´ç›¸å…³æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„TiMoEæ¨¡å‹é€šè¿‡å¯¹ä¸åŒæ—¶é—´æ®µçš„ä¸“å®¶è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨æ¨ç†æ—¶å±è”½ä¸ç›¸å…³çš„ä¸“å®¶ï¼Œä»è€Œç¡®ä¿å› æœæœ‰æ•ˆæ€§ï¼Œé¿å…æœªæ¥ä¿¡æ¯çš„å¹²æ‰°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTiMoEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªGPTé£æ ¼çš„ä¸“å®¶æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åˆ†åˆ«åœ¨ä¸åŒçš„æ—¶é—´æ®µä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨æ¨ç†æ—¶ï¼Œæ¨¡å‹ä¼šæ ¹æ®æŸ¥è¯¢æ—¶é—´æˆ³é€‰æ‹©åˆé€‚çš„ä¸“å®¶è¿›è¡Œé¢„æµ‹ï¼Œåˆå¹¶å…¶è¾“å‡ºçš„å¯¹æ•°æ¦‚ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šTiMoEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ—¶é—´æ„ŸçŸ¥çš„ä¸“å®¶æ··åˆæœºåˆ¶ï¼Œé€šè¿‡æ¨¡å—åŒ–çš„æ—¶é—´åˆ†æ®µé¢„è®­ç»ƒå’Œå› æœè·¯ç”±ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ—¶é—´ç›¸å…³æ€§å¤„ç†èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œé¿å…äº†æœªæ¥çŸ¥è¯†çš„å¹²æ‰°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åˆ†æ®µè®­ç»ƒçš„ç­–ç•¥ï¼Œç¡®ä¿æ¯ä¸ªä¸“å®¶åªæ¥è§¦åˆ°å…¶å¯¹åº”æ—¶é—´æ®µçš„æ•°æ®ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†æ—¶é—´å› ç´ ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨ä¸åŒæ—¶é—´æ®µçš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTiMoEåœ¨å…«ä¸ªæ ‡å‡†NLPä»»åŠ¡åŠTSQAåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡å°‘æœªæ¥çŸ¥è¯†é”™è¯¯æ–¹é¢ï¼Œæå‡å¹…åº¦è¾¾åˆ°15%ã€‚è¯¥æ¨¡å‹çš„å…±åŒé€‚åº”å˜ä½“åœ¨æ€§èƒ½ä¸ŠåŒ¹é…æˆ–è¶…è¿‡äº†æœ€ä½³å•æœŸä¸“å®¶ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–°é—»ç”Ÿæˆã€ç¤¾äº¤åª’ä½“åˆ†æå’Œæ—¶é—´æ•æ„Ÿçš„é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜è¯­è¨€æ¨¡å‹å¯¹æ—¶é—´ä¿¡æ¯çš„å¤„ç†èƒ½åŠ›ï¼ŒTiMoEèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­æä¾›æ›´å‡†ç¡®çš„é¢„æµ‹å’Œå“åº”ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are typically trained on fixed snapshots of the web, which means that their knowledge becomes stale and their predictions risk temporal leakage: relying on information that lies in the future relative to a query. We tackle this problem by pre-training from scratch a set of GPT-style experts on disjoint two-year slices of a 2013-2024 corpus and combining them through TiMoE, a Time-aware Mixture of Language Experts. At inference time, TiMoE masks all experts whose training window ends after the query timestamp and merges the remaining log-probabilities in a shared space, guaranteeing strict causal validity while retaining the breadth of multi-period knowledge. We also release TSQA, a 10k-question benchmark whose alternatives are explicitly labelled as past, future or irrelevant, allowing fine-grained measurement of temporal hallucinations. Experiments on eight standard NLP tasks plus TSQA show that a co-adapted TiMoE variant matches or exceeds the best single-period expert and cuts future-knowledge errors by up to 15%. Our results demonstrate that modular, time-segmented pre-training paired with causal routing is a simple yet effective path toward LLMs that stay chronologically grounded without sacrificing general performance much. We open source our code at TiMoE (Github): https://github.com/epfml/TiMoE

