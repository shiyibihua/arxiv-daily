---
layout: default
title: Reasoning-Intensive Regression
---

# Reasoning-Intensive Regression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21762" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21762v2</a>
  <a href="https://arxiv.org/pdf/2508.21762.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21762v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21762v2', 'Reasoning-Intensive Regression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Diane Tchuindjo, Omar Khattab

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29 (æ›´æ–°: 2025-11-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMENTATä»¥è§£å†³æ¨ç†å¯†é›†å‹å›å½’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†å¯†é›†å‹å›å½’` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ‰¹åå°„æç¤ºä¼˜åŒ–` `ç¥ç»é›†æˆå­¦ä¹ ` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ–¹æ³•åœ¨æ¨ç†å¯†é›†å‹å›å½’ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡åˆ†æå’Œæ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºMENTATæ–¹æ³•ï¼Œé€šè¿‡æ‰¹åå°„æç¤ºä¼˜åŒ–ä¸ç¥ç»é›†æˆå­¦ä¹ ç›¸ç»“åˆï¼Œæ—¨åœ¨æé«˜RiRä»»åŠ¡çš„æ€§èƒ½ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šMENTATåœ¨åŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å®ç°äº†é«˜è¾¾65%çš„æ€§èƒ½æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€AIç ”ç©¶è€…å’Œä»ä¸šè€…è¶Šæ¥è¶Šå¤šåœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºæ¨ç†å¯†é›†å‹å›å½’ï¼ˆRiRï¼‰ä»»åŠ¡ï¼Œå³ä»æ–‡æœ¬ä¸­æ¨å¯¼å‡ºå¾®å¦™çš„æ•°å€¼è¯„åˆ†ï¼Œæœ¬æ–‡æ¢è®¨äº†è¿™ä¸€é¢†åŸŸçš„æŒ‘æˆ˜ã€‚ä¸æ ‡å‡†è¯­è¨€å›å½’ä»»åŠ¡ä¸åŒï¼ŒRiRå¸¸å¸¸å‡ºç°åœ¨éœ€è¦æ·±å…¥åˆ†æä¸Šä¸‹æ–‡çš„ç‰¹å®šé—®é¢˜ä¸­ï¼Œå¦‚åŸºäºè¯„åˆ†æ ‡å‡†çš„è¯„åˆ†ã€å¤æ‚ç¯å¢ƒä¸­çš„ç¨ å¯†å¥–åŠ±å»ºæ¨¡æˆ–é¢†åŸŸç‰¹å®šæ£€ç´¢ã€‚æœ¬æ–‡å°†å››ä¸ªç°å®é—®é¢˜è§†ä¸ºRiRä»»åŠ¡ï¼Œå»ºç«‹åˆæ­¥åŸºå‡†ï¼Œå¹¶æµ‹è¯•äº†åœ¨æ­¤ä»»åŠ¡ä¸­ï¼Œå†»ç»“LLMsçš„æç¤ºå’Œé€šè¿‡æ¢¯åº¦ä¸‹é™å¾®è°ƒTransformerç¼–ç å™¨çš„æ•ˆæœã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§ç®€å•ä¸”è½»é‡çš„æ–¹æ³•MENTATï¼Œç»“åˆäº†æ‰¹åå°„æç¤ºä¼˜åŒ–å’Œç¥ç»é›†æˆå­¦ä¹ ï¼Œç»“æœæ˜¾ç¤ºMENTATåœ¨åŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡äº†65%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨ç†å¯†é›†å‹å›å½’ï¼ˆRiRï¼‰ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡å’Œæœ‰é™ä»»åŠ¡ç‰¹å®šè®­ç»ƒæ•°æ®æ—¶è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æœ‰æ•ˆæ¨å¯¼å‡ºæ•°å€¼è¯„åˆ†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„MENTATæ–¹æ³•é€šè¿‡ç»“åˆæ‰¹åå°„æç¤ºä¼˜åŒ–ä¸ç¥ç»é›†æˆå­¦ä¹ ï¼Œæ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•åœ¨RiRä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œæä¾›æ›´å‡†ç¡®çš„è¯„åˆ†ç»“æœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMENTATçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯æ‰¹åå°„æç¤ºä¼˜åŒ–ï¼Œé€šè¿‡ä¼˜åŒ–æç¤ºæ¥å¢å¼ºæ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£ï¼›å…¶æ¬¡æ˜¯ç¥ç»é›†æˆå­¦ä¹ ï¼Œé€šè¿‡é›†æˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœæ¥æé«˜æœ€ç»ˆè¯„åˆ†çš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šMENTATçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ‰¹åå°„æç¤ºä¼˜åŒ–ä¸ç¥ç»é›†æˆå­¦ä¹ ç›¸ç»“åˆï¼Œè¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„æ¨ç†ä»»åŠ¡æ—¶èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒMENTATé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æç¤ºçš„ç”Ÿæˆï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šä½¿ç”¨äº†å¤šç§Transformerç¼–ç å™¨çš„é›†æˆï¼Œç¡®ä¿äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒMENTATåœ¨RiRä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒMENTATç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å®ç°äº†é«˜è¾¾65%çš„æ€§èƒ½æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ¨ç†å¯†é›†å‹å›å½’ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒMENTATåœ¨å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡æ—¶å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²è¯„åˆ†ç³»ç»Ÿã€å¤æ‚ç¯å¢ƒä¸­çš„å¥–åŠ±å»ºæ¨¡ä»¥åŠé¢†åŸŸç‰¹å®šçš„ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡æ¨ç†å¯†é›†å‹å›å½’ä»»åŠ¡çš„æ€§èƒ½ï¼ŒMENTATå¯ä»¥ä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´å‡†ç¡®çš„è¯„åˆ†å’Œåé¦ˆï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³åº”ç”¨çš„å‘å±•ä¸ä¼˜åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> AI researchers and practitioners increasingly apply large language models (LLMs) to what we call reasoning-intensive regression (RiR), i.e., deducing subtle numerical scores from text. Unlike standard language regression tasks, e.g., for sentiment or similarity, RiR often appears instead in ad-hoc problems such as rubric-based scoring, modeling dense rewards in complex environments, or domain-specific retrieval, where much deeper analysis of context is required while only limited task-specific training data and computation are available. We cast four realistic problems as RiR tasks to establish an initial benchmark, and use that to test our hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR. We then propose MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning. MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR.

