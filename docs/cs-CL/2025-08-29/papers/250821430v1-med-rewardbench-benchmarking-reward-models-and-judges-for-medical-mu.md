---
layout: default
title: Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models
---

# Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21430" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21430v1</a>
  <a href="https://arxiv.org/pdf/2508.21430.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21430v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21430v1', 'Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Meidan Ding, Jipeng Zhang, Wenxuan Wang, Cheng-Yi Li, Wei-Chieh Fang, Hsin-Yu Wu, Haiqin Zhong, Wenting Chen, Linlin Shen

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

**å¤‡æ³¨**: 19 pages, 5 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMed-RewardBenchä»¥è§£å†³åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»ç–—AI` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¥–åŠ±æ¨¡å‹` `åŸºå‡†è¯„ä¼°` `ä¸´åºŠå†³ç­–` `ä¸“å®¶æ³¨é‡Š` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒ»ç–—å¥–åŠ±æ¨¡å‹å’Œè¯„ä¼°è€…ç ”ç©¶ä¸è¶³ï¼Œç¼ºä¹é’ˆå¯¹ä¸´åºŠéœ€æ±‚çš„ä¸“é—¨åŸºå‡†ï¼Œå¯¼è‡´è¯„ä¼°ç»´åº¦çš„ç¼ºå¤±ã€‚
2. è®ºæ–‡æå‡ºMed-RewardBenchåŸºå‡†ï¼Œä¸“æ³¨äºåŒ»ç–—åœºæ™¯ä¸­MRMså’Œè¯„ä¼°è€…çš„è¯„ä¼°ï¼ŒåŒ…å«å¤šæ¨¡æ€æ•°æ®é›†å’Œä¸¥æ ¼çš„è¯„ä¼°æµç¨‹ã€‚
3. è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œ32ä¸ªæœ€å…ˆè¿›çš„MLLMsåœ¨ä¸ä¸“å®¶åˆ¤æ–­å¯¹é½æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒæ—¶åŸºçº¿æ¨¡å‹é€šè¿‡å¾®è°ƒå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—åº”ç”¨ä¸­å…·æœ‰é‡è¦æ½œåŠ›ï¼ŒåŒ…æ‹¬ç–¾ç—…è¯Šæ–­å’Œä¸´åºŠå†³ç­–ã€‚ç„¶è€Œï¼Œè¿™äº›ä»»åŠ¡éœ€è¦é«˜åº¦å‡†ç¡®ã€ä¸Šä¸‹æ–‡æ•æ„Ÿä¸”ä¸“ä¸šå¯¹é½çš„å“åº”ï¼Œå› æ­¤å¯é çš„å¥–åŠ±æ¨¡å‹å’Œè¯„ä¼°è€…è‡³å…³é‡è¦ã€‚å°½ç®¡å…¶é‡è¦æ€§ï¼ŒåŒ»ç–—å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰å’Œè¯„ä¼°è€…ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ï¼Œç¼ºä¹ä¸“é—¨é’ˆå¯¹ä¸´åºŠéœ€æ±‚çš„åŸºå‡†ã€‚ç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨ä¸€èˆ¬MLLMèƒ½åŠ›æˆ–å°†æ¨¡å‹è§†ä¸ºæ±‚è§£å™¨ï¼Œå¿½è§†äº†è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ç­‰é‡è¦è¯„ä¼°ç»´åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Med-RewardBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°åŒ»ç–—åœºæ™¯ä¸­MRMså’Œè¯„ä¼°è€…çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«è·¨è¶Š13ä¸ªå™¨å®˜ç³»ç»Ÿå’Œ8ä¸ªä¸´åºŠç§‘å®¤çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå…±æœ‰1,026ä¸ªä¸“å®¶æ³¨é‡Šæ¡ˆä¾‹ã€‚é€šè¿‡ä¸¥æ ¼çš„ä¸‰æ­¥æµç¨‹ï¼Œç¡®ä¿åœ¨å…­ä¸ªä¸´åºŠå…³é”®ç»´åº¦ä¸Šæä¾›é«˜è´¨é‡çš„è¯„ä¼°æ•°æ®ã€‚æˆ‘ä»¬è¯„ä¼°äº†32ä¸ªæœ€å…ˆè¿›çš„MLLMsï¼ŒåŒ…æ‹¬å¼€æºã€ä¸“æœ‰å’ŒåŒ»ç–—ç‰¹å®šæ¨¡å‹ï¼Œæ­ç¤ºäº†è¾“å‡ºä¸ä¸“å®¶åˆ¤æ–­å¯¹é½çš„é‡å¤§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘çš„åŸºçº¿æ¨¡å‹é€šè¿‡å¾®è°ƒæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„è¯„ä¼°é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ç­‰å…³é”®ç»´åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMed-RewardBenchåŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°åŒ»ç–—åœºæ™¯ä¸­çš„å¥–åŠ±æ¨¡å‹å’Œè¯„ä¼°è€…ï¼Œé€šè¿‡æ„å»ºå¤šæ¨¡æ€æ•°æ®é›†å’Œä¸¥æ ¼çš„è¯„ä¼°æµç¨‹æ¥æé«˜è¯„ä¼°çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€ä¸“å®¶æ³¨é‡Šã€è¯„ä¼°æµç¨‹ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æ¶µç›–13ä¸ªå™¨å®˜ç³»ç»Ÿå’Œ8ä¸ªä¸´åºŠç§‘å®¤ï¼Œç¡®ä¿å¤šæ ·æ€§å’Œä»£è¡¨æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šMed-RewardBenchæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹åŒ»ç–—åœºæ™¯çš„è¯„ä¼°åŸºå‡†ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ï¼Œæä¾›äº†ç³»ç»ŸåŒ–çš„è¯„ä¼°æ ‡å‡†å’Œæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œé‡‡ç”¨äº†ä¸¥æ ¼çš„ä¸‰æ­¥æµç¨‹ï¼Œç¡®ä¿æ•°æ®è´¨é‡ï¼›è¯„ä¼°ç»´åº¦åŒ…æ‹¬è¯Šæ–­å‡†ç¡®æ€§ã€ä¸´åºŠç›¸å…³æ€§ç­‰ï¼Œä½¿ç”¨ä¸“å®¶æ³¨é‡Šæ¥éªŒè¯æ¨¡å‹è¾“å‡ºçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ32ä¸ªæœ€å…ˆè¿›çš„MLLMsåœ¨ä¸ä¸“å®¶åˆ¤æ–­å¯¹é½æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§æ–¹é¢ã€‚åŒæ—¶ï¼ŒåŸºçº¿æ¨¡å‹é€šè¿‡å¾®è°ƒå®ç°äº†æ€§èƒ½çš„æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†è¯¥åŸºå‡†çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—è¯Šæ–­æ”¯æŒç³»ç»Ÿã€ä¸´åºŠå†³ç­–è¾…åŠ©å·¥å…·ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œå†³ç­–æ•ˆç‡ã€‚æœªæ¥ï¼ŒMed-RewardBenchçš„å»ºç«‹å°†æ¨åŠ¨åŒ»ç–—AIé¢†åŸŸçš„æ ‡å‡†åŒ–è¯„ä¼°ï¼Œä¿ƒè¿›ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) hold significant potential in medical applications, including disease diagnosis and clinical decision-making. However, these tasks require highly accurate, context-sensitive, and professionally aligned responses, making reliable reward models and judges critical. Despite their importance, medical reward models (MRMs) and judges remain underexplored, with no dedicated benchmarks addressing clinical requirements. Existing benchmarks focus on general MLLM capabilities or evaluate models as solvers, neglecting essential evaluation dimensions like diagnostic accuracy and clinical relevance. To address this, we introduce Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and judges in medical scenarios. Med-RewardBench features a multimodal dataset spanning 13 organ systems and 8 clinical departments, with 1,026 expert-annotated cases. A rigorous three-step process ensures high-quality evaluation data across six clinically critical dimensions. We evaluate 32 state-of-the-art MLLMs, including open-source, proprietary, and medical-specific models, revealing substantial challenges in aligning outputs with expert judgment. Additionally, we develop baseline models that demonstrate substantial performance improvements through fine-tuning.

