---
layout: default
title: Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval
---

# Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00276" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00276v1</a>
  <a href="https://arxiv.org/pdf/2509.00276.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00276v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00276v1', 'Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxiang Liu, Tian Wang, Gourab Kundu, Tianyu Cao, Guang Cheng, Zhen Ge, Jianshu Chen, Qingjun Cui, Trishul Chilimbi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

**å¤‡æ³¨**: CIKM 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRITEä»¥è§£å†³å¤æ‚æŸ¥è¯¢çš„æ–‡æ¡£æ£€ç´¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åµŒå…¥` `æ¨ç†èƒ½åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯æ£€ç´¢` `é›¶-shotå­¦ä¹ ` `é€»è¾‘æ¨ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡æœ¬åµŒå…¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶ï¼Œå¾€å¾€ä¾èµ–è¡¨é¢è¯æ±‡åŒ¹é…ï¼Œå¯¼è‡´æ£€ç´¢æ•ˆæœä¸ä½³ã€‚
2. RITEæ–¹æ³•é€šè¿‡ç”Ÿæˆä¸­é—´æ¨ç†æ–‡æœ¬ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¢å¼ºæ–‡æœ¬åµŒå…¥çš„æ¨ç†æ·±åº¦ã€‚
3. åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRITEæ˜¾è‘—æé«˜äº†é›¶-shotæ£€ç´¢çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šé¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºTransformerçš„æ¨¡å‹å¦‚BERTå’ŒE5åœ¨æ–‡æœ¬åµŒå…¥æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†é¢å¯¹å¤æ‚çš„çœŸå®ä¸–ç•ŒæŸ¥è¯¢æ—¶ï¼Œä¼ ç»Ÿçš„ç¼–ç å™¨æ£€ç´¢å™¨å¾€å¾€æ— æ³•æ»¡è¶³éœ€æ±‚ã€‚æœ¬æ–‡æå‡ºäº†Reasoning-Infused Text Embedding (RITE)ï¼Œé€šè¿‡ç”Ÿæˆä¸­é—´æ¨ç†æ–‡æœ¬æ¥å¢å¼ºæ–‡æœ¬åµŒå…¥è¿‡ç¨‹ï¼Œå……åˆ†åˆ©ç”¨è§£ç å™¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRITEåœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æå‡äº†é›¶-shotæ£€ç´¢æ€§èƒ½ï¼Œè¯æ˜äº†å°†æ¨ç†èå…¥åµŒå…¥è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤æ‚æŸ¥è¯¢ä¸‹æ–‡æ¡£æ£€ç´¢çš„ä¸è¶³ï¼Œç°æœ‰çš„ç¼–ç å™¨æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™ï¼Œæ— æ³•æ»¡è¶³æ·±å±‚æ¬¡çš„è¯­ä¹‰ç†è§£éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRITEé€šè¿‡åœ¨æ–‡æœ¬åµŒå…¥è¿‡ç¨‹ä¸­å¼•å…¥é€»è¾‘æ¨ç†ï¼Œç”Ÿæˆä¸­é—´æ¨ç†æ–‡æœ¬ï¼Œä»è€Œä¸°å¯ŒåµŒå…¥è¡¨ç¤ºï¼Œæå‡æ£€ç´¢æ•ˆæœã€‚è¯¥è®¾è®¡æ—¨åœ¨å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¼˜åŠ¿ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRITEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç”Ÿæˆä¸­é—´æ¨ç†æ–‡æœ¬çš„æ¨¡å—å’Œè®¡ç®—åµŒå…¥çš„æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è§£ç å™¨ç”Ÿæˆæ¨ç†æ–‡æœ¬ï¼Œç„¶åå°†å…¶ç”¨äºå¢å¼ºæœ€ç»ˆçš„æ–‡æœ¬åµŒå…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šRITEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†æ¨ç†è¿‡ç¨‹èå…¥æ–‡æœ¬åµŒå…¥çš„ç”Ÿæˆä¸­ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•ä»…å…³æ³¨ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå……åˆ†æŒ–æ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸Šï¼ŒRITEé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ¨ç†æ–‡æœ¬èƒ½å¤Ÿæœ‰æ•ˆæå‡åµŒå…¥è´¨é‡ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRITEæ–¹æ³•åœ¨é›¶-shotæ£€ç´¢ä»»åŠ¡ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†çº¦15%çš„æ£€ç´¢å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤æ‚æŸ¥è¯¢å¤„ç†ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¿¡æ¯æ£€ç´¢ã€é—®ç­”ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶çš„å“åº”èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼ŒRITEæ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´å¤šåŸºäºæ¨ç†çš„æ£€ç´¢æŠ€æœ¯çš„å‘å±•ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformer-based models such as BERT and E5 have significantly advanced text embedding by capturing rich contextual representations. However, many complex real-world queries require sophisticated reasoning to retrieve relevant documents beyond surface-level lexical matching, where encoder-only retrievers often fall short. Decoder-only large language models (LLMs), known for their strong reasoning capabilities, offer a promising alternative. Despite this potential, existing LLM-based embedding methods primarily focus on contextual representation and do not fully exploit the reasoning strength of LLMs. To bridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple but effective approach that integrates logical reasoning into the text embedding process using generative LLMs. RITE builds upon existing language model embedding techniques by generating intermediate reasoning texts in the token space before computing embeddings, thereby enriching representations with inferential depth. Experimental results on BRIGHT, a reasoning-intensive retrieval benchmark, demonstrate that RITE significantly enhances zero-shot retrieval performance across diverse domains, underscoring the effectiveness of incorporating reasoning into the embedding process.

