---
layout: default
title: Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing
---

# Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04469" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04469v1</a>
  <a href="https://arxiv.org/pdf/2509.04469.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04469v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04469v1', 'Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: David Berghaus, Armin Berger, Lars Hillebrand, Kostadin Cvejoski, Rafet Sifa

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åŸºäºå¤šæ¨¡æ€è§†è§‰çš„å‘ç¥¨å¤„ç†ç­–ç•¥æ¯”è¾ƒç ”ç©¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤„ç†` `å‘ç¥¨è§£æ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å›¾åƒå¤„ç†` `ç»“æ„åŒ–è§£æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‘ç¥¨å¤„ç†æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®æ—¶é¢ä¸´æ€§èƒ½ä¸å‡å’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€èƒ½åŠ›çš„ç›´æ¥å›¾åƒå¤„ç†æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å‘ç¥¨è§£æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸç”Ÿå›¾åƒå¤„ç†åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¼˜äºç»“æ„åŒ–è§£æï¼Œä¸”ä¸åŒæ¨¡å‹çš„è¡¨ç°å·®å¼‚æ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¯¹ä¸‰ç§å®¶æ—çš„å…«ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT-5ã€Gemini 2.5 å’Œå¼€æºçš„ Gemma 3ï¼‰åœ¨ä¸‰ç§ä¸åŒçš„å…¬å¼€å‘ç¥¨æ–‡æ¡£æ•°æ®é›†ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œé‡‡ç”¨é›¶-shot æç¤ºã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§å¤„ç†ç­–ç•¥ï¼šç›´æ¥ä½¿ç”¨å¤šæ¨¡æ€èƒ½åŠ›è¿›è¡Œå›¾åƒå¤„ç†å’Œå…ˆå°†æ–‡æ¡£è½¬æ¢ä¸º Markdown çš„ç»“æ„åŒ–è§£ææ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒåŸç”Ÿå›¾åƒå¤„ç†é€šå¸¸ä¼˜äºç»“æ„åŒ–æ–¹æ³•ï¼Œä¸”æ€§èƒ½å› æ¨¡å‹ç±»å‹å’Œæ–‡æ¡£ç‰¹å¾è€Œå¼‚ã€‚æœ¬åŸºå‡†ä¸ºé€‰æ‹©é€‚å½“çš„æ¨¡å‹å’Œå¤„ç†ç­–ç•¥æä¾›äº†è§è§£ï¼Œä»£ç å·²åœ¨çº¿å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å‘ç¥¨å¤„ç†æ–¹æ³•åœ¨å¤šæ¨¡æ€æ•°æ®è§£æä¸­çš„æ€§èƒ½ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒå¤„ç†ä¸ç»“æ„åŒ–è§£æä¹‹é—´çš„é€‰æ‹©å›°å¢ƒã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå°†æ–‡æ¡£è½¬æ¢ä¸ºç»“æ„åŒ–æ ¼å¼ï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±å’Œå¤„ç†æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å›¾åƒå¤„ç†èƒ½åŠ›ï¼Œç›´æ¥è§£æå‘ç¥¨å›¾åƒï¼Œè€Œä¸æ˜¯å…ˆè¿›è¡Œæ ¼å¼è½¬æ¢ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨ä¿ç•™æ›´å¤šçš„åŸå§‹ä¿¡æ¯ï¼Œæé«˜è§£æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†é€‰æ‹©ã€æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€ä»¥åŠæ€§èƒ½æ¯”è¾ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œé€‰æ‹©ä¸‰ç§ä¸åŒçš„å…¬å¼€å‘ç¥¨æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨ä¸åŒçš„å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œè§£æï¼›æœ€åï¼Œæ¯”è¾ƒç›´æ¥å›¾åƒå¤„ç†ä¸ç»“æ„åŒ–è§£æçš„æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºç›´æ¥ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå›¾åƒè§£æï¼Œè€Œä¸æ˜¯ä¾èµ–ä¼ ç»Ÿçš„ç»“æ„åŒ–æ–¹æ³•ã€‚è¿™ä¸€æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æ–‡æ¡£æ—¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹é€‰æ‹©ä¸Šï¼Œæœ¬æ–‡ä½¿ç”¨äº†GPT-5ã€Gemini 2.5å’ŒGemma 3ç­‰å¤šç§æ¨¡å‹ï¼Œå¹¶é€šè¿‡é›¶-shotæç¤ºè¿›è¡Œæµ‹è¯•ã€‚å…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬å›¾åƒè¾“å…¥çš„é¢„å¤„ç†æ–¹å¼å’Œæ¨¡å‹çš„è¶…å‚æ•°è°ƒæ•´ï¼Œä»¥ä¼˜åŒ–è§£ææ•ˆæœã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨ä»£ç ä¸­æä¾›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸç”Ÿå›¾åƒå¤„ç†æ–¹æ³•åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¼˜äºç»“æ„åŒ–è§£æï¼Œå°¤å…¶åœ¨å¤æ‚æ–‡æ¡£ä¸Šè¡¨ç°æ›´ä¸ºçªå‡ºã€‚ä¸åŒæ¨¡å‹çš„æ€§èƒ½å·®å¼‚æ˜æ˜¾ï¼Œæä¾›äº†é€‰æ‹©åˆé€‚æ¨¡å‹çš„ä¾æ®ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œéœ€å‚è€ƒåœ¨çº¿ä»£ç è·å–ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–æ–‡æ¡£å¤„ç†ã€è´¢åŠ¡å®¡è®¡å’Œä¼ä¸šèµ„æºç®¡ç†ç­‰ã€‚é€šè¿‡æé«˜å‘ç¥¨è§£æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½äººå·¥æˆæœ¬ï¼Œæå‡ä¼ä¸šè¿è¥æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ–‡æ¡£è§£æä»»åŠ¡ä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper benchmarks eight multi-modal large language models from three families (GPT-5, Gemini 2.5, and open-source Gemma 3) on three diverse openly available invoice document datasets using zero-shot prompting. We compare two processing strategies: direct image processing using multi-modal capabilities and a structured parsing approach converting documents to markdown first. Results show native image processing generally outperforms structured approaches, with performance varying across model types and document characteristics. This benchmark provides insights for selecting appropriate models and processing strategies for automated document systems. Our code is available online.

