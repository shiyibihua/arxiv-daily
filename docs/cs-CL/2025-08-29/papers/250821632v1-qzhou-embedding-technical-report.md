---
layout: default
title: QZhou-Embedding Technical Report
---

# QZhou-Embedding Technical Report

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21632" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21632v1</a>
  <a href="https://arxiv.org/pdf/2508.21632.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21632v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21632v1', 'QZhou-Embedding Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQZhou-Embeddingä»¥æå‡æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åµŒå…¥` `å¤šä»»åŠ¡å­¦ä¹ ` `æ•°æ®åˆæˆ` `ä¿¡æ¯æ£€ç´¢` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨å¤šæ ·æ€§å’Œè¯­ä¹‰ä¸°å¯Œæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚
2. QZhou-Embeddingé€šè¿‡ç»Ÿä¸€çš„å¤šä»»åŠ¡æ¡†æ¶å’Œæ•°æ®åˆæˆæŠ€æœ¯ï¼Œæå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ–‡æœ¬è¡¨ç¤ºèƒ½åŠ›ã€‚
3. åœ¨MTEBå’ŒCMTEBåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹å–å¾—äº†é¢†å…ˆçš„æˆç»©ï¼ŒéªŒè¯äº†é«˜è´¨é‡æ•°æ®å¯¹æ£€ç´¢æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†QZhou-Embeddingï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„ä¸Šä¸‹æ–‡æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œå…·å¤‡å“è¶Šçš„æ–‡æœ¬è¡¨ç¤ºèƒ½åŠ›ã€‚è¯¥æ¨¡å‹åŸºäºQwen2.5-7B-InstructåŸºç¡€æ¨¡å‹ï¼Œè®¾è®¡äº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡æ¡†æ¶ï¼ŒåŒ…å«ä¸“é—¨çš„æ•°æ®è½¬æ¢å’Œè®­ç»ƒç­–ç•¥ã€‚æ•°æ®è½¬æ¢æ–¹æ¡ˆå…è®¸æ›´å¤šæ ·åŒ–çš„æ–‡æœ¬è®­ç»ƒæ•°æ®é›†çš„èå…¥ï¼Œè€Œä»»åŠ¡ç‰¹å®šçš„è®­ç»ƒç­–ç•¥åˆ™æé«˜äº†æ¨¡å‹å­¦ä¹ æ•ˆç‡ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€æ¡æ•°æ®åˆæˆç®¡é“ï¼Œåˆ©ç”¨LLM APIï¼Œç»“åˆäº†é‡Šä¹‰ã€å¢å¼ºå’Œå›°éš¾è´Ÿä¾‹ç”Ÿæˆç­‰æŠ€æœ¯ï¼Œä»¥æå‡è®­ç»ƒé›†çš„è¯­ä¹‰ä¸°å¯Œæ€§å’Œæ ·æœ¬éš¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆè¿›è¡Œä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„é¢„è®­ç»ƒï¼Œç„¶åè¿›è¡Œå…¨ä»»åŠ¡å¾®è°ƒï¼Œä½¿åµŒå…¥æ¨¡å‹èƒ½å¤ŸåŸºäºå¼ºå¤§çš„æ£€ç´¢æ€§èƒ½æ‰©å±•å…¶èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨MTEBå’ŒCMTEBåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨å¤„ç†å¤šæ ·åŒ–æ–‡æœ¬æ•°æ®æ—¶ï¼Œå¾€å¾€é¢ä¸´è¯­ä¹‰è¡¨è¾¾ä¸è¶³å’Œè®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šQZhou-Embeddingé€šè¿‡å¼•å…¥ç»Ÿä¸€çš„å¤šä»»åŠ¡æ¡†æ¶å’Œæ•°æ®åˆæˆæŠ€æœ¯ï¼Œæ—¨åœ¨æå‡æ¨¡å‹çš„æ–‡æœ¬è¡¨ç¤ºèƒ½åŠ›å’Œå­¦ä¹ æ•ˆç‡ã€‚é€šè¿‡å¤šæ ·åŒ–çš„æ•°æ®è½¬æ¢å’Œä»»åŠ¡ç‰¹å®šçš„è®­ç»ƒç­–ç•¥ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„æ–‡æœ¬ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®åˆæˆç®¡é“ã€ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒæ¨¡å—ã€‚æ•°æ®åˆæˆç®¡é“åˆ©ç”¨LLM APIè¿›è¡Œé‡Šä¹‰ã€å¢å¼ºå’Œç”Ÿæˆå›°éš¾è´Ÿä¾‹ï¼Œä»¥ä¸°å¯Œè®­ç»ƒæ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ•°æ®åˆæˆç®¡é“çš„è®¾è®¡å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥çš„å®æ–½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒQZhou-Embeddingæ›´æœ‰æ•ˆåœ°åˆ©ç”¨äº†é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ•°æ®ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œæ•°æ®åˆæˆè¿‡ç¨‹ä¸­å¯¹æ ·æœ¬éš¾åº¦çš„æ§åˆ¶ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„è®¾è®¡è€ƒé‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨MTEBå’ŒCMTEBåŸºå‡†æµ‹è¯•ä¸­ï¼ŒQZhou-Embeddingæ¨¡å‹åˆ†åˆ«å–å¾—äº†ç¬¬ä¸€åçš„æˆç»©ï¼Œå±•ç¤ºäº†å…¶åœ¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹åœ¨é‡æ’åºå’Œèšç±»ç­‰ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†é«˜è´¨é‡æ•°æ®å¯¹æ¨¡å‹æ€§èƒ½çš„ç§¯æå½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

QZhou-Embeddingæ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢ã€æ–‡æœ¬åˆ†ç±»ã€èšç±»å’Œé‡æ’åºç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„æ–‡æœ¬è¡¨ç¤ºèƒ½åŠ›èƒ½å¤Ÿå¸®åŠ©æå‡æœç´¢å¼•æ“çš„å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒï¼ŒåŒæ—¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†çš„å…¶ä»–ä»»åŠ¡ä¸­ä¹Ÿèƒ½å‘æŒ¥é‡è¦ä½œç”¨ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹çš„æŠ€æœ¯è¿›æ­¥å¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šæ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present QZhou-Embedding, a general-purpose contextual text embedding model with exceptional text representation capabilities. Built upon the Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task framework comprising specialized data transformation and training strategies. The data transformation scheme enables the incorporation of more diverse textual training datasets, while the task-specific training strategies enhance model learning efficiency. We developed a data synthesis pipeline leveraging LLM API, incorporating techniques such as paraphrasing, augmentation, and hard negative example generation to improve the semantic richness and sample difficulty of the training set. Additionally, we employ a two-stage training strategy, comprising initial retrieval-focused pretraining followed by full-task fine-tuning, enabling the embedding model to extend its capabilities based on robust retrieval performance. Our model achieves state-of-the-art results on the MTEB and CMTEB benchmarks, ranking first on both leaderboards (August 27 2025), and simultaneously achieves state-of-the-art performance on tasks including reranking, clustering, etc. Our findings demonstrate that higher-quality, more diverse data is crucial for advancing retrieval model performance, and that leveraging LLMs generative capabilities can further optimize data quality for embedding model breakthroughs. Our model weights are released on HuggingFace under Apache 2.0 license. For reproducibility, we provide evaluation code and instructions on GitHub.

