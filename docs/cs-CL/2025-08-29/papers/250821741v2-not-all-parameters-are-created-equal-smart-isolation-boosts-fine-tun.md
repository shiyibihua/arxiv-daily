---
layout: default
title: Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance
---

# Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21741" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21741v2</a>
  <a href="https://arxiv.org/pdf/2508.21741.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21741v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21741v2', 'Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yao Wang, Di Liang, Minlong Peng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29 (æ›´æ–°: 2025-09-19)

**å¤‡æ³¨**: Accepted to EMNLP 2025 Main Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ ¸å¿ƒå‚æ•°éš”ç¦»å¾®è°ƒæ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ ¸å¿ƒå‚æ•°éš”ç¦»` `å¾®è°ƒæ¡†æ¶` `å¤šä»»åŠ¡å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å‚æ•°èåˆ` `ç¾éš¾æ€§é—å¿˜` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨å‚æ•°æ›´æ–°æ—¶å®¹æ˜“å¯¼è‡´ä»»åŠ¡é—´çš„æ€§èƒ½å†²çªï¼Œå½±å“æ•´ä½“æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºçš„CPI-FTæ¡†æ¶é€šè¿‡è¯†åˆ«å’Œéš”ç¦»æ ¸å¿ƒå‚æ•°ï¼Œä¼˜åŒ–äº†å¤šä»»åŠ¡å¾®è°ƒè¿‡ç¨‹ï¼Œå‡å°‘äº†ä»»åŠ¡é—´çš„å¹²æ‰°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCPI-FTåœ¨å¤šä¸ªå…¬å…±åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†å¾®è°ƒæ€§èƒ½ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”äºä¸‹æ¸¸ä»»åŠ¡çš„é‡è¦æ–¹æ³•ï¼Œä½†å¸¸å¸¸é¢ä¸´â€œè··è··æ¿ç°è±¡â€ï¼Œå³æ— å·®åˆ«çš„å‚æ•°æ›´æ–°åœ¨æŸäº›ä»»åŠ¡ä¸Šå–å¾—è¿›å±•çš„åŒæ—¶å´æŸå®³äº†å…¶ä»–ä»»åŠ¡çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ ¸å¿ƒå‚æ•°éš”ç¦»å¾®è°ƒï¼ˆCPI-FTï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆç‹¬ç«‹åœ°å¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä»¥é‡åŒ–å‚æ•°æ›´æ–°å¹…åº¦æ¥è¯†åˆ«æ ¸å¿ƒå‚æ•°åŒºåŸŸã€‚ç„¶åï¼Œæ ¹æ®åŒºåŸŸé‡å å°†å…·æœ‰ç›¸ä¼¼æ ¸å¿ƒåŒºåŸŸçš„ä»»åŠ¡è¿›è¡Œåˆ†ç»„ï¼Œå½¢æˆè”åˆå»ºæ¨¡çš„é›†ç¾¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å‚æ•°èåˆæŠ€æœ¯ï¼šå¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œå°†å…¶å•ç‹¬å¾®è°ƒæ¨¡å‹çš„æ ¸å¿ƒå‚æ•°ç›´æ¥ç§»æ¤åˆ°ç»Ÿä¸€çš„ä¸»å¹²ç½‘ç»œä¸­ï¼Œè€Œä¸åŒä»»åŠ¡çš„éæ ¸å¿ƒå‚æ•°åˆ™é€šè¿‡çƒé¢çº¿æ€§æ’å€¼ï¼ˆSLERPï¼‰å¹³æ»‘æ•´åˆï¼Œä»è€Œå‡è½»ç ´åæ€§å¹²æ‰°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ç¼“è§£äº†ä»»åŠ¡é—´å¹²æ‰°å’Œé—å¿˜ï¼ŒæŒç»­è¶…è¶Šäº†ä¼ ç»Ÿçš„å¤šä»»åŠ¡å’Œå¤šé˜¶æ®µå¾®è°ƒåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šä»»åŠ¡å¾®è°ƒä¸­ï¼Œå‚æ•°æ›´æ–°å¯¼è‡´çš„â€œè··è··æ¿ç°è±¡â€ï¼Œå³æŸäº›ä»»åŠ¡çš„æ€§èƒ½æå‡ä¼´éšå…¶ä»–ä»»åŠ¡çš„æ€§èƒ½ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåŒºåˆ†ä»»åŠ¡é—´çš„æ ¸å¿ƒå‚æ•°ï¼Œå¯¼è‡´å¹²æ‰°å’Œé—å¿˜é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„CPI-FTæ¡†æ¶é€šè¿‡ç‹¬ç«‹å¾®è°ƒæ¯ä¸ªä»»åŠ¡ï¼Œè¯†åˆ«å…¶æ ¸å¿ƒå‚æ•°åŒºåŸŸï¼Œå¹¶å°†ç›¸ä¼¼ä»»åŠ¡çš„æ ¸å¿ƒåŒºåŸŸè¿›è¡Œåˆ†ç»„ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„è”åˆå»ºæ¨¡ã€‚æ­¤è®¾è®¡æ—¨åœ¨å‡å°‘ä»»åŠ¡é—´çš„å¹²æ‰°ï¼Œæå‡æ•´ä½“å¾®è°ƒæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCPI-FTæ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œç‹¬ç«‹å¾®è°ƒæ¯ä¸ªä»»åŠ¡ä»¥è¯†åˆ«æ ¸å¿ƒå‚æ•°ï¼›å…¶æ¬¡ï¼ŒåŸºäºæ ¸å¿ƒåŒºåŸŸé‡å å°†ä»»åŠ¡åˆ†ç»„ï¼›ç„¶åï¼Œé‡‡ç”¨å‚æ•°èåˆæŠ€æœ¯å°†æ ¸å¿ƒå‚æ•°ç§»æ¤åˆ°ç»Ÿä¸€ä¸»å¹²ä¸­ï¼Œæœ€åé€šè¿‡æ··åˆä»»åŠ¡æ•°æ®è¿›è¡Œè½»é‡åŒ–çš„å¾®è°ƒè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šCPI-FTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡æ ¸å¿ƒå‚æ•°éš”ç¦»ä¸èåˆæŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½äº†ä»»åŠ¡é—´çš„å¹²æ‰°ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­æ™®éå­˜åœ¨çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ä¸ç°æœ‰çš„å¤šä»»åŠ¡å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒCPI-FTåœ¨å‚æ•°ç®¡ç†ä¸Šæ›´ä¸ºç²¾ç»†åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒCPI-FTé€šè¿‡é‡åŒ–å‚æ•°æ›´æ–°å¹…åº¦æ¥è¯†åˆ«æ ¸å¿ƒå‚æ•°åŒºåŸŸï¼Œé‡‡ç”¨SLERPæŠ€æœ¯å¹³æ»‘æ•´åˆéæ ¸å¿ƒå‚æ•°ï¼Œç¡®ä¿ä¸åŒä»»åŠ¡é—´çš„å‚æ•°äº¤äº’ä¸äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åŒæ—¶ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­å†»ç»“æ ¸å¿ƒåŒºåŸŸä»¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCPI-FTåœ¨å¤šä¸ªå…¬å…±åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†å¾®è°ƒæ€§èƒ½ï¼Œå°¤å…¶åœ¨ä»»åŠ¡å¹²æ‰°å’Œé—å¿˜æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚ä¸ä¼ ç»Ÿçš„å¤šä»»åŠ¡å’Œå¤šé˜¶æ®µå¾®è°ƒåŸºçº¿ç›¸æ¯”ï¼ŒCPI-FTåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡å®ç°äº†è‡³å°‘10%çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œå¤šä»»åŠ¡å­¦ä¹ ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ï¼ŒCPI-FTèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„é€‚åº”æ€§å’Œæ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆåˆ©ç”¨æœ‰é™è®¡ç®—èµ„æºçš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Supervised fine-tuning (SFT) is a pivotal approach to adapting large language models (LLMs) for downstream tasks; however, performance often suffers from the ``seesaw phenomenon'', where indiscriminate parameter updates yield progress on certain tasks at the expense of others. To address this challenge, we propose a novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework. Specifically, we first independently fine-tune the LLM on each task to identify its core parameter regions by quantifying parameter update magnitudes. Tasks with similar core regions are then grouped based on region overlap, forming clusters for joint modeling. We further introduce a parameter fusion technique: for each task, core parameters from its individually fine-tuned model are directly transplanted into a unified backbone, while non-core parameters from different tasks are smoothly integrated via Spherical Linear Interpolation (SLERP), mitigating destructive interference. A lightweight, pipelined SFT training phase using mixed-task data is subsequently employed, while freezing core regions from prior tasks to prevent catastrophic forgetting. Extensive experiments on multiple public benchmarks demonstrate that our approach significantly alleviates task interference and forgetting, consistently outperforming vanilla multi-task and multi-stage fine-tuning baselines.

