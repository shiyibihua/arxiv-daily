---
layout: default
title: Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: A Data Perspective
---

# Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: A Data Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23508" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23508v3</a>
  <a href="https://arxiv.org/pdf/2506.23508.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23508v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23508v3', 'Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: A Data Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhihao Zhang, Qiaole Dong, Qi Zhang, Jun Zhao, Enyu Zhou, Zhiheng Xi, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Mingqi Wu, Yanwei Fu, Tao Ji, Tao Gui, Xuanjing Huang, Kai Chen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30 (æ›´æ–°: 2025-12-16)

**å¤‡æ³¨**: 28 pages (Preprint.)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼ºåŒ–å¾®è°ƒæ–¹æ³•ä»¥æ›´å¥½åœ°ä¿ç•™å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å…ˆå‰çŸ¥è¯†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å¾®è°ƒ` `ç›‘ç£å¾®è°ƒ` `å¤šæ¨¡æ€å­¦ä¹ ` `çŸ¥è¯†ä¿ç•™` `ç¾éš¾æ€§é—å¿˜` `æ¨¡å‹é€‚åº”æ€§` `æ‹¼å›¾ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„SFTå’ŒRFTæ–¹æ³•åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶å­˜åœ¨ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼Œå½±å“äº†æ¨¡å‹å¯¹å…ˆå‰çŸ¥è¯†çš„ä¿ç•™ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¼•å…¥æ‹¼å›¾ä»»åŠ¡ï¼Œç³»ç»Ÿç ”ç©¶SFTå’ŒRFTåœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†ä¸¤è€…åœ¨çŸ¥è¯†ä¿ç•™ä¸Šçš„å·®å¼‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRFTåœ¨ä¿æŒå…ˆå‰çŸ¥è¯†æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œä¸”é€šè¿‡åˆé€‚çš„æ•°æ®åˆ†å¸ƒè®¾è®¡ï¼ŒSFTä¹Ÿèƒ½æ›´å¥½åœ°ä¿ç•™çŸ¥è¯†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åè®­ç»ƒç®—æ³•å¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¢«å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä¸‹æ¸¸ä»»åŠ¡é€‚åº”ã€‚å°½ç®¡åœ¨ä»»åŠ¡é€‚åº”ä¸Šæœ‰æ•ˆï¼Œä½†å®ƒä»¬å¯¹å…ˆå‰çŸ¥è¯†çš„å½±å“ä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡å¼•å…¥æ‹¼å›¾ä½œä¸ºä¸€ç§æ–°ä»»åŠ¡ï¼Œç³»ç»Ÿç ”ç©¶SFTå’ŒRFTåœ¨å¼€æºå¤šæ¨¡æ€æ¨¡å‹Qwen2.5-VLç³»åˆ—ä¸Šçš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSFTå¿«é€Ÿè·å–ä»»åŠ¡ä½†å¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œè€ŒRFTå­¦ä¹ è¾ƒæ…¢ä½†èƒ½æ›´å¥½åœ°ä¿æŒå…ˆå‰çŸ¥è¯†ã€‚é€šè¿‡å­¦ä¹ åŠ¨æ€åˆ†æï¼Œæˆ‘ä»¬å‘ç°RFTä¸»è¦å¼ºåŒ–ä¸åŸºç¡€æ¨¡å‹æ¦‚ç‡åˆ†å¸ƒè‡ªç„¶å¯¹é½çš„æ­£ç¡®æ ·æœ¬ï¼Œä»è€Œå¯¹å…ˆå‰çŸ¥è¯†çš„å¹²æ‰°è¾ƒå¼±ã€‚æ­¤å¤–ï¼ŒåŸºäºRFTæ¨¡æ‹Ÿçš„å›æ»šè®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨å¿«é€Ÿå­¦ä¹ æ–°ä»»åŠ¡çš„åŒæ—¶æ›´å¥½åœ°ä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè®­ç»ƒæ•°æ®çš„åˆ†å¸ƒåœ¨é—å¿˜ä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ï¼Œå¼ºè°ƒäº†RFTåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­ç¨³å®šæŒç»­å­¦ä¹ çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åè®­ç»ƒç®—æ³•åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶å¯¼è‡´çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¦‚ä½•åœ¨å¿«é€Ÿå­¦ä¹ æ–°ä»»åŠ¡çš„åŒæ—¶ä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚ç°æœ‰çš„SFTæ–¹æ³•åœ¨å¿«é€Ÿé€‚åº”ä»»åŠ¡æ—¶ï¼Œå¾€å¾€ä¼šå¯¼è‡´æ¨¡å‹å¯¹å…ˆå‰çŸ¥è¯†çš„é—å¿˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ä½œä¸ºä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥ï¼Œå¼ºè°ƒé€šè¿‡åˆç†çš„æ•°æ®åˆ†å¸ƒæ¥å‡å°‘å¯¹å…ˆå‰çŸ¥è¯†çš„å¹²æ‰°ï¼Œä»è€Œå®ç°æ›´å¥½çš„çŸ¥è¯†ä¿ç•™ã€‚RFTé€šè¿‡å¼ºåŒ–ä¸åŸºç¡€æ¨¡å‹æ¦‚ç‡åˆ†å¸ƒä¸€è‡´çš„æ ·æœ¬ï¼Œæ¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å¼•å…¥æ‹¼å›¾ä»»åŠ¡ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨RFTå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼›æœ€åï¼Œé€šè¿‡å¯¹æ¯”å®éªŒè¯„ä¼°æ¨¡å‹åœ¨æ–°ä»»åŠ¡å’Œå…ˆå‰çŸ¥è¯†ä¿ç•™ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†RFTæ–¹æ³•ï¼Œé€šè¿‡å¯¹è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒè¿›è¡Œä¼˜åŒ–ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨¡å‹åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶å¯¹å…ˆå‰çŸ¥è¯†çš„å¹²æ‰°ï¼Œè¿™ä¸ä¼ ç»Ÿçš„SFTæ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RFTçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å°å¹…åº¦å½±å“çš„å›æ»šè®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿è®­ç»ƒæ ·æœ¬ä¸å…ˆå‰çŸ¥è¯†çš„æ–¹å‘ä¸€è‡´ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šå¼ºè°ƒäº†å¯¹æ­£ç¡®æ ·æœ¬çš„å¼ºåŒ–ï¼Œç¡®ä¿æ¨¡å‹åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ä¸å¿˜è®°æ—§çŸ¥è¯†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRFTåœ¨ä¿æŒå…ˆå‰çŸ¥è¯†æ–¹é¢æ˜¾è‘—ä¼˜äºSFTï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æ‹¼å›¾ä»»åŠ¡ä¸Šï¼ŒRFTæ¨¡å‹çš„çŸ¥è¯†ä¿ç•™ç‡æé«˜äº†çº¦30%ï¼Œè€ŒSFTæ¨¡å‹åˆ™å‡ºç°äº†50%çš„çŸ¥è¯†é—å¿˜ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººç­‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„çŸ¥è¯†ä¿ç•™èƒ½åŠ›ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„ç¨³å®šæ€§å’Œå¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æŒç»­å­¦ä¹ çš„åœºæ™¯ä¸­ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Post-training algorithms such as Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large language models to downstream tasks. While effective at task adaptation, their impact on prior knowledge remains unclear. In this paper, we introduce jigsaw puzzles as a novel task absent from existing pretraining corpora and systematically study the behavior of SFT and RFT on open-source multimodal model, Qwen2.5-VL series. Our experiments reveal a sharp trade-off: SFT enables rapid task acquisition but leads to catastrophic forgetting, whereas RFT learns more slowly but maintains prior knowledge. We study this phenomenon through learning dynamics by examining both the magnitude and direction of how training data influence prior knowledge. Our analysis shows that RFT mainly reinforces correct samples naturally aligned with the base model's probability landscape, leading to weaker interference with prior knowledge. Moreover, training on RFT-simulated rollouts, which exert a small magnitude of influence and are well aligned in direction to prior knowledge, allows SFT to preserve prior knowledge better while rapidly learning new tasks. These findings suggest that distribution of training data, rather than algorithmic differences, plays a central role in forgetting, and highlight RFT's potential for stable continual learning in multimodal large language models.

