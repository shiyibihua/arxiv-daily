---
layout: default
title: L0: Reinforcement Learning to Become General Agents
---

# L0: Reinforcement Learning to Become General Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23667" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23667v1</a>
  <a href="https://arxiv.org/pdf/2506.23667.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23667v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23667v1', 'L0: Reinforcement Learning to Become General Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junjie Zhang, Jingyi Xi, Zhuoyang Song, Junyu Lu, Yuhua Ke, Ting Sun, Yukun Yang, Jiaxing Zhang, Songxin Zhang, Zejian Xie

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/cmriat/l0)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºL-Zeroä»¥è§£å†³å¤§è§„æ¨¡è‡ªä¸»æ™ºèƒ½ä½“è®­ç»ƒæ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªä¸»æ™ºèƒ½ä½“` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¹¶å‘è®­ç»ƒ` `å¯éªŒè¯å¥–åŠ±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“æ—¶é¢ä¸´å¯æ‰©å±•æ€§å’Œæ•ˆç‡çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºL-Zeroï¼ˆL0ï¼‰ï¼Œä¸€ä¸ªå¯æ‰©å±•çš„ç«¯åˆ°ç«¯è®­ç»ƒç®¡é“ï¼Œç»“åˆä½æˆæœ¬çš„å¹¶å‘æ™ºèƒ½ä½“å·¥ä½œæ± ï¼Œç®€åŒ–äº†å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒL0åœ¨å¤šä¸ªé—®ç­”åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶åœ¨é—®é¢˜è§£å†³èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“ä»¥åº”å¯¹å¤šè½®é•¿æ—¶é—´ä»»åŠ¡ä»é¢ä¸´å¯æ‰©å±•æ€§å’Œè®­ç»ƒæ•ˆç‡çš„é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†L-Zeroï¼ˆL0ï¼‰ï¼Œä¸€ä¸ªå¯æ‰©å±•çš„ç«¯åˆ°ç«¯è®­ç»ƒç®¡é“ï¼Œæ—¨åœ¨ä¸ºé€šç”¨æ™ºèƒ½ä½“æä¾›æ”¯æŒã€‚L0å…·æœ‰ä½æˆæœ¬ã€å¯æ‰©å±•å’Œæ²™ç®±å¼çš„å¹¶å‘æ™ºèƒ½ä½“å·¥ä½œæ± ï¼Œé™ä½äº†åœ¨å¤æ‚ç¯å¢ƒä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ çš„é—¨æ§›ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†NB-Agentï¼Œä½œä¸ºL0ä¸­çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡è¯»-è¯„-æ‰“å°-å¾ªç¯ï¼ˆREPLï¼‰ä»¥â€œä»£ç å³è¡ŒåŠ¨â€çš„æ–¹å¼è¿ä½œã€‚æˆ‘ä»¬åœ¨äº‹å®æ€§é—®ç­”åŸºå‡†ä¸Šè¯„ä¼°äº†L0ï¼Œå®éªŒè¡¨æ˜åŸºç¡€æ¨¡å‹ä»…é€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å³å¯å‘å±•å‡ºç¨³å¥çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚åœ¨Qwen2.5-7B-Instructæ¨¡å‹ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿SimpleQAçš„å‡†ç¡®ç‡ä»30%æå‡è‡³80%ï¼ŒHotpotQAçš„å‡†ç¡®ç‡ä»22%æå‡è‡³41%ã€‚æˆ‘ä»¬å·²å¼€æºæ•´ä¸ªL0ç³»ç»Ÿï¼ŒåŒ…æ‹¬L0ç³»åˆ—æ¨¡å‹ã€NB-Agentã€å®Œæ•´çš„è®­ç»ƒç®¡é“åŠç›¸åº”çš„è®­ç»ƒé…æ–¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“æ—¶çš„å¯æ‰©å±•æ€§å’Œè®­ç»ƒæ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ æ—¶ï¼Œå¾€å¾€é¢ä¸´é«˜æˆæœ¬å’Œä½æ•ˆç‡çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šL-Zeroï¼ˆL0ï¼‰é€šè¿‡æ„å»ºä¸€ä¸ªä½æˆæœ¬ã€å¯æ‰©å±•çš„å¹¶å‘æ™ºèƒ½ä½“å·¥ä½œæ± ï¼Œé™ä½äº†å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨é—¨æ§›ã€‚å¼•å…¥çš„NB-Agentä»¥â€œä»£ç å³è¡ŒåŠ¨â€çš„æ–¹å¼è¿ä½œï¼Œæå‡äº†æ™ºèƒ½ä½“çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šL0çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå¹¶å‘æ™ºèƒ½ä½“å·¥ä½œæ± å’ŒNB-Agentæ¨¡å—ã€‚å·¥ä½œæ± è´Ÿè´£ç®¡ç†å¤šä¸ªæ™ºèƒ½ä½“çš„å¹¶è¡Œè®­ç»ƒï¼Œè€ŒNB-Agentåˆ™é€šè¿‡REPLæœºåˆ¶æ‰§è¡Œä»£ç ï¼Œå®æ—¶åé¦ˆå’Œè°ƒæ•´ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šL0çš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¹¶å‘æ™ºèƒ½ä½“å·¥ä½œæ± çš„è®¾è®¡å’ŒNB-Agentçš„â€œä»£ç å³è¡ŒåŠ¨â€æœºåˆ¶ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´é«˜æ•ˆåœ°å­¦ä¹ å’Œé€‚åº”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨L0ä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬æ™ºèƒ½ä½“çš„å¹¶å‘æ•°é‡ã€å¥–åŠ±æœºåˆ¶çš„è®¾è®¡ï¼ˆå¯éªŒè¯å¥–åŠ±ï¼‰ä»¥åŠNB-Agentçš„ä»£ç æ‰§è¡Œæ•ˆç‡ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹çš„é«˜æ•ˆæ€§å’Œæ™ºèƒ½ä½“çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒL0åœ¨SimpleQAåŸºå‡†ä¸Šçš„å‡†ç¡®ç‡ä»30%æå‡è‡³80%ï¼Œåœ¨HotpotQAåŸºå‡†ä¸Šçš„å‡†ç¡®ç‡ä»22%æå‡è‡³41%ã€‚è¿™äº›æ˜¾è‘—çš„æå‡è¡¨æ˜L0åœ¨é—®é¢˜è§£å†³èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€è‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿå’Œå¤æ‚ä»»åŠ¡çš„è‡ªä¸»æ‰§è¡Œç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªä¸»å­¦ä¹ èƒ½åŠ›ï¼ŒL0èƒ½å¤Ÿåœ¨å¤šç§å®é™…åœºæ™¯ä¸­æä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨æ™ºèƒ½ä½“æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training large language models (LLMs) to act as autonomous agents for multi-turn, long-horizon tasks remains significant challenges in scalability and training efficiency. To address this, we introduce L-Zero (L0), a scalable, end-to-end training pipeline for general-purpose agents. Featuring a low-cost, extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier for applying reinforcement learning in complex environments. We also introduce NB-Agent, the agent scaffold within L0, which operates in a "code-as-action" fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality question-answering benchmarks. Our experiments demonstrate that a base model can develop robust problem-solving skills using solely Reinforcement Learning with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41 %. We have open-sourced the entire L0 system, including our L0 series models, the NB-Agent, a complete training pipeline, and the corresponding training recipes on (https://github.com/cmriat/l0).

