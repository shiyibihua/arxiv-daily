---
layout: default
title: Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting
---

# Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23888" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23888v1</a>
  <a href="https://arxiv.org/pdf/2506.23888.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23888v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23888v1', 'Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: AndrÃ© de Souza Loureiro, Jorge Valverde-Rebaza, Julieta Noguez, David Escarcega, Ricardo Marcacini

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: Accepted for publication in: European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2025). Research Track

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMAPSæ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ•°å­¦æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¤šæ­¥æ¨ç†` `è‡ªæˆ‘åæ€` `åŠ¨æ€æç¤º` `æ€ç»´é“¾` `æ•°å­¦æ¨ç†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å¤šæ­¥æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„MAPSæ¡†æ¶é€šè¿‡è¿­ä»£çš„è‡ªæˆ‘åæ€å’ŒåŠ¨æ€æç¤ºç”Ÿæˆï¼Œæ—¨åœ¨æå‡æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMAPSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¼ ç»Ÿçš„CoTæ–¹æ³•ï¼Œå¹¶ä¸ä¸“é—¨ä¼˜åŒ–çš„æ¨ç†æ¨¡å‹ç›¸åª²ç¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é—®é¢˜è§£å†³èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œä½†åœ¨å¤æ‚çš„å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­ä»å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šå±‚è‡ªæˆ‘åæ€ä¸è‡ªåŠ¨æç¤ºï¼ˆMAPSï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆæ€ç»´é“¾ï¼ˆCoTï¼‰ã€è‡ªæˆ‘åæ€å’Œè‡ªåŠ¨æç¤ºç­‰æŠ€æœ¯ï¼Œå¢å¼ºLLMsçš„å¤šæ­¥æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„é™æ€æç¤ºæ–¹æ³•ä¸åŒï¼ŒMAPSé‡‡ç”¨è¿­ä»£ç²¾ç‚¼è¿‡ç¨‹ï¼Œåˆæ­¥ç”Ÿæˆè§£å†³æ–¹æ¡ˆåï¼Œè‹¥å‘ç°é”™è¯¯ï¼Œé€‚åº”æ€§è‡ªæˆ‘åæ€æœºåˆ¶å°†è¯†åˆ«å¹¶åˆ†æé”™è¯¯ï¼Œç”Ÿæˆå®šåˆ¶æç¤ºä»¥æŒ‡å¯¼ä¿®æ­£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAPSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†CoTï¼Œå¹¶ä¸ä¼˜åŒ–æ¨ç†çš„æ¨¡å‹è¾¾æˆç«äº‰æ€§ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šæ­¥æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯æ¨ç†é”™è¯¯é¢‘å‘çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚é™æ€æç¤ºæ— æ³•æœ‰æ•ˆåº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMAPSæ¡†æ¶çš„æ ¸å¿ƒåœ¨äºé€šè¿‡è‡ªæˆ‘åæ€æœºåˆ¶å’ŒåŠ¨æ€æç¤ºç”Ÿæˆï¼Œå¸®åŠ©æ¨¡å‹åœ¨å‘ç°é”™è¯¯åè¿›è¡Œæœ‰æ•ˆçš„ä¿®æ­£ï¼Œä»è€Œæå‡æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMAPSçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åˆæ­¥ç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„æ€ç»´é“¾ï¼ˆCoTï¼‰é˜¶æ®µã€é”™è¯¯æ£€æµ‹ä¸è‡ªæˆ‘åæ€é˜¶æ®µï¼Œä»¥åŠç”Ÿæˆå®šåˆ¶æç¤ºä»¥è¿›è¡Œè¿­ä»£ä¿®æ­£çš„é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šMAPSçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€è°ƒæ•´æç¤ºçš„èƒ½åŠ›ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸æ–­ä¼˜åŒ–ï¼Œè€Œä¸æ˜¯ä¾èµ–äºé™æ€çš„æç¤ºä¿¡æ¯ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨é¢å¯¹å¤æ‚é—®é¢˜æ—¶æ›´å…·çµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒMAPSé™åˆ¶äº†åæ€çš„æ·±åº¦ï¼Œä»¥å¹³è¡¡æ¨ç†æ€§èƒ½ä¸è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥æ”¯æŒè‡ªæˆ‘åæ€å’ŒåŠ¨æ€æç¤ºç”Ÿæˆçš„è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMAPSåœ¨å››ä¸ªæˆç†ŸåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†æ ‡å‡†çš„æ€ç»´é“¾æ–¹æ³•ï¼Œä¸”åœ¨æ¨ç†ä¼˜åŒ–æ¨¡å‹ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒMAPSåœ¨æŸäº›ä»»åŠ¡ä¸Šæå‡äº†æ¨ç†å‡†ç¡®ç‡è¾¾20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šæ­¥æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èåˆ†æå’Œç§‘å­¦ç ”ç©¶ç­‰éœ€è¦å¤æ‚æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒMAPSæ¡†æ¶èƒ½å¤Ÿåœ¨è¿™äº›é¢†åŸŸæä¾›æ›´ä¸ºç²¾å‡†çš„å†³ç­–æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in Large Language Models (LLMs) have significantly improved their problem-solving capabilities. However, these models still struggle when faced with complex multi-step reasoning tasks. In this paper, we propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework, a novel approach designed to enhance multi-step mathematical reasoning in LLMs by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an iterative refinement process. Initially, the model generates a solution using CoT prompting. When errors are detected, an adaptive self-reflection mechanism identifies and analyzes them, generating tailored prompts to guide corrections. These dynamically adjusted prompts enable the model to iteratively refine its reasoning. Experiments on four well-established benchmarks across multiple LLMs show that MAPS significantly outperforms standard CoT and achieves competitive results with reasoning-optimized models. In addition, MAPS enables general-purpose LLMs to reach performance levels comparable to specialized reasoning models. While deeper reflection layers improve accuracy, they also increase token usage and costs. To balance this trade-off, MAPS strategically limits reflection depth, ensuring an optimal balance between cost and reasoning performance.

