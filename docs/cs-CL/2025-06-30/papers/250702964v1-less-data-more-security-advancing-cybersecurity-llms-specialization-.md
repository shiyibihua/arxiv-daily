---
layout: default
title: Less Data, More Security: Advancing Cybersecurity LLMs Specialization via Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal Tokens
---

# Less Data, More Security: Advancing Cybersecurity LLMs Specialization via Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal Tokens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.02964" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.02964v1</a>
  <a href="https://arxiv.org/pdf/2507.02964.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.02964v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.02964v1', 'Less Data, More Security: Advancing Cybersecurity LLMs Specialization via Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Salahuddin Salahuddin, Ahmed Hussain, Jussi LÃ¶ppÃ¶nen, Toni Jutila, Panos Papadimitratos

**åˆ†ç±»**: cs.CL, cs.AI, cs.CR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: 15 Pages and 10 Figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡èµ„æºé«˜æ•ˆçš„é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæå‡ç½‘ç»œå®‰å…¨LLMä¸“ä¸šåŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç½‘ç»œå®‰å…¨` `å¤§å‹è¯­è¨€æ¨¡å‹` `é¢†åŸŸè‡ªé€‚åº”` `è¿ç»­é¢„è®­ç»ƒ` `æ¨¡å‹å¾®è°ƒ` `æ•°æ®æ•ˆç‡` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨åˆ†æä¸­ç¼ºä¹ä¸“ä¸šçŸ¥è¯†ï¼Œå¯¼è‡´æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºé¢†åŸŸè‡ªé€‚åº”è¿ç»­é¢„è®­ç»ƒï¼ˆDAPï¼‰æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹ä»¥å¢å¼ºç½‘ç»œå®‰å…¨ç†è§£ï¼ŒåŒæ—¶ä¿æŒè¯­è¨€èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLlama-3.3-70B-Ins-DAPæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æ˜¾è‘—é«˜äºå…¶ä»–ä¸“ä¸šæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†é€šç”¨æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨åˆ†æä¸­ç¼ºä¹ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ã€‚æœ¬æ–‡æ¢è®¨äº†é¢†åŸŸè‡ªé€‚åº”è¿ç»­é¢„è®­ç»ƒï¼ˆDAPï¼‰ä½œä¸ºå¢å¼ºé¢„è®­ç»ƒLLMç½‘ç»œå®‰å…¨ç†è§£çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒé€šç”¨è¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è°ƒæ•´äº†ä¸‰ç§è§£ç å™¨æ¶æ„ï¼Œä½¿ç”¨äº†æ¥è‡ªæ ‡å‡†ã€å­¦æœ¯æ–‡çŒ®ç­‰å¤šç§æ¥æºçš„1.26äº¿å­—ç½‘ç»œå®‰å…¨è¯­æ–™åº“ã€‚é€šè¿‡çº¦æŸè®­ç»ƒå‚æ•°å’Œåˆ†å¸ƒå¼FSDPè®­ç»ƒï¼Œæˆ‘ä»¬åœ¨é¢†åŸŸä¸“ä¸šåŒ–ä¸çŸ¥è¯†ä¿ç•™ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒLlama-3.3-70B-Ins-DAPæ¨¡å‹åœ¨CTI-MCQã€CyberMetricå’ŒSecEvalä¸‰ä¸ªç½‘ç»œå®‰å…¨åŸºå‡†ä¸Šå‡å®ç°äº†æ˜¾è‘—æå‡ï¼Œåˆ†åˆ«è¾¾åˆ°äº†0.718ã€0.933å’Œ0.864çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†åŒ…æ‹¬Llama-Primus-Baseåœ¨å†…çš„ä¸“ä¸šæ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨æ˜¾è‘—æ›´å°çš„æ•°æ®é›†ï¼ˆ1.188äº¿å¯¹æ¯”2.77äº¿tokensï¼‰å®ç°äº†ç«äº‰æ€§è¡¨ç°ï¼Œè¯æ˜äº†é«˜æ•ˆé¢†åŸŸä¸“ä¸šåŒ–çš„å¯è¡Œæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸç¼ºä¹ä¸“ä¸šçŸ¥è¯†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºåºå¤§çš„æ•°æ®é›†ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œè®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé¢†åŸŸè‡ªé€‚åº”è¿ç»­é¢„è®­ç»ƒï¼ˆDAPï¼‰ä½œä¸ºè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ä½¿ç”¨é’ˆå¯¹æ€§çš„ç½‘ç»œå®‰å…¨è¯­æ–™åº“è¿›è¡Œå¾®è°ƒï¼Œå¢å¼ºæ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶é€šç”¨è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯æ•°æ®å‡†å¤‡ï¼Œæ„å»º126ç™¾ä¸‡å­—çš„ç½‘ç»œå®‰å…¨è¯­æ–™åº“ï¼›å…¶æ¬¡æ˜¯æ¨¡å‹å¾®è°ƒï¼Œé‡‡ç”¨çº¦æŸè®­ç»ƒå‚æ•°å’Œåˆ†å¸ƒå¼FSDPè®­ç»ƒï¼›æœ€åæ˜¯æ¨¡å‹è¯„ä¼°ï¼Œé€šè¿‡CTI-MCQã€CyberMetricå’ŒSecEvalç­‰åŸºå‡†æµ‹è¯•éªŒè¯æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå®ç°äº†åœ¨è¾ƒå°æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆä¸“ä¸šåŒ–ï¼ŒæŒ‘æˆ˜äº†å¯¹å¤§å‹æ•°æ®é›†çš„ä¼ ç»Ÿä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†åˆ†å¸ƒå¼FSDPè®­ç»ƒç­–ç•¥ï¼Œå¹¶å¯¹è®­ç»ƒå‚æ•°è¿›è¡Œäº†ä¸¥æ ¼çº¦æŸï¼Œä»¥ç¡®ä¿åœ¨ä¿æŒçŸ¥è¯†çš„åŒæ—¶å®ç°é¢†åŸŸä¸“ä¸šåŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLlama-3.3-70B-Ins-DAPæ¨¡å‹åœ¨CTI-MCQã€CyberMetricå’ŒSecEvalåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«è¾¾åˆ°äº†0.718ã€0.933å’Œ0.864çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†åŒ…æ‹¬Llama-Primus-Baseåœ¨å†…çš„å¤šä¸ªä¸“ä¸šæ¨¡å‹ï¼Œä¸”ä½¿ç”¨çš„æ•°æ®é›†æ˜¾è‘—å°äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºé«˜æ•ˆçš„é¢†åŸŸä¸“ä¸šåŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç½‘ç»œå®‰å…¨å¨èƒåˆ†æã€æ¼æ´è¯„ä¼°å’Œå®‰å…¨æ–‡æ¡£ç¼–å†™ã€‚é€šè¿‡æå‡LLMåœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„ä¸“ä¸šèƒ½åŠ›ï¼Œå¯ä»¥ä¸ºå®‰å…¨ä¸“å®¶æä¾›æ›´æœ‰æ•ˆçš„è¾…åŠ©å·¥å…·ï¼Œå¢å¼ºæ•´ä½“ç½‘ç»œå®‰å…¨é˜²æŠ¤èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Large Language Models (LLMs) demonstrate exceptional natural language capabilities, general-purpose models lack specialized domain knowledge for effective cybersecurity analysis. In this work, we investigate Domain-Adaptive Continuous Pretraining (DAP) as a methodology for enhancing cybersecurity understanding in pretrained LLMs while preserving general language capabilities. We systematically adapted three decoder-based architectures -- Llama-3.1-8B, DeepSeek-R1-Distill-Qwen-14B, and Llama-3.3-70B-Instruct -- using a curated 126-million-word cybersecurity corpus from standards, academic literature, and various other sources. Our approach employed constrained training parameters and distributed FSDP training to balance domain specialization with knowledge preservation. Evaluation across three cybersecurity benchmarks, namely, CTI-MCQ, CyberMetric, and SecEval, demonstrates consistent improvements post-adaptation. The Llama-3.3-70B-Ins-DAP model achieved state-of-the-art accuracies of 0.718, 0.933, and 0.864, respectively, outperforming specialized models, including Llama-Primus-Base. Notably, competitive performance was achieved using substantially smaller datasets (118.8 million versus 2.77 billion tokens), demonstrating efficient domain specialization viability. We establish that targeted continuous pretraining enables effective cybersecurity domain adaptation with computational feasibility, providing foundations for specialized AI assistants in threat analysis, vulnerability assessment, and security documentation while challenging prevailing assumptions about data requirements for LLM specialization.

