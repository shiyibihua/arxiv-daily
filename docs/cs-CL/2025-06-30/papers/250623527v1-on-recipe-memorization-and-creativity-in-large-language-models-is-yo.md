---
layout: default
title: On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?
---

# On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23527" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23527v1</a>
  <a href="https://arxiv.org/pdf/2506.23527.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23527v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23527v1', 'On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jan Kvapil, Martin Fajcik

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: 13 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªåŠ¨åŒ–æ¡†æ¶ä»¥è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„é£Ÿè°±è®°å¿†ä¸åˆ›é€ åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é£Ÿè°±ç”Ÿæˆ` `è‡ªåŠ¨åŒ–è¯„ä¼°` `åˆ›é€ åŠ›åˆ†æ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„é£Ÿè°±æ—¶ï¼Œç¼ºä¹ç³»ç»Ÿæ€§å’Œè§„æ¨¡åŒ–çš„åˆ†ææ‰‹æ®µï¼Œéš¾ä»¥å‡†ç¡®åŒºåˆ†è®°å¿†å†…å®¹ä¸åˆ›é€ æ€§ç”Ÿæˆã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„â€œLLMä½œä¸ºè¯„åˆ¤è€…â€æ¡†æ¶ï¼Œé€šè¿‡æœºå™¨å­¦ä¹ æŠ€æœ¯å®ç°é£Ÿè°±ç”Ÿæˆã€æˆåˆ†è§£æå’Œæ— æ„ä¹‰å†…å®¹æ£€æµ‹çš„è‡ªåŠ¨åŒ–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Llama 3.1+Gemma 2 9Bçš„è‡ªåŠ¨åŒ–æ¡†æ¶åœ¨æˆåˆ†åŒ¹é…ä¸Šè¾¾åˆ°äº†78%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—æå‡äº†åˆ†æçš„æ•ˆç‡å’Œè§„æ¨¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„çƒ¹é¥ªé£Ÿè°±ä¸­çš„è®°å¿†ã€åˆ›é€ åŠ›å’Œæ— æ„ä¹‰å†…å®¹ã€‚æˆ‘ä»¬æ—¨åœ¨é€šè¿‡é«˜è´¨é‡çš„äººç±»åˆ¤æ–­åˆ†æé£Ÿè°±çš„è®°å¿†å’Œåˆ›é€ åŠ›ï¼Œå¹¶è®¾è®¡è‡ªåŠ¨åŒ–æ–¹æ³•ä»¥æ‰©å±•ç ”ç©¶è§„æ¨¡ã€‚é€šè¿‡å¯¹20ä¸ªé¢„é€‰é£Ÿè°±çš„è¯¦ç»†äººå·¥æ³¨é‡Šï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹Mixtralåœ¨ç”Ÿæˆé£Ÿè°±æ—¶å¼ºçƒˆä¾èµ–äºåœ¨çº¿æ–‡æ¡£ä¸­çš„è®°å¿†å†…å®¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªâ€œLLMä½œä¸ºè¯„åˆ¤è€…â€çš„è‡ªåŠ¨åŒ–ç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆé£Ÿè°±ã€æ£€æµ‹æ— æ„ä¹‰å†…å®¹ã€è§£ææˆåˆ†å’Œæ­¥éª¤ï¼Œå¹¶è¿›è¡Œæ³¨é‡Šã€‚è¯¥æ¡†æ¶çš„åº”ç”¨ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿå¤§è§„æ¨¡é‡åŒ–ç”Ÿæˆé£Ÿè°±ä¸­çš„è®°å¿†ã€åˆ›é€ åŠ›å’Œæ— æ„ä¹‰å†…å®¹ï¼Œæä¾›äº†æ¨¡å‹åˆ›é€ èƒ½åŠ›çš„ä¸¥è°¨è¯æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„é£Ÿè°±ä¸­çš„è®°å¿†ã€åˆ›é€ åŠ›å’Œæ— æ„ä¹‰å†…å®¹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äººå·¥è¯„ä¼°ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¸”è¯„ä¼°ç»“æœçš„ä¸»è§‚æ€§è¾ƒå¼ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªè‡ªåŠ¨åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨LLMè‡ªèº«ä½œä¸ºè¯„åˆ¤è€…ï¼Œè‡ªåŠ¨ç”Ÿæˆé£Ÿè°±å¹¶è¿›è¡Œå†…å®¹åˆ†æï¼Œä»¥å®ç°å¤§è§„æ¨¡çš„è¯„ä¼°å’Œé‡åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šé£Ÿè°±ç”Ÿæˆæ¨¡å—ã€æ— æ„ä¹‰å†…å®¹æ£€æµ‹æ¨¡å—ã€æˆåˆ†è§£ææ¨¡å—å’Œæ³¨é‡Šæ¨¡å—ã€‚é€šè¿‡è¿™äº›æ¨¡å—çš„ååŒå·¥ä½œï¼Œèƒ½å¤Ÿå®ç°ä»ç”Ÿæˆåˆ°è¯„ä¼°çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†LLMä½œä¸ºè¯„åˆ¤è€…ï¼Œåˆ©ç”¨å…¶ç”Ÿæˆèƒ½åŠ›å’Œç†è§£èƒ½åŠ›è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„äººå·¥è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ•ˆç‡å’Œä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œé‡‡ç”¨äº†Llama 3.1+Gemma 2 9Bä½œä¸ºæˆåˆ†æå–å’Œæ³¨é‡Šçš„å·¥å…·ï¼Œè®¾ç½®äº†é€‚å½“çš„é˜ˆå€¼å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æˆåˆ†åŒ¹é…çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨Llama 3.1+Gemma 2 9Bçš„è‡ªåŠ¨åŒ–æ¡†æ¶åœ¨æˆåˆ†åŒ¹é…ä¸Šè¾¾åˆ°äº†78%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿäººå·¥è¯„ä¼°æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†åˆ†ææ•ˆç‡å’Œä¸€è‡´æ€§ï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨é£Ÿè°±ç”Ÿæˆä¸­çš„åˆ›é€ æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é£Ÿå“ç§‘æŠ€ã€çƒ¹é¥ªæ•™è‚²å’Œäººå·¥æ™ºèƒ½åˆ›ä½œç­‰ã€‚é€šè¿‡è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œç ”ç©¶è€…å’Œå¼€å‘è€…å¯ä»¥æ›´é«˜æ•ˆåœ°åˆ†æå’Œæ”¹è¿›å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé£Ÿè°±æ–¹é¢çš„è¡¨ç°ï¼Œæ¨åŠ¨æ™ºèƒ½çƒ¹é¥ªåŠ©æ‰‹å’Œä¸ªæ€§åŒ–é¥®é£Ÿæ¨èç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work-in-progress investigates the memorization, creativity, and nonsense found in cooking recipes generated from Large Language Models (LLMs). Precisely, we aim (i) to analyze memorization, creativity, and non-sense in LLMs using a small, high-quality set of human judgments and (ii) to evaluate potential approaches to automate such a human annotation in order to scale our study to hundreds of recipes. To achieve (i), we conduct a detailed human annotation on 20 preselected recipes generated by LLM (Mixtral), extracting each recipe's ingredients and step-by-step actions to assess which elements are memorized--i.e., directly traceable to online sources possibly seen during training--and which arise from genuine creative synthesis or outright nonsense. We find that Mixtral consistently reuses ingredients that can be found in online documents, potentially seen during model training, suggesting strong reliance on memorized content. To achieve aim (ii) and scale our analysis beyond small sample sizes and single LLM validation, we design an ``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection, parsing ingredients and recipe steps, and their annotation. For instance, comparing its output against human annotations, the best ingredient extractor and annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on ingredient matching. This automated framework enables large-scale quantification of memorization, creativity, and nonsense in generated recipes, providing rigorous evidence of the models' creative capacities.

