---
layout: default
title: Impact of Fine-Tuning Methods on Memorization in Large Language Models
---

# Impact of Fine-Tuning Methods on Memorization in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00258" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00258v1</a>
  <a href="https://arxiv.org/pdf/2507.00258.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00258v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00258v1', 'Impact of Fine-Tuning Methods on Memorization in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jie Hou, Chuxiong Wu, Lannan Luo, Qiang Zeng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»†åŒ–è°ƒä¼˜æ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„è®°å¿†æ³„éœ²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¾®è°ƒæ–¹æ³•` `éšç§ä¿æŠ¤` `è®°å¿†æ³„éœ²` `æˆå‘˜æ¨æ–­æ”»å‡»` `åŸºäºæç¤ºçš„å¾®è°ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºå‚æ•°çš„å¾®è°ƒæ–¹æ³•åœ¨éšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨è¾ƒå¤§é£é™©ï¼Œå®¹æ˜“å¯¼è‡´è®°å¿†æ³„éœ²ã€‚
2. è®ºæ–‡é€šè¿‡åˆ†ç±»æµè¡Œçš„å¾®è°ƒæ–¹æ³•ï¼Œæå‡ºåŸºäºæç¤ºçš„å¾®è°ƒä½œä¸ºä¸€ç§æ›´å®‰å…¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºæç¤ºçš„å¾®è°ƒåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†å¯¹æˆå‘˜æ¨æ–­æ”»å‡»çš„è„†å¼±æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½åŠ›çš„ä¸æ–­æå‡ï¼Œâ€œé¢„è®­ç»ƒä¸å¾®è°ƒâ€èŒƒå¼æ—¥ç›Šæˆä¸ºä¸»æµï¼Œå¯¼è‡´å¤šç§å¾®è°ƒæ–¹æ³•çš„å‘å±•ã€‚ç„¶è€Œï¼Œå¾®è°ƒè¿‡ç¨‹ä¸­å› è®°å¿†å¯¼è‡´çš„éšç§é£é™©å´ç›¸å¯¹è¾ƒå°‘å—åˆ°å…³æ³¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¯¹æµè¡Œçš„å¾®è°ƒæ–¹æ³•è¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶é€šè¿‡æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAï¼‰çš„è§†è§’è¯„ä¼°å…¶å¯¹è®°å¿†çš„å½±å“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸åŸºäºå‚æ•°çš„å¾®è°ƒç›¸æ¯”ï¼ŒåŸºäºæç¤ºçš„å¾®è°ƒåœ¨æ€§èƒ½ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶å¯¹MIAçš„è„†å¼±æ€§è¾ƒä½ã€‚æ­¤å¤–ï¼ŒåŸºäºæç¤ºçš„æ–¹æ³•åœ¨æ¨¡å‹è§„æ¨¡å˜åŒ–æ—¶ä»èƒ½ä¿æŒä½è®°å¿†ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒåŸºäºå‚æ•°çš„å¾®è°ƒæ›´å®¹æ˜“æ³„éœ²ç§äººä¿¡æ¯ï¼Œè€ŒåŸºäºæç¤ºçš„å¾®è°ƒåˆ™æ˜¯æ›´å…·éšç§ä¿æŠ¤çš„é€‰æ‹©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œç”±äºè®°å¿†å¯¼è‡´çš„éšç§æ³„éœ²é—®é¢˜ã€‚ç°æœ‰çš„åŸºäºå‚æ•°çš„å¾®è°ƒæ–¹æ³•åœ¨è¿™æ–¹é¢å­˜åœ¨è¾ƒå¤§é£é™©ï¼Œå®¹æ˜“è¢«æˆå‘˜æ¨æ–­æ”»å‡»åˆ©ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ”¹å˜å¾®è°ƒç­–ç•¥æ¥é™ä½æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„è®°å¿†ï¼Œä»è€Œå¢å¼ºéšç§ä¿æŠ¤ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–æç¤ºè®¾è®¡ï¼Œå‡å°‘æ¨¡å‹å¯¹ç‰¹å®šè¾“å…¥çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€å¾®è°ƒç­–ç•¥é€‰æ‹©å’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚åœ¨æ•°æ®å‡†å¤‡é˜¶æ®µï¼Œç ”ç©¶è€…å¯¹ä¸åŒå¾®è°ƒæ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼›åœ¨å¾®è°ƒç­–ç•¥é€‰æ‹©é˜¶æ®µï¼Œé‡ç‚¹æ¯”è¾ƒåŸºäºå‚æ•°å’ŒåŸºäºæç¤ºçš„å¾®è°ƒæ–¹æ³•ï¼›æœ€åï¼Œé€šè¿‡æˆå‘˜æ¨æ–­æ”»å‡»è¯„ä¼°æ¨¡å‹çš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºäºæç¤ºçš„å¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¸ä¼ ç»Ÿçš„åŸºäºå‚æ•°çš„å¾®è°ƒç›¸å½“ï¼Œä½†åœ¨éšç§ä¿æŠ¤æ–¹é¢è¡¨ç°æ›´ä½³ã€‚è¿™ä¸€æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶è®¾è®¡ç†å¿µä¾§é‡äºå‡å°‘æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„è®°å¿†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡å…³æ³¨äº†æç¤ºçš„æ„å»ºæ–¹å¼ã€å¾®è°ƒçš„è¶…å‚æ•°è®¾ç½®ä»¥åŠæŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œç¡®ä¿åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­éƒ½èƒ½æœ‰æ•ˆé™ä½è®°å¿†æ³„éœ²é£é™©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæç¤ºçš„å¾®è°ƒæ–¹æ³•åœ¨å¯¹æŠ—æˆå‘˜æ¨æ–­æ”»å‡»æ—¶è¡¨ç°å‡ºæ›´ä½çš„è„†å¼±æ€§ï¼Œç›¸è¾ƒäºåŸºäºå‚æ•°çš„å¾®è°ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½è®°å¿†æ³„éœ²é£é™©ã€‚å…·ä½“è€Œè¨€ï¼ŒåŸºäºæç¤ºçš„æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹å‡ä¿æŒäº†è¾ƒä½çš„è®°å¿†æ°´å¹³ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œä¸ªæ€§åŒ–æ¨èç­‰ã€‚é€šè¿‡é‡‡ç”¨åŸºäºæç¤ºçš„å¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥åœ¨æå‡æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œå¢å¼ºç”¨æˆ·æ•°æ®çš„éšç§ä¿æŠ¤ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As the capabilities of pre-trained large language models (LLMs) continue to advance, the "pre-train and fine-tune" paradigm has become increasingly mainstream, leading to the development of various fine-tuning methods. However, the privacy risks arising from memorization during fine-tuning have received relatively little attention. To address this gap, we categorize popular fine-tuning approaches and assess their impact on memorization through the lens of membership inference attacks (MIAs). Our results show that, compared to parameter-based fine-tuning, prompt-based fine-tuning achieves competitive performance while exhibiting lower vulnerability to MIAs. Furthermore, prompt-based methods maintain low memorization regardless of model scale. These findings suggest that parameter-based fine-tuning is more prone to leaking private information, whereas prompt-based fine-tuning serves as a more privacy-preserving option.

