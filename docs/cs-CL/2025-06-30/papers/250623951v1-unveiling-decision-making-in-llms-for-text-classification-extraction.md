---
layout: default
title: Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders
---

# Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23951" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23951v1</a>
  <a href="https://arxiv.org/pdf/2506.23951.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23951v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23951v1', 'Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mathis Le Bail, JÃ©rÃ©mie Dentan, Davide Buscaldi, Sonia Vanier

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¨€ç–è‡ªç¼–ç å™¨ä»¥æå‡æ–‡æœ¬åˆ†ç±»ä¸­çš„å¯è§£é‡Šæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `å¯è§£é‡Šæ€§` `æ–‡æœ¬åˆ†ç±»` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç‰¹å¾æå–` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•åœ¨æ–‡æœ¬åˆ†ç±»é¢†åŸŸçš„åº”ç”¨å°šä¸å……åˆ†ï¼Œå¯¼è‡´æå–çš„ç‰¹å¾ç¼ºä¹æ¸…æ™°çš„å› æœå…³ç³»å’Œå¯è§£é‡Šæ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„SAEæ¶æ„ï¼Œç»“åˆäº†ä¸“é—¨çš„åˆ†ç±»å™¨å¤´å’Œæ¿€æ´»ç‡ç¨€ç–æŸå¤±ï¼Œä»¥æé«˜æ–‡æœ¬åˆ†ç±»çš„å¯è§£é‡Šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¶æ„åœ¨ä¸¤ä¸ªåˆ†ç±»åŸºå‡†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æå‡äº†æå–ç‰¹å¾çš„å› æœæ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰å·²æˆåŠŸç”¨äºæ¢æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹¶æå–å…¶å†…éƒ¨è¡¨ç¤ºä¸­çš„å¯è§£é‡Šæ¦‚å¿µã€‚è¿™äº›æ¦‚å¿µæ˜¯ç¥ç»å…ƒæ¿€æ´»çš„çº¿æ€§ç»„åˆï¼Œä»£è¡¨äººç±»å¯ç†è§£çš„ç‰¹å¾ã€‚æœ¬æ–‡æ¢è®¨äº†åŸºäºSAEçš„å¯è§£é‡Šæ€§æ–¹æ³•åœ¨å¥å­åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„SAEæ¶æ„ï¼Œä¸“é—¨é’ˆå¯¹æ–‡æœ¬åˆ†ç±»ï¼Œç»“åˆäº†åˆ†ç±»å™¨å¤´å’Œæ¿€æ´»ç‡ç¨€ç–æŸå¤±ã€‚æˆ‘ä»¬å°†è¯¥æ¶æ„ä¸ConceptShapã€ç‹¬ç«‹æˆåˆ†åˆ†æåŠå…¶ä»–SAEæ¦‚å¿µæå–æŠ€æœ¯è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°æ¶µç›–ä¸¤ä¸ªåˆ†ç±»åŸºå‡†å’Œå››ä¸ªå¾®è°ƒçš„Pythiaç³»åˆ—LLMsã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ç§æ–°é¢–çš„åº¦é‡æ ‡å‡†æ¥è¯„ä¼°åŸºäºæ¦‚å¿µçš„è§£é‡Šçš„ç²¾ç¡®æ€§ï¼Œä½¿ç”¨å¤–éƒ¨å¥å­ç¼–ç å™¨ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨æå–ç‰¹å¾çš„å› æœæ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å‡æœ‰æ‰€æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–‡æœ¬åˆ†ç±»æ–¹æ³•åœ¨å¯è§£é‡Šæ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¦‚ä½•ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–æ¸…æ™°ä¸”å¯ç†è§£çš„ç‰¹å¾ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹å¯¹ç‰¹å¾å› æœå…³ç³»çš„æ˜ç¡®è§£é‡Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¨€ç–è‡ªç¼–ç å™¨çš„æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡çº¿æ€§ç»„åˆç¥ç»å…ƒæ¿€æ´»æ¥æå–å¯è§£é‡Šçš„æ¦‚å¿µï¼Œç»“åˆæ¿€æ´»ç‡ç¨€ç–æŸå¤±ä»¥å¢å¼ºç‰¹å¾çš„å¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å±‚ã€ç¨€ç–è‡ªç¼–ç å™¨æ¨¡å—ã€åˆ†ç±»å™¨å¤´å’ŒæŸå¤±è®¡ç®—æ¨¡å—ã€‚ç¨€ç–è‡ªç¼–ç å™¨è´Ÿè´£æå–ç‰¹å¾ï¼Œåˆ†ç±»å™¨å¤´ç”¨äºæœ€ç»ˆçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†æ¿€æ´»ç‡ç¨€ç–æŸå¤±çš„SAEæ¶æ„ï¼Œä½¿å¾—æå–çš„ç‰¹å¾ä¸ä»…å¯è§£é‡Šï¼Œè€Œä¸”åœ¨å› æœæ€§ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚è¿™ä¸ä¼ ç»Ÿçš„SAEæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ¸…æ™°çš„ç‰¹å¾è§£é‡Šã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œè®¾è®¡äº†ä¸“é—¨çš„åˆ†ç±»å™¨å¤´ï¼Œå¹¶å¼•å…¥äº†æ¿€æ´»ç‡ç¨€ç–æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æå–çš„ç‰¹å¾åœ¨ç¨€ç–æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„SAEæ¶æ„åœ¨ä¸¤ä¸ªåˆ†ç±»åŸºå‡†ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å› æœæ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œç ”ç©¶æˆæœèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­å¢å¼ºä¿¡ä»»åº¦å’Œé€æ˜åº¦ã€‚æœªæ¥ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½ä¼šè¢«å¹¿æ³›åº”ç”¨äºéœ€è¦é«˜å¯è§£é‡Šæ€§çš„AIç³»ç»Ÿä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse Autoencoders (SAEs) have been successfully used to probe Large Language Models (LLMs) and extract interpretable concepts from their internal representations. These concepts are linear combinations of neuron activations that correspond to human-interpretable features. In this paper, we investigate the effectiveness of SAE-based explainability approaches for sentence classification, a domain where such methods have not been extensively explored. We present a novel SAE-based architecture tailored for text classification, leveraging a specialized classifier head and incorporating an activation rate sparsity loss. We benchmark this architecture against established methods such as ConceptShap, Independent Component Analysis, and other SAE-based concept extraction techniques. Our evaluation covers two classification benchmarks and four fine-tuned LLMs from the Pythia family. We further enrich our analysis with two novel metrics for measuring the precision of concept-based explanations, using an external sentence encoder. Our empirical results show that our architecture improves both the causality and interpretability of the extracted features.

