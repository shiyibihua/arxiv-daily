---
layout: default
title: Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning
---

# Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00214" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00214v1</a>
  <a href="https://arxiv.org/pdf/2507.00214.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00214v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00214v1', 'Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mads Henrichsen, Rasmus Krebs

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸¤é˜¶æ®µæ¨ç†å¢å¼ºå­¦ä¹ ä»¥æå‡æ–‡æœ¬åˆ†ç±»æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ†ç±»` `æ¨ç†ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æƒ…æ„Ÿåˆ†æ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è®­ç»ƒ` `æ•°æ®å¢å¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åˆ†ç±»æ¨¡å‹ç¼ºä¹æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½å’Œå¯è§£é‡Šæ€§ä¸è¶³ã€‚
2. æå‡ºçš„ä¸¤é˜¶æ®µæ–¹æ³•é€šè¿‡LLMç”Ÿæˆæ¨ç†ï¼Œå¢å¼ºè®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œæå‡åˆ†ç±»æ•ˆæœã€‚
3. åœ¨æƒ…æ„Ÿåˆ†ç±»å®éªŒä¸­ï¼Œä½¿ç”¨æ–°æ–¹æ³•çš„æ¨¡å‹å‡†ç¡®ç‡æé«˜äº†8.7ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ ‡å‡†åˆ†ç±»æ¨¡å‹é€šå¸¸ç›´æ¥å°†è¾“å…¥æ˜ å°„åˆ°æ ‡ç­¾ï¼Œç¼ºä¹æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯èƒ½é™åˆ¶å…¶æ€§èƒ½ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„æ¨ç†æ¥å¢å¼ºæ–‡æœ¬åˆ†ç±»ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¯¹Llama-3.2-1B-Instructæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆç»™å®šé—®é¢˜åŠå…¶ç­”æ¡ˆçš„æ–‡æœ¬æ¨ç†ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåˆ©ç”¨ç»è¿‡è®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆå¢å¼ºçš„è®­ç»ƒæ•°æ®é›†ï¼Œä»¥è®­ç»ƒä¸‹æ¸¸ç”Ÿæˆæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†8.7ä¸ªç™¾åˆ†ç‚¹çš„å‡†ç¡®ç‡ï¼Œå¼ºè°ƒäº†æ¨ç†ç”Ÿæˆçš„å¼ºæ³›åŒ–èƒ½åŠ›åŠå…¶åœ¨ä¸‹æ¸¸NLPä»»åŠ¡ä¸­çš„æ½œåœ¨ä»·å€¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ ‡å‡†åˆ†ç±»æ¨¡å‹ç¼ºä¹æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ï¼Œè¿™ç§ç¼ºé™·å¯èƒ½å½±å“æ¨¡å‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥å°†è¾“å…¥æ˜ å°„åˆ°æ ‡ç­¾ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ¨ç†ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„ä¸¤é˜¶æ®µæ–¹æ³•é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ¨ç†æ–‡æœ¬ï¼Œå¢å¼ºè®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œæé«˜ä¸‹æ¸¸æ¨¡å‹çš„åˆ†ç±»èƒ½åŠ›ã€‚ç¬¬ä¸€é˜¶æ®µç”Ÿæˆæ¨ç†ï¼Œç¬¬äºŒé˜¶æ®µå°†æ¨ç†ä¸è¾“å…¥ç»“åˆè¿›è¡Œè®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å¯¹Llama-3.2-1B-Instructæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆæ–‡æœ¬æ¨ç†ï¼›ç¬¬äºŒé˜¶æ®µä½¿ç”¨ç”Ÿæˆçš„æ¨ç†åˆ›å»ºå¢å¼ºçš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶è®­ç»ƒä¸‹æ¸¸ç”Ÿæˆæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ¨ç†ç”Ÿæˆä¸æƒ…æ„Ÿåˆ†ç±»ç»“åˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ç›´æ¥æ ‡ç­¾æ˜ å°„å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œå¼ºè°ƒäº†æ¨ç†åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨ç†ç”Ÿæˆå’Œæƒ…æ„Ÿé¢„æµ‹çš„è”åˆè¾“å‡ºï¼Œç¡®ä¿ç”Ÿæˆçš„æ¨ç†ä¸æƒ…æ„Ÿæ ‡ç­¾çš„ç›¸å…³æ€§ã€‚åŒæ—¶ï¼Œä½¿ç”¨äº†å¢å¼ºçš„è®­ç»ƒæ•°æ®é›†ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æ–°æ–¹æ³•è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹åœ¨æƒ…æ„Ÿé¢„æµ‹ä»»åŠ¡ä¸­å‡†ç¡®ç‡æé«˜äº†8.7ä¸ªç™¾åˆ†ç‚¹ï¼Œç›¸è¾ƒäºä»…è¾“å‡ºæƒ…æ„Ÿçš„åŸºçº¿æ¨¡å‹ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™ä¸€ç»“æœçªæ˜¾äº†æ¨ç†ç”Ÿæˆåœ¨å¢å¼ºæ¨¡å‹èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿå’Œå…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡ç”Ÿæˆæ¨ç†ï¼Œæ¨¡å‹ä¸ä»…èƒ½å¤Ÿæä¾›æ›´å‡†ç¡®çš„åˆ†ç±»ç»“æœï¼Œè¿˜èƒ½å¢å¼ºå…¶å¯è§£é‡Šæ€§ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£æ¨¡å‹å†³ç­–çš„ä¾æ®ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šNLPä»»åŠ¡ä¸­æ¨å¹¿åº”ç”¨ï¼Œæå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Standard classification models often map inputs directly to labels without explicit reasoning, potentially limiting their performance, robustness, and interpretability. This paper introduces a novel two-stage approach to enhance text classification by leveraging Large Language Model (LLM)-generated reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model (henceforth Llama-R-Gen) on a general-purpose reasoning dataset (syvai/reasoning-gen) to generate textual reasoning (R) given a question and its answer. In the second stage, this generally trained Llama-R-Gen is used offline to create an augmented training dataset for a downstream generative model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the input text (Q) and is trained to output the generated reasoning (R) immediately followed by the predicted emotion (A). We demonstrate this methodology on the dair-ai/emotion dataset for emotion classification. Our experiments show that the generative model trained to output reasoning and the emotion (Classifier Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy (for emotion prediction) compared to a baseline generative model trained solely to output the emotion (Classifier Q->A), highlighting the strong generalization capabilities of the reasoning generation and the benefit of explicit reasoning training. This work underscores the potential of LLM-generated reasonings for creating richer training datasets, thereby improving the performance of diverse downstream NLP tasks and providing explicit explanations.

