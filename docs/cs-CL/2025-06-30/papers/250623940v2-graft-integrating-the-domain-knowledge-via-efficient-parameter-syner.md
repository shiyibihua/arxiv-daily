---
layout: default
title: Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs
---

# Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23940" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23940v2</a>
  <a href="https://arxiv.org/pdf/2506.23940.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23940v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23940v2', 'Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Dai, Jianxiang An, Tianwei Lin, Hongyang He, Hongzhe Huang, Wenqiao Zhang, Zheqi Lv, Siliang Tang, Yueting Zhuang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30 (æ›´æ–°: 2025-07-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€å‚æ•°é›†æˆæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†ç¢ç‰‡åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `çŸ¥è¯†å…±äº«` `å‚æ•°é›†æˆ` `å…¼å®¹æ€§æ„ŸçŸ¥` `é¢†åŸŸé€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸åŒæ•°æ®è¾“å…¥æ—¶é€‚ç”¨æ€§ä¸‹é™ï¼Œå°¤å…¶æ˜¯é¢†åŸŸç‰¹å®šæ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†å…±äº«ç ”ç©¶è¾ƒå°‘ã€‚
2. æå‡ºäº†ä¸€ç§å…¼å®¹æ€§æ„ŸçŸ¥å‚æ•°æ‹¼æ¥ç­–ç•¥ï¼Œé€šè¿‡å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯å¼•å¯¼å‚æ•°èåˆï¼Œå®ç°é¢†åŸŸçŸ¥è¯†çš„é«˜æ•ˆé›†æˆã€‚
3. åœ¨å¤šé¡¹å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨é¢†åŸŸè‡ªé€‚åº”æ–¹é¢çš„æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œä½†åœ¨é¢å¯¹ä¸åŒç±»å‹çš„æ•°æ®è¾“å…¥æ—¶ï¼Œå…¶é€‚ç”¨æ€§å¾€å¾€ä¸‹é™ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„MLLMsã€‚å°½ç®¡çŸ¥è¯†å…±äº«åœ¨é¢†åŸŸç‰¹å®šçš„MLLMsä¸­è‡³å…³é‡è¦ï¼Œä½†ç›¸å…³ç ”ç©¶ä»ç„¶è¾ƒå°‘ã€‚ä¸ºäº†è§£å†³é¢†åŸŸä¸“ç”¨MLLMsä¹‹é—´çŸ¥è¯†çš„ç¢ç‰‡åŒ–é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å‚æ•°é›†æˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ¨¡å—åŒ–ç»„åˆä¸“å®¶èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åŸºäºä¸€ç§æ–°é¢–çš„å…¼å®¹æ€§æ„ŸçŸ¥å‚æ•°æ‹¼æ¥ï¼ˆCAPSï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨å±€éƒ¨åŠŸèƒ½å½’å› å’Œå…¨å±€ä¿¡æ¯è®ºä¿¡å·æŒ‡å¯¼é€‰æ‹©æ€§å‚æ•°èåˆã€‚é€šè¿‡å°†è¯¥æœºåˆ¶æ‰©å±•åˆ°ä½ç§©é€‚åº”å±‚çš„ç²’åº¦ï¼Œæˆ‘ä»¬ç¡®ä¿äº†é«˜æ•ˆé›†æˆä¸”æœ€å°åŒ–æ¨ç†å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é¢†åŸŸå…¼å®¹æ€§è¯„åˆ†æœºåˆ¶ï¼Œé‡åŒ–æ¿€æ´»çº§åˆ«çš„ä¸“å®¶é—´å¯¹é½ï¼Œå¹¶ä¸ä¸‹æ¸¸ä»»åŠ¡æ•ˆç”¨ç›¸å…³è”ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç»„åˆæ€§ã€é¢†åŸŸè‡ªé€‚åº”çš„MLLMsæä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸åŒæ•°æ®è¾“å…¥ä¸‹çš„é€‚ç”¨æ€§ä¸‹é™é—®é¢˜ï¼Œå°¤å…¶æ˜¯é¢†åŸŸç‰¹å®šæ¨¡å‹ä¹‹é—´çŸ¥è¯†çš„ç¢ç‰‡åŒ–ç°è±¡ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå®ç°é¢†åŸŸçŸ¥è¯†çš„å…±äº«ä¸èåˆï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å‚æ•°é›†æˆæ¡†æ¶ï¼Œåˆ©ç”¨å…¼å®¹æ€§æ„ŸçŸ¥å‚æ•°æ‹¼æ¥ï¼ˆCAPSï¼‰ç­–ç•¥ï¼Œé€šè¿‡å±€éƒ¨åŠŸèƒ½å½’å› å’Œå…¨å±€ä¿¡æ¯è®ºä¿¡å·å®ç°é€‰æ‹©æ€§å‚æ•°èåˆï¼Œä»è€Œé«˜æ•ˆæ•´åˆä¸åŒé¢†åŸŸçš„ä¸“å®¶èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å‚æ•°é›†æˆæ¨¡å—å’Œé¢†åŸŸå…¼å®¹æ€§è¯„åˆ†æœºåˆ¶ã€‚å‚æ•°é›†æˆæ¨¡å—è´Ÿè´£ä¸åŒé¢†åŸŸæ¨¡å‹çš„å‚æ•°èåˆï¼Œè€Œé¢†åŸŸå…¼å®¹æ€§è¯„åˆ†æœºåˆ¶åˆ™é‡åŒ–ä¸“å®¶é—´çš„å¯¹é½ç¨‹åº¦ï¼Œç¡®ä¿æœ€ç»ˆæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†å…¼å®¹æ€§æ„ŸçŸ¥å‚æ•°æ‹¼æ¥ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡ç»“åˆå±€éƒ¨å’Œå…¨å±€ä¿¡æ¯æ¥æŒ‡å¯¼å‚æ•°èåˆï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†çŸ¥è¯†å…±äº«çš„æ•ˆç‡å’Œæ¨¡å‹çš„é€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨ä½ç§©é€‚åº”å±‚çš„è®¾è®¡ä»¥å‡å°‘æ¨ç†å¼€é”€ï¼ŒåŒæ—¶å¼•å…¥é¢†åŸŸå…¼å®¹æ€§è¯„åˆ†æœºåˆ¶ï¼Œç¡®ä¿æ¿€æ´»çº§åˆ«çš„ä¸“å®¶å¯¹é½ä¸ä¸‹æ¸¸ä»»åŠ¡æ•ˆç”¨çš„ç›¸å…³æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šé¡¹å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼Œæå‡ºçš„æ¡†æ¶åœ¨é¢†åŸŸè‡ªé€‚åº”ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†15%çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨çŸ¥è¯†èåˆå’Œæ¨¡å‹é€‚åº”æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç¼–ç¨‹è¾…åŠ©ã€ç§‘å­¦è®¡ç®—ç­‰å¤šä¸ªéœ€è¦å¤šæ¨¡æ€ç†è§£çš„åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆæ•´åˆä¸åŒé¢†åŸŸçš„çŸ¥è¯†ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæå‡å®é™…åº”ç”¨çš„ä»·å€¼å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›æ¨åŠ¨é¢†åŸŸè‡ªé€‚åº”æ¨¡å‹çš„å‘å±•ï¼Œä¿ƒè¿›è·¨é¢†åŸŸçŸ¥è¯†çš„å…±äº«ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have achieved success across various domains. However, their applicability tends to degrade when confronted with different types of data inputs, especially for MLLMs that have been fine-tuned for specific tasks. Despite its importance, the study of knowledge sharing among domain-specific MLLMs--such as those trained for mathematics or code--remains largely underexplored. To address the fragmentation of knowledge across domain-specialized MLLMs, we propose a unified parameter integration framework that enables modular composition of expert capabilities. Our method is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy, which leverages both local functional attribution and global information-theoretic signals to guide selective parameter fusion. By extending this mechanism to the low-rank adaptation layer granularity, we ensure efficient integration with minimal inference overhead. Furthermore, we introduce a domain compatibility scoring mechanism that quantifies inter-expert alignment at the activation level and correlates with downstream task utility. This principled fusion protocol allows the final model to synergize heterogeneous expertise while preserving structural modularity. Extensive evaluations across diverse multimodal benchmarks validate the effectiveness of our framework, offering a scalable path toward compositional, domain-adaptive MLLMs.

