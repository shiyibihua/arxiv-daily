---
layout: default
title: EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations
---

# EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.24016" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.24016v1</a>
  <a href="https://arxiv.org/pdf/2506.24016.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.24016v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.24016v1', 'EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hyunjong Kim, Sangyeop Kim, Jongheon Jeong, Yeongjae Cho, Sungzoon Cho

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: Accepted at ACL 2025 Findings

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hjkim811/EXPERT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEXPERTä»¥è§£å†³å›¾åƒæè¿°è¯„ä¼°æ ‡å‡†åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒæè¿°` `å¯è§£é‡Šæ€§è¯„ä¼°` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ç»“æ„åŒ–è§£é‡Š` `æ— å‚è€ƒè¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾åƒæè¿°è¯„ä¼°æŒ‡æ ‡ç¼ºä¹æ ‡å‡†åŒ–çš„è§£é‡Šç”Ÿæˆæ ‡å‡†ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§£é‡Šè´¨é‡ä¸ä¸€ã€‚
2. æœ¬æ–‡æå‡ºEXPERTï¼Œé€šè¿‡æµç•…æ€§ã€ç›¸å…³æ€§å’Œæè¿°æ€§ä¸‰ä¸ªæ ‡å‡†æä¾›ç»“æ„åŒ–è§£é‡Šï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. EXPERTåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨è§£é‡Šè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰è¯„ä¼°æŒ‡æ ‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹çš„è¿›å±•ï¼Œå›¾åƒæè¿°çš„å¯è§£é‡Šè¯„ä¼°æŒ‡æ ‡å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æŒ‡æ ‡ç”Ÿæˆçš„è§£é‡Šç¼ºä¹æ ‡å‡†åŒ–çš„æ ‡å‡†ï¼Œä¸”ç”Ÿæˆè§£é‡Šçš„æ•´ä½“è´¨é‡å°šæœªå¾—åˆ°éªŒè¯ã€‚æœ¬æ–‡æå‡ºäº†EXPERTï¼Œè¿™æ˜¯ä¸€ç§æ— å‚è€ƒçš„è¯„ä¼°æŒ‡æ ‡ï¼ŒåŸºäºæµç•…æ€§ã€ç›¸å…³æ€§å’Œæè¿°æ€§ä¸‰ä¸ªåŸºæœ¬æ ‡å‡†æä¾›ç»“æ„åŒ–è§£é‡Šã€‚é€šè¿‡æ„å»ºé«˜è´¨é‡ç»“æ„åŒ–è§£é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸¤é˜¶æ®µè¯„ä¼°æ¨¡æ¿ï¼Œæœ‰æ•ˆç›‘ç£è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„åˆ†å’Œè§£é‡Šç”Ÿæˆã€‚EXPERTåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶é€šè¿‡å…¨é¢çš„äººç±»è¯„ä¼°éªŒè¯äº†å…¶ç”Ÿæˆçš„è§£é‡Šè´¨é‡æ˜¾è‘—é«˜äºç°æœ‰æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨https://github.com/hjkim811/EXPERTè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å›¾åƒæè¿°è¯„ä¼°ä¸­ç¼ºä¹æ ‡å‡†åŒ–è§£é‡Šç”Ÿæˆçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æœªèƒ½æä¾›ä¸€è‡´å’Œé«˜è´¨é‡çš„è§£é‡Šï¼Œå½±å“äº†è¯„ä¼°çš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEXPERTçš„æ ¸å¿ƒæ€è·¯æ˜¯åŸºäºæµç•…æ€§ã€ç›¸å…³æ€§å’Œæè¿°æ€§ä¸‰ä¸ªåŸºæœ¬æ ‡å‡†æ„å»ºç»“æ„åŒ–è§£é‡Šï¼Œç¡®ä¿è¯„ä¼°çš„å®¢è§‚æ€§å’Œä¸€è‡´æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒEXPERTèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°å›¾åƒæè¿°çš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEXPERTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œæ„å»ºé«˜è´¨é‡ç»“æ„åŒ–è§£é‡Šçš„æ•°æ®é›†ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨è¯¥æ•°æ®é›†ç›‘ç£è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„åˆ†å’Œè§£é‡Šç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šEXPERTçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æ— å‚è€ƒè¯„ä¼°æœºåˆ¶å’Œç»“æ„åŒ–è§£é‡Šç”Ÿæˆï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•ä¾èµ–äºå‚è€ƒæè¿°çš„è¯„ä¼°æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒEXPERTé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹çš„è¯„åˆ†å’Œè§£é‡Šç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†çš„æ„å»ºç¡®ä¿äº†è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒEXPERTåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œç”Ÿæˆçš„è§£é‡Šè´¨é‡æ˜¾è‘—é«˜äºç°æœ‰è¯„ä¼°æŒ‡æ ‡ã€‚å…·ä½“è€Œè¨€ï¼ŒEXPERTåœ¨æµç•…æ€§ã€ç›¸å…³æ€§å’Œæè¿°æ€§æ–¹é¢çš„è¯„åˆ†å‡è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EXPERTçš„ç ”ç©¶æˆæœåœ¨å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”å’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„è¯„ä¼°æ ‡å‡†ï¼ŒEXPERTèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´å¥½åœ°ç†è§£å’Œæ”¹è¿›è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models and vision-language models have led to growing interest in explainable evaluation metrics for image captioning. However, these metrics generate explanations without standardized criteria, and the overall quality of the generated explanations remains unverified. In this paper, we propose EXPERT, a reference-free evaluation metric that provides structured explanations based on three fundamental criteria: fluency, relevance, and descriptiveness. By constructing large-scale datasets of high-quality structured explanations, we develop a two-stage evaluation template to effectively supervise a vision-language model for both scoring and explanation generation. EXPERT achieves state-of-the-art results on benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation. Our code and datasets are available at https://github.com/hjkim811/EXPERT.

