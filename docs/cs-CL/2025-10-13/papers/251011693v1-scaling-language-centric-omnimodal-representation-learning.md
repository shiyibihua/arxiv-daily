---
layout: default
title: Scaling Language-Centric Omnimodal Representation Learning
---

# Scaling Language-Centric Omnimodal Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11693" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11693v1</a>
  <a href="https://arxiv.org/pdf/2510.11693.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11693v1" onclick="toggleFavorite(this, '2510.11693v1', 'Scaling Language-Centric Omnimodal Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

**å¤‡æ³¨**: NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/LCO-Embedding/LCO-Embedding)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLCO-Embæ¡†æ¶ï¼Œé€šè¿‡è¯­è¨€ä¸­å¿ƒçš„å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ ï¼Œæå‡è·¨æ¨¡æ€æ£€ç´¢æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è·¨æ¨¡æ€æ£€ç´¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `è¡¨å¾å­¦ä¹ ` `ç”Ÿæˆå¼é¢„è®­ç»ƒ` `è§†è§‰-è¯­è¨€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºMLLMçš„å¤šæ¨¡æ€åµŒå…¥æ–¹æ³•ç¼ºä¹å¯¹å…¶ä¼˜è¶Šæ€§çš„æ·±å…¥ç†è§£ï¼Œéœ€è¦æ¢ç©¶å…¶å†…åœ¨æœºåˆ¶ã€‚
2. è®ºæ–‡æå‡ºLCO-Embæ¡†æ¶ï¼Œåˆ©ç”¨MLLMç”Ÿæˆå¼é¢„è®­ç»ƒä¸­çš„éšå¼è·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLCO-Embåœ¨å¤šç§æ¨¡æ€ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶éªŒè¯äº†ç”Ÿæˆèƒ½åŠ›ä¸è¡¨ç¤ºèƒ½åŠ›ä¹‹é—´çš„ç¼©æ”¾å®šå¾‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨æ¢ç´¢åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰è¿›è¡Œå¾®è°ƒçš„å¤šæ¨¡æ€åµŒå…¥æ–¹æ³•çš„ä¼˜åŠ¿ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMLLMæ–¹æ³•çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿åœ¨äºç”Ÿæˆå¼é¢„è®­ç»ƒæœŸé—´å®ç°çš„éšå¼è·¨æ¨¡æ€å¯¹é½ï¼Œå…¶ä¸­è¯­è¨€è§£ç å™¨å­¦ä¹ åˆ©ç”¨å…±äº«è¡¨ç¤ºç©ºé—´å†…çš„å¤šæ¨¡æ€ä¿¡å·æ¥ç”Ÿæˆå•æ¨¡æ€è¾“å‡ºã€‚é€šè¿‡å¯¹å„å‘å¼‚æ€§å’Œæ ¸ç›¸ä¼¼æ€§ç»“æ„çš„åˆ†æï¼Œè¯å®äº†MLLMè¡¨ç¤ºä¸­æ½œåœ¨å¯¹é½çš„å‡ºç°ï¼Œä½¿å¾—CLèƒ½å¤Ÿä½œä¸ºè½»é‡çº§çš„ä¼˜åŒ–é˜¶æ®µã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„å…¨æ¨¡æ€åµŒå…¥æ¡†æ¶ï¼Œç§°ä¸ºLCO-Embã€‚åœ¨ä¸åŒçš„éª¨å¹²ç½‘ç»œå’ŒåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¤§é‡å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å„ç§æ¨¡æ€ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†ä¸€ä¸ªç”Ÿæˆ-è¡¨ç¤ºç¼©æ”¾å®šå¾‹ï¼ˆGRSLï¼‰ï¼Œè¡¨æ˜é€šè¿‡å¯¹æ¯”ä¼˜åŒ–è·å¾—çš„è¡¨ç¤ºèƒ½åŠ›ä¸MLLMçš„ç”Ÿæˆèƒ½åŠ›å‘ˆæ­£ç›¸å…³ã€‚è¿™è¡¨æ˜ï¼Œæé«˜ç”Ÿæˆèƒ½åŠ›æ˜¯å¢å¼ºè¡¨ç¤ºè´¨é‡çš„æœ‰æ•ˆèŒƒä¾‹ã€‚æˆ‘ä»¬å¯¹GRSLè¿›è¡Œäº†ç†è®ºè§£é‡Šï¼Œå°†MLLMçš„ç”Ÿæˆè´¨é‡ä¸å…¶è¡¨ç¤ºæ€§èƒ½çš„ä¸Šé™æ­£å¼è”ç³»èµ·æ¥ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä½èµ„æºè§†è§‰-æ–‡æ¡£æ£€ç´¢ä»»åŠ¡ä¸­éªŒè¯äº†å®ƒï¼Œè¡¨æ˜åœ¨CLä¹‹å‰è¿›è¡ŒæŒç»­çš„ç”Ÿæˆå¼é¢„è®­ç»ƒå¯ä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹åµŒå…¥èƒ½åŠ›çš„æ½œåŠ›ã€‚ä»£ç ã€æ¨¡å‹å’Œèµ„æºå¯åœ¨https://github.com/LCO-Embedding/LCO-Embeddingè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆå­¦ä¹ å¤šæ¨¡æ€æ•°æ®çš„ç»Ÿä¸€è¡¨å¾ï¼Œä»è€Œæå‡è·¨æ¨¡æ€æ£€ç´¢ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ï¼Œè™½ç„¶å–å¾—äº†ä¸€å®šçš„è¿›å±•ï¼Œä½†ç¼ºä¹å¯¹MLLMåœ¨å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ ä¸­ä½œç”¨çš„æ·±å…¥ç†è§£ï¼Œä»¥åŠå¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨MLLMçš„ç”Ÿæˆèƒ½åŠ›æ¥æå‡è¡¨å¾è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼ŒMLLMåœ¨ç”Ÿæˆå¼é¢„è®­ç»ƒé˜¶æ®µå·²ç»å­¦ä¹ åˆ°äº†éšå¼çš„è·¨æ¨¡æ€å¯¹é½ï¼Œè¿™ç§å¯¹é½ä½¿å¾—è¯­è¨€è§£ç å™¨èƒ½å¤Ÿåˆ©ç”¨å¤šæ¨¡æ€ä¿¡å·ç”Ÿæˆå•æ¨¡æ€è¾“å‡ºã€‚å› æ­¤ï¼Œå¯ä»¥é€šè¿‡å¯¹æ¯”å­¦ä¹ å¯¹MLLMçš„è¡¨å¾è¿›è¡Œå¾®è°ƒï¼Œä»è€Œè¿›ä¸€æ­¥æå‡è¡¨å¾çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ç”Ÿæˆ-è¡¨ç¤ºç¼©æ”¾å®šå¾‹ï¼ˆGRSLï¼‰ï¼Œè®¤ä¸ºMLLMçš„ç”Ÿæˆèƒ½åŠ›è¶Šå¼ºï¼Œå…¶è¡¨å¾èƒ½åŠ›ä¹Ÿè¶Šå¼ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLCO-Embæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š1) åˆ©ç”¨MLLMè¿›è¡Œç”Ÿæˆå¼é¢„è®­ç»ƒï¼Œå­¦ä¹ è·¨æ¨¡æ€çš„éšå¼å¯¹é½ï¼›2) ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å¯¹MLLMçš„è¡¨å¾è¿›è¡Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥æå‡è¡¨å¾çš„è´¨é‡ã€‚æ¡†æ¶å¯ä»¥é‡‡ç”¨ä¸åŒçš„MLLMä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œä¾‹å¦‚BLIP-2ã€InstructBLIPç­‰ã€‚åœ¨å¯¹æ¯”å­¦ä¹ é˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚InfoNCEã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†LCO-Embæ¡†æ¶ï¼Œå¹¶æ­ç¤ºäº†MLLMåœ¨å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ ä¸­çš„ä½œç”¨ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLCO-Embæ›´åŠ æ³¨é‡åˆ©ç”¨MLLMçš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ç”Ÿæˆ-è¡¨ç¤ºç¼©æ”¾å®šå¾‹ï¼ˆGRSLï¼‰ï¼Œä¸ºå¤šæ¨¡æ€è¡¨å¾å­¦ä¹ æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç”Ÿæˆå¼é¢„è®­ç»ƒé˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†å’Œè®­ç»ƒç­–ç•¥ã€‚åœ¨å¯¹æ¯”å­¦ä¹ é˜¶æ®µï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„å¯¹æ¯”æŸå¤±å‡½æ•°å’Œè´Ÿæ ·æœ¬é‡‡æ ·ç­–ç•¥ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†InfoNCEæŸå¤±å‡½æ•°ï¼Œå¹¶é‡‡ç”¨hard negative miningç­–ç•¥ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹MLLMçš„ç»“æ„è¿›è¡Œäº†ä¸€äº›è°ƒæ•´ï¼Œä¾‹å¦‚æ·»åŠ äº†é¢å¤–çš„çº¿æ€§å±‚æ¥æ˜ å°„ä¸åŒæ¨¡æ€çš„ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LCO-Embåœ¨å¤šä¸ªè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰-æ–‡æ¡£æ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒLCO-Embç›¸æ¯”äºç°æœ‰æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†ç”Ÿæˆ-è¡¨ç¤ºç¼©æ”¾å®šå¾‹ï¼ˆGRSLï¼‰ï¼Œè¡¨æ˜é€šè¿‡æŒç»­çš„ç”Ÿæˆå¼é¢„è®­ç»ƒå¯ä»¥è¿›ä¸€æ­¥æå‡æ¨¡å‹åµŒå…¥èƒ½åŠ›çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè·¨æ¨¡æ€ä¿¡æ¯æ£€ç´¢ã€è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤šæ¨¡æ€è¡¨å¾çš„è´¨é‡ï¼Œå¯ä»¥æ”¹å–„ç”¨æˆ·åœ¨ä¸åŒæ¨¡æ€æ•°æ®ä¹‹é—´è¿›è¡Œä¿¡æ¯äº¤äº’çš„ä½“éªŒï¼Œä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ–‡æœ¬æŸ¥è¯¢æ£€ç´¢ç›¸å…³çš„å›¾åƒæˆ–è§†é¢‘ï¼Œæˆ–è€…é€šè¿‡å›¾åƒæŸ¥è¯¢æ£€ç´¢ç›¸å…³çš„æ–‡æ¡£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approaches stems from implicit cross-modal alignment achieved during generative pretraining, where the language decoder learns to exploit multimodal signals within a shared representation space for generating unimodal outputs. Through analysis of anisotropy and kernel similarity structure, we empirically confirm that latent alignment emerges within MLLM representations, allowing CL to serve as a lightweight refinement stage. Leveraging this insight, we propose a Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive experiments across diverse backbones and benchmarks demonstrate its effectiveness, achieving state-of-the-art performance across modalities. Furthermore, we identify a Generation-Representation Scaling Law (GRSL), showing that the representational capabilities gained through contrastive refinement scales positively with the MLLM's generative capabilities. This suggests that improving generative abilities evolves as an effective paradigm for enhancing representation quality. We provide a theoretical explanation of GRSL, which formally links the MLLM's generative quality to the upper bound on its representation performance, and validate it on a challenging, low-resource visual-document retrieval task, showing that continual generative pretraining before CL can further enhance the potential of a model's embedding capabilities. Codes, models, and resources are available at https://github.com/LCO-Embedding/LCO-Embedding.

