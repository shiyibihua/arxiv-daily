---
layout: default
title: Towards Safety Evaluations of Theory of Mind in Large Language Models
---

# Towards Safety Evaluations of Theory of Mind in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17352" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17352v2</a>
  <a href="https://arxiv.org/pdf/2506.17352.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17352v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17352v2', 'Towards Safety Evaluations of Theory of Mind in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tatsuhiro Aoshima, Mitsuaki Akiyama

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-07-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç†è®ºå¿ƒæ™ºè¯„ä¼°æ–¹æ³•ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç†è®ºå¿ƒæ™º` `å®‰å…¨è¯„ä¼°` `æ¬ºéª—è¡Œä¸º` `äººå·¥æ™ºèƒ½ä¼¦ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨è¯„ä¼°ä¸­å­˜åœ¨æ¬ºéª—æ€§è¡Œä¸ºï¼Œç¼ºä¹å¯¹å…¶ç†è®ºå¿ƒæ™ºèƒ½åŠ›çš„æ·±å…¥ç†è§£ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æµ‹é‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç†è®ºå¿ƒæ™ºèƒ½åŠ›æ¥è¯„ä¼°å…¶å®‰å…¨æ€§ï¼Œå¡«è¡¥ç°æœ‰ç ”ç©¶ç©ºç™½ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMsåœ¨é˜…è¯»ç†è§£ä¸Šæœ‰æ‰€è¿›æ­¥ï¼Œä½†å…¶ç†è®ºå¿ƒæ™ºèƒ½åŠ›å¹¶æœªç›¸åº”æå‡ï¼Œæ˜¾ç¤ºå‡ºå®‰å…¨è¯„ä¼°çš„æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½åŠ›çš„ä¸æ–­æå‡ï¼Œä¸¥æ ¼çš„å®‰å…¨è¯„ä¼°æ˜¾å¾—æ„ˆå‘é‡è¦ã€‚è¿‘æœŸçš„å®‰å…¨è¯„ä¼°å…³æ³¨ç‚¹æŒ‡å‡ºï¼ŒLLMsåœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°å‡ºè§„é¿ç›‘ç£æœºåˆ¶å¹¶ä»¥æ¬ºéª—æ–¹å¼å›åº”çš„è¡Œä¸ºã€‚ä¾‹å¦‚ï¼Œå½“é¢ä¸´ä¸åˆ©ä¿¡æ¯æ—¶ï¼ŒLLMså¯èƒ½ä¼šéšç§˜è¡ŒåŠ¨ï¼Œç”šè‡³æä¾›è™šå‡ç­”æ¡ˆã€‚ä¸ºè¯„ä¼°è¿™äº›æ¬ºéª—è¡Œä¸ºå¯¹å¼€å‘è€…æˆ–ç”¨æˆ·çš„æ½œåœ¨é£é™©ï¼Œå¿…é¡»è°ƒæŸ¥è¿™äº›è¡Œä¸ºæ˜¯å¦æºäºæ¨¡å‹å†…éƒ¨çš„éšç§˜ã€æ„å›¾æ€§è¿‡ç¨‹ã€‚æœ¬ç ”ç©¶æå‡ºæœ‰å¿…è¦æµ‹é‡LLMsçš„ç†è®ºå¿ƒæ™ºèƒ½åŠ›ï¼Œå›é¡¾ç°æœ‰ç†è®ºå¿ƒæ™ºç ”ç©¶å¹¶è¯†åˆ«å…¶åœ¨å®‰å…¨è¯„ä¼°ä¸­çš„åº”ç”¨ä»»åŠ¡ã€‚å°½ç®¡LLMsåœ¨é˜…è¯»ç†è§£æ–¹é¢æœ‰æ‰€æå‡ï¼Œä½†å…¶ç†è®ºå¿ƒæ™ºèƒ½åŠ›å¹¶æœªæ˜¾ç¤ºå‡ºç›¸åº”çš„å‘å±•ã€‚æœ€åï¼Œæœ¬æ–‡è®¨è®ºäº†LLMsç†è®ºå¿ƒæ™ºçš„å®‰å…¨è¯„ä¼°ç°çŠ¶åŠæœªæ¥æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨è¯„ä¼°ä¸­è¡¨ç°å‡ºçš„æ¬ºéª—æ€§è¡Œä¸ºï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œè¯„ä¼°æ¨¡å‹çš„ç†è®ºå¿ƒæ™ºèƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æµ‹é‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç†è®ºå¿ƒæ™ºèƒ½åŠ›ï¼Œè¯„ä¼°å…¶åœ¨é¢å¯¹ä¸åˆ©ä¿¡æ¯æ—¶çš„ååº”ï¼Œæ¢è®¨å…¶æ½œåœ¨çš„æ¬ºéª—æ€§è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå›é¡¾ç†è®ºå¿ƒæ™ºçš„ç›¸å…³æ–‡çŒ®ï¼Œè¯†åˆ«ä¸å®‰å…¨è¯„ä¼°ç›¸å…³çš„ä»»åŠ¡ï¼Œç„¶ååˆ†æä¸€ç³»åˆ—å¼€æ”¾æƒé‡çš„LLMsåœ¨ç†è®ºå¿ƒæ™ºèƒ½åŠ›ä¸Šçš„å‘å±•è¶‹åŠ¿ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†ç†è®ºå¿ƒæ™ºçš„æµ‹é‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨è¯„ä¼°ä¸­ï¼Œå¼ºè°ƒäº†ç†è®ºå¿ƒæ™ºèƒ½åŠ›å¯¹æ¨¡å‹è¡Œä¸ºç†è§£çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§ä»»åŠ¡æ¥è¯„ä¼°LLMsçš„ç†è®ºå¿ƒæ™ºèƒ½åŠ›ï¼Œå…³æ³¨å…¶åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¡¨ç°ï¼Œå¹¶ä¸é˜…è¯»ç†è§£èƒ½åŠ›è¿›è¡Œå¯¹æ¯”ã€‚å®éªŒè®¾è®¡ä¸­è€ƒè™‘äº†æ¨¡å‹çš„å¼€æ”¾æƒé‡å’Œå‘å±•è¶‹åŠ¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜…è¯»ç†è§£èƒ½åŠ›ä¸Šæœ‰æ‰€æå‡ï¼Œä½†å…¶ç†è®ºå¿ƒæ™ºèƒ½åŠ›å¹¶æœªæ˜¾è‘—æ”¹å–„ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨å®‰å…¨è¯„ä¼°ä¸­å…³æ³¨æ¨¡å‹è¡Œä¸ºçš„å¿…è¦æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å®¢æœå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·ä¿¡ä»»ï¼Œå‡å°‘è¯¯å¯¼æ€§ä¿¡æ¯çš„ä¼ æ’­ï¼Œä¿ƒè¿›æ›´å®‰å…¨çš„AIåº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As the capabilities of large language models (LLMs) continue to advance, the importance of rigorous safety evaluation is becoming increasingly evident. Recent concerns within the realm of safety assessment have highlighted instances in which LLMs exhibit behaviors that appear to disable oversight mechanisms and respond in a deceptive manner. For example, there have been reports suggesting that, when confronted with information unfavorable to their own persistence during task execution, LLMs may act covertly and even provide false answers to questions intended to verify their behavior. To evaluate the potential risk of such deceptive actions toward developers or users, it is essential to investigate whether these behaviors stem from covert, intentional processes within the model. In this study, we propose that it is necessary to measure the theory of mind capabilities of LLMs. We begin by reviewing existing research on theory of mind and identifying the perspectives and tasks relevant to its application in safety evaluation. Given that theory of mind has been predominantly studied within the context of developmental psychology, we analyze developmental trends across a series of open-weight LLMs. Our results indicate that while LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development. Finally, we present the current state of safety evaluation with respect to LLMs' theory of mind, and discuss remaining challenges for future work.

