---
layout: default
title: Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation
---

# Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17088" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17088v3</a>
  <a href="https://arxiv.org/pdf/2506.17088.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17088v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17088v3', 'Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-09-16)

**å¤‡æ³¨**: Accepted at EMNLP 2025 Findings

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ECNU-Text-Computing/cot-hallu-detect)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé“¾å¼æ€ç»´æç¤ºä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¹»è§‰æ£€æµ‹` `é“¾å¼æ€ç»´` `é€æ­¥æ¨ç†` `å®è¯è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶å¸¸å¸¸å‡ºç°å¹»è§‰ï¼Œå¯¼è‡´è¾“å‡ºä¸å‡†ç¡®çš„ä¿¡æ¯ï¼Œå½±å“å®é™…åº”ç”¨æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡é“¾å¼æ€ç»´æç¤ºï¼ˆCoTï¼‰æ¥æ”¹å–„å¹»è§‰ç°è±¡ï¼ŒåŒæ—¶ç³»ç»Ÿè¯„ä¼°å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoTæç¤ºèƒ½å¤Ÿé™ä½å¹»è§‰é¢‘ç‡ï¼Œä½†ä¹Ÿä¼šå½±å“æ£€æµ‹ä¿¡å·çš„æ¸…æ™°åº¦ï¼Œé™ä½æ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å¸¸å‡ºç°å¹»è§‰ç°è±¡ï¼Œå³ç”Ÿæˆäº‹å®ä¸å‡†ç¡®æˆ–è¯­ä¹‰ä¸ç›¸å…³çš„å†…å®¹ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥é€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥å‡è½»å¹»è§‰ï¼Œä½†å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“å°šæœªæ·±å…¥æ¢è®¨ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯è¯„ä¼°ã€‚é€šè¿‡åˆæ­¥å®éªŒå‘ç°ï¼ŒCoTæ¨ç†æ˜¾è‘—å½±å“LLMçš„å†…éƒ¨çŠ¶æ€å’Œæ ‡è®°æ¦‚ç‡åˆ†å¸ƒã€‚è¿›ä¸€æ­¥è¯„ä¼°äº†ä¸åŒCoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„å½±å“ï¼Œç ”ç©¶äº†å¹»è§‰è¯„åˆ†åˆ†å¸ƒçš„å˜åŒ–ã€æ£€æµ‹å‡†ç¡®ç‡çš„å˜åŒ–ä»¥åŠæ£€æµ‹ä¿¡å¿ƒçš„å˜åŒ–ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡CoTæç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†ä¹Ÿä¼šæ¨¡ç³Šæ£€æµ‹æ‰€éœ€çš„å…³é”®ä¿¡å·ï¼Œä»è€Œå‰Šå¼±å„ç§æ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†æ¨ç†ä½¿ç”¨ä¸­çš„ä¸€ä¸ªè¢«å¿½è§†çš„æƒè¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶å‡ºç°çš„å¹»è§‰ç°è±¡ï¼Œç°æœ‰æ–¹æ³•åœ¨å¹»è§‰æ£€æµ‹æ–¹é¢å­˜åœ¨ä¿¡å·æ¨¡ç³Šçš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥é“¾å¼æ€ç»´æç¤ºï¼ˆCoTï¼‰ï¼Œé¼“åŠ±æ¨¡å‹è¿›è¡Œé€æ­¥æ¨ç†ï¼Œä»è€Œæ”¹å–„ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶è¯„ä¼°å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆè¿›è¡Œåˆæ­¥å®éªŒï¼Œåˆ†æCoTæ¨ç†å¯¹æ¨¡å‹å†…éƒ¨çŠ¶æ€çš„å½±å“ï¼Œéšåè¯„ä¼°ä¸åŒCoTæç¤ºæ–¹æ³•å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“ï¼Œæ¶µç›–å¹»è§‰è¯„åˆ†åˆ†å¸ƒã€æ£€æµ‹å‡†ç¡®ç‡å’Œæ£€æµ‹ä¿¡å¿ƒç­‰å¤šä¸ªç»´åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†CoTæç¤ºåœ¨é™ä½å¹»è§‰é¢‘ç‡çš„åŒæ—¶ï¼Œå¯èƒ½ä¼šæ¨¡ç³Šå¹»è§‰æ£€æµ‹çš„å…³é”®ä¿¡å·ï¼Œè¿™ä¸€æƒè¡¡åœ¨ä»¥å¾€ç ”ç©¶ä¸­æœªè¢«å……åˆ†æ¢è®¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†å¤šç§CoTæç¤ºæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ä¸»æµçš„å¹»è§‰æ£€æµ‹æŠ€æœ¯è¿›è¡Œå¯¹æ¯”ï¼Œå…³æ³¨å‚æ•°è®¾ç½®å’Œæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨ä»£ç ä¸­å…¬å¼€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé“¾å¼æ€ç»´æç¤ºèƒ½å¤Ÿæ˜¾è‘—é™ä½å¹»è§‰é¢‘ç‡ï¼Œä½†åŒæ—¶ä¹Ÿå¯¼è‡´å¹»è§‰æ£€æµ‹çš„ä¿¡å·æ¨¡ç³Šï¼Œå½±å“æ£€æµ‹å‡†ç¡®æ€§ã€‚å…·ä½“æ•°æ®æ˜¾ç¤ºï¼Œå°½ç®¡å¹»è§‰é¢‘ç‡é™ä½ï¼Œä½†æ£€æµ‹å‡†ç¡®ç‡æœ‰æ‰€ä¸‹é™ï¼Œæ­ç¤ºäº†ä½¿ç”¨æ¨ç†çš„æ½œåœ¨æƒè¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æ”¹è¿›å¹»è§‰æ£€æµ‹ï¼Œèƒ½å¤Ÿæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM's internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: https://github.com/ECNU-Text-Computing/cot-hallu-detect .

