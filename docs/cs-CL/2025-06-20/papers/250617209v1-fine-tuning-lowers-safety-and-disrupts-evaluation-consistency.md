---
layout: default
title: Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency
---

# Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17209" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17209v1</a>
  <a href="https://arxiv.org/pdf/2506.17209.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17209v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17209v1', 'Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kathleen C. Fraser, Hillary Dawkins, Isar Nejadgholi, Svetlana Kiritchenko

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20

**å¤‡æ³¨**: to appear at LLMSEC 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨å¾®è°ƒå¯¹å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„å½±å“åŠè¯„ä¼°ä¸€è‡´æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `å®‰å…¨æ€§è¯„ä¼°` `å®éªŒä¸€è‡´æ€§` `æ¨¡å‹å®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå®‰å…¨æ€§å¯¹é½ç‰¹æ€§å¯èƒ½ä¼šè¢«å‰Šå¼±ï¼Œå¯¼è‡´æ½œåœ¨çš„å®‰å…¨éšæ‚£ã€‚
2. è®ºæ–‡é€šè¿‡ç ”ç©¶å®‰å…¨è¯„ä¼°çš„å¯é æ€§ï¼Œæ­ç¤ºå¾®è°ƒè¿‡ç¨‹ä¸­çš„ç»†å¾®å˜åŒ–å¯¹è¯„ä¼°ç»“æœçš„å½±å“ã€‚
3. åˆæ­¥å®éªŒæ˜¾ç¤ºï¼Œå¾®è°ƒè®¾ç½®çš„ç»†å¾®è°ƒæ•´ä¼šå¯¼è‡´å®‰å…¨è¯„ä¼°ç»“æœçš„æ˜¾è‘—æ³¢åŠ¨ï¼Œå½±å“ç»“æœçš„ä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥é€‚åº”ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡å·²æˆä¸ºæ™®é€šç”¨æˆ·çš„å¸¸è§„æ“ä½œã€‚ç„¶è€Œï¼Œå¾®è°ƒè¢«è®¤ä¸ºä¼šå‰Šå¼±æ¨¡å‹çš„å®‰å…¨å¯¹é½ç‰¹æ€§ï¼Œå³ä½¿å¾®è°ƒæ•°æ®ä¸åŒ…å«ä»»ä½•æœ‰å®³å†…å®¹ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯LLMçš„ä¸€ä¸ªå…³é”®å¤±æ•ˆæ¨¡å¼ï¼Œå°¤å…¶æ˜¯åœ¨å¾®è°ƒå¹¿æ³›åº”ç”¨çš„èƒŒæ™¯ä¸‹ã€‚å¤§å¤šæ•°å–„æ„çš„å¼€å‘è€…å¯èƒ½å¹¶ä¸çŸ¥é“ä»–ä»¬éƒ¨ç½²çš„LLMå®‰å…¨æ€§é™ä½ã€‚å¦ä¸€æ–¹é¢ï¼Œè¿™ä¸€å·²çŸ¥æ¼æ´å¯èƒ½è¢«æ¶æ„è¡Œä¸ºè€…åˆ©ç”¨ï¼Œä»¥ç»•è¿‡å®‰å…¨é˜²æŠ¤ã€‚ä¸ºäº†æœ‰æ•ˆç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å¯é ä¸”å¯é‡å¤çš„å®‰å…¨è¯„ä¼°ã€‚æœ¬æ–‡ç ”ç©¶äº†å®‰å…¨åŸºå‡†å¯¹å®éªŒç¨‹åºå¾®å°å˜åŒ–å’ŒLLMéšæœºæ€§çš„ä¸æ•æ„Ÿæ€§ï¼Œåˆæ­¥å®éªŒæ­ç¤ºäº†å®‰å…¨è¯„ä¼°ç»“æœçš„æ˜¾è‘—å·®å¼‚ï¼Œå³ä½¿åœ¨å¾®è°ƒè®¾ç½®ä¸Šåšå‡ºçœ‹ä¼¼æ— å…³çš„æ›´æ”¹æ—¶ã€‚è¿™äº›è§‚å¯Ÿå¯¹è¯¥é¢†åŸŸç ”ç©¶è€…æŠ¥å‘Šç»“æœçš„æ–¹å¼å…·æœ‰é‡è¦å½±å“ï¼Œä»¥ä¾¿æœªæ¥èƒ½å¤Ÿè¿›è¡Œæœ‰æ„ä¹‰çš„æ¯”è¾ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹åå®‰å…¨æ€§é™ä½çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘å¾®è°ƒå¯¹å®‰å…¨è¯„ä¼°ä¸€è‡´æ€§çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿæ€§åœ°åˆ†æå¾®è°ƒè¿‡ç¨‹ä¸­çš„ç»†å¾®å˜åŒ–å¦‚ä½•å½±å“å®‰å…¨è¯„ä¼°ç»“æœï¼Œæå‡ºæ”¹è¿›è¯„ä¼°æ–¹æ³•çš„å¿…è¦æ€§ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯é æ€§å’Œå¯é‡å¤æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å®éªŒè®¾è®¡æ–¹æ³•ï¼Œè®¾ç½®ä¸åŒçš„å¾®è°ƒå‚æ•°å’Œæ¡ä»¶ï¼Œè¯„ä¼°å…¶å¯¹å®‰å…¨æ€§è¯„ä¼°ç»“æœçš„å½±å“ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬å®éªŒè®¾è®¡ã€æ•°æ®æ”¶é›†å’Œç»“æœåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæ­ç¤ºäº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç»†å¾®å˜åŒ–å¯¹å®‰å…¨è¯„ä¼°ç»“æœçš„æ˜¾è‘—å½±å“ï¼Œå¼ºè°ƒäº†å®‰å…¨è¯„ä¼°æ–¹æ³•çš„æ”¹è¿›éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†å¤šç§å¾®è°ƒå‚æ•°å’Œæ¡ä»¶ï¼Œé‡‡ç”¨ç»Ÿè®¡åˆ†ææ–¹æ³•è¯„ä¼°ç»“æœæ³¢åŠ¨ï¼Œç¡®ä¿å®éªŒçš„ç³»ç»Ÿæ€§å’Œå¯é‡å¤æ€§ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒè®¾ç½®ä¸‹çš„è¯„ä¼°ç»“æœï¼Œæ­ç¤ºäº†å®‰å…¨æ€§è¯„ä¼°çš„ä¸€è‡´æ€§é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¾®è°ƒè®¾ç½®ä¸Šè¿›è¡Œç»†å¾®è°ƒæ•´ä¼šå¯¼è‡´å®‰å…¨è¯„ä¼°ç»“æœçš„æ˜¾è‘—æ³¢åŠ¨ï¼ŒæŸäº›æƒ…å†µä¸‹æ³¢åŠ¨å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†å½“å‰å®‰å…¨è¯„ä¼°æ–¹æ³•çš„è„†å¼±æ€§ï¼Œå‘¼åç ”ç©¶è€…åœ¨æŠ¥å‘Šç»“æœæ—¶éœ€æ›´åŠ è°¨æ…ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯æ¯”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘ä¸éƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜å®‰å…¨æ€§çš„åº”ç”¨åœºæ™¯ï¼Œå¦‚åŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ç­‰é¢†åŸŸã€‚é€šè¿‡æ”¹è¿›å®‰å…¨è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œé™ä½è¢«æ¶æ„åˆ©ç”¨çš„é£é™©ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning a general-purpose large language model (LLM) for a specific domain or task has become a routine procedure for ordinary users. However, fine-tuning is known to remove the safety alignment features of the model, even when the fine-tuning data does not contain any harmful content. We consider this to be a critical failure mode of LLMs due to the widespread uptake of fine-tuning, combined with the benign nature of the "attack". Most well-intentioned developers are likely unaware that they are deploying an LLM with reduced safety. On the other hand, this known vulnerability can be easily exploited by malicious actors intending to bypass safety guardrails. To make any meaningful progress in mitigating this issue, we first need reliable and reproducible safety evaluations. In this work, we investigate how robust a safety benchmark is to trivial variations in the experimental procedure, and the stochastic nature of LLMs. Our initial experiments expose surprising variance in the results of the safety evaluation, even when seemingly inconsequential changes are made to the fine-tuning setup. Our observations have serious implications for how researchers in this field should report results to enable meaningful comparisons in the future.

