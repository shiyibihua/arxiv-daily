---
layout: default
title: MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models
---

# MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17046" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17046v2</a>
  <a href="https://arxiv.org/pdf/2506.17046.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17046v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17046v2', 'MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaolong Wang, Zhaolu Kang, Wangyuxuan Zhai, Xinyue Lou, Yunghwei Lai, Ziyue Wang, Yawen Wang, Kaiyu Huang, Yile Wang, Peng Li, Yang Liu

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-09-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMUCARä»¥è§£å†³å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡ç³Šæ€§è§£å†³` `è·¨æ¨¡æ€ç†è§£` `å¤šè¯­è¨€æ•°æ®é›†` `åŒæ¨¡ç³Šæ•°æ®é›†` `æ¶ˆæ­§æ–¹æ³•` `è§†è§‰-è¯­è¨€ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€åŸºå‡†å¿½è§†è¯­è¨€å’Œè§†è§‰çš„æ¨¡ç³Šæ€§ï¼Œä¸»è¦ä¾èµ–å•æ¨¡æ€ä¸Šä¸‹æ–‡è¿›è¡Œæ¶ˆæ­§ï¼Œå¯¼è‡´æ•ˆæœä¸ä½³ã€‚
2. MUCARåŸºå‡†é€šè¿‡æ„å»ºå¤šè¯­è¨€æ•°æ®é›†å’ŒåŒæ¨¡ç³Šæ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œè§£å†³å¤šæ¨¡æ€åœºæ™¯ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚
3. å¯¹19ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸äººç±»è¡¨ç°ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨æ¨¡ç³Šæ€§ç†è§£ä¸Šä»æœ‰æ˜¾è‘—å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†ç°å®ä¸–ç•Œä¸­çš„è¯­è¨€å’Œè§†è§‰ä¸Šä¸‹æ–‡çš„å›ºæœ‰æ¨¡ç³Šæ€§æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¤šæ¨¡æ€åŸºå‡†é€šå¸¸å¿½è§†è¯­è¨€å’Œè§†è§‰çš„æ¨¡ç³Šæ€§ï¼Œä¸»è¦ä¾èµ–å•æ¨¡æ€ä¸Šä¸‹æ–‡è¿›è¡Œæ¶ˆæ­§ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ¨¡æ€é—´çš„ç›¸äº’æ¾„æ¸…æ½œåŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MUCARï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šè¯­è¨€å’Œè·¨æ¨¡æ€åœºæ™¯ä¸‹æ¨¡ç³Šæ€§è§£å†³çš„æ–°åŸºå‡†ã€‚MUCARåŒ…å«ä¸€ä¸ªå¤šè¯­è¨€æ•°æ®é›†ï¼Œé€šè¿‡ç›¸åº”çš„è§†è§‰ä¸Šä¸‹æ–‡å”¯ä¸€è§£å†³æ¨¡ç³Šçš„æ–‡æœ¬è¡¨è¾¾ï¼Œä»¥åŠä¸€ä¸ªåŒæ¨¡ç³Šæ•°æ®é›†ï¼Œç³»ç»Ÿæ€§åœ°å°†æ¨¡ç³Šå›¾åƒä¸æ¨¡ç³Šæ–‡æœ¬ä¸Šä¸‹æ–‡é…å¯¹ï¼Œä»¥å®ç°é€šè¿‡ç›¸äº’æ¶ˆæ­§å¾—åˆ°å•ä¸€æ¸…æ™°è§£é‡Šçš„ç›®æ ‡ã€‚å¯¹19ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œä¸äººç±»æ°´å¹³è¡¨ç°ç›¸æ¯”ï¼Œä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼ºè°ƒäº†æœªæ¥åœ¨è·¨æ¨¡æ€æ¨¡ç³Šç†è§£æ–¹æ³•ä¸Šçš„ç ”ç©¶éœ€æ±‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¯­è¨€å’Œè§†è§‰ä¸Šä¸‹æ–‡ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†æ¨¡æ€é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´æ¶ˆæ­§æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMUCARåŸºå‡†é€šè¿‡å¼•å…¥å¤šè¯­è¨€å’ŒåŒæ¨¡ç³Šæ•°æ®é›†ï¼Œåˆ©ç”¨è§†è§‰ä¸Šä¸‹æ–‡æ¥å”¯ä¸€è§£ææ¨¡ç³Šçš„æ–‡æœ¬è¡¨è¾¾ï¼Œä¿ƒè¿›æ¨¡æ€é—´çš„ç›¸äº’æ¾„æ¸…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMUCARçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå¤šè¯­è¨€æ•°æ®é›†å’ŒåŒæ¨¡ç³Šæ•°æ®é›†ã€‚å¤šè¯­è¨€æ•°æ®é›†æä¾›äº†é€šè¿‡è§†è§‰ä¸Šä¸‹æ–‡è§£æçš„æ¨¡ç³Šæ–‡æœ¬ï¼Œè€ŒåŒæ¨¡ç³Šæ•°æ®é›†åˆ™ç³»ç»Ÿæ€§åœ°å°†æ¨¡ç³Šå›¾åƒä¸æ¨¡ç³Šæ–‡æœ¬é…å¯¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMUCARçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶ç³»ç»Ÿæ€§åœ°æ„å»ºäº†åŒæ¨¡ç³Šæ•°æ®é›†ï¼Œä½¿å¾—æ¯ä¸€å¯¹æ¨¡ç³Šå›¾åƒå’Œæ–‡æœ¬éƒ½èƒ½é€šè¿‡ç›¸äº’æ¶ˆæ­§å¾—åˆ°æ¸…æ™°çš„è§£é‡Šï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œé‡‡ç”¨äº†ç²¾å¿ƒè®¾è®¡çš„é…å¯¹ç­–ç•¥ï¼Œç¡®ä¿æ¯ä¸ªæ¨¡ç³Šç»„åˆéƒ½èƒ½é€šè¿‡è§†è§‰å’Œæ–‡æœ¬çš„ç›¸äº’ä½œç”¨å®ç°æ¸…æ™°çš„ç†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ19ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨MUCARåŸºå‡†ä¸Šçš„è¡¨ç°ä¸äººç±»æ°´å¹³å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼ºè°ƒäº†å½“å‰æ¨¡å‹åœ¨è·¨æ¨¡æ€æ¨¡ç³Šç†è§£æ–¹é¢çš„ä¸è¶³ã€‚è¿™ä¸€å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ï¼Œæ¨åŠ¨äº†æ›´å¤æ‚çš„æ¶ˆæ­§æ–¹æ³•çš„å‘å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ã€æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€æ¨¡å‹åœ¨æ¨¡ç³Šæ€§ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼ŒMUCARæœ‰åŠ©äºæ¨åŠ¨æ›´æ™ºèƒ½çš„äº¤äº’ç³»ç»Ÿçš„å‘å±•ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œä¿¡æ¯è·å–çš„å‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have demonstrated significant advances across numerous vision-language tasks. MLLMs have shown promising capability in aligning visual and textual modalities, allowing them to process image-text pairs with clear and explicit meanings. However, resolving the inherent ambiguities present in real-world language and visual contexts remains a challenge. Existing multimodal benchmarks typically overlook linguistic and visual ambiguities, relying mainly on unimodal context for disambiguation and thus failing to exploit the mutual clarification potential between modalities. To bridge this gap, we introduce MUCAR, a novel and challenging benchmark designed explicitly for evaluating multimodal ambiguity resolution across multilingual and cross-modal scenarios. MUCAR includes first a multilingual dataset where ambiguous textual expressions are uniquely resolved by corresponding visual contexts, and second a dual-ambiguity dataset that systematically pairs ambiguous images with ambiguous textual contexts, with each combination carefully constructed to yield a single, clear interpretation through mutual disambiguation. Extensive evaluations involving 19 state-of-the-art multimodal models--encompassing both open-source and proprietary architectures--reveal substantial gaps compared to human-level performance, highlighting the need for future research into more sophisticated cross-modal ambiguity comprehension methods, further pushing the boundaries of multimodal reasoning.

