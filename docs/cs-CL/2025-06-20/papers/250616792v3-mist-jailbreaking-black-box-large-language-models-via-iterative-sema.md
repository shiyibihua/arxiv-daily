---
layout: default
title: MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning
---

# MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16792" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16792v3</a>
  <a href="https://arxiv.org/pdf/2506.16792.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16792v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16792v3', 'MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Muyang Zheng, Yuanzhi Yao, Changting Lin, Caihong Kai, Yanxiang Chen, Zhiquan Liu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-09-20)

**å¤‡æ³¨**: 13 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMISTä»¥è§£å†³é»‘ç®±å¤§è¯­è¨€æ¨¡å‹çš„è¶Šç‹±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é»‘ç®±æ¨¡å‹` `è¶Šç‹±æ”»å‡»` `è¯­ä¹‰è°ƒä¼˜` `å¯¹æŠ—æ€§è®­ç»ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å®‰å…¨æ€§è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯¹æŠ—è¶Šç‹±æ”»å‡»æ—¶å­˜åœ¨æ˜¾è‘—è„†å¼±æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è¾“å…¥çš„ç¦»æ•£æ€§å’ŒæŸ¥è¯¢é¢„ç®—é™åˆ¶ä¸‹ã€‚
2. MISTæ–¹æ³•é€šè¿‡è¿­ä»£è¯­ä¹‰è°ƒä¼˜ï¼Œå…è®¸æ”»å‡»è€…åœ¨ä¿ç•™è¯­ä¹‰æ„å›¾çš„åŒæ—¶ï¼Œé€æ­¥ä¼˜åŒ–æç¤ºä»¥è¯±å¯¼æœ‰å®³å†…å®¹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMISTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†ç«äº‰æ€§çš„æ”»å‡»æˆåŠŸç‡å’Œè¾ƒä½çš„æŸ¥è¯¢æ¬¡æ•°ï¼Œä¼˜äºæˆ–åŒ¹é…äº†ç°æœ‰çš„è¶Šç‹±æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å·²æœ‰åŠªåŠ›ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç¤¾ä¼šå’Œé“å¾·ä»·å€¼è§‚ä¿æŒä¸€è‡´ï¼Œè¿™äº›æ¨¡å‹ä»ç„¶å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ï¼Œå³æ—¨åœ¨å¼•å‘æœ‰å®³å“åº”çš„æ–¹æ³•ã€‚ç”±äºè¾“å…¥çš„ç¦»æ•£æ€§ã€å¯¹ç›®æ ‡LLMçš„é™åˆ¶è®¿é—®ä»¥åŠæœ‰é™çš„æŸ¥è¯¢é¢„ç®—ï¼Œè¶Šç‹±é»‘ç®±LLMè¢«è®¤ä¸ºæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºMISTï¼Œé€šè¿‡è¿­ä»£è¯­ä¹‰è°ƒä¼˜æ¥è¶Šç‹±é»‘ç®±å¤§å‹è¯­è¨€æ¨¡å‹ã€‚MISTä½¿æ”»å‡»è€…èƒ½å¤Ÿè¿­ä»£åœ°ä¼˜åŒ–æç¤ºï¼Œä¿ç•™åŸå§‹è¯­ä¹‰æ„å›¾çš„åŒæ—¶è¯±å¯¼æœ‰å®³å†…å®¹ã€‚å…·ä½“è€Œè¨€ï¼Œä¸ºäº†å¹³è¡¡è¯­ä¹‰ç›¸ä¼¼æ€§ä¸è®¡ç®—æ•ˆç‡ï¼ŒMISTç»“åˆäº†ä¸¤ä¸ªå…³é”®ç­–ç•¥ï¼šé¡ºåºåŒä¹‰è¯æœç´¢åŠå…¶é«˜çº§ç‰ˆæœ¬â€”â€”é¡ºåºç¡®å®šä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMISTåœ¨æ”»å‡»æˆåŠŸç‡ã€æŸ¥è¯¢æ¬¡æ•°å’Œå¯è½¬ç§»æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šæˆ–åŒ¹é…äº†ç°æœ‰çš„æœ€å…ˆè¿›è¶Šç‹±æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é»‘ç®±å¤§å‹è¯­è¨€æ¨¡å‹çš„è¶Šç‹±é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¾“å…¥ç¦»æ•£æ€§ã€è®¿é—®é™åˆ¶å’ŒæŸ¥è¯¢é¢„ç®—æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMISTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¿­ä»£ä¼˜åŒ–æç¤ºï¼Œä¿æŒåŸå§‹è¯­ä¹‰æ„å›¾çš„åŒæ—¶è¯±å¯¼æœ‰å®³å†…å®¹ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„è¶Šç‹±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMISTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¡ºåºåŒä¹‰è¯æœç´¢å’Œé¡ºåºç¡®å®šä¼˜åŒ–ï¼Œå‰è€…ç”¨äºåˆæ­¥ä¼˜åŒ–æç¤ºï¼Œåè€…åˆ™è¿›ä¸€æ­¥æå‡ä¼˜åŒ–æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šMISTçš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆäº†é¡ºåºåŒä¹‰è¯æœç´¢ä¸é¡ºåºç¡®å®šä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†æ”»å‡»çš„æˆåŠŸç‡å’Œæ•ˆç‡ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMISTé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¿æŒè¯­ä¹‰ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶é™ä½è®¡ç®—å¤æ‚åº¦ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œä¼˜åŒ–ç­–ç•¥åœ¨å®éªŒä¸­å¾—åˆ°äº†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMISTåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ï¼ŒæŸ¥è¯¢æ¬¡æ•°ç›¸å¯¹è¾ƒä½ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„å¯è½¬ç§»æ€§ã€‚ä¸ç°æœ‰æœ€å…ˆè¿›çš„è¶Šç‹±æ–¹æ³•ç›¸æ¯”ï¼ŒMISTåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ€§æµ‹è¯•ã€æ¨¡å‹é²æ£’æ€§è¯„ä¼°ä»¥åŠå¯¹æŠ—æ€§è®­ç»ƒç­‰ã€‚é€šè¿‡æœ‰æ•ˆçš„è¶Šç‹±æ–¹æ³•ï¼Œç ”ç©¶è€…å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œè¿›è€Œæ¨åŠ¨æ›´å®‰å…¨çš„AIç³»ç»Ÿçš„å¼€å‘ã€‚æœªæ¥ï¼ŒMISTå¯èƒ½åœ¨å¯¹æŠ—æ€§æ”»å‡»å’Œé˜²å¾¡ç­–ç•¥çš„ç ”ç©¶ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks -- methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version -- order-determining optimization. We conduct extensive experiments on two datasets using two open-source and four closed-source models. Results show that MIST achieves competitive attack success rate, relatively low query count, and fair transferability, outperforming or matching state-of-the-art jailbreak methods. Additionally, we conduct analysis on computational efficiency to validate the practical viability of MIST.

