---
layout: default
title: From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models
---

# From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21609" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21609v1</a>
  <a href="https://arxiv.org/pdf/2506.21609.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21609v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21609v1', 'From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junhao Liu, Zhenhao Xu, Yuxin Fang, Yichuan Chen, Zuobin Ying, Wenhan Chang

**åˆ†ç±»**: cs.CL, cs.AI, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20

**å¤‡æ³¨**: 18 pages, 3 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ChangWenhan/FromThinking2Output)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–°æ¡†æ¶åˆ†ææ¨ç†è¯­è¨€æ¨¡å‹çš„æ€ç»´ä¸è¾“å‡ºç‰¹å¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘åæ€` `å…³é”®è¯ç»Ÿè®¡` `æ¨¡å‹è¯„ä¼°` `é€»è¾‘æ¨ç†` `å› æœæ¨æ–­` `å¤šæ­¥éª¤é—®é¢˜è§£å†³`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹å’Œè¾“å‡ºç¼ºä¹ç³»ç»Ÿæ¯”è¾ƒï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªæˆ‘åæ€æ¨¡å¼å’Œè·¨é¢†åŸŸå…³è”æ€§æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç»“åˆå…³é”®è¯ç»Ÿè®¡å’ŒLLMä½œä¸ºè¯„åˆ¤è€…çš„èŒƒå¼ï¼Œåˆ†æå››ç§å‰æ²¿æ¨ç†æ¨¡å‹çš„ç‰¹å¾ã€‚
3. ç ”ç©¶å‘ç°æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨å¤šç§æ¨¡å¼ï¼Œæ­ç¤ºäº†æ¨ç†æ·±åº¦å’Œè¾“å‡ºå‡†ç¡®æ€§ç­‰æ–¹é¢çš„æ˜¾è‘—å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å¯¹è¿™äº›æ¨¡å‹æ¨ç†è¿‡ç¨‹å’Œè¾“å‡ºçš„ç³»ç»Ÿæ¯”è¾ƒä»æ˜¾ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªæˆ‘åæ€æ¨¡å¼åŠè·¨é¢†åŸŸå…³è”æ€§æ–¹é¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡å…³é”®è¯ç»Ÿè®¡å’ŒLLMä½œä¸ºè¯„åˆ¤è€…çš„èŒƒå¼ï¼Œåˆ†æå››ç§å‰æ²¿æ¨ç†æ¨¡å‹ï¼ˆGPT-o1ã€DeepSeek-R1ã€Kimi-k1.5å’ŒGrok-3ï¼‰çš„æ¨ç†ç‰¹å¾ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€å¤„ç†é—®é¢˜åŠå¾—å‡ºç»“è®ºçš„å¤šç§æ¨¡å¼ï¼Œå¹¶é€šè¿‡å®šé‡å’Œå®šæ€§æ¯”è¾ƒï¼Œè¯†åˆ«å‡ºæ¨¡å‹é—´åœ¨æ¨ç†æ·±åº¦ã€å¯¹ä¸­é—´æ­¥éª¤çš„ä¾èµ–ç¨‹åº¦åŠæ€ç»´è¿‡ç¨‹ä¸è¾“å‡ºæ¨¡å¼çš„ç›¸ä¼¼æ€§ç­‰æ–¹é¢çš„å·®å¼‚ã€‚è¯¥ç ”ç©¶ä¸ºæé«˜æ¨¡å‹è®¾è®¡å’Œè¯„ä¼°æä¾›äº†å®ç”¨å»ºè®®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¯¹å¤§å‹æ¨ç†è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹å’Œè¾“å‡ºçš„ç³»ç»Ÿæ¯”è¾ƒä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æ·±å…¥æ¢è®¨æ¨¡å‹çš„è‡ªæˆ‘åæ€æ¨¡å¼å’Œè·¨é¢†åŸŸå…³è”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§æ–°é¢–çš„åˆ†ææ¡†æ¶ï¼Œé€šè¿‡å…³é”®è¯ç»Ÿè®¡å’ŒLLMä½œä¸ºè¯„åˆ¤è€…çš„æ–¹å¼ï¼Œè¿æ¥æ¨¡å‹çš„å†…éƒ¨æ€ç»´è¿‡ç¨‹ä¸æœ€ç»ˆè¾“å‡ºï¼Œæä¾›æ›´å…¨é¢çš„æ¯”è¾ƒè§†è§’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹æ¨ç†è¿‡ç¨‹åˆ†æå’Œè¾“å‡ºè¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æ¶µç›–é€»è¾‘æ¨ç†ã€å› æœæ¨æ–­å’Œå¤šæ­¥éª¤é—®é¢˜è§£å†³çš„çœŸå®åœºæ™¯é—®é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€å¥—æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿé‡åŒ–æ¨ç†çš„è¿è´¯æ€§å’Œè¾“å‡ºçš„å‡†ç¡®æ€§ï¼Œæ­ç¤ºæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹çš„æ¨ç†æ·±åº¦å’Œè¾“å‡ºå‡†ç¡®æ€§ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ•´ä½“ç½‘ç»œç»“æ„åˆ™åŸºäºç°æœ‰çš„æ¨ç†æ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼Œå¢å¼ºå…¶å¯¹ä¸­é—´æ­¥éª¤çš„ä¾èµ–æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå››ç§æ¨¡å‹åœ¨æ¨ç†æ·±åº¦å’Œè¾“å‡ºå‡†ç¡®æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ç‰¹åˆ«æ˜¯ï¼ŒGPT-o1åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œæ¨ç†æ·±åº¦æé«˜äº†çº¦15%ï¼Œè¾“å‡ºå‡†ç¡®æ€§æå‡äº†20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå†³ç­–æ”¯æŒç³»ç»Ÿç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£æ¨¡å‹çš„æ¨ç†ç‰¹å¾ï¼Œå¯ä»¥ä¼˜åŒ–æ¨¡å‹è®¾è®¡ï¼Œæé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œå¯é æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, there have been notable advancements in large language models (LLMs), demonstrating their growing abilities in complex reasoning. However, existing research largely overlooks a thorough and systematic comparison of these models' reasoning processes and outputs, particularly regarding their self-reflection pattern (also termed "Aha moment") and the interconnections across diverse domains. This paper proposes a novel framework for analyzing the reasoning characteristics of four cutting-edge large reasoning models (GPT-o1, DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge paradigm. Our approach connects their internal thinking processes with their final outputs. A diverse dataset consists of real-world scenario-based questions covering logical deduction, causal inference, and multi-step problem-solving. Additionally, a set of metrics is put forward to assess both the coherence of reasoning and the accuracy of the outputs. The research results uncover various patterns of how these models balance exploration and exploitation, deal with problems, and reach conclusions during the reasoning process. Through quantitative and qualitative comparisons, disparities among these models are identified in aspects such as the depth of reasoning, the reliance on intermediate steps, and the degree of similarity between their thinking processes and output patterns and those of GPT-o1. This work offers valuable insights into the trade-off between computational efficiency and reasoning robustness and provides practical recommendations for enhancing model design and evaluation in practical applications. We publicly release our project at: https://github.com/ChangWenhan/FromThinking2Output

