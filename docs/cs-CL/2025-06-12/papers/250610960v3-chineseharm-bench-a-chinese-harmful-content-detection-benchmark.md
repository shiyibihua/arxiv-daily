---
layout: default
title: ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark
---

# ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10960" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10960v3</a>
  <a href="https://arxiv.org/pdf/2506.10960.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10960v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10960v3', 'ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kangwei Liu, Siyuan Cheng, Bozhong Tian, Xiaozhuan Liang, Yuyang Yin, Meng Han, Ningyu Zhang, Bryan Hooi, Xi Chen, Shumin Deng

**åˆ†ç±»**: cs.CL, cs.AI, cs.CR, cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12 (æ›´æ–°: 2025-08-13)

**å¤‡æ³¨**: Work in progress

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zjunlp/ChineseHarm-bench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºChineseHarm-Benchä»¥è§£å†³ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸­æ–‡å†…å®¹æ£€æµ‹` `æœ‰å®³å†…å®¹è¯†åˆ«` `çŸ¥è¯†å¢å¼º` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœ‰å®³å†…å®¹æ£€æµ‹èµ„æºä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œä¸­æ–‡æ•°æ®é›†ç¨€ç¼ºä¸”èŒƒå›´æœ‰é™ï¼Œå¯¼è‡´ä¸­æ–‡å†…å®¹å®¡æ ¸æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹åŸºå‡†ï¼Œç»“åˆçœŸå®æ•°æ®å’Œä¸“å®¶çŸ¥è¯†ï¼Œæå‡äº†æ¨¡å‹çš„æ£€æµ‹èƒ½åŠ›ã€‚
3. é€šè¿‡çŸ¥è¯†å¢å¼ºåŸºçº¿ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¯ä¸æœ€å…ˆè¿›çš„LLMsç›¸åª²ç¾ï¼Œæ˜¾è‘—æé«˜äº†æ£€æµ‹æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–æœ‰å®³å†…å®¹æ£€æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¢åŠ ï¼Œå¸®åŠ©å®¡æ ¸äººå‘˜è¯†åˆ«æ”¿ç­–è¿è§„å¹¶æé«˜å†…å®¹å®¡æ ¸çš„æ•´ä½“æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æœ‰å®³å†…å®¹æ£€æµ‹èµ„æºä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œä¸­æ–‡æ•°æ®é›†åˆ™ç¨€ç¼ºä¸”èŒƒå›´æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢ã€ä¸“ä¸šæ ‡æ³¨çš„ä¸­æ–‡å†…å®¹æœ‰å®³æ£€æµ‹åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªä»£è¡¨æ€§ç±»åˆ«ï¼Œå®Œå…¨åŸºäºçœŸå®ä¸–ç•Œæ•°æ®æ„å»ºã€‚æˆ‘ä»¬çš„æ ‡æ³¨è¿‡ç¨‹è¿˜ç”Ÿæˆäº†ä¸€ä¸ªçŸ¥è¯†è§„åˆ™åº“ï¼Œä¸ºLLMsåœ¨ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹ä¸­æä¾›æ˜ç¡®çš„ä¸“å®¶çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çŸ¥è¯†å¢å¼ºåŸºçº¿ï¼Œç»“åˆäººç±»æ ‡æ³¨çš„çŸ¥è¯†è§„åˆ™å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„éšæ€§çŸ¥è¯†ï¼Œä½¿å¾—è¾ƒå°çš„æ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°ä¸æœ€å…ˆè¿›çš„LLMsç›¸å½“çš„æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨https://github.com/zjunlp/ChineseHarm-benchè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹èµ„æºç¨€ç¼ºçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸­æ–‡å†…å®¹å®¡æ ¸ä¸­é¢ä¸´æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹åŸºå‡†ï¼Œç»“åˆçœŸå®ä¸–ç•Œæ•°æ®å’Œä¸“å®¶çŸ¥è¯†ï¼Œå¢å¼ºæ¨¡å‹çš„æ£€æµ‹èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ä¸“ä¸šæ ‡æ³¨ã€çŸ¥è¯†è§„åˆ™åº“æ„å»ºå’ŒçŸ¥è¯†å¢å¼ºæ¨¡å‹è®­ç»ƒç­‰ä¸»è¦æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªåŸºäºçœŸå®æ•°æ®çš„ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹åŸºå‡†ï¼Œå¹¶æå‡ºäº†çŸ¥è¯†å¢å¼ºçš„æ¨¡å‹ï¼Œä½¿å¾—å°æ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°è¾ƒé«˜çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œç»“åˆäº†äººç±»æ ‡æ³¨çš„çŸ¥è¯†è§„åˆ™å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„éšæ€§çŸ¥è¯†ï¼Œä¼˜åŒ–äº†å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥æå‡æ¨¡å‹çš„æ£€æµ‹æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒçŸ¥è¯†å¢å¼ºåŸºçº¿æ¨¡å‹åœ¨ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½è¾¾åˆ°äº†ä¸æœ€å…ˆè¿›çš„LLMsç›¸å½“çš„æ°´å¹³ï¼Œæå‡å¹…åº¦å¯è¾¾20%ä»¥ä¸Šï¼Œæ˜¾è‘—æé«˜äº†å°æ¨¡å‹çš„æ£€æµ‹èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨ç¤¾äº¤åª’ä½“ã€åœ¨çº¿è¯„è®ºå’Œå†…å®¹å®¡æ ¸ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æœ‰å®³ä¿¡æ¯çš„ä¼ æ’­ï¼Œä¿æŠ¤ç”¨æˆ·çš„ç½‘ç»œå®‰å…¨ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å’Œæ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–è¯­è¨€å’Œå†…å®¹ç±»å‹çš„æ£€æµ‹ä¸­ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.

