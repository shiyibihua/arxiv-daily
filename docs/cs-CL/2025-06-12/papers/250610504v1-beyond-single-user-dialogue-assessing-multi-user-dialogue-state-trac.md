---
layout: default
title: Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models
---

# Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10504" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10504v1</a>
  <a href="https://arxiv.org/pdf/2506.10504.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10504v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10504v1', 'Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sangmin Song, Juhwan Choi, JungMin Yun, YoungBin Kim

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç”¨æˆ·å¯¹è¯çŠ¶æ€è·Ÿè¸ªä¸­çš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹è¯çŠ¶æ€è·Ÿè¸ª` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šç”¨æˆ·äº¤äº’` `è¨€è¯­è¡Œä¸ºç†è®º` `æ•°æ®é›†æ‰©å±•` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹è¯çŠ¶æ€è·Ÿè¸ªæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•ç”¨æˆ·åœºæ™¯ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å¤šç”¨æˆ·äº¤äº’çš„å¤æ‚æ€§ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡ç”Ÿæˆç¬¬äºŒç”¨æˆ·çš„å‘è¨€ï¼Œæ‰©å±•ç°æœ‰æ•°æ®é›†ï¼Œä»¥ä¾¿å¯¹LLMsåœ¨å¤šç”¨æˆ·ç¯å¢ƒä¸­çš„è¡¨ç°è¿›è¡Œç³»ç»Ÿè¯„ä¼°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å¤šç”¨æˆ·DSTä¸­çš„æ€§èƒ½æ˜¾è‘—ä½äºå•ç”¨æˆ·DSTï¼Œæ˜¾ç¤ºå‡ºå½“å‰æ¨¡å‹çš„å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡å°‘äº†å¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„DSTåŸºå‡†ä¸»è¦é›†ä¸­åœ¨ç»“æ„åŒ–çš„ç”¨æˆ·-ä»£ç†å¯¹è¯ä¸Šï¼Œæœªèƒ½æ•æ‰ç°å®ä¸–ç•Œå¤šç”¨æˆ·äº¤äº’çš„å¤æ‚æ€§ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†LLMsåœ¨å¤šç”¨æˆ·DSTä¸­çš„é²æ£’æ€§ï¼ŒåŒæ—¶é™ä½æ•°æ®é›†æ„å»ºæˆæœ¬ã€‚æˆ‘ä»¬åŸºäºè¨€è¯­è¡Œä¸ºç†è®ºï¼Œæ‰©å±•ç°æœ‰DSTæ•°æ®é›†ï¼Œé€šè¿‡ç”Ÿæˆç¬¬äºŒç”¨æˆ·çš„å‘è¨€æ¥å®ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸å•ç”¨æˆ·DSTç›¸æ¯”ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œçªæ˜¾äº†å½“å‰LLMsåœ¨å¤šå‘è¨€è€…ç¯å¢ƒä¸­æå–å’Œè·Ÿè¸ªå¯¹è¯çŠ¶æ€çš„å±€é™æ€§ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æœªæ¥ç ”ç©¶åœ¨å¤šç”¨æˆ·DSTåœºæ™¯ä¸­å¢å¼ºLLMsçš„å¿…è¦æ€§ï¼Œä¸ºæ›´ç°å®å’Œé²æ£’çš„DSTæ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç”¨æˆ·å¯¹è¯çŠ¶æ€è·Ÿè¸ªä¸­çš„èƒ½åŠ›ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹å•ç”¨æˆ·åœºæ™¯ï¼Œæ— æ³•é€‚åº”å¤šå‘è¨€è€…çš„å¤æ‚å¯¹è¯ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç”Ÿæˆç¬¬äºŒç”¨æˆ·çš„å‘è¨€ï¼ŒåŸºäºè¨€è¯­è¡Œä¸ºç†è®ºæ‰©å±•ç°æœ‰DSTæ•°æ®é›†ï¼Œä»è€Œå®ç°å¯¹å¤šç”¨æˆ·å¯¹è¯çŠ¶æ€è·Ÿè¸ªçš„è¯„ä¼°ã€‚æ­¤è®¾è®¡æ—¨åœ¨æ¨¡æ‹ŸçœŸå®çš„å¤šç”¨æˆ·å¯¹è¯åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå¯¹ç°æœ‰DSTæ•°æ®é›†è¿›è¡Œæ‰©å±•ï¼Œç”Ÿæˆç¬¬äºŒç”¨æˆ·çš„å‘è¨€ï¼Œç„¶åå°†è¿™äº›å‘è¨€æ•´åˆåˆ°å¯¹è¯ä¸­ï¼Œæœ€åä½¿ç”¨LLMsè¿›è¡Œå¯¹è¯çŠ¶æ€è·Ÿè¸ªçš„è¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é›†æ‰©å±•ã€å¯¹è¯ç”Ÿæˆå’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºé€šè¿‡å¼•å…¥ç¬¬äºŒç”¨æˆ·çš„å‘è¨€ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMsåœ¨å¤šç”¨æˆ·å¯¹è¯ä¸­çš„è¡¨ç°ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æ›´è´´è¿‘çœŸå®çš„å¯¹è¯åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ‰©å±•è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†è¨€è¯­è¡Œä¸ºç†è®ºæ¥ç”Ÿæˆç¬¬äºŒç”¨æˆ·çš„å‘è¨€ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„åˆç†æ€§å’Œå¤šæ ·æ€§ã€‚å®éªŒä¸­ä½¿ç”¨äº†æ ‡å‡†çš„è¯„ä¼°æŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨å¤šç”¨æˆ·å¯¹è¯çŠ¶æ€è·Ÿè¸ªä¸­çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå…·ä½“è¡¨ç°ä¸ºä¸å•ç”¨æˆ·DSTç›¸æ¯”ï¼Œæ€§èƒ½ä¸‹é™å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ã€‚è¿™ä¸€ç»“æœå¼ºè°ƒäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤šå‘è¨€è€…å¯¹è¯æ—¶çš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€ç¤¾äº¤æœºå™¨äººå’Œå¤šç”¨æˆ·åä½œç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡LLMsåœ¨å¤šç”¨æˆ·å¯¹è¯ä¸­çš„è¡¨ç°ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶å’Œé«˜æ•ˆçš„äººæœºäº¤äº’ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated remarkable performance in zero-shot dialogue state tracking (DST), reducing the need for task-specific training. However, conventional DST benchmarks primarily focus on structured user-agent conversations, failing to capture the complexities of real-world multi-user interactions. In this study, we assess the robustness of LLMs in multi-user DST while minimizing dataset construction costs. Inspired by recent advances in LLM-based data annotation, we extend an existing DST dataset by generating utterances of a second user based on speech act theory. Our methodology systematically incorporates a second user's utterances into conversations, enabling a controlled evaluation of LLMs in multi-user settings. Experimental results reveal a significant performance drop compared to single-user DST, highlighting the limitations of current LLMs in extracting and tracking dialogue states amidst multiple speakers. Our findings emphasize the need for future research to enhance LLMs for multi-user DST scenarios, paving the way for more realistic and robust DST models.

