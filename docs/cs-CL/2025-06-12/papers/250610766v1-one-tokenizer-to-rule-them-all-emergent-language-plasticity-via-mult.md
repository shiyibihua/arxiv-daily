---
layout: default
title: One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers
---

# One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10766" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10766v1</a>
  <a href="https://arxiv.org/pdf/2506.10766.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10766v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10766v1', 'One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Diana Abagyan, Alejandro R. Salamanca, Andres Felipe Cruz-Salinas, Kris Cao, Hangyu Lin, Acyr Locatelli, Marzieh Fadaee, Ahmet ÃœstÃ¼n, Sara Hooker

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨åˆ†è¯å™¨ä»¥æå‡å¤šè¯­è¨€æ¨¡å‹çš„é€‚åº”èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æ¨¡å‹` `é€šç”¨åˆ†è¯å™¨` `è¯­è¨€é€‚åº”æ€§` `é¢„è®­ç»ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šè¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µé¢ä¸´å®¹é‡ã€æ•°æ®å’Œè®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œå¯¼è‡´è¯­è¨€é€‚åº”æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºä½¿ç”¨é€šç”¨åˆ†è¯å™¨ï¼Œæ—¨åœ¨é€šè¿‡è®­ç»ƒæ›´å¤šè¯­è¨€æ¥æå‡æ¨¡å‹å¯¹æ–°è¯­è¨€çš„é€‚åº”èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šç”¨åˆ†è¯å™¨åœ¨è¯­è¨€é€‚åº”æ€§ä¸Šæå‡æ˜¾è‘—ï¼Œèµ¢ç‡æå‡é«˜è¾¾20.2%ï¼Œä¸”å¯¹æœªè§è¯­è¨€çš„é€‚åº”æ€§ä¹Ÿæœ‰æ‰€æé«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨é¢„è®­ç»ƒå¤§è§„æ¨¡å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ï¼Œç”±äºæ¨¡å‹å®¹é‡æœ‰é™ã€é«˜è´¨é‡æ•°æ®ç¨€ç¼ºä»¥åŠè®¡ç®—èµ„æºå—é™ï¼Œé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œåˆ†è¯å™¨çš„è¯­è¨€è¦†ç›–ä¸è¶³ä½¿å¾—åœ¨è®­ç»ƒåé˜¶æ®µéš¾ä»¥è§£å†³æ–°è¯­è¨€çš„é€‚åº”é—®é¢˜ã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨è®­ç»ƒæ—©æœŸè¿›è¡Œç›¸å¯¹å»‰ä»·çš„å¹²é¢„ï¼Œä»¥æé«˜æ¨¡å‹åœ¨è®­ç»ƒåå¯¹æ–°è¯­è¨€çš„é€‚åº”èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨ä¸€ä¸ªé€šç”¨åˆ†è¯å™¨ï¼Œè¯¥åˆ†è¯å™¨é’ˆå¯¹æ¯”ä¸»è¦é¢„è®­ç»ƒè¯­è¨€æ›´å¤šçš„è¯­è¨€è¿›è¡Œè®­ç»ƒï¼Œä»è€Œåœ¨é¢„è®­ç»ƒåæœ‰æ•ˆæ‰©å±•è¯­è¨€è¦†ç›–ã€‚é€šè¿‡å¯¹ä¸åŒè¯­è¨€ç»„å’Œè®­ç»ƒç­–ç•¥çš„ç³»ç»Ÿå®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œé€šç”¨åˆ†è¯å™¨æ˜¾è‘—æé«˜äº†è¯­è¨€é€‚åº”æ€§ï¼Œèµ¢ç‡æå‡é«˜è¾¾20.2%ã€‚æ­¤å¤–ï¼Œé€šç”¨åˆ†è¯å™¨å¯¹å®Œå…¨æœªè§è¿‡çš„è¯­è¨€ä¹Ÿè¡¨ç°å‡ºæ›´å¥½çš„é€‚åº”æ€§ï¼Œèµ¢ç‡æå‡å¯è¾¾5%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå¯¹æ–°è¯­è¨€é€‚åº”èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç”±äºåˆ†è¯å™¨çš„è¯­è¨€è¦†ç›–é™åˆ¶ï¼Œéš¾ä»¥åœ¨è®­ç»ƒåæœ‰æ•ˆé€‚åº”æ–°è¯­è¨€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºè®¾è®¡ä¸€ä¸ªé€šç”¨åˆ†è¯å™¨ï¼Œè¯¥åˆ†è¯å™¨é’ˆå¯¹æ¯”ä¸»è¦é¢„è®­ç»ƒè¯­è¨€æ›´å¤šçš„è¯­è¨€è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹åœ¨é¢„è®­ç»ƒåå¯¹æ–°è¯­è¨€çš„é€‚åº”èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—å½±å“å·²æœ‰è¯­è¨€æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæ‰©å±•æ¨¡å‹çš„è¯­è¨€è¦†ç›–èŒƒå›´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é€šç”¨åˆ†è¯å™¨çš„è®¾è®¡ä¸è®­ç»ƒã€æ¨¡å‹çš„é¢„è®­ç»ƒä»¥åŠåç»­çš„è¯­è¨€é€‚åº”æ€§æµ‹è¯•ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬åˆ†è¯å™¨è®­ç»ƒæ¨¡å—ã€æ¨¡å‹é¢„è®­ç»ƒæ¨¡å—å’Œé€‚åº”æ€§è¯„ä¼°æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†é€šç”¨åˆ†è¯å™¨çš„æ¦‚å¿µï¼Œå¹¶é€šè¿‡ç³»ç»Ÿå®éªŒéªŒè¯äº†å…¶åœ¨è¯­è¨€é€‚åº”æ€§ä¸Šçš„æ˜¾è‘—æå‡ï¼Œä¸ä¼ ç»Ÿçš„ç‰¹å®šè¯­è¨€åˆ†è¯å™¨ç›¸æ¯”ï¼Œé€šç”¨åˆ†è¯å™¨åœ¨é€‚åº”æ–°è¯­è¨€æ—¶è¡¨ç°å‡ºæ›´é«˜çš„çµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡é€šç”¨åˆ†è¯å™¨æ—¶ï¼Œè€ƒè™‘äº†å¤šç§è¯­è¨€çš„ç‰¹å¾ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°å’Œå¤šè¯­è¨€è®­ç»ƒç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸Šçš„æ€§èƒ½å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨é€šç”¨åˆ†è¯å™¨çš„æ¨¡å‹åœ¨è¯­è¨€é€‚åº”æ€§ä¸Šæ˜¾è‘—æé«˜ï¼Œèµ¢ç‡ç›¸æ¯”äºç‰¹å®šè¯­è¨€åˆ†è¯å™¨æå‡é«˜è¾¾20.2%ã€‚æ­¤å¤–ï¼Œå¯¹å®Œå…¨æœªè§è¿‡çš„è¯­è¨€ï¼Œé€šç”¨åˆ†è¯å™¨ä¹Ÿè¡¨ç°å‡º5%çš„èµ¢ç‡å¢ç›Šï¼Œè¡¨æ˜å…¶åœ¨è¯­è¨€é€‚åº”æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œå…¨çƒåŒ–çš„è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ã€‚é€šè¿‡æå‡æ¨¡å‹çš„è¯­è¨€é€‚åº”èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æœåŠ¡äºå¤šè¯­è¨€ç”¨æˆ·ï¼Œä¿ƒè¿›ä¸åŒè¯­è¨€ä¹‹é—´çš„äº¤æµä¸ç†è§£ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pretraining massively multilingual Large Language Models (LLMs) for many languages at once is challenging due to limited model capacity, scarce high-quality data, and compute constraints. Moreover, the lack of language coverage of the tokenizer makes it harder to address the gap for new languages purely at the post-training stage. In this work, we study what relatively cheap interventions early on in training improve "language plasticity", or adaptation capabilities of the model post-training to new languages. We focus on tokenizer design and propose using a universal tokenizer that is trained for more languages than the primary pretraining languages to enable efficient adaptation in expanding language coverage after pretraining. Our systematic experiments across diverse groups of languages and different training strategies show that a universal tokenizer enables significantly higher language adaptation, with up to 20.2% increase in win rates compared to tokenizers specific to pretraining languages. Furthermore, a universal tokenizer also leads to better plasticity towards languages that are completely unseen in the tokenizer and pretraining, by up to 5% win rate gain. We achieve this adaptation to an expanded set of languages with minimal compromise in performance on the majority of languages included in pretraining.

