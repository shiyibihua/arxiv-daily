---
layout: default
title: Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA
---

# Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21569" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21569v1</a>
  <a href="https://arxiv.org/pdf/2506.21569.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21569v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21569v1', 'Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weihua Xiao, Derek Ekberg, Siddharth Garg, Ramesh Karri

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHybrid-NL2SVAæ¡†æ¶ä»¥è§£å†³NL2SVAè‡ªåŠ¨åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªç„¶è¯­è¨€å¤„ç†` `ç³»ç»ŸéªŒè¯` `ç¡¬ä»¶è®¾è®¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨åŒ–ç”Ÿæˆ` `å¾®è°ƒæŠ€æœ¯` `æ£€ç´¢å¢å¼ºç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„NL2SVAæ–¹æ³•åœ¨ç†è§£é¢†åŸŸç‰¹å®šçš„è¯­æ³•å’Œè¯­ä¹‰æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´ç”Ÿæˆçš„SVAsè´¨é‡ä¸é«˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œåˆæˆå¾®è°ƒæ•°æ®é›†çš„æ¡†æ¶ï¼Œä»¥æå‡LLMsåœ¨NL2SVAä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå®šåˆ¶çš„RAGæ¡†æ¶å’Œå¾®è°ƒæ•°æ®é›†æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨åŠŸèƒ½åŒ¹é…ä¸Šçš„å‡†ç¡®æ€§ï¼Œè¾¾åˆ°äº†58.42%å’Œ59.05%çš„æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

SystemVerilog Assertions (SVAs) å¯¹äºç¡¬ä»¶è®¾è®¡çš„æ­£ç¡®æ€§éªŒè¯è‡³å…³é‡è¦ï¼Œä½†ä»è‡ªç„¶è¯­è¨€å±æ€§æè¿°è‡ªåŠ¨ç”ŸæˆSVAsçš„è¿‡ç¨‹ä»ç„¶åŠ³åŠ¨å¯†é›†ä¸”æ˜“å‡ºé”™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å®šåˆ¶çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶å’Œåˆæˆå¾®è°ƒæ•°æ®é›†ï¼Œä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨NL2SVAä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡æä¾›é€å±‚æ„å»ºå¹¶å‘SVAsçš„æç¤ºå¼•å¯¼è§£é‡Šï¼Œè¯¥å¾®è°ƒæ•°æ®é›†æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¯­æ³•å’ŒåŠŸèƒ½å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå®šåˆ¶çš„RAGæ¡†æ¶ä½¿å¾—åŠŸèƒ½åŒ¹é…çš„SVAsæ•°é‡è¾ƒGPT-4o-miniæå‡äº†58.42%ï¼Œè€Œåœ¨æˆ‘ä»¬çš„å¾®è°ƒæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå¹¶é›†æˆHybridRetrievalçš„Qwen2.5-Coder-7B-Instructæ¨¡å‹åˆ™è¾ƒåŸºç¡€Qwenæ¨¡å‹æå‡äº†59.05%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»è‡ªç„¶è¯­è¨€æè¿°è‡ªåŠ¨ç”ŸæˆSystemVerilog Assertions (SVAs)çš„å›°éš¾ï¼Œç°æœ‰æ–¹æ³•åœ¨ç†è§£ç‰¹å®šé¢†åŸŸçš„è¯­æ³•å’Œè¯­ä¹‰æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´ç”Ÿæˆçš„SVAsè´¨é‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§å®šåˆ¶çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œç»“åˆåˆæˆå¾®è°ƒæ•°æ®é›†ï¼Œé€šè¿‡é€å±‚æ„å»ºSVAsçš„æç¤ºå¼•å¯¼è§£é‡Šæ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ£€ç´¢æ¨¡å—ç”¨äºè·å–ç›¸å…³çš„SVAså’Œæè¿°ï¼Œç”Ÿæˆæ¨¡å—åˆ™åŸºäºæ£€ç´¢ç»“æœå’Œå¾®è°ƒæ•°æ®é›†ç”Ÿæˆé«˜è´¨é‡çš„SVAsã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†RAGæ¡†æ¶ä¸åˆæˆå¾®è°ƒæ•°æ®é›†ï¼Œæä¾›äº†é€å±‚æ„å»ºSVAsçš„æç¤ºå¼•å¯¼ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¯­æ³•å’ŒåŠŸèƒ½å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç”Ÿæˆçš„SVAsè´¨é‡ï¼Œå¹¶è®¾è®¡äº†é€‚åˆé¢†åŸŸç‰¹å®šè¯­æ³•çš„ç½‘ç»œç»“æ„ï¼Œä»¥æé«˜æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ å¹¶ç”Ÿæˆç¬¦åˆè¦æ±‚çš„SVAsã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå®šåˆ¶çš„RAGæ¡†æ¶ä½¿å¾—åŠŸèƒ½åŒ¹é…çš„SVAsæ•°é‡è¾ƒGPT-4o-miniæå‡äº†58.42%ï¼Œè€Œåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒçš„Qwen2.5-Coder-7B-Instructæ¨¡å‹åˆ™è¾ƒåŸºç¡€Qwenæ¨¡å‹æå‡äº†59.05%ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†æ–°æ–¹æ³•åœ¨NL2SVAä»»åŠ¡ä¸­çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¡¬ä»¶è®¾è®¡éªŒè¯ã€è‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆä»¥åŠæ™ºèƒ½åˆçº¦çš„éªŒè¯ç­‰ã€‚é€šè¿‡æé«˜NL2SVAçš„è‡ªåŠ¨åŒ–ç¨‹åº¦ï¼Œå¯ä»¥æ˜¾è‘—é™ä½äººå·¥ç¼–å†™SVAsçš„å·¥ä½œé‡ï¼Œæå‡ç¡¬ä»¶è®¾è®¡çš„éªŒè¯æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> SystemVerilog Assertions (SVAs) are critical for verifying the correctness of hardware designs, but manually writing them from natural language property descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task. Recent advances in large language models (LLMs) offer opportunities to automate this translation. However, existing models still struggle with understanding domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we propose a customized retrieval-augmented generation (RAG) framework and a synthetic fine-tuning dataset that together improve LLM's performance. To further improve lightweight models over NL2SVA, our fine-tuning dataset provides prompt-guided explanations that teach LLMs the layer-by-layer construction process of concurrent SVAs, enabling supervised fine-tuning that greatly improves syntax and functionality accuracy. To evaluate the performance of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA, comprising 40 Verilog designs and 229 formally verified SVAs with detailed annotations. Experimental results show that our customized RAG framework increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini, while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.

