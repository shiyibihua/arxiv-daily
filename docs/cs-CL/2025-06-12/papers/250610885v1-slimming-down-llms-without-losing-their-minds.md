---
layout: default
title: Slimming Down LLMs Without Losing Their Minds
---

# Slimming Down LLMs Without Losing Their Minds

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10885" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10885v1</a>
  <a href="https://arxiv.org/pdf/2506.10885.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10885v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10885v1', 'Slimming Down LLMs Without Losing Their Minds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qingda, Mai

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: 10 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLoRAå’ŒQLoRAçš„é«˜æ•ˆå¾®è°ƒæ–¹æ³•ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `å‚æ•°é«˜æ•ˆ` `LoRA` `QLoRA` `å¸¸è¯†æ¨ç†` `æ•°å­¦æ¨ç†` `å¤šé¢†åŸŸçŸ¥è¯†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºäº†åŸºäºLoRAå’ŒQLoRAçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶é™ä½è®¡ç®—èµ„æºæ¶ˆè€—ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoRAæ–¹æ³•åœ¨ç‰¹å®šä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œä¸”å¾®è°ƒæ•°æ®é›†ä¸ä»»åŠ¡åŸºå‡†çš„å¯¹é½æ€§å¯¹æ¨¡å‹è¡¨ç°è‡³å…³é‡è¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶å¹¶éªŒè¯äº†å¾®è°ƒå¯¹å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œé‡ç‚¹å…³æ³¨å‚æ•°é«˜æ•ˆçš„æ–¹æ³•ï¼ˆLoRAå’ŒQLoRAï¼‰ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…³é”®é¢†åŸŸè¯„ä¼°æ¨¡å‹èƒ½åŠ›ï¼šå¸¸è¯†æ¨ç†ï¼ˆHellaSwagï¼‰ã€æ•°å­¦æ¨ç†ï¼ˆGSM8Kï¼‰å’Œå¤šé¢†åŸŸçŸ¥è¯†ï¼ˆMMLU-CSï¼‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼š1ï¼‰åŸºäºLoRAçš„æ–¹æ³•æœ‰æ•ˆæå‡äº†ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼›2ï¼‰æ€§èƒ½å¼ºçƒˆä¾èµ–äºå¾®è°ƒæ•°æ®é›†ä¸åŸºå‡†ä»»åŠ¡ä¹‹é—´çš„å¯¹é½ã€‚è¯¥ç ”ç©¶ä¸ºå‚æ•°é«˜æ•ˆæœºåˆ¶æä¾›äº†ç†è®ºè§è§£ï¼Œå¹¶ä¸ºå¼€å‘è€…åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹å®æ–½é«˜æ•ˆçš„LLMé€‚åº”æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œå¯¼è‡´åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåŸºäºLoRAå’ŒQLoRAçš„å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨å‚æ•°é«˜æ•ˆçš„æœºåˆ¶æ¥æå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„é€‚åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹å¾®è°ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡é€‰æ‹©åˆé€‚çš„å¾®è°ƒæ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼›å…¶æ¬¡ï¼Œåº”ç”¨LoRAå’ŒQLoRAæ–¹æ³•è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼›æœ€åï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒåŸºå‡†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†LoRAå’ŒQLoRAä¸¤ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å…¨å‚æ•°å¾®è°ƒæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…é€šå¸¸éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒLoRAå’ŒQLoRAæ–¹æ³•é€šè¿‡ä½ç§©çŸ©é˜µåˆ†è§£æ¥å‡å°‘å‚æ•°é‡ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šåˆ™æ³¨é‡ä»»åŠ¡ç‰¹å®šçš„ä¼˜åŒ–ï¼Œç½‘ç»œç»“æ„ä¸Šä¿æŒäº†åŸæœ‰æ¨¡å‹çš„æ¶æ„ï¼Œç¡®ä¿äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä¿ç•™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLoRAçš„æ–¹æ³•åœ¨HellaSwagã€GSM8Kå’ŒMMLU-CSä¸‰ä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æå‡å¹…åº¦å¯è¾¾20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å¾®è°ƒæ•°æ®é›†ä¸ä»»åŠ¡å¯¹é½çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å®¢æœã€æ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¼€å‘è€…èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿé€‚åº”å¤§è¯­è¨€æ¨¡å‹ä»¥æ»¡è¶³ç‰¹å®šä»»åŠ¡éœ€æ±‚ï¼Œæå‡åº”ç”¨çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½åº”ç”¨å‘å±•ï¼Œé™ä½æŠ€æœ¯é—¨æ§›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper investigates and validates the impact of fine-tuning on large language model performance, focusing on parameter-efficient methods (LoRA and QLoRA). We evaluate model capabilities across three key domains: (1) commonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3) multi-domain knowledge (MMLU-CS).
>   Our findings demonstrate that: (1) LoRA-based methods effectively improve task-specific performance while maintaining computational efficiency, and (2) performance strongly depends on alignment between fine-tuning dataset and benchmark tasks. The study provides both theoretical insights into parameter-efficient mechanisms and practical guidance for developers implementing efficient LLM adaptation with limited resources.

