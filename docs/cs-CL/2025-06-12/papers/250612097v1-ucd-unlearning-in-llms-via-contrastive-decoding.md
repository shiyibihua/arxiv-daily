---
layout: default
title: UCD: Unlearning in LLMs via Contrastive Decoding
---

# UCD: Unlearning in LLMs via Contrastive Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12097" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12097v1</a>
  <a href="https://arxiv.org/pdf/2506.12097.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12097v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12097v1', 'UCD: Unlearning in LLMs via Contrastive Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vinith M. Suriyakumar, Ayush Sekhari, Ashia Wilson

**åˆ†ç±»**: cs.CL, cs.CR, cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æ¯”è§£ç æ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ä¿¡æ¯é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨é—å¿˜` `å¯¹æ¯”è§£ç ` `å¤§è¯­è¨€æ¨¡å‹` `ä¿¡æ¯å®‰å…¨` `æ¨¡å‹æ€§èƒ½` `éšç§ä¿æŠ¤` `å†…å®¹å®¡æ ¸`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šä¿¡æ¯æ—¶ï¼Œå¾€å¾€éš¾ä»¥å¹³è¡¡é—å¿˜æ•ˆæœä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”è§£ç çš„æ¨ç†æ—¶é—å¿˜ç®—æ³•ï¼Œåˆ©ç”¨ä¸¤ä¸ªè¾…åŠ©æ¨¡å‹çš„å·®å¼‚æ¥æŒ‡å¯¼åŸå§‹æ¨¡å‹çš„è¾“å‡ºã€‚
3. åœ¨TOFUå’ŒMUSEä¸¤ä¸ªé—å¿˜åŸºå‡†ä¸Šï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨é—å¿˜è´¨é‡å’Œä¿ç•™æ€§èƒ½ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºå™¨é—å¿˜æ—¨åœ¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šä¿¡æ¯ï¼Œå¦‚æ•æ„Ÿæˆ–ä¸è‰¯å†…å®¹ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨ç†æ—¶çš„é—å¿˜ç®—æ³•ï¼Œåˆ©ç”¨å¯¹æ¯”è§£ç ï¼Œå€ŸåŠ©ä¸¤ä¸ªè¾…åŠ©çš„å°å‹æ¨¡å‹ï¼Œä¸€ä¸ªåœ¨æœªåŒ…å«é—å¿˜é›†çš„æƒ…å†µä¸‹è®­ç»ƒï¼Œå¦ä¸€ä¸ªåˆ™åŒ…å«é—å¿˜é›†ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡å®ƒä»¬çš„å·®å¼‚æ¥å¼•å¯¼åŸå§‹æ¨¡å‹çš„è¾“å‡ºã€‚è¯¥ç­–ç•¥æ˜¾è‘—æ”¹å–„äº†é—å¿˜æ•ˆæœä¸æ¨¡å‹æ•ˆç”¨ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªé—å¿˜åŸºå‡†TOFUå’ŒMUSEä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºä¸å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼Œé—å¿˜è´¨é‡å’Œä¿ç•™æ€§èƒ½å‡æœ‰æ˜¾è‘—æå‡ï¼Œè¡¨æ˜å¯¹æ¯”è§£ç ä¸ºå¤§è§„æ¨¡æ¨¡å‹ä¸­çš„æ¦‚å¿µé—å¿˜æä¾›äº†ä¸€æ¡é«˜æ•ˆã€å®ç”¨çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆåœ°ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šä¿¡æ¯çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é—å¿˜ç‰¹å®šå†…å®¹æ—¶ï¼Œå¾€å¾€ä¼šå¯¼è‡´æ¨¡å‹æ•´ä½“æ€§èƒ½ä¸‹é™ï¼Œéš¾ä»¥å®ç°æœ‰æ•ˆçš„é—å¿˜ä¸æ€§èƒ½ä¿ç•™ä¹‹é—´çš„å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æ¯”è§£ç ï¼Œåˆ©ç”¨ä¸¤ä¸ªè¾…åŠ©æ¨¡å‹çš„è¾“å‡ºå·®å¼‚æ¥å¼•å¯¼åŸå§‹æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„é—å¿˜ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—åœ¨ä¸ç›´æ¥ä¿®æ”¹åŸå§‹æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿçµæ´»åœ°æ§åˆ¶é—å¿˜å†…å®¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŸå§‹æ¨¡å‹å’Œä¸¤ä¸ªè¾…åŠ©æ¨¡å‹ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡å‹åœ¨æœªåŒ…å«é—å¿˜é›†çš„æƒ…å†µä¸‹è®­ç»ƒï¼Œå¦ä¸€ä¸ªåˆ™åŒ…å«é—å¿˜é›†ã€‚åœ¨æ¨ç†æ—¶ï¼Œé€šè¿‡å¯¹æ¯”è¿™ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºï¼Œæ¥è°ƒæ•´åŸå§‹æ¨¡å‹çš„ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†å¯¹æ¯”è§£ç æœºåˆ¶ï¼Œé€šè¿‡ä¸¤ä¸ªä¸åŒè®­ç»ƒæ¡ä»¶ä¸‹çš„æ¨¡å‹è¾“å‡ºå·®å¼‚æ¥å®ç°ä¿¡æ¯çš„é€‰æ‹©æ€§é—å¿˜ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç›´æ¥ä¿®æ”¹æ¨¡å‹å‚æ•°çš„æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬è¾…åŠ©æ¨¡å‹çš„è®­ç»ƒç­–ç•¥ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°æ•´åˆä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„æ¶æ„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å¦‚ä½•æœ€å¤§åŒ–ä¿¡æ¯çš„ä¿ç•™ä¸é—å¿˜æ•ˆæœçš„å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨å¯¹æ¯”è§£ç çš„é—å¿˜ç®—æ³•åœ¨TOFUå’ŒMUSEåŸºå‡†ä¸Šï¼Œé—å¿˜è´¨é‡å’Œä¿ç•™æ€§èƒ½å‡æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºç›¸è¾ƒäºå…ˆå‰æ–¹æ³•ï¼Œé—å¿˜æ•ˆæœæé«˜äº†XX%ï¼Œè€Œæ¨¡å‹æ€§èƒ½ä¿æŒåœ¨YY%çš„æ°´å¹³ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éšç§ä¿æŠ¤ã€å†…å®¹å®¡æ ¸å’Œä¸ªæ€§åŒ–æ¨èç­‰åœºæ™¯ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»æ¨¡å‹ä¸­ç§»é™¤æ•æ„Ÿæˆ–ä¸è‰¯ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ï¼Œå°†å…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šå…³äºæœºå™¨é—å¿˜çš„ç ”ç©¶ï¼Œä¿ƒè¿›æ¨¡å‹çš„å®‰å…¨æ€§ä¸å¯é æ€§æå‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Machine unlearning aims to remove specific information, e.g. sensitive or undesirable content, from large language models (LLMs) while preserving overall performance. We propose an inference-time unlearning algorithm that uses contrastive decoding, leveraging two auxiliary smaller models, one trained without the forget set and one trained with it, to guide the outputs of the original model using their difference during inference. Our strategy substantially improves the tradeoff between unlearning effectiveness and model utility. We evaluate our approach on two unlearning benchmarks, TOFU and MUSE. Results show notable gains in both forget quality and retained performance in comparison to prior approaches, suggesting that incorporating contrastive decoding can offer an efficient, practical avenue for unlearning concepts in large-scale models.

