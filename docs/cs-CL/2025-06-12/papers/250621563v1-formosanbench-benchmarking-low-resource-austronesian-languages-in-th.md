---
layout: default
title: FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models
---

# FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21563" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21563v1</a>
  <a href="https://arxiv.org/pdf/2506.21563.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21563v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21563v1', 'FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaiying Kevin Lin, Hsiyu Chen, Haopeng Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFORMOSANBENCHä»¥è¯„ä¼°ä½èµ„æºå—å²›è¯­è¨€çš„LLMè¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½èµ„æºè¯­è¨€` `å—å²›è¯­è¨€` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `è¯­è¨€è¯„ä¼°` `æœºå™¨ç¿»è¯‘` `è‡ªåŠ¨è¯­éŸ³è¯†åˆ«` `æ–‡æœ¬æ‘˜è¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºå’Œå°‘æ•°è¯­è¨€çš„è¡¨ç°æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç¦å°”æ‘©æ²™è¯­è¨€ä¸­ã€‚
2. æœ¬æ–‡æå‡ºFORMOSANBENCHåŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°ä½èµ„æºå—å²›è¯­è¨€çš„LLMæ€§èƒ½ï¼Œæ¶µç›–å¤šé¡¹NLPä»»åŠ¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰LLMsåœ¨ç¦å°”æ‘©æ²™è¯­è¨€ä¸Šçš„è¡¨ç°æ™®éè¾ƒå·®ï¼Œå¼ºè°ƒäº†å¯¹åŒ…å®¹æ€§NLPæŠ€æœ¯çš„éœ€æ±‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜èµ„æºè¯­è¨€çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä½èµ„æºå’Œå°‘æ•°è¯­è¨€ä¸­çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†FORMOSANBENCHï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°ä½èµ„æºå—å²›è¯­è¨€çš„åŸºå‡†ï¼Œæ¶µç›–äº†ä¸‰ç§æ¿’å±çš„ç¦å°”æ‘©æ²™è¯­è¨€ï¼šé˜¿ç¾è¯­ã€æ’æ¹¾è¯­å’Œæ³°é›…è¯­ï¼Œæ¶‰åŠæœºå™¨ç¿»è¯‘ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬æ‘˜è¦ç­‰ä¸‰é¡¹æ ¸å¿ƒNLPä»»åŠ¡ã€‚é€šè¿‡é›¶æ ·æœ¬ã€10æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç»“æœæ˜¾ç¤ºé«˜èµ„æºè¯­è¨€ä¸ç¦å°”æ‘©æ²™è¯­è¨€ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œç°æœ‰LLMsåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œ10æ ·æœ¬å­¦ä¹ å’Œå¾®è°ƒä»…æä¾›æœ‰é™æ”¹è¿›ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¼€å‘æ›´å…·åŒ…å®¹æ€§çš„NLPæŠ€æœ¯ä»¥æœ‰æ•ˆæ”¯æŒæ¿’å±å’Œè¢«å¿½è§†è¯­è¨€çš„ç´§è¿«æ€§ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ•°æ®é›†å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºå—å²›è¯­è¨€ï¼ˆå¦‚ç¦å°”æ‘©æ²™è¯­è¨€ï¼‰ä¸­çš„è¯„ä¼°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™äº›è¯­è¨€ä¸Šè¡¨ç°ä¸ä½³ï¼Œç¼ºä¹æœ‰æ•ˆçš„åŸºå‡†å’Œè¯„ä¼°å·¥å…·ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºFORMOSANBENCHåŸºå‡†ï¼Œé€šè¿‡æ¶µç›–å¤šç§NLPä»»åŠ¡ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMsåœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„è¡¨ç°ï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFORMOSANBENCHåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæœºå™¨ç¿»è¯‘ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬æ‘˜è¦ï¼Œé’ˆå¯¹ä¸‰ç§ç¦å°”æ‘©æ²™è¯­è¨€è¿›è¡Œè¯„ä¼°ï¼Œé‡‡ç”¨é›¶æ ·æœ¬ã€10æ ·æœ¬å’Œå¾®è°ƒä¸‰ç§è®¾ç½®ã€‚

**å…³é”®åˆ›æ–°**ï¼šFORMOSANBENCHæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹ä½èµ„æºå—å²›è¯­è¨€çš„è¯„ä¼°åŸºå‡†ï¼Œå¡«è¡¥äº†ç°æœ‰LLMsè¯„ä¼°çš„ç©ºç™½ï¼Œå¼ºè°ƒäº†å¯¹æ¿’å±è¯­è¨€çš„å…³æ³¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«å…³æ³¨åœ¨ä¸åŒæ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°å·®å¼‚ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰LLMsåœ¨ç¦å°”æ‘©æ²™è¯­è¨€çš„æ‰€æœ‰ä»»åŠ¡ä¸­å‡è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½å·®è·æ˜¾è‘—ã€‚10æ ·æœ¬å­¦ä¹ å’Œå¾®è°ƒä»…å¸¦æ¥æœ‰é™çš„æ€§èƒ½æå‡ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å¯¹ä½èµ„æºè¯­è¨€æŠ€æœ¯æ”¯æŒçš„è¿«åˆ‡éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¯­è¨€ä¿æŠ¤ã€æ•™è‚²å’Œæ–‡åŒ–ä¼ æ‰¿ç­‰ã€‚é€šè¿‡æå‡å¯¹ç¦å°”æ‘©æ²™è¯­è¨€çš„æ”¯æŒï¼Œèƒ½å¤Ÿä¿ƒè¿›è¿™äº›æ¿’å±è¯­è¨€çš„å¤å…´å’Œä½¿ç”¨ï¼Œæ¨åŠ¨å¤šæ ·æ€§å’ŒåŒ…å®¹æ€§çš„å‘å±•ã€‚æœªæ¥ï¼ŒFORMOSANBENCHå¯ä½œä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€è¯„ä¼°çš„å‚è€ƒæ¡†æ¶ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While large language models (LLMs) have demonstrated impressive performance across a wide range of natural language processing (NLP) tasks in high-resource languages, their capabilities in low-resource and minority languages remain significantly underexplored. Formosan languages -- a subgroup of Austronesian languages spoken in Taiwan -- are both linguistically rich and endangered, largely due to the sociolinguistic dominance of Mandarin. In this work, we introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on low-resource Austronesian languages. It covers three endangered Formosan languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine translation, automatic speech recognition (ASR), and text summarization. We assess model performance in zero-shot, 10-shot, and fine-tuned settings using FORMOSANBENCH. Our results reveal a substantial performance gap between high-resource and Formosan languages. Existing LLMs consistently underperform across all tasks, with 10-shot learning and fine-tuning offering only limited improvements. These findings underscore the urgent need for more inclusive NLP technologies that can effectively support endangered and underrepresented languages. We release our datasets and code to facilitate future research in this direction.

