---
layout: default
title: Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models
---

# Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11344" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11344v1</a>
  <a href="https://arxiv.org/pdf/2506.11344.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11344v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11344v1', 'Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peilin Wu, Jinho D. Choi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–‡æœ¬åŸºç¡€çš„è¯´è¯äººåˆ†ç¦»æ–¹æ³•ä»¥è§£å†³éŸ³é¢‘è´¨é‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯´è¯äººåˆ†ç¦»` `æ–‡æœ¬åˆ†æ` `å¤šæ¨¡æ€èåˆ` `è¯­ä¹‰ç†è§£` `å¯¹è¯ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„éŸ³é¢‘åŸºç¡€è¯´è¯äººåˆ†ç¦»ç³»ç»Ÿå¸¸å¸¸å—åˆ°éŸ³é¢‘è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´è¯†åˆ«æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬çš„è¯´è¯äººåˆ†ç¦»æ–¹æ³•ï¼Œåˆ©ç”¨å¯¹è¯æ–‡æœ¬è¿›è¡Œå¥å­çº§è¯´è¯äººå˜åŒ–æ£€æµ‹ï¼Œå¼€å‘äº†å•ä¸€é¢„æµ‹æ¨¡å‹å’Œå¤šé‡é¢„æµ‹æ¨¡å‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ­å¯¹è¯ä¸­ï¼Œæ–‡æœ¬åŸºç¡€çš„æ–¹æ³•åœ¨è¯†åˆ«è¯´è¯äººå˜åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„éŸ³é¢‘åŸºç¡€ç³»ç»Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯´è¯äººåˆ†ç¦»ï¼ˆSDï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨åŸºäºæ–‡æœ¬çš„æŠ€æœ¯ä¸“æ³¨äºå¯¹è¯ä¸­çš„å¥å­çº§è¯´è¯äººå˜åŒ–æ£€æµ‹ã€‚ä¸å¸¸è§çš„éŸ³é¢‘åŸºç¡€SDç³»ç»Ÿé¢ä¸´çš„éŸ³é¢‘è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§æŒ‘æˆ˜ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä¾èµ–å¯¹è¯æ–‡æœ¬ã€‚å¼€å‘äº†å•ä¸€é¢„æµ‹æ¨¡å‹ï¼ˆSPMï¼‰å’Œå¤šé‡é¢„æµ‹æ¨¡å‹ï¼ˆMPMï¼‰ï¼Œä¸¤è€…åœ¨è¯†åˆ«è¯´è¯äººå˜åŒ–æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨çŸ­å¯¹è¯ä¸­ã€‚åŸºäºæ¶µç›–å¤šæ ·å¯¹è¯åœºæ™¯çš„ç²¾å¿ƒç­–åˆ’æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ–‡æœ¬åŸºç¡€çš„SDæ–¹æ³•ï¼Œå°¤å…¶æ˜¯MPMï¼Œåœ¨çŸ­å¯¹è¯ä¸Šä¸‹æ–‡ä¸­ä¸æœ€å…ˆè¿›çš„éŸ³é¢‘åŸºç¡€SDç³»ç»Ÿç«äº‰ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚æœ¬æ–‡ä¸ä»…å±•ç¤ºäº†åˆ©ç”¨è¯­è¨€ç‰¹å¾è¿›è¡ŒSDçš„æ½œåŠ›ï¼Œè¿˜å¼ºè°ƒäº†å°†è¯­ä¹‰ç†è§£æ•´åˆåˆ°SDç³»ç»Ÿä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€å’ŒåŸºäºè¯­ä¹‰ç‰¹å¾çš„åˆ†ç¦»ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸéŸ³é¢‘åŸºç¡€è¯´è¯äººåˆ†ç¦»æ–¹æ³•åœ¨éŸ³é¢‘è´¨é‡å·®å’Œè¯´è¯äººç›¸ä¼¼æ€§é«˜æ—¶çš„è¯†åˆ«å›°éš¾ï¼Œæå‡ºä¸€ç§æ–°çš„æ–‡æœ¬åŸºç¡€æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ä»…ä¾èµ–å¯¹è¯æ–‡æœ¬è¿›è¡Œå¥å­çº§è¯´è¯äººå˜åŒ–æ£€æµ‹ï¼Œé¿å…äº†éŸ³é¢‘å¤„ç†ä¸­çš„å¤æ‚æ€§ï¼Œåˆ©ç”¨è¯­è¨€ç‰¹å¾æå‡è¯†åˆ«å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼Œä½¿ç”¨å•ä¸€é¢„æµ‹æ¨¡å‹ï¼ˆSPMï¼‰å’Œå¤šé‡é¢„æµ‹æ¨¡å‹ï¼ˆMPMï¼‰è¿›è¡Œè¯´è¯äººå˜åŒ–æ£€æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¤šé‡é¢„æµ‹æ¨¡å‹ï¼ˆMPMï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨çŸ­å¯¹è¯åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†è¯´è¯äººå˜åŒ–çš„è¯†åˆ«èƒ½åŠ›ï¼Œä¸ä¼ ç»ŸéŸ³é¢‘æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ï¼Œç¡®ä¿äº†åœ¨çŸ­å¯¹è¯ä¸­çš„é«˜æ•ˆå­¦ä¹ å’Œå‡†ç¡®é¢„æµ‹ï¼ŒåŒæ—¶å¯¹è¯­è¨€ç‰¹å¾çš„æå–è¿›è¡Œäº†ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„å¤šé‡é¢„æµ‹æ¨¡å‹ï¼ˆMPMï¼‰åœ¨çŸ­å¯¹è¯ä¸­çš„è¯´è¯äººå˜åŒ–è¯†åˆ«å‡†ç¡®ç‡æ˜¾è‘—é«˜äºç°æœ‰çš„éŸ³é¢‘åŸºç¡€ç³»ç»Ÿï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œè¯æ˜äº†æ–‡æœ¬åŸºç¡€æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¼šè®®è®°å½•ã€å®¢æˆ·æœåŠ¡å¯¹è¯åˆ†æå’Œç¤¾äº¤åª’ä½“å†…å®¹åˆ†æç­‰ã€‚é€šè¿‡æé«˜è¯´è¯äººåˆ†ç¦»çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ†æå¯¹è¯å†…å®¹ï¼Œæå‡äººæœºäº¤äº’çš„è´¨é‡å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨å¤šæ¨¡æ€å’Œè¯­ä¹‰ç‰¹å¾ç»“åˆçš„ç ”ç©¶ï¼Œæ‹“å±•å…¶åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a novel approach to Speaker Diarization (SD) by leveraging text-based methods focused on Sentence-level Speaker Change Detection within dialogues. Unlike audio-based SD systems, which are often challenged by audio quality and speaker similarity, our approach utilizes the dialogue transcript alone. Two models are developed: the Single Prediction Model (SPM) and the Multiple Prediction Model (MPM), both of which demonstrate significant improvements in identifying speaker changes, particularly in short conversations. Our findings, based on a curated dataset encompassing diverse conversational scenarios, reveal that the text-based SD approach, especially the MPM, performs competitively against state-of-the-art audio-based SD systems, with superior performance in short conversational contexts. This paper not only showcases the potential of leveraging linguistic features for SD but also highlights the importance of integrating semantic understanding into SD systems, opening avenues for future research in multimodal and semantic feature-based diarization.

