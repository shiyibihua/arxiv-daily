---
layout: default
title: PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier
---

# PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10406" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10406v1</a>
  <a href="https://arxiv.org/pdf/2506.10406.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10406v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10406v1', 'PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuhua Jiang, Yuwen Xiong, Yufeng Yuan, Chao Xin, Wenyuan Xu, Yu Yue, Qianchuan Zhao, Lin Yan

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPAGæ¡†æ¶ä»¥è§£å†³LLMè‡ªæˆ‘éªŒè¯ä¸ä¿®æ­£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘ä¿®æ­£` `å¼ºåŒ–å­¦ä¹ ` `ç”ŸæˆéªŒè¯å™¨` `æ¨ç†èƒ½åŠ›` `é€‰æ‹©æ€§ä¿®æ­£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨éªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æ­£ç¡®æ€§æ—¶ï¼Œå¾€å¾€ä¾èµ–ç‹¬ç«‹çš„éªŒè¯å™¨æˆ–å¤æ‚çš„å¤šé˜¶æ®µæµç¨‹ï¼Œå¯¼è‡´å¯æ‰©å±•æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„PAGæ¡†æ¶é€šè¿‡åœ¨å¤šè½®å¼ºåŒ–å­¦ä¹ ä¸­äº¤æ›¿è§’è‰²ï¼Œå…è®¸æ¨¡å‹åœ¨æ£€æµ‹åˆ°é”™è¯¯æ—¶è¿›è¡Œé€‰æ‹©æ€§ä¿®æ­£ï¼Œä»è€Œæé«˜è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPAGåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½œä¸ºæ”¿ç­–æå‡äº†ç”Ÿæˆå’Œè‡ªæˆ‘ä¿®æ­£çš„å‡†ç¡®æ€§ï¼Œä½œä¸ºéªŒè¯å™¨çš„è‡ªæˆ‘éªŒè¯æ€§èƒ½è¶…è¿‡äº†è‡ªæˆ‘ä¸€è‡´æ€§æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨å¯é éªŒè¯è‡ªèº«è¾“å‡ºçš„æ­£ç¡®æ€§æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸ä¾èµ–äºç‹¬ç«‹çš„éªŒè¯æ¨¡å‹æˆ–å¤šé˜¶æ®µè‡ªæˆ‘ä¿®æ­£è®­ç»ƒæµç¨‹ï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºäº†æ”¿ç­–ä½œä¸ºç”ŸæˆéªŒè¯å™¨ï¼ˆPAGï¼‰æ¡†æ¶ï¼Œé€šè¿‡åœ¨ç»Ÿä¸€çš„å¤šè½®å¼ºåŒ–å­¦ä¹ èŒƒå¼ä¸­äº¤æ›¿æ‰®æ¼”æ”¿ç­–å’ŒéªŒè¯å™¨è§’è‰²ï¼Œèµ‹èƒ½LLMsè‡ªæˆ‘ä¿®æ­£ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒPAGå¼•å…¥äº†é€‰æ‹©æ€§ä¿®æ­£æœºåˆ¶ï¼šæ¨¡å‹ä»…åœ¨ç”ŸæˆéªŒè¯æ­¥éª¤æ£€æµ‹åˆ°é”™è¯¯æ—¶æ‰ä¿®æ­£ç­”æ¡ˆã€‚è¿™ç§éªŒè¯-ä¿®æ­£å·¥ä½œæµç¨‹ä¸ä»…ç¼“è§£äº†æ¨¡å‹å´©æºƒé—®é¢˜ï¼Œè¿˜å…±åŒæå‡äº†æ¨ç†å’ŒéªŒè¯èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPAGåœ¨ç›´æ¥ç”Ÿæˆå’Œè‡ªæˆ‘ä¿®æ­£å‡†ç¡®æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸”è‡ªæˆ‘éªŒè¯æ€§èƒ½ä¼˜äºè‡ªæˆ‘ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªæˆ‘éªŒè¯è¾“å‡ºæ­£ç¡®æ€§æ—¶çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç‹¬ç«‹çš„éªŒè¯æ¨¡å‹æˆ–å¤æ‚çš„è®­ç»ƒæµç¨‹ï¼Œé™åˆ¶äº†å…¶åº”ç”¨çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPAGæ¡†æ¶é€šè¿‡åœ¨å¤šè½®å¼ºåŒ–å­¦ä¹ ä¸­äº¤æ›¿æ‰®æ¼”æ”¿ç­–å’ŒéªŒè¯å™¨è§’è‰²ï¼Œå…è®¸æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œè‡ªæˆ‘éªŒè¯å’Œä¿®æ­£ï¼Œåªæœ‰åœ¨æ£€æµ‹åˆ°é”™è¯¯æ—¶æ‰è¿›è¡Œä¿®æ­£ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›å’Œæ¨ç†å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPAGæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ”¿ç­–æ¨¡å—å’ŒéªŒè¯å™¨æ¨¡å—ã€‚æ”¿ç­–æ¨¡å—è´Ÿè´£ç”Ÿæˆåˆæ­¥ç­”æ¡ˆï¼ŒéªŒè¯å™¨æ¨¡å—åˆ™å¯¹ç”Ÿæˆçš„ç­”æ¡ˆè¿›è¡ŒéªŒè¯ï¼Œè‹¥å‘ç°é”™è¯¯ï¼Œåˆ™è§¦å‘ä¿®æ­£è¿‡ç¨‹ã€‚æ•´ä¸ªæµç¨‹é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œå½¢æˆä¸€ä¸ªé—­ç¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šPAGçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†é€‰æ‹©æ€§ä¿®æ­£æœºåˆ¶ï¼Œæ¨¡å‹ä»…åœ¨è‡ªæˆ‘éªŒè¯æ­¥éª¤å‘ç°é”™è¯¯æ—¶æ‰è¿›è¡Œä¿®æ­£ï¼Œè¿™ä¸ä»¥å¾€æ–¹æ³•çš„æ— å·®åˆ«ä¿®æ­£ç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å´©æºƒçš„é£é™©ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒPAGé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼ŒæŸå¤±å‡½æ•°ç»“åˆäº†ç”Ÿæˆå‡†ç¡®æ€§å’ŒéªŒè¯ä¸€è‡´æ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨è‡ªæˆ‘ä¿®æ­£æ—¶èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPAGæ¡†æ¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½œä¸ºæ”¿ç­–æ—¶ï¼Œç”Ÿæˆå’Œè‡ªæˆ‘ä¿®æ­£çš„å‡†ç¡®æ€§æ˜¾è‘—æå‡ï¼Œä½œä¸ºéªŒè¯å™¨æ—¶ï¼Œå…¶è‡ªæˆ‘éªŒè¯æ€§èƒ½è¶…è¿‡äº†ä¼ ç»Ÿçš„è‡ªæˆ‘ä¸€è‡´æ€§æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ¨ç†èƒ½åŠ›å’ŒéªŒè¯æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆå’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘éªŒè¯å’Œä¿®æ­£èƒ½åŠ›ï¼ŒPAGæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æé«˜è¿™äº›ç³»ç»Ÿçš„å¯é æ€§å’Œç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€å®¢æœç­‰å¤šä¸ªè¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks, yet they still struggle to reliably verify the correctness of their own outputs. Existing solutions to this verification challenge often depend on separate verifier models or require multi-stage self-correction training pipelines, which limit scalability. In this paper, we propose Policy as Generative Verifier (PAG), a simple and effective framework that empowers LLMs to self-correct by alternating between policy and verifier roles within a unified multi-turn reinforcement learning (RL) paradigm. Distinct from prior approaches that always generate a second attempt regardless of model confidence, PAG introduces a selective revision mechanism: the model revises its answer only when its own generative verification step detects an error. This verify-then-revise workflow not only alleviates model collapse but also jointly enhances both reasoning and verification abilities. Extensive experiments across diverse reasoning benchmarks highlight PAG's dual advancements: as a policy, it enhances direct generation and self-correction accuracy; as a verifier, its self-verification outperforms self-consistency.

