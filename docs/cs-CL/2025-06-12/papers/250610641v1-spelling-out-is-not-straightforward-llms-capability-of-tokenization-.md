---
layout: default
title: Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters
---

# Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10641" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10641v1</a>
  <a href="https://arxiv.org/pdf/2506.10641.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10641v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10641v1', 'Spelling-out is not Straightforward: LLMs\' Capability of Tokenization from Token to Characters')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tatsuya Hiraoka, Kentaro Inui

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºLLMsåœ¨å­—ç¬¦æ‹¼å†™ä¸­çš„å¤æ‚æ€§ä¸å†…éƒ¨è¡¨å¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å­—ç¬¦æ‹¼å†™` `Transformer` `çŸ¥è¯†è¡¨ç¤º` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMsåœ¨å¤„ç†å­—ç¬¦æ‹¼å†™æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¯†åˆ«æ ‡è®°çš„å¤æ‚ç»„æˆéƒ¨åˆ†æ—¶å´å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡åˆ†æLLMsçš„å†…éƒ¨è¡¨å¾ï¼Œæ­ç¤ºäº†å…¶åœ¨æ‹¼å†™è¿‡ç¨‹ä¸­å¯¹å­—ç¬¦çº§ä¿¡æ¯çš„ä¾èµ–åŠå¤„ç†æ–¹å¼ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨ä¸­é—´å’Œé«˜å±‚Transformerå±‚çš„æ‹¼å†™è¡Œä¸ºä¸­å‡ºç°æ˜¾è‘—æ”¹å–„ï¼ŒéªŒè¯äº†æˆ‘ä»¬æå‡ºçš„æœºåˆ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿä»¥é«˜å‡†ç¡®ç‡é€å­—ç¬¦æ‹¼å†™å‡ºæ ‡è®°ï¼Œä½†åœ¨æ›´å¤æ‚çš„å­—ç¬¦çº§ä»»åŠ¡ä¸­ï¼Œå¦‚è¯†åˆ«æ ‡è®°å†…çš„ç»„æˆå­ç»„ä»¶æ—¶å´è¡¨ç°ä¸ä½³ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†LLMsåœ¨æ‹¼å†™è¿‡ç¨‹ä¸­å¦‚ä½•å†…éƒ¨è¡¨ç¤ºå’Œåˆ©ç”¨å­—ç¬¦çº§ä¿¡æ¯ã€‚åˆ†æè¡¨æ˜ï¼Œå°½ç®¡æ‹¼å†™å¯¹äººç±»è€Œè¨€æ˜¯ç®€å•ä»»åŠ¡ï¼Œä½†LLMså¹¶æœªä»¥ç›´æ¥æ–¹å¼å¤„ç†æ­¤ä»»åŠ¡ã€‚å…·ä½“è€Œè¨€ï¼ŒåµŒå…¥å±‚æœªèƒ½å……åˆ†ç¼–ç å­—ç¬¦çº§ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯è¶…å‡ºç¬¬ä¸€ä¸ªå­—ç¬¦çš„éƒ¨åˆ†ã€‚å› æ­¤ï¼ŒLLMsä¾èµ–ä¸­é—´å’Œæ›´é«˜å±‚çš„Transformerå±‚æ¥é‡æ„å­—ç¬¦çº§çŸ¥è¯†ï¼Œå¹¶åœ¨æ‹¼å†™è¡Œä¸ºä¸­è§‚å¯Ÿåˆ°æ˜æ˜¾çš„â€œçªç ´â€ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ç§äº’è¡¥åˆ†æéªŒè¯äº†è¿™ä¸€æœºåˆ¶ï¼šæ¢æµ‹åˆ†ç±»å™¨ã€çŸ¥è¯†ç¥ç»å…ƒè¯†åˆ«å’Œæ³¨æ„åŠ›æƒé‡æ£€æŸ¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³LLMsåœ¨å­—ç¬¦æ‹¼å†™è¿‡ç¨‹ä¸­å¯¹å­—ç¬¦çº§ä¿¡æ¯å¤„ç†ä¸å……åˆ†çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯†åˆ«å¤æ‚æ ‡è®°ç»„æˆéƒ¨åˆ†æ—¶å­˜åœ¨å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†æLLMsçš„åµŒå…¥å±‚å’ŒTransformerå±‚ï¼Œæ­ç¤ºå…¶åœ¨æ‹¼å†™è¿‡ç¨‹ä¸­å¦‚ä½•é‡æ„å­—ç¬¦çº§çŸ¥è¯†ï¼Œå¼ºè°ƒä¸­é—´å±‚çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†æ¢æµ‹åˆ†ç±»å™¨ã€çŸ¥è¯†ç¥ç»å…ƒè¯†åˆ«å’Œæ³¨æ„åŠ›æƒé‡æ£€æŸ¥ä¸‰ç§åˆ†ææ–¹æ³•ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMsçš„æ‹¼å†™èƒ½åŠ›ä¸å†…éƒ¨æœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæ­ç¤ºäº†åµŒå…¥å±‚æœªèƒ½å……åˆ†ç¼–ç å­—ç¬¦ä¿¡æ¯çš„ç°è±¡ï¼Œå¹¶æŒ‡å‡ºä¸­é—´å±‚åœ¨æ‹¼å†™è¡Œä¸ºä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­ä½¿ç”¨äº†å¤šç§æ¢æµ‹åˆ†ç±»å™¨æ¥åˆ†ææ¨¡å‹çš„çŸ¥è¯†è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æƒé‡çš„æ£€æŸ¥æ¥è¯†åˆ«å½±å“æ‹¼å†™è¡Œä¸ºçš„å…³é”®ç¥ç»å…ƒã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ‹¼å†™ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­é—´å±‚çš„æ‹¼å†™è¡Œä¸ºä¸­è§‚å¯Ÿåˆ°æ˜æ˜¾çš„â€œçªç ´â€ã€‚é€šè¿‡ä¸‰ç§åˆ†ææ–¹æ³•çš„éªŒè¯ï¼Œæ¨¡å‹åœ¨å­—ç¬¦çº§çŸ¥è¯†é‡æ„æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æœ‰æ•ˆæå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿å°†åœ¨è®ºæ–‡ä¸­è¯¦ç»†åˆ—å‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºæ”¹è¿›LLMsåœ¨å­—ç¬¦çº§ä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†ç†è®ºåŸºç¡€ï¼Œæ½œåœ¨åº”ç”¨åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ‹¼å†™çº é”™ã€æ–‡æœ¬ç”Ÿæˆå’Œä¿¡æ¯æå–ç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„å†…éƒ¨è¡¨å¾ï¼Œæœªæ¥å¯èƒ½æå‡LLMsåœ¨å¤æ‚è¯­è¨€ä»»åŠ¡ä¸­çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) can spell out tokens character by character with high accuracy, yet they struggle with more complex character-level tasks, such as identifying compositional subcomponents within tokens. In this work, we investigate how LLMs internally represent and utilize character-level information during the spelling-out process. Our analysis reveals that, although spelling out is a simple task for humans, it is not handled in a straightforward manner by LLMs. Specifically, we show that the embedding layer does not fully encode character-level information, particularly beyond the first character. As a result, LLMs rely on intermediate and higher Transformer layers to reconstruct character-level knowledge, where we observe a distinct "breakthrough" in their spelling behavior. We validate this mechanism through three complementary analyses: probing classifiers, identification of knowledge neurons, and inspection of attention weights.

