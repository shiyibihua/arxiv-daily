---
layout: default
title: AC/DC: LLM-based Audio Comprehension via Dialogue Continuation
---

# AC/DC: LLM-based Audio Comprehension via Dialogue Continuation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10312" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10312v1</a>
  <a href="https://arxiv.org/pdf/2506.10312.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10312v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10312v1', 'AC/DC: LLM-based Audio Comprehension via Dialogue Continuation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yusuke Fujita, Tomoya Mizumoto, Atsushi Kojima, Lianbo Liu, Yui Sudo

**åˆ†ç±»**: eess.AS, cs.CL, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: Accepted to Interspeech 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¯¹è¯å»¶ç»­çš„éŸ³é¢‘ç†è§£æ¨¡å‹ä»¥è§£å†³æŒ‡ä»¤è·Ÿéšé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘ç†è§£` `å¯¹è¯ç”Ÿæˆ` `æŒ‡ä»¤è·Ÿéš` `å¤šæ¨¡æ€å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³é¢‘ç†è§£æ¨¡å‹åœ¨å¤„ç†å­—å¹•å˜å¼‚é—®é¢˜æ—¶å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰å­—å¹•çš„æ·±å±‚å«ä¹‰ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¯¹è¯å»¶ç»­è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶åœ¨è¾“å…¥å­—å¹•åç”Ÿæˆå¯¹è¯å“åº”ï¼Œä»è€Œå‡è½»å­—å¹•å˜å¼‚é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é›¶-shotæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ä¸€ç§éµå¾ªæŒ‡ä»¤çš„éŸ³é¢‘ç†è§£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹è¯å»¶ç»­èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å¹¶éç›´æ¥ç”Ÿæˆè®­ç»ƒæ•°æ®ä¸­çš„ç›®æ ‡å­—å¹•ï¼Œè€Œæ˜¯è®­ç»ƒæ¨¡å‹åœ¨è¾“å…¥å­—å¹•è§¦å‘å¯¹è¯æ—¶ç”Ÿæˆå“åº”ã€‚è¿™ç§å¯¹è¯å»¶ç»­è®­ç»ƒå‡è½»äº†å­—å¹•å˜å¼‚é—®é¢˜ï¼Œæœ‰æ•ˆæ•æ‰å­—å¹•çš„æ·±å±‚å«ä¹‰ã€‚ç»“æœï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ²¡æœ‰å¤šä»»åŠ¡æŒ‡ä»¤è°ƒä¼˜çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°é›¶-shotæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œå³ä½¿ä»…åœ¨éŸ³é¢‘å­—å¹•æ•°æ®é›†ä¸Šè®­ç»ƒã€‚é€šè¿‡åœ¨AudioCapsã€WavCapså’ŒClothoæ•°æ®é›†ä¸Šçš„å®éªŒï¼Œä»¥åŠAudioBenchéŸ³é¢‘åœºæ™¯é—®ç­”æµ‹è¯•ï¼Œå±•ç¤ºäº†æˆ‘ä»¬æ¨¡å‹è·Ÿéšå„ç§æœªè§æŒ‡ä»¤çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³éŸ³é¢‘ç†è§£æ¨¡å‹åœ¨å¤„ç†å­—å¹•å˜å¼‚æ—¶çš„å±€é™æ€§ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæ•æ‰å­—å¹•çš„æ·±å±‚å«ä¹‰ï¼Œå¯¼è‡´æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¯¹è¯å»¶ç»­çš„æ–¹å¼è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶åœ¨æ¥æ”¶åˆ°å­—å¹•è¾“å…¥åï¼Œèƒ½å¤Ÿç”Ÿæˆç›¸åº”çš„å¯¹è¯å†…å®¹ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å’Œæ•æ‰å­—å¹•çš„å®é™…å«ä¹‰ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿäººç±»å¯¹è¯çš„æ–¹å¼ï¼Œæå‡æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥éŸ³é¢‘æ•°æ®å’Œå¯¹åº”å­—å¹•ï¼Œé€šè¿‡å¯¹è¯å»¶ç»­è®­ç»ƒï¼Œæ¨¡å‹ç”Ÿæˆå¯¹è¯å“åº”ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬éŸ³é¢‘ç‰¹å¾æå–ã€å¯¹è¯ç”Ÿæˆç½‘ç»œå’ŒæŒ‡ä»¤ç†è§£æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡å¯¹è¯å»¶ç»­è®­ç»ƒæ¥è§£å†³å­—å¹•å˜å¼‚é—®é¢˜ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç›´æ¥ç”Ÿæˆå­—å¹•çš„æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…å¾€å¾€å¿½è§†äº†å­—å¹•çš„æ·±å±‚è¯­ä¹‰ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¯¹è¯ç”Ÿæˆçš„è´¨é‡ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”éŸ³é¢‘ç‰¹å¾çš„è¾“å…¥å’Œå¯¹è¯ç”Ÿæˆçš„è¾“å‡ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨AudioCapsã€WavCapså’ŒClothoæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿåœ¨æœªè§æŒ‡ä»¤çš„æƒ…å†µä¸‹å®ç°é«˜è¾¾85%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸéŸ³é¢‘ç†è§£æ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½éŸ³é¢‘åŠ©æ‰‹ã€è‡ªåŠ¨å­—å¹•ç”Ÿæˆå’ŒéŸ³é¢‘å†…å®¹æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡éŸ³é¢‘ç†è§£çš„å‡†ç¡®æ€§ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§åœºæ™¯ä¸­æä¾›æ›´ä¸ºè‡ªç„¶çš„ç”¨æˆ·äº¤äº’ä½“éªŒï¼Œæœªæ¥å¯èƒ½å¯¹äººæœºäº¤äº’å’Œä¿¡æ¯æ£€ç´¢é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose an instruction-following audio comprehension model that leverages the dialogue continuation ability of large language models (LLMs). Instead of directly generating target captions in training data, the proposed method trains a model to produce responses as if the input caption triggered a dialogue. This dialogue continuation training mitigates the caption variation problem. Learning to continue a dialogue effectively captures the caption's meaning beyond its surface-level words. As a result, our model enables zero-shot instruction-following capability without multitask instruction tuning, even trained solely on audio captioning datasets. Experiments on AudioCaps, WavCaps, and Clotho datasets with AudioBench audio-scene question-answering tests demonstrate our model's ability to follow various unseen instructions.

