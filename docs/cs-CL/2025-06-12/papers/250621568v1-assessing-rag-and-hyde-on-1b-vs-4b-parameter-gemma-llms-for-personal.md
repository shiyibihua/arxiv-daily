---
layout: default
title: Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion
---

# Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21568" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21568v1</a>
  <a href="https://arxiv.org/pdf/2506.21568.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21568v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21568v1', 'Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrejs Sorstkins

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: Technical report as part of research project

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°RAGä¸HyDEåœ¨Gemma LLMsä¸­çš„åº”ç”¨ä»¥æå‡ä¸ªäººåŠ©æ‰‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å‡è®¾æ–‡æ¡£åµŒå…¥` `ä¸ªäººåŠ©æ‰‹` `éšç§ä¿æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. èµ„æºæ•ˆç‡æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜å’Œéšç§æ•æ„Ÿåº”ç”¨ä¸­çš„ä¸»è¦æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ»¡è¶³è¿™äº›éœ€æ±‚ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†RAGå’ŒHyDEä¸¤ç§å¢å¼ºç­–ç•¥ï¼Œæ—¨åœ¨æå‡Gemma LLMsåœ¨ä¸ªäººåŠ©æ‰‹ä¸­çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éšç§ä¿æŠ¤æ–¹é¢ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAGæ˜¾è‘—é™ä½äº†å“åº”å»¶è¿Ÿå¹¶æ¶ˆé™¤äº†å¹»è§‰ï¼Œè€ŒHyDEåˆ™æé«˜äº†è¯­ä¹‰ç›¸å…³æ€§ï¼Œä½†å¢åŠ äº†å“åº”æ—¶é—´å’Œå¹»è§‰ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

èµ„æºæ•ˆç‡æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºè¾¹ç¼˜å’Œéšç§æ•æ„Ÿåœºæ™¯çš„å…³é”®éšœç¢ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸¤ç§å¢å¼ºç­–ç•¥â€”â€”æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå‡è®¾æ–‡æ¡£åµŒå…¥ï¼ˆHyDEï¼‰â€”â€”åœ¨1äº¿å’Œ4äº¿å‚æ•°çš„ç´§å‡‘å‹Gemma LLMsä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éšç§ä¼˜å…ˆçš„ä¸ªäººåŠ©æ‰‹ç¯å¢ƒä¸­ã€‚é€šè¿‡MongoDBå®ç°çŸ­æœŸè®°å¿†ï¼Œé€šè¿‡Qdrantå®ç°é•¿æœŸè¯­ä¹‰å­˜å‚¨ï¼Œåˆ©ç”¨FastAPIå’ŒLangChainè¿›è¡Œåè°ƒï¼Œå¹¶é€šè¿‡React.jså‰ç«¯è¿›è¡Œç³»ç»Ÿå±•ç¤ºã€‚ç»“æœè¡¨æ˜ï¼ŒRAGåœ¨å“åº”ç”¨æˆ·ç‰¹å®šå’Œé¢†åŸŸç‰¹å®šæŸ¥è¯¢æ—¶ï¼Œå»¶è¿Ÿå‡å°‘äº†17%ï¼Œå¹¶æ¶ˆé™¤äº†äº‹å®å¹»è§‰ï¼›è€ŒHyDEåˆ™å¢å¼ºäº†è¯­ä¹‰ç›¸å…³æ€§ï¼Œä½†å“åº”æ—¶é—´å¢åŠ äº†25-40%ï¼Œå¹¶åœ¨ä¸ªäººæ•°æ®æ£€ç´¢ä¸­å­˜åœ¨ä¸å¯å¿½è§†çš„å¹»è§‰ç‡ã€‚æ¯”è¾ƒ1äº¿ä¸4äº¿æ¨¡å‹ï¼Œå‘ç°æ‰©å±•å¯¹åŸºçº¿å’ŒRAGç®¡é“çš„ååé‡æå‡æœ‰é™ï¼Œä½†åŠ å¤§äº†HyDEçš„è®¡ç®—å¼€é”€å’Œå˜å¼‚æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒRAGæ˜¯å°è§„æ¨¡LLMsé©±åŠ¨çš„è®¾å¤‡ç«¯ä¸ªäººåŠ©æ‰‹çš„åŠ¡å®é€‰æ‹©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜å’Œéšç§æ•æ„Ÿåº”ç”¨ä¸­çš„èµ„æºæ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç”¨æˆ·ç‰¹å®šå’Œé¢†åŸŸç‰¹å®šæŸ¥è¯¢æ—¶ï¼Œå­˜åœ¨å»¶è¿Ÿå’Œäº‹å®å¹»è§‰ç­‰ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸¤ç§å¢å¼ºç­–ç•¥ï¼ŒRAGé€šè¿‡æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å‡å°‘å»¶è¿Ÿå’Œå¹»è§‰ï¼ŒHyDEé€šè¿‡å‡è®¾æ–‡æ¡£åµŒå…¥æé«˜è¯­ä¹‰ç›¸å…³æ€§ï¼Œæ—¨åœ¨æå‡ä¸ªäººåŠ©æ‰‹çš„å“åº”è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬çŸ­æœŸè®°å¿†å­˜å‚¨ï¼ˆMongoDBï¼‰ã€é•¿æœŸè¯­ä¹‰å­˜å‚¨ï¼ˆQdrantï¼‰ã€åè°ƒæœåŠ¡ï¼ˆFastAPIå’ŒLangChainï¼‰ä»¥åŠå‰ç«¯å±•ç¤ºï¼ˆReact.jsï¼‰ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„ä¸ªäººåŠ©æ‰‹ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šRAGåœ¨å‡å°‘å»¶è¿Ÿå’Œæ¶ˆé™¤å¹»è§‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒHyDEåˆ™åœ¨å¤æ‚æŸ¥è¯¢ä¸­æå‡äº†è¯­ä¹‰ç›¸å…³æ€§ï¼ŒäºŒè€…çš„ç»“åˆä¸ºå°è§„æ¨¡LLMsçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒRAGé€šè¿‡ä¼˜åŒ–æ£€ç´¢æœºåˆ¶æ¥é™ä½å“åº”æ—¶é—´ï¼Œè€ŒHyDEåˆ™é€šè¿‡å¤æ‚çš„åµŒå…¥ç­–ç•¥æ¥å¢å¼ºè¯­ä¹‰ç†è§£ï¼ŒäºŒè€…çš„è®¾è®¡å‡è€ƒè™‘äº†åœ¨éšç§æ•æ„Ÿç¯å¢ƒä¸­çš„åº”ç”¨éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAGåœ¨å“åº”ç”¨æˆ·ç‰¹å®šå’Œé¢†åŸŸç‰¹å®šæŸ¥è¯¢æ—¶ï¼Œå»¶è¿Ÿå‡å°‘äº†17%ï¼Œå¹¶æ¶ˆé™¤äº†äº‹å®å¹»è§‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒHyDEåœ¨å¤æ‚ç‰©ç†æç¤ºä¸­å¢å¼ºäº†è¯­ä¹‰ç›¸å…³æ€§ï¼Œä½†å“åº”æ—¶é—´å¢åŠ äº†25-40%ï¼Œå¹¶å­˜åœ¨ä¸€å®šçš„å¹»è§‰ç‡ã€‚æ•´ä½“ä¸Šï¼ŒRAGè¢«å®šä½ä¸ºå°è§„æ¨¡LLMsé©±åŠ¨çš„ä¸ªäººåŠ©æ‰‹çš„æœ€ä½³é€‰æ‹©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ä¸ªäººåŠ©æ‰‹ã€éšç§ä¿æŠ¤çš„å¯¹è¯ç³»ç»Ÿä»¥åŠè¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸­çš„è‡ªç„¶è¯­è¨€å¤„ç†ã€‚é€šè¿‡æå‡èµ„æºæ•ˆç‡å’Œå“åº”è´¨é‡ï¼Œç ”ç©¶æˆæœèƒ½å¤Ÿåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶ï¼Œæä¾›æ›´ä¸ºæ™ºèƒ½çš„äº¤äº’ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Resource efficiency is a critical barrier to deploying large language models (LLMs) in edge and privacy-sensitive applications. This study evaluates the efficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG) and Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion and 4 billion parameters, within the context of a privacy-first personal assistant. We implement short-term memory via MongoDB and long-term semantic storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the system through a React.js frontend. Across both model scales, RAG consistently reduces latency by up to 17\% and eliminates factual hallucinations when responding to user-specific and domain-specific queries. HyDE, by contrast, enhances semantic relevance--particularly for complex physics prompts--but incurs a 25--40\% increase in response time and a non-negligible hallucination rate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that scaling yields marginal throughput gains for baseline and RAG pipelines, but magnifies HyDE's computational overhead and variability. Our findings position RAG as the pragmatic choice for on-device personal assistants powered by small-scale LLMs.

