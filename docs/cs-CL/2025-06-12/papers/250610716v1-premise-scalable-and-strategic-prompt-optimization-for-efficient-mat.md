---
layout: default
title: PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models
---

# PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10716" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.10716v1</a>
  <a href="https://arxiv.org/pdf/2506.10716.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10716v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10716v1', 'PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ye Yu, Yaoning Yu, Haohan Wang

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-12

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PREMISE‰ª•Ëß£ÂÜ≥Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÂÜó‰ΩôËÆ°ÁÆóÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãÊé®ÁêÜÊ®°Âûã` `Êï∞Â≠¶Êé®ÁêÜ` `ÊèêÁ§∫‰ºòÂåñ` `Â§öÁõÆÊ†á‰ºòÂåñ` `Êé®ÁêÜÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÊé®ÁêÜËøáÁ®ãÂÜóÈïøÔºåÂØºËá¥ËµÑÊ∫êÊµ™Ë¥πÂíåÊàêÊú¨Â¢ûÂä†„ÄÇ
2. PREMISEÈÄöËøá‰ºòÂåñÊèêÁ§∫ÔºåÂáèÂ∞ëÂÜó‰ΩôËÆ°ÁÆóÔºå‰øùÊåÅÊé®ÁêÜÂáÜÁ°ÆÊÄßÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ´òÊïàÊé®ÁêÜÊñπÊ≥ï„ÄÇ
3. Âú®Â§ö‰∏™Êï∞Â≠¶Âü∫ÂáÜÊµãËØï‰∏≠ÔºåPREMISEÂÆûÁé∞‰∫ÜÈ´òËææ87.5%ÁöÑ‰ª§ÁâåÂáèÂ∞ëÔºåÂêåÊó∂‰øùÊåÅÊàñÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂ¶ÇClaude 3.7 SonnetÂíåOpenAI o1Âú®Êï∞Â≠¶Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰ΩÜÂÖ∂Êé®ÁêÜËøáÁ®ãÂæÄÂæÄÂÜóÈïøÔºåÂØºËá¥‰ª§Áâå‰ΩøÁî®ÈáèÂíåÊàêÊú¨Â¢ûÂä†ÔºåÈôêÂà∂‰∫ÜÂú®Âª∂ËøüÊïèÊÑüÊàñAPIÂèóÈôêÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜPREMISEÔºàÂü∫‰∫éÊèêÁ§∫ÁöÑÈ´òÊïàÊï∞Â≠¶Êé®ÁêÜ‰∏éÊàòÁï•ËØÑ‰º∞ÔºâÔºå‰∏Ä‰∏™‰ªÖÂü∫‰∫éÊèêÁ§∫ÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÂáèÂ∞ëÊé®ÁêÜÂºÄÈîÄËÄå‰∏ç‰øÆÊîπÊ®°ÂûãÊùÉÈáç„ÄÇPREMISEÁªìÂêà‰∫ÜËøΩË∏™Á∫ßËØäÊñ≠‰∏éÊ¢ØÂ∫¶ÂêØÂèëÂºèÁöÑÊèêÁ§∫‰ºòÂåñÔºåÊó®Âú®Âú®‰øùÊåÅÁ≠îÊ°àÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÔºåÊúÄÂ∞èÂåñÂÜó‰ΩôËÆ°ÁÆó„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ§öÁõÆÊ†áÊñáÊú¨ÊêúÁ¥¢ÂÖ±Âêå‰ºòÂåñÁÆÄÊ¥ÅÊÄßÂíåÊ≠£Á°ÆÊÄßÔºåÂπ≥Ë°°‰ª§ÁâåÈïøÂ∫¶ÂíåÁ≠îÊ°àÊúâÊïàÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPREMISEÂú®GSM8K„ÄÅSVAMPÂíåMath500‰∏äÂåπÈÖçÊàñË∂ÖË∂äÂü∫Á∫øÂáÜÁ°ÆÁéáÔºåÂêåÊó∂Â∞ÜÊé®ÁêÜ‰ª§ÁâåÂáèÂ∞ëÈ´òËææ87.5%ÔºåÊàêÊú¨Èôç‰Ωé69%Ëá≥82%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏≠Êé®ÁêÜËøáÁ®ãÂÜóÈïøÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂØºËá¥‰ª§Áâå‰ΩøÁî®ÈáèÂíåÊàêÊú¨Â¢ûÂä†ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöPREMISEÈÄöËøá‰ªÖ‰ºòÂåñÊèêÁ§∫ÔºåÁªìÂêàËøΩË∏™Á∫ßËØäÊñ≠‰∏éÊ¢ØÂ∫¶ÂêØÂèëÂºèÁöÑ‰ºòÂåñÊñπÊ≥ïÔºåÊó®Âú®ÂáèÂ∞ëÂÜó‰ΩôËÆ°ÁÆóÔºåÂêåÊó∂‰øùÊåÅÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ§öÁõÆÊ†áÊñáÊú¨ÊêúÁ¥¢ÔºåÂπ≥Ë°°‰ª§ÁâåÈïøÂ∫¶ÂíåÁ≠îÊ°àÊúâÊïàÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPREMISEÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÊèêÁ§∫‰ºòÂåñÊ®°Âùó„ÄÅËøΩË∏™Á∫ßËØäÊñ≠Ê®°ÂùóÂíåÂ§öÁõÆÊ†áÊêúÁ¥¢Ê®°Âùó„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáËØäÊñ≠ÂàÜÊûêÊé®ÁêÜËøáÁ®ãÔºåËØÜÂà´ÂÜó‰ΩôÈÉ®ÂàÜÔºåÁÑ∂ÂêéËøõË°åÊèêÁ§∫‰ºòÂåñÔºåÊúÄÂêéÈÄöËøáÂ§öÁõÆÊ†áÊêúÁ¥¢ÂÆûÁé∞ÁÆÄÊ¥ÅÊÄß‰∏éÊ≠£Á°ÆÊÄßÁöÑÂπ≥Ë°°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPREMISEÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ÂçïÊ¨°ÈªëÁÆ±Êé•Âè£ÁöÑËøêË°åÊñπÂºèÔºå‰ΩøÂÖ∂ËÉΩÂ§üÁõ¥Êé•Â∫îÁî®‰∫éÂïÜ‰∏öÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÔºåËÄåÊó†ÈúÄ‰øÆÊîπÊ®°ÂûãÊùÉÈáç„ÄÇ‰∏é‰ª•ÂæÄÊñπÊ≥ïÁõ∏ÊØîÔºåPREMISEÂú®Êé®ÁêÜÊïàÁéáÂíåÂáÜÁ°ÆÊÄß‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÊèêÂçá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåPREMISEÈááÁî®‰∫ÜÂ§öÁõÆÊ†á‰ºòÂåñÁ≠ñÁï•ÔºåËÆæÁΩÆ‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•Âπ≥Ë°°Êé®ÁêÜÁöÑÁÆÄÊ¥ÅÊÄß‰∏éÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂ÈÄöËøáËøΩË∏™Á∫ßËØäÊñ≠Êù•ÊåáÂØº‰ºòÂåñËøáÁ®ãÔºåÁ°Æ‰øùÊúÄÁªàÁªìÊûúÁöÑÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®GSM8K„ÄÅSVAMPÂíåMath500Âü∫ÂáÜÊµãËØï‰∏≠ÔºåPREMISEÂÆûÁé∞‰∫ÜÈ´òËææ87.5%ÁöÑÊé®ÁêÜ‰ª§ÁâåÂáèÂ∞ëÔºåÂêåÊó∂Âú®ClaudeÊ®°Âûã‰∏ä‰øùÊåÅ96%ÁöÑÂáÜÁ°ÆÁéáÔºåÂú®GeminiÊ®°Âûã‰∏äÊèêÈ´òËá≥92%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåPREMISEÂú®Êé®ÁêÜÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÊñπÈù¢ÂùáÊúâÊòæËëóÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂ÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PREMISEÁöÑÁ†îÁ©∂ÊàêÊûúÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÈ´òÊïàÊé®ÁêÜÁöÑÂú∫ÊôØ‰∏≠ÔºåÂ¶ÇÊïôËÇ≤„ÄÅÈáëËûçÂàÜÊûêÂíåÂÆûÊó∂ÂÜ≥Á≠ñÊîØÊåÅÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÂáèÂ∞ëÊé®ÁêÜÂºÄÈîÄÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÈôç‰ΩéÊàêÊú¨Âπ∂ÊèêÈ´òÂìçÂ∫îÈÄüÂ∫¶ÔºåÈÄÇÁî®‰∫éAPIÂèóÈôêÊàñÂª∂ËøüÊïèÊÑüÁöÑÂ∫îÁî®ÁéØÂ¢ÉÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êõ¥Â§öÊô∫ËÉΩÁ≥ªÁªüÁöÑÊôÆÂèä‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve strong performance on mathematical benchmarks using lengthy chain-of-thought (CoT) reasoning, but the resulting traces are often unnecessarily verbose. This inflates token usage and cost, limiting deployment in latency-sensitive or API-constrained settings. We introduce PREMISE (PRompt-based Efficient Mathematical Inference with Strategic Evaluation), a prompt-only framework that reduces reasoning overhead without modifying model weights. PREMISE combines trace-level diagnostics with gradient-inspired prompt optimization to minimize redundant computation while preserving answer accuracy. The approach jointly optimizes brevity and correctness through a multi-objective textual search that balances token length and answer validity. Unlike prior work, PREMISE runs in a single-pass black-box interface, so it can be applied directly to commercial LLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy ($96\%\rightarrow96\%$ with Claude, $91\%\rightarrow92\%$ with Gemini) while reducing reasoning tokens by up to $87.5\%$ and cutting dollar cost by $69$--$82\%$. These results show that prompt-level optimization is a practical and scalable path to efficient LRM inference without compromising reasoning quality.

