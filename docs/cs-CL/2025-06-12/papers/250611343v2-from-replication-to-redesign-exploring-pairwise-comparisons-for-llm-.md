---
layout: default
title: From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review
---

# From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11343" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11343v2</a>
  <a href="https://arxiv.org/pdf/2506.11343.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11343v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11343v2', 'From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yaohui Zhang, Haijing Zhang, Wenlong Ji, Tianyu Hua, Nick Haber, Hancheng Cao, Weixin Liang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12 (æ›´æ–°: 2025-09-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLLMçš„æˆå¯¹æ¯”è¾ƒæœºåˆ¶ä»¥ä¼˜åŒ–åŒè¡Œè¯„å®¡æµç¨‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åŒè¡Œè¯„å®¡` `æˆå¯¹æ¯”è¾ƒ` `å­¦æœ¯è¯„ä¼°` `è¯„å®¡æœºåˆ¶` `ç ”ç©¶å…¬å¹³æ€§` `å¤šæ ·æ€§æŒ‘æˆ˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒè¡Œè¯„å®¡æ–¹æ³•ä¸»è¦ä¾èµ–äººç±»è¯„å®¡è€…ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨LLMsçš„æ½œåŠ›ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œè¯„å®¡è´¨é‡ä¸å‡ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡LLMä»£ç†è¿›è¡Œæ‰‹ç¨¿çš„æˆå¯¹æ¯”è¾ƒï¼Œèšç„¦äºç›¸å¯¹è´¨é‡è¯„ä¼°ï¼Œè€Œéå•ä¸€è¯„åˆ†ï¼Œæ—¨åœ¨æé«˜è¯„å®¡çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è¯†åˆ«é«˜å½±å“åŠ›è®ºæ–‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿè¯„åˆ†æ–¹æ³•ï¼Œæå‡äº†è¯„å®¡çš„æœ‰æ•ˆæ€§ï¼Œä½†ä¹Ÿæš´éœ²å‡ºé€‰æ‹©è¿‡ç¨‹ä¸­çš„åè§é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°ä¸ºé‡æ–°æ„æƒ³åŒè¡Œè¯„å®¡æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šã€‚å°½ç®¡å¦‚æ­¤ï¼Œä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å°†LLMsä½œä¸ºäººç±»è¯„å®¡è€…çš„ç›´æ¥æ›¿ä»£å“ï¼Œè€Œå¯¹å¦‚ä½•æ ¹æœ¬æ€§åœ°é‡æ–°æ€è€ƒLLMsåœ¨å­¦æœ¯è¯„å®¡è¿‡ç¨‹ä¸­çš„å‚ä¸æ–¹å¼å…³æ³¨è¾ƒå°‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æœºåˆ¶ï¼Œåˆ©ç”¨LLMä»£ç†å¯¹æ‰‹ç¨¿è¿›è¡Œæˆå¯¹æ¯”è¾ƒï¼Œè€Œéå•ç‹¬è¯„åˆ†ã€‚é€šè¿‡æ±‡æ€»å¤§é‡çš„æˆå¯¹è¯„ä¼°ç»“æœï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®ã€ç¨³å¥åœ°è¡¡é‡æ‰‹ç¨¿çš„ç›¸å¯¹è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ¯”è¾ƒæ–¹æ³•åœ¨è¯†åˆ«é«˜å½±å“åŠ›è®ºæ–‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿè¯„åˆ†æ–¹æ³•ã€‚ç„¶è€Œï¼Œåˆ†æä¹Ÿæ­ç¤ºäº†é€‰æ‹©è¿‡ç¨‹ä¸­çš„æ–°å…´åè§ï¼Œå°¤å…¶æ˜¯ç ”ç©¶ä¸»é¢˜çš„æ–°é¢–æ€§é™ä½å’Œæœºæ„é—´çš„ä¸å¹³è¡¡ã€‚è¿™äº›å‘ç°çªæ˜¾äº†åˆ©ç”¨LLMsé‡æ–°æ€è€ƒåŒè¡Œè¯„å®¡çš„å˜é©æ½œåŠ›åŠæœªæ¥ç³»ç»Ÿå¿…é¡»è§£å†³çš„å…¬å¹³æ€§å’Œå¤šæ ·æ€§æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸåŒè¡Œè¯„å®¡ä¸­æ•ˆç‡ä½ä¸‹å’Œè¯„å®¡è´¨é‡ä¸å‡çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–äººç±»è¯„å®¡è€…ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨LLMsçš„ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåˆ©ç”¨LLMä»£ç†è¿›è¡Œæ‰‹ç¨¿çš„æˆå¯¹æ¯”è¾ƒï¼Œé€šè¿‡èšåˆæˆå¯¹è¯„ä¼°ç»“æœï¼Œæä¾›æ›´å‡†ç¡®çš„ç›¸å¯¹è´¨é‡è¡¡é‡ï¼Œæ—¨åœ¨çªç ´ä¼ ç»Ÿè¯„åˆ†çš„å±€é™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æˆå¯¹æ¯”è¾ƒè¯„ä¼°ã€ç»“æœèšåˆå’Œè´¨é‡è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†æ‰‹ç¨¿æ•°æ®ï¼Œç„¶åé€šè¿‡LLMè¿›è¡Œæˆå¯¹æ¯”è¾ƒï¼Œæœ€åèšåˆè¯„ä¼°ç»“æœä»¥å¾—å‡ºç›¸å¯¹è´¨é‡è¯„åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥æˆå¯¹æ¯”è¾ƒæœºåˆ¶ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„å•ä¸€è¯„åˆ†æ–¹æ³•ï¼Œè¿™ä¸€æ–¹æ³•èƒ½å¤Ÿæ›´å…¨é¢åœ°åæ˜ æ‰‹ç¨¿çš„ç›¸å¯¹è´¨é‡ï¼Œå‡å°‘ä¸»è§‚åå·®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œè®¾è®¡äº†ç‰¹å®šçš„æ¯”è¾ƒç®—æ³•ï¼Œè®¾ç½®äº†é€‚å½“çš„å‚æ•°ä»¥ä¼˜åŒ–è¯„ä¼°è¿‡ç¨‹ï¼ŒåŒæ—¶è€ƒè™‘äº†æŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œä»¥ç¡®ä¿è¯„ä¼°ç»“æœçš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæˆå¯¹æ¯”è¾ƒæ–¹æ³•åœ¨è¯†åˆ«é«˜å½±å“åŠ›è®ºæ–‡æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿè¯„åˆ†æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚åŒæ—¶ï¼Œç ”ç©¶ä¹Ÿæ­ç¤ºäº†åœ¨é€‰æ‹©è¿‡ç¨‹ä¸­å‡ºç°çš„æ–°å…´åè§ï¼Œå°¤å…¶æ˜¯åœ¨ç ”ç©¶ä¸»é¢˜çš„æ–°é¢–æ€§å’Œæœºæ„é—´çš„å¹³è¡¡æ€§æ–¹é¢ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å­¦æœ¯æœŸåˆŠçš„åŒè¡Œè¯„å®¡æµç¨‹ã€ç§‘ç ”æœºæ„çš„è®ºæ–‡ç­›é€‰ä»¥åŠå­¦æœ¯ä¼šè®®çš„è®ºæ–‡è¯„å®¡ç­‰ã€‚é€šè¿‡å¼•å…¥LLMçš„æˆå¯¹æ¯”è¾ƒæœºåˆ¶ï¼Œå¯ä»¥æé«˜è¯„å®¡æ•ˆç‡å’Œè´¨é‡ï¼Œä¿ƒè¿›å­¦æœ¯ç•Œçš„å…¬å¹³æ€§ä¸å¤šæ ·æ€§ï¼Œæœªæ¥å¯èƒ½å¯¹å­¦æœ¯å‡ºç‰ˆå’Œç ”ç©¶è¯„ä¼°äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows. Despite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process. In this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring. By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality. Our experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers. However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance. These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.

