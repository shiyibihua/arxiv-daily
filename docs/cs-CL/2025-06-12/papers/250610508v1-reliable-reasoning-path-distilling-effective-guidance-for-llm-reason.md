---
layout: default
title: Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs
---

# Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10508" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10508v1</a>
  <a href="https://arxiv.org/pdf/2506.10508.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10508v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10508v1', 'Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yilin Xiao, Chuang Zhou, Qinggang Zhang, Bo Li, Qing Li, Xiao Huang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRRPæ¡†æ¶ä»¥è§£å†³LLMæ¨ç†ä¸­çš„çŸ¥è¯†å›¾è°±è·¯å¾„æå–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†å›¾è°±` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†è·¯å¾„` `å…³ç³»åµŒå…¥` `åŒå‘åˆ†å¸ƒå­¦ä¹ ` `é‡æ€æ¨¡å—` `çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰KGå¢å¼ºLLMsä¸»è¦å…³æ³¨äº‹å®çŸ¥è¯†ï¼Œä½†åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶ä»è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ¨ç†è·¯å¾„æå–ã€‚
2. æœ¬æ–‡æå‡ºRRPæ¡†æ¶ï¼Œç»“åˆLLMsçš„è¯­ä¹‰ä¼˜åŠ¿ä¸çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯ï¼Œä¼˜åŒ–æ¨ç†è·¯å¾„çš„æå–ä¸è¯„ä¼°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRRPåœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ¨ç†èƒ½åŠ›æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­å¸¸å¸¸é¢ä¸´èƒŒæ™¯çŸ¥è¯†ä¸è¶³å’Œå¹»è§‰å€¾å‘çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œç ”ç©¶è€…ä»¬ç§¯ææ¢ç´¢å°†çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ä¸LLMsç»“åˆçš„æ–¹å¼ã€‚ç°æœ‰çš„KGå¢å¼ºLLMsä¸»è¦å…³æ³¨è¡¥å……äº‹å®çŸ¥è¯†ï¼Œä½†åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶ä»ç„¶å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºRRPæ¡†æ¶ï¼Œé€šè¿‡å…³ç³»åµŒå…¥å’ŒåŒå‘åˆ†å¸ƒå­¦ä¹ ï¼Œç»“åˆLLMsçš„è¯­ä¹‰ä¼˜åŠ¿ä¸ç»“æ„ä¿¡æ¯ï¼Œæå–å¯é çš„æ¨ç†è·¯å¾„ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­å¼•å…¥äº†é‡æ€æ¨¡å—ï¼Œæ ¹æ®æ¨ç†è·¯å¾„çš„é‡è¦æ€§è¿›è¡Œè¯„ä¼°å’Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRRPåœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥æ–¹ä¾¿åœ°é›†æˆåˆ°å„ç§LLMsä¸­ï¼Œå¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­æ¨ç†è·¯å¾„æå–çš„å›°éš¾ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚é—®é¢˜ä¸Šè¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æœ‰æ•ˆåŒºåˆ†æœ‰ç”¨ä¸å†—ä½™çš„æ¨ç†è·¯å¾„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºRRPæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆLLMsçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ä¸çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯ï¼Œæå–å¹¶ä¼˜åŒ–æ¨ç†è·¯å¾„ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRRPæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šçŸ¥è¯†å›¾è°±æŒ–æ˜æ¨¡å—å’Œé‡æ€æ¨¡å—ã€‚å‰è€…è´Ÿè´£ä»çŸ¥è¯†å›¾è°±ä¸­æå–æ¨ç†è·¯å¾„ï¼Œåè€…åˆ™æ ¹æ®è·¯å¾„çš„é‡è¦æ€§è¿›è¡Œè¯„ä¼°å’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šRRPæ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†é‡æ€æ¨¡å—ï¼Œèƒ½å¤ŸåŠ¨æ€è¯„ä¼°æ¨ç†è·¯å¾„çš„æœ‰æ•ˆæ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒRRPä½¿ç”¨å…³ç³»åµŒå…¥æŠ€æœ¯æ¥æ•æ‰çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨åŒå‘åˆ†å¸ƒå­¦ä¹ æ¥å¢å¼ºè·¯å¾„çš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ï¼Œè®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨ç†è·¯å¾„çš„é€‰æ‹©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRRPåœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œæ¨ç†å‡†ç¡®ç‡æå‡äº†çº¦15%ã€‚è¯¥æ¡†æ¶çš„çµæ´»æ€§ä½¿å…¶èƒ½å¤Ÿè½»æ¾é›†æˆåˆ°å¤šç§LLMsä¸­ï¼Œè¿›ä¸€æ­¥å¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†æ¨ç†å’Œè‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒç­‰ã€‚é€šè¿‡æå‡LLMsçš„æ¨ç†èƒ½åŠ›ï¼ŒRRPæ¡†æ¶èƒ½å¤Ÿä¸ºå¤æ‚é—®é¢˜æä¾›æ›´ä¸ºå‡†ç¡®å’Œå¯é çš„è§£ç­”ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) often struggle with knowledge-intensive tasks due to a lack of background knowledge and a tendency to hallucinate. To address these limitations, integrating knowledge graphs (KGs) with LLMs has been intensively studied. Existing KG-enhanced LLMs focus on supplementary factual knowledge, but still struggle with solving complex questions. We argue that refining the relationships among facts and organizing them into a logically consistent reasoning path is equally important as factual knowledge itself. Despite their potential, extracting reliable reasoning paths from KGs poses the following challenges: the complexity of graph structures and the existence of multiple generated paths, making it difficult to distinguish between useful and redundant ones. To tackle these challenges, we propose the RRP framework to mine the knowledge graph, which combines the semantic strengths of LLMs with structural information obtained through relation embedding and bidirectional distribution learning. Additionally, we introduce a rethinking module that evaluates and refines reasoning paths according to their significance. Experimental results on two public datasets show that RRP achieves state-of-the-art performance compared to existing baseline methods. Moreover, RRP can be easily integrated into various LLMs to enhance their reasoning abilities in a plug-and-play manner. By generating high-quality reasoning paths tailored to specific questions, RRP distills effective guidance for LLM reasoning.

