---
layout: default
title: Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers
---

# Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10887" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.10887v3</a>
  <a href="https://arxiv.org/pdf/2506.10887.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10887v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10887v3', 'Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yixiao Huang, Hanlin Zhu, Tianyu Guo, Jiantao Jiao, Somayeh Sojoudi, Michael I. Jordan, Stuart Russell, Song Mei

**ÂàÜÁ±ª**: cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-12 (Êõ¥Êñ∞: 2025-10-25)

**Â§áÊ≥®**: NeurIPS 2025, first three authors contributed equally

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Áªü‰∏ÄÊú∫Âà∂‰ª•ÁêÜËß£ÂèòÊç¢Âô®‰∏≠ÁöÑ‰∏ä‰∏ãÊñáÊé®ÁêÜÁé∞Ë±°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `‰∏ä‰∏ãÊñáÂ§ñÊé®ÁêÜ` `Áü•ËØÜÊ≥®ÂÖ•` `Áü©ÈòµÂàÜËß£` `Êé®ÁêÜËÉΩÂäõ` `ÂêàÊàê‰∫ãÂÆûÂõûÂøÜ` `Ê¢ØÂ∫¶‰∏ãÈôç`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Áü•ËØÜÊ≥®ÂÖ•Êó∂Ë°®Áé∞Âá∫Ê≥õÂåñ‰∏éÂπªËßâÁöÑÂèåÈáçÊÄßÔºåÂéüÂõ†Â∞ö‰∏çÊòéÁ°Æ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∏ä‰∏ãÊñáÂ§ñÊé®ÁêÜÔºàOCRÔºâ‰Ωú‰∏∫Áªü‰∏ÄÊú∫Âà∂ÔºåËß£ÈáäÊ®°ÂûãÂ¶Ç‰ΩïÈÄöËøáÊ¶ÇÂøµÂÖ≥ËÅîËøõË°åÊé®ÁêÜ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂçïÂ±ÇÂçïÂ§¥Ê≥®ÊÑèÂäõÂèòÊç¢Âô®ËÉΩÂ§üÊúâÊïàËß£ÂÜ≥OCR‰ªªÂä°ÔºåÂ±ïÁ§∫‰∫ÜÁü©ÈòµÂàÜËß£ÁöÑÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÉΩÂ§üÈÄöËøáÂæÆË∞ÉËé∑ÂèñÊñ∞Áü•ËØÜÔºå‰ΩÜËøô‰∏ÄËøáÁ®ãË°®Áé∞Âá∫‰ª§‰∫∫Âõ∞ÊÉëÁöÑÂèåÈáçÊÄßÔºöÊ®°ÂûãËÉΩÂ§ü‰ªéÊñ∞‰∫ãÂÆû‰∏≠ÊòæËëóÊ≥õÂåñÔºå‰ΩÜ‰πüÂÆπÊòì‰∫ßÁîüÈîôËØØ‰ø°ÊÅØÁöÑÂπªËßâ„ÄÇÊú¨ÊñáÊèêÂá∫ÔºåËøô‰∏§ÁßçË°å‰∏∫Ê∫ê‰∫é‰∏ÄÁßçÁß∞‰∏∫‰∏ä‰∏ãÊñáÂ§ñÊé®ÁêÜÔºàOCRÔºâÁöÑÂçï‰∏ÄÊú∫Âà∂ÔºåÂç≥ÈÄöËøáÂÖ≥ËÅîÊ¶ÇÂøµÊé®ÂØºÂê´‰πâÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨Âú®‰∫î‰∏™‰∏ªË¶ÅLLM‰∏äÁöÑÂÆûÈ™åÁ°ÆËÆ§ÔºåOCRÁ°ÆÂÆûÈ©±Âä®‰∫ÜÊ≥õÂåñÂíåÂπªËßâÔºåÂÖ∑‰ΩìÂèñÂÜ≥‰∫éÂÖ≥ËÅîÊ¶ÇÂøµÊòØÂê¶Â≠òÂú®Âõ†ÊûúÂÖ≥Á≥ª„ÄÇÊàë‰ª¨Â∞ÜOCRÂΩ¢ÂºèÂåñ‰∏∫‰∏ÄÁßçÂêàÊàê‰∫ãÂÆûÂõûÂøÜ‰ªªÂä°ÔºåÂπ∂Â±ïÁ§∫‰∫ÜÂçïÂ±ÇÂçïÂ§¥Ê≥®ÊÑèÂäõÂèòÊç¢Âô®Âú®Ê≠§‰ªªÂä°‰∏äÁöÑÂ≠¶‰π†ËÉΩÂäõÔºåÂº∫Ë∞É‰∫ÜÁü©ÈòµÂàÜËß£ÁöÑÈáçË¶ÅÊÄß„ÄÇÊàë‰ª¨ÁöÑÁêÜËÆ∫ÂàÜÊûêË°®ÊòéÔºåOCRËÉΩÂäõÊ∫ê‰∫éÊ¢ØÂ∫¶‰∏ãÈôçÁöÑÈöêÊÄßÂÅèÂ∑ÆÔºåËß£Èáä‰∫ÜÊ®°ÂûãÂ¶Ç‰ΩïÈ´òÊïàÂú∞Â≠¶‰π†ÂÖ≥ËÅî‰∫ãÂÆûÂíåÂê´‰πâ„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÁöÑÂ∑•‰Ωú‰∏∫ÁêÜËß£OCRÁé∞Ë±°Êèê‰æõ‰∫ÜÁêÜËÆ∫Âü∫Á°ÄÔºåÂπ∂‰∏∫ÂàÜÊûêÂíåÂáèËΩªÁü•ËØÜÊ≥®ÂÖ•Â∏¶Êù•ÁöÑ‰∏çËâØË°å‰∏∫Êèê‰æõ‰∫ÜÊñ∞ËßÜËßí„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Áü•ËØÜÊ≥®ÂÖ•ËøáÁ®ã‰∏≠Ê≥õÂåñ‰∏éÂπªËßâÁé∞Ë±°ÁöÑÂéüÂõ†ÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàËß£ÈáäËøô‰∏ÄÂèåÈáçÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰∏ä‰∏ãÊñáÂ§ñÊé®ÁêÜÔºàOCRÔºâ‰Ωú‰∏∫Áªü‰∏ÄÊú∫Âà∂ÔºåÂº∫Ë∞ÉÈÄöËøáÊ¶ÇÂøµÂÖ≥ËÅîËøõË°åÊé®ÁêÜÁöÑËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Âõ†ÊûúÂÖ≥Á≥ª‰∏çÊòéÁ°ÆÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂‰∏≠ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂçïÂ±ÇÂçïÂ§¥Ê≥®ÊÑèÂäõÂèòÊç¢Âô®ÔºåÈááÁî®ÂàÜËß£ÁöÑËæìÂá∫Âíå‰ª∑ÂÄºÁü©ÈòµÔºåËøõË°åÂêàÊàê‰∫ãÂÆûÂõûÂøÜ‰ªªÂä°ÁöÑÂ≠¶‰π†„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜOCRÂΩ¢ÂºèÂåñ‰∏∫ÂêàÊàê‰∫ãÂÆûÂõûÂøÜ‰ªªÂä°ÔºåÂπ∂Â±ïÁ§∫‰∫ÜÁü©ÈòµÂàÜËß£Âú®Â≠¶‰π†ËøáÁ®ã‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºå‰∏é‰º†ÁªüÊ®°ÂûãÁöÑÁªìÂêàÊùÉÈáçÊñπÊ≥ïÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ®°ÂûãÈááÁî®ÂçïÂ±ÇÂçïÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåËæìÂá∫Âíå‰ª∑ÂÄºÁü©ÈòµËøõË°åÂàÜËß£ÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏∫ÊúÄÂ∞èÂåñÂêàÂπ∂ËæìÂá∫-‰ª∑ÂÄºÁü©ÈòµÁöÑÊ†∏ËåÉÊï∞Ôºå‰ª•Ê≠§‰øÉËøõÊ®°ÂûãÁöÑÈ´òÊïàÂ≠¶‰π†„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂçïÂ±ÇÂçïÂ§¥Ê≥®ÊÑèÂäõÂèòÊç¢Âô®Âú®ÂêàÊàê‰∫ãÂÆûÂõûÂøÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊàêÂäüÂ≠¶‰π†Âà∞OCRËÉΩÂäõÔºå‰∏îÂú®Ê†∑Êú¨ÊïàÁéá‰∏äÊòæËëó‰ºò‰∫éÁªìÂêàÊùÉÈáçÁöÑÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÁü©ÈòµÂàÜËß£ÁöÑÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂‰∏∫ÁêÜËß£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÊèê‰æõ‰∫ÜÊñ∞ÁöÑÁêÜËÆ∫Ê°ÜÊû∂ÔºåÊΩúÂú®Â∫îÁî®‰∫éÊîπËøõÁü•ËØÜÊ≥®ÂÖ•ÊäÄÊúØÔºåÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊú™Êù•ÂèØÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÁü•ËØÜÂõæË∞±ÊûÑÂª∫Á≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) can acquire new knowledge through fine-tuning, but this process exhibits a puzzling duality: models can generalize remarkably from new facts, yet are also prone to hallucinating incorrect information. However, the reasons for this phenomenon remain poorly understood. In this work, we argue that both behaviors stem from a single mechanism known as out-of-context reasoning (OCR): the ability to deduce implications by associating concepts, even those without a causal link. Our experiments across five prominent LLMs confirm that OCR indeed drives both generalization and hallucination, depending on whether the associated concepts are causally related. To build a rigorous theoretical understanding of this phenomenon, we then formalize OCR as a synthetic factual recall task. We empirically show that a one-layer single-head attention-only transformer with factorized output and value matrices can learn to solve this task, while a model with combined weights cannot, highlighting the crucial role of matrix factorization. Our theoretical analysis shows that the OCR capability can be attributed to the implicit bias of gradient descent, which favors solutions that minimize the nuclear norm of the combined output-value matrix. This mathematical structure explains why the model learns to associate facts and implications with high sample efficiency, regardless of whether the correlation is causal or merely spurious. Ultimately, our work provides a theoretical foundation for understanding the OCR phenomenon, offering a new lens for analyzing and mitigating undesirable behaviors from knowledge injection.

