---
layout: default
title: Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles
---

# Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10848" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10848v2</a>
  <a href="https://arxiv.org/pdf/2506.10848.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10848v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10848v2', 'Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, Linfeng Zhang

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12 (æ›´æ–°: 2025-06-13)

**å¤‡æ³¨**: 11 pages; 5 figures;

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSlowFasté‡‡æ ·ä»¥è§£å†³æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£è¯­è¨€æ¨¡å‹` `åŠ¨æ€é‡‡æ ·` `åŠ é€Ÿè§£ç ` `è‡ªç„¶è¯­è¨€å¤„ç†` `é«˜æ•ˆç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨é‡‡æ ·ç­–ç•¥ä¸Šå­˜åœ¨é™æ€è¡Œä¸ºï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œçµæ´»æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„SlowFasté‡‡æ ·ç­–ç•¥é€šè¿‡åŠ¨æ€åˆ‡æ¢è§£ç é˜¶æ®µï¼Œæå‡äº†é‡‡æ ·æ•ˆç‡å’Œçµæ´»æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSlowFasté‡‡æ ·åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼Œä¸”å‡†ç¡®ç‡ä¿æŒç¨³å®šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºä¼ ç»Ÿè‡ªå›å½’è¯­è¨€æ¨¡å‹çš„æœ‰åŠ›æ›¿ä»£ï¼Œèƒ½å¤Ÿå®ç°å¹¶è¡Œç”Ÿæˆå¹¶æ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿã€‚ç„¶è€Œï¼Œç°æœ‰çš„é‡‡æ ·ç­–ç•¥å¦‚åŸºäºç½®ä¿¡åº¦æˆ–åŠè‡ªå›å½’è§£ç ï¼Œå¸¸å¸¸è¡¨ç°å‡ºé™æ€è¡Œä¸ºï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œçµæ´»æ€§ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŠ¨æ€é‡‡æ ·ç­–ç•¥â€”â€”SlowFasté‡‡æ ·ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°åœ¨æ¢ç´¢æ€§å’ŒåŠ é€Ÿè§£ç é˜¶æ®µä¹‹é—´åˆ‡æ¢ã€‚è¯¥æ–¹æ³•éµå¾ªä¸‰ä¸ªé»„é‡‘åŸåˆ™ï¼šç¡®å®šæ€§åŸåˆ™ã€æ”¶æ•›åŸåˆ™å’Œä½ç½®åŸåˆ™ï¼ŒæŒ‡å¯¼ä½•æ—¶ä½•åœ°å¯ä»¥è‡ªä¿¡ä¸”é«˜æ•ˆåœ°è§£ç ä»¤ç‰Œã€‚é€šè¿‡ä¸dLLM-Cacheçš„ç»“åˆï¼Œè¿›ä¸€æ­¥å‡å°‘å†—ä½™è®¡ç®—ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSlowFasté‡‡æ ·åœ¨LLaDAä¸Šå®ç°äº†æœ€é«˜15.63å€çš„åŠ é€Ÿï¼Œç»“åˆç¼“å­˜æ—¶å¯è¾¾34.22å€ï¼Œä¸”å‡†ç¡®ç‡ä¸‹é™æå°ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨ååé‡ä¸Šè¶…è¶Šäº†å¼ºå¤§çš„è‡ªå›å½’åŸºçº¿LLaMA3 8Bï¼Œè¯æ˜äº†ç²¾å¿ƒè®¾è®¡çš„é‡‡æ ·èƒ½å¤Ÿå……åˆ†é‡Šæ”¾dLLMsåœ¨å¿«é€Ÿé«˜è´¨é‡ç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨é‡‡æ ·ç­–ç•¥ä¸Šçš„é™æ€è¡Œä¸ºé—®é¢˜ï¼Œè¿™å¯¼è‡´äº†æ•ˆç‡ä½ä¸‹å’Œçµæ´»æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºSlowFasté‡‡æ ·ç­–ç•¥ï¼Œé€šè¿‡åŠ¨æ€åˆ‡æ¢æ¢ç´¢æ€§å’ŒåŠ é€Ÿè§£ç é˜¶æ®µï¼Œä¼˜åŒ–é‡‡æ ·è¿‡ç¨‹ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šæ¢ç´¢æ€§è§£ç å’ŒåŠ é€Ÿè§£ç ï¼Œç»“åˆä¸‰ä¸ªé»„é‡‘åŸåˆ™æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œå¹¶ä¸dLLM-Cacheé›†æˆä»¥å‡å°‘å†—ä½™è®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåŠ¨æ€é‡‡æ ·ç­–ç•¥çš„è®¾è®¡ï¼Œèƒ½å¤Ÿæ ¹æ®å½“å‰è§£ç çŠ¶æ€è‡ªé€‚åº”è°ƒæ•´é‡‡æ ·ç­–ç•¥ï¼Œä¸ä¼ ç»Ÿé™æ€æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œç»“åˆäº†ç¡®å®šæ€§åŸåˆ™ã€æ”¶æ•›åŸåˆ™å’Œä½ç½®åŸåˆ™ï¼Œç¡®ä¿åœ¨ä¸åŒé˜¶æ®µçš„é‡‡æ ·æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶é‡‡ç”¨äº†é€‚åº”æ€§ç¼“å­˜æœºåˆ¶ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSlowFasté‡‡æ ·åœ¨LLaDAä¸Šå®ç°äº†æœ€é«˜15.63å€çš„é€Ÿåº¦æå‡ï¼Œç»“åˆdLLM-Cacheæ—¶å¯è¾¾34.22å€ï¼Œä¸”å‡†ç¡®ç‡ä¸‹é™æå°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ååé‡ä¸Šè¶…è¶Šäº†LLaMA3 8Bç­‰å¼ºå¤§çš„è‡ªå›å½’åŸºçº¿ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”Ÿæˆæ¨¡å‹çš„æ¨ç†é€Ÿåº¦å’Œå“åº”èƒ½åŠ›ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼ŒSlowFasté‡‡æ ·æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„AIåº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®æ—¶äº¤äº’èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion-based language models (dLLMs) have emerged as a promising alternative to traditional autoregressive LLMs by enabling parallel token generation and significantly reducing inference latency. However, existing sampling strategies for dLLMs, such as confidence-based or semi-autoregressive decoding, often suffer from static behavior, leading to suboptimal efficiency and limited flexibility. In this paper, we propose SlowFast Sampling, a novel dynamic sampling strategy that adaptively alternates between exploratory and accelerated decoding stages. Our method is guided by three golden principles: certainty principle, convergence principle, and positional principle, which govern when and where tokens can be confidently and efficiently decoded. We further integrate our strategy with dLLM-Cache to reduce redundant computation. Extensive experiments across benchmarks and models show that SlowFast Sampling achieves up to 15.63$\times$ speedup on LLaDA with minimal accuracy drop, and up to 34.22$\times$ when combined with caching. Notably, our approach outperforms strong autoregressive baselines like LLaMA3 8B in throughput, demonstrating that well-designed sampling can unlock the full potential of dLLMs for fast and high-quality generation.

