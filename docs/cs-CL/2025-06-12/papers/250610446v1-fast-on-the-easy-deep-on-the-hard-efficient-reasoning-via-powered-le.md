---
layout: default
title: Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty
---

# Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10446" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10446v1</a>
  <a href="https://arxiv.org/pdf/2506.10446.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10446v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10446v1', 'Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zehui Ling, Deshu Chen, Hongwei Zhang, Yifeng Jiao, Xin Guo, Yuan Cheng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPowered Length Penaltyä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç‡` `é•¿åº¦æƒ©ç½š` `å¼ºåŒ–å­¦ä¹ ` `é—®é¢˜å¤æ‚æ€§` `æ¨¡å‹æ€§èƒ½` `æ•°æ®é›†è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†æ–¹æ³•å¸¸å› è¾“å‡ºå†—é•¿è€Œå¯¼è‡´è®¡ç®—å»¶è¿Ÿï¼Œå½±å“æ•ˆç‡ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æƒ©ç½šæœºåˆ¶ï¼Œé’ˆå¯¹ä¸åŒå¤æ‚åº¦çš„é—®é¢˜è°ƒæ•´æ¨ç†é•¿åº¦ã€‚
3. åœ¨GSM8Kå’ŒMATH500æ•°æ®é›†ä¸Šï¼Œæ–¹æ³•æœ‰æ•ˆç¼©çŸ­è¾“å‡ºé•¿åº¦å¹¶æå‡å‡†ç¡®æ€§ï¼ŒAIME2024æ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†æ›´å¥½çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å¦‚é“¾å¼æ€ç»´æç¤ºå¸¸å¯¼è‡´è¾“å‡ºå†—é•¿ï¼Œå¢åŠ è®¡ç®—å»¶è¿Ÿã€‚å°½ç®¡æœ‰äº›æ–¹æ³•é€šè¿‡å¼ºåŒ–å­¦ä¹ ç¼©çŸ­æ¨ç†æ—¶é—´ï¼Œä½†å¾€å¾€é‡‡ç”¨ç»Ÿä¸€çš„æƒ©ç½šæœºåˆ¶ï¼Œæœªè€ƒè™‘é—®é¢˜å¤æ‚æ€§ï¼Œå¯¼è‡´æ•ˆæœä¸ä½³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¯¹ç®€å•é—®é¢˜ä¿ƒè¿›ç®€æ´æ€§ï¼ŒåŒæ—¶å¯¹å¤æ‚é—®é¢˜ä¿æŒå……åˆ†æ¨ç†ï¼Œä»è€Œæé«˜æ¨¡å‹æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡åˆ’åˆ†å¥–åŠ±å‡½æ•°å¹¶å¼•å…¥æ–°çš„è¾“å‡ºé•¿åº¦æƒ©ç½šï¼Œæ˜¾è‘—æå‡äº†åœ¨GSM8Kã€MATH500å’ŒAIME2024ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶è§£å†³çš„æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶è¾“å‡ºå†—é•¿çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè€ƒè™‘é—®é¢˜å¤æ‚æ€§ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡å¯¹ç®€å•é—®é¢˜æ–½åŠ è¾ƒé«˜çš„ç®€æ´æ€§æƒ©ç½šï¼Œè€Œå¯¹å¤æ‚é—®é¢˜åˆ™ä¿æŒå……åˆ†çš„æ¨ç†æ·±åº¦ï¼Œä»¥æ­¤æé«˜æ•´ä½“æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å¤„ç†ã€æ¨ç†æ¨¡å—å’Œè¾“å‡ºç”Ÿæˆä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œåˆ†ç±»ï¼Œç„¶åæ ¹æ®é—®é¢˜å¤æ‚åº¦è°ƒæ•´æ¨ç†ç­–ç•¥ï¼Œæœ€åç”Ÿæˆè¾“å‡ºå¹¶åº”ç”¨é•¿åº¦æƒ©ç½šã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥äº†Powered Length Penaltyä½œä¸ºæ–°çš„è¾“å‡ºé•¿åº¦æƒ©ç½šæœºåˆ¶ï¼ŒåŒºåˆ«äºç°æœ‰æ–¹æ³•çš„ç»Ÿä¸€æƒ©ç½šï¼Œèƒ½å¤Ÿæ ¹æ®é—®é¢˜å¤æ‚æ€§åŠ¨æ€è°ƒæ•´æƒ©ç½šåŠ›åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­ï¼Œè®¾è®¡äº†é’ˆå¯¹ä¸åŒå¤æ‚åº¦é—®é¢˜çš„å¥–åŠ±å’Œæƒ©ç½šæœºåˆ¶ï¼Œç¡®ä¿ç®€å•é—®é¢˜çš„è¾“å‡ºç®€æ´ï¼Œè€Œå¤æ‚é—®é¢˜çš„æ¨ç†æ·±åº¦å¾—åˆ°ä¿ç•™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨GSM8Kå’ŒMATH500æ•°æ®é›†ä¸Šï¼Œæå‡ºçš„æ–¹æ³•æœ‰æ•ˆç¼©çŸ­äº†è¾“å‡ºé•¿åº¦ï¼Œå‡†ç¡®æ€§ä¿æŒä¸å˜æˆ–æœ‰æ‰€æå‡ï¼›åœ¨AIME2024æ•°æ®é›†ä¸Šï¼Œå‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†æ–¹æ³•åœ¨ä¸åŒå¤æ‚åº¦é—®é¢˜ä¸Šçš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èå’Œç§‘å­¦ç ”ç©¶ç­‰éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æé«˜æ¨ç†æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶ç³»ç»Ÿä¸­æ›´å¥½åœ°æ”¯æŒå†³ç­–åˆ¶å®šï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated significant advancements in reasoning capabilities, performing well on various challenging benchmarks. Techniques like Chain-of-Thought prompting have been introduced to further improve reasoning. However, these approaches frequently generate longer outputs, which in turn increase computational latency. Although some methods use reinforcement learning to shorten reasoning, they often apply uniform penalties without considering the problem's complexity, leading to suboptimal outcomes. In this study, we seek to enhance the efficiency of LLM reasoning by promoting conciseness for simpler problems while preserving sufficient reasoning for more complex ones for accuracy, thus improving the model's overall performance. Specifically, we manage the model's reasoning efficiency by dividing the reward function and including a novel penalty for output length. Our approach has yielded impressive outcomes in benchmark evaluations across three datasets: GSM8K, MATH500, and AIME2024. For the comparatively simpler datasets GSM8K and MATH500, our method has effectively shortened output lengths while preserving or enhancing accuracy. On the more demanding AIME2024 dataset, our approach has resulted in improved accuracy.

