---
layout: default
title: Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models
---

# Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10491" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10491v2</a>
  <a href="https://arxiv.org/pdf/2506.10491.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10491v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10491v2', 'Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aleksandra Sorokovikova, Pavel Chizhov, Iuliia Eremenko, Ivan P. Yamshchikov

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12 (æ›´æ–°: 2025-09-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¯­è¨€æ¨¡å‹ä¸­çš„åè§é—®é¢˜åŠå…¶å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `åè§ç ”ç©¶` `ä¸ªæ€§åŒ–æŠ€æœ¯` `äººæœºäº¤äº’` `å…¬å¹³æ€§è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç”¨æˆ·ä¸ªæ€§åŒ–æ—¶ï¼Œå¯èƒ½ä¼šè¡¨ç°å‡ºåè§ï¼Œå½±å“ç»“æœçš„å…¬å¹³æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¯¹æ¯”ä¸åŒçš„åè§æµ‹é‡æ–¹æ³•ï¼Œæ¢è®¨æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸‹çš„åè§è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨è¯„åˆ†å’Œè–ªèµ„å»ºè®®åœºæ™¯ä¸­ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨ç”¨æˆ·å›ç­”è¯„åˆ†æ—¶è¡¨ç°å‡ºæ˜¾è‘—åè§ï¼Œè€Œåœ¨è–ªèµ„è°ˆåˆ¤å»ºè®®ä¸­åè§æ›´ä¸ºæ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£è¯­è¨€æ¨¡å‹åœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ•°æ®ä¸å¯é¿å…åœ°åŒ…å«äº‰è®®æ€§å’Œåˆ»æ¿å°è±¡å†…å®¹ï¼Œæ¶‰åŠæ€§åˆ«ã€å‡ºèº«ã€å¹´é¾„ç­‰å„ç§åè§ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯èƒ½è¡¨è¾¾åè§è§‚ç‚¹æˆ–æ ¹æ®ç”¨æˆ·çš„ä¸ªæ€§äº§ç”Ÿä¸åŒç»“æœã€‚æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å„ç§åè§ä»£ç†æµ‹é‡ï¼Œå‘ç°é€šè¿‡é¢„è®¾ä¸ªæ€§è¿›è¡Œè¯„ä¼°çš„æ¨¡å‹åœ¨å¤šä¸»é¢˜åŸºå‡†ï¼ˆMMLUï¼‰ä¸Šçš„å¾—åˆ†å·®å¼‚å¾®ä¹å…¶å¾®ä¸”å¤§å¤šéšæœºã€‚ç„¶è€Œï¼Œå½“è¦æ±‚æ¨¡å‹å¯¹ç”¨æˆ·çš„å›ç­”è¿›è¡Œè¯„åˆ†æ—¶ï¼Œåè§çš„è¿¹è±¡æ›´ä¸ºæ˜æ˜¾ã€‚æœ€åï¼Œåœ¨è¦æ±‚æ¨¡å‹æä¾›è–ªèµ„è°ˆåˆ¤å»ºè®®æ—¶ï¼Œç­”æ¡ˆä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„åè§ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹åŠ©æ‰‹è®°å¿†å’Œä¸ªæ€§åŒ–çš„è¶‹åŠ¿ï¼Œè¿™äº›é—®é¢˜ä»ä¸åŒè§’åº¦æ˜¾ç°ï¼šç°ä»£ç”¨æˆ·æ— éœ€é¢„è®¾ä¸ªæ€§æè¿°ï¼Œå› ä¸ºæ¨¡å‹å·²ç»äº†è§£ä»–ä»¬çš„ç¤¾ä¼šäººå£ç‰¹å¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„åè§é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¸ªæ€§åŒ–å’Œç”¨æˆ·äº¤äº’æ—¶è¡¨ç°å‡ºçš„åè§ã€‚ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°æ¨¡å‹åè§æ—¶ï¼Œå¾€å¾€æœªèƒ½æ­ç¤ºå…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸‹çš„çœŸå®è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡é‡æ–°è®¾è®¡ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯è®©æ¨¡å‹å¯¹ç”¨æˆ·çš„å›ç­”è¿›è¡Œè¯„åˆ†ï¼Œæ¥æ­ç¤ºæ¨¡å‹æ½œåœ¨çš„åè§ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æ¸…æ™°åœ°åæ˜ å‡ºæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„åè§è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å¤šä¸»é¢˜åŸºå‡†ï¼ˆMMLUï¼‰è¿›è¡Œè¯„ä¼°ï¼Œåˆ†ä¸ºé¢„è®¾ä¸ªæ€§è¯„ä¼°å’Œç”¨æˆ·å›ç­”è¯„åˆ†ä¸¤ç§ä¸»è¦ä»»åŠ¡ã€‚é€šè¿‡å¯¹æ¯”è¿™ä¸¤ç§ä»»åŠ¡çš„ç»“æœï¼Œåˆ†ææ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„åè§è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°åœ¨äºé€šè¿‡ä»»åŠ¡é‡æ„ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨ç”¨æˆ·äº¤äº’ä¸­çš„åè§è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨è–ªèµ„è°ˆåˆ¤å»ºè®®ä¸­ï¼Œæ˜¾ç¤ºå‡ºä¸ç°æœ‰è¯„ä¼°æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œå…³æ³¨æ¨¡å‹åœ¨ä¸åŒä¸ªæ€§åŒ–åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯è–ªèµ„è°ˆåˆ¤å»ºè®®çš„åè§åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šä¸»é¢˜åŸºå‡†ï¼ˆMMLUï¼‰ä¸Šï¼Œé¢„è®¾ä¸ªæ€§è¯„ä¼°çš„å¾—åˆ†å·®å¼‚å¾®ä¹å…¶å¾®ï¼Œè€Œåœ¨ç”¨æˆ·å›ç­”è¯„åˆ†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„åè§ã€‚ç‰¹åˆ«æ˜¯åœ¨è–ªèµ„è°ˆåˆ¤å»ºè®®ä¸­ï¼Œæ¨¡å‹çš„åè§è¡¨ç°å°¤ä¸ºæ˜æ˜¾ï¼Œè¡¨æ˜ä¸ªæ€§åŒ–å¯¹ç»“æœçš„å½±å“ä¸å®¹å¿½è§†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€æ™ºèƒ½åŠ©æ‰‹å’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡è¯†åˆ«å’Œå‡è½»è¯­è¨€æ¨¡å‹ä¸­çš„åè§ï¼Œå¯ä»¥æé«˜è¿™äº›ç³»ç»Ÿçš„å…¬å¹³æ€§å’Œç”¨æˆ·ä½“éªŒï¼Œä¿ƒè¿›æ›´ä¸ºå…¬æ­£çš„æŠ€æœ¯åº”ç”¨ã€‚æœªæ¥ï¼Œéšç€ä¸ªæ€§åŒ–æŠ€æœ¯çš„å‘å±•ï¼Œç†è§£å’Œç®¡ç†æ¨¡å‹åè§å°†å˜å¾—æ„ˆåŠ é‡è¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern language models are trained on large amounts of data. These data inevitably include controversial and stereotypical content, which contains all sorts of biases related to gender, origin, age, etc. As a result, the models express biased points of view or produce different results based on the assigned personality or the personality of the user. In this paper, we investigate various proxy measures of bias in large language models (LLMs). We find that evaluating models with pre-prompted personae on a multi-subject benchmark (MMLU) leads to negligible and mostly random differences in scores. However, if we reformulate the task and ask a model to grade the user's answer, this shows more significant signs of bias. Finally, if we ask the model for salary negotiation advice, we see pronounced bias in the answers. With the recent trend for LLM assistant memory and personalization, these problems open up from a different angle: modern LLM users do not need to pre-prompt the description of their persona since the model already knows their socio-demographics.

