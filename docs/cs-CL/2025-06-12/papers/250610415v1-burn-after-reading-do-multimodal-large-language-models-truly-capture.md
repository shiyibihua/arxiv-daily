---
layout: default
title: Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?
---

# Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10415" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10415v1</a>
  <a href="https://arxiv.org/pdf/2506.10415.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10415v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10415v1', 'Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yingjin Song, Yupei Du, Denis Paperno, Albert Gatt

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: 27 pages, 14 figures. Accepted to ACL 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yjsong22/TempVS)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTempVSåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„äº‹ä»¶é¡ºåºç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ—¶é—´æ¨ç†` `äº‹ä»¶å…³ç³»æ¨ç†` `å›¾åƒåºåˆ—ç†è§£` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç†è§£å›¾åƒåºåˆ—ä¸­çš„äº‹ä»¶é¡ºåºæ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆæ¨ç†äº‹ä»¶ä¹‹é—´çš„å…³ç³»ã€‚
2. è®ºæ–‡æå‡ºäº†TempVSåŸºå‡†ï¼Œé€šè¿‡äº‹ä»¶å…³ç³»æ¨ç†ã€å¥å­æ’åºå’Œå›¾åƒæ’åºç­‰æµ‹è¯•ï¼Œè¯„ä¼°MLLMsçš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œ38ç§æœ€å…ˆè¿›çš„MLLMsåœ¨TempVSåŸºå‡†ä¸Šçš„è¡¨ç°è¿œä½äºäººç±»ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨æ—¶é—´æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†TempVSåŸºå‡†ï¼Œä¸“æ³¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒåºåˆ—ä¸­çš„æ—¶é—´åŸºç¡€å’Œæ¨ç†èƒ½åŠ›ã€‚TempVSåŒ…å«ä¸‰ä¸ªä¸»è¦æµ‹è¯•ï¼ˆäº‹ä»¶å…³ç³»æ¨ç†ã€å¥å­æ’åºå’Œå›¾åƒæ’åºï¼‰ï¼Œæ¯ä¸ªæµ‹è¯•éƒ½é™„å¸¦åŸºæœ¬çš„åŸºç¡€æµ‹è¯•ã€‚è¯¥åŸºå‡†è¦æ±‚MLLMsä¾èµ–è§†è§‰å’Œè¯­è¨€ä¸¤ç§æ¨¡æ€æ¥ç†è§£äº‹ä»¶çš„æ—¶é—´é¡ºåºã€‚æˆ‘ä»¬è¯„ä¼°äº†38ç§æœ€å…ˆè¿›çš„MLLMsï¼Œç»“æœæ˜¾ç¤ºè¿™äº›æ¨¡å‹åœ¨è§£å†³TempVSæ—¶è¡¨ç°ä¸ä½³ï¼Œä¸äººç±»èƒ½åŠ›ç›¸æ¯”å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ç»†è‡´çš„è§è§£ï¼ŒæŒ‡å‡ºæœªæ¥ç ”ç©¶çš„æœ‰å¸Œæœ›æ–¹å‘ã€‚TempVSåŸºå‡†æ•°æ®å’Œä»£ç å¯åœ¨https://github.com/yjsong22/TempVSè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒåºåˆ—ä¸­ç†è§£äº‹ä»¶é¡ºåºçš„èƒ½åŠ›ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•åœ¨æ—¶é—´æ¨ç†å’Œäº‹ä»¶å…³ç³»ç†è§£ä¸Šå­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºTempVSåŸºå‡†ï¼Œè¦æ±‚æ¨¡å‹åŒæ—¶åˆ©ç”¨è§†è§‰å’Œè¯­è¨€ä¿¡æ¯æ¥è¿›è¡Œæ—¶é—´é¡ºåºçš„ç†è§£å’Œæ¨ç†ã€‚é€šè¿‡è®¾è®¡å¤šç§æµ‹è¯•ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTempVSåŸºå‡†åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æµ‹è¯•æ¨¡å—ï¼šäº‹ä»¶å…³ç³»æ¨ç†ã€å¥å­æ’åºå’Œå›¾åƒæ’åºï¼Œæ¯ä¸ªæ¨¡å—éƒ½æœ‰åŸºç¡€çš„æ—¶é—´ç†è§£æµ‹è¯•ã€‚è¿™äº›æ¨¡å—å…±åŒæ„æˆäº†è¯„ä¼°MLLMsæ—¶é—´æ¨ç†èƒ½åŠ›çš„æ•´ä½“æ¡†æ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šTempVSåŸºå‡†çš„è®¾è®¡æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œæä¾›äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„è¯„ä¼°æ–¹å¼ï¼Œå¡«è¡¥äº†ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨æ—¶é—´æ¨ç†èƒ½åŠ›è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æµ‹è¯•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ ·åŒ–çš„äº‹ä»¶å…³ç³»å’Œæ’åºä»»åŠ¡ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¡¨ç°å¾—åˆ°å…¨é¢è¯„ä¼°ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œ38ç§æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨TempVSåŸºå‡†ä¸Šçš„å¹³å‡è¡¨ç°æ˜¾è‘—ä½äºäººç±»ï¼Œæ˜¾ç¤ºå‡ºåœ¨äº‹ä»¶é¡ºåºç†è§£æ–¹é¢çš„æ˜æ˜¾ä¸è¶³ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†å½“å‰æ¨¡å‹åœ¨æ—¶é—´æ¨ç†èƒ½åŠ›ä¸Šçš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç†è§£ã€æ™ºèƒ½ç›‘æ§å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„è¿™äº›é¢†åŸŸçš„æ™ºèƒ½ç³»ç»Ÿè¡¨ç°ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚æœªæ¥ï¼ŒTempVSåŸºå‡†å¯èƒ½æˆä¸ºå¤šæ¨¡æ€æ¨¡å‹ç ”ç©¶çš„é‡è¦å‚è€ƒæ ‡å‡†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces the TempVS benchmark, which focuses on temporal grounding and reasoning capabilities of Multimodal Large Language Models (MLLMs) in image sequences. TempVS consists of three main tests (i.e., event relation inference, sentence ordering and image ordering), each accompanied with a basic grounding test. TempVS requires MLLMs to rely on both visual and linguistic modalities to understand the temporal order of events. We evaluate 38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS, with a substantial performance gap compared to human capabilities. We also provide fine-grained insights that suggest promising directions for future research. Our TempVS benchmark data and code are available at https://github.com/yjsong22/TempVS.

