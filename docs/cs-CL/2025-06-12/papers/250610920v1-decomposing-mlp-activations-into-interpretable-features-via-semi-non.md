---
layout: default
title: Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization
---

# Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10920" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10920v1</a>
  <a href="https://arxiv.org/pdf/2506.10920.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10920v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10920v1', 'Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Or Shafran, Atticus Geiger, Mor Geva

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠéè´ŸçŸ©é˜µåˆ†è§£ä»¥è§£æMLPæ¿€æ´»ç‰¹å¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠéè´ŸçŸ©é˜µåˆ†è§£` `å¯è§£é‡Šæ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¥ç»ç½‘ç»œ` `ç‰¹å¾å­¦ä¹ ` `å› æœæ¨æ–­` `ç¨€ç–è‡ªç¼–ç å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç¨€ç–è‡ªç¼–ç å™¨è¿›è¡Œå­—å…¸å­¦ä¹ ï¼Œä½†åœ¨å› æœè¯„ä¼°ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸”ç¼ºä¹ä¸æ¨¡å‹è®¡ç®—çš„ç›´æ¥å…³è”ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡åŠéè´ŸçŸ©é˜µåˆ†è§£ï¼ˆSNMFï¼‰ç›´æ¥åˆ†è§£MLPæ¿€æ´»ï¼Œå­¦ä¹ ç¨€ç–çº¿æ€§ç»„åˆçš„å¯è§£é‡Šç‰¹å¾ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSNMFç‰¹å¾åœ¨å› æœå¼•å¯¼ä¸Šè¶…è¶Šäº†SAEså’Œå¼ºç›‘ç£åŸºçº¿ï¼Œä¸”ä¸äººç±»å¯è§£é‡Šæ¦‚å¿µé«˜åº¦ä¸€è‡´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºåˆ¶å¯è§£é‡Šæ€§çš„æ ¸å¿ƒç›®æ ‡æ˜¯è¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­èƒ½å¤Ÿå› æœè§£é‡Šå…¶è¾“å‡ºçš„åˆ†æå•å…ƒã€‚å°½ç®¡æ—©æœŸç ”ç©¶é›†ä¸­åœ¨å•ä¸ªç¥ç»å…ƒä¸Šï¼Œä½†ç¥ç»å…ƒé€šå¸¸ç¼–ç å¤šä¸ªæ¦‚å¿µçš„è¯æ®ä¿ƒä½¿æˆ‘ä»¬è½¬å‘åˆ†ææ¿€æ´»ç©ºé—´ä¸­çš„æ–¹å‘ã€‚å½“å‰æ–¹æ³•ä¾èµ–äºç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰è¿›è¡Œå­—å…¸å­¦ä¹ ï¼Œä½†åœ¨å› æœè¯„ä¼°ä¸­è¡¨ç°ä¸ä½³ä¸”ç¼ºä¹å†…åœ¨å¯è§£é‡Šæ€§ã€‚æœ¬æ–‡é€šè¿‡åŠéè´ŸçŸ©é˜µåˆ†è§£ï¼ˆSNMFï¼‰ç›´æ¥åˆ†è§£MLPæ¿€æ´»ï¼Œå­¦ä¹ çš„ç‰¹å¾æ˜¯ç¨€ç–çº¿æ€§ç»„åˆçš„å…±æ¿€æ´»ç¥ç»å…ƒï¼Œå¹¶æ˜ å°„åˆ°å…¶æ¿€æ´»è¾“å…¥ï¼Œä½¿å…¶ç›´æ¥å¯è§£é‡Šã€‚å®éªŒè¡¨æ˜ï¼ŒSNMFè¡ç”Ÿçš„ç‰¹å¾åœ¨å› æœå¼•å¯¼ä¸Šä¼˜äºSAEså’Œå¼ºç›‘ç£åŸºçº¿ï¼ŒåŒæ—¶ä¸äººç±»å¯è§£é‡Šæ¦‚å¿µä¸€è‡´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä»¥æ— ç›‘ç£æ–¹å¼æ‰¾åˆ°å¯è§£é‡Šç‰¹å¾çš„æ–¹å‘ã€‚ç°æœ‰çš„ç¨€ç–è‡ªç¼–ç å™¨æ–¹æ³•åœ¨å› æœè¯„ä¼°ä¸­å­˜åœ¨å±€é™æ€§ï¼Œä¸”ç¼ºä¹ä¸æ¨¡å‹è®¡ç®—çš„ç›´æ¥è”ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åŠéè´ŸçŸ©é˜µåˆ†è§£ï¼ˆSNMFï¼‰ç›´æ¥åˆ†è§£MLPæ¿€æ´»ï¼Œå­¦ä¹ çš„ç‰¹å¾æ˜¯ç¨€ç–çš„çº¿æ€§ç»„åˆï¼Œä¸”èƒ½å¤Ÿæ˜ å°„åˆ°å…¶æ¿€æ´»è¾“å…¥ï¼Œä»è€Œå®ç°ç›´æ¥å¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€SNMFåˆ†è§£æ¨¡å—å’Œç‰¹å¾æ˜ å°„æ¨¡å—ã€‚æ•°æ®é¢„å¤„ç†é˜¶æ®µè´Ÿè´£æ”¶é›†å’Œå‡†å¤‡æ¿€æ´»æ•°æ®ï¼ŒSNMFæ¨¡å—è¿›è¡Œç‰¹å¾å­¦ä¹ ï¼Œç‰¹å¾æ˜ å°„æ¨¡å—åˆ™å°†å­¦ä¹ åˆ°çš„ç‰¹å¾ä¸è¾“å…¥æ•°æ®å…³è”ã€‚

**å…³é”®åˆ›æ–°**ï¼šSNMFçš„å¼•å…¥ä½¿å¾—ç‰¹å¾å­¦ä¹ ä¸ä»…ç¨€ç–ä¸”å…·æœ‰å¯è§£é‡Šæ€§ï¼Œè§£å†³äº†ä¼ ç»Ÿç¨€ç–è‡ªç¼–ç å™¨åœ¨å› æœè¯„ä¼°ä¸­çš„ä¸è¶³ï¼Œæä¾›äº†ä¸€ç§æ–°çš„åˆ†ææ–¹å‘ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SNMFä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ç¡®ä¿å­¦ä¹ çš„ç‰¹å¾èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨ç¤ºæ¿€æ´»ç©ºé—´ä¸­çš„ä¿¡æ¯ï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§çš„å‚æ•°è®¾ç½®ä»¥ä¼˜åŒ–åˆ†è§£æ•ˆæœã€‚ç‰¹å¾çš„ç¨€ç–æ€§å’Œå¯è§£é‡Šæ€§æ˜¯é€šè¿‡å¯¹ç¥ç»å…ƒæ¿€æ´»çš„ç»„åˆè¿›è¡Œçº¦æŸå®ç°çš„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSNMFè¡ç”Ÿçš„ç‰¹å¾åœ¨å› æœå¼•å¯¼ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç¨€ç–è‡ªç¼–ç å™¨å’Œå¼ºç›‘ç£åŸºçº¿ï¼ˆå‡å€¼å·®å¼‚ï¼‰ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚æ­¤å¤–ï¼ŒSNMFç‰¹å¾ä¸äººç±»å¯è§£é‡Šæ¦‚å¿µé«˜åº¦ä¸€è‡´ï¼Œå±•ç¤ºäº†å…¶åœ¨è§£æMLPæ¿€æ´»ç©ºé—´ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œè§£æå¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶ã€‚é€šè¿‡æä¾›å¯è§£é‡Šçš„ç‰¹å¾ï¼Œç ”ç©¶è€…å¯ä»¥æ›´æœ‰æ•ˆåœ°è°ƒä¼˜æ¨¡å‹ï¼Œæå‡å…¶æ€§èƒ½å’Œå¯é æ€§ï¼Œæœªæ¥å¯èƒ½å¯¹AIç³»ç»Ÿçš„é€æ˜æ€§å’Œä¿¡ä»»åº¦äº§ç”Ÿç§¯æå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A central goal for mechanistic interpretability has been to identify the right units of analysis in large language models (LLMs) that causally explain their outputs. While early work focused on individual neurons, evidence that neurons often encode multiple concepts has motivated a shift toward analyzing directions in activation space. A key question is how to find directions that capture interpretable features in an unsupervised manner. Current methods rely on dictionary learning with sparse autoencoders (SAEs), commonly trained over residual stream activations to learn directions from scratch. However, SAEs often struggle in causal evaluations and lack intrinsic interpretability, as their learning is not explicitly tied to the computations of the model. Here, we tackle these limitations by directly decomposing MLP activations with semi-nonnegative matrix factorization (SNMF), such that the learned features are (a) sparse linear combinations of co-activated neurons, and (b) mapped to their activating inputs, making them directly interpretable. Experiments on Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs and a strong supervised baseline (difference-in-means) on causal steering, while aligning with human-interpretable concepts. Further analysis reveals that specific neuron combinations are reused across semantically-related features, exposing a hierarchical structure in the MLP's activation space. Together, these results position SNMF as a simple and effective tool for identifying interpretable features and dissecting concept representations in LLMs.

