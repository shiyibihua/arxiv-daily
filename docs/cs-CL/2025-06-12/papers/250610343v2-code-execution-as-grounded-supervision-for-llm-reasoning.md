---
layout: default
title: Code Execution as Grounded Supervision for LLM Reasoning
---

# Code Execution as Grounded Supervision for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10343" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10343v2</a>
  <a href="https://arxiv.org/pdf/2506.10343.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10343v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10343v2', 'Code Execution as Grounded Supervision for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dongwon Jung, Wenxuan Zhou, Muhao Chen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12 (æ›´æ–°: 2025-10-17)

**å¤‡æ³¨**: EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä»£ç æ‰§è¡Œçš„ç›‘ç£æ–¹æ³•ä»¥æå‡LLMæ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é“¾å¼æ€ç»´` `æ¨ç†èƒ½åŠ›` `ä»£ç æ‰§è¡Œ` `ç›‘ç£å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•°æ®ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è·å–å¯é çš„æ¨ç†ç›‘ç£æ•°æ®æ—¶é¢ä¸´é«˜æˆæœ¬å’Œä½å‡†ç¡®æ€§çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡ä»£ç æ‰§è¡Œæå–å¯éªŒè¯çš„æ¨ç†è½¨è¿¹ï¼Œè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€çš„CoTæ¨ç†ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæœ‰æ•ˆæå‡äº†LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶å‡å°‘äº†æ¨ç†è¿‡ç¨‹ä¸­çš„tokené•¿åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ï¼Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£å·²è¢«è¯æ˜èƒ½æœ‰æ•ˆå¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè·å–å¯é ä¸”å‡†ç¡®çš„æ¨ç†ç›‘ç£ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ç¨‹åºæ‰§è¡Œçš„ç¡®å®šæ€§ç”Ÿæˆé«˜è´¨é‡çš„CoTç›‘ç£æ•°æ®é›†ã€‚ä¸ä¾èµ–æ˜‚è´µçš„äººç±»æ ‡æ³¨æˆ–æ˜“å‡ºé”™çš„LLMç”ŸæˆCoTçš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»ä»£ç æ‰§è¡Œä¸­æå–å¯éªŒè¯çš„é€æ­¥æ¨ç†è½¨è¿¹ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€çš„CoTæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°èµ‹äºˆLLMsåœ¨å¤šæ ·ä»»åŠ¡ä¸­çš„å¯è¿ç§»æ¨ç†èƒ½åŠ›ï¼Œæ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„æ¨ç†æ•°æ®é«˜åº¦å‡†ç¡®ï¼Œå¹¶é€šè¿‡å‡å°‘æ— æ„ä¹‰çš„é‡å¤å’Œè¿‡åº¦æ€è€ƒé™ä½äº†æ¨ç†è¿‡ç¨‹ä¸­çš„æ•´ä½“tokené•¿åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¦‚ä½•è·å–é«˜è´¨é‡çš„æ¨ç†ç›‘ç£æ•°æ®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººç±»æ ‡æ³¨æˆ–ä¸å¯é çš„LLMç”Ÿæˆï¼Œå¯¼è‡´æ¨ç†ç›‘ç£çš„å‡†ç¡®æ€§å’Œå¯é æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¨‹åºæ‰§è¡Œçš„ç¡®å®šæ€§ï¼Œæå–å¯éªŒè¯çš„é€æ­¥æ¨ç†è½¨è¿¹ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€çš„é“¾å¼æ€ç»´æ¨ç†ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†ç”Ÿæˆçš„æ¨ç†æ•°æ®å…·æœ‰é«˜å‡†ç¡®æ€§å’Œå¯éªŒè¯æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡ä»£ç æ‰§è¡Œç”Ÿæˆé€æ­¥æ¨ç†è½¨è¿¹ï¼›å…¶æ¬¡ï¼Œå°†è¿™äº›è½¨è¿¹è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€çš„CoTæ¨ç†ï¼›æœ€åï¼Œåˆ©ç”¨ç”Ÿæˆçš„æ•°æ®è®­ç»ƒLLMsä»¥æå‡å…¶æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡ä»£ç æ‰§è¡Œæå–æ¨ç†è½¨è¿¹ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¯¹äººç±»æ ‡æ³¨çš„ä¾èµ–ï¼Œæ˜¾è‘—æé«˜äº†æ•°æ®çš„å‡†ç¡®æ€§å’Œç”Ÿæˆæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæœ¬æ–‡è®¾è®¡äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®ä»¥ä¼˜åŒ–æ¨ç†è½¨è¿¹çš„æå–è¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ç¡®ä¿ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€CoTæ¨ç†ä¸åŸå§‹æ¨ç†è½¨è¿¹çš„ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„æ¨ç†æ•°æ®å‡†ç¡®ç‡é«˜è¾¾95%ï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ•´ä½“tokené•¿åº¦å‡å°‘äº†30%ï¼Œæœ‰æ•ˆé™ä½äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–æ¨ç†ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„æ¨ç†ç›‘ç£æ•°æ®ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡LLMsåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training large language models (LLMs) with chain-of-thought (CoT) supervision has proven effective for enhancing their reasoning abilities. However, obtaining reliable and accurate reasoning supervision remains a significant challenge. We propose a scalable method for generating a high-quality CoT supervision dataset by leveraging the determinism of program execution. Unlike existing reasoning dataset generation methods that rely on costly human annotations or error-prone LLM-generated CoT, our approach extracts verifiable, step-by-step reasoning traces from code execution and transforms them into a natural language CoT reasoning. Experiments on reasoning benchmarks across various domains show that our method effectively equips LLMs with transferable reasoning abilities across diverse tasks. Furthermore, the ablation studies validate that our method produces highly accurate reasoning data and reduces overall token length during inference by reducing meaningless repetition and overthinking.

