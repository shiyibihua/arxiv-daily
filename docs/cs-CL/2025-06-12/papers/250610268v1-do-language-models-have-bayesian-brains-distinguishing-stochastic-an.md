---
layout: default
title: Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models
---

# Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10268" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10268v1</a>
  <a href="https://arxiv.org/pdf/2506.10268.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10268v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10268v1', 'Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrea Yaoyun Cui, Pengfei Yu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒºåˆ†è¯­è¨€æ¨¡å‹å†³ç­–æ¨¡å¼çš„æ–¹æ³•ä»¥è§£å†³è´å¶æ–¯æ¨æ–­é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `è´å¶æ–¯æ¨æ–­` `å†³ç­–æ¨¡å¼` `å‰å¸ƒæ–¯é‡‡æ ·` `æ¦‚ç‡åˆ†å¸ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å‡è®¾è¯­è¨€æ¨¡å‹è¿›è¡Œéšæœºå†³ç­–ï¼Œä½†å®é™…è¡¨ç°å¯èƒ½è¿‘ä¹ç¡®å®šæ€§ï¼ŒæŒ‘æˆ˜äº†è¿™ä¸€å‡è®¾ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åŒºåˆ†éšæœºä¸ç¡®å®šæ€§å†³ç­–æ¨¡å¼ï¼Œè§£å†³äº†è¯¯å¯¼æ€§å…ˆéªŒæ¨æ–­çš„é—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„å†³ç­–æ¨¡å¼å…·æœ‰æ˜¾è‘—å·®å¼‚ï¼Œä¸ºç†è§£å…¶å†³ç­–æœºåˆ¶æä¾›äº†æ–°è§†è§’ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­è¨€æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯å¯¹æ ‡è®°åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒã€‚è‡ªå›å½’æ¨¡å‹é€šè¿‡è¿­ä»£è®¡ç®—å’Œä»ä¸‹ä¸€ä¸ªæ ‡è®°çš„åˆ†å¸ƒä¸­é‡‡æ ·ç”Ÿæˆå¥å­ã€‚è¿™ç§è¿­ä»£é‡‡æ ·å¼•å…¥äº†éšæœºæ€§ï¼Œä½¿å¾—è¯­è¨€æ¨¡å‹è¢«è®¤ä¸ºæ˜¯è¿›è¡Œæ¦‚ç‡å†³ç­–ã€‚æœ¬æ–‡æ¢è®¨äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šè¯­è¨€æ¨¡å‹æ˜¯å¦å…·å¤‡è´å¶æ–¯æ€ç»´ï¼Ÿç ”ç©¶å‘ç°ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥è¡¨ç°å‡ºè¿‘ä¹ç¡®å®šæ€§çš„å†³ç­–è¡Œä¸ºï¼ŒæŒ‘æˆ˜äº†éšæœºé‡‡æ ·çš„å‡è®¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥åŒºåˆ†å‰å¸ƒæ–¯é‡‡æ ·ä¸­çš„éšæœºå’Œç¡®å®šæ€§å†³ç­–æ¨¡å¼ï¼Œä»¥é˜²æ­¢è¯¯å¯¼æ€§çš„è¯­è¨€æ¨¡å‹å…ˆéªŒæ¨æ–­ã€‚å®éªŒç»“æœä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„å†³ç­–æä¾›äº†é‡è¦è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¯­è¨€æ¨¡å‹åœ¨å†³ç­–è¿‡ç¨‹ä¸­æ˜¯å¦å…·å¤‡è´å¶æ–¯æ€ç»´çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å‡è®¾è¯­è¨€æ¨¡å‹è¿›è¡Œéšæœºå†³ç­–ï¼Œä½†å®é™…å¯èƒ½å­˜åœ¨ç¡®å®šæ€§å†³ç­–ï¼Œå¯¼è‡´å…ˆéªŒæ¨æ–­çš„è¯¯å¯¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŒºåˆ†å‰å¸ƒæ–¯é‡‡æ ·ä¸­çš„éšæœºä¸ç¡®å®šæ€§å†³ç­–æ¨¡å¼ï¼Œé‡æ–°å®¡è§†è¯­è¨€æ¨¡å‹çš„å†³ç­–è¡Œä¸ºï¼Œä»è€Œé¿å…è¯¯å¯¼æ€§å…ˆéªŒçš„æ¨æ–­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹è¯­è¨€æ¨¡å‹çš„å†³ç­–æ¨¡å¼è¿›è¡Œåˆ†ç±»ï¼Œé‡‡ç”¨å®éªŒæ–¹æ³•éªŒè¯ä¸åŒæ¡ä»¶ä¸‹çš„å†³ç­–è¡Œä¸ºï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ¨¡å‹è®­ç»ƒå’Œå†³ç­–æ¨¡å¼åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥åŒºåˆ†éšæœºå’Œç¡®å®šæ€§å†³ç­–æ¨¡å¼ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„éšæœºå‡è®¾æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„é‡‡æ ·æ¸©åº¦å’Œå†³ç­–æ¡ä»¶ï¼Œä½¿ç”¨äº†æœ€å¤§ä¼¼ç„¶ä¼°è®¡ç­‰æŠ€æœ¯ç»†èŠ‚ï¼Œä»¥ç¡®ä¿å¯¹å†³ç­–æ¨¡å¼çš„å‡†ç¡®åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œè¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¡¨ç°å‡ºè¿‘ä¹ç¡®å®šæ€§çš„å†³ç­–è¡Œä¸ºï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„éšæœºå†³ç­–å‡è®¾ã€‚é€šè¿‡æ–°æ–¹æ³•ï¼ŒæˆåŠŸåŒºåˆ†äº†ä¸åŒå†³ç­–æ¨¡å¼ï¼Œä¸ºç†è§£è¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡æ›´å¥½åœ°ç†è§£è¯­è¨€æ¨¡å‹çš„å†³ç­–æœºåˆ¶ï¼Œå¯ä»¥æå‡æ¨¡å‹çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Language models are essentially probability distributions over token sequences. Auto-regressive models generate sentences by iteratively computing and sampling from the distribution of the next token. This iterative sampling introduces stochasticity, leading to the assumption that language models make probabilistic decisions, similar to sampling from unknown distributions. Building on this assumption, prior research has used simulated Gibbs sampling, inspired by experiments designed to elicit human priors, to infer the priors of language models. In this paper, we revisit a critical question: Do language models possess Bayesian brains? Our findings show that under certain conditions, language models can exhibit near-deterministic decision-making, such as producing maximum likelihood estimations, even with a non-zero sampling temperature. This challenges the sampling assumption and undermines previous methods for eliciting human-like priors. Furthermore, we demonstrate that without proper scrutiny, a system with deterministic behavior undergoing simulated Gibbs sampling can converge to a "false prior." To address this, we propose a straightforward approach to distinguish between stochastic and deterministic decision patterns in Gibbs sampling, helping to prevent the inference of misleading language model priors. We experiment on a variety of large language models to identify their decision patterns under various circumstances. Our results provide key insights in understanding decision making of large language models.

