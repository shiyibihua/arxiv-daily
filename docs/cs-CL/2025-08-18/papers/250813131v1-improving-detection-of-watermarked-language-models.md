---
layout: default
title: Improving Detection of Watermarked Language Models
---

# Improving Detection of Watermarked Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13131" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13131v1</a>
  <a href="https://arxiv.org/pdf/2508.13131.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13131v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13131v1', 'Improving Detection of Watermarked Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dara Bahri, John Wieting

**åˆ†ç±»**: cs.CL, cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆæ£€æµ‹æ–¹æ³•ä»¥æå‡æ°´å°è¯­è¨€æ¨¡å‹çš„æ£€æµ‹èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ°´å°æ£€æµ‹` `è¯­è¨€æ¨¡å‹` `æ··åˆæ£€æµ‹` `æ·±åº¦å­¦ä¹ ` `å†…å®¹ç”Ÿæˆç›‘æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ°´å°æ£€æµ‹æ–¹æ³•åœ¨åè®­ç»ƒæ¨¡å‹ä¸­é¢ä¸´ç†µé™åˆ¶ï¼Œå¯¼è‡´æ£€æµ‹æ•ˆæœä¸ç†æƒ³ã€‚
2. æœ¬æ–‡æå‡ºå°†æ°´å°æ£€æµ‹å™¨ä¸éæ°´å°æ£€æµ‹å™¨ç»“åˆçš„æ··åˆæ–¹æ¡ˆï¼Œä»¥æå‡æ£€æµ‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§æ¡ä»¶ä¸‹ï¼Œæ··åˆæ£€æµ‹æ–¹æ¡ˆç›¸è¾ƒäºå•ä¸€æ£€æµ‹å™¨æœ‰æ˜¾è‘—æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ°´å°æŠ€æœ¯æœ€è¿‘æˆä¸ºæ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå†…å®¹çš„æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œæ°´å°çš„å¼ºåº¦é€šå¸¸ä¾èµ–äºè¯­è¨€æ¨¡å‹çš„ç†µå’Œè¾“å…¥æç¤ºé›†ï¼Œå®é™…åº”ç”¨ä¸­ç†µå¯èƒ½å—åˆ°é™åˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨åè®­ç»ƒæ¨¡å‹ä¸­ã€‚æœ¬æ–‡æ¢è®¨äº†é€šè¿‡å°†æ°´å°æ£€æµ‹å™¨ä¸éæ°´å°æ£€æµ‹å™¨ç»“åˆæ¥æ”¹å–„æ£€æµ‹æ•ˆæœï¼Œæå‡ºå¤šç§æ··åˆæ–¹æ¡ˆï¼Œå¹¶åœ¨å¤šç§å®éªŒæ¡ä»¶ä¸‹è§‚å¯Ÿåˆ°æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ°´å°è¯­è¨€æ¨¡å‹æ£€æµ‹ä¸­ç†µé™åˆ¶å¯¼è‡´çš„æ£€æµ‹å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨åè®­ç»ƒæ¨¡å‹ä¸­ï¼Œç°æœ‰æ–¹æ³•çš„æ£€æµ‹æ•ˆæœå—åˆ°æ˜¾è‘—å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç»“åˆæ°´å°æ£€æµ‹å™¨ä¸éæ°´å°æ£€æµ‹å™¨ï¼Œåˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿æ¥æé«˜æ•´ä½“æ£€æµ‹æ€§èƒ½ï¼Œæ—¨åœ¨å…‹æœå•ä¸€æ£€æµ‹å™¨çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ°´å°æ£€æµ‹æ¨¡å—å’Œéæ°´å°æ£€æµ‹æ¨¡å—ï¼ŒäºŒè€…é€šè¿‡ç‰¹å®šçš„èåˆç­–ç•¥è¿›è¡Œç»„åˆï¼Œå½¢æˆä¸€ä¸ªç»¼åˆæ£€æµ‹ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¤šç§æ··åˆæ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨ä¸åŒå®éªŒæ¡ä»¶ä¸‹ä¼˜åŒ–æ£€æµ‹æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§è°ƒæ•´çš„ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†æ°´å°å’Œéæ°´å°æ ·æœ¬çš„å¹³è¡¡ï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•ä¸æ–°å…´çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ··åˆæ£€æµ‹æ–¹æ¡ˆåœ¨å¤šç§æ¡ä»¶ä¸‹çš„æ£€æµ‹å‡†ç¡®ç‡æå‡äº†15%è‡³30%ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå•ä¸€æ£€æµ‹å™¨ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œå°¤å…¶åœ¨é«˜ç†µç¯å¢ƒä¸‹çš„æ£€æµ‹æ•ˆæœæ˜¾è‘—æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†…å®¹ç”Ÿæˆç›‘æµ‹ã€ç‰ˆæƒä¿æŠ¤ä»¥åŠè™šå‡ä¿¡æ¯æ£€æµ‹ç­‰ã€‚é€šè¿‡æå‡æ°´å°æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥æœ‰æ•ˆé˜²æ­¢æ¨¡å‹ç”Ÿæˆçš„å†…å®¹è¢«æ»¥ç”¨ï¼Œä¿æŠ¤çŸ¥è¯†äº§æƒï¼Œå¹¶ä¸ºå†…å®¹å®¡æ ¸æä¾›æ›´å¯é çš„å·¥å…·ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Watermarking has recently emerged as an effective strategy for detecting the generations of large language models (LLMs). The strength of a watermark typically depends strongly on the entropy afforded by the language model and the set of input prompts. However, entropy can be quite limited in practice, especially for models that are post-trained, for example via instruction tuning or reinforcement learning from human feedback (RLHF), which makes detection based on watermarking alone challenging. In this work, we investigate whether detection can be improved by combining watermark detectors with non-watermark ones. We explore a number of hybrid schemes that combine the two, observing performance gains over either class of detector under a wide range of experimental conditions.

