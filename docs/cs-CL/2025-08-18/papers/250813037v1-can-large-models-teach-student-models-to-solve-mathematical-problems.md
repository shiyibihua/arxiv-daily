---
layout: default
title: Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction
---

# Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13037" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.13037v1</a>
  <a href="https://arxiv.org/pdf/2508.13037.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13037v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13037v1', 'Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xinhe Li, Jiajun Liu, Peng Wang

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-18

**Â§áÊ≥®**: Accepted by IJCAI2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â§öLoRA‰∫§‰∫íÁöÑËí∏È¶èÊñπÊ≥ï‰ª•ÊèêÂçáÂ∞èÊ®°ÂûãÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êï∞Â≠¶Êé®ÁêÜ` `Â∞èÂûãËØ≠Ë®ÄÊ®°Âûã` `Ëí∏È¶èËÆ≠ÁªÉ` `Â§öLoRA‰∫§‰∫í` `Áü•ËØÜÁîüÊàê` `Ê∑±Â∫¶Êé®ÁêÜ` `‰∫∫Â∑•Êô∫ËÉΩ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÊï∞ÊçÆÔºåÂØºËá¥Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏äË°®Áé∞‰∏ç‰Ω≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑLoRIDÊñπÊ≥ïÈÄöËøáÂ§öLoRA‰∫§‰∫íÔºåÁªìÂêàÁõ¥ËßÇÊé®ÁêÜÂíåÁü•ËØÜÁîüÊàêÔºåÊ®°‰ªø‰∫∫Á±ªÁöÑ‰∏§ÁßçÊÄùÁª¥Ê®°Âºè„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLoRIDÂú®GSM8KÊï∞ÊçÆÈõÜ‰∏äË∂ÖË∂ä‰∫ÜÁ¨¨‰∫åÂêçÊñπÊ≥ïÔºåÂáÜÁ°ÆÁéáÊèêÂçáÂπÖÂ∫¶ËææÂà∞2.3%Ëá≥16.1%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÊúüÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂‰æùËµñ‰∫éÊï∞Áôæ‰∫øÂèÇÊï∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàSLMsÔºâÊé®ÁêÜËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âà©Áî®LLMsÁîüÊàêÂ§ßÈáèÊï∞ÊçÆËøõË°åËÆ≠ÁªÉ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ§öLoRA‰∫§‰∫íÁöÑÊï∞Â≠¶Êé®ÁêÜËí∏È¶èÊñπÊ≥ïÔºàLoRIDÔºâÔºåÈÄöËøáËæìÂÖ•ÈóÆÈ¢òÂíåÊé®ÁêÜÁîüÊàêÁü•ËØÜÂ¢ûÂº∫Êï∞ÊçÆÈõÜÔºåËÆ≠ÁªÉÁõ¥ËßÇÊé®ÁêÜÂô®ÔºàIRÔºâÂíåÁü•ËØÜÁîüÊàêÂô®ÔºàKGÔºâ„ÄÅÊ∑±Â∫¶Êé®ÁêÜÂô®ÔºàDRÔºâ‰ª•Ê®°‰ªø‰∫∫Á±ªÁöÑ‰∏§ÁßçÊÄùÁª¥Ê®°Âºè„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLoRIDÂú®GSM8KÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁ¨¨‰∫åÂêçÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÊï∞ÊçÆÔºåÂØºËá¥Êé®ÁêÜËÉΩÂäõÊúâÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÁöÑLoRIDÊñπÊ≥ïÈÄöËøáÂ§öLoRA‰∫§‰∫íÔºåÊ®°Êãü‰∫∫Á±ªÁöÑÁõ¥ËßÇÊé®ÁêÜÔºàSystem 1ÔºâÂíåÊ∑±Â∫¶Êé®ÁêÜÔºàSystem 2ÔºâÔºå‰ªéËÄåÊèêÂçáÂ∞èÂûãÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÁõ¥ËßÇÊé®ÁêÜÂô®ÔºàIRÔºâÁîüÊàêÈìæÂºèÊÄùÁª¥ÔºåÁü•ËØÜÁîüÊàêÂô®ÔºàKGÔºâËæìÂá∫Áü•ËØÜÔºåÊ∑±Â∫¶Êé®ÁêÜÂô®ÔºàDRÔºâÂü∫‰∫éÁü•ËØÜËøõË°åÊé®ÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLoRIDÁöÑÂàõÊñ∞Âú®‰∫éÈÄöËøáÂ§öLoRA‰∫§‰∫íÂÆûÁé∞Áü•ËØÜÁöÑÁõ∏‰∫íÂèçÈ¶àÔºåÂ¢ûÂº∫‰∫ÜÂ∞èÂûãÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõ‰∫ÜÊõ¥‰∏∫Á≥ªÁªüÁöÑÂ≠¶‰π†Êú∫Âà∂„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåIR„ÄÅKGÂíåDR‰πãÈó¥ÁöÑËæìÂá∫ÈúÄË¶ÅËøõË°å‰∏ÄËá¥ÊÄßËØÑ‰º∞ÔºåËã•‰∏ç‰∏ÄËá¥ÂàôÈúÄËø≠‰ª£Êé®ÁêÜËøáÁ®ãÔºå‰ª•Á°Æ‰øùÁîüÊàêÁªìÊûúÁöÑÂèØÈù†ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

LoRIDÂú®GSM8KÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÂÆûÈ™åÁªìÊûúÔºåÂáÜÁ°ÆÁéáÂàÜÂà´ÊèêÂçá‰∫Ü2.3%„ÄÅ16.1%„ÄÅ2.4%„ÄÅ12.3%Âíå1.8%ÔºåË∂ÖË∂ä‰∫ÜÁ¨¨‰∫åÂêçÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅÊô∫ËÉΩËæÖÂØºÁ≥ªÁªüÂíåËá™Âä®ÂåñÊï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥Á≠â„ÄÇÈÄöËøáÊèêÂçáÂ∞èÂûãÊ®°ÂûãÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•Âú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ÂÆûÁé∞È´òÊïàÁöÑÂ≠¶‰π†ÂíåÊé®ÁêÜÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent studies have demonstrated that Large Language Models (LLMs) have strong mathematical reasoning abilities but rely on hundreds of billions of parameters. To tackle the challenge of poor reasoning in Small Language Models (SLMs), existing methods typically leverage LLMs to generate massive amounts of data for cramming training. In psychology, they are akin to System 1 thinking, which resolves reasoning problems rapidly based on experience and intuition. However, human learning also requires System 2 thinking, where knowledge is first acquired and then reinforced through practice. Inspired by such two distinct modes of thinking, we propose a novel method based on the multi-LoRA Interaction for mathematical reasoning Distillation (LoRID). First, we input the question and reasoning of each sample into an LLM to create knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only knowledge after receiving problems, while the latter uses that knowledge to perform reasoning. Finally, to address the randomness in the generation of IR and DR, we evaluate whether their outputs are consistent, and the inference process needs to be iterated if not. This step can enhance the mathematical reasoning ability of SLMs through mutual feedback. Experimental results show that LoRID achieves state-of-the-art performance, especially on the GSM8K dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%, 12.3%, and 1.8% accuracy across the five base models, respectively.

