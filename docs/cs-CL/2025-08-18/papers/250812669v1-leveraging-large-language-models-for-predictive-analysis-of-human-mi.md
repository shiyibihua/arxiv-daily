---
layout: default
title: Leveraging Large Language Models for Predictive Analysis of Human Misery
---

# Leveraging Large Language Models for Predictive Analysis of Human Misery

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12669" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12669v1</a>
  <a href="https://arxiv.org/pdf/2508.12669.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12669v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12669v1', 'Leveraging Large Language Models for Predictive Analysis of Human Misery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bishanka Seal, Rahul Seetharaman, Aman Bansal, Abhilash Nandy

**åˆ†ç±»**: cs.CL, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

**å¤‡æ³¨**: 14 pages, 4 tables

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹äººç±»ç—›è‹¦è¯„åˆ†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æƒ…æ„Ÿé¢„æµ‹` `æ¸¸æˆåŒ–è¯„ä¼°` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æƒ…æ„Ÿé¢„æµ‹ä¸­å¾€å¾€ä¾èµ–äºé™æ€çš„è¾“å…¥ï¼Œç¼ºä¹å¯¹ä¸Šä¸‹æ–‡çš„å……åˆ†åˆ©ç”¨ï¼Œå¯¼è‡´é¢„æµ‹å‡†ç¡®æ€§ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¤šç§æç¤ºç­–ç•¥å’Œæ¸¸æˆåŒ–è¯„ä¼°æ¡†æ¶ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿé¢„æµ‹ä¸­çš„è¡¨ç°å’Œé€‚åº”èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°‘æ ·æœ¬æ–¹æ³•åœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬åŸºçº¿ï¼Œä¸”æ¸¸æˆåŒ–è¯„ä¼°èƒ½å¤Ÿæœ‰æ•ˆæµ‹è¯•æ¨¡å‹çš„åŠ¨æ€æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­é¢„æµ‹äººç±»æ„ŸçŸ¥çš„ç—›è‹¦è¯„åˆ†ã€‚è¯¥ä»»åŠ¡è¢«æ¡†å®šä¸ºå›å½’é—®é¢˜ï¼Œæ¨¡å‹ä¸ºæ¯ä¸ªè¾“å…¥è¯­å¥åˆ†é…ä¸€ä¸ªä»0åˆ°100çš„æ ‡é‡å€¼ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å›ºå®šä¸Šä¸‹æ–‡å°‘æ ·æœ¬å’ŒåŸºäºæ£€ç´¢çš„æç¤ºã€‚ç»“æœè¡¨æ˜ï¼Œå°‘æ ·æœ¬æ–¹æ³•åœ¨æƒ…æ„Ÿé¢„æµ‹ä¸­å§‹ç»ˆä¼˜äºé›¶æ ·æœ¬åŸºçº¿ï¼Œå¼ºè°ƒäº†ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„é‡è¦æ€§ã€‚ä¸ºäº†è¶…è¶Šé™æ€è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œç—›è‹¦æ¸¸æˆç§€â€ï¼Œè¿™ä¸€æ–°é¢–çš„æ¸¸æˆåŒ–æ¡†æ¶é€šè¿‡ç»“æ„åŒ–å›åˆæµ‹è¯•LLMsï¼Œè¯„ä¼°å…¶é¢„æµ‹å‡†ç¡®æ€§åŠåŸºäºåé¦ˆçš„é€‚åº”èƒ½åŠ›ã€‚è¯¥è¯„ä¼°æ–¹å¼å±•ç¤ºäº†LLMsåœ¨åŠ¨æ€æƒ…æ„Ÿæ¨ç†ä»»åŠ¡ä¸­çš„å¹¿æ³›æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¦‚ä½•ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­å‡†ç¡®é¢„æµ‹äººç±»æ„ŸçŸ¥çš„ç—›è‹¦è¯„åˆ†ã€‚ç°æœ‰æ–¹æ³•åœ¨æƒ…æ„Ÿé¢„æµ‹ä¸­ç¼ºä¹å¯¹ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤šç§æç¤ºç­–ç•¥ï¼ˆå¦‚å°‘æ ·æœ¬å’Œæ£€ç´¢åŸºäºçš„æç¤ºï¼‰ï¼Œä»¥åŠä¸€ä¸ªæ¸¸æˆåŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿé¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°å’Œé€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥è‡ªç„¶è¯­è¨€æè¿°ã€åº”ç”¨ä¸åŒçš„æç¤ºç­–ç•¥ç”Ÿæˆé¢„æµ‹ã€ä»¥åŠé€šè¿‡â€œç—›è‹¦æ¸¸æˆç§€â€æ¡†æ¶è¿›è¡ŒåŠ¨æ€è¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è¾“å…¥å¤„ç†ã€æ¨¡å‹é¢„æµ‹å’Œåé¦ˆæœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†â€œç—›è‹¦æ¸¸æˆç§€â€è¿™ä¸€æ¸¸æˆåŒ–è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­æµ‹è¯•æ¨¡å‹çš„é€‚åº”æ€§å’Œæ¨ç†èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿçš„é™æ€è¯„ä¼°æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†BERTå¥å­åµŒå…¥è¿›è¡Œæ£€ç´¢æç¤ºï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºå›å½’æŸå¤±ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„é¢„æµ‹ç²¾åº¦ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œç»“åˆäº†å¤šç§æç¤ºç­–ç•¥ä»¥å¢å¼ºæ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°‘æ ·æœ¬æ–¹æ³•åœ¨æƒ…æ„Ÿé¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬åŸºçº¿ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæ¸¸æˆåŒ–è¯„ä¼°æ¡†æ¶æœ‰æ•ˆæµ‹è¯•äº†æ¨¡å‹åœ¨åŠ¨æ€æƒ…æ„Ÿæ¨ç†ä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œå±•ç¤ºäº†LLMsåœ¨å¤æ‚æƒ…æ„Ÿä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¿ƒç†å¥åº·è¯„ä¼°ã€ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡å‡†ç¡®é¢„æµ‹äººç±»æƒ…æ„ŸçŠ¶æ€ï¼Œèƒ½å¤Ÿä¸ºå¿ƒç†å¥åº·å¹²é¢„æä¾›æ•°æ®æ”¯æŒï¼Œæå‡äººæœºäº¤äº’çš„æƒ…æ„Ÿç†è§£èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This study investigates the use of Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The task is framed as a regression problem, where the model assigns a scalar value from 0 to 100 to each input statement. We evaluate multiple prompting strategies, including zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT sentence embeddings. Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction. To move beyond static evaluation, we introduce the "Misery Game Show", a novel gamified framework inspired by a television format. It tests LLMs through structured rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup enables us to assess not only predictive accuracy but also the model's ability to adapt based on corrective feedback. The gamified evaluation highlights the broader potential of LLMs in dynamic emotional reasoning tasks beyond standard regression. Code and data link: https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub

