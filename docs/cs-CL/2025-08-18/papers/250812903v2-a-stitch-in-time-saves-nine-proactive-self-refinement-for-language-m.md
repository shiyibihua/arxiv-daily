---
layout: default
title: A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models
---

# A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12903" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12903v2</a>
  <a href="https://arxiv.org/pdf/2508.12903.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12903v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12903v2', 'A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinyi Han, Xinyi Wang, Haiquan Zhao, Tingyun li, Zishang Jiang, Sihang Jiang, Jiaqing Liang, Xin Lin, Weikang Zhou, Zeye Sun, Fei Yu, Yanghua Xiao

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18 (æ›´æ–°: 2025-10-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸»åŠ¨è‡ªæˆ‘ç²¾ç‚¼æ–¹æ³•ä»¥æå‡è¯­è¨€æ¨¡å‹è¾“å‡ºè´¨é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸»åŠ¨è‡ªæˆ‘ç²¾ç‚¼` `è¯­è¨€æ¨¡å‹` `åŠ¨æ€è°ƒæ•´` `ç”Ÿæˆè¿‡ç¨‹` `æ€§èƒ½æå‡` `é—®é¢˜è§£å†³èƒ½åŠ›` `å®éªŒè¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªæˆ‘ç²¾ç‚¼æ–¹æ³•å¤šä¾èµ–å›ºå®šè¿­ä»£æ¬¡æ•°ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´ç²¾ç‚¼æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºä¸»åŠ¨è‡ªæˆ‘ç²¾ç‚¼ï¼ˆPASRï¼‰æ–¹æ³•ï¼Œä½¿è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€å†³å®šç²¾ç‚¼æ—¶æœºå’Œå†…å®¹ï¼Œæå‡è¾“å‡ºè´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPASRåœ¨å¤šé¡¹ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨Qwen3-8Bä¸Šï¼Œtokenæ¶ˆè€—å‡å°‘41.6%ï¼Œå‡†ç¡®ç‡æå‡8.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè‡ªæˆ‘ç²¾ç‚¼åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¾“å‡ºæ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–äºå›ºå®šè¿­ä»£æ¬¡æ•°çš„ååº”å¼è¿‡ç¨‹ï¼Œéš¾ä»¥æ ¹æ®ç”Ÿæˆä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´ç²¾ç‚¼æ—¶æœºå’Œå†…å®¹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸»åŠ¨è‡ªæˆ‘ç²¾ç‚¼ï¼ˆPASRï¼‰æ–¹æ³•ï¼Œä½¿LLMsèƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸»åŠ¨ç²¾ç‚¼è¾“å‡ºã€‚ä¸å®Œå…¨é‡ç”Ÿæˆå“åº”çš„æ–¹æ³•ä¸åŒï¼ŒPASRæ ¹æ®æ¨¡å‹å†…éƒ¨çŠ¶æ€å’Œä¸Šä¸‹æ–‡åŠ¨æ€å†³å®šæ˜¯å¦ã€ä½•æ—¶åŠå¦‚ä½•è¿›è¡Œç²¾ç‚¼ã€‚é€šè¿‡åœ¨10ä¸ªå¤šæ ·åŒ–ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜PASRæ˜¾è‘—æå‡äº†é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œå°¤å…¶åœ¨Qwen3-8Bä¸Šï¼Œå¹³å‡tokenæ¶ˆè€—å‡å°‘41.6%ï¼Œå‡†ç¡®ç‡æé«˜8.2%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è‡ªæˆ‘ç²¾ç‚¼æ–¹æ³•çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å…¶ä¾èµ–å›ºå®šè¿­ä»£æ¬¡æ•°çš„ååº”å¼è¿‡ç¨‹ï¼Œæ— æ³•æ ¹æ®ç”Ÿæˆä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´ç²¾ç‚¼ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸»åŠ¨è‡ªæˆ‘ç²¾ç‚¼ï¼ˆPASRï¼‰æ–¹æ³•ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸»åŠ¨å†³å®šæ˜¯å¦ã€ä½•æ—¶åŠå¦‚ä½•è¿›è¡Œè¾“å‡ºç²¾ç‚¼ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPASRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç”Ÿæˆæ¨¡å—ã€çŠ¶æ€è¯„ä¼°æ¨¡å—å’Œç²¾ç‚¼å†³ç­–æ¨¡å—ã€‚ç”Ÿæˆæ¨¡å—è´Ÿè´£ç”Ÿæˆåˆæ­¥è¾“å‡ºï¼ŒçŠ¶æ€è¯„ä¼°æ¨¡å—åˆ†ææ¨¡å‹å†…éƒ¨çŠ¶æ€å’Œä¸Šä¸‹æ–‡ï¼Œç²¾ç‚¼å†³ç­–æ¨¡å—åˆ™åŸºäºè¯„ä¼°ç»“æœå†³å®šæ˜¯å¦è¿›è¡Œç²¾ç‚¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šPASRçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ä¸»åŠ¨æ€§ï¼Œèƒ½å¤Ÿæ ¹æ®å®æ—¶ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´ç²¾ç‚¼ç­–ç•¥ï¼Œè€Œéä¾èµ–å›ºå®šçš„è¿­ä»£æ¬¡æ•°ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ›´å…·çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒPASRå¼•å…¥äº†åŠ¨æ€é˜ˆå€¼æœºåˆ¶ï¼Œä»¥å†³å®šç²¾ç‚¼çš„å¿…è¦æ€§ï¼›æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡çš„æƒè¡¡ï¼›ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†è½»é‡çº§çš„çŠ¶æ€è¯„ä¼°ç½‘ç»œï¼Œä»¥æé«˜å®æ—¶æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPASRåœ¨Qwen3-8Bæ¨¡å‹ä¸Šå®ç°äº†å¹³å‡tokenæ¶ˆè€—å‡å°‘41.6%çš„æ˜¾è‘—æå‡ï¼ŒåŒæ—¶å‡†ç¡®ç‡æé«˜äº†8.2%ã€‚è¿™äº›ç»“æœè¡¨æ˜PASRåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è‡ªæˆ‘ç²¾ç‚¼æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€å†…å®¹ç”Ÿæˆã€æ•™è‚²è¾…å¯¼ç­‰åœºæ™¯ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œä¸»åŠ¨è‡ªæˆ‘ç²¾ç‚¼èƒ½å¤Ÿæ˜¾è‘—æå‡è¯­è¨€æ¨¡å‹çš„å“åº”è´¨é‡å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼ŒPASRæ–¹æ³•å¯èƒ½ä¼šåœ¨æ›´å¤šå¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´å¤§çš„æ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in self-refinement have demonstrated significant potential for improving the outputs of large language models (LLMs) through iterative refinement. However, most existing self-refinement methods rely on a reactive process with a fixed number of iterations, making it difficult to determine the optimal timing and content of refinement based on the evolving generation context. Inspired by the way humans dynamically refine their thoughts during execution, we propose ProActive Self-Refinement (PASR), a novel method that enables LLMs to refine their outputs during the generation process. Unlike methods that regenerate entire responses, PASR proactively decides whether, when, and how to refine based on the model's internal state and evolving context. We conduct extensive experiments on a diverse set of 10 tasks to evaluate the effectiveness of PASR. Experimental results show that PASR significantly enhances problem-solving performance. In particular, on Qwen3-8B, PASR reduces average token consumption by 41.6% compared to standard generation, while also achieving an 8.2% improvement in accuracy. Our code and baselines used in the paper are available on GitHub.

