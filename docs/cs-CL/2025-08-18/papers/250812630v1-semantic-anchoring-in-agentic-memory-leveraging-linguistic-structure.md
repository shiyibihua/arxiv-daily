---
layout: default
title: Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context
---

# Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12630" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12630v1</a>
  <a href="https://arxiv.org/pdf/2508.12630.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12630v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12630v1', 'Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maitreyi Chatterjee, Devansh Agarwal

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

**å¤‡æ³¨**: Paper is currently in peer review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰é”šå®šä»¥è§£å†³å¯¹è¯è®°å¿†æŒä¹…æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹è¯ç³»ç»Ÿ` `è¯­è¨€æ¨¡å‹` `è®°å¿†æŒä¹…æ€§` `è¯­ä¹‰é”šå®š` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç»“æ„åŒ–è®°å¿†` `ä¸Šä¸‹æ–‡ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹è¯ç³»ç»Ÿåœ¨å¤šä¼šè¯å’Œé•¿æœŸäº¤äº’ä¸­ï¼Œè®°å¿†æŒä¹…æ€§ä¸è¶³ï¼Œå¯¼è‡´ä¿¡æ¯é—å¤±å’Œä¸Šä¸‹æ–‡ç†è§£å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºè¯­ä¹‰é”šå®šï¼Œé€šè¿‡ç»“åˆä¾èµ–è§£æã€è¯è¯­å…³ç³»æ ‡æ³¨å’ŒæŒ‡ä»£è§£æï¼Œåˆ›å»ºç»“æ„åŒ–çš„è®°å¿†æ¡ç›®ä»¥å¢å¼ºè®°å¿†æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯­ä¹‰é”šå®šåœ¨äº‹å®å›å¿†å’Œè¯è¯­è¿è´¯æ€§æ–¹é¢æ¯”å¼ºåŸºçº¿æå‡äº†18%ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹è¯åœºæ™¯ä¸­å±•ç°äº†å‡ºè‰²çš„æµç•…æ€§å’Œä»»åŠ¡èƒ½åŠ›ï¼Œä½†åœ¨å¤šä¼šè¯å’Œé•¿æœŸäº¤äº’ä¸­ï¼Œç”±äºè®°å¿†æŒä¹…æ€§æœ‰é™ï¼Œå…¶æ•ˆæœå—åˆ°åˆ¶çº¦ã€‚ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå°†å¯¹è¯å†å²å­˜å‚¨ä¸ºå¯†é›†å‘é‡ï¼Œè™½ç„¶èƒ½å¤Ÿæ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä½†å¿½ç•¥äº†æ›´ç»†è‡´çš„è¯­è¨€ç»“æ„ï¼Œå¦‚å¥æ³•ä¾èµ–ã€è¯è¯­å…³ç³»å’ŒæŒ‡ä»£é“¾æ¥ã€‚æœ¬æ–‡æå‡ºäº†è¯­ä¹‰é”šå®šï¼Œä¸€ç§æ··åˆä»£ç†è®°å¿†æ¶æ„ï¼Œé€šè¿‡æ˜¾å¼çš„è¯­è¨€çº¿ç´¢ä¸°å¯Œå‘é‡å­˜å‚¨ï¼Œä»è€Œæ”¹å–„å¯¹ç»†è…»ã€ä¸°å¯Œä¸Šä¸‹æ–‡äº¤æµçš„å›å¿†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰é”šå®šåœ¨äº‹å®å›å¿†å’Œè¯è¯­è¿è´¯æ€§æ–¹é¢æ¯”å¼ºåŸºçº¿æå‡äº†å¤šè¾¾18%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¼šè¯å’Œé•¿æœŸå¯¹è¯ä¸­çš„è®°å¿†æŒä¹…æ€§ä¸è¶³é—®é¢˜ã€‚ç°æœ‰çš„RAGç³»ç»Ÿè™½ç„¶èƒ½å­˜å‚¨å¯¹è¯å†å²ï¼Œä½†ä»…ä¾èµ–å¯†é›†å‘é‡ï¼Œæ— æ³•æ•æ‰å¤æ‚çš„è¯­è¨€ç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè¯­ä¹‰é”šå®šï¼Œé€šè¿‡å¼•å…¥æ˜¾å¼çš„è¯­è¨€çº¿ç´¢ï¼ˆå¦‚å¥æ³•ä¾èµ–ã€è¯è¯­å…³ç³»å’ŒæŒ‡ä»£è§£æï¼‰ï¼Œå¢å¼ºå‘é‡å­˜å‚¨çš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ï¼Œä»è€Œæé«˜å¯¹è¯çš„ä¸Šä¸‹æ–‡ç†è§£å’Œè®°å¿†å›å¿†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä¾èµ–è§£ææ¨¡å—ã€è¯è¯­å…³ç³»æ ‡æ³¨æ¨¡å—å’ŒæŒ‡ä»£è§£ææ¨¡å—ã€‚é¦–å…ˆå¯¹è¾“å…¥å¯¹è¯è¿›è¡Œè§£æï¼Œæå–è¯­è¨€ç»“æ„ä¿¡æ¯ï¼Œç„¶åå°†è¿™äº›ä¿¡æ¯ä¸ä¼ ç»Ÿçš„å‘é‡å­˜å‚¨ç»“åˆï¼Œå½¢æˆç»“æ„åŒ–çš„è®°å¿†æ¡ç›®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è¯­è¨€å­¦ç»“æ„ä¸å‘é‡å­˜å‚¨ç›¸ç»“åˆï¼Œå½¢æˆä¸€ç§æ–°çš„æ··åˆä»£ç†è®°å¿†æ¶æ„ã€‚è¿™ç§æ–¹æ³•ä¸ä¼ ç»Ÿçš„RAGç³»ç»Ÿæœ¬è´¨ä¸Šä¸åŒï¼Œå› ä¸ºå®ƒä¸ä»…å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè¿˜å…³æ³¨è¯­è¨€çš„ç»†å¾®ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä¾èµ–è§£æå’ŒæŒ‡ä»£è§£æé‡‡ç”¨äº†æœ€æ–°çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œç¡®ä¿é«˜æ•ˆå’Œå‡†ç¡®çš„ç»“æ„æå–ã€‚åŒæ—¶ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†å¤šç§è¯­è¨€ç‰¹å¾çš„æƒé‡ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯­ä¹‰é”šå®šåœ¨äº‹å®å›å¿†å’Œè¯è¯­è¿è´¯æ€§æ–¹é¢æ¯”å¼ºåŸºçº¿æå‡äº†å¤šè¾¾18%ã€‚é€šè¿‡æ¶ˆèç ”ç©¶å’Œäººç±»è¯„ä¼°ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•çš„é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤æ‚å¯¹è¯åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹å’Œç¤¾äº¤æœºå™¨äººç­‰éœ€è¦é•¿æœŸå¯¹è¯è®°å¿†çš„åœºæ™¯ã€‚é€šè¿‡æé«˜å¯¹è¯ç³»ç»Ÿçš„è®°å¿†æŒä¹…æ€§å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’è´¨é‡ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€å¿ƒç†å’¨è¯¢ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated impressive fluency and task competence in conversational settings. However, their effectiveness in multi-session and long-term interactions is hindered by limited memory persistence. Typical retrieval-augmented generation (RAG) systems store dialogue history as dense vectors, which capture semantic similarity but neglect finer linguistic structures such as syntactic dependencies, discourse relations, and coreference links. We propose Semantic Anchoring, a hybrid agentic memory architecture that enriches vector-based storage with explicit linguistic cues to improve recall of nuanced, context-rich exchanges. Our approach combines dependency parsing, discourse relation tagging, and coreference resolution to create structured memory entries. Experiments on adapted long-term dialogue datasets show that semantic anchoring improves factual recall and discourse coherence by up to 18% over strong RAG baselines. We further conduct ablation studies, human evaluations, and error analysis to assess robustness and interpretability.

