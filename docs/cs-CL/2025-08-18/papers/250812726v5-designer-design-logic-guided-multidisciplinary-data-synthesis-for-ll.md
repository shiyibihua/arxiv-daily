---
layout: default
title: DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning
---

# DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12726" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12726v5</a>
  <a href="https://arxiv.org/pdf/2508.12726.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12726v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12726v5', 'DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weize Liu, Yongchi Zhao, Yijia Luo, Mingyu Xu, Jiaheng Liu, Yanan Li, Xiguo Hu, Zhiqi Bai, Yuchi Xu, Wenbo Su, Bo Zheng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18 (æ›´æ–°: 2025-12-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDESIGNERä»¥è§£å†³å¤šå­¦ç§‘æ¨ç†æ•°æ®åˆæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†æ•°æ®åˆæˆ` `è®¾è®¡é€»è¾‘` `å¤šå­¦ç§‘æ¨ç†` `æ•™è‚²æŠ€æœ¯` `æ™ºèƒ½é—®ç­”ç³»ç»Ÿ` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†æ•°æ®é›†åœ¨å­¦ç§‘å¹¿åº¦ã€æ¨ç†æ·±åº¦å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†LLMsçš„æ¨ç†èƒ½åŠ›ã€‚
2. æå‡ºDESIGNERç®¡é“ï¼Œé€šè¿‡è®¾è®¡é€»è¾‘æŒ‡å¯¼LLMsæ¨¡ä»¿äººç±»æ•™è‚²è€…çš„é—®é¢˜åˆ›å»ºè¿‡ç¨‹ï¼Œå®ç°é«˜éš¾åº¦é—®é¢˜çš„è‡ªåŠ¨åˆæˆã€‚
3. åˆæˆçš„DLR-Bookå’ŒDLR-Webæ•°æ®é›†åœ¨å¤šå­¦ç§‘æ¨ç†èƒ½åŠ›ä¸Šæ˜¾è‘—æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰æ•°æ®é›†çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒå­¦ç§‘ä¹‹é—´ã€‚ç°æœ‰çš„æ¨ç†æ•°æ®é›†å¾€å¾€ç¼ºä¹å­¦ç§‘å¹¿åº¦ã€æ¨ç†æ·±åº¦å’Œå¤šæ ·æ€§ï¼Œä»¥åŠé—®é¢˜åˆæˆçš„æŒ‡å¯¼åŸåˆ™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†DESIGNERï¼šä¸€ç§åŸºäºè®¾è®¡é€»è¾‘çš„æ¨ç†æ•°æ®åˆæˆç®¡é“ï¼Œåˆ©ç”¨è‡ªç„¶å¯ç”¨çš„å¹¿æ³›åŸå§‹æ–‡æ¡£ï¼ˆå¦‚ä¹¦ç±è¯­æ–™åº“å’Œç½‘ç»œè¯­æ–™åº“ï¼‰ç”Ÿæˆå¤šå­¦ç§‘çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡åå‘å·¥ç¨‹ä»ç°æœ‰é—®é¢˜ä¸­æå–äº†è¶…è¿‡120,000ç§è®¾è®¡é€»è¾‘ï¼Œå¹¶é€šè¿‡ä¸æºæ–‡æ¡£åŒ¹é…ç”Ÿæˆå¯æ§ç±»å‹å’Œéš¾åº¦çš„æ¨ç†é—®é¢˜ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åˆæˆäº†ä¸¤ä¸ªæ¶µç›–75ä¸ªå­¦ç§‘çš„å¤§è§„æ¨¡æ¨ç†æ•°æ®é›†ï¼ŒéªŒè¯ç»“æœæ˜¾ç¤ºåˆæˆé—®é¢˜çš„éš¾åº¦å’Œå¤šæ ·æ€§æ˜¾è‘—ä¼˜äºåŸºçº¿æ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨ç†æ•°æ®é›†åœ¨å­¦ç§‘å¹¿åº¦å’Œæ¨ç†æ·±åº¦ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´LLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥â€œè®¾è®¡é€»è¾‘â€æ¦‚å¿µï¼ŒæŒ‡å¯¼LLMsæ¨¡ä»¿äººç±»æ•™è‚²è€…çš„é—®é¢˜åˆ›å»ºè¿‡ç¨‹ï¼Œä»è€Œå®ç°é«˜éš¾åº¦ã€å¤šæ ·åŒ–é—®é¢˜çš„è‡ªåŠ¨åˆæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€è®¾è®¡é€»è¾‘æå–ã€é—®é¢˜ç”Ÿæˆå’Œæ•°æ®é›†æ„å»ºå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»ä¹¦ç±å’Œç½‘ç»œè¯­æ–™åº“ä¸­æå–åŸå§‹æ–‡æ¡£ï¼Œç„¶ååå‘å·¥ç¨‹æå–è®¾è®¡é€»è¾‘ï¼Œæœ€åç”Ÿæˆæ¨ç†é—®é¢˜å¹¶æ„å»ºæ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè®¾è®¡é€»è¾‘çš„å¼•å…¥ï¼Œä½¿å¾—é—®é¢˜åˆæˆè¿‡ç¨‹æ›´å…·æŒ‡å¯¼æ€§å’Œç³»ç»Ÿæ€§ï¼Œæ˜¾è‘—æå‡äº†é—®é¢˜çš„éš¾åº¦å’Œå¤šæ ·æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿ç”Ÿæˆé—®é¢˜çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶ä½¿ç”¨äº†å¤šç§ç½‘ç»œç»“æ„æ¥ä¼˜åŒ–é—®é¢˜ç”Ÿæˆçš„æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåˆæˆçš„æ•°æ®é›†åœ¨æ¨ç†èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ•°æ®é›†ã€‚ç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨Qwen3å’ŒLlama3æ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒæ—¶ï¼Œä»…ä½¿ç”¨åˆæˆæ•°æ®å°±è¶…è¶Šäº†ç»è¿‡å®Œæ•´åè®­ç»ƒè¿‡ç¨‹çš„å®˜æ–¹æœ€ç»ˆæ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºåˆæˆæ•°æ®çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¤šå­¦ç§‘çŸ¥è¯†æ¨ç†ç­‰ã€‚é€šè¿‡æå‡LLMsåœ¨å¤šå­¦ç§‘æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºæ•™è‚²ã€ç§‘ç ”å’Œä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸå¸¦æ¥æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often lack disciplinary breadth, reasoning depth, and diversity, as well as guiding principles for question synthesis. We propose DESIGNER: a DESIGN-logic-guidEd Reasoning data synthesis pipeline that leverages naturally available, extensive raw documents (e.g., book corpus and web corpus) to generate multidisciplinary challenging questions. We introduce the concept of "design logic" and instruct LLMs to mimic human educators' question-creation process, enabling the automated synthesis of large-scale, high-difficulty questions. We use LLMs to reverse-engineer and abstract over 120,000 design logics from existing questions across various disciplines. By matching these design logics with source documents, we are able to generate reasoning questions with controllable question types and difficulty levels. Using this pipeline, we synthesized two large-scale reasoning datasets that span 75 disciplines: DLR-Book (3.04 million questions from the book corpus) and DLR-Web (1.66 million questions from the web corpus). Data analysis indicates that the questions synthesized by our method exhibit greater difficulty and diversity compared to those in the baseline datasets. We validate our synthesized data through supervised fine-tuning (SFT) on the Qwen3 and Llama3 model families. Our data substantially enhances their multidisciplinary reasoning capabilities, outperforming existing datasets. Notably, by applying SFT on the base versions of these models using only our data, we even surpass their official final models that have undergone the full post-training process.

