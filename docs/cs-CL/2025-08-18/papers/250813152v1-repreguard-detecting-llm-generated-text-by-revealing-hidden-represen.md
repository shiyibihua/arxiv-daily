---
layout: default
title: RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns
---

# RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13152" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13152v1</a>
  <a href="https://arxiv.org/pdf/2508.13152.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13152v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13152v1', 'RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin Chen, Junchao Wu, Shu Yang, Runzhe Zhan, Zeyu Wu, Ziyang Luo, Di Wang, Min Yang, Lidia S. Chao, Derek F. Wong

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

**å¤‡æ³¨**: Accepted to TACL 2025. This version is a pre-MIT Press publication version

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/NLP2CT/RepreGuard)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRepreGuardä»¥è§£å†³LLMç”Ÿæˆæ–‡æœ¬æ£€æµ‹çš„é²æ£’æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬æ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `å†…éƒ¨è¡¨ç¤º` `ç¥ç»æ¿€æ´»` `ç»Ÿè®¡ç‰¹å¾` `é²æ£’æ€§` `å†…å®¹å®¡æ ¸` `è™šå‡ä¿¡æ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMç”Ÿæˆæ–‡æœ¬æ£€æµ‹æ–¹æ³•åœ¨åˆ†å¸ƒå¤–åœºæ™¯ä¸­çš„é²æ£’æ€§ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆåŒºåˆ†ç”Ÿæˆæ–‡æœ¬ä¸äººç±»æ–‡æœ¬ã€‚
2. æœ¬æ–‡æå‡ºRepreGuardï¼Œé€šè¿‡æ”¶é›†LLMçš„å†…éƒ¨è¡¨ç¤ºï¼Œæå–ç‹¬ç‰¹çš„æ¿€æ´»ç‰¹å¾æ¥è¯†åˆ«LLMç”Ÿæˆæ–‡æœ¬ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRepreGuardåœ¨å¤šç§åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡AUROCè¾¾åˆ°94.92%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„å†…å®¹å¯¹äºé˜²æ­¢æ»¥ç”¨å’Œæ„å»ºå¯ä¿¡çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿè‡³å…³é‡è¦ã€‚å°½ç®¡ç°æœ‰æ£€æµ‹æ–¹æ³•è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸­çš„é²æ£’æ€§ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡å‡è®¾ï¼Œä¸ç°æœ‰æ£€æµ‹æ–¹æ³•ä½¿ç”¨çš„ç‰¹å¾ç›¸æ¯”ï¼ŒLLMsçš„å†…éƒ¨è¡¨ç¤ºåŒ…å«æ›´å…¨é¢å’ŒåŸå§‹çš„ç‰¹å¾ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰å’ŒåŒºåˆ†LLMç”Ÿæˆæ–‡æœ¬ï¼ˆLGTï¼‰ä¸äººç±»æ’°å†™æ–‡æœ¬ï¼ˆHWTï¼‰ä¹‹é—´çš„ç»Ÿè®¡æ¨¡å¼å·®å¼‚ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„LLMsä¸ŠéªŒè¯äº†è¿™ä¸€å‡è®¾ï¼Œè§‚å¯Ÿåˆ°å¤„ç†è¿™ä¸¤ç§æ–‡æœ¬æ—¶ç¥ç»æ¿€æ´»æ¨¡å¼å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RepreGuardï¼Œä¸€ç§é«˜æ•ˆçš„åŸºäºç»Ÿè®¡çš„æ£€æµ‹æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepreGuardåœ¨åˆ†å¸ƒå†…ï¼ˆIDï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸­å‡ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œå¹³å‡AUROCè¾¾åˆ°94.92%ï¼ŒåŒæ—¶åœ¨å„ç§æ–‡æœ¬å¤§å°å’Œä¸»æµæ”»å‡»ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMç”Ÿæˆæ–‡æœ¬æ£€æµ‹æ–¹æ³•åœ¨åˆ†å¸ƒå¤–åœºæ™¯ä¸­çš„é²æ£’æ€§ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè¡¨é¢ç‰¹å¾ï¼Œéš¾ä»¥æ•æ‰æ·±å±‚æ¬¡çš„ç»Ÿè®¡æ¨¡å¼å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬å‡è®¾LLMsçš„å†…éƒ¨è¡¨ç¤ºåŒ…å«æ›´ä¸°å¯Œçš„ç‰¹å¾ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åŒºåˆ†LGTä¸HWTã€‚é€šè¿‡åˆ†æç¥ç»æ¿€æ´»æ¨¡å¼ï¼Œæˆ‘ä»¬æå–å‡ºèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«LGTçš„ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRepreGuardçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç‰¹å¾æå–å’Œåˆ†ç±»ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆä½¿ç”¨ä»£ç†æ¨¡å‹æ”¶é›†LGTå’ŒHWTçš„è¡¨ç¤ºï¼Œç„¶åæå–æ¿€æ´»ç‰¹å¾ï¼Œæœ€åé€šè¿‡è®¡ç®—æŠ•å½±åˆ†æ•°è¿›è¡Œåˆ†ç±»ã€‚

**å…³é”®åˆ›æ–°**ï¼šRepreGuardçš„ä¸»è¦åˆ›æ–°åœ¨äºåˆ©ç”¨LLMçš„å†…éƒ¨è¡¨ç¤ºè¿›è¡Œæ–‡æœ¬æ£€æµ‹ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»ŸåŸºäºè¡¨é¢ç‰¹å¾çš„æ£€æµ‹æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•æ‰æ–‡æœ¬çš„ç»Ÿè®¡ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†ç‰¹å¾æå–çš„é˜ˆå€¼ï¼Œå¹¶é‡‡ç”¨äº†é€‚åˆçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åˆ†ç±»æ€§èƒ½ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œä½¿ç”¨äº†é€‚é…äºä¸åŒLLMçš„ä»£ç†æ¨¡å‹ï¼Œä»¥ç¡®ä¿ç‰¹å¾æå–çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRepreGuardåœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–åœºæ™¯ä¸­çš„å¹³å‡AUROCè¾¾åˆ°äº†94.92%ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸åŒæ–‡æœ¬å¤§å°å’Œä¸»æµæ”»å‡»ä¸‹çš„é²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RepreGuardçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬å†…å®¹å®¡æ ¸ã€è™šå‡ä¿¡æ¯æ£€æµ‹å’Œæ•™è‚²é¢†åŸŸçš„ä½œä¸šæ£€æµ‹ç­‰ã€‚éšç€LLMæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«ç”Ÿæˆæ–‡æœ¬çš„å·¥å…·å°†æœ‰åŠ©äºç»´æŠ¤ä¿¡æ¯çš„çœŸå®æ€§å’Œå¯ä¿¡åº¦ï¼Œä¿ƒè¿›AIç³»ç»Ÿçš„å®‰å…¨ä½¿ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Detecting content generated by large language models (LLMs) is crucial for preventing misuse and building trustworthy AI systems. Although existing detection methods perform well, their robustness in out-of-distribution (OOD) scenarios is still lacking. In this paper, we hypothesize that, compared to features used by existing detection methods, the internal representations of LLMs contain more comprehensive and raw features that can more effectively capture and distinguish the statistical pattern differences between LLM-generated texts (LGT) and human-written texts (HWT). We validated this hypothesis across different LLMs and observed significant differences in neural activation patterns when processing these two types of texts. Based on this, we propose RepreGuard, an efficient statistics-based detection method. Specifically, we first employ a surrogate model to collect representation of LGT and HWT, and extract the distinct activation feature that can better identify LGT. We can classify the text by calculating the projection score of the text representations along this feature direction and comparing with a precomputed threshold. Experimental results show that RepreGuard outperforms all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD scenarios, while also demonstrating robust resilience to various text sizes and mainstream attacks. Data and code are publicly available at: https://github.com/NLP2CT/RepreGuard

