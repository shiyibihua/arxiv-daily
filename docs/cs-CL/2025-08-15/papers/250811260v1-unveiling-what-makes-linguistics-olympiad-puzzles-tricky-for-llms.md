---
layout: default
title: UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?
---

# UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.11260" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.11260v1</a>
  <a href="https://arxiv.org/pdf/2508.11260.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.11260v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.11260v1', 'UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mukund Choudhary, KV Aditya Srivatsa, Gaurja Aeron, Antara Raaghavi Bhattacharya, Dang Khoa Dang Dinh, Ikhlasul Akmal Hanif, Daria Kotova, Ekaterina Kochmar, Monojit Choudhury

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-15

**å¤‡æ³¨**: Accepted to COLM 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºè¯­è¨€å­¦å¥¥æ—åŒ¹å…‹éš¾é¢˜å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯­è¨€å­¦æ¨ç†` `ä½èµ„æºè¯­è¨€` `å½¢æ€å¤æ‚æ€§` `è¯­è¨€ç‰¹å¾æ ‡è®°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€å­¦éš¾é¢˜ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å½¢æ€å¤æ‚æ€§è¾ƒé«˜çš„ä»»åŠ¡ä¸­ã€‚
2. è®ºæ–‡é€šè¿‡åˆ†æ629ä¸ªè¯­è¨€å­¦éš¾é¢˜ï¼Œæå‡ºäº†ä½¿ç”¨è¯­è¨€å­¦ç‰¹å¾æ ‡è®°æ¥æ­ç¤ºLLMsçš„å¼±ç‚¹ï¼Œå¹¶å»ºè®®æ”¹è¿›åˆ†è¯å™¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ†è§£è¯ä¸ºè¯­ç´ çš„é¢„å¤„ç†æ­¥éª¤æ˜¾è‘—æé«˜äº†LLMsçš„è§£é¢˜èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨è¯­è¨€å­¦éš¾é¢˜ä¸Šçš„è¡¨ç°å´å§‹ç»ˆä¸ä½³ã€‚è¿™äº›éš¾é¢˜é€šå¸¸æºè‡ªè¯­è¨€å­¦å¥¥æ—åŒ¹å…‹ï¼ˆLOï¼‰ç«èµ›ï¼Œä¸ºè¯„ä¼°LLMsåœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„è¯­è¨€æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ€å°æ±¡æŸ“ç¯å¢ƒã€‚æœ¬æ–‡åˆ†æäº†629ä¸ªé—®é¢˜åœ¨41ç§ä½èµ„æºè¯­è¨€ä¸Šçš„è¡¨ç°ï¼Œé€šè¿‡æ ‡è®°æ¯ä¸ªé—®é¢˜çš„è¯­è¨€å­¦ç‰¹å¾æ¥æ­ç¤ºå…¶å¼±ç‚¹ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨æ¶‰åŠè¾ƒé«˜å½¢æ€å¤æ‚æ€§çš„éš¾é¢˜ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œåœ¨åŒ…å«ä¸è‹±è¯­ç›¸ä¼¼çš„è¯­è¨€ç‰¹å¾çš„éš¾é¢˜ä¸Šè¡¨ç°è¾ƒå¥½ã€‚æ­¤å¤–ï¼Œåˆ†è§£è¯ä¸ºè¯­ç´ çš„é¢„å¤„ç†æ­¥éª¤æé«˜äº†å¯è§£æ€§ï¼Œè¡¨æ˜éœ€è¦æ›´å…·è¯­è¨€ç‰¹å¾çš„åˆ†è¯å™¨ã€‚è¿™äº›å‘ç°ä¸ºè¯­è¨€æ¨ç†å’Œä½èµ„æºè¯­è¨€å»ºæ¨¡ä¸­çš„ä¸€äº›æŒ‘æˆ˜æä¾›äº†è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€å­¦å¥¥æ—åŒ¹å…‹éš¾é¢˜ä¸­çš„è¡¨ç°ä¸ä½³é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å½¢æ€å¤æ‚æ€§è¾ƒé«˜çš„ä»»åŠ¡ä¸­ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹629ä¸ªé—®é¢˜è¿›è¡Œåˆ†æï¼Œæ ‡è®°è¯­è¨€å­¦ç‰¹å¾ï¼Œæ­ç¤ºLLMsçš„å¼±ç‚¹ï¼Œå¹¶æå‡ºåˆ†è§£è¯ä¸ºè¯­ç´ çš„é¢„å¤„ç†æ–¹æ³•ï¼Œä»¥æé«˜è§£é¢˜èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»å’Œæ ‡è®°ï¼Œæ¥ç€åˆ†æLLMsåœ¨ä¸åŒç±»å‹é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œæœ€åé€šè¿‡å®éªŒéªŒè¯åˆ†è§£è¯ä¸ºè¯­ç´ çš„é¢„å¤„ç†å¯¹è§£é¢˜èƒ½åŠ›çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†è¯­è¨€å­¦ç‰¹å¾ä¸LLMsçš„è¡¨ç°ç›¸ç»“åˆï¼Œæ­ç¤ºäº†å½¢æ€å¤æ‚æ€§å¯¹è§£é¢˜èƒ½åŠ›çš„å½±å“ï¼Œå¹¶æå‡ºäº†æ›´å…·è¯­è¨€ç‰¹å¾çš„åˆ†è¯å™¨éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¯¹æ¯”åˆ†æçš„æ–¹æ³•ï¼Œè®¾ç½®äº†ä¸åŒçš„é¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶è¯„ä¼°äº†å…¶å¯¹LLMsè§£é¢˜èƒ½åŠ›çš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨äº†å½¢æ€å¤æ‚æ€§å’Œè¯­è¨€ç‰¹å¾çš„å…³è”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨å¤„ç†å½¢æ€å¤æ‚æ€§è¾ƒä½çš„è¯­è¨€å­¦éš¾é¢˜æ—¶è¡¨ç°æ˜¾è‘—ä¼˜äºé«˜å¤æ‚æ€§éš¾é¢˜ï¼Œä¸”é€šè¿‡åˆ†è§£è¯ä¸ºè¯­ç´ çš„é¢„å¤„ç†æ­¥éª¤ï¼Œè§£é¢˜èƒ½åŠ›æå‡äº†çº¦20%ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥çš„è¯­è¨€æ¨¡å‹ä¼˜åŒ–æä¾›äº†é‡è¦ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­è¨€å­¦ä¹ å·¥å…·å’Œä½èµ„æºè¯­è¨€çš„æœºå™¨ç¿»è¯‘ã€‚é€šè¿‡æ”¹è¿›LLMsåœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„è¡¨ç°ï¼Œå¯ä»¥æ¨åŠ¨è¿™äº›è¯­è¨€çš„æ•°å­—åŒ–å’Œä¿¡æ¯åŒ–è¿›ç¨‹ï¼Œæå‡å…¶åœ¨å…¨çƒåŒ–èƒŒæ™¯ä¸‹çš„å¯ç”¨æ€§å’Œå½±å“åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated potential in reasoning tasks, but their performance on linguistics puzzles remains consistently poor. These puzzles, often derived from Linguistics Olympiad (LO) contests, provide a minimal contamination environment to assess LLMs' linguistic reasoning abilities across low-resource languages. This work analyses LLMs' performance on 629 problems across 41 low-resource languages by labelling each with linguistically informed features to unveil weaknesses. Our analyses show that LLMs struggle with puzzles involving higher morphological complexity and perform better on puzzles involving linguistic features that are also found in English. We also show that splitting words into morphemes as a pre-processing step improves solvability, indicating a need for more informed and language-specific tokenisers. These findings thus offer insights into some challenges in linguistic reasoning and modelling of low-resource languages.

