---
layout: default
title: From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR
---

# From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07534" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07534v2</a>
  <a href="https://arxiv.org/pdf/2508.07534.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07534v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07534v2', 'From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yinqian Min, Yanzipeng Gao, Wayne Xin Zhao, Ji-Rong Wen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-08-16)

**å¤‡æ³¨**: 27pages,25figures. arXiv admin note: text overlap with arXiv:2508.02260

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿåˆ†æRLVRä¸­LLMæ¢ç´¢æœºåˆ¶ä»¥æå‡æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¯éªŒè¯å¥–åŠ±` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æ¢ç´¢æœºåˆ¶` `ç†µ-æ€§èƒ½åˆ†æ` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLæ–¹æ³•åœ¨LLMsçš„æ¢ç´¢è¡Œä¸ºæœºåˆ¶ä¸Šç ”ç©¶ä¸è¶³ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›æå‡çš„æ½œåŠ›æœªè¢«å……åˆ†æŒ–æ˜ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ¢ç´¢ç©ºé—´å¡‘é€ ã€ç†µ-æ€§èƒ½åˆ†æå’ŒRLæ€§èƒ½ä¼˜åŒ–ç­‰æ–¹æ³•ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æLLMsçš„æ¢ç´¢èƒ½åŠ›ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æœ‰æ•ˆçš„æ¢ç´¢ç­–ç•¥ï¼ŒLLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œæä¾›äº†æ–°çš„å®è¯è¯æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä½œä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆèŒƒå¼ï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„åé¦ˆæŒ‡å¯¼LLMsç”Ÿæˆå’Œä¼˜åŒ–å¤æ‚æ¨ç†é“¾ã€‚ç„¶è€Œï¼ŒLLMsçš„æ¢ç´¢è¡Œä¸ºæœºåˆ¶å°šæœªå¾—åˆ°æ·±å…¥ç ”ç©¶ã€‚æœ¬æ–‡ç³»ç»Ÿæ¢è®¨äº†RLVRä¸­çš„æ¢ç´¢èƒ½åŠ›ï¼Œæ¶µç›–å››ä¸ªä¸»è¦æ–¹é¢ï¼šæ¢ç´¢ç©ºé—´çš„å¡‘é€ ã€ç†µ-æ€§èƒ½äº¤æ¢çš„åˆ†æä»¥åŠRLæ€§èƒ½ä¼˜åŒ–ã€‚é€šè¿‡æ•´åˆå·²æœ‰è§è§£ä¸æ–°å®è¯è¯æ®ï¼Œæ—¨åœ¨ä¸ºRLVRç³»ç»Ÿçš„è¿›æ­¥æä¾›åŸºç¡€æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMsåœ¨RLVRä¸­çš„æ¢ç´¢è¡Œä¸ºæœºåˆ¶ä¸æ˜ç¡®çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ¢ç´¢ç­–ç•¥æ¥æå‡æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿåˆ†ææ¢ç´¢ç©ºé—´ã€ç†µä¸æ€§èƒ½çš„å…³ç³»ï¼Œä»¥åŠRLæ€§èƒ½ä¼˜åŒ–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶æ¥æŒ‡å¯¼LLMsçš„æ¢ç´¢è¡Œä¸ºã€‚è¿™æ ·çš„è®¾è®¡æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨LLMsçš„æ½œåŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ¢ç´¢ç©ºé—´å¡‘é€ æ¨¡å—ã€ç†µ-æ€§èƒ½åˆ†ææ¨¡å—å’ŒRLæ€§èƒ½ä¼˜åŒ–æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—é’ˆå¯¹ä¸åŒçš„æ¢ç´¢ç­–ç•¥è¿›è¡Œæ·±å…¥åˆ†æä¸ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†é‡åŒ–åº¦é‡æ¥è¡¨å¾LLMsçš„èƒ½åŠ›è¾¹ç•Œï¼Œå¹¶é€šè¿‡ç†µ-æ€§èƒ½äº¤æ¢çš„åˆ†ææ­ç¤ºäº†è®­ç»ƒé˜¶æ®µä¸å®ä¾‹é—´çš„å…³ç³»ï¼Œè¿™åœ¨ç°æœ‰ç ”ç©¶ä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŸºäºè§„åˆ™çš„åé¦ˆæœºåˆ¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™å¼•å…¥äº†å¤šå±‚æ¬¡çš„æ¨ç†é“¾ç”Ÿæˆæœºåˆ¶ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæå‡äº†LLMsåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ–°æå‡ºçš„æ¢ç´¢ç­–ç•¥åï¼ŒLLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†æ¢ç´¢æœºåˆ¶çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆå’Œå¤æ‚å†³ç­–æ”¯æŒç­‰ã€‚é€šè¿‡æå‡LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå®é™…åœºæ™¯ä¸­æä¾›æ›´ä¸ºå‡†ç¡®å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½å¯¹è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning with verifiable rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based feedback to guide LLMs in generating and refining complex reasoning chains -- a process critically dependent on effective exploration strategies. While prior work has demonstrated RLVR's empirical success, the fundamental mechanisms governing LLMs' exploration behaviors remain underexplored. This technical report presents a systematic investigation of exploration capacities in RLVR, covering four main aspects: (1) exploration space shaping, where we develop quantitative metrics to characterize LLMs' capability boundaries; (2) entropy-performance exchange, analyzed across training stages, individual instances, and token-level patterns; and (3) RL performance optimization, examining methods to effectively translate exploration gains into measurable improvements. By unifying previously identified insights with new empirical evidence, this work aims to provide a foundational framework for advancing RLVR systems.

