---
layout: default
title: SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling
---

# SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08211" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08211v1</a>
  <a href="https://arxiv.org/pdf/2508.08211.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08211v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08211v1', 'SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhuohao Yu, Xingru Jiang, Weizheng Gu, Yidong Wang, Shikun Zhang, Wei Ye

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: 24 pages, 12 figures, code available: https://zhuohaoyu.github.io/SAEMark

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAEMarkä»¥è§£å†³LLMæ°´å°è´¨é‡ä¸å¯è®¿é—®æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æ°´å°æŠ€æœ¯` `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `æ–‡æœ¬ç”Ÿæˆ` `ç‰¹å¾æå–` `æ‹’ç»é‡‡æ ·` `å†…å®¹å½’å±` `è™šå‡ä¿¡æ¯é˜²èŒƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ°´å°æ–¹æ³•åœ¨æ–‡æœ¬è´¨é‡ã€æ¨¡å‹è®¿é—®å’Œæ—¥å¿—æ“ä½œä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨APIæ¨¡å‹å’Œå¤šè¯­è¨€åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. SAEMarkæå‡ºäº†ä¸€ç§åå¤„ç†çš„å¤šä½æ°´å°æ¡†æ¶ï¼Œé€šè¿‡æ¨ç†æ—¶çš„ç‰¹å¾åŸºç¡€æ‹’ç»é‡‡æ ·åµŒå…¥ä¸ªæ€§åŒ–ä¿¡æ¯ï¼Œé¿å…äº†å¯¹æ¨¡å‹æ—¥å¿—çš„ä¿®æ”¹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSAEMarkåœ¨å››ä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨è‹±è¯­æ–‡æœ¬ä¸Šè¾¾åˆ°äº†99.7%çš„F1åˆ†æ•°ï¼Œå±•ç°äº†å…¶å¼ºå¤§çš„å¤šä½æ£€æµ‹èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ–‡æœ¬çš„æ°´å°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SAEMarkæ¡†æ¶ã€‚ç°æœ‰æ–¹æ³•åœ¨æ–‡æœ¬è´¨é‡ã€æ¨¡å‹è®¿é—®å’Œæ—¥å¿—æ“ä½œä¸Šå­˜åœ¨å±€é™ï¼Œæ— æ³•é€‚ç”¨äºAPIæ¨¡å‹å’Œå¤šè¯­è¨€åœºæ™¯ã€‚SAEMarké€šè¿‡æ¨ç†æ—¶çš„ç‰¹å¾åŸºç¡€æ‹’ç»é‡‡æ ·åµŒå…¥ä¸ªæ€§åŒ–ä¿¡æ¯ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æ—¥å¿—æˆ–é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•åŸºäºä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–çš„ç¡®å®šæ€§ç‰¹å¾ï¼Œé€‰æ‹©ä¸ç›®æ ‡ç‰¹å¾ç»Ÿè®¡ä¸€è‡´çš„è¾“å‡ºï¼Œç¡®ä¿æ–‡æœ¬è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAEMarkåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨è‹±è¯­æ–‡æœ¬ä¸Šè¾¾åˆ°äº†99.7%çš„F1åˆ†æ•°ï¼Œå±•ç¤ºäº†å…¶åœ¨å¯æ‰©å±•æ°´å°æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ°´å°æ–¹æ³•åœ¨æ–‡æœ¬è´¨é‡ã€æ¨¡å‹è®¿é—®å’Œæ—¥å¿—æ“ä½œä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯æ— æ³•é€‚ç”¨äºAPIæ¨¡å‹å’Œå¤šè¯­è¨€åœºæ™¯çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSAEMarké€šè¿‡æ¨ç†æ—¶çš„ç‰¹å¾åŸºç¡€æ‹’ç»é‡‡æ ·æ¥åµŒå…¥ä¸ªæ€§åŒ–ä¿¡æ¯ï¼Œé¿å…äº†å¯¹æ¨¡å‹æ—¥å¿—çš„ä¿®æ”¹å’Œé‡æ–°è®­ç»ƒï¼Œä»è€Œä¿æŒäº†æ–‡æœ¬çš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSAEMarkçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾æå–ã€ç›®æ ‡ç‰¹å¾ç»Ÿè®¡åŒ¹é…å’Œæ‹’ç»é‡‡æ ·ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–ç¡®å®šæ€§ç‰¹å¾ï¼Œç„¶åé€‰æ‹©ä¸ç›®æ ‡ç‰¹å¾ç»Ÿè®¡ä¸€è‡´çš„è¾“å‡ºï¼Œæœ€åé€šè¿‡æ‹’ç»é‡‡æ ·ç”Ÿæˆå¸¦æ°´å°çš„æ–‡æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šSAEMarkçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åå¤„ç†çš„å¤šä½æ°´å°æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹æ¨¡å‹æ—¥å¿—çš„æƒ…å†µä¸‹å®ç°ä¸ªæ€§åŒ–ä¿¡æ¯çš„åµŒå…¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºä¸å†ä¾èµ–äºç™½ç›’æ¨¡å‹è®¿é—®å’Œæ—¥å¿—æ“ä½œã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒSAEMarkä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œå¹¶æä¾›äº†ä¸æ°´å°æˆåŠŸæ¦‚ç‡å’Œè®¡ç®—é¢„ç®—ç›¸å…³çš„ç†è®ºä¿è¯ï¼Œç¡®ä¿å…¶åœ¨ä»»ä½•åˆé€‚çš„ç‰¹å¾æå–å™¨ä¸Šå‡æœ‰æ•ˆã€‚å®éªŒä¸­å±•ç¤ºäº†å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ä¸€è‡´æ€§è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SAEMarkåœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶åœ¨è‹±è¯­æ–‡æœ¬ä¸Šçš„F1åˆ†æ•°é«˜è¾¾99.7%ï¼Œå¹¶ä¸”åœ¨å¤šä½æ£€æµ‹å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™è¡¨æ˜SAEMarkåœ¨ä¿æŒæ–‡æœ¬è´¨é‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå®ç°æ°´å°åŠŸèƒ½ï¼Œå…·æœ‰è¾ƒå¼ºçš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SAEMarkçš„ç ”ç©¶æˆæœåœ¨å†…å®¹å½’å±å’Œé˜²æ­¢è™šå‡ä¿¡æ¯ä¼ æ’­æ–¹é¢å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºå¤šè¯­è¨€å’Œå¤šé¢†åŸŸçš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦ä¿æŠ¤ç”Ÿæˆå†…å®¹çš„åœºæ™¯ï¼Œå¦‚æ–°é—»ã€ç¤¾äº¤åª’ä½“å’Œå­¦æœ¯å‡ºç‰ˆç­‰ã€‚æœªæ¥ï¼ŒSAEMarkæœ‰æœ›æ¨åŠ¨æ°´å°æŠ€æœ¯çš„æ ‡å‡†åŒ–å’Œæ™®åŠï¼Œæå‡å†…å®¹çš„å¯è¿½æº¯æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Watermarking LLM-generated text is critical for content attribution and misinformation prevention. However, existing methods compromise text quality, require white-box model access and logit manipulation. These limitations exclude API-based models and multilingual scenarios. We propose SAEMark, a general framework for post-hoc multi-bit watermarking that embeds personalized messages solely via inference-time, feature-based rejection sampling without altering model logits or requiring training. Our approach operates on deterministic features extracted from generated text, selecting outputs whose feature statistics align with key-derived targets. This framework naturally generalizes across languages and domains while preserving text quality through sampling LLM outputs instead of modifying. We provide theoretical guarantees relating watermark success probability and compute budget that hold for any suitable feature extractor. Empirically, we demonstrate the framework's effectiveness using Sparse Autoencoders (SAEs), achieving superior detection accuracy and text quality. Experiments across 4 datasets show SAEMark's consistent performance, with 99.7% F1 on English and strong multi-bit detection accuracy. SAEMark establishes a new paradigm for scalable watermarking that works out-of-the-box with closed-source LLMs while enabling content attribution.

