---
layout: default
title: Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions
---

# Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08192" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08192v1</a>
  <a href="https://arxiv.org/pdf/2508.08192.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08192v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08192v1', 'Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bangsheng Tang, Carl Chengyan Fu, Fei Kou, Grigory Sizov, Haoci Zhang, Jason Park, Jiawen Liu, Jie You, Qirui Yang, Sachin Mehta, Shengyong Cai, Xiaodong Wang, Xingyu Liu, Yunlu Li, Yanjun Zhou, Wei Wei, Zhiwei Zhao, Zixi Qi, Adolfo Victoria, Aya Ibrahim, Bram Wasti, Changkyu Kim, Daniel Haziza, Fei Sun, Giancarlo Delfin, Emily Guo, Jialin Ouyang, Jaewon Lee, Jianyu Huang, Jeremy Reizenstein, Lu Fang, Quinn Zhu, Ria Verma, Vlad Mihailescu, Xingwen Guo, Yan Cui, Ye Hu, Yejin Lee

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: 15 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆçš„æ¨æµ‹è§£ç æ–¹æ³•ä»¥è§£å†³å¤§è§„æ¨¡æ¨ç†æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨æµ‹è§£ç ` `å¤§è§„æ¨¡æ¨ç†` `EAGLEæ¡†æ¶` `GPUä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨æµ‹è§£ç æ–¹æ³•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ‰©å±•æ—¶é¢ä¸´å¤šç§å·¥ç¨‹æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨GPUä¸Šå®ç°ä¸åŒæ“ä½œçš„æ•ˆç‡é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç³»åˆ—è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨å®ç°åŸºäºEAGLEçš„æ¨æµ‹è§£ç ï¼Œä»¥é€‚åº”å¤§è§„æ¨¡ç”Ÿäº§éœ€æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLlama4 Maverickåœ¨æ¨ç†å»¶è¿Ÿä¸Šè¾¾åˆ°äº†æ–°çš„æœ€ä¼˜ï¼Œä¸”åœ¨å¤§æ‰¹é‡æƒ…å†µä¸‹å®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨æµ‹è§£ç æ˜¯åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†é€Ÿåº¦çš„æ ‡å‡†æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ‰©å±•è¿™ä¸€æ–¹æ³•é¢ä¸´å¤šé¡¹å·¥ç¨‹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åœ¨GPUä¸Šé«˜æ•ˆå®ç°ä¸åŒæ“ä½œï¼ˆå¦‚æ ‘å½¢æ³¨æ„åŠ›å’Œå¤šè½®æ¨æµ‹è§£ç ï¼‰ã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†æˆ‘ä»¬ä¸ºLlamaæ¨¡å‹åœ¨ç”Ÿäº§è§„æ¨¡ä¸Šå®ç°çš„åŸºäºEAGLEçš„æ¨æµ‹è§£ç çš„è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–æŠ€æœ¯ã€‚é€šè¿‡è¿™äº›æ”¹è¿›ï¼Œæˆ‘ä»¬å®ç°äº†Llamaæ¨¡å‹çš„æ–°ä¸€ä»£æ¨ç†å»¶è¿Ÿã€‚ä¾‹å¦‚ï¼ŒLlama4 Maverickåœ¨8ä¸ªNVIDIA H100 GPUä¸Šä»¥çº¦4æ¯«ç§’æ¯ä¸ªtokençš„é€Ÿåº¦è§£ç ï¼Œæ¯”ä¹‹å‰å·²çŸ¥çš„æœ€ä½³æ–¹æ³•å¿«10%ã€‚æ­¤å¤–ï¼ŒåŸºäºEAGLEçš„æ¨æµ‹è§£ç ä½¿æˆ‘ä»¬åœ¨ç”Ÿäº§è§„æ¨¡ä¸Šå®ç°äº†å¤§æ‰¹é‡çš„1.4å€è‡³2.0å€çš„åŠ é€Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ‰©å±•æ¨æµ‹è§£ç çš„æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨GPUä¸Šå®ç°ä¸åŒæ“ä½œæ—¶å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥EAGLEæ¡†æ¶ï¼Œä¼˜åŒ–æ¨æµ‹è§£ç çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦å’Œæ•ˆç‡ã€‚è®¾è®¡ä¸Šè€ƒè™‘äº†GPUçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œæ—¨åœ¨æœ€å¤§åŒ–èµ„æºåˆ©ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€æ¨ç†ä¼˜åŒ–å’Œç»“æœè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—é’ˆå¯¹ç‰¹å®šçš„æ€§èƒ½ç“¶é¢ˆè¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿æ•´ä½“æµç¨‹çš„é«˜æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºç»“åˆEAGLEæ¡†æ¶ä¸æ¨æµ‹è§£ç æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ï¼Œå°¤å…¶æ˜¯åœ¨å¤§æ‰¹é‡å¤„ç†æ—¶çš„æ•ˆç‡æå‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨GPUèµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„è¶…å‚æ•°ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†é«˜æ•ˆçš„æ ‘å½¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æå‡æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚å…·ä½“çš„è®¾è®¡ç»†èŠ‚åŒ…æ‹¬æ‰¹é‡å¤§å°çš„åŠ¨æ€è°ƒæ•´å’Œå¤šè½®æ¨æµ‹è§£ç çš„å®ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLlama4 Maverickåœ¨8ä¸ªNVIDIA H100 GPUä¸Šå®ç°äº†çº¦4æ¯«ç§’æ¯ä¸ªtokençš„è§£ç é€Ÿåº¦ï¼Œæ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•å¿«10%ã€‚æ­¤å¤–ï¼ŒåŸºäºEAGLEçš„æ¨æµ‹è§£ç åœ¨å¤§æ‰¹é‡å¤„ç†æ—¶å®ç°äº†1.4å€è‡³2.0å€çš„åŠ é€Ÿï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œå®æ—¶ç¿»è¯‘ç­‰ã€‚é€šè¿‡æé«˜æ¨ç†é€Ÿåº¦ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´æµç•…çš„ç”¨æˆ·ä½“éªŒï¼Œæ»¡è¶³å¤§è§„æ¨¡ç”¨æˆ·çš„éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Speculative decoding is a standard method for accelerating the inference speed of large language models. However, scaling it for production environments poses several engineering challenges, including efficiently implementing different operations (e.g., tree attention and multi-round speculative decoding) on GPU. In this paper, we detail the training and inference optimization techniques that we have implemented to enable EAGLE-based speculative decoding at a production scale for Llama models. With these changes, we achieve a new state-of-the-art inference latency for Llama models. For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the previously best known method. Furthermore, for EAGLE-based speculative decoding, our optimizations enable us to achieve a speed-up for large batch sizes between 1.4x and 2.0x at production scale.

