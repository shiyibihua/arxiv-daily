---
layout: default
title: Can You Trick the Grader? Adversarial Persuasion of LLM Judges
---

# Can You Trick the Grader? Adversarial Persuasion of LLM Judges

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07805" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07805v1</a>
  <a href="https://arxiv.org/pdf/2508.07805.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07805v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07805v1', 'Can You Trick the Grader? Adversarial Persuasion of LLM Judges')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yerin Hwang, Dongryeol Lee, Taegwan Kang, Yongil Kim, Kyomin Jung

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: 19 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºè¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­çš„è¯´æœæ€§åè§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨è¯„ä¼°` `è¯´æœæŠ€å·§` `æ•°å­¦æ¨ç†` `è¯„åˆ†åè§` `äºšé‡Œå£«å¤šå¾·ä¿®è¾` `æ¨¡å‹è„†å¼±æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å¯èƒ½å—åˆ°è¯´æœæ€§è¯­è¨€çš„å½±å“ï¼Œå¯¼è‡´è¯„åˆ†ä¸å…¬æ­£ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸ƒç§è¯´æœæŠ€å·§ï¼Œå¹¶é€šè¿‡åµŒå…¥è¿™äº›æŠ€å·§æ¥è¯„ä¼°å…¶å¯¹è¯­è¨€æ¨¡å‹è¯„åˆ†çš„å½±å“ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¯´æœæ€§è¯­è¨€å¯ä»¥ä½¿é”™è¯¯è§£ç­”çš„è¯„åˆ†å¹³å‡æé«˜8%ï¼Œä¸”æ¨¡å‹è§„æ¨¡çš„å¢åŠ æœªèƒ½æœ‰æ•ˆç¼“è§£è¿™ä¸€é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­ä½œä¸ºè‡ªåŠ¨è¯„ä¼°è€…çš„è§’è‰²æ—¥ç›Šå¢åŠ ï¼Œæœ¬æ–‡é¦–æ¬¡æ­ç¤ºäº†åµŒå…¥ç­–ç•¥æ€§è¯´æœè¯­è¨€å¯èƒ½å¯¼è‡´è¯­è¨€æ¨¡å‹è¯„ä¼°è€…åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ç»™äºˆä¸å…¬æ­£é«˜åˆ†çš„ç°è±¡ã€‚åŸºäºäºšé‡Œå£«å¤šå¾·çš„ä¿®è¾åŸåˆ™ï¼Œç ”ç©¶å½¢å¼åŒ–äº†ä¸ƒç§è¯´æœæŠ€å·§ï¼Œå¹¶åœ¨ç›¸åŒçš„å›ç­”ä¸­åµŒå…¥è¿™äº›æŠ€å·§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¯´æœæ€§è¯­è¨€ä¼šå¯¼è‡´è¯­è¨€æ¨¡å‹è¯„ä¼°è€…å¯¹é”™è¯¯è§£ç­”çš„è¯„åˆ†å¹³å‡æé«˜8%ï¼Œå…¶ä¸­ä¸€è‡´æ€§æŠ€å·§é€ æˆçš„åå·®æœ€ä¸ºä¸¥é‡ã€‚å¢åŠ æ¨¡å‹è§„æ¨¡å¹¶æœªæ˜¾è‘—å‡è½»è¿™ç§è„†å¼±æ€§ï¼Œä¸”å¤šç§è¯´æœæŠ€å·§çš„ç»“åˆä¼šè¿›ä¸€æ­¥æ”¾å¤§åè§ï¼Œè¡¨æ˜åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­å­˜åœ¨é‡è¦çš„è„†å¼±æ€§ï¼ŒäºŸéœ€å¯¹æŠ—è¯´æœæ€§æ”»å‡»çš„æœ‰æ•ˆé˜²å¾¡æªæ–½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å¯èƒ½å—åˆ°è¯´æœæ€§è¯­è¨€å½±å“è€Œå¯¼è‡´è¯„åˆ†ä¸å…¬æ­£çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’ŒæŠµå¾¡è¿™ç§åè§ï¼Œé€ æˆè¯„ä¼°ç»“æœçš„ä¸å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šç ”ç©¶é€šè¿‡å½¢å¼åŒ–ä¸ƒç§è¯´æœæŠ€å·§ï¼Œæ¢è®¨å…¶åœ¨ç›¸åŒå›ç­”ä¸­åµŒå…¥çš„æ•ˆæœï¼Œæ­ç¤ºè¯­è¨€æ¨¡å‹åœ¨è¯„ä¼°æ—¶çš„è„†å¼±æ€§ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨ç³»ç»Ÿæ€§åœ°åˆ†æè¯´æœè¯­è¨€å¯¹è¯„åˆ†çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹ä¸ƒç§è¯´æœæŠ€å·§çš„å®šä¹‰ä¸åµŒå…¥ï¼Œéšååœ¨å…­ä¸ªæ•°å­¦åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è¯´æœæŠ€å·§çš„å®ç°ã€æ¨¡å‹è¯„åˆ†çš„æ”¶é›†ä¸åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†è¯´æœæ€§è¯­è¨€å¯¹è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å½±å“ï¼Œå°¤å…¶æ˜¯ä¸åŒæŠ€å·§çš„ç»„åˆå¦‚ä½•æ”¾å¤§è¯„åˆ†åè§ï¼Œè¿™åœ¨ç°æœ‰æ–‡çŒ®ä¸­å°šæœªè¢«å……åˆ†æ¢è®¨ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­ä½¿ç”¨äº†æ ‡å‡†çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä½œä¸ºåŸºå‡†ï¼Œè®¾è®¡äº†ç›¸åº”çš„å®éªŒæµç¨‹ï¼Œç¡®ä¿äº†è¯´æœæŠ€å·§çš„æœ‰æ•ˆåµŒå…¥ä¸è¯„ä¼°ï¼Œä¸”å¯¹æ¯”äº†ä¸åŒæ¨¡å‹è§„æ¨¡çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¯´æœæ€§è¯­è¨€çš„é”™è¯¯è§£ç­”è¯„åˆ†å¹³å‡æé«˜8%ï¼Œä¸”ä¸€è‡´æ€§æŠ€å·§é€ æˆçš„åå·®æœ€ä¸ºæ˜¾è‘—ã€‚å¢åŠ æ¨¡å‹è§„æ¨¡æœªèƒ½æœ‰æ•ˆå‡è½»è¿™ç§åè§ï¼Œè¡¨æ˜å½“å‰è¯„ä¼°ç³»ç»Ÿå­˜åœ¨é‡è¦çš„è„†å¼±æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²è¯„ä¼°ã€è‡ªåŠ¨è¯„åˆ†ç³»ç»ŸåŠå…¶ä»–ä¾èµ–äºè¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°çš„åœºæ™¯ã€‚é€šè¿‡è¯†åˆ«å’Œç†è§£è¯´æœæ€§è¯­è¨€çš„å½±å“ï¼Œå¯ä»¥ä¸ºæœªæ¥çš„è¯„ä¼°ç³»ç»Ÿè®¾è®¡æä¾›é‡è¦çš„å‚è€ƒï¼Œå¢å¼ºå…¶å…¬æ­£æ€§å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models take on growing roles as automated evaluators in practical settings, a critical question arises: Can individuals persuade an LLM judge to assign unfairly high scores? This study is the first to reveal that strategically embedded persuasive language can bias LLM judges when scoring mathematical reasoning tasks, where correctness should be independent of stylistic variation. Grounded in Aristotle's rhetorical principles, we formalize seven persuasion techniques (Majority, Consistency, Flattery, Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical responses. Across six math benchmarks, we find that persuasive language leads LLM judges to assign inflated scores to incorrect solutions, by up to 8% on average, with Consistency causing the most severe distortion. Notably, increasing model size does not substantially mitigate this vulnerability. Further analysis demonstrates that combining multiple persuasion techniques amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover, the persuasive effect persists under counter prompting strategies, highlighting a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need for robust defenses against persuasion-based attacks.

