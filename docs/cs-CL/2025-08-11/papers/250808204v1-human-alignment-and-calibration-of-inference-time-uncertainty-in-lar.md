---
layout: default
title: Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models
---

# Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08204" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08204v1</a>
  <a href="https://arxiv.org/pdf/2508.08204.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08204v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08204v1', 'Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kyle Moore, Jesse Roberts, Daryl Watson

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: preprint, under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäººç±»å¯¹é½ä¸æ¨ç†æ—¶ä¸ç¡®å®šæ€§æ ¡å‡†æ–¹æ³•ä»¥æå‡LLMç”¨æˆ·ä½“éªŒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ç¡®å®šæ€§æ ¡å‡†` `å¤§å‹è¯­è¨€æ¨¡å‹` `äººç±»å¯¹é½` `æ¨ç†æ—¶ä¸ç¡®å®šæ€§` `æ¨¡å‹æ ¡å‡†` `ç”¨æˆ·ä½“éªŒ` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ ¡å‡†å…³æ³¨è¾ƒå¤šï¼Œä½†å¯¹æ¨¡å‹ä¸ç¡®å®šæ€§ä¸äººç±»ä¸ç¡®å®šæ€§çš„å¯¹é½è¯„ä¼°è¾ƒå°‘ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡è¯„ä¼°å¤šç§æ¨ç†æ—¶ä¸ç¡®å®šæ€§åº¦é‡ï¼Œæ¢ç´¢å…¶ä¸äººç±»ä¸ç¡®å®šæ€§åŠæ¨¡å‹æ ¡å‡†çš„å¯¹é½ç¨‹åº¦ï¼Œæå‡ºäº†æ–°çš„åº¦é‡å˜ä½“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šä¸ªåº¦é‡ä¸äººç±»ä¸ç¡®å®šæ€§é«˜åº¦å¯¹é½ï¼Œå¹¶åœ¨æ­£ç¡®æ€§ç›¸å…³æ€§å’Œåˆ†å¸ƒåˆ†æä¸­å±•ç°å‡ºä¸­åˆ°å¼ºçš„æ¨¡å‹æ ¡å‡†è¯æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ ¡å‡†å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä»¥ä¾¿äºæ¨¡å‹æ§åˆ¶å’Œè°ƒèŠ‚ç”¨æˆ·ä¿¡ä»»ã€‚æ¨ç†æ—¶çš„ä¸ç¡®å®šæ€§ä¸ºæ¨¡å‹æˆ–å¤–éƒ¨æ§åˆ¶æ¨¡å—æä¾›å®æ—¶ä¿¡å·ï¼Œå°¤å…¶é‡è¦ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸€ç³»åˆ—æ¨ç†æ—¶ä¸ç¡®å®šæ€§åº¦é‡ï¼Œä½¿ç”¨æ—¢æœ‰æŒ‡æ ‡å’Œæ–°å˜ä½“ï¼Œæ¢è®¨å…¶ä¸äººç±»ç¾¤ä½“ä¸ç¡®å®šæ€§åŠä¼ ç»Ÿæ¨¡å‹æ ¡å‡†çš„å¯¹é½ç¨‹åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œå¤šä¸ªåº¦é‡ä¸äººç±»ä¸ç¡®å®šæ€§é«˜åº¦å¯¹é½ï¼Œå°½ç®¡ä¸äººç±»ç­”æ¡ˆåå¥½ä¸ä¸€è‡´ã€‚å¯¹äºè¿™äº›æˆåŠŸçš„åº¦é‡ï¼Œæˆ‘ä»¬å‘ç°å…¶åœ¨æ­£ç¡®æ€§ç›¸å…³æ€§å’Œåˆ†å¸ƒåˆ†ææ–¹é¢è¡¨ç°å‡ºä¸­åˆ°å¼ºçš„æ¨¡å‹æ ¡å‡†è¯æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ—¶ä¸ç¡®å®šæ€§ä¸äººç±»ä¸ç¡®å®šæ€§ä¹‹é—´çš„å¯¹é½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹æ ¡å‡†ä¸Šï¼Œç¼ºä¹å¯¹äººç±»ä¸ç¡®å®šæ€§çš„ç›´æ¥è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡è¯„ä¼°ä¸€ç³»åˆ—æ¨ç†æ—¶ä¸ç¡®å®šæ€§åº¦é‡ï¼Œç»“åˆä¼ ç»ŸæŒ‡æ ‡å’Œæ–°å˜ä½“ï¼Œæ¢è®¨å…¶ä¸äººç±»ä¸ç¡®å®šæ€§ä¹‹é—´çš„å…³ç³»ï¼Œä»¥æé«˜æ¨¡å‹çš„å®ç”¨æ€§å’Œç”¨æˆ·ä½“éªŒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå®šä¹‰äº†ä¸ç¡®å®šæ€§åº¦é‡çš„æ ‡å‡†ï¼Œç„¶åé€šè¿‡å®éªŒæ¯”è¾ƒè¿™äº›åº¦é‡ä¸äººç±»ç¾¤ä½“ä¸ç¡®å®šæ€§ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼Œæœ€ååˆ†ææ¨¡å‹æ ¡å‡†çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºæå‡ºäº†æ–°çš„ä¸ç¡®å®šæ€§åº¦é‡å˜ä½“ï¼Œå¹¶é€šè¿‡å®è¯åˆ†æéªŒè¯äº†è¿™äº›åº¦é‡ä¸äººç±»ä¸ç¡®å®šæ€§ä¹‹é—´çš„å¼ºå¯¹é½æ€§ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æ ‡å‡†åº¦é‡å’Œæ–°æå‡ºçš„å˜ä½“ï¼Œè®¾è®¡äº†ç›¸åº”çš„æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿å¯¹é½æ€§å’Œæ ¡å‡†æ€§çš„å…¨é¢è¯„ä¼°ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šä¸ªæ¨ç†æ—¶ä¸ç¡®å®šæ€§åº¦é‡ä¸äººç±»ä¸ç¡®å®šæ€§é«˜åº¦å¯¹é½ï¼Œå°¤å…¶åœ¨æ­£ç¡®æ€§ç›¸å…³æ€§å’Œåˆ†å¸ƒåˆ†æä¸­è¡¨ç°å‡ºä¸­åˆ°å¼ºçš„æ¨¡å‹æ ¡å‡†è¯æ®ã€‚è¿™è¡¨æ˜æ‰€æå‡ºçš„æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—çš„æ½œåŠ›å’Œä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é—®ç­”ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ ¡å‡†ï¼Œèƒ½å¤Ÿå¢å¼ºç”¨æˆ·å¯¹æ¨¡å‹çš„ä¿¡ä»»ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„äº¤äº’ç³»ç»Ÿçš„å‘å±•ï¼Œä¿ƒè¿›äººæœºåä½œçš„æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> There has been much recent interest in evaluating large language models for uncertainty calibration to facilitate model control and modulate user trust. Inference time uncertainty, which may provide a real-time signal to the model or external control modules, is particularly important for applying these concepts to improve LLM-user experience in practice. While many of the existing papers consider model calibration, comparatively little work has sought to evaluate how closely model uncertainty aligns to human uncertainty. In this work, we evaluate a collection of inference-time uncertainty measures, using both established metrics and novel variations, to determine how closely they align with both human group-level uncertainty and traditional notions of model calibration. We find that numerous measures show evidence of strong alignment to human uncertainty, even despite the lack of alignment to human answer preference. For those successful metrics, we find moderate to strong evidence of model calibration in terms of both correctness correlation and distributional analysis.

