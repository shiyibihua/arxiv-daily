---
layout: default
title: Capabilities of GPT-5 on Multimodal Medical Reasoning
---

# Capabilities of GPT-5 on Multimodal Medical Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08224" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08224v2</a>
  <a href="https://arxiv.org/pdf/2508.08224.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08224v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08224v2', 'Capabilities of GPT-5 on Multimodal Medical Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shansong Wang, Mingzhe Hu, Qiang Li, Mojtaba Safari, Xiaofeng Yang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-08-13)

**å¤‡æ³¨**: Corrected some typos

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGPT-5ä½œä¸ºå¤šæ¨¡æ€åŒ»å­¦æ¨ç†çš„é€šç”¨è§£å†³æ–¹æ¡ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `åŒ»å­¦å†³ç­–æ”¯æŒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬ä¸å›¾åƒèåˆ` `é›¶-shotå­¦ä¹ ` `æ¨ç†èƒ½åŠ›æå‡` `ä¸´åºŠåº”ç”¨` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŒ»å­¦å†³ç­–æ”¯æŒç³»ç»Ÿåœ¨æ•´åˆå¤šç§ä¿¡æ¯æºæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ–‡æœ¬å’Œå›¾åƒæ•°æ®æ—¶çš„æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºå°†GPT-5ä½œä¸ºå¤šæ¨¡æ€æ¨ç†å™¨ï¼Œé€šè¿‡ç»Ÿä¸€åè®®è¯„ä¼°å…¶åœ¨åŒ»å­¦é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå±•ç¤ºå…¶å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-5åœ¨å¤šä¸ªåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†GPT-4oå’Œäººç±»ä¸“å®¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å’Œç†è§£å¾—åˆ†ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä½¿å¾—é€šç”¨ç³»ç»Ÿèƒ½å¤Ÿåœ¨æ— éœ€å¤§é‡å¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œè¶Šæ¥è¶Šå¤æ‚çš„é¢†åŸŸç‰¹å®šæ¨ç†ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œå†³ç­–å¾€å¾€éœ€è¦æ•´åˆå¼‚æ„ä¿¡æ¯æºï¼ŒåŒ…æ‹¬æ‚£è€…å™è¿°ã€ç»“æ„åŒ–æ•°æ®å’ŒåŒ»å­¦å›¾åƒã€‚æœ¬ç ”ç©¶å°†GPT-5å®šä½ä¸ºåŒ»å­¦å†³ç­–æ”¯æŒçš„é€šç”¨å¤šæ¨¡æ€æ¨ç†å™¨ï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°å…¶åœ¨æ–‡æœ¬å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­çš„é›¶-shoté“¾å¼æ¨ç†æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-5åœ¨æ‰€æœ‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå…¶ä»–åŸºçº¿ï¼Œå°¤å…¶åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—æå‡ï¼Œè¶…è¶Šäº†äººç±»ä¸“å®¶çš„è¡¨ç°ã€‚è¿™ä¸€è¿›æ­¥å¯èƒ½ä¸ºæœªæ¥ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿçš„è®¾è®¡æä¾›é‡è¦å‚è€ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰åŒ»å­¦å†³ç­–æ”¯æŒç³»ç»Ÿåœ¨å¤šæ¨¡æ€ä¿¡æ¯æ•´åˆå’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ–‡æœ¬å’ŒåŒ»å­¦å›¾åƒæ—¶çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†GPT-5ä½œä¸ºä¸€ä¸ªé€šç”¨çš„å¤šæ¨¡æ€æ¨ç†å™¨ï¼Œé€šè¿‡é›¶-shoté“¾å¼æ¨ç†æ¥å¤„ç†åŒ»å­¦é—®ç­”ä»»åŠ¡ï¼Œæ—¨åœ¨æé«˜å…¶åœ¨å¤æ‚å†³ç­–åœºæ™¯ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è¾“å…¥æ•´åˆã€æ¨ç†è¿‡ç¨‹å’Œè¾“å‡ºç”Ÿæˆå››ä¸ªä¸»è¦æ¨¡å—ï¼Œç¡®ä¿æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯çš„æœ‰æ•ˆç»“åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºGPT-5çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¤„ç†å¼‚æ„æ•°æ®æ—¶è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å•ä¸€æ¨¡æ€æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä¼˜åŒ–çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç‰¹åˆ«å…³æ³¨äºå¤šæ¨¡æ€ä¿¡æ¯çš„èåˆä¸æ¨ç†é“¾çš„æ„å»ºï¼Œä»¥æå‡æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-5åœ¨MedXpertQA MMä»»åŠ¡ä¸­ï¼Œæ¨ç†å’Œç†è§£å¾—åˆ†åˆ†åˆ«æé«˜äº†29.26%å’Œ26.18%ï¼Œè¶…è¶Šäº†GPT-4oå’Œäººç±»ä¸“å®¶ï¼Œæ ‡å¿—ç€å…¶åœ¨å¤šæ¨¡æ€æ¨ç†ä¸Šçš„æ˜¾è‘—è¿›æ­¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿã€åŒ»å­¦æ•™è‚²å’ŒåŒ»ç–—å›¾åƒåˆ†æç­‰ã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼ŒGPT-5èƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å¥½åœ°æ•´åˆä¿¡æ¯ï¼Œä»è€Œåšå‡ºæ›´å‡†ç¡®çš„è¯Šæ–­å’Œæ²»ç–—å†³ç­–ï¼Œæœªæ¥å¯èƒ½å¯¹åŒ»ç–—è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) have enabled general-purpose systems to perform increasingly complex domain-specific reasoning without extensive fine-tuning. In the medical domain, decision-making often requires integrating heterogeneous information sources, including patient narratives, structured data, and medical images. This study positions GPT-5 as a generalist multimodal reasoner for medical decision support and systematically evaluates its zero-shot chain-of-thought reasoning performance on both text-based question answering and visual question answering tasks under a unified protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that GPT-5 consistently outperforms all baselines, achieving state-of-the-art accuracy across all QA benchmarks and delivering substantial gains in multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and understanding scores by +29.26% and +26.18% over GPT-4o, respectively, and surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in understanding. In contrast, GPT-4o remains below human expert performance in most dimensions. A representative case study demonstrates GPT-5's ability to integrate visual and textual cues into a coherent diagnostic reasoning chain, recommending appropriate high-stakes interventions. Our results show that, on these controlled multimodal reasoning benchmarks, GPT-5 moves from human-comparable to above human-expert performance. This improvement may substantially inform the design of future clinical decision-support systems.

