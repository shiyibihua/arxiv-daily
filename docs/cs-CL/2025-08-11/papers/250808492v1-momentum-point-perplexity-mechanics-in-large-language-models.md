---
layout: default
title: Momentum Point-Perplexity Mechanics in Large Language Models
---

# Momentum Point-Perplexity Mechanics in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08492" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08492v1</a>
  <a href="https://arxiv.org/pdf/2508.08492.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08492v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08492v1', 'Momentum Point-Perplexity Mechanics in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lorenzo Tomaz, Judd Rosenblatt, Thomas Berry Jones, Diogo Schwerz de Lucena

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨é‡ç‚¹-å›°æƒ‘åº¦æœºåˆ¶ä»¥ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹çš„å†…éƒ¨çŠ¶æ€å˜åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `çŠ¶æ€å˜åŒ–` `ç‰©ç†å­¦è§†è§’` `é›…å¯æ¯”å¼•å¯¼` `å¯è§£é‡Šæ€§` `æ–‡æœ¬ç”Ÿæˆ` `æ¨¡å‹æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå†…éƒ¨çŠ¶æ€å˜åŒ–çš„è§„å¾‹æ€§å’Œå¯è§£é‡Šæ€§ä¸è¶³ï¼Œå¯¼è‡´æ¨¡å‹è¡Œä¸ºéš¾ä»¥é¢„æµ‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºåŠ¨é‡ç‚¹-å›°æƒ‘åº¦æœºåˆ¶ï¼Œé€šè¿‡ç‰©ç†å­¦è§†è§’åˆ†æéšè—çŠ¶æ€å˜åŒ–ï¼Œå¹¶å¼•å…¥é›…å¯æ¯”å¼•å¯¼æ–¹æ³•è¿›è¡Œæ§åˆ¶ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨ä¸¤ä¸ªæ¨¡å‹ä¸­ï¼Œè¯¥æ–¹æ³•ä¿æŒäº†è¿‘ä¹æ’å®šçš„èƒ½é‡ï¼Œå¹¶ç”Ÿæˆäº†è¯­ä¹‰è´¨é‡æ›´é«˜çš„æ–‡æœ¬å»¶ç»­ï¼Œä¼˜äºè‡ªç„¶è¾“å‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶é‡‡ç”¨åŸºäºç‰©ç†çš„è§†è§’ï¼Œæ¢è®¨å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å†…éƒ¨éšè—çŠ¶æ€å¦‚ä½•éšæ¯ä¸ªtokenå˜åŒ–ã€‚ç ”ç©¶æ¶µç›–20ä¸ªå¼€æºå˜æ¢å™¨æ¨¡å‹ï¼ˆå‚æ•°ä»135Måˆ°3Bï¼‰ï¼Œå‘ç°ä¸€ç§ç»“åˆéšè—çŠ¶æ€å˜åŒ–ç‡å’Œä¸‹ä¸€ä¸ªtokenç¡®å®šæ€§çš„é‡ï¼Œç±»ä¼¼äºç‰©ç†ä¸­çš„èƒ½é‡ï¼Œå‡ ä¹ä¿æŒä¸å˜ã€‚éšæœºæƒé‡æ¨¡å‹æ¯”é¢„è®­ç»ƒæ¨¡å‹æ›´ç´§å¯†åœ°ä¿æŒè¿™ç§â€œèƒ½é‡â€ï¼Œè€Œè®­ç»ƒåˆ™ä½¿æ¨¡å‹è¿›å…¥æ›´å¿«ã€æ›´æœæ–­çš„çŠ¶æ€ï¼Œå…·æœ‰æ›´å¤§çš„å˜å¼‚æ€§ã€‚é€šè¿‡è¿™ç§â€œå¯¹æ•°æ‹‰æ ¼æœ—æ—¥â€è§†è§’ï¼Œæå‡ºäº†ä¸€ç§åä¸ºé›…å¯æ¯”å¼•å¯¼çš„æ§åˆ¶æ–¹æ³•ï¼Œèƒ½å¤Ÿä»¥æœ€å°çš„æ–¹å¼æ‰°åŠ¨éšè—çŠ¶æ€ä»¥åå‘ç›®æ ‡tokenã€‚è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæµ‹è¯•æ¨¡å‹ä¸­ä¿æŒäº†è¿‘ä¹æ’å®šçš„èƒ½é‡ï¼Œå¹¶äº§ç”Ÿäº†æ¯”æ¨¡å‹è‡ªç„¶è¾“å‡ºæ›´é«˜è¯­ä¹‰è´¨é‡çš„å»¶ç»­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å†…éƒ¨çŠ¶æ€å˜åŒ–çš„å¯è§£é‡Šæ€§å’Œå¯é¢„æµ‹æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹å¯¹çŠ¶æ€å˜åŒ–çš„æ·±å…¥ç†è§£ï¼Œå¯¼è‡´æ¨¡å‹è¡Œä¸ºéš¾ä»¥æ§åˆ¶å’Œè§£é‡Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶é€šè¿‡å¼•å…¥ç‰©ç†å­¦ä¸­çš„èƒ½é‡æ¦‚å¿µï¼Œåˆ†æéšè—çŠ¶æ€çš„å˜åŒ–ç‡ä¸ä¸‹ä¸€ä¸ªtokençš„ç¡®å®šæ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ§åˆ¶æ–¹æ³•â€”â€”é›…å¯æ¯”å¼•å¯¼ï¼Œæ—¨åœ¨ä»¥æœ€å°çš„æ‰°åŠ¨å¼•å¯¼æ¨¡å‹ç”Ÿæˆç›®æ ‡tokenã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬çŠ¶æ€å˜åŒ–åˆ†æã€èƒ½é‡ä¿æŒæœºåˆ¶å’Œé›…å¯æ¯”å¼•å¯¼æ§åˆ¶ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆåˆ†ææ¨¡å‹çš„éšè—çŠ¶æ€å˜åŒ–ï¼Œç„¶ååº”ç”¨èƒ½é‡ä¿æŒåŸåˆ™ï¼Œæœ€åé€šè¿‡é›…å¯æ¯”å¼•å¯¼è¿›è¡ŒçŠ¶æ€æ‰°åŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†ç‰©ç†å­¦çš„èƒ½é‡æ¦‚å¿µå¼•å…¥åˆ°å¤§è¯­è¨€æ¨¡å‹çš„åˆ†æä¸­ï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„è§†è§’æ¥ç†è§£å’Œæ§åˆ¶æ¨¡å‹è¡Œä¸ºã€‚è¿™ä¸ä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œç ”ç©¶ä¸­ä½¿ç”¨äº†20ä¸ªä¸åŒè§„æ¨¡çš„å˜æ¢å™¨æ¨¡å‹ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šé‡‡ç”¨äº†ä¸èƒ½é‡ä¿æŒç›¸å…³çš„æŒ‡æ ‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒçŠ¶æ€å˜åŒ–çš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨é›…å¯æ¯”å¼•å¯¼æ–¹æ³•çš„æ¨¡å‹åœ¨ä¿æŒè¿‘ä¹æ’å®šçš„èƒ½é‡çš„åŒæ—¶ï¼Œç”Ÿæˆçš„æ–‡æœ¬å»¶ç»­åœ¨è¯­ä¹‰è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºæ¨¡å‹çš„è‡ªç„¶è¾“å‡ºï¼Œå…·ä½“æå‡å¹…åº¦æœªæ˜ç¡®ç»™å‡ºï¼Œä½†è¯„ä»·ç»“æœæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿå’Œå†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„å¯é¢„æµ‹æ€§å’Œå¯¹äººç±»æ„å›¾çš„å¯¹é½ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å‡å°‘é£é™©ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¸ºå¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§æä¾›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We take a physics-based approach to studying how the internal hidden states of large language models change from token to token during inference. Across 20 open-source transformer models (135M-3B parameters), we find that a quantity combining the rate of change in hidden states and the model's next-token certainty, analogous to energy in physics, remains nearly constant. Random-weight models conserve this "energy" more tightly than pre-trained ones, while training shifts models into a faster, more decisive regime with greater variability. Using this "log-Lagrangian" view, we derive a control method called Jacobian steering, which perturbs hidden states in the minimal way needed to favor a target token. This approach maintained near-constant energy in two tested models and produced continuations rated higher in semantic quality than the models' natural outputs. Viewing transformers through this mechanics lens offers a principled basis for interpretability, anomaly detection, and low-risk steering. This could help make powerful models more predictable and aligned with human intent.

