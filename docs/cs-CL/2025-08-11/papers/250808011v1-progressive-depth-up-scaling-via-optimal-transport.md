---
layout: default
title: Progressive Depth Up-scaling via Optimal Transport
---

# Progressive Depth Up-scaling via Optimal Transport

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08011" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08011v1</a>
  <a href="https://arxiv.org/pdf/2508.08011.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08011v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08011v1', 'Progressive Depth Up-scaling via Optimal Transport')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingzi Cao, Xi Wang, Nikolaos Aletras

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOptimal Transportæ·±åº¦ä¸Šé‡‡æ ·ä»¥è§£å†³ç¥ç»å…ƒæ’åˆ—ä¸åŒ¹é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ·±åº¦å­¦ä¹ ` `æœ€ä¼˜ä¼ è¾“` `æ¨¡å‹è®­ç»ƒ` `ç¥ç»å…ƒå¯¹é½` `Transformer` `æ€§èƒ½æå‡` `è®­ç»ƒæ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ·±åº¦ä¸Šé‡‡æ ·æ–¹æ³•é€šå¸¸é€šè¿‡å¤åˆ¶æˆ–å¹³å‡åŸºç¡€å±‚çš„æƒé‡ï¼Œå¿½è§†äº†ç¥ç»å…ƒæ’åˆ—çš„å·®å¼‚ï¼Œå¯¼è‡´æ½œåœ¨çš„æ€§èƒ½æŸå¤±ã€‚
2. æœ¬æ–‡æå‡ºOptimal Transportæ·±åº¦ä¸Šé‡‡æ ·ï¼ˆOpT-DeUSï¼‰ï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“å¯¹ç›¸é‚»åŸºç¡€å±‚çš„Transformerå—è¿›è¡Œå¯¹é½å’Œèåˆï¼Œä»¥å‡è½»ç¥ç»å…ƒæ’åˆ—ä¸åŒ¹é…çš„é—®é¢˜ã€‚
3. OpT-DeUSåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡çš„æŒç»­é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒä¸­è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨æ–°å±‚æ’å…¥ä½ç½®çš„é€‰æ‹©ä¸Šä¹Ÿæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ·±åº¦ä¸Šé‡‡æ ·èƒ½å¤Ÿæé«˜æ€§èƒ½ï¼Œä½†ä¼šå¸¦æ¥æ˜¾è‘—çš„è®­ç»ƒæˆæœ¬ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¤åˆ¶æˆ–å¹³å‡åŸºç¡€å±‚çš„æƒé‡æ¥å®ç°æ·±åº¦ä¸Šé‡‡æ ·ï¼Œå¿½è§†äº†ç¥ç»å…ƒæ’åˆ—çš„å·®å¼‚ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Optimal Transportæ·±åº¦ä¸Šé‡‡æ ·ï¼ˆOpT-DeUSï¼‰ï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“æ–¹æ³•å¯¹ç›¸é‚»åŸºç¡€å±‚çš„Transformerå—è¿›è¡Œå¯¹é½å’Œèåˆï¼Œä»è€Œåˆ›å»ºæ–°çš„å±‚ï¼Œå‡è½»å±‚é—´ç¥ç»å…ƒæ’åˆ—ä¸åŒ¹é…çš„é—®é¢˜ã€‚OpT-DeUSåœ¨æŒç»­é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒä¸­ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šå®ç°äº†æ›´å¥½çš„æ•´ä½“æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œå°†æ–°å±‚æ’å…¥é è¿‘é¡¶éƒ¨çš„ä½ç½®èƒ½å¤Ÿæé«˜è®­ç»ƒæ•ˆç‡ï¼Œå¹¶è·å¾—é¢å¤–çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ·±åº¦ä¸Šé‡‡æ ·æ–¹æ³•ä¸­ç”±äºç¥ç»å…ƒæ’åˆ—å·®å¼‚å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç®€å•å¤åˆ¶æˆ–å¹³å‡æƒé‡ï¼Œæœªèƒ½æœ‰æ•ˆå¯¹é½ç¥ç»å…ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºOptimal Transportæ·±åº¦ä¸Šé‡‡æ ·ï¼ˆOpT-DeUSï¼‰ï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“æŠ€æœ¯å¯¹ç›¸é‚»åŸºç¡€å±‚çš„Transformerå—è¿›è¡Œå¯¹é½å’Œèåˆï¼Œä»¥åˆ›å»ºæ–°çš„å±‚ï¼Œä»è€Œå‡è½»ç¥ç»å…ƒæ’åˆ—ä¸åŒ¹é…çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOpT-DeUSçš„æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆè¯†åˆ«ç›¸é‚»åŸºç¡€å±‚çš„Transformerå—ï¼Œç„¶ååº”ç”¨æœ€ä¼˜ä¼ è¾“ç®—æ³•è¿›è¡Œå¯¹é½ï¼Œæœ€åèåˆè¿™äº›å—ä»¥ç”Ÿæˆæ–°çš„å±‚ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬å¯¹é½æ¨¡å—å’Œèåˆæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šOpT-DeUSçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä½¿ç”¨æœ€ä¼˜ä¼ è¾“æ–¹æ³•è¿›è¡Œç¥ç»å…ƒå¯¹é½ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„æƒé‡å¤åˆ¶æˆ–å¹³å‡å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒOpT-DeUSé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¯¹é½æ•ˆæœï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†æ–°çš„å±‚æ’å…¥ç­–ç•¥ï¼Œå°¤å…¶æ˜¯é è¿‘é¡¶éƒ¨çš„æ’å…¥ä½ç½®è¢«è¯æ˜èƒ½æœ‰æ•ˆç¼©çŸ­åå‘ä¼ æ’­æ—¶é—´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOpT-DeUSåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡çš„æŒç»­é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒä¸­ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æé«˜äº†æ•´ä½“æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæ’å…¥æ–°å±‚é è¿‘é¡¶éƒ¨çš„ä½ç½®æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œç¼©çŸ­äº†åå‘ä¼ æ’­æ—¶é—´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆè®­ç»ƒå’Œå¿«é€Ÿè¿­ä»£çš„åœºæ™¯ä¸­ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ï¼ŒOpT-DeUSèƒ½å¤Ÿä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸå¸¦æ¥å®é™…ä»·å€¼ï¼Œå¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Scaling Large Language Models (LLMs) yields performance gains but incurs substantial training costs. Depth up-scaling offers training efficiency by adding new layers to pre-trained models. However, most existing methods copy or average weights from base layers, neglecting neuron permutation differences. This limitation can potentially cause misalignment that harms performance. Inspired by applying Optimal Transport (OT) for neuron alignment, we propose Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses Transformer blocks in adjacent base layers via OT for new layer creation, to mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better overall performance and offers improved training efficiency than existing methods for continual pre-training and supervised fine-tuning across different model sizes. To further evaluate the impact of interpolation positions, our extensive analysis shows that inserting new layers closer to the top results in higher training efficiency due to shorter back-propagation time while obtaining additional performance gains.

