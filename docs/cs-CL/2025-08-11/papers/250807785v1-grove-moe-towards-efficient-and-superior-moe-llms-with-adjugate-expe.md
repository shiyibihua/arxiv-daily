---
layout: default
title: Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts
---

# Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07785" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07785v1</a>
  <a href="https://arxiv.org/pdf/2508.07785.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07785v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07785v1', 'Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoyuan Wu, Haoxing Chen, Xiaodong Chen, Zhanchao Zhou, Tieyuan Chen, Yihong Zhuang, Guoshan Lu, Zenan Huang, Junbo Zhao, Lin Liu, Zhenzhong Lan, Bei Yu, Jianguo Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGrove MoEä»¥è§£å†³ä¼ ç»ŸMoEæ¨¡å‹çš„è®¡ç®—æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `åŠ¨æ€æ¿€æ´»` `è®¡ç®—æ•ˆç‡` `æ¨¡å‹æ‰©å±•æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„MoEæ¨¡å‹åœ¨å¤„ç†ä¸åŒå¤æ‚åº¦è¾“å…¥æ—¶ï¼Œå›ºå®šæ¿€æ´»å‚æ•°æ•°é‡ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚
2. Grove MoEé€šè¿‡å¼•å…¥ä¸åŒå¤§å°çš„ä¸“å®¶å’ŒåŠ¨æ€æ¿€æ´»æœºåˆ¶ï¼Œæå‡äº†æ¨¡å‹çš„è®¡ç®—æ•ˆç‡å’Œçµæ´»æ€§ã€‚
3. GroveMoEæ¨¡å‹åœ¨åŠ¨æ€æ¿€æ´»å‚æ•°çš„åŸºç¡€ä¸Šï¼Œæ€§èƒ½ä¸åŒç±»æˆ–æ›´å¤§è§„æ¨¡çš„å¼€æºæ¨¡å‹ç›¸å½“ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„æ˜¯ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é‡è¦åŸºç¡€ï¼Œèƒ½å¤Ÿé€šè¿‡ç¨€ç–å‚æ•°æ¿€æ´»å®ç°å¯æ‰©å±•æ€§ã€‚ç„¶è€Œï¼Œä¼ ç»ŸMoEæ¶æ„ä½¿ç”¨å‡åŒ€å¤§å°çš„ä¸“å®¶ï¼Œå›ºå®šæ¿€æ´»å‚æ•°æ•°é‡ï¼Œé™åˆ¶äº†è®¡ç®—æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºGrove MoEï¼Œä¸€ç§æ–°é¢–çš„æ¶æ„ï¼Œç»“åˆäº†ä¸åŒå¤§å°çš„ä¸“å®¶ï¼Œçµæ„Ÿæ¥æºäºå¼‚æ„big.LITTLE CPUæ¶æ„ã€‚è¯¥æ¶æ„å¼•å…¥äº†åŠ¨æ€æ¿€æ´»æœºåˆ¶çš„ä¼´éšä¸“å®¶ï¼Œæ‰©å±•äº†æ¨¡å‹å®¹é‡ï¼ŒåŒæ—¶ä¿æŒå¯æ§çš„è®¡ç®—å¼€é”€ã€‚åŸºäºæ­¤æ¶æ„ï¼Œæœ¬æ–‡å±•ç¤ºäº†GroveMoE-Baseå’ŒGroveMoE-Instä¸¤ä¸ª33Bå‚æ•°çš„LLMsï¼Œé‡‡ç”¨äº†ä¸­æœŸè®­ç»ƒå’ŒåæœŸè®­ç»ƒçš„å‡çº§ç­–ç•¥ï¼ŒåŠ¨æ€æ¿€æ´»3.14-3.28Bå‚æ•°ï¼Œæ€§èƒ½ä¸åŒç±»æˆ–æ›´å¤§è§„æ¨¡çš„å¼€æºæ¨¡å‹ç›¸å½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¼ ç»Ÿçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ä½¿ç”¨å‡åŒ€å¤§å°çš„ä¸“å®¶ï¼Œå›ºå®šæ¿€æ´»å‚æ•°æ•°é‡ï¼Œæ— æ³•æ ¹æ®è¾“å…¥å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGrove MoEé€šè¿‡å¼•å…¥ä¸åŒå¤§å°çš„ä¸“å®¶å’ŒåŠ¨æ€æ¿€æ´»æœºåˆ¶ï¼Œçµæ„Ÿæ¥æºäºbig.LITTLE CPUæ¶æ„ï¼Œæ—¨åœ¨æ ¹æ®è¾“å…¥å¤æ‚åº¦çµæ´»æ¿€æ´»å‚æ•°ï¼Œä»è€Œæå‡è®¡ç®—æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGrove MoEæ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œå…¶ä¸­ä¼´éšä¸“å®¶è´Ÿè´£åŠ¨æ€æ¿€æ´»ï¼Œæ¨¡å‹å®¹é‡å¯ä»¥æ ¹æ®è¾“å…¥å¤æ‚åº¦è¿›è¡Œæ‰©å±•ï¼Œæ•´ä½“æ¶æ„è®¾è®¡æ³¨é‡åœ¨ä¿æŒè®¡ç®—å¼€é”€å¯æ§çš„åŒæ—¶æå‡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šGrove MoEçš„æœ€å¤§åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¼´éšä¸“å®¶çš„åŠ¨æ€æ¿€æ´»æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„å¤æ‚æ€§çµæ´»è°ƒæ•´æ¿€æ´»çš„å‚æ•°æ•°é‡ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å›ºå®šæ¿€æ´»æœºåˆ¶å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†33Bå‚æ•°çš„GroveMoE-Baseå’ŒGroveMoE-Instï¼Œç»“åˆä¸­æœŸå’ŒåæœŸè®­ç»ƒçš„å‡çº§ç­–ç•¥ï¼ŒåŠ¨æ€æ¿€æ´»3.14-3.28Bå‚æ•°ï¼Œç¡®ä¿åœ¨ä¸åŒè¾“å…¥æ¡ä»¶ä¸‹çš„é«˜æ•ˆè®¡ç®—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Grove MoEæ¨¡å‹åœ¨åŠ¨æ€æ¿€æ´»å‚æ•°çš„åŸºç¡€ä¸Šï¼ŒæˆåŠŸæ¿€æ´»3.14-3.28Bå‚æ•°ï¼Œæ€§èƒ½ä¸åŒç±»æˆ–æ›´å¤§è§„æ¨¡çš„å¼€æºæ¨¡å‹ç›¸å½“ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Grove MoEçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è®¡ç®—æ•ˆç‡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜çš„æ€§èƒ½ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate scalability by enabling sparse parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size.

