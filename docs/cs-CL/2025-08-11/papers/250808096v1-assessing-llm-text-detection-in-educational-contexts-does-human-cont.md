---
layout: default
title: Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?
---

# Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08096" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08096v1</a>
  <a href="https://arxiv.org/pdf/2508.08096.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08096v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08096v1', 'Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lukas Gehring, Benjamin PaaÃŸen

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: Preprint as provided by the authors (19 pages, 12 figures, 9 tables)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGEDEæ•°æ®é›†ä»¥è§£å†³æ•™è‚²ç¯å¢ƒä¸­çš„LLMæ–‡æœ¬æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬æ£€æµ‹` `æ•™è‚²æŠ€æœ¯` `å­¦æœ¯è¯šä¿¡` `æ•°æ®é›†æ„å»º` `å­¦ä¹ åˆ†æ` `è¯¯æŠ¥ç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMæ–‡æœ¬æ£€æµ‹æ–¹æ³•åœ¨å¤„ç†å­¦ç”Ÿè´¡çŒ®æ°´å¹³ä¸­é—´çš„æ–‡æœ¬æ—¶è¡¨ç°ä¸ä½³ï¼Œå®¹æ˜“äº§ç”Ÿè¯¯æŠ¥ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†GEDEï¼Œå¹¶å¼•å…¥è´¡çŒ®æ°´å¹³çš„æ¦‚å¿µï¼Œä»¥æ›´å¥½åœ°æ•æ‰å­¦ç”Ÿåœ¨ä½œä¸šä¸­çš„è´¡çŒ®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ£€æµ‹å™¨åœ¨åˆ†ç±»LLMæ”¹è¿›çš„å­¦ç”Ÿæ–‡æœ¬æ—¶å‡†ç¡®ç‡è¾ƒä½ï¼Œå°¤å…¶åœ¨æ•™è‚²ç¯å¢ƒä¸­å½±å“æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥å’Œå¯åŠæ€§æé«˜ï¼Œå­¦ç”Ÿè‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬çš„ç°è±¡æ—¥ç›Šæ™®éï¼Œè¿™ç»™æ•™è‚²æœºæ„å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚ä¸ºç»´æŠ¤å­¦æœ¯è¯šä¿¡å¹¶ç¡®ä¿å­¦ç”Ÿå­¦ä¹ ï¼Œè‡ªåŠ¨æ£€æµ‹LLMç”Ÿæˆæ–‡æœ¬çš„å­¦ä¹ åˆ†ææ–¹æ³•å˜å¾—æ„ˆå‘é‡è¦ã€‚æœ¬æ–‡åŸºå‡†æµ‹è¯•äº†ä¸åŒæœ€å…ˆè¿›æ£€æµ‹å™¨åœ¨æ•™è‚²ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†â€”â€”æ•™è‚²ä¸­çš„ç”Ÿæˆæ€§è®ºæ–‡æ£€æµ‹ï¼ˆGEDEï¼‰ï¼Œè¯¥æ•°æ®é›†åŒ…å«900å¤šç¯‡å­¦ç”Ÿæ’°å†™çš„è®ºæ–‡å’Œè¶…è¿‡12,500ç¯‡æ¥è‡ªä¸åŒé¢†åŸŸçš„LLMç”Ÿæˆè®ºæ–‡ã€‚æˆ‘ä»¬å¼•å…¥äº†è´¡çŒ®æ°´å¹³çš„æ¦‚å¿µï¼Œè¡¨ç¤ºå­¦ç”Ÿåœ¨ä½œä¸šä¸­çš„è´¡çŒ®ç¨‹åº¦ï¼Œå‘ç°å¤§å¤šæ•°æ£€æµ‹å™¨åœ¨å‡†ç¡®åˆ†ç±»ä¸­é—´è´¡çŒ®æ°´å¹³çš„æ–‡æœ¬æ—¶å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶å®¹æ˜“äº§ç”Ÿè¯¯æŠ¥ï¼Œè¿™åœ¨æ•™è‚²ç¯å¢ƒä¸­å¯èƒ½å¯¹å­¦ç”Ÿé€ æˆä¸¥é‡å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ•™è‚²ç¯å¢ƒä¸­LLMç”Ÿæˆæ–‡æœ¬çš„æ£€æµ‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å­¦ç”Ÿè´¡çŒ®æ°´å¹³ä¸­é—´çš„æ–‡æœ¬æ—¶å­˜åœ¨è¯¯æŠ¥ç‡é«˜çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è´¡çŒ®æ°´å¹³çš„æ¦‚å¿µï¼Œè®ºæ–‡èƒ½å¤Ÿæ›´ç»†è‡´åœ°åˆ†æå­¦ç”Ÿåœ¨ä½œä¸šä¸­çš„è´¡çŒ®ï¼Œä»è€Œæé«˜æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€è´¡çŒ®æ°´å¹³å®šä¹‰ã€æ£€æµ‹å™¨åŸºå‡†æµ‹è¯•ç­‰ä¸»è¦æ¨¡å—ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°ä¸åŒæ£€æµ‹å™¨çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†GEDEæ•°æ®é›†å’Œè´¡çŒ®æ°´å¹³çš„æ¦‚å¿µï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„åˆ†ç±»æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ å­¦ç”Ÿçš„å®é™…è´¡çŒ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œè®¾ç½®äº†å¤šç§è´¡çŒ®æ°´å¹³çš„æ–‡æœ¬æ ·æœ¬ï¼Œå¹¶ä½¿ç”¨äº†å¤šç§æ£€æµ‹å™¨è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç¡®ä¿äº†å®éªŒçš„å…¨é¢æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°æ£€æµ‹å™¨åœ¨å¤„ç†LLMæ”¹è¿›çš„å­¦ç”Ÿæ–‡æœ¬æ—¶å‡†ç¡®ç‡ä½äº50%ï¼Œå°¤å…¶åœ¨ä¸­é—´è´¡çŒ®æ°´å¹³çš„æ–‡æœ¬ä¸Šï¼Œè¯¯æŠ¥ç‡æ˜¾è‘—é«˜äºé¢„æœŸã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨æ•™è‚²ç¯å¢ƒä¸­ä½¿ç”¨è¿™äº›æ£€æµ‹å™¨çš„æ½œåœ¨é£é™©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æœºæ„çš„å­¦æœ¯è¯šä¿¡ç»´æŠ¤ã€åœ¨çº¿å­¦ä¹ å¹³å°çš„å†…å®¹å®¡æ ¸ä»¥åŠæ–‡æœ¬ç”Ÿæˆå·¥å…·çš„ä½¿ç”¨è§„èŒƒã€‚é€šè¿‡æé«˜LLMæ–‡æœ¬æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è¯¯æŠ¥ï¼Œä¿æŠ¤å­¦ç”Ÿçš„å­¦ä¹ æƒç›Šï¼Œä¿ƒè¿›æ•™è‚²å…¬å¹³ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨æ›´å¤šé’ˆå¯¹LLMç”Ÿæˆæ–‡æœ¬çš„æ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in Large Language Models (LLMs) and their increased accessibility have made it easier than ever for students to automatically generate texts, posing new challenges for educational institutions. To enforce norms of academic integrity and ensure students' learning, learning analytics methods to automatically detect LLM-generated text appear increasingly appealing. This paper benchmarks the performance of different state-of-the-art detectors in educational contexts, introducing a novel dataset, called Generative Essay Detection in Education (GEDE), containing over 900 student-written essays and over 12,500 LLM-generated essays from various domains. To capture the diversity of LLM usage practices in generating text, we propose the concept of contribution levels, representing students' contribution to a given assignment. These levels range from purely human-written texts, to slightly LLM-improved versions, to fully LLM-generated texts, and finally to active attacks on the detector by "humanizing" generated texts. We show that most detectors struggle to accurately classify texts of intermediate student contribution levels, like LLM-improved human-written texts. Detectors are particularly likely to produce false positives, which is problematic in educational settings where false suspicions can severely impact students' lives. Our dataset, code, and additional supplementary materials are publicly available at https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.

