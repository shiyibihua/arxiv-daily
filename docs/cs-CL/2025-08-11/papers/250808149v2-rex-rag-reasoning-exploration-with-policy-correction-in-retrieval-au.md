---
layout: default
title: REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation
---

# REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08149" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.08149v2</a>
  <a href="https://arxiv.org/pdf/2508.08149.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08149v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08149v2', 'REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Wentao Jiang, Xiang Feng, Zengmao Wang, Yong Luo, Pingbo Xu, Zhe Chen, Bo Du, Jing Zhang

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-11 (Êõ¥Êñ∞: 2025-08-12)

**Â§áÊ≥®**: 17 pages, 4 figures; updated references

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/MiliLab/REX-RAG)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫REX-RAG‰ª•Ëß£ÂÜ≥Êé®ÁêÜË∑ØÂæÑÊó†ÊïàÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê` `Êé®ÁêÜË∑ØÂæÑ` `Á≠ñÁï•‰ºòÂåñ` `ÈóÆÁ≠îÁ≥ªÁªü` `ÈáçË¶ÅÊÄßÈááÊ†∑` `Ê®°ÂûãÊÄßËÉΩÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Á≠ñÁï•È©±Âä®ÁöÑËΩ®ËøπÈááÊ†∑‰∏≠ÔºåLLMsÂ∏∏Â∏∏Èô∑ÂÖ•Êó†ÊïàÊé®ÁêÜË∑ØÂæÑÔºåÂØºËá¥ÂÜ≥Á≠ñË¥®Èáè‰∏ãÈôç„ÄÇ
2. REX-RAGÈÄöËøáÊ∑∑ÂêàÈááÊ†∑Á≠ñÁï•ÂíåÁ≠ñÁï•‰øÆÊ≠£Êú∫Âà∂ÔºåÊé¢Á¥¢Êõø‰ª£Êé®ÁêÜË∑ØÂæÑÂπ∂‰ºòÂåñÁ≠ñÁï•Â≠¶‰π†„ÄÇ
3. Âú®‰∏É‰∏™ÈóÆÁ≠îÂü∫ÂáÜ‰∏äÔºåREX-RAGÂú®Qwen2.5-3BÂíåQwen2.5-7BÊ®°Âûã‰∏äÂàÜÂà´ÊèêÂçá‰∫Ü5.1%Âíå3.6%ÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰Ωú‰∏∫‰∏ÄÁßçÂº∫Â§ßÁöÑËåÉÂºèÔºåÊ≠£Âú®Êé®Âä®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊâßË°åÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°„ÄÇÂ∞ÜRL‰∏éÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁªìÂêàÔºåÂèØ‰ª•‰ΩøLLMsÂä®ÊÄÅÊï¥ÂêàÂ§ñÈÉ®Áü•ËØÜÔºå‰ªéËÄåÂÆûÁé∞Êõ¥‰∏∫ÊòéÊô∫ÂíåÁ®≥ÂÅ•ÁöÑÂÜ≥Á≠ñ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Á≠ñÁï•È©±Âä®ÁöÑËΩ®ËøπÈááÊ†∑‰∏≠Èù¢‰∏¥ÈáçÂ§ßÊåëÊàòÔºåLLMsÂ∏∏Â∏∏Èô∑ÂÖ•Êó†ÊïàÁöÑÊé®ÁêÜË∑ØÂæÑÔºåÂØºËá¥Ëøá‰∫éËá™‰ø°‰ΩÜÈîôËØØÁöÑÁªìËÆ∫„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜREX-RAGÔºàÊé®ÁêÜÊé¢Á¥¢‰∏éÁ≠ñÁï•‰øÆÊ≠£Ê°ÜÊû∂ÔºâÔºåËØ•Ê°ÜÊû∂Âú®‰øùÊåÅ‰∏•Ê†ºÁöÑÁ≠ñÁï•Â≠¶‰π†ÁöÑÂêåÊó∂ÔºåÊé¢Á¥¢Êõø‰ª£Êé®ÁêÜË∑ØÂæÑ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÊ∑∑ÂêàÈááÊ†∑Á≠ñÁï•ÂíåÁ≠ñÁï•‰øÆÊ≠£Êú∫Âà∂ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåREX-RAGÂú®Â§ö‰∏™ÈóÆÁ≠îÂü∫ÂáÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÂπ≥ÂùáÊèêÂçá5.1%Âíå3.6%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥LLMsÂú®Á≠ñÁï•È©±Âä®ËΩ®ËøπÈááÊ†∑‰∏≠Èô∑ÂÖ•Êó†ÊïàÊé®ÁêÜË∑ØÂæÑÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÂÜ≥Á≠ñË¥®Èáè‰∏ãÈôç„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Êé¢Á¥¢ËøáÁ®ã‰∏≠ÂÆπÊòì‰∫ßÁîüËøá‰∫éËá™‰ø°‰ΩÜÈîôËØØÁöÑÁªìËÆ∫ÔºåÂΩ±ÂìçÁ≠ñÁï•‰ºòÂåñÊïàÊûú„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöREX-RAGÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•Ê∑∑ÂêàÈááÊ†∑Á≠ñÁï•ÂíåÁ≠ñÁï•‰øÆÊ≠£Êú∫Âà∂ÔºåÊé¢Á¥¢Êõø‰ª£Êé®ÁêÜË∑ØÂæÑÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÂÜ≥Á≠ñË¥®Èáè„ÄÇÊ∑∑ÂêàÈááÊ†∑Á≠ñÁï•ÁªìÂêà‰∫ÜÊñ∞È¢ñÁöÑÊé¢ÊµãÈááÊ†∑ÊñπÊ≥ïÂíåÊé¢Á¥¢ÊÄßÊèêÁ§∫ÔºåÂ∏ÆÂä©Ê®°ÂûãÈÄÉÁ¶ªÊó†ÊïàË∑ØÂæÑ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöREX-RAGÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊ∑∑ÂêàÈááÊ†∑Ê®°ÂùóÂíåÁ≠ñÁï•‰øÆÊ≠£Ê®°Âùó„ÄÇÊ∑∑ÂêàÈááÊ†∑Ê®°ÂùóË¥üË¥£ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊé®ÁêÜË∑ØÂæÑÔºåËÄåÁ≠ñÁï•‰øÆÊ≠£Ê®°ÂùóÂàôÈÄöËøáÈáçË¶ÅÊÄßÈááÊ†∑Êù•Ê†°Ê≠£Áî±Ê∑∑ÂêàÈááÊ†∑ÂºïËµ∑ÁöÑÂàÜÂ∏ÉÂÅèÁßªÔºåÁ°Æ‰øùÁ≠ñÁï•Â≠¶‰π†ÁöÑÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöREX-RAGÁöÑ‰∏§Â§ßÂàõÊñ∞ÁÇπÂú®‰∫éÊ∑∑ÂêàÈááÊ†∑Á≠ñÁï•ÂíåÁ≠ñÁï•‰øÆÊ≠£Êú∫Âà∂„ÄÇÊ∑∑ÂêàÈááÊ†∑Á≠ñÁï•ÈÄöËøáÁªìÂêàÊé¢ÊµãÈááÊ†∑ÂíåÊé¢Á¥¢ÊÄßÊèêÁ§∫ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõÔºõËÄåÁ≠ñÁï•‰øÆÊ≠£Êú∫Âà∂ÂàôÊúâÊïàÂáèÂ∞è‰∫ÜÊ¢ØÂ∫¶‰º∞ËÆ°ÂÅèÂ∑ÆÔºåÁ°Æ‰øù‰∫ÜÁ≠ñÁï•Â≠¶‰π†ÁöÑÁ®≥ÂÆöÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÖ≥ÈîÆËÆæËÆ°‰∏äÔºåREX-RAGÈááÁî®‰∫ÜÈáçË¶ÅÊÄßÈááÊ†∑Êù•Ê†°Ê≠£ÂàÜÂ∏ÉÂÅèÁßªÔºåÂπ∂Âú®ÊçüÂ§±ÂáΩÊï∞‰∏≠ÂºïÂÖ•‰∫ÜÈ¢ùÂ§ñÁöÑÊ≠£ÂàôÂåñÈ°πÔºå‰ª•Âπ≥Ë°°Êé¢Á¥¢‰∏éÂà©Áî®‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÊ®°ÂûãÁªìÊûÑ‰∏äÔºåÈááÁî®‰∫ÜÂ§öÂ±ÇTransformerÊû∂ÊûÑÔºå‰ª•Â¢ûÂº∫Ê®°ÂûãÁöÑË°®ËææËÉΩÂäõÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®‰∏É‰∏™ÈóÆÁ≠îÂü∫ÂáÜ‰∏äÔºåREX-RAGÂú®Qwen2.5-3BÂíåQwen2.5-7BÊ®°Âûã‰∏äÂàÜÂà´ÂÆûÁé∞‰∫Ü5.1%Âíå3.6%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÊòæÁ§∫Âá∫Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÁ´û‰∫âÂäõÔºåË∂ÖË∂ä‰∫ÜÂ§ö‰∏™Âº∫Âü∫Á∫øÊ®°Âûã„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

REX-RAGÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÊΩúÂú®Â∫îÁî®‰ª∑ÂÄºÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÂ§çÊùÇÊé®ÁêÜÂíåÂÜ≥Á≠ñÊîØÊåÅÁöÑÂú∫ÊôØÔºåÂ¶ÇÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅÂØπËØùÁ≥ªÁªüÂíåÁü•ËØÜÂõæË∞±ÊûÑÂª∫Á≠â„ÄÇÈÄöËøáÊèêÈ´òÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåREX-RAGËÉΩÂ§ü‰∏∫Áî®Êà∑Êèê‰æõÊõ¥‰∏∫ÂáÜÁ°ÆÂíåÂèØÈù†ÁöÑ‰ø°ÊÅØÔºåÊé®Âä®‰∫∫Â∑•Êô∫ËÉΩÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) is emerging as a powerful paradigm for enabling large language models (LLMs) to perform complex reasoning tasks. Recent advances indicate that integrating RL with retrieval-augmented generation (RAG) allows LLMs to dynamically incorporate external knowledge, leading to more informed and robust decision making. However, we identify a critical challenge during policy-driven trajectory sampling: LLMs are frequently trapped in unproductive reasoning paths, which we refer to as "dead ends", committing to overconfident yet incorrect conclusions. This severely hampers exploration and undermines effective policy optimization. To address this challenge, we propose REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation), a novel framework that explores alternative reasoning paths while maintaining rigorous policy learning through principled distributional corrections. Our approach introduces two key innovations: (1) Mixed Sampling Strategy, which combines a novel probe sampling method with exploratory prompts to escape dead ends; and (2) Policy Correction Mechanism, which employs importance sampling to correct distribution shifts induced by mixed sampling, thereby mitigating gradient estimation bias. We evaluate it on seven question-answering benchmarks, and the experimental results show that REX-RAG achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.

