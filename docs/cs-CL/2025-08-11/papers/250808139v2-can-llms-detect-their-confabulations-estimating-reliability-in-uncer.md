---
layout: default
title: Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models
---

# Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08139" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08139v2</a>
  <a href="https://arxiv.org/pdf/2508.08139.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08139v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08139v2', 'Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyi Zhou, Johanne Medina, Sanjay Chawla

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-12-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¸ç¡®å®šæ€§å¼•å¯¼çš„æ¢æµ‹æ–¹æ³•ä»¥æé«˜LLMçš„å¯é æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸ç¡®å®šæ€§ä¼°è®¡` `è™šæ„æ£€æµ‹` `å¤šè½®å¯¹è¯` `å¯é æ€§é¢„æµ‹` `å¼€æ”¾é—®ç­”` `æ¨¡å‹è¡Œä¸ºåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„LLMsåœ¨å¤šè½®å¯¹è¯ä¸­å®¹æ˜“ç”Ÿæˆè™šæ„å†…å®¹ï¼Œå¯¼è‡´è¾“å‡ºçš„å¯é æ€§ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ ‡è®°çº§ä¸ç¡®å®šæ€§çš„å¯é æ€§ä¼°è®¡æ–¹æ³•ï¼Œä»¥æ”¹å–„æ¨¡å‹å¯¹ä¸å¯é å“åº”çš„è¯†åˆ«èƒ½åŠ›ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šé€šè¿‡å®éªŒéªŒè¯ï¼Œæ­£ç¡®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ˜¾è‘—æé«˜äº†ç­”æ¡ˆå‡†ç¡®æ€§å’Œæ¨¡å‹ä¿¡å¿ƒï¼Œä¸”æ¢æµ‹æ–¹æ³•æœ‰æ•ˆæ•æ‰äº†æ¨¡å‹è¡Œä¸ºçš„å˜åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®¹æ˜“ç”Ÿæˆæµç•…ä½†ä¸æ­£ç¡®çš„å†…å®¹ï¼Œç§°ä¸ºè™šæ„ï¼Œè¿™åœ¨å¤šè½®å¯¹è¯æˆ–ä»£ç†åº”ç”¨ä¸­å¸¦æ¥äº†é£é™©ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä½•å½±å“æ¨¡å‹è¡Œä¸ºï¼Œä»¥åŠLLMsæ˜¯å¦èƒ½å¤Ÿè¯†åˆ«å…¶ä¸å¯é çš„å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯é æ€§ä¼°è®¡æ–¹æ³•ï¼Œåˆ©ç”¨æ ‡è®°çº§ä¸ç¡®å®šæ€§æ¥æŒ‡å¯¼å†…éƒ¨æ¨¡å‹è¡¨ç¤ºçš„èšåˆã€‚é€šè¿‡åœ¨å¼€æ”¾é—®ç­”åŸºå‡†ä¸Šçš„æ§åˆ¶å®éªŒï¼Œæˆ‘ä»¬å‘ç°æ­£ç¡®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æé«˜äº†ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œæ¨¡å‹çš„ä¿¡å¿ƒï¼Œè€Œè¯¯å¯¼æ€§ä¸Šä¸‹æ–‡åˆ™å¸¸å¸¸å¯¼è‡´è‡ªä¿¡ä½†é”™è¯¯çš„å“åº”ï¼Œæ­ç¤ºäº†ä¸ç¡®å®šæ€§ä¸æ­£ç¡®æ€§ä¹‹é—´çš„é”™ä½ã€‚æˆ‘ä»¬çš„æ¢æµ‹æ–¹æ³•æ•æ‰äº†æ¨¡å‹è¡Œä¸ºçš„è¿™äº›å˜åŒ–ï¼Œå¹¶æé«˜äº†å¤šä¸ªå¼€æºLLMsä¸­ä¸å¯é è¾“å‡ºçš„æ£€æµ‹èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶çš„è™šæ„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œå¤„ç†ä¸å¯é è¾“å‡ºï¼Œå¯¼è‡´ç”¨æˆ·ä¿¡ä»»åº¦ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ ‡è®°çº§ä¸ç¡®å®šæ€§æ¥å¼•å¯¼å†…éƒ¨è¡¨ç¤ºèšåˆçš„å¯é æ€§ä¼°è®¡æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«æ˜¾è‘—æ ‡è®°æ¥æé«˜æ¨¡å‹å¯¹ä¸å¯é å“åº”çš„æ£€æµ‹èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè®¡ç®—è¾“å‡ºlogitsçš„éšæœºæ€§å’ŒçŸ¥è¯†æ€§ä¸ç¡®å®šæ€§ï¼Œç„¶åèšåˆæ˜¾è‘—æ ‡è®°çš„éšè—çŠ¶æ€ä»¥è¿›è¡Œå“åº”çº§çš„å¯é æ€§é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†éšæœºæ€§å’ŒçŸ¥è¯†æ€§ä¸ç¡®å®šæ€§æ¥è¯†åˆ«å’Œèšåˆæ¨¡å‹å†…éƒ¨è¡¨ç¤ºï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ç›´æ¥ä¸ç¡®å®šæ€§ä¿¡å·ä½¿ç”¨å½¢æˆäº†æ˜æ˜¾å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¯é æ€§é¢„æµ‹ï¼Œå¹¶é€šè¿‡æ§åˆ¶å®éªŒéªŒè¯äº†ä¸åŒä¸Šä¸‹æ–‡å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ã€‚å®éªŒä¸­ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ç¡®ä¿äº†ç»“æœçš„å¯é æ€§å’Œå¯é‡å¤æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ­£ç¡®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä½¿å¾—ç­”æ¡ˆå‡†ç¡®ç‡æé«˜äº†çº¦15%ï¼Œè€Œè¯¯å¯¼æ€§ä¸Šä¸‹æ–‡åˆ™å¯¼è‡´äº†30%çš„é”™è¯¯å“åº”ç‡ã€‚æˆ‘ä»¬çš„æ¢æµ‹æ–¹æ³•åœ¨å¤šä¸ªå¼€æºLLMsä¸Šå‡æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†ä¸ç¡®å®šæ€§å¼•å¯¼æ¢æµ‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€è‡ªåŠ¨é—®ç­”ç³»ç»Ÿå’Œä»»ä½•éœ€è¦é«˜å¯é æ€§è¾“å‡ºçš„å¯¹è¯ç³»ç»Ÿã€‚é€šè¿‡æé«˜LLMså¯¹ä¸å¯é å†…å®¹çš„è¯†åˆ«èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·ä¿¡ä»»ï¼Œé™ä½é”™è¯¯ä¿¡æ¯çš„ä¼ æ’­é£é™©ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.

