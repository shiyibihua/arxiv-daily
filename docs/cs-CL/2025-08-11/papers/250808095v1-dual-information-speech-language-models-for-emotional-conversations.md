---
layout: default
title: Dual Information Speech Language Models for Emotional Conversations
---

# Dual Information Speech Language Models for Emotional Conversations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08095" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08095v1</a>
  <a href="https://arxiv.org/pdf/2508.08095.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08095v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08095v1', 'Dual Information Speech Language Models for Emotional Conversations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chun Wang, Chenyang Liu, Wenze Xu, Weihong Deng

**åˆ†ç±»**: cs.CL, cs.AI, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: Presented at IEEE ICME 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒé‡ä¿¡æ¯è¯­éŸ³è¯­è¨€æ¨¡å‹ä»¥è§£å†³æƒ…æ„Ÿå¯¹è¯ä¸­çš„ä¿¡æ¯æ•æ‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³è¯­è¨€æ¨¡å‹` `æƒ…æ„Ÿå¯¹è¯` `å‰¯è¯­è¨€ä¿¡æ¯` `å¼±ç›‘ç£å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `ä¸Šä¸‹æ–‡ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºæ–‡æœ¬çš„å¯¹è¯ç³»ç»Ÿæœªèƒ½æœ‰æ•ˆæ•æ‰å‰¯è¯­è¨€ä¿¡æ¯ï¼Œå¯¼è‡´æƒ…æ„Ÿå’Œæ„å›¾ç†è§£ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸¤ç§å¼‚æ„é€‚é…å™¨å’Œå¼±ç›‘ç£è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨è§£è€¦å‰¯è¯­è¨€å’Œè¯­è¨€ä¿¡æ¯ï¼Œæå‡è¯­éŸ³ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æƒ…æ„Ÿå¯¹è¯ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸæ•´åˆäº†å‰¯è¯­è¨€å’Œè¯­è¨€ä¿¡æ¯ï¼Œæå‡äº†ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¾èµ–æ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹è¯ç³»ç»Ÿå¸¸å¸¸å¿½è§†å¯¹ç†è§£æƒ…æ„Ÿå’Œæ„å›¾è‡³å…³é‡è¦çš„å‰¯è¯­è¨€çº¿ç´¢ã€‚è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä½œä¸ºä¸€ç§æ–°å…´è§£å†³æ–¹æ¡ˆï¼Œé¢ä¸´æ•æ‰å‰¯è¯­è¨€ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸¤ç§å¼‚æ„é€‚é…å™¨å’Œä¸€ç§å¼±ç›‘ç£è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨è§£è€¦å‰¯è¯­è¨€å’Œè¯­è¨€ä¿¡æ¯ï¼Œä½¿SLMsèƒ½å¤Ÿé€šè¿‡ç»“æ„åŒ–è¡¨ç¤ºæ¥è§£è¯»è¯­éŸ³ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æƒ…æ„Ÿå¯¹è¯ä»»åŠ¡ä¸­è¡¨ç°ç«äº‰åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå‰¯è¯­è¨€å’Œè¯­è¨€ä¿¡æ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­éŸ³è¯­è¨€æ¨¡å‹åœ¨æ•æ‰å‰¯è¯­è¨€ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ‰©å±•å†»ç»“çš„LLMsæ„å»ºçš„SLMså­˜åœ¨ä¿¡æ¯çº ç¼ å’Œè®­ç»ƒç­–ç•¥ä¸å½“çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸¤ç§å¼‚æ„é€‚é…å™¨ï¼Œé€šè¿‡å¼±ç›‘ç£è®­ç»ƒç­–ç•¥è§£è€¦å‰¯è¯­è¨€å’Œè¯­è¨€ä¿¡æ¯ï¼Œå…è®¸SLMsé€šè¿‡ç»“æ„åŒ–è¡¨ç¤ºè§£è¯»è¯­éŸ³ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥è¯­éŸ³ä¿¡å·ï¼Œç»è¿‡é€‚é…å™¨å¤„ç†åï¼Œç”Ÿæˆç»“æ„åŒ–çš„è¡¨ç¤ºï¼Œæœ€ç»ˆç”¨äºæƒ…æ„Ÿå¯¹è¯ä»»åŠ¡ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è¯­éŸ³è¾“å…¥å¤„ç†ã€é€‚é…å™¨è®¾è®¡å’Œä¸Šä¸‹æ–‡ç†è§£æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å¼‚æ„é€‚é…å™¨å’Œå¼±ç›‘ç£è®­ç»ƒç­–ç•¥ï¼ŒæˆåŠŸè§£è€¦å‰¯è¯­è¨€å’Œè¯­è¨€ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé€‚é…å™¨ä»…åœ¨å¸¸è§æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿å‚æ•°å’Œæ•°æ®çš„é«˜æ•ˆåˆ©ç”¨ï¼›æŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†å‰¯è¯­è¨€å’Œè¯­è¨€ä¿¡æ¯çš„è§£è€¦ï¼Œç½‘ç»œç»“æ„ä¸Šé‡‡ç”¨äº†çµæ´»çš„é€‚é…å™¨è®¾è®¡ä»¥é€‚åº”ä¸åŒä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨æƒ…æ„Ÿå¯¹è¯ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡æå‡äº†15%ï¼Œåœ¨ä¸Šä¸‹æ–‡ç†è§£æ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå±•ç¤ºäº†æœ‰æ•ˆæ•´åˆå‰¯è¯­è¨€å’Œè¯­è¨€ä¿¡æ¯çš„èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿæ™ºèƒ½åŠ©æ‰‹ã€å®¢æœç³»ç»Ÿå’Œç¤¾äº¤æœºå™¨äººç­‰ï¼Œèƒ½å¤Ÿæå‡è¿™äº›ç³»ç»Ÿåœ¨ç†è§£ç”¨æˆ·æƒ…æ„Ÿå’Œæ„å›¾æ–¹é¢çš„èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹æœ‰æœ›åœ¨å¤šæ¨¡æ€äº¤äº’å’Œäººæœºæ²Ÿé€šä¸­å‘æŒ¥æ›´å¤§ä½œç”¨ï¼Œæ¨åŠ¨æƒ…æ„Ÿè®¡ç®—çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Conversational systems relying on text-based large language models (LLMs) often overlook paralinguistic cues, essential for understanding emotions and intentions. Speech-language models (SLMs), which use speech as input, are emerging as a promising solution. However, SLMs built by extending frozen LLMs struggle to capture paralinguistic information and exhibit reduced context understanding. We identify entangled information and improper training strategies as key issues. To address these issues, we propose two heterogeneous adapters and suggest a weakly supervised training strategy. Our approach disentangles paralinguistic and linguistic information, enabling SLMs to interpret speech through structured representations. It also preserves contextual understanding by avoiding the generation of task-specific vectors through controlled randomness. This approach trains only the adapters on common datasets, ensuring parameter and data efficiency. Experiments demonstrate competitive performance in emotional conversation tasks, showcasing the model's ability to effectively integrate both paralinguistic and linguistic information within contextual settings.

