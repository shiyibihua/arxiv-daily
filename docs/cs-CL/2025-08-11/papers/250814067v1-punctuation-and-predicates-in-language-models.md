---
layout: default
title: Punctuation and Predicates in Language Models
---

# Punctuation and Predicates in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14067" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14067v1</a>
  <a href="https://arxiv.org/pdf/2508.14067.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14067v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14067v1', 'Punctuation and Predicates in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sonakshi Chauhan, Maheep Chaudhary, Koby Choy, Samuel Nellessen, Nandi Schoots

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨æ ‡ç‚¹ç¬¦å·åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦æ€§åŠæ¨ç†æœºåˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `æ ‡ç‚¹ç¬¦å·` `æ¨ç†æœºåˆ¶` `æ¨¡å‹å¯è§£é‡Šæ€§` `å¹²é¢„æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶æœªå……åˆ†æ­ç¤ºæ ‡ç‚¹ç¬¦å·åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è®¡ç®—é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯å…¶åœ¨ä¸åŒæ¨¡å‹ä¸­çš„ä½œç”¨å·®å¼‚ã€‚
2. æœ¬æ–‡é€šè¿‡å¹²é¢„æŠ€æœ¯è¯„ä¼°æ ‡ç‚¹ç¬¦å·åœ¨GPT-2ã€DeepSeekå’ŒGemmaä¸­çš„å¿…è¦æ€§å’Œå……åˆ†æ€§ï¼Œæ¢ç´¢æ¨¡å‹å¯¹è¾“å…¥æˆåˆ†çš„å¤„ç†æ–¹å¼ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-2å¯¹æ ‡ç‚¹ç¬¦å·çš„ä¾èµ–æ€§æ˜¾è‘—ï¼Œè€ŒDeepSeekå’ŒGemmaåˆ™è¡¨ç°å‡ºè¾ƒä½çš„ä¾èµ–æ€§ï¼Œæ­ç¤ºäº†æ¨¡å‹é—´çš„å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ä¿¡æ¯çš„æ”¶é›†å’Œä¼ æ’­æ–¹å¼ã€‚ç ”ç©¶å‘ç°æ ‡ç‚¹ç¬¦å·åœ¨æ¨¡å‹ä¸­çš„è®¡ç®—é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨GPT-2ä¸­ï¼Œæ ‡ç‚¹ç¬¦å·åœ¨å¤šä¸ªå±‚æ¬¡ä¸Šæ—¢æ˜¯å¿…è¦çš„ä¹Ÿæ˜¯å……åˆ†çš„ã€‚é€šè¿‡å¹²é¢„æŠ€æœ¯ï¼Œè¯„ä¼°äº†æ ‡ç‚¹ç¬¦å·åœ¨ä¸åŒæ¨¡å‹ä¸­çš„ä½œç”¨ï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶äº†æ¨¡å‹å¯¹è¾“å…¥ä¸åŒæˆåˆ†çš„å¤„ç†æ–¹å¼ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†æ¡ä»¶è¯­å¥å’Œå…¨ç§°é‡åŒ–æ—¶çš„æ˜¾è‘—å·®å¼‚ï¼Œä¸ºç†è§£LLMsçš„å†…éƒ¨æœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œå¹¶å¯¹å¯è§£é‡Šæ€§æœ‰é‡è¦å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ ‡ç‚¹ç¬¦å·åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä½œç”¨åŠå…¶å¯¹æ¨¡å‹æ¨ç†çš„å½±å“ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ¢è®¨ä¸åŒæ¨¡å‹å¯¹æ ‡ç‚¹ç¬¦å·çš„ä¾èµ–æ€§å’Œå¤„ç†æ–¹å¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¹²é¢„æŠ€æœ¯è¯„ä¼°æ ‡ç‚¹ç¬¦å·åœ¨ä¸åŒå±‚æ¬¡ä¸Šçš„å¿…è¦æ€§å’Œå……åˆ†æ€§ï¼Œæ¢ç´¢æ¨¡å‹å¯¹è¾“å…¥æˆåˆ†çš„å¤„ç†æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯æ¡ä»¶è¯­å¥å’Œå…¨ç§°é‡åŒ–çš„æ¨ç†å·®å¼‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å¹²é¢„å®éªŒå’Œå±‚äº¤æ¢å®éªŒï¼Œåˆ†æä¸åŒæ¨¡å‹ï¼ˆGPT-2ã€DeepSeekã€Gemmaï¼‰å¯¹æ ‡ç‚¹ç¬¦å·å’Œæ¨ç†è§„åˆ™çš„å¤„ç†ï¼Œæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€å¹²é¢„å®æ–½åŠç»“æœåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ­ç¤ºäº†æ ‡ç‚¹ç¬¦å·åœ¨ä¸åŒæ¨¡å‹ä¸­çš„è®¡ç®—é‡è¦æ€§å·®å¼‚ï¼Œå°¤å…¶æ˜¯GPT-2å¯¹æ ‡ç‚¹ç¬¦å·çš„å¼ºä¾èµ–æ€§ï¼Œè¿™ä¸ºç†è§£æ¨¡å‹å†…éƒ¨æœºåˆ¶æä¾›äº†æ–°è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­é‡‡ç”¨äº†å¹²é¢„æŠ€æœ¯ï¼Œè¯„ä¼°æ ‡ç‚¹ç¬¦å·çš„å½±å“ï¼Œè®¾ç½®äº†ä¸åŒçš„å¹²é¢„æ¡ä»¶ï¼Œå¹¶å¯¹æ¨¡å‹çš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§å’Œå¯é‡å¤æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-2åœ¨å¤šä¸ªå±‚æ¬¡ä¸Šå¯¹æ ‡ç‚¹ç¬¦å·çš„ä¾èµ–æ€§å¼ºï¼Œè€ŒDeepSeekå’ŒGemmaåˆ™è¡¨ç°å‡ºè¾ƒä½çš„ä¾èµ–æ€§ã€‚é€šè¿‡å¹²é¢„å®éªŒï¼Œæ¡ä»¶è¯­å¥å’Œå…¨ç§°é‡åŒ–çš„å¤„ç†å·®å¼‚æ˜¾è‘—ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹é—´çš„æ¨ç†æœºåˆ¶å·®å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæä¾›äº†æ–°çš„ç†è§£æ¡†æ¶ï¼Œå°¤å…¶åœ¨æ¨¡å‹å¯è§£é‡Šæ€§å’Œä¼˜åŒ–æ–¹é¢å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚æœªæ¥å¯åœ¨æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨å¯¹æ ‡ç‚¹ç¬¦å·å’Œæ¨ç†æœºåˆ¶çš„æ·±å…¥ç†è§£ï¼Œæå‡æ¨¡å‹æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper we explore where information is collected and how it is propagated throughout layers in large language models (LLMs). We begin by examining the surprising computational importance of punctuation tokens which previous work has identified as attention sinks and memory aids. Using intervention-based techniques, we evaluate the necessity and sufficiency (for preserving model performance) of punctuation tokens across layers in GPT-2, DeepSeek, and Gemma. Our results show stark model-specific differences: for GPT-2, punctuation is both necessary and sufficient in multiple layers, while this holds far less in DeepSeek and not at all in Gemma. Extending beyond punctuation, we ask whether LLMs process different components of input (e.g., subjects, adjectives, punctuation, full sentences) by forming early static summaries reused across the network, or if the model remains sensitive to changes in these components across layers. Extending beyond punctuation, we investigate whether different reasoning rules are processed differently by LLMs. In particular, through interchange intervention and layer-swapping experiments, we find that conditional statements (if, then), and universal quantification (for all) are processed very differently. Our findings offer new insight into the internal mechanisms of punctuation usage and reasoning in LLMs and have implications for interpretability.

