---
layout: default
title: CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data Augmentation
---

# CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data Augmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08386" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08386v1</a>
  <a href="https://arxiv.org/pdf/2508.08386.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08386v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08386v1', 'CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuzhou Yuan, William LaCroix, Hardik Ghoshal, Ercong Nie, Michael FÃ¤rber

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoDAEæ¡†æ¶ä»¥è§£å†³æ•™è‚²åœºæ™¯ä¸­LLMé€‚åº”æ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•™è‚²æŠ€æœ¯` `é“¾å¼æ€ç»´` `æ•°æ®å¢å¼º` `ä¸ªæ€§åŒ–å­¦ä¹ ` `æ™ºèƒ½è¾…å¯¼`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMsåœ¨æ•™è‚²ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œå­˜åœ¨è¿‡åº¦é¡ºä»ã€å“åº”é€‚åº”æ€§ä½å’Œå¯¹æƒ…æ„Ÿæ“æ§æ€§æç¤ºè„†å¼±ç­‰é—®é¢˜ã€‚
2. æå‡ºCoDAEæ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´æ•°æ®å¢å¼ºï¼Œæ”¶é›†çœŸå®å¯¹è¯å¹¶è¿›è¡Œä¸°å¯Œï¼Œä»¥ä¿ƒè¿›é€æ­¥æ¨ç†å’Œæ•™è‚²æŒ‡å¯¼ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨æ•™è‚²åœºæ™¯ä¸­æä¾›äº†æ›´åˆé€‚çš„æŒ‡å¯¼ï¼Œæ”¯æŒæ¨ç†è¿‡ç¨‹ï¼Œå¹¶æœ‰æ•ˆæŠµå¾¡è¿‡æ—©æ­ç¤ºç­”æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶å¯æ‰©å±•æ€§å’Œä¸ªæ€§åŒ–æ•™å­¦æ½œåŠ›è€Œè¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œAIè¾…å¯¼å‘˜ã€‚ç„¶è€Œï¼Œç°æˆçš„LLMsåœ¨æ•™è‚²ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œå¸¸å¸¸è¿‡äºè½»æ˜“åœ°æ­ç¤ºç­”æ¡ˆï¼Œæ— æ³•æ ¹æ®å­¦ç”Ÿçš„ä¸ç¡®å®šæ€§è°ƒæ•´å“åº”ï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°æƒ…æ„Ÿæ“æ§æ€§æç¤ºçš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CoDAEæ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®å¢å¼ºæ¥é€‚åº”LLMsäºæ•™è‚²ç”¨é€”ã€‚æˆ‘ä»¬æ”¶é›†äº†å­¦ç”Ÿä¸åŸºäºChatGPTçš„è¾…å¯¼å‘˜ä¹‹é—´çš„çœŸå®å¯¹è¯ï¼Œå¹¶åˆ©ç”¨CoTæç¤ºè¿›è¡Œä¸°å¯Œï¼Œä»¥ä¿ƒè¿›é€æ­¥æ¨ç†å’Œç¬¦åˆæ•™è‚²ç›®æ ‡çš„æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†é’ˆå¯¹æ€§çš„å¯¹è¯æ¡ˆä¾‹ï¼Œä»¥æ˜ç¡®å‡è½»è¿‡åº¦é¡ºä»ã€ä½å“åº”é€‚åº”æ€§å’Œå¨èƒè„†å¼±æ€§ç­‰ä¸‰å¤§å…³é”®é™åˆ¶ã€‚é€šè¿‡åœ¨ä¸åŒå˜ä½“çš„å¢å¼ºæ•°æ®é›†ä¸Šå¾®è°ƒå››ä¸ªå¼€æºLLMsï¼Œå¹¶åœ¨æ¨¡æ‹Ÿæ•™è‚²åœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œä½¿ç”¨CoDAEå¾®è°ƒçš„æ¨¡å‹æä¾›äº†æ›´ç¬¦åˆæ•™è‚²è¦æ±‚çš„æŒ‡å¯¼ï¼Œæ›´å¥½åœ°æ”¯æŒæ¨ç†è¿‡ç¨‹ï¼Œå¹¶æœ‰æ•ˆæŠµå¾¡è¿‡æ—©æ­ç¤ºç­”æ¡ˆçš„é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMsåœ¨æ•™è‚²åœºæ™¯ä¸­çš„é€‚åº”æ€§ä¸è¶³ï¼Œå…·ä½“è¡¨ç°ä¸ºè¿‡åº¦é¡ºä»ã€ä½å“åº”é€‚åº”æ€§å’Œå¯¹æƒ…æ„Ÿæ“æ§æ€§æç¤ºçš„è„†å¼±æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®å¢å¼ºï¼Œä¸°å¯Œå­¦ç”Ÿä¸AIè¾…å¯¼å‘˜ä¹‹é—´çš„å¯¹è¯ï¼Œä¿ƒè¿›é€æ­¥æ¨ç†å’Œç¬¦åˆæ•™è‚²ç›®æ ‡çš„æŒ‡å¯¼ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹åœ¨æ•™è‚²ç¯å¢ƒä¸­çš„è¡¨ç°å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoDAEæ¡†æ¶åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å¯¹è¯ä¸°å¯Œã€æ¨¡å‹å¾®è°ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†çœŸå®å¯¹è¯ï¼Œç„¶åé€šè¿‡CoTæç¤ºå¢å¼ºæ•°æ®ï¼Œæ¥ç€å¯¹å››ä¸ªå¼€æºLLMsè¿›è¡Œå¾®è°ƒï¼Œæœ€ååœ¨æ¨¡æ‹Ÿæ•™è‚²åœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡é“¾å¼æ€ç»´æ•°æ®å¢å¼ºæ¥æå‡LLMsçš„æ•™è‚²é€‚åº”æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç›´æ¥ä½¿ç”¨LLMsçš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…æœªèƒ½æœ‰æ•ˆè§£å†³æ•™è‚²åœºæ™¯ä¸­çš„ç‰¹å®šéœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå“åº”æ•™è‚²åœºæ™¯ä¸­çš„å¤æ‚å¯¹è¯éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨CoDAEå¾®è°ƒçš„æ¨¡å‹åœ¨æ•™è‚²åœºæ™¯ä¸­æä¾›äº†æ›´ç¬¦åˆæ•™è‚²è¦æ±‚çš„æŒ‡å¯¼ï¼Œæ”¯æŒæ¨ç†è¿‡ç¨‹çš„èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œä¸”æœ‰æ•ˆæŠµå¾¡äº†è¿‡æ—©æ­ç¤ºç­”æ¡ˆçš„é—®é¢˜ã€‚å…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€ä¸ªæ€§åŒ–å­¦ä¹ å¹³å°å’Œæ™ºèƒ½è¾…å¯¼ç³»ç»Ÿã€‚é€šè¿‡æå‡LLMsåœ¨æ•™è‚²åœºæ™¯ä¸­çš„é€‚åº”æ€§ï¼ŒCoDAEæ¡†æ¶èƒ½å¤Ÿä¸ºå­¦ç”Ÿæä¾›æ›´æœ‰æ•ˆçš„å­¦ä¹ æ”¯æŒï¼Œä¿ƒè¿›æ›´æ·±å±‚æ¬¡çš„ç†è§£ä¸æ€è€ƒï¼Œæœªæ¥å¯èƒ½å¯¹æ•™è‚²è¡Œä¸šäº§ç”Ÿæ·±è¿œçš„å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly employed as AI tutors due to their scalability and potential for personalized instruction. However, off-the-shelf LLMs often underperform in educational settings: they frequently reveal answers too readily, fail to adapt their responses to student uncertainty, and remain vulnerable to emotionally manipulative prompts. To address these challenges, we introduce CoDAE, a framework that adapts LLMs for educational use through Chain-of-Thought (CoT) data augmentation. We collect real-world dialogues between students and a ChatGPT-based tutor and enrich them using CoT prompting to promote step-by-step reasoning and pedagogically aligned guidance. Furthermore, we design targeted dialogue cases to explicitly mitigate three key limitations: over-compliance, low response adaptivity, and threat vulnerability. We fine-tune four open-source LLMs on different variants of the augmented datasets and evaluate them in simulated educational scenarios using both automatic metrics and LLM-as-a-judge assessments. Our results show that models fine-tuned with CoDAE deliver more pedagogically appropriate guidance, better support reasoning processes, and effectively resist premature answer disclosure.

