---
layout: default
title: What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction
---

# What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07702" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07702v1</a>
  <a href="https://arxiv.org/pdf/2508.07702.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07702v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07702v1', 'What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Charlie Wyatt, Aditya Joshi, Flora Salim

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: Under Review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æ©ç å¥å­é¢„æµ‹ä¸­çš„è¡¨ç°ä»¥è§£å†³é•¿ç¨‹ä¸€è‡´æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ©ç å¥å­é¢„æµ‹` `é•¿æ–‡æœ¬å¤„ç†` `å…¨å±€è¿è´¯æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸‹ä¸€æ ‡è®°é¢„æµ‹æ–¹æ³•åœ¨é•¿ç¨‹ä¸€è‡´æ€§å’Œå…¨å±€è¿è´¯æ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„è¡¨ç°ã€‚
2. è®ºæ–‡é€šè¿‡æ©ç å¥å­é¢„æµ‹ä»»åŠ¡è¯„ä¼°å•†ä¸šLLMsåœ¨ä¸åŒæ–‡æœ¬é¢†åŸŸçš„è¡¨ç°ï¼Œæ—¨åœ¨æ­ç¤ºå…¶åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å•†ä¸šLLMsåœ¨å…¶ä»–ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä½ç»“æ„åŸŸçš„æ©ç å¥å­é¢„æµ‹ä¸­æ•ˆæœä¸ä½³ï¼Œå­˜åœ¨èƒ½åŠ›ç¼ºå£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ©ç å¥å­é¢„æµ‹ï¼ˆMSPï¼‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒæŒ‡å‡ºç°æœ‰çš„ä¸‹ä¸€æ ‡è®°é¢„æµ‹ï¼ˆNTPï¼‰æ–¹æ³•åœ¨é•¿ç¨‹ä¸€è‡´æ€§å’Œå…¨å±€è¿è´¯æ€§æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡å¯¹ä¸‰ç§å•†ä¸šLLMsï¼ˆGPT-4oã€Claude 3.5 Sonnetå’ŒGemini 2.0 Flashï¼‰åœ¨ä¸åŒé¢†åŸŸï¼ˆå™äº‹ã€ç¨‹åºæ€§å’Œè¯´æ˜æ€§æ–‡æœ¬ï¼‰ä¸Šçš„è¯„ä¼°ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨ä½ç»“æ„åŸŸçš„æ©ç å¥å­é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹èƒ½åŠ›çš„ç¼ºå£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ©ç å¥å­é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä½ç»“æ„åŸŸæ–‡æœ¬ä¸­ï¼Œç°æœ‰çš„NTPæ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†é•¿ç¨‹ä¸€è‡´æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è¯„ä¼°LLMsåœ¨æ©ç å¥å­é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¥æ¢è®¨å…¶åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­çš„èƒ½åŠ›ï¼Œå¼ºè°ƒå…¨å±€è¿è´¯æ€§çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶è¯„ä¼°äº†ä¸‰ç§å•†ä¸šLLMsåœ¨ä¸‰ä¸ªä¸åŒé¢†åŸŸçš„æ©ç å¥å­é¢„æµ‹ä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯å™äº‹ï¼ˆROCStoriesï¼‰ã€ç¨‹åºæ€§ï¼ˆRecipe1Mï¼‰å’Œè¯´æ˜æ€§ï¼ˆWikipediaï¼‰æ–‡æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºå°†æ©ç å¥å­é¢„æµ‹ä½œä¸ºè¯„ä¼°LLMså…¨å±€è¿è´¯æ€§èƒ½åŠ›çš„æ ‡å‡†ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ï¼Œå¼ºè°ƒäº†æ¨¡å‹åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨äº†ç›¸ä¼¼æ€§å’Œè¿è´¯æ€§ä¸¤ä¸ªæŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ï¼Œå…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒå®Œæ•´è®ºæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å•†ä¸šLLMsåœ¨å…¶ä»–ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ©ç å¥å­é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨ä½ç»“æ„åŸŸæ–‡æœ¬ä¸­ï¼Œå…¶è¡¨ç°æ˜¾è‘—ä½äºé¢„æœŸï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„èƒ½åŠ›ç¼ºå£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡LLMsåœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­çš„èƒ½åŠ›ï¼Œå¯ä»¥æ”¹å–„è‡ªåŠ¨å†™ä½œã€å¯¹è¯ç³»ç»Ÿå’Œå†…å®¹æ¨èç­‰å®é™…åº”ç”¨çš„æ•ˆæœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformer-based models primarily rely on Next Token Prediction (NTP), which predicts the next token in a sequence based on the preceding context. However, NTP's focus on single-token prediction often limits a model's ability to plan ahead or maintain long-range coherence, raising questions about how well LLMs can predict longer contexts, such as full sentences within structured documents. While NTP encourages local fluency, it provides no explicit incentive to ensure global coherence across sentence boundaries-an essential skill for reconstructive or discursive tasks. To investigate this, we evaluate three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on Masked Sentence Prediction (MSP) - the task of infilling a randomly removed sentence - from three domains: ROCStories (narrative), Recipe1M (procedural), and Wikipedia (expository). We assess both fidelity (similarity to the original sentence) and cohesiveness (fit within the surrounding context). Our key finding reveals that commercial LLMs, despite their superlative performance in other tasks, are poor at predicting masked sentences in low-structured domains, highlighting a gap in current model capabilities.

