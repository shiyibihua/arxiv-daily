---
layout: default
title: VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models
---

# VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17759" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17759v1</a>
  <a href="https://arxiv.org/pdf/2510.17759.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17759v1" onclick="toggleFavorite(this, '2510.17759v1', 'VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qilin Liao, Anamika Lochab, Ruqi Zhang

**åˆ†ç±»**: cs.CR, cs.CL, cs.CV, cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

**å¤‡æ³¨**: 18 pages, 7 Figures,

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVERA-Væ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹çš„æ¼æ´å‘ç°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `å¯¹æŠ—æ”»å‡»` `å˜åˆ†æ¨æ–­` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å®‰å…¨æ€§æµ‹è¯•` `æ¨¡å‹è„†å¼±æ€§` `éšè”½æ”»å‡»` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ”»å‡»æ–¹æ³•ä¾èµ–è„†å¼±æ¨¡æ¿ï¼Œä¸”é›†ä¸­äºå•ä¸€æ”»å‡»åœºæ™¯ï¼Œå¯¼è‡´æ¼æ´å‘ç°çš„å±€é™æ€§ã€‚
2. VERA-Væ¡†æ¶é€šè¿‡å­¦ä¹ æ–‡æœ¬-å›¾åƒæç¤ºçš„è”åˆåéªŒåˆ†å¸ƒï¼Œç”Ÿæˆéšè”½çš„å¯¹æŠ—è¾“å…¥ï¼Œä»è€Œæœ‰æ•ˆç»•è¿‡æ¨¡å‹çš„ä¿æŠ¤æœºåˆ¶ã€‚
3. åœ¨HarmBenchå’ŒHADESåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVERA-Våœ¨æ”»å‡»æˆåŠŸç‡ä¸Šæ˜¾è‘—æå‡ï¼Œæœ€é«˜å¯è¾¾53.75%çš„å¢å¹…ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶å¤šæ¨¡æ€è®¾è®¡ä¹Ÿå¼•å…¥äº†æ–°çš„ã€å°šæœªå……åˆ†æ¢ç´¢çš„è„†å¼±æ€§ã€‚ç°æœ‰çš„å¤šæ¨¡æ€æ”»å‡»æ–¹æ³•ä¸»è¦ä¾èµ–è„†å¼±çš„æ¨¡æ¿ï¼Œé›†ä¸­äºå•ä¸€æ”»å‡»åœºæ™¯ï¼Œå¹¶ä»…æš´éœ²å‡ºç‹­çª„çš„æ¼æ´ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†VERA-Vï¼Œä¸€ä¸ªå˜åˆ†æ¨æ–­æ¡†æ¶ï¼Œå°†å¤šæ¨¡æ€è¶Šç‹±å‘ç°é‡æ–°è¡¨è¿°ä¸ºå­¦ä¹ æˆå¯¹æ–‡æœ¬-å›¾åƒæç¤ºçš„è”åˆåéªŒåˆ†å¸ƒã€‚è¿™ç§æ¦‚ç‡è§†è§’ä½¿å¾—ç”Ÿæˆéšè”½çš„ã€è€¦åˆçš„å¯¹æŠ—è¾“å…¥æˆä¸ºå¯èƒ½ï¼Œä»è€Œç»•è¿‡æ¨¡å‹çš„ä¿æŠ¤æœºåˆ¶ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§æ”»å‡»è€…æ¥è¿‘ä¼¼åéªŒï¼Œä»è€Œé«˜æ•ˆé‡‡æ ·å¤šæ ·çš„è¶Šç‹±è¾“å…¥ï¼Œå¹¶æä¾›å¯¹æ¼æ´çš„åˆ†å¸ƒæ€§æ´å¯Ÿã€‚VERA-Vè¿˜æ•´åˆäº†ä¸‰ç§äº’è¡¥ç­–ç•¥ï¼šåŸºäºæ’ç‰ˆçš„æ–‡æœ¬æç¤ºã€åŸºäºæ‰©æ•£çš„å›¾åƒåˆæˆä»¥åŠç»“æ„åŒ–å¹²æ‰°ç‰©ï¼Œä»¥åˆ†æ•£VLMçš„æ³¨æ„åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVERA-Våœ¨HarmBenchå’ŒHADESåŸºå‡†æµ‹è¯•ä¸­ï¼ŒæŒç»­è¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œåœ¨GPT-4oä¸Šæ”»å‡»æˆåŠŸç‡æé«˜äº†53.75%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨è¶Šç‹±æ”»å‡»ä¸­çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè„†å¼±çš„æ¨¡æ¿ï¼Œæ— æ³•å…¨é¢æ­ç¤ºæ¨¡å‹çš„æ¼æ´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVERA-Væ¡†æ¶é€šè¿‡å˜åˆ†æ¨æ–­çš„æ–¹å¼ï¼Œå°†å¤šæ¨¡æ€è¶Šç‹±é—®é¢˜è½¬åŒ–ä¸ºå­¦ä¹ æ–‡æœ¬ä¸å›¾åƒæç¤ºçš„è”åˆåéªŒåˆ†å¸ƒï¼Œä»è€Œç”Ÿæˆéšè”½çš„å¯¹æŠ—è¾“å…¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVERA-Vçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) è½»é‡çº§æ”»å‡»è€…ç”¨äºè¿‘ä¼¼åéªŒåˆ†å¸ƒï¼›2) åŸºäºæ’ç‰ˆçš„æ–‡æœ¬æç¤ºåµŒå…¥æœ‰å®³çº¿ç´¢ï¼›3) åŸºäºæ‰©æ•£çš„å›¾åƒåˆæˆå¼•å…¥å¯¹æŠ—ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šVERA-Vçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è¶Šç‹±å‘ç°è§†ä¸ºæ¦‚ç‡é—®é¢˜ï¼Œå…è®¸ç”Ÿæˆè€¦åˆçš„å¯¹æŠ—è¾“å…¥ï¼Œæ˜¾è‘—æé«˜äº†æ”»å‡»çš„éšè”½æ€§å’ŒæˆåŠŸç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åéªŒè¿‘ä¼¼ï¼Œç½‘ç»œç»“æ„ç»è¿‡ç²¾ç®€ä»¥æé«˜é‡‡æ ·æ•ˆç‡ï¼ŒåŒæ—¶å¼•å…¥ç»“æ„åŒ–å¹²æ‰°ç‰©ä»¥åˆ†æ•£æ¨¡å‹çš„æ³¨æ„åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVERA-Våœ¨HarmBenchå’ŒHADESåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºæœ€ä½³åŸºçº¿åœ¨GPT-4oä¸Šæ”»å‡»æˆåŠŸç‡æé«˜äº†53.75%ï¼Œå±•ç°äº†å…¶åœ¨å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ€§æµ‹è¯•ã€å¯¹æŠ—æ ·æœ¬ç”Ÿæˆä»¥åŠå¤šæ¨¡æ€æ¨¡å‹çš„é²æ£’æ€§è¯„ä¼°ã€‚é€šè¿‡æ­ç¤ºæ¨¡å‹çš„è„†å¼±æ€§ï¼ŒVERA-Vèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´å®‰å…¨çš„è§†è§‰-è¯­è¨€ç³»ç»Ÿï¼Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) extend large language models with visual reasoning, but their multimodal design also introduces new, underexplored vulnerabilities. Existing multimodal red-teaming methods largely rely on brittle templates, focus on single-attack settings, and expose only a narrow subset of vulnerabilities. To address these limitations, we introduce VERA-V, a variational inference framework that recasts multimodal jailbreak discovery as learning a joint posterior distribution over paired text-image prompts. This probabilistic view enables the generation of stealthy, coupled adversarial inputs that bypass model guardrails. We train a lightweight attacker to approximate the posterior, allowing efficient sampling of diverse jailbreaks and providing distributional insights into vulnerabilities. VERA-V further integrates three complementary strategies: (i) typography-based text prompts that embed harmful cues, (ii) diffusion-based image synthesis that introduces adversarial signals, and (iii) structured distractors to fragment VLM attention. Experiments on HarmBench and HADES benchmarks show that VERA-V consistently outperforms state-of-the-art baselines on both open-source and frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the best baseline on GPT-4o.

