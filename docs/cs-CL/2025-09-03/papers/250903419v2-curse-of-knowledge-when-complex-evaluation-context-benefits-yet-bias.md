---
layout: default
title: Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases LLM Judges
---

# Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases LLM Judges

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03419" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03419v2</a>
  <a href="https://arxiv.org/pdf/2509.03419.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03419v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03419v2', 'Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases LLM Judges')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weiyuan Li, Xintao Wang, Siyu Yuan, Rui Xu, Jiangjie Chen, Qingqing Dong, Yanghua Xiao, Deqing Yang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03 (æ›´æ–°: 2025-10-31)

**å¤‡æ³¨**: EMNLP 2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ„å»ºComplexEvalåŸºå‡†ï¼Œæ­ç¤ºå¹¶é‡åŒ–LLMè¯„åˆ¤åœ¨å¤æ‚è¯„ä¼°ä¸­å­˜åœ¨çš„è¾…åŠ©ä¿¡æ¯è¯±å¯¼åå·®é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `LLMè¯„åˆ¤` `å¤æ‚è¯„ä¼°` `è¾…åŠ©ä¿¡æ¯åå·®` `ComplexEval` `åŸºå‡†æµ‹è¯•` `åå·®é‡åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè¯„åˆ¤æ–¹æ³•åœ¨ç®€å•åœºæ™¯è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸­ï¼Œå…¶å¯é æ€§é¢ä¸´å¤šæ–¹é¢è§„åˆ™ã€éç»“æ„åŒ–å‚è€ƒç­”æ¡ˆç­‰æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æ„å»ºComplexEvalåŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°æš´éœ²å’Œé‡åŒ–LLMè¯„åˆ¤ä¸­å­˜åœ¨çš„è¾…åŠ©ä¿¡æ¯è¯±å¯¼åå·®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰è¯„ä¼°æ¨¡å‹éƒ½å®¹æ˜“å—åˆ°åå·®å½±å“ï¼Œä¸”åå·®ç¨‹åº¦éšä»»åŠ¡å¤æ‚æ€§å¢åŠ ï¼Œå¤§å‹æ¨ç†æ¨¡å‹è¡¨ç°å‡ºåå¸¸çš„è„†å¼±æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)èƒ½åŠ›çš„å¢å¼ºï¼Œå®ƒä»¬é¢ä¸´ç€æ—¥ç›Šå¤šæ ·åŒ–å’Œå¤æ‚çš„ä»»åŠ¡ï¼Œè¿™ä½¿å¾—å¯é çš„è¯„ä¼°å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°†LLMsä½œä¸ºè¯„åˆ¤è€…çš„èŒƒå¼å·²ç»æˆä¸ºä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å…ˆå‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ç®€å•çš„è®¾ç½®ä¸­ã€‚å®ƒä»¬åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å¯é æ€§â€”â€”å…¶ä¸­å¤šæ–¹é¢çš„è§„åˆ™ã€éç»“æ„åŒ–çš„å‚è€ƒç­”æ¡ˆå’Œç»†å¾®çš„åˆ¤åˆ«æ ‡å‡†è‡³å…³é‡è¦â€”â€”ä»ç„¶æœªè¢«å……åˆ†ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†ComplexEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿåœ°æš´éœ²å’Œé‡åŒ–è¾…åŠ©ä¿¡æ¯è¯±å¯¼åå·®çš„æŒ‘æˆ˜åŸºå‡†ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è°ƒæŸ¥å¹¶éªŒè¯äº†12ä¸ªåŸºæœ¬åœºæ™¯å’Œ3ä¸ªé«˜çº§åœºæ™¯ä¸­å…ˆå‰æœªæ¢ç´¢çš„6ç§åå·®ã€‚å…³é”®å‘ç°è¡¨æ˜ï¼šï¼ˆ1ï¼‰æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹éƒ½è¡¨ç°å‡ºå¯¹è¿™äº›åå·®çš„æ˜¾è‘—æ•æ„Ÿæ€§ï¼Œå¹¶ä¸”åå·®å¹…åº¦éšä»»åŠ¡å¤æ‚æ€§è€Œå¢åŠ ï¼›ï¼ˆ2ï¼‰å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰è¡¨ç°å‡ºåå¸¸çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬çš„æ·±å…¥åˆ†æä¸ºæé«˜è¯„ä¼°ä¿¡å·çš„å‡†ç¡®æ€§å’Œå¯éªŒè¯æ€§æä¾›äº†å…³é”®è§è§£ï¼Œä¸ºæ›´é€šç”¨å’Œé²æ£’çš„è¯„ä¼°æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè¯„åˆ¤è€…åœ¨å¤æ‚è¯„ä¼°ä»»åŠ¡ä¸­å­˜åœ¨çš„åå·®é—®é¢˜ã€‚ç°æœ‰çš„LLMè¯„åˆ¤æ–¹æ³•åœ¨ç®€å•åœºæ™¯ä¸‹è¡¨ç°å°šå¯ï¼Œä½†åœ¨æ¶‰åŠå¤šæ–¹é¢è§„åˆ™ã€éç»“æ„åŒ–å‚è€ƒç­”æ¡ˆå’Œç»†å¾®åˆ¤åˆ«æ ‡å‡†çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œå…¶å¯é æ€§å—åˆ°æŒ‘æˆ˜ã€‚è¿™äº›å¤æ‚ä»»åŠ¡å®¹æ˜“å—åˆ°è¾…åŠ©ä¿¡æ¯è¯±å¯¼åå·®çš„å½±å“ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å‡†ç¡®ç”šè‡³äº§ç”Ÿè¯¯å¯¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªä¸“é—¨çš„åŸºå‡†æµ‹è¯•é›†ComplexEvalï¼Œç³»ç»Ÿæ€§åœ°æš´éœ²å’Œé‡åŒ–LLMè¯„åˆ¤ä¸­å­˜åœ¨çš„è¾…åŠ©ä¿¡æ¯è¯±å¯¼åå·®ã€‚é€šè¿‡è®¾è®¡åŒ…å«å¤šç§åå·®ç±»å‹çš„æµ‹è¯•ç”¨ä¾‹ï¼Œè¯„ä¼°LLMè¯„åˆ¤è€…åœ¨ä¸åŒå¤æ‚ç¨‹åº¦ä»»åŠ¡ä¸‹çš„è¡¨ç°ï¼Œä»è€Œæ­ç¤ºå…¶æ½œåœ¨çš„è„†å¼±æ€§ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†æ›´å…¨é¢åœ°äº†è§£LLMè¯„åˆ¤è€…çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæ”¹è¿›è¯„ä¼°æ–¹æ³•æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1ï¼‰æ„å»ºComplexEvalåŸºå‡†ï¼ŒåŒ…å«12ä¸ªåŸºæœ¬åœºæ™¯å’Œ3ä¸ªé«˜çº§åœºæ™¯ï¼Œæ¶µç›–6ç§ä¸åŒç±»å‹çš„è¾…åŠ©ä¿¡æ¯è¯±å¯¼åå·®ï¼›2ï¼‰é€‰æ‹©å¤šä¸ªLLMä½œä¸ºè¯„åˆ¤è€…ï¼ŒåŒ…æ‹¬ä¸åŒè§„æ¨¡å’Œç±»å‹çš„æ¨¡å‹ï¼Œä¾‹å¦‚GPT-3.5ã€GPT-4ç­‰ï¼›3ï¼‰è®¾è®¡å®éªŒæµç¨‹ï¼Œé’ˆå¯¹æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè®©LLMè¯„åˆ¤è€…è¿›è¡Œè¯„ä¼°ï¼Œå¹¶è®°å½•å…¶è¾“å‡ºç»“æœï¼›4ï¼‰åˆ†æå®éªŒç»“æœï¼Œé‡åŒ–ä¸åŒLLMè¯„åˆ¤è€…åœ¨ä¸åŒåå·®ç±»å‹ä¸‹çš„è¡¨ç°ï¼Œå¹¶è¿›è¡Œæ·±å…¥åˆ†æï¼Œæ‰¾å‡ºå¯¼è‡´åå·®çš„åŸå› ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ„å»ºäº†ComplexEvalåŸºå‡†ï¼Œè¯¥åŸºå‡†ä¸“é—¨ç”¨äºè¯„ä¼°LLMè¯„åˆ¤è€…åœ¨å¤æ‚è¯„ä¼°ä»»åŠ¡ä¸­çš„åå·®ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒComplexEvalæ›´åŠ å…³æ³¨å¤æ‚ä»»åŠ¡ä¸­å­˜åœ¨çš„è¾…åŠ©ä¿¡æ¯è¯±å¯¼åå·®ï¼Œå¹¶è®¾è®¡äº†å¤šç§æµ‹è¯•ç”¨ä¾‹æ¥ç³»ç»Ÿæ€§åœ°æš´éœ²è¿™äº›åå·®ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ·±å…¥åˆ†æäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°å‡ºçš„åå¸¸è„†å¼±æ€§ï¼Œä¸ºæ”¹è¿›LLMè¯„åˆ¤æ–¹æ³•æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šComplexEvalåŸºå‡†çš„å…³é”®è®¾è®¡åœ¨äºå…¶æµ‹è¯•ç”¨ä¾‹çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹éƒ½åŒ…å«å¤šä¸ªç»´åº¦çš„ä¿¡æ¯ï¼Œä¾‹å¦‚å¤šæ–¹é¢çš„è§„åˆ™ã€éç»“æ„åŒ–çš„å‚è€ƒç­”æ¡ˆå’Œç»†å¾®çš„åˆ¤åˆ«æ ‡å‡†ã€‚æ­¤å¤–ï¼Œæµ‹è¯•ç”¨ä¾‹è¿˜åŒ…å«äº†ä¸åŒç±»å‹çš„è¾…åŠ©ä¿¡æ¯ï¼Œä¾‹å¦‚é”™è¯¯çš„å…ˆéªŒçŸ¥è¯†ã€ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç­‰ï¼Œç”¨äºè¯±å¯¼LLMè¯„åˆ¤è€…äº§ç”Ÿåå·®ã€‚è®ºæ–‡è¿˜è®¾è®¡äº†ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–LLMè¯„åˆ¤è€…åœ¨ä¸åŒåå·®ç±»å‹ä¸‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰è¯„ä¼°æ¨¡å‹éƒ½è¡¨ç°å‡ºå¯¹è¾…åŠ©ä¿¡æ¯è¯±å¯¼åå·®çš„æ˜¾è‘—æ•æ„Ÿæ€§ï¼Œä¸”åå·®å¹…åº¦éšä»»åŠ¡å¤æ‚æ€§å¢åŠ ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰è¡¨ç°å‡ºåå¸¸çš„è„†å¼±æ€§ï¼Œè¿™è¡¨æ˜å³ä½¿æ˜¯å¼ºå¤§çš„LLMä¹Ÿå¯èƒ½åœ¨å¤æ‚è¯„ä¼°ä»»åŠ¡ä¸­å—åˆ°åå·®çš„å½±å“ã€‚ComplexEvalåŸºå‡†çš„æ„å»ºå’Œå®éªŒç»“æœä¸ºæ”¹è¿›LLMè¯„åˆ¤æ–¹æ³•æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡LLMè¯„åˆ¤çš„å¯é æ€§å’Œå…¬æ­£æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤æ‚è¯„ä¼°çš„åœºæ™¯ï¼Œå¦‚æ•™è‚²è¯„ä¼°ã€ç§‘ç ”è¯„å®¡ã€å†…å®¹å®¡æ ¸ç­‰ã€‚é€šè¿‡è¯†åˆ«å’Œå‡è½»LLMè¯„åˆ¤ä¸­çš„åå·®ï¼Œå¯ä»¥æé«˜è¯„ä¼°ç»“æœçš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ï¼Œä»è€Œä¿ƒè¿›ç›¸å…³é¢†åŸŸçš„å¥åº·å‘å±•ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°æ›´å¤šç±»å‹çš„å¤æ‚ä»»åŠ¡å’Œåå·®ç±»å‹ï¼Œå¹¶å¼€å‘æ›´æœ‰æ•ˆçš„åå·®ç¼“è§£æ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) grow more capable, they face increasingly diverse and complex tasks, making reliable evaluation challenging. The paradigm of LLMs as judges has emerged as a scalable solution, yet prior work primarily focuses on simple settings. Their reliability in complex tasks--where multi-faceted rubrics, unstructured reference answers, and nuanced criteria are critical--remains understudied. In this paper, we constructed ComplexEval, a challenge benchmark designed to systematically expose and quantify Auxiliary Information Induced Biases. We systematically investigated and validated 6 previously unexplored biases across 12 basic and 3 advanced scenarios. Key findings reveal: (1) all evaluated models exhibit significant susceptibility to these biases, with bias magnitude scaling with task complexity; (2) notably, Large Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth analysis offers crucial insights for improving the accuracy and verifiability of evaluation signals, paving the way for more general and robust evaluation models.

