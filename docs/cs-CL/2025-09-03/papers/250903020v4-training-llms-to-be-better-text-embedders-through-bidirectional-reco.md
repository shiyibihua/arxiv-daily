---
layout: default
title: Training LLMs to be Better Text Embedders through Bidirectional Reconstruction
---

# Training LLMs to be Better Text Embedders through Bidirectional Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03020" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03020v4</a>
  <a href="https://arxiv.org/pdf/2509.03020.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03020v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03020v4', 'Training LLMs to be Better Text Embedders through Bidirectional Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chang Su, Dengliang Shi, Siyuan Huang, Jintao Du, Changhua Meng, Yu Cheng, Weiqiang Wang, Zhouhan Lin

**åˆ†ç±»**: cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03 (æ›´æ–°: 2025-10-09)

**å¤‡æ³¨**: accepted by EMNLP 2025 Main Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒå‘é‡å»ºè®­ç»ƒæ–¹æ³•ï¼Œæå‡LLMä½œä¸ºæ–‡æœ¬åµŒå…¥æ¨¡å‹çš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åµŒå…¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŒå‘é‡å»º` `å¯¹æ¯”å­¦ä¹ ` `ä¿¡æ¯æ£€ç´¢` `è¯­ä¹‰è¡¨ç¤º` `MTEBåŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„æ–‡æœ¬åµŒå…¥æ–¹æ³•ä¾èµ–æœ€ç»ˆtokenï¼Œä½†è¿™äº›tokenç¼ºä¹å¯¹å…¨å±€è¯­ä¹‰çš„è®­ç»ƒï¼Œé™åˆ¶äº†åµŒå…¥è´¨é‡ã€‚
2. æå‡ºåŒå‘ç”Ÿæˆé‡å»ºä»»åŠ¡EBQ2Då’ŒEBD2Qï¼Œé€šè¿‡äº¤æ›¿é‡å»ºæŸ¥è¯¢-æ–‡æ¡£å¯¹æ¥å¢å¼ºæœ€ç»ˆtokençš„è¯­ä¹‰è¡¨è¾¾ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MTEBåŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†LLMçš„æ–‡æœ¬åµŒå…¥æ€§èƒ½ï¼Œè¾¾åˆ°äº†æ–°çš„state-of-the-artã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œå¼ºå¤§çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ã€‚ç°æœ‰çš„åŸºäºLLMçš„æ–‡æœ¬åµŒå…¥æ–¹æ³•é€šå¸¸åˆ©ç”¨æœ€ç»ˆtokenï¼ˆé€šå¸¸æ˜¯[EOS]ç­‰ç‰¹æ®Štokenï¼‰çš„åµŒå…¥ã€‚ç„¶è€Œï¼Œè¿™äº›tokenå¹¶æ²¡æœ‰ç»è¿‡ä¸“é—¨è®­ç»ƒæ¥æ•æ‰æ•´ä¸ªä¸Šä¸‹æ–‡çš„è¯­ä¹‰ï¼Œé™åˆ¶äº†å®ƒä»¬ä½œä¸ºæ–‡æœ¬åµŒå…¥çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ£€ç´¢å’Œé‡æ’åºä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬æå‡ºåœ¨å¯¹æ¯”å­¦ä¹ ä¹‹å‰æ·»åŠ ä¸€ä¸ªæ–°çš„è®­ç»ƒé˜¶æ®µï¼Œä»¥ä¸°å¯Œæœ€ç»ˆtokenåµŒå…¥çš„è¯­ä¹‰ã€‚è¯¥é˜¶æ®µé‡‡ç”¨åŒå‘ç”Ÿæˆé‡å»ºä»»åŠ¡ï¼Œå³EBQ2Dï¼ˆåŸºäºåµŒå…¥çš„æŸ¥è¯¢åˆ°æ–‡æ¡£ï¼‰å’ŒEBD2Qï¼ˆåŸºäºåµŒå…¥çš„æ–‡æ¡£åˆ°æŸ¥è¯¢ï¼‰ï¼Œå®ƒä»¬äº¤æ›¿è¿›è¡Œï¼Œä»¥é”šå®š[EOS]åµŒå…¥å¹¶é‡å»ºæŸ¥è¯¢-æ–‡æ¡£å¯¹çš„ä»»ä¸€ä¾§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é™„åŠ è®­ç»ƒé˜¶æ®µæ˜¾è‘—æé«˜äº†LLMåœ¨æµ·é‡æ–‡æœ¬åµŒå…¥åŸºå‡†ï¼ˆMTEBï¼‰ä¸Šçš„æ€§èƒ½ï¼Œåœ¨ä¸åŒçš„LLMåŸºç¡€æ¨¡å‹å’Œè§„æ¨¡ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºLLMçš„æ–‡æœ¬åµŒå…¥æ–¹æ³•ï¼Œé€šå¸¸ç›´æ¥ä½¿ç”¨LLMçš„æœ€ç»ˆtokenï¼ˆå¦‚[EOS]ï¼‰çš„åµŒå…¥ä½œä¸ºæ–‡æœ¬çš„å‘é‡è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›tokenåœ¨é¢„è®­ç»ƒé˜¶æ®µå¹¶æ²¡æœ‰è¢«æ˜ç¡®åœ°è®­ç»ƒæ¥æ•æ‰æ•´ä¸ªä¸Šä¸‹æ–‡çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´å…¶ä½œä¸ºæ–‡æœ¬åµŒå…¥çš„è´¨é‡ä¸é«˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®è¯­ä¹‰åŒ¹é…çš„æ£€ç´¢å’Œé‡æ’åºä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥é¢å¤–çš„è®­ç»ƒé˜¶æ®µï¼Œä¸“é—¨å¢å¼ºLLMæœ€ç»ˆtokençš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è®¾è®¡åŒå‘ç”Ÿæˆé‡å»ºä»»åŠ¡ï¼Œè®©LLMå­¦ä¹ å¦‚ä½•åˆ©ç”¨æœ€ç»ˆtokençš„åµŒå…¥æ¥é‡å»ºæŸ¥è¯¢æˆ–æ–‡æ¡£ï¼Œä»è€Œè¿«ä½¿è¯¥tokenåŒ…å«æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬é¢„è®­ç»ƒçš„LLMã€åŒå‘é‡å»ºè®­ç»ƒé˜¶æ®µå’Œå¯¹æ¯”å­¦ä¹ é˜¶æ®µã€‚é¦–å…ˆï¼Œä½¿ç”¨æå‡ºçš„EBQ2Då’ŒEBD2Qä»»åŠ¡å¯¹LLMè¿›è¡Œè®­ç»ƒï¼Œå¢å¼ºæœ€ç»ˆtokençš„è¯­ä¹‰è¡¨è¾¾ã€‚ç„¶åï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–LLMçš„æ–‡æœ¬åµŒå…¥èƒ½åŠ›ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†EBQ2Då’ŒEBD2QåŒå‘é‡å»ºä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„å•å‘ç”Ÿæˆä»»åŠ¡ä¸åŒï¼Œè¯¥æ–¹æ³•åŒæ—¶è€ƒè™‘äº†ä»æŸ¥è¯¢åˆ°æ–‡æ¡£å’Œä»æ–‡æ¡£åˆ°æŸ¥è¯¢çš„é‡å»ºï¼Œä»è€Œæ›´å…¨é¢åœ°åˆ©ç”¨äº†æŸ¥è¯¢-æ–‡æ¡£å¯¹çš„ä¿¡æ¯ï¼Œå¹¶æœ‰æ•ˆåœ°æå‡äº†æœ€ç»ˆtokençš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šEBQ2Dä»»åŠ¡çš„ç›®æ ‡æ˜¯åˆ©ç”¨æŸ¥è¯¢çš„æ–‡æœ¬åµŒå…¥ï¼ˆé€šè¿‡LLMè·å¾—ï¼‰æ¥é‡å»ºæ–‡æ¡£ï¼Œè€ŒEBD2Qä»»åŠ¡åˆ™ç›¸åã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è¡¡é‡é‡å»ºçš„è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªæŸ¥è¯¢-æ–‡æ¡£å¯¹ï¼Œé¦–å…ˆä½¿ç”¨LLMè·å¾—æŸ¥è¯¢å’Œæ–‡æ¡£çš„åµŒå…¥è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨æŸ¥è¯¢çš„åµŒå…¥æ¥ç”Ÿæˆæ–‡æ¡£ï¼Œå¹¶ä½¿ç”¨æ–‡æ¡£çš„åµŒå…¥æ¥ç”ŸæˆæŸ¥è¯¢ã€‚é€šè¿‡æœ€å°åŒ–é‡å»ºè¯¯å·®ï¼Œå¯ä»¥ä½¿LLMå­¦ä¹ åˆ°æ›´æœ‰æ•ˆçš„æ–‡æœ¬åµŒå…¥è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MTEBåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†ç°æœ‰çš„state-of-the-artæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†LLMçš„æ€§èƒ½æå‡è¶…è¿‡5ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒçš„LLMåŸºç¡€æ¨¡å‹å’Œè§„æ¨¡ä¸Šéƒ½è¡¨ç°å‡ºäº†è‰¯å¥½çš„æ•ˆæœï¼Œè¡¨æ˜å…¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºä¿¡æ¯æ£€ç´¢ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ã€é—®ç­”ç³»ç»Ÿã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æ–‡æœ¬åµŒå…¥çš„è´¨é‡ï¼Œå¯ä»¥æé«˜æœç´¢ç»“æœçš„ç›¸å…³æ€§ã€é—®ç­”ç³»ç»Ÿçš„å‡†ç¡®æ€§ä»¥åŠæ¨èç³»ç»Ÿçš„ä¸ªæ€§åŒ–ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†çš„å…¶ä»–ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have increasingly been explored as powerful text embedders. Existing LLM-based text embedding approaches often leverage the embedding of the final token, typically a reserved special token such as [EOS]. However, these tokens have not been intentionally trained to capture the semantics of the whole context, limiting their capacity as text embeddings, especially for retrieval and re-ranking tasks. We propose to add a new training stage before contrastive learning to enrich the semantics of the final token embedding. This stage employs bidirectional generative reconstruction tasks, namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based Document-to-Query), which interleave to anchor the [EOS] embedding and reconstruct either side of Query-Document pairs. Experimental results demonstrate that our additional training stage significantly improves LLM performance on the Massive Text Embedding Benchmark (MTEB), achieving new state-of-the-art results across different LLM base models and scales.

