---
layout: default
title: Advancing SLM Tool-Use Capability using Reinforcement Learning
---

# Advancing SLM Tool-Use Capability using Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04518" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04518v2</a>
  <a href="https://arxiv.org/pdf/2509.04518.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04518v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04518v2', 'Advancing SLM Tool-Use Capability using Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dhruvi Paprunia, Vansh Kharidia, Pankti Doshi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03 (æ›´æ–°: 2025-09-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¼ºåŒ–å­¦ä¹ GRPOæå‡å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å·¥å…·ä½¿ç”¨` `ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–` `å‡½æ•°è°ƒç”¨` `JSONè¾“å‡º` `å¥–åŠ±å‡½æ•°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ï¼Œå‡†ç¡®é›†æˆå·¥å…·ä½¿ç”¨ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡å¥–åŠ±ç³»ç»Ÿï¼Œä¼˜åŒ–SLMçš„å·¥å…·é€‰æ‹©å’Œå‚æ•°ä½¿ç”¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGRPOèƒ½å¤Ÿæ˜¾è‘—æé«˜SLMåœ¨å·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢çš„å‡†ç¡®æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæ¥æé«˜å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å·¥å…·ä½¿ç”¨å‡†ç¡®æ€§ã€‚ç”±äºå·¥å…·ä½¿ç”¨èƒ½åŠ›å¯¹äºè®¿é—®å¤–éƒ¨æ•°æ®å’Œå†…éƒ¨èµ„æºè‡³å…³é‡è¦ï¼Œå› æ­¤å®ƒå·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªå…³é”®ç‰¹å¾ã€‚è™½ç„¶LLMåœ¨è¿™æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†SLMåœ¨å‡†ç¡®é›†æˆå·¥å…·ä½¿ç”¨æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚é€šè¿‡è®¾è®¡ä¸€ä¸ªæ˜ç¡®çš„å¥–åŠ±ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¼ºåŒ–ç»“æ„åŒ–çš„JSONè¾“å‡ºã€æ­£ç¡®çš„å·¥å…·é€‰æ‹©å’Œç²¾ç¡®çš„å‚æ•°ä½¿ç”¨ï¼Œæœ¬æ–‡è¯æ˜äº†GRPOèƒ½å¤Ÿä½¿SLMåœ¨å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼ˆå‡½æ•°è°ƒç”¨/JSONè¾“å‡ºï¼‰æ–¹é¢å–å¾—æ˜¾è‘—æ”¹è¿›ã€‚è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„è®­ç»ƒæ–¹æ³•ï¼Œå¢å¼ºäº†SLMåœ¨å®é™…AIåº”ç”¨ä¸­çš„å®é™…éƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨å·¥å…·ä½¿ç”¨æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥è®­ç»ƒSLMè¿›è¡Œå·¥å…·è°ƒç”¨ï¼Œå¾€å¾€éš¾ä»¥è¾¾åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œä¸”å¯¹è®¡ç®—èµ„æºè¦æ±‚è¾ƒé«˜ã€‚SLMéš¾ä»¥å‡†ç¡®é€‰æ‹©åˆé€‚çš„å·¥å…·å¹¶æ­£ç¡®ä½¿ç”¨å‚æ•°ï¼Œå¯¼è‡´å·¥å…·ä½¿ç”¨æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å¥–åŠ±æœºåˆ¶å¼•å¯¼SLMå­¦ä¹ å¦‚ä½•æ›´æœ‰æ•ˆåœ°ä½¿ç”¨å·¥å…·ã€‚å…·ä½“è€Œè¨€ï¼Œé‡‡ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œé€šè¿‡å¥–åŠ±æ­£ç¡®çš„å·¥å…·é€‰æ‹©ã€ç²¾ç¡®çš„å‚æ•°ä½¿ç”¨ä»¥åŠç»“æ„åŒ–çš„JSONè¾“å‡ºï¼Œæ¥ä¼˜åŒ–SLMçš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨ä½¿SLMèƒ½å¤Ÿæ›´å‡†ç¡®ã€æ›´å¯é åœ°è¿›è¡Œå·¥å…·è°ƒç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) å®šä¹‰å·¥å…·ä½¿ç”¨ä»»åŠ¡ï¼Œä¾‹å¦‚å‡½æ•°è°ƒç”¨æˆ–JSONè¾“å‡ºï¼›2) æ„å»ºSLMä½œä¸ºæ™ºèƒ½ä½“ï¼Œè´Ÿè´£é€‰æ‹©å·¥å…·å’Œå‚æ•°ï¼›3) è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œç”¨äºè¯„ä¼°SLMçš„å·¥å…·ä½¿ç”¨æ•ˆæœï¼›4) ä½¿ç”¨GRPOç®—æ³•è®­ç»ƒSLMï¼Œä½¿å…¶æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒSLMä¸ç¯å¢ƒäº¤äº’ï¼Œæ ¹æ®å¥–åŠ±è°ƒæ•´ç­–ç•¥ï¼Œæœ€ç»ˆå­¦ä¼šé«˜æ•ˆåœ°ä½¿ç”¨å·¥å…·ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†GRPOåº”ç”¨äºSLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›æå‡ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å·¥å…·ä½¿ç”¨çš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ã€‚GRPOé€šè¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢ç­–ç•¥ç©ºé—´ï¼Œæ‰¾åˆ°æ›´ä¼˜çš„å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è®¾è®¡çš„å¥–åŠ±å‡½æ•°èƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼SLMå­¦ä¹ æ­£ç¡®çš„å·¥å…·é€‰æ‹©å’Œå‚æ•°ä½¿ç”¨æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šå¥–åŠ±å‡½æ•°æ˜¯å…³é”®çš„è®¾è®¡è¦ç´ ã€‚è®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªå¤šæ–¹é¢çš„å¥–åŠ±ç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š1) ç»“æ„åŒ–JSONè¾“å‡ºå¥–åŠ±ï¼Œé¼“åŠ±SLMç”Ÿæˆç¬¦åˆè§„èŒƒçš„JSONæ ¼å¼ï¼›2) æ­£ç¡®å·¥å…·é€‰æ‹©å¥–åŠ±ï¼Œå¥–åŠ±SLMé€‰æ‹©æ­£ç¡®çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ï¼›3) ç²¾ç¡®å‚æ•°ä½¿ç”¨å¥–åŠ±ï¼Œå¥–åŠ±SLMä½¿ç”¨æ­£ç¡®çš„å‚æ•°å€¼ã€‚GRPOç®—æ³•çš„å…·ä½“å‚æ•°è®¾ç½®ï¼ˆå¦‚å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ç­‰ï¼‰ä»¥åŠSLMçš„ç½‘ç»œç»“æ„ï¼ˆå¦‚Transformerå±‚æ•°ã€éšè—å±‚å¤§å°ç­‰ï¼‰ä¹Ÿä¼šå½±å“æœ€ç»ˆçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡GRPOè®­ç»ƒï¼ŒSLMåœ¨å·¥å…·ä½¿ç”¨å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œå·¥å…·é€‰æ‹©å‡†ç¡®ç‡ã€å‚æ•°ä½¿ç”¨ç²¾ç¡®åº¦ï¼‰å’Œä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœï¼ˆä¾‹å¦‚ï¼Œç›‘ç£å­¦ä¹ æ–¹æ³•ï¼‰éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚æ€»ä½“è€Œè¨€ï¼ŒGRPOä¸ºæå‡SLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦SLMè¿›è¡Œå·¥å…·è°ƒç”¨çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å®¢æœã€æ•°æ®åˆ†æç­‰ã€‚é€šè¿‡æå‡SLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°å®Œæˆå¤æ‚ä»»åŠ¡ï¼Œæé«˜å·¥ä½œæ•ˆç‡ï¼Œé™ä½è¿è¥æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤šç±»å‹çš„å·¥å…·å’Œæ›´å¤æ‚çš„ä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨SLMåœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In an era where tool-augmented AI agents are becoming increasingly vital, our findings highlight the ability of Group Relative Policy Optimization (GRPO) to empower SLMs, which are traditionally constrained in tool use. The ability to use tools effectively has become a defining feature of Large Language Models (LLMs), allowing them to access external data and internal resources. As AI agents grow more sophisticated, tool-use capabilities have become indispensable. While LLMs have made significant progress in this area, Small Language Models (SLMs) still face challenges in accurately integrating tool use, especially in resource-constrained settings.
>   This study investigates how Reinforcement Learning, specifically Group Relative Policy Optimization (GRPO), can enhance the tool-use accuracy of SLMs. By designing a well-defined reward system that reinforces structured JSON output, correct tool selection, and precise parameter usage, we demonstrate that GRPO enables SLMs to achieve significant improvements in tool-use capabilities (function calling/JSON output). Our approach provides a computationally efficient training method that enhances SLMs practical deployment in real-world AI applications.

