---
layout: default
title: Artificially Fluent: Swahili AI Performance Benchmarks Between English-Trained and Natively-Trained Datasets
---

# Artificially Fluent: Swahili AI Performance Benchmarks Between English-Trained and Natively-Trained Datasets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04516" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04516v2</a>
  <a href="https://arxiv.org/pdf/2509.04516.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04516v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04516v2', 'Artificially Fluent: Swahili AI Performance Benchmarks Between English-Trained and Natively-Trained Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sophie Jaffer, Simeon Sayer

**åˆ†ç±»**: cs.CL, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03 (æ›´æ–°: 2025-09-28)

**å¤‡æ³¨**: 13 Pages, 3 Figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¯¹æ¯”è‹±è¯­è®­ç»ƒä¸æ–¯ç“¦å¸Œé‡Œè¯­åŸç”Ÿè®­ç»ƒï¼Œæ­ç¤ºLLMè·¨è¯­è¨€æ€§èƒ½å·®å¼‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æ¨¡å‹` `è¯­è¨€å…¬å¹³æ€§` `æ–¯ç“¦å¸Œé‡Œè¯­` `BERTæ¨¡å‹` `è·¨è¯­è¨€è¿ç§»` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨ç¿»è¯‘` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€å¤„ç†ä¸­å­˜åœ¨æ€§èƒ½å·®å¼‚ï¼Œè‹±è¯­æ•°æ®ä¸»å¯¼åœ°ä½å¯èƒ½å¯¼è‡´éè‹±è¯­ä½¿ç”¨è€…å¤„äºåŠ£åŠ¿ã€‚
2. é€šè¿‡å¯¹æ¯”è‹±è¯­è®­ç»ƒå’Œæ–¯ç“¦å¸Œé‡Œè¯­åŸç”Ÿè®­ç»ƒçš„BERTæ¨¡å‹ï¼Œç ”ç©¶è¯­è¨€ä¸€è‡´æ€§ä¸è·¨è¯­è¨€æŠ½è±¡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸç”Ÿæ–¯ç“¦å¸Œé‡Œè¯­è®­ç»ƒæ¨¡å‹ä¼˜äºç¿»è¯‘åçš„è‹±è¯­æ¨¡å‹ï¼Œæ­ç¤ºäº†ç¿»è¯‘æ— æ³•å®Œå…¨å¼¥åˆè¯­è¨€è¡¨å¾å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹(LLM)å¤šè¯­è¨€èƒ½åŠ›çš„æ‰©å±•ï¼Œå…¶åœ¨ä¸åŒè¯­è¨€ä¸Šçš„æ€§èƒ½å…¬å¹³æ€§é—®é¢˜æ—¥ç›Šçªå‡ºã€‚ä¸ºäº†éªŒè¯æ•°æ®å·®å¼‚å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½çš„å‡è®¾ï¼Œæœ¬ç ”ç©¶æ¯”è¾ƒäº†ä¸¤ä¸ªå•è¯­BERTæ¨¡å‹ï¼šä¸€ä¸ªå®Œå…¨åœ¨æ–¯ç“¦å¸Œé‡Œè¯­æ•°æ®ä¸Šè®­ç»ƒå’Œæµ‹è¯•ï¼Œå¦ä¸€ä¸ªåœ¨å¯æ¯”çš„è‹±è¯­æ–°é—»æ•°æ®ä¸Šè®­ç»ƒå’Œæµ‹è¯•ã€‚ä¸ºäº†æ¨¡æ‹Ÿå¤šè¯­è¨€LLMå¦‚ä½•é€šè¿‡å†…éƒ¨ç¿»è¯‘å’ŒæŠ½è±¡å¤„ç†éè‹±è¯­æŸ¥è¯¢ï¼Œæˆ‘ä»¬å°†æ–¯ç“¦å¸Œé‡Œè¯­æ–°é—»æ•°æ®ç¿»è¯‘æˆè‹±è¯­ï¼Œå¹¶ä½¿ç”¨è‹±è¯­è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡æ¯”è¾ƒåœ¨è‹±è¯­æ¨¡å‹ä¸Šè¯„ä¼°æ–¯ç“¦å¸Œé‡Œè¯­ç¿»è¯‘è¾“å…¥çš„æ€§èƒ½ï¼Œä¸å®Œå…¨åœ¨æ–¯ç“¦å¸Œé‡Œè¯­ä¸­è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œåˆ†ç¦»è¯­è¨€ä¸€è‡´æ€§ä¸è·¨è¯­è¨€æŠ½è±¡çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç¿»è¯‘è´¨é‡å¾ˆé«˜ï¼Œä½†åŸç”Ÿæ–¯ç“¦å¸Œé‡Œè¯­è®­ç»ƒçš„æ¨¡å‹è¡¨ç°ä¼˜äºæ–¯ç“¦å¸Œé‡Œè¯­åˆ°è‹±è¯­ç¿»è¯‘çš„æ¨¡å‹ï¼Œé”™è¯¯ç‡åˆ†åˆ«0.36%å’Œ1.47%ï¼Œå‰è€…é”™è¯¯ç‡çº¦ä¸ºåè€…çš„å››åˆ†ä¹‹ä¸€ã€‚è¿™è¡¨æ˜ç¿»è¯‘æœ¬èº«å¹¶ä¸èƒ½å¼¥åˆè¯­è¨€ä¹‹é—´çš„è¡¨å¾å·®å¼‚ï¼Œå¹¶ä¸”åœ¨ä¸€ç§è¯­è¨€ä¸­è®­ç»ƒçš„æ¨¡å‹å¯èƒ½éš¾ä»¥å‡†ç¡®è§£é‡Šç¿»è¯‘åçš„è¾“å…¥ï¼Œå› ä¸ºå…¶å†…éƒ¨çŸ¥è¯†è¡¨ç¤ºä¸å®Œå–„ã€‚å› æ­¤ï¼Œå¯¹äºå¯é çš„ç»“æœï¼Œæ¯è¯­è®­ç»ƒä»ç„¶å¾ˆé‡è¦ã€‚æœªæ¥çš„ç ”ç©¶åº”ä¾§é‡äºä¸ºä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€å¼€å‘æ›´å¹¿æ³›çš„æ•°æ®é›†ï¼Œå¹¶é‡æ–°å…³æ³¨å¤šè¯­è¨€æ¨¡å‹è¯„ä¼°ï¼Œç¡®ä¿å…¨çƒäººå·¥æ™ºèƒ½éƒ¨ç½²å¯¹ç°æœ‰æ•°å­—é¸¿æ²Ÿçš„å¼ºåŒ–æ•ˆåº”å¾—ä»¥é™ä½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸åŒè¯­è¨€æ—¶å­˜åœ¨çš„æ€§èƒ½å·®å¼‚é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä¾èµ–äºè‹±è¯­æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨å¤„ç†éè‹±è¯­è¯­è¨€æ—¶å¯èƒ½è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•å……åˆ†ç†è§£å’Œè¡¨ç¤ºè¿™äº›è¯­è¨€çš„ç»†å¾®å·®åˆ«ã€‚è¿™ç§æ€§èƒ½å·®è·å¯èƒ½ä¼šåŠ å‰§æ•°å­—é¸¿æ²Ÿï¼Œä½¿éè‹±è¯­ä½¿ç”¨è€…åœ¨è·å–ä¿¡æ¯å’Œæ•™è‚²èµ„æºæ–¹é¢å¤„äºä¸åˆ©åœ°ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æ¯”åœ¨åŸç”Ÿè¯­è¨€ï¼ˆæ–¯ç“¦å¸Œé‡Œè¯­ï¼‰ä¸Šè®­ç»ƒçš„æ¨¡å‹å’Œå°†æ–¯ç“¦å¸Œé‡Œè¯­ç¿»è¯‘æˆè‹±è¯­ååœ¨è‹±è¯­æ¨¡å‹ä¸Šè¯„ä¼°çš„æ¨¡å‹ï¼Œæ¥è¯„ä¼°è¯­è¨€ä¸€è‡´æ€§å’Œè·¨è¯­è¨€æŠ½è±¡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åˆ†ç¦»å‡ºè¯­è¨€æœ¬èº«å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ¢è®¨ç¿»è¯‘æ˜¯å¦èƒ½å¤Ÿå¼¥åˆè¯­è¨€ä¹‹é—´çš„è¡¨å¾å·®å¼‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§å¯¹æ¯”å®éªŒçš„è®¾è®¡ã€‚é¦–å…ˆï¼Œä½¿ç”¨æ–¯ç“¦å¸Œé‡Œè¯­æ–°é—»æ•°æ®è®­ç»ƒä¸€ä¸ªå•è¯­BERTæ¨¡å‹ã€‚ç„¶åï¼Œå°†ç›¸åŒçš„æ–¯ç“¦å¸Œé‡Œè¯­æ–°é—»æ•°æ®ç¿»è¯‘æˆè‹±è¯­ï¼Œå¹¶ä½¿ç”¨è‹±è¯­æ–°é—»æ•°æ®è®­ç»ƒå¦ä¸€ä¸ªå•è¯­BERTæ¨¡å‹ã€‚æœ€åï¼Œä½¿ç”¨åŸå§‹æ–¯ç“¦å¸Œé‡Œè¯­æ•°æ®å’Œç¿»è¯‘åçš„è‹±è¯­æ•°æ®åˆ†åˆ«å¯¹ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½ï¼Œå¯ä»¥è¯„ä¼°è¯­è¨€ä¸€è‡´æ€§å’Œè·¨è¯­è¨€æŠ½è±¡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå…¶è¯„ä¼°æ–¹æ³•ï¼Œå³é€šè¿‡ç¿»è¯‘éè‹±è¯­æ•°æ®å¹¶åœ¨è‹±è¯­æ¨¡å‹ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ¥æ¨¡æ‹Ÿå¤šè¯­è¨€LLMå¤„ç†éè‹±è¯­æŸ¥è¯¢çš„è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ†ç¦»å‡ºè¯­è¨€æœ¬èº«å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ¢è®¨ç¿»è¯‘æ˜¯å¦èƒ½å¤Ÿå¼¥åˆè¯­è¨€ä¹‹é—´çš„è¡¨å¾å·®å¼‚ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å…³æ³¨äº†ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€ï¼ˆæ–¯ç“¦å¸Œé‡Œè¯­ï¼‰ï¼Œè¿™æœ‰åŠ©äºæé«˜äººä»¬å¯¹å¤šè¯­è¨€æ¨¡å‹æ€§èƒ½å…¬å¹³æ€§çš„è®¤è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†BERTæ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨äº†æ ‡å‡†çš„æ–°é—»æ•°æ®ä½œä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚å…³é”®çš„å‚æ•°è®¾ç½®åŒ…æ‹¬BERTæ¨¡å‹çš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ‰¹å¤§å°ç­‰ï¼‰ä»¥åŠç¿»è¯‘æ¨¡å‹çš„è´¨é‡ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚ç½‘ç»œç»“æ„ä¸ºæ ‡å‡†çš„BERTç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸç”Ÿæ–¯ç“¦å¸Œé‡Œè¯­è®­ç»ƒçš„BERTæ¨¡å‹é”™è¯¯ç‡ä¸º0.36%ï¼Œè€Œæ–¯ç“¦å¸Œé‡Œè¯­ç¿»è¯‘æˆè‹±è¯­ååœ¨è‹±è¯­æ¨¡å‹ä¸Šè¯„ä¼°çš„é”™è¯¯ç‡ä¸º1.47%ã€‚åŸç”Ÿæ¨¡å‹é”™è¯¯ç‡çº¦ä¸ºç¿»è¯‘æ¨¡å‹çš„å››åˆ†ä¹‹ä¸€ï¼Œæ˜¾è‘—ä¼˜äºç¿»è¯‘åçš„æ¨¡å‹ï¼Œè¡¨æ˜è¯­è¨€ä¸€è‡´æ€§å¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤šè¯­è¨€ç¯å¢ƒä¸‹äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ•™è‚²ã€ä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸã€‚é€šè¿‡å…³æ³¨ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€ï¼Œæœ‰åŠ©äºç¼©å°æ•°å­—é¸¿æ²Ÿï¼Œä¿ƒè¿›å…¨çƒèŒƒå›´å†…æ›´å…¬å¹³çš„äººå·¥æ™ºèƒ½åº”ç”¨ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•åˆ©ç”¨è¿ç§»å­¦ä¹ ã€å¤šè¯­è¨€è®­ç»ƒç­‰æŠ€æœ¯ï¼Œæé«˜æ¨¡å‹åœ¨å„ç§è¯­è¨€ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) expand multilingual capabilities, questions remain about the equity of their performance across languages. While many communities stand to benefit from AI systems, the dominance of English in training data risks disadvantaging non-English speakers. To test the hypothesis that such data disparities may affect model performance, this study compares two monolingual BERT models: one trained and tested entirely on Swahili data, and another on comparable English news data. To simulate how multilingual LLMs process non-English queries through internal translation and abstraction, we translated the Swahili news data into English and evaluated it using the English-trained model. This approach tests the hypothesis by evaluating whether translating Swahili inputs for evaluation on an English model yields better or worse performance compared to training and testing a model entirely in Swahili, thus isolating the effect of language consistency versus cross-lingual abstraction. The results prove that, despite high-quality translation, the native Swahili-trained model performed better than the Swahili-to-English translated model, producing nearly four times fewer errors: 0.36% vs. 1.47% respectively. This gap suggests that translation alone does not bridge representational differences between languages and that models trained in one language may struggle to accurately interpret translated inputs due to imperfect internal knowledge representation, suggesting that native-language training remains important for reliable outcomes. In educational and informational contexts, even small performance gaps may compound inequality. Future research should focus on addressing broader dataset development for underrepresented languages and renewed attention to multilingual model evaluation, ensuring the reinforcing effect of global AI deployment on existing digital divides is reduced.

