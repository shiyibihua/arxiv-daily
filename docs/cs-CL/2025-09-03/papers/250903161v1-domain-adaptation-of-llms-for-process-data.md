---
layout: default
title: Domain Adaptation of LLMs for Process Data
---

# Domain Adaptation of LLMs for Process Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03161" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03161v1</a>
  <a href="https://arxiv.org/pdf/2509.03161.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03161v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03161v1', 'Domain Adaptation of LLMs for Process Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rafael Seidi Oyamada, Jari Peeperkorn, Jochen De Weerdt, Johannes De Smedt

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLLMé¢†åŸŸè‡ªé€‚åº”çš„è¿‡ç¨‹æ•°æ®é¢„æµ‹æ–¹æ³•ï¼Œæå‡é¢„æµ‹è¿‡ç¨‹ç›‘æ§æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¿‡ç¨‹æŒ–æ˜` `é¢„æµ‹è¿‡ç¨‹ç›‘æ§` `é¢†åŸŸè‡ªé€‚åº”` `å‚æ•°é«˜æ•ˆå¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¿‡ç¨‹æŒ–æ˜æ–¹æ³•ä¾èµ–äºRNNæˆ–å°†äº‹ä»¶æ—¥å¿—è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€ï¼Œå­˜åœ¨æ€§èƒ½ç“¶é¢ˆå’Œä¿¡æ¯æŸå¤±ã€‚
2. è¯¥è®ºæ–‡æå‡ºç›´æ¥å°†é¢„è®­ç»ƒLLMè‡ªé€‚åº”äºè¿‡ç¨‹æ•°æ®ï¼Œåˆ©ç”¨å…¶åºåˆ—ç”Ÿæˆèƒ½åŠ›ï¼Œé¿å…è‡ªç„¶è¯­è¨€è½¬æ¢ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹è¿‡ç¨‹ç›‘æ§ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨å¤šä»»åŠ¡åœºæ™¯ä¸‹ï¼Œä¼˜äºç°æœ‰RNNå’Œå™è¿°å¼æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºåŒ…æ‹¬è¿‡ç¨‹æŒ–æ˜ï¼ˆPMï¼‰åœ¨å†…çš„å„ä¸ªç ”ç©¶é¢†åŸŸå¤‡å—å…³æ³¨çš„çƒ­ç‚¹ã€‚ç›®å‰PMä¸­çš„åº”ç”¨ä¸»è¦é›†ä¸­äºæç¤ºå·¥ç¨‹ç­–ç•¥æˆ–å°†äº‹ä»¶æ—¥å¿—è½¬æ¢ä¸ºå™è¿°å¼æ•°æ®é›†ï¼Œä»è€Œåˆ©ç”¨LLMçš„è¯­ä¹‰èƒ½åŠ›æ¥è§£å†³å„ç§ä»»åŠ¡ã€‚ä¸æ­¤ä¸åŒï¼Œæœ¬ç ”ç©¶è°ƒæŸ¥äº†é¢„è®­ç»ƒLLMç›´æ¥é€‚åº”è¿‡ç¨‹æ•°æ®çš„æ–¹æ³•ï¼Œæ— éœ€è‡ªç„¶è¯­è¨€é‡æ„ï¼Œå…¶åŠ¨æœºæ˜¯è¿™äº›æ¨¡å‹æ“…é•¿ç”Ÿæˆtokenåºåˆ—ï¼Œç±»ä¼¼äºPMä¸­çš„ç›®æ ‡ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œä»¥å‡è½»é€šå¸¸ä¸æ­¤ç±»æ¨¡å‹ç›¸å…³çš„è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬çš„å®éªŒè®¾ç½®ä¾§é‡äºé¢„æµ‹è¿‡ç¨‹ç›‘æ§ï¼ˆPPMï¼‰ï¼Œå¹¶è€ƒè™‘å•ä»»åŠ¡å’Œå¤šä»»åŠ¡é¢„æµ‹ã€‚ç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ–¹æ³•å’Œæœ€è¿‘åŸºäºå™è¿°å¼é£æ ¼çš„è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œé¢„æµ‹æ€§èƒ½å…·æœ‰æ½œåœ¨çš„æ”¹è¿›ï¼Œå°¤å…¶æ˜¯åœ¨å¤šä»»åŠ¡è®¾ç½®ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¾®è°ƒåçš„æ¨¡å‹è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶ä¸”éœ€è¦æ˜¾è‘—æ›´å°‘çš„è¶…å‚æ•°ä¼˜åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é¢„æµ‹è¿‡ç¨‹ç›‘æ§ï¼ˆPPMï¼‰æ–¹æ³•ï¼Œå¦‚RNNï¼Œåœ¨å¤„ç†å¤æ‚è¿‡ç¨‹æ•°æ®æ—¶å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚å°†äº‹ä»¶æ—¥å¿—è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€å™è¿°çš„æ–¹å¼ï¼Œè™½ç„¶å¯ä»¥åˆ©ç”¨LLMçš„è¯­ä¹‰èƒ½åŠ›ï¼Œä½†ä¼šå¼•å…¥ä¿¡æ¯æŸå¤±ï¼Œä¸”ä¾èµ–äºé«˜è´¨é‡çš„è½¬æ¢è§„åˆ™ã€‚å› æ­¤ï¼Œå¦‚ä½•ç›´æ¥åˆ©ç”¨LLMçš„å¼ºå¤§åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œé¿å…è‡ªç„¶è¯­è¨€è½¬æ¢ï¼Œæˆä¸ºä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥å°†é¢„è®­ç»ƒçš„LLMåº”ç”¨äºè¿‡ç¨‹æ•°æ®ï¼Œæ— éœ€å°†å…¶è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€ã€‚LLMæœ¬è´¨ä¸Šæ“…é•¿ç”Ÿæˆtokenåºåˆ—ï¼Œè€Œè¿‡ç¨‹æ•°æ®ä¹Ÿå¯ä»¥è¢«è§†ä¸ºäº‹ä»¶åºåˆ—ã€‚é€šè¿‡é¢†åŸŸè‡ªé€‚åº”ï¼Œä½¿LLMèƒ½å¤Ÿç†è§£å’Œé¢„æµ‹è¿‡ç¨‹æ•°æ®çš„æ¼”å˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆParameter-Efficient Fine-Tuningï¼‰æŠ€æœ¯ï¼Œä»¥é™ä½è®¡ç®—æˆæœ¬ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š1) é€‰æ‹©ä¸€ä¸ªé¢„è®­ç»ƒçš„LLMä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚2) ä½¿ç”¨è¿‡ç¨‹æ•°æ®é›†å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”è¿‡ç¨‹æ•°æ®çš„ç‰¹æ€§ã€‚3) åœ¨é¢„æµ‹è¿‡ç¨‹ç›‘æ§ä»»åŠ¡ä¸­è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬å•ä»»åŠ¡å’Œå¤šä»»åŠ¡é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç›´æ¥å°†LLMåº”ç”¨äºè¿‡ç¨‹æ•°æ®ï¼Œé¿å…äº†è‡ªç„¶è¯­è¨€è½¬æ¢å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆåˆ©ç”¨LLMã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥ç ”ç©¶é‡ç‚¹å…³æ³¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œä¾‹å¦‚Adapteræˆ–LoRAç­‰ï¼Œä»¥å‡å°‘éœ€è¦è®­ç»ƒçš„å‚æ•°æ•°é‡ï¼Œä»è€Œé™ä½è®¡ç®—å¼€é”€å¹¶åŠ é€Ÿæ”¶æ•›ã€‚æŸå¤±å‡½æ•°æ ¹æ®é¢„æµ‹ä»»åŠ¡çš„ç±»å‹è¿›è¡Œé€‰æ‹©ï¼Œä¾‹å¦‚ï¼Œåˆ†ç±»ä»»åŠ¡ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼Œå›å½’ä»»åŠ¡ä½¿ç”¨å‡æ–¹è¯¯å·®æŸå¤±ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å–å†³äºæ‰€é€‰æ‹©çš„LLMï¼Œä½†é€šå¸¸ä¼šæ·»åŠ ä¸€ä¸ªæˆ–å¤šä¸ªçº¿æ€§å±‚æ¥å°†LLMçš„è¾“å‡ºæ˜ å°„åˆ°é¢„æµ‹ç›®æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹è¿‡ç¨‹ç›‘æ§ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨å¤šä»»åŠ¡åœºæ™¯ä¸‹ï¼Œä¼˜äºç°æœ‰çš„RNNæ–¹æ³•å’ŒåŸºäºå™è¿°å¼é£æ ¼çš„è§£å†³æ–¹æ¡ˆã€‚å¾®è°ƒåçš„æ¨¡å‹è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶ä¸”éœ€è¦æ˜¾è‘—æ›´å°‘çš„è¶…å‚æ•°ä¼˜åŒ–ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç›´æ¥å°†LLMåº”ç”¨äºè¿‡ç¨‹æ•°æ®å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è¿‡ç¨‹ç›‘æ§å’Œç®¡ç†é¢†åŸŸï¼Œä¾‹å¦‚ä¾›åº”é“¾ç®¡ç†ã€åŒ»ç–—æµç¨‹ä¼˜åŒ–ã€é‡‘èé£é™©æ§åˆ¶ç­‰ã€‚é€šè¿‡æ›´å‡†ç¡®åœ°é¢„æµ‹è¿‡ç¨‹äº‹ä»¶çš„å‘ç”Ÿï¼Œå¯ä»¥æå‰é‡‡å–å¹²é¢„æªæ–½ï¼Œæé«˜æ•ˆç‡ã€é™ä½æˆæœ¬ã€æ”¹å–„æœåŠ¡è´¨é‡ï¼Œå¹¶ä¸ºå†³ç­–æä¾›æ›´å¯é çš„ä¾æ®ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„è¿‡ç¨‹åœºæ™¯ï¼Œå¹¶ä¸å…¶ä»–æŠ€æœ¯ï¼ˆå¦‚å¼ºåŒ–å­¦ä¹ ï¼‰ç›¸ç»“åˆï¼Œå®ç°æ›´æ™ºèƒ½åŒ–çš„è¿‡ç¨‹ç®¡ç†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, Large Language Models (LLMs) have emerged as a prominent area of interest across various research domains, including Process Mining (PM). Current applications in PM have predominantly centered on prompt engineering strategies or the transformation of event logs into narrative-style datasets, thereby exploiting the semantic capabilities of LLMs to address diverse tasks. In contrast, this study investigates the direct adaptation of pretrained LLMs to process data without natural language reformulation, motivated by the fact that these models excel in generating sequences of tokens, similar to the objective in PM. More specifically, we focus on parameter-efficient fine-tuning techniques to mitigate the computational overhead typically associated with such models. Our experimental setup focuses on Predictive Process Monitoring (PPM), and considers both single- and multi-task predictions. The results demonstrate a potential improvement in predictive performance over state-of-the-art recurrent neural network (RNN) approaches and recent narrative-style-based solutions, particularly in the multi-task setting. Additionally, our fine-tuned models exhibit faster convergence and require significantly less hyperparameter optimization.

