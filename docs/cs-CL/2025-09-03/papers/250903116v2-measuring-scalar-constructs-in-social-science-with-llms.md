---
layout: default
title: Measuring Scalar Constructs in Social Science with LLMs
---

# Measuring Scalar Constructs in Social Science with LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03116" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03116v2</a>
  <a href="https://arxiv.org/pdf/2509.03116.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03116v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03116v2', 'Measuring Scalar Constructs in Social Science with LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hauke Licht, Rupak Sarkar, Patrick Y. Wu, Pranav Goel, Niklas Stoehr, Elliott Ash, Alexander Miserlis Hoyle

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03 (æ›´æ–°: 2025-09-22)

**å¤‡æ³¨**: Accepted to EMNLP 2025 (Main)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨LLMæµ‹é‡ç¤¾ä¼šç§‘å­¦ä¸­çš„æ ‡é‡ç»“æ„ï¼Œæå‡ºtokenæ¦‚ç‡åŠ æƒè¯„åˆ†æ–¹æ³•å¹¶éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ ‡é‡æµ‹é‡` `ç¤¾ä¼šç§‘å­¦` `tokenæ¦‚ç‡` `å¾®è°ƒ` `æ–‡æœ¬åˆ†æ` `æƒ…æ„Ÿåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æµ‹é‡è¯­è¨€çš„å¤æ‚æ€§æˆ–æƒ…æ„Ÿæ€§ç­‰æ ‡é‡ç»“æ„æ—¶å­˜åœ¨ä¸è¶³ï¼ŒLLMç›´æ¥è¾“å‡ºåˆ†æ•°å®¹æ˜“èšé›†åœ¨ä»»æ„æ•°å­—é™„è¿‘ã€‚
2. è®ºæ–‡æå‡ºtokenæ¦‚ç‡åŠ æƒé€ç‚¹è¯„åˆ†æ–¹æ³•ï¼Œé€šè¿‡å¯¹LLMè¾“å‡ºçš„tokenæ¦‚ç‡è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œæ›´å‡†ç¡®åœ°æµ‹é‡æ ‡é‡ç»“æ„ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œtokenæ¦‚ç‡åŠ æƒè¯„åˆ†ä¼˜äºç›´æ¥è¯„åˆ†å’Œæˆå¯¹æ¯”è¾ƒï¼Œä¸”å¾®è°ƒå°æ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°ç”šè‡³è¶…è¿‡prompt LLMçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šç”¨äºæè¿°è¯­è¨€çš„ç»“æ„ï¼Œå¦‚å¤æ‚æ€§æˆ–æƒ…æ„Ÿæ€§ï¼Œéƒ½å…·æœ‰å¤©ç„¶çš„è¿ç»­è¯­ä¹‰ç»“æ„ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯æµ‹é‡æ ‡é‡ç»“æ„çš„æœ‰æ•ˆå·¥å…·ï¼Œä½†å…¶å¯¹æ•°å€¼è¾“å‡ºçš„ç‰¹æ®Šå¤„ç†æ–¹å¼å¼•å‘äº†å¦‚ä½•æœ€ä½³åº”ç”¨å®ƒä»¬çš„é—®é¢˜ã€‚æœ¬æ–‡é’ˆå¯¹ç¤¾ä¼šç§‘å­¦ä¸­åŸºäºLLMçš„æ ‡é‡ç»“æ„æµ‹é‡æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ä½¿ç”¨æ¥è‡ªæ”¿æ²»å­¦æ–‡çŒ®çš„å¤šä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å››ç§æ–¹æ³•ï¼šæ— æƒé‡ç›´æ¥é€ç‚¹è¯„åˆ†ã€æˆå¯¹æ¯”è¾ƒèšåˆã€tokenæ¦‚ç‡åŠ æƒé€ç‚¹è¯„åˆ†å’Œå¾®è°ƒã€‚ç ”ç©¶å‘ç°ï¼ŒLLMè¿›è¡Œçš„æˆå¯¹æ¯”è¾ƒæ¯”ç›´æ¥æç¤ºLLMè¾“å‡ºåˆ†æ•°äº§ç”Ÿæ›´å¥½çš„æµ‹é‡ç»“æœï¼Œåè€…å®¹æ˜“èšé›†åœ¨ä»»æ„æ•°å­—é™„è¿‘ã€‚ç„¶è€Œï¼Œå¯¹åˆ†æ•°çš„tokenæ¦‚ç‡è¿›è¡ŒåŠ æƒå¹³å‡è¿›ä¸€æ­¥æ”¹è¿›äº†æµ‹é‡ç»“æœã€‚æœ€åï¼Œä½¿ç”¨å°‘è‡³1000ä¸ªè®­ç»ƒå¯¹å¾®è°ƒè¾ƒå°çš„æ¨¡å‹å¯ä»¥åŒ¹é…æˆ–è¶…è¿‡æç¤ºLLMçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¤¾ä¼šç§‘å­¦ä¸­å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ›´å‡†ç¡®åœ°æµ‹é‡æ ‡é‡ç»“æ„çš„é—®é¢˜ï¼Œä¾‹å¦‚æ–‡æœ¬çš„å¤æ‚æ€§æˆ–æƒ…æ„Ÿå€¾å‘ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ç›´æ¥æç¤ºLLMè¾“å‡ºåˆ†æ•°çš„æ–¹æ³•ï¼Œå­˜åœ¨ä¸€ä¸ªä¸»è¦ç—›ç‚¹ï¼šLLMå€¾å‘äºå°†è¾“å‡ºèšé›†åœ¨æŸäº›ç‰¹å®šçš„ã€ä»»æ„çš„æ•°å­—é™„è¿‘ï¼Œå¯¼è‡´æµ‹é‡ç»“æœçš„åå·®å’Œä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œä¸å…¶ç›´æ¥ä¾èµ–LLMè¾“å‡ºçš„æ•°å€¼ç»“æœï¼Œä¸å¦‚åˆ©ç”¨LLMè¾“å‡ºçš„tokenæ¦‚ç‡åˆ†å¸ƒï¼Œé€šè¿‡åŠ æƒå¹³å‡çš„æ–¹å¼æ¥æ›´ç²¾ç»†åœ°ä¼°è®¡æ ‡é‡å€¼ã€‚è¿™ç§æ–¹æ³•å‡è®¾LLMåœ¨ç”Ÿæˆä¸åŒtokenæ—¶ï¼Œå…¶æ¦‚ç‡åˆ†å¸ƒè•´å«äº†å…³äºæ ‡é‡ç»“æ„çš„æ›´ä¸°å¯Œä¿¡æ¯ï¼Œé€šè¿‡åˆç†åŠ æƒå¯ä»¥æå–è¿™äº›ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†é€šè¿‡å¾®è°ƒå°æ¨¡å‹æ¥æé«˜æµ‹é‡å‡†ç¡®æ€§çš„æ–¹æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®å‡†å¤‡ï¼šæ”¶é›†æ¥è‡ªæ”¿æ²»å­¦æ–‡çŒ®çš„å¤šä¸ªæ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†åŒ…å«éœ€è¦æµ‹é‡çš„æ ‡é‡ç»“æ„ï¼ˆå¦‚æ–‡æœ¬å¤æ‚æ€§ï¼‰ã€‚2) æ–¹æ³•è¯„ä¼°ï¼šè¯„ä¼°å››ç§ä¸åŒçš„LLMåº”ç”¨æ–¹æ³•ï¼ŒåŒ…æ‹¬æ— æƒé‡ç›´æ¥é€ç‚¹è¯„åˆ†ã€æˆå¯¹æ¯”è¾ƒèšåˆã€tokenæ¦‚ç‡åŠ æƒé€ç‚¹è¯„åˆ†å’Œå¾®è°ƒã€‚3) æ€§èƒ½æ¯”è¾ƒï¼šä½¿ç”¨é€‚å½“çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå…·ä½“æŒ‡æ ‡æœªçŸ¥ï¼‰æ¯”è¾ƒä¸åŒæ–¹æ³•çš„æµ‹é‡å‡†ç¡®æ€§ã€‚4) æ¨¡å‹å¾®è°ƒï¼šä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®ï¼ˆ1000ä¸ªè®­ç»ƒå¯¹ï¼‰å¾®è°ƒè¾ƒå°çš„æ¨¡å‹ï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†tokenæ¦‚ç‡åŠ æƒé€ç‚¹è¯„åˆ†æ–¹æ³•ã€‚ä¸ç›´æ¥è¯„åˆ†ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†LLMè¾“å‡ºçš„tokenæ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œèƒ½å¤Ÿæ›´ç²¾ç»†åœ°æ•æ‰æ ‡é‡ç»“æ„çš„ç»†å¾®å˜åŒ–ã€‚ä¸æˆå¯¹æ¯”è¾ƒç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é¿å…äº†å¤§é‡çš„æˆå¯¹æ¯”è¾ƒè®¡ç®—ï¼Œæé«˜äº†æ•ˆç‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¾®è°ƒå°æ¨¡å‹ï¼Œè®ºæ–‡å±•ç¤ºäº†åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œä»ç„¶å¯ä»¥è·å¾—ä¸å¤§å‹LLMç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) tokenæ¦‚ç‡åŠ æƒçš„å…·ä½“æ–¹å¼ï¼ˆåŠ æƒå‡½æ•°æœªçŸ¥ï¼‰ã€‚2) æˆå¯¹æ¯”è¾ƒçš„å…·ä½“å®ç°æ–¹å¼ï¼ˆæ¯”è¾ƒç­–ç•¥æœªçŸ¥ï¼‰ã€‚3) å¾®è°ƒæ¨¡å‹çš„å…·ä½“æ¶æ„å’Œè®­ç»ƒå‚æ•°ï¼ˆæ¨¡å‹ç»“æ„ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ç­‰æœªçŸ¥ï¼‰ã€‚4) è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ï¼ˆå…·ä½“æŒ‡æ ‡æœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œtokenæ¦‚ç‡åŠ æƒé€ç‚¹è¯„åˆ†æ–¹æ³•ä¼˜äºç›´æ¥è¯„åˆ†å’Œæˆå¯¹æ¯”è¾ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æµ‹é‡æ ‡é‡ç»“æ„ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®ï¼ˆ1000ä¸ªè®­ç»ƒå¯¹ï¼‰å¾®è°ƒè¾ƒå°çš„æ¨¡å‹å¯ä»¥åŒ¹é…æˆ–è¶…è¿‡æç¤ºLLMçš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œä»ç„¶å¯ä»¥é€šè¿‡å¾®è°ƒè·å¾—è‰¯å¥½çš„æµ‹é‡æ•ˆæœã€‚ï¼ˆå…·ä½“æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªçŸ¥ï¼‰

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºç¤¾ä¼šç§‘å­¦é¢†åŸŸï¼Œä¾‹å¦‚æ”¿æ²»å­¦ã€ç¤¾ä¼šå­¦ã€ä¼ æ’­å­¦ç­‰ã€‚å®ƒå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å‡†ç¡®åœ°æµ‹é‡æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ã€å¤æ‚æ€§ã€å¯è¯»æ€§ç­‰æ ‡é‡ç‰¹å¾ï¼Œä»è€Œè¿›è¡Œæ›´æ·±å…¥çš„ç¤¾ä¼šç°è±¡åˆ†æã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºèˆ†æƒ…ç›‘æ§ã€å†…å®¹å®¡æ ¸ã€æ™ºèƒ½æ¨èç­‰é¢†åŸŸï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæœªæ¥å‘å±•æ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Many constructs that characterize language, like its complexity or emotionality, have a naturally continuous semantic structure; a public speech is not just "simple" or "complex," but exists on a continuum between extremes. Although large language models (LLMs) are an attractive tool for measuring scalar constructs, their idiosyncratic treatment of numerical outputs raises questions of how to best apply them. We address these questions with a comprehensive evaluation of LLM-based approaches to scalar construct measurement in social science. Using multiple datasets sourced from the political science literature, we evaluate four approaches: unweighted direct pointwise scoring, aggregation of pairwise comparisons, token-probability-weighted pointwise scoring, and finetuning. Our study finds that pairwise comparisons made by LLMs produce better measurements than simply prompting the LLM to directly output the scores, which suffers from bunching around arbitrary numbers. However, taking the weighted mean over the token probability of scores further improves the measurements over the two previous approaches. Finally, finetuning smaller models with as few as 1,000 training pairs can match or exceed the performance of prompted LLMs.

