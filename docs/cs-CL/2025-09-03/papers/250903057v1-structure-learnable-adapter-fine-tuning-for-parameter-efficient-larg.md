---
layout: default
title: Structure-Learnable Adapter Fine-Tuning for Parameter-Efficient Large Language Models
---

# Structure-Learnable Adapter Fine-Tuning for Parameter-Efficient Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03057" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03057v1</a>
  <a href="https://arxiv.org/pdf/2509.03057.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03057v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03057v1', 'Structure-Learnable Adapter Fine-Tuning for Parameter-Efficient Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ming Gong, Yingnan Deng, Nia Qi, Yujun Zou, Zhihao Xue, Yun Zi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“æ„å¯å­¦ä¹ çš„Adapterå¾®è°ƒæ–¹æ³•ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹å‚æ•°æ•ˆç‡å’Œä»»åŠ¡é€‚åº”æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§è¯­è¨€æ¨¡å‹` `Adapter` `ç»“æ„å­¦ä¹ ` `è‡ªç„¶è¯­è¨€ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ–¹æ³•å­˜åœ¨å‚æ•°å†—ä½™ã€ç»“æ„å›ºå®šã€ä»»åŠ¡é€‚åº”æ€§å·®ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. æå‡ºä¸€ç§ç»“æ„å¯å­¦ä¹ çš„Adapterå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨æœç´¢å’Œä¼˜åŒ–Adapterç»“æ„ï¼Œå®ç°ä»»åŠ¡ç‰¹å®šçš„é«˜æ•ˆå­ç»“æ„ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œæå‡äº†æ¨¡å‹æ€§èƒ½å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­å­˜åœ¨çš„å‚æ•°å†—ä½™ã€ç»“æ„åƒµåŒ–å’Œä»»åŠ¡é€‚åº”æ€§æœ‰é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç»“æ„å¯å­¦ä¹ æœºåˆ¶çš„Adapterå¾®è°ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¯å¾®åˆ†çš„é—¨æ§å‡½æ•°å’Œç»“æ„ç¨€ç–æ€§æ§åˆ¶å˜é‡ï¼Œå®ç°äº†Adapteræ’å…¥ç‚¹ã€æ¿€æ´»è·¯å¾„å’Œæ¨¡å—ç»„åˆçš„è‡ªåŠ¨ä¼˜åŒ–ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä»»åŠ¡è®¾ç½®ä¸­çµæ´»è°ƒæ•´å…¶ç»“æ„ï¼Œä»¥åŒ¹é…ä¸åŒçš„ä»»åŠ¡ç‰¹å¾ã€‚åœ¨å†»ç»“éª¨å¹²å‚æ•°çš„åŒæ—¶ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç»“æ„æœç´¢æœºåˆ¶æ¥æŒ‡å¯¼è®­ç»ƒæœŸé—´ç‰¹å®šä»»åŠ¡çš„é«˜æ•ˆå­ç»“æ„çš„åŠ¨æ€æ„å»ºï¼Œä»è€Œæ˜¾è‘—æé«˜å‚æ•°åˆ©ç”¨ç‡å’Œè¡¨å¾èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç»„æ•æ„Ÿæ€§åˆ†æå®éªŒï¼Œä»¥ç³»ç»Ÿåœ°è¯„ä¼°ç¨€ç–æƒé‡ã€å™ªå£°æ³¨å…¥ç‡å’Œæ•°æ®æ‰°åŠ¨å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚è¿™äº›å®éªŒéªŒè¯äº†æ‰€æå‡ºçš„æ–¹æ³•åœ¨å„ç§å¤šä»»åŠ¡è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸­çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¼˜äºä¸»æµçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œå¹¶åœ¨å‡†ç¡®æ€§ã€å‹ç¼©ç‡ä»¥åŠå¯¹å™ªå£°å’Œæ‰°åŠ¨çš„é²æ£’æ€§ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œå¦‚å…¨å‚æ•°å¾®è°ƒï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å®¹æ˜“è¿‡æ‹Ÿåˆã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚Adapterï¼‰è™½ç„¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½†é€šå¸¸é‡‡ç”¨å›ºå®šçš„ç»“æ„ï¼Œæ— æ³•é’ˆå¯¹ä¸åŒä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œå¯¼è‡´å‚æ•°åˆ©ç”¨ç‡ä¸é«˜ï¼Œä»»åŠ¡é€‚åº”æ€§å—é™ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ä»»åŠ¡ç‰¹å®šç»“æ„çš„å¾®è°ƒæ–¹æ³•ï¼Œä»¥æé«˜å‚æ•°æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ç»“æ„å¯å­¦ä¹ æœºåˆ¶ï¼Œä½¿Adapterèƒ½å¤Ÿæ ¹æ®ä¸åŒä»»åŠ¡çš„ç‰¹ç‚¹è‡ªåŠ¨è°ƒæ•´å…¶ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å¼•å…¥å¯å¾®åˆ†çš„é—¨æ§å‡½æ•°å’Œç»“æ„ç¨€ç–æ€§æ§åˆ¶å˜é‡ï¼Œå®ç°Adapteræ’å…¥ä½ç½®ã€æ¿€æ´»è·¯å¾„å’Œæ¨¡å—ç»„åˆçš„è‡ªåŠ¨ä¼˜åŒ–ã€‚è¿™ç§åŠ¨æ€ç»“æ„è°ƒæ•´ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ„å»ºé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é«˜æ•ˆå­ç»“æ„ï¼Œä»è€Œæé«˜å‚æ•°åˆ©ç”¨ç‡å’Œè¡¨å¾èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŸºäºAdapterçš„å¾®è°ƒæ¡†æ¶ï¼Œå¹¶åœ¨å…¶ä¸­å¼•å…¥äº†ç»“æ„å­¦ä¹ æ¨¡å—ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œåœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„Transformerå±‚ä¸­æ’å…¥Adapteræ¨¡å—ã€‚ç„¶åï¼Œå¼•å…¥å¯å¾®åˆ†çš„é—¨æ§å‡½æ•°å’Œç»“æ„ç¨€ç–æ€§æ§åˆ¶å˜é‡ï¼Œç”¨äºæ§åˆ¶Adapteræ¨¡å—çš„æ¿€æ´»å’Œè¿æ¥ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ç»“æ„æœç´¢æœºåˆ¶ï¼Œè‡ªåŠ¨ä¼˜åŒ–Adapterçš„ç»“æ„ï¼Œä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚æœ€åï¼Œä½¿ç”¨ä¼˜åŒ–åçš„Adapterç»“æ„è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†ç»“æ„å¯å­¦ä¹ æœºåˆ¶ï¼Œä½¿å¾—Adapterèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„ç»“æ„ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šç»“æ„Adapterç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å‚æ•°ï¼Œå¹¶æé«˜æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜è®¾è®¡äº†ä¸€å¥—æ•æ„Ÿæ€§åˆ†æå®éªŒï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒå™ªå£°å’Œæ‰°åŠ¨ä¸‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Gumbel-SoftmaxæŠ€å·§å®ç°å¯å¾®åˆ†çš„é—¨æ§å‡½æ•°ï¼Œç”¨äºæ§åˆ¶Adapteræ¨¡å—çš„æ¿€æ´»ï¼›2) å¼•å…¥L0æ­£åˆ™åŒ–é¡¹ä½œä¸ºç»“æ„ç¨€ç–æ€§æ§åˆ¶å˜é‡ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ ç¨€ç–çš„Adapterç»“æ„ï¼›3) è®¾è®¡ç»“æ„æœç´¢æœºåˆ¶ï¼Œé€šè¿‡ä¼˜åŒ–é—¨æ§å‡½æ•°å’Œç¨€ç–æ€§æ§åˆ¶å˜é‡ï¼Œè‡ªåŠ¨æœç´¢æœ€ä¼˜çš„Adapterç»“æ„ï¼›4) è®¾è®¡æ•æ„Ÿæ€§åˆ†æå®éªŒï¼Œè¯„ä¼°ç¨€ç–æƒé‡ã€å™ªå£°æ³¨å…¥ç‡å’Œæ•°æ®æ‰°åŠ¨å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šä¼˜äºä¸»æµçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ã€‚ä¾‹å¦‚ï¼Œåœ¨GLUEåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè¾ƒé«˜å‹ç¼©ç‡çš„åŒæ—¶ï¼Œå–å¾—äº†ä¸å…¨å‚æ•°å¾®è°ƒç›¸è¿‘çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ•æ„Ÿæ€§åˆ†æå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸åŒå™ªå£°å’Œæ‰°åŠ¨ä¸‹çš„é²æ£’æ€§ï¼Œè¡¨æ˜å…¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸‹ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡ç»“æ„å¯å­¦ä¹ çš„Adapterå¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—é™ä½æ¨¡å‹å‚æ•°é‡ï¼Œæé«˜éƒ¨ç½²æ•ˆç‡ï¼Œå¹¶æå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå¤šä»»åŠ¡å­¦ä¹ ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ¨¡å‹ç»“æ„ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper addresses the issues of parameter redundancy, rigid structure, and limited task adaptability in the fine-tuning of large language models. It proposes an adapter-based fine-tuning method built on a structure-learnable mechanism. By introducing differentiable gating functions and structural sparsity control variables, the method enables automatic optimization of adapter insertion points, activation paths, and module combinations. This allows the model to adjust its structure flexibly in multi-task settings to match different task characteristics. With the backbone parameters kept frozen, the method uses a structure search mechanism to guide the dynamic construction of task-specific efficient substructures during training. This significantly improves parameter utilization and representational capacity. In addition, the paper designs a set of sensitivity analysis experiments to systematically evaluate the effects of sparsity weight, noise injection ratio, and data perturbation on model performance. These experiments verify the stability and robustness of the proposed method across various multi-task natural language understanding tasks. The experimental results show that the proposed method outperforms mainstream parameter-efficient tuning techniques on multiple tasks. It achieves a better balance among accuracy, compression rate, and robustness to noise and perturbation.

