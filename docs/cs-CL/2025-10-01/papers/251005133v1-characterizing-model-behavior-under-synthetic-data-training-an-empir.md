---
layout: default
title: Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios
---

# Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.05133" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.05133v1</a>
  <a href="https://arxiv.org/pdf/2510.05133.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.05133v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.05133v1', 'Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Y. Du, G. Wu, G. Tang, W. Wang, Q. Fan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**å¤‡æ³¨**: 17 pages. Technical report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶åˆæˆæ•°æ®æ¯”ä¾‹å¯¹ä¸åŒè§„æ¨¡NLPæ¨¡å‹è¡Œä¸ºçš„å½±å“ï¼Œä¸ºå®é™…åº”ç”¨æä¾›æŒ‡å¯¼ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆæˆæ•°æ®` `æ¨¡å‹è®­ç»ƒ` `æ•°æ®å¢å¼º` `æ¨¡å‹æ ¡å‡†` `NLP` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹è§„æ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹åˆæˆæ•°æ®æ¯”ä¾‹å¦‚ä½•å½±å“ä¸åŒè§„æ¨¡æ¨¡å‹è¡Œä¸ºçš„ç³»ç»Ÿæ€§ç†è§£ï¼Œé˜»ç¢äº†åˆæˆæ•°æ®åœ¨NLPè®­ç»ƒä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚
2. é€šè¿‡æ§åˆ¶åˆæˆæ•°æ®æ¯”ä¾‹ï¼Œåœ¨ä¸åŒè§„æ¨¡çš„Pythiaæ¨¡å‹ä¸Šè¿›è¡Œå®éªŒï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€æ ¡å‡†å’Œè¾“å‡ºç‰¹å¾ï¼Œæ­ç¤ºåˆæˆæ•°æ®æ¯”ä¾‹ä¸æ¨¡å‹è¡Œä¸ºä¹‹é—´çš„å…³ç³»ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹åœ¨ä¸€å®šæ¯”ä¾‹çš„åˆæˆæ•°æ®ä¸‹èƒ½ä¿æŒæ€§èƒ½ï¼Œä½†è¶…è¿‡é˜ˆå€¼åæ€§èƒ½ä¸‹é™ï¼Œä¸”æ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡ç±»å‹ä¼šå½±å“æ¨¡å‹å¯¹åˆæˆæ•°æ®çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®åœ¨ç°ä»£NLPè®­ç»ƒæµç¨‹ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚é€šè¿‡æ§åˆ¶åˆæˆæ•°æ®ä¸å¤–éƒ¨æ•°æ®çš„æ¯”ä¾‹ï¼Œå¹¶ä½¿ç”¨Pythiaæ¨¡å‹å¥—ä»¶ï¼ˆ410M-12Bå‚æ•°ï¼‰åœ¨äº”ä¸ªä¸åŒçš„ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶äº†æ¨¡å‹åœ¨ä¸åŒæ¯”ä¾‹åˆæˆæ•°æ®è®­ç»ƒä¸‹çš„æ€§èƒ½ã€æ ¡å‡†å’Œè¾“å‡ºç‰¹å¾ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨åˆæˆæ•°æ®æ¯”ä¾‹é«˜è¾¾20%æ—¶ä»èƒ½ä¿æŒç¨³å®šæ€§èƒ½ï¼Œä½†è¶…è¿‡30%åæ€§èƒ½ä¸‹é™åŠ é€Ÿï¼›è¾ƒå¤§æ¨¡å‹ï¼ˆ6.9B-12Bï¼‰æ¯”å°å‹æ¨¡å‹ï¼ˆ410M-1.4Bï¼‰å¯¹åˆæˆæ•°æ®æ›´å…·é²æ£’æ€§ï¼›æ ¡å‡†é€€åŒ–å…ˆäºå‡†ç¡®ç‡æŸå¤±ï¼Œå¯ä½œä¸ºæ—©æœŸé¢„è­¦ä¿¡å·ï¼›ä»»åŠ¡ç‰¹æ€§ä¹Ÿå¾ˆé‡è¦ï¼Œæ¨ç†ä»»åŠ¡åœ¨åˆæˆæ•°æ®è®­ç»ƒä¸‹æ¯”æ£€ç´¢ä»»åŠ¡é€€åŒ–æ›´å¿«ã€‚ç ”ç©¶ç»“æœä¸ºå®é™…åº”ç”¨ä¸­åˆæˆæ•°æ®çš„ä½¿ç”¨æä¾›äº†æŒ‡å¯¼ï¼Œå¹¶ä¸Shumailovç­‰äººçš„æ¨¡å‹å´©æºƒç ”ç©¶è¿›è¡Œäº†æ¯”è¾ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç ”ç©¶åœ¨NLPæ¨¡å‹è®­ç»ƒä¸­ä½¿ç”¨åˆæˆæ•°æ®æ—¶ï¼Œåˆæˆæ•°æ®ä¸å¤–éƒ¨æ•°æ®æ¯”ä¾‹å¯¹æ¨¡å‹æ€§èƒ½ã€æ ¡å‡†å’Œè¾“å‡ºç‰¹å¾çš„å½±å“ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹è¿™ä¸€æ¯”ä¾‹çš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­éš¾ä»¥ç¡®å®šåˆé€‚çš„åˆæˆæ•°æ®é¢„ç®—ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ç”šè‡³å´©æºƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ§åˆ¶åˆæˆæ•°æ®ä¸å¤–éƒ¨æ•°æ®çš„æ¯”ä¾‹ï¼Œå¹¶åœ¨ä¸åŒè§„æ¨¡çš„NLPæ¨¡å‹ä¸Šè¿›è¡Œå®éªŒï¼Œè§‚å¯Ÿæ¨¡å‹åœ¨ä¸åŒæ¯”ä¾‹ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æ­ç¤ºåˆæˆæ•°æ®æ¯”ä¾‹ä¸æ¨¡å‹è¡Œä¸ºä¹‹é—´çš„å…³ç³»ï¼Œä¸ºå®é™…åº”ç”¨æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨å®è¯ç ”ç©¶çš„æ–¹æ³•ï¼Œä½¿ç”¨Pythiaæ¨¡å‹å¥—ä»¶ï¼ˆ410M-12Bå‚æ•°ï¼‰åœ¨äº”ä¸ªä¸åŒçš„NLPä»»åŠ¡ä¸Šè¿›è¡Œå®éªŒã€‚å®éªŒæµç¨‹åŒ…æ‹¬ï¼š1ï¼‰å‡†å¤‡å¤–éƒ¨æ•°æ®å’Œåˆæˆæ•°æ®ï¼›2ï¼‰ä»¥ä¸åŒçš„åˆæˆæ•°æ®æ¯”ä¾‹è®­ç»ƒæ¨¡å‹ï¼›3ï¼‰è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€æ ¡å‡†å’Œè¾“å‡ºç‰¹å¾ï¼›4ï¼‰åˆ†æå®éªŒç»“æœï¼Œå¾—å‡ºç»“è®ºã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹åˆæˆæ•°æ®æ¯”ä¾‹ä¸æ¨¡å‹è¡Œä¸ºä¹‹é—´çš„å…³ç³»è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„ç ”ç©¶ã€‚é€šè¿‡å®éªŒï¼Œè®ºæ–‡æ­ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒæ¯”ä¾‹çš„åˆæˆæ•°æ®ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶å‘ç°äº†æ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡ç±»å‹å¯¹æ¨¡å‹é²æ£’æ€§çš„å½±å“ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å‘ç°æ ¡å‡†é€€åŒ–å…ˆäºå‡†ç¡®ç‡æŸå¤±ï¼Œå¯ä½œä¸ºæ—©æœŸé¢„è­¦ä¿¡å·ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰é€‰æ‹©Pythiaæ¨¡å‹å¥—ä»¶ä½œä¸ºå®éªŒå¯¹è±¡ï¼Œæ¶µç›–ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼›2ï¼‰é€‰æ‹©äº”ä¸ªä¸åŒçš„NLPä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¨ç†å’Œæ£€ç´¢ä»»åŠ¡ï¼›3ï¼‰æ§åˆ¶åˆæˆæ•°æ®æ¯”ä¾‹åœ¨0-50%ä¹‹é—´ï¼›4ï¼‰ä½¿ç”¨æ ‡å‡†è¯„ä¼°æŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¦‚å‡†ç¡®ç‡å’Œæ ¡å‡†è¯¯å·®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨åˆæˆæ•°æ®æ¯”ä¾‹é«˜è¾¾20%æ—¶ä»èƒ½ä¿æŒç¨³å®šæ€§èƒ½ï¼Œä½†è¶…è¿‡30%åæ€§èƒ½ä¸‹é™åŠ é€Ÿã€‚è¾ƒå¤§æ¨¡å‹ï¼ˆ6.9B-12Bï¼‰æ¯”å°å‹æ¨¡å‹ï¼ˆ410M-1.4Bï¼‰å¯¹åˆæˆæ•°æ®æ›´å…·é²æ£’æ€§ã€‚æ ¡å‡†é€€åŒ–å…ˆäºå‡†ç¡®ç‡æŸå¤±ï¼Œå¯ä½œä¸ºæ—©æœŸé¢„è­¦ä¿¡å·ã€‚æ¨ç†ä»»åŠ¡åœ¨åˆæˆæ•°æ®è®­ç»ƒä¸‹æ¯”æ£€ç´¢ä»»åŠ¡é€€åŒ–æ›´å¿«ã€‚è¿™äº›å‘ç°ä¸ºå®é™…åº”ç”¨ä¸­åˆæˆæ•°æ®çš„ä½¿ç”¨æä¾›äº†é‡åŒ–æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§NLPä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æŒ‡ä»¤è·Ÿéšã€æ¨ç†å’Œé—®ç­”ç­‰ã€‚é€šè¿‡äº†è§£åˆæˆæ•°æ®æ¯”ä¾‹å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¼€å‘è€…å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨åˆæˆæ•°æ®æ¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œé™ä½æ•°æ®æ ‡æ³¨æˆæœ¬ï¼Œå¹¶ä¸ºç‰¹å®šé¢†åŸŸå®šåˆ¶æ¨¡å‹ã€‚ç ”ç©¶ç»“æœä¸ºåˆæˆæ•°æ®çš„ä½¿ç”¨æä¾›äº†å®è·µæŒ‡å¯¼ï¼Œæœ‰åŠ©äºæå‡NLPæ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Synthetic data generated by large language models has become integral to modern NLP training pipelines, from bootstrapping reasoning capabilities to augmenting instruction-following datasets. While recent work demonstrates successful applications maintaining high external data ratios, systematic understanding of how synthetic data proportion affects model behavior across different scales remains limited. This paper presents a controlled empirical study examining model performance, calibration, and output characteristics when trained on varying synthetic-to-external data ratios. Using the Pythia model suite (410M-12B parameters) across five diverse tasks, we evaluate models after one to three training iterations with synthetic data proportions ranging from 0-50\%. Our key findings include: models maintain stable performance with up to 20\% synthetic data, but degradation accelerates beyond 30\%; larger models (6.9B-12B) show greater robustness to synthetic data than smaller models (410M-1.4B); calibration degradation precedes accuracy loss, providing an early warning signal; and task characteristics matter, with reasoning tasks degrading faster than retrieval tasks under synthetic data training. Importantly, we find that current best practices, such as those employed in STaR and Self-Instruct systems that maintain greater than 80\% external data, operate well within safe regimes identified by our experiments. We provide practical guidance for practitioners on synthetic data budgets based on model scale and task requirements, alongside detailed comparison with concurrent work including Shumailov et al.'s model collapse findings.

