---
layout: default
title: KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning
---

# KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02392" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02392v2</a>
  <a href="https://arxiv.org/pdf/2510.02392.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02392v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02392v2', 'KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yinyi Luo, Zhexian Zhou, Hao Chen, Kai Qiu, Marios Savvides, Sharon Li, Jindong Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01 (æ›´æ–°: 2025-10-14)

**å¤‡æ³¨**: Technical report

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/AIFrontierLab/KnowledgeSmith.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**KnowledgeSmithï¼šé€šè¿‡æ¨¡å‹ç¼–è¾‘ä¸é—å¿˜æ­ç¤ºLLMä¸­çš„çŸ¥è¯†æ›´æ–°æœºåˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†ç¼–è¾‘` `æœºå™¨é—å¿˜` `çŸ¥è¯†æ›´æ–°` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†ç¼–è¾‘å’Œæœºå™¨é—å¿˜æ–¹æ³•ç¼ºä¹ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œéš¾ä»¥å……åˆ†ç†è§£LLMçš„çŸ¥è¯†æ›´æ–°æœºåˆ¶ï¼Œé˜»ç¢äº†æ¨¡å‹ä¼˜åŒ–ã€‚
2. KnowledgeSmithæ¡†æ¶å°†çŸ¥è¯†ç¼–è¾‘å’Œé—å¿˜ç»Ÿä¸€ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆå¤šå°ºåº¦ç»“æ„åŒ–å¹²é¢„æ•°æ®é›†ï¼Œå®ç°å¯æ§ç ”ç©¶ã€‚
3. å®éªŒæ­ç¤ºäº†LLMåœ¨çŸ¥è¯†æ›´æ–°æ–¹é¢ä¸äººç±»çš„å·®å¼‚ï¼Œä»¥åŠä¸€è‡´æ€§ä¸å®¹é‡ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºæ¨¡å‹è®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†ç¼–è¾‘å’Œæœºå™¨é—å¿˜æ˜¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä¿æŒæœ€æ–°çŠ¶æ€çš„ä¸¤ç§å¸¸ç”¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºè¯„ä¼°çš„ä¸å……åˆ†ã€å­¤ç«‹å’Œå°è§„æ¨¡ï¼ŒLLMçš„çŸ¥è¯†æ›´æ–°æœºåˆ¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«æ¢ç´¢ã€‚ä¾‹å¦‚ï¼ŒLLMåœ¨ä¿®æ”¹ç‰¹å®šçŸ¥è¯†æ–¹é¢æ˜¯å¦ä¸äººç±»ç›¸ä¼¼ï¼Ÿéšç€è®­ç»ƒæ•°æ®å¢åŠ ï¼Œç¼–è¾‘å’Œé—å¿˜æœ‰ä½•ä¸åŒï¼Ÿæœ¬æ–‡æå‡ºäº†KnowledgeSmithï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿåœ°ç†è§£LLMçš„æ›´æ–°æœºåˆ¶ã€‚æˆ‘ä»¬é¦–å…ˆå°†ç¼–è¾‘å’Œé—å¿˜è§†ä¸ºä¸€ä¸ªçº¦æŸä¼˜åŒ–é—®é¢˜çš„å®ä¾‹ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®é›†ç”Ÿæˆå™¨ï¼Œè¯¥ç”Ÿæˆå™¨æä¾›è·¨å¤šä¸ªå›¾çº§åˆ«å’Œæ•°æ®è§„æ¨¡çš„ç»“æ„åŒ–å¹²é¢„ï¼Œä»è€Œèƒ½å¤Ÿå¯¹ä¸åŒçš„ä¿®æ”¹ç­–ç•¥å¦‚ä½•é€šè¿‡æ¨¡å‹çŸ¥è¯†ä¼ æ’­è¿›è¡Œå—æ§ç ”ç©¶ã€‚å¤§é‡çš„å®éªŒè¯æ˜äº†çŸ¥è¯†ä¼ æ’­ã€å¯å¡‘æ€§ç¼©æ”¾ã€ä¸€è‡´æ€§å’Œé²æ£’æ€§çš„ç»†å¾®è§è§£ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLMå¯¹äºä¸åŒçº§åˆ«çš„çŸ¥è¯†å¹¶æ²¡æœ‰è¡¨ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„æ›´æ–°ï¼Œå¹¶ä¸”å­˜åœ¨ä¸€è‡´æ€§-å®¹é‡çš„æƒè¡¡ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å‘ç°èƒ½ä¸ºæ›´å¯é å’Œå¯æ‰©å±•çš„ç­–ç•¥è®¾è®¡æä¾›å»ºè®®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çŸ¥è¯†æ›´æ–°æ–¹æ³•ï¼Œå¦‚çŸ¥è¯†ç¼–è¾‘å’Œæœºå™¨é—å¿˜ï¼Œç¼ºä¹ç³»ç»Ÿæ€§çš„è¯„ä¼°å’Œæ¯”è¾ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸æ¸…æ¥šLLMå¦‚ä½•ä¿®æ”¹ä¸åŒç±»å‹çš„çŸ¥è¯†ï¼Œä»¥åŠç¼–è¾‘å’Œé—å¿˜åœ¨ä¸åŒæ•°æ®è§„æ¨¡ä¸‹çš„è¡¨ç°å·®å¼‚ã€‚è¿™é˜»ç¢äº†æˆ‘ä»¬å¯¹LLMçŸ¥è¯†æ›´æ–°æœºåˆ¶çš„æ·±å…¥ç†è§£ï¼Œä¹Ÿé™åˆ¶äº†æˆ‘ä»¬è®¾è®¡æ›´å¯é å’Œå¯æ‰©å±•çš„æ›´æ–°ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šKnowledgeSmithçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†çŸ¥è¯†ç¼–è¾‘å’Œæœºå™¨é—å¿˜ç»Ÿä¸€åˆ°ä¸€ä¸ªçº¦æŸä¼˜åŒ–é—®é¢˜çš„æ¡†æ¶ä¸‹è¿›è¡Œç ”ç©¶ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªè‡ªåŠ¨æ•°æ®é›†ç”Ÿæˆå™¨ï¼Œå¯ä»¥ç³»ç»Ÿåœ°ç”Ÿæˆå…·æœ‰ä¸åŒå›¾ç»“æ„å’Œæ•°æ®è§„æ¨¡çš„å¹²é¢„æ•°æ®ï¼Œä»è€Œå®ç°å¯¹LLMçŸ¥è¯†æ›´æ–°è¿‡ç¨‹çš„å¯æ§ç ”ç©¶ã€‚è¿™ç§æ–¹æ³•å…è®¸ç ”ç©¶è€…è§‚å¯Ÿä¸åŒä¿®æ”¹ç­–ç•¥å¦‚ä½•å½±å“æ¨¡å‹çŸ¥è¯†çš„ä¼ æ’­ã€å¯å¡‘æ€§ã€ä¸€è‡´æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKnowledgeSmithæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) **çŸ¥è¯†è¡¨ç¤ºæ¨¡å—**ï¼šå°†çŸ¥è¯†è¡¨ç¤ºä¸ºå›¾ç»“æ„ï¼Œå…è®¸å®šä¹‰ä¸åŒå±‚æ¬¡çš„çŸ¥è¯†å•å…ƒã€‚2) **æ•°æ®é›†ç”Ÿæˆæ¨¡å—**ï¼šè‡ªåŠ¨ç”ŸæˆåŒ…å«ç»“æ„åŒ–å¹²é¢„çš„æ•°æ®é›†ï¼Œå¯ä»¥æ§åˆ¶å›¾çš„è§„æ¨¡å’Œå¹²é¢„çš„å¼ºåº¦ã€‚3) **æ¨¡å‹ç¼–è¾‘/é—å¿˜æ¨¡å—**ï¼šåº”ç”¨ä¸åŒçš„çŸ¥è¯†ç¼–è¾‘æˆ–æœºå™¨é—å¿˜ç®—æ³•æ¥ä¿®æ”¹LLMçš„çŸ¥è¯†ã€‚4) **è¯„ä¼°æ¨¡å—**ï¼šè¯„ä¼°ä¿®æ”¹åçš„æ¨¡å‹åœ¨çŸ¥è¯†ä¼ æ’­ã€å¯å¡‘æ€§ã€ä¸€è‡´æ€§å’Œé²æ£’æ€§ç­‰æ–¹é¢çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šKnowledgeSmithçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„æ¡†æ¶å’Œè‡ªåŠ¨æ•°æ®é›†ç”Ÿæˆå™¨ã€‚å®ƒå°†çŸ¥è¯†ç¼–è¾‘å’Œæœºå™¨é—å¿˜è§†ä¸ºåŒä¸€é—®é¢˜çš„ä¸¤ä¸ªæ–¹é¢ï¼Œå¹¶æä¾›äº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•æ¥ç ”ç©¶å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚è‡ªåŠ¨æ•°æ®é›†ç”Ÿæˆå™¨å…è®¸ç ”ç©¶è€…åœ¨å—æ§çš„ç¯å¢ƒä¸­ç ”ç©¶ä¸åŒä¿®æ”¹ç­–ç•¥çš„å½±å“ï¼Œè€Œæ— éœ€æ‰‹åŠ¨åˆ›å»ºå¤§é‡çš„æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šKnowledgeSmithçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **å›¾ç»“æ„çŸ¥è¯†è¡¨ç¤º**ï¼šä½¿ç”¨å›¾ç»“æ„æ¥è¡¨ç¤ºçŸ¥è¯†ï¼Œå…è®¸å®šä¹‰ä¸åŒå±‚æ¬¡çš„çŸ¥è¯†å•å…ƒï¼Œå¹¶ç ”ç©¶å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚2) **è‡ªåŠ¨æ•°æ®é›†ç”Ÿæˆ**ï¼šè®¾è®¡äº†ä¸€ç§ç®—æ³•ï¼Œå¯ä»¥è‡ªåŠ¨ç”ŸæˆåŒ…å«ç»“æ„åŒ–å¹²é¢„çš„æ•°æ®é›†ï¼Œå¯ä»¥æ§åˆ¶å›¾çš„è§„æ¨¡å’Œå¹²é¢„çš„å¼ºåº¦ã€‚3) **å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡**ï¼šä½¿ç”¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°ä¿®æ”¹åçš„æ¨¡å‹åœ¨çŸ¥è¯†ä¼ æ’­ã€å¯å¡‘æ€§ã€ä¸€è‡´æ€§å’Œé²æ£’æ€§ç­‰æ–¹é¢çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMåœ¨çŸ¥è¯†æ›´æ–°æ–¹é¢ä¸äººç±»å­˜åœ¨å·®å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒå±‚çº§çš„çŸ¥è¯†æ›´æ–°ä¸Šã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°LLMçš„çŸ¥è¯†æ›´æ–°å­˜åœ¨ä¸€è‡´æ€§-å®¹é‡çš„æƒè¡¡ï¼Œå³æé«˜æ¨¡å‹çš„ä¸€è‡´æ€§å¯èƒ½ä¼šé™ä½å…¶å®¹é‡ã€‚è¿™äº›å‘ç°ä¸ºè®¾è®¡æ›´å¯é å’Œå¯æ‰©å±•çš„çŸ¥è¯†æ›´æ–°ç­–ç•¥æä¾›äº†é‡è¦çš„å¯ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

KnowledgeSmithçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡LLMçš„çŸ¥è¯†æ›´æ–°èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é€‚åº”æ–°çš„ä¿¡æ¯å’Œçº æ­£é”™è¯¯ã€‚è¿™å¯¹äºéœ€è¦æŒç»­å­¦ä¹ å’Œæ›´æ–°çŸ¥è¯†çš„LLMåº”ç”¨è‡³å…³é‡è¦ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€çŸ¥è¯†é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨å†…å®¹ç”Ÿæˆã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥ç”¨äºè¯„ä¼°ä¸åŒçŸ¥è¯†ç¼–è¾‘å’Œæœºå™¨é—å¿˜ç®—æ³•çš„æ€§èƒ½ï¼Œå¹¶æŒ‡å¯¼æ–°ç®—æ³•çš„è®¾è®¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge editing and machine unlearning are two popular approaches for large language models (LLMs) to stay up-to-date. However, the knowledge updating mechanism of LLMs remains largely unexplored due to insufficient, isolated, and small-scale evaluation. For instance, are LLMs similar to humans in modifying certain knowledge? What differs editing and unlearning as training data increases? This paper proposes KnowledgeSmith, a unified framework to systematically understand the updating mechanism of LLMs. We first cast editing and unlearning as instances of one constrained optimization problem. Then, we propose an automatic dataset generator that provides structured interventions across multiple graph levels and data scales, enabling controlled studies of how different modification strategies propagate through model knowledge. Extensive experiments demonstrate nuanced insights over knowledge propagation, plasticity scaling, consistency, and robustness. For instance, our results show that LLMs do not exhibit similar updating as humans for different levels of knowledge, and there exists consistency-capacity trade-off. We hope our findings can offer suggestions to the design of more reliable and scalable strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git

