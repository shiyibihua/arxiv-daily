---
layout: default
title: Training Large Language Models To Reason In Parallel With Global Forking Tokens
---

# Training Large Language Models To Reason In Parallel With Global Forking Tokens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.05132" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.05132v2</a>
  <a href="https://arxiv.org/pdf/2510.05132.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.05132v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.05132v2', 'Training Large Language Models To Reason In Parallel With Global Forking Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sheng Jia, Xiao Wang, Shiva Prasad Kasiviswanathan

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01 (æ›´æ–°: 2025-11-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSSFTæ–¹æ³•ï¼Œé€šè¿‡å…¨å±€Forking Tokensè®­ç»ƒLLMè¿›è¡Œå¹¶è¡Œæ¨ç†ï¼Œæå‡å¤æ‚é—®é¢˜æ±‚è§£èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¹¶è¡Œæ¨ç†` `ç›‘ç£å¾®è°ƒ` `é›†åˆé¢„æµ‹` `å…¨å±€Forking Tokens`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é¼“åŠ±LLMè¿›è¡Œå¤šæ ·åŒ–æ¨ç†æ—¶ï¼Œéš¾ä»¥å…¼é¡¾æ¨ç†çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚é—®é¢˜ä¸­ã€‚
2. è®ºæ–‡æå‡ºSet Supervised Fine-Tuning (SSFT) æ–¹æ³•ï¼Œå°†å¹¶è¡Œæ¨ç†è§†ä¸ºé›†åˆé¢„æµ‹é—®é¢˜ï¼Œå¹¶å¼•å…¥å…¨å±€æŸå¤±ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSSFTæ–¹æ³•èƒ½å¤Ÿä¿ç•™ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼ï¼Œå¹¶äº§ç”Ÿå…¨å±€forking tokensï¼Œåœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºSFTã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ‰©å±•å¹¶è¡Œæµ‹è¯•æ—¶è®¡ç®—èƒ½åŠ›æ¥æå‡æ€§èƒ½ï¼Œä½†è¿™ä¾èµ–äºç”Ÿæˆæ—¢å¤šæ ·åˆå‡†ç¡®çš„æ¨ç†è·¯å¾„ã€‚å¯¹äºå¤æ‚é—®é¢˜ï¼Œè§¦å‘å¤šæ ·ä¸”æ­£ç¡®æ¨ç†æ¨¡å¼çš„forking tokensé€šå¸¸ä½äºé‡‡æ ·æ ‘çš„æ·±å¤„ã€‚å› æ­¤ï¼Œè¯¸å¦‚æ¸©åº¦ç¼©æ”¾ç­‰é¼“åŠ±å¤šæ ·æ€§çš„å¸¸ç”¨ç­–ç•¥ä¼šåŠ å‰§å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†å¹¶è¡Œæ¨ç†è§†ä¸ºä¸€ä¸ªé›†åˆçš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹é—®é¢˜ï¼Œå¹¶ä½¿ç”¨å…¨å±€forking tokenså’Œå”¯ä¸€æ¨ç†è½¨è¿¹ä¹‹é—´çš„è‡ªç›‘ç£äºŒåˆ†åŒ¹é…ï¼Œå°†åŸºäºé›†åˆçš„å…¨å±€æŸå¤±çº³å…¥åˆ°ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸­ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œä½¿ç”¨å¤šä¸ªæ¨ç†è½¨è¿¹è¿›è¡Œæœ´ç´ å¾®è°ƒä¼šä½¿è¿™äº›ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼å´©æºƒï¼Œè€Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ï¼Œé›†åˆç›‘ç£å¾®è°ƒï¼ˆSSFTï¼‰ï¼Œä¿ç•™äº†è¿™äº›æ¨¡å¼å¹¶äº§ç”Ÿäº†æ–°å…´çš„å…¨å±€forking tokensã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SSFTåœ¨Pass@1å’ŒCons@kæŒ‡æ ‡ä¸‹å§‹ç»ˆä¼˜äºSFTã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œå¹¶è¡Œæ¨ç†æ—¶å¤šæ ·æ€§å’Œå‡†ç¡®æ€§éš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚æ¸©åº¦ç¼©æ”¾ï¼Œè™½ç„¶å¯ä»¥å¢åŠ æ¨ç†è·¯å¾„çš„å¤šæ ·æ€§ï¼Œä½†å¾€å¾€ä¼šé™ä½æ¨ç†çš„å‡†ç¡®æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™æ˜¯å› ä¸ºè§¦å‘å¤šæ ·ä¸”æ­£ç¡®æ¨ç†æ¨¡å¼çš„forking tokensé€šå¸¸ä½äºé‡‡æ ·æ ‘çš„æ·±å¤„ï¼Œç®€å•çš„å¤šæ ·æ€§é¼“åŠ±ç­–ç•¥éš¾ä»¥æœ‰æ•ˆæ•æ‰è¿™äº›å…³é”®tokenã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¹¶è¡Œæ¨ç†è¿‡ç¨‹è§†ä¸ºä¸€ä¸ªé›†åˆé¢„æµ‹é—®é¢˜ï¼Œå³é¢„æµ‹ä¸€ç»„å¯èƒ½çš„ä¸‹ä¸€ä¸ªtokenï¼ˆforking tokensï¼‰ã€‚é€šè¿‡å¼•å…¥åŸºäºé›†åˆçš„å…¨å±€æŸå¤±ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ åˆ°èƒ½å¤Ÿè§¦å‘å¤šæ ·ä¸”æ­£ç¡®çš„æ¨ç†è·¯å¾„çš„forking tokensã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å…‹æœä¼ ç»Ÿæ–¹æ³•ä¸­å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œä»è€Œæå‡LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSSFTæ–¹æ³•åœ¨æ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¡†æ¶ä¹‹ä¸Šè¿›è¡Œæ”¹è¿›ã€‚ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨LLMç”Ÿæˆå¤šä¸ªæ¨ç†è½¨è¿¹ï¼›2) å®šä¹‰å…¨å±€forking tokensï¼Œè¿™äº›tokensä»£è¡¨äº†ä¸åŒæ¨ç†è·¯å¾„çš„åˆ†å‰ç‚¹ï¼›3) ä½¿ç”¨è‡ªç›‘ç£äºŒåˆ†åŒ¹é…ç®—æ³•ï¼Œå°†å…¨å±€forking tokensä¸å”¯ä¸€çš„æ¨ç†è½¨è¿¹è¿›è¡ŒåŒ¹é…ï¼›4) å°†åŸºäºé›†åˆçš„å…¨å±€æŸå¤±å‡½æ•°åŠ å…¥åˆ°SFTçš„æŸå¤±å‡½æ•°ä¸­ï¼Œå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åŸºäºé›†åˆçš„å…¨å±€æŸå¤±å‡½æ•°ï¼Œå¹¶å°†å…¶åº”ç”¨äºLLMçš„å¾®è°ƒè¿‡ç¨‹ä¸­ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ åˆ°å…¨å±€forking tokensï¼Œä»è€Œä¿ç•™äº†ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼ï¼Œå¹¶æå‡äº†å¹¶è¡Œæ¨ç†çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSSFTæ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œä»è€Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šSSFTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å…¨å±€forking tokensçš„å®šä¹‰ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„æ¨ç†ä»»åŠ¡è¿›è¡Œé€‰æ‹©ï¼›2) è‡ªç›‘ç£äºŒåˆ†åŒ¹é…ç®—æ³•çš„é€‰æ‹©ï¼Œéœ€è¦ä¿è¯åŒ¹é…çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼›3) åŸºäºé›†åˆçš„å…¨å±€æŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¡é‡é¢„æµ‹çš„forking tokensä¸çœŸå®æ¨ç†è½¨è¿¹ä¹‹é—´çš„å·®å¼‚ã€‚è®ºæ–‡ä¸­å…·ä½“ä½¿ç”¨çš„æŸå¤±å‡½æ•°å’ŒåŒ¹é…ç®—æ³•çš„å…·ä½“ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSSFTæ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºSFTæ–¹æ³•ï¼Œåœ¨Pass@1å’ŒCons@kæŒ‡æ ‡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚è¿™è¡¨æ˜SSFTæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿ç•™ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼ï¼Œå¹¶äº§ç”Ÿå…¨å±€forking tokensï¼Œä»è€Œæå‡äº†LLMçš„å¹¶è¡Œæ¨ç†èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„å„ç§åœºæ™¯ï¼Œä¾‹å¦‚ï¼šè‡ªåŠ¨å®šç†è¯æ˜ã€ä»£ç ç”Ÿæˆã€é—®é¢˜æ±‚è§£ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡LLMçš„å¹¶è¡Œæ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è¿™äº›åº”ç”¨åœºæ™¯çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæå‡LLMçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics.

