---
layout: default
title: TokMem: Tokenized Procedural Memory for Large Language Models
---

# TokMem: Tokenized Procedural Memory for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00444" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00444v1</a>
  <a href="https://arxiv.org/pdf/2510.00444.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00444v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00444v1', 'TokMem: Tokenized Procedural Memory for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zijun Wu, Yongchang Hao, Lili Mou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTokMemï¼šä¸€ç§ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„TokenåŒ–è¿‡ç¨‹è®°å¿†ï¼Œæå‡ä»»åŠ¡æ³›åŒ–ä¸æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¿‡ç¨‹è®°å¿†` `å¤§å‹è¯­è¨€æ¨¡å‹` `TokenåŒ–` `æç¤ºå·¥ç¨‹` `å‡½æ•°è°ƒç”¨` `çŸ¥è¯†è¡¨ç¤º` `æŒç»­å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ä¾èµ–æç¤ºå·¥ç¨‹ï¼Œæ•ˆç‡ä½ã€æ³›åŒ–å·®ï¼Œä¸”ç¼ºä¹æ¨¡å—åŒ–å¤ç”¨æœºåˆ¶ã€‚
2. TokMemå°†é‡å¤è¿‡ç¨‹ç¼–ç ä¸ºå¯è®­ç»ƒçš„TokenåµŒå…¥ï¼Œå®ç°æ’å®šå¼€é”€ä¸‹çš„ç›®æ ‡è¡Œä¸ºæ§åˆ¶ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTokMemåœ¨åŸå­å›å¿†å’Œå‡½æ•°è°ƒç”¨ä»»åŠ¡ä¸Šä¼˜äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œä¸”å‚æ•°é‡æ›´å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ä¸¥é‡ä¾èµ–æç¤ºæ¥æŒ‡å®šä»»åŠ¡ã€å›å¿†çŸ¥è¯†å’ŒæŒ‡å¯¼æ¨ç†ã€‚ç„¶è€Œï¼Œè¿™ç§ä¾èµ–æ•ˆç‡ä½ä¸‹ï¼Œå› ä¸ºæç¤ºå¿…é¡»åœ¨æ¯ä¸ªæ­¥éª¤ä¸­é‡æ–°è¯»å–ï¼Œè·¨ä»»åŠ¡çš„å¯æ‰©å±•æ€§å·®ï¼Œå¹¶ä¸”ç¼ºä¹æ¨¡å—åŒ–é‡ç”¨æœºåˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†TokMemï¼Œä¸€ç§TokenåŒ–çš„è¿‡ç¨‹è®°å¿†ï¼Œå®ƒå°†é‡å¤å‡ºç°çš„è¿‡ç¨‹å­˜å‚¨ä¸ºç´§å‡‘çš„å¯è®­ç»ƒåµŒå…¥ã€‚æ¯ä¸ªè®°å¿†Tokenç¼–ç ä¸€ä¸ªè¿‡ç¨‹çš„åœ°å€å’Œä¸€ä¸ªæ§åˆ¶ä¿¡å·ï¼Œä»¥æ’å®šå¤§å°çš„å¼€é”€å®ç°ç›®æ ‡è¡Œä¸ºã€‚ä¸ºäº†æ”¯æŒæŒç»­é€‚åº”ï¼ŒTokMemä¿æŒéª¨å¹²æ¨¡å‹å†»ç»“ï¼Œå…è®¸æ·»åŠ æ–°è¿‡ç¨‹è€Œä¸å¹²æ‰°ç°æœ‰è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨1000ä¸ªåŸå­å›å¿†ä»»åŠ¡å’Œå‡½æ•°è°ƒç”¨ç»„åˆå›å¿†ä»»åŠ¡ä¸Šè¯„ä¼°äº†TokMemï¼Œå®ƒå§‹ç»ˆä¼˜äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ŒåŒæ—¶é¿å…äº†é‡å¤çš„ä¸Šä¸‹æ–‡å¼€é”€ï¼Œå¹¶ä¸”ä½¿ç”¨æ›´å°‘çš„å‚æ•°è¿›è¡Œå¾®è°ƒã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒTokMemæ˜¯æç¤ºå·¥ç¨‹å’Œå¾®è°ƒçš„å¯æ‰©å±•å’Œæ¨¡å—åŒ–æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºLLMæä¾›äº†ä¸€ç§æ˜¾å¼çš„è¿‡ç¨‹è®°å¿†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä¾èµ–æç¤ºï¼ˆpromptsï¼‰æ¥å®Œæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬çŸ¥è¯†å›å¿†ã€æ¨ç†å¼•å¯¼ç­‰ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹å¼å­˜åœ¨æ•ˆç‡é—®é¢˜ï¼Œå› ä¸ºæ¯æ¬¡éƒ½éœ€è¦é‡æ–°è¯»å–æç¤ºï¼Œå¹¶ä¸”åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œç¼ºä¹æ¨¡å—åŒ–å’Œå¯é‡ç”¨æ€§ã€‚ç°æœ‰æ–¹æ³•å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è™½ç„¶å¯ä»¥ç¼“è§£éƒ¨åˆ†é—®é¢˜ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸Šä¸‹æ–‡å†—ä½™å’Œè®¡ç®—å¼€é”€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTokMemçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†é‡å¤ä½¿ç”¨çš„è¿‡ç¨‹ï¼ˆprocedural knowledgeï¼‰å­˜å‚¨ä¸ºå¯è®­ç»ƒçš„TokenåµŒå…¥ï¼Œè¿™äº›TokenåµŒå…¥å¯ä»¥è¢«è§†ä¸ºä¸€ç§â€œè¿‡ç¨‹è®°å¿†â€ã€‚æ¯ä¸ªTokenä¸ä»…åŒ…å«æŒ‡å‘ç‰¹å®šè¿‡ç¨‹çš„åœ°å€ï¼Œè¿˜åŒ…å«æ§åˆ¶ä¿¡å·ï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨éœ€è¦æ—¶å¿«é€Ÿè°ƒç”¨ç›¸åº”çš„è¿‡ç¨‹ï¼Œè€Œæ— éœ€é‡å¤è¯»å–å†—é•¿çš„æç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTokMemä¸»è¦åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼šTokenåŒ–çš„è¿‡ç¨‹è®°å¿†æ¨¡å—å’Œè¯­è¨€æ¨¡å‹ã€‚è¿‡ç¨‹è®°å¿†æ¨¡å—è´Ÿè´£å­˜å‚¨å’Œç®¡ç†è¿‡ç¨‹Tokenï¼Œè¯­è¨€æ¨¡å‹åˆ™è´Ÿè´£åˆ©ç”¨è¿™äº›Tokenè¿›è¡Œç”Ÿæˆã€‚å½“æ¨¡å‹éœ€è¦æ‰§è¡ŒæŸä¸ªä»»åŠ¡æ—¶ï¼Œé¦–å…ˆä»è¿‡ç¨‹è®°å¿†ä¸­é€‰æ‹©ç›¸å…³çš„Tokenï¼Œç„¶åå°†è¿™äº›Tokenæ·»åŠ åˆ°è¾“å…¥åºåˆ—ä¸­ã€‚è¯­è¨€æ¨¡å‹æ ¹æ®è¿™äº›Tokençš„å¼•å¯¼ï¼Œç”Ÿæˆç›¸åº”çš„è¾“å‡ºã€‚å…³é”®åœ¨äºï¼ŒTokMemå…è®¸åœ¨ä¸å¾®è°ƒæ•´ä¸ªLLMçš„æƒ…å†µä¸‹æ·»åŠ æ–°çš„è¿‡ç¨‹ï¼Œä»è€Œå®ç°æŒç»­å­¦ä¹ å’Œé€‚åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šTokMemçš„å…³é”®åˆ›æ–°åœ¨äºå®ƒå°†è¿‡ç¨‹çŸ¥è¯†æ˜¾å¼åœ°å­˜å‚¨ä¸ºå¯è®­ç»ƒçš„Tokenï¼Œå¹¶åˆ©ç”¨è¿™äº›Tokenæ¥æ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºã€‚ä¸ä¼ ç»Ÿçš„æç¤ºå·¥ç¨‹ç›¸æ¯”ï¼ŒTokMemå¯ä»¥é¿å…é‡å¤çš„ä¸Šä¸‹æ–‡å¼€é”€ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸å¾®è°ƒç›¸æ¯”ï¼ŒTokMemå¯ä»¥åœ¨ä¸å¹²æ‰°ç°æœ‰çŸ¥è¯†çš„æƒ…å†µä¸‹æ·»åŠ æ–°çš„è¿‡ç¨‹ï¼Œä»è€Œå®ç°æŒç»­å­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šTokMemçš„è®¾è®¡åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªå…³é”®æ–¹é¢ï¼š1) è¿‡ç¨‹Tokençš„åµŒå…¥è¡¨ç¤ºï¼šå¦‚ä½•å°†è¿‡ç¨‹çŸ¥è¯†ç¼–ç ä¸ºç´§å‡‘çš„TokenåµŒå…¥ï¼Ÿ2) Tokené€‰æ‹©æœºåˆ¶ï¼šå¦‚ä½•æ ¹æ®å½“å‰ä»»åŠ¡é€‰æ‹©ç›¸å…³çš„è¿‡ç¨‹Tokenï¼Ÿ3) æ§åˆ¶ä¿¡å·çš„è®¾è®¡ï¼šå¦‚ä½•åˆ©ç”¨Tokenä¸­çš„æ§åˆ¶ä¿¡å·æ¥æŒ‡å¯¼æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Ÿè®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒè¿‡ç¨‹Tokençš„åµŒå…¥è¡¨ç¤ºï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„ç½‘ç»œç»“æ„æ¥å®ç°Tokené€‰æ‹©å’Œæ§åˆ¶ä¿¡å·çš„ä¼ é€’ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚éœ€è¦å‚è€ƒè®ºæ–‡åŸæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TokMemåœ¨1000ä¸ªåŸå­å›å¿†ä»»åŠ¡å’Œå‡½æ•°è°ƒç”¨ç»„åˆå›å¿†ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒTokMemå§‹ç»ˆä¼˜äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼ŒåŒæ—¶é¿å…äº†é‡å¤çš„ä¸Šä¸‹æ–‡å¼€é”€ï¼Œå¹¶ä¸”ä½¿ç”¨æ›´å°‘çš„å‚æ•°è¿›è¡Œå¾®è°ƒã€‚è¿™äº›ç»“æœè¯æ˜äº†TokMemä½œä¸ºä¸€ç§å¯æ‰©å±•å’Œæ¨¡å—åŒ–æ›¿ä»£æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TokMemå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å¯ä»¥åº”ç”¨äºæ™ºèƒ½åŠ©æ‰‹ã€ä»£ç ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡å°†å¸¸ç”¨çš„æ“ä½œæµç¨‹å­˜å‚¨ä¸ºè¿‡ç¨‹è®°å¿†ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è¿™äº›ç³»ç»Ÿçš„æ•ˆç‡å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚æ­¤å¤–ï¼ŒTokMemè¿˜å¯ä»¥ç”¨äºæ„å»ºå¯æ‰©å±•çš„çŸ¥è¯†åº“ï¼Œæ”¯æŒæŒç»­å­¦ä¹ å’ŒçŸ¥è¯†æ›´æ–°ã€‚æœªæ¥ï¼ŒTokMemæœ‰æœ›æˆä¸ºæ„å»ºæ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡è¦æŠ€æœ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models rely heavily on prompts to specify tasks, recall knowledge and guide reasoning. However, this reliance is inefficient as prompts must be re-read at each step, scale poorly across tasks, and lack mechanisms for modular reuse. We introduce TokMem, a tokenized procedural memory that stores recurring procedures as compact, trainable embeddings. Each memory token encodes both an address to a procedure and a control signal that steers generation, enabling targeted behavior with constant-size overhead. To support continual adaptation, TokMem keeps the backbone model frozen, allowing new procedures to be added without interfering with existing ones. We evaluate TokMem on 1,000 tasks for atomic recall, and on function-calling tasks for compositional recall, where it consistently outperforms retrieval-augmented generation while avoiding repeated context overhead, and fine-tuning with far fewer parameters. These results establish TokMem as a scalable and modular alternative to prompt engineering and fine-tuning, offering an explicit procedural memory for LLMs.

