---
layout: default
title: Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding
---

# Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15253" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.15253v1</a>
  <a href="https://arxiv.org/pdf/2510.15253.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15253v1" onclick="toggleFavorite(this, '2510.15253v1', 'Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sensen Gao, Shanshan Zhao, Xu Jiang, Lunhao Duan, Yong Xien Chng, Qing-Guo Chen, Weihua Luo, Kaifu Zhang, Jia-Wang Bian, Mingming Gong

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¤šæ¨¡æ€RAGç»¼è¿°ï¼šæå‡æ–‡æ¡£ç†è§£èƒ½åŠ›ï¼Œè¶…è¶Šä¸Šä¸‹æ–‡é™åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `æ–‡æ¡£ç†è§£` `å¤§å‹è¯­è¨€æ¨¡å‹` `è·¨æ¨¡æ€æ£€ç´¢` `çŸ¥è¯†èåˆ` `å›¾ç¥ç»ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æ¡£ç†è§£æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ–‡æ¡£æ—¶å­˜åœ¨å±€é™ï¼ŒOCRæ–¹æ³•æŸå¤±ç»“æ„ä¿¡æ¯ï¼ŒMLLMåˆ™ç¼ºä¹æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºå¤šæ¨¡æ€RAGèŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡è·¨æ¨¡æ€æ£€ç´¢å’Œæ¨ç†ï¼Œæå‡æ¨¡å‹å¯¹æ–‡æ¡£çš„æ•´ä½“ç†è§£èƒ½åŠ›ã€‚
3. è®ºæ–‡ç³»ç»Ÿæ€§åœ°ç»¼è¿°äº†å¤šæ¨¡æ€RAGåœ¨æ–‡æ¡£ç†è§£ä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œå¦‚æ•ˆç‡ã€ç»†ç²’åº¦è¡¨ç¤ºå’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æ¡£ç†è§£åœ¨é‡‘èåˆ†æå’Œç§‘å­¦å‘ç°ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ç›®å‰çš„æ–¹æ³•ï¼Œæ— è®ºæ˜¯åŸºäºOCRçš„æµæ°´çº¿ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¿˜æ˜¯åŸç”Ÿå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰ï¼Œéƒ½é¢ä¸´å…³é”®é™åˆ¶ï¼šå‰è€…ä¸¢å¤±ç»“æ„ç»†èŠ‚ï¼Œè€Œåè€…éš¾ä»¥è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æœ‰åŠ©äºæ¨¡å‹åŸºäºå¤–éƒ¨æ•°æ®è¿›è¡Œæ¨ç†ï¼Œä½†æ–‡æ¡£çš„å¤šæ¨¡æ€ç‰¹æ€§ï¼ˆå³ç»“åˆæ–‡æœ¬ã€è¡¨æ ¼ã€å›¾è¡¨å’Œå¸ƒå±€ï¼‰éœ€è¦ä¸€ç§æ›´å…ˆè¿›çš„èŒƒå¼ï¼šå¤šæ¨¡æ€RAGã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå®ç°è·¨æ‰€æœ‰æ¨¡æ€çš„æ•´ä½“æ£€ç´¢å’Œæ¨ç†ï¼Œä»è€Œé‡Šæ”¾å…¨é¢çš„æ–‡æ¡£æ™ºèƒ½ã€‚æœ¬æ–‡å¯¹ç”¨äºæ–‡æ¡£ç†è§£çš„å¤šæ¨¡æ€RAGè¿›è¡Œäº†ç³»ç»Ÿçš„ç»¼è¿°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºé¢†åŸŸã€æ£€ç´¢æ¨¡æ€å’Œç²’åº¦çš„åˆ†ç±»æ³•ï¼Œå¹¶å›é¡¾äº†æ¶‰åŠå›¾ç»“æ„å’Œä»£ç†æ¡†æ¶çš„è¿›å±•ã€‚æˆ‘ä»¬è¿˜æ€»ç»“äº†å…³é”®æ•°æ®é›†ã€åŸºå‡†å’Œåº”ç”¨ï¼Œå¹¶å¼ºè°ƒäº†æ•ˆç‡ã€ç»†ç²’åº¦è¡¨ç¤ºå’Œé²æ£’æ€§æ–¹é¢çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºæ–‡æ¡£AIçš„æœªæ¥å‘å±•æä¾›äº†è·¯çº¿å›¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–‡æ¡£ç†è§£æ–¹æ³•åœ¨å¤„ç†å¤æ‚ã€å¤šæ¨¡æ€æ–‡æ¡£æ—¶çš„å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºOCRçš„ä¼ ç»Ÿæ–¹æ³•ä¼šä¸¢å¤±æ–‡æ¡£çš„ç»“æ„ä¿¡æ¯ï¼Œè€ŒåŸç”Ÿå¤šæ¨¡æ€LLMåœ¨ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨æ–‡æ¡£ä¸­çš„å„ç§æ¨¡æ€ä¿¡æ¯ï¼ˆæ–‡æœ¬ã€è¡¨æ ¼ã€å›¾è¡¨ã€å¸ƒå±€ç­‰ï¼‰ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†æ¨¡å‹åœ¨é‡‘èåˆ†æã€ç§‘å­¦å‘ç°ç­‰é¢†åŸŸçš„åº”ç”¨æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èŒƒå¼ï¼Œå°†å¤–éƒ¨çŸ¥è¯†ä¸æ–‡æ¡£å†…å®¹ç›¸ç»“åˆï¼Œä»è€Œæå‡æ¨¡å‹å¯¹æ–‡æ¡£çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡è·¨æ¨¡æ€æ£€ç´¢ï¼Œæ¨¡å‹å¯ä»¥ä»æ–‡æ¡£çš„ä¸åŒæ¨¡æ€ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥å¢å¼ºç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œå®ç°æ›´å…¨é¢ã€æ›´å‡†ç¡®çš„æ–‡æ¡£ç†è§£ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•åœ¨ç»“æ„ä¿¡æ¯ä¿ç•™å’Œä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€RAGçš„ç³»ç»Ÿæ€§æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ–‡æ¡£è¡¨ç¤ºï¼šå°†æ–‡æ¡£ä¸­çš„ä¸åŒæ¨¡æ€ä¿¡æ¯ï¼ˆæ–‡æœ¬ã€è¡¨æ ¼ã€å›¾è¡¨ã€å¸ƒå±€ç­‰ï¼‰è¿›è¡Œç¼–ç ï¼Œå½¢æˆç»Ÿä¸€çš„å‘é‡è¡¨ç¤ºï¼›2) æ£€ç´¢æ¨¡å—ï¼šæ ¹æ®æŸ¥è¯¢è¯­å¥ï¼Œä»æ–‡æ¡£åº“ä¸­æ£€ç´¢å‡ºç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µæˆ–ä¿¡æ¯ï¼›3) èåˆæ¨¡å—ï¼šå°†æ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸åŸå§‹æ–‡æ¡£å†…å®¹è¿›è¡Œèåˆï¼Œå½¢æˆå¢å¼ºçš„ä¸Šä¸‹æ–‡ï¼›4) ç”Ÿæˆæ¨¡å—ï¼šåˆ©ç”¨å¢å¼ºçš„ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆæœ€ç»ˆçš„ç­”æ¡ˆæˆ–ç»“æœã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†å›¾ç»“æ„å’Œä»£ç†æ¡†æ¶åœ¨å¤šæ¨¡æ€RAGä¸­çš„åº”ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„å¤šæ¨¡æ€RAGæ¡†æ¶ï¼Œç”¨äºè§£å†³æ–‡æ¡£ç†è§£é—®é¢˜ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè·¨è¶Šä¸åŒçš„æ¨¡æ€è¿›è¡Œæ£€ç´¢å’Œæ¨ç†ï¼Œä»è€Œå®ç°æ›´å…¨é¢çš„æ–‡æ¡£ç†è§£ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹ç°æœ‰çš„å¤šæ¨¡æ€RAGæ–¹æ³•è¿›è¡Œäº†åˆ†ç±»å’Œæ€»ç»“ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œå¦‚æ•ˆç‡ã€ç»†ç²’åº¦è¡¨ç¤ºå’Œé²æ£’æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶æ›´æ³¨é‡æ–‡æ¡£çš„æ•´ä½“ç»“æ„å’Œå¤šæ¨¡æ€ä¿¡æ¯ä¹‹é—´çš„å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ¶‰åŠçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•æœ‰æ•ˆåœ°è¡¨ç¤ºæ–‡æ¡£ä¸­çš„ä¸åŒæ¨¡æ€ä¿¡æ¯ï¼›2) å¦‚ä½•è®¾è®¡è·¨æ¨¡æ€çš„æ£€ç´¢ç­–ç•¥ï¼›3) å¦‚ä½•å°†æ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸åŸå§‹æ–‡æ¡£å†…å®¹è¿›è¡Œèåˆï¼›4) å¦‚ä½•ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†å›¾ç»“æ„å’Œä»£ç†æ¡†æ¶åœ¨å¤šæ¨¡æ€RAGä¸­çš„åº”ç”¨ï¼Œå¹¶å¯¹è¿™äº›æŠ€æœ¯çš„å…·ä½“å®ç°è¿›è¡Œäº†è®¨è®ºã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°ã€ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚éœ€è¦å‚è€ƒç›¸å…³è®ºæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡æ˜¯ä¸€ç¯‡ç»¼è¿°æ€§æ–‡ç« ï¼Œæ²¡æœ‰å…·ä½“çš„å®éªŒç»“æœã€‚å…¶äº®ç‚¹åœ¨äºå¯¹å¤šæ¨¡æ€RAGåœ¨æ–‡æ¡£ç†è§£ä¸­çš„åº”ç”¨è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„æ€»ç»“å’Œåˆ†æï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºé‡‘èåˆ†æã€ç§‘å­¦å‘ç°ã€æ³•å¾‹å’¨è¯¢ç­‰é¢†åŸŸï¼Œæå‡æ–‡æ¡£å¤„ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨é‡‘èé¢†åŸŸï¼Œå¯ä»¥å¸®åŠ©åˆ†æå¸ˆå¿«é€Ÿç†è§£è´¢åŠ¡æŠ¥è¡¨ï¼Œå‘ç°æ½œåœ¨é£é™©ï¼›åœ¨ç§‘å­¦é¢†åŸŸï¼Œå¯ä»¥è¾…åŠ©ç ”ç©¶äººå‘˜ä»å¤§é‡æ–‡çŒ®ä¸­æå–å…³é”®ä¿¡æ¯ï¼ŒåŠ é€Ÿç§‘ç ”è¿›å±•ã€‚æœªæ¥ï¼Œå¤šæ¨¡æ€RAGæœ‰æœ›æˆä¸ºæ–‡æ¡£æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Document understanding is critical for applications from financial analysis to scientific discovery. Current approaches, whether OCR-based pipelines feeding Large Language Models (LLMs) or native Multimodal LLMs (MLLMs), face key limitations: the former loses structural detail, while the latter struggles with context modeling. Retrieval-Augmented Generation (RAG) helps ground models in external data, but documents' multimodal nature, i.e., combining text, tables, charts, and layout, demands a more advanced paradigm: Multimodal RAG. This approach enables holistic retrieval and reasoning across all modalities, unlocking comprehensive document intelligence. Recognizing its importance, this paper presents a systematic survey of Multimodal RAG for document understanding. We propose a taxonomy based on domain, retrieval modality, and granularity, and review advances involving graph structures and agentic frameworks. We also summarize key datasets, benchmarks, and applications, and highlight open challenges in efficiency, fine-grained representation, and robustness, providing a roadmap for future progress in document AI.

