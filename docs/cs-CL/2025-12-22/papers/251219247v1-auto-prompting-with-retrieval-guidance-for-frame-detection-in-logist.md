---
layout: default
title: Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics
---

# Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19247" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.19247v1</a>
  <a href="https://arxiv.org/pdf/2512.19247.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19247v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19247v1', 'Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Do Minh Duc, Quan Xuan Truong, Nguyen Tat Dat, Nguyen Van Vinh

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ£€ç´¢å¼•å¯¼çš„è‡ªåŠ¨Promptä¼˜åŒ–æ–¹æ³•ï¼Œæå‡LLMåœ¨ç‰©æµæ–‡æœ¬æ¡†æ¶æ£€æµ‹ä¸­çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Promptå·¥ç¨‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç‰©æµæ–‡æœ¬` `æ¡†æ¶æ£€æµ‹` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `æ€ç»´é“¾` `è‡ªåŠ¨Promptä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°†LLMåº”ç”¨äºç‰©æµæ–‡æœ¬æ¡†æ¶æ£€æµ‹æ—¶ï¼Œä¾èµ–äººå·¥è®¾è®¡çš„promptï¼Œæ•ˆç‡ä½ä¸”æ•ˆæœæœ‰é™ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºæ£€ç´¢å¼•å¯¼çš„è‡ªåŠ¨promptä¼˜åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨RAGã€CoTç­‰æŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆå’Œä¼˜åŒ–promptã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªLLMä¸Šå‡èƒ½æ˜¾è‘—æå‡æ¡†æ¶æ£€æµ‹çš„å‡†ç¡®ç‡ï¼Œæœ€é«˜æå‡è¾¾15%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„promptä¼˜åŒ–æµç¨‹ï¼Œç”¨äºç‰©æµæ–‡æœ¬ä¸­çš„æ¡†æ¶æ£€æµ‹ã€‚è¯¥æµç¨‹ç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€å°‘æ ·æœ¬promptingã€æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å’Œè‡ªåŠ¨CoTåˆæˆï¼ˆAuto-CoTï¼‰ï¼Œä»¥ç”Ÿæˆé«˜æ•ˆçš„ä»»åŠ¡ç‰¹å®špromptã€‚æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºLLMçš„promptä¼˜åŒ–ä»£ç†ï¼Œå®ƒä½¿ç”¨æ£€ç´¢åˆ°çš„ç¤ºä¾‹ã€æ€§èƒ½åé¦ˆå’Œå†…éƒ¨è‡ªæˆ‘è¯„ä¼°æ¥è¿­ä»£åœ°æ”¹è¿›promptã€‚è¯¥æ¡†æ¶åœ¨å®é™…çš„ç‰©æµæ–‡æœ¬æ ‡æ³¨ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–çš„promptï¼ˆç‰¹åˆ«æ˜¯é€šè¿‡Auto-CoTå’ŒRAGå¢å¼ºçš„promptï¼‰ä¸åŸºçº¿é›¶æ ·æœ¬æˆ–é™æ€promptç›¸æ¯”ï¼Œå®é™…æ¨ç†å‡†ç¡®ç‡æé«˜äº†15%ã€‚è¯¥ç³»ç»Ÿåœ¨å¤šä¸ªLLMï¼ˆåŒ…æ‹¬GPT-4oã€Qwen 2.5 (72B)å’ŒLLaMA 3.1 (70B)ï¼‰ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶é€šç”¨æ€§å’Œå®ç”¨ä»·å€¼ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç»“æ„åŒ–çš„promptä¼˜åŒ–æ˜¯å®Œå…¨å¾®è°ƒçš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºåœ¨ç‰©æµç­‰ç‰¹å®šé¢†åŸŸçš„NLPåº”ç”¨ä¸­éƒ¨ç½²LLMæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç‰©æµæ–‡æœ¬ä¸­æ¡†æ¶æ£€æµ‹ä»»åŠ¡çš„promptå·¥ç¨‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥è®¾è®¡promptï¼Œè€—æ—¶ä¸”æ•ˆæœå—é™äºä¸“å®¶çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œé™æ€promptéš¾ä»¥é€‚åº”å¤æ‚å¤šå˜çš„ç‰©æµåœºæ™¯ï¼Œå¯¼è‡´LLMæ¨ç†ç²¾åº¦ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMè‡ªèº«çš„èƒ½åŠ›ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºå’Œè‡ªåŠ¨ä¼˜åŒ–ï¼Œç”Ÿæˆæ›´æœ‰æ•ˆçš„ä»»åŠ¡ç‰¹å®špromptã€‚é€šè¿‡æ£€ç´¢ç›¸ä¼¼çš„ç¤ºä¾‹ï¼Œä¸ºLLMæä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è¿­ä»£ä¼˜åŒ–promptï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ç›®æ ‡ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ£€ç´¢æ¨¡å—ï¼šä»é¢„å®šä¹‰çš„è¯­æ–™åº“ä¸­æ£€ç´¢ä¸è¾“å…¥æ–‡æœ¬ç›¸å…³çš„ç¤ºä¾‹ï¼›2) Promptä¼˜åŒ–ä»£ç†ï¼šåŸºäºLLMï¼Œåˆ©ç”¨æ£€ç´¢åˆ°çš„ç¤ºä¾‹ã€æ€§èƒ½åé¦ˆå’Œå†…éƒ¨è‡ªæˆ‘è¯„ä¼°ï¼Œè¿­ä»£åœ°æ”¹è¿›promptï¼›3) æ¨ç†æ¨¡å—ï¼šä½¿ç”¨ä¼˜åŒ–åçš„promptï¼Œåˆ©ç”¨LLMè¿›è¡Œæ¡†æ¶æ£€æµ‹ï¼›4) è¯„ä¼°æ¨¡å—ï¼šè¯„ä¼°LLMçš„æ¨ç†ç»“æœï¼Œå¹¶å°†è¯„ä¼°ç»“æœåé¦ˆç»™Promptä¼˜åŒ–ä»£ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºPromptä¼˜åŒ–ä»£ç†çš„è®¾è®¡ã€‚è¯¥ä»£ç†èƒ½å¤Ÿåˆ©ç”¨æ£€ç´¢å¢å¼ºå’Œè‡ªåŠ¨CoTåˆæˆæŠ€æœ¯ï¼Œè‡ªåŠ¨ç”Ÿæˆå’Œä¼˜åŒ–promptï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚æ­¤å¤–ï¼Œè¯¥ä»£ç†è¿˜èƒ½å¤Ÿè¿›è¡Œå†…éƒ¨è‡ªæˆ‘è¯„ä¼°ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ”¹è¿›promptã€‚

**å…³é”®è®¾è®¡**ï¼šPromptä¼˜åŒ–ä»£ç†çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨RAGæŠ€æœ¯ï¼Œä»è¯­æ–™åº“ä¸­æ£€ç´¢ä¸è¾“å…¥æ–‡æœ¬ç›¸å…³çš„ç¤ºä¾‹ï¼Œä¸ºLLMæä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›2) ä½¿ç”¨Auto-CoTæŠ€æœ¯ï¼Œè‡ªåŠ¨ç”Ÿæˆæ€ç»´é“¾ï¼Œå¼•å¯¼LLMè¿›è¡Œæ¨ç†ï¼›3) ä½¿ç”¨æ€§èƒ½åé¦ˆå’Œå†…éƒ¨è‡ªæˆ‘è¯„ä¼°ï¼Œè¿­ä»£åœ°æ”¹è¿›promptï¼›4) ä½¿ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–promptçš„ç”Ÿæˆè¿‡ç¨‹ï¼ˆå…·ä½“æŸå¤±å‡½æ•°ç»†èŠ‚æœªçŸ¥ï¼‰ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19247v1/image/wordcloud.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19247v1/image/sentence_length_distribution.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19247v1/image/copy.jpg" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®é™…ç‰©æµæ–‡æœ¬æ ‡æ³¨ä»»åŠ¡ä¸­ï¼Œä¸åŸºçº¿é›¶æ ·æœ¬æˆ–é™æ€promptç›¸æ¯”ï¼Œæ¨ç†å‡†ç¡®ç‡æé«˜äº†15%ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªLLMï¼ˆåŒ…æ‹¬GPT-4oã€Qwen 2.5 (72B)å’ŒLLaMA 3.1 (70B)ï¼‰ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶é€šç”¨æ€§å’Œå®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½ç‰©æµé¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨æå–ç‰©æµå•æ®ä¸­çš„å…³é”®ä¿¡æ¯ã€è¯†åˆ«å¼‚å¸¸ç‰©æµäº‹ä»¶ã€ä¼˜åŒ–ç‰©æµè·¯å¾„è§„åˆ’ç­‰ã€‚é€šè¿‡æå‡LLMåœ¨ç‰©æµæ–‡æœ¬ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥æé«˜ç‰©æµæ•ˆç‡ã€é™ä½è¿è¥æˆæœ¬ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›æ›´ä¼˜è´¨çš„æœåŠ¡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚é‡‘èã€åŒ»ç–—ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.

