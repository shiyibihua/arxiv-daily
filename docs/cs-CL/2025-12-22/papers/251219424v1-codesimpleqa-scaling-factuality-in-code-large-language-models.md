---
layout: default
title: CodeSimpleQA: Scaling Factuality in Code Large Language Models
---

# CodeSimpleQA: Scaling Factuality in Code Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19424" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.19424v1</a>
  <a href="https://arxiv.org/pdf/2512.19424.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19424v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19424v1', 'CodeSimpleQA: Scaling Factuality in Code Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jian Yang, Wei Zhang, Yizhi Li, Shawn Guo, Haowen Wang, Aishan Liu, Ge Zhang, Zili Wang, Zhoujun Li, Xianglong Liu, Weifeng Lv

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CodeSimpleQAï¼šæå‡ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§å‡†ç¡®åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç å¤§è¯­è¨€æ¨¡å‹` `äº‹å®æ€§è¯„ä¼°` `åŸºå‡†æµ‹è¯•` `æŒ‡ä»¤å¾®è°ƒ` `å¼ºåŒ–å­¦ä¹ ` `ç¼–ç¨‹çŸ¥è¯†` `ä»£ç ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä»£ç åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ä»£ç æ‰§è¡Œçš„æ­£ç¡®æ€§ï¼Œå¿½ç•¥äº†æ¨¡å‹å¯¹ç¼–ç¨‹çŸ¥è¯†ç†è§£çš„äº‹å®å‡†ç¡®æ€§ã€‚
2. è®ºæ–‡æå‡ºCodeSimpleQAåŸºå‡†å’ŒCodeSimpleQA-InstructæŒ‡ä»¤æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å’Œæå‡ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§ã€‚
3. é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨CodeSimpleQAä¸Šçš„äº‹å®æ€§å‡†ç¡®åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåœ¨ä»è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆæˆä»£ç ç‰‡æ®µæ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ï¼Œå³ç¡®ä¿LLMç”Ÿæˆå…³äºç¼–ç¨‹æ¦‚å¿µã€æŠ€æœ¯å®ç°ç­‰æ–¹é¢çš„ã€åœ¨äº‹å®ä¸Šå‡†ç¡®çš„å“åº”ã€‚ä»¥å¾€å¤§å¤šæ•°ä»£ç ç›¸å…³åŸºå‡†ä¾§é‡äºä»£ç æ‰§è¡Œçš„æ­£ç¡®æ€§ï¼Œè€Œå¿½ç•¥äº†ç¼–ç¨‹çŸ¥è¯†çš„äº‹å®å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†CodeSimpleQAï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŒè¯­åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°ä»£ç LLMåœ¨å›ç­”ä»£ç ç›¸å…³é—®é¢˜æ—¶çš„äº‹å®å‡†ç¡®æ€§ï¼Œå…¶ä¸­åŒ…å«ç²¾å¿ƒç­–åˆ’çš„è‹±è¯­å’Œä¸­æ–‡é—®ç­”å¯¹ï¼Œæ¶µç›–ä¸åŒçš„ç¼–ç¨‹è¯­è¨€å’Œä¸»è¦çš„è®¡ç®—æœºç§‘å­¦é¢†åŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºäº†CodeSimpleQA-Instructï¼Œä¸€ä¸ªåŒ…å«6600ä¸‡ä¸ªæ ·æœ¬çš„å¤§è§„æ¨¡æŒ‡ä»¤è¯­æ–™åº“ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬å¯¹å„ç§LLMçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LLMä¹Ÿåœ¨ä»£ç äº‹å®æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶è¯æ˜äº†ç›¸å¯¹äºåŸºç¡€æ¨¡å‹çš„æ˜¾è‘—æ”¹è¿›ï¼Œçªæ˜¾äº†åœ¨å¼€å‘å¯é çš„ä»£ç LLMä¸­ï¼Œäº‹å®æ€§æ„ŸçŸ¥å¯¹é½çš„å…³é”®é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ä»£ç å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆä»£ç æ—¶ï¼Œè™½ç„¶ä»£ç å¯ä»¥æ‰§è¡Œï¼Œä½†å¯¹ç¼–ç¨‹æ¦‚å¿µã€æŠ€æœ¯å®ç°ç­‰æ–¹é¢çš„ç†è§£å¯èƒ½å­˜åœ¨äº‹å®æ€§é”™è¯¯ã€‚ä»¥å¾€çš„è¯„æµ‹åŸºå‡†ä¸»è¦å…³æ³¨ä»£ç æ‰§è¡Œçš„æ­£ç¡®æ€§ï¼Œç¼ºä¹å¯¹æ¨¡å‹ç¼–ç¨‹çŸ¥è¯†äº‹å®å‡†ç¡®æ€§çš„è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°ä»£ç å¤§è¯­è¨€æ¨¡å‹äº‹å®æ€§å‡†ç¡®åº¦çš„åŸºå‡†æµ‹è¯•é›†CodeSimpleQAï¼Œå¹¶åˆ©ç”¨å¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†CodeSimpleQA-Instructï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æ¥æå‡æ¨¡å‹çš„äº‹å®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) æ„å»ºåŒè¯­åŸºå‡†æµ‹è¯•é›†CodeSimpleQAï¼ŒåŒ…å«é«˜è´¨é‡çš„ç¼–ç¨‹ç›¸å…³é—®ç­”å¯¹ï¼›2) åˆ›å»ºå¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†CodeSimpleQA-Instructï¼Œç”¨äºæ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒï¼›3) æå‡ºä¸€ä¸ªåè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥æå‡æ¨¡å‹çš„äº‹å®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ä»£ç å¤§è¯­è¨€æ¨¡å‹äº‹å®æ€§è¯„ä¼°çš„åŸºå‡†æµ‹è¯•é›†ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ¡†æ¶ã€‚ä¸ä»¥å¾€ä¾§é‡äºä»£ç æ‰§è¡Œæ­£ç¡®æ€§çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ç›´æ¥å…³æ³¨æ¨¡å‹å¯¹ç¼–ç¨‹çŸ¥è¯†çš„ç†è§£æ˜¯å¦å‡†ç¡®ã€‚

**å…³é”®è®¾è®¡**ï¼šCodeSimpleQAåŒ…å«è‹±è¯­å’Œä¸­æ–‡ä¸¤ç§è¯­è¨€çš„é—®ç­”å¯¹ï¼Œè¦†ç›–å¤šç§ç¼–ç¨‹è¯­è¨€å’Œè®¡ç®—æœºç§‘å­¦é¢†åŸŸã€‚CodeSimpleQA-InstructåŒ…å«6600ä¸‡ä¸ªæ ·æœ¬ï¼Œç”¨äºæŒ‡ä»¤å¾®è°ƒã€‚åè®­ç»ƒæ¡†æ¶ä¸­ï¼Œé¦–å…ˆä½¿ç”¨CodeSimpleQA-Instructè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„äº‹å®æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19424v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19424v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19424v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡åœ¨CodeSimpleQAåŸºå‡†æµ‹è¯•é›†ä¸Šå¯¹å¤šç§LLMè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯æœ€å…ˆè¿›çš„LLMåœ¨ä»£ç äº‹å®æ€§æ–¹é¢ä¹Ÿå­˜åœ¨å›°éš¾ã€‚æå‡ºçš„åè®­ç»ƒæ¡†æ¶æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨CodeSimpleQAä¸Šçš„äº‹å®æ€§å‡†ç¡®åº¦ï¼Œè¯æ˜äº†äº‹å®æ€§æ„ŸçŸ¥å¯¹é½åœ¨å¼€å‘å¯é ä»£ç LLMä¸­çš„é‡è¦æ€§ã€‚å…·ä½“æå‡å¹…åº¦æœªåœ¨æ‘˜è¦ä¸­æ˜ç¡®ç»™å‡ºï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„å¯é æ€§å’Œå¯ä¿¡åº¦ï¼Œä½¿å…¶åœ¨ä»£ç ç”Ÿæˆã€ä»£ç è§£é‡Šã€ç¼–ç¨‹é—®ç­”ç­‰åœºæ™¯ä¸­æä¾›æ›´å‡†ç¡®ã€æ›´å¯é çš„ç¼–ç¨‹çŸ¥è¯†ã€‚è¿™æœ‰åŠ©äºæé«˜å¼€å‘æ•ˆç‡ï¼Œé™ä½é”™è¯¯ç‡ï¼Œå¹¶ä¿ƒè¿›ç¼–ç¨‹æ•™è‚²å’ŒçŸ¥è¯†å…±äº«ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.

