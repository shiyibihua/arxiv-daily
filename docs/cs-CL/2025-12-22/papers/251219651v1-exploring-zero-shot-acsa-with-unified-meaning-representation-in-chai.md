---
layout: default
title: Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting
---

# Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19651" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.19651v1</a>
  <a href="https://arxiv.org/pdf/2512.19651.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19651v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19651v1', 'Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Filippos Ventirozos, Peter Appleby, Matthew Shardlow

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

**å¤‡æ³¨**: 9 pages, 3 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºUMRçš„CoTæç¤ºæ–¹æ³•ï¼Œæ¢ç´¢é›¶æ ·æœ¬ACSAä»»åŠ¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é›¶æ ·æœ¬å­¦ä¹ ` `æ–¹é¢æƒ…æ„Ÿåˆ†æ` `æ€ç»´é“¾` `ç»Ÿä¸€æ„ä¹‰è¡¨ç¤º` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ACSAä»»åŠ¡ä¾èµ–æ ‡æ³¨æ•°æ®ï¼Œä½†æ•°æ®æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”ç¨€ç¼ºï¼Œé™åˆ¶äº†å…¶åœ¨æ–°é¢†åŸŸçš„åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºç»Ÿä¸€æ„ä¹‰è¡¨ç¤º(UMR)çš„CoTæç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨æå‡é›¶æ ·æœ¬ACSAæ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒUMRçš„æœ‰æ•ˆæ€§å¯èƒ½ä¸æ¨¡å‹ç›¸å…³ï¼Œä¸­ç­‰è§„æ¨¡æ¨¡å‹è¡¨ç°å‡ºå¯æ¯”æ€§ï¼Œä½†éœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–¹é¢-ç±»åˆ«æƒ…æ„Ÿåˆ†æ(ACSA)é€šè¿‡è¯†åˆ«è¯„è®ºä¸­çš„ç‰¹å®šä¸»é¢˜åŠå…¶ç›¸å…³æƒ…æ„Ÿæ¥æä¾›ç»†ç²’åº¦çš„æ´å¯Ÿã€‚è™½ç„¶ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è¯¥é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ï¼Œä½†æ–°é¢†åŸŸå¸¦æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§å’Œé«˜æˆæœ¬å¸¦æ¥äº†é‡å¤§éšœç¢ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨æ•°æ®æ ‡æ³¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­æ˜¯ä¸€ç§å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ€ç»´é“¾(CoT)æç¤ºæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨ä¸­é—´çš„ç»Ÿä¸€æ„ä¹‰è¡¨ç¤º(UMR)æ¥æ„å»ºACSAä»»åŠ¡çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬é’ˆå¯¹ä¸‰ä¸ªæ¨¡å‹(Qwen3-4Bã€Qwen3-8Bå’ŒGemini-2.5-Pro)å’Œå››ä¸ªä¸åŒçš„æ•°æ®é›†ï¼Œè¯„ä¼°äº†è¿™ç§åŸºäºUMRçš„æ–¹æ³•ä¸æ ‡å‡†CoTåŸºçº¿ç›¸æ¯”çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒUMRçš„æœ‰æ•ˆæ€§å¯èƒ½å–å†³äºæ¨¡å‹ã€‚è™½ç„¶åˆæ­¥ç»“æœè¡¨æ˜ï¼Œå¯¹äºåƒQwen3-8Bè¿™æ ·çš„ä¸­å‹æ¨¡å‹ï¼Œæ€§èƒ½å…·æœ‰å¯æ¯”æ€§ï¼Œä½†è¿™äº›è§‚å¯Ÿç»“æœå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯å…³äºå…¶å¯¹è¾ƒå°æ¨¡å‹æ¶æ„çš„æ½œåœ¨é€‚ç”¨æ€§ã€‚éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ä»¥ç¡®å®šè¿™äº›å‘ç°åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šçš„æ™®éé€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹ï¼Œå¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œæ–¹é¢-ç±»åˆ«æƒ…æ„Ÿåˆ†æ(ACSA)çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œä½†åœ¨æ–°é¢†åŸŸä¸­ï¼Œæ ‡æ³¨æ•°æ®çš„è·å–æˆæœ¬é«˜æ˜‚ä¸”æ•°æ®ç¨€ç¼ºï¼Œé™åˆ¶äº†ACSAçš„åº”ç”¨ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨æ²¡æœ‰æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå……åˆ†åˆ©ç”¨LLMçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæˆä¸ºä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Chain-of-Thought (CoT) æç¤ºæŠ€æœ¯ï¼Œå¹¶å¼•å…¥ç»Ÿä¸€æ„ä¹‰è¡¨ç¤º(UMR)ä½œä¸ºä¸­é—´æ­¥éª¤ï¼Œæ¥å¼•å¯¼LLMè¿›è¡Œæ›´ç»“æ„åŒ–çš„æ¨ç†ã€‚é€šè¿‡å°†ACSAä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„ã€æ›´æ˜“äºç†è§£çš„æ­¥éª¤ï¼Œå¹¶åˆ©ç”¨UMRæ¥è¡¨ç¤ºè¿™äº›æ­¥éª¤ä¹‹é—´çš„å…³ç³»ï¼Œå¯ä»¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) è¾“å…¥è¯„è®ºæ–‡æœ¬ï¼›2) ä½¿ç”¨CoTæç¤ºï¼Œå¼•å¯¼LLMç”Ÿæˆä¸­é—´çš„UMRè¡¨ç¤ºï¼›3) åŸºäºUMRè¡¨ç¤ºï¼ŒLLMæ¨æ–­å‡ºæ–¹é¢ã€ç±»åˆ«å’Œæƒ…æ„Ÿææ€§ï¼›4) è¾“å‡ºæœ€ç»ˆçš„ACSAç»“æœã€‚UMRä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œè¿æ¥äº†è¾“å…¥æ–‡æœ¬å’Œæœ€ç»ˆçš„ACSAç»“æœï¼Œèµ·åˆ°äº†æ¡¥æ¢çš„ä½œç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†UMRä½œä¸ºCoTæç¤ºçš„ä¸­é—´æ­¥éª¤ã€‚UMRæä¾›äº†ä¸€ç§ç»“æ„åŒ–çš„æ–¹å¼æ¥è¡¨ç¤ºè¯„è®ºæ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å¸®åŠ©LLMæ›´å¥½åœ°ç†è§£æ–‡æœ¬çš„å«ä¹‰ã€‚ä¸ä¼ ç»Ÿçš„CoTæç¤ºç›¸æ¯”ï¼ŒåŸºäºUMRçš„CoTæç¤ºå¯ä»¥æ›´æœ‰æ•ˆåœ°å¼•å¯¼LLMè¿›è¡Œæ¨ç†ï¼Œå¹¶æé«˜ACSAçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­UMRçš„å…·ä½“å½¢å¼å’Œç”Ÿæˆæ–¹å¼æœªè¯¦ç»†è¯´æ˜ï¼Œè¿™éƒ¨åˆ†ä¿¡æ¯æœªçŸ¥ã€‚CoTæç¤ºçš„è®¾è®¡ä¹Ÿæœªç»™å‡ºå…·ä½“ç»†èŠ‚ï¼Œä¾‹å¦‚ä½¿ç”¨çš„æç¤ºè¯­æ¨¡æ¿ã€è¶…å‚æ•°è®¾ç½®ç­‰ã€‚å®éªŒä¸­ä½¿ç”¨äº†Qwen3-4Bã€Qwen3-8Bå’ŒGemini-2.5-Proç­‰LLMï¼Œä½†æ²¡æœ‰è¯¦ç»†è¯´æ˜è¿™äº›æ¨¡å‹çš„å…·ä½“é…ç½®å’Œè®­ç»ƒæ–¹å¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºUMRçš„CoTæç¤ºæ–¹æ³•åœ¨æŸäº›æ¨¡å‹ä¸Šè¡¨ç°å‡ºä¸æ ‡å‡†CoTåŸºçº¿ç›¸å½“çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­ç­‰è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚Qwen3-8Bï¼‰ä¸Šã€‚ç„¶è€Œï¼Œç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼ŒUMRçš„æœ‰æ•ˆæ€§å¯èƒ½ä¸æ¨¡å‹ç›¸å…³ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ä»¥ç¡®å®šå…¶åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºåœ¨çº¿è¯„è®ºåˆ†æã€èˆ†æƒ…ç›‘æ§ã€äº§å“æ”¹è¿›ç­‰é¢†åŸŸã€‚é€šè¿‡é›¶æ ·æœ¬ACSAï¼Œä¼ä¸šå¯ä»¥å¿«é€Ÿäº†è§£ç”¨æˆ·å¯¹äº§å“æˆ–æœåŠ¡çš„è¯„ä»·ï¼Œæ— éœ€å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œé™ä½äº†æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.

