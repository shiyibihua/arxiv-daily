---
layout: default
title: A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation
---

# A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23356" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.23356v1</a>
  <a href="https://arxiv.org/pdf/2512.23356.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23356v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23356v1', 'A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin Zhang, Yang Cao, Baoxing Wu, Xinyi Chen, Kai Song, Siying Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤–éƒ¨å­å›¾ç”Ÿæˆçš„é€æ­¥å¢å¼ºæ¨ç†æ¡†æ¶SGRï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `çŸ¥è¯†å›¾è°±` `æ¨ç†å¢å¼º` `å¤–éƒ¨çŸ¥è¯†` `å­å›¾ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ˜“å—å™ªå£°ä¿¡æ¯å¹²æ‰°ï¼Œå¯¼è‡´é”™è¯¯é¢„æµ‹æˆ–ä¸äº‹å®ä¸ç¬¦çš„è¾“å‡ºã€‚
2. SGRæ¡†æ¶é€šè¿‡åŠ¨æ€æ„å»ºå¤–éƒ¨çŸ¥è¯†å­å›¾ï¼Œå¼•å¯¼æ¨¡å‹åœ¨ç»“æ„åŒ–çŸ¥è¯†ä¸Šè¿›è¡Œé€æ­¥æ¨ç†ï¼Œå‡å°‘å™ªå£°å½±å“ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSGRåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆå’Œé—®ç­”ç­‰è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œå½“åº”ç”¨æ‰©å±•åˆ°æ—¥ç›Šå¤æ‚çš„åœºæ™¯æ—¶ï¼ŒLLMsåœ¨éœ€è¦æ·±åº¦æ¨ç†å’Œé€»è¾‘æ¨æ–­çš„ä»»åŠ¡ä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯èƒ½ä¼šåŒ…å«å™ªå£°æˆ–ä¸ç›¸å…³çš„ä¿¡æ¯ï¼Œä»è€Œå¯¼è‡´ä¸æ­£ç¡®çš„é¢„æµ‹æˆ–ä¸äº‹å®çŸ¥è¯†ä¸ä¸€è‡´çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤–éƒ¨å­å›¾ç”Ÿæˆçš„LLMsé€æ­¥æ¨ç†å¢å¼ºæ¡†æ¶ï¼Œç§°ä¸ºSGRã€‚è¯¥æ¡†æ¶åŠ¨æ€åœ°ä»å¤–éƒ¨çŸ¥è¯†åº“æ„å»ºä¸æŸ¥è¯¢ç›¸å…³çš„å­å›¾ï¼Œå¹¶åˆ©ç”¨å…¶è¯­ä¹‰ç»“æ„æ¥æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡åœ¨ç»“æ„åŒ–å­å›¾ä¸Šé€æ­¥æ‰§è¡Œæ¨ç†ï¼ŒSGRå‡å°‘äº†å™ªå£°ä¿¡æ¯çš„å½±å“ï¼Œæé«˜äº†æ¨ç†å‡†ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶é¦–å…ˆç”Ÿæˆä¸€ä¸ªé’ˆå¯¹è¾“å…¥æŸ¥è¯¢é‡èº«å®šåˆ¶çš„å¤–éƒ¨å­å›¾ï¼Œç„¶åå¼•å¯¼æ¨¡å‹åœ¨å­å›¾çš„åŸºç¡€ä¸Šè¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œæœ€åæ•´åˆå¤šä¸ªæ¨ç†è·¯å¾„ä»¥äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSGRå§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œè¡¨æ˜å…¶åœ¨å¢å¼ºLLMsæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œç”±äºå—åˆ°è®­ç»ƒæ•°æ®ä¸­å™ªå£°æˆ–ä¸ç›¸å…³ä¿¡æ¯çš„å½±å“ï¼Œå¯¼è‡´æ¨ç†å‡†ç¡®æ€§ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ï¼Œå®¹æ˜“äº§ç”Ÿä¸äº‹å®çŸ¥è¯†ä¸ä¸€è‡´çš„è¾“å‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“æ„å»ºä¸è¾“å…¥æŸ¥è¯¢ç›¸å…³çš„å­å›¾ï¼Œå¹¶å¼•å¯¼LLMåœ¨è¿™äº›ç»“æ„åŒ–çš„å­å›¾ä¸Šè¿›è¡Œé€æ­¥æ¨ç†ã€‚é€šè¿‡é™åˆ¶æ¨ç†è¿‡ç¨‹åœ¨ç›¸å…³çŸ¥è¯†èŒƒå›´å†…ï¼Œå‡å°‘å™ªå£°ä¿¡æ¯çš„å¹²æ‰°ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSGRæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) **å­å›¾ç”Ÿæˆ**ï¼šæ ¹æ®è¾“å…¥æŸ¥è¯¢ï¼Œä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æå–ç›¸å…³çš„å­å›¾ã€‚2) **å¤šæ­¥æ¨ç†**ï¼šå¼•å¯¼LLMåœ¨ç”Ÿæˆçš„å­å›¾ä¸Šè¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œæ¯ä¸€æ­¥æ¨ç†éƒ½åŸºäºå­å›¾çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚3) **ç­”æ¡ˆæ•´åˆ**ï¼šæ•´åˆå¤šæ¡æ¨ç†è·¯å¾„çš„ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šSGRçš„å…³é”®åˆ›æ–°åœ¨äºåŠ¨æ€æ„å»ºä¸æŸ¥è¯¢ç›¸å…³çš„å¤–éƒ¨å­å›¾ï¼Œå¹¶å°†LLMçš„æ¨ç†è¿‡ç¨‹é™åˆ¶åœ¨è¿™äº›å­å›¾çš„èŒƒå›´å†…ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†å¤–éƒ¨çŸ¥è¯†ï¼Œå¹¶å‡å°‘äº†å™ªå£°ä¿¡æ¯çš„å¹²æ‰°ï¼Œä»è€Œæé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSGRèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ï¼Œå¹¶é¿å…ç”Ÿæˆä¸äº‹å®çŸ¥è¯†ä¸ä¸€è‡´çš„è¾“å‡ºã€‚

**å…³é”®è®¾è®¡**ï¼šå­å›¾ç”Ÿæˆè¿‡ç¨‹ä¾èµ–äºçŸ¥è¯†åº“çš„ç»“æ„å’ŒæŸ¥è¯¢çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå…·ä½“å®ç°å¯èƒ½æ¶‰åŠå®ä½“é“¾æ¥ã€å…³ç³»æŠ½å–ç­‰æŠ€æœ¯ã€‚å¤šæ­¥æ¨ç†è¿‡ç¨‹å¯ä»¥é€šè¿‡æç¤ºå·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰å¼•å¯¼LLMé€æ­¥æ¢ç´¢å­å›¾ä¸­çš„ä¿¡æ¯ã€‚ç­”æ¡ˆæ•´åˆå¯ä»¥é‡‡ç”¨æŠ•ç¥¨ã€åŠ æƒå¹³å‡ç­‰æ–¹æ³•ï¼Œæ ¹æ®ä¸åŒæ¨ç†è·¯å¾„çš„å¯é æ€§è¿›è¡Œæ•´åˆã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23356v1/Fig1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23356v1/Fig2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23356v1/Fig3.png" alt="img_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSGRæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Š consistently ä¼˜äºç°æœ‰çš„å¼ºå¤§åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨å¢å¼ºLLMsæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®åœ¨æ‘˜è¦ä¸­æœªç»™å‡ºï¼Œä½†å¼ºè°ƒäº†SGRåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ä¸€è‡´æ€§è¡¨ç°ä¼˜è¶Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºé—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ¨ç†ã€æ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœ¨å¤æ‚åœºæ™¯ä¸‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æç­‰éœ€è¦ç²¾ç¡®æ¨ç†çš„é¢†åŸŸå…·æœ‰é‡è¦ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†è¿›è¡Œæ¨ç†çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have achieved strong performance across a wide range of natural language processing tasks in recent years, including machine translation, text generation, and question answering. As their applications extend to increasingly complex scenarios, however, LLMs continue to face challenges in tasks that require deep reasoning and logical inference. In particular, models trained on large scale textual corpora may incorporate noisy or irrelevant information during generation, which can lead to incorrect predictions or outputs that are inconsistent with factual knowledge. To address this limitation, we propose a stepwise reasoning enhancement framework for LLMs based on external subgraph generation, termed SGR. The proposed framework dynamically constructs query relevant subgraphs from external knowledge bases and leverages their semantic structure to guide the reasoning process. By performing reasoning in a step by step manner over structured subgraphs, SGR reduces the influence of noisy information and improves reasoning accuracy. Specifically, the framework first generates an external subgraph tailored to the input query, then guides the model to conduct multi step reasoning grounded in the subgraph, and finally integrates multiple reasoning paths to produce the final answer. Experimental results on multiple benchmark datasets demonstrate that SGR consistently outperforms strong baselines, indicating its effectiveness in enhancing the reasoning capabilities of LLMs.

