---
layout: default
title: Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings
---

# Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23471" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.23471v1</a>
  <a href="https://arxiv.org/pdf/2512.23471.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23471v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23471v1', 'Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thomas Haschka, Joseph Bakarji

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-29

**å¤‡æ³¨**: 20 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºåµŒå¥—å¯†åº¦èšç±»å’ŒLLMåµŒå…¥çš„è¯­ä¹‰æ ‘æ¨æ–­æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬è¯­æ–™åº“çš„è¯­ä¹‰ç»“æ„å‘ç°ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­ä¹‰æ ‘æ¨æ–­` `åµŒå¥—å¯†åº¦èšç±»` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬åˆ†ç±»` `è¯­ä¹‰åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ­ç¤ºæ–‡æœ¬è¯­æ–™åº“ä¸­å…¨å±€è¯­ä¹‰å…³ç³»çš„ç»“æ„ï¼Œé˜»ç¢äº†å¯¹æ–‡æœ¬æ•°æ®çš„æ·±å…¥ç†è§£å’Œåˆ©ç”¨ã€‚
2. è¯¥æ–¹æ³•é€šè¿‡åµŒå¥—å¯†åº¦èšç±»ï¼Œä»LLMåµŒå…¥ç©ºé—´ä¸­æ„å»ºè¯­ä¹‰æ ‘ï¼Œæ­ç¤ºæ–‡æœ¬é—´çš„åˆ†å±‚è¯­ä¹‰å…³ç³»ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç§‘å­¦æ‘˜è¦åˆ†ç±»ã€20 News-groupså’ŒIMDB 50k Movie Reviewsç­‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠå…¶é«˜ç»´åµŒå…¥çš„å…´èµ·ï¼Œè¯­ä¹‰æ–‡æœ¬åˆ†ç±»å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è™½ç„¶LLMåµŒå…¥ç»å¸¸è¢«ç”¨äºåœ¨å‘é‡æ•°æ®åº“ä¸­é€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§æ¥å­˜å‚¨å’Œæ£€ç´¢æ–‡æœ¬ï¼Œä½†æ–‡æœ¬è¯­æ–™åº“ä¸­å…¨å±€ç»“æ„çš„è¯­ä¹‰å…³ç³»é€šå¸¸ä»ç„¶ä¸æ˜ç¡®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åµŒå¥—å¯†åº¦èšç±»æ–¹æ³•ï¼Œç”¨äºæ¨æ–­è¯­ä¹‰ç›¸å…³æ–‡æœ¬çš„å±‚æ¬¡æ ‘ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡æœç´¢LLMåµŒå…¥ç©ºé—´ä¸­çš„å¯†é›†ç°‡æ¥è¯†åˆ«å…·æœ‰å¼ºè¯­ä¹‰ç›¸ä¼¼æ€§çš„æ–‡æœ¬ã€‚éšç€å¯†åº¦æ ‡å‡†çš„é€æ¸æ”¾å®½ï¼Œè¿™äº›å¯†é›†ç°‡åˆå¹¶æˆæ›´åˆ†æ•£çš„ç°‡ï¼Œç›´åˆ°æ•´ä¸ªæ•°æ®é›†ç”±å•ä¸ªç°‡è¡¨ç¤ºâ€”â€”æ ‘çš„æ ¹ã€‚é€šè¿‡å°†å¯†é›†ç°‡åµŒå…¥åˆ°è¶Šæ¥è¶Šåˆ†æ•£çš„ç°‡ä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ ‘ç»“æ„ï¼Œè¯¥ç»“æ„æ•è·äº†æ–‡æœ¬ä¹‹é—´åˆ†å±‚çš„è¯­ä¹‰å…³ç³»ã€‚æˆ‘ä»¬æ¦‚è¿°äº†å¦‚ä½•å°†è¿™ç§æ–¹æ³•ç”¨äºç§‘å­¦æ‘˜è¦çš„æ–‡æœ¬æ•°æ®åˆ†ç±»ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ã€‚è¿™ä½¿å¾—æ— éœ€é¢„å®šä¹‰ç±»åˆ«å³å¯è¿›è¡Œæ•°æ®é©±åŠ¨çš„ç ”ç©¶é¢†åŸŸåŠå…¶å­é¢†åŸŸçš„å‘ç°ã€‚ä¸ºäº†è¯„ä¼°è¯¥æ–¹æ³•çš„ä¸€èˆ¬é€‚ç”¨æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å°†å…¶åº”ç”¨äºå·²å»ºç«‹çš„åŸºå‡†æ•°æ®é›†ï¼Œå¦‚20 News-groupså’ŒIMDB 50k Movie Reviewsï¼Œè¯æ˜äº†å…¶è·¨é¢†åŸŸçš„ç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åœ¨ç§‘å­¦è®¡é‡å­¦ã€ä¸»é¢˜æ¼”å˜æ–¹é¢çš„å¯èƒ½åº”ç”¨ï¼Œå¼ºè°ƒäº†åµŒå¥—å¯†åº¦æ ‘å¦‚ä½•æ­ç¤ºæ–‡æœ¬æ•°æ®é›†ä¸­çš„è¯­ä¹‰ç»“æ„å’Œæ¼”å˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•ä»æ–‡æœ¬è¯­æ–™åº“ä¸­è‡ªåŠ¨æ¨æ–­å‡ºè¯­ä¹‰ç»“æ„çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ä¼ ç»Ÿçš„èšç±»æˆ–ä¸»é¢˜æ¨¡å‹ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰é«˜ç»´LLMåµŒå…¥ç©ºé—´ä¸­å¤æ‚çš„è¯­ä¹‰å…³ç³»ï¼Œå¹¶ä¸”é€šå¸¸éœ€è¦é¢„å…ˆå®šä¹‰ç±»åˆ«æˆ–ä¸»é¢˜æ•°é‡ï¼Œé™åˆ¶äº†å…¶çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åµŒå¥—å¯†åº¦èšç±»ï¼Œä»LLMåµŒå…¥ç©ºé—´ä¸­é€æ­¥æ„å»ºè¯­ä¹‰æ ‘ã€‚è¯¥æ–¹æ³•å‡è®¾è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨åµŒå…¥ç©ºé—´ä¸­ä¼šå½¢æˆå¯†é›†ç°‡ï¼Œé€šè¿‡é€æ­¥æ”¾å®½å¯†åº¦æ ‡å‡†ï¼Œå°†è¿™äº›å¯†é›†ç°‡åˆå¹¶æˆæ›´å¤§çš„ç°‡ï¼Œä»è€Œå½¢æˆä¸€ä¸ªå±‚æ¬¡åŒ–çš„ç»“æ„ï¼Œåæ˜ æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨LLMå¯¹æ–‡æœ¬è¿›è¡ŒåµŒå…¥ï¼Œå¾—åˆ°é«˜ç»´å‘é‡è¡¨ç¤ºï¼›2) åˆå§‹åŒ–å¯†åº¦é˜ˆå€¼ï¼Œåœ¨åµŒå…¥ç©ºé—´ä¸­å¯»æ‰¾å¯†é›†ç°‡ï¼›3) é€æ­¥é™ä½å¯†åº¦é˜ˆå€¼ï¼Œåˆå¹¶ç›¸é‚»çš„å¯†é›†ç°‡ï¼Œå½¢æˆæ›´å¤§çš„ç°‡ï¼›4) å°†ç°‡ä¹‹é—´çš„åˆå¹¶å…³ç³»æ„å»ºæˆæ ‘ç»“æ„ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç°‡ï¼Œçˆ¶èŠ‚ç‚¹ä»£è¡¨åŒ…å«å­èŠ‚ç‚¹çš„æ›´å¹¿æ³›çš„è¯­ä¹‰ç±»åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå°†åµŒå¥—å¯†åº¦èšç±»ä¸LLMåµŒå…¥ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰æ–‡æœ¬ä¹‹é—´å¤æ‚çš„åˆ†å±‚è¯­ä¹‰å…³ç³»ï¼Œæ— éœ€é¢„å…ˆå®šä¹‰ç±»åˆ«æˆ–ä¸»é¢˜æ•°é‡ï¼Œå…·æœ‰å¾ˆå¼ºçš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚ä¸ä¼ ç»Ÿçš„èšç±»æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å…³è”ï¼Œå¹¶æä¾›æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ¶‰åŠçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¯†åº¦é˜ˆå€¼çš„é€‰æ‹©ç­–ç•¥ï¼Œéœ€è¦æ ¹æ®æ•°æ®é›†çš„ç‰¹ç‚¹è¿›è¡Œè°ƒæ•´ï¼›2) ç°‡åˆå¹¶çš„å‡†åˆ™ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨åŸºäºè·ç¦»æˆ–ç›¸ä¼¼åº¦çš„åº¦é‡ï¼›3) æ ‘ç»“æ„çš„æ„å»ºæ–¹å¼ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨è‡ªåº•å‘ä¸Šçš„æ–¹å¼é€æ­¥åˆå¹¶ç°‡ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23471v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23471v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23471v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨ç§‘å­¦æ‘˜è¦åˆ†ç±»ã€20 News-groupså’ŒIMDB 50k Movie Reviewsç­‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨æ–­å‡ºæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œå¹¶æ„å»ºå‡ºå…·æœ‰è‰¯å¥½å±‚æ¬¡ç»“æ„çš„è¯­ä¹‰æ ‘ã€‚å°¤å…¶æ˜¯åœ¨ç§‘å­¦æ‘˜è¦åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨å‘ç°ç ”ç©¶é¢†åŸŸåŠå…¶å­é¢†åŸŸï¼Œä¸ºç§‘ç ”äººå‘˜æä¾›æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç§‘å­¦è®¡é‡å­¦ã€ä¸»é¢˜æ¼”å˜åˆ†æã€æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºè‡ªåŠ¨å‘ç°ç ”ç©¶é¢†åŸŸåŠå…¶å­é¢†åŸŸï¼Œè·Ÿè¸ªä¸»é¢˜çš„æ¼”å˜è¶‹åŠ¿ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ï¼Œæé«˜ä¿¡æ¯æ£€ç´¢çš„å‡†ç¡®ç‡å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨æ–‡æœ¬æ•°æ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster - the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 News- groups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.

