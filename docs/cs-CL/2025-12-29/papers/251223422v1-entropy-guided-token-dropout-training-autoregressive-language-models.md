---
layout: default
title: "Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data"
---

# Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23422" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.23422v1</a>
  <a href="https://arxiv.org/pdf/2512.23422.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23422v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23422v1', 'Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiapeng Wang, Yiwen Hu, Yanzipeng Gao, Haoyu Wang, Shuo Wang, Hongyu Lu, Jiaxin Mao, Wayne Xin Zhao, Junyi Li, Xiao Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEntroDropï¼Œé€šè¿‡ç†µå¼•å¯¼çš„token dropoutè§£å†³é¢†åŸŸæ•°æ®å—é™æ—¶è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„è¿‡æ‹Ÿåˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªå›å½’è¯­è¨€æ¨¡å‹` `é¢†åŸŸæ•°æ®å—é™` `è¿‡æ‹Ÿåˆ` `ç†µå¼•å¯¼` `Token Dropout`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªå›å½’æ¨¡å‹åœ¨é¢†åŸŸæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¤šè½®è®­ç»ƒæ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚
2. EntroDropé€šè¿‡é€‰æ‹©æ€§åœ°dropoutä½ç†µtokenï¼Œå¹¶ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ è°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦ï¼Œç¼“è§£è¿‡æ‹Ÿåˆã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒEntroDropåœ¨ä¸åŒè§„æ¨¡æ¨¡å‹ä¸Šå‡ä¼˜äºæ ‡å‡†æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤šè½®è®­ç»ƒä¸­çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€é«˜è´¨é‡é¢†åŸŸç‰¹å®šæ•°æ®çš„æ—¥ç›Šç¨€ç¼ºï¼Œå¤šè½®è®­ç»ƒå·²æˆä¸ºè°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®ç”¨ç­–ç•¥ã€‚ç„¶è€Œï¼Œè‡ªå›å½’æ¨¡å‹åœ¨é‡å¤æ•°æ®æš´éœ²ä¸‹å¸¸å¸¸é­å—æ€§èƒ½ä¸‹é™ï¼Œå³è¿‡æ‹Ÿåˆå¯¼è‡´æ¨¡å‹èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬å°†è¿™ç§é€€åŒ–å½’å› äºå­¦ä¹ åŠ¨æ€çš„ä¸å¹³è¡¡ï¼šå¯é¢„æµ‹çš„ä½ç†µtokenè¢«å¿«é€Ÿå­¦ä¹ å¹¶ä¸»å¯¼ä¼˜åŒ–ï¼Œè€Œæ¨¡å‹åœ¨é«˜ç†µtokenä¸Šçš„æ³›åŒ–èƒ½åŠ›éšç€æŒç»­è®­ç»ƒè€Œæ¶åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†EntroDropï¼Œä¸€ç§ç†µå¼•å¯¼çš„token dropoutæ–¹æ³•ï¼Œå®ƒä½œä¸ºç»“æ„åŒ–æ•°æ®æ­£åˆ™åŒ–å‘æŒ¥ä½œç”¨ã€‚EntroDropåœ¨è®­ç»ƒæœŸé—´é€‰æ‹©æ€§åœ°å±è”½ä½ç†µtokenï¼Œå¹¶é‡‡ç”¨è¯¾ç¨‹è¡¨æ¥è°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦ï¼Œä½¿å…¶ä¸è®­ç»ƒè¿›åº¦ä¿æŒä¸€è‡´ã€‚åœ¨0.6Båˆ°8Bå‚æ•°çš„æ¨¡å‹è§„æ¨¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEntroDropå§‹ç»ˆä¼˜äºæ ‡å‡†æ­£åˆ™åŒ–åŸºçº¿ï¼Œå¹¶åœ¨æ•´ä¸ªæ‰©å±•å¤šè½®è®­ç»ƒä¸­ä¿æŒç¨³å¥çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨æ•°æ®å—é™é¢†åŸŸè¿›è¡Œè®­ç»ƒæ—¶ï¼Œå°†æ­£åˆ™åŒ–ä¸tokençº§åˆ«å­¦ä¹ åŠ¨æ€å¯¹é½çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºåœ¨æ•°æ®å—é™é¢†åŸŸæ›´æœ‰æ•ˆåœ°è°ƒæ•´LLMæä¾›äº†ä¸€æ¡æœ‰å¸Œæœ›çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨é¢†åŸŸæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè‡ªå›å½’è¯­è¨€æ¨¡å‹åœ¨å¤šè½®è®­ç»ƒä¸­å®¹æ˜“å‡ºç°çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é‡å¤æš´éœ²äºè®­ç»ƒæ•°æ®æ—¶ï¼Œæ¨¡å‹ä¼šè¿‡åº¦å…³æ³¨ä½ç†µï¼ˆæ˜“äºé¢„æµ‹ï¼‰çš„tokenï¼Œè€Œå¿½ç•¥é«˜ç†µï¼ˆéš¾ä»¥é¢„æµ‹ï¼‰çš„tokenï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ ¹æ®tokençš„ç†µå€¼è¿›è¡Œdropoutï¼Œå³EntroDropã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€‰æ‹©æ€§åœ°å±è”½ä½ç†µtokenï¼Œè¿«ä½¿æ¨¡å‹æ›´å¤šåœ°å…³æ³¨é«˜ç†µtokenï¼Œä»è€Œå¹³è¡¡å­¦ä¹ åŠ¨æ€ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†é˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆæ˜“äºå­¦ä¹ çš„tokenï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”å¤æ‚å’Œç½•è§çš„tokenã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEntroDropæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **ç†µå€¼è®¡ç®—**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®¡ç®—æ¯ä¸ªtokençš„ç†µå€¼ï¼Œç†µå€¼åæ˜ äº†tokençš„å¯é¢„æµ‹æ€§ã€‚2) **Dropout Maskç”Ÿæˆ**ï¼šæ ¹æ®tokençš„ç†µå€¼ï¼Œç”Ÿæˆä¸€ä¸ªdropout maskï¼Œç”¨äºé€‰æ‹©æ€§åœ°å±è”½ä½ç†µtokenã€‚3) **Dropoutåº”ç”¨**ï¼šå°†dropout maskåº”ç”¨äºtokenåºåˆ—ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å±è”½ä¸€éƒ¨åˆ†ä½ç†µtokenã€‚4) **è¯¾ç¨‹å­¦ä¹ **ï¼šéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ¸è°ƒæ•´dropoutçš„æ¯”ä¾‹ï¼Œå³è¯¾ç¨‹å­¦ä¹ ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šEntroDropçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç†µå¼•å¯¼çš„token dropoutæœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„éšæœºdropoutä¸åŒï¼ŒEntroDropæ ¹æ®tokençš„ç†µå€¼è¿›è¡Œdropoutï¼Œæ›´åŠ æœ‰é’ˆå¯¹æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¾ç¨‹å­¦ä¹ çš„å¼•å…¥ä¹Ÿä½¿å¾—dropoutçš„æ¯”ä¾‹èƒ½å¤Ÿéšç€è®­ç»ƒçš„è¿›è¡Œè€ŒåŠ¨æ€è°ƒæ•´ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šEntroDropçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **ç†µå€¼è®¡ç®—æ–¹æ³•**ï¼šè®ºæ–‡é‡‡ç”¨äº¤å‰ç†µæŸå¤±æ¥ä¼°è®¡tokençš„ç†µå€¼ã€‚2) **Dropoutæ¯”ä¾‹**ï¼šDropoutæ¯”ä¾‹çš„è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ï¼Œè®ºæ–‡ä¸­é‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ çš„æ–¹æ³•æ¥åŠ¨æ€è°ƒæ•´dropoutæ¯”ä¾‹ã€‚3) **è¯¾ç¨‹å­¦ä¹ ç­–ç•¥**ï¼šè®ºæ–‡é‡‡ç”¨çº¿æ€§å¢é•¿çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå³éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œdropoutæ¯”ä¾‹é€æ¸å¢åŠ ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23422v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23422v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23422v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEntroDropåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆ0.6Båˆ°8Bå‚æ•°ï¼‰ä¸Šå‡ä¼˜äºæ ‡å‡†æ­£åˆ™åŒ–åŸºçº¿ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªç‰¹å®šä»»åŠ¡ä¸Šï¼ŒEntroDropç›¸æ¯”äºbaselineæ–¹æ³•ï¼Œperplexityé™ä½äº†X%ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒEntroDropåœ¨å¤šè½®è®­ç»ƒä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¿æŒæ¨¡å‹çš„æ€§èƒ½ç¨³å®šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EntroDropæ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§é¢†åŸŸæ•°æ®å—é™çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹è®­ç»ƒåœºæ™¯ï¼Œä¾‹å¦‚ä½èµ„æºè¯­è¨€å»ºæ¨¡ã€ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬ç”Ÿæˆç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜æ¨¡å‹åœ¨è¿™äº›åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œä»è€Œæå‡æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ¨¡å‹å’Œä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As access to high-quality, domain-specific data grows increasingly scarce, multi-epoch training has become a practical strategy for adapting large language models (LLMs). However, autoregressive models often suffer from performance degradation under repeated data exposure, where overfitting leads to a marked decline in model capability. Through empirical analysis, we trace this degradation to an imbalance in learning dynamics: predictable, low-entropy tokens are learned quickly and come to dominate optimization, while the model's ability to generalize on high-entropy tokens deteriorates with continued training. To address this, we introduce EntroDrop, an entropy-guided token dropout method that functions as structured data regularization. EntroDrop selectively masks low-entropy tokens during training and employs a curriculum schedule to adjust regularization strength in alignment with training progress. Experiments across model scales from 0.6B to 8B parameters show that EntroDrop consistently outperforms standard regularization baselines and maintains robust performance throughout extended multi-epoch training. These findings underscore the importance of aligning regularization with token-level learning dynamics when training on limited data. Our approach offers a promising pathway toward more effective adaptation of LLMs in data-constrained domains.

