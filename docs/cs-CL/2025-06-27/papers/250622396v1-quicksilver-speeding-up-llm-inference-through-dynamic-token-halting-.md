---
layout: default
title: QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization
---

# QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22396" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22396v1</a>
  <a href="https://arxiv.org/pdf/2506.22396.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22396v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22396v1', 'QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: Preprint. Under submission

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQuickSilverä»¥åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†ä¼˜åŒ–` `åŠ¨æ€ä»¤ç‰Œåœæ­¢` `KVç¼“å­˜è·³è¿‡` `ä¸Šä¸‹æ–‡ä»¤ç‰Œèåˆ` `è‡ªé€‚åº”é‡åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸­å­˜åœ¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªå›å½’è§£ç æ—¶ï¼Œå¯¼è‡´å»¶è¿Ÿå’Œèƒ½è€—é«˜ã€‚
2. QuickSilveræå‡ºäº†ä¸€ç§æ¨¡å—åŒ–çš„ä»¤ç‰Œçº§æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€ä»¤ç‰Œåœæ­¢ã€KVç¼“å­˜è·³è¿‡å’Œä¸Šä¸‹æ–‡ä»¤ç‰Œèåˆç­‰æœºåˆ¶ï¼Œå®ç°æ¨ç†æ—¶çš„è¯­ä¹‰é€‚åº”æ€§ã€‚
3. åœ¨å®éªŒä¸­ï¼ŒQuickSilveråœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šåº”ç”¨äºGPT-2å’ŒLlama-2ï¼Œå–å¾—äº†é«˜è¾¾39.6%çš„FLOPå‡å°‘ï¼Œä¸”å›°æƒ‘åº¦å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²ä¸­å æ®äº†å¤§éƒ¨åˆ†å»¶è¿Ÿå’Œèƒ½è€—ï¼Œé€šå¸¸è¶…è¿‡90%çš„æ€»æˆæœ¬ã€‚å°½ç®¡è®­ç»ƒæ•ˆç‡å·²æœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†è¿è¡Œæ—¶ä¼˜åŒ–ä»æ˜¯å…³é”®ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨è‡ªå›å½’è§£ç ä¸‹ã€‚ç°æœ‰æ–¹æ³•å¦‚å‰ªæã€é‡åŒ–ã€æ—©æœŸé€€å‡ºå’Œæ¨æµ‹è§£ç ï¼Œå¾€å¾€éœ€è¦é‡æ–°è®­ç»ƒã€æ¶æ„æ›´æ”¹æˆ–ç ´åè§£ç å…¼å®¹æ€§ã€‚æˆ‘ä»¬æå‡ºäº†QuickSilverï¼Œä¸€ä¸ªæ¨¡å—åŒ–çš„ã€åŸºäºä»¤ç‰Œçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶å®ç°è¯­ä¹‰é€‚åº”æ€§ï¼Œè€Œæ— éœ€æ”¹å˜æ¨¡å‹æƒé‡æˆ–ç»“æ„ã€‚QuickSilveré›†æˆäº†å››ä¸ªååŒæœºåˆ¶ï¼šåŠ¨æ€ä»¤ç‰Œåœæ­¢ã€KVç¼“å­˜è·³è¿‡ã€ä¸Šä¸‹æ–‡ä»¤ç‰Œèåˆå’Œè‡ªé€‚åº”Matryoshkaé‡åŒ–ã€‚ä¸æ¨æµ‹è§£ç æˆ–MoEè·¯ç”±ä¸åŒï¼ŒQuickSilverå®Œå…¨åœ¨å†»ç»“çš„å¯†é›†æ¨¡å‹ä¸Šè¿è¡Œï¼Œå¹¶ä¸”ä¸éœ€è¦è¾…åŠ©ç½‘ç»œã€‚åœ¨WikiText-103å’ŒC4ä¸Šåº”ç”¨äºGPT-2å’ŒLlama-2ï¼ŒQuickSilverå®ç°äº†é«˜è¾¾39.6%çš„FLOPå‡å°‘ï¼Œä¸”å›°æƒ‘åº¦é™å¹…å¾®ä¹å…¶å¾®ï¼ˆ<=0.2ï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„å»¶è¿Ÿå’Œèƒ½è€—é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚å‰ªæå’Œé‡åŒ–ç­‰ï¼Œå¾€å¾€éœ€è¦é‡æ–°è®­ç»ƒæˆ–æ”¹å˜æ¨¡å‹æ¶æ„ï¼Œå½±å“è§£ç å…¼å®¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šQuickSilverçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸æ”¹å˜æ¨¡å‹æƒé‡æˆ–ç»“æ„çš„æ–¹å¼ï¼Œå®ç°æ¨ç†æ—¶çš„è¯­ä¹‰é€‚åº”æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€ä»¤ç‰Œåœæ­¢ç­‰æœºåˆ¶ï¼Œä¼˜åŒ–è®¡ç®—è¿‡ç¨‹ï¼Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQuickSilverçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šåŠ¨æ€ä»¤ç‰Œåœæ­¢ã€KVç¼“å­˜è·³è¿‡ã€ä¸Šä¸‹æ–‡ä»¤ç‰Œèåˆå’Œè‡ªé€‚åº”Matryoshkaé‡åŒ–ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œä»¥æé«˜æ¨ç†æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šQuickSilverçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–è¾…åŠ©ç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œå®Œå…¨åœ¨å†»ç»“çš„å¯†é›†æ¨¡å‹ä¸Šè¿è¡Œã€‚è¿™ä¸ç°æœ‰çš„æ¨æµ‹è§£ç å’ŒMoEè·¯ç”±æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒQuickSilveré€šè¿‡åŠ¨æ€ä»¤ç‰Œåœæ­¢æœºåˆ¶æ¥ä¸­æ–­è®¡ç®—ï¼ŒKVç¼“å­˜è·³è¿‡æœºåˆ¶æ¥å‡å°‘å†…å­˜å†™å…¥ï¼Œä¸Šä¸‹æ–‡ä»¤ç‰Œèåˆæœºåˆ¶æ¥åˆå¹¶å†—ä½™ä»¤ç‰Œï¼Œä»è€Œæœ‰æ•ˆç¼©çŸ­åºåˆ—é•¿åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒQuickSilveråœ¨WikiText-103å’ŒC4æ•°æ®é›†ä¸Šåº”ç”¨äºGPT-2å’ŒLlama-2ï¼ŒæˆåŠŸå®ç°äº†é«˜è¾¾39.6%çš„FLOPå‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†å›°æƒ‘åº¦çš„å¾®å°é™å¹…ï¼ˆ<=0.2ï¼‰ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ¨ç†æ•ˆç‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

QuickSilverçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚å…¶é«˜æ•ˆçš„æ¨ç†æœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—é™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæé«˜å®æ—¶å“åº”èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms:
>   (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length.
>   Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).

