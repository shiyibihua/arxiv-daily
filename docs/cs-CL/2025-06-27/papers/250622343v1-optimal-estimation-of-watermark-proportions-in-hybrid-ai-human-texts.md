---
layout: default
title: Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts
---

# Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22343" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22343v1</a>
  <a href="https://arxiv.org/pdf/2506.22343.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22343v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22343v1', 'Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiang Li, Garrett Wen, Weiqing He, Jiayuan Wu, Qi Long, Weijie J. Su

**åˆ†ç±»**: stat.ML, cs.CL, cs.LG, stat.ME

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœ€ä¼˜ä¼°è®¡æ–¹æ³•ä»¥è§£å†³æ··åˆæ¥æºæ–‡æœ¬æ°´å°æ¯”ä¾‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬æ°´å°` `æ··åˆæ¥æºæ–‡æœ¬` `å‚æ•°ä¼°è®¡` `å…³é”®ç»Ÿè®¡é‡` `å†…å®¹å®¡æ ¸` `ä¿¡æ¯å®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ•´ä¸ªæ–‡æœ¬çš„æ°´å°åˆ¤æ–­ï¼Œç¼ºä¹å¯¹æ··åˆæ¥æºæ–‡æœ¬ä¸­æ°´å°æ¯”ä¾‹çš„æœ‰æ•ˆä¼°è®¡ã€‚
2. æœ¬æ–‡æå‡ºåŸºäºå…³é”®ç»Ÿè®¡é‡çš„æ··åˆæ¨¡å‹ï¼Œä¼˜åŒ–ä¼°è®¡æ··åˆæ¥æºæ–‡æœ¬ä¸­çš„æ°´å°æ¯”ä¾‹ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚
3. é€šè¿‡åœ¨åˆæˆæ•°æ®å’Œå®é™…ç”Ÿæˆçš„æ··åˆæ–‡æœ¬ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†æ‰€æä¼°è®¡å™¨åœ¨å‡†ç¡®æ€§ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œæ–‡æœ¬æ°´å°æ˜¯æ£€æµ‹åˆæˆæ–‡æœ¬çš„é‡è¦å·¥å…·ï¼Œèƒ½å¤ŸåŒºåˆ†äººç±»æ’°å†™çš„å†…å®¹ä¸LLMç”Ÿæˆçš„æ–‡æœ¬ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ¤æ–­æ•´ä¸ªæ–‡æœ¬æ˜¯å¦è¢«æ°´å°æ ‡è®°ï¼Œè€Œç°å®åœºæ™¯ä¸­å¸¸å¸¸æ¶‰åŠæ··åˆæ¥æºçš„æ–‡æœ¬ï¼Œå³äººç±»æ’°å†™ä¸æ°´å°å†…å®¹çš„ç»“åˆã€‚æœ¬æ–‡é’ˆå¯¹æ··åˆæ¥æºæ–‡æœ¬ä¸­çš„æ°´å°æ¯”ä¾‹ä¼˜åŒ–ä¼°è®¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå…³é”®ç»Ÿè®¡é‡çš„æ··åˆæ¨¡å‹å‚æ•°ä¼°è®¡æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æŸäº›æ°´å°æ–¹æ¡ˆä¸­ï¼Œè¯¥å‚æ•°ç”šè‡³ä¸å¯è¯†åˆ«ï¼Œä½†å¯¹äºé‡‡ç”¨è¿ç»­å…³é”®ç»Ÿè®¡é‡çš„æ°´å°æ–¹æ³•ï¼Œåœ¨æ¸©å’Œæ¡ä»¶ä¸‹è¯¥æ¯”ä¾‹å‚æ•°æ˜¯å¯è¯†åˆ«çš„ã€‚æˆ‘ä»¬æå‡ºäº†é«˜æ•ˆçš„ä¼°è®¡å™¨ï¼Œå¹¶åœ¨åˆæˆæ•°æ®å’Œå¼€æºæ¨¡å‹ç”Ÿæˆçš„æ··åˆæ–‡æœ¬ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„ä¼°è®¡å™¨åœ¨å‡†ç¡®æ€§ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯æ··åˆæ¥æºæ–‡æœ¬ä¸­æ°´å°æ¯”ä¾‹çš„æœ€ä¼˜ä¼°è®¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ··åˆæ–‡æœ¬æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆè¯†åˆ«æ°´å°æ¯”ä¾‹ï¼Œå¯¼è‡´ä¼°è®¡ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬å°†æ°´å°æ¯”ä¾‹ä¼°è®¡é—®é¢˜è½¬åŒ–ä¸ºåŸºäºå…³é”®ç»Ÿè®¡é‡çš„æ··åˆæ¨¡å‹å‚æ•°ä¼°è®¡ã€‚é€šè¿‡åˆ†æä¸åŒæ°´å°æ–¹æ¡ˆçš„å¯è¯†åˆ«æ€§ï¼Œæå‡ºäº†é€‚ç”¨äºè¿ç»­å…³é”®ç»Ÿè®¡é‡çš„é«˜æ•ˆä¼°è®¡å™¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å…³é”®ç»Ÿè®¡é‡è®¡ç®—ã€å‚æ•°ä¼°è®¡å’Œæ¨¡å‹è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå¯¹æ··åˆæ–‡æœ¬è¿›è¡Œåˆ†æï¼Œæå–å…³é”®ç»Ÿè®¡é‡ï¼Œç„¶ååˆ©ç”¨è¿™äº›ç»Ÿè®¡é‡è¿›è¡Œæ°´å°æ¯”ä¾‹çš„ä¼°è®¡ï¼Œæœ€åé€šè¿‡å®éªŒéªŒè¯ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„ä¼°è®¡æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æŸäº›æ°´å°æ–¹æ¡ˆä¸­å®ç°æ¯”ä¾‹å‚æ•°çš„å¯è¯†åˆ«æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ä¸å¯è¯†åˆ«æ€§å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¼°è®¡å™¨çš„è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œå¹¶ç¡®ä¿å…¶åœ¨ä¸åŒæ°´å°æ–¹æ¡ˆä¸‹çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨äº†å¤šç§æµè¡Œçš„æ— åæ°´å°ä½œä¸ºç¤ºä¾‹ï¼ŒéªŒè¯äº†æ–¹æ³•çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æä¼°è®¡å™¨åœ¨åˆæˆæ•°æ®å’Œæ··åˆæ¥æºæ–‡æœ¬ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®æ€§æ˜¾è‘—é«˜äºåŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚æ··åˆåœºæ™¯ä¸­ï¼Œä¼°è®¡è¯¯å·®é™ä½äº†20%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€å†…å®¹å®¡æ ¸å’Œä¿¡æ¯å®‰å…¨ç­‰ã€‚é€šè¿‡å‡†ç¡®ä¼°è®¡æ··åˆæ–‡æœ¬ä¸­çš„æ°´å°æ¯”ä¾‹ï¼Œå¯ä»¥æœ‰æ•ˆè¯†åˆ«åˆæˆå†…å®¹ï¼Œæå‡å†…å®¹çš„å¯ä¿¡åº¦å’Œå®‰å…¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text watermarks in large language models (LLMs) are an increasingly important tool for detecting synthetic text and distinguishing human-written content from LLM-generated text. While most existing studies focus on determining whether entire texts are watermarked, many real-world scenarios involve mixed-source texts, which blend human-written and watermarked content. In this paper, we address the problem of optimally estimating the watermark proportion in mixed-source texts. We cast this problem as estimating the proportion parameter in a mixture model based on \emph{pivotal statistics}. First, we show that this parameter is not even identifiable in certain watermarking schemes, let alone consistently estimable. In stark contrast, for watermarking methods that employ continuous pivotal statistics for detection, we demonstrate that the proportion parameter is identifiable under mild conditions. We propose efficient estimators for this class of methods, which include several popular unbiased watermarks as examples, and derive minimax lower bounds for any measurable estimator based on pivotal statistics, showing that our estimators achieve these lower bounds. Through evaluations on both synthetic data and mixed-source text generated by open-source models, we demonstrate that our proposed estimators consistently achieve high estimation accuracy.

