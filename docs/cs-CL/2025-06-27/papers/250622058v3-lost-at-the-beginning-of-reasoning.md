---
layout: default
title: Lost at the Beginning of Reasoning
---

# Lost at the Beginning of Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22058" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22058v3</a>
  <a href="https://arxiv.org/pdf/2506.22058.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22058v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22058v3', 'Lost at the Beginning of Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Baohao Liao, Xinyi Chen, Sara Rajaee, Yuhui Xu, Christian Herold, Anders SÃ¸gaard, Maarten de Rijke, Christof Monz

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-10-18)

**å¤‡æ³¨**: remove the benchmark part. (10 pages, 6 figures, 5 tables)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆé‡‡æ ·ç­–ç•¥ä»¥ä¼˜åŒ–æ¨ç†åˆæ­¥æ­¥éª¤**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é“¾å¼æ¨ç†` `è‡ªæˆ‘ä¿®æ­£` `æ¨ç†ä¼˜åŒ–` `å¥–åŠ±æ¨¡å‹` `æ¨ç†æ•ˆç‡` `æˆæœ¬é™ä½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é•¿é“¾æ¨ç†ä¸­è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´æ¨ç†è´¨é‡å—æŸã€‚
2. æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¥–åŠ±æ¨¡å‹çš„é«˜æ•ˆé‡‡æ ·ç­–ç•¥ï¼Œä¸“æ³¨äºä¼˜åŒ–ç¬¬ä¸€æ¨ç†æ­¥éª¤ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ¨ç†æˆæœ¬é™ä½äº†70%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•æ˜¾è‘—æå‡äº†å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ‰©å±•çš„é“¾å¼æ¨ç†ï¼ˆCoTï¼‰æœºåˆ¶ã€‚ç„¶è€Œï¼ŒLLMsåœ¨é•¿é“¾æ¨ç†ä¸­çš„è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†çš„ç¬¬ä¸€æ­¥å¯¹æœ€ç»ˆé¢„æµ‹æœ‰ç€ä¸æˆæ¯”ä¾‹çš„å½±å“ï¼Œé”™è¯¯çš„å¼•å…¥ä¼šæ˜¾è‘—é™ä½åç»­æ¨ç†è´¨é‡ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„é‡‡æ ·ç­–ç•¥ï¼Œåˆ©ç”¨å¥–åŠ±æ¨¡å‹è¯†åˆ«å’Œä¿ç•™é«˜è´¨é‡çš„ç¬¬ä¸€æ¨ç†æ­¥éª¤ï¼ŒåŒæ—¶ä¸¢å¼ƒæ¬¡ä¼˜æ­¥éª¤ï¼Œå®ç°æ¨ç†æˆæœ¬é™ä½70%ï¼Œè€Œä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†ç¬¬ä¸€æ¨ç†æ­¥éª¤åœ¨ç”Ÿæˆé«˜è´¨é‡æ¨ç†è½¨è¿¹ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„é«˜æ•ˆé‡‡æ ·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾æ¨ç†ä¸­è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç¬¬ä¸€æ¨ç†æ­¥éª¤çš„é”™è¯¯å¯¹åç»­æ¨ç†è´¨é‡çš„å½±å“ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œä¼˜åŒ–è¿™ä¸€å…³é”®æ­¥éª¤ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¥–åŠ±æ¨¡å‹æ¥è¯†åˆ«å’Œä¿ç•™é«˜è´¨é‡çš„ç¬¬ä¸€æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æ•´ä½“æ¨ç†è´¨é‡å’Œæ•ˆç‡ã€‚é€šè¿‡ä¼˜åŒ–ç¬¬ä¸€æ­¥ï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘åç»­æ¨ç†ä¸­çš„é”™è¯¯ä¼ æ’­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€ç¬¬ä¸€æ¨ç†æ­¥éª¤ç”Ÿæˆã€å¥–åŠ±æ¨¡å‹è¯„ä¼°å’Œæœ€ç»ˆæ¨ç†ç»“æœè¾“å‡ºå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆç”Ÿæˆåˆæ­¥æ¨ç†æ­¥éª¤ï¼Œç„¶åé€šè¿‡å¥–åŠ±æ¨¡å‹è¯„ä¼°å…¶è´¨é‡ï¼Œæœ€åé€‰æ‹©é«˜è´¨é‡æ­¥éª¤è¿›è¡Œåç»­æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°å’Œä¼˜åŒ–ç¬¬ä¸€æ¨ç†æ­¥éª¤ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ¨ç†æ¨¡å‹ä¸åŒï¼Œå¼ºè°ƒäº†æ¨ç†è¿‡ç¨‹ä¸­çš„å…³é”®åˆå§‹æ­¥éª¤ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œè®¾ç½®äº†å¥–åŠ±æ¨¡å‹çš„å‚æ•°ä»¥ä¼˜åŒ–è¯„ä¼°æ ‡å‡†ï¼ŒåŒæ—¶è®¾è®¡äº†æŸå¤±å‡½æ•°ä»¥å¹³è¡¡æ¨ç†è´¨é‡å’Œè®¡ç®—æˆæœ¬ï¼Œç¡®ä¿åœ¨é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹çš„ç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æ”¯æŒé«˜æ•ˆçš„æ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°æå‡ºçš„é‡‡æ ·ç­–ç•¥åï¼Œæ¨ç†æˆæœ¬é™ä½äº†70%ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸºçº¿æ¨¡å‹ç›¸åŒçš„å‡†ç¡®æ€§ã€‚è¿™ä¸€æ˜¾è‘—æå‡å±•ç¤ºäº†ç¬¬ä¸€æ¨ç†æ­¥éª¤åœ¨æ•´ä½“æ¨ç†è´¨é‡ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨åŒ–æ¨ç†å·¥å…·å’Œå¤æ‚å†³ç­–æ”¯æŒç³»ç»Ÿã€‚é€šè¿‡ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸæé«˜æ¨¡å‹çš„å“åº”é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in large language models (LLMs) have significantly advanced complex reasoning capabilities, particularly through extended chain-of-thought (CoT) reasoning that incorporates mechanisms such as backtracking, self-reflection, and self-correction. Despite these developments, the self-correction abilities of LLMs during long CoT reasoning remain underexplored. And recent findings on overthinking suggest that such models often engage in unnecessarily redundant reasoning. In this work, we empirically show that the first reasoning step exerts a disproportionately large influence on the final prediction. I.e., errors introduced at this stage can substantially degrade subsequent reasoning quality. This phenomenon is consistently observed across various state-of-the-art open- and closed-source reasoning models. Leveraging this insight, we propose an efficient sampling strategy that leverages a reward model to identify and retain high-quality first reasoning steps while discarding suboptimal ones, achieving up to a 70% reduction in inference cost without sacrificing any accuracy. Our work highlights the central role of the first reasoning step in generating a high-quality reasoning trajectory, and thus enabling significantly efficient sampling.

