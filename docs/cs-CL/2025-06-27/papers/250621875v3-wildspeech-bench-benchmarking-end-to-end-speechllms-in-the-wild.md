---
layout: default
title: WildSpeech-Bench: Benchmarking End-to-End SpeechLLMs in the Wild
---

# WildSpeech-Bench: Benchmarking End-to-End SpeechLLMs in the Wild

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21875" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21875v3</a>
  <a href="https://arxiv.org/pdf/2506.21875.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21875v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21875v3', 'WildSpeech-Bench: Benchmarking End-to-End SpeechLLMs in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Linhao Zhang, Jian Zhang, Bokai Lei, Chuhan Wu, Aiwei Liu, Wei Jia, Xiao Zhou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-09-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWildSpeech-Benchä»¥è§£å†³è¯­éŸ³LLMè¯„ä¼°ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³è¯†åˆ«` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `è¯„ä¼°åŸºå‡†` `æŸ¥è¯¢æ„ŸçŸ¥è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¤šåŸºäºæ–‡æœ¬ï¼Œå¿½è§†äº†è¯­éŸ³çš„ç‹¬ç‰¹ç‰¹æ€§ï¼Œå¯¼è‡´æ— æ³•å…¨é¢è¯„ä¼°è¯­éŸ³LLMçš„æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºäº†WildSpeech-BenchåŸºå‡†ï¼Œç³»ç»Ÿæ•´ç†çœŸå®è¯­éŸ³å¯¹è¯æ•°æ®ï¼Œå¹¶å¼•å…¥å¤šæ ·åŒ–çš„è¯´è¯è€…å’Œå£°å­¦æ¡ä»¶ã€‚
3. é€šè¿‡æŸ¥è¯¢æ„ŸçŸ¥è¯„ä¼°æ–¹æ³•ï¼Œæå‡äº†è‡ªåŠ¨è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œå®éªŒç»“æœæ˜¾ç¤ºä¸åŒæ¨¡å‹åœ¨è¯­éŸ³åœºæ™¯ä¸‹è¡¨ç°å·®å¼‚æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚GPT-4oå±•ç°äº†ç›´æ¥è¯­éŸ³äº¤äº’çš„å¼ºå¤§èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç¼ºä¹ä¸“é—¨ä¸”å…¨é¢çš„åŸºå‡†æ¥è¯„ä¼°ç«¯åˆ°ç«¯è¯­éŸ³LLMï¼Œé™åˆ¶äº†éŸ³é¢‘LLMåœ¨å®é™…åº”ç”¨ä¸­çš„ç”¨æˆ·ä½“éªŒä¼˜åŒ–ã€‚ç°æœ‰è¯„ä¼°æ–¹æ³•å¾€å¾€é€‚åº”æ–‡æœ¬åŸºå‡†ï¼Œå¿½è§†äº†è¯­éŸ³çš„ç‹¬ç‰¹ç‰¹æ€§å’ŒæŒ‘æˆ˜ï¼Œå¦‚éŸµå¾‹ã€åŒéŸ³è¯ã€å£åƒåŠä¸åŒç”¨æˆ·æœŸæœ›ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å®é™…è¯­éŸ³å¯¹è¯ä¸­çš„ç«¯åˆ°ç«¯è¯­éŸ³LLMã€‚æˆ‘ä»¬ç³»ç»Ÿæ•´ç†äº†ä¸å£è¯­åœºæ™¯ç›¸å…³çš„çœŸå®èŠå¤©æ•°æ®ï¼Œå¼•å…¥äº†è¯´è¯è€…å±æ€§å’Œå£°å­¦æ¡ä»¶çš„å¤šæ ·æ€§ï¼Œå¹¶ç”¨è¯­éŸ³ç‰¹æœ‰ç°è±¡å¢å¼ºæ•°æ®é›†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†æŸ¥è¯¢æ„ŸçŸ¥è¯„ä¼°æ–¹æ³•ï¼Œåˆ©ç”¨å®šåˆ¶çš„è¯„ä¼°æ¸…å•å’Œæç¤ºæé«˜è‡ªåŠ¨è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹å¤šç§ä¸»æµè¯­éŸ³æ¨¡å‹çš„å…¨é¢æµ‹è¯•å’Œè¯¦ç»†åˆ†æï¼Œæ­ç¤ºäº†ä¸åŒè¯­éŸ³åœºæ™¯ä¸‹æ¨¡å‹æ€§èƒ½çš„æ˜¾è‘—å·®å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­éŸ³LLMè¯„ä¼°ç¼ºä¹ä¸“é—¨åŸºå‡†çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–æ–‡æœ¬åŸºå‡†ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°è¯­éŸ³ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºWildSpeech-BenchåŸºå‡†ï¼Œé€šè¿‡ç³»ç»Ÿæ•´ç†çœŸå®è¯­éŸ³å¯¹è¯æ•°æ®ï¼Œå¢å¼ºè¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ•´ç†ã€ç‰¹æ€§å¢å¼ºå’ŒæŸ¥è¯¢æ„ŸçŸ¥è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ•´ç†é˜¶æ®µæ”¶é›†çœŸå®è¯­éŸ³æ•°æ®ï¼Œç‰¹æ€§å¢å¼ºé˜¶æ®µå¼•å…¥å¤šæ ·åŒ–çš„è¯´è¯è€…å’Œå£°å­¦æ¡ä»¶ï¼Œè¯„ä¼°é˜¶æ®µè®¾è®¡å®šåˆ¶åŒ–çš„è¯„ä¼°æ¸…å•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥æŸ¥è¯¢æ„ŸçŸ¥è¯„ä¼°æ–¹æ³•ï¼Œä½¿å¾—è¯„ä¼°èƒ½å¤Ÿé’ˆå¯¹è¯­éŸ³ç‰¹æœ‰ç°è±¡è¿›è¡Œæ›´ç»†è‡´çš„åˆ†æï¼Œä¸ä¼ ç»Ÿæ–‡æœ¬åŸºå‡†ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºç²¾å‡†çš„è¯„ä¼°ç»“æœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œè€ƒè™‘äº†éŸµå¾‹ã€åŒéŸ³è¯ç­‰è¯­éŸ³ç‰¹æ€§ï¼Œè¯„ä¼°æ–¹æ³•ä¸­ä½¿ç”¨äº†å®šåˆ¶çš„è¯„ä¼°æ¸…å•å’Œæç¤ºï¼Œä»¥æé«˜è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨WildSpeech-BenchåŸºå‡†åï¼Œè¯­éŸ³æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¯„ä¼°å‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼Œå°¤å…¶åœ¨å¤„ç†éŸµå¾‹å’ŒåŒéŸ³è¯æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼Œæ¨¡å‹æ€§èƒ½å·®å¼‚çš„è¯†åˆ«èƒ½åŠ›æå‡äº†çº¦30%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€è¯­éŸ³è¯†åˆ«ç³»ç»ŸåŠäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æä¾›å…¨é¢çš„è¯„ä¼°åŸºå‡†ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…ä¼˜åŒ–è¯­éŸ³æ¨¡å‹ï¼Œæé«˜ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨è¯­éŸ³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent multi-modal Large Language Models (LLMs) such as GPT-4o have demonstrated strong capabilities of direct speech interaction. However, the lack of specialized and comprehensive benchmarks for end-to-end speech LLM evaluation hinders optimizing the user experience of Audio LLMs in real-world applications. Existing evaluation methods often adapt text-based benchmarks, overlooking speech's unique characteristics and challenges, including prosody, homophones, stuttering, and differing user expectations. Here, we introduce the first comprehensive benchmark designed to systematically evaluate end-to-end speechLLMs in practical speech conversations. We systematically curate real-world chat data relevant to spoken scenarios, introduce diversity in speaker attributes and acoustic conditions, and augment the dataset with speech-specific phenomena. We further design a query-aware evaluation method to use customized evaluation checklists and prompts to enhance the accuracy of automatic evaluation. We conduct comprehensive testing and detailed analysis of various mainstream speech models, revealing significant differences in model performance across different speech scenarios. The use of query-aware evaluation further enables a finer-grained assessment under various speech-specific scenarios. Our benchmark can provide valuable insights for speech model development and evaluation.

