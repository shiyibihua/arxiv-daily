---
layout: default
title: Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation
---

# Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21876" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21876v1</a>
  <a href="https://arxiv.org/pdf/2506.21876.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21876v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21876v1', 'Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qiyue Gao, Xinyu Pi, Kevin Liu, Junrong Chen, Ruolan Yang, Xinqi Huang, Xinyu Fang, Lu Sun, Gautham Kishore, Bo Ai, Stone Tao, Mengyang Liu, Jiaxi Yang, Chao-Jung Lai, Chuanyang Jin, Jiannan Xiang, Benhao Huang, Zeming Chen, David Danks, Hao Su, Tianmin Shu, Ziqiao Ma, Lianhui Qin, Zhiting Hu

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: ACL 2025 (Findings)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸¤é˜¶æ®µæ¡†æ¶è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„å†…éƒ¨ä¸–ç•Œæ¨¡å‹èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å†…éƒ¨ä¸–ç•Œæ¨¡å‹` `è¯„ä¼°æ¡†æ¶` `æ„ŸçŸ¥èƒ½åŠ›` `é¢„æµ‹èƒ½åŠ›` `WM-ABench` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸºæœ¬ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œç¼ºä¹ç³»ç»Ÿçš„è¯„ä¼°æ–¹æ³•ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ†åˆ«é’ˆå¯¹æ„ŸçŸ¥å’Œé¢„æµ‹èƒ½åŠ›è¿›è¡Œè¯„ä¼°ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚
3. é€šè¿‡660æ¬¡å®éªŒï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨åŸºæœ¬èƒ½åŠ›ä¸Šè¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨è¿åŠ¨è½¨è¿¹åŒºåˆ†å’Œè§£è€¦ç†è§£æ–¹é¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å†…éƒ¨ä¸–ç•Œæ¨¡å‹ï¼ˆWMsï¼‰ä½¿ä»£ç†èƒ½å¤Ÿç†è§£ä¸–ç•ŒçŠ¶æ€å¹¶é¢„æµ‹è½¬å˜ï¼Œæ˜¯é«˜çº§æ¨ç†çš„åŸºç¡€ã€‚å°½ç®¡æœ€æ–°çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚OpenAI o3ã€GPT-4oå’ŒGeminiå±•ç°å‡ºä½œä¸ºé€šç”¨WMsçš„æ½œåŠ›ï¼Œä½†å¯¹å…¶åŸºæœ¬WMèƒ½åŠ›çš„ç³»ç»Ÿè¯„ä¼°ä»ç„¶ç¼ºå¤±ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œè¯„ä¼°æ„ŸçŸ¥ï¼ˆè§†è§‰ã€ç©ºé—´ã€æ—¶é—´ã€æ•°é‡å’Œè¿åŠ¨ï¼‰å’Œé¢„æµ‹ï¼ˆæœºæ¢°æ¨¡æ‹Ÿã€ä¼ é€’æ¨ç†ã€ç»„åˆæ¨ç†ï¼‰ï¼Œå¹¶å¼•å…¥WM-ABenchåŸºå‡†ï¼Œæ¶µç›–23ä¸ªç»†åˆ†è¯„ä¼°ç»´åº¦å’Œ6ä¸ªå¤šæ ·åŒ–çš„æ¨¡æ‹Ÿç¯å¢ƒã€‚é€šè¿‡660æ¬¡å®éªŒï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨åŸºæœ¬ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå‡ ä¹æ‰€æœ‰æ¨¡å‹åœ¨åŒºåˆ†è¿åŠ¨è½¨è¿¹æ—¶è¡¨ç°æ¥è¿‘éšæœºå‡†ç¡®ç‡ï¼Œä¸”ç¼ºä¹è§£è€¦ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å†…éƒ¨ä¸–ç•Œæ¨¡å‹èƒ½åŠ›è¯„ä¼°ä¸Šçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½ç³»ç»Ÿæ€§åœ°è¯„ä¼°å…¶åŸºæœ¬èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹æ¨¡å‹æ€§èƒ½çš„ç†è§£ä¸å¤Ÿå…¨é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ä¸ªåŸºäºæ¯”è¾ƒå¿ƒç†å­¦å’Œè®¤çŸ¥ç§‘å­¦çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œåˆ†åˆ«è¯„ä¼°æ¨¡å‹çš„æ„ŸçŸ¥å’Œé¢„æµ‹èƒ½åŠ›ï¼Œä»¥æä¾›æ›´ç»†è‡´çš„è¯„ä¼°æ ‡å‡†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µè¯„ä¼°æ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§†è§‰ã€ç©ºé—´ã€æ—¶é—´ã€æ•°é‡å’Œè¿åŠ¨ï¼›ç¬¬äºŒé˜¶æ®µè¯„ä¼°é¢„æµ‹èƒ½åŠ›ï¼ŒåŒ…æ‹¬æœºæ¢°æ¨¡æ‹Ÿã€ä¼ é€’æ¨ç†å’Œç»„åˆæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥WM-ABenchåŸºå‡†ï¼Œæ¶µç›–23ä¸ªç»†åˆ†è¯„ä¼°ç»´åº¦ï¼Œæä¾›äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„è¯„ä¼°å·¥å…·ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ä½¿ç”¨äº†660æ¬¡å®éªŒï¼Œæ¶µç›–15ä¸ªæœ€æ–°çš„å•†ä¸šå’Œå¼€æºVLMsï¼Œè®¾è®¡äº†æ§åˆ¶åäº‹å®æ¨¡æ‹Ÿçš„å¤šæ ·åŒ–ç¯å¢ƒï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„ä¸¥è°¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå‡ ä¹æ‰€æœ‰æ¨¡å‹åœ¨åŒºåˆ†è¿åŠ¨è½¨è¿¹æ—¶çš„å‡†ç¡®ç‡æ¥è¿‘éšæœºæ°´å¹³ï¼Œä¸”åœ¨è§£è€¦ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸºæœ¬ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ä¸Šçš„é‡å¤§ä¸è¶³ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ä»£ç†ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©æå‡æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚æœªæ¥ï¼Œéšç€è¯„ä¼°æ–¹æ³•çš„å®Œå–„ï¼Œå¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„å¤šæ¨¡æ€ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs' fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding -- e.g., some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.

