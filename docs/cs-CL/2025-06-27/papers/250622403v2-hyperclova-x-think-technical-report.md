---
layout: default
title: HyperCLOVA X THINK Technical Report
---

# HyperCLOVA X THINK Technical Report

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22403" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22403v2</a>
  <a href="https://arxiv.org/pdf/2506.22403.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22403v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22403v2', 'HyperCLOVA X THINK Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: NAVER Cloud HyperCLOVA X Team

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-07-01)

**å¤‡æ³¨**: 50 pages, 13 figures; fixed figures in the appendix

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHyperCLOVA X THINKä»¥å¢å¼ºæ¨ç†èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `åŒè¯­ä¸€è‡´æ€§` `å¼ºåŒ–å­¦ä¹ ` `ä¸Šä¸‹æ–‡å¤„ç†` `éŸ©è¯­åº”ç”¨` `æ¨¡å‹è’¸é¦` `è®¡ç®—-å†…å­˜å¹³è¡¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›å’ŒåŒè¯­ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç‰¹å®šè¯­è¨€å’Œé¢†åŸŸçš„åº”ç”¨ä¸­ã€‚
2. HyperCLOVA X THINKé€šè¿‡è®¡ç®—-å†…å­˜å¹³è¡¡çš„Peri-LN Transformeræ¶æ„å’Œä¸‰é˜¶æ®µè¯¾ç¨‹é¢„è®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›å’Œä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚
3. åœ¨å¤šä¸ªéŸ©å›½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHyperCLOVA X THINKçš„è¡¨ç°ä¼˜äºåŒç±»æ¨¡å‹ï¼Œå¹¶åœ¨KCSAT STEMåŸºå‡†æµ‹è¯•ä¸­ä¸GPT-4.1ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†HyperCLOVA X THINKï¼Œè¿™æ˜¯HyperCLOVA Xç³»åˆ—ä¸­é¦–ä¸ªä¸“æ³¨äºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé¢„è®­ç»ƒæ•°æ®é‡çº¦ä¸º6ä¸‡äº¿é«˜è´¨é‡éŸ©æ–‡å’Œè‹±æ–‡æ ‡è®°ï¼Œå¹¶é€šè¿‡é’ˆå¯¹æ€§çš„åˆæˆéŸ©æ–‡æ•°æ®è¿›è¡Œå¢å¼ºã€‚è¯¥æ¨¡å‹é‡‡ç”¨è®¡ç®—-å†…å­˜å¹³è¡¡çš„Peri-LN Transformeræ¶æ„ï¼Œç»è¿‡ä¸‰é˜¶æ®µè¯¾ç¨‹é¢„è®­ç»ƒï¼Œæ‰©å±•ä¸Šä¸‹æ–‡çª—å£è‡³128Kæ ‡è®°ï¼Œå¹¶é€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œæ”¯æŒè¯¦ç»†æ¨ç†å’Œç®€æ´å›ç­”æ¨¡å¼ã€‚åœ¨éŸ©å›½ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„åŒè¯­ä¸€è‡´æ€§å’Œç¿»è¯‘è´¨é‡ã€‚æ­¤å¤–ï¼Œè§†è§‰å¢å¼ºå˜ä½“åœ¨KCSAT STEMåŸºå‡†æµ‹è¯•ä¸­ä¸GPT-4.1ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼Œä¸”è®­ç»ƒè®¡ç®—éœ€æ±‚æ˜¾è‘—ä½äºåŒç±»æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å‰ªæå’Œè’¸é¦æŠ€æœ¯ï¼Œæœªæ¥å°†åº”ç”¨äºHyperCLOVA X THINKï¼Œæ—¨åœ¨æ‰“é€ ä¸€ä¸ªå¼€æºä¸”é€‚åˆå•†ä¸šåº”ç”¨çš„åŸºç¡€æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›å’ŒåŒè¯­ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨éŸ©æ–‡å’Œè‹±æ–‡çš„åº”ç”¨åœºæ™¯ä¸­ã€‚ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸”åœ¨ç‰¹å®šè¯­è¨€çš„ç¿»è¯‘è´¨é‡ä¸Šå­˜åœ¨å·®è·ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHyperCLOVA X THINKçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¡ç®—-å†…å­˜å¹³è¡¡çš„Peri-LN Transformeræ¶æ„ï¼Œç»“åˆä¸‰é˜¶æ®µè¯¾ç¨‹é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ›´å¤§ä¸Šä¸‹æ–‡çª—å£å†…è¿›è¡Œæ¨ç†ï¼Œé€‚åº”å¤æ‚çš„è¯­è¨€ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨6ä¸‡äº¿æ ‡è®°è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒï¼›å…¶æ¬¡æ˜¯æ‰©å±•ä¸Šä¸‹æ–‡çª—å£è‡³128Kæ ‡è®°ï¼›æœ€åæ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨æ¨ç†å’Œå›ç­”æ¨¡å¼ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶è®¡ç®—-å†…å­˜å¹³è¡¡çš„Peri-LN Transformeræ¶æ„å’Œä¸‰é˜¶æ®µè¯¾ç¨‹é¢„è®­ç»ƒæ–¹æ³•ï¼Œè¿™ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶èƒ½å¤Ÿä¿æŒé«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹çš„å…³é”®è®¾è®¡åŒ…æ‹¬ä¸Šä¸‹æ–‡çª—å£çš„æ‰©å±•ã€æŸå¤±å‡½æ•°çš„ä¼˜åŒ–ä»¥åŠåœ¨å¾®è°ƒé˜¶æ®µå¼•å…¥å¯éªŒè¯å¥–åŠ±æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æ¨ç†å’Œå›ç­”æ—¶çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HyperCLOVA X THINKåœ¨å¤šä¸ªéŸ©å›½åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨KMMLUã€CSATå’ŒKoBALT-700ç­‰æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½è¶…è¿‡åŒç±»æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè§†è§‰å¢å¼ºå˜ä½“åœ¨KCSAT STEMåŸºå‡†æµ‹è¯•ä¸­ä¸GPT-4.1ç›¸å½“ï¼Œä¸”è®­ç»ƒè®¡ç®—éœ€æ±‚æ˜¾è‘—ä½äºåŒç±»æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HyperCLOVA X THINKçš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ•™è‚²ã€ç¿»è¯‘ã€å®¢æˆ·æœåŠ¡ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›é«˜è´¨é‡çš„åŒè¯­æ”¯æŒå’Œå¤æ‚æ¨ç†èƒ½åŠ›ã€‚å…¶åˆ›æ–°çš„è®­ç»ƒæ–¹æ³•å’Œæ¶æ„è®¾è®¡å°†æ¨åŠ¨éŸ©è¯­åŠå…¶ä»–è¯­è¨€çš„äººå·¥æ™ºèƒ½åº”ç”¨å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with $Î¼$P, pre-trained through a three-stage curriculum that expands the context window to $128$K tokens, and post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes. It delivers competitive performance against similarly sized models on Korea-focused benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while preserving robust bilingual consistency and translation quality. In addition, a vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM benchmark, all of which are achieved with substantially lower training compute than existing models of similar sizes. We also present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model. Altogether, these capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI innovation and a valuable resource for the global research community.

