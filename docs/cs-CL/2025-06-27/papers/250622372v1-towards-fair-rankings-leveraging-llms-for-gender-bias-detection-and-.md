---
layout: default
title: Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement
---

# Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22372" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22372v1</a>
  <a href="https://arxiv.org/pdf/2506.22372.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22372v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22372v1', 'Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maryam Mousavian, Zahra Abbasiantaeb, Mohammad Aliannejadi, Fabio Crestani

**åˆ†ç±»**: cs.IR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: Accepted by ACM SIGIR Conference on Innovative Concepts and Theories in Information Retrieval (ICTIR 2025)

**DOI**: [10.1145/3731120.3744620](https://doi.org/10.1145/3731120.3744620)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ£€æµ‹å’Œæµ‹é‡æ€§åˆ«åè§ä»¥å®ç°å…¬å¹³æ’å**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ€§åˆ«åè§` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯æ£€ç´¢` `å…¬å¹³æ€§è¯„ä¼°` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•°æ®é›†æ„å»º` `ç®—æ³•å…¬å¹³æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ€§åˆ«å…¬å¹³åº¦é‡æ–¹æ³•ä¸»è¦ä¾èµ–äºè¯æ±‡å’Œé¢‘ç‡åŸºç¡€çš„æµ‹é‡ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å¾®å¦™çš„æ€§åˆ«å·®å¼‚ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§åˆ«åè§æ£€æµ‹æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†æ–°çš„æ€§åˆ«å…¬å¹³åº¦é‡CWExï¼Œä»¥å…‹æœç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æåº¦é‡åœ¨å…¬å¹³æ€§è¯„ä¼°ä¸Šæ¯”ä¹‹å‰çš„åº¦é‡æ›´ä¸ºè¯¦ç»†ï¼Œä¸äººç±»æ ‡ç­¾çš„å¯¹é½åº¦æ˜¾è‘—æé«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¤¾ä¼šåè§åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¿¡æ¯æ£€ç´¢ç³»ç»Ÿä¸­çš„å­˜åœ¨æ˜¯ä¸€ä¸ªæŒç»­çš„æŒ‘æˆ˜ï¼Œè¿™å‡¸æ˜¾äº†å¼€å‘å¼ºå¤§æ–¹æ³•ä»¥è¯†åˆ«å’Œè¯„ä¼°è¿™äº›åè§çš„é‡è¦æ€§ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æ£€æµ‹å’Œæµ‹é‡æ®µè½æ’åä¸­çš„æ€§åˆ«åè§ã€‚ç°æœ‰çš„æ€§åˆ«å…¬å¹³åº¦é‡ä¾èµ–äºè¯æ±‡å’Œé¢‘ç‡åŸºç¡€çš„æµ‹é‡ï¼Œå¯¼è‡´å„ç§å±€é™æ€§ï¼Œä¾‹å¦‚é”™è¿‡å¾®å¦™çš„æ€§åˆ«å·®å¼‚ã€‚åŸºäºæˆ‘ä»¬çš„LLMæ€§åˆ«åè§æ£€æµ‹æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ€§åˆ«å…¬å¹³åº¦é‡ï¼Œç§°ä¸ºç±»åŠ æƒæ›å…‰ï¼ˆCWExï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰çš„å±€é™æ€§ã€‚ä¸ºäº†æµ‹é‡æˆ‘ä»¬æå‡ºçš„åº¦é‡çš„æœ‰æ•ˆæ€§å¹¶ç ”ç©¶LLMsåœ¨æ£€æµ‹æ€§åˆ«åè§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯¹MS MARCOæ®µè½æ’åé›†åˆçš„ä¸€ä¸ªå­é›†è¿›è¡Œäº†æ ‡æ³¨ï¼Œå¹¶å‘å¸ƒäº†æ–°çš„æ€§åˆ«åè§é›†åˆMSMGenderBiasï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„åº¦é‡æä¾›äº†æ¯”ä»¥å‰çš„åº¦é‡æ›´è¯¦ç»†çš„å…¬å¹³æ€§è¯„ä¼°ï¼Œä¸”ä¸äººç±»æ ‡ç­¾çš„å¯¹é½åº¦æœ‰æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¿¡æ¯æ£€ç´¢ç³»ç»Ÿä¸­çš„æ€§åˆ«åè§æ£€æµ‹ä¸æµ‹é‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•æ‰å¾®å¦™æ€§åˆ«å·®å¼‚æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¯¼è‡´åè§è¯„ä¼°ä¸å¤Ÿå…¨é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ€§åˆ«åè§çš„æ£€æµ‹ï¼Œå¹¶æå‡ºä¸€ç§æ–°çš„æ€§åˆ«å…¬å¹³åº¦é‡CWExï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ€§åˆ«åè§çš„å­˜åœ¨ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¯†åˆ«å’Œé‡åŒ–æ€§åˆ«åè§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå¯¹MS MARCOæ®µè½æ’åé›†åˆçš„å­é›†è¿›è¡Œæ ‡æ³¨ï¼Œæ„å»ºæ–°çš„æ€§åˆ«åè§æ•°æ®é›†MSMGenderBiasã€‚ç„¶åï¼Œåˆ©ç”¨LLMsè¿›è¡Œæ€§åˆ«åè§æ£€æµ‹ï¼Œå¹¶é€šè¿‡CWExåº¦é‡è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†CWExè¿™ä¸€æ–°çš„æ€§åˆ«å…¬å¹³åº¦é‡ï¼Œèƒ½å¤Ÿæ›´ç»†è‡´åœ°è¯„ä¼°æ€§åˆ«åè§ï¼Œå¹¶ä¸äººç±»æ ‡ç­¾çš„å¯¹é½åº¦æ˜¾è‘—æé«˜ï¼Œå…‹æœäº†ä¼ ç»Ÿåº¦é‡çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†Cohen's Kappaä½œä¸ºä¸€è‡´æ€§åº¦é‡ï¼Œè¯„ä¼°äº†Grep-BiasIRå’ŒMSMGenderBiasçš„å¯¹é½åº¦ï¼Œåˆ†åˆ«è¾¾åˆ°äº†58.77%å’Œ18.51%ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„CWExåº¦é‡åœ¨å…¬å¹³æ€§è¯„ä¼°ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿåº¦é‡ï¼ŒGrep-BiasIRå’ŒMSMGenderBiasçš„å¯¹é½åº¦åˆ†åˆ«è¾¾åˆ°äº†58.77%å’Œ18.51%ï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„æ€§åˆ«åè§åŒºåˆ†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿã€æ¨èç³»ç»Ÿå’Œç¤¾äº¤åª’ä½“å¹³å°ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´å…¬å¹³çš„ç®—æ³•ï¼Œå‡å°‘æ€§åˆ«åè§å¯¹ç”¨æˆ·ä½“éªŒçš„å½±å“ã€‚æœªæ¥ï¼Œéšç€å¯¹æ€§åˆ«åè§çš„æ·±å…¥ç ”ç©¶ï¼Œå¯èƒ½ä¼šæ¨åŠ¨æ›´å¹¿æ³›çš„ç¤¾ä¼šå…¬å¹³æ€§æå‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The presence of social biases in Natural Language Processing (NLP) and Information Retrieval (IR) systems is an ongoing challenge, which underlines the importance of developing robust approaches to identifying and evaluating such biases. In this paper, we aim to address this issue by leveraging Large Language Models (LLMs) to detect and measure gender bias in passage ranking. Existing gender fairness metrics rely on lexical- and frequency-based measures, leading to various limitations, e.g., missing subtle gender disparities. Building on our LLM-based gender bias detection method, we introduce a novel gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to address existing limitations. To measure the effectiveness of our proposed metric and study LLMs' effectiveness in detecting gender bias, we annotate a subset of the MS MARCO Passage Ranking collection and release our new gender bias collection, called MSMGenderBias, to foster future research in this area. Our extensive experimental results on various ranking models show that our proposed metric offers a more detailed evaluation of fairness compared to previous metrics, with improved alignment to human labels (58.77% for Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa agreement), effectively distinguishing gender bias in ranking. By integrating LLM-driven bias detection, an improved fairness metric, and gender bias annotations for an established dataset, this work provides a more robust framework for analyzing and mitigating bias in IR systems.

