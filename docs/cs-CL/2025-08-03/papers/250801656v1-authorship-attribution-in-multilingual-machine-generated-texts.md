---
layout: default
title: Authorship Attribution in Multilingual Machine-Generated Texts
---

# Authorship Attribution in Multilingual Machine-Generated Texts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01656" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01656v1</a>
  <a href="https://arxiv.org/pdf/2508.01656.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01656v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01656v1', 'Authorship Attribution in Multilingual Machine-Generated Texts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lucio La Cava, Dominik Macko, RÃ³bert MÃ³ro, Ivan Srba, Andrea Tagarelli

**åˆ†ç±»**: cs.CL, cs.AI, cs.CY, cs.HC, physics.soc-ph

**å‘å¸ƒæ—¥æœŸ**: 2025-08-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¯­è¨€ä½œè€…å½’å±æ–¹æ³•ä»¥è§£å†³æœºå™¨ç”Ÿæˆæ–‡æœ¬è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½œè€…å½’å±` `æœºå™¨ç”Ÿæˆæ–‡æœ¬` `å¤šè¯­è¨€å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬åˆ†æ` `ä¿¡æ¯éªŒè¯` `ç¤¾äº¤åª’ä½“ç›‘æ§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä½œè€…å½’å±æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•è¯­ç¯å¢ƒï¼Œç¼ºä¹å¯¹å¤šè¯­è¨€æ–‡æœ¬çš„æœ‰æ•ˆå¤„ç†ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. æœ¬æ–‡æå‡ºäº†å¤šè¯­è¨€ä½œè€…å½’å±çš„æ¦‚å¿µï¼Œæ—¨åœ¨è¯†åˆ«å¤šç§è¯­è¨€ä¸­äººç±»ä¸LLMç”Ÿæˆçš„æ–‡æœ¬ï¼Œæ‰©å±•äº†ç°æœ‰ç ”ç©¶çš„è¾¹ç•Œã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡æŸäº›å•è¯­æ–¹æ³•å¯é€‚åº”å¤šè¯­è¨€è®¾ç½®ï¼Œä½†åœ¨ä¸åŒè¯­è¨€å®¶æ—é—´çš„è¿ç§»èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¾¾åˆ°äººç±»èˆ¬çš„æµç•…æ€§å’Œè¿è´¯æ€§ï¼ŒåŒºåˆ†æœºå™¨ç”Ÿæˆæ–‡æœ¬ï¼ˆMGTï¼‰ä¸äººç±»æ’°å†™å†…å®¹å˜å¾—æ„ˆåŠ å›°éš¾ã€‚æ—©æœŸçš„MGTæ£€æµ‹ä¸»è¦é›†ä¸­äºäºŒåˆ†ç±»ï¼Œè€Œéšç€LLMsçš„å¤šæ ·æ€§ï¼Œä½œè€…å½’å±ï¼ˆAAï¼‰é—®é¢˜å˜å¾—æ›´åŠ å¤æ‚ã€‚æœ¬æ–‡å¼•å…¥äº†å¤šè¯­è¨€ä½œè€…å½’å±é—®é¢˜ï¼Œæ—¨åœ¨è¯†åˆ«æ–‡æœ¬èƒŒåçš„ç”Ÿæˆè€…ï¼ˆäººç±»æˆ–å¤šç§LLMï¼‰ï¼Œå¹¶è¦†ç›–18ç§è¯­è¨€å’Œ8ä¸ªç”Ÿæˆå™¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æŸäº›å•è¯­AAæ–¹æ³•å¯ä»¥é€‚åº”å¤šè¯­è¨€ç¯å¢ƒï¼Œä½†åœ¨ä¸åŒè¯­è¨€å®¶æ—é—´çš„è¿ç§»ä»é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†å¤šè¯­è¨€AAçš„å¤æ‚æ€§å’Œå¯¹æ›´å¼ºå¤§æ–¹æ³•çš„éœ€æ±‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„ä½œè€…å½’å±é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å±€é™äºå•ä¸€è¯­è¨€ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–çš„LLMç”Ÿæˆæ–‡æœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤šè¯­è¨€ä½œè€…å½’å±çš„æ¡†æ¶ï¼Œè¯†åˆ«æ–‡æœ¬èƒŒåçš„ç”Ÿæˆè€…ï¼Œè€ƒè™‘å¤šç§è¯­è¨€å’Œç”Ÿæˆå™¨çš„å½±å“ï¼Œä»¥é€‚åº”ç°ä»£LLMçš„å¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶æ¶µç›–18ç§è¯­è¨€å’Œ8ä¸ªç”Ÿæˆå™¨ï¼Œé‡‡ç”¨å•è¯­AAæ–¹æ³•çš„é€‚åº”æ€§åˆ†æï¼Œè¯„ä¼°å…¶åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„è¡¨ç°å’Œè¿ç§»èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†å¤šè¯­è¨€ä½œè€…å½’å±çš„æ¦‚å¿µï¼Œå¼ºè°ƒäº†ä¸åŒè¯­è¨€å®¶æ—é—´çš„è¿ç§»æŒ‘æˆ˜ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§å•è¯­AAæ–¹æ³•ï¼Œå¹¶å¯¹å…¶åœ¨å¤šè¯­è¨€è®¾ç½®ä¸‹çš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå…³æ³¨ç”Ÿæˆå™¨å¯¹å½’å±æ€§èƒ½çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æŸäº›å•è¯­AAæ–¹æ³•åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­è¡¨ç°å‡ºä¸€å®šçš„é€‚åº”æ€§ï¼Œä½†åœ¨ä¸åŒè¯­è¨€å®¶æ—é—´çš„è¿ç§»èƒ½åŠ›ä»æ˜¾ä¸è¶³ï¼Œå¼ºè°ƒäº†å¤šè¯­è¨€ä½œè€…å½’å±çš„å¤æ‚æ€§ã€‚å…·ä½“æ•°æ®æœªæä¾›ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶ä»¥ä¼˜åŒ–ç°æœ‰æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†…å®¹å®¡æ ¸ã€ä¿¡æ¯éªŒè¯å’Œç¤¾äº¤åª’ä½“ç›‘æ§ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©è¯†åˆ«å’Œè¿½è¸ªæœºå™¨ç”Ÿæˆå†…å®¹çš„æ¥æºï¼Œæå‡ä¿¡æ¯çš„å¯ä¿¡åº¦å’Œé€æ˜åº¦ã€‚æœªæ¥ï¼Œéšç€LLMæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œè¯¥æ–¹æ³•å°†å¯¹å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æ–‡æœ¬åˆ†æå’Œå¤„ç†äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As Large Language Models (LLMs) have reached human-like fluency and coherence, distinguishing machine-generated text (MGT) from human-written content becomes increasingly difficult. While early efforts in MGT detection have focused on binary classification, the growing landscape and diversity of LLMs require a more fine-grained yet challenging authorship attribution (AA), i.e., being able to identify the precise generator (LLM or human) behind a text. However, AA remains nowadays confined to a monolingual setting, with English being the most investigated one, overlooking the multilingual nature and usage of modern LLMs. In this work, we introduce the problem of Multilingual Authorship Attribution, which involves attributing texts to human or multiple LLM generators across diverse languages. Focusing on 18 languages -- covering multiple families and writing scripts -- and 8 generators (7 LLMs and the human-authored class), we investigate the multilingual suitability of monolingual AA methods, their cross-lingual transferability, and the impact of generators on attribution performance. Our results reveal that while certain monolingual AA methods can be adapted to multilingual settings, significant limitations and challenges remain, particularly in transferring across diverse language families, underscoring the complexity of multilingual AA and the need for more robust approaches to better match real-world scenarios.

