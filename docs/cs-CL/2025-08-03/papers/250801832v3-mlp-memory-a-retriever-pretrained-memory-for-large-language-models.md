---
layout: default
title: MLP Memory: A Retriever-Pretrained Memory for Large Language Models
---

# MLP Memory: A Retriever-Pretrained Memory for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01832" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01832v3</a>
  <a href="https://arxiv.org/pdf/2508.01832.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01832v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01832v3', 'MLP Memory: A Retriever-Pretrained Memory for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rubin Wei, Jiaqi Cao, Jiarui Wang, Jushi Kai, Qipeng Guo, Bowen Zhou, Zhouhan Lin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-03 (æ›´æ–°: 2025-10-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMLP Memoryä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†è·å–æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `çŸ¥è¯†è·å–` `å‚æ•°åŒ–å­¦ä¹ ` `å¤šå±‚æ„ŸçŸ¥æœº` `æ¨ç†æ•ˆç‡` `ä¿¡æ¯æ£€ç´¢` `é—®ç­”ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†è·å–æ•ˆç‡æ—¶å­˜åœ¨æ¨ç†å»¶è¿Ÿé«˜å’Œé›†æˆæµ…çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„MLP Memoryé€šè¿‡é¢„è®­ç»ƒå¤šå±‚æ„ŸçŸ¥æœºæ¥å†…éƒ¨åŒ–æ£€ç´¢æ¨¡å¼ï¼Œæ— éœ€æ˜¾å¼æ–‡æ¡£è®¿é—®ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLP Memoryåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒåŒæ—¶åŠ å¿«äº†æ¨ç†é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§å’ŒçŸ¥è¯†åˆ©ç”¨çš„æ–¹æ³•é¢ä¸´åŸºæœ¬çš„æƒè¡¡ï¼šéå‚æ•°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æä¾›çµæ´»çš„å¤–éƒ¨çŸ¥è¯†è®¿é—®ï¼Œä½†æ¨ç†å»¶è¿Ÿé«˜ä¸”é›†æˆæµ…ï¼Œè€Œå‚æ•°å¾®è°ƒæ–¹æ³•å¦‚LoRAåˆ™å­˜åœ¨ç¾éš¾æ€§é—å¿˜å’Œèƒ½åŠ›ä¸‹é™çš„é£é™©ã€‚æœ¬æ–‡æå‡ºMLP Memoryï¼Œä¸€ä¸ªè½»é‡çº§çš„å‚æ•°æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜¾å¼æ–‡æ¡£è®¿é—®çš„æƒ…å†µä¸‹å­¦ä¹ æ£€ç´¢æ¨¡å¼ã€‚é€šè¿‡é¢„è®­ç»ƒä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ¥æ¨¡ä»¿kNNæ£€ç´¢å™¨åœ¨æ•´ä¸ªé¢„è®­ç»ƒæ•°æ®é›†ä¸Šçš„è¡Œä¸ºï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¯å¾®åˆ†çš„è®°å¿†ç»„ä»¶ï¼Œä»¥å®Œå…¨å‚æ•°åŒ–çš„å½¢å¼æ•è·åŸºäºæ£€ç´¢çš„çŸ¥è¯†è®¿é—®çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æ¶æ„é€šè¿‡ç®€å•çš„æ¦‚ç‡æ’å€¼å°†é¢„è®­ç»ƒçš„MLP Memoryä¸Transformerè§£ç å™¨é›†æˆï¼Œåœ¨WikiText-103å’ŒWebæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†17.5%å’Œ24.1%çš„æ‰©å±•å¢ç›Šï¼Œå¹¶åœ¨äº”ä¸ªé—®ç­”åŸºå‡†ä¸Šå®ç°äº†12.3%çš„ç›¸å¯¹æå‡ï¼Œåœ¨ä¹ä¸ªé€šç”¨NLPä»»åŠ¡ä¸Šå®ç°äº†5.2åˆ†çš„ç»å¯¹å¢ç›Šï¼ŒåŒæ—¶åœ¨HaluEvalä¸Šå‡å°‘äº†å¤šè¾¾10åˆ†çš„å¹»è§‰ã€‚æ­¤å¤–ï¼ŒMLP Memoryçš„æ¨ç†é€Ÿåº¦æ¯”RAGå¿«2.5å€ä¸”å‡†ç¡®æ€§æ›´é«˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå‚æ•°åŒ–å­¦ä¹ æ£€ç´¢æ¨¡å¼å¼¥åˆäº†é«˜æ•ˆæ¨ç†ä¸æœ‰æ•ˆçŸ¥è¯†è®¿é—®ä¹‹é—´çš„å·®è·ï¼Œä¸ºRAGå’Œå¾®è°ƒæ–¹æ³•æä¾›äº†å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†è·å–æ–¹æ³•åœ¨æ¨ç†æ•ˆç‡å’ŒçŸ¥è¯†é›†æˆæ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯RAGæ–¹æ³•çš„é«˜å»¶è¿Ÿå’Œå¾®è°ƒæ–¹æ³•çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„MLP Memoryé€šè¿‡é¢„è®­ç»ƒä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œæ¨¡ä»¿kNNæ£€ç´¢å™¨çš„è¡Œä¸ºï¼Œä»è€Œåœ¨ä¸ä¾èµ–å¤–éƒ¨æ–‡æ¡£çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ åˆ°æœ‰æ•ˆçš„æ£€ç´¢æ¨¡å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªé¢„è®­ç»ƒçš„MLP Memoryæ¨¡å—ï¼Œè¯¥æ¨¡å—ä¸Transformerè§£ç å™¨é€šè¿‡æ¦‚ç‡æ’å€¼è¿›è¡Œé›†æˆã€‚é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨æ•´ä¸ªæ•°æ®é›†æ¥è®­ç»ƒMLPï¼Œä½¿å…¶èƒ½å¤Ÿæ•è·æ£€ç´¢æ¨¡å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ£€ç´¢æ¨¡å¼çš„å­¦ä¹ è½¬åŒ–ä¸ºå‚æ•°åŒ–çš„å½¢å¼ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„é«˜å»¶è¿Ÿå’Œç¾éš¾æ€§é—å¿˜ï¼Œæä¾›äº†ä¸€ç§æ–°çš„çŸ¥è¯†è®¿é—®æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMLPçš„ç»“æ„å’ŒæŸå¤±å‡½æ•°ç»è¿‡ç²¾å¿ƒé€‰æ‹©ï¼Œä»¥ç¡®ä¿å…¶èƒ½å¤Ÿæœ‰æ•ˆæ¨¡ä»¿kNNæ£€ç´¢å™¨çš„è¡Œä¸ºï¼Œå…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMLP Memoryåœ¨WikiText-103å’ŒWebæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†17.5%å’Œ24.1%çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨äº”ä¸ªé—®ç­”åŸºå‡†ä¸Šç›¸å¯¹æå‡12.3%ã€‚æ­¤å¤–ï¼Œæ¨ç†é€Ÿåº¦æ¯”RAGå¿«2.5å€ï¼ŒåŒæ—¶å‡å°‘äº†å¤šè¾¾10åˆ†çš„å¹»è§‰ç°è±¡ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€é—®ç­”ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜çŸ¥è¯†è·å–çš„æ•ˆç‡ï¼ŒMLP Memoryèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern approaches to enhancing Large Language Models' factual accuracy and knowledge utilization face a fundamental trade-off: non-parametric retrieval-augmented generation (RAG) provides flexible access to external knowledge but suffers from high inference latency and shallow integration, while parametric fine-tuning methods like LoRA risk catastrophic forgetting and degraded general capabilities. In this work, we propose MLP Memory, a lightweight parametric module that learns to internalize retrieval patterns without explicit document access. By pretraining an MLP to imitate a $k$NN retriever's behavior on the entire pretraining dataset, we create a differentiable memory component that captures the benefits of retrieval-based knowledge access in a fully parametric form. Our architecture integrates this pretrained MLP Memory with Transformer decoders through simple probability interpolation, yielding 17.5\% and 24.1\% scaling gains on WikiText-103 and Web datasets, respectively. It further achieves 12.3\% relative improvement on five question-answering benchmarks and 5.2 points absolute gain across nine general NLP tasks, while reducing hallucinations by up to 10 points on HaluEval. Moreover, MLP Memory delivers 2.5$\times$ faster inference than RAG with superior accuracy. Our findings show that learning retrieval patterns parametrically bridges the gap between efficient inference and effective knowledge access, offering a practical alternative to both RAG and fine-tuning approaches.

