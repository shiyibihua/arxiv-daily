---
layout: default
title: The Art of Breaking Words: Rethinking Multilingual Tokenizer Design
---

# The Art of Breaking Words: Rethinking Multilingual Tokenizer Design

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06533" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.06533v1</a>
  <a href="https://arxiv.org/pdf/2508.06533.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06533v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06533v1', 'The Art of Breaking Words: Rethinking Multilingual Tokenizer Design')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Aamod Thakur, Ajay Nagpal, Atharva Savarkar, Kundeshwar Pundalik, Siddhesh Dosi, Piyush Sawarkar, Viraj Thakur, Rohit Saluja, Maunendra Sankar Desarkar, Ganesh Ramakrishnan

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-03

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ÄÁßçÊñ∞ÁÆóÊ≥ï‰ª•‰ºòÂåñÂ§öËØ≠Ë®ÄÂàÜËØçÂô®ËÆæËÆ°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öËØ≠Ë®ÄÂ§ÑÁêÜ` `ÂàÜËØçÂô®ËÆæËÆ°` `Êï∞ÊçÆÁªÑÊàê` `Ê®°Âûã‰ºòÂåñ` `Âç∞Âú∞ËØ≠ËÑöÊú¨`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öËØ≠Ë®ÄÂàÜËØçÂô®Âú®ËØçÂÖÉ‰∏éÂçïËØçÊØîÁéá„ÄÅ‰∏ä‰∏ãÊñáÈïøÂ∫¶Âà©Áî®ÂíåÊé®ÁêÜÈÄüÂ∫¶ÊñπÈù¢Â≠òÂú®ÊòæËëó‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁÆóÊ≥ïÔºåÈÄöËøá‰ºòÂåñÊï∞ÊçÆÁªÑÊàêÂíåÈ¢ÑÂàÜËØçÁ≠ñÁï•Êù•ÊèêÈ´òÂàÜËØçÂô®ÁöÑÊïàÁéáÂíåÊ®°ÂûãÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂàÜËØçÂô®Âú®Âπ≥ÂùáËØçÂÖÉ‰∏éÂçïËØçÊØîÁéá‰∏äËæÉÁé∞ÊúâÊúÄÂÖàËøõÁöÑÂç∞Âú∞ËØ≠Ê®°ÂûãÊèêÂçáË∂ÖËøá40%ÔºåÂπ∂Âä†Âø´‰∫ÜÊé®ÁêÜÈÄüÂ∫¶„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â∞ΩÁÆ°Ê®°ÂûãÊû∂ÊûÑÂíåËÆ≠ÁªÉÁõÆÊ†áÂ∑≤Ë¢´ÂπøÊ≥õÁ†îÁ©∂Ôºå‰ΩÜÂú®Â§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠ÁöÑÂàÜËØç‰ªçÁÑ∂ÊòØÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂºÄÂèë‰∏≠Áõ∏ÂØπË¢´ÂøΩËßÜÁöÑÊñπÈù¢„ÄÇÁé∞ÊúâÁöÑÂàÜËØçÂô®ÈÄöÂ∏∏Ë°®Áé∞Âá∫È´òÁöÑËØçÂÖÉ‰∏éÂçïËØçÊØîÁéá„ÄÅ‰ΩéÊïàÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶Âà©Áî®ÂíåËæÉÊÖ¢ÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇÊú¨ÊñáÁ≥ªÁªüÁ†îÁ©∂‰∫ÜËØçÊ±áÂ§ßÂ∞è„ÄÅÈ¢ÑÂàÜËØçËßÑÂàôÂíåËÆ≠ÁªÉËØ≠ÊñôÁªÑÊàêÂØπËØçÂÖÉ‰∏éÂçïËØçÊïàÁéáÂèäÊ®°ÂûãË¥®ÈáèÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨Âú®Âç∞Âú∞ËØ≠ËÑöÊú¨‰∏äËøõË°å‰∫ÜÂπøÊ≥õÂÆûÈ™åÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÁªÑÊàêÁÆóÊ≥ïÔºå‰ª•Âπ≥Ë°°Â§öËØ≠Ë®ÄÊï∞ÊçÆÁî®‰∫éÂàÜËØçÂô®ËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÁöÑËßÇÂØüÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåÂπ∂Â∞ÜÂπ≥ÂùáËØçÂÖÉ‰∏éÂçïËØçÊØîÁéáÂáèÂ∞ëÁ∫¶6%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öËØ≠Ë®ÄÂàÜËØçÂô®Âú®ËØçÂÖÉ‰∏éÂçïËØçÊØîÁéáÈ´ò„ÄÅ‰∏ä‰∏ãÊñáÂà©Áî®‰ΩéÂíåÊé®ÁêÜÈÄüÂ∫¶ÊÖ¢Á≠âÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÂ§ÑÁêÜÂ§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠ÁöÑÂ§çÊùÇÊÄßÔºåÂØºËá¥Ê®°ÂûãÊÄßËÉΩÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÁªÑÊàêÁÆóÊ≥ïÔºåÊó®Âú®ÈÄöËøá‰ºòÂåñËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄßÂíåÈ¢ÑÂàÜËØçÁ≠ñÁï•Êù•ÊèêÈ´òÂàÜËØçÂô®ÁöÑÊïàÁéá„ÄÇËøôÁßçËÆæËÆ°ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠ÁöÑËØ≠Ë®ÄÁâπÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÁªÑÊàê„ÄÅÈ¢ÑÂàÜËØçËßÑÂàôÂíåÊ®°ÂûãËÆ≠ÁªÉ‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÂàÜÊûêËØ≠ÊñôÂ∫ìÁöÑÁªÑÊàêÊù•‰ºòÂåñÊï∞ÊçÆÔºåÁÑ∂ÂêéÂ∫îÁî®Êñ∞ÁöÑÈ¢ÑÂàÜËØçÁ≠ñÁï•ÔºåÊúÄÂêéÂú®ÊîπËøõÁöÑÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÊ®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÁªÑÊàêÁÆóÊ≥ïÔºåËØ•ÁÆóÊ≥ïÂú®Â§öËØ≠Ë®ÄÊï∞ÊçÆÂπ≥Ë°°ÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºå‰∏é‰º†ÁªüÁöÑÈöèÊú∫ÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåÊòæËëóÈôç‰Ωé‰∫ÜËØçÂÖÉ‰∏éÂçïËØçÊØîÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåËÆ∫ÊñáÂØπËØçÊ±áÂ§ßÂ∞èÂíåÈ¢ÑÂàÜËØçËßÑÂàôËøõË°å‰∫ÜÁªÜËá¥Ë∞ÉÊï¥ÔºåÈááÁî®‰∫ÜÈÄÇÂ∫îÊÄßÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñËÆ≠ÁªÉËøáÁ®ãÔºåÁ°Æ‰øùÊ®°ÂûãÂú®Â§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÁªÜËäÇÂíåËÆ≠ÁªÉÁ≠ñÁï•‰πüËøõË°å‰∫Ü‰ºòÂåñÔºå‰ª•ÈÄÇÂ∫îÂç∞Âú∞ËØ≠ËÑöÊú¨ÁöÑÂ§çÊùÇÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêÂàÜËØçÂô®Âú®Âπ≥ÂùáËØçÂÖÉ‰∏éÂçïËØçÊØîÁéá‰∏äËæÉÁé∞ÊúâÊúÄÂÖàËøõÁöÑÂç∞Âú∞ËØ≠Ê®°ÂûãÊèêÂçáË∂ÖËøá40%ÔºåÂπ∂‰∏îÂú®Êé®ÁêÜÈÄüÂ∫¶‰∏ä‰πüÊúâÊòæËëóÊîπÂñÑ„ÄÇËøô‰∏ÄÊàêÊûúÁ™ÅÊòæ‰∫ÜÂàÜËØçÂô®ËÆæËÆ°Âú®Â§öËØ≠Ë®ÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊûÑÂª∫‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Â§öËØ≠Ë®ÄËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊú∫Âô®ÁøªËØëÂíåË∑®ËØ≠Ë®Ä‰ø°ÊÅØÊ£ÄÁ¥¢Á≠â„ÄÇÈÄöËøá‰ºòÂåñÂàÜËØçÂô®ËÆæËÆ°ÔºåËÉΩÂ§üÊòæËëóÊèêÂçáÂ§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊé®ÁêÜÊïàÁéáÔºå‰ªéËÄåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂπøÊ≥õÈááÁî®ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> While model architecture and training objectives are well-studied, tokenization, particularly in multilingual contexts, remains a relatively neglected aspect of Large Language Model (LLM) development. Existing tokenizers often exhibit high token-to-word ratios, inefficient use of context length, and slower inference. We present a systematic study that links vocabulary size, pre-tokenization rules, and training-corpus composition to both token-to-word efficiency and model quality. To ground our analysis in a linguistically diverse context, we conduct extensive experiments on Indic scripts, which present unique challenges due to their high script diversity and orthographic complexity. Drawing on the insights from these analyses, we propose a novel algorithm for data composition that balances multilingual data for tokenizer training. Our observations on pretokenization strategies significantly improve model performance, and our data composition algorithm reduces the average token-to-word ratio by approximately 6% with respect to the conventional data randomization approach. Our tokenizer achieves more than 40% improvement on average token-to-word ratio against stateof-the-art multilingual Indic models. This improvement yields measurable gains in both model performance and inference speed. This highlights tokenization alongside architecture and training objectives as a critical lever for building efficient, scalable multilingual LLMs

