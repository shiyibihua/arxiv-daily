---
layout: default
title: Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption
---

# Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01708" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01708v1</a>
  <a href="https://arxiv.org/pdf/2508.01708.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01708v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01708v1', 'Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Berkay KÃ¶prÃ¼, Mehrzad Mashal, Yigit Gurses, Akos Kadar, Maximilian Schmitt, Ditty Mathew, Felix Burkhardt, Florian Eyben, BjÃ¶rn W. Schuller

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¡¨è¾¾æ³„æ¼æ¦‚å¿µä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„æ— å…³ä¿¡æ¯å¹²æ‰°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è¡¨è¾¾æ³„æ¼` `æƒ…æ„Ÿåˆ†æ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è¯„ä¼°` `æ— å…³ä¿¡æ¯å¹²æ‰°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è¯­ä¹‰æ³„æ¼ä¸Šï¼Œæœªèƒ½å……åˆ†æ¢è®¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æƒ…æ„Ÿè¡¨è¾¾ä¸è¾“å…¥ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†è¡¨è¾¾æ³„æ¼çš„æ¦‚å¿µï¼Œå¹¶é€šè¿‡æ„å»ºåŸºå‡†æ•°æ®é›†å’Œè‡ªåŠ¨è¯„ä¼°ç®¡é“æ¥åˆ†æè¿™ä¸€ç°è±¡ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œè¡¨è¾¾æ³„æ¼ç°è±¡æœ‰æ‰€å‡å°‘ï¼Œä½†è´Ÿé¢æƒ…æ„Ÿçš„å½±å“æ›´ä¸ºæ˜¾è‘—ï¼Œæç¤ºæ¨¡å‹æ„å»ºéœ€ç‰¹åˆ«å…³æ³¨è¿™ä¸€é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ•´åˆå¹¿æ³›ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ä½¿å…¶å®¹æ˜“å¼•å…¥æ— å…³ä¿¡æ¯ã€‚æœ¬æ–‡å¼•å…¥äº†è¡¨è¾¾æ³„æ¼è¿™ä¸€æ–°ç°è±¡ï¼ŒæŒ‡LLMsç³»ç»Ÿæ€§ç”Ÿæˆä¸è¾“å…¥ä¸Šä¸‹æ–‡è¯­ä¹‰æ— å…³çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚ä¸ºåˆ†æè¡¨è¾¾æ³„æ¼ï¼Œä½œè€…æ”¶é›†äº†åŸºå‡†æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨è¯„ä¼°ç®¡é“ï¼Œä¸äººç±»åˆ¤æ–­é«˜åº¦ç›¸å…³ã€‚å®éªŒè¡¨æ˜ï¼Œéšç€æ¨¡å‹å‚æ•°è§„æ¨¡çš„å¢åŠ ï¼ŒåŒä¸€LLMå®¶æ—ä¸­çš„è¡¨è¾¾æ³„æ¼ç°è±¡æœ‰æ‰€å‡å°‘ï¼Œä½†åœ¨æ¨¡å‹æ„å»ºè¿‡ç¨‹ä¸­éœ€è¦ç‰¹åˆ«å…³æ³¨ä»¥å‡è½»è¡¨è¾¾æ³„æ¼ï¼Œæç¤ºæ— æ³•æœ‰æ•ˆç¼“è§£æ­¤é—®é¢˜ã€‚æ­¤å¤–ï¼Œè´Ÿé¢æƒ…æ„Ÿçš„æ³¨å…¥å¯¹ç”Ÿæˆè¿‡ç¨‹çš„å¹²æ‰°ç¨‹åº¦é«˜äºæ­£é¢æƒ…æ„Ÿï¼Œå¯¼è‡´æ›´é«˜çš„è¡¨è¾¾æ³„æ¼ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥ä¸ä¸Šä¸‹æ–‡æ— å…³çš„æƒ…æ„Ÿè¡¨è¾¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¯­ä¹‰æ³„æ¼ï¼Œæœªèƒ½æ·±å…¥æ¢è®¨æƒ…æ„Ÿè¡¨è¾¾çš„ç›¸å…³æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†è¡¨è¾¾æ³„æ¼è¿™ä¸€æ–°æ¦‚å¿µï¼Œå¼ºè°ƒåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æƒ…æ„Ÿè¡¨è¾¾çš„æ— å…³æ€§ï¼Œå¹¶é€šè¿‡æ„å»ºåŸºå‡†æ•°æ®é›†å’Œè‡ªåŠ¨è¯„ä¼°ç®¡é“æ¥è¿›è¡Œåˆ†æå’ŒéªŒè¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†çš„æ„å»ºã€è¡¨è¾¾æ³„æ¼çš„åˆ†æå’Œè‡ªåŠ¨è¯„ä¼°ç®¡é“ã€‚æ•°æ®é›†é€šè¿‡ä»å…¬å…±ç½‘ç»œæŠ“å–è‡ªç”±æ–‡æœ¬ç”Ÿæˆï¼Œè¯„ä¼°ç®¡é“åˆ™ä¸äººç±»åˆ¤æ–­é«˜åº¦ç›¸å…³ï¼Œèƒ½å¤ŸåŠ é€ŸåŸºå‡†æµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†è¡¨è¾¾æ³„æ¼çš„æ¦‚å¿µï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œé‡åŒ–è¿™ä¸€ç°è±¡ï¼Œä¸ä¼ ç»Ÿçš„è¯­ä¹‰æ³„æ¼åˆ†ææ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹æ„å»ºè¿‡ç¨‹ä¸­ï¼Œç‰¹åˆ«å…³æ³¨å‚æ•°è®¾ç½®å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥å‡è½»è¡¨è¾¾æ³„æ¼ç°è±¡ã€‚å®éªŒè¡¨æ˜ï¼Œè´Ÿé¢æƒ…æ„Ÿçš„æ³¨å…¥å¯¹ç”Ÿæˆè¿‡ç¨‹çš„å¹²æ‰°ç¨‹åº¦é«˜äºæ­£é¢æƒ…æ„Ÿï¼Œéœ€åœ¨æ¨¡å‹è®¾è®¡æ—¶åŠ ä»¥è€ƒè™‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œéšç€æ¨¡å‹å‚æ•°çš„å¢åŠ ï¼Œè¡¨è¾¾æ³„æ¼ç°è±¡åœ¨åŒä¸€LLMå®¶æ—ä¸­æœ‰æ‰€å‡å°‘ã€‚è´Ÿé¢æƒ…æ„Ÿçš„æ³¨å…¥å¯¼è‡´çš„è¡¨è¾¾æ³„æ¼ç‡æ˜¾è‘—é«˜äºæ­£é¢æƒ…æ„Ÿï¼Œæç¤ºåœ¨æ¨¡å‹æ„å»ºè¿‡ç¨‹ä¸­éœ€ç‰¹åˆ«å…³æ³¨æƒ…æ„Ÿçš„å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€å¯¹è¯ç³»ç»Ÿå’Œå†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡å‡è½»è¡¨è¾¾æ³„æ¼ï¼Œèƒ½å¤Ÿæé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„ç”Ÿæˆè´¨é‡å’Œç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½å¯¹äººæœºäº¤äº’å’Œè‡ªåŠ¨åŒ–å†™ä½œç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have advanced natural language processing (NLP) skills such as through next-token prediction and self-attention, but their ability to integrate broad context also makes them prone to incorporating irrelevant information. Prior work has focused on semantic leakage, bias introduced by semantically irrelevant context. In this paper, we introduce expression leakage, a novel phenomenon where LLMs systematically generate sentimentally charged expressions that are semantically unrelated to the input context. To analyse the expression leakage, we collect a benchmark dataset along with a scheme to automatically generate a dataset from free-form text from common-crawl. In addition, we propose an automatic evaluation pipeline that correlates well with human judgment, which accelerates the benchmarking by decoupling from the need of annotation for each analysed model. Our experiments show that, as the model scales in the parameter space, the expression leakage reduces within the same LLM family. On the other hand, we demonstrate that expression leakage mitigation requires specific care during the model building process, and cannot be mitigated by prompting. In addition, our experiments indicate that, when negative sentiment is injected in the prompt, it disrupts the generation process more than the positive sentiment, causing a higher expression leakage rate.

