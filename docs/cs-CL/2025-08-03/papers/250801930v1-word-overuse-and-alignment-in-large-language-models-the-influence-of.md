---
layout: default
title: Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback
---

# Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01930" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01930v1</a>
  <a href="https://arxiv.org/pdf/2508.01930.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01930v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01930v1', 'Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tom S. Juzek, Zina B. Ward

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-03

**å¤‡æ³¨**: Accepted for publication in the Proceedings of the 5th Workshop on Bias and Fairness in AI (BIAS 2025) at ECML PKDD

**DOI**: [10.17605/OSF.IO/JY48S](https://doi.org/10.17605/OSF.IO/JY48S)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–¹æ³•ä»¥æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹çš„è¯æ±‡è¿‡åº¦ä½¿ç”¨é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `äººç±»åé¦ˆå­¦ä¹ ` `è¯æ±‡åå¥½` `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `æ–‡æœ¬ç”Ÿæˆ` `å¯¹è¯ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰ç ”ç©¶æœªèƒ½æ˜ç¡®è§£é‡Šå¤§è¯­è¨€æ¨¡å‹åœ¨è¯æ±‡é€‰æ‹©ä¸Šçš„è¿‡åº¦ä½¿ç”¨ç°è±¡ï¼Œå°¤å…¶æ˜¯ä¸äººç±»åé¦ˆå­¦ä¹ çš„å…³ç³»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ£€æµ‹å¤§è¯­è¨€æ¨¡å‹è¯æ±‡åå¥½çš„æ–°æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†äººç±»åé¦ˆå­¦ä¹ å¯¹è¯æ±‡é€‰æ‹©çš„å½±å“ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå®éªŒç»“æœæ˜¾ç¤ºï¼Œå‚ä¸è€…æ›´å€¾å‘äºé€‰æ‹©åŒ…å«ç‰¹å®šè¯æ±‡çš„æ–‡æœ¬å˜ä½“ï¼Œæ­ç¤ºäº†è¯æ±‡è¿‡åº¦ä½¿ç”¨çš„æ½œåœ¨åŸå› ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½¿ç”¨æŸäº›æœ¯è¯­æ—¶è¡¨ç°å‡ºè¿‡åº¦ä½¿ç”¨çš„ç°è±¡ï¼Œä¾‹å¦‚â€œæ·±å…¥â€å’Œâ€œå¤æ‚â€ã€‚ç„¶è€Œï¼Œè¿™äº›è¯æ±‡é€‰æ‹©çš„å…·ä½“åŸå› å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡åˆ©ç”¨Metaçš„Llamaæ¨¡å‹ï¼Œç ”ç©¶äº†äººç±»åé¦ˆå­¦ä¹ ï¼ˆLHFï¼‰å¯¹è¯æ±‡åå¥½çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„ç¨‹åºæ¥æ£€æµ‹å¯èƒ½ç”±LHFå¼•èµ·çš„è¯æ±‡åå¥½ï¼Œå¹¶é€šè¿‡å®éªŒæ¨¡æ‹ŸLHFè¿‡ç¨‹ï¼Œè¯æ˜å‚ä¸è€…ç³»ç»Ÿæ€§åœ°åå¥½åŒ…å«æŸäº›è¯çš„æ–‡æœ¬å˜ä½“ã€‚è¿™ç§è¯æ±‡è¿‡åº¦ä½¿ç”¨å¯ä»¥è§†ä¸ºä¸€ç§ä¸ä¸€è‡´ç°è±¡ï¼Œç ”ç©¶å¼ºè°ƒäº†ä¸åŒäººç¾¤ï¼ˆå¦‚LHFå·¥ä½œè€…ä¸LLMç”¨æˆ·ï¼‰ä¹‹é—´çš„è¯æ±‡æœŸæœ›å·®å¼‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¯è§£é‡Šäººå·¥æ™ºèƒ½çš„ç ”ç©¶æä¾›äº†è´¡çŒ®ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®å’Œè¿‡ç¨‹é€æ˜æ€§åœ¨å¯¹é½ç ”ç©¶ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨è¯æ±‡é€‰æ‹©ä¸Šå­˜åœ¨çš„è¿‡åº¦ä½¿ç”¨ç°è±¡ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè§£é‡Šè¿™ç§ç°è±¡çš„æ ¹æœ¬åŸå› ï¼Œå°¤å…¶æ˜¯ä¸äººç±»åé¦ˆå­¦ä¹ çš„å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šç ”ç©¶é€šè¿‡æ¨¡æ‹Ÿäººç±»åé¦ˆå­¦ä¹ çš„è¿‡ç¨‹ï¼Œæ¢ç´¢å…¶å¯¹å¤§è¯­è¨€æ¨¡å‹è¯æ±‡åå¥½çš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ£€æµ‹æ–¹æ³•ï¼Œä»¥æ­ç¤ºæ½œåœ¨çš„è¯æ±‡è¿‡åº¦ä½¿ç”¨ç°è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å®éªŒè®¾è®¡å’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†åŒ…å«ä¸åŒè¯æ±‡çš„æ–‡æœ¬æ•°æ®ï¼Œç„¶åè®¾è®¡å®éªŒä»¥è¯„ä¼°å‚ä¸è€…çš„åå¥½ï¼Œæœ€ååˆ†æç»“æœä»¥ç¡®è®¤è¯æ±‡åå¥½çš„å­˜åœ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†äººç±»åé¦ˆå­¦ä¹ ä¸è¯æ±‡é€‰æ‹©çš„å…³ç³»è¿›è¡Œäº†ç³»ç»Ÿæ€§æ¢è®¨ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å®éªŒæ–¹æ³•æ¥éªŒè¯è¿™ä¸€å…³ç³»ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒè®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç›´æ¥åå¥½ä¼˜åŒ–å’Œå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œè®¾ç½®äº†ç‰¹å®šçš„è¯æ±‡å˜ä½“ä»¥ä¾›å‚ä¸è€…é€‰æ‹©ï¼Œç¡®ä¿äº†å®éªŒçš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚å‚ä¸è€…çš„é€‰æ‹©åå¥½è¢«é‡åŒ–å¹¶ç”¨äºåˆ†æè¯æ±‡è¿‡åº¦ä½¿ç”¨çš„æ¨¡å¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå‚ä¸è€…åœ¨é€‰æ‹©æ–‡æœ¬å˜ä½“æ—¶ï¼Œç³»ç»Ÿæ€§åœ°åå¥½åŒ…å«ç‰¹å®šè¯æ±‡çš„é€‰é¡¹ï¼ŒéªŒè¯äº†äººç±»åé¦ˆå­¦ä¹ å¯¹è¯æ±‡é€‰æ‹©çš„å½±å“ã€‚é€šè¿‡æ¨¡æ‹ŸLHFè¿‡ç¨‹ï¼Œç ”ç©¶æ­ç¤ºäº†è¯æ±‡è¿‡åº¦ä½¿ç”¨çš„æ½œåœ¨æœºåˆ¶ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦çš„å®éªŒæ•°æ®å’Œç†è®ºæ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„è¯æ±‡åå¥½ï¼Œå¯ä»¥ä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºï¼Œæé«˜ä¸ç”¨æˆ·çš„å¯¹è¯è´¨é‡ï¼Œè¿›è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœä¹Ÿä¸ºå¯è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€ï¼Œä¿ƒè¿›äº†æ¨¡å‹çš„é€æ˜æ€§å’Œå¯æ§æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are known to overuse certain terms like "delve" and "intricate." The exact reasons for these lexical choices, however, have been unclear. Using Meta's Llama model, this study investigates the contribution of Learning from Human Feedback (LHF), under which we subsume Reinforcement Learning from Human Feedback and Direct Preference Optimization. We present a straightforward procedure for detecting the lexical preferences of LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to lexical overuse by experimentally emulating the LHF procedure and demonstrating that participants systematically prefer text variants that include certain words. This lexical overuse can be seen as a sort of misalignment, though our study highlights the potential divergence between the lexical expectations of different populations -- namely LHF workers versus LLM users. Our work contributes to the growing body of research on explainable artificial intelligence and emphasizes the importance of both data and procedural transparency in alignment research.

