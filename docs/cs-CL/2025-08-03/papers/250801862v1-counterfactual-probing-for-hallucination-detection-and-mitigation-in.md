---
layout: default
title: Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models
---

# Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01862" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01862v1</a>
  <a href="https://arxiv.org/pdf/2508.01862.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01862v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01862v1', 'Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yijun Feng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåäº‹å®æ¢æµ‹æ–¹æ³•ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¹»è§‰æ£€æµ‹` `åäº‹å®æ¢æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªé€‚åº”å‡è½»` `å®æ—¶éªŒè¯æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ç»å¸¸ç”Ÿæˆå¹»è§‰è¾“å‡ºï¼Œå¯¼è‡´ä¿¡æ¯ä¸å‡†ç¡®ï¼Œå½±å“å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„åäº‹å®æ¢æµ‹æ–¹æ³•é€šè¿‡ç”Ÿæˆåäº‹å®é™ˆè¿°æ¥è¯„ä¼°æ¨¡å‹å¯¹äº‹å®é”™è¯¯çš„æ•æ„Ÿæ€§ï¼Œæ—¨åœ¨æé«˜å¹»è§‰æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œåäº‹å®æ¢æµ‹åœ¨æ£€æµ‹æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸”è‡ªé€‚åº”å‡è½»ç­–ç•¥æœ‰æ•ˆé™ä½äº†å¹»è§‰è¯„åˆ†ï¼Œæå‡äº†æ¨¡å‹çš„å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ç»å¸¸ç”Ÿæˆæµç•…ä½†äº‹å®ä¸å‡†ç¡®æˆ–ç¼ºä¹æ”¯æŒçš„å¹»è§‰è¾“å‡ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åäº‹å®æ¢æµ‹æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹å’Œå‡è½»è¯­è¨€æ¨¡å‹è¾“å‡ºä¸­çš„å¹»è§‰ã€‚è¯¥æ–¹æ³•åŠ¨æ€ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†åŒ…å«å¾®å¦™äº‹å®é”™è¯¯çš„åäº‹å®é™ˆè¿°ï¼Œå¹¶è¯„ä¼°æ¨¡å‹å¯¹è¿™äº›æ‰°åŠ¨çš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬å‡è®¾ï¼ŒçœŸå®çŸ¥è¯†å¯¹åäº‹å®å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œè€Œå¹»è§‰å†…å®¹åœ¨é¢å¯¹åˆç†æ›¿ä»£æ—¶è¡¨ç°å‡ºä¸ä¸€è‡´çš„ç½®ä¿¡æ¨¡å¼ã€‚é€šè¿‡åœ¨TruthfulQAã€äº‹å®é™ˆè¿°æ•°æ®é›†å’Œç­–åˆ’çš„å¹»è§‰ç¤ºä¾‹ä¸Šçš„å…¨é¢è¯„ä¼°ï¼Œåäº‹å®æ¢æµ‹åœ¨æ£€æµ‹æ€§èƒ½ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶æˆ‘ä»¬çš„è‡ªé€‚åº”å‡è½»ç­–ç•¥å°†å¹»è§‰è¯„åˆ†å¹³å‡é™ä½äº†24.5%ã€‚è¯¥æ–¹æ³•æ— éœ€æ¨¡å‹é‡è®­ç»ƒï¼Œå¯ä½œä¸ºå®æ—¶éªŒè¯æœºåˆ¶é›†æˆåˆ°ç°æœ‰çš„è¯­è¨€æ¨¡å‹ç®¡é“ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å¹»è§‰è¾“å‡ºé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ£€æµ‹å’Œå‡è½»å¹»è§‰æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆè¯†åˆ«å’Œå¤„ç†è¿™äº›ä¸å‡†ç¡®çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„åäº‹å®æ¢æµ‹æ–¹æ³•é€šè¿‡åŠ¨æ€ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†åŒ…å«ç»†å¾®äº‹å®é”™è¯¯çš„åäº‹å®é™ˆè¿°ï¼Œæ¥è¯„ä¼°æ¨¡å‹å¯¹è¿™äº›æ‰°åŠ¨çš„æ•æ„Ÿæ€§ï¼Œä»è€Œè¯†åˆ«å¹»è§‰å†…å®¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬åäº‹å®ç”Ÿæˆæ¨¡å—å’Œæ•æ„Ÿæ€§è¯„ä¼°æ¨¡å—ã€‚åäº‹å®ç”Ÿæˆæ¨¡å—è´Ÿè´£åˆ›å»ºåäº‹å®é™ˆè¿°ï¼Œè€Œæ•æ„Ÿæ€§è¯„ä¼°æ¨¡å—åˆ™åˆ†ææ¨¡å‹å¯¹è¿™äº›é™ˆè¿°çš„ååº”ï¼Œä»¥åˆ¤æ–­è¾“å‡ºçš„å¯é æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šåäº‹å®æ¢æµ‹çš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶åŠ¨æ€ç”Ÿæˆåäº‹å®é™ˆè¿°çš„èƒ½åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿçš„é™æ€æ£€æµ‹æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«å¹»è§‰å†…å®¹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬åäº‹å®é™ˆè¿°çš„ç”Ÿæˆç­–ç•¥å’Œæ¨¡å‹å¯¹è¿™äº›é™ˆè¿°çš„ç½®ä¿¡åº¦è¯„ä¼°ï¼ŒæŸå¤±å‡½æ•°åˆ™ç”¨äºä¼˜åŒ–æ¨¡å‹åœ¨é¢å¯¹åäº‹å®æ—¶çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåäº‹å®æ¢æµ‹æ–¹æ³•åœ¨æ£€æµ‹æ€§èƒ½ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºå¹»è§‰æ£€æµ‹å‡†ç¡®ç‡æ˜¾è‘—æå‡ï¼ŒåŒæ—¶è‡ªé€‚åº”å‡è½»ç­–ç•¥å¹³å‡é™ä½äº†å¹»è§‰è¯„åˆ†24.5%ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´é«˜æ•ˆçš„å®æ—¶éªŒè¯æœºåˆ¶çš„å‘å±•ï¼Œæå‡äººæœºäº¤äº’çš„è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models have demonstrated remarkable capabilities across diverse tasks, yet they frequently generate hallucinations outputs that are fluent but factually incorrect or unsupported. We propose Counterfactual Probing, a novel approach for detecting and mitigating hallucinations in LLM outputs. Our method dynamically generates counterfactual statements that appear plausible but contain subtle factual errors, then evaluates the model's sensitivity to these perturbations. We hypothesize that genuine knowledge exhibits robustness to counterfactual variations, while hallucinated content shows inconsistent confidence patterns when confronted with plausible alternatives. Our comprehensive evaluation on TruthfulQA, factual statement datasets, and curated hallucination examples demonstrates that counterfactual probing achieves superior detection performance compared to baseline methods, while our adaptive mitigation strategies reduce hallucination scores by an average of 24.5%. The approach requires no model retraining and can be integrated into existing LLM pipelines as a realtime verification mechanism.

