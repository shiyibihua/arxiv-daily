---
layout: default
title: A comprehensive taxonomy of hallucinations in Large Language Models
---

# A comprehensive taxonomy of hallucinations in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01781" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01781v1</a>
  <a href="https://arxiv.org/pdf/2508.01781.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01781v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01781v1', 'A comprehensive taxonomy of hallucinations in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Manuel Cossio

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-03

**å¤‡æ³¨**: 55 pages, 16 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå…¨é¢åˆ†ç±»æ³•ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¹»è§‰åˆ†ç±»` `è‡ªç„¶è¯­è¨€å¤„ç†` `å†…å®¹ç”Ÿæˆ` `æ¨¡å‹å¯é æ€§` `æ£€æµ‹ç­–ç•¥` `ç¼“è§£æªæ–½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶å¸¸å‡ºç°å¹»è§‰ç°è±¡ï¼Œå¯¼è‡´ç”Ÿæˆçš„å†…å®¹è™½ç„¶çœ‹ä¼¼åˆç†ï¼Œä½†å´ç¼ºä¹äº‹å®ä¾æ®ï¼Œå½±å“å…¶å¯é æ€§ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„å¹»è§‰åˆ†ç±»æ³•ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æäº†å¹»è§‰çš„ç±»å‹ã€æˆå› åŠå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¼ºè°ƒäº†æ£€æµ‹å’Œç¼“è§£çš„é‡è¦æ€§ã€‚
3. é€šè¿‡å¯¹å¹»è§‰ç°è±¡çš„æ·±å…¥ç ”ç©¶ï¼Œè®ºæ–‡ä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡å’Œåº”ç”¨æä¾›äº†ç†è®ºæ”¯æŒå’Œå®è·µæŒ‡å¯¼ï¼Œä¿ƒè¿›äº†å¯¹LLMçš„è´Ÿè´£ä»»ä½¿ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¼•å‘äº†é©å‘½ï¼Œä½†å…¶ç”Ÿæˆä¼¼æ˜¯è€Œéçš„å†…å®¹ï¼ˆå³å¹»è§‰ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚æœ¬æ–‡æä¾›äº†LLMå¹»è§‰çš„å…¨é¢åˆ†ç±»ï¼Œé¦–å…ˆç»™å‡ºäº†æ­£å¼å®šä¹‰å’Œç†è®ºæ¡†æ¶ï¼Œè®¤ä¸ºåœ¨å¯è®¡ç®—çš„LLMä¸­ï¼Œå¹»è§‰çš„äº§ç”Ÿæ˜¯ä¸å¯é¿å…çš„ã€‚æ–‡ç« åŒºåˆ†äº†å†…åœ¨å¹»è§‰ï¼ˆä¸è¾“å…¥ä¸Šä¸‹æ–‡çŸ›ç›¾ï¼‰å’Œå¤–åœ¨å¹»è§‰ï¼ˆä¸è®­ç»ƒæ•°æ®æˆ–ç°å®ä¸ä¸€è‡´ï¼‰ï¼Œä»¥åŠç»å¯¹æ­£ç¡®æ€§å’Œå¿ å®æ€§ã€‚æ¥ç€ï¼Œè¯¦ç»†æè¿°äº†å¹»è§‰çš„å…·ä½“è¡¨ç°ï¼ŒåŒ…æ‹¬äº‹å®é”™è¯¯ã€ä¸Šä¸‹æ–‡å’Œé€»è¾‘ä¸ä¸€è‡´ã€æ—¶é—´é”™ä½ã€ä¼¦ç†è¿è§„ç­‰ã€‚æœ€åï¼Œæ–‡ç« åˆ†æäº†å¹»è§‰çš„æ ¹æœ¬åŸå› ï¼Œå¹¶æå‡ºäº†æ£€æµ‹å’Œç¼“è§£çš„ç­–ç•¥ï¼Œå¼ºè°ƒäº†æœªæ¥åœ¨å…³é”®åº”ç”¨ä¸­éœ€è¦æŒç»­çš„äººç±»ç›‘ç£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶å‡ºç°çš„å¹»è§‰ç°è±¡ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œåˆ†ç±»è¿™äº›å¹»è§‰ï¼Œå¯¼è‡´æ¨¡å‹è¾“å‡ºçš„å¯é æ€§å—åˆ°è´¨ç–‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„å¹»è§‰åˆ†ç±»æ³•ï¼ŒåŸºäºå¯¹å¹»è§‰çš„å®šä¹‰å’Œç†è®ºæ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æå¹»è§‰çš„ç±»å‹åŠå…¶æˆå› ï¼Œå¼ºè°ƒäº†æ£€æµ‹å’Œç¼“è§£ç­–ç•¥çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¹»è§‰çš„å®šä¹‰ã€åˆ†ç±»ã€æˆå› åˆ†æã€æ£€æµ‹æŒ‡æ ‡å’Œç¼“è§£ç­–ç•¥äº”ä¸ªä¸»è¦æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„ç†è®ºå’Œå®è·µæ¡†æ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†å†…åœ¨å¹»è§‰ä¸å¤–åœ¨å¹»è§‰çš„åŒºåˆ†ï¼Œä»¥åŠå¯¹å¹»è§‰çš„å¤šç»´åº¦åˆ†æï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†å¤šç§è¯„ä¼°åŸºå‡†å’ŒæŒ‡æ ‡æ¥æ£€æµ‹å¹»è§‰ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹æ•°æ®ã€æ¨¡å‹å’Œæç¤ºçš„ç¼“è§£ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡ç³»ç»Ÿåˆ†æå¹»è§‰ç°è±¡ï¼Œæå‡ºäº†å¤šç§æ£€æµ‹å’Œç¼“è§£ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å…·ä½“è€Œè¨€ï¼Œé’ˆå¯¹ä¸åŒç±»å‹çš„å¹»è§‰ï¼Œæå‡ºçš„ç¼“è§£æªæ–½åœ¨å®éªŒä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç¡®ä¿äº†ç”Ÿæˆå†…å®¹çš„æ›´é«˜å‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆç­‰ï¼Œèƒ½å¤Ÿä¸ºå¼€å‘æ›´å¯é çš„è¯­è¨€æ¨¡å‹æä¾›ç†è®ºæ”¯æŒå’Œå®è·µæŒ‡å¯¼ã€‚é€šè¿‡æœ‰æ•ˆçš„å¹»è§‰æ£€æµ‹å’Œç¼“è§£ç­–ç•¥ï¼Œæå‡æ¨¡å‹åœ¨å…³é”®åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œç¡®ä¿å…¶è¾“å‡ºçš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have revolutionized natural language processing, yet their propensity for hallucination, generating plausible but factually incorrect or fabricated content, remains a critical challenge. This report provides a comprehensive taxonomy of LLM hallucinations, beginning with a formal definition and a theoretical framework that posits its inherent inevitability in computable LLMs, irrespective of architecture or training. It explores core distinctions, differentiating between intrinsic (contradicting input context) and extrinsic (inconsistent with training data or reality), as well as factuality (absolute correctness) and faithfulness (adherence to input). The report then details specific manifestations, including factual errors, contextual and logical inconsistencies, temporal disorientation, ethical violations, and task-specific hallucinations across domains like code generation and multimodal applications. It analyzes the underlying causes, categorizing them into data-related issues, model-related factors, and prompt-related influences. Furthermore, the report examines cognitive and human factors influencing hallucination perception, surveys evaluation benchmarks and metrics for detection, and outlines architectural and systemic mitigation strategies. Finally, it introduces web-based resources for monitoring LLM releases and performance. This report underscores the complex, multifaceted nature of LLM hallucinations and emphasizes that, given their theoretical inevitability, future efforts must focus on robust detection, mitigation, and continuous human oversight for responsible and reliable deployment in critical applications.

