---
layout: default
title: The Bidirectional Process Reward Model
---

# The Bidirectional Process Reward Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01682" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01682v1</a>
  <a href="https://arxiv.org/pdf/2508.01682.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01682v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01682v1', 'The Bidirectional Process Reward Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingyin Zhang, Jun Gao, Xiaoxue Ren, Ziqiang Cao

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒå‘è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†è´¨é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¿‡ç¨‹å¥–åŠ±æ¨¡å‹` `åŒå‘è¯„ä¼°` `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†è´¨é‡` `æ•°å­¦æ¨ç†` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ™ºèƒ½é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ä¸»è¦é‡‡ç”¨å•å‘è¯„ä¼°ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ¨ç†ä¸€è‡´æ€§éªŒè¯å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºåŒå‘è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆBiPRMï¼‰ï¼Œé€šè¿‡å¼•å…¥å¹¶è¡Œçš„ä»å³åˆ°å·¦è¯„ä¼°æµï¼Œå®æ—¶è¯„ä¼°æ—©æœŸæ¨ç†æ­¥éª¤ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBiPRMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ­¥è¿›å¥–åŠ±è¯„ä¼°æå‡å¹…åº¦æœ€é«˜è¾¾31.9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä½œä¸ºä¸€ç§æ–°å…´æ–¹æ³•ï¼Œé€šè¿‡å¯¹è§£å†³æ–¹æ¡ˆè½¨è¿¹ä¸­çš„ä¸­é—´æ¨ç†æ­¥éª¤èµ‹äºˆç»†ç²’åº¦è¯„åˆ†ï¼Œæå‡äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†è´¨é‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PRMsä¸»è¦é‡‡ç”¨å•å‘çš„ä»å·¦åˆ°å³ï¼ˆL2Rï¼‰è¯„ä¼°æ¨¡å¼ï¼Œé™åˆ¶äº†å…¶åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œå¯¼è‡´éš¾ä»¥æ ¹æ®åç»­æ­¥éª¤éªŒè¯æ—©æœŸæ­¥éª¤çš„ä¸€è‡´æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒå‘è¯„ä¼°æ¨¡å¼ï¼Œç§°ä¸ºåŒå‘è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆBiPRMï¼‰ã€‚BiPRMåœ¨ä¼ ç»ŸL2Ræµçš„åŸºç¡€ä¸Šï¼Œå¢è®¾äº†å¹¶è¡Œçš„ä»å³åˆ°å·¦ï¼ˆR2Lï¼‰è¯„ä¼°æµï¼Œä½¿å¾—åç»­æ¨ç†æ­¥éª¤èƒ½å¤Ÿå®æ—¶å¸®åŠ©è¯„ä¼°æ—©æœŸæ­¥éª¤ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå†…ç½®çš„R2Lè¯„ä¼°ä»…é€šè¿‡åè½¬åŸå§‹æ¨ç†è½¨è¿¹çš„æç¤ºä¿®æ”¹å®ç°ï¼Œæ— éœ€å¼•å…¥é¢å¤–å‚æ•°æˆ–æ¨ç†å»¶è¿Ÿï¼Œä»è€Œç¡®ä¿BiPRMçš„é«˜æ•ˆæ€§å’Œä¸ç°æœ‰PRMç ”ç©¶çš„å¹¿æ³›å…¼å®¹æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜BiPRMåœ¨æ‰€æœ‰è®¾ç½®ä¸­å‡ä¼˜äºå•å‘åŸºçº¿ï¼Œæ­¥è¿›å¥–åŠ±è¯„ä¼°æå‡å¹…åº¦æœ€é«˜å¯è¾¾31.9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»è¦é‡‡ç”¨å•å‘è¯„ä¼°ï¼Œé™åˆ¶äº†å…¶åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œå¯¼è‡´éš¾ä»¥éªŒè¯æ—©æœŸæ¨ç†æ­¥éª¤çš„ä¸€è‡´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºåŒå‘è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆBiPRMï¼‰ï¼Œé€šè¿‡å¹¶è¡Œçš„ä»å³åˆ°å·¦ï¼ˆR2Lï¼‰è¯„ä¼°æµï¼Œå…è®¸åç»­æ¨ç†æ­¥éª¤å®æ—¶å½±å“æ—©æœŸæ­¥éª¤çš„è¯„ä¼°ï¼Œä»è€Œæå‡æ•´ä½“æ¨ç†è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBiPRMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦è¯„ä¼°æµï¼šä¼ ç»Ÿçš„ä»å·¦åˆ°å³ï¼ˆL2Rï¼‰æµå’Œæ–°å¼•å…¥çš„ä»å³åˆ°å·¦ï¼ˆR2Lï¼‰æµã€‚R2Læµé€šè¿‡ç®€å•çš„æç¤ºä¿®æ”¹å®ç°ï¼Œæ— éœ€é¢å¤–çš„å‚æ•°æˆ–æ¨ç†å»¶è¿Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šBiPRMçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŒå‘è¯„ä¼°æœºåˆ¶ï¼Œä½¿å¾—åç»­æ­¥éª¤èƒ½å¤Ÿå®æ—¶åé¦ˆå’Œè°ƒæ•´æ—©æœŸæ­¥éª¤çš„è¯„ä¼°ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†æ¨ç†çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸Šï¼ŒBiPRMé€šè¿‡åè½¬æ¨ç†è½¨è¿¹çš„æç¤ºæ¥æ„å»ºR2Læµï¼Œç¡®ä¿äº†ä¸ç°æœ‰PRMæ–¹æ³•çš„å…¼å®¹æ€§ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒBiPRMåœ¨æ­¥è¿›å¥–åŠ±è¯„ä¼°ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºå•å‘åŸºçº¿ï¼Œæå‡å¹…åº¦æœ€é«˜å¯è¾¾31.9%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜BiPRMåœ¨æ¨ç†è´¨é‡å’Œä¸€è‡´æ€§éªŒè¯æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„åŒå‘è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆBiPRMï¼‰å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¤æ‚æ¨ç†å’Œå†³ç­–æ”¯æŒçš„é¢†åŸŸï¼Œå¦‚æ•°å­¦æ¨ç†ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æ¨ç†è´¨é‡ï¼ŒBiPRMèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´ä¸ºå¯é çš„æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning quality of Large Language Models (LLMs) by assigning fine-grained scores to intermediate reasoning steps within a solution trajectory. However, existing PRMs predominantly adopt a unidirectional left-to-right (L2R) evaluation paradigm, which limits their ability to leverage global context, making it challenging to verify the consistency of earlier steps based on later ones. In light of these challenges, we propose a novel bidirectional evaluation paradigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly incorporates a parallel right-to-left (R2L) evaluation stream alongside the conventional L2R flow, enabling later reasoning steps to help assess earlier ones in real time. Notably, the built-in R2L evaluation is implemented solely through prompt modifications that reverse the original reasoning trajectory, without any additional parameters or inference latency introduced. This ensures BiPRM remains both efficient and broadly compatible with existing PRM studies. We conduct extensive experiments on two mathematical reasoning benchmarks using samples generated by three different policy models. Our method, BiPRM, is evaluated across three backbones and three distinct PRM objectives. Across all settings, BiPRM consistently outperforms unidirectional baselines, achieving up to a 31.9% improvement in stepwise reward evaluation. Generally, our results highlight BiPRM's effectiveness, robustness, and general applicability, offering a promising new direction for process-based reward modeling.

