---
layout: default
title: CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy
---

# CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01696" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01696v3</a>
  <a href="https://arxiv.org/pdf/2508.01696.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01696v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01696v3', 'CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bing Qin

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-03 (æ›´æ–°: 2025-10-09)

**å¤‡æ³¨**: code available at https://github.com/liunian-Jay/CoCoA

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoCoAæ¡†æ¶ä»¥å¢å¼ºå‚æ•°åŒ–ä¸æ£€ç´¢çŸ¥è¯†çš„ååŒä½œç”¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `çŸ¥è¯†ååŒ` `å¤šä»£ç†ç³»ç»Ÿ` `å¼€æ”¾åŸŸé—®ç­”` `é•¿é“¾è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†åˆ©ç”¨å†…éƒ¨å‚æ•°çŸ¥è¯†ä¸å¤–éƒ¨æ£€ç´¢çŸ¥è¯†çš„ååŒä½œç”¨ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å—åˆ°å½±å“ã€‚
2. æœ¬æ–‡æå‡ºçš„CoCoAæ¡†æ¶é€šè¿‡å¼•å…¥å¤šä»£ç†åä½œæœºåˆ¶ï¼Œå¢å¼ºäº†å‚æ•°åŒ–çŸ¥è¯†ä¸æ£€ç´¢çŸ¥è¯†çš„æ˜¾å¼ååŒï¼Œæå‡äº†ç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoCoAåœ¨å¼€æ”¾åŸŸé—®ç­”å’Œå¤šè·³é—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç°æœ‰RAGæ–¹æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†åˆ©ç”¨çŸ¥è¯†ï¼Œå°¤å…¶æ˜¯æ¨¡å‹å†…éƒ¨çš„å‚æ•°çŸ¥è¯†ä¸å¤–éƒ¨æ£€ç´¢çŸ¥è¯†ä¹‹é—´çš„ååŒä½œç”¨æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åä½œä»£ç†é“¾ï¼ˆCoCoAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æ˜¾è‘—å¢å¼ºå‚æ•°åŒ–çŸ¥è¯†ä¸æ£€ç´¢çŸ¥è¯†çš„ååŒã€‚å…·ä½“è€Œè¨€ï¼Œé¦–å…ˆå¼•å…¥CoCoA-zeroå¤šä»£ç†RAGæ¡†æ¶è¿›è¡Œæ¡ä»¶çŸ¥è¯†å½’çº³ï¼Œç„¶åæ¨ç†ç­”æ¡ˆã€‚åŸºäºæ­¤ï¼Œå‘å±•äº†CoCoAé•¿é“¾è®­ç»ƒç­–ç•¥ï¼Œä»CoCoA-zeroåˆæˆæ‰©å±•çš„å¤šä»£ç†æ¨ç†è½¨è¿¹ï¼Œä»¥å¾®è°ƒLLMã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoCoAåœ¨å¼€æ”¾åŸŸé—®ç­”å’Œå¤šè·³é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RAGæ–¹æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†åˆ©ç”¨æ¨¡å‹å†…éƒ¨å‚æ•°çŸ¥è¯†ä¸å¤–éƒ¨æ£€ç´¢çŸ¥è¯†ä¹‹é—´çš„ååŒä½œç”¨çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨çŸ¥è¯†åˆ©ç”¨ä¸Šå­˜åœ¨å±€é™ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„CoCoAæ¡†æ¶é€šè¿‡å¼•å…¥å¤šä»£ç†åä½œæœºåˆ¶ï¼Œé¦–å…ˆè¿›è¡Œæ¡ä»¶çŸ¥è¯†å½’çº³ï¼Œç„¶åè¿›è¡Œç­”æ¡ˆæ¨ç†ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºå‚æ•°åŒ–çŸ¥è¯†ä¸æ£€ç´¢çŸ¥è¯†çš„ååŒä½œç”¨ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜ç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒçŸ¥è¯†æ•´åˆèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoCoAæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šCoCoA-zeroå’ŒCoCoAã€‚CoCoA-zeroè´Ÿè´£æ‰§è¡Œæ¡ä»¶çŸ¥è¯†å½’çº³ï¼Œéšåç”Ÿæˆç­”æ¡ˆï¼›CoCoAåˆ™é€šè¿‡é•¿é“¾è®­ç»ƒç­–ç•¥ï¼Œåˆæˆå¤šä»£ç†æ¨ç†è½¨è¿¹ï¼Œä»¥å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoCoAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶å¤šä»£ç†åä½œæœºåˆ¶å’Œé•¿é“¾è®­ç»ƒç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»ŸRAGæ–¹æ³•çš„å•ä¸€ä»£ç†æ¨ç†æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚é€šè¿‡æ˜¾å¼çš„çŸ¥è¯†ååŒï¼ŒCoCoAèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•´åˆå†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒCoCoAæ¡†æ¶é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–çŸ¥è¯†æ•´åˆæ•ˆæœï¼Œå¹¶é€šè¿‡å¤šä»£ç†çš„æ–¹å¼å¢å¼ºæ¨ç†è¿‡ç¨‹çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCoCoAåœ¨å¼€æ”¾åŸŸé—®ç­”å’Œå¤šè·³é—®ç­”ä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºå‡†ç¡®ç‡æé«˜äº†çº¦15%ï¼ŒéªŒè¯äº†å…¶åœ¨çŸ¥è¯†æ•´åˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¼€æ”¾åŸŸé—®ç­”ç³»ç»Ÿã€æ™ºèƒ½å®¢æœã€æ•™è‚²è¾…åŠ©å·¥å…·ç­‰ã€‚é€šè¿‡å¢å¼ºçŸ¥è¯†çš„ååŒä½œç”¨ï¼ŒCoCoAæ¡†æ¶èƒ½å¤Ÿæé«˜ç”Ÿæˆæ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs), especially for knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to fully exploit knowledge during generation. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experimental results demonstrate the superiority of CoCoA in open-domain QA and multi-hop QA.

