---
layout: default
title: Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective
---

# Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05166" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05166v2</a>
  <a href="https://arxiv.org/pdf/2506.05166.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05166v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05166v2', 'Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bhavik Chandna, Zubair Bashir, Procheta Sen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æœºæ¢°è§£é‡Šæ–¹æ³•åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åè§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åè§åˆ†æ` `æœºæ¢°è§£é‡Š` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å…¬å¹³æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç¤¾ä¼šå’Œæ€§åˆ«åè§æ—¶å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´è¾“å‡ºç»“æœçš„ä¸å…¬æ­£æ€§ã€‚
2. æœ¬æ–‡é€šè¿‡æœºæ¢°è§£é‡Šçš„æ–¹æ³•ï¼Œç³»ç»Ÿåˆ†ææ¨¡å‹å†…éƒ¨ç»“æ„ï¼Œè¯†åˆ«å‡ºå¯¼è‡´åè§çš„å…·ä½“ç»„ä»¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç§»é™¤åè§ç›¸å…³ç»„ä»¶å¯æœ‰æ•ˆå‡å°‘åè§è¾“å‡ºï¼ŒåŒæ—¶å½±å“å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸è¡¨ç°å‡ºç¤¾ä¼šã€äººå£å’Œæ€§åˆ«åè§ï¼Œè¿™å¾€å¾€æ˜¯ç”±äºè®­ç»ƒæ•°æ®çš„å½±å“ã€‚æœ¬æ–‡é‡‡ç”¨æœºæ¢°è§£é‡Šçš„æ–¹æ³•ï¼Œåˆ†æè¿™äº›åè§åœ¨GPT-2å’ŒLlama2ç­‰æ¨¡å‹ä¸­çš„ç»“æ„æ€§è¡¨ç°ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨äººå£å’Œæ€§åˆ«åè§ï¼Œæ¢ç´¢ä¸åŒæŒ‡æ ‡ä»¥è¯†åˆ«å¯¼è‡´åè§è¡Œä¸ºçš„å†…éƒ¨è¾¹ç¼˜ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶ˆèå®éªŒï¼Œæˆ‘ä»¬è¯æ˜åè§ç›¸å…³çš„è®¡ç®—é«˜åº¦å±€éƒ¨åŒ–ï¼Œé€šå¸¸é›†ä¸­åœ¨å°‘æ•°å±‚ä¸­ã€‚æ­¤å¤–ï¼Œè¯†åˆ«çš„ç»„ä»¶åœ¨ä¸åŒçš„å¾®è°ƒè®¾ç½®ä¸­ä¼šå‘ç”Ÿå˜åŒ–ï¼Œç”šè‡³ä¸åè§æ— å…³çš„è®¾ç½®ä¹Ÿä¼šå½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç§»é™¤è¿™äº›ç»„ä»¶ä¸ä»…å‡å°‘äº†åè§è¾“å‡ºï¼Œè¿˜å½±å“äº†å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«å’Œè¯­è¨€å¯æ¥å—æ€§åˆ¤æ–­ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡ä¸é‡è¦ç»„ä»¶å…±äº«ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„ç¤¾ä¼šã€äººå£å’Œæ€§åˆ«åè§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œæ¶ˆé™¤è¿™äº›åè§ï¼Œå¯¼è‡´æ¨¡å‹è¾“å‡ºä¸å…¬æ­£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æœºæ¢°è§£é‡Šçš„æ–¹æ³•ï¼Œåˆ†ææ¨¡å‹å†…éƒ¨ç»“æ„ï¼Œè¯†åˆ«å‡ºä¸åè§ç›¸å…³çš„è®¡ç®—ç»„ä»¶ï¼Œå¹¶è¯„ä¼°å…¶åœ¨ä¸åŒæ•°æ®é›†å’Œè¯­è¨€å˜ä½“ä¸­çš„ç¨³å®šæ€§å’Œå±€éƒ¨åŒ–ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå®šä¹‰åè§çš„åº¦é‡æ ‡å‡†ï¼Œç„¶åé€šè¿‡ç³»ç»Ÿçš„æ¶ˆèå®éªŒï¼Œåˆ†æä¸åŒå±‚æ¬¡çš„åè§è¡¨ç°ï¼Œæœ€åè¯„ä¼°ç§»é™¤åè§ç»„ä»¶å¯¹å…¶ä»–ä»»åŠ¡çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæ­ç¤ºäº†åè§è®¡ç®—çš„é«˜åº¦å±€éƒ¨åŒ–ç‰¹å¾ï¼Œä¸”è¿™äº›ç‰¹å¾åœ¨ä¸åŒçš„å¾®è°ƒè®¾ç½®ä¸­ä¼šå‘ç”Ÿå˜åŒ–ï¼Œæä¾›äº†å¯¹åè§çš„æ·±åˆ»ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨äº†å¤šç§æŒ‡æ ‡æ¥è¯„ä¼°åè§è¡¨ç°ï¼Œè®¾è®¡äº†ç³»ç»Ÿçš„æ¶ˆèå®éªŒä»¥éªŒè¯åè§ç»„ä»¶çš„å½±å“ï¼Œç¡®ä¿äº†å®éªŒçš„å…¨é¢æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç§»é™¤åè§ç›¸å…³ç»„ä»¶åï¼Œæ¨¡å‹çš„åè§è¾“å‡ºæ˜¾è‘—å‡å°‘ï¼Œä¸”åœ¨å‘½åå®ä½“è¯†åˆ«å’Œè¯­è¨€å¯æ¥å—æ€§åˆ¤æ–­ç­‰ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½å˜åŒ–æ˜æ˜¾ã€‚è¿™è¡¨æ˜åè§ç»„ä»¶ä¸å…¶ä»–ä»»åŠ¡å­˜åœ¨é‡è¦çš„å…±äº«å…³ç³»ï¼Œå½±å“äº†æ¨¡å‹çš„æ•´ä½“è¡¨ç°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…¬å¹³æ€§æä¾›äº†æ–°çš„è§†è§’ï¼Œæ½œåœ¨åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†çš„å„ä¸ªé¢†åŸŸï¼Œå¦‚ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ã€æ‹›è˜ç³»ç»Ÿå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡è¯†åˆ«å’Œæ¶ˆé™¤åè§ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„å…¬æ­£æ€§å’Œå¯ä¿¡åº¦ï¼Œä¿ƒè¿›æ›´å¹¿æ³›çš„ç¤¾ä¼šæ¥å—åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are known to exhibit social, demographic, and gender biases, often as a consequence of the data on which they are trained. In this work, we adopt a mechanistic interpretability approach to analyze how such biases are structurally represented within models such as GPT-2 and Llama2. Focusing on demographic and gender biases, we explore different metrics to identify the internal edges responsible for biased behavior. We then assess the stability, localization, and generalizability of these components across dataset and linguistic variations. Through systematic ablations, we demonstrate that bias-related computations are highly localized, often concentrated in a small subset of layers. Moreover, the identified components change across fine-tuning settings, including those unrelated to bias. Finally, we show that removing these components not only reduces biased outputs but also affects other NLP tasks, such as named entity recognition and linguistic acceptability judgment because of the sharing of important components with these tasks.

