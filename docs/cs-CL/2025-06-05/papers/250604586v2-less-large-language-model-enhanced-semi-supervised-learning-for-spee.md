---
layout: default
title: LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models Using in-the-wild Data
---

# LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models Using in-the-wild Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04586" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04586v2</a>
  <a href="https://arxiv.org/pdf/2506.04586.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04586v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04586v2', 'LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models Using in-the-wild Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wen Ding, Fan Qian

**åˆ†ç±»**: cs.CL, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-09-19)

**å¤‡æ³¨**: Submitted to ICASSP 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLESSæ¡†æ¶ä»¥è§£å†³çœŸå®ç¯å¢ƒä¸‹è¯­éŸ³æ¨¡å‹çš„åŠç›‘ç£å­¦ä¹ æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠç›‘ç£å­¦ä¹ ` `è¯­éŸ³è¯†åˆ«` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¼ªæ ‡ç­¾ä¿®æ­£` `æ•°æ®è¿‡æ»¤` `è‡ªç„¶è¯­è¨€å¤„ç†` `çœŸå®ç¯å¢ƒæ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨çœŸå®ç¯å¢ƒæ•°æ®ä¸Šåº”ç”¨åŠç›‘ç£å­¦ä¹ æ—¶ï¼Œé¢ä¸´å£°å­¦ç‰¹æ€§å¤æ‚å¯¼è‡´çš„ä¼ªæ ‡ç­¾è´¨é‡é—®é¢˜ã€‚
2. LESSæ¡†æ¶é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ä¿®æ­£ä¼ªæ ‡ç­¾ï¼Œå¹¶ç»“åˆæ•°æ®è¿‡æ»¤ç­–ç•¥ï¼Œæå‡ä¼ªæ ‡ç­¾çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚
3. åœ¨å®éªŒä¸­ï¼ŒLESSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡å–å¾—æ˜¾è‘—æå‡ï¼Œå­—é”™è¯¯ç‡é™ä½3.8%ï¼ŒBLEUåˆ†æ•°åˆ†åˆ«æé«˜0.8å’Œ0.7ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡ç°æœ‰çš„è¯­éŸ³åŸºç¡€æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬ä¼ªæ ‡ç­¾ï¼Œä½†åœ¨çœŸå®ç¯å¢ƒæ•°æ®ä¸Šåº”ç”¨åŠç›‘ç£å­¦ä¹ ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›æ•°æ®çš„å£°å­¦ç‰¹æ€§æ¯”ç»è¿‡æ•´ç†çš„æ•°æ®é›†æ›´å¤æ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LESSï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„åŠç›‘ç£å­¦ä¹ ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥ä¿®æ­£åœ¨çœŸå®ç¯å¢ƒæ•°æ®ä¸Šç”Ÿæˆçš„ä¼ªæ ‡ç­¾ã€‚åœ¨LESSæ¡†æ¶ä¸­ï¼Œæ¥è‡ªæ— ç›‘ç£æ•°æ®çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–è‡ªåŠ¨è¯­éŸ³ç¿»è¯‘ï¼ˆASTï¼‰ç”Ÿæˆçš„ä¼ªæ ‡ç­¾æ–‡æœ¬é€šè¿‡LLMè¿›è¡Œç²¾ç‚¼ï¼Œå¹¶é€šè¿‡æ•°æ®è¿‡æ»¤ç­–ç•¥è¿›ä¸€æ­¥æ”¹è¿›ã€‚åœ¨æ™®é€šè¯ASRå’Œè¥¿ç­ç‰™è¯­åˆ°è‹±è¯­ASTçš„è¯„ä¼°ä¸­ï¼ŒLESSåœ¨WenetSpeechä¸Šå®ç°äº†3.8%çš„ç»å¯¹å­—é”™è¯¯ç‡é™ä½ï¼Œåœ¨Callhomeå’ŒFisheræµ‹è¯•é›†ä¸Šåˆ†åˆ«å®ç°äº†BLEUåˆ†æ•°çš„0.8å’Œ0.7çš„æå‡ï¼Œæ˜¾ç¤ºå‡ºLESSåœ¨å¤šç§è¯­è¨€ã€ä»»åŠ¡å’Œé¢†åŸŸä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å·²å°†è¯¥æ–¹æ³•çš„å®ç°å¼€æºï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åœ¨çœŸå®ç¯å¢ƒæ•°æ®ä¸Šæœ‰æ•ˆåº”ç”¨åŠç›‘ç£å­¦ä¹ ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚å£°å­¦ç‰¹æ€§æ—¶ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è´¨é‡è¾ƒä½ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä¼ªæ ‡ç­¾è¿›è¡Œä¿®æ­£ï¼Œä»è€Œæé«˜ä¼ªæ ‡ç­¾çš„è´¨é‡ï¼Œå¹¶é€šè¿‡æ•°æ®è¿‡æ»¤ç­–ç•¥è¿›ä¸€æ­¥ä¼˜åŒ–æ•°æ®é›†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLESSæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å—ï¼ˆASRæˆ–ASTï¼‰ã€LLMä¿®æ­£æ¨¡å—å’Œæ•°æ®è¿‡æ»¤æ¨¡å—ã€‚é¦–å…ˆç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œç„¶åé€šè¿‡LLMè¿›è¡Œç²¾ç‚¼ï¼Œæœ€ååº”ç”¨æ•°æ®è¿‡æ»¤ç­–ç•¥æå‡æ•°æ®è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šLESSçš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹å¼•å…¥ä¼ªæ ‡ç­¾ä¿®æ­£è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†ä¼ªæ ‡ç­¾çš„å‡†ç¡®æ€§ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒLESSèƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹å¤æ‚çš„çœŸå®ç¯å¢ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒLESSæ¡†æ¶é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ä¼ªæ ‡ç­¾çš„ä¿®æ­£è¿‡ç¨‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šç»“åˆäº†LLMçš„ç‰¹æ€§ï¼Œä»¥ç¡®ä¿ä¼ªæ ‡ç­¾çš„é«˜è´¨é‡è¾“å‡ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LESSåœ¨å¤šä¸ªè¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ï¼ŒWenetSpeechæ•°æ®é›†ä¸Šå®ç°äº†3.8%çš„å­—é”™è¯¯ç‡é™ä½ï¼ŒCallhomeå’ŒFisheræµ‹è¯•é›†çš„BLEUåˆ†æ•°åˆ†åˆ«æé«˜äº†0.8å’Œ0.7ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ¡†æ¶åœ¨å¤šè¯­è¨€å’Œå¤šä»»åŠ¡åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³ç¿»è¯‘å’Œå…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡åœ¨çœŸå®ç¯å¢ƒä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚LESSæ¡†æ¶çš„å¼€æºå®ç°ä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„å·¥å…·å’Œæ€è·¯ï¼Œæ¨åŠ¨äº†è¯­éŸ³æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although state-of-the-art Speech Foundation Models can produce high-quality text pseudo-labels, applying Semi-Supervised Learning (SSL) for in-the-wild real-world data remains challenging due to its richer and more complex acoustics compared to curated datasets. To address the challenges, we introduce LESS (Large Language Model Enhanced Semi-supervised Learning), a versatile framework that uses Large Language Models (LLMs) to correct pseudo-labels generated on in-the-wild data. In the LESS framework, pseudo-labeled text from Automatic Speech Recognition (ASR) or Automatic Speech Translation (AST) of the unsupervised data is refined by an LLM, and further improved by a data filtering strategy. Across Mandarin ASR and Spanish-to-English AST evaluations, LESS delivers consistent gains, with an absolute Word Error Rate reduction of 3.8% on WenetSpeech, and BLEU score increase of 0.8 and 0.7, achieving 34.0 on Callhome and 64.7 on Fisher testsets respectively. These results highlight LESS's effectiveness across diverse languages, tasks, and domains. We have released the recipe as open source to facilitate further research in this area.

