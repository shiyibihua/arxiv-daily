---
layout: default
title: Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques
---

# Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04788" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04788v1</a>
  <a href="https://arxiv.org/pdf/2506.04788.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04788v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04788v1', 'Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jisu An, Junseok Lee, Jeoungeun Lee, Yongseok Son

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: 18 pages, 3 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMä¸­å¿ƒçš„å¤šæ¨¡æ€èåˆæ¡†æ¶ä»¥è§£å†³ç°æœ‰æ•´åˆç­–ç•¥çš„ä¸è¶³**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡æ€æ•´åˆ` `è¡¨ç¤ºå­¦ä¹ ` `è®­ç»ƒç­–ç•¥` `åˆ†ç±»æ¡†æ¶` `äººå·¥æ™ºèƒ½` `ç³»ç»Ÿåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•´åˆç­–ç•¥ç¼ºä¹ç³»ç»Ÿæ€§ç†è§£ï¼Œå¯¼è‡´ä¸åŒæ¨¡æ€çš„è¿æ¥æ€§ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„åˆ†ç±»æ¡†æ¶ï¼Œç³»ç»Ÿåˆ†ææ¨¡æ€æ•´åˆçš„æ¶æ„ã€è¡¨ç¤ºå­¦ä¹ å’Œè®­ç»ƒç­–ç•¥ã€‚
3. é€šè¿‡å¯¹125ä¸ªMLLMçš„ç ”ç©¶ï¼Œè¯†åˆ«å‡ºæ–°å…´æ¨¡å¼ï¼Œä¸ºæœªæ¥çš„å¤šæ¨¡æ€æ•´åˆæä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ”¹å˜äº†äººå·¥æ™ºèƒ½çš„æ ¼å±€ã€‚è¿™äº›æ¨¡å‹å°†é¢„è®­ç»ƒçš„LLMä¸å„ç§æ¨¡æ€ç¼–ç å™¨ç»“åˆï¼Œè¦æ±‚ç³»ç»Ÿç†è§£ä¸åŒæ¨¡æ€å¦‚ä½•è¿æ¥åˆ°è¯­è¨€ä¸»å¹²ã€‚æœ¬æ–‡å¯¹å½“å‰æ–¹æ³•è¿›è¡Œäº†LLMä¸­å¿ƒçš„åˆ†æï¼Œæ¢è®¨äº†å°†å¤šæ ·åŒ–æ¨¡æ€è¾“å…¥è½¬åŒ–å¹¶å¯¹é½åˆ°è¯­è¨€åµŒå…¥ç©ºé—´çš„æ–¹æ³•ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®ä¸­çš„é‡è¦ç©ºç™½ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºä¸‰ä¸ªå…³é”®ç»´åº¦çš„MLLMåˆ†ç±»æ¡†æ¶ï¼Œåˆ†åˆ«æ˜¯æ¨¡æ€æ•´åˆçš„æ¶æ„ç­–ç•¥ã€è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯çš„åˆ†ç±»ä»¥åŠè®­ç»ƒèŒƒå¼çš„åˆ†æã€‚é€šè¿‡å¯¹2021è‡³2025å¹´é—´å¼€å‘çš„125ä¸ªMLLMè¿›è¡Œç ”ç©¶ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºè¯¥é¢†åŸŸçš„æ–°å…´æ¨¡å¼ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†å½“å‰æ•´åˆæŠ€æœ¯çš„ç»“æ„åŒ–æ¦‚è¿°ï¼Œæ—¨åœ¨æŒ‡å¯¼æœªæ¥åŸºäºé¢„è®­ç»ƒåŸºç¡€çš„æ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ•´åˆç­–ç•¥çš„å¼€å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ä¸åŒæ¨¡æ€å¦‚ä½•æœ‰æ•ˆæ•´åˆçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨¡æ€è¿æ¥æ€§å’Œæ•´åˆç­–ç•¥ä¸Šå­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ç³»ç»Ÿæ€§åˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æå‡ºä¸€ä¸ªLLMä¸­å¿ƒçš„åˆ†ç±»æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°åˆ†ææ¨¡æ€æ•´åˆçš„æ¶æ„ã€è¡¨ç¤ºå­¦ä¹ å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥å¡«è¡¥æ–‡çŒ®ä¸­çš„ç©ºç™½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ¨¡æ€æ•´åˆæ¶æ„ç­–ç•¥ã€è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯åˆ†ç±»ï¼ˆè”åˆè¡¨ç¤ºä¸åè°ƒè¡¨ç¤ºï¼‰ä»¥åŠè®­ç»ƒèŒƒå¼ï¼ˆè®­ç»ƒç­–ç•¥ä¸ç›®æ ‡å‡½æ•°ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æäº†å½“å‰çš„æ•´åˆæŠ€æœ¯ï¼Œè¯†åˆ«å‡ºæ–°å…´æ¨¡å¼ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å…¨é¢çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè®ºæ–‡è¯¦ç»†è®¨è®ºäº†æ¨¡æ€æ•´åˆçš„å…·ä½“æœºåˆ¶ã€èåˆå±‚æ¬¡ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠç½‘ç»œç»“æ„çš„è®¾è®¡ï¼Œä»¥ç¡®ä¿ä¸åŒæ¨¡æ€çš„æœ‰æ•ˆå¯¹é½ä¸æ•´åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

é€šè¿‡å¯¹125ä¸ªMLLMçš„åˆ†æï¼Œè®ºæ–‡è¯†åˆ«å‡ºå¤šæ¨¡æ€æ•´åˆçš„æ–°å…´æ¨¡å¼ï¼Œæä¾›äº†ç»“æ„åŒ–çš„åˆ†ç±»æ¡†æ¶ã€‚è¿™ä¸€æ¡†æ¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æŒ‡å¯¼ï¼Œä¿ƒè¿›äº†å¤šæ¨¡æ€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚é€šè¿‡æä¾›æ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ•´åˆç­–ç•¥ï¼Œæœªæ¥çš„æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†æ¥è‡ªä¸åŒæ¥æºçš„ä¿¡æ¯ï¼Œä»è€Œæå‡äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ™ºèƒ½æ°´å¹³å’Œåº”ç”¨æ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid progress of Multimodal Large Language Models(MLLMs) has transformed the AI landscape. These models combine pre-trained LLMs with various modality encoders. This integration requires a systematic understanding of how different modalities connect to the language backbone. Our survey presents an LLM-centric analysis of current approaches. We examine methods for transforming and aligning diverse modal inputs into the language embedding space. This addresses a significant gap in existing literature. We propose a classification framework for MLLMs based on three key dimensions. First, we examine architectural strategies for modality integration. This includes both the specific integration mechanisms and the fusion level. Second, we categorize representation learning techniques as either joint or coordinate representations. Third, we analyze training paradigms, including training strategies and objective functions. By examining 125 MLLMs developed between 2021 and 2025, we identify emerging patterns in the field. Our taxonomy provides researchers with a structured overview of current integration techniques. These insights aim to guide the development of more robust multimodal integration strategies for future models built on pre-trained foundations.

