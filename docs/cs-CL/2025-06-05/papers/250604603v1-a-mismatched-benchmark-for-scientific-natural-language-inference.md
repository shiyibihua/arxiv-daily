---
layout: default
title: A MISMATCHED Benchmark for Scientific Natural Language Inference
---

# A MISMATCHED Benchmark for Scientific Natural Language Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04603" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04603v1</a>
  <a href="https://arxiv.org/pdf/2506.04603.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04603v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04603v1', 'A MISMATCHED Benchmark for Scientific Natural Language Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Firoz Shaik, Mobashir Sadat, Nikita Gautam, Doina Caragea, Cornelia Caragea

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: Accepted to Findings of ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMISMATCHEDåŸºå‡†ä»¥è§£å†³ç§‘å­¦è‡ªç„¶è¯­è¨€æ¨ç†çš„é¢†åŸŸåå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç§‘å­¦è‡ªç„¶è¯­è¨€æ¨ç†` `æ•°æ®é›†æ„å»º` `é¢†åŸŸåå·®` `æ¨¡å‹è®­ç»ƒ` `è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç§‘å­¦è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨è®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œå¿½è§†äº†å¿ƒç†å­¦ã€å·¥ç¨‹å­¦å’Œå…¬å…±å«ç”Ÿç­‰éè®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œå¯¼è‡´è¯„ä¼°çš„å±€é™æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†MISMATCHEDåŸºå‡†ï¼ŒåŒ…å«æ¥è‡ªä¸‰ä¸ªéè®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„å¥å­å¯¹ï¼Œå¹¶é€šè¿‡å¼•å…¥éšå«NLIå…³ç³»çš„å¥å­å¯¹æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. åœ¨MISMATCHEDåŸºå‡†ä¸Šï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å°å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹å»ºç«‹çš„åŸºçº¿è¡¨ç°å‡ºå®è§‚F1å€¼ä¸º78.17%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ç©ºé—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç§‘å­¦è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ˜¯é¢„æµ‹ç ”ç©¶æ–‡ç« ä¸­å¥å­å¯¹ä¹‹é—´è¯­ä¹‰å…³ç³»çš„ä»»åŠ¡ã€‚ç°æœ‰æ•°æ®é›†ä¸»è¦æ¥è‡ªè®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œå¿½ç•¥äº†éè®¡ç®—æœºç§‘å­¦é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ç§‘å­¦NLIè¯„ä¼°åŸºå‡†â€”â€”MISMATCHEDï¼Œæ¶µç›–å¿ƒç†å­¦ã€å·¥ç¨‹å­¦å’Œå…¬å…±å«ç”Ÿç­‰ä¸‰ä¸ªéè®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œå¹¶åŒ…å«2700å¯¹äººå·¥æ ‡æ³¨çš„å¥å­å¯¹ã€‚æˆ‘ä»¬åœ¨MISMATCHEDä¸Šå»ºç«‹äº†å¼ºåŸºçº¿ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å°å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚æœ€ä½³åŸºçº¿çš„å®è§‚F1å€¼ä¸º78.17%ï¼Œæ˜¾ç¤ºå‡ºæœªæ¥æ”¹è¿›çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜åœ¨æ¨¡å‹è®­ç»ƒä¸­åŠ å…¥å…·æœ‰éšå«ç§‘å­¦NLIå…³ç³»çš„å¥å­å¯¹å¯ä»¥æå‡å…¶æ€§èƒ½ã€‚æˆ‘ä»¬å°†æ•°æ®é›†å’Œä»£ç å…¬å¼€åœ¨GitHubä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç§‘å­¦è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­ç°æœ‰æ•°æ®é›†çš„é¢†åŸŸåå·®é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç¼ºä¹éè®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„å¥å­å¯¹æ•°æ®é›†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥MISMATCHEDåŸºå‡†ï¼Œæ¶µç›–å¿ƒç†å­¦ã€å·¥ç¨‹å­¦å’Œå…¬å…±å«ç”Ÿç­‰é¢†åŸŸçš„å¥å­å¯¹ï¼Œå¹¶åœ¨æ¨¡å‹è®­ç»ƒä¸­åŠ å…¥éšå«NLIå…³ç³»çš„å¥å­å¯¹ï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æ„å»ºé˜¶æ®µæ¶‰åŠäººå·¥æ ‡æ³¨å¥å­å¯¹ï¼Œæ¨¡å‹è®­ç»ƒé˜¶æ®µä½¿ç”¨é¢„è®­ç»ƒçš„å°å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯„ä¼°é˜¶æ®µåˆ™é€šè¿‡å®è§‚F1å€¼æ¥è¡¡é‡æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šMISMATCHEDåŸºå‡†çš„æå‡ºæ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå¡«è¡¥äº†ç§‘å­¦NLIé¢†åŸŸçš„ç©ºç™½ï¼Œå¹¶é€šè¿‡éšå«NLIå…³ç³»çš„å¥å­å¯¹æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¹¶é€šè¿‡è°ƒæ•´è¶…å‚æ•°å’ŒæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œç¡®ä¿åœ¨ä¸åŒé¢†åŸŸçš„å¥å­å¯¹ä¸Šéƒ½èƒ½å–å¾—è‰¯å¥½æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨MISMATCHEDåŸºå‡†ä¸Šï¼Œæœ€ä½³åŸºçº¿æ¨¡å‹çš„å®è§‚F1å€¼è¾¾åˆ°äº†78.17%ï¼Œæ˜¾ç¤ºå‡ºåœ¨éè®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ä»æœ‰æ˜¾è‘—æå‡ç©ºé—´ã€‚è¿™ä¸€ç»“æœä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ˜ç¡®çš„æ”¹è¿›æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§‘å­¦æ–‡çŒ®çš„è‡ªåŠ¨åˆ†æã€ä¿¡æ¯æ£€ç´¢å’ŒçŸ¥è¯†å›¾è°±æ„å»ºç­‰ã€‚é€šè¿‡æå‡ç§‘å­¦NLIçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒç§‘ç ”äººå‘˜åœ¨æ–‡çŒ®ç»¼è¿°ã€æ•°æ®æŒ–æ˜ç­‰æ–¹é¢çš„å·¥ä½œï¼Œæœªæ¥å¯èƒ½å¯¹ç§‘å­¦ç ”ç©¶çš„æ•ˆç‡å’Œå‡†ç¡®æ€§äº§ç”Ÿç§¯æå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Scientific Natural Language Inference (NLI) is the task of predicting the semantic relation between a pair of sentences extracted from research articles. Existing datasets for this task are derived from various computer science (CS) domains, whereas non-CS domains are completely ignored. In this paper, we introduce a novel evaluation benchmark for scientific NLI, called MISMATCHED. The new MISMATCHED benchmark covers three non-CS domains-PSYCHOLOGY, ENGINEERING, and PUBLIC HEALTH, and contains 2,700 human annotated sentence pairs. We establish strong baselines on MISMATCHED using both Pre-trained Small Language Models (SLMs) and Large Language Models (LLMs). Our best performing baseline shows a Macro F1 of only 78.17% illustrating the substantial headroom for future improvements. In addition to introducing the MISMATCHED benchmark, we show that incorporating sentence pairs having an implicit scientific NLI relation between them in model training improves their performance on scientific NLI. We make our dataset and code publicly available on GitHub.

