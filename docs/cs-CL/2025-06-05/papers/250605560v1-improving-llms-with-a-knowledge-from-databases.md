---
layout: default
title: Improving LLMs with a knowledge from databases
---

# Improving LLMs with a knowledge from databases

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05560" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05560v1</a>
  <a href="https://arxiv.org/pdf/2506.05560.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05560v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05560v1', 'Improving LLMs with a knowledge from databases')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Petr MÃ¡Å¡a

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¢å¼ºå…³è”è§„åˆ™çš„LLMçŸ¥è¯†æ”¹è¿›æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¢å¼ºå…³è”è§„åˆ™` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `æ•°æ®é›†` `å¯è§£é‡Šæœºå™¨å­¦ä¹ ` `æ™ºèƒ½é—®ç­”` `å®‰å…¨æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMæ–¹æ³•åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®æ—¶å­˜åœ¨å®‰å…¨æ€§å’Œæ§åˆ¶ä¸è¶³çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¢å¼ºå…³è”è§„åˆ™ç”Ÿæˆè§„åˆ™é›†ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ–‡æœ¬å½¢å¼ä»¥æ”¹è¿›LLMçš„å›ç­”èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›ç­”åŸºäºæ•°æ®é›†çš„é—®é¢˜æ—¶ï¼Œç›¸è¾ƒäºChatGPTæœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨è¿…é€Ÿå–å¾—æ˜¾è‘—è¿›å±•ï¼Œè®¸å¤šå…ˆè¿›æŠ€æœ¯å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå·¥å…·è¢«å¹¿æ³›æ¥å—ã€‚æœ¬æ–‡æ¢è®¨äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ–¹æ³•â€”â€”å¢å¼ºå…³è”è§„åˆ™ï¼ŒåŸºäºæ•°æ®é›†/æ•°æ®åº“æ”¹è¿›LLMçš„å›ç­”ã€‚è¯¥æ–¹æ³•ç”ŸæˆåŸºäºå®šä¹‰çŸ¥è¯†æ¨¡å¼çš„è§„åˆ™é›†ï¼Œå¹¶é€šè¿‡è§„åˆ™åˆ°æ–‡æœ¬è½¬æ¢å™¨å°†è§„åˆ™è½¬æ¢ä¸ºæ–‡æœ¬å½¢å¼ï¼Œä½œä¸ºRAGçš„ä¸€éƒ¨åˆ†çº³å…¥LLMä¸­ã€‚ä¸ChatGPTçš„å¯¹æ¯”å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åŸºäºæ•°æ®é›†å›ç­”é—®é¢˜æ—¶æ˜¾è‘—æå‡äº†æ•ˆæœï¼Œæœªæ¥å¯é€šè¿‡å¼•å…¥å…¶ä»–æ¨¡å¼å’Œè§„åˆ™æŒ–æ˜ç­‰è¿›ä¸€æ­¥æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®æ—¶çš„å›ç­”å‡†ç¡®æ€§å’Œå®‰å…¨æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå‘½ä»¤æ—¶ç¼ºä¹æ§åˆ¶ï¼Œå¯èƒ½å¯¼è‡´å®‰å…¨éšæ‚£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§åŸºäºå¢å¼ºå…³è”è§„åˆ™çš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆè§„åˆ™é›†å¹¶å°†å…¶è½¬åŒ–ä¸ºæ–‡æœ¬å½¢å¼ï¼Œå¢å¼ºLLMçš„å›ç­”èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®æ—¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬å®šä¹‰çŸ¥è¯†æ¨¡å¼ã€ç”Ÿæˆè§„åˆ™é›†ã€ä½¿ç”¨è§„åˆ™åˆ°æ–‡æœ¬è½¬æ¢å™¨å°†è§„åˆ™è½¬åŒ–ä¸ºæ–‡æœ¬ï¼Œæœ€åå°†ç»“æœä½œä¸ºRAGçº³å…¥LLMä¸­ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬çŸ¥è¯†æ¨¡å¼å®šä¹‰ã€è§„åˆ™ç”Ÿæˆå’Œæ–‡æœ¬è½¬æ¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¢å¼ºå…³è”è§„åˆ™ä¸RAGæŠ€æœ¯ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„çŸ¥è¯†å¢å¼ºæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†LLMåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è§„åˆ™ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†ç”Ÿæˆè§„åˆ™çš„æ•°é‡å’Œè´¨é‡å‚æ•°ï¼Œç¡®ä¿ç”Ÿæˆçš„è§„åˆ™å…·æœ‰è¾ƒé«˜çš„ç›¸å…³æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å›ç­”åŸºäºæ•°æ®é›†çš„é—®é¢˜æ—¶ï¼Œç›¸è¾ƒäºChatGPTæœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€æ•°æ®åˆ†æå’Œå†³ç­–æ”¯æŒç­‰ã€‚é€šè¿‡å°†å¢å¼ºå…³è”è§„åˆ™åº”ç”¨äºLLMï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®æ—¶çš„å‡†ç¡®æ€§å’Œå®‰å…¨æ€§ï¼Œæœªæ¥å¯èƒ½åœ¨å¤šä¸ªè¡Œä¸šä¸­äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are achieving significant progress almost every moment now. Many advanced techniques have been introduced and widely accepted, like retrieval-augmentation generation (RAG), agents, and tools. Tools can query the database to answer questions from structured data files or perform groupings or other statistics. This unlocks huge opportunities, such as it can answer any question, but also poses threats, such as safety, because there is no control over the commands that are created. We would like to discuss whether we can create a new method that improves answers based on dataset/database via some interpretable ML methods, namely enhanced association rules. The advantage would be if the method can be also used in some safe technique like RAG. Association rules have a sound history. Since the introduction of CN2 and aproiri, many enhancements have been made. In parallel, enhanced association rules have been introduced and evolved over the last 40 years. The general problem is typically that there are too many rules. There are some techniques for handling it, but when LLM emerged, it turned out to be the best use case for the RAG technique for LLMs. We proposed a method that generates a ruleset based on defined knowledge patterns, then converts rules into text form via a rule-to-text converter, and includes the result as an RAG into LLM. We compared this method with ChatGPT (even with using agents) and we have discovered a significant improvement in answering questions based on the dataset. We have also tried several strategies how much rules to generate. We found this improvement interesting. Moreover, it can also be improved in many ways as future work, like incorporating other patterns, the use of rule mining as an agent, and many others.

