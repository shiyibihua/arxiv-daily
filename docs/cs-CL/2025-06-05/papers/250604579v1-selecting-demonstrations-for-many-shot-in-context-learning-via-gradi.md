---
layout: default
title: Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching
---

# Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04579" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04579v1</a>
  <a href="https://arxiv.org/pdf/2506.04579.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04579v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04579v1', 'Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianfei Zhang, Bei Li, Jun Bai, Rumei Li, Yanmeng Wang, Chenghua Lin, Wenge Rong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: accepted to the ACL2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¢¯åº¦åŒ¹é…æ–¹æ³•ä»¥ä¼˜åŒ–å¤šç¤ºä¾‹ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ¼”ç¤ºé€‰æ‹©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ¢¯åº¦åŒ¹é…` `æ¼”ç¤ºé€‰æ‹©` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šç¤ºä¾‹ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ä¸»è¦ä¾èµ–éšæœºé€‰æ‹©æ¼”ç¤ºï¼Œç¼ºä¹æœ‰æ•ˆçš„é€‰æ‹©æœºåˆ¶ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¢¯åº¦åŒ¹é…çš„æ¼”ç¤ºé€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡å¯¹é½å¾®è°ƒæ¢¯åº¦æ¥ä¼˜åŒ–ç¤ºä¾‹é€‰æ‹©ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºéšæœºé€‰æ‹©ï¼Œå°¤å…¶åœ¨è¾ƒå¤§æ¨¡å‹ä¸Šæå‡æ˜¾è‘—ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿå¿«é€Ÿé€‚åº”ä»»åŠ¡ï¼Œä½†æ¼”ç¤ºé€‰æ‹©çš„ä¾èµ–æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¤šç¤ºä¾‹ICLæ–¹æ³•ä¸»è¦ä¾èµ–éšæœºé€‰æ‹©æ¼”ç¤ºï¼Œç¼ºä¹æœ‰æ•ˆçš„é€‰æ‹©æœºåˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¢¯åº¦åŒ¹é…æ–¹æ³•ï¼Œé€šè¿‡å¯¹é½ç›®æ ‡ä»»åŠ¡çš„æ•´ä¸ªè®­ç»ƒé›†ä¸æ‰€é€‰ç¤ºä¾‹ä¹‹é—´çš„å¾®è°ƒæ¢¯åº¦ï¼Œä»è€Œåœ¨æ‰€é€‰ç¤ºä¾‹ä¸­æ¥è¿‘æ•´ä¸ªè®­ç»ƒé›†çš„å­¦ä¹ æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©ï¼Œå°¤å…¶åœ¨è¾ƒå¤§çš„LLMsä¸Šè¡¨ç°å‡º4%è‡³2%çš„æå‡ï¼Œæ¨åŠ¨äº†å¤šç¤ºä¾‹ICLçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šç¤ºä¾‹ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­æ¼”ç¤ºé€‰æ‹©çš„æœ‰æ•ˆæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–éšæœºé€‰æ‹©ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®çš„æ½œåœ¨ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ¢¯åº¦åŒ¹é…çš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹é½ç›®æ ‡ä»»åŠ¡çš„å¾®è°ƒæ¢¯åº¦ä¸æ‰€é€‰ç¤ºä¾‹çš„æ¢¯åº¦ï¼Œä¼˜åŒ–æ¼”ç¤ºé€‰æ‹©ï¼Œä»è€Œåœ¨å°æ ·æœ¬ä¸Šæ¥è¿‘å…¨æ ·æœ¬çš„å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¢¯åº¦è®¡ç®—ã€æ¼”ç¤ºé€‰æ‹©å’Œæ¨¡å‹è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œå¯¹ç›®æ ‡ä»»åŠ¡çš„è®­ç»ƒé›†è¿›è¡Œæ¢¯åº¦è®¡ç®—ï¼Œç„¶åé€šè¿‡æ¢¯åº¦åŒ¹é…é€‰æ‹©æœ€ä¼˜æ¼”ç¤ºï¼Œæœ€ååœ¨æ¨¡å‹ä¸Šè¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡æ¢¯åº¦åŒ¹é…å®ç°æ¼”ç¤ºé€‰æ‹©çš„ä¼˜åŒ–ï¼Œè¿™ä¸ä¼ ç»Ÿçš„éšæœºé€‰æ‹©æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®­ç»ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è¡¡é‡æ¢¯åº¦å¯¹é½ç¨‹åº¦ï¼Œå¹¶åœ¨å°å‹æ¨¡å‹ï¼ˆå¦‚Qwen2.5-3Bå’ŒLlama3-8Bï¼‰ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œç¡®ä¿æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¢¯åº¦åŒ¹é…æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºéšæœºé€‰æ‹©ï¼Œç‰¹åˆ«æ˜¯åœ¨Qwen2.5-72Bå’ŒLlama3-70Bæ¨¡å‹ä¸Šï¼Œæå‡å¹…åº¦è¾¾åˆ°4%ã€‚æ­¤å¤–ï¼Œåœ¨5ä¸ªé—­æºLLMä¸Šä¹Ÿå®ç°äº†çº¦2%çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ¼”ç¤ºé€‰æ‹©ï¼Œèƒ½å¤Ÿæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›å’Œæ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-Context Learning (ICL) empowers Large Language Models (LLMs) for rapid task adaptation without Fine-Tuning (FT), but its reliance on demonstration selection remains a critical challenge. While many-shot ICL shows promising performance through scaled demonstrations, the selection method for many-shot demonstrations remains limited to random selection in existing work. Since the conventional instance-level retrieval is not suitable for many-shot scenarios, we hypothesize that the data requirements for in-context learning and fine-tuning are analogous. To this end, we introduce a novel gradient matching approach that selects demonstrations by aligning fine-tuning gradients between the entire training set of the target task and the selected examples, so as to approach the learning effect on the entire training set within the selected examples. Through gradient matching on relatively small models, e.g., Qwen2.5-3B or Llama3-8B, our method consistently outperforms random selection on larger LLMs from 4-shot to 128-shot scenarios across 9 diverse datasets. For instance, it surpasses random selection by 4% on Qwen2.5-72B and Llama3-70B, and by around 2% on 5 closed-source LLMs. This work unlocks more reliable and effective many-shot ICL, paving the way for its broader application.

