---
layout: default
title: Static Word Embeddings for Sentence Semantic Representation
---

# Static Word Embeddings for Sentence Semantic Representation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04624" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04624v2</a>
  <a href="https://arxiv.org/pdf/2506.04624.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04624v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04624v2', 'Static Word Embeddings for Sentence Semantic Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Takashi Wada, Yuki Hirakawa, Ryotaro Shimizu, Takahiro Kawashima, Yuki Saito

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-09-30)

**å¤‡æ³¨**: 17 pages; accepted to the Main Conference of EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé™æ€è¯åµŒå…¥ä»¥ä¼˜åŒ–å¥å­è¯­ä¹‰è¡¨ç¤º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é™æ€è¯åµŒå…¥` `å¥å­è¯­ä¹‰è¡¨ç¤º` `ä¸»æˆåˆ†åˆ†æ` `çŸ¥è¯†è’¸é¦` `å¯¹æ¯”å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ–‡æœ¬åµŒå…¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯åµŒå…¥æ–¹æ³•åœ¨å¥å­è¯­ä¹‰è¡¨ç¤ºä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰å¥å­å±‚é¢çš„è¯­ä¹‰ä¿¡æ¯ã€‚
2. æœ¬æ–‡æå‡ºçš„é™æ€è¯åµŒå…¥æ–¹æ³•é€šè¿‡ä¸»æˆåˆ†åˆ†æå’Œå¯¹æ¯”å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œä¼˜åŒ–äº†è¯åµŒå…¥ä»¥æå‡å¥å­è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªå•è¯­å’Œè·¨è¯­ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é™æ€æ¨¡å‹å’ŒåŸºæœ¬çš„å¥å­å˜æ¢å™¨æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é™æ€è¯åµŒå…¥æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å¥å­è¯­ä¹‰è¡¨ç¤ºã€‚é¦–å…ˆï¼Œä»é¢„è®­ç»ƒçš„å¥å­å˜æ¢å™¨ä¸­æå–è¯åµŒå…¥ï¼Œå¹¶é€šè¿‡å¥å­çº§ä¸»æˆåˆ†åˆ†æè¿›è¡Œæ”¹è¿›ï¼Œéšåé‡‡ç”¨çŸ¥è¯†è’¸é¦æˆ–å¯¹æ¯”å­¦ä¹ ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡ç®€å•åœ°å¹³å‡è¯åµŒå…¥æ¥è¡¨ç¤ºå¥å­ï¼Œè®¡ç®—æˆæœ¬ä½ã€‚æˆ‘ä»¬åœ¨å•è¯­å’Œè·¨è¯­ä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨å¥å­è¯­ä¹‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰é™æ€æ¨¡å‹ï¼Œç”šè‡³åœ¨æ–‡æœ¬åµŒå…¥åŸºå‡†ä¸Šè¶…è¶Šäº†åŸºæœ¬çš„å¥å­å˜æ¢å™¨æ¨¡å‹ï¼ˆSimCSEï¼‰ã€‚æœ€åï¼Œé€šè¿‡å¤šç§åˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸå»é™¤äº†ä¸å¥å­è¯­ä¹‰ä¸é«˜åº¦ç›¸å…³çš„è¯åµŒå…¥æˆåˆ†ï¼Œå¹¶æ ¹æ®è¯å¯¹å¥å­è¯­ä¹‰çš„å½±å“è°ƒæ•´äº†å‘é‡èŒƒæ•°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯åµŒå…¥æ–¹æ³•åœ¨å¥å­è¯­ä¹‰è¡¨ç¤ºä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¦‚ä½•æœ‰æ•ˆæå–å’Œä¼˜åŒ–è¯åµŒå…¥ä»¥å¢å¼ºå¥å­å±‚é¢çš„è¯­ä¹‰ç†è§£ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨å¥å­ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´è¯­ä¹‰è¡¨ç¤ºä¸å¤Ÿå‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä»é¢„è®­ç»ƒçš„å¥å­å˜æ¢å™¨ä¸­æå–è¯åµŒå…¥ï¼Œå¹¶ç»“åˆå¥å­çº§ä¸»æˆåˆ†åˆ†æï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›è¯åµŒå…¥ã€‚é€šè¿‡çŸ¥è¯†è’¸é¦æˆ–å¯¹æ¯”å­¦ä¹ ï¼Œå¢å¼ºè¯åµŒå…¥çš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ï¼Œä»è€Œæé«˜å¥å­è¡¨ç¤ºçš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæå–é¢„è®­ç»ƒçš„è¯åµŒå…¥ï¼›å…¶æ¬¡è¿›è¡Œä¸»æˆåˆ†åˆ†æä»¥ä¼˜åŒ–è¯åµŒå…¥ï¼›æœ€åé€šè¿‡çŸ¥è¯†è’¸é¦æˆ–å¯¹æ¯”å­¦ä¹ è¿›ä¸€æ­¥æå‡è¯åµŒå…¥çš„æ•ˆæœã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œé‡‡ç”¨ç®€å•çš„å¹³å‡æ–¹æ³•æ¥è¡¨ç¤ºå¥å­ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡ä¸»æˆåˆ†åˆ†æå»é™¤ä¸å¥å­è¯­ä¹‰ä¸ç›¸å…³çš„è¯åµŒå…¥æˆåˆ†ï¼Œå¹¶æ ¹æ®è¯å¯¹å¥å­è¯­ä¹‰çš„å½±å“è°ƒæ•´å‘é‡èŒƒæ•°ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—æå‡äº†å¥å­è¯­ä¹‰è¡¨ç¤ºçš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œè®ºæ–‡è¯¦ç»†æè¿°äº†ä¸»æˆåˆ†åˆ†æçš„ç»´åº¦é€‰æ‹©ã€æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œä»¥åŠå¯¹æ¯”å­¦ä¹ çš„å…·ä½“å®ç°ç»†èŠ‚ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„é€‚åº”æ€§å’Œè¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ¨¡å‹åœ¨å¥å­è¯­ä¹‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰é™æ€æ¨¡å‹ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æ–‡æœ¬åµŒå…¥åŸºå‡†ä¸Šè¶…è¶Šäº†åŸºæœ¬çš„å¥å­å˜æ¢å™¨æ¨¡å‹ï¼ˆSimCSEï¼‰ï¼Œæå‡å¹…åº¦è¾¾åˆ°æœªçŸ¥ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚é€šè¿‡ä¼˜åŒ–å¥å­è¯­ä¹‰è¡¨ç¤ºï¼Œèƒ½å¤Ÿæå‡è¿™äº›ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯èƒ½ä¸ºå¤šè¯­è¨€å¤„ç†å’Œè·¨æ–‡åŒ–äº¤æµæä¾›æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose new static word embeddings optimised for sentence semantic representation. We first extract word embeddings from a pre-trained Sentence Transformer, and improve them with sentence-level principal component analysis, followed by either knowledge distillation or contrastive learning. During inference, we represent sentences by simply averaging word embeddings, which requires little computational cost. We evaluate models on both monolingual and cross-lingual tasks and show that our model substantially outperforms existing static models on sentence semantic tasks, and even surpasses a basic Sentence Transformer model (SimCSE) on a text embedding benchmark. Lastly, we perform a variety of analyses and show that our method successfully removes word embedding components that are not highly relevant to sentence semantics, and adjusts the vector norms based on the influence of words on sentence semantics.

