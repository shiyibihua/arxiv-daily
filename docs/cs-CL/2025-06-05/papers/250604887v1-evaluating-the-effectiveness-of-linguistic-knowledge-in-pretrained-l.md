---
layout: default
title: Evaluating the Effectiveness of Linguistic Knowledge in Pretrained Language Models: A Case Study of Universal Dependencies
---

# Evaluating the Effectiveness of Linguistic Knowledge in Pretrained Language Models: A Case Study of Universal Dependencies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04887" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04887v1</a>
  <a href="https://arxiv.org/pdf/2506.04887.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04887v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04887v1', 'Evaluating the Effectiveness of Linguistic Knowledge in Pretrained Language Models: A Case Study of Universal Dependencies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenxi Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å°†é€šç”¨ä¾èµ–æ•´åˆè¿›é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥æå‡è·¨è¯­è¨€ä»»åŠ¡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é€šç”¨ä¾èµ–` `é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹` `è·¨è¯­è¨€ä»»åŠ¡` `å¯¹æŠ—æ€§é‡Šä¹‰è¯†åˆ«` `è‡ªç„¶è¯­è¨€å¤„ç†` `è¯­è¨€æ¨¡å‹ä¼˜åŒ–` `å¥æ³•ç»“æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨è·¨è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹æŠ—æ€§é‡Šä¹‰è¯†åˆ«æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºå°†é€šç”¨ä¾èµ–ï¼ˆUDï¼‰æ•´åˆè¿›é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥æœŸæå‡å…¶åœ¨è·¨è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ•´åˆUDåæ¨¡å‹çš„å‡†ç¡®ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«æå‡äº†3.85%å’Œ6.08%ï¼Œåœ¨æŸäº›è¯­è¨€å¯¹ä¸­è¶…è¶Šäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€šç”¨ä¾èµ–ï¼ˆUDï¼‰è¢«å¹¿æ³›è®¤ä¸ºæ˜¯è·¨è¯­è¨€å¥æ³•è¡¨ç¤ºçš„æˆåŠŸè¯­è¨€æ¡†æ¶ï¼Œä½†å…¶æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é€šè¿‡å°†UDæ•´åˆè¿›é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè¯„ä¼°å…¶åœ¨è·¨è¯­è¨€å¯¹æŠ—æ€§é‡Šä¹‰è¯†åˆ«ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ•´åˆUDæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®ç‡å’ŒF1åˆ†æ•°ï¼Œå¹³å‡æå‡åˆ†åˆ«ä¸º3.85%å’Œ6.08%ã€‚è¿™äº›æ”¹è¿›ç¼©å°äº†é¢„è®­ç»ƒæ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŸäº›è¯­è¨€å¯¹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†åè€…ã€‚æ­¤å¤–ï¼Œç»™å®šè¯­è¨€ä¸è‹±è¯­ä¹‹é—´çš„UDç›¸ä¼¼åº¦è¯„åˆ†ä¸è¯¥è¯­è¨€æ¨¡å‹çš„è¡¨ç°å‘ˆæ­£ç›¸å…³ã€‚è¿™äº›å‘ç°çªæ˜¾äº†UDåœ¨åŸŸå¤–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨è·¨è¯­è¨€å¯¹æŠ—æ€§é‡Šä¹‰è¯†åˆ«ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è¯­è¨€é—´çš„å¥æ³•ç»“æ„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†é€šç”¨ä¾èµ–ï¼ˆUDï¼‰ä¿¡æ¯æ•´åˆè¿›é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨UDæä¾›çš„å¥æ³•ç»“æ„æ¥å¢å¼ºæ¨¡å‹çš„ç†è§£èƒ½åŠ›ï¼Œä»è€Œæå‡å…¶åœ¨è·¨è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€UDä¿¡æ¯æå–ã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæå–UDä¿¡æ¯å¹¶ä¸è¾“å…¥æ–‡æœ¬ç»“åˆï¼Œç„¶åè®­ç»ƒæ¨¡å‹å¹¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†UDä½œä¸ºé¢å¤–çš„è¯­è¨€çŸ¥è¯†æ•´åˆè¿›é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨è·¨è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–äºæ–‡æœ¬æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–UDä¿¡æ¯çš„åˆ©ç”¨ï¼ŒåŒæ—¶è°ƒæ•´æ¨¡å‹çš„è¶…å‚æ•°ä»¥é€‚åº”ä¸åŒè¯­è¨€å¯¹çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ•´åˆUDåæ¨¡å‹çš„å‡†ç¡®ç‡æå‡äº†3.85%ï¼ŒF1åˆ†æ•°æå‡äº†6.08%ã€‚è¿™äº›æ”¹è¿›ä¸ä»…ç¼©å°äº†é¢„è®­ç»ƒæ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œåœ¨æŸäº›è¯­è¨€å¯¹ä¸­ç”šè‡³è¶…è¶Šäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†UDåœ¨è·¨è¯­è¨€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è·¨è¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æœåŠ¡äºå¤šè¯­è¨€ç”¨æˆ·ï¼Œå¢å¼ºè·¨æ–‡åŒ–äº¤æµçš„æ•ˆç‡ã€‚æœªæ¥ï¼ŒUDçš„åº”ç”¨å¯èƒ½ä¼šæ‰©å±•åˆ°æ›´å¤šè¯­è¨€å’Œä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨è¯­è¨€æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Universal Dependencies (UD), while widely regarded as the most successful linguistic framework for cross-lingual syntactic representation, remains underexplored in terms of its effectiveness. This paper addresses this gap by integrating UD into pretrained language models and assesses if UD can improve their performance on a cross-lingual adversarial paraphrase identification task. Experimental results show that incorporation of UD yields significant improvements in accuracy and $F_1$ scores, with average gains of 3.85\% and 6.08\% respectively. These enhancements reduce the performance gap between pretrained models and large language models in some language pairs, and even outperform the latter in some others. Furthermore, the UD-based similarity score between a given language and English is positively correlated to the performance of models in that language. Both findings highlight the validity and potential of UD in out-of-domain tasks.

