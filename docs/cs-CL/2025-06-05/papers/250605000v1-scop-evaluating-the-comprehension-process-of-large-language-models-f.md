---
layout: default
title: SCOP: Evaluating the Comprehension Process of Large Language Models from a Cognitive View
---

# SCOP: Evaluating the Comprehension Process of Large Language Models from a Cognitive View

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05000" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05000v1</a>
  <a href="https://arxiv.org/pdf/2506.05000.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05000v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05000v1', 'SCOP: Evaluating the Comprehension Process of Large Language Models from a Cognitive View')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongjie Xiao, Hongru Liang, Peixin Qin, Yao Zhang, Wenqiang Lei

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSCOPä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç†è§£è¿‡ç¨‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨ç†è§£` `è®¤çŸ¥è¯„ä¼°` `SCOPæ¡†æ¶` `æŠ€èƒ½è¯„ä¼°` `æ•°æ®æ„å»º` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£è¿‡ç¨‹ä¸­çš„è¡¨ç°ä¸ä¸“å®¶å­˜åœ¨å·®è·ï¼Œç¼ºä¹åˆç†çš„è¯„ä¼°æ ‡å‡†ã€‚
2. æå‡ºSCOPæ¡†æ¶ï¼Œç³»ç»Ÿå®šä¹‰ç†è§£è¿‡ç¨‹ä¸­çš„äº”é¡¹æŠ€èƒ½ï¼Œå¹¶æ„å»ºç›¸åº”çš„æµ‹è¯•æ•°æ®è¿›è¡Œè¯„ä¼°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨å±€éƒ¨ä¿¡æ¯ç†è§£ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œä½†æ•´ä½“ç†è§£èƒ½åŠ›ä»éœ€æå‡ï¼Œä¸”å­˜åœ¨ä¸å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç†è§£æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»ä»¤äººæ‹…å¿§ï¼Œå› ä¸ºç¼ºä¹åˆç†çš„è§£é‡Šæ¥åˆ¤æ–­å…¶ç†è§£è¿‡ç¨‹æ˜¯å¦ä¸ä¸“å®¶ä¸€è‡´ã€‚æœ¬æ–‡æå‡ºSCOPï¼Œä»è®¤çŸ¥è§†è§’ä»”ç»†æ£€æŸ¥LLMsåœ¨ç†è§£è¿‡ç¨‹ä¸­çš„è¡¨ç°ã€‚å…·ä½“è€Œè¨€ï¼ŒSCOPå®šä¹‰äº†ç†è§£è¿‡ç¨‹ä¸­çš„äº”é¡¹å¿…è¦æŠ€èƒ½ï¼Œæ„å»ºäº†ä¸¥æ ¼çš„æµ‹è¯•æ•°æ®æ¡†æ¶ï¼Œå¹¶å¯¹å…ˆè¿›çš„å¼€æºå’Œé—­æºLLMsè¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsåœ¨ä¸“å®¶çº§ç†è§£è¿‡ç¨‹ä¸­çš„è¡¨ç°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†åœ¨ç†è§£å±€éƒ¨ä¿¡æ¯æ–¹é¢ä¼˜äºå…¨å±€ä¿¡æ¯ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒLLMså¯èƒ½å­˜åœ¨ä¸å¯é æ€§ï¼Œå¯èƒ½é€šè¿‡é”™è¯¯çš„ç†è§£è¿‡ç¨‹å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚åŸºäºSCOPï¼Œå»ºè®®æ”¹è¿›LLMsçš„æ–¹å‘åº”æ›´åŠ å…³æ³¨ç†è§£è¿‡ç¨‹ï¼Œç¡®ä¿åœ¨è®­ç»ƒä¸­å…¨é¢å‘å±•æ‰€æœ‰ç†è§£æŠ€èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£è¿‡ç¨‹ä¸­çš„è¡¨ç°ä¸ä¸“å®¶ä¹‹é—´çš„å·®è·ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹ç³»ç»Ÿçš„è¯„ä¼°æ ‡å‡†å’Œæ¡†æ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æå‡ºSCOPæ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°å®šä¹‰ç†è§£è¿‡ç¨‹ä¸­çš„äº”é¡¹å¿…è¦æŠ€èƒ½ï¼Œå¹¶æ„å»ºæµ‹è¯•æ•°æ®ä»¥è¯„ä¼°è¿™äº›æŠ€èƒ½çš„è¡¨ç°ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†æ›´å¥½åœ°ç†è§£LLMsçš„è®¤çŸ¥è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSCOPæ¡†æ¶åŒ…æ‹¬äº”é¡¹æŠ€èƒ½çš„å®šä¹‰ã€æµ‹è¯•æ•°æ®çš„æ„å»ºå’Œå¯¹LLMsçš„åˆ†æã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æŠ€èƒ½è¯„ä¼°ã€æ•°æ®ç”Ÿæˆå’Œç»“æœåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šSCOPçš„ä¸»è¦åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°å®šä¹‰ç†è§£æŠ€èƒ½å¹¶æä¾›ç›¸åº”çš„è¯„ä¼°æ¡†æ¶ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„è¯„ä¼°æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æµ‹è¯•æ•°æ®æ„å»ºä¸­ï¼Œè®¾è®¡äº†ä¸¥æ ¼çš„æ ‡å‡†ï¼Œç¡®ä¿æ¯é¡¹æŠ€èƒ½éƒ½èƒ½å¾—åˆ°æœ‰æ•ˆè¯„ä¼°ï¼Œæ­¤å¤–ï¼Œåˆ†æè¿‡ç¨‹ä¸­é‡‡ç”¨äº†å¤šç§å…ˆè¿›çš„LLMsè¿›è¡Œå¯¹æ¯”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨ç†è§£å±€éƒ¨ä¿¡æ¯æ—¶è¡¨ç°ä¼˜äºå…¨å±€ä¿¡æ¯ï¼Œå°½ç®¡æ•´ä½“ç†è§£èƒ½åŠ›ä»å­˜åœ¨æŒ‘æˆ˜ã€‚å…·ä½“è€Œè¨€ï¼ŒLLMsåœ¨æŸäº›æµ‹è¯•ä¸­è¾¾åˆ°äº†85%çš„å‡†ç¡®ç‡ï¼Œä½†åœ¨å…¨å±€ç†è§£ä»»åŠ¡ä¸­ä»…ä¸º65%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†LLMsåœ¨ç†è§£è¿‡ç¨‹ä¸­çš„ä¸å¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œå¯ä»¥ä¸ºæ¨¡å‹çš„æ”¹è¿›æä¾›æŒ‡å¯¼ï¼Œä»è€Œæå‡å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„è¡¨ç°å’Œå¯é æ€§ã€‚æœªæ¥ï¼ŒSCOPæ¡†æ¶å¯èƒ½æˆä¸ºè¯„ä¼°å’Œä¼˜åŒ–LLMsçš„é‡è¦å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the great potential of large language models(LLMs) in machine comprehension, it is still disturbing to fully count on them in real-world scenarios. This is probably because there is no rational explanation for whether the comprehension process of LLMs is aligned with that of experts. In this paper, we propose SCOP to carefully examine how LLMs perform during the comprehension process from a cognitive view. Specifically, it is equipped with a systematical definition of five requisite skills during the comprehension process, a strict framework to construct testing data for these skills, and a detailed analysis of advanced open-sourced and closed-sourced LLMs using the testing data. With SCOP, we find that it is still challenging for LLMs to perform an expert-level comprehension process. Even so, we notice that LLMs share some similarities with experts, e.g., performing better at comprehending local information than global information. Further analysis reveals that LLMs can be somewhat unreliable -- they might reach correct answers through flawed comprehension processes. Based on SCOP, we suggest that one direction for improving LLMs is to focus more on the comprehension process, ensuring all comprehension skills are thoroughly developed during training.

