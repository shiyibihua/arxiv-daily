---
layout: default
title: Customizing Speech Recognition Model with Large Language Model Feedback
---

# Customizing Speech Recognition Model with Large Language Model Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11091" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11091v2</a>
  <a href="https://arxiv.org/pdf/2506.11091.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11091v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11091v2', 'Customizing Speech Recognition Model with Large Language Model Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaoshi Ling, Guoli Ye

**åˆ†ç±»**: cs.CL, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-08-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹åé¦ˆçš„ASRæ¨¡å‹å®šåˆ¶æ–¹æ³•ä»¥è§£å†³é¢†åŸŸé€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨è¯­éŸ³è¯†åˆ«` `é¢†åŸŸé€‚åº”` `å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å‘½åå®ä½“è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ASRç³»ç»Ÿåœ¨è¯†åˆ«ç¨€æœ‰å‘½åå®ä½“å’Œé€‚åº”ç‰¹å®šé¢†åŸŸæ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´è½¬å½•è´¨é‡ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ— ç›‘ç£å­¦ä¹ æå‡ASRæ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®ä½“è¯é”™è¯¯ç‡ä¸Šè¾ƒä¼ ç»Ÿè‡ªè®­ç»ƒæ–¹æ³•æå‡äº†21%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆæœæ”¹å–„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨é€šç”¨è½¬å½•ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¯†åˆ«ç¨€æœ‰å‘½åå®ä½“å’Œé€‚åº”é¢†åŸŸä¸åŒ¹é…æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç»è¿‡å¤§è§„æ¨¡äº’è”ç½‘æ•°æ®é›†è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸçš„æ•ˆæœæ›´ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ— ç›‘ç£é¢†åŸŸé€‚åº”æ–¹æ³•ï¼Œåˆ©ç”¨æœªæ ‡è®°æ•°æ®é€šè¿‡LLMåé¦ˆæå‡è½¬å½•è´¨é‡ï¼Œç‰¹åˆ«æ˜¯å—é¢†åŸŸä¸åŒ¹é…å½±å“çš„å‘½åå®ä½“ã€‚è¯¥æ¡†æ¶ä½¿ç”¨LLMä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œå¯¹ASRæ¨¡å‹çš„å‡è®¾è¿›è¡Œè¯„åˆ†ï¼Œè¿™äº›è¯„åˆ†ä½œä¸ºå¥–åŠ±ä¿¡å·ç”¨äºé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒASRæ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨å®ä½“è¯é”™è¯¯ç‡ä¸Šè¾ƒä¼ ç»Ÿè‡ªè®­ç»ƒæ–¹æ³•æé«˜äº†21%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨é¢†åŸŸä¸åŒ¹é…æƒ…å†µä¸‹å¯¹ç¨€æœ‰å‘½åå®ä½“çš„è¯†åˆ«å›°éš¾ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æœªæ ‡è®°æ•°æ®æ—¶æ•ˆæœæœ‰é™ï¼Œæ— æ³•æœ‰æ•ˆæå‡è½¬å½•è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ— ç›‘ç£é¢†åŸŸé€‚åº”æ–¹æ³•ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡å¯¹ASRæ¨¡å‹è¾“å‡ºçš„å‡è®¾è¿›è¡Œè¯„åˆ†ï¼Œè¿›è€Œä¼˜åŒ–ASRæ¨¡å‹çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€ASRæ¨¡å‹ç”Ÿæˆå‡è®¾ã€LLMåé¦ˆè¯„åˆ†å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼ŒASRæ¨¡å‹ç”Ÿæˆåˆæ­¥è½¬å½•ç»“æœï¼Œç„¶åLLMå¯¹è¿™äº›ç»“æœè¿›è¡Œè¯„åˆ†ï¼Œæœ€åæ ¹æ®è¯„åˆ†ç»“æœè°ƒæ•´ASRæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†LLMä½œä¸ºå¥–åŠ±æ¨¡å‹å¼•å…¥ASRç³»ç»Ÿï¼Œé€šè¿‡æ— ç›‘ç£å­¦ä¹ æ–¹å¼æ˜¾è‘—æå‡äº†å¯¹å‘½åå®ä½“çš„è¯†åˆ«èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿè‡ªè®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºæœ‰æ•ˆçš„åé¦ˆæœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™ç»“åˆäº†LLMçš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œç¡®ä¿äº†å¯¹é¢†åŸŸç‰¹å®šæ•°æ®çš„æœ‰æ•ˆé€‚åº”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å®ä½“è¯é”™è¯¯ç‡ä¸Šè¾ƒä¼ ç»Ÿè‡ªè®­ç»ƒæ–¹æ³•æå‡äº†21%ï¼Œæ˜¾è‘—æé«˜äº†ASRç³»ç»Ÿåœ¨é¢†åŸŸé€‚åº”ä¸­çš„è¡¨ç°ï¼ŒéªŒè¯äº†å¤§è¯­è¨€æ¨¡å‹åé¦ˆçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€æ³•å¾‹ã€é‡‘èç­‰ä¸“ä¸šé¢†åŸŸçš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡è¿™äº›é¢†åŸŸä¸­å¯¹ç‰¹å®šæœ¯è¯­å’Œå‘½åå®ä½“çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯æ‰©å±•è‡³æ›´å¤šé¢†åŸŸï¼Œæ¨åŠ¨ASRæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automatic speech recognition (ASR) systems have achieved strong performance on general transcription tasks. However, they continue to struggle with recognizing rare named entities and adapting to domain mismatches. In contrast, large language models (LLMs), trained on massive internet-scale datasets, are often more effective across a wide range of domains. In this work, we propose a reinforcement learning based approach for unsupervised domain adaptation, leveraging unlabeled data to enhance transcription quality, particularly the named entities affected by domain mismatch, through feedback from a LLM. Given contextual information, our framework employs a LLM as the reward model to score the hypotheses from the ASR model. These scores serve as reward signals to fine-tune the ASR model via reinforcement learning. Our method achieves a 21\% improvement on entity word error rate over conventional self-training methods.

