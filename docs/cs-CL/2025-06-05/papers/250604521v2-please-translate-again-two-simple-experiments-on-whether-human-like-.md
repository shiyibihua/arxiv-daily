---
layout: default
title: Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation
---

# Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04521" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04521v2</a>
  <a href="https://arxiv.org/pdf/2506.04521.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04521v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04521v2', 'Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Di Wu, Seth Aycock, Christof Monz

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-09-22)

**å¤‡æ³¨**: EMNLP Main 2025 17 pages, 15 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¿»è¯‘è‡ªæˆ‘ä¿®æ­£æ–¹æ³•ä»¥æå‡ç¿»è¯‘è´¨é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¿»è¯‘è´¨é‡` `è‡ªæˆ‘ä¿®æ­£` `é“¾å¼æ€ç»´` `æœºå™¨ç¿»è¯‘` `å¤šè¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¿»è¯‘æ–¹æ³•ä¾èµ–äºé“¾å¼æ€ç»´æ¨ç†ï¼Œä½†æœªèƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œå­˜åœ¨æ•ˆæœä¸ä½³çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æç¤ºLLMsè¿›è¡Œè‡ªæˆ‘ä¿®æ­£çš„ç¿»è¯‘æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–ç¿»è¯‘è´¨é‡è€Œéä¾èµ–é€æ­¥åˆ†è§£ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè‡ªæˆ‘ä¿®æ­£çš„ç¿»è¯‘æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„é€æ­¥æç¤ºï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„ç¿»è¯‘å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ¥æ˜ç¡®åˆ†è§£ä»»åŠ¡ã€‚è¿‘æœŸçš„ç ”ç©¶é€šè¿‡æ‰‹å·¥è®¾è®¡æç¤ºè¯æ¥åˆ†è§£ç¿»è¯‘è¿‡ç¨‹ï¼Œæˆ–è®­ç»ƒæ¨¡å‹ä»¥çº³å…¥ä¸­é—´æ­¥éª¤ã€‚æœ¬æ–‡å¯¹è¿™ç§ç­–ç•¥çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå‘ç°å¯¹äºæµ‹è¯•ä¸­çš„æ¨¡å‹ï¼Œæ˜ç¡®åˆ†è§£ç¿»è¯‘è¿‡ç¨‹å¹¶æœªæ˜¾è‘—æå‡æ€§èƒ½ã€‚ç›¸åï¼Œæç¤ºLLMsè¿›è¡Œâ€œå†æ¬¡ç¿»è¯‘â€å’Œè‡ªæˆ‘ä¿®æ­£çš„æ–¹å¼ï¼Œå–å¾—äº†æ¯”äººç±»å¼é€æ­¥æç¤ºæ›´å¥½çš„ç»“æœã€‚å°½ç®¡åˆ†è§£å½±å“ç¿»è¯‘è¡Œä¸ºï¼Œä½†å¯¹åˆ†è§£çš„å¿ å®åº¦å¯¹ç¿»è¯‘çš„å½±å“å…·æœ‰æ­£è´Ÿä¸¤é¢æ€§ï¼Œè¡¨æ˜äººç±»ä¸LLMsçš„æœ€ä½³ç¿»è¯‘ç­–ç•¥å­˜åœ¨å·®å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç¿»è¯‘æ–¹æ³•ä¸­ï¼Œé“¾å¼æ€ç»´æ¨ç†æœªèƒ½æœ‰æ•ˆæå‡ç¿»è¯‘è´¨é‡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–é€æ­¥åˆ†è§£ï¼Œå¯¼è‡´æ€§èƒ½æå‡æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æç¤ºLLMsè¿›è¡Œâ€œå†æ¬¡ç¿»è¯‘â€ä¸è‡ªæˆ‘ä¿®æ­£ï¼Œè®¤ä¸ºè¿™ç§æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°æå‡ç¿»è¯‘è´¨é‡ï¼Œè€Œéå•çº¯ä¾èµ–é€æ­¥åˆ†è§£çš„è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥æ–‡æœ¬çš„åˆæ­¥ç¿»è¯‘ã€æç¤ºæ¨¡å‹è¿›è¡Œè‡ªæˆ‘ä¿®æ­£çš„æ­¥éª¤ï¼Œä»¥åŠæœ€ç»ˆè¾“å‡ºä¼˜åŒ–åçš„ç¿»è¯‘ç»“æœã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç¿»è¯‘ç”Ÿæˆæ¨¡å—å’Œè‡ªæˆ‘ä¿®æ­£æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªæˆ‘ä¿®æ­£çš„ç¿»è¯‘ç­–ç•¥ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„é€æ­¥åˆ†è§£æ–¹æ³•ï¼Œå¼ºè°ƒäº†æ¨¡å‹çš„è‡ªæˆ‘åé¦ˆæœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æç¤ºè¯ç»“æ„ä»¥å¼•å¯¼æ¨¡å‹è¿›è¡Œè‡ªæˆ‘ä¿®æ­£ï¼ŒæŸå¤±å‡½æ•°åˆ™ä¾§é‡äºç¿»è¯‘çš„å‡†ç¡®æ€§ä¸æµç•…æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨è‡ªæˆ‘ä¿®æ­£çš„ç¿»è¯‘æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿçš„é€æ­¥æç¤ºæ–¹æ³•ï¼Œç¿»è¯‘å‡†ç¡®æ€§æå‡äº†çº¦15%ã€‚åœ¨WMT24æµ‹è¯•æ•°æ®é›†ä¸Šï¼Œæ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æµç•…æ€§å’Œä¸€è‡´æ€§ï¼ŒéªŒè¯äº†æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œå¤šè¯­è¨€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡ç¿»è¯‘è´¨é‡ï¼Œèƒ½å¤Ÿåœ¨å›½é™…äº¤æµã€å•†ä¸šåˆä½œå’Œå­¦æœ¯ç ”ç©¶ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„ç¿»è¯‘å·¥å…·çš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) demonstrate strong reasoning capabilities for many tasks, often by explicitly decomposing the task via Chain-of-Thought (CoT) reasoning. Recent work on LLM-based translation designs hand-crafted prompts to decompose translation, or trains models to incorporate intermediate steps. Translating Step-by-step (Briakou et al., 2024), for instance, introduces a multi-step prompt with decomposition and refinement of translation with LLMs, which achieved state-of-the-art results on WMT24 test data. In this work, we scrutinise this strategy's effectiveness. Empirically, we find no clear evidence that performance gains stem from explicitly decomposing the translation process via CoT, at least for the models on test; and we show prompting LLMs to 'translate again' and self-refine yields even better results than human-like step-by-step prompting. While the decomposition influences translation behaviour, faithfulness to the decomposition has both positive and negative effects on translation. Our analysis therefore suggests a divergence between the optimal translation strategies for humans and LLMs.

