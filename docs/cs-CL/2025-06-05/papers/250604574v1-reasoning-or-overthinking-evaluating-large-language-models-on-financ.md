---
layout: default
title: Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis
---

# Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04574" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04574v1</a>
  <a href="https://arxiv.org/pdf/2506.04574.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04574v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04574v1', 'Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dimitris Vamvourellis, Dhagash Mehta

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èæƒ…æ„Ÿåˆ†æä¸­çš„æœ‰æ•ˆæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡‘èæƒ…æ„Ÿåˆ†æ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `ç³»ç»Ÿ1æ€ç»´` `ç³»ç»Ÿ2æ€ç»´` `æ¨¡å‹è¯„ä¼°` `æ•°æ®é›†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡‘èæƒ…æ„Ÿåˆ†ææ–¹æ³•åœ¨å¤„ç†å¤æ‚è¯­è¨€å’Œäººç±»æƒ…æ„Ÿä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é£é™©å†³ç­–ä¸­ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ¯”è¾ƒä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹å’Œæç¤ºç­–ç•¥ï¼Œæ¢ç´¢æ¨ç†å¯¹é‡‘èæƒ…æ„Ÿåˆ†æçš„å½±å“ï¼Œé‡ç‚¹åœ¨äºå¿«é€Ÿç›´è§‚çš„æ€ç»´æ–¹å¼ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4oåœ¨æ²¡æœ‰æ¨ç†æç¤ºçš„æƒ…å†µä¸‹è¡¨ç°æœ€ä½³ï¼Œè¡¨æ˜æ¨ç†å¯èƒ½å¯¼è‡´è¿‡åº¦æ€è€ƒï¼Œä»è€Œå½±å“é¢„æµ‹å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶-shoté‡‘èæƒ…æ„Ÿåˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬åŸºäºæ¨ç†å’Œéæ¨ç†æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨ç”±é¢†åŸŸä¸“å®¶æ ‡æ³¨çš„Financial PhraseBankæ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸åŒLLMså’Œæç¤ºç­–ç•¥ä¸äººç±»æ ‡æ³¨æƒ…æ„Ÿçš„ä¸€è‡´æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨ç†å¹¶æœªæå‡æ¨¡å‹æ€§èƒ½ï¼Œæœ€å‡†ç¡®çš„ç»„åˆæ˜¯GPT-4oï¼Œæ— éœ€Chain-of-Thoughtæç¤ºã€‚ç»“æœè¡¨æ˜ï¼Œå¿«é€Ÿç›´è§‚çš„â€œç³»ç»Ÿ1â€æ€ç»´æ›´ç¬¦åˆäººç±»åˆ¤æ–­ï¼ŒæŒ‘æˆ˜äº†æ¨ç†æ€»èƒ½æé«˜å†³ç­–è´¨é‡çš„å‡è®¾ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é£é™©é‡‘èåº”ç”¨ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èæƒ…æ„Ÿåˆ†æä¸­çš„æœ‰æ•ˆæ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯æ¨ç†èƒ½åŠ›å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚è¯­è¨€å¤„ç†å’Œäººç±»æƒ…æ„Ÿä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¯”è¾ƒä¸åŒçš„LLMså’Œæç¤ºç­–ç•¥ï¼Œè¯„ä¼°æ¨ç†åœ¨é‡‘èæƒ…æ„Ÿåˆ†æä¸­çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯å¿«é€Ÿç›´è§‚çš„â€œç³»ç»Ÿ1â€æ€ç»´æ˜¯å¦ä¼˜äºæ…¢é€Ÿçš„â€œç³»ç»Ÿ2â€æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨Financial PhraseBankæ•°æ®é›†ï¼Œæ¯”è¾ƒä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT-4oã€GPT-4.1ã€o3-miniï¼‰ä¸ä¸¤ç§å°å‹æ¨¡å‹ï¼ˆFinBERT-Prosusã€FinBERT-Toneï¼‰ï¼Œåœ¨ä¸åŒæç¤ºç­–ç•¥ä¸‹è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå‘ç°æ¨ç†å¹¶æœªæå‡æ¨¡å‹æ€§èƒ½ï¼Œåè€Œåœ¨æŸäº›æƒ…å†µä¸‹å¯¼è‡´è¿‡åº¦æ€è€ƒï¼Œå½±å“é¢„æµ‹å‡†ç¡®æ€§ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†æ¨ç†æ€»èƒ½æé«˜å†³ç­–è´¨é‡çš„ä¼ ç»Ÿè§‚å¿µã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥æ¥æ¨¡æ‹Ÿâ€œç³»ç»Ÿ1â€å’Œâ€œç³»ç»Ÿ2â€æ€ç»´ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å¤æ‚è¯­è¨€å’Œæ ‡æ³¨ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4oåœ¨æ²¡æœ‰Chain-of-Thoughtæç¤ºçš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºæœ€ä¼˜çš„å‡†ç¡®æ€§å’Œä¸äººç±»æ ‡æ³¨çš„ä¸€è‡´æ€§ï¼Œè¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚æ­¤ç»“æœè¡¨æ˜ï¼Œæ¨ç†å¹¶ä¸æ€»æ˜¯æå‡æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é‡‘èæƒ…æ„Ÿåˆ†æä¸­ï¼Œå¿«é€Ÿç›´è§‚çš„æ€ç»´æ–¹å¼æ›´ä¸ºæœ‰æ•ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºåˆ†æã€æŠ•èµ„å†³ç­–æ”¯æŒå’Œé£é™©è¯„ä¼°ç­‰ã€‚é€šè¿‡ä¼˜åŒ–æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œèƒ½å¤Ÿæé«˜é‡‘èå†³ç­–çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå°¤å…¶åœ¨é«˜é£é™©ç¯å¢ƒä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨é‡‘èç§‘æŠ€é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ï¼Œä¿ƒè¿›æ™ºèƒ½å†³ç­–ç³»ç»Ÿçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate the effectiveness of large language models (LLMs), including reasoning-based and non-reasoning models, in performing zero-shot financial sentiment analysis. Using the Financial PhraseBank dataset annotated by domain experts, we evaluate how various LLMs and prompting strategies align with human-labeled sentiment in a financial context. We compare three proprietary LLMs (GPT-4o, GPT-4.1, o3-mini) under different prompting paradigms that simulate System 1 (fast and intuitive) or System 2 (slow and deliberate) thinking and benchmark them against two smaller models (FinBERT-Prosus, FinBERT-Tone) fine-tuned on financial sentiment analysis. Our findings suggest that reasoning, either through prompting or inherent model design, does not improve performance on this task. Surprisingly, the most accurate and human-aligned combination of model and method was GPT-4o without any Chain-of-Thought (CoT) prompting. We further explore how performance is impacted by linguistic complexity and annotation agreement levels, uncovering that reasoning may introduce overthinking, leading to suboptimal predictions. This suggests that for financial sentiment classification, fast, intuitive "System 1"-like thinking aligns more closely with human judgment compared to "System 2"-style slower, deliberative reasoning simulated by reasoning models or CoT prompting. Our results challenge the default assumption that more reasoning always leads to better LLM decisions, particularly in high-stakes financial applications.

