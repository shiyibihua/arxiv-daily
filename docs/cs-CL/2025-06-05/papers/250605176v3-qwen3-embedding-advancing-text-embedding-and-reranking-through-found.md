---
layout: default
title: Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models
---

# Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05176" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05176v3</a>
  <a href="https://arxiv.org/pdf/2506.05176.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05176v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05176v3', 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, Jingren Zhou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQwen3åµŒå…¥ä»¥æå‡æ–‡æœ¬åµŒå…¥å’Œé‡æ’åºèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åµŒå…¥` `é‡æ’åº` `å¤šè¯­è¨€å¤„ç†` `æ— ç›‘ç£å­¦ä¹ ` `ç›‘ç£å¾®è°ƒ` `æ¨¡å‹åˆå¹¶` `åŸºç¡€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åµŒå…¥å’Œé‡æ’åºæ–¹æ³•åœ¨å¤šè¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å¤šæ ·åŒ–çš„åº”ç”¨éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºçš„Qwen3åµŒå…¥ç³»åˆ—é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆæ— ç›‘ç£é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼Œæå‡äº†æ¨¡å‹çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQwen3åµŒå…¥ç³»åˆ—åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯­è¨€è¯„ä¼°å’Œæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†Qwen3åµŒå…¥ç³»åˆ—ï¼Œç›¸è¾ƒäºå…¶å‰èº«GTE-Qwenç³»åˆ—åœ¨æ–‡æœ¬åµŒå…¥å’Œé‡æ’åºèƒ½åŠ›ä¸Šæœ‰æ˜¾è‘—è¿›å±•ã€‚è¯¥ç³»åˆ—åŸºäºQwen3åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨å…¶åœ¨å¤šè¯­è¨€æ–‡æœ¬ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œé‡‡ç”¨åˆ›æ–°çš„å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆå¤§è§„æ¨¡æ— ç›‘ç£é¢„è®­ç»ƒä¸é«˜è´¨é‡æ•°æ®é›†çš„ç›‘ç£å¾®è°ƒã€‚æœ‰æ•ˆçš„æ¨¡å‹åˆå¹¶ç­–ç•¥è¿›ä¸€æ­¥ç¡®ä¿äº†Qwen3åµŒå…¥ç³»åˆ—çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚Qwen3åµŒå…¥ç³»åˆ—æä¾›å¤šç§æ¨¡å‹è§„æ¨¡ï¼ˆ0.6Bã€4Bã€8Bï¼‰ï¼Œé€‚ç”¨äºä¸åŒçš„éƒ¨ç½²åœºæ™¯ï¼Œç”¨æˆ·å¯æ ¹æ®æ•ˆç‡æˆ–æ•ˆæœè¿›è¡Œä¼˜åŒ–ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»åˆ—åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„ç»“æœï¼Œå°¤å…¶åœ¨å¤šè¯­è¨€è¯„ä¼°åŸºå‡†MTEBå’Œå„ç§æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–‡æœ¬åµŒå…¥å’Œé‡æ’åºæ–¹æ³•åœ¨å¤šè¯­è¨€å¤„ç†ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹å¤šæ ·åŒ–æ•°æ®å’Œä»»åŠ¡çš„é€‚åº”æ€§å·®çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Qwen3åµŒå…¥ç³»åˆ—é€šè¿‡åˆ›æ–°çš„å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆå¤§è§„æ¨¡æ— ç›‘ç£é¢„è®­ç»ƒä¸é«˜è´¨é‡æ•°æ®é›†çš„ç›‘ç£å¾®è°ƒï¼Œæ—¨åœ¨æå‡æ¨¡å‹çš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªé˜¶æ®µï¼šé¦–å…ˆè¿›è¡Œå¤§è§„æ¨¡æ— ç›‘ç£é¢„è®­ç»ƒï¼Œç„¶ååœ¨é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œæœ€åé€šè¿‡æ¨¡å‹åˆå¹¶ç­–ç•¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºåˆ©ç”¨Qwen3 LLMsä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œä¸ä»…æä¾›å¼ºå¤§çš„æ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œè¿˜èƒ½åˆæˆé«˜è´¨é‡çš„å¤šæ ·åŒ–è®­ç»ƒæ•°æ®ï¼Œæ˜¾è‘—æå‡è®­ç»ƒæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆ0.6Bã€4Bã€8Bï¼‰ï¼Œå¹¶é€šè¿‡æœ‰æ•ˆçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒQwen3åµŒå…¥ç³»åˆ—åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯­è¨€è¯„ä¼°åŸºå‡†MTEBä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Qwen3åµŒå…¥ç³»åˆ—å¯å¹¿æ³›åº”ç”¨äºå¤šè¯­è¨€æ–‡æœ¬å¤„ç†ã€ä¿¡æ¯æ£€ç´¢ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚å…¶çµæ´»çš„æ¨¡å‹è§„æ¨¡å’Œé«˜æ•ˆçš„æ€§èƒ½ä½¿å…¶é€‚åˆäºä¸åŒçš„åº”ç”¨åœºæ™¯ï¼Œæ¨åŠ¨äº†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.

