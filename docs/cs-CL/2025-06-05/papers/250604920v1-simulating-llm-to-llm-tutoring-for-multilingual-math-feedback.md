---
layout: default
title: Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback
---

# Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04920" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04920v1</a>
  <a href="https://arxiv.org/pdf/2506.04920.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04920v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04920v1', 'Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junior Cedric Tonga, KV Aditya Srivatsa, Kaushal Kumar Maurya, Fajri Koto, Ekaterina Kochmar

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: Preprint, in submission

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¯­è¨€LLMè¾…å¯¼æ¨¡æ‹Ÿä»¥æå‡æ•°å­¦åé¦ˆæ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šè¯­è¨€æ•™è‚²` `æ•°å­¦æ¨ç†` `åé¦ˆæœºåˆ¶` `ä½èµ„æºè¯­è¨€` `æ•™è‚²å·¥å…·` `è·¨è¯­è¨€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMåœ¨å¤šè¯­è¨€æ•™è‚²ä¸­çš„åº”ç”¨å°šæœªå……åˆ†æ¢è®¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œç¼ºä¹æœ‰æ•ˆçš„è·¨è¯­è¨€åé¦ˆæœºåˆ¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡æ‹Ÿå¤šè¯­è¨€è¾…å¯¼çš„æ¡†æ¶ï¼Œé€šè¿‡å¼ºæ¨¡å‹ç”Ÿæˆåé¦ˆï¼Œå¼±æ¨¡å‹æ¨¡æ‹Ÿå­¦ç”Ÿï¼Œæ¢ç´¢ä¸åŒè¯­è¨€é—´çš„äº’åŠ¨æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šè¯­è¨€æç¤ºåœ¨ä½èµ„æºè¯­è¨€ä¸­æ˜¾è‘—æé«˜å­¦ä¹ æ•ˆæœï¼Œå°¤å…¶æ˜¯å½“åé¦ˆä¸å­¦ç”Ÿæ¯è¯­ä¸€è‡´æ—¶ï¼Œå­¦ä¹ æ”¶ç›Šæ›´ä¸ºæ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆè‹±è¯­çš„å½¢æˆæ€§åé¦ˆå’Œæ•™å­¦æç¤ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨ä¸åŒè¯­è¨€ä¸­æä¾›æœ‰æ•ˆæ•™å­¦æ”¯æŒçš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œå°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡é¦–æ¬¡å¤§è§„æ¨¡æ¨¡æ‹Ÿäº†å¤šè¯­è¨€çš„è¾…å¯¼-å­¦ç”Ÿäº’åŠ¨ï¼Œå¼ºæ¨¡å‹ä½œä¸ºè¾…å¯¼è€…ç”Ÿæˆæç¤ºï¼Œè€Œå¼±æ¨¡å‹æ¨¡æ‹Ÿå­¦ç”Ÿã€‚æˆ‘ä»¬æ¢ç´¢äº†352ç§å®éªŒè®¾ç½®ï¼Œæ¶µç›–11ç§è¯­è¨€ã€å››ç§å…ˆè¿›çš„LLMå’Œå¤šç§æç¤ºç­–ç•¥ï¼Œä»¥è¯„ä¼°è¯­è¨€ç‰¹å®šåé¦ˆæ˜¯å¦èƒ½å¸¦æ¥å¯æµ‹é‡çš„å­¦ä¹ æ”¶ç›Šã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“åé¦ˆä¸å­¦ç”Ÿçš„æ¯è¯­ä¸€è‡´æ—¶ï¼Œå¤šè¯­è¨€æç¤ºèƒ½æ˜¾è‘—æ”¹å–„å­¦ä¹ æ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æœ‰æ•ˆä¸”åŒ…å®¹çš„å¤šè¯­è¨€LLMæ•™è‚²å·¥å…·æä¾›äº†å®ç”¨è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€æ•™è‚²ä¸­ï¼Œå°¤å…¶æ˜¯æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„åé¦ˆæœ‰æ•ˆæ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ä¸åŒè¯­è¨€çš„æœ‰æ•ˆæ”¯æŒï¼Œå¯¼è‡´å­¦ä¹ æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ¨¡æ‹Ÿå¤šè¯­è¨€çš„è¾…å¯¼-å­¦ç”Ÿäº’åŠ¨ï¼Œåˆ©ç”¨å¼ºæ¨¡å‹ç”Ÿæˆé’ˆå¯¹æ€§çš„åé¦ˆï¼Œå¼±æ¨¡å‹æ¨¡æ‹Ÿå­¦ç”Ÿçš„å­¦ä¹ è¿‡ç¨‹ï¼Œä»¥è¯„ä¼°è¯­è¨€ç‰¹å®šåé¦ˆçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¼ºæ¨¡å‹å’Œå¼±æ¨¡å‹çš„åä½œï¼Œå¼ºæ¨¡å‹è´Ÿè´£ç”Ÿæˆå¤šè¯­è¨€æç¤ºï¼Œå¼±æ¨¡å‹åˆ™æ ¹æ®æç¤ºè¿›è¡Œå­¦ä¹ åé¦ˆã€‚å®éªŒè®¾è®¡æ¶µç›–352ç§è®¾ç½®ï¼Œæ¶‰åŠ11ç§è¯­è¨€å’Œå¤šç§æç¤ºç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡å¤§è§„æ¨¡æ¨¡æ‹Ÿå¤šè¯­è¨€LLMè¾…å¯¼ï¼Œæ¢ç´¢äº†ä¸åŒè¯­è¨€é—´çš„åé¦ˆæ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­è®¾ç½®äº†å¤šç§å‚æ•°ï¼ŒåŒ…æ‹¬æ¨¡å‹é€‰æ‹©ã€æç¤ºç­–ç•¥å’Œè¯­è¨€èµ„æºæ°´å¹³ï¼Œé‡‡ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åé¦ˆç”Ÿæˆçš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šè¯­è¨€æç¤ºåœ¨ä½èµ„æºè¯­è¨€ä¸­æ˜¾è‘—æé«˜å­¦ä¹ æ•ˆæœï¼Œå°¤å…¶æ˜¯å½“åé¦ˆä¸å­¦ç”Ÿçš„æ¯è¯­ä¸€è‡´æ—¶ï¼Œå­¦ä¹ æ”¶ç›Šæå‡å¹…åº¦å¯è¾¾æ˜¾è‘—æ°´å¹³ã€‚è¿™ä¸€å‘ç°ä¸ºå¤šè¯­è¨€æ•™è‚²å·¥å…·çš„è®¾è®¡æä¾›äº†é‡è¦ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€æ•™è‚²å·¥å…·çš„å¼€å‘ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦å’Œç§‘å­¦æ•™è‚²ä¸­ã€‚é€šè¿‡æœ‰æ•ˆçš„åé¦ˆæœºåˆ¶ï¼Œå¯ä»¥å¸®åŠ©ä¸åŒè¯­è¨€èƒŒæ™¯çš„å­¦ç”Ÿæ›´å¥½åœ°ç†è§£å’ŒæŒæ¡çŸ¥è¯†ï¼Œä»è€Œæå‡æ•™è‚²çš„å…¬å¹³æ€§å’ŒåŒ…å®¹æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated the ability to generate formative feedback and instructional hints in English, making them increasingly relevant for AI-assisted education. However, their ability to provide effective instructional support across different languages, especially for mathematically grounded reasoning tasks, remains largely unexamined. In this work, we present the first large-scale simulation of multilingual tutor-student interactions using LLMs. A stronger model plays the role of the tutor, generating feedback in the form of hints, while a weaker model simulates the student. We explore 352 experimental settings across 11 typologically diverse languages, four state-of-the-art LLMs, and multiple prompting strategies to assess whether language-specific feedback leads to measurable learning gains. Our study examines how student input language, teacher feedback language, model choice, and language resource level jointly influence performance. Results show that multilingual hints can significantly improve learning outcomes, particularly in low-resource languages when feedback is aligned with the student's native language. These findings offer practical insights for developing multilingual, LLM-based educational tools that are both effective and inclusive.

