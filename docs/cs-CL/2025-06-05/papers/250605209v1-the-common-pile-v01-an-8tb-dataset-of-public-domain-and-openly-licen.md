---
layout: default
title: The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text
---

# The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05209" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05209v1</a>
  <a href="https://arxiv.org/pdf/2506.05209.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05209v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05209v1', 'The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nikhil Kandpal, Brian Lester, Colin Raffel, Sebastian Majstorovic, Stella Biderman, Baber Abbasi, Luca Soldaini, Enrico Shippole, A. Feder Cooper, Aviya Skowron, John Kirchenbauer, Shayne Longpre, Lintang Sutawika, Alon Albalak, Zhenlin Xu, Guilherme Penedo, Loubna Ben Allal, Elie Bakouch, John David Pressman, Honglu Fan, Dashiell Stander, Guangyu Song, Aaron Gokaslan, Tom Goldstein, Brian R. Bartoldson, Bhavya Kailkhura, Tyler Murray

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å‘å¸ƒCommon Pile v0.1æ•°æ®é›†ä»¥è§£å†³LLMè®­ç»ƒä¸­çš„ç‰ˆæƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…¬å¼€è®¸å¯æ–‡æœ¬` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®é›†æ„å»º` `çŸ¥è¯†äº§æƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMè®­ç»ƒé€šå¸¸ä¾èµ–æœªæˆæƒæ–‡æœ¬ï¼Œå¯¼è‡´çŸ¥è¯†äº§æƒå’Œä¼¦ç†é—®é¢˜ï¼ŒäºŸéœ€è§£å†³ã€‚
2. æœ¬æ–‡æå‡ºCommon Pile v0.1æ•°æ®é›†ï¼ŒåŒ…å«8TBçš„å…¬å¼€è®¸å¯æ–‡æœ¬ï¼Œæ—¨åœ¨ä¸ºLLMè®­ç»ƒæä¾›é«˜è´¨é‡æ•°æ®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºCommon Pileè®­ç»ƒçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸æœªæˆæƒæ–‡æœ¬è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸åœ¨å¤§é‡æœªæˆæƒæ–‡æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¼•å‘äº†çŸ¥è¯†äº§æƒå’Œä¼¦ç†é—®é¢˜çš„å…³æ³¨ã€‚åŸºäºå…¬å¼€è®¸å¯æ–‡æœ¬è®­ç»ƒLLMsæ˜¯è§£å†³è¿™äº›é—®é¢˜çš„ç¬¬ä¸€æ­¥ï¼Œä½†ä»¥å¾€çš„æ•°æ®æ”¶é›†å·¥ä½œå¾€å¾€å¯¼è‡´æ•°æ®é›†è¿‡å°æˆ–è´¨é‡ä½ä¸‹ï¼Œæ— æ³•æœ‰æ•ˆè®­ç»ƒLLMsã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ”¶é›†ã€æ•´ç†å¹¶å‘å¸ƒäº†Common Pile v0.1ï¼Œè¿™æ˜¯ä¸€ä¸ª8TBçš„å…¬å¼€è®¸å¯æ–‡æœ¬é›†åˆï¼Œæ—¨åœ¨ä¸ºLLMé¢„è®­ç»ƒæä¾›æ”¯æŒã€‚Common PileåŒ…å«æ¥è‡ª30ä¸ªæ¥æºçš„å†…å®¹ï¼Œæ¶µç›–ç ”ç©¶è®ºæ–‡ã€ä»£ç ã€ä¹¦ç±ã€ç™¾ç§‘å…¨ä¹¦ã€æ•™è‚²ææ–™ã€éŸ³é¢‘è½¬å½•ç­‰å¤šä¸ªé¢†åŸŸã€‚æˆ‘ä»¬é€šè¿‡åœ¨Common Pileæ–‡æœ¬ä¸Šè®­ç»ƒä¸¤ä¸ª70äº¿å‚æ•°çš„LLMsï¼ˆComma v0.1-1Tå’ŒComma v0.1-2Tï¼‰æ¥éªŒè¯æˆ‘ä»¬çš„åŠªåŠ›ï¼Œç»“æœæ˜¾ç¤ºè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸åœ¨ç±»ä¼¼è®¡ç®—é¢„ç®—ä¸‹è®­ç»ƒçš„æœªæˆæƒæ–‡æœ¬çš„LLMsï¼ˆå¦‚Llama 1å’Œ2 7Bï¼‰å…·æœ‰ç«äº‰åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ä½¿ç”¨æœªæˆæƒæ–‡æœ¬å¸¦æ¥çš„çŸ¥è¯†äº§æƒå’Œä¼¦ç†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å°è§„æ¨¡æˆ–ä½è´¨é‡çš„æ•°æ®é›†ï¼Œæ— æ³•æœ‰æ•ˆæ”¯æŒLLMçš„æ€§èƒ½æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ”¶é›†å’Œæ•´ç†å¤§é‡å…¬å¼€è®¸å¯çš„æ–‡æœ¬æ•°æ®ï¼Œæ„å»ºä¸€ä¸ªé«˜è´¨é‡çš„å¤§å‹æ•°æ®é›†Common Pile v0.1ï¼Œä¸ºLLMçš„é¢„è®­ç»ƒæä¾›æ”¯æŒã€‚æ­¤ä¸¾ä¸ä»…è§£å†³äº†ç‰ˆæƒé—®é¢˜ï¼Œè¿˜ä¸ºç ”ç©¶è€…æä¾›äº†ä¸°å¯Œçš„è®­ç»ƒèµ„æºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCommon Pile v0.1çš„æ•°æ®æ¥æºäº30ä¸ªä¸åŒé¢†åŸŸï¼ŒåŒ…æ‹¬ç ”ç©¶è®ºæ–‡ã€ä»£ç ã€ä¹¦ç±ç­‰ï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’Œè¦†ç›–é¢ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†ä¸¤ä¸ªä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆComma v0.1-1Tå’ŒComma v0.1-2Tï¼‰ï¼Œåˆ†åˆ«åœ¨1ä¸‡äº¿å’Œ2ä¸‡äº¿ä¸ªtokenä¸Šè¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ª8TBçš„é«˜è´¨é‡å…¬å¼€è®¸å¯æ–‡æœ¬æ•°æ®é›†ï¼Œå¡«è¡¥äº†ä»¥å¾€æ•°æ®é›†è§„æ¨¡å’Œè´¨é‡ä¸è¶³çš„ç©ºç™½ã€‚é€šè¿‡åœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºä¸æœªæˆæƒæ–‡æœ¬è®­ç»ƒæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­ï¼Œæ³¨é‡æ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œç¡®ä¿æ¶µç›–å¤šä¸ªé¢†åŸŸçš„æ–‡æœ¬ã€‚åŒæ—¶ï¼Œæ¨¡å‹è®­ç»ƒé‡‡ç”¨äº†å…ˆè¿›çš„å‚æ•°è®¾ç½®å’Œä¼˜åŒ–ç­–ç•¥ï¼Œä»¥æå‡è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥é€‚åº”å¤§è§„æ¨¡æ•°æ®çš„è®­ç»ƒéœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºCommon Pile v0.1è®­ç»ƒçš„Comma v0.1-1Tå’ŒComma v0.1-2Tæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸Llama 1å’Œ2 7Bç­‰æœªæˆæƒæ–‡æœ¬è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„å…¬å¼€è®¸å¯æ–‡æœ¬æ•°æ®é›†ï¼Œç ”ç©¶è€…å’Œå¼€å‘è€…å¯ä»¥åœ¨ä¸ä¾µçŠ¯çŸ¥è¯†äº§æƒçš„å‰æä¸‹ï¼Œè®­ç»ƒå‡ºæ›´ä¸ºå¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models.

