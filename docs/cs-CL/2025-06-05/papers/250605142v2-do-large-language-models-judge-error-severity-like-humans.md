---
layout: default
title: Do Large Language Models Judge Error Severity Like Humans?
---

# Do Large Language Models Judge Error Severity Like Humans?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05142" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05142v2</a>
  <a href="https://arxiv.org/pdf/2506.05142.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05142v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05142v2', 'Do Large Language Models Judge Error Severity Like Humans?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Diege Sun, Guanyi Chen, Zhao Fan, Xiaorong Cheng, Tingting He

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒäººç±»ä¸å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é”™è¯¯ä¸¥é‡æ€§åˆ¤æ–­ä¸Šçš„å·®å¼‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é”™è¯¯ä¸¥é‡æ€§` `å¤šæ¨¡æ€è¯„ä¼°` `äººç±»åˆ¤æ–­` `è‡ªåŠ¨è¯„ä¼°å·¥å…·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨è¯„ä¼°å·¥å…·åœ¨åˆ¤æ–­é”™è¯¯ä¸¥é‡æ€§æ—¶ï¼Œå¾€å¾€æ— æ³•å‡†ç¡®åæ˜ äººç±»çš„åˆ¤æ–­æ ‡å‡†ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡æ‰©å±•å®éªŒæ¡†æ¶ï¼Œç³»ç»Ÿæ¯”è¾ƒäººç±»ä¸LLMsåœ¨ä¸åŒé”™è¯¯ç±»å‹ä¸‹çš„è¯„ä¼°ï¼Œæ¢è®¨å…¶åˆ¤æ–­å·®å¼‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepSeek-V3åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€æ¡ä»¶ä¸‹ä¸äººç±»åˆ¤æ–­çš„å¯¹é½åº¦æœ€é«˜ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­è¢«å¹¿æ³›ç”¨ä½œè‡ªåŠ¨è¯„ä¼°å·¥å…·ï¼Œä½†å®ƒä»¬æ˜¯å¦èƒ½å‡†ç¡®å¤åˆ¶äººç±»å¯¹é”™è¯¯ä¸¥é‡æ€§çš„åˆ¤æ–­ä»ä¸æ˜ç¡®ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿæ¯”è¾ƒäº†äººç±»ä¸LLMså¯¹åŒ…å«æ§åˆ¶è¯­ä¹‰é”™è¯¯çš„å›¾åƒæè¿°çš„è¯„ä¼°ã€‚æˆ‘ä»¬æ‰©å±•äº†van Miltenburgç­‰ï¼ˆ2020ï¼‰çš„å®éªŒæ¡†æ¶ï¼Œè¯„ä¼°äº†å››ç§é”™è¯¯ç±»å‹ï¼šå¹´é¾„ã€æ€§åˆ«ã€æœè£…ç±»å‹å’Œé¢œè‰²ã€‚ç ”ç©¶å‘ç°ï¼Œäººç±»å¯¹ä¸åŒé”™è¯¯ç±»å‹èµ‹äºˆä¸åŒçš„ä¸¥é‡æ€§ç­‰çº§ï¼Œè§†è§‰ä¸Šä¸‹æ–‡æ˜¾è‘—å¢å¼ºäº†å¯¹é¢œè‰²å’Œç±»å‹é”™è¯¯çš„æ„ŸçŸ¥ä¸¥é‡æ€§ã€‚å¤§å¤šæ•°LLMså¯¹æ€§åˆ«é”™è¯¯è¯„åˆ†è¾ƒä½ï¼Œä½†å¯¹é¢œè‰²é”™è¯¯è¯„åˆ†å´å¼‚å¸¸é«˜ï¼Œè¿™ä¸äººç±»çš„åˆ¤æ–­å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™è¡¨æ˜è¿™äº›æ¨¡å‹å¯èƒ½å†…åŒ–äº†å½±å“æ€§åˆ«åˆ¤æ–­çš„ç¤¾ä¼šè§„èŒƒï¼Œä½†ç¼ºä¹æ¨¡æ‹Ÿäººç±»å¯¹é¢œè‰²æ•æ„Ÿæ€§çš„æ„ŸçŸ¥åŸºç¡€ã€‚ä»…æœ‰ä¸€ä¸ªLLMï¼ˆDoubaoï¼‰åœ¨é”™è¯¯ä¸¥é‡æ€§æ’åä¸Šæ¥è¿‘äººç±»ï¼Œä½†æœªèƒ½å¦‚äººç±»èˆ¬æ¸…æ™°åœ°åŒºåˆ†é”™è¯¯ç±»å‹ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒDeepSeek-V3ä½œä¸ºå•æ¨¡æ€LLMåœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€æ¡ä»¶ä¸‹ä¸äººç±»åˆ¤æ–­çš„å¯¹é½åº¦æœ€é«˜ï¼Œç”šè‡³è¶…è¿‡äº†æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯„ä¼°å›¾åƒæè¿°é”™è¯¯ä¸¥é‡æ€§æ—¶ä¸äººç±»åˆ¤æ–­ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€è¯„ä¼°ä¸­å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œæ— æ³•å‡†ç¡®åæ˜ äººç±»çš„åˆ¤æ–­æ ‡å‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿæ¯”è¾ƒäººç±»ä¸LLMså¯¹ä¸åŒç±»å‹é”™è¯¯çš„è¯„ä¼°ï¼Œæ¢ç´¢æ¨¡å‹åœ¨æ€§åˆ«å’Œé¢œè‰²é”™è¯¯åˆ¤æ–­ä¸Šçš„å·®å¼‚ï¼Œè¿›è€Œåˆ†æå…¶èƒŒåçš„åŸå› ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†æ‰©å±•çš„å®éªŒæ¡†æ¶ï¼Œæ¶µç›–äº†å•æ¨¡æ€ï¼ˆä»…æ–‡æœ¬ï¼‰å’Œå¤šæ¨¡æ€ï¼ˆæ–‡æœ¬+å›¾åƒï¼‰è®¾ç½®ï¼Œè¯„ä¼°äº†å››ç§é”™è¯¯ç±»å‹ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬é”™è¯¯ç±»å‹å®šä¹‰ã€è¯„ä¼°æ ‡å‡†è®¾å®šå’Œæ¨¡å‹æ¯”è¾ƒåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†LLMsåœ¨é”™è¯¯ä¸¥é‡æ€§åˆ¤æ–­ä¸Šçš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ€§åˆ«å’Œé¢œè‰²é”™è¯¯çš„è¯„ä¼°ä¸Šï¼Œæä¾›äº†å¯¹æ¯”åˆ†æçš„å®è¯æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­é‡‡ç”¨äº†æ§åˆ¶è¯­ä¹‰é”™è¯¯çš„å›¾åƒæè¿°ï¼Œè®¾ç½®äº†å¤šç§è¯„ä¼°æ ‡å‡†ï¼Œå¹¶å¯¹ä¸åŒæ¨¡å‹çš„è¾“å‡ºè¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œç¡®ä¿äº†å®éªŒçš„ä¸¥è°¨æ€§å’Œç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepSeek-V3åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€æ¡ä»¶ä¸‹ä¸äººç±»åˆ¤æ–­çš„å¯¹é½åº¦æœ€é«˜ï¼Œè¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯åœ¨é¢œè‰²é”™è¯¯çš„è¯„ä¼°ä¸Šï¼ŒLLMsè¡¨ç°å‡ºä¸äººç±»åˆ¤æ–­çš„æ˜¾è‘—å·®å¼‚ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨ç¤¾ä¼šè§„èŒƒå†…åŒ–ä¸æ„ŸçŸ¥èƒ½åŠ›ä¸Šçš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿã€æ•™è‚²é¢†åŸŸçš„è‡ªåŠ¨è¯„åˆ†å·¥å…·ä»¥åŠç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹é”™è¯¯ä¸¥é‡æ€§çš„åˆ¤æ–­èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„å†…å®¹ç”Ÿæˆå’Œè¯„ä¼°æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly used as automated evaluators in natural language generation, yet it remains unclear whether they can accurately replicate human judgments of error severity. In this study, we systematically compare human and LLM assessments of image descriptions containing controlled semantic errors. We extend the experimental framework of van Miltenburg et al. (2020) to both unimodal (text-only) and multimodal (text + image) settings, evaluating four error types: age, gender, clothing type, and clothing colour. Our findings reveal that humans assign varying levels of severity to different error types, with visual context significantly amplifying perceived severity for colour and type errors. Notably, most LLMs assign low scores to gender errors but disproportionately high scores to colour errors, unlike humans, who judge both as highly severe but for different reasons. This suggests that these models may have internalised social norms influencing gender judgments but lack the perceptual grounding to emulate human sensitivity to colour, which is shaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao, replicates the human-like ranking of error severity, but it fails to distinguish between error types as clearly as humans. Surprisingly, DeepSeek-V3, a unimodal LLM, achieves the highest alignment with human judgments across both unimodal and multimodal conditions, outperforming even state-of-the-art multimodal models.

