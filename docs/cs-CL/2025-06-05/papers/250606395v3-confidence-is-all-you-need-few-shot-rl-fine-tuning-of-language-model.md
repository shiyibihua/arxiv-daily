---
layout: default
title: Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models
---

# Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06395" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06395v3</a>
  <a href="https://arxiv.org/pdf/2506.06395.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06395v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06395v3', 'Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, Ivan Oseledets

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªä¿¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹å¾®è°ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è¯­è¨€æ¨¡å‹` `è‡ªä¿¡åº¦` `å¾®è°ƒ` `æ— æ ‡ç­¾ç›‘ç£` `æ¨¡å‹ä¼˜åŒ–` `æ•°å­¦åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„äººç±»æ ‡æ³¨æˆ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶åº”ç”¨çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚
2. æå‡ºçš„RLSCæ–¹æ³•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„è‡ªä¿¡åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ ‡ç­¾å’Œå¥–åŠ±å·¥ç¨‹çš„éœ€æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLSCåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¯è¡Œæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åæœŸè®­ç»ƒå¯¹äºä½¿å…¶è¡Œä¸ºä¸ä»»åŠ¡ç›®æ ‡ä¸€è‡´ä»è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„äººç±»æ ‡æ³¨æˆ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è‡ªä¿¡è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼ˆRLSCï¼‰ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„è‡ªä¿¡åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹æ ‡ç­¾ã€åå¥½æ¨¡å‹æˆ–å¥–åŠ±å·¥ç¨‹çš„éœ€æ±‚ã€‚é€šè¿‡åœ¨Qwen2.5-Math-7Bæ¨¡å‹ä¸Šåº”ç”¨RLSCï¼Œä»…ä½¿ç”¨æ¯ä¸ªé—®é¢˜16ä¸ªæ ·æœ¬å’Œ10æˆ–20ä¸ªè®­ç»ƒæ­¥éª¤ï¼ŒRLSCåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ï¼Œåˆ†åˆ«ä¸ºAIME2024æé«˜13.4%ã€MATH500æé«˜21.2%ã€Minerva Mathæé«˜21.7%ã€Olympiadbenchæé«˜20.8%ä»¥åŠAMC23æé«˜9.7%ã€‚RLSCä¸ºæ¨ç†æ¨¡å‹æä¾›äº†ä¸€ç§ç®€å•ã€å¯æ‰©å±•çš„åæœŸè®­ç»ƒæ–¹æ³•ï¼Œä»…éœ€å°‘é‡æ ·æœ¬å’Œæ— æ ‡ç­¾ç›‘ç£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åæœŸè®­ç»ƒä¸­å¯¹æ˜‚è´µäººç±»æ ‡æ³¨å’Œå¤–éƒ¨å¥–åŠ±æ¨¡å‹çš„ä¾èµ–é—®é¢˜ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„RLSCæ–¹æ³•é€šè¿‡åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„è‡ªä¿¡åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œé¿å…äº†å¯¹å¤–éƒ¨æ ‡ç­¾å’Œå¥–åŠ±æ¨¡å‹çš„éœ€æ±‚ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„å¾®è°ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRLSCçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªä¿¡åº¦è®¡ç®—æ¨¡å—ã€å¥–åŠ±ä¿¡å·ç”Ÿæˆæ¨¡å—ä»¥åŠè®­ç»ƒä¼˜åŒ–æ¨¡å—ã€‚æ¨¡å‹é¦–å…ˆè®¡ç®—è‡ªèº«å¯¹æ¯ä¸ªæ ·æœ¬çš„è‡ªä¿¡åº¦ï¼Œç„¶åå°†å…¶ä½œä¸ºå¥–åŠ±ä¿¡å·è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šRLSCçš„æœ€å¤§åˆ›æ–°åœ¨äºå°†è‡ªä¿¡åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œè¿™ä¸€è®¾è®¡ä¸ä¼ ç»ŸRLæ–¹æ³•çš„ä¾èµ–å¤–éƒ¨å¥–åŠ±çš„æ–¹å¼æœ¬è´¨ä¸Šä¸åŒï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RLSCä¸­ï¼Œå…³é”®çš„å‚æ•°è®¾ç½®åŒ…æ‹¬è‡ªä¿¡åº¦é˜ˆå€¼çš„é€‰æ‹©å’Œè®­ç»ƒæ­¥éª¤çš„æ•°é‡ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆè‡ªä¿¡åº¦çš„å¼ºåŒ–å­¦ä¹ æŸå¤±ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡åˆç†çš„è¶…å‚æ•°è°ƒæ•´ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å°‘é‡æ ·æœ¬ä¸Šå®ç°æœ‰æ•ˆçš„å¾®è°ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLSCåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œå…¶ä¸­AIME2024æé«˜äº†13.4%ï¼ŒMATH500æé«˜äº†21.2%ï¼ŒMinerva Mathæé«˜äº†21.7%ï¼ŒOlympiadbenchæé«˜äº†20.8%ï¼ŒAMC23æé«˜äº†9.7%ã€‚è¿™äº›ç»“æœè¡¨æ˜RLSCæ–¹æ³•åœ¨å°‘é‡æ ·æœ¬å’Œæ— æ ‡ç­¾ç›‘ç£æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿå’Œæ™ºèƒ½è¾…å¯¼å·¥å…·ç­‰ã€‚é€šè¿‡ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ï¼ŒRLSCèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œå½±å“åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We propose Reinforcement Learning via Self-Confidence (RLSC), which uses the model's own confidence as reward signals-eliminating the need for labels, preference models, or reward engineering. Applied to Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps, RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a simple, scalable post-training method for inference models, requiring only a small number of samples and unlabelled supervision.

