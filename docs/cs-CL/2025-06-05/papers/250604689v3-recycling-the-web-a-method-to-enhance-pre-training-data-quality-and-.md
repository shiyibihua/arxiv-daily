---
layout: default
title: Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models
---

# Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04689" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04689v3</a>
  <a href="https://arxiv.org/pdf/2506.04689.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04689v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04689v3', 'Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, Xian Li

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-09-15)

**å¤‡æ³¨**: Accepted to COLM 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/datasets/facebook)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºREWIREæ–¹æ³•ä»¥è§£å†³é¢„è®­ç»ƒæ•°æ®è´¨é‡å’Œæ•°é‡ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é¢„è®­ç»ƒ` `æ•°æ®å¢å¼º` `æ–‡æœ¬æ”¹å†™` `åˆæˆæ•°æ®` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é¢„è®­ç»ƒæ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡ç½‘ç»œçˆ¬å–ï¼Œä½†é«˜è´¨é‡æ–‡æœ¬çš„è·å–å—åˆ°é™åˆ¶ï¼Œå¯¼è‡´æ•°æ®è´¨é‡ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºREWIREæ–¹æ³•ï¼Œé€šè¿‡æ”¹å†™è¢«è¿‡æ»¤æ‰çš„ä½è´¨é‡æ–‡æœ¬ï¼Œå¢å¼ºæ•°æ®é›†çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šï¼Œä½¿ç”¨æ··åˆæ•°æ®é›†ç›¸æ¯”ä»…ä½¿ç”¨è¿‡æ»¤åçš„ç½‘ç»œæ•°æ®ï¼Œæ€§èƒ½æå‡è¾¾1.0è‡³2.5ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œæ¨¡å‹æ€§èƒ½ä¸æ•°æ®è§„æ¨¡å‘ˆæ­£ç›¸å…³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç½‘ç»œæ•°æ®çˆ¬å–å¹¶æœªä»¥ç›¸åŒé€Ÿåº¦å¢é•¿ï¼Œä¸”é«˜è´¨é‡æ–‡æœ¬çš„å¯ç”¨æ€§æœ‰é™ã€‚ä¸ºäº†è§£å†³é¢„è®­ç»ƒä¸­çš„æ•°æ®ç“¶é¢ˆï¼Œæœ¬æ–‡æå‡ºäº†REWIREï¼ˆREcycling the Web with guIded REwriteï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ”¹å†™ä½è´¨é‡æ–‡æ¡£æ¥ä¸°å¯Œè®­ç»ƒæ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œç»“åˆé«˜è´¨é‡åŸå§‹æ–‡æœ¬ä¸æ”¹å†™æ–‡æœ¬çš„æ··åˆæ•°æ®é›†ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å…¬å¼€äº†é«˜è´¨é‡åˆæˆæ•°æ®ï¼Œä¾›ç ”ç©¶è€…ä½¿ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„æ•°æ®è´¨é‡å’Œæ•°é‡ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºç½‘ç»œçˆ¬å–çš„æ•°æ®ï¼Œä½†é«˜è´¨é‡æ–‡æœ¬çš„æ¯”ä¾‹æä½ï¼Œå¯¼è‡´æœ‰æ•ˆè®­ç»ƒæ•°æ®ç¨€ç¼ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šREWIREæ–¹æ³•é€šè¿‡å¯¹ä½è´¨é‡æ–‡æ¡£è¿›è¡Œæ”¹å†™ï¼Œä½¿å…¶èƒ½å¤Ÿè¢«æœ‰æ•ˆåˆ©ç”¨ï¼Œä»è€Œå¢åŠ åˆæˆæ•°æ®åœ¨æœ€ç»ˆè®­ç»ƒé›†ä¸­çš„æ¯”ä¾‹ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æå‡æ•°æ®é›†çš„æ•´ä½“è´¨é‡å’Œå¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šREWIREçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ä½è´¨é‡æ–‡æœ¬ç­›é€‰ã€æ–‡æœ¬æ”¹å†™å’Œæœ€ç»ˆæ•°æ®é›†æ„å»ºå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†ç½‘ç»œæ•°æ®ï¼Œç„¶åç­›é€‰å‡ºä½è´¨é‡æ–‡æœ¬ï¼Œæ¥ç€é€šè¿‡æ”¹å†™æŠ€æœ¯æå‡å…¶è´¨é‡ï¼Œæœ€åä¸é«˜è´¨é‡æ–‡æœ¬æ··åˆå½¢æˆæ–°çš„è®­ç»ƒæ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šREWIREçš„åˆ›æ–°åœ¨äºå…¶é€šè¿‡æ”¹å†™ä½è´¨é‡æ–‡æœ¬æ¥ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ä»…ä¾èµ–é«˜è´¨é‡æ•°æ®çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚å®ƒæœ‰æ•ˆåœ°åˆ©ç”¨äº†è¢«è¿‡æ»¤æ‰çš„æ•°æ®ï¼Œæå‡äº†æ•°æ®çš„åˆ©ç”¨ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒREWIREä½¿ç”¨äº†ç‰¹å®šçš„æ”¹å†™ç®—æ³•ï¼Œç¡®ä¿ç”Ÿæˆçš„æ–‡æœ¬åœ¨è¯­ä¹‰ä¸Šä¸åŸæ–‡ç›¸ä¼¼ï¼ŒåŒæ—¶å…·å¤‡æ›´é«˜çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ç›¸åº”çš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ”¹å†™è¿‡ç¨‹ä¸­çš„æ–‡æœ¬è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨DCLMåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨REWIREæ–¹æ³•çš„æ··åˆæ•°æ®é›†åœ¨22ä¸ªä¸åŒä»»åŠ¡ä¸Šåˆ†åˆ«æå‡äº†1.0ã€1.3å’Œ2.5ä¸ªç™¾åˆ†ç‚¹çš„æ€§èƒ½ï¼Œç›¸æ¯”äºä»…ä½¿ç”¨è¿‡æ»¤åçš„ç½‘ç»œæ•°æ®ï¼Œè¡¨ç°æ›´ä¸ºä¼˜è¶Šã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ··åˆæ•°æ®é›†çš„æ•ˆæœä¼˜äºç®€å•å¢åŠ 2å€çš„ç½‘ç»œæ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡é¢„è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œæ•°é‡ï¼ŒREWIREæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æ”¹å–„æ¨¡å‹çš„æ€§èƒ½ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šåœ¨æ›´å¹¿æ³›çš„AIåº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Scaling laws predict that the performance of large language models improves with increasing model size and data size. In practice, pre-training has been relying on massive web crawls, using almost all data sources publicly available on the internet so far. However, this pool of natural data does not grow at the same rate as the compute supply. Furthermore, the availability of high-quality texts is even more limited: data filtering pipelines often remove up to 99% of the initial web scrapes to achieve state-of-the-art. To address the "data wall" of pre-training scaling, our work explores ways to transform and recycle data discarded in existing filtering processes. We propose REWIRE, REcycling the Web with guIded REwrite, a method to enrich low-quality documents so that they could become useful for training. This in turn allows us to increase the representation of synthetic data in the final pre-training set. Experiments at 1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points improvement respectively across 22 diverse tasks, compared to training on only filtered web data. Training on the raw-synthetic data mix is also more effective than having access to 2x web data. Through further analysis, we demonstrate that about 82% of the mixed in texts come from transforming lower-quality documents that would otherwise be discarded. REWIRE also outperforms related approaches of generating synthetic data, including Wikipedia-style paraphrasing, question-answer synthesizing and knowledge extraction. These results suggest that recycling web texts holds the potential for being a simple and effective approach for scaling pre-training data. We make our high-quality synthetic data publicly available at https://huggingface.co/datasets/facebook/recycling_the_web.

