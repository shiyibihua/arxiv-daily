---
layout: default
title: Identifying Reliable Evaluation Metrics for Scientific Text Revision
---

# Identifying Reliable Evaluation Metrics for Scientific Text Revision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04772" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04772v3</a>
  <a href="https://arxiv.org/pdf/2506.04772.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04772v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04772v3', 'Identifying Reliable Evaluation Metrics for Scientific Text Revision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: LÃ©ane Jourdan, Florian Boudin, Richard Dufour, Nicolas Hernandez

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-12)

**å¤‡æ³¨**: V3 contains only the English version, accepted to ACL 2025 main (26 pages). V2 contains both English (ACL 2025) and French (TALN 2025) versions (58 pages)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆè¯„ä¼°æ–¹æ³•ä»¥è§£å†³ç§‘å­¦æ–‡æœ¬ä¿®è®¢è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬ä¿®è®¢` `è¯„ä¼°æŒ‡æ ‡` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç§‘å­¦å†™ä½œ` `æ··åˆæ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡å¦‚ROUGEå’ŒBERTScoreæœªèƒ½æœ‰æ•ˆæ•æ‰æ–‡æœ¬ä¿®è®¢çš„å®é™…æ”¹è¿›ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„å¯é æ€§ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆè¯„ä¼°æ–¹æ³•ï¼Œç»“åˆäº†LLMä½œä¸ºè¯„ä¼°è€…çš„èƒ½åŠ›ä¸é¢†åŸŸç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æé«˜è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå®éªŒç»“æœæ˜¾ç¤ºï¼Œæ··åˆè¯„ä¼°æ–¹æ³•åœ¨ä¿®è®¢è´¨é‡è¯„ä¼°ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç§‘å­¦å†™ä½œä¸­çš„æ–‡æœ¬ä¿®è®¢è¯„ä¼°ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡å¦‚ROUGEå’ŒBERTScoreä¸»è¦å…³æ³¨ç›¸ä¼¼æ€§ï¼Œè€Œæœªèƒ½æ•æ‰åˆ°æœ‰æ„ä¹‰çš„æ”¹è¿›ã€‚æœ¬æ–‡åˆ†æå¹¶è¯†åˆ«äº†è¿™äº›æŒ‡æ ‡çš„å±€é™æ€§ï¼Œæ¢ç´¢äº†æ›´ç¬¦åˆäººç±»åˆ¤æ–­çš„æ›¿ä»£è¯„ä¼°æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹æ‰‹åŠ¨æ ‡æ³¨ç ”ç©¶ï¼Œä»¥è¯„ä¼°ä¸åŒä¿®è®¢çš„è´¨é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æ¥è‡ªç›¸å…³NLPé¢†åŸŸçš„æ— å‚è€ƒè¯„ä¼°æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è€ƒå¯Ÿäº†LLMä½œä¸ºè¯„ä¼°è€…çš„æ–¹æ³•ï¼Œåˆ†æå…¶åœ¨æœ‰æ— é‡‘æ ‡å‡†æƒ…å†µä¸‹è¯„ä¼°ä¿®è®¢çš„èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢æœ‰æ•ˆï¼Œä½†åœ¨æ­£ç¡®æ€§è¯„ä¼°ä¸Šå­˜åœ¨å›°éš¾ï¼Œè€Œé¢†åŸŸç‰¹å®šçš„æŒ‡æ ‡æä¾›äº†äº’è¡¥çš„è§è§£ã€‚æˆ‘ä»¬å‘ç°ï¼Œç»“åˆLLMè¯„ä¼°å’Œä»»åŠ¡ç‰¹å®šæŒ‡æ ‡çš„æ··åˆæ–¹æ³•æä¾›äº†æœ€å¯é çš„ä¿®è®¢è´¨é‡è¯„ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç§‘å­¦æ–‡æœ¬ä¿®è®¢è¯„ä¼°ä¸­çš„å¯é æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œæœªèƒ½æœ‰æ•ˆåæ˜ ä¿®è®¢çš„è´¨é‡å’Œæ”¹è¿›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ··åˆè¯„ä¼°æ–¹æ³•ï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œé¢†åŸŸç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ–‡æœ¬ä¿®è®¢çš„è´¨é‡ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å…‹æœä¼ ç»ŸæŒ‡æ ‡çš„å±€é™æ€§ï¼Œæä¾›æ›´ç¬¦åˆäººç±»åˆ¤æ–­çš„è¯„ä¼°ç»“æœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆè¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨ç ”ç©¶ä»¥è¯„ä¼°ä¿®è®¢è´¨é‡ï¼›å…¶æ¬¡è°ƒæŸ¥æ— å‚è€ƒè¯„ä¼°æŒ‡æ ‡ï¼›æœ€ååˆ†æLLMä½œä¸ºè¯„ä¼°è€…çš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†LLMä¸é¢†åŸŸç‰¹å®šæŒ‡æ ‡çš„ç»“åˆä½¿ç”¨ï¼Œè¿™ç§æ··åˆæ–¹æ³•èƒ½å¤Ÿæä¾›æ›´å¯é çš„è¯„ä¼°ç»“æœï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ æ–‡æœ¬ä¿®è®¢çš„å®é™…æ”¹è¿›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLLMçš„é€‰æ‹©å’Œè®­ç»ƒè¿‡ç¨‹è‡³å…³é‡è¦ï¼Œæ­¤å¤–ï¼Œä»»åŠ¡ç‰¹å®šæŒ‡æ ‡çš„é€‰æ‹©ä¹Ÿå½±å“è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œè®ºæ–‡ä¸­å¯¹è¿™äº›å‚æ•°è¿›è¡Œäº†è¯¦ç»†è®¨è®ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ··åˆè¯„ä¼°æ–¹æ³•åœ¨ä¿®è®¢è´¨é‡è¯„ä¼°ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢ï¼ŒLLMçš„è¯„ä¼°èƒ½åŠ›æ˜¾è‘—æå‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ··åˆæ–¹æ³•æä¾›äº†æ›´å…¨é¢çš„è¯„ä¼°è§†è§’ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ–‡æœ¬ä¿®è®¢çš„å®é™…æ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å­¦æœ¯å†™ä½œã€æ–‡æœ¬ç¼–è¾‘å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚é€šè¿‡æä¾›æ›´å¯é çš„æ–‡æœ¬ä¿®è®¢è¯„ä¼°æ–¹æ³•ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œç¼–è¾‘æ›´æœ‰æ•ˆåœ°æ”¹è¿›ç§‘å­¦è®ºæ–‡çš„è´¨é‡ï¼Œæå‡å­¦æœ¯äº¤æµçš„æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–æ–‡æœ¬ç”Ÿæˆå’Œä¿®æ”¹ä»»åŠ¡ä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Evaluating text revision in scientific writing remains a challenge, as traditional metrics such as ROUGE and BERTScore primarily focus on similarity rather than capturing meaningful improvements. In this work, we analyse and identify the limitations of these metrics and explore alternative evaluation methods that better align with human judgments. We first conduct a manual annotation study to assess the quality of different revisions. Then, we investigate reference-free evaluation metrics from related NLP domains. Additionally, we examine LLM-as-a-judge approaches, analysing their ability to assess revisions with and without a gold reference. Our results show that LLMs effectively assess instruction-following but struggle with correctness, while domain-specific metrics provide complementary insights. We find that a hybrid approach combining LLM-as-a-judge evaluation and task-specific metrics offers the most reliable assessment of revision quality.

