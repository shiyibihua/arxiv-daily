---
layout: default
title: ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests
---

# ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04894" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.04894v1</a>
  <a href="https://arxiv.org/pdf/2506.04894.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04894v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04894v1', 'ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shiyi Xu, Yiwen Hu, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-05

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ICPC-Eval‰ª•Ëß£ÂÜ≥LLMÂú®ÁºñÁ®ãÁ´ûËµõ‰∏≠ÁöÑËØÑ‰º∞ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÁºñÁ®ãÁ´ûËµõ` `Êé®ÁêÜËÉΩÂäõ` `ËØÑ‰º∞Âü∫ÂáÜ` `ÊµãËØïÁî®‰æãÁîüÊàê` `Refine@K` `Â§çÊùÇ‰ªªÂä°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÁºñÁ†ÅËÉΩÂäõËØÑ‰º∞Âü∫ÂáÜÊó†Ê≥ïÊúâÊïàÂèçÊò†Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÁúüÂÆûÁ´ûËµõÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞ÔºåËØÑ‰º∞ÊåáÊ†á‰πüÊú™ËÉΩÊçïÊçâÊé®ÁêÜÊ®°ÂûãÁöÑÂèçÊÄùËÉΩÂäõ„ÄÇ
2. ÊèêÂá∫ICPC-EvalÂü∫ÂáÜÔºåÂåÖÂê´Êù•Ëá™11Âú∫ICPCÁ´ûËµõÁöÑ118‰∏™ÈóÆÈ¢òÔºåËÆæËÆ°‰∫ÜÊú¨Âú∞ËØÑ‰º∞Â∑•ÂÖ∑ÂíåRefine@KËØÑ‰º∞ÊåáÊ†áÔºå‰ª•ÊèêÂçáËØÑ‰º∞ÁöÑÂáÜÁ°ÆÊÄßÂíåÊúâÊïàÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÈ°∂Á∫ßÊé®ÁêÜÊ®°ÂûãÂú®Â§öËΩÆÂèçÈ¶à‰∏ãÊâçËÉΩÂÖÖÂàÜÂèëÊå•ÂÖ∂Êé®ÁêÜËÉΩÂäõÔºå‰∏îÂú®‰ª£Á†ÅÁîüÊàêÊñπÈù¢‰ªçËêΩÂêé‰∫é‰∫∫Á±ªÂõ¢ÈòüÔºåÂá∏Êòæ‰∫ÜËØÑ‰º∞ÁöÑÊåëÊàòÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÂú®Â§çÊùÇÁºñÁ†ÅÂíåÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊòæËëóËøõÂ±ïÔºåÁé∞ÊúâÂü∫ÂáÜÔºàÂ¶ÇLiveCodeBenchÂíåCodeEloÔºâ‰∏çË∂≥‰ª•ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁúüÂÆûÁ´ûËµõÁéØÂ¢É‰∏≠ÁöÑÁºñÁ†ÅËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÂΩìÂâçËØÑ‰º∞ÊåáÊ†áÂ¶ÇPass@KÊú™ËÉΩÊçïÊçâÊé®ÁêÜÊ®°ÂûãÁöÑÂèçÊÄùËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜICPC-EvalÔºåËøôÊòØ‰∏Ä‰∏™È°∂Á∫ßÁöÑÁ´û‰∫âÁºñÁ†ÅÂü∫ÂáÜÔºåÊó®Âú®Êé¢ËÆ®LLMÊé®ÁêÜÁöÑÂâçÊ≤ø„ÄÇICPC-EvalÂåÖÂê´Êù•Ëá™ÂÖ®ÁêÉ11Âú∫ËøëÊúüICPCÁ´ûËµõÁöÑ118‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑÈóÆÈ¢òÔºåÊèê‰æõ‰∫Ü‰∏â‰∏™ÂÖ≥ÈîÆË¥°ÁåÆÔºö1ÔºâÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÁé∞ÂÆûICPCÁ´ûËµõÂú∫ÊôØÔºåÈóÆÈ¢òÁ±ªÂûãÂíåÈöæÂ∫¶ÂàÜÂ∏É‰∏éÂÆûÈôÖÁ´ûËµõ‰∏ÄËá¥Ôºõ2ÔºâÂº∫Â§ßÁöÑÊµãËØïÁî®‰æãÁîüÊàêÊñπÊ≥ïÂèäÁõ∏Â∫îÁöÑÊú¨Âú∞ËØÑ‰º∞Â∑•ÂÖ∑ÂåÖÔºåÂÆûÁé∞È´òÊïàÂáÜÁ°ÆÁöÑÊú¨Âú∞ËØÑ‰º∞Ôºõ3ÔºâÊúâÊïàÁöÑÊµãËØïÊó∂Èó¥Êâ©Â±ïËØÑ‰º∞ÊåáÊ†áRefine@KÔºåÂÖÅËÆ∏Âü∫‰∫éÊâßË°åÂèçÈ¶àÁöÑËø≠‰ª£‰øÆÂ§ç„ÄÇÁªìÊûúÂº∫Ë∞É‰∫ÜËØÑ‰º∞Â§çÊùÇÊé®ÁêÜËÉΩÂäõÁöÑÈáçÂ§ßÊåëÊàòÔºöÈ°∂Á∫ßÊé®ÁêÜÊ®°ÂûãÂ¶ÇDeepSeek-R1ÈÄöÂ∏∏‰æùËµñÂ§öËΩÆ‰ª£Á†ÅÂèçÈ¶àÊù•ÂÖÖÂàÜÈáäÊîæÂÖ∂‰∏ä‰∏ãÊñáÊé®ÁêÜÊΩúÂäõÔºå‰∏éÈùûÊé®ÁêÜÊ®°ÂûãÁõ∏ÊØî„ÄÇÊ≠§Â§ñÔºåÂ∞ΩÁÆ°‰ª£Á†ÅÁîüÊàêÊñπÈù¢ÂèñÂæó‰∫ÜËøëÊúüËøõÂ±ïÔºåËøô‰∫õÊ®°Âûã‰ªçËêΩÂêé‰∫éË°®Áé∞ÊúÄ‰Ω≥ÁöÑ‰∫∫Á±ªÂõ¢Èòü„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËØÑ‰º∞Âü∫ÂáÜÊó†Ê≥ïÊúâÊïàËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÁúüÂÆûÁºñÁ®ãÁ´ûËµõ‰∏≠ÁöÑË°®Áé∞Ëøô‰∏ÄÂÖ∑‰ΩìÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇLiveCodeBenchÂíåCodeEloÊú™ËÉΩÂÖÖÂàÜÊçïÊçâÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÂèçÊÄùËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ICPC-EvalÂü∫ÂáÜÔºåÈÄöËøáËÆæËÆ°ÁúüÂÆûÁöÑÁ´ûËµõÂú∫ÊôØÂíåÊúâÊïàÁöÑËØÑ‰º∞Â∑•ÂÖ∑ÔºåÊù•ÂÖ®Èù¢ËØÑ‰º∞LLMsÁöÑÁºñÁ†ÅËÉΩÂäõÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°Êó®Âú®Êèê‰æõÊõ¥ÂÖ∑ÊåëÊàòÊÄßÂíåÁé∞ÂÆûÊÄßÁöÑÊµãËØïÁéØÂ¢É„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöICPC-EvalÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÈóÆÈ¢òÈõÜÁöÑÊûÑÂª∫„ÄÅÊµãËØïÁî®‰æãÁîüÊàêÊñπÊ≥ïÂíåÊú¨Âú∞ËØÑ‰º∞Â∑•ÂÖ∑„ÄÇÈóÆÈ¢òÈõÜÁî±118‰∏™ÈóÆÈ¢òÁªÑÊàêÔºåÊ∂µÁõñÂ§öÁßçÈöæÂ∫¶ÂíåÁ±ªÂûãÔºõÊµãËØïÁî®‰æãÁîüÊàêÊñπÊ≥ïÁ°Æ‰øù‰∫ÜËØÑ‰º∞ÁöÑÂ§öÊ†∑ÊÄßÂíåÂáÜÁ°ÆÊÄßÔºõÊú¨Âú∞ËØÑ‰º∞Â∑•ÂÖ∑ÂàôÊîØÊåÅÈ´òÊïàÁöÑËØÑ‰º∞ËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÊèêÂá∫‰∫ÜRefine@KËØÑ‰º∞ÊåáÊ†áÔºåÂÆÉÂÖÅËÆ∏Ê®°ÂûãÂú®ÊµãËØïËøáÁ®ã‰∏≠Ê†πÊçÆÊâßË°åÂèçÈ¶àËøõË°åËø≠‰ª£‰øÆÂ§ç„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÈùôÊÄÅËØÑ‰º∞ÊåáÊ†áÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂèçÊò†Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÂ§öËΩÆÂèçÈ¶àÊú∫Âà∂ÔºåÁ°Æ‰øùÊ®°ÂûãËÉΩÂ§üÂú®ÊØèÊ¨°ÂèçÈ¶à‰∏≠ÈÄêÊ≠•ÊîπËøõÂÖ∂Ëß£ÂÜ≥ÊñπÊ°à„ÄÇÊ≠§Â§ñÔºåÊµãËØïÁî®‰æãÁîüÊàêÊñπÊ≥ïÂíåËØÑ‰º∞Â∑•ÂÖ∑ÁöÑËÆæËÆ°‰πüÊ≥®Èáç‰∫ÜÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÔºå‰ª•ÈÄÇÂ∫îÂÆûÈôÖÁ´ûËµõÁöÑÈúÄÊ±Ç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈ°∂Á∫ßÊé®ÁêÜÊ®°ÂûãÂ¶ÇDeepSeek-R1Âú®Â§öËΩÆÂèçÈ¶à‰∏ãÁöÑË°®Áé∞ÊòæËëó‰ºò‰∫éÂçïÊ¨°ËØÑ‰º∞ÔºåÂÖÖÂàÜÂ±ïÁé∞‰∫ÜÂÖ∂Êé®ÁêÜÊΩúÂäõ„ÄÇÊ≠§Â§ñÔºåÂ∞ΩÁÆ°Âú®‰ª£Á†ÅÁîüÊàêÊñπÈù¢ÊúâÊâÄËøõÂ±ïÔºåËøô‰∫õÊ®°ÂûãÁöÑË°®Áé∞‰ªçËêΩÂêé‰∫é‰∫∫Á±ªÂõ¢ÈòüÔºåÂº∫Ë∞É‰∫ÜÂΩìÂâçÊäÄÊúØÁöÑÂ±ÄÈôêÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅÁºñÁ®ãÁ´ûËµõÂíåËá™Âä®Âåñ‰ª£Á†ÅÁîüÊàêÁ≠â„ÄÇÈÄöËøáÊèê‰æõ‰∏Ä‰∏™Ê†áÂáÜÂåñÁöÑËØÑ‰º∞Âü∫ÂáÜÔºåICPC-EvalÂèØ‰ª•Â∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÂíåÂºÄÂèëËÄÖÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÁºñÁ®ã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑËøõÊ≠•‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> With the significant progress of large reasoning models in complex coding and reasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are insufficient to evaluate the coding capabilities of large language models (LLMs) in real competition environments. Moreover, current evaluation metrics such as Pass@K fail to capture the reflective abilities of reasoning models. To address these challenges, we propose \textbf{ICPC-Eval}, a top-level competitive coding benchmark designed to probing the frontiers of LLM reasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent ICPC contests held in various regions of the world, offering three key contributions: 1) A challenging realistic ICPC competition scenario, featuring a problem type and difficulty distribution consistent with actual contests. 2) A robust test case generation method and a corresponding local evaluation toolkit, enabling efficient and accurate local evaluation. 3) An effective test-time scaling evaluation metric, Refine@K, which allows iterative repair of solutions based on execution feedback. The results underscore the significant challenge in evaluating complex reasoning abilities: top-tier reasoning models like DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their in-context reasoning potential when compared to non-reasoning counterparts. Furthermore, despite recent advancements in code generation, these models still lag behind top-performing human teams. We release the benchmark at: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs

