---
layout: default
title: Normative Conflicts and Shallow AI Alignment
---

# Normative Conflicts and Shallow AI Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04679" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04679v1</a>
  <a href="https://arxiv.org/pdf/2506.04679.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04679v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04679v1', 'Normative Conflicts and Shallow AI Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: RaphaÃ«l MilliÃ¨re

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: Published in Philosophical Studies

**æœŸåˆŠ**: Milliere, R. (2025). Normative conflicts and shallow AI alignment. Philosophical Studies, 1-44

**DOI**: [10.1007/s11098-025-02347-3](https://doi.org/10.1007/s11098-025-02347-3)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨LLMçš„è§„èŒƒå†²çªä¸æµ…å±‚AIå¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `AIå®‰å…¨` `ä»·å€¼å¯¹é½` `é“å¾·å¿ƒç†å­¦` `è§„èŒƒå†²çª` `å¯¹æŠ—æ€§æ”»å‡»` `æ·±å±‚å¯¹é½` `ä¼¦ç†AI`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹é½æ–¹æ³•æ— æ³•æœ‰æ•ˆé˜²æ­¢LLMsè¢«æ»¥ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨è§„èŒƒå†²çªçš„æƒ…å†µä¸‹ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¢å¼ºLLMsçš„è§„èŒƒæ€§æ€è€ƒèƒ½åŠ›ï¼Œæ¥è§£å†³å…¶åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸­çš„è„†å¼±æ€§ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„æ¨ç†-focused LLMsæœªèƒ½è§£å†³è§„èŒƒå†²çªé—®é¢˜ï¼Œä»éœ€è¿›ä¸€æ­¥æ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼ŒAIç³»ç»Ÿçš„å®‰å…¨éƒ¨ç½²é—®é¢˜æ—¥ç›Šçªå‡ºã€‚æœ¬æ–‡æ¢è®¨äº†LLMsçš„ä»·å€¼å¯¹é½é—®é¢˜ï¼Œè®¤ä¸ºå½“å‰çš„å¯¹é½ç­–ç•¥æ— æ³•æœ‰æ•ˆé˜²æ­¢æ»¥ç”¨ã€‚å°½ç®¡é€šè¿‡åŸºäºäººç±»åå¥½çš„å¾®è°ƒæ¥çŒè¾“æœ‰ç”¨æ€§ã€è¯šå®æ€§å’Œæ— å®³æ€§ç­‰è§„èŒƒï¼ŒLLMsä¾ç„¶å®¹æ˜“å—åˆ°åˆ©ç”¨è¿™äº›è§„èŒƒä¹‹é—´å†²çªçš„å¯¹æŠ—æ€§æ”»å‡»ã€‚æœ¬æ–‡æŒ‡å‡ºï¼Œè¿™ç§è„†å¼±æ€§åæ˜ äº†ç°æœ‰å¯¹é½æ–¹æ³•çš„æ ¹æœ¬å±€é™æ€§ï¼šå®ƒä»¬å¼ºåŒ–äº†æµ…å±‚è¡Œä¸ºå€¾å‘ï¼Œè€Œæœªèƒ½èµ‹äºˆLLMsçœŸæ­£çš„è§„èŒƒæ€§æ€è€ƒèƒ½åŠ›ã€‚é€šè¿‡å€Ÿé‰´é“å¾·å¿ƒç†å­¦çš„ç ”ç©¶ï¼Œæœ¬æ–‡å±•ç¤ºäº†äººç±»åœ¨è§„èŒƒå†²çªä¸­çš„ç†æ€§æ¨ç†èƒ½åŠ›å¦‚ä½•å¢å¼ºå…¶æŠµå¾¡ç±»ä¼¼å¯¹æŠ—æ€§ç­–ç•¥çš„èƒ½åŠ›ã€‚ç›¸è¾ƒä¹‹ä¸‹ï¼ŒLLMsç¼ºä¹æœ‰æ•ˆæ£€æµ‹å’Œç†æ€§è§£å†³è§„èŒƒå†²çªçš„èƒ½åŠ›ï¼Œä½¿å…¶æ˜“å—æ“æ§ï¼Œç”šè‡³è¿‘æœŸåœ¨æ¨ç†æ–¹é¢çš„è¿›å±•ä¹Ÿæœªèƒ½è§£å†³è¿™ä¸€è„†å¼±æ€§ã€‚æ­¤â€œæµ…å±‚å¯¹é½â€é—®é¢˜å¯¹AIå®‰å…¨å’Œç›‘ç®¡å…·æœ‰é‡è¦å½±å“ï¼Œè¡¨æ˜å½“å‰æ–¹æ³•ä¸è¶³ä»¥å‡è½»æ—¥ç›Šå¼ºå¤§çš„AIç³»ç»Ÿå¯èƒ½å¸¦æ¥çš„å±å®³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰çš„å¯¹é½æ–¹æ³•åœ¨é¢å¯¹è§„èŒƒå†²çªæ—¶çš„è„†å¼±æ€§ï¼Œå¯¼è‡´LLMså®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å¾®è°ƒæ¥å¼ºåŒ–è¡Œä¸ºè§„èŒƒï¼Œä½†æœªèƒ½æœ‰æ•ˆèµ‹äºˆæ¨¡å‹æ·±å±‚æ¬¡çš„è§„èŒƒæ€§æ€è€ƒèƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´é“å¾·å¿ƒç†å­¦çš„ç ”ç©¶ï¼Œæå‡ºé€šè¿‡å¢å¼ºLLMsçš„è§„èŒƒæ€§æ€è€ƒèƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æ£€æµ‹å’Œè§£å†³è§„èŒƒå†²çªï¼Œä»è€Œæé«˜å…¶æŠµå¾¡å¯¹æŠ—æ€§æ”»å‡»çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹LLMsè¿›è¡Œè§„èŒƒæ€§æ€è€ƒèƒ½åŠ›çš„è®­ç»ƒï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬äººç±»åå¥½å¾®è°ƒã€è§„èŒƒå†²çªæ£€æµ‹å’Œç†æ€§æ¨ç†èƒ½åŠ›çš„å¢å¼ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†â€œæ·±å±‚å¯¹é½â€çš„æ¦‚å¿µï¼Œå¼ºè°ƒèµ‹äºˆLLMsçœŸæ­£çš„è§„èŒƒæ€§æ€è€ƒèƒ½åŠ›ï¼Œè€Œä¸ä»…ä»…æ˜¯è¡Œä¸ºä¸Šçš„å¯¹é½ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œåè€…ä¸»è¦å…³æ³¨è¡Œä¸ºè¡¨ç°ï¼Œè€Œéå†…åœ¨çš„æ€è€ƒèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹æŸå¤±å‡½æ•°çš„è°ƒæ•´ï¼Œä»¥é¼“åŠ±æ¨¡å‹åœ¨é¢å¯¹è§„èŒƒå†²çªæ—¶è¿›è¡Œç†æ€§æ¨ç†ï¼Œæ­¤å¤–ï¼Œç½‘ç»œç»“æ„ä¸Šå¯èƒ½éœ€è¦å¼•å…¥æ–°çš„æ¨¡å—ä»¥æ”¯æŒå¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚å°šæœªæ˜ç¡®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºLLMsçš„è§„èŒƒæ€§æ€è€ƒèƒ½åŠ›åï¼Œå…¶åœ¨é¢å¯¹è§„èŒƒå†²çªæ—¶çš„æŠµå¾¡èƒ½åŠ›æ˜¾è‘—æå‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿å°šæœªæä¾›ï¼Œä½†ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰æ¨ç†-focused LLMsåœ¨æ­¤æ–¹é¢çš„ä¸è¶³ï¼Œå¼ºè°ƒäº†æœ¬ç ”ç©¶çš„å¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬AIå®‰å…¨ã€ä¼¦ç†AIè®¾è®¡å’Œæ”¿ç­–åˆ¶å®šã€‚é€šè¿‡å¢å¼ºLLMsçš„è§„èŒƒæ€§æ€è€ƒèƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ›´å¹¿æ³›çš„åœºæ™¯ä¸­å®‰å…¨åœ°éƒ¨ç½²AIç³»ç»Ÿï¼Œå‡å°‘æ½œåœ¨çš„æ»¥ç”¨é£é™©ï¼Œæ¨åŠ¨AIæŠ€æœ¯çš„è´Ÿè´£ä»»å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The progress of AI systems such as large language models (LLMs) raises increasingly pressing concerns about their safe deployment. This paper examines the value alignment problem for LLMs, arguing that current alignment strategies are fundamentally inadequate to prevent misuse. Despite ongoing efforts to instill norms such as helpfulness, honesty, and harmlessness in LLMs through fine-tuning based on human preferences, they remain vulnerable to adversarial attacks that exploit conflicts between these norms. I argue that this vulnerability reflects a fundamental limitation of existing alignment methods: they reinforce shallow behavioral dispositions rather than endowing LLMs with a genuine capacity for normative deliberation. Drawing from on research in moral psychology, I show how humans' ability to engage in deliberative reasoning enhances their resilience against similar adversarial tactics. LLMs, by contrast, lack a robust capacity to detect and rationally resolve normative conflicts, leaving them susceptible to manipulation; even recent advances in reasoning-focused LLMs have not addressed this vulnerability. This ``shallow alignment'' problem carries significant implications for AI safety and regulation, suggesting that current approaches are insufficient for mitigating potential harms posed by increasingly capable AI systems.

