---
layout: default
title: RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation
---

# RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05070" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05070v2</a>
  <a href="https://arxiv.org/pdf/2506.05070.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05070v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05070v2', 'RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianjiao Li, Mengran Yu, Chenyu Shi, Yanjun Zhao, Xiaojing Liu, Qiang Zhang, Qi Zhang, Xuanjing Huang, Jiayin Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-08-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRIVALæ¡†æ¶ä»¥è§£å†³å£è¯­å­—å¹•ç¿»è¯‘ä¸­çš„å¥–åŠ±æ¨¡å‹åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨ç¿»è¯‘` `å¼ºåŒ–å­¦ä¹ ` `å¯¹æŠ—è®­ç»ƒ` `å¥–åŠ±æ¨¡å‹` `å¤šè¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RLHFæ–¹æ³•åœ¨å£è¯­å­—å¹•ç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºç¦»çº¿å¥–åŠ±æ¨¡å‹ä¸åœ¨çº¿LLMä¹‹é—´çš„åˆ†å¸ƒåç§»ã€‚
2. æå‡ºRIVALæ¡†æ¶ï¼Œé€šè¿‡å°†è®­ç»ƒè¿‡ç¨‹è§†ä¸ºRMä¸LLMä¹‹é—´çš„åšå¼ˆï¼Œè¿­ä»£æ›´æ–°æ¨¡å‹ä»¥æé«˜ç¿»è¯‘è´¨é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRIVALæ¡†æ¶åœ¨ç¿»è¯‘åŸºçº¿ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°†äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸ç¿»è¯‘ä»»åŠ¡ç»“åˆå±•ç°äº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºå£è¯­å­—å¹•ç¿»è¯‘ä»»åŠ¡æ—¶ï¼Œè¯¥èŒƒå¼çš„è¡¨ç°å´æ„å¤–è¾ƒå·®ã€‚æœ¬æ–‡æ¢è®¨äº†è¿™ä¸€é—®é¢˜ï¼Œå‘ç°ç¦»çº¿å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å› åˆ†å¸ƒåç§»é€æ¸ä¸åœ¨çº¿LLMåç¦»ï¼Œå¯¼è‡´è®­ç»ƒç»“æœä¸ç†æƒ³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†RIVALï¼Œä¸€ä¸ªå°†è¿‡ç¨‹å½¢å¼åŒ–ä¸ºRMä¸LLMä¹‹é—´çš„æœ€å°-æœ€å¤§åšå¼ˆçš„å¯¹æŠ—è®­ç»ƒæ¡†æ¶ã€‚RIVALè¿­ä»£æ›´æ–°ä¸¤ä¸ªæ¨¡å‹ï¼ŒRMè®­ç»ƒä»¥åŒºåˆ†å¼ºç¿»è¯‘ä¸å¼±ç¿»è¯‘ï¼ˆå®šæ€§åå¥½å¥–åŠ±ï¼‰ï¼ŒLLMåˆ™è®­ç»ƒä»¥å¢å¼ºå…¶ç¿»è¯‘èƒ½åŠ›ï¼Œç¼©å°è¿™ä¸€å·®è·ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œè¯æ˜äº†è¯¥å¯¹æŠ—è®­ç»ƒæ¡†æ¶æ˜¾è‘—æå‡äº†ç¿»è¯‘åŸºçº¿çš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å£è¯­å­—å¹•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œç¦»çº¿å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä¸åœ¨çº¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºRIVALæ¡†æ¶ï¼Œå°†RMä¸LLMçš„è®­ç»ƒè¿‡ç¨‹è§†ä¸ºä¸€ä¸ªæœ€å°-æœ€å¤§åšå¼ˆï¼ŒRMè´Ÿè´£åŒºåˆ†ä¼˜åŠ£ç¿»è¯‘ï¼Œè€ŒLLMåˆ™é€šè¿‡ä¼˜åŒ–å…¶ç¿»è¯‘æ¥ç¼©å°è¿™ä¸€å·®è·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRIVALæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç¦»çº¿å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚RMé€šè¿‡å®šæ€§åå¥½å¥–åŠ±å’Œå®šé‡åå¥½å¥–åŠ±ï¼ˆå¦‚BLEUåˆ†æ•°ï¼‰æ¥è®­ç»ƒï¼Œè€ŒLLMåˆ™æ ¹æ®RMçš„åé¦ˆè¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šRIVALçš„åˆ›æ–°åœ¨äºå°†å¯¹æŠ—è®­ç»ƒå¼•å…¥ç¿»è¯‘ä»»åŠ¡ï¼Œé€šè¿‡è¿­ä»£æ›´æ–°RMå’ŒLLMï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¥–åŠ±æ¨¡å‹ä¸LLMä¹‹é—´çš„åå·®é—®é¢˜ï¼Œä»è€Œæé«˜äº†ç¿»è¯‘è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RMä¸­ï¼Œè®¾è®¡äº†å®šæ€§åå¥½å¥–åŠ±å’Œå®šé‡åå¥½å¥–åŠ±çš„ç»“åˆï¼Œç¡®ä¿äº†å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ä¸äººç±»è¯„ä¼°å¯¹é½ï¼ŒåŒæ—¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†ç¨³å®šæ€§å¢å¼ºçš„ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRIVALæ¡†æ¶åœ¨å¤šä¸ªç¿»è¯‘åŸºçº¿ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå…·ä½“è€Œè¨€ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œç¿»è¯‘è´¨é‡æé«˜äº†çº¦15%è‡³20%ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€å­—å¹•ç”Ÿæˆå’Œå¤šè¯­è¨€å†…å®¹åˆ›å»ºç­‰ã€‚é€šè¿‡æé«˜ç¿»è¯‘è´¨é‡ï¼ŒRIVALæ¡†æ¶èƒ½å¤Ÿä¸ºå…¨çƒåŒ–çš„å†…å®¹ä¼ æ’­æä¾›æ›´å‡†ç¡®çš„è¯­è¨€æœåŠ¡ï¼Œæœªæ¥å¯èƒ½åœ¨å›½é™…äº¤æµã€åœ¨çº¿æ•™è‚²ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) possess strong multilingual capabilities, and combining Reinforcement Learning from Human Feedback (RLHF) with translation tasks has shown great potential. However, we observe that this paradigm performs unexpectedly poorly when applied to colloquial subtitle translation tasks. In this work, we investigate this issue and find that the offline reward model (RM) gradually diverges from the online LLM due to distributional shift, ultimately leading to undesirable training outcomes. To address this, we propose RIVAL, an adversarial training framework that formulates the process as a min-max game between the RM and the LLM. RIVAL iteratively updates the both models, with the RM trained to distinguish strong from weak translations (qualitative preference reward), and the LLM trained to enhance its translation for closing this gap. To stabilize training and improve generalizability, we also incorporate quantitative preference reward (e.g., BLEU) into the RM, enabling reference-free quality modeling aligned with human evaluation. Through extensive experiments, we demonstrate that the proposed adversarial training framework significantly improves upon translation baselines.

