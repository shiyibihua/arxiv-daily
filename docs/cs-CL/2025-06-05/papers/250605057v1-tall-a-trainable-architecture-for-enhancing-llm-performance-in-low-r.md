---
layout: default
title: TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages
---

# TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05057" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05057v1</a>
  <a href="https://arxiv.org/pdf/2506.05057.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05057v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05057v1', 'TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Moshe Ofer, Orel Zamler, Amos Azaria

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTALLä»¥æå‡ä½èµ„æºè¯­è¨€çš„LLMæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½èµ„æºè¯­è¨€` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŒè¯­ç¿»è¯‘` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–` `å‚æ•°é«˜æ•ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºç¼ºä¹è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆæ•æ‰è¯­è¨€ç‰¹å¾ã€‚
2. TALLé€šè¿‡å°†LLMä¸åŒè¯­ç¿»è¯‘æ¨¡å‹ç»“åˆï¼Œèƒ½å¤Ÿå°†ä½èµ„æºè¯­è¨€è¾“å…¥è½¬åŒ–ä¸ºé«˜èµ„æºè¯­è¨€è¡¨ç¤ºï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTALLåœ¨å¸Œä¼¯æ¥è¯­ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜èµ„æºè¯­è¨€ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€ä¸­ç”±äºè®­ç»ƒæ•°æ®æœ‰é™è€Œé¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†TALLï¼ˆå¯è®­ç»ƒæ¶æ„ï¼‰ï¼Œå°†LLMä¸ä¸¤ä¸ªåŒè¯­ç¿»è¯‘æ¨¡å‹ç›¸ç»“åˆï¼Œèƒ½å¤Ÿå°†ä½èµ„æºè¾“å…¥è½¬åŒ–ä¸ºé«˜èµ„æºè¡¨ç¤ºï¼Œå……åˆ†åˆ©ç”¨LLMçš„èƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡ç»´åº¦å¯¹é½å±‚å’Œå®šåˆ¶å˜æ¢å™¨ä¿ç•™è¯­è¨€ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨å¸Œä¼¯æ¥è¯­ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTALLåœ¨å¤šä¸ªåŸºçº¿æ–¹æ³•ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒåŒ…æ‹¬ç›´æ¥ä½¿ç”¨ã€ç®€å•ç¿»è¯‘å’Œå¾®è°ƒæ–¹æ³•ã€‚è¯¥æ¶æ„é‡‡ç”¨å‚æ•°é«˜æ•ˆç­–ç•¥ï¼Œå†»ç»“é¢„è®­ç»ƒç»„ä»¶ï¼Œä»…è®­ç»ƒè½»é‡é€‚é…æ¨¡å—ï¼Œå®ç°è®¡ç®—æ•ˆç‡ä¸æ€§èƒ½æå‡çš„å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„æ€§èƒ½ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨æœ‰é™çš„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTALLçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“åˆåŒè¯­ç¿»è¯‘æ¨¡å‹ï¼Œå°†ä½èµ„æºè¯­è¨€è¾“å…¥è½¬åŒ–ä¸ºé«˜èµ„æºè¯­è¨€è¡¨ç¤ºï¼Œä»è€Œå¢å¼ºLLMçš„è¡¨ç°ã€‚è¯¥è®¾è®¡æ—¨åœ¨å……åˆ†åˆ©ç”¨LLMçš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTALLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯è¾“å…¥çš„ä½èµ„æºè¯­è¨€æ•°æ®ï¼Œå…¶æ¬¡æ˜¯åŒè¯­ç¿»è¯‘æ¨¡å‹ç”¨äºè½¬æ¢ï¼Œæœ€åæ˜¯LLMè¿›è¡Œå¤„ç†å’Œç”Ÿæˆé«˜èµ„æºè¯­è¨€è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šTALLçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å‚æ•°é«˜æ•ˆç­–ç•¥ï¼Œé€šè¿‡å†»ç»“é¢„è®­ç»ƒç»„ä»¶ï¼Œä»…è®­ç»ƒè½»é‡é€‚é…æ¨¡å—ï¼Œä»è€Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æå‡æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒTALLåœ¨å‚æ•°ä½¿ç”¨ä¸Šæ›´ä¸ºé«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒTALLé‡‡ç”¨äº†ç»´åº¦å¯¹é½å±‚å’Œå®šåˆ¶å˜æ¢å™¨ï¼Œä»¥ç¡®ä¿ä½èµ„æºè¯­è¨€å’Œé«˜èµ„æºè¯­è¨€ä¹‹é—´çš„ç‰¹å¾å¯¹é½ï¼Œæ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¸Œä¼¯æ¥è¯­çš„å®éªŒä¸­ï¼ŒTALLç›¸è¾ƒäºç›´æ¥ä½¿ç”¨ã€ç®€å•ç¿»è¯‘å’Œå¾®è°ƒæ–¹æ³•ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒTALLåœ¨ä½èµ„æºè¯­è¨€å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œä½èµ„æºè¯­è¨€çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡æå‡ä½èµ„æºè¯­è¨€çš„æ¨¡å‹æ€§èƒ½ï¼ŒTALLæœ‰åŠ©äºæ¨åŠ¨è¿™äº›è¯­è¨€çš„æ•°å­—åŒ–å’Œä¿¡æ¯è·å–ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œå®é™…æ„ä¹‰ã€‚æœªæ¥ï¼ŒTALLçš„æ¶æ„å¯ä»¥æ‰©å±•åˆ°æ›´å¤šè¯­è¨€å’Œä»»åŠ¡ï¼Œè¿›ä¸€æ­¥æå‡å…¶åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) excel in high-resource languages but struggle with low-resource languages due to limited training data. This paper presents TALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages), which integrates an LLM with two bilingual translation models. TALL transforms low-resource inputs into high-resource representations, leveraging the LLM's capabilities while preserving linguistic features through dimension alignment layers and custom transformers. Our experiments on Hebrew demonstrate significant improvements over several baselines, including direct use, naive translation, and fine-tuning approaches. The architecture employs a parameter-efficient strategy, freezing pre-trained components while training only lightweight adapter modules, balancing computational efficiency with performance gains.

