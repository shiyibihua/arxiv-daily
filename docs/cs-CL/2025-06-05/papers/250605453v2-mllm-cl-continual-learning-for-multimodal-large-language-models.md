---
layout: default
title: MLLM-CL: Continual Learning for Multimodal Large Language Models
---

# MLLM-CL: Continual Learning for Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05453" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05453v2</a>
  <a href="https://arxiv.org/pdf/2506.05453.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05453v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05453v2', 'MLLM-CL: Continual Learning for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongbo Zhao, Fei Zhu, Haiyang Guo, Meng Wang, Rundong Wang, Gaofeng Meng, Zhaoxiang Zhang

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-10-01)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/bjzhb666/MLLM-CL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMLLM-CLä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æŒç»­å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æŒç»­å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `å‚æ•°éš”ç¦»` `è·¯ç”±æœºåˆ¶` `é¢†åŸŸé€‚åº”` `éIIDåœºæ™¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€åœºæ™¯ä¸­éš¾ä»¥æŒç»­æ•´åˆæ–°çŸ¥è¯†ï¼Œå¯¼è‡´é€‚åº”æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºMLLM-CLï¼Œé€šè¿‡å‚æ•°éš”ç¦»å’Œè·¯ç”±æœºåˆ¶æ¥é˜²æ­¢ç¾éš¾æ€§å¹²æ‰°ï¼Œä»è€Œå®ç°æŒç»­å­¦ä¹ ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLLM-CLåœ¨æ•´åˆé¢†åŸŸçŸ¥è¯†å’ŒåŠŸèƒ½èƒ½åŠ›æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œé—å¿˜ç‡æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠ¨æ€ç°å®åœºæ™¯ä¸­é€‚åº”æ–°çŸ¥è¯†å’ŒæŠ€èƒ½çš„èƒ½åŠ›é¢ä¸´æŒ‘æˆ˜ã€‚å°½ç®¡æŒç»­å­¦ä¹ ï¼ˆCLï¼‰æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰åŸºå‡†å’Œæ–¹æ³•å­˜åœ¨å…³é”®å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºäº†MLLM-CLï¼Œä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ¶µç›–é¢†åŸŸå’Œèƒ½åŠ›çš„æŒç»­å­¦ä¹ ï¼Œå…¶ä¸­å‰è€…å…³æ³¨åœ¨ä¸æ–­æ¼”å˜çš„ä¸»æµé¢†åŸŸä¸­è¿›è¡Œç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆIIDï¼‰è¯„ä¼°ï¼Œè€Œåè€…åˆ™åœ¨å…·æœ‰æ–°æ¨¡å‹èƒ½åŠ›çš„éIIDåœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å‚æ•°éš”ç¦»å’ŒåŸºäºMLLMçš„è·¯ç”±æœºåˆ¶æ¥é˜²æ­¢ç¾éš¾æ€§å¹²æ‰°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»¥æœ€å°çš„é—å¿˜æ•´åˆé¢†åŸŸç‰¹å®šçŸ¥è¯†å’ŒåŠŸèƒ½èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­æŒç»­å­¦ä¹ æ–°çŸ¥è¯†æ—¶çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸æ–­å˜åŒ–çš„çŸ¥è¯†å’ŒæŠ€èƒ½æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆé€‚åº”ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å‚æ•°éš”ç¦»å’ŒåŸºäºMLLMçš„è·¯ç”±æœºåˆ¶æ¥é˜²æ­¢ç¾éš¾æ€§å¹²æ‰°ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œä¿ç•™å·²æœ‰çŸ¥è¯†ï¼Œå‡å°‘é—å¿˜ç°è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¢†åŸŸæŒç»­å­¦ä¹ æ¨¡å—å’Œèƒ½åŠ›æŒç»­å­¦ä¹ æ¨¡å—ã€‚é¢†åŸŸæ¨¡å—ä¸“æ³¨äºåœ¨IIDåœºæ™¯ä¸­è¯„ä¼°æ¨¡å‹ï¼Œè€Œèƒ½åŠ›æ¨¡å—åˆ™åœ¨éIIDåœºæ™¯ä¸­æµ‹è¯•æ–°èƒ½åŠ›çš„æ•´åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†å‚æ•°éš”ç¦»æœºåˆ¶å’ŒMLLMè·¯ç”±æœºåˆ¶ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘çŸ¥è¯†é—å¿˜ï¼ŒåŒæ—¶æå‡æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´çš„å­¦ä¹ ç‡å’Œç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´çš„è¿ç§»èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†å¤šå±‚æ¬¡çš„è·¯ç”±æœºåˆ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMLLM-CLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œé—å¿˜ç‡é™ä½äº†30%ä»¥ä¸Šï¼Œä¸”åœ¨æ–°èƒ½åŠ›çš„æ•´åˆä¸Šè¡¨ç°å‡ºè‰²ï¼Œæå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚è¿™äº›ç»“æœéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰éœ€è¦æŒç»­å­¦ä¹ å’Œé€‚åº”æ–°ä¿¡æ¯çš„åœºæ™¯ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´ä¸ºç²¾å‡†å’Œæ™ºèƒ½çš„æœåŠ¡ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent Multimodal Large Language Models (MLLMs) excel in vision-language understanding but face challenges in adapting to dynamic real-world scenarios that require continuous integration of new knowledge and skills. While continual learning (CL) offers a potential solution, existing benchmarks and methods suffer from critical limitations. In this paper, we introduce MLLM-CL, a novel benchmark encompassing domain and ability continual learning, where the former focuses on independently and identically distributed (IID) evaluation across evolving mainstream domains, whereas the latter evaluates on non-IID scenarios with new model abilities. Methodologically, we propose preventing catastrophic interference through parameter isolation and an MLLM-based routing mechanism. Extensive experiments demonstrate that our approach can integrate domain-specific knowledge and functional abilities with minimal forgetting, significantly outperforming existing methods. Our benchmark and code are available at https://github.com/bjzhb666/MLLM-CL.

