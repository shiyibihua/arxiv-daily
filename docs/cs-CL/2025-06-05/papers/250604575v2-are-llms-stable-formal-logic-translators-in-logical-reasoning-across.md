---
layout: default
title: Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?
---

# Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04575" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04575v2</a>
  <a href="https://arxiv.org/pdf/2506.04575.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04575v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04575v2', 'Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qingchuan Li, Jiatong Li, Zirui Liu, Mingyue Cheng, Yuting Zeng, Qi Liu, Tongxuan Liu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-10-16)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/wufeiwuwoshihua/LinguDiver)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSoLTå’ŒMenTaLä»¥è§£å†³LLMé€»è¾‘æ¨ç†ä¸­çš„ç¬¦å·ä¸ä¸€è‡´é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é€»è¾‘æ¨ç†` `ç¬¦å·ä¸€è‡´æ€§` `è¯­è¨€å¤šæ ·æ€§` `æ•°æ®é›†é‡å†™` `æ¨ç†å‡†ç¡®æ€§` `æ¦‚å¿µæ˜ å°„` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMç¿»è¯‘æ–¹æ³•åœ¨å¤„ç†ä¸åŒè¯­è¨€å½¢å¼æ—¶ï¼Œå¸¸å¸¸æ— æ³•ä¿æŒç¬¦å·çš„ä¸€è‡´æ€§ï¼Œå¯¼è‡´é€»è¾‘æ¨ç†é”™è¯¯ã€‚
2. æœ¬æ–‡æå‡ºSoLTåŸºå‡†ä»¥ä¸°å¯Œæ•°æ®é›†çš„è¯­è¨€å¤šæ ·æ€§ï¼Œå¹¶æå‡ºMenTaLæ–¹æ³•æ¥å»ºç«‹æ¦‚å¿µä¸ç¬¦å·ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨è¯­è¨€å˜å¼‚ä¸‹çš„æ¨ç†å‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ï¼Œè€ŒMenTaLæ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ¨¡å‹åœ¨å¤šæ ·è¾“å…¥ä¸‹çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€»è¾‘æ¨ç†ä¸­çš„åº”ç”¨æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œç°æœ‰çš„LLMç¿»è¯‘æ–¹æ³•åœ¨å¤„ç†ä¸åŒè¯­è¨€å½¢å¼æ—¶å¸¸å¸¸æ— æ³•ç”Ÿæˆä¸€è‡´çš„ç¬¦å·è¡¨ç¤ºï¼Œå¯¼è‡´é€»è¾‘è¿è´¯æ€§ç ´è£‚å’Œæ±‚è§£å™¨é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SoLTåŸºå‡†ï¼Œç³»ç»Ÿæ€§åœ°å°†æ¨ç†æ•°æ®é›†é‡å†™ä¸ºå¤šæ ·åŒ–ä½†é€»è¾‘ç­‰ä»·çš„å½¢å¼ã€‚æ­¤å¤–ï¼Œæå‡ºçš„MenTaLæ–¹æ³•é€šè¿‡å¼•å¯¼æ¨¡å‹å»ºç«‹æ¦‚å¿µ-ç¬¦å·æ˜ å°„è¡¨ï¼Œä¿æŒç¬¦å·çš„ä¸€è‡´æ€§ï¼Œå‡è½»ç¬¦å·æ¼‚ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨è¯­è¨€å˜å¼‚ä¸‹ç¡®å®å­˜åœ¨ç¬¦å·æ˜ å°„ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œè€Œåº”ç”¨MenTaLèƒ½æ˜¾è‘—æå‡æ¨ç†å‡†ç¡®æ€§ã€‚æ•´ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶ä¸ºåœ¨å¤šæ ·åŒ–çš„çœŸå®åœºæ™¯ä¸­å®ç°æ›´å¯é çš„é€»è¾‘æ¨ç†æä¾›äº†é‡è¦çš„æ­¥éª¤ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMåœ¨é€»è¾‘æ¨ç†ä¸­å› è¯­è¨€å¤šæ ·æ€§å¯¼è‡´çš„ç¬¦å·ä¸ä¸€è‡´é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹ä¸åŒè¯­è¨€å½¢å¼æ—¶ï¼Œå¸¸å¸¸æ— æ³•ç”Ÿæˆä¸€è‡´çš„ç¬¦å·è¡¨ç¤ºï¼Œè¿›è€Œå½±å“é€»è¾‘æ¨ç†çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„SoLTåŸºå‡†é€šè¿‡å°†æ¨ç†æ•°æ®é›†é‡å†™ä¸ºå¤šæ ·åŒ–ä½†é€»è¾‘ç­‰ä»·çš„å½¢å¼ï¼Œæ¥å¢å¼ºæ•°æ®é›†çš„è¯­è¨€å¤šæ ·æ€§ã€‚åŒæ—¶ï¼ŒMenTaLæ–¹æ³•é€šè¿‡å»ºç«‹æ¦‚å¿µä¸ç¬¦å·çš„æ˜ å°„å…³ç³»ï¼Œç¡®ä¿ç¬¦å·çš„ä¸€è‡´æ€§ï¼Œå‡å°‘ç¬¦å·æ¼‚ç§»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šSoLTåŸºå‡†å’ŒMenTaLæ–¹æ³•ã€‚SoLTè´Ÿè´£ç”Ÿæˆå¤šæ ·åŒ–çš„é€»è¾‘æ¨ç†æ•°æ®é›†ï¼Œè€ŒMenTaLåˆ™åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­å¼•å¯¼æ¨¡å‹å»ºç«‹ç¬¦å·æ˜ å°„è¡¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†SoLTåŸºå‡†å’ŒMenTaLæ–¹æ³•ï¼Œå‰è€…ç³»ç»Ÿæ€§åœ°å¤„ç†è¯­è¨€å¤šæ ·æ€§é—®é¢˜ï¼Œåè€…é€šè¿‡æ¦‚å¿µ-ç¬¦å·æ˜ å°„ä¿æŒç¬¦å·ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡äº†LLMçš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MenTaLæ–¹æ³•ä¸­ï¼Œè®¾è®¡äº†æ¦‚å¿µ-ç¬¦å·æ˜ å°„è¡¨ï¼Œå¹¶é€šè¿‡æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç¬¦å·çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç»“æ„ä¸Šå¯èƒ½é‡‡ç”¨äº†ç‰¹å®šçš„ç¥ç»ç½‘ç»œæ¶æ„ä»¥æ”¯æŒæ˜ å°„å…³ç³»çš„å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨é¢å¯¹è¯­è¨€å˜å¼‚æ—¶ï¼Œæ¨ç†å‡†ç¡®æ€§ä¸‹é™å¹…åº¦å¯è¾¾æ˜¾è‘—æ°´å¹³ï¼Œè€Œåº”ç”¨MenTaLæ–¹æ³•åï¼Œæ¨¡å‹åœ¨å¤šæ ·è¾“å…¥ä¸‹çš„è¡¨ç°æå‡æ˜æ˜¾ï¼Œå±•ç¤ºäº†ç¨³å®šçš„æ€§èƒ½æ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨æ¨ç†ç­‰ã€‚é€šè¿‡æå‡LLMåœ¨å¤šæ ·åŒ–æ–‡æœ¬ä¸­çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒå¤æ‚çš„å†³ç­–åˆ¶å®šå’Œä¿¡æ¯æ£€ç´¢ä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Logical reasoning with large language models (LLMs) has received growing attention. One mainstream approach translates natural language into formal logic and then applies symbolic solvers for deduction. While effective in many tasks, these LLM-based translators often fail to generate consistent symbolic representations when the same concept appears in different linguistic forms. Such inconsistencies break logical coherence and lead to solver errors. However, most existing benchmarks lack this type of linguistic variation, which frequently occurs in real-world text, leaving the problem underexplored. To address this gap, we present SoLT, a benchmark that systematically rewrites reasoning datasets into diverse yet logically equivalent forms across multiple levels. Beyond evaluation, SoLT also provides a general method to enrich any dataset with linguistic diversity while preserving both meaning and logic. To further enhance the stability of LLM-based reasoning, we propose MenTaL, which explicitly guides models to build a concept-symbol mapping table during translation. By linking equivalent expressions to shared symbols, MenTaL maintains consistency and mitigates symbol drift. Experiments on SoLT demonstrate that LLMs indeed suffer from inconsistent symbol mapping under linguistic variation, leading to significant drops in reasoning accuracy. Meanwhile, applying MenTaL brings clear and stable performance improvements across diverse inputs. Overall, our findings reveal that overlooking linguistic diversity hides key weaknesses in LLM-based translators, and our work offers a step toward more reliable logical reasoning in varied real-world scenarios. Our code is available at https://github.com/wufeiwuwoshihua/LinguDiver.

