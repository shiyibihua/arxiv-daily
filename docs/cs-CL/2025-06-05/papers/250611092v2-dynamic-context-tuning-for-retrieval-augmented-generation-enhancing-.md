---
layout: default
title: Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation
---

# Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11092" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11092v2</a>
  <a href="https://arxiv.org/pdf/2506.11092.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11092v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11092v2', 'Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jubin Abhishek Soni, Amit Anand, Rajesh Kumar Pandey, Aniket Abhishek Soni

**åˆ†ç±»**: cs.CL, cs.AI, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-07-19)

**å¤‡æ³¨**: We are withdrawing the submission in order to thoroughly revise the work

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€ä¸Šä¸‹æ–‡è°ƒä¼˜ä»¥è§£å†³å¤šè½®å¯¹è¯å’Œå·¥å…·é€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨æ€ä¸Šä¸‹æ–‡è°ƒä¼˜` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¤šè½®å¯¹è¯` `å·¥å…·é€‚åº”` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŒ»ç–—åŠ©æ‰‹` `æ™ºèƒ½å®¶å±…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RAGç³»ç»Ÿé€šå¸¸åªèƒ½å¤„ç†é™æ€çš„å•è½®äº¤äº’ï¼Œæ— æ³•é€‚åº”ç”¨æˆ·æ„å›¾å’Œå·¥å…·ç¯å¢ƒçš„åŠ¨æ€å˜åŒ–ã€‚
2. æœ¬æ–‡æå‡ºåŠ¨æ€ä¸Šä¸‹æ–‡è°ƒä¼˜ï¼ˆDCTï¼‰æ¡†æ¶ï¼Œé€šè¿‡é›†æˆä¸Šä¸‹æ–‡ç¼“å­˜å’ŒåŠ¨æ€å·¥å…·é€‰æ‹©ï¼Œæ”¯æŒå¤šè½®å¯¹è¯ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDCTåœ¨è®¡åˆ’å‡†ç¡®æ€§ä¸Šæé«˜äº†14%ï¼Œå¹»è§‰å‡å°‘äº†37%ï¼Œä¸”æˆæœ¬æ˜¾è‘—ä½äºGPT-4ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œé€šè¿‡å°†å…¶è¾“å‡ºä¸å¤–éƒ¨å·¥å…·å’ŒçŸ¥è¯†æºç»“åˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„RAGç³»ç»Ÿé€šå¸¸å±€é™äºé™æ€çš„å•è½®äº¤äº’ï¼Œæ— æ³•é€‚åº”åŒ»ç–—å’Œæ™ºèƒ½å®¶å±…ç­‰åŠ¨æ€é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§æ¡†æ¶â€”â€”åŠ¨æ€ä¸Šä¸‹æ–‡è°ƒä¼˜ï¼ˆDCTï¼‰ï¼Œæ”¯æŒå¤šè½®å¯¹è¯å’Œä¸æ–­å˜åŒ–çš„å·¥å…·ç¯å¢ƒï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚DCTé›†æˆäº†åŸºäºæ³¨æ„åŠ›çš„ä¸Šä¸‹æ–‡ç¼“å­˜ã€åŸºäºLoRAçš„æ£€ç´¢å’Œé«˜æ•ˆçš„ä¸Šä¸‹æ–‡å‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDCTåœ¨è®¡åˆ’å‡†ç¡®æ€§ä¸Šæé«˜äº†14%ï¼Œå¹¶å‡å°‘äº†37%çš„å¹»è§‰ï¼ŒåŒæ—¶ä»¥æ˜¾è‘—æ›´ä½çš„æˆæœ¬åŒ¹é…GPT-4çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RAGç³»ç»Ÿåœ¨åŠ¨æ€é¢†åŸŸï¼ˆå¦‚åŒ»ç–—å’Œæ™ºèƒ½å®¶å±…ï¼‰ä¸­æ— æ³•é€‚åº”ç”¨æˆ·æ„å›¾å’Œå·¥å…·ç¯å¢ƒå˜åŒ–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å±€é™äºé™æ€å•è½®äº¤äº’ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å¤šè½®å¯¹è¯å’Œå·¥å…·é€‚åº”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŠ¨æ€ä¸Šä¸‹æ–‡è°ƒä¼˜ï¼ˆDCTï¼‰æ¡†æ¶ï¼Œé€šè¿‡é›†æˆæ³¨æ„åŠ›æœºåˆ¶çš„ä¸Šä¸‹æ–‡ç¼“å­˜å’ŒåŸºäºLoRAçš„åŠ¨æ€å·¥å…·é€‰æ‹©ï¼Œæ”¯æŒå¤šè½®å¯¹è¯è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDCTæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼š1) ä¸Šä¸‹æ–‡ç¼“å­˜ï¼Œè·Ÿè¸ªç›¸å…³çš„å†å²ä¿¡æ¯ï¼›2) åŠ¨æ€å·¥å…·é€‰æ‹©ï¼ŒåŸºäºLoRAæŠ€æœ¯é€‰æ‹©é¢†åŸŸç‰¹å®šå·¥å…·ï¼›3) ä¸Šä¸‹æ–‡å‹ç¼©ï¼Œç¡®ä¿è¾“å…¥åœ¨LLMçš„ä¸Šä¸‹æ–‡é™åˆ¶å†…ã€‚

**å…³é”®åˆ›æ–°**ï¼šDCTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŠ¨æ€é€‚åº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ”¯æŒå¤šè½®å¯¹è¯å’Œæ–°å·¥å…·çš„å¼•å…¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„é™æ€ç‰¹æ€§å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šDCTé‡‡ç”¨äº†åŸºäºæ³¨æ„åŠ›çš„ä¸Šä¸‹æ–‡ç¼“å­˜æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå­˜å‚¨å’Œæ£€ç´¢å†å²ä¿¡æ¯ï¼ŒåŒæ—¶é€šè¿‡LoRAæŠ€æœ¯å®ç°é«˜æ•ˆçš„å·¥å…·é€‰æ‹©ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDCTåœ¨è®¡åˆ’å‡†ç¡®æ€§ä¸Šæé«˜äº†14%ï¼Œå¹»è§‰å‡å°‘äº†37%ã€‚ä¸GPT-4ç›¸æ¯”ï¼ŒDCTåœ¨æ€§èƒ½ä¸Šç›¸å½“ï¼Œä½†æˆæœ¬æ˜¾è‘—é™ä½ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—åŠ©æ‰‹ã€æ™ºèƒ½å®¶å±…ç®¡ç†å’Œå®¢æˆ·æœåŠ¡ç­‰åŠ¨æ€ç¯å¢ƒã€‚DCTæ¡†æ¶çš„çµæ´»æ€§å’Œé€‚åº”æ€§ä½¿å…¶èƒ½å¤Ÿåœ¨ä¸æ–­å˜åŒ–çš„ç”¨æˆ·éœ€æ±‚å’Œå·¥å…·ç¯å¢ƒä¸­æä¾›é«˜æ•ˆçš„æ”¯æŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) has significantly advanced large language models (LLMs) by grounding their outputs in external tools and knowledge sources. However, existing RAG systems are typically constrained to static, single-turn interactions with fixed toolsets, making them ill-suited for dynamic domains such as healthcare and smart homes, where user intent, available tools, and contextual factors evolve over time. We present Dynamic Context Tuning (DCT), a lightweight framework that extends RAG to support multi-turn dialogue and evolving tool environments without requiring retraining. DCT integrates an attention-based context cache to track relevant past information, LoRA-based retrieval to dynamically select domain-specific tools, and efficient context compression to maintain inputs within LLM context limits. Experiments on both synthetic and real-world benchmarks show that DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while matching GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to previously unseen tools, enabling scalable and adaptable AI assistants across a wide range of dynamic environments.

