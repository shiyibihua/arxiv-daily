---
layout: default
title: Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets
---

# Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05346" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05346v1</a>
  <a href="https://arxiv.org/pdf/2506.05346.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05346v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05346v1', 'Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lei Hsiung, Tianyu Pang, Yung-Chen Tang, Linyue Song, Tsung-Yi Ho, Pin-Yu Chen, Yaoqing Yang

**åˆ†ç±»**: cs.CR, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: Project Page: https://hsiung.cc/llm-similarity-risk/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡ç›¸ä¼¼æ€§åˆ†ææå‡ºæ–°æ–¹æ³•ä»¥å¢å¼ºLLMå®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å®‰å…¨å¯¹é½` `å¾®è°ƒ` `æ•°æ®é›†è®¾è®¡` `ç›¸ä¼¼æ€§åˆ†æ` `æ¨¡å‹å®‰å…¨æ€§` `æœ‰å®³æ€§è¯„åˆ†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å®‰å…¨é˜²æŠ¤æªæ–½åœ¨å¾®è°ƒåå®¹æ˜“è¢«æ”»å‡»ï¼Œç¼ºä¹æœ‰æ•ˆçš„é¢„é˜²æœºåˆ¶ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡åˆ†æä¸Šæ¸¸å¯¹é½æ•°æ®é›†ä¸ä¸‹æ¸¸å¾®è°ƒä»»åŠ¡çš„ç›¸ä¼¼æ€§æ¥å¢å¼ºå®‰å…¨é˜²æŠ¤ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½ç›¸ä¼¼æ€§æ•°æ®é›†èƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œé™ä½æœ‰å®³æ€§è¯„åˆ†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•æ­ç¤ºäº†å…¶åœ¨å®‰å…¨å¯¹é½æ–¹é¢çš„è„†å¼±æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä¸‹æ¸¸å¾®è°ƒè¿‡ç¨‹ä¸­ã€‚ç°æœ‰çš„ç¼“è§£ç­–ç•¥ä¸»è¦é›†ä¸­åœ¨äº‹ååº”å¯¹å®‰å…¨é˜²æŠ¤æªæ–½è¢«ç ´åçš„äº‹ä»¶ï¼Œæˆ–è€…åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å»é™¤æœ‰å®³æ¢¯åº¦ï¼Œæˆ–æŒç»­å¼ºåŒ–å®‰å…¨å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½è§†äº†ä¸€ä¸ªå…³é”®çš„ä¸Šæ¸¸å› ç´ ï¼šåŸå§‹å®‰å…¨å¯¹é½æ•°æ®çš„ä½œç”¨ã€‚æœ¬æ–‡é€šè¿‡ç ”ç©¶ä¸Šæ¸¸å¯¹é½æ•°æ®é›†ä¸ä¸‹æ¸¸å¾®è°ƒä»»åŠ¡ä¹‹é—´çš„è¡¨ç¤ºç›¸ä¼¼æ€§ï¼Œæ¢è®¨äº†å®‰å…¨é˜²æŠ¤æªæ–½çš„é€€åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ•°æ®é›†ä¹‹é—´çš„é«˜ç›¸ä¼¼æ€§æ˜¾è‘—å‰Šå¼±äº†å®‰å…¨é˜²æŠ¤ï¼Œä½¿æ¨¡å‹æ›´æ˜“å—åˆ°æ”»å‡»ã€‚ç›¸åï¼Œä½ç›¸ä¼¼æ€§åˆ™ä½¿æ¨¡å‹æ›´åŠ ç¨³å¥ï¼Œé™ä½æœ‰å®³æ€§è¯„åˆ†é«˜è¾¾10.33%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ä¸Šæ¸¸æ•°æ®é›†è®¾è®¡åœ¨æ„å»ºæŒä¹…å®‰å…¨é˜²æŠ¤å’Œå‡å°‘ç°å®ä¸–ç•Œæ”»å‡»è„†å¼±æ€§ä¸­çš„é‡è¦æ€§ï¼Œä¸ºå¾®è°ƒæœåŠ¡æä¾›å•†æä¾›äº†å¯è¡Œçš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒåå®‰å…¨é˜²æŠ¤æªæ–½çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºäº‹åå¤„ç†ï¼Œæœªèƒ½æœ‰æ•ˆé¢„é˜²æ½œåœ¨æ”»å‡»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†æä¸Šæ¸¸å¯¹é½æ•°æ®é›†ä¸ä¸‹æ¸¸å¾®è°ƒä»»åŠ¡ä¹‹é—´çš„è¡¨ç¤ºç›¸ä¼¼æ€§ï¼Œå‘ç°é«˜ç›¸ä¼¼æ€§ä¼šå‰Šå¼±æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œä»è€Œæå‡ºä¼˜åŒ–æ•°æ®é›†è®¾è®¡çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå»ºç«‹äº†å¯¹é½æ•°æ®é›†ä¸å¾®è°ƒä»»åŠ¡çš„ç›¸ä¼¼æ€§åº¦é‡æ¡†æ¶ï¼Œç„¶åé€šè¿‡å®éªŒéªŒè¯ä¸åŒç›¸ä¼¼æ€§æ°´å¹³å¯¹æ¨¡å‹å®‰å…¨æ€§çš„å½±å“ï¼Œæœ€åæå‡ºæ”¹è¿›å»ºè®®ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†æ•°æ®é›†ç›¸ä¼¼æ€§ä¸æ¨¡å‹å®‰å…¨æ€§å…³è”èµ·æ¥ï¼Œæ­ç¤ºäº†ä¸Šæ¸¸æ•°æ®é›†è®¾è®¡çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§ç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•ï¼Œè®¾è®¡äº†ä¸åŒçš„å¯¹é½æ•°æ®é›†å’Œå¾®è°ƒä»»åŠ¡ç»„åˆï¼Œä»¥è¯„ä¼°å…¶å¯¹æ¨¡å‹å®‰å…¨æ€§çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“ä¸Šæ¸¸å¯¹é½æ•°æ®é›†ä¸ä¸‹æ¸¸å¾®è°ƒä»»åŠ¡çš„ç›¸ä¼¼æ€§è¾ƒä½æ—¶ï¼Œæ¨¡å‹çš„æœ‰å®³æ€§è¯„åˆ†é™ä½äº†é«˜è¾¾10.33%ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æ•°æ®é›†è®¾è®¡åœ¨æå‡æ¨¡å‹å®‰å…¨æ€§æ–¹é¢çš„é‡è¦æ€§ï¼Œä¸ºå¾®è°ƒæœåŠ¡æä¾›å•†æä¾›äº†å®ç”¨çš„æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å®¢æœç³»ç»Ÿå’Œè‡ªåŠ¨å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®é›†è®¾è®¡ï¼Œå¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œé™ä½è¢«æ”»å‡»çš„é£é™©ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­ä¿æŠ¤ç”¨æˆ·å’Œç³»ç»Ÿçš„å®‰å…¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in large language models (LLMs) have underscored their vulnerability to safety alignment jailbreaks, particularly when subjected to downstream fine-tuning. However, existing mitigation strategies primarily focus on reactively addressing jailbreak incidents after safety guardrails have been compromised, removing harmful gradients during fine-tuning, or continuously reinforcing safety alignment throughout fine-tuning. As such, they tend to overlook a critical upstream factor: the role of the original safety-alignment data. This paper therefore investigates the degradation of safety guardrails through the lens of representation similarity between upstream alignment datasets and downstream fine-tuning tasks. Our experiments demonstrate that high similarity between these datasets significantly weakens safety guardrails, making models more susceptible to jailbreaks. Conversely, low similarity between these two types of datasets yields substantially more robust models and thus reduces harmfulness score by up to 10.33%. By highlighting the importance of upstream dataset design in the building of durable safety guardrails and reducing real-world vulnerability to jailbreak attacks, these findings offer actionable insights for fine-tuning service providers.

