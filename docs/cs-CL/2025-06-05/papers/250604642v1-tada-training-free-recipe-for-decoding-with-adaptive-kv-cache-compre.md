---
layout: default
title: TaDA: Training-free recipe for Decoding with Adaptive KV Cache Compression and Mean-centering
---

# TaDA: Training-free recipe for Decoding with Adaptive KV Cache Compression and Mean-centering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04642" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04642v1</a>
  <a href="https://arxiv.org/pdf/2506.04642.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04642v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04642v1', 'TaDA: Training-free recipe for Decoding with Adaptive KV Cache Compression and Mean-centering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vinay Joshi, Pratik Prabhanjan Brahma, Zicheng Liu, Emad Barsoum

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: ACL-2025 industry-track accepted

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTaDAä»¥è§£å†³KVç¼“å­˜å‹ç¼©ä¸­çš„ç¨€ç–å¼‚å¸¸å¤„ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…³é”®å€¼ç¼“å­˜` `é‡åŒ–å‹ç¼©` `å‡å€¼ä¸­å¿ƒåŒ–` `å˜æ¢å™¨æ¨¡å‹` `å†…å­˜ä¼˜åŒ–` `æ·±åº¦å­¦ä¹ ` `è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰KVç¼“å­˜å‹ç¼©æ–¹æ³•åœ¨å¤„ç†ç¨€ç–å’Œä¸è¿ç»­å¼‚å¸¸å€¼æ—¶å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´å†…å­˜éœ€æ±‚è¿‡é«˜ã€‚
2. TaDAé€šè¿‡å‡å€¼ä¸­å¿ƒåŒ–å’Œé€‚åº”æ€§é‡åŒ–ç²¾åº¦ï¼Œæ¶ˆé™¤äº†å¯¹å¼‚å¸¸å€¼çš„å•ç‹¬å¤„ç†ï¼Œç®€åŒ–äº†KVç¼“å­˜çš„å‹ç¼©è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTaDAåœ¨å¤šä¸ªæ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ï¼ŒåŒæ—¶å°†å†…å­˜å ç”¨é™ä½è‡³27%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å˜æ¢å™¨æ¨¡å‹ä¸­ï¼Œå…³é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜æ˜¯é«˜æ•ˆè§£ç çš„å…³é”®ç»„ä»¶ï¼Œä½†å…¶å†…å­˜éœ€æ±‚éšç€åºåˆ—é•¿åº¦çš„å¢åŠ è€Œæ€¥å‰§ä¸Šå‡ï¼Œç»™å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯æ‰©å±•éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å°½ç®¡å·²æœ‰å¤šç§KVç¼“å­˜å‹ç¼©æ–¹æ³•è¢«æå‡ºï¼Œä½†å¤§å¤šæ•°ä»éœ€å•ç‹¬å¤„ç†ç¨€ç–å’Œä¸è¿ç»­çš„å¼‚å¸¸å€¼ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†TaDAï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„KVç¼“å­˜å‹ç¼©æ–¹æ¡ˆï¼Œé€šè¿‡é€‚åº”å„å±‚çš„è¯¯å·®æ•æ„Ÿæ€§æ¥è°ƒæ•´é‡åŒ–ç²¾åº¦ï¼Œå¹¶é‡‡ç”¨å‡å€¼ä¸­å¿ƒåŒ–æ–¹æ³•æ¶ˆé™¤å¯¹å¼‚å¸¸å€¼çš„å•ç‹¬å¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼ŒåŒæ—¶å°†KVç¼“å­˜çš„å†…å­˜å ç”¨é™ä½è‡³åŸ16ä½åŸºçº¿çš„27%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å˜æ¢å™¨æ¨¡å‹ä¸­KVç¼“å­˜çš„å†…å­˜éœ€æ±‚é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç¨€ç–å’Œä¸è¿ç»­å¼‚å¸¸å€¼æ—¶æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´å†…å­˜å ç”¨è¿‡é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTaDAæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„KVç¼“å­˜å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡å‡å€¼ä¸­å¿ƒåŒ–æ¶ˆé™¤å¯¹å¼‚å¸¸å€¼çš„å•ç‹¬å¤„ç†ï¼Œå¹¶æ ¹æ®å„å±‚çš„è¯¯å·®æ•æ„Ÿæ€§è‡ªé€‚åº”è°ƒæ•´é‡åŒ–ç²¾åº¦ï¼Œä»è€Œæé«˜å‹ç¼©æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå‡å€¼ä¸­å¿ƒåŒ–æ¨¡å—å’Œè‡ªé€‚åº”é‡åŒ–æ¨¡å—ã€‚å‡å€¼ä¸­å¿ƒåŒ–æ¨¡å—è´Ÿè´£å¯¹KVç¼“å­˜è¿›è¡Œé¢„å¤„ç†ï¼Œè€Œè‡ªé€‚åº”é‡åŒ–æ¨¡å—åˆ™æ ¹æ®è¯¯å·®æ•æ„Ÿæ€§åŠ¨æ€è°ƒæ•´é‡åŒ–ç²¾åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šTaDAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è®­ç»ƒè‡ªç”±æ€§å’Œå¯¹å¼‚å¸¸å€¼å¤„ç†çš„ç®€åŒ–ï¼Œé¿å…äº†ä¼ ç»Ÿé‡åŒ–æ–¹æ³•ä¸­å¯¹ç¨€ç–å’Œä¸è¿ç»­å¼‚å¸¸å€¼çš„å¤æ‚ç®¡ç†ï¼Œæ˜¾è‘—æé«˜äº†å‹ç¼©æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒTaDAé‡‡ç”¨äº†å‡å€¼ä¸­å¿ƒåŒ–æŠ€æœ¯æ¥æ¶ˆé™¤å¼‚å¸¸å€¼çš„å½±å“ï¼Œå¹¶é€šè¿‡åŠ¨æ€è°ƒæ•´é‡åŒ–ç²¾åº¦æ¥é€‚åº”ä¸åŒå±‚çš„è¯¯å·®æ•æ„Ÿæ€§ï¼Œç¡®ä¿äº†å‹ç¼©åçš„KVç¼“å­˜åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTaDAå°†KVç¼“å­˜çš„å†…å­˜å ç”¨é™ä½è‡³åŸ16ä½åŸºçº¿çš„27%ï¼ŒåŒæ—¶åœ¨å¤šä¸ªæ¨¡å‹ä¸Šä¿æŒäº†ç›¸å½“çš„å‡†ç¡®æ€§ã€‚è¿™ä¸€æ˜¾è‘—çš„å†…å­˜å‹ç¼©ä¸æ€§èƒ½ä¿æŒçš„å¹³è¡¡ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯æ‰©å±•æ€§æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œé•¿æ–‡æœ¬ç”Ÿæˆç­‰åœºæ™¯ã€‚é€šè¿‡é™ä½KVç¼“å­˜çš„å†…å­˜å ç”¨ï¼ŒTaDAèƒ½å¤Ÿæ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡çš„æ¨ç†ï¼Œæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The key-value (KV) cache in transformer models is a critical component for efficient decoding or inference, yet its memory demands scale poorly with sequence length, posing a major challenge for scalable deployment of large language models. Among several approaches to KV cache compression, quantization of key and value activations has been widely explored. Most KV cache quantization methods still need to manage sparse and noncontiguous outliers separately. To address this, we introduce TaDA, a training-free recipe for KV cache compression with quantization precision that adapts to error sensitivity across layers and a mean centering to eliminate separate outlier handling. Our approach yields substantial accuracy improvements for multiple models supporting various context lengths. Moreover, our approach does not need to separately manage outlier elements -- a persistent hurdle in most traditional quantization methods. Experiments on standard benchmarks demonstrate that our technique reduces KV cache memory footprint to 27% of the original 16-bit baseline while achieving comparable accuracy. Our method paves the way for scalable and high-performance reasoning in language models by potentially enabling inference for longer context length models, reasoning models, and longer chain of thoughts.

