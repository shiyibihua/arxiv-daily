---
layout: default
title: Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis
---

# Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00038" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00038v1</a>
  <a href="https://arxiv.org/pdf/2509.00038.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00038v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00038v1', 'Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Teo Susnjak

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§ç»“æ„åŒ–æ¡†æ¶ä»¥æå‡ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°çš„å¯é æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°` `å¤§å‹è¯­è¨€æ¨¡å‹` `æç¤ºä¼˜åŒ–` `è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹` `å¯é‡å¤æ€§` `ç§‘å­¦ç ”ç©¶` `é€æ˜æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°æ–¹æ³•ä¾èµ–æ‰‹åŠ¨æç¤ºï¼Œå¯¼è‡´å¯é æ€§å’Œå¯é‡å¤æ€§ä¸è¶³ï¼Œå½±å“ç§‘å­¦ä¿¡å¿ƒã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„é¢†åŸŸç‰¹å®šæ¡†æ¶ï¼Œç»“åˆä»»åŠ¡å£°æ˜ã€æµ‹è¯•å¥—ä»¶å’Œè‡ªåŠ¨æç¤ºè°ƒä¼˜ï¼Œæå‡SLRå·¥ä½œæµç¨‹çš„å¯é æ€§ã€‚
3. é€šè¿‡å…·ä½“çš„è“å›¾å’Œä»£ç ç¤ºä¾‹ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ„å»ºç¬¦åˆé€æ˜æ€§åŸåˆ™çš„å¯éªŒè¯LLMç®¡é“ï¼Œä¿ƒè¿›SLRçš„è‡ªåŠ¨åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŠ é€Ÿç³»ç»Ÿæ–‡çŒ®ç»¼è¿°ï¼ˆSLRsï¼‰æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–è„†å¼±çš„æ‰‹åŠ¨æç¤ºï¼Œå½±å“äº†å¯é æ€§å’Œå¯é‡å¤æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶é€‚åº”äº†æœ€è¿‘åœ¨é€šç”¨LLMåº”ç”¨ä¸­å‘å±•çš„å£°æ˜æ€§æç¤ºä¼˜åŒ–æŠ€æœ¯ï¼Œå¹¶å±•ç¤ºå…¶åœ¨SLRè‡ªåŠ¨åŒ–é¢†åŸŸçš„é€‚ç”¨æ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„é¢†åŸŸç‰¹å®šæ¡†æ¶ï¼Œå°†ä»»åŠ¡å£°æ˜ã€æµ‹è¯•å¥—ä»¶å’Œè‡ªåŠ¨æç¤ºè°ƒä¼˜åµŒå…¥å¯é‡å¤çš„SLRå·¥ä½œæµç¨‹ä¸­ã€‚è¿™äº›æ–°å…´æ–¹æ³•è¢«è½¬åŒ–ä¸ºå…·ä½“çš„è“å›¾ï¼Œå¹¶æä¾›äº†å¯è¿è¡Œçš„ä»£ç ç¤ºä¾‹ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿæ„å»ºç¬¦åˆé€æ˜æ€§å’Œä¸¥è°¨æ€§åŸåˆ™çš„å¯éªŒè¯LLMç®¡é“ã€‚è¿™æ˜¯å¯¹SLRç®¡é“è¿›è¡Œçš„æ–°é¢–åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°ï¼ˆSLRï¼‰æ–¹æ³•ä¸­æ‰‹åŠ¨æç¤ºçš„è„†å¼±æ€§é—®é¢˜ï¼Œè¿™ç§è„†å¼±æ€§å½±å“äº†ç»“æœçš„å¯é æ€§å’Œå¯é‡å¤æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å£°æ˜æ€§æç¤ºä¼˜åŒ–æŠ€æœ¯ï¼Œæ„å»ºä¸€ä¸ªç»“æ„åŒ–çš„æ¡†æ¶ï¼Œä»¥è‡ªåŠ¨åŒ–å’Œæ ‡å‡†åŒ–SLRè¿‡ç¨‹ï¼Œä»è€Œæé«˜å…¶å¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡å£°æ˜ã€æµ‹è¯•å¥—ä»¶å’Œè‡ªåŠ¨æç¤ºè°ƒä¼˜ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªå¯é‡å¤çš„SLRå·¥ä½œæµç¨‹ã€‚æ¯ä¸ªæ¨¡å—ç›¸äº’é…åˆï¼Œç¡®ä¿ç»“æœçš„é€æ˜æ€§å’Œä¸¥è°¨æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å£°æ˜æ€§æç¤ºä¼˜åŒ–æ–¹æ³•åº”ç”¨äºSLRç®¡é“ï¼Œè¿™æ˜¯å¯¹ç°æœ‰æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å·¥ä½œæµç¨‹çš„å¯é æ€§å’Œå¯é‡å¤æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè®ºæ–‡è¯¦ç»†æè¿°äº†ä»»åŠ¡å£°æ˜çš„æ ¼å¼ã€æµ‹è¯•å¥—ä»¶çš„æ„å»ºæ–¹æ³•ä»¥åŠè‡ªåŠ¨æç¤ºè°ƒä¼˜çš„å…·ä½“ç®—æ³•ï¼Œç¡®ä¿æ¯ä¸ªç¯èŠ‚éƒ½èƒ½æœ‰æ•ˆæ”¯æŒSLRçš„è‡ªåŠ¨åŒ–ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿåœ¨æ–‡ä¸­è¿›è¡Œäº†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¯¥æ¡†æ¶çš„SLRå·¥ä½œæµç¨‹åœ¨å¯é æ€§å’Œå¯é‡å¤æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ‰‹åŠ¨æç¤ºæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°30%ã€‚é€šè¿‡è‡ªåŠ¨åŒ–æç¤ºè°ƒä¼˜ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ›´å¿«é€Ÿåœ°è·å¾—é«˜è´¨é‡çš„æ–‡çŒ®ç»¼è¿°ç»“æœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦ã€ç¤¾ä¼šç§‘å­¦å’Œå…¶ä»–éœ€è¦ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°çš„å­¦æœ¯ç ”ç©¶é¢†åŸŸã€‚é€šè¿‡æä¾›ä¸€ä¸ªå¯é‡å¤çš„å·¥ä½œæµç¨‹ï¼Œç ”ç©¶äººå‘˜èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°è¿›è¡Œæ–‡çŒ®ç»¼è¿°ï¼Œæå‡ç ”ç©¶çš„é€æ˜æ€§å’Œå¯ä¿¡åº¦ï¼Œæ¨åŠ¨ç§‘å­¦ç ”ç©¶çš„è¿›å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) offer significant potential to accelerate systematic literature reviews (SLRs), yet current approaches often rely on brittle, manually crafted prompts that compromise reliability and reproducibility. This fragility undermines scientific confidence in LLM-assisted evidence synthesis. In response, this work adapts recent advances in declarative prompt optimisation, developed for general-purpose LLM applications, and demonstrates their applicability to the domain of SLR automation. This research proposes a structured, domain-specific framework that embeds task declarations, test suites, and automated prompt tuning into a reproducible SLR workflow. These emerging methods are translated into a concrete blueprint with working code examples, enabling researchers to construct verifiable LLM pipelines that align with established principles of transparency and rigour in evidence synthesis. This is a novel application of such approaches to SLR pipelines.

