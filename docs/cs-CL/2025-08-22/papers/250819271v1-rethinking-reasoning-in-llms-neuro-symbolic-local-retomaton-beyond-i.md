---
layout: default
title: Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT
---

# Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19271" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19271v1</a>
  <a href="https://arxiv.org/pdf/2508.19271.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19271v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19271v1', 'Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rushitha Santhoshi Mamidala, Anshuman Chhabra, Ankur Mali

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå±€éƒ¨åŠ æƒæœ‰é™è‡ªåŠ¨æœºçš„RetoMatonä»¥è§£å†³LLMæ¨ç†ä¸ç¨³å®šé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†æ–¹æ³•` `åŠ æƒæœ‰é™è‡ªåŠ¨æœº` `ç¬¦å·æ¨ç†` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `é“¾å¼æ€ç»´` `é€æ˜æ€§` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†æ–¹æ³•å¦‚CoTå’ŒICLå­˜åœ¨è¾“å‡ºä¸ä¸€è‡´å’Œç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å±€éƒ¨åŠ æƒæœ‰é™è‡ªåŠ¨æœºï¼ˆWFAï¼‰æ›¿ä»£å…¨å±€æ•°æ®å­˜å‚¨ï¼Œå¢å¼ºäº†æ¨ç†çš„ç»“æ„æ€§å’Œå¯é æ€§ï¼Œæä¾›äº†å¯éªŒè¯çš„æ£€ç´¢è¡Œä¸ºã€‚
3. åœ¨å¯¹LLaMA-3.2-1Bå’ŒGemma-3-1B-PTçš„å®éªŒä¸­ï¼Œå±€éƒ¨RetoMatonåœ¨TriviaQAã€GSM8Kå’ŒMMLUä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºæç¤ºçš„æ¨ç†ç­–ç•¥å¦‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†è¿™äº›æ–¹æ³•ä¾èµ–è„†å¼±çš„éšå¼æœºåˆ¶ï¼Œå¯¼è‡´è¾“å‡ºä¸ä¸€è‡´ï¼Œç¼ºä¹ç¨³å®šæ€§å’Œå¯è§£é‡Šæ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„RetoMatonæ¡†æ¶ï¼Œé€šè¿‡ç”¨å±€éƒ¨ã€ä»»åŠ¡è‡ªé€‚åº”çš„åŠ æƒæœ‰é™è‡ªåŠ¨æœºï¼ˆWFAï¼‰æ›¿ä»£å…¨å±€æ•°æ®å­˜å‚¨ï¼Œå¢å¼ºäº†æ¨ç†çš„å¯é æ€§å’Œé€æ˜æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†ç°ä»£LLMsä¸­å¯ä¿¡èµ–çš„ç¬¦å·æ¨ç†çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºäºæç¤ºçš„æ¨ç†æ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸ç¨³å®šæ€§å’Œä¸å¯è§£é‡Šæ€§é—®é¢˜ã€‚è¿™äº›æ–¹æ³•åœ¨ä¸åŒç§å­ã€æ ¼å¼æˆ–è½»å¾®æç¤ºå˜åŒ–ä¸‹ï¼Œè¾“å‡ºç»“æœå¾€å¾€ä¸ä¸€è‡´ï¼Œéš¾ä»¥æ»¡è¶³å¤æ‚ä»»åŠ¡çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„RetoMatonæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å±€éƒ¨ã€ä»»åŠ¡è‡ªé€‚åº”çš„åŠ æƒæœ‰é™è‡ªåŠ¨æœºï¼ˆWFAï¼‰ï¼Œå®ç°æ›´ä¸ºå¯é å’Œé€æ˜çš„æ¨ç†è¿‡ç¨‹ã€‚WFAçš„è®¾è®¡ä½¿å¾—æ£€ç´¢è¿‡ç¨‹å…·æœ‰æ˜ç¡®çš„ç»“æ„ï¼Œé¿å…äº†æç¤ºæ–¹æ³•ä¸­ä¸Šä¸‹æ–‡ä¸è®°å¿†çš„æ··æ·†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå±€éƒ¨WFAæ¨¡å—ï¼Œè¯¥æ¨¡å—ç›´æ¥ä»å¤–éƒ¨é¢†åŸŸè¯­æ–™æ„å»ºï¼Œæ”¯æŒä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ£€ç´¢ã€‚è¯¥æ¡†æ¶çš„ä¸»è¦é˜¶æ®µåŒ…æ‹¬WFAçš„æ„å»ºã€ç¬¦å·è®°å¿†çš„æ£€ç´¢å’Œæ¨ç†è¿‡ç¨‹çš„æ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºç”¨å±€éƒ¨WFAæ›¿ä»£å…¨å±€æ•°æ®å­˜å‚¨ï¼Œæä¾›äº†å¯éªŒè¯çš„æ¨¡å—åŒ–æ£€ç´¢è¡Œä¸ºã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨ç†è¿‡ç¨‹æ›´åŠ ç¨³å®šï¼Œå¹¶ä¸”åœ¨é¢†åŸŸè¿ç§»å’Œäº’æ“ä½œæ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒWFAçš„æƒé‡æ ¹æ®ä»»åŠ¡éœ€æ±‚è¿›è¡Œè°ƒæ•´ï¼Œç¡®ä¿æ£€ç´¢çš„ç›¸å…³æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿä¿æŒä½æ¨ç†å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå±€éƒ¨RetoMatonåœ¨TriviaQAã€GSM8Kå’ŒMMLUä»»åŠ¡ä¸Šå‡æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹å’Œä¼ ç»Ÿæç¤ºæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€åŒ»ç–—å’Œé‡‘èç­‰éœ€è¦é«˜å¯é æ€§æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æä¾›é€æ˜å’Œå¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å†³ç­–ï¼Œå¢å¼ºäººæœºäº¤äº’çš„ä¿¡ä»»åº¦ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç¬¦å·æ¨ç†çš„å¤æ‚ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and In-Context Learning (ICL) have become widely used for eliciting reasoning capabilities in large language models (LLMs). However, these methods rely on fragile, implicit mechanisms often yielding inconsistent outputs across seeds, formats, or minor prompt variations making them fundamentally unreliable for tasks requiring stable, interpretable reasoning. In contrast, automata-based neuro-symbolic frameworks like RetoMaton offer a more structured and trustworthy alternative by grounding retrieval in symbolic memory with deterministic transitions. In this work, we extend RetoMaton by replacing its global datastore with a local, task-adaptive Weighted Finite Automaton (WFA), constructed directly from external domain corpora. This local automaton structure promotes robust, context-aware retrieval while preserving symbolic traceability and low inference overhead. Unlike prompting, which entangles context and memory in opaque ways, our approach leverages the explicit structure of WFAs to provide verifiable and modular retrieval behavior, making it better suited for domain transfer and interoperability. We evaluate this local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT across three reasoning tasks: TriviaQA (reading comprehension), GSM8K (multi-step math), and MMLU (domain knowledge). Compared to the base model and prompting-based methods, augmenting these setups with local RetoMaton consistently improves performance while enabling transparent and reproducible retrieval dynamics. Our results highlight a promising shift toward trustworthy, symbolic reasoning in modern LLMs via lightweight, automaton-guided memory.

