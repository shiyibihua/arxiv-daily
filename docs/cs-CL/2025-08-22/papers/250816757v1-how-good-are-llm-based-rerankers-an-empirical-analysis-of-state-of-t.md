---
layout: default
title: How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models
---

# How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16757" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.16757v1</a>
  <a href="https://arxiv.org/pdf/2508.16757.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16757v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16757v1', 'How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, Adam Jatowt

**åˆ†ç±»**: cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-08-22

**å¤‡æ³¨**: EMNLP Findings 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/DataScienceUIBK/llm-reranking-generalization-study)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿè¯„ä¼°LLMé‡æ’åºæ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡æ’åº` `ä¿¡æ¯æ£€ç´¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€§èƒ½è¯„ä¼°` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡æ’åºæ–¹æ³•åœ¨å¤„ç†æ–°æŸ¥è¯¢æ—¶çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´æ€§èƒ½å·®å¼‚æ˜æ˜¾ã€‚
2. è®ºæ–‡é€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸åŒé‡æ’åºæ¨¡å‹ï¼Œç‰¹åˆ«å…³æ³¨LLMä¸è½»é‡çº§æ¨¡å‹çš„æ¯”è¾ƒï¼Œæ¢ç´¢å…¶æ€§èƒ½å·®å¼‚çš„åŸå› ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMé‡æ’åºå™¨åœ¨ç†Ÿæ‚‰æŸ¥è¯¢ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä½†åœ¨æ–°æŸ¥è¯¢ä¸Šçš„è¡¨ç°ä¸è½»é‡çº§æ¨¡å‹ç›¸å½“ï¼Œä¸”æŸ¥è¯¢çš„æ–°é¢–æ€§æ˜¾è‘—å½±å“é‡æ’åºæ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶å¯¹æœ€å…ˆè¿›çš„é‡æ’åºæ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿå’Œå…¨é¢çš„å®è¯è¯„ä¼°ï¼Œæ¶µç›–äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€è½»é‡çº§ä¸Šä¸‹æ–‡å’Œé›¶æ ·æœ¬æ–¹æ³•åœ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬è¯„ä¼°äº†22ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬40ç§å˜ä½“ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚ç ”ç©¶çš„ä¸»è¦ç›®æ ‡æ˜¯é€šè¿‡æ§åˆ¶å’Œå…¬å¹³çš„æ¯”è¾ƒï¼Œç¡®å®šLLMé‡æ’åºå™¨ä¸è½»é‡çº§æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œå¹¶é˜æ˜è§‚å¯Ÿåˆ°çš„å·®å¼‚çš„æ½œåœ¨åŸå› ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMé‡æ’åºå™¨åœ¨ç†Ÿæ‚‰æŸ¥è¯¢ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä½†å…¶å¯¹æ–°æŸ¥è¯¢çš„æ³›åŒ–èƒ½åŠ›å­˜åœ¨å·®å¼‚ï¼Œè½»é‡çº§æ¨¡å‹åœ¨æ•ˆç‡ä¸Šè¡¨ç°ç›¸å½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰é‡æ’åºæ–¹æ³•åœ¨æ–°æŸ¥è¯¢ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯LLMé‡æ’åºå™¨ä¸è½»é‡çº§æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿè¯„ä¼°22ç§é‡æ’åºæ–¹æ³•ï¼Œåˆ†æä¸åŒæ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯å¯¹æ–°æŸ¥è¯¢çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆå¦‚TREC DL19ã€DL20å’ŒBEIRï¼‰è¿›è¡Œè¯„ä¼°ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ–°æ•°æ®é›†ä»¥æµ‹è¯•æœªè§è¿‡çš„æŸ¥è¯¢ã€‚è¯„ä¼°è¿‡ç¨‹ä¸­æ§åˆ¶äº†è®­ç»ƒæ•°æ®é‡å ã€æ¨¡å‹æ¶æ„å’Œè®¡ç®—æ•ˆç‡ç­‰å› ç´ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†LLMé‡æ’åºå™¨ä¸è½»é‡çº§æ¨¡å‹çš„æ€§èƒ½ï¼Œæ­ç¤ºäº†åœ¨æ–°æŸ¥è¯¢ä¸Šçš„æ³›åŒ–èƒ½åŠ›å·®å¼‚åŠå…¶åŸå› ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§æ¨¡å‹å˜ä½“ï¼Œå¹¶é€šè¿‡å…¬å¹³çš„æ¯”è¾ƒæ–¹æ³•åˆ†æäº†ä¸åŒæ¨¡å‹çš„è®­ç»ƒæ•°æ®é‡å å’Œæ¶æ„å¯¹é‡æ’åºæ€§èƒ½çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMé‡æ’åºå™¨åœ¨ç†Ÿæ‚‰æŸ¥è¯¢ä¸Šçš„æ€§èƒ½ä¼˜äºè½»é‡çº§æ¨¡å‹ï¼Œä½†åœ¨æ–°æŸ¥è¯¢ä¸Šçš„æ³›åŒ–èƒ½åŠ›å­˜åœ¨å·®å¼‚ã€‚å…·ä½“è€Œè¨€ï¼ŒLLMæ¨¡å‹åœ¨ç†Ÿæ‚‰æŸ¥è¯¢ä¸Šçš„æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œè€Œè½»é‡çº§æ¨¡å‹åœ¨æ–°æŸ¥è¯¢ä¸Šçš„æ•ˆç‡è¡¨ç°ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„ç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿã€æœç´¢å¼•æ“ä¼˜åŒ–å’Œæ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜é‡æ’åºæ¨¡å‹åœ¨æ–°æŸ¥è¯¢ä¸Šçš„è¡¨ç°ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·çš„æ£€ç´¢ä½“éªŒå’Œä¿¡æ¯è·å–æ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we present a systematic and comprehensive empirical evaluation of state-of-the-art reranking methods, encompassing large language model (LLM)-based, lightweight contextual, and zero-shot approaches, with respect to their performance in information retrieval tasks. We evaluate in total 22 methods, including 40 variants (depending on used LLM) across several established benchmarks, including TREC DL19, DL20, and BEIR, as well as a novel dataset designed to test queries unseen by pretrained models. Our primary goal is to determine, through controlled and fair comparisons, whether a performance disparity exists between LLM-based rerankers and their lightweight counterparts, particularly on novel queries, and to elucidate the underlying causes of any observed differences. To disentangle confounding factors, we analyze the effects of training data overlap, model architecture, and computational efficiency on reranking performance. Our findings indicate that while LLM-based rerankers demonstrate superior performance on familiar queries, their generalization ability to novel queries varies, with lightweight models offering comparable efficiency. We further identify that the novelty of queries significantly impacts reranking effectiveness, highlighting limitations in existing approaches. https://github.com/DataScienceUIBK/llm-reranking-generalization-study

