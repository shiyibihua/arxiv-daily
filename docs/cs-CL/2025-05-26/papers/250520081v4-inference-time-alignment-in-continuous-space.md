---
layout: default
title: Inference-time Alignment in Continuous Space
---

# Inference-time Alignment in Continuous Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20081" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20081v4</a>
  <a href="https://arxiv.org/pdf/2505.20081.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20081v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20081v4', 'Inference-time Alignment in Continuous Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yige Yuan, Teng Xiao, Li Yunfan, Bingbing Xu, Shuchang Tao, Yunqi Qiu, Huawei Shen, Xueqi Cheng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: Accepted at NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yuanyige/sea)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç®€å•èƒ½é‡é€‚åº”ç®—æ³•ä»¥è§£å†³æ¨ç†æ—¶å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†å¯¹é½` `å¤§å‹è¯­è¨€æ¨¡å‹` `äººç±»åé¦ˆ` `èƒ½é‡é€‚åº”` `æ¢¯åº¦é‡‡æ ·` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç®—æ³•ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¨ç†æ—¶å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åé¦ˆæ—¶ï¼Œé¢ä¸´åŸºç¡€ç­–ç•¥å¼±æˆ–å€™é€‰é›†å°å¯¼è‡´çš„æœ‰æ•ˆæ€§ä¸è¶³é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„ç®€å•èƒ½é‡é€‚åº”ï¼ˆSEAï¼‰ç®—æ³•ï¼Œé€šè¿‡åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¢¯åº¦é‡‡æ ·ï¼Œç›´æ¥è°ƒæ•´åŸºç¡€ç­–ç•¥çš„å“åº”ï¼Œç®€åŒ–äº†å¯¹é½è¿‡ç¨‹ã€‚
3. SEAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒAdvBenchä¸Šç›¸è¾ƒäºç¬¬äºŒä¼˜åŸºçº¿æå‡äº†77.51%ï¼ŒMATHä¸Šæå‡äº†16.36%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ¨ç†æ—¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åé¦ˆå¯¹é½çš„ç ”ç©¶é€æ¸å—åˆ°å…³æ³¨ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºä»åŸºç¡€ç­–ç•¥ç”Ÿæˆå¤šä¸ªå“åº”è¿›è¡Œæœç´¢ï¼Œä½†åœ¨åŸºç¡€ç­–ç•¥è¾ƒå¼±æˆ–å€™é€‰é›†è¾ƒå°æ—¶ï¼Œæ¢ç´¢æœ‰æ•ˆå€™é€‰çš„èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ç®€å•èƒ½é‡é€‚åº”ï¼ˆSEAï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­è¿›è¡ŒåŸºäºæ¢¯åº¦çš„é‡‡æ ·ï¼Œç›´æ¥å°†åŸºç¡€ç­–ç•¥çš„åŸå§‹å“åº”è°ƒæ•´ä¸ºæœ€ä¼˜å“åº”ï¼Œä»è€Œå®ç°ç®€å•æœ‰æ•ˆçš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEAåœ¨AdvBenchä¸Šç›¸è¾ƒäºç¬¬äºŒä¼˜åŸºçº¿æå‡äº†77.51%ï¼Œåœ¨MATHä¸Šæå‡äº†16.36%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨ç†æ—¶å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åé¦ˆå¯¹é½çš„æœ‰æ•ˆæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºç¦»æ•£å“åº”ç©ºé—´çš„æœç´¢ï¼Œéš¾ä»¥åœ¨åŸºç¡€ç­–ç•¥è¾ƒå¼±æˆ–å€™é€‰é›†è¾ƒå°æ—¶æ‰¾åˆ°æœ‰æ•ˆå€™é€‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç®€å•èƒ½é‡é€‚åº”ï¼ˆSEAï¼‰ç®—æ³•ï¼Œåˆ©ç”¨æ¢¯åº¦é‡‡æ ·åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­ç›´æ¥è°ƒæ•´åŸºç¡€ç­–ç•¥çš„å“åº”ï¼Œé¿å…äº†ç¦»æ•£ç©ºé—´ä¸­æ˜‚è´µçš„æœç´¢è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSEAç®—æ³•å°†æ¨ç†è¿‡ç¨‹è§†ä¸ºåœ¨å®šä¹‰ä¸ºæœ€ä¼˜ç­–ç•¥çš„è¿ç»­ç©ºé—´ä¸­çš„èƒ½é‡å‡½æ•°ä¸Šçš„è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¸»è¦åŒ…æ‹¬å“åº”è°ƒæ•´å’Œèƒ½é‡è®¡ç®—ä¸¤ä¸ªæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šSEAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡åœ¨è¿ç»­ç©ºé—´ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œç®€åŒ–äº†å¯¹é½è¿‡ç¨‹ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢å“åº”å€™é€‰ã€‚

**å…³é”®è®¾è®¡**ï¼šSEAç®—æ³•çš„è®¾è®¡åŒ…æ‹¬é€‰æ‹©åˆé€‚çš„èƒ½é‡å‡½æ•°å’Œæ¢¯åº¦è®¡ç®—æ–¹æ³•ï¼Œä»¥ç¡®ä¿åœ¨è°ƒæ•´è¿‡ç¨‹ä¸­ä¿æŒå“åº”çš„æœ‰æ•ˆæ€§å’Œå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SEAç®—æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨AdvBenchä¸Šç›¸è¾ƒäºç¬¬äºŒä¼˜åŸºçº¿æå‡äº†77.51%ï¼Œåœ¨MATHä¸Šæå‡äº†16.36%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSEAåœ¨æ¨ç†æ—¶å¯¹é½ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„å“åº”è´¨é‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜è¯­è¨€æ¨¡å‹çš„å“åº”è´¨é‡å’Œå¯¹é½èƒ½åŠ›ï¼ŒSEAç®—æ³•èƒ½å¤Ÿå¢å¼ºæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œéšç€ç®—æ³•çš„è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯èƒ½ä¼šåœ¨æ›´å¹¿æ³›çš„AIåº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Aligning large language models with human feedback at inference time has received increasing attention due to its flexibility. Existing methods rely on generating multiple responses from the base policy for search using a reward model, which can be considered as searching in a discrete response space. However, these methods struggle to explore informative candidates when the base policy is weak or the candidate set is small, resulting in limited effectiveness. In this paper, to address this problem, we propose Simple Energy Adaptation ($\textbf{SEA}$), a simple yet effective algorithm for inference-time alignment. In contrast to expensive search over the discrete space, SEA directly adapts original responses from the base policy toward the optimal one via gradient-based sampling in continuous latent space. Specifically, SEA formulates inference as an iterative optimization procedure on an energy function over actions in the continuous space defined by the optimal policy, enabling simple and effective alignment. For instance, despite its simplicity, SEA outperforms the second-best baseline with a relative improvement of up to $ \textbf{77.51% }$ on AdvBench and $\textbf{16.36% }$ on MATH. Our code is publicly available at https://github.com/yuanyige/sea

