---
layout: default
title: ConLID: Supervised Contrastive Learning for Low-Resource Language Identification
---

# ConLID: Supervised Contrastive Learning for Low-Resource Language Identification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15304" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15304v1</a>
  <a href="https://arxiv.org/pdf/2506.15304.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15304v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15304v1', 'ConLID: Supervised Contrastive Learning for Low-Resource Language Identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Negar Foroutan, Jakhongir Saydaliev, Ye Eun Kim, Antoine Bosselut

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

**å¤‡æ³¨**: Submitted to EMNLP

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç›‘ç£å¯¹æ¯”å­¦ä¹ ä»¥è§£å†³ä½èµ„æºè¯­è¨€è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¯­è¨€è¯†åˆ«` `ä½èµ„æºè¯­è¨€` `ç›‘ç£å¯¹æ¯”å­¦ä¹ ` `æ·±åº¦å­¦ä¹ ` `å¤šè¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€è¯†åˆ«æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºè®­ç»ƒæ•°æ®çš„å•ä¸€æ€§å’Œç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å­¦ä¹ ä½èµ„æºè¯­è¨€çš„é¢†åŸŸä¸å˜è¡¨ç¤ºï¼Œä»è€Œæé«˜å…¶è¯†åˆ«æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€çš„åŸŸå¤–æ•°æ®ä¸Šæå‡äº†3.2%çš„LIDæ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰æ˜¯ä»ç½‘ç»œçˆ¬è™«ä¸­æ•´ç†å¤šè¯­è¨€å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­æ–™åº“çš„é‡è¦æ­¥éª¤ã€‚å°½ç®¡è®¸å¤šç ”ç©¶é›†ä¸­åœ¨æ”¶é›†å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ä»¥æé«˜æ€§èƒ½ï¼Œä½†ä½èµ„æºè¯­è¨€ï¼ˆé€šå¸¸ä»…é™äºå•ä¸€é¢†åŸŸæ•°æ®ï¼Œå¦‚ã€Šåœ£ç»ã€‹ï¼‰çš„è¡¨ç°ä»ç„¶è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›ç±»åˆ«ä¸å¹³è¡¡å’Œåå·®é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼ˆSCLï¼‰æ–¹æ³•ï¼Œä»¥å­¦ä¹ ä½èµ„æºè¯­è¨€çš„é¢†åŸŸä¸å˜è¡¨ç¤ºã€‚é€šè¿‡å¹¿æ³›çš„åˆ†æï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€çš„åŸŸå¤–æ•°æ®ä¸Šçš„LIDæ€§èƒ½æé«˜äº†3.2%ï¼Œè¯æ˜äº†å…¶åœ¨å¢å¼ºLIDæ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä½èµ„æºè¯­è¨€è¯†åˆ«ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡å’Œåå·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå•ä¸€é¢†åŸŸçš„æ•°æ®ï¼Œå¯¼è‡´ä½èµ„æºè¯­è¨€çš„è¯†åˆ«æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•é€šè¿‡å­¦ä¹ é¢†åŸŸä¸å˜çš„è¡¨ç¤ºï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ä½èµ„æºè¯­è¨€çš„è¯†åˆ«èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸åŒè¯­è¨€ä¹‹é—´çš„å…±æ€§ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–ã€å¯¹æ¯”å­¦ä¹ æ¨¡å—å’Œåˆ†ç±»å™¨ã€‚æ•°æ®é¢„å¤„ç†é˜¶æ®µè´Ÿè´£æ¸…æ´—å’Œå‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œç‰¹å¾æå–æ¨¡å—ç”¨äºæå–è¯­è¨€ç‰¹å¾ï¼Œå¯¹æ¯”å­¦ä¹ æ¨¡å—åˆ™é€šè¿‡ç›‘ç£ä¿¡å·ä¼˜åŒ–æ¨¡å‹ï¼Œæœ€ååˆ†ç±»å™¨ç”¨äºè¿›è¡Œè¯­è¨€è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä½èµ„æºè¯­è¨€çš„è®­ç»ƒä¸­æœ‰æ•ˆå­¦ä¹ åˆ°é¢†åŸŸä¸å˜çš„ç‰¹å¾ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®çš„æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°çš„è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†å¯¹æ¯”æŸå¤±å‡½æ•°ä»¥å¢å¼ºä¸åŒè¯­è¨€æ ·æœ¬ä¹‹é—´çš„åŒºåˆ†åº¦ã€‚åŒæ—¶ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰å¤æ‚çš„è¯­è¨€ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€çš„åŸŸå¤–æ•°æ®ä¸Šæå‡äº†3.2%çš„LIDæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚è¿™ä¸€æå‡è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€å¤„ç†ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œä½èµ„æºè¯­è¨€çš„è‡ªåŠ¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡æé«˜ä½èµ„æºè¯­è¨€çš„è¯†åˆ«æ€§èƒ½ï¼Œå¯ä»¥ä¿ƒè¿›è¿™äº›è¯­è¨€çš„æ•°å­—åŒ–å’Œä¿¡æ¯åŒ–ï¼Œå¢å¼ºå…¶åœ¨å…¨çƒåŒ–èƒŒæ™¯ä¸‹çš„å¯ç”¨æ€§å’Œå½±å“åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Language identification (LID) is a critical step in curating multilingual LLM pretraining corpora from web crawls. While many studies on LID model training focus on collecting diverse training data to improve performance, low-resource languages -- often limited to single-domain data, such as the Bible -- continue to perform poorly. To resolve these class imbalance and bias issues, we propose a novel supervised contrastive learning (SCL) approach to learn domain-invariant representations for low-resource languages. Through an extensive analysis, we show that our approach improves LID performance on out-of-domain data for low-resource languages by 3.2%, demonstrating its effectiveness in enhancing LID models.

