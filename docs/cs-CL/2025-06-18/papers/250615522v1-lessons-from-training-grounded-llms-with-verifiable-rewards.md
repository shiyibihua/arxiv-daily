---
layout: default
title: Lessons from Training Grounded LLMs with Verifiable Rewards
---

# Lessons from Training Grounded LLMs with Verifiable Rewards

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15522" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15522v1</a>
  <a href="https://arxiv.org/pdf/2506.15522.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15522v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15522v1', 'Lessons from Training Grounded LLMs with Verifiable Rewards')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shang Hong Sim, Tej Deep Pala, Vernon Toh, Hai Leong Chieu, Amir Zadeh, Chuan Li, Navonil Majumder, Soujanya Poria

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥æå‡LLMçš„å¯ä¿¡åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¯éªŒè¯å¥–åŠ±` `æ¨ç†å¢å¼º` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ ¹æ®çš„å“åº”æ—¶è¡¨ç°ä¸ä½³ï¼Œå¸¸å¸¸é—æ¼ç­”æ¡ˆæˆ–é”™è¯¯å¼•ç”¨ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå†…éƒ¨æ¨ç†ç»“åˆGRPOæ–¹æ³•æ¥æå‡æ¨¡å‹çš„åŸºç¡€èƒ½åŠ›ï¼Œä¼˜åŒ–ç­”æ¡ˆå’Œå¼•ç”¨è¡Œä¸ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºæ¨ç†çš„æ¨¡å‹åœ¨å¤„ç†æ— æ³•å›ç­”çš„é—®é¢˜å’Œç”Ÿæˆé«˜è´¨é‡å¼•ç”¨æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆæœ‰æ ¹æ®ä¸”å¯ä¿¡çš„å“åº”ä»ç„¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚å°½ç®¡åŸºäºå¼•ç”¨çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•å±•ç°å‡ºæ½œåŠ›ï¼Œä½†æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹åœ¨ç®€å•åœºæ™¯ä¸­å¸¸å¸¸å¤±è´¥ï¼Œè¡¨ç°ä¸ºé—æ¼æ˜ç¡®ç­”æ¡ˆã€é”™è¯¯å¼•ç”¨æˆ–åœ¨æœ‰è¯æ®æ—¶æ‹’ç»å›ç­”ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå†…éƒ¨æ¨ç†æ¥å¢å¼ºLLMsçš„åŸºç¡€ã€‚æˆ‘ä»¬é‡‡ç”¨GRPOï¼ˆGroup Relative Policy Optimizationï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨å¯éªŒè¯çš„åŸºäºç»“æœçš„å¥–åŠ±æ¥è®­ç»ƒæ¨¡å‹ï¼Œç›®æ ‡æ˜¯æé«˜ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€å¼•ç”¨çš„å……åˆ†æ€§å’Œæ‹’ç»çš„è´¨é‡ï¼Œè€Œæ— éœ€é‡‘æ ‡å‡†æ¨ç†è½¨è¿¹æˆ–æ˜‚è´µçš„æ ‡æ³¨ã€‚é€šè¿‡åœ¨ASQAã€QAMPARIã€ELI5å’ŒExpertQAä¸Šçš„å…¨é¢å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¢å¼ºæ¨ç†çš„æ¨¡å‹åœ¨å¤„ç†æ— æ³•å›ç­”çš„æŸ¥è¯¢å’Œç”Ÿæˆè‰¯å¥½å¼•ç”¨çš„å“åº”æ–¹é¢æ˜¾è‘—ä¼˜äºä»…åŸºäºæŒ‡ä»¤çš„å˜ä½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ ¹æ®å’Œå¯ä¿¡å“åº”æ—¶çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨ç®€å•åœºæ™¯ä¸­å¸¸å¸¸å¤±è´¥ï¼Œå¯¼è‡´ç­”æ¡ˆç¼ºå¤±æˆ–å¼•ç”¨é”™è¯¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ å’Œå†…éƒ¨æ¨ç†ï¼Œç»“åˆGRPOæ–¹æ³•ï¼Œåˆ©ç”¨å¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–æ¨¡å‹çš„å›ç­”è´¨é‡ã€å¼•ç”¨å‡†ç¡®æ€§å’Œæ‹’ç»å“åº”çš„åˆç†æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒï¼šç¬¬ä¸€é˜¶æ®µä¼˜åŒ–ç­”æ¡ˆå’Œå¼•ç”¨è¡Œä¸ºï¼Œç¬¬äºŒé˜¶æ®µä¸“æ³¨äºæ‹’ç»å“åº”çš„è´¨é‡ï¼Œä»¥ç¨³å®šå­¦ä¹ ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºä½¿ç”¨å¯éªŒè¯çš„ç»“æœå¯¼å‘å¥–åŠ±æœºåˆ¶ï¼Œé¿å…äº†å¯¹é‡‘æ ‡å‡†æ¨ç†è½¨è¿¹çš„ä¾èµ–ï¼Œæå‡äº†æ¨¡å‹çš„å¯é æ€§å’Œå¯éªŒè¯æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†é’ˆå¯¹ç­”æ¡ˆæ­£ç¡®æ€§ã€å¼•ç”¨å……åˆ†æ€§å’Œæ‹’ç»è´¨é‡çš„æŸå¤±å‡½æ•°ï¼Œé‡‡ç”¨äº†åˆ†é˜¶æ®µçš„ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å„ä¸ªæ–¹é¢çš„æ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¢å¼ºæ¨ç†çš„æ¨¡å‹åœ¨å¤„ç†æ— æ³•å›ç­”çš„æŸ¥è¯¢æ—¶ï¼Œæ­£ç¡®ç‡æå‡äº†çº¦20%ï¼Œåœ¨ç”Ÿæˆå¼•ç”¨çš„è´¨é‡æ–¹é¢ä¹Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†GRPOæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„å¯ä¿¡åº¦å’Œå“åº”è´¨é‡ï¼Œå¯ä»¥åœ¨æ•™è‚²ã€åŒ»ç–—å’Œå®¢æˆ·æœåŠ¡ç­‰å¤šä¸ªè¡Œä¸šä¸­å®ç°æ›´é«˜æ•ˆçš„äº¤äº’å’Œä¿¡æ¯è·å–ï¼Œæœªæ¥å¯èƒ½å¯¹äººæœºäº¤äº’çš„æ–¹å¼äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generating grounded and trustworthy responses remains a key challenge for large language models (LLMs). While retrieval-augmented generation (RAG) with citation-based grounding holds promise, instruction-tuned models frequently fail even in straightforward scenarios: missing explicitly stated answers, citing incorrectly, or refusing when evidence is available. In this work, we explore how reinforcement learning (RL) and internal reasoning can enhance grounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method to train models using verifiable outcome-based rewards targeting answer correctness, citation sufficiency, and refusal quality, without requiring gold reasoning traces or expensive annotations. Through comprehensive experiments across ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented models significantly outperform instruction-only variants, especially in handling unanswerable queries and generating well-cited responses. A two-stage training setup, first optimizing answer and citation behavior and then refusal, further improves grounding by stabilizing the learning signal. Additionally, we revisit instruction tuning via GPT-4 distillation and find that combining it with GRPO enhances performance on long-form, generative QA tasks. Overall, our findings highlight the value of reasoning, stage-wise optimization, and outcome-driven RL for building more verifiable and reliable LLMs.

