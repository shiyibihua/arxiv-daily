---
layout: default
title: Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning
---

# Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15894" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15894v1</a>
  <a href="https://arxiv.org/pdf/2506.15894.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15894v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15894v1', 'Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sam Silver, Jimin Sun, Ivan Zhang, Sara Hooker, Eddie Kim

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå•è½®è‡ªæˆ‘çº é”™æœºåˆ¶ä»¥æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘çº é”™` `æ¨ç†èƒ½åŠ›` `æ€ç»´é“¾` `åˆæˆæ‰°åŠ¨` `å®éªŒç ”ç©¶` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹è¾“å…¥çš„å¾®å°å˜åŒ–è¡¨ç°å‡ºè„†å¼±æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€‚
2. æœ¬æ–‡é€šè¿‡å®éªŒæ¢è®¨è¯­è¨€æ¨¡å‹åœ¨æ€ç»´é“¾æ¨ç†ä¸­è‡ªæˆ‘çº é”™çš„èƒ½åŠ›ï¼Œæ­ç¤ºå…¶æ½œåœ¨çš„å†…åœ¨ç‰¹æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨é¢å¯¹åˆæˆæ‰°åŠ¨æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¿›è¡Œå•è½®è‡ªæˆ‘çº é”™ï¼Œè¡¨ç°å‡ºè¾ƒå¼ºçš„çº é”™èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹é—®é¢˜æè¿°å’Œæç¤ºç­–ç•¥çš„å¾®å°å˜åŒ–ä»ç„¶æ•æ„Ÿã€‚æ­¤å¤–ï¼Œæ¨ç†è¿‡ç¨‹æ˜“å—é‡‡æ ·å¼•èµ·çš„é”™è¯¯å½±å“ï¼Œå¯¼è‡´è‡ªå›å½’æ¨¡å‹éœ€é€šè¿‡ç”Ÿæˆé¢å¤–çš„æ ‡è®°è¿›è¡Œè‡ªæˆ‘çº é”™ã€‚ä¸ºæ·±å…¥ç†è§£è¿‘æœŸæ¨¡å‹çš„è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼Œæœ¬æ–‡é€šè¿‡å®éªŒæµ‹é‡æ¨¡å‹åœ¨å…¶æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä¸­å¼•å…¥çš„åˆæˆæ‰°åŠ¨ä¸‹çš„è‡ªæˆ‘çº é”™èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤šä¸ªå¼€æ”¾æƒé‡æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºå¼ºå¤§çš„å•è½®è‡ªæˆ‘çº é”™è¡Œä¸ºï¼Œæ¶µç›–ä»å¾®å¦™çš„éšå¼çº æ­£åˆ°æ˜ç¡®çš„é”™è¯¯æ‰¿è®¤å’Œçº æ­£ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æœªé’ˆå¯¹é•¿æ€ç»´é“¾è¿›è¡Œå¾®è°ƒçš„LLMsï¼Œå¯èƒ½ä¹Ÿå…·å¤‡æ¯”æ–‡çŒ®ä¸­å¸¸è§çš„æ›´å¼ºçš„å†…åœ¨è‡ªæˆ‘çº é”™èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹è¾“å…¥æ‰°åŠ¨çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¾®å°å˜åŒ–æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ¨ç†ç»“æœä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥åˆæˆæ‰°åŠ¨ï¼Œç ”ç©¶è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼Œæ¢ç´¢å…¶åœ¨å•è½®æ¨ç†ä¸­çš„è¡¨ç°ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹å†…åœ¨çš„è‡ªæˆ‘çº é”™ç‰¹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šå®éªŒè®¾è®¡åŒ…æ‹¬å¤šä¸ªå¼€æ”¾æƒé‡æ¨¡å‹å’Œæ•°æ®é›†ï¼Œé‡‡ç”¨åˆæˆæ‰°åŠ¨çš„æ–¹æ³•å¯¹æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹è¿›è¡Œå¹²æ‰°ï¼Œå¹¶è§‚å¯Ÿæ¨¡å‹çš„è‡ªæˆ‘çº é”™è¡Œä¸ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºå‘ç°äº†æœªç»è¿‡é•¿æ€ç»´é“¾å¾®è°ƒçš„è¯­è¨€æ¨¡å‹ä¹Ÿå…·å¤‡å¼ºå¤§çš„è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼ŒæŒ‘æˆ˜äº†ç°æœ‰æ–‡çŒ®å¯¹æ¨¡å‹èƒ½åŠ›çš„è®¤çŸ¥ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­è®¾ç½®äº†å¤šç§æ‰°åŠ¨ç±»å‹ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæƒ…å†µä¸‹çš„è‡ªæˆ‘çº é”™è¡¨ç°ï¼Œå…³æ³¨éšå¼å’Œæ˜¾å¼çº æ­£çš„èƒ½åŠ›ã€‚å…·ä½“å‚æ•°å’ŒæŸå¤±å‡½æ•°è®¾ç½®æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒå®Œæ•´è®ºæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šä¸ªå¼€æ”¾æƒé‡æ¨¡å‹åœ¨é¢å¯¹åˆæˆæ‰°åŠ¨æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¿›è¡Œå•è½®è‡ªæˆ‘çº é”™ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„å†…åœ¨çº é”™èƒ½åŠ›ã€‚ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ¨¡å‹åœ¨éšå¼å’Œæ˜¾å¼çº æ­£æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶æ¨ç†èƒ½åŠ›çš„å¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨é¢å¯¹å¤æ‚é—®é¢˜æ—¶çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œå¢å¼ºè‡ªæˆ‘çº é”™èƒ½åŠ›çš„æ¨¡å‹å¯èƒ½åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å¯é æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated impressive mathematical reasoning capabilities, yet their performance remains brittle to minor variations in problem description and prompting strategy. Furthermore, reasoning is vulnerable to sampling-induced errors which autoregressive models must primarily address using self-correction via additionally-generated tokens. To better understand self-correction capabilities of recent models, we conduct experiments measuring models' ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning. We observe robust single-utterance intrinsic self-correction behavior across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors. Our findings suggest that LLMs, including those not finetuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature. The presence of this ability suggests that recent "reasoning" model work involves amplification of traits already meaningfully present in models.

