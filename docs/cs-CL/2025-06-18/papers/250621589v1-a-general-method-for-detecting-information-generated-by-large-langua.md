---
layout: default
title: A General Method for Detecting Information Generated by Large Language Models
---

# A General Method for Detecting Information Generated by Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21589" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21589v1</a>
  <a href="https://arxiv.org/pdf/2506.21589.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21589v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21589v1', 'A General Method for Detecting Information Generated by Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minjia Mao, Dongjun Wei, Xiao Fang, Michael Chau

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨LLMæ£€æµ‹å™¨ä»¥è§£å†³ä¿¡æ¯ç”Ÿæˆè¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯æ£€æµ‹` `åŒè®°å¿†ç½‘ç»œ` `æ³›åŒ–èƒ½åŠ›` `æ•°å­—å¹³å°` `å†…å®¹ç”Ÿæˆ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ£€æµ‹æ–¹æ³•ä¸»è¦é’ˆå¯¹ç‰¹å®šLLMï¼Œç¼ºä¹å¯¹æ–°LLMå’Œé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œé™ä½äº†å®é™…åº”ç”¨æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºçš„é€šç”¨LLMæ£€æµ‹å™¨ï¼ˆGLDï¼‰ç»“åˆåŒè®°å¿†ç½‘ç»œå’Œç†è®ºæŒ‡å¯¼æ¨¡å—ï¼Œæ—¨åœ¨æå‡æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. é€šè¿‡å®è¯è¯„ä¼°ï¼ŒGLDåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ£€æµ‹æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨æ˜¾è‘—æ”¹å˜äº†æ•°å­—ä¿¡æ¯ç¯å¢ƒï¼Œä½¿å¾—åŒºåˆ†äººç±»æ’°å†™å’ŒLLMç”Ÿæˆå†…å®¹å˜å¾—æ„ˆåŠ å›°éš¾ã€‚æ£€æµ‹LLMç”Ÿæˆçš„ä¿¡æ¯å¯¹äºç»´æŠ¤æ•°å­—å¹³å°çš„ä¿¡ä»»è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ç¤¾äº¤åª’ä½“å’Œç”µå­å•†åŠ¡ç½‘ç«™ä¸Šã€‚ç„¶è€Œï¼Œå½“å‰çš„æ£€æµ‹æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è¯†åˆ«ç‰¹å®šLLMç”Ÿæˆçš„å†…å®¹ï¼Œä¸”åœ¨æ–°å‡ºç°çš„LLMå’Œé¢†åŸŸä¸Šç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨LLMæ£€æµ‹å™¨ï¼ˆGLDï¼‰ï¼Œç»“åˆäº†åŒè®°å¿†ç½‘ç»œè®¾è®¡å’Œç†è®ºæŒ‡å¯¼çš„æ£€æµ‹æ³›åŒ–æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨æœªè§è¿‡çš„LLMå’Œé¢†åŸŸä¸­æ£€æµ‹LLMç”Ÿæˆçš„ä¿¡æ¯ã€‚é€šè¿‡çœŸå®ä¸–ç•Œæ•°æ®é›†çš„å¹¿æ³›å®è¯è¯„ä¼°å’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº†GLDåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ£€æµ‹æ–¹æ³•ã€‚æœ¬ç ”ç©¶å¯¹æ•°å­—å¹³å°å’ŒLLMå…·æœ‰é‡è¦çš„å­¦æœ¯å’Œå®è·µæ„ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆæ£€æµ‹LLMç”Ÿæˆçš„ä¿¡æ¯ï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å¯¹æ–°å‡ºç°çš„LLMå’Œé¢†åŸŸæ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ£€æµ‹æ•ˆæœä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„GLDé€šè¿‡ç»“åˆåŒè®°å¿†ç½‘ç»œå’Œç†è®ºæŒ‡å¯¼çš„æ£€æµ‹æ³›åŒ–æ¨¡å—ï¼Œæ—¨åœ¨æå‡å¯¹æœªè§è¿‡çš„LLMå’Œé¢†åŸŸçš„æ£€æµ‹èƒ½åŠ›ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGLDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å±‚ã€åŒè®°å¿†ç½‘ç»œæ¨¡å—å’Œç†è®ºæŒ‡å¯¼çš„æ£€æµ‹æ³›åŒ–æ¨¡å—ã€‚è¾“å…¥å±‚è´Ÿè´£æ¥æ”¶å¾…æ£€æµ‹å†…å®¹ï¼ŒåŒè®°å¿†ç½‘ç»œç”¨äºæå–ç‰¹å¾ï¼Œè€Œæ£€æµ‹æ³›åŒ–æ¨¡å—åˆ™åŸºäºç†è®ºæŒ‡å¯¼è¿›è¡Œä¿¡æ¯åˆ¤æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šGLDçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŒè®°å¿†ç½‘ç»œè®¾è®¡å’Œç†è®ºæŒ‡å¯¼æ¨¡å—çš„ç»“åˆï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœªè§è¿‡çš„LLMå’Œé¢†åŸŸä¸­ä¿æŒé«˜æ•ˆçš„æ£€æµ‹èƒ½åŠ›ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å±€é™æ€§å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ£€æµ‹æ•ˆæœï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„å‚æ•°æ¥æå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œç¡®ä¿å…¶åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGLDåœ¨æ£€æµ‹å‡†ç¡®ç‡ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†çº¦15%ï¼Œå¹¶ä¸”åœ¨å¤„ç†æœªè§è¿‡çš„LLMå’Œé¢†åŸŸæ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶å¹¿æ³›é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“ã€ç”µå­å•†åŠ¡å’Œæ–°é—»å¹³å°ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œè¿‡æ»¤LLMç”Ÿæˆçš„å†…å®¹ï¼Œä»è€Œç»´æŠ¤ä¿¡æ¯çš„çœŸå®æ€§å’Œç”¨æˆ·ä¿¡ä»»ã€‚æœªæ¥ï¼ŒGLDçš„æŠ€æœ¯å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œå¦‚æ•™è‚²å’Œå†…å®¹åˆ›ä½œï¼Œè¿›ä¸€æ­¥æå‡ä¿¡æ¯è´¨é‡å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The proliferation of large language models (LLMs) has significantly transformed the digital information landscape, making it increasingly challenging to distinguish between human-written and LLM-generated content. Detecting LLM-generated information is essential for preserving trust on digital platforms (e.g., social media and e-commerce sites) and preventing the spread of misinformation, a topic that has garnered significant attention in IS research. However, current detection methods, which primarily focus on identifying content generated by specific LLMs in known domains, face challenges in generalizing to new (i.e., unseen) LLMs and domains. This limitation reduces their effectiveness in real-world applications, where the number of LLMs is rapidly multiplying and content spans a vast array of domains. In response, we introduce a general LLM detector (GLD) that combines a twin memory networks design and a theory-guided detection generalization module to detect LLM-generated information across unseen LLMs and domains. Using real-world datasets, we conduct extensive empirical evaluations and case studies to demonstrate the superiority of GLD over state-of-the-art detection methods. The study has important academic and practical implications for digital platforms and LLMs.

