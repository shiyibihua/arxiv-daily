---
layout: default
title: Mix-of-Language-Experts Architecture for Multilingual Programming
---

# Mix-of-Language-Experts Architecture for Multilingual Programming

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18923" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18923v1</a>
  <a href="https://arxiv.org/pdf/2506.18923.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18923v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18923v1', 'Mix-of-Language-Experts Architecture for Multilingual Programming')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Zong, Yuntian Deng, Pengyu Nie

**åˆ†ç±»**: cs.PL, cs.CL, cs.SE

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

**å¤‡æ³¨**: Accepted at LLM4Code @ ICSE 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoLEæ¶æ„ä»¥è§£å†³å¤šè¯­è¨€ç¼–ç¨‹ä¸­çš„æ•ˆç‡ä¸ä¸“ä¸šåŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€ç¼–ç¨‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `LoRA` `çŸ¥è¯†å…±äº«` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ”¯æŒå¤šè¯­è¨€ç¼–ç¨‹æ—¶é¢ä¸´æ•ˆç‡ä¸ä¸“ä¸šåŒ–ä¹‹é—´çš„æƒè¡¡ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºMoLEæ¶æ„ï¼Œé€šè¿‡å…±äº«LoRAæ¨¡å—ä¸è¯­è¨€ç‰¹å®šLoRAæ¨¡å—çš„è”åˆä¼˜åŒ–ï¼Œå®ç°çŸ¥è¯†å…±äº«ä¸ä¸“ä¸šåŒ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoLEåœ¨å‚æ•°æ•ˆç‡ä¸Šä¼˜äºç‹¬ç«‹çš„è¯­è¨€ç‰¹å®šLoRAï¼ŒåŒæ—¶åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†å•ä¸€å…±äº«LLMã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç†è§£ã€ç”Ÿæˆå’Œç¿»è¯‘ç­‰ä»»åŠ¡ä¸­å±•ç°äº†å‡ºè‰²çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ”¯æŒå¤šè¯­è¨€ç¼–ç¨‹é€šå¸¸é¢ä¸´ä¸¤ç§é€‰æ‹©ï¼šä¸€ç§æ˜¯å¯¹å•ä¸€LLMè¿›è¡Œå¾®è°ƒï¼Œè™½ç„¶æˆæœ¬ä½ä½†ç‰ºç‰²äº†è¯­è¨€ç‰¹å®šçš„ä¸“ä¸šåŒ–ï¼›å¦ä¸€ç§æ˜¯ä¸ºæ¯ç§ç¼–ç¨‹è¯­è¨€å¾®è°ƒç‹¬ç«‹çš„LLMï¼Œè™½ç„¶å¯ä»¥å®ç°ä¸“ä¸šåŒ–ï¼Œä½†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬é«˜ã€‚æœ¬æ–‡æå‡ºäº†MoLEï¼ˆMix-of-Language-Expertsï¼‰æ¶æ„ï¼Œæ—¨åœ¨å¹³è¡¡å¤šè¯­è¨€ç¼–ç¨‹ä¸­çš„æ•ˆç‡ä¸ä¸“ä¸šåŒ–ã€‚MoLEç”±åŸºç¡€æ¨¡å‹ã€å…±äº«çš„LoRAæ¨¡å—å’Œä¸€ç»„è¯­è¨€ç‰¹å®šçš„LoRAæ¨¡å—ç»„æˆï¼Œè¿™äº›æ¨¡å—åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å…±åŒä¼˜åŒ–ï¼Œå®ç°äº†çŸ¥è¯†å…±äº«ä¸ä¸“ä¸šåŒ–ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒMoLEä¼šè‡ªåŠ¨è·¯ç”±åˆ°ä¸ç”Ÿæˆä»£ç æ ‡è®°å¯¹åº”çš„è¯­è¨€ç‰¹å®šLoRAæ¨¡å—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoLEåœ¨å‚æ•°æ•ˆç‡ä¸Šä¼˜äºç‹¬ç«‹çš„è¯­è¨€ç‰¹å®šLoRAè®­ç»ƒï¼ŒåŒæ—¶åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ä¸ºæ‰€æœ‰ç¼–ç¨‹è¯­è¨€å¾®è°ƒçš„å•ä¸€å…±äº«LLMã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€ç¼–ç¨‹ä¸­æ•ˆç‡ä¸ä¸“ä¸šåŒ–çš„çŸ›ç›¾ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆç‰ºç‰²è¯­è¨€ç‰¹å®šçš„æ€§èƒ½ï¼Œè¦ä¹ˆå¯¼è‡´è®¡ç®—ä¸å­˜å‚¨çš„é«˜æˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoLEæ¶æ„é€šè¿‡ç»“åˆå…±äº«LoRAæ¨¡å—ä¸è¯­è¨€ç‰¹å®šLoRAæ¨¡å—ï¼Œå…è®¸åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å®ç°çŸ¥è¯†å…±äº«ï¼ŒåŒæ—¶ä¿æŒæ¯ç§è¯­è¨€çš„ä¸“ä¸šåŒ–èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨æ¨ç†æ—¶èƒ½å¤Ÿçµæ´»é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoLEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªåŸºç¡€æ¨¡å‹ã€ä¸€ä¸ªå…±äº«çš„LoRAæ¨¡å—å’Œå¤šä¸ªè¯­è¨€ç‰¹å®šçš„LoRAæ¨¡å—ã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œè¿™äº›æ¨¡å—å…±åŒä¼˜åŒ–ï¼Œä»¥å®ç°æœ€ä½³çš„æ€§èƒ½ã€‚æ¨ç†æ—¶ï¼Œæ¨¡å‹ä¼šæ ¹æ®ç”Ÿæˆçš„ä»£ç æ ‡è®°è‡ªåŠ¨é€‰æ‹©ç›¸åº”çš„è¯­è¨€ç‰¹å®šæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoLEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–è®¾è®¡ï¼Œå…è®¸åœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶å®ç°è¯­è¨€ç‰¹å®šçš„ä¸“ä¸šåŒ–ã€‚è¿™ä¸ä¼ ç»Ÿçš„å•ä¸€LLMæˆ–å®Œå…¨ç‹¬ç«‹çš„LLMæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLoRAæ¨¡å—çš„ä½ç§©é€‚åº”æ€§è¢«ç”¨äºå‡å°‘å‚æ•°é‡ï¼ŒåŒæ—¶ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ æ¯ç§è¯­è¨€çš„ç‰¹æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¿›è¡Œäº†ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ä¿ƒè¿›ä¸åŒæ¨¡å—ä¹‹é—´çš„ååŒå·¥ä½œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMoLEåœ¨å‚æ•°æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç‹¬ç«‹è®­ç»ƒçš„è¯­è¨€ç‰¹å®šLoRAï¼Œä¸”åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ä¸ºæ‰€æœ‰ç¼–ç¨‹è¯­è¨€å¾®è°ƒçš„å•ä¸€å…±äº«LLMã€‚å…·ä½“è€Œè¨€ï¼ŒMoLEåœ¨å¤šè¯­è¨€ç¼–ç¨‹ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘å·¥å…·ã€æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹ä»¥åŠè·¨è¯­è¨€çš„ä»£ç ç”Ÿæˆä¸ç¿»è¯‘ç³»ç»Ÿã€‚MoLEæ¶æ„çš„é«˜æ•ˆæ€§å’Œä¸“ä¸šåŒ–èƒ½åŠ›å°†æå¤§æå‡å¼€å‘è€…çš„ç”Ÿäº§åŠ›ï¼Œå¹¶æ¨åŠ¨å¤šè¯­è¨€ç¼–ç¨‹çš„æ™®åŠä¸å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„ç¼–ç¨‹è¯­è¨€å’Œåº”ç”¨åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated impressive capabilities in aiding developers with tasks like code comprehension, generation, and translation. Supporting multilingual programming -- i.e., coding tasks across multiple programming languages -- typically requires either (1) finetuning a single LLM across all programming languages, which is cost-efficient but sacrifices language-specific specialization and performance, or (2) finetuning separate LLMs for each programming language, which allows for specialization but is computationally expensive and storage-intensive due to the duplication of parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel architecture that balances efficiency and specialization for multilingual programming. MoLE is composed of a base model, a shared LoRA (low-rank adaptation) module, and a collection of language-specific LoRA modules. These modules are jointly optimized during the finetuning process, enabling effective knowledge sharing and specialization across programming languages. During inference, MoLE automatically routes to the language-specific LoRA module corresponding to the programming language of the code token being generated. Our experiments demonstrate that MoLE achieves greater parameter efficiency compared to training separate language-specific LoRAs, while outperforming a single shared LLM finetuned for all programming languages in terms of accuracy.

