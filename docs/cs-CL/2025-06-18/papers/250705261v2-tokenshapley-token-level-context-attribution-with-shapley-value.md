---
layout: default
title: TokenShapley: Token Level Context Attribution with Shapley Value
---

# TokenShapley: Token Level Context Attribution with Shapley Value

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.05261" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.05261v2</a>
  <a href="https://arxiv.org/pdf/2507.05261.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.05261v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.05261v2', 'TokenShapley: Token Level Context Attribution with Shapley Value')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yingtai Xiao, Yuqing Zhu, Sirat Samyoun, Wanrong Zhang, Jiachen T. Wang, Jian Du

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18 (æ›´æ–°: 2025-07-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTokenShapleyä»¥è§£å†³LLMç”Ÿæˆå“åº”çš„å…³é”®è¯å½’å› é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `å½’å› æ–¹æ³•` `Shapleyå€¼` `KNNæ£€ç´¢` `ç»†ç²’åº¦åˆ†æ` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥å­çº§å½’å› æ–¹æ³•æ— æ³•æ»¡è¶³ç”¨æˆ·å¯¹ç‰¹å®šå…³é”®è¯çš„å½’å› éœ€æ±‚ï¼Œå¯¼è‡´åœ¨éªŒè¯ç”Ÿæˆå†…å®¹æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚
2. TokenShapleyé€šè¿‡ç»“åˆShapleyå€¼å’ŒKNNæ£€ç´¢æŠ€æœ¯ï¼Œæä¾›äº†ä¸€ç§ä»¤ç‰Œçº§åˆ«çš„ç»†ç²’åº¦å½’å› æ–¹æ³•ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTokenShapleyçš„å‡†ç¡®ç‡æ¯”æœ€å…ˆè¿›çš„åŸºçº¿æé«˜äº†11-23%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†éªŒè¯å…¶ç”Ÿæˆå“åº”çš„æ­£ç¡®æ€§ä»ç„¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¥å­çº§åˆ«çš„å½’å› ï¼Œä½†åœ¨ç”¨æˆ·éœ€è¦å¯¹ç‰¹å®šå…³é”®è¯ï¼ˆå¦‚æ•°å­—ã€å¹´ä»½æˆ–åç§°ï¼‰è¿›è¡Œå½’å› æ—¶ï¼Œè¿™äº›æ–¹æ³•æ˜¾å¾—ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†TokenShapleyï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºShapleyå€¼çš„ä»¤ç‰Œçº§å½’å› æ–¹æ³•ï¼Œç»“åˆäº†KNNæ£€ç´¢æŠ€æœ¯ã€‚é€šè¿‡åˆ©ç”¨é¢„è®¡ç®—çš„æ•°æ®å­˜å‚¨åº“è¿›è¡Œä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œå¹¶è®¡ç®—Shapleyå€¼æ¥é‡åŒ–ä»¤ç‰Œçš„é‡è¦æ€§ï¼ŒTokenShapleyæä¾›äº†ä¸€ç§ç»†ç²’åº¦çš„æ•°æ®å½’å› æ–¹æ³•ã€‚å¯¹å››ä¸ªåŸºå‡†çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒTokenShapleyåœ¨ä»¤ç‰Œçº§å½’å› æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ï¼Œå‡†ç¡®ç‡æé«˜äº†11-23%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå“åº”æ—¶ï¼Œç”¨æˆ·å¯¹ç‰¹å®šå…³é”®è¯ï¼ˆå¦‚æ•°å­—ã€å¹´ä»½ã€åç§°ç­‰ï¼‰çš„å½’å› éœ€æ±‚ã€‚ç°æœ‰çš„å¥å­çº§å½’å› æ–¹æ³•æ— æ³•æä¾›è¶³å¤Ÿçš„ç»†ç²’åº¦ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨éªŒè¯ç”Ÿæˆå†…å®¹çš„æ­£ç¡®æ€§æ—¶é¢ä¸´å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTokenShapleyçš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆShapleyå€¼çš„å½’å› æ–¹æ³•ä¸KNNæ£€ç´¢æŠ€æœ¯ï¼Œé€šè¿‡é‡åŒ–æ¯ä¸ªä»¤ç‰Œçš„é‡è¦æ€§æ¥å®ç°ç»†ç²’åº¦çš„å½’å› ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«å’Œè§£é‡Šç”Ÿæˆå†…å®¹ä¸­çš„å…³é”®å…ƒç´ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTokenShapleyçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯é¢„è®¡ç®—çš„æ•°æ®å­˜å‚¨åº“ï¼Œç”¨äºä¸Šä¸‹æ–‡æ£€ç´¢ï¼›å…¶æ¬¡æ˜¯åŸºäºShapleyå€¼çš„è®¡ç®—æ¨¡å—ï¼Œç”¨äºé‡åŒ–ä»¤ç‰Œçš„é‡è¦æ€§ã€‚è¿™ä¸¤ä¸ªæ¨¡å—ååŒå·¥ä½œï¼Œå®ç°äº†é«˜æ•ˆçš„å½’å› è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šTokenShapleyçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»“åˆäº†Shapleyå€¼å’ŒKNNæ£€ç´¢çš„åŒé‡æœºåˆ¶ï¼Œä½¿å¾—ä»¤ç‰Œçº§åˆ«çš„å½’å› ä¸ä»…æ›´åŠ ç²¾ç¡®ï¼Œè€Œä¸”èƒ½å¤Ÿå¤„ç†å¤æ‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ä¸ä¼ ç»Ÿçš„å¥å­çº§å½’å› æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒTokenShapleyä½¿ç”¨äº†ä¼˜åŒ–çš„KNNæ£€ç´¢ç®—æ³•ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡æ£€ç´¢çš„æ•ˆç‡ã€‚åŒæ—¶ï¼ŒShapleyå€¼çš„è®¡ç®—é‡‡ç”¨äº†é«˜æ•ˆçš„è¿‘ä¼¼ç®—æ³•ï¼Œä»¥ç¡®ä¿åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å¯è¡Œæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TokenShapleyåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œå…¶å‡†ç¡®ç‡æé«˜äº†11-23%ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡è¯æ˜äº†å…¶åœ¨ä»¤ç‰Œçº§å½’å› ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç”¨æˆ·æä¾›äº†æ›´ä¸ºå¯é çš„å†…å®¹éªŒè¯å·¥å…·ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TokenShapleyçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¿¡æ¯æ£€ç´¢å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›æ›´ä¸ºç»†è‡´çš„å½’å› æœºåˆ¶ï¼Œç”¨æˆ·èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’ŒéªŒè¯æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ï¼Œä»è€Œæå‡ä¿¡ä»»åº¦å’Œä½¿ç”¨ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) demonstrate strong capabilities in in-context learning, but verifying the correctness of their generated responses remains a challenge. Prior work has explored attribution at the sentence level, but these methods fall short when users seek attribution for specific keywords within the response, such as numbers, years, or names. To address this limitation, we propose TokenShapley, a novel token-level attribution method that combines Shapley value-based data attribution with KNN-based retrieval techniques inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed datastore for contextual retrieval and computing Shapley values to quantify token importance, TokenShapley provides a fine-grained data attribution approach. Extensive evaluations on four benchmarks show that TokenShapley outperforms state-of-the-art baselines in token-level attribution, achieving an 11-23% improvement in accuracy.

