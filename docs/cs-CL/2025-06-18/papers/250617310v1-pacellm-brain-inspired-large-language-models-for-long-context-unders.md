---
layout: default
title: PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding
---

# PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17310" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17310v1</a>
  <a href="https://arxiv.org/pdf/2506.17310.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17310v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17310v1', 'PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kangcong Li, Peng Ye, Chongjun Tu, Lin Zhang, Chunfeng Song, Jiamin Wu, Tao Yang, Qihao Zheng, Tao Chen

**åˆ†ç±»**: q-bio.NC, cs.CL, cs.NE

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPaceLLMä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡ç†è§£` `æŒä¹…æ´»åŠ¨æœºåˆ¶` `çš®å±‚ä¸“å®¶èšç±»` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯è¡°å‡` `è¯­ä¹‰æ¨¡å—åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ï¼Œå› ä¿¡æ¯è¡°å‡å’Œè¯­ä¹‰ç¢ç‰‡åŒ–è€Œé¢ä¸´æ€§èƒ½ç“¶é¢ˆã€‚
2. æœ¬æ–‡æå‡ºçš„PaceLLMé€šè¿‡æŒä¹…æ´»åŠ¨æœºåˆ¶å’Œçš®å±‚ä¸“å®¶èšç±»ï¼Œè§£å†³äº†ä¸Šä¸‹æ–‡è¡°å‡å’Œè¯­ä¹‰ç¢ç‰‡åŒ–é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPaceLLMåœ¨å¤šæ–‡æ¡£é—®ç­”ä»»åŠ¡ä¸Šæå‡6%ï¼Œåœ¨Infinite-Benchä»»åŠ¡ä¸Šæå‡12.5%-17.5%ï¼Œå¹¶æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦è‡³200Kä¸ªæ ‡è®°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›å—åˆ°ç¬æ€ç¥ç»æ¿€æ´»å¯¼è‡´çš„ä¿¡æ¯è¡°å‡å’Œæ— ç»“æ„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰æƒé‡å¼•èµ·çš„è¯­ä¹‰ç¢ç‰‡åŒ–çš„é™åˆ¶ã€‚å—å¤§è„‘å·¥ä½œè®°å¿†å’Œçš®å±‚æ¨¡å—åŒ–çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†PaceLLMï¼Œå…·æœ‰ä¸¤é¡¹åˆ›æ–°ï¼šä¸€æ˜¯æŒä¹…æ´»åŠ¨ï¼ˆPAï¼‰æœºåˆ¶ï¼Œé€šè¿‡å¼•å…¥æ¿€æ´»çº§åˆ«çš„è®°å¿†åº“æ¥åŠ¨æ€æ£€ç´¢ã€é‡ç”¨å’Œæ›´æ–°å…³é”®FFNçŠ¶æ€ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡è¡°å‡é—®é¢˜ï¼›äºŒæ˜¯çš®å±‚ä¸“å®¶ï¼ˆCEï¼‰èšç±»ï¼Œæ¨¡æ‹Ÿä»»åŠ¡è‡ªé€‚åº”ç¥ç»ä¸“ä¸šåŒ–ï¼Œå°†FFNæƒé‡é‡æ–°ç»„ç»‡ä¸ºè¯­ä¹‰æ¨¡å—ï¼Œå»ºç«‹è·¨æ ‡è®°ä¾èµ–å…³ç³»ï¼Œå‡è½»ç¢ç‰‡åŒ–ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPaceLLMåœ¨LongBenchçš„å¤šæ–‡æ¡£é—®ç­”ä»»åŠ¡ä¸Šæé«˜äº†6%ï¼Œåœ¨Infinite-Benchä»»åŠ¡ä¸Šæå‡äº†12.5%-17.5%çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨Needle-In-A-Haystackæµ‹è¯•ä¸­å°†å¯æµ‹ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•è‡³200Kä¸ªæ ‡è®°ã€‚æ­¤ç ”ç©¶å¼€åˆ›äº†åŸºäºå¤§è„‘çš„LLMä¼˜åŒ–ï¼Œä¸”å¯æ¨å¹¿è‡³ä»»ä½•æ¨¡å‹ï¼Œå¢å¼ºå…¶é•¿ä¸Šä¸‹æ–‡æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼Œè€Œæ— éœ€ç»“æ„æ€§æ”¹é€ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£ä¸­çš„ä¿¡æ¯è¡°å‡å’Œè¯­ä¹‰ç¢ç‰‡åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å› ç¬æ€ç¥ç»æ¿€æ´»å’Œæ— ç»“æ„FFNæƒé‡ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡ä¿¡æ¯éš¾ä»¥æœ‰æ•ˆä¿æŒå’Œåˆ©ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPaceLLMçš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´å¤§è„‘çš„å·¥ä½œè®°å¿†å’Œçš®å±‚æ¨¡å—åŒ–ï¼Œé€šè¿‡æŒä¹…æ´»åŠ¨æœºåˆ¶å’Œçš®å±‚ä¸“å®¶èšç±»æ¥å¢å¼ºæ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚æŒä¹…æ´»åŠ¨æœºåˆ¶é€šè¿‡è®°å¿†åº“åŠ¨æ€ç®¡ç†FFNçŠ¶æ€ï¼Œè€Œçš®å±‚ä¸“å®¶èšç±»åˆ™é€šè¿‡è¯­ä¹‰æ¨¡å—åŒ–æ¥ä¼˜åŒ–ä¿¡æ¯å¤„ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPaceLLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæŒä¹…æ´»åŠ¨æœºåˆ¶å’Œçš®å±‚ä¸“å®¶èšç±»ã€‚æŒä¹…æ´»åŠ¨æœºåˆ¶è´Ÿè´£ä¿¡æ¯çš„åŠ¨æ€æ£€ç´¢å’Œæ›´æ–°ï¼Œè€Œçš®å±‚ä¸“å®¶èšç±»åˆ™è´Ÿè´£å°†FFNæƒé‡ç»„ç»‡æˆè¯­ä¹‰æ¨¡å—ï¼Œä¿ƒè¿›è·¨æ ‡è®°ä¾èµ–å…³ç³»çš„å»ºç«‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šPaceLLMçš„å…³é”®åˆ›æ–°åœ¨äºæŒä¹…æ´»åŠ¨æœºåˆ¶å’Œçš®å±‚ä¸“å®¶èšç±»çš„ç»“åˆï¼Œå‰è€…è§£å†³äº†ä¿¡æ¯è¡°å‡é—®é¢˜ï¼Œåè€…åˆ™æœ‰æ•ˆå‡è½»äº†è¯­ä¹‰ç¢ç‰‡åŒ–ã€‚è¿™ç§è®¾è®¡ä¸ä¼ ç»Ÿçš„å‰é¦ˆç½‘ç»œæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ¨¡æ‹Ÿäººè„‘çš„å¤„ç†æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒæŒä¹…æ´»åŠ¨æœºåˆ¶å¼•å…¥äº†æ¿€æ´»çº§åˆ«çš„è®°å¿†åº“ï¼Œå…è®¸æ¨¡å‹åœ¨å¤„ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´FFNçŠ¶æ€ã€‚åŒæ—¶ï¼Œçš®å±‚ä¸“å®¶èšç±»é€šè¿‡ä»»åŠ¡è‡ªé€‚åº”çš„æ–¹å¼é‡ç»„FFNæƒé‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è¯­ä¹‰ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PaceLLMåœ¨LongBenchçš„å¤šæ–‡æ¡£é—®ç­”ä»»åŠ¡ä¸Šå®ç°äº†6%çš„æ€§èƒ½æå‡ï¼Œåœ¨Infinite-Benchä»»åŠ¡ä¸Šæå‡äº†12.5%-17.5%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨Needle-In-A-Haystackæµ‹è¯•ä¸­å°†å¯æµ‹ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•è‡³200Kä¸ªæ ‡è®°ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PaceLLMçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¤„ç†é•¿æ–‡æœ¬æˆ–å¤šæ–‡æ¡£ä¿¡æ¯çš„åœºæ™¯ä¸­ï¼Œå¦‚æ³•å¾‹æ–‡ä¹¦åˆ†æã€ç§‘å­¦æ–‡çŒ®ç»¼è¿°å’Œé•¿ç¯‡æ–‡ç« ç”Ÿæˆç­‰ã€‚å…¶ä¼˜åŒ–çš„é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›å°†æå‡ç›¸å…³é¢†åŸŸçš„è‡ªåŠ¨åŒ–æ°´å¹³å’Œæ™ºèƒ½åŒ–åº”ç”¨ï¼Œæœªæ¥å¯èƒ½å¯¹è‡ªç„¶è¯­è¨€å¤„ç†çš„å¤šä¸ªæ–¹å‘äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Large Language Models (LLMs) demonstrate strong performance across domains, their long-context capabilities are limited by transient neural activations causing information decay and unstructured feed-forward network (FFN) weights leading to semantic fragmentation. Inspired by the brain's working memory and cortical modularity, we propose PaceLLM, featuring two innovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal cortex (PFC) neurons' persistent firing by introducing an activation-level memory bank to dynamically retrieve, reuse, and update critical FFN states, addressing contextual decay; and (2) Cortical Expert (CE) Clustering that emulates task-adaptive neural specialization to reorganize FFN weights into semantic modules, establishing cross-token dependencies and mitigating fragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement on LongBench's Multi-document QA and 12.5-17.5% performance gains on Infinite-Bench tasks, while extending measurable context length to 200K tokens in Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM optimization and is complementary to other works. Besides, it can be generalized to any model and enhance their long-context performance and interpretability without structural overhauls.

