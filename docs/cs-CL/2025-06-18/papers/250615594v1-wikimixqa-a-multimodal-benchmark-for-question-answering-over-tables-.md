---
layout: default
title: WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts
---

# WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15594" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15594v1</a>
  <a href="https://arxiv.org/pdf/2506.15594.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15594v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15594v1', 'WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Negar Foroutan, Angelika Romanou, Matin Ansaripour, Julian Martin Eisenschlos, Karl Aberer, RÃ©mi Lebret

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

**å¤‡æ³¨**: ACL 2025 (Findings)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWikiMixQAåŸºå‡†ä»¥è§£å†³å¤šæ¨¡æ€æ–‡æ¡£ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `æ–‡æ¡£ç†è§£` `è§†è§‰-è¯­è¨€æ¨¡å‹` `åŸºå‡†æµ‹è¯•` `ä¿¡æ¯ç»¼åˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ–‡æ¡£å¸ƒå±€ã€è¡¨æ ¼å’Œå›¾è¡¨æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é•¿ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ã€‚
2. è®ºæ–‡æå‡ºWikiMixQAåŸºå‡†ï¼ŒåŒ…å«1000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œå¼ºè°ƒè·¨æ¨¡æ€æ¨ç†ï¼Œè¦æ±‚æ¨¡å‹ç»¼åˆå¤šç§ä¿¡æ¯ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸“æœ‰æ¨¡å‹åœ¨ç›´æ¥ä¸Šä¸‹æ–‡ä¸‹å‡†ç¡®ç‡çº¦70%ï¼Œä½†åœ¨é•¿æ–‡æ¡£æ£€ç´¢æ—¶æ˜¾è‘—ä¸‹é™ï¼Œå¼€æºæ¨¡å‹è¡¨ç°æ›´å·®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æ¡£åœ¨ä¿¡æ¯ä¿å­˜å’Œä¼ æ’­ä¸­è‡³å…³é‡è¦ï¼Œé€šå¸¸åŒ…å«å¤æ‚çš„å¸ƒå±€ã€è¡¨æ ¼å’Œå›¾è¡¨ï¼Œè¿™å¯¹è‡ªåŠ¨æ–‡æ¡£ç†è§£ï¼ˆDUï¼‰æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡è§†è§‰-è¯­è¨€å¤§æ¨¡å‹ï¼ˆVLLMsï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†å…¶åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡è§†è§‰è¾“å…¥æ–¹é¢çš„æœ‰æ•ˆæ€§ä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡æå‡ºWikiMixQAï¼Œä¸€ä¸ªåŒ…å«1000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°ä»4000ä¸ªç»´åŸºç™¾ç§‘é¡µé¢æå–çš„è¡¨æ ¼å’Œå›¾è¡¨çš„è·¨æ¨¡æ€æ¨ç†ã€‚ä¸ç°æœ‰åŸºå‡†ä¸åŒï¼ŒWikiMixQAå¼ºè°ƒå¤æ‚æ¨ç†ï¼Œè¦æ±‚æ¨¡å‹ç»¼åˆæ¥è‡ªå¤šç§æ¨¡æ€çš„ä¿¡æ¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†12ä¸ªæœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å°½ç®¡ä¸“æœ‰æ¨¡å‹åœ¨æä¾›ç›´æ¥ä¸Šä¸‹æ–‡æ—¶èƒ½è¾¾åˆ°çº¦70%çš„å‡†ç¡®ç‡ï¼Œä½†åœ¨éœ€è¦ä»é•¿æ–‡æ¡£ä¸­æ£€ç´¢ä¿¡æ¯æ—¶ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚GPT-4-oæ˜¯å”¯ä¸€åœ¨æ­¤è®¾ç½®ä¸­è¶…è¿‡50%å‡†ç¡®ç‡çš„æ¨¡å‹ï¼Œè€Œå¼€æºæ¨¡å‹çš„è¡¨ç°åˆ™ç›¸å¯¹è¾ƒå·®ï¼Œæœ€é«˜å‡†ç¡®ç‡ä¸º27%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜ï¼Œå¹¶ç¡®ç«‹äº†WikiMixQAä½œä¸ºæ¨åŠ¨æ–‡æ¡£ç†è§£ç ”ç©¶çš„é‡è¦åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ–‡æ¡£ç†è§£ä¸­çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶åœ¨éœ€è¦ç»¼åˆå¤šç§æ¨¡æ€ä¿¡æ¯æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºWikiMixQAåŸºå‡†ï¼Œé€šè¿‡è®¾è®¡1000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œè¦æ±‚æ¨¡å‹åœ¨è¡¨æ ¼å’Œå›¾è¡¨ä¸­è¿›è¡Œå¤æ‚æ¨ç†ï¼Œä»è€Œè¯„ä¼°å…¶è·¨æ¨¡æ€ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€é—®é¢˜è®¾è®¡å’Œæ¨¡å‹è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ¥è‡ª4000ä¸ªç»´åŸºç™¾ç§‘é¡µé¢ï¼Œé—®é¢˜è®¾è®¡å¼ºè°ƒå¤šæ¨¡æ€ä¿¡æ¯çš„ç»¼åˆï¼Œæ¨¡å‹è¯„ä¼°åˆ™é‡‡ç”¨å¤šç§è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚

**å…³é”®åˆ›æ–°**ï¼šWikiMixQAçš„åˆ›æ–°åœ¨äºå…¶å¼ºè°ƒå¤æ‚æ¨ç†å’Œå¤šæ¨¡æ€ä¿¡æ¯çš„ç»¼åˆï¼ŒåŒºåˆ«äºç°æœ‰åŸºå‡†çš„ç®€å•é—®ç­”ä»»åŠ¡ï¼Œæ¨åŠ¨äº†æ–‡æ¡£ç†è§£çš„ç ”ç©¶è¿›å±•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è¯„ä¼°ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸“æœ‰å’Œå¼€æºæ¨¡å‹ï¼Œè®¾ç½®äº†ä¸åŒçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»¥æµ‹è¯•æ¨¡å‹åœ¨é•¿æ–‡æ¡£æ£€ç´¢ä¸­çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸“æœ‰æ¨¡å‹åœ¨ç›´æ¥ä¸Šä¸‹æ–‡ä¸‹çš„å‡†ç¡®ç‡çº¦ä¸º70%ï¼Œä½†åœ¨é•¿æ–‡æ¡£æ£€ç´¢æ—¶æ˜¾è‘—ä¸‹é™ï¼Œåªæœ‰GPT-4-oæ¨¡å‹åœ¨æ­¤è®¾ç½®ä¸­è¶…è¿‡50%çš„å‡†ç¡®ç‡ï¼Œè€Œå¼€æºæ¨¡å‹çš„æœ€é«˜å‡†ç¡®ç‡ä»…ä¸º27%ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¿¡æ¯æ£€ç´¢ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨æ–‡æ¡£åˆ†æç­‰ã€‚WikiMixQAåŸºå‡†çš„å»ºç«‹å°†ä¸ºå¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„ç ”ç©¶æä¾›æ–°çš„è¯„ä¼°æ ‡å‡†ï¼Œä¿ƒè¿›ç›¸å…³æŠ€æœ¯çš„å‘å±•ï¼Œæå‡å®é™…åº”ç”¨çš„æ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. We evaluate 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research.

