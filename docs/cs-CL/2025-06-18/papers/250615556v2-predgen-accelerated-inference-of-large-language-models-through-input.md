---
layout: default
title: PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction
---

# PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15556" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15556v2</a>
  <a href="https://arxiv.org/pdf/2506.15556.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15556v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15556v2', 'PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shufan Li, Aditya Grover

**åˆ†ç±»**: cs.CL, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18 (æ›´æ–°: 2025-10-08)

**å¤‡æ³¨**: 16 pages,4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPredGenä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å®æ—¶è¯­éŸ³äº¤äº’ä¸­çš„å»¶è¿Ÿé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å®æ—¶è¯­éŸ³äº¤äº’` `æ¨æµ‹è§£ç ` `æ–‡æœ¬è½¬è¯­éŸ³` `ç”¨æˆ·ä½“éªŒ` `è®¡ç®—æ•ˆç‡` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å®æ—¶è¯­éŸ³äº¤äº’ä¸­å­˜åœ¨æ˜¾è‘—å»¶è¿Ÿï¼Œå½±å“ç”¨æˆ·ä½“éªŒï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šã€‚
2. è®ºæ–‡æå‡ºçš„PredGenæ¡†æ¶é€šè¿‡è¾“å…¥æ—¶çš„æ¨æµ‹è§£ç ï¼Œå…è®¸åœ¨ç”¨æˆ·è¾“å…¥æ—¶ç”Ÿæˆå€™é€‰å“åº”ï¼Œä»è€Œå‡å°‘å»¶è¿Ÿã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPredGenåœ¨å¤šç§åœºæ™¯ä¸‹æœ‰æ•ˆå°†å»¶è¿Ÿå‡å°‘çº¦2å€ï¼Œä¸”è®¡ç®—æˆæœ¬å¢åŠ æå°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®æ—¶è¯­éŸ³èŠå¤©åº”ç”¨ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œé€šå¸¸ä¸æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç»“åˆç”ŸæˆéŸ³é¢‘å“åº”ã€‚ç„¶è€Œï¼Œç”±äºå…¶åºå¤§çš„æ¨¡å‹è§„æ¨¡ï¼Œç”¨æˆ·è¾“å…¥ç»“æŸä¸éŸ³é¢‘è¾“å‡ºå¼€å§‹ä¹‹é—´çš„å»¶è¿Ÿæ˜¾è‘—ï¼Œå½±å“ç”¨æˆ·ä½“éªŒã€‚æˆ‘ä»¬å‘ç°ï¼Œè¿™ç§å»¶è¿Ÿä¸»è¦ç”±LLMsç”Ÿæˆç¬¬ä¸€å¥æ‰€éœ€çš„æ—¶é—´ä¸»å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†é¢„æµ‹ç”Ÿæˆï¼ˆPredGenï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¾“å…¥æ—¶çš„æ¨æµ‹è§£ç æ¥å‡è½»ç”šè‡³æ¶ˆé™¤è¿™ç§å»¶è¿Ÿã€‚PredGenåœ¨ç”¨æˆ·è¯´è¯æ—¶ç”Ÿæˆå€™é€‰å“åº”ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿä»¥æœ€å°å»¶è¿Ÿå¼€å§‹TTSå¤„ç†ã€‚æ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä½¿ç”¨åœºæ™¯ä¸­æœ‰æ•ˆå°†å»¶è¿Ÿå‡å°‘çº¦2å€ï¼ŒåŒæ—¶åœ¨è¾“å…¥æ—¶ä»…å¢åŠ äº†æå°‘çš„è®¡ç®—æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å®æ—¶è¯­éŸ³äº¤äº’ä¸­å­˜åœ¨çš„æ˜¾è‘—å»¶è¿Ÿé—®é¢˜ï¼Œå°¤å…¶æ˜¯å½“LLMsä½œä¸ºå•ç”¨æˆ·è¯­éŸ³åŠ©æ‰‹åœ¨è®¡ç®—èƒ½åŠ›æœ‰é™çš„ç¡¬ä»¶ä¸Šè¿è¡Œæ—¶ï¼Œå»¶è¿Ÿå°¤ä¸ºæ˜æ˜¾ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆç¬¬ä¸€å¥æ—¶è€—æ—¶è¾ƒé•¿ï¼Œå¯¼è‡´ç”¨æˆ·ä½“éªŒä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é¢„æµ‹ç”Ÿæˆï¼ˆPredGenï¼‰æ¡†æ¶ï¼Œåœ¨ç”¨æˆ·è¾“å…¥æ—¶è¿›è¡Œæ¨æµ‹è§£ç ï¼Œæå‰ç”Ÿæˆå€™é€‰å“åº”ï¼Œä»è€Œä½¿å¾—TTSç³»ç»Ÿèƒ½å¤Ÿæ›´å¿«åœ°å¼€å§‹éŸ³é¢‘è¾“å‡ºã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å‡å°‘ç”¨æˆ·ç­‰å¾…æ—¶é—´ï¼Œæé«˜äº¤äº’çš„æµç•…æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPredGenæ¡†æ¶åŒ…æ‹¬è¾“å…¥æ—¶æ¨æµ‹è§£ç æ¨¡å—å’ŒTTSå¤„ç†æ¨¡å—ã€‚æ¨æµ‹è§£ç æ¨¡å—åœ¨ç”¨æˆ·è¯´è¯æ—¶ç”Ÿæˆå€™é€‰å“åº”ï¼Œè€ŒTTSå¤„ç†æ¨¡å—åˆ™åœ¨æ¥æ”¶åˆ°ç¬¬ä¸€å¥åç«‹å³å¼€å§‹éŸ³é¢‘åˆæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šPredGençš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨æµ‹è§£ç æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ç”¨æˆ·è¾“å…¥çš„åŒæ—¶è¿›è¡Œå“åº”ç”Ÿæˆï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•éœ€è¦ç­‰å¾…ç”¨æˆ·è¾“å…¥ç»“æŸåå†è¿›è¡Œå¤„ç†çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒPredGené‡‡ç”¨äº†è½»é‡çº§çš„è®¡ç®—ç­–ç•¥ï¼Œç¡®ä¿åœ¨è¾“å…¥æ—¶çš„è®¡ç®—æˆæœ¬ä¿æŒåœ¨æœ€ä½æ°´å¹³ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å› ç­‰å¾…è€Œäº§ç”Ÿçš„è®¡ç®—æµªè´¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPredGenåœ¨å¤šç§ä½¿ç”¨åœºæ™¯ä¸­æœ‰æ•ˆå°†å»¶è¿Ÿå‡å°‘çº¦2å€ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼ŒåŒæ—¶ä»…å¢åŠ äº†æå°‘çš„è®¡ç®—æˆæœ¬ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†åœ¨å®æ—¶è¯­éŸ³äº¤äº’ä¸­ä¼˜åŒ–ç”¨æˆ·ä½“éªŒçš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€å®æ—¶è¯­éŸ³ç¿»è¯‘å’Œäº’åŠ¨æ¸¸æˆç­‰åœºæ™¯ã€‚é€šè¿‡å‡å°‘å“åº”å»¶è¿Ÿï¼ŒPredGenèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œä¿ƒè¿›è¯­éŸ³äº¤äº’æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨å’Œå‘å±•ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½åœ¨æ›´å¤šå®æ—¶äº¤äº’ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are widely used in real-time voice chat applications, typically in combination with text-to-speech (TTS) systems to generate audio responses. However, their large size often leads to noticeable latency between the end of user input and the start of audio output, resulting in suboptimal user experiences. This latency is particularly evident when LLMs are deployed as single-user voice assistants on consumer-grade hardware with limited computing capacity. We discovered that this latency is primarily dominated by the time it takes for the LLMs to generate the first sentence, which is required as input by the TTS systems that synthesize audio responses on a sentence-by-sentence basis. To address this bottleneck, we propose Predictive Generation (PredGen), a novel framework that mitigates-or even eliminates-this delay through speculative decoding at input time. PredGen generates candidate responses while the user is still speaking, enabling the system to begin TTS processing with minimal delay. Simulated experiments on the Lmsys and MT-Bench datasets show that the proposed method can effectively reduce the latency by around 2x across a wide range of use cases, while incurring only minimal additional computation cost at input time-computation that would otherwise go unused.

