---
layout: default
title: Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning
---

# Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15415" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15415v1</a>
  <a href="https://arxiv.org/pdf/2506.15415.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15415v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15415v1', 'Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Stanley Ngugi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

**å¤‡æ³¨**: 11 pages, 3 figures, 2 tables. Research on parameter-efficient fine-tuning (PEFT) for low-resource languages (Swahili). Investigates cross-lingual lexical alignment in Lugha-Llama using LoRA and contrastive learning

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç›®æ ‡è¯æ±‡æ³¨å…¥æ–¹æ³•ä»¥æå‡ä½èµ„æºè¯­è¨€æ¨¡å‹çš„è·¨è¯­è¨€å¯¹é½èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½èµ„æºè¯­è¨€` `è·¨è¯­è¨€å¯¹é½` `ç›®æ ‡è¯æ±‡æ³¨å…¥` `ä½ç§©é€‚åº”` `æœºå™¨ç¿»è¯‘` `ä¿¡æ¯æ£€ç´¢` `è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€çš„è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è·¨è¯­è¨€è¯æ±‡å¯¹é½æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºç›®æ ‡è¯æ±‡æ³¨å…¥ï¼ˆTLIï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¨¡å‹æ—©æœŸå±‚è¿›è¡Œä½ç§©é€‚åº”å¾®è°ƒï¼Œå¢å¼ºè·¨è¯­è¨€å¯¹é½èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTLIæ–¹æ³•æ˜¾è‘—æå‡äº†æ–¯ç“¦å¸Œé‡Œè¯­-è‹±è¯­è¯å¯¹çš„ç›¸ä¼¼åº¦ï¼Œä¸”åœ¨æœªè§æ•°æ®ä¸Šä¹Ÿè¡¨ç°è‰¯å¥½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½èµ„æºè¯­è¨€ï¼ˆLRLsï¼‰å¦‚æ–¯ç“¦å¸Œé‡Œè¯­çš„è¡¨ç°å¾€å¾€ä¸å°½å¦‚äººæ„ï¼Œä¸»è¦ç”±äºæ•°æ®ç¨€ç¼ºå’Œé¢„è®­ç»ƒæ—¶çš„ä»£è¡¨æ€§ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç›®æ ‡è¯æ±‡æ³¨å…¥ï¼ˆTLIï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¯¹Lugha-Llama-8B-wuraæ¨¡å‹çš„æ—©æœŸå±‚è¿›è¡Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†æ–¯ç“¦å¸Œé‡Œè¯­ä¸è‹±è¯­è¯å¯¹çš„è¾“å‡ºçº§åˆ«çš„è¯æ±‡å¯¹é½èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡TLIå¾®è°ƒåï¼Œ623ä¸ªè®­ç»ƒçš„æ–¯ç“¦å¸Œé‡Œè¯­-è‹±è¯­è¯å¯¹çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ä»0.3211æå‡è‡³0.4113ï¼Œä¸”å¯¹63ä¸ªæœªè§æ§åˆ¶è¯å¯¹çš„ç›¸ä¼¼åº¦æå‡ä¹Ÿæ˜¾è‘—ã€‚è¿™è¡¨æ˜TLIæœ‰æ•ˆåœ°å¢å¼ºäº†æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„è·¨è¯­è¨€çŸ¥è¯†ä¼ æ’­èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ï¼ˆå¦‚æ–¯ç“¦å¸Œé‡Œè¯­ï¼‰ä¸­çš„è·¨è¯­è¨€è¯æ±‡å¯¹é½ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ¨¡å‹æ—©æœŸå±‚çš„æ½œåœ¨çŸ¥è¯†ï¼Œå¯¼è‡´æœ€ç»ˆè¾“å‡ºè¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„ç›®æ ‡è¯æ±‡æ³¨å…¥ï¼ˆTLIï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¨¡å‹æ—©æœŸå±‚è¿›è¡Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒï¼Œåˆ©ç”¨æ—©æœŸå±‚çš„å¼ºå¯¹é½èƒ½åŠ›æ¥æå‡æœ€ç»ˆè¾“å‡ºçš„è¯æ±‡å¯¹é½æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTLIæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè¯†åˆ«æ¨¡å‹æ—©æœŸå±‚çš„æœ€ä½³åµŒå…¥ï¼Œç„¶åé€šè¿‡å¯¹è¿™äº›åµŒå…¥è¿›è¡Œå¾®è°ƒæ¥ä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šTLIçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨æ¨¡å‹æ—©æœŸå±‚çš„é«˜ç›¸ä¼¼åº¦ç‰¹æ€§ï¼Œé’ˆå¯¹æ€§åœ°è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæœ‰æ•ˆæå‡äº†ä½èµ„æºè¯­è¨€æ¨¡å‹çš„è·¨è¯­è¨€å¯¹é½èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å…¨å±‚å¾®è°ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨TLIä¸­ï¼Œé‡‡ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œå¹¶ç»“åˆå¯¹æ¯”å­¦ä¹ ç›®æ ‡è¿›è¡Œå¾®è°ƒã€‚å…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬é€‰æ‹©æ—©æœŸå±‚çš„åµŒå…¥ä½œä¸ºå¾®è°ƒç›®æ ‡ï¼Œä»¥åŠè®¾è®¡é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç›¸ä¼¼åº¦æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡ç›®æ ‡è¯æ±‡æ³¨å…¥ï¼ˆTLIï¼‰å¾®è°ƒåï¼Œ623ä¸ªæ–¯ç“¦å¸Œé‡Œè¯­-è‹±è¯­è¯å¯¹çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ä»0.3211æå‡è‡³0.4113ï¼Œæå‡å¹…åº¦è¾¾åˆ°28.08%ã€‚æ­¤å¤–ï¼Œå¯¹63ä¸ªæœªè§æ§åˆ¶è¯å¯¹çš„ç›¸ä¼¼åº¦ä¹Ÿä»0.3143æå‡è‡³0.4033ï¼Œæå‡å¹…åº¦ä¸º28.32%ã€‚è¿™äº›ç»“æœè¡¨æ˜TLIåœ¨è·¨è¯­è¨€å¯¹é½æ–¹é¢çš„æ˜¾è‘—æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œå¤šè¯­è¨€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡ä½èµ„æºè¯­è¨€çš„æ¨¡å‹æ€§èƒ½ï¼ŒTLIæ–¹æ³•èƒ½å¤Ÿå¸®åŠ©æ›´å¥½åœ°æœåŠ¡äºå¤šè¯­è¨€ç”¨æˆ·ï¼Œä¿ƒè¿›è¯­è¨€çš„å¤šæ ·æ€§å’ŒåŒ…å®¹æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šä½èµ„æºè¯­è¨€çš„ç ”ç©¶ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their performance in low-resource languages (LRLs), such as Swahili, often lags due to data scarcity and underrepresentation in pre-training. A key challenge is achieving robust cross-lingual lexical alignment, crucial for tasks like translation and cross-lingual information retrieval. This paper introduces Targeted Lexical Injection (TLI), a novel and efficient fine-tuning approach. We first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits strong, near-perfect lexical alignment for Swahili-English word pairs in its early internal layers (specifically Layer 2, with ~0.99998 average cosine similarity based on a pilot study), a capability not fully reflected in its final output representations (baseline ~0.32 similarity on our evaluation set). TLI leverages this insight by using Low-Rank Adaptation (LoRA) and a contrastive learning objective to fine-tune the model, specifically targeting embeddings from this empirically identified optimal early layer. Our experiments show that TLI significantly improves the output-level lexical alignment for 623 trained Swahili-English word pairs, increasing average cosine similarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More importantly, these improvements generalize remarkably well to 63 unseen control word pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17 x 10^-27). These findings suggest TLI enhances the model's ability to preserve and propagate its inherent early-layer cross-lingual knowledge, offering a parameter-efficient and effective strategy for improving lexical alignment in LRL-focused LLMs.

