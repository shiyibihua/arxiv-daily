---
layout: default
title: CC-LEARN: Cohort-based Consistency Learning
---

# CC-LEARN: Cohort-based Consistency Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15662" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15662v1</a>
  <a href="https://arxiv.org/pdf/2506.15662.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15662v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15662v1', 'CC-LEARN: Cohort-based Consistency Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiao Ye, Shaswat Shrivastava, Zhaonan Li, Jacob Dineen, Shijie Lu, Avneet Ahuja, Ming Shen, Zhikun Xu, Ben Zhou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCC-Learnä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸€è‡´æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ä¸€è‡´æ€§å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†ç¨³å®šæ€§` `é—®é¢˜åˆ†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä¸€è‡´æ€§å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å¯é æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„CC-Learnæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨ç›¸ä¼¼é—®é¢˜çš„é˜Ÿåˆ—æ¥æå‡æ¨¡å‹çš„æ¨ç†ä¸€è‡´æ€§ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„æ¨ç†æ¨¡å¼ã€‚
3. åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCC-Learnæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ¨ç†ç¨³å®šæ€§ï¼Œç›¸è¾ƒäºé¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒåŸºçº¿å‡æœ‰æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸€è‡´æ€§å’Œç¨³å¥æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºé˜Ÿåˆ—çš„ä¸€è‡´æ€§å­¦ä¹ ï¼ˆCC-Learnï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¥è‡ªå…±äº«ç¨‹åºæŠ½è±¡çš„ç›¸ä¼¼é—®é¢˜é˜Ÿåˆ—è¿›è¡Œè®­ç»ƒï¼Œæå‡äº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å¯é æ€§ã€‚ä¸ºå¼ºåˆ¶æ‰§è¡Œé˜Ÿåˆ—çº§ä¸€è‡´æ€§ï¼Œå®šä¹‰äº†ä¸€ä¸ªå¤åˆç›®æ ‡ï¼Œç»“åˆäº†é˜Ÿåˆ—å‡†ç¡®æ€§ã€æœ‰æ•ˆé—®é¢˜åˆ†è§£çš„æ£€ç´¢å¥–åŠ±å’Œå¯¹çç¢æˆ–æ— æ•ˆæŸ¥æ‰¾çš„æ‹’ç»æƒ©ç½šï¼Œè¿™äº›éƒ½å¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCC-Learnåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ï¼ˆå¦‚ARC-Challengeå’ŒStrategyQAï¼‰ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œæ¨ç†ç¨³å®šæ€§ï¼Œè¯æ˜äº†é˜Ÿåˆ—çº§å¼ºåŒ–å­¦ä¹ æœ‰æ•ˆå¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨çš„ä¸€è‡´æ€§å’Œç¨³å¥æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¤„ç†å¤æ‚é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†ç»“æœä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCC-Learné€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ç›¸ä¼¼é—®é¢˜çš„é˜Ÿåˆ—è¿›è¡Œè®­ç»ƒï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨é˜Ÿåˆ—å†…ä¿æŒä¸€è‡´çš„æ¨ç†æ¨¡å¼ï¼Œä»è€Œæå‡æ¨ç†çš„å¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé˜Ÿåˆ—å‡†ç¡®æ€§è¯„ä¼°ã€æ£€ç´¢å¥–åŠ±æœºåˆ¶å’Œæ‹’ç»æƒ©ç½šæœºåˆ¶ã€‚é€šè¿‡ä¼˜åŒ–è¿™äº›æ¨¡å—çš„å¤åˆç›®æ ‡ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šCC-Learnçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†é˜Ÿåˆ—çº§åˆ«çš„ä¸€è‡´æ€§å­¦ä¹ ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ–å¤åˆç›®æ ‡ï¼Œè€Œéä¾èµ–ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†é˜Ÿåˆ—å‡†ç¡®æ€§å’Œæ£€ç´¢å¥–åŠ±ï¼ŒåŒæ—¶å¼•å…¥äº†å¯¹æ— æ•ˆæŸ¥æ‰¾çš„æƒ©ç½šæœºåˆ¶ï¼Œä»¥å¼•å¯¼æ¨¡å‹å­¦ä¹ æœ‰æ•ˆçš„æ¨ç†ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCC-Learnåœ¨ARC-Challengeå’ŒStrategyQAç­‰æ¨ç†åŸºå‡†ä¸Šï¼Œç›¸è¾ƒäºé¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒåŸºçº¿ï¼Œå‡†ç¡®æ€§å’Œæ¨ç†ç¨³å®šæ€§å‡æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨é—®ç­”ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæå‡è¿™äº›ç³»ç»Ÿåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¢å¼ºæ¨ç†ä¸€è‡´æ€§ï¼ŒCC-Learnæœ‰åŠ©äºæé«˜ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿçš„å¯é æ€§ï¼Œæœªæ¥å¯èƒ½åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models excel at many tasks but still struggle with consistent, robust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a reinforcement learning framework that improves the reliability of LLM reasoning by training on cohorts of similar questions derived from shared programmatic abstractions. To enforce cohort-level consistency, we define a composite objective combining cohort accuracy, a retrieval bonus for effective problem decomposition, and a rejection penalty for trivial or invalid lookups that reinforcement learning can directly optimize, unlike supervised fine-tuning. Optimizing this reward guides the model to adopt uniform reasoning patterns across all cohort members. Experiments on challenging reasoning benchmarks (including ARC-Challenge and StrategyQA) show that CC-Learn boosts both accuracy and reasoning stability over pretrained and SFT baselines. These results demonstrate that cohort-level RL effectively enhances reasoning consistency in LLMs.

