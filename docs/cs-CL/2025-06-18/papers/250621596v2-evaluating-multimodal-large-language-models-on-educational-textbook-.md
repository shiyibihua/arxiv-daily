---
layout: default
title: Evaluating Multimodal Large Language Models on Educational Textbook Question Answering
---

# Evaluating Multimodal Large Language Models on Educational Textbook Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21596" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.21596v2</a>
  <a href="https://arxiv.org/pdf/2506.21596.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21596v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21596v2', 'Evaluating Multimodal Large Language Models on Educational Textbook Question Answering')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hessa A. Alawwad, Anas Zafar, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.IR

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-18 (Êõ¥Êñ∞: 2025-07-15)

**Â§áÊ≥®**: 8 Pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÊïôÊùêÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑË°®Áé∞**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ÊïôÊùêÈóÆÁ≠î` `Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê` `ÁÅæÈöæÊÄß‰∏ä‰∏ãÊñáÂπ≤Êâ∞` `ÊïôËÇ≤ÊäÄÊúØ` `Ê®°ÂûãÂæÆË∞É` `Â§çÊùÇÊé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÊïôËÇ≤ÊùêÊñôÊó∂ÁöÑÊé®ÁêÜËÉΩÂäõÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÈ™åËØÅÔºåÂ≠òÂú®ÊÄßËÉΩ‰∏çÁ®≥ÂÆöÁöÑÈóÆÈ¢ò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÁÆ°ÈÅìÔºå‰ª•Êèê‰æõÁõ∏ÂÖ≥ÁöÑËØæÁ®ãÊÆµËêΩÂíåÂõæË°®‰Ωú‰∏∫‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÊ®°ÊãüÁúüÂÆûÂ≠¶‰π†Âú∫ÊôØ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLLaVAÂú®ÊñáÊú¨ÈóÆÈ¢ò‰∏äÁöÑË°®Áé∞ÊúâÊâÄÊèêÂçáÔºå‰ΩÜLLaMA 3.2-VisionÂú®ÂõæË°®‰ªªÂä°‰∏äÁöÑÂáÜÁ°ÆÊÄßÊòæËëó‰∏ãÈôçÔºåÊè≠Á§∫‰∫ÜÁÅæÈöæÊÄß‰∏ä‰∏ãÊñáÂπ≤Êâ∞Áé∞Ë±°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊàêÂäüÔºå‰ΩÜÂÖ∂Âú®Â§çÊùÇÊïôËÇ≤ÊùêÊñô‰∏äÁöÑÊé®ÁêÜËÉΩÂäõÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊµãËØï„ÄÇÊú¨Á†îÁ©∂È¶ñÊ¨°ËØÑ‰º∞‰∫ÜLLaVA-1.5ÂíåLLaMA 3.2-VisionÁ≠âÊúÄÂÖàËøõÁöÑMLLMsÂú®ÊïôÊùêÈóÆÁ≠îÔºàTQAÔºâ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞Ôºå‰ΩøÁî®CK12-QAÊï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁÆ°ÈÅìÔºå‰ª•Ê®°ÊãüÁúüÂÆûÂ≠¶‰π†ÁéØÂ¢É„ÄÇÈõ∂-shotÂÆûÈ™åÊè≠Á§∫‰∫Ü‰∏Ä‰∏™ÈáçË¶ÅÁöÑÊùÉË°°ÔºöËôΩÁÑ∂Ê£ÄÁ¥¢ÁöÑ‰∏ä‰∏ãÊñáÊèêÈ´ò‰∫ÜLLaVAÂú®ÊñáÊú¨ÈóÆÈ¢ò‰∏äÁöÑË°®Áé∞Ôºå‰ΩÜÊòæËëóÈôç‰Ωé‰∫ÜLLaMA 3.2-VisionÂú®ÂõæË°®‰ªªÂä°‰∏äÁöÑÂáÜÁ°ÆÊÄßÔºå‰ªé74.07%ÈôçËá≥25.93%„ÄÇÊàë‰ª¨Â∞ÜËøô‰∏ÄÊòæËëóÁé∞Ë±°Áß∞‰∏∫‚ÄúÁÅæÈöæÊÄß‰∏ä‰∏ãÊñáÂπ≤Êâ∞‚Äù„ÄÇÊ≠§Â§ñÔºåÂæÆË∞ÉÁªìÊûúÊòæÁ§∫Êû∂ÊûÑÂ∑ÆÂºÇÔºöLLaMA 3.2-VisionÂú®ÊµãËØïÈõÜ‰∏äÁöÑË°®Áé∞ÊèêÈ´òËá≥71.16%ÔºåËÄåLLaVAÁöÑË°®Áé∞‰∏ãÈôçÔºåË°®ÊòéÂÖ∂Âú®Ê≥õÂåñÊñπÈù¢Èù¢‰∏¥ÊåëÊàò„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÂº∫Ë∞É‰∫ÜMLLMsÂú®Ê®°ÊÄÅ‰ºòÂÖàÁ∫ßÂíå‰∏ä‰∏ãÊñáÊï¥ÂêàÊñπÈù¢ÁöÑÊåëÊàòÔºå‰∏∫ÂºÄÂèëÊõ¥Âº∫Â§ßÁöÑAIÈ©±Âä®ÊïôËÇ≤Â∑•ÂÖ∑Êèê‰æõ‰∫ÜÂü∫ÂáÜÂíåÂÖ≥ÈîÆÊñπÂêë„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨Á†îÁ©∂Êó®Âú®ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÊïôÊùêÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§çÊùÇÊïôËÇ≤ÊùêÊñôÁöÑÊé®ÁêÜËÉΩÂäõ‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂõæË°®‰ªªÂä°‰∏≠ÁöÑÂáÜÁ°ÆÊÄßÊòæËëó‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁÆ°ÈÅìÔºåÊèê‰æõÁõ∏ÂÖ≥ÁöÑËØæÁ®ãÊÆµËêΩÂíåÂõæË°®‰Ωú‰∏∫‰∏ä‰∏ãÊñáÔºå‰ª•ÊèêÂçáÊ®°ÂûãÂú®ÊïôÊùêÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÊ£ÄÁ¥¢Ê®°Âùó„ÄÅ‰∏ä‰∏ãÊñáÁîüÊàêÊ®°ÂùóÂíåÈóÆÁ≠îÁîüÊàêÊ®°Âùó„ÄÇÊï∞ÊçÆÊ£ÄÁ¥¢Ê®°ÂùóË¥üË¥£‰ªéCK12-QAÊï∞ÊçÆÈõÜ‰∏≠ÊèêÂèñÁõ∏ÂÖ≥‰ø°ÊÅØÔºå‰∏ä‰∏ãÊñáÁîüÊàêÊ®°ÂùóÊï¥ÂêàÊñáÊú¨ÂíåÂõæË°®‰ø°ÊÅØÔºåÈóÆÁ≠îÁîüÊàêÊ®°ÂùóÂàôÂü∫‰∫éÊï¥ÂêàÁöÑ‰∏ä‰∏ãÊñáÁîüÊàêÁ≠îÊ°à„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊèêÂá∫‰∫Ü‚ÄúÁÅæÈöæÊÄß‰∏ä‰∏ãÊñáÂπ≤Êâ∞‚ÄùËøô‰∏ÄÊ¶ÇÂøµÔºåÊè≠Á§∫‰∫ÜÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠‰∏ä‰∏ãÊñáÊ£ÄÁ¥¢ÂØπ‰∏çÂêåÊ®°ÂûãÁöÑÂΩ±ÂìçÔºåÂ∞§ÂÖ∂ÊòØÂØπLLaMA 3.2-VisionÁöÑË¥üÈù¢ÂΩ±Âìç„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠ÔºåÈááÁî®‰∫ÜÈõ∂-shotÂ≠¶‰π†ÂíåÂæÆË∞ÉÁ≠ñÁï•ÔºåÈíàÂØπ‰∏çÂêåÊ®°ÂûãÁöÑÊû∂ÊûÑËøõË°å‰∫Ü‰ºòÂåñÔºåLLaMA 3.2-VisionÂú®ÂæÆË∞ÉÂêéË°®Áé∞ÊèêÂçáËá≥71.16%ÔºåËÄåLLaVAÁöÑË°®Áé∞ÂàôÂá∫Áé∞‰∏ãÈôçÔºåÂèçÊò†Âá∫ÂÖ∂Âú®Ê≥õÂåñËÉΩÂäõ‰∏äÁöÑÊåëÊàò„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLLaVAÂú®ÊñáÊú¨ÈóÆÈ¢ò‰∏äÁöÑË°®Áé∞ÊúâÊâÄÊèêÂçáÔºåËÄåLLaMA 3.2-VisionÂú®ÂõæË°®‰ªªÂä°‰∏äÁöÑÂáÜÁ°ÆÊÄß‰ªé74.07%ÈôçËá≥25.93%ÔºåÊè≠Á§∫‰∫ÜÁÅæÈöæÊÄß‰∏ä‰∏ãÊñáÂπ≤Êâ∞Áé∞Ë±°„ÄÇÁªèËøáÂæÆË∞ÉÔºåLLaMA 3.2-VisionÁöÑË°®Áé∞ÊèêÂçáËá≥71.16%ÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Â§öÊ®°ÊÄÅÊï¥ÂêàËÉΩÂäõ‰∏äÁöÑÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊïôËÇ≤È¢ÜÂüüÔºåÂ∞§ÂÖ∂ÊòØÂú®Êô∫ËÉΩÊïôËÇ≤Â∑•ÂÖ∑ÁöÑÂºÄÂèë‰∏≠ÔºåËÉΩÂ§üÂ∏ÆÂä©Â≠¶ÁîüÊõ¥ÊúâÊïàÂú∞ÁêÜËß£Â§çÊùÇÁöÑÊïôÊùêÂÜÖÂÆπ„ÄÇÊú™Êù•ÔºåÂü∫‰∫éÊ≠§Á†îÁ©∂ÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊúâÊúõÂú®‰∏™ÊÄßÂåñÂ≠¶‰π†ÂíåËá™ÈÄÇÂ∫îÂ≠¶‰π†Á≥ªÁªü‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal large language models (MLLMs) have shown success in vision-language tasks, but their ability to reason over complex educational materials remains largely untested. This work presents the first evaluation of state-of-the-art MLLMs, including LLaVA-1.5 and LLaMA 3.2-Vision, on the textbook question answering (TQA) task using the CK12-QA dataset. We introduce a multimodal retrieval-augmented generation (RAG) pipeline to simulate real-world learning by providing relevant lesson paragraphs and diagrams as context. Our zero-shot experiments reveal a critical trade-off: while retrieved context improves LLaVA's performance on text-based questions, it significantly degrades the accuracy of the more powerful LLaMA 3.2-Vision on diagram-based tasks, dropping its validation accuracy from 74.07% to 25.93%. We term this statistically significant phenomenon "catastrophic context interference." Furthermore, fine-tuning highlights architectural differences: LLaMA 3.2-Vision's performance improves to 71.16% on the test set, demonstrating its capacity to learn multimodal integration, whereas LLaVA's performance declines, indicating challenges with generalization. Our results underscore the challenges MLLMs face in modality prioritization and context integration, providing a benchmark and pointing to key directions for developing more robust AI-driven educational tools.

