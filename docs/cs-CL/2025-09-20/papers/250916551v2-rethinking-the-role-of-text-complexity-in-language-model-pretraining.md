---
layout: default
title: Rethinking the Role of Text Complexity in Language Model Pretraining
---

# Rethinking the Role of Text Complexity in Language Model Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16551" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16551v2</a>
  <a href="https://arxiv.org/pdf/2509.16551.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16551v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16551v2', 'Rethinking the Role of Text Complexity in Language Model Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dan John Velasco, Matthew Theodore Roque

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20 (æ›´æ–°: 2025-10-04)

**å¤‡æ³¨**: Camera-ready version for BabyLM Workshop at EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶æ–‡æœ¬å¤æ‚åº¦å¯¹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„å½±å“ï¼Œæ­ç¤ºæ•°æ®å¤šæ ·æ€§ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½é—´çš„å…³ç³»ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ` `æ–‡æœ¬å¤æ‚åº¦` `æ•°æ®å¤šæ ·æ€§` `é›¶æ ·æœ¬å­¦ä¹ ` `å¾®è°ƒ` `çŸ¥è¯†è¿ç§»` `å› æœè¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶è¾ƒå°‘å…³æ³¨é¢„è®­ç»ƒæ•°æ®ä¸­æ–‡æœ¬å¤æ‚åº¦çš„å½±å“ï¼Œè€Œé«˜è´¨é‡å’Œå¤§è§„æ¨¡çš„æ•°æ®è¢«è®¤ä¸ºæ˜¯æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å…³é”®ã€‚
2. è¯¥ç ”ç©¶é€šè¿‡ç®€åŒ–æ–‡æœ¬çš„è¡¨é¢å¤æ‚åº¦ï¼ˆå¦‚å¥å­é•¿åº¦ã€è¯æ±‡éš¾åº¦å’Œç»“æ„ï¼‰ï¼ŒåŒæ—¶ä¿æŒæ ¸å¿ƒå†…å®¹ä¸å˜ï¼Œæ¥ç ”ç©¶æ–‡æœ¬å¤æ‚åº¦çš„ä½œç”¨ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ–‡æœ¬å¤æ‚åº¦å¯¹å¾®è°ƒå½±å“è¾ƒå°ï¼Œä½†å¯¹é›¶æ ·æœ¬å­¦ä¹ æœ‰æ˜¾è‘—å½±å“ï¼Œç®€å•æ–‡æœ¬åˆ©äºè¯­è¨€çŸ¥è¯†ï¼Œå¤æ‚æ–‡æœ¬åˆ©äºä¸–ç•ŒçŸ¥è¯†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬å¤æ‚åº¦åœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„ä½œç”¨ã€‚é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç®€åŒ–æ–‡æœ¬ï¼Œå¹¶åœ¨ä¸åŒå¤æ‚åº¦çš„æ–‡æœ¬ä¸Šä»å¤´é¢„è®­ç»ƒä¸åŒè§„æ¨¡çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆ28M-500Mï¼‰ï¼Œç ”ç©¶äººå‘˜è¯„ä¼°äº†æ¨¡å‹åœ¨å¾®è°ƒå’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œå›°æƒ‘åº¦å—æ¨¡å‹å®¹é‡å’Œæ–‡æœ¬å¤æ‚åº¦äº¤äº’å½±å“ï¼Œå°æ¨¡å‹åœ¨ç®€å•æ–‡æœ¬ä¸Šçš„æ€§èƒ½ä¸‹é™è¾ƒå°‘ã€‚æ–‡æœ¬å¤æ‚åº¦å¯¹å¾®è°ƒå½±å“ä¸å¤§ï¼Œä½†åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­ï¼Œç®€å•æ–‡æœ¬æœ‰åˆ©äºè¯­è¨€çŸ¥è¯†ä»»åŠ¡ï¼Œå¤æ‚æ–‡æœ¬åˆ™æœ‰åˆ©äºä¸–ç•ŒçŸ¥è¯†å’Œå®ä½“è·Ÿè¸ªä»»åŠ¡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸åŒç±»å‹çš„æ•°æ®å¤šæ ·æ€§å¯¹è¿ç§»å­¦ä¹ å’Œé›¶æ ·æœ¬æ€§èƒ½æœ‰ä¸åŒå½±å“ï¼Œä¸ºé’ˆå¯¹ç‰¹å®šç›®æ ‡çš„æ•°æ®ç®¡ç†æä¾›äº†è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç ”ç©¶é¢„è®­ç»ƒæ•°æ®ä¸­æ–‡æœ¬å¤æ‚åº¦å¯¹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ•°æ®è§„æ¨¡å’Œè´¨é‡ï¼Œè€Œå¿½ç•¥äº†æ–‡æœ¬å¤æ‚åº¦è¿™ä¸€é‡è¦å› ç´ ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ–‡æœ¬å¤æ‚åº¦ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´å…³ç³»çš„æ·±å…¥ç†è§£ï¼Œæ— æ³•æŒ‡å¯¼æ•°æ®é€‰æ‹©å’Œé¢„å¤„ç†ï¼Œä»è€Œå½±å“ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ§åˆ¶é¢„è®­ç»ƒæ•°æ®çš„æ–‡æœ¬å¤æ‚åº¦ï¼Œè§‚å¯Ÿå…¶å¯¹ä¸åŒè§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒå’Œé›¶æ ·æœ¬å­¦ä¹ ä¸­çš„å½±å“ã€‚é€šè¿‡ç®€åŒ–æ–‡æœ¬çš„è¡¨é¢ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒæ ¸å¿ƒè¯­ä¹‰ä¸å˜ï¼Œæ¥æ¢ç©¶æ¨¡å‹åœ¨ä¸åŒå¤æ‚åº¦æ•°æ®ä¸Šçš„å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶åˆ†æå…¶å¯¹ä¸åŒç±»å‹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç®€åŒ–åŸå§‹æ–‡æœ¬ï¼›2) åœ¨åŸå§‹æ–‡æœ¬å’Œç®€åŒ–æ–‡æœ¬ä¸Šåˆ†åˆ«é¢„è®­ç»ƒä¸åŒè§„æ¨¡çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆ28M-500Mï¼‰ï¼›3) åœ¨å¾®è°ƒè®¾ç½®ä¸‹ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼›4) åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒç±»å‹çš„çŸ¥è¯†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†æ–‡æœ¬å¤æ‚åº¦å¯¹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„å½±å“ï¼Œå¹¶æ­ç¤ºäº†æ–‡æœ¬å¤æ‚åº¦ä¸æ¨¡å‹å®¹é‡ã€ä¸‹æ¸¸ä»»åŠ¡ç±»å‹ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚é€šè¿‡å¯¹æ¯”åœ¨ä¸åŒå¤æ‚åº¦æ•°æ®ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå‘ç°æ–‡æœ¬å¤æ‚åº¦å¯¹å¾®è°ƒå’Œé›¶æ ·æœ¬å­¦ä¹ çš„å½±å“ä¸åŒï¼Œä¸ºæ•°æ®é€‰æ‹©å’Œæ¨¡å‹è®­ç»ƒæä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç®€åŒ–ï¼Œä¿è¯ç®€åŒ–åçš„æ–‡æœ¬åœ¨è¯­ä¹‰ä¸Šä¸åŸå§‹æ–‡æœ¬ä¸€è‡´ï¼›2) é¢„è®­ç»ƒä¸åŒè§„æ¨¡çš„è¯­è¨€æ¨¡å‹ï¼Œä»¥ç ”ç©¶æ¨¡å‹å®¹é‡å¯¹æ–‡æœ¬å¤æ‚åº¦å½±å“çš„è°ƒèŠ‚ä½œç”¨ï¼›3) åœ¨å¾®è°ƒå’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒç±»å‹çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä»¥åˆ†ææ–‡æœ¬å¤æ‚åº¦å¯¹ä¸åŒç±»å‹çŸ¥è¯†å­¦ä¹ çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ–‡æœ¬å¤æ‚åº¦å¯¹å›°æƒ‘åº¦æœ‰æ˜¾è‘—å½±å“ï¼Œå°æ¨¡å‹åœ¨ç®€å•æ–‡æœ¬ä¸Šçš„å›°æƒ‘åº¦ä¸‹é™è¾ƒå°‘ã€‚åœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­ï¼Œç®€å•æ–‡æœ¬æœ‰åˆ©äºè¯­è¨€çŸ¥è¯†ä»»åŠ¡ï¼Œå¤æ‚æ–‡æœ¬æœ‰åˆ©äºä¸–ç•ŒçŸ¥è¯†å’Œå®ä½“è·Ÿè¸ªä»»åŠ¡ã€‚å¾®è°ƒç»“æœæ˜¾ç¤ºæ–‡æœ¬å¤æ‚åº¦å½±å“è¾ƒå°ï¼Œè¡¨æ˜æ¨¡å‹å¯ä»¥é€šè¿‡å¾®è°ƒé€‚åº”ä¸åŒå¤æ‚åº¦çš„æ–‡æœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ•°æ®é€‰æ‹©å’Œé¢„å¤„ç†ã€‚é€šè¿‡äº†è§£æ–‡æœ¬å¤æ‚åº¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¯ä»¥é’ˆå¯¹ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ•°æ®ï¼Œæé«˜æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥æŒ‡å¯¼æ•™è‚²é¢†åŸŸçš„æ–‡æœ¬ç”Ÿæˆï¼Œä¾‹å¦‚ç”Ÿæˆé€‚åˆä¸åŒå¹´é¾„æ®µå­¦ç”Ÿçš„é˜…è¯»ææ–™ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Improving pretraining data quality and size is known to boost downstream performance, but the role of text complexity--how hard a text is to read--remains less explored. We reduce surface-level complexity (shorter sentences, simpler words, simpler structure) while keeping core content approximately constant and ask: (i) How does complexity affect language modeling across model sizes? (ii) Can useful representations be learned from simpler text alone? (iii) How does pretraining text complexity influence downstream language understanding? We simplify human-written texts using a large language model, pretrain causal models (28M-500M) from scratch on original vs. simplified data, and evaluate them in fine-tuning and zero-shot setups. We find that perplexity is sensitive to the interaction between model capacity and text complexity--smaller models degrade far less on simpler texts--while text complexity has little impact on fine-tuning evaluations, with zero-shot evaluations indicating that simpler texts benefit performance on linguistic knowledge tasks, whereas more complex texts favor tasks requiring world knowledge and entity tracking. Our findings suggest that different types of data diversity affect transfer and zero-shot performance differently, providing insight into tailoring data curation to specific goals.

