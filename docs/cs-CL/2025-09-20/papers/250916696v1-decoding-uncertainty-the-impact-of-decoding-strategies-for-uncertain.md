---
layout: default
title: Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models
---

# Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16696" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16696v1</a>
  <a href="https://arxiv.org/pdf/2509.16696.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16696v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16696v1', 'Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wataru Hashimoto, Hidetaka Kamigaito, Taro Watanabe

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

**å¤‡æ³¨**: Accepted at EMNLP 2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¯¹æ¯”æœç´¢æå‡å¤§è¯­è¨€æ¨¡å‹ä¸ç¡®å®šæ€§ä¼°è®¡çš„æœ‰æ•ˆæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸ç¡®å®šæ€§ä¼°è®¡` `è§£ç ç­–ç•¥` `å¯¹æ¯”æœç´¢` `åå¥½å¯¹é½` `ç›‘ç£å¾®è°ƒ` `æ–‡æœ¬ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä¸ç¡®å®šæ€§æ—¶ï¼Œå¿½ç•¥äº†è§£ç ç­–ç•¥çš„å½±å“ï¼Œå¯èƒ½å¯¼è‡´è¯„ä¼°ç»“æœä¸å‡†ç¡®ã€‚
2. è¯¥ç ”ç©¶æ¢ç´¢ä¸åŒè§£ç ç­–ç•¥å¯¹LLMä¸ç¡®å®šæ€§ä¼°è®¡çš„å½±å“ï¼Œæ—¨åœ¨æ‰¾åˆ°æ›´æœ‰æ•ˆçš„è§£ç ç­–ç•¥ä»¥æå‡ä¸ç¡®å®šæ€§è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹æ¯”æœç´¢åœ¨åå¥½å¯¹é½çš„LLMä¸­èƒ½äº§ç”Ÿæ›´ä¼˜çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä½†åœ¨ä»…ç»è¿‡ç›‘ç£å¾®è°ƒçš„æ¨¡å‹ä¸­æ•ˆæœä¸ç¨³å®šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§£ç ç­–ç•¥ä¼šå½±å“è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿›è€Œå½±å“ç”Ÿæˆè´¨é‡å’Œä¸ç¡®å®šæ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è§£ç ç­–ç•¥å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç¡®å®šæ€§ä¼°è®¡çš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œå¯¹æ¯”æœç´¢ï¼ˆContrastive Searchï¼‰èƒ½å¤Ÿå‡è½»é‡å¤ç”Ÿæˆï¼Œä»è€Œåœ¨ä¸€ç³»åˆ—åå¥½å¯¹é½çš„LLMä¸­äº§ç”Ÿæ›´å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå½“æ¨¡å‹ä»…ç»è¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è€Œæ²¡æœ‰æ˜¾å¼å¯¹é½æ—¶ï¼Œè¿™äº›ç­–ç•¥çš„ä¼˜åŠ¿æœ‰æ—¶ä¼šå‘ç”Ÿåˆ†æ­§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•é€šå¸¸å¿½ç•¥äº†è§£ç ç­–ç•¥çš„å½±å“ã€‚ä¸åŒçš„è§£ç ç­–ç•¥ä¼šæ”¹å˜æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿›è€Œå½±å“ä¸ç¡®å®šæ€§çš„è¯„ä¼°ã€‚å› æ­¤ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„è§£ç ç­–ç•¥ä»¥æå‡LLMä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å®éªŒå¯¹æ¯”ä¸åŒçš„è§£ç ç­–ç•¥ï¼ˆå¦‚å¯¹æ¯”æœç´¢ã€è´ªå©ªè§£ç ç­‰ï¼‰åœ¨LLMä¸ç¡®å®šæ€§ä¼°è®¡ä¸Šçš„è¡¨ç°ï¼Œä»è€Œæ‰¾åˆ°èƒ½å¤Ÿäº§ç”Ÿæ›´å¯é ä¸ç¡®å®šæ€§ä¼°è®¡çš„è§£ç ç­–ç•¥ã€‚å¯¹æ¯”æœç´¢é€šè¿‡æƒ©ç½šä¸å·²ç”Ÿæˆtokenç›¸ä¼¼çš„tokenï¼Œä»è€Œå‡è½»é‡å¤ç”Ÿæˆï¼Œå¯èƒ½æœ‰åŠ©äºæå‡ä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶ä¸»è¦é‡‡ç”¨å®éªŒå¯¹æ¯”çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œé€‰æ‹©ä¸€ç³»åˆ—å…·æœ‰ä»£è¡¨æ€§çš„LLMï¼ŒåŒ…æ‹¬ç»è¿‡åå¥½å¯¹é½çš„æ¨¡å‹å’Œä»…ç»è¿‡ç›‘ç£å¾®è°ƒçš„æ¨¡å‹ã€‚ç„¶åï¼Œä½¿ç”¨ä¸åŒçš„è§£ç ç­–ç•¥ç”Ÿæˆæ–‡æœ¬ï¼Œå¹¶è®¡ç®—ç›¸åº”çš„ä¸ç¡®å®šæ€§æŒ‡æ ‡ã€‚æœ€åï¼Œå¯¹æ¯”ä¸åŒè§£ç ç­–ç•¥ä¸‹ä¸ç¡®å®šæ€§æŒ‡æ ‡çš„è¡¨ç°ï¼Œåˆ†æå…¶å¯¹ä¸ç¡®å®šæ€§ä¼°è®¡çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå…³æ³¨äº†è§£ç ç­–ç•¥å¯¹LLMä¸ç¡®å®šæ€§ä¼°è®¡çš„å½±å“ï¼Œå¹¶å‘ç°å¯¹æ¯”æœç´¢åœ¨åå¥½å¯¹é½çš„LLMä¸­èƒ½å¤Ÿäº§ç”Ÿæ›´å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚è¿™ä¸ºæå‡LLMä¸ç¡®å®šæ€§è¯„ä¼°çš„å‡†ç¡®æ€§æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨äº†å¤šç§è§£ç ç­–ç•¥ï¼ŒåŒ…æ‹¬è´ªå©ªè§£ç ã€æŸæœç´¢ã€å¯¹æ¯”æœç´¢ç­‰ã€‚ä¸ç¡®å®šæ€§æŒ‡æ ‡çš„é€‰æ‹©å¯èƒ½åŒ…æ‹¬æ¨¡å‹è¾“å‡ºæ¦‚ç‡çš„ç†µã€æ–¹å·®ç­‰ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ï¼ˆå¦‚å¯¹æ¯”æœç´¢çš„æƒ©ç½šç³»æ•°ï¼‰éœ€è¦æ ¹æ®å®éªŒç»“æœè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹æ¯”æœç´¢åœ¨åå¥½å¯¹é½çš„LLMä¸­ï¼Œå¹³å‡è€Œè¨€èƒ½å¤Ÿäº§ç”Ÿæ›´å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚ç„¶è€Œï¼Œåœ¨ä»…ç»è¿‡ç›‘ç£å¾®è°ƒçš„æ¨¡å‹ä¸­ï¼Œä¸åŒè§£ç ç­–ç•¥çš„ä¼˜åŠ¿å¹¶ä¸ç¨³å®šï¼Œè¿™è¡¨æ˜æ¨¡å‹å¯¹é½æ–¹å¼ä¹Ÿä¼šå½±å“è§£ç ç­–ç•¥çš„é€‰æ‹©ã€‚å…·ä½“çš„ä¸ç¡®å®šæ€§æŒ‡æ ‡æå‡å¹…åº¦æœªçŸ¥ï¼Œéœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¯¹LLMç”Ÿæˆå†…å®¹å¯é æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ç­‰ã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„è§£ç ç­–ç•¥ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°LLMç”Ÿæˆå†…å®¹çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œè¾…åŠ©å†³ç­–è€…åšå‡ºæ›´æ˜æ™ºçš„åˆ¤æ–­ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿæœ‰åŠ©äºæå‡LLMåœ¨å¼€æ”¾åŸŸå¯¹è¯ã€çŸ¥è¯†é—®ç­”ç­‰ä»»åŠ¡ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Decoding strategies manipulate the probability distribution underlying the output of a language model and can therefore affect both generation quality and its uncertainty. In this study, we investigate the impact of decoding strategies on uncertainty estimation in Large Language Models (LLMs). Our experiments show that Contrastive Search, which mitigates repetition, yields better uncertainty estimates on average across a range of preference-aligned LLMs. In contrast, the benefits of these strategies sometimes diverge when the model is only post-trained with supervised fine-tuning, i.e. without explicit alignment.

