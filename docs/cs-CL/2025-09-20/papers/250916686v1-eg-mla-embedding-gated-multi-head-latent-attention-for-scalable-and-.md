---
layout: default
title: EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs
---

# EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16686" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.16686v1</a>
  <a href="https://arxiv.org/pdf/2509.16686.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16686v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16686v1', 'EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhengge Cai, Haowen Hou

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-20

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EG-MLAÔºåÈÄöËøáÂµåÂÖ•Èó®ÊéßÊú∫Âà∂ÂéãÁº©KVÁºìÂ≠òÔºåÊèêÂçáLLMÊé®ÁêÜÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `KVÁºìÂ≠òÂéãÁº©` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `ÂµåÂÖ•Èó®Êéß` `Ê®°ÂûãÊé®ÁêÜÂä†ÈÄü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâLLMÊé®ÁêÜÈù¢‰∏¥KVÁºìÂ≠òËøáÂ§ßÁöÑÊåëÊàòÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏ãÁöÑÈÉ®ÁΩ≤„ÄÇ
2. EG-MLAÈÄöËøáÂú®ÊΩúÂú®Á©∫Èó¥ÂºïÂÖ•ÂµåÂÖ•Èó®ÊéßÊú∫Âà∂ÔºåÂÆûÁé∞ÂØπÂéãÁº©KVÂêëÈáèÁöÑÁªÜÁ≤íÂ∫¶Ë∞ÉÂà∂„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåEG-MLAÂú®ÊòæËëóÂáèÂ∞ëKVÁºìÂ≠òÁöÑÂêåÊó∂ÔºåÊèêÂçá‰∫Ü‰ªªÂä°ÂáÜÁ°ÆÊÄßÔºåÂπ∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂáèÂ∞ëÈîÆÂÄº(KV)ÁºìÂ≠òÂ§ßÂ∞èÊòØÂÆûÁé∞Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)È´òÊïàÊé®ÁêÜÁöÑÂÖ≥ÈîÆ‰∏ÄÊ≠•ÔºåÂ∞§ÂÖ∂ÊòØÂú®Âª∂ËøüÂíåÂÜÖÂ≠òÂèóÈôêÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÂ§öÂ§¥Ê≥®ÊÑèÂäõ(MHA)ËôΩÁÑ∂ÂÖ∑ÊúâÂº∫Â§ßÁöÑË°®ÂæÅËÉΩÂäõÔºå‰ΩÜ‰ºö‰∫ßÁîüÊòæËëóÁöÑÂÜÖÂ≠òÂºÄÈîÄ„ÄÇÂ§öÂ§¥ÊΩúÂú®Ê≥®ÊÑèÂäõ(MLA)ÈÄöËøáÂ∞ÜKVË°®ÂæÅÂéãÁº©Âà∞ÂÖ±‰∫´ÊΩúÂú®Á©∫Èó¥Êù•ÁºìËß£Ëøô‰∏™ÈóÆÈ¢òÔºå‰ªéËÄåÂú®ÊÄßËÉΩÂíåÁºìÂ≠òÊïàÁéá‰πãÈó¥ÂèñÂæóÊõ¥Â•ΩÁöÑÂπ≥Ë°°„ÄÇËôΩÁÑ∂MLAÂ∑≤ÁªèÂÆûÁé∞‰∫ÜÊòæËëóÁöÑKVÁºìÂ≠òÂáèÂ∞ëÔºå‰ΩÜÂú®‰∏çÊçüÂ§±ÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÔºåËøõ‰∏ÄÊ≠•ÂéãÁº©ÁöÑÁ©∫Èó¥‰ªçÁÑ∂ÊúâÈôê„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂµåÂÖ•Èó®ÊéßÂ§öÂ§¥ÊΩúÂú®Ê≥®ÊÑèÂäõ(EG-MLA)ÔºåËøôÊòØMLAÁöÑ‰∏ÄÁßçÊñ∞È¢ñÊâ©Â±ïÔºåÂÆÉËøõ‰∏ÄÊ≠•ÂáèÂ∞ë‰∫ÜKVÁºìÂ≠òÂ§ßÂ∞èÔºåÂêåÊó∂Â¢ûÂº∫‰∫ÜË°®ÂæÅË°®ËææËÉΩÂäõ„ÄÇEG-MLAÂºïÂÖ•‰∫Ü‰∏ÄÁßçtokenÁâπÂÆöÁöÑÂµåÂÖ•Èó®ÊéßÊú∫Âà∂ÔºåÂ∫îÁî®‰∫éÊΩúÂú®Á©∫Èó¥Ôºå‰ªéËÄåËÉΩÂ§ü‰ª•ÊúÄÂ∞èÁöÑÈ¢ùÂ§ñËÆ°ÁÆóÈáèÂØπÂéãÁº©ÁöÑKVÂêëÈáèËøõË°åÁªÜÁ≤íÂ∫¶Ë∞ÉÂà∂„ÄÇ‰∏éMHAÁõ∏ÊØîÔºåEG-MLAÂÆûÁé∞‰∫ÜË∂ÖËøá91.6%ÁöÑKVÁºìÂ≠òÂ§ßÂ∞èÂáèÂ∞ëÔºåËÄåÊÄßËÉΩ‰∏ãÈôçÂèØÂøΩÁï•‰∏çËÆ°„ÄÇÁõ∏ÂØπ‰∫éMLAÔºåEG-MLAÂú®‰∏çÂêåÁöÑÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÂßãÁªàÊèêÈ´ò‰∫Ü‰ªªÂä°ÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂ÂÆûÁé∞‰∫ÜÈ´òËææ59.9%ÁöÑÈ¢ùÂ§ñÂÜÖÂ≠òËäÇÁúÅ„ÄÇÊàë‰ª¨ÁöÑÁêÜËÆ∫ÂàÜÊûêÂº∫Ë∞É‰∫ÜÂµåÂÖ•Èó®ÊéßÂ¶Ç‰ΩïËØ±ÂØºÈöêÂºèÈ´òÈò∂‰∫§‰∫íÔºåÁªèÈ™åËØÑ‰º∞Ë°®Êòé‰∫ÜË∑®Ê®°ÂûãËßÑÊ®°ÂíåÂéãÁº©ÊñπÊ°àÁöÑÈ≤ÅÊ£íÊ≥õÂåñËÉΩÂäõ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàë‰ª¨ÊàêÂäüÂú∞Â∞ÜEG-MLAÊâ©Â±ïÂà∞Ë∂ÖËøá10‰∫ø‰∏™ÂèÇÊï∞ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§ßÂûãLLMÈÉ®ÁΩ≤‰∏≠ÁöÑÂÆûÈôÖÂèØË°åÊÄß„ÄÇËøô‰∫õÁªìÊûúÂ∞ÜEG-MLAÁ°ÆÁ´ã‰∏∫‰∏ÄÁßçÂÜÖÂ≠òÂíåËÆ°ÁÆóÊïàÁéáÈ´òÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÂú®Áé∞‰ª£LLM‰∏≠ÂÆûÁé∞ÂèØÊâ©Â±ïÁöÑ„ÄÅÈ´òÊÄßËÉΩÁöÑÊé®ÁêÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÈúÄË¶ÅÂ≠òÂÇ®Â§ßÈáèÁöÑKey-Value (KV) ÁºìÂ≠òÔºåËøôÂØºËá¥‰∫ÜÈ´òÊòÇÁöÑÂÜÖÂ≠òÈúÄÊ±ÇÂíåËÆ°ÁÆóÊàêÊú¨ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈïøÂ∫èÂàóÂíåÈ´òÂπ∂ÂèëÂú∫ÊôØ‰∏ã„ÄÇÁé∞ÊúâÁöÑÂ§öÂ§¥Ê≥®ÊÑèÂäõÔºàMHAÔºâÊú∫Âà∂ËôΩÁÑ∂ÂÖ∑ÊúâÂº∫Â§ßÁöÑË°®ÂæÅËÉΩÂäõÔºå‰ΩÜÂÖ∂KVÁºìÂ≠òÂ§ßÂ∞è‰∏éÂ∫èÂàóÈïøÂ∫¶ÂíåÊ®°ÂûãËßÑÊ®°ÊàêÊ≠£ÊØîÔºåÊàê‰∏∫LLMÈÉ®ÁΩ≤ÁöÑÁì∂È¢à„ÄÇMulti-head Latent Attention (MLA) Â∞ùËØïÈÄöËøáÂéãÁº©KVË°®ÂæÅÊù•ÁºìËß£Ëøô‰∏™ÈóÆÈ¢òÔºå‰ΩÜ‰ªçÁÑ∂Â≠òÂú®Ëøõ‰∏ÄÊ≠•ÂéãÁº©ÁöÑÁ©∫Èó¥Ôºå‰∏îÂèØËÉΩÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEG-MLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂú®MLAÁöÑÂü∫Á°Ä‰∏äÔºåÂºïÂÖ•‰∏Ä‰∏™tokenÁâπÂÆöÁöÑÂµåÂÖ•Èó®ÊéßÊú∫Âà∂Ôºå‰ΩúÁî®‰∫éÂéãÁº©ÂêéÁöÑÊΩúÂú®Á©∫Èó¥„ÄÇËøô‰∏™Èó®ÊéßÊú∫Âà∂ÂÖÅËÆ∏Ê®°ÂûãÊ†πÊçÆÊØè‰∏™tokenÁöÑÁâπÊÄßÔºåÂØπKVÂêëÈáèËøõË°åÁªÜÁ≤íÂ∫¶Ë∞ÉÂà∂Ôºå‰ªéËÄåÂú®Ëøõ‰∏ÄÊ≠•ÂéãÁº©KVÁºìÂ≠òÁöÑÂêåÊó∂ÔºåÂ¢ûÂº∫Ê®°ÂûãÁöÑË°®ÂæÅËÉΩÂäõ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåEG-MLAÊó®Âú®ÂÆûÁé∞Êõ¥È´òÁöÑÂÜÖÂ≠òÊïàÁéáÂíåÊõ¥Âº∫ÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEG-MLAÁöÑÊï¥‰ΩìÊ°ÜÊû∂Âü∫‰∫éMLAÔºå‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ËæìÂÖ•tokenÁªèËøáÂµåÂÖ•Â±ÇÂæóÂà∞token embeddingÔºõ2) ‰ΩøÁî®Á∫øÊÄßÂèòÊç¢Â∞ÜQuery„ÄÅKeyÂíåValueÊäïÂΩ±Âà∞ÊΩúÂú®Á©∫Èó¥Ôºõ3) Âú®ÊΩúÂú®Á©∫Èó¥‰∏≠Â∫îÁî®Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºõ4) ÂºïÂÖ•ÂµåÂÖ•Èó®ÊéßÊú∫Âà∂ÔºåÂØπÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑKVÂêëÈáèËøõË°åË∞ÉÂà∂Ôºõ5) Â∞ÜË∞ÉÂà∂ÂêéÁöÑKVÂêëÈáèÁî®‰∫éÂêéÁª≠ÁöÑÊ≥®ÊÑèÂäõËÆ°ÁÆó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEG-MLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂµåÂÖ•Èó®ÊéßÊú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏çÂêåÔºåEG-MLA‰∏çÊòØÁõ¥Êé•ÂØπÂéüÂßãÁöÑKVÂêëÈáèËøõË°åÊìç‰ΩúÔºåËÄåÊòØÂú®ÂéãÁº©ÂêéÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ÔºåÈÄöËøá‰∏Ä‰∏™tokenÁâπÂÆöÁöÑÈó®ÊéßÂêëÈáèÊù•ÊéßÂà∂‰ø°ÊÅØÁöÑÊµÅÂä®„ÄÇËøôÁßçÈó®ÊéßÊú∫Âà∂ÂÖÅËÆ∏Ê®°ÂûãÊ†πÊçÆÊØè‰∏™tokenÁöÑÈáçË¶ÅÊÄßÔºåÈÄâÊã©ÊÄßÂú∞‰øùÁïôÊàñÊäëÂà∂Êüê‰∫õ‰ø°ÊÅØÔºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÊïàÁöÑË°®ÂæÅÂ≠¶‰π†„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂµåÂÖ•Èó®ÊéßÊú∫Âà∂ÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÊñπÂºèÊòØÔºöÈ¶ñÂÖàÔºåÂ∞Ütoken embeddingÈÄöËøá‰∏Ä‰∏™Á∫øÊÄßÂ±ÇÊò†Â∞ÑÂà∞Èó®ÊéßÂêëÈáèÔºõÁÑ∂ÂêéÔºåÂ∞ÜÈó®ÊéßÂêëÈáè‰∏éÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑKVÂêëÈáèËøõË°åÈÄêÂÖÉÁ¥†Áõ∏‰πòÔºåÂæóÂà∞Ë∞ÉÂà∂ÂêéÁöÑKVÂêëÈáè„ÄÇÈó®ÊéßÂêëÈáèÁöÑÁª¥Â∫¶‰∏éÊΩúÂú®Á©∫Èó¥ÁöÑÁª¥Â∫¶Áõ∏ÂêåÔºå‰ªéËÄåÂèØ‰ª•ÂØπÊØè‰∏™Áª¥Â∫¶ËøõË°åÁã¨Á´ãÁöÑÊéßÂà∂„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂØπÈó®ÊéßÂêëÈáèËøõË°å‰∫ÜÂΩí‰∏ÄÂåñÂ§ÑÁêÜÔºå‰ª•‰øùËØÅËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EG-MLAÂú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÁõ∏ÂØπ‰∫éMHAÔºåÂÆûÁé∞‰∫ÜË∂ÖËøá91.6%ÁöÑKVÁºìÂ≠òÂ§ßÂ∞èÂáèÂ∞ëÔºå‰∏îÊÄßËÉΩ‰∏ãÈôçÂèØÂøΩÁï•‰∏çËÆ°„ÄÇ‰∏éMLAÁõ∏ÊØîÔºåEG-MLAÂú®ÊèêÈ´ò‰ªªÂä°ÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫ÜÈ´òËææ59.9%ÁöÑÈ¢ùÂ§ñÂÜÖÂ≠òËäÇÁúÅ„ÄÇÊ≠§Â§ñÔºåEG-MLAÊàêÂäüÊâ©Â±ïÂà∞Ë∂ÖËøá10‰∫ø‰∏™ÂèÇÊï∞ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Â§ßËßÑÊ®°LLMÈÉ®ÁΩ≤‰∏≠ÁöÑÂèØË°åÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EG-MLAÈÄÇÁî®‰∫éÂØπÂÜÖÂ≠òÂíåËÆ°ÁÆóËµÑÊ∫êÊúâ‰∏•Ê†ºË¶ÅÊ±ÇÁöÑLLMÈÉ®ÁΩ≤Âú∫ÊôØÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§á„ÄÅËæπÁºòËÆ°ÁÆóÂíåÈ´òÂπ∂ÂèëÂú®Á∫øÊúçÂä°„ÄÇÈÄöËøáÂáèÂ∞ëKVÁºìÂ≠òÂ§ßÂ∞èÔºåEG-MLAÂèØ‰ª•Èôç‰ΩéÁ°¨‰ª∂ÊàêÊú¨ÔºåÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶ÔºåÂπ∂ÊîØÊåÅÊõ¥Â§ßËßÑÊ®°ÁöÑÊ®°ÂûãÈÉ®ÁΩ≤„ÄÇËØ•ÊäÄÊúØËøòÊúâÂä©‰∫éÊé®Âä®LLMÂú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏ãÁöÑÂ∫îÁî®Ôºå‰æãÂ¶ÇÊô∫ËÉΩÂä©Êâã„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÊêúÁ¥¢ÂíåÊú∫Âô®ÁøªËØë„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reducing the key-value (KV) cache size is a crucial step toward enabling efficient inference in large language models (LLMs), especially under latency and memory constraints. While Multi-Head Attention (MHA) offers strong representational power, it incurs significant memory overhead. Recent work on Multi-head Latent Attention (MLA) mitigates this by compressing KV representations into a shared latent space, achieving a better trade-off between performance and cache efficiency. While MLA already achieves significant KV cache reduction, the scope for further compression remains limited without performance loss. In this paper, we propose \textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel extension of MLA that further reduces KV cache size while enhancing representational expressiveness. EG-MLA introduces a token-specific embedding gating mechanism applied in the latent space, enabling fine-grained modulation of compressed KV vectors with minimal additional computation. Compared to MHA, EG-MLA achieves over 91.6\% reduction in KV cache size with negligible performance degradation. Relative to MLA, EG-MLA consistently improves task accuracy across diverse reasoning benchmarks while achieving up to 59.9\% additional memory savings. Our theoretical analysis highlights how embedding gating induces implicit high-order interactions, and empirical evaluations demonstrate robust generalization across model scales and compression regimes. Notably, we successfully scale EG-MLA to over 1 billion parameters, demonstrating its practical viability for large-scale LLM deployment. These results establish EG-MLA as a memory- and compute-efficient attention mechanism that enables scalable, high-performance inference in modern LLMs.

