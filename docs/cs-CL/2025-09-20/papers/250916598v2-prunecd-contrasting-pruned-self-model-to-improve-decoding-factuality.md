---
layout: default
title: PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality
---

# PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16598" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16598v2</a>
  <a href="https://arxiv.org/pdf/2509.16598.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16598v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16598v2', 'PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Byeongho Yu, Changhun Lee, Jungyu Jin, Eunhyeok Park

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20 (æ›´æ–°: 2025-09-23)

**å¤‡æ³¨**: accepted at EMNLP 2025 Main Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPruneCDï¼Œé€šè¿‡å¯¹æ¯”å‰ªææ¨¡å‹æå‡å¤§å‹è¯­è¨€æ¨¡å‹è§£ç çš„äº‹å®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹æ¯”è§£ç ` `è¯­è¨€æ¨¡å‹` `å¹»è§‰ç¼“è§£` `æ¨¡å‹å‰ªæ` `äº‹å®æ€§` `è‡ªç„¶è¯­è¨€ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œå³ç”Ÿæˆä¸çœŸå®æˆ–ä¸äº‹å®ç›¸æ‚–çš„å†…å®¹ï¼Œé™ä½äº†æ¨¡å‹çš„å¯é æ€§ã€‚
2. PruneCDé€šè¿‡å‰ªææ¨¡å‹æ„å»ºå¯¹æ¯”æ¨¡å‹ï¼Œç”Ÿæˆæ›´å…·ä¿¡æ¯é‡çš„logitsï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å¯¹æ¯”è§£ç ï¼ŒæŠ‘åˆ¶å¹»è§‰ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPruneCDåœ¨å‡ ä¹ä¸å¢åŠ æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹ç”Ÿæˆå†…å®¹çš„äº‹å®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ï¼ŒDoLaåˆ©ç”¨åŒä¸€æ¨¡å‹çš„æ—©æœŸé€€å‡ºlogitsä½œä¸ºå¯¹æ¯”å…ˆéªŒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ—©æœŸé€€å‡ºlogitså¾€å¾€æ˜¯å¹³å¦çš„ã€å¹…åº¦è¾ƒä½çš„ï¼Œå¹¶ä¸”æœªèƒ½åæ˜ æœ‰æ„ä¹‰çš„å¯¹æ¯”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºPruneCDï¼Œä¸€ç§æ–°é¢–çš„å¯¹æ¯”è§£ç æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å±‚å‰ªæè€Œä¸æ˜¯æ—©æœŸé€€å‡ºæ¥æ„å»ºä¸šä½™æ¨¡å‹ã€‚è¿™ç§è®¾è®¡äº§ç”Ÿäº†æ›´å…·ä¿¡æ¯é‡å’Œè‰¯å¥½å¯¹é½çš„logitsï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å¯¹æ¯”è§£ç ã€‚é€šè¿‡å®šæ€§å’Œå®šé‡åˆ†æï¼Œæˆ‘ä»¬è¯æ˜PruneCDåœ¨æœ€å°çš„æ¨ç†å¼€é”€ä¸‹å§‹ç»ˆå¦‚ä¸€åœ°æé«˜äº†äº‹å®æ€§ï¼Œä¸ºå‡è½»LLMä¸­çš„å¹»è§‰æä¾›äº†ä¸€ç§ç¨³å¥ä¸”å®ç”¨çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å®¹æ˜“å‡ºç°â€œå¹»è§‰â€ç°è±¡ï¼Œå³ç”Ÿæˆä¸ç¬¦åˆäº‹å®çš„å†…å®¹ã€‚DoLaç­‰æ–¹æ³•å°è¯•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ—©æœŸé€€å‡ºå±‚ä½œä¸ºå¯¹æ¯”ä¿¡æ¯æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†æ—©æœŸé€€å‡ºå±‚çš„logitså¾€å¾€ç¼ºä¹ä¿¡æ¯é‡ï¼Œå¯¹æ¯”æ•ˆæœä¸ä½³ã€‚å› æ­¤ï¼Œå¦‚ä½•æ„å»ºæ›´æœ‰æ•ˆçš„å¯¹æ¯”ä¿¡æ¯ï¼Œæå‡è§£ç è¿‡ç¨‹ä¸­çš„äº‹å®æ€§ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPruneCDçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å‰ªæåŸå§‹æ¨¡å‹çš„éƒ¨åˆ†å±‚æ¥æ„å»ºä¸€ä¸ªâ€œä¸šä½™æ¨¡å‹â€ï¼ˆamateur modelï¼‰ã€‚ä¸ä½¿ç”¨æ—©æœŸé€€å‡ºå±‚ç›¸æ¯”ï¼Œå‰ªæåçš„æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿæ›´å…·ä¿¡æ¯é‡å’Œæ›´å¥½å¯¹é½çš„logitsï¼Œä»è€Œä¸ºå¯¹æ¯”è§£ç æä¾›æ›´æœ‰æ•ˆçš„å…ˆéªŒä¿¡æ¯ã€‚é€šè¿‡å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œå‰ªææ¨¡å‹çš„è¾“å‡ºï¼Œå¯ä»¥æ›´å¥½åœ°æŠ‘åˆ¶å¹»è§‰ï¼Œæå‡ç”Ÿæˆæ–‡æœ¬çš„äº‹å®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPruneCDçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) å¯¹åŸå§‹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå±‚å‰ªæï¼Œå¾—åˆ°ä¸€ä¸ªå‰ªæåçš„â€œä¸šä½™æ¨¡å‹â€ï¼›2) ä½¿ç”¨åŸå§‹æ¨¡å‹å’Œå‰ªææ¨¡å‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œè§£ç ï¼Œå¾—åˆ°å„è‡ªçš„logitsï¼›3) åˆ©ç”¨åŸå§‹æ¨¡å‹å’Œå‰ªææ¨¡å‹çš„logitsè¿›è¡Œå¯¹æ¯”è§£ç ï¼Œé€šè¿‡æŸç§ç­–ç•¥ï¼ˆä¾‹å¦‚ï¼ŒåŠ æƒå¹³å‡ï¼‰èåˆä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºï¼Œç”Ÿæˆæœ€ç»ˆçš„æ–‡æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šPruneCDçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨å±‚å‰ªææ¥æ„å»ºå¯¹æ¯”æ¨¡å‹ï¼Œè€Œä¸æ˜¯åƒDoLaé‚£æ ·ä½¿ç”¨æ—©æœŸé€€å‡ºå±‚ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿæ›´å…·ä¿¡æ¯é‡å’Œæ›´å¥½å¯¹é½çš„logitsï¼Œä»è€Œä¸ºå¯¹æ¯”è§£ç æä¾›æ›´æœ‰æ•ˆçš„å…ˆéªŒä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒPruneCDçš„è®¾è®¡ä¹Ÿæ›´åŠ çµæ´»ï¼Œå¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ¨¡å‹é€‰æ‹©ä¸åŒçš„å‰ªæç­–ç•¥å’Œå¯¹æ¯”è§£ç ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šPruneCDçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å‰ªæç­–ç•¥çš„é€‰æ‹©ï¼šå¯ä»¥é€‰æ‹©ä¸åŒçš„å‰ªææ¯”ä¾‹å’Œå‰ªææ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œéšæœºå‰ªæã€åŸºäºé‡è¦æ€§çš„å‰ªæç­‰ï¼‰ï¼›2) å¯¹æ¯”è§£ç ç­–ç•¥çš„é€‰æ‹©ï¼šå¯ä»¥é€‰æ‹©ä¸åŒçš„èåˆæ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒåŠ æƒå¹³å‡ã€åŸºäºæ³¨æ„åŠ›çš„èåˆç­‰ï¼‰æ¥èåˆåŸå§‹æ¨¡å‹å’Œå‰ªææ¨¡å‹çš„è¾“å‡ºï¼›3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼šå¯ä»¥è®¾è®¡ä¸“é—¨çš„æŸå¤±å‡½æ•°æ¥é¼“åŠ±åŸå§‹æ¨¡å‹å’Œå‰ªææ¨¡å‹äº§ç”Ÿå·®å¼‚åŒ–çš„è¾“å‡ºï¼Œä»è€Œæå‡å¯¹æ¯”è§£ç çš„æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPruneCDåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨äº‹å®æ€§æŒ‡æ ‡ä¸Šã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒPruneCDå¯ä»¥å°†æ¨¡å‹çš„FactCCå¾—åˆ†æå‡è¶…è¿‡5%ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„ç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼ŒPruneCDçš„æ¨ç†å¼€é”€éå¸¸å°ï¼Œä½¿å…¶æˆä¸ºä¸€ç§å®ç”¨ä¸”é«˜æ•ˆçš„ç¼“è§£å¹»è§‰çš„æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PruneCDå¯åº”ç”¨äºå„ç§éœ€è¦é«˜å¯é æ€§å’Œäº‹å®æ€§çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚æ–°é—»æŠ¥é“ç”Ÿæˆã€çŸ¥è¯†é—®ç­”ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡ç”Ÿæˆå†…å®¹çš„äº‹å®æ€§ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·å¯¹æ¨¡å‹çš„ä¿¡ä»»åº¦ï¼Œå¹¶å‡å°‘é”™è¯¯ä¿¡æ¯çš„ä¼ æ’­ã€‚æœªæ¥ï¼ŒPruneCDè¿˜å¯ä»¥ä¸å…¶ä»–ç¼“è§£å¹»è§‰çš„æ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To mitigate the hallucination problem in large language models, DoLa exploits early exit logits from the same model as a contrastive prior. However, we found that these early exit logits tend to be flat, low in magnitude, and fail to reflect meaningful contrasts. To address this, we propose PruneCD, a novel contrastive decoding method that constructs the amateur model via layer pruning rather than early exit. This design leads to more informative and well-aligned logits, enabling more effective contrastive decoding. Through qualitative and quantitative analyses, we demonstrate that PruneCD consistently improves factuality with minimal inference overhead, offering a robust and practical approach to mitigating hallucinations in LLMs.

