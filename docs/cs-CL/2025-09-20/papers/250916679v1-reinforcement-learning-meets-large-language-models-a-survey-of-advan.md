---
layout: default
title: Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle
---

# Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16679" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16679v1</a>
  <a href="https://arxiv.org/pdf/2509.16679.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16679v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16679v1', 'Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Keliang Liu, Dingkang Yang, Ziyun Qian, Weijie Yin, Yuchi Wang, Hongsheng Li, Jun Liu, Peng Zhai, Yang Liu, Lihua Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

**å¤‡æ³¨**: A Survey of Reinforcement Learning for Large Language Models

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°ï¼šå¼ºåŒ–å­¦ä¹ èµ‹èƒ½å¤§è¯­è¨€æ¨¡å‹å…¨ç”Ÿå‘½å‘¨æœŸï¼Œæå‡æ¨ç†ä¸å¯¹é½æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å¯¹é½å¾®è°ƒ` `å¼ºåŒ–æ¨ç†` `é¢„è®­ç»ƒ` `ç»¼è¿°` `LLMç”Ÿå‘½å‘¨æœŸ` `å¯éªŒè¯å¥–åŠ±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…³äºå¼ºåŒ–å­¦ä¹ å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„ç»¼è¿°ç¼ºä¹å¯¹LLMå…¨ç”Ÿå‘½å‘¨æœŸçš„è¦†ç›–ï¼Œæœªèƒ½ç³»ç»Ÿæ€§åœ°æ€»ç»“RLåœ¨å„ä¸ªé˜¶æ®µçš„ä½œç”¨ã€‚
2. æœ¬æ–‡å…¨é¢å›é¡¾äº†RLåœ¨LLMé¢„è®­ç»ƒã€å¯¹é½å¾®è°ƒå’Œå¼ºåŒ–æ¨ç†ç­‰é˜¶æ®µçš„åº”ç”¨ï¼Œå¹¶é‡ç‚¹å…³æ³¨äº†RLVRæ–¹æ³•ã€‚
3. æœ¬æ–‡æ•´ç†äº†RLå¾®è°ƒæ‰€éœ€çš„æ•°æ®é›†ã€è¯„ä¼°åŸºå‡†ä»¥åŠå¼€æºå·¥å…·å’Œè®­ç»ƒæ¡†æ¶ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†å®è·µå‚è€ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œä»¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ–¹æ³•æ˜¾è‘—å¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œå¯¹é½æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£äººç±»æ„å›¾ã€éµå¾ªç”¨æˆ·æŒ‡ä»¤å’Œå¢å¼ºæ¨ç†èƒ½åŠ›æ–¹é¢ã€‚è™½ç„¶ç°æœ‰çš„ç»¼è¿°æä¾›äº†å¯¹RLå¢å¼ºLLMçš„æ¦‚è¿°ï¼Œä½†å®ƒä»¬çš„èŒƒå›´é€šå¸¸æœ‰é™ï¼Œæœªèƒ½å…¨é¢æ€»ç»“RLå¦‚ä½•åœ¨LLMçš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­è¿ä½œã€‚æœ¬æ–‡ç³»ç»Ÿåœ°å›é¡¾äº†RLèµ‹èƒ½LLMçš„ç†è®ºå’Œå®è·µè¿›å±•ï¼Œç‰¹åˆ«æ˜¯å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚é¦–å…ˆï¼Œç®€è¦ä»‹ç»äº†RLçš„åŸºæœ¬ç†è®ºã€‚å…¶æ¬¡ï¼Œè¯¦ç»†é˜è¿°äº†RLåœ¨LLMç”Ÿå‘½å‘¨æœŸçš„å„ä¸ªé˜¶æ®µçš„åº”ç”¨ç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¯¹é½å¾®è°ƒå’Œå¼ºåŒ–æ¨ç†ã€‚ç‰¹åˆ«å¼ºè°ƒï¼Œå¼ºåŒ–æ¨ç†é˜¶æ®µçš„RLæ–¹æ³•æ˜¯æ¨åŠ¨æ¨¡å‹æ¨ç†èƒ½åŠ›è¾¾åˆ°æé™çš„å…³é”®é©±åŠ¨åŠ›ã€‚æ¥ä¸‹æ¥ï¼Œæ•´ç†äº†ç°æœ‰çš„ç”¨äºRLå¾®è°ƒçš„æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–äººå·¥æ ‡æ³¨æ•°æ®é›†ã€AIè¾…åŠ©åå¥½æ•°æ®å’Œç¨‹åºéªŒè¯é£æ ¼çš„è¯­æ–™åº“ã€‚éšåï¼Œå›é¡¾äº†ä¸»æµçš„å¼€æºå·¥å…·å’Œè®­ç»ƒæ¡†æ¶ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ¸…æ™°çš„å®è·µå‚è€ƒã€‚æœ€åï¼Œåˆ†æäº†RLå¢å¼ºLLMé¢†åŸŸæœªæ¥çš„æŒ‘æˆ˜å’Œè¶‹åŠ¿ã€‚æœ¬ç»¼è¿°æ—¨åœ¨å‘ç ”ç©¶äººå‘˜å’Œä»ä¸šäººå‘˜ä»‹ç»RLå’ŒLLMäº¤å‰é¢†åŸŸçš„æœ€æ–°è¿›å±•å’Œå‰æ²¿è¶‹åŠ¿ï¼Œä»¥ä¿ƒè¿›æ›´æ™ºèƒ½ã€æ›´é€šç”¨å’Œæ›´å®‰å…¨çš„LLMçš„å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†å’Œå¯¹é½æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥å®Œå…¨ç†è§£äººç±»æ„å›¾å¹¶å‡†ç¡®æ‰§è¡Œå¤æ‚æŒ‡ä»¤ã€‚ç°æœ‰çš„RLå¢å¼ºLLMçš„ç»¼è¿°é€šå¸¸åªå…³æ³¨ç‰¹å®šé˜¶æ®µï¼Œç¼ºä¹å¯¹LLMå…¨ç”Ÿå‘½å‘¨æœŸçš„ç³»ç»Ÿæ€§åˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç³»ç»Ÿæ€§åœ°æ¢³ç†RLåœ¨LLMå…¨ç”Ÿå‘½å‘¨æœŸä¸­çš„åº”ç”¨ï¼Œä»é¢„è®­ç»ƒã€å¯¹é½å¾®è°ƒåˆ°å¼ºåŒ–æ¨ç†ï¼Œå…¨é¢åˆ†æRLå¦‚ä½•æå‡LLMçš„æ¨ç†å’Œå¯¹é½èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¸åŒé˜¶æ®µçš„RLæ–¹æ³•è¿›è¡Œå½’çº³å’Œæ€»ç»“ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€ä¸ªæ¸…æ™°çš„æ¡†æ¶ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å’Œåº”ç”¨RLæŠ€æœ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦éƒ¨åˆ†ï¼šé¦–å…ˆï¼Œç®€è¦ä»‹ç»RLçš„åŸºæœ¬ç†è®ºã€‚å…¶æ¬¡ï¼Œè¯¦ç»†é˜è¿°RLåœ¨LLMç”Ÿå‘½å‘¨æœŸçš„å„ä¸ªé˜¶æ®µçš„åº”ç”¨ç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¯¹é½å¾®è°ƒå’Œå¼ºåŒ–æ¨ç†ã€‚ç„¶åï¼Œæ•´ç†ç°æœ‰çš„ç”¨äºRLå¾®è°ƒçš„æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ã€‚æ¥ç€ï¼Œå›é¡¾ä¸»æµçš„å¼€æºå·¥å…·å’Œè®­ç»ƒæ¡†æ¶ã€‚æœ€åï¼Œåˆ†ææœªæ¥çš„æŒ‘æˆ˜å’Œè¶‹åŠ¿ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹RLåœ¨LLMå…¨ç”Ÿå‘½å‘¨æœŸä¸­çš„åº”ç”¨è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„æ¢³ç†å’Œæ€»ç»“ï¼Œç‰¹åˆ«æ˜¯å¼ºè°ƒäº†å¼ºåŒ–æ¨ç†é˜¶æ®µçš„RLæ–¹æ³•å¯¹äºæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ•´ç†äº†å¤§é‡çš„èµ„æºï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€è¯„ä¼°åŸºå‡†å’Œå¼€æºå·¥å…·ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¾¿åˆ©ã€‚

**å…³é”®è®¾è®¡**ï¼šæœ¬æ–‡ä¸»è¦æ˜¯ä¸€ç¯‡ç»¼è¿°æ–‡ç« ï¼Œæ²¡æœ‰æå‡ºæ–°çš„ç®—æ³•æˆ–æ¨¡å‹ã€‚ä½†æ˜¯ï¼Œæ–‡ç« å¯¹ç°æœ‰RLæ–¹æ³•åœ¨LLMä¸åŒé˜¶æ®µçš„åº”ç”¨è¿›è¡Œäº†è¯¦ç»†çš„åˆ†æå’Œæ€»ç»“ï¼Œå¹¶å¯¹æœªæ¥çš„ç ”ç©¶æ–¹å‘è¿›è¡Œäº†å±•æœ›ã€‚ä¾‹å¦‚ï¼Œæ–‡ç« æåˆ°äº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨æå‡LLMå®‰å…¨æ€§å’Œå¯é æ€§æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

æœ¬æ–‡ç³»ç»Ÿæ€§åœ°å›é¡¾äº†RLåœ¨LLMå…¨ç”Ÿå‘½å‘¨æœŸä¸­çš„åº”ç”¨ï¼Œå¹¶æ•´ç†äº†å¤§é‡çš„æ•°æ®é›†ã€è¯„ä¼°åŸºå‡†å’Œå¼€æºå·¥å…·ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†å…¨é¢çš„å‚è€ƒã€‚ç‰¹åˆ«å¼ºè°ƒäº†å¼ºåŒ–æ¨ç†é˜¶æ®µçš„RLæ–¹æ³•å¯¹äºæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå¹¶å¯¹æœªæ¥çš„ç ”ç©¶æ–¹å‘è¿›è¡Œäº†å±•æœ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´é€šç”¨å’Œæ›´å®‰å…¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥æå‡èŠå¤©æœºå™¨äººçš„å¯¹è¯è´¨é‡ã€æé«˜æ™ºèƒ½åŠ©æ‰‹çš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€å¢å¼ºä»£ç ç”Ÿæˆæ¨¡å‹çš„å‡†ç¡®æ€§ç­‰ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºæ¨åŠ¨äººæœºåä½œå’Œäººå·¥æ™ºèƒ½å®‰å…¨ç­‰é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, training methods centered on Reinforcement Learning (RL) have markedly enhanced the reasoning and alignment performance of Large Language Models (LLMs), particularly in understanding human intents, following user instructions, and bolstering inferential strength. Although existing surveys offer overviews of RL augmented LLMs, their scope is often limited, failing to provide a comprehensive summary of how RL operates across the full lifecycle of LLMs. We systematically review the theoretical and practical advancements whereby RL empowers LLMs, especially Reinforcement Learning with Verifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL. Second, we thoroughly detail application strategies for RL across various phases of the LLM lifecycle, including pre-training, alignment fine-tuning, and reinforced reasoning. In particular, we emphasize that RL methods in the reinforced reasoning phase serve as a pivotal driving force for advancing model reasoning to its limits. Next, we collate existing datasets and evaluation benchmarks currently used for RL fine-tuning, spanning human-annotated datasets, AI-assisted preference data, and program-verification-style corpora. Subsequently, we review the mainstream open-source tools and training frameworks available, providing clear practical references for subsequent research. Finally, we analyse the future challenges and trends in the field of RL-enhanced LLMs. This survey aims to present researchers and practitioners with the latest developments and frontier trends at the intersection of RL and LLMs, with the goal of fostering the evolution of LLMs that are more intelligent, generalizable, and secure.

