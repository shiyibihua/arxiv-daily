---
layout: default
title: The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia
---

# The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16487" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16487v1</a>
  <a href="https://arxiv.org/pdf/2509.16487.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16487v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16487v1', 'The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zixun Chen, Petr Babkin, Akshat Gupta, Gopala Anumanchipalli, Xiaomo Liu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å¤šç»´åº¦è¯„ä¼°Pythiaæ¨¡å‹å¯¹è¯èƒ½åŠ›ï¼Œæ­ç¤ºæ¨¡å‹è§„æ¨¡å’Œå¾®è°ƒçš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹è¯è¯„ä¼°` `å¤§è¯­è¨€æ¨¡å‹` `Pythiaæ¨¡å‹` `ç›‘ç£å¾®è°ƒ` `æ¨¡å‹æŒ‡æ ‡` `è¯­è¨€å­¦ç†è®º` `å¯¹è¯ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹å¤§è¯­è¨€æ¨¡å‹å¯¹è¯èƒ½åŠ›ç»†ç²’åº¦è¦ç´ çš„åŒºåˆ†å’Œè¯„ä¼°ï¼Œéš¾ä»¥æ·±å…¥ç†è§£æ¨¡å‹è¡Œä¸ºã€‚
2. æœ¬ç ”ç©¶æå‡ºä¸€å¥—åŸºäºæ¨¡å‹çš„æŒ‡æ ‡ï¼Œä»è¯­è¨€å­¦ç†è®ºå‡ºå‘ï¼Œé’ˆå¯¹å¯¹è¯çš„ä¸åŒæ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹å¤§å°å¯¹å¯¹è¯èƒ½åŠ›å½±å“æœ‰é™ï¼Œå¾®è°ƒè¿…é€Ÿæå‡æ€§èƒ½ï¼Œä½†æŒ‡æ ‡å¯é æ€§å­˜åœ¨ç–‘é—®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹è¯æ˜¯å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ ‡å¿—æ€§èƒ½åŠ›ä¹‹ä¸€ã€‚å°½ç®¡å…¶åº”ç”¨å¹¿æ³›ï¼Œä½†å¾ˆå°‘æœ‰ç ”ç©¶çœŸæ­£åŒºåˆ†åè®­ç»ƒè¿‡ç¨‹ä¸­å¯¹è¯è¡Œä¸ºçš„ç‰¹å®šè¦ç´ ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸€å¥—å…¨é¢çš„ã€åŸºäºæ¨¡å‹çš„æŒ‡æ ‡ï¼Œæ¯ä¸ªæŒ‡æ ‡éƒ½é’ˆå¯¹å¯¹è¯çš„ä¸åŒç»†ç²’åº¦æ–¹é¢ï¼Œå¹¶å—åˆ°è¯­è¨€ç†è®ºçš„é©±åŠ¨ã€‚æˆ‘ä»¬è¯„ä¼°äº†é¢„è®­ç»ƒçš„Pythiaæ¨¡å‹åœ¨è¿™äº›ç»´åº¦ä¸Šçš„æ€§èƒ½å¦‚ä½•éšæ¨¡å‹å¤§å°ä»¥åŠåœ¨å¯¹è¯æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒè€Œå˜åŒ–ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°åŸå§‹æ¨¡å‹å¤§å°å¯¹å¤§å¤šæ•°æŒ‡æ ‡çš„å½±å“å¾ˆå°ï¼Œè€Œå¾®è°ƒè¿…é€Ÿä½¿æ‰€æœ‰æµ‹è¯•æ¨¡å‹ï¼ˆæœ€å°çš„æ¨¡å‹é™¤å¤–ï¼‰çš„åˆ†æ•°é¥±å’Œã€‚ä¸æˆ‘ä»¬çš„é¢„æœŸç›¸åï¼Œè®¸å¤šæŒ‡æ ‡æ˜¾ç¤ºå‡ºéå¸¸ç›¸ä¼¼çš„è¶‹åŠ¿ï¼Œç‰¹åˆ«æ˜¯å¦‚æœå®ƒä»¬éƒ½åŸºäºç›¸åŒçš„è¯„ä¼°å™¨æ¨¡å‹ï¼Œè¿™å¼•å‘äº†å®ƒä»¬åœ¨è¡¡é‡ç‰¹å®šç»´åº¦æ—¶çš„å¯é æ€§é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹åˆ†æ•°åˆ†å¸ƒã€æŒ‡æ ‡ç›¸å…³æ€§å’Œç”Ÿæˆå“åº”ä¸­çš„æœ¯è¯­é¢‘ç‡è¿›è¡Œäº†é¢å¤–çš„åˆ†æï¼Œä»¥å¸®åŠ©è§£é‡Šæˆ‘ä»¬çš„è§‚å¯Ÿç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨æ·±å…¥ç†è§£å’Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹è¯èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯Pythiaæ¨¡å‹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨æ•´ä½“æ€§èƒ½ï¼Œç¼ºä¹å¯¹å¯¹è¯èƒ½åŠ›ç»†ç²’åº¦è¦ç´ çš„åŒºåˆ†å’Œè¯„ä¼°ã€‚è¿™ä½¿å¾—æˆ‘ä»¬éš¾ä»¥ç†è§£æ¨¡å‹åœ¨å¯¹è¯ä¸­è¡¨ç°å‡ºçš„å…·ä½“è¡Œä¸ºï¼Œä»¥åŠæ¨¡å‹è§„æ¨¡å’Œå¾®è°ƒç­‰å› ç´ å¯¹è¿™äº›è¡Œä¸ºçš„å½±å“ã€‚ç°æœ‰è¯„ä¼°æ–¹æ³•å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ æ¨¡å‹åœ¨ä¸åŒå¯¹è¯ç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œå¯¼è‡´å¯¹æ¨¡å‹èƒ½åŠ›çš„è¯¯åˆ¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨ä¸€å¥—å…¨é¢çš„ã€åŸºäºæ¨¡å‹çš„æŒ‡æ ‡ï¼Œä»è¯­è¨€å­¦ç†è®ºå‡ºå‘ï¼Œé’ˆå¯¹å¯¹è¯çš„ä¸åŒç»†ç²’åº¦æ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡è¿™äº›æŒ‡æ ‡ï¼Œå¯ä»¥æ›´æ·±å…¥åœ°äº†è§£æ¨¡å‹åœ¨ä¸åŒå¯¹è¯ç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œä»¥åŠæ¨¡å‹è§„æ¨¡å’Œå¾®è°ƒç­‰å› ç´ å¯¹è¿™äº›ç»´åº¦çš„å½±å“ã€‚è¿™ç§ç»†ç²’åº¦çš„è¯„ä¼°æ–¹æ³•æœ‰åŠ©äºæ­ç¤ºæ¨¡å‹å¯¹è¯èƒ½åŠ›çš„å†…åœ¨æœºåˆ¶ï¼Œå¹¶ä¸ºæ¨¡å‹æ”¹è¿›æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) é€‰æ‹©é¢„è®­ç»ƒçš„Pythiaæ¨¡å‹ä½œä¸ºç ”ç©¶å¯¹è±¡ï¼›2) æ„å»ºä¸€å¥—åŸºäºæ¨¡å‹çš„æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°å¯¹è¯çš„ä¸åŒæ–¹é¢ï¼Œä¾‹å¦‚è¿è´¯æ€§ã€ç›¸å…³æ€§ã€æµç•…æ€§ç­‰ï¼›3) åœ¨å¯¹è¯æ•°æ®é›†ä¸Šå¯¹Pythiaæ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒï¼›4) ä½¿ç”¨æ„å»ºçš„æŒ‡æ ‡è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé˜¶æ®µï¼ˆé¢„è®­ç»ƒã€å¾®è°ƒï¼‰çš„æ€§èƒ½ï¼›5) åˆ†æè¯„ä¼°ç»“æœï¼Œæ¢è®¨æ¨¡å‹è§„æ¨¡å’Œå¾®è°ƒå¯¹å¯¹è¯èƒ½åŠ›çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€å¥—å…¨é¢çš„ã€åŸºäºæ¨¡å‹çš„æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°å¯¹è¯çš„ä¸åŒç»†ç²’åº¦æ–¹é¢ã€‚è¿™äº›æŒ‡æ ‡çš„è®¾è®¡å—åˆ°è¯­è¨€å­¦ç†è®ºçš„é©±åŠ¨ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹åœ¨ä¸åŒå¯¹è¯ç»´åº¦ä¸Šçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é€šè¿‡å®éªŒåˆ†æäº†æ¨¡å‹è§„æ¨¡å’Œå¾®è°ƒå¯¹å¯¹è¯èƒ½åŠ›çš„å½±å“ï¼Œæ­ç¤ºäº†ä¸€äº›æœ‰è¶£çš„ç°è±¡ï¼Œä¾‹å¦‚æ¨¡å‹å¤§å°å¯¹å¤§å¤šæ•°æŒ‡æ ‡çš„å½±å“å¾ˆå°ï¼Œè€Œå¾®è°ƒè¿…é€Ÿä½¿æ‰€æœ‰æµ‹è¯•æ¨¡å‹ï¼ˆæœ€å°çš„æ¨¡å‹é™¤å¤–ï¼‰çš„åˆ†æ•°é¥±å’Œã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æŒ‡æ ‡çš„é€‰æ‹©å’Œè®¾è®¡ï¼Œéœ€è¦ç¡®ä¿æŒ‡æ ‡èƒ½å¤Ÿå‡†ç¡®åæ˜ å¯¹è¯çš„ä¸åŒæ–¹é¢ï¼›2) è¯„ä¼°å™¨æ¨¡å‹çš„é€‰æ‹©ï¼Œéœ€è¦é€‰æ‹©å…·æœ‰è‰¯å¥½æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›çš„æ¨¡å‹ï¼›3) å¾®è°ƒæ•°æ®é›†çš„é€‰æ‹©ï¼Œéœ€è¦é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„å¯¹è¯æ•°æ®é›†ï¼›4) å®éªŒå‚æ•°çš„è®¾ç½®ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€batch sizeç­‰ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹åˆ†æ•°åˆ†å¸ƒã€æŒ‡æ ‡ç›¸å…³æ€§å’Œç”Ÿæˆå“åº”ä¸­çš„æœ¯è¯­é¢‘ç‡è¿›è¡Œäº†é¢å¤–çš„åˆ†æï¼Œä»¥å¸®åŠ©è§£é‡Šå®éªŒç»“æœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸå§‹æ¨¡å‹å¤§å°å¯¹å¤§å¤šæ•°å¯¹è¯æŒ‡æ ‡çš„å½±å“å¾ˆå°ï¼Œè€Œå¾®è°ƒèƒ½å¤Ÿè¿…é€Ÿæå‡æ¨¡å‹æ€§èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿå‘ç°è®¸å¤šæŒ‡æ ‡è¡¨ç°å‡ºç›¸ä¼¼çš„è¶‹åŠ¿ï¼Œå°¤å…¶å½“å®ƒä»¬åŸºäºç›¸åŒçš„è¯„ä¼°å™¨æ¨¡å‹æ—¶ï¼Œè¿™å¼•å‘äº†å¯¹æŒ‡æ ‡å¯é æ€§çš„è´¨ç–‘ã€‚ç ”ç©¶è¿˜é€šè¿‡åˆ†æåˆ†æ•°åˆ†å¸ƒã€æŒ‡æ ‡ç›¸å…³æ€§å’Œç”Ÿæˆæ–‡æœ¬ä¸­çš„è¯é¢‘æ¥è¿›ä¸€æ­¥è§£é‡Šè¿™äº›ç°è±¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¯„ä¼°å’Œæ”¹è¿›å¯¹è¯ç³»ç»Ÿï¼Œä¾‹å¦‚èŠå¤©æœºå™¨äººã€æ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡ç»†ç²’åº¦çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°äº†è§£å¯¹è¯ç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è¿›è¡Œæ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¸ºå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒæä¾›æŒ‡å¯¼ï¼Œå¸®åŠ©æå‡æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´å¤šç»´åº¦çš„å¯¹è¯èƒ½åŠ›è¯„ä¼°ï¼Œå¹¶å¼€å‘æ›´æœ‰æ•ˆçš„æ¨¡å‹æ”¹è¿›æ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dialogue is one of the landmark abilities of large language models (LLMs). Despite its ubiquity, few studies actually distinguish specific ingredients underpinning dialogue behavior emerging during post-training. We employ a comprehensive suite of model-based metrics, each targeting a distinct fine-grained aspect of dialogue, motivated by linguistic theory. We evaluate how the performance of pre-trained Pythia models changes with respect to each of those dimensions, depending on model size and as a result of supervised fine-tuning on conversational datasets. We observe only a mild impact of raw model size on most metrics, whereas fine-tuning quickly saturates the scores for all but the smallest models tested. Somewhat contrary to our expectations, many metrics show very similar trends, especially if they are all rooted in the same evaluator model, which raises the question of their reliability in measuring a specific dimension. To that end, we conduct additional analyses of score distributions, metric correlations, and term frequencies in generated responses to help explain our observations.

