---
layout: default
title: LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding
---

# LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05657" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05657v3</a>
  <a href="https://arxiv.org/pdf/2509.05657.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05657v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05657v3', 'LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06 (æ›´æ–°: 2025-09-25)

**å¤‡æ³¨**: EMNLP 2025 Main

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Ashone3/LM-Searcher)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LM-Searcherï¼šåˆ©ç”¨LLMå’Œç»Ÿä¸€æ•°å€¼ç¼–ç å®ç°è·¨é¢†åŸŸç¥ç»æ¶æ„æœç´¢**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¥ç»æ¶æ„æœç´¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `è·¨é¢†åŸŸå­¦ä¹ ` `æ•°å€¼ç¼–ç ` `æŒ‡ä»¤è°ƒä¼˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„NASæ–¹æ³•ä¾èµ–äºæç¤ºå·¥ç¨‹å’Œé¢†åŸŸç‰¹å®šè°ƒæ•´ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚
2. LM-Searcheré€šè¿‡NCodeç»Ÿä¸€æ•°å€¼ç¼–ç ï¼Œå°†NASé—®é¢˜è½¬åŒ–ä¸ºæ’åºä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨æŒ‡ä»¤è°ƒä¼˜è®­ç»ƒLLMé€‰æ‹©é«˜æ€§èƒ½æ¶æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLM-Searcheråœ¨å›¾åƒåˆ†ç±»ã€åˆ†å‰²å’Œç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLM-Searcherçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè·¨é¢†åŸŸç¥ç»æ¶æ„ä¼˜åŒ–ï¼Œæ— éœ€å¤§é‡çš„é¢†åŸŸç‰¹å®šè°ƒæ•´ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯NCodeï¼Œä¸€ç§ç”¨äºç¥ç»æ¶æ„çš„é€šç”¨æ•°å€¼å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œå®ƒå®ç°äº†è·¨é¢†åŸŸçš„æ¶æ„ç¼–ç å’Œæœç´¢ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å°†ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰é—®é¢˜é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªæ’åºä»»åŠ¡ï¼Œé€šè¿‡ä½¿ç”¨åŸºäºå‰ªæçš„å­ç©ºé—´é‡‡æ ·ç­–ç•¥ç”Ÿæˆçš„æŒ‡ä»¤è°ƒä¼˜æ ·æœ¬ï¼Œè®­ç»ƒLLMä»å€™é€‰æ± ä¸­é€‰æ‹©é«˜æ€§èƒ½çš„æ¶æ„ã€‚æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¹¿æ³›æ¶æ„-æ€§èƒ½å¯¹çš„æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›é²æ£’å’Œå¯è¿ç§»çš„å­¦ä¹ ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒLM-Searcheråœ¨é¢†åŸŸå†…ï¼ˆä¾‹å¦‚ï¼Œç”¨äºå›¾åƒåˆ†ç±»çš„CNNï¼‰å’Œé¢†åŸŸå¤–ï¼ˆä¾‹å¦‚ï¼Œç”¨äºåˆ†å‰²å’Œç”Ÿæˆçš„LoRAé…ç½®ï¼‰ä»»åŠ¡ä¸­å‡å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºåŸºäºLLMçš„çµæ´»å’Œå¯æ³›åŒ–çš„æ¶æ„æœç´¢å»ºç«‹äº†ä¸€ç§æ–°çš„èŒƒä¾‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºLLMçš„ç¥ç»æ¶æ„æœç´¢æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„promptå·¥ç¨‹å’Œé¢†åŸŸç‰¹å®šçš„è°ƒä¼˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿè·¨é¢†åŸŸè¿›è¡Œç¥ç»æ¶æ„æœç´¢ï¼Œå¹¶ä¸”ä¸éœ€è¦å¤§é‡é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLM-Searcherçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç¥ç»æ¶æ„æœç´¢é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªæ’åºé—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å­¦ä¹ æ¶æ„çš„æ€§èƒ½æ’åºã€‚é€šè¿‡ä¸€ç§é€šç”¨çš„æ•°å€¼ç¼–ç æ–¹å¼ï¼ˆNCodeï¼‰æ¥è¡¨ç¤ºä¸åŒçš„ç¥ç»æ¶æ„ï¼Œä½¿å¾—LLMèƒ½å¤Ÿç†è§£å’Œæ¯”è¾ƒä¸åŒæ¶æ„çš„ä¼˜åŠ£ã€‚åŒæ—¶ï¼Œåˆ©ç”¨æŒ‡ä»¤è°ƒä¼˜çš„æ–¹å¼æ¥è®­ç»ƒLLMï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç»™å®šçš„ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œé€‰æ‹©å‡ºé«˜æ€§èƒ½çš„æ¶æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLM-Searcherçš„æ•´ä½“æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1ï¼‰**NCodeç¼–ç å™¨**ï¼šå°†ä¸åŒçš„ç¥ç»æ¶æ„ç¼–ç æˆç»Ÿä¸€çš„æ•°å€¼å­—ç¬¦ä¸²è¡¨ç¤ºã€‚2ï¼‰**å­ç©ºé—´é‡‡æ ·å™¨**ï¼šåŸºäºå‰ªæç­–ç•¥ï¼Œä»æ•´ä¸ªæ¶æ„ç©ºé—´ä¸­é‡‡æ ·å‡ºå…·æœ‰ä»£è¡¨æ€§çš„å­ç©ºé—´ã€‚3ï¼‰**æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ç”Ÿæˆå™¨**ï¼šæ ¹æ®é‡‡æ ·å‡ºçš„å­ç©ºé—´ï¼Œç”ŸæˆåŒ…å«æ¶æ„-æ€§èƒ½å¯¹çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ã€‚4ï¼‰**LLMæ’åºå™¨**ï¼šåˆ©ç”¨æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†è®­ç»ƒLLMï¼Œä½¿å…¶èƒ½å¤Ÿå¯¹ä¸åŒçš„æ¶æ„è¿›è¡Œæ’åºã€‚5ï¼‰**æ¶æ„æœç´¢å™¨**ï¼šåˆ©ç”¨è®­ç»ƒå¥½çš„LLMï¼Œä»å€™é€‰æ¶æ„æ± ä¸­é€‰æ‹©å‡ºé«˜æ€§èƒ½çš„æ¶æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šLM-Searcherçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ç‚¹ï¼š1ï¼‰**NCodeé€šç”¨æ•°å€¼ç¼–ç **ï¼šæå‡ºäº†ä¸€ç§é€šç”¨çš„æ•°å€¼ç¼–ç æ–¹å¼ï¼Œèƒ½å¤Ÿè¡¨ç¤ºä¸åŒç±»å‹çš„ç¥ç»æ¶æ„ï¼Œå®ç°äº†è·¨é¢†åŸŸçš„æ¶æ„è¡¨ç¤ºã€‚2ï¼‰**åŸºäºå‰ªæçš„å­ç©ºé—´é‡‡æ ·**ï¼šæå‡ºäº†ä¸€ç§åŸºäºå‰ªæç­–ç•¥çš„å­ç©ºé—´é‡‡æ ·æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘æœç´¢ç©ºé—´ï¼Œæé«˜æœç´¢æ•ˆç‡ã€‚3ï¼‰**æŒ‡ä»¤è°ƒä¼˜çš„LLMæ’åºå™¨**ï¼šåˆ©ç”¨æŒ‡ä»¤è°ƒä¼˜çš„æ–¹å¼è®­ç»ƒLLMï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå­¦ä¹ æ¶æ„çš„æ€§èƒ½æ’åºã€‚

**å…³é”®è®¾è®¡**ï¼šNCodeç¼–ç çš„å…³é”®è®¾è®¡åœ¨äºå°†æ¶æ„çš„å„ä¸ªç»„ä»¶ï¼ˆå¦‚å·ç§¯å±‚ã€æ± åŒ–å±‚ã€æ¿€æ´»å‡½æ•°ç­‰ï¼‰æ˜ å°„åˆ°æ•°å€¼ç©ºé—´ï¼Œå¹¶ä½¿ç”¨å­—ç¬¦ä¸²çš„å½¢å¼è¿›è¡Œè¡¨ç¤ºã€‚å­ç©ºé—´é‡‡æ ·å™¨ä½¿ç”¨L1èŒƒæ•°å‰ªææ¥é€‰æ‹©é‡è¦çš„è¿æ¥ï¼Œä»è€Œå‡å°‘æœç´¢ç©ºé—´ã€‚æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†åŒ…å«æ¶æ„çš„NCodeè¡¨ç¤ºã€ä»»åŠ¡æè¿°å’Œæ€§èƒ½æŒ‡æ ‡ã€‚LLMæ’åºå™¨ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯é¢„æµ‹æ¶æ„çš„æ€§èƒ½æ’åã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLM-Searcheråœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†ä¸ä¼ ç»ŸNASæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨LoRAé…ç½®æœç´¢ä»»åŠ¡ä¸Šï¼Œç›¸æ¯”äºéšæœºæœç´¢å’Œè´å¶æ–¯ä¼˜åŒ–ç­‰åŸºçº¿æ–¹æ³•ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒLM-Searcheræœç´¢åˆ°çš„LoRAé…ç½®ç›¸æ¯”äºéšæœºæœç´¢ï¼ŒIoUæå‡äº†5%ä»¥ä¸Šã€‚è¿™äº›ç»“æœéªŒè¯äº†LM-Searcherçš„æœ‰æ•ˆæ€§å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LM-Searcherå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸã€‚å®ƒèƒ½å¤Ÿè‡ªåŠ¨åŒ–åœ°æœç´¢é«˜æ€§èƒ½çš„ç¥ç»æ¶æ„ï¼Œé™ä½äº†äººå·¥è®¾è®¡æ¶æ„çš„æˆæœ¬å’Œéš¾åº¦ã€‚æ­¤å¤–ï¼ŒLM-Searcherçš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºæ–°çš„ä»»åŠ¡å’Œé¢†åŸŸï¼ŒåŠ é€Ÿäº†AIæŠ€æœ¯çš„åˆ›æ–°å’Œå‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at https://github.com/Ashone3/LM-Searcher.

