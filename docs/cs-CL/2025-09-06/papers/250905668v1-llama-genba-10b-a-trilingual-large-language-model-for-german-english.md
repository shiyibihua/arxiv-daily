---
layout: default
title: Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian
---

# Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05668" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05668v1</a>
  <a href="https://arxiv.org/pdf/2509.05668.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05668v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05668v1', 'Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Michael Hoffmann, Jophin John, Stefan Schweter, Gokul Ramakrishnan, Hoi-Fong Mak, Alice Zhang, Dmitry Gaynullin, Nicolay J. Hammer

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06

**å¤‡æ³¨**: Michael Hoffmann and Jophin John contributed equally to this work

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Llama-GENBA-10Bï¼šä¸€ç§ç”¨äºå¾·è¯­ã€è‹±è¯­å’Œå·´ä¼åˆ©äºšè¯­çš„ä¸‰è¯­å¤§å‹è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æ¨¡å‹` `ä½èµ„æºè¯­è¨€` `è·¨è¯­è¨€è¿ç§»` `å·´ä¼åˆ©äºšè¯­` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„åå·®ï¼Œå¿½ç•¥äº†å…¶ä»–è¯­è¨€ï¼Œå°¤å…¶å¯¹ä½èµ„æºè¯­è¨€æ”¯æŒä¸è¶³ã€‚
2. Llama-GENBA-10Bé€šè¿‡å¹³è¡¡è‹±è¯­ã€å¾·è¯­å’Œå·´ä¼åˆ©äºšè¯­çš„é¢„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¼˜åŒ–æ¨¡å‹æ¶æ„ï¼Œå®ç°æ›´å¥½çš„è·¨è¯­è¨€æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLlama-GENBA-10Båœ¨å·´ä¼åˆ©äºšè¯­ä¸Šè¶…è¶Šç°æœ‰æ¨¡å‹ï¼Œå¹¶åœ¨è‹±è¯­å’Œå¾·è¯­ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†Llama-GENBA-10Bï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„åå·®çš„ä¸‰è¯­åŸºç¡€æ¨¡å‹ã€‚Llama-GENBA-10BåŸºäºLlama 3.1-8Bæ„å»ºï¼Œæ‰©å±•åˆ°100äº¿å‚æ•°ï¼Œå¹¶åœ¨1640äº¿tokensä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ˆ820äº¿è‹±è¯­ï¼Œ820äº¿å¾·è¯­å’Œ8000ä¸‡å·´ä¼åˆ©äºšè¯­ï¼‰ï¼Œåœ¨å¹³è¡¡èµ„æºçš„åŒæ—¶é˜²æ­¢è‹±è¯­çš„ä¸»å¯¼åœ°ä½ã€‚è¯¥æ¨¡å‹é¢å‘å¾·å›½NLPç¤¾åŒºï¼ŒåŒæ—¶ä¹Ÿæ¨å¹¿å·´ä¼åˆ©äºšè¯­è¿™ç§ä½èµ„æºè¯­è¨€ã€‚å¼€å‘è¿‡ç¨‹è§£å†³äº†å››ä¸ªæŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰åœ¨å·´ä¼åˆ©äºšè¯­ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œç®¡ç†ä¸€ä¸ªå¤šè¯­è¯­æ–™åº“ï¼›ï¼ˆ2ï¼‰ä¸ºè‹±è¯­ã€å¾·è¯­å’Œå·´ä¼åˆ©äºšè¯­åˆ›å»ºä¸€ä¸ªç»Ÿä¸€çš„åˆ†è¯å™¨ï¼›ï¼ˆ3ï¼‰ä¼˜åŒ–æ¶æ„å’Œè¯­è¨€æ¯”ä¾‹è¶…å‚æ•°ä»¥å®ç°è·¨è¯­è¨€è¿ç§»ï¼›ï¼ˆ4ï¼‰é€šè¿‡å°†å¾·å›½åŸºå‡†ç¿»è¯‘æˆå·´ä¼åˆ©äºšè¯­ï¼Œå»ºç«‹ç¬¬ä¸€ä¸ªæ ‡å‡†åŒ–çš„ä¸‰è¯­è¯„ä¼°å¥—ä»¶ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒLlama-GENBA-10Bå®ç°äº†å¼ºå¤§çš„è·¨è¯­è¨€æ€§èƒ½ï¼Œå¾®è°ƒåçš„å˜ä½“åœ¨å·´ä¼åˆ©äºšè¯­ä¸­è¶…è¿‡äº†Apertus-8B-2509å’Œgemma-2-9bï¼Œå¹¶æˆä¸ºè¯¥è¯­è¨€åŒç±»æœ€ä½³æ¨¡å‹ï¼ŒåŒæ—¶åœ¨è‹±è¯­ä¸­ä¼˜äºEuroLLMï¼Œå¹¶åœ¨å¾·è¯­ä¸­ä¸å…¶ç»“æœç›¸åŒ¹é…ã€‚åœ¨Cerebras CS-2ä¸Šè¿›è¡Œçš„è®­ç»ƒè¯æ˜äº†é«˜æ•ˆçš„å¤§è§„æ¨¡å¤šè¯­é¢„è®­ç»ƒï¼Œå¹¶è®°å½•äº†èƒ½æºä½¿ç”¨æƒ…å†µï¼Œä¸ºæ•´åˆä½èµ„æºè¯­è¨€çš„åŒ…å®¹æ€§åŸºç¡€æ¨¡å‹æä¾›äº†è“å›¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥è‹±è¯­ä¸ºä¸­å¿ƒï¼Œå¯¼è‡´åœ¨å…¶ä»–è¯­è¨€ï¼Œç‰¹åˆ«æ˜¯ä½èµ„æºè¯­è¨€ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚è¿™é™åˆ¶äº†è¿™äº›æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œå¹¶ä¸”å¯èƒ½åŠ å‰§è¯­è¨€ä¹‹é—´çš„æ•°å­—é¸¿æ²Ÿã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å¹³è¡¡ä¸åŒè¯­è¨€çš„æ•°æ®é‡ï¼Œå¹¶ä¸”ç¼ºä¹é’ˆå¯¹ä½èµ„æºè¯­è¨€çš„ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLlama-GENBA-10Bçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¹³è¡¡ä¸åŒè¯­è¨€çš„é¢„è®­ç»ƒæ•°æ®ï¼Œå¹¶é’ˆå¯¹è·¨è¯­è¨€è¿ç§»è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæ„å»ºä¸€ä¸ªæ›´å…¬å¹³ã€æ›´é«˜æ•ˆçš„å¤šè¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç‰¹åˆ«å…³æ³¨å·´ä¼åˆ©äºšè¯­ï¼Œä¸€ç§ä½èµ„æºè¯­è¨€ï¼Œæ—¨åœ¨æå‡å…¶åœ¨è¯¥è¯­è¨€ä¸Šçš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLlama-GENBA-10BåŸºäºLlama 3.1-8Bæ„å»ºï¼Œå¹¶æ‰©å±•åˆ°100äº¿å‚æ•°ã€‚è¯¥æ¨¡å‹åœ¨åŒ…å«è‹±è¯­ã€å¾·è¯­å’Œå·´ä¼åˆ©äºšè¯­çš„1640äº¿tokensä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„ä¸‰è¯­è¯„ä¼°å¥—ä»¶ï¼ŒåŒ…æ‹¬å°†å¾·å›½åŸºå‡†ç¿»è¯‘æˆå·´ä¼åˆ©äºšè¯­ã€‚è®­ç»ƒè¿‡ç¨‹åœ¨Cerebras CS-2ä¸Šè¿›è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼šï¼ˆ1ï¼‰æ„å»ºäº†ä¸€ä¸ªå¹³è¡¡çš„å¤šè¯­è¯­æ–™åº“ï¼Œç‰¹åˆ«å…³æ³¨ä½èµ„æºè¯­è¨€å·´ä¼åˆ©äºšè¯­ï¼›ï¼ˆ2ï¼‰åˆ›å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†è¯å™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è‹±è¯­ã€å¾·è¯­å’Œå·´ä¼åˆ©äºšè¯­ï¼›ï¼ˆ3ï¼‰ä¼˜åŒ–äº†æ¨¡å‹æ¶æ„å’Œè¯­è¨€æ¯”ä¾‹è¶…å‚æ•°ï¼Œä»¥å®ç°æ›´å¥½çš„è·¨è¯­è¨€è¿ç§»ï¼›ï¼ˆ4ï¼‰å»ºç«‹äº†ç¬¬ä¸€ä¸ªæ ‡å‡†åŒ–çš„ä¸‰è¯­è¯„ä¼°å¥—ä»¶ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹ä½¿ç”¨Llama 3.1-8Bä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶æ‰©å±•åˆ°100äº¿å‚æ•°ã€‚é¢„è®­ç»ƒæ•°æ®åŒ…å«820äº¿è‹±è¯­tokensï¼Œ820äº¿å¾·è¯­tokenså’Œ8000ä¸‡å·´ä¼åˆ©äºšè¯­tokensã€‚ä½œè€…é’ˆå¯¹è·¨è¯­è¨€è¿ç§»ä¼˜åŒ–äº†æ¨¡å‹æ¶æ„å’Œè¯­è¨€æ¯”ä¾‹è¶…å‚æ•°ï¼Œä½†å…·ä½“ç»†èŠ‚æœªçŸ¥ã€‚è®­ç»ƒè¿‡ç¨‹åœ¨Cerebras CS-2ä¸Šè¿›è¡Œï¼Œå¹¶è®°å½•äº†èƒ½æºä½¿ç”¨æƒ…å†µã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Llama-GENBA-10Båœ¨å·´ä¼åˆ©äºšè¯­ä¸Šçš„å¾®è°ƒå˜ä½“è¶…è¶Šäº†Apertus-8B-2509å’Œgemma-2-9bï¼Œæˆä¸ºè¯¥è¯­è¨€åŒç±»æœ€ä½³æ¨¡å‹ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹åœ¨è‹±è¯­ä¸Šä¼˜äºEuroLLMï¼Œå¹¶åœ¨å¾·è¯­ä¸Šä¸å…¶ç»“æœç›¸åŒ¹é…ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLlama-GENBA-10Båœ¨è·¨è¯­è¨€æ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Llama-GENBA-10Bçš„åº”ç”¨åœºæ™¯åŒ…æ‹¬å¤šè¯­è¨€æœºå™¨ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ã€å¤šè¯­è¨€å†…å®¹ç”Ÿæˆç­‰ã€‚è¯¥æ¨¡å‹ç‰¹åˆ«é€‚ç”¨äºéœ€è¦å¤„ç†å¾·è¯­ã€è‹±è¯­å’Œå·´ä¼åˆ©äºšè¯­çš„åœºæ™¯ã€‚é€šè¿‡æå‡ä½èµ„æºè¯­è¨€çš„æ€§èƒ½ï¼Œè¯¥ç ”ç©¶æœ‰åŠ©äºä¿ƒè¿›è¯­è¨€å¤šæ ·æ€§å’Œæ•°å­—åŒ…å®¹æ€§ï¼Œå¹¶ä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€æ¨¡å‹çš„å¼€å‘æä¾›å‚è€ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages.

