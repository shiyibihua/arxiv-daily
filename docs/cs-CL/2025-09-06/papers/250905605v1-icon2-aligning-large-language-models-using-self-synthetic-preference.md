---
layout: default
title: Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation
---

# Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05605" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05605v1</a>
  <a href="https://arxiv.org/pdf/2509.05605.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05605v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05605v1', 'Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qiyuan Chen, Hongsen Huang, Qian Shao, Jiahe Chen, Jintai Chen, Hongxia Xu, Renjie Hua, Ren Chuan, Jian Wu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06

**å¤‡æ³¨**: EMNLP 2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**IconÂ²ï¼šåˆ©ç”¨LLMå†…åœ¨è°ƒæ§çš„è‡ªåˆæˆåå¥½æ•°æ®å¯¹é½å¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹å¯¹é½` `åå¥½å­¦ä¹ ` `è‡ªåˆæˆæ•°æ®` `å†…åœ¨è°ƒæ§` `è¡¨å¾å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åå¥½æ•°æ®é›†æ„å»ºæ–¹æ³•ä¾èµ–é¢„æ”¶é›†æŒ‡ä»¤ï¼Œæ˜“å¯¼è‡´ä¸ç›®æ ‡æ¨¡å‹åˆ†å¸ƒä¸åŒ¹é…ï¼Œä¸”æŠ½æ ·å¤šå“åº”è®¡ç®—å¼€é”€å¤§ã€‚
2. IconÂ²åˆ©ç”¨LLMè¡¨å¾ç©ºé—´çš„å†…åœ¨è°ƒæ§ï¼Œæå–åˆ†å±‚æ–¹å‘å‘é‡ç¼–ç åå¥½ï¼Œè¿‡æ»¤è‡ªåˆæˆæŒ‡ä»¤ï¼Œå¹¶åŒå‘æ§åˆ¶tokenè¡¨å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒIconÂ²åœ¨AlpacaEval 2.0å’ŒArena-Hardä¸Šåˆ†åˆ«æå‡13.89%å’Œ13.45%èƒœç‡ï¼Œè®¡ç®—æˆæœ¬é™ä½é«˜è¾¾48.1%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦é«˜è´¨é‡çš„åå¥½æ•°æ®é›†æ¥ä¸äººç±»åå¥½å¯¹é½ã€‚ç„¶è€Œï¼Œæ„å»ºæ­¤ç±»æ•°æ®é›†çš„ä¼ ç»Ÿæ–¹æ³•é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼šä¾èµ–äºé¢„å…ˆæ”¶é›†çš„æŒ‡ä»¤é€šå¸¸å¯¼è‡´ä¸ç›®æ ‡æ¨¡å‹å­˜åœ¨åˆ†å¸ƒä¸åŒ¹é…ï¼Œè€Œå¯¹å¤šä¸ªéšæœºå“åº”è¿›è¡ŒæŠ½æ ·çš„éœ€æ±‚å¼•å…¥äº†å¤§é‡çš„è®¡ç®—å¼€é”€ã€‚æœ¬æ–‡é€šè¿‡åˆ©ç”¨LLMè¡¨å¾ç©ºé—´çš„å†…åœ¨è°ƒæ§æ¥è¿›è¡Œé«˜æ•ˆä¸”å®šåˆ¶åŒ–çš„åå¥½æ•°æ®é›†æ„å»ºï¼Œä»è€Œæ¢ç´¢äº†ä¸€ç§èŒƒå¼è½¬å˜ï¼Œå‘½åä¸ºIconÂ²ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé¦–å…ˆæå–åˆ†å±‚æ–¹å‘å‘é‡æ¥ç¼–ç å¤æ‚çš„äººç±»åå¥½ï¼Œç„¶åä½¿ç”¨è¿™äº›å‘é‡æ¥è¿‡æ»¤è‡ªåˆæˆæŒ‡ä»¤ï¼ŒåŸºäºå®ƒä»¬çš„å†…åœ¨ä¸€è‡´æ€§ã€‚åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œåº”ç”¨åŒå‘å†…åœ¨æ§åˆ¶æ¥å¼•å¯¼tokenè¡¨å¾ï¼Œä»è€Œèƒ½å¤Ÿç²¾ç¡®ç”Ÿæˆå…·æœ‰æ¸…æ™°å¯¹é½åŒºåˆ†çš„å“åº”å¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¯¹é½å’Œæ•ˆç‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚Llama3-8Bå’ŒQwen2-7Båœ¨AlpacaEval 2.0ä¸Šå¹³å‡èƒœç‡æé«˜äº†13.89%ï¼Œåœ¨Arena-Hardä¸Šæé«˜äº†13.45%ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬é™ä½äº†é«˜è¾¾48.1%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•ä¾èµ–äºäººå·¥æ ‡æ³¨æˆ–é¢„å…ˆæ”¶é›†çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¾€å¾€å­˜åœ¨åˆ†å¸ƒåç§»çš„é—®é¢˜ï¼Œæ— æ³•å¾ˆå¥½åœ°ä»£è¡¨ç›®æ ‡æ¨¡å‹çš„å®é™…åº”ç”¨åœºæ™¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ„å»ºé«˜è´¨é‡çš„åå¥½æ•°æ®é›†ï¼Œé€šå¸¸éœ€è¦å¯¹æ¯ä¸ªæŒ‡ä»¤ç”Ÿæˆå¤šä¸ªä¸åŒçš„å“åº”ï¼Œå¹¶è¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒï¼Œè¿™å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆåœ°æ„å»ºä¸ç›®æ ‡æ¨¡å‹åˆ†å¸ƒä¸€è‡´ä¸”å…·æœ‰æ˜ç¡®åå¥½åŒºåˆ†çš„è®­ç»ƒæ•°æ®æ˜¯å½“å‰é¢ä¸´çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šIconÂ²çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è‡ªèº«è¡¨å¾ç©ºé—´çš„å†…åœ¨è°ƒæ§èƒ½åŠ›ï¼Œè‡ªåŠ¨åˆæˆé«˜è´¨é‡çš„åå¥½æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé¦–å…ˆé€šè¿‡æå–æ¨¡å‹å†…éƒ¨çš„å±‚çº§æ–¹å‘å‘é‡æ¥ç¼–ç äººç±»åå¥½ï¼Œç„¶ååˆ©ç”¨è¿™äº›å‘é‡æ¥è¿‡æ»¤è‡ªåˆæˆçš„æŒ‡ä»¤ï¼Œç¡®ä¿æŒ‡ä»¤ä¸åå¥½çš„ä¸€è‡´æ€§ã€‚åœ¨ç”Ÿæˆå“åº”æ—¶ï¼Œé€šè¿‡åŒå‘å†…åœ¨æ§åˆ¶æ¥å¼•å¯¼tokençš„è¡¨å¾ï¼Œä»è€Œç”Ÿæˆå…·æœ‰æ˜ç¡®å¯¹é½åŒºåˆ†çš„å“åº”å¯¹ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹å¤–éƒ¨æ•°æ®çš„ä¾èµ–ï¼Œå¹¶å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIconÂ²çš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š
1. **åå¥½å‘é‡æå–**ï¼šä»é¢„è®­ç»ƒçš„LLMä¸­æå–å±‚çº§çš„æ–¹å‘å‘é‡ï¼Œç”¨äºç¼–ç äººç±»åå¥½ã€‚
2. **æŒ‡ä»¤è¿‡æ»¤**ï¼šä½¿ç”¨åå¥½å‘é‡è¿‡æ»¤è‡ªåˆæˆçš„æŒ‡ä»¤ï¼Œç¡®ä¿æŒ‡ä»¤ä¸åå¥½çš„ä¸€è‡´æ€§ã€‚
3. **å“åº”ç”Ÿæˆ**ï¼šé€šè¿‡åŒå‘å†…åœ¨æ§åˆ¶æ¥å¼•å¯¼tokençš„è¡¨å¾ï¼Œç”Ÿæˆå…·æœ‰æ˜ç¡®å¯¹é½åŒºåˆ†çš„å“åº”å¯¹ã€‚
4. **æ¨¡å‹è®­ç»ƒ**ï¼šä½¿ç”¨ç”Ÿæˆçš„åå¥½æ•°æ®é›†æ¥è®­ç»ƒLLMï¼Œä½¿å…¶ä¸äººç±»åå¥½å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šIconÂ²æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨äº†LLMè‡ªèº«çš„è¡¨å¾ç©ºé—´æ¥è¿›è¡Œåå¥½æ•°æ®çš„åˆæˆå’Œè¿‡æ»¤ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–å¤–éƒ¨æ•°æ®çš„æ–¹æ³•ä¸åŒï¼ŒIconÂ²èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç›®æ ‡æ¨¡å‹çš„åˆ†å¸ƒï¼Œå¹¶ç”Ÿæˆå…·æœ‰æ˜ç¡®åå¥½åŒºåˆ†çš„è®­ç»ƒæ•°æ®ã€‚æ­¤å¤–ï¼ŒåŒå‘å†…åœ¨æ§åˆ¶æœºåˆ¶èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°æ§åˆ¶å“åº”çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œæé«˜å¯¹é½çš„æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åå¥½å‘é‡æå–æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†å±‚çº§åŒ–çš„æ–¹æ³•ï¼Œä»ä¸åŒçš„ç½‘ç»œå±‚æå–æ–¹å‘å‘é‡ï¼Œä»¥æ•æ‰ä¸åŒå±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨æŒ‡ä»¤è¿‡æ»¤æ–¹é¢ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ç§åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„è¿‡æ»¤æœºåˆ¶ï¼Œç”¨äºç­›é€‰ä¸åå¥½å‘é‡ä¸€è‡´çš„æŒ‡ä»¤ã€‚åœ¨å“åº”ç”Ÿæˆæ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†åŒå‘å†…åœ¨æ§åˆ¶æœºåˆ¶ï¼Œé€šè¿‡è°ƒæ•´tokençš„è¡¨å¾æ¥å¼•å¯¼å“åº”çš„ç”Ÿæˆï¼Œä½¿å…¶æ›´ç¬¦åˆäººç±»åå¥½ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIconÂ²åœ¨Llama3-8Bå’ŒQwen2-7Bæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨AlpacaEval 2.0è¯„ä¼°åŸºå‡†ä¸Šï¼Œå¹³å‡èƒœç‡æé«˜äº†13.89%ï¼Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„Arena-Hardè¯„ä¼°åŸºå‡†ä¸Šï¼Œå¹³å‡èƒœç‡æé«˜äº†13.45%ã€‚åŒæ—¶ï¼ŒIconÂ²æ–¹æ³•å°†è®¡ç®—æˆæœ¬é™ä½äº†é«˜è¾¾48.1%ï¼Œè¯æ˜äº†å…¶åœ¨å¯¹é½æ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

IconÂ²æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„å¯¹é½è®­ç»ƒï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹é«˜è´¨é‡äººå·¥æ ‡æ³¨æ•°æ®æˆ–è®¡ç®—èµ„æºå—é™çš„åœºæ™¯ä¸‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡LLMåœ¨å¯¹è¯ç”Ÿæˆã€æ–‡æœ¬æ‘˜è¦ã€ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä½¿å…¶æ›´å¥½åœ°ç¬¦åˆäººç±»çš„ä»·å€¼è§‚å’Œåå¥½ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå®ç°æ›´å®‰å…¨ã€å¯é å’Œè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) require high quality preference datasets to align with human preferences. However, conventional methods for constructing such datasets face significant challenges: reliance on pre-collected instructions often leads to distribution mismatches with target models, while the need for sampling multiple stochastic responses introduces substantial computational overhead. In this work, we explore a paradigm shift by leveraging inherent regulation of LLMs' representation space for efficient and tailored preference dataset construction, named Icon$^{2}$. Specifically, it first extracts layer-wise direction vectors to encode sophisticated human preferences and then uses these vectors to filter self-synthesized instructions based on their inherent consistency. During decoding, bidirectional inherent control is applied to steer token representations, enabling the precise generation of response pairs with clear alignment distinctions. Experimental results demonstrate significant improvements in both alignment and efficiency. Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by up to 48.1%.

