---
layout: default
title: Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation
---

# Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05602" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05602v2</a>
  <a href="https://arxiv.org/pdf/2509.05602.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05602v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05602v2', 'Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongyan Xie, Yitong Yao, Yikun Ban, Zixuan Huang, Deqing Wang, Zhenhe Wu, Haoxiang Su, Chao Wang, Shuangyong Song

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06 (æ›´æ–°: 2025-09-09)

**å¤‡æ³¨**: PrePrint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoPeDï¼Œé€šè¿‡çº æ­£æ€§æ„ŸçŸ¥è’¸é¦ç¼“è§£CoTæ•°æ®ä¸­çš„ä¼ªç›¸å…³æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é“¾å¼æ€è€ƒ` `çŸ¥è¯†è’¸é¦` `ä¼ªç›¸å…³æ€§` `æ¨ç†è´¨é‡` `è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„CoTæ•°æ®å¾®è°ƒå°å‹æ¨¡å‹ï¼Œä½†CoTæ•°æ®ä¸­åŒ…å«çš„å™ªå£°æ¨ç†ä¼šå¼•å…¥é—®é¢˜ä¸ç­”æ¡ˆä¹‹é—´çš„ä¼ªç›¸å…³æ€§ã€‚
2. CoPeDé€šè¿‡å¼•å…¥æ­£ç¡®æ€§æ„ŸçŸ¥çš„ä»»åŠ¡è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹åŸºäºæ­£ç¡®çš„æ¨ç†é¢„æµ‹ç­”æ¡ˆï¼Œå¹¶ä»é”™è¯¯ä¸­å­¦ä¹ ï¼Œä»è€Œæé«˜æ¨ç†çš„å¿ å®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCoPeDåœ¨åŒåˆ†å¸ƒå’Œå¼‚åˆ†å¸ƒçš„æ¨ç†æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†éƒ¨ç½²æˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼Œå°å‹è¯­è¨€æ¨¡å‹(SLMs)é€šå¸¸åœ¨LLMsç”Ÿæˆçš„CoTæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥å¤åˆ¶LLMsçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›CoTæ•°æ®å¯èƒ½åŒ…å«å™ªå£°æ¨ç†ï¼Œè¿™äº›æ¨ç†è¦ä¹ˆæœªèƒ½è¯å®ç­”æ¡ˆï¼Œè¦ä¹ˆæ²¡æœ‰æä¾›é¢å¤–çš„æ”¯æŒç­”æ¡ˆé¢„æµ‹çš„ä¿¡æ¯ï¼Œè¿™å¯¼è‡´SLMsæ•è·é—®é¢˜å’Œç­”æ¡ˆä¹‹é—´çš„è™šå‡ç›¸å…³æ€§ï¼Œå¹¶æŸå®³æ¨ç†è´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Chain-of-Thought Correctness Perception Distillation (CoPeD)ï¼Œæ—¨åœ¨ä»ä»»åŠ¡è®¾ç½®å’Œæ•°æ®åˆ©ç”¨çš„è§’åº¦æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ­£ç¡®æ€§æ„ŸçŸ¥ä»»åŠ¡è®¾ç½®ï¼Œé¼“åŠ±å­¦ç”Ÿæ¨¡å‹åŸºäºæ­£ç¡®çš„æ¨ç†æ¥é¢„æµ‹ç­”æ¡ˆï¼Œå¹¶åœ¨æ¨ç†ä¸æ­£ç¡®æ—¶ä¿®æ”¹ç­”æ¡ˆã€‚è¿™ç§è®¾ç½®æé«˜äº†æ¨ç†çš„å¿ å®æ€§ï¼Œå¹¶å…è®¸æ¨¡å‹ä»é”™è¯¯ä¸­å­¦ä¹ ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ­£ç¡®æ€§æ„ŸçŸ¥åŠ æƒæŸå¤±ï¼Œå®ƒæ ¹æ®æ¨ç†å’Œç­”æ¡ˆçš„ç»„åˆæŸå¤±åŠ¨æ€è°ƒæ•´æ¯ä¸ªè®­ç»ƒå®ä¾‹çš„è´¡çŒ®ã€‚è¿™ç§ç­–ç•¥é¼“åŠ±æ¨¡å‹æ›´å¤šåœ°å…³æ³¨æ¨ç†ä¸ºæ­£ç¡®ç­”æ¡ˆæä¾›æ›´å¼ºæ”¯æŒçš„æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒCoPeDåœ¨åŒåˆ†å¸ƒ(IND)å’Œå¼‚åˆ†å¸ƒ(OOD)åŸºå‡†æ¨ç†æ•°æ®é›†ä¸Šå‡æœ‰æ•ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹(SLMs)åœ¨æ¨¡ä»¿å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†èƒ½åŠ›æ—¶ï¼Œç”±äºè®­ç»ƒæ•°æ®ï¼ˆLLMsç”Ÿæˆçš„Chain-of-Thought, CoTï¼‰ä¸­å­˜åœ¨å™ªå£°æ¨ç†è€Œå¯¼è‡´çš„ä¼ªç›¸å…³æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç›´æ¥åœ¨CoTæ•°æ®ä¸Šè®­ç»ƒSLMsï¼Œå¿½ç•¥äº†CoTæ¨ç†è¿‡ç¨‹çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´SLMså­¦ä¹ åˆ°é—®é¢˜å’Œç­”æ¡ˆä¹‹é—´çš„è™šå‡å…³è”ï¼Œé™ä½äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ­£ç¡®æ€§æ„ŸçŸ¥æœºåˆ¶ï¼Œè®©SLMèƒ½å¤ŸåŒºåˆ†å’Œåˆ©ç”¨æ­£ç¡®çš„CoTæ¨ç†ï¼ŒåŒæ—¶é¿å…å—åˆ°é”™è¯¯æ¨ç†çš„å½±å“ã€‚å…·ä½“æ¥è¯´ï¼ŒCoPeDé¼“åŠ±æ¨¡å‹åœ¨æ¨ç†é”™è¯¯æ—¶è¿›è¡Œä¿®æ­£ï¼Œå¹¶æ ¹æ®æ¨ç†çš„æ­£ç¡®æ€§åŠ¨æ€è°ƒæ•´è®­ç»ƒæ ·æœ¬çš„æƒé‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¨ç†è´¨é‡å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoPeDåŒ…å«ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šæ­£ç¡®æ€§æ„ŸçŸ¥ä»»åŠ¡è®¾ç½®å’Œæ­£ç¡®æ€§æ„ŸçŸ¥åŠ æƒæŸå¤±ã€‚åœ¨æ­£ç¡®æ€§æ„ŸçŸ¥ä»»åŠ¡è®¾ç½®ä¸­ï¼Œæ¨¡å‹ä¸ä»…éœ€è¦é¢„æµ‹ç­”æ¡ˆï¼Œè¿˜éœ€è¦åˆ¤æ–­æ¨ç†è¿‡ç¨‹æ˜¯å¦æ­£ç¡®ï¼Œå¹¶åœ¨æ¨ç†é”™è¯¯æ—¶ä¿®æ­£ç­”æ¡ˆã€‚åœ¨æ­£ç¡®æ€§æ„ŸçŸ¥åŠ æƒæŸå¤±ä¸­ï¼Œæ¨¡å‹æ ¹æ®æ¨ç†å’Œç­”æ¡ˆçš„ç»„åˆæŸå¤±åŠ¨æ€è°ƒæ•´æ¯ä¸ªè®­ç»ƒå®ä¾‹çš„è´¡çŒ®ï¼Œä½¿å¾—æ¨¡å‹æ›´åŠ å…³æ³¨é‚£äº›æ¨ç†èƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒæ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬ã€‚æ•´ä½“è®­ç»ƒæµç¨‹ä¸ºï¼šé¦–å…ˆï¼Œä½¿ç”¨LLMç”ŸæˆCoTæ•°æ®ï¼›ç„¶åï¼Œä½¿ç”¨CoPeDæ–¹æ³•è®­ç»ƒSLMï¼Œä½¿å…¶èƒ½å¤Ÿè¯†åˆ«å’Œåˆ©ç”¨æ­£ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoPeDçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ­£ç¡®æ€§æ„ŸçŸ¥æœºåˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒCoPeDä¸ä»…å…³æ³¨ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œè¿˜å…³æ³¨æ¨ç†è¿‡ç¨‹çš„æ­£ç¡®æ€§ï¼Œå¹¶åˆ©ç”¨æ¨ç†çš„æ­£ç¡®æ€§æ¥æŒ‡å¯¼æ¨¡å‹çš„å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£CoTæ•°æ®ä¸­çš„ä¼ªç›¸å…³æ€§é—®é¢˜ï¼Œæé«˜æ¨¡å‹çš„æ¨ç†è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šæ­£ç¡®æ€§æ„ŸçŸ¥åŠ æƒæŸå¤±æ˜¯CoPeDçš„å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚è¯¥æŸå¤±å‡½æ•°æ ¹æ®æ¨ç†å’Œç­”æ¡ˆçš„ç»„åˆæŸå¤±åŠ¨æ€è°ƒæ•´æ¯ä¸ªè®­ç»ƒå®ä¾‹çš„æƒé‡ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœæ¨ç†å’Œç­”æ¡ˆéƒ½æ­£ç¡®ï¼Œåˆ™èµ‹äºˆè¾ƒé«˜çš„æƒé‡ï¼›å¦‚æœæ¨ç†é”™è¯¯ä½†ç­”æ¡ˆæ­£ç¡®ï¼Œåˆ™èµ‹äºˆè¾ƒä½çš„æƒé‡ï¼›å¦‚æœæ¨ç†å’Œç­”æ¡ˆéƒ½é”™è¯¯ï¼Œåˆ™èµ‹äºˆæœ€ä½çš„æƒé‡ã€‚è¿™ç§åŠ æƒæ–¹å¼é¼“åŠ±æ¨¡å‹æ›´åŠ å…³æ³¨é‚£äº›æ¨ç†èƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒæ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å…·ä½“çš„æƒé‡è®¡ç®—å…¬å¼æœªçŸ¥ï¼Œéœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†CoPeDåœ¨åŒåˆ†å¸ƒå’Œå¼‚åˆ†å¸ƒæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦è¡¨æ˜CoPeDèƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¨¡å‹çš„æ¨ç†è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ç›´æ¥åœ¨CoTæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼ŒCoPeDèƒ½å¤Ÿæ›´å¥½åœ°ç¼“è§£ä¼ªç›¸å…³æ€§é—®é¢˜ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚å…·ä½“çš„æå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoPeDæ–¹æ³•å¯åº”ç”¨äºå„ç§éœ€è¦è¿›è¡Œå¤æ‚æ¨ç†çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€å¸¸è¯†æ¨ç†å’ŒçŸ¥è¯†å›¾è°±æ¨ç†ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒCoPeDè¿˜å¯ä»¥ç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡çº æ­£æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œæé«˜æ¨¡å‹çš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) excel at reasoning tasks but are expensive to deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated by LLMs to copy LLMs' abilities. However, these CoT data may include noisy rationales that either fail to substantiate the answers or contribute no additional information to support answer prediction, which leads SLMs to capture spurious correlations between questions and answers and compromise the quality of reasoning. In this work, we propose Chain-of-Thought Correctness Perception Distillation (CoPeD), which aims to improve the reasoning quality of the student model from the perspectives of task setting and data utilization. Firstly, we introduce a correctness-aware task setting that encourages the student model to predict answers based on correct rationales and revise them when they are incorrect. This setting improves the faithfulness of reasoning and allows the model to learn from its mistakes. Then, we propose a Correctness-Aware Weighted loss, which dynamically adjusts the contribution of each training instance based on the combined loss of the rationale and the answer. This strategy encourages the model to focus more on samples where the rationale offers stronger support for the correct answer. Experiments have shown that CoPeD is effective on both in-distribution (IND) and out-of-distribution (OOD) benchmark reasoning datasets.

