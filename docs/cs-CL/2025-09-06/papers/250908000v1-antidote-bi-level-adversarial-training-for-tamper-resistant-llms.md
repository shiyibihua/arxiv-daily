---
layout: default
title: AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs
---

# AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08000" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08000v1</a>
  <a href="https://arxiv.org/pdf/2509.08000.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08000v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08000v1', 'AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Debdeep Sanyal, Manodeep Ray, Murari Mandal

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06

**å¤‡æ³¨**: 19 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AntiDoteï¼šé¢å‘æŠ—ç¯¡æ”¹å¤§è¯­è¨€æ¨¡å‹çš„åŒå±‚å¯¹æŠ—è®­ç»ƒæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¯¹æŠ—è®­ç»ƒ` `æŠ—ç¯¡æ”¹` `åŒå±‚ä¼˜åŒ–` `ä½ç§©é€‚åº”` `å®‰å…¨æ€§` `çº¢é˜Ÿæ”»å‡»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå®‰å…¨æªæ–½éš¾ä»¥æŠµæŠ—æ¶æ„æ”»å‡»è€…é€šè¿‡å¾®è°ƒæ¶ˆé™¤å®‰å…¨é˜²æŠ¤ï¼Œå°¤å…¶æ˜¯åœ¨æ”»å‡»è€…æ‹¥æœ‰æ¨¡å‹å…¨éƒ¨æƒé‡å’Œæ¶æ„è®¿é—®æƒé™æ—¶ã€‚
2. AntiDoteé€šè¿‡åŒå±‚ä¼˜åŒ–ï¼Œè®­ç»ƒLLMæŠµæŠ—ç¯¡æ”¹ï¼Œåˆ©ç”¨å¯¹æŠ—è¶…ç½‘ç»œç”Ÿæˆæ¶æ„LoRAæƒé‡ï¼Œå¹¶è®­ç»ƒé˜²å¾¡æ¨¡å‹æ¶ˆé™¤å…¶å½±å“ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAntiDoteåœ¨æŠµæŠ—å¯¹æŠ—æ”»å‡»æ–¹é¢æ¯”ç°æœ‰æ–¹æ³•æ›´é²æ£’ï¼Œæå‡é«˜è¾¾27.4%ï¼Œä¸”å¯¹æ¨¡å‹é€šç”¨èƒ½åŠ›å½±å“æå°ï¼Œæ€§èƒ½ä¸‹é™å°äº0.5%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼€æ”¾æƒé‡çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å¸ƒï¼Œåœ¨æ¨è¿›å¯è®¿é—®ç ”ç©¶å’Œé˜²æ­¢æ¶æ„ä½¿ç”¨ï¼ˆå¦‚å¼•å‘æœ‰å®³å†…å®¹çš„æ¶æ„å¾®è°ƒï¼‰ä¹‹é—´é€ æˆäº†ä¸€ç§ç´§å¼ å…³ç³»ã€‚ç°æœ‰çš„å®‰å…¨æªæ–½éš¾ä»¥åœ¨ä¿æŒLLMé€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼ŒæŠµæŠ—èƒ½å¤Ÿå®Œå…¨è®¿é—®æ¨¡å‹æƒé‡å’Œæ¶æ„çš„åšå®šå¯¹æŠ—è€…ï¼Œè¿™äº›å¯¹æŠ—è€…å¯ä»¥ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒæ¥æ¶ˆé™¤ç°æœ‰çš„å®‰å…¨æªæ–½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AntiDoteï¼Œè¿™æ˜¯ä¸€ç§åŒå±‚ä¼˜åŒ–ç¨‹åºï¼Œç”¨äºè®­ç»ƒLLMä»¥æŠµæŠ—è¿™ç§ç¯¡æ”¹ã€‚AntiDoteæ¶‰åŠä¸€ä¸ªè¾…åŠ©å¯¹æŠ—è¶…ç½‘ç»œï¼Œè¯¥ç½‘ç»œå­¦ä¹ ç”Ÿæˆæ¶æ„çš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æƒé‡ï¼Œè¿™äº›æƒé‡ä»¥é˜²å¾¡è€…æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»ä¸ºæ¡ä»¶ã€‚ç„¶åï¼Œè®­ç»ƒé˜²å¾¡è€…LLMçš„ç›®æ ‡æ˜¯æ¶ˆé™¤è¿™äº›å¯¹æŠ—æ€§æƒé‡æ·»åŠ çš„å½±å“ï¼Œè¿«ä½¿å…¶ä¿æŒå…¶å®‰å…¨å¯¹é½ã€‚æˆ‘ä»¬é’ˆå¯¹åŒ…å«52ä¸ªçº¢é˜Ÿæ”»å‡»çš„å¤šæ ·åŒ–å¥—ä»¶éªŒè¯äº†è¿™ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬è¶Šç‹±æç¤ºã€æ½œåœ¨ç©ºé—´æ“çºµå’Œç›´æ¥æƒé‡ç©ºé—´æ”»å‡»ã€‚ä¸æŠ—ç¯¡æ”¹å’Œéå­¦ä¹ åŸºçº¿ç›¸æ¯”ï¼ŒAntiDoteå¯¹æŠ—å¯¹æŠ—æ€§æ”»å‡»çš„é²æ£’æ€§æé«˜äº†27.4%ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œè¿™ç§é²æ£’æ€§æ˜¯åœ¨æ•ˆç”¨æ–¹é¢ä»¥æœ€å°çš„æƒè¡¡å®ç°çš„ï¼Œåœ¨åŒ…æ‹¬MMLUã€HellaSwagå’ŒGSM8Kåœ¨å†…çš„èƒ½åŠ›åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¸‹é™ä¸åˆ°0.5%ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæ„å»ºå¼€æ”¾æƒé‡æ¨¡å‹æä¾›äº†ä¸€ç§å®ç”¨ä¸”è®¡ç®—é«˜æ•ˆçš„æ–¹æ³•ï¼Œå…¶ä¸­å®‰å…¨æ€§æ˜¯ä¸€ç§æ›´å®Œæ•´å’Œæ›´å…·å¼¹æ€§çš„å±æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼€æ”¾æƒé‡LLMå®¹æ˜“è¢«æ¶æ„ç¯¡æ”¹çš„é—®é¢˜ã€‚ç°æœ‰çš„å®‰å…¨æªæ–½æ— æ³•æœ‰æ•ˆæŠµæŠ—æ‹¥æœ‰æ¨¡å‹å…¨éƒ¨è®¿é—®æƒé™çš„æ”»å‡»è€…ï¼Œä»–ä»¬å¯ä»¥é€šè¿‡å¾®è°ƒæ¥ç§»é™¤æˆ–ç»•è¿‡å®‰å…¨æœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹äº§ç”Ÿæœ‰å®³å†…å®¹ã€‚è¿™ç§æ”»å‡»æ–¹å¼çš„ç—›ç‚¹åœ¨äºï¼Œæ”»å‡»è€…å¯ä»¥é’ˆå¯¹æ€§åœ°ä¿®æ”¹æ¨¡å‹æƒé‡ï¼Œè€Œç°æœ‰çš„é˜²å¾¡æ–¹æ³•éš¾ä»¥åœ¨ä¸ç‰ºç‰²æ¨¡å‹é€šç”¨èƒ½åŠ›çš„å‰æä¸‹è¿›è¡Œæœ‰æ•ˆé˜²å¾¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨åŒå±‚å¯¹æŠ—è®­ç»ƒï¼Œæ¨¡æ‹Ÿæ”»å‡»è€…å’Œé˜²å¾¡è€…ä¹‹é—´çš„åšå¼ˆã€‚æ”»å‡»è€…ï¼ˆé€šè¿‡è¶…ç½‘ç»œï¼‰å­¦ä¹ å¦‚ä½•ç”Ÿæˆèƒ½å¤Ÿç ´åæ¨¡å‹å®‰å…¨æ€§çš„æ¶æ„æƒé‡ï¼Œè€Œé˜²å¾¡è€…åˆ™å­¦ä¹ å¦‚ä½•æŠµæŠ—è¿™äº›æ¶æ„æƒé‡çš„å½±å“ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æŠ—ç¯¡æ”¹èƒ½åŠ›ã€‚è¿™ç§å¯¹æŠ—è®­ç»ƒè¿«ä½¿æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾è¡¨ç¤ºï¼Œä½¿å…¶å¯¹æƒé‡æ‰°åŠ¨æ›´åŠ ä¸æ•æ„Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAntiDoteçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šé˜²å¾¡è€…LLMå’Œä¸€ä¸ªè¾…åŠ©å¯¹æŠ—è¶…ç½‘ç»œã€‚é¦–å…ˆï¼Œå¯¹æŠ—è¶…ç½‘ç»œä»¥é˜²å¾¡è€…LLMçš„å†…éƒ¨æ¿€æ´»ä¸ºæ¡ä»¶ï¼Œç”Ÿæˆæ¶æ„çš„LoRAæƒé‡ã€‚ç„¶åï¼Œå°†è¿™äº›æ¶æ„æƒé‡æ·»åŠ åˆ°é˜²å¾¡è€…LLMä¸­ï¼Œæ¨¡æ‹Ÿæ”»å‡»ã€‚æ¥ä¸‹æ¥ï¼Œè®­ç»ƒé˜²å¾¡è€…LLMï¼Œä½¿å…¶èƒ½å¤Ÿæœ€å°åŒ–åœ¨å—åˆ°æ”»å‡»åçš„æ€§èƒ½æŸå¤±ï¼Œä»è€Œæé«˜å…¶æŠ—ç¯¡æ”¹èƒ½åŠ›ã€‚è¿™ä¸ªè¿‡ç¨‹é€šè¿‡åŒå±‚ä¼˜åŒ–æ¥å®ç°ï¼Œå…¶ä¸­å¯¹æŠ—è¶…ç½‘ç»œçš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–é˜²å¾¡è€…çš„æŸå¤±ï¼Œè€Œé˜²å¾¡è€…çš„ç›®æ ‡æ˜¯æœ€å°åŒ–å…¶è‡ªèº«çš„æŸå¤±ã€‚

**å…³é”®åˆ›æ–°**ï¼šAntiDoteçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨å¯¹æŠ—è¶…ç½‘ç»œæ¥ç”Ÿæˆæ¶æ„çš„LoRAæƒé‡ã€‚ä¸ä¼ ç»Ÿçš„å¯¹æŠ—è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒAntiDoteä¸æ˜¯ç›´æ¥ä¿®æ”¹è¾“å…¥æ•°æ®ï¼Œè€Œæ˜¯ä¿®æ”¹æ¨¡å‹çš„æƒé‡ï¼Œä»è€Œæ¨¡æ‹Ÿæ›´çœŸå®çš„æ”»å‡»åœºæ™¯ã€‚æ­¤å¤–ï¼Œä½¿ç”¨LoRAæƒé‡å¯ä»¥å‡å°‘è®¡ç®—æˆæœ¬ï¼Œå¹¶å…è®¸åœ¨ä¸å½±å“æ¨¡å‹é€šç”¨èƒ½åŠ›çš„å‰æä¸‹è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®è®¾è®¡**ï¼šAntiDoteçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¯¹æŠ—è¶…ç½‘ç»œçš„ç»“æ„ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°ç”Ÿæˆæ¶æ„çš„LoRAæƒé‡ï¼›2) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¡é‡æ¨¡å‹åœ¨å—åˆ°æ”»å‡»åçš„æ€§èƒ½æŸå¤±ï¼›3) åŒå±‚ä¼˜åŒ–çš„ç®—æ³•ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡æ”»å‡»è€…å’Œé˜²å¾¡è€…ä¹‹é—´çš„ç›®æ ‡ã€‚è®ºæ–‡ä¸­å…·ä½“ä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è¡¡é‡æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½æŸå¤±ï¼Œå¹¶ä½¿ç”¨Adamä¼˜åŒ–å™¨æ¥è®­ç»ƒå¯¹æŠ—è¶…ç½‘ç»œå’Œé˜²å¾¡è€…LLMã€‚LoRAçš„ç§©ï¼ˆrankï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAntiDoteåœ¨æŠµæŠ—52ç§çº¢é˜Ÿæ”»å‡»ï¼ˆåŒ…æ‹¬è¶Šç‹±æç¤ºã€æ½œåœ¨ç©ºé—´æ“çºµå’Œç›´æ¥æƒé‡ç©ºé—´æ”»å‡»ï¼‰æ–¹é¢ï¼Œæ¯”æŠ—ç¯¡æ”¹å’Œéå­¦ä¹ åŸºçº¿æ–¹æ³•æé«˜äº†é«˜è¾¾27.4%çš„é²æ£’æ€§ã€‚åŒæ—¶ï¼ŒAntiDoteå¯¹æ¨¡å‹é€šç”¨èƒ½åŠ›çš„æŸå®³æå°ï¼Œåœ¨MMLUã€HellaSwagå’ŒGSM8Kç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¸‹é™å°äº0.5%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒAntiDoteæ˜¯ä¸€ç§å®ç”¨ä¸”æœ‰æ•ˆçš„æŠ—ç¯¡æ”¹æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AntiDoteæŠ€æœ¯å¯åº”ç”¨äºå„ç§éœ€è¦é˜²æ­¢æ¶æ„ç¯¡æ”¹çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚ç”¨äºå†…å®¹å®¡æ ¸ã€ä»£ç ç”Ÿæˆã€æ™ºèƒ½å®¢æœç­‰åœºæ™¯çš„æ¨¡å‹ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„æŠ—ç¯¡æ”¹èƒ½åŠ›ï¼Œå¯ä»¥é™ä½æ¨¡å‹è¢«ç”¨äºç”Ÿæˆæœ‰å®³ä¿¡æ¯æˆ–æ‰§è¡Œæ¶æ„ä»»åŠ¡çš„é£é™©ï¼Œä»è€Œä¿éšœç”¨æˆ·å®‰å…¨å’Œç¤¾ä¼šç¨³å®šã€‚è¯¥æŠ€æœ¯è¿˜æœ‰åŠ©äºæ¨åŠ¨å®‰å…¨å¯é çš„å¼€æºLLMç”Ÿæ€å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The release of open-weight large language models (LLMs) creates a tension between advancing accessible research and preventing misuse, such as malicious fine-tuning to elicit harmful content. Current safety measures struggle to preserve the general capabilities of the LLM while resisting a determined adversary with full access to the model's weights and architecture, who can use full-parameter fine-tuning to erase existing safeguards. To address this, we introduce AntiDote, a bi-level optimization procedure for training LLMs to be resistant to such tampering. AntiDote involves an auxiliary adversary hypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA) weights conditioned on the defender model's internal activations. The defender LLM is then trained with an objective to nullify the effect of these adversarial weight additions, forcing it to maintain its safety alignment. We validate this approach against a diverse suite of 52 red-teaming attacks, including jailbreak prompting, latent space manipulation, and direct weight-space attacks. AntiDote is upto 27.4\% more robust against adversarial attacks compared to both tamper-resistance and unlearning baselines. Crucially, this robustness is achieved with a minimal trade-off in utility, incurring a performance degradation of upto less than 0.5\% across capability benchmarks including MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute efficient methodology for building open-weight models where safety is a more integral and resilient property.

