---
layout: default
title: Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study
---

# Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05553" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05553v1</a>
  <a href="https://arxiv.org/pdf/2509.05553.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05553v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05553v1', 'Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Serge Lionel Nikiema, Jordan Samhi, Micheline BÃ©nÃ©dicte Moumoula, AlbÃ©rick Euraste DjirÃ©, Abdoul Kader KaborÃ©, Jacques Klein, TegawendÃ© F. BissyandÃ©

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æ¯”å­¦ä¹ å¾®è°ƒæ–¹æ³•ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç æ··æ·†ä»»åŠ¡ä¸­çš„åŒå‘æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `åŒå‘æ¨ç†` `å¤§è¯­è¨€æ¨¡å‹` `ä»£ç æ··æ·†` `è®¤çŸ¥ä¸“ä¸šåŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ­£å‘ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åå‘æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œè¡¨æ˜æ¨¡å‹å¯èƒ½åªæ˜¯åœ¨è¿›è¡Œæ¨¡å¼åŒ¹é…ï¼Œç¼ºä¹çœŸæ­£çš„ç†è§£ã€‚
2. è®ºæ–‡æå‡ºå¯¹æ¯”å¾®è°ƒï¼ˆCFTï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ­£ã€è´Ÿæ ·æœ¬å’Œæ­£å‘æ··æ·†æ ·æœ¬ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ æ·±å±‚è¯­ä¹‰ï¼Œä»è€Œæå‡åŒå‘æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCFTæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨ä»£ç æ··æ·†ä»»åŠ¡ä¸­çš„åŒå‘æ¨ç†èƒ½åŠ›ï¼Œåœ¨ä¿æŒæ­£å‘æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡åå‘æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†ä¸€ä¸ªäººå·¥æ™ºèƒ½é¢†åŸŸçš„åŸºç¡€é—®é¢˜ï¼šå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯çœŸæ­£ç†è§£æ¦‚å¿µï¼Œè¿˜æ˜¯ä»…ä»…è¯†åˆ«æ¨¡å¼ï¼Ÿä½œè€…æå‡ºåŒå‘æ¨ç†èƒ½åŠ›ä½œä¸ºè¡¡é‡æ¨¡å‹æ˜¯å¦çœŸæ­£ç†è§£æ¦‚å¿µçš„æ ‡å‡†ï¼Œå³æ¨¡å‹æ— éœ€åœ¨åå‘æ–¹å‘ä¸Šè¿›è¡Œæ˜¾å¼è®­ç»ƒï¼Œä¹Ÿèƒ½åº”ç”¨åå‘è½¬æ¢ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨æ­£å‘ä»»åŠ¡ä¸Šå¾®è°ƒåï¼Œæ€§èƒ½æå‡ï¼Œä½†åŒå‘æ¨ç†èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œä½œè€…ç§°ä¹‹ä¸ºè®¤çŸ¥ä¸“ä¸šåŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»–ä»¬å¼€å‘äº†å¯¹æ¯”å¾®è°ƒï¼ˆCFTï¼‰æ–¹æ³•ï¼Œä½¿ç”¨ä¸‰ç±»æ ·æœ¬è®­ç»ƒæ¨¡å‹ï¼šä¿æŒè¯­ä¹‰å«ä¹‰çš„æ­£æ ·æœ¬ã€å…·æœ‰ä¸åŒè¯­ä¹‰çš„è´Ÿæ ·æœ¬å’Œæ­£å‘æ··æ·†æ ·æœ¬ã€‚è¯¥æ–¹æ³•æ—¨åœ¨åŸ¹å…»æ›´æ·±å±‚æ¬¡çš„ç†è§£ï¼Œè€Œéè¡¨é¢æ¨¡å¼è¯†åˆ«ï¼Œå¹¶å…è®¸åå‘èƒ½åŠ›åœ¨æ²¡æœ‰æ˜¾å¼åå‘è®­ç»ƒçš„æƒ…å†µä¸‹è‡ªç„¶å‘å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒCFTæˆåŠŸå®ç°äº†åŒå‘æ¨ç†ï¼Œåœ¨ä¿æŒæ­£å‘ä»»åŠ¡èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†å¼ºå¤§çš„åå‘æ€§èƒ½ã€‚ä½œè€…è®¤ä¸ºï¼ŒåŒå‘æ¨ç†æ—¢æ˜¯è¯„ä¼°çœŸæ­£ç†è§£çš„ç†è®ºæ¡†æ¶ï¼Œä¹Ÿæ˜¯å¼€å‘æ›´å¼ºå¤§AIç³»ç»Ÿçš„å®ç”¨è®­ç»ƒæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒå‘æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä»£ç æ··æ·†ä»»åŠ¡ä¸­ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥åœ¨æ­£å‘ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¼šå¯¼è‡´æ¨¡å‹åœ¨æ­£å‘ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åå‘æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œè¿™è¡¨æ˜æ¨¡å‹å¯èƒ½åªæ˜¯åœ¨è¿›è¡Œæ¨¡å¼åŒ¹é…ï¼Œç¼ºä¹å¯¹æ¦‚å¿µçš„çœŸæ­£ç†è§£ã€‚è¿™ç§â€œè®¤çŸ¥ä¸“ä¸šåŒ–â€é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¯¹æ¯”å­¦ä¹ çš„æ€æƒ³ï¼Œé€šè¿‡å¼•å…¥æ­£æ ·æœ¬ã€è´Ÿæ ·æœ¬å’Œæ­£å‘æ··æ·†æ ·æœ¬ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ æ·±å±‚è¯­ä¹‰è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–äºè¡¨é¢æ¨¡å¼ã€‚æ­£æ ·æœ¬ä¿æŒè¯­ä¹‰ä¸å˜ï¼Œè´Ÿæ ·æœ¬æ”¹å˜è¯­ä¹‰ï¼Œæ­£å‘æ··æ·†æ ·æœ¬åˆ™æ¨¡æ‹Ÿäº†éœ€è¦è¿›è¡Œåå‘æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ¦‚å¿µçš„æœ¬è´¨ï¼Œä»è€Œæå‡åŒå‘æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šæ•°æ®æ„å»ºã€å¯¹æ¯”å¾®è°ƒå’Œè¯„ä¼°ã€‚æ•°æ®æ„å»ºé˜¶æ®µï¼Œç”Ÿæˆæ­£æ ·æœ¬ã€è´Ÿæ ·æœ¬å’Œæ­£å‘æ··æ·†æ ·æœ¬ã€‚å¯¹æ¯”å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨è¿™äº›æ ·æœ¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç›®æ ‡æ˜¯ä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼Œå¹¶èƒ½å¤Ÿä»æ­£å‘æ··æ·†æ ·æœ¬ä¸­æ¨æ–­å‡ºåŸå§‹è¯­ä¹‰ã€‚è¯„ä¼°é˜¶æ®µï¼Œä½¿ç”¨æ­£å‘å’Œåå‘æ¨ç†ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯å¯¹æ¯”å¾®è°ƒï¼ˆCFTï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼•å…¥è´Ÿæ ·æœ¬å’Œæ­£å‘æ··æ·†æ ·æœ¬ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†æ¨¡å‹åœ¨åŒå‘æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒCFTæ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¼•å¯¼æ¨¡å‹å­¦ä¹ æ·±å±‚è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCFTæ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ­£è´Ÿæ ·æœ¬çš„æ„å»ºæ–¹å¼ï¼Œéœ€è¦ä¿è¯æ­£æ ·æœ¬åœ¨è¯­ä¹‰ä¸Šä¸åŸå§‹æ ·æœ¬ä¸€è‡´ï¼Œè€Œè´Ÿæ ·æœ¬åœ¨è¯­ä¹‰ä¸Šä¸åŸå§‹æ ·æœ¬ä¸åŒï¼›2) æ­£å‘æ··æ·†æ ·æœ¬çš„ç”Ÿæˆæ–¹å¼ï¼Œéœ€è¦æ¨¡æ‹Ÿéœ€è¦è¿›è¡Œåå‘æ¨ç†çš„åœºæ™¯ï¼›3) å¯¹æ¯”æŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ã€‚è®ºæ–‡ä¸­å…·ä½“ä½¿ç”¨çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®æœªæ˜ç¡®è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹æ¯”å¾®è°ƒï¼ˆCFTï¼‰æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨ä»£ç æ··æ·†ä»»åŠ¡ä¸­çš„åŒå‘æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿æœªåœ¨æ‘˜è¦ä¸­æ˜ç¡®ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†ç»“è®ºæ˜ç¡®æŒ‡å‡ºï¼ŒCFTåœ¨ä¿æŒæ­£å‘ä»»åŠ¡èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†å¼ºå¤§çš„åå‘æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä»£ç ç†è§£ã€ç¨‹åºä¿®å¤ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨ä»£ç ç†è§£ä¸­ï¼Œå¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»£ç çš„è¯­ä¹‰ï¼Œä»è€Œè¿›è¡Œä»£ç åˆ†æã€æ¼æ´æ£€æµ‹ç­‰ä»»åŠ¡ã€‚åœ¨ç¨‹åºä¿®å¤ä¸­ï¼Œå¯ä»¥å¸®åŠ©æ¨¡å‹ç†è§£é”™è¯¯ä»£ç çš„æ„å›¾ï¼Œä»è€Œç”Ÿæˆæ­£ç¡®çš„ä»£ç ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨è¯­ä¹‰æ¨ç†ã€é—®ç­”ç³»ç»Ÿç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This research addresses a fundamental question in AI: whether large language models truly understand concepts or simply recognize patterns. The authors propose bidirectional reasoning,the ability to apply transformations in both directions without being explicitly trained on the reverse direction, as a test for genuine understanding. They argue that true comprehension should naturally allow reversibility. For example, a model that can change a variable name like userIndex to i should also be able to infer that i represents a user index without reverse training. The researchers tested current language models and discovered what they term cognitive specialization: when models are fine-tuned on forward tasks, their performance on those tasks improves, but their ability to reason bidirectionally becomes significantly worse. To address this issue, they developed Contrastive Fine-Tuning (CFT), which trains models using three types of examples: positive examples that maintain semantic meaning, negative examples with different semantics, and forward-direction obfuscation examples. This approach aims to develop deeper understanding rather than surface-level pattern recognition and allows reverse capabilities to develop naturally without explicit reverse training. Their experiments demonstrated that CFT successfully achieved bidirectional reasoning, enabling strong reverse performance while maintaining forward task capabilities. The authors conclude that bidirectional reasoning serves both as a theoretical framework for assessing genuine understanding and as a practical training approach for developing more capable AI systems.

