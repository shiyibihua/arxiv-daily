---
layout: default
title: SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning
---

# SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15212" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15212v3</a>
  <a href="https://arxiv.org/pdf/2508.15212.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15212v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15212v3', 'SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-21 (æ›´æ–°: 2025-11-12)

**å¤‡æ³¨**: accepted to AAAI 2026

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Xnhyacinth/SparK)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSparKä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„KVç¼“å­˜ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡æ¨ç†` `KVç¼“å­˜` `é€šé“çº§ä¿®å‰ª` `éç»“æ„åŒ–ç¨€ç–æ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `è®¡ç®—æ•ˆç‡` `æ¨¡å‹å‡†ç¡®æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ¨ç†æ—¶ï¼ŒKVç¼“å­˜çš„å†…å­˜ä½¿ç”¨ä¸åºåˆ—é•¿åº¦æˆçº¿æ€§å…³ç³»ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚
2. SparKé€šè¿‡åœ¨é€šé“çº§åˆ«ä¿®å‰ªKVç¼“å­˜ï¼ŒåŠ¨æ€æ¢å¤ä¿®å‰ªæ¡ç›®ï¼Œè§£å†³äº†ç‰¹å¾ç»´åº¦é‡è¦æ€§å˜åŒ–çš„é—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSparKåœ¨80%çš„ä¿®å‰ªæ¯”ç‡ä¸‹ï¼Œæ€§èƒ½ä¸‹é™å°äº5%ï¼Œå¹¶ä¸”KVç¼“å­˜å­˜å‚¨å‡å°‘è¶…è¿‡30%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œé•¿ä¸Šä¸‹æ–‡æ¨ç†å—åˆ°KVç¼“å­˜ç“¶é¢ˆçš„é™åˆ¶ï¼šå†…å­˜ä½¿ç”¨éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼Œè€Œæ³¨æ„åŠ›è®¡ç®—åˆ™å‘ˆäºŒæ¬¡å¢é•¿ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡åœ¨æ—¶é—´è½´ä¸Šå‹ç¼©KVç¼“å­˜æ¥åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œä½†å¾€å¾€å¿½è§†äº†ç‰¹å¾ç»´åº¦ä¸Šçš„ç»†ç²’åº¦é‡è¦æ€§å˜åŒ–ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSparKçš„è®­ç»ƒæ— å…³çš„å³æ’å³ç”¨æ–¹æ³•ï¼Œé€šè¿‡åœ¨é€šé“çº§åˆ«ä¿®å‰ªKVå®ç°éç»“æ„åŒ–ç¨€ç–æ€§ï¼Œå¹¶åœ¨æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—è¿‡ç¨‹ä¸­åŠ¨æ€æ¢å¤ä¿®å‰ªçš„æ¡ç›®ã€‚SparKèƒ½å¤Ÿåœ¨ç›¸åŒå†…å­˜é¢„ç®—ä¸‹å¤„ç†æ›´é•¿çš„åºåˆ—ï¼Œå¹¶åœ¨ä¿æŒæˆ–æé«˜æ¨¡å‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œè¾ƒåŸºäºé©±é€çš„æ–¹æ³•å‡å°‘è¶…è¿‡30%çš„KVç¼“å­˜å­˜å‚¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„KVç¼“å­˜ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æ—¶é—´è½´å‹ç¼©KVç¼“å­˜ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨ç‰¹å¾ç»´åº¦ä¸Šçš„é‡è¦æ€§å˜åŒ–ï¼Œå¯¼è‡´æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSparKçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨é€šé“çº§åˆ«è¿›è¡ŒKVä¿®å‰ªï¼Œåˆ©ç”¨åŠ¨æ€æ¢å¤æœºåˆ¶ï¼Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºè®­ç»ƒè¿‡ç¨‹ï¼Œä¾¿äºå¿«é€Ÿé›†æˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSparKçš„æ•´ä½“æ¶æ„åŒ…æ‹¬KVç¼“å­˜çš„é€šé“çº§ä¿®å‰ªæ¨¡å—å’ŒåŠ¨æ€æ¢å¤æ¨¡å—ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆå¯¹KVç¼“å­˜è¿›è¡Œä¿®å‰ªï¼Œç„¶ååœ¨è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°æ—¶åŠ¨æ€æ¢å¤å¿…è¦çš„é€šé“ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šSparKçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åœ¨é€šé“çº§åˆ«çš„éç»“æ„åŒ–ç¨€ç–æ€§å¤„ç†ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„æ—¶é—´è½´å‹ç¼©æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´çµæ´»åœ°åº”å¯¹ç‰¹å¾ç»´åº¦çš„å˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSparKé‡‡ç”¨äº†åŠ¨æ€æ¢å¤æœºåˆ¶ï¼Œç¡®ä¿åœ¨æ³¨æ„åŠ›è®¡ç®—æ—¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ä¿®å‰ªåçš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSparKåœ¨80%çš„ä¿®å‰ªæ¯”ç‡ä¸‹ï¼Œæ€§èƒ½ä¸‹é™å°äº5%ï¼Œç›¸æ¯”äºåŸºäºé©±é€çš„æ–¹æ³•ï¼ŒKVç¼“å­˜å­˜å‚¨å‡å°‘è¶…è¿‡30%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜SparKåœ¨ä¿æŒæ¨¡å‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SparKçš„ç ”ç©¶æˆæœåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ”¯æŒæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.

