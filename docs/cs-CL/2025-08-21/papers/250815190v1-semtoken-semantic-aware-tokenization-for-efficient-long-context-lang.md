---
layout: default
title: SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling
---

# SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15190" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15190v1</a>
  <a href="https://arxiv.org/pdf/2508.15190.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15190v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15190v1', 'SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dong Liu, Yanxuan Yu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-21

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSemTokenä»¥è§£å†³é•¿æ–‡æœ¬è¯­è¨€å»ºæ¨¡ä¸­çš„è¯­ä¹‰æ„ŸçŸ¥åˆ†è¯é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­ä¹‰æ„ŸçŸ¥` `åˆ†è¯` `é•¿æ–‡æœ¬å»ºæ¨¡` `è¯­è¨€æ¨¡å‹` `è®¡ç®—æ•ˆç‡` `ä¸Šä¸‹æ–‡ç†è§£` `èšç±»ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åˆ†è¯æ–¹æ³•ä¸»è¦ä¾èµ–é¢‘ç‡ç»Ÿè®¡ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ–‡æœ¬çš„è¯­ä¹‰ç»“æ„ï¼Œå¯¼è‡´å†—ä½™åˆ†è¯å’Œä¸Šä¸‹æ–‡è¿è´¯æ€§ä¸è¶³ã€‚
2. SemTokené€šè¿‡æå–ä¸Šä¸‹æ–‡è¯­ä¹‰åµŒå…¥å’Œå±€éƒ¨è¯­ä¹‰èšç±»ï¼Œä¼˜åŒ–äº†åˆ†è¯è¿‡ç¨‹ï¼Œå‡å°‘äº†å†—ä½™å¹¶æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚
3. åœ¨WikiText-103å’ŒLongBenchç­‰é•¿æ–‡æœ¬è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šï¼ŒSemTokenå®ç°äº†2.4å€çš„æ ‡è®°å‡å°‘å’Œ1.9å€çš„é€Ÿåº¦æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆ†è¯åœ¨è¯­è¨€å»ºæ¨¡ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•å¦‚å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰æˆ–WordPieceä»…åŸºäºé¢‘ç‡ç»Ÿè®¡ï¼Œå¿½è§†äº†æ–‡æœ¬çš„è¯­ä¹‰ç»“æ„ã€‚è¿™å¯¼è‡´è¯­ä¹‰å†—ä½™çš„ç‰‡æ®µè¢«è¿‡åº¦åˆ†è¯ï¼Œè€Œä¸Šä¸‹æ–‡è¿è´¯æ€§æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸­ã€‚æœ¬æ–‡æå‡ºäº†SemTokenï¼Œä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥çš„åˆ†è¯æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘åˆ†è¯å†—ä½™å¹¶æé«˜è®¡ç®—æ•ˆç‡ã€‚SemTokené¦–å…ˆé€šè¿‡è½»é‡çº§ç¼–ç å™¨æå–ä¸Šä¸‹æ–‡è¯­ä¹‰åµŒå…¥ï¼Œå¹¶è¿›è¡Œå±€éƒ¨è¯­ä¹‰èšç±»ä»¥åˆå¹¶è¯­ä¹‰ç­‰ä»·çš„æ ‡è®°ã€‚ç„¶åï¼Œæ ¹æ®è¯­ä¹‰å¯†åº¦åˆ†é…å¼‚æ„çš„æ ‡è®°ç²’åº¦ï¼Œåœ¨å†…å®¹ä¸°å¯Œçš„åŒºåŸŸè¿›è¡Œæ›´ç»†ç²’åº¦çš„åˆ†è¯ï¼Œè€Œåœ¨é‡å¤æˆ–ä½ç†µçš„ç‰‡æ®µä¸­è¿›è¡Œç²—ç²’åº¦å‹ç¼©ã€‚å®éªŒè¡¨æ˜ï¼ŒSemTokenåœ¨é•¿æ–‡æœ¬è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šå®ç°äº†é«˜è¾¾2.4å€çš„æ ‡è®°æ•°é‡å‡å°‘å’Œ1.9å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶åœ¨å›°æƒ‘åº¦å’Œä¸‹æ¸¸å‡†ç¡®æ€§ä¸Šå‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åˆ†è¯æ–¹æ³•åœ¨é•¿æ–‡æœ¬è¯­è¨€å»ºæ¨¡ä¸­å­˜åœ¨çš„è¯­ä¹‰å†—ä½™å’Œä¸Šä¸‹æ–‡è¿è´¯æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚BPEå’ŒWordPieceä»…ä¾èµ–äºé¢‘ç‡ç»Ÿè®¡ï¼Œæœªè€ƒè™‘æ–‡æœ¬çš„è¯­ä¹‰ç»“æ„ï¼Œå¯¼è‡´è¿‡åº¦åˆ†è¯å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSemTokençš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¯­ä¹‰æ„ŸçŸ¥çš„æ–¹å¼è¿›è¡Œåˆ†è¯ï¼Œé¦–å…ˆæå–ä¸Šä¸‹æ–‡çš„è¯­ä¹‰åµŒå…¥ï¼Œç„¶åè¿›è¡Œå±€éƒ¨è¯­ä¹‰èšç±»ï¼Œä»¥åˆå¹¶è¯­ä¹‰ç›¸ä¼¼çš„æ ‡è®°ï¼Œä»è€Œå‡å°‘å†—ä½™å¹¶æé«˜è®¡ç®—æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSemTokençš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯è½»é‡çº§ç¼–ç å™¨ç”¨äºæå–ä¸Šä¸‹æ–‡è¯­ä¹‰åµŒå…¥ï¼Œå…¶æ¬¡æ˜¯å±€éƒ¨è¯­ä¹‰èšç±»æ¨¡å—ç”¨äºåˆå¹¶è¯­ä¹‰ç­‰ä»·çš„æ ‡è®°ã€‚æœ€åï¼Œæ ¹æ®è¯­ä¹‰å¯†åº¦åŠ¨æ€è°ƒæ•´æ ‡è®°çš„ç²’åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šSemTokençš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥çš„åˆ†è¯ç­–ç•¥ï¼Œé€šè¿‡è¯­ä¹‰èšç±»å’ŒåŠ¨æ€ç²’åº¦åˆ†é…ï¼Œæ˜¾è‘—æ”¹å–„äº†åˆ†è¯çš„æ•ˆç‡å’Œæ•ˆæœã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é¢‘ç‡ç»Ÿè®¡æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒSemTokené‡‡ç”¨äº†è½»é‡çº§çš„ç¼–ç å™¨ä»¥é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶é€šè¿‡å±€éƒ¨èšç±»ç®—æ³•å®ç°è¯­ä¹‰æ ‡è®°çš„åˆå¹¶ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€ç²’åº¦åˆ†é…ç­–ç•¥ä½¿å¾—åœ¨å†…å®¹ä¸°å¯ŒåŒºåŸŸä½¿ç”¨ç»†ç²’åº¦åˆ†è¯ï¼Œè€Œåœ¨ä½ç†µåŒºåŸŸåˆ™é‡‡ç”¨ç²—ç²’åº¦å‹ç¼©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SemTokenåœ¨é•¿æ–‡æœ¬è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ ‡è®°æ•°é‡å‡å°‘é«˜è¾¾2.4å€ï¼Œè®¡ç®—é€Ÿåº¦æå‡1.9å€ï¼ŒåŒæ—¶åœ¨å›°æƒ‘åº¦å’Œä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®æ€§ä¸Šå‡ ä¹æ²¡æœ‰ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SemTokençš„ç ”ç©¶æˆæœåœ¨é•¿æ–‡æœ¬å¤„ç†ã€è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶è¯­ä¹‰æ„ŸçŸ¥çš„åˆ†è¯æ–¹æ³•å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ç­‰æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tokenization plays a critical role in language modeling, yet existing approaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on frequency statistics, ignoring the underlying semantic structure of text. This leads to over-tokenization of semantically redundant spans and underutilization of contextual coherence, particularly in long-context scenarios. In this work, we propose \textbf{SemToken}, a semantic-aware tokenization framework that jointly reduces token redundancy and improves computation efficiency. SemToken first extracts contextual semantic embeddings via lightweight encoders and performs local semantic clustering to merge semantically equivalent tokens. Then, it allocates heterogeneous token granularity based on semantic density, allowing finer-grained tokenization in content-rich regions and coarser compression in repetitive or low-entropy spans. SemToken can be seamlessly integrated with modern language models and attention acceleration methods. Experiments on long-context language modeling benchmarks such as WikiText-103 and LongBench show that SemToken achieves up to $2.4\times$ reduction in token count and $1.9\times$ speedup, with negligible or no degradation in perplexity and downstream accuracy. Our findings suggest that semantic structure offers a promising new axis for optimizing tokenization and computation in large language models.

