---
layout: default
title: CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning
---

# CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15868" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.15868v2</a>
  <a href="https://arxiv.org/pdf/2508.15868.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15868v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15868v2', 'CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-21 (Êõ¥Êñ∞: 2025-09-08)

**Â§áÊ≥®**: 14 pages, to appear in EMNLP25

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/WNQzhu/CARFT)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CARFT‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜËÉΩÂäõ` `ÂØπÊØîÂ≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `ÊÄùÁª¥Èìæ` `ÂæÆË∞ÉÊñπÊ≥ï` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÊñπÊ≥ïÂøΩËßÜ‰∫ÜÂ∏¶Ê≥®ÈáäÁöÑÊÄùÁª¥ÈìæÔºåÂØºËá¥Ê®°ÂûãËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÂíåÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂØπÊØîÂ≠¶‰π†‰∏éÂ∏¶Ê≥®ÈáäÊÄùÁª¥ÈìæÁªìÂêàÁöÑÂº∫ÂåñÂæÆË∞ÉÊñπÊ≥ïÔºåÊó®Âú®ÂÖÖÂàÜÂà©Áî®Ê≥®Èáä‰ø°ÊÅØÂπ∂Á®≥ÂÆöËÆ≠ÁªÉËøáÁ®ã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCARFTÂú®È≤ÅÊ£íÊÄßÂíåÊÄßËÉΩ‰∏äÊòæËëó‰ºò‰∫é‰∏âÁßçÂü∫Á∫øÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÈ´òËææ10.15%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êé®ÁêÜËÉΩÂäõÂú®Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂπøÊ≥õÂ∫îÁî®‰∏≠ÊâÆÊºîÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑËßíËâ≤„ÄÇ‰∏∫ÊèêÂçáLLMsÁöÑÊé®ÁêÜÊÄßËÉΩÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÊèêÂá∫‰∫ÜÂ§öÁßçÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁöÑÂæÆË∞ÉÊñπÊ≥ïÔºå‰ª•Ëß£ÂÜ≥‰ªÖÈÄöËøáÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâËÆ≠ÁªÉÁöÑLLMsÁöÑÊúâÈôêÊ≥õÂåñËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÂ≠òÂú®‰∏§‰∏™‰∏ªË¶ÅÈôêÂà∂Ôºö‰∏ÄÊòØ‰º†ÁªüÁöÑRLÊñπÊ≥ïÂøΩËßÜ‰∫ÜÂ∏¶Ê≥®ÈáäÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÔºåÂπ∂ÈááÁî®‰∏çÁ®≥ÂÆöÁöÑÊé®ÁêÜË∑ØÂæÑÈááÊ†∑ÔºåÂØºËá¥Ê®°ÂûãÂ¥©Ê∫ÉÂíåËÆ≠ÁªÉËøáÁ®ã‰∏çÁ®≥ÂÆöÔºõ‰∫åÊòØÁé∞ÊúâÁöÑSFTÊñπÊ≥ïËøá‰∫éÂº∫Ë∞ÉÊ≥®ÈáäÁöÑCoTÔºåÂèØËÉΩÂõ†Êú™ÂÖÖÂàÜÂà©Áî®ÊΩúÂú®CoTËÄåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ∏¶Ê≥®ÈáäCoTÁöÑÂØπÊØîÂ≠¶‰π†Âº∫ÂåñÂæÆË∞ÉÊñπÊ≥ïCARFTÔºåÊó®Âú®ÊèêÂçáLLMsÁöÑÊé®ÁêÜÊÄßËÉΩÔºåÂêåÊó∂ÂÖãÊúç‰∏äËø∞ÈôêÂà∂„ÄÇÊàë‰ª¨ÈÄöËøáÂÖ®Èù¢ÁöÑÂÆûÈ™åÂíåÊ∑±ÂÖ•ÂàÜÊûêÔºåÂ±ïÁ§∫‰∫ÜCARFTÂú®È≤ÅÊ£íÊÄß„ÄÅÊÄßËÉΩÔºàÊèêÂçáÈ´òËææ10.15%ÔºâÂíåÊïàÁéáÔºàÊèêÂçáÈ´òËææ30.62%ÔºâÊñπÈù¢ÁöÑÊòæËëó‰ºòÂäø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Âà©Áî®Â∏¶Ê≥®ÈáäÁöÑÊÄùÁª¥ÈìæÊó∂Â≠òÂú®‰∏çÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩ‰∏ãÈôçÁöÑÈ£éÈô©„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªìÂêàÂØπÊØîÂ≠¶‰π†‰∏éÂ∏¶Ê≥®ÈáäÊÄùÁª¥ÈìæÁöÑÂº∫ÂåñÂæÆË∞ÉÊñπÊ≥ïÔºåÈÄöËøáÂ≠¶‰π†ÊØè‰∏™ÊÄùÁª¥ÈìæÁöÑË°®Á§∫ÔºåËÆæËÆ°Êñ∞ÁöÑÂØπÊØî‰ø°Âè∑Êù•ÊåáÂØºÂæÆË∞ÉËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂ∏¶Ê≥®ÈáäÊÄùÁª¥ÈìæÁöÑË°®Á§∫Â≠¶‰π†ÂíåÂØπÊØî‰ø°Âè∑ÁöÑÁîüÊàê‰∏éÂ∫îÁî®„ÄÇÈ¶ñÂÖàÔºåÊ®°ÂûãÂ≠¶‰π†ÊÄùÁª¥ÈìæÁöÑË°®Á§∫ÔºåÁÑ∂ÂêéÂà©Áî®Ëøô‰∫õË°®Á§∫ÁîüÊàêÂØπÊØî‰ø°Âè∑‰ª•Á®≥ÂÆöËÆ≠ÁªÉËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÈÄöËøáÂØπÊØîÂ≠¶‰π†ÂÖÖÂàÜÂà©Áî®Â∏¶Ê≥®ÈáäÁöÑÊÄùÁª¥ÈìæÔºåÂêåÊó∂ÂºïÂÖ•È¢ùÂ§ñÁöÑÊó†ÁõëÁù£Â≠¶‰π†‰ø°Âè∑‰ª•Â¢ûÂº∫ËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÔºåËøô‰∏é‰º†ÁªüÊñπÊ≥ïÁöÑÂçï‰∏Ä‰æùËµñÊ≥®Èáä‰ø°ÊÅØÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÔºåÁªìÂêà‰∫ÜÂØπÊØîÊçüÂ§±ÂíåÂº∫ÂåñÂ≠¶‰π†‰ø°Âè∑ÔºåÁ°Æ‰øùÊ®°ÂûãÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠ËÉΩÂ§üÊúâÊïàÂà©Áî®Ê≥®Èáä‰ø°ÊÅØÂíåÊΩúÂú®ÁöÑÊÄùÁª¥Èìæ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCARFTÂú®È≤ÅÊ£íÊÄßÂíåÊÄßËÉΩÊñπÈù¢ÊòæËëó‰ºò‰∫é‰∏âÁßçÂü∫Á∫øÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÈ´òËææ10.15%ÔºåÊïàÁéáÊèêÂçáÈ´òËææ30.62%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåCARFTÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÖ∑ÊúâËæÉÂº∫ÁöÑÁ´û‰∫âÂäõÂíåÂÆûÁî®‰ª∑ÂÄº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªüÂíåÂØπËØùÁîüÊàêÁ≠â„ÄÇÈÄöËøáÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåCARFTÂèØ‰ª•Âú®Â§öÁßçÂÆûÈôÖÂú∫ÊôØ‰∏≠Êèê‰æõÊõ¥‰∏∫ÂáÜÁ°ÆÂíåÂèØÈù†ÁöÑÁªìÊûúÔºåÊé®Âä®Êô∫ËÉΩÂä©ÊâãÂíåËá™Âä®ÂåñÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \TheName{} in terms of robustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Code is available at https://github.com/WNQzhu/CARFT.

