---
layout: default
title: Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models
---

# Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15202" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15202v1</a>
  <a href="https://arxiv.org/pdf/2508.15202.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15202v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15202v1', 'Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-21

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/aliyun/qwen-dianjin)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFin-PRMä»¥è§£å†³é‡‘èé¢†åŸŸæ¨ç†æ¨¡å‹ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¿‡ç¨‹å¥–åŠ±æ¨¡å‹` `é‡‘èæ¨ç†` `é¢†åŸŸä¸“ç”¨æ¨¡å‹` `è½¨è¿¹æ„ŸçŸ¥` `å¼ºåŒ–å­¦ä¹ ` `ç›‘ç£å­¦ä¹ ` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹åœ¨é‡‘èç­‰é¢†åŸŸçš„æ¨ç†ä¸­è¡¨ç°ä¸è¶³ï¼Œç¼ºä¹é’ˆå¯¹æ€§å’Œç»“æ„åŒ–çš„è¯„ä¼°æœºåˆ¶ã€‚
2. Fin-PRMé€šè¿‡ç»“åˆæ­¥éª¤çº§å’Œè½¨è¿¹çº§å¥–åŠ±ç›‘ç£ï¼Œä¸“é—¨è®¾è®¡ç”¨äºé‡‘èä»»åŠ¡çš„ä¸­é—´æ¨ç†è¯„ä¼°ã€‚
3. åœ¨CFLUEå’ŒFinQAç­‰é‡‘èæ¨ç†åŸºå‡†ä¸Šï¼ŒFin-PRMåœ¨è½¨è¿¹é€‰æ‹©è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºé€šç”¨æ¨¡å‹ï¼Œå¸¦æ¥äº†12.9%çš„ç›‘ç£å­¦ä¹ æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä½œä¸ºç›‘ç£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­é—´æ¨ç†çš„æœ‰å‰æ™¯æ¡†æ¶ï¼Œç°æœ‰æ¨¡å‹ä¸»è¦è®­ç»ƒäºä¸€èˆ¬æˆ–STEMé¢†åŸŸï¼Œéš¾ä»¥æ»¡è¶³é‡‘èç­‰ç‰¹å®šé¢†åŸŸçš„éœ€æ±‚ã€‚æœ¬æ–‡æå‡ºäº†Fin-PRMï¼Œä¸€ç§é’ˆå¯¹é‡‘èä»»åŠ¡çš„é¢†åŸŸä¸“ç”¨ã€è½¨è¿¹æ„ŸçŸ¥çš„PRMï¼Œèƒ½å¤Ÿè¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤ã€‚Fin-PRMç»“åˆäº†æ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„å¥–åŠ±ç›‘ç£ï¼Œæ”¯æŒé«˜è´¨é‡æ¨ç†è½¨è¿¹é€‰æ‹©ã€å¯†é›†è¿‡ç¨‹çº§å¥–åŠ±æä¾›ä»¥åŠæµ‹è¯•æ—¶çš„å¥–åŠ±å¼•å¯¼æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFin-PRMåœ¨é‡‘èæ¨ç†åŸºå‡†ä¸Šä¼˜äºé€šç”¨PRMså’Œå¼ºåŸºçº¿ï¼Œæ˜¾è‘—æå‡äº†ä¸‹æ¸¸æ¨¡å‹çš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä¸»è¦è®­ç»ƒäºä¸€èˆ¬é¢†åŸŸï¼Œéš¾ä»¥æ»¡è¶³é‡‘èé¢†åŸŸæ¨ç†çš„ç»“æ„åŒ–å’Œç¬¦å·åŒ–éœ€æ±‚ï¼Œå¯¼è‡´åœ¨é‡‘èä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFin-PRMé€šè¿‡å¼•å…¥è½¨è¿¹æ„ŸçŸ¥çš„å¥–åŠ±æœºåˆ¶ï¼Œç»“åˆæ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„å¥–åŠ±ç›‘ç£ï¼Œæ—¨åœ¨æä¾›æ›´ç²¾ç»†çš„æ¨ç†è¯„ä¼°ï¼Œé€‚åº”é‡‘èé¢†åŸŸçš„ç‰¹æ®Šéœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFin-PRMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€è½¨è¿¹è¯„ä¼°æ¨¡å—å’Œå¥–åŠ±è®¡ç®—æ¨¡å—ã€‚æ•°æ®è¾“å…¥æ¨¡å—è´Ÿè´£æ¥æ”¶é‡‘èä»»åŠ¡æ•°æ®ï¼Œè½¨è¿¹è¯„ä¼°æ¨¡å—åˆ†ææ¨ç†æ­¥éª¤ï¼Œå¥–åŠ±è®¡ç®—æ¨¡å—åˆ™æ ¹æ®è¯„ä¼°ç»“æœç”Ÿæˆç›¸åº”çš„å¥–åŠ±ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šFin-PRMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è½¨è¿¹æ„ŸçŸ¥çš„å¥–åŠ±è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨æ­¥éª¤çº§å’Œè½¨è¿¹çº§ä¸Šè¿›è¡Œç»†è‡´çš„è¯„ä¼°ï¼Œä¸ä¼ ç»Ÿçš„é€šç”¨PRMsç›¸æ¯”ï¼Œæä¾›äº†æ›´ç¬¦åˆé‡‘èé€»è¾‘çš„å¥–åŠ±æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒFin-PRMé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¥–åŠ±ä¿¡å·çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥æ›´å¥½åœ°æ•æ‰é‡‘èæ¨ç†çš„å¤æ‚æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFin-PRMåœ¨CFLUEå’ŒFinQAåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè½¨è¿¹é€‰æ‹©è´¨é‡æ˜¾è‘—é«˜äºé€šç”¨PRMsï¼Œç›‘ç£å­¦ä¹ æå‡è¾¾12.9%ï¼Œå¼ºåŒ–å­¦ä¹ æå‡5.2%ï¼Œæµ‹è¯•æ—¶æ€§èƒ½æå‡5.1%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé¢†åŸŸä¸“ç”¨çš„å¥–åŠ±å»ºæ¨¡å¯¹é‡‘èæ¨ç†çš„æœ‰æ•ˆæ€§è‡³å…³é‡è¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Fin-PRMåœ¨é‡‘èé¢†åŸŸçš„æ½œåœ¨åº”ç”¨å¹¿æ³›ï¼ŒåŒ…æ‹¬é‡‘èå†³ç­–æ”¯æŒã€é£é™©è¯„ä¼°å’Œåˆè§„æ€§æ£€æŸ¥ç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„æ¨ç†è½¨è¿¹å’Œå¥–åŠ±ä¿¡å·ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡é‡‘èæ¨¡å‹çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œå¸®åŠ©é‡‘èæœºæ„æ›´å¥½åœ°åº”å¯¹å¤æ‚çš„å¸‚åœºç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM}, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\% in supervised learning, 5.2\% in reinforcement learning, and 5.1\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin.

