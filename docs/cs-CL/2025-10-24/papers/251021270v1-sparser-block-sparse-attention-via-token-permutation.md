---
layout: default
title: Sparser Block-Sparse Attention via Token Permutation
---

# Sparser Block-Sparse Attention via Token Permutation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21270" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21270v1</a>
  <a href="https://arxiv.org/pdf/2510.21270.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21270v1" onclick="toggleFavorite(this, '2510.21270v1', 'Sparser Block-Sparse Attention via Token Permutation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinghao Wang, Pengyu Wang, Dong Zhang, Chenkun Tan, Shaojun Zhou, Zhaoxiang Liu, Shiguo Lian, Fangxu Liu, Kai Song, Xipeng Qiu

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-24

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/xinghaow99/pbs-attn)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºTokenç½®æ¢çš„ç¨€ç–å—æ³¨æ„åŠ›æœºåˆ¶PBS-Attnï¼ŒåŠ é€Ÿé•¿æ–‡æœ¬LLMé¢„å¡«å……ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æœ¬å¤„ç†` `ç¨€ç–æ³¨æ„åŠ›` `å—ç¨€ç–æ€§` `Tokenç½®æ¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `è®¡ç®—æ•ˆç‡` `FlashAttention`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿æ–‡æœ¬å¤„ç†ä¸­ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¹³æ–¹å¤æ‚åº¦æ˜¯LLMæ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦çš„ä¸»è¦ç“¶é¢ˆã€‚
2. è®ºæ–‡æå‡ºPBS-Attnï¼Œé€šè¿‡tokenç½®æ¢å¢åŠ å—ç¨€ç–æ€§ï¼Œæå‡è®¡ç®—æ•ˆç‡ï¼Œä¸”æ˜“äºé›†æˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPBS-Attnåœ¨ç²¾åº¦ä¸Šä¼˜äºç°æœ‰å—ç¨€ç–æ–¹æ³•ï¼Œå¹¶æ¥è¿‘å®Œæ•´æ³¨æ„åŠ›ï¼ŒåŠ é€Ÿé«˜è¾¾2.75å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡é•¿åº¦å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚è¿™ç§æˆæœ¬ä¸»è¦æºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ç›¸å¯¹äºåºåˆ—é•¿åº¦çš„$O(N^2)$å¤æ‚åº¦å¯¹å†…å­˜å’Œå»¶è¿Ÿæ„æˆäº†ä¸»è¦ç“¶é¢ˆã€‚å¹¸è¿çš„æ˜¯ï¼Œæ³¨æ„åŠ›çŸ©é˜µé€šå¸¸æ˜¯ç¨€ç–çš„ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿åºåˆ—ï¼Œè¿™è¡¨æ˜å­˜åœ¨ä¼˜åŒ–çš„æœºä¼šã€‚å—ç¨€ç–æ³¨æ„åŠ›å·²ç»æˆä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒå°†åºåˆ—åˆ’åˆ†ä¸ºå—ï¼Œå¹¶è·³è¿‡éƒ¨åˆ†å—çš„è®¡ç®—ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºåº•å±‚çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¬¡ä¼˜çš„å—çº§ç¨€ç–æ€§ã€‚ä¾‹å¦‚ï¼Œå•ä¸ªå—å†…æŸ¥è¯¢çš„é‡è¦é”®tokenå¯èƒ½åˆ†æ•£åœ¨è®¸å¤šå…¶ä»–å—ä¸­ï¼Œå¯¼è‡´è®¡ç®—å†—ä½™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç½®æ¢å—ç¨€ç–æ³¨æ„åŠ›ï¼ˆPBS-Attnï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ³¨æ„åŠ›çš„ç½®æ¢å±æ€§æ¥å¢åŠ å—çº§ç¨€ç–æ€§å¹¶æé«˜LLMé¢„å¡«å……çš„è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œè¡¨æ˜PBS-Attnåœ¨æ¨¡å‹ç²¾åº¦æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰çš„å—ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œå¹¶ä¸”ä¸å®Œæ•´æ³¨æ„åŠ›åŸºçº¿éå¸¸æ¥è¿‘ã€‚åœ¨æˆ‘ä»¬çš„å®šåˆ¶ç½®æ¢FlashAttentionå†…æ ¸çš„æ”¯æŒä¸‹ï¼ŒPBS-Attnåœ¨é•¿ä¸Šä¸‹æ–‡é¢„å¡«å……ä¸­å®ç°äº†é«˜è¾¾2.75å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œè¯å®äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é•¿æ–‡æœ¬å¤„ç†ä¸­ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶è®¡ç®—å¤æ‚åº¦è¿‡é«˜çš„é—®é¢˜ã€‚ç°æœ‰å—ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•è™½ç„¶èƒ½é™ä½è®¡ç®—é‡ï¼Œä½†å…¶æ€§èƒ½å—é™äºåº•å±‚æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¯èƒ½å¯¼è‡´æ¬¡ä¼˜çš„å—çº§ç¨€ç–æ€§å’Œè®¡ç®—å†—ä½™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨tokenç½®æ¢çš„ç‰¹æ€§ï¼Œé‡æ–°æ’åˆ—tokençš„é¡ºåºï¼Œä½¿å¾—åŸæœ¬åˆ†æ•£åœ¨ä¸åŒå—ä¸­çš„é‡è¦tokenèƒ½å¤Ÿé›†ä¸­åˆ°å°‘æ•°å—ä¸­ï¼Œä»è€Œæé«˜å—ç¨€ç–æ€§ï¼Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ã€‚é€šè¿‡ç½®æ¢æ“ä½œï¼Œä½¿å¾—æ³¨æ„åŠ›æ›´åŠ é›†ä¸­ï¼Œä»è€Œæå‡æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPBS-Attnæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯ä»¥åµŒå…¥åˆ°ç°æœ‰çš„Transformeræ¶æ„ä¸­ã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) è¾“å…¥åºåˆ—åˆ†å—ï¼›2) å¯¹æ¯ä¸ªå—å†…çš„tokenè¿›è¡Œç½®æ¢ï¼›3) è¿›è¡Œå—ç¨€ç–æ³¨æ„åŠ›è®¡ç®—ï¼›4) å¯¹è¾“å‡ºè¿›è¡Œé€†ç½®æ¢ï¼Œæ¢å¤åŸå§‹é¡ºåºã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€ä¿®æ”¹åŸæœ‰çš„æ¨¡å‹ç»“æ„ï¼Œæ˜“äºé›†æˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŸºäºtokenç½®æ¢çš„å—ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„å—ç¨€ç–æ–¹æ³•ä¸åŒï¼ŒPBS-Attnä¸æ˜¯ç®€å•åœ°è·³è¿‡æŸäº›å—çš„è®¡ç®—ï¼Œè€Œæ˜¯é€šè¿‡ç½®æ¢æ“ä½œæ”¹å˜äº†æ³¨æ„åŠ›æ¨¡å¼ï¼Œä½¿å¾—æ³¨æ„åŠ›æ›´åŠ é›†ä¸­ï¼Œä»è€Œæé«˜äº†å—ç¨€ç–æ€§çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å®šåˆ¶äº†permuted-FlashAttentionå†…æ ¸ï¼Œè¿›ä¸€æ­¥æå‡äº†è®¡ç®—æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç½®æ¢ç­–ç•¥çš„é€‰æ‹©ï¼šè®ºæ–‡å¯èƒ½æ¢ç´¢äº†ä¸åŒçš„ç½®æ¢ç­–ç•¥ï¼Œä¾‹å¦‚éšæœºç½®æ¢ã€åŸºäºæ³¨æ„åŠ›æƒé‡çš„ç½®æ¢ç­‰ï¼Œä»¥æ‰¾åˆ°æœ€ä¼˜çš„ç½®æ¢æ–¹å¼ã€‚2) å—å¤§å°çš„è®¾ç½®ï¼šå—å¤§å°çš„é€‰æ‹©ä¼šå½±å“å—ç¨€ç–æ€§çš„æ•ˆæœï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚3) å®šåˆ¶çš„permuted-FlashAttentionå†…æ ¸ï¼šè¯¥å†…æ ¸é’ˆå¯¹ç½®æ¢åçš„æ•°æ®è¿›è¡Œäº†ä¼˜åŒ–ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æºï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPBS-Attnåœ¨é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰çš„å—ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹ç²¾åº¦ä¸Šä¸å®Œæ•´æ³¨æ„åŠ›åŸºçº¿éå¸¸æ¥è¿‘ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œé€šè¿‡å®šåˆ¶çš„permuted-FlashAttentionå†…æ ¸ï¼ŒPBS-Attnåœ¨é•¿ä¸Šä¸‹æ–‡é¢„å¡«å……ä¸­å®ç°äº†é«˜è¾¾2.75å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PBS-Attnå¯åº”ç”¨äºéœ€è¦å¤„ç†é•¿æ–‡æœ¬çš„å„ç§åœºæ™¯ï¼Œå¦‚æ–‡æ¡£æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ã€ä»£ç ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—æˆæœ¬ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒPBS-Attnçš„å³æ’å³ç”¨ç‰¹æ€§ä½¿å…¶æ˜“äºé›†æˆåˆ°ç°æœ‰çš„LLMä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose $O(N^2)$ complexity with respect to sequence length presents a major bottleneck for both memory and latency. Fortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization. Block-sparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks. However, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity. For instance, important key tokens for queries within a single block may be scattered across numerous other blocks, leading to computational redundancy. In this work, we propose Permuted Block-Sparse Attention (\textbf{PBS-Attn}), a plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling. We conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to $2.75\times$ in long-context prefilling, confirming its practical viability. Code available at https://github.com/xinghaow99/pbs-attn

