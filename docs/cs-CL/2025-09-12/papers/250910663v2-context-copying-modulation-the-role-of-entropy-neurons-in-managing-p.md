---
layout: default
title: Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts
---

# Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10663" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10663v2</a>
  <a href="https://arxiv.org/pdf/2509.10663.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10663v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10663v2', 'Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zineddine Tighidet, Andrea Mogini, Hedi Ben-younes, Jiali Mei, Patrick Gallinari, Benjamin Piwowarski

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-09-17)

**å¤‡æ³¨**: Accepted at EMNLP 2025

**æœŸåˆŠ**: EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨ç†µç¥ç»å…ƒæŠ‘åˆ¶ä¸Šä¸‹æ–‡å¤åˆ¶ï¼Œè§£å†³LLMå‚æ•°çŸ¥è¯†ä¸ä¸Šä¸‹æ–‡å†²çªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸Šä¸‹æ–‡å¤åˆ¶` `ç†µç¥ç»å…ƒ` `çŸ¥è¯†å†²çª` `Transformeræ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸è‡ªèº«çŸ¥è¯†å†²çªçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ—¶ï¼Œè¡Œä¸ºä¸ä¸€è‡´ï¼Œç¼ºä¹åˆç†çš„è§£é‡Šã€‚
2. è¯¥ç ”ç©¶æå‡ºç†µç¥ç»å…ƒåœ¨æŠ‘åˆ¶ä¸Šä¸‹æ–‡å¤åˆ¶ä¸­èµ·ä½œç”¨ï¼Œé€šè¿‡è°ƒèŠ‚å®ƒä»¬æ¥è§£å†³å‚æ•°çŸ¥è¯†ä¸ä¸Šä¸‹æ–‡çš„å†²çªã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç†µç¥ç»å…ƒç¡®å®è´Ÿè´£æŠ‘åˆ¶ä¸Šä¸‹æ–‡å¤åˆ¶ï¼Œæ¶ˆèå®ƒä»¬ä¼šæ˜¾è‘—æ”¹å˜LLMçš„ç”Ÿæˆè¡Œä¸ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨é¢å¯¹ä¸å…¶å†…éƒ¨å‚æ•°çŸ¥è¯†ç›¸å†²çªçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ—¶ï¼Œå…¶è¡Œä¸ºè¡¨ç°ä¸ä¸€è‡´ï¼Œå¹¶ä¸”å¯¹äºé¢„æœŸè¾“å‡ºåˆ†å¸ƒæ²¡æœ‰æ™®éæ¥å—çš„è§£é‡Šã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è‡ªå›å½’Transformeræ¨¡å‹ä¸­å­˜åœ¨ä¸€ç±»ç¥ç»å…ƒâ€”â€”ç§°ä¸ºç†µç¥ç»å…ƒâ€”â€”å®ƒä»¬å¯¹æ¨¡å‹è¾“å‡ºç†µäº§ç”Ÿæ˜¾è‘—å½±å“ï¼ŒåŒæ—¶å¯¹é¢„æµ‹tokençš„æ’åºå½±å“é€‚ä¸­ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ä¸ªåˆæ­¥çš„è§‚ç‚¹ï¼Œå³è¿™äº›ç¥ç»å…ƒå‚ä¸æŠ‘åˆ¶Transformerä¸­çš„ä¸Šä¸‹æ–‡å¤åˆ¶è¡Œä¸ºï¼Œé€šè¿‡è§‚å¯Ÿå®ƒä»¬åœ¨è§£å†³ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå‚æ•°ä¿¡æ¯ä¹‹é—´çš„å†²çªä¸­çš„ä½œç”¨ã€‚æˆ‘ä»¬è¯æ˜äº†ç†µç¥ç»å…ƒè´Ÿè´£æŠ‘åˆ¶ä¸€ç³»åˆ—LLMä¸­çš„ä¸Šä¸‹æ–‡å¤åˆ¶ï¼Œå¹¶ä¸”æ¶ˆèå®ƒä»¬ä¼šå¯¼è‡´ç”Ÿæˆè¿‡ç¨‹çš„æ˜¾è‘—å˜åŒ–ã€‚è¿™äº›ç»“æœå¢å¼ºäº†æˆ‘ä»¬å¯¹LLMåœ¨å¤„ç†å†²çªä¿¡æ¯æ—¶çš„å†…éƒ¨åŠ¨æ€çš„ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸è‡ªèº«å‚æ•°çŸ¥è¯†ç›¸å†²çªçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ—¶ï¼Œå…¶è¡Œä¸ºè¡¨ç°ä¸ç¨³å®šï¼Œéš¾ä»¥é¢„æµ‹ã€‚ç°æœ‰çš„æ–¹æ³•ç¼ºä¹å¯¹è¿™ç§å†²çªè§£å†³æœºåˆ¶çš„æ·±å…¥ç†è§£ï¼Œæ— æ³•æœ‰æ•ˆæ§åˆ¶æ¨¡å‹çš„è¾“å‡ºè¡Œä¸ºã€‚æ¨¡å‹æœ‰æ—¶ä¼šå¿½ç•¥ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåšæŒè‡ªèº«çš„å‚æ•°çŸ¥è¯†ï¼Œæœ‰æ—¶åˆä¼šç›²ç›®å¤åˆ¶ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœä¸ç¬¦åˆé¢„æœŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç ”ç©¶Transformeræ¨¡å‹ä¸­çš„â€œç†µç¥ç»å…ƒâ€åœ¨è§£å†³å‚æ•°çŸ¥è¯†ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯å†²çªä¸­çš„ä½œç”¨ã€‚ä½œè€…å‡è®¾è¿™äº›ç¥ç»å…ƒå‚ä¸äº†æŠ‘åˆ¶ä¸Šä¸‹æ–‡å¤åˆ¶çš„è¡Œä¸ºï¼Œé€šè¿‡è°ƒèŠ‚è¿™äº›ç¥ç»å…ƒï¼Œå¯ä»¥æ§åˆ¶æ¨¡å‹åœ¨å‚æ•°çŸ¥è¯†å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚è¿™ç§æ€è·¯æ—¨åœ¨æ­ç¤ºLLMå†…éƒ¨çš„å†²çªè§£å†³æœºåˆ¶ï¼Œå¹¶ä¸ºæ§åˆ¶æ¨¡å‹çš„ç”Ÿæˆè¡Œä¸ºæä¾›ä¸€ç§æ–°çš„æ–¹æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡çš„ç ”ç©¶æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) è¯†åˆ«LLMä¸­çš„ç†µç¥ç»å…ƒï¼›2) è®¾è®¡å®éªŒï¼Œä½¿æ¨¡å‹é¢ä¸´å‚æ•°çŸ¥è¯†ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯å†²çªçš„æƒ…å†µï¼›3) é€šè¿‡æ¶ˆèï¼ˆablatingï¼‰ç†µç¥ç»å…ƒï¼Œè§‚å¯Ÿæ¨¡å‹ç”Ÿæˆè¡Œä¸ºçš„å˜åŒ–ï¼›4) åˆ†æå®éªŒç»“æœï¼Œè¯„ä¼°ç†µç¥ç»å…ƒåœ¨æŠ‘åˆ¶ä¸Šä¸‹æ–‡å¤åˆ¶ä¸­çš„ä½œç”¨ã€‚æ•´ä½“æµç¨‹å›´ç»•ç†µç¥ç»å…ƒå±•å¼€ï¼Œé€šè¿‡å¹²é¢„å’Œè§‚å¯Ÿå…¶å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ï¼Œæ¥éªŒè¯å…¶åœ¨å†²çªè§£å†³ä¸­çš„ä½œç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå‘ç°äº†ç†µç¥ç»å…ƒåœ¨æŠ‘åˆ¶ä¸Šä¸‹æ–‡å¤åˆ¶ä¸­çš„ä½œç”¨ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨æ¨¡å‹æ•´ä½“çš„æ€§èƒ½å’Œè¡Œä¸ºï¼Œè€Œæœ¬æ–‡æ·±å…¥åˆ°ç¥ç»å…ƒå±‚é¢ï¼Œæ­ç¤ºäº†ç‰¹å®šç±»å‹çš„ç¥ç»å…ƒåœ¨è§£å†³å†²çªä¿¡æ¯ä¸­çš„ç‰¹æ®ŠåŠŸèƒ½ã€‚è¿™ç§å¾®è§‚å±‚é¢çš„åˆ†æä¸ºç†è§£LLMçš„å†…éƒ¨æœºåˆ¶æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•å‡†ç¡®è¯†åˆ«ç†µç¥ç»å…ƒï¼›2) å¦‚ä½•æ„å»ºå‚æ•°çŸ¥è¯†ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯å†²çªçš„å®éªŒåœºæ™¯ï¼›3) å¦‚ä½•æœ‰æ•ˆåœ°æ¶ˆèç†µç¥ç»å…ƒï¼Œå¹¶è§‚å¯Ÿå…¶å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚å¯èƒ½æ ¹æ®ä¸åŒçš„LLMè€Œæœ‰æ‰€å·®å¼‚ï¼Œä½†æ•´ä½“æ€è·¯æ˜¯ä¿æŒä¸€è‡´çš„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ¶ˆèç†µç¥ç»å…ƒä¼šå¯¼è‡´LLMç”Ÿæˆè¡Œä¸ºçš„æ˜¾è‘—å˜åŒ–ï¼Œå…·ä½“è¡¨ç°ä¸ºä¸Šä¸‹æ–‡å¤åˆ¶çš„å€¾å‘å¢åŠ ã€‚è¿™è¯å®äº†ç†µç¥ç»å…ƒåœ¨æŠ‘åˆ¶ä¸Šä¸‹æ–‡å¤åˆ¶ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶åœ¨ä¸€ç³»åˆ—LLMä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œè¡¨æ˜è¿™ä¸€å‘ç°å…·æœ‰ä¸€å®šçš„æ™®é€‚æ€§ã€‚è™½ç„¶è®ºæ–‡ä¸­æ²¡æœ‰ç»™å‡ºå…·ä½“çš„æ€§èƒ½æ•°æ®æå‡ï¼Œä½†å…¶æ­ç¤ºçš„å†…éƒ¨æœºåˆ¶ä¸ºæœªæ¥ä¼˜åŒ–LLMæä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ¨ç†ç­‰ã€‚é€šè¿‡æ§åˆ¶ç†µç¥ç»å…ƒï¼Œå¯ä»¥ä½¿æ¨¡å‹æ›´å¥½åœ°æƒè¡¡å‚æ•°çŸ¥è¯†å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®ã€æ›´ç¬¦åˆç”¨æˆ·æ„å›¾çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿæœ‰åŠ©äºå¼€å‘æ›´å¯æ§ã€æ›´å¯é çš„LLMï¼Œå‡å°‘æ¨¡å‹äº§ç”Ÿå¹»è§‰å’Œé”™è¯¯ä¿¡æ¯çš„é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The behavior of Large Language Models (LLMs) when facing contextual information that conflicts with their internal parametric knowledge is inconsistent, with no generally accepted explanation for the expected outcome distribution. Recent work has identified in autoregressive transformer models a class of neurons -- called entropy neurons -- that produce a significant effect on the model output entropy while having an overall moderate impact on the ranking of the predicted tokens. In this paper, we investigate the preliminary claim that these neurons are involved in inhibiting context copying behavior in transformers by looking at their role in resolving conflicts between contextual and parametric information. We show that entropy neurons are responsible for suppressing context copying across a range of LLMs, and that ablating them leads to a significant change in the generation process. These results enhance our understanding of the internal dynamics of LLMs when handling conflicting information.

