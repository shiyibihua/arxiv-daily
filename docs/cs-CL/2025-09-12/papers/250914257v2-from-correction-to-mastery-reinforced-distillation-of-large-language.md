---
layout: default
title: From Correction to Mastery: Reinforced Distillation of Large Language Model Agents
---

# From Correction to Mastery: Reinforced Distillation of Large Language Model Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14257" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14257v2</a>
  <a href="https://arxiv.org/pdf/2509.14257.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14257v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14257v2', 'From Correction to Mastery: Reinforced Distillation of Large Language Model Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuanjie Lyu, Chengyu Wang, Jun Huang, Tong Xu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-10-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSCoReæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–è’¸é¦æå‡å°æ¨¡å‹Agentåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œåª²ç¾å¤§æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹Agent` `è’¸é¦å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `çŸ¥è¯†è¿ç§»` `æ¨¡å‹å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Agentè’¸é¦æ–¹æ³•æ˜“å—æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹èƒ½åŠ›å·®è·å½±å“ï¼Œå¯¼è‡´è¯¯å·®ç´¯ç§¯ï¼Œå½±å“å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ã€‚
2. SCoReæ¡†æ¶ä»¥å­¦ç”Ÿä¸ºä¸­å¿ƒï¼Œé€šè¿‡æ•™å¸ˆçº æ­£å­¦ç”Ÿè½¨è¿¹ä¸­çš„æ—©æœŸé”™è¯¯ï¼Œç”Ÿæˆæ›´é€‚åˆå­¦ç”Ÿèƒ½åŠ›çš„è®­ç»ƒæ•°æ®ã€‚
3. SCoReç»“åˆå¾®è°ƒå’ŒçŸ­ç¨‹å¼ºåŒ–å­¦ä¹ ï¼Œé¼“åŠ±å­¦ç”Ÿè‡ªä¸»è§£å†³é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å°æ¨¡å‹Agentçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹Agentæ“…é•¿é€šè¿‡è¿­ä»£æ¨ç†å’Œå·¥å…·ä½¿ç”¨è§£å†³å¤æ‚ä»»åŠ¡ï¼Œä½†é€šå¸¸ä¾èµ–äºè¶…å¤§å‹ã€é«˜æˆæœ¬çš„éª¨å¹²æ¨¡å‹ã€‚ç°æœ‰çš„è’¸é¦æ–¹æ³•è®­ç»ƒè¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹æ¥æ¨¡ä»¿å®Œæ•´çš„æ•™å¸ˆæ¨¡å‹è½¨è¿¹ï¼Œä½†æ•™å¸ˆå’Œå­¦ç”Ÿä¹‹é—´çš„æ¨ç†å’ŒçŸ¥è¯†å·®è·å¯èƒ½å¯¼è‡´è¯¯å·®ç´¯ç§¯ã€‚æˆ‘ä»¬æå‡ºäº†SCoReï¼Œä¸€ä¸ªä»¥å­¦ç”Ÿä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œå…¶ä¸­å­¦ç”Ÿç”Ÿæˆè®­ç»ƒè½¨è¿¹ï¼Œæ•™å¸ˆä»…çº æ­£æœ€æ—©çš„é”™è¯¯ï¼Œä»è€Œäº§ç”Ÿä¸å­¦ç”Ÿèƒ½åŠ›ç›¸åŒ¹é…çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶æš´éœ²ç‰¹å®šçš„å¼±ç‚¹ã€‚å­¦ç”Ÿé¦–å…ˆåœ¨çº æ­£åçš„è½¨è¿¹ä¸Šè¿›è¡Œå¾®è°ƒã€‚éšåï¼Œä»éªŒè¯è¿‡çš„ã€æœ€æ—©é”™è¯¯ä¹‹å‰çš„åºåˆ—å¼€å§‹è¿›è¡ŒçŸ­ç¨‹å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶åœ¨è¯¥æ­¥éª¤åˆ†é…ç›®æ ‡å¥–åŠ±ã€‚è¿™ç§è®¾è®¡é¼“åŠ±è¶…è¶Šæ¨¡ä»¿çš„è‡ªä¸»é—®é¢˜è§£å†³ï¼Œå¹¶å¢å¼ºè®­ç»ƒç¨³å®šæ€§ã€‚åœ¨12ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨SCoReè’¸é¦çš„70äº¿å‚æ•°å­¦ç”Ÿæ¨¡å‹ä¸720äº¿å‚æ•°æ•™å¸ˆæ¨¡å‹çš„Agentæ€§èƒ½ç›¸åŒ¹é…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹Agentè™½ç„¶åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¾èµ–äºå‚æ•°é‡å·¨å¤§çš„æ¨¡å‹ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¼ ç»Ÿçš„è’¸é¦æ–¹æ³•ç›´æ¥è®©å°æ¨¡å‹æ¨¡ä»¿å¤§æ¨¡å‹çš„è¡Œä¸ºï¼Œç„¶è€Œå¤§æ¨¡å‹å’Œå°æ¨¡å‹ä¹‹é—´å­˜åœ¨æ¨ç†èƒ½åŠ›å’ŒçŸ¥è¯†å‚¨å¤‡çš„å·®è·ï¼Œå¯¼è‡´åœ¨æ¨¡ä»¿è¿‡ç¨‹ä¸­è¯¯å·®ä¸æ–­ç´¯ç§¯ï¼Œæœ€ç»ˆå½±å“å°æ¨¡å‹çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†å¤§æ¨¡å‹çš„Agentèƒ½åŠ›è¿ç§»åˆ°å°æ¨¡å‹ï¼ŒåŒæ—¶é¿å…è¯¯å·®ç´¯ç§¯ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSCoReçš„æ ¸å¿ƒæ€è·¯æ˜¯ä»¥å­¦ç”Ÿæ¨¡å‹ä¸ºä¸­å¿ƒï¼Œè®©å­¦ç”Ÿæ¨¡å‹ä¸»åŠ¨æ¢ç´¢ï¼Œå¹¶ç”±æ•™å¸ˆæ¨¡å‹è¿›è¡Œé’ˆå¯¹æ€§çš„æŒ‡å¯¼ã€‚å…·ä½“æ¥è¯´ï¼Œå­¦ç”Ÿæ¨¡å‹é¦–å…ˆç”Ÿæˆè‡ªå·±çš„è½¨è¿¹ï¼Œç„¶åæ•™å¸ˆæ¨¡å‹åªçº æ­£å­¦ç”Ÿè½¨è¿¹ä¸­æœ€æ—©å‡ºç°çš„é”™è¯¯ã€‚è¿™æ ·ç”Ÿæˆçš„è®­ç»ƒæ•°æ®æ›´ç¬¦åˆå­¦ç”Ÿæ¨¡å‹çš„èƒ½åŠ›ï¼Œé¿å…äº†è®©å­¦ç”Ÿæ¨¡å‹ç›´æ¥æ¨¡ä»¿è¶…å‡ºå…¶èƒ½åŠ›èŒƒå›´çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼ŒSCoReè¿˜åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œé¼“åŠ±å­¦ç”Ÿæ¨¡å‹åœ¨æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼çš„åŸºç¡€ä¸Šè¿›è¡Œè‡ªä¸»æ¢ç´¢ï¼Œè¿›ä¸€æ­¥æå‡å…¶è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSCoReæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šçº æ­£è½¨è¿¹å¾®è°ƒå’ŒçŸ­ç¨‹å¼ºåŒ–å­¦ä¹ ã€‚åœ¨çº æ­£è½¨è¿¹å¾®è°ƒé˜¶æ®µï¼Œå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆè½¨è¿¹ï¼Œæ•™å¸ˆæ¨¡å‹çº æ­£æœ€æ—©çš„é”™è¯¯ï¼Œç„¶åä½¿ç”¨çº æ­£åçš„è½¨è¿¹å¯¹å­¦ç”Ÿæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚åœ¨çŸ­ç¨‹å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä»éªŒè¯è¿‡çš„ã€æœ€æ—©é”™è¯¯ä¹‹å‰çš„åºåˆ—å¼€å§‹ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼Œå¹¶åœ¨è¯¥æ­¥éª¤åˆ†é…ç›®æ ‡å¥–åŠ±ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œé¼“åŠ±å­¦ç”Ÿæ¨¡å‹åœ¨æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼çš„åŸºç¡€ä¸Šè¿›è¡Œè‡ªä¸»æ¢ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šSCoReçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä»¥å­¦ç”Ÿä¸ºä¸­å¿ƒçš„è®­ç»ƒæ–¹å¼ã€‚ä¼ ç»Ÿçš„è’¸é¦æ–¹æ³•å¾€å¾€å¿½ç•¥äº†å­¦ç”Ÿæ¨¡å‹çš„èƒ½åŠ›ï¼Œç›´æ¥è®©å­¦ç”Ÿæ¨¡å‹æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºã€‚è€ŒSCoReåˆ™å……åˆ†è€ƒè™‘äº†å­¦ç”Ÿæ¨¡å‹çš„èƒ½åŠ›ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹çº æ­£å­¦ç”Ÿè½¨è¿¹ä¸­çš„æ—©æœŸé”™è¯¯ï¼Œç”Ÿæˆæ›´é€‚åˆå­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒæ•°æ®ã€‚æ­¤å¤–ï¼ŒSCoReè¿˜åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œé¼“åŠ±å­¦ç”Ÿæ¨¡å‹åœ¨æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼çš„åŸºç¡€ä¸Šè¿›è¡Œè‡ªä¸»æ¢ç´¢ï¼Œè¿›ä¸€æ­¥æå‡å…¶è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šSCoReçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•™å¸ˆæ¨¡å‹åªçº æ­£å­¦ç”Ÿè½¨è¿¹ä¸­æœ€æ—©çš„é”™è¯¯ï¼Œé¿å…äº†è¯¯å·®ç´¯ç§¯ï¼›2) ä½¿ç”¨çŸ­ç¨‹å¼ºåŒ–å­¦ä¹ ï¼Œé¼“åŠ±å­¦ç”Ÿæ¨¡å‹åœ¨æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼çš„åŸºç¡€ä¸Šè¿›è¡Œè‡ªä¸»æ¢ç´¢ï¼›3) åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œåœ¨éªŒè¯è¿‡çš„ã€æœ€æ—©é”™è¯¯ä¹‹å‰çš„åºåˆ—å¼€å§‹ï¼Œå¹¶åœ¨è¯¥æ­¥éª¤åˆ†é…ç›®æ ‡å¥–åŠ±ï¼Œå¼•å¯¼å­¦ç”Ÿæ¨¡å‹æœç€æ­£ç¡®çš„æ–¹å‘å‰è¿›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°é€‰æ‹©å–å†³äºå…·ä½“çš„ä»»åŠ¡å’Œæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SCoReæ¡†æ¶è’¸é¦çš„70äº¿å‚æ•°å­¦ç”Ÿæ¨¡å‹ï¼Œåœ¨12ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgentæ€§èƒ½ä¸720äº¿å‚æ•°çš„æ•™å¸ˆæ¨¡å‹ç›¸åŒ¹é…ã€‚è¿™è¡¨æ˜SCoReæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹Agentçš„èƒ½åŠ›è¿ç§»åˆ°å°å‹æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SCoReæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦Agentè¿›è¡Œå¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨åŒ–ç¼–ç¨‹ã€æ¸¸æˆAIç­‰ã€‚é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹Agentçš„èƒ½åŠ›è¿ç§»åˆ°å°å‹æ¨¡å‹ï¼Œå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜éƒ¨ç½²æ•ˆç‡ï¼Œä½¿å¾—AgentæŠ€æœ¯èƒ½å¤Ÿæ›´å¹¿æ³›åœ°åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student can cause compounding errors. We propose SCoRe, a student-centered framework in which the student generates training trajectories and the teacher corrects only the earliest error, producing training data matched to the student's ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix preceding the earliest error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and enhances training stability. On 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.

