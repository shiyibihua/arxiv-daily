---
layout: default
title: DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL
---

# DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10446" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10446v2</a>
  <a href="https://arxiv.org/pdf/2509.10446.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10446v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10446v2', 'DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rui Lu, Zhenyu Hou, Zihan Wang, Hanchen Zhang, Xiao Liu, Yujiang Li, Shi Feng, Jie Tang, Yuxiao Dong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-10-14)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/THUDM/DeepDive)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DeepDiveï¼šåˆ©ç”¨çŸ¥è¯†å›¾è°±å’Œå¤šè½®å¼ºåŒ–å­¦ä¹ æå‡æ·±åº¦æœç´¢Agentèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ·±åº¦æœç´¢` `çŸ¥è¯†å›¾è°±` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æ”¾LLMåœ¨åˆ©ç”¨æµè§ˆå·¥å…·è¿›è¡Œæ·±åº¦æœç´¢æ—¶ï¼Œé¢ä¸´é•¿ç¨‹æ¨ç†èƒ½åŠ›ä¸è¶³å’Œç¼ºä¹é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ã€‚
2. DeepDiveé€šè¿‡ä»çŸ¥è¯†å›¾è°±è‡ªåŠ¨ç”Ÿæˆå¤æ‚é—®é¢˜ï¼Œå¹¶é‡‡ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ æ¥æå‡LLMçš„æ·±åº¦æœç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepDiveåœ¨BrowseCompç­‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å¤šè½®å¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†DeepDiveï¼Œæ—¨åœ¨æå‡æ·±åº¦æœç´¢Agentçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³å¼€æ”¾LLMåœ¨å¤æ‚ç°å®ä»»åŠ¡ä¸­ï¼Œç”±äºæµè§ˆå·¥å…·å¸¦æ¥çš„é•¿ç¨‹æ¨ç†èƒ½åŠ›ä¸è¶³ä»¥åŠç¼ºä¹è¶³å¤Ÿå›°éš¾çš„ç›‘ç£æ•°æ®çš„é—®é¢˜ï¼ŒDeepDiveé¦–å…ˆæå‡ºäº†ä¸€ç§ä»å¼€æ”¾çŸ¥è¯†å›¾è°±ä¸­è‡ªåŠ¨åˆæˆå¤æ‚ã€å›°éš¾ä¸”éš¾ä»¥æ‰¾åˆ°çš„é—®é¢˜çš„ç­–ç•¥ã€‚å…¶æ¬¡ï¼Œåº”ç”¨ç«¯åˆ°ç«¯çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥å¢å¼ºLLMåœ¨æ·±åº¦æœç´¢ä¸­çš„é•¿ç¨‹æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†é¼“åŠ±å¤šæ ·æ€§å¹¶å‡å°‘å†—ä½™ï¼Œè®¾è®¡äº†ä¸€ç§å†—ä½™æƒ©ç½šï¼Œä»¥é¿å…é‡å¤ç›¸ä¼¼çš„æŸ¥è¯¢ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepDive-32Båœ¨BrowseCompä¸Šå–å¾—äº†æ–°çš„å¼€æºç«äº‰ç»“æœï¼Œä¼˜äºWebSailorã€DeepSeek-R1-Browseå’ŒSearch-o1ã€‚å¤šè½®RLè®­ç»ƒæé«˜äº†æ·±åº¦æœç´¢èƒ½åŠ›ï¼Œå¹¶æ˜¾è‘—ä¿ƒè¿›äº†å¤šä¸ªåŸºå‡†æµ‹è¯•çš„æ€§èƒ½æå‡ã€‚DeepDiveè¿˜æ”¯æŒæµ‹è¯•æ—¶å·¥å…·è°ƒç”¨çš„æ‰©å±•å’Œå¹¶è¡Œé‡‡æ ·ã€‚æ‰€æœ‰æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å‡å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿›è¡Œæ·±åº¦æœç´¢æ—¶ï¼Œé¢ä¸´ç€ä¸¤ä¸ªä¸»è¦çš„ç—›ç‚¹ã€‚ä¸€æ˜¯åˆ©ç”¨æµè§ˆå·¥å…·è¿›è¡Œé•¿ç¨‹æ¨ç†çš„èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥å¤„ç†éœ€è¦å¤šæ¬¡äº¤äº’å’Œå¤æ‚æ¨ç†çš„ä»»åŠ¡ã€‚äºŒæ˜¯ç¼ºä¹è¶³å¤Ÿå›°éš¾å’Œé«˜è´¨é‡çš„ç›‘ç£æ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDeepDiveçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®å’Œé‡‡ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ æ¥æå‡LLMçš„æ·±åº¦æœç´¢èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±è‡ªåŠ¨ç”Ÿæˆå¤æ‚çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜éœ€è¦å¤šæ¬¡æœç´¢å’Œæ¨ç†æ‰èƒ½è§£å†³ã€‚ç„¶åï¼Œä½¿ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒLLMï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æµè§ˆå·¥å…·è¿›è¡Œæœç´¢å’Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDeepDiveçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **é—®é¢˜ç”Ÿæˆæ¨¡å—**ï¼šä»çŸ¥è¯†å›¾è°±ä¸­è‡ªåŠ¨ç”Ÿæˆå¤æ‚ã€å›°éš¾çš„é—®é¢˜ã€‚2) **æœç´¢Agentæ¨¡å—**ï¼šåŸºäºLLMæ„å»ºçš„æœç´¢Agentï¼Œè´Ÿè´£æ ¹æ®é—®é¢˜ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼Œå¹¶è§£ææœç´¢ç»“æœã€‚3) **å¼ºåŒ–å­¦ä¹ æ¨¡å—**ï¼šä½¿ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒæœç´¢Agentï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°è¿›è¡Œé•¿ç¨‹æ¨ç†å’Œæœç´¢ã€‚4) **å†—ä½™æƒ©ç½šæ¨¡å—**ï¼šé€šè¿‡å¼•å…¥å†—ä½™æƒ©ç½šï¼Œé¼“åŠ±Agentç”Ÿæˆå¤šæ ·åŒ–çš„æŸ¥è¯¢ï¼Œé¿å…é‡å¤æœç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šDeepDiveæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºç»“åˆäº†çŸ¥è¯†å›¾è°±çš„é—®é¢˜ç”Ÿæˆå’Œå¤šè½®å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ–¹æ³•ã€‚é€šè¿‡çŸ¥è¯†å›¾è°±ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ç¼ºä¹è¶³å¤Ÿè®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚é€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼Œæå‡äº†LLMçš„é•¿ç¨‹æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚æœç´¢ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¼ºåŒ–å­¦ä¹ æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†Proximal Policy Optimization (PPO) ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†é¼“åŠ±å¤šæ ·æ€§å¹¶å‡å°‘å†—ä½™ï¼Œè®¾è®¡äº†ä¸€ç§å†—ä½™æƒ©ç½šé¡¹ï¼Œè¯¥æƒ©ç½šé¡¹åŸºäºæŸ¥è¯¢ä¹‹é—´çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°å¥–åŠ±å‡½æ•°ä¸­ã€‚å…·ä½“è€Œè¨€ï¼Œå¦‚æœAgentç”Ÿæˆçš„æŸ¥è¯¢ä¸ä¹‹å‰çš„æŸ¥è¯¢è¿‡äºç›¸ä¼¼ï¼Œåˆ™ä¼šå—åˆ°æƒ©ç½šã€‚æ­¤å¤–ï¼ŒDeepDiveè¿˜æ”¯æŒæµ‹è¯•æ—¶å·¥å…·è°ƒç”¨çš„æ‰©å±•å’Œå¹¶è¡Œé‡‡æ ·ï¼Œè¿›ä¸€æ­¥æå‡äº†æœç´¢æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DeepDive-32Båœ¨BrowseCompåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„å¼€æºé¢†å…ˆç»“æœï¼Œè¶…è¶Šäº†WebSailorã€DeepSeek-R1-Browseå’ŒSearch-o1ç­‰ç°æœ‰æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ˜¾è‘—æå‡äº†æ·±åº¦æœç´¢èƒ½åŠ›ï¼Œå¹¶å¯¹å¤šä¸ªåŸºå‡†æµ‹è¯•çš„æ€§èƒ½æå‡åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚æ­¤å¤–ï¼ŒDeepDiveè¿˜æ”¯æŒæµ‹è¯•æ—¶å·¥å…·è°ƒç”¨çš„æ‰©å±•å’Œå¹¶è¡Œé‡‡æ ·ï¼Œè¿›ä¸€æ­¥æå‡äº†æœç´¢æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DeepDiveå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢ã€çŸ¥è¯†å‘ç°ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©ç”¨æˆ·æ›´é«˜æ•ˆåœ°è·å–æ‰€éœ€ä¿¡æ¯ï¼Œè§£å†³å¤æ‚é—®é¢˜ã€‚æœªæ¥ï¼ŒDeepDiveæœ‰æœ›åº”ç”¨äºè‡ªåŠ¨åŒ–ç ”ç©¶ã€å†³ç­–æ”¯æŒç­‰æ›´é«˜çº§çš„ä»»åŠ¡ä¸­ï¼Œæå‡äººå·¥æ™ºèƒ½çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Augmenting large language models (LLMs) with browsing tools substantially improves their potential as deep search agents to solve complex, real-world tasks. Yet, open LLMs still perform poorly in such settings due to limited long-horizon reasoning capacity with browsing tools and the lack of sufficiently difficult supervised data. To address these challenges, we present DeepDive to advance deep search agents. First, we propose a strategy to automatically synthesize complex, difficult, and hard-to-find questions from open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement learning (RL) to enhance LLMs' long-horizon reasoning with deep search. To encourage diversity and reduce redundancy, we design a redundancy penalty that discourages repeated similar queries. Experiments show that DeepDive-32B achieves a new open-source competitive result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL training improves deep search ability and significantly contributes to the performance improvements across multiple benchmarks. We observe that DeepDive enables test-time scaling of tool calls and parallel sampling. All datasets, models, and code are publicly available at https://github.com/THUDM/DeepDive.

