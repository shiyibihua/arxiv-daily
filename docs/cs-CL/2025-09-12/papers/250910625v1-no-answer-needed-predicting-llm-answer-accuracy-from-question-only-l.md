---
layout: default
title: No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes
---

# No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10625" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10625v1</a>
  <a href="https://arxiv.org/pdf/2509.10625.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10625v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10625v1', 'No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: IvÃ¡n Vicente Moreno Cencerrado, Arnau PadrÃ©s Masdemont, Anton Gonzalvez Hawthorne, David Demitri Africa, Lorenzo Pacchiardi

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ä»…å‡­é—®é¢˜é¢„æµ‹LLMç­”æ¡ˆå‡†ç¡®æ€§ï¼šçº¿æ€§æ¢é’ˆæ­ç¤ºæ¨¡å‹å†…éƒ¨ç½®ä¿¡åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç­”æ¡ˆå‡†ç¡®æ€§é¢„æµ‹` `çº¿æ€§æ¢é’ˆ` `æ¨¡å‹ç½®ä¿¡åº¦` `å†…éƒ¨æœºåˆ¶` `å¯è§£é‡Šæ€§` `åˆ†å¸ƒå¤–æ³›åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨ä¸ç”Ÿæˆç­”æ¡ˆçš„æƒ…å†µä¸‹é¢„æµ‹LLMç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œé™åˆ¶äº†å¯¹æ¨¡å‹å†…éƒ¨ç½®ä¿¡åº¦è¯„ä¼°æœºåˆ¶çš„ç†è§£ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºé—®é¢˜æœ¬èº«è®­ç»ƒçº¿æ€§æ¢é’ˆçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ¨¡å‹åœ¨é—®é¢˜è¯»å–åçš„æ¿€æ´»å€¼æ¥é¢„æµ‹ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºé»‘ç›’åŸºçº¿ï¼Œä¸”èƒ½æœ‰æ•ˆé¢„æµ‹æ¨¡å‹æ˜¯å¦ä¼šå›ç­”â€œæˆ‘ä¸çŸ¥é“â€ï¼Œæ­ç¤ºäº†æ¨¡å‹å†…éƒ¨ç½®ä¿¡åº¦çš„å½¢æˆè¿‡ç¨‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦èƒ½é¢„åˆ¤è‡ªèº«ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨æ¨¡å‹è¯»å–é—®é¢˜åã€ç”Ÿæˆä»»ä½•tokenå‰æå–æ¿€æ´»å€¼ï¼Œå¹¶è®­ç»ƒçº¿æ€§æ¢é’ˆæ¥é¢„æµ‹æ¨¡å‹å³å°†ç»™å‡ºçš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨70äº¿åˆ°700äº¿å‚æ•°çš„ä¸‰ä¸ªå¼€æºæ¨¡å‹å®¶æ—ä¸­ï¼ŒåŸºäºé€šç”¨çäº‹é—®é¢˜è®­ç»ƒçš„â€œé¢„å…ˆæ­£ç¡®æ€§æ–¹å‘â€é¢„æµ‹å™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹æ¨¡å‹åœ¨åŒåˆ†å¸ƒå’Œå„ç§åˆ†å¸ƒå¤–çŸ¥è¯†æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œä¼˜äºé»‘ç›’åŸºçº¿å’Œå£å¤´ç½®ä¿¡åº¦é¢„æµ‹ã€‚é¢„æµ‹èƒ½åŠ›åœ¨ä¸­é—´å±‚è¾¾åˆ°é¥±å’Œï¼Œè¡¨æ˜è‡ªæˆ‘è¯„ä¼°èƒ½åŠ›åœ¨è®¡ç®—è¿‡ç¨‹ä¸­é€æ¸æ˜¾ç°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ–¹æ³•åœ¨éœ€è¦æ•°å­¦æ¨ç†çš„é—®é¢˜ä¸Šæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å›ç­”â€œæˆ‘ä¸çŸ¥é“â€ä¸æ¢é’ˆåˆ†æ•°é«˜åº¦ç›¸å…³ï¼Œè¡¨æ˜åŒä¸€æ–¹å‘ä¹Ÿæ•æ‰åˆ°äº†ç½®ä¿¡åº¦ã€‚æœ¬ç ”ç©¶é€šè¿‡æ¢é’ˆå’Œç¨€ç–è‡ªç¼–ç å™¨ï¼Œè¡¥å……äº†å…ˆå‰å…³äºçœŸå®æ€§å’Œå…¶ä»–è¡Œä¸ºçš„ç»“æœï¼Œä¸ºé˜æ˜LLMå†…éƒ¨æœºåˆ¶åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•ä»…é€šè¿‡è¾“å…¥é—®é¢˜ï¼Œåœ¨LLMç”Ÿæˆç­”æ¡ˆä¹‹å‰ï¼Œé¢„æµ‹å…¶ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé»‘ç›’è¯„ä¼°æˆ–éœ€è¦æ¨¡å‹ç”Ÿæˆç­”æ¡ˆåè¿›è¡Œè¯„ä¼°ï¼Œæ— æ³•ç›´æ¥æ´å¯Ÿæ¨¡å‹å†…éƒ¨çš„ç½®ä¿¡åº¦è¯„ä¼°æœºåˆ¶ï¼Œä¹Ÿæ— æ³•æå‰å¹²é¢„å¯èƒ½å‡ºç°çš„é”™è¯¯ç­”æ¡ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼ŒLLMåœ¨å¤„ç†é—®é¢˜æ—¶ï¼Œå…¶å†…éƒ¨æ¿€æ´»çŠ¶æ€åŒ…å«äº†å…³äºç­”æ¡ˆæ­£ç¡®æ€§çš„ä¿¡æ¯ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªç®€å•çš„çº¿æ€§æ¢é’ˆï¼Œå­¦ä¹ ä»è¿™äº›æ¿€æ´»çŠ¶æ€åˆ°ç­”æ¡ˆæ­£ç¡®æ€§çš„æ˜ å°„å…³ç³»ï¼Œä»è€Œå®ç°ä»…å‡­é—®é¢˜é¢„æµ‹ç­”æ¡ˆå‡†ç¡®æ€§çš„ç›®æ ‡ã€‚è¿™ç§æ–¹æ³•å‡è®¾æ¨¡å‹åœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰å·²ç»å¯¹ç­”æ¡ˆçš„ç½®ä¿¡åº¦æœ‰äº†ä¸€å®šçš„è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) ç»™å®šä¸€ä¸ªé—®é¢˜ï¼Œå°†å…¶è¾“å…¥åˆ°LLMä¸­ï¼›2) åœ¨LLMçš„ç‰¹å®šå±‚æå–æ¿€æ´»å‘é‡ï¼›3) ä½¿ç”¨æå–çš„æ¿€æ´»å‘é‡è®­ç»ƒä¸€ä¸ªçº¿æ€§æ¢é’ˆï¼Œä»¥é¢„æµ‹ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼›4) ä½¿ç”¨è®­ç»ƒå¥½çš„çº¿æ€§æ¢é’ˆé¢„æµ‹æ–°é—®é¢˜çš„ç­”æ¡ˆæ­£ç¡®æ€§ï¼Œå¹¶ä¸å®é™…ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°é¢„æµ‹æ€§èƒ½ã€‚è®ºæ–‡åœ¨å¤šä¸ªLLMæ¨¡å‹å®¶æ—å’Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œå®ƒè¯æ˜äº†ä»…å‡­é—®é¢˜æœ¬èº«ï¼Œå°±å¯ä»¥é¢„æµ‹LLMç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚è¿™è¡¨æ˜LLMåœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰ï¼Œå·²ç»å¯¹ç­”æ¡ˆçš„ç½®ä¿¡åº¦æœ‰äº†ä¸€å®šçš„è¯„ä¼°ï¼Œå¹¶ä¸”è¿™ç§è¯„ä¼°ä¿¡æ¯éšè—åœ¨æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»çŠ¶æ€ä¸­ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦ç”Ÿæˆç­”æ¡ˆï¼Œå¯ä»¥ç›´æ¥æ´å¯Ÿæ¨¡å‹å†…éƒ¨çš„ç½®ä¿¡åº¦è¯„ä¼°æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„LLMå±‚æå–æ¿€æ´»å‘é‡ã€‚å®éªŒè¡¨æ˜ï¼Œä¸­é—´å±‚çš„æ¿€æ´»å‘é‡åŒ…å«çš„ä¿¡æ¯æœ€ä¸ºä¸°å¯Œï¼›2) ä½¿ç”¨çº¿æ€§æ¢é’ˆè¿›è¡Œé¢„æµ‹ï¼Œä¿è¯äº†æ¨¡å‹çš„ç®€å•æ€§å’Œå¯è§£é‡Šæ€§ï¼›3) ä½¿ç”¨é€šç”¨çäº‹é—®é¢˜è®­ç»ƒæ¢é’ˆï¼Œå¹¶åœ¨å„ç§åˆ†å¸ƒå¤–çŸ¥è¯†æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›4) æ¢é’ˆçš„è®­ç»ƒç›®æ ‡æ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼Œå³é¢„æµ‹ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºé€šç”¨çäº‹é—®é¢˜è®­ç»ƒçš„çº¿æ€§æ¢é’ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹æ¨¡å‹åœ¨åŒåˆ†å¸ƒå’Œå„ç§åˆ†å¸ƒå¤–çŸ¥è¯†æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œä¼˜äºé»‘ç›’åŸºçº¿å’Œå£å¤´ç½®ä¿¡åº¦é¢„æµ‹ã€‚é¢„æµ‹èƒ½åŠ›åœ¨ä¸­é—´å±‚è¾¾åˆ°é¥±å’Œï¼Œè¡¨æ˜è‡ªæˆ‘è¯„ä¼°èƒ½åŠ›åœ¨è®¡ç®—è¿‡ç¨‹ä¸­é€æ¸æ˜¾ç°ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å›ç­”â€œæˆ‘ä¸çŸ¥é“â€ä¸æ¢é’ˆåˆ†æ•°é«˜åº¦ç›¸å…³ï¼Œè¡¨æ˜åŒä¸€æ–¹å‘ä¹Ÿæ•æ‰åˆ°äº†ç½®ä¿¡åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºLLMçš„å¯é æ€§è¯„ä¼°ã€é”™è¯¯æ£€æµ‹ä¸çº æ­£ã€‚ä¾‹å¦‚ï¼Œåœ¨LLMç”Ÿæˆç­”æ¡ˆä¹‹å‰ï¼Œåˆ©ç”¨è¯¥æ–¹æ³•é¢„æµ‹ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¦‚æœé¢„æµ‹ç»“æœä¸ä½³ï¼Œå¯ä»¥é‡‡å–æªæ–½ï¼ˆå¦‚é‡æ–°æé—®ã€ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ç­‰ï¼‰é¿å…é”™è¯¯ç­”æ¡ˆçš„äº§ç”Ÿã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºç ”ç©¶LLMå†…éƒ¨çš„ç½®ä¿¡åº¦è¯„ä¼°æœºåˆ¶ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å’Œæ§åˆ¶LLMçš„è¡Œä¸ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Do large language models (LLMs) anticipate when they will answer correctly? To study this, we extract activations after a question is read but before any tokens are generated, and train linear probes to predict whether the model's forthcoming answer will be correct. Across three open-source model families ranging from 7 to 70 billion parameters, projections on this "in-advance correctness direction" trained on generic trivia questions predict success in distribution and on diverse out-of-distribution knowledge datasets, outperforming black-box baselines and verbalised predicted confidence. Predictive power saturates in intermediate layers, suggesting that self-assessment emerges mid-computation. Notably, generalisation falters on questions requiring mathematical reasoning. Moreover, for models responding "I don't know", doing so strongly correlates with the probe score, indicating that the same direction also captures confidence. By complementing previous results on truthfulness and other behaviours obtained with probes and sparse auto-encoders, our work contributes essential findings to elucidate LLM internals.

