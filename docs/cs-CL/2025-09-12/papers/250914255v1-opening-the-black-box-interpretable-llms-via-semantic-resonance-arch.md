---
layout: default
title: Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture
---

# Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14255" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14255v1</a>
  <a href="https://arxiv.org/pdf/2509.14255.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14255v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14255v1', 'Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ivan Ternovtsii

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

**å¤‡æ³¨**: 13 pages, 5 figures. Code available at https://github.com/ITernovtsii/semantic-resonance. Preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰å…±æŒ¯æ¶æ„SRAï¼Œæå‡LLMå¯è§£é‡Šæ€§ä¸ä¸“å®¶åˆ©ç”¨ç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `æ··åˆä¸“å®¶æ¨¡å‹` `è¯­ä¹‰è·¯ç”±` `ä¸“å®¶åˆ©ç”¨ç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå¯è§£é‡Šæ€§å·®ï¼ŒMoEæ¨¡å‹é—¨æ§å‡½æ•°ä¸é€æ˜ï¼Œé™åˆ¶äº†æ¨¡å‹ç†è§£å’Œæ§åˆ¶ã€‚
2. SRAé€šè¿‡è¯­ä¹‰å…±æŒ¯å®¤CSRï¼Œåˆ©ç”¨tokenä¸è¯­ä¹‰é”šç‚¹çš„ç›¸ä¼¼æ€§è¿›è¡Œè·¯ç”±ï¼Œæå‡å¯è§£é‡Šæ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSRAåœ¨å›°æƒ‘åº¦ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—æå‡äº†ä¸“å®¶åˆ©ç”¨ç‡ï¼Œé™ä½äº†æ­»äº¡ä¸“å®¶æ¯”ä¾‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ€§èƒ½å“è¶Šï¼Œä½†å¯è§£é‡Šæ€§ä»ç„¶ä¸è¶³ã€‚æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰é€šè¿‡ç¨€ç–æ¿€æ´»æé«˜æ•ˆç‡ï¼Œä½†é€šå¸¸ä¾èµ–äºä¸é€æ˜çš„å­¦ä¹ é—¨æ§å‡½æ•°ã€‚è™½ç„¶åŸºäºç›¸ä¼¼æ€§çš„è·¯ç”±ï¼ˆä½™å¼¦è·¯ç”±å™¨ï¼‰å·²è¢«ç”¨äºè®­ç»ƒç¨³å®šï¼Œä½†å…¶å†…åœ¨å¯è§£é‡Šæ€§çš„æ½œåŠ›å°šæœªè¢«å……åˆ†æŒ–æ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰å…±æŒ¯æ¶æ„ï¼ˆSRAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§MoEæ–¹æ³•ï¼Œæ—¨åœ¨ç¡®ä¿è·¯ç”±å†³ç­–å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ã€‚SRAç”¨è¯­ä¹‰å…±æŒ¯å®¤ï¼ˆCSRï¼‰æ¨¡å—å–ä»£äº†å­¦ä¹ é—¨æ§ï¼Œè¯¥æ¨¡å—åŸºäºä¸å¯è®­ç»ƒè¯­ä¹‰é”šç‚¹çš„ä½™å¼¦ç›¸ä¼¼æ€§æ¥è·¯ç”±tokenã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„åˆ†æ•£æŸå¤±ï¼Œé¼“åŠ±é”šç‚¹ä¹‹é—´çš„æ­£äº¤æ€§ï¼Œä»¥å®ç°å¤šæ ·åŒ–çš„ä¸“ä¸šåŒ–ã€‚åœ¨WikiText-103ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSRAå®ç°äº†13.41çš„éªŒè¯å›°æƒ‘åº¦ï¼Œä¼˜äºå¯†é›†åŸºçº¿ï¼ˆ14.13ï¼‰å’Œæ ‡å‡†MoEåŸºçº¿ï¼ˆ13.53ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†åŒ¹é…çš„æ¿€æ´»å‚æ•°çº¦æŸï¼ˆ29.0Mï¼‰ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼ŒSRAè¡¨ç°å‡ºå“è¶Šçš„ä¸“å®¶åˆ©ç”¨ç‡ï¼ˆ1.0%çš„æ­»äº¡ä¸“å®¶ï¼Œè€Œæ ‡å‡†MoEä¸º14.8%ï¼‰ï¼Œå¹¶å‘å±•å‡ºç‹¬ç‰¹çš„ã€è¯­ä¹‰è¿è´¯çš„ä¸“ä¸šåŒ–æ¨¡å¼ï¼Œè¿™ä¸æ ‡å‡†MoEä¸­è§‚å¯Ÿåˆ°çš„å˜ˆæ‚ä¸“ä¸šåŒ–ä¸åŒã€‚è¿™é¡¹å·¥ä½œç¡®ç«‹äº†è¯­ä¹‰è·¯ç”±ä½œä¸ºæ„å»ºæ›´é€æ˜å’Œå¯æ§è¯­è¨€æ¨¡å‹çš„å¼ºå¤§æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯è§£é‡Šæ€§å·®ï¼Œå°¤å…¶æ˜¯åœ¨æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ä¸­ï¼Œå…¶è·¯ç”±å†³ç­–ä¾èµ–äºå¤æ‚çš„ã€å­¦ä¹ å¾—åˆ°çš„é—¨æ§å‡½æ•°ï¼Œè¿™äº›å‡½æ•°éš¾ä»¥ç†è§£å’Œæ§åˆ¶ã€‚è¿™ä½¿å¾—æˆ‘ä»¬éš¾ä»¥ç†è§£æ¨¡å‹ä¸ºä»€ä¹ˆåšå‡ºç‰¹å®šçš„å†³ç­–ï¼Œä»¥åŠå¦‚ä½•æ”¹è¿›æ¨¡å‹çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçš„MoEæ¨¡å‹ç»å¸¸å‡ºç°ä¸“å®¶åˆ©ç”¨ç‡ä½çš„é—®é¢˜ï¼Œå³æŸäº›ä¸“å®¶å§‹ç»ˆæœªè¢«æ¿€æ´»ï¼ˆâ€œæ­»äº¡ä¸“å®¶â€ï¼‰ï¼Œå¯¼è‡´è®¡ç®—èµ„æºçš„æµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç”¨è¯­ä¹‰ç›¸ä¼¼æ€§æ¥æŒ‡å¯¼tokençš„è·¯ç”±å†³ç­–ï¼Œä»è€Œä½¿è·¯ç”±è¿‡ç¨‹æ›´åŠ é€æ˜å’Œå¯è§£é‡Šã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡å¼•å…¥äº†â€œè¯­ä¹‰å…±æŒ¯æ¶æ„â€ï¼ˆSRAï¼‰ï¼Œè¯¥æ¶æ„ä½¿ç”¨ä¸€ç»„å¯è®­ç»ƒçš„â€œè¯­ä¹‰é”šç‚¹â€æ¥ä»£è¡¨ä¸åŒçš„è¯­ä¹‰æ¦‚å¿µã€‚æ¯ä¸ªtokenæ ¹æ®å…¶ä¸è¿™äº›é”šç‚¹çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ˆé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼‰è¢«è·¯ç”±åˆ°ç›¸åº”çš„ä¸“å®¶ã€‚è¿™ç§åŸºäºè¯­ä¹‰çš„è·¯ç”±æ–¹å¼ä½¿å¾—æˆ‘ä»¬å¯ä»¥æ›´å®¹æ˜“åœ°ç†è§£æ¯ä¸ªä¸“å®¶è´Ÿè´£å¤„ç†å“ªäº›ç±»å‹çš„è¾“å…¥ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSRAçš„æ ¸å¿ƒæ˜¯â€œè¯­ä¹‰å…±æŒ¯å®¤â€ï¼ˆCSRï¼‰æ¨¡å—ï¼Œå®ƒå–ä»£äº†ä¼ ç»ŸMoEä¸­çš„å­¦ä¹ é—¨æ§å‡½æ•°ã€‚CSRæ¨¡å—åŒ…å«ä¸€ç»„å¯è®­ç»ƒçš„è¯­ä¹‰é”šç‚¹ï¼Œæ¯ä¸ªé”šç‚¹ä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„è¯­ä¹‰æ¦‚å¿µã€‚å½“ä¸€ä¸ªtokenè¾“å…¥CSRæ¨¡å—æ—¶ï¼Œå®ƒä¼šè®¡ç®—ä¸æ¯ä¸ªé”šç‚¹çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ç„¶åï¼Œæ ¹æ®è¿™äº›ç›¸ä¼¼åº¦ï¼Œtokenè¢«è·¯ç”±åˆ°ç›¸åº”çš„ä¸“å®¶ã€‚æ•´ä¸ªæ¶æ„å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªMoEæ¨¡å‹ï¼Œå…¶ä¸­CSRæ¨¡å—è´Ÿè´£è·¯ç”±ï¼Œè€Œä¸“å®¶è´Ÿè´£å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šSRAæœ€å…³é”®çš„åˆ›æ–°åœ¨äºä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼æ€§æ¥æŒ‡å¯¼tokençš„è·¯ç”±å†³ç­–ã€‚ä¸ä¼ ç»Ÿçš„MoEæ¨¡å‹ä¸åŒï¼ŒSRAä¸ä¾èµ–äºå¤æ‚çš„ã€å­¦ä¹ å¾—åˆ°çš„é—¨æ§å‡½æ•°ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ç»„å¯è§£é‡Šçš„è¯­ä¹‰é”šç‚¹ã€‚è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥æ›´å®¹æ˜“åœ°ç†è§£æ¯ä¸ªä¸“å®¶è´Ÿè´£å¤„ç†å“ªäº›ç±»å‹çš„è¾“å…¥ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„åˆ†æ•£æŸå¤±ï¼Œé¼“åŠ±é”šç‚¹ä¹‹é—´çš„æ­£äº¤æ€§ï¼Œä»¥å®ç°å¤šæ ·åŒ–çš„ä¸“ä¸šåŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šSRAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºè¯­ä¹‰ç›¸ä¼¼æ€§çš„åº¦é‡æ ‡å‡†ï¼›2ï¼‰å¼•å…¥è¯­ä¹‰é”šç‚¹æ¥ä»£è¡¨ä¸åŒçš„è¯­ä¹‰æ¦‚å¿µï¼›3ï¼‰ä½¿ç”¨åˆ†æ•£æŸå¤±æ¥é¼“åŠ±é”šç‚¹ä¹‹é—´çš„æ­£äº¤æ€§ã€‚åˆ†æ•£æŸå¤±çš„å…·ä½“å½¢å¼æœªçŸ¥ï¼Œä½†å…¶ç›®çš„æ˜¯ç¡®ä¿ä¸åŒçš„é”šç‚¹ä»£è¡¨ä¸åŒçš„è¯­ä¹‰æ¦‚å¿µï¼Œä»è€Œé¿å…å‡ºç°å¤šä¸ªé”šç‚¹ä»£è¡¨ç›¸åŒæ¦‚å¿µçš„æƒ…å†µã€‚ä¸“å®¶ç½‘ç»œçš„ç»“æ„æœªçŸ¥ï¼Œä½†å¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SRAåœ¨WikiText-103æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼ŒéªŒè¯å›°æƒ‘åº¦ä¸º13.41ï¼Œä¼˜äºå¯†é›†åŸºçº¿ï¼ˆ14.13ï¼‰å’Œæ ‡å‡†MoEåŸºçº¿ï¼ˆ13.53ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸åŒçš„æ¿€æ´»å‚æ•°æ•°é‡ï¼ˆ29.0Mï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒSRAæ˜¾è‘—æé«˜äº†ä¸“å®¶åˆ©ç”¨ç‡ï¼Œæ­»äº¡ä¸“å®¶æ¯”ä¾‹ä»…ä¸º1.0%ï¼Œè¿œä½äºæ ‡å‡†MoEçš„14.8%ï¼Œå¹¶ä¸”å±•ç°å‡ºæ›´æ¸…æ™°çš„è¯­ä¹‰ä¸“ä¸šåŒ–æ¨¡å¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SRAæ¶æ„å¯åº”ç”¨äºéœ€è¦é«˜å¯è§£é‡Šæ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ç­‰ã€‚é€šè¿‡ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œå¯ä»¥æé«˜ç”¨æˆ·å¯¹æ¨¡å‹çš„ä¿¡ä»»åº¦ï¼Œå¹¶æ›´å¥½åœ°è¿›è¡Œé”™è¯¯åˆ†æå’Œæ¨¡å‹æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒSRAæ¶æ„ä¹Ÿæœ‰åŠ©äºå¼€å‘æ›´å¯æ§çš„è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚å¯ä»¥æ§åˆ¶æ¨¡å‹ç”Ÿæˆç‰¹å®šä¸»é¢˜æˆ–é£æ ¼çš„æ–‡æœ¬ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) achieve remarkable performance but remain difficult to interpret. Mixture-of-Experts (MoE) models improve efficiency through sparse activation, yet typically rely on opaque, learned gating functions. While similarity-based routing (Cosine Routers) has been explored for training stabilization, its potential for inherent interpretability remains largely untapped. We introduce the Semantic Resonance Architecture (SRA), an MoE approach designed to ensure that routing decisions are inherently interpretable. SRA replaces learned gating with a Chamber of Semantic Resonance (CSR) module, which routes tokens based on cosine similarity with trainable semantic anchors. We also introduce a novel Dispersion Loss that encourages orthogonality among anchors to enforce diverse specialization. Experiments on WikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41, outperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53) under matched active parameter constraints (29.0M). Crucially, SRA exhibits superior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE) and develops distinct, semantically coherent specialization patterns, unlike the noisy specialization observed in standard MoEs. This work establishes semantic routing as a robust methodology for building more transparent and controllable language models.

