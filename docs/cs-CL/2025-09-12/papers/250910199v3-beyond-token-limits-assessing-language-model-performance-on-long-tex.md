---
layout: default
title: Beyond Token Limits: Assessing Language Model Performance on Long Text Classification
---

# Beyond Token Limits: Assessing Language Model Performance on Long Text Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10199" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10199v3</a>
  <a href="https://arxiv.org/pdf/2509.10199.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10199v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10199v3', 'Beyond Token Limits: Assessing Language Model Performance on Long Text Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: MiklÃ³s SebÅ‘k, Viktor KovÃ¡cs, Martin BÃ¡nÃ³czy, Daniel MÃ¸ller Eriksen, Nathalie Neptune, Philippe Roussille

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-09-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨é•¿æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå‘ç°é•¿æ–‡æœ¬ä¸“ç”¨æ¨¡å‹å¹¶æ— æ˜æ˜¾ä¼˜åŠ¿ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æœ¬åˆ†ç±»` `è¯­è¨€æ¨¡å‹` `Longformer` `XLM-RoBERTa` `GPT-3.5` `GPT-4` `æ”¿ç­–åˆ†æ` `å¤šè¯­è¨€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰BERTç­‰æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æ—¶å—é™äºè¾“å…¥é•¿åº¦ï¼Œæ— æ³•ç›´æ¥åº”ç”¨äºæ³•å¾‹ç­‰é•¿æ–‡æ¡£ã€‚
2. è¯¥ç ”ç©¶å¯¹æ¯”äº†XLM-RoBERTaã€Longformerã€GPTç­‰æ¨¡å‹åœ¨é•¿æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä¸“é—¨ä¸ºé•¿æ–‡æœ¬è®¾è®¡çš„Longformeræ¨¡å‹å¹¶æœªå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¼€æºæ¨¡å‹è¡¨ç°æ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¤¾äº¤ç§‘å­¦é¢†åŸŸå¹¿æ³›ä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTåŠå…¶è¡ç”Ÿæ¨¡å‹RoBERTaï¼‰åœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥æ—¶å­˜åœ¨é•¿åº¦é™åˆ¶ã€‚è¿™å¯¹äºéœ€è¦å¤„ç†é•¿æ–‡æœ¬çš„åˆ†ç±»ä»»åŠ¡ï¼ˆä¾‹å¦‚æ³•å¾‹å’Œè‰æ¡ˆï¼‰æ¥è¯´æ˜¯ä¸€ä¸ªä¸¥å³»çš„é—®é¢˜ï¼Œå› ä¸ºè¿™äº›æ–‡æœ¬å¯èƒ½é•¿è¾¾æ•°ç™¾é¡µï¼Œè¶…å‡ºæ¨¡å‹å¯å¤„ç†çš„tokenæ•°é‡ï¼ˆä¾‹å¦‚512ä¸ªï¼‰ã€‚æœ¬æ–‡ä½¿ç”¨XLM-RoBERTaã€Longformerã€GPT-3.5å’ŒGPT-4æ¨¡å‹ï¼Œé’ˆå¯¹æ¯”è¾ƒè®®ç¨‹é¡¹ç›®ï¼ˆComparative Agendas Projectï¼‰çš„å¤šç±»åˆ«åˆ†ç±»ä»»åŠ¡ï¼ˆåŒ…å«ä»æ•™è‚²åˆ°åŒ»ç–—ä¿å¥çš„21ä¸ªæ”¿ç­–ä¸»é¢˜æ ‡ç­¾ï¼‰è¿›è¡Œäº†äº”ç§è¯­è¨€çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œä¸“é—¨ä¸ºå¤„ç†é•¿è¾“å…¥è€Œé¢„è®­ç»ƒçš„Longformeræ¨¡å‹å¹¶æ²¡æœ‰è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚GPTæ¨¡å‹ä¸è¡¨ç°æœ€ä½³çš„å¼€æºæ¨¡å‹ç›¸æ¯”ï¼Œåè€…æ›´èƒœä¸€ç­¹ã€‚å¯¹ç±»åˆ«å±‚é¢å› ç´ çš„åˆ†æè¡¨æ˜ï¼Œåœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥æ—¶ï¼Œç‰¹å®šç±»åˆ«ä¹‹é—´çš„æ”¯æŒå’Œå®è´¨æ€§é‡å éå¸¸é‡è¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè¯¥è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ³•å¾‹ã€è‰æ¡ˆç­‰ç¯‡å¹…è¾ƒé•¿çš„æ–‡æ¡£ã€‚ç°æœ‰æ–¹æ³•å¦‚BERTç­‰æ¨¡å‹å—é™äºè¾“å…¥é•¿åº¦ï¼Œæ— æ³•ç›´æ¥å¤„ç†è¿™äº›é•¿æ–‡æœ¬ã€‚è™½ç„¶æœ‰Longformerç­‰ä¸“é—¨ä¸ºé•¿æ–‡æœ¬è®¾è®¡çš„æ¨¡å‹ï¼Œä½†å…¶æ€§èƒ½ä¼˜åŠ¿å°šä¸æ˜ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å®éªŒå¯¹æ¯”ä¸åŒç±»å‹çš„è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬é€šç”¨æ¨¡å‹å’Œé•¿æ–‡æœ¬ä¸“ç”¨æ¨¡å‹ï¼‰åœ¨é•¿æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä»è€Œè¯„ä¼°å®ƒä»¬åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„èƒ½åŠ›ã€‚é€šè¿‡åˆ†ææ¨¡å‹åœ¨ä¸åŒç±»åˆ«ä¸Šçš„è¡¨ç°ï¼Œæ¢ç©¶å½±å“é•¿æ–‡æœ¬åˆ†ç±»æ€§èƒ½çš„å…³é”®å› ç´ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶é‡‡ç”¨æ¯”è¾ƒè®®ç¨‹é¡¹ç›®ï¼ˆComparative Agendas Projectï¼‰çš„å¤šç±»åˆ«åˆ†ç±»ä»»åŠ¡ä½œä¸ºå®éªŒå¹³å°ã€‚è¯¥ä»»åŠ¡åŒ…å«21ä¸ªæ”¿ç­–ä¸»é¢˜æ ‡ç­¾ï¼Œæ¶µç›–ä»æ•™è‚²åˆ°åŒ»ç–—ä¿å¥ç­‰é¢†åŸŸã€‚ç ”ç©¶ä½¿ç”¨äº†XLM-RoBERTaã€Longformerã€GPT-3.5å’ŒGPT-4ç­‰æ¨¡å‹ï¼Œå¹¶å¯¹äº”ç§è¯­è¨€çš„æ•°æ®è¿›è¡Œäº†å®éªŒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹é•¿æ–‡æœ¬ä¸“ç”¨æ¨¡å‹Longformerçš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å‘ç°å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¹¶æ²¡æœ‰è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åˆ†æäº†ç±»åˆ«å±‚é¢çš„å› ç´ ï¼Œæ­ç¤ºäº†ç±»åˆ«ä¹‹é—´çš„æ”¯æŒå’Œå®è´¨æ€§é‡å å¯¹é•¿æ–‡æœ¬åˆ†ç±»æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥ç ”ç©¶ä½¿ç”¨äº†XLM-RoBERTaä½œä¸ºåŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸Longformerå’ŒGPTç³»åˆ—æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒä¸­ï¼Œç ”ç©¶äººå‘˜ä½¿ç”¨äº†æ¯”è¾ƒè®®ç¨‹é¡¹ç›®æä¾›çš„å¤šè¯­è¨€æ•°æ®é›†ï¼Œå¹¶é’ˆå¯¹ä¸åŒçš„æ¨¡å‹è¿›è¡Œäº†é€‚å½“çš„å‚æ•°è°ƒæ•´ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚å–å†³äºæ‰€ä½¿ç”¨çš„æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸“é—¨ä¸ºå¤„ç†é•¿è¾“å…¥è€Œé¢„è®­ç»ƒçš„Longformeræ¨¡å‹å¹¶æ²¡æœ‰è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚åœ¨æ¯”è¾ƒè®®ç¨‹é¡¹ç›®çš„å¤šç±»åˆ«åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¡¨ç°æœ€ä½³çš„å¼€æºæ¨¡å‹ä¼˜äºGPTæ¨¡å‹ã€‚ç±»åˆ«å±‚é¢çš„åˆ†æè¡¨æ˜ï¼Œç±»åˆ«ä¹‹é—´çš„æ”¯æŒå’Œå®è´¨æ€§é‡å æ˜¯å½±å“é•¿æ–‡æœ¬åˆ†ç±»æ€§èƒ½çš„é‡è¦å› ç´ ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ³•å¾‹æ–‡æœ¬åˆ†æã€æ”¿ç­–æ–‡ä»¶åˆ†ç±»ã€ç¤¾ä¼šç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æé«˜é•¿æ–‡æœ¬åˆ†ç±»çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„å†³ç­–æä¾›æ”¯æŒã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•ä¼˜åŒ–æ¨¡å‹ç»“æ„å’Œè®­ç»ƒæ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°å¤„ç†é•¿æ–‡æœ¬æ•°æ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The most widely used large language models in the social sciences (such as BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text length that they can process to produce predictions. This is a particularly pressing issue for some classification tasks, where the aim is to handle long input texts. One such area deals with laws and draft laws (bills), which can have a length of multiple hundred pages and, therefore, are not particularly amenable for processing with models that can only handle e.g. 512 tokens. In this paper, we show results from experiments covering 5 languages with XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass classification task of the Comparative Agendas Project, which has a codebook of 21 policy topic labels from education to health care. Results show no particular advantage for the Longformer model, pre-trained specifically for the purposes of handling long inputs. The comparison between the GPT variants and the best-performing open model yielded an edge for the latter. An analysis of class-level factors points to the importance of support and substance overlaps between specific categories when it comes to performance on long text inputs.

