---
layout: default
title: Is In-Context Learning Learning?
---

# Is In-Context Learning Learning?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10414" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10414v2</a>
  <a href="https://arxiv.org/pdf/2509.10414.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10414v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10414v2', 'Is In-Context Learning Learning?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adrian de Wynter

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-09-15)

**å¤‡æ³¨**: Director's cut

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¡¨æ˜ä¸Šä¸‹æ–‡å­¦ä¹ æ˜¯ä¸€ç§æœ‰æ•ˆçš„å­¦ä¹ èŒƒå¼ï¼Œä½†å…¶æ³›åŒ–èƒ½åŠ›æœ‰é™**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `è‡ªå›å½’æ¨¡å‹` `æ³›åŒ–èƒ½åŠ›` `promptå·¥ç¨‹` `åˆ†å¸ƒåç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¯¹ä¸Šä¸‹æ–‡å­¦ä¹ (ICL)æ˜¯å¦çœŸæ­£å…·å¤‡å­¦ä¹ èƒ½åŠ›å­˜åœ¨äº‰è®®ï¼ŒICLä¾èµ–é¢„è®­ç»ƒçŸ¥è¯†å’Œpromptç¤ºä¾‹ï¼Œç¼ºä¹æ˜¾å¼ç¼–ç ã€‚
2. è¯¥ç ”ç©¶é€šè¿‡æ•°å­¦è®ºè¯å’Œå¤§è§„æ¨¡å®éªŒåˆ†æï¼Œè®ºè¯äº†ICLåœ¨æ•°å­¦ä¸Šæ„æˆå­¦ä¹ ï¼Œä½†éœ€è¦è¿›ä¸€æ­¥çš„å®éªŒéªŒè¯å…¶ç‰¹æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒICLæ˜¯ä¸€ç§æœ‰æ•ˆçš„å­¦ä¹ èŒƒå¼ï¼Œä½†å…¶å­¦ä¹ å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡çš„èƒ½åŠ›å—åˆ°é™åˆ¶ï¼Œå¯¹promptçš„è§„å¾‹æ€§æ•æ„Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸Šä¸‹æ–‡å­¦ä¹ (ICL)å…è®¸ä¸€äº›è‡ªå›å½’æ¨¡å‹é€šè¿‡ä¸‹ä¸€ä¸ªtokené¢„æµ‹æ¥è§£å†³ä»»åŠ¡ï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥çš„è®­ç»ƒã€‚è¿™å¯¼è‡´äº†å…³äºè¿™äº›æ¨¡å‹ä»…é€šè¿‡promptä¸­çš„å°‘é‡ç¤ºä¾‹å°±èƒ½è§£å†³(å­¦ä¹ )æœªè§è¿‡çš„ä»»åŠ¡çš„èƒ½åŠ›çš„ä¸»å¼ ã€‚ç„¶è€Œï¼Œæ¼”ç»å¹¶ä¸æ€»æ˜¯æ„å‘³ç€å­¦ä¹ ï¼Œå› ä¸ºICLæ²¡æœ‰æ˜¾å¼åœ°ç¼–ç ç»™å®šçš„è§‚å¯Ÿã€‚ç›¸åï¼Œæ¨¡å‹ä¾èµ–äºå®ƒä»¬çš„å…ˆéªŒçŸ¥è¯†å’Œç»™å®šçš„ç¤ºä¾‹(å¦‚æœæœ‰çš„è¯)ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œä»æ•°å­¦ä¸Šè®²ï¼ŒICLç¡®å®æ„æˆäº†å­¦ä¹ ï¼Œä½†å…¶å®Œæ•´çš„è¡¨å¾éœ€è¦ç»éªŒç ”ç©¶ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹ICLè¿›è¡Œäº†å¤§è§„æ¨¡çš„åˆ†æï¼Œæ¶ˆé™¤äº†æˆ–è§£é‡Šäº†è®°å¿†ã€é¢„è®­ç»ƒã€åˆ†å¸ƒåç§»ä»¥åŠprompté£æ ¼å’Œæªè¾ã€‚æˆ‘ä»¬å‘ç°ICLæ˜¯ä¸€ç§æœ‰æ•ˆçš„å­¦ä¹ èŒƒå¼ï¼Œä½†å…¶å­¦ä¹ å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡çš„èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨ç¤ºä¾‹å˜å¾—è¶Šæ¥è¶Šå¤šçš„æé™æƒ…å†µä¸‹ï¼Œå‡†ç¡®æ€§å¯¹ç¤ºä¾‹åˆ†å¸ƒã€æ¨¡å‹ã€prompté£æ ¼å’Œè¾“å…¥çš„è¯­è¨€ç‰¹å¾ä¸æ•æ„Ÿã€‚ç›¸åï¼Œå®ƒä»promptä¸­çš„è§„å¾‹æ€§ä¸­æ¨æ–­å‡ºæ¨¡å¼ï¼Œè¿™å¯¼è‡´äº†åˆ†å¸ƒæ•æ„Ÿæ€§ï¼Œå°¤å…¶æ˜¯åœ¨è¯¸å¦‚æ€ç»´é“¾ä¹‹ç±»çš„prompté£æ ¼ä¸­ã€‚é‰´äºå½¢å¼ä¸Šç›¸ä¼¼çš„ä»»åŠ¡ä¸Šçš„å„ç§å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œè‡ªå›å½’çš„ad-hocç¼–ç ä¸æ˜¯ä¸€ç§é²æ£’çš„æœºåˆ¶ï¼Œå¹¶è¡¨æ˜å…¶å…¨èƒ½æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ (ICL)æ˜¯å¦çœŸæ­£å…·å¤‡å­¦ä¹ èƒ½åŠ›ï¼Œä»¥åŠICLçš„å­¦ä¹ æœºåˆ¶å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰ç ”ç©¶å¯¹ICLçš„ç†è§£å­˜åœ¨äº‰è®®ï¼Œä¸€äº›ç ”ç©¶è®¤ä¸ºICLä»…ä»…æ˜¯åˆ©ç”¨äº†é¢„è®­ç»ƒçŸ¥è¯†å’Œpromptä¸­çš„ç¤ºä¾‹è¿›è¡Œæ¼”ç»ï¼Œè€ŒéçœŸæ­£çš„å­¦ä¹ ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ICLå­¦ä¹ æœºåˆ¶çš„æ·±å…¥åˆ†æï¼Œä»¥åŠå¯¹å½±å“ICLæ€§èƒ½çš„å…³é”®å› ç´ çš„ç³»ç»Ÿæ€§ç ”ç©¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ•°å­¦è®ºè¯å’Œå¤§è§„æ¨¡å®éªŒåˆ†æç›¸ç»“åˆçš„æ–¹å¼ï¼Œæ·±å…¥ç ”ç©¶ICLçš„å­¦ä¹ æœºåˆ¶å’Œæ³›åŒ–èƒ½åŠ›ã€‚é¦–å…ˆï¼Œä»æ•°å­¦è§’åº¦è®ºè¯ICLæ„æˆå­¦ä¹ ã€‚ç„¶åï¼Œé€šè¿‡æ§åˆ¶å˜é‡çš„æ–¹å¼ï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶è®°å¿†ã€é¢„è®­ç»ƒã€åˆ†å¸ƒåç§»å’Œprompté£æ ¼ç­‰å› ç´ å¯¹ICLæ€§èƒ½çš„å½±å“ã€‚é€šè¿‡åˆ†æå®éªŒç»“æœï¼Œæ­ç¤ºICLçš„å­¦ä¹ æœºåˆ¶å’Œå±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„ç ”ç©¶æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) æ•°å­¦è®ºè¯ï¼šä»æ•°å­¦è§’åº¦è¯æ˜ICLæ„æˆå­¦ä¹ ã€‚2) å¤§è§„æ¨¡å®éªŒåˆ†æï¼šè®¾è®¡ä¸€ç³»åˆ—å®éªŒï¼Œç ”ç©¶ä¸åŒå› ç´ å¯¹ICLæ€§èƒ½çš„å½±å“ã€‚è¿™äº›å› ç´ åŒ…æ‹¬ï¼ša) è®°å¿†ï¼šé€šè¿‡å»é™¤è®­ç»ƒæ•°æ®ä¸­çš„ç›¸å…³ä¿¡æ¯ï¼Œç ”ç©¶è®°å¿†å¯¹ICLçš„å½±å“ã€‚b) é¢„è®­ç»ƒï¼šé€šè¿‡æ”¹å˜é¢„è®­ç»ƒæ•°æ®ï¼Œç ”ç©¶é¢„è®­ç»ƒå¯¹ICLçš„å½±å“ã€‚c) åˆ†å¸ƒåç§»ï¼šé€šè¿‡æ”¹å˜è¾“å…¥æ•°æ®çš„åˆ†å¸ƒï¼Œç ”ç©¶åˆ†å¸ƒåç§»å¯¹ICLçš„å½±å“ã€‚d) Prompté£æ ¼ï¼šé€šè¿‡æ”¹å˜promptçš„é£æ ¼å’Œæªè¾ï¼Œç ”ç©¶prompté£æ ¼å¯¹ICLçš„å½±å“ã€‚3) ç»“æœåˆ†æï¼šåˆ†æå®éªŒç»“æœï¼Œæ­ç¤ºICLçš„å­¦ä¹ æœºåˆ¶å’Œå±€é™æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†å½±å“ICLæ€§èƒ½çš„å¤šä¸ªå› ç´ ï¼ŒåŒ…æ‹¬è®°å¿†ã€é¢„è®­ç»ƒã€åˆ†å¸ƒåç§»å’Œprompté£æ ¼ç­‰ã€‚2) æ­ç¤ºäº†ICLå¯¹promptè§„å¾‹æ€§çš„æ•æ„Ÿæ€§ï¼Œä»¥åŠåœ¨ç¤ºä¾‹æ•°é‡è¶³å¤Ÿå¤šçš„æƒ…å†µä¸‹ï¼ŒICLå¯¹ç¤ºä¾‹åˆ†å¸ƒã€æ¨¡å‹å’Œprompté£æ ¼çš„ä¸æ•æ„Ÿæ€§ã€‚3) æå‡ºäº†è‡ªå›å½’çš„ad-hocç¼–ç ä¸æ˜¯ä¸€ç§é²æ£’çš„æœºåˆ¶ï¼Œå¹¶è¡¨æ˜å…¶å…¨èƒ½æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç²¾å¿ƒè®¾è®¡çš„å®éªŒï¼Œé€šè¿‡æ§åˆ¶å˜é‡çš„æ–¹å¼ï¼Œç ”ç©¶ä¸åŒå› ç´ å¯¹ICLæ€§èƒ½çš„å½±å“ã€‚2) å¤§è§„æ¨¡çš„å®éªŒæ•°æ®ï¼Œä¿è¯äº†å®éªŒç»“æœçš„å¯é æ€§ã€‚3) ç»†è‡´çš„å®éªŒç»“æœåˆ†æï¼Œæ­ç¤ºäº†ICLçš„å­¦ä¹ æœºåˆ¶å’Œå±€é™æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚å–å†³äºæ‰€ä½¿ç”¨çš„å…·ä½“æ¨¡å‹å’Œæ•°æ®é›†ï¼Œè®ºæ–‡ä¸­æœªè¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒICLæ˜¯ä¸€ç§æœ‰æ•ˆçš„å­¦ä¹ èŒƒå¼ï¼Œä½†åœ¨å­¦ä¹ å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡çš„èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™æ€§ã€‚å½“ç¤ºä¾‹æ•°é‡å¢å¤šæ—¶ï¼Œå‡†ç¡®ç‡å¯¹ç¤ºä¾‹åˆ†å¸ƒã€æ¨¡å‹ã€prompté£æ ¼å’Œè¾“å…¥è¯­è¨€ç‰¹å¾ä¸æ•æ„Ÿï¼Œè€Œæ˜¯ä»promptçš„è§„å¾‹æ€§ä¸­æ¨æ–­æ¨¡å¼ï¼Œå¯¼è‡´å¯¹åˆ†å¸ƒçš„æ•æ„Ÿæ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ€ç»´é“¾ç­‰prompté£æ ¼ä¸­ã€‚åœ¨å½¢å¼ç›¸ä¼¼çš„ä»»åŠ¡ä¸Šï¼Œå‡†ç¡®ç‡å·®å¼‚è¾ƒå¤§ï¼Œè¡¨æ˜è‡ªå›å½’çš„ad-hocç¼–ç ä¸æ˜¯ä¸€ç§é²æ£’çš„æœºåˆ¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæ›´å¥½åœ°ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶ä¸ºæ”¹è¿›ICLæ–¹æ³•æä¾›æŒ‡å¯¼ã€‚æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–promptè®¾è®¡å’Œè®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥æé«˜ICLçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œæ›´å¥½åœ°è§£å†³å„ç§å®é™…é—®é¢˜ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ICLçš„å­¦ä¹ æœºåˆ¶ï¼Œå¹¶å¼€å‘æ›´é²æ£’å’Œé«˜æ•ˆçš„ICLæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model's ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. We argue that, mathematically, ICL does constitute learning, but its full characterisation requires empirical work. We then carry out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. We find that ICL is an effective learning paradigm, but limited in its ability to learn and generalise to unseen tasks. We note that, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input's linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies on formally similar tasks, we conclude that autoregression's ad-hoc encoding is not a robust mechanism, and suggests limited all-purpose generalisability.

