---
layout: default
title: Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs
---

# Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10377" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10377v1</a>
  <a href="https://arxiv.org/pdf/2509.10377.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10377v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10377v1', 'Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yixiao Zhou, Ziyu Zhao, Dongzhou Cheng, zhiliang wu, Jie Gui, Yi Yang, Fei Wu, Yu Cheng, Hehe Fan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

**å¤‡æ³¨**: Accepted to EMNLP2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDERNï¼šä¸€ç§å…è®­ç»ƒçš„ä¸“å®¶å‰ªæä¸ç¥ç»å…ƒé‡ç»„æ¡†æ¶ï¼Œæå‡ç¨€ç–MoE LLMæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹` `æ¨¡å‹å‰ªæ` `ç¥ç»å…ƒé‡ç»„` `å…è®­ç»ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. SMoEæ¨¡å‹è™½é«˜æ•ˆï¼Œä½†éœ€åŠ è½½æ‰€æœ‰ä¸“å®¶å‚æ•°ï¼Œå¯¼è‡´é«˜å†…å­˜å ç”¨ï¼Œé™åˆ¶äº†éƒ¨ç½²ã€‚
2. DERNé€šè¿‡å‰ªæå†—ä½™ä¸“å®¶ï¼Œå¹¶å°†å‰©ä½™ä¸“å®¶åˆ†è§£ä¸ºç¥ç»å…ƒç‰‡æ®µï¼Œå†é‡ç»„ä¼˜åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDERNåœ¨50%ç¨€ç–åº¦ä¸‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œæ€§èƒ½æå‡è¶…5%ï¼Œå¹¶æ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–æ··åˆä¸“å®¶(SMoE)æ¶æ„å› å…¶è®¡ç®—æ•ˆç‡è€Œè¢«å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­ã€‚ç„¶è€Œï¼Œå°½ç®¡æ¯ä¸ªtokenåªæ¿€æ´»å°‘æ•°ä¸“å®¶ï¼ŒSMoEä»ç„¶éœ€è¦åŠ è½½æ‰€æœ‰ä¸“å®¶å‚æ•°ï¼Œå¯¼è‡´é«˜å†…å­˜å ç”¨å’Œéƒ¨ç½²æŒ‘æˆ˜ã€‚å…ˆå‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¸“å®¶çº§åˆ«çš„å‰ªæå’Œåˆå¹¶ï¼Œè€Œå¿½ç•¥äº†ç¥ç»å…ƒçº§åˆ«çš„ç»“æ„ã€‚æˆ‘ä»¬æå‡ºäº†DERNï¼ˆDropping Experts, Recombining Neuronsï¼‰ï¼Œä¸€ä¸ªä»»åŠ¡æ— å…³ä¸”å…è®­ç»ƒçš„ä¸“å®¶å‰ªæå’Œé‡æ„æ¡†æ¶ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸“å®¶åœ¨ç¥ç»å…ƒçº§åˆ«ä¸Šç»å¸¸ä¸å¯¹é½ï¼Œå¹¶åŒ…å«è¯­ä¹‰å†²çªï¼Œè¿™å¯¹ç›´æ¥åˆå¹¶æå‡ºäº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒDERNåˆ†ä¸‰ä¸ªæ­¥éª¤å·¥ä½œï¼šé¦–å…ˆï¼Œå®ƒä½¿ç”¨è·¯ç”±ç»Ÿè®¡ä¿¡æ¯å‰ªæå†—ä½™ä¸“å®¶ï¼›ç„¶åï¼Œå®ƒå°†å®ƒä»¬åˆ†è§£ä¸ºç¥ç»å…ƒçº§åˆ«çš„ä¸“å®¶ç‰‡æ®µï¼Œå¹¶å°†æ¯ä¸ªç‰‡æ®µåˆ†é…ç»™å…¶æœ€å…¼å®¹çš„ä¿ç•™ä¸“å®¶ï¼›æœ€åï¼Œå®ƒåˆå¹¶æ¯ä¸ªä¿ç•™ä¸“å®¶å†…çš„ç‰‡æ®µï¼Œä»¥æ„å»ºç´§å‡‘çš„è¡¨ç¤ºã€‚åœ¨Mixtralã€Qwenå’ŒDeepSeek SMoEæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨50%çš„ä¸“å®¶ç¨€ç–æ€§ä¸‹ï¼ŒDERNåœ¨å¸¸è¯†æ¨ç†å’ŒMMLUåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†5%ä»¥ä¸Šçš„æ€§èƒ½ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚å®ƒè¿˜å¤§å¤§å‡å°‘äº†ä¸“å®¶æ•°é‡å’Œå†…å­˜ä½¿ç”¨ï¼Œä½¿SMoE LLMåœ¨å®è·µä¸­æ›´å®¹æ˜“éƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆSMoE LLMsï¼‰è™½ç„¶åœ¨è®¡ç®—ä¸Šé«˜æ•ˆï¼Œä½†ç”±äºéœ€è¦åŠ è½½æ‰€æœ‰ä¸“å®¶çš„å‚æ•°ï¼Œå› æ­¤å†…å­˜å ç”¨ä»ç„¶å¾ˆé«˜ï¼Œè¿™ç»™æ¨¡å‹çš„éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸“å®¶å±‚é¢çš„å‰ªæå’Œåˆå¹¶ï¼Œè€Œå¿½ç•¥äº†ç¥ç»å…ƒå±‚é¢çš„ç»“æ„ï¼Œå¯¼è‡´ä¼˜åŒ–æ•ˆæœå—é™ã€‚æ­¤å¤–ï¼Œç›´æ¥åˆå¹¶ä¸“å®¶å¯èƒ½ä¼šå› ä¸ºç¥ç»å…ƒçº§åˆ«çš„ä¸å¯¹é½å’Œè¯­ä¹‰å†²çªè€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDERNçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»†ç²’åº¦çš„ç¥ç»å…ƒçº§åˆ«çš„æ“ä½œæ¥å®ç°æ›´æœ‰æ•ˆçš„ä¸“å®¶å‰ªæå’Œé‡æ„ã€‚å®ƒé¦–å…ˆå‰ªæå†—ä½™çš„ä¸“å®¶ï¼Œç„¶åå°†å‰©ä½™çš„ä¸“å®¶åˆ†è§£æˆç¥ç»å…ƒçº§åˆ«çš„ç‰‡æ®µï¼Œå¹¶å°†è¿™äº›ç‰‡æ®µé‡æ–°åˆ†é…ç»™æœ€åˆé€‚çš„ä¿ç•™ä¸“å®¶ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒDERNèƒ½å¤Ÿé¿å…ç›´æ¥åˆå¹¶ä¸“å®¶å¸¦æ¥çš„è¯­ä¹‰å†²çªé—®é¢˜ï¼Œå¹¶æ„å»ºæ›´ç´§å‡‘çš„æ¨¡å‹è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDERNæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š1) **ä¸“å®¶å‰ªæ**ï¼šä½¿ç”¨è·¯ç”±ç»Ÿè®¡ä¿¡æ¯æ¥è¯†åˆ«å’Œå‰ªæå†—ä½™çš„ä¸“å®¶ã€‚2) **ç¥ç»å…ƒç‰‡æ®µåˆ†è§£ä¸åˆ†é…**ï¼šå°†å‰ªæçš„ä¸“å®¶åˆ†è§£ä¸ºç¥ç»å…ƒçº§åˆ«çš„ç‰‡æ®µï¼Œå¹¶å°†æ¯ä¸ªç‰‡æ®µåˆ†é…ç»™æœ€å…¼å®¹çš„ä¿ç•™ä¸“å®¶ã€‚å…¼å®¹æ€§é€šè¿‡æŸç§ç›¸ä¼¼åº¦åº¦é‡æ¥è¡¡é‡ã€‚3) **ç‰‡æ®µåˆå¹¶**ï¼šåœ¨æ¯ä¸ªä¿ç•™ä¸“å®¶å†…éƒ¨ï¼Œåˆå¹¶åˆ†é…ç»™å®ƒçš„ç¥ç»å…ƒç‰‡æ®µï¼Œä»¥æ„å»ºä¸€ä¸ªæ›´ç´§å‡‘çš„ä¸“å®¶è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šDERNçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç¥ç»å…ƒçº§åˆ«çš„ä¸“å®¶é‡æ„æ–¹æ³•ã€‚ä¸ä»¥å¾€çš„ä¸“å®¶å±‚é¢æ“ä½œä¸åŒï¼ŒDERNèƒ½å¤Ÿæ›´ç²¾ç»†åœ°æ§åˆ¶æ¨¡å‹çš„ç»“æ„ï¼Œå¹¶é¿å…äº†ç›´æ¥åˆå¹¶ä¸“å®¶å¯èƒ½å¸¦æ¥çš„è¯­ä¹‰å†²çªã€‚æ­¤å¤–ï¼ŒDERNæ˜¯ä¸€ä¸ªå…è®­ç»ƒçš„æ¡†æ¶ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ç›´æ¥åº”ç”¨äºç°æœ‰çš„SMoEæ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šDERNçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨è·¯ç”±ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œä¸“å®¶å‰ªæï¼Œä¾‹å¦‚å¯ä»¥åŸºäºä¸“å®¶è¢«æ¿€æ´»çš„é¢‘ç‡æ¥åˆ¤æ–­å…¶å†—ä½™ç¨‹åº¦ã€‚2) å®šä¹‰ç¥ç»å…ƒç‰‡æ®µä¹‹é—´çš„å…¼å®¹æ€§åº¦é‡ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–å…¶ä»–çš„ç¥ç»å…ƒæ¿€æ´»æ¨¡å¼ç›¸ä¼¼åº¦åº¦é‡ã€‚3) è®¾è®¡ç‰‡æ®µåˆå¹¶ç­–ç•¥ï¼Œä¾‹å¦‚å¯ä»¥ç®€å•åœ°å°†åˆ†é…ç»™åŒä¸€ä¸“å®¶çš„ç¥ç»å…ƒç‰‡æ®µè¿›è¡Œæ‹¼æ¥æˆ–åŠ æƒå¹³å‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DERNåœ¨Mixtralã€Qwenå’ŒDeepSeekç­‰SMoEæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨50%çš„ä¸“å®¶ç¨€ç–æ€§ä¸‹ï¼ŒDERNåœ¨å¸¸è¯†æ¨ç†å’ŒMMLUåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¶…è¿‡5%çš„æ€§èƒ½æå‡ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚æ­¤å¤–ï¼ŒDERNè¿˜æ˜¾è‘—å‡å°‘äº†ä¸“å®¶æ•°é‡å’Œå†…å­˜ä½¿ç”¨ï¼ŒéªŒè¯äº†å…¶åœ¨æ¨¡å‹å‹ç¼©å’Œæ€§èƒ½æå‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DERNæ¡†æ¶å¯åº”ç”¨äºå„ç§åŸºäºSMoEæ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„éƒ¨ç½²ç¯å¢ƒï¼Œå¦‚ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€‚é€šè¿‡é™ä½å†…å­˜å ç”¨å’Œæ¨¡å‹å¤§å°ï¼ŒDERNå¯ä»¥ä½¿è¿™äº›æ¨¡å‹æ›´å®¹æ˜“éƒ¨ç½²å’Œè¿è¡Œï¼Œä»è€ŒåŠ é€ŸLLMåœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠã€‚æ­¤å¤–ï¼ŒDERNè¿˜å¯ä»¥ä½œä¸ºä¸€ç§æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿçš„é€šç”¨æŠ€æœ¯ï¼Œåº”ç”¨äºå…¶ä»–ç±»å‹çš„ç¥ç»ç½‘ç»œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse Mixture-of-Experts (SMoE) architectures are widely used in large language models (LLMs) due to their computational efficiency. However, though only a few experts are activated for each token, SMoE still requires loading all expert parameters, leading to high memory usage and challenges in deployment. Previous work has tried to reduce the overhead by pruning and merging experts, but primarily focused on expert-level operations, leaving neuron-level structure underexplored. We propose DERN (Dropping Experts, Recombining Neurons), a task-agnostic and retraining-free framework for expert pruning and reconstruction. We observe that experts are often misaligned and contain semantic conflicts at the neuron level, which poses challenges for direct merging. To solve this, DERN works in three steps: it first prunes redundant experts using router statistics; then it decomposes them into neuron-level expert segments, assigning each segment to its most compatible retained expert; and finally, it merges segments within each retained expert to build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE models show that DERN improves performance by more than 5% on commonsense reasoning and MMLU benchmarks under 50% expert sparsity, without extra training. It also greatly reduces the number of experts and memory usage, making SMoE LLMs easier to deploy in practice.

