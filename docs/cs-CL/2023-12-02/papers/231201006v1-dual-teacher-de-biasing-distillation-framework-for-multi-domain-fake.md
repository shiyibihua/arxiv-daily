---
layout: default
title: Dual-Teacher De-biasing Distillation Framework for Multi-domain Fake News Detection
---

# Dual-Teacher De-biasing Distillation Framework for Multi-domain Fake News Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.01006" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.01006v1</a>
  <a href="https://arxiv.org/pdf/2312.01006.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.01006v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.01006v1', 'Dual-Teacher De-biasing Distillation Framework for Multi-domain Fake News Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiayang Li, Xuan Feng, Tianlong Gu, Liang Chang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2023-12-02

**å¤‡æ³¨**: ICDE 2024

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒæ•™å¸ˆè§£åè’¸é¦æ¡†æ¶DTDBDï¼Œç¼“è§£å¤šé¢†åŸŸå‡æ–°é—»æ£€æµ‹ä¸­çš„é¢†åŸŸåå·®é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å‡æ–°é—»æ£€æµ‹` `é¢†åŸŸåå·®` `çŸ¥è¯†è’¸é¦` `å¯¹æŠ—è®­ç»ƒ` `å¤šé¢†åŸŸå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å‡æ–°é—»æ£€æµ‹æ–¹æ³•ä¾§é‡äºæ•´ä½“æ€§èƒ½æå‡ï¼Œå¿½ç•¥äº†æ•°æ®ä¸å¹³è¡¡å¯¼è‡´çš„é¢†åŸŸåå·®é—®é¢˜ã€‚
2. DTDBDæ¡†æ¶é‡‡ç”¨åŒæ•™å¸ˆç»“æ„ï¼Œé€šè¿‡æ— åæ•™å¸ˆå’Œå¹²å‡€æ•™å¸ˆå…±åŒæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ï¼Œç¼“è§£é¢†åŸŸåå·®å¹¶ä¿æŒæ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDTDBDåœ¨åå·®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿è¯äº†è‰¯å¥½çš„æ£€æµ‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šé¢†åŸŸå‡æ–°é—»æ£€æµ‹æ—¨åœ¨è¯†åˆ«æ¥è‡ªä¸åŒé¢†åŸŸçš„å„ç§æ–°é—»çš„çœŸä¼ªï¼Œè¿™å·²å˜å¾—ç´§è¿«è€Œé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è‡´åŠ›äºæé«˜å‡æ–°é—»æ£€æµ‹çš„æ•´ä½“æ€§èƒ½ï¼Œå¿½ç•¥äº†ä¸å¹³è¡¡æ•°æ®å¯¼è‡´å¯¹ä¸åŒé¢†åŸŸçš„åŒºåˆ«å¯¹å¾…ï¼Œå³é¢†åŸŸåå·®é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒæ•™å¸ˆè§£åè’¸é¦æ¡†æ¶ï¼ˆDTDBDï¼‰æ¥å‡è½»ä¸åŒé¢†åŸŸä¹‹é—´çš„åå·®ã€‚éµå¾ªçŸ¥è¯†è’¸é¦æ–¹æ³•ï¼ŒDTDBDé‡‡ç”¨æ•™å¸ˆ-å­¦ç”Ÿç»“æ„ï¼Œå…¶ä¸­é¢„è®­ç»ƒçš„å¤§å‹æ•™å¸ˆæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ã€‚ç‰¹åˆ«åœ°ï¼ŒDTDBDç”±ä¸€ä¸ªæ— åæ•™å¸ˆå’Œä¸€ä¸ªå¹²å‡€æ•™å¸ˆç»„æˆï¼Œå®ƒä»¬å…±åŒæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹å‡è½»é¢†åŸŸåå·®å¹¶ä¿æŒæ€§èƒ½ã€‚å¯¹äºæ— åæ•™å¸ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯¹æŠ—æ€§è§£åè’¸é¦æŸå¤±ï¼Œä»¥æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ— åé¢†åŸŸçŸ¥è¯†ã€‚å¯¹äºå¹²å‡€æ•™å¸ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†é¢†åŸŸçŸ¥è¯†è’¸é¦æŸå¤±ï¼Œæœ‰æ•ˆåœ°æ¿€åŠ±å­¦ç”Ÿæ¨¡å‹ä¸“æ³¨äºè¡¨ç¤ºé¢†åŸŸç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŠ¨é‡çš„åŠ¨æ€è°ƒæ•´ç®—æ³•æ¥æƒè¡¡ä¸¤ä½æ•™å¸ˆçš„å½±å“ã€‚åœ¨ä¸­æ–‡å’Œè‹±æ–‡æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åå·®æŒ‡æ ‡æ–¹é¢å¤§å¤§ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶ä¿è¯äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šé¢†åŸŸå‡æ–°é—»æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œç”±äºå„é¢†åŸŸæ•°æ®é‡ä¸å¹³è¡¡ï¼Œæ¨¡å‹å®¹æ˜“äº§ç”Ÿé¢†åŸŸåå·®ï¼Œå¯¼è‡´åœ¨æŸäº›é¢†åŸŸè¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨æ•´ä½“æ€§èƒ½ï¼Œå¿½ç•¥äº†è¿™ç§åå·®å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨çŸ¥è¯†è’¸é¦ï¼Œé€šè¿‡ä¸¤ä¸ªæ•™å¸ˆæ¨¡å‹åˆ†åˆ«æä¾›æ— åå’Œå¹²å‡€çš„é¢†åŸŸçŸ¥è¯†ï¼ŒæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹å­¦ä¹ ã€‚æ— åæ•™å¸ˆè´Ÿè´£æ¶ˆé™¤é¢†åŸŸåå·®ï¼Œå¹²å‡€æ•™å¸ˆè´Ÿè´£ä¿æŒæ•´ä½“æ€§èƒ½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå­¦ç”Ÿæ¨¡å‹å¯ä»¥åŒæ—¶å­¦ä¹ åˆ°é¢†åŸŸæ— å…³çš„çŸ¥è¯†å’Œé¢†åŸŸç‰¹å®šçš„ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDTDBDæ¡†æ¶åŒ…å«ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹å’Œä¸¤ä¸ªæ•™å¸ˆæ¨¡å‹ï¼šæ— åæ•™å¸ˆå’Œå¹²å‡€æ•™å¸ˆã€‚é¦–å…ˆï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ã€‚ç„¶åï¼Œæ— åæ•™å¸ˆé€šè¿‡å¯¹æŠ—æ€§è§£åè’¸é¦æŸå¤±ï¼ŒæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ— åé¢†åŸŸçŸ¥è¯†ã€‚å¹²å‡€æ•™å¸ˆé€šè¿‡é¢†åŸŸçŸ¥è¯†è’¸é¦æŸå¤±ï¼ŒæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹å­¦ä¹ é¢†åŸŸç‰¹å¾ã€‚æœ€åï¼Œä½¿ç”¨åŸºäºåŠ¨é‡çš„åŠ¨æ€è°ƒæ•´ç®—æ³•ï¼Œå¹³è¡¡ä¸¤ä¸ªæ•™å¸ˆçš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæ ¸å¿ƒåˆ›æ–°åœ¨äºåŒæ•™å¸ˆçš„è§£åè’¸é¦æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥æ— åæ•™å¸ˆå’Œå¯¹æŠ—æ€§è§£åè’¸é¦æŸå¤±ï¼Œæ˜¾å¼åœ°å‡å°‘äº†æ¨¡å‹ä¸­çš„é¢†åŸŸåå·®ã€‚åŒæ—¶ï¼Œå¹²å‡€æ•™å¸ˆçš„å­˜åœ¨ä¿è¯äº†æ¨¡å‹åœ¨æ•´ä½“æ€§èƒ½ä¸Šçš„ç«äº‰åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå¯¹æŠ—æ€§è§£åè’¸é¦æŸå¤±çš„è®¾è®¡æ˜¯å…³é”®ã€‚è¯¥æŸå¤±å‡½æ•°é€šè¿‡å¯¹æŠ—è®­ç»ƒï¼Œä½¿å­¦ç”Ÿæ¨¡å‹å­¦ä¹ åˆ°çš„ç‰¹å¾å°½å¯èƒ½ä¸é¢†åŸŸæ— å…³ã€‚é¢†åŸŸçŸ¥è¯†è’¸é¦æŸå¤±åˆ™é¼“åŠ±å­¦ç”Ÿæ¨¡å‹å­¦ä¹ é¢†åŸŸç‰¹å®šçš„ç‰¹å¾è¡¨ç¤ºã€‚åŸºäºåŠ¨é‡çš„åŠ¨æ€è°ƒæ•´ç®—æ³•ç”¨äºå¹³è¡¡ä¸¤ä¸ªæ•™å¸ˆçš„å½±å“ï¼Œå…¶æƒé‡æ ¹æ®è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€§èƒ½åŠ¨æ€è°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸­æ–‡å’Œè‹±æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDTDBDæ¡†æ¶åœ¨åå·®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæ•°æ®é›†ä¸Šï¼ŒDTDBDçš„åå·®æŒ‡æ ‡é™ä½äº†XX%ï¼ŒåŒæ—¶ä¿æŒäº†ä¸æœ€ä½³åŸºçº¿æ–¹æ³•ç›¸å½“çš„æ•´ä½“æ€§èƒ½ã€‚è¿™è¡¨æ˜DTDBDèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£é¢†åŸŸåå·®ï¼Œæé«˜å‡æ–°é—»æ£€æµ‹çš„å…¬å¹³æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§åœ¨çº¿æ–°é—»å¹³å°ï¼Œä»¥æé«˜å‡æ–°é—»æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå…¬å¹³æ€§ã€‚é€šè¿‡å‡è½»é¢†åŸŸåå·®ï¼Œå¯ä»¥ç¡®ä¿ä¸åŒé¢†åŸŸçš„æ–°é—»å¾—åˆ°å…¬æ­£çš„è¯„ä¼°ï¼Œä»è€Œå‡å°‘è™šå‡ä¿¡æ¯ä¼ æ’­ï¼Œç»´æŠ¤ç¤¾ä¼šç¨³å®šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯æ¨å¹¿åˆ°å…¶ä»–å­˜åœ¨é¢†åŸŸåå·®çš„åˆ†ç±»ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-domain fake news detection aims to identify whether various news from different domains is real or fake and has become urgent and important. However, existing methods are dedicated to improving the overall performance of fake news detection, ignoring the fact that unbalanced data leads to disparate treatment for different domains, i.e., the domain bias problem. To solve this problem, we propose the Dual-Teacher De-biasing Distillation framework (DTDBD) to mitigate bias across different domains. Following the knowledge distillation methods, DTDBD adopts a teacher-student structure, where pre-trained large teachers instruct a student model. In particular, the DTDBD consists of an unbiased teacher and a clean teacher that jointly guide the student model in mitigating domain bias and maintaining performance. For the unbiased teacher, we introduce an adversarial de-biasing distillation loss to instruct the student model in learning unbiased domain knowledge. For the clean teacher, we design domain knowledge distillation loss, which effectively incentivizes the student model to focus on representing domain features while maintaining performance. Moreover, we present a momentum-based dynamic adjustment algorithm to trade off the effects of two teachers. Extensive experiments on Chinese and English datasets show that the proposed method substantially outperforms the state-of-the-art baseline methods in terms of bias metrics while guaranteeing competitive performance.

