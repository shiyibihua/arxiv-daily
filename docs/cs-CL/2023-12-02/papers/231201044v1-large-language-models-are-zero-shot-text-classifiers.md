---
layout: default
title: Large Language Models Are Zero-Shot Text Classifiers
---

# Large Language Models Are Zero-Shot Text Classifiers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.01044" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.01044v1</a>
  <a href="https://arxiv.org/pdf/2312.01044.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.01044v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.01044v1', 'Large Language Models Are Zero-Shot Text Classifiers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiqiang Wang, Yiran Pang, Yanbin Lin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2023-12-02

**å¤‡æ³¨**: 9 pages, 3 figures, 6 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**éªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™åœºæ™¯ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é›¶æ ·æœ¬å­¦ä¹ ` `æ–‡æœ¬åˆ†ç±»` `æ€ç»´é“¾æç¤º` `GPTæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ–‡æœ¬åˆ†ç±»é¢ä¸´è®¡ç®—æˆæœ¬é«˜ã€è€—æ—¶ä»¥åŠå¯¹æœªè§ç±»åˆ«æ³›åŒ–èƒ½åŠ›å¼±ç­‰æŒ‘æˆ˜ã€‚
2. åˆ©ç”¨æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰å’Œé›¶æ ·æœ¬å­¦ä¹ ï¼Œç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒLLMsè¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMsåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä½œä¸ºé›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»å™¨è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶é€‚åˆèµ„æºæœ‰é™çš„åœºæ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»è¿‡é‡æ–°è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å„ä¸ªå­é¢†åŸŸã€‚åœ¨NLPä¸­ï¼Œæ–‡æœ¬åˆ†ç±»é—®é¢˜å—åˆ°äº†ç›¸å½“å¤§çš„å…³æ³¨ï¼Œä½†ä»ç„¶é¢ä¸´ç€è®¡ç®—æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä»¥åŠå¯¹æœªè§ç±»åˆ«çš„é²æ£’æ€§ä¸è¶³ç­‰é™åˆ¶ã€‚éšç€æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰çš„æå‡ºï¼ŒLLMså¯ä»¥ä½¿ç”¨é›¶æ ·æœ¬å­¦ä¹ ï¼ˆZSLï¼‰ä»¥åŠé€æ­¥æ¨ç†æç¤ºæ¥å®ç°ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„é—®ç­”å½¢å¼ã€‚é›¶æ ·æœ¬LLMsåœ¨æ–‡æœ¬åˆ†ç±»é—®é¢˜ä¸­å¯ä»¥é€šè¿‡ç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥é¢„æµ‹å·²è§å’Œæœªè§ç±»åˆ«ï¼Œä»è€Œç¼“è§£è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸»è¦éªŒè¯äº†GPTæ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸“æ³¨äºæœ‰æ•ˆåœ°åˆ©ç”¨æç¤ºç­–ç•¥æ¥é€‚åº”å„ç§æ–‡æœ¬åˆ†ç±»åœºæ™¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†é›¶æ ·æœ¬LLMsçš„æ€§èƒ½ä¸å…¶ä»–æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•ï¼ˆåŒ…æ‹¬ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ã€æ·±åº¦å­¦ä¹ æ–¹æ³•å’ŒZSLæ–¹æ³•ï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åˆ†æçš„å››ä¸ªæ•°æ®é›†ä¸­ï¼ŒLLMsçš„æ€§èƒ½çªæ˜¾äº†å®ƒä»¬ä½œä¸ºé›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»å™¨çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§èƒ½åŠ›å¯¹äºå¯èƒ½æ²¡æœ‰å¹¿æ³›æ–‡æœ¬åˆ†ç±»çŸ¥è¯†çš„å°ä¼ä¸šæˆ–å›¢é˜Ÿå°¤å…¶æœ‰åˆ©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä¼ ç»Ÿæ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ï¼Œä¸”å¯¹æœªè§ç±»åˆ«æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯¹äºå°ä¼ä¸šæˆ–å›¢é˜Ÿè€Œè¨€ï¼Œè·å–å’Œå¤„ç†è¿™äº›æ•°æ®çš„æˆæœ¬å¾ˆé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§zero-shotèƒ½åŠ›ï¼Œé€šè¿‡æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰å¼•å¯¼æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œä»è€Œå®ç°æ— éœ€é¢å¤–è®­ç»ƒæ•°æ®çš„æ–‡æœ¬åˆ†ç±»ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬å’Œæ—¶é—´æ¶ˆè€—ï¼Œå¹¶æé«˜å¯¹æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦ä¾èµ–äºé¢„è®­ç»ƒçš„GPTæ¨¡å‹ã€‚é¦–å…ˆï¼Œè®¾è®¡åˆé€‚çš„æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ï¼Œå¼•å¯¼æ¨¡å‹é€æ­¥æ¨ç†æ–‡æœ¬çš„ç±»åˆ«ã€‚ç„¶åï¼Œå°†æ–‡æœ¬å’Œæç¤ºè¾“å…¥åˆ°GPTæ¨¡å‹ä¸­ï¼Œæ¨¡å‹ç”Ÿæˆæ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆçš„ç±»åˆ«é¢„æµ‹ã€‚æœ€åï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„åˆ†ç±»æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå°†æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ä¸é›¶æ ·æœ¬LLMsç›¸ç»“åˆï¼Œç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥é—®ç­”å½¢å¼ä¸åŒï¼ŒCoTæç¤ºå…è®¸æ¨¡å‹é€æ­¥æ¨ç†ï¼Œä»è€Œæé«˜åˆ†ç±»çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ— éœ€é’ˆå¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œé™ä½äº†æ•°æ®æ ‡æ³¨å’Œæ¨¡å‹è®­ç»ƒçš„æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åœ¨äºæç¤ºç­–ç•¥çš„è®¾è®¡ã€‚ä¸åŒçš„æç¤ºç­–ç•¥ä¼šæ˜¾è‘—å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚è®ºæ–‡æ¢ç´¢äº†å¤šç§æç¤ºç­–ç•¥ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç ”ç©¶äº†ä¸åŒè§„æ¨¡çš„GPTæ¨¡å‹å¯¹åˆ†ç±»æ€§èƒ½çš„å½±å“ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„æ²¿ç”¨äº†GPTæ¨¡å‹çš„é»˜è®¤è®¾ç½®ï¼Œæ²¡æœ‰è¿›è¡Œç‰¹åˆ«çš„ä¿®æ”¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å››ä¸ªæ•°æ®é›†ä¸­çš„ä¸‰ä¸ªä¸Šè¡¨ç°å‡ºä½œä¸ºé›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»å™¨çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•åœ¨æŸäº›æ•°æ®é›†ä¸Šç”šè‡³å¯ä»¥ä¸ç»è¿‡ä¸“é—¨è®­ç»ƒçš„ä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸åª²ç¾ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é‡è¾ƒå°çš„æƒ…å†µä¸‹ï¼Œä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ã€‚è¿™è¡¨æ˜LLMså…·æœ‰å¼ºå¤§çš„zero-shotå­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§æ–‡æœ¬åˆ†ç±»åœºæ™¯ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±»ã€åƒåœ¾é‚®ä»¶æ£€æµ‹ç­‰ã€‚å°¤å…¶é€‚ç”¨äºèµ„æºæœ‰é™çš„å°ä¼ä¸šæˆ–å›¢é˜Ÿï¼Œä»–ä»¬å¯ä»¥åˆ©ç”¨é¢„è®­ç»ƒçš„LLMså¿«é€Ÿæ„å»ºæ–‡æœ¬åˆ†ç±»ç³»ç»Ÿï¼Œè€Œæ— éœ€æŠ•å…¥å¤§é‡èµ„é‡‘å’Œæ—¶é—´è¿›è¡Œæ•°æ®æ ‡æ³¨å’Œæ¨¡å‹è®­ç»ƒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrained large language models (LLMs) have become extensively used across various sub-disciplines of natural language processing (NLP). In NLP, text classification problems have garnered considerable focus, but still faced with some limitations related to expensive computational cost, time consumption, and robust performance to unseen classes. With the proposal of chain of thought prompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with the step by step reasoning prompts, instead of conventional question and answer formats. The zero-shot LLMs in the text classification problems can alleviate these limitations by directly utilizing pretrained models to predict both seen and unseen classes. Our research primarily validates the capability of GPT models in text classification. We focus on effectively utilizing prompt strategies to various text classification scenarios. Besides, we compare the performance of zero shot LLMs with other state of the art text classification methods, including traditional machine learning methods, deep learning methods, and ZSL methods. Experimental results demonstrate that the performance of LLMs underscores their effectiveness as zero-shot text classifiers in three of the four datasets analyzed. The proficiency is especially advantageous for small businesses or teams that may not have extensive knowledge in text classification.

