---
layout: default
title: Voice Activity Projection Model with Multimodal Encoders
---

# Voice Activity Projection Model with Multimodal Encoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03980" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.03980v1</a>
  <a href="https://arxiv.org/pdf/2506.03980.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03980v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03980v1', 'Voice Activity Projection Model with Multimodal Encoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Takeshi Saga, Catherine Pelachaud

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/sagatake/VAPwithAudioFaceEncoders)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€ç¼–ç å™¨çš„è¯­éŸ³æ´»åŠ¨æŠ•å½±æ¨¡å‹ä»¥æ”¹å–„äººæœºäº¤äº’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³æ´»åŠ¨æŠ•å½±` `å¤šæ¨¡æ€ç¼–ç å™¨` `äººæœºäº¤äº’` `è½®æµå‘è¨€ç®¡ç†` `æƒ…æ„Ÿè¯†åˆ«` `æ™ºèƒ½åŠ©æ‰‹` `ç¤¾äº¤æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è½®æµå‘è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„ç¤¾äº¤äº’åŠ¨æ—¶å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ä¿¡æ¯çš„æ•´åˆä¸Šã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆé¢„è®­ç»ƒéŸ³é¢‘å’Œé¢éƒ¨ç¼–ç å™¨çš„å¤šæ¨¡æ€VAPæ¨¡å‹ï¼Œä»¥æ•æ‰æ›´ç»†è…»çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨è½®æµå‘è¨€é¢„æµ‹æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œéƒ¨åˆ†æƒ…å†µä¸‹è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è½®æµå‘è¨€ç®¡ç†å¯¹ä»»ä½•ç¤¾äº¤äº’åŠ¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºç¤¾äº¤èƒŒæ™¯çš„å¤æ‚æ€§åŠå…¶å¤šæ¨¡æ€ç‰¹æ€§ï¼Œå»ºæ¨¡äººæœºäº¤äº’ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸åŸºäºé™é»˜æŒç»­æ—¶é—´çš„ä¼ ç»Ÿç³»ç»Ÿä¸åŒï¼Œç°æœ‰çš„è¯­éŸ³æ´»åŠ¨æŠ•å½±ï¼ˆVAPï¼‰æ¨¡å‹æˆåŠŸåˆ©ç”¨ç»Ÿä¸€çš„è½®æµå‘è¨€è¡Œä¸ºè¡¨ç¤ºä½œä¸ºé¢„æµ‹ç›®æ ‡ï¼Œä»è€Œæé«˜äº†é¢„æµ‹æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºçš„å¤šæ¨¡æ€VAPæ¨¡å‹ï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„éŸ³é¢‘å’Œé¢éƒ¨ç¼–ç å™¨ï¼Œä»¥æ•æ‰ç»†å¾®è¡¨æƒ…ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è½®æµå‘è¨€æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚æ‰€æœ‰æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨https://github.com/sagatake/VAPwithAudioFaceEncodersè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³äººæœºäº¤äº’ä¸­çš„è½®æµå‘è¨€ç®¡ç†é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€ä¿¡æ¯èåˆå’Œç»†å¾®è¡¨æƒ…æ•æ‰æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ¨¡å‹é€šè¿‡å¼•å…¥é¢„è®­ç»ƒçš„éŸ³é¢‘å’Œé¢éƒ¨ç¼–ç å™¨ï¼Œå¢å¼ºäº†å¯¹ç”¨æˆ·æƒ…æ„Ÿå’Œæ„å›¾çš„ç†è§£ï¼Œä»è€Œæé«˜äº†è½®æµå‘è¨€çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ¨¡å‹æ•´ä½“æ¶æ„åŒ…æ‹¬éŸ³é¢‘ç¼–ç å™¨å’Œé¢éƒ¨ç¼–ç å™¨ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼Œåˆ†åˆ«æå–éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œéšåå°†è¿™äº›ä¿¡æ¯èåˆç”¨äºè½®æµå‘è¨€çš„é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡å¤šæ¨¡æ€ç¼–ç å™¨çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†å¯¹å¤æ‚ç¤¾äº¤åœºæ™¯ä¸­ç»†å¾®æƒ…æ„Ÿå˜åŒ–çš„æ•æ‰èƒ½åŠ›ï¼Œä¸ä¼ ç»ŸåŸºäºé™é»˜æ—¶é—´çš„æ¨¡å‹ç›¸æ¯”ï¼Œå…·æœ‰æœ¬è´¨ä¸Šçš„æ€§èƒ½æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆæ•ˆæœï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šåˆ™è€ƒè™‘äº†éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯çš„åŒæ­¥å¤„ç†ï¼Œç¡®ä¿äº†ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¨¡å‹åœ¨å¤šä¸ªè½®æµå‘è¨€é¢„æµ‹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œè¯æ˜äº†å¤šæ¨¡æ€ä¿¡æ¯èåˆçš„æœ‰æ•ˆæ€§å’Œé‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€ç¤¾äº¤æœºå™¨äººå’Œè™šæ‹Ÿç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæµç•…æ€§ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œè¯¥æ¨¡å‹æœ‰æœ›åœ¨æ›´å¤šå¤æ‚ç¤¾äº¤ç¯å¢ƒä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Turn-taking management is crucial for any social interaction. Still, it is challenging to model human-machine interaction due to the complexity of the social context and its multimodal nature. Unlike conventional systems based on silence duration, previous existing voice activity projection (VAP) models successfully utilized a unified representation of turn-taking behaviors as prediction targets, which improved turn-taking prediction performance. Recently, a multimodal VAP model outperformed the previous state-of-the-art model by a significant margin. In this paper, we propose a multimodal model enhanced with pre-trained audio and face encoders to improve performance by capturing subtle expressions. Our model performed competitively, and in some cases, even better than state-of-the-art models on turn-taking metrics. All the source codes and pretrained models are available at https://github.com/sagatake/VAPwithAudioFaceEncoders.

