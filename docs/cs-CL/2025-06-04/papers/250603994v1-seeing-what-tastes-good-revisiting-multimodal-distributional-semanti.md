---
layout: default
title: Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era
---

# Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03994" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.03994v1</a>
  <a href="https://arxiv.org/pdf/2506.03994.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03994v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03994v1', 'Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dan Oneata, Desmond Elliott, Stella Frank

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**å¤‡æ³¨**: ACL Findings 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨å¤šæ¨¡æ€åˆ†å¸ƒè¯­ä¹‰åœ¨äº¿å‚æ•°æ—¶ä»£çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `åˆ†å¸ƒè¯­ä¹‰` `å›¾åƒç¼–ç å™¨` `è¯­è¨€æ¨¡å‹` `å±æ€§é¢„æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºç¡€æ¨¡å‹åœ¨ç†è§£å…·ä½“ç‰©ä½“æ¦‚å¿µçš„è¯­ä¹‰ç‰¹å¾æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹éè§†è§‰å±æ€§çš„ç†è§£ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ¢æµ‹ä»»åŠ¡è¯„ä¼°å¤šæ¨¡æ€å›¾åƒç¼–ç å™¨ä¸è¯­è¨€æ¨¡å‹åœ¨ç‰©ä½“å±æ€§è®¤çŸ¥ä¸Šçš„è¡¨ç°ï¼Œä»¥æ¢ç´¢æ¨¡æ€é—´çš„äº’è¡¥æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šæ¨¡æ€å›¾åƒç¼–ç å™¨åœ¨æ€§èƒ½ä¸Šç•¥ä¼˜äºè¯­è¨€æ¨¡å‹ï¼Œè€Œä»…å›¾åƒç¼–ç å™¨åœ¨æŸäº›æ–¹é¢è¡¨ç°ç›¸å½“ï¼Œæ­ç¤ºäº†å•æ¨¡æ€å­¦ä¹ çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»çš„å­¦ä¹ å’Œæ¦‚å¿µè¡¨å¾æ˜¯åŸºäºæ„ŸçŸ¥è¿åŠ¨ç»éªŒçš„ï¼Œè¿™ä¸å½“å‰æœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹å½¢æˆå¯¹æ¯”ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤§è§„æ¨¡æ¨¡å‹åœ¨è¡¨ç¤ºå…·ä½“ç‰©ä½“æ¦‚å¿µçš„è¯­ä¹‰ç‰¹å¾è§„èŒƒæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚â€œç«ç‘°æ˜¯çº¢è‰²çš„ï¼Œé—»èµ·æ¥é¦™ç”œï¼Œæ˜¯ä¸€ç§èŠ±â€ã€‚æˆ‘ä»¬ä½¿ç”¨æ¢æµ‹ä»»åŠ¡æµ‹è¯•è¿™äº›æ¨¡å‹å¯¹ç‰©ä½“å±æ€§çš„è®¤çŸ¥ã€‚è¯„ä¼°äº†ä»…è®­ç»ƒäºå›¾åƒæ•°æ®çš„å›¾åƒç¼–ç å™¨ã€å¤šæ¨¡æ€è®­ç»ƒçš„å›¾åƒç¼–ç å™¨å’Œä»…è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹ç»å…¸McRaeè§„èŒƒçš„æ‰©å±•ç‰ˆæœ¬åŠBinderå±æ€§è¯„åˆ†æ•°æ®é›†çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€å›¾åƒç¼–ç å™¨ç•¥ä¼˜äºä»…è¯­è¨€çš„æ–¹æ³•ï¼Œè€Œä»…å›¾åƒç¼–ç å™¨åœ¨éè§†è§‰å±æ€§ä¸Šä¸è¯­è¨€æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚è¿™äº›ç»“æœä¸ºå•æ¨¡æ€å­¦ä¹ çš„æ½œåŠ›åŠæ¨¡æ€é—´çš„äº’è¡¥æ€§æä¾›äº†æ–°è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ¨¡å‹åœ¨ç†è§£å…·ä½“ç‰©ä½“æ¦‚å¿µçš„è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹éè§†è§‰å±æ€§çš„è®¤çŸ¥èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå•ä¸€æ¨¡æ€ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯çš„äº’è¡¥æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ¢æµ‹ä»»åŠ¡è¯„ä¼°ä¸åŒç±»å‹çš„ç¼–ç å™¨ï¼ŒåŒ…æ‹¬ä»…å›¾åƒã€ä»…è¯­è¨€å’Œå¤šæ¨¡æ€è®­ç»ƒçš„å›¾åƒç¼–ç å™¨ï¼Œæ¥æµ‹è¯•å®ƒä»¬å¯¹ç‰©ä½“å±æ€§çš„è®¤çŸ¥èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æ­ç¤ºæ¨¡æ€é—´çš„äº’è¡¥æ€§åŠå•æ¨¡æ€å­¦ä¹ çš„æ½œåŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¤šç§ç¼–ç å™¨è¿›è¡Œå¯¹æ¯”å®éªŒï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬å›¾åƒç¼–ç å™¨ã€è¯­è¨€æ¨¡å‹å’Œæ¢æµ‹ä»»åŠ¡çš„è®¾è®¡ã€‚å®éªŒé€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹åœ¨McRaeè§„èŒƒå’ŒBinderæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œè¯„ä¼°å…¶å¯¹ç‰©ä½“å±æ€§çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡å¤šæ¨¡æ€ä¸å•æ¨¡æ€æ¨¡å‹çš„å¯¹æ¯”ï¼Œæ­ç¤ºäº†å›¾åƒç¼–ç å™¨åœ¨ç†è§£éè§†è§‰å±æ€§æ–¹é¢çš„æ½œåŠ›ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿå¯¹è¯­è¨€æ¨¡å‹çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†æ‰©å±•çš„McRaeè§„èŒƒå’ŒBinderæ•°æ®é›†ä½œä¸ºè¯„ä¼°æ ‡å‡†ï¼Œæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨äº†æ ‡å‡†çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œç¡®ä¿äº†ç»“æœçš„å¯é æ€§ä¸å¯æ¯”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šæ¨¡æ€å›¾åƒç¼–ç å™¨åœ¨å±æ€§é¢„æµ‹ä»»åŠ¡ä¸­ç•¥ä¼˜äºè¯­è¨€æ¨¡å‹ï¼Œä¸”ä»…å›¾åƒç¼–ç å™¨åœ¨æŸäº›éè§†è§‰å±æ€§ä¸Šè¡¨ç°å‡ºä¸è¯­è¨€æ¨¡å‹ç›¸å½“çš„èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œå•æ¨¡æ€å­¦ä¹ åœ¨ç†è§£å…·ä½“ç‰©ä½“æ¦‚å¿µæ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ï¼Œæ‹“å±•äº†å¯¹å¤šæ¨¡æ€å­¦ä¹ çš„ç†è§£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†åŠäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£å¤šæ¨¡æ€å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œæœªæ¥å¯ä»¥åœ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººç­‰é¢†åŸŸå®ç°æ›´ä¸ºç²¾å‡†çš„æ„ŸçŸ¥ä¸å†³ç­–èƒ½åŠ›ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human learning and conceptual representation is grounded in sensorimotor experience, in contrast to state-of-the-art foundation models. In this paper, we investigate how well such large-scale models, trained on vast quantities of data, represent the semantic feature norms of concrete object concepts, e.g. a ROSE is red, smells sweet, and is a flower. More specifically, we use probing tasks to test which properties of objects these models are aware of. We evaluate image encoders trained on image data alone, as well as multimodally-trained image encoders and language-only models, on predicting an extended denser version of the classic McRae norms and the newer Binder dataset of attribute ratings. We find that multimodal image encoders slightly outperform language-only approaches, and that image-only encoders perform comparably to the language models, even on non-visual attributes that are classified as "encyclopedic" or "function". These results offer new insights into what can be learned from pure unimodal learning, and the complementarity of the modalities.

