---
layout: default
title: Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models
---

# Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04182" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04182v1</a>
  <a href="https://arxiv.org/pdf/2506.04182.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04182v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04182v1', 'Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruiqi Zhang, Changyi Xiao, Yixin Cao

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSwitchCoTä»¥è§£å†³é•¿çŸ­CoTç­–ç•¥é€‰æ‹©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿é“¾å¼æ€ç»´` `æ¨ç†æ¨¡å‹` `åŠ¨æ€é€‰æ‹©` `è®¡ç®—æ•ˆç‡` `èµ„æºçº¦æŸ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é•¿CoTç­–ç•¥åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶é«˜ä»¤ç‰Œæ¶ˆè€—ä½¿å¾—åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹éš¾ä»¥åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºSwitchCoTæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚å’Œèµ„æºæƒ…å†µè‡ªåŠ¨é€‰æ‹©é•¿çŸ­CoTç­–ç•¥ï¼Œå¹³è¡¡æ¨ç†å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSwitchCoTåœ¨æœ‰é™ä»¤ç‰Œé¢„ç®—ä¸‹çš„æ€§èƒ½ä¸å•ç‹¬ä½¿ç”¨é•¿æˆ–çŸ­CoTç›¸å½“ï¼Œç”šè‡³æ›´ä¼˜ï¼ŒåŒæ—¶æ¨ç†æˆæœ¬é™ä½äº†50%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹æ¨ç†æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œé•¿é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ä»£ä»·æ˜¯æ˜¾è‘—å¢åŠ çš„ä»¤ç‰Œæ¶ˆè€—ã€‚æœ¬æ–‡é€šè¿‡å…¨é¢çš„å®è¯åˆ†ææ¯”è¾ƒäº†é•¿çŸ­CoTç­–ç•¥ï¼Œå‘ç°é•¿CoTåœ¨èµ„æºå……è¶³æ—¶èƒ½æé«˜æ€§èƒ½ï¼Œä½†ç›¸è¾ƒäºå…¶é«˜ä»¤ç‰Œæ¶ˆè€—ï¼Œå…¶æ”¶ç›Šå¾€å¾€æ˜¯è¾¹é™…çš„ã€‚çŸ­CoTåœ¨èµ„æºç´§å¼ æ—¶æ›´ä¸ºæœ‰æ•ˆã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SwitchCoTï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ ¹æ®ä»»åŠ¡ä¸Šä¸‹æ–‡å’Œèµ„æºå¯ç”¨æ€§åŠ¨æ€é€‰æ‹©åˆé€‚çš„CoTç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSwitchCoTåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå°†æ¨ç†æˆæœ¬é™ä½å¤šè¾¾50%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ä¸åŒèµ„æºçº¦æŸä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆé€‰æ‹©é•¿çŸ­CoTç­–ç•¥çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨èµ„æºæœ‰é™æ—¶å¾€å¾€æ— æ³•å……åˆ†å‘æŒ¥å…¶æ½œåŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSwitchCoTæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯åŠ¨æ€é€‰æ‹©é€‚åˆçš„CoTç­–ç•¥ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡ä¸Šä¸‹æ–‡å’Œèµ„æºé™åˆ¶ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSwitchCoTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡åˆ†ææ¨¡å—ã€ç­–ç•¥é€‰æ‹©æ¨¡å—å’Œæ¨ç†æ‰§è¡Œæ¨¡å—ã€‚ä»»åŠ¡åˆ†ææ¨¡å—è¯„ä¼°å½“å‰ä»»åŠ¡çš„å¤æ‚æ€§å’Œèµ„æºå¯ç”¨æ€§ï¼Œç­–ç•¥é€‰æ‹©æ¨¡å—æ ¹æ®åˆ†æç»“æœé€‰æ‹©åˆé€‚çš„CoTç­–ç•¥ï¼Œæ¨ç†æ‰§è¡Œæ¨¡å—åˆ™è´Ÿè´£å®é™…çš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šSwitchCoTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€é€‰æ‹©æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨é•¿çŸ­CoTä¹‹é—´è¿›è¡Œæ™ºèƒ½åˆ‡æ¢ï¼Œè€Œä¸æ˜¯å›ºå®šä½¿ç”¨æŸä¸€ç§ç­–ç•¥ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹éƒ½èƒ½ä¿æŒé«˜æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SwitchCoTä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ä»¤ç‰Œé¢„ç®—çš„è®¾å®šå’Œä»»åŠ¡å¤æ‚åº¦çš„è¯„ä¼°æ ‡å‡†ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œè€ƒè™‘äº†æ¨ç†å‡†ç¡®æ€§ä¸è®¡ç®—æˆæœ¬çš„å¹³è¡¡ï¼Œç¡®ä¿åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä»èƒ½å–å¾—è‰¯å¥½æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSwitchCoTåœ¨æœ‰é™ä»¤ç‰Œé¢„ç®—ä¸‹çš„æ¨ç†æ€§èƒ½ä¸å•ç‹¬ä½¿ç”¨é•¿æˆ–çŸ­CoTç­–ç•¥ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº†è¿™ä¸¤è€…ã€‚åŒæ—¶ï¼ŒSwitchCoTèƒ½å¤Ÿå°†æ¨ç†æˆæœ¬é™ä½å¤šè¾¾50%ï¼Œå±•ç°å‡ºè‰¯å¥½çš„è®¡ç®—æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SwitchCoTæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œé€‚ç”¨äºéœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”å’Œè‡ªåŠ¨åŒ–å†³ç­–ç­‰é¢†åŸŸã€‚å…¶åŠ¨æ€é€‰æ‹©æœºåˆ¶èƒ½å¤Ÿæ ¹æ®å®é™…èµ„æºæƒ…å†µä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæå‡ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šå¤æ‚ä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨æ¨ç†æ¨¡å‹çš„å®ç”¨åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid advancement of large reasoning models, long Chain-of-Thought (CoT) prompting has demonstrated strong performance on complex tasks. However, this often comes with a significant increase in token usage. In this paper, we conduct a comprehensive empirical analysis comparing long and short CoT strategies. Our findings reveal that while long CoT can lead to performance improvements, its benefits are often marginal relative to its significantly higher token consumption. Specifically, long CoT tends to outperform when ample generation budgets are available, whereas short CoT is more effective under tighter budget constraints. These insights underscore the need for a dynamic approach that selects the proper CoT strategy based on task context and resource availability. To address this, we propose SwitchCoT, an automatic framework that adaptively chooses between long and short CoT strategies to balance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is designed to be budget-aware, making it broadly applicable across scenarios with varying resource constraints. Experimental results demonstrate that SwitchCoT can reduce inference costs by up to 50% while maintaining high accuracy. Notably, under limited token budgets, it achieves performance comparable to, or even exceeding, that of using either long or short CoT alone.

