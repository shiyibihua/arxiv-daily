---
layout: default
title: Rectified Sparse Attention
---

# Rectified Sparse Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04108" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04108v2</a>
  <a href="https://arxiv.org/pdf/2506.04108.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04108v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04108v2', 'Rectified Sparse Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yutao Sun, Tianzhu Ye, Li Dong, Yuqing Xia, Jian Chen, Yizhao Gao, Shijie Cao, Jianyong Wang, Furu Wei

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04 (æ›´æ–°: 2025-06-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRectified Sparse Attentionä»¥è§£å†³é•¿åºåˆ—ç”Ÿæˆæ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿åºåˆ—ç”Ÿæˆ` `ç¨€ç–æ³¨æ„åŠ›` `KVç¼“å­˜` `ç”Ÿæˆæ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•ˆç‡æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¨€ç–è§£ç æ–¹æ³•åœ¨é•¿åºåˆ—ç”Ÿæˆä¸­æ•ˆç‡æå‡æœ‰é™ï¼Œä¸”KVç¼“å­˜å¯¹é½ä¸å‡†ç¡®å¯¼è‡´ç”Ÿæˆè´¨é‡ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºçš„ReSAæ–¹æ³•é€šè¿‡ç»“åˆå—ç¨€ç–æ³¨æ„åŠ›ä¸å‘¨æœŸæ€§å¯†é›†æ ¡æ­£ï¼Œæœ‰æ•ˆå‡å°‘äº†è¯¯å·®ç´¯ç§¯ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReSAåœ¨å¤šä¸ªä»»åŠ¡ä¸­å®ç°äº†æ¥è¿‘æ— æŸçš„ç”Ÿæˆè´¨é‡ï¼Œå¹¶åœ¨256Kåºåˆ—é•¿åº¦ä¸‹æé«˜äº†2.42å€çš„é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜æ•ˆçš„é•¿åºåˆ—ç”Ÿæˆæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚å°½ç®¡è¿‘æœŸçš„ç¨€ç–è§£ç æ–¹æ³•æé«˜äº†æ•ˆç‡ï¼Œä½†å®ƒä»¬å­˜åœ¨KVç¼“å­˜å¯¹é½ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œå¯¼è‡´è¿‘ä¼¼è¯¯å·®ç´¯ç§¯å¹¶é™ä½ç”Ÿæˆè´¨é‡ã€‚æœ¬æ–‡æå‡ºäº†Rectified Sparse Attentionï¼ˆReSAï¼‰ï¼Œä¸€ç§å°†å—ç¨€ç–æ³¨æ„åŠ›ä¸å‘¨æœŸæ€§å¯†é›†æ ¡æ­£ç›¸ç»“åˆçš„ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ã€‚é€šè¿‡åœ¨å›ºå®šé—´éš”å†…ä½¿ç”¨å¯†é›†å‰å‘ä¼ é€’åˆ·æ–°KVç¼“å­˜ï¼ŒReSAé™åˆ¶äº†è¯¯å·®ç´¯ç§¯å¹¶ä¿æŒä¸é¢„è®­ç»ƒåˆ†å¸ƒçš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReSAåœ¨æ•°å­¦æ¨ç†ã€è¯­è¨€å»ºæ¨¡å’Œæ£€ç´¢ä»»åŠ¡ä¸­å®ç°äº†æ¥è¿‘æ— æŸçš„ç”Ÿæˆè´¨é‡ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ•ˆç‡ã€‚ç‰¹åˆ«æ˜¯åœ¨256Kåºåˆ—é•¿åº¦ä¸‹ï¼ŒReSAå®ç°äº†é«˜è¾¾2.42å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œæˆä¸ºå¯æ‰©å±•é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿åºåˆ—ç”Ÿæˆä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰ç¨€ç–è§£ç æ–¹æ³•ç”±äºKVç¼“å­˜å¯¹é½ä¸å‡†ç¡®ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡ä¸‹é™å’Œè¯¯å·®ç´¯ç§¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReSAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å—ç¨€ç–æ³¨æ„åŠ›ä¸å‘¨æœŸæ€§å¯†é›†æ ¡æ­£ç»“åˆï¼Œé€šè¿‡å®šæœŸåˆ·æ–°KVç¼“å­˜æ¥é™åˆ¶è¯¯å·®çš„ç´¯ç§¯ï¼Œä»è€Œä¿æŒç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReSAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå—ç¨€ç–æ³¨æ„åŠ›æ¨¡å—å’Œå¯†é›†æ ¡æ­£æ¨¡å—ã€‚å—ç¨€ç–æ³¨æ„åŠ›è´Ÿè´£é«˜æ•ˆå¤„ç†é•¿åºåˆ—ï¼Œè€Œå¯†é›†æ ¡æ­£æ¨¡å—åˆ™åœ¨å›ºå®šæ—¶é—´é—´éš”å†…è¿›è¡ŒKVç¼“å­˜çš„åˆ·æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šReSAçš„åˆ›æ–°åœ¨äºå…¶å‘¨æœŸæ€§å¯†é›†æ ¡æ­£æœºåˆ¶ï¼Œè¿™ä¸€è®¾è®¡æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿç¨€ç–è§£ç æ–¹æ³•ä¸­çš„KVç¼“å­˜å¯¹é½é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒReSAé‡‡ç”¨äº†é€‚å½“çš„å—å¤§å°å’Œæ ¡æ­£é¢‘ç‡ï¼Œä»¥å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šä¹Ÿè€ƒè™‘äº†ç”Ÿæˆè´¨é‡ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒReSAåœ¨å¤šä¸ªä»»åŠ¡ä¸­å®ç°äº†æ¥è¿‘æ— æŸçš„ç”Ÿæˆè´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨256Kåºåˆ—é•¿åº¦ä¸‹ï¼ŒReSAå®ç°äº†é«˜è¾¾2.42å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é•¿æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿä»¥åŠä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜é•¿åºåˆ—ç”Ÿæˆçš„æ•ˆç‡ï¼ŒReSAå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†å¤§è§„æ¨¡æ•°æ®çš„åœºæ™¯ä¸­ã€‚æœªæ¥ï¼ŒReSAå¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šé«˜æ•ˆç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42$\times$ end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM.

