---
layout: default
title: Aligning Large Language Models with Implicit Preferences from User-Generated Content
---

# Aligning Large Language Models with Implicit Preferences from User-Generated Content

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04463" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04463v1</a>
  <a href="https://arxiv.org/pdf/2506.04463.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04463v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04463v1', 'Aligning Large Language Models with Implicit Preferences from User-Generated Content')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaoxuan Tan, Zheng Li, Tianyi Liu, Haodong Wang, Hyokun Yun, Ming Zeng, Pei Chen, Zhihan Zhang, Yifan Gao, Ruijie Wang, Priyanka Nigam, Bing Yin, Meng Jiang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**å¤‡æ³¨**: Accepted to ACL 2025 Main Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPUGCæ¡†æ¶ä»¥åˆ©ç”¨ç”¨æˆ·ç”Ÿæˆå†…å®¹éšå¼åå¥½æ”¹å–„LLMå¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”¨æˆ·ç”Ÿæˆå†…å®¹` `éšå¼åå¥½` `å¤§å‹è¯­è¨€æ¨¡å‹` `åå¥½å­¦ä¹ ` `å“åº”ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åå¥½å­¦ä¹ æ–¹æ³•è¿‡äºä¾èµ–äººå·¥æˆ–é«˜çº§LLMsçš„ç­–åˆ’æ•°æ®ï¼Œå¯¼è‡´æˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ã€‚
2. æœ¬æ–‡æå‡ºçš„PUGCæ¡†æ¶åˆ©ç”¨æœªæ ‡è®°çš„ç”¨æˆ·ç”Ÿæˆå†…å®¹ä¸­çš„éšå¼åå¥½ç”Ÿæˆåå¥½æ•°æ®ï¼Œæå‡äº†å¯¹é½æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨DPOå’ŒPUGCè®­ç»ƒçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæé«˜äº†9.37%ï¼Œå¹¶åœ¨é•¿åº¦æ§åˆ¶ä¸Šè¾¾åˆ°äº†35.93%çš„èƒœç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å­¦ä¹ åå¥½åé¦ˆå¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½ä»¥åŠæé«˜ç”Ÿæˆå“åº”çš„è´¨é‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åå¥½å­¦ä¹ æ–¹æ³•è¿‡äºä¾èµ–äººå·¥æˆ–å…ˆè¿›LLMsçš„ç­–åˆ’æ•°æ®ï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶PUGCï¼Œåˆ©ç”¨æœªæ ‡è®°çš„ç”¨æˆ·ç”Ÿæˆå†…å®¹ï¼ˆUGCï¼‰ä¸­çš„éšå¼äººç±»åå¥½ç”Ÿæˆåå¥½æ•°æ®ã€‚å°½ç®¡UGCå¹¶éä¸“é—¨ä¸ºæŒ‡å¯¼LLMsç”Ÿæˆç¬¦åˆäººç±»åå¥½çš„å“åº”è€Œåˆ›å»ºï¼Œä½†å®ƒé€šå¸¸åæ˜ äº†åˆ›ä½œè€…çš„å®è´µè§è§£å’Œéšå¼åå¥½ã€‚PUGCå°†UGCè½¬åŒ–ä¸ºç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶ä»ç­–ç•¥æ¨¡å‹ç”Ÿæˆå“åº”ï¼Œéšååˆ©ç”¨UGCä½œä¸ºå“åº”è¯„åˆ†çš„å‚è€ƒæ–‡æœ¬ï¼Œä»è€Œä½¿æ¨¡å‹ä¸è¿™äº›éšå¼åå¥½å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DPOå’ŒPUGCè®­ç»ƒçš„æ¨¡å‹åœ¨Alpaca Eval 2ä¸Šæ¯”ä¼ ç»Ÿæ–¹æ³•æé«˜äº†9.37%çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†35.93%çš„æœ€å…ˆè¿›çš„é•¿åº¦æ§åˆ¶èƒœç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºäººå·¥ç­–åˆ’æ•°æ®ï¼Œéš¾ä»¥æ‰©å±•ä¸”æˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºPUGCæ¡†æ¶ï¼Œåˆ©ç”¨ç”¨æˆ·ç”Ÿæˆå†…å®¹ä¸­çš„éšå¼åå¥½ç”Ÿæˆåå¥½æ•°æ®ï¼Œé¿å…äº†å¯¹ç­–åˆ’æ•°æ®çš„ä¾èµ–ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPUGCæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œå°†UGCè½¬åŒ–ä¸ºç”¨æˆ·æŸ¥è¯¢ï¼›å…¶æ¬¡ï¼Œä»ç­–ç•¥æ¨¡å‹ç”Ÿæˆå“åº”ï¼›æœ€åï¼Œåˆ©ç”¨UGCä½œä¸ºå‚è€ƒæ–‡æœ¬è¿›è¡Œå“åº”è¯„åˆ†ï¼Œä»¥å®ç°æ¨¡å‹ä¸éšå¼åå¥½çš„å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šPUGCçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨UGCä¸­çš„éšå¼åå¥½ï¼Œè€Œéä¾èµ–äºäººå·¥æ ‡æ³¨çš„æ•°æ®ï¼Œä»è€Œæé«˜äº†åå¥½æ•°æ®çš„è´¨é‡å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å“åº”è¯„åˆ†ï¼Œå¹¶è®¾è®¡äº†é€‚åº”UGCç‰¹æ€§çš„ç½‘ç»œç»“æ„ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DPOå’ŒPUGCè®­ç»ƒçš„æ¨¡å‹åœ¨Alpaca Eval 2ä¸Šå®ç°äº†9.37%çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨é•¿åº¦æ§åˆ¶ä¸Šè¾¾åˆ°äº†35.93%çš„èƒœç‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨åå¥½å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€å†…å®¹æ¨èå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡åˆ©ç”¨ç”¨æˆ·ç”Ÿæˆå†…å®¹çš„éšå¼åå¥½ï¼ŒPUGCæ¡†æ¶èƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸå®ç°æ›´é«˜è´¨é‡çš„å“åº”ç”Ÿæˆï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning from preference feedback is essential for aligning large language models (LLMs) with human values and improving the quality of generated responses. However, existing preference learning methods rely heavily on curated data from humans or advanced LLMs, which is costly and difficult to scale. In this work, we present PUGC, a novel framework that leverages implicit human Preferences in unlabeled User-Generated Content (UGC) to generate preference data. Although UGC is not explicitly created to guide LLMs in generating human-preferred responses, it often reflects valuable insights and implicit preferences from its creators that has the potential to address readers' questions. PUGC transforms UGC into user queries and generates responses from the policy model. The UGC is then leveraged as a reference text for response scoring, aligning the model with these implicit preferences. This approach improves the quality of preference data while enabling scalable, domain-specific alignment. Experimental results on Alpaca Eval 2 show that models trained with DPO and PUGC achieve a 9.37% performance improvement over traditional methods, setting a 35.93% state-of-the-art length-controlled win rate using Mistral-7B-Instruct. Further studies highlight gains in reward quality, domain-specific alignment effectiveness, robustness against UGC quality, and theory of mind capabilities. Our code and dataset are available at https://zhaoxuan.info/PUGC.github.io/

