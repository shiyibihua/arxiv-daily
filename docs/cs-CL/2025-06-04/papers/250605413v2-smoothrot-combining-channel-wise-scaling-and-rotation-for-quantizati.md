---
layout: default
title: SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs
---

# SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05413" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05413v2</a>
  <a href="https://arxiv.org/pdf/2506.05413.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05413v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05413v2', 'SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Patrik CzakÃ³, GÃ¡bor KertÃ©sz, SÃ¡ndor SzÃ©nÃ¡si

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04 (æ›´æ–°: 2025-07-29)

**å¤‡æ³¨**: 6 pages, 3 figures, 5 tables. Accepted to IEEE SMC 2025 conference proceedings

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/czakop/smoothrot)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSmoothRotä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é‡åŒ–ä¸­çš„æ¿€æ´»å¼‚å¸¸é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åè®­ç»ƒé‡åŒ–` `å¤§è¯­è¨€æ¨¡å‹` `æ¿€æ´»å¼‚å¸¸å€¼` `é€šé“çº§ç¼©æ”¾` `Hadamardå˜æ¢` `é‡åŒ–ç²¾åº¦` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨å¤„ç†å¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¸¸å¸¸é¢ä¸´æ¿€æ´»å¼‚å¸¸å€¼è¿‡å¤šçš„é—®é¢˜ï¼Œå¯¼è‡´é‡åŒ–ç²¾åº¦ä¸‹é™ã€‚
2. SmoothRoté€šè¿‡ç»“åˆé€šé“çº§ç¼©æ”¾å’ŒHadamardå˜æ¢ï¼Œæ—¨åœ¨å°†æç«¯å¼‚å¸¸å€¼è½¬åŒ–ä¸ºé€‚åˆé‡åŒ–çš„æ¿€æ´»ï¼Œä»è€Œæå‡é‡åŒ–æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSmoothRotåœ¨å¤šä¸ªæµè¡Œçš„LLMsä¸Šæœ‰æ•ˆå‡å°‘äº†é‡åŒ–ä¸FP16æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œæå‡å¹…åº¦è¾¾åˆ°10-30%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†SmoothRotï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åè®­ç»ƒé‡åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­4ä½é‡åŒ–çš„æ•ˆç‡ã€‚SmoothRoté€šè¿‡ç»“åˆé€šé“çº§ç¼©æ”¾å’ŒHadamardå˜æ¢ï¼Œè§£å†³äº†å¤§é‡æ¿€æ´»å¼‚å¸¸å€¼çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥æŠ€æœ¯æœ‰æ•ˆåœ°å°†æç«¯å¼‚å¸¸å€¼è½¬åŒ–ä¸ºé€‚åˆé‡åŒ–çš„æ¿€æ´»ï¼Œæ˜¾è‘—æé«˜äº†é‡åŒ–ç²¾åº¦ã€‚åœ¨å¯¹æµè¡Œçš„LLMsï¼ˆå¦‚LLaMA2 7Bã€LLaMA3.1 8Bå’ŒMistral 7Bï¼‰è¿›è¡Œçš„å®éªŒä¸­ï¼ŒSmoothRotåœ¨è¯­è¨€ç”Ÿæˆå’Œé›¶-shotæ¨ç†ä»»åŠ¡ä¸­ï¼Œå§‹ç»ˆå°†é‡åŒ–æ¨¡å‹ä¸FP16æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·å‡å°‘äº†çº¦10-30%ï¼Œä¸”æ²¡æœ‰å¼•å…¥é¢å¤–çš„æ¨ç†å»¶è¿Ÿã€‚ä»£ç å¯åœ¨https://github.com/czakop/smoothrotè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨4ä½é‡åŒ–è¿‡ç¨‹ä¸­é¢ä¸´çš„æ¿€æ´»å¼‚å¸¸å€¼è¿‡å¤šçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›å¼‚å¸¸å€¼æ—¶ï¼Œå¾€å¾€å¯¼è‡´é‡åŒ–ç²¾åº¦æ˜¾è‘—ä¸‹é™ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSmoothRotçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“åˆé€šé“çº§ç¼©æ”¾å’ŒHadamardå˜æ¢ï¼Œæ¥æœ‰æ•ˆåœ°å°†æç«¯å¼‚å¸¸å€¼è½¬åŒ–ä¸ºé€‚åˆé‡åŒ–çš„æ¿€æ´»ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å‡å°‘æ¿€æ´»å€¼çš„åˆ†å¸ƒåå·®ï¼Œä»è€Œæé«˜é‡åŒ–çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSmoothRotçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé€šé“çº§ç¼©æ”¾æ¨¡å—å’ŒHadamardå˜æ¢æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šé“çº§ç¼©æ”¾æ¨¡å—å¯¹æ¿€æ´»è¿›è¡Œç¼©æ”¾ï¼Œä»¥å‡å°å¼‚å¸¸å€¼çš„å½±å“ï¼›ç„¶åï¼ŒHadamardå˜æ¢æ¨¡å—è¿›ä¸€æ­¥å¤„ç†è¿™äº›æ¿€æ´»ï¼Œä½¿å…¶æ›´é€‚åˆäºé‡åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šSmoothRotçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†é€šé“çº§ç¼©æ”¾ä¸Hadamardå˜æ¢ç›¸ç»“åˆï¼Œè¿™ä¸€æ–¹æ³•åœ¨å¤„ç†æ¿€æ´»å¼‚å¸¸å€¼æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é‡åŒ–æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒSmoothRoté‡‡ç”¨äº†ç‰¹å®šçš„ç¼©æ”¾å› å­å’Œå˜æ¢å‚æ•°ï¼Œä»¥ç¡®ä¿æ¿€æ´»å€¼çš„åˆ†å¸ƒæ›´å‡åŒ€ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ä¼˜åŒ–é‡åŒ–åçš„æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒSmoothRotèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ¨ç†å»¶è¿Ÿçš„æƒ…å†µä¸‹ï¼Œæå‡é‡åŒ–æ¨¡å‹çš„æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSmoothRotåœ¨LLaMA2 7Bã€LLaMA3.1 8Bå’ŒMistral 7Bç­‰æµè¡Œå¤§è¯­è¨€æ¨¡å‹ä¸Šï¼ŒæˆåŠŸå°†é‡åŒ–æ¨¡å‹ä¸FP16æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·å‡å°‘äº†çº¦10-30%ã€‚è¿™ä¸€æ˜¾è‘—æå‡è¡¨æ˜SmoothRotåœ¨é‡åŒ–å‹å¥½æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶æ²¡æœ‰å¼•å…¥é¢å¤–çš„æ¨ç†å»¶è¿Ÿï¼Œä¿æŒäº†æ¨¡å‹çš„å®æ—¶æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SmoothRotçš„ç ”ç©¶æˆæœåœ¨å¤§è¯­è¨€æ¨¡å‹çš„é‡åŒ–è¿‡ç¨‹ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜é‡åŒ–ç²¾åº¦ï¼ŒSmoothRotèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´é«˜æ•ˆçš„æ¨¡å‹ï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šåœ¨æ›´å¤šæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é‡åŒ–ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present SmoothRot, a novel post-training quantization technique to enhance the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot addresses the critical challenge of massive activation outliers, by integrating channel-wise scaling with Hadamard transformations. Our technique effectively transforms extreme outliers into quantization-friendly activations, significantly improving quantization accuracy. Experiments conducted on popular LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot consistently reduces the performance gap between quantized and FP16 models by approximately 10-30\% across language generation and zero-shot reasoning tasks, without introducing additional inference latency. Code is available at https://github.com/czakop/smoothrot.

