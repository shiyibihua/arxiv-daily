---
layout: default
title: High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning
---

# High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04051" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04051v1</a>
  <a href="https://arxiv.org/pdf/2506.04051.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04051v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04051v1', 'High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tim Franzmeyer, Archie Sravankumar, Lijuan Liu, Yuning Mao, Rui Hou, Sinong Wang, Jakob N. Foerster, Luke Zettlemoyer, Madian Khabsa

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHALTæ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `åè®­ç»ƒ` `èƒ½åŠ›å¯¹é½` `å¹»è§‰é—®é¢˜` `å“åº”å¯é æ€§` `å¾®è°ƒ` `æ•°æ®ç”Ÿæˆ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨ç¼ºä¹çŸ¥è¯†æ—¶å®¹æ˜“äº§ç”Ÿé”™è¯¯å›ç­”ï¼Œå¯¼è‡´å¹»è§‰ç°è±¡ï¼Œå½±å“å…¶å¯é æ€§ã€‚
2. HALTæ–¹æ³•é€šè¿‡ç”Ÿæˆèƒ½åŠ›å¯¹é½çš„åè®­ç»ƒæ•°æ®ï¼Œä½¿æ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶é€‰æ‹©éƒ¨åˆ†æ”¾å¼ƒå›ç­”ï¼Œä»è€Œæé«˜ç”Ÿæˆå†…å®¹çš„å¯é æ€§ã€‚
3. åœ¨å››ä¸ªé¢†åŸŸçš„å®éªŒä¸­ï¼ŒHALTä½¿å¾—æ¨¡å‹çš„ç‰‡æ®µæ­£ç¡®æ€§å¹³å‡æé«˜15%ï¼ŒF1åˆ†æ•°æå‡4%ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ•´ä½“å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å¯¹æ¯ä¸ªæç¤ºæ—¶éƒ½ä¼šç»™å‡ºå›åº”ï¼Œä½†åœ¨ç¼ºä¹çŸ¥è¯†æˆ–èƒ½åŠ›æ—¶å¯èƒ½äº§ç”Ÿé”™è¯¯ç­”æ¡ˆï¼Œè¿™ä¸€ç°è±¡è¢«ç§°ä¸ºå¹»è§‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åè®­ç»ƒæ–¹æ³•HALTï¼Œä½¿å¾—LLMåœ¨å¯¹å…¶ç”Ÿæˆå†…å®¹æœ‰ä¿¡å¿ƒæ—¶æ‰è¿›è¡Œå›åº”ï¼Œå¦åˆ™éƒ¨åˆ†æ”¾å¼ƒã€‚HALTé€šè¿‡å°†é¢„è®­ç»ƒLLMçš„å“åº”åˆ†å‰²ä¸ºäº‹å®ç‰‡æ®µï¼Œå¹¶åˆ©ç”¨çœŸå®ä¿¡æ¯è¯†åˆ«ä¸æ­£ç¡®çš„ç‰‡æ®µï¼Œä»è€Œç”Ÿæˆèƒ½åŠ›å¯¹é½çš„åè®­ç»ƒæ•°æ®ã€‚é€šè¿‡è°ƒæ•´é˜ˆå€¼ï¼ŒHALTèƒ½å¤Ÿåœ¨å“åº”å®Œæ•´æ€§å’Œç‰‡æ®µæ­£ç¡®æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒHALTåœ¨å››ä¸ªé¢†åŸŸçš„å¹³å‡ç‰‡æ®µæ­£ç¡®æ€§æé«˜äº†15%ï¼ŒF1åˆ†æ•°æå‡äº†4%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ä¸ç¡®å®šæ€§æ—¶äº§ç”Ÿé”™è¯¯å›ç­”çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†æ¨¡å‹çš„å¹»è§‰ç°è±¡ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹çš„å¯é æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHALTæ–¹æ³•é€šè¿‡åè®­ç»ƒä½¿æ¨¡å‹åœ¨å¯¹ç”Ÿæˆå†…å®¹æœ‰ä¿¡å¿ƒæ—¶æ‰è¿›è¡Œå›åº”ï¼Œé¿å…åœ¨ä¸ç¡®å®šæ—¶äº§ç”Ÿé”™è¯¯å›ç­”ã€‚å…·ä½“è€Œè¨€ï¼ŒHALTç”Ÿæˆçš„èƒ½åŠ›å¯¹é½æ•°æ®èƒ½å¤Ÿæ˜ç¡®æ¨¡å‹å¯ä»¥å’Œä¸å¯ä»¥å¯é ç”Ÿæˆçš„å†…å®¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHALTçš„æ•´ä½“æµç¨‹åŒ…æ‹¬å°†é¢„è®­ç»ƒLLMçš„å“åº”åˆ†å‰²ä¸ºäº‹å®ç‰‡æ®µï¼Œåˆ©ç”¨çœŸå®ä¿¡æ¯è¯†åˆ«ä¸æ­£ç¡®ç‰‡æ®µï¼Œå¹¶é€šè¿‡è°ƒæ•´é˜ˆå€¼æ¥å†³å®šæ˜¯åˆ é™¤é”™è¯¯ç‰‡æ®µè¿˜æ˜¯æ›¿æ¢ä¸ºâ€œUnsure from Hereâ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šHALTçš„åˆ›æ–°åœ¨äºå…¶èƒ½åŠ›å¯¹é½çš„åè®­ç»ƒæ•°æ®ç”Ÿæˆæ–¹å¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ¨¡å‹åœ¨ä¸ç¡®å®šæƒ…å†µä¸‹çš„é”™è¯¯å›ç­”ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆå†…å®¹çš„å¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼ŒHALTå…è®¸ç”¨æˆ·æ ¹æ®éœ€æ±‚è°ƒæ•´é˜ˆå€¼ï¼Œä»¥åœ¨å“åº”å®Œæ•´æ€§å’Œç‰‡æ®µæ­£ç¡®æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨å››ä¸ªä¸åŒé¢†åŸŸçš„å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé’ˆå¯¹æ¯ä¸ªé¢†åŸŸè®¾å®šäº†ä¸åŒçš„é˜ˆå€¼ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHALTæ–¹æ³•åœ¨å››ä¸ªé¢†åŸŸçš„å¹³å‡ç‰‡æ®µæ­£ç¡®æ€§æé«˜äº†15%ï¼ŒF1åˆ†æ•°æå‡äº†4%ã€‚é€šè¿‡é’ˆå¯¹æœ€é«˜æ­£ç¡®æ€§è¿›è¡Œå¾®è°ƒï¼ŒLlama3-70Bæ¨¡å‹çš„æ­£ç¡®æ€§ä»51%æå‡è‡³87%ï¼ŒåŒæ—¶ä¿æŒäº†53%çš„å“åº”å®Œæ•´æ€§ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HALTæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦é«˜å¯é æ€§çš„é¢†åŸŸï¼Œå¦‚åŒ»ç–—ã€æ³•å¾‹å’Œæ•™è‚²ç­‰ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„å›ç­”å‡†ç¡®æ€§ï¼ŒHALTèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´å¯ä¿¡çš„ç”Ÿæˆå†…å®¹ï¼Œå‡å°‘é”™è¯¯ä¿¡æ¯çš„ä¼ æ’­ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œè¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) currently respond to every prompt. However, they can produce incorrect answers when they lack knowledge or capability -- a problem known as hallucination. We instead propose post-training an LLM to generate content only when confident in its correctness and to otherwise (partially) abstain. Specifically, our method, HALT, produces capability-aligned post-training data that encodes what the model can and cannot reliably generate. We generate this data by splitting responses of the pretrained LLM into factual fragments (atomic statements or reasoning steps), and use ground truth information to identify incorrect fragments. We achieve capability-aligned finetuning responses by either removing incorrect fragments or replacing them with "Unsure from Here" -- according to a tunable threshold that allows practitioners to trade off response completeness and mean correctness of the response's fragments. We finetune four open-source models for biography writing, mathematics, coding, and medicine with HALT for three different trade-off thresholds. HALT effectively trades off response completeness for correctness, increasing the mean correctness of response fragments by 15% on average, while resulting in a 4% improvement in the F1 score (mean of completeness and correctness of the response) compared to the relevant baselines. By tuning HALT for highest correctness, we train a single reliable Llama3-70B model with correctness increased from 51% to 87% across all four domains while maintaining 53% of the response completeness achieved with standard finetuning.

