---
layout: default
title: SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling
---

# SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04179" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04179v1</a>
  <a href="https://arxiv.org/pdf/2506.04179.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04179v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04179v1', 'SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anhao Zhao, Fanghua Ye, Yingqi Fan, Junlong Tong, Zhiwei Fei, Hui Su, Xiaoyu Shen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/EIT-NLP/SkipGPT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSkipGPTä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„åŠ¨æ€å±‚ä¿®å‰ªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨æ€å±‚ä¿®å‰ª` `å¤§å‹è¯­è¨€æ¨¡å‹` `è®¡ç®—æ•ˆç‡` `tokenæ„ŸçŸ¥` `è§£è€¦ç­–ç•¥` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é™æ€å±‚ä¿®å‰ªæ–¹æ³•æœªèƒ½è€ƒè™‘LLMæ¨ç†ä¸­çš„æ°´å¹³å’Œå‚ç›´åŠ¨æ€æ€§ï¼Œå¯¼è‡´ä¿®å‰ªæ•ˆæœä¸ä½³ã€‚
2. SkipGPTé€šè¿‡å…¨çƒtokenæ„ŸçŸ¥è·¯ç”±å’Œè§£è€¦çš„ä¿®å‰ªç­–ç•¥ï¼Œé’ˆå¯¹ä¸åŒç»„ä»¶è¿›è¡ŒåŠ¨æ€ä¿®å‰ªï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkipGPTåœ¨å‡å°‘è¶…è¿‡40%æ¨¡å‹å‚æ•°çš„åŒæ—¶ï¼Œæ€§èƒ½ä¸åŸå§‹æ¨¡å‹æŒå¹³æˆ–æ›´ä¼˜ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå…¶æ·±å±‚æ¬¡ã€å¤šå±‚æ¬¡çš„æ¶æ„ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚å±‚ä¿®å‰ªä½œä¸ºä¸€ç§ç¼“è§£è¿™äº›ä½æ•ˆçš„æ–¹æ³•ï¼Œä¼ ç»Ÿçš„é™æ€ä¿®å‰ªæ–¹æ³•å¿½è§†äº†LLMæ¨ç†ä¸­å›ºæœ‰çš„ä¸¤ç§åŠ¨æ€æ€§ï¼šæ°´å¹³åŠ¨æ€æ€§å’Œå‚ç›´åŠ¨æ€æ€§ã€‚æœ¬æ–‡æå‡ºSkipGPTï¼Œä¸€ä¸ªåŠ¨æ€å±‚ä¿®å‰ªæ¡†æ¶ï¼Œé€šè¿‡å…¨çƒtokenæ„ŸçŸ¥è·¯ç”±å’Œè§£è€¦çš„MLPä¸è‡ªæ³¨æ„åŠ›ç»„ä»¶ä¿®å‰ªç­–ç•¥æ¥ä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ã€‚ä¸ºç¼“è§£è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œæå‡ºäº†ä¸¤é˜¶æ®µä¼˜åŒ–èŒƒå¼ï¼Œæœ€ç»ˆå®éªŒè¡¨æ˜SkipGPTåœ¨å‡å°‘40%ä»¥ä¸Šæ¨¡å‹å‚æ•°çš„åŒæ—¶ï¼Œæ€§èƒ½ä¸åŸå§‹å¯†é›†æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ã€‚è¯¥ç ”ç©¶æ¨åŠ¨äº†å¯æ‰©å±•ã€èµ„æºæ„ŸçŸ¥LLMsçš„å®é™…éƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”±äºæ·±å±‚æ¶æ„å¯¼è‡´çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ç°æœ‰çš„é™æ€å±‚ä¿®å‰ªæ–¹æ³•æœªèƒ½æœ‰æ•ˆåº”å¯¹æ¨¡å‹ä¸­tokençš„å¼‚è´¨æ€§å’Œä¸åŒå±‚çš„åŠŸèƒ½å·®å¼‚ï¼Œå¯¼è‡´ä¿®å‰ªæ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSkipGPTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŠ¨æ€å±‚ä¿®å‰ªæ¥ä¼˜åŒ–è®¡ç®—èµ„æºçš„åˆ†é…ï¼Œå…·ä½“åŒ…æ‹¬å…¨çƒtokenæ„ŸçŸ¥è·¯ç”±å’Œé’ˆå¯¹ä¸åŒç»„ä»¶çš„è§£è€¦ä¿®å‰ªç­–ç•¥ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSkipGPTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºè§£è€¦è®­ç»ƒï¼Œé€šè¿‡è½¯å‚æ•°åŒ–å­¦ä¹ è·¯ç”±ç­–ç•¥ï¼Œé¿å…è¿‡æ—©çš„ä¿®å‰ªå†³ç­–ï¼›ç¬¬äºŒé˜¶æ®µä¸ºå‚æ•°é«˜æ•ˆçš„LoRAå¾®è°ƒï¼Œä»¥æ¢å¤å› å±‚ç§»é™¤è€Œå—åˆ°å½±å“çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSkipGPTçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åŠ¨æ€çš„tokenæ„ŸçŸ¥ä¿®å‰ªå’Œè§£è€¦çš„ä¿®å‰ªç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿçš„é™æ€ä¿®å‰ªæ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¿æŒè¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶å®ç°äº†åŠ¨æ€æ•ˆç‡çš„æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒSkipGPTé‡‡ç”¨äº†è½¯å‚æ•°åŒ–çš„è·¯ç”±ç­–ç•¥å’ŒLoRAå¾®è°ƒæŠ€æœ¯ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¿®å‰ªåçš„æ€§èƒ½æ¢å¤ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹MLPå’Œè‡ªæ³¨æ„åŠ›å±‚çš„ä¸åŒåŠŸèƒ½ï¼Œè®¾è®¡äº†ä¸“é—¨çš„ä¿®å‰ªç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SkipGPTåœ¨å®éªŒä¸­æˆåŠŸå‡å°‘äº†è¶…è¿‡40%çš„æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸åŸå§‹å¯†é›†æ¨¡å‹çš„æ€§èƒ½æŒå¹³æˆ–æ›´ä¼˜ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒSkipGPTåœ¨åŠ¨æ€ä¿®å‰ªæ–¹é¢çš„åˆ›æ–°èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SkipGPTçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡ä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ”¯æŒæ›´å¤§è§„æ¨¡çš„æ¨¡å‹éƒ¨ç½²ï¼Œé™ä½è¿è¡Œæˆæœ¬ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼ŒSkipGPTæœ‰æœ›æ¨åŠ¨æ›´é«˜æ•ˆçš„LLMåœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) achieve remarkable performance across tasks but incur substantial computational costs due to their deep, multi-layered architectures. Layer pruning has emerged as a strategy to alleviate these inefficiencies, but conventional static pruning methods overlook two critical dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level heterogeneity demands context-aware pruning decisions, and (2) vertical dynamics, where the distinct functional roles of MLP and self-attention layers necessitate component-specific pruning policies. We introduce SkipGPT, a dynamic layer pruning framework designed to optimize computational resource allocation through two core innovations: (1) global token-aware routing to prioritize critical tokens, and (2) decoupled pruning policies for MLP and self-attention components. To mitigate training instability, we propose a two-stage optimization paradigm: first, a disentangled training phase that learns routing strategies via soft parameterization to avoid premature pruning decisions, followed by parameter-efficient LoRA fine-tuning to restore performance impacted by layer removal. Extensive experiments demonstrate that SkipGPT reduces over 40% of model parameters while matching or exceeding the performance of the original dense model across benchmarks. By harmonizing dynamic efficiency with preserved expressivity, SkipGPT advances the practical deployment of scalable, resource-aware LLMs. Our code is publicly available at: https://github.com/EIT-NLP/SkipGPT.

