---
layout: default
title: From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding
---

# From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03968" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.03968v1</a>
  <a href="https://arxiv.org/pdf/2506.03968.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03968v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03968v1', 'From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chiwei Zhu, Benfeng Xu, Xiaorui Wang, Zhendong Mao

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**å¤‡æ³¨**: To be published at ACL 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Ignoramus0817/SynthQuestions)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå±æ€§å¼•å¯¼çš„åˆæˆæ–¹æ³•ä»¥ç”Ÿæˆå¤šæ ·åŒ–ç”¨æˆ·æŒ‡ä»¤**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆæˆæŒ‡ä»¤` `å±æ€§å¼•å¯¼` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®é›†æ„å»º` `å¤æ‚æ€§ç”Ÿæˆ` `äººæœºäº¤äº’` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åˆæˆæŒ‡ä»¤çš„æ–¹æ³•åœ¨æ•°æ®æ¥æºä¸Šå—é™ï¼Œå¯¼è‡´ç”Ÿæˆçš„æŒ‡ä»¤åˆ†å¸ƒç‹­çª„ï¼Œæ— æ³•æ»¡è¶³å¤æ‚æ€§éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå±æ€§å¼•å¯¼çš„åˆæˆæ¡†æ¶ï¼Œé€šè¿‡çœŸå®æŒ‡ä»¤ä¸ç”¨æˆ·çš„ç»“åˆï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æŒ‡ä»¤ã€‚
3. æ„å»ºçš„SynthQuestionsæ•°æ®é›†åŒ…å«100ä¸‡æ¡æŒ‡ä»¤ï¼Œè®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½éšæ•°æ®é‡å¢åŠ è€Œæå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è‡ªåŠ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ï¼Œè·å–å¤šæ ·ã€å¤æ‚ä¸”å¤§è§„æ¨¡çš„æŒ‡ä»¤æ•°æ®è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”ŸæˆåˆæˆæŒ‡ä»¤æ—¶ï¼Œå¾€å¾€å—é™äºåŸºç¡€æ•°æ®æ¥æºï¼Œå¯¼è‡´åˆ†å¸ƒç‹­çª„ï¼Œæˆ–ä¾èµ–ç®€å•æ‰©å±•ï¼Œæ— æ³•äº§ç”Ÿæœ‰æ„ä¹‰çš„å¤æ‚æ€§è½¨è¿¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå±æ€§å¼•å¯¼çš„åˆæˆæ–¹æ³•ï¼Œé€šè¿‡è‡ªä¸Šè€Œä¸‹çš„å½’å› è¿‡ç¨‹å°†çœŸå®æŒ‡ä»¤ä¸ç‰¹å®šç”¨æˆ·ç›¸ç»“åˆï¼Œå¹¶åˆ©ç”¨ç½‘ç»œæ–‡æ¡£ç”Ÿæˆæƒ…å¢ƒå’Œæœ‰æ„ä¹‰çš„æŒ‡ä»¤ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«100ä¸‡æ¡æŒ‡ä»¤çš„æ•°æ®é›†SynthQuestionsï¼Œå®éªŒè¡¨æ˜ï¼ŒåŸºäºè¯¥æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”éšç€ç½‘ç»œè¯­æ–™çš„å¢åŠ ï¼Œæ€§èƒ½æŒç»­æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åˆæˆæŒ‡ä»¤æ–¹æ³•åœ¨æ•°æ®æ¥æºå’Œå¤æ‚æ€§ä¸Šçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•ç”Ÿæˆå¤šæ ·åŒ–ä¸”å¤æ‚çš„æŒ‡ä»¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŸºäºå±æ€§å¼•å¯¼çš„åˆæˆæ¡†æ¶ï¼Œé€šè¿‡è‡ªä¸Šè€Œä¸‹çš„å½’å› å’Œè‡ªä¸‹è€Œä¸Šçš„åˆæˆè¿‡ç¨‹ï¼Œç»“åˆçœŸå®æŒ‡ä»¤å’Œç½‘ç»œæ–‡æ¡£ç”Ÿæˆæœ‰æ„ä¹‰çš„ç”¨æˆ·æŒ‡ä»¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè‡ªä¸Šè€Œä¸‹çš„å½’å› è¿‡ç¨‹å’Œè‡ªä¸‹è€Œä¸Šçš„åˆæˆè¿‡ç¨‹ã€‚å½’å› è¿‡ç¨‹å°†çœŸå®æŒ‡ä»¤ä¸ç‰¹å®šç”¨æˆ·æƒ…å¢ƒç›¸ç»“åˆï¼Œè€Œåˆæˆè¿‡ç¨‹åˆ™åˆ©ç”¨ç½‘ç»œæ–‡æ¡£ç”Ÿæˆæƒ…å¢ƒå’ŒæŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†çœŸå®æŒ‡ä»¤çš„å½’å› ä¸ç½‘ç»œæ–‡æ¡£çš„åˆæˆï¼Œå½¢æˆäº†ä¸€ä¸ªé«˜æ•ˆçš„æŒ‡ä»¤ç”Ÿæˆæœºåˆ¶ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å¤æ‚å’Œå¤šæ ·åŒ–çš„æŒ‡ä»¤ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å½’å› ç®—æ³•å’Œåˆæˆç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„æŒ‡ä»¤æ—¢æœ‰å®ç”¨æ€§åˆå…·å¤‡å¤æ‚æ€§ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°çš„è®¾ç½®ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºSynthQuestionsæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æŒ‡ä»¤ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€æ•™è‚²æŠ€æœ¯å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡ç”Ÿæˆå¤šæ ·åŒ–çš„ç”¨æˆ·æŒ‡ä»¤ï¼Œå¯ä»¥æå‡æ¨¡å‹çš„é€‚åº”æ€§å’Œå®ç”¨æ€§ï¼Œæœªæ¥å¯èƒ½åœ¨è‡ªåŠ¨åŒ–å®¢æœã€ä¸ªæ€§åŒ–å­¦ä¹ ç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.

