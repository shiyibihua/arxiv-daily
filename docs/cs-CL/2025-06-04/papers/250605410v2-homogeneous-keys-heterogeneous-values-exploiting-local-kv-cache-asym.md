---
layout: default
title: Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs
---

# Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05410" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05410v2</a>
  <a href="https://arxiv.org/pdf/2506.05410.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05410v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05410v2', 'Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wanyun Cui, Mingwei Xu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04 (æ›´æ–°: 2025-11-06)

**å¤‡æ³¨**: 14 pages,7 figures;Accepted by NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/the-scale-lab/Asymkv)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAsymKVä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡LLMä¸­çš„KVç¼“å­˜ä¸å¯¹ç§°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡` `å¤§å‹è¯­è¨€æ¨¡å‹` `KVç¼“å­˜` `å‹ç¼©æŠ€æœ¯` `æ— æŸå‹ç¼©` `æ³¨æ„åŠ›æœºåˆ¶` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­é¢ä¸´æ³¨æ„åŠ›æœºåˆ¶çš„å¹³æ–¹å¤æ‚åº¦é—®é¢˜ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºçš„AsymKVæ¡†æ¶åˆ©ç”¨é”®å€¼ç¼“å­˜çš„ä¸å¯¹ç§°æ€§ï¼Œé€šè¿‡æ— æŸå‹ç¼©æŠ€æœ¯ä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€‚
3. åœ¨LLaMA3.1-8Bæ¨¡å‹ä¸Šï¼ŒAsymKVåœ¨LongBenchä¸Šå¹³å‡å¾—åˆ†è¾¾åˆ°43.95ï¼Œæ˜¾è‘—è¶…è¶Šäº†SOTAæ–¹æ³•H$_2$Oçš„38.89åˆ†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•çªæ˜¾äº†æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦çš„é‡è¦æ€§ï¼Œä½†æ³¨æ„åŠ›æœºåˆ¶çš„å¹³æ–¹å¤æ‚åº¦å¯¹é«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚KVç¼“å­˜å‹ç¼©å·²æˆä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„å…³é”®æ–¹æ³•ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†KVç¼“å­˜ä¸­ä¸€ä¸ªåŸºæœ¬ä½†è¢«å¿½è§†çš„ä¸å¯¹ç§°æ€§ï¼šç›¸é‚»çš„é”®æ¥æ”¶ç›¸ä¼¼çš„æ³¨æ„åŠ›æƒé‡ï¼ˆå±€éƒ¨åŒè´¨æ€§ï¼‰ï¼Œè€Œç›¸é‚»çš„å€¼åˆ™è¡¨ç°å‡ºä¸åŒçš„å¼‚è´¨åˆ†å¸ƒã€‚è¿™ç§é”®å€¼ä¸å¯¹ç§°æ€§æ­ç¤ºäº†ç°æœ‰å‹ç¼©æ–¹æ³•çš„ä¸€ä¸ªå…³é”®å±€é™æ€§ï¼Œå³å°†é”®å’Œå€¼ç»Ÿä¸€å¤„ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„å‹ç¼©æ¡†æ¶ï¼ˆAsymKVï¼‰ï¼Œç»“åˆåŸºäºåŒè´¨æ€§çš„é”®åˆå¹¶ä¸æ•°å­¦ä¸Šè¯æ˜çš„æ— æŸå€¼å‹ç¼©ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAsymKVåœ¨å„ç§ä»»åŠ¡å’ŒåŸºç¡€æ¨¡å‹ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„é•¿ä¸Šä¸‹æ–‡æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„æ˜¯é•¿ä¸Šä¸‹æ–‡LLMä¸­KVç¼“å­˜çš„æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨é”®å€¼ä¹‹é—´çš„å¼‚è´¨æ€§ï¼Œå¯¼è‡´å‹ç¼©æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯åˆ©ç”¨é”®çš„åŒè´¨æ€§ä¸å€¼çš„å¼‚è´¨æ€§ï¼Œé€šè¿‡æ— è®­ç»ƒçš„æ–¹å¼å®ç°æ›´é«˜æ•ˆçš„KVç¼“å­˜å‹ç¼©ï¼Œä»è€Œæå‡é•¿ä¸Šä¸‹æ–‡çš„å¤„ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAsymKVæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šåŒè´¨æ€§åŸºç¡€çš„é”®åˆå¹¶å’Œæ— æŸå€¼å‹ç¼©ã€‚é¦–å…ˆå¯¹ç›¸ä¼¼çš„é”®è¿›è¡Œåˆå¹¶ï¼Œç„¶åå¯¹å€¼è¿›è¡Œç‹¬ç«‹çš„æ— æŸå‹ç¼©ï¼Œä»¥ä¿æŒä¿¡æ¯å®Œæ•´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºè¯†åˆ«å¹¶åˆ©ç”¨KVç¼“å­˜ä¸­çš„ä¸å¯¹ç§°æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å‹ç¼©æ–¹æ³•ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„å‡åŒ€å¤„ç†æ–¹å¼ï¼Œæ˜¾è‘—æé«˜äº†å‹ç¼©æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„åˆå¹¶ç­–ç•¥å’Œå‹ç¼©ç®—æ³•ï¼Œç¡®ä¿åœ¨å‹ç¼©è¿‡ç¨‹ä¸­ä¸æŸå¤±å…³é”®ä¿¡æ¯ï¼ŒåŒæ—¶ä¼˜åŒ–äº†å‚æ•°è®¾ç½®ä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚é€šè¿‡æ•°å­¦è¯æ˜ï¼Œç¡®ä¿äº†å€¼çš„å‹ç¼©æ˜¯æ— æŸçš„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒAsymKVåœ¨LLaMA3.1-8Bæ¨¡å‹ä¸Šå–å¾—äº†43.95çš„å¹³å‡å¾—åˆ†ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€ä¼˜æ–¹æ³•H$_2$Oçš„38.89åˆ†ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„æ•ˆç‡ï¼ŒAsymKVèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡æ¨¡å‹çš„å“åº”é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights ({\it local homogeneity}), adjacent values demonstrate distinct {\it heterogeneous} distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in this link:https://github.com/the-scale-lab/Asymkv.

