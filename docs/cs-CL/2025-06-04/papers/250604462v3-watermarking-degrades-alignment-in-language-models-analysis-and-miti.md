---
layout: default
title: Watermarking Degrades Alignment in Language Models: Analysis and Mitigation
---

# Watermarking Degrades Alignment in Language Models: Analysis and Mitigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04462" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04462v3</a>
  <a href="https://arxiv.org/pdf/2506.04462.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04462v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04462v3', 'Watermarking Degrades Alignment in Language Models: Analysis and Mitigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Apurv Verma, NhatHai Phan, Shubhendu Trivedi

**åˆ†ç±»**: cs.CL, cs.CR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04 (æ›´æ–°: 2025-07-12)

**å¤‡æ³¨**: Published at the 1st Workshop on GenAI Watermarking (ICLR 2025). Code: https://github.com/dapurv5/alignmark

**æœŸåˆŠ**: 1st Workshop on GenAI Watermarking, ICLR 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æŠ—æ°´å°å½±å“çš„å¯¹é½æ¢å¤æ–¹æ³•ä»¥æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ°´å°æŠ€æœ¯` `è¯­è¨€æ¨¡å‹` `å¯¹é½æ¢å¤` `å¥–åŠ±æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å®‰å…¨æ€§` `è¾“å‡ºè´¨é‡` `å®éªŒåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ°´å°æŠ€æœ¯å¯¹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºè´¨é‡é€ æˆè´Ÿé¢å½±å“ï¼Œå°¤å…¶åœ¨çœŸå®åº¦å’Œå®‰å…¨æ€§æ–¹é¢çš„å½±å“å°šæœªè¢«å……åˆ†æ¢è®¨ã€‚
2. æå‡ºå¯¹é½é‡é‡‡æ ·ï¼ˆARï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¤–éƒ¨å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†æ—¶æ¢å¤æ¨¡å‹çš„å¯¹é½æ€§èƒ½ï¼Œè§£å†³æ°´å°å¸¦æ¥çš„é™çº§é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ARæ–¹æ³•èƒ½å¤Ÿåœ¨æ°´å°æƒ…å†µä¸‹æœ‰æ•ˆæ¢å¤æˆ–è¶…è¶ŠåŸºçº¿å¯¹é½åˆ†æ•°ï¼Œä¸”ä¿æŒæ°´å°çš„å¯æ£€æµ‹æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ°´å°æŠ€æœ¯å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¾“å‡ºè´¨é‡æœ‰æ˜¾è‘—å½±å“ï¼Œä½†å…¶å¯¹çœŸå®åº¦ã€å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†ä¸¤ç§æµè¡Œçš„æ°´å°æ–¹æ³•â€”â€”Gumbelå’ŒKGWâ€”â€”å¦‚ä½•å½±å“å››ä¸ªå¯¹é½LLMsçš„æ ¸å¿ƒå¯¹é½å±æ€§ã€‚å®éªŒæ­ç¤ºäº†ä¸¤ç§ä¸åŒçš„é™çº§æ¨¡å¼ï¼šä¿æŠ¤è¡°å‡å’Œä¿æŠ¤å¢å¼ºã€‚ä¸ºç¼“è§£è¿™äº›é™çº§ï¼Œæå‡ºäº†å¯¹é½é‡é‡‡æ ·ï¼ˆARï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¤–éƒ¨å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†æ—¶æ¢å¤å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡æ ·2-4ä¸ªæ°´å°ç”Ÿæˆçš„ç»“æœèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤æˆ–è¶…è¶ŠåŸºçº¿å¯¹é½åˆ†æ•°ï¼Œç¡®ä¿æ°´å°çš„å¯æ£€æµ‹æ€§ã€‚æ­¤ç ”ç©¶æ­ç¤ºäº†æ°´å°å¼ºåº¦ä¸æ¨¡å‹å¯¹é½ä¹‹é—´çš„å…³é”®å¹³è¡¡ï¼Œä¸ºè´Ÿè´£ä»»åœ°éƒ¨ç½²æ°´å°LLMsæä¾›äº†ç®€å•çš„æ¨ç†æ—¶è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ°´å°æŠ€æœ¯å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ€§èƒ½çš„è´Ÿé¢å½±å“ï¼Œç°æœ‰æ–¹æ³•åœ¨æå‡æœ‰ç”¨æ€§ä¸ä¿æŒå®‰å…¨æ€§ä¹‹é—´å­˜åœ¨çŸ›ç›¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¯¹é½é‡é‡‡æ ·ï¼ˆARï¼‰æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†é˜¶æ®µå¼•å…¥å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œæ¢å¤æ¨¡å‹çš„å¯¹é½æ€§èƒ½ï¼Œç¼“è§£æ°´å°å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šARæ–¹æ³•åœ¨æ¨ç†æ—¶å¯¹æ°´å°ç”Ÿæˆçš„ç»“æœè¿›è¡Œé‡é‡‡æ ·ï¼Œç»“åˆå¤–éƒ¨å¥–åŠ±æ¨¡å‹è¯„ä¼°ç”Ÿæˆç»“æœçš„å¯¹é½ç¨‹åº¦ï¼Œç¡®ä¿ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§ä¸å¯æ£€æµ‹æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šARæ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å¤–éƒ¨å¥–åŠ±æ¨¡å‹çš„å¼•å…¥ï¼Œè§£å†³äº†æ°´å°æŠ€æœ¯å¯¼è‡´çš„å¯¹é½æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œä¸ä¼ ç»Ÿæ°´å°æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºçµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ARæ–¹æ³•ä¸­ï¼Œé‡‡æ ·çš„æ°´å°ç”Ÿæˆæ•°é‡è®¾ç½®ä¸º2-4ä¸ªï¼Œä»¥ç¡®ä¿æœ‰æ•ˆæ¢å¤å¯¹é½åˆ†æ•°ï¼ŒåŒæ—¶åœ¨è®¾è®¡ä¸­ç‰ºç‰²äº†ä¸€å®šçš„å¤±çœŸè‡ªç”±åº¦ï¼Œä»¥ä¿æŒæ°´å°çš„å¼ºæ£€æµ‹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å¯¹é½é‡é‡‡æ ·ï¼ˆARï¼‰æ–¹æ³•èƒ½å¤Ÿåœ¨æ°´å°æƒ…å†µä¸‹æœ‰æ•ˆæ¢å¤æˆ–è¶…è¶ŠåŸºçº¿å¯¹é½åˆ†æ•°ï¼Œå…·ä½“è€Œè¨€ï¼Œé‡‡æ ·2-4ä¸ªæ°´å°ç”Ÿæˆç»“æœæ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¯¹é½æ€§èƒ½ï¼Œç¡®ä¿æ°´å°çš„å¯æ£€æµ‹æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œå†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æœ‰æ•ˆåœ°æ¢å¤æ°´å°LLMsçš„å¯¹é½æ€§èƒ½ï¼Œç ”ç©¶ä¸ºè´Ÿè´£ä»»åœ°ä½¿ç”¨æ°´å°æŠ€æœ¯æä¾›äº†ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ï¼Œä¿ƒè¿›äº†å®‰å…¨å’Œå¯é çš„AIåº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Watermarking techniques for large language models (LLMs) can significantly impact output quality, yet their effects on truthfulness, safety, and helpfulness remain critically underexamined. This paper presents a systematic analysis of how two popular watermarking approaches-Gumbel and KGW-affect these core alignment properties across four aligned LLMs. Our experiments reveal two distinct degradation patterns: guard attenuation, where enhanced helpfulness undermines model safety, and guard amplification, where excessive caution reduces model helpfulness. These patterns emerge from watermark-induced shifts in token distribution, surfacing the fundamental tension that exists between alignment objectives.
>   To mitigate these degradations, we propose Alignment Resampling (AR), an inference-time sampling method that uses an external reward model to restore alignment. We establish a theoretical lower bound on the improvement in expected reward score as the sample size is increased and empirically demonstrate that sampling just 2-4 watermarked generations effectively recovers or surpasses baseline (unwatermarked) alignment scores. To overcome the limited response diversity of standard Gumbel watermarking, our modified implementation sacrifices strict distortion-freeness while maintaining robust detectability, ensuring compatibility with AR. Experimental results confirm that AR successfully recovers baseline alignment in both watermarking approaches, while maintaining strong watermark detectability. This work reveals the critical balance between watermark strength and model alignment, providing a simple inference-time solution to responsibly deploy watermarked LLMs in practice.

