---
layout: default
title: Enhancing Decision-Making of Large Language Models via Actor-Critic
---

# Enhancing Decision-Making of Large Language Models via Actor-Critic

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06376" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06376v1</a>
  <a href="https://arxiv.org/pdf/2506.06376.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06376v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06376v1', 'Enhancing Decision-Making of Large Language Models via Actor-Critic')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Heng Dong, Kefei Duan, Chongjie Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**å¤‡æ³¨**: Forty-second International Conference on Machine Learning (ICML 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLACæ¡†æ¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å†³ç­–èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å†³ç­–ä¼˜åŒ–` `é•¿æœŸæ¨ç†` `Actor-Critic` `Qå€¼è¯„ä¼°` `æ— æ¢¯åº¦ç­–ç•¥` `æ™ºèƒ½å†³ç­–` `å¤æ‚ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚å†³ç­–åœºæ™¯ä¸­ç¼ºä¹é•¿æœŸæ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´å†³ç­–æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºLACæ¡†æ¶ï¼Œé€šè¿‡è®¡ç®—Qå€¼å’Œæ— æ¢¯åº¦ç­–ç•¥æ”¹è¿›ï¼Œæå‡LLMçš„å†³ç­–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLACåœ¨å¤šç§ç¯å¢ƒä¸­è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­å…·æœ‰ç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨éœ€è¦é•¿æœŸæ¨ç†å’Œé«˜å±‚æ¬¡ç›®æ ‡å¯¹é½çš„å¤æ‚å†³ç­–åœºæ™¯ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–çŸ­æœŸè‡ªå›å½’åŠ¨ä½œç”Ÿæˆæˆ–åœ¨å‡†ç¡®æ¨¡æ‹Ÿå›æ»šå’Œè¯„ä¼°ç»“æœæ–¹é¢å­˜åœ¨å±€é™ï¼Œå¯¼è‡´å†³ç­–æ¬¡ä¼˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºLLMçš„Actor-Criticæ¡†æ¶LACï¼Œæœ‰æ•ˆæ”¹å–„LLMç­–ç•¥ï¼Œé€šè¿‡é•¿æœŸåŠ¨ä½œè¯„ä¼°ä»¥åŸåˆ™æ€§å’Œå¯æ‰©å±•çš„æ–¹å¼è§£å†³äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€æ˜¯é€šè¿‡ä¸æ­£/è´Ÿç»“æœç›¸å…³çš„token logitsè®¡ç®—Qå€¼ï¼Œæå–ç¨³å¥çš„åŠ¨ä½œè¯„ä¼°ï¼Œå¹¶é€šè¿‡æœªæ¥è½¨è¿¹å›æ»šå’Œæ¨ç†å¢å¼ºï¼›äºŒæ˜¯é€šè¿‡æ— æ¢¯åº¦æœºåˆ¶å®ç°é«˜æ•ˆçš„ç­–ç•¥æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§ç¯å¢ƒä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­è¶…è¶Šäº†ä½¿ç”¨GPT-4çš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å†³ç­–åœºæ™¯ä¸­é•¿æœŸæ¨ç†ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–çŸ­æœŸå†³ç­–ç”Ÿæˆï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°é•¿æœŸç»“æœï¼Œå¯¼è‡´å†³ç­–è´¨é‡ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„LACæ¡†æ¶é€šè¿‡è®¡ç®—ä¸æ­£/è´Ÿç»“æœç›¸å…³çš„Qå€¼ï¼Œç»“åˆæœªæ¥è½¨è¿¹å›æ»šï¼Œå¢å¼ºäº†åŠ¨ä½œè¯„ä¼°çš„ç¨³å¥æ€§ï¼Œå¹¶é‡‡ç”¨æ— æ¢¯åº¦æœºåˆ¶è¿›è¡Œç­–ç•¥æ”¹è¿›ï¼Œä»è€Œå®ç°äº†æ›´ä¼˜çš„å†³ç­–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLACæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šä¸€æ˜¯åŠ¨ä½œè¯„ä¼°æ¨¡å—ï¼Œé€šè¿‡token logitsè®¡ç®—Qå€¼ï¼›äºŒæ˜¯ç­–ç•¥æ”¹è¿›æ¨¡å—ï¼Œåˆ©ç”¨æ— æ¢¯åº¦æ–¹æ³•ä¼˜åŒ–å†³ç­–ç­–ç•¥ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šè¾“å…¥ç¯å¢ƒçŠ¶æ€ï¼Œè®¡ç®—Qå€¼ï¼Œè¯„ä¼°åŠ¨ä½œï¼Œä¼˜åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šLACçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆäº†é•¿æœŸåŠ¨ä½œè¯„ä¼°ä¸æ— æ¢¯åº¦ç­–ç•¥æ”¹è¿›æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†LLMåœ¨å¤æ‚å†³ç­–ä¸­çš„è¡¨ç°ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»ŸçŸ­æœŸå†³ç­–ç”Ÿæˆæ–¹æ³•å½¢æˆäº†æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒLACä½¿ç”¨äº†ä¸æ­£è´Ÿç»“æœç›¸å…³çš„token logitsè¿›è¡ŒQå€¼è®¡ç®—ï¼Œå¹¶é€šè¿‡æœªæ¥è½¨è¿¹å›æ»šå¢å¼ºè¯„ä¼°çš„å‡†ç¡®æ€§ã€‚æ— æ¢¯åº¦ç­–ç•¥æ”¹è¿›æœºåˆ¶åˆ™é¿å…äº†å¤æ‚çš„æ¢¯åº¦è®¡ç®—ï¼Œæé«˜äº†æ•ˆç‡ã€‚å®éªŒä¸­ä½¿ç”¨äº†7B/8Bå‚æ•°çš„LLMï¼Œç¡®ä¿äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLACæ¡†æ¶åœ¨å¤šç§ç¯å¢ƒä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨é«˜å±‚æ¬¡å†³ç­–ä»»åŠ¡ALFWorldå’Œå¤æ‚ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨7B/8Bå‚æ•°çš„LLMæ—¶ï¼Œæ€§èƒ½è¶…è¿‡äº†åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨GPT-4çš„åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æå‡å¹…åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å†³ç­–ç³»ç»Ÿã€è‡ªåŠ¨åŒ–æœºå™¨äººã€æ¸¸æˆAIç­‰ï¼Œèƒ½å¤Ÿåœ¨éœ€è¦å¤æ‚å†³ç­–å’Œé•¿æœŸè§„åˆ’çš„åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚é€šè¿‡æå‡LLMçš„å†³ç­–èƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½åº”ç”¨çš„å‘å±•ï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have achieved remarkable advancements in natural language processing tasks, yet they encounter challenges in complex decision-making scenarios that require long-term reasoning and alignment with high-level objectives. Existing methods either rely on short-term auto-regressive action generation or face limitations in accurately simulating rollouts and assessing outcomes, leading to sub-optimal decisions. This paper introduces a novel LLM-based Actor-Critic framework, termed LAC, that effectively improves LLM policies with long-term action evaluations in a principled and scalable way. Our approach addresses two key challenges: (1) extracting robust action evaluations by computing Q-values via token logits associated with positive/negative outcomes, enhanced by future trajectory rollouts and reasoning; and (2) enabling efficient policy improvement through a gradient-free mechanism. Experiments across diverse environments -- including high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text), and large action spaces (WebShop) -- demonstrate the framework's generality and superiority over state-of-the-art methods. Notably, our approach achieves competitive performance using 7B/8B parameter LLMs, even outperforming baseline methods employing GPT-4 in complex tasks. These results underscore the potential of integrating structured policy optimization with LLMs' intrinsic knowledge to advance decision-making capabilities in multi-step environments.

