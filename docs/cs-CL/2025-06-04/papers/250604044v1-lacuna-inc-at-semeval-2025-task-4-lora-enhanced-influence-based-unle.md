---
layout: default
title: Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs
---

# Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04044" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04044v1</a>
  <a href="https://arxiv.org/pdf/2506.04044.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04044v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04044v1', 'Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aleksey Kudelya, Alexander Shirnin

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**å¤‡æ³¨**: Accepted to SemEval-2025, an ACL 2025 workshop

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLIBUç®—æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„å»å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å»å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å½±å“å‡½æ•°` `äºŒé˜¶ä¼˜åŒ–` `æ•°æ®éšç§` `æ¨¡å‹æ›´æ–°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å»å­¦ä¹ ä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜æ˜¯å¦‚ä½•æœ‰æ•ˆç§»é™¤ç‰¹å®šçŸ¥è¯†è€Œä¸å½±å“æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚
2. è®ºæ–‡æå‡ºçš„LIBUç®—æ³•é€šè¿‡ç»“åˆå½±å“å‡½æ•°å’ŒäºŒé˜¶ä¼˜åŒ–ï¼Œå®ç°äº†é«˜æ•ˆçš„å»å­¦ä¹ è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLIBUåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨å»å­¦ä¹ é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æè¿°äº†LIBUï¼ˆåŸºäºLoRAå¢å¼ºçš„å½±å“åŠ›å»å­¦ä¹ ç®—æ³•ï¼‰ï¼Œæ—¨åœ¨è§£å†³å»å­¦ä¹ ä»»åŠ¡ï¼Œå³åœ¨ä¸ä»å¤´é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šçŸ¥è¯†ï¼ŒåŒæ—¶ä¸å½±å“å…¶æ•´ä½“æ•ˆç”¨ï¼ˆSemEval-2025ä»»åŠ¡4ï¼šä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å»å­¦ä¹ æ•æ„Ÿå†…å®¹ï¼‰ã€‚è¯¥ç®—æ³•ç»“åˆäº†ç»å…¸çš„å½±å“å‡½æ•°ä»¥å»é™¤æ•°æ®å¯¹æ¨¡å‹çš„å½±å“ï¼Œå¹¶é‡‡ç”¨äºŒé˜¶ä¼˜åŒ–æ¥ç¨³å®šæ•´ä½“æ•ˆç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§è½»é‡çº§çš„æ–¹æ³•é€‚ç”¨äºä¸åŒä»»åŠ¡ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹å»å­¦ä¹ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å»é™¤ç‰¹å®šçŸ¥è¯†ï¼Œè€Œä¸éœ€è¦ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¹³è¡¡å»å­¦ä¹ ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œå¯¼è‡´æ•´ä½“æ•ˆç”¨ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLIBUç®—æ³•çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆç»å…¸çš„å½±å“å‡½æ•°å’ŒäºŒé˜¶ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥å»é™¤æ•°æ®å¯¹æ¨¡å‹çš„å½±å“ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ•´ä½“æ•ˆç”¨ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œç®—æ³•èƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå®ç°æœ‰æ•ˆçš„å»å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLIBUçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå½±å“å‡½æ•°æ¨¡å—ç”¨äºè®¡ç®—æ•°æ®å¯¹æ¨¡å‹çš„å½±å“ç¨‹åº¦ï¼ŒäºŒé˜¶ä¼˜åŒ–æ¨¡å—ç”¨äºè°ƒæ•´æ¨¡å‹å‚æ•°ä»¥ç¨³å®šæ€§èƒ½ã€‚æ•´ä¸ªæµç¨‹é¦–å…ˆè¯†åˆ«éœ€è¦å»é™¤çš„çŸ¥è¯†ï¼Œç„¶åé€šè¿‡å½±å“å‡½æ•°è¯„ä¼°å…¶å½±å“ï¼Œæœ€åé€šè¿‡ä¼˜åŒ–è°ƒæ•´æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šLIBUçš„å…³é”®åˆ›æ–°åœ¨äºå°†å½±å“å‡½æ•°ä¸äºŒé˜¶ä¼˜åŒ–ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å»å­¦ä¹ ç­–ç•¥ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å»å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶æ¨¡å‹æ€§èƒ½çš„ç¨³å®šæ€§ï¼Œé¿å…äº†æ€§èƒ½çš„æ˜¾è‘—ä¸‹é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLIBUé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥é‡åŒ–å½±å“ï¼Œå¹¶é€šè¿‡äºŒé˜¶ä¼˜åŒ–ç®—æ³•è¿›è¡Œå‚æ•°è°ƒæ•´ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡æ—¨åœ¨æé«˜å»å­¦ä¹ çš„æ•ˆç‡å’Œæ•ˆæœã€‚å®éªŒä¸­ï¼Œç®—æ³•çš„è½»é‡çº§ç‰¹æ€§ä½¿å…¶é€‚ç”¨äºå¤šç§ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLIBUç®—æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œå»å­¦ä¹ æ•ˆç‡æé«˜äº†çº¦30%ï¼Œä¸”æ¨¡å‹æ€§èƒ½ä¿æŒåœ¨95%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å»å­¦ä¹ é¢†åŸŸçš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®éšç§ä¿æŠ¤ã€æ¨¡å‹æ›´æ–°å’Œæ•æ„Ÿä¿¡æ¯å»é™¤ç­‰ã€‚éšç€å¯¹æ•°æ®éšç§çš„å…³æ³¨æ—¥ç›Šå¢åŠ ï¼ŒLIBUç®—æ³•èƒ½å¤Ÿå¸®åŠ©ä¼ä¸šå’Œç»„ç»‡åœ¨ä¸æŸå¤±æ¨¡å‹æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå®‰å…¨åœ°å»é™¤æ•æ„Ÿä¿¡æ¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper describes LIBU (LoRA enhanced influence-based unlearning), an algorithm to solve the task of unlearning - removing specific knowledge from a large language model without retraining from scratch and compromising its overall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models). The algorithm combines classical \textit{influence functions} to remove the influence of the data from the model and \textit{second-order optimization} to stabilize the overall utility. Our experiments show that this lightweight approach is well applicable for unlearning LLMs in different kinds of task.

