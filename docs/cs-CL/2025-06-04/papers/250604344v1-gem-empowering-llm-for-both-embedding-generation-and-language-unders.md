---
layout: default
title: GEM: Empowering LLM for both Embedding Generation and Language Understanding
---

# GEM: Empowering LLM for both Embedding Generation and Language Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04344" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04344v1</a>
  <a href="https://arxiv.org/pdf/2506.04344.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04344v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04344v1', 'GEM: Empowering LLM for both Embedding Generation and Language Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Caojin Zhang, Qiang Zhang, Ke Li, Sai Vidyaranya Nuthalapati, Benyu Zhang, Jason Liu, Serena Li, Lizhu Zhang, Xiangjun Fan

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGEMä»¥è§£å†³LLMåµŒå…¥ç”Ÿæˆä¸è¯­è¨€ç†è§£çš„çŸ›ç›¾é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åµŒå…¥ç”Ÿæˆ` `è‡ªç›‘ç£å­¦ä¹ ` `æ–‡æœ¬ç”Ÿæˆ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ£€ç´¢å¢å¼ºç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMåœ¨ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åµŒå…¥ç”Ÿæˆæ–¹é¢ä»ä¾èµ–äºç‹¬ç«‹æ¨¡å‹ï¼Œå¯¼è‡´ç³»ç»Ÿå¤æ‚æ€§å’Œç†è§£å·®å¼‚ã€‚
2. æœ¬æ–‡æå‡ºGEMæ–¹æ³•ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ ä½¿LLMèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬åµŒå…¥ï¼Œä¿æŒå…¶ç”Ÿæˆå’Œæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGEMåœ¨MTEBåŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†LLMçš„æ€§èƒ½ï¼Œè€Œå¯¹MMLUçš„å½±å“å¾®ä¹å…¶å¾®ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†è®¸å¤šåº”ç”¨ï¼ˆå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ä»ä¾èµ–äºå•ç‹¬çš„åµŒå…¥æ¨¡å‹ï¼Œè¿™å¯¼è‡´ç³»ç»Ÿå¤æ‚æ€§å¢åŠ ï¼Œå¹¶å¯èƒ½åœ¨æŸ¥è¯¢ç†è§£ä¸Šäº§ç”Ÿå·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„è‡ªç›‘ç£æ–¹æ³•â€”â€”ç”ŸæˆåµŒå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGEMï¼‰ï¼Œä½¿ä»»ä½•å¤§å‹è§£ç å™¨LLMèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬åµŒå…¥ï¼ŒåŒæ—¶ä¿æŒå…¶åŸæœ‰çš„æ–‡æœ¬ç”Ÿæˆå’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ–‡æœ¬ä¸­æ’å…¥æ–°çš„ç‰¹æ®Šæ ‡è®°ï¼Œå¹¶æ“æ§æ³¨æ„åŠ›æ©ç ç”Ÿæˆæ–‡æœ¬çš„æ‘˜è¦åµŒå…¥ï¼Œèƒ½å¤Ÿè½»æ¾é›†æˆåˆ°ç°æœ‰LLMsçš„åè®­ç»ƒæˆ–å¾®è°ƒé˜¶æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬åµŒå…¥åŸºå‡†ï¼ˆMTEBï¼‰ä¸Šæ˜¾è‘—æå‡äº†åŸæœ‰LLMsçš„æ€§èƒ½ï¼ŒåŒæ—¶å¯¹è‡ªç„¶è¯­è¨€å¤„ç†åŸºå‡†ï¼ˆMMLUï¼‰çš„å½±å“æœ€å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§£ç å™¨è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åµŒå…¥ç”Ÿæˆä¸­çš„å±€é™æ€§ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‹¬ç«‹çš„åµŒå…¥æ¨¡å‹ï¼Œå¯¼è‡´ç³»ç»Ÿå¤æ‚ä¸”å¯èƒ½äº§ç”Ÿç†è§£ä¸Šçš„ä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGEMæ–¹æ³•é€šè¿‡åœ¨æ–‡æœ¬ä¸­æ’å…¥ç‰¹æ®Šæ ‡è®°å¹¶æ“æ§æ³¨æ„åŠ›æ©ç ï¼Œä½¿LLMèƒ½å¤Ÿè‡ªç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬åµŒå…¥ï¼Œä»è€Œæ¶ˆé™¤å¯¹ç‹¬ç«‹åµŒå…¥æ¨¡å‹çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGEMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œåœ¨æ–‡æœ¬ä¸­æ’å…¥ç‰¹æ®Šæ ‡è®°ï¼›å…¶æ¬¡ï¼Œé€šè¿‡è°ƒæ•´æ³¨æ„åŠ›æ©ç ç”Ÿæˆæ–‡æœ¬çš„æ‘˜è¦åµŒå…¥ã€‚è¯¥æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰LLMsçš„åè®­ç»ƒæˆ–å¾®è°ƒè¿‡ç¨‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šGEMçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†åµŒå…¥ç”Ÿæˆä¸æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ç»“åˆï¼Œä½¿å¾—LLMä¸ä»…èƒ½ç”Ÿæˆæ–‡æœ¬ï¼Œè¿˜èƒ½æœ‰æ•ˆåœ°ç”Ÿæˆå…¶åµŒå…¥è¡¨ç¤ºï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•æˆªç„¶ä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼ŒGEMé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åµŒå…¥è´¨é‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†é€‚å½“çš„è°ƒæ•´ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„åµŒå…¥ä¸æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›çš„å…¼å®¹æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGEMåœ¨æ–‡æœ¬åµŒå…¥åŸºå‡†ï¼ˆMTEBï¼‰ä¸Šæ˜¾è‘—æå‡äº†åŸæœ‰LLMsçš„æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€å‚è€ƒåŸæ–‡ï¼‰ï¼Œè€Œå¯¹è‡ªç„¶è¯­è¨€å¤„ç†åŸºå‡†ï¼ˆMMLUï¼‰çš„å½±å“åˆ™ä¿æŒåœ¨æœ€å°èŒƒå›´å†…ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GEMæ–¹æ³•çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡å°†åµŒå…¥ç”Ÿæˆä¸è¯­è¨€ç†è§£èƒ½åŠ›ç»“åˆï¼ŒGEMèƒ½å¤Ÿæå‡ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ï¼Œç®€åŒ–æ¶æ„ï¼Œé™ä½å¤æ‚æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large decoder-only language models (LLMs) have achieved remarkable success in generation and reasoning tasks, where they generate text responses given instructions. However, many applications, e.g., retrieval augmented generation (RAG), still rely on separate embedding models to generate text embeddings, which can complicate the system and introduce discrepancies in understanding of the query between the embedding model and LLMs. To address this limitation, we propose a simple self-supervised approach, Generative Embedding large language Model (GEM), that enables any large decoder-only LLM to generate high-quality text embeddings while maintaining its original text generation and reasoning capabilities. Our method inserts new special token(s) into a text body, and generates summarization embedding of the text by manipulating the attention mask. This method could be easily integrated into post-training or fine tuning stages of any existing LLMs. We demonstrate the effectiveness of our approach by applying it to two popular LLM families, ranging from 1B to 8B parameters, and evaluating the transformed models on both text embedding benchmarks (MTEB) and NLP benchmarks (MMLU). The results show that our proposed method significantly improves the original LLMs on MTEB while having a minimal impact on MMLU. Our strong results indicate that our approach can empower LLMs with state-of-the-art text embedding capabilities while maintaining their original NLP performance

