---
layout: default
title: R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning
---

# R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04185" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04185v1</a>
  <a href="https://arxiv.org/pdf/2506.04185.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04185v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04185v1', 'R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, Limin Liu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**å¤‡æ³¨**: 16 pages, 3 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/QingFei1/R-Search)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR-Searchä»¥è§£å†³LLMæ¨ç†ä¸æœç´¢äº¤äº’ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†ä¸æœç´¢` `å¤šé‡å¥–åŠ±` `çŸ¥è¯†æ£€ç´¢` `æ™ºèƒ½é—®ç­”` `é€»è¾‘æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¨ç†ä¸æœç´¢çš„äº¤äº’ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´LLMsæ— æ³•æœ‰æ•ˆè¯†åˆ«æœ€ä½³çš„æ¨ç†-æœç´¢è½¨è¿¹ï¼Œå½±å“å“åº”è´¨é‡ã€‚
2. æœ¬æ–‡æå‡ºR-Searchæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°æ¨ç†ä¸æœç´¢çš„æ·±åº¦é›†æˆï¼ŒåŠ¨æ€å†³å®šä½•æ—¶æ£€ç´¢æˆ–æ¨ç†ï¼Œä¼˜åŒ–äº¤äº’è½¨è¿¹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒR-Searchåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰RAGåŸºçº¿ï¼Œé¢†åŸŸå†…æå‡æœ€é«˜è¾¾32.2%ï¼Œé¢†åŸŸå¤–æå‡è¾¾25.1%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥éª¤å’Œé•¿é“¾æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°†å…¶æ¨ç†èƒ½åŠ›æ‰©å±•åˆ°ä¸æœç´¢çš„æ·±åº¦äº¤äº’ä»ç„¶æ˜¯ä¸€ä¸ªéå¹³å‡¡çš„æŒ‘æˆ˜ï¼Œå› ä¸ºæ¨¡å‹å¾€å¾€æ— æ³•è¯†åˆ«æœ€ä½³çš„æ¨ç†-æœç´¢äº¤äº’è½¨è¿¹ï¼Œå¯¼è‡´å“åº”è´¨é‡ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†R-Searchï¼Œä¸€ä¸ªæ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æ¨ç†ä¸æœç´¢çš„é›†æˆï¼Œä½¿LLMsèƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå¤šæ­¥éª¤æ¨ç†å¹¶ä¸æœç´¢æ·±åº¦äº¤äº’ï¼Œé€šè¿‡å¤šé‡å¥–åŠ±ä¿¡å·å­¦ä¹ æœ€ä½³çš„æ¨ç†æœç´¢äº¤äº’è½¨è¿¹ï¼Œä»è€Œæé«˜å¤æ‚é€»è¾‘å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„å“åº”è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR-Searchåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆè¿›çš„RAGåŸºçº¿ï¼Œæå‡å¹…åº¦æœ€é«˜å¯è¾¾32.2%ï¼ˆé¢†åŸŸå†…ï¼‰å’Œ25.1%ï¼ˆé¢†åŸŸå¤–ï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä¸æœç´¢äº¤äº’ä¸­çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆè¯†åˆ«æœ€ä½³çš„æ¨ç†-æœç´¢äº¤äº’è½¨è¿¹ï¼Œå¯¼è‡´å“åº”è´¨é‡ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šR-Searchæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°æ¨ç†ä¸æœç´¢çš„æ·±åº¦é›†æˆï¼Œå…è®¸æ¨¡å‹è‡ªä¸»å†³å®šä½•æ—¶è¿›è¡Œæ£€ç´¢æˆ–æ¨ç†ï¼Œå¹¶é€šè¿‡å¤šé‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–äº¤äº’è½¨è¿¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šR-Searchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯æ¨ç†æ¨¡å—å’Œæœç´¢æ¨¡å—çš„é›†æˆï¼Œå…¶æ¬¡æ˜¯åŠ¨æ€å†³ç­–æœºåˆ¶ï¼Œæœ€åæ˜¯å¤šé˜¶æ®µã€å¤šç±»å‹å¥–åŠ±çš„è®¾è®¡ï¼Œä»¥ä¼˜åŒ–æ¨ç†-æœç´¢è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šR-Searchçš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡å¤šé‡å¥–åŠ±ä¿¡å·è”åˆä¼˜åŒ–æ¨ç†ä¸æœç´¢çš„äº¤äº’è½¨è¿¹ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•ä¸€å¥–åŠ±æœºåˆ¶æ˜¾è‘—ä¸åŒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¼•å¯¼æ¨¡å‹å­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒR-Searché‡‡ç”¨äº†å¤šé˜¶æ®µçš„å¥–åŠ±æœºåˆ¶ï¼Œç»“åˆäº†ä¸åŒç±»å‹çš„å¥–åŠ±ä¿¡å·ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æ¨ç†ä¸æœç´¢ä¹‹é—´çš„å¹³è¡¡ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æå‡æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒR-Searchåœ¨é¢†åŸŸå†…çš„æ€§èƒ½æå‡æœ€é«˜å¯è¾¾32.2%ï¼Œåœ¨é¢†åŸŸå¤–çš„æå‡å¹…åº¦è¾¾åˆ°25.1%ã€‚è¿™äº›ç»“æœæ˜¾ç¤ºå‡ºR-Searchåœ¨æ¨ç†ä¸æœç´¢é›†æˆæ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œè¶…è¶Šäº†ç°æœ‰çš„RAGåŸºçº¿ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

R-Searchçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¤æ‚é€»è¾‘æ¨ç†å’ŒçŸ¥è¯†æ£€ç´¢çš„é¢†åŸŸï¼Œå¦‚æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ„å»ºå’Œè‡ªåŠ¨åŒ–å®¢æœç­‰ã€‚é€šè¿‡æå‡LLMsçš„æ¨ç†ä¸æœç´¢èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„å¯¹è¯ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have notably progressed in multi-step and long-chain reasoning. However, extending their reasoning capabilities to encompass deep interactions with search remains a non-trivial challenge, as models often fail to identify optimal reasoning-search interaction trajectories, resulting in suboptimal responses. We propose R-Search, a novel reinforcement learning framework for Reasoning-Search integration, designed to enable LLMs to autonomously execute multi-step reasoning with deep search interaction, and learn optimal reasoning search interaction trajectories via multi-reward signals, improving response quality in complex logic- and knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when to retrieve or reason, while globally integrating key evidence to enhance deep knowledge interaction between reasoning and search. During RL training, R-Search provides multi-stage, multi-type rewards to jointly optimize the reasoning-search trajectory. Experiments on seven datasets show that R-Search outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1% (out-of-domain). The code and data are available at https://github.com/QingFei1/R-Search.

