---
layout: default
title: From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation
---

# From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16024" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16024v1</a>
  <a href="https://arxiv.org/pdf/2506.16024.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16024v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16024v1', 'From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhihan Guo, Jiele Wu, Wenqian Cui, Yifei Zhang, Minda Hu, Yufei Wang, Irwin King

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProxyRewardä»¥æå‡é•¿æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æœ¬ç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±ä¿¡å·` `æ•°æ®é›†ç”Ÿæˆ` `ä¿¡æ¯è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é•¿æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ä¸»è¦ä¾èµ–ä¸€èˆ¬æ€§è¯„ä¼°ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œä¿¡æ¯å®Œæ•´æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºProxyRewardæ¡†æ¶ï¼Œé€šè¿‡ç®€å•æç¤ºç”Ÿæˆæ•°æ®é›†ï¼Œå¹¶æä¾›é’ˆå¯¹ç‰¹å®šé—®é¢˜çš„å¥–åŠ±ä¿¡å·ï¼Œæå‡ç”Ÿæˆæ¨¡å‹çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProxyRewardåœ¨Open-LTGä»»åŠ¡ä¸Šè¶…è¶Šäº†GPT-4-Turboï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ï¼Œå¹¶ä¼˜äºLLM-as-a-Judgeæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆOpen-LTGï¼‰æ–¹é¢çš„ç ”ç©¶ä»æ˜¾ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹é«˜è´¨é‡å‚è€ƒæ•°æ®çš„æƒ…å†µä¸‹ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºä¸€èˆ¬æ€§è¯„ä¼°ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¯¼è‡´ç”Ÿæˆå‡†ç¡®æ€§å—é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ProxyRewardï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼ŒåŒ…å«æ•°æ®é›†ç”Ÿæˆå’Œå¥–åŠ±ä¿¡å·è®¡ç®—æ–¹æ³•ã€‚é€šè¿‡ç®€å•æç¤ºç”ŸæˆProxyRewardæ•°æ®é›†ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨åˆ›å»ºå†…å®¹ï¼Œé¿å…äº†å¤§é‡æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚åŒæ—¶ï¼ŒProxyRewardä¿¡å·ä¸ºç‰¹å®šé—®é¢˜æä¾›äº†é’ˆå¯¹æ€§çš„ä¿¡æ¯å…¨é¢æ€§å’Œå‡†ç¡®æ€§è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProxyRewardåœ¨Open-LTGä»»åŠ¡ä¸Šè¶…è¶Šäº†GPT-4-Turboï¼Œæ˜¾è‘—æå‡äº†20%çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ç¼ºä¹é«˜è´¨é‡å‚è€ƒæ•°æ®çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–ä¸€èˆ¬æ€§è¯„ä¼°ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œä¿¡æ¯å®Œæ•´æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºProxyRewardæ¡†æ¶ï¼Œé€šè¿‡ç®€å•æç¤ºç”Ÿæˆæ•°æ®é›†ï¼Œé¿å…äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒåŒæ—¶æä¾›é’ˆå¯¹æ€§å¥–åŠ±ä¿¡å·ï¼Œä»¥æå‡ç”Ÿæˆæ¨¡å‹çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šProxyRewardæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šProxyRewardæ•°æ®é›†ç”Ÿæˆå’ŒProxyRewardä¿¡å·è®¡ç®—ã€‚æ•°æ®é›†ç”Ÿæˆé€šè¿‡ç®€å•çš„æç¤ºè‡ªåŠ¨åˆ›å»ºå†…å®¹ï¼Œè€Œä¿¡å·è®¡ç®—åˆ™é’ˆå¯¹ç‰¹å®šé—®é¢˜è¯„ä¼°ç”Ÿæˆå†…å®¹çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šProxyRewardçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æ•°æ®é›†ç”Ÿæˆå’Œå¥–åŠ±ä¿¡å·è®¡ç®—æ–¹æ³•ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–ä¸€èˆ¬æ€§è¯„ä¼°ï¼Œæä¾›äº†æ›´ä¸ºç²¾å‡†çš„åé¦ˆæœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ProxyRewardæ¡†æ¶ä¸­ï¼Œæ•°æ®é›†ç”Ÿæˆé‡‡ç”¨ç®€å•æç¤ºï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§å’Œç›¸å…³æ€§ï¼›å¥–åŠ±ä¿¡å·è®¡ç®—åˆ™ä¸“æ³¨äºä¿¡æ¯çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç‰¹å®šé—®é¢˜ä¸Šçš„è¡¨ç°å¾—åˆ°æœ‰æ•ˆæå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒProxyRewardåœ¨Open-LTGä»»åŠ¡ä¸Šè¶…è¶Šäº†GPT-4-Turboï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ã€‚æ­¤å¤–ï¼ŒProxyRewardè¿˜ä¼˜äºä¼ ç»Ÿçš„LLM-as-a-Judgeæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€å†…å®¹åˆ›ä½œå’Œå®¢æˆ·æœåŠ¡ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”Ÿæˆæ›´ä¸ºå‡†ç¡®å’Œä¸°å¯Œçš„é•¿æ–‡æœ¬å†…å®¹ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼ŒProxyRewardæ¡†æ¶æœ‰æœ›åœ¨æ›´å¤šå¤æ‚çš„å¼€æ”¾å¼é—®é¢˜ç”Ÿæˆä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current research on long-form context in Large Language Models (LLMs) primarily focuses on the understanding of long-contexts, the Open-ended Long Text Generation (Open-LTG) remains insufficiently explored. Training a long-context generation model requires curation of gold standard reference data, which is typically nonexistent for informative Open-LTG tasks. However, previous methods only utilize general assessments as reward signals, which limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative reinforcement learning (RL) based framework, which includes a dataset and a reward signal computation method. Firstly, ProxyReward Dataset generation is accomplished through simple prompts that enables the model to create automatically, obviating extensive labeled data or significant manual effort. Secondly, ProxyReward Signal offers a targeted evaluation of information comprehensiveness and accuracy for specific questions. The experimental results indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can significantly enhance performance by 20% on the Open-LTG task when training widely used open-source models, while also surpassing the LLM-as-a-Judge approach. Our work presents effective methods to enhance the ability of LLMs to address complex open-ended questions posed by human.

