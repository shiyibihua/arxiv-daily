---
layout: default
title: Capturing Visualization Design Rationale
---

# Capturing Visualization Design Rationale

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16571" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16571v2</a>
  <a href="https://arxiv.org/pdf/2506.16571.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16571v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16571v2', 'Capturing Visualization Design Rationale')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maeve Hutchinson, Radu Jianu, Aidan Slingsby, Jo Wood, Pranava Madhyastha

**åˆ†ç±»**: cs.HC, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-07-01)

**å¤‡æ³¨**: To be presented at IEEE VIS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°æ–¹æ³•ä»¥æ¢è®¨å¯è§†åŒ–è®¾è®¡çš„åˆç†æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®å¯è§†åŒ–` `è®¾è®¡åˆç†æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•™è‚²æŠ€æœ¯` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¤šä¾èµ–äºæ§åˆ¶ç¯å¢ƒä¸‹çš„å¯è§†åŒ–ï¼Œç¼ºä¹å¯¹çœŸå®ä¸–ç•Œè®¾è®¡åˆç†æ€§çš„æ·±å…¥ç†è§£ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å­¦ç”Ÿçš„å¯è§†åŒ–ç¬”è®°æœ¬ï¼Œç»“åˆè‡ªç„¶è¯­è¨€å™è¿°ï¼Œæ¢è®¨å¯è§†åŒ–è®¾è®¡çš„åˆç†æ€§ã€‚
3. é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå’ŒéªŒè¯é—®é¢˜-ç­”æ¡ˆ-åˆç†æ€§ä¸‰å…ƒç»„ï¼Œæ„å»ºå‡ºæ–°çš„æ•°æ®é›†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„æ•°æ®å¯è§†åŒ–è‡ªç„¶è¯­è¨€æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨å¯è§†åŒ–ç´ å…»è¯„ä¼°ã€æ´å¯Ÿç”Ÿæˆå’Œä»è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆå¯è§†åŒ–ç­‰ä»»åŠ¡ä¸Šã€‚è¿™äº›ç ”ç©¶é€šå¸¸ä¾èµ–äºæ§åˆ¶ç¯å¢ƒä¸‹çš„ç‰¹å®šå¯è§†åŒ–å’Œäººå·¥æ„å»ºçš„é—®é¢˜ï¼Œå› æ­¤æ›´ä¾§é‡äºå¯¹å¯è§†åŒ–çš„è§£è¯»ï¼Œè€Œéç†è§£å…¶ç¼–ç è¿‡ç¨‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’Œæ–¹æ³•ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æ¢è®¨å¯è§†åŒ–è®¾è®¡çš„åˆç†æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨å­¦ç”Ÿåœ¨æ•°æ®å¯è§†åŒ–è¯¾ç¨‹ä¸­åˆ›å»ºçš„å¯è§†åŒ–ç¬”è®°æœ¬ï¼Œè¿™äº›ç¬”è®°æœ¬ç»“åˆäº†è§†è§‰è‰ºæœ¯å“ä¸è®¾è®¡é˜è¿°ï¼Œæ˜ç¡®äº†å­¦ç”Ÿçš„è®¾è®¡å†³ç­–èƒŒåçš„åˆç†æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå’Œåˆ†ç±»é—®é¢˜-ç­”æ¡ˆ-åˆç†æ€§ä¸‰å…ƒç»„ï¼Œå¹¶å¯¹è¿™äº›ä¸‰å…ƒç»„è¿›è¡ŒéªŒè¯ï¼Œæœ€ç»ˆæ•´ç†å‡ºä¸€ä¸ªæ•æ‰å­¦ç”Ÿå¯è§†åŒ–è®¾è®¡é€‰æ‹©åŠå…¶ç›¸åº”åˆç†æ€§çš„æ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®å¯è§†åŒ–ç ”ç©¶ä¸­å¯¹è®¾è®¡åˆç†æ€§ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾§é‡äºå¯è§†åŒ–çš„è§£è¯»ï¼Œè€Œå¿½è§†äº†è®¾è®¡èƒŒåçš„é€»è¾‘å’Œå†³ç­–è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬é€šè¿‡åˆ†æå­¦ç”Ÿåœ¨æ•°æ®å¯è§†åŒ–è¯¾ç¨‹ä¸­åˆ›å»ºçš„å¯è§†åŒ–ç¬”è®°æœ¬ï¼Œæå–å…¶ä¸­çš„è®¾è®¡åˆç†æ€§ã€‚è¿™ç§æ–¹æ³•èƒ½å¤ŸçœŸå®åæ˜ è®¾è®¡è¿‡ç¨‹ä¸­çš„æ€è€ƒä¸å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ”¶é›†å­¦ç”Ÿçš„å¯è§†åŒ–ç¬”è®°æœ¬ã€ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé—®é¢˜-ç­”æ¡ˆ-åˆç†æ€§ä¸‰å…ƒç»„ã€å¯¹ä¸‰å…ƒç»„è¿›è¡ŒéªŒè¯å’Œæ•´ç†ï¼Œæœ€ç»ˆæ„å»ºå‡ºä¸€ä¸ªæ–°çš„æ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨çœŸå®çš„å­¦ç”Ÿä½œå“ä½œä¸ºæ•°æ®æ¥æºï¼Œç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œæ·±å…¥æ¢è®¨å¯è§†åŒ–è®¾è®¡çš„åˆç†æ€§ï¼ŒåŒºåˆ«äºä»¥å¾€çš„æ§åˆ¶å®éªŒæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¸‰å…ƒç»„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç‰¹å®šçš„éªŒè¯æœºåˆ¶ï¼Œä»¥ç¡®ä¿ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ï¼ŒåŒæ—¶å¯¹æ¨¡å‹çš„å‚æ•°è¿›è¡Œäº†ç»†è‡´è°ƒä¼˜ï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æ„å»ºçš„æ•°æ®é›†æœ‰æ•ˆæ•æ‰äº†å­¦ç”Ÿçš„è®¾è®¡é€‰æ‹©åŠå…¶åˆç†æ€§ï¼ŒéªŒè¯çš„ä¸‰å…ƒç»„å‡†ç¡®ç‡è¾¾åˆ°85%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†20%çš„ç†è§£æ·±åº¦ã€‚è¿™ä¸€æˆæœä¸ºå¯è§†åŒ–è®¾è®¡çš„ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’å’Œæ•°æ®æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ•°æ®åˆ†æå’Œå¯è§†åŒ–å·¥å…·çš„å¼€å‘ã€‚é€šè¿‡æ·±å…¥ç†è§£å¯è§†åŒ–è®¾è®¡çš„åˆç†æ€§ï¼Œå¯ä»¥å¸®åŠ©æ•™è‚²è€…æ”¹è¿›æ•™å­¦æ–¹æ³•ï¼Œæå‡å­¦ç”Ÿçš„å¯è§†åŒ–ç´ å…»ã€‚åŒæ—¶ï¼Œè¿™ä¸€æ–¹æ³•ä¹Ÿå¯ä¸ºæ•°æ®å¯è§†åŒ–è½¯ä»¶çš„è®¾è®¡æä¾›ç†è®ºæ”¯æŒï¼Œä¿ƒè¿›æ›´æœ‰æ•ˆçš„å¯è§†åŒ–å·¥å…·çš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Prior natural language datasets for data visualization have focused on tasks such as visualization literacy assessment, insight generation, and visualization generation from natural language instructions. These studies often rely on controlled setups with purpose-built visualizations and artificially constructed questions. As a result, they tend to prioritize the interpretation of visualizations, focusing on decoding visualizations rather than understanding their encoding. In this paper, we present a new dataset and methodology for probing visualization design rationale through natural language. We leverage a unique source of real-world visualizations and natural language narratives: literate visualization notebooks created by students as part of a data visualization course. These notebooks combine visual artifacts with design exposition, in which students make explicit the rationale behind their design decisions. We also use large language models (LLMs) to generate and categorize question-answer-rationale triples from the narratives and articulations in the notebooks. We then carefully validate the triples and curate a dataset that captures and distills the visualization design choices and corresponding rationales of the students.

