---
layout: default
title: Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models
---

# Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16447" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16447v1</a>
  <a href="https://arxiv.org/pdf/2506.16447.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16447v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16447v1', 'Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Biao Yi, Tiansheng Huang, Sishuo Chen, Tong Li, Zheli Liu, Zhixuan Chu, Yiming Li

**åˆ†ç±»**: cs.CR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: Accepted at ICLR 2025

**æœŸåˆŠ**: Proceedings of The Thirteenth International Conference on Learning Representations (ICLR 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBEATä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„åé—¨ä¸å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åé—¨æ”»å‡»` `å¤§å‹è¯­è¨€æ¨¡å‹` `é»‘ç®±é˜²å¾¡` `å®‰å…¨å¯¹é½` `æ¢æµ‹è¿æ¥æ•ˆåº”` `æ¨¡å‹å®‰å…¨` `æ¶æ„æ¢æµ‹` `æµè¡Œè¶Šç‹±æ”»å‡»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åé—¨ä¸å¯¹é½æ”»å‡»ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å—åˆ°ä¸¥é‡å¨èƒï¼Œç°æœ‰é˜²å¾¡æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè¯†åˆ«å’Œåº”å¯¹è¿™ç§æ”»å‡»ã€‚
2. æœ¬æ–‡æå‡ºBEATï¼Œé€šè¿‡æ¢æµ‹è¿æ¥æ•ˆåº”æ¥è¯†åˆ«è¾“å…¥æ˜¯å¦è¢«è§¦å‘ï¼Œä»è€Œå®ç°å¯¹åé—¨çš„é˜²å¾¡ï¼Œå…‹æœäº†é»‘ç®±è®¿é—®çš„é™åˆ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBEATåœ¨å¤šç§åé—¨æ”»å‡»åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½åé—¨æ¨¡å‹çš„æ‹’ç»ç‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åé—¨ä¸å¯¹é½æ”»å‡»å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨æ€§æ„æˆäº†ä¸¥é‡å¨èƒï¼Œè¿™ç§æ”»å‡»é€šè¿‡éšè—è§¦å‘å™¨æ‚„ç„¶ç ´åå®‰å…¨å¯¹é½ï¼Œå¹¶ä¸”èƒ½å¤Ÿè§„é¿æ­£å¸¸çš„å®‰å…¨å®¡è®¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBEATçš„é»‘ç®±é˜²å¾¡æ–¹æ³•ï¼Œæ—¨åœ¨æ£€æµ‹æ¨ç†è¿‡ç¨‹ä¸­çš„è§¦å‘æ ·æœ¬ï¼Œä»è€Œå…³é—­åé—¨ã€‚BEATçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨æ¢æµ‹è¿æ¥æ•ˆåº”ï¼Œé€šè¿‡æµ‹é‡è¾“å…¥ä¸æ¢æµ‹æ ·æœ¬è¿æ¥å‰åçš„è¾“å‡ºåˆ†å¸ƒæ‰­æ›²ç¨‹åº¦æ¥è¯†åˆ«è¾“å…¥æ˜¯å¦è¢«è§¦å‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBEATåœ¨å¤šç§åé—¨æ”»å‡»å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„é˜²å¾¡æ•ˆæœï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆæŠµå¾¡æµè¡Œçš„è¶Šç‹±æ”»å‡»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åé—¨ä¸å¯¹é½æ”»å‡»é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹æ ·æœ¬ä¾èµ–æ€§æ”»å‡»ç›®æ ‡æ—¶è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æœ‰æ•ˆè¯†åˆ«è¢«è§¦å‘çš„è¾“å…¥æ ·æœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBEATçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¢æµ‹è¿æ¥æ•ˆåº”ï¼Œé€šè¿‡åˆ†æè¾“å…¥ä¸æ¢æµ‹æ ·æœ¬è¿æ¥å‰åçš„è¾“å‡ºåˆ†å¸ƒå˜åŒ–ï¼Œæ¥åˆ¤æ–­è¾“å…¥æ˜¯å¦è¢«è§¦å‘ã€‚è¿™ç§æ–¹æ³•ä»æ‹’ç»ä¿¡å·çš„è§’åº¦å‡ºå‘ï¼Œé¿å…äº†æ ·æœ¬ç‰¹å®šçš„æˆåŠŸæ”»å‡»è¡Œä¸ºçš„å¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBEATçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥æ ·æœ¬çš„æ¥æ”¶ã€æ¢æµ‹æ ·æœ¬çš„ç”Ÿæˆã€è¾“å‡ºåˆ†å¸ƒçš„æµ‹é‡å’Œè§¦å‘æ ·æœ¬çš„è¯†åˆ«å››ä¸ªä¸»è¦æ¨¡å—ã€‚é€šè¿‡å¤šæ¬¡é‡‡æ ·æ¥è¿‘ä¼¼è¾“å‡ºåˆ†å¸ƒï¼Œä»è€Œå®ç°é»‘ç®±é˜²å¾¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šBEATçš„ä¸»è¦åˆ›æ–°åœ¨äºæ¢æµ‹è¿æ¥æ•ˆåº”çš„å¼•å…¥ï¼Œè¿™ä¸€æ•ˆåº”ä½¿å¾—è¿æ¥åçš„è§¦å‘æ ·æœ¬æ˜¾è‘—é™ä½äº†åé—¨æ¨¡å‹å¯¹æ¶æ„æ¢æµ‹çš„æ‹’ç»ç‡ï¼Œæä¾›äº†ä¸€ç§æ–°çš„é˜²å¾¡æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒBEATé‡‡ç”¨äº†å¤šæ¬¡é‡‡æ ·ç­–ç•¥æ¥æé«˜è¾“å‡ºåˆ†å¸ƒçš„è¿‘ä¼¼ç²¾åº¦ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¢æµ‹æ•ˆæœï¼Œç¡®ä¿åœ¨é»‘ç®±ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚å®éªŒä¸­ä½¿ç”¨äº†å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬é—­æºçš„GPT-3.5-turboï¼ŒéªŒè¯äº†æ–¹æ³•çš„é€šç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šç§åé—¨æ”»å‡»å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å®éªŒä¸­ï¼ŒBEATå±•ç¤ºäº†ä¼˜å¼‚çš„é˜²å¾¡æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½äº†åé—¨æ¨¡å‹çš„æ‹’ç»ç‡ï¼ŒéªŒè¯äº†å…¶åœ¨é»‘ç®±ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚å…·ä½“å®éªŒç»“æœè¡¨æ˜ï¼ŒBEATåœ¨å¤„ç†é—­æºæ¨¡å‹æ—¶ä¹Ÿèƒ½ä¿æŒè‰¯å¥½çš„é˜²å¾¡æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨é˜²æŠ¤ï¼Œå°¤å…¶æ˜¯åœ¨æä¾›è¯­è¨€æ¨¡å‹æœåŠ¡çš„å•†ä¸šç¯å¢ƒä¸­ã€‚BEATçš„é˜²å¾¡æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆä¿æŠ¤ç”¨æˆ·æ•°æ®å’Œæ¨¡å‹å®‰å…¨ï¼Œé˜²æ­¢æ¶æ„æ”»å‡»å¯¹æ¨¡å‹çš„å½±å“ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Backdoor unalignment attacks against Large Language Models (LLMs) enable the stealthy compromise of safety alignment using a hidden trigger while evading normal safety auditing. These attacks pose significant threats to the applications of LLMs in the real-world Large Language Model as a Service (LLMaaS) setting, where the deployed model is a fully black-box system that can only interact through text. Furthermore, the sample-dependent nature of the attack target exacerbates the threat. Instead of outputting a fixed label, the backdoored LLM follows the semantics of any malicious command with the hidden trigger, significantly expanding the target space. In this paper, we introduce BEAT, a black-box defense that detects triggered samples during inference to deactivate the backdoor. It is motivated by an intriguing observation (dubbed the probe concatenate effect), where concatenated triggered samples significantly reduce the refusal rate of the backdoored LLM towards a malicious probe, while non-triggered samples have little effect. Specifically, BEAT identifies whether an input is triggered by measuring the degree of distortion in the output distribution of the probe before and after concatenation with the input. Our method addresses the challenges of sample-dependent targets from an opposite perspective. It captures the impact of the trigger on the refusal signal (which is sample-independent) instead of sample-specific successful attack behaviors. It overcomes black-box access limitations by using multiple sampling to approximate the output distribution. Extensive experiments are conducted on various backdoor attacks and LLMs (including the closed-source GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense. Besides, we also preliminarily verify that BEAT can effectively defend against popular jailbreak attacks, as they can be regarded as 'natural backdoors'.

