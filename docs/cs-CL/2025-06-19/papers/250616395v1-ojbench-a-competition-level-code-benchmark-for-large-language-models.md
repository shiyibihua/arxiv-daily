---
layout: default
title: OJBench: A Competition Level Code Benchmark For Large Language Models
---

# OJBench: A Competition Level Code Benchmark For Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16395" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16395v1</a>
  <a href="https://arxiv.org/pdf/2506.16395.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16395v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16395v1', 'OJBench: A Competition Level Code Benchmark For Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhexu Wang, Yiping Liu, Yejie Wang, Wenyang He, Bofei Gao, Muxi Diao, Yanxu Chen, Kelin Fu, Flood Sung, Zhilin Yang, Tianyu Liu, Weiran Xu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: 9 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOJBenchä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç æ¨ç†` `OJBench` `ç¼–ç¨‹ç«èµ›` `æ¨¡å‹è¯„ä¼°` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä»£ç åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç«äº‰çº§åˆ«çš„ä»£ç æ¨ç†èƒ½åŠ›ï¼Œå­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚
2. OJBenchæ˜¯ä¸€ä¸ªæ–°æå‡ºçš„åŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LLMsåœ¨ç¼–ç¨‹ç«èµ›ä¸­çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«232ä¸ªç«èµ›é—®é¢˜ã€‚
3. å¯¹37ä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨ç†å¯¼å‘æ¨¡å‹åœ¨è§£å†³é«˜éš¾åº¦ç«èµ›é—®é¢˜æ—¶ä»é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦å’Œä»£ç æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»£ç åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°è¿™äº›èƒ½åŠ›çš„å…¨é¢æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç«äº‰çº§åˆ«æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†OJBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMsç«äº‰çº§åˆ«ä»£ç æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†ã€‚OJBenchåŒ…å«æ¥è‡ªNOIå’ŒICPCçš„232ä¸ªç¼–ç¨‹ç«èµ›é—®é¢˜ï¼Œä¸ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†æ›´ä¸¥æ ¼çš„æµ‹è¯•ã€‚æˆ‘ä»¬å¯¹37ä¸ªæ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬å°é—­æºå’Œå¼€æ”¾æºæ¨¡å‹ï¼Œæ¨ç†å¯¼å‘å’Œéæ¨ç†å¯¼å‘æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨ç†å¯¼å‘æ¨¡å‹ï¼Œå¦‚o4-miniå’ŒGemini-2.5-pro-expï¼Œåœ¨é¢å¯¹é«˜åº¦æŒ‘æˆ˜çš„ç«èµ›çº§åˆ«é—®é¢˜æ—¶ä¹Ÿè¡¨ç°ä¸ä½³ï¼Œè¿™çªæ˜¾äº†æ¨¡å‹åœ¨ç«äº‰çº§åˆ«ä»£ç æ¨ç†ä¸­é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä»£ç åŸºå‡†æµ‹è¯•æ— æ³•æœ‰æ•ˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç«äº‰çº§åˆ«ä»£ç æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æµ‹è¯•æ·±åº¦å’Œå¹¿åº¦ä¸Šå­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOJBenché€šè¿‡å¼•å…¥232ä¸ªç¼–ç¨‹ç«èµ›é—®é¢˜ï¼Œæä¾›äº†ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„æµ‹è¯•ç¯å¢ƒï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOJBenchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é—®é¢˜é€‰æ‹©ã€æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é—®é¢˜é€‰æ‹©ä»NOIå’ŒICPCä¸­æå–ï¼Œæ¨¡å‹è¯„ä¼°åˆ™æ¶µç›–äº†å¤šç§ç±»å‹çš„è¯­è¨€æ¨¡å‹ï¼Œæœ€åé€šè¿‡å¯¹æ¯”åˆ†æå¾—å‡ºç»“è®ºã€‚

**å…³é”®åˆ›æ–°**ï¼šOJBenchçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶é’ˆå¯¹ç«äº‰çº§åˆ«çš„è®¾è®¡ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°æ·±åº¦å’Œå¹¿åº¦ä¸Šçš„ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜éš¾åº¦é—®é¢˜çš„é€‰æ‹©ä¸Šã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡OJBenchæ—¶ï¼Œé€‰æ‹©äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¼–ç¨‹é—®é¢˜ï¼Œå¹¶ç¡®ä¿è¦†ç›–ä¸åŒç±»å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¯„ä¼°è¿‡ç¨‹ä¸­é‡‡ç”¨äº†å¤šç§æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä½¿ç”¨äº†æœ€å…ˆè¿›çš„æ¨ç†å¯¼å‘æ¨¡å‹ï¼Œå¦‚o4-miniå’ŒGemini-2.5-pro-expï¼Œä½†åœ¨è§£å†³OJBenchä¸­çš„é«˜éš¾åº¦ç«èµ›é—®é¢˜æ—¶ï¼Œè¿™äº›æ¨¡å‹çš„è¡¨ç°ä»ç„¶ä¸å°½å¦‚äººæ„ï¼Œè¡¨æ˜å½“å‰æŠ€æœ¯åœ¨ç«äº‰çº§åˆ«ä»£ç æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OJBenchçš„æå‡ºä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æä¾›äº†ä¸€ä¸ªæ–°çš„æ ‡å‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼–ç¨‹å’Œç®—æ³•é¢†åŸŸã€‚å…¶æ½œåœ¨åº”ç”¨åŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–ç¼–ç¨‹åŠ©æ‰‹å’Œä»£ç å®¡æŸ¥å·¥å…·ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…å’Œç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨ç¼–ç¨‹é¢†åŸŸçš„åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in large language models (LLMs) have demonstrated significant progress in math and code reasoning capabilities. However, existing code benchmark are limited in their ability to evaluate the full spectrum of these capabilities, particularly at the competitive level. To bridge this gap, we introduce OJBench, a novel and challenging benchmark designed to assess the competitive-level code reasoning abilities of LLMs. OJBench comprises 232 programming competition problems from NOI and ICPC, providing a more rigorous test of models' reasoning skills. We conducted a comprehensive evaluation using OJBench on 37 models, including both closed-source and open-source models, reasoning-oriented and non-reasoning-oriented models. Our results indicate that even state-of-the-art reasoning-oriented models, such as o4-mini and Gemini-2.5-pro-exp, struggle with highly challenging competition-level problems. This highlights the significant challenges that models face in competitive-level code reasoning.

