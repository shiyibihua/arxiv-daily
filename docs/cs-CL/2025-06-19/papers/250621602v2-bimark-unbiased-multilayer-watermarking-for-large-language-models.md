---
layout: default
title: BiMark: Unbiased Multilayer Watermarking for Large Language Models
---

# BiMark: Unbiased Multilayer Watermarking for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21602" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21602v2</a>
  <a href="https://arxiv.org/pdf/2506.21602.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21602v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21602v2', 'BiMark: Unbiased Multilayer Watermarking for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyan Feng, He Zhang, Yanjun Zhang, Leo Yu Zhang, Shirui Pan

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-08-25)

**å¤‡æ³¨**: This paper is accepted by International Conference on Machine Learning (ICML) 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBiMarkä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æ°´å°è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ°´å°æŠ€æœ¯` `æ–‡æœ¬ç”Ÿæˆ` `ä¿¡æ¯åµŒå…¥` `æ¨¡å‹æ— å…³æ£€æµ‹` `å¤šå±‚æ¶æ„` `æ–‡æœ¬è´¨é‡ä¿æŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ°´å°æ–¹æ³•éš¾ä»¥åŒæ—¶æ»¡è¶³æ–‡æœ¬è´¨é‡ä¿æŒã€æ¨¡å‹æ— å…³æ£€æµ‹å’Œä¿¡æ¯åµŒå…¥èƒ½åŠ›ç­‰å…³é”®è¦æ±‚ï¼Œå½±å“å®é™…åº”ç”¨ã€‚
2. BiMarké€šè¿‡æ— åé‡çš„ä½ç¿»è½¬é‡åŠ æƒæœºåˆ¶ã€å¤šå±‚æ¶æ„å’Œå¤šä½æ°´å°ç¼–ç æ–¹æ³•ï¼Œè§£å†³äº†æ°´å°æŠ€æœ¯çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒBiMarkåœ¨çŸ­æ–‡æœ¬æå–ç‡ä¸Šæ¯”ç°æœ‰æ–¹æ³•æé«˜äº†30%ï¼Œä¸”åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸éæ°´å°æ–‡æœ¬ç›¸å½“çš„è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆæ–‡æœ¬çš„çœŸå®æ€§å¼•å‘äº†å¹¿æ³›å…³æ³¨ï¼ŒäºŸéœ€å¯é çš„è¯†åˆ«æœºåˆ¶ã€‚æ°´å°æŠ€æœ¯ä½œä¸ºä¸€ç§æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œç„¶è€Œç°æœ‰æ–¹æ³•åœ¨æ–‡æœ¬è´¨é‡ä¿æŒã€æ¨¡å‹æ— å…³æ£€æµ‹å’Œä¿¡æ¯åµŒå…¥èƒ½åŠ›ç­‰æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†BiMarkï¼Œä¸€ä¸ªæ–°é¢–çš„æ°´å°æ¡†æ¶ï¼Œé€šè¿‡ä¸‰é¡¹å…³é”®åˆ›æ–°å®ç°äº†è¿™äº›è¦æ±‚ï¼š1ï¼‰æ— åé‡çš„ä½ç¿»è½¬é‡åŠ æƒæœºåˆ¶ï¼Œå®ç°æ¨¡å‹æ— å…³æ£€æµ‹ï¼›2ï¼‰å¤šå±‚æ¶æ„æå‡å¯æ£€æµ‹æ€§è€Œä¸å½±å“ç”Ÿæˆè´¨é‡ï¼›3ï¼‰æ”¯æŒå¤šä½æ°´å°çš„ä¿¡æ¯ç¼–ç æ–¹æ³•ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå¤§é‡å®éªŒï¼ŒBiMarkåœ¨çŸ­æ–‡æœ¬çš„æå–ç‡ä¸Šæ¯”ç°æœ‰å¤šä½æ°´å°æ–¹æ³•æé«˜äº†30%ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½çš„å›°æƒ‘åº¦ï¼Œå¹¶åœ¨æ‘˜è¦å’Œç¿»è¯‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¸éæ°´å°æ–‡æœ¬ç›¸å½“çš„æ•ˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ°´å°è¯†åˆ«é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ–‡æœ¬è´¨é‡ã€æ£€æµ‹èƒ½åŠ›å’Œä¿¡æ¯åµŒå…¥æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBiMarkçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ›æ–°çš„æ°´å°æœºåˆ¶ï¼Œå¹³è¡¡æ–‡æœ¬è´¨é‡ä¸ä¿¡æ¯åµŒå…¥èƒ½åŠ›ï¼Œä»¥å®ç°é«˜æ•ˆçš„æ°´å°æ£€æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBiMarkæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰æ— åé‡çš„ä½ç¿»è½¬é‡åŠ æƒæœºåˆ¶ï¼›2ï¼‰å¤šå±‚æ°´å°æ¶æ„ï¼›3ï¼‰å¤šä½ä¿¡æ¯ç¼–ç æ–¹æ³•ï¼Œç¡®ä¿æ°´å°çš„æœ‰æ•ˆåµŒå…¥ä¸æ£€æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šBiMarkçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ— åé‡çš„é‡åŠ æƒæœºåˆ¶å’Œå¤šå±‚æ¶æ„è®¾è®¡ï¼Œä½¿å¾—æ°´å°æ£€æµ‹ä¸ä¾èµ–äºç‰¹å®šæ¨¡å‹ï¼ŒåŒæ—¶æå‡äº†æ°´å°çš„å¯æ£€æµ‹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ°´å°çš„åµŒå…¥æ•ˆæœï¼Œå¹¶é€šè¿‡å¤šå±‚ç»“æ„å¢å¼ºäº†ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ï¼Œç¡®ä¿æ°´å°ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBiMarkåœ¨çŸ­æ–‡æœ¬çš„æ°´å°æå–ç‡ä¸Šæ¯”ç°æœ‰å¤šä½æ°´å°æ–¹æ³•æé«˜äº†30%ï¼ŒåŒæ—¶åœ¨æ–‡æœ¬è´¨é‡ä¸Šä¿æŒè¾ƒä½çš„å›°æƒ‘åº¦ï¼Œå¹¶åœ¨æ‘˜è¦å’Œç¿»è¯‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸éæ°´å°æ–‡æœ¬ç›¸å½“çš„æ•ˆæœï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BiMarkçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€å†…å®¹å®¡æ ¸å’Œç‰ˆæƒä¿æŠ¤ç­‰ã€‚é€šè¿‡æœ‰æ•ˆçš„æ°´å°æŠ€æœ¯ï¼Œå¯ä»¥å¸®åŠ©è¯†åˆ«å’ŒéªŒè¯ç”Ÿæˆæ–‡æœ¬çš„çœŸå®æ€§ï¼Œä¿ƒè¿›å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨ä½¿ç”¨ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„äººå·¥æ™ºèƒ½åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Large Language Models (LLMs) have raised urgent concerns about LLM-generated text authenticity, prompting regulatory demands for reliable identification mechanisms. Although watermarking offers a promising solution, existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity, which are crucial for practical implementation. To achieve these goals, the key challenge lies in balancing the trade-off between text quality preservation and message embedding capacity. To address this challenge, we propose BiMark, a novel watermarking framework that achieves these requirements through three key innovations: (1) a bit-flip unbiased reweighting mechanism enabling model-agnostic detection, (2) a multilayer architecture enhancing detectability without compromising generation quality, and (3) an information encoding approach supporting multi-bit watermarking. Through theoretical analysis and extensive experiments, we validate that, compared to state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.

