---
layout: default
title: Structured Attention Matters to Multimodal LLMs in Document Understanding
---

# Structured Attention Matters to Multimodal LLMs in Document Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21600" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21600v1</a>
  <a href="https://arxiv.org/pdf/2506.21600.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21600v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21600v1', 'Structured Attention Matters to Multimodal LLMs in Document Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chang Liu, Hongkai Chen, Yujun Cai, Hang Wu, Qingwen Ye, Ming-Hsuan Yang, Yiwei Wang

**åˆ†ç±»**: cs.CL, cs.AI, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“æ„åŒ–æ³¨æ„åŠ›ä»¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æ¡£ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ–‡æ¡£ç†è§£` `ç»“æ„åŒ–æ³¨æ„åŠ›` `LaTexç¼–ç ` `ä¿¡æ¯æå–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¯æ®å®šä½ï¼Œå¿½è§†äº†è¾“å…¥æ ¼å¼å¯¹æ–‡æ¡£ç†è§£çš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æå‡ºäº†ä¸€ç§åŸºäºLaTexçš„ç»“æ„ä¿æŒæ–¹æ³•ï¼Œç¼–ç æ–‡æ¡£å…ƒç´ ä»¥ç»´æŠ¤å±‚æ¬¡å’Œç©ºé—´å…³ç³»ï¼Œä»è€Œæ”¹å–„ç†è§£æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ–‡æ¡£ç±»å‹ä¸Šçš„é—®ç­”æ€§èƒ½æ˜¾è‘—æå‡ï¼Œä¸”æ— éœ€å¯¹æ¨¡å‹æ¶æ„è¿›è¡Œä¿®æ”¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æ¡£ç†è§£å¯¹äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ä»¥å¾€ç ”ç©¶ä¸»è¦é›†ä¸­äºé€šè¿‡ç²¾ç¡®çš„å¤šæ¨¡æ€æŸ¥è¯¢å®šä½è¯æ®é¡µé¢ï¼Œè€Œæœ¬ç ”ç©¶åˆ™æ¢è®¨äº†ä¸€ä¸ªåŸºæœ¬ä½†è¢«å¿½è§†çš„æ–¹é¢ï¼šè¾“å…¥æ ¼å¼å¦‚ä½•å½±å“æ–‡æ¡£ç†è§£æ€§èƒ½ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬å‘ç°åŸå§‹OCRæ–‡æœ¬å¾€å¾€ä¼šå‰Šå¼±MLLMsçš„æ€§èƒ½ï¼Œè¿™ä¸€åç›´è§‰çš„å‘ç°å½’å› äºæ³¨æ„åŠ›åˆ†æ•£å’Œç»“æ„ä¸§å¤±ã€‚ä¸ºè¿›ä¸€æ­¥éªŒè¯æˆ‘ä»¬çš„å‡è®¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»“æ„ä¿æŒæ–¹æ³•ï¼Œé‡‡ç”¨LaTexèŒƒå¼å¯¹æ–‡æ¡£å…ƒç´ è¿›è¡Œç¼–ç ï¼Œä¿æŒå¯¹ç†è§£è‡³å…³é‡è¦çš„å±‚æ¬¡ç»„ç»‡å’Œç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬çš„æ³¨æ„åŠ›åˆ†æè¡¨æ˜ï¼Œç»“æ„åŒ–æ–‡æœ¬åœ¨æ–‡æœ¬å’Œè§†è§‰å†…å®¹ä¸Šè¯±å¯¼äº†ç»“æ„åŒ–çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸï¼ŒåŒæ—¶å‡å°‘æ³¨æ„åŠ›æµªè´¹ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æå‡äº†MLLMsåœ¨å¤šç§æ–‡æ¡£ç±»å‹ä¸Šçš„é—®ç­”æ€§èƒ½ï¼Œæ— éœ€æ¶æ„ä¿®æ”¹æˆ–é¢å¤–è®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡æ¡£ç†è§£ä¸­å› è¾“å…¥æ ¼å¼ä¸å½“è€Œå¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–åŸå§‹OCRæ–‡æœ¬ï¼Œé€ æˆæ³¨æ„åŠ›åˆ†æ•£å’Œç»“æ„ä¸§å¤±ï¼Œå½±å“ç†è§£æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ç»“æ„ä¿æŒçš„æ–¹æ³•ï¼Œé€šè¿‡LaTexèŒƒå¼å¯¹æ–‡æ¡£å…ƒç´ è¿›è¡Œç¼–ç ï¼Œä¿æŒæ–‡æ¡£çš„å±‚æ¬¡ç»“æ„å’Œç©ºé—´å…³ç³»ï¼Œä»è€Œæå‡æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ–‡æ¡£å…ƒç´ çš„ç»“æ„åŒ–ç¼–ç ã€æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–å’Œæ¨¡å‹çš„é—®ç­”èƒ½åŠ›æå‡ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è¾“å…¥å¤„ç†ã€ç»“æ„åŒ–ç¼–ç å’Œæ³¨æ„åŠ›åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥ç»“æ„åŒ–æ–‡æœ¬ä»¥è¯±å¯¼ç»“æ„åŒ–æ³¨æ„åŠ›æ¨¡å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å…³æ³¨è¯­ä¹‰ç›¸å…³åŒºåŸŸï¼Œå‡å°‘æ³¨æ„åŠ›æµªè´¹ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ— åºæ³¨æ„åŠ›æœºåˆ¶å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šå¼ºè°ƒç»“æ„ä¿æŒï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºç°æœ‰çš„MLLMæ¶æ„è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿å…¼å®¹æ€§ä¸æ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ç»“æ„ä¿æŒæ–¹æ³•åï¼Œæ¨¡å‹åœ¨å¤šç§æ–‡æ¡£ç±»å‹ä¸Šçš„é—®ç­”æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–‡æ¡£è‡ªåŠ¨åŒ–å¤„ç†ã€ä¿¡æ¯æ£€ç´¢å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æ¡£ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ³•å¾‹ã€åŒ»ç–—ã€æ•™è‚²ç­‰è¡Œä¸šä¸­å®ç°æ›´é«˜æ•ˆçš„ä¿¡æ¯æå–å’Œå†³ç­–æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Document understanding remains a significant challenge for multimodal large language models (MLLMs). While previous research has primarily focused on locating evidence pages through precise multimodal queries, our work investigates a fundamental yet overlooked aspect: how input format influences document comprehension performance. Through systematic analysis, we discover that raw OCR text often impairs rather than improves MLLMs' performance, which is a counterintuitive finding we attribute to attention dispersion and structure loss. To further substantiate our hypothesis, we propose a novel structure-preserving approach that encodes document elements using the LaTex paradigm, maintaining the hierarchical organization and spatial relationships critical for comprehension. Our attention analysis reveals that structured text induces structured attention patterns on both textual and visual content, directing models to focus on semantically meaningful regions while reducing attention waste. This approach significantly enhances MLLMs' document question answering performance across diverse document types without requiring architectural modifications or additional training.

