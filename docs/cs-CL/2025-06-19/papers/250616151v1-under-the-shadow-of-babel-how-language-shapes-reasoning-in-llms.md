---
layout: default
title: Under the Shadow of Babel: How Language Shapes Reasoning in LLMs
---

# Under the Shadow of Babel: How Language Shapes Reasoning in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16151" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16151v1</a>
  <a href="https://arxiv.org/pdf/2506.16151.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16151v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16151v1', 'Under the Shadow of Babel: How Language Shapes Reasoning in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenxi Wang, Yixuan Zhang, Lang Gao, Zixiang Xu, Zirui Song, Yanbo Wang, Xiuying Chen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: 15 pages, 10 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBICAUSEæ•°æ®é›†ä»¥éªŒè¯è¯­è¨€å¯¹LLMsæ¨ç†çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å› æœæ¨ç†` `åŒè¯­æ•°æ®é›†` `è¯­è¨€æ¨¡å‹` `æ¨ç†åè§` `è¯­è¨€ç›¸å¯¹è®º` `ç»“æ„åˆ†æ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨å› æœæ¨ç†ä¸­æœªèƒ½å……åˆ†è€ƒè™‘è¯­è¨€ç»“æ„å¯¹æ¨ç†æ¨¡å¼çš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸å‡ã€‚
2. è®ºæ–‡æå‡ºBICAUSEæ•°æ®é›†ï¼Œé€šè¿‡åŒè¯­æ ·æœ¬åˆ†æLLMsåœ¨å› æœæ¨ç†ä¸­çš„è¯­è¨€ç‰¹å®šåå¥½ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼ŒLLMsåœ¨ä¸­æ–‡å› æœæ¨ç†ä¸­è¡¨ç°å‡ºè¾ƒå¼ºçš„è¯­è¨€ä¾èµ–æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨éå…¸å‹è¾“å…¥ä¸Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­è¨€ä¸ä»…æ˜¯äº¤æµå·¥å…·ï¼Œä¹Ÿæ˜¯äººç±»è®¤çŸ¥å’Œæ¨ç†çš„åª’ä»‹ã€‚æ ¹æ®è¯­è¨€ç›¸å¯¹è®ºï¼Œè¯­è¨€ç»“æ„å¯èƒ½å½±å“è®¤çŸ¥æ¨¡å¼ã€‚æœ¬æ–‡æå‡ºBICAUSEï¼Œä¸€ä¸ªç»“æ„åŒ–çš„åŒè¯­å› æœæ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«è¯­ä¹‰å¯¹é½çš„ä¸­è‹±æ–‡æ ·æœ¬ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsåœ¨å› æœæ¨ç†ä¸­è¡¨ç°å‡ºè¯­è¨€ç‰¹å®šçš„åå¥½å’Œæ³¨æ„æ¨¡å¼ï¼Œå°¤å…¶åœ¨ä¸­æ–‡ä¸­è¡¨ç°å‡ºè¾ƒå¤§çš„å±€é™æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMsä¸ä»…æ¨¡ä»¿è¡¨é¢è¯­è¨€å½¢å¼ï¼Œè¿˜å†…åŒ–äº†è¯­è¨€æ‰€å¡‘é€ çš„æ¨ç†åè§ã€‚è¯¥ç°è±¡é¦–æ¬¡é€šè¿‡æ¨¡å‹å†…éƒ¨ç»“æ„åˆ†æå¾—åˆ°å®è¯éªŒè¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨è¯­è¨€ç»“æ„å¦‚ä½•å½±å“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å› æœæ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘è¯­è¨€ç‰¹æ€§å¯¹æ¨ç†çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸­çš„è¡¨ç°ä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºBICAUSEæ•°æ®é›†ï¼ŒåŒ…å«ä¸­è‹±æ–‡å› æœæ¨ç†æ ·æœ¬ï¼Œç ”ç©¶LLMsåœ¨ä¸åŒè¯­è¨€ä¸‹çš„æ¨ç†åå¥½å’Œè¡¨ç°ã€‚è¯¥è®¾è®¡æ—¨åœ¨æ­ç¤ºè¯­è¨€å¯¹æ¨ç†è¿‡ç¨‹çš„æ·±å±‚å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆæ„å»ºåŒè¯­æ•°æ®é›†BICAUSEï¼Œéšåå¯¹LLMsè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œåˆ†æå…¶åœ¨å› æœæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œç»“æœåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šé¦–æ¬¡é€šè¿‡ç»“æ„åŒ–åˆ†æéªŒè¯äº†è¯­è¨€å¯¹LLMsæ¨ç†çš„å½±å“ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸­çš„æ³¨æ„æ¨¡å¼å’Œå› æœåå¥½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œç¡®ä¿ä¸­è‹±æ–‡æ ·æœ¬çš„è¯­ä¹‰å¯¹é½ï¼Œå¹¶è®¾è®¡äº†é€‚åº”ä¸åŒè¯­è¨€ç‰¹æ€§çš„æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜æ¨¡å‹åœ¨å› æœæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨ä¸­æ–‡å› æœæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜æ˜¾ä½äºè‹±æ–‡ï¼Œå°¤å…¶åœ¨å¤„ç†éå…¸å‹è¾“å…¥æ—¶æ€§èƒ½ä¸‹é™æ˜¾è‘—ã€‚æ¨¡å‹åœ¨ä¸­æ–‡ä¸­å¯¹å› æœè¯åºçš„åå¥½å¯¼è‡´äº†æ¨ç†èƒ½åŠ›çš„é™åˆ¶ï¼ŒéªŒè¯äº†è¯­è¨€ç»“æ„å¯¹æ¨ç†çš„æ·±è¿œå½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œè·¨è¯­è¨€æ¨ç†ç­‰ã€‚é€šè¿‡ç†è§£è¯­è¨€å¯¹æ¨ç†çš„å½±å“ï¼Œå¯ä»¥ä¼˜åŒ–LLMsçš„è®¾è®¡ï¼Œæå‡å…¶åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Language is not only a tool for communication but also a medium for human cognition and reasoning. If, as linguistic relativity suggests, the structure of language shapes cognitive patterns, then large language models (LLMs) trained on human language may also internalize the habitual logical structures embedded in different languages. To examine this hypothesis, we introduce BICAUSE, a structured bilingual dataset for causal reasoning, which includes semantically aligned Chinese and English samples in both forward and reversed causal forms. Our study reveals three key findings: (1) LLMs exhibit typologically aligned attention patterns, focusing more on causes and sentence-initial connectives in Chinese, while showing a more balanced distribution in English. (2) Models internalize language-specific preferences for causal word order and often rigidly apply them to atypical inputs, leading to degraded performance, especially in Chinese. (3) When causal reasoning succeeds, model representations converge toward semantically aligned abstractions across languages, indicating a shared understanding beyond surface form. Overall, these results suggest that LLMs not only mimic surface linguistic forms but also internalize the reasoning biases shaped by language. Rooted in cognitive linguistic theory, this phenomenon is for the first time empirically verified through structural analysis of model internals.

