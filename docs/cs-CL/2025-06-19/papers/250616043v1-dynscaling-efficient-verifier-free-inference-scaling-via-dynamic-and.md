---
layout: default
title: DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling
---

# DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16043" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16043v1</a>
  <a href="https://arxiv.org/pdf/2506.16043.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16043v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16043v1', 'DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ã–. ArÄ±k

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDynScalingä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç‡` `åŠ¨æ€é¢„ç®—åˆ†é…` `é‡‡æ ·ç­–ç•¥` `å¤šè‡‚å¼ºç›—é—®é¢˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•å¸¸ä¾èµ–å¤–éƒ¨éªŒè¯å™¨ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚
2. DynScalingé€šè¿‡é›†æˆå¹¶è¡Œä¸é¡ºåºé‡‡æ ·ç­–ç•¥ï¼Œä»¥åŠåŠ¨æ€é¢„ç®—åˆ†é…æ¡†æ¶ï¼Œä¼˜åŒ–äº†æ¨ç†è¿‡ç¨‹ï¼Œæå‡äº†è®¡ç®—æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDynScalingåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å‡è¶…è¶Šäº†ç°æœ‰çš„æ— éªŒè¯æ¨ç†æ‰©å±•æ–¹æ³•ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½å’Œæ›´ä½çš„è®¡ç®—æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨ç†æ—¶é—´æ‰©å±•åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½æ–¹é¢å·²è¢«è¯æ˜æœ‰æ•ˆï¼Œä½†å…¶å®é™…åº”ç”¨å¸¸å› ä¾èµ–å¤–éƒ¨éªŒè¯å™¨æˆ–ç¼ºä¹å¯¹ç°å®è®¡ç®—çº¦æŸçš„ä¼˜åŒ–è€Œå—é™ã€‚æœ¬æ–‡æå‡ºDynScalingï¼Œé€šè¿‡ä¸¤é¡¹ä¸»è¦åˆ›æ–°è§£å†³è¿™äº›å±€é™ï¼šé›†æˆçš„å¹¶è¡Œ-é¡ºåºé‡‡æ ·ç­–ç•¥å’ŒåŸºäºå¼ºç›—ç®—æ³•çš„åŠ¨æ€é¢„ç®—åˆ†é…æ¡†æ¶ã€‚é›†æˆé‡‡æ ·ç­–ç•¥é€šè¿‡æ„å»ºåˆæˆçš„é¡ºåºæ¨ç†é“¾ï¼Œä¿ƒè¿›å¤šæ ·ä¸”è¿è´¯çš„æ¨ç†è½¨è¿¹ã€‚åŠ¨æ€é¢„ç®—åˆ†é…æ¡†æ¶å°†è®¡ç®—èµ„æºåˆ†é…å½¢å¼åŒ–ä¸ºå¤šè‡‚å¼ºç›—é—®é¢˜ï¼ŒåŸºäºå…ˆå‰é‡‡æ ·å“åº”çš„ä¸ç¡®å®šæ€§è‡ªé€‚åº”åˆ†é…æ¨ç†é¢„ç®—ï¼Œä»è€Œæœ€å¤§åŒ–è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDynScalingåœ¨ä»»åŠ¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ä¸Šå‡ä¼˜äºç°æœ‰çš„æ— éªŒè¯æ¨ç†æ‰©å±•åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å¤–éƒ¨éªŒè¯å™¨ï¼Œå¯¼è‡´åº”ç”¨å—é™ï¼Œä¸”æœªèƒ½å……åˆ†è€ƒè™‘ç°å®è®¡ç®—çº¦æŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDynScalingçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é›†æˆå¹¶è¡Œå’Œé¡ºåºé‡‡æ ·ï¼Œæ„å»ºåˆæˆçš„æ¨ç†é“¾ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€é¢„ç®—åˆ†é…æ¥ä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ï¼Œä»è€Œæå‡æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDynScalingçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé›†æˆé‡‡æ ·æ¨¡å—å’ŒåŠ¨æ€é¢„ç®—åˆ†é…æ¨¡å—ã€‚é›†æˆé‡‡æ ·æ¨¡å—è´Ÿè´£ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†é“¾ï¼Œè€ŒåŠ¨æ€é¢„ç®—åˆ†é…æ¨¡å—åˆ™æ ¹æ®å…ˆå‰å“åº”çš„ä¸ç¡®å®šæ€§è‡ªé€‚åº”è°ƒæ•´è®¡ç®—èµ„æºåˆ†é…ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å¹¶è¡Œå’Œé¡ºåºé‡‡æ ·ç­–ç•¥ç»“åˆï¼Œå½¢æˆåˆæˆæ¨ç†é“¾ï¼Œå¹¶å°†è®¡ç®—èµ„æºåˆ†é…è§†ä¸ºå¤šè‡‚å¼ºç›—é—®é¢˜ï¼Œä»è€Œå®ç°åŠ¨æ€ä¼˜åŒ–ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„é™æ€èµ„æºåˆ†é…å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬é‡‡æ ·ç­–ç•¥çš„é€‰æ‹©å’Œé¢„ç®—åˆ†é…ç®—æ³•çš„å®ç°ï¼ŒæŸå¤±å‡½æ•°åˆ™è€ƒè™‘äº†æ¨ç†é“¾çš„è¿è´¯æ€§å’Œå¤šæ ·æ€§ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ¨ç†ç»“æœæ—¢å‡†ç¡®åˆä¸°å¯Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDynScalingåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å‡è¶…è¶Šäº†ç°æœ‰çš„æ— éªŒè¯æ¨ç†æ‰©å±•åŸºçº¿ï¼Œå…·ä½“è¡¨ç°ä¸ºæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬é™ä½äº†20%ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DynScalingçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½é—®ç­”ç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ¨ç†ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inference-time scaling has proven effective in boosting large language model (LLM) performance through increased test-time computation. Yet, its practical application is often hindered by reliance on external verifiers or a lack of optimization for realistic computational constraints. We propose DynScaling, which addresses these limitations through two primary innovations: an integrated parallel-sequential sampling strategy and a bandit-based dynamic budget allocation framework. The integrated sampling strategy unifies parallel and sequential sampling by constructing synthetic sequential reasoning chains from initially independent parallel responses, promoting diverse and coherent reasoning trajectories. The dynamic budget allocation framework formulates the allocation of computational resources as a multi-armed bandit problem, adaptively distributing the inference budget across queries based on the uncertainty of previously sampled responses, thereby maximizing computational efficiency. By combining these components, DynScaling effectively improves LLM performance under practical resource constraints without the need for external verifiers. Experimental results demonstrate that DynScaling consistently surpasses existing verifier-free inference scaling baselines in both task performance and computational cost.

