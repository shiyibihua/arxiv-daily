---
layout: default
title: Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples
---

# Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16502" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.16502v1</a>
  <a href="https://arxiv.org/pdf/2506.16502.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16502v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16502v1', 'Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Soumya Suvra Ghosal, Vaibhav Singh, Akash Ghosh, Soumyabrata Pal, Subhadip Baidya, Sriparna Saha, Dinesh Manocha

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-19

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫RELICÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥‰ΩéËµÑÊ∫êÂç∞Âú∞ËØ≠Â•ñÂä±Ê®°ÂûãÊ≥õÂåñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â•ñÂä±Ê®°Âûã` `‰ΩéËµÑÊ∫êËØ≠Ë®Ä` `‰∏ä‰∏ãÊñáÂ≠¶‰π†` `ÊàêÂØπÊéíÂêç` `ËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öËØ≠Ë®ÄÂ•ñÂä±Ê®°Âûã‰∏ªË¶Å‰æùËµñÈ´òËµÑÊ∫êËØ≠Ë®ÄÁöÑÊï∞ÊçÆÔºåÂØºËá¥‰ΩéËµÑÊ∫êÂç∞Âú∞ËØ≠ÁöÑÂ•ñÂä±‰ø°Âè∑‰∏çÂèØÈù†„ÄÇ
2. RELICÊ°ÜÊû∂ÈÄöËøáÊàêÂØπÊéíÂêçÁõÆÊ†áËÆ≠ÁªÉÊ£ÄÁ¥¢Âô®Ôºå‰ªéÈ´òËµÑÊ∫êËØ≠Ë®Ä‰∏≠ÈÄâÊã©ÊúâÊïàÁöÑ‰∏ä‰∏ãÊñáÁ§∫‰æãÔºåÊèêÂçá‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ•ñÂä±Ê®°ÂûãÊÄßËÉΩ„ÄÇ
3. Âú®BodoËØ≠Ë®ÄÁöÑÂÆûÈ™å‰∏≠ÔºåRELICÂú®ÂáÜÁ°ÆÊÄß‰∏äÂàÜÂà´ÊØîÈõ∂-shotÊèêÁ§∫ÂíåÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫Ü12.81%Âíå10.13%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â•ñÂä±Ê®°ÂûãÂØπ‰∫éÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∞ÂºÄÊ∫êÂ§öËØ≠Ë®ÄÂ•ñÂä±Ê®°Âûã‰∏ªË¶ÅÂú®È´òËµÑÊ∫êËØ≠Ë®ÄÁöÑÂÅèÂ•ΩÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåÂØºËá¥Âú®‰ΩéËµÑÊ∫êÂç∞Âú∞ËØ≠‰∏≠‰∫ßÁîü‰∏çÂèØÈù†ÁöÑÂ•ñÂä±‰ø°Âè∑„ÄÇÊî∂ÈõÜËøô‰∫õËØ≠Ë®ÄÁöÑÂ§ßËßÑÊ®°È´òË¥®ÈáèÂÅèÂ•ΩÊï∞ÊçÆ‰ª£‰ª∑È´òÊòÇÔºå‰ΩøÂæóÂü∫‰∫éÂÅèÂ•ΩÁöÑËÆ≠ÁªÉÊñπÊ≥ï‰∏çÂàáÂÆûÈôÖ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÊåëÊàòÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜRELICÔºå‰∏Ä‰∏™Áî®‰∫é‰ΩéËµÑÊ∫êÂç∞Âú∞ËØ≠Â•ñÂä±Âª∫Ê®°ÁöÑÊñ∞Âûã‰∏ä‰∏ãÊñáÂ≠¶‰π†Ê°ÜÊû∂„ÄÇRELICÈÄöËøáÊàêÂØπÊéíÂêçÁõÆÊ†áËÆ≠ÁªÉÊ£ÄÁ¥¢Âô®Ôºå‰ªéËæÖÂä©È´òËµÑÊ∫êËØ≠Ë®Ä‰∏≠ÈÄâÊã©ÊúÄÊúâÊïàÁöÑ‰∏ä‰∏ãÊñáÁ§∫‰æãÔºå‰ª•Á™ÅÂá∫ÂÅèÂ•ΩÂíåÈùûÂÅèÂ•ΩÂìçÂ∫î‰πãÈó¥ÁöÑÂå∫Âà´„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåRELICÊòæËëóÊèêÈ´ò‰∫Ü‰ΩéËµÑÊ∫êÂç∞Âú∞ËØ≠ÁöÑÂ•ñÂä±Ê®°ÂûãÂáÜÁ°ÆÊÄßÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁ§∫‰æãÈÄâÊã©ÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰ΩéËµÑÊ∫êÂç∞Âú∞ËØ≠Â•ñÂä±Ê®°ÂûãÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÈ´òËµÑÊ∫êËØ≠Ë®ÄÁöÑÂÅèÂ•ΩÊï∞ÊçÆÔºåÂØºËá¥Âú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏≠ÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöRELICÊ°ÜÊû∂ÈÄöËøáÊàêÂØπÊéíÂêçÁõÆÊ†áËÆ≠ÁªÉÊ£ÄÁ¥¢Âô®ÔºåÈÄâÊã©È´òËµÑÊ∫êËØ≠Ë®Ä‰∏≠ÁöÑ‰∏ä‰∏ãÊñáÁ§∫‰æãÔºå‰ª•ÊúâÊïàÂå∫ÂàÜÂÅèÂ•ΩÂíåÈùûÂÅèÂ•ΩÂìçÂ∫îÔºå‰ªéËÄåÊèêÂçá‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ•ñÂä±Ê®°ÂûãÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRELICÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÊ£ÄÁ¥¢Ê®°ÂùóÂíåÂ•ñÂä±Ê®°ÂûãËÆ≠ÁªÉÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÂà©Áî®Ê£ÄÁ¥¢Âô®‰ªéÈ´òËµÑÊ∫êËØ≠Ë®Ä‰∏≠ÈÄâÊã©Á§∫‰æãÔºåÁÑ∂ÂêéÂ∞ÜËøô‰∫õÁ§∫‰æãÁî®‰∫éËÆ≠ÁªÉ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ•ñÂä±Ê®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRELICÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÈÄöËøáÊàêÂØπÊéíÂêçÁöÑÊñπÂºèÈÄâÊã©‰∏ä‰∏ãÊñáÁ§∫‰æãÔºåËøôÁßçÊñπÊ≥ïÂú®‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ•ñÂä±Âª∫Ê®°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÔºå‰∏é‰º†ÁªüÁöÑÁ§∫‰æãÈÄâÊã©ÊñπÊ≥ïÁõ∏ÊØîÂÖ∑ÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåRELIC‰ΩøÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÊ£ÄÁ¥¢Âô®ÁöÑÈÄâÊã©ËÉΩÂäõÔºåÂπ∂ÈááÁî®‰∫ÜÈÄÇÂêà‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÁΩëÁªúÁªìÊûÑÔºå‰ª•Á°Æ‰øùÊ®°ÂûãÁöÑÊúâÊïàÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂÆûÈ™å‰∏≠ÔºåRELICÂú®BodoËØ≠Ë®ÄÁöÑÂ•ñÂä±Ê®°Âûã‰∏äÂèñÂæó‰∫ÜÊòæËëóÊèêÂçáÔºåÂáÜÁ°ÆÊÄßÊØîÈõ∂-shotÊèêÁ§∫ÊèêÈ´ò‰∫Ü12.81%ÔºåÊØîÁé∞ÊúâÁöÑÁ§∫‰æãÈÄâÊã©ÊñπÊ≥ïÊèêÈ´ò‰∫Ü10.13%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéRELICÂú®‰ΩéËµÑÊ∫êËØ≠Ë®ÄÂ•ñÂä±Âª∫Ê®°‰∏≠ÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊú∫Âô®ÁøªËØëÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠â„ÄÇRELICÊ°ÜÊû∂ËÉΩÂ§üÊúâÊïàÊèêÂçá‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÊ®°ÂûãÊÄßËÉΩÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÔºåÊú™Êù•ÂèØ‰∏∫Êõ¥Â§ö‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÁ†îÁ©∂Êèê‰æõÊîØÊåÅÔºå‰øÉËøõËØ≠Ë®ÄÊäÄÊúØÁöÑÂÖ¨Âπ≥ÊÄßÂíåÂèØÂèäÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reward models are essential for aligning large language models (LLMs) with human preferences. However, most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical. To address this challenge, we propose RELIC, a novel in-context learning framework for reward modeling in low-resource Indic languages. RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses. Extensive experiments on three preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods. For example, on Bodo-a low-resource Indic language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13% improvement in accuracy over zero-shot prompting and state-of-the-art example selection method, respectively.

