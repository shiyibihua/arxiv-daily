---
layout: default
title: Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning
---

# Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16064" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16064v1</a>
  <a href="https://arxiv.org/pdf/2506.16064.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16064v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16064v1', 'Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Duc Hieu Ho, Chenglin Fan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæˆ‘æ‰¹è¯„å¼•å¯¼çš„å¥½å¥‡å¿ƒä¼˜åŒ–ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯šå®æ€§ä¸å¸®åŠ©æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘æ‰¹è¯„` `å¥½å¥‡å¿ƒä¼˜åŒ–` `è¯šå®æ€§` `å¸®åŠ©æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¯šå®å’Œæœ‰å¸®åŠ©çš„è¾“å‡ºæ–¹é¢ä»å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¯¼è‡´è¾“å‡ºè´¨é‡ä¸ç¨³å®šã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘æ‰¹è¯„å¼•å¯¼çš„å¥½å¥‡å¿ƒä¼˜åŒ–æç¤ºç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹è‡ªæˆ‘ä¼˜åŒ–å“åº”ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°æ¨¡å‹ä¸­å‡å®ç°äº†1.4%è‡³4.3%çš„è¯šå®æ€§ä¸å¸®åŠ©æ€§å¾—åˆ†æå‡ï¼Œæ˜¾è‘—æ”¹å–„äº†è¾“å‡ºè´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å§‹ç»ˆé¢ä¸´ç”Ÿæˆä¸€è‡´è¯šå®å’Œæœ‰å¸®åŠ©çš„è¾“å‡ºçš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é€šè¿‡å¯¹åç§å¹¿æ³›ä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå…¨é¢åŸºå‡†è¯„ä¼°ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æç¤ºç­–ç•¥â€”â€”è‡ªæˆ‘æ‰¹è¯„å¼•å¯¼çš„å¥½å¥‡å¿ƒä¼˜åŒ–æç¤ºã€‚è¯¥ç­–ç•¥çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘æ‰¹è¯„å¹¶åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ä¼˜åŒ–å…¶å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨HONESETæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¯šå®æ€§å’Œå¸®åŠ©æ€§å¾—åˆ†ï¼Œå‡å°‘äº†ä½è´¨é‡å“åº”ï¼Œæå‡äº†é«˜è´¨é‡å“åº”çš„æ¯”ä¾‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¾“å‡ºæ—¶ç¼ºä¹ä¸€è‡´æ€§å’Œå¯é æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæå‡æ¨¡å‹çš„è¯šå®æ€§å’Œå¸®åŠ©æ€§ï¼Œå¯¼è‡´ç”¨æˆ·ä½“éªŒä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„è‡ªæˆ‘æ‰¹è¯„å¼•å¯¼çš„å¥½å¥‡å¿ƒä¼˜åŒ–ç­–ç•¥ï¼Œå…è®¸æ¨¡å‹åœ¨ç”Ÿæˆå“åº”åè¿›è¡Œè‡ªæˆ‘è¯„ä¼°å’Œä¼˜åŒ–ï¼Œä»è€Œæé«˜è¾“å‡ºçš„è´¨é‡å’Œå¯é æ€§ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œå…·æœ‰è¾ƒé«˜çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè‡ªæˆ‘æ‰¹è¯„æ­¥éª¤å’Œä¼˜åŒ–æ­¥éª¤ã€‚é¦–å…ˆï¼Œæ¨¡å‹ç”Ÿæˆåˆæ­¥å“åº”ï¼Œç„¶åé€šè¿‡è‡ªæˆ‘æ‰¹è¯„æ­¥éª¤è¯„ä¼°è¯¥å“åº”çš„è´¨é‡ï¼Œæœ€ååœ¨ä¼˜åŒ–æ­¥éª¤ä¸­è¿›è¡Œè°ƒæ•´å’Œæ”¹è¿›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è‡ªæˆ‘æ‰¹è¯„æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸»åŠ¨åæ€å’Œæ”¹è¿›ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„é™æ€ç”Ÿæˆæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œè®¾è®¡äº†è½»é‡çº§çš„ä¸Šä¸‹æ–‡æ­¥éª¤ä»¥æ”¯æŒè‡ªæˆ‘æ‰¹è¯„å’Œä¼˜åŒ–ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹æå‡è¾“å‡ºè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è‡ªæˆ‘æ‰¹è¯„å¼•å¯¼çš„å¥½å¥‡å¿ƒä¼˜åŒ–æç¤ºç­–ç•¥åï¼Œæ‰€æœ‰è¯„ä¼°æ¨¡å‹çš„è¯šå®æ€§ä¸å¸®åŠ©æ€§å¾—åˆ†å‡æœ‰æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å¥½å¥‡å¿ƒé©±åŠ¨æç¤ºï¼Œå¾—åˆ†æå‡å¹…åº¦åœ¨1.4%è‡³4.3%ä¹‹é—´ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼ã€å†…å®¹ç”Ÿæˆç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’ä½“éªŒã€‚é€šè¿‡å¢å¼ºæ¨¡å‹çš„è¯šå®æ€§å’Œå¸®åŠ©æ€§ï¼Œè¯¥æ–¹æ³•æœ‰åŠ©äºæ„å»ºæ›´å¯ä¿¡èµ–çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated robust capabilities across various natural language tasks. However, producing outputs that are consistently honest and helpful remains an open challenge. To overcome this challenge, this paper tackles the problem through two complementary directions. It conducts a comprehensive benchmark evaluation of ten widely used large language models, including both proprietary and open-weight models from OpenAI, Meta, and Google. In parallel, it proposes a novel prompting strategy, self-critique-guided curiosity refinement prompting. The key idea behind this strategy is enabling models to self-critique and refine their responses without additional training. The proposed method extends the curiosity-driven prompting strategy by incorporating two lightweight in-context steps including self-critique step and refinement step.
>   The experiment results on the HONESET dataset evaluated using the framework $\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a judge of honesty and helpfulness, show consistent improvements across all models. The approach reduces the number of poor-quality responses, increases high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores ranging from 1.4% to 4.3% compared to curiosity-driven prompting across evaluated models. These results highlight the effectiveness of structured self-refinement as a scalable and training-free strategy to improve the trustworthiness of LLMs outputs.

