---
layout: default
title: Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models
---

# Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03363" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03363v4</a>
  <a href="https://arxiv.org/pdf/2508.03363.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03363v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03363v4', 'Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haotian Wu, Bo Xu, Yao Shu, Menglin Yang, Chengwei Qin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-10-12)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNothinkingæ ¡å‡†ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ€è€ƒæ ¡å‡†` `å¤šæ ·æ€§æ£€æŸ¥` `ä¸€è‡´æ€§æ£€éªŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è®­ç»ƒå’Œæ¨ç†ç­–ç•¥çš„æ”¹è¿›ï¼Œæœªå……åˆ†æŒ–æ˜å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚
2. æœ¬æ–‡æå‡ºJointThinkingï¼Œé€šè¿‡å¹¶è¡Œç”Ÿæˆæ€è€ƒå’Œéæ€è€ƒæ¨¡å¼çš„ç­”æ¡ˆï¼Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒJointThinkingåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶åœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆRLLMsï¼‰æœ€è¿‘åœ¨ç»“æ„åŒ–å’Œå¤šæ­¥éª¤æ¨ç†æ–¹é¢å±•ç°äº†æ˜¾è‘—èƒ½åŠ›ã€‚å°½ç®¡ä»¥å¾€ç ”ç©¶ä¸»è¦é›†ä¸­äºæ”¹è¿›è®­ç»ƒå’Œæ¨ç†ç­–ç•¥ï¼Œä½†å…¶åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­çš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†Thinking with Nothinking Calibrationï¼ˆJointThinkingï¼‰ï¼Œä¸€ç§æ–°çš„ICLèŒƒå¼ï¼Œä¿ƒä½¿æ¨¡å‹å¹¶è¡Œç”Ÿæˆä¸¤ä¸ªç­”æ¡ˆï¼šä¸€ä¸ªåœ¨æ€è€ƒæ¨¡å¼ä¸‹ï¼Œå¦ä¸€ä¸ªåœ¨éæ€è€ƒæ¨¡å¼ä¸‹ã€‚å½“ä¸¤ä¸ªåˆå§‹å“åº”ä¸ä¸€è‡´æ—¶ï¼Œè§¦å‘ç¬¬äºŒè½®æ€è€ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒJointThinkingåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå°‘æ ·æœ¬é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ã€åŒé‡æ€è€ƒå’Œå¤šæ•°æŠ•ç¥¨ï¼Œä¸”åœ¨åˆ†å¸ƒå†…æ€§èƒ½ä¸Šä¸åŸºäºè®­ç»ƒçš„æœ€å…ˆè¿›æ¨ç†æ–¹æ³•ç›¸å½“ï¼Œåœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆä¸€è‡´æ€§ç­”æ¡ˆæ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ¨¡å‹çš„æ¨ç†æ½œåŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºJointThinkingï¼Œé€šè¿‡å¹¶è¡Œç”Ÿæˆæ€è€ƒå’Œéæ€è€ƒæ¨¡å¼çš„ç­”æ¡ˆï¼Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å½“ä¸¤ä¸ªç­”æ¡ˆä¸ä¸€è‡´æ—¶ï¼Œè§¦å‘ç¬¬äºŒè½®æ€è€ƒï¼Œä»¥æé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µç”Ÿæˆæ€è€ƒå’Œéæ€è€ƒæ¨¡å¼çš„ç­”æ¡ˆï¼Œç¬¬äºŒé˜¶æ®µåœ¨ä¸ä¸€è‡´æ—¶è¿›è¡Œå†æ¬¡æ€è€ƒã€‚æ¨¡å‹é€šè¿‡å•ä¸€æç¤ºç”Ÿæˆä¸¤ä¸ªä¸åŒçš„ç­”æ¡ˆï¼Œç¡®ä¿å¤šæ ·æ€§å’Œä¸€è‡´æ€§æ£€æŸ¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†Nothinkingæ ¡å‡†æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ€è€ƒå’Œéæ€è€ƒæ¨¡å¼ä¹‹é—´åˆ‡æ¢ï¼Œä»è€Œæå‡æ¨ç†çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€æ€è€ƒæ¨¡å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæ¨¡å‹é€šè¿‡è°ƒæ•´æç¤ºå’Œç­”æ¡ˆç”Ÿæˆç­–ç•¥ï¼Œç¡®ä¿å¤šæ ·æ€§å’Œä¸€è‡´æ€§æ£€æŸ¥çš„æœ‰æ•ˆæ€§ã€‚æŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†ç­”æ¡ˆä¸€è‡´æ€§çš„é‡è¦æ€§ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒJointThinkingåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå°‘æ ·æœ¬é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ã€åŒé‡æ€è€ƒå’Œå¤šæ•°æŠ•ç¥¨ï¼Œå°¤å…¶åœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºåœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡äº†20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆå’Œå¤æ‚å†³ç­–æ”¯æŒç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒJointThinkingèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´å‡†ç¡®å’Œä¸€è‡´çš„ç»“æœï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning large language models (RLLMs) have recently demonstrated remarkable capabilities through structured and multi-step reasoning. While prior research has primarily focused on improving their training and inference strategies, their potential for in-context learning (ICL) remains largely underexplored. To fill this gap, we propose Thinking with Nothinking Calibration (JointThinking), a new ICL paradigm that prompts the model to generate two answers in parallel: one in Thinking mode and the other in Nothinking mode. A second round of Thinking is triggered only when the two initial responses are inconsistent, using a single prompt with two different answers. Extensive experiments across multiple reasoning benchmarks demonstrate that JointThinking significantly outperforms few-shot chain-of-thought (CoT), thinking twice and majority voting. Moreover, it achieves comparable in-distribution performance to training-based SOTA reasoning method, while substantially outperforming on out-of-distribution tasks. We further conduct a systematic analysis of the calibration mechanism, showing the importance of structural thinking diversity and the benefits of consistency check. Additionally, we observe that the performance gap between actual and ideal reasoning narrows as model size increases in the second thinking, indicating the strong scalability of our approach. Finally, we discuss current limitations and outline promising directions for future ICL research in RLLMs.

