---
layout: default
title: EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models
---

# EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03533" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03533v1</a>
  <a href="https://arxiv.org/pdf/2508.03533.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03533v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03533v1', 'EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoming Hou, Jiquan Zhang, Zibin Lin, DaCheng Tao, Shengli Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEmbedGradä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æç¤ºåµŒå…¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ–‡æœ¬æç¤ºä¼˜åŒ–` `åµŒå…¥ç©ºé—´` `æ¢¯åº¦ä¼˜åŒ–` `ä»»åŠ¡é€‚åº”` `æ•°å­¦æ¨ç†` `æƒ…æ„Ÿåˆ†æ` `å› æœåˆ¤æ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é€‚åº”å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹æ—¶é¢ä¸´ç¦»æ•£ä¼˜åŒ–ä¸å‚æ•°å¤æ‚æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå¯¼è‡´ç²¾åº¦ä¸å¯è§£é‡Šæ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºEmbedGradï¼Œé€šè¿‡æ¢¯åº¦ä¼˜åŒ–æ–‡æœ¬æç¤ºåµŒå…¥ï¼Œè§£è€¦è®­ç»ƒä¸éƒ¨ç½²ï¼Œå®ç°äº†æ›´ç²¾ç»†çš„è°ƒæ•´ã€‚
3. åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼ŒEmbedGradå°†Qwen2.5-Math-1.5Bæ¨¡å‹çš„å‡†ç¡®ç‡ä»14.74%æå‡è‡³58.96%ï¼Œåœ¨æ‰€æœ‰æ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¸€è‡´çš„æ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆåœ°å°†å¼ºå¤§çš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹é€‚åº”äºå¤šæ ·åŒ–ä»»åŠ¡ä»ç„¶æ˜¯AIéƒ¨ç½²ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚ç›®å‰çš„æ–¹æ³•ä¸»è¦éµå¾ªä¸¤ç§èŒƒå¼ï¼šé€šè¿‡æç¤ºå·¥ç¨‹è¿›è¡Œç¦»æ•£ä¼˜åŒ–æˆ–é€šè¿‡é¢å¤–çš„å¯è®­ç»ƒå‚æ•°è¿›è¡Œè¿ç»­é€‚åº”ã€‚è¿™ä¸¤è€…éƒ½æœ‰å±€é™æ€§ï¼šç¦»æ•£æ–¹æ³•ç¼ºä¹ç²¾ç»†è°ƒæ•´ï¼Œè€ŒåŸºäºå‚æ•°çš„æŠ€æœ¯åˆ™å¢åŠ äº†å¤æ‚æ€§å¹¶é™ä½äº†å¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†EmbedGradï¼Œä¸€ä¸ªé€šè¿‡åŸºäºæ¢¯åº¦çš„ç²¾ç»†åŒ–ä¼˜åŒ–æ–‡æœ¬æç¤ºåµŒå…¥çš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€šè¿‡æ ‡è®°ç¤ºä¾‹æŒ‡å¯¼åµŒå…¥çš„ç²¾ç¡®è°ƒæ•´ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰æ„ä¹‰ï¼›åœ¨æ¨ç†é˜¶æ®µï¼Œä»…ä¼˜åŒ–åçš„åµŒå…¥ä¸ç”¨æˆ·æŸ¥è¯¢é›†æˆã€‚å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒEmbedGradåœ¨æ•°å­¦æ¨ç†ã€æƒ…æ„Ÿåˆ†æå’Œå› æœåˆ¤æ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆé€‚åº”å¼ºå¤§çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥åº”å¯¹å¤šæ ·åŒ–ä»»åŠ¡çš„é—®é¢˜ã€‚ç°æœ‰çš„ç¦»æ•£ä¼˜åŒ–æ–¹æ³•ç¼ºä¹ç²¾ç»†è°ƒæ•´ï¼Œè€ŒåŸºäºå‚æ•°çš„æŠ€æœ¯åˆ™å¢åŠ äº†æ¨¡å‹å¤æ‚æ€§å¹¶é™ä½äº†å¯è§£é‡Šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEmbedGradçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¢¯åº¦ä¼˜åŒ–æ–‡æœ¬æç¤ºçš„åµŒå…¥ï¼Œå…è®¸åœ¨è®­ç»ƒé˜¶æ®µè¿›è¡Œç²¾ç¡®è°ƒæ•´ï¼ŒåŒæ—¶åœ¨æ¨ç†é˜¶æ®µä»…ä½¿ç”¨ä¼˜åŒ–åçš„åµŒå…¥ä¸ç”¨æˆ·æŸ¥è¯¢ç»“åˆã€‚è¿™ç§è®¾è®¡ä½¿å¾—åœ¨æ–‡æœ¬ç©ºé—´ä¸­éš¾ä»¥å®ç°çš„ç»†ç²’åº¦æ ¡å‡†æˆä¸ºå¯èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEmbedGradçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šä¼˜åŒ–é˜¶æ®µå’Œæ¨ç†é˜¶æ®µã€‚åœ¨ä¼˜åŒ–é˜¶æ®µï¼Œåˆ©ç”¨æ ‡è®°ç¤ºä¾‹æŒ‡å¯¼åµŒå…¥çš„è°ƒæ•´ï¼›åœ¨æ¨ç†é˜¶æ®µï¼Œä½¿ç”¨ä¼˜åŒ–åçš„åµŒå…¥ä¸ç”¨æˆ·è¾“å…¥è¿›è¡Œç»“åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šEmbedGradçš„æœ€å¤§åˆ›æ–°åœ¨äºå°†æ–‡æœ¬æç¤ºçš„ä¼˜åŒ–ä¸æ¨¡å‹å‚æ•°çš„å¤æ‚æ€§è§£è€¦ï¼Œå»ºç«‹äº†ä¸€ç§æ–°çš„åµŒå…¥ç²¾ç»†åŒ–èŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹å®ç°ä»»åŠ¡é€‚åº”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒEmbedGradé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥æŒ‡å¯¼åµŒå…¥çš„ä¼˜åŒ–ï¼Œå¹¶é€šè¿‡æ¢¯åº¦ä¸‹é™æ–¹æ³•è¿›è¡Œè°ƒæ•´ï¼Œç¡®ä¿åµŒå…¥åœ¨ä¿æŒè¯­ä¹‰çš„åŒæ—¶èƒ½å¤Ÿç²¾ç¡®åæ˜ ä»»åŠ¡éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒEmbedGradæ˜¾è‘—æé«˜äº†Qwen2.5-Math-1.5Bæ¨¡å‹åœ¨æ•°å­¦é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ï¼Œä»14.74%æå‡è‡³58.96%ã€‚æ­¤å¤–ï¼Œåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆ0.5B-14Bï¼‰å’Œå„ç±»ä»»åŠ¡ä¸­å‡è§‚å¯Ÿåˆ°ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨å¤æ‚çš„å› æœåˆ¤æ–­é—®é¢˜ä¸Šï¼Œå°æ¨¡å‹çš„è¡¨ç°æå‡å°¤ä¸ºæ˜¾è‘—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EmbedGradçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦é«˜ç²¾åº¦æ¨ç†å’Œæƒ…æ„Ÿåˆ†æçš„åœºæ™¯ä¸­ï¼Œå¦‚æ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼å’Œé‡‘èåˆ†æç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–æç¤ºåµŒå…¥ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šåŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effectively adapting powerful pretrained foundation models to diverse tasks remains a key challenge in AI deployment. Current approaches primarily follow two paradigms:discrete optimization of text prompts through prompt engineering, or continuous adaptation via additional trainable parameters. Both exhibit limitations-discrete methods lack refinement precision while parameter-based techniques increase complexity and reduce interpretability. To address these constraints, we propose EmbedGrad, a novel framework that optimizes text prompt embeddings through gradient-based refinement. Our approach uniquely decouples training from deployment:during optimization,labeled examples guide precise embedding adjustments while preserving semantic meaning; during inference, only optimized embeddings integrate with user queries. This enables fine-grained calibration impossible in text space, such as enhancing the reasoning capability of prompts like please reason step by step. Comprehensive evaluations across mathematical reasoning, sentiment analysis, and causal judgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning prompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\% to 58.96\% on mathematical problems. Consistent improvements were observed across model scales (0.5B-14B) and all tasks, with particularly significant gains for smaller models on complex problems like causal judgment. By bridging prompt engineering and parameter efficiency without architectural changes, our work establishes embedding refinement as a powerful new paradigm for task adaptation.

