---
layout: default
title: Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs
---

# Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08292" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08292v2</a>
  <a href="https://arxiv.org/pdf/2508.08292.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08292v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08292v2', 'Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aryan Gulati, Brando Miranda, Eric Chen, Emily Xia, Kai Fronsdal, Bruno Dumont, Elyas Obbad, Sanmi Koyejo

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, cs.LO, cs.NE

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-08-27)

**å¤‡æ³¨**: 27 pages total (10-page main paper + 17-page appendix), 12 figures, 6 tables. Submitted to ICML 2025 (under review)

**æœŸåˆŠ**: ICML 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/brando90/putnam-axiom)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPutnam-AXIOMä»¥è§£å†³LLMsæ•°å­¦æ¨ç†åŸºå‡†çš„é¥±å’Œé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°å­¦æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŸºå‡†æµ‹è¯•` `åŠ¨æ€è¯„ä¼°` `åŠŸèƒ½å˜ä½“` `è®°å¿†åŒ–ç°è±¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°å­¦æ¨ç†åŸºå‡†å·²æ¥è¿‘é¥±å’Œï¼Œä¸”å—åˆ°è®­ç»ƒé›†æ±¡æŸ“çš„å½±å“ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¯é ã€‚
2. æå‡ºPutnam-AXIOMåŸºå‡†ï¼Œé€šè¿‡å¼•å…¥åŠŸèƒ½å˜ä½“ç”Ÿæˆæ–°é—®é¢˜ï¼Œæä¾›æŠ—æ±¡æŸ“çš„è¯„ä¼°æ¡†æ¶ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå¼ºæ¨¡å‹åœ¨å˜ä½“é›†ä¸Šçš„è¡¨ç°æ˜æ˜¾ä¸‹é™ï¼Œæ­ç¤ºäº†è®°å¿†åŒ–ç°è±¡ï¼Œå¼ºè°ƒåŠ¨æ€åŸºå‡†çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦æ¨ç†åŸºå‡†å·²æ¥è¿‘é¥±å’Œï¼Œéƒ¨åˆ†æ¨¡å‹çš„å‡†ç¡®ç‡è¶…è¿‡90%ï¼Œä½†è®­ç»ƒé›†æ±¡æŸ“é—®é¢˜æ—¥ç›Šä¸¥é‡ã€‚æœ¬æ–‡æå‡ºPutnam-AXIOMåŸºå‡†ï¼ŒåŒ…å«522ä¸ªæ¥è‡ªå¨å»‰Â·æ´›å„å°”Â·æ™®ç‰¹å—æ•°å­¦ç«èµ›çš„å¤§å­¦çº§ç«èµ›é—®é¢˜ï¼Œä»¥åŠé€šè¿‡ç¨‹åºæ€§æ‰°åŠ¨ç”Ÿæˆçš„100ä¸ªåŠŸèƒ½å˜ä½“ã€‚è¯¥å˜ä½“åè®®èƒ½å¤Ÿç”Ÿæˆæ— é™æ•°é‡çš„åŒç­‰éš¾åº¦çš„æ–°å®ä¾‹ï¼Œä»è€Œæä¾›ä¸€ä¸ªæŠ—æ±¡æŸ“çš„æµ‹è¯•å¹³å°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOpenAIçš„o1-previewæ¨¡å‹åœ¨åŸå§‹æ•°æ®é›†ä¸Šçš„å¾—åˆ†ä¸º41.9%ï¼Œè€Œåœ¨å˜ä½“é›†ä¸Šçš„å‡†ç¡®ç‡ä¸‹é™äº†19.6%ã€‚è¿™äº›ç»“æœè¡¨æ˜äº†è®°å¿†åŒ–çš„ç°è±¡ï¼Œå¹¶å¼ºè°ƒäº†åŠ¨æ€åŸºå‡†çš„å¿…è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ•°å­¦æ¨ç†åŸºå‡†çš„é¥±å’Œå’Œè®­ç»ƒé›†æ±¡æŸ“é—®é¢˜ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„å‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥Putnam-AXIOMåŸºå‡†å’ŒåŠŸèƒ½å˜ä½“ï¼Œæä¾›ä¸€ä¸ªåŠ¨æ€ä¸”æŠ—æ±¡æŸ“çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ›´å‡†ç¡®åœ°æµ‹é‡LLMsçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šåŸå§‹é—®é¢˜é›†ï¼ˆ522ä¸ªç«èµ›é—®é¢˜ï¼‰å’Œå˜ä½“é›†ï¼ˆ100ä¸ªåŠŸèƒ½å˜ä½“ï¼‰ï¼Œåè€…é€šè¿‡ç¨‹åºæ€§æ‰°åŠ¨ç”Ÿæˆï¼Œå½¢æˆæ— é™çš„æ–°å®ä¾‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†åŠŸèƒ½å˜ä½“ç”Ÿæˆæœºåˆ¶ï¼Œä½¿å¾—åŸºå‡†æµ‹è¯•èƒ½å¤ŸæŒç»­äº§ç”Ÿæ–°é—®é¢˜ï¼Œé¿å…äº†æ¨¡å‹çš„è®°å¿†åŒ–ç°è±¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†Teacher-Forced Accuracyï¼ˆTFAï¼‰ä½œä¸ºè½»é‡çº§è¯„ä¼°æŒ‡æ ‡ï¼Œç›´æ¥è¯„åˆ†æ¨ç†è¿‡ç¨‹ï¼Œå¹¶è‡ªåŠ¨åŒ–è‡ªç„¶è¯­è¨€è¯æ˜çš„è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOpenAIçš„o1-previewæ¨¡å‹åœ¨åŸå§‹æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º41.9%ï¼Œè€Œåœ¨å˜ä½“é›†ä¸Šä¸‹é™è‡³46.8%ï¼Œä¸‹é™å¹…åº¦è¾¾åˆ°19.6%ã€‚è¿™ä¸€ç°è±¡è¡¨æ˜äº†æ¨¡å‹çš„è®°å¿†åŒ–é—®é¢˜ï¼Œå¼ºè°ƒäº†åŠ¨æ€åŸºå‡†çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ ‡å‡†ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•™è‚²ã€ç§‘å­¦ç ”ç©¶å’Œè‡ªåŠ¨åŒ–æ¨ç†ç­‰é¢†åŸŸã€‚æœªæ¥ï¼ŒPutnam-AXIOMå¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æ–¹æ³•çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current mathematical reasoning benchmarks for large language models (LLMs) are approaching saturation, with some achieving > 90% accuracy, and are increasingly compromised by training-set contamination. We introduce Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn from the prestigious William Lowell Putnam Mathematical Competition, and Putnam-AXIOM Variation, an unseen companion set of 100 functional variants generated by programmatically perturbing variables and constants. The variation protocol produces an unlimited stream of equally difficult, unseen instances -- yielding a contamination-resilient test bed. On the Original set, OpenAI's o1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy drops by 19.6% (46.8% relative decrease) on the paired Variations. The remaining eighteen models show the same downward trend, ten of them with non-overlapping 95% confidence intervals. These gaps suggest memorization and highlight the necessity of dynamic benchmarks. We complement "boxed" accuracy with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores reasoning traces and automates natural language proof evaluations. Putnam-AXIOM therefore provides a rigorous, contamination-resilient evaluation framework for assessing advanced mathematical reasoning of LLMs. Data and evaluation code are publicly available at https://github.com/brando90/putnam-axiom.

