---
layout: default
title: More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation
---

# More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03678" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03678v1</a>
  <a href="https://arxiv.org/pdf/2508.03678.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03678v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03678v1', 'More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yangtian Zi, Harshitha Menon, Arjun Guha

**åˆ†ç±»**: cs.CL, cs.LG, cs.PL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPartialOrderEvalä»¥è§£å†³LLMä»£ç ç”Ÿæˆä¸­çš„æç¤ºç»†èŠ‚ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç ç”Ÿæˆ` `æç¤ºå·¥ç¨‹` `æ€§èƒ½è¯„ä¼°` `é¢†åŸŸçŸ¥è¯†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMåœ¨ç‰¹å®šé¢†åŸŸçš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå¯èƒ½æ˜¯ç”±äºç¼ºä¹è¶³å¤Ÿçš„æç¤ºç»†èŠ‚ã€‚
2. æœ¬æ–‡æå‡ºPartialOrderEvalï¼Œé€šè¿‡å¼•å…¥æç¤ºçš„éƒ¨åˆ†é¡ºåºæ¥è¯„ä¼°æç¤ºç»†èŠ‚å¯¹ä»£ç ç”Ÿæˆçš„å½±å“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæç¤ºç»†èŠ‚çš„æå‡æ˜¾è‘—æ”¹å–„äº†LLMåœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚è¾“å…¥æ—¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„æœ€å…ˆè¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šç”¨åŸºå‡†æµ‹è¯•å¦‚HumanEvalä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä¸“ä¸šæµ‹è¯•å¥—ä»¶å¦‚ParEvalä¸­è¡¨ç°ä¸ä½³ã€‚è¿™ç§å·®å¼‚æ˜¯å¦æºäºLLMsç¼ºä¹é¢†åŸŸçŸ¥è¯†æˆ–æç¤ºç»†èŠ‚ä¸è¶³ï¼Ÿä¸ºäº†è§£ç­”è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†PartialOrderEvalï¼Œè¯¥æ–¹æ³•é€šè¿‡ä»æœ€å°åˆ°æœ€å¤§è¯¦ç»†ç¨‹åº¦çš„æç¤ºéƒ¨åˆ†é¡ºåºå¢å¼ºä»»ä½•ä»£ç ç”ŸæˆåŸºå‡†ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äºHumanEvalä»¥åŠParEvalçš„ä¸²è¡Œå’ŒOpenMPå­é›†ï¼Œæµ‹é‡æç¤ºç»†èŠ‚å¯¹pass@1çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLlama-3.xå’ŒQwen2.5-Coderåœ¨ä¸åŒä»»åŠ¡ä¸­å¯¹æç¤ºçš„æ•æ„Ÿæ€§å­˜åœ¨å·®å¼‚ï¼Œå®šæ€§åˆ†ææŒ‡å‡ºï¼Œæ˜ç¡®çš„è¾“å…¥/è¾“å‡ºè§„èŒƒã€è¾¹ç¼˜æ¡ˆä¾‹å¤„ç†å’Œé€æ­¥åˆ†è§£æ˜¯æå‡æç¤ºç»†èŠ‚çš„å…³é”®é©±åŠ¨å› ç´ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»£ç ç”Ÿæˆä»»åŠ¡ä¸­ç”±äºæç¤ºç»†èŠ‚ä¸è¶³è€Œå¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸“ä¸šé¢†åŸŸçš„ä»£ç ç”Ÿæˆæ—¶ï¼Œå¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨æç¤ºä¿¡æ¯ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥PartialOrderEvalï¼Œé€šè¿‡æ„å»ºæç¤ºçš„éƒ¨åˆ†é¡ºåºï¼Œä»æœ€å°‘åˆ°æœ€å¤šçš„ç»†èŠ‚ï¼Œæ¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°æç¤ºç»†èŠ‚å¯¹ä»£ç ç”Ÿæˆæ€§èƒ½çš„å½±å“ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ­ç¤ºæç¤ºç»†èŠ‚å¯¹æ¨¡å‹è¾“å‡ºè´¨é‡çš„å…³é”®ä½œç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯æç¤ºç”Ÿæˆæ¨¡å—ï¼Œæ„å»ºä¸åŒç»†èŠ‚çº§åˆ«çš„æç¤ºï¼›å…¶æ¬¡æ˜¯ä»£ç ç”Ÿæˆæ¨¡å—ï¼Œä½¿ç”¨LLMç”Ÿæˆä»£ç ï¼›æœ€åæ˜¯è¯„ä¼°æ¨¡å—ï¼Œæ¯”è¾ƒä¸åŒæç¤ºä¸‹çš„ç”Ÿæˆç»“æœï¼Œè®¡ç®—pass@1æŒ‡æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†æç¤ºçš„éƒ¨åˆ†é¡ºåºè¯„ä¼°æ–¹æ³•ï¼Œè¿™ä¸ç°æœ‰çš„å•ä¸€æç¤ºè¯„ä¼°æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£æç¤ºç»†èŠ‚å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„æç¤ºç»†èŠ‚çº§åˆ«ï¼Œå¹¶ä½¿ç”¨Llama-3.xå’ŒQwen2.5-Coderè¿›è¡Œå¯¹æ¯”ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©åŸºäºç°æœ‰LLMçš„ä¼˜åŒ–ç­–ç•¥ï¼Œç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­éƒ½èƒ½æœ‰æ•ˆè¯„ä¼°æç¤ºçš„å½±å“ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€æç¤ºç»†èŠ‚çš„å¢åŠ ï¼ŒLlama-3.xå’ŒQwen2.5-Coderåœ¨HumanEvalå’ŒParEvalä»»åŠ¡ä¸­çš„pass@1æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚è¾“å…¥æ—¶ï¼Œæå‡å¹…åº¦å¯è¾¾20%ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥ç¡®è®¤äº†æ˜ç¡®çš„è¾“å…¥/è¾“å‡ºè§„èŒƒå’Œè¾¹ç¼˜æ¡ˆä¾‹å¤„ç†å¯¹æ€§èƒ½æå‡çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€è‡ªåŠ¨åŒ–æµ‹è¯•å’Œæ•™è‚²ç­‰ã€‚é€šè¿‡ä¼˜åŒ–æç¤ºç»†èŠ‚ï¼ŒLLMså¯ä»¥åœ¨ç‰¹å®šé¢†åŸŸçš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­æä¾›æ›´é«˜è´¨é‡çš„è¾“å‡ºï¼Œä»è€Œæé«˜å¼€å‘æ•ˆç‡å’Œä»£ç è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸçš„ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆå’Œå›¾åƒç”Ÿæˆç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general benchmarks like HumanEval but underperform on specialized suites such as ParEval. Is this due to LLMs missing domain knowledge or insufficient prompt detail is given? To answer this, we introduce PartialOrderEval, which augments any code generation benchmark with a partial order of prompts from minimal to maximally detailed. Applying it to HumanEval and both serial and OpenMP subsets of ParEval, we measure how pass@1 scales with prompt specificity. Our experiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of prompt sensitivity across different tasks, and a qualitative analysis highlights explicit I/O specifications, edge-case handling, and stepwise breakdowns as the key drivers of prompt detail improvement.

