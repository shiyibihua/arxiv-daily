---
layout: default
title: CTTS: Collective Test-Time Scaling
---

# CTTS: Collective Test-Time Scaling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03333" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.03333v2</a>
  <a href="https://arxiv.org/pdf/2508.03333.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03333v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03333v2', 'CTTS: Collective Test-Time Scaling')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Lei Bai, Tao Chen, Wanli Ouyang

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-05 (Êõ¥Êñ∞: 2025-09-28)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/magent4aci/CTTS-MM)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CTTS‰ª•Ëß£ÂÜ≥Âçï‰∏ÄÊµãËØïÊó∂Èó¥Áº©ÊîæÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÊµãËØïÊó∂Èó¥Áº©Êîæ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÈõÜ‰ΩìÂ≠¶‰π†` `Â§ö‰ª£ÁêÜÂçè‰Ωú` `Â•ñÂä±Ê®°Âûã` `ÊÄßËÉΩÊèêÂçá` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊµãËØïÊó∂Èó¥Áº©ÊîæÊñπÊ≥ïÂ¶ÇBest-of-NÂíåSelf-ConsistencyÂèóÈôê‰∫éÂçï‰∏Ä‰ª£ÁêÜ‰∏éÂçï‰∏ÄÂ•ñÂä±Ê®°ÂûãÁöÑ‰∫§‰∫íÔºåÂØºËá¥ÊÄßËÉΩÊèêÂçáÊúâÈôê„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑÈõÜ‰ΩìÊµãËØïÊó∂Èó¥Áº©ÊîæÔºàCTTSÔºâÊñπÊ≥ïÔºåÈááÁî®Â§ö‰ª£ÁêÜ‰∏éÂ§öÂ•ñÂä±ÁöÑÂçè‰ΩúÊú∫Âà∂ÔºåÊó®Âú®Á™ÅÁ†¥Áé∞ÊúâÊñπÊ≥ïÁöÑÊÄßËÉΩÁì∂È¢à„ÄÇ
3. CTTS-MMÊ°ÜÊû∂Âú®‰∏É‰∏™‰∏ªÊµÅÂü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËæÉBest-of-NÊèêÂçá4.82%ÔºåËæÉGPT-4.1ÊèêÂçá7.06%ÔºåÊòæÁ§∫Âá∫ÈõÜ‰ΩìÁº©ÊîæÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÊµãËØïÊó∂Èó¥Áº©ÊîæÔºàTTSÔºâ‰Ωú‰∏∫‰∏ÄÁßçÂ¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊÄßËÉΩÁöÑÊúâÂâçÊôØÁöÑÊó†ËÆ≠ÁªÉÊñπÊ≥ïÔºåÁÑ∂ËÄåÁé∞ÊúâÊñπÊ≥ïÂ¶ÇBest-of-NÂíåSelf-ConsistencyÂèóÈôê‰∫éÂçï‰∏ÄÊµãËØïÊó∂Èó¥Áº©ÊîæÔºàSTTSÔºâËåÉÂºè„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÈõÜ‰ΩìÊµãËØïÊó∂Èó¥Áº©ÊîæÔºàCTTSÔºâÔºåÈÄöËøáÁ≥ªÁªüÁ†îÁ©∂Â§öÊ®°ÂûãÁöÑ‰∏âÁßç‰∏ªË¶Å‰∫§‰∫íËåÉÂºèÔºåÂèëÁé∞Â§ö‰ª£ÁêÜ-Â§öÂ•ñÂä±ÔºàMA-MRÔºâËåÉÂºèË°®Áé∞‰ºòË∂ä„ÄÇÂü∫‰∫éÊ≠§ÔºåÊèêÂá∫CTTS-MMÊ°ÜÊû∂ÔºåÊï¥Âêà‰∫Ü‰ª£ÁêÜÂçè‰ΩúÊêúÁ¥¢ÔºàACSÔºâÂíåÂ•ñÂä±Ê®°ÂûãÊ∑∑ÂêàÔºàMoRÔºâÁ≠ñÁï•„ÄÇÂÆûÈ™åË°®ÊòéÔºåCTTS-MMÂú®‰∏É‰∏™‰∏ªÊµÅÂü∫ÂáÜ‰∏äÊòæËëóË∂ÖË∂ä‰∫ÜÈ¢ÜÂÖàÁöÑSTTSÊñπÊ≥ïÔºåÂπ∂Âú®ÊÄßËÉΩ‰∏äË∂ÖËøá‰∫ÜÊóóËà∞ÁöÑ‰∏ìÊúâLLMÂíåÂºÄÊ∫êLLM„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊµãËØïÊó∂Èó¥Áº©ÊîæÊñπÊ≥ïÂú®ÊÄßËÉΩ‰∏äÁöÑÂ±ÄÈôêÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂçï‰∏Ä‰ª£ÁêÜ‰∏éÂçï‰∏ÄÂ•ñÂä±Ê®°ÂûãÁöÑ‰∫§‰∫íÊ®°ÂºèÂØºËá¥ÁöÑÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•Â§ö‰ª£ÁêÜ‰∏éÂ§öÂ•ñÂä±ÁöÑÂçè‰ΩúÊú∫Âà∂ÔºåCTTSÊó®Âú®Âà©Áî®ÈõÜ‰ΩìÊô∫ÊÖßË∂ÖË∂äÂçï‰∏ÄÊ®°ÂûãÁöÑÊÄßËÉΩÈôêÂà∂Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÊïàÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCTTS-MMÊ°ÜÊû∂ÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö‰ª£ÁêÜÂçè‰ΩúÊêúÁ¥¢ÔºàACSÔºâÁî®‰∫éÈÄâÊã©ÊúÄ‰Ω≥ÁöÑLLMÁªÑÂêàÔºå‰ª•ÂèäÊ∑∑ÂêàÂ•ñÂä±Ê®°ÂûãÔºàMoRÔºâÁ≠ñÁï•ÔºåÈÄöËøáPrior RewardÊ®°ÂûãÈõÜÊàêÈÄâÊã©ÔºàPRESÔºâÁÆóÊ≥ïÊù•‰ºòÂåñÂ•ñÂä±Ê®°ÂûãÁöÑÁªÑÂêà„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCTTS-MMÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂêåÊó∂ÂÆûÁé∞Â§ö‰ª£ÁêÜ‰∏éÂ§öÂ•ñÂä±ÁöÑÂçè‰ΩúÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜÊÄßËÉΩÔºå‰∏é‰º†ÁªüÁöÑÂçï‰∏ÄÊ®°ÂûãÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®‰∏çÂêåÊ®°ÂûãÁöÑ‰ºòÂäø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ACS‰∏≠ÔºåËÆæËÆ°‰∫ÜÊúâÊïàÁöÑÁªÑÂêàÈÄâÊã©Êú∫Âà∂ÔºõÂú®MoR‰∏≠ÔºåÈááÁî®‰∫ÜPRESÁÆóÊ≥ïÊù•‰ºòÂåñÂ•ñÂä±Ê®°ÂûãÁöÑÈÄâÊã©ÔºåÁ°Æ‰øù‰∫ÜÊ®°ÂûãÈó¥ÁöÑÂçèÂêåÊïàÂ∫îÊúÄÂ§ßÂåñ„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Âú®ÂÆûÈ™å‰∏≠ÁªèËøáÁ≤æÁªÜË∞É‰ºò„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CTTS-MMÂú®‰∏É‰∏™‰∏ªÊµÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËæÉBest-of-NÊñπÊ≥ïÊèêÂçá4.82%ÔºåÂπ∂‰∏îÂú®‰∏éÊóóËà∞‰∏ìÊúâLLMÔºàÂ¶ÇGPT-4.1ÔºâÊØîËæÉÊó∂ÔºåÊèêÂçáÂπÖÂ∫¶ËææÂà∞7.06%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÈõÜ‰ΩìÁº©ÊîæÊñπÊ≥ïÂú®LLMÊé®ÁêÜ‰∏≠ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÂØπËØùÁ≥ªÁªüÂíåÊô∫ËÉΩÈóÆÁ≠îÁ≠âÔºåËÉΩÂ§üÊòæËëóÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑË°®Áé∞„ÄÇÊú™Êù•ÔºåCTTSÊñπÊ≥ïÊúâÊúõÊé®Âä®Êõ¥È´òÊïàÁöÑÊ®°ÂûãÊé®ÁêÜÂíåÊõ¥ÂπøÊ≥õÁöÑÂ∫îÁî®Âú∫ÊôØÔºå‰øÉËøõ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Test-time scaling (TTS) has emerged as a promising, training-free approach for enhancing large language model (LLM) performance. However, the efficacy of existing methods, such as Best-of-N and Self-Consistency, is fundamentally constrained by the dominant single test-time scaling (STTS) paradigm, which relies on a single LLM agent interacting with a single reward model (SA-SR). Inspired by recent work showing that collective methods can surpass the performance ceiling of individual models, we introduce Collective Test-Time Scaling (CTTS). First, we systematically investigate three primary interaction paradigms of existing multiple models: single-agent-multi-reward (SA-MR), multi-agent-single-reward (MA-SR), and multi-agent-multi-reward (MA-MR). Extensive experiments reveal that the MA-MR paradigm is consistently superior. Based on this finding, we further propose CTTS-MM, a novel framework that operationalizes multi-agent and multi-reward collaboration. CTTS-MM integrates two key technical contributions: (1) for agent collaboration, an Agent Collaboration Search (ACS) that identifies the most effective combination of LLMs from a candidate pool; and (2) for reward model collaboration, a Mixture of Reward Models (MoR) strategy that leverages a Prior Reward model Ensemble Selection (PRES) algorithm to select the optimal ensemble. Evaluations across seven mainstream benchmarks demonstrate that CTTS-MM significantly outperforms leading STTS methods (+4.82% over Best-of-N) and surpasses even flagship proprietary LLMs (+7.06% over GPT-4.1) and open-source LLMs. These results highlight the substantial potential of collective scaling to push the frontier of LLM inference. Code will be released at https://github.com/magent4aci/CTTS-MM.

