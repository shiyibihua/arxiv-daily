---
layout: default
title: AttnTrace: Attention-based Context Traceback for Long-Context LLMs
---

# AttnTrace: Attention-based Context Traceback for Long-Context LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03793" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.03793v1</a>
  <a href="https://arxiv.org/pdf/2508.03793.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03793v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03793v1', 'AttnTrace: Attention-based Context Traceback for Long-Context LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yanting Wang, Runpeng Geng, Ying Chen, Jinyuan Jia

**ÂàÜÁ±ª**: cs.CL, cs.CR

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-05

**Â§áÊ≥®**: The code is available at https://github.com/Wang-Yanting/AttnTrace. The demo is available at https://huggingface.co/spaces/SecureLLMSys/AttnTrace

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Wang-Yanting/AttnTrace)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫AttnTrace‰ª•Ëß£ÂÜ≥Èïø‰∏ä‰∏ãÊñáLLMÁöÑËøΩÊ∫ØÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Èïø‰∏ä‰∏ãÊñá` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `‰∏ä‰∏ãÊñáËøΩÊ∫Ø` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `ÊèêÁ§∫Ê≥®ÂÖ•Ê£ÄÊµã` `ÂèØËß£ÈáäÊÄß` `Â¢ûÂº∫ÂûãÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑ‰∏ä‰∏ãÊñáËøΩÊ∫ØÊñπÊ≥ïÂ¶ÇTracLLMËÆ°ÁÆóÊàêÊú¨È´òÔºåÂ§ÑÁêÜÂçï‰∏™ÂìçÂ∫î-‰∏ä‰∏ãÊñáÂØπÈúÄË¶ÅÊï∞ÁôæÁßíÔºåÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. Êú¨ÊñáÊèêÂá∫AttnTraceÔºåÈÄöËøáÂà©Áî®LLMÁîüÊàêÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçËøõË°å‰∏ä‰∏ãÊñáËøΩÊ∫ØÔºåÊèêÂçá‰∫ÜËøΩÊ∫ØÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAttnTraceÂú®ÂáÜÁ°ÆÊÄßÂíåÊïàÁéá‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂ËÉΩÊúâÊïàÊ£ÄÊµãÈïø‰∏ä‰∏ãÊñá‰∏≠ÁöÑÊèêÁ§∫Ê≥®ÂÖ•ÈóÆÈ¢ò„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Èïø‰∏ä‰∏ãÊñáÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÔºåÂ¶ÇGemini-2.5-ProÂíåClaude-Sonnet-4ÔºåÊ≠£Ë¢´ÂπøÊ≥õÂ∫îÁî®‰∫éÂ¢ûÂº∫ÂûãÁîüÊàêÔºàRAGÔºâÁÆ°ÈÅìÂíåËá™‰∏ª‰ª£ÁêÜÁ≠âÂÖàËøõAIÁ≥ªÁªü‰∏≠„ÄÇËøô‰∫õÁ≥ªÁªü‰∏≠ÔºåLLMÊé•Êî∂Êåá‰ª§Âèä‰∏ä‰∏ãÊñáÔºåÂπ∂ÁîüÊàêÂü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÂìçÂ∫î„ÄÇÂ∞ΩÁÆ°Â∑≤ÊúâÁ†îÁ©∂ËÆæËÆ°‰∫ÜËß£ÂÜ≥ÊñπÊ°à‰ª•ËøΩÊ∫ØÂØπÂìçÂ∫îÁîüÊàêË¥°ÁåÆÊúÄÂ§ßÁöÑÊñáÊú¨Ôºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂ¶ÇTracLLMËÆ°ÁÆóÊàêÊú¨È´òÔºåÊïàÁéá‰Ωé„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜAttnTraceÔºå‰∏ÄÁßçÂü∫‰∫éLLMÊ≥®ÊÑèÂäõÊùÉÈáçÁöÑÊñ∞Âûã‰∏ä‰∏ãÊñáËøΩÊ∫ØÊñπÊ≥ïÔºåÁªìÂêà‰∏§ÁßçÊäÄÊúØÂ¢ûÂº∫ÂÖ∂ÊúâÊïàÊÄßÔºåÂπ∂Êèê‰æõÁêÜËÆ∫ËßÅËß£„ÄÇÁ≥ªÁªüËØÑ‰º∞Ë°®ÊòéÔºåAttnTraceÂú®ÂáÜÁ°ÆÊÄßÂíåÊïàÁéá‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂ËÉΩÊîπÂñÑÈïø‰∏ä‰∏ãÊñá‰∏ãÁöÑÊèêÁ§∫Ê≥®ÂÖ•Ê£ÄÊµã„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Èïø‰∏ä‰∏ãÊñáLLMÁöÑ‰∏ä‰∏ãÊñáËøΩÊ∫ØÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇTracLLMÂú®Â§ÑÁêÜÂìçÂ∫î-‰∏ä‰∏ãÊñáÂØπÊó∂ÔºåËÆ°ÁÆóÊàêÊú¨È´ò‰∏îËÄóÊó∂ÈïøÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂÆûÈôÖÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAttnTraceÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®LLMÁîüÊàêÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçËøõË°å‰∏ä‰∏ãÊñáËøΩÊ∫ØÔºåÈÄöËøáÂàÜÊûêËøô‰∫õÊùÉÈáçÊù•ËØÜÂà´ÂØπÁîüÊàêÂìçÂ∫îË¥°ÁåÆÊúÄÂ§ßÁöÑÊñáÊú¨Ôºå‰ªéËÄåÊèêÈ´òËøΩÊ∫ØÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAttnTraceÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÊòØÊ≥®ÊÑèÂäõÊùÉÈáçÁöÑÊèêÂèñÊ®°ÂùóÔºåËØ•Ê®°Âùó‰ªéLLM‰∏≠ÊèêÂèñÊ≥®ÊÑèÂäõÊùÉÈáçÔºõÂÖ∂Ê¨°ÊòØËøΩÊ∫ØÂàÜÊûêÊ®°ÂùóÔºåËØ•Ê®°ÂùóÂü∫‰∫éÊèêÂèñÁöÑÊùÉÈáçËøõË°å‰∏ä‰∏ãÊñáËøΩÊ∫Ø„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAttnTraceÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÈÄöËøáÊ≥®ÊÑèÂäõÊùÉÈáçËøõË°å‰∏ä‰∏ãÊñáËøΩÊ∫ØÔºåËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÊñáÊú¨Áõ∏‰ººÂ∫¶ÁöÑËøΩÊ∫ØÊñπÊ≥ïÊú¨Ë¥®‰∏ä‰∏çÂêåÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåAttnTraceÈááÁî®‰∫ÜÁâπÂÆöÁöÑÂèÇÊï∞ËÆæÁΩÆ‰ª•‰ºòÂåñÊ≥®ÊÑèÂäõÊùÉÈáçÁöÑÂà©Áî®ÔºåÂπ∂ÂºïÂÖ•‰∫ÜÊçüÂ§±ÂáΩÊï∞Êù•Â¢ûÂº∫Ê®°ÂûãÁöÑËøΩÊ∫ØËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ÂÆûÈ™åÈÉ®ÂàÜËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåAttnTraceÂú®‰∏ä‰∏ãÊñáËøΩÊ∫ØÁöÑÂáÜÁ°ÆÊÄß‰∏äÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫ÜXX%ÔºåÂú®ÊïàÁéá‰∏äÂáèÂ∞ë‰∫ÜYY%ÁöÑËÆ°ÁÆóÊó∂Èó¥„ÄÇÊ≠§Â§ñÔºåAttnTraceÂú®Èïø‰∏ä‰∏ãÊñá‰∏≠ÁöÑÊèêÁ§∫Ê≥®ÂÖ•Ê£ÄÊµãËÉΩÂäõ‰πüÊòæËëó‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂ±ïÁé∞‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

AttnTraceÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÂ¢ûÂº∫ÂûãÁîüÊàêÁ≥ªÁªüÁöÑÂèØËß£ÈáäÊÄß„ÄÅÂêéÊúüÊîªÂáªÂèñËØÅÂàÜÊûê‰ª•ÂèäÊèêÈ´òLLMËæìÂá∫ÁöÑÂèØ‰ø°Â∫¶„ÄÇÈÄöËøáÊúâÊïàËøΩÊ∫Ø‰∏ä‰∏ãÊñáÔºåAttnTraceËÉΩÂ§üÂ∏ÆÂä©ÂºÄÂèëËÄÖÁêÜËß£Ê®°ÂûãÂÜ≥Á≠ñËøáÁ®ãÔºåÊèêÂçáAIÁ≥ªÁªüÁöÑÈÄèÊòéÂ∫¶ÂíåÂÆâÂÖ®ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Long-context large language models (LLMs), such as Gemini-2.5-Pro and Claude-Sonnet-4, are increasingly used to empower advanced AI systems, including retrieval-augmented generation (RAG) pipelines and autonomous agents. In these systems, an LLM receives an instruction along with a context--often consisting of texts retrieved from a knowledge database or memory--and generates a response that is contextually grounded by following the instruction. Recent studies have designed solutions to trace back to a subset of texts in the context that contributes most to the response generated by the LLM. These solutions have numerous real-world applications, including performing post-attack forensic analysis and improving the interpretability and trustworthiness of LLM outputs. While significant efforts have been made, state-of-the-art solutions such as TracLLM often lead to a high computation cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a single response-context pair. In this work, we propose AttnTrace, a new context traceback method based on the attention weights produced by an LLM for a prompt. To effectively utilize attention weights, we introduce two techniques designed to enhance the effectiveness of AttnTrace, and we provide theoretical insights for our design choice. We also perform a systematic evaluation for AttnTrace. The results demonstrate that AttnTrace is more accurate and efficient than existing state-of-the-art context traceback methods. We also show that AttnTrace can improve state-of-the-art methods in detecting prompt injection under long contexts through the attribution-before-detection paradigm. As a real-world application, we demonstrate that AttnTrace can effectively pinpoint injected instructions in a paper designed to manipulate LLM-generated reviews. The code is at https://github.com/Wang-Yanting/AttnTrace.

