---
layout: default
title: Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following
---

# Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03178" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03178v1</a>
  <a href="https://arxiv.org/pdf/2508.03178.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03178v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03178v1', 'Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenyang Wang, Liang Wen, Shousheng Jia, Xiangzheng Zhang, Liang Xu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

**å¤‡æ³¨**: 12 pages, 10 figures, 7 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLight-IFæ¡†æ¶ä»¥è§£å†³å¤æ‚æŒ‡ä»¤éµå¾ªä¸­çš„æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤æ‚æŒ‡ä»¤` `æ¨ç†æœºåˆ¶` `é¢„è§ˆä¸è‡ªæ£€` `å¼ºåŒ–å­¦ä¹ ` `ç†µä¿æŒå¾®è°ƒ` `æ•°æ®é›†ç­–åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éµå¾ªå¤æ‚æŒ‡ä»¤æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºæ¨ç†è¿‡ç¨‹ä¸­çš„æ‡’æƒ°æ€ç»´å¯¼è‡´çš„ã€‚
2. è®ºæ–‡æå‡ºäº†Light-IFæ¡†æ¶ï¼Œé€šè¿‡é¢„è§ˆå’Œè‡ªæ£€æœºåˆ¶æ¥å¢å¼ºæ¨ç†è¿‡ç¨‹ï¼Œç¡®ä¿æ¨¡å‹èƒ½æœ‰æ•ˆéµå¾ªå¤æ‚æŒ‡ä»¤ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLight-IF-32Bæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šç°æœ‰çš„å¼€æºå’Œé—­æºæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦é—®é¢˜ã€ç¼–ç ä»»åŠ¡å’Œä¸€èˆ¬è°œé¢˜çš„æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤æ‚æŒ‡ä»¤çš„éµå¾ªä¸Šä»å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ¨ç†é˜¶æ®µçš„æ‡’æƒ°æ€ç»´æ˜¯å¯¼è‡´æŒ‡ä»¤éµå¾ªä¸ä½³çš„ä¸»è¦åŸå› ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œé€šè¿‡é¢„è§ˆå’Œè‡ªæ£€æ¥å¢å¼ºæ¨ç†è¿‡ç¨‹ï¼Œä»¥æ»¡è¶³ä¸¥æ ¼çš„æŒ‡ä»¤çº¦æŸã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ç”Ÿæˆå…·æœ‰å¤æ‚çº¦æŸçš„æŒ‡ä»¤ï¼Œå¹¶é€šè¿‡è¿‡æ»¤è¿‡ç¨‹è·å¾—æœ‰æ•ˆæç¤ºï¼Œå½¢æˆéš¾åº¦åˆ†ç±»çš„ä¸‰ç§æç¤ºæ•°æ®é›†ã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨æ‹’ç»é‡‡æ ·æ–¹æ³•æ¥ç­–åˆ’é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¹¶ç»“åˆç†µä¿æŒçš„ç›‘ç£å¾®è°ƒå’ŒåŸºäºè§„åˆ™çš„å¯†é›†å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œé¼“åŠ±æ¨¡å‹è½¬å˜æ¨ç†æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLight-IF-32Bæ¨¡å‹åœ¨å¤šä¸ªæŒ‡ä»¤éµå¾ªåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šç°æœ‰æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤éµå¾ªä¸­çš„æ¨ç†ä¸è¶³é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶å¸¸å¸¸å‡ºç°æ‡’æƒ°æ¨ç†ï¼Œå¯¼è‡´æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Light-IFæ¡†æ¶é€šè¿‡å¼•å…¥é¢„è§ˆå’Œè‡ªæ£€æœºåˆ¶ï¼Œå¢å¼ºæ¨ç†è¿‡ç¨‹çš„ä¸¥è°¨æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹å¤æ‚æŒ‡ä»¤çš„éµå¾ªèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç”Ÿæˆå¤æ‚çº¦æŸçš„æŒ‡ä»¤ã€è¿‡æ»¤æœ‰æ•ˆæç¤ºã€æ‹’ç»é‡‡æ ·é«˜è´¨é‡æ•°æ®é›†ã€ç†µä¿æŒçš„ç›‘ç£å¾®è°ƒï¼ˆEntropy-SFTï¼‰å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆTEA-RLï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†é¢„è§ˆå’Œè‡ªæ£€æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œè‡ªæˆ‘éªŒè¯ï¼Œä»è€Œæå‡æ¨ç†çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç†µä¿æŒçš„æŸå¤±å‡½æ•°ï¼Œå¹¶é€šè¿‡è§„åˆ™å¼•å¯¼çš„å¯†é›†å¥–åŠ±æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†æœºåˆ¶ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤ä¸‹çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLight-IF-32Bæ¨¡å‹åœ¨å¤šä¸ªæŒ‡ä»¤éµå¾ªåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†DeepSeek-R1å’ŒDoubao-1.6ç­‰æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–ç¼–ç¨‹ã€æ•™è‚²è¾…å¯¼ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„æŒ‡ä»¤éµå¾ªç³»ç»Ÿçš„å¼€å‘ï¼Œä¿ƒè¿›äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While advancements in the reasoning abilities of LLMs have significantly enhanced their performance in solving mathematical problems, coding tasks, and general puzzles, their effectiveness in accurately adhering to instructions remains inconsistent, particularly with more complex directives. Our investigation identifies lazy reasoning during the thinking stage as the primary factor contributing to poor instruction adherence. To mitigate this issue, we propose a comprehensive framework designed to enable rigorous reasoning processes involving preview and self-checking, essential for satisfying strict instruction constraints. Specifically, we first generate instructions with complex constraints and apply a filtering process to obtain valid prompts, resulting in three distinct prompt datasets categorized as hard, easy, and pass. Then, we employ rejection sampling on the pass prompts to curate a small yet high-quality dataset, enabling a cold-start initialization of the model and facilitating its adaptation to effective reasoning patterns. Subsequently, we employ an entropy-preserving supervised fine-tuning (Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL) reinforcement learning guided by rule-based dense rewards. This approach encourages the model to transform its reasoning mechanism, ultimately fostering generalizable reasoning abilities that encompass preview and self-checking. Extensive experiments conducted on instruction-following benchmarks demonstrate remarkable performance improvements across various model scales. Notably, our Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1 and closed-source models like Doubao-1.6.

