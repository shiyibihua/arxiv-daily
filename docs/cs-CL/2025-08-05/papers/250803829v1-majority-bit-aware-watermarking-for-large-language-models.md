---
layout: default
title: Majority Bit-Aware Watermarking For Large Language Models
---

# Majority Bit-Aware Watermarking For Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03829" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03829v1</a>
  <a href="https://arxiv.org/pdf/2508.03829.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03829v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03829v1', 'Majority Bit-Aware Watermarking For Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiahao Xu, Rui Hu, Zikai Zhang

**åˆ†ç±»**: cs.CL, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

**å¤‡æ³¨**: Preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMajorMarkä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æ°´å°è´¨é‡ä¸è§£ç å‡†ç¡®æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ°´å°æŠ€æœ¯` `æ–‡æœ¬ç”Ÿæˆ` `è§£ç å‡†ç¡®æ€§` `å†…å®¹éªŒè¯` `èšç±»åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¯”ç‰¹æ°´å°æ–¹æ¡ˆåœ¨æ–‡æœ¬è´¨é‡ä¸è§£ç å‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œé™åˆ¶äº†ä¼˜å…ˆæ ‡è®°é›†åˆçš„å¤§å°ï¼Œå½±å“ç”Ÿæˆå†…å®¹çš„è´¨é‡ã€‚
2. æœ¬æ–‡æå‡ºçš„MajorMarkæ–¹æ³•é€šè¿‡åŸºäºå¤šæ•°æ¯”ç‰¹çš„ç¼–ç ç­–ç•¥ï¼Œé€‰æ‹©ä¼˜å…ˆæ ‡è®°é›†åˆï¼Œæå‡äº†æ ‡è®°çš„çµæ´»æ€§å’Œé‡‡æ ·èŒƒå›´ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMajorMarkåœ¨è§£ç å‡†ç¡®æ€§å’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¯”ç‰¹æ°´å°æ–¹æ¡ˆï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ï¼Œå¦‚ä½•é˜²æ­¢å…¶ç”Ÿæˆæœ‰å®³æˆ–è¯¯å¯¼æ€§å†…å®¹æˆä¸ºä¸€ä¸ªé‡è¦é—®é¢˜ã€‚æ°´å°æŠ€æœ¯ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å°†å¯è¯†åˆ«çš„äºŒè¿›åˆ¶ä¿¡æ¯åµŒå…¥ç”Ÿæˆæ–‡æœ¬ä¸­ï¼Œå®ç°æ¥æºéªŒè¯å’Œæ»¥ç”¨è¿½è¸ªã€‚å°½ç®¡ç°æœ‰çš„å¤šæ¯”ç‰¹æ°´å°æ–¹æ¡ˆèƒ½å¤ŸåµŒå…¥ä¸°å¯Œçš„ä¿¡æ¯ï¼Œä½†é€šå¸¸é¢ä¸´æ–‡æœ¬è´¨é‡ä¸è§£ç å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ã€‚æœ¬æ–‡æå‡ºçš„MajorMarkæ–¹æ³•é€šè¿‡åŸºäºå¤šæ•°æ¯”ç‰¹çš„ç¼–ç ç­–ç•¥ï¼Œé€‰æ‹©ä¼˜å…ˆçš„æ ‡è®°é›†åˆï¼Œä»è€Œå®ç°æ›´å¤§ä¸”çµæ´»çš„æ ‡è®°é‡‡æ ·ã€‚ä¸ä¾èµ–æ ‡è®°é¢‘ç‡åˆ†æçš„è§£ç æ–¹æ³•ä¸åŒï¼ŒMajorMarké‡‡ç”¨åŸºäºèšç±»çš„è§£ç ç­–ç•¥ï¼Œç¡®ä¿åœ¨ä¼˜å…ˆæ ‡è®°é›†åˆè¾ƒå¤§æ—¶ä»èƒ½ä¿æŒé«˜è§£ç å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†MajorMark$^+$ï¼Œå°†æ¶ˆæ¯åˆ†ä¸ºå¤šä¸ªå—ç‹¬ç«‹ç¼–ç å’Œè§£ç ï¼Œè¿›ä¸€æ­¥æå‡æ°´å°æ–‡æœ¬çš„è´¨é‡å’Œè§£ç å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§£ç å‡†ç¡®æ€§å’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¯”ç‰¹æ°´å°åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¯”ç‰¹æ°´å°æ–¹æ¡ˆåœ¨æ–‡æœ¬ç”Ÿæˆè´¨é‡ä¸è§£ç å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é™åˆ¶ä¼˜å…ˆæ ‡è®°é›†åˆçš„å¤§å°ï¼Œä»¥ç¡®ä¿å¯é çš„æ¶ˆæ¯è§£ç ï¼Œè¿™å¯¼è‡´ç”Ÿæˆå†…å®¹è´¨é‡ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„MajorMarkæ–¹æ³•é€šè¿‡åŸºäºå¤šæ•°æ¯”ç‰¹çš„ç¼–ç ç­–ç•¥ï¼Œé€‰æ‹©ä¼˜å…ˆæ ‡è®°é›†åˆï¼Œå…è®¸æ›´å¤§ä¸”çµæ´»çš„æ ‡è®°é‡‡æ ·ï¼Œä»è€Œåœ¨ä¿æŒæ–‡æœ¬è´¨é‡çš„åŒæ—¶æé«˜è§£ç å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMajorMarkçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºå¤šæ•°æ¯”ç‰¹çš„ç¼–ç æ¨¡å—ï¼Œå…¶æ¬¡æ˜¯èšç±»è§£ç æ¨¡å—ã€‚ç¼–ç æ¨¡å—æ ¹æ®æ¶ˆæ¯çš„å¤šæ•°æ¯”ç‰¹é€‰æ‹©æ ‡è®°ï¼Œè€Œè§£ç æ¨¡å—åˆ™é€šè¿‡èšç±»åˆ†æå®ç°é«˜å‡†ç¡®æ€§çš„è§£ç ã€‚

**å…³é”®åˆ›æ–°**ï¼šMajorMarkçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŸºäºå¤šæ•°æ¯”ç‰¹çš„ç¼–ç ç­–ç•¥å’Œèšç±»è§£ç æ–¹æ³•ï¼Œè¿™ä¸ä¼ ç»Ÿä¾èµ–æ ‡è®°é¢‘ç‡åˆ†æçš„è§£ç æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿåœ¨ä¼˜å…ˆæ ‡è®°é›†åˆè¾ƒå¤§æ—¶ä»ä¿æŒé«˜è§£ç å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒMajorMarké€šè¿‡åŠ¨æ€è°ƒæ•´ä¼˜å…ˆæ ‡è®°é›†åˆçš„å¤§å°æ¥ä¼˜åŒ–ç¼–ç è¿‡ç¨‹ï¼ŒåŒæ—¶åœ¨è§£ç æ—¶é‡‡ç”¨èšç±»ç®—æ³•æ¥æé«˜å‡†ç¡®æ€§ã€‚MajorMark$^+$è¿›ä¸€æ­¥å°†æ¶ˆæ¯åˆ†å—ç¼–ç ï¼Œç¡®ä¿æ¯ä¸ªå—çš„ç‹¬ç«‹æ€§å’Œå¯è§£ç æ€§ï¼Œæå‡æ•´ä½“æ°´å°æ–‡æœ¬çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMajorMarkåœ¨è§£ç å‡†ç¡®æ€§ä¸Šæå‡äº†XX%ï¼Œæ–‡æœ¬ç”Ÿæˆè´¨é‡æé«˜äº†YY%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¯”ç‰¹æ°´å°åŸºçº¿ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†…å®¹ç”Ÿæˆã€ç¤¾äº¤åª’ä½“ã€åœ¨çº¿æ•™è‚²ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å†…å®¹è¢«æ»¥ç”¨æˆ–è¯¯ç”¨ã€‚é€šè¿‡æ°´å°æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥è¿½è¸ªå†…å®¹æ¥æºï¼Œç¡®ä¿ä¿¡æ¯çš„çœŸå®æ€§å’Œå¯é æ€§ï¼Œæœªæ¥å¯èƒ½å¯¹å†…å®¹å®¡æ ¸å’Œç‰ˆæƒä¿æŠ¤äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The growing deployment of Large Language Models (LLMs) in real-world applications has raised concerns about their potential misuse in generating harmful or deceptive content. To address this issue, watermarking techniques have emerged as a promising solution by embedding identifiable binary messages into generated text for origin verification and misuse tracing. While recent efforts have explored multi-bit watermarking schemes capable of embedding rich information such as user identifiers, they typically suffer from the fundamental trade-off between text quality and decoding accuracy: to ensure reliable message decoding, they have to restrict the size of preferred token sets during encoding, yet such restrictions reduce the quality of the generated content. In this work, we propose MajorMark, a novel watermarking method that improves this trade-off through majority bit-aware encoding. MajorMark selects preferred token sets based on the majority bit of the message, enabling a larger and more flexible sampling of tokens. In contrast to prior methods that rely on token frequency analysis for decoding, MajorMark employs a clustering-based decoding strategy, which maintains high decoding accuracy even when the preferred token set is large, thus preserving both content quality and decoding accuracy. We further introduce MajorMark$^+$, which partitions the message into multiple blocks to independently encode and deterministically decode each block, thereby further enhancing the quality of watermarked text and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs demonstrate that our methods significantly enhance both decoding accuracy and text generation quality, outperforming prior multi-bit watermarking baselines.

