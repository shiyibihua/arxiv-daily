---
layout: default
title: Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation
---

# Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03098" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03098v1</a>
  <a href="https://arxiv.org/pdf/2508.03098.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03098v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03098v1', 'Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoran Wang, Xiongxiao Xu, Baixiang Huang, Kai Shu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/wang2226/PAD)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéšç§æ„ŸçŸ¥è§£ç ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹éšç§æ³„éœ²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éšç§ä¿æŠ¤` `å¤§è¯­è¨€æ¨¡å‹` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å·®åˆ†éšç§` `é«˜æ–¯å™ªå£°` `ä¿¡æ¯å®‰å…¨` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†ç§å¯†æ•°æ®æ—¶å®¹æ˜“é­å—æå–æ”»å‡»ï¼Œå¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚
2. æœ¬æ–‡æå‡ºéšç§æ„ŸçŸ¥è§£ç ï¼ˆPADï¼‰ï¼Œé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ³¨å…¥é«˜æ–¯å™ªå£°æ¥ä¿æŠ¤é«˜é£é™©tokenï¼Œå¢å¼ºéšç§ä¿æŠ¤ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPADåœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†éšç§æ³„éœ²ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆå“åº”çš„è´¨é‡ï¼Œè¶…è¶Šäº†ç°æœ‰é˜²å¾¡æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¤–éƒ¨çŸ¥è¯†æºæé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„äº‹å®å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå½“æ£€ç´¢æ¶‰åŠç§å¯†æˆ–æ•æ„Ÿæ•°æ®æ—¶ï¼ŒRAGç³»ç»Ÿå®¹æ˜“å—åˆ°æå–æ”»å‡»ï¼Œå¯¼è‡´æœºå¯†ä¿¡æ¯æ³„éœ²ã€‚æœ¬æ–‡æå‡ºéšç§æ„ŸçŸ¥è§£ç ï¼ˆPADï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„æ¨ç†æ—¶é˜²å¾¡æ–¹æ³•ï¼Œé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°å‘token logitsæ³¨å…¥ç»è¿‡æ ¡å‡†çš„é«˜æ–¯å™ªå£°ã€‚PADç»“åˆåŸºäºç½®ä¿¡åº¦çš„ç­›é€‰ã€æœ‰æ•ˆçš„æ•æ„Ÿæ€§ä¼°è®¡å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å™ªå£°æ ¡å‡†ï¼Œä»¥å¹³è¡¡éšç§ä¸ç”Ÿæˆè´¨é‡ã€‚é€šè¿‡enyiå·®åˆ†éšç§ï¼ˆRDPï¼‰ä¼šè®¡ï¼Œä¸¥æ ¼è·Ÿè¸ªç´¯ç§¯éšç§æŸå¤±ï¼Œä¸ºæ•æ„Ÿè¾“å‡ºæä¾›æ˜ç¡®çš„æ¯å“åº”$(	ext{Îµ}, 	ext{Î´})$-DPä¿è¯ã€‚ä¸éœ€è¦é‡æ–°è®­ç»ƒæˆ–è¯­æ–™åº“çº§è¿‡æ»¤çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒPADæ˜¯æ¨¡å‹æ— å…³çš„ï¼Œå®Œå…¨åœ¨è§£ç æ—¶æ“ä½œï¼Œä¸”è®¡ç®—å¼€é”€æœ€å°ã€‚å®éªŒè¡¨æ˜ï¼ŒPADæ˜¾è‘—å‡å°‘äº†ç§å¯†ä¿¡æ¯æ³„éœ²ï¼ŒåŒæ—¶ä¿æŒå“åº”çš„å®ç”¨æ€§ï¼Œä¼˜äºç°æœ‰çš„æ£€ç´¢å’Œåå¤„ç†é˜²å¾¡æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨å¤„ç†ç§å¯†æ•°æ®æ—¶çš„éšç§æ³„éœ²é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹æå–æ”»å‡»æ—¶ï¼Œæ— æ³•æœ‰æ•ˆä¿æŠ¤ç”Ÿæˆçš„å“åº”ï¼Œå¯¼è‡´æœºå¯†ä¿¡æ¯çš„æ³„éœ²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºéšç§æ„ŸçŸ¥è§£ç ï¼ˆPADï¼‰ï¼Œé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°å‘token logitsæ³¨å…¥é«˜æ–¯å™ªå£°ï¼Œç»“åˆç½®ä¿¡åº¦ç­›é€‰å’Œæ•æ„Ÿæ€§ä¼°è®¡ï¼Œé€‰æ‹©æ€§åœ°ä¿æŠ¤é«˜é£é™©tokenï¼Œä»è€Œå¢å¼ºéšç§ä¿æŠ¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPADçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç½®ä¿¡åº¦ç­›é€‰ï¼Œè¯†åˆ«é«˜é£é™©tokenï¼›2) æ•æ„Ÿæ€§ä¼°è®¡ï¼Œè¯„ä¼°å™ªå£°æ³¨å…¥çš„å¿…è¦æ€§ï¼›3) å™ªå£°æ ¡å‡†ï¼Œç¡®ä¿ç”Ÿæˆè´¨é‡ä¸éšç§ä¿æŠ¤ä¹‹é—´çš„å¹³è¡¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šPADçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨¡å‹æ— å…³æ€§å’Œæ¨ç†æ—¶æ“ä½œçš„èƒ½åŠ›ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•éœ€è¦é‡æ–°è®­ç»ƒæˆ–è¿›è¡Œè¯­æ–™åº“çº§è¿‡æ»¤çš„ç¼ºé™·ã€‚é€šè¿‡RDPä¼šè®¡ï¼ŒPADèƒ½å¤Ÿæä¾›æ˜ç¡®çš„éšç§ä¿è¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒPADä½¿ç”¨äº†åŸºäºç½®ä¿¡åº¦çš„ç­›é€‰æœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´å™ªå£°æ³¨å…¥çš„å¼ºåº¦ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œå™ªå£°çš„æ ¡å‡†ï¼Œç¡®ä¿ç”Ÿæˆçš„å“åº”æ—¢ä¿æŠ¤éšç§åˆä¿æŒå®ç”¨æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒä¸­è¿›è¡Œäº†ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPADåœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†ç§å¯†ä¿¡æ¯æ³„éœ²ï¼Œå…·ä½“æ€§èƒ½æå‡è¶…è¿‡äº†ç°æœ‰çš„æ£€ç´¢å’Œåå¤„ç†é˜²å¾¡æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨éšç§ä¿æŠ¤æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œç¤¾äº¤åª’ä½“ç­‰æ•æ„Ÿæ•°æ®å¤„ç†åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆä¿æŠ¤ç”¨æˆ·éšç§ï¼ŒPADä¸ºå¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§æä¾›äº†ä¿éšœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large language models (LLMs) by conditioning outputs on external knowledge sources. However, when retrieval involves private or sensitive data, RAG systems are susceptible to extraction attacks that can leak confidential information through generated responses. We propose Privacy-Aware Decoding (PAD), a lightweight, inference-time defense that adaptively injects calibrated Gaussian noise into token logits during generation. PAD integrates confidence-based screening to selectively protect high-risk tokens, efficient sensitivity estimation to minimize unnecessary noise, and context-aware noise calibration to balance privacy with generation quality. A \renyi Differential Privacy (RDP) accountant rigorously tracks cumulative privacy loss, enabling explicit per-response $(\varepsilon, Î´)$-DP guarantees for sensitive outputs. Unlike prior approaches requiring retraining or corpus-level filtering, PAD is model-agnostic and operates entirely at decoding time with minimal computational overhead. Experiments on three real-world datasets demonstrate that PAD substantially reduces private information leakage while preserving response utility, outperforming existing retrieval- and post-processing-based defenses. Our work takes an important step toward mitigating privacy risks in RAG via decoding strategies, paving the way for universal and scalable privacy solutions in sensitive domains. Our code is available: https://github.com/wang2226/PAD.

