---
layout: default
title: SABER: Switchable and Balanced Training for Efficient LLM Reasoning
---

# SABER: Switchable and Balanced Training for Efficient LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10026" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10026v1</a>
  <a href="https://arxiv.org/pdf/2508.10026.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10026v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10026v1', 'SABER: Switchable and Balanced Training for Efficient LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kai Zhao, Yanjun Zhao, Jiaming Song, Shien He, Lusheng Zhang, Qiang Zhang, Tianjiao Li

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSABERä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç‡` `å¼ºåŒ–å­¦ä¹ ` `å¯åˆ‡æ¢æ¨ç†` `ä»¤ç‰Œé¢„ç®—` `è·¨é¢†åŸŸæ³›åŒ–` `æ— æ€è€ƒç¤ºä¾‹` `ç³»ç»Ÿæç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œæ¨ç†æˆæœ¬å’Œå»¶è¿Ÿè¿‡é«˜ï¼Œéš¾ä»¥é«˜æ•ˆåº”ç”¨äºå„ç§é—®é¢˜ã€‚
2. SABERé€šè¿‡å¼•å…¥ç”¨æˆ·å¯æ§çš„ä»¤ç‰Œé¢„ç®—å’Œå¤šç§æ¨ç†æ¨¡å¼ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„æ¨ç†æ•ˆç‡å’Œçµæ´»æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSABERåœ¨MATHåŸºå‡†ä¸Šå®ç°äº†65.4%çš„æ¨ç†é•¿åº¦ç¼©å‡å’Œ3.6%çš„å‡†ç¡®æ€§æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡é“¾å¼æ¨ç†åœ¨å¤æ‚ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„å‡†ç¡®æ€§ï¼Œä½†åœ¨ç»Ÿä¸€åº”ç”¨äºæ‰€æœ‰é—®é¢˜æ—¶ï¼Œé¢ä¸´è¿‡é«˜çš„æ¨ç†æˆæœ¬å’Œå»¶è¿Ÿã€‚æœ¬æ–‡æå‡ºäº†SABERï¼ˆå¯åˆ‡æ¢å’Œå‡è¡¡è®­ç»ƒæ¡†æ¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿LLMså…·å¤‡ç”¨æˆ·å¯æ§çš„ã€åŸºäºä»¤ç‰Œé¢„ç®—çš„æ¨ç†èƒ½åŠ›ã€‚SABERé¦–å…ˆåˆ†ææ¯ä¸ªè®­ç»ƒç¤ºä¾‹çš„åŸºç¡€æ¨¡å‹æ€ç»´ä»¤ç‰Œä½¿ç”¨æƒ…å†µï¼Œå¹¶å°†å…¶åˆ†é…åˆ°é¢„å®šä¹‰çš„é¢„ç®—å±‚çº§ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡ç³»ç»Ÿæç¤ºå’Œé•¿åº¦æ„ŸçŸ¥å¥–åŠ±æ¥éµå¾ªå…¶åˆ†é…çš„é¢„ç®—ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥æ— æ€è€ƒç¤ºä¾‹ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å…³é—­æ˜¾å¼æ¨ç†æ—¶ä»ç„¶å¯é ã€‚SABERæ”¯æŒå››ç§ç¦»æ•£æ¨ç†æ¨¡å¼â€”â€”NoThinkã€FastThinkã€CoreThinkå’ŒDeepThinkï¼Œçµæ´»æƒè¡¡å»¶è¿Ÿä¸æ¨ç†æ·±åº¦ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼ŒSABERåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸­ï¼Œåœ¨ä¸¥æ ¼é¢„ç®—ä¸‹å®ç°äº†é«˜å‡†ç¡®æ€§å’Œæœ‰æ•ˆçš„è·¨å°ºåº¦ã€è·¨é¢†åŸŸæ³›åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´çš„é«˜æˆæœ¬å’Œå»¶è¿Ÿé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒå¤æ‚åº¦ä»»åŠ¡æ—¶ï¼Œæœªèƒ½æœ‰æ•ˆè°ƒæ•´æ¨ç†èµ„æºï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSABERé€šè¿‡å»ºç«‹ç”¨æˆ·å¯æ§çš„ä»¤ç‰Œé¢„ç®—å’Œå¤šç§æ¨ç†æ¨¡å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚çµæ´»è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚è¯¥è®¾è®¡å…è®¸æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´è¿›è¡Œæœ‰æ•ˆçš„èµ„æºåˆ†é…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSABERçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è®­ç»ƒç¤ºä¾‹çš„é¢„ç®—åˆ†é…ã€ç³»ç»Ÿæç¤ºå¼•å¯¼çš„å¾®è°ƒè¿‡ç¨‹ä»¥åŠæ— æ€è€ƒç¤ºä¾‹çš„å¼•å…¥ã€‚æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®æ¯ä¸ªç¤ºä¾‹çš„å¤æ‚æ€§åˆ†é…é¢„ç®—ï¼Œå¹¶åœ¨æ¨ç†æ—¶é€‰æ‹©é€‚å½“çš„æ¨¡å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šSABERçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¯åˆ‡æ¢çš„æ¨ç†æ¨¡å¼ï¼ˆNoThinkã€FastThinkã€CoreThinkã€DeepThinkï¼‰ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ·±åº¦å’Œå»¶è¿Ÿä¹‹é—´è¿›è¡Œçµæ´»çš„æƒè¡¡ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„å›ºå®šæ¨ç†æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨é•¿åº¦æ„ŸçŸ¥å¥–åŠ±æœºåˆ¶æ¥å¼•å¯¼æ¨¡å‹éµå¾ªé¢„ç®—ï¼ŒåŒæ—¶å¼•å…¥æ— æ€è€ƒç¤ºä¾‹ä»¥å¢å¼ºæ¨¡å‹çš„å¯é æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨MATHåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSABER-FastThinkæ¨¡å¼å°†æ¨ç†é•¿åº¦ç¼©çŸ­äº†65.4%ï¼ŒåŒæ—¶å®ç°äº†3.6%çš„å‡†ç¡®æ€§æå‡ï¼Œæ˜¾ç¤ºå‡ºåœ¨ä¸¥æ ¼é¢„ç®—ä¸‹çš„é«˜æ•ˆæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSABERåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œé€»è¾‘æ¨ç†ç­‰ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SABERçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ•™è‚²ã€ç¼–ç¨‹è¾…åŠ©å’Œé€»è¾‘æ¨ç†ç­‰ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼ŒSABERå¯ä»¥å¸®åŠ©ç”¨æˆ·åœ¨æ›´çŸ­çš„æ—¶é—´å†…è·å¾—æ›´å‡†ç¡®çš„ç»“æœï¼Œæå‡å·¥ä½œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶çš„çµæ´»æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ï¼Œå…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems. We propose SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. SABER first profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. In parallel, we incorporate no-think examples to ensure the model remains reliable even when explicit reasoning is turned off. SABER further supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling flexible trade-offs between latency and reasoning depth. Extensive evaluations on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning (LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.

