---
layout: default
title: Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages
---

# Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06435" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06435v1</a>
  <a href="https://arxiv.org/pdf/2508.06435.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06435v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06435v1', 'Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrea Nasuto, Stefano Maria Iacus, Francisco Rowe, Devika Jain

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§LLMä»¥è·¨è¯­è¨€åˆ†ç±»ç§»æ°‘è¯é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è·¨è¯­è¨€åˆ†ç±»` `ç§»æ°‘è¯é¢˜` `å¾®è°ƒæŠ€æœ¯` `ç¤¾ä¼šç§‘å­¦ç ”ç©¶` `å¼€æºæ¨¡å‹` `æ¨æ–‡åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è·¨è¯­è¨€è¯é¢˜æ£€æµ‹ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹æœªè§è¯­è¨€çš„åˆ†ç±»èƒ½åŠ›ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¯¹LLaMAæ¨¡å‹è¿›è¡Œè½»é‡çº§å¾®è°ƒï¼Œåˆ©ç”¨å•è¯­ã€åŒè¯­æˆ–å¤šè¯­æ•°æ®é›†è¿›è¡Œç§»æ°‘è¯é¢˜åˆ†ç±»ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨æœªè§è¯­è¨€ä¸­è¡¨ç°è‰¯å¥½ï¼Œä¸”å¤šè¯­è¨€å¾®è°ƒæ˜¾è‘—æé«˜äº†åˆ†ç±»å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨é€šè¿‡å®ç°å¯æ‰©å±•ã€ç²¾å‡†çš„åˆ†æï¼Œæ”¹å˜ç¤¾ä¼šç§‘å­¦ç ”ç©¶ã€‚æœ¬æ–‡æ¢è®¨äº†é€šè¿‡åœ¨å°‘æ•°è¯­è¨€ä¸Šå¾®è°ƒè·å¾—çš„çŸ¥è¯†æ˜¯å¦èƒ½è½¬ç§»åˆ°åœ¨é¢„è®­ç»ƒä¸­æœªè§è¿‡çš„è¯­è¨€ã€‚ç ”ç©¶é€šè¿‡å¯¹LLaMA 3.2-3Bæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨å•è¯­ã€åŒè¯­æˆ–å¤šè¯­æ•°æ®é›†å¯¹æ¥è‡ªX/Twitterçš„ç§»æ°‘ç›¸å…³æ¨æ–‡è¿›è¡Œåˆ†ç±»ã€‚ç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„LLMsèƒ½å¤Ÿåœ¨æœªè§è¯­è¨€ä¸­å¯é åœ°åˆ†ç±»ç§»æ°‘å†…å®¹ï¼Œä¸”å¤šè¯­è¨€å¾®è°ƒæœ‰åŠ©äºæ›´å¥½åœ°è¯†åˆ«æ¨æ–‡çš„ç§»æ°‘ç«‹åœºã€‚å³ä½¿åœ¨å¾®è°ƒä¸­å¯¹ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€è¿›è¡Œæœ€å°æš´éœ²ï¼Œä¹Ÿèƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚è¯¥ç ”ç©¶æŒ‘æˆ˜äº†è·¨è¯­è¨€æŒæ¡éœ€è¦å¹¿æ³›å¤šè¯­è¨€è®­ç»ƒçš„å‡è®¾ï¼Œæä¾›äº†å¼€æºçš„ã€å¯é‡å¤çš„LLMæ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨è¯­è¨€ç§»æ°‘è¯é¢˜åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æœªè§è¯­è¨€æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸”å­˜åœ¨é¢„è®­ç»ƒåè§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹LLaMAæ¨¡å‹è¿›è¡Œè½»é‡çº§å¾®è°ƒï¼Œåˆ©ç”¨æœ‰é™çš„è¯­è¨€æ•°æ®å®ç°è·¨è¯­è¨€è¯é¢˜æ£€æµ‹ï¼Œæ—¨åœ¨è¯æ˜å°‘é‡è¯­è¨€è¦†ç›–è¶³ä»¥å®ç°ä¸»é¢˜çº§åˆ«çš„æ³›åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨LLaMA 3.2-3Bæ¨¡å‹ï¼Œè¿›è¡Œå•è¯­ã€åŒè¯­å’Œå¤šè¯­æ•°æ®é›†çš„å¾®è°ƒï¼Œæ¨¡å‹ç»“æ„ä¿æŒè½»é‡åŒ–ï¼Œä¾¿äºå¿«é€Ÿæ¨ç†å’Œéƒ¨ç½²ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå±•ç¤ºäº†å³ä½¿åœ¨å¾®è°ƒä¸­å¯¹ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€è¿›è¡Œæå°çš„æš´éœ²ï¼Œä¹Ÿèƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨æœªè§è¯­è¨€ä¸­çš„åˆ†ç±»èƒ½åŠ›ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„å¤šè¯­è¨€è®­ç»ƒå‡è®¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ ä¸åŒè¯­è¨€çš„ç§»æ°‘è¯é¢˜ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„LLMsåœ¨æœªè§è¯­è¨€çš„ç§»æ°‘å†…å®¹åˆ†ç±»ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè¯­è¨€å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåˆ†ç±»å‡†ç¡®æ€§æ˜¾è‘—æé«˜ã€‚å¾®è°ƒæ¨¡å‹çš„æ¨ç†é€Ÿåº¦æ¯”OpenAIçš„GPT-4oå¿«35å€ï¼Œä¸”æˆæœ¬ä»…ä¸ºå…¶0.00000989%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾ä¼šç§‘å­¦ç ”ç©¶ã€ç§»æ°‘æ”¿ç­–åˆ†æå’Œå…¬å…±èˆ†è®ºç›‘æµ‹ã€‚é€šè¿‡æä¾›é«˜æ•ˆçš„è·¨è¯­è¨€åˆ†ç±»å·¥å…·ï¼Œç ”ç©¶èƒ½å¤Ÿå¸®åŠ©æ”¿ç­–åˆ¶å®šè€…å’Œç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„ç§»æ°‘è¯é¢˜ï¼Œä»è€Œæ¨åŠ¨æ›´å…·åŒ…å®¹æ€§çš„ç¤¾ä¼šå¯¹è¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are transforming social-science research by enabling scalable, precise analysis. Their adaptability raises the question of whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training. To examine this, we fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages, a domain characterised by polarised, culturally specific discourse. We evaluate whether minimal language-specific fine-tuning enables cross-lingual topic detection and whether adding targeted languages corrects pre-training biases. Results show that LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. However, identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training token volume) yields significant gains. These findings challenge the assumption that cross-lingual mastery requires extensive multilingual training: limited language coverage suffices for topic-level generalisation, and structural biases can be corrected with lightweight interventions. By releasing 4-bit-quantised, LoRA fine-tuned models, we provide an open-source, reproducible alternative to proprietary LLMs that delivers 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model, enabling scalable, inclusive research.

