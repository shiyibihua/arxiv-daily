---
layout: default
title: Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge
---

# Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06709" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06709v1</a>
  <a href="https://arxiv.org/pdf/2508.06709.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06709v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06709v1', 'Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Evangelia Spiliopoulou, Riccardo Fogliato, Hanna Burnsky, Tamer Soliman, Jie Ma, Graham Horwood, Miguel Ballesteros

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿè®¡æ–¹æ³•ä»¥æµ‹é‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘åè§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæˆ‘åè§` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç»Ÿè®¡æ–¹æ³•` `æ¨¡å‹è¯„ä¼°` `å…¬å¹³æ€§åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¸¸å¸¸å°†æ¨¡å‹è´¨é‡çš„çœŸå®å·®å¼‚ä¸åè§æ··æ·†ï¼Œå¯¼è‡´å¯¹æ¨¡å‹æ€§èƒ½çš„é”™è¯¯è¯„ä¼°ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿè®¡æ¡†æ¶ï¼Œèƒ½å¤Ÿæ˜ç¡®è¯†åˆ«å’Œé‡åŒ–LLMçš„è‡ªæˆ‘åè§ï¼Œç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚
3. é€šè¿‡å¯¹è¶…è¿‡5000ä¸ªæç¤º-å®Œæˆå¯¹çš„å®è¯åˆ†æï¼Œå‘ç°æŸäº›æ¨¡å‹å­˜åœ¨ç³»ç»Ÿæ€§çš„è‡ªæˆ‘åè§å’Œå®¶æ—åè§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥ä½œä¸ºè¯„åˆ¤è€…ï¼Œå¿«é€Ÿå¯é åœ°è¯„ä¼°å…¶ä»–LLMçš„è¾“å‡ºã€‚ç„¶è€Œï¼Œæ¨¡å‹å¯èƒ½ä¼šç³»ç»Ÿæ€§åœ°å¯¹è‡ªèº«è¾“å‡ºç»™äºˆè¿‡äºå®½æ¾çš„è¯„åˆ†ï¼Œè¿™ç§ç°è±¡ç§°ä¸ºè‡ªæˆ‘åè§ï¼Œå¯èƒ½ä¼šæ‰­æ›²å¯¹æ¨¡å‹çœŸå®æ€§èƒ½çš„è¯„ä¼°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿè®¡æ¡†æ¶ï¼Œæ˜ç¡®äº†è¯†åˆ«å’Œä¼°è®¡è‡ªæˆ‘åè§çš„å‡è®¾ã€‚è¯¥æ–¹æ³•å»ºæ¨¡LLMä½œä¸ºè¯„åˆ¤è€…å¯¹è‡ªèº«å®Œæˆçš„è¯„åˆ†åˆ†å¸ƒä¸å…¶ä»–æ¨¡å‹çš„å·®å¼‚ï¼ŒåŒæ—¶è€ƒè™‘ç‹¬ç«‹ç¬¬ä¸‰æ–¹è¯„åˆ¤è€…ï¼ˆå¦‚äººç±»ï¼‰æä¾›çš„å®Œæˆè´¨é‡ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªåŒ…å«è¶…è¿‡5000ä¸ªæç¤º-å®Œæˆå¯¹çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®è¯åˆ†æï¼Œå‘ç°æŸäº›æ¨¡å‹ï¼ˆå¦‚GPT-4oå’ŒClaude 3.5 Sonnetï¼‰ç³»ç»Ÿæ€§åœ°å¯¹è‡ªèº«è¾“å‡ºç»™äºˆæ›´é«˜çš„è¯„åˆ†ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ä½¿ç”¨LLMè¯„åˆ¤è€…çš„æ½œåœ¨é™·é˜±ï¼Œå¹¶æä¾›äº†å‡è½»åè§çš„å®ç”¨æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªæˆ‘è¯„ä¼°æ—¶å¯èƒ½å­˜åœ¨çš„è‡ªæˆ‘åè§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåŒºåˆ†æ¨¡å‹è´¨é‡ä¸åè§ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„ä¸å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„ç»Ÿè®¡æ¡†æ¶é€šè¿‡å»ºæ¨¡LLMå¯¹è‡ªèº«è¾“å‡ºä¸å…¶ä»–æ¨¡å‹è¾“å‡ºçš„è¯„åˆ†åˆ†å¸ƒå·®å¼‚ï¼Œæ˜ç¡®è¯†åˆ«è‡ªæˆ‘åè§ï¼ŒåŒæ—¶è€ƒè™‘ç¬¬ä¸‰æ–¹è¯„åˆ¤è€…çš„è´¨é‡è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…æ‹¬æ•°æ®æ”¶é›†ã€è¯„åˆ†åˆ†å¸ƒå»ºæ¨¡ã€åè§è¯†åˆ«ä¸é‡åŒ–ç­‰ä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†å¤§é‡æç¤º-å®Œæˆå¯¹ï¼Œç„¶åé€šè¿‡ç»Ÿè®¡åˆ†ææ¯”è¾ƒä¸åŒæ¨¡å‹çš„è¯„åˆ†åˆ†å¸ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„ç»Ÿè®¡æ–¹æ³•æ¥é‡åŒ–è‡ªæˆ‘åè§ï¼Œé¿å…äº†ä»¥å¾€æ–¹æ³•å¯¹æ¨¡å‹è´¨é‡ä¸åè§çš„æ··æ·†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†å¤šä¸ªå‚æ•°ä»¥ä¼˜åŒ–è¯„åˆ†åˆ†å¸ƒçš„æ¯”è¾ƒï¼Œé‡‡ç”¨äº†é€‚åˆçš„æŸå¤±å‡½æ•°æ¥ç¡®ä¿åè§çš„å‡†ç¡®è¯†åˆ«ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒæŸäº›æ¨¡å‹ï¼ˆå¦‚GPT-4oå’ŒClaude 3.5 Sonnetï¼‰åœ¨è‡ªæˆ‘è¯„åˆ†æ—¶ç³»ç»Ÿæ€§åœ°ç»™äºˆæ›´é«˜çš„åˆ†æ•°ï¼Œä¸”å­˜åœ¨å®¶æ—åè§ã€‚è¿™äº›å‘ç°ä¸ºä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤è€…æä¾›äº†é‡è¦çš„è­¦ç¤ºï¼Œå¹¶æå‡ºäº†å‡è½»åè§çš„å®ç”¨å»ºè®®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿã€æ¨¡å‹æ€§èƒ½ç›‘æµ‹å’Œå…¬å¹³æ€§åˆ†æç­‰ã€‚é€šè¿‡è¯†åˆ«å’Œå‡è½»è‡ªæˆ‘åè§ï¼Œå¯ä»¥æé«˜LLMåœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå…¬æ­£æ€§ï¼Œä¿ƒè¿›å…¶åœ¨æ•™è‚²ã€æ³•å¾‹å’Œå†…å®¹åˆ›ä½œç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) can serve as judges that offer rapid and reliable assessments of other LLM outputs. However, models may systematically assign overly favorable ratings to their own outputs, a phenomenon known as self-bias, which can distort evaluations of true model performance. Previous studies often conflate genuine differences in model quality with bias or incorrectly assume that evaluations from LLMs and humans follow the same rating distributions. In this work, we present a statistical framework that explicitly formalizes assumptions under which self-bias can be identified and estimated. Our method models the difference in the scoring distribution that LLM-as-a-judge assigns to its own completions compared to other models, while accounting for the underlying quality of the completions provided by an independent, third-party judge (e.g., humans). Our method reliably isolates and quantifies self-bias, even when models vary in ability, ensuring that genuine performance differences are not mistaken for self-bias. We conduct an empirical analysis of self-bias on a large dataset (>5000 prompt-completion pairs) consisting of expert human annotations and judgments from nine different LLM judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet, systematically assign higher scores to their own outputs. These models also display family-bias; systematically assigning higher ratings to outputs produced by other models of the same family. Our findings highlight potential pitfalls of using LLM judges and offer practical guidance to mitigate biases when interpreting automated evaluations.

