---
layout: default
title: Do Biased Models Have Biased Thoughts?
---

# Do Biased Models Have Biased Thoughts?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06671" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06671v2</a>
  <a href="https://arxiv.org/pdf/2508.06671.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06671v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06671v2', 'Do Biased Models Have Biased Thoughts?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Swati Rajwal, Shivank Garg, Reem Abdel-Salam, Abdelrahman Zayed

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08 (æ›´æ–°: 2025-08-12)

**å¤‡æ³¨**: Accepted at main track of the Second Conference on Language Modeling (COLM 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶é“¾å¼æ€ç»´æç¤ºå¯¹è¯­è¨€æ¨¡å‹åè§çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `åè§ç ”ç©¶` `é“¾å¼æ€ç»´æç¤º` `å…¬å¹³æ€§è¯„ä¼°` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹æ€ç»´` `å®éªŒåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åè§é—®é¢˜æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ€§åˆ«ã€ç§æ—ç­‰æ–¹é¢çš„åè§å½±å“å…¶è¾“å‡ºç»“æœã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡é“¾å¼æ€ç»´æç¤ºçš„æ–¹æ³•ï¼Œç ”ç©¶æ¨¡å‹åœ¨ç”Ÿæˆå“åº”å‰çš„æ€ç»´è¿‡ç¨‹ï¼Œä»¥è¯„ä¼°å…¶å…¬å¹³æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹æ€ç»´ä¸­çš„åè§ä¸å…¶è¾“å‡ºåè§ä¹‹é—´çš„ç›¸å…³æ€§è¾ƒä½ï¼Œè¡¨æ˜æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ä¸äººç±»æ€ç»´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­è¨€æ¨¡å‹çš„å“è¶Šæ€§èƒ½æ¯‹åº¸ç½®ç–‘ï¼Œä½†åŸºäºæ€§åˆ«ã€ç§æ—ã€ç¤¾ä¼šç»æµåœ°ä½ã€å¤–è²Œå’Œæ€§å–å‘çš„åè§ä½¿å¾—å…¶éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ç ”ç©¶äº†é“¾å¼æ€ç»´æç¤ºå¯¹å…¬å¹³æ€§çš„å½±å“ï¼Œæ¢è®¨äº†åè§æ¨¡å‹æ˜¯å¦å…·æœ‰åè§æ€ç»´ã€‚é€šè¿‡å¯¹äº”ä¸ªæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå®éªŒï¼Œä½¿ç”¨å…¬å¹³æ€§æŒ‡æ ‡é‡åŒ–æ¨¡å‹æ€ç»´å’Œè¾“å‡ºä¸­çš„11ç§ä¸åŒåè§ã€‚ç»“æœè¡¨æ˜ï¼Œæ€ç»´æ­¥éª¤ä¸­çš„åè§ä¸è¾“å‡ºåè§ä¹‹é—´çš„ç›¸å…³æ€§ä¸é«˜ï¼ˆå¤§å¤šæ•°æƒ…å†µä¸‹ç›¸å…³æ€§ä½äº0.6ï¼Œpå€¼å°äº0.001ï¼‰ã€‚æ¢å¥è¯è¯´ï¼Œä¸äººç±»ä¸åŒï¼Œæµ‹è¯•çš„åè§å†³ç­–æ¨¡å‹å¹¶ä¸æ€»æ˜¯å…·æœ‰åè§æ€ç»´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨åè§æ¨¡å‹æ˜¯å¦å…·æœ‰åè§æ€ç»´ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ­ç¤ºæ¨¡å‹æ€ç»´è¿‡ç¨‹ä¸è¾“å‡ºä¹‹é—´çš„å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡é“¾å¼æ€ç»´æç¤ºï¼Œåˆ†ææ¨¡å‹åœ¨ç”Ÿæˆå“åº”å‰çš„æ€ç»´æ­¥éª¤ï¼Œè¯„ä¼°å…¶å¯¹å…¬å¹³æ€§çš„å½±å“ã€‚æ­¤æ–¹æ³•èƒ½å¤Ÿæ­ç¤ºæ¨¡å‹å†…éƒ¨çš„æ€ç»´è¿‡ç¨‹ï¼Œæä¾›æ›´æ·±å…¥çš„ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹é€‰æ‹©ã€é“¾å¼æ€ç»´æç¤ºå®æ–½ã€åè§é‡åŒ–å’Œç»“æœåˆ†æç­‰ä¸»è¦æ¨¡å—ã€‚é¦–å…ˆé€‰æ‹©äº”ä¸ªæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç„¶ååº”ç”¨é“¾å¼æ€ç»´æç¤ºï¼Œæœ€åä½¿ç”¨å…¬å¹³æ€§æŒ‡æ ‡é‡åŒ–åè§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡é“¾å¼æ€ç»´æç¤ºæ­ç¤ºæ¨¡å‹æ€ç»´è¿‡ç¨‹ä¸è¾“å‡ºä¹‹é—´çš„å…³ç³»ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿå¯¹æ¨¡å‹åè§çš„ç†è§£ï¼Œè¡¨æ˜æ¨¡å‹çš„æ€ç»´ä¸è¾“å‡ºä¸ä¸€å®šä¸€è‡´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†11ç§ä¸åŒçš„åè§é‡åŒ–æŒ‡æ ‡ï¼Œè®¾ç½®äº†ä¸¥æ ¼çš„ç»Ÿè®¡åˆ†ææ ‡å‡†ï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹æ€ç»´æ­¥éª¤ä¸­çš„åè§ä¸è¾“å‡ºåè§çš„ç›¸å…³æ€§ä½äº0.6ï¼Œä¸”å¤§å¤šæ•°æƒ…å†µä¸‹på€¼å°äº0.001ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œåè§å†³ç­–æ¨¡å‹çš„æ€ç»´è¿‡ç¨‹ä¸è¾“å‡ºç»“æœä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿè§‚ç‚¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯¹è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨å¼€å‘å…¬å¹³æ€§æ›´é«˜çš„è¯­è¨€æ¨¡å‹æ—¶ã€‚é€šè¿‡ç†è§£æ¨¡å‹çš„æ€ç»´è¿‡ç¨‹ï¼Œå¯ä»¥æ›´å¥½åœ°è®¾è®¡å’Œä¼˜åŒ–æ¨¡å‹ï¼Œå‡å°‘å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„åè§ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œç¤¾ä¼šè´£ä»»æ„Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The impressive performance of language models is undeniable. However, the presence of biases based on gender, race, socio-economic status, physical appearance, and sexual orientation makes the deployment of language models challenging. This paper studies the effect of chain-of-thought prompting, a recent approach that studies the steps followed by the model before it responds, on fairness. More specifically, we ask the following question: $\textit{Do biased models have biased thoughts}$? To answer our question, we conduct experiments on $5$ popular large language models using fairness metrics to quantify $11$ different biases in the model's thoughts and output. Our results show that the bias in the thinking steps is not highly correlated with the output bias (less than $0.6$ correlation with a $p$-value smaller than $0.001$ in most cases). In other words, unlike human beings, the tested models with biased decisions do not always possess biased thoughts.

