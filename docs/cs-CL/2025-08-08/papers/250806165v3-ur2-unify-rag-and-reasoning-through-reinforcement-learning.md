---
layout: default
title: UR$^2$: Unify RAG and Reasoning through Reinforcement Learning
---

# UR$^2$: Unify RAG and Reasoning through Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06165" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06165v3</a>
  <a href="https://arxiv.org/pdf/2508.06165.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06165v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06165v3', 'UR$^2$: Unify RAG and Reasoning through Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08 (æ›´æ–°: 2025-09-21)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Tsinghua-dhy/UR2)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUR$^2$ä»¥ç»Ÿä¸€æ£€ç´¢å¢å¼ºç”Ÿæˆä¸æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `çŸ¥è¯†è®¿é—®` `è¯¾ç¨‹è®­ç»ƒ` `å¼€æ”¾åŸŸé—®ç­”` `åŒ»å­¦æ¨ç†` `æ•°å­¦æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RAGå’ŒRLæ–¹æ³•é€šå¸¸æ˜¯å­¤ç«‹å‘å±•çš„ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ•´åˆï¼Œé™åˆ¶äº†å…¶åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨ã€‚
2. UR2é€šè¿‡éš¾åº¦æ„ŸçŸ¥çš„è¯¾ç¨‹è®­ç»ƒå’Œæ··åˆçŸ¥è¯†è®¿é—®ç­–ç•¥ï¼ŒåŠ¨æ€åè°ƒæ£€ç´¢ä¸æ¨ç†ï¼Œæé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUR$^2$åœ¨å¼€æ”¾åŸŸé—®ç­”ã€åŒ»å­¦å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½æ¥è¿‘æœ€æ–°çš„GPTæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¸¤ä¸ªäº’è¡¥èŒƒå¼ä¸­å±•ç°äº†æ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›é€šå¸¸æ˜¯å­¤ç«‹å‘å±•çš„ï¼Œç°æœ‰çš„ç»Ÿä¸€åŠªåŠ›èŒƒå›´æœ‰é™ï¼Œä¸»è¦é›†ä¸­åœ¨å¼€æ”¾åŸŸé—®ç­”å’Œç‰¹å®šä»»åŠ¡çº¦æŸä¸‹ã€‚è¿™ç§ç¼ºä¹æ•´åˆé™åˆ¶äº†RAG-RLæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚ç”¨æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºUR2ï¼ˆç»Ÿä¸€æ£€ç´¢ä¸æ¨ç†ï¼‰ï¼Œä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€æ£€ç´¢ä¸æ¨ç†çš„é€šç”¨æ¡†æ¶ã€‚UR2å¼•å…¥äº†ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šéš¾åº¦æ„ŸçŸ¥çš„è¯¾ç¨‹è®­ç»ƒï¼Œä»…åœ¨æŒ‘æˆ˜æ€§é—®é¢˜ä¸Šé€‰æ‹©æ€§è°ƒç”¨æ£€ç´¢ï¼Œä»¥åŠç»“åˆé¢†åŸŸç‰¹å®šç¦»çº¿è¯­æ–™åº“ä¸LLMç”Ÿæˆæ‘˜è¦çš„æ··åˆçŸ¥è¯†è®¿é—®ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUR$^2$åœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGå’ŒRLæ–¹æ³•ï¼Œæ€§èƒ½ä¸GPT-4o-miniå’ŒGPT-4.1-miniç›¸å½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RAGå’ŒRLæ–¹æ³•åœ¨æ•´åˆä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾åŸŸé—®ç­”å’Œç‰¹å®šä»»åŠ¡çº¦æŸä¸‹çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUR2çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡éš¾åº¦æ„ŸçŸ¥çš„è¯¾ç¨‹è®­ç»ƒå’Œæ··åˆçŸ¥è¯†è®¿é—®ç­–ç•¥ï¼ŒåŠ¨æ€åè°ƒæ£€ç´¢ä¸æ¨ç†ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUR2æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šéš¾åº¦æ„ŸçŸ¥è¯¾ç¨‹è®­ç»ƒæ¨¡å—å’Œæ··åˆçŸ¥è¯†è®¿é—®æ¨¡å—ã€‚å‰è€…æ ¹æ®é—®é¢˜çš„éš¾åº¦é€‰æ‹©æ€§è°ƒç”¨æ£€ç´¢ï¼Œåè€…ç»“åˆé¢†åŸŸç‰¹å®šçš„ç¦»çº¿è¯­æ–™å’ŒLLMç”Ÿæˆçš„æ‘˜è¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šUR2çš„å…³é”®åˆ›æ–°åœ¨äºå…¶éš¾åº¦æ„ŸçŸ¥çš„è¯¾ç¨‹è®­ç»ƒæ–¹æ³•å’Œæ··åˆçŸ¥è¯†è®¿é—®ç­–ç•¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å›ºå®šæ£€ç´¢è®¾ç½®å’Œä»»åŠ¡ç‰¹å®šçº¦æŸå½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒUR2é‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´çš„æ£€ç´¢ç­–ç•¥ï¼Œç»“åˆäº†å¤šç§æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶ä½¿ç”¨äº†Qwen-2.5-3/7Bå’ŒLLaMA-3.1-8Bä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUR$^2$åœ¨å¼€æ”¾åŸŸé—®ç­”ã€MMLU-Proã€åŒ»å­¦å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGå’ŒRLæ–¹æ³•ï¼Œæ€§èƒ½ä¸GPT-4o-miniå’ŒGPT-4.1-miniç›¸å½“ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„å¼ºå¤§é€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UR2çš„ç ”ç©¶æˆæœåœ¨å¼€æ”¾åŸŸé—®ç­”ã€åŒ»å­¦æ¨ç†å’Œæ•°å­¦æ¨ç†ç­‰å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ£€ç´¢ä¸æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€åŒ»ç–—å†³ç­–æ”¯æŒå’Œæ•™è‚²é¢†åŸŸæä¾›æ›´ä¸ºç²¾å‡†å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope -- typically limited to open-domain QA with fixed retrieval settings and task-specific constraints. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR$^2$ (built on Qwen-2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at https://github.com/Tsinghua-dhy/UR2.

