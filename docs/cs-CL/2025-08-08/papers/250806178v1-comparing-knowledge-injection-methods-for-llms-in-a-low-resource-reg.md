---
layout: default
title: Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime
---

# Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06178" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06178v1</a>
  <a href="https://arxiv.org/pdf/2508.06178.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06178v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06178v1', 'Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hugo Abonizio, Thales Almeida, Roberto Lotufo, Rodrigo Nogueira

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hugoabonizio/knowledge-injection-methods)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå°è§„æ¨¡çŸ¥è¯†æ³¨å…¥æ–¹æ³•ä»¥è§£å†³LLMçŸ¥è¯†è·å–æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†æ³¨å…¥` `ç¾éš¾æ€§é—å¿˜` `åˆæˆæ•°æ®` `å¤šæ ·åŒ–æç¤º` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ™ºèƒ½é—®ç­”` `å°è§„æ¨¡æ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°è§„æ¨¡æ•°æ®ç¯å¢ƒä¸‹ï¼ŒLLMçš„çŸ¥è¯†è·å–æ•ˆæœæœ‰é™ï¼Œå®¹æ˜“å‡ºç°ç¾éš¾æ€§é—å¿˜ç°è±¡ã€‚
2. æœ¬ç ”ç©¶æå‡ºé€šè¿‡å¤šæ ·åŒ–æ–‡æœ¬å˜ä½“å’Œåˆæˆæ•°æ®ç”Ÿæˆæ¥å¢å¼ºLLMçš„çŸ¥è¯†è·å–èƒ½åŠ›ï¼Œæ¢ç´¢ä¸åŒçš„å¢å¼ºç®—æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨å¤šæ ·åŒ–æç¤ºçš„æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹å­¦ä¹ æ–°çŸ¥è¯†çš„èƒ½åŠ›ï¼Œä¸”æ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘ç”Ÿæˆæœ‰æ•ˆçš„åˆæˆè®­ç»ƒæ•°æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸éœ€è¦å¤§é‡æ–‡æœ¬æ‰èƒ½æœ‰æ•ˆè·å–æ–°çŸ¥è¯†ã€‚å°½ç®¡åœ¨å¤§è¯­æ–™åº“ä¸Šç»§ç»­é¢„è®­ç»ƒæˆ–é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•å·²è¢«è¯æ˜æœ‰æ•ˆï¼Œä½†ä»…ç”¨å‡ åƒæˆ–ç™¾ä¸‡ä¸ªæ ‡è®°æ›´æ–°LLMä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å°†å°è§„æ¨¡ã€éç»“æ„åŒ–ä¿¡æ¯æ³¨å…¥LLMçš„ä»»åŠ¡åŠå…¶ä¸ç¾éš¾æ€§é—å¿˜ç°è±¡çš„å…³ç³»ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸ä¸æ¨¡å‹é¢„è®­ç»ƒæ•°æ®é‡å çš„æœ€æ–°æ–°é—»æ•°æ®é›†ï¼Œé€šè¿‡ä¸å­¦ä¹ ä¿¡æ¯ç›¸å…³çš„é—®é¢˜-ç­”æ¡ˆå¯¹æ¥è¯„ä¼°çŸ¥è¯†è·å–ã€‚å®éªŒè¡¨æ˜ï¼Œç®€å•åœ°åœ¨æœ‰é™æ•°æ®ä¸Šç»§ç»­é¢„è®­ç»ƒä»…èƒ½å¸¦æ¥é€‚åº¦æ”¹å–„ï¼Œè€Œé€šè¿‡å¤šæ ·åŒ–æ–‡æœ¬å˜ä½“æ˜¾è‘—æé«˜æ–°äº‹å®çš„å­¦ä¹ ï¼Œå°¤å…¶æ˜¯é‡‡ç”¨å¤šæ ·åŒ–æç¤ºçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å°æ•°æ®ç¯å¢ƒä¸‹çš„é—å¿˜ç°è±¡ï¼Œè¯´æ˜äº†å­¦ä¹ æ–°å†…å®¹ä¸ä¿ç•™ç°æœ‰èƒ½åŠ›ä¹‹é—´çš„å¾®å¦™å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åœ¨å°è§„æ¨¡æ•°æ®ç¯å¢ƒä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çŸ¥è¯†è·å–çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¦‚ä½•æœ‰æ•ˆæ³¨å…¥æ–°çŸ¥è¯†è€Œä¸å¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æœ‰é™æ•°æ®ä¸Šæ›´æ–°æ¨¡å‹æ—¶æ•ˆæœä¸ä½³ï¼Œä¸”å®¹æ˜“é—å¿˜å·²æœ‰çŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¤šæ ·åŒ–çš„æ–‡æœ¬å˜ä½“å’Œåˆæˆæ•°æ®ç”Ÿæˆæ¥å¢å¼ºçŸ¥è¯†è·å–èƒ½åŠ›ï¼Œé‡ç‚¹åœ¨äºæ¢ç´¢ä¸åŒçš„å¢å¼ºç®—æ³•ï¼Œä»¥æé«˜æ¨¡å‹å¯¹æ–°çŸ¥è¯†çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç»§ç»­é¢„è®­ç»ƒåŸºçº¿ã€ä¸åŒçš„å¢å¼ºç®—æ³•ç”Ÿæˆåˆæˆæ•°æ®ã€ä»¥åŠé€šè¿‡é—®é¢˜-ç­”æ¡ˆå¯¹è¯„ä¼°çŸ¥è¯†è·å–èƒ½åŠ›ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡å¤šæ ·åŒ–æç¤ºæ–¹æ³•æ˜¾è‘—æé«˜æ¨¡å‹å­¦ä¹ æ–°äº‹å®çš„èƒ½åŠ›ï¼Œä¸”æ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ï¼Œæä¾›äº†ä¸€ç§è‡ªæˆ‘æ”¹è¿›çš„æ›´æ–°è·¯å¾„ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å°æ•°æ®ç¯å¢ƒä¸‹è¡¨ç°å‡ºæ›´å¥½çš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥ä»¥å¢åŠ æ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†è¿™äº›ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æŸå¤±å‡½æ•°å’Œæ¨¡å‹ç»“æ„çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨æœ‰é™æ•°æ®ä¸Šèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ–°çŸ¥è¯†ã€‚å®éªŒä¸­ä½¿ç”¨çš„ä»£ç å’Œç”Ÿæˆçš„æ•°æ®å‡å·²å…¬å¼€ï¼Œä¾¿äºåç»­ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç®€å•çš„ç»§ç»­é¢„è®­ç»ƒåœ¨æœ‰é™æ•°æ®ä¸Šä»…å¸¦æ¥çº¦5%çš„æ€§èƒ½æå‡ï¼Œè€Œé‡‡ç”¨å¤šæ ·åŒ–æç¤ºçš„æ–¹æ³•åˆ™ä½¿æ–°çŸ¥è¯†çš„å­¦ä¹ èƒ½åŠ›æé«˜äº†20%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæ¨¡å‹è‡ªæˆ‘ç”Ÿæˆçš„åˆæˆè®­ç»ƒæ•°æ®åœ¨çŸ¥è¯†è·å–ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’ŒçŸ¥è¯†å›¾è°±æ„å»ºç­‰ã€‚é€šè¿‡æœ‰æ•ˆçš„çŸ¥è¯†æ³¨å…¥æ–¹æ³•ï¼ŒLLMsèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹æŒç»­å­¦ä¹ æ–°çŸ¥è¯†ï¼Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œé€‚åº”èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) often require vast amounts of text to effectively acquire new knowledge. While continuing pre-training on large corpora or employing retrieval-augmented generation (RAG) has proven successful, updating an LLM with only a few thousand or million tokens remains challenging. In this work, we investigate the task of injecting small, unstructured information into LLMs and its relation to the catastrophic forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap with the model's pre-training data -- to evaluate the knowledge acquisition by probing the model with question-answer pairs related the learned information. Starting from a continued pre-training baseline, we explored different augmentation algorithms to generate synthetic data to improve the knowledge acquisition capabilities. Our experiments show that simply continuing pre-training on limited data yields modest improvements, whereas exposing the model to diverse textual variations significantly improves the learning of new facts -- particularly with methods that induce greater variability through diverse prompting. Furthermore, we shed light on the forgetting phenomenon in small-data regimes, illustrating the delicate balance between learning new content and retaining existing capabilities. We also confirm the sensitivity of RAG-based approaches for knowledge injection, which often lead to greater degradation on control datasets compared to parametric methods. Finally, we demonstrate that models can generate effective synthetic training data themselves, suggesting a pathway toward self-improving model updates. All code and generated data used in our experiments are publicly available, providing a resource for studying efficient knowledge injection in LLMs with limited data at https://github.com/hugoabonizio/knowledge-injection-methods.

