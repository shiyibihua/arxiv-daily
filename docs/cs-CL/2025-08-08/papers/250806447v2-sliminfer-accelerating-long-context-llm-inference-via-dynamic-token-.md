---
layout: default
title: SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning
---

# SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06447" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06447v2</a>
  <a href="https://arxiv.org/pdf/2508.06447.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06447v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06447v2', 'SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08 (æ›´æ–°: 2025-11-24)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Longxmas/SlimInfer)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSlimInferä»¥åŠ é€Ÿé•¿ä¸Šä¸‹æ–‡LLMæ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡æ¨ç†` `åŠ¨æ€ä¿®å‰ª` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯æ‰©æ•£` `æ¨ç†åŠ é€Ÿ` `å†…å­˜ä¼˜åŒ–` `KVç¼“å­˜ç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­æ•ˆç‡ä½ä¸‹ï¼Œä»éœ€å¤„ç†å®Œæ•´çš„éšè—çŠ¶æ€ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ã€‚
2. SlimInferé€šè¿‡åŠ¨æ€ä¿®å‰ªä¸é‡è¦çš„æç¤ºtokenï¼Œåˆ©ç”¨ä¿¡æ¯æ‰©æ•£ç°è±¡æ¥æé«˜æ¨ç†é€Ÿåº¦å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSlimInferåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦å’Œå»¶è¿Ÿï¼Œå…·æœ‰è‰¯å¥½çš„å®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿ä¸Šä¸‹æ–‡æ¨ç†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¡ç®—éœ€æ±‚ä¸Šå­˜åœ¨å¾ˆå¤§é™åˆ¶ã€‚å°½ç®¡å·²æœ‰å¤šç§æ–¹æ³•ä¼˜åŒ–äº†æ³¨æ„åŠ›è®¡ç®—ï¼Œä½†å®ƒä»¬ä»éœ€åœ¨æ¯ä¸€å±‚å¤„ç†å®Œæ•´çš„éšè—çŠ¶æ€ï¼Œé™åˆ¶äº†æ•´ä½“æ•ˆç‡ã€‚æœ¬æ–‡æå‡ºäº†SlimInferï¼Œä¸€ä¸ªåˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­åŠ¨æ€ä¿®å‰ªä¸é‡è¦çš„æç¤ºtokenæ¥åŠ é€Ÿæ¨ç†ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ä¿¡æ¯æ‰©æ•£ç°è±¡ï¼šéšç€å…³é”®ä¿¡æ¯åœ¨å±‚é—´ä¼ æ’­ï¼Œå®ƒä¼šåˆ†å¸ƒåœ¨æ•´ä¸ªåºåˆ—ä¸­ã€‚è¿™ä¸€æ‰©æ•£è¿‡ç¨‹è¡¨æ˜ï¼ŒLLMsåœ¨éšè—çŠ¶æ€ä¸­ä¿®å‰ªè¿‡å¤šçš„tokenï¼ˆç”šè‡³åŒ…æ‹¬è¿™äº›å…³é”®tokenï¼‰æ—¶ï¼Œä»èƒ½ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚SlimInferå¼•å…¥äº†ä¸€ç§åŠ¨æ€ç»†ç²’åº¦ä¿®å‰ªæœºåˆ¶ï¼Œå‡†ç¡®å»é™¤ä¸­é—´å±‚éšè—çŠ¶æ€ä¸­çš„å†—ä½™tokenã€‚å®éªŒè¡¨æ˜ï¼ŒSlimInferåœ¨å•ä¸ªRTX 4090ä¸Šå¯¹LLaMA3.1-8B-Instructå®ç°äº†æœ€é«˜2.53å€çš„é¦–æ¬¡tokenå“åº”æ—¶é—´åŠ é€Ÿå’Œ1.88å€çš„ç«¯åˆ°ç«¯å»¶è¿Ÿå‡å°‘ï¼ŒåŒæ—¶åœ¨LongBenchä¸Šä¿æŒäº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé•¿ä¸Šä¸‹æ–‡æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é¢ä¸´é«˜è®¡ç®—éœ€æ±‚çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä»éœ€å¤„ç†å®Œæ•´çš„éšè—çŠ¶æ€ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œèµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSlimInferçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¿¡æ¯æ‰©æ•£ç°è±¡ï¼Œåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­åŠ¨æ€ä¿®å‰ªä¸é‡è¦çš„æç¤ºtokenï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒè¯­ä¹‰å®Œæ•´æ€§çš„åŒæ—¶ï¼Œå‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSlimInferçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åŠ¨æ€ç»†ç²’åº¦ä¿®å‰ªæœºåˆ¶å’Œå¼‚æ­¥KVç¼“å­˜ç®¡ç†å™¨ã€‚åŠ¨æ€ä¿®å‰ªæœºåˆ¶åœ¨ä¸­é—´å±‚å‡†ç¡®å»é™¤å†—ä½™tokenï¼Œè€ŒKVç¼“å­˜ç®¡ç†å™¨åˆ™é¢„å–æ‰€éœ€çš„tokenå—ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨å’ŒI/Oæˆæœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šSlimInferçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€ä¿®å‰ªæœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®æ—¶è¯†åˆ«å¹¶å»é™¤ä¸é‡è¦çš„tokenã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„é™æ€ä¿®å‰ªæˆ–å…¨é‡å¤„ç†æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSlimInferé‡‡ç”¨äº†ç»†ç²’åº¦çš„ä¿®å‰ªç­–ç•¥ï¼Œç»“åˆä¿¡æ¯æ‰©æ•£ç†è®ºï¼Œç¡®ä¿åœ¨ä¿®å‰ªè¿‡ç¨‹ä¸­ä¸å½±å“æ¨¡å‹çš„è¯­ä¹‰ç†è§£ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†é«˜æ•ˆçš„KVç¼“å­˜ç®¡ç†æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–å†…å­˜å’ŒI/Oæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SlimInferåœ¨å®éªŒä¸­å®ç°äº†æœ€é«˜2.53å€çš„é¦–æ¬¡tokenå“åº”æ—¶é—´åŠ é€Ÿå’Œ1.88å€çš„ç«¯åˆ°ç«¯å»¶è¿Ÿå‡å°‘ï¼Œè¡¨ç°å‡ºè‰²ã€‚ä¸LLaMA3.1-8B-Instructçš„å¯¹æ¯”å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ï¼Œå…·æœ‰è‰¯å¥½çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SlimInferçš„ç ”ç©¶æˆæœåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ•ˆç‡ï¼ŒSlimInferèƒ½å¤Ÿæ”¯æŒæ›´å¤æ‚çš„ä»»åŠ¡å’Œæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚æœªæ¥ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„ä¸æ–­æ‰©å¤§ï¼Œè¯¥æ–¹æ³•çš„å®é™…ä»·å€¼å°†æ„ˆåŠ æ˜¾è‘—ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token (TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code is available at https://github.com/Longxmas/SlimInfer.

