---
layout: default
title: HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning
---

# HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06475" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.06475v1</a>
  <a href="https://arxiv.org/pdf/2508.06475.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06475v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06475v1', 'HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Guimin Hu, Daniel Hershcovich, Hasti Seifi

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-08

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫HapticLLaMA‰ª•Ëß£ÂÜ≥Ëß¶Ëßâ‰ø°Âè∑ÊèèËø∞ÁîüÊàêÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ëß¶ËßâÊèèËø∞ÁîüÊàê` `Â§öÊ®°ÊÄÅÊÑüÁü•` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `ËôöÊãüÁé∞ÂÆû` `‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ËßÜËßâÂíåÈü≥È¢ëÔºåËß¶Ëßâ‰ø°Âè∑ÁöÑÁ†îÁ©∂Áõ∏ÂØπËæÉÂ∞ëÔºåÂØºËá¥Ëß¶ËßâÊèèËø∞ÁîüÊàê‰ªªÂä°ÁöÑÊé¢Á¥¢‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫HapticLLaMAÊ®°ÂûãÔºåÈÄöËøáÂ∞ÜËß¶Ëßâ‰ø°Âè∑ËΩ¨Êç¢‰∏∫Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÔºåÂ°´Ë°•‰∫ÜËß¶Ëßâ‰ø°Âè∑Â§ÑÁêÜÁöÑÁ†îÁ©∂Á©∫ÁôΩ„ÄÇ
3. HapticLLaMAÂú®Ëá™Âä®Âåñn-gramÊåáÊ†áÂíå‰∫∫Â∑•ËØÑ‰º∞‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåMETEORÂæóÂàÜ‰∏∫59.98ÔºåBLEU-4ÂæóÂàÜ‰∏∫32.06ÔºåÊòæÁ§∫Âá∫ËâØÂ•ΩÁöÑÁîüÊàêËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ëß¶ËßâÊèèËø∞ÁîüÊàêÊòØ‰ªéËß¶Ëßâ‰ø°Âè∑ÔºàÂ¶ÇÊåØÂä®ÔºâÁîüÊàêËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÁöÑ‰ªªÂä°ÔºåÂ∫îÁî®‰∫éËôöÊãüÁé∞ÂÆû„ÄÅÊó†ÈöúÁ¢çÂíåÂ∫∑Â§çÁ≠âÈ¢ÜÂüü„ÄÇÂ∞ΩÁÆ°‰ª•ÂæÄÁöÑÂ§öÊ®°ÊÄÅÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ËßÜËßâÂíåÈü≥È¢ë‰∏äÔºå‰ΩÜËß¶Ëßâ‰ø°Âè∑‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫Â°´Ë°•Ëøô‰∏ÄÁ©∫ÁôΩÔºåÊú¨ÊñáÊ≠£ÂºèÂÆö‰πâ‰∫ÜËß¶ËßâÊèèËø∞ÁîüÊàê‰ªªÂä°ÔºåÂπ∂ÊèêÂá∫‰∫ÜHapticLLaMAÔºå‰∏Ä‰∏™Â§öÊ®°ÊÄÅÊÑüÁü•ËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂ∞ÜÊåØÂä®‰ø°Âè∑Ëß£Èáä‰∏∫ÁâπÂÆöÊÑüÁü•„ÄÅÊÉÖÊÑüÊàñËÅîÊÉ≥Á±ªÂà´ÁöÑÊèèËø∞„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫Ü‰∏§ÁßçËß¶ËßâÊ†áËÆ∞Âô®ÔºåÂàÜÂà´ÊòØÂü∫‰∫éÈ¢ëÁéáÁöÑÊ†áËÆ∞Âô®ÂíåÂü∫‰∫éEnCodecÁöÑÊ†áËÆ∞Âô®ÔºåÂ∞ÜËß¶Ëßâ‰ø°Âè∑ËΩ¨Êç¢‰∏∫Á¶ªÊï£Âçï‰ΩçÂ∫èÂàóÔºå‰ªéËÄå‰∏éLLaMAÊ®°ÂûãÈõÜÊàê„ÄÇHapticLLaMAÁªèËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÔºåË°®Áé∞Âá∫Âº∫Â§ßÁöÑËß¶ËßâÊåØÂä®‰ø°Âè∑Ëß£ÈáäËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ëß¶Ëßâ‰ø°Âè∑ÁîüÊàêËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÁöÑ‰ªªÂä°ÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Ëß¶Ëßâ‰ø°Âè∑Â§ÑÁêÜ‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÁº∫‰πèÊúâÊïàÁöÑÊ®°ÂûãÊù•Ëß£ÈáäÂíåÁîüÊàêËß¶ËßâÊèèËø∞„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöHapticLLaMAÈÄöËøáÂ∞ÜËß¶Ëßâ‰ø°Âè∑ËΩ¨Âåñ‰∏∫Á¶ªÊï£Âçï‰ΩçÂ∫èÂàóÔºåÂπ∂Âà©Áî®LLaMAÊû∂ÊûÑËøõË°åËÆ≠ÁªÉÔºåÊó®Âú®ÊèêÈ´òËß¶Ëßâ‰ø°Âè∑ÁöÑÁêÜËß£ÂíåÊèèËø∞ËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHapticLLaMAÁöÑËÆ≠ÁªÉÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµÊòØ‰ΩøÁî®LoRAÈÄÇÂ∫îÁöÑLLaMAÊû∂ÊûÑËøõË°åÁõëÁù£ÂæÆË∞ÉÔºåÁ¨¨‰∫åÈò∂ÊÆµÊòØÈÄöËøá‰∫∫Á±ªÂèçÈ¶àÁöÑÂº∫ÂåñÂ≠¶‰π†ËøõË°åÂæÆË∞É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨Á†îÁ©∂ÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂºïÂÖ•‰∫Ü‰∏§ÁßçËß¶ËßâÊ†áËÆ∞Âô®ÔºåÂàÜÂà´ÊòØÂü∫‰∫éÈ¢ëÁéáÁöÑÊ†áËÆ∞Âô®ÂíåEnCodecÊ†áËÆ∞Âô®ÔºåËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜËß¶Ëßâ‰ø°Âè∑ËΩ¨Êç¢‰∏∫ÂèØÂ§ÑÁêÜÁöÑÂ∫èÂàóÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜLoRAÈÄÇÂ∫îÊäÄÊúØ‰ª•ÂáèÂ∞ëËÆ≠ÁªÉÂèÇÊï∞ÔºåÂπ∂ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÁîüÊàêÁöÑÊèèËø∞Ôºå‰ΩøÂÖ∂Êõ¥Á¨¶Âêà‰∫∫Á±ªÁöÑËß¶ËßâÊÑüÁü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

HapticLLaMAÂú®Ëß¶ËßâÊèèËø∞ÁîüÊàê‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåMETEORÂæóÂàÜËææÂà∞59.98ÔºåBLEU-4ÂæóÂàÜ‰∏∫32.06ÔºåË∂ÖËøá61%ÁöÑÁîüÊàêÊèèËø∞Âú®7ÂàÜÂà∂‰∏≠Ëé∑Âæó‰∫Ü3.5‰ª•‰∏äÁöÑËØÑÂàÜ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÔºåÊï¥‰ΩìËØÑÂàÜÂàÜÂ∏ÉÊèêÂçá‰∫Ü10%ÔºåÊòæÁ§∫Âá∫‰∏é‰∫∫Á±ªËß¶ËßâÊÑüÁü•ÁöÑÊõ¥Âº∫‰∏ÄËá¥ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HapticLLaMAÁöÑÁ†îÁ©∂ÊàêÊûúÂú®ËôöÊãüÁé∞ÂÆû„ÄÅÊó†ÈöúÁ¢çÊäÄÊúØÂíåÂ∫∑Â§çÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÂ∞ÜËß¶Ëßâ‰ø°Âè∑ËΩ¨Âåñ‰∏∫Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÔºåËØ•Ê®°ÂûãÂèØ‰ª•Â∏ÆÂä©Áî®Êà∑Êõ¥Â•ΩÂú∞ÁêÜËß£Âíå‰ΩìÈ™åËß¶Ëßâ‰ø°ÊÅØÔºåÊèêÂçá‰∫§‰∫í‰ΩìÈ™åÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜËßâÊàñÂê¨ËßâÂèóÈôêÁöÑÁéØÂ¢É‰∏≠„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÊé®Âä®Ëß¶ËßâÂèçÈ¶àÁ≥ªÁªüÁöÑÂèëÂ±ïÔºåÂ¢ûÂº∫‰∫∫Êú∫‰∫§‰∫íÁöÑËá™ÁÑ∂ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Haptic captioning is the task of generating natural language descriptions from haptic signals, such as vibrations, for use in virtual reality, accessibility, and rehabilitation applications. While previous multimodal research has focused primarily on vision and audio, haptic signals for the sense of touch remain underexplored. To address this gap, we formalize the haptic captioning task and propose HapticLLaMA, a multimodal sensory language model that interprets vibration signals into descriptions in a given sensory, emotional, or associative category. We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units, enabling their integration with the LLaMA model. HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation, and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We assess HapticLLaMA's captioning performance using both automated n-gram metrics and human evaluation. HapticLLaMA demonstrates strong capability in interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated captions received human ratings above 3.5 on a 7-point scale, with RLHF yielding a 10% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception. These findings highlight the potential of large language models to process and adapt to sensory data.

