---
layout: default
title: Memp: Exploring Agent Procedural Memory
---

# Memp: Exploring Agent Procedural Memory

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06433" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06433v2</a>
  <a href="https://arxiv.org/pdf/2508.06433.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06433v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06433v2', 'Memp: Exploring Agent Procedural Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08 (æ›´æ–°: 2025-08-13)

**å¤‡æ³¨**: Work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMempä»¥è§£å†³ä»£ç†ç¨‹åºè®°å¿†è„†å¼±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨‹åºè®°å¿†` `åŠ¨æ€æ›´æ–°` `æ™ºèƒ½ä»£ç†` `ä»»åŠ¡æ‰§è¡Œ` `è¿ç§»å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¨‹åºè®°å¿†æ–¹æ³•å¾€å¾€ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡æˆ–é™æ€å‚æ•°ï¼Œå¯¼è‡´å…¶è„†å¼±æ€§å’Œçµæ´»æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºMempï¼Œé€šè¿‡æç‚¼ä»£ç†çš„å†å²è½¨è¿¹ï¼Œæ„å»ºå¯å­¦ä¹ å’Œå¯æ›´æ–°çš„ç¨‹åºè®°å¿†ï¼Œä»¥æå‡ä»£ç†çš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚
3. åœ¨TravelPlannerå’ŒALFWorldçš„å®éªŒä¸­ï¼Œéšç€è®°å¿†åº“çš„ä¼˜åŒ–ï¼Œä»£ç†çš„æˆåŠŸç‡å’Œæ•ˆç‡æ˜¾è‘—æé«˜ï¼Œä¸”ä»å¼ºæ¨¡å‹è¿ç§»çš„è®°å¿†åœ¨å¼±æ¨¡å‹ä¸­ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŸºç¡€ä¸Šçš„ä»£ç†åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ç¨‹åºè®°å¿†å¾€å¾€è„†å¼±ï¼Œä¾èµ–äºæ‰‹åŠ¨è®¾è®¡æˆ–é™æ€å‚æ•°ã€‚æœ¬æ–‡æ¢è®¨äº†èµ‹äºˆä»£ç†å¯å­¦ä¹ ã€å¯æ›´æ–°å’Œç»ˆèº«ç¨‹åºè®°å¿†çš„ç­–ç•¥ã€‚æˆ‘ä»¬æå‡ºMempï¼Œé€šè¿‡å°†è¿‡å»çš„ä»£ç†è½¨è¿¹æç‚¼ä¸ºç»†ç²’åº¦çš„é€æ­¥æŒ‡ä»¤å’Œæ›´é«˜å±‚æ¬¡çš„è„šæœ¬å¼æŠ½è±¡ï¼Œæ¢ç´¢ç¨‹åºè®°å¿†çš„æ„å»ºã€æ£€ç´¢å’Œæ›´æ–°çš„ä¸åŒç­–ç•¥ã€‚ç»“åˆåŠ¨æ€æœºåˆ¶ï¼ŒæŒç»­æ›´æ–°ã€çº æ­£å’Œæ·˜æ±°å†…å®¹ï¼Œä½¿å¾—è®°å¿†åº“éšç€æ–°ç»éªŒä¸æ–­æ¼”å˜ã€‚å®è¯è¯„ä¼°æ˜¾ç¤ºï¼Œéšç€è®°å¿†åº“çš„ä¼˜åŒ–ï¼Œä»£ç†åœ¨ç±»ä¼¼ä»»åŠ¡ä¸ŠæˆåŠŸç‡å’Œæ•ˆç‡ç¨³æ­¥æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨ç¨‹åºè®°å¿†æ–¹é¢çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé™æ€å‚æ•°å’Œæ‰‹åŠ¨è®¾è®¡ï¼Œå¯¼è‡´çµæ´»æ€§ä¸è¶³å’Œé€‚åº”æ€§å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMempçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æç‚¼ä»£ç†çš„å†å²è½¨è¿¹ï¼Œæ„å»ºä¸€ä¸ªå¯å­¦ä¹ ã€å¯æ›´æ–°çš„ç¨‹åºè®°å¿†åº“ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿåœ¨ä¸æ–­å˜åŒ–çš„ç¯å¢ƒä¸­ä¿æŒé«˜æ•ˆçš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMempçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè®°å¿†æ„å»ºã€è®°å¿†æ£€ç´¢å’Œè®°å¿†æ›´æ–°ã€‚è®°å¿†æ„å»ºæ¨¡å—å°†å†å²è½¨è¿¹è½¬åŒ–ä¸ºç»†ç²’åº¦æŒ‡ä»¤å’Œé«˜å±‚æ¬¡æŠ½è±¡ï¼›è®°å¿†æ£€ç´¢æ¨¡å—è´Ÿè´£æ ¹æ®å½“å‰ä»»åŠ¡ä»è®°å¿†åº“ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼›è®°å¿†æ›´æ–°æ¨¡å—åˆ™æŒç»­ä¼˜åŒ–è®°å¿†å†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMempçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€æ›´æ–°æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ–°ç»éªŒä¸æ–­è°ƒæ•´å’Œä¼˜åŒ–è®°å¿†åº“ï¼Œä¸ä¼ ç»Ÿé™æ€è®°å¿†æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMempé‡‡ç”¨äº†å¤šå±‚æ¬¡çš„è®°å¿†è¡¨ç¤ºï¼Œç»“åˆäº†ç»†ç²’åº¦æŒ‡ä»¤å’Œé«˜å±‚æ¬¡æŠ½è±¡ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†è®°å¿†æ›´æ–°çš„æƒ©ç½šé¡¹ï¼Œä»¥ç¡®ä¿è®°å¿†åº“çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨TravelPlannerå’ŒALFWorldçš„å®éªŒä¸­ï¼Œéšç€è®°å¿†åº“çš„ä¼˜åŒ–ï¼Œä»£ç†çš„æˆåŠŸç‡æé«˜äº†çº¦20%ï¼Œæ•ˆç‡æå‡äº†15%ã€‚æ­¤å¤–ï¼Œå°†ä»å¼ºæ¨¡å‹è¿ç§»çš„ç¨‹åºè®°å¿†åº”ç”¨äºå¼±æ¨¡å‹ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°30%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–è§„åˆ’å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡ä»£ç†çš„ç¨‹åºè®°å¿†èƒ½åŠ›ï¼ŒMempèƒ½å¤Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­æä¾›æ›´é«˜æ•ˆçš„æ”¯æŒï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½ç³»ç»Ÿçš„è‡ªä¸»å­¦ä¹ å’Œé€‚åº”æ€§æ–¹é¢äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.

