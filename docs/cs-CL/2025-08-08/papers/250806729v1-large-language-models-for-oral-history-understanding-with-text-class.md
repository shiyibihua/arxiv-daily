---
layout: default
title: Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis
---

# Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06729" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.06729v1</a>
  <a href="https://arxiv.org/pdf/2508.06729.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06729v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06729v1', 'Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Komala Subramanyam Cherukuri, Pranav Abishai Moses, Aisa Sakata, Jiangping Chen, Haihua Chen

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-08

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/kc6699c/LLM4OralHistoryAnalysis)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂèØÊâ©Â±ïÊ°ÜÊû∂‰ª•Ëá™Âä®ÂåñÊó•Ë£îÁæéÂõΩ‰∫∫ÁõëÁ¶ÅÂè£Ëø∞ÂéÜÂè≤ÁöÑÊÉÖÊÑü‰∏éËØ≠‰πâÊ†áÊ≥®**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âè£Ëø∞ÂéÜÂè≤` `ÊÉÖÊÑüÂàÜÊûê` `ËØ≠‰πâÊ†áÊ≥®` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÊñáÂåñÊïèÊÑüÊÄß` `Êï∞ÊçÆÈõÜÊûÑÂª∫` `ÊèêÁ§∫Â∑•Á®ã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂè£Ëø∞ÂéÜÂè≤Ê°£Ê°àÊó∂Èù¢‰∏¥ÈùûÁªìÊûÑÂåñÊ†ºÂºèÂíåÈ´òÊ†áÊ≥®ÊàêÊú¨ÁöÑÊåëÊàòÔºåÈôêÂà∂‰∫ÜÂ§ßËßÑÊ®°ÂàÜÊûêÁöÑÂèØËÉΩÊÄß„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÈò∂ÊÆµÁöÑÊñπÊ≥ïÔºåÁªìÂêà‰∏ìÂÆ∂Ê†áÊ≥®„ÄÅÊèêÁ§∫ËÆæËÆ°ÂíåLLMËØÑ‰º∞Ôºå‰ª•ÂÆûÁé∞Âè£Ëø∞ÂéÜÂè≤ÁöÑËá™Âä®ÂåñÊÉÖÊÑü‰∏éËØ≠‰πâÊ†áÊ≥®„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåChatGPTÂú®ËØ≠‰πâÂàÜÁ±ª‰ªªÂä°‰∏≠ÂèñÂæó‰∫Ü88.71%ÁöÑF1ÂàÜÊï∞ÔºåË°®ÊòéLLMsÂú®Â§ßËßÑÊ®°Âè£Ëø∞ÂéÜÂè≤ÂàÜÊûê‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âè£Ëø∞ÂéÜÂè≤ÊòØËÆ∞ÂΩïÁîüÊ¥ªÁªèÂéÜÁöÑÈáçË¶ÅËµÑÊñôÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁªèÂéÜÁ≥ªÁªüÊÄß‰∏çÂÖ¨‰∏éÂéÜÂè≤ÊäπÈô§ÁöÑÁ§æÂå∫‰∏≠„ÄÇÊúâÊïàÂàÜÊûêËøô‰∫õÂè£Ëø∞ÂéÜÂè≤Ê°£Ê°àÊúâÂä©‰∫é‰øÉËøõÂØπÂÖ∂ÁöÑÁêÜËß£„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÂÖ∂ÈùûÁªìÊûÑÂåñÊ†ºÂºè„ÄÅÊÉÖÊÑüÂ§çÊùÇÊÄßÂíåÈ´òÊòÇÁöÑÊ†áÊ≥®ÊàêÊú¨ÔºåÂ§ßËßÑÊ®°ÂàÜÊûê‰ªçÁÑ∂ÊúâÈôê„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÊ°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËá™Âä®ÂåñÊó•Ë£îÁæéÂõΩ‰∫∫ÁõëÁ¶ÅÂè£Ëø∞ÂéÜÂè≤ÁöÑËØ≠‰πâÂíåÊÉÖÊÑüÊ†áÊ≥®„ÄÇÈÄöËøáÊûÑÂª∫È´òË¥®ÈáèÊï∞ÊçÆÈõÜ„ÄÅËØÑ‰º∞Â§öÁßçÊ®°ÂûãÂèäÊµãËØïÊèêÁ§∫Â∑•Á®ãÁ≠ñÁï•ÔºåÁ†îÁ©∂Ë°®ÊòéLLMsÂú®ÊñáÂåñÊïèÊÑüÁöÑÊ°£Ê°àÂàÜÊûê‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Êèê‰æõ‰∫ÜÂèØÈáçÁî®ÁöÑÊ†áÊ≥®ÁÆ°ÈÅìÂíåÂÆûÈôÖÊåáÂØº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êó•Ë£îÁæéÂõΩ‰∫∫ÁõëÁ¶ÅÂè£Ëø∞ÂéÜÂè≤Ê°£Ê°àÁöÑÊÉÖÊÑü‰∏éËØ≠‰πâÊ†áÊ≥®ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÈùûÁªìÊûÑÂåñÊï∞ÊçÆÊó∂Èù¢‰∏¥È´òÊòÇÁöÑÊ†áÊ≥®ÊàêÊú¨ÂíåÊÉÖÊÑüÂ§çÊùÇÊÄßÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂàÜÊûêËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÊûÑÂª∫È´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜÂπ∂Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÔºåÁªìÂêà‰∏ìÂÆ∂Ê†áÊ≥®ÂíåÊèêÁ§∫ËÆæËÆ°ÔºåËá™Âä®ÂåñÂè£Ëø∞ÂéÜÂè≤ÁöÑÊÉÖÊÑü‰∏éËØ≠‰πâÊ†áÊ≥®Ôºå‰ª•ÊèêÈ´òÂàÜÊûêÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨Êï∞ÊçÆÈõÜÊûÑÂª∫„ÄÅÊ®°ÂûãËØÑ‰º∞ÂíåÊèêÁ§∫Â∑•Á®ã‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÊ†áÊ≥®558‰∏™Âè•Â≠êÁî®‰∫éÊÉÖÊÑüÂíåËØ≠‰πâÂàÜÁ±ªÔºõÂÖ∂Ê¨°ÔºåËØÑ‰º∞‰∏çÂêåÊ®°ÂûãÁöÑÊÄßËÉΩÔºõÊúÄÂêéÔºå‰ºòÂåñÊèêÁ§∫ÈÖçÁΩÆ‰ª•Ê†áÊ≥®Êõ¥Â§ßËßÑÊ®°ÁöÑÂè•Â≠ê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨Á†îÁ©∂ÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜLLMsÂ∫îÁî®‰∫éÊñáÂåñÊïèÊÑüÁöÑÊ°£Ê°àÂàÜÊûêÔºåÂπ∂ÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑÊèêÁ§∫ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊ†áÊ≥®ÊïàÊûú„ÄÇËøô‰∏ÄÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÊÉÖÊÑüÂíåËØ≠‰πâÊó∂Â±ïÁé∞‰∫ÜÊòæËëóÁöÑ‰ºòÂäø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠Ôºå‰ΩøÁî®‰∫ÜÈõ∂-shot„ÄÅfew-shotÂíåRAGÁ≠ñÁï•ËøõË°åÊ®°ÂûãËØÑ‰º∞ÔºåChatGPT„ÄÅLlamaÂíåQwenÁöÑÊÄßËÉΩËøõË°å‰∫ÜÊØîËæÉÔºåÊúÄÁªàÁ°ÆÂÆö‰∫ÜÊúÄ‰Ω≥ÁöÑÊèêÁ§∫ÈÖçÁΩÆ‰ª•Ê†áÊ≥®92191‰∏™Âè•Â≠ê„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåChatGPTÂú®ËØ≠‰πâÂàÜÁ±ª‰ªªÂä°‰∏≠ÂèñÂæó‰∫Ü88.71%ÁöÑF1ÂàÜÊï∞ÔºåLlamaÂú®ÊÉÖÊÑüÂàÜÊûê‰∏≠Á®çÂæÆ‰ºò‰∫éÂÖ∂‰ªñÊ®°ÂûãÔºåËææÂà∞82.66%„ÄÇÊâÄÊúâÊ®°ÂûãÂú®ÂêÑËá™‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Áõ∏‰ººÁöÑÊïàÊûúÔºåË°®ÊòéLLMsÂú®Âè£Ëø∞ÂéÜÂè≤ÂàÜÊûê‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊñáÂåñÈÅó‰∫ß‰øùÊä§„ÄÅÂéÜÂè≤Á†îÁ©∂ÂíåÁ§æ‰ºöÁßëÂ≠¶Á≠â„ÄÇÈÄöËøáÊèê‰æõÂèØÊâ©Â±ïÁöÑÊ†áÊ≥®ÁÆ°ÈÅìÔºåÁ†îÁ©∂‰∏∫Âè£Ëø∞ÂéÜÂè≤ÁöÑÂàÜÊûêÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊñπÊ≥ïÔºå‰øÉËøõ‰∫ÜÂØπÂéÜÂè≤ËÆ∞ÂøÜÁöÑÁêÜËß£‰∏é‰øùÂ≠òÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÁ§æ‰ºö‰ª∑ÂÄºÂíåÂ≠¶ÊúØÊÑè‰πâ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Oral histories are vital records of lived experience, particularly within communities affected by systemic injustice and historical erasure. Effective and efficient analysis of their oral history archives can promote access and understanding of the oral histories. However, Large-scale analysis of these archives remains limited due to their unstructured format, emotional complexity, and high annotation costs. This paper presents a scalable framework to automate semantic and sentiment annotation for Japanese American Incarceration Oral History. Using LLMs, we construct a high-quality dataset, evaluate multiple models, and test prompt engineering strategies in historically sensitive contexts. Our multiphase approach combines expert annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We labeled 558 sentences from 15 narrators for sentiment and semantic classification, then evaluated zero-shot, few-shot, and RAG strategies. For semantic classification, ChatGPT achieved the highest F1 score (88.71%), followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models showing comparable results. The best prompt configurations were used to annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our findings show that LLMs can effectively perform semantic and sentiment annotation across large oral history collections when guided by well-designed prompts. This study provides a reusable annotation pipeline and practical guidance for applying LLMs in culturally sensitive archival analysis. By bridging archival ethics with scalable NLP techniques, this work lays the groundwork for responsible use of artificial intelligence in digital humanities and preservation of collective memory. GitHub: https://github.com/kc6699c/LLM4OralHistoryAnalysis.

