---
layout: default
title: Evaluating Style-Personalized Text Generation: Challenges and Directions
---

# Evaluating Style-Personalized Text Generation: Challenges and Directions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06374" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06374v2</a>
  <a href="https://arxiv.org/pdf/2508.06374.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06374v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06374v2', 'Evaluating Style-Personalized Text Generation: Challenges and Directions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anubhav Jangra, Bahareh Sarrafzadeh, Silviu Cucerzan, Adrian de Wynter, Sujay Kumar Jauhar

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08 (æ›´æ–°: 2025-10-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé£æ ¼ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆè¯„ä¼°æ–¹æ³•ä»¥è§£å†³ç°æœ‰æŒ‡æ ‡ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é£æ ¼ä¸ªæ€§åŒ–` `æ–‡æœ¬ç”Ÿæˆ` `è¯„ä¼°æŒ‡æ ‡` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ ·åŒ–è¯„ä¼°` `è‡ªç„¶è¯­è¨€å¤„ç†` `å†™ä½œä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é£æ ¼ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆæ–¹æ³•åœ¨è¯„ä¼°æŒ‡æ ‡ä¸Šå­˜åœ¨æ ‡å‡†åŒ–ä¸è¶³å’Œä¸äººç±»è¯„ä¼°è€…ç›¸å…³æ€§å·®çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„é£æ ¼åŒºåˆ†åŸºå‡†ï¼Œé€šè¿‡å¤šç§è¯„ä¼°è®¾ç½®æ¥å…¨é¢è¯„ä¼°æ–‡æœ¬ç”Ÿæˆçš„é£æ ¼ä¸ªæ€§åŒ–æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¤šæ ·åŒ–çš„è¯„ä¼°æŒ‡æ ‡ç»„åˆåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å•ä¸€è¯„ä¼°æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œé£æ ¼ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆæˆä¸ºä¸€ä¸ªæ—¥ç›Šé‡è¦çš„ç ”ç©¶é¢†åŸŸã€‚ç„¶è€Œï¼Œé£æ ¼ä¸ªæ€§åŒ–é«˜åº¦ä¾èµ–ç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚åŠè¯­å¢ƒï¼Œè¿™ä½¿å¾—è¯„ä¼°å˜å¾—æå…·æŒ‘æˆ˜æ€§ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶æå‡ºäº†ä¸€äº›åŸºå‡†å’ŒæŒ‡æ ‡ï¼Œä½†è¿™äº›æŒ‡æ ‡å¾€å¾€ç¼ºä¹æ ‡å‡†åŒ–ï¼Œä¸”ä¸äººç±»è¯„ä¼°è€…çš„ç›¸å…³æ€§è¾ƒå·®ã€‚æœ¬æ–‡æ‰¹åˆ¤æ€§åœ°å®¡è§†äº†å½“å‰é¢†åŸŸå†…å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚BLEUã€åµŒå…¥å‘é‡å’ŒLLMsä½œä¸ºè¯„åˆ¤è€…çš„æ•ˆæœï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„é£æ ¼åŒºåˆ†åŸºå‡†ï¼Œæ¶µç›–äº†å…«ç§ä¸åŒçš„å†™ä½œä»»åŠ¡ã€‚ç ”ç©¶å‘ç°ï¼Œé‡‡ç”¨å¤šæ ·åŒ–çš„è¯„ä¼°æŒ‡æ ‡ç»„åˆèƒ½å¤Ÿæ˜¾è‘—ä¼˜äºå•ä¸€è¯„ä¼°æ–¹æ³•ï¼Œå¹¶æä¾›äº†å¯é è¯„ä¼°é£æ ¼ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆçš„æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é£æ ¼ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆè¯„ä¼°ä¸­ç°æœ‰æŒ‡æ ‡çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬ä¸äººç±»è¯„ä¼°è€…çš„ç›¸å…³æ€§è¾ƒå·®çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„é£æ ¼åŒºåˆ†åŸºå‡†ï¼Œç»“åˆå¤šç§è¯„ä¼°è®¾ç½®ï¼Œå…¨é¢è¯„ä¼°æ–‡æœ¬ç”Ÿæˆçš„é£æ ¼ä¸ªæ€§åŒ–æ•ˆæœï¼Œæ—¨åœ¨æé«˜è¯„ä¼°çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¢†åŸŸåŒºåˆ†ã€ä½œè€…å½’å±å’Œä¸ªæ€§åŒ–ä¸éä¸ªæ€§åŒ–æ–‡æœ¬çš„åŒºåˆ†ã€‚æ¯ä¸ªæ¨¡å—é’ˆå¯¹ä¸åŒçš„è¯„ä¼°ä»»åŠ¡è¿›è¡Œè®¾è®¡ï¼Œä»¥ç¡®ä¿å…¨é¢è¦†ç›–é£æ ¼ä¸ªæ€§åŒ–çš„å„ä¸ªæ–¹é¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å¤šæ ·åŒ–çš„è¯„ä¼°æŒ‡æ ‡ç»„åˆï¼Œå¼ºè°ƒäº†ä¸åŒè¯„ä¼°æ–¹æ³•çš„äº’è¡¥æ€§ï¼Œä»è€Œå…‹æœäº†å•ä¸€è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚BLEUã€åµŒå…¥å‘é‡å’ŒLLMsä½œä¸ºè¯„åˆ¤è€…ï¼Œç»“åˆä¸åŒçš„å†™ä½œä»»åŠ¡å’Œè¯„ä¼°è®¾ç½®ï¼Œä»¥ç¡®ä¿è¯„ä¼°ç»“æœçš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨å¤šæ ·åŒ–è¯„ä¼°æŒ‡æ ‡ç»„åˆçš„æ¨¡å‹åœ¨é£æ ¼ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå•ä¸€è¯„ä¼°æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¶…è¿‡20%ã€‚è¿™è¡¨æ˜å¤šæ ·åŒ–è¯„ä¼°ç­–ç•¥åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸ªæ€§åŒ–å†…å®¹ç”Ÿæˆã€ç¤¾äº¤åª’ä½“æ–‡æœ¬åˆ›ä½œä»¥åŠæ•™è‚²é¢†åŸŸçš„è‡ªåŠ¨å†™ä½œè¾…åŠ©å·¥å…·ã€‚é€šè¿‡æé«˜é£æ ¼ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆçš„è¯„ä¼°å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the surge of large language models (LLMs) and their ability to produce customized output, style-personalized text generation--"write like me"--has become a rapidly growing area of interest. However, style personalization is highly specific, relative to every user, and depends strongly on the pragmatic context, which makes it uniquely challenging. Although prior research has introduced benchmarks and metrics for this area, they tend to be non-standardized and have known limitations (e.g., poor correlation with human subjects). LLMs have been found to not capture author-specific style well, it follows that the metrics themselves must be scrutinized carefully. In this work we critically examine the effectiveness of the most common metrics used in the field, such as BLEU, embeddings, and LLMs-as-judges. We evaluate these metrics using our proposed style discrimination benchmark, which spans eight diverse writing tasks across three evaluation settings: domain discrimination, authorship attribution, and LLM-generated personalized vs non-personalized discrimination. We find strong evidence that employing ensembles of diverse evaluation metrics consistently outperforms single-evaluator methods, and conclude by providing guidance on how to reliably assess style-personalized text generation.

