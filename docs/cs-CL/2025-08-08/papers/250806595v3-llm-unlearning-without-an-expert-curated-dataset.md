---
layout: default
title: LLM Unlearning Without an Expert Curated Dataset
---

# LLM Unlearning Without an Expert Curated Dataset

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06595" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06595v3</a>
  <a href="https://arxiv.org/pdf/2508.06595.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06595v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06595v3', 'LLM Unlearning Without an Expert Curated Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyuan Zhu, Muru Zhang, Ollie Liu, Robin Jia, Willie Neiswanger

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08 (æ›´æ–°: 2025-10-07)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/xyzhu123/Synthetic_Textbook)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§è‡ªåŠ¨åŒ–ç”Ÿæˆé—å¿˜é›†çš„æ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†é—å¿˜` `è¯­è¨€æ¨¡å‹` `æ•°æ®åˆæˆ` `è‡ªåŠ¨åŒ–ç”Ÿæˆ` `éšç§ä¿æŠ¤` `æ¨¡å‹å®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é—å¿˜æ–¹æ³•åœ¨æ„å»ºæœ‰æ•ˆçš„é—å¿˜é›†æ–¹é¢å­˜åœ¨ç“¶é¢ˆï¼Œéš¾ä»¥å®ç°é«˜æ•ˆçš„çŸ¥è¯†ç§»é™¤ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–ç”Ÿæˆé—å¿˜é›†çš„æ–¹æ³•ï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹è‡ªèº«åˆæˆæ•™ç§‘ä¹¦é£æ ¼çš„æ•°æ®ï¼Œç®€åŒ–äº†é—å¿˜é›†çš„æ„å»ºè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆæˆæ•°æ®é›†åœ¨å¤šä¸ªé¢†åŸŸçš„é—å¿˜ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸”æ•°æ®å¤šæ ·æ€§æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹å¾€å¾€ç¼–ç æ•æ„Ÿã€æœ‰å®³æˆ–å—ç‰ˆæƒä¿æŠ¤çš„çŸ¥è¯†ï¼Œå› æ­¤éœ€è¦åæœŸé—å¿˜çš„èƒ½åŠ›ï¼Œå³åœ¨ä¸å®Œå…¨é‡è®­ç»ƒçš„æƒ…å†µä¸‹ä»æ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ã€‚ç›®å‰ï¼Œç°æœ‰çš„é—å¿˜æµç¨‹ä¸­ï¼Œæ„å»ºæœ‰æ•ˆçš„é—å¿˜é›†æ˜¯ä¸€ä¸ªä¸»è¦ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œé€šè¿‡è¯­è¨€æ¨¡å‹è‡ªèº«ç”Ÿæˆé«˜è´¨é‡çš„é—å¿˜é›†ã€‚è¯¥æ–¹æ³•ä»…éœ€è¾“å…¥é¢†åŸŸåç§°ï¼Œé€šè¿‡ç»“æ„åŒ–æç¤ºç”Ÿæˆæ•™ç§‘ä¹¦é£æ ¼çš„æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆæˆæ•°æ®é›†åœ¨ç”Ÿç‰©å®‰å…¨ã€ç½‘ç»œå®‰å…¨å’Œã€Šå“ˆåˆ©Â·æ³¢ç‰¹ã€‹å°è¯´çš„é—å¿˜ä»»åŠ¡ä¸­ï¼Œè¡¨ç°ä¼˜äºåŸºçº¿åˆæˆæ•°æ®é›†ï¼Œå¹¶ä¸ä¸“å®¶ç­–åˆ’çš„æ•°æ®é›†ç›¸å½“ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œå¤šæ­¥éª¤ç”Ÿæˆç®¡é“æ˜¾è‘—æé«˜äº†æ•°æ®å¤šæ ·æ€§ï¼Œä»è€Œæå‡äº†é—å¿˜çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆæˆæ•°æ®é›†ä¸ºå¹¿æ³›æ–°å…´é¢†åŸŸçš„å®é™…ã€å¯æ‰©å±•çš„é—å¿˜æä¾›äº†æœ‰å¸Œæœ›çš„è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ•æ„ŸçŸ¥è¯†çš„é—å¿˜é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ„å»ºæœ‰æ•ˆçš„é—å¿˜é›†æ—¶é¢ä¸´æ•°æ®è´¨é‡å’Œæ„å»ºæ•ˆç‡çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–ç”Ÿæˆé—å¿˜é›†çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–æç¤ºç”Ÿæˆé«˜è´¨é‡çš„åˆæˆæ•°æ®ï¼Œé¿å…äº†æ‰‹åŠ¨å¹²é¢„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥é¢†åŸŸåç§°ã€ç»“æ„åŒ–æç¤ºç”Ÿæˆæ•™ç§‘ä¹¦é£æ ¼æ•°æ®ã€ä»¥åŠæœ€ç»ˆå½¢æˆé—å¿˜é›†çš„å¤šæ­¥éª¤ç”Ÿæˆç®¡é“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºåˆ©ç”¨è¯­è¨€æ¨¡å‹è‡ªèº«ç”Ÿæˆé—å¿˜é›†ï¼Œæ˜¾è‘—æé«˜äº†æ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œä¸ä¼ ç»Ÿçš„æ‰‹åŠ¨æ„å»ºæ–¹æ³•ç›¸æ¯”ï¼Œæ•ˆç‡æ›´é«˜ä¸”æ›´å…·å¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ­¥éª¤çš„æç¤ºè®¾è®¡ï¼Œç¡®ä¿ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å’Œç›¸å…³æ€§ï¼Œå…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåˆæˆæ•°æ®é›†åœ¨ç”Ÿç‰©å®‰å…¨ã€ç½‘ç»œå®‰å…¨å’Œã€Šå“ˆåˆ©Â·æ³¢ç‰¹ã€‹å°è¯´çš„é—å¿˜ä»»åŠ¡ä¸­ï¼Œå‡ä¼˜äºåŸºçº¿åˆæˆæ•°æ®é›†ï¼Œä¸”ä¸ä¸“å®¶ç­–åˆ’çš„æ•°æ®é›†ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºåˆæˆæ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®éšç§ä¿æŠ¤ã€æ¨¡å‹å®‰å…¨æ€§æå‡ä»¥åŠçŸ¥è¯†ç®¡ç†ç­‰ã€‚é€šè¿‡æœ‰æ•ˆçš„çŸ¥è¯†é—å¿˜æœºåˆ¶ï¼Œèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šå’Œç»„ç»‡åœ¨å¤„ç†æ•æ„Ÿä¿¡æ¯æ—¶ï¼Œé™ä½æ³•å¾‹é£é™©å’Œé“å¾·è´£ä»»ã€‚æ­¤å¤–ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€åŒ»ç–—ç­‰é¢†åŸŸä¸­ï¼Œä¿ƒè¿›å¯¹ç‰¹å®šçŸ¥è¯†çš„å®‰å…¨ç®¡ç†å’Œä½¿ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.

