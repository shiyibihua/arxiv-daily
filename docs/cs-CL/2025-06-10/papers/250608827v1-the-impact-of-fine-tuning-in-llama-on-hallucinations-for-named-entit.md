---
layout: default
title: The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation
---

# The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.08827" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.08827v1</a>
  <a href="https://arxiv.org/pdf/2506.08827.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.08827v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.08827v1', 'The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Francisco Vargas, Alejandro Gonz√°lez Coene, Gaston Escalante, Exequiel Lob√≥n, Manuel Pulido

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-10

**ÊúüÂàä**: Electronic Journal of SADIO; vol. 24 (2025), no. 1

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éLLaMAÂæÆË∞ÉÁöÑÂëΩÂêçÂÆû‰ΩìÊèêÂèñÊñπÊ≥ï‰ª•ÂáèÂ∞ëÊ≥ïÂæãÊñáÊ°£‰∏≠ÁöÑÂπªËßâÁé∞Ë±°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂëΩÂêçÂÆû‰ΩìÊèêÂèñ` `Ê≥ïÂæãÊñáÊ°£ÂàÜÊûê` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÂæÆË∞ÉÊäÄÊúØ` `‰ø°ÊÅØÊèêÂèñ` `Êú∫Âô®Â≠¶‰π†` `ÊñáÊú¨Â§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Ê≥ïÂæãÊñáÊ°£‰∏≠ÊèêÂèñÂëΩÂêçÂÆû‰ΩìÊó∂ÂÆπÊòì‰∫ßÁîüÂπªËßâÔºåÂØºËá¥ÊèêÂèñÁªìÊûú‰∏çÂáÜÁ°Æ„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏§Ê≠•Ê≥ïÔºåÈ¶ñÂÖàËøõË°åÊñáÊ°£ÂàÜÊÆµÔºåÁÑ∂ÂêéÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂÆû‰ΩìÊèêÂèñÔºåÊó®Âú®ÊèêÈ´òÂáÜÁ°ÆÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂæÆË∞ÉÂêéÁöÑLLaMA-2 70BÊ®°ÂûãÂáÜÁ°ÆÁéáËææÂà∞79.4%ÔºåÊòæËëóÈ´ò‰∫éÁªèÂÖ∏ÊñπÊ≥ïÁöÑ39.5%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰ªéÊ≥ïÂæãÊñáÊ°£‰∏≠ÊèêÂèñ‰∫§ÈÄö‰∫ãÊïÖ‰ø°ÊÅØÂØπ‰∫éÈáèÂåñ‰øùÈô©ÂÖ¨Âè∏ÊàêÊú¨Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÊèêÂèñËØ∏Â¶ÇË∫´‰ΩìÂíå/ÊàñÂøÉÁêÜÊÆãÁñæÁöÑÁôæÂàÜÊØîÂèäÁõ∏ÂÖ≥ËµîÂÅøÈáëÈ¢ùÁ≠âÂÆû‰ΩìÁöÑËøáÁ®ãÂÖÖÊª°ÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê≥ïÂ∫≠Âà§ÂÜ≥‰∏≠Â≠òÂú®ÂæÆÂ¶ôÁöÑËÆ∫ËØÅÂíåÊé®ÁêÜ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏§Ê≠•Á®ãÂ∫èÔºöÈ¶ñÂÖàÂØπÊñáÊ°£ËøõË°åÂàÜÊÆµÔºåËØÜÂà´ÊúÄÁõ∏ÂÖ≥ÁöÑÈÉ®ÂàÜÔºåÁÑ∂ÂêéÊèêÂèñÂÆû‰Ωì„ÄÇÈÄöËøáÊØîËæÉÁªèÂÖ∏ÁöÑÊ≠£ÂàôË°®ËææÂºèÊñπÊ≥ï‰∏éÂü∫‰∫én-tokenÂùóÁöÑÂêëÈáèÂåñÊñπÊ≥ïÔºåÁªìÂêàÂ§öËØ≠Ë®ÄÊ®°ÂûãËøõË°åËØ≠‰πâÊêúÁ¥¢ÔºåÊúÄÁªàÂ∫îÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇLLaMAÂíåGPT-4 TurboÔºâËøõË°åÂÆû‰ΩìÊèêÂèñ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁªèËøáLoRAÂæÆË∞ÉÁöÑLLaMAÊ®°ÂûãÂú®ÂáèÂ∞ëÂπªËßâÁé∞Ë±°ÊñπÈù¢Ë°®Áé∞ÊòæËëóÔºåLLaMA-2 70BÁöÑÂáÜÁ°ÆÁéáËææÂà∞79.4%ÔºåË∂ÖË∂ä‰∫ÜÂü∫Á∫øÁöÑ61.7%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰ªéÊ≥ïÂæãÊñáÊ°£‰∏≠ÊèêÂèñÂëΩÂêçÂÆû‰ΩìÊó∂Âá∫Áé∞ÁöÑÂπªËßâÁé∞Ë±°ÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÁöÑÊ≥ïÂæãËØ≠Ë®ÄÊó∂ÂáÜÁ°ÆÊÄß‰∏çË∂≥ÔºåÂØºËá¥ÊèêÂèñÁªìÊûú‰∏çÂèØÈù†„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰∏ÄÁßç‰∏§Ê≠•Ê≥ïÔºåÈ¶ñÂÖàÂØπÊ≥ïÂæãÊñáÊ°£ËøõË°åÂàÜÊÆµÔºåËØÜÂà´Âá∫ÊúÄÁõ∏ÂÖ≥ÁöÑÈÉ®ÂàÜÔºåÁÑ∂ÂêéÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂÆû‰ΩìÊèêÂèñÔºåÁâπÂà´ÊòØÂØπLLaMAÊ®°ÂûãËøõË°åÂæÆË∞É‰ª•ÂáèÂ∞ëÂπªËßâÁé∞Ë±°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨ÊñáÊ°£ÂàÜÊÆµ„ÄÅÂêëÈáèÂåñ„ÄÅÂÆû‰ΩìÊèêÂèñ‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÊñáÊ°£ÂàÜÊÆµÈááÁî®ÁªèÂÖ∏ÁöÑÊ≠£ÂàôË°®ËææÂºèÊñπÊ≥ï‰∏éÂü∫‰∫én-tokenÂùóÁöÑÂêëÈáèÂåñÊñπÊ≥ïËøõË°åÊØîËæÉÔºåÈöèÂêé‰ΩøÁî®LLaMAÂíåGPT-4 TurboÁ≠âÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂÆû‰ΩìÊèêÂèñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÈÄöËøáÂØπLLaMAÊ®°ÂûãËøõË°åLoRAÂæÆË∞ÉÔºåÊòæËëóÂáèÂ∞ë‰∫ÜÂú®ÂÆû‰ΩìÊèêÂèñËøáÁ®ã‰∏≠Âá∫Áé∞ÁöÑÂπªËßâÁé∞Ë±°ÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåÂæÆË∞ÉÂêéÁöÑÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÊ≥ïÂæãÊñáÊú¨Êó∂Ë°®Áé∞Âá∫Êõ¥È´òÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑË∂ÖÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•‰ºòÂåñÊ®°ÂûãÂú®Ê≥ïÂæãÊñáÊ°£‰∏≠ÁöÑË°®Áé∞ÔºåÁ°Æ‰øùÊèêÂèñÁªìÊûúÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂæÆË∞ÉÂêéÁöÑLLaMA-2 70BÊ®°ÂûãÂú®ÂëΩÂêçÂÆû‰ΩìÊèêÂèñ‰ªªÂä°‰∏≠ÂáÜÁ°ÆÁéáËææÂà∞79.4%ÔºåÊòæËëóÈ´ò‰∫éÁªèÂÖ∏ÊñπÊ≥ïÁöÑ39.5%„ÄÇÊ≠§Â§ñÔºåÂü∫Á∫øLLaMA-3 8BÊ®°ÂûãÁöÑË°®Áé∞‰πüÁõ∏ÂΩìÂá∫Ëâ≤ÔºåËææÂà∞76.6%ÔºåËÄåGPT-4 TurboÂàô‰ª•86.1%ÁöÑÂáÜÁ°ÆÁéáÊàê‰∏∫ÊúÄÈ´òË°®Áé∞ËÄÖÔºåÂ±ïÁ§∫‰∫ÜÊ®°ÂûãÂèëÂ±ïÁöÑÂø´ÈÄüËøõÊ≠•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊ≥ïÂæãÊñáÊ°£ÂàÜÊûê„ÄÅ‰øùÈô©Á¥¢ËµîÂ§ÑÁêÜÂèäÂÖ∂‰ªñÈúÄË¶Å‰ªéÂ§çÊùÇÊñáÊú¨‰∏≠ÊèêÂèñÂÖ≥ÈîÆ‰ø°ÊÅØÁöÑÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÈ´òÂëΩÂêçÂÆû‰ΩìÊèêÂèñÁöÑÂáÜÁ°ÆÊÄßÔºåËÉΩÂ§üÊúâÊïàÈôç‰Ωé‰øùÈô©ÂÖ¨Âè∏ÁöÑËøêËê•ÊàêÊú¨ÔºåÂπ∂ÊèêÂçáÊ≥ïÂæãÊúçÂä°ÁöÑÊïàÁéá„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÁ±ªÂûãÁöÑÊñáÊ°£ÂàÜÊûê‰∏≠ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The extraction of information about traffic accidents from legal documents is crucial for quantifying insurance company costs. Extracting entities such as percentages of physical and/or psychological disability and the involved compensation amounts is a challenging process, even for experts, due to the subtle arguments and reasoning in the court decision. A two-step procedure is proposed: first, segmenting the document identifying the most relevant segments, and then extracting the entities. For text segmentation, two methodologies are compared: a classic method based on regular expressions and a second approach that divides the document into blocks of n-tokens, which are then vectorized using multilingual models for semantic searches (text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models (LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to the selected segments for entity extraction. For the LLaMA models, fine-tuning is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a significant number of hallucinations in extractions which are an important contention point for named entity extraction. This work shows that these hallucinations are substantially reduced after finetuning the model. The performance of the methodology based on segment vectorization and subsequent use of LLMs significantly surpasses the classic method which achieves an accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning achieves the highest accuracy 79.4%, surpassing its base version 61.7%. Notably, the base LLaMA-3 8B model already performs comparably to the finetuned LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

