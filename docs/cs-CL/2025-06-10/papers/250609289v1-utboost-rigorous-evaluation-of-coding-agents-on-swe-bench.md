---
layout: default
title: UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench
---

# UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09289" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09289v1</a>
  <a href="https://arxiv.org/pdf/2506.09289.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09289v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09289v1', 'UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Boxi Yu, Yuxuan Zhu, Pinjia He, Daniel Kang

**åˆ†ç±»**: cs.SE, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10

**æœŸåˆŠ**: ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUTBoostä»¥è§£å†³SWE-Benchä¸­æµ‹è¯•ç”¨ä¾‹ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¼–ç ä»£ç†` `æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨åŒ–æµ‹è¯•` `è½¯ä»¶å·¥ç¨‹` `åŸºå‡†æµ‹è¯•` `ä»£ç ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„SWE-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰‹åŠ¨ç¼–å†™çš„æµ‹è¯•ç”¨ä¾‹ä¸è¶³ï¼Œå¯¼è‡´ç”Ÿæˆçš„è¡¥ä¸æœªèƒ½æœ‰æ•ˆè§£å†³é—®é¢˜ã€‚
2. æå‡ºUTGeneratorä½œä¸ºæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ï¼Œè‡ªåŠ¨åˆ†æä»£ç åº“å¹¶ç”Ÿæˆé€‚ç”¨äºçœŸå®Pythoné¡¹ç›®çš„æµ‹è¯•ç”¨ä¾‹ï¼Œè¿›è€Œæ„å»ºUTBoostæ¡†æ¶ã€‚
3. è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¿®æ­£äº†345ä¸ªé”™è¯¯è¡¥ä¸ï¼Œæ˜¾è‘—å½±å“äº†SWE-Benchçš„å¤šä¸ªæ¡ç›®æ’åï¼Œæå‡äº†åŸºå‡†æµ‹è¯•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°æ¨åŠ¨äº†ç¼–ç ä»£ç†çš„å¼€å‘ï¼Œæ—¨åœ¨è§£å†³å®é™…ä»£ç ç”Ÿæˆé—®é¢˜ã€‚SWE-Benchä½œä¸ºè¯„ä¼°è¿™äº›ä»£ç†ä»£ç ç”Ÿæˆèƒ½åŠ›çš„åŸºå‡†ï¼Œä½¿ç”¨åŸºäºGitHubé—®é¢˜åŠå…¶å¯¹åº”æ‹‰å–è¯·æ±‚çš„çœŸå®ä¸–ç•Œé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ‹‰å–è¯·æ±‚ä¸­æ‰‹åŠ¨ç¼–å†™çš„æµ‹è¯•ç”¨ä¾‹å¾€å¾€ä¸è¶³ï¼Œå¯¼è‡´ç”Ÿæˆçš„è¡¥ä¸åœ¨æœªè§£å†³æ ¹æœ¬é—®é¢˜çš„æƒ…å†µä¸‹é€šè¿‡æµ‹è¯•ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†UTGeneratorï¼Œä¸€ä¸ªåŸºäºLLMçš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆ†æä»£ç åº“åŠå…¶ä¾èµ–å…³ç³»ï¼Œä¸ºçœŸå®çš„Pythoné¡¹ç›®ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†UTBoostï¼Œä¸€ä¸ªå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹å¢å¼ºæ¡†æ¶ã€‚æˆ‘ä»¬çš„è¯„ä¼°å‘ç°36ä¸ªä»»åŠ¡å®ä¾‹çš„æµ‹è¯•ç”¨ä¾‹ä¸è¶³ï¼Œå¹¶æ­ç¤ºäº†345ä¸ªé”™è¯¯è¡¥ä¸è¢«é”™è¯¯æ ‡è®°ä¸ºé€šè¿‡ï¼Œè¿™äº›ä¿®æ­£å½±å“äº†SWE-Bench Liteçš„40.9%å’ŒSWE-Bench Verifiedçš„24.4%æ¡ç›®ï¼Œåˆ†åˆ«å¯¼è‡´18å’Œ11ä¸ªæ’åå˜åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³SWE-BenchåŸºå‡†æµ‹è¯•ä¸­æµ‹è¯•ç”¨ä¾‹ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•çš„æ‰‹åŠ¨æµ‹è¯•ç”¨ä¾‹å¸¸å¸¸æ— æ³•æœ‰æ•ˆéªŒè¯ç”Ÿæˆè¡¥ä¸çš„æ­£ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥UTGeneratorï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œç¡®ä¿ç”Ÿæˆçš„è¡¥ä¸èƒ½å¤ŸçœŸæ­£è§£å†³ä»£ç ä¸­çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUTBoostæ¡†æ¶åŒ…æ‹¬UTGeneratoræ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ†æä»£ç åº“åŠå…¶ä¾èµ–å…³ç³»ï¼Œç”Ÿæˆé€‚åˆçš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶ä¸ç°æœ‰çš„SWE-Benchè¿›è¡Œé›†æˆã€‚

**å…³é”®åˆ›æ–°**ï¼šUTGeneratorçš„å¼•å…¥æ˜¯æœ¬æ–‡çš„ä¸»è¦åˆ›æ–°ç‚¹ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œæ˜¾è‘—æé«˜äº†æµ‹è¯•çš„å…¨é¢æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ä¼ ç»Ÿæ‰‹åŠ¨ç¼–å†™æµ‹è¯•ç”¨ä¾‹çš„æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨UTGeneratorä¸­ï¼Œé‡‡ç”¨äº†åŸºäºLLMçš„åˆ†ææ–¹æ³•ï¼Œç»“åˆç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹èƒ½å¤Ÿè¦†ç›–æ›´å¤šçš„ä»£ç è·¯å¾„å’Œæ½œåœ¨é”™è¯¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUTBoostä¿®æ­£äº†345ä¸ªé”™è¯¯è¡¥ä¸ï¼Œè¿™äº›è¡¥ä¸åœ¨åŸSWE-Benchä¸­è¢«é”™è¯¯æ ‡è®°ä¸ºé€šè¿‡ï¼Œå½±å“äº†40.9%çš„SWE-Bench Liteå’Œ24.4%çš„SWE-Bench Verifiedæ¡ç›®ï¼Œåˆ†åˆ«å¯¼è‡´18å’Œ11ä¸ªæ’åå˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†åŸºå‡†æµ‹è¯•çš„å‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€è‡ªåŠ¨åŒ–æµ‹è¯•å’Œä»£ç å®¡æŸ¥ç­‰ã€‚é€šè¿‡æé«˜æµ‹è¯•ç”¨ä¾‹çš„è´¨é‡å’Œè¦†ç›–ç‡ï¼ŒUTBoostèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´æœ‰æ•ˆåœ°è¯†åˆ«å’Œä¿®å¤ä»£ç ä¸­çš„é—®é¢˜ï¼Œä»è€Œæå‡è½¯ä»¶çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›æ‰©å±•åˆ°å…¶ä»–ç¼–ç¨‹è¯­è¨€å’Œå¼€å‘ç¯å¢ƒä¸­ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨è‡ªåŠ¨åŒ–æµ‹è¯•æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The advent of Large Language Models (LLMs) has spurred the development of coding agents for real-world code generation. As a widely used benchmark for evaluating the code generation capabilities of these agents, SWE-Bench uses real-world problems based on GitHub issues and their corresponding pull requests. However, the manually written test cases included in these pull requests are often insufficient, allowing generated patches to pass the tests without resolving the underlying issue. To address this challenge, we introduce UTGenerator, an LLM-driven test case generator that automatically analyzes codebases and dependencies to generate test cases for real-world Python projects. Building on UTGenerator, we propose UTBoost, a comprehensive framework for test case augmentation. In our evaluation, we identified 36 task instances with insufficient test cases and uncovered 345 erroneous patches incorrectly labeled as passed in the original SWE Bench. These corrections, impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard entries, yield 18 and 11 ranking changes, respectively.

