---
layout: default
title: Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning
---

# Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09033" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09033v3</a>
  <a href="https://arxiv.org/pdf/2506.09033.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09033v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09033v3', 'Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haozhen Zhang, Tao Feng, Jiaxuan You

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: Accepted by NeurIPS 2025. Code is available at https://github.com/ulab-uiuc/Router-R1. Models and Datasets are available at https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRouter-R1ä»¥è§£å†³å¤šè½®è·¯ç”±ä¸èšåˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¤šè½®è·¯ç”±` `æ¨¡å‹èšåˆ` `æ€§èƒ½ä¼˜åŒ–` `æˆæœ¬ç®¡ç†` `æ™ºèƒ½ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMè·¯ç”±å™¨é€šå¸¸ä»…æ”¯æŒå•è½®ã€ä¸€å¯¹ä¸€çš„æ˜ å°„ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†éœ€è¦å¤šæ¨¡å‹åä½œçš„å¤æ‚ä»»åŠ¡ã€‚
2. Router-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ å°†å¤šLLMè·¯ç”±å’Œèšåˆå»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†å’ŒåŠ¨æ€è°ƒç”¨èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRouter-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å¤šä¸ªå¼ºåŸºçº¿ï¼Œå±•ç°äº†æ›´å¥½çš„æ€§èƒ½å’Œæˆæœ¬ç®¡ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤šæ ·åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‡ºç°ï¼ŒLLMè·¯ç”±å™¨çš„å‘å±•ä¹Ÿéšä¹‹åŠ é€Ÿã€‚ç°æœ‰çš„LLMè·¯ç”±å™¨é€šå¸¸é‡‡ç”¨å•è½®ã€ä¸€å¯¹ä¸€çš„æ˜ å°„æ–¹å¼ï¼Œè¿™é™åˆ¶äº†å…¶å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†Router-R1ï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œå°†å¤šLLMè·¯ç”±å’Œèšåˆè§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–è¿‡ç¨‹ã€‚Router-R1å°†è·¯ç”±å™¨æœ¬èº«å®ä¾‹åŒ–ä¸ºä¸€ä¸ªå¼ºå¤§çš„LLMï¼Œåˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›å°†â€œæ€è€ƒâ€åŠ¨ä½œä¸â€œè·¯ç”±â€åŠ¨ä½œäº¤æ›¿è¿›è¡Œï¼Œå¹¶å°†æ¯ä¸ªå“åº”æ•´åˆåˆ°å…¶ä¸æ–­æ¼”å˜çš„ä¸Šä¸‹æ–‡ä¸­ã€‚é€šè¿‡é‡‡ç”¨è½»é‡çº§çš„åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼ŒRouter-R1åœ¨æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´ä¼˜åŒ–å¹³è¡¡ï¼Œå±•ç¤ºäº†åœ¨ä¸ƒä¸ªé€šç”¨å’Œå¤šè·³é—®ç­”åŸºå‡†ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMè·¯ç”±å™¨åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯å•è½®ã€ä¸€å¯¹ä¸€çš„æ˜ å°„æ–¹å¼æ— æ³•å……åˆ†åˆ©ç”¨å¤šä¸ªLLMçš„äº’è¡¥ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRouter-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ å°†å¤šLLMè·¯ç”±å’Œèšåˆè§†ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›äº¤æ›¿è¿›è¡Œå†…éƒ¨æ€è€ƒä¸åŠ¨æ€è·¯ç”±ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹è°ƒç”¨å’Œå“åº”æ•´åˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRouter-R1çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è·¯ç”±å™¨æœ¬èº«ä½œä¸ºLLMçš„å®ä¾‹ï¼Œç»“åˆè½»é‡çº§çš„åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ†ä¸ºæ€è€ƒã€è·¯ç”±å’Œå“åº”æ•´åˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šRouter-R1çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è·¯ç”±å’Œèšåˆè¿‡ç¨‹è§†ä¸ºä¸€ä¸ªåŠ¨æ€çš„å†³ç­–è¿‡ç¨‹ï¼Œå…è®¸æ¨¡å‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è¿›è¡Œè‡ªæˆ‘è°ƒæ•´ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ä¸æˆæœ¬çš„å¹³è¡¡èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒRouter-R1é‡‡ç”¨äº†ç®€å•çš„æ¨¡å‹æè¿°ä½œä¸ºæ¡ä»¶ï¼Œå¦‚å®šä»·ã€å»¶è¿Ÿå’Œç¤ºä¾‹æ€§èƒ½ï¼Œå¹¶å¼•å…¥äº†æ ¼å¼å¥–åŠ±ã€æœ€ç»ˆç»“æœå¥–åŠ±å’Œæ–°é¢–çš„æˆæœ¬å¥–åŠ±ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸ƒä¸ªé€šç”¨å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRouter-R1çš„è¡¨ç°è¶…è¶Šäº†å¤šä¸ªå¼ºåŸºçº¿ï¼Œå±•ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶åœ¨æˆæœ¬ç®¡ç†æ–¹é¢ä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Router-R1çš„ç ”ç©¶æˆæœåœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­å…·æœ‰æ½œåœ¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆå¤„ç†å¤æ‚æŸ¥è¯¢çš„æ™ºèƒ½å®¢æœã€ä¿¡æ¯æ£€ç´¢å’Œå¤šæ¨¡æ€äº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹é€‰æ‹©å’Œè°ƒç”¨ç­–ç•¥ï¼ŒRouter-R1èƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿçš„å“åº”é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management.

