---
layout: default
title: Can A Gamer Train A Mathematical Reasoning Model?
---

# Can A Gamer Train A Mathematical Reasoning Model?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.08935" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.08935v1</a>
  <a href="https://arxiv.org/pdf/2506.08935.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.08935v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.08935v1', 'Can A Gamer Train A Mathematical Reasoning Model?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrew Shin

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/shinandrew/YouronMath)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨æ¸¸æˆGPUè®­ç»ƒæ•°å­¦æ¨ç†æ¨¡å‹ä»¥é™ä½èµ„æºéœ€æ±‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°å­¦æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å†…å­˜ä¼˜åŒ–` `æ¸¸æˆGPU` `æ¨¡å‹è®­ç»ƒ` `èµ„æºä¼˜åŒ–` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†é€šå¸¸éœ€è¦æ˜‚è´µçš„è®¡ç®—èµ„æºå’Œé«˜ç«¯ç¡¬ä»¶æ”¯æŒã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨æ™®é€šæ¸¸æˆGPUå’Œç»“åˆå¼ºåŒ–å­¦ä¹ åŠå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼ŒæˆåŠŸè®­ç»ƒæ•°å­¦æ¨ç†æ¨¡å‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå‡ å€æ›´å¤§çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¼€å‘é€šå¸¸éœ€è¦é«˜æ˜‚çš„è®¡ç®—èµ„æºã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ä¸€å°æ™®é€šæ¸¸æˆGPUï¼ˆRTX 3080 Tiï¼‰ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œè®­ç»ƒå‡ºä¸€ä¸ªå…·æœ‰1.5äº¿å‚æ•°çš„æ•°å­¦æ¨ç†æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œèƒ½å¤Ÿåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°ä¸å‡ å€æ›´å¤§æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚è¿™ä¸€ç»“æœæŒ‘æˆ˜äº†å½“å‰å¯¹æ•°å­¦æ¨ç†æ¨¡å‹éœ€è¦åºå¤§åŸºç¡€è®¾æ–½çš„ä¼ ç»Ÿè§‚å¿µï¼Œä¿ƒè¿›äº†é«˜æ€§èƒ½AIç ”ç©¶çš„æ™®åŠã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å¯¹é«˜æ˜‚è®¡ç®—èµ„æºçš„ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é«˜ç«¯ç¡¬ä»¶é›†ç¾¤ï¼Œé™åˆ¶äº†å…¶æ™®åŠæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ä½¿ç”¨ä¸€å°æ™®é€šçš„æ¸¸æˆGPUï¼ˆRTX 3080 Tiï¼‰ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œè®­ç»ƒå‡ºä¸€ä¸ªé«˜æ•ˆçš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œä»è€Œé™ä½èµ„æºéœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®¾è®¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒå’Œå†…å­˜ä¼˜åŒ–å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é¢„å¤„ç†è´Ÿè´£å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹è®¾è®¡åˆ™åŸºäº1.5äº¿å‚æ•°çš„ç»“æ„ï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒç”¨äºä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå†…å­˜ä¼˜åŒ–åˆ™ç¡®ä¿åœ¨æœ‰é™çš„GPUå†…å­˜ä¸‹é«˜æ•ˆè¿è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåˆ©ç”¨æ™®é€šæ¸¸æˆGPUæˆåŠŸè®­ç»ƒå‡ºé«˜æ€§èƒ½çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œæ‰“ç ´äº†å¯¹å¤§å‹åŸºç¡€è®¾æ–½çš„ä¾èµ–ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†æˆæœ¬å’Œå¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥é€‚åº”æ•°å­¦æ¨ç†ä»»åŠ¡çš„ç‰¹ç‚¹ã€‚åŒæ—¶ï¼Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„é€‰æ‹©å’Œå†…å­˜ç®¡ç†ç­–ç•¥çš„ä¼˜åŒ–ä¹Ÿæ˜¯å…³é”®è®¾è®¡è¦ç´ ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ä»èƒ½ä¿æŒé«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨RTX 3080 Tiè®­ç»ƒçš„æ•°å­¦æ¨ç†æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†ä¸å‡ å€æ›´å¤§æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚è¿™ä¸€æˆæœä¸ä»…è¯æ˜äº†æ™®é€šç¡¬ä»¶çš„æœ‰æ•ˆæ€§ï¼Œè¿˜å±•ç¤ºäº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–æ¨ç†ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡é™ä½å¯¹é«˜ç«¯ç¡¬ä»¶çš„ä¾èµ–ï¼Œæ›´å¤šçš„ç ”ç©¶æœºæ„å’Œä¸ªäººå¼€å‘è€…èƒ½å¤Ÿå‚ä¸åˆ°æ•°å­¦æ¨ç†æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨ä¸­ï¼Œä»è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„æ™®åŠå’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. https://github.com/shinandrew/YouronMath.

