---
layout: default
title: "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic"
---

# Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01363" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01363v1</a>
  <a href="https://arxiv.org/pdf/2509.01363.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01363v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01363v1', 'Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammad Zbeeb, Hasan Abed Al Kader Hammoud, Bernard Ghanem

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01

**å¤‡æ³¨**: Under Review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡ä»»åŠ¡ç®—æœ¯è¿ç§»Chain-of-Thoughtèƒ½åŠ›ï¼šæå–å¹¶å¤ç”¨LLMæ¨ç†å‘é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†èƒ½åŠ›è¿ç§»` `ä»»åŠ¡å‘é‡ç®—æœ¯` `å¼ºåŒ–å­¦ä¹ ` `ç›‘ç£å¾®è°ƒ` `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¾èµ–æ˜‚è´µçš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œæˆæœ¬é«˜æ˜‚ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ä»»åŠ¡å‘é‡ç®—æœ¯ï¼Œå°†å·²è®­ç»ƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¿ç§»åˆ°å…¶ä»–æ¨¡å‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡æ€§èƒ½ï¼Œä¸”å…·æœ‰é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸éœ€è¦æ˜‚è´µçš„ä¼˜åŒ–ï¼Œä¾‹å¦‚å¼ºåŒ–å­¦ä¹ ï¼Œæ‰èƒ½æŒæ¡å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚æœ¬æ–‡è¯æ˜ï¼Œæ¨ç†èƒ½åŠ›ä¸€æ—¦è¢«å­¦ä¹ ï¼Œå°±å¯ä»¥ä½œä¸ºç´§å‡‘çš„ä»»åŠ¡å‘é‡åœ¨æ¨¡å‹ä¹‹é—´æå–å’Œè½¬ç§»ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„ã€ç›¸åŒåˆå§‹åŒ–çš„Qwen2.5æ¨¡å‹ï¼Œä¸€ä¸ªé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œå¾®è°ƒï¼Œå¦ä¸€ä¸ªé€šè¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨ç›¸åŒæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚ç”±æ­¤ï¼Œæˆ‘ä»¬æå–äº†ä¸€ä¸ªæ¨ç†å‘é‡ï¼š$v_{\text{reason} } = Î¸_{\text{GRPO} } - Î¸_{\text{SFT} }$ã€‚æˆ‘ä»¬å‡è®¾è¿™ä¸ªå‘é‡æ•è·äº†å¼ºåŒ–å­¦ä¹ æ‰€çŒè¾“çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å‰”é™¤äº†SFTè¿‡ç¨‹ä¸­çš„å…±äº«çŸ¥è¯†ã€‚å½“é€šè¿‡ç®€å•çš„ç®—æœ¯å°†å…¶æ·»åŠ åˆ°å…¼å®¹çš„æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ—¶ï¼Œè¿™ä¸ªå‘é‡å§‹ç»ˆå¦‚ä¸€åœ°æé«˜äº†å„ç§æ¨ç†åŸºå‡†çš„æ€§èƒ½ï¼šGSM8K (+4.9%)ï¼ŒHumanEval (+4.3%)ï¼ŒSciQ (+1.7%)ï¼Œä»¥åŠBigBenchHardï¼ˆ1.5Bæ¨¡å‹+12.3%ï¼‰ã€‚æ€§èƒ½æå‡åœ¨å¯¹æŠ—æ¡ä»¶ä¸‹ä»ç„¶å­˜åœ¨ã€‚ç›¸åï¼Œå‡å»è¯¥å‘é‡ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼ˆGSM8Kä¸Š-11.8%ï¼‰ï¼Œè¡¨æ˜è¯¥å‘é‡å¯¹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æœ‰å¾ˆå¼ºçš„è´¡çŒ®ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å¦‚ä½•ä»ç°æœ‰çš„å¼€æºæ¨¡å‹ä¸­æå–é€šå¸¸é€šè¿‡æ˜‚è´µè®­ç»ƒå¼€å‘çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ç®€å•çš„å¼ é‡ç®—æœ¯é‡ç”¨ï¼Œä»è€Œæä¾›äº†ä¸€ç§é€šè¿‡å›æ”¶å…ˆå‰çš„è®¡ç®—æŠ•èµ„æ¥å¢å¼ºæ¨¡å‹çš„å®ç”¨æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†é€šå¸¸éœ€è¦è€—è´¹å¤§é‡è®¡ç®—èµ„æºçš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚å¦‚ä½•é™ä½æ¨ç†èƒ½åŠ›è®­ç»ƒçš„æˆæœ¬ï¼Œå¹¶å®ç°æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆè¿ç§»ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥è¿›è¡Œç›‘ç£å¾®è°ƒæˆ–ä»å¤´å¼€å§‹è®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œå°†é€šè¿‡å¼ºåŒ–å­¦ä¹ è·å¾—çš„æ¨ç†èƒ½åŠ›ï¼Œä»¥å‘é‡çš„å½¢å¼ä»ä¸€ä¸ªæ¨¡å‹è¿ç§»åˆ°å¦ä¸€ä¸ªæ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡è®¡ç®—ç»è¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æ¨¡å‹å‚æ•°ä¸ç»è¿‡ç›‘ç£å¾®è°ƒçš„æ¨¡å‹å‚æ•°ä¹‹é—´çš„å·®å€¼ï¼Œå¾—åˆ°ä¸€ä¸ªâ€œæ¨ç†å‘é‡â€ï¼Œè¯¥å‘é‡ä»£è¡¨äº†å¼ºåŒ–å­¦ä¹ æ‰€å¸¦æ¥çš„æ¨ç†èƒ½åŠ›çš„å¢é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) ä½¿ç”¨ç›¸åŒçš„åˆå§‹åŒ–æ¨¡å‹ï¼Œåˆ†åˆ«è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼›2) è®¡ç®—GRPOæ¨¡å‹å’ŒSFTæ¨¡å‹å‚æ•°çš„å·®å€¼ï¼Œå¾—åˆ°æ¨ç†å‘é‡ï¼›3) å°†æ¨ç†å‘é‡åŠ åˆ°å…¶ä»–å…¼å®¹çš„æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸Šï¼Œä»è€Œå¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€é‡æ–°è®­ç»ƒï¼Œåªéœ€è¿›è¡Œç®€å•çš„å‘é‡åŠ æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œå®ƒæå‡ºäº†ä¸€ç§å°†æ¨ç†èƒ½åŠ›ä»ä¸€ä¸ªæ¨¡å‹â€œæå–â€å¹¶â€œæ³¨å…¥â€åˆ°å¦ä¸€ä¸ªæ¨¡å‹çš„æ–°æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ˜‚è´µçš„é‡æ–°è®­ç»ƒè¿‡ç¨‹ï¼Œå®ç°äº†æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆè¿ç§»å’Œå¤ç”¨ã€‚é€šè¿‡ä»»åŠ¡ç®—æœ¯çš„æ–¹å¼ï¼Œå°†æ¨ç†èƒ½åŠ›è¡¨ç¤ºä¸ºå¯æ“ä½œçš„å‘é‡ï¼Œä¸ºæ¨¡å‹èƒ½åŠ›çš„æ¨¡å—åŒ–å’Œç»„åˆæä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åœ¨äºæ¨ç†å‘é‡çš„è®¡ç®—æ–¹å¼ï¼š$v_{\text{reason}} = Î¸_{\text{GRPO}} - Î¸_{\text{SFT}}$ã€‚è¿™ç§è®¡ç®—æ–¹å¼æ—¨åœ¨æå–å¼ºåŒ–å­¦ä¹ å¸¦æ¥çš„æ¨ç†èƒ½åŠ›å¢é‡ï¼ŒåŒæ—¶æ¶ˆé™¤ç›‘ç£å¾®è°ƒå¸¦æ¥çš„å…±äº«çŸ¥è¯†çš„å½±å“ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜éªŒè¯äº†æ¨ç†å‘é‡çš„å¯åŠ æ€§å’Œå¯å‡æ€§ï¼Œå³æ·»åŠ æ¨ç†å‘é‡å¯ä»¥æå‡æ€§èƒ½ï¼Œè€Œå‡å»æ¨ç†å‘é‡åˆ™ä¼šé™ä½æ€§èƒ½ï¼Œä»è€ŒéªŒè¯äº†æ¨ç†å‘é‡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°†æ¨ç†å‘é‡æ·»åŠ åˆ°å…¶ä»–æ¨¡å‹åï¼Œåœ¨GSM8Kã€HumanEvalã€SciQå’ŒBigBenchHardç­‰å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨BigBenchHardåŸºå‡†ä¸Šï¼Œ1.5Bæ¨¡å‹çš„æ€§èƒ½æå‡äº†12.3%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå‡å»æ¨ç†å‘é‡ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ŒéªŒè¯äº†è¯¥å‘é‡å¯¹æ¨ç†èƒ½åŠ›çš„è´¡çŒ®ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿ç§»æ¨ç†èƒ½åŠ›ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€ç§‘å­¦æ¨ç†ç­‰ã€‚é€šè¿‡è¿ç§»å·²è®­ç»ƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥é™ä½æ–°æ¨¡å‹è®­ç»ƒçš„æˆæœ¬ï¼ŒåŠ é€ŸAIæŠ€æœ¯çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæ„å»ºæ›´æ¨¡å—åŒ–ã€å¯ç»„åˆçš„AIç³»ç»Ÿï¼Œé€šè¿‡ç»„åˆä¸åŒçš„ä»»åŠ¡å‘é‡ï¼Œå®ç°æ›´å¤æ‚çš„åŠŸèƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as a compact task vector. We source two publicly available, identically initialized Qwen2.5 models, one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract a reasoning vector: $v_{\text{reason} } = Î¸_{\text{GRPO} } - Î¸_{\text{SFT} }$. We hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation (-11.8% on GSM8K), demonstrating the vector's strong contribution to the model's reasoning abilities. This work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering a practical way to enhance models by recycling prior computational investments.

