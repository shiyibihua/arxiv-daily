---
layout: default
title: REFRAG: Rethinking RAG based Decoding
---

# REFRAG: Rethinking RAG based Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01092" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.01092v2</a>
  <a href="https://arxiv.org/pdf/2509.01092.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01092v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01092v2', 'REFRAG: Rethinking RAG based Decoding')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xiaoqiang Lin, Aritra Ghosh, Bryan Kian Hsiang Low, Anshumali Shrivastava, Vijai Mohan

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-01 (Êõ¥Êñ∞: 2025-10-12)

**Â§áÊ≥®**: fix typo perplexity->log perplexity; added recent papers

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫REFRAG‰ª•Ëß£ÂÜ≥RAGËß£Á†ÅÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Èïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜ` `Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê` `Ëß£Á†ÅÊïàÁéá` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Á≥ªÁªü‰ºòÂåñ` `Â§öËΩÆÂØπËØù` `ÈïøÊñáÊ°£ÊëòË¶Å`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑRAGÊñπÊ≥ïÂú®Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáËæìÂÖ•Êó∂Èù¢‰∏¥ÊòæËëóÁöÑÁ≥ªÁªüÂª∂ËøüÂíåÂÜÖÂ≠òÊ∂àËÄóÈóÆÈ¢òÔºåÂΩ±Âìç‰∫ÜÊï¥‰ΩìÊïàÁéá„ÄÇ
2. REFRAGÈÄöËøáËØÜÂà´Âπ∂Ê∂àÈô§Â§ßÈÉ®ÂàÜ‰∏çÂøÖË¶ÅÁöÑËÆ°ÁÆóÔºå‰ºòÂåñ‰∫ÜRAGËß£Á†ÅËøáÁ®ãÔºå‰ªéËÄåÊèêÂçá‰∫ÜÁ≥ªÁªüÁöÑÂìçÂ∫îÈÄüÂ∫¶„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåREFRAGÂú®Â§öÁßç‰ªªÂä°‰∏äÂÆûÁé∞‰∫Ü30.85ÁöÑÈ¶ñÊ¨°‰ª§ÁâåÂä†ÈÄüÔºå‰∏îÂú®ÂáÜÁ°ÆÊÄß‰∏äÊ≤°ÊúâÊçüÂ§±Ôºå‰ºò‰∫éÁé∞ÊúâÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öËΩÆÂØπËØùÂíåÊô∫ËÉΩÂ∫îÁî®‰∏≠Â±ïÁé∞‰∫ÜÂçìË∂äÁöÑËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâ‰∏≠„ÄÇÁÑ∂ËÄåÔºåÂ§ÑÁêÜÈïø‰∏ä‰∏ãÊñáËæìÂÖ•‰ºöÂØºËá¥Á≥ªÁªüÂª∂ËøüÂíåÂÜÖÂ≠òÈúÄÊ±ÇÂ¢ûÂä†Ôºå‰ªéËÄåÂΩ±ÂìçÊïàÁéá„ÄÇÊú¨ÊñáÊèêÂá∫REFRAGÔºå‰∏Ä‰∏™È´òÊïàÁöÑËß£Á†ÅÊ°ÜÊû∂ÔºåÈÄöËøáÂéãÁº©„ÄÅÊÑüÁü•ÂíåÊâ©Â±ïÊù•ÊèêÂçáRAGÂ∫îÁî®ÁöÑÂª∂ËøüÊÄßËÉΩ„ÄÇÂÆûÈ™åË°®ÊòéÔºåREFRAGÂú®‰∏çÊçüÂ§±Âõ∞ÊÉëÂ∫¶ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆûÁé∞‰∫Ü30.85ÁöÑÈ¶ñÊ¨°‰ª§ÁâåÂä†ÈÄüÔºå‰∏îËÉΩÂ§üÂ∞Ü‰∏ä‰∏ãÊñáÂ§ßÂ∞èÊâ©Â±ï16ÂÄç„ÄÇÊàë‰ª¨Âú®Â§öÁßçÈïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏äÂØπREFRAGËøõË°å‰∫Ü‰∏•Ê†ºÈ™åËØÅÔºåÁªìÊûúÊòæÁ§∫ÂÖ∂Âú®ÈÄüÂ∫¶ÂíåÂáÜÁ°ÆÊÄß‰∏äÂùá‰ºò‰∫éLLaMAÊ®°ÂûãÂèäÂÖ∂‰ªñÂÖàËøõÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥RAGËß£Á†ÅËøáÁ®ã‰∏≠Áî±‰∫éÈïø‰∏ä‰∏ãÊñáËæìÂÖ•ÂØºËá¥ÁöÑÁ≥ªÁªüÂª∂ËøüÂíåÂÜÖÂ≠òÊ∂àËÄóÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÊó∂ÔºåÂæÄÂæÄÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ°ÁÆóËµÑÊ∫êÔºåÂΩ±Âìç‰∫ÜÁ≥ªÁªüÁöÑÊï¥‰ΩìÊïàÁéá„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöREFRAGÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáÂéãÁº©„ÄÅÊÑüÁü•ÂíåÊâ©Â±ïÁöÑÊñπÂºèÔºåËØÜÂà´Âπ∂Ê∂àÈô§Â§ßÈÉ®ÂàÜ‰∏çÂøÖË¶ÅÁöÑËÆ°ÁÆóÔºå‰ªéËÄåÊèêÂçáRAGÂ∫îÁî®ÁöÑËß£Á†ÅÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÁâπÂà´ÂÖ≥Ê≥®‰∫éRAG‰∏ä‰∏ãÊñá‰∏≠Â§ßÈÉ®ÂàÜ‰ø°ÊÅØÁöÑÂÜó‰ΩôÊÄßÔºåËÆ§‰∏∫ÂèØ‰ª•Âú®‰∏çÂΩ±ÂìçÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÂáèÂ∞ëËÆ°ÁÆóÈáè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöREFRAGÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂéãÁº©Ê®°ÂùóÁî®‰∫éÂáèÂ∞ëËæìÂÖ•Êï∞ÊçÆÁöÑÂÜó‰ΩôÔºåÊÑüÁü•Ê®°ÂùóÁî®‰∫éËØÜÂà´ÈáçË¶Å‰ø°ÊÅØÔºåÊâ©Â±ïÊ®°ÂùóÂàôÁî®‰∫éÂú®ÂøÖË¶ÅÊó∂ÊÅ¢Â§ç‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÈ´òÊïàÁöÑËÆ°ÁÆóÊµÅÁ®ãÔºå‰ºòÂåñ‰∫ÜRAGÁöÑËß£Á†ÅËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöREFRAGÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ËÉΩÂ§üÊúâÊïàÂà©Áî®‰∏ä‰∏ãÊñáÁöÑÁ®ÄÁñèÊÄßÁªìÊûÑÔºåÊòæËëóÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÈúÄÊ±ÇÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËøô‰∏ÄËÆæËÆ°‰∏é‰º†ÁªüÁöÑRAGÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∑ÊúâÊú¨Ë¥®‰∏äÁöÑÂå∫Âà´ÔºåËÉΩÂ§üÂú®‰∏çÁâ∫Áâ≤ÂáÜÁ°ÆÊÄßÁöÑÂâçÊèê‰∏ãÊèêÂçáÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®REFRAG‰∏≠ÔºåÂÖ≥ÈîÆÁöÑÂèÇÊï∞ËÆæÁΩÆÂåÖÊã¨‰∏ä‰∏ãÊñáÂéãÁº©ÊØî‰æãÂíåÊÑüÁü•ÈòàÂÄºÔºåÊçüÂ§±ÂáΩÊï∞ÂàôËÆæËÆ°‰∏∫Âπ≥Ë°°ËÆ°ÁÆóÊïàÁéá‰∏éÊ®°ÂûãÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÁΩëÁªúÁªìÊûÑÈááÁî®‰∫ÜÈíàÂØπÈïø‰∏ä‰∏ãÊñá‰ºòÂåñÁöÑËÆæËÆ°Ôºå‰ª•ÊîØÊåÅÊõ¥Â§ßËßÑÊ®°ÁöÑ‰∏ä‰∏ãÊñáËæìÂÖ•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

REFRAGÂú®Â§öÁßçÈïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÂÖ∂ÂÆûÁé∞‰∫Ü30.85ÁöÑÈ¶ñÊ¨°‰ª§ÁâåÂä†ÈÄüÔºåÁõ∏ËæÉ‰∫é‰πãÂâçÁöÑÂ∑•‰ΩúÊèêÈ´ò‰∫Ü3.75ÂÄçÔºåÂêåÊó∂Âú®ÂáÜÁ°ÆÊÄß‰∏äÊ≤°Êúâ‰ªª‰ΩïÊçüÂ§±„ÄÇËøô‰∏ÄÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá‰ΩøÂæóREFRAGÂú®Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáËæìÂÖ•Êó∂Êàê‰∏∫‰∏ÄÁßçÊúâÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

REFRAGÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÊΩúÂú®Â∫îÁî®‰ª∑ÂÄºÔºåÂåÖÊã¨Êô∫ËÉΩÂØπËØùÁ≥ªÁªü„ÄÅÈïøÊñáÊ°£ÊëòË¶ÅÁîüÊàêÂíåÂ§öËΩÆ‰∫§‰∫íÂ∫îÁî®Á≠â„ÄÇÈÄöËøáÊèêÂçáRAGÁöÑËß£Á†ÅÊïàÁéáÔºåREFRAGËÉΩÂ§ü‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Âø´ÈÄüÁöÑÂìçÂ∫îÔºåÊîπÂñÑÁî®Êà∑‰ΩìÈ™åÔºåÂπ∂Êé®Âä®Êô∫ËÉΩÁ≥ªÁªüÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂπøÊ≥õÈááÁî®„ÄÇÊú™Êù•ÔºåREFRAGÁöÑÊäÄÊúØÊ°ÜÊû∂‰πüÂèØËÉΩË¢´Êâ©Â±ïÂà∞ÂÖ∂‰ªñÈúÄË¶ÅÂ§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÁöÑ‰ªªÂä°‰∏≠ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÂÖ∂Â∫îÁî®ËåÉÂõ¥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive external knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-augmented generation (RAG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing latency for long-context inputs is a primary objective for LLMs, we contend that RAG require specialized consideration. In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting the sparsity structure, we demonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to previous work) without loss in perplexity. In addition, our optimization framework for large context enables REFRAG to extend the context size of LLMs by 16. We provide rigorous validation of REFRAG across diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm that REFRAG delivers substantial speedup with no loss in accuracy compared to LLaMA models and other state-of-the-art baselines across various context sizes.

