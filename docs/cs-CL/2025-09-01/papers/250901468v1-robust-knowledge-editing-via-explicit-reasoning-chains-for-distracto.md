---
layout: default
title: Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA
---

# Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01468" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01468v1</a>
  <a href="https://arxiv.org/pdf/2509.01468.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01468v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01468v1', 'Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuchen Wu, Liang Ding, Li Shen, Dacheng Tao

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01

**å¤‡æ³¨**: EMNLP 2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReason-KEæ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼æ¨ç†é“¾å®ç°å¯¹LLMçš„é²æ£’çŸ¥è¯†ç¼–è¾‘ï¼Œæå‡å¤šè·³QAæŠ—å¹²æ‰°èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†ç¼–è¾‘` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šè·³é—®ç­”` `æ¨ç†é“¾` `é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨å™ªå£°ç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ï¼Œæ˜“å—å¹²æ‰°ä¿¡æ¯å½±å“ï¼Œä¸”ä¾èµ–å¤æ‚è¿­ä»£æµç¨‹ã€‚
2. Reason-KEé€šè¿‡æ˜¾å¼æ¨ç†é“¾ï¼Œå¼•å¯¼LLMè¿›è¡Œäº‹å®ç¡®è®¤ã€ç›¸å…³æ€§åˆ¤æ–­ã€é€‰æ‹©æ€§åº”ç”¨å’Œæœ€ç»ˆæ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒReason-KEæ˜¾è‘—æå‡äº†LLMåœ¨å¤šè·³QAä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡å’ŒæŠ—å¹²æ‰°èƒ½åŠ›ï¼Œè¾¾åˆ°æ–°çš„SOTAã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è•´å«ç€æµ·é‡çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä½†ä¸€æ—¦è®­ç»ƒå®Œæˆå°±å¤„äºé™æ€çŠ¶æ€ï¼Œé€šè¿‡å®Œå…¨é‡æ–°è®­ç»ƒæ¥åŠæ—¶æ•´åˆæ–°å…´äº‹å®çš„æˆæœ¬éå¸¸é«˜æ˜‚ã€‚å› æ­¤ï¼ŒçŸ¥è¯†ç¼–è¾‘æŠ€æœ¯åº”è¿è€Œç”Ÿï¼Œæ—¨åœ¨å°†ç‰¹å®šäº‹å®æ³¨å…¥æˆ–è¦†ç›–åˆ°LLMsä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯è¦ä¹ˆè¿‡åº¦ä¾èµ–è¡¨é¢çº¿ç´¢ï¼Œè¦ä¹ˆäº§ç”Ÿå¤æ‚ã€è¿­ä»£çš„æµç¨‹ï¼Œåœ¨å˜ˆæ‚çš„å¤šè·³æ¡ä»¶ä¸‹å´©æºƒã€‚æˆ‘ä»¬å¼•å…¥äº†Reason-KEï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç«¯åˆ°ç«¯æ¨ç†é“¾çš„ç¼–è¾‘æ¡†æ¶ï¼Œå®ƒå¼•å¯¼é¢„è®­ç»ƒçš„LLMé€šè¿‡å››ä¸ªç»“æ„åŒ–é˜¶æ®µâ€”â€”äº‹å®ç¡®è®¤ã€ç›¸å…³æ€§ç¡®å®šã€é€‰æ‹©æ€§åº”ç”¨å’Œæœ€ç»ˆæ¨ç†â€”â€”ä»¥å•æ¬¡è¿‡æ»¤æ‰å¹²æ‰°å› ç´ ã€‚åœ¨åŒ…å«å¤šè¾¾å››ä¸ªä¸ç›¸å…³äº‹å®çš„MQuAKE-CFä¸Šè®­ç»ƒåï¼ŒReason-KEå°†Qwen2.5-7Bçš„å¤šè·³QAå‡†ç¡®ç‡æå‡è‡³90.2%ï¼ŒåŒæ—¶åœ¨ä¸¥é‡å¹²æ‰°ä¸‹ä»…ä¸‹é™6.3%ï¼Œåœ¨ç­”æ¡ˆæ³„éœ²æ—¶ä¸‹é™<1%ã€‚æˆ‘ä»¬çš„å®šé‡åˆ†æè¯å®äº†Reason-KEçš„é²æ£’æ€§å’Œæ•ˆç‡ï¼Œä¸ºå¯é çš„LLMçŸ¥è¯†æ›´æ–°å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨å¤„ç†åŒ…å«å¹²æ‰°ä¿¡æ¯çš„å¤šè·³é—®ç­”ä»»åŠ¡æ—¶ï¼Œå®¹æ˜“å—åˆ°è¡¨é¢çº¿ç´¢çš„è¯¯å¯¼ï¼Œæˆ–è€…éœ€è¦å¤æ‚çš„è¿­ä»£æµç¨‹ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ç”šè‡³å´©æºƒã€‚è¿™äº›æ–¹æ³•æ— æ³•æœ‰æ•ˆåœ°è¯†åˆ«å’Œè¿‡æ»¤æ‰æ— å…³ä¿¡æ¯ï¼Œä»è€Œå½±å“æœ€ç»ˆçš„ç­”æ¡ˆå‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReason-KEçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ˜¾å¼çš„æ¨ç†é“¾ï¼Œè®©LLMèƒ½å¤Ÿé€æ­¥åœ°è¿›è¡Œäº‹å®ç¡®è®¤ã€ç›¸å…³æ€§åˆ¤æ–­ã€é€‰æ‹©æ€§åº”ç”¨å’Œæœ€ç»ˆæ¨ç†ã€‚è¿™ç§ç»“æ„åŒ–çš„æ¨ç†è¿‡ç¨‹å¯ä»¥å¸®åŠ©LLMæ›´å¥½åœ°ç†è§£é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯†åˆ«å’Œè¿‡æ»¤æ‰å¹²æ‰°ä¿¡æ¯ï¼Œæé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReason-KEæ¡†æ¶åŒ…å«å››ä¸ªä¸»è¦é˜¶æ®µï¼š1) **äº‹å®ç¡®è®¤**ï¼šéªŒè¯è¾“å…¥ä¿¡æ¯æ˜¯å¦ä¸ºå·²çŸ¥äº‹å®ï¼›2) **ç›¸å…³æ€§ç¡®å®š**ï¼šåˆ¤æ–­æ¯ä¸ªäº‹å®ä¸å½“å‰é—®é¢˜çš„ç›¸å…³ç¨‹åº¦ï¼›3) **é€‰æ‹©æ€§åº”ç”¨**ï¼šæ ¹æ®ç›¸å…³æ€§å¾—åˆ†ï¼Œé€‰æ‹©æ€§åœ°åº”ç”¨ç›¸å…³äº‹å®ï¼›4) **æœ€ç»ˆæ¨ç†**ï¼šåŸºäºé€‰æ‹©çš„äº‹å®è¿›è¡Œæ¨ç†ï¼Œå¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€å¤æ‚çš„è¿­ä»£æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šReason-KEçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†æ˜¾å¼çš„æ¨ç†é“¾ï¼Œå°†çŸ¥è¯†ç¼–è¾‘è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªå¯è§£é‡Šçš„æ­¥éª¤ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒReason-KEèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«å’Œè¿‡æ»¤æ‰å¹²æ‰°ä¿¡æ¯ï¼Œæé«˜äº†çŸ¥è¯†ç¼–è¾‘çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒReason-KEé‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒæ–¹å¼ï¼Œç®€åŒ–äº†è®­ç»ƒæµç¨‹ï¼Œæé«˜äº†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šReason-KEä½¿ç”¨é¢„è®­ç»ƒçš„LLMä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ·»åŠ äº†å››ä¸ªç»“æ„åŒ–çš„æ¨ç†é˜¶æ®µã€‚æ¯ä¸ªé˜¶æ®µéƒ½ä½¿ç”¨ç‰¹å®šçš„promptè¿›è¡Œå¼•å¯¼ï¼Œä¾‹å¦‚ï¼Œåœ¨äº‹å®ç¡®è®¤é˜¶æ®µï¼Œpromptä¼šå¼•å¯¼LLMåˆ¤æ–­è¾“å…¥ä¿¡æ¯æ˜¯å¦ä¸ºå·²çŸ¥äº‹å®ã€‚ç›¸å…³æ€§ç¡®å®šé˜¶æ®µä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥è®¡ç®—æ¯ä¸ªäº‹å®ä¸é—®é¢˜çš„ç›¸å…³æ€§å¾—åˆ†ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬äº¤å‰ç†µæŸå¤±å’Œå¯¹æ¯”æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒæŠ—å¹²æ‰°èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Reason-KEåœ¨MQuAKE-CFæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼ŒReason-KEå°†Qwen2.5-7Bçš„å¤šè·³QAå‡†ç¡®ç‡æå‡è‡³90.2%ï¼ŒåŒæ—¶åœ¨ä¸¥é‡å¹²æ‰°ä¸‹ä»…ä¸‹é™6.3%ï¼Œåœ¨ç­”æ¡ˆæ³„éœ²æ—¶ä¸‹é™<1%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒReason-KEå…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§å’ŒæŠ—å¹²æ‰°èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çŸ¥è¯†ç¼–è¾‘æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Reason-KEå¯åº”ç”¨äºéœ€è¦é¢‘ç¹æ›´æ–°çŸ¥è¯†çš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€çŸ¥è¯†å›¾è°±é—®ç­”ã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå¿«é€Ÿã€å‡†ç¡®åœ°å°†æ–°çŸ¥è¯†æ³¨å…¥LLMï¼Œå¹¶æœ‰æ•ˆé¿å…å¹²æ‰°ä¿¡æ¯çš„å¹²æ‰°ï¼Œæé«˜LLMçš„å¯é æ€§å’Œå®ç”¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼Œå¹¶ä¿ƒè¿›LLMåœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) encode vast amounts of world knowledge but remain static once trained, making the timely integration of emerging facts prohibitively expensive via full retraining. Knowledge-editing techniques have thus emerged to inject or overwrite specific facts into LLMs, yet they either over-rely on superficial cues or incur complex, iterative pipelines that collapse under noisy, multi-hop conditions. We introduce Reason-KE, an end-to-end reasoning-chain-based editing framework that steers a pretrained LLM through four structured stages-fact acknowledgment, relevance determination, selective application, and final reasoning-to filter distractors in a single pass. Trained on MQuAKE-CF with up to four irrelevant facts, Reason-KE elevates Qwen2.5-7B's multi-hop QA accuracy to 90.2% while suffering merely a 6.3% drop under heavy distraction and <1% when answers are leaked. Our quantitative analysis confirms Reason-KE's resilience and efficiency, establishing a new state-of-the-art for reliable LLM knowledge updates.

