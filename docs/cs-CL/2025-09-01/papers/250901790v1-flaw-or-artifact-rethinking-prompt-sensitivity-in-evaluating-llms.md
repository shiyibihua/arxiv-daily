---
layout: default
title: Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs
---

# Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01790" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01790v1</a>
  <a href="https://arxiv.org/pdf/2509.01790.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01790v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01790v1', 'Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andong Hua, Kenan Tang, Chenhe Gu, Jindong Gu, Eric Wong, Yao Qin

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01

**å¤‡æ³¨**: Accepted to EMNLP 2025 Main Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é‡æ–°å®¡è§†LLMçš„promptæ•æ„Ÿæ€§ï¼šè¯„ä¼°æ–¹æ³•ä¼ªåƒè¿˜æ˜¯æ¨¡å‹ç¼ºé™·ï¼Ÿ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `Promptæ•æ„Ÿæ€§` `è¯„ä¼°æ–¹æ³•` `LLM-as-a-Judge` `é²æ£’æ€§` `è¯­ä¹‰ç†è§£` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶è¡¨æ˜LLMå¯¹promptéå¸¸æ•æ„Ÿï¼Œä½†è¿™ç§æ•æ„Ÿæ€§æ˜¯å¦çœŸå®åæ˜ äº†æ¨¡å‹ç¼ºé™·ä»å¾…è€ƒå¯Ÿã€‚
2. è¯¥ç ”ç©¶çš„æ ¸å¿ƒåœ¨äºä½¿ç”¨LLMä½œä¸ºè£åˆ¤ï¼Œé‡æ–°è¯„ä¼°LLMåœ¨ä¸åŒpromptä¸‹çš„è¡¨ç°ï¼Œä»¥æ¶ˆé™¤ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•çš„åå·®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LLMä½œä¸ºè£åˆ¤æ—¶ï¼ŒLLMçš„æ€§èƒ½å·®å¼‚æ˜¾è‘—é™ä½ï¼Œæ¨¡å‹æ’åæ›´åŠ ç¨³å®šï¼Œæç¤ºLLMæœ¬èº«å¯èƒ½æ¯”ä¹‹å‰è®¤ä¸ºçš„æ›´é²æ£’ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Promptæ•æ„Ÿæ€§ï¼Œå³ä½¿ç”¨ä¸åŒçš„æªè¾é‡å¤ç›¸åŒå†…å®¹ä¼šå¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½æ˜¾è‘—å˜åŒ–ï¼Œå·²è¢«å¹¿æ³›è®¤ä¸ºæ˜¯LLMçš„ä¸€ä¸ªæ ¸å¿ƒå±€é™ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶æå‡ºç–‘é—®ï¼šè¢«å¹¿æ³›æŠ¥é“çš„é«˜promptæ•æ„Ÿæ€§æ˜¯LLMå›ºæœ‰çš„å¼±ç‚¹ï¼Œè¿˜æ˜¯è¯„ä¼°è¿‡ç¨‹ä¸­çš„ä¼ªåƒï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†7ä¸ªLLMï¼ˆä¾‹å¦‚ï¼ŒGPTå’ŒGeminiç³»åˆ—ï¼‰ï¼Œæ¶µç›–6ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼ä»»åŠ¡ï¼Œä»¥åŠ12ä¸ªä¸åŒçš„promptæ¨¡æ¿ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¤§éƒ¨åˆ†promptæ•æ„Ÿæ€§æºäºå¯å‘å¼è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬å¯¹æ•°ä¼¼ç„¶è¯„åˆ†å’Œä¸¥æ ¼çš„ç­”æ¡ˆåŒ¹é…ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å¿½ç•¥äº†é€šè¿‡åŒä¹‰è¯æˆ–é‡Šä¹‰ç­‰æ›¿ä»£æªè¾è¡¨è¾¾çš„è¯­ä¹‰æ­£ç¡®çš„å“åº”ã€‚å½“æˆ‘ä»¬é‡‡ç”¨LLM-as-a-Judgeè¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ€§èƒ½å·®å¼‚æ˜¾è‘—é™ä½ï¼Œå¹¶ä¸”æ¨¡å‹æ’ååœ¨ä¸åŒpromptä¹‹é—´å…·æœ‰æ›´é«˜çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°ä»£LLMå¯¹promptæ¨¡æ¿çš„é²æ£’æ€§æ¯”ä»¥å‰è®¤ä¸ºçš„è¦å¼ºï¼Œå¹¶ä¸”promptæ•æ„Ÿæ€§å¯èƒ½æ›´å¤šæ˜¯è¯„ä¼°çš„ä¼ªåƒï¼Œè€Œä¸æ˜¯æ¨¡å‹æœ¬èº«çš„ç¼ºé™·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç ”ç©¶æ™®éè®¤ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹promptéå¸¸æ•æ„Ÿï¼Œå³ä½¿ç”¨ä¸åŒçš„è¡¨è¾¾æ–¹å¼ï¼ˆä¾‹å¦‚ï¼ŒåŒä¹‰è¯æ›¿æ¢ã€å¥å­æ”¹å†™ï¼‰æ¥æè¿°åŒä¸€ä¸ªé—®é¢˜ï¼Œä¼šå¯¼è‡´LLMçš„æ€§èƒ½äº§ç”Ÿæ˜¾è‘—å·®å¼‚ã€‚è¿™ç§promptæ•æ„Ÿæ€§è¢«è®¤ä¸ºæ˜¯LLMçš„ä¸€ä¸ªå›ºæœ‰ç¼ºé™·ã€‚ç„¶è€Œï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•ï¼Œå¦‚å¯¹æ•°ä¼¼ç„¶è¯„åˆ†å’Œä¸¥æ ¼çš„ç­”æ¡ˆåŒ¹é…ï¼Œå¯èƒ½æ— æ³•å‡†ç¡®æ•æ‰è¯­ä¹‰ä¸Šçš„ç­‰ä»·æ€§ï¼Œä»è€Œå¤¸å¤§äº†promptæ•æ„Ÿæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨LLMæœ¬èº«ä½œä¸ºè£åˆ¤ï¼ˆLLM-as-a-Judgeï¼‰æ¥è¯„ä¼°LLMåœ¨ä¸åŒpromptä¸‹çš„è¡¨ç°ã€‚LLMä½œä¸ºè£åˆ¤èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è¯­ä¹‰çš„ç»†å¾®å·®åˆ«ï¼Œä»è€Œæ›´å‡†ç¡®åœ°åˆ¤æ–­ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œå‡å°‘å› è¡¨è¾¾æ–¹å¼ä¸åŒè€Œé€ æˆçš„è¯„ä¼°åå·®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æ›´å®¢è§‚åœ°è¯„ä¼°LLMå¯¹promptçš„çœŸå®é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1ï¼‰é€‰æ‹©å¤šä¸ªLLMï¼ˆå¦‚GPTå’ŒGeminiç³»åˆ—ï¼‰ä½œä¸ºè¯„ä¼°å¯¹è±¡ï¼›2ï¼‰é€‰å–å¤šä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼Œæ¶µç›–å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼ä»»åŠ¡ï¼›3ï¼‰è®¾è®¡å¤šä¸ªä¸åŒçš„promptæ¨¡æ¿ï¼Œç”¨äºæè¿°ç›¸åŒçš„é—®é¢˜ï¼›4ï¼‰ä½¿ç”¨ä¼ ç»Ÿçš„å¯å‘å¼è¯„ä¼°æ–¹æ³•ï¼ˆå¦‚å¯¹æ•°ä¼¼ç„¶è¯„åˆ†å’Œä¸¥æ ¼çš„ç­”æ¡ˆåŒ¹é…ï¼‰ä»¥åŠLLM-as-a-Judgeæ–¹æ³•æ¥è¯„ä¼°LLMåœ¨ä¸åŒpromptä¸‹çš„è¡¨ç°ï¼›5ï¼‰æ¯”è¾ƒä¸¤ç§è¯„ä¼°æ–¹æ³•ä¸‹LLMçš„æ€§èƒ½å·®å¼‚å’Œæ¨¡å‹æ’åçš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨LLM-as-a-Judgeæ–¹æ³•æ¥è¯„ä¼°LLMçš„promptæ•æ„Ÿæ€§ã€‚ä¸ä¼ ç»Ÿçš„å¯å‘å¼è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼ŒLLM-as-a-Judgeèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è¯­ä¹‰ï¼Œå‡å°‘å› è¡¨è¾¾æ–¹å¼ä¸åŒè€Œé€ æˆçš„è¯„ä¼°åå·®ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°LLMçš„çœŸå®é²æ£’æ€§ã€‚è¿™ç§æ–¹æ³•ä¸ºé‡æ–°å®¡è§†LLMçš„promptæ•æ„Ÿæ€§é—®é¢˜æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LLM-as-a-Judgeçš„å®ç°ä¸­ï¼Œå…³é”®åœ¨äºé€‰æ‹©åˆé€‚çš„LLMä½œä¸ºè£åˆ¤ï¼Œå¹¶è®¾è®¡åˆç†çš„promptæ¥å¼•å¯¼è£åˆ¤è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶ä¸­å¯èƒ½ä½¿ç”¨äº†ä¸åŒçš„LLMä½œä¸ºè£åˆ¤ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬è¯„ä¼°ç»“æœçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿è¯„ä¼°çš„å…¬å¹³æ€§ï¼Œå¯èƒ½éœ€è¦å¯¹è£åˆ¤LLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶æ›´å¥½åœ°ç†è§£è¯„ä¼°ä»»åŠ¡çš„è¦æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“ä½¿ç”¨LLM-as-a-Judgeè¯„ä¼°æ—¶ï¼ŒLLMåœ¨ä¸åŒpromptä¸‹çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—é™ä½ï¼Œæ¨¡å‹æ’åçš„ä¸€è‡´æ€§æ˜¾è‘—æé«˜ã€‚è¿™è¡¨æ˜ï¼Œç°ä»£LLMå¯¹promptæ¨¡æ¿çš„é²æ£’æ€§å¯èƒ½æ¯”ä»¥å‰è®¤ä¸ºçš„è¦å¼ºã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨LLM-as-a-Judgeåï¼Œæ€§èƒ½æ–¹å·®é™ä½äº†X%ï¼Œæ¨¡å‹æ’åç›¸å…³æ€§æé«˜äº†Y%ã€‚è¿™äº›ç»“æœæœ‰åŠ›åœ°æ”¯æŒäº†è®ºæ–‡çš„ç»“è®ºï¼špromptæ•æ„Ÿæ€§å¯èƒ½æ›´å¤šæ˜¯è¯„ä¼°çš„ä¼ªåƒï¼Œè€Œä¸æ˜¯æ¨¡å‹æœ¬èº«çš„ç¼ºé™·ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ›´å¯é åœ°è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒLLMçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è€ƒè™‘è¯­ä¹‰ç­‰ä»·æ€§çš„åœºæ™¯ä¸‹ã€‚é€šè¿‡ä½¿ç”¨LLM-as-a-Judgeæ–¹æ³•ï¼Œå¯ä»¥å‡å°‘å› prompté€‰æ‹©å¸¦æ¥çš„è¯„ä¼°åå·®ï¼Œä»è€Œæ›´å‡†ç¡®åœ°äº†è§£LLMçš„çœŸå®èƒ½åŠ›ã€‚è¿™æœ‰åŠ©äºå¼€å‘è€…æ›´å¥½åœ°ä¼˜åŒ–LLMï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›æ›´å¯é çš„LLMé€‰æ‹©ä¾æ®ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„è¯„ä¼°ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e., repeating something written or spoken using different words) leads to significant changes in large language model (LLM) performance, has been widely accepted as a core limitation of LLMs. In this work, we revisit this issue and ask: Is the widely reported high prompt sensitivity truly an inherent weakness of LLMs, or is it largely an artifact of evaluation processes? To answer this question, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family) across 6 benchmarks, including both multiple-choice and open-ended tasks on 12 diverse prompt templates. We find that much of the prompt sensitivity stems from heuristic evaluation methods, including log-likelihood scoring and rigid answer matching, which often overlook semantically correct responses expressed through alternative phrasings, such as synonyms or paraphrases. When we adopt LLM-as-a-Judge evaluations, we observe a substantial reduction in performance variance and a consistently higher correlation in model rankings across prompts. Our findings suggest that modern LLMs are more robust to prompt templates than previously believed, and that prompt sensitivity may be more an artifact of evaluation than a flaw in the models.

