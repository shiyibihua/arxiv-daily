---
layout: default
title: CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models
---

# CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01535" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01535v2</a>
  <a href="https://arxiv.org/pdf/2509.01535.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01535v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01535v2', 'CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kairong Han, Wenshuo Zhao, Ziyu Zhao, JunJian Ye, Lujia Pan, Kun Kuang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01 (æ›´æ–°: 2025-09-09)

**å¤‡æ³¨**: Accepted to EMNLP2025 Main conference

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Kairong-Han/CAT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå› æœæ³¨æ„åŠ›è°ƒæ•´ï¼ˆCATï¼‰æ–¹æ³•ï¼Œå°†ç»†ç²’åº¦å› æœçŸ¥è¯†æ³¨å…¥å¤§å‹è¯­è¨€æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å› æœæ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `åˆ†å¸ƒå¤–æ³›åŒ–` `å› æœçŸ¥è¯†æ³¨å…¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ©ç”¨å› æœçŸ¥è¯†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“æ•è·è™šå‡ç›¸å…³æ€§ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å¸ƒå¤–åœºæ™¯ã€‚
2. æå‡ºå› æœæ³¨æ„åŠ›è°ƒæ•´ï¼ˆCATï¼‰æ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ç”Ÿæˆtokençº§åˆ«çš„å› æœä¿¡å·ï¼Œå¹¶ä½¿ç”¨Re-Attentionæœºåˆ¶å¼•å¯¼æ¨¡å‹å…³æ³¨å› æœç»“æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCATæ–¹æ³•åœ¨Spurious Token GameåŸºå‡†å’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å¸ƒå¤–åœºæ™¯ä¸‹ï¼Œæ¨¡å‹æ€§èƒ½å¾—åˆ°å¤§å¹…æ”¹å–„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼Œä¸€ä¸ªæ ¹æœ¬é—®é¢˜ä»ç„¶å­˜åœ¨ï¼šLLMsèƒ½å¦æœ‰æ•ˆåœ°åˆ©ç”¨å› æœçŸ¥è¯†è¿›è¡Œé¢„æµ‹å’Œç”Ÿæˆï¼Ÿé€šè¿‡å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ç›´æ¥åœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„LLMsé€šå¸¸æ•è·è™šå‡ç›¸å…³æ€§ï¼Œè€Œä¸æ˜¯çœŸå®çš„å› æœå…³ç³»ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå³å› æœæ³¨æ„åŠ›è°ƒæ•´ï¼ˆCATï¼‰ï¼Œå°†ç»†ç²’åº¦çš„å› æœçŸ¥è¯†æ³¨å…¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æµç¨‹ï¼Œåˆ©ç”¨äººç±»å…ˆéªŒçŸ¥è¯†è‡ªåŠ¨ç”Ÿæˆtokençº§åˆ«çš„å› æœä¿¡å·ï¼Œå¹¶å¼•å…¥äº†Re-Attentionæœºåˆ¶æ¥æŒ‡å¯¼è®­ç»ƒï¼Œå¸®åŠ©æ¨¡å‹å…³æ³¨å› æœç»“æ„ï¼ŒåŒæ—¶å‡è½»æ³¨æ„åŠ›åˆ†æ•°ä¸­çš„å™ªå£°å’Œåå·®ã€‚åœ¨æˆ‘ä»¬æå‡ºçš„Spurious Token Gameï¼ˆSTGï¼‰åŸºå‡†å’Œå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†å› æœçŸ¥è¯†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶åœ¨OODåœºæ™¯ä¸­ä¿æŒäº†é²æ£’æ€§ã€‚CATåœ¨STGæ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†5.76%ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†1.56%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLlama-3.1-8Bæ¨¡å‹åœ¨STG_Mä¸Šçš„OODæ€§èƒ½ä»64.5%æé«˜åˆ°90.5%ï¼ŒQwenåœ¨STG_Hæ•°æ®é›†ä¸Šçš„OODæ€§èƒ½ä»25.4%æé«˜åˆ°55.9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ©ç”¨å› æœçŸ¥è¯†è¿›è¡Œé¢„æµ‹å’Œç”Ÿæˆæ—¶è¡¨ç°å‡ºçš„ä¸è¶³ã€‚ç°æœ‰LLMså®¹æ˜“å­¦ä¹ åˆ°æ•°æ®ä¸­çš„è™šå‡ç›¸å…³æ€§ï¼Œè€ŒéçœŸå®çš„å› æœå…³ç³»ï¼Œå¯¼è‡´æ¨¡å‹åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚è¿™ç§ç°è±¡é™åˆ¶äº†LLMsåœ¨éœ€è¦å¯é å› æœæ¨ç†çš„åº”ç”¨ä¸­çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç»†ç²’åº¦çš„å› æœçŸ¥è¯†æ³¨å…¥åˆ°LLMsçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹å…³æ³¨æ•°æ®ä¸­çœŸå®çš„å› æœç»“æ„ï¼Œå‡å°‘å¯¹è™šå‡ç›¸å…³æ€§çš„ä¾èµ–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ•°æ®èƒŒåçš„å› æœå…³ç³»ï¼Œæé«˜åœ¨OODåœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCATæ–¹æ³•åŒ…å«ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æµç¨‹ï¼Œç”¨äºç”Ÿæˆtokençº§åˆ«çš„å› æœä¿¡å·ã€‚è¯¥æµç¨‹åˆ©ç”¨äººç±»å…ˆéªŒçŸ¥è¯†ï¼Œä¾‹å¦‚é¢†åŸŸä¸“å®¶æä¾›çš„å› æœå…³ç³»å›¾ï¼Œæ¥è‡ªåŠ¨æ ‡æ³¨è®­ç»ƒæ•°æ®ä¸­æ¯ä¸ªtokençš„å› æœé‡è¦æ€§ã€‚ç„¶åï¼Œå¼•å…¥Re-Attentionæœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®tokençš„å› æœä¿¡å·è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹æ›´åŠ å…³æ³¨å› æœç›¸å…³çš„tokenã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡å¾®è°ƒLLMæ¥å®ç°ï¼Œç›®æ ‡æ˜¯ä½¿æ¨¡å‹åœ¨é¢„æµ‹å’Œç”Ÿæˆè¿‡ç¨‹ä¸­æ›´åŠ ä¾èµ–å› æœçŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šCATæ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿä»¥ç»†ç²’åº¦çš„æ–¹å¼å°†å› æœçŸ¥è¯†æ³¨å…¥åˆ°LLMsçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚ä¸ä»¥å¾€çš„æ–¹æ³•ç›¸æ¯”ï¼ŒCATä¸éœ€è¦æ‰‹åŠ¨è®¾è®¡å¤æ‚çš„å› æœæ¨ç†æ¨¡å—ï¼Œè€Œæ˜¯é€šè¿‡è‡ªåŠ¨åŒ–çš„æµç¨‹å’ŒRe-Attentionæœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ å’Œåˆ©ç”¨å› æœçŸ¥è¯†ã€‚è¿™ç§æ–¹æ³•æ›´åŠ çµæ´»å’Œé«˜æ•ˆï¼Œèƒ½å¤Ÿé€‚ç”¨äºå„ç§ä¸åŒçš„é¢†åŸŸå’Œä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šCATçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è‡ªåŠ¨åŒ–å› æœä¿¡å·ç”Ÿæˆæµç¨‹ï¼Œè¯¥æµç¨‹èƒ½å¤Ÿæ ¹æ®äººç±»å…ˆéªŒçŸ¥è¯†è‡ªåŠ¨æ ‡æ³¨è®­ç»ƒæ•°æ®ï¼›2) Re-Attentionæœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®tokençš„å› æœä¿¡å·è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œå…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼›3) å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¾®è°ƒLLMï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æ³¨å…¥çš„å› æœçŸ¥è¯†ã€‚è®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®è¯´æ˜å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚ï¼Œè¿™éƒ¨åˆ†ä¿¡æ¯æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCATæ–¹æ³•åœ¨Spurious Token Gameï¼ˆSTGï¼‰åŸºå‡†å’Œå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚åœ¨STGæ•°æ®é›†ä¸Šï¼ŒCATå¹³å‡æé«˜äº†5.76%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸‹ï¼ŒLlama-3.1-8Bæ¨¡å‹åœ¨STG_Mä¸Šçš„OODæ€§èƒ½ä»64.5%æé«˜åˆ°90.5%ï¼ŒQwenåœ¨STG_Hæ•°æ®é›†ä¸Šçš„OODæ€§èƒ½ä»25.4%æé«˜åˆ°55.9%ï¼Œè¡¨æ˜CATæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜LLMsåœ¨OODåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¯é å› æœæ¨ç†çš„é¢†åŸŸï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èé£é™©è¯„ä¼°ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æé«˜LLMså¯¹å› æœå…³ç³»çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶åœ¨è¿™äº›é¢†åŸŸåšå‡ºæ›´å‡†ç¡®ã€æ›´å¯é çš„å†³ç­–ï¼Œä»è€Œæå‡æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºæé«˜LLMsåœ¨é¢å¯¹æ•°æ®åå·®å’Œå¯¹æŠ—æ”»å‡»æ—¶çš„é²æ£’æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. The CAT achieves an average improvement of 5.76% on the STG dataset and 1.56% on downstream tasks. Notably, the OOD performance of the Llama-3.1-8B model on STG_M increased from 64.5% to 90.5%, and Qwen's OOD performance on the STG_H dataset improved from 25.4% to 55.9%. Implementation details can be found at https://github.com/Kairong-Han/CAT.

