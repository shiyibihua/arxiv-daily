---
layout: default
title: Can Large Language Models Master Complex Card Games?
---

# Can Large Language Models Master Complex Card Games?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01328" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01328v5</a>
  <a href="https://arxiv.org/pdf/2509.01328.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01328v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01328v5', 'Can Large Language Models Master Complex Card Games?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wei Wang, Fuqing Bie, Junzhe Chen, Dan Zhang, Shiyu Huang, Evgeny Kharlamov, Jie Tang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01 (æ›´æ–°: 2025-10-21)

**å¤‡æ³¨**: Accepted by NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/THUDM/LLM4CardGame)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢ç´¢LLMåœ¨å¤æ‚å¡ç‰Œæ¸¸æˆä¸­çš„èƒ½åŠ›ï¼šé€šè¿‡å¾®è°ƒå®ç°ç±»äººæ™ºèƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¡ç‰Œæ¸¸æˆ` `ç›‘ç£å¾®è°ƒ` `é€šç”¨èƒ½åŠ›` `æ¸¸æˆAI`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰AIåœ¨å¤æ‚æ¸¸æˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†é€šç”¨æ€§ä¸è¶³ï¼Œéš¾ä»¥åŒæ—¶æŒæ¡å¤šç§æ¸¸æˆã€‚
2. é€šè¿‡å¾®è°ƒLLMï¼Œä½¿å…¶å­¦ä¹ é«˜è´¨é‡æ¸¸æˆæ•°æ®ï¼Œæ¢ç´¢å…¶åœ¨å¤æ‚å¡ç‰Œæ¸¸æˆä¸­çš„è¡¨ç°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMèƒ½æ¥è¿‘ä¸“ä¸šAIæ°´å¹³ï¼Œä½†æŒæ¡å¤šæ¬¾æ¸¸æˆå’Œä¿æŒé€šç”¨æ€§å­˜åœ¨æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤æ‚æ¸¸æˆä¸€ç›´æ˜¯æµ‹è¯•äººå·¥æ™ºèƒ½ç®—æ³•è¿›å±•çš„é‡è¦åŸºå‡†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå¼•å‘äº†LLMæ˜¯å¦èƒ½åœ¨å¤æ‚æ¸¸æˆä¸­å–å¾—ç±»ä¼¼æˆåŠŸçš„ç–‘é—®ã€‚æœ¬æ–‡æ¢è®¨äº†LLMåœ¨æŒæ¡å¤æ‚å¡ç‰Œæ¸¸æˆæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†LLMåœ¨å…«ç§ä¸åŒå¡ç‰Œæ¸¸æˆä¸­çš„å­¦ä¹ èƒ½åŠ›ï¼Œè¯„ä¼°äº†åœ¨é«˜è´¨é‡æ¸¸æˆæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„å½±å“ï¼Œå¹¶è€ƒå¯Ÿäº†æ¨¡å‹åœ¨æŒæ¡è¿™äº›æ¸¸æˆçš„åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼šï¼ˆ1ï¼‰LLMå¯ä»¥é€šè¿‡åœ¨é«˜è´¨é‡æ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒæ¥æ¥è¿‘å¼ºå¤§çš„æ¸¸æˆAIçš„æ€§èƒ½ï¼›ï¼ˆ2ï¼‰LLMå¯ä»¥åŒæ—¶åœ¨å¤šä¸ªå¤æ‚å¡ç‰Œæ¸¸æˆä¸­è¾¾åˆ°ä¸€å®šçš„ç†Ÿç»ƒç¨‹åº¦ï¼Œè§„åˆ™ç›¸ä¼¼çš„æ¸¸æˆæ€§èƒ½å¢å¼ºï¼Œè§„åˆ™ä¸åŒçš„æ¸¸æˆæ€§èƒ½å†²çªï¼›ï¼ˆ3ï¼‰LLMåœ¨æŒæ¡å¤æ‚æ¸¸æˆæ—¶ä¼šé™ä½é€šç”¨èƒ½åŠ›ï¼Œä½†å¯ä»¥é€šè¿‡æ•´åˆä¸€å®šæ•°é‡çš„é€šç”¨æŒ‡ä»¤æ•°æ®æ¥ç¼“è§£è¿™ç§ä¸‹é™ã€‚è¯„ä¼°ç»“æœè¡¨æ˜LLMå…·æœ‰å¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›å’Œé€šç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å¡ç‰Œæ¸¸æˆä¸­çš„å­¦ä¹ èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚AlphaGoç­‰ï¼Œè™½ç„¶åœ¨ç‰¹å®šæ¸¸æˆä¸­è¡¨ç°å“è¶Šï¼Œä½†ç¼ºä¹é€šç”¨æ€§ï¼Œéš¾ä»¥åŒæ—¶æŒæ¡å¤šç§æ¸¸æˆã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç‰¹å®šæ¸¸æˆè®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¼šç‰ºç‰²å…¶é€šç”¨èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸‹é™ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©LLMåœ¨æŒæ¡å¤æ‚æ¸¸æˆçš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æå‡å…¶é€šç”¨èƒ½åŠ›ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuning, SFTï¼‰çš„æ–¹æ³•ï¼Œè®©LLMå­¦ä¹ é«˜è´¨é‡çš„æ¸¸æˆæ•°æ®ï¼Œä»è€Œæå‡å…¶åœ¨ç‰¹å®šå¡ç‰Œæ¸¸æˆä¸­çš„è¡¨ç°ã€‚åŒæ—¶ï¼Œé€šè¿‡æ··åˆæ¸¸æˆæ•°æ®å’Œé€šç”¨æŒ‡ä»¤æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥ç¼“è§£æ¨¡å‹åœ¨æŒæ¡ç‰¹å®šæ¸¸æˆåé€šç”¨èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æ¢ç´¢LLMåœ¨å¤æ‚æ¸¸æˆä¸­çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶æ‰¾åˆ°ä¸€ç§å¹³è¡¡ç‰¹å®šæ¸¸æˆæ€§èƒ½å’Œé€šç”¨èƒ½åŠ›çš„æ–¹æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®æ”¶é›†ä¸å¤„ç†ã€æ¨¡å‹å¾®è°ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ”¶é›†é«˜è´¨é‡çš„å¡ç‰Œæ¸¸æˆæ•°æ®ï¼ŒåŒ…æ‹¬æ¸¸æˆçŠ¶æ€ã€ç©å®¶è¡ŒåŠ¨ç­‰ä¿¡æ¯ã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›æ•°æ®å¯¹LLMè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä½¿å…¶å­¦ä¹ æ¸¸æˆç­–ç•¥ã€‚æœ€åï¼Œé€šè¿‡ä¸å…¶ä»–æ¸¸æˆAIæˆ–äººç±»ç©å®¶å¯¹æˆ˜ï¼Œä»¥åŠåœ¨é€šç”¨ä»»åŠ¡ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¯„ä¼°LLMåœ¨ç‰¹å®šæ¸¸æˆå’Œé€šç”¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æ¡†æ¶çš„å…³é”®æ¨¡å—åŒ…æ‹¬æ•°æ®é¢„å¤„ç†æ¨¡å—ã€æ¨¡å‹å¾®è°ƒæ¨¡å—å’Œæ€§èƒ½è¯„ä¼°æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†LLMåœ¨å¤šä¸ªå¤æ‚å¡ç‰Œæ¸¸æˆä¸­çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶æ¢ç´¢äº†å¦‚ä½•é€šè¿‡æ··åˆæ¸¸æˆæ•°æ®å’Œé€šç”¨æŒ‡ä»¤æ•°æ®æ¥ç¼“è§£æ¨¡å‹åœ¨æŒæ¡ç‰¹å®šæ¸¸æˆåé€šç”¨èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ·±å…¥åˆ†æäº†ä¸åŒæ¸¸æˆè§„åˆ™å¯¹LLMå­¦ä¹ çš„å½±å“ï¼Œä»¥åŠLLMåœ¨ä¸åŒæ¸¸æˆä¹‹é—´çš„çŸ¥è¯†è¿ç§»èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©äº†å…«ç§ä¸åŒçš„å¡ç‰Œæ¸¸æˆï¼Œä»¥è¯„ä¼°LLMåœ¨ä¸åŒæ¸¸æˆè§„åˆ™ä¸‹çš„å­¦ä¹ èƒ½åŠ›ï¼›2) ä½¿ç”¨é«˜è´¨é‡çš„æ¸¸æˆæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä»¥æå‡LLMåœ¨ç‰¹å®šæ¸¸æˆä¸­çš„è¡¨ç°ï¼›3) é€šè¿‡æ··åˆæ¸¸æˆæ•°æ®å’Œé€šç”¨æŒ‡ä»¤æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥ç¼“è§£æ¨¡å‹é€šç”¨èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ï¼›4) è®¾è®¡äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ¸¸æˆèƒœç‡ã€é€šç”¨ä»»åŠ¡å‡†ç¡®ç‡ç­‰ï¼Œä»¥å…¨é¢è¯„ä¼°LLMçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ŒLLMå¯ä»¥æ¥è¿‘ç”šè‡³è¾¾åˆ°ä¸“ä¸šæ¸¸æˆAIçš„æ°´å¹³ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›å¡ç‰Œæ¸¸æˆä¸­ï¼ŒLLMçš„èƒœç‡å¯ä»¥è¾¾åˆ°70%ä»¥ä¸Šã€‚åŒæ—¶ï¼Œç ”ç©¶å‘ç°ï¼Œæ··åˆæ¸¸æˆæ•°æ®å’Œé€šç”¨æŒ‡ä»¤æ•°æ®å¯ä»¥æœ‰æ•ˆç¼“è§£LLMåœ¨æŒæ¡ç‰¹å®šæ¸¸æˆåé€šç”¨èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ï¼Œé€šç”¨ä»»åŠ¡å‡†ç¡®ç‡ä¸‹é™å¹…åº¦æ§åˆ¶åœ¨5%ä»¥å†…ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ¸¸æˆAIå¼€å‘ã€æ™ºèƒ½å†³ç­–ç³»ç»Ÿå’Œé€šç”¨äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚é€šè¿‡è®­ç»ƒLLMæŒæ¡å¤æ‚æ¸¸æˆï¼Œå¯ä»¥å¼€å‘å‡ºæ›´æ™ºèƒ½ã€æ›´å…·é€‚åº”æ€§çš„æ¸¸æˆAIï¼Œæå‡æ¸¸æˆä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶çš„æ–¹æ³•ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–å¤æ‚å†³ç­–é—®é¢˜ï¼Œå¦‚é‡‘èæŠ•èµ„ã€èµ„æºè°ƒåº¦ç­‰ï¼Œä¸ºæ™ºèƒ½å†³ç­–ç³»ç»Ÿæä¾›æ–°çš„æ€è·¯ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œä½¿AIèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œè§£å†³ç°å®ä¸–ç•Œä¸­çš„å¤æ‚é—®é¢˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Complex games have long been an important benchmark for testing the progress of artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have defeated top human players in Go and Chess, garnering widespread societal attention towards artificial intelligence. Concurrently, large language models (LLMs) have exhibited remarkable capabilities across various tasks, raising the question of whether LLMs can achieve similar success in complex games. In this paper, we explore the potential of LLMs in mastering complex card games. We systematically assess the learning capabilities of LLMs across eight diverse card games, evaluating the impact of fine-tuning on high-quality gameplay data, and examining the models' ability to retain general capabilities while mastering these games. Our findings indicate that: (1) LLMs can approach the performance of strong game AIs through supervised fine-tuning on high-quality data, (2) LLMs can achieve a certain level of proficiency in multiple complex card games simultaneously, with performance augmentation for games with similar rules and conflicts for dissimilar ones, and (3) LLMs experience a decline in general capabilities when mastering complex games, but this decline can be mitigated by integrating a certain amount of general instruction data. The evaluation results demonstrate strong learning ability and versatility of LLMs. The code is available at https://github.com/THUDM/LLM4CardGame

