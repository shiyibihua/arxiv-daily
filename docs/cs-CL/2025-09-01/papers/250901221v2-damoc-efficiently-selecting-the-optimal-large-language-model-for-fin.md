---
layout: default
title: DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Tasks Based on Data and Model Compression
---

# DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Tasks Based on Data and Model Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01221" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01221v2</a>
  <a href="https://arxiv.org/pdf/2509.01221.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01221v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01221v2', 'DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Tasks Based on Data and Model Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wei Huang, Huang Wei, Yinggui Wang

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01 (æ›´æ–°: 2025-09-04)

**å¤‡æ³¨**: Accepted by EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DaMoCï¼šåŸºäºæ•°æ®å’Œæ¨¡å‹å‹ç¼©é«˜æ•ˆé€‰æ‹©é¢†åŸŸä»»åŠ¡å¾®è°ƒçš„æœ€ä½³å¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å¾®è°ƒ` `æ•°æ®å‹ç¼©` `æ¨¡å‹å‹ç¼©` `é¢†åŸŸçŸ¥è¯†` `æ¨¡å‹é€‰æ‹©` `çŸ¥è¯†é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥å¿«é€Ÿæœ‰æ•ˆåœ°ä¸ºç‰¹å®šé¢†åŸŸä»»åŠ¡é€‰æ‹©æœ€ä½³çš„å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé¢ä¸´ç€è®¡ç®—èµ„æºå’Œæ—¶é—´æˆæœ¬çš„æŒ‘æˆ˜ã€‚
2. DaMoCæ¡†æ¶é€šè¿‡æ•°æ®å±‚é¢çš„å‹ç¼©å’Œä¼˜åŒ–ä»¥åŠæ¨¡å‹å±‚é¢çš„å‰ªæå’Œåˆå¹¶ï¼Œæ—¨åœ¨åŠ é€Ÿæœ€ä½³LLMçš„è¯†åˆ«è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDaMoCèƒ½å¤Ÿåœ¨èŠ‚çœçº¦20å€è®­ç»ƒæ—¶é—´çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°é€‰æ‹©å‡ºé€‚åˆç‰¹å®šä»»åŠ¡çš„æœ€ä½³LLMã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šç”¨ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œéœ€è¦ä½¿ç”¨ç‰¹å®šæ•°æ®è¿›è¡Œå¾®è°ƒã€‚ç”±äºæœ‰è®¸å¤šå¼€æºLLMå¯ç”¨ï¼Œå› æ­¤é€‰æ‹©æœ€é€‚åˆå¾®è°ƒä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦é›†ä¸­åœ¨å¦‚ä½•å¿«é€Ÿè¯†åˆ«æœ€ä½³LLMã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ•°æ®å’Œæ¨¡å‹å‹ç¼©æ¡†æ¶ï¼ˆDaMoCï¼‰ï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼š1ï¼‰æ•°æ®å±‚é¢ï¼šé¦–å…ˆç³»ç»Ÿåœ°å¯¹LLMçš„æ•°æ®è¿‡æ»¤æ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼Œå°†å…¶åˆ†ä¸ºä¸‰ç§ä¸åŒçš„èŒƒå¼ï¼šï¼ˆ1ï¼‰åˆ†å¸ƒæ„ŸçŸ¥æ–¹æ³•ï¼Œï¼ˆ2ï¼‰è´¨é‡æ„ŸçŸ¥æ–¹æ³•ï¼Œä»¥åŠï¼ˆ3ï¼‰åŒæ—¶è€ƒè™‘è¿™ä¸¤ä¸ªç»´åº¦çš„æ··åˆæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æé«˜äº†æ–‡æœ¬ä¸­å…³é”®tokençš„å¯†åº¦ï¼Œå®ç°äº†tokenå‹ç¼©ã€‚éšåï¼Œæˆ‘ä»¬ä½¿ç”¨LLMè¿­ä»£åœ°é‡å†™æ–‡æœ¬ï¼Œä»¥ä¼˜åŒ–å…¶è¡¨è¾¾ã€‚2ï¼‰æ¨¡å‹å±‚é¢ï¼šæˆ‘ä»¬ä½¿ç”¨å±‚ç›¸ä¼¼åº¦åˆ†æ•°æ¥è¯„ä¼°æ¯ä¸€å±‚çš„é‡è¦æ€§ï¼Œå¹¶åˆ é™¤é‡è¦æ€§è¾ƒä½çš„å±‚ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¨€ç–åˆå¹¶èŒƒå¼ï¼Œä»¥å°½å¯èƒ½ä¿ç•™åŸå§‹æ¨¡å‹çš„èƒ½åŠ›ã€‚åœ¨åŒ»ç–—é—®ç­”ã€é‡‘èé—®ç­”ã€é€šç”¨é—®ç­”å’Œé˜…è¯»ç†è§£å››ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©æœ€ä½³LLMï¼ŒåŒæ—¶èŠ‚çœå¤§çº¦20å€çš„è®­ç»ƒæ—¶é—´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä¸ºç‰¹å®šé¢†åŸŸä»»åŠ¡é€‰æ‹©æœ€ä½³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¯¹å¤šä¸ªLLMè¿›è¡Œå®Œæ•´çš„å¾®è°ƒå®éªŒï¼Œè€—è´¹å¤§é‡æ—¶é—´å’Œè®¡ç®—èµ„æºï¼Œæ•ˆç‡ä½ä¸‹ã€‚ç—›ç‚¹åœ¨äºç¼ºä¹ä¸€ç§å¿«é€Ÿæœ‰æ•ˆçš„æ–¹æ³•æ¥è¯„ä¼°ä¸åŒLLMåœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸Šçš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDaMoCçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ•°æ®å’Œæ¨¡å‹å‹ç¼©ï¼Œå‡å°‘å¾®è°ƒæ‰€éœ€çš„è®¡ç®—é‡ï¼Œä»è€ŒåŠ é€Ÿæœ€ä½³LLMçš„è¯†åˆ«è¿‡ç¨‹ã€‚æ•°æ®å‹ç¼©æ—¨åœ¨ä¿ç•™å…³é”®ä¿¡æ¯å¹¶å‡å°‘å†—ä½™ï¼Œæ¨¡å‹å‹ç¼©åˆ™é€šè¿‡å‰ªæå’Œåˆå¹¶å‡å°‘æ¨¡å‹å‚æ•°é‡ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDaMoCæ¡†æ¶åŒ…å«æ•°æ®å±‚é¢å’Œæ¨¡å‹å±‚é¢ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨æ•°æ®å±‚é¢ï¼Œé¦–å…ˆå¯¹æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼Œé‡‡ç”¨åˆ†å¸ƒæ„ŸçŸ¥ã€è´¨é‡æ„ŸçŸ¥æˆ–æ··åˆæ–¹æ³•å»é™¤å™ªå£°æ•°æ®ã€‚ç„¶åï¼Œé€šè¿‡tokenå‹ç¼©æé«˜å…³é”®tokenå¯†åº¦ï¼Œå¹¶ä½¿ç”¨LLMè¿­ä»£é‡å†™æ–‡æœ¬ä»¥ä¼˜åŒ–è¡¨è¾¾ã€‚åœ¨æ¨¡å‹å±‚é¢ï¼Œä½¿ç”¨å±‚ç›¸ä¼¼åº¦è¯„ä¼°æ¯ä¸€å±‚çš„é‡è¦æ€§ï¼Œå‰ªæä¸é‡è¦çš„å±‚ï¼Œå¹¶é‡‡ç”¨ç¨€ç–åˆå¹¶èŒƒå¼ä¿ç•™åŸå§‹æ¨¡å‹èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šDaMoCçš„å…³é”®åˆ›æ–°åœ¨äºç»“åˆäº†æ•°æ®å’Œæ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œå¹¶é’ˆå¯¹LLMå¾®è°ƒçš„ç‰¹ç‚¹è¿›è¡Œäº†ä¼˜åŒ–ã€‚æ•°æ®å±‚é¢çš„tokenå‹ç¼©å’ŒLLMé‡å†™ä»¥åŠæ¨¡å‹å±‚é¢çš„ç¨€ç–åˆå¹¶èŒƒå¼ï¼Œéƒ½æ˜¯é’ˆå¯¹LLMå¾®è°ƒä»»åŠ¡çš„åˆ›æ–°è®¾è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®è¿‡æ»¤æ–¹æ³•åŒ…æ‹¬åˆ†å¸ƒæ„ŸçŸ¥ï¼ˆä¾‹å¦‚åŸºäºå›°æƒ‘åº¦è¿‡æ»¤ï¼‰ã€è´¨é‡æ„ŸçŸ¥ï¼ˆä¾‹å¦‚åŸºäºè§„åˆ™æˆ–æ¨¡å‹é¢„æµ‹è´¨é‡è¿‡æ»¤ï¼‰å’Œæ··åˆæ–¹æ³•ã€‚Tokenå‹ç¼©çš„å…·ä½“å®ç°æœªçŸ¥ã€‚æ¨¡å‹å±‚é¢çš„å±‚ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•æœªçŸ¥ï¼Œç¨€ç–åˆå¹¶èŒƒå¼çš„å…·ä½“å®ç°ä¹ŸæœªçŸ¥ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„æ²¿ç”¨åŸå§‹LLMçš„è®¾ç½®ï¼Œæœªåšä¿®æ”¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDaMoCæ¡†æ¶èƒ½å¤Ÿåœ¨åŒ»ç–—é—®ç­”ã€é‡‘èé—®ç­”ã€é€šç”¨é—®ç­”å’Œé˜…è¯»ç†è§£å››ä¸ªæ•°æ®é›†ä¸Šï¼Œä»¥å¤§çº¦20å€çš„è®­ç»ƒæ—¶é—´èŠ‚çœï¼Œæœ‰æ•ˆåœ°é€‰æ‹©å‡ºé€‚åˆç‰¹å®šä»»åŠ¡çš„æœ€ä½³LLMã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†æ—¶é—´èŠ‚çœçš„å¹…åº¦éå¸¸æ˜¾è‘—ï¼Œè¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦é¢†åŸŸçŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒåœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—ã€é‡‘èã€æ³•å¾‹ç­‰ã€‚é€šè¿‡DaMoCæ¡†æ¶ï¼Œå¯ä»¥æ˜¾è‘—é™ä½é€‰æ‹©æœ€ä½³LLMçš„æˆæœ¬ï¼ŒåŠ é€Ÿé¢†åŸŸæ¨¡å‹çš„å¼€å‘å’Œéƒ¨ç½²ï¼Œæå‡ç‰¹å®šé¢†åŸŸä»»åŠ¡çš„æ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–æ¨¡å‹å‹ç¼©å’ŒåŠ é€ŸæŠ€æœ¯ï¼Œå¹¶åº”ç”¨äºæ›´å¤šé¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) excel in general tasks but struggle with domain-specific ones, requiring fine-tuning with specific data. With many open-source LLMs available, selecting the best model for fine-tuning downstream tasks is challenging, primarily focusing on how to quickly identify the optimal LLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses this challenge by: 1) Data Level: A systematic categorization of data filtering methodologies for LLMs is first established, classifying them into three distinct paradigms: (1) distribution-aware methods, (2) quality-aware methods, and (3) hybrid approaches considering both dimensions. Further, we enhance the density of key tokens in the text achieving token compression. Subsequently, we use an LLM to iterative rewrite the text to optimize its expression. 2) Model Level: We use layer similarity scores to assess each layer's importance and remove those with lower importance. Then, we introduce a sparse merging paradigm to preserve as much of the original model's capability as possible. Extensive experiments on four datasets, medical Q&A, financial Q&A, general Q&A, and reading comprehension, show that we can select the optimal LLM while saving approximately 20-fold in training time.

