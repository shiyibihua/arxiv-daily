---
layout: default
title: Mitigating Catastrophic Forgetting in Continual Learning through Model Growth
---

# Mitigating Catastrophic Forgetting in Continual Learning through Model Growth

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01213" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01213v1</a>
  <a href="https://arxiv.org/pdf/2509.01213.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01213v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01213v1', 'Mitigating Catastrophic Forgetting in Continual Learning through Model Growth')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ege SÃ¼alp, Mina Rezaei

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ¨¡å‹å¢é•¿ç¼“è§£æŒç»­å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `æ¨¡å‹å¢é•¿` `å¤§å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†ä¿ç•™`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŒç»­å­¦ä¹ ä¸­é¢ä¸´ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå³åœ¨æ–°ä»»åŠ¡ä¸Šå¾®è°ƒåï¼Œæ¨¡å‹ä¼šä¸¢å¤±å…ˆå‰å­¦ä¹ çš„çŸ¥è¯†ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨æ¨¡å‹å¢é•¿ç­–ç•¥ï¼Œé€šè¿‡ä»å°æ¨¡å‹åˆ°å¤§æ¨¡å‹çš„è®­ç»ƒæ–¹å¼ï¼ŒæœŸæœ›ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œæå‡æ¨¡å‹å¯¹å…ˆå‰çŸ¥è¯†çš„ä¿ç•™èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¢é•¿çš„æ¨¡å‹åœ¨é˜…è¯»ç†è§£æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„çŸ¥è¯†ä¿ç•™èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†ç¤¾ä¼šåè§æ–¹é¢å­˜åœ¨æƒè¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¾éš¾æ€§é—å¿˜æ˜¯æŒç»­å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œæ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ä¼šä¸¢å¤±å…ˆå‰çš„çŸ¥è¯†ã€‚è¿™ä¸ªé—®é¢˜å¯¹äºè¿›è¡ŒæŒç»­å­¦ä¹ çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°¤ä¸ºå…³é”®ï¼Œå› ä¸ºä¿æŒè·¨å¤šä¸ªé¢†åŸŸçš„èƒ½åŠ›å¯¹äºå…¶é€šç”¨æ€§è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¢è®¨äº†æ¨¡å‹å¢é•¿ï¼Œè¿™æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„ç­–ç•¥ï¼Œå®ƒåˆ©ç”¨è¾ƒå°çš„æ¨¡å‹æ¥åŠ é€Ÿå’Œæ„å»ºè¾ƒå¤§æ¨¡å‹çš„è®­ç»ƒï¼Œä»¥ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚è™½ç„¶åŸºäºå¢é•¿çš„é¢„è®­ç»ƒï¼Œç‰¹åˆ«æ˜¯é€šè¿‡Transformerå †å ï¼Œå·²æ˜¾ç¤ºå‡ºåŠ é€Ÿæ”¶æ•›çš„æ½œåŠ›ï¼Œä½†å…¶å¯¹é—å¿˜çš„å½±å“ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯„ä¼°äº†åŸºäºå¢é•¿çš„æ¨¡å‹ï¼ˆStack LLMï¼‰å’ŒæœªåŸºäºå¢é•¿çš„æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦èƒ½æ›´æœ‰æ•ˆåœ°ä¿ç•™å…ˆå‰å­¦ä¹ çš„èƒ½åŠ›ï¼Œè·¨è¶Šä¸€ç³»åˆ—æ¶‰åŠé¢†åŸŸçŸ¥è¯†ã€æ¨ç†ã€é˜…è¯»ç†è§£å’Œåå·®çš„å¾®è°ƒä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸¤ç§æ¨¡å‹åœ¨é¢†åŸŸçŸ¥è¯†æ–¹é¢éƒ½æœ‰æ‰€æé«˜ã€‚ç„¶è€Œï¼Œæ¨ç†å’Œé˜…è¯»ç†è§£éšç€æ—¶é—´çš„æ¨ç§»è€Œé€€åŒ–ï¼Œè¡¨æ˜å­˜åœ¨ç¾éš¾æ€§é—å¿˜çš„è¿¹è±¡ã€‚Stack LLMå§‹ç»ˆè¡¨ç°å‡ºè¾ƒå°çš„é€€åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨é˜…è¯»ç†è§£æ–¹é¢ï¼Œè¡¨æ˜å…¶å…·æœ‰æ›´å¼ºçš„ä¿ç•™èƒ½åŠ›ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨åå·®è¯„ä¼°ä¸­ï¼ŒåŸºçº¿LLMéšç€æŒç»­å¾®è°ƒè€Œé€æ¸å˜å¾—æ›´åŠ ä¸­æ€§ï¼Œè€ŒStack LLMå°†åå·®ç‡ç¨³å®šåœ¨60-61ï¼…å·¦å³ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŸºäºå¢é•¿çš„é¢„è®­ç»ƒå¯èƒ½åœ¨æŠµæŠ—ç¾éš¾æ€§é—å¿˜æ–¹é¢å¸¦æ¥é€‚åº¦çš„æ”¹è¿›ï¼Œä½†åœ¨å¤„ç†ç¤¾ä¼šåè§æ–¹é¢ä»ç„¶å­˜åœ¨æƒè¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŒç»­å­¦ä¹ è¿‡ç¨‹ä¸­é‡åˆ°çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åº”å¯¹æ–°ä»»åŠ¡æ—¶ï¼Œå¾€å¾€ä¼šæ˜¾è‘—é™ä½æ¨¡å‹åœ¨å…ˆå‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¯¼è‡´çŸ¥è¯†çš„ä¸¢å¤±ã€‚è¿™ç§ç°è±¡ä¸¥é‡é™åˆ¶äº†LLMåœ¨éœ€è¦æŒç»­å­¦ä¹ å’Œé€‚åº”æ–°çŸ¥è¯†çš„åº”ç”¨åœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡å‹å¢é•¿ï¼ˆModel Growthï¼‰ç­–ç•¥ã€‚é€šè¿‡ä»å°æ¨¡å‹å¼€å§‹è®­ç»ƒï¼Œé€æ­¥æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹ï¼ŒæœŸæœ›èƒ½å¤Ÿæ›´å¥½åœ°ç»„ç»‡å’Œç»“æ„åŒ–çŸ¥è¯†çš„å­¦ä¹ è¿‡ç¨‹ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹å…ˆå‰çŸ¥è¯†çš„ä¿ç•™èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•å€Ÿé‰´äº†Transformerå †å ç­‰å·²æœ‰çš„å¢é•¿å¼é¢„è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºæŒç»­å­¦ä¹ åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§å¯¹æ¯”å®éªŒæ¡†æ¶ï¼Œæ¯”è¾ƒäº†ä¸¤ç§æ¨¡å‹çš„æ€§èƒ½ï¼šä¸€ç§æ˜¯åŸºäºå¢é•¿è®­ç»ƒçš„æ¨¡å‹ï¼ˆStack LLMï¼‰ï¼Œå¦ä¸€ç§æ˜¯ç›´æ¥è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ä¸¤ç§æ¨¡å‹éƒ½ç»è¿‡ä¸€ç³»åˆ—å¾®è°ƒä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡æ¶µç›–äº†é¢†åŸŸçŸ¥è¯†ã€æ¨ç†ã€é˜…è¯»ç†è§£å’Œåå·®è¯„ä¼°ç­‰å¤šä¸ªæ–¹é¢ã€‚é€šè¿‡æ¯”è¾ƒä¸¤ç§æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½å˜åŒ–ï¼Œè¯„ä¼°æ¨¡å‹å¢é•¿ç­–ç•¥å¯¹ç¼“è§£ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ¨¡å‹å¢é•¿ç­–ç•¥åº”ç”¨äºæŒç»­å­¦ä¹ çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£è¯¥é—®é¢˜çš„æœ‰æ•ˆæ€§ã€‚è™½ç„¶å¢é•¿å¼é¢„è®­ç»ƒå·²è¢«ç”¨äºåŠ é€Ÿæ¨¡å‹æ”¶æ•›ï¼Œä½†å…¶å¯¹æŒç»­å­¦ä¹ ä¸­çŸ¥è¯†ä¿ç•™çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚è¯¥ç ”ç©¶å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œå¹¶ä¸ºæŒç»­å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©äº†æ¶µç›–ä¸åŒæ–¹é¢çš„å¾®è°ƒä»»åŠ¡ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„çŸ¥è¯†ä¿ç•™èƒ½åŠ›ï¼›2) å¯¹æ¯”äº†åŸºäºå¢é•¿è®­ç»ƒçš„æ¨¡å‹å’Œç›´æ¥è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥çªå‡ºæ¨¡å‹å¢é•¿ç­–ç•¥çš„æ•ˆæœï¼›3) å…³æ³¨äº†æ¨¡å‹åœ¨ç¤¾ä¼šåè§æ–¹é¢çš„è¡¨ç°ï¼Œæ­ç¤ºäº†æ¨¡å‹å¢é•¿ç­–ç•¥å¯èƒ½å¸¦æ¥çš„æ½œåœ¨é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¢é•¿è®­ç»ƒçš„æ¨¡å‹ï¼ˆStack LLMï¼‰åœ¨é˜…è¯»ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„çŸ¥è¯†ä¿ç•™èƒ½åŠ›ï¼Œç›¸æ¯”äºç›´æ¥è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå…¶æ€§èƒ½é€€åŒ–ç¨‹åº¦æ›´å°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼ŒStack LLMåœ¨ç¤¾ä¼šåè§æ–¹é¢è¡¨ç°å‡ºä¸LLMä¸åŒçš„è¡Œä¸ºï¼Œç»´æŒäº†ç›¸å¯¹ç¨³å®šçš„åå·®ç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦æŒç»­å­¦ä¹ å’Œé€‚åº”æ–°çŸ¥è¯†çš„å„ç§å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€çŸ¥è¯†é—®ç­”ç³»ç»Ÿã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡ç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨ä¸æ–­å˜åŒ–çš„ç¯å¢ƒä¸­çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æœåŠ¡äºç”¨æˆ·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Catastrophic forgetting is a significant challenge in continual learning, in which a model loses prior knowledge when it is fine-tuned on new tasks. This problem is particularly critical for large language models (LLMs) undergoing continual learning, as retaining performance across diverse domains is important for their general utility. In this paper, we explore model growth, a promising strategy that leverages smaller models to expedite and structure the training of larger ones for mitigating the catastrophic forgetting problem. Although growth-based pretraining, particularly via transformer stacking, has shown promise in accelerating convergence, its impact on forgetting remains under-explored. Therefore, we evaluate whether growth-based models can retain previously learned capabilities more effectively across a sequence of fine-tuning tasks involving domain knowledge, reasoning, reading comprehension, and bias. Our findings show that both models -- one trained with growth (Stack LLM) and one without (LLM) -- exhibit improvements in domain knowledge. However, reasoning and reading comprehension degrade over time, indicating signs of catastrophic forgetting. Stack LLM consistently shows less degradation, especially in reading comprehension, suggesting enhanced retention capabilities. Interestingly, in bias evaluation, the baseline LLM becomes progressively more neutral with continued fine-tuning, while Stack LLM maintains a steady bias ratio around 60--61\%. These results indicate that growth-based pretraining may deliver modest improvements in resisting catastrophic forgetting, though trade-offs remain in handling social biases.

