---
layout: default
title: Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning
---

# Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01166" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01166v1</a>
  <a href="https://arxiv.org/pdf/2509.01166.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01166v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01166v1', 'Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Liu, Yanan Cao, Xixun Lin, Yanmin Shang, Shi Wang, Shirui Pan

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01

**å¤‡æ³¨**: EMNLP 2025, Main, Long Paper

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSATæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„æ„ŸçŸ¥å¯¹é½å¾®è°ƒå¢å¼ºLLMçš„çŸ¥è¯†å›¾è°±è¡¥å…¨èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†å›¾è°±è¡¥å…¨` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç»“æ„æ„ŸçŸ¥å¯¹é½` `å¯¹æ¯”å­¦ä¹ ` `æŒ‡ä»¤å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå¢å¼ºçš„çŸ¥è¯†å›¾è°±è¡¥å…¨æ–¹æ³•å¿½ç•¥äº†è‡ªç„¶è¯­è¨€å’Œå›¾ç»“æ„è¡¨ç¤ºç©ºé—´çš„ä¸ä¸€è‡´æ€§ï¼Œä¸”ä»»åŠ¡æŒ‡ä»¤è®¾è®¡å†—ä½™ã€‚
2. SATæ¡†æ¶é€šè¿‡åˆ†å±‚çŸ¥è¯†å¯¹é½å’Œç»“æ„æŒ‡ä»¤å¾®è°ƒï¼Œå°†å›¾åµŒå…¥ä¸è‡ªç„¶è¯­è¨€ç©ºé—´å¯¹é½ï¼Œå¹¶ç»Ÿä¸€ä¸åŒKGCä»»åŠ¡çš„æŒ‡ä»¤ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSATåœ¨é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½æå‡èŒƒå›´ä¸º8.7%åˆ°29.8%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†å›¾è°±è¡¥å…¨(KGC)æ—¨åœ¨ä»çŸ¥è¯†å›¾è°±ä¸­æ¨æ–­æ–°çŸ¥è¯†å¹¶è¿›è¡Œé¢„æµ‹ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹(LLM)å±•ç°å‡ºäº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚LLMå¢å¼ºçš„KGCæ–¹æ³•ä¸»è¦ä¾§é‡äºè®¾è®¡ç‰¹å®šäºä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶å–å¾—äº†å¯å–œçš„è¿›å±•ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å¿½ç•¥äº†è‡ªç„¶è¯­è¨€å’Œå›¾ç»“æ„ä¹‹é—´ä¸ä¸€è‡´çš„è¡¨ç¤ºç©ºé—´ã€‚å…¶æ¬¡ï¼Œå¤§å¤šæ•°æ–¹æ³•ä¸ºä¸åŒçš„KGCä»»åŠ¡è®¾è®¡å•ç‹¬çš„æŒ‡ä»¤ï¼Œå¯¼è‡´é‡å¤å·¥ä½œå’Œè€—æ—¶çš„è¿‡ç¨‹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶SATï¼Œé€šè¿‡ç»“æ„æ„ŸçŸ¥å¯¹é½å¾®è°ƒæ¥å¢å¼ºLLMçš„KGCèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥åˆ†å±‚çŸ¥è¯†å¯¹é½ï¼Œé€šè¿‡å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ å°†å›¾åµŒå…¥ä¸è‡ªç„¶è¯­è¨€ç©ºé—´å¯¹é½ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºç»“æ„æŒ‡ä»¤å¾®è°ƒï¼Œä½¿ç”¨ç»Ÿä¸€çš„å›¾æŒ‡ä»¤ç»“åˆè½»é‡çº§çŸ¥è¯†é€‚é…å™¨ï¼Œå¼•å¯¼LLMåœ¨KGä¸Šæ‰§è¡Œç»“æ„æ„ŸçŸ¥æ¨ç†ã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ä¸¤ä¸ªKGCä»»åŠ¡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSATæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œæ”¹è¿›èŒƒå›´ä»8.7%åˆ°29.8%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŸ¥è¯†å›¾è°±è¡¥å…¨æ—¨åœ¨é¢„æµ‹çŸ¥è¯†å›¾è°±ä¸­ç¼ºå¤±çš„å…³ç³»ä¸‰å…ƒç»„ã€‚ç°æœ‰åŸºäºLLMçš„æ–¹æ³•ä¸»è¦ä¾èµ–äºè®¾è®¡ç‰¹å®šä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä½†å¿½ç•¥äº†è‡ªç„¶è¯­è¨€å’Œå›¾ç»“æ„è¡¨ç¤ºç©ºé—´çš„å·®å¼‚ï¼Œå¯¼è‡´LLMéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å›¾ç»“æ„ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚æ­¤å¤–ï¼Œä¸ºæ¯ä¸ªKGCä»»åŠ¡å•ç‹¬è®¾è®¡æŒ‡ä»¤å¯¼è‡´äº†å¤§é‡é‡å¤å·¥ä½œã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSATæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“æ„æ„ŸçŸ¥å¯¹é½å¾®è°ƒï¼Œå¼¥åˆè‡ªç„¶è¯­è¨€å’Œå›¾ç»“æ„è¡¨ç¤ºç©ºé—´ä¹‹é—´çš„å·®è·ï¼Œå¹¶åˆ©ç”¨ç»Ÿä¸€çš„æŒ‡ä»¤æ¥å¤„ç†ä¸åŒçš„KGCä»»åŠ¡ã€‚é€šè¿‡å¯¹é½è¡¨ç¤ºç©ºé—´ï¼ŒLLMå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å›¾ç»“æ„ä¿¡æ¯ï¼Œä»è€Œæé«˜çŸ¥è¯†å›¾è°±è¡¥å…¨çš„æ€§èƒ½ã€‚ç»Ÿä¸€æŒ‡ä»¤çš„è®¾è®¡å‡å°‘äº†ä»»åŠ¡ç›¸å…³çš„æŒ‡ä»¤å·¥ç¨‹ï¼Œæé«˜äº†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSATæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šåˆ†å±‚çŸ¥è¯†å¯¹é½å’Œç»“æ„æŒ‡ä»¤å¾®è°ƒã€‚åœ¨åˆ†å±‚çŸ¥è¯†å¯¹é½é˜¶æ®µï¼Œä½¿ç”¨å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ å°†å›¾åµŒå…¥ä¸è‡ªç„¶è¯­è¨€ç©ºé—´å¯¹é½ã€‚åœ¨ç»“æ„æŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨ç»Ÿä¸€çš„å›¾æŒ‡ä»¤ç»“åˆè½»é‡çº§çŸ¥è¯†é€‚é…å™¨ï¼Œå¼•å¯¼LLMåœ¨çŸ¥è¯†å›¾è°±ä¸Šæ‰§è¡Œç»“æ„æ„ŸçŸ¥æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šSATçš„å…³é”®åˆ›æ–°åœ¨äºç»“æ„æ„ŸçŸ¥å¯¹é½å¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼Œåˆ†å±‚çŸ¥è¯†å¯¹é½é€šè¿‡å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ ï¼Œæ˜¾å¼åœ°å°†å›¾åµŒå…¥ä¸è‡ªç„¶è¯­è¨€ç©ºé—´å¯¹é½ï¼Œä»è€Œä½¿LLMèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å›¾ç»“æ„ä¿¡æ¯ã€‚ç»“æ„æŒ‡ä»¤å¾®è°ƒä½¿ç”¨ç»Ÿä¸€çš„å›¾æŒ‡ä»¤ï¼Œé¿å…äº†ä¸ºæ¯ä¸ªKGCä»»åŠ¡å•ç‹¬è®¾è®¡æŒ‡ä»¤çš„éœ€è¦ï¼Œæé«˜äº†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåˆ†å±‚çŸ¥è¯†å¯¹é½ä½¿ç”¨å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ ï¼ŒåŒ…å«å®ä½“å¯¹é½å’Œå…³ç³»å¯¹é½ä¸¤ä¸ªä»»åŠ¡ã€‚ç»“æ„æŒ‡ä»¤å¾®è°ƒä½¿ç”¨ç»Ÿä¸€çš„å›¾æŒ‡ä»¤ï¼Œè¯¥æŒ‡ä»¤åŒ…å«å®ä½“å’Œå…³ç³»çš„æè¿°ä¿¡æ¯ã€‚è½»é‡çº§çŸ¥è¯†é€‚é…å™¨ç”¨äºå°†å›¾ç»“æ„ä¿¡æ¯èå…¥åˆ°LLMä¸­ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬å¯¹æ¯”å­¦ä¹ æŸå¤±å’ŒçŸ¥è¯†å›¾è°±è¡¥å…¨æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSATæ¡†æ¶åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ä¸¤ä¸ªKGCä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å°¤å…¶æ˜¯åœ¨é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒSATçš„æ€§èƒ½æå‡èŒƒå›´ä»8.7%åˆ°29.8%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç»“æ„æ„ŸçŸ¥å¯¹é½å¾®è°ƒèƒ½å¤Ÿæœ‰æ•ˆæé«˜LLMçš„çŸ¥è¯†å›¾è°±è¡¥å…¨èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½é—®ç­”ã€æ¨èç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸï¼Œé€šè¿‡å¢å¼ºLLMå¯¹çŸ¥è¯†å›¾è°±çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œæé«˜è¿™äº›åº”ç”¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„çŸ¥è¯†å›¾è°±ç»“æ„å’Œæ›´å¤šçš„KGCä»»åŠ¡ï¼Œå¹¶åº”ç”¨äºå…¶ä»–éœ€è¦çŸ¥è¯†æ¨ç†çš„åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge graph completion (KGC) aims to infer new knowledge and make predictions from knowledge graphs. Recently, large language models (LLMs) have exhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily focus on designing task-specific instructions, achieving promising advancements. However, there are still two critical challenges. First, existing methods often ignore the inconsistent representation spaces between natural language and graph structures. Second, most approaches design separate instructions for different KGC tasks, leading to duplicate works and time-consuming processes. To address these challenges, we propose SAT, a novel framework that enhances LLMs for KGC via structure-aware alignment-tuning. Specifically, we first introduce hierarchical knowledge alignment to align graph embeddings with the natural language space through multi-task contrastive learning. Then, we propose structural instruction tuning to guide LLMs in performing structure-aware reasoning over KGs, using a unified graph instruction combined with a lightweight knowledge adapter. Experimental results on two KGC tasks across four benchmark datasets demonstrate that SAT significantly outperforms state-of-the-art methods, especially in the link prediction task with improvements ranging from 8.7% to 29.8%.

