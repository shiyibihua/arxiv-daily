---
layout: default
title: A Simple Linear Patch Revives Layer-Pruned Large Language Models
---

# A Simple Linear Patch Revives Layer-Pruned Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24680" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24680v2</a>
  <a href="https://arxiv.org/pdf/2505.24680.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24680v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24680v2', 'A Simple Linear Patch Revives Layer-Pruned Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinrui Chen, Haoli Bai, Tao Yuan, Ruikang Liu, Kang Zhao, Xianzhi Yu, Lu Hou, Tian Guan, Yonghong He, Chun Yuan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-10-25)

**å¤‡æ³¨**: 26 pages, accepted to NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/chenxinrui-tsinghua/LinearPatch)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLinearPatchä»¥è§£å†³å±‚ä¿®å‰ªæ¨¡å‹æ€§èƒ½ä¸‹é™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å±‚ä¿®å‰ª` `æ¿€æ´»å¯¹é½` `Hadamardå˜æ¢` `è¯­è¨€æ¨¡å‹å‹ç¼©` `æ¨¡å‹è’¸é¦` `æ€§èƒ½ä¼˜åŒ–` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å±‚ä¿®å‰ªæ–¹æ³•åœ¨å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¾€å¾€å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä¸»è¦ç”±äºæ¿€æ´»å¹…åº¦ä¸åŒ¹é…é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºLinearPatchï¼Œé€šè¿‡èåˆHadamardå˜æ¢å’Œé€šé“çº§ç¼©æ”¾ï¼Œè§£å†³äº†æ¿€æ´»å¹…åº¦ä¸åŒ¹é…çš„é—®é¢˜ã€‚
3. åœ¨LLaMA-3-8Bæ¨¡å‹ä¸Šï¼ŒLinearPatchä¿®å‰ª5å±‚æ—¶ä¿ç•™äº†94.15%çš„æ€§èƒ½ï¼Œä¸”é€šè¿‡ç¦»çº¿è’¸é¦è¿›ä¸€æ­¥æå‡è‡³95.16%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å±‚ä¿®å‰ªå·²æˆä¸ºå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å±‚ä¿®å‰ªæ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬å‘ç°è¿™ç§ä¸‹é™ä¸»è¦æºäºä¸€ä¸ªè¢«å¿½è§†çš„é—®é¢˜ï¼šä¿®å‰ªæ¥å£å¤„æ¿€æ´»å¹…åº¦çš„ä¸åŒ¹é…ã€‚ä¿®å‰ªå‰åçš„æ¿€æ´»åœ¨è§„æ¨¡ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¯¼è‡´åœ¨å‰©ä½™å±‚ä¸­ä¼ æ’­æ—¶å‡ºç°åˆ†å¸ƒåç§»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LinearPatchï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„å³æ’å³ç”¨æŠ€æœ¯ï¼Œå®ƒå°†ä¸¤ç§æ“ä½œèåˆä¸ºä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼šä¸€æ˜¯Hadamardå˜æ¢ä»¥æŠ‘åˆ¶ç‰¹å®štokençš„å·¨å¤§å¼‚å¸¸å€¼ï¼ŒäºŒæ˜¯é€šé“çº§ç¼©æ”¾ä»¥å¯¹é½æ¿€æ´»ç»Ÿè®¡ã€‚åœ¨LLaMA-3-8Bæ¨¡å‹ä¸Šï¼ŒLinearPatchåœ¨ä¿®å‰ª32å±‚ä¸­çš„5å±‚æ—¶ï¼Œä¿ç•™äº†åŸæ¨¡å‹94.15%çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä¼˜ç»“æœ4%ã€‚è¯¥è¡¥ä¸è¿˜å¯ä»¥é€šè¿‡å†…å­˜é«˜æ•ˆçš„ç¦»çº¿è’¸é¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä¿ç•™ç‡åœ¨ä»…30åˆ†é’Ÿå†…æå‡è‡³95.16%ã€‚ä»£ç å¯åœ¨https://github.com/chenxinrui-tsinghua/LinearPatchè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„å…·ä½“é—®é¢˜æ˜¯å±‚ä¿®å‰ªå¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼Œä¸»è¦ç”±äºä¿®å‰ªæ¥å£å¤„æ¿€æ´»å¹…åº¦çš„ä¸åŒ¹é…ï¼Œé€ æˆäº†æ¿€æ´»åœ¨åç»­å±‚ä¸­çš„åˆ†å¸ƒåç§»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯å¼•å…¥LinearPatchæŠ€æœ¯ï¼Œé€šè¿‡å°†Hadamardå˜æ¢å’Œé€šé“çº§ç¼©æ”¾ç»“åˆï¼Œæ¥æŠ‘åˆ¶å¼‚å¸¸å€¼å¹¶å¯¹é½æ¿€æ´»ç»Ÿè®¡ï¼Œä»è€Œå‡è½»æ€§èƒ½æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯Hadamardå˜æ¢æ¨¡å—ï¼Œç”¨äºå¤„ç†ç‰¹å®štokençš„å¼‚å¸¸å€¼ï¼ŒäºŒæ˜¯é€šé“çº§ç¼©æ”¾æ¨¡å—ï¼Œç”¨äºè°ƒæ•´æ¿€æ´»çš„ç»Ÿè®¡ç‰¹æ€§ã€‚è¿™ä¸¤ä¸ªæ¨¡å—åœ¨ä¿®å‰ªæ¥å£å¤„è¿›è¡Œèåˆï¼Œå½¢æˆä¸€ä¸ªé«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•æ“ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†ä¸¤ç§æ“ä½œèåˆä¸ºä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œè§£å†³äº†æ¿€æ´»å¹…åº¦ä¸åŒ¹é…çš„é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†ä¿®å‰ªåçš„æ¨¡å‹æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§è®¾è®¡åœ¨æ€§èƒ½ä¿æŒä¸Šå…·æœ‰æœ¬è´¨çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒLinearPatché‡‡ç”¨äº†ç‰¹å®šçš„Hadamardå˜æ¢å‚æ•°å’Œé€šé“ç¼©æ”¾å› å­ï¼Œä»¥ç¡®ä¿åœ¨ä¿®å‰ªåæ¿€æ´»çš„ç»Ÿè®¡ç‰¹æ€§èƒ½å¤Ÿå¾—åˆ°æœ‰æ•ˆå¯¹é½ã€‚æ­¤å¤–ï¼Œç¦»çº¿è’¸é¦è¿‡ç¨‹ä¹Ÿç»è¿‡ä¼˜åŒ–ï¼Œä»¥æé«˜æ¨¡å‹çš„ä¿ç•™ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLinearPatchåœ¨LLaMA-3-8Bæ¨¡å‹ä¸Šä¿®å‰ª5å±‚æ—¶ï¼Œä¿ç•™äº†94.15%çš„åŸå§‹æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä¼˜ç»“æœ4%ã€‚é€šè¿‡è¿›ä¸€æ­¥çš„ç¦»çº¿è’¸é¦ï¼Œä¿ç•™ç‡åœ¨ä»…30åˆ†é’Ÿå†…æå‡è‡³95.16%ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ä¿æŒèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæœºå™¨ç¿»è¯‘ç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–ã€‚é€šè¿‡æœ‰æ•ˆçš„å±‚ä¿®å‰ªæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—èµ„æºçš„æ¶ˆè€—ï¼Œæå‡æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨æ›´å¤šè½»é‡çº§æ¨¡å‹çš„å¼€å‘ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Layer pruning has emerged as a widely used technique for compressing large language models (LLMs). However, existing layer pruning approaches often incur substantial performance degradation. We identify the majority of this degradation to a single yet previously overlooked issue: \textit{the mismatch of activation magnitudes at the pruning interface}. The pre-interface activations exhibit significantly different scales from the post-interface ones, causing the distributional shift as it propagates through the remaining layers. To address this issue, we introduce \textsc{LinearPatch}, a lightweight and plug-and-play technique that fuses two operations into one matrix multiply at the pruning interface: (i) a Hadamard transformation that suppresses massive outliers at particular tokens and (ii) a channel-wise scaling that aligns activation statistics. On LLaMA-3-8B, \textsc{LinearPatch} preserves up to \textbf{94.15\% } of the original model's performance when pruning 5 out of 32 layers, outperforming the previous state of the art by \textbf{4\% }. The patch can be further refined with 5K unlabeled samples via memory-efficient offline distillation, pushing the retention to 95.16\% within only 30 minutes on a single GPU. Code is available at https://github.com/chenxinrui-tsinghua/LinearPatch.

