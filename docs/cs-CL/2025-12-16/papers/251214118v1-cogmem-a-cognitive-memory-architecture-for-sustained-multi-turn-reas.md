---
layout: default
title: CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models
---

# CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14118" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14118v1</a>
  <a href="https://arxiv.org/pdf/2512.14118.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14118v1" onclick="toggleFavorite(this, '2512.14118v1', 'CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiran Zhang, Jincheng Hu, Mark Dras, Usman Naseem

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: underreview

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCogMemä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè½®æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šè½®æ¨ç†` `è®¤çŸ¥æ¶æ„` `è®°å¿†å¢å¼º` `æ¨ç†ä¸€è‡´æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®æ¨ç†ä¸­é¢ä¸´å‡†ç¡®æ€§å’Œè¿è´¯æ€§ä¸‹é™çš„é—®é¢˜ï¼Œå°¤å…¶åœ¨é•¿æ—¶é—´äº¤äº’ä¸­è¡¨ç°ä¸ä½³ã€‚
2. CogMemé€šè¿‡å¼•å…¥é•¿æœŸè®°å¿†ã€ç›´æ¥è®¿é—®è®°å¿†å’Œæ³¨æ„ç„¦ç‚¹æœºåˆ¶ï¼Œæä¾›äº†ä¸€ç§ç»“æ„åŒ–çš„æŒä¹…è®°å¿†æ¶æ„ï¼Œä»¥æ”¯æŒæŒç»­çš„æ¨ç†è¿‡ç¨‹ã€‚
3. åœ¨TurnBenchä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCogMemæ˜¾è‘—å‡å°‘äº†æ¨ç†å¤±è´¥ï¼Œæ§åˆ¶äº†ä¸Šä¸‹æ–‡çš„å¢é•¿ï¼Œå¹¶æå‡äº†æ¨ç†çš„ä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•è½®æ¨ç†ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å»¶ç»­çš„å¤šè½®äº¤äº’ä¸­å¸¸å¸¸å¤±å»å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚æœ€è¿‘çš„è¯„ä¼°å¦‚TurnBenchæ­ç¤ºäº†æ¨ç†åå·®ã€ä»»åŠ¡æ¼‚ç§»ã€å¹»è§‰ã€è¿‡åº¦è‡ªä¿¡å’Œè®°å¿†è¡°é€€ç­‰åå¤å‡ºç°çš„å¤±è´¥æ¨¡å¼ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡é™„åŠ å®Œæ•´çš„å¯¹è¯å†å²æ¥å¤„ç†è¿™äº›é—®é¢˜ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡æ— é™å¢é•¿ã€è®¡ç®—æˆæœ¬å¢åŠ å’Œæ¨ç†æ•ˆç‡ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†CogMemï¼Œè¿™æ˜¯ä¸€ç§å—è®¤çŸ¥å¯å‘çš„ã€å¢å¼ºè®°å¿†çš„LLMæ¶æ„ï¼Œæ”¯æŒé€šè¿‡ç»“æ„åŒ–çš„æŒä¹…è®°å¿†è¿›è¡ŒæŒç»­çš„è¿­ä»£æ¨ç†ã€‚CogMemåŒ…å«ä¸‰ä¸ªå±‚æ¬¡ï¼šé•¿æœŸè®°å¿†ï¼ˆLTMï¼‰æ•´åˆè·¨ä¼šè¯çš„æ¨ç†ç­–ç•¥ï¼›ç›´æ¥è®¿é—®ï¼ˆDAï¼‰è®°å¿†ç»´æŠ¤ä¼šè¯çº§ç¬”è®°å¹¶æ£€ç´¢ç›¸å…³çš„é•¿æœŸè®°å¿†ï¼›æ³¨æ„ç„¦ç‚¹ï¼ˆFoAï¼‰æœºåˆ¶åœ¨æ¯è½®åŠ¨æ€é‡æ„ç®€æ´çš„ä»»åŠ¡ç›¸å…³ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åˆ†å±‚è®¾è®¡æœ‰æ•ˆå‡è½»äº†æ¨ç†å¤±è´¥ï¼Œæ§åˆ¶äº†ä¸Šä¸‹æ–‡å¢é•¿ï¼Œå¹¶æé«˜äº†å»¶ç»­æ¨ç†é“¾ä¸­çš„ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®æ¨ç†ä¸­é¢ä¸´çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡é™„åŠ å®Œæ•´çš„å¯¹è¯å†å²ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡æ— é™å¢é•¿ï¼Œè®¡ç®—æˆæœ¬å¢åŠ ï¼Œæ¨ç†æ•ˆç‡ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCogMemçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ç»“æ„åŒ–çš„æŒä¹…è®°å¿†æ¥æ”¯æŒæŒç»­çš„è¿­ä»£æ¨ç†ã€‚é€šè¿‡åˆ†å±‚è®¾è®¡ï¼ŒCogMemèƒ½å¤Ÿæœ‰æ•ˆç®¡ç†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å¤±è¯¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCogMemæ¶æ„åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé•¿æœŸè®°å¿†ï¼ˆLTMï¼‰ç”¨äºæ•´åˆè·¨ä¼šè¯çš„æ¨ç†ç­–ç•¥ï¼›ç›´æ¥è®¿é—®ï¼ˆDAï¼‰è®°å¿†ç”¨äºç»´æŠ¤ä¼šè¯çº§ç¬”è®°å¹¶æ£€ç´¢ç›¸å…³çš„é•¿æœŸè®°å¿†ï¼›æ³¨æ„ç„¦ç‚¹ï¼ˆFoAï¼‰æœºåˆ¶ç”¨äºåŠ¨æ€é‡æ„ä»»åŠ¡ç›¸å…³çš„ä¸Šä¸‹æ–‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCogMemçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åˆ†å±‚è®°å¿†ç»“æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶ä¸Šä¸‹æ–‡å¢é•¿å¹¶æé«˜æ¨ç†ä¸€è‡´æ€§ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºä¸å†ç®€å•ä¾èµ–å®Œæ•´çš„å¯¹è¯å†å²ï¼Œè€Œæ˜¯é€šè¿‡ç»“æ„åŒ–çš„è®°å¿†ç®¡ç†æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé•¿æœŸè®°å¿†æ¨¡å—è´Ÿè´£å­˜å‚¨å’Œæ•´åˆè·¨ä¼šè¯çš„æ¨ç†ç­–ç•¥ï¼Œç›´æ¥è®¿é—®è®°å¿†æ¨¡å—åˆ™é€šè¿‡ç¬”è®°å’Œæ£€ç´¢æœºåˆ¶ä¿æŒä¼šè¯ä¿¡æ¯çš„ç›¸å…³æ€§ï¼Œæ³¨æ„ç„¦ç‚¹æœºåˆ¶åˆ™ç¡®ä¿æ¯è½®æ¨ç†æ—¶ä¸Šä¸‹æ–‡çš„ç®€æ´æ€§å’Œç›¸å…³æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨TurnBenchçš„å®éªŒä¸­ï¼ŒCogMemæ˜¾è‘—å‡å°‘äº†æ¨ç†å¤±è´¥çš„å‘ç”Ÿï¼Œæ§åˆ¶äº†ä¸Šä¸‹æ–‡çš„å¢é•¿ï¼Œå¹¶åœ¨å»¶ç»­æ¨ç†é“¾ä¸­æé«˜äº†ä¸€è‡´æ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒCogMemåœ¨å¤šè½®æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CogMemçš„ç ”ç©¶æˆæœå¯ä»¥å¹¿æ³›åº”ç”¨äºéœ€è¦é•¿æ—¶é—´äº¤äº’çš„æ™ºèƒ½åŠ©æ‰‹ã€å®¢æœæœºå™¨äººå’Œæ•™è‚²é¢†åŸŸçš„å¯¹è¯ç³»ç»Ÿã€‚é€šè¿‡æé«˜å¤šè½®æ¨ç†çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ï¼ŒCogMemæœ‰åŠ©äºæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶æ¨åŠ¨äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæ™ºèƒ½åŒ–ã€‚æœªæ¥ï¼ŒCogMemçš„æ¶æ„å¯èƒ½ä¼šå½±å“æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½ç³»ç»Ÿè®¾è®¡ï¼Œä¿ƒè¿›æ›´å¤æ‚çš„æ¨ç†ä»»åŠ¡çš„å®ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) excel at single-turn reasoning but often lose accuracy and coherence over extended, multi-turn interactions. Recent evaluations such as TurnBench highlight recurring failure modes-reasoning bias, task drift, hallucination, overconfidence, and memory decay. Current approaches typically append full conversational histories, causing unbounded context growth, higher computational costs, and degraded reasoning efficiency. We introduce CogMem, a cognitively inspired, memory-augmented LLM architecture that supports sustained iterative reasoning through structured, persistent memory. CogMem incorporates three layers: a Long-Term Memory (LTM) that consolidates cross-session reasoning strategies; a Direct Access (DA) memory that maintains session-level notes and retrieves relevant long-term memories; and a Focus of Attention (FoA) mechanism that dynamically reconstructs concise, task-relevant context at each turn. Experiments on TurnBench show that this layered design mitigates reasoning failures, controls context growth, and improves consistency across extended reasoning chains, moving toward more reliable, human-like reasoning in LLMs.

