---
layout: default
title: Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets
---

# Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14237" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14237v1</a>
  <a href="https://arxiv.org/pdf/2512.14237.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14237v1" onclick="toggleFavorite(this, '2512.14237v1', 'Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Estelle Zheng, Nathan Cerisara, SÃ©bastien Warichet, Emmanuel Helbert, Christophe Cerisara

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Ladder Side Tuningé€šè¿‡è½»é‡çº§ä¾§ç½‘ç»œå®ç°ä½æˆæœ¬å¤§æ¨¡å‹å¾®è°ƒï¼Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§è¯­è¨€æ¨¡å‹` `ä¾§ç½‘ç»œ` `ä½å†…å­˜å ç”¨` `Ladder Side Tuning` `xLadder` `æ·±åº¦æ‰©å±•` `æ¶ˆè´¹çº§GPU`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œå¦‚QLoRAï¼Œè™½ç„¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°ï¼Œä½†å…¨æ¨¡å‹åå‘ä¼ æ’­å¯¼è‡´å†…å­˜å ç”¨ä»ç„¶å¾ˆé«˜ã€‚
2. è®ºæ–‡æå‡ºLadder Side Tuning (LST)ï¼Œé€šè¿‡æ·»åŠ è½»é‡çº§ä¾§ç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œé™ä½å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLSTåœ¨å†…å­˜æ•ˆç‡ä¸Šä¼˜äºQLoRAï¼Œå¯ä»¥åœ¨æ¶ˆè´¹çº§GPUä¸Šå¾®è°ƒ70äº¿å‚æ•°æ¨¡å‹ï¼Œä¸”æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é‡æ–°å®¡è§†äº†ä¸€ç§é²œå°‘è¢«æ¢ç´¢çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯â€”â€”Ladder Side Tuning (LST)ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ·»åŠ è½»é‡çº§ä¾§ç½‘ç»œè¿›è¡Œå¾®è°ƒã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLSTåœ¨è®¡ç®—æ‰©å±•æ€§ä¸Šä¸QLoRAç›¸å½“ï¼ŒåŒæ—¶å³°å€¼å†…å­˜å ç”¨é™ä½äº†50%ã€‚åœ¨æ¶µç›–è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦å’ŒLLM-criticä»»åŠ¡çš„å¤šä¸ªä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLSTåœ¨å¹³å‡ç²¾åº¦ä¸Šä¸QLoRAå…·æœ‰ç«äº‰åŠ›ï¼Œä½†å†…å­˜æ•ˆç‡æ›´é«˜ã€‚è¿™ç§æ•ˆç‡ä½¿å¾—åœ¨å•ä¸ª12GBæ¶ˆè´¹çº§GPUä¸Šï¼Œæ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹å³å¯å¯¹å…·æœ‰2k tokenä¸Šä¸‹æ–‡çš„70äº¿å‚æ•°æ¨¡å‹è¿›è¡Œå¾®è°ƒâ€”â€”è€ŒQLoRAåœ¨è¿™ç§æ¡ä»¶ä¸‹ä¼šè€—å°½å†…å­˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å»ºç«‹äº†ç¼©æ”¾å®šå¾‹ï¼Œè¡¨æ˜LSTçš„ç¼©æ”¾æ€§ä¸QLoRAç›¸ä¼¼ã€‚é€šè¿‡å¼•å…¥xLadderï¼Œä¸€ç§æ·±åº¦æ‰©å±•å˜ä½“ï¼Œåˆ©ç”¨Ladderçš„æ¶æ„çµæ´»æ€§ï¼Œé€šè¿‡äº¤å‰è¿æ¥å¢åŠ æœ‰æ•ˆæ·±åº¦ï¼Œå¹¶åœ¨å›ºå®šå‚æ•°æ•°é‡ä¸‹ç¼©çŸ­æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚Ladderåœ¨å†…å­˜æ˜¯ç“¶é¢ˆæ—¶è¡¨ç°å‡ºè‰²ï¼›xLadderåœ¨æ­¤åŸºç¡€ä¸Šé€šè¿‡æ— éœ€é¢å¤–å†…å­˜å¼€é”€å³å¯å®ç°æ›´æ·±å±‚æ¬¡çš„æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒé€šå¸¸å—åˆ°å•†å“çº§GPUä¸Šå¯ç”¨å†…å­˜çš„é™åˆ¶ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå¦‚QLoRAï¼Œè™½ç„¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œä½†ç”±äºå®Œæ•´æ¨¡å‹ä¸­çš„åå‘ä¼ æ’­ï¼Œä»ç„¶ä¼šäº§ç”Ÿè¾ƒé«˜çš„å†…å­˜ä½¿ç”¨é‡ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½å¾®è°ƒè¿‡ç¨‹ä¸­çš„å†…å­˜å ç”¨ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚QLoRAï¼Œåœ¨å†…å­˜æ•ˆç‡æ–¹é¢ä»æœ‰æå‡ç©ºé—´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Ladder Side Tuning (LST) è¿™ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œé€šè¿‡åœ¨åŸå§‹æ¨¡å‹æ—è¾¹æ·»åŠ ä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œè¿›è¡Œå¾®è°ƒã€‚ä¸»æ¨¡å‹çš„å‚æ•°ä¿æŒå†»ç»“ï¼Œåªè®­ç»ƒä¾§ç½‘ç»œçš„å‚æ•°ã€‚è¿™æ ·å¯ä»¥æ˜¾è‘—å‡å°‘éœ€è¦è®¡ç®—æ¢¯åº¦çš„å‚æ•°é‡ï¼Œä»è€Œé™ä½å†…å­˜å ç”¨ã€‚LSTçš„è®¾è®¡æ€æƒ³æ˜¯åœ¨ä¸æ”¹å˜åŸå§‹æ¨¡å‹ç»“æ„çš„å‰æä¸‹ï¼Œå¼•å…¥é¢å¤–çš„å¯è®­ç»ƒå‚æ•°ï¼Œå®ç°é«˜æ•ˆçš„å¾®è°ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLST çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆä¸»ç½‘ç»œï¼‰å’Œä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œï¼ˆLadderï¼‰ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä¸»ç½‘ç»œçš„å‚æ•°è¢«å†»ç»“ï¼Œåªè®­ç»ƒä¾§ç½‘ç»œçš„å‚æ•°ã€‚è¾“å…¥æ•°æ®åŒæ—¶è¾“å…¥åˆ°ä¸»ç½‘ç»œå’Œä¾§ç½‘ç»œä¸­ã€‚ä¸»ç½‘ç»œçš„è¾“å‡ºå’Œä¾§ç½‘ç»œçš„è¾“å‡ºè¿›è¡ŒæŸç§å½¢å¼çš„èåˆï¼ˆä¾‹å¦‚ï¼ŒåŠ æƒæ±‚å’Œï¼‰ï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚æŸå¤±å‡½æ•°åŸºäºæœ€ç»ˆé¢„æµ‹ç»“æœè®¡ç®—ï¼Œå¹¶ä»…ç”¨äºæ›´æ–°ä¾§ç½‘ç»œçš„å‚æ•°ã€‚xLadderæ˜¯LSTçš„æ·±åº¦æ‰©å±•å˜ä½“ï¼Œé€šè¿‡äº¤å‰è¿æ¥å¢åŠ æœ‰æ•ˆæ·±åº¦ï¼Œå¹¶åœ¨å›ºå®šå‚æ•°æ•°é‡ä¸‹ç¼©çŸ­æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºé‡æ–°å®¡è§†å¹¶æœ‰æ•ˆåˆ©ç”¨äº†Ladder Side Tuning (LST) è¿™ç§é²œå°‘è¢«æ¢ç´¢çš„PEFTæŠ€æœ¯ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨é™ä½å†…å­˜å ç”¨æ–¹é¢çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†xLadderï¼Œä¸€ç§æ·±åº¦æ‰©å±•çš„LSTå˜ä½“ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€å¢åŠ é¢å¤–çš„å†…å­˜å¼€é”€ã€‚ä¸ç°æœ‰æ–¹æ³•ï¼ˆå¦‚QLoRAï¼‰ç›¸æ¯”ï¼ŒLSTçš„ä¸»è¦åŒºåˆ«åœ¨äºå…¶ä¾§ç½‘ç»œç»“æ„å’Œè®­ç»ƒæ–¹å¼ï¼Œè¿™ä½¿å¾—å®ƒåœ¨å†…å­˜æ•ˆç‡æ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šLSTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ä¾§ç½‘ç»œçš„ç»“æ„ã€èåˆæ–¹å¼å’Œè®­ç»ƒç­–ç•¥ã€‚ä¾§ç½‘ç»œçš„ç»“æ„å¯ä»¥æ˜¯ä»»æ„çš„ï¼Œä½†é€šå¸¸é€‰æ‹©è½»é‡çº§çš„ç½‘ç»œç»“æ„ï¼Œä»¥å‡å°‘å‚æ•°é‡å’Œè®¡ç®—é‡ã€‚èåˆæ–¹å¼å¯ä»¥æ˜¯ç®€å•çš„åŠ æƒæ±‚å’Œï¼Œä¹Ÿå¯ä»¥æ˜¯æ›´å¤æ‚çš„èåˆç­–ç•¥ã€‚è®­ç»ƒç­–ç•¥é€šå¸¸é‡‡ç”¨æ ‡å‡†çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œä½†ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€äº›ä¼˜åŒ–æŠ€å·§ï¼Œå¦‚å­¦ä¹ ç‡è¡°å‡å’Œæ¢¯åº¦è£å‰ªã€‚xLadderçš„å…³é”®è®¾è®¡åœ¨äºäº¤å‰è¿æ¥çš„å¼•å…¥ï¼Œè¿™ä½¿å¾—æ¨¡å‹å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨ä¸åŒå±‚çš„ä¿¡æ¯ï¼Œä»è€Œæå‡æ¨ç†èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°é€‰æ‹©å–å†³äºå…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLSTåœ¨å¤šä¸ªä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ä¸QLoRAå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶å³°å€¼å†…å­˜å ç”¨é™ä½äº†50%ã€‚LSTå¯ä»¥åœ¨å•ä¸ª12GBæ¶ˆè´¹çº§GPUä¸Šï¼Œæ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹å³å¯å¯¹å…·æœ‰2k tokenä¸Šä¸‹æ–‡çš„70äº¿å‚æ•°æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€ŒQLoRAåœ¨è¿™ç§æ¡ä»¶ä¸‹ä¼šè€—å°½å†…å­˜ã€‚æ­¤å¤–ï¼Œç¼©æ”¾å®šå¾‹è¡¨æ˜LSTçš„ç¼©æ”¾æ€§ä¸QLoRAç›¸ä¼¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LSTæŠ€æœ¯å¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºå—é™çš„ç¯å¢ƒä¸‹ï¼Œä¾‹å¦‚è¾¹ç¼˜è®¾å¤‡ã€ç§»åŠ¨è®¾å¤‡æˆ–ä½ç«¯GPUæœåŠ¡å™¨ã€‚è¯¥æŠ€æœ¯å¯ä»¥é™ä½å¾®è°ƒçš„æˆæœ¬å’Œé—¨æ§›ï¼Œä½¿å¾—æ›´å¤šç ”ç©¶è€…å’Œå¼€å‘è€…èƒ½å¤Ÿå‚ä¸åˆ°å¤§æ¨¡å‹çš„å¾®è°ƒå’Œåº”ç”¨ä¸­ã€‚æ­¤å¤–ï¼ŒLSTè¿˜å¯ä»¥ç”¨äºä¸ªæ€§åŒ–æ¨èã€æ™ºèƒ½å®¢æœã€æ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œåº”ç”¨æ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.

