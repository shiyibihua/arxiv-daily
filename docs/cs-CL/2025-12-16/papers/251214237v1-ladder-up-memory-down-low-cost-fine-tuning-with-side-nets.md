---
layout: default
title: Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets
---

# Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14237" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14237v1</a>
  <a href="https://arxiv.org/pdf/2512.14237.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14237v1" onclick="toggleFavorite(this, '2512.14237v1', 'Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Estelle Zheng, Nathan Cerisara, SÃ©bastien Warichet, Emmanuel Helbert, Christophe Cerisara

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLadder Side Tuningä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„å†…å­˜ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `å†…å­˜æ•ˆç‡` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `Ladder Side Tuning` `æœºå™¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¾®è°ƒæ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­é¢ä¸´å†…å­˜é™åˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨æ™®é€šGPUæ—¶ã€‚
2. æœ¬æ–‡æå‡ºLadder Side Tuningï¼ˆLSTï¼‰ï¼Œé€šè¿‡å¼•å…¥è½»é‡çº§ä¾§ç½‘ç»œæ¥æé«˜å†…å­˜æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLSTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸QLoRAçš„å‡†ç¡®æ€§ç›¸å½“ï¼Œä½†å†…å­˜ä½¿ç”¨æ•ˆç‡æé«˜äº†50%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸å—åˆ°æ™®é€šGPUå¯ç”¨å†…å­˜çš„é™åˆ¶ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚QLoRAå‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œä½†åœ¨å…¨æ¨¡å‹çš„åå‘ä¼ æ’­ä¸­ä»ç„¶æ¶ˆè€—å¤§é‡å†…å­˜ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†Ladder Side Tuningï¼ˆLSTï¼‰ï¼Œè¿™ä¸€è¾ƒå°‘è¢«æ¢ç´¢çš„PEFTæŠ€æœ¯ï¼Œé€šè¿‡æ·»åŠ è½»é‡çº§ä¾§ç½‘ç»œï¼Œå±•ç¤ºäº†å…¶åœ¨è®¡ç®—è§„æ¨¡ä¸Šä¸QLoRAç›¸åŒ¹é…ï¼ŒåŒæ—¶å°†å³°å€¼å†…å­˜å‡å°‘äº†50%ã€‚åœ¨ä¸åŒçš„ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLSTåœ¨è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦å’ŒLLMè¯„ä¼°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¸QLoRAç›¸å½“çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†å†…å­˜æ•ˆç‡ï¼Œä½¿å¾—åœ¨å•ä¸ª12GBæ¶ˆè´¹çº§GPUä¸Šä»¥2k-tokenä¸Šä¸‹æ–‡å¾®è°ƒ7Bå‚æ•°æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œä¸”æ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚é™¤äº†å†…å­˜æ•ˆç‡å¤–ï¼Œæœ¬æ–‡è¿˜å»ºç«‹äº†LSTçš„æ‰©å±•è§„å¾‹ï¼Œè¡¨æ˜å…¶ä¸QLoRAçš„æ‰©å±•æ€§ç›¸ä¼¼ã€‚é€šè¿‡å¼•å…¥xLadderï¼ŒLSTåœ¨ä¸å¢åŠ å†…å­˜å¼€é”€çš„æƒ…å†µä¸‹ï¼Œå¢å¼ºäº†æ¨ç†æ·±åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æ™®é€šGPUä¸Šå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰çš„PEFTæ–¹æ³•å¦‚QLoRAè™½ç„¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°ï¼Œä½†åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä»ç„¶æ¶ˆè€—å¤§é‡å†…å­˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„è®­ç»ƒèƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºLadder Side Tuningï¼ˆLSTï¼‰ï¼Œé€šè¿‡æ·»åŠ ä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œæ¥é™ä½å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒä¸QLoRAç›¸ä¼¼çš„è®¡ç®—æ€§èƒ½ã€‚è¿™ç§è®¾è®¡ä½¿å¾—åœ¨å†…å­˜å—é™çš„ç¯å¢ƒä¸­ï¼Œä»èƒ½æœ‰æ•ˆå¾®è°ƒå¤§å‹æ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLSTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸»ç½‘ç»œå’Œä¾§ç½‘ç»œï¼Œä¸»ç½‘ç»œè´Ÿè´£ä¸»è¦çš„è®¡ç®—ä»»åŠ¡ï¼Œè€Œä¾§ç½‘ç»œåˆ™é€šè¿‡è¾…åŠ©è®¡ç®—æ¥å‡è½»ä¸»ç½‘ç»œçš„å†…å­˜è´Ÿæ‹…ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿›è¡Œè¯„ä¼°ï¼Œä»¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šLSTçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†è½»é‡çº§ä¾§ç½‘ç»œï¼Œæ˜¾è‘—é™ä½äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„å†…å­˜éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒäº†ä¸QLoRAç›¸å½“çš„æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•åœ¨å†…å­˜æˆä¸ºç“¶é¢ˆçš„æƒ…å†µä¸‹è¡¨ç°å°¤ä¸ºçªå‡ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLSTé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿ä¾§ç½‘ç»œçš„è½»é‡æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒxLadderä½œä¸ºLSTçš„æ‰©å±•ç‰ˆæœ¬ï¼Œé€šè¿‡äº¤å‰è¿æ¥å¢åŠ äº†æœ‰æ•ˆæ·±åº¦ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸å¢åŠ é¢å¤–çš„å†…å­˜å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLSTåœ¨å¤šä¸ªä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸QLoRAç›¸å½“çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å†…å­˜ä½¿ç”¨æ•ˆç‡æé«˜äº†50%ã€‚åœ¨å•ä¸ª12GBçš„æ¶ˆè´¹çº§GPUä¸Šï¼ŒLSTèƒ½å¤Ÿæœ‰æ•ˆå¾®è°ƒ7Bå‚æ•°çš„æ¨¡å‹ï¼Œä¸”æ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œè¿™åœ¨QLoRAä¸­æ˜¯ä¸å¯è¡Œçš„ã€‚æ•´ä½“æ¥çœ‹ï¼ŒLSTåœ¨å†…å­˜å—é™çš„ç¯å¢ƒä¸­å±•ç°å‡ºäº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæ•ˆç‡ï¼ŒLSTèƒ½å¤Ÿä½¿å¾—æ›´å¤šç ”ç©¶è€…å’Œå¼€å‘è€…åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨å¤§å‹æ¨¡å‹è¿›è¡Œåˆ›æ–°å’Œå¼€å‘ï¼Œä»è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚æœªæ¥ï¼ŒLSTåŠå…¶å˜ä½“æœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆè®¡ç®—èµ„æºçš„æƒ…å†µä¸‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.

