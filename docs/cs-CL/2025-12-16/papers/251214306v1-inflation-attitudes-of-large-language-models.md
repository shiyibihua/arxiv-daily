---
layout: default
title: Inflation Attitudes of Large Language Models
---

# Inflation Attitudes of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14306" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14306v1</a>
  <a href="https://arxiv.org/pdf/2512.14306.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14306v1" onclick="toggleFavorite(this, '2512.14306v1', 'Inflation Attitudes of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nikoleta Anesti, Edward Hill, Andreas Joseph

**åˆ†ç±»**: cs.CL, econ.EM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 41 pages, 11 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿé€šèƒ€é¢„æœŸï¼Œåˆ†æå…¶å¯¹å®è§‚ç»æµä¿¡å·çš„ååº”**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é€šè´§è†¨èƒ€é¢„æœŸ` `å®è§‚ç»æµå»ºæ¨¡` `Shapleyå€¼åˆ†è§£` `ç¤¾ä¼šç§‘å­¦` `GPT-3.5` `è°ƒæŸ¥æ¨¡æ‹Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ¨¡æ‹Ÿå’Œé¢„æµ‹ä¸ªä½“åŠç¾¤ä½“å¯¹é€šè´§è†¨èƒ€çš„æ„ŸçŸ¥å’Œé¢„æœŸï¼Œå°¤å…¶æ˜¯åœ¨å¿«é€Ÿå˜åŒ–çš„ç»æµç¯å¢ƒä¸‹ã€‚
2. æœ¬æ–‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹GPT-3.5ï¼Œé€šè¿‡æ¨¡æ‹Ÿè°ƒæŸ¥ç¯å¢ƒï¼Œç ”ç©¶å…¶å¯¹å®è§‚ç»æµä¿¡å·çš„ååº”ï¼Œä»è€Œè¯„ä¼°å…¶é€šèƒ€æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGPTåœ¨çŸ­æœŸå†…èƒ½è¾ƒå¥½åœ°è·Ÿè¸ªæ€»ä½“è°ƒæŸ¥é¢„æµ‹å’Œå®˜æ–¹ç»Ÿè®¡æ•°æ®ï¼Œå¹¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¤ç°äº†äººç±»åœ¨é€šèƒ€æ„ŸçŸ¥ä¸Šçš„è§„å¾‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç‰¹åˆ«æ˜¯GPT-3.5-turboï¼ˆGPTï¼‰ï¼ŒåŸºäºå®è§‚ç»æµä»·æ ¼ä¿¡å·å½¢æˆé€šèƒ€æ„ŸçŸ¥å’Œé¢„æœŸçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†LLMçš„è¾“å‡ºä¸å®¶åº­è°ƒæŸ¥æ•°æ®å’Œå®˜æ–¹ç»Ÿè®¡æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œæ¨¡æ‹Ÿè‹±å›½å¤®è¡Œé€šèƒ€æ€åº¦è°ƒæŸ¥ï¼ˆIASï¼‰çš„ä¿¡æ¯é›†å’Œäººå£ç‰¹å¾ã€‚æˆ‘ä»¬çš„å‡†å®éªŒè®¾è®¡åˆ©ç”¨äº†GPTåœ¨2021å¹´9æœˆçš„è®­ç»ƒæˆªæ­¢æ—¶é—´ï¼Œè¿™æ„å‘³ç€å®ƒä¸äº†è§£éšåçš„è‹±å›½é€šèƒ€é£™å‡ã€‚æˆ‘ä»¬å‘ç°GPTåœ¨çŸ­æœŸå†…è·Ÿè¸ªæ€»ä½“è°ƒæŸ¥é¢„æµ‹å’Œå®˜æ–¹ç»Ÿè®¡æ•°æ®ã€‚åœ¨åˆ†è§£å±‚é¢ï¼ŒGPTå¤åˆ¶äº†å®¶åº­é€šèƒ€æ„ŸçŸ¥çš„å…³é”®ç»éªŒè§„å¾‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¶å…¥ã€ä½æˆ¿ä¿æœ‰æƒå’Œç¤¾ä¼šé˜¶å±‚æ–¹é¢ã€‚ä¸€ç§æ–°é¢–çš„Shapleyå€¼åˆ†è§£æ–¹æ³•é€‚ç”¨äºåˆæˆè°ƒæŸ¥ç¯å¢ƒï¼Œä¸ºä¸æç¤ºå†…å®¹ç›¸å…³çš„æ¨¡å‹è¾“å‡ºé©±åŠ¨å› ç´ æä¾›äº†æ˜ç¡®çš„è§è§£ã€‚æˆ‘ä»¬å‘ç°ï¼ŒGPTè¡¨ç°å‡ºä¸äººç±»å—è®¿è€…ç›¸ä¼¼çš„å¯¹é£Ÿå“é€šèƒ€ä¿¡æ¯çš„æ•æ„Ÿæ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿå‘ç°å®ƒç¼ºä¹ä¸€è‡´çš„æ¶ˆè´¹è€…ä»·æ ¼é€šèƒ€æ¨¡å‹ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ç”¨äºè¯„ä¼°LLMåœ¨ç¤¾ä¼šç§‘å­¦ä¸­çš„è¡Œä¸ºï¼Œæ¯”è¾ƒä¸åŒçš„æ¨¡å‹ï¼Œæˆ–ååŠ©è°ƒæŸ¥è®¾è®¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’Œé¢„æµ‹é€šè´§è†¨èƒ€é¢„æœŸæ—¶ï¼Œå¾€å¾€ä¾èµ–äºä¼ ç»Ÿçš„è®¡é‡ç»æµå­¦æ¨¡å‹æˆ–è°ƒæŸ¥æ•°æ®ï¼Œè¿™äº›æ–¹æ³•éš¾ä»¥æ•æ‰ä¸ªä½“å¼‚è´¨æ€§å’Œå¿«é€Ÿå˜åŒ–çš„å¸‚åœºåŠ¨æ€ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨éç»“æ„åŒ–æ•°æ®ï¼Œå¦‚æ–°é—»æŠ¥é“å’Œç¤¾ä¼šåª’ä½“ä¿¡æ¯ï¼Œè€Œè¿™äº›ä¿¡æ¯å¯èƒ½å¯¹é€šèƒ€é¢„æœŸäº§ç”Ÿé‡è¦å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå°†å…¶ä½œä¸ºä¸€ç§â€œåˆæˆä»£ç†â€ï¼Œæ¨¡æ‹Ÿä¸ªä½“å¯¹é€šè´§è†¨èƒ€çš„æ„ŸçŸ¥å’Œé¢„æœŸã€‚é€šè¿‡å‘LLMè¾“å…¥ç‰¹å®šçš„å®è§‚ç»æµä¿¡æ¯å’Œäººå£ç»Ÿè®¡ç‰¹å¾ï¼Œè§‚å¯Ÿå…¶è¾“å‡ºçš„é€šèƒ€é¢„æœŸï¼Œå¹¶ä¸çœŸå®ä¸–ç•Œçš„è°ƒæŸ¥æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œè¯„ä¼°LLMçš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå°†LLMè§†ä¸ºä¸€ä¸ªå¯æ§çš„å®éªŒå¯¹è±¡ï¼Œé€šè¿‡æ“çºµè¾“å…¥æ¥ç ”ç©¶å…¶å†…éƒ¨çš„â€œè®¤çŸ¥â€è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1ï¼‰æ„å»ºæ¨¡æ‹Ÿè°ƒæŸ¥ç¯å¢ƒï¼ŒåŒ…æ‹¬è®¾è®¡æç¤ºè¯ï¼ˆpromptsï¼‰å’Œé€‰æ‹©äººå£ç»Ÿè®¡ç‰¹å¾ï¼›2ï¼‰å‘GPT-3.5è¾“å…¥æç¤ºè¯ï¼Œç”Ÿæˆé€šèƒ€é¢„æœŸï¼›3ï¼‰å°†GPT-3.5çš„è¾“å‡ºä¸è‹±å›½å¤®è¡Œé€šèƒ€æ€åº¦è°ƒæŸ¥ï¼ˆIASï¼‰çš„çœŸå®æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼›4ï¼‰ä½¿ç”¨Shapleyå€¼åˆ†è§£æ–¹æ³•ï¼Œåˆ†ææç¤ºè¯ä¸­ä¸åŒä¿¡æ¯å¯¹GPT-3.5è¾“å‡ºçš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºé€šè´§è†¨èƒ€é¢„æœŸçš„æ¨¡æ‹Ÿå’Œåˆ†æã€‚ä¸ä¼ ç»Ÿçš„è®¡é‡ç»æµå­¦æ¨¡å‹ç›¸æ¯”ï¼ŒLLMèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†éçº¿æ€§å…³ç³»å’Œå¼‚è´¨æ€§æ•ˆåº”ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„Shapleyå€¼åˆ†è§£æ–¹æ³•ï¼Œç”¨äºåˆ†æLLMè¾“å‡ºçš„å½±å“å› ç´ ï¼Œè¿™ä¸ºç†è§£LLMçš„å†…éƒ¨æœºåˆ¶æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æç¤ºè¯è®¾è®¡æ–¹é¢ï¼Œç ”ç©¶äººå‘˜æ¨¡ä»¿äº†è‹±å›½å¤®è¡Œé€šèƒ€æ€åº¦è°ƒæŸ¥ï¼ˆIASï¼‰çš„é—®å·å½¢å¼ï¼Œå¹¶åŠ å…¥äº†å®è§‚ç»æµä¿¡æ¯ï¼Œå¦‚é€šè´§è†¨èƒ€ç‡ã€å¤±ä¸šç‡ç­‰ã€‚åœ¨Shapleyå€¼åˆ†è§£æ–¹é¢ï¼Œç ”ç©¶äººå‘˜å°†æç¤ºè¯ä¸­çš„ä¸åŒä¿¡æ¯è§†ä¸ºâ€œå‚ä¸è€…â€ï¼Œè®¡ç®—æ¯ä¸ªâ€œå‚ä¸è€…â€å¯¹GPT-3.5è¾“å‡ºçš„è´¡çŒ®ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜åˆ†æäº†GPT-3.5å¯¹ä¸åŒç±»å‹ä¿¡æ¯çš„æ•æ„Ÿæ€§ï¼Œä¾‹å¦‚é£Ÿå“é€šèƒ€ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶å‘ç°ï¼ŒGPT-3.5åœ¨çŸ­æœŸå†…èƒ½å¤Ÿè¾ƒå¥½åœ°è·Ÿè¸ªæ€»ä½“è°ƒæŸ¥é¢„æµ‹å’Œå®˜æ–¹ç»Ÿè®¡æ•°æ®ã€‚åœ¨åˆ†è§£å±‚é¢ï¼ŒGPT-3.5èƒ½å¤Ÿå¤ç°å®¶åº­é€šèƒ€æ„ŸçŸ¥çš„å…³é”®ç»éªŒè§„å¾‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¶å…¥ã€ä½æˆ¿ä¿æœ‰æƒå’Œç¤¾ä¼šé˜¶å±‚æ–¹é¢ã€‚Shapleyå€¼åˆ†è§£ç»“æœè¡¨æ˜ï¼ŒGPT-3.5å¯¹é£Ÿå“é€šèƒ€ä¿¡æ¯è¡¨ç°å‡ºé«˜åº¦æ•æ„Ÿæ€§ï¼Œè¿™ä¸äººç±»å—è®¿è€…çš„è¡Œä¸ºç›¸ä¼¼ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿå‘ç°GPT-3.5ç¼ºä¹ä¸€è‡´çš„æ¶ˆè´¹è€…ä»·æ ¼é€šèƒ€æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ï¼š1) æ”¹è¿›é€šè´§è†¨èƒ€é¢„æœŸçš„å»ºæ¨¡å’Œé¢„æµ‹ï¼›2) è¯„ä¼°ä¸åŒæ”¿ç­–å¯¹é€šèƒ€é¢„æœŸçš„å½±å“ï¼›3) è¾…åŠ©è°ƒæŸ¥è®¾è®¡ï¼Œä¾‹å¦‚ä¼˜åŒ–é—®å·å†…å®¹å’ŒæŠ½æ ·æ–¹æ³•ï¼›4) åˆ©ç”¨LLMè¿›è¡Œç¤¾ä¼šç§‘å­¦ç ”ç©¶ï¼Œä¾‹å¦‚ç ”ç©¶å…¬ä¼—å¯¹æ°”å€™å˜åŒ–ã€å…¬å…±å«ç”Ÿç­‰é—®é¢˜çš„æ€åº¦ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å®è§‚ç»æµå˜é‡çš„é¢„æµ‹å’Œåˆ†æï¼Œå¹¶ä¸å…¶ä»–æœºå™¨å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆï¼Œæé«˜é¢„æµ‹ç²¾åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, mimicking the information set and demographic characteristics of the Bank of England's Inflation Attitudes Survey (IAS). Our quasi-experimental design exploits the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. We find that GPT tracks aggregate survey projections and official statistics at short horizons. At a disaggregated level, GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content. We find that GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents. However, we also find that it lacks a consistent model of consumer price inflation. More generally, our approach could be used to evaluate the behaviour of LLMs for use in the social sciences, to compare different models, or to assist in survey design.

