---
layout: default
title: Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents
---

# Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14142" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14142v1</a>
  <a href="https://arxiv.org/pdf/2512.14142.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14142v1" onclick="toggleFavorite(this, '2512.14142v1', 'Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongqiu Ni, Jiabao Zhang, Guopeng Li, Zilong Wang, Ruiqi Wu, Chi Zhang, Haisheng Tan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 12 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Astraeaï¼šé¢å‘LLMæ™ºèƒ½ä½“çš„çŠ¶æ€æ„ŸçŸ¥è°ƒåº¦å¼•æ“ï¼Œä¼˜åŒ–ç«¯åˆ°ç«¯å»¶è¿Ÿ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LLMæ™ºèƒ½ä½“` `çŠ¶æ€æ„ŸçŸ¥è°ƒåº¦` `ç«¯åˆ°ç«¯å»¶è¿Ÿä¼˜åŒ–` `åˆ†å±‚è°ƒåº¦` `KVç¼“å­˜ç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ¨ç†ç³»ç»Ÿåœ¨å¤„ç†æ™ºèƒ½ä»£ç†çš„å¤šé˜¶æ®µå·¥ä½œæµæ—¶ï¼Œæ— æ³•æœ‰æ•ˆä¼˜åŒ–ç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œå¯¼è‡´å…¨å±€ä½œä¸šå®Œæˆæ—¶é—´è¾ƒé•¿ã€‚
2. Astraeaé€šè¿‡çŠ¶æ€æ„ŸçŸ¥çš„åˆ†å±‚è°ƒåº¦ç®—æ³•ï¼Œç»“åˆè¯·æ±‚å†å²çŠ¶æ€å’Œæœªæ¥é¢„æµ‹ï¼ŒåŠ¨æ€åˆ†ç±»è¯·æ±‚å¹¶ä¼˜åŒ–èµ„æºåˆ†é…ï¼Œä»è€Œé™ä½ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAstraeaç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼Œå¹³å‡ä½œä¸šå®Œæˆæ—¶é—´é™ä½é«˜è¾¾25.5%ï¼Œå¹¶åœ¨é«˜è´Ÿè½½ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²ä¸ºæ™ºèƒ½ä»£ç†ã€‚å®ƒä»¬çš„å¤šé˜¶æ®µå·¥ä½œæµç¨‹åœ¨æœ¬åœ°è®¡ç®—å’Œè°ƒç”¨Web APIç­‰å¤–éƒ¨ç½‘ç»œæœåŠ¡ä¹‹é—´äº¤æ›¿ï¼Œè¿™å¯¼è‡´å®ƒä»¬çš„æ‰§è¡Œæ¨¡å¼ä¸ç°æœ‰æ¨ç†ç³»ç»Ÿï¼ˆå¦‚vLLMï¼‰çš„è°ƒåº¦ç²’åº¦ä¸åŒ¹é…ã€‚ç°æœ‰ç³»ç»Ÿé€šå¸¸ä¾§é‡äºæ¯ä¸ªç‰‡æ®µçš„ä¼˜åŒ–ï¼Œè¿™å¦¨ç¢äº†å®ƒä»¬æœ€å°åŒ–å®Œæ•´ä»£ç†å·¥ä½œæµç¨‹çš„ç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œå³æ•´ä¸ªè¯·æ±‚ç”Ÿå‘½å‘¨æœŸçš„å…¨å±€ä½œä¸šå®Œæˆæ—¶é—´ï¼ˆJCTï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Astraeaï¼Œä¸€ç§æ—¨åœ¨å°†ä¼˜åŒ–ä»æœ¬åœ°ç‰‡æ®µè½¬ç§»åˆ°å…¨å±€è¯·æ±‚ç”Ÿå‘½å‘¨æœŸçš„æœåŠ¡å¼•æ“ã€‚Astraeaé‡‡ç”¨äº†ä¸€ç§çŠ¶æ€æ„ŸçŸ¥çš„åˆ†å±‚è°ƒåº¦ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†è¯·æ±‚çš„å†å²çŠ¶æ€ä¸æœªæ¥é¢„æµ‹ç›¸ç»“åˆã€‚å®ƒæ ¹æ®è¯·æ±‚çš„I/Oå’Œè®¡ç®—å¯†é›†å‹æ€§è´¨åŠ¨æ€åœ°å¯¹è¯·æ±‚è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä½¿ç”¨å¢å¼ºçš„HRRNç­–ç•¥æ¥å¹³è¡¡æ•ˆç‡å’Œå…¬å¹³æ€§ã€‚Astraeaè¿˜å®ç°äº†ä¸€ä¸ªè‡ªé€‚åº”KVç¼“å­˜ç®¡ç†å™¨ï¼Œè¯¥ç®¡ç†å™¨æ ¹æ®ç³»ç»Ÿå†…å­˜å‹åŠ›æ™ºèƒ½åœ°å¤„ç†I/Oç­‰å¾…æœŸé—´çš„ä»£ç†çŠ¶æ€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒAstraeaå°†å¹³å‡JCTé™ä½äº†é«˜è¾¾25.5%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æ¨¡å‹è§„æ¨¡çš„é«˜è´Ÿè½½ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³LLMé©±åŠ¨çš„æ™ºèƒ½ä»£ç†åœ¨å¤šé˜¶æ®µå·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œç°æœ‰æ¨ç†ç³»ç»Ÿæ— æ³•æœ‰æ•ˆä¼˜åŒ–ç«¯åˆ°ç«¯å»¶è¿Ÿçš„é—®é¢˜ã€‚ç°æœ‰ç³»ç»Ÿé€šå¸¸å…³æ³¨å•ä¸ªè®¡ç®—ç‰‡æ®µçš„ä¼˜åŒ–ï¼Œå¿½ç•¥äº†å…¨å±€ä½œä¸šå®Œæˆæ—¶é—´ï¼ˆJCTï¼‰ï¼Œå¯¼è‡´æ•´ä½“æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•æ ¹æ®è¯·æ±‚çš„çŠ¶æ€å’Œæœªæ¥çš„èµ„æºéœ€æ±‚è¿›è¡ŒåŠ¨æ€è°ƒåº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAstraeaçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä¼˜åŒ–ç›®æ ‡ä»å±€éƒ¨ç‰‡æ®µè½¬ç§»åˆ°å…¨å±€è¯·æ±‚ç”Ÿå‘½å‘¨æœŸã€‚é€šè¿‡çŠ¶æ€æ„ŸçŸ¥çš„è°ƒåº¦ç®—æ³•ï¼Œç³»ç»Ÿèƒ½å¤Ÿäº†è§£è¯·æ±‚çš„å†å²æ‰§è¡Œæƒ…å†µå’Œæœªæ¥çš„èµ„æºéœ€æ±‚ï¼Œä»è€Œåšå‡ºæ›´åˆç†çš„è°ƒåº¦å†³ç­–ï¼Œæœ€å°åŒ–å…¨å±€JCTã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ™ºèƒ½ä»£ç†å·¥ä½œæµä¸­è®¡ç®—å’ŒI/Oäº¤æ›¿çš„ç‰¹ç‚¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAstraeaé‡‡ç”¨åˆ†å±‚è°ƒåº¦æ¶æ„ï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è¯·æ±‚åˆ†ç±»å™¨ï¼šæ ¹æ®è¯·æ±‚çš„I/Oå’Œè®¡ç®—å¯†é›†ç¨‹åº¦è¿›è¡ŒåŠ¨æ€åˆ†ç±»ã€‚2) çŠ¶æ€æ„ŸçŸ¥è°ƒåº¦å™¨ï¼šåŸºäºè¯·æ±‚çš„å†å²çŠ¶æ€å’Œæœªæ¥é¢„æµ‹ï¼Œä½¿ç”¨å¢å¼ºçš„HRRNï¼ˆHighest Response Ratio Nextï¼‰ç­–ç•¥è¿›è¡Œè°ƒåº¦ï¼Œå¹³è¡¡æ•ˆç‡å’Œå…¬å¹³æ€§ã€‚3) è‡ªé€‚åº”KVç¼“å­˜ç®¡ç†å™¨ï¼šæ ¹æ®ç³»ç»Ÿå†…å­˜å‹åŠ›ï¼Œæ™ºèƒ½åœ°ç®¡ç†I/Oç­‰å¾…æœŸé—´çš„ä»£ç†çŠ¶æ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šAstraeaçš„å…³é”®åˆ›æ–°åœ¨äºå…¶çŠ¶æ€æ„ŸçŸ¥çš„è°ƒåº¦ç®—æ³•å’Œè‡ªé€‚åº”KVç¼“å­˜ç®¡ç†ã€‚çŠ¶æ€æ„ŸçŸ¥è°ƒåº¦èƒ½å¤Ÿæ ¹æ®è¯·æ±‚çš„åŠ¨æ€å˜åŒ–è°ƒæ•´èµ„æºåˆ†é…ï¼Œè€Œè‡ªé€‚åº”KVç¼“å­˜ç®¡ç†åˆ™èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ç³»ç»Ÿå†…å­˜ï¼Œå‡å°‘I/Oç­‰å¾…æ—¶é—´ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAstraeaèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ™ºèƒ½ä»£ç†å·¥ä½œæµçš„ç‰¹ç‚¹ï¼Œå®ç°å…¨å±€ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šAstraeaçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è¯·æ±‚åˆ†ç±»å™¨çš„åˆ†ç±»æ ‡å‡†ï¼Œä¾‹å¦‚I/Oå¯†é›†å‹å’Œè®¡ç®—å¯†é›†å‹çš„åŒºåˆ†é˜ˆå€¼ã€‚2) å¢å¼ºçš„HRRNç­–ç•¥çš„å…·ä½“å®ç°ï¼Œä¾‹å¦‚å¦‚ä½•æ ¹æ®å†å²çŠ¶æ€å’Œæœªæ¥é¢„æµ‹è°ƒæ•´ä¼˜å…ˆçº§ã€‚3) è‡ªé€‚åº”KVç¼“å­˜ç®¡ç†å™¨çš„ç¼“å­˜æ›¿æ¢ç­–ç•¥ï¼Œä¾‹å¦‚LRUï¼ˆLeast Recently Usedï¼‰æˆ–LFUï¼ˆLeast Frequently Usedï¼‰çš„å˜ä½“ï¼Œä»¥åŠå¦‚ä½•æ ¹æ®å†…å­˜å‹åŠ›åŠ¨æ€è°ƒæ•´ç¼“å­˜å¤§å°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAstraeaç›¸æ¯”äºåŸºçº¿æ–¹æ³•ï¼Œå¹³å‡ä½œä¸šå®Œæˆæ—¶é—´ï¼ˆJCTï¼‰é™ä½äº†é«˜è¾¾25.5%ã€‚æ­¤å¤–ï¼ŒAstraeaåœ¨é«˜è´Ÿè½½æƒ…å†µä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§å’Œç¨³å®šæ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å„ç§æ¨¡å‹è§„æ¨¡çš„éœ€æ±‚ã€‚è¿™äº›ç»“æœéªŒè¯äº†Astraeaåœ¨ä¼˜åŒ–LLMé©±åŠ¨çš„æ™ºèƒ½ä»£ç†æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Astraeaé€‚ç”¨äºå„ç§éœ€è¦LLMé©±åŠ¨çš„æ™ºèƒ½ä»£ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨åŒ–æµç¨‹ç®¡ç†ã€æ™ºèƒ½å®¶å±…æ§åˆ¶ç­‰ã€‚é€šè¿‡ä¼˜åŒ–ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ŒAstraeaå¯ä»¥æé«˜ç”¨æˆ·ä½“éªŒï¼Œé™ä½è¿è¥æˆæœ¬ï¼Œå¹¶ä¿ƒè¿›æ™ºèƒ½ä»£ç†åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ã€‚æœªæ¥ï¼ŒAstraeaå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ”¯æŒæ›´å¤æ‚çš„ä»£ç†å·¥ä½œæµå’Œå¼‚æ„è®¡ç®—ç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.

