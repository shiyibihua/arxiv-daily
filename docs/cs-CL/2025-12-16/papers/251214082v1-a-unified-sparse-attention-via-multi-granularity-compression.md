---
layout: default
title: A Unified Sparse Attention via Multi-Granularity Compression
---

# A Unified Sparse Attention via Multi-Granularity Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14082" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14082v1</a>
  <a href="https://arxiv.org/pdf/2512.14082.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14082v1" onclick="toggleFavorite(this, '2512.14082v1', 'A Unified Sparse Attention via Multi-Granularity Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siran Liu, Zane Cao, Yongchao He

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniSparseï¼šä¸€ç§é€šè¿‡å¤šç²’åº¦å‹ç¼©å®ç°çš„ç»Ÿä¸€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ é€ŸLLMé•¿æ–‡æœ¬å¤„ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–æ³¨æ„åŠ›` `é•¿æ–‡æœ¬å¤„ç†` `å¤šç²’åº¦å‹ç¼©` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•åœ¨æ•ˆç‡ã€é€šç”¨æ€§å’Œè®­ç»ƒæˆæœ¬ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œé™åˆ¶äº†å…¶åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­çš„åº”ç”¨ã€‚
2. UniSparseé€šè¿‡å¼•å…¥å¤åˆtokenå’Œå¤šç²’åº¦å‹ç¼©ï¼Œå®ç°äº†åŠ¨æ€ã€é«˜æ•ˆä¸”ç¡¬ä»¶å‹å¥½çš„ç¨€ç–æ³¨æ„åŠ›æ„å»ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUniSparseåœ¨å¤šç§æ¨¡æ€å’Œä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æå‡äº†æ•ˆç‡å’Œå‡†ç¡®ç‡ï¼Œä¼˜äºç°æœ‰ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè½®å¯¹è¯å’Œç¨‹åºåˆ†æç­‰åº”ç”¨ä¸­å¯¹é•¿ä¸Šä¸‹æ–‡çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUniSparseçš„ç»Ÿä¸€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚ç°æœ‰ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•å­˜åœ¨æƒè¡¡ï¼šåŸºäºè®­ç»ƒçš„æ–¹æ³•æˆæœ¬é«˜æ˜‚ï¼Œä¸èƒ½ç›´æ¥ä½œä¸ºåŠ é€Ÿæ’ä»¶åº”ç”¨äºå…¶ä»–æ¨¡å‹ï¼›è€Œæ¨ç†æ—¶æ–¹æ³•é€šå¸¸ä¼šç‰ºç‰²æ•ˆç‡æˆ–è·¨æ¨¡æ€é€šç”¨æ€§ã€‚UniSparseå¼•å…¥äº†å¤åˆtokençš„æ¦‚å¿µï¼Œå³èšåˆå¤šç²’åº¦ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç´§å‡‘è¡¨ç¤ºã€‚åŸºäºæ­¤ï¼ŒUniSparseé€šè¿‡å¤šç²’åº¦å‹ç¼©å’Œå—çº§é€‰æ‹©åŠ¨æ€æ„å»ºç¨€ç–æ³¨æ„åŠ›ï¼Œä»è€Œåœ¨GPUä¸Šå®ç°é«˜æ•ˆä¸”ç¡¬ä»¶å‹å¥½çš„æ‰§è¡Œã€‚åœ¨ä»åˆæˆåŸºå‡†åˆ°å®é™…åº”ç”¨çš„å¤šç§æ¨¡æ€å’Œä»»åŠ¡ä¸­ï¼ŒUniSparseå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼ˆå¦‚MInferenceã€XAttentionã€FlexPrefillï¼‰ï¼Œåœ¨è¾¾åˆ°â‰¥99%çš„å®Œæ•´æ³¨æ„åŠ›å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æ¯”FlashAttentionå¿«é«˜è¾¾2.61å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦å‘ˆå¹³æ–¹å¢é•¿ï¼Œæˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œè¦ä¹ˆéœ€è¦é¢å¤–çš„è®­ç»ƒæˆæœ¬ï¼Œè¦ä¹ˆåœ¨æ•ˆç‡å’Œé€šç”¨æ€§ä¸Šæœ‰æ‰€å¦¥åï¼Œéš¾ä»¥åŒæ—¶æ»¡è¶³é«˜æ€§èƒ½å’Œæ˜“ç”¨æ€§çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniSparseçš„æ ¸å¿ƒåœ¨äºå¼•å…¥â€œå¤åˆtokenâ€çš„æ¦‚å¿µï¼Œå°†å¤šä¸ªtokenå‹ç¼©æˆä¸€ä¸ªæ›´ç´§å‡‘çš„è¡¨ç¤ºï¼Œä»è€Œé™ä½æ³¨æ„åŠ›è®¡ç®—çš„è§„æ¨¡ã€‚é€šè¿‡å¤šç²’åº¦å‹ç¼©ï¼Œæ¨¡å‹å¯ä»¥çµæ´»åœ°é€‰æ‹©ä¸åŒç²’åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåœ¨æ•ˆç‡å’Œä¿¡æ¯æŸå¤±ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniSparseä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **å¤šç²’åº¦å‹ç¼©**ï¼šå°†åŸå§‹tokenåºåˆ—å‹ç¼©æˆä¸åŒç²’åº¦çš„å¤åˆtokenåºåˆ—ã€‚2) **å—çº§é€‰æ‹©**ï¼šæ ¹æ®ä¸€å®šçš„ç­–ç•¥ï¼Œé€‰æ‹©é‡è¦çš„å¤åˆtokenå—è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚3) **æ³¨æ„åŠ›è®¡ç®—**ï¼šåœ¨é€‰å®šçš„å¤åˆtokenå—ä¸Šæ‰§è¡Œç¨€ç–æ³¨æ„åŠ›è®¡ç®—ã€‚4) **ä¿¡æ¯èšåˆ**ï¼šå°†å¤åˆtokençš„ä¿¡æ¯è§£å‹å¹¶èšåˆåˆ°åŸå§‹tokenä¸Šã€‚

**å…³é”®åˆ›æ–°**ï¼šUniSparseçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡å¤šç²’åº¦å‹ç¼©åŠ¨æ€åœ°æ„å»ºç¨€ç–æ³¨æ„åŠ›ã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼ŒUniSparseæ— éœ€é¢å¤–çš„è®­ç»ƒï¼Œå³å¯ä½œä¸ºæ’ä»¶å¼åŠ é€Ÿæ¨¡å—åº”ç”¨äºå„ç§æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å¤šç§æ¨¡æ€å’Œä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šUniSparseçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **å¤šç²’åº¦å‹ç¼©ç­–ç•¥**ï¼šå¯ä»¥ä½¿ç”¨å¹³å‡æ± åŒ–ã€æœ€å¤§æ± åŒ–ç­‰æ–¹æ³•è¿›è¡Œå‹ç¼©ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å¯å­¦ä¹ çš„å‹ç¼©æ¨¡å—ã€‚2) **å—çº§é€‰æ‹©ç­–ç•¥**ï¼šå¯ä»¥ä½¿ç”¨åŸºäºé‡è¦æ€§çš„é€‰æ‹©ã€åŸºäºè·ç¦»çš„é€‰æ‹©ç­‰ç­–ç•¥ã€‚3) **æ³¨æ„åŠ›è®¡ç®—æ–¹å¼**ï¼šå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–é«˜æ•ˆçš„æ³¨æ„åŠ›å˜ä½“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UniSparseåœ¨å¤šä¸ªæ¨¡æ€å’Œä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUniSparseè¾¾åˆ°äº†â‰¥99%çš„å®Œæ•´æ³¨æ„åŠ›å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æ¯”FlashAttentionå¿«é«˜è¾¾2.61å€ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒUniSparseä¹Ÿä¼˜äºå…¶ä»–ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œä¾‹å¦‚MInferenceã€XAttentionå’ŒFlexPrefillã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniSparseå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿æ–‡æœ¬çš„åœºæ™¯ï¼Œä¾‹å¦‚å¤šè½®å¯¹è¯ã€ç¨‹åºåˆ†æã€æ–‡æ¡£æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒUniSparseå¯ä»¥æ˜¾è‘—æå‡LLMåœ¨è¿™äº›åœºæ™¯ä¸­çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå¹¶é™ä½éƒ¨ç½²æˆæœ¬ã€‚æœªæ¥ï¼ŒUniSparseæœ‰æœ›æˆä¸ºLLMé•¿æ–‡æœ¬å¤„ç†çš„æ ‡å‡†åŠ é€Ÿæ¨¡å—ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\ge$ 99% of full-attention accuracy and up to 2.61$\times$ faster attention computation than FlashAttention.

