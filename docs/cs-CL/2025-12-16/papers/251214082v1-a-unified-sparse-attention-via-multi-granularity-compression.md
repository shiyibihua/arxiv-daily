---
layout: default
title: A Unified Sparse Attention via Multi-Granularity Compression
---

# A Unified Sparse Attention via Multi-Granularity Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14082" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14082v1</a>
  <a href="https://arxiv.org/pdf/2512.14082.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14082v1" onclick="toggleFavorite(this, '2512.14082v1', 'A Unified Sparse Attention via Multi-Granularity Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siran Liu, Zane Cao, Yongchao He

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniSparseä»¥è§£å†³é•¿åºåˆ—è‡ªæ³¨æ„åŠ›è®¡ç®—ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿åºåˆ—å¤„ç†` `ç¨€ç–æ³¨æ„åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šç²’åº¦å‹ç¼©` `å¤åˆæ ‡è®°` `è®¡ç®—æ•ˆç‡` `ç¨‹åºåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é¢ä¸´æ•ˆç‡ä¸å‡†ç¡®ç‡çš„æƒè¡¡ï¼Œéš¾ä»¥æ»¡è¶³é•¿åºåˆ—å¤„ç†çš„éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºUniSparseï¼Œé€šè¿‡å¼•å…¥å¤åˆæ ‡è®°å’Œå¤šç²’åº¦å‹ç¼©ï¼ŒåŠ¨æ€æ„å»ºç¨€ç–æ³¨æ„åŠ›ï¼Œæå‡è®¡ç®—æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUniSparseåœ¨å¤šä¸ªåŸºå‡†å’Œå®é™…åº”ç”¨ä¸­ï¼Œå‡†ç¡®ç‡è¶…è¿‡99%ï¼Œè®¡ç®—é€Ÿåº¦æ¯”ç°æœ‰æ–¹æ³•å¿«2.61å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡ç†è§£ä¸æ¨ç†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨å¦‚å¤šè½®å¯¹è¯å’Œç¨‹åºåˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ ¸å¿ƒè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦éšåºåˆ—é•¿åº¦å‘ˆå¹³æ–¹å¢é•¿ï¼Œå½¢æˆäº†æ ¹æœ¬çš„è®¡ç®—ç“¶é¢ˆã€‚ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•è™½ç„¶ç¼“è§£äº†è¿™ä¸€é—®é¢˜ï¼Œä½†å­˜åœ¨è®­ç»ƒæˆæœ¬é«˜æˆ–æ¨ç†æ•ˆç‡ä½ç­‰æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†UniSparseï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€æœºåˆ¶ï¼Œå¼•å…¥äº†å¤åˆæ ‡è®°çš„æ¦‚å¿µï¼Œèšåˆå¤šç²’åº¦ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åŸºäºè¿™ä¸€æŠ½è±¡ï¼ŒUniSparseé€šè¿‡å¤šç²’åº¦å‹ç¼©å’Œå—çº§é€‰æ‹©åŠ¨æ€æ„å»ºç¨€ç–æ³¨æ„åŠ›ï¼Œå®ç°äº†é«˜æ•ˆä¸”é€‚åˆGPUæ‰§è¡Œçš„æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniSparseåœ¨å¤šä¸ªæ¨¡æ€å’Œä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œå‡†ç¡®ç‡è¾¾åˆ°å…¨æ³¨æ„åŠ›çš„99%ä»¥ä¸Šï¼Œè®¡ç®—é€Ÿåº¦æ¯”FlashAttentionå¿«2.61å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶è‡ªæ³¨æ„åŠ›è®¡ç®—çš„é«˜å¤æ‚åº¦é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œå‡†ç¡®ç‡ä¸è¶³çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniSparseçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥å¤åˆæ ‡è®°ï¼Œèšåˆå¤šç²’åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå®ç°åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›çš„æ„å»ºã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜è®¡ç®—æ•ˆç‡å¹¶é™ä½èµ„æºæ¶ˆè€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniSparseçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤åˆæ ‡è®°ç”Ÿæˆã€å¤šç²’åº¦å‹ç¼©å’Œå—çº§é€‰æ‹©ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚å¤åˆæ ‡è®°ç”¨äºè¡¨ç¤ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¤šç²’åº¦å‹ç¼©åˆ™ç”¨äºå‡å°‘è®¡ç®—é‡ï¼Œå—çº§é€‰æ‹©ç¡®ä¿äº†é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šUniSparseçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å¤åˆæ ‡è®°çš„å¼•å…¥å’ŒåŠ¨æ€ç¨€ç–æ³¨æ„åŠ›çš„æ„å»ºæ–¹å¼ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„é™æ€ç¨€ç–ç­–ç•¥å½¢æˆäº†æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´çµæ´»åœ°é€‚åº”ä¸åŒçš„ä¸Šä¸‹æ–‡éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒUniSparseé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®ä»¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œå¹¶åˆ©ç”¨é€‚åº”æ€§æŸå¤±å‡½æ•°æ¥å¹³è¡¡å‡†ç¡®ç‡ä¸è®¡ç®—é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æ”¯æŒå¤šæ¨¡æ€æ•°æ®çš„å¤„ç†ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14082v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14082v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14082v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUniSparseåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œå¦‚MInferenceã€XAttentionå’ŒFlexPrefillï¼Œå‡†ç¡®ç‡è¾¾åˆ°å…¨æ³¨æ„åŠ›çš„99%ä»¥ä¸Šï¼Œä¸”è®¡ç®—é€Ÿåº¦æ¯”FlashAttentionå¿«2.61å€ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè½®å¯¹è¯ç³»ç»Ÿã€ç¨‹åºåˆ†æã€é•¿æ–‡æœ¬ç†è§£ç­‰ã€‚UniSparseçš„é«˜æ•ˆè®¡ç®—èƒ½åŠ›å’Œå‡†ç¡®æ€§ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\ge$ 99% of full-attention accuracy and up to 2.61$\times$ faster attention computation than FlashAttention.

