---
layout: default
title: VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse
---

# VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14531v1</a>
  <a href="https://arxiv.org/pdf/2512.14531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14531v1" onclick="toggleFavorite(this, '2512.14531v1', 'VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ying Nie, Kai Han, Hongguang Li, Hang Zhou, Tianyu Guo, Enhua Wu, Xinghao Chen, Yunhe Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/huawei-noah/noah-research/tree)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VersatileFFNï¼šé€šè¿‡è‡ªé€‚åº”å®½æ·±å¤ç”¨æå‡LLMçš„å‚æ•°æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å‰é¦ˆç½‘ç»œ` `å‚æ•°å¤ç”¨` `å®½åº¦æ·±åº¦` `è‡ªé€‚åº”` `è®¤çŸ¥åŒè¿‡ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå‚æ•°é«˜æ•ˆæ–¹æ³•ä¸»è¦é€šè¿‡å‹ç¼©é¢„è®­ç»ƒæ¨¡å‹ï¼Œéš¾ä»¥çªç ´åŸºç¡€æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ä¸Šé™ã€‚
2. VersatileFFNé€šè¿‡å®½åº¦å’Œæ·±åº¦ä¸¤ä¸ªç»´åº¦ä¸Šçš„å‚æ•°å¤ç”¨ï¼Œåœ¨å›ºå®šå‚æ•°é¢„ç®—ä¸‹æå‡æ¨¡å‹å®¹é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVersatileFFNåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹è§„æ¨¡ä¸Šå‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿæ‰©å±•å¸¦æ¥äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†ä¹Ÿå¯¼è‡´äº†å·¨å¤§çš„å†…å­˜æˆæœ¬ã€‚ç°æœ‰çš„å‚æ•°é«˜æ•ˆæ–¹æ³•ï¼Œå¦‚å‰ªæå’Œé‡åŒ–ï¼Œä¸»è¦æ˜¯åœ¨ä¸å¢å¼ºæ¶æ„èƒ½åŠ›çš„æƒ…å†µä¸‹å‹ç¼©é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œè§¦åŠäº†åŸºç¡€æ¨¡å‹çš„è¡¨å¾ä¸Šé™ã€‚æœ¬æ–‡æå‡ºäº†VersatileFFNï¼Œä¸€ç§æ–°é¢–çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œå®ƒèƒ½å¤Ÿåœ¨å›ºå®šå‚æ•°é¢„ç®—å†…çµæ´»åœ°å¤ç”¨å®½åº¦å’Œæ·±åº¦ç»´åº¦ä¸Šçš„å‚æ•°ã€‚å—åˆ°è®¤çŸ¥åŒè¿‡ç¨‹ç†è®ºçš„å¯å‘ï¼ŒVersatileFFNåŒ…å«ä¸¤ä¸ªè‡ªé€‚åº”è·¯å¾„ï¼šä¸€ä¸ªå®½åº¦å¤šåŠŸèƒ½è·¯å¾„ï¼Œä»å•ä¸ªå…±äº«FFNç”Ÿæˆå­ä¸“å®¶æ··åˆï¼Œæ¨¡æ‹Ÿç¨€ç–ä¸“å®¶è·¯ç”±è€Œä¸å¢åŠ å‚æ•°ï¼›ä¸€ä¸ªæ·±åº¦å¤šåŠŸèƒ½è·¯å¾„ï¼Œé€’å½’åœ°åº”ç”¨ç›¸åŒçš„FFNæ¥æ¨¡æ‹Ÿæ›´æ·±å±‚æ¬¡çš„å¤„ç†ï¼Œä»¥åº”å¯¹å¤æ‚çš„tokenã€‚ä¸€ä¸ªéš¾åº¦æ„ŸçŸ¥é—¨æ§åŠ¨æ€åœ°å¹³è¡¡è¿™ä¸¤ä¸ªè·¯å¾„ï¼Œå¼•å¯¼â€œç®€å•â€çš„tokené€šè¿‡é«˜æ•ˆçš„å®½åº¦è·¯å¾„ï¼Œå¹¶ä¸ºâ€œå›°éš¾â€çš„tokenåˆ†é…æ›´æ·±å±‚æ¬¡çš„è¿­ä»£ç»†åŒ–ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œè¿™ä¸¤ä¸ªè·¯å¾„éƒ½å¤ç”¨ç›¸åŒçš„å‚æ•°ï¼Œå› æ­¤æ‰€æœ‰é¢å¤–çš„å®¹é‡éƒ½æ¥è‡ªè®¡ç®—è€Œéå†…å­˜ã€‚åœ¨ä¸åŒçš„åŸºå‡†å’Œæ¨¡å‹è§„æ¨¡ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶æ€§èƒ½å“è¶Šï¼Œä½†å…¶åºå¤§çš„å‚æ•°é‡å¯¼è‡´äº†å·¨å¤§çš„å†…å­˜å¼€é”€ã€‚ç°æœ‰çš„å‚æ•°é«˜æ•ˆæ–¹æ³•ï¼Œå¦‚å‰ªæå’Œé‡åŒ–ï¼Œä¸»è¦é›†ä¸­åœ¨å‹ç¼©é¢„è®­ç»ƒæ¨¡å‹ä¸Šï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹æ¶æ„æœ¬èº«çš„å¢å¼ºï¼Œå› æ­¤éš¾ä»¥çªç ´åŸºç¡€æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ä¸Šé™ã€‚è¿™äº›æ–¹æ³•æ— æ³•åœ¨ä¸æ˜¾è‘—é™ä½æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œæœ‰æ•ˆé™ä½å†…å­˜éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVersatileFFNçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨å›ºå®šå‚æ•°é¢„ç®—ä¸‹ï¼Œé€šè¿‡å‚æ•°çš„çµæ´»å¤ç”¨ï¼ŒåŒæ—¶æå‡æ¨¡å‹çš„å®½åº¦å’Œæ·±åº¦ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å®¹é‡å’Œè¡¨è¾¾èƒ½åŠ›ã€‚å®ƒå€Ÿé‰´äº†è®¤çŸ¥åŒè¿‡ç¨‹ç†è®ºï¼Œæ¨¡æ‹Ÿäººç±»çš„å¿«é€Ÿç›´è§‰å’Œæ·±åº¦æ€è€ƒä¸¤ç§è®¤çŸ¥æ¨¡å¼ï¼Œè®¾è®¡äº†å®½åº¦å¤šåŠŸèƒ½è·¯å¾„å’Œæ·±åº¦å¤šåŠŸèƒ½è·¯å¾„ï¼Œå¹¶ä½¿ç”¨éš¾åº¦æ„ŸçŸ¥é—¨æ§æœºåˆ¶åŠ¨æ€åœ°å¹³è¡¡è¿™ä¸¤æ¡è·¯å¾„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVersatileFFNä¸»è¦åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šå®½åº¦å¤šåŠŸèƒ½è·¯å¾„ï¼ˆWidth-Versatile Pathï¼‰ã€æ·±åº¦å¤šåŠŸèƒ½è·¯å¾„ï¼ˆDepth-Versatile Pathï¼‰å’Œéš¾åº¦æ„ŸçŸ¥é—¨æ§ï¼ˆDifficulty-Aware Gatingï¼‰ã€‚å®½åº¦å¤šåŠŸèƒ½è·¯å¾„é€šè¿‡å…±äº«çš„FFNç”Ÿæˆå­ä¸“å®¶æ··åˆï¼Œæ¨¡æ‹Ÿç¨€ç–ä¸“å®¶è·¯ç”±ï¼›æ·±åº¦å¤šåŠŸèƒ½è·¯å¾„é€’å½’åœ°åº”ç”¨ç›¸åŒçš„FFNï¼Œæ¨¡æ‹Ÿæ›´æ·±å±‚æ¬¡çš„å¤„ç†ã€‚éš¾åº¦æ„ŸçŸ¥é—¨æ§æ ¹æ®è¾“å…¥tokençš„éš¾åº¦ï¼ŒåŠ¨æ€åœ°è°ƒæ•´ä¸¤æ¡è·¯å¾„çš„æƒé‡ï¼Œå°†â€œç®€å•â€çš„tokenå¼•å¯¼åˆ°å®½åº¦è·¯å¾„ï¼Œå°†â€œå›°éš¾â€çš„tokenå¼•å¯¼åˆ°æ·±åº¦è·¯å¾„ã€‚

**å…³é”®åˆ›æ–°**ï¼šVersatileFFNçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å‚æ•°å¤ç”¨æœºåˆ¶ï¼Œå®ƒåœ¨å®½åº¦å’Œæ·±åº¦ä¸¤ä¸ªç»´åº¦ä¸Šéƒ½å®ç°äº†å‚æ•°çš„å…±äº«å’Œå¤ç”¨ã€‚å®½åº¦å¤šåŠŸèƒ½è·¯å¾„é€šè¿‡å…±äº«FFNç”Ÿæˆå¤šä¸ªå­ä¸“å®¶ï¼Œæ·±åº¦å¤šåŠŸèƒ½è·¯å¾„é€šè¿‡é€’å½’åº”ç”¨ç›¸åŒçš„FFNæ¥æ¨¡æ‹Ÿæ›´æ·±å±‚æ¬¡çš„å¤„ç†ã€‚è¿™ç§å‚æ•°å¤ç”¨æ–¹å¼ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸å¢åŠ å‚æ•°é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡æ¨¡å‹çš„å®¹é‡å’Œè¡¨è¾¾èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œéš¾åº¦æ„ŸçŸ¥é—¨æ§ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®è¾“å…¥tokençš„éš¾åº¦ï¼ŒåŠ¨æ€åœ°è°ƒæ•´ä¸¤æ¡è·¯å¾„çš„æƒé‡ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„è®¡ç®—ã€‚

**å…³é”®è®¾è®¡**ï¼šå®½åº¦å¤šåŠŸèƒ½è·¯å¾„ä½¿ç”¨ä¸€ä¸ªå…±äº«çš„FFNï¼Œé€šè¿‡ä¸åŒçš„çº¿æ€§å˜æ¢ç”Ÿæˆå¤šä¸ªå­ä¸“å®¶ã€‚æ·±åº¦å¤šåŠŸèƒ½è·¯å¾„é€’å½’åœ°åº”ç”¨ç›¸åŒçš„FFNï¼Œé€’å½’æ¬¡æ•°å¯ä»¥æ ¹æ®è®¡ç®—èµ„æºè¿›è¡Œè°ƒæ•´ã€‚éš¾åº¦æ„ŸçŸ¥é—¨æ§ä½¿ç”¨ä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œæ¥é¢„æµ‹è¾“å…¥tokençš„éš¾åº¦ï¼Œå¹¶æ ¹æ®éš¾åº¦å€¼è®¡ç®—ä¸¤æ¡è·¯å¾„çš„æƒé‡ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚å…·ä½“å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14531v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14531v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14531v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVersatileFFNåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹è§„æ¨¡ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒVersatileFFNèƒ½å¤Ÿåœ¨å‚æ•°é‡ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå°†æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜å‡ ä¸ªç™¾åˆ†ç‚¹ã€‚ä¸ä¼ ç»Ÿçš„å‚æ•°é«˜æ•ˆæ–¹æ³•ç›¸æ¯”ï¼ŒVersatileFFNèƒ½å¤Ÿåœ¨æ›´å°çš„å‚æ•°é‡ä¸‹è¾¾åˆ°æ›´é«˜çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VersatileFFNå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸‹ã€‚å®ƒå¯ä»¥ç”¨äºç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿç­‰è®¡ç®—èƒ½åŠ›æœ‰é™çš„å¹³å°ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒVersatileFFNè¿˜å¯ä»¥åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨LLMåœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering "easy" tokens through the efficient width-wise route and allocating deeper iterative refinement to "hard" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.

