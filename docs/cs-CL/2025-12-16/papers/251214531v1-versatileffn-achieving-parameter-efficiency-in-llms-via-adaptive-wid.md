---
layout: default
title: VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse
---

# VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14531v1</a>
  <a href="https://arxiv.org/pdf/2512.14531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14531v1" onclick="toggleFavorite(this, '2512.14531v1', 'VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ying Nie, Kai Han, Hongguang Li, Hang Zhou, Tianyu Guo, Enhua Wu, Xinghao Chen, Yunhe Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/huawei-noah/noah-research/tree)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVersatileFFNä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å‚æ•°æ•ˆç‡` `å‰é¦ˆç½‘ç»œ` `åŠ¨æ€é—¨æ§` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹å‹ç¼©` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‚æ•°é«˜æ•ˆæ–¹æ³•å¦‚å‰ªæå’Œé‡åŒ–æœªèƒ½å¢å¼ºæ¨¡å‹æ¶æ„èƒ½åŠ›ï¼Œå¯¼è‡´è¡¨ç°å—é™ã€‚
2. VersatileFFNé€šè¿‡å®½åº¦å’Œæ·±åº¦çš„çµæ´»å‚æ•°é‡ç”¨ï¼Œæå‡äº†å¤§è¯­è¨€æ¨¡å‹çš„å¤„ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVersatileFFNåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿæ‰©å±•ï¼Œè™½ç„¶å…¶æ€§èƒ½æ˜¾è‘—æå‡ï¼Œä½†ä¹Ÿå¯¼è‡´äº†å·¨å¤§çš„å†…å­˜æˆæœ¬ã€‚ç°æœ‰çš„å‚æ•°é«˜æ•ˆæ–¹æ³•å¦‚å‰ªæå’Œé‡åŒ–ä¸»è¦æ˜¯å‹ç¼©é¢„è®­ç»ƒæ¨¡å‹ï¼Œè€Œæœªèƒ½å¢å¼ºæ¶æ„èƒ½åŠ›ï¼Œé™åˆ¶äº†åŸºç¡€æ¨¡å‹çš„è¡¨ç°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å‰é¦ˆç½‘ç»œVersatileFFNï¼Œèƒ½å¤Ÿåœ¨å›ºå®šå‚æ•°é¢„ç®—å†…çµæ´»é‡ç”¨å®½åº¦å’Œæ·±åº¦ç»´åº¦çš„å‚æ•°ã€‚VersatileFFNåŒ…å«ä¸¤ä¸ªè‡ªé€‚åº”è·¯å¾„ï¼šå®½åº¦çµæ´»è·¯å¾„ç”Ÿæˆæ¥è‡ªå•ä¸€å…±äº«FFNçš„å­ä¸“å®¶æ··åˆï¼Œæ¨¡æ‹Ÿç¨€ç–ä¸“å®¶è·¯ç”±è€Œä¸å¢åŠ å‚æ•°ï¼›æ·±åº¦çµæ´»è·¯å¾„é€’å½’åº”ç”¨ç›¸åŒçš„FFNä»¥æ¨¡æ‹Ÿå¤æ‚æ ‡è®°çš„æ·±å±‚å¤„ç†ã€‚åŠ¨æ€çš„å›°éš¾æ„ŸçŸ¥é—¨æ§å¹³è¡¡è¿™ä¸¤æ¡è·¯å¾„ï¼Œåˆç†åˆ†é…å¤„ç†èµ„æºã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šç§åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹è§„æ¨¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ‰©å±•è¿‡ç¨‹ä¸­é¢ä¸´çš„å†…å­˜æˆæœ¬é«˜å’Œå‚æ•°æ•ˆç‡ä½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚å‰ªæå’Œé‡åŒ–è™½ç„¶èƒ½å‹ç¼©æ¨¡å‹ï¼Œä½†æœªèƒ½æå‡æ¨¡å‹çš„æ¶æ„èƒ½åŠ›ï¼Œå¯¼è‡´è¡¨ç°å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVersatileFFNçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å®½åº¦å’Œæ·±åº¦çš„çµæ´»å‚æ•°é‡ç”¨ï¼Œæ¥æå‡æ¨¡å‹çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†è®¤çŸ¥çš„åŒè¿‡ç¨‹ç†è®ºï¼Œè®¾è®¡äº†ä¸¤ä¸ªè‡ªé€‚åº”è·¯å¾„ï¼Œä»¥å®ç°é«˜æ•ˆçš„å‚æ•°åˆ©ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVersatileFFNçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå®½åº¦çµæ´»è·¯å¾„å’Œæ·±åº¦çµæ´»è·¯å¾„ã€‚å®½åº¦çµæ´»è·¯å¾„ç”Ÿæˆå¤šä¸ªå­ä¸“å®¶ï¼Œè€Œæ·±åº¦çµæ´»è·¯å¾„åˆ™é€’å½’åº”ç”¨ç›¸åŒçš„FFNä»¥å¤„ç†å¤æ‚æ ‡è®°ã€‚åŠ¨æ€çš„å›°éš¾æ„ŸçŸ¥é—¨æ§åœ¨è¿™ä¸¤æ¡è·¯å¾„ä¹‹é—´è¿›è¡Œå¹³è¡¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šVersatileFFNçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å‚æ•°é‡ç”¨æœºåˆ¶ï¼Œå…è®¸åœ¨ä¸å¢åŠ å‚æ•°çš„æƒ…å†µä¸‹å®ç°æ›´æ·±å±‚æ¬¡çš„å¤„ç†ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å‚æ•°å‹ç¼©æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŠ¨æ€å›°éš¾æ„ŸçŸ¥é—¨æ§æœºåˆ¶ï¼Œä»¥æ ¹æ®è¾“å…¥æ ‡è®°çš„å¤æ‚æ€§åŠ¨æ€è°ƒæ•´è·¯å¾„é€‰æ‹©ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰è·¯å¾„å‡é‡ç”¨ç›¸åŒçš„å‚æ•°ï¼Œç¡®ä¿é¢å¤–çš„èƒ½åŠ›æ¥è‡ªè®¡ç®—è€Œéå†…å­˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVersatileFFNå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚è¾“å…¥æ—¶ï¼Œæ¨¡å‹çš„æ•ˆç‡æé«˜äº†20%ä»¥ä¸Šã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç›¸åŒå‚æ•°é¢„ç®—ä¸‹å®ç°äº†æ›´æ·±å±‚æ¬¡çš„å¤„ç†èƒ½åŠ›ï¼Œå±•ç°äº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VersatileFFNåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„å‚æ•°åˆ©ç”¨æ–¹å¼å¯ä»¥å¸®åŠ©å¼€å‘æ›´ä¸ºå¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œé™ä½å†…å­˜éœ€æ±‚ï¼Œæå‡æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹è®¾è®¡å’Œä¼˜åŒ–ç­–ç•¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering "easy" tokens through the efficient width-wise route and allocating deeper iterative refinement to "hard" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.

