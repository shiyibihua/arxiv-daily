---
layout: default
title: LongReasonArena: A Long Reasoning Benchmark for Large Language Models
---

# LongReasonArena: A Long Reasoning Benchmark for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19363" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19363v1</a>
  <a href="https://arxiv.org/pdf/2508.19363.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19363v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19363v1', 'LongReasonArena: A Long Reasoning Benchmark for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiayu Ding, Shuming Ma, Lei Cui, Nanning Zheng, Furu Wei

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/LongReasonArena/LongReasonArena)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLongReasonArenaä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ¨ç†èƒ½åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŸºå‡†è¯„ä¼°` `å¤šæ­¥éª¤ç®—æ³•` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é•¿æ–‡æœ¬åŸºå‡†æœªèƒ½æœ‰æ•ˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿æ¨ç†èƒ½åŠ›ï¼Œå­˜åœ¨æ˜æ˜¾çš„è¯„ä¼°ç©ºç™½ã€‚
2. LongReasonArenaé€šè¿‡è®¾è®¡å¤šæ­¥éª¤ç®—æ³•ä»»åŠ¡ï¼Œä¸“æ³¨äºé•¿æ¨ç†çš„å…³é”®æ–¹é¢ï¼Œå¦‚æ£€ç´¢å’Œå›æº¯ï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨LongReasonArenaä¸Šçš„è¡¨ç°ä¸ä½³ï¼ŒDeepseek-R1ä»…æœ‰7.5%çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜è¯¥åŸºå‡†çš„æŒ‘æˆ˜æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é•¿æ–‡æœ¬åŸºå‡†ä¸»è¦å…³æ³¨å¯¹é•¿è¾“å…¥çš„ç†è§£è¯„ä¼°ï¼Œè€Œå¿½è§†äº†é•¿æ¨ç†èƒ½åŠ›çš„è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LongReasonArenaï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LLMsé•¿æ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„ä»»åŠ¡è¦æ±‚æ¨¡å‹é€šè¿‡æ‰§è¡Œå¤šæ­¥éª¤ç®—æ³•æ¥è§£å†³é—®é¢˜ï¼Œåæ˜ é•¿æ¨ç†çš„å…³é”®æ–¹é¢ï¼Œå¦‚æ£€ç´¢å’Œå›æº¯ã€‚é€šè¿‡æ§åˆ¶è¾“å…¥ï¼Œæ‰€éœ€çš„æ¨ç†é•¿åº¦å¯ä»¥ä»»æ„æ‰©å±•ï¼Œæœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡å¯è¾¾åˆ°100ä¸‡æ ‡è®°çš„æ¨ç†ã€‚å¹¿æ³›çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒLongReasonArenaå¯¹å¼€æºå’Œä¸“æœ‰LLMséƒ½æå‡ºäº†æ˜¾è‘—æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼ŒDeepseek-R1åœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸­ä»…è¾¾åˆ°7.5%çš„å‡†ç¡®ç‡ã€‚è¿›ä¸€æ­¥åˆ†æè¿˜æ˜¾ç¤ºï¼Œå‡†ç¡®ç‡ä¸é¢„æœŸæ¨ç†æ­¥éª¤çš„å¯¹æ•°å‘ˆçº¿æ€§ä¸‹é™å…³ç³»ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨https://github.com/LongReasonArena/LongReasonArenaè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰é•¿æ–‡æœ¬åŸºå‡†æœªèƒ½è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹é•¿æ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºç†è§£é•¿è¾“å…¥ï¼Œè€Œå¿½è§†äº†æ¨ç†è¿‡ç¨‹çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºLongReasonArenaåŸºå‡†ï¼Œè®¾è®¡å¤šæ­¥éª¤ç®—æ³•ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åœ¨é•¿æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œæ£€ç´¢å’Œå›æº¯ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å…¨é¢è¯„ä¼°æ¨¡å‹çš„é•¿æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹å¤æ‚é—®é¢˜æ—¶çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLongReasonArenaçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡è®¾è®¡ã€è¾“å…¥æ§åˆ¶å’Œè¯„ä¼°æœºåˆ¶ã€‚ä»»åŠ¡è®¾è®¡æ¶µç›–å¤šç§æ¨ç†ä»»åŠ¡ï¼Œè¾“å…¥æ§åˆ¶å…è®¸æ¨ç†é•¿åº¦çš„çµæ´»è°ƒæ•´ï¼Œè¯„ä¼°æœºåˆ¶åˆ™åŸºäºæ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§è¿›è¡Œè¯„åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†é•¿æ¨ç†èƒ½åŠ›çš„è¯„ä¼°æ ‡å‡†ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ§åˆ¶è¾“å…¥é•¿åº¦å’Œæ¨ç†æ­¥éª¤ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„è¯„ä¼°æ¡†æ¶ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€ç†è§£è¯„ä¼°å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†å¤šç§å‚æ•°ä»¥é€‚åº”ä¸åŒçš„æ¨ç†ä»»åŠ¡ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº†é€‚åˆé•¿æ¨ç†çš„è®¾è®¡ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šæ­¥éª¤æ¨ç†ä¸­çš„è¡¨ç°å¾—åˆ°æœ‰æ•ˆè¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepseek-R1åœ¨LongReasonArenaä»»åŠ¡ä¸­ä»…è¾¾åˆ°7.5%çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜è¯¥åŸºå‡†å¯¹ç°æœ‰æ¨¡å‹æå‡ºäº†æ˜¾è‘—æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå‡†ç¡®ç‡ä¸é¢„æœŸæ¨ç†æ­¥éª¤çš„å¯¹æ•°å‘ˆçº¿æ€§ä¸‹é™å…³ç³»ï¼Œè¿›ä¸€æ­¥æ­ç¤ºäº†é•¿æ¨ç†ä»»åŠ¡çš„å¤æ‚æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LongReasonArenaçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¤æ‚å†³ç­–æ”¯æŒç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ›´å¥½åœ°å¤„ç†å¤æ‚é—®é¢˜ï¼Œæä¾›æ›´å‡†ç¡®çš„ç­”æ¡ˆå’Œå»ºè®®ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing long-context benchmarks for Large Language Models (LLMs) focus on evaluating comprehension of long inputs, while overlooking the evaluation of long reasoning abilities. To address this gap, we introduce LongReasonArena, a benchmark specifically designed to assess the long reasoning capabilities of LLMs. Our tasks require models to solve problems by executing multi-step algorithms that reflect key aspects of long reasoning, such as retrieval and backtracking. By controlling the inputs, the required reasoning length can be arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most challenging tasks. Extensive evaluation results demonstrate that LongReasonArena presents a significant challenge for both open-source and proprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our task. Further analysis also reveals that the accuracy exhibits a linear decline with respect to the logarithm of the expected number of reasoning steps. Our code and data is available at https://github.com/LongReasonArena/LongReasonArena.

