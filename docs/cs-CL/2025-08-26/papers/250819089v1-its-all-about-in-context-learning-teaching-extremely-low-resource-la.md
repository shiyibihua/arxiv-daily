---
layout: default
title: It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs
---

# It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19089" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19089v1</a>
  <a href="https://arxiv.org/pdf/2508.19089.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19089v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19089v1', 'It\'s All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yue Li, Zhixue Zhao, Carolina Scarton

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

**å¤‡æ³¨**: Accepted by EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–¹æ³•ä»¥è§£å†³æä½èµ„æºè¯­è¨€çš„æ”¯æŒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æä½èµ„æºè¯­è¨€` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `è¯­è¨€å¯¹é½` `å¤šè¯­è¨€å¤„ç†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æä½èµ„æºè¯­è¨€çš„æ”¯æŒä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹è®­ç»ƒæ•°æ®å’Œä¹¦å†™ç³»ç»Ÿçš„æƒ…å†µä¸‹ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥è§£å†³æä½èµ„æºè¯­è¨€çš„å­¦ä¹ é—®é¢˜ï¼Œæ¢ç´¢å…¶ä¸å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰çš„æ¯”è¾ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé›¶-shot ICLç»“åˆè¯­è¨€å¯¹é½åœ¨æä½èµ„æºè¯­è¨€ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè€ŒPEFTåœ¨ç›¸å¯¹è¾ƒå¥½è¡¨ç°çš„è¯­è¨€ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æä½èµ„æºè¯­è¨€ï¼Œå°¤å…¶æ˜¯é‚£äº›ä½¿ç”¨ç¨€æœ‰ä¹¦å†™ç³»ç»Ÿçš„è¯­è¨€ï¼Œä»ç„¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ç¼ºä¹æ”¯æŒï¼Œéƒ¨åˆ†åŸå› æ˜¯ç¼ºä¹è®­ç»ƒæ•°æ®ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢åˆ†æäº†LLMsæ˜¯å¦å¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çº¯ç²¹æŒæ¡è¿™äº›è¯­è¨€ï¼Œå¹¶ä¸å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬ç³»ç»Ÿè¯„ä¼°äº†20ç§ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€åœ¨ä¸‰ç§æœ€å…ˆè¿›çš„å¤šè¯­è¨€LLMsä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå½“è¯­è¨€åŠå…¶ä¹¦å†™ç³»ç»Ÿåœ¨LLMä¸­æåº¦ç¼ºä¹æ—¶ï¼ŒPEFTçš„æ•ˆæœæœ‰é™ï¼Œè€Œé›¶-shot ICLç»“åˆè¯­è¨€å¯¹é½åœ¨æä½èµ„æºè¯­è¨€ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç›¸å¯¹è€Œè¨€ï¼Œfew-shot ICLæˆ–PEFTæ›´é€‚åˆç›¸å¯¹è¾ƒå¥½è¡¨ç°çš„è¯­è¨€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æä½èµ„æºè¯­è¨€åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ”¯æŒä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¦‚PEFTåœ¨è¿™äº›è¯­è¨€ä¸Šæ•ˆæœä¸ä½³ï¼Œå°¤å…¶æ˜¯å½“è¯­è¨€åŠå…¶ä¹¦å†™ç³»ç»Ÿæåº¦ç¼ºä¹æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥è®©LLMså­¦ä¹ æä½èµ„æºè¯­è¨€ï¼Œæ¢ç´¢å…¶åœ¨æ²¡æœ‰è¾…åŠ©å¯¹é½ä¿¡å·çš„æƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸PEFTè¿›è¡Œæ¯”è¾ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹20ç§ä»£è¡¨æ€§ä¸è¶³è¯­è¨€çš„ç³»ç»Ÿè¯„ä¼°ï¼Œä½¿ç”¨ä¸‰ç§æœ€å…ˆè¿›çš„å¤šè¯­è¨€LLMsè¿›è¡Œå®éªŒï¼Œæ¯”è¾ƒä¸åŒå­¦ä¹ ç­–ç•¥çš„æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°åˆ†æäº†ICLåœ¨æä½èµ„æºè¯­è¨€å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå±•ç¤ºäº†å…¶ç›¸è¾ƒäºPEFTçš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†é›¶-shotå’Œfew-shot ICLç­–ç•¥ï¼Œç»“åˆè¯­è¨€å¯¹é½ä¿¡å·è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿äº†å¯¹ä¸åŒè¯­è¨€è¡¨ç°çš„å…¨é¢åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé›¶-shot ICLç»“åˆè¯­è¨€å¯¹é½åœ¨æä½èµ„æºè¯­è¨€ä¸Šå–å¾—äº†æ˜¾è‘—æ•ˆæœï¼Œç›¸è¾ƒäºPEFTï¼Œè¡¨ç°æå‡å¹…åº¦æ˜æ˜¾ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æŸäº›è¯­è¨€ä¸Šï¼ŒICLçš„è¡¨ç°è¶…è¿‡äº†ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ä½èµ„æºç¯å¢ƒä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¯­è¨€ä¿æŠ¤ã€æ•™è‚²å’Œç¿»è¯‘ç­‰ï¼Œå°¤å…¶æ˜¯åœ¨æ”¯æŒé‚£äº›æä½èµ„æºè¯­è¨€çš„æŠ€æœ¯å¼€å‘ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚é€šè¿‡æå‡LLMså¯¹è¿™äº›è¯­è¨€çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä¿ƒè¿›æ–‡åŒ–å¤šæ ·æ€§å’Œä¿¡æ¯è·å–çš„å…¬å¹³æ€§ï¼Œæœªæ¥å¯èƒ½å½±å“è¯­è¨€å­¦ä¹ å’Œè·¨æ–‡åŒ–äº¤æµçš„æ–¹å¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Extremely low-resource languages, especially those written in rare scripts, as shown in Figure 1, remain largely unsupported by large language models (LLMs). This is due in part to compounding factors such as the lack of training data. This paper delivers the first comprehensive analysis of whether LLMs can acquire such languages purely via in-context learning (ICL), with or without auxiliary alignment signals, and how these methods compare to parameter-efficient fine-tuning (PEFT). We systematically evaluate 20 under-represented languages across three state-of-the-art multilingual LLMs. Our findings highlight the limitation of PEFT when both language and its script are extremely under-represented by the LLM. In contrast, zero-shot ICL with language alignment is impressively effective on extremely low-resource languages, while few-shot ICL or PEFT is more beneficial for languages relatively better represented by LLMs. For LLM practitioners working on extremely low-resource languages, we summarise guidelines grounded by our results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning a multilingual model on languages of unseen scripts.

