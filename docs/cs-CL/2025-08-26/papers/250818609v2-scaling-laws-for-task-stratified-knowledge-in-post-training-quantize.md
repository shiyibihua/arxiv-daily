---
layout: default
title: Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models
---

# Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18609" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18609v2</a>
  <a href="https://arxiv.org/pdf/2508.18609.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18609v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18609v2', 'Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenxi Zhou, Pengfei Cao, Jiang Li, Jun Zhao, Kang Liu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-08-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»»åŠ¡åˆ†å±‚çŸ¥è¯†çš„ç¼©æ”¾è§„å¾‹ä»¥ä¼˜åŒ–åè®­ç»ƒé‡åŒ–å¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åè®­ç»ƒé‡åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†èƒ½åŠ›` `ç¼©æ”¾è§„å¾‹` `ä»»åŠ¡åˆ†å±‚` `é‡åŒ–ç­–ç•¥` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åè®­ç»ƒé‡åŒ–æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†èƒ½åŠ›æ—¶å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å¯¹PTQç‰¹å®šå‚æ•°çš„ç†è§£ä¸å¤Ÿæ·±å…¥ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§ä»»åŠ¡åˆ†å±‚çš„ç¼©æ”¾è§„å¾‹ï¼Œé€šè¿‡è§£æ„LLMçŸ¥è¯†ä¸ºè®°å¿†å’Œåˆ©ç”¨èƒ½åŠ›ï¼Œå»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„å®šé‡æ¡†æ¶ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒçŸ¥è¯†è®°å¿†å¯¹æ¨¡å‹å‚æ•°å˜åŒ–çš„æ•æ„Ÿæ€§æ˜¾è‘—é«˜äºçŸ¥è¯†åˆ©ç”¨ï¼Œä¸ºé‡åŒ–ç­–ç•¥çš„ä¼˜åŒ–æä¾›äº†æ–°æ€è·¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éƒ¨ç½²æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œåè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰ä½œä¸ºä¸€ç§å®ç”¨çš„å‹ç¼©è§£å†³æ–¹æ¡ˆé€æ¸å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼ŒPTQå¯¹ä¸åŒLLMçŸ¥è¯†èƒ½åŠ›çš„å…·ä½“å½±å“å°šä¸æ˜ç¡®ï¼Œç°æœ‰çš„é‡åŒ–æ¨¡å‹ç¼©æ”¾è§„å¾‹å¾€å¾€å¿½è§†äº†PTQç‰¹å®šå‚æ•°å’Œä»»åŠ¡ç‰¹å®šæ•æ„Ÿæ€§ã€‚æœ¬æ–‡é€šè¿‡å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œå»ºç«‹äº†ä»»åŠ¡åˆ†å±‚çš„ç¼©æ”¾è§„å¾‹ï¼Œè§£æ„äº†LLMçŸ¥è¯†ä¸ºè®°å¿†å’Œåˆ©ç”¨èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„å®šé‡æ¡†æ¶ï¼Œæ¶µç›–æ¨¡å‹å¤§å°ã€æœ‰æ•ˆä½å®½ã€æ ¡å‡†é›†å¤§å°å’Œç»„å¤§å°ã€‚ç ”ç©¶å‘ç°ï¼ŒçŸ¥è¯†è®°å¿†å¯¹æœ‰æ•ˆä½å®½ã€æ ¡å‡†é›†å¤§å°å’Œæ¨¡å‹å¤§å°çš„å˜åŒ–è¡¨ç°å‡ºæ˜æ˜¾æ›´é«˜çš„æ•æ„Ÿæ€§ï¼Œè€ŒçŸ¥è¯†åˆ©ç”¨åˆ™ç›¸å¯¹ç¨³å¥ã€‚è¿™äº›å‘ç°ä¸ºPTQçš„å½±å“æä¾›äº†ç»†è‡´çš„ç†è§£ï¼Œå¹¶ä¸ºå¼€å‘çŸ¥è¯†æ„ŸçŸ¥çš„é‡åŒ–ç­–ç•¥æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åè®­ç»ƒé‡åŒ–å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çŸ¥è¯†èƒ½åŠ›å½±å“çš„ä¸ç¡®å®šæ€§ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘PTQç‰¹å®šå‚æ•°å’Œä»»åŠ¡æ•æ„Ÿæ€§çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å®è¯ç ”ç©¶ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ä»»åŠ¡åˆ†å±‚çš„ç¼©æ”¾è§„å¾‹ï¼Œè§£æ„LLMçš„çŸ¥è¯†èƒ½åŠ›ä¸ºè®°å¿†å’Œåˆ©ç”¨ï¼Œè¿›è€Œå»ºç«‹äº†ä¸€ä¸ªåŒ…å«å¤šç§å‚æ•°çš„å®šé‡æ¡†æ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶æ¡†æ¶åŒ…æ‹¬æ¨¡å‹å¤§å°ã€æœ‰æ•ˆä½å®½ã€æ ¡å‡†é›†å¤§å°å’Œç»„å¤§å°ç­‰å¤šä¸ªæ¨¡å—ï¼Œé‡‡ç”¨å®éªŒæ•°æ®è¿›è¡Œåˆ†æï¼Œæ¢è®¨å„å‚æ•°å¯¹çŸ¥è¯†èƒ½åŠ›çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°åˆ†æäº†PTQå¯¹LLMçŸ¥è¯†çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯è®°å¿†èƒ½åŠ›çš„æ•æ„Ÿæ€§ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„æœ‰æ•ˆä½å®½å’Œæ ¡å‡†é›†å¤§å°ï¼Œé‡‡ç”¨å®šé‡åˆ†ææ–¹æ³•è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„çŸ¥è¯†è®°å¿†å’Œåˆ©ç”¨èƒ½åŠ›ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ä¹Ÿè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”é‡åŒ–éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒçŸ¥è¯†è®°å¿†å¯¹æœ‰æ•ˆä½å®½å’Œæ ¡å‡†é›†å¤§å°çš„å˜åŒ–è¡¨ç°å‡ºæ˜¾è‘—çš„æ•æ„Ÿæ€§ï¼Œæ¨¡å‹åœ¨è¿™äº›å‚æ•°ä¸‹çš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œè€ŒçŸ¥è¯†åˆ©ç”¨åˆ™ä¿æŒç›¸å¯¹ç¨³å®šã€‚è¿™ä¸€å‘ç°ä¸ºé‡åŒ–ç­–ç•¥çš„è®¾è®¡æä¾›äº†é‡è¦çš„å®è¯ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡ä¼˜åŒ–åè®­ç»ƒé‡åŒ–ç­–ç•¥ï¼Œå¯ä»¥åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæå‡æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„é‡åŒ–æŠ€æœ¯åœ¨å„ç±»AIåº”ç”¨ä¸­çš„å¹¿æ³›é‡‡ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) present significant deployment challenges due to their scale, with post-training quantization (PTQ) emerging as a practical compression solution. However, a comprehensive understanding of how PTQ precisely impacts diverse LLM knowledge capabilities remains elusive, and existing scaling laws for quantized models often overlook crucial PTQ-specific parameters and task-specific sensitivities. This paper addresses these gaps by conducting an extensive empirical investigation to establish task-stratified scaling laws. We disentangle LLM knowledge into memorization and utilization capabilities and develop a unified quantitative framework that incorporates model size, effective bit-width, calibration set size, and group size. Our central finding reveals that knowledge memorization exhibits markedly greater sensitivity to variations in effective bit-width, calibration set size, and model size compared to the more robust knowledge utilization. These findings offer a fine-grained understanding of PTQ's impact and provide guidance for developing knowledge-aware quantization strategies that can better preserve targeted cognitive functions.

