---
layout: default
title: Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck
---

# Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03533" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.03533v1</a>
  <a href="https://arxiv.org/pdf/2509.03533.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03533v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03533v1', 'Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Igor Halperin

**ÂàÜÁ±ª**: cs.CL, cs.LG, q-fin.GN

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-26

**Â§áÊ≥®**: 26 pages, 4 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫UDIBÊñπÊ≥ï‰ª•ÊèêÈ´òLLMËæìÂÖ•ËæìÂá∫ÂØπÁöÑ‰∏ªÈ¢òËØÜÂà´ËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `‰∏ªÈ¢òËØÜÂà´` `‰ø°ÊÅØÁì∂È¢à` `ËÅöÁ±ªÁÆóÊ≥ï` `ËØ≠‰πâÂÅèÂ∑ÆÊ£ÄÊµã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËØ≠‰πâÂÅèÂ∑ÆÂ∫¶ÈáèÊñπÊ≥ïÂú®ËØÜÂà´LLMÊèêÁ§∫‰∏éÂìçÂ∫î‰πãÈó¥ÁöÑ‰∏ªÈ¢òÊó∂ÔºåÂ≠òÂú®‰ø°ÊÅØÁêÜËÆ∫ÂàÜÊûê‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁ°ÆÂÆöÊÄß‰ø°ÊÅØÁì∂È¢àÁöÑ‰∏ªÈ¢òËØÜÂà´ÊñπÊ≥ïUDIBÔºåËÉΩÂ§üÊúâÊïàËÅöÁ±ªÈ´òÁª¥Êï∞ÊçÆÂπ∂ÁîüÊàê‰ø°ÊÅØ‰∏∞ÂØåÁöÑ‰∏ªÈ¢òË°®Á§∫„ÄÇ
3. ÈÄöËøáÂ∫îÁî®UDIBÔºåÊú¨ÊñáÂú®confabulationsÊ£ÄÊµã‰∏äÂèñÂæó‰∫ÜÊòæËëóÊèêÂçáÔºåÊèê‰æõ‰∫ÜÊõ¥ÊïèÊÑüÁöÑÂ∑•ÂÖ∑Êù•ËØÜÂà´ËØ≠‰πâÂÅèÂ∑Æ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂÆπÊòìÂá∫Áé∞ÂÖ≥ÈîÆÂ§±ÊïàÊ®°ÂºèÔºåÂ¶ÇÂÜÖÂú®ÁúüÂÆûÊÑüÂπªËßâÔºàconfabulationsÔºâÔºåÂç≥ÂìçÂ∫îÂú®ËØ≠‰πâ‰∏äÂÅèÁ¶ªÊèê‰æõÁöÑ‰∏ä‰∏ãÊñá„ÄÇÁé∞ÊúâÁöÑÊ£ÄÊµãÊ°ÜÊû∂ÔºåÂ¶ÇËØ≠‰πâÂÅèÂ∑ÆÂ∫¶ÈáèÔºàSDMÔºâÔºå‰æùËµñ‰∫éËØÜÂà´ÊèêÁ§∫‰∏éÂìçÂ∫î‰πãÈó¥ÂÖ±‰∫´ÁöÑÊΩúÂú®‰∏ªÈ¢òÔºåÈÄöÂ∏∏ÈÄöËøáÂá†‰ΩïËÅöÁ±ªÂÖ∂Âè•Â≠êÂµåÂÖ•Êù•ÂÆûÁé∞„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ïÂú®‰ø°ÊÅØÁêÜËÆ∫ÂàÜÊûê‰∏äÂ≠òÂú®‰∏çË∂≥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁ°ÆÂÆöÊÄß‰ø°ÊÅØÁì∂È¢àÔºàDIBÔºâÁöÑ‰∏ªÈ¢òËØÜÂà´ÊñπÊ≥ïÔºåÂºÄÂèëÂá∫‰∏ÄÁßçÂÆûÁî®ÁöÑÈ´òÁª¥Êï∞ÊçÆËÅöÁ±ªÁÆóÊ≥ïUDIBÔºåËÉΩÂ§üÁîüÊàêÊõ¥ÂÖ∑‰ø°ÊÅØÊÄßÁöÑÂÖ±‰∫´‰∏ªÈ¢òË°®Á§∫Ôºå‰∏∫SDMÊ°ÜÊû∂Êèê‰æõ‰∫ÜÊõ¥‰ºòÁöÑÂü∫Á°ÄÔºåËøõËÄåÊèêÈ´ò‰∫ÜÂØπconfabulationsÁöÑÊ£ÄÊµãËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÁîüÊàêÂìçÂ∫îÊó∂ÂèØËÉΩÂá∫Áé∞ÁöÑÂÜÖÂú®ÁúüÂÆûÊÑüÂπªËßâÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®‰∏ªÈ¢òËØÜÂà´Âíå‰ø°ÊÅØÁêÜËÆ∫ÂàÜÊûê‰∏äÂ≠òÂú®‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫UDIBÊñπÊ≥ïÔºåÈÄöËøáÂ∞Ü‰∏çÂèØÂ§ÑÁêÜÁöÑKLÊï£Â∫¶È°πÊõøÊç¢‰∏∫ËÆ°ÁÆóÊïàÁéáÊõ¥È´òÁöÑ‰∏äÁïåÔºåÊûÑÂª∫‰∏Ä‰∏™Âü∫‰∫éÁ°ÆÂÆöÊÄß‰ø°ÊÅØÁì∂È¢àÁöÑËÅöÁ±ªÁÆóÊ≥ïÔºå‰ª•ÁîüÊàêÊõ¥ÂÖ∑‰ø°ÊÅØÊÄßÁöÑ‰∏ªÈ¢òË°®Á§∫„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöUDIBÊñπÊ≥ïÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÂè•Â≠êÂµåÂÖ•ÁîüÊàê„ÄÅ‰∏ªÈ¢òËÅöÁ±ªÂíå‰ø°ÊÅØÊèêÂèñÂõõ‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºåÁ°Æ‰øùËÅöÁ±ªÁªìÊûúÂú®Á©∫Èó¥‰∏äËøûË¥Ø‰∏î‰ø°ÊÅØ‰∏ä‰∏∞ÂØå„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöUDIBÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂ∞Ü‰ø°ÊÅØÁì∂È¢àÊñπÊ≥ïËΩ¨Âåñ‰∏∫ÂÆûÁî®ÁÆóÊ≥ïÔºå‰ºòÂåñ‰∫ÜÈ´òÁª¥Êï∞ÊçÆÁöÑËÅöÁ±ªËøáÁ®ãÔºå‰∏é‰º†ÁªüÁöÑÂá†‰ΩïËÅöÁ±ªÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÊèêÁ§∫‰∏éÂìçÂ∫î‰πãÈó¥ÁöÑ‰∏ªÈ¢òÂÖ≥Á≥ª„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöUDIBÈááÁî®‰∫ÜÁÜµÊ≠£ÂàôÂåñÁöÑK-meansÂèò‰ΩìÔºåÂº∫Ë∞É‰ø°ÊÅØËÅöÁ±ªÁöÑÁ®ÄÁñèÊÄßÔºåÂÖ≥ÈîÆÂèÇÊï∞ÂåÖÊã¨ËÅöÁ±ªÊï∞ÁõÆÂíåÁÜµÊ≠£ÂàôÂåñÁ≥ªÊï∞ÔºåËøô‰∫õËÆæËÆ°‰ΩøÂæóËÅöÁ±ªÁªìÊûúÊõ¥Âä†Á®≥ÂÅ•Âíå‰ø°ÊÅØ‰∏∞ÂØå„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUDIBÊñπÊ≥ïÂú®confabulationsÊ£ÄÊµã‰∏äÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑËØ≠‰πâÂÅèÂ∑ÆÂ∫¶ÈáèÊñπÊ≥ïÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®‰∏ªÈ¢òËØÜÂà´Âíå‰ø°ÊÅØÊèêÂèñÊñπÈù¢ÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÂØπËØùÁ≥ªÁªüÂíå‰ø°ÊÅØÊ£ÄÁ¥¢Á≠â„ÄÇÈÄöËøáÊèêÈ´òÂØπLLMÁîüÊàêÂÜÖÂÆπÁöÑÁêÜËß£ÂíåÂàÜÊûêËÉΩÂäõÔºåUDIBÊñπÊ≥ïËÉΩÂ§üÂ∏ÆÂä©ÂºÄÂèëÊõ¥ÂèØÈù†ÁöÑAIÁ≥ªÁªüÔºåÂáèÂ∞ëÈîôËØØ‰ø°ÊÅØÁöÑ‰º†Êí≠ÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÂèØËÉΩÂú®Â§öÊ®°ÊÄÅÂ≠¶‰π†ÂíåË∑®È¢ÜÂüüÁü•ËØÜËøÅÁßª‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) are prone to critical failure modes, including \textit{intrinsic faithfulness hallucinations} (also known as confabulations), where a response deviates semantically from the provided context. Frameworks designed to detect this, such as Semantic Divergence Metrics (SDM), rely on identifying latent topics shared between prompts and responses, typically by applying geometric clustering to their sentence embeddings. This creates a disconnect, as the topics are optimized for spatial proximity, not for the downstream information-theoretic analysis. In this paper, we bridge this gap by developing a principled topic identification method grounded in the Deterministic Information Bottleneck (DIB) for geometric clustering. Our key contribution is to transform the DIB method into a practical algorithm for high-dimensional data by substituting its intractable KL divergence term with a computationally efficient upper bound. The resulting method, which we dub UDIB, can be interpreted as an entropy-regularized and robustified version of K-means that inherently favors a parsimonious number of informative clusters. By applying UDIB to the joint clustering of LLM prompt and response embeddings, we generate a shared topic representation that is not merely spatially coherent but is fundamentally structured to be maximally informative about the prompt-response relationship. This provides a superior foundation for the SDM framework and offers a novel, more sensitive tool for detecting confabulations.

