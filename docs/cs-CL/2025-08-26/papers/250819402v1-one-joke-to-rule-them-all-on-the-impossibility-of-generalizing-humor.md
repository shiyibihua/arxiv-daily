---
layout: default
title: One Joke to Rule them All? On the (Im)possibility of Generalizing Humor
---

# One Joke to Rule them All? On the (Im)possibility of Generalizing Humor

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19402" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19402v1</a>
  <a href="https://arxiv.org/pdf/2508.19402.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19402v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19402v1', 'One Joke to Rule them All? On the (Im)possibility of Generalizing Humor')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mor Turgeman, Chen Shani, Dafna Shahaf

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¹½é»˜ç±»å‹è¿ç§»å­¦ä¹ æ–¹æ³•ä»¥è§£å†³å¹½é»˜ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¹½é»˜ç†è§£` `è¿ç§»å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `ç¤¾äº¤åª’ä½“` `æ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¹½é»˜ç†è§£ç ”ç©¶é€šå¸¸é›†ä¸­äºç‰¹å®šç±»å‹ï¼Œç¼ºä¹å¯¹å¹½é»˜ç±»å‹é—´è¿ç§»èƒ½åŠ›çš„æ¢è®¨ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡è¿ç§»å­¦ä¹ å®éªŒï¼Œæ¢è®¨åœ¨ä¸åŒå¹½é»˜ä»»åŠ¡é—´çš„çŸ¥è¯†è¿ç§»èƒ½åŠ›ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹çš„å¹½é»˜ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šå¯è¾¾åˆ°75%çš„å‡†ç¡®ç‡ï¼Œä¸”å¤šæ ·åŒ–è®­ç»ƒæºæå‡äº†è¿ç§»èƒ½åŠ›ï¼Œè¡¨ç°å‡ºå¹½é»˜ç±»å‹é—´çš„å…³ç³»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¹½é»˜æ˜¯ä¸€ç§å¹¿æ³›è€Œå¤æ‚çš„äº¤æµå½¢å¼ï¼Œæœºå™¨ç†è§£å¹½é»˜ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶é›†ä¸­äºç‰¹å®šå¹½é»˜ç±»å‹çš„å»ºæ¨¡ï¼Œä½†æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨åœ¨ç‰¹å®šå¹½é»˜ä»»åŠ¡ä¸Šçš„èƒ½åŠ›æ˜¯å¦èƒ½å¤Ÿè¿ç§»åˆ°æ–°çš„ã€æœªè§è¿‡çš„å¹½é»˜ç±»å‹ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿè¿›è¡Œäº†å¤šé¡¹è¿ç§»å­¦ä¹ å®éªŒï¼Œä½¿ç”¨å››ä¸ªä¸åŒçš„å¹½é»˜ä»»åŠ¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šå¯è¾¾åˆ°75%çš„å‡†ç¡®ç‡ï¼Œä¸”åœ¨å¤šæ ·åŒ–æ•°æ®æºè®­ç»ƒä¸‹ï¼Œè¿ç§»èƒ½åŠ›æœ‰æ‰€æå‡ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œçˆ¶äº²ç¬‘è¯åœ¨è¿ç§»ä¸­è¡¨ç°æœ€ä½³ï¼Œä½†å…¶æœ¬èº«çš„è¿ç§»éš¾åº¦è¾ƒå¤§ã€‚ç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†ç›¸å…³æ•°æ®å’Œä»£ç ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³æœºå™¨åœ¨å¹½é»˜ç†è§£ä¸­çš„è¿ç§»èƒ½åŠ›é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å±€é™äºç‰¹å®šå¹½é»˜ç±»å‹ï¼Œéš¾ä»¥é€‚åº”æ–°å…´å¹½é»˜å½¢å¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è¿ç§»å­¦ä¹ å®éªŒï¼Œæ¢ç´¢åœ¨ä¸åŒå¹½é»˜ä»»åŠ¡é—´çš„çŸ¥è¯†è¿ç§»ï¼ŒéªŒè¯å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿæ•æ‰å¹½é»˜çš„æ·±å±‚æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä½¿ç”¨å››ä¸ªä¸åŒçš„å¹½é»˜ä»»åŠ¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹åœ¨1-3ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ–°çš„å¹½é»˜ä»»åŠ¡ä¸Šè¿›è¡Œæµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡å¤šæ ·åŒ–çš„æ•°æ®æºè®­ç»ƒæå‡æ¨¡å‹çš„è¿ç§»èƒ½åŠ›ï¼Œå‘ç°çˆ¶äº²ç¬‘è¯åœ¨è¿ç§»ä¸­è¡¨ç°æœ€ä½³ï¼Œä½†å…¶æœ¬èº«çš„è¿ç§»éš¾åº¦è¾ƒé«˜ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­è®¾ç½®äº†å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®æºï¼Œè¯„ä¼°äº†æ¨¡å‹åœ¨ä¸åŒå¹½é»˜ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œé‡‡ç”¨äº†æ ‡å‡†çš„å‡†ç¡®ç‡ä½œä¸ºæ€§èƒ½æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šå¯è¾¾åˆ°75%çš„å‡†ç¡®ç‡ï¼Œä¸”å¤šæ ·åŒ–è®­ç»ƒæºæå‡äº†1.88-4.05%çš„è¿ç§»èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šå¯è¾¾åˆ°75%çš„å‡†ç¡®ç‡ï¼Œä¸”é€šè¿‡å¤šæ ·åŒ–è®­ç»ƒæºï¼Œè¿ç§»èƒ½åŠ›æå‡äº†1.88-4.05%ã€‚ç‰¹åˆ«æ˜¯çˆ¶äº²ç¬‘è¯åœ¨è¿ç§»ä¸­è¡¨ç°æœ€ä½³ï¼Œå°½ç®¡å…¶æœ¬èº«çš„è¿ç§»éš¾åº¦è¾ƒé«˜ï¼Œè¿™ä¸€å‘ç°ä¸ºå¹½é»˜ç†è§£æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹ç”Ÿæˆã€åœ¨çº¿å¹½é»˜æ¨èç³»ç»Ÿä»¥åŠäººæœºäº¤äº’ä¸­çš„å¹½é»˜ç†è§£ã€‚é€šè¿‡æå‡æœºå™¨å¯¹å¹½é»˜çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œå¢å¼ºäººæœºäº’åŠ¨çš„è‡ªç„¶æ€§å’Œè¶£å‘³æ€§ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨å¹½é»˜ç”Ÿæˆå’Œç†è§£æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œé€‚åº”ä¸æ–­å˜åŒ–çš„ç½‘ç»œæ–‡åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humor is a broad and complex form of communication that remains challenging for machines. Despite its broadness, most existing research on computational humor traditionally focused on modeling a specific type of humor. In this work, we wish to understand whether competence on one or more specific humor tasks confers any ability to transfer to novel, unseen types; in other words, is this fragmentation inevitable? This question is especially timely as new humor types continuously emerge in online and social media contexts (e.g., memes, anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this evolving landscape, they must be able to generalize across humor types by capturing deeper, transferable mechanisms. To investigate this, we conduct a series of transfer learning experiments across four datasets, representing different humor tasks. We train LLMs under varied diversity settings (1-3 datasets in training, testing on a novel task). Experiments reveal that models are capable of some transfer, and can reach up to 75% accuracy on unseen datasets; training on diverse sources improves transferability (1.88-4.05%) with minimal-to-no drop in in-domain performance. Further analysis suggests relations between humor types, with Dad Jokes surprisingly emerging as the best enabler of transfer (but is difficult to transfer to). We release data and code.

