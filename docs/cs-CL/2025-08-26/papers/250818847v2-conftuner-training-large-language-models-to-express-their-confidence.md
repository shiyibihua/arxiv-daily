---
layout: default
title: ConfTuner: Training Large Language Models to Express Their Confidence Verbally
---

# ConfTuner: Training Large Language Models to Express Their Confidence Verbally

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18847" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18847v2</a>
  <a href="https://arxiv.org/pdf/2508.18847.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18847v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18847v2', 'ConfTuner: Training Large Language Models to Express Their Confidence Verbally')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yibo Li, Miao Xiong, Jiaying Wu, Bryan Hooi

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-11-25)

**å¤‡æ³¨**: Accepted by NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/liushiliushi/ConfTuner)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConfTunerä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¿¡å¿ƒè¡¨è¾¾ä¸å‡†ç¡®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ä¿¡å¿ƒæ ¡å‡†` `å¾®è°ƒæ–¹æ³•` `æ ‡è®°å¸ƒé‡Œå°”åˆ†æ•°` `è‡ªæˆ‘ä¿®æ­£` `æ¨¡å‹çº§è”` `é«˜é£é™©é¢†åŸŸ` `å¯ä¿¡äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨é«˜é£é™©é¢†åŸŸä¸­å¸¸å¸¸è¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡ï¼Œå¯¼è‡´ç”Ÿæˆé”™è¯¯ç­”æ¡ˆï¼Œå½±å“å…¶å¯é æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„ConfTuneræ˜¯ä¸€ç§æ–°å‹å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ ‡è®°å¸ƒé‡Œå°”åˆ†æ•°æŸå¤±å‡½æ•°æ¥æœ‰æ•ˆæ ¡å‡†æ¨¡å‹çš„ä¿¡å¿ƒè¡¨è¾¾ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒConfTuneråœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„ä¿¡å¿ƒæ ¡å‡†ï¼Œå¹¶ä¿ƒè¿›äº†è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›çš„æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦ã€æ³•å¾‹å’ŒåŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œå‡†ç¡®è¡¨è¾¾ä¸ç¡®å®šæ€§å¯¹å¯é æ€§å’Œä¿¡ä»»è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰LLMså¸¸å¸¸ä»¥è¿‡é«˜çš„ä¿¡å¿ƒç”Ÿæˆé”™è¯¯ç­”æ¡ˆï¼Œç§°ä¸ºâ€œè¿‡åº¦è‡ªä¿¡â€ã€‚ç°æœ‰çš„æ ¡å‡†æ–¹æ³•ä¾èµ–äºæç¤ºå·¥ç¨‹æˆ–ä½¿ç”¨å¯å‘å¼ç”Ÿæˆçš„ä¸ç¡®å®šæ€§ä¼°è®¡è¿›è¡Œå¾®è°ƒï¼Œæ•ˆæœå’Œé€šç”¨æ€§æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ConfTunerï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†æœ€å°çš„å¼€é”€ï¼Œå¹¶ä¸”ä¸éœ€è¦çœŸå®çš„ä¿¡å¿ƒåˆ†æ•°æˆ–ä»£ç†ä¿¡å¿ƒä¼°è®¡ã€‚ConfTunerä¾èµ–äºä¸€ç§æ–°çš„æŸå¤±å‡½æ•°â€”â€”æ ‡è®°å¸ƒé‡Œå°”åˆ†æ•°ï¼Œç†è®ºä¸Šè¯æ˜å…¶ä¸ºé€‚å½“çš„è¯„åˆ†è§„åˆ™ï¼Œèƒ½å¤Ÿæ­£ç¡®æ¿€åŠ±æ¨¡å‹æŠ¥å‘Šå…¶æ­£ç¡®æ¦‚ç‡ã€‚ConfTuneråœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­æ”¹å–„äº†æ ¡å‡†æ•ˆæœï¼Œå¹¶ä¸”èƒ½å¤Ÿæ¨å¹¿åˆ°é»‘ç®±æ¨¡å‹å¦‚GPT-4oã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ›´å¥½çš„æ ¡å‡†ä¿¡å¿ƒä¿ƒè¿›äº†è‡ªæˆ‘ä¿®æ­£å’Œæ¨¡å‹çº§è”çš„ä¸‹æ¸¸æ”¶ç›Šï¼Œæ¨åŠ¨äº†å¯ä¿¡LLMç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨è¡¨è¾¾ä¿¡å¿ƒæ—¶çš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæç¤ºå·¥ç¨‹æˆ–å¯å‘å¼ç”Ÿæˆçš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ•ˆæœæœ‰é™ä¸”ç¼ºä¹é€šç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šConfTuneré€šè¿‡å¼•å…¥æ ‡è®°å¸ƒé‡Œå°”åˆ†æ•°ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œæ¿€åŠ±æ¨¡å‹å‡†ç¡®æŠ¥å‘Šå…¶æ­£ç¡®æ¦‚ç‡ï¼Œä»è€Œå®ç°ä¿¡å¿ƒçš„æœ‰æ•ˆæ ¡å‡†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šConfTunerçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹å¾®è°ƒå’ŒæŸå¤±è®¡ç®—ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œå‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œç„¶ååœ¨æ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæœ€åè®¡ç®—æŸå¤±ä»¥ä¼˜åŒ–æ¨¡å‹çš„ä¿¡å¿ƒè¡¨è¾¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†æ ‡è®°å¸ƒé‡Œå°”åˆ†æ•°ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œè¿™ä¸€è®¾è®¡ç¡®ä¿äº†æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®è¡¨è¾¾å…¶ä¿¡å¿ƒï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´å¥½çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒConfTunerä¸éœ€è¦çœŸå®çš„ä¿¡å¿ƒåˆ†æ•°æˆ–ä»£ç†ä¿¡å¿ƒä¼°è®¡ï¼Œé™ä½äº†æ¨¡å‹è®­ç»ƒçš„å¤æ‚æ€§ï¼ŒåŒæ—¶é€šè¿‡ç†è®ºè¯æ˜å…¶æŸå¤±å‡½æ•°ä¸ºé€‚å½“çš„è¯„åˆ†è§„åˆ™ï¼Œç¡®ä¿äº†æ¨¡å‹çš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ConfTunerè¿›è¡Œå¾®è°ƒåï¼Œæ¨¡å‹çš„ä¿¡å¿ƒæ ¡å‡†æ˜¾è‘—æ”¹å–„ï¼Œå°¤å…¶åœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ ¡å‡†æ•ˆæœæå‡å¹…åº¦è¶…è¿‡20%ã€‚æ­¤å¤–ï¼Œç»è¿‡æ ¡å‡†çš„æ¨¡å‹åœ¨è‡ªæˆ‘ä¿®æ­£å’Œæ¨¡å‹çº§è”ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ConfTunerçš„ç ”ç©¶æˆæœåœ¨ç§‘å­¦ã€æ³•å¾‹å’ŒåŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„ä¿¡å¿ƒè¡¨è¾¾å‡†ç¡®æ€§ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·å¯¹æ¨¡å‹è¾“å‡ºçš„ä¿¡ä»»ï¼Œä¿ƒè¿›å…¶åœ¨å†³ç­–æ”¯æŒã€æ³•å¾‹å’¨è¯¢å’ŒåŒ»ç–—è¯Šæ–­ç­‰å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æœªæ¥ï¼ŒConfTuneræœ‰æœ›æ¨åŠ¨å¯ä¿¡èµ–çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ï¼Œæå‡äººæœºäº¤äº’çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly deployed in high-stakes domains such as science, law, and healthcare, where accurate expressions of uncertainty are essential for reliability and trust. However, current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as "overconfidence". Recent efforts have focused on calibrating LLMs' verbalized confidence: i.e., their expressions of confidence in text form, such as "I am 80% confident that...". Existing approaches either rely on prompt engineering or fine-tuning with heuristically generated uncertainty estimates, both of which have limited effectiveness and generalizability. Motivated by the notion of proper scoring rules for calibration in classical machine learning models, we introduce ConfTuner, a simple and efficient fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. ConfTuner relies on a new loss function, tokenized Brier score, which we theoretically prove to be a proper scoring rule, intuitively meaning that it "correctly incentivizes the model to report its true probability of being correct". ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our results further show that better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems. The code is available at https://github.com/liushiliushi/ConfTuner.

