---
layout: default
title: Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness
---

# Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18824" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18824v1</a>
  <a href="https://arxiv.org/pdf/2508.18824.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18824v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18824v1', 'Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sirui Chen, Changxin Tian, Binbin Hu, Kunlong Chen, Ziqi Liu, Zhiqiang Zhang, Jun Zhou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¨‹åºè¾…åŠ©åˆæˆæ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°å­¦æ¨ç†` `å¤§è¯­è¨€æ¨¡å‹` `æ•°æ®åˆæˆ` `ç¨‹åºè¾…åŠ©` `éªŒè¯æœºåˆ¶` `æœºå™¨å­¦ä¹ ` `æ•™è‚²æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡æ•°å­¦æ¨ç†æ•°æ®æ—¶é¢ä¸´å¯æ‰©å±•æ€§å’Œæ•°æ®å¯é æ€§ç­‰é‡å¤§æŒ‘æˆ˜ã€‚
2. æå‡ºçš„ç¨‹åºè¾…åŠ©åˆæˆæ¡†æ¶é€šè¿‡æ•´åˆæ•°å­¦çŸ¥è¯†å’Œå·¥å…·ï¼Œç³»ç»Ÿç”Ÿæˆé«˜è´¨é‡çš„æ•°å­¦é—®é¢˜è§£å†³å¯¹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¯¥æ¡†æ¶ç”Ÿæˆçš„æ•°æ®å¾®è°ƒæ¨¡å‹åï¼Œæ¨ç†èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°å¤šé¡¹åŸºå‡†æ•°æ®é›†çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›éœ€è¦é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•åœ¨å¯æ‰©å±•æ€§ã€æˆæœ¬å’Œæ•°æ®å¯é æ€§æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¨‹åºè¾…åŠ©åˆæˆæ¡†æ¶ï¼Œç³»ç»Ÿåœ°ç”Ÿæˆå…·æœ‰å¤šæ ·æ€§ã€å¤æ‚æ€§å’Œæ­£ç¡®æ€§çš„é«˜è´¨é‡æ•°å­¦è¯­æ–™åº“ã€‚è¯¥æ¡†æ¶æ•´åˆäº†æ•°å­¦çŸ¥è¯†ä½“ç³»å’Œé¢†åŸŸç‰¹å®šå·¥å…·ï¼Œåˆ›å»ºå¯æ‰§è¡Œç¨‹åºï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€é—®é¢˜-è§£å†³å¯¹ï¼Œå¹¶é€šè¿‡åŒå‘éªŒè¯æœºåˆ¶ç¡®ä¿è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§å’Œç¨‹åºä¸é—®é¢˜çš„ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºæˆ‘ä»¬ç”Ÿæˆçš„æ•°æ®å¾®è°ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œå±•ç¤ºäº†åˆæˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„è®­ç»ƒæ•°æ®ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•°æ®ç”Ÿæˆçš„å¯æ‰©å±•æ€§ã€æˆæœ¬å’Œå¯é æ€§ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ¡†æ¶é€šè¿‡ç¨‹åºè¾…åŠ©çš„æ–¹å¼ï¼Œç³»ç»Ÿç”Ÿæˆæ•°å­¦é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿æ•°æ®çš„å¤šæ ·æ€§ã€å¤æ‚æ€§å’Œæ­£ç¡®æ€§ã€‚è¯¥è®¾è®¡æ—¨åœ¨æé«˜æ•°æ®ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°å­¦çŸ¥è¯†ç³»ç»Ÿã€é¢†åŸŸç‰¹å®šå·¥å…·å’ŒåŒå‘éªŒè¯æœºåˆ¶ã€‚é¦–å…ˆï¼Œåˆ©ç”¨æ•°å­¦çŸ¥è¯†ç”Ÿæˆå¯æ‰§è¡Œç¨‹åºï¼Œç„¶åå°†ç¨‹åºè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€é—®é¢˜-è§£å†³å¯¹ï¼Œæœ€åé€šè¿‡éªŒè¯æœºåˆ¶ç¡®ä¿æ•°æ®çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåŒå‘éªŒè¯æœºåˆ¶ï¼Œå®ƒä¸ä»…éªŒè¯äº†è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ï¼Œè¿˜ç¡®ä¿äº†ç¨‹åºä¸é—®é¢˜ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚è¿™ä¸€æœºåˆ¶æ˜¾è‘—æå‡äº†æ•°æ®çš„å¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæ¡†æ¶é‡‡ç”¨äº†å¤šæ ·åŒ–çš„æ•°å­¦çŸ¥è¯†åº“å’Œé¢†åŸŸå·¥å…·ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡è§£å†³æ–¹æ¡ˆçš„å‡†ç¡®æ€§ä¸å¤šæ ·æ€§ï¼Œç½‘ç»œç»“æ„åˆ™æ”¯æŒé«˜æ•ˆçš„ç¨‹åºç”Ÿæˆä¸éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæœ¬æ–‡ç”Ÿæˆçš„12.3ç™¾ä¸‡ä¸ªé—®é¢˜è§£å†³å¯¹å¾®è°ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—æå‡ï¼Œæ¨ç†èƒ½åŠ›è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼ŒéªŒè¯äº†åˆæˆæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–æ•°å­¦é—®é¢˜ç”Ÿæˆç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„æ•°å­¦é—®é¢˜è§£å†³å¯¹ï¼Œå¯ä»¥å¸®åŠ©å­¦ç”Ÿå’Œç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨æ•°å­¦çŸ¥è¯†ï¼Œæå‡å­¦ä¹ æ•ˆæœã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸçš„çŸ¥è¯†ç”Ÿæˆä¸éªŒè¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Enhancing the mathematical reasoning of large language models (LLMs) demands high-quality training data, yet conventional methods face critical challenges in scalability, cost, and data reliability. To address these limitations, we propose a novel program-assisted synthesis framework that systematically generates a high-quality mathematical corpus with guaranteed diversity, complexity, and correctness. This framework integrates mathematical knowledge systems and domain-specific tools to create executable programs. These programs are then translated into natural language problem-solution pairs and vetted by a bilateral validation mechanism that verifies solution correctness against program outputs and ensures program-problem consistency. We have generated 12.3 million such problem-solving triples. Experiments demonstrate that models fine-tuned on our data significantly improve their inference capabilities, achieving state-of-the-art performance on several benchmark datasets and showcasing the effectiveness of our synthesis approach.

