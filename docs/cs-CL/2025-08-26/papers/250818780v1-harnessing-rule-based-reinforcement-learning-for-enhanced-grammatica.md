---
layout: default
title: Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction
---

# Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18780" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18780v1</a>
  <a href="https://arxiv.org/pdf/2508.18780.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18780v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18780v1', 'Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yilin Li, Xunjian Yin, Yilin Chen, Xiaojun Wan

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

**å¤‡æ³¨**: Code will be released upon publication

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥æå‡è¯­æ³•é”™è¯¯çº æ­£æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¯­æ³•é”™è¯¯çº æ­£` `å¼ºåŒ–å­¦ä¹ ` `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `è§„åˆ™åŸºç¡€æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­æ³•é”™è¯¯çº æ­£æ–¹æ³•ä¸»è¦ä¾èµ–äºç›‘ç£å­¦ä¹ ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ å¼•å¯¼LLMsè¿›è¡Œæ›´æœ‰æ•ˆçš„è¯­æ³•çº æ­£ã€‚
3. åœ¨ä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼Œå°¤å…¶åœ¨å¬å›ç‡ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­æ³•é”™è¯¯çº æ­£æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é‡è¦ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„åŸºäºç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„æ–¹æ³•å–å¾—äº†ä¸€å®šçš„æˆåŠŸï¼Œä½†åœ¨è¯¥é¢†åŸŸå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åº”ç”¨ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç›®å‰çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºç›‘ç£å¾®è°ƒæ¥è®­ç»ƒLLMsç›´æ¥ç”Ÿæˆçº æ­£åçš„å¥å­ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ çš„æ–°æ¡†æ¶ã€‚é€šè¿‡åœ¨ä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†å¬å›ç‡ã€‚è¿™ä¸€ç»“æœæ¸…æ™°åœ°çªæ˜¾äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¼•å¯¼LLMsçš„ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥çš„è¯­æ³•é”™è¯¯çº æ­£å‘å±•æä¾›äº†æ›´å¯æ§å’Œå¯é çš„èŒƒå¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿè¯­æ³•é”™è¯¯çº æ­£æ–¹æ³•åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ä¾èµ–äºç›‘ç£å¾®è°ƒçš„å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨LLMsçš„æ½œåŠ›ï¼Œå¯¼è‡´çº æ­£æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆè§„åˆ™åŸºç¡€çš„å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ä¼˜åŒ–èƒ½åŠ›æ¥å¼•å¯¼LLMsè¿›è¡Œè¯­æ³•é”™è¯¯çº æ­£ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ¨ç†å’Œçº æ­£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è§„åˆ™å®šä¹‰ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒå’Œæ¨¡å‹è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è§„åˆ™å®šä¹‰ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒæ¨¡å‹ï¼Œæœ€åè¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå°†è§„åˆ™åŸºç¡€çš„å¼ºåŒ–å­¦ä¹ å¼•å…¥è¯­æ³•é”™è¯¯çº æ­£ä»»åŠ¡ï¼Œæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯æ¥æå‡LLMsçš„åº”ç”¨æ•ˆæœã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œå¼ºè°ƒäº†æ¨¡å‹çš„å¯æ§æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å¥–åŠ±æœºåˆ¶ä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†ä¼ ç»Ÿçš„äº¤å‰ç†µæŸå¤±ä¸å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ï¼Œä»¥å¹³è¡¡ç”Ÿæˆè´¨é‡ä¸æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨ä¸­æ–‡æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¬å›ç‡æ˜¾è‘—æé«˜ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚è¿™ä¸€æˆæœè¡¨æ˜è¯¥æ–¹æ³•åœ¨è¯­æ³•é”™è¯¯çº æ­£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€è‡ªåŠ¨å†™ä½œè¾…åŠ©å·¥å…·å’Œè¯­è¨€å­¦ä¹ å¹³å°ç­‰ã€‚é€šè¿‡æé«˜è¯­æ³•é”™è¯¯çº æ­£çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·çš„å†™ä½œä½“éªŒå’Œå­¦ä¹ æ•ˆæœï¼Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Grammatical error correction is a significant task in NLP. Traditional methods based on encoder-decoder models have achieved certain success, but the application of LLMs in this field is still underexplored. Current research predominantly relies on supervised fine-tuning to train LLMs to directly generate the corrected sentence, which limits the model's powerful reasoning ability. To address this limitation, we propose a novel framework based on Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL framework achieves \textbf{state-of-the-art }performance, with a notable increase in \textbf{recall}. This result clearly highlights the advantages of using RL to steer LLMs, offering a more controllable and reliable paradigm for future development in GEC.

