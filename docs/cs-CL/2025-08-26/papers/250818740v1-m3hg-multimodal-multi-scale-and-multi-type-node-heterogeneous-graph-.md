---
layout: default
title: M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations
---

# M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18740" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18740v1</a>
  <a href="https://arxiv.org/pdf/2508.18740.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18740v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18740v1', 'M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qiao Liang, Ying Shen, Tiantian Chen, Lin Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

**å¤‡æ³¨**: 16 pages, 8 figures. Accepted to Findings of ACL 2025

**æœŸåˆŠ**: Findings of ACL 2025 (2025) 11416-11431

**DOI**: [10.18653/v1/2025.findings-acl.596](https://doi.org/10.18653/v1/2025.findings-acl.596)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/redifinition/M3HG)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM3HGä»¥è§£å†³å¤šæ¨¡æ€å¯¹è¯ä¸­çš„æƒ…æ„ŸåŸå› ä¸‰å…ƒç»„æå–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æƒ…æ„Ÿåˆ†æ` `å¤šæ¨¡æ€å¯¹è¯` `å›¾ç¥ç»ç½‘ç»œ` `å¼‚æ„å›¾` `å› æœå…³ç³»æå–` `ç¤¾äº¤åª’ä½“åˆ†æ` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MECTECæ–¹æ³•æœªèƒ½æœ‰æ•ˆå»ºæ¨¡æƒ…æ„Ÿå’Œå› æœä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºM3HGæ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€å¼‚æ„å›¾æ˜¾å¼æ•æ‰æƒ…æ„Ÿå’Œå› æœä¸Šä¸‹æ–‡ï¼Œèåˆä¸åŒå±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM3HGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æƒ…æ„ŸåŸå› ä¸‰å…ƒç»„æå–ï¼ˆMECTECï¼‰åœ¨å¤šæ¨¡æ€å¯¹è¯åˆ†æä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œæ—¨åœ¨åŒæ—¶æå–æƒ…æ„Ÿå‘è¨€ã€åŸå› å‘è¨€å’Œæƒ…æ„Ÿç±»åˆ«ã€‚ç„¶è€Œï¼Œç›¸å…³æ•°æ®é›†çš„ç¨€ç¼ºæ€§é™åˆ¶äº†æ¨¡å‹çš„å‘å±•ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†MECADï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šåœºæ™¯çš„MECTECæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª56éƒ¨ç”µè§†å‰§çš„989ä¸ªå¯¹è¯ï¼Œæ¶µç›–å¤šç§å¯¹è¯èƒŒæ™¯ã€‚æ­¤å¤–ï¼Œç°æœ‰MECTECæ–¹æ³•æœªèƒ½æ˜ç¡®å»ºæ¨¡æƒ…æ„Ÿå’Œå› æœä¸Šä¸‹æ–‡ï¼Œä¸”å¿½è§†äº†ä¸åŒå±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯èåˆï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºäº†M3HGæ¨¡å‹ï¼Œèƒ½å¤Ÿæ˜ç¡®æ•æ‰æƒ…æ„Ÿå’Œå› æœä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€å¼‚æ„å›¾æœ‰æ•ˆèåˆä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒM3HGåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¯¹è¯ä¸­æƒ…æ„ŸåŸå› ä¸‰å…ƒç»„æå–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å»ºæ¨¡æƒ…æ„Ÿå’Œå› æœä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸”æœªèƒ½æœ‰æ•ˆèåˆä¸åŒå±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šM3HGæ¨¡å‹çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šæ¨¡æ€å¼‚æ„å›¾æ˜¾å¼æ•æ‰æƒ…æ„Ÿå’Œå› æœä¸Šä¸‹æ–‡ã€‚è¯¥è®¾è®¡æ—¨åœ¨å……åˆ†åˆ©ç”¨å¯¹è¯ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œæå‡æƒ…æ„Ÿå’Œå› æœå…³ç³»çš„æå–æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM3HGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡å¤šæ¨¡æ€è¾“å…¥è·å–å¯¹è¯çš„æƒ…æ„Ÿå’Œå› æœä¿¡æ¯ï¼›æ¥ç€ï¼Œæ„å»ºå¼‚æ„å›¾ä»¥è¡¨ç¤ºä¸åŒç±»å‹çš„èŠ‚ç‚¹å’Œè¾¹ï¼›æœ€åï¼Œé€šè¿‡å›¾ç¥ç»ç½‘ç»œè¿›è¡Œä¿¡æ¯èåˆå’Œç‰¹å¾æå–ã€‚

**å…³é”®åˆ›æ–°**ï¼šM3HGçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€å¼‚æ„å›¾çš„è®¾è®¡ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†æƒ…æ„Ÿå’Œå› æœä¸Šä¸‹æ–‡ï¼Œæ˜¾è‘—æå‡äº†ä¿¡æ¯èåˆçš„æ•ˆæœã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å¯¹ä¸Šä¸‹æ–‡çš„æ˜¾å¼å»ºæ¨¡èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æƒ…æ„Ÿå’Œå› æœä¿¡æ¯çš„æå–æ•ˆæœï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†å›¾ç¥ç»ç½‘ç»œä»¥å¢å¼ºç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒM3HGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æƒ…æ„Ÿå’Œå› æœæå–çš„å‡†ç¡®æ€§ä¸Šï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æå‡äº†çº¦15%çš„F1åˆ†æ•°ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“åˆ†æã€å®¢æˆ·æœåŠ¡å¯¹è¯ç³»ç»Ÿä»¥åŠæƒ…æ„Ÿè®¡ç®—ç­‰ã€‚é€šè¿‡æå‡æƒ…æ„ŸåŸå› ä¸‰å…ƒç»„çš„æå–èƒ½åŠ›ï¼ŒM3HGèƒ½å¤Ÿä¸ºæƒ…æ„Ÿåˆ†æã€ç”¨æˆ·ä½“éªŒä¼˜åŒ–å’Œæ™ºèƒ½å¯¹è¯ç³»ç»Ÿæä¾›æ›´ç²¾å‡†çš„æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has recently gained significant attention in social media analysis, aiming to extract emotion utterances, cause utterances, and emotion categories simultaneously. However, the scarcity of related datasets, with only one published dataset featuring highly uniform dialogue scenarios, hinders model development in this field. To address this, we introduce MECAD, the first multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56 TV series spanning a wide range of dialogue contexts. In addition, existing MECTEC methods fail to explicitly model emotional and causal contexts and neglect the fusion of semantic information at different levels, leading to performance degradation. In this paper, we propose M3HG, a novel model that explicitly captures emotional and causal contexts and effectively fuses contextual information at both inter- and intra-utterance levels via a multimodal heterogeneous graph. Extensive experiments demonstrate the effectiveness of M3HG compared with existing state-of-the-art methods. The codes and dataset are available at https://github.com/redifinition/M3HG.

