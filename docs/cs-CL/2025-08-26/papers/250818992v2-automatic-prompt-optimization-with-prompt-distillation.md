---
layout: default
title: Automatic Prompt Optimization with Prompt Distillation
---

# Automatic Prompt Optimization with Prompt Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18992" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18992v2</a>
  <a href="https://arxiv.org/pdf/2508.18992.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18992v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18992v2', 'Automatic Prompt Optimization with Prompt Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ernest A. Dyagin, Nikita I. Kulin, Artur R. Khairullin, Viktor N. Zhuravlev, Alena N. Sitkina

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-09-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDistillPromptä»¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨æç¤ºé€‰æ‹©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨æç¤ºä¼˜åŒ–` `è¯­è¨€æ¨¡å‹` `è’¸é¦` `æ–‡æœ¬åˆ†ç±»` `ç”Ÿæˆä»»åŠ¡` `å¤šé˜¶æ®µé›†æˆ` `ä»»åŠ¡ç‰¹å®šä¿¡æ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨æç¤ºæ–¹æ³•åœ¨ä¼˜åŒ–æç¤ºé€‰æ‹©æ—¶é¢ä¸´æ•ˆç‡ä½ä¸‹å’Œæ•ˆæœä¸ä½³çš„æŒ‘æˆ˜ã€‚
2. DistillPrompté€šè¿‡å¤šé˜¶æ®µé›†æˆä»»åŠ¡ç‰¹å®šä¿¡æ¯ï¼Œåˆ©ç”¨è’¸é¦å’Œèšåˆæ“ä½œæ¥ä¼˜åŒ–æç¤ºé€‰æ‹©ã€‚
3. åœ¨å¤šä¸ªæ–‡æœ¬åˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡çš„æ•°æ®é›†ä¸Šï¼ŒDistillPromptåœ¨å…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°20.12%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨æç¤ºä¼˜åŒ–æ˜¯ä¸ºè¯­è¨€æ¨¡å‹è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–æç¤ºçš„è¿‡ç¨‹ï¼Œéšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸçš„å¿«é€Ÿå‘å±•è€Œå—åˆ°å…³æ³¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDistillPromptçš„æ–°å‹è‡ªåŠ¨æç¤ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¤šé˜¶æ®µé›†æˆä»»åŠ¡ç‰¹å®šä¿¡æ¯æ¥ä¼˜åŒ–æç¤ºã€‚DistillPromptåˆ©ç”¨è’¸é¦ã€å‹ç¼©å’Œèšåˆæ“ä½œï¼Œæ›´å…¨é¢åœ°æ¢ç´¢æç¤ºç©ºé—´ã€‚è¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡çš„ä¸åŒæ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºåœ¨å…³é”®æŒ‡æ ‡ä¸Šç›¸æ¯”ç°æœ‰æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼ˆä¾‹å¦‚ï¼Œä¸Gripsç›¸æ¯”ï¼Œæ•´ä½“æ•°æ®é›†å¹³å‡æå‡20.12%ï¼‰ï¼Œç¡®ç«‹äº†DistillPromptä½œä¸ºä¸€ç§æœ‰æ•ˆçš„éæ¢¯åº¦è‡ªåŠ¨æç¤ºæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨æç¤ºæ–¹æ³•åœ¨æç¤ºé€‰æ‹©æ•ˆç‡å’Œæ•ˆæœä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨ä»»åŠ¡ç‰¹å®šä¿¡æ¯ï¼Œå¯¼è‡´æç¤ºé€‰æ‹©ä¸å¤Ÿä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDistillPromptçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šé˜¶æ®µé›†æˆä»»åŠ¡ç‰¹å®šä¿¡æ¯ï¼Œç»“åˆè’¸é¦ã€å‹ç¼©å’Œèšåˆæ“ä½œï¼Œå…¨é¢æ¢ç´¢æç¤ºç©ºé—´ï¼Œä»è€Œä¼˜åŒ–æç¤ºé€‰æ‹©çš„è¿‡ç¨‹ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDistillPromptçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ä»»åŠ¡ç‰¹å®šä¿¡æ¯é›†æˆã€æç¤ºç”Ÿæˆå’Œä¼˜åŒ–å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ¨¡å‹é€šè¿‡é¢„å¤„ç†æ­¥éª¤è·å–è®­ç»ƒæ•°æ®ï¼Œç„¶ååœ¨é›†æˆé˜¶æ®µå¼•å…¥ä»»åŠ¡ç‰¹å®šä¿¡æ¯ï¼Œæœ€åç”Ÿæˆå’Œä¼˜åŒ–æç¤ºä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šDistillPromptçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šé˜¶æ®µé›†æˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†ä»»åŠ¡ç‰¹å®šä¿¡æ¯èå…¥æç¤ºä¸­ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€æç¤ºé€‰æ‹©ç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒDistillPrompté‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æç¤ºç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†è’¸é¦å’Œèšåˆæœºåˆ¶ï¼Œä»¥æé«˜æç¤ºçš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDistillPromptåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šç›¸æ¯”äºåŸºçº¿æ–¹æ³•Gripsï¼Œå¹³å‡æå‡äº†20.12%çš„å…³é”®æŒ‡æ ‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è‡ªåŠ¨æç¤ºä¼˜åŒ–ä¸­çš„æ˜¾è‘—æ•ˆæœã€‚è¿™ä¸€æå‡ä¸ä»…éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬åˆ†ç±»ã€ç”Ÿæˆä»»åŠ¡ä»¥åŠå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–æç¤ºé€‰æ‹©ï¼ŒDistillPromptèƒ½å¤Ÿæ˜¾è‘—æå‡è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œåº”ç”¨å‰æ™¯ï¼Œæœªæ¥å¯èƒ½å¯¹æ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹ç”Ÿæˆç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting.

