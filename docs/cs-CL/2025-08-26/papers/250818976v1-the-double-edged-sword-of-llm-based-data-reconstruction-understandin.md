---
layout: default
title: The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization
---

# The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18976" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.18976v1</a>
  <a href="https://arxiv.org/pdf/2508.18976.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18976v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18976v1', 'The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Stephen Meisenbacher, Alexandra Klymenko, Andreea-Elena Bodea, Florian Matthes

**ÂàÜÁ±ª**: cs.CR, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-26

**Â§áÊ≥®**: 15 pages, 4 figures, 8 tables. Accepted to WPES @ CCS 2025

**DOI**: [10.1145/3733802.3764058](https://doi.org/10.1145/3733802.3764058)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LLMËæÖÂä©ÁöÑÊï∞ÊçÆÈáçÊûÑÊñπÊ≥ï‰ª•ÁºìËß£ÊñáÊú¨ÈöêÁßÅÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â∑ÆÂàÜÈöêÁßÅ` `ÊñáÊú¨Ê∂àÊØí` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `‰∏ä‰∏ãÊñáËÑÜÂº±ÊÄß` `Êï∞ÊçÆÈáçÊûÑ` `ÈöêÁßÅ‰øùÊä§` `Êú∫Âô®Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂü∫‰∫éÂ∑ÆÂàÜÈöêÁßÅÁöÑÊñáÊú¨Ê∂àÊØíÊñπÊ≥ïÂú®ÈöèÊú∫ÂåñËøáÁ®ã‰∏≠ÂÆπÊòìÁïô‰∏ã‰∏ä‰∏ãÊñáÁ∫øÁ¥¢ÔºåÂØºËá¥ÈöêÁßÅ‰øùÊä§‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊù•ËØÜÂà´ÂíåÂà©Áî®ÊñáÊú¨ÁöÑ‰∏ä‰∏ãÊñáËÑÜÂº±ÊÄßÔºåÂêåÊó∂Êé¢Á¥¢ÂÖ∂Âú®ÈöêÁßÅ‰øùÊä§‰∏≠ÁöÑÂèåÈáç‰ΩúÁî®„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLLMsÂú®Êé®Êñ≠ÂéüÂßãËØ≠‰πâÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÂêåÊó∂‰πüËÉΩÊîπÂñÑÂ∑ÆÂàÜÈöêÁßÅÊñáÊú¨ÁöÑË¥®ÈáèÔºåÊèêÂçáÈöêÁßÅ‰øùÊä§ÊïàÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â∑ÆÂàÜÈöêÁßÅÊñáÊú¨Ê∂àÊØíÊòØÊåáÂú®Â∑ÆÂàÜÈöêÁßÅÊ°ÜÊû∂‰∏ãÂØπÊñáÊú¨ËøõË°åÈöêÁßÅÂåñÂ§ÑÁêÜÔºå‰ª•Êèê‰æõÂèØËØÅÊòéÁöÑÈöêÁßÅ‰øùÈöú„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂü∫‰∫éÂçïËØçÁ∫ßÂà´ÁöÑÂ∑ÆÂàÜÈöêÁßÅÊñáÊú¨Ê∂àÊØíÊñπÊ≥ïÂ≠òÂú®‰∏ÄÂÆöÁöÑÂ±ÄÈôêÊÄßÔºåÂ∞§ÂÖ∂ÊòØÁî±‰∫éÈöèÊú∫ÂåñÂ§ÑÁêÜÁïô‰∏ãÁöÑ‰∏ä‰∏ãÊñáÁ∫øÁ¥¢ÔºåÂØºËá¥ÊâÄË∞ìÁöÑ‚Äú‰∏ä‰∏ãÊñáËÑÜÂº±ÊÄß‚Äù„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Âà©Áî®ËøôÁßç‰∏ä‰∏ãÊñáËÑÜÂº±ÊÄßÊñπÈù¢ÁöÑÊΩúÂäõÔºåÂèëÁé∞LLMs‰∏ç‰ªÖÂèØ‰ª•Êé®Êñ≠ÂéüÂßãËØ≠‰πâÔºåËøòÂèØËÉΩÂâäÂº±ÈöêÁßÅ‰øùÊä§Ôºå‰ΩÜ‰πüÂèØ‰ª•Áî®‰∫éÊîπÂñÑÂ∑ÆÂàÜÈöêÁßÅÊñáÊú¨ÁöÑË¥®ÈáèÂíåÈöêÁßÅÊÄß„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåÊèêÂá∫‰∫ÜÂ∞ÜLLMÊï∞ÊçÆÈáçÊûÑ‰Ωú‰∏∫ÂêéÂ§ÑÁêÜÊ≠•È™§ÁöÑÂª∫ËÆÆÔºå‰ª•Â¢ûÂº∫ÈöêÁßÅ‰øùÊä§„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Âü∫‰∫éÂ∑ÆÂàÜÈöêÁßÅÁöÑÊñáÊú¨Ê∂àÊØíÊñπÊ≥ïÂú®ÈöèÊú∫ÂåñËøáÁ®ã‰∏≠Áïô‰∏ãÁöÑ‰∏ä‰∏ãÊñáÁ∫øÁ¥¢ÈóÆÈ¢òÔºåËøôÁßç‰∏ä‰∏ãÊñáËÑÜÂº±ÊÄßÂèØËÉΩË¢´ÊîªÂáªËÄÖÂà©Áî®Ôºå‰ªéËÄåÂâäÂº±ÈöêÁßÅ‰øùÊä§ÊïàÊûú„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑ‰∏ä‰∏ãÊñáÁêÜËß£ËÉΩÂäõÔºåÊé¢Á¥¢Â¶Ç‰ΩïÂú®ËØÜÂà´‰∏ä‰∏ãÊñáËÑÜÂº±ÊÄßÁöÑÂêåÊó∂ÔºåÂà©Áî®ÂÖ∂ÈáçÊûÑËÉΩÂäõÊù•ÊîπÂñÑÊñáÊú¨ÁöÑÈöêÁßÅÊÄßÂíåË¥®Èáè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂ÈááÁî®‰∫ÜÂ§öÁßçÊñáÊú¨Ê∂àÊØíÊú∫Âà∂ÔºåÂπ∂Âú®‰∏çÂêåÈöêÁßÅÁ∫ßÂà´‰∏ãËøõË°åÊµãËØïÔºåÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅLLMÈáçÊûÑ„ÄÅÈöêÁßÅËØÑ‰º∞ÂíåÊïàÊûúÈ™åËØÅÁ≠â‰∏ªË¶ÅÊ®°Âùó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑÂàõÊñ∞Âú®‰∫éÈ¶ñÊ¨°Á≥ªÁªüÊÄßÂú∞Êé¢ËÆ®‰∫ÜLLMsÂú®Â∑ÆÂàÜÈöêÁßÅÊñáÊú¨Ê∂àÊØí‰∏≠ÁöÑÂèåÈáç‰ΩúÁî®ÔºåÊó¢ÂèØ‰ª•Ë¢´Áî®‰∫éÊîªÂáªÔºå‰πüÂèØ‰ª•‰Ωú‰∏∫ÊèêÂçáÈöêÁßÅ‰øùÊä§ÁöÑÂ∑•ÂÖ∑„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠ÔºåËÆæÁΩÆ‰∫Ü‰∏çÂêåÁöÑÈöêÁßÅÂèÇÊï∞ÔºåÈááÁî®‰∫ÜÂ§öÁßçÊçüÂ§±ÂáΩÊï∞Êù•ËØÑ‰º∞ÊñáÊú¨ÈáçÊûÑÁöÑË¥®ÈáèÔºåÂπ∂ËÆæËÆ°‰∫ÜÈÄÇÂ∫îÊÄßÁΩëÁªúÁªìÊûÑ‰ª•‰ºòÂåñLLMÁöÑÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLLMsÂú®Êé®Êñ≠ÂéüÂßãÊñáÊú¨ËØ≠‰πâÊñπÈù¢ÁöÑÂáÜÁ°ÆÁéáÊòæËëóÊèêÈ´òÔºåÈÉ®ÂàÜÊÉÖÂÜµ‰∏ãÈöêÁßÅ‰øùÊä§ÊïàÊûúÊèêÂçá‰∫Ü20%‰ª•‰∏ä„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®LLMsËøõË°åÂêéÂ§ÑÁêÜÁöÑÊñáÊú¨Âú®Ë¥®Èáè‰∏ä‰πüÊúâÊòéÊòæÊîπÂñÑÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÂèåÈáç‰ΩúÁî®ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïèÊÑü‰ø°ÊÅØÁöÑÊñáÊú¨Â§ÑÁêÜ„ÄÅÁ§æ‰∫§Â™í‰ΩìÂÜÖÂÆπÁöÑÈöêÁßÅ‰øùÊä§‰ª•ÂèäÂåªÁñóËÆ∞ÂΩïÁöÑÂÆâÂÖ®Â≠òÂÇ®Á≠â„ÄÇÈÄöËøáÊîπËøõÂ∑ÆÂàÜÈöêÁßÅÊñáÊú¨Ê∂àÊØíÊäÄÊúØÔºåÂèØ‰ª•Âú®‰øùÊä§Áî®Êà∑ÈöêÁßÅÁöÑÂêåÊó∂ÔºåÁ°Æ‰øù‰ø°ÊÅØÁöÑÂèØÁî®ÊÄßÂíåË¥®ÈáèÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Differentially private text sanitization refers to the process of privatizing texts under the framework of Differential Privacy (DP), providing provable privacy guarantees while also empirically defending against adversaries seeking to harm privacy. Despite their simplicity, DP text sanitization methods operating at the word level exhibit a number of shortcomings, among them the tendency to leave contextual clues from the original texts due to randomization during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual vulnerability}$. Given the powerful contextual understanding and inference capabilities of Large Language Models (LLMs), we explore to what extent LLMs can be leveraged to exploit the contextual vulnerability of DP-sanitized texts. We expand on previous work not only in the use of advanced LLMs, but also in testing a broader range of sanitization mechanisms at various privacy levels. Our experiments uncover a double-edged sword effect of LLM-based data reconstruction attacks on privacy and utility: while LLMs can indeed infer original semantics and sometimes degrade empirical privacy protections, they can also be used for good, to improve the quality and privacy of DP-sanitized texts. Based on our findings, we propose recommendations for using LLM data reconstruction as a post-processing step, serving to increase privacy protection by thinking adversarially.

