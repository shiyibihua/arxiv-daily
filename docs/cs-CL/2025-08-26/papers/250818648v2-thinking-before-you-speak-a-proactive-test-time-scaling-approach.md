---
layout: default
title: Thinking Before You Speak: A Proactive Test-time Scaling Approach
---

# Thinking Before You Speak: A Proactive Test-time Scaling Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18648" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18648v2</a>
  <a href="https://arxiv.org/pdf/2508.18648.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18648v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18648v2', 'Thinking Before You Speak: A Proactive Test-time Scaling Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cong Liu, Wenchang Chai, Hejun Wu, Yan Pan, Pengxu Wei, Liang Lin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-08-27)

**æœŸåˆŠ**: EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTBYSæ¡†æ¶ä»¥è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€ç»´ç¼ºå¤±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤æ‚æ¨ç†` `ä¸»åŠ¨ç”Ÿæˆ` `æ¨ç†æ¡†æ¶` `æ•°å­¦é—®é¢˜` `æ´å¯Ÿæœºåˆ¶` `è‡ªåŠ¨åŒ–æ¨ç†` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦é—®é¢˜ä¸Šï¼Œç¼ºä¹æœ‰æ•ˆçš„æ€ç»´è¿‡ç¨‹è¡¨è¾¾ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¡†æ¶TBYSï¼Œé€šè¿‡åœ¨æ¨ç†æ­¥éª¤ä¹‹é—´æ’å…¥ä¸»åŠ¨ç”Ÿæˆçš„â€œæ´å¯Ÿâ€æ¥å¼•å¯¼æ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTBYSåœ¨å¤šä¸ªæ•°å­¦æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶å¸¸å¸¸è¡¨ç°å‡ºä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦é—®é¢˜ä¸Šã€‚è¿™ä¸»è¦æºäºäººç±»æ¨ç†æ¨¡å¼ä¸LLMsè®­ç»ƒæ•°æ®ä¹‹é—´çš„å·®å¼‚ã€‚äººç±»åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶å€¾å‘äºä»”ç»†æ€è€ƒï¼Œä½†å¾€å¾€ä¸è¡¨è¾¾å†…å¿ƒçš„æƒ³æ³•å’Œæ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåœ¨è¿ç»­æ¨ç†æ­¥éª¤ä¹‹é—´æ’å…¥â€œæ´å¯Ÿâ€ï¼Œä»¥å›é¡¾çŠ¶æ€å¹¶å¼•å¯¼ä¸‹ä¸€ä¸ªæ¨ç†æ­¥éª¤ã€‚ä¸ä»¥å¾€ä¾èµ–é™æ€æç¤ºçš„ç­–ç•¥ä¸åŒï¼Œâ€œæ´å¯Ÿâ€æ˜¯ä¸»åŠ¨ç”Ÿæˆçš„ï¼Œæ—¨åœ¨æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬å®ç°äº†è¿™ä¸€æ€æƒ³ï¼Œæ„å»ºäº†åä¸ºâ€œæ€è€ƒå†å‘è¨€â€ï¼ˆTBYSï¼‰çš„æ¨ç†æ¡†æ¶ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ”¶é›†å’Œè¿‡æ»¤ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„ç®¡é“ï¼Œä»¥ç”Ÿæˆâ€œæ´å¯Ÿâ€ï¼Œä»è€Œå‡è½»äººå·¥æ ‡æ³¨å’Œå¾®è°ƒçš„è´Ÿæ‹…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTBYSåœ¨æŒ‘æˆ˜æ€§çš„æ•°å­¦æ•°æ®é›†ä¸Šæœ‰æ•ˆæå‡äº†æ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹æœ‰æ•ˆæ€ç»´è¿‡ç¨‹è¡¨è¾¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–é™æ€æç¤ºï¼Œæ— æ³•æœ‰æ•ˆå¼•å¯¼æ¨ç†æ­¥éª¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåœ¨æ¨ç†æ­¥éª¤ä¹‹é—´æ’å…¥â€œæ´å¯Ÿâ€ï¼Œè¿™äº›â€œæ´å¯Ÿâ€æ˜¯ä¸»åŠ¨ç”Ÿæˆçš„ï¼Œèƒ½å¤Ÿå›é¡¾å½“å‰çŠ¶æ€å¹¶å¼•å¯¼ä¸‹ä¸€æ­¥æ¨ç†ï¼Œä»è€Œæ›´å¥½åœ°æ¨¡æ‹Ÿäººç±»çš„æ€ç»´è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTBYSæ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆæ˜¯è‡ªåŠ¨æ”¶é›†å’Œè¿‡æ»¤ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„ç®¡é“ï¼Œå…¶æ¬¡æ˜¯ç”Ÿæˆâ€œæ´å¯Ÿâ€çš„æœºåˆ¶ï¼Œæœ€åæ˜¯å°†â€œæ´å¯Ÿâ€èå…¥æ¨ç†è¿‡ç¨‹çš„æ¨ç†å¼•æ“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºâ€œæ´å¯Ÿâ€çš„ä¸»åŠ¨ç”Ÿæˆï¼Œè¿™ä¸ä¼ ç»Ÿçš„é™æ€æç¤ºæ–¹æ³•å½¢æˆäº†æœ¬è´¨åŒºåˆ«ï¼Œä½¿å¾—æ¨ç†è¿‡ç¨‹æ›´åŠ çµæ´»å’Œæœ‰æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬â€œæ´å¯Ÿâ€çš„ç”Ÿæˆç®—æ³•å’Œè¿‡æ»¤æ ‡å‡†ï¼ŒæŸå¤±å‡½æ•°åˆ™ç”¨äºä¼˜åŒ–æ¨ç†çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºç°æœ‰çš„è¯­è¨€æ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼Œä»¥é€‚åº”æ–°çš„æ¨ç†éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTBYSåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§æ•°å­¦æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ¨ç†å‡†ç¡®ç‡æé«˜äº†çº¦15%ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–æ¨ç†ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒTBYSæ¡†æ¶èƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®çš„è§£ç­”å’Œæ›´é«˜æ•ˆçš„å­¦ä¹ æ”¯æŒï¼Œæœªæ¥å¯èƒ½åœ¨å¤šä¸ªè¡Œä¸šä¸­äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often exhibit deficiencies with complex reasoning tasks, such as maths, which we attribute to the discrepancy between human reasoning patterns and those presented in the LLMs' training data. When dealing with complex problems, humans tend to think carefully before expressing solutions. However, they often do not articulate their inner thoughts, including their intentions and chosen methodologies. Consequently, critical insights essential for bridging reasoning steps may be absent in training data collected from human sources. To bridge this gap, we proposes inserting \emph{insight}s between consecutive reasoning steps, which review the status and initiate the next reasoning steps. Unlike prior prompting strategies that rely on a single or a workflow of static prompts to facilitate reasoning, \emph{insight}s are \emph{proactively} generated to guide reasoning processes. We implement our idea as a reasoning framework, named \emph{Thinking Before You Speak} (TBYS), and design a pipeline for automatically collecting and filtering in-context examples for the generation of \emph{insight}s, which alleviates human labeling efforts and fine-tuning overheads. Experiments on challenging mathematical datasets verify the effectiveness of TBYS. Project website: https://gitee.com/jswrt/TBYS

