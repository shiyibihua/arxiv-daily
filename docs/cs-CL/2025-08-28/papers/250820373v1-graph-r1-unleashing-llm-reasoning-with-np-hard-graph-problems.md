---
layout: default
title: Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems
---

# Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20373" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20373v1</a>
  <a href="https://arxiv.org/pdf/2508.20373.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20373v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20373v1', 'Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuyao Wang, Bowen Liu, Jianheng Tang, Nuo Chen, Yuhan Li, Qifan Zhang, Jia Li

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-28

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Graph-Reasoner/Graph-R1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNPéš¾åº¦å›¾é—®é¢˜ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `NPéš¾åº¦å›¾` `åˆæˆè®­ç»ƒ` `å¼ºåŒ–å­¦ä¹ ` `é•¿é“¾æ¨ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šä¾èµ–é«˜è´¨é‡æ•°æ®é›†ï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ã€‚
2. æå‡ºä½¿ç”¨NPéš¾åº¦å›¾é—®é¢˜ä½œä¸ºåˆæˆè®­ç»ƒè¯­æ–™åº“ï¼Œå¢å¼ºæ¨¡å‹çš„é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚
3. Graph-R1-7Bæ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œæå‡äº†æ¨ç†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆRLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™ä¸»è¦å¾—ç›Šäºå…¶é•¿é“¾æ¨ç†ï¼ˆLong CoTï¼‰èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¼€å‘è¿™äº›èƒ½åŠ›ä¾èµ–äºé«˜è´¨é‡æ•°æ®é›†çš„åæœŸè®­ç»ƒï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸æˆæœ¬é«˜ä¸”éœ€äººå·¥ç­–åˆ’ã€‚æœ¬æ–‡æå‡ºå°†NPéš¾åº¦å›¾é—®é¢˜ä½œä¸ºä¸€ç§æ–°å‹åˆæˆè®­ç»ƒè¯­æ–™åº“ï¼Œå› ä¸ºè¿™äº›é—®é¢˜æœ¬è´¨ä¸Šéœ€è¦æ·±åº¦æ¨ç†ã€å¹¿æ³›æ¢ç´¢å’Œåæ€ç­–ç•¥ï¼Œè¿™äº›éƒ½æ˜¯é•¿é“¾æ¨ç†çš„æ ¸å¿ƒç‰¹å¾ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„åæœŸè®­ç»ƒæ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ·±åº¦å’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„æ——èˆ°æ¨¡å‹Graph-R1-7Båœ¨æ•°å­¦ã€ç¼–ç¨‹ã€STEMå’Œé€»è¾‘ç­‰é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨NPéš¾åº¦å›¾é—®é¢˜ä¸Šè¶…è¶Šäº†QwQ-32Bï¼Œå±•ç°äº†è‰¯å¥½çš„å‡†ç¡®æ€§å’Œæ¨ç†æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å¯¹é«˜è´¨é‡æ•°æ®é›†çš„ä¾èµ–é—®é¢˜ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸æˆæœ¬é«˜ä¸”éš¾ä»¥è·å–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥NPéš¾åº¦å›¾é—®é¢˜ä½œä¸ºåˆæˆè®­ç»ƒè¯­æ–™åº“ï¼Œåˆ©ç”¨å…¶æ·±åº¦æ¨ç†å’Œæ¢ç´¢çš„ç‰¹æ€§ï¼Œæ¥å¢å¼ºæ¨¡å‹çš„é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å¯¹æ‹’ç»é‡‡æ ·çš„NPéš¾åº¦å›¾å®ä¾‹è¿›è¡Œé•¿é“¾æ¨ç†çš„ç›‘ç£å¾®è°ƒï¼Œç¬¬äºŒé˜¶æ®µæ˜¯é€šè¿‡ç»†ç²’åº¦å¥–åŠ±è®¾è®¡çš„å¼ºåŒ–å­¦ä¹ æ¥æå‡æ¨ç†æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†NPéš¾åº¦å›¾é—®é¢˜ä½œä¸ºè®­ç»ƒæ•°æ®æºï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„æ–¹å¼æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿä¾èµ–äººå·¥ç­–åˆ’æ•°æ®é›†çš„æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†æ‹’ç»é‡‡æ ·æŠ€æœ¯æ¥é€‰æ‹©å›¾å®ä¾‹ï¼Œå¹¶è®¾è®¡äº†ç»†ç²’åº¦çš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGraph-R1-7Båœ¨NPéš¾åº¦å›¾é—®é¢˜ä¸Šè¶…è¶Šäº†QwQ-32Bï¼Œè¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ¨ç†æ•ˆç‡ï¼Œæ ‡å¿—ç€åœ¨æ•°å­¦ã€ç¼–ç¨‹å’Œé€»è¾‘æ¨ç†ç­‰é¢†åŸŸçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç¼–ç¨‹è¾…åŠ©ã€ç§‘å­¦ç ”ç©¶ç­‰ï¼Œèƒ½å¤Ÿä¸ºå¤æ‚æ¨ç†ä»»åŠ¡æä¾›é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½åœ¨è‡ªåŠ¨åŒ–å†³ç­–ã€æ™ºèƒ½é—®ç­”ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. Our flagship model, Graph-R1-7B, demonstrates strong generalization across mathematics, coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLMs, opening a new frontier for LLM post-training. Our implementation is available at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted in our Hugging Face collection HKUST-DSAIL/Graph-R1.

