---
layout: default
title: Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction
---

# Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20395" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20395v1</a>
  <a href="https://arxiv.org/pdf/2508.20395.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20395v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20395v1', 'Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xu Guo

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-28

**å¤‡æ³¨**: 11 pages, 4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ¡ä»¶ç†µå‡å°‘æµ‹é‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç”¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç”¨` `æ¡ä»¶ç†µ` `ç”Ÿæˆæ¨¡å‹` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯„ä¼°æ¨ç†æ­¥éª¤å¯¹æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„è´¡çŒ®ï¼Œå¯¼è‡´ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸å¿…è¦å¹²æ‰°ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ¡ä»¶ç†µæ¥é‡åŒ–æ¨ç†é“¾çš„æ•ˆç”¨ï¼Œè¿›è€Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ï¼Œé¿å…æ— æ•ˆæ¨ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¡ä»¶ç†µé€æ­¥é™ä½ä¸æ­£ç¡®ç­”æ¡ˆé«˜åº¦ç›¸å…³ï¼Œè€Œé”™è¯¯æ¨ç†è·¯å¾„é€šå¸¸æ›´é•¿ï¼Œæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•é€šå¸¸ä¾èµ–äºç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ä»¥æé«˜å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå…³äºæ¨ç†æ•ˆç”¨å¦‚ä½•å½±å“æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„ç ”ç©¶è¾ƒå°‘ã€‚ç”±äºè‡ªå›å½’ç”Ÿæˆçš„éšæœºæ€§ï¼Œç”Ÿæˆæ›´å¤šä¸Šä¸‹æ–‡å¹¶ä¸ä¸€å®šèƒ½æé«˜ç­”æ¡ˆçš„ä¿¡å¿ƒã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é¢„æµ‹æŸä¸ªæ¨ç†æ­¥éª¤æ˜¯å¦æœ‰ç”¨ï¼Œå°±å¯ä»¥æå‰åœæ­¢æˆ–ä¿®å‰ªæ— æ•ˆæ­¥éª¤ï¼Œä»è€Œé¿å…å¯¹æœ€ç»ˆå†³ç­–çš„å¹²æ‰°ã€‚æœ¬æ–‡åœ¨MATHæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸€é¡¹oracleç ”ç©¶ï¼Œä½¿ç”¨Qwen2.5-32Bå’ŒGPT-4oç”Ÿæˆæ¨ç†é“¾ï¼Œç„¶ååˆ©ç”¨ä¸€ä¸ªç‹¬ç«‹æ¨¡å‹ï¼ˆQwen3-8Bï¼‰é‡åŒ–è¿™äº›é“¾å¯¹æœ€ç»ˆå‡†ç¡®æ€§çš„æ•ˆç”¨ã€‚ç»“æœè¡¨æ˜ï¼Œæ¡ä»¶ç†µåœ¨æ­¥éª¤ä¸­é€æ¸é™ä½ä¸æ­£ç¡®ç­”æ¡ˆæœ‰å¼ºå…³è”ï¼Œè€Œå¹³å¦æˆ–å¢åŠ çš„ç†µå¾€å¾€å¯¼è‡´é”™è¯¯ç­”æ¡ˆã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥è®¾è®¡é«˜æ•ˆçš„æ¨ç†ç®¡é“æä¾›äº†åŸºç¡€ï¼Œèƒ½å¤Ÿæ—©æœŸæ£€æµ‹å’Œé¿å…æ— æ•ˆæ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ¨ç†æ­¥éª¤å¯¹æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„å½±å“ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œå¯¼è‡´ç”Ÿæˆçš„ä¸Šä¸‹æ–‡å¯èƒ½æ— æ•ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¡ä»¶ç†µæ¥é‡åŒ–æ¨ç†æ­¥éª¤çš„æ•ˆç”¨ï¼Œé€šè¿‡åˆ†ææ¯ä¸€æ­¥çš„ç†µå˜åŒ–æ¥åˆ¤æ–­æ¨ç†çš„æœ‰æ•ˆæ€§ï¼Œä»è€Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨Qwen2.5-32Bå’ŒGPT-4oç”Ÿæˆæ¨ç†é“¾ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨Qwen3-8Bæ¨¡å‹é‡åŒ–æ¨ç†é“¾çš„æ•ˆç”¨ï¼›æœ€åï¼Œé€šè¿‡æ¡ä»¶ç†µåˆ†æè¯„ä¼°æ¯ä¸€æ­¥çš„è´¡çŒ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥æ¡ä»¶ç†µä½œä¸ºè¯„ä¼°æ¨ç†æ­¥éª¤æœ‰æ•ˆæ€§çš„æŒ‡æ ‡ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ¨ç†ç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€è¯„ä¼°æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡é€æ­¥æ‰©å±•ä¸Šä¸‹æ–‡æ¥è®¡ç®—æ¡ä»¶ç†µï¼Œç¡®ä¿æ¯ä¸€æ­¥çš„æ¨ç†éƒ½èƒ½è¢«æœ‰æ•ˆè¯„ä¼°ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¡ä»¶ç†µåœ¨æ¨ç†æ­¥éª¤ä¸­é€æ­¥é™ä½ä¸æ­£ç¡®ç­”æ¡ˆæœ‰æ˜¾è‘—å…³è”ï¼Œä¸”é”™è¯¯æ¨ç†è·¯å¾„çš„é•¿åº¦æ™®éè¾ƒé•¿ã€‚è¿™ä¸€å‘ç°ä¸ºæ¨ç†è¿‡ç¨‹çš„ä¼˜åŒ–æä¾›äº†æ–°çš„è§†è§’ï¼Œå¼ºè°ƒäº†æœ‰æ•ˆæ¨ç†çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©æå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæœªæ¥çš„ç³»ç»Ÿå¯ä»¥æ›´é«˜æ•ˆåœ°æä¾›å‡†ç¡®ç­”æ¡ˆï¼Œå‡å°‘ç”¨æˆ·çš„è®¤çŸ¥è´Ÿæ‹…ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in large language models (LLMs) often rely on generating intermediate reasoning steps to enhance accuracy. However, little work has examined how reasoning utility contributes to the final answer's correctness. Due to the stochastic nature of autoregressive generation, generating more context does not guarantee increased confidence in the answer. If we could predict, during generation, whether a reasoning step will be useful, we could stop early or prune ineffective steps, avoiding distractions in the final decision.
>   We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to generate reasoning chains, and then employing a separate model (Qwen3-8B) to quantify the utility of these chains for final accuracy. Specifically, we measure the model's uncertainty on the answer span Y at each reasoning step using conditional entropy (expected negative log-likelihood over the vocabulary) with context expanding step by step. Our results show a clear pattern: conditional entropy that decreases over steps is strongly associated with correct answers, whereas flat or increasing entropy often results in wrong answers. We also corroborate that incorrect reasoning paths tend to be longer than correct ones, suggesting that longer reasoning does not necessarily yield better outcomes. These findings serve as a foundation to inspire future work on designing efficient reasoning pipelines that detect and avoid unproductive reasoning early.

