---
layout: default
title: MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models
---

# MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05928" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05928v1</a>
  <a href="https://arxiv.org/pdf/2506.05928.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05928v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05928v1', 'MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jie Cao, Tianwei Lin, Hongyang He, Rolan Yan, Wenqiao Zhang, Juncheng Li, Dongping Zhang, Siliang Tang, Yueting Zhuang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/DCDmllm/MoA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼‚æ„é€‚é…å™¨æ··åˆæ¨¡å‹ä»¥è§£å†³å‚æ•°é«˜æ•ˆå¾®è°ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¼‚æ„é€‚é…å™¨` `ä¸“å®¶æ··åˆ` `ä½ç§©é€‚é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒè´¨MoE-LoRAæ–¹æ³•å­˜åœ¨è¡¨ç¤ºå´©æºƒå’Œä¸“å®¶è´Ÿè½½ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œé™åˆ¶äº†å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºçš„å¼‚æ„æ··åˆé€‚é…å™¨ï¼ˆMoAï¼‰æ–¹æ³•é€šè¿‡åŠ¨æ€æ•´åˆå¤šæ ·ç»“æ„çš„é€‚é…å™¨ä¸“å®¶ï¼Œæå‡äº†çŸ¥è¯†è½¬ç§»çš„æœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼‚æ„MoAåœ¨æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„åŒè´¨MoE-LoRAæ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—çš„æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥çš„ç ”ç©¶å°†ä½ç§©é€‚é…ï¼ˆLoRAï¼‰å’Œä¸“å®¶æ··åˆï¼ˆMoEï¼‰ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨ä¸­çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é‡‡ç”¨åŒè´¨çš„MoE-LoRAæ¶æ„ï¼Œç”±ç»“æ„å’Œèƒ½åŠ›ç›¸ä¼¼æˆ–ç›¸åŒçš„LoRAä¸“å®¶ç»„æˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸é¢ä¸´è¡¨ç¤ºå´©æºƒå’Œä¸“å®¶è´Ÿè½½ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œå½±å“LLMçš„æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¼‚æ„çš„æ··åˆé€‚é…å™¨ï¼ˆMoAï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŠ¨æ€æ•´åˆå…·æœ‰å¤šæ ·ç»“æ„çš„PEFTé€‚é…å™¨ä¸“å®¶ï¼Œåˆ©ç”¨å…¶äº’è¡¥çš„è¡¨ç¤ºèƒ½åŠ›ä¿ƒè¿›ä¸“å®¶ä¸“ä¸šåŒ–ï¼Œä»è€Œå¢å¼ºé¢„è®­ç»ƒçŸ¥è¯†å‘ä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆè½¬ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼‚æ„MoAåœ¨æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ä¸Šå‡ä¼˜äºåŒè´¨çš„MoE-LoRAæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŒè´¨MoE-LoRAæ–¹æ³•ç”±äºä¸“å®¶ç»“æ„ç›¸ä¼¼ï¼Œå¯¼è‡´è¡¨ç¤ºå´©æºƒå’Œè´Ÿè½½ä¸å¹³è¡¡ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„å¼‚æ„æ··åˆé€‚é…å™¨ï¼ˆMoAï¼‰æ–¹æ³•é€šè¿‡å¼•å…¥å¤šæ ·åŒ–çš„é€‚é…å™¨ä¸“å®¶ï¼Œåˆ©ç”¨å…¶äº’è¡¥çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œä¿ƒè¿›ä¸“å®¶çš„ä¸“ä¸šåŒ–ï¼Œä»è€Œå¢å¼ºçŸ¥è¯†çš„æœ‰æ•ˆè½¬ç§»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoAæ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå˜ä½“ï¼šè½¯MoAå’Œç¨€ç–MoAã€‚è½¯MoAé€šè¿‡åŠ æƒèåˆæ‰€æœ‰ä¸“å®¶çš„è¾“å‡ºå®ç°ç»†ç²’åº¦é›†æˆï¼Œè€Œç¨€ç–MoAåˆ™æ ¹æ®ä¸“å®¶çš„è´¡çŒ®ç¨€ç–æ¿€æ´»é€‚é…å™¨ä¸“å®¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥å¼‚æ„é€‚é…å™¨ä¸“å®¶ï¼Œè§£å†³äº†åŒè´¨æ–¹æ³•ä¸­çš„è´Ÿè½½ä¸å¹³è¡¡å’Œè¡¨ç¤ºå´©æºƒé—®é¢˜ï¼Œæå‡äº†æ¨¡å‹çš„å‚æ•°æ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMoAé‡‡ç”¨äº†åŠ¨æ€åŠ æƒæœºåˆ¶æ¥èåˆä¸“å®¶è¾“å‡ºï¼Œå¹¶é€šè¿‡ç¨€ç–æ¿€æ´»ç­–ç•¥æ¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œç¡®ä¿åœ¨æ€§èƒ½ä¸‹é™æå°çš„æƒ…å†µä¸‹å®ç°ä¸“å®¶çš„æœ‰æ•ˆåˆ©ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¼‚æ„MoAåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå‡ä¼˜äºåŒè´¨MoE-LoRAæ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œä¸”åœ¨å‚æ•°æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°æ•ˆç‡å’Œæ€§èƒ½ï¼ŒMoAæ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒå®é™…åº”ç”¨ä¸­çš„çŸ¥è¯†è¿ç§»å’Œä»»åŠ¡é€‚åº”ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameter-efficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ \emph{homogeneous} MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose a \emph{heterogeneous} \textbf{Mixture-of-Adapters (MoA)} approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: \textbf{(i)} \textit{Soft MoA} achieves fine-grained integration by performing a weighted fusion of all expert outputs; \textbf{(ii)} \textit{Sparse MoA} activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA.

