---
layout: default
title: SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities
---

# SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06406" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06406v2</a>
  <a href="https://arxiv.org/pdf/2506.06406.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06406v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06406v2', 'SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guoyang Xia, Yifeng Ding, Fengfa Li, Lei Ren, Wei Chen, Fangxiang Feng, Xiaojie Wang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-06-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSMARä»¥è§£å†³å¤šæ¨¡æ€MoEæ¨¡å‹è¯­è¨€èƒ½åŠ›ä¸‹é™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `å¤šæ¨¡æ€å­¦ä¹ ` `è¯­è¨€æ¨¡å‹` `Kullback-Leibleræ•£åº¦` `è·¯ç”±ç­–ç•¥` `è§†è§‰æŒ‡ä»¤è°ƒä¼˜` `ä¸“å®¶é€‰æ‹©` `æ¨¡å‹æ­£åˆ™åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€MoEæ¨¡å‹åœ¨è®­ç»ƒæˆæœ¬é«˜æˆ–è¯­è¨€èƒ½åŠ›ä¸‹é™æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºçš„SMARé€šè¿‡Kullback-Leibleræ•£åº¦æ§åˆ¶æ¨¡æ€è·¯ç”±æ¦‚ç‡ï¼Œä¿ƒè¿›ä¸“å®¶ä¸“ä¸šåŒ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSMARåœ¨ä»…2.5%æ–‡æœ¬æ•°æ®ä¸‹ï¼Œè¯­è¨€èƒ½åŠ›ä¿ç•™ç‡è¾¾åˆ°86.6%ï¼Œä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å·²æˆä¸ºæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ„å»ºå¤šæ¨¡æ€MoEæ¨¡å‹çš„æ–¹æ³•å¾€å¾€é¢ä¸´é«˜è®­ç»ƒæˆæœ¬æˆ–åœ¨é€‚åº”é¢„è®­ç»ƒæ¨¡å‹æ—¶è¯­è¨€èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ­£åˆ™åŒ–æŠ€æœ¯â€”â€”è½¯æ¨¡æ€æ„ŸçŸ¥è·¯ç”±ï¼ˆSMARï¼‰ï¼Œé€šè¿‡ä½¿ç”¨Kullback-Leibleræ•£åº¦æ§åˆ¶æ¨¡æ€é—´çš„è·¯ç”±æ¦‚ç‡åˆ†å¸ƒï¼Œé¼“åŠ±ä¸“å®¶ä¸“ä¸šåŒ–ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„æˆ–è¿‡åº¦ä¾èµ–æ–‡æœ¬æ•°æ®ã€‚åœ¨è§†è§‰æŒ‡ä»¤è°ƒä¼˜å®éªŒä¸­ï¼ŒSMARåœ¨ä»…ä½¿ç”¨2.5%çº¯æ–‡æœ¬çš„æƒ…å†µä¸‹ï¼Œè¯­è¨€èƒ½åŠ›ä¿ç•™ç‡è¾¾åˆ°86.6%ï¼Œè¶…è¶Šäº†åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„å¤šæ¨¡æ€æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ºåœ¨å¤šæ¨¡æ€MoEæ¨¡å‹ä¸­å¹³è¡¡æ¨¡æ€å·®å¼‚åŒ–å’Œè¯­è¨€èƒ½åŠ›æä¾›äº†å®ç”¨é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€MoEæ¨¡å‹åœ¨é€‚åº”é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶ï¼Œé¢ä¸´çš„é«˜è®­ç»ƒæˆæœ¬å’Œè¯­è¨€èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¹³è¡¡æ¨¡æ€é—´çš„å·®å¼‚åŒ–ä¸è¯­è¨€èƒ½åŠ›çš„ä¿ç•™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSMARé€šè¿‡å¼•å…¥Kullback-Leibleræ•£åº¦ä½œä¸ºæ­£åˆ™åŒ–æ‰‹æ®µï¼Œæ§åˆ¶ä¸åŒæ¨¡æ€é—´çš„è·¯ç”±æ¦‚ç‡åˆ†å¸ƒï¼Œé¼“åŠ±æ¨¡å‹åœ¨ä¸æ”¹å˜æ¶æ„çš„æƒ…å†µä¸‹å®ç°ä¸“å®¶çš„ä¸“ä¸šåŒ–ã€‚æ­¤è®¾è®¡æ—¨åœ¨å‡å°‘å¯¹æ–‡æœ¬æ•°æ®çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒè¯­è¨€èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡æ€è·¯ç”±æ¨¡å—å’Œä¸“å®¶é€‰æ‹©æœºåˆ¶ã€‚æ¨¡æ€è·¯ç”±æ¨¡å—è´Ÿè´£æ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹å¾åŠ¨æ€è°ƒæ•´è·¯ç”±æ¦‚ç‡ï¼Œè€Œä¸“å®¶é€‰æ‹©æœºåˆ¶åˆ™ç¡®ä¿åœ¨æ¯æ¬¡æ¨ç†æ—¶é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶è¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šSMARçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä½¿ç”¨Kullback-Leibleræ•£åº¦æ¥ä¼˜åŒ–æ¨¡æ€é—´çš„è·¯ç”±ç­–ç•¥ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„åŸºäºç¡¬è·¯ç”±æˆ–ç®€å•åŠ æƒçš„ç­–ç•¥æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤šæ¨¡æ€è¾“å…¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼ŒSMARè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡æ¨¡æ€é—´çš„è·¯ç”±æ¦‚ç‡ï¼Œå¹¶é€šè¿‡è°ƒèŠ‚è¶…å‚æ•°æ¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œä¿æŒäº†åŸæœ‰çš„MoEæ¶æ„ï¼Œå¢åŠ äº†æ¨¡æ€æ„ŸçŸ¥çš„è·¯ç”±æœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSMARåœ¨è§†è§‰æŒ‡ä»¤è°ƒä¼˜ä»»åŠ¡ä¸­ï¼Œè¯­è¨€èƒ½åŠ›ä¿ç•™ç‡è¾¾åˆ°86.6%ï¼Œä»…éœ€2.5%çš„çº¯æ–‡æœ¬æ•°æ®ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å¼ºå¤§æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€è‡ªç„¶è¯­è¨€å¤„ç†ã€è§†è§‰é—®ç­”ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æœ‰æ•ˆå¹³è¡¡æ¨¡æ€é—´çš„å·®å¼‚åŒ–å’Œè¯­è¨€èƒ½åŠ›ï¼ŒSMARèƒ½å¤Ÿæå‡å¤šæ¨¡æ€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture of Experts (MoE) architectures have become a key approach for scaling large language models, with growing interest in extending them to multimodal tasks. Existing methods to build multimodal MoE models either incur high training costs or suffer from degraded language capabilities when adapting pretrained models. To address this, we propose Soft ModalityAware Routing (SMAR), a novel regularization technique that uses Kullback Leibler divergence to control routing probability distributions across modalities, encouraging expert specialization without modifying model architecture or heavily relying on textual data. Experiments on visual instruction tuning show that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance. Our approach offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models.

