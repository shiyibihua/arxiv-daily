---
layout: default
title: Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models
---

# Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06060" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06060v1</a>
  <a href="https://arxiv.org/pdf/2506.06060.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06060v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06060v1', 'Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yingqi Hu, Zhuo Zhang, Jingyuan Zhang, Lizhen Qu, Zenglin Xu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: 10 pages, 4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç®€å•æœ‰æ•ˆçš„æå–æ”»å‡»ç®—æ³•ä»¥è§£å†³è”é‚¦å¾®è°ƒä¸­çš„éšç§æ•°æ®é£é™©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `éšç§ä¿æŠ¤` `æ•°æ®æå–` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯` `å®‰å…¨æ€§è¯„ä¼°` `æ”»å‡»æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨ä¿æŠ¤æ•°æ®éšç§çš„åŒæ—¶ï¼Œé¢ä¸´ç€å¤§å‹è¯­è¨€æ¨¡å‹çš„è®°å¿†èƒ½åŠ›å¸¦æ¥çš„è®­ç»ƒæ•°æ®æå–æ”»å‡»é£é™©ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æå–æ”»å‡»ç®—æ³•ï¼Œæ”»å‡»è€…ä»…éœ€è®¿é—®å•ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å‰ç¼€è¿›è¡Œè·¨å®¢æˆ·ç«¯çš„ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯æå–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæå–é«˜è¾¾56.57%çš„å—å®³è€…ä¸“å±PIIï¼Œä¸”åœ¨å¤šä¸ªç±»åˆ«ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„è„†å¼±æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆFedLLMsï¼‰æ˜¯ä¸€ç§åœ¨æ•æ„Ÿé¢†åŸŸå®ç°å¼ºå¤§æ¨¡å‹æ€§èƒ½çš„æœ‰å‰æ™¯çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŠ¤æ•°æ®éšç§ã€‚ç„¶è€Œï¼ŒLLMså›ºæœ‰çš„è®°å¿†èƒ½åŠ›ä½¿å…¶å®¹æ˜“å—åˆ°è®­ç»ƒæ•°æ®æå–æ”»å‡»ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸“é—¨é’ˆå¯¹FedLLMsçš„ç®€å•æœ‰æ•ˆçš„æå–æ”»å‡»ç®—æ³•ï¼Œä¸ä»¥å¾€å‡è®¾è®¿é—®æ‰€æœ‰è®­ç»ƒæ•°æ®ç‰‡æ®µçš„â€œé€å­—â€æå–æ”»å‡»ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ›´ç°å®çš„å¨èƒæ¨¡å‹ä¸‹è¿ä½œï¼Œæ”»å‡»è€…ä»…è®¿é—®å•ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®ï¼Œæ—¨åœ¨æå–å…¶ä»–å®¢æˆ·ç«¯çš„æœªè§ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ï¼ˆPIIï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªä¸¥æ ¼çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æ‰©å±•äº†ä¸€ä¸ªä¸CPISã€GDPRå’ŒCCPAæ ‡å‡†å¯¹é½çš„çœŸå®æ³•å¾‹æ•°æ®é›†ï¼Œè·å¾—äº†89.9%çš„äººå·¥éªŒè¯ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æå–é«˜è¾¾56.57%çš„å—å®³è€…ä¸“å±PIIï¼Œå…¶ä¸­â€œåœ°å€â€ã€â€œç”Ÿæ—¥â€å’Œâ€œå§“åâ€æ˜¯æœ€è„†å¼±çš„ç±»åˆ«ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å¼ºå¤§é˜²å¾¡ç­–ç•¥çš„è¿«åˆ‡éœ€æ±‚ï¼Œå¹¶ä¸ºæœªæ¥éšç§ä¿æŠ¤è”é‚¦å­¦ä¹ çš„ç ”ç©¶è´¡çŒ®äº†æ–°çš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è”é‚¦å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„éšç§æ•°æ®æå–é£é™©ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾æ”»å‡»è€…å¯ä»¥è®¿é—®æ‰€æœ‰è®­ç»ƒæ•°æ®ç‰‡æ®µï¼Œæœªèƒ½è€ƒè™‘æ›´ç°å®çš„æ”»å‡»åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„æ”»å‡»ç®—æ³•å…è®¸æ”»å‡»è€…ä»…è®¿é—®å•ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®ï¼Œé€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡å‰ç¼€æ¥æå–å…¶ä»–å®¢æˆ·ç«¯çš„æœªè§PIIã€‚è¿™ç§æ–¹æ³•æ›´è´´è¿‘å®é™…æ”»å‡»æƒ…å†µï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ä¸Šä¸‹æ–‡å‰ç¼€ç”Ÿæˆã€PIIæå–å’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ”»å‡»è€…æ”¶é›†ç›®æ ‡å®¢æˆ·ç«¯çš„æ•°æ®ï¼Œç„¶åç”Ÿæˆä¸Šä¸‹æ–‡å‰ç¼€ï¼Œæ¥ç€è¿›è¡ŒPIIæå–ï¼Œæœ€åé€šè¿‡è¯„ä¼°æŒ‡æ ‡éªŒè¯æå–æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ¨¡å‹ï¼Œå…è®¸åœ¨ä»…è®¿é—®å•ä¸ªå®¢æˆ·ç«¯æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„PIIæå–ã€‚è¿™ä¸ä¼ ç»Ÿçš„â€œé€å­—â€æå–æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„ç°å®æ„ä¹‰ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸¤ä¸ªè¯„ä¼°æŒ‡æ ‡ï¼šè¦†ç›–ç‡å’Œæ•ˆç‡ï¼Œå¹¶ä½¿ç”¨äº†ä¸æ³•å¾‹æ ‡å‡†å¯¹é½çš„çœŸå®æ•°æ®é›†è¿›è¡ŒéªŒè¯ã€‚å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç²¾ç¡®åº¦ä¸º89.9%çš„PIIæ³¨é‡Šï¼Œç¡®ä¿äº†ç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæå–é«˜è¾¾56.57%çš„å—å®³è€…ä¸“å±PIIï¼Œå°¤å…¶åœ¨â€œåœ°å€â€ã€â€œç”Ÿæ—¥â€å’Œâ€œå§“åâ€ç±»åˆ«ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„è„†å¼±æ€§ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨è”é‚¦å­¦ä¹ ä¸­åŠ å¼ºéšç§ä¿æŠ¤çš„å¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œç¤¾äº¤ç½‘ç»œç­‰æ•æ„Ÿæ•°æ®å¤„ç†åœºæ™¯ã€‚é€šè¿‡æé«˜å¯¹éšç§æ•°æ®æå–é£é™©çš„è®¤è¯†ï¼Œç ”ç©¶æˆæœå¯ä¸ºè®¾è®¡æ›´å®‰å…¨çš„è”é‚¦å­¦ä¹ ç³»ç»Ÿæä¾›ç†è®ºæ”¯æŒï¼Œæ¨åŠ¨éšç§ä¿æŠ¤æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated fine-tuning of large language models (FedLLMs) presents a promising approach for achieving strong model performance while preserving data privacy in sensitive domains. However, the inherent memorization ability of LLMs makes them vulnerable to training data extraction attacks. To investigate this risk, we introduce simple yet effective extraction attack algorithms specifically designed for FedLLMs. In contrast to prior "verbatim" extraction attacks, which assume access to fragments from all training data, our approach operates under a more realistic threat model, where the attacker only has access to a single client's data and aims to extract previously unseen personally identifiable information (PII) from other clients. This requires leveraging contextual prefixes held by the attacker to generalize across clients. To evaluate the effectiveness of our approaches, we propose two rigorous metrics-coverage rate and efficiency-and extend a real-world legal dataset with PII annotations aligned with CPIS, GDPR, and CCPA standards, achieving 89.9% human-verified precision. Experimental results show that our method can extract up to 56.57% of victim-exclusive PII, with "Address," "Birthday," and "Name" being the most vulnerable categories. Our findings underscore the pressing need for robust defense strategies and contribute a new benchmark and evaluation framework for future research in privacy-preserving federated learning.

