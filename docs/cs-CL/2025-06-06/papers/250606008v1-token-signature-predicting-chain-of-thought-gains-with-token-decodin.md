---
layout: default
title: Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models
---

# Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06008" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06008v1</a>
  <a href="https://arxiv.org/pdf/2506.06008.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06008v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06008v1', 'Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peijie Liu, Fengli Xu, Yong Li

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: 20 pages, 6 figures, 13 tables(Accept by ICML2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€CoTæ–¹æ³•ä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é“¾å¼æ€ç»´` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†ä»»åŠ¡` `åŠ¨æ€é€‰æ‹©` `æ ‡è®°æ¦‚ç‡åˆ†å¸ƒ` `é€»è¾‘å›å½’` `æ•ˆç‡æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é“¾å¼æ€ç»´æŠ€æœ¯åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡ä¸ä¸€è‡´ï¼Œä¸”å…¶èƒŒåçš„æœºåˆ¶å°šä¸æ˜ç¡®ã€‚
2. æœ¬æ–‡æå‡ºäº†åŸºäºæ ‡è®°æ¦‚ç‡åˆ†å¸ƒçš„æŒ‡æ ‡ï¼Œç»“åˆé€»è¾‘å›å½’æ¨¡å‹å®ç°åŠ¨æ€é€‰æ‹©CoTæˆ–ç›´æ¥å›ç­”ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯„ä¼°æŒ‡æ ‡çš„å‡†ç¡®ç‡ä¸º89.2%ï¼ŒåŠ¨æ€CoTåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶å‡å°‘äº†35%ä»¥ä¸Šçš„æ ‡è®°æ¶ˆè€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æŠ€æœ¯å·²è¢«è¯æ˜åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¸åŒä»»åŠ¡é—´çš„æ€§èƒ½æå‡ä¸ä¸€è‡´ï¼Œå…¶èƒŒåçš„æœºåˆ¶ä»æ˜¯ä¸€ä¸ªé•¿æœŸç ”ç©¶é—®é¢˜ã€‚æœ¬æ–‡åˆæ­¥è§‚å¯Ÿåˆ°ï¼Œæ ‡è®°æ¦‚ç‡åˆ†å¸ƒçš„å•è°ƒæ€§å¯èƒ½ä¸CoTæ¨ç†çš„æ”¶ç›Šç›¸å…³ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§åŸºäºæ ‡è®°æ¦‚ç‡åˆ†å¸ƒçš„æŒ‡æ ‡æ¥è¯„ä¼°CoTåœ¨ä¸åŒä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ç»“åˆå®ä¾‹çº§æŒ‡æ ‡ä¸é€»è¾‘å›å½’æ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€CoTæ–¹æ³•ï¼ŒåŠ¨æ€é€‰æ‹©CoTæˆ–ç›´æ¥å›ç­”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†åŠ¨æ€CoTæ‰©å±•åˆ°é—­æºæ¨¡å‹ï¼Œé€šè¿‡è½¬ç§»ä»å¼€æºæ¨¡å‹å­¦ä¹ çš„å†³ç­–ç­–ç•¥ã€‚æˆ‘ä»¬çš„è¯„ä¼°æŒ‡æ ‡å‡†ç¡®ç‡è¾¾åˆ°89.2%ï¼ŒåŠ¨æ€CoTåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶å‡å°‘äº†35%ä»¥ä¸Šçš„æ ‡è®°æ¶ˆè€—ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºCoTæ¨ç†çš„æœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œå¹¶ä¸ºå…¶æ›´é«˜æ•ˆçš„éƒ¨ç½²æä¾›äº†æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸­æ€§èƒ½æå‡ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè§£é‡Šå…¶æœºåˆ¶å’Œåº”ç”¨æ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†åŸºäºæ ‡è®°æ¦‚ç‡åˆ†å¸ƒçš„ä¸¤ç§æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°CoTçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼•å…¥åŠ¨æ€CoTæ–¹æ³•ï¼Œæ ¹æ®ä»»åŠ¡åŠ¨æ€é€‰æ‹©ä½¿ç”¨CoTæˆ–ç›´æ¥å›ç­”ï¼Œä»¥æé«˜æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯æ ‡è®°æ¦‚ç‡åˆ†å¸ƒçš„åˆ†æä¸æŒ‡æ ‡è®¡ç®—ï¼ŒäºŒæ˜¯åŠ¨æ€CoTçš„å†³ç­–æœºåˆ¶ï¼Œåè€…ç»“åˆé€»è¾‘å›å½’æ¨¡å‹è¿›è¡Œé€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†æ–°çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŠ¨æ€é€‰æ‹©æœºåˆ¶ï¼Œä½¿å¾—CoTçš„åº”ç”¨æ›´åŠ çµæ´»å’Œé«˜æ•ˆï¼Œæ˜¾è‘—å‡å°‘äº†æ ‡è®°æ¶ˆè€—ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨é€»è¾‘å›å½’æ¨¡å‹ç»“åˆå®ä¾‹çº§æŒ‡æ ‡è¿›è¡Œå†³ç­–ï¼Œç¡®ä¿åŠ¨æ€é€‰æ‹©çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒé«˜å‡†ç¡®ç‡ã€‚å®éªŒä¸­è¯„ä¼°æŒ‡æ ‡çš„å‡†ç¡®ç‡è¾¾åˆ°89.2%ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„è¯„ä¼°æŒ‡æ ‡å‡†ç¡®ç‡è¾¾åˆ°89.2%ï¼Œè€ŒåŠ¨æ€CoTæ–¹æ³•åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼ŒæˆåŠŸå‡å°‘äº†è¶…è¿‡35%çš„æ ‡è®°æ¶ˆè€—ã€‚è¿™ä¸€æˆæœè¡¨æ˜ï¼ŒåŠ¨æ€é€‰æ‹©æœºåˆ¶åœ¨æå‡æ¨ç†æ•ˆç‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤æ‚æ¨ç†ä»»åŠ¡ï¼Œå¦‚é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç”Ÿæˆå’Œæ–‡æœ¬ç†è§£ç­‰ã€‚é€šè¿‡æé«˜CoTçš„åº”ç”¨æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å®é™…åœºæ™¯ä¸­æ›´å¥½åœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½åº”ç”¨çš„å¼€å‘ä¸ä¼˜åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-Thought (CoT) technique has proven effective in improving the performance of large language models (LLMs) on complex reasoning tasks. However, the performance gains are inconsistent across different tasks, and the underlying mechanism remains a long-standing research question. In this work, we make a preliminary observation that the monotonicity of token probability distributions may be correlated with the gains achieved through CoT reasoning. Leveraging this insight, we propose two indicators based on the token probability distribution to assess CoT effectiveness across different tasks. By combining instance-level indicators with logistic regression model, we introduce Dynamic CoT, a method that dynamically select between CoT and direct answer. Furthermore, we extend Dynamic CoT to closed-source models by transferring decision strategies learned from open-source models. Our indicators for assessing CoT effectiveness achieve an accuracy of 89.2\%, and Dynamic CoT reduces token consumption by more than 35\% while maintaining high accuracy. Overall, our work offers a novel perspective on the underlying mechanisms of CoT reasoning and provides a framework for its more efficient deployment.

