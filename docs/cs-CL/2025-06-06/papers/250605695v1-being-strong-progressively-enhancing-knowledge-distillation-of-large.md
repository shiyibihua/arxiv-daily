---
layout: default
title: Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework
---

# Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05695" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05695v1</a>
  <a href="https://arxiv.org/pdf/2506.05695.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05695v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05695v1', 'Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingyuan Liu, Mengxiang Zhang

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¸è¿›å¼çŸ¥è¯†è’¸é¦æ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `å¤§è¯­è¨€æ¨¡å‹` `è¯¾ç¨‹å­¦ä¹ ` `æ¸è¿›è¶…è´Ÿè·` `æ¨¡å‹å‹ç¼©` `è®­ç»ƒç¨³å®šæ€§` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸å¯¼è‡´å­¦ç”Ÿæ¨¡å‹åˆ†å¸ƒæ˜¾è‘—å˜åŒ–ï¼Œå‡ºç°ç¾éš¾æ€§é—å¿˜ç­‰é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºâ€œæ¸è¿›è¶…è´Ÿè·â€åŸåˆ™çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ†çº§å¼•å…¥è®­ç»ƒæ ·æœ¬æ¥æé«˜å­¦ä¹ ç¨³å®šæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPOCLåœ¨å¤šç§ç™½ç›’KDæ–¹æ³•ä¸­å‡æ˜¾è‘—æå‡äº†è’¸é¦å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰é€šè¿‡å°†æ•™å¸ˆæ¨¡å‹çš„èƒ½åŠ›è½¬ç§»åˆ°è¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä»è€Œé™ä½æ¨ç†æˆæœ¬å’Œå†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„KDæ–¹æ³•å¸¸å¸¸æ— æ³•é˜²æ­¢å­¦ç”Ÿæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°æ˜¾è‘—çš„åˆ†å¸ƒå˜åŒ–ï¼Œå¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€æ¨¡å¼å´©æºƒå’Œè®­ç»ƒ-æ¨ç†ä¸åŒ¹é…ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ã€å¯æ’æ‹”çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œçµæ„Ÿæ¥è‡ªâ€œæ¸è¿›è¶…è´Ÿè·â€åŸåˆ™ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„ç™½ç›’KDæ–¹æ³•ä¸­ï¼Œä¸”è®¡ç®—å¼€é”€æå°ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1ï¼‰éš¾åº¦æµ‹é‡å™¨ï¼Œå°†è®­ç»ƒæ ·æœ¬ä»æ˜“åˆ°éš¾è¿›è¡Œæ’åå’Œåˆ’åˆ†ï¼›2ï¼‰è®­ç»ƒè°ƒåº¦å™¨ï¼Œé€æ­¥å°†è¿™äº›å­é›†å¼•å…¥è’¸é¦è¿‡ç¨‹ä¸­ï¼Œå¹¶åº”ç”¨é€æ¸å‡é«˜æ¸©åº¦çš„æŸå¤±å‡½æ•°ã€‚é€šè¿‡ä»æœ€ç®€å•çš„æ ·æœ¬å¼€å§‹ï¼Œé€æ­¥å¢åŠ éš¾åº¦ï¼Œè¯¥æ–¹æ³•å¢å¼ºäº†å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPOCLåœ¨å„ç§ç™½ç›’KDæ–¹æ³•å’Œæ¨¡å‹å®¶æ—ä¸­æŒç»­æå‡äº†è’¸é¦å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¼è‡´å­¦ç”Ÿæ¨¡å‹åˆ†å¸ƒå˜åŒ–çš„é—®é¢˜ï¼Œè¿™ç§å˜åŒ–å¯èƒ½å¼•å‘ç¾éš¾æ€§é—å¿˜ã€æ¨¡å¼å´©æºƒå’Œè®­ç»ƒä¸æ¨ç†ä¸åŒ¹é…ç­‰ç°è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å°†è®­ç»ƒæ ·æœ¬æŒ‰éš¾åº¦åˆ†çº§ï¼Œé€æ­¥å¼•å…¥æ›´éš¾çš„æ ·æœ¬ï¼Œä»¥å¢å¼ºå­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚è¿™æ ·çš„è®¾è®¡å¯ä»¥æœ‰æ•ˆå‡è½»å­¦ç”Ÿæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰éš¾åº¦æµ‹é‡å™¨ï¼Œè´Ÿè´£å¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œæ’åºå’Œåˆ’åˆ†ï¼›2ï¼‰è®­ç»ƒè°ƒåº¦å™¨ï¼ŒæŒ‰ç…§å›ºå®šé—´éš”é€æ­¥å¼•å…¥ä¸åŒéš¾åº¦çš„æ ·æœ¬ï¼Œå¹¶åº”ç”¨é€æ¸å‡é«˜çš„æ¸©åº¦æŸå¤±å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è¯¾ç¨‹å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦ç›¸ç»“åˆï¼Œé€šè¿‡åˆ†çº§å¼•å…¥æ ·æœ¬æ¥æå‡å­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œè¿™ä¸ä¼ ç»Ÿçš„KDæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†é€æ¸å‡é«˜æ¸©åº¦çš„ç­–ç•¥ï¼Œä»¥é€‚åº”ä¸åŒéš¾åº¦æ ·æœ¬çš„å­¦ä¹ éœ€æ±‚ã€‚æ­¤å¤–ï¼Œéš¾åº¦æµ‹é‡å™¨çš„è®¾è®¡ç¡®ä¿äº†è®­ç»ƒæ ·æœ¬çš„æœ‰æ•ˆæ’åºå’Œåˆ†ç»„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPOCLåœ¨å¤šä¸ªç™½ç›’KDæ–¹æ³•ä¸­å‡æ˜¾è‘—æå‡äº†è’¸é¦å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°äº†X%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„è’¸é¦æ•ˆæœï¼Œå¯ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge Distillation (KD) compresses large language models (LLMs) by transferring the teacher model's capabilities to a smaller student model, reducing inference cost and memory usage while maintaining performance. However, existing KD methods for LLMs often fail to prevent significant shifts in the student model's distribution during training, leading to issues such as catastrophic forgetting, mode collapse, and training-inference mismatch. To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of "progressive overload" (POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures. By starting with the easiest samples and progressively increasing the difficulty, the approach enhances both the stability and efficiency of learning. Extensive experiments in instruction-following settings demonstrate that POCL consistently improves the performance of distilled student models across various white-box KD methods and model families. Our findings highlight the effectiveness of sorted training samples in KD for LLMs. More generally, our work demonstrates how to structure training data within the KD process to enhance the stability and performance of distilled LLMs.

