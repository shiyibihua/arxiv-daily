---
layout: default
title: Let's CONFER: A Dataset for Evaluating Natural Language Inference Models on CONditional InFERence and Presupposition
---

# Let's CONFER: A Dataset for Evaluating Natural Language Inference Models on CONditional InFERence and Presupposition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06133" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06133v1</a>
  <a href="https://arxiv.org/pdf/2506.06133.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06133v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06133v1', 'Let\'s CONFER: A Dataset for Evaluating Natural Language Inference Models on CONditional InFERence and Presupposition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tara Azin, Daniel Dumitrescu, Diana Inkpen, Raj Singh

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: This paper is published in the Proceedings of the 38th Canadian Conference on Artificial Intelligence (CAIAC 2025). Please cite the conference version at https://caiac.pubpub.org/pub/keh8ij01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCONFERæ•°æ®é›†ä»¥è¯„ä¼°NLIæ¨¡å‹åœ¨æ¡ä»¶æ¨ç†ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªç„¶è¯­è¨€æ¨ç†` `æ¡ä»¶æ¨ç†` `é¢„è®¾æ¨ç†` `æ•°æ®é›†æ„å»º` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„NLIæ¨¡å‹åœ¨å¤„ç†æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ¨ç†æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç»†ç²’åº¦çš„è¯­ç”¨æ¨ç†æ–¹é¢ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†CONFERæ•°æ®é›†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°NLIæ¨¡å‹åœ¨æ¡ä»¶å¥æ¨ç†ä¸­çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNLIæ¨¡å‹åœ¨æ¡ä»¶å¥çš„é¢„è®¾æ¨ç†ä¸Šå­˜åœ¨æ˜¾è‘—å›°éš¾ï¼Œå¾®è°ƒç°æœ‰æ•°æ®é›†æœªèƒ½æœ‰æ•ˆæå‡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰ä»»åŠ¡æ—¨åœ¨åˆ¤æ–­å¥å­å¯¹ä¹‹é—´çš„è•´å«ã€çŸ›ç›¾æˆ–ä¸­ç«‹å…³ç³»ã€‚å°½ç®¡ç°æœ‰NLIæ¨¡å‹åœ¨è®¸å¤šæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†ç»†ç²’åº¦çš„è¯­ç”¨æ¨ç†ï¼Œç‰¹åˆ«æ˜¯æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ–¹é¢ä»ç„¶ä¸è¶³ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†CONFERï¼Œä¸€ä¸ªæ–°é¢–çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°NLIæ¨¡å‹å¦‚ä½•å¤„ç†æ¡ä»¶å¥ä¸­çš„æ¨ç†ã€‚æˆ‘ä»¬è¯„ä¼°äº†å››ä¸ªNLIæ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥æ£€éªŒå®ƒä»¬åœ¨æ¡ä»¶æ¨ç†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨é›¶-shotå’Œfew-shotæç¤ºè®¾ç½®ä¸‹è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¦‚GPT-4oã€LLaMAã€Gemmaå’ŒDeepSeek-R1ï¼Œåˆ†æå®ƒä»¬åœ¨æœ‰æ— ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹æ¨æ–­é¢„è®¾çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒNLIæ¨¡å‹åœ¨æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ¨ç†ä¸Šå­˜åœ¨å›°éš¾ï¼Œä¸”åœ¨ç°æœ‰NLIæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå¹¶ä¸ä¸€å®šèƒ½æé«˜å…¶æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰NLIæ¨¡å‹åœ¨æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç»†ç²’åº¦çš„è¯­ç”¨æ¨ç†æ—¶ï¼Œå°¤å…¶æ˜¯æ¡ä»¶å¥ä¸­çš„é¢„è®¾ï¼Œè¡¨ç°ä¸ä½³ï¼Œç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ ‡å‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†CONFERæ•°æ®é›†ï¼Œä¸“æ³¨äºæ¡ä»¶å¥çš„æ¨ç†è¯„ä¼°ï¼Œæ—¨åœ¨é€šè¿‡è¿™ä¸€æ–°æ•°æ®é›†æ¥æµ‹è¯•å’Œæå‡NLIæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œç ”ç©¶å…¶åœ¨æ¡ä»¶æ¨ç†ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹é€‰æ‹©ä¸è¯„ä¼°ã€‚é¦–å…ˆæ„å»ºCONFERæ•°æ®é›†ï¼Œç„¶åé€‰æ‹©å››ä¸ªNLIæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œæœ€ååˆ†æå¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶-shotå’Œfew-shotè®¾ç½®ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šCONFERæ•°æ®é›†æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ¨ç†è¿›è¡Œè¯„ä¼°çš„æ•°æ®é›†ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚ä¸ä¼ ç»ŸNLIæ•°æ®é›†ç›¸æ¯”ï¼ŒCONFERæ›´å…³æ³¨ç»†ç²’åº¦çš„è¯­ç”¨æ¨ç†ï¼Œæä¾›äº†æ–°çš„è¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è¯„ä¼°ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨ä¸åŒçš„æç¤ºè®¾ç½®ä¸‹è¿›è¡Œæµ‹è¯•ã€‚å…³é”®å‚æ•°åŒ…æ‹¬æ¨¡å‹çš„é¢„è®­ç»ƒçŠ¶æ€ã€æç¤ºæ–¹å¼ä»¥åŠè¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ï¼Œç¡®ä¿äº†å®éªŒçš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒNLIæ¨¡å‹åœ¨æ¡ä»¶å¥çš„é¢„è®¾æ¨ç†ä¸Šå­˜åœ¨æ˜¾è‘—å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ã€‚å¾®è°ƒç°æœ‰NLIæ•°æ®é›†æœªèƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºè¯¥é¢†åŸŸçš„ç ”ç©¶ä»éœ€æ·±å…¥æ¢ç´¢ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ¨ç†ç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿä»¥åŠæ™ºèƒ½é—®ç­”ç­‰ã€‚CONFERæ•°æ®é›†çš„å¼•å…¥ä¸ºè¯„ä¼°å’Œæ”¹è¿›NLIæ¨¡å‹åœ¨å¤æ‚è¯­å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„å·¥å…·ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„è¯­è¨€ç†è§£ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Natural Language Inference (NLI) is the task of determining whether a sentence pair represents entailment, contradiction, or a neutral relationship. While NLI models perform well on many inference tasks, their ability to handle fine-grained pragmatic inferences, particularly presupposition in conditionals, remains underexplored. In this study, we introduce CONFER, a novel dataset designed to evaluate how NLI models process inference in conditional sentences. We assess the performance of four NLI models, including two pre-trained models, to examine their generalization to conditional reasoning. Additionally, we evaluate Large Language Models (LLMs), including GPT-4o, LLaMA, Gemma, and DeepSeek-R1, in zero-shot and few-shot prompting settings to analyze their ability to infer presuppositions with and without prior context. Our findings indicate that NLI models struggle with presuppositional reasoning in conditionals, and fine-tuning on existing NLI datasets does not necessarily improve their performance.

