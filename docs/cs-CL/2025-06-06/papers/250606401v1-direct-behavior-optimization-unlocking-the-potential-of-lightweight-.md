---
layout: default
title: Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs
---

# Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06401" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.06401v1</a>
  <a href="https://arxiv.org/pdf/2506.06401.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06401v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06401v1', 'Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hongming Yang, Shi Lin, Jun Shao, Changting Lin, Donghai Zhu, Meng Han, Qinglei Kong

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-06

**Â§áÊ≥®**: This work is accepted at ACL 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DeBoP‰ª•‰ºòÂåñËΩªÈáèÁ∫ßÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑË°å‰∏∫**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËΩªÈáèÁ∫ßÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Ë°å‰∏∫‰ºòÂåñ` `Ëá™Âä®Âåñ‰ºòÂåñ` `ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `Êé®ÁêÜËÉΩÂäõ` `ËÆ°ÁÆóÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊèêÁ§∫‰ºòÂåñÊñπÊ≥ïÂú®ËΩªÈáèÁ∫ßÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏äÊïàÊûú‰∏ç‰Ω≥Ôºå‰∏ªË¶Å‰æùËµñ‰∫∫Â∑•Âä™ÂäõÊàñÂ§çÊùÇÁöÑÂÖÉËÆ§Áü•ËÉΩÂäõ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑDeBoPÊñπÊ≥ïÈÄöËøáÊó†Ê¢ØÂ∫¶ÁöÑËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºåËá™Âä®‰ºòÂåñLwLLMsÁöÑË°å‰∏∫ÔºåÁÆÄÂåñ‰∫ÜÂ§çÊùÇÊèêÁ§∫ÁöÑ‰ºòÂåñËøáÁ®ã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDeBoP‰ºòÂåñÁöÑLwLLMsÂú®Â§ö‰∏™‰ªªÂä°‰∏äË∂ÖË∂ä‰∫ÜGPT-3.5ÔºåÂπ∂ÊòæËëóÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÊó∂Èó¥ÔºåÊèêÂçá‰∫ÜÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËΩªÈáèÁ∫ßÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLwLLMsÔºâÊòØ‰∏∫Âú®Ê∂àË¥πÁ∫ßÁ°¨‰ª∂‰∏äÈ´òÊïàËøêË°åËÄåËÆæËÆ°ÁöÑ‰ºòÂåñÊ®°ÂûãÔºåÂÖ∑ÊúâËµÑÊ∫êÊïàÁéáÈ´ò„ÄÅÊàêÊú¨‰ΩéÂíåÊï∞ÊçÆÈöêÁßÅ‰øùÊä§Á≠âÊòæËëó‰ºòÂäø„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÂú®Êé®ÁêÜÂíåÊé®Êñ≠ËÉΩÂäõ‰∏äÂ∏∏Â∏∏ÂèóÈôêÔºåÂΩ±ÂìçÂÖ∂Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÁé∞ÊúâÁöÑÊèêÁ§∫‰ºòÂåñÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂ§ßÈáèÁöÑ‰∫∫Â∑•Âä™ÂäõÊàñÂÖàËøõÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖÉËÆ§Áü•ËÉΩÂäõÔºåËøô‰ΩøÂæóÂÆÉ‰ª¨Âú®LwLLMs‰∏äÊïàÊûú‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁõ¥Êé•Ë°å‰∏∫‰ºòÂåñËåÉÂºèDeBoPÔºåËØ•ÊñπÊ≥ïÊ∫êËá™ÈìæÂºèÊÄùÁª¥ÊèêÁ§∫ÊäÄÊúØ„ÄÇDeBoPÊòØ‰∏ÄÁßçËá™Âä®‰ºòÂåñÊñπÊ≥ïÔºå‰∏ìÊ≥®‰∫éÁõ¥Êé•‰ºòÂåñLwLLMsÁöÑË°å‰∏∫ÔºåÈÄöËøáÊó†Ê¢ØÂ∫¶ÁöÑËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢Â∞ÜÂ§çÊùÇÊèêÁ§∫ÁöÑ‰ºòÂåñËΩ¨Âåñ‰∏∫Á¶ªÊï£„ÄÅÂèØÈáèÂåñÁöÑÊâßË°åÂ∫èÂàó‰ºòÂåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDeBoPÂú®‰∏É‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éËøëÊúüÁöÑÊèêÁ§∫‰ºòÂåñÊñπÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØDeBoP‰ºòÂåñÁöÑLwLLMsÂú®Â§ßÂ§öÊï∞‰ªªÂä°‰∏äË∂ÖË∂ä‰∫ÜGPT-3.5ÔºåÂêåÊó∂ËÆ°ÁÆóÊó∂Èó¥ÂáèÂ∞ëÁ∫¶60%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ËΩªÈáèÁ∫ßÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏≠Êé®ÁêÜÂíåÊé®Êñ≠ËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÊèêÁ§∫‰ºòÂåñÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáè‰∫∫Â∑•Âπ≤È¢ÑÔºå‰∏îÊïàÊûúÊúâÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDeBoPÊñπÊ≥ïÈÄöËøáËá™Âä®ÂåñÁöÑÊñπÂºè‰ºòÂåñLwLLMsÁöÑË°å‰∏∫ÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑ‰∫∫Â∑•Âπ≤È¢ÑÔºåÂà©Áî®Êó†Ê¢ØÂ∫¶ÁöÑËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢Êù•‰ºòÂåñÊâßË°åÂ∫èÂàó„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDeBoPÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ËæìÂÖ•ÊèêÁ§∫ÁöÑËΩ¨Êç¢„ÄÅÊâßË°åÂ∫èÂàóÁöÑÁîüÊàêÂíå‰ºòÂåñ‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÂ∞ÜÂ§çÊùÇÊèêÁ§∫ËΩ¨Âåñ‰∏∫ÂèØÈáèÂåñÁöÑÊâßË°åÂ∫èÂàóÔºåÁÑ∂ÂêéÈÄöËøáËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ËøõË°å‰ºòÂåñÔºåÊúÄÂêéËæìÂá∫‰ºòÂåñÂêéÁöÑÊ®°ÂûãË°å‰∏∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDeBoPÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂Ëá™Âä®ÂåñÁöÑ‰ºòÂåñËøáÁ®ãÔºåÁõ¥Êé•ÈíàÂØπÊ®°ÂûãË°å‰∏∫ËøõË°å‰ºòÂåñÔºåËÄå‰∏çÊòØ‰æùËµñ‰∫é‰∫∫Â∑•ËÆæËÆ°ÁöÑÊèêÁ§∫„ÄÇËøô‰∏ÄÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜËΩªÈáèÁ∫ßÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåDeBoPÈááÁî®‰∫ÜÊó†Ê¢ØÂ∫¶‰ºòÂåñÁ≠ñÁï•ÔºåÁªìÂêà‰∫ÜËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÁöÑÈöèÊú∫ÊÄßÂíåÊé¢Á¥¢ÊÄßÔºå‰ª•Á°Æ‰øù‰ºòÂåñËøáÁ®ãÁöÑÈ´òÊïàÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDeBoP‰ºòÂåñÁöÑËΩªÈáèÁ∫ßÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ßÂ§öÊï∞‰ªªÂä°‰∏äË∂ÖË∂ä‰∫ÜGPT-3.5Ôºå‰∏îËÆ°ÁÆóÊó∂Èó¥ÂáèÂ∞ëÁ∫¶60%„ÄÇËøô‰∏ÄÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáË°®ÊòéDeBoPÂú®‰ºòÂåñÊ®°ÂûãË°å‰∏∫ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂä©Êâã„ÄÅÊïôËÇ≤Â∑•ÂÖ∑Âíå‰ΩéËµÑÊ∫êÁéØÂ¢É‰∏ãÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°„ÄÇÈÄöËøá‰ºòÂåñËΩªÈáèÁ∫ßÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑË°å‰∏∫ÔºåDeBoPÂèØ‰ª•Âú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÂÆûÁé∞Êõ¥È´òÊïàÁöÑÊé®ÁêÜÂíåÂìçÂ∫îÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized models designed to run efficiently on consumer-grade hardware, offering significant advantages in resource efficiency, cost-effectiveness, and data privacy. However, these models often struggle with limited inference and reasoning capabilities, which restrict their performance on complex tasks and limit their practical applicability. Moreover, existing prompt optimization methods typically rely on extensive manual effort or the meta-cognitive abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To address these challenges, we introduce DeBoP, a new Direct Behavior Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting technique. Unlike CoT Prompting, DeBoP is an automatic optimization method, which focuses on the optimization directly on the behavior of LwLLMs. In particular, DeBoP transforms the optimization of complex prompts into the optimization of discrete, quantifiable execution sequences using a gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging tasks where state-of-the-art LLMs excel but LwLLMs generally underperform. Experimental results demonstrate that DeBoP significantly outperforms recent prompt optimization methods on most tasks. In particular, DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60% compared to other automatic prompt optimization methods.

