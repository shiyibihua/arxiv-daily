---
layout: default
title: Large Language Models are Demonstration Pre-Selectors for Themselves
---

# Large Language Models are Demonstration Pre-Selectors for Themselves

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06033" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06033v1</a>
  <a href="https://arxiv.org/pdf/2506.06033.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06033v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06033v1', 'Large Language Models are Demonstration Pre-Selectors for Themselves')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiarui Jin, Yuwei Wu, Haoxuan Li, Xiaoting He, Weinan Zhang, Yiming Yang, Yong Yu, Jun Wang, Mengyue Yang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFEEDERæ¡†æ¶ä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„ç¤ºä¾‹é€‰æ‹©æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `ç¤ºä¾‹é€‰æ‹©` `é¢„é€‰æ‹©æ¡†æ¶` `è®­ç»ƒæ•ˆç‡` `åŒå±‚ä¼˜åŒ–` `æ ‘å½¢ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ICLæ–¹æ³•åœ¨é€‰æ‹©ç¤ºä¾‹æ—¶è®¡ç®—æˆæœ¬é«˜ï¼Œéœ€é‡å¤ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­æ£€ç´¢ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºFEEDERæ¡†æ¶ï¼Œé€šè¿‡é¢„é€‰æ‹©ä»£è¡¨æ€§ç¤ºä¾‹å­é›†ï¼Œé™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFEEDERèƒ½å°†è®­ç»ƒæ•°æ®å¤§å°å‡å°‘è¶…è¿‡20%ï¼ŒåŒæ—¶ä¿æŒä¸å®Œæ•´æ•°æ®é›†ç›¸å½“çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡ä»æ•´ä¸ªè®­ç»ƒæ•°æ®ä¸­é€‰æ‹©å°‘é‡ç¤ºä¾‹æ¥å®ç°å¼ºå¤§çš„å°‘æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ICLæ–¹æ³•ä¾èµ–äºç›¸ä¼¼æ€§æˆ–å¤šæ ·æ€§è¯„åˆ†æ¥é€‰æ‹©ç¤ºä¾‹ï¼Œè¿™å¯¼è‡´åœ¨æ¯ä¸ªæŸ¥è¯¢ä¸­éƒ½éœ€ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­é‡å¤æ£€ç´¢ï¼Œé€ æˆé«˜è®¡ç®—æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FEEDERï¼ˆå°‘é‡ä½†å¿…è¦çš„ç¤ºä¾‹é¢„é€‰æ‹©å™¨ï¼‰ï¼Œä¸€ä¸ªæ–°é¢–çš„é¢„é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨è¯†åˆ«åŒ…å«è®­ç»ƒæ•°æ®ä¸­æœ€å…·ä»£è¡¨æ€§çš„ç¤ºä¾‹çš„å­é›†ã€‚é€šè¿‡å¼•å…¥â€œå……åˆ†æ€§â€å’Œâ€œå¿…è¦æ€§â€æŒ‡æ ‡ï¼Œå¹¶è®¾è®¡åŸºäºæ ‘çš„ç®—æ³•æ¥é«˜æ•ˆè¯†åˆ«ä»£è¡¨æ€§ç¤ºä¾‹ï¼ŒFEEDERèƒ½å¤Ÿæœ‰æ•ˆæ›¿ä»£å®Œæ•´è®­ç»ƒæ•°æ®ï¼Œæé«˜æ•ˆç‡ï¼ŒåŒæ—¶åœ¨ICLä¸­ä¿æŒå¯æ¯”æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥é¢„é€‰æ‹©å­é›†è¿˜å¯ç”¨äºå¾®è°ƒLLMsï¼Œé‡‡ç”¨åŒå±‚ä¼˜åŒ–æ–¹æ³•æå‡è®­ç»ƒæ•ˆç‡è€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFEEDERèƒ½å¤Ÿå°†è®­ç»ƒæ•°æ®å¤§å°å‡å°‘è¶…è¿‡20%ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ï¼Œå¹¶ä¸å¤šç§ä¸‹æ¸¸ç¤ºä¾‹é€‰æ‹©ç­–ç•¥æ— ç¼é›†æˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ICLæ–¹æ³•åœ¨ç¤ºä¾‹é€‰æ‹©ä¸­è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºç›¸ä¼¼æ€§æˆ–å¤šæ ·æ€§è¯„åˆ†ï¼Œå¯¼è‡´åœ¨æ¯æ¬¡æŸ¥è¯¢æ—¶éƒ½éœ€é‡å¤æ£€ç´¢å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFEEDERæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é¢„é€‰æ‹©ä¸€ä¸ªåŒ…å«æœ€å…·ä»£è¡¨æ€§ç¤ºä¾‹çš„å­é›†ï¼Œæ¥æ›¿ä»£å®Œæ•´çš„è®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œæé«˜ç¤ºä¾‹é€‰æ‹©çš„æ•ˆç‡ã€‚é€šè¿‡å¼•å…¥â€œå……åˆ†æ€§â€å’Œâ€œå¿…è¦æ€§â€æŒ‡æ ‡ï¼ŒFEEDERèƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«å‡ºæœ€å…·ä»£è¡¨æ€§çš„ç¤ºä¾‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFEEDERçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é¢„é€‰æ‹©é˜¶æ®µå’Œåç»­çš„ç¤ºä¾‹é€‰æ‹©é˜¶æ®µã€‚é¢„é€‰æ‹©é˜¶æ®µä½¿ç”¨æ ‘å½¢ç®—æ³•æ¥è¯†åˆ«ä»£è¡¨æ€§ç¤ºä¾‹ï¼Œè€Œåç»­é˜¶æ®µåˆ™åˆ©ç”¨è¿™äº›ç¤ºä¾‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šFEEDERçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†â€œå……åˆ†æ€§â€å’Œâ€œå¿…è¦æ€§â€æŒ‡æ ‡ï¼Œå¹¶è®¾è®¡äº†é«˜æ•ˆçš„æ ‘å½¢ç®—æ³•æ¥è¿›è¡Œç¤ºä¾‹çš„é¢„é€‰æ‹©ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰ä¾èµ–ç›¸ä¼¼æ€§è¯„åˆ†çš„é€‰æ‹©æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒFEEDERé‡‡ç”¨äº†åŒå±‚ä¼˜åŒ–æ–¹æ³•ä»¥æå‡è®­ç»ƒæ•ˆç‡ï¼Œç¡®ä¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¸ç‰ºç‰²æ€§èƒ½ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†è¯´æ˜ï¼Œå¯èƒ½ä¸ºæœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFEEDERèƒ½å¤Ÿå°†è®­ç»ƒæ•°æ®å¤§å°å‡å°‘è¶…è¿‡20%ï¼ŒåŒæ—¶åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„LLMsï¼ˆä»300Måˆ°8Bå‚æ•°ï¼‰ä¸Šä¿æŒæ€§èƒ½ï¼Œå±•ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§ã€‚æ­¤å¤–ï¼ŒFEEDERä¸å¤šç§ä¸‹æ¸¸ç¤ºä¾‹é€‰æ‹©ç­–ç•¥çš„æ— ç¼é›†æˆï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å…¶å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FEEDERæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦é«˜æ•ˆç¤ºä¾‹é€‰æ‹©çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜ç¤ºä¾‹é€‰æ‹©çš„æ•ˆç‡ï¼ŒFEEDERèƒ½å¤ŸåŠ é€Ÿæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context learning (ICL) with large language models (LLMs) delivers strong few-shot performance by choosing few-shot demonstrations from the entire training data. However, existing ICL methods, which rely on similarity or diversity scores to choose demonstrations, incur high computational costs due to repeatedly retrieval from large-scale datasets for each query. To this end, we propose FEEDER (FEw yet Essential Demonstration prE-selectoR), a novel pre-selection framework that identifies a representative subset of demonstrations containing the most representative examples in the training data, tailored to specific LLMs. To construct this subset, we introduce the "sufficiency" and "necessity" metrics in the pre-selection stage and design a tree-based algorithm to identify representative examples efficiently. Once pre-selected, this representative subset can effectively replace the full training data, improving efficiency while maintaining comparable performance in ICL. Additionally, our pre-selected subset also benefits fine-tuning LLMs, where we introduce a bi-level optimization method that enhances training efficiency without sacrificing performance. Experiments with LLMs ranging from 300M to 8B parameters show that FEEDER can reduce training data size by over 20% while maintaining performance and seamlessly integrating with various downstream demonstration selection strategies in ICL.

