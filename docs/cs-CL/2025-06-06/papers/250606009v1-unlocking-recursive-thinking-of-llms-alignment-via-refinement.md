---
layout: default
title: Unlocking Recursive Thinking of LLMs: Alignment via Refinement
---

# Unlocking Recursive Thinking of LLMs: Alignment via Refinement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06009" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06009v1</a>
  <a href="https://arxiv.org/pdf/2506.06009.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06009v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06009v1', 'Unlocking Recursive Thinking of LLMs: Alignment via Refinement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoke Zhang, Xiaobo Liang, Cunxiang Wang, Juntao Li, Min Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: Accepted to the Findings of ACL 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Banner-Z/AvR.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAvRæ–¹æ³•ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„é€’å½’æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é€’å½’æ¨ç†` `é•¿å½¢å¼æ€ç»´é“¾` `å¯å¾®å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–` `åˆæˆæ ·æœ¬`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é€’å½’æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™ï¼Œå°¤å…¶ç¼ºä¹é«˜è´¨é‡çš„ä¸“å®¶æ•°æ®è¿›è¡Œæœ‰æ•ˆè’¸é¦ã€‚
2. æœ¬æ–‡æå‡ºAvRæ–¹æ³•ï¼Œé€šè¿‡ç²¾ç‚¼è¿‡ç¨‹ç»“åˆæ‰¹è¯„ä¸æ”¹è¿›ï¼Œåˆ©ç”¨å¯å¾®å­¦ä¹ ä¼˜åŒ–å¥–åŠ±ï¼Œæå‡LLMsçš„é€’å½’æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAvRåœ¨ä½¿ç”¨3000ä¸ªåˆæˆæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œèƒœç‡æé«˜è¶…è¿‡20%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

OpenAIçš„o1ç³»åˆ—æ¨¡å‹è¡¨æ˜ï¼Œåˆ©ç”¨é•¿å½¢å¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é€’å½’æ€ç»´èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹ä¸“å®¶ç­–åˆ’çš„æ•°æ®è¿›è¡Œè’¸é¦çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•AvRï¼ˆAlignment via Refinementï¼‰ï¼Œæ—¨åœ¨é€šè¿‡é•¿å½¢å¼çš„CoTè§£é”LLMsçš„é€’å½’æ¨ç†æ½œåŠ›ã€‚AvRå¼•å…¥äº†ä¸€ç§æ•´åˆæ‰¹è¯„å’Œæ”¹è¿›è¡ŒåŠ¨çš„ç²¾ç‚¼è¿‡ç¨‹ï¼Œé€šè¿‡å¯å¾®å­¦ä¹ æŠ€æœ¯ä¼˜åŒ–ç²¾ç‚¼æ„ŸçŸ¥å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAvRæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œä»…ä½¿ç”¨3000ä¸ªåˆæˆæ ·æœ¬ï¼Œå°±èƒ½ä½¿LLaMA-3-8B-Instructæ¨¡å‹åœ¨AlpacaEval 2.0ä¸Šçš„èƒœç‡æå‡è¶…è¿‡20%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨é€’å½’æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹ä¸“å®¶ç­–åˆ’æ•°æ®çš„æƒ…å†µä¸‹ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAvRæ–¹æ³•é€šè¿‡å¼•å…¥ç²¾ç‚¼è¿‡ç¨‹ï¼Œç»“åˆæ‰¹è¯„å’Œæ”¹è¿›è¡ŒåŠ¨ï¼Œåˆ©ç”¨å¯å¾®å­¦ä¹ æŠ€æœ¯æ¥ä¼˜åŒ–ç²¾ç‚¼æ„ŸçŸ¥å¥–åŠ±ï¼Œä»è€Œæå‡æ¨¡å‹çš„é€’å½’æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAvRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®åˆæˆã€æ‰¹è¯„ä¸æ”¹è¿›æ¨¡å—ï¼Œä»¥åŠåŸºäºå¯å¾®å­¦ä¹ çš„å¥–åŠ±ä¼˜åŒ–è¿‡ç¨‹ã€‚æ•°æ®åˆæˆé˜¶æ®µç”Ÿæˆå¤šè½®æ•°æ®ï¼Œæ‰¹è¯„ä¸æ”¹è¿›æ¨¡å—åˆ™è´Ÿè´£å¯¹ç”Ÿæˆçš„ç»“æœè¿›è¡Œè¯„ä¼°å’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šAvRçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç²¾ç‚¼è¿‡ç¨‹çš„è®¾è®¡ï¼Œé€šè¿‡æ‰¹è¯„ä¸æ”¹è¿›çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€’å½’æ€ç»´èƒ½åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿçš„åå¥½ä¼˜åŒ–æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒAvRä½¿ç”¨äº†ç²¾ç‚¼æ„ŸçŸ¥å¥–åŠ±ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ é€’å½’æ¨ç†çš„èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAvRæ–¹æ³•åœ¨ä½¿ç”¨ä»…3000ä¸ªåˆæˆæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†LLaMA-3-8B-Instructæ¨¡å‹åœ¨AlpacaEval 2.0ä¸Šçš„èƒœç‡ï¼Œæå‡å¹…åº¦è¶…è¿‡20%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜AvRåœ¨ä¼˜åŒ–æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„åå¥½ä¼˜åŒ–æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„é€’å½’æ¨ç†èƒ½åŠ›ï¼ŒAvRæ–¹æ³•èƒ½å¤Ÿåœ¨å¤æ‚é—®é¢˜æ±‚è§£ã€çŸ¥è¯†æ¨ç†ç­‰ä»»åŠ¡ä¸­å±•ç°æ›´é«˜çš„æ™ºèƒ½æ°´å¹³ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The OpenAI o1-series models have demonstrated that leveraging long-form Chain of Thought (CoT) can substantially enhance performance. However, the recursive thinking capabilities of Large Language Models (LLMs) remain limited, particularly in the absence of expert-curated data for distillation. In this paper, we propose \textbf{AvR}: \textbf{Alignment via Refinement}, a novel method aimed at unlocking the potential of LLMs for recursive reasoning through long-form CoT. AvR introduces a refinement process that integrates criticism and improvement actions, guided by differentiable learning techniques to optimize \textbf{refinement-aware rewards}. As a result, the synthesized multi-round data can be organized as a long refinement thought, further enabling test-time scaling. Experimental results show that AvR significantly outperforms conventional preference optimization methods. Notably, with only 3k synthetic samples, our method boosts the performance of the LLaMA-3-8B-Instruct model by over 20\% in win rate on AlpacaEval 2.0. Our code is available at Github (https://github.com/Banner-Z/AvR.git).

