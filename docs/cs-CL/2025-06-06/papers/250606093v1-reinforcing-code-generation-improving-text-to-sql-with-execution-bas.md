---
layout: default
title: Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning
---

# Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06093" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06093v1</a>
  <a href="https://arxiv.org/pdf/2506.06093.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06093v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06093v1', 'Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Atharv Kulkarni, Vivek Srikumar

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: Under review at EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ‰§è¡Œåé¦ˆå¼ºåŒ–ä»£ç ç”Ÿæˆä»¥æå‡æ–‡æœ¬åˆ°SQLçš„è½¬æ¢èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç ç”Ÿæˆ` `æ–‡æœ¬åˆ°SQL` `å¼ºåŒ–å­¦ä¹ ` `æ‰§è¡Œåé¦ˆ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç›‘ç£å¾®è°ƒï¼Œç¼ºä¹ä¸å®é™…æ•°æ®åº“äº¤äº’çš„èƒ½åŠ›ï¼Œå¯¼è‡´ç”Ÿæˆçš„SQLæŸ¥è¯¢å‡†ç¡®æ€§ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æ‰§è¡Œåé¦ˆæ¥è°ƒæ•´æ¨¡å‹ï¼Œå¢å¼ºå…¶ç”ŸæˆSQLæŸ¥è¯¢çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLè°ƒä¼˜æ˜¾è‘—æé«˜äº†SQLä»£ç çš„å‡†ç¡®ç‡å’Œé™ä½äº†é”™è¯¯ç‡ï¼Œæ¥è¿‘æ›´å¤§æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä»£ç ç”Ÿæˆçš„é—®é¢˜ï¼Œé‡ç‚¹æ˜¯ä»è‡ªç„¶è¯­è¨€é—®é¢˜ç”ŸæˆSQLæŸ¥è¯¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ä¸æ•°æ®åº“å¼•æ“äº¤äº’æ¥è°ƒæ•´æ¨¡å‹çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯ä¾èµ–äºæ–‡æœ¬-ä»£ç å¯¹çš„ç›‘ç£å¾®è°ƒã€‚æˆ‘ä»¬å°†æ­¤é—®é¢˜æ¡†å®šä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œæ¨¡å‹é€šè¿‡ç¯å¢ƒåé¦ˆè·å¾—æ‰§è¡ŒåŸºç¡€çš„åé¦ˆï¼Œåé¦ˆä»¥æ ‡é‡å¥–åŠ±çš„å½¢å¼å‘ˆç°ï¼Œæƒ©ç½šæ‰§è¡Œå¤±è´¥å¹¶åœ¨æŸ¥è¯¢è¿”å›æ­£ç¡®ç­”æ¡ˆæ—¶ç»™äºˆæ­£å€¼ã€‚é€šè¿‡åœ¨Group Relative Policy Optimization (GRPO)æ¡†æ¶ä¸‹ä½¿ç”¨è¿™äº›å¥–åŠ±ï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªè¡¨æ ¼æ¨ç†åŸºå‡†ä¸Šæµ‹è¯•å’Œè¯„ä¼°äº†æˆ‘ä»¬çš„å‘ç°ã€‚ç»“æœè¡¨æ˜ï¼Œä»…é€šè¿‡é—®é¢˜-ç­”æ¡ˆå¯¹çš„å¼±ç›‘ç£ï¼ŒRLè°ƒä¼˜å°†æ¨¡å‹ç”Ÿæˆçš„SQLä»£ç çš„å‡†ç¡®ç‡ä»31.49æå‡è‡³49.83ï¼ŒåŒæ—¶é”™è¯¯ç‡ä»25.43%é™ä½è‡³14.71%ã€‚è¿™ä¸€æ”¹è¿›ä½¿æ¨¡å‹çš„æ€§èƒ½å‡ ä¹ä¸æ›´å¤§è§„æ¨¡çš„SQLCoder-70Bæ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†ä½¿ç”¨æ‰§è¡Œåé¦ˆæ¥æå‡LLMç¬¦å·æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰æ–‡æœ¬åˆ°SQLç”Ÿæˆæ–¹æ³•ä¸­ï¼Œä¾èµ–ç›‘ç£å¾®è°ƒå¯¼è‡´çš„å‡†ç¡®æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ•°æ®åº“æ‰§è¡Œåé¦ˆï¼Œé™åˆ¶äº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨ä¸æ•°æ®åº“çš„äº¤äº’æ¥è·å–æ‰§è¡Œåé¦ˆï¼Œä»¥æ­¤æ¥ä¼˜åŒ–æ¨¡å‹çš„SQLç”Ÿæˆèƒ½åŠ›ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿè®©æ¨¡å‹åœ¨å®é™…æ‰§è¡Œä¸­å­¦ä¹ ï¼Œä»è€Œæé«˜ç”Ÿæˆä»£ç çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹ä¸æ•°æ®åº“å¼•æ“çš„äº¤äº’ï¼Œæ¨¡å‹ç”ŸæˆSQLæŸ¥è¯¢åï¼Œæ•°æ®åº“æ‰§è¡Œè¯¥æŸ¥è¯¢å¹¶è¿”å›ç»“æœï¼Œæ¨¡å‹æ ¹æ®ç»“æœè·å¾—å¥–åŠ±ã€‚ä½¿ç”¨Group Relative Policy Optimization (GRPO)æ¡†æ¶æ¥å¤„ç†è¿™äº›å¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†æ‰§è¡Œåé¦ˆå¼•å…¥åˆ°ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å®é™…æ‰§è¡Œä¸­å­¦ä¹ ï¼Œè€Œä¸ä»…ä»…ä¾èµ–äºé™æ€çš„æ–‡æœ¬-ä»£ç å¯¹ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç¬¦å·æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä½¿ç”¨äº†å¼±ç›‘ç£çš„æ–¹å¼ï¼Œä»…ä¾èµ–é—®é¢˜-ç­”æ¡ˆå¯¹è¿›è¡Œåˆæ­¥è®­ç»ƒã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†æ‰§è¡Œåé¦ˆçš„å¥–åŠ±æœºåˆ¶ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç”ŸæˆSQLæ—¶èƒ½å¤Ÿæœ€å¤§åŒ–æ­£ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨RLè°ƒä¼˜åï¼Œæ¨¡å‹ç”Ÿæˆçš„SQLä»£ç å‡†ç¡®ç‡ä»31.49æå‡è‡³49.83ï¼Œé”™è¯¯ç‡ä»25.43%é™ä½è‡³14.71%ã€‚è¿™ä¸€æ”¹è¿›ä½¿å¾—æ¨¡å‹çš„æ€§èƒ½æ¥è¿‘æ›´å¤§è§„æ¨¡çš„SQLCoder-70Bæ¨¡å‹ï¼Œå±•ç¤ºäº†æ‰§è¡Œåé¦ˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®åº“æŸ¥è¯¢ç”Ÿæˆã€æ™ºèƒ½åŠ©æ‰‹ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„SQLç”Ÿæˆèƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ•°æ®åˆ†æã€å•†ä¸šæ™ºèƒ½ç­‰é¢†åŸŸæä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½å¯¹è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†å’Œå†³ç­–æ”¯æŒç³»ç»Ÿäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we study the problem of code generation with a large language model (LLM), with a focus on generating SQL queries from natural language questions. We ask: Instead of using supervised fine tuning with text-code pairs, can we tune a model by having it interact with a database engine? We frame this problem as a reinforcement learning problem where the model receives execution-based feedback from the environment in the form of scalar rewards. These rewards penalize execution failures and assign positive values when a query returns a correct answer. We use the rewards within the Group Relative Policy Optimization (GRPO) framework. We use a tabular reasoning benchmark to test and evaluate our findings. We find that with only weak supervision in the form of question-answer pairs, RL-tuning improves the accuracy of model generated SQL code from 31.49 to 49.83 while reducing error percentage from 25.43% to 14.71%. This improvement allowed the model nearly match the performance performance to the larger SQLCoder-70B model. Our work demonstrates the potential of using execution-based feedback to improve symbolic reasoning capabilities of LLMs.

