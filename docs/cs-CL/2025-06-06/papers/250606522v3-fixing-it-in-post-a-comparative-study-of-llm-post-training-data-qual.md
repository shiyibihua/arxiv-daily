---
layout: default
title: Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance
---

# Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06522" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06522v3</a>
  <a href="https://arxiv.org/pdf/2506.06522.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06522v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06522v3', 'Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aladin Djuhera, Swanand Ravindra Kadhe, Syed Zawad, Farhan Ahmed, Heiko Ludwig, Holger Boche

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-12-15)

**æœŸåˆŠ**: The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS) 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTuluTalkæ•°æ®é›†ä»¥æå‡LLMåè®­ç»ƒæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åè®­ç»ƒ` `æ•°æ®é›†æ„å»º` `æ€§èƒ½æå‡` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åè®­ç»ƒæ•°æ®é›†ç¼ºä¹é€æ˜åº¦ï¼Œå¯¼è‡´å¯¹å…¶æ„å»ºè¿‡ç¨‹å’Œæ•°æ®è´¨é‡çš„äº†è§£æœ‰é™ã€‚
2. æœ¬æ–‡é€šè¿‡å¯¹ä¸¤ä¸ªå¼€æ”¾åè®­ç»ƒæ•°æ®é›†è¿›è¡Œæ¯”è¾ƒåˆ†æï¼Œæå‡ºäº†æ–°çš„æ•°æ®æ··åˆTuluTalkï¼Œä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTuluTalkåœ¨æ ·æœ¬æ•°é‡å‡å°‘çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿåœ¨å…³é”®åŸºå‡†ä¸ŠåŒ¹é…æˆ–è¶…è¶Šæºæ•°æ®é›†çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç ”ç©¶è¶Šæ¥è¶Šå…³æ³¨åè®­ç»ƒå’Œä¸æ•°æ®é›†çš„å¯¹é½ï¼Œä»¥å¢å¼ºæŒ‡ä»¤éµå¾ªã€ä¸–ç•ŒçŸ¥è¯†å’Œä¸“ä¸šæŠ€èƒ½ã€‚ç„¶è€Œï¼Œè®¸å¤šé¢†å…ˆçš„LLMæ‰€ä½¿ç”¨çš„åè®­ç»ƒæ•°æ®é›†å¯¹å…¬ä¼—ä¸å¯è·å–ï¼Œç¼ºä¹é€æ˜åº¦ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹ä¸¤ä¸ªå¼€æ”¾åè®­ç»ƒæ•°æ®é›†Tulu-3-SFT-Mixå’ŒSmolTalkè¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒåˆ†æï¼Œå¹¶åŸºäºåˆ†æç»“æœè®¾è®¡äº†æ–°çš„æ•°æ®æ··åˆTuluTalkï¼Œæ ·æœ¬æ•°é‡å‡å°‘14%ï¼Œä½†åœ¨å…³é”®åŸºå‡†ä¸Šè¡¨ç°ç›¸å½“æˆ–æ›´ä¼˜ã€‚ç ”ç©¶ç»“æœä¸ºæ„å»ºæ›´æœ‰æ•ˆçš„åè®­ç»ƒæ•°æ®é›†æä¾›äº†å¯è¡Œçš„è§è§£ï¼Œå¹¶å…¬å¼€äº†æ³¨é‡Šçš„æºæ•°æ®é›†å’ŒTuluTalkæ··åˆæ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åè®­ç»ƒæ•°æ®é›†çš„é€æ˜åº¦ä¸è¶³å’Œå¯¹æ¨¡å‹æ€§èƒ½å½±å“çš„ä¸ç¡®å®šæ€§ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ç³»ç»Ÿæ¯”è¾ƒï¼Œå¯¼è‡´éš¾ä»¥è¯„ä¼°æ•°æ®è´¨é‡å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹Tulu-3-SFT-Mixå’ŒSmolTalkä¸¤ä¸ªå¼€æ”¾æ•°æ®é›†è¿›è¡Œè¯¦ç»†çš„è´¨é‡æŒ‡æ ‡æ³¨é‡Šï¼Œåˆ†æå…¶ç»“æ„å’Œè´¨é‡å·®å¼‚ï¼Œå¹¶åŸºäºæ­¤è®¾è®¡æ–°çš„æ•°æ®æ··åˆTuluTalkï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨Magpieæ¡†æ¶å¯¹æ ·æœ¬è¿›è¡Œæ³¨é‡Šï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é›†é€‰æ‹©ã€è´¨é‡æŒ‡æ ‡è¯„ä¼°ã€æ•°æ®æ··åˆè®¾è®¡å’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§æ¯”è¾ƒå¼€æ”¾åè®­ç»ƒæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†åŸºäºæ•°æ®è´¨é‡åˆ†æçš„æ–°æ•°æ®æ··åˆæ–¹æ³•TuluTalkï¼Œæ˜¾è‘—æå‡äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ··åˆè¿‡ç¨‹ä¸­ï¼Œæ³¨é‡æ ·æœ¬çš„å›åˆç»“æ„ã€ä»»åŠ¡ç±»åˆ«ã€è¾“å…¥è´¨é‡å’Œå“åº”è´¨é‡ç­‰å…³é”®å‚æ•°è®¾ç½®ï¼Œç¡®ä¿æ–°æ•°æ®é›†åœ¨æ€§èƒ½ä¸Šä¼˜äºæºæ•°æ®é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTuluTalkåœ¨æ ·æœ¬æ•°é‡å‡å°‘14%çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå…³é”®åŸºå‡†ä¸ŠåŒ¹é…æˆ–è¶…è¶ŠTulu-3-SFT-Mixå’ŒSmolTalkçš„æ€§èƒ½ã€‚è¿™ä¸€å‘ç°ä¸ºåè®­ç»ƒæ•°æ®é›†çš„æ„å»ºæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æ„å»ºæ›´æœ‰æ•ˆçš„åè®­ç»ƒæ•°æ®é›†ï¼Œç ”ç©¶èƒ½å¤Ÿå¸®åŠ©æå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œé™ä½èµ„æºæ¶ˆè€—ï¼Œå¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture.

