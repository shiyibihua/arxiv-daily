---
layout: default
title: dots.llm1 Technical Report
---

# dots.llm1 Technical Report

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05767" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05767v1</a>
  <a href="https://arxiv.org/pdf/2506.05767.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05767v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05767v1', 'dots.llm1 Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bi Huo, Bin Tu, Cheng Qin, Da Zheng, Debing Zhang, Dongjie Zhang, En Li, Fu Guo, Jian Yao, Jie Lou, Junfeng Tian, Li Hu, Ran Zhu, Shengdong Chen, Shuo Liu, Su Guang, Te Wo, Weijun Zhang, Xiaoming Shi, Xinxin Peng, Xing Wu, Yawen Liu, Yuqiu Ji, Ze Wen, Zhenhai Liu, Zichao Li, Zilong Liao

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºdots.llm1ä»¥é«˜æ•ˆæ¿€æ´»è¯­è¨€æ¨¡å‹å‚æ•°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `è¯­è¨€æ¨¡å‹` `é«˜æ•ˆè®­ç»ƒ` `å‚æ•°æ¿€æ´»` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å¼€æº` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨å‚æ•°æ¿€æ´»æ•ˆç‡å’Œè®­ç»ƒæˆæœ¬ä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ‰©å±•è§„æ¨¡ã€‚
2. dots.llm1é€šè¿‡æ··åˆä¸“å®¶æœºåˆ¶ï¼Œä»…æ¿€æ´»éƒ¨åˆ†å‚æ•°ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—èµ„æºæ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒäº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚
3. ç»è¿‡11.2Té«˜è´¨é‡æ ‡è®°çš„é¢„è®­ç»ƒï¼Œdots.llm1åœ¨æ€§èƒ½ä¸Šä¸Qwen2.5-72Bç›¸å½“ï¼Œä¸”æä¾›äº†ä¸­é—´è®­ç»ƒæ£€æŸ¥ç‚¹ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ä½œä¸ºä¸€ç§æœ‰æ•ˆæ‰©å±•è¯­è¨€æ¨¡å‹çš„èŒƒå¼ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªè¾“å…¥æ ‡è®°æ¿€æ´»éƒ¨åˆ†å‚æ•°è€Œå®ç°é«˜æ•ˆè®¡ç®—ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†dots.llm1ï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„MoEæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ€»è®¡142Bå‚æ•°ä¸­æ¿€æ´»14Bå‚æ•°ï¼Œæ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®å¤„ç†æµç¨‹ï¼Œdots.llm1åœ¨é¢„è®­ç»ƒ11.2Té«˜è´¨é‡æ ‡è®°åï¼Œæ€§èƒ½å¯ä¸Qwen2.5-72Bç›¸åª²ç¾ï¼Œä¸”åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æœªä½¿ç”¨ä»»ä½•åˆæˆæ•°æ®ã€‚ä¸ºä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å¼€æºäº†æ¯ä¸€ä¸‡äº¿æ ‡è®°çš„ä¸­é—´è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œæä¾›äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å­¦ä¹ åŠ¨æ€çš„å®è´µè§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨å‚æ•°æ¿€æ´»æ•ˆç‡å’Œè®­ç»ƒæˆæœ¬ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è®¡ç®—èµ„æºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šdots.llm1é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œä»…åœ¨æ¯æ¬¡è¾“å…¥æ—¶æ¿€æ´»éƒ¨åˆ†å‚æ•°ï¼Œä»è€Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªä¸“å®¶æ¨¡å—ï¼Œè¾“å…¥æ•°æ®ç»è¿‡é«˜æ•ˆçš„æ•°æ®å¤„ç†ç®¡é“ï¼Œæ¿€æ´»ç‰¹å®šçš„ä¸“å®¶è¿›è¡Œè®¡ç®—ï¼Œæœ€ç»ˆè¾“å‡ºç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ç²¾ç¡®çš„å‚æ•°æ¿€æ´»ç­–ç•¥ï¼Œdots.llm1èƒ½å¤Ÿåœ¨æ€»å‚æ•°é‡è¾¾åˆ°142Bçš„æƒ…å†µä¸‹ï¼Œä»…æ¿€æ´»14Bå‚æ•°ï¼Œè¾¾åˆ°ä¸æ›´å¤§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é«˜æ•ˆçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿åœ¨æ¿€æ´»å‚æ•°çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå¿«é€Ÿæ”¶æ•›å¹¶æå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

dots.llm1åœ¨ç»è¿‡11.2Té«˜è´¨é‡æ ‡è®°çš„é¢„è®­ç»ƒåï¼Œæ€§èƒ½ä¸Qwen2.5-72Bç›¸å½“ï¼Œä¸”åœ¨å‚æ•°æ¿€æ´»æ–¹é¢ä»…ä½¿ç”¨äº†14Bå‚æ•°ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚è¯¥æ¨¡å‹çš„å¼€æºä¸­é—´æ£€æŸ¥ç‚¹ä¸ºç ”ç©¶è€…æä¾›äº†æ·±å…¥ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹å­¦ä¹ åŠ¨æ€çš„æœºä¼šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

dots.llm1çš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸã€‚é€šè¿‡é«˜æ•ˆçš„å‚æ•°æ¿€æ´»æœºåˆ¶ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæä¾›é«˜è´¨é‡çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å•†ä¸šåŒ–åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture of Experts (MoE) models have emerged as a promising paradigm for scaling language models efficiently by activating only a subset of parameters for each input token. In this report, we present dots.llm1, a large-scale MoE model that activates 14B parameters out of a total of 142B parameters, delivering performance on par with state-of-the-art models while reducing training and inference costs. Leveraging our meticulously crafted and efficient data processing pipeline, dots.llm1 achieves performance comparable to Qwen2.5-72B after pretraining on 11.2T high-quality tokens and post-training to fully unlock its capabilities. Notably, no synthetic data is used during pretraining. To foster further research, we open-source intermediate training checkpoints at every one trillion tokens, providing valuable insights into the learning dynamics of large language models.

