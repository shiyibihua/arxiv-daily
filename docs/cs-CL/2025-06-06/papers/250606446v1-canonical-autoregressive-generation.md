---
layout: default
title: Canonical Autoregressive Generation
---

# Canonical Autoregressive Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06446" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06446v1</a>
  <a href="https://arxiv.org/pdf/2506.06446.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06446v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06446v1', 'Canonical Autoregressive Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ivi Chatzi, Nina Corvelo Benz, Stratis Tsirtsis, Manuel Gomez-Rodriguez

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§„èŒƒè‡ªå›å½’ç”Ÿæˆæ–¹æ³•ä»¥è§£å†³è¯­è¨€æ¨¡å‹ç”Ÿæˆéè§„èŒƒåºåˆ—é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `è‡ªå›å½’ç”Ÿæˆ` `è§„èŒƒé‡‡æ ·` `æ–‡æœ¬ç”Ÿæˆ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¸¸å¸¸äº§ç”Ÿéè§„èŒƒçš„æ ‡è®°åºåˆ—ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœçš„è´¨é‡ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºè§„èŒƒé‡‡æ ·æ–¹æ³•ï¼Œé€šè¿‡åœ¨è‡ªå›å½’ç”Ÿæˆçš„æ¯ä¸€æ­¥ç¡®ä¿ç”Ÿæˆéƒ¨åˆ†è§„èŒƒæ ‡è®°åºåˆ—ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè§„èŒƒé‡‡æ ·ç”Ÿæˆçš„æ ‡è®°åºåˆ—åˆ†å¸ƒæ˜¾è‘—æ›´æ¥è¿‘è®­ç»ƒæ—¶çš„çœŸå®åˆ†å¸ƒï¼Œæå‡äº†ç”Ÿæˆæ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¾èµ–äºä»åŸå§‹æ–‡æœ¬ä¸­æå–çš„æ ‡è®°è¿›è¡Œè®­ç»ƒï¼Œè€Œæ ‡è®°å™¨å†³å®šäº†æ¨¡å‹æ¨ç†æ—¶ä½¿ç”¨çš„æ ‡è®°è¯æ±‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¹¶ä¸æ€»æ˜¯ç”Ÿæˆè§„èŒƒçš„æ ‡è®°åºåˆ—ï¼Œè¿™ä¼šå¸¦æ¥è´Ÿé¢å½±å“ã€‚æœ¬æ–‡é¦–å…ˆè¯æ˜ï¼Œä¸ºäº†ç”Ÿæˆè§„èŒƒçš„æ ‡è®°åºåˆ—ï¼Œæ¨¡å‹åœ¨è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­éœ€è¦åœ¨æ¯ä¸€æ­¥ç”Ÿæˆéƒ¨åˆ†è§„èŒƒçš„æ ‡è®°åºåˆ—ã€‚åŸºäºè¿™ä¸€ç†è®ºç»“æœï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•é«˜æ•ˆçš„é‡‡æ ·æ–¹æ³•â€”â€”è§„èŒƒé‡‡æ ·ï¼Œèƒ½å¤Ÿé˜²æ­¢æ¨¡å‹ç”Ÿæˆéè§„èŒƒçš„æ ‡è®°åºåˆ—ã€‚ä¸æ ‡å‡†é‡‡æ ·ç›¸æ¯”ï¼Œä½¿ç”¨è§„èŒƒé‡‡æ ·ç”Ÿæˆçš„æ ‡è®°åºåˆ—åˆ†å¸ƒæ›´æ¥è¿‘è®­ç»ƒæ—¶ä½¿ç”¨çš„çœŸå®æ ‡è®°åºåˆ—åˆ†å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­ç”Ÿæˆéè§„èŒƒæ ‡è®°åºåˆ—çš„ç°è±¡ï¼Œè¿™ç§ç°è±¡ä¼šå½±å“ç”Ÿæˆç»“æœçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆä¿è¯ç”Ÿæˆçš„æ ‡è®°åºåˆ—ç¬¦åˆè§„èŒƒï¼Œå¯¼è‡´ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥è§„èŒƒé‡‡æ ·æ–¹æ³•ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¯ä¸€æ­¥ç”Ÿæˆè¿‡ç¨‹ä¸­éƒ½ç”Ÿæˆéƒ¨åˆ†è§„èŒƒçš„æ ‡è®°åºåˆ—ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜ç”Ÿæˆåºåˆ—çš„è´¨é‡ï¼Œä½¿å…¶æ›´æ¥è¿‘è®­ç»ƒæ—¶çš„çœŸå®åˆ†å¸ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ ‡è®°å™¨ã€æ¨¡å‹ç”Ÿæˆæ¨¡å—å’Œè§„èŒƒé‡‡æ ·æ¨¡å—ã€‚æ ‡è®°å™¨è´Ÿè´£å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°ï¼Œæ¨¡å‹ç”Ÿæˆæ¨¡å—è¿›è¡Œè‡ªå›å½’ç”Ÿæˆï¼Œè€Œè§„èŒƒé‡‡æ ·æ¨¡å—åˆ™åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çº¦æŸç”Ÿæˆçš„æ ‡è®°åºåˆ—ä¸ºè§„èŒƒåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è§„èŒƒé‡‡æ ·è¿™ä¸€æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢ç”Ÿæˆéè§„èŒƒæ ‡è®°åºåˆ—ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œè§„èŒƒé‡‡æ ·åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥äº†é¢å¤–çš„çº¦æŸï¼Œç¡®ä¿ç”Ÿæˆçš„æ ‡è®°åºåˆ—ç¬¦åˆé¢„æœŸçš„è§„èŒƒæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è§„èŒƒé‡‡æ ·ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ ‡è®°åºåˆ—åœ¨æ¯ä¸€æ­¥éƒ½ç¬¦åˆè§„èŒƒã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æ”¯æŒè¿™ä¸€æ–°çš„é‡‡æ ·ç­–ç•¥ï¼Œæå‡ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è§„èŒƒé‡‡æ ·ç”Ÿæˆçš„æ ‡è®°åºåˆ—åˆ†å¸ƒä¸è®­ç»ƒæ—¶çš„çœŸå®åˆ†å¸ƒç›¸æ¯”ï¼Œæ˜¾è‘—æ›´æ¥è¿‘ï¼Œæå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ã€‚ä¸æ ‡å‡†é‡‡æ ·æ–¹æ³•ç›¸æ¯”ï¼Œè§„èŒƒé‡‡æ ·åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è§„èŒƒæ€§å’Œä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æå‡ç”¨æˆ·ä½“éªŒå’Œç”Ÿæˆå†…å®¹çš„è´¨é‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> State of the art large language models are trained using large amounts of tokens derived from raw text using what is called a tokenizer. Crucially, the tokenizer determines the (token) vocabulary a model will use during inference as well as, in principle, the (token) language. This is because, while the token vocabulary may allow for different tokenizations of a string, the tokenizer always maps the string to only one of these tokenizations--the canonical tokenization. However, multiple lines of empirical evidence suggest that large language models do not always generate canonical token sequences, and this comes with several negative consequences. In this work, we first show that, to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process underpinning its functioning. Building upon this theoretical result, we introduce canonical sampling, a simple and efficient sampling method that precludes a given model from generating non-canonical token sequences. Further, we also show that, in comparison with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.

