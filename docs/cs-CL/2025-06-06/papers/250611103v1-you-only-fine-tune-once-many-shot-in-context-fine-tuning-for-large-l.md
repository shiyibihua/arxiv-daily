---
layout: default
title: You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model
---

# You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11103" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11103v1</a>
  <a href="https://arxiv.org/pdf/2506.11103.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11103v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11103v1', 'You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenchong He, Liqian Peng, Zhe Jiang, Alex Go

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: 16 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºManyICLä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å¾®è°ƒæ–¹æ³•` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å°‘é‡ä¸Šä¸‹æ–‡å¾®è°ƒæ–¹æ³•åœ¨æ€§èƒ½ä¸Šä»ç„¶è½åäºä¸“é—¨å¾®è°ƒï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚
2. æœ¬æ–‡æå‡ºManyICLæ–¹æ³•ï¼Œé€šè¿‡å°†æ¯ä¸ªä¸Šä¸‹æ–‡ç­”æ¡ˆè§†ä¸ºç›‘ç£ç›®æ ‡ï¼Œæå‡äº†å¤šæ ·æœ¬å¾®è°ƒçš„æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒManyICLåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºé›¶/å°‘é‡å¾®è°ƒï¼Œæ¥è¿‘ä¸“é—¨å¾®è°ƒçš„æ€§èƒ½ï¼Œå¹¶å‡è½»äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡å‡ºè‰²çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡è€Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„å°‘é‡ä¸Šä¸‹æ–‡å¾®è°ƒæ–¹æ³•ä»ç„¶ä¸åŠä¸“é—¨å¾®è°ƒçš„æ•ˆæœã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•Many-Shot In-Context Fine-tuningï¼ˆManyICLï¼‰ï¼Œé€šè¿‡å°†ICLæ‰©å±•åˆ°å¤šæ ·æœ¬è®¾ç½®ï¼Œæ˜¾è‘—ç¼©å°äº†è¿™ä¸€æ€§èƒ½å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°è®­ç»ƒç›®æ ‡ï¼Œå°†ä¸Šä¸‹æ–‡ä¸­çš„æ¯ä¸ªç­”æ¡ˆè§†ä¸ºç›‘ç£å­¦ä¹ çš„ç›®æ ‡ï¼Œä»è€Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒManyICLåœ¨åˆ†ç±»ã€æ‘˜è¦ã€é—®ç­”ç­‰å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ¥è¿‘ä¸“é—¨å¾®è°ƒçš„æ•ˆæœï¼Œå¹¶æ˜¾è‘—å‡è½»äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å°‘é‡ä¸Šä¸‹æ–‡å¾®è°ƒæ–¹æ³•åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é•¿åºåˆ—å’Œå¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹æ—¶çš„ä½æ•ˆé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šManyICLæ–¹æ³•é€šè¿‡å°†ä¸Šä¸‹æ–‡ä¸­çš„æ¯ä¸ªç­”æ¡ˆè§†ä¸ºç›‘ç£å­¦ä¹ ç›®æ ‡ï¼Œæ”¹å˜äº†ä¼ ç»Ÿçš„å¾®è°ƒæ–¹å¼ï¼Œä½¿å¾—å¤šæ ·æœ¬ç¤ºä¾‹ä¸ä»…ä½œä¸ºæç¤ºï¼Œè¿˜ä½œä¸ºè‡ªå›å½’å­¦ä¹ çš„ç›®æ ‡ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€ä¸Šä¸‹æ–‡æ„å»ºã€è®­ç»ƒç›®æ ‡å®šä¹‰å’Œæ¨¡å‹è®­ç»ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®å‡†å¤‡é˜¶æ®µæ”¶é›†å¤šæ ·æœ¬æ•°æ®ï¼Œæ„å»ºä¸Šä¸‹æ–‡åè¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šManyICLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†æ¯ä¸ªä¸Šä¸‹æ–‡ç­”æ¡ˆè§†ä¸ºè®­ç»ƒç›®æ ‡ï¼Œè¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºæå‡äº†æ¨¡å‹å¯¹å¤šæ ·æœ¬çš„å­¦ä¹ èƒ½åŠ›ï¼Œç¼©å°äº†ä¸ä¸“é—¨å¾®è°ƒçš„æ€§èƒ½å·®è·ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨æ–°çš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¯ä¸ªä¸Šä¸‹æ–‡ç­”æ¡ˆçš„é¢„æµ‹ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°å¤šæ ·æœ¬ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæ¨¡å‹æ¶æ„ä¿æŒäº†è‡ªå›å½’ç‰¹æ€§ï¼Œä»¥é€‚åº”æ–°çš„è®­ç»ƒç›®æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒManyICLåœ¨åˆ†ç±»ã€æ‘˜è¦ã€é—®ç­”ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é›¶/å°‘é‡å¾®è°ƒæ–¹æ³•ï¼Œæ€§èƒ½æ¥è¿‘ä¸“é—¨å¾®è°ƒçš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒManyICLçš„å‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œä¸”æœ‰æ•ˆå‡è½»äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå±•ç°å‡ºæ›´å¥½çš„æ¨¡å‹ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ManyICLæ–¹æ³•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç”Ÿæˆã€é—®ç­”ç³»ç»Ÿå’Œè‡ªç„¶è¯­è¨€æ¨ç†ç­‰ã€‚å…¶é«˜æ•ˆçš„å¾®è°ƒæ–¹å¼èƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´å¿«é€Ÿåœ°é€‚åº”ä¸åŒä»»åŠ¡ï¼Œæå‡æ¨¡å‹çš„å®ç”¨æ€§å’Œçµæ´»æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½åº”ç”¨ï¼Œæå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) possess a remarkable ability to perform in-context learning (ICL), which enables them to handle multiple downstream tasks simultaneously without requiring task-specific fine-tuning. Recent studies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma 7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of all tasks at once. However, this approach still lags behind dedicated fine-tuning, where a separate model is trained for each individual task.
>   In this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning (ManyICL), which significantly narrows this performance gap by extending the principles of ICL to a many-shot setting. To unlock the full potential of ManyICL and address the inherent inefficiency of processing long sequences with numerous in-context examples, we propose a novel training objective. Instead of solely predicting the final answer, our approach treats every answer within the context as a supervised training target. This effectively shifts the role of many-shot examples from prompts to targets for autoregressive learning. Through extensive experiments on diverse downstream tasks, including classification, summarization, question answering, natural language inference, and math, we demonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning. Furthermore, ManyICL significantly mitigates catastrophic forgetting issues observed in zero/few-shot fine-tuning. The code will be made publicly available upon publication.

