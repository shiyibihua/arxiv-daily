---
layout: default
title: Masked Language Models are Good Heterogeneous Graph Generalizers
---

# Masked Language Models are Good Heterogeneous Graph Generalizers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06157" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06157v2</a>
  <a href="https://arxiv.org/pdf/2506.06157.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06157v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06157v2', 'Masked Language Models are Good Heterogeneous Graph Generalizers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinyu Yang, Cheng Yang, Shanyuan Cui, Zeyuan Guo, Liangwei Yang, Muhan Zhang, Zhiqiang Zhang, Chuan Shi

**åˆ†ç±»**: cs.SI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-07-30)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/BUPT-GAMMA/MLM4HG)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMLM4HGä»¥è§£å†³å¼‚æ„å›¾æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼‚æ„å›¾ç¥ç»ç½‘ç»œ` `æ©ç è¯­è¨€å»ºæ¨¡` `è·¨é¢†åŸŸå­¦ä¹ ` `å¤šä»»åŠ¡å­¦ä¹ ` `å›¾è¡¨ç¤ºå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„HGNNsåœ¨è·¨é¢†åŸŸå’Œä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå½±å“äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºçš„MLM4HGæ–¹æ³•é€šè¿‡ä½¿ç”¨åŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬åºåˆ—æ›¿ä»£HGæ ‡è®°ï¼Œæ—¨åœ¨æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. åœ¨å››ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„è·¨é¢†åŸŸå’Œå¤šä»»åŠ¡å®éªŒä¸­ï¼ŒMLM4HGåœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼‚æ„å›¾ç¥ç»ç½‘ç»œï¼ˆHGNNsï¼‰åœ¨æ•æ‰å¼‚æ„å›¾ï¼ˆHGsï¼‰çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è·¨é¢†åŸŸå’Œä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè¿‘æœŸç ”ç©¶æ¢ç´¢äº†HGNNsä¸LLMsçš„ç»“åˆä»¥å®ç°å¯æ³›åŒ–çš„å¼‚æ„å›¾å­¦ä¹ ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å°†ç»“æ„ä¿¡æ¯ç¼–ç ä¸ºHGæ ‡è®°ï¼Œå¯¼è‡´HGNNsä¸LLMsä¹‹é—´çš„åµŒå…¥ç©ºé—´å·®å¼‚å½±å“LLMså¯¹HGsçš„ç†è§£ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„åŸºäºæ©ç è¯­è¨€å»ºæ¨¡çš„æ–¹æ³•MLM4HGï¼Œé€šè¿‡å¼•å…¥åŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬åºåˆ—æ¥æå–HGsä¸­çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶è®¾è®¡å®šåˆ¶çš„æ–‡æœ¬æ¨¡æ¿ï¼Œå°†ä¸åŒå›¾ä»»åŠ¡ç»Ÿä¸€ä¸ºè¿è´¯çš„å¡«ç©ºå¼â€œæ©ç â€æ ‡è®°é¢„æµ‹èŒƒå¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMLM4HGåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼‚æ„å›¾ç¥ç»ç½‘ç»œåœ¨è·¨é¢†åŸŸå’Œä»»åŠ¡æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºHGæ ‡è®°ï¼Œå¯¼è‡´HGNNsä¸LLMsä¹‹é—´çš„åµŒå…¥ç©ºé—´å·®å¼‚ï¼Œå½±å“äº†æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„MLM4HGæ–¹æ³•é€šè¿‡å¼•å…¥åŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬åºåˆ—ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„HGæ ‡è®°ï¼Œä»è€Œæå–HGsä¸­çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶è®¾è®¡ç»Ÿä¸€çš„æ–‡æœ¬æ¨¡æ¿ï¼Œä»¥å®ç°ä¸åŒå›¾ä»»åŠ¡çš„è¿è´¯æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMLM4HGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œå°†æ¥è‡ªä¸åŒé¢†åŸŸçš„HGsè½¬æ¢ä¸ºåŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬ï¼›å…¶æ¬¡ï¼Œå°†è¿™äº›æ–‡æœ¬ä¸ç»Ÿä¸€çš„ä»»åŠ¡æ–‡æœ¬ç»“åˆï¼Œå½¢æˆHGåŸºç¡€è¯­æ–™åº“ï¼›æœ€åï¼Œå°†è¯¥è¯­æ–™åº“è¾“å…¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šMLM4HGçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä½¿ç”¨åŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬åºåˆ—æ›¿ä»£HGæ ‡è®°ï¼Œè¿™ä¸€è®¾è®¡æœ‰æ•ˆè§£å†³äº†HGNNsä¸LLMsä¹‹é—´çš„åµŒå…¥ç©ºé—´å·®å¼‚é—®é¢˜ï¼Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒMLM4HGé‡‡ç”¨äº†å®šåˆ¶çš„æ–‡æœ¬æ¨¡æ¿å’Œçº¦æŸç›®æ ‡è¯æ±‡ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°HGsçš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å››ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLM4HGåœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸‹çš„æ³›åŒ–æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ„å»ºç­‰ã€‚é€šè¿‡æå‡å¼‚æ„å›¾çš„æ³›åŒ–èƒ½åŠ›ï¼ŒMLM4HGèƒ½å¤Ÿåœ¨å¤šç§å®é™…åœºæ™¯ä¸­æä¾›æ›´å‡†ç¡®çš„åˆ†æå’Œé¢„æµ‹ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Heterogeneous graph neural networks (HGNNs) excel at capturing structural and semantic information in heterogeneous graphs (HGs), while struggling to generalize across domains and tasks. With the rapid advancement of large language models (LLMs), a recent study explored the integration of HGNNs with LLMs for generalizable heterogeneous graph learning. However, this approach typically encodes structural information as HG tokens using HGNNs, and disparities in embedding spaces between HGNNs and LLMs have been shown to bias the LLM's comprehension of HGs. Moreover, since these HG tokens are often derived from node-level tasks, the model's ability to generalize across tasks remains limited. To this end, we propose a simple yet effective Masked Language Modeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens to extract structural and semantic information inherent in HGs, and designs customized textual templates to unify different graph tasks into a coherent cloze-style 'mask' token prediction paradigm. Specifically,MLM4HG first converts HGs from various domains to texts based on metapaths, and subsequently combines them with the unified task texts to form a HG-based corpus. Moreover, the corpus is fed into a pretrained LM for fine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to generalize to unseen target HGs. Extensive cross-domain and multi-task experiments on four real-world datasets demonstrate the superior generalization performance of MLM4HG over state-of-the-art methods in both few-shot and zero-shot scenarios. Our code is available at https://github.com/BUPT-GAMMA/MLM4HG.

