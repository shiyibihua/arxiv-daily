---
layout: default
title: LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles
---

# LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06561" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06561v4</a>
  <a href="https://arxiv.org/pdf/2506.06561.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06561v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06561v4', 'LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ho Yin 'Sam' Ng, Ting-Yao Hsu, Aashish Anantha Ramakrishnan, Branislav Kveton, Nedim Lipka, Franck Dernoncourt, Dongwon Lee, Tong Yu, Sungchul Kim, Ryan A. Rossi, Ting-Hao 'Kenneth' Huang

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-09-22)

**å¤‡æ³¨**: Accepted to EMNLP 2025 Findings. The LaMP-CAP dataset is publicly available at: https://github.com/Crowd-AI-Lab/lamp-cap

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLaMP-Capä»¥è§£å†³ä¸ªæ€§åŒ–å›¾å½¢æ ‡é¢˜ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–ç”Ÿæˆ` `å›¾å½¢æ ‡é¢˜` `å¤šæ¨¡æ€å­¦ä¹ ` `ä¸Šä¸‹æ–‡ç†è§£` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾å½¢æ ‡é¢˜ç”Ÿæˆæ¨¡å‹é€šå¸¸ç”Ÿæˆé€šç”¨æ ‡é¢˜ï¼Œç¼ºä¹ä¸ªæ€§åŒ–ï¼Œå¯¼è‡´ä½œè€…éœ€è¦è¿›è¡Œå¤§é‡ä¿®æ”¹ä»¥é€‚åº”å…¶é£æ ¼ã€‚
2. æœ¬æ–‡æå‡ºLaMP-Capæ•°æ®é›†ï¼Œé€šè¿‡æä¾›å¤šæ¨¡æ€å›¾å½¢ç‰¹å¾ï¼Œå¸®åŠ©ç”Ÿæˆä¸ªæ€§åŒ–çš„å›¾å½¢æ ‡é¢˜ï¼Œæå‡ç”Ÿæˆè´¨é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¤šæ¨¡æ€ç‰¹å¾ä¿¡æ¯ç”Ÿæˆçš„æ ‡é¢˜ä¸åŸä½œè€…çš„æ ‡é¢˜æ›´ä¸ºæ¥è¿‘ï¼Œä¸”å›¾åƒä¿¡æ¯çš„è´¡çŒ®æ›´å¤§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾å½¢æ ‡é¢˜å¯¹äºå¸®åŠ©è¯»è€…ç†è§£å’Œè®°å¿†å›¾å½¢çš„å…³é”®ä¿¡æ¯è‡³å…³é‡è¦ã€‚å°½ç®¡å·²æœ‰å¤šç§æ¨¡å‹ç”¨äºç”Ÿæˆè¿™äº›æ ‡é¢˜ï¼Œä½†ç”Ÿæˆçš„é€šç”¨æ ‡é¢˜å¾€å¾€éœ€è¦ä½œè€…è¿›è¡Œä¿®æ”¹ä»¥åŒ¹é…å…¶å†™ä½œé£æ ¼å’Œé¢†åŸŸç‰¹å¾ï¼Œçªæ˜¾äº†ä¸ªæ€§åŒ–çš„éœ€æ±‚ã€‚æœ¬æ–‡æå‡ºäº†LaMP-Capï¼Œä¸€ä¸ªç”¨äºä¸ªæ€§åŒ–å›¾å½¢æ ‡é¢˜ç”Ÿæˆçš„æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ¨¡æ€å›¾å½¢ç‰¹å¾ã€‚è¯¥æ•°æ®é›†ä¸ä»…æä¾›ç›®æ ‡å›¾å½¢çš„å›¾åƒï¼Œè¿˜åŒ…æ‹¬æ¥è‡ªåŒä¸€æ–‡æ¡£çš„æœ€å¤šä¸‰ä¸ªå…¶ä»–å›¾å½¢çš„å›¾åƒã€æ ‡é¢˜å’ŒæåŠæ®µè½ï¼Œä»¥è¡¨å¾ä¸Šä¸‹æ–‡ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¿™äº›ç‰¹å¾ä¿¡æ¯å¯ä»¥ç”Ÿæˆæ›´æ¥è¿‘åŸä½œè€…æ’°å†™çš„æ ‡é¢˜ï¼Œä¸”æ¶ˆèç ”ç©¶æ˜¾ç¤ºï¼Œå›¾åƒä¿¡æ¯æ¯”æåŠæ®µè½æ›´ä¸ºæœ‰æ•ˆï¼Œå¼ºè°ƒäº†å¤šæ¨¡æ€ç‰¹å¾çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³ä¸ªæ€§åŒ–å›¾å½¢æ ‡é¢˜ç”Ÿæˆçš„é—®é¢˜ï¼Œç°æœ‰æ¨¡å‹ç”Ÿæˆçš„é€šç”¨æ ‡é¢˜å¾€å¾€æ— æ³•æ»¡è¶³ä½œè€…çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼Œå¯¼è‡´éœ€è¦å¤§é‡ä¿®æ”¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºLaMP-Capæ•°æ®é›†ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å›¾å½¢ç‰¹å¾ï¼ˆåŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬ï¼‰æ¥ç”Ÿæˆæ›´ç¬¦åˆä½œè€…é£æ ¼çš„æ ‡é¢˜ï¼Œå¼ºè°ƒä¸Šä¸‹æ–‡ä¿¡æ¯çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€ç‰¹å¾æå–å’Œæ ‡é¢˜ç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æä¾›ç›®æ ‡å›¾å½¢åŠå…¶ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç‰¹å¾æå–æ¨¡å—ä»å›¾åƒå’Œæ–‡æœ¬ä¸­æå–æœ‰ç”¨ä¿¡æ¯ï¼Œæ ‡é¢˜ç”Ÿæˆæ¨¡å—ä½¿ç”¨è¿™äº›ä¿¡æ¯ç”Ÿæˆä¸ªæ€§åŒ–æ ‡é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å¤šæ¨¡æ€å›¾å½¢ç‰¹å¾ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ ‡é¢˜ç”Ÿæˆçš„ä¸ªæ€§åŒ–ç¨‹åº¦ï¼Œä¸ä¼ ç»Ÿçš„æ–‡æœ¬å•ä¸€è¾“å…¥æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç”Ÿæˆæ ‡é¢˜çš„è´¨é‡ï¼Œå¹¶åœ¨ç‰¹å¾æå–è¿‡ç¨‹ä¸­ç»“åˆäº†å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ï¼Œç¡®ä¿ç”Ÿæˆçš„æ ‡é¢˜æ›´å…·ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨LaMP-Capæ•°æ®é›†ç”Ÿæˆçš„æ ‡é¢˜ä¸åŸä½œè€…çš„æ ‡é¢˜åœ¨è¯­ä¹‰ä¸Šæ›´ä¸ºæ¥è¿‘ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œå›¾åƒä¿¡æ¯çš„ä½¿ç”¨æ¯”æ–‡æœ¬ä¿¡æ¯æ›´ä¸ºæœ‰æ•ˆï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å¤šæ¨¡æ€ç‰¹å¾çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å­¦æœ¯å‡ºç‰ˆã€æ•™è‚²ææ–™å’Œåœ¨çº¿å†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡ç”Ÿæˆä¸ªæ€§åŒ–çš„å›¾å½¢æ ‡é¢˜ï¼Œå¯ä»¥æé«˜è¯»è€…çš„ç†è§£å’Œè®°å¿†æ•ˆæœï¼Œè¿›è€Œæå‡å†…å®¹çš„ä¼ æ’­æ•ˆç‡å’Œè´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½åœ¨è‡ªåŠ¨åŒ–æ–‡æ¡£ç”Ÿæˆå’Œæ™ºèƒ½å†™ä½œåŠ©æ‰‹ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.

