---
layout: default
title: Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach
---

# Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06175" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06175v1</a>
  <a href="https://arxiv.org/pdf/2506.06175.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06175v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06175v1', 'Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: James Ford, Anthony Rios

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: 8 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šä»£ç†æ–¹æ³•ä»¥é™ä½æ–‡æœ¬åˆ°å›¾è¡¨ç”Ÿæˆä¸­çš„æ‰§è¡Œé”™è¯¯ç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾è¡¨ç”Ÿæˆ` `å¤šä»£ç†æ–¹æ³•` `æ‰§è¡Œé”™è¯¯ç‡` `æ•°æ®å¯è§†åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°†è‡ªç„¶è¯­è¨€æè¿°è½¬æ¢ä¸ºå›¾è¡¨ä»£ç æ—¶ï¼Œä»å­˜åœ¨çº¦15%çš„æ‰§è¡Œå¤±è´¥ç‡ï¼Œå½±å“äº†å®é™…åº”ç”¨æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»£ç†ç®¡é“ï¼Œåˆ†ç¦»äº†è‰æ‹Ÿã€æ‰§è¡Œã€ä¿®å¤å’Œåˆ¤æ–­è¿‡ç¨‹ï¼Œä»¥æé«˜ç”Ÿæˆä»£ç çš„æ‰§è¡ŒæˆåŠŸç‡ã€‚
3. åœ¨Text2Chart31å’ŒChartXåŸºå‡†ä¸Šï¼Œç³»ç»Ÿçš„æ‰§è¡Œé”™è¯¯ç‡åˆ†åˆ«é™è‡³4.5%å’Œ4.6%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å¾®è°ƒæ–¹æ³•ï¼Œä¸”è®¡ç®—èµ„æºéœ€æ±‚æ›´ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€å›¾è¡¨æè¿°è½¬æ¢ä¸ºå¯è¿è¡Œçš„ä»£ç ï¼Œä½†ç”Ÿæˆçš„è„šæœ¬ä»æœ‰çº¦15%çš„æ‰§è¡Œå¤±è´¥ç‡ã€‚æœ¬æ–‡æ¢è®¨è¿™ä¸€é”™è¯¯ç‡æ˜¯å¦æºäºæ¨¡å‹å±€é™æ€§æˆ–å•ä¸€æç¤ºè®¾è®¡çš„ä¾èµ–ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§çš„å¤šä»£ç†ç®¡é“ï¼Œåˆ†ç¦»äº†è‰æ‹Ÿã€æ‰§è¡Œã€ä¿®å¤å’Œåˆ¤æ–­è¿‡ç¨‹ï¼Œä½¿ç”¨ç°æˆçš„GPT-4o-miniæ¨¡å‹ã€‚åœ¨Text2Chart31åŸºå‡†ä¸Šï¼Œç³»ç»Ÿå°†æ‰§è¡Œé”™è¯¯ç‡é™ä½è‡³4.5%ï¼Œè¶…è¶Šæœ€å¼ºçš„å¾®è°ƒåŸºçº¿è¿‘5ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶è®¡ç®—éœ€æ±‚æ˜¾è‘—é™ä½ã€‚åœ¨ChartXåŸºå‡†ä¸Šä¹Ÿè§‚å¯Ÿåˆ°ç±»ä¼¼è¡¨ç°ï¼Œé”™è¯¯ç‡ä¸º4.6%ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å°½ç®¡æ‰§è¡ŒæˆåŠŸç‡å·²å¤§å¹…æå‡ï¼Œä½†æ‰‹åŠ¨å®¡æŸ¥å‘ç°100ä¸ªæ ·æœ¬ä¸­æœ‰6ä¸ªå›¾è¡¨å­˜åœ¨è™šæ„å†…å®¹ï¼Œä¸”æ— éšœç¢å®¡è®¡æ˜¾ç¤ºç”Ÿæˆå›¾è¡¨åœ¨è‰²ç›²æŒ‡å—çš„åˆè§„æ€§ä¸Šè¡¨ç°ä¸ä½³ã€‚è¿™è¡¨æ˜æœªæ¥å·¥ä½œåº”å…³æ³¨å›¾è¡¨ç¾å­¦ã€è¯­ä¹‰å‡†ç¡®æ€§å’Œæ— éšœç¢æ€§æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾è¡¨ç”Ÿæˆä¸­çš„æ‰§è¡Œé”™è¯¯ç‡é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å•ä¸€æç¤ºè®¾è®¡ï¼Œå¯¼è‡´ç”Ÿæˆä»£ç çš„æ‰§è¡Œå¤±è´¥ç‡è¾ƒé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„å¤šä»£ç†ç®¡é“é€šè¿‡å°†ç”Ÿæˆè¿‡ç¨‹åˆ†ä¸ºè‰æ‹Ÿã€æ‰§è¡Œã€ä¿®å¤å’Œåˆ¤æ–­å››ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆä»£ç çš„æ‰§è¡ŒæˆåŠŸç‡å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šè‰æ‹Ÿæ¨¡å—è´Ÿè´£ç”Ÿæˆåˆå§‹ä»£ç ï¼Œæ‰§è¡Œæ¨¡å—è¿è¡Œä»£ç ï¼Œä¿®å¤æ¨¡å—å¤„ç†æ‰§è¡Œé”™è¯¯ï¼Œåˆ¤æ–­æ¨¡å—è¯„ä¼°ç”Ÿæˆç»“æœçš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé‡‡ç”¨å¤šä»£ç†è®¾è®¡ï¼Œå…è®¸ä¸åŒæ¨¡å—ä¸“æ³¨äºç‰¹å®šä»»åŠ¡ï¼Œä»è€Œæé«˜äº†æ•´ä½“ç³»ç»Ÿçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¿™ä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹ç”Ÿæˆæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä½¿ç”¨äº†ç°æˆçš„GPT-4o-miniæ¨¡å‹ï¼Œä¼˜åŒ–äº†æ¯ä¸ªæ¨¡å—çš„è¾“å…¥è¾“å‡ºæ ¼å¼ï¼Œå¹¶åœ¨ä¿®å¤é˜¶æ®µå¼•å…¥äº†è¿­ä»£æœºåˆ¶ä»¥é™ä½é”™è¯¯ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„å¤šä»£ç†æ–¹æ³•åœ¨Text2Chart31åŸºå‡†ä¸Šå°†æ‰§è¡Œé”™è¯¯ç‡é™ä½è‡³4.5%ï¼Œåœ¨ChartXåŸºå‡†ä¸Šä¸º4.6%ï¼Œç›¸æ¯”æœ€å¼ºå¾®è°ƒåŸºçº¿æå‡è¿‘5ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”è®¡ç®—èµ„æºéœ€æ±‚æ˜¾è‘—å‡å°‘ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®å¯è§†åŒ–ã€å•†ä¸šæ™ºèƒ½å’Œæ•™è‚²ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å‡†ç¡®åœ°ç”Ÿæˆå›¾è¡¨ï¼Œæé«˜æ•°æ®åˆ†æçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œéšç€å¯¹å›¾è¡¨ç¾å­¦å’Œæ— éšœç¢æ€§çš„å…³æ³¨ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models can translate natural-language chart descriptions into runnable code, yet approximately 15\% of the generated scripts still fail to execute, even after supervised fine-tuning and reinforcement learning. We investigate whether this persistent error rate stems from model limitations or from reliance on a single-prompt design. To explore this, we propose a lightweight multi-agent pipeline that separates drafting, execution, repair, and judgment, using only an off-the-shelf GPT-4o-mini model. On the \textsc{Text2Chart31} benchmark, our system reduces execution errors to 4.5\% within three repair iterations, outperforming the strongest fine-tuned baseline by nearly 5 percentage points while requiring significantly less compute. Similar performance is observed on the \textsc{ChartX} benchmark, with an error rate of 4.6\%, demonstrating strong generalization. Under current benchmarks, execution success appears largely solved. However, manual review reveals that 6 out of 100 sampled charts contain hallucinations, and an LLM-based accessibility audit shows that only 33.3\% (\textsc{Text2Chart31}) and 7.2\% (\textsc{ChartX}) of generated charts satisfy basic colorblindness guidelines. These findings suggest that future work should shift focus from execution reliability toward improving chart aesthetics, semantic fidelity, and accessibility.

