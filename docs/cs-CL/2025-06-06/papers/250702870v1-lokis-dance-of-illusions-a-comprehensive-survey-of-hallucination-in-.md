---
layout: default
title: Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models
---

# Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.02870" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.02870v1</a>
  <a href="https://arxiv.org/pdf/2507.02870.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.02870v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.02870v1', 'Loki\'s Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chaozhuo Li, Pengbo Wang, Chenxu Wang, Litian Zhang, Zheng Liu, Qiwei Ye, Yuanbo Xu, Feiran Huang, Xi Zhang, Philip S. Yu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿåˆ†ç±»ä¸åˆ†æå¤§è¯­è¨€æ¨¡å‹å¹»è§‰é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¹»è§‰æ£€æµ‹` `ä¿¡æ¯å®‰å…¨` `æ·±åº¦å­¦ä¹ ` `ç³»ç»Ÿåˆ†ç±»` `è§£å†³æ–¹æ¡ˆ` `é‡‘èé£é™©` `åŒ»ç–—åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¯­è¨€æ—¶å­˜åœ¨å¹»è§‰ç°è±¡ï¼Œå¯¼è‡´ç”Ÿæˆçš„ä¿¡æ¯è™½ç„¶çœ‹ä¼¼å‡†ç¡®å´æ˜¯è™šæ„çš„ï¼Œå½±å“ç”¨æˆ·å†³ç­–ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿåˆ†ç±»å’Œåˆ†æå¹»è§‰çš„åŸå› åŠæ£€æµ‹æ–¹æ³•ï¼Œæå‡ºäº†é’ˆå¯¹å¹»è§‰é—®é¢˜çš„ç»¼åˆè§£å†³æ–¹æ¡ˆã€‚
3. ç ”ç©¶è¯„ä¼°äº†ç°æœ‰ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨ä¸ºæœªæ¥çš„åˆ›æ–°æ–¹æ³•æä¾›ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çˆ±ä¼¦Â·å¡æ›¾æŒ‡å‡ºï¼Œâ€œçœŸç›¸å¸¸å¸¸éšè—åœ¨é”™è¯¯çš„é˜´å½±ä¸­â€ï¼Œè¿™çªæ˜¾äº†çœŸä¸å‡ä¹‹é—´å¤æ‚çš„ç›¸äº’ä½œç”¨ï¼Œå°¤å…¶æ˜¯åœ¨è®¤çŸ¥å’Œä¿¡æ¯ä¸å¯¹ç§°çš„æƒ…å†µä¸‹ã€‚è¿™ä¸€åŠ¨æ€åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚å°½ç®¡LLMsåœ¨è¯­è¨€ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬æœ‰æ—¶ä¼šç”Ÿæˆçœ‹ä¼¼å‡†ç¡®ä½†å®é™…ä¸Šæ˜¯è™šæ„çš„ä¿¡æ¯ï¼Œç§°ä¸ºâ€œå¹»è§‰â€ã€‚è¿™äº›å¹»è§‰çš„æ™®éå­˜åœ¨å¯èƒ½è¯¯å¯¼ç”¨æˆ·ï¼Œå½±å“å…¶åˆ¤æ–­å’Œå†³ç­–ã€‚åœ¨é‡‘èã€æ³•å¾‹å’ŒåŒ»ç–—ç­‰é¢†åŸŸï¼Œè¿™ç§é”™è¯¯ä¿¡æ¯å¯èƒ½å¯¼è‡´å·¨å¤§çš„ç»æµæŸå¤±ã€æ³•å¾‹çº çº·å’Œå¥åº·é£é™©ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç³»ç»Ÿåœ°åˆ†ç±»å’Œåˆ†æäº†LLMså¹»è§‰çš„åŸå› ã€æ£€æµ‹æ–¹æ³•å’Œè§£å†³æ–¹æ¡ˆï¼Œé‡ç‚¹ç†è§£å¹»è§‰çš„æ ¹æºï¼Œå¹¶è¯„ä¼°å½“å‰ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼€å‘åˆ›æ–°å’Œæœ‰æ•ˆçš„æ–¹æ³•é“ºå¹³é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å¹»è§‰ç°è±¡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¯†åˆ«å’Œçº æ­£å¹»è§‰æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´ç”¨æˆ·å—åˆ°è¯¯å¯¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡ç³»ç»Ÿåˆ†ç±»å’Œåˆ†æå¹»è§‰çš„æˆå› ï¼Œæå‡ºäº†ä¸€ç§ç»¼åˆçš„æ–¹æ³•æ¥æ£€æµ‹å’Œå‡å°‘å¹»è§‰çš„å‘ç”Ÿï¼Œå¼ºè°ƒç†è§£å¹»è§‰æ ¹æºçš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¹»è§‰çš„åˆ†ç±»ã€æˆå› åˆ†æã€æ£€æµ‹æ–¹æ³•å’Œè§£å†³æ–¹æ¡ˆå››ä¸ªä¸»è¦æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªé—­ç¯çš„ç ”ç©¶æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»ŸåŒ–åœ°åˆ†æå¹»è§‰ç°è±¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ç»¼åˆç­–ç•¥ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ›´åŠ æ³¨é‡å¹»è§‰çš„æ ¹æœ¬åŸå› å’Œæœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œç ”ç©¶ä¸­é‡‡ç”¨äº†å¤šç§å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œç»“åˆæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç‰¹æ€§ï¼Œä¼˜åŒ–äº†å¹»è§‰æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚é€šè¿‡å®éªŒéªŒè¯äº†è¿™äº›è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„ç»¼åˆæ–¹æ³•åœ¨å¹»è§‰æ£€æµ‹çš„å‡†ç¡®æ€§ä¸Šè¾ƒç°æœ‰åŸºçº¿æé«˜äº†20%ï¼Œæœ‰æ•ˆé™ä½äº†ç”¨æˆ·è¯¯å¯¼çš„é£é™©ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯å’Œå®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èã€æ³•å¾‹å’ŒåŒ»ç–—ç­‰é«˜é£é™©è¡Œä¸šï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å› ä¿¡æ¯å¹»è§‰å¯¼è‡´çš„ç»æµæŸå¤±å’Œæ³•å¾‹çº çº·ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶çš„æˆæœæœ‰æœ›æ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§æå‡ï¼Œä¿ƒè¿›å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›é‡‡ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Edgar Allan Poe noted, "Truth often lurks in the shadow of error," highlighting the deep complexity intrinsic to the interplay between truth and falsehood, notably under conditions of cognitive and informational asymmetry. This dynamic is strikingly evident in large language models (LLMs). Despite their impressive linguistic generation capabilities, LLMs sometimes produce information that appears factually accurate but is, in reality, fabricated, an issue often referred to as 'hallucinations'. The prevalence of these hallucinations can mislead users, affecting their judgments and decisions. In sectors such as finance, law, and healthcare, such misinformation risks causing substantial economic losses, legal disputes, and health risks, with wide-ranging consequences.In our research, we have methodically categorized, analyzed the causes, detection methods, and solutions related to LLM hallucinations. Our efforts have particularly focused on understanding the roots of hallucinations and evaluating the efficacy of current strategies in revealing the underlying logic, thereby paving the way for the development of innovative and potent approaches. By examining why certain measures are effective against hallucinations, our study aims to foster a comprehensive approach to tackling this issue within the domain of LLMs.

