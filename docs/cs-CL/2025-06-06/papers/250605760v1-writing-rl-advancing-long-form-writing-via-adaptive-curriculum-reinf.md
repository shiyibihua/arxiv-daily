---
layout: default
title: Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning
---

# Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05760" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05760v1</a>
  <a href="https://arxiv.org/pdf/2506.05760.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05760v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05760v1', 'Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuanyu Lei, Chenliang Li, Yuning Wu, Kaiming Liu, Weizhou Shen, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: Work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWriting-RLæ¡†æ¶ä»¥æå‡é•¿ç¯‡å†™ä½œèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ç¯‡å†™ä½œ` `å¼ºåŒ–å­¦ä¹ ` `è‡ªé€‚åº”è¯¾ç¨‹` `æ•°æ®é€‰æ‹©` `å¥–åŠ±æœºåˆ¶` `æ¨¡å‹æ³›åŒ–` `é•¿ä¸Šä¸‹æ–‡è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨é•¿ç¯‡å†™ä½œä¸­é¢ä¸´æ•°æ®é¥±å’Œå’Œå­¦ä¹ èƒ½åŠ›å—é™çš„é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„è¡¨ç°ã€‚
2. æœ¬æ–‡æå‡ºWriting-RLæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ï¼Œä¼˜åŒ–é•¿ç¯‡å†™ä½œèƒ½åŠ›ï¼ŒåŒ…å«æ•°æ®é€‰æ‹©ã€å¥–åŠ±æœºåˆ¶å’Œä»»åŠ¡è°ƒåº¦ç­‰æ ¸å¿ƒæ€æƒ³ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¯¥æ¡†æ¶çš„æ¨¡å‹åœ¨é•¿ç¯‡å†™ä½œä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šä¼ ç»ŸSFTåŸºçº¿ï¼Œå¹¶åœ¨é•¿è¾“å…¥æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—é•¿ç¯‡å†™ä½œè¡¨ç°æ˜¾è‘—æå‡ï¼Œä½†ç°æœ‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•å­˜åœ¨æ•°æ®é¥±å’Œå’Œå­¦ä¹ èƒ½åŠ›å—é™ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†Writing-RLï¼šä¸€ç§è‡ªé€‚åº”è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥è¶…è¶ŠSFTæå‡é•¿ç¯‡å†™ä½œèƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå…³æ³¨è¾¹é™…çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œä¼˜å…ˆé€‰æ‹©å…·æœ‰é«˜å­¦ä¹ æ½œåŠ›çš„æ ·æœ¬ï¼›æˆå¯¹æ¯”è¾ƒå¥–åŠ±æœºåˆ¶ï¼Œåœ¨ç¼ºä¹å¯éªŒè¯å¥–åŠ±çš„æƒ…å†µä¸‹æä¾›åŒºåˆ†æ€§å­¦ä¹ ä¿¡å·ï¼›åŠ¨æ€å‚è€ƒè°ƒåº¦æ–¹æ³•ï¼Œæ ¹æ®æ¨¡å‹æ€§èƒ½çš„å˜åŒ–è‡ªé€‚åº”è°ƒæ•´ä»»åŠ¡éš¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäº7Bè§„æ¨¡å†™ä½œæ¨¡å‹çš„RLæ¡†æ¶åœ¨é•¿ç¯‡å†™ä½œæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿SFTã€‚æ­¤å¤–ï¼Œä½¿ç”¨é•¿è¾“å‡ºRLè®­ç»ƒçš„æ¨¡å‹åœ¨é•¿è¾“å…¥æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯èƒ½ä¸ºé‡æ–°æ€è€ƒé•¿ä¸Šä¸‹æ–‡è®­ç»ƒæä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰é•¿ç¯‡å†™ä½œæ¨¡å‹åœ¨ç›‘ç£å¾®è°ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„æ•°æ®é¥±å’Œå’Œå­¦ä¹ èƒ½åŠ›å—é™çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¯¼è‡´æ¨¡å‹åœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ—¶çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºWriting-RLæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ï¼Œä¼˜åŒ–é•¿ç¯‡å†™ä½œèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€è°ƒæ•´å­¦ä¹ ä»»åŠ¡çš„éš¾åº¦å’Œå¥–åŠ±æœºåˆ¶ï¼Œæå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) è¾¹é™…æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œä¼˜å…ˆé€‰æ‹©é«˜å­¦ä¹ æ½œåŠ›æ ·æœ¬ï¼›2) æˆå¯¹æ¯”è¾ƒå¥–åŠ±æœºåˆ¶ï¼Œæä¾›åŒºåˆ†æ€§å­¦ä¹ ä¿¡å·ï¼›3) åŠ¨æ€å‚è€ƒè°ƒåº¦æ–¹æ³•ï¼Œæ ¹æ®æ¨¡å‹æ€§èƒ½å˜åŒ–è‡ªé€‚åº”è°ƒæ•´ä»»åŠ¡éš¾åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†åŠ¨æ€å‚è€ƒè°ƒåº¦å’Œæˆå¯¹æ¯”è¾ƒå¥–åŠ±æœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…é€šå¸¸ä¾èµ–å›ºå®šçš„å¥–åŠ±ä¿¡å·ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é€‰æ‹©ä¸­ï¼Œé‡‡ç”¨è¾¹é™…æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹æ¥è§¦åˆ°å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼›åœ¨å¥–åŠ±æœºåˆ¶ä¸­ï¼Œè®¾è®¡æˆå¯¹æ¯”è¾ƒå¥–åŠ±ï¼Œä»¥åœ¨ç¼ºä¹æ˜ç¡®å¥–åŠ±çš„æƒ…å†µä¸‹ä»èƒ½æä¾›æœ‰æ•ˆçš„å­¦ä¹ ä¿¡å·ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Writing-RLæ¡†æ¶çš„æ¨¡å‹åœ¨é•¿ç¯‡å†™ä½œä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„SFTåŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹åœ¨é•¿è¾“å…¥æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¡¨æ˜å…¶è®­ç»ƒæ•ˆæœçš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€å†…å®¹åˆ›ä½œå’Œè‡ªåŠ¨åŒ–å†™ä½œç­‰ã€‚é€šè¿‡æå‡é•¿ç¯‡å†™ä½œèƒ½åŠ›ï¼ŒWriting-RLæ¡†æ¶å¯ä»¥å¸®åŠ©ç”¨æˆ·ç”Ÿæˆæ›´é«˜è´¨é‡çš„æ–‡æœ¬ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯ä¸‹çš„å†™ä½œéœ€æ±‚ï¼Œæœªæ¥å¯èƒ½å¯¹å†…å®¹ç”Ÿæˆè¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Large Language Models (LLMs) have enabled strong performance in long-form writing, yet existing supervised fine-tuning (SFT) approaches suffer from limitations such as data saturation and restricted learning capacity bounded by teacher signals. In this work, we present Writing-RL: an Adaptive Curriculum Reinforcement Learning framework to advance long-form writing capabilities beyond SFT. The framework consists of three key components: Margin-aware Data Selection strategy that prioritizes samples with high learning potential, Pairwise Comparison Reward mechanism that provides discriminative learning signals in the absence of verifiable rewards, and Dynamic Reference Scheduling approach, which plays a particularly critical role by adaptively adjusting task difficulty based on evolving model performance. Experiments on 7B-scale writer models show that our RL framework largely improves long-form writing performance over strong SFT baselines. Furthermore, we observe that models trained with long-output RL generalize surprisingly well to long-input reasoning tasks, potentially offering a promising perspective for rethinking long-context training.

