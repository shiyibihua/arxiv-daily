---
layout: default
title: Is There a Case for Conversation Optimized Tokenizers in Large Language Models?
---

# Is There a Case for Conversation Optimized Tokenizers in Large Language Models?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18674" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18674v1</a>
  <a href="https://arxiv.org/pdf/2506.18674.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18674v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18674v1', 'Is There a Case for Conversation Optimized Tokenizers in Large Language Models?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Raquel Ferrando, Javier Conde, Gonzalo MartÃ­nez, Pedro Reviriego

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆ†è¯å™¨ä¼˜åŒ–` `èŠå¤©æœºå™¨äºº` `è‡ªç„¶è¯­è¨€å¤„ç†` `èƒ½é‡èŠ‚çœ` `å¯¹è¯ç³»ç»Ÿ` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åˆ†è¯å™¨ä¸»è¦é’ˆå¯¹è®­ç»ƒè¯­æ–™åº“è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæœªè€ƒè™‘èŠå¤©æœºå™¨äººç­‰åº”ç”¨åœºæ™¯çš„ç‰¹å®šéœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹èŠå¤©å¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨ï¼Œé€šè¿‡é‡æ–°è®¾è®¡è¯æ±‡æ¥æé«˜åœ¨ç”¨æˆ·è¾“å…¥å’Œå“åº”ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨èƒ½å¤Ÿå‡å°‘5%åˆ°10%çš„tokenæ•°é‡ï¼Œå¸¦æ¥æ˜¾è‘—çš„èƒ½é‡èŠ‚çœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®¡ç®—å’Œèƒ½è€—æˆæœ¬å› æ¨¡å‹è§„æ¨¡çš„å¢é•¿å’Œç”¨æˆ·çš„å¹¿æ³›é‡‡ç”¨è€Œå‘ˆæŒ‡æ•°çº§ä¸Šå‡ã€‚LLMçš„å•ä½æˆæœ¬æ˜¯è®¡ç®—ä¸€ä¸ªtokenï¼Œå› æ­¤åˆ†è¯å™¨åœ¨æ¨¡å‹æ•ˆç‡ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç°æœ‰çš„åˆ†è¯å™¨ä¸»è¦é’ˆå¯¹è®­ç»ƒè¯­æ–™åº“è¿›è¡Œäº†ä¼˜åŒ–ï¼Œè€Œå¯¹äºèŠå¤©æœºå™¨äººç­‰åº”ç”¨åœºæ™¯ï¼Œç”¨æˆ·è¾“å…¥å’ŒèŠå¤©æœºå™¨äººå“åº”çš„æ–‡æœ¬å¯èƒ½ä¸è®­ç»ƒè¯­æ–™åº“ä¸åŒã€‚æœ¬æ–‡æ¢è®¨äº†é’ˆå¯¹èŠå¤©å¯¹è¯ä¼˜åŒ–åˆ†è¯å™¨çš„æ½œåœ¨ç›Šå¤„ï¼Œé€šè¿‡ä½¿ç”¨å…¬å¼€çš„èŠå¤©å¯¹è¯è¯­æ–™åº“é‡æ–°è®¾è®¡åˆ†è¯å™¨çš„è¯æ±‡ï¼Œå¹¶è¯„ä¼°å…¶åœ¨è¯¥é¢†åŸŸçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œå¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨åœ¨èŠå¤©å¯¹è¯ä¸­èƒ½å¤ŸæŒç»­å‡å°‘tokenæ•°é‡ï¼Œä»è€Œå®ç°5%åˆ°10%çš„èƒ½é‡èŠ‚çœï¼ŒåŒæ—¶å¯¹åŸè®­ç»ƒè¯­æ–™çš„tokenåŒ–æ•ˆç‡å½±å“è¾ƒå°æˆ–ç•¥æœ‰æ­£é¢å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åˆ†è¯å™¨åœ¨èŠå¤©æœºå™¨äººåº”ç”¨ä¸­æ•ˆç‡ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½è€ƒè™‘ç”¨æˆ·è¾“å…¥å’Œå“åº”çš„ç‰¹å®šæ–‡æœ¬ç‰¹å¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡åˆ†æèŠå¤©å¯¹è¯è¯­æ–™åº“ï¼Œé‡æ–°è®¾è®¡åˆ†è¯å™¨çš„è¯æ±‡ï¼Œä»¥ä¼˜åŒ–å…¶åœ¨å¯¹è¯åœºæ™¯ä¸­çš„è¡¨ç°ï¼Œæå‡tokenåŒ–æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆæ”¶é›†å…¬å¼€çš„èŠå¤©å¯¹è¯è¯­æ–™åº“ï¼Œç„¶åå¯¹æ¯”ä¸åŒåˆ†è¯å™¨åœ¨è¯¥è¯­æ–™åº“ä¸Šçš„è¡¨ç°ï¼Œæœ€åè¯„ä¼°ä¼˜åŒ–åçš„åˆ†è¯å™¨åœ¨èƒ½è€—å’Œæ•ˆç‡ä¸Šçš„æ”¹è¿›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨è®¾è®¡æ€è·¯ï¼Œé’ˆå¯¹èŠå¤©åœºæ™¯çš„ç‰¹å®šéœ€æ±‚è¿›è¡Œä¼˜åŒ–ï¼Œä¸ä¼ ç»Ÿåˆ†è¯å™¨çš„è®¾è®¡æ€è·¯å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œç ”ç©¶è€…å¯¹è¯æ±‡è¿›è¡Œäº†é‡æ–°æ„å»ºï¼Œå…³æ³¨tokenæ•°é‡çš„å‡å°‘ï¼ŒåŒæ—¶ä¿æŒå¯¹åŸè®­ç»ƒè¯­æ–™çš„tokenåŒ–æ•ˆç‡ï¼Œç¡®ä¿ä¼˜åŒ–çš„åˆ†è¯å™¨åœ¨ä¸åŒåœºæ™¯ä¸‹å‡èƒ½æœ‰æ•ˆè¿ä½œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨åœ¨èŠå¤©å¯¹è¯ä¸­èƒ½å¤Ÿå‡å°‘tokenæ•°é‡5%åˆ°10%ï¼Œå®ç°æ˜¾è‘—çš„èƒ½é‡èŠ‚çœï¼ŒåŒæ—¶å¯¹åŸè®­ç»ƒè¯­æ–™çš„tokenåŒ–æ•ˆç‡å½±å“è¾ƒå°æˆ–ç•¥æœ‰æ­£é¢å½±å“ã€‚è¿™ä¸€å‘ç°ä¸ºåˆ†è¯å™¨çš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’ï¼Œå…·æœ‰é‡è¦çš„å®é™…æ„ä¹‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬èŠå¤©æœºå™¨äººã€æ™ºèƒ½å®¢æœç³»ç»Ÿå’Œå…¶ä»–éœ€è¦è‡ªç„¶è¯­è¨€å¤„ç†çš„äº¤äº’å¼åº”ç”¨ã€‚é€šè¿‡ä¼˜åŒ–åˆ†è¯å™¨ï¼Œå¯ä»¥æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œèƒ½è€—ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿçš„å¯æŒç»­æ€§ã€‚æœªæ¥ï¼Œè¿™ä¸€ç ”ç©¶æˆæœå¯èƒ½æ¨åŠ¨æ›´å¤šé’ˆå¯¹ç‰¹å®šåº”ç”¨åœºæ™¯çš„åˆ†è¯å™¨è®¾è®¡ï¼Œä¿ƒè¿›è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.

