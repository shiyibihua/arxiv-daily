---
layout: default
title: LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning
---

# LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18841" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18841v1</a>
  <a href="https://arxiv.org/pdf/2506.18841.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18841v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18841v1', 'LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/THU-KEG/LongWriter-Zero-32B)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLongWriter-Zeroä»¥è§£å†³è¶…é•¿æ–‡æœ¬ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¶…é•¿æ–‡æœ¬ç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ–‡æœ¬è´¨é‡ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¦‚LongWriterä¾èµ–åˆæˆæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå¯¼è‡´ç”Ÿæˆæ–‡æœ¬ç¼ºä¹ä¸€è‡´æ€§å’Œè‡ªç„¶æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¿€åŠ±æ–¹æ³•ï¼Œä»é›¶å¼€å§‹è®­ç»ƒLLMsï¼Œæ— éœ€åˆæˆæˆ–æ ‡æ³¨æ•°æ®ã€‚
3. LongWriter-Zeroåœ¨é•¿æ–‡æœ¬å†™ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šä¼ ç»Ÿæ–¹æ³•å’Œå¤§å‹æ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æŒ‡æ ‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¶…é•¿æ–‡æœ¬ç”Ÿæˆæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´çš„é‡è¦æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¦‚LongWriterä¾èµ–äºåˆæˆæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå­˜åœ¨æ•°æ®æ„å»ºå›°éš¾ã€ç¼ºä¹ä¸€è‡´æ€§å’Œè‡ªç„¶æ€§ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¿€åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»é›¶å¼€å§‹è®­ç»ƒLLMsï¼Œæ—¨åœ¨æå‡è¶…é•¿æ–‡æœ¬ç”Ÿæˆçš„è´¨é‡å’Œæ§åˆ¶èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongWriter-Zeroåœ¨é•¿æ–‡æœ¬å†™ä½œä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œè¾¾åˆ°äº†WritingBenchå’ŒArena-Writeä¸Šçš„æœ€æ–°æˆæœï¼Œç”šè‡³è¶…è¶Šäº†100B+æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¶…é•¿æ–‡æœ¬ç”Ÿæˆä¸­çš„è´¨é‡ä¸‹é™å’Œç”Ÿæˆé•¿åº¦é™åˆ¶é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºåˆæˆæ•°æ®ï¼Œå¯¼è‡´ç”Ÿæˆæ–‡æœ¬çš„è‡ªç„¶æ€§å’Œä¸€è‡´æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¿€åŠ±æœºåˆ¶ï¼Œä»é›¶å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¥–åŠ±æœºåˆ¶å¼•å¯¼æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„è¶…é•¿æ–‡æœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæµç¨‹ï¼Œä½¿ç”¨ä¸“é—¨çš„å¥–åŠ±æ¨¡å‹æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œç¡®ä¿æ–‡æœ¬é•¿åº¦æ§åˆ¶ã€å†™ä½œè´¨é‡å’Œç»“æ„æ ¼å¼çš„ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå®Œå…¨ä¸ä¾èµ–åˆæˆæˆ–æ ‡æ³¨æ•°æ®ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›çš„è‡ªæˆ‘æå‡ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ï¼Œå¹¶è®¾è®¡äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹çš„ç”Ÿæˆç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLongWriter-Zeroåœ¨WritingBenchå’ŒArena-Writeçš„é•¿æ–‡æœ¬å†™ä½œä»»åŠ¡ä¸­ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æŒ‡æ ‡ï¼Œä¸”åœ¨ä¸100B+æ¨¡å‹å¦‚DeepSeek R1å’ŒQwen3-235Bçš„å¯¹æ¯”ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æå‡å¹…åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†…å®¹åˆ›ä½œã€è‡ªåŠ¨åŒ–å†™ä½œã€é•¿ç¯‡æŠ¥å‘Šç”Ÿæˆç­‰ï¼Œèƒ½å¤Ÿä¸ºæ•™è‚²ã€åª’ä½“å’Œå•†ä¸šç­‰è¡Œä¸šæä¾›é«˜æ•ˆçš„æ–‡æœ¬ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨æ›´è‡ªç„¶å’Œè¿è´¯çš„é•¿æ–‡æœ¬ç”Ÿæˆï¼Œæå‡äººæœºäº¤äº’çš„è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B

