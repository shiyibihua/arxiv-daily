---
layout: default
title: USAD: Universal Speech and Audio Representation via Distillation
---

# USAD: Universal Speech and Audio Representation via Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18843" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18843v2</a>
  <a href="https://arxiv.org/pdf/2506.18843.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18843v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18843v2', 'USAD: Universal Speech and Audio Representation via Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Heng-Jui Chang, Saurabhchand Bhati, James Glass, Alexander H. Liu

**åˆ†ç±»**: cs.SD, cs.CL, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23 (æ›´æ–°: 2025-08-18)

**å¤‡æ³¨**: Accepted to ASRU 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUSADä»¥è§£å†³éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ çš„é¢†åŸŸç‰¹å®šé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `è’¸é¦è®­ç»ƒ` `å¤šæ¨¡æ€èåˆ` `è¯­éŸ³å¤„ç†` `éŸ³é¢‘åˆ†ç±»` `ç»Ÿä¸€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ–¹æ³•å¾€å¾€å±€é™äºç‰¹å®šé¢†åŸŸï¼Œç¼ºä¹ç»Ÿä¸€çš„æ¨¡å‹æ¥å¤„ç†å¤šç§éŸ³é¢‘ç±»å‹ã€‚
2. USADé€šè¿‡å±‚é—´è’¸é¦æŠ€æœ¯ï¼Œå°†ä¸åŒé¢†åŸŸçš„è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹æ•´åˆä¸ºä¸€ä¸ªé€šç”¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ¨¡å‹ã€‚
3. USADåœ¨å¤šä¸ªéŸ³é¢‘å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨SUPERBå’ŒHEARåŸºå‡†ä¸Šæ¥è¿‘æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨éŸ³é¢‘è¡¨ç¤ºé¢†åŸŸå–å¾—äº†é©å‘½æ€§è¿›å±•ï¼Œä½†ç°æœ‰æ¨¡å‹å¾€å¾€å±€é™äºç‰¹å®šé¢†åŸŸï¼Œä¸“æ³¨äºè¯­éŸ³æˆ–éè¯­éŸ³ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†é€šç”¨è¯­éŸ³ä¸éŸ³é¢‘è’¸é¦ï¼ˆUSADï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ç­‰å¤šç§éŸ³é¢‘ç±»å‹æ•´åˆåˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ã€‚USADé€šè¿‡ä»é¢†åŸŸç‰¹å®šçš„SSLæ¨¡å‹è¿›è¡Œé«˜æ•ˆçš„å±‚é—´è’¸é¦ï¼Œè®­ç»ƒä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ï¼Œä½¿ç”¨å…¨é¢çš„éŸ³é¢‘æ•°æ®é›†ã€‚USADåœ¨å¤šä¸ªåŸºå‡†å’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¸§çº§å’Œå®ä¾‹çº§è¯­éŸ³å¤„ç†ä»»åŠ¡ã€éŸ³é¢‘æ ‡è®°å’Œå£°éŸ³åˆ†ç±»ï¼Œåœ¨SUPERBå’ŒHEARåŸºå‡†ä¸Šå®ç°äº†æ¥è¿‘æœ€å…ˆè¿›çš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ¨¡å‹é€šå¸¸ä¸“æ³¨äºç‰¹å®šçš„ä»»åŠ¡ï¼Œå¦‚è¯­éŸ³æˆ–éŸ³ä¹ï¼Œå¯¼è‡´æ¨¡å‹çš„é€šç”¨æ€§ä¸è¶³ï¼Œéš¾ä»¥å¤„ç†å¤šç§éŸ³é¢‘ç±»å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUSADé€šè¿‡å±‚é—´è’¸é¦çš„æ–¹å¼ï¼Œå°†å¤šä¸ªé¢†åŸŸçš„è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„çŸ¥è¯†æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ä¸­ï¼Œä»è€Œå®ç°å¯¹å¤šç§éŸ³é¢‘ç±»å‹çš„æœ‰æ•ˆè¡¨ç¤ºå­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUSADçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹å’Œå¤šä¸ªæ•™å¸ˆæ¨¡å‹ï¼Œæ•™å¸ˆæ¨¡å‹æ¥è‡ªä¸åŒçš„é¢†åŸŸç‰¹å®šSSLæ¨¡å‹ã€‚é€šè¿‡å±‚é—´è’¸é¦ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨ä¸€ä¸ªç»¼åˆéŸ³é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šUSADçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å±‚é—´è’¸é¦ç­–ç•¥ï¼Œä½¿å¾—ä¸åŒé¢†åŸŸçš„çŸ¥è¯†èƒ½å¤Ÿæœ‰æ•ˆä¼ é€’ï¼Œå…‹æœäº†ä¼ ç»Ÿæ¨¡å‹çš„é¢†åŸŸé™åˆ¶ï¼Œæå‡äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒUSADé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è’¸é¦è¿‡ç¨‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”å¤šç§éŸ³é¢‘ç±»å‹çš„ç‰¹å¾æå–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

USADåœ¨å¤šä¸ªéŸ³é¢‘å¤„ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨SUPERBå’ŒHEARåŸºå‡†ä¸Šï¼Œä½¿ç”¨å•ä¸€ç¼–ç å™¨å®ç°äº†æ¥è¿‘æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜USADåœ¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œç«äº‰åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¸§çº§å’Œå®ä¾‹çº§è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

USADçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿåœ¨è¯­éŸ³è¯†åˆ«ã€éŸ³é¢‘åˆ†ç±»ã€éŸ³ä¹æ¨èç­‰å¤šä¸ªé¢†åŸŸå‘æŒ¥ä½œç”¨ã€‚é€šè¿‡æä¾›ä¸€ä¸ªç»Ÿä¸€çš„éŸ³é¢‘è¡¨ç¤ºæ¨¡å‹ï¼ŒUSADå¯ä»¥é™ä½æ¨¡å‹å¼€å‘çš„å¤æ‚æ€§ï¼Œæé«˜å¤šä»»åŠ¡å­¦ä¹ çš„æ•ˆç‡ï¼Œæ¨åŠ¨éŸ³é¢‘å¤„ç†æŠ€æœ¯çš„è¿›æ­¥ã€‚æœªæ¥ï¼ŒUSADå¯èƒ½ä¼šåœ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨éŸ³é¢‘æ ‡è®°å’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks.

