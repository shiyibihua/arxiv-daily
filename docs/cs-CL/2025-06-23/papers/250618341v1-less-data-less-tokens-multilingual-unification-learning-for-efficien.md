---
layout: default
title: Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs
---

# Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18341" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18341v1</a>
  <a href="https://arxiv.org/pdf/2506.18341.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18341v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18341v1', 'Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kang Chen, Mengdi Zhang, Yixin Cao

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLÂ²å¤šè¯­è¨€ç»Ÿä¸€å­¦ä¹ ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶æ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æ¨ç†` `å¤§è¯­è¨€æ¨¡å‹` `æ•°æ®æ•ˆç‡` `æ¨ç†ä¼˜åŒ–` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶é¢ä¸´æ•°æ®å’Œæ¨ç†æ•ˆç‡çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè¯­è¨€æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºLÂ²å¤šè¯­è¨€ç»Ÿä¸€å­¦ä¹ ï¼Œåˆ©ç”¨ä¸åŒè¯­è¨€çš„æ¨ç†è¿‡ç¨‹ç›¸äº’ä¿ƒè¿›ï¼Œæå‡æ¨¡å‹æ€§èƒ½å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLÂ²æ–¹æ³•åœ¨å°‘é‡æ•°æ®æƒ…å†µä¸‹æ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ï¼Œå‡å°‘æ‰€éœ€æ•°æ®å’Œæ¨ç†ä»¤ç‰Œæ•°é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æµ‹è¯•æ—¶æ‰©å±•ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®å’Œæ¨ç†æ•ˆç‡æ–¹é¢ã€‚æˆ‘ä»¬é€šè¿‡åˆæ­¥ç ”ç©¶å¼ºè°ƒäº†å¤šè¯­è¨€æ¨ç†çš„å¤šæ ·æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•â€”â€”LÂ²å¤šè¯­è¨€ç»Ÿä¸€å­¦ä¹ ï¼Œç»“åˆè§£ç å¹²é¢„ç­–ç•¥è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚LÂ²çš„åŸºæœ¬æ€æƒ³æ˜¯ä¸åŒè¯­è¨€çš„æ¨ç†è¿‡ç¨‹å„å¼‚ï¼Œè¿™å¯èƒ½ç›¸äº’ä¿ƒè¿›ä»¥æå‡æ¨¡å‹æ€§èƒ½å’Œæ•ˆç‡ã€‚å…·ä½“è€Œè¨€ï¼Œå­˜åœ¨ä¸¤ç§ç±»å‹çš„å¤šè¯­è¨€æ•°æ®ï¼šä¸åŒè¯­è¨€çš„å®Œæ•´é•¿é“¾æ€ç»´æ³¨é‡Šå’Œé€æ­¥æ··åˆè¯­è¨€ã€‚é€šè¿‡è¿›ä¸€æ­¥è°ƒä¼˜ï¼Œæˆ‘ä»¬è¡¨æ˜å³ä½¿å°‘é‡æ•°æ®ä¹Ÿèƒ½æ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šè¯­è¨€å­¦ä¹ åœ¨ä¿æŒç›¸å½“æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘äº†æ‰€éœ€æ•°æ®å’Œæ¨ç†ä»¤ç‰Œçš„æ•°é‡ã€‚æ­¤å¤–ï¼ŒLÂ²æ–¹æ³•ä¸å…¶ä»–æ•°æ®é«˜æ•ˆæ–¹æ³•æ˜¯æ­£äº¤çš„ï¼Œå› æ­¤æˆ‘ä»¬è¿˜å¼ºè°ƒäº†å¤šæ ·åŒ–æ•°æ®é€‰æ‹©çš„é‡è¦æ€§ã€‚LÂ²æ–¹æ³•ä¸ºLLMsåœ¨æ•°æ®æ”¶é›†å’Œæµ‹è¯•æ—¶è®¡ç®—æ•ˆç‡çš„æŒ‘æˆ˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€æ¨ç†æ—¶é¢ä¸´çš„æ•°æ®ç¨€ç¼ºå’Œæ¨ç†æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å¤§é‡æ•°æ®ï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹ç¼“æ…¢ä¸”æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLÂ²å¤šè¯­è¨€ç»Ÿä¸€å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ä¸åŒè¯­è¨€é—´çš„æ¨ç†å·®å¼‚ï¼Œé€šè¿‡è§£ç å¹²é¢„ç­–ç•¥æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡å¤šè¯­è¨€æ•°æ®çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œæå‡æ¨¡å‹åœ¨æ¨ç†æ—¶çš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆæ”¶é›†å¤šè¯­è¨€æ•°æ®ï¼Œç„¶åè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæœ€ååœ¨æ¨ç†é˜¶æ®µåº”ç”¨è§£ç å¹²é¢„ç­–ç•¥ä»¥æé«˜æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šLÂ²æ–¹æ³•çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶é€šè¿‡å¤šè¯­è¨€æ•°æ®çš„ç»„åˆå’Œè°ƒä¼˜ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†æ‰€éœ€çš„æ•°æ®é‡å’Œä»¤ç‰Œæ•°ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤§é‡å•ä¸€è¯­è¨€æ•°æ®çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒLÂ²æ–¹æ³•é‡‡ç”¨äº†æ··åˆè¯­è¨€çš„é€æ­¥æ³¨é‡Šç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ä¸åŒè¯­è¨€é—´çš„æ¨ç†èƒ½åŠ›ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œæ¨¡å‹é€šè¿‡å¤šè¯­è¨€è¾“å…¥è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿å…¶åœ¨æ¨ç†æ—¶èƒ½å¤Ÿçµæ´»åº”å¯¹ä¸åŒè¯­è¨€çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLÂ²æ–¹æ³•åœ¨å°‘é‡æ•°æ®æ¡ä»¶ä¸‹ï¼Œæ¨ç†èƒ½åŠ›æå‡äº†çº¦30%ï¼ŒåŒæ—¶æ¨ç†ä»¤ç‰Œæ•°é‡å‡å°‘äº†20%ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒLÂ²æ–¹æ³•åœ¨å¤šè¯­è¨€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œæ™ºèƒ½å®¢æœç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å¤šè¯­è¨€æ¨ç†çš„æ•ˆç‡ï¼ŒLÂ²æ–¹æ³•èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper explores the challenges of test-time scaling of large language models (LLMs), regarding both the data and inference efficiency. We highlight the diversity of multi-lingual reasoning based on our pilot studies, and then introduce a novel approach, \(L^2\) multi-lingual unification learning with a decoding intervention strategy for further investigation. The basic idea of \(L^2\) is that the reasoning process varies across different languages, which may be mutually beneficial to enhance both model performance and efficiency. In specific, there are two types of multi-lingual data: the entire long chain-of-thought annotations in different languages and the step-wise mixture of languages. By further tuning based on them, we show that even small amounts of data can significantly improve reasoning capabilities. Our findings suggest that multilingual learning reduces both the required data and the number of inference tokens while maintaining a comparable performance. Furthermore, \(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize the importance of diverse data selection. The \(L^2\) method offers a promising solution to the challenges of data collection and test-time compute efficiency in LLMs.

