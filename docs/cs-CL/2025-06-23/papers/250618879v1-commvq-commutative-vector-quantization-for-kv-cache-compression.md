---
layout: default
title: CommVQ: Commutative Vector Quantization for KV Cache Compression
---

# CommVQ: Commutative Vector Quantization for KV Cache Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18879" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.18879v1</a>
  <a href="https://arxiv.org/pdf/2506.18879.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18879v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18879v1', 'CommVQ: Commutative Vector Quantization for KV Cache Compression')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-23

**Â§áÊ≥®**: ICML 2025 poster

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/UMass-Embodied-AGI/CommVQ)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CommVQ‰ª•Ëß£ÂÜ≥Èïø‰∏ä‰∏ãÊñáLLMÊé®ÁêÜ‰∏≠ÁöÑKVÁºìÂ≠òÁì∂È¢àÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Èïø‰∏ä‰∏ãÊñáÊé®ÁêÜ` `ÂêëÈáèÈáèÂåñ` `ÂÜÖÂ≠ò‰ºòÂåñ` `Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÈöèÁùÄ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑÂ¢ûÂä†ÔºåÁé∞ÊúâÁöÑKVÁºìÂ≠òÊñπÊ≥ïÂú®GPU‰∏äÈù¢‰∏¥ÂÜÖÂ≠òÁì∂È¢àÔºåÈôêÂà∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜÂèØ‰∫§Êç¢ÂêëÈáèÈáèÂåñÔºàCommVQÔºâÔºåÈÄöËøáÂä†Ê≥ïÈáèÂåñÂíåËΩªÈáèÁ∫ßÁºñÁ†ÅÂô®ÂéãÁº©KVÁºìÂ≠òÔºå‰ºòÂåñËß£Á†ÅËøáÁ®ã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCommVQÂú®2‰ΩçÈáèÂåñ‰∏ãÂ∞ÜFP16 KVÁºìÂ≠òÂ§ßÂ∞èÂáèÂ∞ë87.5%ÔºåÂπ∂ÂÆûÁé∞‰∫Ü1‰ΩçÈáèÂåñÁöÑ‰ΩéÁ≤æÂ∫¶ÊçüÂ§±ÔºåÊîØÊåÅÊõ¥Èïø‰∏ä‰∏ãÊñáÁöÑÊé®ÁêÜ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÈúÄË¶ÅÈïø‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑÂ∫îÁî®‰∏≠Êó•ÁõäÊôÆÂèäÔºå‰ΩÜÈöèÁùÄ‰∏ä‰∏ãÊñáÁöÑÂ¢ûÈïøÔºåÈîÆÂÄºÔºàKVÔºâÁºìÂ≠òÂ∏∏Â∏∏Êàê‰∏∫GPU‰∏äÁöÑÂÜÖÂ≠òÁì∂È¢à„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÂèØ‰∫§Êç¢ÂêëÈáèÈáèÂåñÔºàCommVQÔºâÔºåÊòæËëóÂáèÂ∞ëÈïø‰∏ä‰∏ãÊñáLLMÊé®ÁêÜÁöÑÂÜÖÂ≠ò‰ΩøÁî®„ÄÇÊàë‰ª¨È¶ñÂÖàÂºïÂÖ•‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁºñÁ†ÅÂô®Âíå‰ª£Á†ÅÊú¨ÁöÑÂä†Ê≥ïÈáèÂåñÊñπÊ≥ïÊù•ÂéãÁº©KVÁºìÂ≠òÔºåËß£Á†ÅËøáÁ®ãÈÄöËøáÁÆÄÂçïÁöÑÁü©Èòµ‰πòÊ≥ïÂÆûÁé∞„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•Èôç‰ΩéËß£Á†ÅËøáÁ®ã‰∏≠ÁöÑËÆ°ÁÆóÊàêÊú¨ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏éÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÂèØ‰∫§Êç¢ÁöÑ‰ª£Á†ÅÊú¨ÔºåÂπ∂‰ΩøÁî®ÊúüÊúõÊúÄÂ§ßÂåñÔºàEMÔºâÁÆóÊ≥ïËøõË°åËÆ≠ÁªÉ„ÄÇËøô‰ΩøÂæóËß£Á†ÅËÉΩÂ§üÈ´òÊïàÂú∞ÈõÜÊàêÂà∞Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏≠„ÄÇÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®2‰ΩçÈáèÂåñ‰∏ãÂ∞ÜFP16 KVÁºìÂ≠òÂ§ßÂ∞èÂáèÂ∞ë‰∫Ü87.5%ÔºåÂπ∂Âú®Èïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÂíåGSM8K‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑKVÁºìÂ≠òÈáèÂåñÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Èïø‰∏ä‰∏ãÊñáÊé®ÁêÜ‰∏≠KVÁºìÂ≠òÁöÑÂÜÖÂ≠òÁì∂È¢àÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®‰∏ä‰∏ãÊñáÂ¢ûÈïøÊó∂Êó†Ê≥ïÊúâÊïàÁÆ°ÁêÜÂÜÖÂ≠òÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÂèØ‰∫§Êç¢ÂêëÈáèÈáèÂåñÔºàCommVQÔºâÔºåÈÄöËøáÂä†Ê≥ïÈáèÂåñÂíåËΩªÈáèÁ∫ßÁºñÁ†ÅÂô®ÂéãÁº©KVÁºìÂ≠òÔºåÂπ∂ËÆæËÆ°‰∏éRoPEÂèØ‰∫§Êç¢ÁöÑ‰ª£Á†ÅÊú¨Ôºå‰ºòÂåñËß£Á†ÅËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Âä†Ê≥ïÈáèÂåñÊ®°Âùó„ÄÅËΩªÈáèÁ∫ßÁºñÁ†ÅÂô®„ÄÅRoPEÂèØ‰∫§Êç¢‰ª£Á†ÅÊú¨ÂíåËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÈõÜÊàê„ÄÇËß£Á†ÅËøáÁ®ãÈÄöËøáÁÆÄÂçïÁöÑÁü©Èòµ‰πòÊ≥ïÂÆûÁé∞ÔºåÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCommVQÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫Ü‰∏éRoPEÂèØ‰∫§Êç¢ÁöÑ‰ª£Á†ÅÊú¨ËÆæËÆ°Ôºå‰ΩøÂæóËß£Á†ÅËøáÁ®ãËÉΩÂ§üÈ´òÊïàÈõÜÊàêÂà∞Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÔºåÊòæËëóÊèêÂçá‰∫ÜÂÜÖÂ≠ò‰ΩøÁî®ÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜËΩªÈáèÁ∫ßÁºñÁ†ÅÂô®ÂíåEMÁÆóÊ≥ïËÆ≠ÁªÉ‰ª£Á†ÅÊú¨ÔºåÁ°Æ‰øù‰∫ÜÈ´òÂáÜÁ°ÆÁéáÂíå‰ΩéËÆ°ÁÆóÂºÄÈîÄ„ÄÇÈáèÂåñËøáÁ®ã‰∏≠ÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰πüÁªèËøáÁ≤æÂøÉË∞ÉÊï¥Ôºå‰ª•ÂÆûÁé∞ÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCommVQÂú®2‰ΩçÈáèÂåñ‰∏ãÂ∞ÜFP16 KVÁºìÂ≠òÂ§ßÂ∞èÂáèÂ∞ë‰∫Ü87.5%ÔºåÂπ∂‰∏îÂú®1‰ΩçÈáèÂåñÁöÑÊÉÖÂÜµ‰∏ãÔºåÂáÜÁ°ÆÁéáÊçüÂ§±ÊûÅÂ∞èÔºåÊîØÊåÅLLaMA-3.1 8BÊ®°ÂûãÂú®Âçï‰∏™RTX 4090 GPU‰∏äËøêË°å128K‰∏ä‰∏ãÊñáÈïøÂ∫¶ÔºåÂ±ïÁé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÂ§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÔºåÂ¶ÇÂØπËØùÁ≥ªÁªü„ÄÅÊñáÊú¨ÁîüÊàêÂíå‰ø°ÊÅØÊ£ÄÁ¥¢Á≠â„ÄÇÈÄöËøáÊúâÊïàÂéãÁº©KVÁºìÂ≠òÔºåCommVQËÉΩÂ§üÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ËøêË°åÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊé®Âä®Êô∫ËÉΩÂ∫îÁî®ÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.

