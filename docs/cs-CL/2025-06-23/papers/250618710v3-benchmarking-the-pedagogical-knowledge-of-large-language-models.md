---
layout: default
title: Benchmarking the Pedagogical Knowledge of Large Language Models
---

# Benchmarking the Pedagogical Knowledge of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18710" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18710v3</a>
  <a href="https://arxiv.org/pdf/2506.18710.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18710v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18710v3', 'Benchmarking the Pedagogical Knowledge of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maxime LeliÃ¨vre, Amy Waldock, Meng Liu, Natalia ValdÃ©s Aspillaga, Alasdair Mackintosh, MarÃ­a JosÃ© Ogando Portela, Jared Lee, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23 (æ›´æ–°: 2025-07-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•™å­¦çŸ¥è¯†åŸºå‡†ä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•™è‚²èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•™å­¦çŸ¥è¯†` `åŸºå‡†æµ‹è¯•` `ç‰¹æ®Šæ•™è‚²` `æ•™è‚²æŠ€æœ¯` `æ¨¡å‹è¯„ä¼°` `è·¨é¢†åŸŸçŸ¥è¯†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å†…å®¹çŸ¥è¯†ï¼Œç¼ºä¹å¯¹æ¨¡å‹æ•™å­¦æ³•ç†è§£çš„è¯„ä¼°ï¼Œå¯¼è‡´æ•™è‚²åº”ç”¨ä¸­çš„å…³é”®çŸ¥è¯†ç¼ºå£ã€‚
2. æœ¬æ–‡æå‡ºäº†æ•™å­¦åŸºå‡†ï¼Œè®¾è®¡äº†ä¸€å¥—æ–°é¢–çš„æ•°æ®é›†ï¼Œä¸“æ³¨äºè¯„ä¼°æ¨¡å‹çš„è·¨é¢†åŸŸæ•™å­¦çŸ¥è¯†å’Œç‰¹æ®Šæ•™è‚²éœ€æ±‚çŸ¥è¯†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œ97ä¸ªæ¨¡å‹åœ¨æ•™å­¦çŸ¥è¯†é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä»28%åˆ°89%ä¸ç­‰ï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨æ•™è‚²é¢†åŸŸçš„æ½œåŠ›å’Œå·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºå‡†æµ‹è¯•å¦‚å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½åœ¨å„é¢†åŸŸçš„çŸ¥è¯†å’Œèƒ½åŠ›æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†ä¸»è¦é›†ä¸­åœ¨å†…å®¹çŸ¥è¯†ä¸Šï¼Œæœªèƒ½æœ‰æ•ˆè¯„ä¼°æ¨¡å‹å¯¹æ•™å­¦æ³•çš„ç†è§£ã€‚æœ¬æ–‡å¼•å…¥äº†æ•™å­¦åŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨é¢†åŸŸæ•™å­¦çŸ¥è¯†ï¼ˆCDPKï¼‰å’Œç‰¹æ®Šæ•™è‚²éœ€æ±‚ä¸æ®‹ç–¾ï¼ˆSENDï¼‰æ–¹é¢çš„èƒ½åŠ›ã€‚åŸºå‡†åŸºäºä¸“ä¸šå‘å±•è€ƒè¯•ä¸­çš„é—®é¢˜ï¼Œæ¶µç›–æ•™å­¦ç­–ç•¥å’Œè¯„ä¼°æ–¹æ³•ç­‰å¤šä¸ªå­é¢†åŸŸã€‚æˆ‘ä»¬æŠ¥å‘Šäº†97ä¸ªæ¨¡å‹çš„ç»“æœï¼Œå‡†ç¡®ç‡ä»28%åˆ°89%ä¸ç­‰ï¼Œå¹¶æä¾›äº†åœ¨çº¿æ’è¡Œæ¦œï¼Œå…è®¸æ ¹æ®æ¨¡å‹å±æ€§è¿›è¡Œäº¤äº’å¼æ¢ç´¢å’Œè¿‡æ»¤ã€‚æ•™è‚²ç›¸å…³çš„åŸºå‡†å¯¹äºè¡¡é‡æ¨¡å‹ç†è§£æ•™å­¦æ¦‚å¿µçš„èƒ½åŠ›è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºæ”¯æŒæœ‰æ•ˆçš„æ•™å­¦å®è·µã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½æœ‰æ•ˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™å­¦æ³•ç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨ä¸­å­˜åœ¨çš„çŸ¥è¯†ç¼ºå£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ•™å­¦åŸºå‡†ï¼Œæ„å»ºä¸€ä¸ªä¸“æ³¨äºè¯„ä¼°æ¨¡å‹è·¨é¢†åŸŸæ•™å­¦çŸ¥è¯†å’Œç‰¹æ®Šæ•™è‚²éœ€æ±‚çŸ¥è¯†çš„æ•°æ®é›†ï¼Œä»¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†çš„æ„å»ºã€æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†ç”±ä¸“ä¸šå‘å±•è€ƒè¯•ä¸­çš„é—®é¢˜æ„æˆï¼Œæ¶µç›–å¤šç§æ•™å­¦ç­–ç•¥å’Œè¯„ä¼°æ–¹æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•™å­¦çŸ¥è¯†ï¼Œç‰¹åˆ«æ˜¯è·¨é¢†åŸŸå’Œç‰¹æ®Šæ•™è‚²éœ€æ±‚æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å†…å®¹çŸ¥è¯†è¯„ä¼°å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†ä¸­çš„é—®é¢˜ç»è¿‡ç²¾å¿ƒç­–åˆ’ï¼Œç¡®ä¿æ¶µç›–å¤šç§æ•™å­¦æ³•çš„å­é¢†åŸŸï¼Œæ¨¡å‹è¯„ä¼°é‡‡ç”¨å‡†ç¡®ç‡ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼Œå¹¶æä¾›åœ¨çº¿æ’è¡Œæ¦œä»¥ä¾¿äºå®æ—¶æ›´æ–°å’Œäº¤äº’åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ97ä¸ªæ¨¡å‹åœ¨æ•™å­¦çŸ¥è¯†é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä»28%åˆ°89%ä¸ç­‰ï¼Œè¡¨æ˜ä¸åŒæ¨¡å‹åœ¨æ•™è‚²é¢†åŸŸçš„è¡¨ç°å·®å¼‚ã€‚è¿™ä¸€å‘ç°ä¸ºæœªæ¥æ¨¡å‹çš„ä¼˜åŒ–å’Œæ•™è‚²åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œæ•™å¸ˆåŸ¹è®­å·¥å…·ã€‚é€šè¿‡è¯„ä¼°æ¨¡å‹çš„æ•™å­¦çŸ¥è¯†ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æŒ‡å¯¼æ•™è‚²å·¥å…·çš„å¼€å‘ï¼Œç¡®ä¿å…¶èƒ½å¤Ÿæ»¡è¶³å­¦ä¹ è€…çš„éœ€æ±‚ï¼Œè¿›è€Œæå‡æ•™è‚²è´¨é‡å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https://rebrand.ly/pedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.

