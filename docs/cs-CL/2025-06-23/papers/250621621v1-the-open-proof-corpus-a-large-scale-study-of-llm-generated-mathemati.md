---
layout: default
title: The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs
---

# The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21621" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21621v1</a>
  <a href="https://arxiv.org/pdf/2506.21621.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21621v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21621v1', 'The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jasper Dekoninck, Ivo Petrov, Kristian Minchev, Mislav Balunovic, Martin Vechev, Miroslav Marinov, Maria Drencheva, Lyuba Konova, Milen Shumanov, Kaloyan Tsvetkov, Nikolay Drenchev, Lazar Todorov, Kalina Nikolova, Nikolay Georgiev, Vanesa Kalinkova, Margulan Ismoldayev

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼€æ”¾è¯æ˜è¯­æ–™åº“ä»¥æ¨åŠ¨æ•°å­¦è¯æ˜ç”Ÿæˆç ”ç©¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°å­¦è¯æ˜ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼€æ”¾è¯æ˜è¯­æ–™åº“` `æ•°æ®é›†æ„å»º` `è‡ªåŠ¨åŒ–è¯æ˜` `æ¨¡å‹å¾®è°ƒ` `äººå·¥æ™ºèƒ½ç ”ç©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°å­¦è¯æ˜ç”Ÿæˆæ–¹æ³•ç¼ºä¹é«˜è´¨é‡çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé™åˆ¶äº†æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚
2. æœ¬æ–‡æå‡ºå¼€æ”¾è¯æ˜è¯­æ–™åº“ï¼ˆOPCï¼‰ï¼ŒåŒ…å«5000å¤šä¸ªç»è¿‡äººç±»è¯„ä¼°çš„LLMç”Ÿæˆè¯æ˜ï¼Œæ—¨åœ¨æ¨åŠ¨è¯æ˜ç”Ÿæˆç ”ç©¶ã€‚
3. é€šè¿‡åœ¨OPCä¸Šå¾®è°ƒæ¨¡å‹ï¼Œå–å¾—äº†ä¸å½“å‰æœ€ä½³æ¨¡å‹ç›¸å½“çš„è¯æ˜æ­£ç¡®æ€§è¯„ä¼°æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦è¯æ˜ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç¼ºä¹é«˜è´¨é‡çš„äººç±»è¯„ä¼°è¯æ˜çš„å¤§è§„æ¨¡æ•°æ®é›†é™åˆ¶äº†è¿›ä¸€æ­¥å‘å±•ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†å¼€æ”¾è¯æ˜è¯­æ–™åº“ï¼ˆOPCï¼‰ï¼Œè¯¥æ•°æ®é›†åŒ…å«5000å¤šä¸ªç”±æœ€å…ˆè¿›çš„LLMsç”Ÿæˆå¹¶ç»è¿‡äººç±»è¯„ä¼°çš„è¯æ˜ã€‚OPCæ—¨åœ¨å¹¿æ³›é€‚ç”¨å¹¶æ”¯æŒè¯æ˜ç”Ÿæˆç ”ç©¶ï¼Œé¦–æ¬¡åŒ…å«æ¥è‡ªç¾å›½æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆUSAMOï¼‰å’Œå›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIMOï¼‰ç­‰è‘—åæ•°å­¦ç«èµ›çš„æ­£ç¡®LLMç”Ÿæˆè§£ç­”ã€‚é€šè¿‡OPCï¼Œæœ¬æ–‡æ¢è®¨äº†è‡ªåŠ¨è¯æ˜ç”Ÿæˆä¸­çš„å…³é”®é—®é¢˜ï¼Œå¹¶å±•ç¤ºäº†åœ¨è¯¥æ•°æ®é›†ä¸Šå¾®è°ƒçš„8Bå‚æ•°æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œå…¶æ€§èƒ½ä¸æœ€ä½³æ¨¡å‹Gemini-2.5-Proç›¸å½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¼ºä¹é«˜è´¨é‡ã€ç»è¿‡äººç±»è¯„ä¼°çš„æ•°å­¦è¯æ˜æ•°æ®é›†çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¼€æ”¾è¯æ˜è¯­æ–™åº“ï¼ˆOPCï¼‰ï¼Œè¯¥æ•°æ®é›†åŒ…å«5000å¤šä¸ªLLMç”Ÿæˆçš„ã€ç»è¿‡äººç±»è¯„ä¼°çš„æ•°å­¦è¯æ˜ï¼Œæ—¨åœ¨ä¸ºè¯æ˜ç”Ÿæˆç ”ç©¶æä¾›åŸºç¡€æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOPCçš„æ„å»ºåŒ…æ‹¬æ•°æ®æ”¶é›†ã€è¯„ä¼°å’Œæ•´ç†ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼Œç¡®ä¿æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œä»¥æ”¯æŒåç»­çš„ç ”ç©¶å’Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šOPCæ˜¯é¦–ä¸ªåŒ…å«å¤§é‡æ­£ç¡®LLMç”Ÿæˆè§£ç­”çš„æ•°å­¦è¯æ˜æ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è‘—åæ•°å­¦ç«èµ›çš„é—®é¢˜ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œé‡‡ç”¨äº†ä¸¥æ ¼çš„è¯„ä¼°æ ‡å‡†ï¼Œç¡®ä¿æ¯ä¸ªè¯æ˜çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ï¼ŒåŒæ—¶åœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†é€‚å½“çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–è¯æ˜çš„ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä½¿ç”¨å¼€æ”¾è¯æ˜è¯­æ–™åº“ï¼ˆOPCï¼‰è¿›è¡Œå¾®è°ƒåï¼Œ8Bå‚æ•°æ¨¡å‹åœ¨è¯æ˜æ­£ç¡®æ€§è¯„ä¼°ä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸æœ€ä½³æ¨¡å‹Gemini-2.5-Proç›¸å½“çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

å¼€æ”¾è¯æ˜è¯­æ–™åº“ï¼ˆOPCï¼‰å¯å¹¿æ³›åº”ç”¨äºæ•°å­¦æ•™è‚²ã€è‡ªåŠ¨è¯æ˜ç”Ÿæˆå’Œäººå·¥æ™ºèƒ½ç ”ç©¶ç­‰é¢†åŸŸã€‚å…¶æä¾›çš„é«˜è´¨é‡æ•°æ®é›†å°†æ¨åŠ¨ç›¸å…³æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œä¿ƒè¿›æ•°å­¦è¯æ˜è‡ªåŠ¨åŒ–çš„è¿›æ­¥ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²å’Œç§‘ç ”ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent months, large language models (LLMs) have made significant progress in mathematical proof generation, but further advancement is hindered by the lack of a large-scale, high-quality dataset of human-evaluated proofs. While expensive to create, such a dataset is essential for driving improvements in training and enabling a rigorous analysis of proof generation capabilities. In this work, we present the Open Proof Corpus (OPC), a dataset comprising over 5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was specifically designed for broad applicability and downstream usage in proof generation research and is the first to include a substantial number of correct, LLM-generated solutions to problems from prestigious mathematics competitions such as the USAMO and IMO. Using the OPC, we explore critical questions in automated proof generation: (1) the performance gap between natural language and formal proof generation, (2) the discrepancy between final-answer accuracy and full-proof validity, and (3) the impact of best-of-n selection on proof quality. Finally, to showcase the utility of the OPC, we finetune an 8B-parameter model on the dataset, obtaining a model that performs on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof correctness.

