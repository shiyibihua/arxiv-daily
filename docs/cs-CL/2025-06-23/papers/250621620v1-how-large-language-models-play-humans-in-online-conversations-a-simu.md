---
layout: default
title: How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit
---

# How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21620" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21620v1</a>
  <a href="https://arxiv.org/pdf/2506.21620.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21620v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21620v1', 'How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daniele Cirulli, Giulio Cimini, Giovanni Palermo

**åˆ†ç±»**: cs.CL, cs.AI, cs.CY, cs.SI, physics.soc-ph

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨2016å¹´ç¾å›½æ”¿æ²»è®¨è®ºä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€ç”Ÿæˆ` `æ”¿æ²»è®¨è®º` `ç¤¾äº¤åª’ä½“` `æƒ…æ„Ÿåˆ†æ` `è¯­ä¹‰åµŒå…¥` `åœ¨çº¿è¾©è®º` `AIæ“æ§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¨¡æ‹Ÿäººç±»åœ¨çº¿æ”¿æ²»è®¨è®ºæ—¶ï¼Œé¢ä¸´ç”Ÿæˆå†…å®¹çš„çœŸå®æ€§å’Œå¤šæ ·æ€§æŒ‘æˆ˜ã€‚
2. è®ºæ–‡é€šè¿‡GPT-4ç”Ÿæˆè¯„è®ºï¼Œæ¨¡æ‹ŸçœŸå®å’Œäººå·¥ç”¨æˆ·ï¼Œæ¢ç´¢å…¶åœ¨æ”¿æ²»è®¨è®ºä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4ç”Ÿæˆçš„è¯„è®ºåœ¨è¯­ä¹‰ä¸Šä¸çœŸå®è¯„è®ºç›¸ä¼¼ï¼Œä½†æ›´å€¾å‘äºå½¢æˆå…±è¯†è€Œéå¼‚è®®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘æˆä¸ºè‡ªç„¶è¯­è¨€ç”Ÿæˆçš„å¼ºå¤§å·¥å…·ï¼Œåº”ç”¨èŒƒå›´å¹¿æ³›ï¼ŒåŒ…æ‹¬å†…å®¹åˆ›ä½œå’Œç¤¾ä¼šæ¨¡æ‹Ÿã€‚æœ¬æ–‡ç ”ç©¶äº†LLMsåœ¨æ¨¡æ‹Ÿ2016å¹´ç¾å›½æ€»ç»Ÿé€‰ä¸¾æœŸé—´Redditè®¨è®ºä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡GPT-4ç”Ÿæˆç”¨æˆ·è¯„è®ºï¼Œåˆ†æå…¶æ”¿æ²»å€¾å‘ã€æƒ…æ„Ÿå’Œè¯­è¨€ç‰¹å¾ã€‚ç ”ç©¶å‘ç°ï¼ŒGPT-4èƒ½å¤Ÿç”Ÿæˆä¸ç¤¾åŒºæ”¯æŒçš„å€™é€‰äººä¸€è‡´çš„ç°å®è¯„è®ºï¼Œä½†æ›´å®¹æ˜“åˆ›é€ å…±è¯†è€Œéå¼‚è®®ã€‚æ­¤å¤–ï¼ŒçœŸå®ä¸äººå·¥è¯„è®ºåœ¨è¯­ä¹‰åµŒå…¥ç©ºé—´ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„åˆ†ç¦»æ€§ï¼Œå°½ç®¡æ‰‹åŠ¨æ£€æŸ¥æ—¶éš¾ä»¥åŒºåˆ†ã€‚è¿™äº›å‘ç°ä¸ºLLMsåœ¨åœ¨çº¿è®¨è®ºä¸­çš„æ½œåœ¨å½±å“æä¾›äº†è§è§£ï¼Œå°¤å…¶æ˜¯åœ¨æ”¿æ²»è¾©è®ºå’Œå™äº‹å¡‘é€ æ–¹é¢ï¼Œå…·æœ‰æ›´å¹¿æ³›çš„AIé©±åŠ¨è¯è¯­æ“æ§çš„å«ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨¡æ‹Ÿäººç±»åœ¨çº¿æ”¿æ²»è®¨è®ºæ—¶ç”Ÿæˆå†…å®¹çš„çœŸå®æ€§å’Œå¤šæ ·æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™æ–¹é¢çš„è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„æ”¿æ²»æƒ…å¢ƒæ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨GPT-4ç”Ÿæˆç”¨æˆ·è¯„è®ºï¼Œé€šè¿‡æ¨¡æ‹ŸçœŸå®å’Œäººå·¥ç”¨æˆ·çš„æ–¹å¼ï¼Œè¯„ä¼°å…¶åœ¨æ”¿æ²»è®¨è®ºä¸­çš„è¡¨ç°å’Œå½±å“ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æ­ç¤ºLLMsåœ¨å¤æ‚ç¤¾ä¼šæƒ…å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†ä¸‰ç§ä¸åŒçš„å®éªŒè®¾è®¡ï¼Œåˆ†åˆ«è®©GPT-4æ¨¡æ‹ŸçœŸå®ç”¨æˆ·å’Œäººå·¥ç”¨æˆ·ç”Ÿæˆè¯„è®ºã€‚åˆ†æé˜¶æ®µåŒ…æ‹¬å¯¹ç”Ÿæˆè¯„è®ºçš„æ”¿æ²»å€¾å‘ã€æƒ…æ„Ÿå’Œè¯­è¨€ç‰¹å¾è¿›è¡Œæ¯”è¾ƒï¼ŒåŸºäºçœŸå®ç”¨æˆ·è´¡çŒ®å’ŒåŸºçº¿æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡LLMsç”Ÿæˆçš„è¯„è®ºèƒ½å¤Ÿåœ¨è¯­ä¹‰ä¸Šä¸çœŸå®è¯„è®ºç›¸ä¼¼ï¼Œä¸”åœ¨æ‰‹åŠ¨æ£€æŸ¥æ—¶éš¾ä»¥åŒºåˆ†ï¼Œè¿™è¡¨æ˜LLMsåœ¨åœ¨çº¿è®¨è®ºä¸­çš„æ½œåœ¨å½±å“åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼ŒGPT-4çš„å‚æ•°è®¾ç½®å’Œç”Ÿæˆç­–ç•¥ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿ç”Ÿæˆè¯„è®ºçš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚æŸå¤±å‡½æ•°å’Œè¯„ä¼°æ ‡å‡†ä¹Ÿç»è¿‡è°ƒæ•´ï¼Œä»¥ä¾¿æ›´å¥½åœ°åæ˜ è¯„è®ºçš„æ”¿æ²»å€¾å‘å’Œæƒ…æ„Ÿç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4ç”Ÿæˆçš„è¯„è®ºåœ¨è¯­ä¹‰ä¸Šä¸çœŸå®ç”¨æˆ·çš„è¯„è®ºè¡¨ç°å‡ºè‰¯å¥½çš„ç›¸ä¼¼æ€§ï¼Œä¸”åœ¨æ”¿æ²»å€¾å‘ä¸Šèƒ½å¤Ÿæœ‰æ•ˆæ¨¡æ‹Ÿæ”¯æŒæˆ–åå¯¹å€™é€‰äººçš„è§‚ç‚¹ã€‚å°½ç®¡ç”Ÿæˆçš„è¯„è®ºæ›´å®¹æ˜“å½¢æˆå…±è¯†ï¼Œä½†åœ¨è¯­ä¹‰åµŒå…¥ç©ºé—´ä¸­ï¼ŒçœŸå®ä¸äººå·¥è¯„è®ºè¡¨ç°å‡ºæ˜æ˜¾çš„åˆ†ç¦»æ€§ï¼Œè¡¨æ˜LLMsåœ¨åœ¨çº¿è®¨è®ºä¸­çš„æ½œåœ¨å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹ç”Ÿæˆã€æ”¿æ²»èˆ†è®ºåˆ†æå’Œåœ¨çº¿è¾©è®ºçš„æ¨¡æ‹Ÿã€‚é€šè¿‡ç†è§£LLMsåœ¨æ”¿æ²»è®¨è®ºä¸­çš„è¡¨ç°ï¼Œå¯ä»¥ä¸ºæ”¿ç­–åˆ¶å®šè€…å’Œç¤¾ä¼šç§‘å­¦å®¶æä¾›é‡è¦çš„è§è§£ï¼Œå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°åº”å¯¹AIé©±åŠ¨çš„èˆ†è®ºæ“æ§å’Œä¿¡æ¯ä¼ æ’­é—®é¢˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have recently emerged as powerful tools for natural language generation, with applications spanning from content creation to social simulations. Their ability to mimic human interactions raises both opportunities and concerns, particularly in the context of politically relevant online discussions. In this study, we evaluate the performance of LLMs in replicating user-generated content within a real-world, divisive scenario: Reddit conversations during the 2016 US Presidential election. In particular, we conduct three different experiments, asking GPT-4 to generate comments by impersonating either real or artificial partisan users. We analyze the generated comments in terms of political alignment, sentiment, and linguistic features, comparing them against real user contributions and benchmarking against a null model. We find that GPT-4 is able to produce realistic comments, both in favor of or against the candidate supported by the community, yet tending to create consensus more easily than dissent. In addition we show that real and artificial comments are well separated in a semantically embedded space, although they are indistinguishable by manual inspection. Our findings provide insights on the potential use of LLMs to sneak into online discussions, influence political debate and shape political narratives, bearing broader implications of AI-driven discourse manipulation.

