---
layout: default
title: Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations
---

# Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19004" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19004v1</a>
  <a href="https://arxiv.org/pdf/2506.19004.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19004v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19004v1', 'Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Brian Siyuan Zheng, Alisa Liu, Orevaoghene Ahia, Jonathan Hayase, Yejin Choi, Noah A. Smith

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

**å¤‡æ³¨**: preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶éæ ‡å‡†åˆ†è¯å¯¹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `éæ ‡å‡†åˆ†è¯` `é²æ£’æ€§` `æŒ‡ä»¤è°ƒä¼˜` `æ–‡æœ¬å¤„ç†` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åˆ†è¯æ–¹æ³•é€šå¸¸ä¾èµ–äºç¡®å®šæ€§ç®—æ³•ï¼Œå¯¼è‡´æ¨¡å‹å¯¹éæ ‡å‡†åˆ†è¯å½¢å¼çš„é²æ£’æ€§ä¸è¶³ã€‚
2. æœ¬ç ”ç©¶æå‡ºæ¢è®¨è¯­è¨€æ¨¡å‹åœ¨æœªè§è¿‡çš„éæ ‡å‡†åˆ†è¯ä¸‹çš„è¡¨ç°ï¼Œå‘ç°æ¨¡å‹åœ¨éšæœºå’Œå­—ç¬¦çº§åˆ†è¯ä¸‹ä»èƒ½ä¿æŒè¾ƒé«˜æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­èƒ½å¤Ÿæ˜¾è‘—æå‡æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å­—ç¬¦ä¸²æ“ä½œå’Œå¤§æ•°ç®—æœ¯ä»»åŠ¡ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£åˆ†è¯å™¨é‡‡ç”¨ç¡®å®šæ€§ç®—æ³•å°†æ–‡æœ¬æ˜ å°„ä¸ºå•ä¸€çš„â€œæ ‡å‡†â€æ ‡è®°åºåˆ—ï¼Œä½†åŒä¸€å­—ç¬¦ä¸²å¯ä»¥ä½¿ç”¨åˆ†è¯è¯æ±‡è¡¨ç¼–ç ä¸ºå¤šç§éæ ‡å‡†åˆ†è¯å½¢å¼ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹å¯¹æœªåœ¨è®­ç»ƒä¸­è§è¿‡çš„éæ ‡å‡†åˆ†è¯çš„é²æ£’æ€§ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåœ¨20ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹åœ¨éšæœºé‡‡æ ·çš„åˆ†è¯ä¸‹ä»èƒ½ä¿æŒé«˜è¾¾93.4%çš„åŸå§‹æ€§èƒ½ï¼Œå­—ç¬¦çº§åˆ†è¯ä¸‹ä¸º90.8%ã€‚æ›´å¼ºçš„æ¨¡å‹é€šå¸¸è¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§ï¼Œè€Œé²æ£’æ€§éšç€åˆ†è¯åç¦»æ ‡å‡†å½¢å¼è€Œå‡å¼±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æŸäº›éæ ‡å‡†åˆ†è¯æ–¹æ¡ˆå¯ä»¥æå‡æ€§èƒ½ï¼Œä¾‹å¦‚å­—ç¬¦çº§åˆ†è¯åœ¨å­—ç¬¦ä¸²æ“ä½œå’Œä»£ç ç†è§£ä»»åŠ¡ä¸­æé«˜äº†14%ï¼Œè€Œå³å¯¹é½æ•°å­—åˆ†ç»„åœ¨å¤§æ•°ç®—æœ¯ä¸­æå‡äº†33%ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†è¿™ç§é²æ£’æ€§çš„æ¥æºï¼Œå‘ç°å…¶åœ¨æŒ‡ä»¤è°ƒä¼˜é˜¶æ®µå½¢æˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹æœªè§è¿‡çš„éæ ‡å‡†åˆ†è¯æ—¶çš„é²æ£’æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾æ¨¡å‹ä»…èƒ½å¤„ç†æ ‡å‡†åˆ†è¯ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯éªŒè¯è¯­è¨€æ¨¡å‹åœ¨éæ ‡å‡†åˆ†è¯ä¸‹çš„è¡¨ç°ï¼Œå¹¶æ¢ç´¢å¦‚ä½•åˆ©ç”¨è¿™äº›éæ ‡å‡†åˆ†è¯å½¢å¼æå‡æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒåˆ†è¯æ–¹å¼ï¼Œç ”ç©¶å‘ç°æŸäº›éæ ‡å‡†åˆ†è¯å½¢å¼èƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜é˜¶æ®µï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒåˆ†è¯å½¢å¼ä¸‹çš„è¡¨ç°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ ‡å‡†åˆ†è¯ä¸éæ ‡å‡†åˆ†è¯çš„æ¯”è¾ƒã€æ€§èƒ½è¯„ä¼°ä»¥åŠé²æ£’æ€§åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæ­ç¤ºäº†è¯­è¨€æ¨¡å‹å¯¹åˆ†è¯å™¨çš„ä¾èµ–ç¨‹åº¦ä½äºé¢„æœŸï¼Œä¸”åœ¨æ¨ç†é˜¶æ®µå¯¹åˆ†è¯çš„å¹²é¢„èƒ½å¤Ÿæ˜¾è‘—æå‡æ€§èƒ½ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†ä¼ ç»Ÿè§‚ç‚¹ï¼Œè¡¨æ˜æ¨¡å‹å¯ä»¥çµæ´»é€‚åº”ä¸åŒçš„åˆ†è¯å½¢å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§åˆ†è¯ç­–ç•¥ï¼ŒåŒ…æ‹¬éšæœºåˆ†è¯å’Œå­—ç¬¦çº§åˆ†è¯ï¼Œå¹¶é€šè¿‡æŒ‡ä»¤è°ƒä¼˜æ¥å¢å¼ºæ¨¡å‹å¯¹éæ ‡å‡†åˆ†è¯çš„ç†è§£ã€‚æ¨¡å‹åœ¨è®­ç»ƒæ—¶æœªè§è¿‡çš„åˆ†è¯å½¢å¼è¢«è§†ä¸ºæ‹¼å†™é”™è¯¯ï¼ŒåŸºäºæ­¤è®¾è®¡äº†ç›¸åº”çš„æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹åœ¨éšæœºåˆ†è¯ä¸‹ä¿æŒ93.4%çš„åŸå§‹æ€§èƒ½ï¼Œè€Œåœ¨å­—ç¬¦çº§åˆ†è¯ä¸‹ä¸º90.8%ã€‚æ­¤å¤–ï¼Œå­—ç¬¦çº§åˆ†è¯åœ¨å­—ç¬¦ä¸²æ“ä½œå’Œä»£ç ç†è§£ä»»åŠ¡ä¸­æå‡äº†14%ï¼Œå³å¯¹é½æ•°å­—åˆ†ç»„åœ¨å¤§æ•°ç®—æœ¯ä¸­æå‡äº†33%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œéæ ‡å‡†åˆ†è¯å½¢å¼åœ¨ç‰¹å®šä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€ä»£ç ç†è§£å’Œå­—ç¬¦ä¸²æ“ä½œç­‰ä»»åŠ¡ã€‚é€šè¿‡ä¼˜åŒ–åˆ†è¯ç­–ç•¥ï¼Œæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤šæ ·åŒ–çš„è¾“å…¥ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œç ”ç©¶ç»“æœå¯èƒ½æ¨åŠ¨æ›´çµæ´»çš„åˆ†è¯æ–¹æ³•åœ¨å„ç±»è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern tokenizers employ deterministic algorithms to map text into a single "canonical" token sequence, yet the same string can be encoded as many non-canonical tokenizations using the tokenizer vocabulary. In this work, we investigate the robustness of LMs to text encoded with non-canonical tokenizations entirely unseen during training. Surprisingly, when evaluated across 20 benchmarks, we find that instruction-tuned models retain up to 93.4% of their original performance when given a randomly sampled tokenization, and 90.8% with character-level tokenization. We see that overall stronger models tend to be more robust, and robustness diminishes as the tokenization departs farther from the canonical form. Motivated by these results, we then identify settings where non-canonical tokenization schemes can *improve* performance, finding that character-level segmentation improves string manipulation and code understanding tasks by up to +14%, and right-aligned digit grouping enhances large-number arithmetic by +33%. Finally, we investigate the source of this robustness, finding that it arises in the instruction-tuning phase. We show that while both base and post-trained models grasp the semantics of non-canonical tokenizations (perceiving them as containing misspellings), base models try to mimic the imagined mistakes and degenerate into nonsensical output, while post-trained models are committed to fluent responses. Overall, our findings suggest that models are less tied to their tokenizer than previously believed, and demonstrate the promise of intervening on tokenization at inference time to boost performance.

