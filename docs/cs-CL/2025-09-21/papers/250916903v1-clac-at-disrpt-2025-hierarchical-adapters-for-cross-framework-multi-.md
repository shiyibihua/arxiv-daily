---
layout: default
title: CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification
---

# CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16903" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16903v1</a>
  <a href="https://arxiv.org/pdf/2509.16903.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16903v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16903v1', 'CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nawar Turk, Daniele Comitogianni, Leila Kosseim

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-21

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHiDACæ¨¡å‹ï¼Œè§£å†³è·¨æ¡†æ¶å¤šè¯­ç¯‡ç« å…³ç³»åˆ†ç±»ä»»åŠ¡ä¸­çš„ç»Ÿä¸€æ ‡ç­¾æŒ‘æˆ˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¯‡ç« å…³ç³»åˆ†ç±»` `è·¨æ¡†æ¶å­¦ä¹ ` `å¤šè¯­è¨€å¤„ç†` `é€‚é…å™¨ç½‘ç»œ` `å¯¹æ¯”å­¦ä¹ ` `è‡ªç„¶è¯­è¨€ç†è§£` `é¢„è®­ç»ƒæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è·¨æ¡†æ¶å¤šè¯­ç¯‡ç« å…³ç³»åˆ†ç±»ä»»åŠ¡é¢ä¸´å¤šè¯­è¨€å’Œå½¢å¼ä¸»ä¹‰å·®å¼‚çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†ç»Ÿä¸€æ ‡ç­¾ã€‚
2. HiDACæ¨¡å‹é‡‡ç”¨åˆ†å±‚åŒé€‚é…å™¨å’Œå¯¹æ¯”å­¦ä¹ ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨ç»Ÿä¸€æ ‡ç­¾ä¸‹çš„è·¨æ¡†æ¶ç¯‡ç« å…³ç³»åˆ†ç±»æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHiDACæ¨¡å‹åœ¨å‚æ•°æ•ˆç‡æ–¹é¢ä¼˜äºå®Œå…¨å¾®è°ƒï¼Œå¹¶åœ¨æ€»ä½“å‡†ç¡®ç‡ä¸Šå–å¾—äº†æœ€ä½³ç»“æœï¼ˆ67.5%ï¼‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨DISRPT 2025å…±äº«ä»»åŠ¡Task 3ï¼ˆç¯‡ç« å…³ç³»åˆ†ç±»ï¼‰ä¸­çš„æäº¤ã€‚Task 3æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ã€åŒ…å«17ä¸ªç¯‡ç« å…³ç³»æ ‡ç­¾çš„é›†åˆï¼Œæ¶µç›–16ç§è¯­è¨€å’Œå…­ç§ç¯‡ç« æ¡†æ¶ä¸­çš„39ä¸ªè¯­æ–™åº“ï¼Œå¸¦æ¥äº†æ˜¾è‘—çš„å¤šè¯­è¨€å’Œè·¨å½¢å¼ä¸»ä¹‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡å¾®è°ƒåŸºäºå¤šè¯­è¨€BERTçš„æ¨¡å‹ï¼ˆmBERTã€XLM-RoBERTa-Baseå’ŒXLM-RoBERTa-Largeï¼‰ï¼Œé‡‡ç”¨ä¸¤ç§å‚æ•°æ’åºç­–ç•¥å’Œæ¸è¿›å¼è§£å†»æ¯”ä¾‹ï¼Œä¸ºè¯¥ä»»åŠ¡å»ºç«‹äº†å¼ºå¤§çš„åŸºçº¿ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹è¯„ä¼°äº†åŸºäºæç¤ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå³Claude Opus 4.0ï¼‰ï¼Œä»¥äº†è§£LLMå¦‚ä½•å“åº”æ–°æå‡ºçš„ç»Ÿä¸€æ ‡ç­¾ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†HiDACï¼Œä¸€ç§åˆ†å±‚åŒé€‚é…å™¨å¯¹æ¯”å­¦ä¹ æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶è¾ƒå¤§çš„Transformeræ¨¡å‹å®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œä½†æ”¹è¿›å¹…åº¦ä¸å¤§ï¼Œå¹¶ä¸”è§£å†»ç¼–ç å™¨å±‚çš„å‰75%å¯ä»¥äº§ç”Ÿä¸å®Œå…¨å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶è®­ç»ƒçš„å‚æ•°è¦å°‘å¾—å¤šã€‚åŸºäºæç¤ºçš„æ¨¡å‹æ˜æ˜¾è½åäºå¾®è°ƒçš„Transformeræ¨¡å‹ï¼Œè€ŒHiDACå®ç°äº†æœ€é«˜çš„æ€»ä½“å‡†ç¡®ç‡ï¼ˆ67.5%ï¼‰ï¼ŒåŒæ—¶æ¯”å®Œå…¨å¾®è°ƒæ›´å…·å‚æ•°æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è·¨æ¡†æ¶ã€å¤šè¯­è¨€çš„ç¯‡ç« å…³ç³»åˆ†ç±»é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥å¾®è°ƒå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè™½ç„¶å¯ä»¥å–å¾—ä¸€å®šæ•ˆæœï¼Œä½†å¿½ç•¥äº†ä¸åŒæ¡†æ¶å’Œè¯­è¨€ä¹‹é—´çš„å·®å¼‚æ€§ï¼Œä¸”å®Œå…¨å¾®è°ƒå‚æ•°é‡å·¨å¤§ï¼Œæ•ˆç‡è¾ƒä½ã€‚æ­¤å¤–ï¼Œç›´æ¥ä½¿ç”¨Prompt-based LLMåœ¨zero-shotæˆ–few-shot settingä¸‹æ•ˆæœä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨ç»Ÿä¸€æ ‡ç­¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åˆ†å±‚é€‚é…å™¨æ¥å­¦ä¹ ä¸åŒæ¡†æ¶å’Œè¯­è¨€çš„ç‰¹å®šè¡¨ç¤ºï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ æ¥å¢å¼ºæ¨¡å‹å¯¹ç»Ÿä¸€ç¯‡ç« å…³ç³»æ ‡ç­¾çš„åŒºåˆ†èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”è·¨æ¡†æ¶å’Œå¤šè¯­è¨€çš„å¤æ‚åœºæ™¯ï¼ŒåŒæ—¶ä¿æŒå‚æ•°æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHiDACæ¨¡å‹çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š
1. **é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹**ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„Transformeræ¨¡å‹ï¼ˆå¦‚XLM-RoBERTaï¼‰ä½œä¸ºåŸºç¡€ç¼–ç å™¨ã€‚
2. **åˆ†å±‚é€‚é…å™¨**ï¼šåœ¨Transformerçš„æ¯ä¸€å±‚æ·»åŠ é€‚é…å™¨æ¨¡å—ï¼Œç”¨äºå­¦ä¹ ç‰¹å®šæ¡†æ¶å’Œè¯­è¨€çš„è¡¨ç¤ºã€‚
3. **å¯¹æ¯”å­¦ä¹ **ï¼šä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹å°†ç›¸åŒç¯‡ç« å…³ç³»çš„æ ·æœ¬æ‹‰è¿‘ï¼Œä¸åŒç¯‡ç« å…³ç³»çš„æ ·æœ¬æ¨è¿œã€‚
4. **åˆ†ç±»å™¨**ï¼šä½¿ç”¨çº¿æ€§åˆ†ç±»å™¨å°†å­¦ä¹ åˆ°çš„è¡¨ç¤ºæ˜ å°„åˆ°ç¯‡ç« å…³ç³»æ ‡ç­¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šHiDACæ¨¡å‹çš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1. **åˆ†å±‚é€‚é…å™¨ç»“æ„**ï¼šé€šè¿‡åœ¨Transformerçš„æ¯ä¸€å±‚æ·»åŠ é€‚é…å™¨ï¼Œèƒ½å¤Ÿæ›´ç»†ç²’åº¦åœ°å­¦ä¹ ä¸åŒæ¡†æ¶å’Œè¯­è¨€çš„ç‰¹å¾ã€‚
2. **åŒé€‚é…å™¨è®¾è®¡**ï¼šä½¿ç”¨ä¸¤ä¸ªé€‚é…å™¨åˆ†åˆ«å¤„ç†ç¯‡ç« çš„ä¸¤ä¸ªè®ºå…ƒï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰è®ºå…ƒä¹‹é—´çš„å…³ç³»ã€‚
3. **å¯¹æ¯”å­¦ä¹ ç›®æ ‡**ï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œå¢å¼ºæ¨¡å‹å¯¹ç¯‡ç« å…³ç³»æ ‡ç­¾çš„åŒºåˆ†èƒ½åŠ›ï¼Œæé«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼š
1. **é€‚é…å™¨å¤§å°**ï¼šé€‚é…å™¨é€šå¸¸é‡‡ç”¨ bottleneck ç»“æ„ï¼Œå‡å°‘å‚æ•°é‡ï¼Œä¾‹å¦‚å°†éšè—å±‚ç»´åº¦å…ˆé™ç»´åˆ°è¾ƒå°ç»´åº¦ï¼Œå†å‡ç»´å›åŸå§‹ç»´åº¦ã€‚
2. **å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°**ï¼šé‡‡ç”¨ InfoNCE loss æˆ–ç±»ä¼¼çš„å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œéœ€è¦ç²¾å¿ƒè®¾è®¡æ­£è´Ÿæ ·æœ¬çš„é€‰å–ç­–ç•¥ã€‚
3. **è®­ç»ƒç­–ç•¥**ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆé¢„è®­ç»ƒé€‚é…å™¨ï¼Œç„¶åå¾®è°ƒæ•´ä¸ªæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HiDACæ¨¡å‹åœ¨DISRPT 2025 Task 3ä¸­å–å¾—äº†æœ€ä½³çš„æ€»ä½“å‡†ç¡®ç‡ï¼ˆ67.5%ï¼‰ï¼Œè¶…è¿‡äº†å®Œå…¨å¾®è°ƒçš„Transformeræ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†æ›´é«˜çš„å‚æ•°æ•ˆç‡ã€‚å®éªŒè¿˜è¡¨æ˜ï¼Œè§£å†»ç¼–ç å™¨å±‚çš„å‰75%å¯ä»¥è¾¾åˆ°ä¸å®Œå…¨å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒå‚æ•°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†çš„å¤šä¸ªé¢†åŸŸï¼Œå¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æç­‰ã€‚é€šè¿‡æå‡ç¯‡ç« å…³ç³»åˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨æ›´å¥½åœ°ç†è§£æ–‡æœ¬çš„æ·±å±‚å«ä¹‰ï¼Œä»è€Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨è·¨è¯­è¨€å’Œè·¨é¢†åŸŸçš„è¿ç§»å­¦ä¹ æ–¹é¢ä¹Ÿå…·æœ‰æ½œåŠ›ï¼Œå¯ä»¥åº”ç”¨äºèµ„æºåŒ®ä¹çš„è¯­è¨€å’Œé¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present our submission to Task 3 (Discourse Relation Classification) of the DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourse relation labels across 39 corpora in 16 languages and six discourse frameworks, posing significant multilingual and cross-formalism challenges. We first benchmark the task by fine-tuning multilingual BERT-based models (mBERT, XLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategies and progressive unfreezing ratios to establish strong baselines. We then evaluate prompt-based large language models (namely Claude Opus 4.0) in zero-shot and few-shot settings to understand how LLMs respond to the newly proposed unified labels. Finally, we introduce HiDAC, a Hierarchical Dual-Adapter Contrastive learning model. Results show that while larger transformer models achieve higher accuracy, the improvements are modest, and that unfreezing the top 75% of encoder layers yields performance comparable to full fine-tuning while training far fewer parameters. Prompt-based models lag significantly behind fine-tuned transformers, and HiDAC achieves the highest overall accuracy (67.5%) while remaining more parameter-efficient than full fine-tuning.

