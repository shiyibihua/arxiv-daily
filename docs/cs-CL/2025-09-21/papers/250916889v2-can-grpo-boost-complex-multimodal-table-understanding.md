---
layout: default
title: Can GRPO Boost Complex Multimodal Table Understanding?
---

# Can GRPO Boost Complex Multimodal Table Understanding?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16889" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16889v2</a>
  <a href="https://arxiv.org/pdf/2509.16889.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16889v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16889v2', 'Can GRPO Boost Complex Multimodal Table Understanding?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoqiang Kang, Shengen Wu, Zimu Wang, Yilin Liu, Xiaobo Jin, Kaizhu Huang, Wei Wang, Yutao Yue, Xiaowei Huang, Qiufeng Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-21 (æ›´æ–°: 2025-09-23)

**å¤‡æ³¨**: EMNLP 2025

**æœŸåˆŠ**: EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Table-R1ï¼šé€šè¿‡ä¸‰é˜¶æ®µå¼ºåŒ–å­¦ä¹ æå‡å¤æ‚å¤šæ¨¡æ€è¡¨æ ¼ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¡¨æ ¼ç†è§£` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¥–åŠ±å‡½æ•°è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¡¨æ ¼ç†è§£æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç»“æ„å’Œé€»è¾‘æ¨ç†æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç›‘ç£å¾®è°ƒè™½ä¸»æµï¼Œä½†å¼ºåŒ–å­¦ä¹ æ–¹æ³•å—é™äºåˆå§‹ç­–ç•¥ç²¾åº¦å’Œç²—ç³™å¥–åŠ±ã€‚
2. Table-R1æ¡†æ¶é€šè¿‡é¢„çƒ­ã€æ„ŸçŸ¥å¯¹é½GRPOå’Œæç¤ºè¡¥å…¨GRPOä¸‰ä¸ªé˜¶æ®µï¼Œå…‹æœåˆå§‹åŒ–ç“¶é¢ˆå’Œå¥–åŠ±ç¨€ç–æ€§ï¼Œæå‡è¡¨æ ¼ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜Table-R1æ˜¾è‘—æå‡æ¨¡å‹åœ¨è¡¨æ ¼æ¨ç†ä¸Šçš„è¡¨ç°ï¼Œç”šè‡³è¶…è¶Šäº†æ›´å¤§çš„ç‰¹å®šè¡¨æ ¼ç†è§£æ¨¡å‹ï¼Œæ¥è¿‘é—­æºæ¨¡å‹GPT-4oçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è¡¨æ ¼ç†è§£æ–¹æ³•é¢ä¸´ç€å¤æ‚è¡¨æ ¼ç»“æ„å’Œå¤æ‚é€»è¾‘æ¨ç†çš„æŒ‘æˆ˜ã€‚è™½ç„¶ç›‘ç£å¾®è°ƒ(SFT)åœ¨ç°æœ‰ç ”ç©¶ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†å¼ºåŒ–å­¦ä¹ (RL)ï¼Œå¦‚Group Relative Policy Optimization (GRPO)ï¼Œå·²ç»æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨è¡¨æ ¼ä¸Šä¸‹æ–‡ä¸­ï¼Œå®ƒä¹Ÿé¢ä¸´ç€åˆå§‹ç­–ç•¥å‡†ç¡®ç‡ä½å’Œç²—ç³™å¥–åŠ±çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†Table-R1ï¼Œä¸€ä¸ªä¸‰é˜¶æ®µçš„RLæ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼å¢å¼ºå¤šæ¨¡æ€è¡¨æ ¼ç†è§£ï¼š(1)é¢„çƒ­é˜¶æ®µï¼Œæ¿€å‘åˆå§‹æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼›(2)æ„ŸçŸ¥å¯¹é½GRPO (PA-GRPO)ï¼Œå®ƒé‡‡ç”¨è¿ç»­çš„æ ‘ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦(TEDS)å¥–åŠ±æ¥è¯†åˆ«è¡¨æ ¼ç»“æ„å’Œå†…å®¹ï¼›(3)æç¤ºè¡¥å…¨GRPO (HC-GRPO)ï¼Œå®ƒåˆ©ç”¨åŸºäºæç¤ºå¼•å¯¼é—®é¢˜çš„å‰©ä½™æ­¥éª¤çš„ç»†ç²’åº¦å¥–åŠ±ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒTable-R1å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ä¸Šçš„è¡¨æ ¼æ¨ç†æ€§èƒ½ï¼Œå¤§å¤§ä¼˜äºSFTå’ŒGRPOã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨Table-R1çš„Qwen2-VL-7Bè¶…è¿‡äº†æ›´å¤§çš„ç‰¹å®šè¡¨æ ¼ç†è§£æ¨¡å‹(ä¾‹å¦‚ï¼ŒTable-LLaVA 13B)ï¼Œç”šè‡³åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šå®ç°äº†ä¸é—­æºæ¨¡å‹GPT-4oç›¸å½“çš„æ€§èƒ½ï¼Œè¯æ˜äº†Table-R1çš„æ¯ä¸ªé˜¶æ®µåœ¨å…‹æœåˆå§‹åŒ–ç“¶é¢ˆå’Œå¥–åŠ±ç¨€ç–æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä»è€Œæ¨è¿›äº†é²æ£’çš„å¤šæ¨¡æ€è¡¨æ ¼ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤æ‚å¤šæ¨¡æ€è¡¨æ ¼ç†è§£é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºç›‘ç£å¾®è°ƒ(SFT)çš„æ–¹æ³•ï¼Œåœ¨å¤„ç†å¤æ‚è¡¨æ ¼ç»“æ„å’Œéœ€è¦å¤æ‚é€»è¾‘æ¨ç†çš„åœºæ™¯æ—¶è¡¨ç°ä¸è¶³ã€‚å¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶æœ‰æ½œåŠ›ï¼Œä½†å—é™äºåˆå§‹ç­–ç•¥å‡†ç¡®ç‡ä½å’Œå¥–åŠ±ä¿¡å·ç¨€ç–çš„é—®é¢˜ï¼Œéš¾ä»¥æœ‰æ•ˆè®­ç»ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ä¸ªä¸‰é˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶Table-R1ï¼Œé€æ­¥æå‡æ¨¡å‹åœ¨è¡¨æ ¼ç†è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é¦–å…ˆé€šè¿‡é¢„çƒ­é˜¶æ®µæå‡æ¨¡å‹çš„åˆå§‹æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œç„¶ååˆ©ç”¨æ„ŸçŸ¥å¯¹é½GRPOå’Œæç¤ºè¡¥å…¨GRPOä¸¤ä¸ªé˜¶æ®µï¼Œåˆ†åˆ«è§£å†³è¡¨æ ¼ç»“æ„å’Œå†…å®¹çš„è¯†åˆ«ä»¥åŠå¤æ‚æ¨ç†é—®é¢˜ã€‚è¿™ç§åˆ†é˜¶æ®µçš„æ–¹æ³•æ—¨åœ¨å…‹æœå¼ºåŒ–å­¦ä¹ ä¸­çš„åˆå§‹åŒ–ç“¶é¢ˆå’Œå¥–åŠ±ç¨€ç–æ€§é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTable-R1æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š
1. **Warm-up (é¢„çƒ­)**ï¼šåˆ©ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä½¿æ¨¡å‹å…·å¤‡åˆæ­¥çš„è¡¨æ ¼æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œä¸ºåç»­çš„å¼ºåŒ–å­¦ä¹ æä¾›ä¸€ä¸ªè¾ƒå¥½çš„åˆå§‹ç­–ç•¥ã€‚
2. **Perception Alignment GRPO (PA-GRPO)**ï¼šä½¿ç”¨è¿ç»­çš„æ ‘ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦(TEDS)ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ è¯†åˆ«è¡¨æ ¼çš„ç»“æ„å’Œå†…å®¹ã€‚TEDSå¥–åŠ±èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹å¯¹è¡¨æ ¼ç»“æ„çš„ç†è§£ç¨‹åº¦ã€‚
3. **Hint-Completion GRPO (HC-GRPO)**ï¼šåˆ©ç”¨åŸºäºæç¤ºå¼•å¯¼é—®é¢˜çš„å‰©ä½™æ­¥éª¤çš„ç»†ç²’åº¦å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹è¿›è¡Œæ›´æ·±å…¥çš„æ¨ç†ã€‚é€šè¿‡æç¤ºä¿¡æ¯ï¼Œæ¨¡å‹å¯ä»¥é€æ­¥å®Œæˆå¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œå¹¶è·å¾—ç›¸åº”çš„å¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šTable-R1çš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä¸‰é˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥åŠé’ˆå¯¹è¡¨æ ¼ç†è§£ä»»åŠ¡è®¾è®¡çš„ç‰¹å®šå¥–åŠ±å‡½æ•°ã€‚PA-GRPOé˜¶æ®µä½¿ç”¨TEDSå¥–åŠ±ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹å¯¹è¡¨æ ¼ç»“æ„çš„ç†è§£ã€‚HC-GRPOé˜¶æ®µä½¿ç”¨ç»†ç²’åº¦çš„å‰©ä½™æ­¥éª¤å¥–åŠ±ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æ¨¡å‹è¿›è¡Œå¤æ‚æ¨ç†ã€‚ä¸ä¼ ç»Ÿçš„å•é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒTable-R1èƒ½å¤Ÿæ›´å¥½åœ°å…‹æœåˆå§‹åŒ–ç“¶é¢ˆå’Œå¥–åŠ±ç¨€ç–æ€§é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼š
* **TEDSå¥–åŠ±**ï¼šä½¿ç”¨æ ‘ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦ä½œä¸ºPA-GRPOé˜¶æ®µçš„å¥–åŠ±ä¿¡å·ï¼Œç”¨äºè¡¡é‡æ¨¡å‹é¢„æµ‹çš„è¡¨æ ¼ç»“æ„ä¸çœŸå®ç»“æ„ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚TEDSå¥–åŠ±æ˜¯è¿ç»­çš„ï¼Œèƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œçš„åé¦ˆä¿¡æ¯ã€‚
* **å‰©ä½™æ­¥éª¤å¥–åŠ±**ï¼šåœ¨HC-GRPOé˜¶æ®µï¼Œæ ¹æ®æ¨¡å‹å®Œæˆæç¤ºå¼•å¯¼é—®é¢˜çš„å‰©ä½™æ­¥éª¤æ•°æ¥è®¾è®¡å¥–åŠ±ã€‚å‰©ä½™æ­¥éª¤è¶Šå°‘ï¼Œå¥–åŠ±è¶Šé«˜ï¼Œé¼“åŠ±æ¨¡å‹æ›´æœ‰æ•ˆåœ°å®Œæˆæ¨ç†ä»»åŠ¡ã€‚
* **Qwen2-VL-7B**ï¼šé€‰æ‹©Qwen2-VL-7Bä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡ŒTable-R1çš„è®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTable-R1åœ¨å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºSFTå’ŒGRPOæ–¹æ³•ã€‚ç‰¹åˆ«åœ°ï¼Œä½¿ç”¨Table-R1çš„Qwen2-VL-7Bæ¨¡å‹è¶…è¶Šäº†Table-LLaVA 13Bç­‰æ›´å¤§çš„ç‰¹å®šè¡¨æ ¼ç†è§£æ¨¡å‹ï¼Œå¹¶åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šå–å¾—äº†ä¸é—­æºæ¨¡å‹GPT-4oç›¸å½“çš„æ€§èƒ½ï¼ŒéªŒè¯äº†Table-R1æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½æ–‡æ¡£å¤„ç†ã€æ•°æ®åˆ†æã€é—®ç­”ç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è¡¨æ ¼ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°ä»è¡¨æ ¼æ•°æ®ä¸­æå–ä¿¡æ¯ï¼Œæ”¯æŒå†³ç­–åˆ¶å®šå’ŒçŸ¥è¯†å‘ç°ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„è¡¨æ ¼åœºæ™¯ï¼Œä¾‹å¦‚åŒ…å«åµŒå¥—ç»“æ„æˆ–è·¨é¡µè¡¨æ ¼çš„æ–‡æ¡£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing table understanding methods face challenges due to complex table structures and intricate logical reasoning. While supervised finetuning (SFT) dominates existing research, reinforcement learning (RL), such as Group Relative Policy Optimization (GRPO), has shown promise but struggled with low initial policy accuracy and coarse rewards in tabular contexts. In this paper, we introduce Table-R1, a three-stage RL framework that enhances multimodal table understanding through: (1) Warm-up that prompts initial perception and reasoning capabilities, (2) Perception Alignment GRPO (PA-GRPO), which employs continuous Tree-Edit-Distance Similarity (TEDS) rewards for recognizing table structures and contents, and (3) Hint-Completion GRPO (HC-GRPO), which utilizes fine-grained rewards of residual steps based on the hint-guided question. Extensive experiments demonstrate that Table-R1 can boost the model's table reasoning performance obviously on both held-in and held-out datasets, outperforming SFT and GRPO largely. Notably, Qwen2-VL-7B with Table-R1 surpasses larger specific table understanding models (e.g., Table-LLaVA 13B), even achieving comparable performance to the closed-source model GPT-4o on held-in datasets, demonstrating the efficacy of each stage of Table-R1 in overcoming initialization bottlenecks and reward sparsity, thereby advancing robust multimodal table understanding.

