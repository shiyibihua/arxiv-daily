---
layout: default
title: CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models
---

# CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06524" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.06524v1</a>
  <a href="https://arxiv.org/pdf/2508.06524.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06524v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06524v1', 'CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Lei Jiang, Fan Chen

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.CY, cs.DC, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-02

**Â§áÊ≥®**: 8 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CarbonScalingÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ¢≥Ë∂≥ËøπÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Á¢≥Ë∂≥Ëøπ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Á•ûÁªèÊâ©Â±ïÊ≥ïÂàô` `ÂèØÊåÅÁª≠AI` `ËÆ≠ÁªÉ‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÁ•ûÁªèÊâ©Â±ïÊ≥ïÂàôÊú™ËÄÉËôëÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉ‰∏≠ÁöÑÁ¢≥ÊéíÊîæÈóÆÈ¢òÔºåÂØºËá¥ÂØπÁéØÂ¢ÉÂΩ±ÂìçÁöÑÂøΩËßÜ„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫CarbonScalingÊ°ÜÊû∂ÔºåÈÄöËøáÊï¥ÂêàÂ§öÁßçÊ®°ÂûãÔºåÂÆöÈáèÂàÜÊûêÊ®°ÂûãÂáÜÁ°ÆÊÄß‰∏éÁ¢≥Ë∂≥Ëøπ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËôΩÁÑ∂ÂáÜÁ°ÆÊÄß‰∏éÁ¢≥ÊéíÊîæ‰πãÈó¥Â≠òÂú®ÂπÇÂæãÂÖ≥Á≥ªÔºå‰ΩÜÂÆûÈôÖ‰ΩéÊïàÊòæËëóÂ¢ûÂä†‰∫ÜÊâ©Â±ïÂõ†Â≠êÔºå‰ºòÂåñÊé™ÊñΩËÉΩÊúâÊïàÈôç‰ΩéÁ¢≥ÊéíÊîæ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á•ûÁªèÁΩëÁªúÊâ©Â±ïÊ≥ïÂàôÊé®Âä®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂèëÂ±ïÔºåÂ∞ÜÂáÜÁ°ÆÊÄßÊèêÂçá‰∏éÂèÇÊï∞Êï∞Èáè„ÄÅÊï∞ÊçÆÈõÜËßÑÊ®°ÂíåËÆ°ÁÆóËÉΩÂäõÁöÑÂ¢ûÈïøËÅîÁ≥ªËµ∑Êù•„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ≥ïÂàôÂøΩËßÜ‰∫Ü‰∏éLLMËßÑÊ®°ÂëàÊåáÊï∞Â¢ûÈïøÁöÑÁ¢≥ÊéíÊîæ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜCarbonScalingÔºå‰∏Ä‰∏™ÂàÜÊûêÊ°ÜÊû∂ÔºåÊâ©Â±ï‰∫ÜÁ•ûÁªèÊâ©Â±ïÊ≥ïÂàôÔºå‰ª•Á∫≥ÂÖ•LLMËÆ≠ÁªÉ‰∏≠ÁöÑÊìç‰ΩúÁ¢≥ÂíåÈöêÂê´Á¢≥„ÄÇÈÄöËøáÊï¥ÂêàÁ•ûÁªèÊâ©Â±ïÊ®°Âûã„ÄÅGPUÁ°¨‰ª∂ÊºîÂèò„ÄÅÂπ∂Ë°å‰ºòÂåñÂíåÁ¢≥‰º∞ÁÆóÔºåCarbonScalingÂÆöÈáèËøûÊé•‰∫ÜÊ®°ÂûãÂáÜÁ°ÆÊÄß‰∏éÁ¢≥Ë∂≥Ëøπ„ÄÇÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°ÂáÜÁ°ÆÊÄß‰∏éÁ¢≥‰πãÈó¥Â≠òÂú®ÂπÇÂæãÂÖ≥Á≥ªÔºå‰ΩÜÁé∞ÂÆû‰∏≠ÁöÑ‰ΩéÊïàÊòæËëóÂ¢ûÂä†‰∫ÜÊâ©Â±ïÂõ†Â≠ê„ÄÇÁ°¨‰ª∂ÊäÄÊúØÊâ©Â±ïÈôç‰Ωé‰∫ÜÂ∞èÂà∞‰∏≠ÂûãÊ®°ÂûãÁöÑÁ¢≥ÊéíÊîæÔºå‰ΩÜÂØπ‰∫éÊûÅÂ§ßÂûãLLMsÔºåÁî±‰∫éÈÄö‰ø°ÂºÄÈîÄÂíåGPUÂà©Áî®‰∏çË∂≥ÔºåÊî∂ÁõäÈÄíÂáè„ÄÇËÆ≠ÁªÉ‰ºòÂåñÔºåÁâπÂà´ÊòØÊøÄËøõÁöÑÂÖ≥ÈîÆÊâπÈáèÂ§ßÂ∞èÊâ©Â±ïÔºåÊúâÂä©‰∫éÁºìËß£ËøôÁßç‰ΩéÊïà„ÄÇCarbonScaling‰∏∫ËÆ≠ÁªÉÊõ¥ÂèØÊåÅÁª≠ÂíåÁ¢≥È´òÊïàÁöÑLLMsÊèê‰æõ‰∫ÜÂÖ≥ÈîÆËßÅËß£„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉËøáÁ®ã‰∏≠Á¢≥ÊéíÊîæÊú™Ë¢´ÂÖÖÂàÜËÄÉËôëÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÈáèÂåñÊ®°ÂûãËßÑÊ®°‰∏éÁ¢≥Ë∂≥Ëøπ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂØºËá¥ÁéØÂ¢ÉÂΩ±ÂìçËØÑ‰º∞‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCarbonScalingÊ°ÜÊû∂ÈÄöËøáÊï¥ÂêàÁ•ûÁªèÊâ©Â±ïÊ≥ïÂàô„ÄÅGPUÁ°¨‰ª∂ÊºîÂèò„ÄÅÂπ∂Ë°å‰ºòÂåñÂíåÁ¢≥‰º∞ÁÆóÊ®°ÂûãÔºåÂª∫Á´ã‰∫ÜÊ®°ÂûãÂáÜÁ°ÆÊÄß‰∏éÁ¢≥ÊéíÊîæ‰πãÈó¥ÁöÑÂÆöÈáèËÅîÁ≥ªÔºå‰ªéËÄå‰∏∫ÂèØÊåÅÁª≠ËÆ≠ÁªÉÊèê‰æõÊåáÂØº„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÊã¨Âõõ‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Á•ûÁªèÊâ©Â±ïÊ®°ÂûãÔºåÂàÜÊûêÂèÇÊï∞‰∏éÂáÜÁ°ÆÊÄßÁöÑÂÖ≥Á≥ªÔºõ2) GPUÁ°¨‰ª∂ÊºîÂèòÊ®°ÂûãÔºåËØÑ‰º∞ÊäÄÊúØËøõÊ≠•ÂØπÁ¢≥ÊéíÊîæÁöÑÂΩ±ÂìçÔºõ3) Âπ∂Ë°å‰ºòÂåñÊ®°ÂùóÔºå‰ºòÂåñËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑËµÑÊ∫êÂà©Áî®Ôºõ4) Á¢≥‰º∞ÁÆóÊ®°ÂûãÔºåÈáèÂåñËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÁ¢≥ÊéíÊîæ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCarbonScalingÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜÁ¢≥ÊéíÊîæÁ∫≥ÂÖ•Á•ûÁªèÊâ©Â±ïÊ≥ïÂàôÁöÑÂàÜÊûê‰∏≠ÔºåÊè≠Á§∫‰∫ÜÂú®‰∏çÂêåËßÑÊ®°Ê®°Âûã‰∏≠Á¢≥ÊéíÊîæÁöÑÈùûÁ∫øÊÄßÁâπÂæÅÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõ‰∫ÜÊõ¥ÂÖ®Èù¢ÁöÑÁéØÂ¢ÉÂΩ±ÂìçËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÂÖ≥ÈîÆÊâπÈáèÂ§ßÂ∞èÊâ©Â±ïÁ≠ñÁï•Ôºå‰ª•ÊèêÈ´òËÆ≠ÁªÉÊïàÁéáÂπ∂Èôç‰ΩéÁ¢≥ÊéíÊîæ„ÄÇÊ≠§Â§ñÔºåÊ®°Âûã‰∏≠ËøòËÄÉËôë‰∫ÜGPUÂà©Áî®ÁéáÂíåÈÄö‰ø°ÂºÄÈîÄÁ≠âÂõ†Á¥†Ôºå‰ª•‰ºòÂåñÊï¥‰ΩìËÆ≠ÁªÉËøáÁ®ã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°ÂáÜÁ°ÆÊÄß‰∏éÁ¢≥ÊéíÊîæ‰πãÈó¥Â≠òÂú®ÂπÇÂæãÂÖ≥Á≥ªÔºå‰ΩÜÂÆûÈôÖÁöÑ‰ΩéÊïà‰ΩøÂæóÊâ©Â±ïÂõ†Â≠êÊòæËëóÂ¢ûÂä†„ÄÇÈÄöËøá‰ºòÂåñËÆ≠ÁªÉËøáÁ®ãÔºåÂ∞§ÂÖ∂ÊòØÊâπÈáèÂ§ßÂ∞èÁöÑË∞ÉÊï¥ÔºåËÉΩÂ§üÊúâÊïàÈôç‰ΩéÁ¢≥ÊéíÊîæÔºåÊèêÂçáÊ®°ÂûãÁöÑÁéØÂ¢ÉÂèãÂ•ΩÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CarbonScalingÊ°ÜÊû∂ÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉ‰∏é‰ºòÂåñÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅËÄÉËôëÁéØÂ¢ÉÂΩ±ÂìçÁöÑÂú∫ÊôØ‰∏≠„ÄÇËØ•Á†îÁ©∂‰∏∫ÂºÄÂèëÊõ¥ÂèØÊåÅÁª≠ÁöÑAIÁ≥ªÁªüÊèê‰æõ‰∫ÜÁêÜËÆ∫Âü∫Á°ÄÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Ë°å‰∏öÊ†áÂáÜÁöÑÂà∂ÂÆöÔºå‰øÉËøõÁªøËâ≤ËÆ°ÁÆóÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Neural scaling laws have driven the development of increasingly large language models (LLMs) by linking accuracy improvements to growth in parameter count, dataset size, and compute. However, these laws overlook the carbon emissions that scale exponentially with LLM size. This paper presents \textit{CarbonScaling}, an analytical framework that extends neural scaling laws to incorporate both operational and embodied carbon in LLM training. By integrating models for neural scaling, GPU hardware evolution, parallelism optimization, and carbon estimation, \textit{CarbonScaling} quantitatively connects model accuracy to carbon footprint. Results show that while a power-law relationship between accuracy and carbon holds, real-world inefficiencies significantly increase the scaling factor. Hardware technology scaling reduces carbon emissions for small to mid-sized models, but offers diminishing returns for extremely large LLMs due to communication overhead and underutilized GPUs. Training optimizations-especially aggressive critical batch size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers key insights for training more sustainable and carbon-efficient LLMs.

