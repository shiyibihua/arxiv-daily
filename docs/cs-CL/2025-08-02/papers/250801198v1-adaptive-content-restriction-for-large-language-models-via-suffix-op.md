---
layout: default
title: Adaptive Content Restriction for Large Language Models via Suffix Optimization
---

# Adaptive Content Restriction for Large Language Models via Suffix Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01198" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.01198v1</a>
  <a href="https://arxiv.org/pdf/2508.01198.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01198v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01198v1', 'Adaptive Content Restriction for Large Language Models via Suffix Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yige Li, Peihai Jiang, Jun Sun, Peng Shu, Tianming Liu, Zhen Xiang

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-02

**Â§áÊ≥®**: 19 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ëá™ÈÄÇÂ∫îÂÜÖÂÆπÈôêÂà∂ÊñπÊ≥ï‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÊúâÂÆ≥ÂÜÖÂÆπÁöÑÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ëá™ÈÄÇÂ∫îÂÜÖÂÆπÈôêÂà∂` `ÂêéÁºÄ‰ºòÂåñ` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÂÜÖÂÆπÁîüÊàê` `Ê®°ÂûãÂØπÈΩê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â∫îÂØπÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÊúâÂÆ≥ÂÜÖÂÆπÊó∂Èù¢‰∏¥È´òËÆ°ÁÆóÂíåÂ≠òÂÇ®ÈúÄÊ±ÇÔºå‰∏îÈöæ‰ª•ÈÄÇÂ∫î‰∏çÂêåÁî®Êà∑ÁöÑÂø´ÈÄüÂèòÂåñÈúÄÊ±Ç„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Ëá™ÈÄÇÂ∫îÂÜÖÂÆπÈôêÂà∂ÔºàAdaCoReÔºâ‰ªªÂä°ÔºåÂà©Áî®ÂêéÁºÄ‰ºòÂåñÔºàSOPÔºâÊñπÊ≥ïÔºåËΩªÈáèÂåñÂú∞Èò≤Ê≠¢ÁîüÊàêÈôêÂà∂ËØçËÄåÊó†ÈúÄÊ®°ÂûãÂæÆË∞É„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSOPÂú®Â§ö‰∏™Â§ßËØ≠Ë®ÄÊ®°Âûã‰∏äÂπ≥ÂùáÊèêÂçáÈôêÂà∂Áéá15%-17%ÔºåÂπ∂Âú®ÂÆûÈôÖÂ∫îÁî®Âπ≥Âè∞‰∏äÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öÁßçÂ∫îÁî®‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÁî±‰∫éÂÖ∂ËæìÂá∫Á©∫Èó¥ÂπøÊ≥õÔºåÂÜÖÂÆπÈôêÂà∂‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçÂ§ßÊåëÊàò„ÄÇÁé∞ÊúâÁöÑÊ®°ÂûãÂØπÈΩêÊñπÊ≥ïÔºåÂ¶ÇÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÔºåÂú®Â∫îÂØπ‰∏çÂêåÁî®Êà∑Áæ§‰ΩìÁöÑÂÜÖÂÆπÈôêÂà∂ÈúÄÊ±ÇÊó∂ÊòæÂæó‰∏çÂ§üÁÅµÊ¥ª‰∏îËÆ°ÁÆóËµÑÊ∫êÊ∂àËÄóÂ§ß„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ªªÂä°‚Äî‚ÄîËá™ÈÄÇÂ∫îÂÜÖÂÆπÈôêÂà∂ÔºàAdaCoReÔºâÔºåÂπ∂ÊèêÂá∫‰∫ÜÈ¶ñ‰∏™ÊñπÊ≥ï‚Äî‚ÄîÂêéÁºÄ‰ºòÂåñÔºàSOPÔºâÔºåÈÄöËøáÂú®ÊèêÁ§∫ÂêéÈôÑÂä†‰ºòÂåñÂêéÁöÑÁü≠ÂêéÁºÄÔºåÊù•Èò≤Ê≠¢ÁîüÊàêÁâπÂÆöÁöÑÈôêÂà∂ËØçÔºåÂêåÊó∂‰øùÊåÅËæìÂá∫Ë¥®Èáè„ÄÇÊàë‰ª¨ËøòÂàõÂª∫‰∫ÜÂÜÖÂÆπÈôêÂà∂Âü∫ÂáÜÔºàCoReBenchÔºâÊù•ËØÑ‰º∞AdaCoReÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéSOPÂú®Â§ö‰∏™Ê®°Âûã‰∏äÂùá‰ºò‰∫éÁ≥ªÁªüÁ∫ßÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÁîüÊàêÂÜÖÂÆπÊó∂ÁöÑÊúâÂÆ≥ÊÄßÈóÆÈ¢òÔºåÁé∞ÊúâÁöÑÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ïÂú®ËÆ°ÁÆóÂíåÊï∞ÊçÆÈúÄÊ±Ç‰∏äËøá‰∫éÂ∫ûÂ§ßÔºåÈöæ‰ª•Êª°Ë∂≥‰∏çÂêåÁî®Êà∑ÁöÑÁâπÂÆöÈúÄÊ±Ç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫Ëá™ÈÄÇÂ∫îÂÜÖÂÆπÈôêÂà∂ÔºàAdaCoReÔºâ‰ªªÂä°ÔºåËÆæËÆ°ÂêéÁºÄ‰ºòÂåñÔºàSOPÔºâÊñπÊ≥ïÔºåÈÄöËøáÂú®ËæìÂÖ•ÊèêÁ§∫ÂêéÈôÑÂä†‰ºòÂåñÂêéÁöÑÁü≠ÂêéÁºÄÔºåÊù•ÊúâÊïàÈò≤Ê≠¢ÁîüÊàêÁâπÂÆöÁöÑÈôêÂà∂ËØçÔºåÂêåÊó∂‰øùÊåÅËæìÂá∫ÁöÑË¥®Èáè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨ËæìÂÖ•ÊèêÁ§∫ÁöÑÊé•Êî∂„ÄÅÂêéÁºÄÁöÑÁîüÊàê‰∏éÈôÑÂä†„ÄÅ‰ª•ÂèäÊúÄÁªàËæìÂá∫ÁöÑÁîüÊàê„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨ÊèêÁ§∫Â§ÑÁêÜÊ®°Âùó„ÄÅÂêéÁºÄ‰ºòÂåñÊ®°ÂùóÂíåËæìÂá∫ÁîüÊàêÊ®°Âùó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSOPÊñπÊ≥ïÊòØÈ¶ñ‰∏™ÈíàÂØπËá™ÈÄÇÂ∫îÂÜÖÂÆπÈôêÂà∂ÁöÑËΩªÈáèÂåñÁ≠ñÁï•ÔºåÂå∫Âà´‰∫é‰º†ÁªüÁöÑÂæÆË∞ÉÊñπÊ≥ïÔºåÈÅøÂÖç‰∫ÜÈ´òÊòÇÁöÑËÆ°ÁÆóÂíåÂ≠òÂÇ®ÊàêÊú¨„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂêéÁºÄ‰ºòÂåñËøáÁ®ã‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•Âπ≥Ë°°ÁîüÊàêÂÜÖÂÆπÁöÑË¥®Èáè‰∏éÈôêÂà∂ÊïàÊûúÔºåÂêåÊó∂ËÆæËÆ°‰∫ÜÈ´òÊïàÁöÑÂèÇÊï∞ËÆæÁΩÆ‰ª•Á°Æ‰øù‰ºòÂåñËøáÁ®ãÁöÑÂø´ÈÄüÊî∂Êïõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSOPÂú®Gemma2-2B„ÄÅMistral-7B„ÄÅVicuna-7B„ÄÅLlama3-8BÂíåLlama3.1-8BÁ≠âÊ®°Âûã‰∏äÔºåÂπ≥ÂùáÈôêÂà∂ÁéáÂàÜÂà´ÊèêÂçá‰∫Ü15%„ÄÅ17%„ÄÅ10%„ÄÅ9%Âíå6%ÔºåÊòæËëó‰ºò‰∫éÁ≥ªÁªüÁ∫ßÂü∫Á∫øÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß‰∏éÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Á§æ‰∫§Â™í‰Ωì„ÄÅÂú®Á∫øÂÜÖÂÆπÁîüÊàêÂíåÊïôËÇ≤Á≠âÂ§ö‰∏™Âú∫ÊôØÔºåËÉΩÂ§üÊúâÊïàÈò≤Ê≠¢ÊúâÂÆ≥ÂÜÖÂÆπÁöÑÁîüÊàêÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™å‰∏éÂÆâÂÖ®ÊÄß„ÄÇÊú™Êù•ÔºåÈöèÁùÄÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπøÊ≥õÂ∫îÁî®ÔºåËØ•ÊñπÊ≥ïÂ∞ÜÂØπÂÜÖÂÆπÁõëÁÆ°ÂíåÈÅìÂæ∑ËßÑËåÉÁöÑÂÆûÊñΩ‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) have demonstrated significant success across diverse applications. However, enforcing content restrictions remains a significant challenge due to their expansive output space. One aspect of content restriction is preventing LLMs from generating harmful content via model alignment approaches such as supervised fine-tuning (SFT). Yet, the need for content restriction may vary significantly across user groups, change rapidly over time, and not always align with general definitions of harmfulness. Applying SFT to each of these specific use cases is impractical due to the high computational, data, and storage demands. Motivated by this need, we propose a new task called \textit{Adaptive Content Restriction} (AdaCoRe), which focuses on lightweight strategies -- methods without model fine-tuning -- to prevent deployed LLMs from generating restricted terms for specific use cases. We propose the first method for AdaCoRe, named \textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to any prompt to a) prevent a target LLM from generating a set of restricted terms, while b) preserving the output quality. To evaluate AdaCoRe approaches, including our SOP, we create a new \textit{Content Restriction Benchmark} (CoReBench), which contains 400 prompts for 80 restricted terms across 8 carefully selected categories. We demonstrate the effectiveness of SOP on CoReBench, which outperforms the system-level baselines such as system suffix by 15\%, 17\%, 10\%, 9\%, and 6\% on average restriction rates for Gemma2-2B, Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also demonstrate that SOP is effective on POE, an online platform hosting various commercial LLMs, highlighting its practicality in real-world scenarios.

