---
layout: default
title: Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models
---

# Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12461" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12461v3</a>
  <a href="https://arxiv.org/pdf/2508.12461.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12461v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12461v3', 'Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI\'s Latest Open Source Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziqian Bi, Keyu Chen, Chiung-Yi Tseng, Danyang Zhang, Tianyang Wang, Hongying Luo, Lu Chen, Junming Huang, Jibin Guan, Junfeng Hao, Xinyuan Song, Junhao Song

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-17 (æ›´æ–°: 2025-12-13)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://ai-agent-lab.github.io/gpt-oss)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°OpenAIçš„GPT-OSSæ¨¡å‹åœ¨å¤šä»»åŠ¡ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼€æºæ¨¡å‹` `ä¸“å®¶æ··åˆæ¶æ„` `å¤šä»»åŠ¡å­¦ä¹ ` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä»»åŠ¡å¤„ç†ä¸Šå­˜åœ¨æ€§èƒ½ä¸å‡è¡¡çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°è¾ƒå¼±ã€‚
2. è®ºæ–‡æå‡ºäº†GPT-OSSæ¨¡å‹ï¼Œé€šè¿‡ä¸“å®¶æ··åˆæ¶æ„ä¼˜åŒ–å‚æ•°ä½¿ç”¨ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨å¤šä»»åŠ¡ä¸­çš„è¡¨ç°å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œgpt-oss-20Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºgpt-oss-120Bï¼Œä¸”åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰ç›¸å¯¹ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

2025å¹´8æœˆï¼ŒOpenAIå‘å¸ƒäº†GPT-OSSæ¨¡å‹ï¼Œè¿™æ˜¯è‡ª2019å¹´GPT-2ä»¥æ¥é¦–ä¸ªå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å«120Bå’Œ20Bå‚æ•°çš„ä¸¤ç§ä¸“å®¶æ··åˆæ¶æ„ã€‚æˆ‘ä»¬å¯¹è¿™ä¸¤ç§å˜ä½“è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¯”è¾ƒäº†å…­ç§å½“ä»£å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ¶µç›–äº†ä»14.7Båˆ°235Bå‚æ•°çš„å¯†é›†å’Œç¨€ç–è®¾è®¡ï¼Œæ¶‰åŠåä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸€èˆ¬çŸ¥è¯†ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€å¤šè¯­è¨€ç†è§£å’Œå¯¹è¯èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œgpt-oss-20Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºgpt-oss-120Bï¼Œå°½ç®¡å…¶æ¯æ¬¡å“åº”æ‰€éœ€çš„å†…å­˜å’Œèƒ½é‡æ˜¾è‘—æ›´å°‘ã€‚è¿™äº›å‘ç°ä¸ºç¨€ç–æ¶æ„çš„æ‰©å±•ä¸æ€§èƒ½æå‡ä¹‹é—´çš„å…³ç³»æä¾›äº†å®è¯ä¾æ®ï¼Œå¼ºè°ƒäº†ä¼˜åŒ–ç­–ç•¥çš„è¿›ä¸€æ­¥ç ”ç©¶éœ€æ±‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä»»åŠ¡å¤„ç†ä¸­çš„æ€§èƒ½ä¸å‡è¡¡é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè¯­è¨€ç†è§£å’Œä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸­çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•åœ¨ç¨€ç–æ¶æ„çš„æ‰©å±•ä¸Šæœªèƒ½å®ç°é¢„æœŸçš„æ€§èƒ½æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ä¸“å®¶æ··åˆæ¶æ„ï¼Œä¼˜åŒ–æ¨¡å‹å‚æ•°çš„ä½¿ç”¨ï¼Œæå‡æ¨¡å‹åœ¨å¤šä»»åŠ¡ä¸­çš„è¡¨ç°å’Œæ•ˆç‡ã€‚è¯¥è®¾è®¡æ—¨åœ¨å‡å°‘å†…å­˜å’Œèƒ½é‡æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œåˆ†åˆ«ä¸º120Bå’Œ20Bå‚æ•°ã€‚æ¨¡å‹åœ¨æ ‡å‡†åŒ–çš„æ¨ç†è®¾ç½®ä¸‹è¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–å¤šä¸ªåŸºå‡†æµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé‡‡ç”¨äº†ç¨€ç–æ¶æ„çš„ä¸“å®¶æ··åˆè®¾è®¡ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¿æŒè¾ƒä½èµ„æºæ¶ˆè€—çš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨ç‰¹å®šä»»åŠ¡ä¸Šå®ç°æ›´ä¼˜çš„æ€§èƒ½ã€‚ä¸ç°æœ‰å¯†é›†æ¨¡å‹ç›¸æ¯”ï¼Œè¿™ç§è®¾è®¡åœ¨èµ„æºåˆ©ç”¨ä¸Šæ›´ä¸ºé«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨å¤šä»»åŠ¡å¤„ç†ä¸­çš„è¡¨ç°ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚å’Œå‚æ•°é…ç½®åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œgpt-oss-20Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºgpt-oss-120Bï¼Œå°¤å…¶åœ¨HumanEvalå’ŒMMLUç­‰ä»»åŠ¡ä¸­ï¼Œå°½ç®¡å…¶å†…å­˜å’Œèƒ½é‡æ¶ˆè€—æ˜¾è‘—æ›´ä½ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†ç¨€ç–æ¶æ„åœ¨æ€§èƒ½ä¼˜åŒ–ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œå¤šè¯­è¨€ç¿»è¯‘ç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½å’Œèµ„æºæ¶ˆè€—ï¼ŒGPT-OSSæ¨¡å‹èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemars test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments. More details and evaluation scripts are available at https://ai-agent-lab.github.io/gpt-oss (Project Webpage).

