---
layout: default
title: What do Speech Foundation Models Learn? Analysis and Applications
---

# What do Speech Foundation Models Learn? Analysis and Applications

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12255" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12255v1</a>
  <a href="https://arxiv.org/pdf/2508.12255.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12255v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12255v1', 'What do Speech Foundation Models Learn? Analysis and Applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ankita Pasad

**åˆ†ç±»**: cs.CL, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-08-17

**å¤‡æ³¨**: Ph.D. Thesis

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§åˆ†ææ¡†æ¶ä»¥æå‡è¯­éŸ³åŸºç¡€æ¨¡å‹çš„ç†è§£ä¸åº”ç”¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³åŸºç¡€æ¨¡å‹` `è‡ªç›‘ç£å­¦ä¹ ` `å£è¯­ç†è§£` `å‘½åå®ä½“è¯†åˆ«` `ç»Ÿè®¡åˆ†æ` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­éŸ³åŸºç¡€æ¨¡å‹è™½ç„¶åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å¯¹å…¶æ‰€å­¦çŸ¥è¯†çš„ç†è§£ä»ç„¶ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å£è¯­ç†è§£ä»»åŠ¡ä¸Šã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„åˆ†ææ¡†æ¶ï¼Œåˆ©ç”¨ç»Ÿè®¡å·¥å…·å’Œæ— è®­ç»ƒä»»åŠ¡æ¥æ¢è®¨SFMä¸­ç¼–ç çš„å£°å­¦å’Œè¯­è¨€çŸ¥è¯†ã€‚
3. é€šè¿‡å¯¹SFMçš„æ¯”è¾ƒç ”ç©¶ï¼Œå‘ç°åŸºäºSFMçš„ç«¯åˆ°ç«¯æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«å’Œå®šä½ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„çº§è”æ–¹æ³•ï¼Œæå‡äº†ä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰æ—¨åœ¨ä¸ºå¤šç§è¯­éŸ³å¤„ç†ä»»åŠ¡æä¾›é€šç”¨è¡¨ç¤ºã€‚å°½ç®¡è¿‘å¹´æ¥è‡ªç›‘ç£å’Œç›‘ç£é¢„è®­ç»ƒæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¯¹å…¶æ‰€å­¦çŸ¥è¯†çš„ç†è§£ä»æ˜¾ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§åˆ†ææ¡†æ¶ï¼Œåˆ©ç”¨ç»Ÿè®¡å·¥å…·å’Œæ— è®­ç»ƒä»»åŠ¡ï¼Œæ¢è®¨SFMå±‚ä¸­ç¼–ç çš„å£°å­¦å’Œè¯­è¨€çŸ¥è¯†ã€‚é€šè¿‡å¯¹å¤šç§SFMå’Œç»Ÿè®¡å·¥å…·çš„æ¯”è¾ƒç ”ç©¶ï¼Œå‘ç°åˆ†æç»“æœå¯¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å…·æœ‰å®é™…å½±å“ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä¸ºå£è¯­ç†è§£ä»»åŠ¡è´¡çŒ®äº†æ–°çš„æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†åŸºäºSFMçš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œå‘½åå®ä½“å®šä½ï¼ˆNELï¼‰æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œç«¯åˆ°ç«¯æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šä¼˜äºä¼ ç»Ÿçº§è”æ–¹æ³•ã€‚æ•´ä½“è€Œè¨€ï¼Œæœ¬æ–‡ä¸ºSFMçš„ç†è§£å’Œæœªæ¥æ¨¡å‹è®¾è®¡æä¾›äº†å·¥å…·å’Œæ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¯¹è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰æ‰€å­¦çŸ¥è¯†çš„ç†è§£ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å£è¯­ç†è§£ä»»åŠ¡ï¼ˆSLUï¼‰ä¸­çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•å¯¹SLUçš„æ¢ç´¢æœ‰é™ï¼Œä¸»è¦ç”±äºç¼ºä¹ç›¸å…³æ•°æ®é›†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§è½»é‡çº§çš„åˆ†ææ¡†æ¶ï¼Œç»“åˆç»Ÿè®¡å·¥å…·å’Œæ— è®­ç»ƒä»»åŠ¡ï¼Œæ·±å…¥åˆ†æSFMä¸­å£°å­¦å’Œè¯­è¨€çŸ¥è¯†çš„ç¼–ç ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ­ç¤ºSFMçš„å†…éƒ¨æœºåˆ¶ï¼Œå¹¶ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€SFMåˆ†æã€ä»»åŠ¡è®¾è®¡å’Œæ€§èƒ½è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ„å»ºæ–°çš„SLUæ•°æ®é›†ï¼Œç„¶ååˆ©ç”¨ç»Ÿè®¡å·¥å…·åˆ†æSFMçš„å±‚çº§çŸ¥è¯†ï¼Œæœ€åå¼€å‘åŸºäºSFMçš„NERå’ŒNELæ–¹æ³•è¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„åˆ†ææ¡†æ¶ï¼Œèƒ½å¤Ÿæ­ç¤ºSFMä¸­å£°å­¦å’Œè¯­è¨€çŸ¥è¯†çš„å…·ä½“ç¼–ç æ–¹å¼ï¼Œå¹¶é€šè¿‡æ–°çš„ä»»åŠ¡è®¾è®¡æ¨åŠ¨SLUé¢†åŸŸçš„å‘å±•ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶æä¾›äº†æ›´æ·±å…¥çš„ç†è§£å’Œæ›´é«˜çš„ä»»åŠ¡æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨NERå’ŒNELä»»åŠ¡ä¸­ï¼Œé‡‡ç”¨äº†ç«¯åˆ°ç«¯æ¨¡å‹æ¶æ„ï¼Œç»“åˆSFMçš„ç‰¹å¾è¡¨ç¤ºï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºSFMçš„ç«¯åˆ°ç«¯æ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«å’Œå®šä½ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„çº§è”æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œå±•ç¤ºäº†SFMåœ¨å£è¯­ç†è§£ä»»åŠ¡ä¸­çš„æ½œåŠ›å’Œåº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¯­éŸ³åŠ©æ‰‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå’Œæ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡æå‡è¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨å£è¯­ç†è§£ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ç²¾å‡†çš„è¯­éŸ³äº¤äº’ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å•†ä¸šåŒ–åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Speech foundation models (SFMs) are designed to serve as general-purpose representations for a wide range of speech-processing tasks. The last five years have seen an influx of increasingly successful self-supervised and supervised pre-trained models with impressive performance on various downstream tasks.
>   Although the zoo of SFMs continues to grow, our understanding of the knowledge they acquire lags behind. This thesis presents a lightweight analysis framework using statistical tools and training-free tasks to investigate the acoustic and linguistic knowledge encoded in SFM layers. We conduct a comparative study across multiple SFMs and statistical tools. Our study also shows that the analytical insights have concrete implications for downstream task performance.
>   The effectiveness of an SFM is ultimately determined by its performance on speech applications. Yet it remains unclear whether the benefits extend to spoken language understanding (SLU) tasks that require a deeper understanding than widely studied ones, such as speech recognition. The limited exploration of SLU is primarily due to a lack of relevant datasets. To alleviate that, this thesis contributes tasks, specifically spoken named entity recognition (NER) and named entity localization (NEL), to the Spoken Language Understanding Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded (speech recognition followed by a text model) approaches. Further, we evaluate E2E SLU models across SFMs and adaptation strategies to assess the impact on task performance.
>   Collectively, this thesis tackles previously unanswered questions about SFMs, providing tools and datasets to further our understanding and to enable the community to make informed design choices for future model development and adoption.

