---
layout: default
title: The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping
---

# The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12482" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12482v1</a>
  <a href="https://arxiv.org/pdf/2508.12482.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12482v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12482v1', 'The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaomeng Zhu, R. Thomas McCoy, Robert Frank

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¥æ³•å¼•å¯¼å¯¹åŠ¨è¯æ„ä¹‰å­¦ä¹ çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥æ³•å¼•å¯¼` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŠ¨è¯å­¦ä¹ ` `RoBERTa` `GPT-2` `è‡ªç„¶è¯­è¨€å¤„ç†` `å„¿ç«¥è¯­è¨€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰ç ”ç©¶æœªå……åˆ†æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨è¯æ„ä¹‰å­¦ä¹ ä¸­çš„å¥æ³•å¼•å¯¼ä½œç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¯¹RoBERTaå’ŒGPT-2è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨æ‰°åŠ¨æ•°æ®é›†ä»¥æ¶ˆé™¤å¥æ³•ä¿¡æ¯ï¼Œè§‚å¯Ÿæ¨¡å‹è¡¨ç°çš„å˜åŒ–ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šç»“æœæ˜¾ç¤ºå¥æ³•ä¿¡æ¯çš„ç¼ºå¤±å¯¹åŠ¨è¯è¡¨ç¤ºçš„å½±å“æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯å¿ƒç†åŠ¨è¯çš„è¡¨ç°æ›´ä¸ºè„†å¼±ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥æ³•å¼•å¯¼å‡è®¾è®¤ä¸ºå„¿ç«¥é€šè¿‡åŠ¨è¯å‡ºç°çš„å¥æ³•ç¯å¢ƒæ¥å­¦ä¹ å…¶æ„ä¹‰ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦è¡¨ç°å‡ºç±»ä¼¼è¡Œä¸ºã€‚é€šè¿‡å¯¹RoBERTaå’ŒGPT-2è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨äº†æ‰°åŠ¨æ•°æ®é›†ä»¥æ¶ˆé™¤å¥æ³•ä¿¡æ¯ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å¥æ³•çº¿ç´¢è¢«ç§»é™¤æ—¶ï¼Œæ¨¡å‹çš„åŠ¨è¯è¡¨ç¤ºæ¯”å½“å…±ç°ä¿¡æ¯è¢«ç§»é™¤æ—¶æ›´ä¸ºé€€åŒ–ã€‚æ­¤å¤–ï¼Œå¿ƒç†åŠ¨è¯çš„è¡¨ç¤ºåœ¨è¿™ç§è®­ç»ƒæ¨¡å¼ä¸‹å—åˆ°çš„è´Ÿé¢å½±å“å¤§äºç‰©ç†åŠ¨è¯ã€‚è¿™äº›å‘ç°ä¸ä»…å¼ºåŒ–äº†å¥æ³•å¼•å¯¼åœ¨åŠ¨è¯å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Œè¿˜å±•ç¤ºäº†é€šè¿‡æ“æ§å¤§å‹è¯­è¨€æ¨¡å‹çš„å­¦ä¹ ç¯å¢ƒæ¥æµ‹è¯•å‘å±•å‡è®¾çš„å¯è¡Œæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿé€šè¿‡å¥æ³•ç¯å¢ƒæ¥å­¦ä¹ åŠ¨è¯çš„æ„ä¹‰ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†éªŒè¯è¿™ä¸€å‡è®¾çš„æœ‰æ•ˆæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹RoBERTaå’ŒGPT-2æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨æ‰°åŠ¨æ•°æ®é›†æ¥æ¶ˆé™¤å¥æ³•ä¿¡æ¯ï¼Œä»è€Œè§‚å¯Ÿæ¨¡å‹åœ¨åŠ¨è¯å­¦ä¹ ä¸­çš„è¡¨ç°å˜åŒ–ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æ¨¡æ‹Ÿå„¿ç«¥å­¦ä¹ åŠ¨è¯çš„è¿‡ç¨‹ï¼ŒéªŒè¯å¥æ³•å¼•å¯¼çš„ä½œç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é¢„å¤„ç†é˜¶æ®µæ¶‰åŠå¯¹å¥æ³•ä¿¡æ¯çš„æ‰°åŠ¨ï¼Œæ¨¡å‹è®­ç»ƒé˜¶æ®µä½¿ç”¨RoBERTaå’ŒGPT-2è¿›è¡Œè®­ç»ƒï¼Œç»“æœåˆ†æé˜¶æ®µåˆ™è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡å°†å¥æ³•å¼•å¯¼çš„æ¦‚å¿µåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œæ­ç¤ºäº†å¥æ³•ä¿¡æ¯å¯¹åŠ¨è¯å­¦ä¹ çš„é‡è¦æ€§ï¼Œä¸ä¼ ç»Ÿçš„å…±ç°ä¿¡æ¯å­¦ä¹ å½¢æˆå¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„æ‰°åŠ¨å‚æ•°ï¼Œä»¥æ§åˆ¶å¥æ³•å’Œå…±ç°ä¿¡æ¯çš„å½±å“ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œç½‘ç»œç»“æ„åŸºäºRoBERTaå’ŒGPT-2çš„æ ‡å‡†æ¶æ„ï¼Œç¡®ä¿äº†å®éªŒçš„å¯é‡å¤æ€§å’Œç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å¥æ³•çº¿ç´¢è¢«ç§»é™¤æ—¶ï¼Œæ¨¡å‹çš„åŠ¨è¯è¡¨ç¤ºé€€åŒ–ç¨‹åº¦æ˜¾è‘—é«˜äºå…±ç°ä¿¡æ¯çš„ç§»é™¤ï¼Œå°¤å…¶æ˜¯å¿ƒç†åŠ¨è¯çš„è¡¨ç°ä¸‹é™æ›´ä¸ºæ˜æ˜¾ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†å¥æ³•å¼•å¯¼åœ¨åŠ¨è¯å­¦ä¹ ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ•™è‚²æŠ€æœ¯å’Œå„¿ç«¥è¯­è¨€å­¦ä¹ ç­‰ã€‚é€šè¿‡ç†è§£å¥æ³•å¼•å¯¼åœ¨åŠ¨è¯å­¦ä¹ ä¸­çš„ä½œç”¨ï¼Œå¯ä»¥ä¸ºå¼€å‘æ›´æ™ºèƒ½çš„è¯­è¨€å­¦ä¹ å·¥å…·å’Œæ•™è‚²è½¯ä»¶æä¾›ç†è®ºæ”¯æŒï¼Œä¿ƒè¿›å„¿ç«¥è¯­è¨€èƒ½åŠ›çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use the syntactic environments in which a verb occurs to learn its meaning. In this paper, we examine whether large language models exhibit a similar behavior. We do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic information is ablated. Our results show that models' verb representation degrades more when syntactic cues are removed than when co-occurrence information is removed. Furthermore, the representation of mental verbs, for which syntactic bootstrapping has been shown to be particularly crucial in human verb learning, is more negatively impacted in such training regimes than physical verbs. In contrast, models' representation of nouns is affected more when co-occurrences are distorted than when syntax is distorted. In addition to reinforcing the important role of syntactic bootstrapping in verb learning, our results demonstrated the viability of testing developmental hypotheses on a larger scale through manipulating the learning environments of large language models.

