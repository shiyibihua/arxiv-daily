---
layout: default
title: ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads
---

# ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12407" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12407v1</a>
  <a href="https://arxiv.org/pdf/2508.12407.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12407v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12407v1', 'ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhuorui Liu, Chen Zhang, Dawei Song

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-17

**å¤‡æ³¨**: 5 pages, 4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZigzagAttentionä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„KVç¼“å­˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `æ€§èƒ½ä¼˜åŒ–` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è‡³å…³é‡è¦ï¼Œä½†KVç¼“å­˜çš„é«˜æ¶ˆè€—å¯¼è‡´éƒ¨ç½²å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºZigzagAttentionï¼Œé€šè¿‡ä¼˜åŒ–æ£€ç´¢å¤´å’Œæµå¼å¤´çš„è¯†åˆ«ï¼Œç¡®ä¿æ¯å±‚ä»…åŒ…å«ä¸€ç§ç±»å‹çš„å¤´ï¼Œé™ä½å»¶è¿Ÿã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒZigzagAttentionåœ¨å»¶è¿Ÿå‡å°‘å’Œæ€§èƒ½ä¿æŒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·å¤‡è‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¤„ç†é•¿ä¸Šä¸‹æ–‡å·²æˆä¸ºLLMsçš„é‡è¦èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§èƒ½åŠ›åœ¨éƒ¨ç½²æ—¶é¢ä¸´KVç¼“å­˜æ¶ˆè€—å¢åŠ çš„æŒ‘æˆ˜ã€‚å·²æœ‰ç ”ç©¶å°è¯•ä¼˜åŒ–KVç¼“å­˜çš„å†…å­˜å ç”¨ï¼ŒåŸºäºæ³¨æ„åŠ›å¤´çš„åˆ†ç±»ï¼Œå°†å…¶åˆ†ä¸ºé‡è¦çš„æ£€ç´¢å¤´å’Œç›¸å¯¹ä¸é‡è¦çš„æµå¼å¤´ã€‚é€šè¿‡è¯†åˆ«æµå¼å¤´å¹¶æ”¾å¼ƒå…¶KVç¼“å­˜ï¼Œå¯ä»¥æ˜¾è‘—é™ä½å¼€é”€è€Œä¸æ˜¾è‘—å½±å“æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºZigzagAttentionï¼Œé€šè¿‡æ”¹è¿›æ£€ç´¢å¤´å’Œæµå¼å¤´çš„è¯†åˆ«è¿‡ç¨‹ï¼Œç¡®ä¿æ¯å±‚ä»…åŒ…å«ä¸€ç§ç±»å‹çš„å¤´ï¼Œä»è€Œæ¶ˆé™¤é¢å¤–å»¶è¿Ÿå¹¶ä»…å¸¦æ¥å¾®å°çš„æ€§èƒ½ä¸‹é™ã€‚ZigzagAttentionåœ¨å‡å°‘å»¶è¿Ÿå’Œä¿æŒæ€§èƒ½æ–¹é¢ä¸ç°æœ‰åŸºçº¿å…·æœ‰ç«äº‰åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶KVç¼“å­˜æ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä½¿ç”¨æ£€ç´¢å¤´å’Œæµå¼å¤´æ—¶ï¼Œå¯èƒ½å¯¼è‡´é¢å¤–çš„å»¶è¿Ÿå’Œæ€§èƒ½æŸå¤±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„ZigzagAttentioné€šè¿‡æ”¹è¿›æ£€ç´¢å¤´å’Œæµå¼å¤´çš„è¯†åˆ«è¿‡ç¨‹ï¼Œç¡®ä¿æ¯å±‚ä»…åŒ…å«ä¸€ç§ç±»å‹çš„å¤´ï¼Œä»è€Œæ¶ˆé™¤ä¸å¿…è¦çš„å»¶è¿Ÿã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šZigzagAttentionçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè¯†åˆ«æ£€ç´¢å¤´å’Œæµå¼å¤´ï¼Œå…¶æ¬¡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…ä½¿ç”¨ä¸€ç§ç±»å‹çš„å¤´ã€‚é€šè¿‡è¿™æ ·çš„åˆ†ç¦»ï¼Œæ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†é•¿ä¸Šä¸‹æ–‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šZigzagAttentionçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ç‹¬ç‰¹çš„å¤´è¯†åˆ«æ ‡å‡†ï¼Œç¡®ä¿æ¯å±‚ä»…åŒ…å«æ£€ç´¢å¤´æˆ–æµå¼å¤´ï¼Œä»è€Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç”±äºæ··åˆä½¿ç”¨è€Œå¯¼è‡´çš„é¢å¤–å»¶è¿Ÿã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å¯¹å¤´çš„ä¸¥æ ¼åˆ†ç±»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ZigzagAttentionä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬å¤´çš„æ•°é‡å’Œç±»å‹çš„é€‰æ‹©ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºå¹³è¡¡æ€§èƒ½å’Œå»¶è¿Ÿã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ä¸Šï¼Œæ¨¡å‹é€šè¿‡åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥é€‚åº”ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒZigzagAttentionåœ¨å»¶è¿Ÿæ–¹é¢ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•å‡å°‘äº†çº¦30%ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸºçº¿æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡è¡¨æ˜ï¼ŒZigzagAttentionåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ZigzagAttentionçš„ç ”ç©¶æˆæœåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„æ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ”¯æŒæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚é•¿ç¯‡æ–‡ç« ç†è§£å’Œå®æ—¶å¯¹è¯ç”Ÿæˆï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid development of large language models (LLMs), handling long context has become one of the vital abilities in LLMs. Such long-context ability is accompanied by difficulties in deployment, especially due to the increased consumption of KV cache. There is certain work aiming to optimize the memory footprint of KV cache, inspired by the observation that attention heads can be categorized into retrieval heads that are of great significance and streaming heads that are of less significance. Typically, identifying the streaming heads and and waiving the KV cache in the streaming heads would largely reduce the overhead without hurting the performance that much. However, since employing both retrieval and streaming heads in one layer decomposes one large round of attention computation into two small ones, it may unexpectedly bring extra latency on accessing and indexing tensors. Based on this intuition, we impose an important improvement to the identification process of retrieval and streaming heads, in which we design a criterion that enforces exclusively retrieval or streaming heads gathered in one unique layer. In this way, we further eliminate the extra latency and only incur negligible performance degradation. Our method named \textsc{ZigzagAttention} is competitive among considered baselines owing to reduced latency and comparable performance.

