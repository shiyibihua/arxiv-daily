---
layout: default
title: M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following
---

# M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12458" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12458v1</a>
  <a href="https://arxiv.org/pdf/2508.12458.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12458v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12458v1', 'M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruirui Gao, Emily Johnson, Bowen Tan, Yanfei Qian

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM3POä»¥è§£å†³LVLMåå¥½ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéš` `åå¥½ä¼˜åŒ–` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ ` `è‡ªä¸€è‡´æ€§è¯„åˆ†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•åœ¨é«˜æ•ˆåˆ©ç”¨æ¨¡å‹ç”Ÿæˆç©ºé—´å’Œè¯†åˆ«å›°éš¾è´Ÿæ ·æœ¬æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†LVLMçš„æ€§èƒ½æå‡ã€‚
2. æœ¬æ–‡æå‡ºçš„M3POæ–¹æ³•é€šè¿‡æ™ºèƒ½é€‰æ‹©å­¦ä¹ ä»·å€¼é«˜çš„åå¥½æ ·æœ¬å¯¹ï¼Œç»“åˆå¤šæ¨¡æ€å¯¹é½è¯„åˆ†å’Œæ¨¡å‹è‡ªä¿¡åº¦ï¼Œä¼˜åŒ–åå¥½å­¦ä¹ è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM3POåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„SFTã€æ¨¡æ‹ŸRLHFã€æ™®é€šDPOå’ŒRM-DPOç­‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤æ‚çš„å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å…¶å‘å±•å¸¸å—åˆ°é«˜æ˜‚çš„äººç±»æ ‡æ³¨æˆæœ¬å’Œä¸ä¸€è‡´æ€§çš„é™åˆ¶ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•å¦‚RLHFå’ŒDPOåœ¨æœ‰æ•ˆåˆ©ç”¨æ¨¡å‹ç”Ÿæˆç©ºé—´ä»¥è¯†åˆ«é«˜ä¿¡æ¯é‡çš„â€œå›°éš¾è´Ÿæ ·æœ¬â€æ–¹é¢å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”æ•°æ®é«˜æ•ˆçš„æ–¹æ³•â€”â€”å¤šæ¨¡æ€æ¨¡å‹å¼•å¯¼çš„åå¥½ä¼˜åŒ–ï¼ˆM3POï¼‰ï¼Œæ—¨åœ¨å¢å¼ºLVLMåœ¨è§†è§‰æŒ‡ä»¤è·Ÿéšä¸­çš„èƒ½åŠ›ã€‚M3POæ™ºèƒ½åœ°ä»å¤šæ ·çš„LVLMç”Ÿæˆå€™é€‰æ ·æœ¬ä¸­é€‰æ‹©æœ€å…·â€œå­¦ä¹ ä»·å€¼â€çš„åå¥½æ ·æœ¬å¯¹ï¼Œç»“åˆå¤šæ¨¡æ€å¯¹é½è¯„åˆ†ï¼ˆMASï¼‰å’Œæ¨¡å‹è‡ªä¸€è‡´æ€§/ç½®ä¿¡åº¦ï¼ˆå¯¹æ•°æ¦‚ç‡ï¼‰æ¥è¯„ä¼°æ ·æœ¬è´¨é‡ï¼Œæœ€ç»ˆé€šè¿‡é«˜è´¨é‡çš„åå¥½å¯¹è¿›è¡Œé«˜æ•ˆçš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM3POåœ¨å¤šä¸ªå¤šæ¨¡æ€æŒ‡ä»¤è·ŸéšåŸºå‡†ä¸Šå‡ä¼˜äºå¼ºåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšä¸­çš„åå¥½ä¼˜åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚SFTå’ŒRLHFåœ¨é«˜æ•ˆåˆ©ç”¨æ¨¡å‹ç”Ÿæˆç©ºé—´å’Œè¯†åˆ«é«˜ä¿¡æ¯é‡çš„å›°éš¾è´Ÿæ ·æœ¬æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šM3POé€šè¿‡æ™ºèƒ½é€‰æ‹©æœ€å…·å­¦ä¹ ä»·å€¼çš„åå¥½æ ·æœ¬å¯¹ï¼Œç»“åˆå¤–éƒ¨è´¨é‡è¯„ä¼°çš„å¤šæ¨¡æ€å¯¹é½è¯„åˆ†ï¼ˆMASï¼‰å’Œæ¨¡å‹å†…éƒ¨ç½®ä¿¡åº¦ï¼Œä¼˜åŒ–åå¥½å­¦ä¹ è¿‡ç¨‹ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜æ ·æœ¬é€‰æ‹©çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM3POçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ ·æœ¬ç”Ÿæˆã€æ ·æœ¬é€‰æ‹©å’Œåå¥½ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»LVLMç”Ÿæˆå¤šæ ·çš„å€™é€‰æ ·æœ¬ï¼›å…¶æ¬¡ï¼Œé€šè¿‡MASå’Œè‡ªä¸€è‡´æ€§è¯„åˆ†é€‰æ‹©é«˜è´¨é‡çš„åå¥½æ ·æœ¬å¯¹ï¼›æœ€åï¼Œåˆ©ç”¨è¿™äº›æ ·æœ¬å¯¹è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šM3POçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†M3P-Scoreï¼Œè¿™ä¸€è¯„åˆ†æœºåˆ¶ç»“åˆäº†å¤–éƒ¨å’Œå†…éƒ¨ä¿¡å·ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å‡ºæ¨¡å‹è‡ªä¿¡ä½†é”™è¯¯çš„å“åº”ï¼Œä»è€Œä¼˜åŒ–åå¥½å­¦ä¹ è¿‡ç¨‹ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ ·æœ¬é€‰æ‹©æœºåˆ¶æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨M3POä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å¤šæ¨¡æ€å¯¹é½è¯„åˆ†çš„è®¡ç®—æ–¹å¼å’Œè‡ªä¸€è‡´æ€§è¯„åˆ†çš„è¯„ä¼°æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨LoRAæŠ€æœ¯è¿›è¡ŒDPOå¾®è°ƒï¼Œä»¥æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿé’ˆå¯¹åå¥½æ ·æœ¬å¯¹çš„ä¼˜åŒ–è¿›è¡Œäº†è°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒM3POåœ¨å¤šä¸ªå¤šæ¨¡æ€æŒ‡ä»¤è·ŸéšåŸºå‡†ï¼ˆå¦‚MME-Benchã€POPEã€IFTã€Human Pref. Scoreï¼‰ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨åå¥½ä¼˜åŒ–ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰å¤šæ¨¡æ€äº¤äº’åœºæ™¯ã€‚é€šè¿‡æå‡LVLMåœ¨è§†è§‰æŒ‡ä»¤è·Ÿéšä¸­çš„è¡¨ç°ï¼ŒM3POèƒ½å¤Ÿæ˜¾è‘—æé«˜äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) hold immense potential for complex multimodal instruction following, yet their development is often hindered by the high cost and inconsistency of human annotation required for effective fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT) and existing preference optimization methods like RLHF and DPO frequently struggle to efficiently leverage the model's own generation space to identify highly informative "hard negative" samples. To address these challenges, we propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and data-efficient method designed to enhance LVLMs' capabilities in visual instruction following. M3PO intelligently selects the most "learning-valuable" preference sample pairs from a diverse pool of LVLM-generated candidates. This selection is driven by a sophisticated mechanism that integrates two crucial signals: a Multimodal Alignment Score (MAS) to assess external quality and the model's Self-Consistency / Confidence (log-probability) to gauge internal belief. These are combined into a novel M3P-Score, which specifically identifies preferred responses and challenging dispreferred responses that the model might confidently generate despite being incorrect. These high-quality preference pairs are then used for efficient Direct Preference Optimization (DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our extensive experiments demonstrate that M3PO consistently outperforms strong baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a comprehensive suite of multimodal instruction following benchmarks (MME-Bench, POPE, IFT, Human Pref. Score).

