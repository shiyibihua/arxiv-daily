---
layout: default
title: Mitigating Hallucinations in Large Language Models via Causal Reasoning
---

# Mitigating Hallucinations in Large Language Models via Causal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12495" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12495v2</a>
  <a href="https://arxiv.org/pdf/2508.12495.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12495v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12495v2', 'Mitigating Hallucinations in Large Language Models via Causal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuangang Li, Yiqing Shen, Yi Nian, Jiechao Gao, Ziyi Wang, Chenxiao Yu, Shawn Li, Jie Wang, Xiyang Hu, Yue Zhao

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-17 (æ›´æ–°: 2025-11-12)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MrLYG/CDCR-SFT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCDCR-SFTä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å› æœæ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœ‰å‘æ— ç¯å›¾` `é€»è¾‘ä¸€è‡´æ€§` `ç›‘ç£å¾®è°ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†æ–¹æ³•åœ¨å¤„ç†å› æœå…³ç³»æ—¶å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆè¡¨ç¤ºæ¡ä»¶ç‹¬ç«‹æ€§å’Œæ»¡è¶³å› æœè¯†åˆ«å‡è®¾ã€‚
2. æœ¬æ–‡æå‡ºCDCR-SFTæ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼æ„å»ºå˜é‡çº§DAGæ¥å¢å¼ºLLMsçš„å› æœæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCDCR-SFTåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†å› æœæ¨ç†èƒ½åŠ›ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†å¹»è§‰ç°è±¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å¸¸å‡ºç°é€»è¾‘ä¸ä¸€è‡´çš„å¹»è§‰ç°è±¡ï¼Œè¿™äº›å¹»è§‰çœ‹ä¼¼è¿è´¯å´è¿åæ¨ç†åŸåˆ™ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå› æœæ¨ç†èƒ½åŠ›ä¸å¹»è§‰ç°è±¡ä¹‹é—´å­˜åœ¨åå‘å…³ç³»ã€‚ç°æœ‰çš„æ¨ç†æ–¹æ³•å¦‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä¸»è¦åœ¨è¯­è¨€ç¬¦å·å±‚é¢æ“ä½œï¼Œæœªèƒ½æœ‰æ•ˆå»ºæ¨¡å˜é‡ä¹‹é—´çš„å› æœå…³ç³»ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å› æœDAGæ„å»ºä¸æ¨ç†çš„ç›‘ç£å¾®è°ƒæ¡†æ¶ï¼ˆCDCR-SFTï¼‰ï¼Œè¯¥æ¡†æ¶è®­ç»ƒLLMsæ˜¾å¼æ„å»ºå˜é‡çº§æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œå¹¶åœ¨å…¶ä¸Šè¿›è¡Œæ¨ç†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«25,368ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼ˆCausalDRï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…æ‹¬è¾“å…¥é—®é¢˜ã€æ˜¾å¼å› æœDAGã€åŸºäºå›¾çš„æ¨ç†è½¨è¿¹å’ŒéªŒè¯ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCDCR-SFTåœ¨CLADDERä»»åŠ¡ä¸Šè¾¾åˆ°äº†95.33%çš„å‡†ç¡®ç‡ï¼Œé¦–æ¬¡è¶…è¶Šäººç±»è¡¨ç°çš„94.8%ï¼Œå¹¶åœ¨HaluEvalä¸Šå‡å°‘äº†10%çš„å¹»è§‰ç°è±¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„é€»è¾‘ä¸ä¸€è‡´å¹»è§‰é—®é¢˜ã€‚ç°æœ‰çš„æ¨ç†æ–¹æ³•å¦‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åœ¨è¯­è¨€ç¬¦å·å±‚é¢æ“ä½œï¼Œæœªèƒ½æœ‰æ•ˆå»ºæ¨¡å˜é‡ä¹‹é—´çš„å› æœå…³ç³»ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„CDCR-SFTæ¡†æ¶é€šè¿‡æ˜¾å¼æ„å»ºå˜é‡çº§çš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å› æœç»“æ„ä¸Šè¿›è¡Œæ¨ç†ï¼Œä»è€Œæå‡å› æœæ¨ç†èƒ½åŠ›å¹¶å‡å°‘å¹»è§‰ç°è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCDCR-SFTæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯å› æœDAGçš„æ„å»ºæ¨¡å—ï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥é—®é¢˜ç”Ÿæˆç›¸åº”çš„DAGï¼›å…¶æ¬¡æ˜¯æ¨ç†æ¨¡å—ï¼Œæ¨¡å‹åœ¨æ„å»ºçš„DAGä¸Šè¿›è¡Œæ¨ç†ï¼Œè¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ˜¾å¼å› æœç»“æ„å»ºæ¨¡ï¼ŒåŒºåˆ«äºç°æœ‰æ–¹æ³•ä»…åœ¨è¯­è¨€å±‚é¢è¿›è¡Œæ¨ç†ï¼ŒCDCR-SFTèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å˜é‡ä¹‹é—´çš„å› æœå…³ç³»ï¼Œæå‡æ¨ç†çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆæ¨ç†å‡†ç¡®æ€§å’Œå› æœç»“æ„çš„æŸå¤±ï¼Œç¡®ä¿æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆæ„å»ºå’Œåˆ©ç”¨DAGã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCDCR-SFTåœ¨CLADDERä»»åŠ¡ä¸Šè¾¾åˆ°äº†95.33%çš„å‡†ç¡®ç‡ï¼Œé¦–æ¬¡è¶…è¶Šäººç±»è¡¨ç°çš„94.8%ã€‚æ­¤å¤–ï¼Œåœ¨HaluEvalä»»åŠ¡ä¸Šï¼Œæ¨¡å‹çš„å¹»è§‰ç°è±¡å‡å°‘äº†10%ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†çš„å¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨æ¨ç†ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å› æœæ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ›´å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­æä¾›æ›´å‡†ç¡®çš„ç­”æ¡ˆï¼Œå‡å°‘é€»è¾‘ä¸ä¸€è‡´çš„ç°è±¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at https://github.com/MrLYG/CDCR-SFT.

