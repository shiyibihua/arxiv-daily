---
layout: default
title: The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution
---

# The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12277" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12277v1</a>
  <a href="https://arxiv.org/pdf/2508.12277.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12277v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12277v1', 'The Self-Execution Benchmark: Measuring LLMs\' Attempts to Overcome Their Lack of Self-Execution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Elon Ezra, Ariel Weizman, Amos Azaria

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-17

**å¤‡æ³¨**: 11 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæ‰§è¡ŒåŸºå‡†ä»¥è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘é¢„æµ‹èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘é¢„æµ‹` `è‡ªæ‰§è¡ŒåŸºå‡†` `æ€§èƒ½è¯„ä¼°` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰è¯„ä¼°æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨LLMçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæœªèƒ½æœ‰æ•ˆè¯„ä¼°å…¶è‡ªæˆ‘é¢„æµ‹èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºè‡ªæ‰§è¡ŒåŸºå‡†ï¼Œä¸“æ³¨äºLLMå¯¹è‡ªèº«è¾“å‡ºç‰¹æ€§çš„é¢„æµ‹èƒ½åŠ›ï¼Œå¡«è¡¥ç°æœ‰è¯„ä¼°çš„ç©ºç™½ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå®éªŒæ˜¾ç¤ºï¼ŒLLMåœ¨è‡ªæ‰§è¡ŒåŸºå‡†ä¸Šçš„è¡¨ç°æ™®éè¾ƒå·®ï¼Œä¸”æ¨¡å‹è§„æ¨¡çš„å¢åŠ æœªå¿…å¸¦æ¥æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸é€šè¿‡æµ‹è¯•å…¶çŸ¥è¯†æˆ–æ¨ç†èƒ½åŠ›æ¥è¿›è¡Œè¯„ä¼°ã€‚æœ¬æ–‡æ¢è®¨äº†ä¸€ç§ä¸åŒçš„è¯„ä¼°æ–¹å¼ï¼šLLMæ˜¯å¦èƒ½å¤Ÿé¢„æµ‹å…¶è‡ªèº«å“åº”çš„æŸäº›æ–¹é¢ã€‚ç”±äºLLMç¼ºä¹è‡ªæˆ‘æ‰§è¡Œçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªæ‰§è¡ŒåŸºå‡†ï¼Œæµ‹é‡æ¨¡å‹é¢„æµ‹è¾“å‡ºç‰¹æ€§ï¼ˆå¦‚é—®é¢˜éš¾åº¦ã€æ‹’ç»å›ç­”çš„å¯èƒ½æ€§åŠå…¶äº§ç”Ÿçš„å…³è”ç±»å‹ï¼‰çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šçš„è¡¨ç°æ™®éè¾ƒå·®ï¼Œä¸”æ¨¡å‹è§„æ¨¡æˆ–èƒ½åŠ›çš„å¢åŠ å¹¶æœªå§‹ç»ˆå¯¼è‡´æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜LLMåœ¨è¡¨ç¤ºå’Œæ¨ç†è‡ªèº«è¡Œä¸ºæ–¹é¢å­˜åœ¨æ ¹æœ¬æ€§é™åˆ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨è‡ªæˆ‘é¢„æµ‹èƒ½åŠ›è¯„ä¼°æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå¿½è§†äº†æ¨¡å‹å¯¹è‡ªèº«è¡Œä¸ºçš„ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºè‡ªæ‰§è¡ŒåŸºå‡†ï¼Œæ—¨åœ¨æµ‹é‡LLMé¢„æµ‹å…¶è¾“å‡ºç‰¹æ€§çš„èƒ½åŠ›ï¼Œå¦‚é—®é¢˜éš¾åº¦å’Œæ‹’ç»å›ç­”çš„å¯èƒ½æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ›´æ·±å…¥åœ°ç†è§£LLMçš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªæ‰§è¡ŒåŸºå‡†çš„è®¾è®¡å’Œå®éªŒè¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ¨¡å‹è¾“å‡ºç‰¹æ€§çš„å®šä¹‰ã€é¢„æµ‹ä»»åŠ¡çš„è®¾è®¡ä»¥åŠæ€§èƒ½è¯„ä¼°æŒ‡æ ‡çš„è®¾å®šã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†è‡ªæ‰§è¡ŒåŸºå‡†è¿™ä¸€å…¨æ–°è¯„ä¼°æ¡†æ¶ï¼Œå¼ºè°ƒLLMå¯¹è‡ªèº«è¡Œä¸ºçš„é¢„æµ‹èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›è¯„ä¼°æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒè®¾è®¡ä¸­ï¼Œè®¾ç½®äº†å¤šä¸ªé¢„æµ‹ä»»åŠ¡ï¼Œæ¶µç›–ä¸åŒç±»å‹çš„é—®é¢˜å’Œè¾“å‡ºç‰¹æ€§ï¼Œé‡‡ç”¨äº†æ ‡å‡†çš„æ€§èƒ½è¯„ä¼°æŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMåœ¨è‡ªæ‰§è¡ŒåŸºå‡†ä¸Šçš„æ•´ä½“è¡¨ç°è¾ƒå·®ï¼Œå…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨é¢„æµ‹é—®é¢˜éš¾åº¦å’Œæ‹’ç»å›ç­”çš„èƒ½åŠ›ä¸Šå‡æœªè¾¾åˆ°é¢„æœŸï¼Œä¸”æ¨¡å‹è§„æ¨¡çš„å¢åŠ å¹¶æœªæ˜¾è‘—æ”¹å–„æ€§èƒ½ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†LLMåœ¨è‡ªæˆ‘ç†è§£å’Œé¢„æµ‹æ–¹é¢çš„æ ¹æœ¬æ€§é™åˆ¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡è¯„ä¼°LLMçš„è‡ªæˆ‘é¢„æµ‹èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºæ¨¡å‹çš„æ”¹è¿›å’Œä¼˜åŒ–æä¾›æ–°çš„æ–¹å‘ï¼Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are commonly evaluated on tasks that test their knowledge or reasoning abilities. In this paper, we explore a different type of evaluation: whether an LLM can predict aspects of its own responses. Since LLMs lack the ability to execute themselves, we introduce the Self-Execution Benchmark, which measures a model's ability to anticipate properties of its output, such as whether a question will be difficult for it, whether it will refuse to answer, or what kinds of associations it is likely to produce. Our experiments show that models generally perform poorly on this benchmark, and that increased model size or capability does not consistently lead to better performance. These results suggest a fundamental limitation in how LLMs represent and reason about their own behavior.

