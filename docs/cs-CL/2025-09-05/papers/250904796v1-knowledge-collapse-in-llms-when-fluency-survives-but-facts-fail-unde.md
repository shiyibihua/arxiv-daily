---
layout: default
title: Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under Recursive Synthetic Training
---

# Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under Recursive Synthetic Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04796" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04796v1</a>
  <a href="https://arxiv.org/pdf/2509.04796.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04796v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04796v1', 'Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under Recursive Synthetic Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Figarri Keisha, Zekun Wu, Ze Wang, Adriano Koshiyama, Philip Treleaven

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºLLMé€’å½’åˆæˆè®­ç»ƒä¸­çš„çŸ¥è¯†å´©å¡Œç°è±¡ï¼Œæå‡ºé¢†åŸŸç‰¹å®šè®­ç»ƒç¼“è§£ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†å´©å¡Œ` `é€’å½’åˆæˆè®­ç»ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `é¢†åŸŸç‰¹å®šè®­ç»ƒ` `æ¨¡å‹é€€åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¾èµ–åˆæˆæ•°æ®ï¼Œä½†é€’å½’è®­ç»ƒæ˜“å¯¼è‡´çŸ¥è¯†å´©å¡Œï¼Œäº§ç”Ÿâ€œè‡ªä¿¡åœ°é”™è¯¯â€çš„è¾“å‡ºã€‚
2. è®ºæ–‡æå‡ºé¢†åŸŸç‰¹å®šçš„åˆæˆè®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨é€’å½’è®­ç»ƒä¸‹çš„æŠ—å´©å¡Œèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥èƒ½æœ‰æ•ˆæŠµæŠ—çŸ¥è¯†å´©å¡Œï¼Œå¹¶ç»“åˆæ¨¡å‹å’Œä»»åŠ¡æŒ‡æ ‡ï¼Œå®ç°è®¤çŸ¥é€€åŒ–çš„å¯é‡å¤è¯„ä¼°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°ä¾èµ–åˆæˆæ•°æ®ï¼Œä½†é€’å½’åœ°è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºä¼šå¯¼è‡´æ¨¡å‹å´©å¡Œï¼Œè¿™æ˜¯ä¸€ä¸ªå¨èƒäº‹å®å¯é æ€§çš„é€€åŒ–è¿‡ç¨‹ã€‚æœ¬æ–‡å°†çŸ¥è¯†å´©å¡Œå®šä¹‰ä¸ºä¸€ä¸ªç‹¬ç‰¹çš„ä¸‰é˜¶æ®µç°è±¡ï¼Œå…¶ä¸­äº‹å®å‡†ç¡®æ€§ä¸‹é™ï¼Œè€Œè¡¨é¢æµç•…æ€§ä»ç„¶å­˜åœ¨ï¼Œä»è€Œäº§ç”Ÿâ€œè‡ªä¿¡åœ°é”™è¯¯â€çš„è¾“å‡ºï¼Œè¿™åœ¨ä¾èµ–å‡†ç¡®æ€§çš„é¢†åŸŸä¸­æ„æˆäº†ä¸¥é‡é£é™©ã€‚é€šè¿‡å—æ§çš„é€’å½’åˆæˆè®­ç»ƒå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†å´©å¡Œè½¨è¿¹å’Œæ—¶é—´å®‰æ’å…³é”®å–å†³äºæŒ‡ä»¤æ ¼å¼ï¼Œé€šè¿‡å…¶æœ‰æ¡ä»¶çš„ã€æç¤ºä¾èµ–çš„æ€§è´¨ï¼Œå°†æŒ‡ä»¤éµå¾ªå´©å¡Œä¸ä¼ ç»Ÿçš„æ¨¡å‹å´©å¡ŒåŒºåˆ†å¼€æ¥ã€‚æˆ‘ä»¬æå‡ºé¢†åŸŸç‰¹å®šçš„åˆæˆè®­ç»ƒä½œä¸ºä¸€ç§æœ‰é’ˆå¯¹æ€§çš„ç¼“è§£ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†å´©å¡ŒæŠµæŠ—çš„æ˜¾ç€æ”¹è¿›ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å°†ä»¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„æŒ‡æ ‡ä¸ä»¥ä»»åŠ¡ä¸ºä¸­å¿ƒçš„æŒ‡æ ‡ç›¸ç»“åˆï¼Œä»¥æ£€æµ‹ä¸åŒçš„é€€åŒ–é˜¶æ®µï¼Œä»è€Œèƒ½å¤Ÿå¯¹ä¸åŒè¯­è¨€æ¨¡å‹çš„è®¤çŸ¥é€€åŒ–è¿›è¡Œå¯é‡å¤çš„è¯„ä¼°ã€‚è¿™äº›å‘ç°ä¸ºå´©å¡ŒåŠ¨åŠ›å­¦æä¾›äº†ç†è®ºè§è§£ï¼Œå¹¶ä¸ºçŸ¥è¯†å¯†é›†å‹åº”ç”¨ä¸­å¯æŒç»­çš„AIè®­ç»ƒæä¾›äº†å®è·µæŒ‡å¯¼ï¼Œåœ¨è¿™äº›åº”ç”¨ä¸­ï¼Œå‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€’å½’åˆæˆæ•°æ®è®­ç»ƒä¸­å‡ºç°çš„çŸ¥è¯†å´©å¡Œé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹æ¨¡å‹ç”Ÿæˆæ•°æ®æ—¶ï¼Œå®¹æ˜“å‡ºç°äº‹å®å‡†ç¡®æ€§ä¸‹é™ï¼Œä½†è¡¨é¢æµç•…æ€§ä»ç„¶å­˜åœ¨çš„æƒ…å†µï¼Œå¯¼è‡´æ¨¡å‹äº§ç”Ÿâ€œè‡ªä¿¡åœ°é”™è¯¯â€çš„è¾“å‡ºï¼Œè¿™åœ¨éœ€è¦é«˜å‡†ç¡®æ€§çš„çŸ¥è¯†å¯†é›†å‹åº”ç”¨ä¸­æ˜¯ä¸å¯æ¥å—çš„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼ŒçŸ¥è¯†å´©å¡Œçš„å‘ç”Ÿä¸æŒ‡ä»¤æ ¼å¼å¯†åˆ‡ç›¸å…³ï¼Œå¹¶ä¸”å­˜åœ¨promptä¾èµ–æ€§ã€‚å› æ­¤ï¼Œé€šè¿‡é¢†åŸŸç‰¹å®šçš„åˆæˆè®­ç»ƒï¼Œå¯ä»¥æœ‰é’ˆå¯¹æ€§åœ°æå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æŠ—å´©å¡Œèƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨é€šè¿‡æ§åˆ¶è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼Œé¿å…æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç”Ÿæˆæ•°æ®ä¸­çš„é”™è¯¯æ¨¡å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) å®šä¹‰çŸ¥è¯†å´©å¡Œçš„ä¸‰é˜¶æ®µç°è±¡ï¼›2) é€šè¿‡å—æ§å®éªŒï¼Œç ”ç©¶é€’å½’åˆæˆè®­ç»ƒå¯¹æ¨¡å‹çš„å½±å“ï¼›3) æå‡ºé¢†åŸŸç‰¹å®šçš„åˆæˆè®­ç»ƒç­–ç•¥ï¼›4) è®¾è®¡è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆæ¨¡å‹å’Œä»»åŠ¡æŒ‡æ ‡ï¼Œæ£€æµ‹ä¸åŒé€€åŒ–é˜¶æ®µã€‚è¯¥æ¡†æ¶æ—¨åœ¨å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨é€’å½’è®­ç»ƒä¸‹çš„çŸ¥è¯†ä¿æŒèƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ˜ç¡®å®šä¹‰äº†çŸ¥è¯†å´©å¡Œçš„ä¸‰é˜¶æ®µç°è±¡ï¼Œå¹¶å°†å…¶ä¸ä¼ ç»Ÿçš„æ¨¡å‹å´©å¡ŒåŒºåˆ†å¼€æ¥ï¼›2) æå‡ºäº†é¢†åŸŸç‰¹å®šçš„åˆæˆè®­ç»ƒç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§æœ‰é’ˆå¯¹æ€§çš„ç¼“è§£çŸ¥è¯†å´©å¡Œçš„æ–¹æ³•ï¼›3) è®¾è®¡äº†ä¸€ä¸ªç»¼åˆæ€§çš„è¯„ä¼°æ¡†æ¶ï¼Œå¯ä»¥å¯¹ä¸åŒè¯­è¨€æ¨¡å‹çš„è®¤çŸ¥é€€åŒ–è¿›è¡Œå¯é‡å¤çš„è¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç²¾å¿ƒè®¾è®¡çš„é€’å½’åˆæˆè®­ç»ƒå®éªŒï¼Œç”¨äºæ¨¡æ‹Ÿæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½é‡åˆ°çš„æƒ…å†µï¼›2) é¢†åŸŸç‰¹å®šçš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æ§åˆ¶è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼Œé¿å…æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç”Ÿæˆæ•°æ®ä¸­çš„é”™è¯¯æ¨¡å¼ï¼›3) ç»¼åˆæ€§çš„è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ¨¡å‹å±‚é¢çš„æŒ‡æ ‡ï¼ˆå¦‚å›°æƒ‘åº¦ï¼‰å’Œä»»åŠ¡å±‚é¢çš„æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰ï¼Œç”¨äºå…¨é¢è¯„ä¼°æ¨¡å‹çš„çŸ¥è¯†ä¿æŒèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒè¯æ˜ï¼Œé¢†åŸŸç‰¹å®šçš„åˆæˆè®­ç»ƒèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨é€’å½’è®­ç»ƒä¸‹çš„æŠ—å´©å¡Œèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæœ‰æ•ˆç¼“è§£äº†çŸ¥è¯†å´©å¡Œç°è±¡ï¼Œæé«˜äº†æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºçŸ¥è¯†å¯†é›†å‹é¢†åŸŸï¼Œå¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æã€æ³•å¾‹å’¨è¯¢ç­‰ï¼Œä»¥æå‡LLMåœ¨è¿™äº›é¢†åŸŸçš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡é¢†åŸŸç‰¹å®šçš„åˆæˆè®­ç»ƒï¼Œå¯ä»¥æ„å»ºæ›´å€¼å¾—ä¿¡èµ–çš„AIç³»ç»Ÿï¼Œå‡å°‘å› æ¨¡å‹å´©å¡Œå¯¼è‡´çš„é”™è¯¯å†³ç­–é£é™©ï¼Œå¹¶ä¸ºå¯æŒç»­çš„AIè®­ç»ƒæä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models increasingly rely on synthetic data due to human-written content scarcity, yet recursive training on model-generated outputs leads to model collapse, a degenerative process threatening factual reliability. We define knowledge collapse as a distinct three-stage phenomenon where factual accuracy deteriorates while surface fluency persists, creating "confidently wrong" outputs that pose critical risks in accuracy-dependent domains. Through controlled experiments with recursive synthetic training, we demonstrate that collapse trajectory and timing depend critically on instruction format, distinguishing instruction-following collapse from traditional model collapse through its conditional, prompt-dependent nature. We propose domain-specific synthetic training as a targeted mitigation strategy that achieves substantial improvements in collapse resistance while maintaining computational efficiency. Our evaluation framework combines model-centric indicators with task-centric metrics to detect distinct degradation phases, enabling reproducible assessment of epistemic deterioration across different language models. These findings provide both theoretical insights into collapse dynamics and practical guidance for sustainable AI training in knowledge-intensive applications where accuracy is paramount.

