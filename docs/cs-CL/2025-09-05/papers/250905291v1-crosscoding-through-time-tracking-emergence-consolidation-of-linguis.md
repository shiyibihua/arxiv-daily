---
layout: default
title: Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining
---

# Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05291" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05291v1</a>
  <a href="https://arxiv.org/pdf/2509.05291.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05291v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05291v1', 'Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Deniz Bayazit, Aaron Mueller, Antoine Bosselut

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç¨€ç–äº’ç¼–ç å™¨çš„LLMé¢„è®­ç»ƒè¿‡ç¨‹è¯­è¨€è¡¨å¾è¿½è¸ªæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é¢„è®­ç»ƒ` `è¡¨å¾å­¦ä¹ ` `äº’ç¼–ç å™¨` `ç‰¹å¾å¯¹é½` `å› æœæ¨æ–­` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè¯„ä¼°æ–¹æ³•éš¾ä»¥æ­ç¤ºæ¨¡å‹å¦‚ä½•è·å¾—æ¦‚å¿µå’Œèƒ½åŠ›ï¼Œç¼ºä¹å¯¹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è¯­è¨€èƒ½åŠ›æ¶Œç°çš„ç»†ç²’åº¦ç†è§£ã€‚
2. åˆ©ç”¨ç¨€ç–äº’ç¼–ç å™¨å¯¹é½ä¸åŒæ¨¡å‹æ£€æŸ¥ç‚¹ä¹‹é—´çš„ç‰¹å¾ï¼Œä»è€Œè·Ÿè¸ªé¢„è®­ç»ƒæœŸé—´è¯­è¨€ç‰¹å¾çš„æ¼”å˜è¿‡ç¨‹ã€‚
3. é€šè¿‡ç›¸å¯¹é—´æ¥æ•ˆåº”(RelIE)æŒ‡æ ‡ï¼Œè¿½è¸ªå•ä¸ªç‰¹å¾å¯¹ä»»åŠ¡æ€§èƒ½äº§ç”Ÿå› æœé‡è¦æ€§çš„è®­ç»ƒé˜¶æ®µï¼Œå®ç°ç‰¹å¾æ¶Œç°ã€ç»´æŒå’Œåœæ­¢çš„æ£€æµ‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨é¢„è®­ç»ƒæœŸé—´å­¦ä¹ åˆ°éå‡¡çš„æŠ½è±¡æ¦‚å¿µï¼Œä¾‹å¦‚è¯†åˆ«ä¸è§„åˆ™å¤æ•°åè¯ä¸»è¯­ã€‚ç„¶è€Œï¼Œå¯¹äºç‰¹å®šè¯­è¨€èƒ½åŠ›ä½•æ—¶ä»¥åŠå¦‚ä½•æ¶Œç°ï¼Œæˆ‘ä»¬çŸ¥ä¹‹ç”šå°‘ï¼Œå› ä¸ºä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•ï¼ˆå¦‚åŸºå‡†æµ‹è¯•ï¼‰æ— æ³•æ­ç¤ºæ¨¡å‹å¦‚ä½•è·å¾—æ¦‚å¿µå’Œèƒ½åŠ›ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œå¹¶åœ¨æ¦‚å¿µå±‚é¢æ›´å¥½åœ°ç†è§£æ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ç¨€ç–äº’ç¼–ç å™¨æ¥å‘ç°å’Œå¯¹é½æ¨¡å‹æ£€æŸ¥ç‚¹ä¹‹é—´çš„ç‰¹å¾ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬è·Ÿè¸ªäº†é¢„è®­ç»ƒæœŸé—´è¯­è¨€ç‰¹å¾çš„æ¼”å˜ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æ˜¾è‘—æ€§èƒ½å’Œè¡¨å¾å˜åŒ–çš„å¼€æºæ£€æŸ¥ç‚¹ä¸‰å…ƒç»„ä¹‹é—´è®­ç»ƒäº’ç¼–ç å™¨ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æŒ‡æ ‡ï¼Œå³ç›¸å¯¹é—´æ¥æ•ˆåº”(RelIE)ï¼Œä»¥è¿½è¸ªå•ä¸ªç‰¹å¾å¯¹ä»»åŠ¡æ€§èƒ½äº§ç”Ÿå› æœé‡è¦æ€§çš„è®­ç»ƒé˜¶æ®µã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œäº’ç¼–ç å™¨å¯ä»¥æ£€æµ‹é¢„è®­ç»ƒæœŸé—´ç‰¹å¾çš„æ¶Œç°ã€ç»´æŒå’Œåœæ­¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ¶æ„æ— å…³ä¸”å¯æ‰©å±•ï¼Œä¸ºåœ¨æ•´ä¸ªé¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹è¡¨å¾å­¦ä¹ è¿›è¡Œæ›´å…·å¯è§£é‡Šæ€§å’Œç»†ç²’åº¦çš„åˆ†ææä¾›äº†ä¸€æ¡æœ‰å¸Œæœ›çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLM)é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç‰¹å®šè¯­è¨€èƒ½åŠ›ä½•æ—¶ä»¥åŠå¦‚ä½•æ¶Œç°çš„é—®é¢˜ã€‚ç°æœ‰è¯„ä¼°æ–¹æ³•ï¼ˆå¦‚åŸºå‡†æµ‹è¯•ï¼‰æ— æ³•æä¾›è¶³å¤Ÿç»†ç²’åº¦çš„ä¿¡æ¯ï¼Œéš¾ä»¥ç†è§£æ¨¡å‹å†…éƒ¨è¡¨å¾çš„å­¦ä¹ è¿‡ç¨‹ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–¹æ³•æ¥è·Ÿè¸ªå’Œåˆ†æLLMåœ¨é¢„è®­ç»ƒæœŸé—´å­¦ä¹ åˆ°çš„è¯­è¨€ç‰¹å¾çš„æ¼”å˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä½¿ç”¨ç¨€ç–äº’ç¼–ç å™¨(sparse crosscoders)æ¥å‘ç°å’Œå¯¹é½ä¸åŒæ¨¡å‹æ£€æŸ¥ç‚¹ä¹‹é—´çš„ç‰¹å¾ã€‚é€šè¿‡è®­ç»ƒäº’ç¼–ç å™¨ï¼Œå¯ä»¥å°†ä¸€ä¸ªæ£€æŸ¥ç‚¹ä¸­çš„ç‰¹å¾æ˜ å°„åˆ°å¦ä¸€ä¸ªæ£€æŸ¥ç‚¹ä¸­çš„å¯¹åº”ç‰¹å¾ï¼Œä»è€Œè·Ÿè¸ªç‰¹å¾åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†ç›¸å¯¹é—´æ¥æ•ˆåº”(RelIE)æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°å•ä¸ªç‰¹å¾å¯¹ä»»åŠ¡æ€§èƒ½çš„å› æœé‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) é€‰æ‹©å…·æœ‰æ˜¾è‘—æ€§èƒ½å’Œè¡¨å¾å˜åŒ–çš„LLMæ£€æŸ¥ç‚¹ä¸‰å…ƒç»„ï¼›2) åœ¨è¿™äº›æ£€æŸ¥ç‚¹ä¹‹é—´è®­ç»ƒç¨€ç–äº’ç¼–ç å™¨ï¼Œå­¦ä¹ ç‰¹å¾ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼›3) ä½¿ç”¨RelIEæŒ‡æ ‡è¯„ä¼°æ¯ä¸ªç‰¹å¾å¯¹ä»»åŠ¡æ€§èƒ½çš„å› æœé‡è¦æ€§ï¼›4) åˆ†æç‰¹å¾çš„æ¼”å˜è¿‡ç¨‹ï¼Œæ£€æµ‹ç‰¹å¾çš„æ¶Œç°ã€ç»´æŒå’Œåœæ­¢ã€‚äº’ç¼–ç å™¨çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–é‡æ„è¯¯å·®ï¼ŒåŒæ—¶é¼“åŠ±ç‰¹å¾çš„ç¨€ç–æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§åŸºäºç¨€ç–äº’ç¼–ç å™¨çš„ç‰¹å¾å¯¹é½æ–¹æ³•ï¼Œå¯ä»¥è·Ÿè¸ªLLMé¢„è®­ç»ƒè¿‡ç¨‹ä¸­è¯­è¨€ç‰¹å¾çš„æ¼”å˜ï¼›2) å¼•å…¥äº†ç›¸å¯¹é—´æ¥æ•ˆåº”(RelIE)æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ç‰¹å¾çš„å› æœé‡è¦æ€§ï¼›3) è¯¥æ–¹æ³•ä¸æ¨¡å‹æ¶æ„æ— å…³ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šäº’ç¼–ç å™¨çš„è®­ç»ƒé‡‡ç”¨äº†L1æ­£åˆ™åŒ–ï¼Œä»¥é¼“åŠ±ç‰¹å¾çš„ç¨€ç–æ€§ã€‚RelIEæŒ‡æ ‡çš„è®¡ç®—æ¶‰åŠå¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œå¹²é¢„ï¼Œå¹¶è¯„ä¼°å¹²é¢„å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚å…·ä½“è€Œè¨€ï¼ŒRelIEå®šä¹‰ä¸ºç‰¹å¾æ¿€æ´»å€¼å˜åŒ–å¯¹æ¨¡å‹è¾“å‡ºå˜åŒ–çš„å› æœæ•ˆåº”ï¼Œé€šè¿‡è®¡ç®—æ€»æ•ˆåº”å’Œç›´æ¥æ•ˆåº”çš„å·®å€¼å¾—åˆ°é—´æ¥æ•ˆåº”ï¼Œå†è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†äº’ç¼–ç å™¨å¯ä»¥æœ‰æ•ˆåœ°æ£€æµ‹é¢„è®­ç»ƒæœŸé—´ç‰¹å¾çš„æ¶Œç°ã€ç»´æŒå’Œåœæ­¢ã€‚RelIEæŒ‡æ ‡èƒ½å¤Ÿå‡†ç¡®åœ°è¯„ä¼°ç‰¹å¾çš„å› æœé‡è¦æ€§ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªå¼€æºLLMæ£€æŸ¥ç‚¹ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒæŸäº›è¯­è¨€ç‰¹å¾åœ¨é¢„è®­ç»ƒçš„æ—©æœŸé˜¶æ®µå°±å·²æ¶Œç°ï¼Œå¹¶åœ¨åç»­é˜¶æ®µå¾—åˆ°ç»´æŒï¼Œè€Œå¦ä¸€äº›ç‰¹å¾åˆ™åœ¨åæœŸé˜¶æ®µé€æ¸æ¶ˆå¤±ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç†è§£å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºæŒ‡å¯¼é¢„è®­ç»ƒæ•°æ®çš„é€‰æ‹©ã€ä¼˜åŒ–è®­ç»ƒç­–ç•¥ã€ä»¥åŠè¯Šæ–­æ¨¡å‹ä¸­çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºåˆ†æä¸åŒæ¨¡å‹æ¶æ„ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥åŠè¯„ä¼°ä¸åŒé¢„è®­ç»ƒç›®æ ‡çš„å½±å“ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºå¼€å‘æ›´å¯æ§ã€æ›´é«˜æ•ˆçš„LLMè®­ç»ƒæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) learn non-trivial abstractions during pretraining, like detecting irregular plural noun subjects. However, it is not well understood when and how specific linguistic abilities emerge as traditional evaluation methods such as benchmarking fail to reveal how models acquire concepts and capabilities. To bridge this gap and better understand model training at the concept level, we use sparse crosscoders to discover and align features across model checkpoints. Using this approach, we track the evolution of linguistic features during pretraining. We train crosscoders between open-sourced checkpoint triplets with significant performance and representation shifts, and introduce a novel metric, Relative Indirect Effects (RelIE), to trace training stages at which individual features become causally important for task performance. We show that crosscoders can detect feature emergence, maintenance, and discontinuation during pretraining. Our approach is architecture-agnostic and scalable, offering a promising path toward more interpretable and fine-grained analysis of representation learning throughout pretraining.

