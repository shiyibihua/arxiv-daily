---
layout: default
title: ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning
---

# ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04903" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04903v2</a>
  <a href="https://arxiv.org/pdf/2509.04903.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04903v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04903v2', 'ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianghao Chen, Wei Sun, Qixiang Yin, Lingxing Kong, Zhixing Tan, Jiajun Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-09-10)

**å¤‡æ³¨**: Under review, our code is available at https://github.com/ZNLP/ACE-RL

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºACE-RLæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”çº¦æŸå¢å¼ºå¥–åŠ±ï¼Œæå‡é•¿æ–‡æœ¬ç”Ÿæˆè´¨é‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æœ¬ç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `è‡ªé€‚åº”çº¦æŸ` `å¥–åŠ±å‡½æ•°` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é•¿æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ä¾èµ–å¤§é‡é«˜è´¨é‡æ•°æ®ï¼Œä¸”ä¼˜åŒ–ç»´åº¦ç²—ç³™ï¼Œéš¾ä»¥æ»¡è¶³å¤šæ ·åŒ–åœºæ™¯éœ€æ±‚ã€‚
2. ACE-RLæ¡†æ¶é€šè¿‡è‡ªé€‚åº”åœ°å°†æŒ‡ä»¤åˆ†è§£ä¸ºç»†ç²’åº¦çº¦æŸï¼Œå¹¶ä»¥æ­¤è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒACE-RLåœ¨é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†GPT-4oç­‰ä¸“æœ‰ç³»ç»Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨é•¿æ–‡æœ¬ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é«˜è´¨é‡é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼š(1)è¿‡åº¦ä¾èµ–ç¨€ç¼ºçš„é«˜è´¨é‡é•¿æ–‡æœ¬å›å¤æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒ(SFT)æˆ–å¼ºåŒ–å­¦ä¹ (RL)ä¸­çš„æˆå¯¹åå¥½å¥–åŠ±ã€‚(2)ä¾§é‡äºç²—ç²’åº¦çš„è´¨é‡ä¼˜åŒ–ç»´åº¦ï¼Œå¦‚ç›¸å…³æ€§ã€è¿è´¯æ€§å’Œæœ‰ç”¨æ€§ï¼Œå¿½ç•¥äº†å„ç§é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­å›ºæœ‰çš„ç»†ç²’åº¦ç‰¹æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½¿ç”¨è‡ªé€‚åº”çº¦æŸå¢å¼ºå¥–åŠ±çš„é•¿æ–‡æœ¬ç”Ÿæˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶(ACE-RL)ã€‚ACE-RLé¦–å…ˆé€šè¿‡è¯†åˆ«å…¶æ½œåœ¨æ„å›¾å’Œéœ€æ±‚ï¼Œè‡ªåŠ¨å°†æ¯ä¸ªæŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç»„ç»†ç²’åº¦çš„è‡ªé€‚åº”çº¦æŸæ¡ä»¶ã€‚éšåï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¥–åŠ±æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åŸºäºé•¿æ–‡æœ¬å›å¤å¯¹ç›¸åº”çº¦æŸçš„æ»¡è¶³ç¨‹åº¦æ¥é‡åŒ–å…¶è´¨é‡ï¼Œå°†ä¸»è§‚è´¨é‡è¯„ä¼°è½¬åŒ–ä¸ºçº¦æŸéªŒè¯ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥å¼•å¯¼æ¨¡å‹æœç€å“è¶Šçš„é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›å‘å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ACE-RLæ¡†æ¶åœ¨WritingBenchä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„SFTå’ŒRLåŸºçº¿ï¼Œåˆ†åˆ«æé«˜äº†20.70%å’Œ7.32%ï¼Œæˆ‘ä»¬è¡¨ç°æœ€ä½³çš„æ¨¡å‹ç”šè‡³è¶…è¿‡äº†GPT-4oç­‰ä¸“æœ‰ç³»ç»Ÿ7.10%ï¼Œä¸ºLLMåœ¨å„ç§é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­ç”Ÿæˆé«˜è´¨é‡å†…å®¹æä¾›äº†ä¸€ç§æ›´æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é•¿æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ä¸»è¦é¢ä¸´ä¸¤ä¸ªé—®é¢˜ï¼šä¸€æ˜¯éœ€è¦å¤§é‡é«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œç›‘ç£å­¦ä¹ æˆ–å¼ºåŒ–å­¦ä¹ ï¼Œè€Œè¿™äº›æ•°æ®å¾€å¾€ç¨€ç¼ºä¸”æ˜‚è´µï¼›äºŒæ˜¯ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨ç²—ç²’åº¦çš„è´¨é‡æŒ‡æ ‡ï¼Œå¦‚ç›¸å…³æ€§ã€è¿è´¯æ€§å’Œæœ‰ç”¨æ€§ï¼Œå¿½ç•¥äº†ä¸åŒé•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸‹ç»†ç²’åº¦çš„éœ€æ±‚ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡éš¾ä»¥ä¿è¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šACE-RLçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—ç»†ç²’åº¦çš„çº¦æŸæ¡ä»¶ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªå¥–åŠ±å‡½æ•°æ¥è¡¡é‡ç”Ÿæˆæ–‡æœ¬å¯¹è¿™äº›çº¦æŸæ¡ä»¶çš„æ»¡è¶³ç¨‹åº¦ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ å¦‚ä½•ç”Ÿæˆæ»¡è¶³è¿™äº›çº¦æŸæ¡ä»¶çš„æ–‡æœ¬ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚è¿™ç§æ–¹æ³•å°†ä¸»è§‚çš„è´¨é‡è¯„ä¼°è½¬åŒ–ä¸ºå®¢è§‚çš„çº¦æŸéªŒè¯ï¼Œé™ä½äº†å¯¹é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šACE-RLæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æŒ‡ä»¤è§£ææ¨¡å—ï¼šå°†ç”¨æˆ·æŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç»„ç»†ç²’åº¦çš„çº¦æŸæ¡ä»¶ï¼Œè¿™äº›çº¦æŸæ¡ä»¶åæ˜ äº†ç”¨æˆ·å¯¹ç”Ÿæˆæ–‡æœ¬çš„æœŸæœ›ã€‚2) å¥–åŠ±å‡½æ•°è®¾è®¡æ¨¡å—ï¼šè®¾è®¡ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œç”¨äºè¡¡é‡ç”Ÿæˆæ–‡æœ¬å¯¹çº¦æŸæ¡ä»¶çš„æ»¡è¶³ç¨‹åº¦ã€‚è¯¥å¥–åŠ±å‡½æ•°å°†ä¸»è§‚çš„è´¨é‡è¯„ä¼°è½¬åŒ–ä¸ºå®¢è§‚çš„çº¦æŸéªŒè¯ã€‚3) å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ»¡è¶³çº¦æŸæ¡ä»¶çš„æ–‡æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šACE-RLçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªé€‚åº”çº¦æŸå¢å¼ºå¥–åŠ±æœºåˆ¶ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸ä½¿ç”¨äººå·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œè¿™äº›å¥–åŠ±å‡½æ•°éš¾ä»¥æ•æ‰é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„å¤æ‚æ€§ã€‚ACE-RLé€šè¿‡è‡ªåŠ¨å°†æŒ‡ä»¤åˆ†è§£ä¸ºç»†ç²’åº¦çš„çº¦æŸæ¡ä»¶ï¼Œå¹¶ä»¥æ­¤è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¡¡é‡ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ï¼Œä»è€Œæé«˜ç”Ÿæˆæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šACE-RLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•å°†æŒ‡ä»¤åˆ†è§£ä¸ºç»†ç²’åº¦çš„çº¦æŸæ¡ä»¶ï¼Ÿè®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œä¾‹å¦‚è¯­ä¹‰è§£æã€å®ä½“è¯†åˆ«ç­‰ã€‚2) å¦‚ä½•è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Ÿå¥–åŠ±å‡½æ•°éœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°è¡¡é‡ç”Ÿæˆæ–‡æœ¬å¯¹çº¦æŸæ¡ä»¶çš„æ»¡è¶³ç¨‹åº¦ã€‚è®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†å„ç§æŒ‡æ ‡ï¼Œä¾‹å¦‚æ–‡æœ¬ç›¸ä¼¼åº¦ã€ä¿¡æ¯å®Œæ•´åº¦ç­‰ã€‚3) ä½¿ç”¨äº†ä»€ä¹ˆå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Ÿå¸¸è§çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åŒ…æ‹¬ç­–ç•¥æ¢¯åº¦ã€Q-learningç­‰ã€‚å…·ä½“ä½¿ç”¨çš„ç®—æ³•ä»¥åŠè¶…å‚æ•°è®¾ç½®éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ACE-RLæ¡†æ¶åœ¨WritingBenchæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸æ¯”äºç°æœ‰çš„SFTåŸºçº¿æé«˜äº†20.70%ï¼Œç›¸æ¯”äºRLåŸºçº¿æé«˜äº†7.32%ã€‚æ›´ä»¤äººç©ç›®çš„æ˜¯ï¼ŒACE-RLçš„æœ€ä½³æ¨¡å‹ç”šè‡³è¶…è¶Šäº†GPT-4oç­‰ä¸“æœ‰ç³»ç»Ÿ7.10%ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ACE-RLæ¡†æ¶å¯åº”ç”¨äºå¤šç§é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ï¼Œå¦‚æ–‡ç« å†™ä½œã€æ•…äº‹åˆ›ä½œã€å¯¹è¯ç”Ÿæˆç­‰ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæå‡LLMåœ¨è¿™äº›åœºæ™¯ä¸‹çš„ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated remarkable progress in long-context understanding, yet they face significant challenges in high-quality long-form generation. Existing studies primarily suffer from two limitations: (1) A heavy reliance on scarce, high-quality long-form response data for supervised fine-tuning (SFT) or for pairwise preference reward in reinforcement learning (RL). (2) Focus on coarse-grained quality optimization dimensions, such as relevance, coherence, and helpfulness, overlooking the fine-grained specifics inherent to diverse long-form generation scenarios. To address this issue, we propose a framework using Adaptive Constraint-Enhanced reward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first automatically deconstructs each instruction into a set of fine-grained, adaptive constraint criteria by identifying its underlying intents and demands. Subsequently, we design a reward mechanism that quantifies the quality of long-form responses based on their satisfaction over corresponding constraints, converting subjective quality evaluation into constraint verification. Finally, we utilize reinforcement learning to guide models toward superior long-form generation capabilities. Experimental results demonstrate that our ACE-RL framework significantly outperforms existing SFT and RL baselines by 20.70% and 7.32% on WritingBench, and our top-performing model even surpasses proprietary systems like GPT-4o by 7.10%, providing a more effective training paradigm for LLMs to generate high-quality content across diverse long-form generation scenarios.

