---
layout: default
title: Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation
---

# Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05226" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05226v1</a>
  <a href="https://arxiv.org/pdf/2509.05226.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05226v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05226v1', 'Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Abdul Waheed, Chancharik Mitra, Laurie Z. Wang, Deva Ramanan, Bhiksha Raj

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

**å¤‡æ³¨**: 28 Pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéš¾åº¦æ„ŸçŸ¥çš„æ€ç»´é“¾è’¸é¦æ–¹æ³•ï¼Œæå‡æ•°å­¦æ¨ç†æ•ˆç‡å¹¶å‡å°‘å†—ä½™tokenã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ€ç»´é“¾æ¨ç†` `éš¾åº¦æ„ŸçŸ¥` `è’¸é¦è®­ç»ƒ` `æ•°å­¦æ¨ç†` `ç›‘ç£å¾®è°ƒ` `ç›´æ¥åå¥½ä¼˜åŒ–` `åŠ¨æ€æ¨ç†` `åè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ€ç»´é“¾æ¨ç†æ–¹æ³•åœ¨ç®€å•é—®é¢˜ä¸Šäº§ç”Ÿå†—ä½™è¾“å‡ºï¼Œæ•ˆç‡è¾ƒä½ã€‚
2. æå‡ºéš¾åº¦æ„ŸçŸ¥çš„æ¨ç†æ¡†æ¶ï¼Œæ¨¡å‹æ ¹æ®é—®é¢˜éš¾åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ã€‚
3. é€šè¿‡ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæ¨¡å‹åœ¨å‡å°‘tokençš„åŒæ—¶ä¿æŒæˆ–æå‡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ€ç»´é“¾æ¨ç†è™½ç„¶å¼ºå¤§ï¼Œä½†å¯¹äºç®€å•é—®é¢˜å¯èƒ½ä¼šäº§ç”Ÿä¸å¿…è¦çš„å†—é•¿è¾“å‡ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªéš¾åº¦æ„ŸçŸ¥çš„æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒæ¨¡å‹æ ¹æ®é—®é¢˜çš„å¤æ‚æ€§åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œç ”ç©¶è¡¨æ˜ï¼Œæ— éœ€ä»»ä½•æ¶æ„ä¿®æ”¹ï¼Œä»…é€šè¿‡åœ¨ç²¾å¿ƒç­–åˆ’çš„æ•°æ®ä¸Šè¿›è¡Œåè®­ç»ƒï¼Œå³å¯èµ‹äºˆæ¨¡å‹è¿™ç§åŠ¨æ€æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›æ•°æ®åŒ…å«é•¿åº¦ä¸é—®é¢˜éš¾åº¦æˆæ¯”ä¾‹çš„æ€ç»´é“¾è½¨è¿¹ã€‚åˆ†æè¡¨æ˜ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œçš„åè®­ç»ƒä¸»è¦æ•è·æ¨ç†é•¿åº¦å’Œæ ¼å¼ç­‰æ¨¡å¼ï¼Œè€Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åˆ™ä¿ç•™æ¨ç†å‡†ç¡®æ€§ï¼Œå®ƒä»¬çš„ç»„åˆæ—¢èƒ½å‡å°‘é•¿åº¦ï¼Œåˆèƒ½ä¿æŒæˆ–æé«˜æ€§èƒ½ã€‚å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°éƒ½è¯å®ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¼šâ€œæŒ‰æ¯”ä¾‹æ€è€ƒâ€ï¼Œåœ¨ç®€å•é—®é¢˜ä¸Šè¿›è¡Œæœ€å°åŒ–æ¨ç†ï¼ŒåŒæ—¶ä¿æŒå¤æ‚é—®é¢˜çš„æ·±åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ•°å­¦æ¨ç†ä¸­ï¼Œç°æœ‰æ€ç»´é“¾æ–¹æ³•åœ¨å¤„ç†ç®€å•é—®é¢˜æ—¶äº§ç”Ÿè¿‡å¤šå†—ä½™tokençš„é—®é¢˜ã€‚ç°æœ‰çš„æ€ç»´é“¾æ–¹æ³•é€šå¸¸é‡‡ç”¨å›ºå®šçš„æ¨ç†æ·±åº¦ï¼Œæ— è®ºé—®é¢˜éš¾åº¦å¦‚ä½•ï¼Œéƒ½ä¼šç”Ÿæˆç›¸å¯¹å†—é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´è®¡ç®—èµ„æºçš„æµªè´¹å’Œæ¨ç†æ•ˆç‡çš„é™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®©æ¨¡å‹å…·å¤‡éš¾åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œå³èƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„éš¾åº¦åŠ¨æ€è°ƒæ•´æ¨ç†çš„æ·±åº¦ã€‚å¯¹äºç®€å•çš„é—®é¢˜ï¼Œæ¨¡å‹åº”è¯¥èƒ½å¤Ÿå¿«é€Ÿç»™å‡ºç­”æ¡ˆï¼Œè€Œå¯¹äºå¤æ‚çš„é—®é¢˜ï¼Œåˆ™éœ€è¦è¿›è¡Œæ›´æ·±å…¥çš„æ¨ç†ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å‡å°‘å†—ä½™tokençš„ç”Ÿæˆï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦é€šè¿‡åè®­ç»ƒï¼ˆpost-trainingï¼‰æ¥å®ç°éš¾åº¦æ„ŸçŸ¥èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œé¦–å…ˆæ„å»ºä¸€ä¸ªåŒ…å«ä¸åŒéš¾åº¦çº§åˆ«æ•°å­¦é—®é¢˜çš„æ•°æ®é›†ï¼Œå¹¶ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆä¸å…¶éš¾åº¦ç›¸åŒ¹é…çš„æ€ç»´é“¾æ¨ç†è¿‡ç¨‹ã€‚ç„¶åï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåè®­ç»ƒã€‚SFTä¸»è¦ç”¨äºå­¦ä¹ æ¨ç†é•¿åº¦å’Œæ ¼å¼ç­‰æ¨¡å¼ï¼Œè€ŒDPOåˆ™ç”¨äºä¿ç•™æ¨ç†çš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹æ¨¡å‹æ¶æ„çš„å‰æä¸‹ï¼Œèµ‹äºˆæ¨¡å‹éš¾åº¦æ„ŸçŸ¥çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®é›†å’Œåè®­ç»ƒç­–ç•¥ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¼šæ ¹æ®é—®é¢˜çš„éš¾åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œä»è€Œåœ¨ä¿è¯æ¨ç†å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘å†—ä½™tokençš„ç”Ÿæˆã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ„å»ºéš¾åº¦åˆ†çº§çš„æ•°æ®é›†ï¼Œç¡®ä¿æ•°æ®é›†ä¸­åŒ…å«ä¸åŒéš¾åº¦çº§åˆ«çš„æ•°å­¦é—®é¢˜ï¼Œå¹¶ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆä¸å…¶éš¾åº¦ç›¸åŒ¹é…çš„æ€ç»´é“¾æ¨ç†è¿‡ç¨‹ã€‚2) ä½¿ç”¨SFTå’ŒDPOè¿›è¡Œåè®­ç»ƒï¼ŒSFTç”¨äºå­¦ä¹ æ¨ç†é•¿åº¦å’Œæ ¼å¼ï¼ŒDPOç”¨äºä¿ç•™æ¨ç†å‡†ç¡®æ€§ã€‚3) æ¢ç´¢SFTå’ŒDPOçš„ç»„åˆæ–¹å¼ï¼Œä»¥å®ç°æœ€ä½³çš„æ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å‡å°‘æ¨ç†é•¿åº¦çš„åŒæ—¶ï¼Œä¿æŒæˆ–æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚é€šè¿‡SFTå’ŒDPOçš„ç»„åˆï¼Œæ¨¡å‹å¯ä»¥å­¦ä¼šâ€œæŒ‰æ¯”ä¾‹æ€è€ƒâ€ï¼Œåœ¨ç®€å•é—®é¢˜ä¸Šè¿›è¡Œæœ€å°åŒ–æ¨ç†ï¼ŒåŒæ—¶ä¿æŒå¤æ‚é—®é¢˜çš„æ·±åº¦ã€‚å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°éƒ½è¯å®äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æ•°å­¦æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨è§£é¢˜ç³»ç»Ÿã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿç­‰ã€‚é€šè¿‡å‡å°‘å†—ä½™tokençš„ç”Ÿæˆï¼Œå¯ä»¥æé«˜æ¨ç†æ•ˆç‡ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶æå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å¸¸è¯†æ¨ç†ã€é€»è¾‘æ¨ç†ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-thought reasoning, while powerful, can produce unnecessarily verbose output for simpler problems. We present a framework for difficulty-aware reasoning that teaches models to dynamically adjust reasoning depth based on problem complexity. Remarkably, we show that models can be endowed with such dynamic inference pathways without any architectural modifications; we simply post-train on data that is carefully curated to include chain-of-thought traces that are proportional in length to problem difficulty. Our analysis reveals that post-training via supervised fine-tuning (SFT) primarily captures patterns like reasoning length and format, while direct preference optimization (DPO) preserves reasoning accuracy, with their combination reducing length and maintaining or improving performance. Both quantitative metrics and qualitative assessments confirm that models can learn to "think proportionally", reasoning minimally on simple problems while maintaining depth for complex ones.

