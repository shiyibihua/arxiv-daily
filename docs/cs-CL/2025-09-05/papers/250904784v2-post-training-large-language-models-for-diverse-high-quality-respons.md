---
layout: default
title: Post-training Large Language Models for Diverse High-Quality Responses
---

# Post-training Large Language Models for Diverse High-Quality Responses

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04784" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04784v2</a>
  <a href="https://arxiv.org/pdf/2509.04784.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04784v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04784v2', 'Post-training Large Language Models for Diverse High-Quality Responses')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yilei Chen, Souradip Chakraborty, Lorenz Wolf, Yannis Paschalidis, Aldo Pacchiano

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-10-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDQOæ–¹æ³•ï¼Œåœ¨åè®­ç»ƒé˜¶æ®µæå‡å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›å¤çš„å¤šæ ·æ€§å’Œè´¨é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `åè®­ç»ƒ` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ ·æ€§ä¼˜åŒ–` `è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ åè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œé€šå¸¸ä¼šé™ä½æ¨¡å‹è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œå¯¼è‡´å›å¤è¿‡äºå•ä¸€ã€‚
2. è®ºæ–‡æå‡ºDQOæ–¹æ³•ï¼ŒåŸºäºè¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼Œè”åˆä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„è´¨é‡å’Œè¯­ä¹‰å¤šæ ·æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDQOæ–¹æ³•åœ¨æŒ‡ä»¤è·Ÿéšã€æ‘˜è¦ã€æ•…äº‹ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æé«˜äº†è¯­ä¹‰å¤šæ ·æ€§ï¼Œä¸”ä¸ç‰ºç‰²æ¨¡å‹è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)å·²æˆä¸ºåè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ä¸€ç§æµè¡Œæ–¹æ³•ã€‚è™½ç„¶å®ƒæé«˜äº†æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä½†é€šå¸¸ä¼šé™ä½æ¨¡å‹è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œå¯¼è‡´ç‹­çª„çš„ã€è§„èŒƒçš„å›å¤ã€‚ç°æœ‰çš„å¢å¼ºå¤šæ ·æ€§çš„æ–¹æ³•æ˜¯æœ‰é™çš„ï¼Œè¦ä¹ˆåœ¨æ¨ç†æ—¶æ“ä½œï¼Œè¦ä¹ˆä¾§é‡äºè¡¨é¢ä¸Šçš„å·®å¼‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºDQO(å¤šæ ·æ€§è´¨é‡ä¼˜åŒ–)çš„æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºè¡Œåˆ—å¼ç‚¹è¿‡ç¨‹(DPPs)æ¥è”åˆä¼˜åŒ–LLMsçš„è´¨é‡å’Œè¯­ä¹‰å¤šæ ·æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæ¯ä¸ªæç¤ºé‡‡æ ·å¹¶åµŒå…¥ä¸€ç»„å“åº”ï¼Œç„¶åä½¿ç”¨åŸºäºæ ¸çš„ç›¸ä¼¼çŸ©é˜µçš„è¡Œåˆ—å¼æ¥æµ‹é‡å¤šæ ·æ€§ï¼Œä½œä¸ºè¿™äº›å“åº”åµŒå…¥æ‰€è·¨è¶Šçš„ä½“ç§¯ã€‚DQOæ˜¯çµæ´»çš„ï¼Œå¯ä»¥åº”ç”¨äºç°æœ‰çš„RLç®—æ³•ä¹‹ä¸Šã€‚è·¨æŒ‡ä»¤è·Ÿéšã€æ‘˜è¦ã€æ•…äº‹ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ç‰ºç‰²æ¨¡å‹è´¨é‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜äº†è¯­ä¹‰å¤šæ ·æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMåè®­ç»ƒæ–¹æ³•ï¼Œè™½ç„¶èƒ½æå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä½†å¾€å¾€ä¼šç‰ºç‰²ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œå¯¼è‡´æ¨¡å‹å€¾å‘äºç”Ÿæˆé‡å¤ã€åˆ»æ¿çš„å›å¤ã€‚ç°æœ‰çš„æå‡å¤šæ ·æ€§çš„æ–¹æ³•è¦ä¹ˆåªèƒ½åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œï¼Œè¦ä¹ˆåªå…³æ³¨è¡¨é¢çš„è¯æ±‡å·®å¼‚ï¼Œæ— æ³•æœ‰æ•ˆæå‡è¯­ä¹‰å±‚é¢çš„å¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDQOçš„æ ¸å¿ƒæ€è·¯æ˜¯åŒæ—¶ä¼˜åŒ–LLMç”Ÿæˆå›å¤çš„è´¨é‡å’Œè¯­ä¹‰å¤šæ ·æ€§ã€‚é€šè¿‡è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPsï¼‰æ¥å»ºæ¨¡å›å¤é›†åˆçš„å¤šæ ·æ€§ï¼Œå¹¶å°†å…¶èå…¥åˆ°è®­ç»ƒç›®æ ‡ä¸­ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„å›å¤ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯è®©æ¨¡å‹åœ¨æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿèƒ½ä¿æŒæˆ–æå‡ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDQOæ–¹æ³•å¯ä»¥çœ‹ä½œæ˜¯ç°æœ‰å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å¢å¼ºæ¨¡å—ã€‚å…¶æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1. å¯¹äºæ¯ä¸ªè¾“å…¥promptï¼ŒLLMç”Ÿæˆä¸€ç»„å€™é€‰å›å¤ã€‚2. å°†è¿™äº›å›å¤åµŒå…¥åˆ°é«˜ç»´è¯­ä¹‰ç©ºé—´ä¸­ã€‚3. è®¡ç®—è¿™äº›åµŒå…¥å‘é‡çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼Œå¹¶åˆ©ç”¨è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹è®¡ç®—å¤šæ ·æ€§åº¦é‡ã€‚4. å°†å¤šæ ·æ€§åº¦é‡ä¸è´¨é‡åº¦é‡ï¼ˆä¾‹å¦‚å¥–åŠ±ä¿¡å·ï¼‰ç»“åˆï¼Œå½¢æˆæœ€ç»ˆçš„ä¼˜åŒ–ç›®æ ‡ã€‚5. ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ›´æ–°LLMçš„å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šDQOçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPsï¼‰æ¥å»ºæ¨¡å’Œä¼˜åŒ–å›å¤é›†åˆçš„è¯­ä¹‰å¤šæ ·æ€§ã€‚DPPsèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¡é‡é›†åˆä¸­å…ƒç´ ä¹‹é—´çš„äº’æ–¥æ€§ï¼Œä»è€Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„å›å¤ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDQOèƒ½å¤Ÿç›´æ¥åœ¨è®­ç»ƒé˜¶æ®µä¼˜åŒ–å¤šæ ·æ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ•æ‰åˆ°è¯­ä¹‰å±‚é¢çš„å·®å¼‚ï¼Œè€Œä¸ä»…ä»…æ˜¯è¡¨é¢ä¸Šçš„è¯æ±‡å·®å¼‚ã€‚

**å…³é”®è®¾è®¡**ï¼šDQOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. å¦‚ä½•é€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹æ¥è¡¨ç¤ºå›å¤çš„è¯­ä¹‰ä¿¡æ¯ã€‚2. å¦‚ä½•å®šä¹‰ç›¸ä¼¼åº¦çŸ©é˜µï¼Œä»¥å‡†ç¡®åæ˜ å›å¤ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚3. å¦‚ä½•å¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´çš„æƒé‡ï¼Œä»¥è·å¾—æœ€ä½³çš„æ€§èƒ½ã€‚4. å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆå›å¤çš„åµŒå…¥å‘é‡ï¼Œä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°æ¥è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼Œå¹¶ä½¿ç”¨è¶…å‚æ•°æ¥æ§åˆ¶è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´çš„trade-offã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDQOæ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ•…äº‹ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒDQOèƒ½å¤Ÿç”Ÿæˆæ›´åŠ å¤šæ ·åŒ–å’Œå¯Œæœ‰åˆ›æ„çš„æ•…äº‹ï¼ŒåŒæ—¶ä¿æŒäº†æ•…äº‹çš„è¿è´¯æ€§å’Œå¯è¯»æ€§ã€‚åœ¨æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­ï¼ŒDQOèƒ½å¤Ÿç”Ÿæˆæ›´åŠ ç¬¦åˆç”¨æˆ·æ„å›¾å’Œéœ€æ±‚çš„å¤šæ ·åŒ–å›å¤ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒDQOåœ¨å¤šæ ·æ€§æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼ŒåŒæ—¶ä¿æŒäº†æˆ–ç•¥å¾®æå‡äº†è´¨é‡æŒ‡æ ‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DQOæ–¹æ³•å¯ä»¥å¹¿æ³›åº”ç”¨äºéœ€è¦å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–é«˜è´¨é‡å›å¤çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€å†…å®¹ç”Ÿæˆã€åˆ›æ„å†™ä½œç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹ç”Ÿæˆå›å¤çš„å¤šæ ·æ€§ï¼Œå¯ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæé«˜å†…å®¹ç”Ÿæˆçš„è´¨é‡å’Œåˆ›æ–°æ€§ã€‚è¯¥æ–¹æ³•è¿˜æœ‰æ½œåŠ›åº”ç”¨äºå¤šæ™ºèƒ½ä½“åä½œç­‰é¢†åŸŸï¼Œä¿ƒè¿›æ™ºèƒ½ä½“ä¹‹é—´çš„å¤šæ ·åŒ–è¡Œä¸ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has emerged as a popular method for post-training large language models (LLMs). While improving the model's performance on downstream tasks, it often reduces the model's output diversity, leading to narrow, canonical responses. Existing methods to enhance diversity are limited, either by operating at inference time or by focusing on surface-level differences. We propose a novel training method named DQO (Diversity Quality Optimization) based on determinantal point processes (DPPs) to jointly optimize LLMs for quality and semantic diversity. Our approach samples and embeds a group of responses for each prompt, then uses the determinant of a kernel-based similarity matrix to measure diversity as the volume spanned by the embeddings of these responses. DQO is flexible and can be applied on top of existing RL algorithms. Experiments across instruction-following, summarization, story generation, and reasoning tasks demonstrate that our method substantially improves semantic diversity without sacrificing model quality.

