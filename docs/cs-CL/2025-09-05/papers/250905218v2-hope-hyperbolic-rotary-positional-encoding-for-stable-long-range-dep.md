---
layout: default
title: HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models
---

# HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05218" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.05218v2</a>
  <a href="https://arxiv.org/pdf/2509.05218.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05218v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05218v2', 'HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Chang Dai, Hongyu Shan, Mingyang Song, Di Liang

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-05 (Êõ¥Êñ∞: 2025-09-08)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫HoPEÔºö‰∏ÄÁßçÁî®‰∫éÁ®≥ÂÆöÈïøÁ®ã‰æùËµñÂª∫Ê®°ÁöÑÂèåÊõ≤ÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†Å**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰ΩçÁΩÆÁºñÁ†Å` `ÈïøÁ®ã‰æùËµñ` `Transformer` `ÂèåÊõ≤Âá†‰Ωï` `Ê¥õ‰º¶ÂÖπÂèòÊç¢`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ≥ïÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂Â≠òÂú®ÈóÆÈ¢òÔºåÁªùÂØπ‰ΩçÁΩÆÁºñÁ†ÅÂ§ñÊé®ÊÄßÂ∑ÆÔºåRoPEÂ≠òÂú®ÊåØËç°Ê≥®ÊÑèÂäõÊ®°ÂºèÔºåÂΩ±ÂìçÈïøÁ®ã‰æùËµñÂª∫Ê®°„ÄÇ
2. HoPEÈÄöËøáÂá†‰ΩïÈáçÊûÑ‰ΩçÁΩÆÁºñÁ†ÅÔºåÂà©Áî®ÂèåÊõ≤Âá†‰Ωï‰∏≠ÁöÑÊ¥õ‰º¶ÂÖπÂèòÊç¢ÔºåÂØπtokenË°®Á§∫ËøõË°åÊóãËΩ¨ÔºåÂÆûÁé∞Ê≥®ÊÑèÂäõÊùÉÈáçÁöÑÂçïË∞ÉË°∞Âáè„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHoPEÂú®ÈïøÂ∫èÂàóÂª∫Ê®°‰ªªÂä°‰∏≠ÔºåÊÄßËÉΩ‰ºò‰∫éÁé∞Êúâ‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ≥ïÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Ë°®Á§∫ÂíåÊé®ÂπøÈïøÁ®ã‰æùËµñÂÖ≥Á≥ª„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰ΩçÁΩÆÁºñÁ†ÅÊú∫Âà∂‰ΩøTransformerËÉΩÂ§üÂØπÊñáÊú¨‰∏≠ÁöÑÂ∫èÂàóÁªìÊûÑÂíåÈïøÁ®ã‰æùËµñÂÖ≥Á≥ªËøõË°åÂª∫Ê®°„ÄÇÁÑ∂ËÄåÔºåÁªùÂØπ‰ΩçÁΩÆÁºñÁ†ÅÁî±‰∫éÂõ∫ÂÆöÁöÑ‰ΩçÁΩÆË°®Á§∫ÔºåÈöæ‰ª•Êé®ÂπøÂà∞Êõ¥ÈïøÁöÑÂ∫èÂàóÔºõËÄåÂÉèAlibiËøôÊ†∑ÁöÑÁõ∏ÂØπÊñπÊ≥ïÂú®ÊûÅÈïø‰∏ä‰∏ãÊñá‰∏≠Ë°®Áé∞Âá∫ÊÄßËÉΩ‰∏ãÈôç„ÄÇÂπøÊ≥õ‰ΩøÁî®ÁöÑÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†ÅÔºàRoPEÔºâÂºïÂÖ•‰∫ÜÊåØËç°ÁöÑÊ≥®ÊÑèÂäõÊ®°ÂºèÔºåÈòªÁ¢ç‰∫ÜÁ®≥ÂÆöÁöÑÈïøË∑ùÁ¶ª‰æùËµñÂª∫Ê®°„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÈÄöËøáÂØπ‰ΩçÁΩÆÁºñÁ†ÅËøõË°åÂá†‰ΩïÈáçÊûÑ„ÄÇÂèóÂà∞ÂèåÊõ≤Âá†‰Ωï‰∏≠Ê¥õ‰º¶ÂÖπÂèòÊç¢ÁöÑÂêØÂèëÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂèåÊõ≤ÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†ÅÔºàHoPEÔºâÔºåÂÆÉÂà©Áî®ÂèåÊõ≤ÂáΩÊï∞ÂØπtokenË°®Á§∫ÊâßË°åÊ¥õ‰º¶ÂÖπÊóãËΩ¨„ÄÇÁêÜËÆ∫ÂàÜÊûêË°®ÊòéÔºåRoPEÊòØÊàë‰ª¨ÁöÑÂπø‰πâÂÖ¨ÂºèÁöÑ‰∏Ä‰∏™Áâπ‰æã„ÄÇHoPEÈÄöËøáÂº∫Âà∂Ê≥®ÊÑèÂäõÊùÉÈáçÈöètokenË∑ùÁ¶ªÁöÑÂ¢ûÂä†ËÄåÂçïË∞ÉË°∞ÂáèÔºå‰ªéÊ†πÊú¨‰∏äËß£ÂÜ≥‰∫ÜRoPEÁöÑÊåØËç°ÈóÆÈ¢ò„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åÁªìÊûúÔºåÂåÖÊã¨Âú®Âá†‰∏™Êâ©Â±ïÂ∫èÂàóÂü∫ÂáÜ‰∏ãÁöÑÂõ∞ÊÉëÂ∫¶ËØÑ‰º∞ÔºåË°®ÊòéHoPEÂßãÁªà‰ºò‰∫éÁé∞ÊúâÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ≥ï„ÄÇËøô‰∫õÂèëÁé∞Âº∫Ë∞É‰∫ÜHoPEÂú®Ë°®Á§∫ÂíåÊé®ÂπøÈïøÁ®ã‰æùËµñÂÖ≥Á≥ªÊñπÈù¢ÁöÑÂ¢ûÂº∫ËÉΩÂäõ„ÄÇÊï∞ÊçÆÂíå‰ª£Á†ÅÂ∞Ü‰ºöÂÖ¨ÂºÄ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâTransformerÊ®°Âûã‰∏≠ÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ≥ïÂú®Â§ÑÁêÜË∂ÖÈïøÂ∫èÂàóÊó∂Èù¢‰∏¥ÊåëÊàò„ÄÇÁªùÂØπ‰ΩçÁΩÆÁºñÁ†ÅÈöæ‰ª•Ê≥õÂåñÂà∞ËÆ≠ÁªÉÈïøÂ∫¶‰πãÂ§ñÁöÑÂ∫èÂàóÔºåËÄåRoPEËôΩÁÑ∂Áõ∏ÂØπ‰ΩçÁΩÆÁºñÁ†ÅÔºå‰ΩÜÂÖ∂ÊóãËΩ¨ÁâπÊÄßÂØºËá¥Ê≥®ÊÑèÂäõÊùÉÈáçÂá∫Áé∞ÊåØËç°Ôºå‰∏çÂà©‰∫éÁ®≥ÂÆöÂú∞Âª∫Ê®°ÈïøË∑ùÁ¶ª‰æùËµñÂÖ≥Á≥ª„ÄÇËøôÁßçÊåØËç°‰ºöÂπ≤Êâ∞Ê®°ÂûãÂ≠¶‰π†token‰πãÈó¥ÁöÑÁúüÂÆûÂÖ≥Á≥ªÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÊäëÂà∂ËøúÂ§ÑtokenÂΩ±ÂìçÁöÑÂú∫ÊôØ‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöHoPEÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂÄüÈâ¥ÂèåÊõ≤Âá†‰Ωï‰∏≠ÁöÑÊ¥õ‰º¶ÂÖπÂèòÊç¢ÔºåÂ∞Ü‰ΩçÁΩÆÁºñÁ†ÅËßÜ‰∏∫Âú®ÂèåÊõ≤Á©∫Èó¥‰∏≠ÁöÑÊóãËΩ¨„ÄÇÈÄöËøá‰ΩøÁî®ÂèåÊõ≤ÂáΩÊï∞Êù•ÂÆö‰πâÊóãËΩ¨Êìç‰ΩúÔºåÂèØ‰ª•Á°Æ‰øùÊ≥®ÊÑèÂäõÊùÉÈáçÈöèÁùÄtokenË∑ùÁ¶ªÁöÑÂ¢ûÂä†ËÄåÂçïË∞ÉË°∞Âáè„ÄÇËøôÁßçÂçïË∞ÉË°∞ÂáèËÉΩÂ§üÊõ¥Ëá™ÁÑ∂Âú∞ÂèçÊò†ÊñáÊú¨‰∏≠Ë∑ùÁ¶ªË∂äËøúÁöÑtokenÁõ∏ÂÖ≥ÊÄßË∂ä‰ΩéÁöÑËßÑÂæãÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÂØπÈïøÁ®ã‰æùËµñÂÖ≥Á≥ªÁöÑÂª∫Ê®°ËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHoPEÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Áé∞ÊúâÁöÑTransformerÊû∂ÊûÑ‰∏≠ÔºåÊõøÊç¢ÂéüÊúâÁöÑRoPE‰ΩçÁΩÆÁºñÁ†Å„ÄÇÂÖ∂‰∏ªË¶ÅÊµÅÁ®ãÂåÖÊã¨Ôºö1) Â∞ÜtokenË°®Á§∫Êò†Â∞ÑÂà∞ÂèåÊõ≤Á©∫Èó¥Ôºõ2) ‰ΩøÁî®ÂèåÊõ≤ÂáΩÊï∞ËÆ°ÁÆóÊ¥õ‰º¶ÂÖπÊóãËΩ¨Áü©ÈòµÔºåËØ•Áü©Èòµ‰æùËµñ‰∫étokenÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØÔºõ3) Â∞ÜtokenË°®Á§∫‰∏éÊóãËΩ¨Áü©ÈòµÁõ∏‰πòÔºåÂæóÂà∞‰ΩçÁΩÆÁºñÁ†ÅÂêéÁöÑtokenË°®Á§∫Ôºõ4) Â∞Ü‰ΩçÁΩÆÁºñÁ†ÅÂêéÁöÑtokenË°®Á§∫ËæìÂÖ•Âà∞TransformerÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏≠„ÄÇÊï¥‰∏™ËøáÁ®ã‰∏çÈúÄË¶Å‰øÆÊîπTransformerÁöÑÂÖ∂‰ªñÈÉ®ÂàÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöHoPEÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞Ü‰ΩçÁΩÆÁºñÁ†ÅÈóÆÈ¢ò‰∏éÂèåÊõ≤Âá†‰ΩïËÅîÁ≥ªËµ∑Êù•ÔºåÂπ∂Âà©Áî®Ê¥õ‰º¶ÂÖπÂèòÊç¢Êù•ËÆæËÆ°‰ΩçÁΩÆÁºñÁ†Å„ÄÇ‰∏éRoPEÁõ∏ÊØîÔºåHoPEÈÄöËøáÂèåÊõ≤ÂáΩÊï∞ÁöÑÁâπÊÄßÔºåÂº∫Âà∂Ê≥®ÊÑèÂäõÊùÉÈáçÂçïË∞ÉË°∞ÂáèÔºåÈÅøÂÖç‰∫ÜRoPEÁöÑÊåØËç°ÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòËØÅÊòéRoPEÊòØHoPEÁöÑ‰∏ÄÁßçÁâπÊÆäÊÉÖÂÜµÔºåË°®ÊòéHoPEÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöHoPEÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®ÂèåÊõ≤Ê≠£Âº¶ÂíåÂèåÊõ≤‰ΩôÂº¶ÂáΩÊï∞Êù•ÂÆö‰πâÊóãËΩ¨Áü©ÈòµÔºåÁ°Æ‰øùÂçïË∞ÉË°∞ÂáèÁâπÊÄßÔºõ2) ÂºïÂÖ•ÂèØÂ≠¶‰π†ÁöÑÁº©ÊîæÂõ†Â≠êÊù•ÊéßÂà∂Ë°∞ÂáèÈÄüÂ∫¶ÔºåÂÖÅËÆ∏Ê®°ÂûãÊ†πÊçÆ‰∏çÂêåÁöÑ‰ªªÂä°Ë∞ÉÊï¥Ë°∞ÂáèÁ≠ñÁï•Ôºõ3) ÈááÁî®‰∏éRoPEÁ±ª‰ººÁöÑÊóãËΩ¨ÊñπÂºèÔºå‰øùËØÅËÆ°ÁÆóÊïàÁéá„ÄÇÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑ‰∏éÊ†áÂáÜTransformer‰øùÊåÅ‰∏ÄËá¥ÔºåÊó†ÈúÄÈ¢ùÂ§ñË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHoPEÂú®Â§ö‰∏™ÈïøÂ∫èÂàóÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éRoPEÂíåÂÖ∂‰ªñ‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∫õ‰ªªÂä°‰∏≠ÔºåHoPEÁöÑÂõ∞ÊÉëÂ∫¶ÊØîRoPEÈôç‰Ωé‰∫Ü10%‰ª•‰∏ä„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåHoPEËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âª∫Ê®°ÈïøÁ®ã‰æùËµñÂÖ≥Á≥ªÔºåÂπ∂ÊèêÈ´òÊ®°ÂûãÂú®ÈïøÂ∫èÂàó‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂÆûÈ™åËøòÈ™åËØÅ‰∫ÜHoPEÁöÑÊ≥õÂåñËÉΩÂäõÔºåË°®ÊòéÂÖ∂Âú®‰∏çÂêåÈïøÂ∫¶ÁöÑÂ∫èÂàó‰∏äÈÉΩËÉΩ‰øùÊåÅËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HoPEÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÂ§ÑÁêÜË∂ÖÈïøÂ∫èÂàóÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÈïøÊñáÊú¨ÊëòË¶Å„ÄÅÊñáÊ°£ÁøªËØë„ÄÅ‰ª£Á†ÅÁîüÊàê„ÄÅÂØπËØùÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊõ¥ÊúâÊïàÂú∞Âª∫Ê®°ÈïøÁ®ã‰æùËµñÂÖ≥Á≥ªÔºåHoPEÂèØ‰ª•ÊèêÈ´òÊ®°ÂûãÂú®Ëøô‰∫õ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÔºåÂπ∂ÊúâÊúõÊé®Âä®Áõ∏ÂÖ≥È¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇÊ≠§Â§ñÔºåHoPEÁöÑËÆæËÆ°ÊÄùÊÉ≥‰πüÂèØ‰ª•Â∫îÁî®‰∫éÂÖ∂‰ªñÂ∫èÂàóÂª∫Ê®°‰ªªÂä°Ôºå‰æãÂ¶ÇÊó∂Èó¥Â∫èÂàóÂàÜÊûê„ÄÅËØ≠Èü≥ËØÜÂà´Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Positional encoding mechanisms enable Transformers to model sequential structure and long-range dependencies in text. While absolute positional encodings struggle with extrapolation to longer sequences due to fixed positional representations, and relative approaches like Alibi exhibit performance degradation on extremely long contexts, the widely-used Rotary Positional Encoding (RoPE) introduces oscillatory attention patterns that hinder stable long-distance dependency modelling. We address these limitations through a geometric reformulation of positional encoding. Drawing inspiration from Lorentz transformations in hyperbolic geometry, we propose Hyperbolic Rotary Positional Encoding (HoPE), which leverages hyperbolic functions to implement Lorentz rotations on token representations. Theoretical analysis demonstrates that RoPE is a special case of our generalized formulation. HoPE fundamentally resolves RoPE's slation issues by enforcing monotonic decay of attention weights with increasing token distances. Extensive experimental results, including perplexity evaluations under several extended sequence benchmarks, show that HoPE consistently exceeds existing positional encoding methods. These findings underscore HoPE's enhanced capacity for representing and generalizing long-range dependencies. Data and code will be available.

