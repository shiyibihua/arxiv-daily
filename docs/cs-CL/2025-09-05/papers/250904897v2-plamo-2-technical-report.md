---
layout: default
title: PLaMo 2 Technical Report
---

# PLaMo 2 Technical Report

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04897" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04897v2</a>
  <a href="https://arxiv.org/pdf/2509.04897.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04897v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04897v2', 'PLaMo 2 Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Preferred Networks, :, Kaizaburo Chubachi, Yasuhiro Fujita, Shinichi Hemmi, Yuta Hirokawa, Kentaro Imajo, Toshiki Kataoka, Goro Kobayashi, Kenichi Maehashi, Calvin Metzger, Hiroaki Mikami, Shogo Murai, Daisuke Nishino, Kento Nozawa, Toru Ogawa, Shintarou Okada, Daisuke Okanohara, Shunta Saito, Shotaro Sano, Shuji Suzuki, Kuniyuki Takahashi, Daisuke Tanaka, Avinash Ummadisingu, Hanqin Wang, Sixue Wang, Tianqi Xu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-09-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PLaMo 2ï¼šé¢å‘æ—¥è¯­çš„æ··åˆæ¶æ„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ€§èƒ½åª²ç¾åƒäº¿çº§æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ—¥è¯­NLP` `æ··åˆæ¶æ„` `ç»“æ„åŒ–å‰ªæ` `åˆæˆæ•°æ®` `æŒ‡ä»¤å¾®è°ƒ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ—¥è¯­å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œè®¡ç®—æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚
2. PLaMo 2é€šè¿‡æ··åˆæ¶æ„ã€åˆæˆæ•°æ®å¢å¼ºã€æƒé‡é‡ç”¨å’Œç»“æ„åŒ–å‰ªæç­‰æŠ€æœ¯ï¼Œæœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPLaMo 2çš„8Bæ¨¡å‹æ€§èƒ½å¯ä¸ä¹‹å‰çš„100Bæ¨¡å‹åª²ç¾ï¼Œå¹¶åœ¨æ—¥è¯­åŸºå‡†æµ‹è¯•ä¸­è¶…è¶ŠåŒç­‰è§„æ¨¡çš„å¼€æºæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æŠ¥å‘Šä»‹ç»äº†PLaMo 2ï¼Œä¸€ç³»åˆ—ä»¥æ—¥è¯­ä¸ºä¸­å¿ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨åŸºäºSambaçš„æ··åˆæ¶æ„ï¼Œé€šè¿‡æŒç»­é¢„è®­ç»ƒè¿‡æ¸¡åˆ°å…¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ”¯æŒ32K tokençš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚è®­ç»ƒåˆ©ç”¨äº†å¹¿æ³›çš„åˆæˆè¯­æ–™åº“æ¥å…‹æœæ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶é€šè¿‡æƒé‡é‡ç”¨å’Œç»“æ„åŒ–å‰ªæå®ç°è®¡ç®—æ•ˆç‡ã€‚è¿™ç§é«˜æ•ˆçš„å‰ªææ–¹æ³•äº§ç”Ÿäº†ä¸€ä¸ª8Bæ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¸æˆ‘ä»¬ä¹‹å‰çš„100Bæ¨¡å‹ç›¸å½“ã€‚åè®­ç»ƒä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„æµç¨‹è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ï¼Œå¹¶ç”±åˆæˆæ—¥è¯­æŒ‡ä»¤æ•°æ®å’Œæ¨¡å‹åˆå¹¶æŠ€æœ¯å¢å¼ºã€‚PLaMo 2æ¨¡å‹ä½¿ç”¨vLLMè¿›è¡Œä¼˜åŒ–æ¨ç†ï¼Œå¹¶ä½¿ç”¨é‡åŒ–æŠ€æœ¯ä»¥æœ€å°çš„ç²¾åº¦æŸå¤±ï¼Œåœ¨æ—¥è¯­åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨æŒ‡ä»¤éµå¾ªã€è¯­è¨€æµç•…æ€§å’Œæ—¥è¯­ç‰¹å®šçŸ¥è¯†æ–¹é¢ä¼˜äºç±»ä¼¼è§„æ¨¡çš„å¼€æºæ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ—¥è¯­å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ—¥è¯­ç‰¹å®šçŸ¥è¯†å’Œè¯­è¨€æµç•…æ€§æ–¹é¢è¡¨ç°ä¸è¶³ã€‚åŒæ—¶ï¼Œè®­ç»ƒå’Œéƒ¨ç½²å¤§å‹æ¨¡å‹éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPLaMo 2çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åˆæˆæ•°æ®å¢å¼ºæ¥å¼¥è¡¥æ•°æ®ç¨€ç¼ºï¼Œå¹¶é‡‡ç”¨æ··åˆæ¶æ„å’Œé«˜æ•ˆå‰ªææŠ€æœ¯æ¥é™ä½è®¡ç®—æˆæœ¬ã€‚é€šè¿‡æŒç»­é¢„è®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”é•¿ä¸Šä¸‹æ–‡ï¼Œæå‡æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPLaMo 2çš„è®­ç»ƒæµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) åŸºäºSambaçš„æ··åˆæ¶æ„é¢„è®­ç»ƒï¼Œé€æ­¥è¿‡æ¸¡åˆ°å…¨æ³¨æ„åŠ›æœºåˆ¶ï¼›2) åˆ©ç”¨åˆæˆæ—¥è¯­è¯­æ–™è¿›è¡Œæ•°æ®å¢å¼ºï¼›3) é€šè¿‡æƒé‡é‡ç”¨å’Œç»“æ„åŒ–å‰ªæé™ä½æ¨¡å‹è§„æ¨¡ï¼›4) ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œåè®­ç»ƒï¼Œæå‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šPLaMo 2çš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ··åˆæ¶æ„å’Œé«˜æ•ˆå‰ªææ–¹æ³•ã€‚æ··åˆæ¶æ„å…è®¸æ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œè€Œç»“æ„åŒ–å‰ªæåˆ™å¯ä»¥åœ¨ä¸æ˜¾è‘—é™ä½æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œå¤§å¹…å‡å°‘æ¨¡å‹å‚æ•°é‡ã€‚æ­¤å¤–ï¼Œåˆæˆæ•°æ®çš„æœ‰æ•ˆåˆ©ç”¨ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦åˆ›æ–°ã€‚

**å…³é”®è®¾è®¡**ï¼šPLaMo 2é‡‡ç”¨äº†32K tokençš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¹¶é€šè¿‡æŒç»­é¢„è®­ç»ƒæ¥æ”¯æŒé•¿ä¸Šä¸‹æ–‡ã€‚åœ¨å‰ªææ–¹é¢ï¼Œé‡‡ç”¨äº†ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œä¿ç•™äº†æ¨¡å‹çš„é‡è¦è¿æ¥ã€‚åè®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨äº†åˆæˆæ—¥è¯­æŒ‡ä»¤æ•°æ®å’Œæ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PLaMo 2çš„8Bæ¨¡å‹åœ¨æ—¥è¯­åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå…¶æ€§èƒ½ä¸ä¹‹å‰çš„100Bæ¨¡å‹ç›¸å½“ã€‚åœ¨æŒ‡ä»¤éµå¾ªã€è¯­è¨€æµç•…æ€§å’Œæ—¥è¯­ç‰¹å®šçŸ¥è¯†æ–¹é¢ï¼ŒPLaMo 2ä¼˜äºåŒç­‰è§„æ¨¡çš„å¼€æºæ¨¡å‹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ··åˆæ¶æ„ã€åˆæˆæ•°æ®å¢å¼ºå’Œé«˜æ•ˆå‰ªæç­‰æŠ€æœ¯ï¼Œå¯ä»¥åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PLaMo 2å¯å¹¿æ³›åº”ç”¨äºæ—¥è¯­è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚å…¶é«˜æ•ˆçš„æ€§èƒ½å’Œè¾ƒå°çš„æ¨¡å‹è§„æ¨¡ä½¿å…¶æ›´æ˜“äºéƒ¨ç½²åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—å¹³å°ã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨æ—¥è¯­è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å‘å±•ï¼Œå¹¶ä¿ƒè¿›ç›¸å…³åº”ç”¨çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this report, we introduce PLaMo 2, a series of Japanese-focused large language models featuring a hybrid Samba-based architecture that transitions to full attention via continual pre-training to support 32K token contexts. Training leverages extensive synthetic corpora to overcome data scarcity, while computational efficiency is achieved through weight reuse and structured pruning. This efficient pruning methodology produces an 8B model that achieves performance comparable to our previous 100B model. Post-training further refines the models using a pipeline of supervised fine-tuning (SFT) and direct preference optimization (DPO), enhanced by synthetic Japanese instruction data and model merging techniques. Optimized for inference using vLLM and quantization with minimal accuracy loss, the PLaMo 2 models achieve state-of-the-art results on Japanese benchmarks, outperforming similarly-sized open models in instruction-following, language fluency, and Japanese-specific knowledge.

