---
layout: default
title: Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation
---

# Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04810" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04810v1</a>
  <a href="https://arxiv.org/pdf/2509.04810.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04810v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04810v1', 'Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yogev Cohen, Dudi Ohayon, Romy Somkin, Yehudit Aperstein, Alexander Apartsin

**åˆ†ç±»**: cs.SE, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

**å¤‡æ³¨**: 4 pages, 1 figure

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨LLMç”Ÿæˆåˆæˆæ•°æ®ï¼Œè§£å†³æ–°å…´è¯­è¨€ä»£ç å®¡æŸ¥æ¨èç³»ç»Ÿè®­ç»ƒæ•°æ®ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç å®¡æŸ¥` `ä»£ç æ¨è` `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆæˆæ•°æ®` `ä½èµ„æºå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä»£ç å®¡æŸ¥æ¨èç³»ç»Ÿåœ¨æ–°å…´è¯­è¨€ä¸­é¢ä¸´æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œé˜»ç¢äº†å…¶åº”ç”¨ã€‚
2. åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†èµ„æºä¸°å¯Œçš„è¯­è¨€çš„ä»£ç å˜æ›´ç¿»è¯‘ä¸ºæ–°å…´è¯­è¨€çš„ç­‰æ•ˆå˜æ›´ï¼Œç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨LLMç”Ÿæˆçš„åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ–°å…´è¯­è¨€çš„ä»£ç å®¡æŸ¥æ¨èæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç°ä»£è½¯ä»¶å¼€å‘æµç¨‹ä¸­ï¼Œè‡ªåŠ¨åˆ¤æ–­ä»£ç å˜æ›´æ˜¯å¦éœ€è¦äººå·¥å®¡æŸ¥è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ–°å…´ç¼–ç¨‹è¯­è¨€å’Œæ¡†æ¶çš„å‡ºç°å¸¦æ¥äº†ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼šè™½ç„¶å¤§é‡æœªæ ‡è®°çš„ä»£ç å”¾æ‰‹å¯å¾—ï¼Œä½†ç”¨äºè®­ç»ƒæœ‰ç›‘ç£çš„å®¡æŸ¥åˆ†ç±»æ¨¡å‹çš„å·²æ ‡è®°æ•°æ®å´ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†æ¥è‡ªèµ„æºä¸°å¯Œçš„è¯­è¨€çš„ä»£ç å˜æ›´ç¿»è¯‘æˆä»£è¡¨æ€§ä¸è¶³æˆ–æ–°å…´è¯­è¨€ä¸­çš„ç­‰æ•ˆå˜æ›´ï¼Œä»è€Œåœ¨æ ‡è®°ç¤ºä¾‹ç¨€ç¼ºçš„æƒ…å†µä¸‹ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬å‡è®¾ï¼Œè™½ç„¶LLMå·²ç»ä»å¯ç”¨çš„æœªæ ‡è®°ä»£ç ä¸­å­¦ä¹ äº†æ–°è¯­è¨€çš„è¯­æ³•å’Œè¯­ä¹‰ï¼Œä½†å®ƒä»¬å°šæœªå®Œå…¨æŒæ¡å“ªäº›ä»£ç å˜æ›´åœ¨æ–°å…´ç”Ÿæ€ç³»ç»Ÿä¸­è¢«è®¤ä¸ºæ˜¯é‡è¦çš„æˆ–å€¼å¾—å®¡æŸ¥çš„ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨LLMç”Ÿæˆåˆæˆå˜æ›´ç¤ºä¾‹ï¼Œå¹¶ç”¨å®ƒä»¬è®­ç»ƒæœ‰ç›‘ç£çš„åˆ†ç±»å™¨ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å°†è¿™äº›åˆ†ç±»å™¨çš„æ€§èƒ½ä¸åœ¨çœŸå®æ ‡è®°æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨å¤šä¸ªGitHubå­˜å‚¨åº“å’Œè¯­è¨€å¯¹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLLMç”Ÿæˆçš„åˆæˆæ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼å®¡æŸ¥æ¨èç³»ç»Ÿï¼Œå³ä½¿åœ¨ä½èµ„æºç¯å¢ƒä¸­ä¹Ÿèƒ½ç¼©å°æ€§èƒ½å·®è·ã€‚è¿™ç§æ–¹æ³•ä¸ºå°†è‡ªåŠ¨ä»£ç å®¡æŸ¥èƒ½åŠ›æ‰©å±•åˆ°å¿«é€Ÿå‘å±•çš„æŠ€æœ¯å †æ ˆæä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„é€”å¾„ï¼Œå³ä½¿åœ¨æ²¡æœ‰æ³¨é‡Šæ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ–°å…´ç¼–ç¨‹è¯­è¨€å’Œæ¡†æ¶ä¸­ï¼Œç”±äºç¼ºä¹è¶³å¤Ÿçš„æ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆè®­ç»ƒä»£ç å®¡æŸ¥æ¨èç³»ç»Ÿçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä½†åœ¨æ–°å…´è¯­è¨€ä¸­ï¼Œè·å–è¿™äº›æ•°æ®æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œé™åˆ¶äº†è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥çš„æ¨å¹¿åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è·¨è¯­è¨€ä»£ç ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå°†å·²æœ‰çš„ã€èµ„æºä¸°å¯Œçš„è¯­è¨€çš„ä»£ç å˜æ›´ç¿»è¯‘æˆæ–°å…´è¯­è¨€çš„ç­‰æ•ˆå˜æ›´ï¼Œä»è€Œç”Ÿæˆåˆæˆçš„è®­ç»ƒæ•°æ®ã€‚è¿™ç§æ–¹æ³•é¿å…äº†äººå·¥æ ‡æ³¨çš„æˆæœ¬ï¼Œå¹¶èƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆå¤§é‡è®­ç»ƒæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) é€‰æ‹©èµ„æºä¸°å¯Œçš„æºè¯­è¨€å’Œèµ„æºåŒ®ä¹çš„ç›®æ ‡è¯­è¨€ï¼›2) ä»æºè¯­è¨€çš„ä»£ç ä»“åº“ä¸­æå–ä»£ç å˜æ›´ï¼›3) ä½¿ç”¨LLMå°†æºè¯­è¨€çš„ä»£ç å˜æ›´ç¿»è¯‘æˆç›®æ ‡è¯­è¨€çš„ç­‰æ•ˆå˜æ›´ï¼Œç”Ÿæˆåˆæˆæ•°æ®ï¼›4) ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒä»£ç å®¡æŸ¥åˆ†ç±»å™¨ï¼›5) åœ¨çœŸå®çš„ç›®æ ‡è¯­è¨€æ•°æ®é›†ä¸Šè¯„ä¼°åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨LLMè¿›è¡Œè·¨è¯­è¨€çš„ä»£ç å˜æ›´ç¿»è¯‘ï¼Œä»è€Œç”Ÿæˆç”¨äºè®­ç»ƒä»£ç å®¡æŸ¥æ¨èç³»ç»Ÿçš„åˆæˆæ•°æ®ã€‚ä¸ä»¥å¾€ä¾èµ–äººå·¥æ ‡æ³¨æˆ–ç®€å•æ•°æ®å¢å¼ºçš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨LLMçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œç”Ÿæˆæ›´å…·ä»£è¡¨æ€§çš„è®­ç»ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼ŒLLMçš„é€‰æ‹©å’Œä½¿ç”¨æ˜¯å…³é”®ã€‚å…·ä½“æ¥è¯´ï¼Œéœ€è¦é€‰æ‹©å…·æœ‰è¾ƒå¼ºè·¨è¯­è¨€ä»£ç ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„LLMï¼Œä¾‹å¦‚åŸºäºTransformerçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¦‚ä½•è®¾è®¡åˆé€‚çš„promptï¼Œå¼•å¯¼LLMç”Ÿæˆé«˜è´¨é‡çš„åˆæˆæ•°æ®ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„æŠ€æœ¯ç»†èŠ‚ã€‚è®ºæ–‡å¯èƒ½è¿˜æ¶‰åŠäº†å¯¹åˆæˆæ•°æ®è¿›è¡Œè¿‡æ»¤å’Œæ¸…æ´—ï¼Œä»¥æé«˜è®­ç»ƒæ•°æ®çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LLMç”Ÿæˆçš„åˆæˆæ•°æ®è®­ç»ƒçš„ä»£ç å®¡æŸ¥åˆ†ç±»å™¨ï¼Œåœ¨å¤šä¸ªGitHubä»“åº“å’Œè¯­è¨€å¯¹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å³ä½¿åœ¨ä½èµ„æºç¯å¢ƒä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿèƒ½æœ‰æ•ˆç¼©å°ä¸ä½¿ç”¨çœŸå®æ ‡æ³¨æ•°æ®è®­ç»ƒçš„æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œè¯æ˜äº†LLMç”Ÿæˆåˆæˆæ•°æ®åœ¨ä»£ç å®¡æŸ¥æ¨èç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡æ­£æ–‡ä¸­ç»™å‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æ–°å…´ç¼–ç¨‹è¯­è¨€å’Œæ¡†æ¶çš„ä»£ç å®¡æŸ¥è‡ªåŠ¨åŒ–ï¼Œæé«˜è½¯ä»¶å¼€å‘æ•ˆç‡å’Œè´¨é‡ã€‚é€šè¿‡é™ä½å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå¯ä»¥åŠ é€Ÿè‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥æŠ€æœ¯åœ¨æ–°å…´æŠ€æœ¯æ ˆä¸­çš„æ™®åŠï¼Œå¹¶æ”¯æŒæ›´å¿«é€Ÿçš„è½¯ä»¶è¿­ä»£å’Œåˆ›æ–°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•åˆ°å…¶ä»–ä½èµ„æºåœºæ™¯ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä½èµ„æºè¯­è¨€ç¿»è¯‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automating the decision of whether a code change requires manual review is vital for maintaining software quality in modern development workflows. However, the emergence of new programming languages and frameworks creates a critical bottleneck: while large volumes of unlabelled code are readily available, there is an insufficient amount of labelled data to train supervised models for review classification. We address this challenge by leveraging Large Language Models (LLMs) to translate code changes from well-resourced languages into equivalent changes in underrepresented or emerging languages, generating synthetic training data where labelled examples are scarce. We assume that although LLMs have learned the syntax and semantics of new languages from available unlabelled code, they have yet to fully grasp which code changes are considered significant or review-worthy within these emerging ecosystems. To overcome this, we use LLMs to generate synthetic change examples and train supervised classifiers on them. We systematically compare the performance of these classifiers against models trained on real labelled data. Our experiments across multiple GitHub repositories and language pairs demonstrate that LLM-generated synthetic data can effectively bootstrap review recommendation systems, narrowing the performance gap even in low-resource settings. This approach provides a scalable pathway to extend automated code review capabilities to rapidly evolving technology stacks, even in the absence of annotated data.

