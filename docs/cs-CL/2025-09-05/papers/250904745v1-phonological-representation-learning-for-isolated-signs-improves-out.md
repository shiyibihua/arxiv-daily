---
layout: default
title: Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization
---

# Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04745" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04745v1</a>
  <a href="https://arxiv.org/pdf/2509.04745.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04745v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04745v1', 'Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lee Kezar, Zed Sehyr, Jesse Thomason

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºéŸ³ä½è¡¨å¾å­¦ä¹ çš„å­¤ç«‹æ‰‹è¯­è¯†åˆ«æ¨¡å‹ï¼Œæå‡æœªè§æ‰‹è¯­çš„æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ‰‹è¯­è¯†åˆ«` `éŸ³ä½è¡¨å¾å­¦ä¹ ` `å‘é‡é‡åŒ–` `è‡ªç¼–ç å™¨` `æ³›åŒ–èƒ½åŠ›` `å‚æ•°è§£è€¦` `éŸ³ä½åŠç›‘ç£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰‹è¯­è¯†åˆ«æ¨¡å‹é¢ä¸´è¯æ±‡é‡ä¸è¶³çš„æŒ‘æˆ˜ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„æ‰‹è¯­ã€‚
2. è®ºæ–‡æå‡ºç»“åˆå‚æ•°è§£è€¦å’ŒéŸ³ä½åŠç›‘ç£çš„éŸ³ä½å½’çº³åç½®æ–¹æ³•ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æœªè§æ‰‹è¯­çš„å•æ ·æœ¬é‡å»ºå’Œæ‰‹è¯­è¯†åˆ«æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰‹è¯­æ•°æ®é›†é€šå¸¸åœ¨è¯æ±‡æ–¹é¢ä¸å…·æœ‰ä»£è¡¨æ€§ï¼Œè¿™çªæ˜¾äº†æ¨¡å‹æ³›åŒ–åˆ°æœªè§æ‰‹è¯­çš„éœ€æ±‚ã€‚å‘é‡é‡åŒ–æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„ç¦»æ•£ã€ç±»tokenè¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œä½†å°šæœªè¯„ä¼°å­¦ä¹ åˆ°çš„å•å…ƒæ˜¯å¦æ•è·äº†é˜»ç¢è¯æ±‡å¤–æ€§èƒ½çš„è™šå‡ç›¸å…³æ€§ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†ä¸¤ç§éŸ³ä½å½’çº³åç½®ï¼šå‚æ•°è§£è€¦ï¼ˆä¸€ç§æ¶æ„åç½®ï¼‰å’ŒéŸ³ä½åŠç›‘ç£ï¼ˆä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼‰ï¼Œä»¥æ”¹è¿›å·²çŸ¥æ‰‹è¯­çš„å­¤ç«‹æ‰‹è¯­è¯†åˆ«å’Œæœªè§æ‰‹è¯­çš„é‡å»ºè´¨é‡ï¼Œæ¨¡å‹åŸºäºå‘é‡é‡åŒ–è‡ªç¼–ç å™¨ã€‚ä¸»è¦å‘ç°æ˜¯ï¼Œä¸å—æ§åŸºçº¿ç›¸æ¯”ï¼Œæ‰€æå‡ºæ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨å¾å¯¹äºæœªè§æ‰‹è¯­çš„å•æ ·æœ¬é‡å»ºæ›´æœ‰æ•ˆï¼Œå¹¶ä¸”å¯¹äºæ‰‹è¯­è¯†åˆ«æ›´å…·åŒºåˆ†æ€§ã€‚è¿™é¡¹å·¥ä½œå¯¹æ˜¾å¼çš„ã€è¯­è¨€å­¦åŠ¨æœºçš„åç½®å¦‚ä½•æé«˜æ‰‹è¯­å­¦ä¹ è¡¨å¾çš„æ³›åŒ–èƒ½åŠ›è¿›è¡Œäº†å®šé‡åˆ†æã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ‰‹è¯­è¯†åˆ«æ¨¡å‹åœ¨é¢å¯¹è¯æ±‡é‡æœ‰é™çš„æ•°æ®é›†æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œéš¾ä»¥è¯†åˆ«æœªè§è¿‡çš„æ‰‹è¯­ã€‚ç°æœ‰æ–¹æ³•å¯èƒ½å­¦ä¹ åˆ°æ•°æ®é›†ä¸­å­˜åœ¨çš„è™šå‡ç›¸å…³æ€§ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦æ‹Ÿåˆå·²çŸ¥æ‰‹è¯­ï¼Œè€Œæ— æ³•æœ‰æ•ˆå¤„ç†æ–°å‡ºç°çš„æ‰‹è¯­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥éŸ³ä½å½’çº³åç½®ï¼Œé€šè¿‡å‚æ•°è§£è€¦å’ŒéŸ³ä½åŠç›‘ç£çš„æ–¹å¼ï¼Œä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´å…·æ³›åŒ–èƒ½åŠ›çš„éŸ³ä½è¡¨å¾ã€‚è¿™ç§è¡¨å¾èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ‰‹è¯­çš„æœ¬è´¨ç‰¹å¾ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨æœªè§æ‰‹è¯­ä¸Šçš„è¯†åˆ«å’Œé‡å»ºèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹åŸºäºå‘é‡é‡åŒ–è‡ªç¼–ç å™¨ï¼ˆVQ-VAEï¼‰ã€‚é¦–å…ˆï¼Œè¾“å…¥æ‰‹è¯­è§†é¢‘ç»è¿‡ç¼–ç å™¨å¾—åˆ°è¿ç»­çš„ç‰¹å¾å‘é‡ã€‚ç„¶åï¼Œä½¿ç”¨å‘é‡é‡åŒ–å±‚å°†è¿ç»­ç‰¹å¾å‘é‡æ˜ å°„åˆ°ç¦»æ•£çš„ç æœ¬ç´¢å¼•ã€‚è§£ç å™¨åˆ™æ ¹æ®è¿™äº›ç¦»æ•£ç´¢å¼•é‡å»ºæ‰‹è¯­è§†é¢‘ã€‚ä¸ºäº†å¼•å…¥éŸ³ä½å½’çº³åç½®ï¼Œæ¨¡å‹é‡‡ç”¨äº†å‚æ•°è§£è€¦å’ŒéŸ³ä½åŠç›‘ç£ä¸¤ç§æŠ€æœ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†éŸ³ä½å­¦çš„çŸ¥è¯†èå…¥åˆ°æ‰‹è¯­è¯†åˆ«æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ä¸­ã€‚é€šè¿‡å‚æ•°è§£è€¦ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°ç‹¬ç«‹çš„éŸ³ä½ç‰¹å¾è¡¨ç¤ºã€‚éŸ³ä½åŠç›‘ç£åˆ™åˆ©ç”¨éŸ³ä½ä¿¡æ¯ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´ç¬¦åˆè¯­è¨€å­¦è§„å¾‹çš„è¡¨å¾ã€‚è¿™ç§ç»“åˆè¯­è¨€å­¦çŸ¥è¯†çš„è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå‚æ•°è§£è€¦çš„å…·ä½“å®ç°æ–¹å¼æ˜¯ï¼Œå°†ç¼–ç å™¨å’Œè§£ç å™¨çš„éƒ¨åˆ†å‚æ•°è¿›è¡Œåˆ†ç»„ï¼Œæ¯ç»„å‚æ•°è´Ÿè´£å­¦ä¹ ä¸€ç§ç‰¹å®šçš„éŸ³ä½ç‰¹å¾ã€‚éŸ³ä½åŠç›‘ç£åˆ™é€šè¿‡ä¸€ä¸ªè¾…åŠ©çš„éŸ³ä½åˆ†ç±»å™¨æ¥å®ç°ã€‚è¯¥åˆ†ç±»å™¨ä»¥ç¼–ç å™¨çš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹æ‰‹è¯­è§†é¢‘å¯¹åº”çš„éŸ³ä½åºåˆ—ã€‚éŸ³ä½åˆ†ç±»å™¨çš„æŸå¤±å‡½æ•°è¢«æ·»åŠ åˆ°æ€»æŸå¤±å‡½æ•°ä¸­ï¼Œä½œä¸ºæ­£åˆ™åŒ–é¡¹ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±ã€é‡åŒ–æŸå¤±å’ŒéŸ³ä½åˆ†ç±»æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æœªè§æ‰‹è¯­çš„å•æ ·æœ¬é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œé‡å»ºè´¨é‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåœ¨æ‰‹è¯­è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºæ›´å¼ºçš„åŒºåˆ†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«ä¸åŒçš„æ‰‹è¯­ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®åœ¨è®ºæ–‡ä¸­ç»™å‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ‰‹è¯­ç¿»è¯‘ã€æ‰‹è¯­æ•™å­¦ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜æ‰‹è¯­è¯†åˆ«æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œå¤„ç†å„ç§æ‰‹è¯­è¡¨è¾¾ï¼Œä¿ƒè¿›è‹å“‘äººä¸å¥å¬äººä¹‹é—´çš„äº¤æµã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ™ºèƒ½åŠ©å¬è®¾å¤‡ã€æ‰‹è¯­è¾“å…¥æ³•ç­‰äº§å“ï¼Œä¸ºè‹å“‘äººæä¾›æ›´ä¾¿æ·çš„ç”Ÿæ´»æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sign language datasets are often not representative in terms of vocabulary, underscoring the need for models that generalize to unseen signs. Vector quantization is a promising approach for learning discrete, token-like representations, but it has not been evaluated whether the learned units capture spurious correlations that hinder out-of-vocabulary performance. This work investigates two phonological inductive biases: Parameter Disentanglement, an architectural bias, and Phonological Semi-Supervision, a regularization technique, to improve isolated sign recognition of known signs and reconstruction quality of unseen signs with a vector-quantized autoencoder. The primary finding is that the learned representations from the proposed model are more effective for one-shot reconstruction of unseen signs and more discriminative for sign identification compared to a controlled baseline. This work provides a quantitative analysis of how explicit, linguistically-motivated biases can improve the generalization of learned representations of sign language.

