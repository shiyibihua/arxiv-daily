---
layout: default
title: A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning
---

# A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04753" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.04753v1</a>
  <a href="https://arxiv.org/pdf/2509.04753.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04753v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04753v1', 'A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Cheng Peng, Xinyu Dong, Mengxian Lyu, Daniel Paredes, Yaoyun Zhang, Yonghui Wu

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-05

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Á†îÁ©∂Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÊÇ£ËÄÖ‰ø°ÊÅØÊäΩÂèñ‰∏≠ÁöÑÂ∫îÁî®ÔºåÊé¢Á¥¢Ê®°ÂûãÊû∂ÊûÑ„ÄÅÂæÆË∞ÉÁ≠ñÁï•ÂíåÂ§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÊÇ£ËÄÖ‰ø°ÊÅØÊäΩÂèñ` `ÂèÇÊï∞È´òÊïàÂæÆË∞É` `Â§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºò` `‰∏¥Â∫äËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰∏¥Â∫äÂèôËø∞‰∏≠ÊäΩÂèñÊÇ£ËÄÖ‰ø°ÊÅØËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÊÄßÂíåÊ≥õÂåñÊÄßÊñπÈù¢Â≠òÂú®ÊåëÊàò„ÄÇ
2. Êú¨Á†îÁ©∂Êé¢Á¥¢‰∏çÂêåLLMÊû∂ÊûÑ„ÄÅÂæÆË∞ÉÁ≠ñÁï•ÂíåÂ§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºòÔºå‰ª•ÊèêÂçáÊÇ£ËÄÖ‰ø°ÊÅØÊäΩÂèñÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ
3. ÈÄöËøáÂü∫ÂáÜÊµãËØïÂíåÂØπÊØîÂÆûÈ™åÔºåËØÑ‰º∞‰∫Ü‰∏çÂêåLLMÂíåÂæÆË∞ÉÊñπÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞ÔºåÂπ∂ÂàÜÊûê‰∫ÜÂÖ∂‰ºòÁº∫ÁÇπ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÊÇ£ËÄÖ‰ø°ÊÅØÊäΩÂèñ‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÈáçÁÇπÂÖ≥Ê≥®LLMÊû∂ÊûÑ„ÄÅÂæÆË∞ÉÁ≠ñÁï•ÂíåÂ§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºòÊäÄÊúØÔºåÊó®Âú®ÂºÄÂèëÁ®≥ÂÅ•‰∏îÂÖ∑ÊúâÊ≥õÂåñËÉΩÂäõÁöÑÊÇ£ËÄÖ‰ø°ÊÅØÊäΩÂèñÁ≥ªÁªü„ÄÇÁ†îÁ©∂Êé¢Á¥¢‰∫Ü‰ΩøÁî®LLMËøõË°å‰∏¥Â∫äÊ¶ÇÂøµÂíåÂÖ≥Á≥ªÊäΩÂèñÁöÑÂÖ≥ÈîÆÊ¶ÇÂøµÔºåÂåÖÊã¨ÔºöÔºà1Ôºâencoder-onlyÊàñdecoder-only LLMÔºõÔºà2ÔºâÂü∫‰∫épromptÁöÑÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºàPEFTÔºâÁÆóÊ≥ïÔºõÔºà3ÔºâÂ§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºòÂØπÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÂØπ‰∏ÄÁ≥ªÂàóLLMËøõË°å‰∫ÜÂü∫ÂáÜÊµãËØïÔºåÂåÖÊã¨Âü∫‰∫éencoderÁöÑLLMÔºàBERT„ÄÅGatorTronÔºâÂíåÂü∫‰∫édecoderÁöÑLLMÔºàGatorTronGPT„ÄÅLlama 3.1„ÄÅGatorTronLlamaÔºâÔºå‰ΩøÁî®‰∫Ü‰∫î‰∏™Êï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨ÊØîËæÉ‰∫Ü‰º†ÁªüÁöÑÂÖ®Â∞∫ÂØ∏ÂæÆË∞ÉÂíåÂü∫‰∫épromptÁöÑPEFT„ÄÇÊàë‰ª¨Êé¢Á¥¢‰∫Ü‰∏Ä‰∏™Â§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºòÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÂõõ‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑ‰ªªÂä°Ôºå‰ª•ËØÑ‰º∞‰ΩøÁî®Áïô‰∏ÄÊï∞ÊçÆÈõÜÁ≠ñÁï•ÁöÑÈõ∂Ê†∑Êú¨ÂíåÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥‰ªé‰∏¥Â∫äÊñáÊú¨‰∏≠ÂáÜÁ°Æ„ÄÅÈ´òÊïàÂú∞ÊäΩÂèñÊÇ£ËÄÖ‰ø°ÊÅØÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØ‰º†ÁªüNLPÊñπÊ≥ïÔºåÂú®Â§ÑÁêÜÂ§çÊùÇÁöÑ‰∏¥Â∫äËØ≠Ë®Ä„ÄÅÊúØËØ≠ÂèòÂºÇÊÄßÂíå‰∏ä‰∏ãÊñá‰æùËµñÊÄßÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄßÔºåÂπ∂‰∏îÈúÄË¶ÅÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆ„ÄÇÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËôΩÁÑ∂ÊΩúÂäõÂ∑®Â§ßÔºå‰ΩÜÂ¶Ç‰ΩïÈíàÂØπÊÇ£ËÄÖ‰ø°ÊÅØÊäΩÂèñ‰ªªÂä°ËøõË°å‰ºòÂåñÂíåÊúâÊïàÂà©Áî®‰ªçÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊé¢Á¥¢‰∏çÂêåÁ±ªÂûãÁöÑLLMÔºàencoder-onlyÂíådecoder-onlyÔºâÔºåÂπ∂ÁªìÂêàÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºàPEFTÔºâÂíåÂ§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºòÔºå‰ª•ÊèêÂçáÊ®°ÂûãÂú®ÊÇ£ËÄÖ‰ø°ÊÅØÊäΩÂèñ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÊØîËæÉ‰∏çÂêåÊ®°ÂûãÊû∂ÊûÑÂíåÂæÆË∞ÉÁ≠ñÁï•ÔºåÊâæÂà∞ÊúÄÈÄÇÂêàËØ•‰ªªÂä°ÁöÑÈÖçÁΩÆ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºö1ÔºâÈÄâÊã©ÂíåÂáÜÂ§áÊï∞ÊçÆÈõÜÔºõ2ÔºâÈÄâÊã©ÂíåÂæÆË∞ÉLLMÔºåÂåÖÊã¨encoder-onlyÊ®°ÂûãÔºàBERT, GatorTronÔºâÂíådecoder-onlyÊ®°ÂûãÔºàGatorTronGPT, Llama 3.1, GatorTronLlamaÔºâÔºåÂπ∂ÈááÁî®ÂÖ®Â∞∫ÂØ∏ÂæÆË∞ÉÂíåPEFT‰∏§ÁßçÁ≠ñÁï•Ôºõ3ÔºâÊûÑÂª∫Â§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºòÊ°ÜÊû∂ÔºåÂ∞ÜÂ§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑ‰ªªÂä°ÁªìÂêàËµ∑Êù•ËøõË°åËÆ≠ÁªÉÔºåÂπ∂‰ΩøÁî®Áïô‰∏ÄÊï∞ÊçÆÈõÜÁ≠ñÁï•ËØÑ‰º∞Èõ∂Ê†∑Êú¨ÂíåÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÁ≥ªÁªüÊÄßÂú∞ÊØîËæÉ‰∫Ü‰∏çÂêåLLMÊû∂ÊûÑ„ÄÅÂæÆË∞ÉÁ≠ñÁï•ÂíåÂ§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºòÊñπÊ≥ïÂú®ÊÇ£ËÄÖ‰ø°ÊÅØÊäΩÂèñ‰ªªÂä°‰∏äÁöÑÊïàÊûú„ÄÇÁâπÂà´ÂÖ≥Ê≥®‰∫ÜÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºàPEFTÔºâÂú®ÂáèÂ∞ëËÆ°ÁÆóËµÑÊ∫êÈúÄÊ±ÇÁöÑÂêåÊó∂Ôºå‰øùÊåÅÁîöËá≥ÊèêÂçáÊ®°ÂûãÊÄßËÉΩÁöÑÊΩúÂäõ„ÄÇÊ≠§Â§ñÔºåÂ§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºòÊ°ÜÊû∂Êó®Âú®ÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ΩøÂÖ∂ËÉΩÂ§üÈÄÇÂ∫î‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÂíå‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂæÆË∞ÉÁ≠ñÁï•ÊñπÈù¢ÔºåËÆ∫ÊñáÊØîËæÉ‰∫ÜÂÖ®Â∞∫ÂØ∏ÂæÆË∞ÉÂíåÂü∫‰∫épromptÁöÑPEFTÊñπÊ≥ïÔºå‰æãÂ¶ÇLoRA„ÄÇÂú®Â§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºòÊñπÈù¢ÔºåËÆ∫ÊñáËÆæËÆ°‰∫ÜÁªü‰∏ÄÁöÑÊåá‰ª§Ê®°ÊùøÔºåÂ∞Ü‰∏çÂêåÊï∞ÊçÆÈõÜ‰∏äÁöÑ‰ªªÂä°ËΩ¨Âåñ‰∏∫Áªü‰∏ÄÁöÑÊåá‰ª§Ê†ºÂºèÔºå‰ª•‰æøÊ®°ÂûãËÉΩÂ§üÂêåÊó∂Â≠¶‰π†Â§ö‰∏™‰ªªÂä°„ÄÇÊçüÂ§±ÂáΩÊï∞ÊñπÈù¢ÔºåÈááÁî®Ê†áÂáÜÁöÑ‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ËøõË°åËÆ≠ÁªÉ„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÔºàÂ¶ÇÂ≠¶‰π†Áéá„ÄÅbatch sizeÁ≠âÔºâÊú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•Á†îÁ©∂ÈÄöËøáÂÆûÈ™åÂØπÊØî‰∫ÜÂ§öÁßçLLMÊû∂ÊûÑÂíåÂæÆË∞ÉÁ≠ñÁï•Âú®ÊÇ£ËÄÖ‰ø°ÊÅØÊäΩÂèñ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇÁªìÊûúË°®ÊòéÔºåÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºàPEFTÔºâÂú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇÂ§ö‰ªªÂä°Êåá‰ª§Ë∞É‰ºòËÉΩÂ§üÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ΩøÂÖ∂Âú®Êú™ËßÅÊï∞ÊçÆÈõÜ‰∏ä‰πüËÉΩÂèñÂæóËæÉÂ•ΩÁöÑË°®Áé∞„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Êú™Áü•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂ§öÁßçÂåªÁñóÂú∫ÊôØÔºå‰æãÂ¶ÇËæÖÂä©‰∏¥Â∫äÂÜ≥Á≠ñ„ÄÅËá™Âä®ÂåñÁóÖÂéÜÂàÜÊûê„ÄÅËçØÁâ©Á†îÂèëÂíåÊÇ£ËÄÖÈ£éÈô©È¢ÑÊµã„ÄÇÈÄöËøáÈ´òÊïàÂáÜÁ°ÆÂú∞ÊäΩÂèñÊÇ£ËÄÖ‰ø°ÊÅØÔºåÂèØ‰ª•ÊèêÂçáÂåªÁñóÊúçÂä°ÁöÑË¥®ÈáèÂíåÊïàÁéáÔºåÂπ∂‰∏∫ÂåªÁñóÁ†îÁ©∂Êèê‰æõÊúâÂäõÊîØÊåÅ„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõ‰∏éÁîµÂ≠êÁóÖÂéÜÁ≥ªÁªüÈõÜÊàêÔºåÂÆûÁé∞Êõ¥Êô∫ËÉΩÂåñÁöÑÂåªÁñó‰ø°ÊÅØÁÆ°ÁêÜ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Natural language processing (NLP) is a key technology to extract important patient information from clinical narratives to support healthcare applications. The rapid development of large language models (LLMs) has revolutionized many NLP tasks in the clinical domain, yet their optimal use in patient information extraction tasks requires further exploration. This study examines LLMs' effectiveness in patient information extraction, focusing on LLM architectures, fine-tuning strategies, and multi-task instruction tuning techniques for developing robust and generalizable patient information extraction systems. This study aims to explore key concepts of using LLMs for clinical concept and relation extraction tasks, including: (1) encoder-only or decoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT) algorithms, and (3) multi-task instruction tuning on few-shot learning performance. We benchmarked a suite of LLMs, including encoder-based LLMs (BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1, GatorTronLlama), across five datasets. We compared traditional full-size fine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning framework that combines both tasks across four datasets to evaluate the zero-shot and few-shot learning performance using the leave-one-dataset-out strategy.

