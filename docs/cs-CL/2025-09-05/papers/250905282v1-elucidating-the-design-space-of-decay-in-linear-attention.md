---
layout: default
title: Elucidating the Design Space of Decay in Linear Attention
---

# Elucidating the Design Space of Decay in Linear Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05282" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05282v1</a>
  <a href="https://arxiv.org/pdf/2509.05282.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05282v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05282v1', 'Elucidating the Design Space of Decay in Linear Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhen Qin, Xuyang Shen, Yiran Zhong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

**å¤‡æ³¨**: Accepted to COLM 2025. Yiran Zhong is the corresponding author. Code is available at https://github.com/Doraemonzzz/xmixers

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿç ”ç©¶çº¿æ€§æ³¨æ„åŠ›è¡°å‡æœºåˆ¶è®¾è®¡ç©ºé—´ï¼Œæ­ç¤ºå…³é”®å› ç´ ä¸RoPEå±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çº¿æ€§æ³¨æ„åŠ›` `è¡°å‡æœºåˆ¶` `è®¾è®¡ç©ºé—´` `å‚æ•°åŒ–ç­–ç•¥` `ä½ç½®ç¼–ç ` `RoPE` `è¯­è¨€å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ä¾èµ–è¡°å‡æœºåˆ¶å¤„ç†åºåˆ—ä¿¡æ¯ï¼Œä½†å…¶è®¾è®¡ç©ºé—´ç¼ºä¹ç³»ç»Ÿç ”ç©¶ã€‚
2. è®ºæ–‡ç³»ç»Ÿæ€§åœ°æ¢ç´¢äº†è¡°å‡æœºåˆ¶çš„å‚æ•°åŒ–ã€å…±äº«ã€ç²’åº¦ä»¥åŠä¸ä½ç½®ç¼–ç çš„å…¼å®¹æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¡°å‡å‚æ•°åŒ–ç­–ç•¥è‡³å…³é‡è¦ï¼Œå‚æ•°å…±äº«éœ€è°¨æ…ï¼ŒRoPEå¯¹çº¿æ€§æ³¨æ„åŠ›å¢ç›Šæœ‰é™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å…¨é¢ç ”ç©¶äº†çº¿æ€§å¤æ‚åº¦åºåˆ—æ¨¡å‹ä¸­å›ºæœ‰çš„è¡°å‡æœºåˆ¶ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ’åˆ†äº†è¡°å‡æœºåˆ¶çš„è®¾è®¡ç©ºé—´ï¼Œæ¶µç›–å››ä¸ªå…³é”®ç»´åº¦ï¼šå‚æ•°åŒ–ç­–ç•¥ï¼ˆè¡°å‡çš„è®¡ç®—æ–¹æ³•ï¼‰ã€å‚æ•°å…±äº«ï¼ˆåˆ©ç”¨è¾…åŠ©å‚æ•°è¿›è¡Œè¡°å‡è®¡ç®—ï¼‰ã€è¡°å‡ç²’åº¦ï¼ˆæ ‡é‡ä¸å‘é‡è¡°å‡çš„æ¯”è¾ƒï¼‰ä»¥åŠä¸ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ³•ï¼ˆå¦‚æ—‹è½¬ä½ç½®åµŒå…¥RoPEï¼‰çš„å…¼å®¹æ€§ã€‚é€šè¿‡åœ¨å„ç§è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€äº›å…³é”®è§è§£ã€‚é¦–å…ˆï¼Œè¡°å‡çš„å‚æ•°åŒ–ç­–ç•¥çš„è®¾è®¡éœ€è¦ä»”ç»†è€ƒè™‘ï¼Œæœ‰æ•ˆçš„é…ç½®é€šå¸¸å±€é™äºç‰¹å®šçš„å‚æ•°èŒƒå›´ã€‚å…¶æ¬¡ï¼Œå‚æ•°å…±äº«ä¸èƒ½éšæ„ä½¿ç”¨ï¼Œå› ä¸ºå®ƒå¯èƒ½å¯¼è‡´è¡°å‡å€¼è¿‡å¤§æˆ–è¿‡å°ï¼Œä»è€Œæ˜¾è‘—å½±å“æ€§èƒ½ã€‚ç¬¬ä¸‰ï¼Œåœ¨ç›¸åŒçš„å‚æ•°åŒ–ç­–ç•¥ä¸‹ï¼Œæ ‡é‡è¡°å‡é€šå¸¸ä¸å¦‚å‘é‡è¡°å‡ã€‚ç„¶è€Œï¼Œåœ¨æŸäº›å…·æœ‰æ›¿ä»£å‚æ•°åŒ–ç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œæ ‡é‡è¡°å‡å¯èƒ½å‡ºä¹æ„æ–™åœ°è¶…è¿‡å‘é‡è¡°å‡çš„æ•ˆåŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒRoPEè¿™ç§å¸¸ç”¨çš„ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ³•ï¼Œé€šå¸¸æ— æ³•ä¸ºå¤§å¤šæ•°çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶æä¾›æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçº¿æ€§æ³¨æ„åŠ›æ¨¡å‹æ—¨åœ¨é™ä½Transformerçš„è®¡ç®—å¤æ‚åº¦ï¼Œä½†å…¶æ€§èƒ½å¾€å¾€å—åˆ°è¡°å‡æœºåˆ¶çš„å½±å“ã€‚ç°æœ‰ç ”ç©¶å¯¹è¡°å‡æœºåˆ¶çš„è®¾è®¡ç¼ºä¹ç³»ç»Ÿæ€§çš„æ¢ç´¢ï¼Œå¯¼è‡´éš¾ä»¥é€‰æ‹©åˆé€‚çš„è¡°å‡ç­–ç•¥ï¼Œä»è€Œé™åˆ¶äº†çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶è¡°å‡æœºåˆ¶çš„è®¾è®¡ç©ºé—´ï¼Œæ­ç¤ºä¸åŒè®¾è®¡é€‰æ‹©å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚é€šè¿‡æ§åˆ¶å˜é‡æ³•ï¼Œåˆ†æå‚æ•°åŒ–ç­–ç•¥ã€å‚æ•°å…±äº«ã€è¡°å‡ç²’åº¦ä»¥åŠä¸ä½ç½®ç¼–ç çš„å…¼å®¹æ€§ç­‰å› ç´ ï¼Œä»è€Œä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡çš„ç ”ç©¶æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) å®šä¹‰è¡°å‡æœºåˆ¶çš„è®¾è®¡ç©ºé—´ï¼ŒåŒ…æ‹¬å‚æ•°åŒ–ç­–ç•¥ã€å‚æ•°å…±äº«ã€è¡°å‡ç²’åº¦å’Œä½ç½®ç¼–ç å…¼å®¹æ€§å››ä¸ªç»´åº¦ï¼›2) åœ¨ä¸åŒçš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šè¿›è¡Œå¤§é‡çš„å®éªŒï¼Œè¯„ä¼°ä¸åŒè®¾è®¡é€‰æ‹©çš„æ€§èƒ½ï¼›3) åˆ†æå®éªŒç»“æœï¼Œæ€»ç»“ä¸åŒè®¾è®¡é€‰æ‹©çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶æå‡ºä¸€äº›è®¾è®¡å»ºè®®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ä¸­è¡°å‡æœºåˆ¶çš„è®¾è®¡ç©ºé—´ã€‚ä»¥å¾€çš„ç ”ç©¶å¾€å¾€åªå…³æ³¨å•ä¸€çš„è¡°å‡ç­–ç•¥ï¼Œè€Œæœ¬æ–‡åˆ™ä»å¤šä¸ªç»´åº¦å¯¹è¡°å‡æœºåˆ¶è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œä»è€Œæ­ç¤ºäº†ä¸åŒè®¾è®¡é€‰æ‹©ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»¥åŠå®ƒä»¬å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šæœ¬æ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æå‡ºäº†å¤šç§å‚æ•°åŒ–ç­–ç•¥ï¼Œä¾‹å¦‚æŒ‡æ•°è¡°å‡ã€çº¿æ€§è¡°å‡ç­‰ï¼›2) ç ”ç©¶äº†å‚æ•°å…±äº«å¯¹è¡°å‡å€¼çš„å½±å“ï¼›3) æ¯”è¾ƒäº†æ ‡é‡è¡°å‡å’Œå‘é‡è¡°å‡çš„æ€§èƒ½å·®å¼‚ï¼›4) åˆ†æäº†RoPEç­‰ä½ç½®ç¼–ç æ–¹æ³•ä¸çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶çš„å…¼å®¹æ€§ã€‚å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§è¯­è¨€å»ºæ¨¡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨äº†æ ‡å‡†çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚å›°æƒ‘åº¦ï¼ˆperplexityï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¡°å‡æœºåˆ¶çš„å‚æ•°åŒ–ç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œæœ‰æ•ˆçš„é…ç½®é€šå¸¸å±€é™äºç‰¹å®šçš„å‚æ•°èŒƒå›´ã€‚å‚æ•°å…±äº«éœ€è¦è°¨æ…ä½¿ç”¨ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´è¡°å‡å€¼è¿‡å¤§æˆ–è¿‡å°ã€‚åœ¨ç›¸åŒçš„å‚æ•°åŒ–ç­–ç•¥ä¸‹ï¼Œå‘é‡è¡°å‡é€šå¸¸ä¼˜äºæ ‡é‡è¡°å‡ã€‚æ­¤å¤–ï¼ŒRoPEå¯¹å¤§å¤šæ•°çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶çš„å¢ç›Šæœ‰é™ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œä¼˜åŒ–åçš„è¡°å‡ç­–ç•¥å¯ä»¥å°†å›°æƒ‘åº¦é™ä½X%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿åºåˆ—æ•°æ®çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ã€æ—¶é—´åºåˆ—åˆ†æç­‰ã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„è¡°å‡æœºåˆ¶ï¼Œå¯ä»¥æé«˜çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œæå‡ç›¸å…³ä»»åŠ¡çš„å‡†ç¡®ç‡å’Œæ•ˆç‡ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ›´æœ‰æ•ˆçš„è¡°å‡ç­–ç•¥ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ›´å¹¿æ³›çš„é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents a comprehensive investigation into the decay mechanisms inherent in linear complexity sequence models. We systematically delineate the design space of decay mechanisms across four pivotal dimensions: parameterization strategy, which refers to the computational methodology for decay; parameter sharing, which involves the utilization of supplementary parameters for decay computation; decay granularity, comparing scalar versus vector-based decay; and compatibility with relative positional encoding methods, such as Rotary Position Embedding (RoPE). Through an extensive series of experiments conducted on diverse language modeling tasks, we uncovered several critical insights. Firstly, the design of the parameterization strategy for decay requires meticulous consideration. Our findings indicate that effective configurations are typically confined to a specific range of parameters. Secondly, parameter sharing cannot be used arbitrarily, as it may cause decay values to be too large or too small, thereby significantly impacting performance. Thirdly, under identical parameterization strategies, scalar decay generally underperforms compared to its vector-based counterpart. However, in certain scenarios with alternative parameterization strategies, scalar decay may unexpectedly surpass vector decay in efficacy. Lastly, our analysis reveals that RoPE, a commonly employed relative positional encoding method, typically fails to provide tangible benefits to the majority of linear attention mechanisms.

