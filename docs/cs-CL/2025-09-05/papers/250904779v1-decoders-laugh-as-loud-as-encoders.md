---
layout: default
title: Decoders Laugh as Loud as Encoders
---

# Decoders Laugh as Loud as Encoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04779" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04779v1</a>
  <a href="https://arxiv.org/pdf/2509.04779.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04779v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04779v1', 'Decoders Laugh as Loud as Encoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eli Borodach, Raj Dandekar, Rajat Dandekar, Sreedath Panat

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è§£ç å™¨åœ¨å¹½é»˜ç†è§£ä¸Šå¯ä¸ç¼–ç å™¨åª²ç¾ï¼šGPT-4oåœ¨å¹½é»˜ç†è§£ä¸Šè¾¾åˆ°RoBERTaæ°´å¹³**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¹½é»˜ç†è§£` `è§£ç å™¨` `ç¼–ç å™¨` `GPT-4o` `RoBERTa` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¯¹å¹½é»˜ç­‰å¾®å¦™ä¸»é¢˜çš„ç†è§£ç¨‹åº¦å°šä¸æ˜ç¡®ã€‚
2. è¯¥ç ”ç©¶é€šè¿‡æ¯”è¾ƒå¾®è°ƒåçš„è§£ç å™¨ï¼ˆGPT-4oï¼‰å’Œç¼–ç å™¨ï¼ˆRoBERTaï¼‰åœ¨å¹½é»˜ç†è§£ä»»åŠ¡ä¸Šçš„è¡¨ç°æ¥è¯„ä¼°è§£ç å™¨çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„GPT-4oåœ¨å¹½é»˜ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¸RoBERTaç›¸å½“ï¼Œè¯æ˜äº†è§£ç å™¨åœ¨ç†è§£å¹½é»˜æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‰¾ä¼¦Â·å›¾çµæ›¾æ¢¦æƒ³åˆ›é€ å‡ºèƒ½åƒäººç±»ä¸€æ ·ç”¨è¯­è¨€äº¤æµçš„æœºå™¨äººã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°è¿›å±•éœ‡æƒŠäº†ç§‘å­¦ç•Œï¼Œå•ä¸ªæ¨¡å‹å³å¯åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ï¼Œå…¶è¾“å‡ºç»“æœæœ‰æ—¶ç”šè‡³ä¼˜äºäººç±»çš„æ²Ÿé€šæŠ€å·§ã€‚GPTã€Claudeã€Grokç­‰æ¨¡å‹åœ¨ç§‘å­¦ç•Œç•™ä¸‹äº†æ·±åˆ»çš„å°è®°ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šç†è§£å®ƒä»¬æ‰€äº§ç”Ÿçš„å†…å®¹å°šä¸æ¸…æ¥šï¼Œå°¤å…¶æ˜¯åœ¨å¹½é»˜è¿™ç§å¾®å¦™çš„ä¸»é¢˜ä¸Šã€‚è®¡ç®—æœºæ˜¯å¦ç†è§£å¹½é»˜çš„é—®é¢˜ä»ç„¶æ‚¬è€Œæœªå†³ï¼ˆåœ¨è§£ç å™¨ä¸­ï¼Œæœ€æ–°æ£€æŸ¥çš„æ˜¯GPT-2ï¼‰ã€‚æœ¬æ–‡æ¢è®¨äº†è¿™ä¸ªé—®é¢˜ï¼Œç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„è§£ç å™¨ï¼ˆGPT-4oï¼‰çš„æ€§èƒ½ï¼ˆå¹³å‡F1-macroå¾—åˆ†ä¸º0.85ï¼‰ä¸æœ€ä½³å¾®è°ƒç¼–ç å™¨ï¼ˆRoBERTaï¼Œå¹³å‡F1-scoreä¸º0.86ï¼‰ç›¸å½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯è§£ç å™¨ï¼Œåœ¨ç†è§£å¹½é»˜æ–¹é¢çš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç¼–ç å™¨æ¨¡å‹ä¸Šï¼Œè€Œå¯¹è§£ç å™¨åœ¨å¹½é»˜ç†è§£æ–¹é¢çš„ç ”ç©¶è¾ƒå°‘ã€‚å› æ­¤ï¼Œè¯¥ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œå¹¶è¯„ä¼°è§£ç å™¨æ˜¯å¦èƒ½å¤Ÿåƒç¼–ç å™¨ä¸€æ ·æœ‰æ•ˆåœ°ç†è§£å¹½é»˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¾®è°ƒè§£ç å™¨æ¨¡å‹ï¼ˆGPT-4oï¼‰å¹¶åœ¨å¹½é»˜ç†è§£ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå°†å…¶æ€§èƒ½ä¸ç»è¿‡å¾®è°ƒçš„ç¼–ç å™¨æ¨¡å‹ï¼ˆRoBERTaï¼‰è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœè§£ç å™¨èƒ½å¤Ÿè¾¾åˆ°ä¸ç¼–ç å™¨ç›¸å½“çš„æ€§èƒ½ï¼Œåˆ™è¡¨æ˜è§£ç å™¨ä¹Ÿå…·å¤‡ç†è§£å¹½é»˜çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1ï¼‰é€‰æ‹©åˆé€‚çš„è§£ç å™¨æ¨¡å‹ï¼ˆGPT-4oï¼‰å’Œç¼–ç å™¨æ¨¡å‹ï¼ˆRoBERTaï¼‰ï¼›2ï¼‰æ”¶é›†å¹½é»˜ç†è§£æ•°æ®é›†ï¼›3ï¼‰ä½¿ç”¨æ•°æ®é›†å¯¹GPT-4oå’ŒRoBERTaè¿›è¡Œå¾®è°ƒï¼›4ï¼‰åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹çš„æ€§èƒ½ï¼›5ï¼‰æ¯”è¾ƒGPT-4oå’ŒRoBERTaçš„æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚F1-macroå¾—åˆ†ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå®ƒå…³æ³¨äº†è§£ç å™¨åœ¨å¹½é»˜ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œè€Œä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç¼–ç å™¨ä¸Šã€‚é€šè¿‡æ¯”è¾ƒGPT-4oå’ŒRoBERTaçš„æ€§èƒ½ï¼Œè¯¥ç ”ç©¶ä¸ºè§£ç å™¨åœ¨å¹½é»˜ç†è§£æ–¹é¢çš„æ½œåŠ›æä¾›äº†æ–°çš„è§è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥ç ”ç©¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨GPT-4oä½œä¸ºè§£ç å™¨æ¨¡å‹ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ç§å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼›2ï¼‰ä½¿ç”¨RoBERTaä½œä¸ºç¼–ç å™¨æ¨¡å‹ï¼Œå› ä¸ºå®ƒåœ¨å„ç§NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼›3ï¼‰ä½¿ç”¨F1-macroå¾—åˆ†ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿç»¼åˆè€ƒè™‘ç²¾ç¡®ç‡å’Œå¬å›ç‡ï¼›4ï¼‰å¯¹GPT-4oå’ŒRoBERTaè¿›è¡Œå¾®è°ƒï¼Œä»¥ä½¿å…¶é€‚åº”å¹½é»˜ç†è§£ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„GPT-4oåœ¨å¹½é»˜ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†ä¸RoBERTaç›¸å½“çš„æ€§èƒ½ï¼Œå¹³å‡F1-macroå¾—åˆ†ä¸º0.85ï¼Œè€ŒRoBERTaçš„å¹³å‡F1-scoreä¸º0.86ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œè§£ç å™¨åœ¨ç†è§£å¹½é»˜æ–¹é¢å…·æœ‰ä¸ç¼–ç å™¨ç›¸å½“çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´å…·äººæƒ…å‘³çš„èŠå¤©æœºå™¨äººå’Œè™šæ‹ŸåŠ©æ‰‹ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å’Œå›åº”ç”¨æˆ·çš„å¹½é»˜ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ç†è§£èƒ½åŠ›çš„æ·±å…¥ç ”ç©¶ï¼Œå¹¶ä¸ºå¼€å‘æ›´æ™ºèƒ½çš„è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿæä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> From the dawn of the computer, Allen Turing dreamed of a robot that could communicate using language as a human being. The recent advances in the field of Large Language Models (LLMs) shocked the scientific community when a single model can apply for various natural language processing (NLP) tasks, while the output results are sometimes even better than most human communication skills. Models such as GPT, Claude, Grok, etc. have left their mark on the scientific community. However, it is unclear how much these models understand what they produce, especially in a nuanced theme such as humor. The question of whether computers understand humor is still open (among the decoders, the latest to be checked was GPT-2). We addressed this issue in this paper; we have showed that a fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well as the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)

