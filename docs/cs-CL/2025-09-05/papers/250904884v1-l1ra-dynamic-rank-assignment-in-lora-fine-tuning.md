---
layout: default
title: L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning
---

# L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04884" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04884v1</a>
  <a href="https://arxiv.org/pdf/2509.04884.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04884v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04884v1', 'L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Raul Singh, Nicolo Brunello, Vincenzo Scotti, Mark James Carman

**åˆ†ç±»**: cs.CL, cs.PF

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

**å¤‡æ³¨**: Work published at ICNLSP 2025, waiting for publication link

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**L1RAï¼šLoRAå¾®è°ƒä¸­åŸºäºL1æ­£åˆ™åŒ–çš„åŠ¨æ€ç§©åˆ†é…æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©é€‚é…å™¨` `LoRAå¾®è°ƒ` `L1æ­£åˆ™åŒ–` `åŠ¨æ€ç§©åˆ†é…` `æ¨¡å‹å‹ç¼©` `èµ„æºä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LoRAå¾®è°ƒæ–¹æ³•å¯¹æ‰€æœ‰é€‚é…å™¨åˆ†é…å›ºå®šç§©ï¼Œæœªè€ƒè™‘ä¸åŒå±‚å¯¹ä»»åŠ¡çš„é€‚åº”éœ€æ±‚å·®å¼‚ï¼Œå¯¼è‡´èµ„æºæµªè´¹ã€‚
2. L1RAé€šè¿‡L1æ­£åˆ™åŒ–åŠ¨æ€è°ƒæ•´LoRAé€‚é…å™¨çš„ç§©ï¼Œè‡ªåŠ¨å‰ªæä¸é‡è¦çš„ç§©å¹¶é‡æ–°åˆ†é…ï¼Œä¼˜åŒ–èµ„æºåˆ©ç”¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒL1RAåœ¨ä¿æŒæˆ–é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œå®ç°äº†ä¸ç°æœ‰LoRAæ–¹æ³•ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ä½¿å…¶åœ¨åŸºäºäººå·¥æ™ºèƒ½çš„åº”ç”¨ç¨‹åºå¼€å‘ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒè¿™äº›LLMçš„é«˜è®¡ç®—éœ€æ±‚å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†L1RAï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æŠ€æœ¯ï¼Œæ—¨åœ¨åˆ©ç”¨LoRAåœ¨å¾®è°ƒæœŸé—´åŠ¨æ€åˆ†é…ä½ç§©é€‚é…å™¨çš„ç§©ã€‚ç»™å®šä¸€ä¸ªç§©é¢„ç®—ï¼ˆå³é€‚é…å™¨ç§©çš„æ€»å’Œï¼‰ï¼ŒL1RAåˆ©ç”¨L1æ­£åˆ™åŒ–æ¥ä¿®å‰ªå†—ä½™ç§©ï¼Œå¹¶å°†å®ƒä»¬é‡æ–°åˆ†é…åˆ°å„ä¸ªé€‚é…å™¨ï¼Œä»è€Œä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡ã€‚é€šè¿‡ä¸€ç³»åˆ—å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼Œä¸å…¶ä»–LoRAå˜ä½“ï¼ˆåŒ…æ‹¬vanillaæ–¹æ³•ï¼‰ç›¸æ¯”ï¼ŒL1RAä¿æŒäº†ç›¸å½“ç”šè‡³æ›´ä½çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶å®ç°äº†ç›¸åŒæˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯¹ç§©åˆ†å¸ƒçš„è®­ç»ƒååˆ†ææ­ç¤ºäº†å¯¹ç‰¹å®šæ¨¡å‹ç»„ä»¶çš„æ·±å…¥äº†è§£ï¼Œè¿™äº›ç»„ä»¶éœ€è¦æœ€å¤šçš„é€‚åº”æ‰èƒ½ä¸ä»»åŠ¡ç›®æ ‡å¯¹é½ï¼šå‰é¦ˆå±‚å’Œæ³¨æ„åŠ›è¾“å‡ºæŠ•å½±ã€‚è¿™äº›ç»“æœçªå‡ºäº†L1RAåœ¨æé«˜LLMå¾®è°ƒæ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¹Ÿä¸ºæ¨¡å‹æ”¹è¿›å’Œå®šåˆ¶æä¾›äº†æœ‰ä»·å€¼çš„è¯Šæ–­ä¿¡æ¯ã€‚æ€»ä¹‹ï¼ŒL1RAæ˜¯ä¸€ç§æœ‰å‰é€”çš„æŠ€æœ¯ï¼Œå¯ä»¥æé«˜LLMé€‚åº”çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºå—é™çš„æƒ…å†µä¸‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„è®¡ç®—èµ„æºçš„é—®é¢˜ã€‚ç°æœ‰çš„LoRAæ–¹æ³•é€šå¸¸ä¸ºæ‰€æœ‰é€‚é…å™¨åˆ†é…å›ºå®šçš„ç§©ï¼Œå¿½ç•¥äº†ä¸åŒæ¨¡å‹å±‚å¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”ç¨‹åº¦ä¸åŒï¼Œå¯¼è‡´éƒ¨åˆ†é€‚é…å™¨å­˜åœ¨å†—ä½™ï¼Œæµªè´¹è®¡ç®—èµ„æºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šL1RAçš„æ ¸å¿ƒæ€è·¯æ˜¯åŠ¨æ€åœ°ä¸ºLoRAé€‚é…å™¨åˆ†é…ç§©ï¼Œæ ¹æ®å„ä¸ªé€‚é…å™¨å¯¹ä»»åŠ¡çš„é‡è¦æ€§ç¨‹åº¦ï¼Œè‡ªåŠ¨è°ƒæ•´å…¶ç§©çš„å¤§å°ã€‚é€šè¿‡L1æ­£åˆ™åŒ–ï¼Œé¼“åŠ±æ¨¡å‹å°†ç§©é›†ä¸­åœ¨å¯¹ä»»åŠ¡è´¡çŒ®æœ€å¤§çš„é€‚é…å™¨ä¸Šï¼Œè€Œå¯¹ä¸é‡è¦çš„é€‚é…å™¨è¿›è¡Œå‰ªæï¼Œä»è€Œåœ¨ç»™å®šçš„ç§©é¢„ç®—ä¸‹ï¼Œæœ€å¤§åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šL1RAåŸºäºLoRAæ¡†æ¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯¹LoRAé€‚é…å™¨çš„æƒé‡çŸ©é˜µæ–½åŠ L1æ­£åˆ™åŒ–ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š
1. åˆå§‹åŒ–LoRAé€‚é…å™¨ã€‚
2. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®¡ç®—æŸå¤±å‡½æ•°ï¼Œå¹¶åŠ å…¥L1æ­£åˆ™åŒ–é¡¹ã€‚
3. ä½¿ç”¨ä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹å‚æ•°ï¼ŒåŒ…æ‹¬LoRAé€‚é…å™¨çš„æƒé‡ã€‚
4. L1æ­£åˆ™åŒ–ä¼šä¿ƒä½¿éƒ¨åˆ†é€‚é…å™¨çš„æƒé‡è¶‹è¿‘äºé›¶ï¼Œä»è€Œå®ç°ç§©çš„åŠ¨æ€åˆ†é…ã€‚
5. è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥æ ¹æ®é€‚é…å™¨çš„æƒé‡å¤§å°ï¼Œåˆ†æä¸åŒå±‚å¯¹ä»»åŠ¡çš„é‡è¦æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šL1RAçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†L1æ­£åˆ™åŒ–æ¥åŠ¨æ€è°ƒæ•´LoRAé€‚é…å™¨çš„ç§©ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šç§©åˆ†é…æ–¹æ³•ç›¸æ¯”ï¼ŒL1RAèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å¹¶å‰ªæä¸é‡è¦çš„é€‚é…å™¨ï¼Œå¹¶å°†èµ„æºé›†ä¸­åœ¨é‡è¦çš„é€‚é…å™¨ä¸Šï¼Œä»è€Œæé«˜äº†èµ„æºåˆ©ç”¨ç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šL1RAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š
1. **L1æ­£åˆ™åŒ–å¼ºåº¦**ï¼šéœ€è¦ä»”ç»†è°ƒæ•´L1æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼Œä»¥å¹³è¡¡æ¨¡å‹çš„æ€§èƒ½å’Œç¨€ç–æ€§ã€‚
2. **ç§©é¢„ç®—**ï¼šéœ€è¦æ ¹æ®è®¡ç®—èµ„æºå’Œä»»åŠ¡å¤æ‚åº¦ï¼Œè®¾ç½®åˆé€‚çš„ç§©é¢„ç®—ã€‚
3. **ä¼˜åŒ–å™¨é€‰æ‹©**ï¼šé€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨ï¼Œä¾‹å¦‚AdamWï¼Œä»¥åŠ é€Ÿæ”¶æ•›å¹¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚
4. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼Œä¾‹å¦‚ä½™å¼¦é€€ç«ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒL1RAåœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šå–å¾—äº†ä¸ä¼ ç»ŸLoRAæ–¹æ³•ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—å¼€é”€ã€‚ä¾‹å¦‚ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒL1RAåœ¨ä¿æŒç›¸åŒæ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å°†é€‚é…å™¨çš„ç§©é™ä½20%ã€‚æ­¤å¤–ï¼ŒL1RAè¿˜æ­ç¤ºäº†æ¨¡å‹ä¸­ä¸åŒå±‚å¯¹ä»»åŠ¡çš„é‡è¦æ€§ï¼Œä¾‹å¦‚ï¼Œå‰é¦ˆå±‚å’Œæ³¨æ„åŠ›è¾“å‡ºæŠ•å½±é€šå¸¸éœ€è¦æ›´å¤šçš„é€‚åº”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

L1RAå¯åº”ç”¨äºå„ç§éœ€è¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºå—é™çš„æƒ…å†µä¸‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²LLMæ—¶ï¼Œå¯ä»¥ä½¿ç”¨L1RAæ¥å‡å°æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œæé«˜æ¨ç†é€Ÿåº¦å’Œé™ä½åŠŸè€—ã€‚æ­¤å¤–ï¼ŒL1RAè¿˜å¯ä»¥ç”¨äºæ¨¡å‹å‹ç¼©å’ŒçŸ¥è¯†è’¸é¦ï¼Œå°†å¤§å‹æ¨¡å‹å‹ç¼©æˆæ›´å°çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The ability of Large Language Models (LLMs) to solve complex tasks has made them crucial in the development of AI-based applications. However, the high computational requirements to fine-tune these LLMs on downstream tasks pose significant challenges, particularly when resources are limited. In response to this challenge, we introduce L1RA, a novel technique aimed at dynamically distributing the rank of low-rank adapters during fine-tuning using LoRA. Given a rank budget (i.e., total sum of adapters rank), L1RA leverages L1 regularisation to prune redundant ranks and redistribute them across adapters, thereby optimising resource utilisation. Through a series of comprehensive experiments, we empirically demonstrate that L1RA maintains comparable or even reduced computational overhead compared to other LoRA variants, including the vanilla approach, while achieving same or better performances. Moreover, the post-training analysis of rank distribution unveiled insights into the specific model components requiring the most adaptation to align with the task objective: the feed-forward layers and the attention output projection. These results highlight the efficacy of L1RA in not only enhancing the efficiency of LLM fine-tuning, but also in providing valuable diagnostic information for model refinement and customisation. In conclusion, L1RA stands as a promising technique for advancing the performance and interpretability of LLM adaptation, particularly in scenarios where computational resources are constrained.

