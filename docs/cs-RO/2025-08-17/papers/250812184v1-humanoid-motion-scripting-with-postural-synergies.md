---
layout: default
title: Humanoid Motion Scripting with Postural Synergies
---

# Humanoid Motion Scripting with Postural Synergies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12184" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12184v1</a>
  <a href="https://arxiv.org/pdf/2508.12184.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12184v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12184v1', 'Humanoid Motion Scripting with Postural Synergies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rhea Malhotra, William Chong, Catie Cuan, Oussama Khatib

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-17

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://rhea-mal.github.io/humanoidsynergies.io)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSynSculptorä»¥è§£å†³ç±»äººæœºå™¨äººè¿åŠ¨ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `ç±»äººæœºå™¨äºº` `è¿åŠ¨ç”Ÿæˆ` `å§¿æ€ååŒ` `ä¸»æˆåˆ†åˆ†æ` `è¿åŠ¨å¹³æ»‘åº¦` `æ— è®­ç»ƒè„šæœ¬` `è¿åŠ¨è¯­è¨€å˜æ¢å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆç±»äººæœºå™¨äººè¿åŠ¨æ—¶é¢ä¸´æ•°æ®æ”¶é›†å’Œè¿åŠ¨æ˜ å°„çš„æŒ‘æˆ˜ï¼Œéš¾ä»¥å®ç°è‡ªç„¶æµç•…çš„è¿åŠ¨ã€‚
2. æœ¬æ–‡æå‡ºçš„SynSculptoræ¡†æ¶é€šè¿‡å§¿æ€ååŒå®ç°æ— è®­ç»ƒçš„äººç±»è¿åŠ¨è„šæœ¬ç”Ÿæˆï¼Œç®€åŒ–äº†è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å§¿æ€ååŒç”Ÿæˆçš„è¿åŠ¨åœ¨è„šæ»‘æ¯”å’Œè¿åŠ¨å¹³æ»‘åº¦æŒ‡æ ‡ä¸Šä¼˜äºå‚è€ƒè¿åŠ¨ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆç±»äººæœºå™¨äººçš„äººç±»è¿åŠ¨åºåˆ—é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ”¶é›†å’Œåˆ†æå‚è€ƒäººç±»è¿åŠ¨ã€åŸºäºè¿™äº›å‚è€ƒè¿åŠ¨åˆæˆæ–°è¿åŠ¨ä»¥åŠå°†ç”Ÿæˆçš„è¿åŠ¨æ˜ å°„åˆ°ç±»äººæœºå™¨äººä¸Šã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SynSculptorï¼Œä¸€ä¸ªåˆ©ç”¨å§¿æ€ååŒçš„æ— è®­ç»ƒäººç±»è¿åŠ¨è„šæœ¬æ¡†æ¶ã€‚æˆ‘ä»¬æ”¶é›†äº†20åä¸ªä½“è¶…è¿‡3å°æ—¶çš„åŠ¨ä½œæ•æ‰æ•°æ®ï¼Œä½¿ç”¨å®æ—¶æ“ä½œç©ºé—´æ§åˆ¶å™¨åœ¨æ¨¡æ‹Ÿç±»äººæœºå™¨äººä¸Šæ¨¡ä»¿äººç±»è¿åŠ¨ã€‚é€šè¿‡ä¸»æˆåˆ†åˆ†ææå–ä¸»è¦å§¿æ€ååŒï¼Œæ„å»ºäº†ä¸€ä¸ªé£æ ¼æ¡ä»¶çš„ååŒåº“ç”¨äºè‡ªç”±ç©ºé—´è¿åŠ¨ç”Ÿæˆã€‚æœ€åï¼Œç»“åˆè¿åŠ¨è¯­è¨€å˜æ¢å™¨ï¼Œç±»äººåœ¨æ‰§è¡Œè¿åŠ¨ä»»åŠ¡æ—¶æ ¹æ®é€‰æ‹©çš„ååŒè‡ªé€‚åº”å§¿æ€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç±»äººæœºå™¨äººè¿åŠ¨ç”Ÿæˆä¸­çš„æ•°æ®æ”¶é›†ã€è¿åŠ¨åˆæˆå’Œè¿åŠ¨æ˜ å°„ç­‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´ç”Ÿæˆçš„è¿åŠ¨ä¸å¤Ÿè‡ªç„¶æµç•…ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„SynSculptoræ¡†æ¶åˆ©ç”¨å§¿æ€ååŒè¿›è¡Œæ— è®­ç»ƒçš„äººç±»è¿åŠ¨è„šæœ¬ç”Ÿæˆï¼Œæ—¨åœ¨é€šè¿‡åˆ†æäººç±»è¿åŠ¨çš„ä¸»è¦ç‰¹å¾æ¥ç®€åŒ–è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å§¿æ€ååŒæå–ã€é£æ ¼æ¡ä»¶ååŒåº“æ„å»ºå’Œè¿åŠ¨ç”Ÿæˆå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†åŠ¨ä½œæ•æ‰æ•°æ®ï¼Œç„¶åé€šè¿‡ä¸»æˆåˆ†åˆ†ææå–å§¿æ€ååŒï¼Œæœ€ååˆ©ç”¨è¿åŠ¨è¯­è¨€å˜æ¢å™¨ç”Ÿæˆè¿åŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªé£æ ¼æ¡ä»¶çš„å§¿æ€ååŒåº“ï¼Œä½¿å¾—ç”Ÿæˆçš„è¿åŠ¨èƒ½å¤Ÿåœ¨ä¸åŒé£æ ¼ä¸‹è‡ªé€‚åº”è°ƒæ•´ï¼Œæ˜¾è‘—æé«˜äº†è¿åŠ¨çš„è‡ªç„¶æ€§å’Œæµç•…æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œä½¿ç”¨ä¸»æˆåˆ†åˆ†ææå–å§¿æ€ååŒï¼Œè®¾è®¡äº†æ–°çš„è¿åŠ¨å¹³æ»‘åº¦è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶é€šè¿‡å®æ—¶æ“ä½œç©ºé—´æ§åˆ¶å™¨å®ç°äº†è¿åŠ¨çš„å®æ—¶æ¨¡æ‹Ÿã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªè¯¦ç»†æŠ«éœ²ï¼Œæ ‡è®°ä¸ºæœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å§¿æ€ååŒç”Ÿæˆçš„è¿åŠ¨åœ¨è„šæ»‘æ¯”å’Œè¿åŠ¨å¹³æ»‘åº¦æ–¹é¢ä¼˜äºå‚è€ƒè¿åŠ¨ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚é€šè¿‡ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¯¹æ¯”ï¼ŒéªŒè¯äº†SynSculptoråœ¨è¿åŠ¨ç”Ÿæˆä¸Šçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç±»äººæœºå™¨äººã€è™šæ‹Ÿç°å®å’ŒåŠ¨ç”»åˆ¶ä½œç­‰ã€‚é€šè¿‡å®ç°è‡ªç„¶æµç•…çš„è¿åŠ¨ç”Ÿæˆï¼ŒSynSculptorå¯ä»¥æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„äº¤äº’èƒ½åŠ›ï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒï¼Œå¹¶æ¨åŠ¨äººæœºåä½œçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generating sequences of human-like motions for humanoid robots presents challenges in collecting and analyzing reference human motions, synthesizing new motions based on these reference motions, and mapping the generated motion onto humanoid robots. To address these issues, we introduce SynSculptor, a humanoid motion analysis and editing framework that leverages postural synergies for training-free human-like motion scripting. To analyze human motion, we collect 3+ hours of motion capture data across 20 individuals where a real-time operational space controller mimics human motion on a simulated humanoid robot. The major postural synergies are extracted using principal component analysis (PCA) for velocity trajectories segmented by changes in robot momentum, constructing a style-conditioned synergy library for free-space motion generation. To evaluate generated motions using the synergy library, the foot-sliding ratio and proposed metrics for motion smoothness involving total momentum and kinetic energy deviations are computed for each generated motion, and compared with reference motions. Finally, we leverage the synergies with a motion-language transformer, where the humanoid, during execution of motion tasks with its end-effectors, adapts its posture based on the chosen synergy. Supplementary material, code, and videos are available at https://rhea-mal.github.io/humanoidsynergies.io.

