---
layout: default
title: Efficient Environment Design for Multi-Robot Navigation via Continuous Control
---

# Efficient Environment Design for Multi-Robot Navigation via Continuous Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14105" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14105v1</a>
  <a href="https://arxiv.org/pdf/2508.14105.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14105v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14105v1', 'Efficient Environment Design for Multi-Robot Navigation via Continuous Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jahid Chowdhury Choton, John Woods, William Hsu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-17

**å¤‡æ³¨**: 12 pages, 3 figures, conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆç¯å¢ƒè®¾è®¡ä»¥è§£å†³å¤šæœºå™¨äººå¯¼èˆªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæœºå™¨äººå¯¼èˆª` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `ç¯å¢ƒè®¾è®¡` `è·¯å¾„è§„åˆ’` `å†œä¸šæœºå™¨äºº` `ä¼˜åŒ–é—®é¢˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæœºå™¨äººå¯¼èˆªæ–¹æ³•åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­é¢ä¸´æ ·æœ¬æ•ˆç‡ä½å’Œè®­ç»ƒæ—¶é—´é•¿çš„æŒ‘æˆ˜ï¼Œç¼ºä¹æ­£å¼çš„ç¯å¢ƒè®¾è®¡ä¿è¯ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”å¯å®šåˆ¶çš„ç¯å¢ƒï¼Œåˆ©ç”¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å½¢å¼åŒ–å¤šæœºå™¨äººå¯¼èˆªä»»åŠ¡ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–é—®é¢˜æ¥æ±‚è§£æœ€ä¼˜ç­–ç•¥ã€‚
3. åœ¨CoppeliaSimæ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æç¯å¢ƒåœ¨3Då†œä¸šé¢†åŸŸä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæœºå™¨äººåœ¨ä¸ç¡®å®šç¯å¢ƒä¸­çš„å¯¼èˆªå’Œè·¯å¾„è§„åˆ’ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯è§£å†³æ­¤ä»»åŠ¡çš„çƒ­é—¨èŒƒå¼ï¼Œä½†ç”±äºæ ·æœ¬æ•ˆç‡ä½å’Œè®­ç»ƒæ—¶é—´é•¿ï¼Œå…¶å®é™…åº”ç”¨å—åˆ°é™åˆ¶ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„å¤šæœºå™¨äººå¯¼èˆªç ”ç©¶åœ¨ç¯å¢ƒè®¾è®¡ä¸Šç¼ºä¹æ­£å¼ä¿è¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”é«˜åº¦å¯å®šåˆ¶çš„ç¯å¢ƒï¼Œç”¨äºè¿ç»­æ§åˆ¶çš„å¤šæœºå™¨äººå¯¼èˆªï¼Œæœºå™¨äººéœ€é€šè¿‡æœ€çŸ­è·¯å¾„è®¿é—®ä¸€ç»„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ã€‚è¯¥ä»»åŠ¡è¢«å½¢å¼åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–é—®é¢˜æ¥æè¿°å¤šæœºå™¨äººå¯¼èˆªä»»åŠ¡ã€‚æˆ‘ä»¬è®¾è®¡äº†å¤šç§ç¯å¢ƒå˜ä½“ï¼Œå¹¶ä½¿ç”¨å¤šç§RLæ–¹æ³•è¿›è¡Œæ€§èƒ½æµ‹é‡ã€‚ä¸ºå±•ç¤ºå®é™…åº”ç”¨ï¼Œæˆ‘ä»¬åœ¨CoppeliaSimæœºå™¨äººæ¨¡æ‹Ÿå™¨ä¸­å°†ç¯å¢ƒéƒ¨ç½²åˆ°å…·æœ‰ä¸ç¡®å®šæ€§çš„3Då†œä¸šé¢†åŸŸï¼Œå¹¶å¯¹å­¦ä¹ æ¨¡å‹çš„é²æ£’æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œå°†æŒ‡å¯¼ç ”ç©¶äººå‘˜å¼€å‘é€‚ç”¨äºç°å®ç³»ç»Ÿçš„åŸºäºMDPçš„ç¯å¢ƒï¼Œå¹¶ä½¿ç”¨ç°æœ‰çš„æœ€å…ˆè¿›RLæ–¹æ³•åœ¨æœ‰é™èµ„æºå’Œåˆç†æ—¶é—´å†…è§£å†³è¿™äº›é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæœºå™¨äººåœ¨ä¸ç¡®å®šç¯å¢ƒä¸­çš„å¯¼èˆªå’Œè·¯å¾„è§„åˆ’é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´æ ·æœ¬æ•ˆç‡ä½å’Œè®­ç»ƒæ—¶é—´é•¿çš„ç—›ç‚¹ï¼Œä¸”ç¼ºä¹ç¯å¢ƒè®¾è®¡çš„æ­£å¼ä¿è¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”é«˜åº¦å¯å®šåˆ¶çš„ç¯å¢ƒï¼Œåˆ©ç”¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ¥å½¢å¼åŒ–å¤šæœºå™¨äººå¯¼èˆªä»»åŠ¡ã€‚é€šè¿‡å°†ä»»åŠ¡è§†ä¸ºä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œä»è€Œæé«˜å¯¼èˆªæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç¯å¢ƒè®¾è®¡ã€ä»»åŠ¡å»ºæ¨¡å’Œç­–ç•¥ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚ç¯å¢ƒè®¾è®¡éƒ¨åˆ†å…è®¸çµæ´»é…ç½®ï¼Œä»»åŠ¡å»ºæ¨¡å°†å¯¼èˆªä»»åŠ¡å½¢å¼åŒ–ä¸ºMDPï¼Œè€Œç­–ç•¥ä¼˜åŒ–åˆ™é‡‡ç”¨å¤šç§RLæ–¹æ³•è¿›è¡Œæ±‚è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§å¯å®šåˆ¶çš„MDPç¯å¢ƒè®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸ç¡®å®šæ€§æ¡ä»¶ä¸‹æœ‰æ•ˆæ”¯æŒå¤šæœºå™¨äººå¯¼èˆªä»»åŠ¡ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç¯å¢ƒè®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†å¤šä¸ªå‚æ•°ä»¥é€‚åº”ä¸åŒçš„å¯¼èˆªåœºæ™¯ï¼Œå¹¶é‡‡ç”¨äº†å¤šç§æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„æ¥ä¼˜åŒ–RLç®—æ³•çš„æ€§èƒ½ã€‚å…·ä½“ä½¿ç”¨äº†A2Cã€PPOã€TRPOç­‰å¤šç§æ–¹æ³•è¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æç¯å¢ƒåœ¨3Då†œä¸šé¢†åŸŸä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ï¼Œä½¿ç”¨å¤šç§RLæ–¹æ³•ï¼ˆå¦‚A2Cã€PPOç­‰ï¼‰è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ¨¡å‹åœ¨å¯¼èˆªä»»åŠ¡ä¸­çš„æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†œä¸šã€ç‰©æµå’Œæ•‘ç¾ç­‰å¤šæœºå™¨äººåä½œåœºæ™¯ã€‚é€šè¿‡æä¾›é«˜æ•ˆçš„ç¯å¢ƒè®¾è®¡ï¼Œç ”ç©¶æˆæœèƒ½å¤Ÿå¸®åŠ©æœºå™¨äººåœ¨å¤æ‚å’Œä¸ç¡®å®šçš„ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆå¯¼èˆªï¼Œä»è€Œæé«˜å®é™…åº”ç”¨çš„æ•ˆç‡å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿åˆ°å…¶ä»–é¢†åŸŸçš„å¤šæœºå™¨äººç³»ç»Ÿä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-robot navigation and path planning in continuous state and action spaces with uncertain environments remains an open challenge. Deep Reinforcement Learning (RL) is one of the most popular paradigms for solving this task, but its real-world application has been limited due to sample inefficiency and long training periods. Moreover, the existing works using RL for multi-robot navigation lack formal guarantees while designing the environment. In this paper, we introduce an efficient and highly customizable environment for continuous-control multi-robot navigation, where the robots must visit a set of regions of interest (ROIs) by following the shortest paths. The task is formally modeled as a Markov Decision Process (MDP). We describe the multi-robot navigation task as an optimization problem and relate it to finding an optimal policy for the MDP. We crafted several variations of the environment and measured the performance using both gradient and non-gradient based RL methods: A2C, PPO, TRPO, TQC, CrossQ and ARS. To show real-world applicability, we deployed our environment to a 3-D agricultural field with uncertainties using the CoppeliaSim robot simulator and measured the robustness by running inference on the learned models. We believe our work will guide the researchers on how to develop MDP-based environments that are applicable to real-world systems and solve them using the existing state-of-the-art RL methods with limited resources and within reasonable time periods.

