---
layout: default
title: GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions
---

# GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07650" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07650v2</a>
  <a href="https://arxiv.org/pdf/2508.07650.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07650v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07650v2', 'GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Helong Huang, Min Cen, Kai Tan, Xingyue Quan, Guowei Huang, Hong Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-08-23)

**å¤‡æ³¨**: 10 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGraphCoT-VLAä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ¨¡ç³ŠæŒ‡ä»¤é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-è¡ŒåŠ¨` `æœºå™¨äººæ“ä½œ` `æ¨¡ç³ŠæŒ‡ä»¤` `ä¸‰ç»´äº¤äº’` `Chain-of-Thoughtæ¨ç†` `3Då§¿æ€-ç‰©ä½“å›¾` `æ··åˆæ¨ç†ç­–ç•¥` `ä»»åŠ¡è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹åœ¨å¤„ç†æ¨¡ç³ŠæŒ‡ä»¤å’ŒåŠ¨æ€ç¯å¢ƒæ—¶è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†æœºå™¨äººæ“ä½œçš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚
2. æœ¬æ–‡æå‡ºGraphCoT-VLAæ¨¡å‹ï¼Œé€šè¿‡ç»“æ„åŒ–çš„Chain-of-Thoughtæ¨ç†æ¨¡å—å’Œ3Då§¿æ€-ç‰©ä½“å›¾ï¼Œæå‡äº†å¯¹æ¨¡ç³ŠæŒ‡ä»¤çš„ç†è§£å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGraphCoT-VLAåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„æœºå™¨äººä»»åŠ¡ä¸­ï¼Œä»»åŠ¡æˆåŠŸç‡å’Œå“åº”é€Ÿåº¦æ˜¾è‘—æé«˜ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­å·²æˆä¸ºé‡è¦èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨å¤„ç†æ¨¡ç³Šè¯­è¨€æŒ‡ä»¤å’ŒæœªçŸ¥ç¯å¢ƒçŠ¶æ€æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„æ„ŸçŸ¥ä¸»è¦å±€é™äºé™æ€äºŒç»´è§‚å¯Ÿï¼Œç¼ºä¹å»ºæ¨¡æœºå™¨äººä¸ç¯å¢ƒä¹‹é—´ä¸‰ç»´äº¤äº’çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†GraphCoT-VLAï¼Œä¸€ä¸ªé«˜æ•ˆçš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚é€šè¿‡è®¾è®¡ç»“æ„åŒ–çš„Chain-of-Thoughtæ¨ç†æ¨¡å—ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¢å¼ºå¯¹æ¨¡ç³ŠæŒ‡ä»¤çš„ç†è§£å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæ„å»ºäº†å®æ—¶å¯æ›´æ–°çš„3Då§¿æ€-ç‰©ä½“å›¾ï¼Œä»¥æ•æ‰æœºå™¨äººå…³èŠ‚çš„ç©ºé—´é…ç½®åŠç‰©ä½“åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„æ‹“æ‰‘å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphCoT-VLAåœ¨ä»»åŠ¡æˆåŠŸç‡å’Œå“åº”é€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºåœ¨å¼€æ”¾ç¯å¢ƒå’Œä¸ç¡®å®šæŒ‡ä»¤ä¸‹çš„å¼ºæ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹åœ¨å¤„ç†æ¨¡ç³ŠæŒ‡ä»¤åŠåŠ¨æ€ç¯å¢ƒä¸­çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä¸‰ç»´äº¤äº’å»ºæ¨¡æ–¹é¢çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ç»“æ„åŒ–çš„Chain-of-Thoughtæ¨ç†æ¨¡å—å’Œå®æ—¶æ›´æ–°çš„3Då§¿æ€-ç‰©ä½“å›¾ï¼Œå¢å¼ºæ¨¡å‹å¯¹æ¨¡ç³ŠæŒ‡ä»¤çš„ç†è§£èƒ½åŠ›å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ï¼Œä»è€Œæå‡æœºå™¨äººæ“ä½œçš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGraphCoT-VLAæ¨¡å‹åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šChain-of-Thoughtæ¨ç†æ¨¡å—ã€3Då§¿æ€-ç‰©ä½“å›¾æ¨¡å—å’Œæ··åˆæ¨ç†ç­–ç•¥æ¨¡å—ã€‚å‰è€…è´Ÿè´£é«˜å±‚æ¬¡ä»»åŠ¡ç†è§£å’Œè§„åˆ’ï¼Œåè€…æ•æ‰ä¸‰ç»´ç©ºé—´ä¸­çš„ç‰©ä½“å…³ç³»ï¼Œæ··åˆæ¨ç†ç­–ç•¥åˆ™ä¼˜åŒ–æ§åˆ¶è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†3Då§¿æ€-ç‰©ä½“å›¾ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå®æ—¶æ•æ‰å’Œæ›´æ–°æœºå™¨äººä¸ç¯å¢ƒä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œä»è€Œå®ç°æ›´å¤æ‚çš„ä¸‰ç»´äº¤äº’ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGraphCoT-VLAåœ¨å¤„ç†åŠ¨æ€å’Œæ¨¡ç³ŠæŒ‡ä»¤æ—¶è¡¨ç°å‡ºæ›´é«˜çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†æ··åˆæ¨ç†ç­–ç•¥ï¼Œé€šè¿‡dropoutæŠ€æœ¯æé«˜æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†ä»»åŠ¡æˆåŠŸç‡å’Œå“åº”é€Ÿåº¦çš„å¹³è¡¡ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šç§ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚æ•´ä½“ç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥é€‚åº”å®æ—¶æ›´æ–°çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphCoT-VLAåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„æœºå™¨äººä»»åŠ¡ä¸­ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜äº†çº¦30%ï¼Œå“åº”é€Ÿåº¦æå‡äº†20%ã€‚ä¸ç°æœ‰åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œå°¤å…¶åœ¨å¤„ç†ä¸ç¡®å®šæŒ‡ä»¤æ—¶è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’ŒæœåŠ¡æœºå™¨äººç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹æ¨¡ç³ŠæŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ï¼ŒGraphCoT-VLAèƒ½å¤Ÿåœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­æ‰§è¡Œæ›´ä¸ºç²¾ç¡®çš„æ“ä½œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. However, existing VLA models exhibit notable limitations in handling ambiguous language instructions and unknown environmental states. Furthermore, their perception is largely constrained to static two-dimensional observations, lacking the capability to model three-dimensional interactions between the robot and its environment. To address these challenges, this paper proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's ability to interpret ambiguous instructions and improve task planning, we design a structured Chain-of-Thought reasoning module that integrates high-level task understanding and planning, failed task feedback, and low-level imaginative reasoning about future object positions and robot actions. Additionally, we construct a real-time updatable 3D Pose-Object graph, which captures the spatial configuration of robot joints and the topological relationships between objects in 3D space, enabling the model to better understand and manipulate their interactions. We further integrates a dropout hybrid reasoning strategy to achieve efficient control outputs. Experimental results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA significantly outperforms existing methods in terms of task success rate and response speed, exhibiting strong generalization and robustness in open environments and under uncertain instructions.

