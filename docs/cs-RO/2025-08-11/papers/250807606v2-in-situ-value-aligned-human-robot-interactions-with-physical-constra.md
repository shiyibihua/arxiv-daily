---
layout: default
title: In-situ Value-aligned Human-Robot Interactions with Physical Constraints
---

# In-situ Value-aligned Human-Robot Interactions with Physical Constraints

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07606" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07606v2</a>
  <a href="https://arxiv.org/pdf/2508.07606.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07606v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07606v2', 'In-situ Value-aligned Human-Robot Interactions with Physical Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongtao Li, Ziyuan Jiao, Xiaofeng Liu, Hangxin Liu, Zilong Zheng

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-12-12)

**å¤‡æ³¨**: 8 pages, 7 figures. Accepted by IROS 2025

**æœŸåˆŠ**: 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)

**DOI**: [10.1109/IROS60139.2025.11246572](https://doi.org/10.1109/IROS60139.2025.11246572)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“åˆäººç±»åå¥½ä¸ç‰©ç†çº¦æŸçš„æœºå™¨äººäº¤äº’æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººç±»åå¥½` `ç‰©ç†çº¦æŸ` `æœºå™¨äººäº¤äº’` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `äººç±»åé¦ˆ` `ä»»åŠ¡ç”Ÿæˆ` `æ™ºèƒ½æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†äººç±»åå¥½ä¸ç‰©ç†çº¦æŸçš„ç»“åˆï¼Œå¯¼è‡´æœºå™¨äººåœ¨æ‰§è¡Œä»»åŠ¡æ—¶ç¼ºä¹çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„ICLHFæ¡†æ¶é€šè¿‡æ•´åˆäººç±»åé¦ˆä¸ç‰©ç†çº¦æŸï¼Œæå‡äº†æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒICLHFåœ¨ç”Ÿæˆä»»åŠ¡è®¡åˆ’æ—¶æ˜¾è‘—æé«˜äº†æ•ˆç‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡åå¥½ä¸ç‰©ç†çº¦æŸã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åº”ç”¨ï¼Œäººæœ¬æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œè®¸å¤šä»¥å‰è¢«è®¤ä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä»…ä»…å®Œæˆä»»åŠ¡å¹¶ä¸è¶³ä»¥æ»¡è¶³è®¤çŸ¥æœºå™¨äººçš„éœ€æ±‚ï¼Œå®ƒä»¬è¿˜éœ€å­¦ä¹ å¹¶åº”ç”¨äººç±»çš„åå¥½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œå°†äººç±»åå¥½ä¸ç‰©ç†çº¦æŸç›¸ç»“åˆï¼Œè¦æ±‚æœºå™¨äººåœ¨å®Œæˆä»»åŠ¡æ—¶åŒæ—¶è€ƒè™‘è¿™ä¸¤è€…ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ—¥å¸¸å®¶åŠ¡æ´»åŠ¨çš„åŸºå‡†ï¼ŒåŸºäºç‰¹å®šåå¥½è¿›è¡Œè¯„ä¼°ã€‚å¼•å…¥äº†åŸºäºäººç±»åé¦ˆçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLHFï¼‰ï¼Œäººç±»åé¦ˆæ¥è‡ªäºæ—¥å¸¸ç”Ÿæ´»ä¸­çš„ç›´æ¥æŒ‡ä»¤å’Œæœ‰æ„æˆ–æ— æ„çš„è°ƒæ•´ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒéªŒè¯äº†ICLHFåœ¨ç”Ÿæˆä»»åŠ¡è®¡åˆ’å’Œæƒè¡¡ç‰©ç†çº¦æŸä¸åå¥½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨æ‰§è¡Œä»»åŠ¡æ—¶å¦‚ä½•æœ‰æ•ˆç»“åˆäººç±»åå¥½ä¸ç‰©ç†çº¦æŸçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»…å…³æ³¨ä»»åŠ¡å®Œæˆï¼Œè€Œå¿½è§†äº†äººç±»çš„å®é™…éœ€æ±‚å’Œç¯å¢ƒé™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥äººç±»åé¦ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„åé¦ˆï¼Œæ¥æŒ‡å¯¼æœºå™¨äººåœ¨æ‰§è¡Œä»»åŠ¡æ—¶è€ƒè™‘äººç±»çš„åå¥½å’Œç‰©ç†çº¦æŸã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æœºå™¨äººçš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šäººç±»åé¦ˆæ”¶é›†æ¨¡å—ã€ä»»åŠ¡ç”Ÿæˆæ¨¡å—å’Œçº¦æŸå¹³è¡¡æ¨¡å—ã€‚äººç±»åé¦ˆé€šè¿‡ç›´æ¥æŒ‡ä»¤å’Œæ—¥å¸¸è°ƒæ•´æ”¶é›†ï¼Œéšåç”¨äºç”Ÿæˆä»»åŠ¡è®¡åˆ’ï¼Œå¹¶åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å¹³è¡¡ç‰©ç†çº¦æŸä¸åå¥½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åŸºäºäººç±»åé¦ˆçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLHFï¼‰ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­å®æ—¶è°ƒæ•´ä»»åŠ¡æ‰§è¡Œç­–ç•¥ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ä»»åŠ¡æ‰§è¡Œæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒICLHFæ¡†æ¶é‡‡ç”¨äº†å¤šå±‚æ¬¡çš„åé¦ˆæœºåˆ¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†åå¥½ä¸çº¦æŸçš„æƒé‡å¹³è¡¡ï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†æ·±åº¦å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„ä»»åŠ¡ç”Ÿæˆä¸æ‰§è¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ICLHFæ¡†æ¶çš„æœºå™¨äººåœ¨ä»»åŠ¡è®¡åˆ’ç”Ÿæˆæ•ˆç‡ä¸Šæé«˜äº†çº¦30%ï¼Œå¹¶ä¸”åœ¨å¹³è¡¡äººç±»åå¥½ä¸ç‰©ç†çº¦æŸæ–¹é¢çš„æˆåŠŸç‡è¾¾åˆ°äº†85%ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›å’Œä»»åŠ¡æ‰§è¡Œæ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—è¾…åŠ©æœºå™¨äººä»¥åŠæ™ºèƒ½åˆ¶é€ ç­‰åœºæ™¯ã€‚é€šè¿‡æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨äººç±»åå¥½ï¼Œæœºå™¨äººèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æä¾›æ›´ä¸ºä¸ªæ€§åŒ–å’Œé«˜æ•ˆçš„æœåŠ¡ï¼Œæå‡äººæœºäº¤äº’çš„è´¨é‡ä¸æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›æ¨åŠ¨äººæœºåä½œçš„è¿›ä¸€æ­¥å‘å±•ï¼Œä¿ƒè¿›æ™ºèƒ½æœºå™¨äººåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Equipped with Large Language Models (LLMs), human-centered robots are now capable of performing a wide range of tasks that were previously deemed challenging or unattainable. However, merely completing tasks is insufficient for cognitive robots, who should learn and apply human preferences to future scenarios. In this work, we propose a framework that combines human preferences with physical constraints, requiring robots to complete tasks while considering both. Firstly, we developed a benchmark of everyday household activities, which are often evaluated based on specific preferences. We then introduced In-Context Learning from Human Feedback (ICLHF), where human feedback comes from direct instructions and adjustments made intentionally or unintentionally in daily life. Extensive sets of experiments, testing the ICLHF to generate task plans and balance physical constraints with preferences, have demonstrated the efficiency of our approach. Project page: https://iclhf.github.io .

