---
layout: default
title: MolmoAct: Action Reasoning Models that can Reason in Space
---

# MolmoAct: Action Reasoning Models that can Reason in Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07917" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.07917v4</a>
  <a href="https://arxiv.org/pdf/2508.07917.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07917v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07917v4', 'MolmoAct: Action Reasoning Models that can Reason in Space')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-11 (Êõ¥Êñ∞: 2025-09-18)

**Â§áÊ≥®**: Updated GR00T result to N1.5

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MolmoAct‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫Ë°åÂä®Êé®ÁêÜ‰∏çË∂≥ÁöÑÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êé®ÁêÜ` `Ê∑±Â∫¶Â≠¶‰π†` `Á©∫Èó¥ËßÑÂàí` `Â§öÊ®°ÊÄÅÊÑüÁü•` `ÂºÄÊîæÊåá‰ª§Ë∑üÈöè`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫Ê®°ÂûãÂú®ÊÑüÁü•‰∏éÊéßÂà∂‰πãÈó¥Áº∫‰πèÊúâÊïàÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂØºËá¥ÈÄÇÂ∫îÊÄßÂíåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇ
2. MolmoActÈÄöËøá‰∏âÈò∂ÊÆµÁÆ°ÈÅìÈõÜÊàêÊÑüÁü•„ÄÅËßÑÂàíÂíåÊéßÂà∂ÔºåÊèêÂçá‰∫ÜÊú∫Âô®‰∫∫Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ
3. Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåMolmoActÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈïøÊó∂Èó¥‰ªªÂä°ÂíåÂºÄÊîæÊåá‰ª§Ë∑üÈöèÊñπÈù¢„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êé®ÁêÜÊòØÊúâÁõÆÁöÑË°åÂä®ÁöÑÊ†∏ÂøÉÔºå‰ΩÜÂ§ßÂ§öÊï∞Êú∫Âô®‰∫∫Âü∫Á°ÄÊ®°ÂûãÁõ¥Êé•Â∞ÜÊÑüÁü•ÂíåÊåá‰ª§Êò†Â∞ÑÂà∞ÊéßÂà∂ÔºåËøôÈôêÂà∂‰∫ÜÈÄÇÂ∫îÊÄß„ÄÅÊ≥õÂåñËÉΩÂäõÂíåËØ≠‰πâÂü∫Á°Ä„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜË°åÂä®Êé®ÁêÜÊ®°ÂûãÔºàARMsÔºâÔºåÈÄöËøáÁªìÊûÑÂåñÁöÑ‰∏âÈò∂ÊÆµÁÆ°ÈÅìÈõÜÊàêÊÑüÁü•„ÄÅËßÑÂàíÂíåÊéßÂà∂„ÄÇMolmoActÂ∞ÜËßÇÂØüÂíåÊåá‰ª§ÁºñÁ†Å‰∏∫Ê∑±Â∫¶ÊÑüÁü•Ê†áËÆ∞ÔºåÁîüÊàêÂèØÁºñËæëÁöÑ‰∏≠Á∫ßÁ©∫Èó¥ËÆ°ÂàíÔºåÂπ∂È¢ÑÊµãÁ≤æÁ°ÆÁöÑ‰ΩéÁ∫ßÂä®‰ΩúÔºå‰ªéËÄåÂÆûÁé∞ÂèØËß£ÈáäÂíåÂèØÂºïÂØºÁöÑË°å‰∏∫„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMolmoActÂú®Â§ö‰∏™‰ªªÂä°‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÂü∫Á∫øÔºåÂπ∂È¶ñÊ¨°ÂèëÂ∏É‰∫ÜÂåÖÂê´Ë∂ÖËøá10,000Êù°È´òË¥®ÈáèÊú∫Âô®‰∫∫ËΩ®ËøπÁöÑMolmoActÊï∞ÊçÆÈõÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊú∫Âô®‰∫∫Âü∫Á°ÄÊ®°ÂûãÂú®ÊÑüÁü•‰∏éÊéßÂà∂‰πãÈó¥Áº∫‰πèÊúâÊïàÊé®ÁêÜÁöÑÈóÆÈ¢ò„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏Áõ¥Êé•Â∞ÜÊÑüÁü•‰ø°ÊÅØÂíåÊåá‰ª§Êò†Â∞ÑÂà∞ÊéßÂà∂Êåá‰ª§ÔºåÂØºËá¥ÈÄÇÂ∫îÊÄßÂíåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÊó†Ê≥ïÂ§ÑÁêÜÂ§çÊùÇÁöÑ‰ªªÂä°Âú∫ÊôØ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMolmoActÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁªìÊûÑÂåñÁöÑ‰∏âÈò∂ÊÆµÁÆ°ÈÅìÔºåÂ∞ÜÊÑüÁü•„ÄÅËßÑÂàíÂíåÊéßÂà∂ÊúâÊïàÁªìÂêà„ÄÇËØ•Ê®°ÂûãÈ¶ñÂÖàÂ∞ÜËßÇÂØüÂíåÊåá‰ª§ÁºñÁ†Å‰∏∫Ê∑±Â∫¶ÊÑüÁü•Ê†áËÆ∞ÔºåÁÑ∂ÂêéÁîüÊàêÂèØÁºñËæëÁöÑ‰∏≠Á∫ßÁ©∫Èó¥ËÆ°ÂàíÔºåÊúÄÂêéÈ¢ÑÊµãÁ≤æÁ°ÆÁöÑ‰ΩéÁ∫ßÂä®‰ΩúÔºå‰ªéËÄåÂÆûÁé∞ÂèØËß£ÈáäÂíåÂèØÂºïÂØºÁöÑË°å‰∏∫„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMolmoActÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊ∑±Â∫¶ÊÑüÁü•Ê†áËÆ∞ÁîüÊàêÊ®°Âùó„ÄÅ‰∏≠Á∫ßÁ©∫Èó¥ËÆ°ÂàíÁîüÊàêÊ®°ÂùóÂíå‰ΩéÁ∫ßÂä®‰ΩúÈ¢ÑÊµãÊ®°Âùó„ÄÇÊØè‰∏™Ê®°ÂùóÂú®Â§ÑÁêÜ‰ø°ÊÅØÊó∂ÈÉΩËÄÉËôë‰∫ÜÁ©∫Èó¥ÂíåÊó∂Èó¥ÁöÑ‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMolmoActÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂‰∏âÈò∂ÊÆµÁöÑÁªìÊûÑÂåñÊé®ÁêÜËøáÁ®ãÔºå‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ËøõË°åÊúâÊïàÁöÑÁ©∫Èó¥Êé®ÁêÜÂíåÂÜ≥Á≠ñ„ÄÇËøô‰∏ÄËÆæËÆ°‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÁõ¥Êé•Êò†Â∞ÑÊñπÂºèÂΩ¢Êàê‰∫ÜÈ≤úÊòéÂØπÊØîÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÈÄÇÂ∫îÊÄßÂíåËß£ÈáäËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåMolmoActÈááÁî®‰∫ÜÊ∑±Â∫¶ÊÑüÁü•Ê†áËÆ∞Êù•ÊçïÊçâÁéØÂ¢É‰ø°ÊÅØÔºåÂπ∂‰ΩøÁî®ÂèØÁºñËæëÁöÑËΩ®ËøπÁîüÊàê‰∏≠Á∫ßÁ©∫Èó¥ËÆ°Âàí„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂºïÂÖ•‰∫ÜÂ§öÁßçÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•‰ºòÂåñ‰∏çÂêåÈò∂ÊÆµÁöÑËæìÂá∫ÔºåÁ°Æ‰øùÊúÄÁªàÂä®‰ΩúÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÊéßÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MolmoActÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºöÂú®SimperEnvËßÜËßâÂåπÈÖç‰ªªÂä°‰∏≠ÂÆûÁé∞70.5%ÁöÑÈõ∂-shotÂáÜÁ°ÆÁéáÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÈó≠Ê∫êÊ®°ÂûãÔºõÂú®LIBERO‰ªªÂä°‰∏≠Âπ≥ÂùáÊàêÂäüÁéáËææÂà∞86.6%ÔºåÊØîThinkActÊèêÂçá6.3%ÔºõÂú®ÁúüÂÆû‰∏ñÁïåÂæÆË∞É‰∏≠ÔºåÂçïËáÇ‰ªªÂä°ËøõÂ±ïÊèêÂçá10%ÔºåÂèåËáÇ‰ªªÂä°ËøõÂ±ïÊèêÂçá22.7%„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãÂú®ÂºÄÊîæÊåá‰ª§Ë∑üÈöèÂíåËΩ®ËøπÂºïÂØºÊñπÈù¢Ëé∑Âæó‰∫ÜÊúÄÈ´òÁöÑ‰∫∫Á±ªÂÅèÂ•ΩËØÑÂàÜ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MolmoActÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂåÖÊã¨ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂà∂ÈÄ†Á≠â„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõÔºåËØ•Ê®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊâßË°å‰ªªÂä°Âπ∂ÈÄÇÂ∫îÂä®ÊÄÅÂèòÂåñÁöÑÂú∫ÊôØÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ïÂíåÊôÆÂèä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of robotic foundation models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1.5; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset -- a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: https://allenai.org/blog/molmoact

