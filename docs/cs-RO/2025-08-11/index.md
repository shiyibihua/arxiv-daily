---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-08-11
---

# cs.ROï¼ˆ2025-08-11ï¼‰

ğŸ“Š å…± **18** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (13 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250808240v1-odyssey-open-world-quadrupeds-exploration-and-manipulation-for-long-.html">ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks</a></td>
  <td>æå‡ºODYSSEYæ¡†æ¶ä»¥è§£å†³é•¿æ—¶é—´ç§»åŠ¨æ“æ§æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">whole-body control</span> <span class="paper-tag">locomotion</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08240v1" data-paper-url="./papers/250808240v1-odyssey-open-world-quadrupeds-exploration-and-manipulation-for-long-.html" onclick="toggleFavorite(this, '2508.08240v1', 'ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250807770v2-agentworld-an-interactive-simulation-platform-for-scene-construction.html">AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation</a></td>
  <td>æå‡ºAgentWorldä»¥è§£å†³å®¶åº­ç§»åŠ¨æ“æ§èƒ½åŠ›çš„è®­ç»ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid locomotion</span> <span class="paper-tag">locomotion</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07770v2" data-paper-url="./papers/250807770v2-agentworld-an-interactive-simulation-platform-for-scene-construction.html" onclick="toggleFavorite(this, '2508.07770v2', 'AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250807611v1-end-to-end-humanoid-robot-safe-and-comfortable-locomotion-policy.html">End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</a></td>
  <td>æå‡ºç«¯åˆ°ç«¯çš„äººå½¢æœºå™¨äººå®‰å…¨èˆ’é€‚çš„è¿åŠ¨ç­–ç•¥ä»¥è§£å†³å¤æ‚ç¯å¢ƒå¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07611v1" data-paper-url="./papers/250807611v1-end-to-end-humanoid-robot-safe-and-comfortable-locomotion-policy.html" onclick="toggleFavorite(this, '2508.07611v1', 'End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250807650v2-graphcot-vla-a-3d-spatial-aware-reasoning-vision-language-action-mod.html">GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions</a></td>
  <td>æå‡ºGraphCoT-VLAä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ¨¡ç³ŠæŒ‡ä»¤é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07650v2" data-paper-url="./papers/250807650v2-graphcot-vla-a-3d-spatial-aware-reasoning-vision-language-action-mod.html" onclick="toggleFavorite(this, '2508.07650v2', 'GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250808241v4-beyondmimic-from-motion-tracking-to-versatile-humanoid-control-via-g.html">BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion</a></td>
  <td>æå‡ºBeyondMimicæ¡†æ¶ä»¥è§£å†³äººå½¢æœºå™¨äººè¿åŠ¨æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">humanoid control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08241v4" data-paper-url="./papers/250808241v4-beyondmimic-from-motion-tracking-to-versatile-humanoid-control-via-g.html" onclick="toggleFavorite(this, '2508.08241v4', 'BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250807689v1-lauron-vi-a-six-legged-robot-for-dynamic-walking.html">LAURON VI: A Six-Legged Robot for Dynamic Walking</a></td>
  <td>æå‡ºLAURON VIä»¥è§£å†³å…­è¶³æœºå™¨äººåŠ¨æ€è¡Œèµ°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span> <span class="paper-tag">legged locomotion</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07689v1" data-paper-url="./papers/250807689v1-lauron-vi-a-six-legged-robot-for-dynamic-walking.html" onclick="toggleFavorite(this, '2508.07689v1', 'LAURON VI: A Six-Legged Robot for Dynamic Walking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250807917v4-molmoact-action-reasoning-models-that-can-reason-in-space.html">MolmoAct: Action Reasoning Models that can Reason in Space</a></td>
  <td>æå‡ºMolmoActä»¥è§£å†³æœºå™¨äººè¡ŒåŠ¨æ¨ç†ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">bi-manual</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07917v4" data-paper-url="./papers/250807917v4-molmoact-action-reasoning-models-that-can-reason-in-space.html" onclick="toggleFavorite(this, '2508.07917v4', 'MolmoAct: Action Reasoning Models that can Reason in Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250808113v1-aimbot-a-simple-auxiliary-visual-cue-to-enhance-spatial-awareness-of.html">AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies</a></td>
  <td>æå‡ºAimBotä»¥å¢å¼ºæœºå™¨äººæ“ä½œä¸­çš„ç©ºé—´æ„è¯†</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08113v1" data-paper-url="./papers/250808113v1-aimbot-a-simple-auxiliary-visual-cue-to-enhance-spatial-awareness-of.html" onclick="toggleFavorite(this, '2508.08113v1', 'AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250808108v1-capsizing-guided-trajectory-optimization-for-autonomous-navigation-w.html">Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain</a></td>
  <td>æå‡ºä¸€ç§å€¾è¦†å¼•å¯¼çš„è½¨è¿¹ä¼˜åŒ–æ–¹æ³•ä»¥è§£å†³ç²—ç³™åœ°å½¢å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">trajectory optimization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08108v1" data-paper-url="./papers/250808108v1-capsizing-guided-trajectory-optimization-for-autonomous-navigation-w.html" onclick="toggleFavorite(this, '2508.08108v1', 'Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250807758v1-robot-and-overhead-crane-collaboration-scheme-to-enhance-payload-man.html">Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation</a></td>
  <td>æå‡ºæœºå™¨äººä¸åŠè½¦åä½œæ–¹æ¡ˆä»¥æå‡è´Ÿè½½æ“æ§ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07758v1" data-paper-url="./papers/250807758v1-robot-and-overhead-crane-collaboration-scheme-to-enhance-payload-man.html" onclick="toggleFavorite(this, '2508.07758v1', 'Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250807686v2-risk-map-as-middleware-towards-interpretable-cooperative-end-to-end-.html">Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning</a></td>
  <td>æå‡ºé£é™©åœ°å›¾ä¸­ä»‹ä»¥è§£å†³è‡ªä¸»é©¾é©¶ä¸­çš„å¯è§£é‡Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07686v2" data-paper-url="./papers/250807686v2-risk-map-as-middleware-towards-interpretable-cooperative-end-to-end-.html" onclick="toggleFavorite(this, '2508.07686v2', 'Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250808144v1-component-aware-pruning-for-accelerated-control-tasks-in-latent-spac.html">COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models</a></td>
  <td>æå‡ºç»„ä»¶æ„ŸçŸ¥å‰ªæä»¥è§£å†³èµ„æºå—é™ç¯å¢ƒä¸­çš„æ§åˆ¶ä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08144v1" data-paper-url="./papers/250808144v1-component-aware-pruning-for-accelerated-control-tasks-in-latent-spac.html" onclick="toggleFavorite(this, '2508.08144v1', 'COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250807945v2-pchands-pca-based-hand-pose-synergy-representation-on-manipulators-w.html">PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF</a></td>
  <td>æå‡ºPCHandsä»¥è§£å†³ä¸åŒå½¢æ€æ“æ§å™¨çš„çµå·§æ“ä½œè¡¨ç¤ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07945v2" data-paper-url="./papers/250807945v2-pchands-pca-based-hand-pose-synergy-representation-on-manipulators-w.html" onclick="toggleFavorite(this, '2508.07945v2', 'PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250807839v2-touch-speaks-sound-feels-a-multimodal-approach-to-affective-and-soci.html">Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans</a></td>
  <td>æå‡ºå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿä»¥å¢å¼ºæœºå™¨äººæƒ…æ„Ÿä¼ è¾¾èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07839v2" data-paper-url="./papers/250807839v2-touch-speaks-sound-feels-a-multimodal-approach-to-affective-and-soci.html" onclick="toggleFavorite(this, '2508.07839v2', 'Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250807560v1-progressive-birds-eye-view-perception-for-safety-critical-autonomous.html">Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey</a></td>
  <td>æå‡ºæ¸è¿›å¼é¸Ÿç°è§†è§’æ„ŸçŸ¥ä»¥è§£å†³å®‰å…¨å…³é”®çš„è‡ªåŠ¨é©¾é©¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07560v1" data-paper-url="./papers/250807560v1-progressive-birds-eye-view-perception-for-safety-critical-autonomous.html" onclick="toggleFavorite(this, '2508.07560v1', 'Progressive Bird&#39;s Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250807606v2-in-situ-value-aligned-human-robot-interactions-with-physical-constra.html">In-situ Value-aligned Human-Robot Interactions with Physical Constraints</a></td>
  <td>æå‡ºç»“åˆäººç±»åå¥½ä¸ç‰©ç†çº¦æŸçš„æœºå™¨äººäº¤äº’æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07606v2" data-paper-url="./papers/250807606v2-in-situ-value-aligned-human-robot-interactions-with-physical-constra.html" onclick="toggleFavorite(this, '2508.07606v2', 'In-situ Value-aligned Human-Robot Interactions with Physical Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250807885v1-autonomous-navigation-of-cloud-controlled-quadcopters-in-confined-sp.html">Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning</a></td>
  <td>æå‡ºäº‘æ§åˆ¶å››æ—‹ç¿¼è‡ªä¸»å¯¼èˆªç³»ç»Ÿä»¥è§£å†³GPSç¼ºå¤±ç¯å¢ƒä¸­çš„å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07885v1" data-paper-url="./papers/250807885v1-autonomous-navigation-of-cloud-controlled-quadcopters-in-confined-sp.html" onclick="toggleFavorite(this, '2508.07885v1', 'Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250807842v2-detach-cross-domain-learning-for-long-horizon-tasks-via-mixture-of-d.html">DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts</a></td>
  <td>æå‡ºDETACHä»¥è§£å†³é•¿æ—¶é—´ä»»åŠ¡è·¨åŸŸå­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">human-scene interaction</span> <span class="paper-tag">HSI</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07842v2" data-paper-url="./papers/250807842v2-detach-cross-domain-learning-for-long-horizon-tasks-via-mixture-of-d.html" onclick="toggleFavorite(this, '2508.07842v2', 'DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)