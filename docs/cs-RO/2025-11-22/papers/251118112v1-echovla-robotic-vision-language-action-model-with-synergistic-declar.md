---
layout: default
title: EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation
---

# EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation

**arXiv**: [2511.18112v1](https://arxiv.org/abs/2511.18112) | [PDF](https://arxiv.org/pdf/2511.18112.pdf)

**ä½œè€…**: Min Lin, Xiwen Liang, Bingqian Lin, Liu Jingzhi, Zijian Jiao, Kehan Li, Yuhan Ma, Yuecheng Liu, Shen Zhao, Yuzheng Zhuang, Xiaodan Liang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-22

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EchoVLAï¼šé¢å‘ç§»åŠ¨æ“ä½œçš„ååŒå£°æ˜Žå¼è®°å¿†æœºå™¨äººè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ç§»åŠ¨æ“ä½œ` `é•¿æ—¶ç¨‹ä»»åŠ¡` `å£°æ˜Žå¼è®°å¿†` `æœºå™¨äºº` `æ‰©æ•£ç­–ç•¥` `å¤šæ¨¡æ€å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åœ¨é•¿æ—¶ç¨‹ç§»åŠ¨æ“ä½œä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹åœ¨åŠ¨æ€çŽ¯å¢ƒä¸­åè°ƒå¯¼èˆªå’Œæ“ä½œæ‰€éœ€çš„è®°å¿†ä¸ŽæŽ¨ç†èƒ½åŠ›ã€‚
2. EchoVLAé€šè¿‡å¼•å…¥ååŒå£°æ˜Žå¼è®°å¿†ï¼ŒåŒ…æ‹¬åœºæ™¯è®°å¿†å’Œæƒ…èŠ‚è®°å¿†ï¼Œå¢žå¼ºäº†æ¨¡åž‹å¯¹çŽ¯å¢ƒå’Œä»»åŠ¡åŽ†å²çš„ç†è§£å’Œåˆ©ç”¨ã€‚
3. MoManiåŸºå‡†æµ‹è¯•å’Œå®žéªŒç»“æžœè¡¨æ˜Žï¼ŒEchoVLAåœ¨é•¿æ—¶ç¨‹ç§»åŠ¨æ“ä½œä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŽ°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹ä¸»è¦å±€é™äºŽçŸ­æ—¶ç¨‹ã€æ¡Œé¢æ“ä½œï¼Œç¼ºä¹é•¿æ—¶ç¨‹ç§»åŠ¨æ“ä½œæ‰€éœ€çš„è®°å¿†å’ŒæŽ¨ç†èƒ½åŠ›ï¼Œè€Œé•¿æ—¶ç¨‹ç§»åŠ¨æ“ä½œéœ€è¦æ™ºèƒ½ä½“åœ¨ä¸æ–­å˜åŒ–çš„ç©ºé—´çŽ¯å¢ƒä¸­åè°ƒå¯¼èˆªå’Œæ“ä½œã€‚æœ¬æ–‡æå‡ºäº†EchoVLAï¼Œä¸€ç§ç”¨äºŽé•¿æ—¶ç¨‹ç§»åŠ¨æ“ä½œçš„ã€å…·æœ‰è®°å¿†æ„ŸçŸ¥èƒ½åŠ›çš„VLAæ¨¡åž‹ã€‚EchoVLAå—åˆ°äººè„‘çš„å¯å‘ï¼Œèžåˆäº†ååŒå£°æ˜Žå¼è®°å¿†ï¼ŒåŒ…å«ç»´æŠ¤ç©ºé—´è¯­ä¹‰åœ°å›¾çš„åœºæ™¯è®°å¿†å’Œå­˜å‚¨å…·æœ‰å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ç‰¹å¾çš„ä»»åŠ¡çº§ç»éªŒçš„æƒ…èŠ‚è®°å¿†ã€‚åœ¨è®­ç»ƒå’ŒæŽ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¿™ä¸¤ä¸ªè®°å¿†åº“åŸºäºŽå½“å‰çš„è§‚å¯Ÿã€ä»»åŠ¡åŽ†å²å’ŒæŒ‡ä»¤è¿›è¡Œç‹¬ç«‹å­˜å‚¨ã€æ›´æ–°å’Œæ£€ç´¢ï¼Œå¹¶ä¸”å®ƒä»¬æ£€ç´¢åˆ°çš„è¡¨ç¤ºé€šè¿‡ç²—ç²’åº¦å’Œç»†ç²’åº¦çš„æ³¨æ„åŠ›æœºåˆ¶èžåˆï¼Œä»¥æŒ‡å¯¼ç§»åŠ¨æœºæ¢°è‡‚æ‰©æ•£ç­–ç•¥ã€‚ä¸ºäº†æ”¯æŒå¤§è§„æ¨¡è®­ç»ƒå’Œè¯„ä¼°ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†MoManiï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMï¼‰å¼•å¯¼çš„è§„åˆ’å’Œåé¦ˆé©±åŠ¨çš„ç»†åŒ–ï¼Œå¹¶è¾…ä»¥çœŸå®žæœºå™¨äººæ¼”ç¤ºï¼Œç”Ÿæˆä¸“å®¶çº§çš„é•¿æ—¶ç¨‹è½¨è¿¹ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žçŽ¯å¢ƒä¸­çš„å®žéªŒè¡¨æ˜Žï¼ŒEchoVLAæé«˜äº†é•¿æ—¶ç¨‹æ€§èƒ½ï¼Œåœ¨æ“ä½œ/å¯¼èˆªä»»åŠ¡ä¸Šè¾¾åˆ°äº†0.52çš„æˆåŠŸçŽ‡ï¼ˆSRï¼‰ï¼Œåœ¨ç§»åŠ¨æ“ä½œä»»åŠ¡ä¸Šè¾¾åˆ°äº†0.31çš„æˆåŠŸçŽ‡ï¼Œåˆ†åˆ«è¶…è¿‡äº†$Ï€_{0.5}$åŸºçº¿+0.08å’Œ+0.11ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹åœ¨é•¿æ—¶ç¨‹ç§»åŠ¨æ“ä½œä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ï¼Œä¸»è¦åŽŸå› æ˜¯å®ƒä»¬ç¼ºä¹è¶³å¤Ÿçš„è®°å¿†èƒ½åŠ›å’ŒæŽ¨ç†èƒ½åŠ›ï¼Œæ— æ³•æœ‰æ•ˆåœ°å¤„ç†å¤æ‚ã€åŠ¨æ€çš„çŽ¯å¢ƒå˜åŒ–ï¼Œä»¥åŠé•¿æ—¶é—´è·¨åº¦çš„ä»»åŠ¡ä¾èµ–å…³ç³»ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨çŸ­æ—¶ç¨‹çš„æ¡Œé¢æ“ä½œï¼Œå¿½ç•¥äº†ç§»åŠ¨æ“ä½œä¸­å¯¼èˆªå’Œæ“ä½œçš„ååŒï¼Œä»¥åŠçŽ¯å¢ƒä¸Šä¸‹æ–‡ä¿¡æ¯çš„é‡è¦æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEchoVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡ä»¿äººè„‘çš„è®°å¿†æœºåˆ¶ï¼Œå¼•å…¥ååŒå£°æ˜Žå¼è®°å¿†ï¼ŒåŒ…æ‹¬åœºæ™¯è®°å¿†å’Œæƒ…èŠ‚è®°å¿†ã€‚åœºæ™¯è®°å¿†ç”¨äºŽç»´æŠ¤çŽ¯å¢ƒçš„ç©ºé—´è¯­ä¹‰åœ°å›¾ï¼Œæƒ…èŠ‚è®°å¿†ç”¨äºŽå­˜å‚¨ä»»åŠ¡çº§åˆ«çš„ç»éªŒï¼Œå¹¶ç»“åˆå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ç‰¹å¾ã€‚é€šè¿‡è¿™ä¸¤ä¸ªè®°å¿†æ¨¡å—çš„ååŒå·¥ä½œï¼Œæ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å½“å‰çŽ¯å¢ƒï¼Œå›žå¿†åŽ†å²ç»éªŒï¼Œä»Žè€Œåšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šEchoVLAçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬è§†è§‰æ„ŸçŸ¥æ¨¡å—ã€è¯­è¨€ç†è§£æ¨¡å—ã€åœºæ™¯è®°å¿†æ¨¡å—ã€æƒ…èŠ‚è®°å¿†æ¨¡å—ã€ç­–ç•¥ç”Ÿæˆæ¨¡å—ã€‚è§†è§‰æ„ŸçŸ¥æ¨¡å—è´Ÿè´£ä»Žå›¾åƒä¸­æå–è§†è§‰ç‰¹å¾ï¼Œè¯­è¨€ç†è§£æ¨¡å—è´Ÿè´£è§£æžæŒ‡ä»¤ï¼Œåœºæ™¯è®°å¿†æ¨¡å—å­˜å‚¨å’Œæ›´æ–°çŽ¯å¢ƒçš„ç©ºé—´è¯­ä¹‰åœ°å›¾ï¼Œæƒ…èŠ‚è®°å¿†æ¨¡å—å­˜å‚¨å’Œæ›´æ–°ä»»åŠ¡çº§åˆ«çš„ç»éªŒï¼Œç­–ç•¥ç”Ÿæˆæ¨¡å—æ ¹æ®è§†è§‰ç‰¹å¾ã€è¯­è¨€æŒ‡ä»¤ä»¥åŠä»Žåœºæ™¯è®°å¿†å’Œæƒ…èŠ‚è®°å¿†ä¸­æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œç”Ÿæˆç§»åŠ¨æœºæ¢°è‡‚çš„åŠ¨ä½œç­–ç•¥ã€‚æ¨¡åž‹é‡‡ç”¨æ‰©æ•£ç­–ç•¥ï¼Œé€šè¿‡å­¦ä¹ å™ªå£°åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼Œç”Ÿæˆå¹³æ»‘è‡ªç„¶çš„åŠ¨ä½œåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šEchoVLAçš„å…³é”®åˆ›æ–°åœ¨äºŽå¼•å…¥äº†ååŒå£°æ˜Žå¼è®°å¿†ï¼Œå¹¶è®¾è®¡äº†ç²—ç²’åº¦å’Œç»†ç²’åº¦çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºŽèžåˆä»Žåœºæ™¯è®°å¿†å’Œæƒ…èŠ‚è®°å¿†ä¸­æ£€ç´¢åˆ°çš„ä¿¡æ¯ã€‚è¿™ç§ååŒè®°å¿†æœºåˆ¶ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£çŽ¯å¢ƒä¸Šä¸‹æ–‡ï¼Œå›žå¿†åŽ†å²ç»éªŒï¼Œä»Žè€Œåšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚æ­¤å¤–ï¼ŒMoManiåŸºå‡†æµ‹è¯•çš„æå‡ºä¹Ÿä¸ºé•¿æ—¶ç¨‹ç§»åŠ¨æ“ä½œä»»åŠ¡çš„ç ”ç©¶æä¾›äº†æ–°çš„è¯„ä¼°å¹³å°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœºæ™¯è®°å¿†é‡‡ç”¨ç©ºé—´è¯­ä¹‰åœ°å›¾çš„å½¢å¼ï¼Œä½¿ç”¨SLAMæŠ€æœ¯æž„å»ºå’Œæ›´æ–°ã€‚æƒ…èŠ‚è®°å¿†ä½¿ç”¨Transformerç»“æž„å­˜å‚¨ä»»åŠ¡çº§åˆ«çš„ç»éªŒï¼Œå¹¶ç»“åˆå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œå¦‚è§†è§‰ç‰¹å¾ã€è¯­è¨€æŒ‡ä»¤å’ŒåŠ¨ä½œåºåˆ—ã€‚ç²—ç²’åº¦æ³¨æ„åŠ›æœºåˆ¶ç”¨äºŽé€‰æ‹©é‡è¦çš„è®°å¿†ç‰‡æ®µï¼Œç»†ç²’åº¦æ³¨æ„åŠ›æœºåˆ¶ç”¨äºŽèžåˆä¸åŒè®°å¿†ç‰‡æ®µçš„ä¿¡æ¯ã€‚æ‰©æ•£ç­–ç•¥é‡‡ç”¨U-Netç»“æž„ï¼Œå­¦ä¹ å™ªå£°åˆ°åŠ¨ä½œçš„æ˜ å°„ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ç­–ç•¥æŸå¤±ã€è®°å¿†æŸå¤±å’Œæ³¨æ„åŠ›æŸå¤±ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

EchoVLAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žçŽ¯å¢ƒä¸­çš„å®žéªŒç»“æžœè¡¨æ˜Žï¼Œå…¶åœ¨é•¿æ—¶ç¨‹ç§»åŠ¨æ“ä½œä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ“ä½œ/å¯¼èˆªä»»åŠ¡ä¸Šï¼ŒEchoVLAè¾¾åˆ°äº†0.52çš„æˆåŠŸçŽ‡ï¼ˆSRï¼‰ï¼Œåœ¨ç§»åŠ¨æ“ä½œä»»åŠ¡ä¸Šè¾¾åˆ°äº†0.31çš„æˆåŠŸçŽ‡ï¼Œåˆ†åˆ«è¶…è¿‡äº†$Ï€_{0.5}$åŸºçº¿+0.08å’Œ+0.11ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒEchoVLAçš„ååŒå£°æ˜Žå¼è®°å¿†æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æœºå™¨äººçš„è®°å¿†èƒ½åŠ›å’ŒæŽ¨ç†èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

EchoVLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨å®¶åº­æœåŠ¡æœºå™¨äººã€ä»“å‚¨ç‰©æµæœºå™¨äººã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­å®Œæˆé•¿æ—¶ç¨‹çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å¯¼èˆªåˆ°æŒ‡å®šåœ°ç‚¹ã€æ‹¾å–ç‰©å“ã€æ”¾ç½®ç‰©å“ç­‰ã€‚é€šè¿‡ä¸æ–­å­¦ä¹ å’Œç§¯ç´¯ç»éªŒï¼ŒEchoVLAå¯ä»¥é€‚åº”ä¸åŒçš„çŽ¯å¢ƒå’Œä»»åŠ¡ï¼Œæé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $Ï€_{0.5}$ by +0.08 and +0.11.

