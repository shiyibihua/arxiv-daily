---
layout: default
title: URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model
---

# URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.00940" target="_blank" class="toolbar-btn">arXiv: 2511.00940v1</a>
    <a href="https://arxiv.org/pdf/2511.00940.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00940v1" 
            onclick="toggleFavorite(this, '2511.00940v1', 'URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhe Li, Xiang Bai, Jieyu Zhang, Zhuangzhe Wu, Che Xu, Ying Li, Chengkai Hou, Shanghang Zhang

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-02

**Â§áÊ≥®**: Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**URDF-AnythingÔºöÂü∫‰∫é3DÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÊûÑÂª∫ÂèØÂä®ÂØπË±°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÂèØÂä®ÂØπË±°` `Êï∞Â≠óÂ≠™Áîü` `3DÂ§öÊ®°ÊÄÅLLM` `ÁÇπ‰∫ëÂ§ÑÁêÜ` `ËøêÂä®Â≠¶ÂèÇÊï∞‰º∞ËÆ°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÊûÑÂª∫ÂèØÂä®ÂØπË±°Êï∞Â≠óÂ≠™ÁîüÊó∂ÔºåÈúÄË¶ÅËÄóÊó∂ÁöÑ‰∫∫Â∑•Âª∫Ê®°ÊàñÂ§çÊùÇÁöÑÂ§öÈò∂ÊÆµÊµÅÁ®ãÔºåÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. URDF-AnythingÂà©Áî®3DÂ§öÊ®°ÊÄÅLLMÔºåÈÄöËøáÁÇπ‰∫ëÂíåÊñáÊú¨ËæìÂÖ•ÔºåËá™ÂõûÂΩíÈ¢ÑÊµãÂá†‰ΩïÂàÜÂâ≤ÂíåËøêÂä®Â≠¶ÂèÇÊï∞ÔºåÂÆûÁé∞Á´ØÂà∞Á´ØËá™Âä®ÈáçÂª∫„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåURDF-AnythingÂú®Âá†‰ΩïÂàÜÂâ≤„ÄÅËøêÂä®Â≠¶ÂèÇÊï∞È¢ÑÊµãÂíåÁâ©ÁêÜÂèØÊâßË°åÊÄßÊñπÈù¢ÂùáÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫URDF-AnythingÔºå‰∏Ä‰∏™Âü∫‰∫é3DÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLM)ÁöÑÁ´ØÂà∞Á´ØËá™Âä®ÈáçÂª∫Ê°ÜÊû∂ÔºåÁî®‰∫éÊûÑÂª∫ÂèØÂä®ÂØπË±°ÁöÑÁ≤æÁ°ÆÊï∞Â≠óÂ≠™Áîü„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®Âü∫‰∫éÁÇπ‰∫ëÂíåÊñáÊú¨Â§öÊ®°ÊÄÅËæìÂÖ•ÁöÑËá™ÂõûÂΩíÈ¢ÑÊµãÔºåËÅîÂêà‰ºòÂåñÂá†‰ΩïÂàÜÂâ≤ÂíåËøêÂä®Â≠¶ÂèÇÊï∞È¢ÑÊµã„ÄÇÂÆÉÂÆûÁé∞‰∫Ü‰∏ÄÁßçÁâπÊÆäÁöÑ$[SEG]$ tokenÊú∫Âà∂ÔºåÁõ¥Êé•‰∏éÁÇπ‰∫ëÁâπÂæÅ‰∫§‰∫íÔºåÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÈÉ®‰ª∂Á∫ßÂàÜÂâ≤ÔºåÂêåÊó∂‰øùÊåÅ‰∏éËøêÂä®Â≠¶ÂèÇÊï∞È¢ÑÊµãÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂú®Ê®°ÊãüÂíåÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Âá†‰ΩïÂàÜÂâ≤ÔºàmIoUÊèêÂçá17%Ôºâ„ÄÅËøêÂä®Â≠¶ÂèÇÊï∞È¢ÑÊµãÔºàÂπ≥ÂùáËØØÂ∑ÆÈôç‰Ωé29%ÔºâÂíåÁâ©ÁêÜÂèØÊâßË°åÊÄßÔºàË∂ÖËøáÂü∫Á∫ø50%ÔºâÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂç≥‰ΩøÂú®ËÆ≠ÁªÉÈõÜ‰πãÂ§ñÁöÑÂØπË±°‰∏ä‰πüËÉΩË°®Áé∞ËâØÂ•Ω„ÄÇËøôÈ°πÂ∑•‰Ωú‰∏∫Êú∫Âô®‰∫∫‰ªøÁúüÊûÑÂª∫Êï∞Â≠óÂ≠™ÁîüÊèê‰æõ‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÊòæËëóÂ¢ûÂº∫‰∫Üsim-to-realÁöÑËøÅÁßªËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂèØÂä®ÂØπË±°Êï∞Â≠óÂ≠™ÁîüÊûÑÂª∫ÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫é‰∫∫Â∑•Âª∫Ê®°ÊàñÂ§çÊùÇÁöÑÂ§öÈò∂ÊÆµÊµÅÁ®ãÔºåÊàêÊú¨È´òÊòÇ‰∏îÊïàÁéá‰Ωé‰∏ã„ÄÇËøô‰∫õÊñπÊ≥ïÈöæ‰ª•Ëá™Âä®„ÄÅÁ≤æÁ°ÆÂú∞ÈáçÂª∫ÂØπË±°ÁöÑÂá†‰ΩïÁªìÊûÑÂíåËøêÂä®Â≠¶ÂèÇÊï∞ÔºåÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫Âú®‰ªøÁúüÁéØÂ¢É‰∏≠ÁöÑËÆ≠ÁªÉÂíåÂÖ∑Ë∫´Êô∫ËÉΩ‰∏ñÁïåÊ®°ÂûãÁöÑÊûÑÂª∫„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®3DÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLM)ÔºåÂ∞ÜÂá†‰Ωï‰ø°ÊÅØÔºàÁÇπ‰∫ëÔºâÂíåËØ≠‰πâ‰ø°ÊÅØÔºàÊñáÊú¨ÊèèËø∞ÔºâËûçÂêàÔºåÈÄöËøáËá™ÂõûÂΩíÈ¢ÑÊµãÁöÑÊñπÂºèÔºåÂêåÊó∂‰ºòÂåñÂá†‰ΩïÂàÜÂâ≤ÂíåËøêÂä®Â≠¶ÂèÇÊï∞È¢ÑÊµã„ÄÇËøôÁßçÁ´ØÂà∞Á´ØÁöÑÊñπÊ≥ïÈÅøÂÖç‰∫Ü‰º†ÁªüÊñπÊ≥ï‰∏≠Â§çÊùÇÁöÑ‰∏≠Èó¥Ê≠•È™§ÔºåÊèêÈ´ò‰∫ÜÈáçÂª∫ÊïàÁéáÂíåÁ≤æÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöURDF-AnythingÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Â§öÊ®°ÊÄÅËæìÂÖ•Ê®°ÂùóÔºöÊé•Êî∂ÁÇπ‰∫ëÂíåÊñáÊú¨ÊèèËø∞‰Ωú‰∏∫ËæìÂÖ•Ôºõ2) ÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºöÊèêÂèñÁÇπ‰∫ëÂíåÊñáÊú¨ÁöÑÁâπÂæÅË°®Á§∫Ôºõ3) Ëá™ÂõûÂΩíÈ¢ÑÊµãÊ®°ÂùóÔºöÂü∫‰∫éÊèêÂèñÁöÑÁâπÂæÅÔºåËá™ÂõûÂΩíÂú∞È¢ÑÊµãÈÉ®‰ª∂ÂàÜÂâ≤ÁªìÊûúÂíåËøêÂä®Â≠¶ÂèÇÊï∞Ôºõ4) $[SEG]$ tokenÊú∫Âà∂ÔºöÈÄöËøáÁâπÊÆäÁöÑtoken‰∏éÁÇπ‰∫ëÁâπÂæÅ‰∫§‰∫íÔºåÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÈÉ®‰ª∂Á∫ßÂàÜÂâ≤„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÁ´ØÂà∞Á´ØÂèØËÆ≠ÁªÉÁöÑ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞Ü3DÂ§öÊ®°ÊÄÅLLMÂ∫îÁî®‰∫éÂèØÂä®ÂØπË±°ÁöÑÊï∞Â≠óÂ≠™ÁîüÊûÑÂª∫ÔºåÂπ∂ÊèêÂá∫‰∫Ü$[SEG]$ tokenÊú∫Âà∂„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÂá†‰ΩïÂíåËØ≠‰πâ‰ø°ÊÅØÔºåÂÆûÁé∞Êõ¥Á≤æÁ°ÆÁöÑÈÉ®‰ª∂ÂàÜÂâ≤ÂíåËøêÂä®Â≠¶ÂèÇÊï∞È¢ÑÊµã„ÄÇ$[SEG]$ tokenÊú∫Âà∂ÂÖÅËÆ∏Ê®°ÂûãÁõ¥Êé•Êìç‰ΩúÁÇπ‰∫ëÁâπÂæÅÔºå‰ªéËÄåÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÂàÜÂâ≤ÊéßÂà∂„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊäÄÊúØÁªÜËäÇÊñπÈù¢ÔºåËÆ∫ÊñáÂèØËÉΩÈááÁî®‰∫ÜTransformerÊû∂ÊûÑ‰Ωú‰∏∫LLMÁöÑÂü∫Á°ÄÔºåÂπ∂ËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•ËÅîÂêà‰ºòÂåñÂá†‰ΩïÂàÜÂâ≤ÂíåËøêÂä®Â≠¶ÂèÇÊï∞È¢ÑÊµã„ÄÇ$[SEG]$ tokenÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÊñπÂºèÔºà‰æãÂ¶ÇÔºåÂ¶Ç‰Ωï‰∏éÁÇπ‰∫ëÁâπÂæÅËøõË°å‰∫§‰∫íÔºåÂ¶Ç‰ΩïÂΩ±ÂìçÂàÜÂâ≤ÁªìÊûúÔºâÊòØÂÖ≥ÈîÆÁöÑËÆæËÆ°ÁªÜËäÇ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Ëøõ‰∏ÄÊ≠•Êü•Êâæ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

URDF-AnythingÂú®Ê®°ÊãüÂíåÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜ‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®Âá†‰ΩïÂàÜÂâ≤ÊñπÈù¢ÔºåmIoUÊåáÊ†áÊèêÂçá‰∫Ü17%„ÄÇÂú®ËøêÂä®Â≠¶ÂèÇÊï∞È¢ÑÊµãÊñπÈù¢ÔºåÂπ≥ÂùáËØØÂ∑ÆÈôç‰Ωé‰∫Ü29%„ÄÇÂú®Áâ©ÁêÜÂèØÊâßË°åÊÄßÊñπÈù¢ÔºåË∂ÖËøáÂü∫Á∫øÊñπÊ≥ï50%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåURDF-AnythingËÉΩÂ§üÊúâÊïàÂú∞ÊûÑÂª∫ÂèØÂä®ÂØπË±°ÁöÑÁ≤æÁ°ÆÊï∞Â≠óÂ≠™ÁîüÔºåÂπ∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

URDF-AnythingÂú®Êú∫Âô®‰∫∫‰ªøÁúüËÆ≠ÁªÉ„ÄÅÂÖ∑Ë∫´Êô∫ËÉΩ‰∏ñÁïåÊ®°ÂûãÊûÑÂª∫Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÂø´ÈÄüÊûÑÂª∫ÂêÑÁßçÂèØÂä®ÂØπË±°ÁöÑÊï∞Â≠óÂ≠™ÁîüÔºå‰ªéËÄåÂä†ÈÄüÊú∫Âô®‰∫∫ÁöÑÂºÄÂèëÂíåÈÉ®ÁΩ≤„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éËôöÊãüÁé∞ÂÆû„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÔºå‰∏∫Áî®Êà∑Êèê‰æõÊõ¥ÈÄºÁúüÁöÑ‰∫§‰∫í‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõËøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑÂú∫ÊôØÂíåÂØπË±°Ôºå‰æãÂ¶Ç‰∫∫‰ΩìÂª∫Ê®°„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Constructing accurate digital twins of articulated objects is essential for robotic simulation training and embodied AI world model building, yet historically requires painstaking manual modeling or multi-stage pipelines. In this work, we propose \textbf{URDF-Anything}, an end-to-end automatic reconstruction framework based on a 3D multimodal large language model (MLLM). URDF-Anything utilizes an autoregressive prediction framework based on point-cloud and text multimodal input to jointly optimize geometric segmentation and kinematic parameter prediction. It implements a specialized $[SEG]$ token mechanism that interacts directly with point cloud features, enabling fine-grained part-level segmentation while maintaining consistency with the kinematic parameter predictions. Experiments on both simulated and real-world datasets demonstrate that our method significantly outperforms existing approaches regarding geometric segmentation (mIoU 17\% improvement), kinematic parameter prediction (average error reduction of 29\%), and physical executability (surpassing baselines by 50\%). Notably, our method exhibits excellent generalization ability, performing well even on objects outside the training set. This work provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing the sim-to-real transfer capability.

