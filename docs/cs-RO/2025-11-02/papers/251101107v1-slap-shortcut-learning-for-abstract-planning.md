---
layout: default
title: SLAP: Shortcut Learning for Abstract Planning
---

# SLAP: Shortcut Learning for Abstract Planning

**arXiv**: [2511.01107v1](https://arxiv.org/abs/2511.01107) | [PDF](https://arxiv.org/pdf/2511.01107.pdf)

**ä½œè€…**: Y. Isabel Liu, Bowen Li, Benjamin Eysenbach, Tom Silver

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-02

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SLAPï¼šé€šè¿‡å­¦ä¹ æŠ½è±¡è§„åˆ’æ·å¾„ï¼Œæå‡æœºå™¨äººé•¿æ—¶ç¨‹å†³ç­–èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `æŠ½è±¡è§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `é•¿æ—¶ç¨‹å†³ç­–` `ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰TAMPæ–¹æ³•ä¾èµ–äºŽæ‰‹åŠ¨å®šä¹‰çš„æŠ½è±¡åŠ¨ä½œï¼Œé™åˆ¶äº†æœºå™¨äººè¡Œä¸ºçš„å¤šæ ·æ€§å’Œè§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚
2. SLAPé€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨å‘çŽ°æŠ½è±¡è§„åˆ’å›¾ä¸­çš„æ·å¾„ï¼Œä»Žè€Œå­¦ä¹ æ–°çš„æŠ½è±¡åŠ¨ä½œï¼Œæ‰©å±•äº†æœºå™¨äººçš„è¡Œä¸ºèƒ½åŠ›ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒSLAPèƒ½å¤Ÿæ˜¾è‘—ç¼©çŸ­è§„åˆ’é•¿åº¦ï¼Œæé«˜ä»»åŠ¡æˆåŠŸçŽ‡ï¼Œå¹¶åœ¨å¤šä¸ªæœºå™¨äººçŽ¯å¢ƒä¸­è¡¨çŽ°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç¨€ç–å¥–åŠ±å’Œè¿žç»­çŠ¶æ€ä¸ŽåŠ¨ä½œç©ºé—´ä¸‹çš„é•¿æ—¶ç¨‹å†³ç­–æ˜¯äººå·¥æ™ºèƒ½å’Œæœºå™¨äººé¢†åŸŸçš„ä¸€ä¸ªæ ¹æœ¬æŒ‘æˆ˜ã€‚ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’(TAMP)æ˜¯ä¸€ç§åŸºäºŽæ¨¡åž‹çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡æŠ½è±¡åŠ¨ä½œï¼ˆé€‰é¡¹ï¼‰è¿›è¡Œåˆ†å±‚è§„åˆ’æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›é€‰é¡¹æ˜¯æ‰‹åŠ¨å®šä¹‰çš„ï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“åªèƒ½æ‰§è¡Œäººç±»å·¥ç¨‹å¸ˆçŸ¥é“å¦‚ä½•ç¼–ç¨‹çš„è¡Œä¸ºï¼ˆä¾‹å¦‚ï¼Œæ‹¾å–ã€æ”¾ç½®ã€ç§»åŠ¨ï¼‰ã€‚æœ¬æ–‡æå‡ºäº†æŠ½è±¡è§„åˆ’æ·å¾„å­¦ä¹ (SLAP)ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨çŽ°æœ‰çš„TAMPé€‰é¡¹æ¥è‡ªåŠ¨å‘çŽ°æ–°çš„é€‰é¡¹ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨æ— æ¨¡åž‹å¼ºåŒ–å­¦ä¹ (RL)æ¥å­¦ä¹ ç”±TAMPä¸­çŽ°æœ‰é€‰é¡¹å¼•èµ·çš„æŠ½è±¡è§„åˆ’å›¾ä¸­çš„æ·å¾„ã€‚åœ¨æ²¡æœ‰ä»»ä½•é¢å¤–å‡è®¾æˆ–è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œæ·å¾„å­¦ä¹ æ¯”çº¯è§„åˆ’äº§ç”Ÿæ›´çŸ­çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”æ¯”æ‰å¹³åŒ–å’Œåˆ†å±‚å¼ºåŒ–å­¦ä¹ å…·æœ‰æ›´é«˜çš„ä»»åŠ¡æˆåŠŸçŽ‡ã€‚SLAPåœ¨è´¨é‡ä¸Šå‘çŽ°äº†ä¸Žæ‰‹åŠ¨å®šä¹‰çš„é€‰é¡¹æ˜¾è‘—ä¸åŒçš„åŠ¨æ€ç‰©ç†å³å…´åŠ¨ä½œï¼ˆä¾‹å¦‚ï¼Œæ‹æ‰“ã€æ‘†åŠ¨ã€æ“¦æ‹­ï¼‰ã€‚åœ¨å››ä¸ªæ¨¡æ‹Ÿæœºå™¨äººçŽ¯å¢ƒä¸­çš„å®žéªŒè¡¨æ˜Žï¼ŒSLAPèƒ½å¤Ÿè§£å†³å¹¶æ³›åŒ–åˆ°å„ç§ä»»åŠ¡ï¼Œå°†æ•´ä½“è®¡åˆ’é•¿åº¦ç¼©çŸ­äº†50%ä»¥ä¸Šï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºŽè§„åˆ’å’Œå¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’ï¼ˆTAMPï¼‰æ–¹æ³•ä¾èµ–äºŽäººå·¥è®¾è®¡çš„æŠ½è±¡åŠ¨ä½œï¼ˆoptionsï¼‰ï¼Œè¿™é™åˆ¶äº†æœºå™¨äººè§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚äººå·¥è®¾è®¡çš„optionséš¾ä»¥è¦†ç›–æ‰€æœ‰å¯èƒ½çš„æœ‰æ•ˆè¡Œä¸ºï¼Œå¯¼è‡´è§„åˆ’æ•ˆçŽ‡ä½Žä¸‹ï¼Œç”šè‡³æ— æ³•å®Œæˆä»»åŠ¡ã€‚å°¤å…¶æ˜¯åœ¨é•¿æ—¶ç¨‹å†³ç­–é—®é¢˜ä¸­ï¼Œè¿™ç§å±€é™æ€§æ›´åŠ æ˜Žæ˜¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSLAPçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è‡ªåŠ¨å‘çŽ°æŠ½è±¡è§„åˆ’å›¾ä¸­çš„æ·å¾„ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå°†çŽ°æœ‰çš„TAMP optionsè§†ä¸ºåˆå§‹çš„æŠ½è±¡åŠ¨ä½œé›†åˆï¼Œç„¶åŽé€šè¿‡RLå­¦ä¹ æ–°çš„æŠ½è±¡åŠ¨ä½œï¼Œè¿™äº›æ–°åŠ¨ä½œèƒ½å¤Ÿç›´æŽ¥è¿žæŽ¥è§„åˆ’å›¾ä¸­çš„éžç›¸é‚»èŠ‚ç‚¹ï¼Œä»Žè€Œç¼©çŸ­è§„åˆ’è·¯å¾„ã€‚è¿™ç§æ–¹æ³•æ— éœ€äººå·¥å¹²é¢„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æŽ¢ç´¢æ›´æœ‰æ•ˆçš„è¡Œä¸ºç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šSLAPçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) æž„å»ºæŠ½è±¡è§„åˆ’å›¾ï¼šåŸºäºŽçŽ°æœ‰çš„TAMP optionsï¼Œæž„å»ºä¸€ä¸ªæŠ½è±¡çš„çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ã€‚2) æ·å¾„å­¦ä¹ ï¼šä½¿ç”¨æ— æ¨¡åž‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚Q-learningæˆ–SARSAï¼‰åœ¨æŠ½è±¡è§„åˆ’å›¾ä¸Šå­¦ä¹ æ–°çš„æŠ½è±¡åŠ¨ä½œï¼ˆæ·å¾„ï¼‰ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡é¼“åŠ±æ™ºèƒ½ä½“æ‰¾åˆ°èƒ½å¤Ÿå¿«é€Ÿåˆ°è¾¾ç›®æ ‡çŠ¶æ€çš„æ·å¾„ã€‚3) è§„åˆ’æ‰§è¡Œï¼šåœ¨è§„åˆ’é˜¶æ®µï¼ŒåŒæ—¶è€ƒè™‘çŽ°æœ‰çš„TAMP optionså’Œå­¦ä¹ åˆ°çš„æ·å¾„ï¼Œé€‰æ‹©æœ€ä¼˜çš„åŠ¨ä½œåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šSLAPçš„å…³é”®åˆ›æ–°åœ¨äºŽå®ƒèƒ½å¤Ÿè‡ªåŠ¨å‘çŽ°æ–°çš„ã€éžäººå·¥è®¾è®¡çš„æŠ½è±¡åŠ¨ä½œã€‚ä¸Žä¼ ç»Ÿçš„TAMPæ–¹æ³•ç›¸æ¯”ï¼ŒSLAPæ— éœ€äººå·¥å¹²é¢„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æŽ¢ç´¢æ›´æœ‰æ•ˆçš„è¡Œä¸ºç­–ç•¥ã€‚æ­¤å¤–ï¼ŒSLAPå°†å¼ºåŒ–å­¦ä¹ ä¸ŽæŠ½è±¡è§„åˆ’ç›¸ç»“åˆï¼Œå……åˆ†åˆ©ç”¨äº†ä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œæé«˜äº†è§„åˆ’æ•ˆçŽ‡å’Œä»»åŠ¡æˆåŠŸçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šSLAPä½¿ç”¨æ— æ¨¡åž‹å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥å­¦ä¹ æ·å¾„ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œé€šå¸¸é‡‡ç”¨ç¨€ç–å¥–åŠ±ï¼Œåªæœ‰å½“æ™ºèƒ½ä½“åˆ°è¾¾ç›®æ ‡çŠ¶æ€æ—¶æ‰ç»™äºˆå¥–åŠ±ã€‚ä¸ºäº†åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ï¼Œå¯ä»¥é‡‡ç”¨å¥–åŠ±å¡‘é€ ï¼ˆreward shapingï¼‰æŠ€æœ¯ï¼Œä¾‹å¦‚ï¼Œæ ¹æ®æ™ºèƒ½ä½“ä¸Žç›®æ ‡çŠ¶æ€çš„è·ç¦»ç»™äºˆä¸­é—´å¥–åŠ±ã€‚æ­¤å¤–ï¼ŒæŽ¢ç´¢ç­–ç•¥çš„é€‰æ‹©ä¹Ÿå¾ˆé‡è¦ï¼Œå¸¸ç”¨çš„æ–¹æ³•åŒ…æ‹¬Îµ-greedyç­–ç•¥å’ŒBoltzmannæŽ¢ç´¢ç­–ç•¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒSLAPåœ¨å››ä¸ªæ¨¡æ‹Ÿæœºå™¨äººçŽ¯å¢ƒä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸Žçº¯è§„åˆ’æ–¹æ³•ç›¸æ¯”ï¼ŒSLAPèƒ½å¤Ÿå°†æ•´ä½“è®¡åˆ’é•¿åº¦ç¼©çŸ­50%ä»¥ä¸Šã€‚ä¸Žæ‰å¹³åŒ–å’Œåˆ†å±‚å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒSLAPå…·æœ‰æ›´é«˜çš„ä»»åŠ¡æˆåŠŸçŽ‡ã€‚æ­¤å¤–ï¼ŒSLAPè¿˜èƒ½å¤Ÿå‘çŽ°ä¸€äº›äººå·¥éš¾ä»¥è®¾è®¡çš„åŠ¨æ€ç‰©ç†å³å…´åŠ¨ä½œï¼Œä¾‹å¦‚ï¼Œæ‹æ‰“ã€æ‘†åŠ¨ã€æ“¦æ‹­ç­‰ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

SLAPå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥åº”ç”¨äºŽæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚åœ¨æœºå™¨äººæ“ä½œä¸­ï¼ŒSLAPå¯ä»¥å¸®åŠ©æœºå™¨äººè‡ªåŠ¨å­¦ä¹ å¤æ‚çš„è£…é…ã€æŠ“å–å’Œæ”¾ç½®ç­–ç•¥ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒSLAPå¯ä»¥å¸®åŠ©è½¦è¾†è‡ªåŠ¨è§„åˆ’æ›´é«˜æ•ˆçš„è¡Œé©¶è·¯çº¿ï¼Œå¹¶åº”å¯¹å„ç§å¤æ‚çš„äº¤é€šåœºæ™¯ã€‚åœ¨æ¸¸æˆAIä¸­ï¼ŒSLAPå¯ä»¥å¸®åŠ©æ¸¸æˆè§’è‰²è‡ªåŠ¨å­¦ä¹ æ›´æ™ºèƒ½çš„æˆ˜æ–—ç­–ç•¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long-horizon decision-making with sparse rewards and continuous states and actions remains a fundamental challenge in AI and robotics. Task and motion planning (TAMP) is a model-based framework that addresses this challenge by planning hierarchically with abstract actions (options). These options are manually defined, limiting the agent to behaviors that we as human engineers know how to program (pick, place, move). In this work, we propose Shortcut Learning for Abstract Planning (SLAP), a method that leverages existing TAMP options to automatically discover new ones. Our key idea is to use model-free reinforcement learning (RL) to learn shortcuts in the abstract planning graph induced by the existing options in TAMP. Without any additional assumptions or inputs, shortcut learning leads to shorter solutions than pure planning, and higher task success rates than flat and hierarchical RL. Qualitatively, SLAP discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that differ significantly from the manually-defined ones. In experiments in four simulated robotic environments, we show that SLAP solves and generalizes to a wide range of tasks, reducing overall plan lengths by over 50% and consistently outperforming planning and RL baselines.

