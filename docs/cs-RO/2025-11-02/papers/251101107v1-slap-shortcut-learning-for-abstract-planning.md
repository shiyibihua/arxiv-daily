---
layout: default
title: SLAP: Shortcut Learning for Abstract Planning
---

# SLAP: Shortcut Learning for Abstract Planning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.01107" target="_blank" class="toolbar-btn">arXiv: 2511.01107v1</a>
    <a href="https://arxiv.org/pdf/2511.01107.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01107v1" 
            onclick="toggleFavorite(this, '2511.01107v1', 'SLAP: Shortcut Learning for Abstract Planning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Y. Isabel Liu, Bowen Li, Benjamin Eysenbach, Tom Silver

**ÂàÜÁ±ª**: cs.RO, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-02

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SLAPÔºöÈÄöËøáÂ≠¶‰π†ÊäΩË±°ËßÑÂàíÊç∑ÂæÑÔºåÊèêÂçáÊú∫Âô®‰∫∫ÈïøÊó∂Á®ãÂÜ≥Á≠ñËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÊäΩË±°ËßÑÂàí` `Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫Êìç‰Ωú` `ÈïøÊó∂Á®ãÂÜ≥Á≠ñ` `‰ªªÂä°ÂíåËøêÂä®ËßÑÂàí`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâTAMPÊñπÊ≥ï‰æùËµñ‰∫éÊâãÂä®ÂÆö‰πâÁöÑÊäΩË±°Âä®‰ΩúÔºåÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫Ë°å‰∏∫ÁöÑÂ§öÊ†∑ÊÄßÂíåËß£ÂÜ≥Â§çÊùÇ‰ªªÂä°ÁöÑËÉΩÂäõ„ÄÇ
2. SLAPÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ëá™Âä®ÂèëÁé∞ÊäΩË±°ËßÑÂàíÂõæ‰∏≠ÁöÑÊç∑ÂæÑÔºå‰ªéËÄåÂ≠¶‰π†Êñ∞ÁöÑÊäΩË±°Âä®‰ΩúÔºåÊâ©Â±ï‰∫ÜÊú∫Âô®‰∫∫ÁöÑË°å‰∏∫ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSLAPËÉΩÂ§üÊòæËëóÁº©Áü≠ËßÑÂàíÈïøÂ∫¶ÔºåÊèêÈ´ò‰ªªÂä°ÊàêÂäüÁéáÔºåÂπ∂Âú®Â§ö‰∏™Êú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠Ë°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®Á®ÄÁñèÂ•ñÂä±ÂíåËøûÁª≠Áä∂ÊÄÅ‰∏éÂä®‰ΩúÁ©∫Èó¥‰∏ãÁöÑÈïøÊó∂Á®ãÂÜ≥Á≠ñÊòØ‰∫∫Â∑•Êô∫ËÉΩÂíåÊú∫Âô®‰∫∫È¢ÜÂüüÁöÑ‰∏Ä‰∏™Ê†πÊú¨ÊåëÊàò„ÄÇ‰ªªÂä°ÂíåËøêÂä®ËßÑÂàí(TAMP)ÊòØ‰∏ÄÁßçÂü∫‰∫éÊ®°ÂûãÁöÑÊñπÊ≥ïÔºåÂÆÉÈÄöËøáÊäΩË±°Âä®‰ΩúÔºàÈÄâÈ°πÔºâËøõË°åÂàÜÂ±ÇËßÑÂàíÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÈÄâÈ°πÊòØÊâãÂä®ÂÆö‰πâÁöÑÔºåÈôêÂà∂‰∫ÜÊô∫ËÉΩ‰ΩìÂè™ËÉΩÊâßË°å‰∫∫Á±ªÂ∑•Á®ãÂ∏àÁü•ÈÅìÂ¶Ç‰ΩïÁºñÁ®ãÁöÑË°å‰∏∫Ôºà‰æãÂ¶ÇÔºåÊãæÂèñ„ÄÅÊîæÁΩÆ„ÄÅÁßªÂä®Ôºâ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÊäΩË±°ËßÑÂàíÊç∑ÂæÑÂ≠¶‰π†(SLAP)ÔºåËØ•ÊñπÊ≥ïÂà©Áî®Áé∞ÊúâÁöÑTAMPÈÄâÈ°πÊù•Ëá™Âä®ÂèëÁé∞Êñ∞ÁöÑÈÄâÈ°π„ÄÇÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØ‰ΩøÁî®Êó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†(RL)Êù•Â≠¶‰π†Áî±TAMP‰∏≠Áé∞ÊúâÈÄâÈ°πÂºïËµ∑ÁöÑÊäΩË±°ËßÑÂàíÂõæ‰∏≠ÁöÑÊç∑ÂæÑ„ÄÇÂú®Ê≤°Êúâ‰ªª‰ΩïÈ¢ùÂ§ñÂÅáËÆæÊàñËæìÂÖ•ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊç∑ÂæÑÂ≠¶‰π†ÊØîÁ∫ØËßÑÂàí‰∫ßÁîüÊõ¥Áü≠ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂπ∂‰∏îÊØîÊâÅÂπ≥ÂåñÂíåÂàÜÂ±ÇÂº∫ÂåñÂ≠¶‰π†ÂÖ∑ÊúâÊõ¥È´òÁöÑ‰ªªÂä°ÊàêÂäüÁéá„ÄÇSLAPÂú®Ë¥®Èáè‰∏äÂèëÁé∞‰∫Ü‰∏éÊâãÂä®ÂÆö‰πâÁöÑÈÄâÈ°πÊòæËëó‰∏çÂêåÁöÑÂä®ÊÄÅÁâ©ÁêÜÂç≥ÂÖ¥Âä®‰ΩúÔºà‰æãÂ¶ÇÔºåÊãçÊâì„ÄÅÊëÜÂä®„ÄÅÊì¶Êã≠Ôºâ„ÄÇÂú®Âõõ‰∏™Ê®°ÊãüÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÁöÑÂÆûÈ™åË°®ÊòéÔºåSLAPËÉΩÂ§üËß£ÂÜ≥Âπ∂Ê≥õÂåñÂà∞ÂêÑÁßç‰ªªÂä°ÔºåÂ∞ÜÊï¥‰ΩìËÆ°ÂàíÈïøÂ∫¶Áº©Áü≠‰∫Ü50%‰ª•‰∏äÔºåÂπ∂‰∏îÂßãÁªà‰ºò‰∫éËßÑÂàíÂíåÂº∫ÂåñÂ≠¶‰π†Âü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑ‰ªªÂä°ÂíåËøêÂä®ËßÑÂàíÔºàTAMPÔºâÊñπÊ≥ï‰æùËµñ‰∫é‰∫∫Â∑•ËÆæËÆ°ÁöÑÊäΩË±°Âä®‰ΩúÔºàoptionsÔºâÔºåËøôÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫Ëß£ÂÜ≥Â§çÊùÇ‰ªªÂä°ÁöÑËÉΩÂäõ„ÄÇ‰∫∫Â∑•ËÆæËÆ°ÁöÑoptionsÈöæ‰ª•Ë¶ÜÁõñÊâÄÊúâÂèØËÉΩÁöÑÊúâÊïàË°å‰∏∫ÔºåÂØºËá¥ËßÑÂàíÊïàÁéá‰Ωé‰∏ãÔºåÁîöËá≥Êó†Ê≥ïÂÆåÊàê‰ªªÂä°„ÄÇÂ∞§ÂÖ∂ÊòØÂú®ÈïøÊó∂Á®ãÂÜ≥Á≠ñÈóÆÈ¢ò‰∏≠ÔºåËøôÁßçÂ±ÄÈôêÊÄßÊõ¥Âä†ÊòéÊòæ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSLAPÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËá™Âä®ÂèëÁé∞ÊäΩË±°ËßÑÂàíÂõæ‰∏≠ÁöÑÊç∑ÂæÑ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉÂ∞ÜÁé∞ÊúâÁöÑTAMP optionsËßÜ‰∏∫ÂàùÂßãÁöÑÊäΩË±°Âä®‰ΩúÈõÜÂêàÔºåÁÑ∂ÂêéÈÄöËøáRLÂ≠¶‰π†Êñ∞ÁöÑÊäΩË±°Âä®‰ΩúÔºåËøô‰∫õÊñ∞Âä®‰ΩúËÉΩÂ§üÁõ¥Êé•ËøûÊé•ËßÑÂàíÂõæ‰∏≠ÁöÑÈùûÁõ∏ÈÇªËäÇÁÇπÔºå‰ªéËÄåÁº©Áü≠ËßÑÂàíË∑ØÂæÑ„ÄÇËøôÁßçÊñπÊ≥ïÊó†ÈúÄ‰∫∫Â∑•Âπ≤È¢ÑÔºåËÉΩÂ§üËá™Âä®Êé¢Á¥¢Êõ¥ÊúâÊïàÁöÑË°å‰∏∫Á≠ñÁï•„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSLAPÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) ÊûÑÂª∫ÊäΩË±°ËßÑÂàíÂõæÔºöÂü∫‰∫éÁé∞ÊúâÁöÑTAMP optionsÔºåÊûÑÂª∫‰∏Ä‰∏™ÊäΩË±°ÁöÑÁä∂ÊÄÅÁ©∫Èó¥ÂíåÂä®‰ΩúÁ©∫Èó¥„ÄÇ2) Êç∑ÂæÑÂ≠¶‰π†Ôºö‰ΩøÁî®Êó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºàÂ¶ÇQ-learningÊàñSARSAÔºâÂú®ÊäΩË±°ËßÑÂàíÂõæ‰∏äÂ≠¶‰π†Êñ∞ÁöÑÊäΩË±°Âä®‰ΩúÔºàÊç∑ÂæÑÔºâ„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÈºìÂä±Êô∫ËÉΩ‰ΩìÊâæÂà∞ËÉΩÂ§üÂø´ÈÄüÂà∞ËææÁõÆÊ†áÁä∂ÊÄÅÁöÑÊç∑ÂæÑ„ÄÇ3) ËßÑÂàíÊâßË°åÔºöÂú®ËßÑÂàíÈò∂ÊÆµÔºåÂêåÊó∂ËÄÉËôëÁé∞ÊúâÁöÑTAMP optionsÂíåÂ≠¶‰π†Âà∞ÁöÑÊç∑ÂæÑÔºåÈÄâÊã©ÊúÄ‰ºòÁöÑÂä®‰ΩúÂ∫èÂàó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSLAPÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÆÉËÉΩÂ§üËá™Âä®ÂèëÁé∞Êñ∞ÁöÑ„ÄÅÈùû‰∫∫Â∑•ËÆæËÆ°ÁöÑÊäΩË±°Âä®‰Ωú„ÄÇ‰∏é‰º†ÁªüÁöÑTAMPÊñπÊ≥ïÁõ∏ÊØîÔºåSLAPÊó†ÈúÄ‰∫∫Â∑•Âπ≤È¢ÑÔºåËÉΩÂ§üËá™Âä®Êé¢Á¥¢Êõ¥ÊúâÊïàÁöÑË°å‰∏∫Á≠ñÁï•„ÄÇÊ≠§Â§ñÔºåSLAPÂ∞ÜÂº∫ÂåñÂ≠¶‰π†‰∏éÊäΩË±°ËßÑÂàíÁõ∏ÁªìÂêàÔºåÂÖÖÂàÜÂà©Áî®‰∫Ü‰∏§ÁßçÊñπÊ≥ïÁöÑ‰ºòÂäøÔºåÊèêÈ´ò‰∫ÜËßÑÂàíÊïàÁéáÂíå‰ªªÂä°ÊàêÂäüÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSLAP‰ΩøÁî®Êó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÊù•Â≠¶‰π†Êç∑ÂæÑ„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÈÄöÂ∏∏ÈááÁî®Á®ÄÁñèÂ•ñÂä±ÔºåÂè™ÊúâÂΩìÊô∫ËÉΩ‰ΩìÂà∞ËææÁõÆÊ†áÁä∂ÊÄÅÊó∂ÊâçÁªô‰∫àÂ•ñÂä±„ÄÇ‰∏∫‰∫ÜÂä†ÈÄüÂ≠¶‰π†ËøáÁ®ãÔºåÂèØ‰ª•ÈááÁî®Â•ñÂä±Â°ëÈÄ†Ôºàreward shapingÔºâÊäÄÊúØÔºå‰æãÂ¶ÇÔºåÊ†πÊçÆÊô∫ËÉΩ‰Ωì‰∏éÁõÆÊ†áÁä∂ÊÄÅÁöÑË∑ùÁ¶ªÁªô‰∫à‰∏≠Èó¥Â•ñÂä±„ÄÇÊ≠§Â§ñÔºåÊé¢Á¥¢Á≠ñÁï•ÁöÑÈÄâÊã©‰πüÂæàÈáçË¶ÅÔºåÂ∏∏Áî®ÁöÑÊñπÊ≥ïÂåÖÊã¨Œµ-greedyÁ≠ñÁï•ÂíåBoltzmannÊé¢Á¥¢Á≠ñÁï•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSLAPÂú®Âõõ‰∏™Ê®°ÊãüÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰∏éÁ∫ØËßÑÂàíÊñπÊ≥ïÁõ∏ÊØîÔºåSLAPËÉΩÂ§üÂ∞ÜÊï¥‰ΩìËÆ°ÂàíÈïøÂ∫¶Áº©Áü≠50%‰ª•‰∏ä„ÄÇ‰∏éÊâÅÂπ≥ÂåñÂíåÂàÜÂ±ÇÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåSLAPÂÖ∑ÊúâÊõ¥È´òÁöÑ‰ªªÂä°ÊàêÂäüÁéá„ÄÇÊ≠§Â§ñÔºåSLAPËøòËÉΩÂ§üÂèëÁé∞‰∏Ä‰∫õ‰∫∫Â∑•Èöæ‰ª•ËÆæËÆ°ÁöÑÂä®ÊÄÅÁâ©ÁêÜÂç≥ÂÖ¥Âä®‰ΩúÔºå‰æãÂ¶ÇÔºåÊãçÊâì„ÄÅÊëÜÂä®„ÄÅÊì¶Êã≠Á≠â„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SLAPÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÔºåÂèØ‰ª•Â∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊ∏∏ÊàèAIÁ≠âÈ¢ÜÂüü„ÄÇÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÔºåSLAPÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Ëá™Âä®Â≠¶‰π†Â§çÊùÇÁöÑË£ÖÈÖç„ÄÅÊäìÂèñÂíåÊîæÁΩÆÁ≠ñÁï•„ÄÇÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåSLAPÂèØ‰ª•Â∏ÆÂä©ËΩ¶ËæÜËá™Âä®ËßÑÂàíÊõ¥È´òÊïàÁöÑË°åÈ©∂Ë∑ØÁ∫øÔºåÂπ∂Â∫îÂØπÂêÑÁßçÂ§çÊùÇÁöÑ‰∫§ÈÄöÂú∫ÊôØ„ÄÇÂú®Ê∏∏ÊàèAI‰∏≠ÔºåSLAPÂèØ‰ª•Â∏ÆÂä©Ê∏∏ÊàèËßíËâ≤Ëá™Âä®Â≠¶‰π†Êõ¥Êô∫ËÉΩÁöÑÊàòÊñóÁ≠ñÁï•„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Long-horizon decision-making with sparse rewards and continuous states and actions remains a fundamental challenge in AI and robotics. Task and motion planning (TAMP) is a model-based framework that addresses this challenge by planning hierarchically with abstract actions (options). These options are manually defined, limiting the agent to behaviors that we as human engineers know how to program (pick, place, move). In this work, we propose Shortcut Learning for Abstract Planning (SLAP), a method that leverages existing TAMP options to automatically discover new ones. Our key idea is to use model-free reinforcement learning (RL) to learn shortcuts in the abstract planning graph induced by the existing options in TAMP. Without any additional assumptions or inputs, shortcut learning leads to shorter solutions than pure planning, and higher task success rates than flat and hierarchical RL. Qualitatively, SLAP discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that differ significantly from the manually-defined ones. In experiments in four simulated robotic environments, we show that SLAP solves and generalizes to a wide range of tasks, reducing overall plan lengths by over 50% and consistently outperforming planning and RL baselines.

