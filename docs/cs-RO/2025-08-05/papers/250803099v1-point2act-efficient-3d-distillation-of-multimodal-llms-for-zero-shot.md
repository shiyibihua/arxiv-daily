---
layout: default
title: Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping
---

# Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03099" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03099v1</a>
  <a href="https://arxiv.org/pdf/2508.03099.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03099v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03099v1', 'Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sang Min Kim, Hyeongjun Heo, Junho Kim, Yonghyeon Lee, Young Min Kim

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://sangminkim-99.github.io/point2act/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPoint2Actä»¥è§£å†³3DæŠ“å–ä»»åŠ¡ä¸­çš„ä¸Šä¸‹æ–‡ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3DæŠ“å–` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ä¸Šä¸‹æ–‡ç†è§£` `æœºå™¨äººæ“ä½œ` `ç©ºé—´å“åº”ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„3DæŠ“å–ä»»åŠ¡æ—¶ï¼Œéš¾ä»¥å‡†ç¡®å®šä½ä¸è‡ªç„¶è¯­è¨€æè¿°ç›¸å…³çš„åŠ¨ä½œç‚¹ï¼Œå¯¼è‡´æŠ“å–æ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºçš„Point2Acté€šè¿‡3Dç›¸å…³æ€§åœºï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›´æ¥æ£€ç´¢ä¸Šä¸‹æ–‡ç›¸å…³çš„3DåŠ¨ä½œç‚¹ï¼Œæå‡äº†æŠ“å–ç²¾åº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPoint2Actåœ¨20ç§’å†…ç”Ÿæˆç©ºé—´å“åº”ï¼Œæ˜¾è‘—æé«˜äº†æŠ“å–ä»»åŠ¡çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œé€‚ç”¨äºå®é™…æ“ä½œåœºæ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†Point2Actï¼Œè¯¥æ–¹æ³•ç›´æ¥æ£€ç´¢ä¸ä¸Šä¸‹æ–‡æè¿°ä»»åŠ¡ç›¸å…³çš„3DåŠ¨ä½œç‚¹ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚åŸºç¡€æ¨¡å‹ä¸ºé€šç”¨æœºå™¨äººåœ¨æœªè§ç¯å¢ƒä¸­æ‰§è¡Œé›¶-shotä»»åŠ¡æä¾›äº†å¯èƒ½æ€§ã€‚å°½ç®¡ä»å¤§è§„æ¨¡å›¾åƒå’Œè¯­è¨€æ•°æ®é›†ä¸­è·å¾—çš„è¯­ä¹‰æä¾›äº†2Då›¾åƒçš„ä¸Šä¸‹æ–‡ç†è§£ï¼Œä½†ä¸°å¯Œè€Œå¾®å¦™çš„ç‰¹å¾ä½¿å¾—æ¨¡ç³Šçš„2DåŒºåŸŸéš¾ä»¥æ‰¾åˆ°ç²¾ç¡®çš„3DåŠ¨ä½œä½ç½®ã€‚æˆ‘ä»¬æå‡ºçš„3Dç›¸å…³æ€§åœºæœ‰æ•ˆåœ°ç»•è¿‡é«˜ç»´ç‰¹å¾ï¼Œè½»é‡åŒ–åœ°æä¾›é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„2Dç‚¹çº§æŒ‡å¯¼ã€‚å¤šè§†è§’èšåˆæœ‰æ•ˆè¡¥å¿äº†ç”±äºå‡ ä½•æ¨¡ç³Šï¼ˆå¦‚é®æŒ¡ï¼‰æˆ–è¯­è¨€æè¿°ä¸­çš„è¯­ä¹‰ä¸ç¡®å®šæ€§å¯¼è‡´çš„é”™ä½ã€‚è¾“å‡ºåŒºåŸŸé«˜åº¦å±€éƒ¨åŒ–ï¼Œæ¨ç†å‡ºç»†ç²’åº¦çš„3Dç©ºé—´ä¸Šä¸‹æ–‡ï¼Œèƒ½å¤Ÿç›´æ¥è½¬åŒ–ä¸ºç‰©ç†åŠ¨ä½œçš„æ˜ç¡®ä½ç½®ã€‚æˆ‘ä»¬çš„å…¨æ ˆç®¡é“åœ¨20ç§’å†…ç”Ÿæˆç©ºé—´ä¸Šæ‰æ ¹çš„å“åº”ï¼Œä¿ƒè¿›äº†å®é™…æ“ä½œä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åœ¨å¤æ‚çš„3Dç¯å¢ƒä¸­ï¼Œå‡†ç¡®å®šä½ä¸è‡ªç„¶è¯­è¨€æè¿°ç›¸å…³çš„åŠ¨ä½œç‚¹ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é«˜ç»´ç‰¹å¾æ—¶ï¼Œå¸¸å¸¸å¯¼è‡´æ¨¡ç³Šçš„2DåŒºåŸŸå’Œä¸ç²¾ç¡®çš„3Dä½ç½®ï¼Œå½±å“æŠ“å–æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é€šè¿‡3Dç›¸å…³æ€§åœºï¼Œç»•è¿‡é«˜ç»´ç‰¹å¾ï¼Œç›´æ¥åˆ©ç”¨è½»é‡åŒ–çš„2Dç‚¹çº§æŒ‡å¯¼æ¥å®šä½åŠ¨ä½œç‚¹ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æŠ“å–ä»»åŠ¡çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æœªè§ç¯å¢ƒä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šæ•è·æ¨¡å—ã€MLLMæŸ¥è¯¢æ¨¡å—ã€3Dé‡å»ºæ¨¡å—å’ŒæŠ“å–å§¿æ€æå–æ¨¡å—ã€‚é€šè¿‡è¿™äº›æ¨¡å—çš„ååŒå·¥ä½œï¼Œå®ç°äº†ä»è‡ªç„¶è¯­è¨€æè¿°åˆ°å…·ä½“3DåŠ¨ä½œç‚¹çš„é«˜æ•ˆè½¬æ¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†3Dç›¸å…³æ€§åœºï¼Œè¿™ä¸€æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨é«˜ç»´ç‰¹å¾å¤„ç†ä¸­çš„ä¸è¶³ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å®šä½3DåŠ¨ä½œç‚¹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†å¤šè§†è§’èšåˆæŠ€æœ¯æ¥è¡¥å¿å‡ ä½•æ¨¡ç³Šå’Œè¯­ä¹‰ä¸ç¡®å®šæ€§ï¼Œç¡®ä¿è¾“å‡ºåŒºåŸŸçš„é«˜åº¦å±€éƒ¨åŒ–ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„è®¾è®¡ä¹Ÿé’ˆå¯¹ä»»åŠ¡ç‰¹å®šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æå‡æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPoint2Actåœ¨20ç§’å†…ç”Ÿæˆç©ºé—´å“åº”ï¼Œæ˜¾è‘—æé«˜äº†æŠ“å–ä»»åŠ¡çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒæŠ“å–ç²¾åº¦æå‡äº†çº¦30%ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸‹è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€è‡ªåŠ¨åŒ–ä»“å‚¨ã€æ™ºèƒ½å®¶å±…ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°ç†è§£å’Œæ‰§è¡ŒæŠ“å–ä»»åŠ¡ã€‚å…¶å®é™…ä»·å€¼åœ¨äºæå‡æœºå™¨äººåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„è‡ªåŠ¨åŒ–åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose Point2Act, which directly retrieves the 3D action point relevant for a contextually described task, leveraging Multimodal Large Language Models (MLLMs). Foundation models opened the possibility for generalist robots that can perform a zero-shot task following natural language descriptions within an unseen environment. While the semantics obtained from large-scale image and language datasets provide contextual understanding in 2D images, the rich yet nuanced features deduce blurry 2D regions and struggle to find precise 3D locations for actions. Our proposed 3D relevancy fields bypass the high-dimensional features and instead efficiently imbue lightweight 2D point-level guidance tailored to the task-specific action. The multi-view aggregation effectively compensates for misalignments due to geometric ambiguities, such as occlusion, or semantic uncertainties inherent in the language descriptions. The output region is highly localized, reasoning fine-grained 3D spatial context that can directly transfer to an explicit position for physical action at the on-the-fly reconstruction of the scene. Our full-stack pipeline, which includes capturing, MLLM querying, 3D reconstruction, and grasp pose extraction, generates spatially grounded responses in under 20 seconds, facilitating practical manipulation tasks. Project page: https://sangminkim-99.github.io/point2act/

