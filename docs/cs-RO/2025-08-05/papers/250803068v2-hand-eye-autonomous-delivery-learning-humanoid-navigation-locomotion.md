---
layout: default
title: Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching
---

# Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03068" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03068v2</a>
  <a href="https://arxiv.org/pdf/2508.03068.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03068v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03068v2', 'Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sirui Chen, Yufei Ye, Zi-Ang Cao, Jennifer Lew, Pei Xu, C. Karen Liu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-08-07)

**æœŸåˆŠ**: Conference on Robot Learning 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHand-Eyeè‡ªä¸»é…é€æ¡†æ¶ä»¥è§£å†³äººå½¢æœºå™¨äººå¯¼èˆªä¸åŠ¨ä½œå­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `äººå½¢æœºå™¨äºº` `è‡ªä¸»é…é€` `å¯¼èˆªå­¦ä¹ ` `åŠ¨ä½œæ§åˆ¶` `æ¨¡å—åŒ–è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­å¯¹äººå½¢æœºå™¨äººçš„å¯¼èˆªå’ŒåŠ¨ä½œå­¦ä¹ å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œé€‚åº”æ€§å·®çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„HEADæ¡†æ¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œå°†é«˜å±‚è§„åˆ’ä¸ä½å±‚æ§åˆ¶åˆ†å¼€ï¼Œæé«˜äº†å­¦ä¹ æ•ˆç‡å’Œåœºæ™¯é€‚åº”èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHEADæ¡†æ¶åœ¨æ¨¡æ‹Ÿå’Œç°å®ç¯å¢ƒä¸­å‡èƒ½æœ‰æ•ˆæå‡äººå½¢æœºå™¨äººçš„å¯¼èˆªå’Œä¼¸æ‰‹èƒ½åŠ›ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†Hand-Eyeè‡ªä¸»é…é€ï¼ˆHEADï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç›´æ¥ä»äººç±»è¿åŠ¨å’Œè§†è§‰æ„ŸçŸ¥æ•°æ®ä¸­å­¦ä¹ äººå½¢æœºå™¨äººçš„å¯¼èˆªã€è¡Œèµ°å’Œä¼¸æ‰‹æŠ€èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨æ¨¡å—åŒ–çš„æ–¹æ³•ï¼Œé«˜å±‚è§„åˆ’å™¨æŒ‡æŒ¥äººå½¢æœºå™¨äººçš„æ‰‹å’Œçœ¼çš„ç›®æ ‡ä½ç½®å’Œæ–¹å‘ï¼Œè€Œä½å±‚ç­–ç•¥åˆ™æ§åˆ¶æ•´ä½“è¿åŠ¨ã€‚å…·ä½“è€Œè¨€ï¼Œä½å±‚å…¨èº«æ§åˆ¶å™¨å­¦ä¹ è·Ÿè¸ªæ¥è‡ªç°æœ‰å¤§è§„æ¨¡äººç±»è¿åŠ¨æ•æ‰æ•°æ®çš„ä¸‰ä¸ªç‚¹ï¼ˆçœ¼ç›ã€å·¦æ‰‹å’Œå³æ‰‹ï¼‰ï¼Œè€Œé«˜å±‚ç­–ç•¥åˆ™ä»é€šè¿‡Ariaçœ¼é•œæ”¶é›†çš„äººç±»æ•°æ®ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ¨¡å—åŒ–æ–¹æ³•å°†è‡ªæˆ‘ä¸­å¿ƒè§†è§‰æ„ŸçŸ¥ä¸ç‰©ç†åŠ¨ä½œè§£è€¦ï¼Œä¿ƒè¿›äº†é«˜æ•ˆå­¦ä¹ å’Œå¯¹æ–°åœºæ™¯çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†äººå½¢æœºå™¨äººåœ¨ä¸ºäººç±»è®¾è®¡çš„å¤æ‚ç¯å¢ƒä¸­å¯¼èˆªå’Œä¼¸æ‰‹çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³äººå½¢æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªå’ŒåŠ¨ä½œå­¦ä¹ çš„æ•ˆç‡ä½ä¸‹å’Œé€‚åº”æ€§å·®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨äººç±»çš„è¿åŠ¨å’Œè§†è§‰æ•°æ®ï¼Œå¯¼è‡´å­¦ä¹ è¿‡ç¨‹ç¼“æ…¢ä¸”éš¾ä»¥é€‚åº”æ–°åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„HEADæ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå°†é«˜å±‚è§„åˆ’ä¸ä½å±‚æ§åˆ¶è§£è€¦ã€‚é«˜å±‚è§„åˆ’å™¨è´Ÿè´£æŒ‡æŒ¥æ‰‹å’Œçœ¼çš„ç›®æ ‡ä½ç½®ï¼Œè€Œä½å±‚æ§åˆ¶å™¨åˆ™å­¦ä¹ å¦‚ä½•é€šè¿‡å…¨èº«è¿åŠ¨å®ç°è¿™äº›ç›®æ ‡ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHEADæ¡†æ¶ç”±ä¸¤ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šé«˜å±‚è§„åˆ’æ¨¡å—å’Œä½å±‚æ§åˆ¶æ¨¡å—ã€‚é«˜å±‚æ¨¡å—ä»äººç±»æ•°æ®ä¸­å­¦ä¹ ç›®æ ‡ä½ç½®å’Œæ–¹å‘ï¼Œä½å±‚æ¨¡å—åˆ™é€šè¿‡è·Ÿè¸ªçœ¼ç›ã€å·¦æ‰‹å’Œå³æ‰‹çš„è¿åŠ¨æ¥å®ç°æ•´ä½“è¿åŠ¨æ§åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šHEADæ¡†æ¶çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶æ¨¡å—åŒ–è®¾è®¡ï¼Œä½¿å¾—è§†è§‰æ„ŸçŸ¥ä¸ç‰©ç†åŠ¨ä½œçš„å­¦ä¹ è¿‡ç¨‹ç›¸äº’ç‹¬ç«‹ï¼Œä»è€Œæé«˜äº†å­¦ä¹ çš„æ•ˆç‡å’Œå¯¹æ–°åœºæ™¯çš„é€‚åº”èƒ½åŠ›ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„ç´§è€¦åˆç‰¹æ€§å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä½å±‚æ§åˆ¶å™¨ä½¿ç”¨äº†å¤§è§„æ¨¡äººç±»è¿åŠ¨æ•æ‰æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è¿åŠ¨è½¨è¿¹çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆåœ°è·Ÿè¸ªå’Œæ§åˆ¶ä¸‰ä¸ªå…³é”®ç‚¹çš„è¿åŠ¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHEADæ¡†æ¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªå’Œä¼¸æ‰‹èƒ½åŠ›æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™è¡¨æ˜è¯¥æ¡†æ¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰è‰¯å¥½çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººå½¢æœºå™¨äººåœ¨å®¶åº­ã€åŒ»ç–—å’ŒæœåŠ¡è¡Œä¸šçš„è‡ªä¸»é…é€ä»»åŠ¡ã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªå’ŒåŠ¨ä½œèƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå¯é æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨äººå½¢æœºå™¨äººåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns navigation, locomotion, and reaching skills for humanoids, directly from human motion and vision perception data. We take a modular approach where the high-level planner commands the target position and orientation of the hands and eyes of the humanoid, delivered by the low-level policy that controls the whole-body movements. Specifically, the low-level whole-body controller learns to track the three points (eyes, left hand, and right hand) from existing large-scale human motion capture data while high-level policy learns from human data collected by Aria glasses. Our modular approach decouples the ego-centric vision perception from physical actions, promoting efficient learning and scalability to novel scenes. We evaluate our method both in simulation and in the real-world, demonstrating humanoid's capabilities to navigate and reach in complex environments designed for humans.

