---
layout: default
title: Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects
---

# Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02982" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02982v1</a>
  <a href="https://arxiv.org/pdf/2508.02982.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02982v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02982v1', 'Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lucas Chen, Guna Avula, Hanwen Ren, Zixing Wang, Ahmed H. Qureshi

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€äººç±»æ„å›¾å»ºæ¨¡ä»¥è§£å†³æœºå™¨äººä¸äººç±»çš„ç‰©å“äº¤æ¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `ç‰©å“äº¤æ¥` `å¤šæ¨¡æ€è¾“å…¥` `äººç±»åå¥½` `æœºå™¨äººæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„äººæœºäº¤æ¥æ–¹æ³•ä¾èµ–äºé¢„é€‰ç‰©å“ï¼Œæœªèƒ½è€ƒè™‘äººç±»çš„éšæ€§å’Œæ˜¾æ€§åå¥½ï¼Œå¯¼è‡´äº¤æ¥è¿‡ç¨‹ä¸å¤Ÿè‡ªç„¶æµç•…ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€äººç±»æ„å›¾å»ºæ¨¡çš„æ–¹æ³•ï¼Œé€šè¿‡äººç±»çš„è¯­è¨€å’Œéè¯­è¨€æŒ‡ä»¤é€‰æ‹©ç›®æ ‡ç‰©å“ï¼Œå¹¶ç”Ÿæˆåˆé€‚çš„äº¤æ¥åŠ¨ä½œã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†æ—¥å¸¸ç‰©å“äº¤æ¥ä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç†è§£äººç±»åå¥½ï¼Œæå‡äº¤æ¥çš„è‡ªç„¶æ€§å’Œé¡ºç•…åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººæœºç‰©å“äº¤æ¥æ˜¯åŠ©ç†æœºå™¨äººåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­å¸®åŠ©äººä»¬çš„é‡è¦ç¯èŠ‚ï¼ŒåŒ…æ‹¬è€å¹´æŠ¤ç†ã€åŒ»é™¢å’Œå·¥å‚ç­‰åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºé¢„å…ˆé€‰æ‹©çš„ç›®æ ‡ç‰©å“ï¼Œæœªèƒ½è€ƒè™‘äººç±»éšæ€§å’Œæ˜¾æ€§åå¥½ï¼Œé™åˆ¶äº†äººæœºä¹‹é—´çš„è‡ªç„¶äº’åŠ¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ï¼Œé€šè¿‡äººç±»çš„è¯­è¨€å’Œéè¯­è¨€æŒ‡ä»¤é€‰æ‹©ç›®æ ‡ç‰©å“ï¼Œå¹¶æ ¹æ®äººç±»çš„åå¥½ç”Ÿæˆæœºå™¨äººæŠ“å–å’Œäº¤æ¥åŠ¨ä½œåºåˆ—ã€‚é€šè¿‡çœŸå®ä¸–ç•Œå®éªŒå’Œç”¨æˆ·ç ”ç©¶è¯„ä¼°äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ç‰©å“äº¤æ¥ä»»åŠ¡ï¼Œç†è§£äººç±»åå¥½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³äººæœºç‰©å“äº¤æ¥ä¸­ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆè€ƒè™‘äººç±»åå¥½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„é€‰çš„ç›®æ ‡ç‰©å“ï¼Œç¼ºä¹å¯¹äººç±»éšæ€§å’Œæ˜¾æ€§åå¥½çš„ç†è§£ï¼Œå¯¼è‡´äº¤æ¥è¿‡ç¨‹ä¸å¤Ÿè‡ªç„¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¤šæ¨¡æ€è¾“å…¥ï¼ˆè¯­è¨€å’Œéè¯­è¨€æŒ‡ä»¤ï¼‰æ¥é€‰æ‹©ç›®æ ‡ç‰©å“ï¼Œå¹¶æ ¹æ®äººç±»çš„åå¥½ç”Ÿæˆæœºå™¨äººæŠ“å–å’Œäº¤æ¥åŠ¨ä½œã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæµç•…åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç›®æ ‡ç‰©å“é€‰æ‹©æ¨¡å—å’Œäº¤æ¥åŠ¨ä½œç”Ÿæˆæ¨¡å—ã€‚ç›®æ ‡ç‰©å“é€‰æ‹©æ¨¡å—é€šè¿‡è§£æäººç±»æŒ‡ä»¤æ¥è¯†åˆ«ç›®æ ‡ç‰©å“ï¼Œäº¤æ¥åŠ¨ä½œç”Ÿæˆæ¨¡å—åˆ™æ ¹æ®äººç±»åå¥½ç”Ÿæˆåˆé€‚çš„æŠ“å–å’Œäº¤æ¥åŠ¨ä½œåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†äººç±»çš„éšæ€§å’Œæ˜¾æ€§åå¥½èå…¥åˆ°ç‰©å“é€‰æ‹©å’Œäº¤æ¥åŠ¨ä½œç”Ÿæˆä¸­ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–é¢„é€‰ç‰©å“çš„æ–¹å¼æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çš„ç°å®åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä½¿ç”¨äº†å¤šæ¨¡æ€è¾“å…¥çš„ç‰¹å¾æå–ç½‘ç»œï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºè€ƒè™‘äººç±»åå¥½çš„äº¤æ¥é¡ºç•…åº¦å’Œå®‰å…¨æ€§ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥æé«˜å¯¹äººç±»æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•åœ¨ç‰©å“äº¤æ¥ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†äº¤æ¥çš„è‡ªç„¶æ€§å’Œé¡ºç•…åº¦ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œäº¤æ¥æˆåŠŸç‡æé«˜äº†çº¦30%ï¼Œç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†ä¹Ÿæ˜¾è‘—ä¸Šå‡ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è€å¹´æŠ¤ç†ã€åŒ»é™¢ã€å·¥å‚ç­‰åŠ©ç†æœºå™¨äººåœºæ™¯ã€‚é€šè¿‡ç†è§£äººç±»çš„åå¥½ï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´è‡ªç„¶åœ°ä¸äººç±»äº’åŠ¨ï¼Œä»è€Œæå‡æœåŠ¡è´¨é‡å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿åˆ°æ›´å¤šå¤æ‚çš„äººæœºäº¤äº’åœºæ™¯ä¸­ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human-robot object handover is a crucial element for assistive robots that aim to help people in their daily lives, including elderly care, hospitals, and factory floors. The existing approaches to solving these tasks rely on pre-selected target objects and do not contextualize human implicit and explicit preferences for handover, limiting natural and smooth interaction between humans and robots. These preferences can be related to the target object selection from the cluttered environment and to the way the robot should grasp the selected object to facilitate desirable human grasping during handovers. Therefore, this paper presents a unified approach that selects target distant objects using human verbal and non-verbal commands and performs the handover operation by contextualizing human implicit and explicit preferences to generate robot grasps and compliant handover motion sequences. We evaluate our integrated framework and its components through real-world experiments and user studies with arbitrary daily-life objects. The results of these evaluations demonstrate the effectiveness of our proposed pipeline in handling object handover tasks by understanding human preferences. Our demonstration videos can be found at https://youtu.be/6z27B2INl-s.

