---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-08-05
---

# cs.ROï¼ˆ2025-08-05ï¼‰

ğŸ“Š å…± **22** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (15 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250803003v1-thruster-enhanced-locomotion-a-decoupled-model-predictive-control-wi.html">Thruster-Enhanced Locomotion: A Decoupled Model Predictive Control with Learned Contact Residuals</a></td>
  <td>æå‡ºè§£è€¦æ¨¡å‹é¢„æµ‹æ§åˆ¶ä»¥è§£å†³æœºå™¨äººè¡Œèµ°ç¨³å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged locomotion</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03003v1" data-paper-url="./papers/250803003v1-thruster-enhanced-locomotion-a-decoupled-model-predictive-control-wi.html" onclick="toggleFavorite(this, '2508.03003v1', 'Thruster-Enhanced Locomotion: A Decoupled Model Predictive Control with Learned Contact Residuals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250803099v1-point2act-efficient-3d-distillation-of-multimodal-llms-for-zero-shot.html">Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping</a></td>
  <td>æå‡ºPoint2Actä»¥è§£å†³3DæŠ“å–ä»»åŠ¡ä¸­çš„ä¸Šä¸‹æ–‡ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">distillation</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03099v1" data-paper-url="./papers/250803099v1-point2act-efficient-3d-distillation-of-multimodal-llms-for-zero-shot.html" onclick="toggleFavorite(this, '2508.03099v1', 'Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250803070v1-optimizing-bipedal-locomotion-for-the-100m-dash-with-comparison-to-h.html">Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running</a></td>
  <td>ä¼˜åŒ–åŒè¶³æœºå™¨äººCassieçš„è·‘æ­¥å§¿æ€ä»¥å®ç°ç™¾ç±³å†²åˆº</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">bipedal</span> <span class="paper-tag">biped</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03070v1" data-paper-url="./papers/250803070v1-optimizing-bipedal-locomotion-for-the-100m-dash-with-comparison-to-h.html" onclick="toggleFavorite(this, '2508.03070v1', 'Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250803129v1-safety-aware-imitation-learning-via-mpc-guided-disturbance-injection.html">Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection</a></td>
  <td>æå‡ºMPC-SafeGILä»¥è§£å†³æ¨¡ä»¿å­¦ä¹ ä¸­çš„å®‰å…¨æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">MPC</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03129v1" data-paper-url="./papers/250803129v1-safety-aware-imitation-learning-via-mpc-guided-disturbance-injection.html" onclick="toggleFavorite(this, '2508.03129v1', 'Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250803068v2-hand-eye-autonomous-delivery-learning-humanoid-navigation-locomotion.html">Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching</a></td>
  <td>æå‡ºHand-Eyeè‡ªä¸»é…é€æ¡†æ¶ä»¥è§£å†³äººå½¢æœºå™¨äººå¯¼èˆªä¸åŠ¨ä½œå­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">whole-body control</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03068v2" data-paper-url="./papers/250803068v2-hand-eye-autonomous-delivery-learning-humanoid-navigation-locomotion.html" onclick="toggleFavorite(this, '2508.03068v2', 'Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250803120v1-can-large-language-models-identify-materials-from-radar-signals.html">Can Large Language Models Identify Materials from Radar Signals?</a></td>
  <td>æå‡ºLLMaterialä»¥è§£å†³é›·è¾¾ä¿¡å·ææ–™è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03120v1" data-paper-url="./papers/250803120v1-can-large-language-models-identify-materials-from-radar-signals.html" onclick="toggleFavorite(this, '2508.03120v1', 'Can Large Language Models Identify Materials from Radar Signals?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250802976v1-physics-informed-neural-time-fields-for-prehensile-object-manipulati.html">Physics-informed Neural Time Fields for Prehensile Object Manipulation</a></td>
  <td>æå‡ºå¤šæ¨¡æ€ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œä»¥è§£å†³ç‰©ä½“æ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.02976v1" data-paper-url="./papers/250802976v1-physics-informed-neural-time-fields-for-prehensile-object-manipulati.html" onclick="toggleFavorite(this, '2508.02976v1', 'Physics-informed Neural Time Fields for Prehensile Object Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250802988v1-gacl-grounded-adaptive-curriculum-learning-with-active-task-and-perf.html">GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring</a></td>
  <td>æå‡ºGACLä»¥è§£å†³æœºå™¨äººä»»åŠ¡å­¦ä¹ ä¸­çš„æ‰‹åŠ¨è®¾è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">curriculum learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.02988v1" data-paper-url="./papers/250802988v1-gacl-grounded-adaptive-curriculum-learning-with-active-task-and-perf.html" onclick="toggleFavorite(this, '2508.02988v1', 'GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250803339v2-unifucgrasp-human-hand-inspired-unified-functional-grasp-annotation-.html">UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands</a></td>
  <td>æå‡ºUniFucGraspä»¥è§£å†³å¤šæ ·åŒ–çµå·§æ‰‹æŠ“å–åŠŸèƒ½ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03339v2" data-paper-url="./papers/250803339v2-unifucgrasp-human-hand-inspired-unified-functional-grasp-annotation-.html" onclick="toggleFavorite(this, '2508.03339v2', 'UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250803246v1-force-compliance-mpc-and-robot-user-cbfs-for-interactive-navigation-.html">Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots</a></td>
  <td>æå‡ºåŠ›åˆè§„æ¨¡å‹é¢„æµ‹æ§åˆ¶ä»¥è§£å†³ç›²äººå¯¼èˆªå®‰å…¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03246v1" data-paper-url="./papers/250803246v1-force-compliance-mpc-and-robot-user-cbfs-for-interactive-navigation-.html" onclick="toggleFavorite(this, '2508.03246v1', 'Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250803043v1-aerobatic-maneuvers-in-insect-scale-flapping-wing-aerial-robots-via-.html">Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control</a></td>
  <td>é€šè¿‡æ·±åº¦å­¦ä¹ é²æ£’ç®¡é“æ¨¡å‹é¢„æµ‹æ§åˆ¶å®ç°æ˜†è™«çº§é£è¡Œæœºå™¨äººç‰¹æŠ€æœºåŠ¨</td>
  <td class="tags-cell"><span class="paper-tag">model predictive control</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03043v1" data-paper-url="./papers/250803043v1-aerobatic-maneuvers-in-insect-scale-flapping-wing-aerial-robots-via-.html" onclick="toggleFavorite(this, '2508.03043v1', 'Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250803944v1-constraint-preserving-data-generation-for-visuomotor-policy-learning.html">Constraint-Preserving Data Generation for Visuomotor Policy Learning</a></td>
  <td>æå‡ºçº¦æŸä¿æŒæ•°æ®ç”Ÿæˆæ–¹æ³•ä»¥æå‡æœºå™¨äººç­–ç•¥å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03944v1" data-paper-url="./papers/250803944v1-constraint-preserving-data-generation-for-visuomotor-policy-learning.html" onclick="toggleFavorite(this, '2508.03944v1', 'Constraint-Preserving Data Generation for Visuomotor Policy Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250803526v1-collabot-vision-language-guided-simultaneous-collaborative-manipulat.html">CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation</a></td>
  <td>æå‡ºCollaBotä»¥è§£å†³å¤šæœºå™¨äººåä½œæ“æ§å¤§ç‰©ä½“é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03526v1" data-paper-url="./papers/250803526v1-collabot-vision-language-guided-simultaneous-collaborative-manipulat.html" onclick="toggleFavorite(this, '2508.03526v1', 'CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250803428v2-residual-neural-terminal-constraint-for-mpc-based-collision-avoidanc.html">Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments</a></td>
  <td>æå‡ºæ··åˆMPCå±€éƒ¨è§„åˆ’å™¨ä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„ç¢°æ’é¿å…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03428v2" data-paper-url="./papers/250803428v2-residual-neural-terminal-constraint-for-mpc-based-collision-avoidanc.html" onclick="toggleFavorite(this, '2508.03428v2', 'Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250803293v1-enhancing-joint-human-ai-inference-in-robot-missions-a-confidence-ba.html">Enhancing Joint Human-AI Inference in Robot Missions: A Confidence-Based Approach</a></td>
  <td>æå‡ºåŸºäºç½®ä¿¡åº¦çš„è”åˆäººæœºæ¨ç†æ–¹æ³•ä»¥æå‡æœºå™¨äººä»»åŠ¡è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">teleoperation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03293v1" data-paper-url="./papers/250803293v1-enhancing-joint-human-ai-inference-in-robot-missions-a-confidence-ba.html" onclick="toggleFavorite(this, '2508.03293v1', 'Enhancing Joint Human-AI Inference in Robot Missions: A Confidence-Based Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250802982v1-multimodal-human-intent-modeling-for-contextual-robot-to-human-hando.html">Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects</a></td>
  <td>æå‡ºå¤šæ¨¡æ€äººç±»æ„å›¾å»ºæ¨¡ä»¥è§£å†³æœºå™¨äººä¸äººç±»çš„ç‰©å“äº¤æ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.02982v1" data-paper-url="./papers/250802982v1-multimodal-human-intent-modeling-for-contextual-robot-to-human-hando.html" onclick="toggleFavorite(this, '2508.02982v1', 'Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250803232v1-cookbench-a-long-horizon-embodied-planning-benchmark-for-complex-coo.html">CookBench: A Long-Horizon Embodied Planning Benchmark for Complex Cooking Scenarios</a></td>
  <td>æå‡ºCookBenchä»¥è§£å†³å¤æ‚çƒ¹é¥ªåœºæ™¯ä¸­çš„é•¿æ—¶é—´è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03232v1" data-paper-url="./papers/250803232v1-cookbench-a-long-horizon-embodied-planning-benchmark-for-complex-coo.html" onclick="toggleFavorite(this, '2508.03232v1', 'CookBench: A Long-Horizon Embodied Planning Benchmark for Complex Cooking Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250803138v1-language-as-cost-proactive-hazard-mapping-using-vlm-for-robot-naviga.html">Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation</a></td>
  <td>æå‡ºè¯­è¨€ä½œä¸ºæˆæœ¬çš„æ˜ å°„æ¡†æ¶ä»¥è§£å†³åŠ¨æ€å±é™©é¢„åˆ¤é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03138v1" data-paper-url="./papers/250803138v1-language-as-cost-proactive-hazard-mapping-using-vlm-for-robot-naviga.html" onclick="toggleFavorite(this, '2508.03138v1', 'Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250803408v3-opti-acoustic-scene-reconstruction-in-highly-turbid-underwater-envir.html">Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater Environments</a></td>
  <td>æå‡ºå®æ—¶å…‰å£°åœºæ™¯é‡å»ºæ–¹æ³•ä»¥è§£å†³æµ‘æµŠæ°´åŸŸä¸­çš„é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03408v3" data-paper-url="./papers/250803408v3-opti-acoustic-scene-reconstruction-in-highly-turbid-underwater-envir.html" onclick="toggleFavorite(this, '2508.03408v3', 'Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250803672v2-inland-loam-voxel-based-structural-semantic-lidar-odometry-and-mappi.html">Inland-LOAM: Voxel-Based Structural Semantic LiDAR Odometry and Mapping for Inland Waterway Navigation</a></td>
  <td>æå‡ºInland-LOAMä»¥è§£å†³å†…é™†æ°´é“å¯¼èˆªä¸­çš„LiDAR SLAMé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">semantic map</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03672v2" data-paper-url="./papers/250803672v2-inland-loam-voxel-based-structural-semantic-lidar-odometry-and-mappi.html" onclick="toggleFavorite(this, '2508.03672v2', 'Inland-LOAM: Voxel-Based Structural Semantic LiDAR Odometry and Mapping for Inland Waterway Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250803541v1-vision-based-perception-system-for-automated-delivery-robot-pedestri.html">Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions</a></td>
  <td>æå‡ºåŸºäºè§†è§‰çš„æ„ŸçŸ¥ç³»ç»Ÿä»¥è§£å†³è‡ªåŠ¨é€è´§æœºå™¨äººä¸è¡Œäººäº¤äº’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03541v1" data-paper-url="./papers/250803541v1-vision-based-perception-system-for-automated-delivery-robot-pedestri.html" onclick="toggleFavorite(this, '2508.03541v1', 'Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250803645v1-diwa-diffusion-policy-adaptation-with-world-models.html">DiWA: Diffusion Policy Adaptation with World Models</a></td>
  <td>æå‡ºDiWAæ¡†æ¶ä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„æ ·æœ¬æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">diffusion policy</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03645v1" data-paper-url="./papers/250803645v1-diwa-diffusion-policy-adaptation-with-world-models.html" onclick="toggleFavorite(this, '2508.03645v1', 'DiWA: Diffusion Policy Adaptation with World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)