---
layout: default
title: MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping
---

# MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20036" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20036v2</a>
  <a href="https://arxiv.org/pdf/2509.20036.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20036v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20036v2', 'MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yinzhao Dong, Ji Ma, Liu Zhao, Wanyue Li, Peng Lu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24 (æ›´æ–°: 2025-09-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MARGï¼šåŸºäºé«˜ç¨‹åœ°å›¾çš„å››è¶³æœºå™¨äººå´å²–åœ°å½¢ï¼ˆé—´éš™ï¼‰å®‰å…¨ç©¿è¶Š**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å››è¶³æœºå™¨äºº` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `åœ°å½¢åœ°å›¾` `é—´éš™åœ°å½¢` `è¿åŠ¨æ§åˆ¶` `é›¶æ ·æœ¬è¿ç§»` `PPO`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å››è¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶å™¨åœ¨å¤æ‚åœ°å½¢ï¼ˆå¦‚é—´éš™ï¼‰ä¸­ï¼Œéš¾ä»¥å…¼é¡¾å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œéœ€è¦æ„ŸçŸ¥åœ°å½¢å¹¶é€‰æ‹©åˆé€‚çš„è½è„šç‚¹ã€‚
2. MARGæ§åˆ¶å™¨èåˆåœ°å½¢åœ°å›¾å’Œæœ¬ä½“æ„Ÿå—ï¼ŒåŠ¨æ€è°ƒæ•´åŠ¨ä½œï¼Œæå‡æœºå™¨äººç¨³å®šæ€§ï¼Œå¹¶åˆ©ç”¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„ç‰¹æƒä¿¡æ¯åŠ é€Ÿç­–ç•¥ä¼˜åŒ–ã€‚
3. æå‡ºçš„åœ°å½¢åœ°å›¾ç”Ÿæˆæ¨¡å‹ï¼ˆTMGï¼‰ä»…ä½¿ç”¨å•ä¸ªæ¿€å…‰é›·è¾¾å³å¯æä¾›å‡†ç¡®çš„åœ°å½¢åœ°å›¾ï¼Œå®éªŒéªŒè¯äº†MARGåœ¨å„ç§é«˜é£é™©åœ°å½¢ä»»åŠ¡ä¸­çš„ç¨³å®šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMARGçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ§åˆ¶å™¨ï¼Œç”¨äºæå‡å››è¶³æœºå™¨äººåœ¨é«˜é£é™©é—´éš™åœ°å½¢ä¸­çš„è¿åŠ¨èƒ½åŠ›ã€‚ç°æœ‰çš„ç›²è¿åŠ¨æ§åˆ¶å™¨éš¾ä»¥ä¿è¯å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œè€ŒåŸºäºæ„ŸçŸ¥çš„æ§åˆ¶å™¨å­˜åœ¨å¤šä¼ æ„Ÿå™¨éƒ¨ç½²å¤æ‚å’Œè®¡ç®—èµ„æºéœ€æ±‚é«˜ç­‰å±€é™æ€§ã€‚MARGé›†æˆäº†åœ°å½¢åœ°å›¾å’Œæœ¬ä½“æ„Ÿå—ï¼Œä»¥åŠ¨æ€è°ƒæ•´åŠ¨ä½œå¹¶å¢å¼ºæœºå™¨äººåœ¨è¿™äº›ä»»åŠ¡ä¸­çš„ç¨³å®šæ€§ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œè¯¥æ§åˆ¶å™¨é€‰æ‹©æ€§åœ°åˆ©ç”¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„ç‰¹æƒä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œè´¨å¿ƒã€æ‘©æ“¦ç³»æ•°ï¼‰æ¥åŠ é€Ÿç­–ç•¥ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸‰ä¸ªä¸è¶³éƒ¨ç›¸å…³çš„å¥–åŠ±ï¼Œä»¥é¼“åŠ±æœºå™¨äººæ¢ç´¢å®‰å…¨çš„è½è„šç‚¹ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæå‡ºäº†ä¸€ç§åœ°å½¢åœ°å›¾ç”Ÿæˆï¼ˆTMGï¼‰æ¨¡å‹ï¼Œä»¥å‡å°‘åœ°å›¾æ¼‚ç§»ï¼Œå¹¶ä»…ä½¿ç”¨ä¸€ä¸ªæ¿€å…‰é›·è¾¾æä¾›å‡†ç¡®çš„åœ°å½¢åœ°å›¾ï¼Œä¸ºå­¦ä¹ ç­–ç•¥çš„é›¶æ ·æœ¬è¿ç§»å¥ å®šåŸºç¡€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMARGåœ¨å„ç§é«˜é£é™©åœ°å½¢ä»»åŠ¡ä¸­ä¿æŒäº†ç¨³å®šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å››è¶³æœºå™¨äººåœ¨å¤æ‚é—´éš™åœ°å½¢ä¸­é¢ä¸´å®‰å…¨æ€§å’Œæ•ˆç‡çš„æŒ‘æˆ˜ã€‚ç›²è¿åŠ¨æ§åˆ¶å™¨æ— æ³•æœ‰æ•ˆåº”å¯¹ï¼Œè€ŒåŸºäºæ„ŸçŸ¥çš„æ§åˆ¶å™¨é€šå¸¸éœ€è¦å¤æ‚çš„å¤šä¼ æ„Ÿå™¨ç³»ç»Ÿå’Œå¤§é‡çš„è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿåˆ©ç”¨æœ‰é™çš„ä¼ æ„Ÿå™¨ä¿¡æ¯ï¼Œå®‰å…¨é«˜æ•ˆåœ°é€šè¿‡é«˜é£é™©é—´éš™åœ°å½¢çš„æ§åˆ¶æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMARGçš„æ ¸å¿ƒæ€è·¯æ˜¯èåˆåœ°å½¢åœ°å›¾å’Œæœ¬ä½“æ„Ÿå—ä¿¡æ¯ï¼Œé€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ§åˆ¶å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç¯å¢ƒä¿¡æ¯åŠ¨æ€è°ƒæ•´åŠ¨ä½œï¼Œä»è€Œå¢å¼ºæœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸­çš„ç¨³å®šæ€§ã€‚åŒæ—¶ï¼Œåˆ©ç”¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„ç‰¹æƒä¿¡æ¯åŠ é€Ÿè®­ç»ƒï¼Œå¹¶è®¾è®¡è¶³éƒ¨ç›¸å…³çš„å¥–åŠ±å‡½æ•°ï¼Œå¼•å¯¼æœºå™¨äººé€‰æ‹©å®‰å…¨çš„è½è„šç‚¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMARGçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) çŠ¶æ€è¾“å…¥æ¨¡å—ï¼šæ¥æ”¶æ¥è‡ªæœºå™¨äººæœ¬ä½“æ„Ÿå—å™¨ï¼ˆå¦‚å…³èŠ‚è§’åº¦ã€è§’é€Ÿåº¦ç­‰ï¼‰å’Œåœ°å½¢åœ°å›¾çš„ä¿¡æ¯ã€‚2) æ·±åº¦å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨ï¼šæ ¹æ®çŠ¶æ€ä¿¡æ¯è¾“å‡ºæœºå™¨äººçš„åŠ¨ä½œæŒ‡ä»¤ã€‚3) å¥–åŠ±å‡½æ•°è®¾è®¡ï¼šåŒ…æ‹¬å‰è¿›å¥–åŠ±ã€ç”Ÿå­˜å¥–åŠ±å’Œè¶³éƒ¨ç›¸å…³å¥–åŠ±ï¼Œç”¨äºå¼•å¯¼æœºå™¨äººå­¦ä¹ æœŸæœ›çš„è¡Œä¸ºã€‚4) åœ°å½¢åœ°å›¾ç”Ÿæˆæ¨¡å‹ï¼ˆTMGï¼‰ï¼šä½¿ç”¨å•ä¸ªæ¿€å…‰é›·è¾¾ç”Ÿæˆå‡†ç¡®çš„åœ°å½¢åœ°å›¾ï¼Œå‡å°‘åœ°å›¾æ¼‚ç§»ã€‚

**å…³é”®åˆ›æ–°**ï¼šMARGçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) èåˆåœ°å½¢åœ°å›¾å’Œæœ¬ä½“æ„Ÿå—ä¿¡æ¯ï¼Œæå‡äº†æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚2) åˆ©ç”¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„ç‰¹æƒä¿¡æ¯åŠ é€Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚3) æå‡ºäº†åœ°å½¢åœ°å›¾ç”Ÿæˆæ¨¡å‹ï¼ˆTMGï¼‰ï¼Œä»…ä½¿ç”¨å•ä¸ªæ¿€å…‰é›·è¾¾å³å¯ç”Ÿæˆå‡†ç¡®çš„åœ°å½¢åœ°å›¾ï¼Œé™ä½äº†ç¡¬ä»¶æˆæœ¬å’Œè®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼š1) è¶³éƒ¨ç›¸å…³å¥–åŠ±ï¼šè®¾è®¡äº†ä¸‰ä¸ªä¸è¶³éƒ¨ç›¸å…³çš„å¥–åŠ±ï¼ŒåŒ…æ‹¬è¶³éƒ¨é«˜åº¦å¥–åŠ±ã€è¶³éƒ¨ç¨³å®šæ€§å¥–åŠ±å’Œè¶³éƒ¨é—´éš™æƒ©ç½šï¼Œé¼“åŠ±æœºå™¨äººé€‰æ‹©å®‰å…¨çš„è½è„šç‚¹ã€‚2) åœ°å½¢åœ°å›¾ç”Ÿæˆæ¨¡å‹ï¼ˆTMGï¼‰ï¼šé‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ï¼Œè¾“å…¥æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®ï¼Œè¾“å‡ºé«˜ç¨‹åœ°å›¾ã€‚3) å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šé‡‡ç”¨PPOï¼ˆProximal Policy Optimizationï¼‰ç®—æ³•è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨Adamä¼˜åŒ–å™¨è¿›è¡Œå‚æ•°æ›´æ–°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMARGåœ¨å„ç§é«˜é£é™©åœ°å½¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„ç¨³å®šæ€§å’Œé€šè¿‡æ€§ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒMARGåœ¨é—´éš™ç©¿è¶Šä»»åŠ¡ä¸­çš„æˆåŠŸç‡æé«˜äº†æ˜¾è‘—æ¯”ä¾‹ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ï¼Œå¹¶ä¸”èƒ½å¤Ÿä»…ä½¿ç”¨å•ä¸ªæ¿€å…‰é›·è¾¾ç”Ÿæˆå‡†ç¡®çš„åœ°å½¢åœ°å›¾ï¼Œå®ç°äº†é›¶æ ·æœ¬è¿ç§»ã€‚è¿™äº›ç»“æœéªŒè¯äº†MARGåœ¨å¤æ‚åœ°å½¢è¿åŠ¨æ§åˆ¶æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MARGæŠ€æœ¯å¯åº”ç”¨äºæœæ•‘ã€å‹˜æ¢ã€ç‰©æµç­‰é¢†åŸŸï¼Œä½¿å››è¶³æœºå™¨äººèƒ½å¤Ÿåœ¨å¤æ‚ã€å´å²–ã€é«˜é£é™©çš„ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¾åæœæ•‘ä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨MARGæŠ€æœ¯å®‰å…¨åœ°ç©¿è¶ŠåºŸå¢Ÿï¼Œå¯»æ‰¾å¹¸å­˜è€…ã€‚åœ¨é‡å¤–å‹˜æ¢ä¸­ï¼Œæœºå™¨äººå¯ä»¥è‡ªä¸»å¯¼èˆªï¼Œæ”¶é›†ç¯å¢ƒæ•°æ®ã€‚è¯¥æŠ€æœ¯è¿˜æœ‰åŠ©äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´å¯é çš„å››è¶³æœºå™¨äººï¼Œæ‰©å±•å…¶åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have demonstrated impressive performance on challenging terrains, allowing robots to execute complex skills such as climbing, running, and jumping. However, existing blind locomotion controllers often struggle to ensure safety and efficient traversal through risky gap terrains, which are typically highly complex, requiring robots to perceive terrain information and select appropriate footholds during locomotion accurately. Meanwhile, existing perception-based controllers still present several practical limitations, including a complex multi-sensor deployment system and expensive computing resource requirements. This paper proposes a DRL controller named MAstering Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to dynamically adjust the action and enhance the robot's stability in these tasks. During the training phase, our controller accelerates policy optimization by selectively incorporating privileged information (e.g., center of mass, friction coefficients) that are available in simulation but unmeasurable directly in real-world deployments due to sensor limitations. We also designed three foot-related rewards to encourage the robot to explore safe footholds. More importantly, a terrain map generation (TMG) model is proposed to reduce the drift existing in mapping and provide accurate terrain maps using only one LiDAR, providing a foundation for zero-shot transfer of the learned policy. The experimental results indicate that MARG maintains stability in various risky terrain tasks.

