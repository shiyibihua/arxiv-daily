---
layout: default
title: Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video
---

# Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20286" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20286v1</a>
  <a href="https://arxiv.org/pdf/2509.20286.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20286v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20286v1', 'Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Georgios Tziafas, Jiayun Zhang, Hamidreza Kasaei

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://gtziafas.github.io/PAD_project/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PADï¼šä»å•ä¸ªäººç±»è§†é¢‘å­¦ä¹ å¯æ³›åŒ–çš„åŒè‡‚è§†è§‰è¿åŠ¨ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒè‡‚æ“ä½œ` `è§†è§‰è¿åŠ¨ç­–ç•¥` `æ¨¡ä»¿å­¦ä¹ ` `å…³é”®ç‚¹è¡¨ç¤º` `ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–å¤§é‡é¥æ“ä½œæ•°æ®ï¼Œæ³›åŒ–æ€§å·®ï¼Œä¸”åŸºäºå›¾åƒçš„ç­–ç•¥å­˜åœ¨sim-to-realå·®è·ã€‚
2. PADæ¡†æ¶é€šè¿‡è§£æäººç±»è§†é¢‘ä¸ºå…³é”®ç‚¹è½¨è¿¹ï¼Œåˆ©ç”¨ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’è¿›è¡Œæ— æ¨¡æ‹Ÿå™¨çš„æ•°æ®å¢å¼ºï¼Œå¹¶è’¸é¦æˆå…³é”®ç‚¹æ¡ä»¶ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPADåœ¨æˆåŠŸç‡å’Œæ ·æœ¬æ•ˆç‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œçš„åŒè‡‚ä»»åŠ¡ä¸­å®ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ è§†è§‰è¿åŠ¨ç­–ç•¥æ˜¯ç°ä»£æœºå™¨äººç ”ç©¶çš„é‡è¦å‰æ²¿ï¼Œç„¶è€Œï¼Œå¤§å¤šæ•°æµè¡Œçš„æ–¹æ³•éœ€è¦å¤§é‡çš„é¥æ“ä½œæ•°æ®æ”¶é›†å·¥ä½œï¼Œå¹¶ä¸”éš¾ä»¥æ³›åŒ–åˆ°åˆ†å¸ƒå¤–ã€‚æ‰©å±•æ•°æ®æ”¶é›†å·²ç»é€šè¿‡åˆ©ç”¨äººç±»è§†é¢‘ä»¥åŠæ¼”ç¤ºå¢å¼ºæŠ€æœ¯è¿›è¡Œäº†æ¢ç´¢ã€‚åä¸€ç§æ–¹æ³•é€šå¸¸éœ€è¦æ˜‚è´µçš„æ¨¡æ‹Ÿrolloutï¼Œå¹¶ä½¿ç”¨åˆæˆå›¾åƒæ•°æ®è®­ç»ƒç­–ç•¥ï¼Œå› æ­¤å¼•å…¥äº†sim-to-realå·®è·ã€‚åŒæ—¶ï¼Œè¯¸å¦‚å…³é”®ç‚¹ä¹‹ç±»çš„æ›¿ä»£çŠ¶æ€è¡¨ç¤ºå·²æ˜¾ç¤ºå‡ºåœ¨ç±»åˆ«çº§åˆ«æ³›åŒ–çš„å·¨å¤§å‰æ™¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è¿™äº›é€”å¾„æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ï¼šPADï¼ˆParse-Augment-Distillï¼‰ï¼Œç”¨äºä»å•ä¸ªäººçš„è§†é¢‘ä¸­å­¦ä¹ å¯æ³›åŒ–çš„åŒè‡‚ç­–ç•¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾èµ–äºä¸‰ä¸ªæ­¥éª¤ï¼šï¼ˆaï¼‰å°†äººçš„è§†é¢‘æ¼”ç¤ºè§£æä¸ºæœºå™¨äººå¯æ‰§è¡Œçš„å…³é”®ç‚¹-åŠ¨ä½œè½¨è¿¹ï¼Œï¼ˆbï¼‰é‡‡ç”¨åŒè‡‚ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’æ¥å¤§è§„æ¨¡åœ°å¢å¼ºæ¼”ç¤ºï¼Œè€Œæ— éœ€æ¨¡æ‹Ÿå™¨ï¼Œä»¥åŠï¼ˆcï¼‰å°†å¢å¼ºçš„è½¨è¿¹æç‚¼æˆå…³é”®ç‚¹æ¡ä»¶ç­–ç•¥ã€‚åœ¨ç»éªŒä¸Šï¼Œæˆ‘ä»¬å±•ç¤ºäº†PADåœ¨æˆåŠŸç‡å’Œæ ·æœ¬/æˆæœ¬æ•ˆç‡æ–¹é¢å‡ä¼˜äºä¾èµ–äºå…·æœ‰æ¨¡æ‹Ÿrolloutçš„å›¾åƒç­–ç•¥çš„æœ€æ–°åŒè‡‚æ¼”ç¤ºå¢å¼ºå·¥ä½œã€‚æˆ‘ä»¬åœ¨å…­ä¸ªä¸åŒçš„ç°å®ä¸–ç•ŒåŒè‡‚ä»»åŠ¡ä¸­éƒ¨ç½²äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œä¾‹å¦‚å€’é¥®æ–™ï¼Œæ¸…ç†åƒåœ¾å’Œæ‰“å¼€å®¹å™¨ï¼Œä»è€Œäº§ç”Ÿå¯ä»¥åœ¨çœ‹ä¸è§çš„ç©ºé—´æ’åˆ—ï¼Œå¯¹è±¡å®ä¾‹å’ŒèƒŒæ™¯å¹²æ‰°ç‰©ä¸­æ³›åŒ–çš„one-shotç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»å°‘é‡ï¼ˆç”šè‡³å•ä¸ªï¼‰äººç±»è§†é¢‘ä¸­å­¦ä¹ å¯æ³›åŒ–çš„åŒè‡‚æ“ä½œç­–ç•¥çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æœºå™¨äººé¥æ“ä½œæ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œä¸”éš¾ä»¥æ³›åŒ–åˆ°æ–°çš„åœºæ™¯ã€‚åŸºäºå›¾åƒçš„ç­–ç•¥è®­ç»ƒé€šå¸¸ä¾èµ–äºæ¨¡æ‹Ÿç¯å¢ƒï¼Œå¼•å…¥äº†sim-to-realçš„å·®è·ï¼Œå½±å“äº†çœŸå®ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å…³é”®ç‚¹ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå°†äººç±»è§†é¢‘è§£æä¸ºæœºå™¨äººå¯æ‰§è¡Œçš„è½¨è¿¹ï¼Œå¹¶ç»“åˆä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’è¿›è¡Œæ•°æ®å¢å¼ºï¼Œæœ€åé€šè¿‡è’¸é¦å­¦ä¹ å¾—åˆ°ä¸€ä¸ªå…³é”®ç‚¹æ¡ä»¶ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥ä»å›¾åƒå­¦ä¹ ç­–ç•¥ï¼Œå‡å°‘äº†sim-to-realå·®è·ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’å®ç°äº†é«˜æ•ˆçš„æ•°æ®å¢å¼ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPADæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) **è§£æ(Parse)**ï¼šå°†äººç±»è§†é¢‘æ¼”ç¤ºè§£æä¸ºæœºå™¨äººå¯æ‰§è¡Œçš„å…³é”®ç‚¹-åŠ¨ä½œè½¨è¿¹ã€‚è¿™é€šå¸¸æ¶‰åŠä½¿ç”¨å§¿æ€ä¼°è®¡æ¨¡å‹æå–å…³é”®ç‚¹ï¼Œå¹¶å°†å…¶ä¸ç›¸åº”çš„åŠ¨ä½œæŒ‡ä»¤å…³è”èµ·æ¥ã€‚2) **å¢å¼º(Augment)**ï¼šåˆ©ç”¨åŒè‡‚ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’å™¨ï¼Œåœ¨ä¸åŒçš„åœºæ™¯å’Œç‰©ä½“æ’åˆ—ä¸‹ç”Ÿæˆæ›´å¤šçš„è½¨è¿¹æ•°æ®ã€‚è¿™ä¸€æ­¥æ— éœ€ä¾èµ–æ¨¡æ‹Ÿå™¨ï¼Œè€Œæ˜¯é€šè¿‡ç®—æ³•è‡ªåŠ¨ç”Ÿæˆã€‚3) **è’¸é¦(Distill)**ï¼šå°†å¢å¼ºåçš„è½¨è¿¹æ•°æ®æç‚¼æˆä¸€ä¸ªå…³é”®ç‚¹æ¡ä»¶ç­–ç•¥ã€‚è¯¥ç­–ç•¥ä»¥å…³é”®ç‚¹ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºç›¸åº”çš„åŠ¨ä½œæŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šPADæ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºå°†å…³é”®ç‚¹è¡¨ç¤ºã€ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’ä»¥åŠè’¸é¦å­¦ä¹ ç»“åˆèµ·æ¥ï¼Œå®ç°äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ³›åŒ–çš„åŒè‡‚æ“ä½œç­–ç•¥å­¦ä¹ æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå›¾åƒçš„ç­–ç•¥å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒPADé¿å…äº†sim-to-realå·®è·ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’å®ç°äº†é«˜æ•ˆçš„æ•°æ®å¢å¼ºã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å§¿æ€ä¼°è®¡æ¨¡å‹æå–å…³é”®ç‚¹ï¼Œå¹¶è®¾è®¡åˆé€‚çš„å…³é”®ç‚¹è¡¨ç¤ºã€‚2) è®¾è®¡åˆé€‚çš„ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’å™¨ï¼Œä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„è½¨è¿¹æ•°æ®ã€‚3) é€‰æ‹©åˆé€‚çš„ç¥ç»ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥è®­ç»ƒå…³é”®ç‚¹æ¡ä»¶ç­–ç•¥ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæåŠã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PADåœ¨å…­ä¸ªä¸åŒçš„çœŸå®ä¸–ç•ŒåŒè‡‚ä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å€’é¥®æ–™ã€æ¸…ç†åƒåœ¾å’Œæ‰“å¼€å®¹å™¨ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPADåœ¨æˆåŠŸç‡å’Œæ ·æœ¬æ•ˆç‡æ–¹é¢å‡ä¼˜äºä¾èµ–äºå…·æœ‰æ¨¡æ‹Ÿrolloutçš„å›¾åƒç­–ç•¥çš„æœ€æ–°åŒè‡‚æ¼”ç¤ºå¢å¼ºå·¥ä½œã€‚PADèƒ½å¤Ÿç”Ÿæˆå¯ä»¥åœ¨çœ‹ä¸è§çš„ç©ºé—´æ’åˆ—ï¼Œå¯¹è±¡å®ä¾‹å’ŒèƒŒæ™¯å¹²æ‰°ç‰©ä¸­æ³›åŒ–çš„one-shotç­–ç•¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦åŒè‡‚æ“ä½œçš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰ã€‚é€šè¿‡ä»å°‘é‡äººç±»æ¼”ç¤ºä¸­å­¦ä¹ ï¼Œæœºå™¨äººå¯ä»¥å¿«é€Ÿé€‚åº”æ–°çš„ä»»åŠ¡å’Œç¯å¢ƒï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œçµæ´»æ€§ã€‚è¯¥æ–¹æ³•æœ‰æœ›é™ä½æœºå™¨äººéƒ¨ç½²çš„æˆæœ¬å’Œéš¾åº¦ï¼ŒåŠ é€Ÿæœºå™¨äººåœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning visuomotor policies from expert demonstrations is an important frontier in modern robotics research, however, most popular methods require copious efforts for collecting teleoperation data and struggle to generalize out-ofdistribution. Scaling data collection has been explored through leveraging human videos, as well as demonstration augmentation techniques. The latter approach typically requires expensive simulation rollouts and trains policies with synthetic image data, therefore introducing a sim-to-real gap. In parallel, alternative state representations such as keypoints have shown great promise for category-level generalization. In this work, we bring these avenues together in a unified framework: PAD (Parse-AugmentDistill), for learning generalizable bimanual policies from a single human video. Our method relies on three steps: (a) parsing a human video demo into a robot-executable keypoint-action trajectory, (b) employing bimanual task-and-motion-planning to augment the demonstration at scale without simulators, and (c) distilling the augmented trajectories into a keypoint-conditioned policy. Empirically, we showcase that PAD outperforms state-ofthe-art bimanual demonstration augmentation works relying on image policies with simulation rollouts, both in terms of success rate and sample/cost efficiency. We deploy our framework in six diverse real-world bimanual tasks such as pouring drinks, cleaning trash and opening containers, producing one-shot policies that generalize in unseen spatial arrangements, object instances and background distractors. Supplementary material can be found in the project webpage https://gtziafas.github.io/PAD_project/.

