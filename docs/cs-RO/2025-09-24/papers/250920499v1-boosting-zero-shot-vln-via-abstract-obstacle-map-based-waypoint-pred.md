---
layout: default
title: Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting
---

# Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20499" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20499v1</a>
  <a href="https://arxiv.org/pdf/2509.20499.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20499v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20499v1', 'Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Boqi Li, Siyuan Li, Weiyi Wang, Anran Li, Zhong Cao, Henry X. Liu

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæŠ½è±¡éšœç¢åœ°å›¾é›¶æ ·æœ¬VLNæ¡†æ¶ï¼Œç»“åˆæ‹“æ‰‘å›¾å’Œè®¿é—®ä¿¡æ¯æç¤ºï¼Œå®ç°æ›´ä¼˜å¯¼èˆªã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `é›¶æ ·æœ¬å­¦ä¹ ` `æŠ½è±¡éšœç¢åœ°å›¾` `æ‹“æ‰‘å›¾` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLNæ–¹æ³•åœ¨è¿ç»­ç¯å¢ƒä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ™ºèƒ½ä½“ç†è§£æŒ‡ä»¤ã€æ„ŸçŸ¥ç¯å¢ƒå¹¶è§„åˆ’åŠ¨ä½œã€‚
2. è®ºæ–‡æå‡ºåŸºäºæŠ½è±¡éšœç¢åœ°å›¾çš„èˆªç‚¹é¢„æµ‹å™¨ï¼Œç»“åˆæ‹“æ‰‘å›¾å’Œè®¿é—®ä¿¡æ¯æç¤ºï¼Œæå‡å¯¼èˆªæ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨R2R-CEå’ŒRxR-CEæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€åŸºç¡€æ¨¡å‹å’Œæœºå™¨äººæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œè§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰å·²æˆä¸ºå…·èº«æ™ºèƒ½ä½“çš„å…³é”®ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…åº”ç”¨ã€‚æœ¬æ–‡ç ”ç©¶è¿ç»­ç¯å¢ƒä¸‹çš„VLNï¼Œè¿™æ˜¯ä¸€ä¸ªç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼Œæ™ºèƒ½ä½“å¿…é¡»è”åˆè§£é‡Šè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€æ„ŸçŸ¥å‘¨å›´ç¯å¢ƒå¹¶è§„åˆ’åº•å±‚åŠ¨ä½œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé›¶æ ·æœ¬æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†ç®€åŒ–çš„èˆªç‚¹é¢„æµ‹å™¨å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚è¯¥é¢„æµ‹å™¨åœ¨æŠ½è±¡éšœç¢åœ°å›¾ä¸Šè¿è¡Œï¼Œç”Ÿæˆçº¿æ€§å¯è¾¾çš„èˆªç‚¹ï¼Œè¿™äº›èˆªç‚¹è¢«åˆå¹¶åˆ°å…·æœ‰æ˜¾å¼è®¿é—®è®°å½•çš„åŠ¨æ€æ›´æ–°çš„æ‹“æ‰‘å›¾ä¸­ã€‚å›¾å’Œè®¿é—®ä¿¡æ¯è¢«ç¼–ç åˆ°æç¤ºä¸­ï¼Œä»è€Œèƒ½å¤Ÿå¯¹ç©ºé—´ç»“æ„å’Œæ¢ç´¢å†å²è¿›è¡Œæ¨ç†ï¼Œä»¥é¼“åŠ±æ¢ç´¢å¹¶ä½¿MLLMå…·å¤‡å±€éƒ¨è·¯å¾„è§„åˆ’èƒ½åŠ›ä»¥è¿›è¡Œé”™è¯¯çº æ­£ã€‚åœ¨R2R-CEå’ŒRxR-CEä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼ŒæˆåŠŸç‡åˆ†åˆ«ä¸º41%å’Œ36%ï¼Œä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»»åŠ¡æ—¨åœ¨è®©æ™ºèƒ½ä½“æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨çœŸå®æˆ–æ¨¡æ‹Ÿç¯å¢ƒä¸­å¯¼èˆªã€‚åœ¨è¿ç»­ç¯å¢ƒä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦å¤„ç†å¤æ‚çš„æ„ŸçŸ¥è¾“å…¥ï¼Œå¹¶è¿›è¡Œè¿ç»­çš„åŠ¨ä½œè§„åˆ’ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨æ¢ç´¢å†å²å’Œç©ºé—´ç»“æ„ä¿¡æ¯ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç¯å¢ƒæŠ½è±¡æˆä¸€ä¸ªéšœç¢åœ°å›¾ï¼Œå¹¶åŸºäºæ­¤é¢„æµ‹å¯è¡Œçš„èˆªç‚¹ã€‚é€šè¿‡æ„å»ºåŠ¨æ€æ›´æ–°çš„æ‹“æ‰‘å›¾ï¼Œå¹¶ç»“åˆè®¿é—®ä¿¡æ¯ï¼Œä¸ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨åˆ©ç”¨MLLMçš„æ¨ç†èƒ½åŠ›ï¼Œç»“åˆå±€éƒ¨è·¯å¾„è§„åˆ’ï¼Œå®ç°æ›´æœ‰æ•ˆçš„å¯¼èˆªã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æŠ½è±¡éšœç¢åœ°å›¾æ„å»ºæ¨¡å—ï¼šå°†ç¯å¢ƒä¿¡æ¯ç®€åŒ–ä¸ºéšœç¢åœ°å›¾ã€‚2) èˆªç‚¹é¢„æµ‹æ¨¡å—ï¼šåŸºäºéšœç¢åœ°å›¾é¢„æµ‹çº¿æ€§å¯è¾¾çš„èˆªç‚¹ã€‚3) æ‹“æ‰‘å›¾æ„å»ºä¸æ›´æ–°æ¨¡å—ï¼šæ„å»ºåŒ…å«èˆªç‚¹å’Œè®¿é—®ä¿¡æ¯çš„æ‹“æ‰‘å›¾ã€‚4) æç¤ºç¼–ç æ¨¡å—ï¼šå°†æ‹“æ‰‘å›¾å’Œè®¿é—®ä¿¡æ¯ç¼–ç æˆæç¤ºã€‚5) å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼šæ¥æ”¶æç¤ºå’Œè§†è§‰è¾“å…¥ï¼Œè¾“å‡ºå¯¼èˆªåŠ¨ä½œã€‚æ•´ä¸ªæµç¨‹æ˜¯åŠ¨æ€æ›´æ–°çš„ï¼Œæ™ºèƒ½ä½“åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­ä¸æ–­å®Œå–„æ‹“æ‰‘å›¾å’Œè®¿é—®ä¿¡æ¯ï¼Œä»è€Œæé«˜å¯¼èˆªæ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†ç¯å¢ƒæŠ½è±¡æˆéšœç¢åœ°å›¾ï¼Œå¹¶ç»“åˆæ‹“æ‰‘å›¾å’Œè®¿é—®ä¿¡æ¯è¿›è¡Œæç¤ºã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†ç©ºé—´ç»“æ„å’Œæ¢ç´¢å†å²ä¿¡æ¯ï¼Œä¸ºMLLMæä¾›äº†æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æŠ½è±¡éšœç¢åœ°å›¾çš„ç®€åŒ–ç¨‹åº¦ï¼Œéœ€è¦åœ¨ä¿¡æ¯æŸå¤±å’Œè®¡ç®—å¤æ‚åº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚2) èˆªç‚¹é¢„æµ‹ç®—æ³•çš„é€‰æ‹©ï¼Œéœ€è¦ä¿è¯èˆªç‚¹çš„å¯è¾¾æ€§å’Œè¦†ç›–èŒƒå›´ã€‚3) æ‹“æ‰‘å›¾çš„æ›´æ–°ç­–ç•¥ï¼Œéœ€è¦è€ƒè™‘æ–°èˆªç‚¹çš„æ·»åŠ å’Œå·²æœ‰èˆªç‚¹çš„è¿æ¥ã€‚4) æç¤ºç¼–ç æ–¹å¼ï¼Œéœ€è¦æœ‰æ•ˆåœ°å°†æ‹“æ‰‘å›¾å’Œè®¿é—®ä¿¡æ¯ä¼ é€’ç»™MLLMã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰æ›´è¯¦ç»†çš„æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨R2R-CEå’ŒRxR-CEæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒæˆåŠŸç‡åˆ†åˆ«è¾¾åˆ°41%å’Œ36%ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„é›¶æ ·æœ¬VLNæ–¹æ³•ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨ç©ºé—´ç»“æ„å’Œæ¢ç´¢å†å²ä¿¡æ¯ï¼Œæé«˜å¯¼èˆªçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚å…·ä½“çš„æå‡å¹…åº¦éœ€è¦å‚è€ƒå¯¹æ¯”çš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºå¼€å‘æ™ºèƒ½å®¶å±…æœºå™¨äººï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„è¯­éŸ³æŒ‡ä»¤åœ¨å®¤å†…ç¯å¢ƒä¸­å¯¼èˆªã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºè‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚çš„åŸå¸‚ç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªã€‚è¯¥ç ”ç©¶çš„æœªæ¥å½±å“åœ¨äºï¼Œå®ƒå¯ä»¥ä¿ƒè¿›å…·èº«æ™ºèƒ½ä½“çš„å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œé€‚åº”çœŸå®ä¸–ç•Œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid progress of foundation models and robotics, vision-language navigation (VLN) has emerged as a key task for embodied agents with broad practical applications. We address VLN in continuous environments, a particularly challenging setting where an agent must jointly interpret natural language instructions, perceive its surroundings, and plan low-level actions. We propose a zero-shot framework that integrates a simplified yet effective waypoint predictor with a multimodal large language model (MLLM). The predictor operates on an abstract obstacle map, producing linearly reachable waypoints, which are incorporated into a dynamically updated topological graph with explicit visitation records. The graph and visitation information are encoded into the prompt, enabling reasoning over both spatial structure and exploration history to encourage exploration and equip MLLM with local path planning for error correction. Extensive experiments on R2R-CE and RxR-CE show that our method achieves state-of-the-art zero-shot performance, with success rates of 41% and 36%, respectively, outperforming prior state-of-the-art methods.

