---
layout: default
title: Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation
---

# Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20623" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20623v1</a>
  <a href="https://arxiv.org/pdf/2509.20623.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20623v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20623v1', 'Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Satyajeet Das, Darren Chiu, Zhehui Huang, Lars Lindemann, Gaurav S. Sukhatme

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLatent Activation Editingï¼Œç”¨äºå¤šæœºå™¨äººå¯¼èˆªä¸­å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„æ¨ç†æ—¶å®‰å…¨ä¼˜åŒ–ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤šæœºå™¨äººå¯¼èˆª` `å®‰å…¨æ§åˆ¶` `æ¿€æ´»ç¼–è¾‘` `æ¨ç†æ—¶ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ— äººæœºååŒå¯¼èˆªç­‰å¤æ‚é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è®­ç»ƒå¥½çš„ç­–ç•¥åœ¨å¤æ‚ç¯å¢ƒä¸­ä»æ˜“å‘ç”Ÿç¢°æ’ã€‚
2. å—å¤§å‹è¯­è¨€æ¨¡å‹æ¿€æ´»å¼•å¯¼å’Œè®¡ç®—æœºè§†è§‰æ½œåœ¨ç¼–è¾‘çš„å¯å‘ï¼Œæå‡ºLAEæ¡†æ¶ï¼Œåœ¨æ¨ç†æ—¶ä¼˜åŒ–ç­–ç•¥ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLAEèƒ½æ˜¾è‘—å‡å°‘ç¢°æ’æ¬¡æ•°ï¼Œæé«˜æ— ç¢°æ’è½¨è¿¹æ¯”ä¾‹ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡å®Œæˆåº¦ï¼Œä¸”é€‚ç”¨äºèµ„æºå—é™ç¡¬ä»¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨ç†æ—¶æ½œåœ¨æ¿€æ´»ç¼–è¾‘ï¼ˆLAEï¼‰æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–é¢„è®­ç»ƒç­–ç•¥çš„è¡Œä¸ºï¼Œæ— éœ€ä¿®æ”¹å…¶æƒé‡æˆ–æ¶æ„ï¼Œä»è€Œæé«˜å¤šæ— äººæœºå¯¼èˆªçš„å®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š(i) åœ¨çº¿åˆ†ç±»å™¨ç›‘æµ‹ä¸­é—´æ¿€æ´»ï¼Œä»¥æ£€æµ‹ä¸ä¸è‰¯è¡Œä¸ºç›¸å…³çš„çŠ¶æ€ï¼›(ii) æ¿€æ´»ç¼–è¾‘æ¨¡å—é€‰æ‹©æ€§åœ°ä¿®æ”¹æ ‡è®°çš„æ¿€æ´»ï¼Œä»¥å°†ç­–ç•¥è½¬ç§»åˆ°æ›´å®‰å…¨çš„åŒºåŸŸã€‚é€šè¿‡è®­ç»ƒæ½œåœ¨ç¢°æ’ä¸–ç•Œæ¨¡å‹æ¥é¢„æµ‹æœªæ¥çš„ç¢°æ’å‰æ¿€æ´»ï¼Œä»è€Œä¿ƒä½¿æ›´æ—©å’Œæ›´è°¨æ…çš„é¿éšœååº”ï¼Œä»¥æ­¤æ¥å¢å¼ºç­–ç•¥å¯¹é£é™©çš„å†…éƒ¨æ„ŸçŸ¥ã€‚å¤§é‡çš„ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œçš„Crazyflieå®éªŒè¡¨æ˜ï¼ŒLAEåœ¨æ˜¾è‘—å‡å°‘ç¢°æ’ï¼ˆä¸æœªç¼–è¾‘çš„åŸºçº¿ç›¸æ¯”ï¼Œç´¯ç§¯ç¢°æ’å‡å°‘è¿‘90%ï¼‰å¹¶å¤§å¹…å¢åŠ æ— ç¢°æ’è½¨è¿¹çš„æ¯”ä¾‹çš„åŒæ—¶ï¼Œä¿æŒäº†ä»»åŠ¡å®Œæˆåº¦ã€‚LAEæ˜¯ä¸€ç§è½»é‡çº§èŒƒä¾‹ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„ç¡¬ä»¶ï¼Œå¯ç”¨äºå·²éƒ¨ç½²æœºå™¨äººç­–ç•¥çš„åéƒ¨ç½²ä¼˜åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæœºå™¨äººå¯¼èˆªä¸­ï¼Œå³ä½¿ç»è¿‡è‰¯å¥½è®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­ä»ç„¶å­˜åœ¨ç¢°æ’é£é™©ã€‚é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒç­–ç•¥ä»¥è§£å†³è¿™äº›å¶å‘ä½†å…³é”®çš„å®‰å…¨é—®é¢˜ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”å¯èƒ½é™ä½å…ˆå‰å­¦ä¹ åˆ°çš„æŠ€èƒ½ã€‚å› æ­¤ï¼Œéœ€è¦åœ¨ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæé«˜ç°æœ‰ç­–ç•¥çš„å®‰å…¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åœ¨æ¨ç†æ—¶ä¿®æ”¹ç­–ç•¥çš„ä¸­é—´æ¿€æ´»å€¼ï¼Œå¼•å¯¼ç­–ç•¥æœç€æ›´å®‰å…¨çš„æ–¹å‘å‘å±•ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯æ”¾å¤§ç­–ç•¥å¯¹é£é™©çš„å†…éƒ¨æ„ŸçŸ¥ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æ—©ã€æ›´è°¨æ…åœ°åšå‡ºé¿éšœååº”ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªæ½œåœ¨ç¢°æ’ä¸–ç•Œæ¨¡å‹ï¼Œé¢„æµ‹æœªæ¥çš„ç¢°æ’å‰æ¿€æ´»ï¼Œä»è€Œå®ç°å¯¹é£é™©çš„æ„ŸçŸ¥å¢å¼ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLAEæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š(1) **åœ¨çº¿åˆ†ç±»å™¨**ï¼šè¯¥æ¨¡å—ç›‘æµ‹ç­–ç•¥çš„ä¸­é—´æ¿€æ´»ï¼Œåˆ¤æ–­å½“å‰çŠ¶æ€æ˜¯å¦ä¸ä¸è‰¯è¡Œä¸ºï¼ˆå¦‚å³å°†å‘ç”Ÿç¢°æ’ï¼‰ç›¸å…³è”ã€‚å¯ä»¥ä½¿ç”¨ä»»ä½•åˆé€‚çš„åˆ†ç±»å™¨ï¼Œä¾‹å¦‚ç¥ç»ç½‘ç»œæˆ–æ”¯æŒå‘é‡æœºã€‚(2) **æ¿€æ´»ç¼–è¾‘æ¨¡å—**ï¼šå¦‚æœåœ¨çº¿åˆ†ç±»å™¨æ£€æµ‹åˆ°æ½œåœ¨çš„å±é™©çŠ¶æ€ï¼Œè¯¥æ¨¡å—ä¼šé€‰æ‹©æ€§åœ°ä¿®æ”¹ç›¸åº”çš„æ¿€æ´»å€¼ã€‚ä¿®æ”¹çš„ç›®æ ‡æ˜¯å¼•å¯¼ç­–ç•¥é‡‡å–æ›´å®‰å…¨çš„è¡ŒåŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šLAEçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ¨ç†æ—¶æ¿€æ´»ç¼–è¾‘çš„èƒ½åŠ›ï¼Œå®ƒå…è®¸åœ¨ä¸ä¿®æ”¹æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œå¯¹å·²éƒ¨ç½²çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿›è¡Œå¾®è°ƒå’Œä¼˜åŒ–ã€‚è¿™ä¸ä¼ ç»Ÿçš„é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒæ–¹æ³•ä¸åŒï¼Œåè€…éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ï¼Œå¹¶ä¸”å¯èƒ½å¯¼è‡´ç­–ç•¥æ€§èƒ½çš„é€€åŒ–ã€‚LAEæä¾›äº†ä¸€ç§è½»é‡çº§ã€é«˜æ•ˆçš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) **æ½œåœ¨ç¢°æ’ä¸–ç•Œæ¨¡å‹**ï¼šè¯¥æ¨¡å‹ç”¨äºé¢„æµ‹æœªæ¥çš„ç¢°æ’å‰æ¿€æ´»ï¼Œä»è€Œæä¾›é£é™©æ„ŸçŸ¥ä¿¡å·ã€‚æ¨¡å‹çš„è®­ç»ƒæ•°æ®æ¥è‡ªå†å²ç¢°æ’è½¨è¿¹ã€‚(2) **æ¿€æ´»ç¼–è¾‘ç­–ç•¥**ï¼šé€‰æ‹©å“ªäº›æ¿€æ´»è¿›è¡Œä¿®æ”¹ï¼Œä»¥åŠå¦‚ä½•ä¿®æ”¹è¿™äº›æ¿€æ´»ï¼Œæ˜¯å½±å“LAEæ€§èƒ½çš„å…³é”®å› ç´ ã€‚è®ºæ–‡ä¸­é‡‡ç”¨äº†ä¸€ç§åŸºäºæ¢¯åº¦çš„æ¿€æ´»ç¼–è¾‘æ–¹æ³•ï¼Œæ ¹æ®æ½œåœ¨ç¢°æ’ä¸–ç•Œæ¨¡å‹çš„æ¢¯åº¦ä¿¡æ¯ï¼Œè°ƒæ•´æ¿€æ´»å€¼ï¼Œä»¥é™ä½ç¢°æ’é£é™©ã€‚(3) **åœ¨çº¿åˆ†ç±»å™¨é˜ˆå€¼**ï¼šéœ€è¦ä»”ç»†è°ƒæ•´åœ¨çº¿åˆ†ç±»å™¨çš„é˜ˆå€¼ï¼Œä»¥å¹³è¡¡è¯¯æŠ¥å’Œæ¼æŠ¥çš„é£é™©ã€‚è¿‡é«˜çš„é˜ˆå€¼å¯èƒ½å¯¼è‡´æ¼æŠ¥ï¼Œæ— æ³•åŠæ—¶è¿›è¡Œæ¿€æ´»ç¼–è¾‘ï¼›è¿‡ä½çš„é˜ˆå€¼å¯èƒ½å¯¼è‡´è¯¯æŠ¥ï¼Œä¸å¿…è¦åœ°ä¿®æ”¹æ¿€æ´»å€¼ï¼Œå½±å“ç­–ç•¥çš„æ­£å¸¸è¡Œä¸ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLAEèƒ½å¤Ÿæ˜¾è‘—å‡å°‘å¤šæ— äººæœºå¯¼èˆªä¸­çš„ç¢°æ’æ¬¡æ•°ã€‚åœ¨ä»¿çœŸç¯å¢ƒä¸­ï¼Œä¸æœªç¼–è¾‘çš„åŸºçº¿ç›¸æ¯”ï¼ŒLAEä½¿ç´¯ç§¯ç¢°æ’æ¬¡æ•°å‡å°‘äº†è¿‘90%ã€‚åœ¨çœŸå®ä¸–ç•Œçš„Crazyflieå®éªŒä¸­ï¼ŒLAEä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„ç¢°æ’å‡å°‘æ•ˆæœï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„ä»»åŠ¡å®Œæˆç‡ã€‚è¿™äº›ç»“æœéªŒè¯äº†LAEåœ¨æé«˜å¼ºåŒ–å­¦ä¹ ç­–ç•¥å®‰å…¨æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LAEå¯åº”ç”¨äºå„ç§æœºå™¨äººå¯¼èˆªåœºæ™¯ï¼Œç‰¹åˆ«æ˜¯é‚£äº›å®‰å…¨è‡³å…³é‡è¦çš„åœºæ™¯ï¼Œå¦‚æ— äººæœºé…é€ã€è‡ªåŠ¨é©¾é©¶å’Œå·¥ä¸šæœºå™¨äººã€‚å®ƒè¿˜å¯ç”¨äºå…¶ä»–å¼ºåŒ–å­¦ä¹ åº”ç”¨ï¼Œä¾‹å¦‚æ¸¸æˆå’Œé‡‘èäº¤æ˜“ï¼Œä»¥æé«˜ç­–ç•¥çš„é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„åµŒå…¥å¼ç³»ç»Ÿï¼Œä¸ºå·²éƒ¨ç½²çš„æœºå™¨äººç­–ç•¥æä¾›äº†ä¸€ç§è½»é‡çº§çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning has enabled significant progress in complex domains such as coordinating and navigating multiple quadrotors. However, even well-trained policies remain vulnerable to collisions in obstacle-rich environments. Addressing these infrequent but critical safety failures through retraining or fine-tuning is costly and risks degrading previously learned skills. Inspired by activation steering in large language models and latent editing in computer vision, we introduce a framework for inference-time Latent Activation Editing (LAE) that refines the behavior of pre-trained policies without modifying their weights or architecture. The framework operates in two stages: (i) an online classifier monitors intermediate activations to detect states associated with undesired behaviors, and (ii) an activation editing module that selectively modifies flagged activations to shift the policy towards safer regimes. In this work, we focus on improving safety in multi-quadrotor navigation. We hypothesize that amplifying a policy's internal perception of risk can induce safer behaviors. We instantiate this idea through a latent collision world model trained to predict future pre-collision activations, thereby prompting earlier and more cautious avoidance responses. Extensive simulations and real-world Crazyflie experiments demonstrate that LAE achieves statistically significant reduction in collisions (nearly 90% fewer cumulative collisions compared to the unedited baseline) and substantially increases the fraction of collision-free trajectories, while preserving task completion. More broadly, our results establish LAE as a lightweight paradigm, feasible on resource-constrained hardware, for post-deployment refinement of learned robot policies.

