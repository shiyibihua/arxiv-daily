---
layout: default
title: Generalist Robot Manipulation beyond Action Labeled Data
---

# Generalist Robot Manipulation beyond Action Labeled Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19958" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19958v1</a>
  <a href="https://arxiv.org/pdf/2509.19958.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19958v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19958v1', 'Generalist Robot Manipulation beyond Action Labeled Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexander Spiridonov, Jan-Nico Zaech, Nikolay Nikolov, Luc Van Gool, Danda Pani Paudel

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24

**å¤‡æ³¨**: Accepted at Conference on Robot Learning 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åˆ©ç”¨æ— åŠ¨ä½œæ ‡ç­¾æ•°æ®çš„é€šç”¨æœºå™¨äººæ“ä½œæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è‡ªç›‘ç£å­¦ä¹ ` `æ— æ ‡ç­¾æ•°æ®` `3DåŠ¨åŠ›å­¦é¢„æµ‹` `é€šç”¨æœºå™¨äºº` `è§†è§‰-è¯­è¨€æ¨¡å‹` `åŠ¨ä½œé¢„æµ‹` `æ•°æ®é«˜æ•ˆå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é€šç”¨æœºå™¨äººæ“ä½œæ–¹æ³•ä¾èµ–å¤§é‡å¸¦åŠ¨ä½œæ ‡ç­¾çš„æ•°æ®ï¼Œè·å–æˆæœ¬é«˜æ˜‚ä¸”é™åˆ¶äº†æ³›åŒ–èƒ½åŠ›ã€‚
2. è¯¥æ–¹æ³•åˆ©ç”¨æ— åŠ¨ä½œæ ‡ç­¾çš„äººç±»å’Œæœºå™¨äººè§†é¢‘ï¼Œé€šè¿‡3DåŠ¨åŠ›å­¦é¢„æµ‹è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œæå‡æ•°æ®æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½æå‡é€šç”¨æœºå™¨äººç­–ç•¥ï¼Œè¿˜èƒ½ä½¿æœºå™¨äººåœ¨æ— åŠ¨ä½œæ ‡ç­¾æƒ…å†µä¸‹å­¦ä¹ æ–°ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰é€šç”¨æœºå™¨äººæ“ä½œçš„è¿›å±•ä¾èµ–äºé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¤§è§„æ¨¡æœºå™¨äººæ¼”ç¤ºï¼Œä»¥é›¶æ ·æœ¬æ–¹å¼å¤„ç†å„ç§ä»»åŠ¡ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ï¼šæ‰©å±•é«˜è´¨é‡ã€å¸¦æœ‰åŠ¨ä½œæ ‡ç­¾çš„æœºå™¨äººæ¼”ç¤ºæ•°æ®ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºè¿™äº›æ•°æ®æ¥å®ç°é²æ£’æ€§å’Œæ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å—ç›Šäºæ²¡æœ‰åŠ¨ä½œæ ‡ç­¾çš„è§†é¢‘â€”â€”åŒ…æ‹¬äººç±»å’Œ/æˆ–æœºå™¨äººçš„åŠ¨ä½œâ€”â€”ä»è€Œå¢å¼ºäº†å¼€æ”¾è¯æ±‡è¡¨çš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†æ–°ä»»åŠ¡çš„æ•°æ®é«˜æ•ˆå­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•æå–æ‰‹æˆ–å¤¹å…·ä½ç½®çš„å¯†é›†ã€åŠ¨æ€3Dç‚¹äº‘ï¼Œå¹¶ä½¿ç”¨æå‡ºçš„3DåŠ¨åŠ›å­¦é¢„æµ‹å™¨è¿›è¡Œè‡ªç›‘ç£ã€‚ç„¶åï¼Œä½¿ç”¨è¾ƒå°çš„æ ‡è®°æ•°æ®é›†å°†è¯¥é¢„æµ‹å™¨è°ƒæ•´ä¸ºåŠ¨ä½œé¢„æµ‹å™¨ï¼Œä»¥è¿›è¡ŒåŠ¨ä½œå¯¹é½ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å¯ä»¥ä»æ— æ ‡ç­¾çš„äººç±»å’Œæœºå™¨äººæ¼”ç¤ºä¸­å­¦ä¹ â€”â€”ä»è€Œæ”¹è¿›ä¸‹æ¸¸çš„é€šç”¨æœºå™¨äººç­–ç•¥â€”â€”è€Œä¸”è¿˜ä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨çœŸå®å’Œæ¨¡æ‹Ÿç¯å¢ƒä¸­å­¦ä¹ æ²¡æœ‰åŠ¨ä½œæ ‡ç­¾çš„æ–°ä»»åŠ¡ï¼ˆå³ï¼Œè¶…å‡ºåŠ¨ä½œçš„æ³›åŒ–ï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é€šç”¨æœºå™¨äººæ“ä½œæ–¹æ³•ä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„ã€å¸¦æœ‰åŠ¨ä½œæ ‡ç­¾çš„æœºå™¨äººæ¼”ç¤ºæ•°æ®ã€‚ç„¶è€Œï¼Œè·å–è¿™äº›æ•°æ®çš„æˆæœ¬éå¸¸é«˜æ˜‚ï¼Œå¹¶ä¸”æ ‡æ³¨åŠ¨ä½œæ ‡ç­¾æœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ªè€—æ—¶ä¸”å®¹æ˜“å‡ºé”™çš„è¿‡ç¨‹ã€‚è¿™é™åˆ¶äº†é€šç”¨æœºå™¨äººæ“ä½œçš„æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶éš¾ä»¥é€‚åº”æ–°çš„ã€æœªè§è¿‡çš„ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§é‡æœªæ ‡æ³¨çš„è§†é¢‘æ•°æ®ï¼ˆåŒ…å«äººç±»å’Œ/æˆ–æœºå™¨äººçš„åŠ¨ä½œï¼‰è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œä»è€Œå‡å°‘å¯¹å¸¦æ ‡ç­¾æ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡å­¦ä¹ æ‰‹éƒ¨æˆ–å¤¹å…·çš„3DåŠ¨åŠ›å­¦ä¿¡æ¯ï¼Œæœºå™¨äººå¯ä»¥ç†è§£åŠ¨ä½œçš„å†…åœ¨è§„å¾‹ï¼Œå³ä½¿æ²¡æœ‰æ˜ç¡®çš„åŠ¨ä½œæ ‡ç­¾ä¹Ÿèƒ½è¿›è¡Œæ“ä½œã€‚ç„¶åï¼Œä½¿ç”¨å°‘é‡å¸¦æ ‡ç­¾çš„æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå°†å­¦ä¹ åˆ°çš„åŠ¨åŠ›å­¦ä¿¡æ¯ä¸å…·ä½“çš„åŠ¨ä½œç±»åˆ«å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **æ•°æ®æ”¶é›†**ï¼šæ”¶é›†å¤§é‡æ— åŠ¨ä½œæ ‡ç­¾çš„è§†é¢‘æ•°æ®ï¼ŒåŒ…å«äººç±»å’Œ/æˆ–æœºå™¨äººçš„æ“ä½œè¿‡ç¨‹ã€‚2) **3Dç‚¹äº‘æå–**ï¼šä»è§†é¢‘ä¸­æå–æ‰‹éƒ¨æˆ–å¤¹å…·ä½ç½®çš„å¯†é›†ã€åŠ¨æ€3Dç‚¹äº‘ã€‚3) **3DåŠ¨åŠ›å­¦é¢„æµ‹å™¨è®­ç»ƒ**ï¼šä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ çš„æ–¹å¼è®­ç»ƒä¸€ä¸ª3DåŠ¨åŠ›å­¦é¢„æµ‹å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿé¢„æµ‹æœªæ¥æ—¶åˆ»çš„ç‚¹äº‘çŠ¶æ€ã€‚4) **åŠ¨ä½œé¢„æµ‹å™¨å¾®è°ƒ**ï¼šä½¿ç”¨å°‘é‡å¸¦æ ‡ç­¾çš„æ•°æ®ï¼Œå°†3DåŠ¨åŠ›å­¦é¢„æµ‹å™¨å¾®è°ƒä¸ºä¸€ä¸ªåŠ¨ä½œé¢„æµ‹å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿé¢„æµ‹å½“å‰åŠ¨ä½œçš„ç±»åˆ«ã€‚5) **ç­–ç•¥å­¦ä¹ **ï¼šä½¿ç”¨å­¦ä¹ åˆ°çš„åŠ¨ä½œé¢„æµ‹å™¨ï¼Œè®­ç»ƒä¸€ä¸ªé€šç”¨æœºå™¨äººæ“ä½œç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åˆ©ç”¨æ— åŠ¨ä½œæ ‡ç­¾æ•°æ®è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œä»è€Œå‡å°‘äº†å¯¹å¸¦æ ‡ç­¾æ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡å­¦ä¹ æ‰‹éƒ¨æˆ–å¤¹å…·çš„3DåŠ¨åŠ›å­¦ä¿¡æ¯ï¼Œæœºå™¨äººå¯ä»¥ç†è§£åŠ¨ä½œçš„å†…åœ¨è§„å¾‹ï¼Œå³ä½¿æ²¡æœ‰æ˜ç¡®çš„åŠ¨ä½œæ ‡ç­¾ä¹Ÿèƒ½è¿›è¡Œæ“ä½œã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜æ•°æ®æ•ˆç‡ï¼Œå¹¶ä½¿æœºå™¨äººèƒ½å¤Ÿé€‚åº”æ–°çš„ã€æœªè§è¿‡çš„ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼š3DåŠ¨åŠ›å­¦é¢„æµ‹å™¨é‡‡ç”¨äº†ä¸€ç§åŸºäºTransformerçš„ç½‘ç»œç»“æ„ï¼Œè¾“å…¥æ˜¯å½“å‰æ—¶åˆ»çš„ç‚¹äº‘çŠ¶æ€ï¼Œè¾“å‡ºæ˜¯æœªæ¥æ—¶åˆ»çš„ç‚¹äº‘çŠ¶æ€ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨Chamfer Distanceï¼Œç”¨äºè¡¡é‡é¢„æµ‹ç‚¹äº‘å’ŒçœŸå®ç‚¹äº‘ä¹‹é—´çš„å·®å¼‚ã€‚åŠ¨ä½œé¢„æµ‹å™¨çš„å¾®è°ƒé‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡é¢„æµ‹åŠ¨ä½œç±»åˆ«å’ŒçœŸå®åŠ¨ä½œç±»åˆ«ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨çœŸå®æœºå™¨äººå®éªŒä¸­ï¼Œä½¿ç”¨äº†RGB-Dç›¸æœºè·å–ç¯å¢ƒä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨PIDæ§åˆ¶å™¨æ§åˆ¶æœºå™¨äººçš„è¿åŠ¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜é€šç”¨æœºå™¨äººæ“ä½œçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨çœŸå®æœºå™¨äººç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿æœºå™¨äººåœ¨æ²¡æœ‰åŠ¨ä½œæ ‡ç­¾çš„æƒ…å†µä¸‹å­¦ä¹ æ–°çš„ä»»åŠ¡ï¼Œå¹¶ä¸”æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿåˆ©ç”¨æ— æ ‡ç­¾çš„äººç±»æ¼”ç¤ºæ•°æ®æ¥æé«˜æœºå™¨äººçš„æ“ä½œèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå¯ä»¥æ›´å¿«é€Ÿã€æ›´ç»æµåœ°è®­ç»ƒæœºå™¨äººå®Œæˆå„ç§å¤æ‚ä»»åŠ¡ï¼Œä¾‹å¦‚äº§å“ç»„è£…ã€ç‰©å“æ•´ç†ã€ç—…äººæŠ¤ç†ç­‰ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›å®ç°æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸‹çš„è‡ªä¸»å­¦ä¹ å’Œæ“ä½œï¼Œæå¤§åœ°æ‹“å±•æœºå™¨äººçš„åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in generalist robot manipulation leverage pre-trained Vision-Language Models (VLMs) and large-scale robot demonstrations to tackle diverse tasks in a zero-shot manner. A key challenge remains: scaling high-quality, action-labeled robot demonstration data, which existing methods rely on for robustness and generalization. To address this, we propose a method that benefits from videos without action labels - featuring humans and/or robots in action - enhancing open-vocabulary performance and enabling data-efficient learning of new tasks. Our method extracts dense, dynamic 3D point clouds at the hand or gripper location and uses a proposed 3D dynamics predictor for self-supervision. This predictor is then tuned to an action predictor using a smaller labeled dataset for action alignment. We show that our method not only learns from unlabeled human and robot demonstrations - improving downstream generalist robot policies - but also enables robots to learn new tasks without action labels (i.e., out-of-action generalization) in both real-world and simulated settings.

