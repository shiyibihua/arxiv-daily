---
layout: default
title: Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training
---

# Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19752" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19752v2</a>
  <a href="https://arxiv.org/pdf/2509.19752.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19752v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19752v2', 'Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rushuai Yang, Hangxing Wei, Ran Zhang, Zhiyuan Feng, Xiaoyu Chen, Tong Li, Chuheng Zhang, Li Zhao, Jiang Bian, Xiu Su, Yi Chen

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24 (æ›´æ–°: 2025-09-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä¸ºVLAæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººæ“ä½œ` `é•¿ç¨‹ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. VLAæ¨¡å‹ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ä¸”é™åˆ¶äº†æ¨¡å‹æ‰©å±•æ€§ï¼Œå› æ­¤éœ€è¦å¯»æ‰¾è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®çš„æ–¹æ³•ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ”¹è¿›çš„æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ã€ä½æ–¹å·®çš„è½¨è¿¹ï¼Œç”¨äºVLAæ¨¡å‹çš„è®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„è½¨è¿¹æ¯”äººå·¥æ•°æ®å’Œä¼ ç»ŸRLæ–¹æ³•æ›´å¹³æ»‘ï¼ŒVLAæ¨¡å‹åœ¨ç”Ÿæˆæ•°æ®ä¸Šè®­ç»ƒåæ€§èƒ½æå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨è·¨ä»»åŠ¡å’Œè·¨å…·èº«æ™ºèƒ½æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹å¤§è§„æ¨¡äººå·¥æ¼”ç¤ºæ•°æ®çš„ä¾èµ–é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ï¼Œå› ä¸ºæ‰‹åŠ¨æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ã€‚å¼ºåŒ–å­¦ä¹ (RL)ä¸ºè‡ªä¸»ç”Ÿæˆæ¼”ç¤ºæ•°æ®æä¾›äº†ä¸€ç§æ½œåœ¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ä¼ ç»Ÿçš„RLç®—æ³•é€šå¸¸éš¾ä»¥åº”å¯¹å…·æœ‰ç¨€ç–å¥–åŠ±çš„é•¿ç¨‹æ“ä½œä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡å’Œä½æ–¹å·®çš„è½¨è¿¹ï¼Œä»è€Œæ„å»ºä¸€ä¸ªåŸºäºæ‰©æ•£RLçš„VLAè®­ç»ƒæµç¨‹ã€‚è¯¥ç®—æ³•ä¸ä»…å—ç›Šäºæ‰©æ•£æ¨¡å‹åœ¨æ¢ç´¢å¤æ‚å’Œå¤šæ ·åŒ–è¡Œä¸ºæ–¹é¢çš„é«˜è¡¨è¾¾æ€§ï¼Œè¿˜å—ç›Šäºè¿­ä»£å»å™ªè¿‡ç¨‹çš„éšå¼æ­£åˆ™åŒ–ï¼Œä»è€Œäº§ç”Ÿå¹³æ»‘å’Œä¸€è‡´çš„æ¼”ç¤ºã€‚æˆ‘ä»¬åœ¨åŒ…å«130ä¸ªé•¿ç¨‹æ“ä½œä»»åŠ¡çš„LIBEROåŸºå‡†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œç”Ÿæˆçš„è½¨è¿¹æ¯”äººå·¥æ¼”ç¤ºå’Œæ ‡å‡†é«˜æ–¯RLç­–ç•¥ç”Ÿæˆçš„è½¨è¿¹æ›´å¹³æ»‘å’Œä¸€è‡´ã€‚æ­¤å¤–ï¼Œä»…åœ¨æ‰©æ•£RLç”Ÿæˆçš„æ•°æ®ä¸Šè®­ç»ƒVLAæ¨¡å‹ï¼Œå¹³å‡æˆåŠŸç‡è¾¾åˆ°81.9%ï¼Œæ¯”åœ¨äººå·¥æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹é«˜+5.3%ï¼Œæ¯”åœ¨é«˜æ–¯RLç”Ÿæˆçš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹é«˜+12.6%ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ‰©æ•£RLæ˜¯ä¸ºVLAæ¨¡å‹ç”Ÿæˆä¸°å¯Œã€é«˜è´¨é‡å’Œä½æ–¹å·®æ¼”ç¤ºæ•°æ®çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVLAæ¨¡å‹ä¾èµ–äºå¤§é‡äººå·¥æ ‡æ³¨çš„æ¼”ç¤ºæ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿ç¨‹æ“ä½œä»»åŠ¡ä¸­ï¼Œç”±äºç¨€ç–å¥–åŠ±å’Œæ¢ç´¢å›°éš¾ï¼Œéš¾ä»¥ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€ä½æ–¹å·®è½¨è¿¹çš„æ–¹æ³•ï¼Œä»¥æ›¿ä»£äººå·¥æ ‡æ³¨æ•°æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›å’Œéšå¼æ­£åˆ™åŒ–ç‰¹æ€§ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è½¨è¿¹ã€‚é€šè¿‡æ”¹è¿›æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä½¿å¾—ç”Ÿæˆçš„è½¨è¿¹æ›´åŠ å¹³æ»‘å’Œä¸€è‡´ï¼Œä»è€Œæé«˜VLAæ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚æ ¸å¿ƒåœ¨äºå°†å¼ºåŒ–å­¦ä¹ ä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œå…‹æœä¼ ç»ŸRLåœ¨é•¿ç¨‹ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£RLçš„VLAè®­ç»ƒæµç¨‹ã€‚é¦–å…ˆï¼Œä½¿ç”¨æ”¹è¿›çš„æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ç®—æ³•ç”Ÿæˆè½¨è¿¹æ•°æ®ã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›ç”Ÿæˆçš„æ•°æ®è®­ç»ƒVLAæ¨¡å‹ã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ç®—æ³•ç”Ÿæˆè½¨è¿¹ï¼›2) VLAæ¨¡å‹ä½¿ç”¨ç”Ÿæˆè½¨è¿¹è¿›è¡Œè®­ç»ƒï¼›3) åœ¨LIBEROåŸºå‡†ä¸Šè¯„ä¼°VLAæ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå°†æ‰©æ•£æ¨¡å‹å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼Œç”¨äºç”ŸæˆVLAæ¨¡å‹çš„è®­ç»ƒæ•°æ®ã€‚ä¸ä¼ ç»Ÿçš„é«˜æ–¯RLç­–ç•¥ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹å…·æœ‰æ›´é«˜çš„è¡¨è¾¾èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ¢ç´¢æ›´å¤æ‚å’Œå¤šæ ·åŒ–çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œæ‰©æ•£æ¨¡å‹çš„è¿­ä»£å»å™ªè¿‡ç¨‹å…·æœ‰éšå¼æ­£åˆ™åŒ–ä½œç”¨ï¼Œå¯ä»¥ç”Ÿæˆæ›´å¹³æ»‘å’Œä¸€è‡´çš„è½¨è¿¹ã€‚è¿™ä½¿å¾—ç”Ÿæˆçš„æ•°æ®æ›´é€‚åˆVLAæ¨¡å‹çš„è®­ç»ƒã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¿®æ”¹äº†æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä½¿å…¶æ›´é€‚åˆç”Ÿæˆé«˜è´¨é‡çš„è½¨è¿¹ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œä½†å¼ºè°ƒäº†åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç‰¹æ€§æ¥ç”Ÿæˆå¹³æ»‘å’Œä¸€è‡´çš„è½¨è¿¹ã€‚LIBEROåŸºå‡†åŒ…å«130ä¸ªé•¿ç¨‹æ“ä½œä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆè½¨è¿¹çš„è´¨é‡å’ŒVLAæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„è½¨è¿¹æ¯”äººå·¥æ•°æ®å’Œé«˜æ–¯RLç­–ç•¥ç”Ÿæˆçš„è½¨è¿¹æ›´å¹³æ»‘å’Œä¸€è‡´ã€‚ä»…ä½¿ç”¨æ‰©æ•£RLç”Ÿæˆçš„æ•°æ®è®­ç»ƒVLAæ¨¡å‹ï¼Œå¹³å‡æˆåŠŸç‡è¾¾åˆ°81.9%ï¼Œæ¯”åœ¨äººå·¥æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹é«˜+5.3%ï¼Œæ¯”åœ¨é«˜æ–¯RLç”Ÿæˆçš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹é«˜+12.6%ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡VLAæ¨¡å‹è®­ç»ƒæ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œé™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒåŠ é€ŸVLAæ¨¡å‹çš„å¼€å‘å’Œéƒ¨ç½²ã€‚å°¤å…¶åœ¨å¤æ‚ã€é•¿ç¨‹ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œæœ‰æœ›æ¨åŠ¨æœºå™¨äººæ™ºèƒ½æ°´å¹³çš„æå‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language-action (VLA) models have shown strong generalization across tasks and embodiments; however, their reliance on large-scale human demonstrations limits their scalability owing to the cost and effort of manual data collection. Reinforcement learning (RL) offers a potential alternative to generate demonstrations autonomously, yet conventional RL algorithms often struggle on long-horizon manipulation tasks with sparse rewards. In this paper, we propose a modified diffusion policy optimization algorithm to generate high-quality and low-variance trajectories, which contributes to a diffusion RL-powered VLA training pipeline. Our algorithm benefits from not only the high expressiveness of diffusion models to explore complex and diverse behaviors but also the implicit regularization of the iterative denoising process to yield smooth and consistent demonstrations. We evaluate our approach on the LIBERO benchmark, which includes 130 long-horizon manipulation tasks, and show that the generated trajectories are smoother and more consistent than both human demonstrations and those from standard Gaussian RL policies. Further, training a VLA model exclusively on the diffusion RL-generated data achieves an average success rate of 81.9%, which outperforms the model trained on human data by +5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight our diffusion RL as an effective alternative for generating abundant, high-quality, and low-variance demonstrations for VLA models.

