---
layout: default
title: RoboSSM: Scalable In-context Imitation Learning via State-Space Models
---

# RoboSSM: Scalable In-context Imitation Learning via State-Space Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19658" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19658v1</a>
  <a href="https://arxiv.org/pdf/2509.19658.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19658v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19658v1', 'RoboSSM: Scalable In-context Imitation Learning via State-Space Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Youngju Yoo, Jiaheng Hu, Yifeng Zhu, Bo Liu, Qiang Liu, Roberto MartÃ­n-MartÃ­n, Peter Stone

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24

**å¤‡æ³¨**: 8 pages, 11 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/youngjuY/RoboSSM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RoboSSMï¼šåŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹å®ç°å¯æ‰©å±•çš„ä¸Šä¸‹æ–‡æ¨¡ä»¿å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡æ¨¡ä»¿å­¦ä¹ ` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `æœºå™¨äººå­¦ä¹ ` `é•¿åºåˆ—å»ºæ¨¡` `å°‘æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºTransformerçš„ä¸Šä¸‹æ–‡æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨è®¡ç®—ä¸Šå­˜åœ¨ç“¶é¢ˆï¼Œä¸”éš¾ä»¥å¤„ç†é•¿åºåˆ—çš„æ¼”ç¤ºæ•°æ®ã€‚
2. RoboSSMåˆ©ç”¨LonghornçŠ¶æ€ç©ºé—´æ¨¡å‹æ›¿ä»£Transformerï¼Œå®ç°çº¿æ€§æ—¶é—´æ¨ç†å’Œæ›´å¼ºçš„å¤–æ¨èƒ½åŠ›ï¼Œä»è€Œæå‡æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRoboSSMåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹ä¸åŒæ•°é‡çš„æ¼”ç¤ºæ•°æ®å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºRoboSSMï¼Œä¸€ç§åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„å¯æ‰©å±•ä¸Šä¸‹æ–‡æ¨¡ä»¿å­¦ä¹ ï¼ˆICILï¼‰æ–¹æ³•ã€‚ICILå…è®¸æœºå™¨äººä»…ä»å°‘é‡æ¼”ç¤ºæç¤ºä¸­å­¦ä¹ ä»»åŠ¡ï¼Œæ— éœ€éƒ¨ç½²æ—¶è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œä»è€Œæ”¯æŒå¯¹æ–°ä»»åŠ¡çš„å°‘æ ·æœ¬é€‚åº”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ICILæ–¹æ³•ä¾èµ–äºTransformerï¼Œå­˜åœ¨è®¡ç®—é™åˆ¶ï¼Œå¹¶ä¸”åœ¨å¤„ç†æ¯”è®­ç»ƒæœŸé—´æ›´é•¿çš„æç¤ºæ—¶è¡¨ç°ä¸ä½³ã€‚RoboSSMç”¨Longhornï¼ˆä¸€ç§å…ˆè¿›çš„SSMï¼‰å–ä»£Transformerï¼ŒLonghornæä¾›çº¿æ€§æ—¶é—´æ¨ç†å’Œå¼ºå¤§çš„å¤–æ¨èƒ½åŠ›ï¼Œéå¸¸é€‚åˆé•¿ä¸Šä¸‹æ–‡æç¤ºã€‚åœ¨LIBEROåŸºå‡†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œå¹¶ä¸åŸºäºTransformerçš„ICILåŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒè¡¨æ˜ï¼ŒRoboSSMèƒ½æœ‰æ•ˆåœ°å¤–æ¨åˆ°ä¸åŒæ•°é‡çš„ä¸Šä¸‹æ–‡æ¼”ç¤ºï¼Œåœ¨æ–°ä»»åŠ¡ä¸Šäº§ç”Ÿé«˜æ€§èƒ½ï¼Œå¹¶åœ¨é•¿æ—¶ç¨‹åœºæ™¯ä¸­ä¿æŒç¨³å¥ã€‚è¿™äº›ç»“æœçªå‡ºäº†SSMä½œä¸ºICILé«˜æ•ˆä¸”å¯æ‰©å±•éª¨å¹²ç½‘ç»œçš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„ä¸Šä¸‹æ–‡æ¨¡ä»¿å­¦ä¹ ï¼ˆICILï¼‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„æ–¹æ³•ï¼Œåœ¨å¤„ç†é•¿åºåˆ—çš„æ¼”ç¤ºæ•°æ®æ—¶é¢ä¸´è®¡ç®—å¤æ‚åº¦é«˜å’Œå¤–æ¨èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚Transformerçš„è®¡ç®—å¤æ‚åº¦éšåºåˆ—é•¿åº¦å‘ˆå¹³æ–¹å¢é•¿ï¼Œé™åˆ¶äº†å…¶åœ¨é•¿æ—¶ç¨‹æœºå™¨äººä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œå½“æµ‹è¯•æ—¶ä½¿ç”¨çš„æ¼”ç¤ºæ•°é‡è¶…è¿‡è®­ç»ƒæ—¶ä½¿ç”¨çš„æ•°é‡æ—¶ï¼ŒTransformerçš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRoboSSMçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œç‰¹åˆ«æ˜¯Longhorn SSMï¼Œæ¥æ›¿ä»£Transformerä½œä¸ºICILçš„éª¨å¹²ç½‘ç»œã€‚SSMå…·æœ‰çº¿æ€§æ—¶é—´æ¨ç†çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°å¤„ç†é•¿åºåˆ—æ•°æ®ã€‚Longhorn SSMè¿›ä¸€æ­¥å¢å¼ºäº†SSMçš„å¤–æ¨èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°ä¸åŒæ•°é‡çš„æ¼”ç¤ºæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoboSSMçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªLonghorn SSMç¼–ç å™¨ï¼Œç”¨äºå¤„ç†è¾“å…¥çš„æ¼”ç¤ºæ•°æ®åºåˆ—ï¼Œä»¥åŠä¸€ä¸ªç­–ç•¥ç½‘ç»œï¼Œç”¨äºæ ¹æ®ç¼–ç åçš„çŠ¶æ€é¢„æµ‹æœºå™¨äººçš„åŠ¨ä½œã€‚è¾“å…¥çš„æ¼”ç¤ºæ•°æ®åºåˆ—åŒ…å«ä¸€ç³»åˆ—çŠ¶æ€-åŠ¨ä½œå¯¹ã€‚Longhorn SSMç¼–ç å™¨å°†è¿™äº›çŠ¶æ€-åŠ¨ä½œå¯¹ç¼–ç æˆä¸€ä¸ªä½ç»´çš„çŠ¶æ€å‘é‡ï¼Œè¯¥çŠ¶æ€å‘é‡éšåè¢«è¾“å…¥åˆ°ç­–ç•¥ç½‘ç»œä¸­ã€‚ç­–ç•¥ç½‘ç»œè¾“å‡ºæœºå™¨äººçš„åŠ¨ä½œï¼Œè¯¥åŠ¨ä½œæ—¨åœ¨æ¨¡ä»¿æ¼”ç¤ºæ•°æ®ä¸­çš„è¡Œä¸ºã€‚

**å…³é”®åˆ›æ–°**ï¼šRoboSSMçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨Longhorn SSMä½œä¸ºICILçš„éª¨å¹²ç½‘ç»œï¼Œä»è€Œå®ç°äº†çº¿æ€§æ—¶é—´æ¨ç†å’Œæ›´å¼ºçš„å¤–æ¨èƒ½åŠ›ã€‚ä¸åŸºäºTransformerçš„æ–¹æ³•ç›¸æ¯”ï¼ŒRoboSSMèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°å¤„ç†é•¿åºåˆ—çš„æ¼”ç¤ºæ•°æ®ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°ä¸åŒæ•°é‡çš„æ¼”ç¤ºæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šRoboSSMçš„å…³é”®è®¾è®¡åŒ…æ‹¬é€‰æ‹©Longhorn SSMä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œä»¥åŠè®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ã€‚æŸå¤±å‡½æ•°æ—¨åœ¨æœ€å°åŒ–é¢„æµ‹åŠ¨ä½œä¸æ¼”ç¤ºåŠ¨ä½œä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†ä¸åŒçš„ç½‘ç»œç»“æ„å’Œè¶…å‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚å¯ä»¥åœ¨è®ºæ–‡çš„å®éªŒéƒ¨åˆ†æ‰¾åˆ°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RoboSSMåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚ä¸åŸºäºTransformerçš„ICILåŸºçº¿ç›¸æ¯”ï¼ŒRoboSSMåœ¨å¤„ç†é•¿åºåˆ—æ•°æ®å’Œæ³›åŒ–åˆ°ä¸åŒæ•°é‡çš„æ¼”ç¤ºæ•°æ®æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚å…·ä½“è€Œè¨€ï¼ŒRoboSSMåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡äº†çº¦10%-20%ã€‚æ­¤å¤–ï¼ŒRoboSSMè¿˜èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤–æ¨åˆ°æ¯”è®­ç»ƒæœŸé—´æ›´é•¿çš„åºåˆ—ï¼Œè¿™è¡¨æ˜å…¶å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RoboSSMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºè®­ç»ƒæœºå™¨äººæ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è£…é…ã€å¯¼èˆªå’ŒæŠ“å–ã€‚é€šè¿‡åˆ©ç”¨å°‘é‡çš„æ¼”ç¤ºæ•°æ®ï¼ŒRoboSSMå¯ä»¥å¿«é€Ÿé€‚åº”æ–°çš„ä»»åŠ¡å’Œç¯å¢ƒï¼Œä»è€Œé™ä½äº†æœºå™¨äººå¼€å‘çš„æˆæœ¬å’Œæ—¶é—´ã€‚æ­¤å¤–ï¼ŒRoboSSMè¿˜å¯ä»¥ç”¨äºå¼€å‘æ›´æ™ºèƒ½çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå’Œæ¸¸æˆAIï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ¨¡ä»¿äººç±»çš„è¡Œä¸ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context imitation learning (ICIL) enables robots to learn tasks from prompts consisting of just a handful of demonstrations. By eliminating the need for parameter updates at deployment time, this paradigm supports few-shot adaptation to novel tasks. However, recent ICIL methods rely on Transformers, which have computational limitations and tend to underperform when handling longer prompts than those seen during training. In this work, we introduce RoboSSM, a scalable recipe for in-context imitation learning based on state-space models (SSM). Specifically, RoboSSM replaces Transformers with Longhorn -- a state-of-the-art SSM that provides linear-time inference and strong extrapolation capabilities, making it well-suited for long-context prompts. We evaluate our approach on the LIBERO benchmark and compare it against strong Transformer-based ICIL baselines. Experiments show that RoboSSM extrapolates effectively to varying numbers of in-context demonstrations, yields high performance on unseen tasks, and remains robust in long-horizon scenarios. These results highlight the potential of SSMs as an efficient and scalable backbone for ICIL. Our code is available at https://github.com/youngjuY/RoboSSM.

