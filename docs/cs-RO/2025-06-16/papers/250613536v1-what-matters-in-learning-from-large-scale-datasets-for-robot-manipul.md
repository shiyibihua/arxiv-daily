---
layout: default
title: What Matters in Learning from Large-Scale Datasets for Robot Manipulation
---

# What Matters in Learning from Large-Scale Datasets for Robot Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13536" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13536v1</a>
  <a href="https://arxiv.org/pdf/2506.13536.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13536v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13536v1', 'What Matters in Learning from Large-Scale Datasets for Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vaibhav Saxena, Matthew Bronars, Nadun Ranawaka Arachchige, Kuancheng Wang, Woo Chul Shin, Soroush Nasiriany, Ajay Mandlekar, Danfei Xu

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®ç”Ÿæˆæ¡†æ¶ä»¥ä¼˜åŒ–æœºå™¨äººæ“ä½œæ•°æ®é›†çš„å­¦ä¹ æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `æ¨¡ä»¿å­¦ä¹ ` `æ•°æ®é›†ä¼˜åŒ–` `å¤šæ ·æ€§æ¥æº` `ç­–ç•¥å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ“ä½œæ•°æ®é›†çš„æ„å»ºç¼ºä¹ç³»ç»Ÿæ€§æŒ‡å¯¼ï¼Œå¯¼è‡´æ•°æ®æ”¶é›†çš„æœ‰æ•ˆæ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿå¤šæ ·æ€§æ¥æºæ¥ä¼˜åŒ–æ•°æ®é›†çš„ç»„æˆï¼Œæå‡å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ–°ç­–ç•¥çš„æ£€ç´¢æ–¹æ³•åœ¨ç°æœ‰æ•°æ®é›†ä¸Šæ€§èƒ½æå‡å¯è¾¾70%ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿè®­ç»ƒç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ ä»å¤§è§„æ¨¡å¤šä»»åŠ¡æ¼”ç¤ºæ•°æ®é›†ä¸­å·²æˆä¸ºæ„å»ºé€šç”¨æœºå™¨äººèƒ½åŠ›çš„æœ‰å‰æ™¯è·¯å¾„ã€‚å°½ç®¡å…¨çƒèŒƒå›´å†…å·²æŠ•å…¥æ•°åƒå°æ—¶æ„å»ºæ­¤ç±»æ•°æ®é›†ï¼Œä½†æˆ‘ä»¬ä»ç¼ºä¹ç³»ç»Ÿæ€§çš„ç†è§£ï¼Œå¦‚ä½•æ”¶é›†æ•°æ®ä»¥æå‡æœºå™¨äººæ•°æ®é›†çš„æ•ˆç”¨å¹¶ä¿ƒè¿›ä¸‹æ¸¸ç­–ç•¥å­¦ä¹ ã€‚æœ¬æ–‡å¼€å±•äº†ä¸€é¡¹å¤§è§„æ¨¡æ•°æ®é›†ç»„æˆç ”ç©¶ï¼Œå¼€å‘äº†ä¸€ä¸ªæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œä»¥ç¨‹åºåŒ–æ¨¡æ‹Ÿç°æœ‰æ•°æ®é›†ä¸­å¸¸è§çš„å¤šæ ·æ€§æ¥æºï¼Œç”Ÿæˆå…·æœ‰å—æ§ç»„æˆçš„å¤§è§„æ¨¡æœºå™¨äººæ•°æ®é›†ã€‚ç ”ç©¶å‘ç°ï¼Œæ‘„åƒæœºå§¿æ€å’Œç©ºé—´æ’åˆ—æ˜¯æ•°æ®æ”¶é›†å’Œæ£€ç´¢å¯¹é½ä¸­çš„å…³é”®ç»´åº¦ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„æ£€ç´¢ç­–ç•¥åœ¨ç°æœ‰æ•°æ®é›†ä¸Šèƒ½å°†è®­ç»ƒç­–ç•¥çš„æ€§èƒ½æå‡å¤šè¾¾70%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆæ”¶é›†å’Œåˆ©ç”¨å¤§è§„æ¨¡æœºå™¨äººæ“ä½œæ•°æ®é›†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ç³»ç»Ÿæ€§æŒ‡å¯¼ï¼Œå¯¼è‡´æ•°æ®é›†çš„å¤šæ ·æ€§å’Œæ•ˆç”¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡ç¨‹åºåŒ–æ¨¡æ‹Ÿæ•°æ®é›†ä¸­çš„å¤šæ ·æ€§æ¥æºï¼Œå¦‚ä¼ æ„Ÿå™¨ä½ç½®å’Œç‰©ä½“ç±»å‹ï¼Œæ¥ä¼˜åŒ–æ•°æ®é›†çš„ç»„æˆã€‚è¿™ç§è®¾è®¡æ—¨åœ¨é™ä½å®é™…æ•°æ®æ”¶é›†çš„æˆæœ¬ï¼ŒåŒæ—¶æé«˜æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—å’Œæ•°æ®é›†ç»„æˆç ”ç©¶æ¨¡å—ã€‚æ•°æ®ç”Ÿæˆæ¨¡å—è´Ÿè´£åˆ›å»ºå…·æœ‰å¤šæ ·æ€§çš„æœºå™¨äººæ•°æ®é›†ï¼Œè€Œç»„æˆç ”ç©¶æ¨¡å—åˆ™åˆ†æä¸åŒç»„æˆå¯¹ä¸‹æ¸¸ç­–ç•¥å­¦ä¹ çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜èƒ½å¤Ÿåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­æ¢ç´¢æ•°æ®é›†ç»„æˆçš„å¤šæ ·æ€§ï¼Œè€Œæ— éœ€åœ¨ç°å®ä¸–ç•Œä¸­è¿›è¡Œæ˜‚è´µçš„å®éªŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œç ”ç©¶é‡ç‚¹å…³æ³¨æ‘„åƒæœºå§¿æ€å’Œç©ºé—´æ’åˆ—çš„å¤šæ ·æ€§ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œç¡®ä¿ç”Ÿæˆçš„æ•°æ®é›†èƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒä¸‹æ¸¸ä»»åŠ¡çš„å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºæ–°æå‡ºçš„æ£€ç´¢ç­–ç•¥ï¼Œæœºå™¨äººåœ¨ç°æœ‰æ•°æ®é›†DROIDä¸Šçš„è®­ç»ƒæ€§èƒ½æå‡å¯è¾¾70%ã€‚è¿™ä¸€æ˜¾è‘—æå‡è¡¨æ˜ï¼Œä¼˜åŒ–æ•°æ®é›†ç»„æˆå’Œæ£€ç´¢ç­–ç•¥å¯¹æœºå™¨äººå­¦ä¹ çš„æœ‰æ•ˆæ€§å…·æœ‰é‡è¦å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€è‡ªåŠ¨åŒ–åˆ¶é€ å’Œäººæœºåä½œç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®é›†çš„ç»„æˆï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å­¦ä¹ æ•ˆç‡å’Œé€‚åº”èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation learning from large multi-task demonstration datasets has emerged as a promising path for building generally-capable robots. As a result, 1000s of hours have been spent on building such large-scale datasets around the globe. Despite the continuous growth of such efforts, we still lack a systematic understanding of what data should be collected to improve the utility of a robotics dataset and facilitate downstream policy learning. In this work, we conduct a large-scale dataset composition study to answer this question. We develop a data generation framework to procedurally emulate common sources of diversity in existing datasets (such as sensor placements and object types and arrangements), and use it to generate large-scale robot datasets with controlled compositions, enabling a suite of dataset composition studies that would be prohibitively expensive in the real world. We focus on two practical settings: (1) what types of diversity should be emphasized when future researchers collect large-scale datasets for robotics, and (2) how should current practitioners retrieve relevant demonstrations from existing datasets to maximize downstream policy performance on tasks of interest. Our study yields several critical insights -- for example, we find that camera poses and spatial arrangements are crucial dimensions for both diversity in collection and alignment in retrieval. In real-world robot learning settings, we find that not only do our insights from simulation carry over, but our retrieval strategies on existing datasets such as DROID allow us to consistently outperform existing training strategies by up to 70%. More results at https://robo-mimiclabs.github.io/

