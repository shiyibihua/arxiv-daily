---
layout: default
title: ROSA: Harnessing Robot States for Vision-Language and Action Alignment
---

# ROSA: Harnessing Robot States for Vision-Language and Action Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13679" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13679v1</a>
  <a href="https://arxiv.org/pdf/2506.13679.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13679v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13679v1', 'ROSA: Harnessing Robot States for Vision-Language and Action Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqing Wen, Kefan Gu, Haoxuan Liu, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiaoyan Sun

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºROSAä»¥è§£å†³è§†è§‰è¯­è¨€ä¸æœºå™¨äººåŠ¨ä½œå¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€` `æœºå™¨äººæ§åˆ¶` `çŠ¶æ€ä¼°è®¡` `å¤šä»»åŠ¡å­¦ä¹ ` `æ•°æ®æ•ˆç‡` `æ¨¡å‹æ³›åŒ–` `è‡ªåŠ¨åŒ–è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§†è§‰è¯­è¨€ä¸æœºå™¨äººåŠ¨ä½œå¯¹é½æ–¹é¢å­˜åœ¨æ—¶ç©ºå·®è·ï¼Œå¯¼è‡´æ•°æ®æ•ˆç‡ä½ä¸‹å’Œå¯¹äººåŠ›çš„ä¾èµ–ã€‚
2. æœ¬æ–‡æå‡ºROSAï¼Œé€šè¿‡åˆ©ç”¨æœºå™¨äººçŠ¶æ€ä¼°è®¡æ¥æ”¹å–„è§†è§‰è¯­è¨€ä¸åŠ¨ä½œç©ºé—´çš„å¯¹é½ï¼Œå¢å¼ºæ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒROSAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨ä½æ•°æ®æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤šä»»åŠ¡ã€ç«¯åˆ°ç«¯çš„æœºå™¨äººæ§åˆ¶ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¾—ç›Šäºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å°†è§†è§‰è¯­è¨€ç©ºé—´ä¸æœºå™¨äººåŠ¨ä½œç©ºé—´æœ‰æ•ˆå¯¹é½æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ä¾èµ–äºä¸“å®¶æ¼”ç¤ºè¿›è¡Œå¾®è°ƒï¼Œå¯¼è‡´æ•°æ®æ•ˆç‡ä½ä¸‹å’Œå¯¹äººåŠ›çš„é«˜åº¦ä¾èµ–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒèŒƒå¼ROSAï¼Œåˆ©ç”¨æœºå™¨äººçŠ¶æ€ä¼°è®¡æ¥æ”¹å–„è§†è§‰è¯­è¨€ä¸åŠ¨ä½œç©ºé—´ä¹‹é—´çš„å¯¹é½ã€‚é€šè¿‡é›†æˆè‡ªåŠ¨åŒ–è¿‡ç¨‹è·å¾—çš„æœºå™¨äººçŠ¶æ€ä¼°è®¡æ•°æ®ï¼ŒROSAå¢å¼ºäº†VLAæ¨¡å‹çš„ç©ºé—´ç†è§£å’Œè‡ªæˆ‘æ„è¯†ï¼Œä»è€Œæå‡äº†æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼ŒROSAåœ¨ä½æ•°æ®ç¯å¢ƒä¸‹å°¤å…¶æœ‰æ•ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸æœºå™¨äººåŠ¨ä½œä¹‹é—´çš„å¯¹é½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–ä¸“å®¶æ¼”ç¤ºè¿›è¡Œå¾®è°ƒï¼Œå¯¼è‡´æ—¶ç©ºå·®è·å’Œæ•°æ®æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šROSAé€šè¿‡é›†æˆæœºå™¨äººçŠ¶æ€ä¼°è®¡æ•°æ®ï¼Œå¢å¼ºäº†VLAæ¨¡å‹çš„ç©ºé—´ç†è§£å’Œè‡ªæˆ‘æ„è¯†ï¼Œä»è€Œæ”¹å–„äº†è§†è§‰è¯­è¨€ä¸åŠ¨ä½œç©ºé—´çš„å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šROSAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€çŠ¶æ€ä¼°è®¡å’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è‡ªåŠ¨åŒ–è¿‡ç¨‹è·å–æœºå™¨äººçŠ¶æ€æ•°æ®ï¼Œç„¶åå°†å…¶ä¸è§†è§‰è¯­è¨€æ¨¡å‹ç»“åˆï¼Œè¿›è¡Œè”åˆè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šROSAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨æœºå™¨äººçŠ¶æ€ä¼°è®¡æ¥å¡«è¡¥è§†è§‰è¯­è¨€ä¸åŠ¨ä½œä¹‹é—´çš„æ—¶ç©ºå·®è·ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ä¾èµ–ä¸“å®¶æ¼”ç¤ºçš„å¾®è°ƒç­–ç•¥æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒROSAé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡è§†è§‰è¯­è¨€ä¸åŠ¨ä½œç©ºé—´çš„å¯¹é½ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥é€‚åº”çŠ¶æ€ä¼°è®¡æ•°æ®çš„è¾“å…¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒROSAåœ¨ä½æ•°æ®ç¯å¢ƒä¸‹çš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†20%ä»¥ä¸Šï¼Œä¸”åœ¨å¤šç§ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ— äººæœºæ§åˆ¶ç­‰é¢†åŸŸã€‚ROSAçš„åˆ›æ–°æ–¹æ³•å¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å†³ç­–èƒ½åŠ›ï¼Œå‡å°‘å¯¹äººç±»ä¸“å®¶çš„ä¾èµ–ï¼Œæ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have recently made significant advance in multi-task, end-to-end robotic control, due to the strong generalization capabilities of Vision-Language Models (VLMs). A fundamental challenge in developing such models is effectively aligning the vision-language space with the robotic action space. Existing approaches typically rely on directly fine-tuning VLMs using expert demonstrations. However, this strategy suffers from a spatio-temporal gap, resulting in considerable data inefficiency and heavy reliance on human labor. Spatially, VLMs operate within a high-level semantic space, whereas robotic actions are grounded in low-level 3D physical space; temporally, VLMs primarily interpret the present, while VLA models anticipate future actions. To overcome these challenges, we propose a novel training paradigm, ROSA, which leverages robot state estimation to improve alignment between vision-language and action spaces. By integrating robot state estimation data obtained via an automated process, ROSA enables the VLA model to gain enhanced spatial understanding and self-awareness, thereby boosting performance and generalization. Extensive experiments in both simulated and real-world environments demonstrate the effectiveness of ROSA, particularly in low-data regimes.

