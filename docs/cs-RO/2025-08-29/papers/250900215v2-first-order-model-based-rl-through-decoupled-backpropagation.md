---
layout: default
title: First Order Model-Based RL through Decoupled Backpropagation
---

# First Order Model-Based RL through Decoupled Backpropagation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00215" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00215v2</a>
  <a href="https://arxiv.org/pdf/2509.00215.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00215v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00215v2', 'First Order Model-Based RL through Decoupled Backpropagation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Joseph Amigo, Rooholla Khorrambakht, Elliot Chane-Sane, Nicolas Mansard, Ludovic Righetti

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29 (æ›´æ–°: 2025-09-04)

**å¤‡æ³¨**: CoRL 2025. Project website: https://machines-in-motion.github.io/DMO/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥æé«˜å­¦ä¹ æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹åŸºæ–¹æ³•` `æ¢¯åº¦è®¡ç®—` `æœºå™¨äººæ§åˆ¶` `åŠ¨æ€æ¨¡å‹` `æ ·æœ¬æ•ˆç‡` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡å‹åŸºå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´é¢„æµ‹è¯¯å·®ç´¯ç§¯çš„é—®é¢˜ï¼Œå¯¼è‡´ç­–ç•¥æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†è½¨è¿¹ç”Ÿæˆä¸æ¢¯åº¦è®¡ç®—è§£è€¦çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¯å¾®æ¨¡å‹è¿›è¡Œé«˜æ•ˆçš„æ¢¯åº¦è®¡ç®—ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œé€Ÿåº¦ä¸Šä¸ä¸“é—¨ä¼˜åŒ–å™¨ç›¸å½“ï¼ŒåŒæ—¶ä¿æŒäº†æ ‡å‡†æ–¹æ³•çš„é€šç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¯¹åˆ©ç”¨æ¨¡æ‹Ÿå™¨å¯¼æ•°ä»¥æé«˜å­¦ä¹ æ•ˆç‡çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å…³æ³¨å¢åŠ ï¼Œæ—©æœŸçš„åŸºäºæ¢¯åº¦çš„æ–¹æ³•å·²æ˜¾ç¤ºå‡ºä¼˜äºæ— å¯¼æ•°æ–¹æ³•çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå®ç°æˆæœ¬æˆ–ä¸å¯ç”¨æ€§ï¼Œè®¿é—®æ¨¡æ‹Ÿå™¨æ¢¯åº¦å¾€å¾€ä¸åˆ‡å®é™…ã€‚æ¨¡å‹åŸºå¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰å¯ä»¥é€šè¿‡å­¦ä¹ çš„åŠ¨æ€æ¨¡å‹æ¥è¿‘ä¼¼è¿™äº›æ¢¯åº¦ï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé¢„æµ‹è¯¯å·®çš„ç´¯ç§¯ä¼šå½±å“æ±‚è§£å™¨çš„æ•ˆç‡ï¼Œä»è€Œé™ä½ç­–ç•¥æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†è½¨è¿¹ç”Ÿæˆä¸æ¢¯åº¦è®¡ç®—è§£è€¦çš„æ–¹æ³•ï¼šä½¿ç”¨æ¨¡æ‹Ÿå™¨å±•å¼€è½¨è¿¹ï¼ŒåŒæ—¶é€šè¿‡å­¦ä¹ çš„å¯å¾®æ¨¡å‹è¿›è¡Œåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ã€‚è¿™ç§æ··åˆè®¾è®¡ä½¿å¾—å³ä½¿åœ¨æ¨¡æ‹Ÿå™¨æ¢¯åº¦ä¸å¯ç”¨çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½å®ç°é«˜æ•ˆä¸”ä¸€è‡´çš„ä¸€é˜¶ç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶ä»æ¨¡æ‹Ÿå›æ”¾ä¸­å­¦ä¹ æ›´å‡†ç¡®çš„è¯„è®ºè€…ã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨åŸºå‡†æ§åˆ¶ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®è¯éªŒè¯ï¼Œå¹¶åœ¨çœŸå®çš„Go2å››è¶³æœºå™¨äººä¸Šå±•ç¤ºäº†å…¶åœ¨å››è¶³å’ŒåŒè¶³è¿åŠ¨ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨¡å‹åŸºå¼ºåŒ–å­¦ä¹ ä¸­ç”±äºé¢„æµ‹è¯¯å·®ç´¯ç§¯å¯¼è‡´çš„ç­–ç•¥æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨æ¨¡æ‹Ÿå™¨çš„æ¢¯åº¦ï¼Œå½±å“äº†å­¦ä¹ æ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è½¨è¿¹ç”Ÿæˆä¸æ¢¯åº¦è®¡ç®—è§£è€¦ï¼Œåˆ©ç”¨æ¨¡æ‹Ÿå™¨ç”Ÿæˆè½¨è¿¹ï¼ŒåŒæ—¶é€šè¿‡å­¦ä¹ çš„å¯å¾®æ¨¡å‹è¿›è¡Œæ¢¯åº¦çš„åå‘ä¼ æ’­è®¡ç®—ã€‚è¿™ç§è®¾è®¡ä½¿å¾—å³ä½¿åœ¨ç¼ºä¹æ¨¡æ‹Ÿå™¨æ¢¯åº¦çš„æƒ…å†µä¸‹ï¼Œä»èƒ½å®ç°é«˜æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè½¨è¿¹ç”Ÿæˆæ¨¡å—å’Œæ¢¯åº¦è®¡ç®—æ¨¡å—ã€‚è½¨è¿¹ç”Ÿæˆæ¨¡å—ä½¿ç”¨æ¨¡æ‹Ÿå™¨è¿›è¡ŒåŠ¨æ€æ¨¡æ‹Ÿï¼Œè€Œæ¢¯åº¦è®¡ç®—æ¨¡å—åˆ™é€šè¿‡å­¦ä¹ çš„å¯å¾®æ¨¡å‹è¿›è¡Œåå‘ä¼ æ’­ï¼Œæœ€ç»ˆå®ç°ç­–ç•¥ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†è½¨è¿¹ç”Ÿæˆä¸æ¢¯åº¦è®¡ç®—è§£è€¦ï¼Œè¿™ä¸ç°æœ‰çš„æ¨¡å‹åŸºå¼ºåŒ–å­¦ä¹ æ–¹æ³•å½¢æˆäº†æœ¬è´¨åŒºåˆ«ï¼Œåè€…é€šå¸¸å°†ä¸¤è€…ç´§å¯†ç»“åˆï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼Œè®ºæ–‡é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å­¦ä¹ çš„åŠ¨æ€æ¨¡å‹ï¼Œå¹¶é€šè¿‡è°ƒèŠ‚ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æ¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ ·æœ¬æ•ˆç‡å’Œé€Ÿåº¦ä¸ä¸“é—¨ä¼˜åŒ–å™¨SHACç›¸å½“ï¼ŒåŒæ—¶é¿å…äº†å…¶ä»–ä¸€é˜¶æ¨¡å‹åŸºå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸­è§‚å¯Ÿåˆ°çš„ä¸è‰¯è¡Œä¸ºã€‚åœ¨çœŸå®çš„Go2å››è¶³æœºå™¨äººä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å››è¶³å’ŒåŒè¶³è¿åŠ¨ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ã€‚é€šè¿‡æé«˜å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡å’Œå­¦ä¹ é€Ÿåº¦ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„å†³ç­–å’Œæ§åˆ¶ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> There is growing interest in reinforcement learning (RL) methods that leverage the simulator's derivatives to improve learning efficiency. While early gradient-based approaches have demonstrated superior performance compared to derivative-free methods, accessing simulator gradients is often impractical due to their implementation cost or unavailability. Model-based RL (MBRL) can approximate these gradients via learned dynamics models, but the solver efficiency suffers from compounding prediction errors during training rollouts, which can degrade policy performance. We propose an approach that decouples trajectory generation from gradient computation: trajectories are unrolled using a simulator, while gradients are computed via backpropagation through a learned differentiable model of the simulator. This hybrid design enables efficient and consistent first-order policy optimization, even when simulator gradients are unavailable, as well as learning a critic from simulation rollouts, which is more accurate. Our method achieves the sample efficiency and speed of specialized optimizers such as SHAC, while maintaining the generality of standard approaches like PPO and avoiding ill behaviors observed in other first-order MBRL methods. We empirically validate our algorithm on benchmark control tasks and demonstrate its effectiveness on a real Go2 quadruped robot, across both quadrupedal and bipedal locomotion tasks.

