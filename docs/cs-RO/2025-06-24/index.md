---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-06-24
---

# cs.ROï¼ˆ2025-06-24ï¼‰

ğŸ“Š å…± **15** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (13 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250619842v1-manigaussian-general-robotic-bimanual-manipulation-with-hierarchical.html">ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model</a></td>
  <td>æå‡ºManiGaussian++ä»¥è§£å†³åŒè‡‚æœºå™¨äººå¤šä»»åŠ¡æ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">dual-arm</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19842v1" data-paper-url="./papers/250619842v1-manigaussian-general-robotic-bimanual-manipulation-with-hierarchical.html" onclick="toggleFavorite(this, '2506.19842v1', 'ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250619269v2-anchordp3-3d-affordance-guided-sparse-diffusion-policy-for-robotic-m.html">AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic Manipulation</a></td>
  <td>æå‡ºAnchorDP3ä»¥è§£å†³åŒè‡‚æœºå™¨äººæ“æ§ä¸­çš„é«˜éšæœºæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dual-arm</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19269v2" data-paper-url="./papers/250619269v2-anchordp3-3d-affordance-guided-sparse-diffusion-policy-for-robotic-m.html" onclick="toggleFavorite(this, '2506.19269v2', 'AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250619816v2-cronusvla-towards-efficient-and-robust-manipulation-via-multi-frame-.html">CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling</a></td>
  <td>æå‡ºCronusVLAä»¥è§£å†³å•å¸§å›¾åƒåœ¨æœºå™¨äººæ“ä½œä¸­çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19816v2" data-paper-url="./papers/250619816v2-cronusvla-towards-efficient-and-robust-manipulation-via-multi-frame-.html" onclick="toggleFavorite(this, '2506.19816v2', 'CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250620036v1-hierarchical-reinforcement-learning-and-value-optimization-for-chall.html">Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion</a></td>
  <td>æå‡ºåˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å››è¶³æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸­çš„è¡Œèµ°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20036v1" data-paper-url="./papers/250620036v1-hierarchical-reinforcement-learning-and-value-optimization-for-chall.html" onclick="toggleFavorite(this, '2506.20036v1', 'Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250619212v2-scaffolding-dexterous-manipulation-with-vision-language-models.html">Scaffolding Dexterous Manipulation with Vision-Language Models</a></td>
  <td>æå‡ºè§†è§‰è¯­è¨€æ¨¡å‹è¾…åŠ©çµå·§æ“æ§ä»¥è§£å†³è®­ç»ƒéš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">dexterous manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19212v2" data-paper-url="./papers/250619212v2-scaffolding-dexterous-manipulation-with-vision-language-models.html" onclick="toggleFavorite(this, '2506.19212v2', 'Scaffolding Dexterous Manipulation with Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250619201v1-the-motif-hand-a-robotic-hand-for-multimodal-observations-with-therm.html">The MOTIF Hand: A Robotic Hand for Multimodal Observations with Thermal, Inertial, and Force Sensors</a></td>
  <td>æå‡ºMOTIFæ‰‹ä»¥è§£å†³å¤šæ¨¡æ€ä¼ æ„Ÿå™¨é›†æˆä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19201v1" data-paper-url="./papers/250619201v1-the-motif-hand-a-robotic-hand-for-multimodal-observations-with-therm.html" onclick="toggleFavorite(this, '2506.19201v1', 'The MOTIF Hand: A Robotic Hand for Multimodal Observations with Thermal, Inertial, and Force Sensors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250621628v2-ark-an-open-source-python-based-framework-for-robot-learning.html">Ark: An Open-source Python-based Framework for Robot Learning</a></td>
  <td>æå‡ºARKæ¡†æ¶ä»¥è§£å†³æœºå™¨äººå­¦ä¹ è½¯ä»¶ç“¶é¢ˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21628v2" data-paper-url="./papers/250621628v2-ark-an-open-source-python-based-framework-for-robot-learning.html" onclick="toggleFavorite(this, '2506.21628v2', 'Ark: An Open-source Python-based Framework for Robot Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250621627v1-frankenbot-brain-morphic-modular-orchestration-for-robotic-manipulat.html">FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models</a></td>
  <td>æå‡ºFrankenBotä»¥è§£å†³æœºå™¨äººæ“ä½œç³»ç»ŸåŠŸèƒ½æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21627v1" data-paper-url="./papers/250621627v1-frankenbot-brain-morphic-modular-orchestration-for-robotic-manipulat.html" onclick="toggleFavorite(this, '2506.21627v1', 'FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250619968v1-evolutionary-gait-reconfiguration-in-damaged-legged-robots.html">Evolutionary Gait Reconfiguration in Damaged Legged Robots</a></td>
  <td>æå‡ºä¸€ç§å¿«é€Ÿæ¢å¤è…¿éƒ¨æŸä¼¤çš„å¤šè¶³æœºå™¨äººæ­¥æ€é‡é…ç½®æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19968v1" data-paper-url="./papers/250619968v1-evolutionary-gait-reconfiguration-in-damaged-legged-robots.html" onclick="toggleFavorite(this, '2506.19968v1', 'Evolutionary Gait Reconfiguration in Damaged Legged Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250619984v1-robust-embodied-self-identification-of-morphology-in-damaged-multi-l.html">Robust Embodied Self-Identification of Morphology in Damaged Multi-Legged Robots</a></td>
  <td>æå‡ºè‡ªæˆ‘å»ºæ¨¡ç®—æ³•ä»¥è§£å†³å¤šè¶³æœºå™¨äººæŸä¼¤è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19984v1" data-paper-url="./papers/250619984v1-robust-embodied-self-identification-of-morphology-in-damaged-multi-l.html" onclick="toggleFavorite(this, '2506.19984v1', 'Robust Embodied Self-Identification of Morphology in Damaged Multi-Legged Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250619498v1-t-rex-task-adaptive-spatial-representation-extraction-for-robotic-ma.html">T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models</a></td>
  <td>æå‡ºT-Rexæ¡†æ¶ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„ç©ºé—´è¡¨ç¤ºæå–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19498v1" data-paper-url="./papers/250619498v1-t-rex-task-adaptive-spatial-representation-extraction-for-robotic-ma.html" onclick="toggleFavorite(this, '2506.19498v1', 'T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250619303v1-robotic-perception-with-a-large-tactile-vision-language-model-for-ph.html">Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference</a></td>
  <td>æå‡ºè·¨æ¨¡æ€æ„ŸçŸ¥æ¡†æ¶ä»¥è§£å†³ç‰©ç†å±æ€§æ¨æ–­é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19303v1" data-paper-url="./papers/250619303v1-robotic-perception-with-a-large-tactile-vision-language-model-for-ph.html" onclick="toggleFavorite(this, '2506.19303v1', 'Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250620049v1-robust-robotic-exploration-and-mapping-using-generative-occupancy-ma.html">Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis</a></td>
  <td>æå‡ºSceneSenseä»¥è§£å†³æœºå™¨äººæ¢ç´¢ä¸æ˜ å°„ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">traversability</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20049v1" data-paper-url="./papers/250620049v1-robust-robotic-exploration-and-mapping-using-generative-occupancy-ma.html" onclick="toggleFavorite(this, '2506.20049v1', 'Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250621630v1-tomd-a-trail-based-off-road-multimodal-dataset-for-traversable-pathw.html">TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions</a></td>
  <td>æå‡ºTOMDæ•°æ®é›†ä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸‹å¯é€šè¡Œè·¯å¾„æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21630v1" data-paper-url="./papers/250621630v1-tomd-a-trail-based-off-road-multimodal-dataset-for-traversable-pathw.html" onclick="toggleFavorite(this, '2506.21630v1', 'TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250619827v1-look-to-locate-vision-based-multisensory-navigation-with-3-d-digital.html">Look to Locate: Vision-Based Multisensory Navigation with 3-D Digital Maps for GNSS-Challenged Environments</a></td>
  <td>æå‡ºåŸºäºè§†è§‰çš„å¤šä¼ æ„Ÿå™¨å¯¼èˆªç³»ç»Ÿä»¥è§£å†³GNSSå—é™ç¯å¢ƒä¸­çš„å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19827v1" data-paper-url="./papers/250619827v1-look-to-locate-vision-based-multisensory-navigation-with-3-d-digital.html" onclick="toggleFavorite(this, '2506.19827v1', 'Look to Locate: Vision-Based Multisensory Navigation with 3-D Digital Maps for GNSS-Challenged Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)