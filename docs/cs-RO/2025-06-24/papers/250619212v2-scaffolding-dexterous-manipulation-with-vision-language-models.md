---
layout: default
title: Scaffolding Dexterous Manipulation with Vision-Language Models
---

# Scaffolding Dexterous Manipulation with Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19212" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19212v2</a>
  <a href="https://arxiv.org/pdf/2506.19212.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19212v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19212v2', 'Scaffolding Dexterous Manipulation with Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vincent de Bakker, Joey Hejna, Tyler Ga Wei Lum, Onur Celik, Aleksandar Taranovic, Denis Blessing, Gerhard Neumann, Jeannette Bohg, Dorsa Sadigh

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24 (æ›´æ–°: 2025-11-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰è¯­è¨€æ¨¡å‹è¾…åŠ©çµå·§æ“æ§ä»¥è§£å†³è®­ç»ƒéš¾é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çµå·§æ“æ§` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ‰‹` `ä»»åŠ¡æŒ‡å¯¼`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çµå·§æ“æ§æ–¹æ³•é¢ä¸´æ¼”ç¤ºæ”¶é›†å›°éš¾å’Œé«˜ç»´æ§åˆ¶æŒ‘æˆ˜ï¼Œé™åˆ¶äº†è®­ç»ƒæ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è‡ªåŠ¨è¯†åˆ«å…³é”®ç‚¹å¹¶åˆæˆ3Dè½¨è¿¹ï¼Œç®€åŒ–äº†ä»»åŠ¡æŒ‡å¯¼è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¨¡æ‹Ÿä»»åŠ¡ä¸­å­¦ä¹ åˆ°çš„æ“æ§ç­–ç•¥å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ï¼Œå¹¶æˆåŠŸè½¬ç§»åˆ°çœŸå®ç¯å¢ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çµå·§æœºå™¨äººæ‰‹åœ¨æ‰§è¡Œå¤æ‚æ“ä½œä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œä½†ç”±äºæ¼”ç¤ºæ”¶é›†å’Œé«˜ç»´æ§åˆ¶çš„æŒ‘æˆ˜ï¼Œè®­ç»ƒä»ç„¶å›°éš¾ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥é€šè¿‡æ¨¡æ‹Ÿç”Ÿæˆç»éªŒæ¥ç¼“è§£æ•°æ®ç“¶é¢ˆï¼Œä½†é€šå¸¸ä¾èµ–äºç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ç‰¹å®šå¥–åŠ±å‡½æ•°ï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç¼–ç çš„å¸¸è¯†ç©ºé—´å’Œè¯­ä¹‰çŸ¥è¯†ï¼Œç»“åˆä»»åŠ¡æè¿°å’Œè§†è§‰åœºæ™¯ï¼Œè¯†åˆ«ä»»åŠ¡ç›¸å…³çš„å…³é”®ç‚¹ï¼Œå¹¶åˆæˆæ‰‹éƒ¨å’Œç‰©ä½“è¿åŠ¨çš„3Dè½¨è¿¹ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿä¸­è®­ç»ƒä½çº§æ®‹å·®RLç­–ç•¥ä»¥é«˜ä¿çœŸåº¦è·Ÿè¸ªè¿™äº›ç²—ç•¥è½¨è¿¹ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ åˆ°ç¨³å¥çš„çµå·§æ“æ§ç­–ç•¥ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨æ²¡æœ‰äººç±»æ¼”ç¤ºæˆ–æ‰‹å·¥å¥–åŠ±çš„æƒ…å†µä¸‹è½¬ç§»åˆ°çœŸå®æœºå™¨äººæ‰‹ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³çµå·§æœºå™¨äººæ‰‹çš„è®­ç»ƒéš¾é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ç‰¹å®šå¥–åŠ±å‡½æ•°ï¼Œå¯¼è‡´å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç¼–ç çš„ç©ºé—´å’Œè¯­ä¹‰çŸ¥è¯†ï¼Œç»“åˆä»»åŠ¡æè¿°å’Œè§†è§‰åœºæ™¯ï¼Œè‡ªåŠ¨è¯†åˆ«å…³é”®ç‚¹å¹¶åˆæˆ3Dè½¨è¿¹ï¼Œä»è€ŒæŒ‡å¯¼å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨VLMè¯†åˆ«ä»»åŠ¡ç›¸å…³çš„å…³é”®ç‚¹ï¼›å…¶æ¬¡ï¼Œåˆæˆæ‰‹éƒ¨å’Œç‰©ä½“è¿åŠ¨çš„3Dè½¨è¿¹ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿä¸­è®­ç»ƒä½çº§æ®‹å·®RLç­–ç•¥ä»¥è·Ÿè¸ªè¿™äº›è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„åˆ›æ–°åœ¨äºå°†è§†è§‰è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œåˆ©ç”¨VLMçš„å¸¸è¯†çŸ¥è¯†æ›¿ä»£ä¼ ç»Ÿçš„å‚è€ƒè½¨è¿¹ï¼Œæ˜¾è‘—ç®€åŒ–äº†ä»»åŠ¡æŒ‡å¯¼è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä½¿ç”¨äº†æ ‡å‡†çš„VLMè¿›è¡Œå…³é”®ç‚¹è¯†åˆ«ï¼Œè½¨è¿¹åˆæˆé‡‡ç”¨äº†åŸºäºä»»åŠ¡æè¿°çš„ç”Ÿæˆç­–ç•¥ï¼ŒRLç­–ç•¥åˆ™é€šè¿‡ä½çº§æ®‹å·®å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿é«˜ä¿çœŸåº¦çš„è½¨è¿¹è·Ÿè¸ªã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¨¡æ‹Ÿä»»åŠ¡ä¸­æˆåŠŸå­¦ä¹ åˆ°ç¨³å¥çš„çµå·§æ“æ§ç­–ç•¥ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰äººç±»æ¼”ç¤ºæˆ–æ‰‹å·¥å¥–åŠ±çš„æƒ…å†µä¸‹ï¼Œç›´æ¥è½¬ç§»åˆ°çœŸå®æœºå™¨äººæ‰‹ä¸Šï¼Œå±•ç¤ºäº†è‰¯å¥½çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œäººæœºåä½œç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜çµå·§æœºå™¨äººæ‰‹çš„æ“ä½œèƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œå¤šç§ä»»åŠ¡ï¼Œæå‡è‡ªåŠ¨åŒ–æ°´å¹³ï¼Œé™ä½äººåŠ›æˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dexterous robotic hands are essential for performing complex manipulation tasks, yet remain difficult to train due to the challenges of demonstration collection and high-dimensional control. While reinforcement learning (RL) can alleviate the data bottleneck by generating experience in simulation, it typically relies on carefully designed, task-specific reward functions, which hinder scalability and generalization. Thus, contemporary works in dexterous manipulation have often bootstrapped from reference trajectories. These trajectories specify target hand poses that guide the exploration of RL policies and object poses that enable dense, task-agnostic rewards. However, sourcing suitable trajectories - particularly for dexterous hands - remains a significant challenge. Yet, the precise details in explicit reference trajectories are often unnecessary, as RL ultimately refines the motion. Our key insight is that modern vision-language models (VLMs) already encode the commonsense spatial and semantic knowledge needed to specify tasks and guide exploration effectively. Given a task description (e.g., "open the cabinet") and a visual scene, our method uses an off-the-shelf VLM to first identify task-relevant keypoints (e.g., handles, buttons) and then synthesize 3D trajectories for hand motion and object motion. Subsequently, we train a low-level residual RL policy in simulation to track these coarse trajectories or "scaffolds" with high fidelity. Across a number of simulated tasks involving articulated objects and semantic understanding, we demonstrate that our method is able to learn robust dexterous manipulation policies. Moreover, we showcase that our method transfers to real-world robotic hands without any human demonstrations or handcrafted rewards.

