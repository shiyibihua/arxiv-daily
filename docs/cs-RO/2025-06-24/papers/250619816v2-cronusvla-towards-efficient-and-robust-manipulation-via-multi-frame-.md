---
layout: default
title: CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling
---

# CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19816" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.19816v2</a>
  <a href="https://arxiv.org/pdf/2506.19816.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19816v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19816v2', 'CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hao Li, Shuai Yang, Yilun Chen, Xinyi Chen, Xiaoda Yang, Yang Tian, Hanqing Wang, Tai Wang, Dahua Lin, Feng Zhao, Jiangmiao Pang

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-24 (Êõ¥Êñ∞: 2025-10-30)

**Â§áÊ≥®**: 39 pages, 24 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CronusVLA‰ª•Ëß£ÂÜ≥ÂçïÂ∏ßÂõæÂÉèÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÂ±ÄÈôêÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `Â§öÂ∏ßÂª∫Ê®°` `Êú∫Âô®‰∫∫Êìç‰Ωú` `È≤ÅÊ£íÊÄßËØÑ‰º∞` `ÁâπÂæÅËÅöÂêà`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂèóÈôê‰∫éÂçïÂ∏ßÂõæÂÉèÔºåÊú™ËÉΩÊúâÊïàÂà©Áî®Â§öÂ∏ßÂéÜÂè≤‰ø°ÊÅØÔºåÂØºËá¥ËÆ°ÁÆóÂºÄÈîÄÂíåÊé®ÁêÜÂª∂Ëøü„ÄÇ
2. CronusVLAÈÄöËøáÂçïÂ∏ßÈ¢ÑËÆ≠ÁªÉÂíåÂ§öÂ∏ßÂêéËÆ≠ÁªÉÁöÑ‰∏§Èò∂ÊÆµËøáÁ®ãÔºåÊâ©Â±ï‰∫ÜÂçïÂ∏ßVLAÊ®°ÂûãÔºåÊèêÂçá‰∫ÜÂ§öÂ∏ß‰ø°ÊÅØÁöÑÂà©Áî®ÊïàÁéá„ÄÇ
3. Âú®Â§ö‰∏™Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ÔºåCronusVLAÂú®SimplerEnv‰∏äÂèñÂæó70.9%ÁöÑÊàêÂäüÁéáÔºåÁõ∏ËæÉ‰∫éOpenVLAÂú®LIBERO‰∏äÊèêÂçá‰∫Ü26.8%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÂü∫‰∫éÈ¢ÑËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÂèóÈôê‰∫éÂçïÂ∏ßÂõæÂÉèËåÉÂºèÔºåÊú™ËÉΩÂÖÖÂàÜÂà©Áî®Â§öÂ∏ßÂéÜÂè≤Êèê‰æõÁöÑÊó∂Èó¥‰ø°ÊÅØ„ÄÇÊú¨ÊñáÊèêÂá∫CronusVLAÔºå‰∏Ä‰∏™Áªü‰∏ÄÊ°ÜÊû∂ÔºåÂ∞ÜÂçïÂ∏ßVLAÊ®°ÂûãÊâ©Â±ïÂà∞Â§öÂ∏ßËåÉÂºè„ÄÇCronusVLAÈááÁî®‰∏§Èò∂ÊÆµËøáÁ®ãÔºöÈ¶ñÂÖàÂú®Â§ßËßÑÊ®°ÁöÑÂÖ∑Ë∫´Êï∞ÊçÆÈõÜ‰∏äËøõË°åÂçïÂ∏ßÈ¢ÑËÆ≠ÁªÉÔºåÂª∫Á´ãÊúâÊïàÁöÑÂÖ∑Ë∫´ËßÜËßâ-ËØ≠Ë®ÄÂü∫Á°ÄÔºõÂÖ∂Ê¨°ËøõË°åÂ§öÂ∏ßÂêéËÆ≠ÁªÉÔºåÈÄöËøáÁâπÂæÅÂùóËÅöÂêàÂéÜÂè≤‰ø°ÊÅØÔºåÈÄÇÂ∫îËßÜËßâ-ËØ≠Ë®Ä‰∏ªÂπ≤ÁöÑÈ¢ÑÊµã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCronusVLAÂú®Â§öÂ∏ßÂª∫Ê®°ÁöÑÊåëÊàò‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑ÊúâÊõ¥È´òÁöÑÊÄßËÉΩÂíåËßÇÂØüÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®ÂçïÂ∏ßÂõæÂÉèËåÉÂºè‰∏ãÁöÑÂ±ÄÈôêÊÄßÔºåÁâπÂà´ÊòØÊú™ËÉΩÂÖÖÂàÜÂà©Áî®Â§öÂ∏ßÂéÜÂè≤‰ø°ÊÅØÁöÑÈóÆÈ¢ò„ÄÇËøôÂØºËá¥‰∫ÜËÆ°ÁÆóÂºÄÈîÄÂíåÊé®ÁêÜÂª∂ËøüÁöÑÂ¢ûÂä†„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCronusVLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøá‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉËøáÁ®ãÔºåÈ¶ñÂÖàÂú®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äËøõË°åÂçïÂ∏ßÈ¢ÑËÆ≠ÁªÉÔºåÁÑ∂ÂêéËøõË°åÂ§öÂ∏ßÂêéËÆ≠ÁªÉÔºå‰ª•ÈÄÇÂ∫îÂ§öÂ∏ß‰ø°ÊÅØÁöÑÂ§ÑÁêÜ„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®ÊúâÊïàÊï¥ÂêàÊó∂Èó¥‰ø°ÊÅØÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCronusVLAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµÊòØÂçïÂ∏ßÈ¢ÑËÆ≠ÁªÉÔºå‰ΩøÁî®Ëá™ÂõûÂΩíÈ¢ÑÊµãÂä®‰ΩúÊ†áËÆ∞ÔºõÁ¨¨‰∫åÈò∂ÊÆµÊòØÂ§öÂ∏ßÂêéËÆ≠ÁªÉÔºåÂ∞ÜËßÜËßâ-ËØ≠Ë®Ä‰∏ªÂπ≤ÁöÑÈ¢ÑÊµã‰ªéÁ¶ªÊï£Ê†áËÆ∞ËΩ¨Âèò‰∏∫ÂèØÂ≠¶‰π†ÁâπÂæÅÔºåÂπ∂ÈÄöËøáÁâπÂæÅÂùóËÅöÂêàÂéÜÂè≤‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCronusVLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Â§öÂ∏ßÂêéËÆ≠ÁªÉÊú∫Âà∂ÔºåÈÄöËøáÁâπÂæÅÂùóËÅöÂêàÂéÜÂè≤‰ø°ÊÅØÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂú®Â§öÂ∏ßÂª∫Ê®°‰∏≠ÁöÑË°®Áé∞„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏éÁé∞ÊúâÂçïÂ∏ßÊ®°ÂûãÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÂÖ∂ÂØπÊó∂Èó¥‰ø°ÊÅØÁöÑÊúâÊïàÂà©Áî®„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏äÔºåCronusVLAÈááÁî®‰∫ÜÁâπÂæÅÂùóËÅöÂêàÊäÄÊúØÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®Â§ÑÁêÜÂ§öÂ∏ß‰ø°ÊÅØÊó∂ÂáèÂ∞ëËÆ°ÁÆóË¥üÊãÖ„ÄÇÊ≠§Â§ñÔºåÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁöÑ‰ºòÂåñ‰πü‰∏∫Ê®°ÂûãÁöÑÊÄßËÉΩÊèêÂçáÊèê‰æõ‰∫ÜÊîØÊåÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCronusVLAÂú®SimplerEnv‰∏äÂèñÂæó‰∫Ü70.9%ÁöÑÊàêÂäüÁéáÔºåÁõ∏ËæÉ‰∫éOpenVLAÂú®LIBERO‰∏äÊèêÂçá‰∫Ü26.8%„ÄÇÊ≠§Â§ñÔºåÂú®SimplerEnv-ORÂü∫ÂáÜÊµãËØï‰∏≠ÔºåCronusVLAÂ±ïÁé∞Âá∫ÊúÄÈ´òÁöÑÈ≤ÅÊ£íÊÄßÂàÜÊï∞ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§ÑÁêÜÊó∂Èó¥ÂíåÁ©∫Èó¥Âπ≤Êâ∞ÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CronusVLAÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Êú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®ÂåñÂà∂ÈÄ†ÂíåÊô∫ËÉΩÂÆ∂Â±ÖÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊìç‰ΩúËÉΩÂäõÔºåËØ•Ê®°ÂûãËÉΩÂ§üÂÆûÁé∞Êõ¥È´òÊïàÁöÑ‰ªªÂä°ÊâßË°åÂíåÊõ¥Âº∫ÁöÑÈÄÇÂ∫îÊÄßÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êô∫ËÉΩÊú∫Âô®‰∫∫Âú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂπøÊ≥õÈÉ®ÁΩ≤„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong performance in robotic manipulation. However, these models remain constrained by the single-frame image paradigm and fail to fully leverage the temporal information offered by multi-frame histories, as directly feeding multiple frames into VLM backbones incurs substantial computational overhead and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame pretraining on large-scale embodied datasets with autoregressive prediction of action tokens, establishing an effective embodied vision-language foundation; (2) Multi-frame post-training, which adapts the prediction of the vision-language backbone from discrete tokens to learnable features, and aggregates historical information via feature chunking. CronusVLA effectively addresses the existing challenges of multi-frame modeling while enhancing performance and observational robustness. To evaluate the robustness under temporal and spatial disturbances, we introduce SimplerEnv-OR, a novel benchmark featuring 24 types of observational disturbances and 120 severity levels. Experiments across three embodiments in simulated and real-world environments demonstrate that CronusVLA achieves leading performance and superior robustness, with a 70.9% success rate on SimplerEnv, a 26.8% improvement over OpenVLA on LIBERO, and the highest robustness score on SimplerEnv-OR. These results highlight the potential of efficient multi-frame adaptation in VLA models for more powerful and robust real-world deployment.

