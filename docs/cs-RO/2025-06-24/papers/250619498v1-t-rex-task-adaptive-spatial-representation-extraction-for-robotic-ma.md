---
layout: default
title: T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models
---

# T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19498" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19498v1</a>
  <a href="https://arxiv.org/pdf/2506.19498.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19498v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19498v1', 'T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiteng Chen, Wenbo Li, Shiyi Wang, Huiping Zhuang, Qingyao Wu

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

**å¤‡æ³¨**: submitted to NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºT-Rexæ¡†æ¶ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„ç©ºé—´è¡¨ç¤ºæå–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ç©ºé—´è¡¨ç¤º` `ä»»åŠ¡è‡ªé€‚åº”` `æ•ˆç‡æå‡` `æ™ºèƒ½æœºå™¨äºº` `å¤šä»»åŠ¡å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æœºå™¨äººæ“ä½œæ–¹æ³•é‡‡ç”¨å›ºå®šçš„ç©ºé—´è¡¨ç¤ºæå–æ–¹æ¡ˆï¼Œå¯¼è‡´è¡¨ç¤ºèƒ½åŠ›ä¸è¶³æˆ–æå–æ—¶é—´è¿‡é•¿ã€‚
2. æœ¬æ–‡æå‡ºçš„T-Rexæ¡†æ¶èƒ½å¤Ÿæ ¹æ®å…·ä½“ä»»åŠ¡éœ€æ±‚åŠ¨æ€é€‰æ‹©ç©ºé—´è¡¨ç¤ºæå–æ–¹æ¡ˆï¼Œä»è€Œæå‡ç³»ç»Ÿçš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚
3. é€šè¿‡åœ¨çœŸå®ç¯å¢ƒä¸­çš„å®éªŒï¼ŒT-Rexåœ¨ç©ºé—´ç†è§£ã€æ•ˆç‡å’Œç¨³å®šæ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ„å»ºä¸€ä¸ªèƒ½å¤Ÿåœ¨ç°å®ç¯å¢ƒä¸­æ‰§è¡Œå¤šç§ä»»åŠ¡çš„é€šç”¨æœºå™¨äººæ“ä½œç³»ç»Ÿæ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œä¸»è¦å¾—ç›Šäºå…¶ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­è·å¾—çš„å¹¿æ³›ä¸–ç•ŒçŸ¥è¯†ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œç©ºé—´è¡¨ç¤ºï¼ˆå¦‚è¡¨ç¤ºç‰©ä½“ä½ç½®çš„ç‚¹æˆ–è¡¨ç¤ºç‰©ä½“æ–¹å‘çš„å‘é‡ï¼‰ä½œä¸ºVLMsä¸ç°å®åœºæ™¯ä¹‹é—´çš„æ¡¥æ¢ï¼Œæœ‰æ•ˆåœ°å°†VLMsçš„æ¨ç†èƒ½åŠ›ä¸ç‰¹å®šä»»åŠ¡åœºæ™¯ç›¸ç»“åˆã€‚ç„¶è€Œï¼Œç°æœ‰åŸºäºVLMçš„æœºå™¨äººæ–¹æ³•é€šå¸¸é‡‡ç”¨å›ºå®šçš„ç©ºé—´è¡¨ç¤ºæå–æ–¹æ¡ˆï¼Œå¯¼è‡´è¡¨ç¤ºèƒ½åŠ›ä¸è¶³æˆ–æå–æ—¶é—´è¿‡é•¿ã€‚æœ¬æ–‡æå‡ºäº†T-Rexï¼Œä¸€ä¸ªä»»åŠ¡è‡ªé€‚åº”çš„ç©ºé—´è¡¨ç¤ºæå–æ¡†æ¶ï¼Œæ ¹æ®ç‰¹å®šä»»åŠ¡éœ€æ±‚åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„ç©ºé—´è¡¨ç¤ºæå–æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œä»»åŠ¡å¤æ‚æ€§å†³å®šäº†ç©ºé—´è¡¨ç¤ºçš„ç±»å‹å’Œç²’åº¦ï¼Œè€Œæ›´å¼ºçš„è¡¨ç¤ºèƒ½åŠ›é€šå¸¸ä¸æ›´é«˜çš„ç³»ç»Ÿæ“ä½œæˆæœ¬ç›¸å…³ã€‚é€šè¿‡åœ¨çœŸå®æœºå™¨äººç¯å¢ƒä¸­çš„ç»¼åˆå®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ç©ºé—´ç†è§£ã€æ•ˆç‡å’Œç¨³å®šæ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œè€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•ä¸­å›ºå®šç©ºé—´è¡¨ç¤ºæå–æ–¹æ¡ˆå¸¦æ¥çš„ä¸è¶³ï¼Œå¯¼è‡´åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šT-Rexæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ ¹æ®ä»»åŠ¡å¤æ‚æ€§åŠ¨æ€é€‰æ‹©ç©ºé—´è¡¨ç¤ºæå–æ–¹æ¡ˆï¼Œä»¥æé«˜è¡¨ç¤ºèƒ½åŠ›å’Œæ“ä½œæ•ˆç‡ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿçµæ´»åº”å¯¹ä¸åŒçš„æ“ä½œä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šT-Rexæ¡†æ¶åŒ…æ‹¬ä»»åŠ¡è¯†åˆ«æ¨¡å—ã€ç©ºé—´è¡¨ç¤ºé€‰æ‹©æ¨¡å—å’Œæ‰§è¡Œæ¨¡å—ã€‚ä»»åŠ¡è¯†åˆ«æ¨¡å—åˆ†æå½“å‰ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œç©ºé—´è¡¨ç¤ºé€‰æ‹©æ¨¡å—æ ¹æ®åˆ†æç»“æœé€‰æ‹©åˆé€‚çš„è¡¨ç¤ºæ–¹æ¡ˆï¼Œæ‰§è¡Œæ¨¡å—åˆ™è´Ÿè´£å…·ä½“çš„æ“ä½œæ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šT-Rexçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä»»åŠ¡è‡ªé€‚åº”çš„ç©ºé—´è¡¨ç¤ºæå–æœºåˆ¶ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å›ºå®šæå–æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚çµæ´»è°ƒæ•´è¡¨ç¤ºæ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒT-Rexé‡‡ç”¨äº†å¤šç§ç©ºé—´è¡¨ç¤ºç±»å‹ï¼Œå¹¶é€šè¿‡ä»»åŠ¡å¤æ‚æ€§è¯„ä¼°ç®—æ³•æ¥é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæ¡†æ¶ä¸­è¿˜è€ƒè™‘äº†è¡¨ç¤ºæå–çš„æ—¶é—´æ•ˆç‡ï¼Œä»¥å¹³è¡¡è¡¨ç¤ºèƒ½åŠ›ä¸ç³»ç»Ÿæ“ä½œæˆæœ¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨çœŸå®ç¯å¢ƒä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒT-Rexæ¡†æ¶åœ¨ç©ºé—´ç†è§£ã€æ•ˆç‡å’Œç¨³å®šæ€§æ–¹é¢ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºç©ºé—´ç†è§£å‡†ç¡®ç‡æé«˜äº†20%ï¼Œæ“ä½œæ•ˆç‡æå‡äº†30%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿å’ŒæœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼ŒT-Rexæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æé«˜æœºå™¨äººåœ¨å®é™…åº”ç”¨ä¸­çš„çµæ´»æ€§å’Œæ•ˆç‡ï¼Œæ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Building a general robotic manipulation system capable of performing a wide variety of tasks in real-world settings is a challenging task. Vision-Language Models (VLMs) have demonstrated remarkable potential in robotic manipulation tasks, primarily due to the extensive world knowledge they gain from large-scale datasets. In this process, Spatial Representations (such as points representing object positions or vectors representing object orientations) act as a bridge between VLMs and real-world scene, effectively grounding the reasoning abilities of VLMs and applying them to specific task scenarios. However, existing VLM-based robotic approaches often adopt a fixed spatial representation extraction scheme for various tasks, resulting in insufficient representational capability or excessive extraction time. In this work, we introduce T-Rex, a Task-Adaptive Framework for Spatial Representation Extraction, which dynamically selects the most appropriate spatial representation extraction scheme for each entity based on specific task requirements. Our key insight is that task complexity determines the types and granularity of spatial representations, and Stronger representational capabilities are typically associated with Higher overall system operation costs. Through comprehensive experiments in real-world robotic environments, we show that our approach delivers significant advantages in spatial understanding, efficiency, and stability without additional training.

