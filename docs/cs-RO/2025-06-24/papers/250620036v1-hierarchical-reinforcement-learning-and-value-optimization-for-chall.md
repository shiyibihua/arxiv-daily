---
layout: default
title: Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion
---

# Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20036" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20036v1</a>
  <a href="https://arxiv.org/pdf/2506.20036.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20036v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20036v1', 'Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jeremiah Coholich, Muhammad Ali Murtaza, Seth Hutchinson, Zsolt Kira

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å››è¶³æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸­çš„è¡Œèµ°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åˆ†å±‚å¼ºåŒ–å­¦ä¹ ` `å››è¶³æœºå™¨äºº` `å¤æ‚åœ°å½¢` `è·¯å¾„è§„åˆ’` `åœ¨çº¿ä¼˜åŒ–` `æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å››è¶³æœºå™¨äººè¡Œèµ°æ–¹æ³•åœ¨å¤æ‚åœ°å½¢ä¸Šè¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆçš„ç›®æ ‡é€‰æ‹©ä¸è·¯å¾„è§„åˆ’ã€‚
2. æœ¬ç ”ç©¶æå‡ºçš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¡†æ¶é€šè¿‡é«˜å±‚ç­–ç•¥ä¼˜åŒ–ä½å±‚ç­–ç•¥çš„ç›®æ ‡é€‰æ‹©ï¼Œæå‡äº†è¡Œèµ°æ•ˆç‡ä¸å®‰å…¨æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§åœ°å½¢ä¸Šç›¸è¾ƒäºä¼ ç»Ÿç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè·å¾—äº†æ›´é«˜çš„å¥–åŠ±å¹¶å‡å°‘äº†ç¢°æ’æ¬¡æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå››è¶³æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸Šçš„è¡Œèµ°ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤å±‚å±‚æ¬¡ç»“æ„ï¼Œé«˜å±‚ç­–ç•¥ï¼ˆHLPï¼‰ä¸ºä½å±‚ç­–ç•¥ï¼ˆLLPï¼‰é€‰æ‹©æœ€ä½³ç›®æ ‡ã€‚LLPä½¿ç”¨åŸºäºç­–ç•¥çš„æ¼”å‘˜-è¯„è®ºå®¶å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œå¹¶å°†è¶³è¿¹æ”¾ç½®ä½œä¸ºç›®æ ‡ã€‚HLPä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–ç¯å¢ƒæ ·æœ¬ï¼Œè€Œæ˜¯é€šè¿‡å¯¹LLPå­¦ä¹ çš„ä»·å€¼å‡½æ•°è¿›è¡Œåœ¨çº¿ä¼˜åŒ–æ¥è¿è¡Œã€‚æˆ‘ä»¬é€šè¿‡ä¸ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æ¯”è¾ƒï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„ä¼˜åŠ¿ï¼Œè§‚å¯Ÿåˆ°åœ¨ä¸åŒåœ°å½¢ä¸Šèƒ½å¤Ÿä»¥æ›´å°‘çš„ç¢°æ’è·å¾—æ›´é«˜çš„å¥–åŠ±ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªé‡åˆ°çš„æ›´å›°éš¾åœ°å½¢ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å››è¶³æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸Šè¡Œèµ°æ—¶çš„ç›®æ ‡é€‰æ‹©ä¸è·¯å¾„è§„åˆ’é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå•ä¸€çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œéš¾ä»¥é€‚åº”å¤šå˜çš„ç¯å¢ƒï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œç¢°æ’é£é™©å¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¡†æ¶é€šè¿‡å¼•å…¥é«˜å±‚ç­–ç•¥ï¼ˆHLPï¼‰å’Œä½å±‚ç­–ç•¥ï¼ˆLLPï¼‰çš„ä¸¤å±‚ç»“æ„ï¼Œä½¿å¾—é«˜å±‚ç­–ç•¥èƒ½å¤ŸåŠ¨æ€é€‰æ‹©ä½å±‚ç­–ç•¥çš„ç›®æ ‡ï¼Œä»è€Œä¼˜åŒ–è¡Œèµ°è·¯å¾„å’Œæé«˜é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé«˜å±‚ç­–ç•¥ï¼ˆHLPï¼‰å’Œä½å±‚ç­–ç•¥ï¼ˆLLPï¼‰ã€‚HLPè´Ÿè´£åœ¨çº¿ä¼˜åŒ–ç›®æ ‡é€‰æ‹©ï¼Œè€ŒLLPåˆ™é€šè¿‡æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œæ‰§è¡Œå…·ä½“çš„è¡Œèµ°ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºHLPçš„è®¾è®¡ï¼Œå®ƒæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–ç¯å¢ƒæ ·æœ¬ï¼Œè€Œæ˜¯é€šè¿‡å¯¹LLPå­¦ä¹ çš„ä»·å€¼å‡½æ•°è¿›è¡Œåœ¨çº¿ä¼˜åŒ–ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„ç›®æ ‡é€‰æ‹©ä¸è·¯å¾„è§„åˆ’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒLLPé‡‡ç”¨äº†åŸºäºç­–ç•¥çš„æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•ï¼Œè®¾ç½®äº†é€‚åº”å¤æ‚åœ°å½¢çš„æŸå¤±å‡½æ•°ï¼Œå¹¶è®¾è®¡äº†é€‚åˆå››è¶³æœºå™¨äººçš„ç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„ç¨³å®šæ€§ä¸æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨å¤šç§å¤æ‚åœ°å½¢ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜çš„å¥–åŠ±ï¼Œä¸”ç¢°æ’æ¬¡æ•°å‡å°‘äº†æ˜¾è‘—çš„æ¯”ä¾‹ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æ•‘æ´æœºå™¨äººã€å†œä¸šæœºå™¨äººç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªä¸è¡Œèµ°èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose a novel hierarchical reinforcement learning framework for quadruped locomotion over challenging terrain. Our approach incorporates a two-layer hierarchy in which a high-level policy (HLP) selects optimal goals for a low-level policy (LLP). The LLP is trained using an on-policy actor-critic RL algorithm and is given footstep placements as goals. We propose an HLP that does not require any additional training or environment samples and instead operates via an online optimization process over the learned value function of the LLP. We demonstrate the benefits of this framework by comparing it with an end-to-end reinforcement learning (RL) approach. We observe improvements in its ability to achieve higher rewards with fewer collisions across an array of different terrains, including terrains more difficult than any encountered during training.

