---
layout: default
title: Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference
---

# Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19303" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19303v1</a>
  <a href="https://arxiv.org/pdf/2506.19303.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19303v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19303v1', 'Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zexiang Guo, Hengxiang Chen, Xinheng Mai, Qiusang Qiu, Gan Ma, Zhanat Kappassov, Qiang Li, Nutan Chen

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

**å¤‡æ³¨**: This paper has been accepted by the 2025 International Conference on Climbing and Walking Robots (CLAWAR). These authors contributed equally to this work: Zexiang Guo, Hengxiang Chen, Xinheng Mai

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨æ¨¡æ€æ„ŸçŸ¥æ¡†æ¶ä»¥è§£å†³ç‰©ç†å±æ€§æ¨æ–­é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§¦è§‰æ„ŸçŸ¥` `è§†è§‰-è§¦è§‰èåˆ` `ç‰©ç†å±æ€§æ¨æ–­` `å¤šæ¨¡æ€é›†æˆ` `æœºå™¨äººæ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸ä»…ä¾èµ–è§¦è§‰æˆ–è§†è§‰æ•°æ®ï¼Œæ— æ³•å…¨é¢æ•æ‰ç‰©ä½“çš„ç‰©ç†å±æ€§ï¼Œé™åˆ¶äº†æœºå™¨äººçš„æ“ä½œèƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ„ŸçŸ¥æ¡†æ¶ï¼Œç»“åˆè§†è§‰è§‚å¯Ÿä¸è§¦è§‰è¡¨ç¤ºï¼Œåˆ©ç”¨å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œç‰©ç†å±æ€§æ¨æ–­ã€‚
3. åœ¨å¯¹35ç§ç‰©ä½“çš„è¯„ä¼°ä¸­ï¼Œæå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œå¹¶è¡¨ç°å‡ºè‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨æ–­ç‰©ç†å±æ€§èƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººæ“ä½œèƒ½åŠ›ï¼Œä½¿å…¶é€šè¿‡è‡ªé€‚åº”æŠ“å–ç­–ç•¥å®‰å…¨é«˜æ•ˆåœ°å¤„ç†ç‰©ä½“ã€‚ä»¥å¾€çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºè§¦è§‰æˆ–è§†è§‰æ•°æ®ï¼Œé™åˆ¶äº†å¯¹ç‰©ä½“å±æ€§çš„å…¨é¢æ•æ‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€æ„ŸçŸ¥æ¡†æ¶ï¼Œå°†è§†è§‰è§‚å¯Ÿä¸è§¦è§‰è¡¨ç¤ºæ•´åˆåœ¨ä¸€ä¸ªå¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„ç‰©ç†æ¨ç†æ¡†æ¶é‡‡ç”¨åˆ†å±‚ç‰¹å¾å¯¹é½æœºåˆ¶å’Œç²¾ç»†åŒ–æç¤ºç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåšå‡ºä¸çœŸå®æµ‹é‡é«˜åº¦ç›¸å…³çš„å±æ€§ç‰¹å®šé¢„æµ‹ã€‚åœ¨å¯¹35ç§å¤šæ ·åŒ–ç‰©ä½“çš„è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨ç‰©ç†å±æ€§æ¨æ–­ä¸­çš„å±€é™æ€§ï¼Œç°æœ‰æ–¹æ³•ä»…ä¾èµ–å•ä¸€æ¨¡æ€ï¼ˆè§¦è§‰æˆ–è§†è§‰ï¼‰ï¼Œæ— æ³•å…¨é¢ç†è§£ç‰©ä½“ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„è·¨æ¨¡æ€æ„ŸçŸ¥æ¡†æ¶é€šè¿‡æ•´åˆè§†è§‰å’Œè§¦è§‰æ•°æ®ï¼Œåˆ©ç”¨å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œç‰©ç†å±æ€§æ¨æ–­ï¼Œä»¥æé«˜æœºå™¨äººçš„æ“ä½œå®‰å…¨æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€ç‰¹å¾æå–ã€ç‰¹å¾å¯¹é½å’Œå±æ€§æ¨æ–­å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†è§†è§‰å’Œè§¦è§‰æ•°æ®ï¼Œç„¶åé€šè¿‡åˆ†å±‚ç‰¹å¾å¯¹é½æœºåˆ¶è¿›è¡Œèåˆï¼Œæœ€åè¿›è¡Œå±æ€§é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†åˆ†å±‚ç‰¹å¾å¯¹é½æœºåˆ¶å’Œç²¾ç»†åŒ–æç¤ºç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šå±æ€§è¿›è¡Œé«˜æ•ˆé¢„æµ‹ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€èåˆçš„æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–äº†ç‰¹å¾å¯¹é½çš„å‚æ•°è®¾ç½®ï¼Œå¹¶è®¾è®¡äº†é€‚åº”ä¸åŒç‰©ä½“å±æ€§çš„ç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¤šæ ·åŒ–ç‰©ä½“ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¯¹35ç§å¤šæ ·åŒ–ç‰©ä½“çš„è¯„ä¼°ä¸­ï¼Œæå‡ºçš„æ–¹æ³•åœ¨ç‰©ç†å±æ€§æ¨æ–­ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›æ–¹é¢ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–ä»“å‚¨ã€ç‰©ä½“è¯†åˆ«ä¸åˆ†ç±»ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹ç‰©ä½“ç‰©ç†å±æ€§çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´å®‰å…¨é«˜æ•ˆçš„æ“ä½œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inferring physical properties can significantly enhance robotic manipulation by enabling robots to handle objects safely and efficiently through adaptive grasping strategies. Previous approaches have typically relied on either tactile or visual data, limiting their ability to fully capture properties. We introduce a novel cross-modal perception framework that integrates visual observations with tactile representations within a multimodal vision-language model. Our physical reasoning framework, which employs a hierarchical feature alignment mechanism and a refined prompting strategy, enables our model to make property-specific predictions that strongly correlate with ground-truth measurements. Evaluated on 35 diverse objects, our approach outperforms existing baselines and demonstrates strong zero-shot generalization. Keywords: tactile perception, visual-tactile fusion, physical property inference, multimodal integration, robot perception

