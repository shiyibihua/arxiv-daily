---
layout: default
title: FlowVLA: Visual Chain of Thought-based Motion Reasoning for Vision-Language-Action Models
---

# FlowVLA: Visual Chain of Thought-based Motion Reasoning for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18269" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.18269v3</a>
  <a href="https://arxiv.org/pdf/2508.18269.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18269v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18269v3', 'FlowVLA: Visual Chain of Thought-based Motion Reasoning for Vision-Language-Action Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Tianran Zhang, Wenxuan Song, Jiayi Chen, Xinhu Zheng, Hesheng Wang, Haoang Li

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-25 (Êõ¥Êñ∞: 2025-10-07)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://irpn-lab.github.io/FlowVLA/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫FlowVLA‰ª•Ëß£ÂÜ≥ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã‰∏≠ÁöÑËøêÂä®Êé®ÁêÜÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `ËøêÂä®Êé®ÁêÜ` `Ëá™ÂõûÂΩíTransformer` `ÂÖâÊµÅÈ¢ÑÊµã` `Êú∫Âô®‰∫∫Êìç‰Ωú` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑVLAÊ®°ÂûãÂú®ËøêÂä®Êé®ÁêÜÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÁõ¥Êé•È¢ÑÊµãÊú™Êù•Â∏ßÁöÑÂ§ñËßÇÂØºËá¥ËßÜËßâÈ¢ÑÊµã‰∏çÂ§üÂêàÁêÜ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜËßÜËßâÊÄùÁª¥ÈìæÔºàVisual CoTÔºâÊ¶ÇÂøµÔºåË¶ÅÊ±ÇÊ®°ÂûãÂú®ÁîüÊàêÊú™Êù•Â∏ß‰πãÂâçÔºåÂÖàËøõË°åËøêÂä®Âä®ÊÄÅÊé®ÁêÜ„ÄÇ
3. FlowVLAÂú®Êú∫Âô®‰∫∫Êìç‰ΩúÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÁîüÊàêÁöÑËßÜËßâÈ¢ÑÊµãÊõ¥‰∏∫ËøûË¥ØÔºåÁ≠ñÁï•ÊÄßËÉΩËææÂà∞ÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºåÊ†∑Êú¨ÊïàÁéáÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËÆ∏Â§öËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°Âûã‰æùËµñ‰∫éÈÄöËøá‰∏ã‰∏ÄÂ∏ßÈ¢ÑÊµãËÆ≠ÁªÉÁöÑÂÜÖÈÉ®‰∏ñÁïåÊ®°Âûã„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ïÁõ¥Êé•È¢ÑÊµãÊú™Êù•Â∏ßÁöÑÂ§ñËßÇÔºåÁº∫‰πèÂØπÊΩúÂú®Âä®ÊÄÅÁöÑÊòéÁ°ÆÊé®ÁêÜÔºåÂØºËá¥ËßÜËßâÈ¢ÑÊµã‰∏çÂ§üÂêàÁêÜ‰∏îÁ≠ñÁï•Â≠¶‰π†ÊïàÁéá‰Ωé‰∏ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÂºïÂÖ•‰∫ÜËßÜËßâÊÄùÁª¥ÈìæÔºàVisual Chain of ThoughtÔºåVisual CoTÔºâÔºåË¶ÅÊ±ÇÊ®°ÂûãÂú®ÁîüÊàêÊú™Êù•Â∏ß‰πãÂâçÔºåÈ¶ñÂÖàÊé®ÁêÜËøêÂä®Âä®ÊÄÅ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜFlowVLAÔºåËøôÊòØ‰∏ÄÁßçËá™ÂõûÂΩíTransformerÔºåÊòéÁ°ÆÂ∞ÜËøô‰∏ÄÊé®ÁêÜËøáÁ®ãË°®Á§∫‰∏∫$v_t ightarrow f_t ightarrow v_{t+1}$ÔºåÂÖ∂‰∏≠$f_t$ÊòØÂõ∫ÊúâÁºñÁ†ÅËøêÂä®ÁöÑ‰∏≠Èó¥ÂÖâÊµÅÈ¢ÑÊµã„ÄÇÈÄöËøáÂº∫Âà∂Ê®°ÂûãÈÅµÂæ™Áî±$f_t$ÁºñÁ†ÅÁöÑËøêÂä®ËÆ°ÂàíÔºåËØ•ËøáÁ®ãËá™ÁÑ∂Âú∞Â∞ÜÂä®ÊÄÅÈ¢ÑÊµãÁöÑÈ¢ÑËÆ≠ÁªÉÁõÆÊ†á‰∏éÂä®‰ΩúÁîüÊàêÁöÑ‰∏ãÊ∏∏‰ªªÂä°ÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFlowVLA‰∏ç‰ªÖÁîüÊàêÊõ¥ËøûË¥Ø‰∏îÁâ©ÁêÜ‰∏äÂêàÁêÜÁöÑËßÜËßâÈ¢ÑÊµãÔºåËøòÂú®Á≠ñÁï•ÊÄßËÉΩ‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºåÂπ∂ÊòæËëóÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâVLAÊ®°ÂûãÂú®ËøêÂä®Êé®ÁêÜÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÁé∞ÊúâÊñπÊ≥ïÁõ¥Êé•È¢ÑÊµãÊú™Êù•Â∏ßÔºåÁº∫‰πèÂØπËøêÂä®Âä®ÊÄÅÁöÑÊòéÁ°ÆÊé®ÁêÜÔºåÂØºËá¥ÁîüÊàêÁöÑËßÜËßâÈ¢ÑÊµã‰∏çÂ§üÂêàÁêÜ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÊèêÂá∫ËßÜËßâÊÄùÁª¥ÈìæÔºàVisual CoTÔºâÔºåË¶ÅÊ±ÇÊ®°ÂûãÈ¶ñÂÖàÊé®ÁêÜËøêÂä®Âä®ÊÄÅÔºåÁÑ∂ÂêéÂÜçÁîüÊàêÊú™Êù•Â∏ß„ÄÇËøô‰∏ÄËÆæËÆ°‰ΩøÂæóÂä®ÊÄÅÈ¢ÑÊµãÁöÑÈ¢ÑËÆ≠ÁªÉÁõÆÊ†á‰∏é‰∏ãÊ∏∏Âä®‰ΩúÁîüÊàê‰ªªÂä°Áõ∏ÂØπÈΩê„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöFlowVLAÈááÁî®Ëá™ÂõûÂΩíTransformerÊû∂ÊûÑÔºåÊï¥‰ΩìÊµÅÁ®ã‰∏∫$v_t ightarrow f_t ightarrow v_{t+1}$ÔºåÂÖ∂‰∏≠$f_t$ÊòØÂÖâÊµÅÈ¢ÑÊµãÔºåÊ®°ÂûãÈÄöËøáËøô‰∏Ä‰∏≠Èó¥Ê≠•È™§ËøõË°åËøêÂä®Êé®ÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöFlowVLAÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜËøêÂä®Êé®ÁêÜÊ≠•È™§ÔºåÂ∞ÜÂÖâÊµÅÈ¢ÑÊµã‰Ωú‰∏∫‰∏≠Èó¥Ë°®Á§∫ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêÁöÑËßÜËßâÈ¢ÑÊµãÁöÑÁâ©ÁêÜÂêàÁêÜÊÄßÂíåËøûË¥ØÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜËá™ÂõûÂΩíÊú∫Âà∂ÂíåÂÖâÊµÅÈ¢ÑÊµãÊ®°ÂùóÔºåÊçüÂ§±ÂáΩÊï∞ÁªìÂêà‰∫ÜÂä®ÊÄÅÈ¢ÑÊµãÂíåÂä®‰ΩúÁîüÊàêÁöÑÁõÆÊ†áÔºå‰ª•Á°Æ‰øùÊ®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ËÉΩÂ§üÊúâÊïàÂ≠¶‰π†ËøêÂä®Âä®ÊÄÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåFlowVLAÂú®Êú∫Âô®‰∫∫Êìç‰ΩúÂü∫ÂáÜÊµãËØï‰∏≠ÁîüÊàêÁöÑËßÜËßâÈ¢ÑÊµãÊØîÁé∞ÊúâÊñπÊ≥ïÊõ¥‰∏∫ËøûË¥Ø‰∏îÁâ©ÁêÜ‰∏äÂêàÁêÜÔºåÁ≠ñÁï•ÊÄßËÉΩËææÂà∞‰∫ÜÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºåÊ†∑Êú¨ÊïàÁéáÊòæËëóÊèêÈ´òÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶Êú™Áü•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®È©æÈ©∂„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÔºåËÉΩÂ§ü‰∏∫Ëøô‰∫õÈ¢ÜÂüüÊèê‰æõÊõ¥‰∏∫ÂêàÁêÜÁöÑËßÜËßâÈ¢ÑÊµãÂíåÂÜ≥Á≠ñÊîØÊåÅ„ÄÇÊú™Êù•ÔºåFlowVLAÂèØËÉΩÊé®Âä®Êõ¥È´òÊïàÁöÑÂ§öÊ®°ÊÄÅÂ≠¶‰π†Âíå‰∫∫Êú∫‰∫§‰∫íÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Many Vision-Language-Action (VLA) models are built upon an internal world model trained via next-frame prediction ``$v_t \rightarrow v_{t+1}$''. However, this paradigm attempts to predict the future frame's appearance directly, without explicitly reasoning about the underlying dynamics. \textbf{This lack of an explicit motion reasoning step} often leads to physically implausible visual forecasts and inefficient policy learning. To address this limitation, we introduce the \textbf{Visual Chain of Thought (Visual CoT)}, a paradigm that compels the model to first reason about \textbf{motion dynamics} before generating the future frame. We instantiate this paradigm by proposing \textbf{FlowVLA}, an autoregressive Transformer that explicitly materializes this reasoning process as ``$v_t \rightarrow f_t \rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction that inherently encodes motion. By forcing the model to first follow the motion plan encoded by $f_t$, this process inherently \textbf{aligns the pre-training objective of dynamics prediction with the downstream task of action generation.} We conduct experiments on challenging robotics manipulation benchmarks, as well as real-robot evaluations. Our FlowVLA not only generates \textbf{more coherent and physically plausible visual predictions}, but also achieves state-of-the-art policy performance with \textbf{substantially improved sample efficiency}, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/

