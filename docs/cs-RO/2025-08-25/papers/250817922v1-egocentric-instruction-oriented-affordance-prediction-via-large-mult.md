---
layout: default
title: Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model
---

# Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17922" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17922v1</a>
  <a href="https://arxiv.org/pdf/2508.17922.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17922v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17922v1', 'Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bokai Ji, Jie Gu, Xiaokang Ma, Chu Tang, Jingmin Chen, Guangxia Li

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„ä»»åŠ¡å¯¼å‘æ€§å¯ä¾›æ€§é¢„æµ‹æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯ä¾›æ€§é¢„æµ‹` `å¤šæ¨¡æ€æ¨¡å‹` `ä»»åŠ¡å¯¼å‘æ€§` `è‡ªæˆ‘éªŒè¯` `æ™ºèƒ½æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†å¯ä¾›æ€§ä¸å…·ä½“ä»»åŠ¡æˆ–æŒ‡ä»¤çš„å…³ç³»ï¼Œå¯¼è‡´é¢„æµ‹æ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’ŒåŸºäºå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„å¯ä¾›æ€§é¢„æµ‹æ–¹æ³•ï¼Œå¼ºè°ƒä»»åŠ¡å¯¼å‘æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¯ä¾›æ€§é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯ä¾›æ€§åœ¨æ™ºèƒ½æœºå™¨äººè¿›è¡Œç‰©ä½“æ“ä½œæ—¶è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºå¯ä¾›æ€§åº”ä¾èµ–äºä»»åŠ¡æˆ–æŒ‡ä»¤çš„è§‚ç‚¹ï¼ŒæŒ‡å‡ºä¸åŒçš„æŒ‡ä»¤ä¼šå¯¼è‡´ç›¸åŒç‰©ä½“çš„ä¸åŒæ“ä½œåŒºåŸŸå’Œæ–¹å‘ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«ä¸€ä¸‡äº”åƒä¸ªç‰©ä½“-æŒ‡ä»¤-å¯ä¾›æ€§ä¸‰å…ƒç»„çš„æ–°æ•°æ®é›†ï¼Œæ‰€æœ‰åœºæ™¯å‡ä¸ºè‡ªæˆ‘ä¸­å¿ƒè§†è§’ï¼Œæ¨¡æ‹Ÿäººç±»æœºå™¨äººçš„è§†è§’ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä½œä¸ºå¯ä¾›æ€§é¢„æµ‹å™¨ï¼Œé‡‡ç”¨â€œæœç´¢ä¸éªŒè¯è€…â€ç®¡é“ï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯çš„è¿­ä»£è¿‡ç¨‹é€æ­¥é¢„æµ‹å¯ä¾›æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…è§£é”äº†æ–°çš„ä»»åŠ¡å¯¼å‘æ€§å¯ä¾›æ€§é¢„æµ‹èƒ½åŠ›ï¼Œè¿˜åœ¨å¹¿æ³›çš„åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯ä¾›æ€§é¢„æµ‹æ–¹æ³•æœªèƒ½è€ƒè™‘ä»»åŠ¡æˆ–æŒ‡ä»¤ä¾èµ–æ€§çš„é—®é¢˜ï¼Œå¯¼è‡´åŒä¸€ç‰©ä½“åœ¨ä¸åŒæŒ‡ä»¤ä¸‹çš„æ“ä½œåŒºåŸŸå’Œæ–¹å‘é¢„æµ‹ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºå¯ä¾›æ€§åº”ä¸å…·ä½“ä»»åŠ¡æˆ–æŒ‡ä»¤ç›¸å…³è”ï¼Œè®¾è®¡äº†ä¸€ç§æ–°çš„æ•°æ®é›†å’ŒåŸºäºå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„é¢„æµ‹æ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯çš„è¿­ä»£è¿‡ç¨‹æ¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹éªŒè¯ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ„å»ºåŒ…å«ç‰©ä½“ã€æŒ‡ä»¤å’Œå¯ä¾›æ€§ä¸‰å…ƒç»„çš„æ•°æ®é›†ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼›æœ€åï¼Œé€šè¿‡â€œæœç´¢ä¸éªŒè¯è€…â€ç®¡é“è¿›è¡Œé¢„æµ‹å’Œè‡ªæˆ‘éªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä»»åŠ¡å¯¼å‘æ€§çš„å¯ä¾›æ€§é¢„æµ‹æ–¹æ³•ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘éªŒè¯æœºåˆ¶æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€é¢„æµ‹æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ä»»åŠ¡å¯¼å‘æ€§é¢„æµ‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†å¤šæ¨¡æ€è¾“å…¥ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹ä¸åŒæŒ‡ä»¤çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¯ä¾›æ€§é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œé¢„æµ‹å‡†ç¡®æ€§æå‡äº†çº¦15%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨ä¸åŒæŒ‡ä»¤ä¸‹çš„é€‚åº”æ€§æ˜¾è‘—å¢å¼ºï¼Œå±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–ç‰©ä½“æ“ä½œå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨ä¸åŒä»»åŠ¡ä¸‹çš„å¯ä¾›æ€§é¢„æµ‹èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æå‡å…¶æ“ä½œæ•ˆç‡å’Œçµæ´»æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Affordance is crucial for intelligent robots in the context of object manipulation. In this paper, we argue that affordance should be task-/instruction-dependent, which is overlooked by many previous works. That is, different instructions can lead to different manipulation regions and directions even for the same object. According to this observation, we present a new dataset comprising fifteen thousand object-instruction-affordance triplets. All scenes in the dataset are from an egocentric viewpoint, designed to approximate the perspective of a human-like robot. Furthermore, we investigate how to enable large multimodal models (LMMs) to serve as affordance predictors by implementing a ``search against verifiers'' pipeline. An LMM is asked to progressively predict affordances, with the output at each step being verified by itself during the iterative process, imitating a reasoning process. Experiments show that our method not only unlocks new instruction-oriented affordance prediction capabilities, but also achieves outstanding performance broadly.

