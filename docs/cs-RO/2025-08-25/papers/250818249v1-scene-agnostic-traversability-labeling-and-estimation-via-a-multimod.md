---
layout: default
title: Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework
---

# Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18249" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18249v1</a>
  <a href="https://arxiv.org/pdf/2508.18249.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18249v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18249v1', 'Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zipeng Fang, Yanbo Wang, Lei Zhao, Weidong Chen

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€è‡ªç›‘ç£æ¡†æ¶ä»¥è§£å†³å¯é€šè¡Œæ€§ä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯é€šè¡Œæ€§ä¼°è®¡` `è‡ªç›‘ç£å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `æ¿€å…‰é›·è¾¾` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨æ•æ‰ä¸å¯é€šè¡ŒåŒºåŸŸç‰¹å¾æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸”å¤šé›†ä¸­äºå•ä¸€æ¨¡æ€ã€‚
2. æœ¬æ–‡æå‡ºçš„å¤šæ¨¡æ€è‡ªç›‘ç£æ¡†æ¶é€šè¿‡æ•´åˆå¤šç§ä¼ æ„Ÿå™¨æ•°æ®ï¼Œæå‡äº†å¯é€šè¡Œæ€§æ ‡æ³¨å’Œä¼°è®¡çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†çº¦88%çš„IoUï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•æå‡äº†1.6-3.5%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯é€šè¡Œæ€§ä¼°è®¡å¯¹äºä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„åœ°å½¢å’Œç¯å¢ƒä¸­å¯¼èˆªè‡³å…³é‡è¦ã€‚å°½ç®¡è¿‘æœŸçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æ•æ‰ä¸å¯é€šè¡ŒåŒºåŸŸçš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°å…ˆå‰çš„ç ”ç©¶é›†ä¸­äºå•ä¸€æ¨¡æ€ï¼Œå¿½è§†äº†æ•´åˆå¼‚æ„ä¼ æ„Ÿå™¨æ¨¡æ€æ‰€å¸¦æ¥çš„äº’è¡¥ä¼˜åŠ¿ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è‡ªç›‘ç£æ¡†æ¶ç”¨äºå¯é€šè¡Œæ€§æ ‡æ³¨å’Œä¼°è®¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„æ ‡æ³¨æµç¨‹æ•´åˆäº†è¶³è¿¹ã€æ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®ï¼Œç”Ÿæˆè€ƒè™‘è¯­ä¹‰å’Œå‡ ä½•çº¿ç´¢çš„å¯é€šè¡Œæ€§æ ‡ç­¾ã€‚ç„¶åï¼Œåˆ©ç”¨è¿™äº›æ ‡ç­¾ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŒæµç½‘ç»œï¼Œä»¥è§£è€¦çš„æ–¹å¼å…±åŒå­¦ä¹ ä¸åŒæ¨¡æ€ï¼Œå¢å¼ºå…¶è¯†åˆ«å¤šæ ·åŒ–å¯é€šè¡Œæ€§æ¨¡å¼çš„èƒ½åŠ›ã€‚æœ€åï¼Œåœ¨åŸå¸‚ã€è¶Šé‡å’Œæ ¡å›­ç¯å¢ƒä¸­è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯é€šè¡Œæ€§ä¼°è®¡æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰ä¸å¯é€šè¡ŒåŒºåŸŸç‰¹å¾çš„é—®é¢˜ï¼Œä¸”å¤§å¤šæ•°æ–¹æ³•ä»…ä¾èµ–å•ä¸€æ¨¡æ€ï¼Œå¯¼è‡´ä¼°è®¡ç»“æœçš„é²æ£’æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§å¤šæ¨¡æ€è‡ªç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè¶³è¿¹ã€æ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®ï¼Œç”Ÿæˆæ›´å…¨é¢çš„å¯é€šè¡Œæ€§æ ‡ç­¾ï¼Œä»è€Œæå‡æ¨¡å‹å¯¹å¤šæ ·åŒ–åœ°å½¢çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ ‡æ³¨æµç¨‹å’ŒåŒæµç½‘ç»œã€‚æ ‡æ³¨æµç¨‹æ•´åˆä¸åŒæ¨¡æ€æ•°æ®ç”Ÿæˆæ ‡ç­¾ï¼ŒåŒæµç½‘ç»œåˆ™è§£è€¦å­¦ä¹ ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼Œä»¥å¢å¼ºå¯¹å¯é€šè¡Œæ€§æ¨¡å¼çš„è¯†åˆ«èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å¤šæ¨¡æ€æ•°æ®çš„æ•´åˆä¸è‡ªç›‘ç£å­¦ä¹ çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†å¯¹å¤æ‚ç¯å¢ƒä¸­å¯é€šè¡Œæ€§åŒºåŸŸçš„è¯†åˆ«èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨åŒæµç½‘ç»œè®¾è®¡ï¼Œåˆ†åˆ«å¤„ç†ä¸åŒæ¨¡æ€æ•°æ®ï¼›æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆç¨€ç–æ¿€å…‰é›·è¾¾ç›‘ç£ä»¥å‡å°‘ä¼ªæ ‡ç­¾å¼•å…¥çš„å™ªå£°ï¼Œç¡®ä¿æ¨¡å‹å­¦ä¹ çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æè‡ªåŠ¨æ ‡æ³¨æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†çº¦88%çš„IoUï¼Œç›¸è¾ƒäºç°æœ‰è‡ªç›‘ç£æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ‰€æå¤šæ¨¡æ€å¯é€šè¡Œæ€§ä¼°è®¡ç½‘ç»œåœ¨æ‰€æœ‰è¯„ä¼°æ•°æ®é›†ä¸Šå‡æå‡äº†1.6-3.5%çš„IoUï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»å¯¼èˆªæœºå™¨äººã€æ— äººé©¾é©¶æ±½è½¦å’Œæ™ºèƒ½åŸå¸‚åŸºç¡€è®¾æ–½ç­‰ã€‚é€šè¿‡æå‡å¯é€šè¡Œæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œä¿ƒè¿›æ™ºèƒ½äº¤é€šå’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traversability estimation is critical for enabling robots to navigate across diverse terrains and environments. While recent self-supervised learning methods achieve promising results, they often fail to capture the characteristics of non-traversable regions. Moreover, most prior works concentrate on a single modality, overlooking the complementary strengths offered by integrating heterogeneous sensory modalities for more robust traversability estimation. To address these limitations, we propose a multimodal self-supervised framework for traversability labeling and estimation. First, our annotation pipeline integrates footprint, LiDAR, and camera data as prompts for a vision foundation model, generating traversability labels that account for both semantic and geometric cues. Then, leveraging these labels, we train a dual-stream network that jointly learns from different modalities in a decoupled manner, enhancing its capacity to recognize diverse traversability patterns. In addition, we incorporate sparse LiDAR-based supervision to mitigate the noise introduced by pseudo labels. Finally, extensive experiments conducted across urban, off-road, and campus environments demonstrate the effectiveness of our approach. The proposed automatic labeling method consistently achieves around 88% IoU across diverse datasets. Compared to existing self-supervised state-of-the-art methods, our multimodal traversability estimation network yields consistently higher IoU, improving by 1.6-3.5% on all evaluated datasets.

