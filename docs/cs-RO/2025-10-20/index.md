---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-10-20
---

# cs.ROï¼ˆ2025-10-20ï¼‰

ğŸ“Š å…± **21** ç¯‡è®ºæ–‡


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (16)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (16 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251017801v1-robobench-a-comprehensive-evaluation-benchmark-for-multimodal-large-.html">Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain</a></td>
  <td>RoboBenchï¼šç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºå…·èº«æ™ºèƒ½å¤§è„‘çš„ç»¼åˆåŸºå‡†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17801v1" onclick="toggleFavorite(this, '2510.17801v1', 'Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251017369v1-bridging-embodiment-gaps-deploying-vision-language-action-models-on-.html">Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots</a></td>
  <td>æå‡ºå°†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åº”ç”¨äºè½¯æœºå™¨äººä»¥è§£å†³å®‰å…¨äº¤äº’é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17369v1" onclick="toggleFavorite(this, '2510.17369v1', 'Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251017111v3-efficient-vision-language-action-models-for-embodied-manipulation-a-.html">Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey</a></td>
  <td>ç»¼è¿°é«˜æ•ˆè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œè§£å†³å…·èº«æ“ä½œä¸­è®¡ç®—èµ„æºå—é™é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17111v3" onclick="toggleFavorite(this, '2510.17111v3', 'Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251018002v1-humanoid-goalkeeper-learning-from-position-conditioned-task-motion-c.html">Humanoid Goalkeeper: Learning from Position Conditioned Task-Motion Constraints</a></td>
  <td>æå‡ºåŸºäºä½ç½®æ¡ä»¶ä»»åŠ¡-è¿åŠ¨çº¦æŸçš„äººå½¢æœºå™¨äººå®ˆé—¨å‘˜å¼ºåŒ–å­¦ä¹ æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18002v1" onclick="toggleFavorite(this, '2510.18002v1', 'Humanoid Goalkeeper: Learning from Position Conditioned Task-Motion Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251017640v2-resample-a-robust-data-augmentation-framework-via-exploratory-sampli.html">RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation</a></td>
  <td>RESampleï¼šæ¢ç´¢å¼é‡‡æ ·å¢å¼ºæœºå™¨äººæ“ä½œçš„é²æ£’æ•°æ®å¢å¼ºæ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17640v2" onclick="toggleFavorite(this, '2510.17640v2', 'RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251017792v1-softmimic-learning-compliant-whole-body-control-from-examples.html">SoftMimic: Learning Compliant Whole-body Control from Examples</a></td>
  <td>SoftMimicï¼šä»ç¤ºä¾‹ä¸­å­¦ä¹ æŸ”é¡ºçš„äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17792v1" onclick="toggleFavorite(this, '2510.17792v1', 'SoftMimic: Learning Compliant Whole-body Control from Examples')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251017143v1-decentralized-real-time-planning-for-multi-uav-cooperative-manipulat.html">Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning</a></td>
  <td>æå‡ºåŸºäºæ¨¡ä»¿å­¦ä¹ çš„æ— äººæœºååŒæ“ä½œåˆ†æ•£å¼å®æ—¶è§„åˆ’æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17143v1" onclick="toggleFavorite(this, '2510.17143v1', 'Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251017249v1-an-adaptive-hierarchical-control-framework-for-quadrupedal-robots-in.html">An adaptive hierarchical control framework for quadrupedal robots in planetary exploration</a></td>
  <td>æå‡ºä¸€ç§è‡ªé€‚åº”åˆ†å±‚æ§åˆ¶æ¡†æ¶ï¼Œç”¨äºè¡Œæ˜Ÿæ¢æµ‹å››è¶³æœºå™¨äººæœªçŸ¥ç¯å¢ƒå¯¼èˆªã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17249v1" onclick="toggleFavorite(this, '2510.17249v1', 'An adaptive hierarchical control framework for quadrupedal robots in planetary exploration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251017270v1-floating-base-deep-lagrangian-networks.html">Floating-Base Deep Lagrangian Networks</a></td>
  <td>æå‡ºFeLaNï¼Œç»“åˆæ·±åº¦å­¦ä¹ ä¸æ‹‰æ ¼æœ—æ—¥åŠ›å­¦ï¼Œè§£å†³æµ®åŠ¨åŸºåº§ç³»ç»Ÿç‰©ç†çº¦æŸå»ºæ¨¡é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17270v1" onclick="toggleFavorite(this, '2510.17270v1', 'Floating-Base Deep Lagrangian Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251017541v1-distributed-spatial-temporal-trajectory-optimization-for-unmanned-ae.html">Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm</a></td>
  <td>æå‡ºåŸºäºADMMå’ŒDDPçš„åˆ†å¸ƒå¼æ—¶ç©ºè½¨è¿¹ä¼˜åŒ–æ¡†æ¶ï¼Œè§£å†³æ— äººæœºé›†ç¾¤è½¨è¿¹è§„åˆ’é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17541v1" onclick="toggleFavorite(this, '2510.17541v1', 'Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251017150v2-omnivic-a-self-improving-variable-impedance-controller-with-vision-l.html">OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation</a></td>
  <td>OmniVICï¼šåŸºäºè§†è§‰è¯­è¨€ä¸Šä¸‹æ–‡å­¦ä¹ çš„è‡ªæå‡å˜é˜»æŠ—æ§åˆ¶å™¨ï¼Œç”¨äºå®‰å…¨æœºå™¨äººæ“ä½œ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17150v2" onclick="toggleFavorite(this, '2510.17150v2', 'OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251017576v1-intent-driven-llm-ensemble-planning-for-flexible-multi-robot-disasse.html">Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries</a></td>
  <td>æå‡ºæ„å›¾é©±åŠ¨çš„LLMé›†æˆè§„åˆ’æ–¹æ³•ï¼Œç”¨äºæŸ”æ€§å¤šæœºå™¨äººæ‹†å¸ç”µåŠ¨æ±½è½¦ç”µæ± ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17576v1" onclick="toggleFavorite(this, '2510.17576v1', 'Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251017525v1-humanmpc-safe-and-efficient-mav-navigation-among-humans.html">HumanMPC - Safe and Efficient MAV Navigation among Humans</a></td>
  <td>HumanMPCï¼šé¢å‘äººæœºå…±å­˜ç¯å¢ƒçš„å®‰å…¨é«˜æ•ˆæ— äººæœºå¯¼èˆª</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17525v1" onclick="toggleFavorite(this, '2510.17525v1', 'HumanMPC - Safe and Efficient MAV Navigation among Humans')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251017315v1-implicit-state-estimation-via-video-replanning.html">Implicit State Estimation via Video Replanning</a></td>
  <td>æå‡ºåŸºäºè§†é¢‘é‡è§„åˆ’çš„éšå¼çŠ¶æ€ä¼°è®¡æ¡†æ¶ï¼Œæå‡äº¤äº’å¼æ“ä½œä»»åŠ¡çš„é€‚åº”æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17315v1" onclick="toggleFavorite(this, '2510.17315v1', 'Implicit State Estimation via Video Replanning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251017335v3-ddbot-differentiable-physics-based-digging-robot-for-unknown-granula.html">DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials</a></td>
  <td>DDBotï¼šç”¨äºæœªçŸ¥é¢—ç²’ææ–™çš„å¯å¾®ç‰©ç†æŒ–æ˜æœºå™¨äºº</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17335v3" onclick="toggleFavorite(this, '2510.17335v3', 'DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251017086v1-learning-to-design-soft-hands-using-reward-models.html">Learning to Design Soft Hands using Reward Models</a></td>
  <td>æå‡ºåŸºäºå¥–åŠ±æ¨¡å‹çš„äº¤å‰ç†µæ–¹æ³•ï¼Œé«˜æ•ˆä¼˜åŒ–æŸ”æ€§æ‰‹çˆªè®¾è®¡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17086v1" onclick="toggleFavorite(this, '2510.17086v1', 'Learning to Design Soft Hands using Reward Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/251017439v1-from-spatial-to-actions-grounding-vision-language-action-model-in-sp.html">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</a></td>
  <td>FALCONï¼šåˆ©ç”¨ç©ºé—´åŸºç¡€å…ˆéªŒå¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„3Dç¯å¢ƒæ³›åŒ–èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17439v1" onclick="toggleFavorite(this, '2510.17439v1', 'From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251017950v1-robochallenge-large-scale-real-robot-evaluation-of-embodied-policies.html">RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies</a></td>
  <td>RoboChallengeï¼šå¤§è§„æ¨¡çœŸå®æœºå™¨äººç¯å¢ƒä¸‹çš„å…·èº«ç­–ç•¥è¯„ä¼°ç³»ç»Ÿ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17950v1" onclick="toggleFavorite(this, '2510.17950v1', 'RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/251017148v4-diffvla-bridging-cognitive-reasoning-and-end-to-end-driving-through-.html">DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment</a></td>
  <td>DiffVLA++ï¼šé€šè¿‡åº¦é‡å¼•å¯¼å¯¹é½æ¡¥æ¥è®¤çŸ¥æ¨ç†ä¸ç«¯åˆ°ç«¯é©¾é©¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17148v4" onclick="toggleFavorite(this, '2510.17148v4', 'DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251018137v1-quality-over-quantity-curating-contact-based-robot-datasets-improves.html">Quality Over Quantity: Curating Contact-Based Robot Datasets Improves Learning</a></td>
  <td>æå‡ºåŸºäºæ¥è§¦æ„ŸçŸ¥çš„æœºå™¨äººæ•°æ®ç­›é€‰æ–¹æ³•ï¼Œæå‡æœºå™¨äººå­¦ä¹ æ•ˆç‡ä¸ç¡®å®šæ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18137v1" onclick="toggleFavorite(this, '2510.18137v1', 'Quality Over Quantity: Curating Contact-Based Robot Datasets Improves Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251018085v1-r2bc-multi-agent-imitation-learning-from-single-agent-demonstrations.html">R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations</a></td>
  <td>R2BCï¼šä»å•æ™ºèƒ½ä½“æ¼”ç¤ºä¸­å­¦ä¹ å¤šæ™ºèƒ½ä½“åä½œç­–ç•¥</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18085v1" onclick="toggleFavorite(this, '2510.18085v1', 'R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)