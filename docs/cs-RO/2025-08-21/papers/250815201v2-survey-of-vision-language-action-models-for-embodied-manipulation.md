---
layout: default
title: Survey of Vision-Language-Action Models for Embodied Manipulation
---

# Survey of Vision-Language-Action Models for Embodied Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15201" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15201v2</a>
  <a href="https://arxiv.org/pdf/2508.15201.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15201v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15201v2', 'Survey of Vision-Language-Action Models for Embodied Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoran Li, Yuhui Chen, Wenbo Cui, Weiheng Liu, Kai Liu, Mingcai Zhou, Zhengtao Zhang, Dongbin Zhao

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-21 (æ›´æ–°: 2025-11-12)

**å¤‡æ³¨**: in Chinese language

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä»¥æå‡å…·èº«æ“ä½œèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `å…·èº«æ™ºèƒ½` `æœºå™¨äººæ§åˆ¶` `å¤šæ¨¡æ€èåˆ` `ç¯å¢ƒäº’åŠ¨` `æ¨¡å‹è¯„ä¼°` `é¢„è®­ç»ƒæ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰å…·èº«æ™ºèƒ½ç³»ç»Ÿåœ¨ç¯å¢ƒäº’åŠ¨èƒ½åŠ›ä¸Šä»å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸å¤Ÿç†æƒ³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„é€šç”¨æœºå™¨äººæ§åˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ä»£ç†ä¸ç¯å¢ƒçš„äº’åŠ¨èƒ½åŠ›ã€‚
3. é€šè¿‡å¯¹ç°æœ‰VLAæ¨¡å‹çš„ç³»ç»Ÿæ€§åˆ†æï¼Œæœ¬æ–‡ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦çš„è§è§£å’ŒæŒ‡å¯¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«æ™ºèƒ½ç³»ç»Ÿé€šè¿‡ä¸ç¯å¢ƒçš„æŒç»­äº’åŠ¨å¢å¼ºä»£ç†èƒ½åŠ›ï¼Œè¿‘å¹´æ¥å—åˆ°å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„å¹¿æ³›å…³æ³¨ã€‚è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å—å¤§å‹åŸºç¡€æ¨¡å‹çš„å¯å‘ï¼Œä½œä¸ºé€šç”¨çš„æœºå™¨äººæ§åˆ¶æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†å…·èº«æ™ºèƒ½ç³»ç»Ÿä¸­ä»£ç†ä¸ç¯å¢ƒçš„äº’åŠ¨èƒ½åŠ›ï¼Œæ‹“å®½äº†å…·èº«AIæœºå™¨äººçš„åº”ç”¨åœºæ™¯ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†ç”¨äºå…·èº«æ“ä½œçš„VLAæ¨¡å‹ï¼Œé¦–å…ˆæ¢³ç†äº†VLAæ¶æ„çš„å‘å±•å†ç¨‹ï¼Œéšåå¯¹å½“å‰ç ”ç©¶åœ¨æ¨¡å‹ç»“æ„ã€è®­ç»ƒæ•°æ®é›†ã€é¢„è®­ç»ƒæ–¹æ³•ã€åè®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹è¯„ä¼°ç­‰äº”ä¸ªå…³é”®ç»´åº¦è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œæœ€åæ€»ç»“äº†VLAå‘å±•å’Œå®é™…éƒ¨ç½²ä¸­çš„ä¸»è¦æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æœ‰å¸Œæœ›æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­ä¸ç¯å¢ƒäº’åŠ¨èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ—¶å­˜åœ¨å±€é™ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆçš„æ“ä½œå’Œå†³ç­–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡æ•´åˆè§†è§‰ä¿¡æ¯ã€è¯­è¨€ç†è§£å’ŒåŠ¨ä½œæ‰§è¡Œï¼Œæå‡ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›å’Œæ“ä½œç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬äº”ä¸ªä¸»è¦æ¨¡å—ï¼šVLAæ¨¡å‹ç»“æ„ã€è®­ç»ƒæ•°æ®é›†ã€é¢„è®­ç»ƒæ–¹æ³•ã€åè®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹è¯„ä¼°ã€‚æ¯ä¸ªæ¨¡å—ç›¸äº’å…³è”ï¼Œå…±åŒæå‡ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°æ•´åˆäº†å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹ç»“æ„å’Œè®­ç»ƒæ–¹æ³•ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†å…·èº«æ“ä½œçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ä¸åŒæ¨¡æ€çš„ä¿¡æ¯æƒé‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹é‡è¦ç‰¹å¾çš„æ•æ‰èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„VLAæ¨¡å‹åœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡çš„æ‰§è¡Œæ•ˆç‡ä¸Šæå‡äº†çº¦20%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨å¤šæ¨¡æ€ä¿¡æ¯èåˆæ–¹é¢çš„è¡¨ç°ä¹Ÿæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å…·èº«æ“ä½œä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–åˆ¶é€ ã€å®¶åº­åŠ©ç†ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆçš„ä»»åŠ¡æ‰§è¡Œï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½å®¶å±…ã€æ™ºèƒ½åˆ¶é€ ç­‰è¡Œä¸šçš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions.

