---
layout: default
title: CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models
---

# CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13446" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13446v1</a>
  <a href="https://arxiv.org/pdf/2508.13446.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13446v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13446v1', 'CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Catherine Glossop, William Chen, Arjun Bhorkar, Dhruv Shah, Sergey Levine

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåäº‹å®æ ‡ç­¾ä»¥æå‡è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `åäº‹å®å­¦ä¹ ` `æŒ‡ä»¤è·Ÿéš` `æœºå™¨äººæŠ€æœ¯` `æ•°æ®å¢å¼º` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹åœ¨æ‰§è¡Œç»†ç²’åº¦æŒ‡ä»¤æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºç¼ºä¹è¯­ä¹‰å¤šæ ·æ€§å’Œè¯­è¨€åŸºç¡€ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆåäº‹å®æ ‡ç­¾çš„æ–¹æ³•ï¼Œä»¥å¢å¼ºæœºå™¨äººæ•°æ®é›†çš„è¯­è¨€å¤šæ ·æ€§å’Œç»†ç²’åº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼Œåäº‹å®é‡æ ‡è®°åœ¨ä¸å¢åŠ æ•°æ®æ”¶é›†çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œä½¿æ¨¡å‹ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸ç«äº‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€šç”¨æœºå™¨äººåº”èƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œç”¨æˆ·æŒ‡ä»¤ï¼Œä½†ç°æœ‰çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨è·Ÿéšç»†ç²’åº¦å‘½ä»¤æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸»è¦åŸå› åœ¨äºç°æœ‰æœºå™¨äººæ•°æ®é›†ä¸­ç¼ºä¹è¯­ä¹‰å¤šæ ·æ€§å’Œè¯­è¨€åŸºç¡€ï¼Œå°¤å…¶æ˜¯åœ¨ç›¸ä¼¼è§‚å¯Ÿä¸‹ç¼ºä¹ç»†ç²’åº¦ä»»åŠ¡å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆåäº‹å®æ ‡ç­¾æ¥å¢å¼ºç°æœ‰æœºå™¨äººæ•°æ®é›†ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç”Ÿæˆåäº‹å®è¯­è¨€å’ŒåŠ¨ä½œï¼Œæé«˜äº†æœºå™¨äººæ•°æ®é›†çš„è¯­è¨€åŸºç¡€çš„å¤šæ ·æ€§å’Œç»†ç²’åº¦ï¼Œä»è€Œæ”¹å–„äº†VLAçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåäº‹å®é‡æ ‡è®°æ˜¾è‘—æé«˜äº†VLAç­–ç•¥çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¯¼èˆªä»»åŠ¡ä¸­çš„æˆåŠŸç‡æé«˜äº†27%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹åœ¨è·Ÿéšç»†ç²’åº¦ç”¨æˆ·æŒ‡ä»¤æ—¶çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç”±äºç¼ºä¹å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç”Ÿæˆåäº‹å®æ ‡ç­¾ï¼Œå¢å¼ºç°æœ‰æœºå™¨äººæ•°æ®é›†çš„è¯­è¨€åŸºç¡€å’Œä»»åŠ¡å¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸åŸå§‹æŒ‡ä»¤ç›¸å¯¹çš„è¯­è¨€å’ŒåŠ¨ä½œï¼Œä»¥ä¸°å¯Œè®­ç»ƒæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†å¢å¼ºæ¨¡å—å’Œæ¨¡å‹è®­ç»ƒæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆåäº‹å®æ ‡ç­¾ï¼Œç„¶åå°†è¿™äº›æ ‡ç­¾ä¸åŸå§‹æ•°æ®ç»“åˆï¼Œè®­ç»ƒVLAæ¨¡å‹ä»¥æé«˜å…¶æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåäº‹å®é‡æ ‡è®°æŠ€æœ¯ï¼Œå®ƒé€šè¿‡ç”Ÿæˆä¸åŸå§‹æŒ‡ä»¤ç›¸å¯¹çš„è¯­è¨€å’ŒåŠ¨ä½œï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¯­è¨€ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ•°æ®å¢å¼ºæŠ€æœ¯æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡åŸå§‹å’Œåäº‹å®æ ‡ç­¾çš„å½±å“ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ä¸Šç»“åˆäº†è§†è§‰å’Œè¯­è¨€ç‰¹å¾çš„èåˆæ¨¡å—ï¼Œä»¥æå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚å®éªŒä¸­è¿˜å¯¹åäº‹å®ç”Ÿæˆçš„è´¨é‡è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œä»¥ç¡®ä¿å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåäº‹å®é‡æ ‡è®°æŠ€æœ¯åœ¨ä¸å¢åŠ é¢å¤–æ•°æ®æ”¶é›†çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†VLAæ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œå¯¼èˆªä»»åŠ¡çš„æˆåŠŸç‡æé«˜äº†27%ã€‚è¿™ä¸€æå‡ä½¿å¾—æ¨¡å‹çš„æ€§èƒ½ä¸å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€å®¶åº­è‡ªåŠ¨åŒ–å’Œå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹å¤æ‚æŒ‡ä»¤çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ï¼Œå¯ä»¥å¤§å¹…æé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œçµæ´»æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½æœºå™¨äººåœ¨æ›´å¤šé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generalist robots should be able to understand and follow user instructions, but current vision-language-action (VLA) models struggle with following fine-grained commands despite providing a powerful architecture for mapping open-vocabulary natural language instructions to robot actions. One cause for this is a lack of semantic diversity and language grounding in existing robot datasets and, specifically, a lack of fine-grained task diversity for similar observations. To address this, we present a novel method to augment existing robot datasets by leveraging vision language models to create counterfactual labels. Our method improves the language-following capabilities of VLAs by increasing the diversity and granularity of language grounding for robot datasets by generating counterfactual language and actions. We evaluate the resulting model's ability to follow language instructions, ranging from simple object-centric commands to complex referential tasks, by conducting visual language navigation experiments in 3 different indoor and outdoor environments. Our experiments demonstrate that counterfactual relabeling, without any additional data collection, significantly improves instruction-following in VLA policies, making them competitive with state-of-the-art methods and increasing success rate by 27% on navigation tasks.

