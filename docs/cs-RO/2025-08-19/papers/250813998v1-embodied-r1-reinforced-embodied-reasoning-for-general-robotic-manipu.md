---
layout: default
title: Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation
---

# Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13998" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13998v1</a>
  <a href="https://arxiv.org/pdf/2508.13998.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13998v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13998v1', 'Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, Jianye Hao

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: Embodied-R1 technical report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEmbodied-R1ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ„ŸçŸ¥-è¡ŒåŠ¨å·®è·é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«äººå·¥æ™ºèƒ½` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æŒ‡å‘èƒ½åŠ›` `æœºå™¨äººæ“ä½œ` `æ•°æ®é›†æ„å»º` `é›¶-shotæ³›åŒ–` `å¤šä»»åŠ¡å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„å…·èº«äººå·¥æ™ºèƒ½æ–¹æ³•åœ¨é€šç”¨åŒ–æ–¹é¢å—åˆ°æ•°æ®ç¨€ç¼ºå’Œå…·èº«æ€§å¼‚è´¨æ€§çš„é™åˆ¶ï¼Œå¯¼è‡´æ„ŸçŸ¥ä¸è¡ŒåŠ¨ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæœ¬æ–‡æå‡ºâ€œæŒ‡å‘â€ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå¹¶è®¾è®¡äº†Embodied-R1æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¥æå‡å…·èº«æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šEmbodied-R1åœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨SIMPLEREnvä¸­å®ç°äº†56.2%çš„æˆåŠŸç‡ï¼Œè¾ƒå¼ºåŸºçº¿æå‡62%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å…·èº«äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œé€šç”¨åŒ–å—åˆ°â€œçœ‹ä¸åšä¹‹é—´çš„å·®è·â€çš„åˆ¶çº¦ï¼Œè¿™ä¸»è¦æºäºæ•°æ®ç¨€ç¼ºå’Œå…·èº«æ€§å¼‚è´¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºâ€œæŒ‡å‘â€ä½œä¸ºç»Ÿä¸€çš„ã€ä¸å…·èº«æ€§æ— å…³çš„ä¸­é—´è¡¨ç¤ºï¼Œå®šä¹‰äº†å››ç§æ ¸å¿ƒçš„å…·èº«æŒ‡å‘èƒ½åŠ›ï¼Œè¿æ¥é«˜å±‚æ¬¡çš„è§†è§‰-è¯­è¨€ç†è§£ä¸ä½å±‚æ¬¡çš„åŠ¨ä½œåŸè¯­ã€‚æˆ‘ä»¬å¼•å…¥äº†Embodied-R1ï¼Œä¸€ä¸ªä¸“ä¸ºå…·èº«æ¨ç†å’ŒæŒ‡å‘è®¾è®¡çš„3Bè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†Embodied-Points-200Kï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¯¾ç¨‹è¿›è¡Œè®­ç»ƒï¼ŒEmbodied-R1åœ¨11ä¸ªå…·èº«ç©ºé—´å’ŒæŒ‡å‘åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨å¤šç§è§†è§‰å¹²æ‰°ä¸‹çš„é«˜é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å…·èº«äººå·¥æ™ºèƒ½ä¸­çš„â€œçœ‹ä¸åšä¹‹é—´çš„å·®è·â€é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºå’Œå…·èº«æ€§å¼‚è´¨æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œé²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºâ€œæŒ‡å‘â€ä½œä¸ºç»Ÿä¸€çš„ä¸­é—´è¡¨ç¤ºï¼Œå®šä¹‰å››ç§æ ¸å¿ƒçš„å…·èº«æŒ‡å‘èƒ½åŠ›ï¼Œä»¥æ­¤è¿æ¥é«˜å±‚æ¬¡çš„è§†è§‰-è¯­è¨€ç†è§£ä¸ä½å±‚æ¬¡çš„åŠ¨ä½œåŸè¯­ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å…·èº«æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEmbodied-R1æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¯¾ç¨‹è¿›è¡Œè®­ç»ƒï¼Œç»“åˆå¤šä»»åŠ¡å¥–åŠ±è®¾è®¡ï¼Œåˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†Embodied-Points-200Kä½œä¸ºè®­ç»ƒåŸºç¡€ï¼Œæ”¯æŒå…³é”®çš„å…·èº«æŒ‡å‘èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥â€œæŒ‡å‘â€ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­å…·å¤‡æ›´å¼ºçš„é›¶-shotæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ä¸“é—¨è®¾è®¡çš„å¤šä»»åŠ¡å¥–åŠ±æœºåˆ¶ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œç¡®ä¿åœ¨ä¸åŒè§†è§‰å¹²æ‰°ä¸‹çš„é²æ£’æ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªè¯¦ç»†æŠ«éœ²ï¼Œæ ‡è®°ä¸ºæœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Embodied-R1åœ¨11ä¸ªå…·èº«ç©ºé—´å’ŒæŒ‡å‘åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨SIMPLEREnvä¸­å®ç°äº†56.2%çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨8ä¸ªçœŸå®ä¸–ç•ŒXArmä»»åŠ¡ä¸­è¾¾åˆ°87.5%çš„æˆåŠŸç‡ï¼Œè¾ƒå¼ºåŸºçº¿æå‡62%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§è§†è§‰å¹²æ‰°ä¸‹è¡¨ç°å‡ºé«˜é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–åˆ¶é€ ã€å®¶åº­æœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼ŒEmbodied-R1æœ‰æœ›åœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜çš„æ•ˆç‡å’Œçµæ´»æ€§ï¼Œæ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generalization in embodied AI is hindered by the "seeing-to-doing gap," which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer "pointing" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.

