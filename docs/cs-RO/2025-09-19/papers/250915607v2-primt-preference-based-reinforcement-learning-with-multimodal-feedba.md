---
layout: default
title: PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models
---

# PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15607" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15607v2</a>
  <a href="https://arxiv.org/pdf/2509.15607.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15607v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15607v2', 'PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruiqi Wang, Dezhong Zhao, Ziqin Yuan, Tianyu Shao, Guohua Chen, Dominic Kao, Sungeun Hong, Byung-Cheol Min

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19 (æ›´æ–°: 2025-12-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PRIMTï¼šåˆ©ç”¨å¤šæ¨¡æ€åé¦ˆå’Œè½¨è¿¹åˆæˆï¼Œæå‡åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `åŸºç¡€æ¨¡å‹` `è½¨è¿¹åˆæˆ` `ç¥ç»ç¬¦å·èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ä¾èµ–å¤§é‡äººå·¥åé¦ˆï¼Œä¸”åœ¨å¥–åŠ±å­¦ä¹ ä¸­å­˜åœ¨æŸ¥è¯¢æ¨¡ç³Šå’Œä¿¡ç”¨åˆ†é…éš¾é¢˜ã€‚
2. PRIMTæ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€åˆæˆåé¦ˆå’Œè½¨è¿¹åˆæˆï¼Œè§£å†³äººå·¥ä¾èµ–å’Œå­¦ä¹ éš¾é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPRIMTåœ¨å¤šä¸ªä»»åŠ¡ä¸­ä¼˜äºåŸºäºåŸºç¡€æ¨¡å‹å’Œè„šæœ¬çš„åŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶PRIMTï¼Œæ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•å¯¹å¤§é‡äººå·¥è¾“å…¥çš„ä¾èµ–ä»¥åŠå¥–åŠ±å­¦ä¹ è¿‡ç¨‹ä¸­å­˜åœ¨çš„æŸ¥è¯¢æ¨¡ç³Šæ€§å’Œä¿¡ç”¨åˆ†é…å›°éš¾ã€‚PRIMTåˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰è¿›è¡Œå¤šæ¨¡æ€åˆæˆåé¦ˆå’Œè½¨è¿¹åˆæˆã€‚ä¸åŒäºä»¥å¾€ä¾èµ–å•ä¸€æ¨¡æ€FMè¯„ä¼°çš„æ–¹æ³•ï¼ŒPRIMTé‡‡ç”¨åˆ†å±‚ç¥ç»ç¬¦å·èåˆç­–ç•¥ï¼Œæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ï¼Œä»è€Œæ›´å¯é ã€å…¨é¢åœ°è¯„ä¼°æœºå™¨äººè¡Œä¸ºã€‚PRIMTè¿˜ç»“åˆäº†å‰ç»è½¨è¿¹ç”Ÿæˆï¼Œé€šè¿‡ä½¿ç”¨è‡ªä¸¾æ ·æœ¬é¢„çƒ­è½¨è¿¹ç¼“å†²åŒºæ¥å‡å°‘æ—©æœŸæŸ¥è¯¢æ¨¡ç³Šæ€§ï¼Œä»¥åŠåè§è½¨è¿¹å¢å¼ºï¼Œé€šè¿‡å› æœè¾…åŠ©æŸå¤±å®ç°åäº‹å®æ¨ç†ï¼Œä»è€Œæ”¹å–„ä¿¡ç”¨åˆ†é…ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•çš„2ä¸ªè¿åŠ¨å’Œ6ä¸ªæ“ä½œä»»åŠ¡ä¸­ï¼ŒPRIMTçš„æ€§èƒ½ä¼˜äºåŸºäºFMå’Œè„šæœ¬çš„åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆPbRLï¼‰æ—¨åœ¨é€šè¿‡äººç±»åå¥½æ¥è®­ç»ƒæœºå™¨äººï¼Œé¿å…æ‰‹åŠ¨è®¾è®¡å¥–åŠ±å‡½æ•°ã€‚ç„¶è€Œï¼Œç°æœ‰PbRLæ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯éœ€è¦å¤§é‡çš„äººå·¥åé¦ˆï¼Œæˆæœ¬é«˜æ˜‚ï¼›äºŒæ˜¯å¥–åŠ±å­¦ä¹ è¿‡ç¨‹ä¸­å­˜åœ¨æŸ¥è¯¢æ¨¡ç³Šæ€§ï¼ˆæ—©æœŸè½¨è¿¹è´¨é‡å·®ï¼Œéš¾ä»¥åŒºåˆ†ä¼˜åŠ£ï¼‰å’Œä¿¡ç”¨åˆ†é…å›°éš¾ï¼ˆéš¾ä»¥ç¡®å®šè¡Œä¸ºåºåˆ—ä¸­å“ªäº›åŠ¨ä½œå¯¼è‡´äº†æœ€ç»ˆç»“æœï¼‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPRIMTçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼ˆFoundation Modelsï¼ŒFMsï¼‰æ¥ç”Ÿæˆåˆæˆåé¦ˆï¼Œä»è€Œå‡å°‘å¯¹äººå·¥åé¦ˆçš„ä¾èµ–ã€‚åŒæ—¶ï¼Œé€šè¿‡å‰ç»è½¨è¿¹ç”Ÿæˆå’Œåè§è½¨è¿¹å¢å¼ºæ¥è§£å†³æŸ¥è¯¢æ¨¡ç³Šæ€§å’Œä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„äº’è¡¥èƒ½åŠ›ï¼Œå¯¹æœºå™¨äººè¡Œä¸ºè¿›è¡Œæ›´å…¨é¢å’Œå¯é çš„è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPRIMTæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€åé¦ˆæ¨¡å—ï¼šåˆ©ç”¨LLMså’ŒVLMså¯¹æœºå™¨äººè½¨è¿¹è¿›è¡Œè¯„ä¼°ï¼Œç”Ÿæˆåå¥½æ ‡ç­¾ã€‚é‡‡ç”¨åˆ†å±‚ç¥ç»ç¬¦å·èåˆç­–ç•¥ï¼Œå°†LLMsçš„è¯­ä¹‰ç†è§£èƒ½åŠ›å’ŒVLMsçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ç›¸ç»“åˆã€‚2) å‰ç»è½¨è¿¹ç”Ÿæˆæ¨¡å—ï¼šåœ¨è®­ç»ƒåˆæœŸï¼Œä½¿ç”¨è‡ªä¸¾ï¼ˆbootstrappedï¼‰æ ·æœ¬é¢„çƒ­è½¨è¿¹ç¼“å†²åŒºï¼Œç”Ÿæˆé«˜è´¨é‡çš„åˆå§‹è½¨è¿¹ï¼Œå‡å°‘æŸ¥è¯¢æ¨¡ç³Šæ€§ã€‚3) åè§è½¨è¿¹å¢å¼ºæ¨¡å—ï¼šé€šè¿‡å› æœè¾…åŠ©æŸå¤±è¿›è¡Œåäº‹å®æ¨ç†ï¼Œå­¦ä¹ ä¸åŒåŠ¨ä½œåºåˆ—å¯¹ç»“æœçš„å½±å“ï¼Œä»è€Œæ”¹å–„ä¿¡ç”¨åˆ†é…ã€‚4) å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼šä½¿ç”¨ç”Ÿæˆçš„åå¥½æ ‡ç­¾å’Œå¢å¼ºçš„è½¨è¿¹æ•°æ®ï¼Œè®­ç»ƒå¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šPRIMTçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§åˆ†å±‚ç¥ç»ç¬¦å·èåˆç­–ç•¥ï¼Œå°†LLMså’ŒVLMsçš„ä¼˜åŠ¿ç»“åˆèµ·æ¥ï¼Œç”Ÿæˆæ›´å¯é å’Œå…¨é¢çš„åé¦ˆã€‚2) å¼•å…¥äº†å‰ç»è½¨è¿¹ç”Ÿæˆå’Œåè§è½¨è¿¹å¢å¼ºï¼Œæœ‰æ•ˆè§£å†³äº†PbRLä¸­çš„æŸ¥è¯¢æ¨¡ç³Šæ€§å’Œä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚3) å°†åŸºç¡€æ¨¡å‹åº”ç”¨äºPbRLï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹äººå·¥åé¦ˆçš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¤šæ¨¡æ€åé¦ˆæ¨¡å—ä¸­ï¼ŒLLMsç”¨äºè¯„ä¼°è½¨è¿¹çš„è¯­ä¹‰åˆç†æ€§ï¼ŒVLMsç”¨äºè¯„ä¼°è½¨è¿¹çš„è§†è§‰æ•ˆæœã€‚é‡‡ç”¨åŠ æƒèåˆçš„æ–¹å¼ï¼Œå°†LLMså’ŒVLMsçš„è¯„ä¼°ç»“æœç»“åˆèµ·æ¥ã€‚åœ¨å‰ç»è½¨è¿¹ç”Ÿæˆæ¨¡å—ä¸­ï¼Œä½¿ç”¨Behavior Cloningæ–¹æ³•ä»ä¸“å®¶è½¨è¿¹ä¸­å­¦ä¹ åˆå§‹ç­–ç•¥ã€‚åœ¨åè§è½¨è¿¹å¢å¼ºæ¨¡å—ä¸­ï¼Œä½¿ç”¨å› æœè¾…åŠ©æŸå¤±æ¥å­¦ä¹ çŠ¶æ€-åŠ¨ä½œå¯¹ç»“æœçš„å½±å“ã€‚å¼ºåŒ–å­¦ä¹ æ¨¡å—å¯ä»¥ä½¿ç”¨å¸¸è§çš„off-policyç®—æ³•ï¼Œå¦‚SACæˆ–TD3ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPRIMTåœ¨2ä¸ªè¿åŠ¨å’Œ6ä¸ªæ“ä½œä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¼˜äºåŸºäºFMå’Œè„šæœ¬çš„åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æ“ä½œä»»åŠ¡ä¸­ï¼ŒPRIMTçš„æˆåŠŸç‡å¹³å‡æå‡äº†15%ä»¥ä¸Šã€‚è¿™äº›ç»“æœéªŒè¯äº†PRIMTæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œè¡¨æ˜å…¶åœ¨è§£å†³PbRLä¸­çš„å…³é”®æŒ‘æˆ˜æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PRIMTæ¡†æ¶å¯åº”ç”¨äºå„ç§æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šæœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ç­‰ã€‚é€šè¿‡åˆ©ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œåˆæˆåé¦ˆï¼Œå¯ä»¥æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬å’Œæ—¶é—´ï¼ŒåŠ é€Ÿæœºå™¨äººæ™ºèƒ½åŒ–è¿›ç¨‹ã€‚è¯¥ç ”ç©¶å¯¹äºæ¨åŠ¨æœºå™¨äººè‡ªä¸»å­¦ä¹ å’Œäººæœºåä½œå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶æœ‰æœ›åœ¨æ™ºèƒ½åˆ¶é€ ã€æ™ºæ…§åŒ»ç–—ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Preference-based reinforcement learning (PbRL) has emerged as a promising paradigm for teaching robots complex behaviors without reward engineering. However, its effectiveness is often limited by two critical challenges: the reliance on extensive human input and the inherent difficulties in resolving query ambiguity and credit assignment during reward learning. In this paper, we introduce PRIMT, a PbRL framework designed to overcome these challenges by leveraging foundation models (FMs) for multimodal synthetic feedback and trajectory synthesis. Unlike prior approaches that rely on single-modality FM evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy, integrating the complementary strengths of large language models and vision-language models in evaluating robot behaviors for more reliable and comprehensive feedback. PRIMT also incorporates foresight trajectory generation, which reduces early-stage query ambiguity by warm-starting the trajectory buffer with bootstrapped samples, and hindsight trajectory augmentation, which enables counterfactual reasoning with a causal auxiliary loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6 manipulation tasks on various benchmarks, demonstrating superior performance over FM-based and scripted baselines.

