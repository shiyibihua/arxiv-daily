---
layout: default
title: End-to-end RL Improves Dexterous Grasping Policies
---

# End-to-end RL Improves Dexterous Grasping Policies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16434" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16434v1</a>
  <a href="https://arxiv.org/pdf/2509.16434.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16434v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16434v1', 'End-to-end RL Improves Dexterous Grasping Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ritvik Singh, Karl Van Wyk, Pieter Abbeel, Jitendra Malik, Nathan Ratliff, Ankur Handa

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: See our blog post: https://e2e4robotics.com/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§£è€¦æ¨¡æ‹Ÿå™¨ä¸å¼ºåŒ–å­¦ä¹ çš„æ¶æ„ï¼Œæå‡çµå·§æŠ“å–çš„ç«¯åˆ°ç«¯ç­–ç•¥å­¦ä¹ æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çµå·§æŠ“å–` `ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ` `è§†è§‰å¼ºåŒ–å­¦ä¹ ` `è§£è€¦æ¶æ„` `æ·±åº¦è’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºè§†è§‰çš„çµå·§æŠ“å–å¼ºåŒ–å­¦ä¹ æ–¹æ³•å—é™äºå†…å­˜æ•ˆç‡ï¼Œå¯¼è‡´æ‰¹é‡å¤§å°å°ï¼Œè®­ç»ƒå›°éš¾ã€‚
2. è®ºæ–‡æå‡ºè§£è€¦æ¨¡æ‹Ÿå™¨å’Œå¼ºåŒ–å­¦ä¹ çš„æ¶æ„ï¼Œå°†å®ƒä»¬åˆ†é…åˆ°ä¸åŒçš„GPUä¸Šï¼Œä»è€Œå¢åŠ æ‰¹é‡å¤§å°ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦ä¿¡æ¯è’¸é¦åˆ°RGBå›¾åƒæ—¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢ç´¢äº†æ‰©å±•åŸºäºå›¾åƒçš„ç«¯åˆ°ç«¯å­¦ä¹ æŠ€æœ¯ï¼Œç”¨äºæœºæ¢°è‡‚+çµå·§æ‰‹ç³»ç»Ÿçš„çµå·§æŠ“å–ã€‚ä¸åŸºäºçŠ¶æ€çš„å¼ºåŒ–å­¦ä¹ ä¸åŒï¼ŒåŸºäºè§†è§‰çš„å¼ºåŒ–å­¦ä¹ åœ¨å†…å­˜æ•ˆç‡æ–¹é¢è¾ƒä½ï¼Œå¯¼è‡´æ‰¹é‡å¤§å°ç›¸å¯¹è¾ƒå°ï¼Œè¿™ä¸åˆ©äºåƒPPOè¿™æ ·çš„ç®—æ³•ã€‚ç„¶è€Œï¼Œç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ä»ç„¶æ˜¯ä¸€ç§æœ‰å¸å¼•åŠ›çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒä¸åƒå¸¸ç”¨çš„å°†åŸºäºçŠ¶æ€çš„ç­–ç•¥æç‚¼åˆ°è§†è§‰ç½‘ç»œä¸­çš„æŠ€æœ¯é‚£æ ·ï¼Œå®ƒå¯ä»¥å®ç°æ¶Œç°å¼çš„ä¸»åŠ¨è§†è§‰è¡Œä¸ºã€‚æˆ‘ä»¬å‘ç°è®­ç»ƒè¿™äº›ç­–ç•¥çš„ä¸€ä¸ªå…³é”®ç“¶é¢ˆæ˜¯ï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡æ‹Ÿå™¨ä½¿ç”¨ä¼ ç»Ÿçš„æ•°æ®å¹¶è¡ŒæŠ€æœ¯æ‰©å±•åˆ°å¤šä¸ªGPUçš„æ–¹å¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†æ¨¡æ‹Ÿå™¨å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆåŒ…æ‹¬è®­ç»ƒå’Œç»éªŒç¼“å†²åŒºï¼‰è§£è€¦åˆ°å•ç‹¬çš„GPUä¸Šã€‚åœ¨ä¸€ä¸ªå…·æœ‰å››ä¸ªGPUçš„èŠ‚ç‚¹ä¸Šï¼Œæˆ‘ä»¬è®©æ¨¡æ‹Ÿå™¨åœ¨å…¶ä¸­ä¸‰ä¸ªGPUä¸Šè¿è¡Œï¼Œè€ŒPPOåœ¨ç¬¬å››ä¸ªGPUä¸Šè¿è¡Œã€‚æˆ‘ä»¬èƒ½å¤Ÿè¯æ˜ï¼Œåœ¨ç›¸åŒæ•°é‡çš„GPUä¸‹ï¼Œä¸ä¹‹å‰çš„æ ‡å‡†æ•°æ®å¹¶è¡ŒåŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç°æœ‰ç¯å¢ƒçš„æ•°é‡å¢åŠ ä¸€å€ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç«¯åˆ°ç«¯åœ°è®­ç»ƒåŸºäºè§†è§‰çš„ç¯å¢ƒï¼Œå¹¶ä½¿ç”¨æ·±åº¦ä¿¡æ¯ï¼Œè€Œè¿™äº›ç¯å¢ƒä¹‹å‰çš„æ€§èƒ½è¿œä½äºåŸºçº¿ã€‚æˆ‘ä»¬è®­ç»ƒå¹¶å°†æ·±åº¦å’ŒåŸºäºçŠ¶æ€çš„ç­–ç•¥æç‚¼åˆ°ç«‹ä½“RGBç½‘ç»œä¸­ï¼Œå¹¶è¡¨æ˜æ·±åº¦æç‚¼å¯ä»¥å¸¦æ¥æ›´å¥½çš„ç»“æœï¼Œæ— è®ºæ˜¯åœ¨æ¨¡æ‹Ÿä¸­è¿˜æ˜¯åœ¨ç°å®ä¸­ã€‚è¿™ç§æ”¹è¿›å¯èƒ½æ˜¯ç”±äºçŠ¶æ€å’Œè§†è§‰ç­–ç•¥ä¹‹é—´çš„å¯è§‚å¯Ÿæ€§å·®è·é€ æˆçš„ï¼Œè€Œå°†æ·±åº¦ç­–ç•¥æç‚¼åˆ°ç«‹ä½“RGBæ—¶ä¸å­˜åœ¨è¿™ç§å·®è·ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œè§£è€¦æ¨¡æ‹Ÿå¸¦æ¥çš„æ‰¹é‡å¤§å°çš„å¢åŠ ä¹Ÿæé«˜äº†ç°å®ä¸–ç•Œçš„æ€§èƒ½ã€‚åœ¨ç°å®ä¸–ç•Œä¸­éƒ¨ç½²æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ç«¯åˆ°ç«¯ç­–ç•¥æ”¹è¿›äº†å…ˆå‰æœ€å…ˆè¿›çš„åŸºäºè§†è§‰çš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè§†è§‰çš„çµå·§æŠ“å–å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œé¢ä¸´ç€å†…å­˜æ•ˆç‡ä½çš„æŒ‘æˆ˜ã€‚ç”±äºéœ€è¦å¤„ç†é«˜ç»´å›¾åƒæ•°æ®ï¼Œä¼ ç»Ÿçš„åŸºäºæ•°æ®å¹¶è¡Œçš„GPUæ‰©å±•æ–¹å¼æ— æ³•æœ‰æ•ˆåˆ©ç”¨è®¡ç®—èµ„æºï¼Œå¯¼è‡´æ‰¹é‡å¤§å°å—é™ï¼Œå½±å“äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOï¼‰çš„è®­ç»ƒæ•ˆæœã€‚æ­¤å¤–ï¼Œå°†åŸºäºçŠ¶æ€çš„ç­–ç•¥è’¸é¦åˆ°è§†è§‰ç½‘ç»œä¸­ï¼Œå¯èƒ½å­˜åœ¨çŠ¶æ€ä¸è§†è§‰ä¿¡æ¯ä¹‹é—´çš„å¯è§‚å¯Ÿæ€§å·®è·ï¼Œå½±å“æœ€ç»ˆæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¨¡æ‹Ÿå™¨å’Œå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹è§£è€¦ï¼Œå¹¶å°†å®ƒä»¬åˆ†é…åˆ°ä¸åŒçš„GPUä¸Šè¿è¡Œã€‚å…·ä½“æ¥è¯´ï¼Œå°†æ¨¡æ‹Ÿç¯å¢ƒè¿è¡Œåœ¨å¤šä¸ªGPUä¸Šï¼Œè€Œå°†å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆåŒ…æ‹¬ç»éªŒç¼“å†²åŒºï¼‰è¿è¡Œåœ¨å¦ä¸€ä¸ªGPUä¸Šã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æ˜¾è‘—å¢åŠ æ‰¹é‡å¤§å°ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†å°†æ·±åº¦ä¿¡æ¯è’¸é¦åˆ°RGBå›¾åƒä¸­çš„æ–¹æ³•ï¼Œä»¥å¼¥è¡¥çŠ¶æ€ä¸è§†è§‰ä¿¡æ¯ä¹‹é—´çš„å·®è·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šæ¨¡æ‹Ÿå™¨å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚æ¨¡æ‹Ÿå™¨è´Ÿè´£ç”Ÿæˆç¯å¢ƒå’Œäº¤äº’æ•°æ®ï¼Œè¿è¡Œåœ¨å¤šä¸ªGPUä¸Šã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆPPOï¼‰è´Ÿè´£æ ¹æ®ç¯å¢ƒæ•°æ®æ›´æ–°ç­–ç•¥ï¼Œè¿è¡Œåœ¨ç‹¬ç«‹çš„GPUä¸Šã€‚ä¸¤è€…é€šè¿‡æ•°æ®ä¼ è¾“è¿›è¡Œé€šä¿¡ã€‚è®ºæ–‡è¿˜ä½¿ç”¨äº†è’¸é¦æŠ€æœ¯ï¼Œå°†æ·±åº¦ä¿¡æ¯æˆ–çŠ¶æ€ä¿¡æ¯æç‚¼åˆ°RGBå›¾åƒä¸­ï¼Œä»¥æé«˜è§†è§‰ç­–ç•¥çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯è§£è€¦æ¨¡æ‹Ÿå™¨å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ¶æ„ã€‚è¿™ç§æ¶æ„èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨GPUèµ„æºï¼Œæ˜¾è‘—å¢åŠ æ‰¹é‡å¤§å°ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„æ•°æ®å¹¶è¡Œæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ‰©å±•åˆ°å¤šä¸ªGPUï¼Œå¹¶æ”¯æŒæ›´å¤§è§„æ¨¡çš„ç¯å¢ƒå’Œæ›´å¤æ‚çš„ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†PPOä½œä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶é’ˆå¯¹çµå·§æŠ“å–ä»»åŠ¡è®¾è®¡äº†å¥–åŠ±å‡½æ•°ã€‚åœ¨è’¸é¦è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†L2æŸå¤±å‡½æ•°æ¥æœ€å°åŒ–æ·±åº¦ä¿¡æ¯æˆ–çŠ¶æ€ä¿¡æ¯ä¸RGBå›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œä½¿ç”¨äº†å·ç§¯ç¥ç»ç½‘ç»œæ¥å¤„ç†å›¾åƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨å…¨è¿æ¥å±‚æ¥è¾“å‡ºåŠ¨ä½œã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ç­‰ï¼‰åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è§£è€¦æ¶æ„èƒ½å¤Ÿæ˜¾è‘—æé«˜è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚åœ¨ç›¸åŒæ•°é‡çš„GPUä¸‹ï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†ç¯å¢ƒæ•°é‡å¢åŠ ä¸€å€ã€‚æ­¤å¤–ï¼Œæ·±åº¦ä¿¡æ¯è’¸é¦åˆ°RGBå›¾åƒçš„æ–¹æ³•ä¹Ÿèƒ½å¤Ÿæé«˜æŠ“å–æˆåŠŸç‡ã€‚åœ¨çœŸå®ä¸–ç•Œå®éªŒä¸­ï¼Œè¯¥æ–¹æ³•ä¹Ÿä¼˜äºç°æœ‰çš„åŸºäºè§†è§‰çš„æŠ“å–æ–¹æ³•ï¼Œå–å¾—äº†state-of-the-artçš„ç»“æœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººçµå·§æ“ä½œã€è‡ªåŠ¨åŒ–è£…é…ã€åŒ»ç–—æ‰‹æœ¯ç­‰é¢†åŸŸã€‚é€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ï¼Œæœºå™¨äººå¯ä»¥ç›´æ¥ä»è§†è§‰è¾“å…¥å­¦ä¹ æŠ“å–ç­–ç•¥ï¼Œæ— éœ€äººå·¥è®¾è®¡å¤æ‚çš„æ§åˆ¶ç®—æ³•ã€‚è¯¥æ–¹æ³•å¯ä»¥æé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆå„ç§å¤æ‚ä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work explores techniques to scale up image-based end-to-end learning for dexterous grasping with an arm + hand system. Unlike state-based RL, vision-based RL is much more memory inefficient, resulting in relatively low batch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is still an attractive method as unlike the more commonly used techniques which distill state-based policies into vision networks, end-to-end RL can allow for emergent active vision behaviors. We identify a key bottleneck in training these policies is the way most existing simulators scale to multiple GPUs using traditional data parallelism techniques. We propose a new method where we disaggregate the simulator and RL (both training and experience buffers) onto separate GPUs. On a node with four GPUs, we have the simulator running on three of them, and PPO running on the fourth. We are able to show that with the same number of GPUs, we can double the number of existing environments compared to the previous baseline of standard data parallelism. This allows us to train vision-based environments, end-to-end with depth, which were previously performing far worse with the baseline. We train and distill both depth and state-based policies into stereo RGB networks and show that depth distillation leads to better results, both in simulation and reality. This improvement is likely due to the observability gap between state and vision policies which does not exist when distilling depth policies into stereo RGB. We further show that the increased batch size brought about by disaggregated simulation also improves real world performance. When deploying in the real world, we improve upon the previous state-of-the-art vision-based results using our end-to-end policies.

