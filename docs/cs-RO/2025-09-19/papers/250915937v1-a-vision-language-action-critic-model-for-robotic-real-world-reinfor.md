---
layout: default
title: A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning
---

# A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15937" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15937v1</a>
  <a href="https://arxiv.org/pdf/2509.15937.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15937v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15937v1', 'A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaopeng Zhai, Qi Zhang, Tianyi Zhang, Fuxian Huang, Haoran Zhang, Ming Zhou, Shengzhe Zhang, Litao Liu, Sixu Lin, Jiangmiao Pang

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: 26 pages,10 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè§†è§‰-è¯­è¨€-åŠ¨ä½œ-è¯„ä»·æ¨¡å‹çš„VLACï¼Œç”¨äºæå‡æœºå™¨äººçœŸå®ä¸–ç•Œå¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡å’ŒæˆåŠŸç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººå¼ºåŒ–å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `è¿‡ç¨‹å¥–åŠ±æ¨¡å‹` `äººæœºåä½œ` `çœŸå®ä¸–ç•Œæ“ä½œ` `InternVL` `å¥–åŠ±å‡½æ•°è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„ç¨€ç–å¥–åŠ±ï¼Œå¯¼è‡´æ¢ç´¢æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æ–°ä»»åŠ¡ã€‚
2. VLACæ¨¡å‹é€šè¿‡å­¦ä¹ è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¹‹é—´çš„å…³ç³»ï¼Œè‡ªåŠ¨ç”Ÿæˆå¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œæ— éœ€äººå·¥è®¾è®¡ï¼Œæ”¯æŒé›¶æ ·æœ¬è¿ç§»ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVLACåœ¨çœŸå®æœºå™¨äººä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†å¼ºåŒ–å­¦ä¹ çš„æˆåŠŸç‡å’Œæ ·æœ¬æ•ˆç‡ï¼Œå¹¶èƒ½é€šè¿‡äººæœºåä½œè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVLACçš„é€šç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œå®ƒåŸºäºInternVLï¼Œå¹¶ä½¿ç”¨å¤§è§„æ¨¡å¼‚æ„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººçœŸå®ä¸–ç•Œå¼ºåŒ–å­¦ä¹ ä¸­å› ç¨€ç–çš„æ‰‹å·¥å¥–åŠ±å’Œä½æ•ˆæ¢ç´¢è€Œå—é™çš„é—®é¢˜ã€‚ç»™å®šæˆå¯¹çš„è§‚å¯Ÿå’Œè¯­è¨€ç›®æ ‡ï¼ŒVLACè¾“å‡ºå¯†é›†çš„è¿›åº¦å¢é‡å’Œå®Œæˆä¿¡å·ï¼Œæ¶ˆé™¤äº†ç‰¹å®šäºä»»åŠ¡çš„å¥–åŠ±å·¥ç¨‹ï¼Œå¹¶æ”¯æŒä¸€æ¬¡æ€§ä¸Šä¸‹æ–‡è¿ç§»åˆ°æœªè§è¿‡çš„ä»»åŠ¡å’Œç¯å¢ƒã€‚VLACé€šè¿‡è§†è§‰-è¯­è¨€æ•°æ®é›†æ¥å¢å¼ºæ„ŸçŸ¥ã€å¯¹è¯å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥åŠæœºå™¨äººå’Œäººç±»è½¨è¿¹æ•°æ®æ¥æ”¯æŒåŠ¨ä½œç”Ÿæˆå’Œè¿›åº¦ä¼°è®¡ï¼Œå¹¶é€šè¿‡æ„å»ºå¤§é‡çš„è´Ÿæ ·æœ¬å’Œè¯­ä¹‰ä¸åŒ¹é…çš„æ ·æœ¬æ¥æ‹’ç»ä¸ç›¸å…³çš„æç¤ºï¼Œå¹¶æ£€æµ‹å›å½’æˆ–åœæ»ã€‚é€šè¿‡æç¤ºæ§åˆ¶ï¼Œå•ä¸ªVLACæ¨¡å‹äº¤æ›¿ç”Ÿæˆå¥–åŠ±å’ŒåŠ¨ä½œtokenï¼Œç»Ÿä¸€äº†è¯„ä»·å™¨å’Œç­–ç•¥ã€‚åœ¨å¼‚æ­¥çœŸå®ä¸–ç•ŒRLå¾ªç¯ä¸­éƒ¨ç½²æ—¶ï¼Œæˆ‘ä»¬åˆ†å±‚ä½¿ç”¨äººæœºåä½œåè®®ï¼ˆç¦»çº¿æ¼”ç¤ºå›æ”¾ã€å›æŠ¥å’Œæ¢ç´¢ã€äººå·¥å¼•å¯¼æ¢ç´¢ï¼‰ï¼Œä»è€ŒåŠ é€Ÿæ¢ç´¢å¹¶ç¨³å®šæ—©æœŸå­¦ä¹ ã€‚åœ¨å››ä¸ªä¸åŒçš„çœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­ï¼ŒVLACå°†æˆåŠŸç‡ä»å¤§çº¦30ï¼…æé«˜åˆ°å¤§çº¦90ï¼…ï¼Œåœ¨200ä¸ªçœŸå®ä¸–ç•Œäº¤äº’episodeå†…ï¼›ç»“åˆäººæœºåä½œå¹²é¢„ï¼Œæ ·æœ¬æ•ˆç‡è¿›ä¸€æ­¥æé«˜äº†50ï¼…ï¼Œæœ€ç»ˆæˆåŠŸç‡é«˜è¾¾100ï¼…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç—›ç‚¹åœ¨äºå¥–åŠ±å‡½æ•°çš„è®¾è®¡ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„ç¨€ç–å¥–åŠ±ï¼Œè¿™ä¸ä»…è€—æ—¶è€—åŠ›ï¼Œè€Œä¸”éš¾ä»¥æ³›åŒ–åˆ°ä¸åŒçš„ä»»åŠ¡å’Œç¯å¢ƒä¸­ã€‚æ­¤å¤–ï¼Œæ¢ç´¢æ•ˆç‡ä½ä¸‹ä¹Ÿæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œæœºå™¨äººéš¾ä»¥æœ‰æ•ˆåœ°æ¢ç´¢çŠ¶æ€ç©ºé—´ï¼Œæ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å­¦ä¹ ä¸€ä¸ªé€šç”¨çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è§†è§‰è¾“å…¥å’Œè¯­è¨€ç›®æ ‡ï¼Œè‡ªåŠ¨ç”Ÿæˆå¯†é›†çš„å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡å­¦ä¹ äººç±»å’Œæœºå™¨äººçš„è¡Œä¸ºè½¨è¿¹ï¼Œæ¨¡å‹èƒ½å¤Ÿç†è§£ä»»åŠ¡çš„è¿›å±•ï¼Œå¹¶ä¸ºæœºå™¨äººæä¾›æ›´æœ‰æ•ˆçš„åé¦ˆã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ‰‹å·¥è®¾è®¡å¥–åŠ±å‡½æ•°çš„éœ€è¦ï¼Œå¹¶æé«˜äº†æ¢ç´¢æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLACæ¨¡å‹åŸºäºInternVLæ¶æ„ï¼Œå¹¶ä½¿ç”¨å¤§è§„æ¨¡å¼‚æ„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰ç¼–ç å™¨ï¼Œç”¨äºæå–è§†è§‰ç‰¹å¾ï¼›2) è¯­è¨€ç¼–ç å™¨ï¼Œç”¨äºæå–è¯­è¨€ç‰¹å¾ï¼›3) åŠ¨ä½œç”Ÿæˆå™¨ï¼Œç”¨äºç”Ÿæˆæœºå™¨äººçš„åŠ¨ä½œï¼›4) è¿›åº¦ä¼°è®¡å™¨ï¼Œç”¨äºä¼°è®¡ä»»åŠ¡çš„è¿›å±•ã€‚æ¨¡å‹é€šè¿‡æç¤ºæ§åˆ¶ï¼Œäº¤æ›¿ç”Ÿæˆå¥–åŠ±å’ŒåŠ¨ä½œtokenï¼Œç»Ÿä¸€äº†è¯„ä»·å™¨å’Œç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†äººæœºåä½œåè®®ï¼Œä»¥åŠ é€Ÿæ¢ç´¢å’Œç¨³å®šæ—©æœŸå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šVLACçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é€šç”¨çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œæ— éœ€äººå·¥è®¾è®¡ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜èƒ½å¤Ÿé€šè¿‡å­¦ä¹ äººç±»å’Œæœºå™¨äººçš„è¡Œä¸ºè½¨è¿¹ï¼Œç†è§£ä»»åŠ¡çš„è¿›å±•ï¼Œå¹¶ä¸ºæœºå™¨äººæä¾›æ›´æœ‰æ•ˆçš„åé¦ˆã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ï¼Œè€Œä¸”å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šVLACæ¨¡å‹ä½¿ç”¨äº†å¤§é‡çš„è´Ÿæ ·æœ¬å’Œè¯­ä¹‰ä¸åŒ¹é…çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä»¥å¢å¼ºæ¨¡å‹æ‹’ç»ä¸ç›¸å…³æç¤ºä»¥åŠæ£€æµ‹å›å½’æˆ–åœæ»çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜ä½¿ç”¨äº†æç¤ºæ§åˆ¶æŠ€æœ¯ï¼Œé€šè¿‡äº¤æ›¿ç”Ÿæˆå¥–åŠ±å’ŒåŠ¨ä½œtokenï¼Œç»Ÿä¸€äº†è¯„ä»·å™¨å’Œç­–ç•¥ã€‚äººæœºåä½œåè®®åŒ…æ‹¬ç¦»çº¿æ¼”ç¤ºå›æ”¾ã€å›æŠ¥å’Œæ¢ç´¢ã€äººå·¥å¼•å¯¼æ¢ç´¢ç­‰ç¯èŠ‚ï¼Œä»¥åŠ é€Ÿæ¢ç´¢å’Œç¨³å®šæ—©æœŸå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVLACæ¨¡å‹åœ¨å››ä¸ªä¸åŒçš„çœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­ï¼Œå°†æˆåŠŸç‡ä»å¤§çº¦30ï¼…æé«˜åˆ°å¤§çº¦90ï¼…ï¼Œåœ¨200ä¸ªçœŸå®ä¸–ç•Œäº¤äº’episodeå†…ã€‚ç»“åˆäººæœºåä½œå¹²é¢„ï¼Œæ ·æœ¬æ•ˆç‡è¿›ä¸€æ­¥æé«˜äº†50ï¼…ï¼Œæœ€ç»ˆæˆåŠŸç‡é«˜è¾¾100ï¼…ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVLACæ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººå¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡å’ŒæˆåŠŸç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººå’ŒåŒ»ç–—è¾…åŠ©æœºå™¨äººã€‚é€šè¿‡è‡ªåŠ¨ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œå¯ä»¥é™ä½æœºå™¨äººéƒ¨ç½²çš„æˆæœ¬å’Œéš¾åº¦ï¼Œå¹¶æé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨æœºå™¨äººæŠ€æœ¯åœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic real-world reinforcement learning (RL) with vision-language-action (VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient exploration. We introduce VLAC, a general process reward model built upon InternVL and trained on large scale heterogeneous datasets. Given pairwise observations and a language goal, it outputs dense progress delta and done signal, eliminating task-specific reward engineering, and supports one-shot in-context transfer to unseen tasks and environments. VLAC is trained on vision-language datasets to strengthen perception, dialogic and reasoning capabilities, together with robot and human trajectories data that ground action generation and progress estimation, and additionally strengthened to reject irrelevant prompts as well as detect regression or stagnation by constructing large numbers of negative and semantically mismatched samples. With prompt control, a single VLAC model alternately generating reward and action tokens, unifying critic and policy. Deployed inside an asynchronous real-world RL loop, we layer a graded human-in-the-loop protocol (offline demonstration replay, return and explore, human guided explore) that accelerates exploration and stabilizes early learning. Across four distinct real-world manipulation tasks, VLAC lifts success rates from about 30\% to about 90\% within 200 real-world interaction episodes; incorporating human-in-the-loop interventions yields a further 50% improvement in sample efficiency and achieves up to 100% final success.

