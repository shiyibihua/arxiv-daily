---
layout: default
title: Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder
---

# Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15880" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15880v1</a>
  <a href="https://arxiv.org/pdf/2509.15880.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15880v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15880v1', 'Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: An Dinh Vuong, Minh Nhat Vu, Ian Reid

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: 9 figures, 7 tables. Project page: https://evggt.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆå‡ ä½•æ„ŸçŸ¥è§†è§‰ç¼–ç å™¨eVGGTï¼Œæå‡æœºå™¨äººæ“ä½œæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `æ¨¡ä»¿å­¦ä¹ ` `å‡ ä½•æ„ŸçŸ¥` `è§†è§‰ç¼–ç å™¨` `çŸ¥è¯†è’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸRGBè§†è§‰ç¼–ç å™¨ç¼ºä¹3Dæ¨ç†èƒ½åŠ›ï¼Œé™åˆ¶äº†æœºå™¨äººæ“ä½œçš„æ€§èƒ½ã€‚
2. æå‡ºé«˜æ•ˆå‡ ä½•æ„ŸçŸ¥ç¼–ç å™¨eVGGTï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦ä¿ç•™VGGTçš„3Dæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œåœ¨æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ä¸­é›†æˆeVGGTï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººæ“ä½œçš„æˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„åŸºäºRGBå›¾åƒçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•é€šå¸¸é‡‡ç”¨ResNetæˆ–ViTç­‰ä¼ ç»Ÿè§†è§‰ç¼–ç å™¨ï¼Œè¿™äº›ç¼–ç å™¨ç¼ºä¹æ˜¾å¼çš„3Dæ¨ç†èƒ½åŠ›ã€‚æœ€è¿‘çš„å‡ ä½•æ„ŸçŸ¥è§†è§‰æ¨¡å‹ï¼Œå¦‚VGGTï¼Œæä¾›äº†å¼ºå¤§çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå¹¶æœ‰æœ›è§£å†³è¿™ä¸€å±€é™æ€§ã€‚æœ¬æ–‡ç ”ç©¶äº†å°†å‡ ä½•æ„ŸçŸ¥è§†è§‰è¡¨å¾é›†æˆåˆ°æœºå™¨äººæ“ä½œä¸­ã€‚ç»“æœè¡¨æ˜ï¼Œå°†å‡ ä½•æ„ŸçŸ¥è§†è§‰ç¼–ç å™¨é›†æˆåˆ°æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼ˆåŒ…æ‹¬ACTå’ŒDPï¼‰ä¸­ï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„å•æ‰‹å’ŒåŒæ‰‹æ“ä½œä»»åŠ¡ä¸­ï¼ŒæˆåŠŸç‡æ¯”æ ‡å‡†è§†è§‰ç¼–ç å™¨æé«˜äº†6.5%ã€‚å°½ç®¡æœ‰è¿™äº›å¥½å¤„ï¼Œä½†å¤§å¤šæ•°å‡ ä½•æ„ŸçŸ¥æ¨¡å‹éœ€è¦é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å®é™…æœºå™¨äººç³»ç»Ÿä¸­çš„éƒ¨ç½²ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†eVGGTï¼Œä¸€ç§ä»VGGTä¸­æç‚¼å‡ºçš„é«˜æ•ˆå‡ ä½•æ„ŸçŸ¥ç¼–ç å™¨ã€‚eVGGTæ¯”VGGTå¿«è¿‘9å€ï¼Œä½“ç§¯å°5å€ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„3Dæ¨ç†èƒ½åŠ›ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†è¢«å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å‡ ä½•æ„ŸçŸ¥æœºå™¨äººé¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºRGBå›¾åƒçš„æœºå™¨äººæ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œä¾èµ–äºResNetã€ViTç­‰ä¼ ç»Ÿè§†è§‰ç¼–ç å™¨ï¼Œè¿™äº›ç¼–ç å™¨æ— æ³•æœ‰æ•ˆè¿›è¡Œ3Dç©ºé—´æ¨ç†ï¼Œå¯¼è‡´æœºå™¨äººéš¾ä»¥ç†è§£åœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼Œä»è€Œå½±å“æ“ä½œæ€§èƒ½ã€‚VGGTç­‰å‡ ä½•æ„ŸçŸ¥æ¨¡å‹è™½ç„¶èƒ½æä¾›æ›´å¼ºçš„ç©ºé—´ç†è§£ï¼Œä½†è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œéš¾ä»¥åœ¨å®é™…æœºå™¨äººç³»ç»Ÿä¸­éƒ¨ç½²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œä»è®¡ç®—é‡å¤§çš„VGGTæ¨¡å‹ä¸­æå–å…¶å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶å°†å…¶è¿ç§»åˆ°ä¸€ä¸ªæ›´å°ã€æ›´é«˜æ•ˆçš„ç¼–ç å™¨eVGGTä¸­ã€‚è¿™æ ·æ—¢èƒ½ä¿æŒè¾ƒå¼ºçš„3Dæ¨ç†èƒ½åŠ›ï¼Œåˆèƒ½æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œä½¿å…¶æ›´é€‚åˆåœ¨èµ„æºå—é™çš„æœºå™¨äººå¹³å°ä¸Šä½¿ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œè®­ç»ƒä¸€ä¸ªé«˜æ€§èƒ½çš„VGGTæ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼›ç„¶åï¼Œä½¿ç”¨VGGTçš„è¾“å‡ºä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®­ç»ƒä¸€ä¸ªæ›´å°çš„eVGGTæ¨¡å‹ä½œä¸ºå­¦ç”Ÿæ¨¡å‹ã€‚æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ACTå’ŒDPï¼‰ä½¿ç”¨eVGGTæå–è§†è§‰ç‰¹å¾ï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°ç­–ç•¥ç½‘ç»œä¸­ï¼Œæœ€ç»ˆæ§åˆ¶æœºå™¨äººæ‰§è¡Œæ“ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†é«˜æ•ˆçš„å‡ ä½•æ„ŸçŸ¥ç¼–ç å™¨eVGGTï¼Œå®ƒé€šè¿‡çŸ¥è¯†è’¸é¦çš„æ–¹å¼ï¼Œåœ¨ä¿æŒè¾ƒå¼º3Dæ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¿™ä½¿å¾—å‡ ä½•æ„ŸçŸ¥è§†è§‰è¡¨å¾èƒ½å¤Ÿæ›´å®¹æ˜“åœ°åº”ç”¨äºå®é™…çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šeVGGTçš„ç½‘ç»œç»“æ„è®¾è®¡ç›®æ ‡æ˜¯å°½å¯èƒ½ç®€åŒ–æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™VGGTçš„å…³é”®ç‰¹å¾æå–èƒ½åŠ›ã€‚è’¸é¦è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†å¤šç§æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬ç‰¹å¾åŒ¹é…æŸå¤±å’Œè¡Œä¸ºæ¨¡ä»¿æŸå¤±ï¼Œä»¥ç¡®ä¿eVGGTèƒ½å¤Ÿå­¦ä¹ åˆ°VGGTçš„å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›å’Œè¡Œä¸ºç­–ç•¥ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚å°†åœ¨è®ºæ–‡ä¸­è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å•æ‰‹å’ŒåŒæ‰‹æ“ä½œä»»åŠ¡ä¸­ï¼Œå°†eVGGTé›†æˆåˆ°æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼ˆACTå’ŒDPï¼‰ä¸­ï¼Œç›¸æ¯”äºä½¿ç”¨æ ‡å‡†è§†è§‰ç¼–ç å™¨ï¼ŒæˆåŠŸç‡æé«˜äº†é«˜è¾¾6.5%ã€‚åŒæ—¶ï¼ŒeVGGTæ¯”VGGTå¿«è¿‘9å€ï¼Œä½“ç§¯å°5å€ï¼Œè¯æ˜äº†å…¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™äº›ç»“æœéªŒè¯äº†eVGGTåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹ç¯å¢ƒçš„3Dç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å…¶æ“ä½œç²¾åº¦ã€é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆã€æ›´æ™ºèƒ½çš„æœºå™¨äººç³»ç»Ÿã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–æœºå™¨äººæ„ŸçŸ¥ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚ä¸‰ç»´é‡å»ºã€ç‰©ä½“è¯†åˆ«å’Œåœºæ™¯ç†è§£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing RGB-based imitation learning approaches typically employ traditional vision encoders such as ResNet or ViT, which lack explicit 3D reasoning capabilities. Recent geometry-grounded vision models, such as VGGT~\cite{wang2025vggt}, provide robust spatial understanding and are promising candidates to address this limitation. This work investigates the integration of geometry-aware visual representations into robotic manipulation. Our results suggest that incorporating the geometry-aware vision encoder into imitation learning frameworks, including ACT and DP, yields up to 6.5% improvement over standard vision encoders in success rate across single- and bi-manual manipulation tasks in both simulation and real-world settings. Despite these benefits, most geometry-grounded models require high computational cost, limiting their deployment in practical robotic systems. To address this challenge, we propose eVGGT, an efficient geometry-aware encoder distilled from VGGT. eVGGT is nearly 9 times faster and 5 times smaller than VGGT, while preserving strong 3D reasoning capabilities. Code and pretrained models will be released to facilitate further research in geometry-aware robotics.

