---
layout: default
title: Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning
---

# Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16136" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16136v1</a>
  <a href="https://arxiv.org/pdf/2509.16136.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16136v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16136v1', 'Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changwei Yao, Xinzi Liu, Chen Li, Marios Savvides

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRE-GoTæ¡†æ¶ï¼Œåˆ©ç”¨å›¾æ¨ç†å’Œè§†è§‰åé¦ˆå®ç°å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°çš„è‡ªåŠ¨è¿›åŒ–ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å‡½æ•°è®¾è®¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `å›¾æ¨ç†` `è‡ªä¸»è¿›åŒ–` `æœºå™¨äººæ“ä½œ` `ä»»åŠ¡åˆ†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„å¥–åŠ±å‡½æ•°è®¾è®¡æ–¹æ³•å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œä¾èµ–äººå·¥åé¦ˆï¼Œä¸”éš¾ä»¥å¤„ç†å¤æ‚å¤šæ­¥ä»»åŠ¡ã€‚
2. RE-GoTæ¡†æ¶åˆ©ç”¨å›¾ç»“æ„åˆ†è§£ä»»åŠ¡ï¼Œç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªåŠ¨è¯„ä¼°å’Œè¿­ä»£ä¼˜åŒ–å¥–åŠ±å‡½æ•°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRE-GoTåœ¨RoboGenå’ŒManiSkill2ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šæå‡æ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„äººå·¥ä¸“ä¸šçŸ¥è¯†å’Œè¿­ä»£æ”¹è¿›ã€‚æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨å¥–åŠ±è®¾è®¡ï¼Œä½†è¿™äº›æ–¹æ³•å—åˆ°å¹»è§‰ã€ä¾èµ–äººå·¥åé¦ˆä»¥åŠå¤„ç†å¤æ‚ã€å¤šæ­¥éª¤ä»»åŠ¡çš„æŒ‘æˆ˜çš„é™åˆ¶ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŒå±‚æ¡†æ¶â€”â€”åŸºäºå›¾æ¨ç†çš„å¥–åŠ±è¿›åŒ–ï¼ˆRE-GoTï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–çš„åŸºäºå›¾çš„æ¨ç†å¢å¼ºäº†LLMï¼Œå¹¶é›†æˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»¥è¿›è¡Œè‡ªåŠ¨rolloutè¯„ä¼°ã€‚RE-GoTé¦–å…ˆå°†ä»»åŠ¡åˆ†è§£ä¸ºæ–‡æœ¬å±æ€§å›¾ï¼Œä»è€Œå®ç°å…¨é¢çš„åˆ†æå’Œå¥–åŠ±å‡½æ•°ç”Ÿæˆï¼Œç„¶åä½¿ç”¨æ¥è‡ªVLMçš„è§†è§‰åé¦ˆè¿­ä»£åœ°æ”¹è¿›å¥–åŠ±ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚åœ¨10ä¸ªRoboGenå’Œ4ä¸ªManiSkill2ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRE-GoTå§‹ç»ˆä¼˜äºç°æœ‰çš„åŸºäºLLMçš„åŸºçº¿ã€‚åœ¨RoboGenä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å¹³å‡ä»»åŠ¡æˆåŠŸç‡æé«˜äº†32.25%ï¼Œåœ¨å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚åœ¨ManiSkill2ä¸Šï¼ŒRE-GoTåœ¨å››ä¸ªä¸åŒçš„æ“ä½œä»»åŠ¡ä¸­å®ç°äº†93.73%çš„å¹³å‡æˆåŠŸç‡ï¼Œæ˜¾è‘—è¶…è¿‡äº†å…ˆå‰çš„åŸºäºLLMçš„æ–¹æ³•ï¼Œç”šè‡³è¶…è¿‡äº†ä¸“å®¶è®¾è®¡çš„å¥–åŠ±ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°†LLMå’ŒVLMä¸å›¾æ¨ç†ç›¸ç»“åˆï¼Œä¸ºRLä¸­çš„è‡ªä¸»å¥–åŠ±è¿›åŒ–æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡éš¾é¢˜ï¼Œç°æœ‰åŸºäºLLMçš„æ–¹æ³•å­˜åœ¨å¹»è§‰ã€ä¾èµ–äººå·¥åé¦ˆã€éš¾ä»¥å¤„ç†å¤æ‚ä»»åŠ¡ç­‰ç—›ç‚¹ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…æœºå™¨äººä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯å°†ä»»åŠ¡åˆ†è§£ä¸ºå›¾ç»“æ„ï¼Œåˆ©ç”¨å›¾æ¨ç†èƒ½åŠ›å¢å¼ºLLMå¯¹ä»»åŠ¡çš„ç†è§£ï¼Œå¹¶å¼•å…¥VLMè¿›è¡Œè§†è§‰åé¦ˆï¼Œå®ç°å¥–åŠ±å‡½æ•°çš„è‡ªåŠ¨è¯„ä¼°å’Œè¿­ä»£ä¼˜åŒ–ï¼Œä»è€Œé¿å…äººå·¥å¹²é¢„å’Œå¹»è§‰é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRE-GoTæ˜¯ä¸€ä¸ªåŒå±‚æ¡†æ¶ã€‚ç¬¬ä¸€å±‚ï¼Œåˆ©ç”¨LLMå°†ä»»åŠ¡åˆ†è§£ä¸ºæ–‡æœ¬å±æ€§å›¾ï¼ˆGraph-of-Thoughtsï¼‰ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä»»åŠ¡çš„ä¸€ä¸ªæ­¥éª¤æˆ–çŠ¶æ€ï¼Œè¾¹è¡¨ç¤ºæ­¥éª¤ä¹‹é—´çš„å…³ç³»ã€‚ç¬¬äºŒå±‚ï¼Œåˆ©ç”¨VLMå¯¹rolloutç»“æœè¿›è¡Œè§†è§‰è¯„ä¼°ï¼Œå¹¶æ ¹æ®è¯„ä¼°ç»“æœè¿­ä»£ä¼˜åŒ–å¥–åŠ±å‡½æ•°ã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€äººå·¥å¹²é¢„ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºç»“åˆäº†å›¾æ¨ç†å’Œè§†è§‰åé¦ˆï¼Œåˆ©ç”¨å›¾ç»“æ„å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¤æ‚ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨VLMçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›è¿›è¡Œè‡ªåŠ¨è¯„ä¼°ï¼Œä»è€Œå®ç°å¥–åŠ±å‡½æ•°çš„è‡ªä¸»è¿›åŒ–ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRE-GoTæ— éœ€äººå·¥åé¦ˆï¼Œä¸”èƒ½æ›´å¥½åœ°å¤„ç†å¤æ‚å¤šæ­¥ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šRE-GoTä½¿ç”¨LLMï¼ˆä¾‹å¦‚GPT-4ï¼‰è¿›è¡Œä»»åŠ¡åˆ†è§£å’Œå¥–åŠ±å‡½æ•°ç”Ÿæˆã€‚VLMç”¨äºè¯„ä¼°rolloutç»“æœï¼Œå¹¶ç”Ÿæˆåé¦ˆä¿¡å·ã€‚å¥–åŠ±å‡½æ•°çš„ä¼˜åŒ–é‡‡ç”¨è¿­ä»£çš„æ–¹å¼ï¼Œæ ¹æ®VLMçš„åé¦ˆä¿¡å·è°ƒæ•´å¥–åŠ±å‡½æ•°çš„å‚æ•°ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæ­¤å¤„æœªçŸ¥å…·ä½“ç»†èŠ‚ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RE-GoTåœ¨RoboGenä»»åŠ¡ä¸Šå¹³å‡æˆåŠŸç‡æå‡32.25%ï¼Œå°¤å…¶åœ¨å¤æ‚å¤šæ­¥ä»»åŠ¡ä¸Šæå‡æ˜¾è‘—ã€‚åœ¨ManiSkill2ä»»åŠ¡ä¸Šï¼ŒRE-GoTå–å¾—äº†93.73%çš„å¹³å‡æˆåŠŸç‡ï¼Œè¶…è¿‡äº†ç°æœ‰åŸºäºLLMçš„æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†ä¸“å®¶è®¾è®¡çš„å¥–åŠ±å‡½æ•°ã€‚è¿™äº›ç»“æœè¡¨æ˜RE-GoTåœ¨è‡ªä¸»å¥–åŠ±è¿›åŒ–æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RE-GoTæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚é€šè¿‡è‡ªåŠ¨è®¾è®¡å’Œä¼˜åŒ–å¥–åŠ±å‡½æ•°ï¼Œå¯ä»¥é™ä½å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨é—¨æ§›ï¼ŒåŠ é€Ÿæ™ºèƒ½ä½“çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶æé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨å¼ºåŒ–å­¦ä¹ åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Designing effective reward functions remains a major challenge in reinforcement learning (RL), often requiring considerable human expertise and iterative refinement. Recent advances leverage Large Language Models (LLMs) for automated reward design, but these approaches are limited by hallucinations, reliance on human feedback, and challenges with handling complex, multi-step tasks. In this work, we introduce Reward Evolution with Graph-of-Thoughts (RE-GoT), a novel bi-level framework that enhances LLMs with structured graph-based reasoning and integrates Visual Language Models (VLMs) for automated rollout evaluation. RE-GoT first decomposes tasks into text-attributed graphs, enabling comprehensive analysis and reward function generation, and then iteratively refines rewards using visual feedback from VLMs without human intervention. Extensive experiments on 10 RoboGen and 4 ManiSkill2 tasks demonstrate that RE-GoT consistently outperforms existing LLM-based baselines. On RoboGen, our method improves average task success rates by 32.25%, with notable gains on complex multi-step tasks. On ManiSkill2, RE-GoT achieves an average success rate of 93.73% across four diverse manipulation tasks, significantly surpassing prior LLM-based approaches and even exceeding expert-designed rewards. Our results indicate that combining LLMs and VLMs with graph-of-thoughts reasoning provides a scalable and effective solution for autonomous reward evolution in RL.

