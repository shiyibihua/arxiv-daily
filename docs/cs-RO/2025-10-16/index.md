---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-10-16
---

# cs.ROï¼ˆ2025-10-16ï¼‰

ğŸ“Š å…± **30** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (22 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (22 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251014952v2-from-language-to-locomotion-retargeting-free-humanoid-control-via-mo.html">From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance</a></td>
  <td>RoboGhostï¼šæå‡ºä¸€ç§æ— é‡å®šå‘çš„è¯­è¨€å¼•å¯¼äººå½¢æœºå™¨äººè¿åŠ¨æ§åˆ¶æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14952v2" onclick="toggleFavorite(this, '2510.14952v2', 'From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251014902v1-vla2-empowering-vision-language-action-models-with-an-agentic-framew.html">VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation</a></td>
  <td>VLA^2ï¼šåˆ©ç”¨Agentæ¡†æ¶å¢å¼ºVLAæ¨¡å‹å¤„ç†æœªè§æ¦‚å¿µæ“ä½œçš„èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14902v1" onclick="toggleFavorite(this, '2510.14902v1', 'VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251014771v1-open-teledex-a-hardware-agnostic-teleoperation-system-for-imitation-.html">Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation</a></td>
  <td>Open TeleDexï¼šä¸€ä¸ªç¡¬ä»¶æ— å…³çš„çµå·§æ“ä½œæ¨¡ä»¿å­¦ä¹ é¥æ“ä½œç³»ç»Ÿ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14771v1" onclick="toggleFavorite(this, '2510.14771v1', 'Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251014454v1-towards-adaptable-humanoid-control-via-adaptive-motion-tracking.html">Towards Adaptable Humanoid Control via Adaptive Motion Tracking</a></td>
  <td>AdaMimicï¼šåŸºäºè‡ªé€‚åº”è¿åŠ¨è·Ÿè¸ªçš„é€šç”¨äººå½¢æœºå™¨äººæ§åˆ¶æ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14454v1" onclick="toggleFavorite(this, '2510.14454v1', 'Towards Adaptable Humanoid Control via Adaptive Motion Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251014947v2-architecture-is-all-you-need-diversity-enabled-sweet-spots-for-robus.html">Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion</a></td>
  <td>æå‡ºåˆ†å±‚æ§åˆ¶æ¶æ„ï¼Œæå‡äººå½¢æœºå™¨äººå¤æ‚åœ°å½¢çš„é²æ£’è¿åŠ¨æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14947v2" onclick="toggleFavorite(this, '2510.14947v2', 'Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251014643v1-generative-models-from-and-for-sampling-based-mpc-a-bootstrapped-app.html">Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation</a></td>
  <td>æå‡ºç”Ÿæˆé¢„æµ‹æ§åˆ¶æ¡†æ¶ä»¥æå‡æ¥è§¦ä¸°å¯Œæ“ä½œçš„é‡‡æ ·æ•ˆç‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14643v1" onclick="toggleFavorite(this, '2510.14643v1', 'Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251014338v1-risk-aware-reinforcement-learning-with-bandit-based-adaptation-for-q.html">Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion</a></td>
  <td>æå‡ºåŸºäºBanditè‡ªé€‚åº”çš„é£é™©æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œæå‡å››è¶³æœºå™¨äººè¿åŠ¨é²æ£’æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14338v1" onclick="toggleFavorite(this, '2510.14338v1', 'Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251014647v1-spatially-anchored-tactile-awareness-for-robust-dexterous-manipulati.html">Spatially anchored Tactile Awareness for Robust Dexterous Manipulation</a></td>
  <td>æå‡ºSaTAæ¡†æ¶ï¼Œé€šè¿‡ç©ºé—´é”šå®šçš„è§¦è§‰æ„ŸçŸ¥å®ç°é²æ£’çš„çµå·§æ“ä½œ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14647v1" onclick="toggleFavorite(this, '2510.14647v1', 'Spatially anchored Tactile Awareness for Robust Dexterous Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251015189v1-rm-rl-role-model-reinforcement-learning-for-precise-robot-manipulati.html">RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation</a></td>
  <td>RM-RLï¼šé¢å‘ç²¾å‡†æœºå™¨äººæ“ä½œçš„è§’è‰²æ¨¡å‹å¼ºåŒ–å­¦ä¹ </td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15189v1" onclick="toggleFavorite(this, '2510.15189v1', 'RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251014783v1-skydreamer-interpretable-end-to-end-vision-based-drone-racing-with-m.html">SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning</a></td>
  <td>SkyDreamerï¼šåŸºäºæ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„å¯è§£é‡Šç«¯åˆ°ç«¯è§†è§‰æ— äººæœºç«é€Ÿ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14783v1" onclick="toggleFavorite(this, '2510.14783v1', 'SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251014300v1-expertise-need-not-monopolize-action-specialized-mixture-of-experts-.html">Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning</a></td>
  <td>æå‡ºAdaMoEï¼Œä¸€ç§åŠ¨ä½œä¸“ç”¨æ··åˆä¸“å®¶æ¨¡å‹ï¼Œæå‡VLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14300v1" onclick="toggleFavorite(this, '2510.14300v1', 'Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251014830v3-rl-100-performant-robotic-manipulation-with-real-world-reinforcement.html">RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning</a></td>
  <td>RL-100ï¼šåŸºäºçœŸå®ä¸–ç•Œå¼ºåŒ–å­¦ä¹ çš„é«˜æ€§èƒ½æœºå™¨äººæ“ä½œæ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14830v3" onclick="toggleFavorite(this, '2510.14830v3', 'RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251014768v1-leveraging-neural-descriptor-fields-for-learning-contact-aware-dynam.html">Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery</a></td>
  <td>æå‡ºCADREæ¡†æ¶ï¼Œåˆ©ç”¨ç¥ç»æè¿°åœºå­¦ä¹ æ¥è§¦æ„ŸçŸ¥çš„åŠ¨æ€æ¢å¤ï¼Œæå‡çµå·§æ“ä½œçš„é²æ£’æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14768v1" onclick="toggleFavorite(this, '2510.14768v1', 'Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251014930v2-vt-refine-learning-bimanual-assembly-with-visuo-tactile-feedback-via.html">VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning</a></td>
  <td>VT-Refineï¼šé€šè¿‡æ¨¡æ‹Ÿå¾®è°ƒå­¦ä¹ åŸºäºè§†è§‰-è§¦è§‰åé¦ˆçš„åŒè‡‚è£…é…</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14930v2" onclick="toggleFavorite(this, '2510.14930v2', 'VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251014959v2-cbf-rl-safety-filtering-reinforcement-learning-in-training-with-cont.html">CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions</a></td>
  <td>æå‡ºCBF-RLæ¡†æ¶ï¼Œé€šè¿‡æ§åˆ¶å±éšœå‡½æ•°åœ¨è®­ç»ƒä¸­å®‰å…¨è¿‡æ»¤å¼ºåŒ–å­¦ä¹ ç­–ç•¥</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14959v2" onclick="toggleFavorite(this, '2510.14959v2', 'CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251014467v1-restoring-noisy-demonstration-for-imitation-learning-with-diffusion-.html">Restoring Noisy Demonstration for Imitation Learning With Diffusion Models</a></td>
  <td>æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ¢å¤å«å™ªå£°çš„ä¸“å®¶æ¼”ç¤ºæ•°æ®ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14467v1" onclick="toggleFavorite(this, '2510.14467v1', 'Restoring Noisy Demonstration for Imitation Learning With Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251014612v1-proprioceptive-image-an-image-representation-of-proprioceptive-data-.html">Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning</a></td>
  <td>æå‡ºä¸€ç§åŸºäºæœ¬ä½“æ„Ÿå—å›¾åƒçš„å››è¶³æœºå™¨äººæ¥è§¦ä¼°è®¡å­¦ä¹ æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14612v1" onclick="toggleFavorite(this, '2510.14612v1', 'Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251014293v1-learning-human-humanoid-coordination-for-collaborative-object-carryi.html">Learning Human-Humanoid Coordination for Collaborative Object Carrying</a></td>
  <td>æå‡ºCOLAç®—æ³•ï¼Œå®ç°åŸºäºæœ¬ä½“æ„Ÿè§‰çš„äººå½¢æœºå™¨äººååŒæ¬è¿ï¼Œæå‡äººæœºåä½œæ•ˆç‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14293v1" onclick="toggleFavorite(this, '2510.14293v1', 'Learning Human-Humanoid Coordination for Collaborative Object Carrying')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251014615v1-accelerated-multi-modal-motion-planning-using-context-conditioned-di.html">Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models</a></td>
  <td>æå‡ºCAMPDï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡æ¡ä»¶æ‰©æ•£æ¨¡å‹åŠ é€Ÿå¤šæ¨¡æ€è¿åŠ¨è§„åˆ’ï¼Œæå‡æ³›åŒ–æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14615v1" onclick="toggleFavorite(this, '2510.14615v1', 'Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251014234v1-prescribed-performance-control-of-deformable-object-manipulation-in-.html">Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space</a></td>
  <td>æå‡ºä¸€ç§åŸºäºç©ºé—´æ½œåœ¨ç©ºé—´çš„æŸ”æ€§ç‰©ä½“æ“ä½œé¢„å®šæ€§èƒ½æ§åˆ¶æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14234v1" onclick="toggleFavorite(this, '2510.14234v1', 'Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251014968v1-rdd-retrieval-based-demonstration-decomposer-for-planner-alignment-i.html">RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks</a></td>
  <td>æå‡ºRDDï¼šä¸€ç§åŸºäºæ£€ç´¢çš„åˆ†è§£å™¨ï¼Œç”¨äºé•¿æ—¶ä»»åŠ¡ä¸­è§„åˆ’å™¨å¯¹é½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14968v1" onclick="toggleFavorite(this, '2510.14968v1', 'RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251014677v1-when-planners-meet-reality-how-learned-reactive-traffic-agents-shift.html">When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks</a></td>
  <td>å¼•å…¥SMARTæ™ºèƒ½ä½“ï¼Œæå‡nuPlanè‡ªåŠ¨é©¾é©¶è§„åˆ’å™¨è¯„ä¼°çš„çœŸå®æ€§å’Œå¯é æ€§</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14677v1" onclick="toggleFavorite(this, '2510.14677v1', 'When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251023615v1-logic-based-task-representation-and-reward-shaping-in-multiagent-rei.html">Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºé€»è¾‘çš„ä»»åŠ¡è¡¨ç¤ºå’Œå¥–åŠ±å¡‘é€ æ–¹æ³•ï¼ŒåŠ é€Ÿå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23615v1" onclick="toggleFavorite(this, '2510.23615v1', 'Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251014946v1-edgenavmamba-mamba-optimized-object-detection-for-energy-efficient-e.html">EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices</a></td>
  <td>EdgeNavMambaï¼šé¢å‘è¾¹ç¼˜è®¾å¤‡çš„èŠ‚èƒ½Mambaä¼˜åŒ–ç›®æ ‡æ£€æµ‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14946v1" onclick="toggleFavorite(this, '2510.14946v1', 'EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251014851v1-sadcher-scheduling-using-attention-based-dynamic-coalitions-of-heter.html">SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time</a></td>
  <td>SADCHERï¼šåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å¼‚æ„å¤šæœºå™¨äººå®æ—¶åŠ¨æ€è”ç›Ÿè°ƒåº¦</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14851v1" onclick="toggleFavorite(this, '2510.14851v1', 'SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251014546v1-quash-using-natural-language-heuristics-to-query-visual-language-rob.html">QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps</a></td>
  <td>QuASHï¼šåˆ©ç”¨è‡ªç„¶è¯­è¨€å¯å‘å¼æ–¹æ³•æŸ¥è¯¢è§†è§‰-è¯­è¨€æœºå™¨äººåœ°å›¾</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14546v1" onclick="toggleFavorite(this, '2510.14546v1', 'QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251014627v2-gopla-generalizable-object-placement-learning-via-synthetic-augmenta.html">GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement</a></td>
  <td>GOPLAï¼šé€šè¿‡åˆæˆå¢å¼ºäººç±»å¸ƒç½®æ•°æ®ï¼Œå­¦ä¹ å¯æ³›åŒ–çš„ç‰©ä½“æ”¾ç½®</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14627v2" onclick="toggleFavorite(this, '2510.14627v2', 'GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/251014511v2-stability-criteria-and-motor-performance-in-delayed-haptic-dyadic-in.html">Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots</a></td>
  <td>é’ˆå¯¹æ—¶å»¶è§¦è§‰äººæœºäº¤äº’ï¼Œæå‡ºæœºå™¨äººè°ƒè§£ä¸‹çš„ç¨³å®šæ€§åˆ¤æ®</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14511v2" onclick="toggleFavorite(this, '2510.14511v2', 'Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/251014357v1-sum-agrivln-spatial-understanding-memory-for-agricultural-vision-and.html">SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</a></td>
  <td>æå‡ºSUM-AgriVLNï¼Œåˆ©ç”¨ç©ºé—´è®°å¿†æå‡å†œä¸šè§†è§‰è¯­è¨€å¯¼èˆªæ€§èƒ½</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14357v1" onclick="toggleFavorite(this, '2510.14357v1', 'SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/251014584v1-a-generalized-placeability-metric-for-model-free-unified-pick-and-pl.html">A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning</a></td>
  <td>æå‡ºä¸€ç§å¹¿ä¹‰å¯æ”¾ç½®æ€§æŒ‡æ ‡ï¼Œç”¨äºæ— æ¨¡å‹ç»Ÿä¸€æŠ“å–æ”¾ç½®æ¨ç†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14584v1" onclick="toggleFavorite(this, '2510.14584v1', 'A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)