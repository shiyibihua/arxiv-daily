---
layout: default
title: TRC: Trust Region Conditional Value at Risk for Safe Reinforcement Learning
---

# TRC: Trust Region Conditional Value at Risk for Safe Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00344" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00344v1</a>
  <a href="https://arxiv.org/pdf/2312.00344.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00344v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00344v1', 'TRC: Trust Region Conditional Value at Risk for Safe Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dohyeong Kim, Songhwai Oh

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

**å¤‡æ³¨**: RA-L and ICRA 2022

**æœŸåˆŠ**: IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 2621-2628, April 2022

**DOI**: [10.1109/LRA.2022.3141829](https://doi.org/10.1109/LRA.2022.3141829)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTRCæ–¹æ³•ä»¥å®ç°å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­çš„CVaRçº¦æŸ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `æ¡ä»¶ä»·å€¼é£é™©` `ä¿¡ä»»åŒºåŸŸ` `ç­–ç•¥æ¢¯åº¦` `æœºå™¨äººå¯¼èˆª` `é£é™©æ§åˆ¶` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†é«˜æˆæœ¬å¯¼è‡´çš„å¤±è´¥æ¦‚ç‡æ—¶å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆæ»¡è¶³å®‰å…¨çº¦æŸã€‚
2. æœ¬æ–‡æå‡ºçš„TRCæ–¹æ³•é€šè¿‡ä¿¡ä»»åŒºåŸŸå’Œæ¡ä»¶ä»·å€¼é£é™©ï¼ˆCVaRï¼‰çº¦æŸï¼Œä¼˜åŒ–äº†ç­–ç•¥çš„å®‰å…¨æ€§å’Œæ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTRCåœ¨å¤šç§æœºå™¨äººå¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½æå‡è¾¾åˆ°1.93å€ï¼ŒåŒæ—¶æ»¡è¶³æ‰€æœ‰å®‰å…¨çº¦æŸã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®‰å…¨æ€§åœ¨æœºå™¨äººé¢†åŸŸè‡³å…³é‡è¦ï¼Œå› æ­¤å®‰å…¨å¼ºåŒ–å­¦ä¹ ï¼ˆsafe RLï¼‰å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¿¡ä»»åŒºåŸŸçš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•TRCï¼Œæ—¨åœ¨æœ€å¤§åŒ–æœŸæœ›å›æŠ¥çš„åŒæ—¶æ»¡è¶³æ¡ä»¶ä»·å€¼é£é™©ï¼ˆCVaRï¼‰çº¦æŸã€‚æˆ‘ä»¬é¦–å…ˆæ¨å¯¼äº†CVaRçš„ä¸Šç•Œï¼Œå¹¶åœ¨ä¿¡ä»»åŒºåŸŸå†…ä»¥å¯å¾®åˆ†çš„å½¢å¼è¿‘ä¼¼è¯¥ä¸Šç•Œã€‚é€šè¿‡è¿™ç§è¿‘ä¼¼ï¼Œæ„å»ºäº†ä¸€ä¸ªç”¨äºè·å–ç­–ç•¥æ¢¯åº¦çš„å­é—®é¢˜ï¼Œå¹¶é€šè¿‡è¿­ä»£æ±‚è§£è¯¥å­é—®é¢˜æ¥è®­ç»ƒç­–ç•¥ã€‚TRCåœ¨å¤šç§æœºå™¨äººæ¨¡æ‹Ÿçš„å®‰å…¨å¯¼èˆªä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åœ¨Clearpathçš„Jackalæœºå™¨äººä¸Šè¿›è¡Œäº†ä»¿çœŸä¸ç°å®ç¯å¢ƒçš„å¯¹æ¯”å®éªŒã€‚ä¸å…¶ä»–å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒTRCåœ¨æ‰€æœ‰å®éªŒä¸­å‡æ»¡è¶³çº¦æŸæ¡ä»¶ï¼Œä¸”æ€§èƒ½æå‡äº†1.93å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­å¦‚ä½•æœ‰æ•ˆæ»¡è¶³æ¡ä»¶ä»·å€¼é£é™©ï¼ˆCVaRï¼‰çº¦æŸçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é«˜æˆæœ¬æƒ…å†µä¸‹çš„å¤±è´¥æ¦‚ç‡æ§åˆ¶ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥ä¿è¯å®‰å…¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTRCæ–¹æ³•é€šè¿‡ä¿¡ä»»åŒºåŸŸçš„æ¡†æ¶ï¼Œæ¨å¯¼å¹¶è¿‘ä¼¼CVaRçš„ä¸Šç•Œï¼Œä»¥æ­¤ä¸ºåŸºç¡€æ„å»ºç­–ç•¥æ¢¯åº¦çš„ä¼˜åŒ–é—®é¢˜ï¼Œä»è€Œå®ç°å®‰å…¨ä¸æ€§èƒ½çš„å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTRCçš„æ•´ä½“æ¶æ„åŒ…æ‹¬CVaRä¸Šç•Œçš„æ¨å¯¼ã€å¯å¾®åˆ†è¿‘ä¼¼çš„æ„å»ºå’Œç­–ç•¥æ¢¯åº¦çš„å­é—®é¢˜æ±‚è§£ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬å®‰å…¨çº¦æŸçš„å®šä¹‰ã€ä¿¡ä»»åŒºåŸŸçš„è®¾ç½®åŠç­–ç•¥çš„è¿­ä»£æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šTRCçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ä¿¡ä»»åŒºåŸŸæ–¹æ³•ä¸CVaRçº¦æŸç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†ç­–ç•¥çš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTRCåœ¨æ»¡è¶³çº¦æŸçš„åŒæ—¶ï¼Œä¼˜åŒ–äº†ç­–ç•¥çš„å›æŠ¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ä¿¡ä»»åŒºåŸŸçš„å¤§å°å’ŒCVaRçš„è®¡ç®—æ–¹å¼ï¼ŒæŸå¤±å‡½æ•°åˆ™ç»“åˆäº†å›æŠ¥å’Œå®‰å…¨çº¦æŸï¼Œç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å§‹ç»ˆæ»¡è¶³å®‰å…¨æ€§è¦æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTRCæ–¹æ³•åœ¨å¤šç§å®‰å…¨å¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½æå‡è¾¾1.93å€ï¼Œä¸”åœ¨æ‰€æœ‰å®éªŒä¸­å‡æ»¡è¶³CVaRçº¦æŸã€‚è¿™ä¸€ç»“æœè¡¨æ˜TRCåœ¨å®‰å…¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TRCæ–¹æ³•åœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶å’Œå…¶ä»–éœ€è¦é«˜å®‰å…¨æ€§çš„å¼ºåŒ–å­¦ä¹ åº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚é€šè¿‡æœ‰æ•ˆæ§åˆ¶é£é™©ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæå‡ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœªæ¥ï¼ŒTRCè¿˜å¯èƒ½æ‰©å±•åˆ°æ›´å¤šé¢†åŸŸï¼Œå¦‚é‡‘èå†³ç­–å’ŒåŒ»ç–—è¯Šæ–­ç­‰ï¼Œè¿›ä¸€æ­¥æå‡å†³ç­–è¿‡ç¨‹çš„å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As safety is of paramount importance in robotics, reinforcement learning that reflects safety, called safe RL, has been studied extensively. In safe RL, we aim to find a policy which maximizes the desired return while satisfying the defined safety constraints. There are various types of constraints, among which constraints on conditional value at risk (CVaR) effectively lower the probability of failures caused by high costs since CVaR is a conditional expectation obtained above a certain percentile. In this paper, we propose a trust region-based safe RL method with CVaR constraints, called TRC. We first derive the upper bound on CVaR and then approximate the upper bound in a differentiable form in a trust region. Using this approximation, a subproblem to get policy gradients is formulated, and policies are trained by iteratively solving the subproblem. TRC is evaluated through safe navigation tasks in simulations with various robots and a sim-to-real environment with a Jackal robot from Clearpath. Compared to other safe RL methods, the performance is improved by 1.93 times while the constraints are satisfied in all experiments.

