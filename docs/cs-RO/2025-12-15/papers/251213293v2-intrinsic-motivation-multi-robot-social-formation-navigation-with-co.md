---
layout: default
title: Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration
---

# Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.13293" target="_blank" class="toolbar-btn">arXiv: 2512.13293v2</a>
    <a href="https://arxiv.org/pdf/2512.13293.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13293v2" 
            onclick="toggleFavorite(this, '2512.13293v2', 'Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Hao Fu, Wei Liu, Shuai Zhou

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-15 (Êõ¥Êñ∞: 2025-12-16)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/czxhunzi/CEMRRL)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂÜÖÂú®Âä®Êú∫ÁöÑÂ§öÊú∫Âô®‰∫∫Á§æ‰ºöÁºñÈòüÂØºËà™ÁÆóÊ≥ïÔºåÂÆûÁé∞ÂçèÂêåÊé¢Á¥¢„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊú∫Âô®‰∫∫Á≥ªÁªü` `Âº∫ÂåñÂ≠¶‰π†` `Á§æ‰ºöÁºñÈòüÂØºËà™` `ÂÜÖÂú®Âä®Êú∫` `ÂçèÂêåÊé¢Á¥¢`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ë°å‰∫∫Ë°å‰∏∫ÁöÑ‰∏çÂèØÈ¢ÑÊµãÊÄßÂíå‰∏çÂêà‰ΩúÊÄßÁªôÂ§öÊú∫Âô®‰∫∫Á§æ‰ºöÁºñÈòüÂØºËà™Â∏¶Êù•ÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂçèÂêåÊé¢Á¥¢ÊïàÁéáÊñπÈù¢„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÂÜÖÂú®Âä®Êú∫ÁöÑÂçèÂêåÊé¢Á¥¢Â§öÊú∫Âô®‰∫∫Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÈÄöËøáËá™Â≠¶‰π†ÂÜÖÂú®Â•ñÂä±Êú∫Âà∂ÁºìËß£Á≠ñÁï•‰øùÂÆàÊÄß„ÄÇ
3. ÈááÁî®ÂèåÈáçÈááÊ†∑Ê®°ÂºèÂ¢ûÂº∫ÂØºËà™Á≠ñÁï•ÂíåÂÜÖÂú®Â•ñÂä±ÁöÑË°®Á§∫ÔºåÂπ∂ÈÄöËøáÂèåÊó∂Èó¥Â∞∫Â∫¶Êõ¥Êñ∞ËßÑÂàôËß£ËÄ¶ÂèÇÊï∞Êõ¥Êñ∞„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®Â§öÊú∫Âô®‰∫∫Á§æ‰ºöÁºñÈòüÂØºËà™‰∏≠ÁöÑÂ∫îÁî®ÔºåËøôÊòØÂÆûÁé∞Êó†Áºù‰∫∫Êú∫ÂÖ±Â≠òÁöÑÂÖ≥ÈîÆËÉΩÂäõ„ÄÇËôΩÁÑ∂RLÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÊúâÂâçÊôØÁöÑËåÉ‰æãÔºå‰ΩÜË°å‰∫∫Ë°å‰∏∫Âõ∫ÊúâÁöÑ‰∏çÂèØÈ¢ÑÊµãÊÄßÂíåÈÄöÂ∏∏‰∏çÂêà‰ΩúÁöÑÂä®ÊÄÅÂ∏¶Êù•‰∫ÜÂ∑®Â§ßÁöÑÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®Êú∫Âô®‰∫∫‰πãÈó¥ÂçèË∞ÉÊé¢Á¥¢ÁöÑÊïàÁéáÊñπÈù¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂçèÂêåÊé¢Á¥¢Â§öÊú∫Âô®‰∫∫RLÁÆóÊ≥ïÔºåÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂÜÖÂú®Âä®Êú∫Êé¢Á¥¢„ÄÇÂÖ∂Ê†∏ÂøÉÁªÑÊàêÈÉ®ÂàÜÊòØ‰∏ÄÁßçËá™Â≠¶‰π†ÂÜÖÂú®Â•ñÂä±Êú∫Âà∂ÔºåÊó®Âú®ÂÖ±ÂêåÁºìËß£Á≠ñÁï•‰øùÂÆàÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÁÆóÊ≥ïÂú®ÈõÜ‰∏≠ËÆ≠ÁªÉÂíåÂàÜÊï£ÊâßË°åÊ°ÜÊû∂ÂÜÖÁªìÂêà‰∫ÜÂèåÈáçÈááÊ†∑Ê®°ÂºèÔºå‰ª•Â¢ûÂº∫ÂØºËà™Á≠ñÁï•ÂíåÂÜÖÂú®Â•ñÂä±ÁöÑË°®Á§∫ÔºåÂà©Áî®ÂèåÊó∂Èó¥Â∞∫Â∫¶Êõ¥Êñ∞ËßÑÂàôÊù•Ëß£ËÄ¶ÂèÇÊï∞Êõ¥Êñ∞„ÄÇÂú®Á§æ‰ºöÁºñÈòüÂØºËà™Âü∫ÂáÜ‰∏äÁöÑÁªèÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÁÆóÊ≥ïÂú®ÂÖ≥ÈîÆÊåáÊ†á‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊú∫Âô®‰∫∫Á§æ‰ºöÁºñÈòüÂØºËà™‰∏≠ÔºåÁî±‰∫éË°å‰∫∫Ë°å‰∏∫ÁöÑÂ§çÊùÇÊÄßÂíå‰∏çÁ°ÆÂÆöÊÄßÔºåÂØºËá¥Êú∫Âô®‰∫∫Èöæ‰ª•È´òÊïàÂçèÂêåÊé¢Á¥¢ÁéØÂ¢ÉÔºå‰ªéËÄåÂΩ±ÂìçÂØºËà™ÊÄßËÉΩÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÂ≠òÂú®Á≠ñÁï•‰øùÂÆàÊÄßÔºåÈöæ‰ª•ÂÖÖÂàÜÊé¢Á¥¢Â§çÊùÇÁéØÂ¢É„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂºïÂÖ•ÂÜÖÂú®Âä®Êú∫ÔºåÈºìÂä±Êú∫Âô®‰∫∫‰∏ªÂä®Êé¢Á¥¢Êú™Áü•Âå∫ÂüüÔºå‰ªéËÄåÊèêÈ´òÂçèÂêåÊé¢Á¥¢ÁöÑÊïàÁéá„ÄÇÈÄöËøáËÆæËÆ°‰∏ÄÁßçËá™Â≠¶‰π†ÁöÑÂÜÖÂú®Â•ñÂä±Êú∫Âà∂ÔºåÂºïÂØºÊú∫Âô®‰∫∫Â≠¶‰π†Êõ¥ÊúâÊïàÁöÑÂØºËà™Á≠ñÁï•ÔºåÂπ∂ÁºìËß£Á≠ñÁï•‰øùÂÆàÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÁÆóÊ≥ïÈááÁî®ÈõÜ‰∏≠ËÆ≠ÁªÉÂíåÂàÜÊï£ÊâßË°åÔºàCTDEÔºâÊ°ÜÊû∂„ÄÇÂú®ÈõÜ‰∏≠ËÆ≠ÁªÉÈò∂ÊÆµÔºåÊâÄÊúâÊú∫Âô®‰∫∫ÁöÑ‰ø°ÊÅØË¢´ÈõÜ‰∏≠Ëµ∑Êù•ËøõË°åÁ≠ñÁï•Â≠¶‰π†ÂíåÂÜÖÂú®Â•ñÂä±Â≠¶‰π†„ÄÇÂú®ÂàÜÊï£ÊâßË°åÈò∂ÊÆµÔºåÊØè‰∏™Êú∫Âô®‰∫∫Ê†πÊçÆÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•ÂíåÂÜÖÂú®Â•ñÂä±Áã¨Á´ãË°åÂä®„ÄÇÁÆóÊ≥ïÂåÖÂê´ÂØºËà™Á≠ñÁï•Â≠¶‰π†Ê®°ÂùóÂíåÂÜÖÂú®Â•ñÂä±Â≠¶‰π†Ê®°ÂùóÔºåÂπ∂ÈááÁî®ÂèåÈáçÈááÊ†∑Ê®°ÂºèÊù•Â¢ûÂº∫Ë°®Á§∫ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™Â≠¶‰π†ÁöÑÂÜÖÂú®Â•ñÂä±Êú∫Âà∂ÔºåËØ•Êú∫Âà∂ËÉΩÂ§üÊ†πÊçÆÁéØÂ¢ÉÁöÑÂ§çÊùÇÊÄßÂíåÊú∫Âô®‰∫∫ÁöÑÊé¢Á¥¢ÊÉÖÂÜµÂä®ÊÄÅË∞ÉÊï¥Â•ñÂä±Ôºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞ÂºïÂØºÊú∫Âô®‰∫∫ËøõË°åÂçèÂêåÊé¢Á¥¢„ÄÇÊ≠§Â§ñÔºåÂèåÈáçÈááÊ†∑Ê®°ÂºèÂíåÂèåÊó∂Èó¥Â∞∫Â∫¶Êõ¥Êñ∞ËßÑÂàô‰πüÊúâÂä©‰∫éÊèêÈ´òÁÆóÊ≥ïÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÜÖÂú®Â•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆÔºåÂÆÉÈúÄË¶ÅËÉΩÂ§üÂèçÊò†ÁéØÂ¢ÉÁöÑÊú™Áü•ÊÄßÂíåÊú∫Âô®‰∫∫ÁöÑÊé¢Á¥¢Á®ãÂ∫¶„ÄÇËÆ∫ÊñáÈááÁî®‰∫Ü‰∏ÄÁßçÂü∫‰∫éÈ¢ÑÊµãËØØÂ∑ÆÁöÑÂÜÖÂú®Â•ñÂä±ÂáΩÊï∞ÔºåÈºìÂä±Êú∫Âô®‰∫∫Êé¢Á¥¢ÈÇ£‰∫õÈ¢ÑÊµãËØØÂ∑ÆËæÉÂ§ßÁöÑÂå∫Âüü„ÄÇÂèåÊó∂Èó¥Â∞∫Â∫¶Êõ¥Êñ∞ËßÑÂàôÁî®‰∫éËß£ËÄ¶ÂØºËà™Á≠ñÁï•ÂíåÂÜÖÂú®Â•ñÂä±ÁöÑÂèÇÊï∞Êõ¥Êñ∞ÔºåÈÅøÂÖçÁõ∏‰∫íÂπ≤Êâ∞„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÊçüÂ§±ÂáΩÊï∞ÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÁÆóÊ≥ïÂú®Á§æ‰ºöÁºñÈòüÂØºËà™Âü∫ÂáÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåËØ•ÁÆóÊ≥ïÂú®ÂØºËà™ÊàêÂäüÁéá„ÄÅË∑ØÂæÑÈïøÂ∫¶ÂíåÁ¢∞ÊíûÁéáÁ≠âÂÖ≥ÈîÆÊåáÊ†á‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇÂºÄÊ∫ê‰ª£Á†ÅÂíåËßÜÈ¢ëÊºîÁ§∫ÂèØÂú®GitHub‰∏äËé∑Âèñ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶Å‰∫∫Êú∫ÂÖ±Â≠òÂíåÂçèÂêåÂØºËà™ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÔºöÂïÜÂú∫„ÄÅÊú∫Âú∫„ÄÅÂçöÁâ©È¶ÜÁ≠âÂÖ¨ÂÖ±Âú∫ÊâÄÁöÑÂØºËßàÊú∫Âô®‰∫∫ÔºåÂåªÈô¢„ÄÅÂÖªËÄÅÈô¢Á≠âÂú∫ÊâÄÁöÑËæÖÂä©Êú∫Âô®‰∫∫Ôºå‰ª•ÂèäÊô∫ËÉΩ‰ªìÂÇ®„ÄÅÊô∫ËÉΩÂ∑•ÂéÇÁ≠âÈ¢ÜÂüüÁöÑÂçè‰ΩúÊú∫Âô®‰∫∫„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÂØºËà™ÊïàÁéáÂíåÂÆâÂÖ®ÊÄßÔºåÂèØ‰ª•ÊîπÂñÑÁî®Êà∑‰ΩìÈ™åÔºåÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÔºåÂπ∂‰øÉËøõ‰∫∫Êú∫Âçè‰ΩúÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.

