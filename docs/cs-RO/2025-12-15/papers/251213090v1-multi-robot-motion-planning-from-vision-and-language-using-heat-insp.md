---
layout: default
title: Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion
---

# Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.13090" target="_blank" class="toolbar-btn">arXiv: 2512.13090v1</a>
    <a href="https://arxiv.org/pdf/2512.13090.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13090v1" 
            onclick="toggleFavorite(this, '2512.13090v1', 'Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jebeom Chae, Junwoo Chang, Seungho Yeom, Yujin Kim, Jongeun Choi

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-15

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÁÉ≠Êâ©Êï£ÁöÑÂ§öÊú∫Âô®‰∫∫ËßÜËßâËØ≠Ë®ÄËøêÂä®ËßÑÂàíÊ°ÜÊû∂LCHD**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊú∫Âô®‰∫∫ËøêÂä®ËßÑÂàí` `ËßÜËßâËØ≠Ë®ÄÂØºËà™` `Êâ©Êï£Ê®°Âûã` `CLIPÊ®°Âûã` `Á¢∞ÊíûÈÅøÂÖç` `ÁÉ≠Êâ©Êï£` `Êú∫Âô®‰∫∫Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÊâ©Êï£ÁöÑÊú∫Âô®‰∫∫ËøêÂä®ËßÑÂàíÊñπÊ≥ïËÆ°ÁÆóÊàêÊú¨È´òÔºåÊ≥õÂåñËÉΩÂäõÂº±ÔºåÈöæ‰ª•Â§ÑÁêÜÂ§öÊú∫Âô®‰∫∫ÂíåËØ≠Ë®ÄÊù°‰ª∂‰ªªÂä°„ÄÇ
2. LCHDÊ°ÜÊû∂ÁªìÂêàCLIPËØ≠‰πâÂÖàÈ™åÂíåÁ¢∞ÊíûÈÅøÂÖçÊâ©Êï£Ê†∏ÔºåÂú®ÂèØËææÂ∑•‰ΩúÁ©∫Èó¥ÂÜÖËß£ÊûêËØ≠Ë®ÄÊåá‰ª§ÔºåÊèêÂçáÊ≥õÂåñÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåLCHDÂú®ÊàêÂäüÁéá‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Èôç‰Ωé‰∫ÜËßÑÂàíÂª∂ËøüÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êâ©Êï£Ê®°ÂûãÂú®Êú∫Âô®‰∫∫ËøêÂä®ËßÑÂàí‰∏≠Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑËÉΩÂäõÔºåËÉΩÂ§üÊçïÊçâÂèØË°åËΩ®ËøπÁöÑÂ§öÊ®°ÊÄÅÂàÜÂ∏É„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜÂÖ∂Êâ©Â±ïÂà∞ÂÖ∑ÊúâÁÅµÊ¥ªÁöÑ„ÄÅËØ≠Ë®ÄÊù°‰ª∂‰ªªÂä°ËßÑËåÉÁöÑÂ§öÊú∫Âô®‰∫∫ÁéØÂ¢É‰ªçÁÑ∂ÊúâÈôê„ÄÇÊ≠§Â§ñÔºåÁé∞ÊúâÁöÑÂü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÂπ∂‰∏îÁî±‰∫éÈúÄË¶ÅÊòæÂºèÊûÑÂª∫ÁéØÂ¢ÉË°®Á§∫‰∏îÁº∫‰πèÂá†‰ΩïÂèØËææÊÄßÊé®ÁêÜÊú∫Âà∂ÔºåÂõ†Ê≠§Èöæ‰ª•Ê≥õÂåñ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÂ±ÄÈôêÊÄßÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËØ≠Ë®ÄÊù°‰ª∂ÁÉ≠Êâ©Êï£ÔºàLCHDÔºâÔºåËøôÊòØ‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÂü∫‰∫éËßÜËßâÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•ÁîüÊàêËØ≠Ë®ÄÊù°‰ª∂‰∏ãÁöÑÊó†Á¢∞ÊíûËΩ®Ëøπ„ÄÇLCHDÈõÜÊàê‰∫ÜÂü∫‰∫éCLIPÁöÑËØ≠‰πâÂÖàÈ™åÁü•ËØÜÂíå‰∏Ä‰∏™ÈÅøÂÖçÁ¢∞ÊíûÁöÑÊâ©Êï£Ê†∏Ôºå‰Ωú‰∏∫‰∏ÄÁßçÁâ©ÁêÜÂΩíÁ∫≥ÂÅèÁΩÆÔºå‰ΩøËßÑÂàíÂô®ËÉΩÂ§üÂú®ÂèØËææÂ∑•‰ΩúÁ©∫Èó¥ÂÜÖ‰∏•Ê†ºÂú∞Ëß£ÈáäËØ≠Ë®ÄÂëΩ‰ª§„ÄÇÈÄöËøáÂºïÂØºÊú∫Âô®‰∫∫ÊâæÂà∞‰∏éËØ≠‰πâÊÑèÂõæÁõ∏ÂåπÈÖçÁöÑÂèØËææÊõø‰ª£ÊñπÊ°àÔºåËá™ÁÑ∂Âú∞Â§ÑÁêÜ‰∫ÜÂèØËææÊÄßÊñπÈù¢ÁöÑÂàÜÂ∏ÉÂ§ñÂú∫ÊôØÔºåÂêåÊó∂Ê∂àÈô§‰∫ÜÊé®ÁêÜÊó∂ÂØπÊòæÂºèÈöúÁ¢çÁâ©‰ø°ÊÅØÁöÑÈúÄÊ±Ç„ÄÇÂú®ÂêÑÁßçÂèóÁé∞ÂÆû‰∏ñÁïåÂêØÂèëÁöÑÂú∞Âõæ‰∏äÁöÑÂ§ßÈáèËØÑ‰º∞‰ª•ÂèäÁúüÂÆûÁöÑÊú∫Âô®‰∫∫ÂÆûÈ™åË°®ÊòéÔºåLCHDÂú®ÊàêÂäüÁéáÊñπÈù¢ÂßãÁªà‰ºò‰∫éÂÖàÂâçÁöÑÂü∫‰∫éÊâ©Êï£ÁöÑËßÑÂàíÂô®ÔºåÂêåÊó∂Èôç‰Ωé‰∫ÜËßÑÂàíÂª∂Ëøü„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂü∫‰∫éÊâ©Êï£ÁöÑÊú∫Âô®‰∫∫ËøêÂä®ËßÑÂàíÊñπÊ≥ïÔºåÂú®Â§öÊú∫Âô®‰∫∫Âú∫ÊôØ‰∏ãÔºåÈöæ‰ª•ÁªìÂêàËØ≠Ë®ÄÊåá‰ª§ËøõË°å‰ªªÂä°ËßÑÂàí„ÄÇÂÆÉ‰ª¨ÈÄöÂ∏∏ÈúÄË¶ÅÊòæÂºèÂú∞ÊûÑÂª∫ÁéØÂ¢ÉË°®Á§∫ÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÂπ∂‰∏îÁº∫‰πèÂØπÂá†‰ΩïÂèØËææÊÄßÁöÑÊúâÊïàÊé®ÁêÜÔºåÂØºËá¥Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÈöæ‰ª•Â§ÑÁêÜÂàÜÂ∏ÉÂ§ñÂú∫ÊôØ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLCHDÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËØ≠Ë®ÄÊåá‰ª§„ÄÅËßÜËßâ‰ø°ÊÅØÂíåÁâ©ÁêÜÁ∫¶ÊùüÔºàÁ¢∞ÊíûÈÅøÂÖçÔºâÈõÜÊàêÂà∞‰∏Ä‰∏™Êâ©Êï£Ê®°Âûã‰∏≠„ÄÇÈÄöËøáCLIPÊ®°ÂûãÊèêÂèñËØ≠Ë®ÄÊåá‰ª§ÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫Êâ©Êï£ËøáÁ®ãÁöÑÊù°‰ª∂„ÄÇÂêåÊó∂ÔºåÂà©Áî®‰∏Ä‰∏™Á¢∞ÊíûÈÅøÂÖçÁöÑÊâ©Êï£Ê†∏‰Ωú‰∏∫Áâ©ÁêÜÂΩíÁ∫≥ÂÅèÁΩÆÔºåÂºïÂØºÊú∫Âô®‰∫∫ÁîüÊàêÊó†Á¢∞ÊíûËΩ®ËøπÔºåÂπ∂Á°Æ‰øùËΩ®ËøπÁöÑÂèØËææÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLCHDÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) Âü∫‰∫éËßÜËßâÁöÑÂú∫ÊôØÁêÜËß£Ê®°ÂùóÔºàËæìÂÖ•ÂõæÂÉèÔºåÊèêÂèñÁéØÂ¢ÉÁâπÂæÅÔºâÔºõ2) Âü∫‰∫éCLIPÁöÑËØ≠Ë®ÄÊåá‰ª§ÁºñÁ†ÅÊ®°ÂùóÔºàËæìÂÖ•ËØ≠Ë®ÄÊåá‰ª§ÔºåÊèêÂèñËØ≠‰πâÁâπÂæÅÔºâÔºõ3) ÁÉ≠Êâ©Êï£ËøêÂä®ËßÑÂàíÊ®°ÂùóÔºàÁªìÂêàËßÜËßâÂíåËØ≠Ë®ÄÁâπÂæÅÔºåÁîüÊàêÊó†Á¢∞ÊíûËΩ®ËøπÔºâ„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÁ´ØÂà∞Á´ØÁöÑÔºåÂèØ‰ª•Áõ¥Êé•‰ªéËßÜËßâËæìÂÖ•ÂíåËØ≠Ë®ÄÊåá‰ª§ÁîüÊàêÊú∫Âô®‰∫∫ÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLCHDÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) Â∞ÜCLIPÊ®°ÂûãÂºïÂÖ•Âà∞Êú∫Âô®‰∫∫ËøêÂä®ËßÑÂàí‰∏≠ÔºåÂÆûÁé∞‰∫ÜËØ≠Ë®ÄÊù°‰ª∂‰∏ãÁöÑËøêÂä®ËßÑÂàíÔºõ2) ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Á¢∞ÊíûÈÅøÂÖçÁöÑÊâ©Êï£Ê†∏Ôºå‰Ωú‰∏∫Áâ©ÁêÜÂΩíÁ∫≥ÂÅèÁΩÆÔºåÊèêÈ´ò‰∫ÜËßÑÂàíÁöÑÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõÔºõ3) Êó†ÈúÄÊòæÂºèÊûÑÂª∫ÁéØÂ¢ÉË°®Á§∫ÔºåÁõ¥Êé•‰ªéËßÜËßâËæìÂÖ•ËøõË°åËßÑÂàíÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöLCHD‰ΩøÁî®CLIPÊ®°ÂûãÊèêÂèñËØ≠Ë®ÄÊåá‰ª§ÁöÑËØ≠‰πâÁâπÂæÅÔºåÂπ∂Â∞ÜÂÖ∂‰∏éËßÜËßâÁâπÂæÅËøõË°åËûçÂêà„ÄÇÊâ©Êï£Ê†∏ÁöÑËÆæËÆ°Âü∫‰∫éÁÉ≠Êâ©Êï£ÊñπÁ®ãÔºåÈÄöËøáË∞ÉÊï¥Êâ©Êï£Á≥ªÊï∞Êù•ÊéßÂà∂ËΩ®ËøπÁöÑÂπ≥ÊªëÊÄßÂíåÁ¢∞ÊíûÈÅøÂÖçËÉΩÂäõ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ËΩ®ËøπÂπ≥ÊªëÊçüÂ§±„ÄÅÁ¢∞ÊíûÈÅøÂÖçÊçüÂ§±ÂíåËØ≠Ë®Ä‰∏ÄËá¥ÊÄßÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñÊâ©Êï£Ê®°ÂûãÁöÑÂèÇÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

LCHDÂú®Â§ö‰∏™ÁúüÂÆûÂú∫ÊôØÂíåÊ®°ÊãüÁéØÂ¢É‰∏≠ËøõË°å‰∫ÜËØÑ‰º∞ÔºåÁªìÊûúË°®ÊòéÔºåLCHDÂú®ÊàêÂäüÁéáÊñπÈù¢ÂßãÁªà‰ºò‰∫éÂÖàÂâçÁöÑÂü∫‰∫éÊâ©Êï£ÁöÑËßÑÂàíÂô®ÔºåÂπ∂‰∏îÊòæËëóÈôç‰Ωé‰∫ÜËßÑÂàíÂª∂Ëøü„ÄÇÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠ÔºåLCHD‰πüË°®Áé∞Âá∫‰∫ÜËâØÂ•ΩÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØË°åÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LCHDÂèØÂ∫îÁî®‰∫éÂ§öÊú∫Âô®‰∫∫ÂçèÂêå‰Ωú‰∏ö„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊúçÂä°Êú∫Âô®‰∫∫Á≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®‰ªìÂ∫ìËá™Âä®ÂåñÂú∫ÊôØ‰∏≠ÔºåÂèØ‰ª•ÈÄöËøáËØ≠Ë®ÄÊåá‰ª§ÊéßÂà∂Â§ö‰∏™Êú∫Âô®‰∫∫ÂÆåÊàêË¥ßÁâ©ÁöÑÊê¨Ëøê‰ªªÂä°„ÄÇÂú®ÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫‰∏≠ÔºåÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑËØ≠Èü≥Êåá‰ª§ÔºåÂºïÂØºÊú∫Âô®‰∫∫ÂÆåÊàêÂêÑÁßçÂÆ∂Âä°‰ªªÂä°„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÊèêÂçáÊú∫Âô®‰∫∫ÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥Âíå‰∫∫Êú∫‰∫§‰∫íËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.

