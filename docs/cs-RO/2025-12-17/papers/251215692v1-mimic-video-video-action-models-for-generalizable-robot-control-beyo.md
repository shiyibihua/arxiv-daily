---
layout: default
title: mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs
---

# mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15692" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15692v1</a>
  <a href="https://arxiv.org/pdf/2512.15692.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15692v1" onclick="toggleFavorite(this, '2512.15692v1', 'mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºmimic-videoä»¥è§£å†³æœºå™¨äººæ§åˆ¶ä¸­çš„ç‰©ç†ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘-åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººæ§åˆ¶` `ç‰©ç†å› æœå…³ç³»` `æ ·æœ¬æ•ˆç‡` `åŠ¨æ€ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `é€†åŠ¨æ€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æ¨æ–­ç‰©ç†åŠ¨æ€å’Œæ—¶é—´ä¾èµ–æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´æ•°æ®æ”¶é›†è´Ÿæ‹…åŠ é‡ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†é¢‘-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡è§†é¢‘é¢„è®­ç»ƒæ•æ‰è¯­ä¹‰å’Œè§†è§‰åŠ¨æ€ï¼Œç®€åŒ–ä½çº§æ§åˆ¶ä»»åŠ¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ ·æœ¬æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰åœ¨æœºå™¨äººæ“ä½œä¸­ä¾èµ–äºå¤§è§„æ¨¡çš„é™æ€ç½‘é¡µæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯¼è‡´å…¶åœ¨æ¨æ–­å¤æ‚ç‰©ç†åŠ¨æ€å’Œæ—¶é—´ä¾èµ–æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘-åŠ¨ä½œæ¨¡å‹ï¼ˆVAMï¼‰ï¼Œé€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„äº’è”ç½‘è§†é¢‘æ¨¡å‹å’ŒåŸºäºæµåŒ¹é…çš„åŠ¨ä½œè§£ç å™¨ï¼Œæ—¨åœ¨åŒæ—¶æ•æ‰è¯­ä¹‰å’Œè§†è§‰åŠ¨æ€ï¼Œä»è€Œæé«˜ä½çº§æ§åˆ¶çš„æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ ·æœ¬æ•ˆç‡æé«˜äº†10å€ï¼Œæ”¶æ•›é€Ÿåº¦æå‡äº†2å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä¸­å¯¹å¤æ‚ç‰©ç†åŠ¨æ€å’Œæ—¶é—´ä¾èµ–æ€§æ¨æ–­çš„ä¸è¶³ï¼Œå¯¼è‡´æ•°æ®æ”¶é›†éœ€æ±‚é«˜æ˜‚çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„è§†é¢‘-åŠ¨ä½œæ¨¡å‹ï¼ˆVAMï¼‰é€šè¿‡ç»“åˆè§†é¢‘é¢„è®­ç»ƒå’ŒæµåŒ¹é…çš„åŠ¨ä½œè§£ç å™¨ï¼Œèƒ½å¤ŸåŒæ—¶æ•æ‰è¯­ä¹‰ä¿¡æ¯å’Œè§†è§‰åŠ¨æ€ï¼Œä»è€Œæœ‰æ•ˆé™ä½å¯¹ä¸“å®¶æ•°æ®çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªé¢„è®­ç»ƒçš„äº’è”ç½‘è§†é¢‘æ¨¡å‹å’Œä¸€ä¸ªåŸºäºæµåŒ¹é…çš„åŠ¨ä½œè§£ç å™¨ï¼Œåè€…æ ¹æ®è§†é¢‘ç©ºé—´çš„æ½œåœ¨è¡¨ç¤ºç”Ÿæˆä½çº§æœºå™¨äººåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è§†é¢‘ä¿¡æ¯ä¸åŠ¨ä½œç”Ÿæˆç›¸ç»“åˆï¼Œå½¢æˆé€†åŠ¨æ€æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œä¸ä¼ ç»Ÿçš„è§†è§‰-è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæ›´åŠ æ³¨é‡ç‰©ç†å› æœå…³ç³»çš„æ•æ‰ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æµåŒ¹é…æœºåˆ¶æ¥ä¼˜åŒ–åŠ¨ä½œè§£ç è¿‡ç¨‹ï¼Œç¡®ä¿ç”Ÿæˆçš„ä½çº§åŠ¨ä½œèƒ½å¤Ÿæœ‰æ•ˆåæ˜ è§†é¢‘ä¸­çš„åŠ¨æ€ä¿¡æ¯ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°çš„è®¾è®¡ä¸Šä¹Ÿè¿›è¡Œäº†é’ˆå¯¹æ€§çš„è°ƒæ•´ï¼Œä»¥æå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15692v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15692v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15692v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œmimic-videoæ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ ·æœ¬æ•ˆç‡æé«˜äº†10å€ï¼Œæ”¶æ•›é€Ÿåº¦æå‡äº†2å€ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¶æ„å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–åˆ¶é€ å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹ç‰©ç†åŠ¨æ€çš„ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ“ä½œï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.

