---
layout: default
title: Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey
---

# Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13073" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.13073v2</a>
  <a href="https://arxiv.org/pdf/2508.13073.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13073v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13073v2', 'Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-18 (Êõ¥Êñ∞: 2025-09-01)

**Â§áÊ≥®**: Project Page: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÁªºËø∞Â§ßÂûãVLMÂü∫Á°ÄÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰ΩúÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `Êú∫Âô®‰∫∫Êìç‰Ωú` `Â§öÊ®°ÊÄÅÁêÜËß£` `Â§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `Â±ÇÊ¨°Ê®°Âûã` `Á≥ªÁªüÂåñÂàÜÁ±ª`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂü∫‰∫éËßÑÂàôÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊñπÊ≥ïÂú®Â§çÊùÇÂíåÈùûÁªìÊûÑÂåñÁéØÂ¢É‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÈöæ‰ª•ÂÆûÁé∞ÊúâÊïàÁöÑÊ≥õÂåñÂíåÊâ©Â±ï„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜÂ§ßÂûãVLMÂü∫Á°ÄÁöÑVLAÊ®°ÂûãÔºåÈÄöËøáÁ≥ªÁªüÂåñÁöÑÂàÜÁ±ªÂíåÂàÜÊûêÔºåÊòéÁ°Æ‰∫ÜÊ®°ÂûãÊû∂ÊûÑÂíåÈõÜÊàêÊñπÊ≥ïÔºåÊé®Âä®‰∫ÜÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÁ†îÁ©∂ËøõÂ±ï„ÄÇ
3. ÈÄöËøáÂØπÁé∞ÊúâÊ®°ÂûãÁöÑÊï¥Âêà‰∏éÂàÜÊûêÔºåÊú¨ÊñáËØÜÂà´Âá∫Â§ö‰∏™ÊúâÂâçÊôØÁöÑÁ†îÁ©∂ÊñπÂêëÔºå‰øÉËøõ‰∫ÜÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÁªìÂêà„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú∫Âô®‰∫∫Êìç‰ΩúÊòØÊú∫Âô®‰∫∫Â≠¶ÂíåÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÁöÑÂÖ≥ÈîÆÂâçÊ≤øÔºåË¶ÅÊ±ÇÁ≤æÁ°ÆÁöÑËøêÂä®ÊéßÂà∂ÂíåÂ§öÊ®°ÊÄÅÁêÜËß£„ÄÇÁÑ∂ËÄåÔºå‰º†ÁªüÁöÑÂü∫‰∫éËßÑÂàôÁöÑÊñπÊ≥ïÂú®ÈùûÁªìÊûÑÂåñÁöÑÊñ∞ÁéØÂ¢É‰∏≠Èöæ‰ª•Êâ©Â±ïÊàñÊ≥õÂåñ„ÄÇËøëÂπ¥Êù•ÔºåÂü∫‰∫éÂ§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÊàê‰∏∫‰∏ÄÁßçÂèòÈù©ÊÄßËåÉÂºè„ÄÇÊú¨ÊñáÈ¶ñÊ¨°Á≥ªÁªüÊÄßÂú∞ÂõûÈ°æ‰∫ÜÂ§ßÂûãVLMÂü∫Á°ÄÁöÑVLAÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÂ∫îÁî®ÔºåÂÆö‰πâ‰∫Ü‰∏§Áßç‰∏ªË¶ÅÊû∂ÊûÑËåÉÂºèÔºåÂπ∂Ê∑±ÂÖ•Êé¢ËÆ®‰∫Ü‰∏éÂÖàËøõÈ¢ÜÂüüÁöÑÈõÜÊàê„ÄÅÁâπÂæÅÁöÑÁªºÂêà‰ª•ÂèäÊú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêëÔºåÂ°´Ë°•‰∫ÜÁé∞ÊúâÁ†îÁ©∂ÁöÑÁ©∫ÁôΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÊú∫Âô®‰∫∫Êìç‰ΩúÊñπÊ≥ïÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ±ÄÈôêÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§öÊ®°ÊÄÅÁêÜËß£ÂíåËøêÂä®ÊéßÂà∂ÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄ‰æùËµñ‰∫éËßÑÂàôÔºåÈöæ‰ª•ÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÁöÑÂèòÂåñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ßÂûãVLMÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÈÄöËøáËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÂä®‰ΩúÁöÑÁªìÂêàÔºåÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊìç‰ΩúËÉΩÂäõ„ÄÇÈÄöËøáÁ≥ªÁªüÂåñÁöÑÂàÜÁ±ªÂíåÂàÜÊûêÔºåÊòéÁ°Æ‰∏çÂêåÊ®°ÂûãÊû∂ÊûÑÁöÑ‰ºòÁº∫ÁÇπ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂàÜ‰∏∫‰∏§Â§ßÁ±ªÔºöÂçï‰ΩìÊ®°ÂûãÂíåÂ±ÇÊ¨°Ê®°Âûã„ÄÇÂçï‰ΩìÊ®°ÂûãÂåÖÊã¨ÂçïÁ≥ªÁªüÂíåÂèåÁ≥ªÁªüËÆæËÆ°ÔºåËÄåÂ±ÇÊ¨°Ê®°ÂûãÂàôÈÄöËøáÂèØËß£ÈáäÁöÑ‰∏≠Èó¥Ë°®Á§∫Â∞ÜËßÑÂàí‰∏éÊâßË°åÊòéÁ°ÆËß£ËÄ¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂ∞ÜÂ§ßÂûãVLM‰∏éÊú∫Âô®‰∫∫Êìç‰ΩúÁªìÂêàÔºåÊèêÂá∫‰∫ÜÊñ∞ÁöÑÊ®°ÂûãÊû∂ÊûÑÂíåÈõÜÊàêÊñπÊ≥ï„ÄÇËøôÁßçËÆæËÆ°‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠Êõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊâßË°å‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÂÖàËøõÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÂ¶ÇÂº∫ÂåñÂ≠¶‰π†ÂíåÊó†ËÆ≠ÁªÉ‰ºòÂåñÔºåÁªìÂêà‰∫∫Á±ªËßÜÈ¢ëÂ≠¶‰π†Âíå‰∏ñÁïåÊ®°ÂûãÈõÜÊàêÔºå‰ºòÂåñ‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÈÄÇÂ∫îÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂü∫‰∫éÂ§ßÂûãVLMÁöÑVLAÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÁõ∏ËæÉ‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåÂ∞§ÂÖ∂Âú®Â§çÊùÇ‰ªªÂä°ÁöÑÊâßË°å‰∏äÂ±ïÁé∞Âá∫ÊòæËëóÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂÆ∂Â±Ö„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÂíåÊúçÂä°Êú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊìç‰ΩúËÉΩÂäõÔºåËÉΩÂ§üÂÆûÁé∞Êõ¥È´òÊïàÁöÑ‰ªªÂä°ÊâßË°åÔºåÊé®Âä®Êô∫ËÉΩÊú∫Âô®‰∫∫ÊäÄÊúØÁöÑÂÆûÈôÖÂ∫îÁî®ÂíåÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation

