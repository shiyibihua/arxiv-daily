---
layout: default
title: D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning
---

# D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06804" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06804v1</a>
  <a href="https://arxiv.org/pdf/2508.06804.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06804v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06804v1', 'D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shu-Ang Yu, Feng Gao, Yi Wu, Chao Yu, Yu Wang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€å»å™ªæ‰©æ•£ç­–ç•¥ä»¥è§£å†³å®æ—¶éƒ¨ç½²ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ‰©æ•£ç­–ç•¥` `å»å™ª` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `å®æ—¶æ¨ç†` `åŠ¨æ€åˆ†é…` `è§†è§‰è¿åŠ¨ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£ç­–ç•¥åœ¨å®æ—¶éƒ¨ç½²ä¸­é¢ä¸´å»å™ªè¿‡ç¨‹çš„æ•ˆç‡ç“¶é¢ˆï¼Œå›ºå®šå»å™ªæ­¥éª¤æ— æ³•é€‚åº”ä¸åŒåŠ¨ä½œçš„é‡è¦æ€§ã€‚
2. D3Pé€šè¿‡è‡ªé€‚åº”åˆ†é…å»å™ªæ­¥éª¤ï¼Œåˆ©ç”¨çŠ¶æ€æ„ŸçŸ¥é€‚é…å™¨ä¼˜åŒ–æ¯ä¸ªåŠ¨ä½œçš„å»å™ªè¿‡ç¨‹ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚
3. åœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­ï¼ŒD3På®ç°äº†2.2å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œå¹¶åœ¨ç‰©ç†æœºå™¨äººä¸Šä¹Ÿå–å¾—äº†1.9å€çš„åŠ é€Ÿæ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£ç­–ç•¥åœ¨å­¦ä¹ å¤æ‚çš„æœºå™¨äººè§†è§‰è¿åŠ¨ä»»åŠ¡ä¸­çš„åŠ¨ä½œåˆ†å¸ƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶è¿­ä»£å»å™ªè¿‡ç¨‹æˆä¸ºå®æ—¶éƒ¨ç½²çš„ä¸»è¦ç“¶é¢ˆã€‚ç°æœ‰åŠ é€Ÿæ–¹æ³•å¯¹æ¯ä¸ªåŠ¨ä½œåº”ç”¨å›ºå®šæ•°é‡çš„å»å™ªæ­¥éª¤ï¼Œéšå«åœ°å°†æ‰€æœ‰åŠ¨ä½œè§†ä¸ºåŒç­‰é‡è¦ã€‚ç„¶è€Œï¼Œå®éªŒè¡¨æ˜ï¼Œæœºå™¨äººä»»åŠ¡é€šå¸¸åŒ…å«å…³é”®å’Œå¸¸è§„åŠ¨ä½œï¼Œè¿™äº›åŠ¨ä½œå¯¹ä»»åŠ¡æˆåŠŸçš„å½±å“ä¸åŒã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€å»å™ªæ‰©æ•£ç­–ç•¥ï¼ˆD3Pï¼‰ï¼Œè¯¥ç­–ç•¥åœ¨æµ‹è¯•æ—¶è‡ªé€‚åº”åœ°ä¸ºæ¯ä¸ªåŠ¨ä½œåˆ†é…å»å™ªæ­¥éª¤ã€‚D3Pä½¿ç”¨è½»é‡çº§çš„çŠ¶æ€æ„ŸçŸ¥é€‚é…å™¨æ¥ä¸ºæ¯ä¸ªåŠ¨ä½œåˆ†é…æœ€ä½³çš„å»å™ªæ­¥éª¤æ•°é‡ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è”åˆä¼˜åŒ–é€‚é…å™¨å’ŒåŸºç¡€æ‰©æ•£ç­–ç•¥ï¼Œä»¥å¹³è¡¡ä»»åŠ¡æ€§èƒ½å’Œæ¨ç†æ•ˆç‡ã€‚åœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­ï¼ŒD3På®ç°äº†æ¯”åŸºçº¿å¿«2.2å€çš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶æœªé™ä½æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨ç‰©ç†æœºå™¨äººä¸ŠéªŒè¯äº†D3Pçš„æœ‰æ•ˆæ€§ï¼Œè¾¾åˆ°äº†1.9å€çš„åŠ é€Ÿæ•ˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ‰©æ•£ç­–ç•¥åœ¨å®æ—¶éƒ¨ç½²ä¸­å› å›ºå®šå»å™ªæ­¥éª¤å¯¼è‡´çš„æ•ˆç‡ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½è€ƒè™‘ä¸åŒåŠ¨ä½œåœ¨ä»»åŠ¡æˆåŠŸä¸­çš„é‡è¦æ€§å·®å¼‚ï¼Œå¯¼è‡´èµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šD3Pçš„æ ¸å¿ƒæ€æƒ³æ˜¯æ ¹æ®ä»»åŠ¡çŠ¶æ€è‡ªé€‚åº”åœ°ä¸ºæ¯ä¸ªåŠ¨ä½œåˆ†é…å»å™ªæ­¥éª¤ï¼Œç¡®ä¿å…³é”®åŠ¨ä½œè·å¾—æ›´å¤šèµ„æºï¼Œè€Œå¸¸è§„åŠ¨ä½œåˆ™å‡å°‘å»å™ªæ­¥éª¤ï¼Œä»è€Œæé«˜æ•´ä½“æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šD3Pçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªçŠ¶æ€æ„ŸçŸ¥é€‚é…å™¨å’ŒåŸºç¡€æ‰©æ•£ç­–ç•¥ã€‚é€‚é…å™¨æ ¹æ®å½“å‰çŠ¶æ€åŠ¨æ€è°ƒæ•´å»å™ªæ­¥éª¤ï¼ŒåŸºç¡€æ‰©æ•£ç­–ç•¥åˆ™è´Ÿè´£ç”ŸæˆåŠ¨ä½œåˆ†å¸ƒã€‚ä¸¤è€…é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä»¥å®ç°æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šD3Pçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€å»å™ªæ­¥éª¤åˆ†é…æœºåˆ¶ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å›ºå®šæ­¥éª¤åˆ†é…ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚çµæ´»è°ƒæ•´ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æˆåŠŸç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé€‚é…å™¨çš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿å…¶èƒ½å¤Ÿå‡†ç¡®æ„ŸçŸ¥çŠ¶æ€å˜åŒ–å¹¶åšå‡ºåˆç†çš„å»å™ªæ­¥éª¤åˆ†é…ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†ä»»åŠ¡æ€§èƒ½ä¸æ¨ç†æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚é€‚é…å™¨å’Œæ‰©æ•£ç­–ç•¥çš„ç½‘ç»œç»“æ„å‡ç»è¿‡ä¼˜åŒ–ï¼Œä»¥å®ç°é«˜æ•ˆçš„è”åˆè®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

D3Påœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­å®ç°äº†2.2å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œä¸”æœªé™ä½æˆåŠŸç‡ã€‚åœ¨ç‰©ç†æœºå™¨äººå®éªŒä¸­ï¼ŒD3Pä¹Ÿè¾¾åˆ°äº†1.9å€çš„åŠ é€Ÿæ•ˆæœï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿ä»¥åŠæ™ºèƒ½å®¶å±…ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å®æ—¶ååº”èƒ½åŠ›ï¼ŒD3Pèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion policies excel at learning complex action distributions for robotic visuomotor tasks, yet their iterative denoising process poses a major bottleneck for real-time deployment. Existing acceleration methods apply a fixed number of denoising steps per action, implicitly treating all actions as equally important. However, our experiments reveal that robotic tasks often contain a mix of \emph{crucial} and \emph{routine} actions, which differ in their impact on task success. Motivated by this finding, we propose \textbf{D}ynamic \textbf{D}enoising \textbf{D}iffusion \textbf{P}olicy \textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising steps across actions at test time. D3P uses a lightweight, state-aware adaptor to allocate the optimal number of denoising steps for each action. We jointly optimize the adaptor and base diffusion policy via reinforcement learning to balance task performance and inference efficiency. On simulated tasks, D3P achieves an averaged 2.2$\times$ inference speed-up over baselines without degrading success. Furthermore, we demonstrate D3P's effectiveness on a physical robot, achieving a 1.9$\times$ acceleration over the baseline.

