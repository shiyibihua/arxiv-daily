---
layout: default
title: Reinforcement Learning with Data Bootstrapping for Dynamic Subgoal Pursuit in Humanoid Robot Navigation
---

# Reinforcement Learning with Data Bootstrapping for Dynamic Subgoal Pursuit in Humanoid Robot Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.02206" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.02206v1</a>
  <a href="https://arxiv.org/pdf/2506.02206.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.02206v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.02206v1', 'Reinforcement Learning with Data Bootstrapping for Dynamic Subgoal Pursuit in Humanoid Robot Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengyang Peng, Zhihao Zhang, Shiting Gong, Sankalp Agrawal, Keith A. Redmill, Ayonga Hereid

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-02

**å¤‡æ³¨**: 8 pages, 5 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€å­ç›®æ ‡è¿½è¸ªæ–¹æ³•ä»¥è§£å†³äººå½¢æœºå™¨äººå¯¼èˆªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `äººå½¢æœºå™¨äºº` `åŠ¨æ€å­ç›®æ ‡` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `å¯¼èˆª` `æ•°æ®è‡ªåŠ©æŠ€æœ¯` `æ­¥æ€ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒè¶³æœºå™¨äººå¯¼èˆªæ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸æ­¥æ€ç¨³å®šæ€§ä¹‹é—´å­˜åœ¨æ˜¾è‘—çŸ›ç›¾ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶å¯¼èˆªéœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ†å±‚æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å­ç›®æ ‡å¼•å¯¼æœºå™¨äººï¼Œç»“åˆé«˜å±‚æ¬¡å¼ºåŒ–å­¦ä¹ ä¸ä½å±‚æ¬¡æ¨¡å‹é¢„æµ‹æ§åˆ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¡†æ¶åœ¨å¤šåœºæ™¯ä¸‹çš„å¯¼èˆªæˆåŠŸç‡å’Œé€‚åº”æ€§æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹æ–¹æ³•åŠå…¶ä»–å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®‰å…¨ä¸”å®æ—¶çš„å¯¼èˆªå¯¹äºäººå½¢æœºå™¨äººåº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŒè¶³æœºå™¨äººå¯¼èˆªæ¡†æ¶å¸¸å¸¸éš¾ä»¥åœ¨è®¡ç®—æ•ˆç‡ä¸ç¨³å®šæ­¥æ€æ‰€éœ€çš„ç²¾åº¦ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†å±‚æ¡†æ¶ï¼Œèƒ½å¤ŸæŒç»­ç”ŸæˆåŠ¨æ€å­ç›®æ ‡ï¼Œå¼•å¯¼æœºå™¨äººç©¿è¶Šå¤æ‚ç¯å¢ƒã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªé«˜å±‚æ¬¡çš„å¼ºåŒ–å­¦ä¹ è§„åˆ’å™¨ï¼Œç”¨äºåœ¨æœºå™¨äººä¸­å¿ƒåæ ‡ç³»ä¸­é€‰æ‹©å­ç›®æ ‡ï¼Œä»¥åŠä¸€ä¸ªåŸºäºæ¨¡å‹é¢„æµ‹æ§åˆ¶çš„ä½å±‚æ¬¡è§„åˆ’å™¨ï¼Œç”Ÿæˆç¨³å¥çš„æ­¥æ€ä»¥è¾¾åˆ°è¿™äº›å­ç›®æ ‡ã€‚ä¸ºäº†åŠ é€Ÿå’Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œæˆ‘ä»¬ç»“åˆäº†æ•°æ®è‡ªåŠ©æŠ€æœ¯ï¼Œåˆ©ç”¨åŸºäºæ¨¡å‹çš„å¯¼èˆªæ–¹æ³•ç”Ÿæˆå¤šæ ·åŒ–ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ•°æ®é›†ã€‚é€šè¿‡åœ¨å¤šä¸ªéšæœºéšœç¢åœºæ™¯ä¸­å¯¹Agility Robotics Digitäººå½¢æœºå™¨äººè¿›è¡Œæ¨¡æ‹ŸéªŒè¯ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¡†æ¶æ˜¾è‘—æé«˜äº†å¯¼èˆªæˆåŠŸç‡å’Œé€‚åº”æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³äººå½¢æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­å®æ—¶å¯¼èˆªçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸æ­¥æ€ç¨³å®šæ€§ä¹‹é—´å­˜åœ¨çŸ›ç›¾ï¼Œå¯¼è‡´å¯¼èˆªæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„æ¡†æ¶é€šè¿‡åŠ¨æ€ç”Ÿæˆå­ç›®æ ‡ï¼Œç»“åˆé«˜å±‚æ¬¡çš„å¼ºåŒ–å­¦ä¹ å’Œä½å±‚æ¬¡çš„æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼Œæ—¨åœ¨æé«˜å¯¼èˆªçš„çµæ´»æ€§å’Œç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé«˜å±‚æ¬¡çš„å¼ºåŒ–å­¦ä¹ è§„åˆ’å™¨è´Ÿè´£é€‰æ‹©å­ç›®æ ‡ï¼Œä½å±‚æ¬¡çš„æ¨¡å‹é¢„æµ‹æ§åˆ¶è§„åˆ’å™¨åˆ™è´Ÿè´£ç”Ÿæˆç¨³å¥çš„æ­¥æ€ä»¥å®ç°ç›®æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥æ•°æ®è‡ªåŠ©æŠ€æœ¯ï¼Œé€šè¿‡æ¨¡å‹é©±åŠ¨çš„æ–¹æ³•ç”Ÿæˆå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œä»è€ŒåŠ é€Ÿå’Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­å°šæœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†æ¨¡å‹é¢„æµ‹æ§åˆ¶çš„å‚æ•°è®¾ç½®ï¼Œå¹¶é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥æé«˜æ­¥æ€çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ï¼ŒåŒæ—¶ç¡®ä¿é«˜å±‚æ¬¡è§„åˆ’å™¨çš„å†³ç­–æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨å¤šä¸ªéšæœºéšœç¢åœºæ™¯ä¸­ï¼Œå¯¼èˆªæˆåŠŸç‡æé«˜äº†æ˜¾è‘—çš„ç™¾åˆ†æ¯”ï¼Œç›¸è¾ƒäºåŸå§‹æ¨¡å‹æ–¹æ³•å’Œå…¶ä»–å­¦ä¹ æ–¹æ³•ï¼Œé€‚åº”æ€§ä¹Ÿæœ‰æ˜æ˜¾æå‡ï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€æ•‘æ´æœºå™¨äººåŠå…¶ä»–éœ€è¦åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»å¯¼èˆªçš„æœºå™¨äººç³»ç»Ÿã€‚é€šè¿‡æé«˜å¯¼èˆªçš„æˆåŠŸç‡å’Œé€‚åº”æ€§ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡æœºå™¨äººçš„å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨äººå½¢æœºå™¨äººåœ¨æ›´å¤šå¤æ‚åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Safe and real-time navigation is fundamental for humanoid robot applications. However, existing bipedal robot navigation frameworks often struggle to balance computational efficiency with the precision required for stable locomotion. We propose a novel hierarchical framework that continuously generates dynamic subgoals to guide the robot through cluttered environments. Our method comprises a high-level reinforcement learning (RL) planner for subgoal selection in a robot-centric coordinate system and a low-level Model Predictive Control (MPC) based planner which produces robust walking gaits to reach these subgoals. To expedite and stabilize the training process, we incorporate a data bootstrapping technique that leverages a model-based navigation approach to generate a diverse, informative dataset. We validate our method in simulation using the Agility Robotics Digit humanoid across multiple scenarios with random obstacles. Results show that our framework significantly improves navigation success rates and adaptability compared to both the original model-based method and other learning-based methods.

