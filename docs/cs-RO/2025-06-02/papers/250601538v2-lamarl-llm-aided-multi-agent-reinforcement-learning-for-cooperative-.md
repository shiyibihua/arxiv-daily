---
layout: default
title: LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation
---

# LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.01538" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.01538v2</a>
  <a href="https://arxiv.org/pdf/2506.01538.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.01538v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.01538v2', 'LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guobin Zhu, Rui Zhou, Wenkang Ji, Shiyu Zhao

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-02 (æ›´æ–°: 2025-06-03)

**å¤‡æ³¨**: Accepted by IEEE Robotics and Automation Letters

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://windylab.github.io/LAMARL/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLAMARLä»¥è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ ·æœ¬æ•ˆç‡` `è‡ªåŠ¨åŒ–è®¾è®¡` `æœºå™¨äººåä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œä¸”é€šå¸¸éœ€è¦åå¤æ‰‹åŠ¨è°ƒæ•´å¥–åŠ±å‡½æ•°ï¼Œå¢åŠ äº†å¼€å‘æˆæœ¬ã€‚
2. æœ¬æ–‡æå‡ºçš„LAMARLæ–¹æ³•é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè‡ªåŠ¨ç”Ÿæˆç­–ç•¥å’Œå¥–åŠ±å‡½æ•°ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡ï¼Œç®€åŒ–äº†è®¾è®¡è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLAMARLåœ¨å½¢çŠ¶ç»„è£…ä»»åŠ¡ä¸­ï¼Œæ ·æœ¬æ•ˆç‡æå‡185.9%ï¼Œå¹¶ä¸”é€šè¿‡ç»“æ„åŒ–æç¤ºæ˜¾è‘—æé«˜äº†LLMçš„è¾“å‡ºæˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨å¤æ‚çš„å¤šæœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°æœ‰æ•ˆï¼Œä½†å…¶æ ·æœ¬æ•ˆç‡ä½ä¸”éœ€è¦æ‰‹åŠ¨è°ƒæ•´å¥–åŠ±ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•æœºå™¨äººç¯å¢ƒä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤šæœºå™¨äººç³»ç»Ÿä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„LLMè¾…åŠ©MARLï¼ˆLAMARLï¼‰æ–¹æ³•ï¼Œå°†MARLä¸LLMsç»“åˆï¼Œæ˜¾è‘—æé«˜æ ·æœ¬æ•ˆç‡ï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡ã€‚LAMARLç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šç¬¬ä¸€ä¸ªæ¨¡å—åˆ©ç”¨LLMsè‡ªåŠ¨ç”Ÿæˆå…ˆéªŒç­–ç•¥å’Œå¥–åŠ±å‡½æ•°ï¼Œç¬¬äºŒä¸ªæ¨¡å—åˆ™ä½¿ç”¨ç”Ÿæˆçš„å‡½æ•°æœ‰æ•ˆæŒ‡å¯¼æœºå™¨äººç­–ç•¥è®­ç»ƒã€‚åœ¨å½¢çŠ¶ç»„è£…åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œå®éªŒå‡å±•ç¤ºäº†LAMARLçš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œå…ˆéªŒç­–ç•¥å¹³å‡æé«˜æ ·æœ¬æ•ˆç‡185.9%ï¼Œå¹¶å¢å¼ºä»»åŠ¡å®Œæˆç‡ï¼Œè€ŒåŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰çš„ç»“æ„åŒ–æç¤ºå’ŒåŸºæœ¬APIåˆ™æé«˜äº†LLMè¾“å‡ºæˆåŠŸç‡28.5%-67.5%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨æ ·æœ¬æ•ˆç‡ä½å’Œæ‰‹åŠ¨å¥–åŠ±è°ƒæ•´æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºäººå·¥è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œå¼€å‘å¤æ‚æ€§å¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLAMARLé€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸MARLç»“åˆï¼Œè‡ªåŠ¨ç”Ÿæˆå…ˆéªŒç­–ç•¥å’Œå¥–åŠ±å‡½æ•°ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡å¹¶å‡å°‘äººå·¥å¹²é¢„ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿæ›´å¿«é€Ÿåœ°é€‚åº”å¤æ‚ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLAMARLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç¬¬ä¸€ä¸ªæ¨¡å—åˆ©ç”¨LLMsç”Ÿæˆå…ˆéªŒç­–ç•¥å’Œå¥–åŠ±å‡½æ•°ï¼Œç¬¬äºŒä¸ªæ¨¡å—åˆ™æ˜¯MARLï¼Œä½¿ç”¨è¿™äº›ç”Ÿæˆçš„å‡½æ•°æ¥æŒ‡å¯¼æœºå™¨äººç­–ç•¥çš„è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šLAMARLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†LLMså¼•å…¥å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œè‡ªåŠ¨åŒ–ç”Ÿæˆç­–ç•¥å’Œå¥–åŠ±å‡½æ•°ï¼Œæ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡å’Œä»»åŠ¡å®Œæˆç‡ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»ŸMARLæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå‡å°‘äº†å¯¹äººå·¥è®¾è®¡çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰çš„ç»“æ„åŒ–æç¤ºæ¥ä¼˜åŒ–LLMçš„è¾“å‡ºï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†å…ˆéªŒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œç¡®ä¿äº†ç”Ÿæˆçš„å¥–åŠ±å‡½æ•°å’Œç­–ç•¥çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å½¢çŠ¶ç»„è£…åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLAMARLçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…ˆéªŒç­–ç•¥å¹³å‡æé«˜æ ·æœ¬æ•ˆç‡185.9%ï¼Œå¹¶ä¸”é€šè¿‡ç»“æ„åŒ–æç¤ºï¼ŒLLMçš„è¾“å‡ºæˆåŠŸç‡æå‡äº†28.5%-67.5%ã€‚è¿™äº›ç»“æœè¡¨æ˜LAMARLåœ¨å¤šæ™ºèƒ½ä½“ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LAMARLçš„ç ”ç©¶æˆæœåœ¨å¤šæœºå™¨äººåä½œä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆå­¦ä¹ å’Œå¿«é€Ÿé€‚åº”çš„åœºæ™¯ï¼Œå¦‚è‡ªåŠ¨åŒ–åˆ¶é€ ã€ç¾å®³æ•‘æ´å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜æ ·æœ¬æ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé™ä½å¼€å‘æˆæœ¬å¹¶åŠ é€Ÿç³»ç»Ÿéƒ¨ç½²ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ™®åŠå’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although Multi-Agent Reinforcement Learning (MARL) is effective for complex multi-robot tasks, it suffers from low sample efficiency and requires iterative manual reward tuning. Large Language Models (LLMs) have shown promise in single-robot settings, but their application in multi-robot systems remains largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL) approach, which integrates MARL with LLMs, significantly enhancing sample efficiency without requiring manual design. LAMARL consists of two modules: the first module leverages LLMs to fully automate the generation of prior policy and reward functions. The second module is MARL, which uses the generated functions to guide robot policy training effectively. On a shape assembly benchmark, both simulation and real-world experiments demonstrate the unique advantages of LAMARL. Ablation studies show that the prior policy improves sample efficiency by an average of 185.9% and enhances task completion, while structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM output success rates by 28.5%-67.5%. Videos and code are available at https://windylab.github.io/LAMARL/

