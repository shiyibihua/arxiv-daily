---
layout: default
title: Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories
---

# Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories

**arXiv**: [2511.19528v1](https://arxiv.org/abs/2511.19528) | [PDF](https://arxiv.org/pdf/2511.19528.pdf)

**ä½œè€…**: Rushuai Yang, Zhiyuan Feng, Tianxiang Zhang, Kaixin Wang, Chuheng Zhang, Li Zhao, Xiu Su, Yi Chen, Jiang Bian

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDLRæ¡†æž¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç”Ÿæˆå¤šæ ·åŒ–è½¨è¿¹ï¼Œæå‡VLAæ¨¡åž‹é¢„è®­ç»ƒæ•ˆæžœã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œé¢„è®­ç»ƒ` `å¼ºåŒ–å­¦ä¹ ` `ä¿¡æ¯è®º` `æ¨¡å¼å‘çŽ°` `å…·èº«æ™ºèƒ½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹é¢„è®­ç»ƒä¾èµ–äººå·¥é¥æ“ä½œæ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ï¼Œé™åˆ¶äº†æ¨¡åž‹æ€§èƒ½ã€‚
2. DLRæ¡†æž¶é€šè¿‡ä¿¡æ¯è®ºæ¨¡å¼å‘çŽ°ï¼Œé¼“åŠ±å¼ºåŒ–å­¦ä¹ æŽ¢ç´¢å¤šç§ä¸åŒçš„æˆåŠŸç­–ç•¥ï¼Œç”Ÿæˆå¤šæ ·åŒ–è½¨è¿¹ã€‚
3. å®žéªŒè¯æ˜Žï¼ŒDLRç”Ÿæˆçš„æ•°æ®èƒ½æ˜¾è‘—æå‡VLAæ¨¡åž‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶å±•çŽ°å‡ºè‰¯å¥½çš„æ•°æ®æ‰©å±•æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹é¢„è®­ç»ƒéœ€è¦å¤§é‡å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„æ“ä½œè½¨è¿¹ã€‚ç›®å‰çš„æ•°æ®ä¸»è¦é€šè¿‡äººå·¥é¥æ“ä½œèŽ·å¾—ï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ã€‚å¼ºåŒ–å­¦ä¹ (RL)æ–¹æ³•é€šè¿‡è‡ªä¸»æŽ¢ç´¢å­¦ä¹ æœ‰ç”¨çš„æŠ€èƒ½ï¼Œä½¿å…¶æˆä¸ºç”Ÿæˆæ•°æ®çš„å¯è¡Œæ–¹æ³•ã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„RLè®­ç»ƒä¼šæ”¶æ•›åˆ°ç‹­çª„çš„æ‰§è¡Œæ¨¡å¼ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­çš„æ•ˆç”¨ã€‚æˆ‘ä»¬æå‡ºäº†Discover, Learn and Reinforce (DLR)ï¼Œä¸€ä¸ªä¿¡æ¯è®ºæ¨¡å¼å‘çŽ°æ¡†æž¶ï¼Œä¸ºVLAé¢„è®­ç»ƒç”Ÿæˆå¤šä¸ªä¸åŒçš„ã€é«˜æˆåŠŸçš„è¡Œä¸ºæ¨¡å¼ã€‚å®žéªŒè¡¨æ˜Žï¼ŒDLRåœ¨LIBEROä¸Šç”Ÿæˆäº†æ˜Žæ˜¾æ›´å¤šæ ·åŒ–çš„è½¨è¿¹è¯­æ–™åº“ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä¸ºåŒä¸€ä»»åŠ¡å­¦ä¹ äº†å¤šç§ä¸åŒçš„ã€é«˜æˆåŠŸçš„ç­–ç•¥ï¼Œè€Œæ ‡å‡†RLåªå‘çŽ°ä¸€ç§ï¼Œå› æ­¤å®ƒè¦†ç›–äº†çŠ¶æ€-åŠ¨ä½œç©ºé—´ä¸­æ›´å¹¿æ³›çš„åŒºåŸŸã€‚å½“åº”ç”¨äºŽæœªè§è¿‡çš„ä¸‹æ¸¸ä»»åŠ¡å¥—ä»¶æ—¶ï¼Œç”¨æˆ‘ä»¬å¤šæ ·åŒ–çš„RLæ•°æ®é¢„è®­ç»ƒçš„VLAæ¨¡åž‹è¶…è¿‡äº†ç”¨ç­‰å¤§å°çš„æ ‡å‡†RLæ•°æ®é›†è®­ç»ƒçš„åŒç±»æ¨¡åž‹ã€‚æ­¤å¤–ï¼ŒDLRè¡¨çŽ°å‡ºå•æ¨¡å¼RLæ‰€ç¼ºä¹çš„ç§¯æžçš„æ•°æ®ç¼©æ”¾è¡Œä¸ºã€‚è¿™äº›ç»“æžœå°†å¤šæ¨¡å¼RLå®šä½ä¸ºå…·èº«åŸºç¡€æ¨¡åž‹çš„å®žç”¨ã€å¯æ‰©å±•çš„æ•°æ®å¼•æ“Žã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰VLAæ¨¡åž‹é¢„è®­ç»ƒä¾èµ–äºŽäººå·¥é¥æ“ä½œæ•°æ®ï¼Œè¿™ç§æ–¹å¼æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ï¼ŒåŒæ—¶æ•°æ®å¤šæ ·æ€§ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶å¯ä»¥è‡ªä¸»ç”Ÿæˆæ•°æ®ï¼Œä½†å®¹æ˜“æ”¶æ•›åˆ°å•ä¸€çš„ç­–ç•¥æ¨¡å¼ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ•°æ®ç¼ºä¹å¤šæ ·æ€§ï¼Œæ— æ³•æ»¡è¶³å¤§è§„æ¨¡é¢„è®­ç»ƒçš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDLRçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¿¡æ¯è®ºçš„æ–¹æ³•ï¼Œé¼“åŠ±å¼ºåŒ–å­¦ä¹ æŽ¢ç´¢å¹¶å­¦ä¹ å¤šç§ä¸åŒçš„æˆåŠŸç­–ç•¥ã€‚é€šè¿‡æœ€å¤§åŒ–ä¸åŒç­–ç•¥ä¹‹é—´çš„äº’ä¿¡æ¯ï¼ŒDLRèƒ½å¤Ÿé¿å…ç­–ç•¥åå¡Œï¼Œç”Ÿæˆè¦†ç›–æ›´å¹¿çŠ¶æ€-åŠ¨ä½œç©ºé—´çš„å¤šæ ·åŒ–è½¨è¿¹æ•°æ®ã€‚è¿™ç§å¤šæ ·åŒ–çš„æ•°æ®èƒ½å¤Ÿæå‡VLAæ¨¡åž‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šDLRæ¡†æž¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šDiscoverã€Learnå’ŒReinforceã€‚åœ¨Discoveré˜¶æ®µï¼Œåˆ©ç”¨ä¿¡æ¯è®ºæ–¹æ³•å‘çŽ°æ½œåœ¨çš„å¤šç§è¡Œä¸ºæ¨¡å¼ã€‚åœ¨Learné˜¶æ®µï¼Œé’ˆå¯¹æ¯ç§è¡Œä¸ºæ¨¡å¼ï¼Œè®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚åœ¨Reinforceé˜¶æ®µï¼Œé€šè¿‡è”åˆè®­ç»ƒæ‰€æœ‰ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æå‡æ•´ä½“æ€§èƒ½å’Œå¤šæ ·æ€§ã€‚æ•´ä½“æµç¨‹æ—¨åœ¨ç”Ÿæˆæ—¢å…·æœ‰é«˜æˆåŠŸçŽ‡åˆå…·æœ‰é«˜åº¦å¤šæ ·æ€§çš„è½¨è¿¹æ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šDLRçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶ä¿¡æ¯è®ºé©±åŠ¨çš„æ¨¡å¼å‘çŽ°æœºåˆ¶ã€‚ä¸Žä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒDLRä¸æ˜¯ç®€å•åœ°è¿½æ±‚å•ä¸€çš„æœ€ä¼˜ç­–ç•¥ï¼Œè€Œæ˜¯é¼“åŠ±æŽ¢ç´¢å¤šç§ä¸åŒçš„ç­–ç•¥ï¼Œä»Žè€Œç”Ÿæˆæ›´å…·å¤šæ ·æ€§çš„æ•°æ®ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆé¿å…ç­–ç•¥åå¡Œï¼Œå¹¶æå‡VLAæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šDLRä½¿ç”¨äº’ä¿¡æ¯æœ€å¤§åŒ–ä½œä¸ºå¥–åŠ±å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œé¼“åŠ±ä¸åŒç­–ç•¥ä¹‹é—´çš„å·®å¼‚æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒDLRä½¿ç”¨å˜åˆ†ä¿¡æ¯ç“¶é¢ˆ(VIB)æ¥ä¼°è®¡ç­–ç•¥ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä½œä¸ºæ­£åˆ™åŒ–é¡¹åŠ å…¥åˆ°å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°ä¸­ã€‚æ­¤å¤–ï¼ŒDLRè¿˜é‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥å¢žåŠ ä»»åŠ¡çš„éš¾åº¦ï¼Œä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆçŽ‡ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„çš„é€‰æ‹©å–å†³äºŽå…·ä½“çš„ä»»åŠ¡å’ŒçŽ¯å¢ƒã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒDLRåœ¨LIBEROæ•°æ®é›†ä¸Šèƒ½å¤Ÿç”Ÿæˆæ¯”æ ‡å‡†RLæ–¹æ³•æ›´å¤šæ ·åŒ–çš„è½¨è¿¹æ•°æ®ã€‚ä½¿ç”¨DLRç”Ÿæˆçš„æ•°æ®é¢„è®­ç»ƒçš„VLAæ¨¡åž‹ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºŽä½¿ç”¨ç­‰é‡æ ‡å‡†RLæ•°æ®é¢„è®­ç»ƒçš„æ¨¡åž‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼ŒDLRé¢„è®­ç»ƒçš„æ¨¡åž‹æ¯”æ ‡å‡†RLé¢„è®­ç»ƒçš„æ¨¡åž‹æå‡äº†10%çš„æˆåŠŸçŽ‡ã€‚æ­¤å¤–ï¼ŒDLRè¿˜å±•çŽ°å‡ºè‰¯å¥½çš„æ•°æ®æ‰©å±•æ€§ï¼Œéšç€æ•°æ®é‡çš„å¢žåŠ ï¼Œæ¨¡åž‹æ€§èƒ½æŒç»­æå‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

DLRæ¡†æž¶ç”Ÿæˆçš„è½¨è¿¹æ•°æ®å¯ç”¨äºŽé¢„è®­ç»ƒå„ç§å…·èº«æ™ºèƒ½æ¨¡åž‹ï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æå‡æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼ŒDLRå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„çŽ¯å¢ƒï¼Œå®Œæˆå„ç§ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºŽæ¸¸æˆAIã€è™šæ‹ŸçŽ°å®žç­‰é¢†åŸŸï¼Œæå‡æ™ºèƒ½ä½“çš„è¡Œä¸ºå¤šæ ·æ€§å’Œæ™ºèƒ½æ°´å¹³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.

