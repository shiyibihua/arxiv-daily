---
layout: default
title: Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories
---

# Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.19528" target="_blank" class="toolbar-btn">arXiv: 2511.19528v1</a>
    <a href="https://arxiv.org/pdf/2511.19528.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19528v1" 
            onclick="toggleFavorite(this, '2511.19528v1', 'Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Rushuai Yang, Zhiyuan Feng, Tianxiang Zhang, Kaixin Wang, Chuheng Zhang, Li Zhao, Xiu Su, Yi Chen, Jiang Bian

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-24

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DLRÊ°ÜÊû∂ÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÁîüÊàêÂ§öÊ†∑ÂåñËΩ®ËøπÔºåÊèêÂçáVLAÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉÊïàÊûú„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÈ¢ÑËÆ≠ÁªÉ` `Âº∫ÂåñÂ≠¶‰π†` `‰ø°ÊÅØËÆ∫` `Ê®°ÂºèÂèëÁé∞` `ÂÖ∑Ë∫´Êô∫ËÉΩ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ‰æùËµñ‰∫∫Â∑•ÈÅ•Êìç‰ΩúÊï∞ÊçÆÔºåÊàêÊú¨È´òÊòÇ‰∏îÈöæ‰ª•Êâ©Â±ïÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÊÄßËÉΩ„ÄÇ
2. DLRÊ°ÜÊû∂ÈÄöËøá‰ø°ÊÅØËÆ∫Ê®°ÂºèÂèëÁé∞ÔºåÈºìÂä±Âº∫ÂåñÂ≠¶‰π†Êé¢Á¥¢Â§öÁßç‰∏çÂêåÁöÑÊàêÂäüÁ≠ñÁï•ÔºåÁîüÊàêÂ§öÊ†∑ÂåñËΩ®Ëøπ„ÄÇ
3. ÂÆûÈ™åËØÅÊòéÔºåDLRÁîüÊàêÁöÑÊï∞ÊçÆËÉΩÊòæËëóÊèêÂçáVLAÊ®°ÂûãÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÔºåÂπ∂Â±ïÁé∞Âá∫ËâØÂ•ΩÁöÑÊï∞ÊçÆÊâ©Â±ïÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËßÑÊ®°ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÈ¢ÑËÆ≠ÁªÉÈúÄË¶ÅÂ§ßÈáèÂ§öÊ†∑Âåñ„ÄÅÈ´òË¥®ÈáèÁöÑÊìç‰ΩúËΩ®Ëøπ„ÄÇÁõÆÂâçÁöÑÊï∞ÊçÆ‰∏ªË¶ÅÈÄöËøá‰∫∫Â∑•ÈÅ•Êìç‰ΩúËé∑ÂæóÔºåÊàêÊú¨È´ò‰∏îÈöæ‰ª•Êâ©Â±ï„ÄÇÂº∫ÂåñÂ≠¶‰π†(RL)ÊñπÊ≥ïÈÄöËøáËá™‰∏ªÊé¢Á¥¢Â≠¶‰π†ÊúâÁî®ÁöÑÊäÄËÉΩÔºå‰ΩøÂÖ∂Êàê‰∏∫ÁîüÊàêÊï∞ÊçÆÁöÑÂèØË°åÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÊ†áÂáÜÁöÑRLËÆ≠ÁªÉ‰ºöÊî∂ÊïõÂà∞Áã≠Á™ÑÁöÑÊâßË°åÊ®°ÂºèÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉ‰∏≠ÁöÑÊïàÁî®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜDiscover, Learn and Reinforce (DLR)Ôºå‰∏Ä‰∏™‰ø°ÊÅØËÆ∫Ê®°ÂºèÂèëÁé∞Ê°ÜÊû∂Ôºå‰∏∫VLAÈ¢ÑËÆ≠ÁªÉÁîüÊàêÂ§ö‰∏™‰∏çÂêåÁöÑ„ÄÅÈ´òÊàêÂäüÁöÑË°å‰∏∫Ê®°Âºè„ÄÇÂÆûÈ™åË°®ÊòéÔºåDLRÂú®LIBERO‰∏äÁîüÊàê‰∫ÜÊòéÊòæÊõ¥Â§öÊ†∑ÂåñÁöÑËΩ®ËøπËØ≠ÊñôÂ∫ì„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉ‰∏∫Âêå‰∏Ä‰ªªÂä°Â≠¶‰π†‰∫ÜÂ§öÁßç‰∏çÂêåÁöÑ„ÄÅÈ´òÊàêÂäüÁöÑÁ≠ñÁï•ÔºåËÄåÊ†áÂáÜRLÂè™ÂèëÁé∞‰∏ÄÁßçÔºåÂõ†Ê≠§ÂÆÉË¶ÜÁõñ‰∫ÜÁä∂ÊÄÅ-Âä®‰ΩúÁ©∫Èó¥‰∏≠Êõ¥ÂπøÊ≥õÁöÑÂå∫Âüü„ÄÇÂΩìÂ∫îÁî®‰∫éÊú™ËßÅËøáÁöÑ‰∏ãÊ∏∏‰ªªÂä°Â•ó‰ª∂Êó∂ÔºåÁî®Êàë‰ª¨Â§öÊ†∑ÂåñÁöÑRLÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉÁöÑVLAÊ®°ÂûãË∂ÖËøá‰∫ÜÁî®Á≠âÂ§ßÂ∞èÁöÑÊ†áÂáÜRLÊï∞ÊçÆÈõÜËÆ≠ÁªÉÁöÑÂêåÁ±ªÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåDLRË°®Áé∞Âá∫ÂçïÊ®°ÂºèRLÊâÄÁº∫‰πèÁöÑÁßØÊûÅÁöÑÊï∞ÊçÆÁº©ÊîæË°å‰∏∫„ÄÇËøô‰∫õÁªìÊûúÂ∞ÜÂ§öÊ®°ÂºèRLÂÆö‰Ωç‰∏∫ÂÖ∑Ë∫´Âü∫Á°ÄÊ®°ÂûãÁöÑÂÆûÁî®„ÄÅÂèØÊâ©Â±ïÁöÑÊï∞ÊçÆÂºïÊìé„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâVLAÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ‰æùËµñ‰∫é‰∫∫Â∑•ÈÅ•Êìç‰ΩúÊï∞ÊçÆÔºåËøôÁßçÊñπÂºèÊàêÊú¨È´òÊòÇ‰∏îÈöæ‰ª•Êâ©Â±ïÔºåÂêåÊó∂Êï∞ÊçÆÂ§öÊ†∑ÊÄß‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ†áÂáÜÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïËôΩÁÑ∂ÂèØ‰ª•Ëá™‰∏ªÁîüÊàêÊï∞ÊçÆÔºå‰ΩÜÂÆπÊòìÊî∂ÊïõÂà∞Âçï‰∏ÄÁöÑÁ≠ñÁï•Ê®°ÂºèÔºåÂØºËá¥ÁîüÊàêÁöÑÊï∞ÊçÆÁº∫‰πèÂ§öÊ†∑ÊÄßÔºåÊó†Ê≥ïÊª°Ë∂≥Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÁöÑÈúÄÊ±Ç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDLRÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøá‰ø°ÊÅØËÆ∫ÁöÑÊñπÊ≥ïÔºåÈºìÂä±Âº∫ÂåñÂ≠¶‰π†Êé¢Á¥¢Âπ∂Â≠¶‰π†Â§öÁßç‰∏çÂêåÁöÑÊàêÂäüÁ≠ñÁï•„ÄÇÈÄöËøáÊúÄÂ§ßÂåñ‰∏çÂêåÁ≠ñÁï•‰πãÈó¥ÁöÑ‰∫í‰ø°ÊÅØÔºåDLRËÉΩÂ§üÈÅøÂÖçÁ≠ñÁï•ÂùçÂ°åÔºåÁîüÊàêË¶ÜÁõñÊõ¥ÂπøÁä∂ÊÄÅ-Âä®‰ΩúÁ©∫Èó¥ÁöÑÂ§öÊ†∑ÂåñËΩ®ËøπÊï∞ÊçÆ„ÄÇËøôÁßçÂ§öÊ†∑ÂåñÁöÑÊï∞ÊçÆËÉΩÂ§üÊèêÂçáVLAÊ®°ÂûãÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏äÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDLRÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöDiscover„ÄÅLearnÂíåReinforce„ÄÇÂú®DiscoverÈò∂ÊÆµÔºåÂà©Áî®‰ø°ÊÅØËÆ∫ÊñπÊ≥ïÂèëÁé∞ÊΩúÂú®ÁöÑÂ§öÁßçË°å‰∏∫Ê®°Âºè„ÄÇÂú®LearnÈò∂ÊÆµÔºåÈíàÂØπÊØèÁßçË°å‰∏∫Ê®°ÂºèÔºåËÆ≠ÁªÉ‰∏Ä‰∏™Áã¨Á´ãÁöÑÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•„ÄÇÂú®ReinforceÈò∂ÊÆµÔºåÈÄöËøáËÅîÂêàËÆ≠ÁªÉÊâÄÊúâÁ≠ñÁï•ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÊï¥‰ΩìÊÄßËÉΩÂíåÂ§öÊ†∑ÊÄß„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊó®Âú®ÁîüÊàêÊó¢ÂÖ∑ÊúâÈ´òÊàêÂäüÁéáÂèàÂÖ∑ÊúâÈ´òÂ∫¶Â§öÊ†∑ÊÄßÁöÑËΩ®ËøπÊï∞ÊçÆ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDLRÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂‰ø°ÊÅØËÆ∫È©±Âä®ÁöÑÊ®°ÂºèÂèëÁé∞Êú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰∏çÂêåÔºåDLR‰∏çÊòØÁÆÄÂçïÂú∞ËøΩÊ±ÇÂçï‰∏ÄÁöÑÊúÄ‰ºòÁ≠ñÁï•ÔºåËÄåÊòØÈºìÂä±Êé¢Á¥¢Â§öÁßç‰∏çÂêåÁöÑÁ≠ñÁï•Ôºå‰ªéËÄåÁîüÊàêÊõ¥ÂÖ∑Â§öÊ†∑ÊÄßÁöÑÊï∞ÊçÆ„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÈÅøÂÖçÁ≠ñÁï•ÂùçÂ°åÔºåÂπ∂ÊèêÂçáVLAÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDLR‰ΩøÁî®‰∫í‰ø°ÊÅØÊúÄÂ§ßÂåñ‰Ωú‰∏∫Â•ñÂä±ÂáΩÊï∞ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÈºìÂä±‰∏çÂêåÁ≠ñÁï•‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊÄß„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåDLR‰ΩøÁî®ÂèòÂàÜ‰ø°ÊÅØÁì∂È¢à(VIB)Êù•‰º∞ËÆ°Á≠ñÁï•‰πãÈó¥ÁöÑ‰∫í‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫Ê≠£ÂàôÂåñÈ°πÂä†ÂÖ•Âà∞Âº∫ÂåñÂ≠¶‰π†ÁöÑÂ•ñÂä±ÂáΩÊï∞‰∏≠„ÄÇÊ≠§Â§ñÔºåDLRËøòÈááÁî®‰∫ÜËØæÁ®ãÂ≠¶‰π†Á≠ñÁï•ÔºåÈÄêÊ≠•Â¢ûÂä†‰ªªÂä°ÁöÑÈöæÂ∫¶Ôºå‰ª•ÊèêÈ´òËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÊïàÁéá„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁöÑÈÄâÊã©ÂèñÂÜ≥‰∫éÂÖ∑‰ΩìÁöÑ‰ªªÂä°ÂíåÁéØÂ¢É„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDLRÂú®LIBEROÊï∞ÊçÆÈõÜ‰∏äËÉΩÂ§üÁîüÊàêÊØîÊ†áÂáÜRLÊñπÊ≥ïÊõ¥Â§öÊ†∑ÂåñÁöÑËΩ®ËøπÊï∞ÊçÆ„ÄÇ‰ΩøÁî®DLRÁîüÊàêÁöÑÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉÁöÑVLAÊ®°ÂûãÔºåÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÊòæËëó‰ºò‰∫é‰ΩøÁî®Á≠âÈáèÊ†áÂáÜRLÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏äÔºåDLRÈ¢ÑËÆ≠ÁªÉÁöÑÊ®°ÂûãÊØîÊ†áÂáÜRLÈ¢ÑËÆ≠ÁªÉÁöÑÊ®°ÂûãÊèêÂçá‰∫Ü10%ÁöÑÊàêÂäüÁéá„ÄÇÊ≠§Â§ñÔºåDLRËøòÂ±ïÁé∞Âá∫ËâØÂ•ΩÁöÑÊï∞ÊçÆÊâ©Â±ïÊÄßÔºåÈöèÁùÄÊï∞ÊçÆÈáèÁöÑÂ¢ûÂä†ÔºåÊ®°ÂûãÊÄßËÉΩÊåÅÁª≠ÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DLRÊ°ÜÊû∂ÁîüÊàêÁöÑËΩ®ËøπÊï∞ÊçÆÂèØÁî®‰∫éÈ¢ÑËÆ≠ÁªÉÂêÑÁßçÂÖ∑Ë∫´Êô∫ËÉΩÊ®°ÂûãÔºå‰æãÂ¶ÇÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®È©æÈ©∂Á≠â„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄßÔºåDLRÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÂ§öÂèòÁöÑÁéØÂ¢ÉÔºåÂÆåÊàêÂêÑÁßç‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éÊ∏∏ÊàèAI„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüüÔºåÊèêÂçáÊô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫Â§öÊ†∑ÊÄßÂíåÊô∫ËÉΩÊ∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.

