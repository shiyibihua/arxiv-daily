---
layout: default
title: First-order Sobolev Reinforcement Learning
---

# First-order Sobolev Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.19165" target="_blank" class="toolbar-btn">arXiv: 2511.19165v1</a>
    <a href="https://arxiv.org/pdf/2511.19165.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19165v1" 
            onclick="toggleFavorite(this, '2511.19165v1', 'First-order Sobolev Reinforcement Learning')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Fabian Schramm, Nicolas Perrin-Gilbert, Justin Carpentier

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

**å¤‡æ³¨**: Workshop paper at Differentiable Systems and Scientific Machine Learning, EurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€é˜¶Sobolevå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡æ¢¯åº¦ä¸€è‡´æ€§åŠ é€Ÿcriticæ”¶æ•›å¹¶ç¨³å®šç­–ç•¥æ¢¯åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ—¶åºå·®åˆ†å­¦ä¹ ` `è´å°”æ›¼æ–¹ç¨‹` `æ¢¯åº¦ä¸€è‡´æ€§` `Sobolevç©ºé—´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä»·å€¼å‡½æ•°å­¦ä¹ ä¸­æ”¶æ•›é€Ÿåº¦æ…¢ï¼Œç­–ç•¥æ¢¯åº¦ä¸ç¨³å®šï¼Œå½±å“æ•´ä½“æ€§èƒ½ã€‚
2. è®ºæ–‡æå‡ºä¸€é˜¶Sobolevå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡åŒ¹é…è´å°”æ›¼ç›®æ ‡çš„ä»·å€¼å’Œå¯¼æ•°ï¼Œå®ç°ä¸€é˜¶è´å°”æ›¼ä¸€è‡´æ€§ã€‚
3. è¯¥æ–¹æ³•å¯é›†æˆåˆ°ç°æœ‰ç®—æ³•ä¸­ï¼ŒåŠ é€Ÿcriticæ”¶æ•›ï¼Œç¨³å®šç­–ç•¥æ¢¯åº¦ï¼Œä¸”ä¸æ”¹å˜ç®—æ³•æ•´ä½“ç»“æ„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ—¶åºå·®åˆ†å­¦ä¹ çš„æ”¹è¿›æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¼ºåˆ¶æ‰§è¡Œä¸€é˜¶è´å°”æ›¼ä¸€è‡´æ€§ï¼šå­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°ä¸ä»…åœ¨ä»·å€¼ä¸Šä¸è´å°”æ›¼ç›®æ ‡åŒ¹é…ï¼Œè€Œä¸”åœ¨å…³äºçŠ¶æ€å’ŒåŠ¨ä½œçš„å¯¼æ•°ä¸Šä¹Ÿä¸ä¹‹åŒ¹é…ã€‚é€šè¿‡å¯å¾®åŠ¨åŠ›å­¦å¯¹è´å°”æ›¼å¤‡ä»½è¿›è¡Œå¾®åˆ†ï¼Œæˆ‘ä»¬è·å¾—äº†åˆ†æä¸Šä¸€è‡´çš„æ¢¯åº¦ç›®æ ‡ã€‚å°†è¿™äº›æ¢¯åº¦ç›®æ ‡ä½¿ç”¨Sobolevå‹æŸå¤±å‡½æ•°æ•´åˆåˆ°criticç›®æ ‡ä¸­ï¼Œé¼“åŠ±criticä¸ç›®æ ‡å‡½æ•°çš„ä»·å€¼å’Œå±€éƒ¨å‡ ä½•ç»“æ„å¯¹é½ã€‚è¿™ç§ä¸€é˜¶TDåŒ¹é…åŸåˆ™å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„ç®—æ³•ä¸­ï¼Œä¾‹å¦‚Q-learningæˆ–actor-criticæ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒDDPGï¼ŒSACï¼‰ï¼Œå¯èƒ½å¯¼è‡´æ›´å¿«çš„criticæ”¶æ•›å’Œæ›´ç¨³å®šçš„ç­–ç•¥æ¢¯åº¦ï¼Œè€Œä¸ä¼šæ”¹å˜å®ƒä»¬çš„æ•´ä½“ç»“æ„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä»·å€¼å‡½æ•°çš„å‡†ç¡®ä¼°è®¡è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•ï¼Œå¦‚æ—¶åºå·®åˆ†å­¦ä¹ ï¼Œé€šå¸¸åªå…³æ³¨ä»·å€¼çš„åŒ¹é…ï¼Œå¿½ç•¥äº†ä»·å€¼å‡½æ•°å±€éƒ¨å‡ ä½•ç»“æ„çš„ä¿¡æ¯ã€‚è¿™å¯¼è‡´criticå­¦ä¹ ç¼“æ…¢ï¼Œç­–ç•¥æ¢¯åº¦ä¼°è®¡æ–¹å·®å¤§ï¼Œè®­ç»ƒä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼ºåˆ¶ä¸€é˜¶è´å°”æ›¼ä¸€è‡´æ€§ï¼Œå³ä¸ä»…è¦æ±‚å­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°ä¸è´å°”æ›¼ç›®æ ‡åœ¨ä»·å€¼ä¸ŠåŒ¹é…ï¼Œè¿˜è¦æ±‚å®ƒä»¬çš„å¯¼æ•°ï¼ˆå…³äºçŠ¶æ€å’ŒåŠ¨ä½œï¼‰ä¹ŸåŒ¹é…ã€‚é€šè¿‡å¼•å…¥æ¢¯åº¦ä¿¡æ¯ï¼Œå¯ä»¥æ›´å¥½åœ°çº¦æŸä»·å€¼å‡½æ•°çš„å­¦ä¹ ï¼Œä½¿å…¶æ›´å‡†ç¡®ã€æ›´ç¨³å®šã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶æ˜¯åœ¨ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚DDPGã€SACï¼‰çš„criticå­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå¢åŠ ä¸€ä¸ªSobolevå‹æŸå¤±å‡½æ•°ã€‚è¯¥æŸå¤±å‡½æ•°ä¸ä»…åŒ…å«ä»·å€¼çš„è¯¯å·®é¡¹ï¼Œè¿˜åŒ…å«ä»·å€¼å‡½æ•°æ¢¯åº¦ä¸è´å°”æ›¼ç›®æ ‡æ¢¯åº¦ä¹‹é—´çš„è¯¯å·®é¡¹ã€‚è´å°”æ›¼ç›®æ ‡æ¢¯åº¦é€šè¿‡å¯å¾®åŠ¨åŠ›å­¦è®¡ç®—å¾—åˆ°ã€‚å› æ­¤ï¼Œcriticçš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–ä»·å€¼è¯¯å·®å’Œæ¢¯åº¦è¯¯å·®çš„åŠ æƒå’Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†ä¸€é˜¶å¯¼æ•°ä¿¡æ¯å¼•å…¥åˆ°ä»·å€¼å‡½æ•°çš„å­¦ä¹ ä¸­ï¼Œä»è€Œå®ç°äº†å¯¹è´å°”æ›¼æ–¹ç¨‹æ›´å¼ºçš„çº¦æŸã€‚ä¸ä¼ ç»Ÿæ–¹æ³•åªå…³æ³¨ä»·å€¼åŒ¹é…ä¸åŒï¼Œè¯¥æ–¹æ³•åŒæ—¶å…³æ³¨ä»·å€¼å’Œæ¢¯åº¦çš„åŒ¹é…ï¼Œä½¿å¾—å­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°æ›´åŠ å‡†ç¡®ï¼Œç­–ç•¥æ¢¯åº¦æ›´åŠ ç¨³å®šã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¯å¾®åŠ¨åŠ›å­¦è®¡ç®—è´å°”æ›¼ç›®æ ‡æ¢¯åº¦ï¼›2) ä½¿ç”¨Sobolevå‹æŸå¤±å‡½æ•°ï¼Œå¹³è¡¡ä»·å€¼è¯¯å·®å’Œæ¢¯åº¦è¯¯å·®ï¼›3) æŸå¤±å‡½æ•°ä¸­ä»·å€¼è¯¯å·®å’Œæ¢¯åº¦è¯¯å·®çš„æƒé‡éœ€è¦ä»”ç»†è°ƒæ•´ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ä¸åŸç®—æ³•ä¿æŒä¸€è‡´ï¼Œæ— éœ€ä¿®æ”¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œä¾‹å¦‚DDPGå’ŒSACã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜criticçš„æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶é™ä½ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œéœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ï¼Œå¯ä»¥æ›´å¿«åœ°è®­ç»ƒå‡ºé«˜æ€§èƒ½çš„æ™ºèƒ½ä½“ï¼Œè§£å†³å®é™…åº”ç”¨ä¸­é¢ä¸´çš„å¤æ‚æ§åˆ¶é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åº”ç”¨äºè®­ç»ƒæ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼Œæˆ–è®­ç»ƒæ›´æ™ºèƒ½çš„æœºå™¨äººå®Œæˆå¤æ‚çš„æ“ä½œä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.

