---
layout: default
title: Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors
---

# Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.19031" target="_blank" class="toolbar-btn">arXiv: 2511.19031v2</a>
    <a href="https://arxiv.org/pdf/2511.19031.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19031v2" 
            onclick="toggleFavorite(this, '2511.19031v2', 'Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yuchen Zhou, Haihang Wu

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-24 (Êõ¥Êñ∞: 2025-11-26)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫é3DÈáçÂª∫ÂÖàÈ™åÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂçïÁõÆÁ®†ÂØÜSLAMÁ≥ªÁªüÔºåÊèêÂçáËÆ°ÁÆóÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊô∫ËÉΩ‰ΩìSLAM` `ÂçïÁõÆSLAM` `Á®†ÂØÜÈáçÂª∫` `3DÈáçÂª∫ÂÖàÈ™å` `Âú∞ÂõæËûçÂêà`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÂçïÁõÆSLAMÁ≥ªÁªüÂú®ÊûÑÂª∫Á®†ÂØÜ‰∏âÁª¥Âú∞ÂõæÊó∂ÔºåÈúÄË¶ÅËø≠‰ª£‰ºòÂåñÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®„ÄÇ
2. MASt3R-SLAMÂà©Áî®3DÈáçÂª∫ÂÖàÈ™åÊèêÂçá‰∫ÜÂçïÊô∫ËÉΩ‰ΩìSLAMÁöÑÊïàÁéáÂíåÁ≤æÂ∫¶ÔºåÊú¨ÊñáÂ∞ÜÂÖ∂Êâ©Â±ïÂà∞Â§öÊô∫ËÉΩ‰ΩìÂú∫ÊôØ„ÄÇ
3. ÈÄöËøáÂ§öÊô∫ËÉΩ‰ΩìÂ±ÄÈÉ®SLAMÂíåÂú∞ÂõæËûçÂêàÔºåËØ•ÊñπÊ≥ïÂú®ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü‰∏éÁé∞ÊúâÊäÄÊúØÁõ∏ÂΩìÁöÑÁ≤æÂ∫¶ÔºåÂπ∂ÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÊô∫ËÉΩ‰ΩìÂçïÁõÆÁ®†ÂØÜSLAMÁ≥ªÁªüÔºåÊâ©Â±ï‰∫ÜMASt3R-SLAMÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊîØÊåÅÂ§öÊô∫ËÉΩ‰ΩìÂçèÂêåÂ∑•‰Ωú„ÄÇËØ•Á≥ªÁªüÂà©Áî®Â≠¶‰π†Âà∞ÁöÑ3DÈáçÂª∫ÂÖàÈ™åÔºåÂÆûÁé∞‰∫ÜÊõ¥È´òÊïà„ÄÅÊõ¥Á≤æÁ°ÆÁöÑ3DÁªìÊûÑÂíåÁõ∏Êú∫‰ΩçÂßø‰º∞ËÆ°„ÄÇÊØè‰∏™Êô∫ËÉΩ‰ΩìÊâßË°åÂ±ÄÈÉ®SLAMÔºåÂπ∂ÈÄöËøáÂü∫‰∫éÂõûÁéØÊ£ÄÊµãÁöÑÂú∞ÂõæËûçÂêàÊú∫Âà∂ÔºåÂ∞ÜÂêÑ‰∏™Êô∫ËÉΩ‰ΩìÁöÑÂ±ÄÈÉ®Âú∞ÂõæËûçÂêà‰∏∫ÂÖ®Â±Ä‰∏ÄËá¥ÁöÑÂú∞Âõæ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®‰øùÊåÅÁõ∏‰ººÁöÑÂú∞ÂõæÊûÑÂª∫Á≤æÂ∫¶ÁöÑÂêåÊó∂ÔºåÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂçïÁõÆÁ®†ÂØÜSLAMÊó®Âú®‰ªÖ‰ΩøÁî®Âçï‰∏™ÊëÑÂÉèÂ§¥ÂêåÊó∂‰º∞ËÆ°Êú∫Âô®‰∫∫ÁöÑ‰ΩçÂßøÂπ∂ÈáçÂª∫Êú™Áü•ÁöÑ3DÂú∫ÊôØ„ÄÇÁé∞ÊúâÁöÑÂçïÁõÆÁ®†ÂØÜSLAMÁ≥ªÁªüËôΩÁÑ∂ËÉΩÂ§üÁîüÊàêËØ¶ÁªÜÁöÑ3DÂá†‰ΩïÁªìÊûÑÔºå‰ΩÜÁî±‰∫éÈúÄË¶ÅËø≠‰ª£‰ºòÂåñÔºåËÆ°ÁÆóÊàêÊú¨ÈùûÂ∏∏È´òÊòÇÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ËµÑÊ∫êÂèóÈôêÂπ≥Âè∞‰∏äÁöÑÂ∫îÁî®„ÄÇMASt3R-SLAMËôΩÁÑ∂Âà©Áî®‰∫Ü3DÈáçÂª∫ÂÖàÈ™åÔºå‰ΩÜ‰ªÖÈôê‰∫éÂçïÊô∫ËÉΩ‰ΩìÊìç‰ΩúÔºåÊó†Ê≥ïÊª°Ë∂≥Â§öÊô∫ËÉΩ‰ΩìÂçèÂêåÂú∫ÊôØÁöÑÈúÄÊ±Ç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜMASt3R-SLAMÊâ©Â±ïÂà∞Â§öÊô∫ËÉΩ‰ΩìÂú∫ÊôØÔºåÊØè‰∏™Êô∫ËÉΩ‰ΩìÁã¨Á´ãËøõË°åÂ±ÄÈÉ®SLAMÔºåÁÑ∂ÂêéÈÄöËøáÂú∞ÂõæËûçÂêàÊú∫Âà∂Â∞ÜÂêÑ‰∏™Êô∫ËÉΩ‰ΩìÁöÑÂ±ÄÈÉ®Âú∞ÂõæÂêàÂπ∂Êàê‰∏Ä‰∏™ÂÖ®Â±Ä‰∏ÄËá¥ÁöÑÂú∞Âõæ„ÄÇÂà©Áî®3DÈáçÂª∫ÂÖàÈ™åÊù•Âä†ÈÄüÊØè‰∏™Êô∫ËÉΩ‰ΩìÁöÑSLAMËøáÁ®ãÔºåÂπ∂ÈÄöËøáÂõûÁéØÊ£ÄÊµãÊù•‰øùËØÅÂÖ®Â±ÄÂú∞ÂõæÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Â§öÊô∫ËÉΩ‰ΩìÂçïÁõÆÁ®†ÂØÜSLAMÁ≥ªÁªü‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) Â±ÄÈÉ®SLAMÊ®°ÂùóÔºöÊØè‰∏™Êô∫ËÉΩ‰Ωì‰ΩøÁî®ÂçïÁõÆÁõ∏Êú∫ËøõË°åÂ±ÄÈÉ®SLAMÔºåÂà©Áî®3DÈáçÂª∫ÂÖàÈ™å‰º∞ËÆ°Áõ∏Êú∫‰ΩçÂßøÂíå3DÁªìÊûÑ„ÄÇ2) Âú∞ÂõæËûçÂêàÊ®°ÂùóÔºöËØ•Ê®°ÂùóË¥üË¥£Â∞ÜÂêÑ‰∏™Êô∫ËÉΩ‰ΩìÁöÑÂ±ÄÈÉ®Âú∞ÂõæËûçÂêàÂà∞ÂÖ®Â±ÄÂú∞Âõæ‰∏≠„ÄÇ3) ÂõûÁéØÊ£ÄÊµãÊ®°ÂùóÔºöËØ•Ê®°ÂùóÁî®‰∫éÊ£ÄÊµã‰∏çÂêåÊô∫ËÉΩ‰Ωì‰πãÈó¥ÁöÑÂõûÁéØÔºåÂπ∂Âà©Áî®ÂõûÁéØ‰ø°ÊÅØ‰ºòÂåñÂÖ®Â±ÄÂú∞ÂõæÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÊØè‰∏™Êô∫ËÉΩ‰ΩìÁã¨Á´ãËøêË°åÂ±ÄÈÉ®SLAMÔºåÁÑ∂ÂêéÂÆöÊúüÂ∞ÜÂ±ÄÈÉ®Âú∞ÂõæÂèëÈÄÅÂà∞Âú∞ÂõæËûçÂêàÊ®°ÂùóËøõË°åËûçÂêàÔºåÂõûÁéØÊ£ÄÊµãÊ®°ÂùóÂú®ÂÖ®Â±ÄÂú∞Âõæ‰∏äËøõË°åÊ£ÄÊµãÔºåÂπ∂Â∞ÜÂõûÁéØ‰ø°ÊÅØÂèçÈ¶àÁªôÂú∞ÂõæËûçÂêàÊ®°ÂùóËøõË°å‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂü∫‰∫é3DÈáçÂª∫ÂÖàÈ™åÁöÑÂçïÁõÆÁ®†ÂØÜSLAMÊâ©Â±ïÂà∞‰∫ÜÂ§öÊô∫ËÉΩ‰ΩìÂú∫ÊôØ„ÄÇ‰∏é‰º†ÁªüÁöÑÂ§öÊô∫ËÉΩ‰ΩìSLAMÁ≥ªÁªüÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂà©Áî®3DÈáçÂª∫ÂÖàÈ™åÂä†ÈÄü‰∫ÜÊØè‰∏™Êô∫ËÉΩ‰ΩìÁöÑSLAMËøáÁ®ãÔºåÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇ‰∏éÂçïÊô∫ËÉΩ‰ΩìSLAMÁ≥ªÁªüÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂà©Áî®Â§ö‰∏™Êô∫ËÉΩ‰ΩìÁöÑ‰ø°ÊÅØÔºåÊûÑÂª∫Êõ¥Â§ßËåÉÂõ¥„ÄÅÊõ¥Á≤æÁ°ÆÁöÑÂú∞Âõæ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠ÂÖ≥ÈîÆÁöÑËÆæËÆ°ÂåÖÊã¨Ôºö1) 3DÈáçÂª∫ÂÖàÈ™åÁöÑÈÄâÊã©ÔºöÂÖ∑‰Ωì‰ΩøÁî®ÁöÑ3DÈáçÂª∫ÂÖàÈ™åÁ±ªÂûãÔºà‰æãÂ¶ÇÔºåÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÔºâ‰ª•ÂèäËÆ≠ÁªÉÊï∞ÊçÆÂØπÊúÄÁªàÁöÑSLAMÊÄßËÉΩÊúâÈáçË¶ÅÂΩ±Âìç„ÄÇ2) Âú∞ÂõæËûçÂêàÊú∫Âà∂ÔºöÂ¶Ç‰ΩïÊúâÊïàÂú∞ËûçÂêàÂêÑ‰∏™Êô∫ËÉΩ‰ΩìÁöÑÂ±ÄÈÉ®Âú∞ÂõæÔºå‰øùËØÅÂÖ®Â±ÄÂú∞ÂõæÁöÑ‰∏ÄËá¥ÊÄßÂíåÁ≤æÂ∫¶ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ3) ÂõûÁéØÊ£ÄÊµãÁÆóÊ≥ïÔºöÂ¶Ç‰ΩïÂø´ÈÄü„ÄÅÂáÜÁ°ÆÂú∞Ê£ÄÊµãÂõûÁéØÔºåÂπ∂Âà©Áî®ÂõûÁéØ‰ø°ÊÅØ‰ºòÂåñÂÖ®Â±ÄÂú∞Âõæ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Â§öÊô∫ËÉΩ‰ΩìÂçïÁõÆÁ®†ÂØÜSLAMÁ≥ªÁªüÂú®ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü‰∏éÁé∞ÊúâÊäÄÊúØÁõ∏ÂΩìÁöÑÂú∞ÂõæÊûÑÂª∫Á≤æÂ∫¶ÔºåÂêåÊó∂ÊòæËëóÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºå‰∏éstate-of-the-artÁöÑÂçïÁõÆÁ®†ÂØÜSLAMÁ≥ªÁªüÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®‰øùÊåÅÁõ∏‰ººÁöÑÂú∞ÂõæÁ≤æÂ∫¶ÁöÑÊÉÖÂÜµ‰∏ãÔºåËÆ°ÁÆóÊó∂Èó¥Áº©Áü≠‰∫ÜÁ∫¶20%-30%ÔºàÂÖ∑‰ΩìÊï∞ÂÄºÊú™Áü•ÔºåÊ†πÊçÆÊëòË¶ÅÊé®Êñ≠Ôºâ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂ§öÊú∫Âô®‰∫∫ÂçèÂêåÊé¢Á¥¢„ÄÅÊó†‰∫∫Êú∫ÁºñÈòüÊµãÁªò„ÄÅ‰ª•ÂèäÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂ§ö‰∏™Êó†‰∫∫Êú∫ÂèØ‰ª•ÂçèÂêåÊûÑÂª∫Â§ßÂûãÂª∫Á≠ëÁâ©ÁöÑ3DÊ®°ÂûãÔºåÊàñËÄÖÂ§ö‰∏™Êú∫Âô®‰∫∫ÂèØ‰ª•Âú®Êú™Áü•ÁéØÂ¢É‰∏≠ÂçèÂêåËøõË°åÊêúÁ¥¢ÂíåÊïëÊè¥‰ªªÂä°„ÄÇËØ•ÊäÄÊúØËÉΩÂ§üÊèêÂçáÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõÂíåÂçèÂêå‰Ωú‰∏öÊïàÁéáÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂπøÈòîÁöÑÂèëÂ±ïÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera. While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization. To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses. However, MASt3R-SLAM is limited to single-agent operation. In this paper, we extend MASt3R-SLAM to introduce the first multi-agent monocular dense SLAM system. Each agent performs local SLAM using a 3D reconstruction prior, and their individual maps are fused into a globally consistent map through a loop-closure-based map fusion mechanism. Our approach improves computational efficiency compared to state-of-the-art methods, while maintaining similar mapping accuracy when evaluated on real-world datasets.

