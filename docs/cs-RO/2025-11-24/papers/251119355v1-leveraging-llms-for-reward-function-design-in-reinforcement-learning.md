---
layout: default
title: Leveraging LLMs for reward function design in reinforcement learning control tasks
---

# Leveraging LLMs for reward function design in reinforcement learning control tasks

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.19355" target="_blank" class="toolbar-btn">arXiv: 2511.19355v1</a>
    <a href="https://arxiv.org/pdf/2511.19355.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19355v1" 
            onclick="toggleFavorite(this, '2511.19355v1', 'Leveraging LLMs for reward function design in reinforcement learning control tasks')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Franklin Cardenoso, Wouter Caarls

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-24

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LEARN-OptÔºåÂà©Áî®LLMËá™‰∏ªËÆæËÆ°Âº∫ÂåñÂ≠¶‰π†ÊéßÂà∂‰ªªÂä°ÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÊó†ÈúÄ‰∫∫Â∑•Âπ≤È¢Ñ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Â•ñÂä±ÂáΩÊï∞ËÆæËÆ°` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Ëá™Âä®Âåñ` `Êó†ÁõëÁù£Â≠¶‰π†` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Ëá™‰∏ªÁ≥ªÁªü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂº∫ÂåñÂ≠¶‰π†Â•ñÂä±ÂáΩÊï∞ËÆæËÆ°‰æùËµñ‰∫∫Â∑•ÁªèÈ™åÔºåËÄóÊó∂‰∏îÊòìÂá∫ÈîôÔºåÁº∫‰πèËá™Âä®ÂåñÊñπÊ≥ï„ÄÇ
2. LEARN-OptÂà©Áî®LLM‰ªéÊñáÊú¨ÊèèËø∞‰∏≠Ëá™‰∏ªÁîüÊàê„ÄÅËØÑ‰º∞Âíå‰ºòÂåñÂ•ñÂä±ÂáΩÊï∞ÔºåÊó†ÈúÄ‰∫∫Â∑•Âπ≤È¢Ñ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåLEARN-OptÊÄßËÉΩÂ™≤ÁæéÊàñ‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºå‰∏îËÉΩÂà©Áî®‰ΩéÊàêÊú¨LLMÊâæÂà∞È´òÊÄßËÉΩÂ•ñÂä±ÂáΩÊï∞„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®Âº∫ÂåñÂ≠¶‰π†(RL)‰∏≠ÔºåËÆæËÆ°ÊúâÊïàÁöÑÂ•ñÂä±ÂáΩÊï∞ÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÁì∂È¢àÔºåÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫Â∑•‰∏ì‰∏öÁü•ËØÜÔºåÂπ∂‰∏îÈùûÂ∏∏ËÄóÊó∂„ÄÇÂÖàÂâçÁöÑÂ∑•‰ΩúÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)ÁöÑÊúÄÊñ∞ËøõÂ±ïÂ∑≤ÁªèËØÅÊòé‰∫ÜÂÆÉ‰ª¨Âú®Ëá™Âä®ÁîüÊàêÂ•ñÂä±ÂáΩÊï∞ÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂàùÊ≠•ÁöÑËØÑ‰º∞ÊåáÊ†á„ÄÅ‰∫∫Â∑•ËÆæËÆ°ÁöÑÂèçÈ¶àÊù•ÊîπËøõËøáÁ®ãÔºåÊàñËÄÖ‰ΩøÁî®ÁéØÂ¢ÉÊ∫ê‰ª£Á†Å‰Ωú‰∏∫‰∏ä‰∏ãÊñá„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éLLMÁöÑÂÆåÂÖ®Ëá™‰∏ª‰∏îÊ®°ÂûãÊó†ÂÖ≥ÁöÑÊ°ÜÊû∂LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization)ÔºåÂÆÉÊó†ÈúÄÂàùÊ≠•ÊåáÊ†áÂíåÁéØÂ¢ÉÊ∫ê‰ª£Á†Å‰Ωú‰∏∫‰∏ä‰∏ãÊñáÔºåÂç≥ÂèØ‰ªéÁ≥ªÁªüÂíå‰ªªÂä°ÁõÆÊ†áÁöÑÊñáÊú¨ÊèèËø∞‰∏≠ÁîüÊàê„ÄÅÊâßË°åÂíåËØÑ‰º∞Â•ñÂä±ÂáΩÊï∞ÂÄôÈÄâ„ÄÇLEARN-OptÁöÑ‰∏ªË¶ÅË¥°ÁåÆÂú®‰∫éÂÆÉËÉΩÂ§üËá™‰∏ªÂú∞‰ªéÁ≥ªÁªüÊèèËø∞Âíå‰ªªÂä°ÁõÆÊ†á‰∏≠Êé®ÂØºÂá∫ÊÄßËÉΩÊåáÊ†áÔºå‰ªéËÄåÂÆûÁé∞ÂØπÂ•ñÂä±ÂáΩÊï∞ÁöÑÊó†ÁõëÁù£ËØÑ‰º∞ÂíåÈÄâÊã©„ÄÇÂÆûÈ™åË°®ÊòéÔºåLEARN-OptÁöÑÊÄßËÉΩ‰∏éEUREKAÁ≠âÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÁõ∏ÂΩìÊàñÊõ¥Â•ΩÔºåÂêåÊó∂ÈúÄË¶ÅÊõ¥Â∞ëÁöÑÂÖàÈ™åÁü•ËØÜ„ÄÇÊàë‰ª¨ÂèëÁé∞Ëá™Âä®Â•ñÂä±ËÆæËÆ°ÊòØ‰∏Ä‰∏™È´òÊñπÂ∑ÆÈóÆÈ¢òÔºåÂπ≥ÂùáÊÉÖÂÜµ‰∏ãÁöÑÂÄôÈÄâÂ•ñÂä±ÂáΩÊï∞‰ºöÂ§±Ë¥•ÔºåÈúÄË¶ÅÂ§öÊ¨°ËøêË°åÊâçËÉΩÊâæÂà∞ÊúÄ‰Ω≥ÂÄôÈÄâÂ•ñÂä±ÂáΩÊï∞„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Ë°®ÊòéLEARN-OptÂèØ‰ª•ÈáäÊîæ‰ΩéÊàêÊú¨LLMÁöÑÊΩúÂäõÔºåÊâæÂà∞‰∏éÂ§ßÂûãÊ®°ÂûãÁõ∏ÂΩìÁîöËá≥Êõ¥Â•ΩÁöÑÈ´òÊÄßËÉΩÂÄôÈÄâÂ•ñÂä±ÂáΩÊï∞„ÄÇËøôÁßçÊÄßËÉΩËØÅÊòé‰∫ÜÂÆÉÂú®‰∏çÈúÄË¶Å‰ªª‰ΩïÂàùÊ≠•ÁöÑ‰∫∫Â∑•ÂÆö‰πâÁöÑÊåáÊ†áÁöÑÊÉÖÂÜµ‰∏ãÁîüÊàêÈ´òË¥®ÈáèÂ•ñÂä±ÂáΩÊï∞ÁöÑÊΩúÂäõÔºå‰ªéËÄåÂáèÂ∞ë‰∫ÜÂ∑•Á®ãÂºÄÈîÄÂπ∂Â¢ûÂº∫‰∫ÜÊ≥õÂåñÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âº∫ÂåñÂ≠¶‰π†‰∏≠Â•ñÂä±ÂáΩÊï∞ËÆæËÆ°È´òÂ∫¶‰æùËµñ‰∫∫Â∑•„ÄÅËÄóÊó∂‰∏îÁº∫‰πèÈÄöÁî®ÊÄßÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÈ¢ÑÂÆö‰πâÁöÑËØÑ‰º∞ÊåáÊ†á„ÄÅ‰∫∫Â∑•ÂèçÈ¶àÊàñÁéØÂ¢ÉÊ∫ê‰ª£Á†ÅÔºåÈôêÂà∂‰∫ÜËá™Âä®ÂåñÁ®ãÂ∫¶ÂíåÂ∫îÁî®ËåÉÂõ¥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂº∫Â§ßÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõÔºå‰ªéÁ≥ªÁªüÂíå‰ªªÂä°ÁöÑÊñáÊú¨ÊèèËø∞‰∏≠Ëá™Âä®Êé®ÂØºÂá∫Â•ñÂä±ÂáΩÊï∞ÁöÑËØÑ‰º∞ÊåáÊ†áÔºå‰ªéËÄåÂÆûÁé∞Êó†ÁõëÁù£ÁöÑÂ•ñÂä±ÂáΩÊï∞‰ºòÂåñ„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫Ü‰∫∫Â∑•ÂÆö‰πâÊåáÊ†áÁöÑÈúÄË¶ÅÔºåÊèêÈ´ò‰∫ÜËá™Âä®ÂåñÁ®ãÂ∫¶ÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLEARN-OptÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÈò∂ÊÆµÔºö1) LLMÊ†πÊçÆÁ≥ªÁªüÊèèËø∞Âíå‰ªªÂä°ÁõÆÊ†áÁîüÊàêÂ§ö‰∏™ÂÄôÈÄâÂ•ñÂä±ÂáΩÊï∞Ôºõ2) LLMËá™‰∏ªÂú∞‰ªéÁ≥ªÁªüÊèèËø∞Âíå‰ªªÂä°ÁõÆÊ†á‰∏≠ÊèêÂèñÊÄßËÉΩÊåáÊ†áÔºõ3) ‰ΩøÁî®ÊèêÂèñÁöÑÊÄßËÉΩÊåáÊ†áËØÑ‰º∞ÊØè‰∏™ÂÄôÈÄâÂ•ñÂä±ÂáΩÊï∞Ôºõ4) Ê†πÊçÆËØÑ‰º∞ÁªìÊûúÈÄâÊã©ÊúÄ‰Ω≥Â•ñÂä±ÂáΩÊï∞„ÄÇÊï¥‰∏™ËøáÁ®ãÊó†ÈúÄ‰∫∫Â∑•Âπ≤È¢ÑÔºåÂÆûÁé∞‰∫ÜÂÆåÂÖ®Ëá™‰∏ªÁöÑÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLEARN-OptÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÂÆåÂÖ®Ëá™‰∏ªÁöÑÂ•ñÂä±ÂáΩÊï∞‰ºòÂåñÊµÅÁ®ãÔºåÊó†ÈúÄ‰ªª‰Ωï‰∫∫Â∑•ÂÆö‰πâÁöÑËØÑ‰º∞ÊåáÊ†áÊàñÁéØÂ¢ÉÊ∫ê‰ª£Á†Å„ÄÇÂÆÉÈÄöËøáLLMËá™‰∏ªÂú∞‰ªéÁ≥ªÁªüÊèèËø∞Âíå‰ªªÂä°ÁõÆÊ†á‰∏≠ÊèêÂèñÊÄßËÉΩÊåáÊ†áÔºåÂÆûÁé∞‰∫ÜÊó†ÁõëÁù£ÁöÑÂ•ñÂä±ÂáΩÊï∞ËØÑ‰º∞ÂíåÈÄâÊã©„ÄÇËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÈúÄË¶Å‰∫∫Â∑•Âπ≤È¢ÑÊàñÈ¢ÑÂÆö‰πâÊåáÊ†áÂΩ¢Êàê‰∫ÜÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöLEARN-OptÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®LLMÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÂÄôÈÄâÂ•ñÂä±ÂáΩÊï∞ÔºåÊé¢Á¥¢‰∏çÂêåÁöÑÂ•ñÂä±Á≠ñÁï•Ôºõ2) ËÆæËÆ°ÊúâÊïàÁöÑLLMÊèêÁ§∫Â∑•Á®ãÔºåÂºïÂØºLLMÂáÜÁ°ÆÊèêÂèñÊÄßËÉΩÊåáÊ†áÔºõ3) ÈááÁî®Â§öËΩÆËøêË°åÁ≠ñÁï•ÔºåÂÖãÊúçËá™Âä®Â•ñÂä±ËÆæËÆ°ÁöÑÈ´òÊñπÂ∑ÆÈóÆÈ¢òÔºåÊâæÂà∞Êõ¥È≤ÅÊ£íÁöÑÂ•ñÂä±ÂáΩÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLEARN-OptÂú®Âº∫ÂåñÂ≠¶‰π†ÊéßÂà∂‰ªªÂä°‰∏≠ÂèñÂæó‰∫Ü‰∏éEUREKAÁ≠âÂÖàËøõÊñπÊ≥ïÁõ∏ÂΩìÊàñÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåLEARN-OptÊó†ÈúÄ‰ªª‰Ωï‰∫∫Â∑•ÂÆö‰πâÁöÑÊåáÊ†áÔºåÂπ∂‰∏îËÉΩÂ§üÂà©Áî®‰ΩéÊàêÊú¨LLMÊâæÂà∞È´òÊÄßËÉΩÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Èôç‰ΩéÂ∑•Á®ãÂºÄÈîÄÂíåÊèêÈ´òÊ≥õÂåñÊÄßÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇÂÆûÈ™åËøòÊè≠Á§∫‰∫ÜËá™Âä®Â•ñÂä±ËÆæËÆ°ÁöÑÈ´òÊñπÂ∑ÆÁâπÊÄßÔºåÂº∫Ë∞É‰∫ÜÂ§öËΩÆËøêË°åÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÅÊ∏∏ÊàèAI„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÔºåÈôç‰ΩéÂº∫ÂåñÂ≠¶‰π†Â∫îÁî®Èó®ÊßõÔºåÂä†ÈÄüÊô∫ËÉΩÁ≥ªÁªüÁöÑÂºÄÂèë„ÄÇÈÄöËøáËá™Âä®ÂåñÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°ÔºåÂèØ‰ª•ÂáèÂ∞ëÂØπÈ¢ÜÂüü‰∏ìÂÆ∂ÁöÑ‰æùËµñÔºåÊèêÈ´òÂºÄÂèëÊïàÁéáÔºåÂπ∂Êé¢Á¥¢Êõ¥‰ºòÁöÑÊéßÂà∂Á≠ñÁï•„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑ‰ªªÂä°ÂíåÁéØÂ¢É„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.

