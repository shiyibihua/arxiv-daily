---
layout: default
title: Leveraging LLMs for reward function design in reinforcement learning control tasks
---

# Leveraging LLMs for reward function design in reinforcement learning control tasks

**arXiv**: [2511.19355v1](https://arxiv.org/abs/2511.19355) | [PDF](https://arxiv.org/pdf/2511.19355.pdf)

**ä½œè€…**: Franklin Cardenoso, Wouter Caarls

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLEARN-Optï¼Œåˆ©ç”¨LLMè‡ªä¸»è®¾è®¡å¼ºåŒ–å­¦ä¹ æŽ§åˆ¶ä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å‡½æ•°è®¾è®¡` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `è‡ªåŠ¨åŒ–` `æ— ç›‘ç£å­¦ä¹ ` `æœºå™¨äººæŽ§åˆ¶` `è‡ªä¸»ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°è®¾è®¡ä¾èµ–äººå·¥ç»éªŒï¼Œè€—æ—¶ä¸”æ˜“å‡ºé”™ï¼Œç¼ºä¹è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚
2. LEARN-Optåˆ©ç”¨LLMä»Žæ–‡æœ¬æè¿°ä¸­è‡ªä¸»ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–å¥–åŠ±å‡½æ•°ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒLEARN-Optæ€§èƒ½åª²ç¾Žæˆ–ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œä¸”èƒ½åˆ©ç”¨ä½Žæˆæœ¬LLMæ‰¾åˆ°é«˜æ€§èƒ½å¥–åŠ±å‡½æ•°ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ (RL)ä¸­ï¼Œè®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°æ˜¯ä¸€ä¸ªé‡è¦çš„ç“¶é¢ˆï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„äººå·¥ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ä¸”éžå¸¸è€—æ—¶ã€‚å…ˆå‰çš„å·¥ä½œå’Œå¤§åž‹è¯­è¨€æ¨¡åž‹(LLM)çš„æœ€æ–°è¿›å±•å·²ç»è¯æ˜Žäº†å®ƒä»¬åœ¨è‡ªåŠ¨ç”Ÿæˆå¥–åŠ±å‡½æ•°æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦åˆæ­¥çš„è¯„ä¼°æŒ‡æ ‡ã€äººå·¥è®¾è®¡çš„åé¦ˆæ¥æ”¹è¿›è¿‡ç¨‹ï¼Œæˆ–è€…ä½¿ç”¨çŽ¯å¢ƒæºä»£ç ä½œä¸ºä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºŽLLMçš„å®Œå…¨è‡ªä¸»ä¸”æ¨¡åž‹æ— å…³çš„æ¡†æž¶LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization)ï¼Œå®ƒæ— éœ€åˆæ­¥æŒ‡æ ‡å’ŒçŽ¯å¢ƒæºä»£ç ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå³å¯ä»Žç³»ç»Ÿå’Œä»»åŠ¡ç›®æ ‡çš„æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆã€æ‰§è¡Œå’Œè¯„ä¼°å¥–åŠ±å‡½æ•°å€™é€‰ã€‚LEARN-Optçš„ä¸»è¦è´¡çŒ®åœ¨äºŽå®ƒèƒ½å¤Ÿè‡ªä¸»åœ°ä»Žç³»ç»Ÿæè¿°å’Œä»»åŠ¡ç›®æ ‡ä¸­æŽ¨å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡ï¼Œä»Žè€Œå®žçŽ°å¯¹å¥–åŠ±å‡½æ•°çš„æ— ç›‘ç£è¯„ä¼°å’Œé€‰æ‹©ã€‚å®žéªŒè¡¨æ˜Žï¼ŒLEARN-Optçš„æ€§èƒ½ä¸ŽEUREKAç­‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬å‘çŽ°è‡ªåŠ¨å¥–åŠ±è®¾è®¡æ˜¯ä¸€ä¸ªé«˜æ–¹å·®é—®é¢˜ï¼Œå¹³å‡æƒ…å†µä¸‹çš„å€™é€‰å¥–åŠ±å‡½æ•°ä¼šå¤±è´¥ï¼Œéœ€è¦å¤šæ¬¡è¿è¡Œæ‰èƒ½æ‰¾åˆ°æœ€ä½³å€™é€‰å¥–åŠ±å‡½æ•°ã€‚æœ€åŽï¼Œæˆ‘ä»¬è¡¨æ˜ŽLEARN-Optå¯ä»¥é‡Šæ”¾ä½Žæˆæœ¬LLMçš„æ½œåŠ›ï¼Œæ‰¾åˆ°ä¸Žå¤§åž‹æ¨¡åž‹ç›¸å½“ç”šè‡³æ›´å¥½çš„é«˜æ€§èƒ½å€™é€‰å¥–åŠ±å‡½æ•°ã€‚è¿™ç§æ€§èƒ½è¯æ˜Žäº†å®ƒåœ¨ä¸éœ€è¦ä»»ä½•åˆæ­¥çš„äººå·¥å®šä¹‰çš„æŒ‡æ ‡çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è´¨é‡å¥–åŠ±å‡½æ•°çš„æ½œåŠ›ï¼Œä»Žè€Œå‡å°‘äº†å·¥ç¨‹å¼€é”€å¹¶å¢žå¼ºäº†æ³›åŒ–æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡é«˜åº¦ä¾èµ–äººå·¥ã€è€—æ—¶ä¸”ç¼ºä¹é€šç”¨æ€§çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢„å®šä¹‰çš„è¯„ä¼°æŒ‡æ ‡ã€äººå·¥åé¦ˆæˆ–çŽ¯å¢ƒæºä»£ç ï¼Œé™åˆ¶äº†è‡ªåŠ¨åŒ–ç¨‹åº¦å’Œåº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰çš„å¼ºå¤§ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä»Žç³»ç»Ÿå’Œä»»åŠ¡çš„æ–‡æœ¬æè¿°ä¸­è‡ªåŠ¨æŽ¨å¯¼å‡ºå¥–åŠ±å‡½æ•°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»Žè€Œå®žçŽ°æ— ç›‘ç£çš„å¥–åŠ±å‡½æ•°ä¼˜åŒ–ã€‚è¿™ç§æ–¹æ³•é¿å…äº†äººå·¥å®šä¹‰æŒ‡æ ‡çš„éœ€è¦ï¼Œæé«˜äº†è‡ªåŠ¨åŒ–ç¨‹åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šLEARN-Optæ¡†æž¶åŒ…å«ä»¥ä¸‹ä¸»è¦é˜¶æ®µï¼š1) LLMæ ¹æ®ç³»ç»Ÿæè¿°å’Œä»»åŠ¡ç›®æ ‡ç”Ÿæˆå¤šä¸ªå€™é€‰å¥–åŠ±å‡½æ•°ï¼›2) LLMè‡ªä¸»åœ°ä»Žç³»ç»Ÿæè¿°å’Œä»»åŠ¡ç›®æ ‡ä¸­æå–æ€§èƒ½æŒ‡æ ‡ï¼›3) ä½¿ç”¨æå–çš„æ€§èƒ½æŒ‡æ ‡è¯„ä¼°æ¯ä¸ªå€™é€‰å¥–åŠ±å‡½æ•°ï¼›4) æ ¹æ®è¯„ä¼°ç»“æžœé€‰æ‹©æœ€ä½³å¥–åŠ±å‡½æ•°ã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€äººå·¥å¹²é¢„ï¼Œå®žçŽ°äº†å®Œå…¨è‡ªä¸»çš„å¥–åŠ±å‡½æ•°è®¾è®¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šLEARN-Optçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶å®Œå…¨è‡ªä¸»çš„å¥–åŠ±å‡½æ•°ä¼˜åŒ–æµç¨‹ï¼Œæ— éœ€ä»»ä½•äººå·¥å®šä¹‰çš„è¯„ä¼°æŒ‡æ ‡æˆ–çŽ¯å¢ƒæºä»£ç ã€‚å®ƒé€šè¿‡LLMè‡ªä¸»åœ°ä»Žç³»ç»Ÿæè¿°å’Œä»»åŠ¡ç›®æ ‡ä¸­æå–æ€§èƒ½æŒ‡æ ‡ï¼Œå®žçŽ°äº†æ— ç›‘ç£çš„å¥–åŠ±å‡½æ•°è¯„ä¼°å’Œé€‰æ‹©ã€‚è¿™ä¸ŽçŽ°æœ‰æ–¹æ³•éœ€è¦äººå·¥å¹²é¢„æˆ–é¢„å®šä¹‰æŒ‡æ ‡å½¢æˆäº†é²œæ˜Žå¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šLEARN-Optçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨LLMç”Ÿæˆå¤šæ ·åŒ–çš„å€™é€‰å¥–åŠ±å‡½æ•°ï¼ŒæŽ¢ç´¢ä¸åŒçš„å¥–åŠ±ç­–ç•¥ï¼›2) è®¾è®¡æœ‰æ•ˆçš„LLMæç¤ºå·¥ç¨‹ï¼Œå¼•å¯¼LLMå‡†ç¡®æå–æ€§èƒ½æŒ‡æ ‡ï¼›3) é‡‡ç”¨å¤šè½®è¿è¡Œç­–ç•¥ï¼Œå…‹æœè‡ªåŠ¨å¥–åŠ±è®¾è®¡çš„é«˜æ–¹å·®é—®é¢˜ï¼Œæ‰¾åˆ°æ›´é²æ£’çš„å¥–åŠ±å‡½æ•°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒLEARN-Optåœ¨å¼ºåŒ–å­¦ä¹ æŽ§åˆ¶ä»»åŠ¡ä¸­å–å¾—äº†ä¸ŽEUREKAç­‰å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒLEARN-Optæ— éœ€ä»»ä½•äººå·¥å®šä¹‰çš„æŒ‡æ ‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿåˆ©ç”¨ä½Žæˆæœ¬LLMæ‰¾åˆ°é«˜æ€§èƒ½çš„å¥–åŠ±å‡½æ•°ï¼Œè¯æ˜Žäº†å…¶åœ¨é™ä½Žå·¥ç¨‹å¼€é”€å’Œæé«˜æ³›åŒ–æ€§æ–¹é¢çš„æ½œåŠ›ã€‚å®žéªŒè¿˜æ­ç¤ºäº†è‡ªåŠ¨å¥–åŠ±è®¾è®¡çš„é«˜æ–¹å·®ç‰¹æ€§ï¼Œå¼ºè°ƒäº†å¤šè½®è¿è¡Œçš„é‡è¦æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽæœºå™¨äººæŽ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œé™ä½Žå¼ºåŒ–å­¦ä¹ åº”ç”¨é—¨æ§›ï¼ŒåŠ é€Ÿæ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ã€‚é€šè¿‡è‡ªåŠ¨åŒ–å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œå¯ä»¥å‡å°‘å¯¹é¢†åŸŸä¸“å®¶çš„ä¾èµ–ï¼Œæé«˜å¼€å‘æ•ˆçŽ‡ï¼Œå¹¶æŽ¢ç´¢æ›´ä¼˜çš„æŽ§åˆ¶ç­–ç•¥ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’ŒçŽ¯å¢ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.

