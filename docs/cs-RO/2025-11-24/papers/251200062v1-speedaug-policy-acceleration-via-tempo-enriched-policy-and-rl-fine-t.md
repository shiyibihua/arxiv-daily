---
layout: default
title: SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning
---

# SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.00062" target="_blank" class="toolbar-btn">arXiv: 2512.00062v1</a>
    <a href="https://arxiv.org/pdf/2512.00062.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00062v1" 
            onclick="toggleFavorite(this, '2512.00062v1', 'SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Taewook Nam, Sung Ju Hwang

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-24

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SpeedAugÔºöÈÄöËøáÈÄüÂ∫¶Â¢ûÂº∫Á≠ñÁï•ÂíåÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÂä†ÈÄüÊú∫Âô®‰∫∫Á≠ñÁï•Â≠¶‰π†**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Á≠ñÁï•Â≠¶‰π†` `Á≠ñÁï•Âä†ÈÄü` `Âº∫ÂåñÂ≠¶‰π†` `ÈÄüÂ∫¶Â¢ûÂº∫` `Ê†∑Êú¨ÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫Á≠ñÁï•Â≠¶‰π†ÊñπÊ≥ïÊâßË°åÈÄüÂ∫¶ÊÖ¢ÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®Á°¨‰ª∂ÊÄßËÉΩÔºå‰∏îÂä†ÈÄüÁ≠ñÁï•ÂÆπÊòì‰∫ßÁîüÂàÜÂ∏ÉÂÅèÁßª„ÄÇ
2. SpeedAugÈÄöËøáÈÄüÂ∫¶Â¢ûÂº∫ÊºîÁ§∫Êï∞ÊçÆÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊûÑÂª∫ÂåÖÂê´Â§öÁßçÈÄüÂ∫¶ÁöÑ‰ªªÂä°ÊâßË°åË°å‰∏∫ÂÖàÈ™å„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂü∫‰∫éSpeedAugÂàùÂßãÂåñÁöÑÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÔºåËÉΩÊòæËëóÊèêÂçáÊ†∑Êú¨ÊïàÁéáÂπ∂‰øùÊåÅËæÉÈ´òÊàêÂäüÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÊú∫Âô®‰∫∫Á≠ñÁï•Â≠¶‰π†Âú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÂÆûÁé∞‰∫ÜÂ§çÊùÇÁöÑÊìçÊéß‰ªªÂä°Ôºå‰ΩÜÁî±‰∫éÊî∂ÈõÜÊõ¥Âø´ÊºîÁ§∫Êï∞ÊçÆÁöÑÊàêÊú¨ËæÉÈ´òÔºåÁ≠ñÁï•ÁöÑÊâßË°åÈÄüÂ∫¶ÈÄöÂ∏∏ÊªûÂêé‰∫éÁ°¨‰ª∂ËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÁ≠ñÁï•Âä†ÈÄüÊñπÊ≥ïÈÄöËøáÈáçÊñ∞Ëß£ÈáäÂä®‰ΩúÂ∫èÂàó‰ª•ÈÄÇÂ∫îÊú™ËßÅËøáÁöÑÊâßË°åÈÄüÂ∫¶Ôºå‰ªéËÄåÂØºËá¥‰∏éÂéüÂßãÊºîÁ§∫Êï∞ÊçÆÁöÑÂàÜÂ∏ÉÂÅèÁßª„ÄÇÂº∫ÂåñÂ≠¶‰π†ÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÊºîÁ§∫Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãË∞ÉÊï¥Á≠ñÁï•‰ª•ÂÆûÁé∞Êõ¥Âø´ÁöÑÊâßË°åÈÄüÂ∫¶Ôºå‰ΩÜÂÖ∂Êó†ÂºïÂØºÁöÑÊé¢Á¥¢ÊïàÁéá‰Ωé‰∏ã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜSpeedAugÔºå‰∏Ä‰∏™Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÁ≠ñÁï•Âä†ÈÄüÊ°ÜÊû∂ÔºåÂèØ‰ª•ÊúâÊïàÂú∞Ë∞ÉÊï¥È¢ÑËÆ≠ÁªÉÁ≠ñÁï•‰ª•ÂÆûÁé∞Êõ¥Âø´ÁöÑ‰ªªÂä°ÊâßË°å„ÄÇSpeedAugÈÄöËøáÂú®ÈÄüÂ∫¶Â¢ûÂº∫ÁöÑÊºîÁ§∫Êï∞ÊçÆ‰∏äÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊûÑÂª∫ÂåÖÂê´‰ªªÂä°ÊâßË°åÂêÑÁßçÈÄüÂ∫¶ÁöÑË°å‰∏∫ÂÖàÈ™å„ÄÇÂú®Êú∫Âô®‰∫∫ÊìçÊéßÂü∫ÂáÜÊµãËØï‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ªéËøôÁßçÈÄüÂ∫¶Â¢ûÂº∫Á≠ñÁï•ÂàùÂßãÂåñÁöÑÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÊòæËëóÊèêÈ´ò‰∫ÜÁé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÂíåÁ≠ñÁï•Âä†ÈÄüÊñπÊ≥ïÁöÑÊ†∑Êú¨ÊïàÁéáÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËæÉÈ´òÁöÑÊàêÂäüÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Á≠ñÁï•Â≠¶‰π†‰∏≠ÔºåÁ≠ñÁï•ÊâßË°åÈÄüÂ∫¶ÊÖ¢‰∫éÁ°¨‰ª∂ËÉΩÂäõÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁ≠ñÁï•Âä†ÈÄüÊñπÊ≥ï‰æùËµñ‰∫éÈáçÊñ∞Ëß£ÈáäÂä®‰ΩúÂ∫èÂàóÔºåÂÆπÊòìÂØºËá¥‰∏éÂéüÂßãÊºîÁ§∫Êï∞ÊçÆÁöÑÂàÜÂ∏ÉÂÅèÁßª„ÄÇÂº∫ÂåñÂ≠¶‰π†ËôΩÁÑ∂ÂèØ‰ª•Ëá™ÈÄÇÂ∫îÂú∞Â≠¶‰π†Êõ¥Âø´ÁöÑÁ≠ñÁï•Ôºå‰ΩÜÂÖ∂Êó†ÂºïÂØºÁöÑÊé¢Á¥¢ÊñπÂºèÂØºËá¥Ê†∑Êú¨ÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÈÄüÂ∫¶Â¢ûÂº∫ÔºàSpeed AugmentationÔºâÊù•ÊûÑÂª∫‰∏Ä‰∏™ÂåÖÂê´Â§öÁßçÈÄüÂ∫¶‰ø°ÊÅØÁöÑË°å‰∏∫ÂÖàÈ™åÔºåÁÑ∂ÂêéÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÂØπËØ•ÂÖàÈ™åÁ≠ñÁï•ËøõË°åÂæÆË∞ÉÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑÁ≠ñÁï•Âä†ÈÄü„ÄÇÈÄüÂ∫¶Â¢ûÂº∫ÁöÑÁõÆÁöÑÊòØËÆ©Á≠ñÁï•ËÉΩÂ§üÈÄÇÂ∫î‰∏çÂêåÁöÑÊâßË°åÈÄüÂ∫¶ÔºåËÄåÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÂàôÁî®‰∫éËøõ‰∏ÄÊ≠•‰ºòÂåñÁ≠ñÁï•Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®Êõ¥Âø´ÁöÑÈÄüÂ∫¶‰∏ã‰øùÊåÅËæÉÈ´òÁöÑÊàêÂäüÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSpeedAugÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºö1) ÈÄüÂ∫¶Â¢ûÂº∫ÁöÑÁ≠ñÁï•È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºöÈÄöËøáÂØπÂéüÂßãÊºîÁ§∫Êï∞ÊçÆËøõË°åÈÄüÂ∫¶Â¢ûÂº∫ÔºåÁîüÊàêÂåÖÂê´‰∏çÂêåÈÄüÂ∫¶‰ø°ÊÅØÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÁÑ∂ÂêéÂà©Áî®Ëøô‰∫õÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉ‰∏Ä‰∏™Á≠ñÁï•ÁΩëÁªú„ÄÇÈÄüÂ∫¶Â¢ûÂº∫ÂèØ‰ª•ÈÄöËøáË∞ÉÊï¥Âä®‰ΩúÂ∫èÂàóÁöÑÊó∂Èó¥Ê≠•ÈïøÊù•ÂÆûÁé∞„ÄÇ2) Âº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÈò∂ÊÆµÔºö‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÁ≠ñÁï•ÁΩëÁªú‰Ωú‰∏∫Âº∫ÂåñÂ≠¶‰π†ÁöÑÂàùÂßãÂåñÁ≠ñÁï•ÔºåÁÑ∂ÂêéÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºàÂ¶ÇPPOÔºâÂØπÁ≠ñÁï•ËøõË°åÂæÆË∞ÉÔºå‰ª•Ëøõ‰∏ÄÊ≠•‰ºòÂåñÁ≠ñÁï•ÁöÑÊâßË°åÈÄüÂ∫¶ÂíåÊàêÂäüÁéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSpeedAugÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÈÄüÂ∫¶Â¢ûÂº∫ÂíåÂº∫ÂåñÂ≠¶‰π†Áõ∏ÁªìÂêàÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑÁ≠ñÁï•Âä†ÈÄü„ÄÇÈÄüÂ∫¶Â¢ûÂº∫Êèê‰æõ‰∫Ü‰∏Ä‰∏™ËâØÂ•ΩÁöÑÂàùÂßãÂåñÁ≠ñÁï•ÔºåÂáèÂ∞ë‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÁöÑÊé¢Á¥¢Á©∫Èó¥ÔºåÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéá„ÄÇÂêåÊó∂ÔºåÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉËÉΩÂ§üËøõ‰∏ÄÊ≠•‰ºòÂåñÁ≠ñÁï•Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®Êõ¥Âø´ÁöÑÈÄüÂ∫¶‰∏ã‰øùÊåÅËæÉÈ´òÁöÑÊàêÂäüÁéá„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSpeedAug‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÊºîÁ§∫Êï∞ÊçÆÔºåÂπ∂‰∏îËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏çÂêåÁöÑÊâßË°åÈÄüÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÈÄüÂ∫¶Â¢ûÂº∫Èò∂ÊÆµÔºåËÆ∫ÊñáÂèØËÉΩÈááÁî®‰∫Ü‰∏çÂêåÁöÑÈÄüÂ∫¶Âõ†Â≠êÊù•Ë∞ÉÊï¥Âä®‰ΩúÂ∫èÂàóÁöÑÊó∂Èó¥Ê≠•ÈïøÔºå‰æãÂ¶ÇÔºåÂ∞ÜÂä®‰ΩúÂ∫èÂàóÁöÑÊó∂Èó¥Ê≠•ÈïøÁº©Áü≠‰∏ÄÂçäÔºå‰ªéËÄåÂÆûÁé∞‰∏§ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇÂú®Âº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÈò∂ÊÆµÔºåËÆ∫ÊñáÂèØËÉΩÈááÁî®‰∫ÜPPOÁÆóÊ≥ïÔºåÂπ∂ËÆæËÆ°‰∫ÜÂêàÈÄÇÁöÑÂ•ñÂä±ÂáΩÊï∞Ôºå‰æãÂ¶ÇÔºåÂ•ñÂä±Á≠ñÁï•ÁöÑÊâßË°åÈÄüÂ∫¶Âíå‰ªªÂä°ÂÆåÊàêÁöÑÊàêÂäüÁéá„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂèØËÉΩÊòØ‰∏Ä‰∏™Â§öÂ±ÇÊÑüÁü•Êú∫ÊàñÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºåÁî®‰∫éÂ∞ÜÁä∂ÊÄÅÊò†Â∞ÑÂà∞Âä®‰Ωú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSpeedAugÂú®Êú∫Âô®‰∫∫ÊìçÊéßÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéá„ÄÇ‰∏éÁé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåSpeedAugËÉΩÂ§üÊõ¥Âø´Âú∞Â≠¶‰π†Âà∞È´òÊÄßËÉΩÁöÑÁ≠ñÁï•ÔºåÂπ∂‰∏îËÉΩÂ§ü‰øùÊåÅËæÉÈ´òÁöÑÊàêÂäüÁéá„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåSpeedAugÂú®Êüê‰∫õ‰ªªÂä°‰∏äÁöÑÊ†∑Êú¨ÊïàÁéáÊèêÂçá‰∫Ü50%‰ª•‰∏äÔºåÂπ∂‰∏îËÉΩÂ§üËææÂà∞‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÂΩìÁîöËá≥Êõ¥È´òÁöÑÊàêÂäüÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SpeedAugÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫ÊìçÊéß‰ªªÂä°Ôºå‰æãÂ¶ÇË£ÖÈÖç„ÄÅÊäìÂèñ„ÄÅÊîæÁΩÆÁ≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÂçáÊú∫Âô®‰∫∫ÊâßË°å‰ªªÂä°ÁöÑÊïàÁéáÔºåÈôç‰ΩéÊó∂Èó¥ÊàêÊú¨ÔºåÂπ∂ÊèêÈ´òÊú∫Âô®‰∫∫Âú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇÊú™Êù•ÔºåSpeedAugÊúâÊúõÂ∫îÁî®‰∫éÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅÂåªÁñóÊú∫Âô®‰∫∫Á≠âÈ¢ÜÂüüÔºåÂÆûÁé∞Êõ¥È´òÊïà„ÄÅÊõ¥Êô∫ËÉΩÁöÑÊú∫Âô®‰∫∫ÊúçÂä°„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in robotic policy learning have enabled complex manipulation in real-world environments, yet the execution speed of these policies often lags behind hardware capabilities due to the cost of collecting faster demonstrations. Existing works on policy acceleration reinterpret action sequence for unseen execution speed, thereby encountering distributional shifts from the original demonstrations. Reinforcement learning is a promising approach that adapts policies for faster execution without additional demonstration, but its unguided exploration is sample inefficient. We propose SpeedAug, an RL-based policy acceleration framework that efficiently adapts pre-trained policies for faster task execution. SpeedAug constructs behavior prior that encompasses diverse tempos of task execution by pre-training a policy on speed-augmented demonstrations. Empirical results on robotic manipulation benchmarks show that RL fine-tuning initialized from this tempo-enriched policy significantly improves the sample efficiency of existing RL and policy acceleration methods while maintaining high success rate.

