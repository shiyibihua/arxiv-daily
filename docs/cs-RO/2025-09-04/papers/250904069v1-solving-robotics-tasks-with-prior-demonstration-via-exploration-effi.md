---
layout: default
title: Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning
---

# Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04069" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.04069v1</a>
  <a href="https://arxiv.org/pdf/2509.04069.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04069v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04069v1', 'Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Chengyandan Shen, Christoffer Sloth

**ÂàÜÁ±ª**: cs.RO, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-04

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ÄÁßçÊé¢Á¥¢È´òÊïàÁöÑÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂DRLRÔºåÈÄöËøáÂÖàÈ™åÊºîÁ§∫Ëß£ÂÜ≥Êú∫Âô®‰∫∫‰ªªÂä°„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫Â≠¶‰π†` `Ê®°‰ªøÂ≠¶‰π†` `Êé¢Á¥¢ÊïàÁéá` `Sim2Real` `QÂÄºÊ†°ÂáÜ` `SACÁÆóÊ≥ï`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÊºîÁ§∫ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂ≠òÂú®Êé¢Á¥¢ÊïàÁéá‰ΩéÂíåÊòìÊî∂ÊïõÂà∞Ê¨°‰ºòÁ≠ñÁï•ÁöÑÈóÆÈ¢ò„ÄÇ
2. DRLRÊ°ÜÊû∂ÈÄöËøáÊ†°ÂáÜQÂÄºÂáèËΩªËá™‰∏æËØØÂ∑ÆÔºåÂπ∂‰ΩøÁî®SACÁ≠ñÁï•Èò≤Ê≠¢ËøáÊãüÂêàÔºåÊèêÂçáÊé¢Á¥¢ÊïàÁéá„ÄÇ
3. Âú®Èì≤ÊñóË£ÖËΩΩÂíåÊâìÂºÄÊäΩÂ±âÁ≠â‰ªªÂä°‰∏äÈ™åËØÅ‰∫ÜDRLRÁöÑÊúâÊïàÊÄßÔºåÂπ∂Âú®ÁúüÂÆûËΩÆÂºèË£ÖËΩΩÊú∫‰∏äÊàêÂäüÈÉ®ÁΩ≤„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊé¢Á¥¢È´òÊïàÁöÑÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†‰∏éÂèÇËÄÉÁ≠ñÁï•ÔºàDRLRÔºâÊ°ÜÊû∂ÔºåÁî®‰∫éÂ≠¶‰π†ÁªìÂêàÊºîÁ§∫ÁöÑÊú∫Âô®‰∫∫‰ªªÂä°„ÄÇDRLRÊ°ÜÊû∂Âü∫‰∫éÊ®°‰ªøÂºïÂØºÂº∫ÂåñÂ≠¶‰π†ÔºàIBRLÔºâÁÆóÊ≥ïÂºÄÂèë„ÄÇÊàë‰ª¨Âª∫ËÆÆÈÄöËøá‰øÆÊîπÂä®‰ΩúÈÄâÊã©Ê®°ÂùóÊù•ÊîπËøõIBRL„ÄÇÊâÄÊèêÂá∫ÁöÑÂä®‰ΩúÈÄâÊã©Ê®°ÂùóÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ê†°ÂáÜÁöÑQÂÄºÔºåÂáèËΩª‰∫ÜËá™‰∏æËØØÂ∑ÆÔºåÂê¶Âàô‰ºöÂØºËá¥‰ΩéÊïàÁöÑÊé¢Á¥¢„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜÈò≤Ê≠¢Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Êî∂ÊïõÂà∞Ê¨°‰ºòÁ≠ñÁï•Ôºå‰ΩøÁî®SAC‰Ωú‰∏∫Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ËÄå‰∏çÊòØTD3„ÄÇÈÄöËøáÂ≠¶‰π†ÈúÄË¶Å‰∏éÁéØÂ¢ÉËøõË°åÂ§ßÈáè‰∫§‰∫íÁöÑ‰∏§‰∏™Êú∫Âô®‰∫∫‰ªªÂä°ÔºöÈì≤ÊñóË£ÖËΩΩÂíåÊâìÂºÄÊäΩÂ±âÔºåÁªèÈ™åËØÅ‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂáèËΩªËá™‰∏æËØØÂ∑ÆÂíåÈò≤Ê≠¢ËøáÊãüÂêàÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ‰ªøÁúüÁªìÊûúËøòËØÅÊòé‰∫ÜDRLRÊ°ÜÊû∂Âú®ÂÖ∑Êúâ‰ΩéÁª¥ÂíåÈ´òÁª¥Áä∂ÊÄÅ-Âä®‰ΩúÁ©∫Èó¥‰ª•Âèä‰∏çÂêåÊºîÁ§∫Ë¥®ÈáèÁöÑ‰ªªÂä°‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ‰∏∫‰∫ÜÂú®ÁúüÂÆûÁöÑÂ∑•‰∏öÊú∫Âô®‰∫∫‰ªªÂä°‰∏äËØÑ‰º∞ÊâÄÂºÄÂèëÁöÑÊ°ÜÊû∂ÔºåÈì≤ÊñóË£ÖËΩΩ‰ªªÂä°Ë¢´ÈÉ®ÁΩ≤Âú®ÁúüÂÆûÁöÑËΩÆÂºèË£ÖËΩΩÊú∫‰∏ä„ÄÇSim2RealÁªìÊûúÈ™åËØÅ‰∫ÜDRLRÊ°ÜÊû∂ÁöÑÊàêÂäüÈÉ®ÁΩ≤„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÔºåÂ¶Ç‰ΩïÂà©Áî®ÂÖàÈ™åÊºîÁ§∫Êï∞ÊçÆÔºåÈ´òÊïàÂú∞ËÆ≠ÁªÉÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÁõ¥Êé•‰ΩøÁî®Ê®°‰ªøÂ≠¶‰π†ÔºåÂÆπÊòìÈô∑ÂÖ•Ê¨°‰ºòËß£„ÄÇËÄå‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†Êé¢Á¥¢ÊïàÁéá‰ΩéÔºåÈúÄË¶ÅÂ§ßÈáèÁöÑÁéØÂ¢É‰∫§‰∫í„ÄÇÁªìÂêàÊ®°‰ªøÂ≠¶‰π†ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºå‰æãÂ¶ÇIBRLÔºåÂÆπÊòìÂèóÂà∞Ëá™‰∏æËØØÂ∑ÆÁöÑÂΩ±ÂìçÔºåÂØºËá¥Êé¢Á¥¢ÊïàÁéá‰∏çÈ´ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊîπËøõIBRLÁÆóÊ≥ïÁöÑÂä®‰ΩúÈÄâÊã©Ê®°ÂùóÔºåÊèê‰æõÊ†°ÂáÜÁöÑQÂÄºÔºå‰ªéËÄåÂáèËΩªËá™‰∏æËØØÂ∑ÆÔºåÊèêÈ´òÊé¢Á¥¢ÊïàÁéá„ÄÇÂêåÊó∂Ôºå‰ΩøÁî®SACÁÆóÊ≥ï‰Ωú‰∏∫Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Ôºå‰ª•Èò≤Ê≠¢Á≠ñÁï•ËøáÊó©Êî∂ÊïõÂà∞Ê¨°‰ºòËß£„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDRLRÊ°ÜÊû∂Âü∫‰∫éIBRLÁÆóÊ≥ïÔºå‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÊ®°ÂùóÔºö1) ÊºîÁ§∫Êï∞ÊçÆÊî∂ÈõÜÊ®°ÂùóÔºöÊî∂ÈõÜÊú∫Âô®‰∫∫‰ªªÂä°ÁöÑÊºîÁ§∫Êï∞ÊçÆ„ÄÇ2) Âä®‰ΩúÈÄâÊã©Ê®°ÂùóÔºöËØ•Ê®°ÂùóÊòØDRLRÁöÑÂÖ≥ÈîÆÔºåÈÄöËøáÊ†°ÂáÜQÂÄºÊù•Âπ≥Ë°°Êé¢Á¥¢ÂíåÂà©Áî®„ÄÇ3) Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Ê®°ÂùóÔºö‰ΩøÁî®SACÁÆóÊ≥ï‰Ωú‰∏∫RLÁ≠ñÁï•ÔºåË¥üË¥£Â≠¶‰π†ÊúÄ‰ºòÁ≠ñÁï•„ÄÇ4) ÁéØÂ¢É‰∫§‰∫íÊ®°ÂùóÔºöÊú∫Âô®‰∫∫‰∏éÁéØÂ¢ÉËøõË°å‰∫§‰∫íÔºåÊî∂ÈõÜÁªèÈ™åÊï∞ÊçÆ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDRLRÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂä®‰ΩúÈÄâÊã©Ê®°Âùó‰∏≠Ê†°ÂáÜQÂÄºÁöÑÂºïÂÖ•„ÄÇ‰º†ÁªüÁöÑIBRLÁÆóÊ≥ïÁõ¥Êé•‰ΩøÁî®QÂÄºËøõË°åÂä®‰ΩúÈÄâÊã©ÔºåÂÆπÊòìÂèóÂà∞Ëá™‰∏æËØØÂ∑ÆÁöÑÂΩ±Âìç„ÄÇDRLRÈÄöËøáÊ†°ÂáÜQÂÄºÔºå‰ΩøÂæóÁÆóÊ≥ïËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞Âä®‰ΩúÁöÑ‰ª∑ÂÄºÔºå‰ªéËÄåÊèêÈ´òÊé¢Á¥¢ÊïàÁéá„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®SACÁÆóÊ≥ïÊõø‰ª£TD3ÔºåÂ¢ûÂº∫‰∫ÜÁ≠ñÁï•ÁöÑÊé¢Á¥¢ËÉΩÂäõÔºåÈÅøÂÖçÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂä®‰ΩúÈÄâÊã©Ê®°ÂùóÁöÑÂÖ≥ÈîÆÂú®‰∫éQÂÄºÁöÑÊ†°ÂáÜÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåËÆ∫ÊñáÂèØËÉΩËÆæËÆ°‰∫Ü‰∏ÄÁßçÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éËÆ≠ÁªÉ‰∏Ä‰∏™QÂÄºÊ†°ÂáÜÂô®ÔºåËØ•Ê†°ÂáÜÂô®ËÉΩÂ§üÊ†πÊçÆÂΩìÂâçÁä∂ÊÄÅÂíåÂä®‰ΩúÔºåÈ¢ÑÊµãQÂÄºÁöÑÂÅèÂ∑ÆÔºåÂπ∂ËøõË°å‰øÆÊ≠£„ÄÇSACÁÆóÊ≥ïÁöÑ‰ΩøÁî®ÔºåÊ∂âÂèäÂà∞Â•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°Ôºå‰ª•ÂèäSACÁÆóÊ≥ïÁöÑË∂ÖÂèÇÊï∞ËÆæÁΩÆ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÔºà‰æãÂ¶ÇÔºåQÁΩëÁªúÂíåÁ≠ñÁï•ÁΩëÁªúÁöÑÁªìÊûÑÔºâ‰πüÊòØÈúÄË¶Å‰ªîÁªÜËÆæËÆ°ÁöÑ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDRLRÊ°ÜÊû∂Âú®Èì≤ÊñóË£ÖËΩΩÂíåÊâìÂºÄÊäΩÂ±â‰∏§‰∏™Êú∫Âô®‰∫∫‰ªªÂä°‰∏äÔºåÁõ∏ËæÉ‰∫éIBRLÁ≠âÂü∫Á∫øÊñπÊ≥ïÔºåËÉΩÂ§üÊõ¥Âø´Âú∞Â≠¶‰π†Âà∞ÊúÄ‰ºòÁ≠ñÁï•ÔºåÂπ∂ÂÖ∑ÊúâÊõ¥È´òÁöÑÊàêÂäüÁéá„ÄÇSim2RealÂÆûÈ™åÈ™åËØÅ‰∫ÜDRLRÊ°ÜÊû∂Âú®ÁúüÂÆûËΩÆÂºèË£ÖËΩΩÊú∫‰∏äÁöÑÈÉ®ÁΩ≤ÂèØË°åÊÄßÔºåË°®ÊòéËØ•ÊñπÊ≥ïÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÊú∫Âô®‰∫∫Ëá™‰∏ªÂÆåÊàê‰ªªÂä°ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÁâ©ÊµÅÊê¨Ëøê„ÄÅÂÆ∂Â∫≠ÊúçÂä°Á≠â„ÄÇÈÄöËøáÁªìÂêàÂÖàÈ™åÁü•ËØÜÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÂèØ‰ª•ÊòæËëóÈôç‰ΩéÊú∫Âô®‰∫∫Â≠¶‰π†ÊàêÊú¨ÔºåÊèêÈ´ò‰ªªÂä°ÂÆåÊàêÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊé®ÂπøÂà∞Êõ¥Â§çÊùÇÁöÑÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÔºåÂÆûÁé∞Êõ¥È´òÁ∫ßÂà´ÁöÑÊú∫Âô®‰∫∫Êô∫ËÉΩÂåñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper proposes an exploration-efficient Deep Reinforcement Learning with Reference policy (DRLR) framework for learning robotics tasks that incorporates demonstrations. The DRLR framework is developed based on an algorithm called Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve IBRL by modifying the action selection module. The proposed action selection module provides a calibrated Q-value, which mitigates the bootstrapping error that otherwise leads to inefficient exploration. Furthermore, to prevent the RL policy from converging to a sub-optimal policy, SAC is used as the RL policy instead of TD3. The effectiveness of our method in mitigating bootstrapping error and preventing overfitting is empirically validated by learning two robotics tasks: bucket loading and open drawer, which require extensive interactions with the environment. Simulation results also demonstrate the robustness of the DRLR framework across tasks with both low and high state-action dimensions, and varying demonstration qualities. To evaluate the developed framework on a real-world industrial robotics task, the bucket loading task is deployed on a real wheel loader. The sim2real results validate the successful deployment of the DRLR framework.

