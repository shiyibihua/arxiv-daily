---
layout: default
title: Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance
---

# Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03889" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03889v1</a>
  <a href="https://arxiv.org/pdf/2509.03889.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03889v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03889v1', 'Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Neha Sunil, Megha Tippur, Arnau Saumell, Edward Adelson, Alberto Rodriguez

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: Accepted at CoRL 2025. Project website: https://mhtippur.github.io/inairclothmanipulation/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç½®ä¿¡åº¦æ„ŸçŸ¥ç¨ å¯†å¯¹åº”å’Œè§¦è§‰åé¦ˆçš„ç©ºä¸­æœè£…æ“ä½œæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æœè£…æ“ä½œ` `æœºå™¨äºº` `è§†è§‰è§¦è§‰` `ç¨ å¯†å¯¹åº”` `æŠ“å–å¯ä¾›æ€§` `ç½®ä¿¡åº¦æ„ŸçŸ¥` `ååº”å¼æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æœè£…æ“ä½œé¢ä¸´å¤æ‚å½¢å˜ã€ææ–™å·®å¼‚å’Œè‡ªé®æŒ¡ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºç®€åŒ–å‡è®¾æˆ–é¢„å…ˆå¯è§çš„å…³é”®ç‰¹å¾ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åŒè‡‚è§†è§‰è§¦è§‰æ¡†æ¶ï¼Œåˆ©ç”¨ç½®ä¿¡åº¦æ„ŸçŸ¥çš„ç¨ å¯†å¯¹åº”å’Œè§¦è§‰ç›‘ç£çš„å¯ä¾›æ€§æŠ“å–æ¥æ“ä½œæœè£…ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†é«˜åº¦é®æŒ¡çš„æœè£…ï¼Œå¹¶åœ¨æŠ˜å å’Œæ‚¬æŒ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶å…·å¤‡ä»äººç±»æ¼”ç¤ºä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœè£…æ“ä½œå› å…¶å¤æ‚çš„å½¢æ€ã€å¤šå˜çš„ææ–™åŠ¨åŠ›å­¦å’Œé¢‘ç¹çš„è‡ªé®æŒ¡è€Œæå…·æŒ‘æˆ˜æ€§ã€‚ç°æœ‰ç³»ç»Ÿé€šå¸¸å°†æœè£…å±•å¹³æˆ–å‡è®¾å…³é”®ç‰¹å¾å¯è§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒè‡‚è§†è§‰è§¦è§‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç½®ä¿¡åº¦æ„ŸçŸ¥çš„ç¨ å¯†è§†è§‰å¯¹åº”å’Œè§¦è§‰ç›‘ç£çš„æŠ“å–å¯ä¾›æ€§ï¼Œå¯ä»¥ç›´æ¥æ“ä½œè¤¶çš±å’Œæ‚¬æŒ‚çš„æœè£…ã€‚å¯¹åº”æ¨¡å‹åœ¨ä¸€ä¸ªå®šåˆ¶çš„é«˜ä¿çœŸæ¨¡æ‹Ÿæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨åˆ†å¸ƒæŸå¤±æ¥æ•è·æœè£…çš„å¯¹ç§°æ€§å¹¶ç”Ÿæˆå¯¹åº”ç½®ä¿¡åº¦ä¼°è®¡ã€‚è¿™äº›ä¼°è®¡æŒ‡å¯¼ä¸€ä¸ªååº”å¼çŠ¶æ€æœºï¼Œè¯¥çŠ¶æ€æœºæ ¹æ®æ„ŸçŸ¥ä¸ç¡®å®šæ€§è°ƒæ•´æŠ˜å ç­–ç•¥ã€‚åŒæ—¶ï¼Œä¸€ä¸ªè§†è§‰è§¦è§‰æŠ“å–å¯ä¾›æ€§ç½‘ç»œï¼Œä½¿ç”¨é«˜åˆ†è¾¨ç‡è§¦è§‰åé¦ˆè¿›è¡Œè‡ªç›‘ç£ï¼Œç¡®å®šå“ªäº›åŒºåŸŸæ˜¯ç‰©ç†ä¸Šå¯æŠ“å–çš„ã€‚ç›¸åŒçš„è§¦è§‰åˆ†ç±»å™¨åœ¨æ‰§è¡ŒæœŸé—´ç”¨äºå®æ—¶æŠ“å–éªŒè¯ã€‚é€šè¿‡åœ¨ä½ç½®ä¿¡åº¦çŠ¶æ€ä¸‹æ¨è¿ŸåŠ¨ä½œï¼Œè¯¥ç³»ç»Ÿå¯ä»¥å¤„ç†é«˜åº¦é®æŒ¡çš„æ¡Œé¢å’Œç©ºä¸­é…ç½®ã€‚æˆ‘ä»¬åœ¨æŠ˜å å’Œæ‚¬æŒ‚ä»»åŠ¡ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„ä»»åŠ¡æ— å…³çš„æŠ“å–é€‰æ‹©æ¨¡å—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç¨ å¯†æè¿°ç¬¦ä¸ºå…¶ä»–è§„åˆ’æ–¹å¼æä¾›äº†ä¸€ä¸ªå¯é‡ç”¨çš„ä¸­é—´è¡¨ç¤ºï¼Œä¾‹å¦‚ä»äººç±»è§†é¢‘æ¼”ç¤ºä¸­æå–æŠ“å–ç›®æ ‡ï¼Œä¸ºæ›´é€šç”¨å’Œå¯æ‰©å±•çš„æœè£…æ“ä½œé“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœè£…æ“ä½œä»»åŠ¡å› æœè£…çš„å¤æ‚å½¢æ€ã€ææ–™çš„åŠ¨æ€å˜åŒ–ä»¥åŠé¢‘ç¹çš„è‡ªé®æŒ¡è€Œæå…·æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å°†æœè£…å±•å¹³ï¼Œæˆ–è€…å‡è®¾å…³é”®ç‰¹å¾æ˜¯å¯è§çš„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚è¯¥è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨å¤æ‚ã€éå¹³æ•´çŠ¶æ€ä¸‹ï¼Œå¯¹æœè£…è¿›è¡Œå¯é æ“ä½œçš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆè§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼Œåˆ©ç”¨ç½®ä¿¡åº¦æ„ŸçŸ¥çš„ç¨ å¯†å¯¹åº”æ¥ç†è§£æœè£…çš„å½¢çŠ¶å’ŒçŠ¶æ€ï¼Œå¹¶åˆ©ç”¨è§¦è§‰åé¦ˆæ¥æŒ‡å¯¼æŠ“å–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç³»ç»Ÿå¯ä»¥å¤„ç†é®æŒ¡å’Œä¸ç¡®å®šæ€§ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„æŠ“å–ç‚¹ã€‚æ ¸å¿ƒåœ¨äºåˆ©ç”¨ç½®ä¿¡åº¦æ¥æŒ‡å¯¼å†³ç­–ï¼Œåœ¨ä¸ç¡®å®šæ€§é«˜æ—¶æ¨è¿ŸåŠ¨ä½œï¼Œä»è€Œæé«˜é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç³»ç»Ÿé‡‡ç”¨åŒè‡‚æœºå™¨äººå¹³å°ï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) ç¨ å¯†å¯¹åº”æ¨¡å‹ï¼šç”¨äºä¼°è®¡æœè£…ä¸åŒéƒ¨åˆ†ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå¹¶è¾“å‡ºç½®ä¿¡åº¦ä¼°è®¡ã€‚2) æŠ“å–å¯ä¾›æ€§ç½‘ç»œï¼šç”¨äºé¢„æµ‹æœè£…ä¸Šå“ªäº›åŒºåŸŸæ˜¯å¯æŠ“å–çš„ï¼Œå¹¶ä½¿ç”¨è§¦è§‰åé¦ˆè¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ã€‚3) ååº”å¼çŠ¶æ€æœºï¼šæ ¹æ®ç¨ å¯†å¯¹åº”æ¨¡å‹çš„ç½®ä¿¡åº¦ï¼ŒåŠ¨æ€è°ƒæ•´æŠ˜å ç­–ç•¥ã€‚4) è§¦è§‰éªŒè¯æ¨¡å—ï¼šåœ¨æŠ“å–æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨è§¦è§‰ä¼ æ„Ÿå™¨éªŒè¯æŠ“å–çš„å¯é æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ç½®ä¿¡åº¦æ„ŸçŸ¥çš„ç¨ å¯†å¯¹åº”æ–¹æ³•ï¼Œèƒ½å¤Ÿå¤„ç†æœè£…çš„å¯¹ç§°æ€§å’Œé®æŒ¡é—®é¢˜ã€‚2) ç»“åˆè§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼Œå®ç°äº†é²æ£’çš„æŠ“å–é€‰æ‹©å’ŒéªŒè¯ã€‚3) è®¾è®¡äº†ååº”å¼çŠ¶æ€æœºï¼Œèƒ½å¤Ÿæ ¹æ®æ„ŸçŸ¥ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´æ“ä½œç­–ç•¥ã€‚4) æå‡ºäº†ä¸€ä¸ªä»»åŠ¡æ— å…³çš„æŠ“å–é€‰æ‹©æ¨¡å—ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒçš„æœè£…æ“ä½œä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šç¨ å¯†å¯¹åº”æ¨¡å‹ä½¿ç”¨åˆ†å¸ƒæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œä»¥æ•è·æœè£…çš„å¯¹ç§°æ€§å¹¶ç”Ÿæˆç½®ä¿¡åº¦ä¼°è®¡ã€‚æŠ“å–å¯ä¾›æ€§ç½‘ç»œä½¿ç”¨é«˜åˆ†è¾¨ç‡è§¦è§‰åé¦ˆè¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨æœ€å¤§åŒ–æŠ“å–çš„æˆåŠŸç‡ã€‚ååº”å¼çŠ¶æ€æœºæ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼æ¥å†³å®šæ˜¯å¦æ‰§è¡ŒåŠ¨ä½œï¼Œæˆ–è€…åˆ‡æ¢åˆ°ä¸åŒçš„æ“ä½œç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åœ¨æŠ˜å å’Œæ‚¬æŒ‚ä»»åŠ¡ä¸­ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤ŸæˆåŠŸå¤„ç†é«˜åº¦é®æŒ¡çš„æœè£…ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿè¿˜èƒ½å¤Ÿä»äººç±»è§†é¢‘æ¼”ç¤ºä¸­æå–æŠ“å–ç›®æ ‡ï¼Œè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœè£…æ“ä½œä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœè£…åˆ¶é€ ã€ä»“å‚¨ç‰©æµã€å®¶åº­æœåŠ¡ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœè£…åˆ¶é€ ä¸­ï¼Œæœºå™¨äººå¯ä»¥è‡ªåŠ¨å®Œæˆæœè£…çš„æŠ˜å ã€æ•´ç†å’ŒåŒ…è£…ã€‚åœ¨ä»“å‚¨ç‰©æµä¸­ï¼Œæœºå™¨äººå¯ä»¥é«˜æ•ˆåœ°å¤„ç†æœè£…çš„æ‹£é€‰å’Œåˆ†æ‹£ã€‚åœ¨å®¶åº­æœåŠ¡ä¸­ï¼Œæœºå™¨äººå¯ä»¥å¸®åŠ©äººä»¬æ•´ç†è¡£ç‰©ï¼Œæé«˜ç”Ÿæ´»è´¨é‡ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°æ›´æ™ºèƒ½ã€æ›´çµæ´»çš„æœè£…æ“ä½œæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Manipulating clothing is challenging due to complex configurations, variable material dynamics, and frequent self-occlusion. Prior systems often flatten garments or assume visibility of key features. We present a dual-arm visuotactile framework that combines confidence-aware dense visual correspondence and tactile-supervised grasp affordance to operate directly on crumpled and suspended garments. The correspondence model is trained on a custom, high-fidelity simulated dataset using a distributional loss that captures cloth symmetries and generates correspondence confidence estimates. These estimates guide a reactive state machine that adapts folding strategies based on perceptual uncertainty. In parallel, a visuotactile grasp affordance network, self-supervised using high-resolution tactile feedback, determines which regions are physically graspable. The same tactile classifier is used during execution for real-time grasp validation. By deferring action in low-confidence states, the system handles highly occluded table-top and in-air configurations. We demonstrate our task-agnostic grasp selection module in folding and hanging tasks. Moreover, our dense descriptors provide a reusable intermediate representation for other planning modalities, such as extracting grasp targets from human video demonstrations, paving the way for more generalizable and scalable garment manipulation.

