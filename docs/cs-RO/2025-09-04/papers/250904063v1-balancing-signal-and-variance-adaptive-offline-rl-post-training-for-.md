---
layout: default
title: Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models
---

# Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04063" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04063v1</a>
  <a href="https://arxiv.org/pdf/2509.04063.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04063v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04063v1', 'Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºARFMï¼Œé€šè¿‡è‡ªé€‚åº”ç¦»çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒVLA Flowæ¨¡å‹ï¼Œæå‡æœºå™¨äººæ“ä½œä»»åŠ¡ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `VLAæ¨¡å‹` `Flow Matching` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `è‡ªé€‚åº”ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. VLA Flowæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†ä¾èµ–æ¨¡ä»¿å­¦ä¹ å¯¼è‡´åœ¨å¤æ‚ä»»åŠ¡ä¸­ç²¾åº¦ä¸è¶³ã€‚
2. ARFMé€šè¿‡å¼•å…¥è‡ªé€‚åº”ç¼©æ”¾å› å­ï¼Œåœ¨FlowæŸå¤±ä¸­å¹³è¡¡å¼ºåŒ–å­¦ä¹ ä¿¡å·ï¼Œå®ç°åå·®-æ–¹å·®æƒè¡¡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒARFMåœ¨æ³›åŒ–æ€§ã€é²æ£’æ€§ã€å°‘æ ·æœ¬å­¦ä¹ å’ŒæŒç»­å­¦ä¹ æ–¹é¢å‡æœ‰æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºFlow Matchingçš„Vision-Language-Action (VLA)æ¨¡å‹åœ¨é€šç”¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤æ‚ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åŠ¨ä½œç²¾åº¦å¹¶ä¸ç†æƒ³ã€‚ä¸€ä¸ªé‡è¦åŸå› æ˜¯å®ƒä»¬ä»…ä¾èµ–æ¨¡ä»¿å­¦ä¹ çš„åè®­ç»ƒèŒƒå¼ï¼Œéš¾ä»¥æ·±å…¥ç†è§£æ•°æ®è´¨é‡çš„åˆ†å¸ƒç‰¹æ€§ï¼Œè€Œè¿™æ­£æ˜¯å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ã€‚æœ¬æ–‡ä»ç†è®ºä¸Šæå‡ºäº†VLA Flowæ¨¡å‹çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ åè®­ç»ƒç›®æ ‡ï¼Œå¹¶æ¨å¯¼å‡ºä¸€ä¸ªé«˜æ•ˆå¯è¡Œçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒç®—æ³•â€”â€”è‡ªé€‚åº”å¼ºåŒ–Flow Matching (ARFM)ã€‚é€šè¿‡åœ¨VLA Flowæ¨¡å‹æŸå¤±ä¸­å¼•å…¥è‡ªé€‚åº”è°ƒæ•´çš„ç¼©æ”¾å› å­ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæœ‰åŸåˆ™çš„åå·®-æ–¹å·®æƒè¡¡ç›®æ ‡å‡½æ•°ï¼Œä»¥æœ€ä½³åœ°æ§åˆ¶å¼ºåŒ–å­¦ä¹ ä¿¡å·å¯¹FlowæŸå¤±çš„å½±å“ã€‚ARFMè‡ªé€‚åº”åœ°å¹³è¡¡äº†å¼ºåŒ–å­¦ä¹ ä¼˜åŠ¿çš„ä¿ç•™å’ŒFlowæŸå¤±æ¢¯åº¦æ–¹å·®çš„æ§åˆ¶ï¼Œä»è€Œå®ç°äº†æ›´ç¨³å®šå’Œé«˜æ•ˆçš„å¾®è°ƒè¿‡ç¨‹ã€‚å¤§é‡çš„æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒç»“æœè¡¨æ˜ï¼ŒARFMè¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–æ€§ã€é²æ£’æ€§ã€å°‘æ ·æœ¬å­¦ä¹ å’ŒæŒç»­å­¦ä¹ æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³VLA Flowæ¨¡å‹åœ¨å¤æ‚æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­åŠ¨ä½œç²¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ¨¡ä»¿å­¦ä¹ ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®ä¸­çš„å¼ºåŒ–å­¦ä¹ ä¿¡å·ï¼Œå¯¼è‡´æ¨¡å‹å¯¹æ•°æ®è´¨é‡çš„ç†è§£ä¸è¶³ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ å¯¹VLA Flowæ¨¡å‹è¿›è¡Œåè®­ç»ƒå¾®è°ƒï¼Œåˆ©ç”¨ç¦»çº¿æ•°æ®ä¸­çš„å¥–åŠ±ä¿¡å·æ¥æå‡æ¨¡å‹çš„ç­–ç•¥ã€‚å…³é”®åœ¨äºå¦‚ä½•å¹³è¡¡æ¨¡ä»¿å­¦ä¹ çš„FlowæŸå¤±å’Œå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿å‡½æ•°ï¼Œé¿å…å¼ºåŒ–å­¦ä¹ ä¿¡å·å¼•å…¥è¿‡å¤§çš„æ–¹å·®ï¼Œå½±å“æ¨¡å‹çš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šARFMç®—æ³•çš„æ•´ä½“æ¡†æ¶æ˜¯åœ¨VLA Flowæ¨¡å‹çš„Flow MatchingæŸå¤±å‡½æ•°ä¸­å¼•å…¥ä¸€ä¸ªè‡ªé€‚åº”è°ƒæ•´çš„ç¼©æ”¾å› å­ã€‚è¯¥ç¼©æ”¾å› å­æ ¹æ®å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿å‡½æ•°è¿›è¡Œè°ƒæ•´ï¼Œä»è€Œæ§åˆ¶å¼ºåŒ–å­¦ä¹ ä¿¡å·å¯¹FlowæŸå¤±çš„å½±å“ã€‚ç®—æ³•é¦–å…ˆä½¿ç”¨ç¦»çº¿æ•°æ®é›†è®­ç»ƒä¸€ä¸ªåˆå§‹çš„VLA Flowæ¨¡å‹ï¼Œç„¶åä½¿ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚Q-learningæˆ–Actor-Criticï¼‰ä¼°è®¡ä¼˜åŠ¿å‡½æ•°ï¼Œæœ€åä½¿ç”¨ARFMç®—æ³•å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šARFMçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”çš„åå·®-æ–¹å·®æƒè¡¡ç›®æ ‡å‡½æ•°ã€‚é€šè¿‡è‡ªé€‚åº”åœ°è°ƒæ•´ç¼©æ”¾å› å­ï¼ŒARFMèƒ½å¤Ÿåœ¨ä¿ç•™å¼ºåŒ–å­¦ä¹ ä¼˜åŠ¿çš„åŒæ—¶ï¼Œæ§åˆ¶FlowæŸå¤±çš„æ¢¯åº¦æ–¹å·®ï¼Œä»è€Œå®ç°æ›´ç¨³å®šå’Œé«˜æ•ˆçš„å¾®è°ƒè¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒARFMèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æ¨¡ä»¿å­¦ä¹ çš„å…ˆéªŒçŸ¥è¯†ï¼Œé¿å…äº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šARFMçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è‡ªé€‚åº”ç¼©æ”¾å› å­çš„è®¡ç®—æ–¹æ³•ï¼Œè¯¥å› å­åŸºäºä¼˜åŠ¿å‡½æ•°çš„ä¼°è®¡å€¼ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼›2) Flow MatchingæŸå¤±å‡½æ•°çš„å…·ä½“å½¢å¼ï¼Œé€šå¸¸é‡‡ç”¨L2æŸå¤±æˆ–äº¤å‰ç†µæŸå¤±ï¼›3) ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•åˆé€‚çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥ä¼°è®¡ä¼˜åŠ¿å‡½æ•°ï¼Œå¦‚Conservative Q-Learning (CQL)æˆ–Batch-Constrained deep Q-learning (BCQ)ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å–å†³äºVLA Flowæ¨¡å‹çš„å…·ä½“å®ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒARFMåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œç¯å¢ƒä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å¤šä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒARFMçš„æˆåŠŸç‡æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†10%-20%ã€‚æ­¤å¤–ï¼ŒARFMè¿˜è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§ã€é²æ£’æ€§ã€å°‘æ ·æœ¬å­¦ä¹ å’ŒæŒç»­å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„ç¯å¢ƒå’Œä»»åŠ¡ä¸­ç¨³å®šè¿è¡Œã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–åŸºäºFlow Matchingçš„ç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚å›¾åƒç”Ÿæˆã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models based on flow matching have shown excellent performance in general-purpose robotic manipulation tasks. However, the action accuracy of these models on complex downstream tasks is unsatisfactory. One important reason is that these models rely solely on the post-training paradigm of imitation learning, which makes it difficult to have a deeper understanding of the distribution properties of data quality, which is exactly what Reinforcement Learning (RL) excels at. In this paper, we theoretically propose an offline RL post-training objective for VLA flow models and induce an efficient and feasible offline RL fine-tuning algorithm -- Adaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted scaling factor in the VLA flow model loss, we construct a principled bias-variance trade-off objective function to optimally control the impact of RL signal on flow loss. ARFM adaptively balances RL advantage preservation and flow loss gradient variance control, resulting in a more stable and efficient fine-tuning process. Extensive simulation and real-world experimental results show that ARFM exhibits excellent generalization, robustness, few-shot learning, and continuous learning performance.

