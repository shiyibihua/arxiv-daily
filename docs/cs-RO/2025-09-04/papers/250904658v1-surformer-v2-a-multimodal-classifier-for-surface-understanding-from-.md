---
layout: default
title: Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision
---

# Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04658" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04658v1</a>
  <a href="https://arxiv.org/pdf/2509.04658.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04658v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04658v1', 'Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Manish Kansana, Sindhuja Penchala, Shahram Rahimi, Noorbakhsh Amiri Golilarz

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: 6 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Surformer v2ï¼šç”¨äºè§¦è§‰ä¸è§†è§‰è¡¨é¢ç†è§£çš„å¤šæ¨¡æ€åˆ†ç±»å™¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `è§¦è§‰æ„ŸçŸ¥` `è¡¨é¢ç†è§£` `æœºå™¨äººæ“ä½œ` `å†³ç­–çº§èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æœºå™¨äººæ“ä½œä¸­ï¼Œå¤šæ¨¡æ€è¡¨é¢æè´¨åˆ†ç±»è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨ç‰¹å¾æå–å’Œèåˆæ–¹é¢å­˜åœ¨å±€é™ã€‚
2. Surformer v2é‡‡ç”¨åæœŸèåˆç­–ç•¥ï¼Œåˆ©ç”¨CNNå’ŒTransformeråˆ†åˆ«æå–è§†è§‰å’Œè§¦è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡å¯å­¦ä¹ æƒé‡è¿›è¡Œå†³ç­–èåˆã€‚
3. åœ¨Touch and Goæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSurformer v2åœ¨ä¿æŒå®æ—¶æ€§çš„å‰æä¸‹ï¼Œå®ç°äº†è‰¯å¥½çš„è¡¨é¢ç†è§£æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºSurformer v2ï¼Œä¸€ç§å¢å¼ºçš„å¤šæ¨¡æ€åˆ†ç±»æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡åæœŸï¼ˆå†³ç­–çº§ï¼‰èåˆæœºåˆ¶æ•´åˆè§†è§‰å’Œè§¦è§‰æ„Ÿå®˜æ•°æ®æµã€‚è¯¥æ¨¡å‹åŸºäºå…ˆå‰çš„Surformer v1æ¡†æ¶ï¼Œä½†å°†ç‰¹å¾æå–è¿‡ç¨‹é›†æˆåˆ°æ¨¡å‹å†…éƒ¨ï¼Œå¹¶è½¬å‘åæœŸèåˆã€‚è§†è§‰åˆ†æ”¯é‡‡ç”¨åŸºäºCNNçš„åˆ†ç±»å™¨ï¼ˆEfficient V-Netï¼‰ï¼Œè€Œè§¦è§‰åˆ†æ”¯é‡‡ç”¨ä»…ç¼–ç å™¨Transformeræ¨¡å‹ï¼Œä½¿æ¯ä¸ªæ¨¡æ€èƒ½å¤Ÿæå–é’ˆå¯¹åˆ†ç±»ä¼˜åŒ–çš„æ¨¡æ€ç‰¹å®šç‰¹å¾ã€‚æ¨¡å‹ä¸åˆå¹¶ç‰¹å¾å›¾ï¼Œè€Œæ˜¯é€šè¿‡ä½¿ç”¨å¯å­¦ä¹ çš„åŠ æƒå’Œç»„åˆè¾“å‡ºlogitsæ¥æ‰§è¡Œå†³ç­–çº§èåˆï¼Œä»è€Œèƒ½å¤Ÿæ ¹æ®æ•°æ®ä¸Šä¸‹æ–‡å’Œè®­ç»ƒåŠ¨æ€è‡ªé€‚åº”åœ°å¼ºè°ƒæ¯ä¸ªæ¨¡æ€ã€‚åœ¨Touch and Goæ•°æ®é›†ä¸Šè¯„ä¼°äº†Surformer v2ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¡¨é¢å›¾åƒå’Œç›¸åº”è§¦è§‰ä¼ æ„Ÿå™¨è¯»æ•°çš„å¤šæ¨¡æ€åŸºå‡†ã€‚ç»“æœè¡¨æ˜ï¼ŒSurformer v2è¡¨ç°è‰¯å¥½ï¼Œä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„æ¨ç†é€Ÿåº¦ï¼Œé€‚ç”¨äºå®æ—¶æœºå™¨äººåº”ç”¨ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å†³ç­–çº§èåˆå’ŒåŸºäºTransformerçš„è§¦è§‰å»ºæ¨¡åœ¨å¢å¼ºå¤šæ¨¡æ€æœºå™¨äººæ„ŸçŸ¥ä¸­çš„è¡¨é¢ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººè§¦è§‰æ„ŸçŸ¥ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆèåˆè§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼Œä»¥å®ç°å‡†ç¡®çš„è¡¨é¢æè´¨åˆ†ç±»é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚Surformer v1ï¼Œä¾èµ–æ‰‹å·¥ç‰¹å¾æå–å’Œä¸­é—´å±‚èåˆï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç—›ç‚¹åœ¨äºå¦‚ä½•è‡ªåŠ¨å­¦ä¹ æ¨¡æ€ç‰¹å®šç‰¹å¾ï¼Œå¹¶è‡ªé€‚åº”åœ°èåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSurformer v2çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨åæœŸèåˆï¼ˆå†³ç­–çº§èåˆï¼‰ç­–ç•¥ï¼Œåˆ†åˆ«è®­ç»ƒè§†è§‰å’Œè§¦è§‰æ¨¡æ€çš„ç‰¹å¾æå–å™¨ï¼Œç„¶åé€šè¿‡å¯å­¦ä¹ çš„æƒé‡ç»„åˆå®ƒä»¬çš„åˆ†ç±»ç»“æœã€‚è¿™ç§æ–¹æ³•å…è®¸æ¯ä¸ªæ¨¡æ€ç‹¬ç«‹åœ°å­¦ä¹ æœ€ä¼˜ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶æ ¹æ®æ•°æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ¯ä¸ªæ¨¡æ€çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSurformer v2åŒ…å«ä¸¤ä¸ªä¸»è¦åˆ†æ”¯ï¼šè§†è§‰åˆ†æ”¯å’Œè§¦è§‰åˆ†æ”¯ã€‚è§†è§‰åˆ†æ”¯ä½¿ç”¨Efficient V-Netï¼Œä¸€ä¸ªåŸºäºCNNçš„åˆ†ç±»å™¨ï¼Œç”¨äºæå–å›¾åƒç‰¹å¾ã€‚è§¦è§‰åˆ†æ”¯ä½¿ç”¨ä¸€ä¸ªä»…ç¼–ç å™¨çš„Transformeræ¨¡å‹ï¼Œç”¨äºå¤„ç†è§¦è§‰ä¼ æ„Ÿå™¨æ•°æ®ã€‚ä¸¤ä¸ªåˆ†æ”¯åˆ†åˆ«è¾“å‡ºlogitsï¼Œç„¶åé€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„åŠ æƒå’Œè¿›è¡Œèåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„åˆ†ç±»ç»“æœã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šSurformer v2çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å°†ç‰¹å¾æå–é›†æˆåˆ°æ¨¡å‹å†…éƒ¨ï¼Œé¿å…äº†æ‰‹å·¥ç‰¹å¾æå–çš„å±€é™æ€§ï¼›2) é‡‡ç”¨åæœŸèåˆç­–ç•¥ï¼Œå…è®¸æ¯ä¸ªæ¨¡æ€ç‹¬ç«‹å­¦ä¹ æœ€ä¼˜ç‰¹å¾è¡¨ç¤ºï¼›3) ä½¿ç”¨å¯å­¦ä¹ çš„æƒé‡è¿›è¡Œå†³ç­–çº§èåˆï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªæ¨¡æ€çš„é‡è¦æ€§ã€‚ä¸Surformer v1ç›¸æ¯”ï¼ŒSurformer v2æ›´åŠ çµæ´»å’Œé«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šè§†è§‰åˆ†æ”¯é‡‡ç”¨Efficient V-Netï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„CNNæ¶æ„ï¼Œé€‚åˆå®æ—¶åº”ç”¨ã€‚è§¦è§‰åˆ†æ”¯çš„Transformeræ¨¡å‹ä½¿ç”¨æ ‡å‡†çš„Transformerç¼–ç å™¨ç»“æ„ï¼Œå…·ä½“å‚æ•°è®¾ç½®ï¼ˆå¦‚å±‚æ•°ã€å¤´æ•°ç­‰ï¼‰æœªçŸ¥ã€‚å†³ç­–çº§èåˆçš„æƒé‡æ˜¯å¯å­¦ä¹ çš„ï¼Œé€šè¿‡åå‘ä¼ æ’­è¿›è¡Œä¼˜åŒ–ã€‚æŸå¤±å‡½æ•°æœªçŸ¥ï¼Œä½†æ¨æµ‹æ˜¯æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Surformer v2åœ¨Touch and Goæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶æ€§èƒ½è‰¯å¥½ï¼Œå¹¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„æ¨ç†é€Ÿåº¦ï¼Œé€‚åˆå®æ—¶æœºå™¨äººåº”ç”¨ã€‚è™½ç„¶è®ºæ–‡ä¸­æ²¡æœ‰æä¾›å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿ï¼Œä½†å¼ºè°ƒäº†è¯¥æ¨¡å‹åœ¨å†³ç­–çº§èåˆå’ŒåŸºäºTransformerçš„è§¦è§‰å»ºæ¨¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä¸Surformer v1ç›¸æ¯”ï¼ŒSurformer v2åœ¨ç‰¹å¾æå–å’Œèåˆæ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼Œé¢„è®¡æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†å…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Surformer v2åœ¨æœºå™¨äººæ“ä½œã€ç‰©ä½“è¯†åˆ«ã€ææ–™åˆ†ç±»ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æ¨¡å‹è¯†åˆ«ä¸åŒæè´¨çš„ç‰©ä½“ï¼Œä»è€Œè¿›è¡Œæ›´ç²¾ç¡®çš„æŠ“å–å’Œæ“ä½œã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å¯ä»¥åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´å¤æ‚çš„èåˆç­–ç•¥å’Œæ›´å¼ºå¤§çš„ç‰¹å¾æå–å™¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal surface material classification plays a critical role in advancing tactile perception for robotic manipulation and interaction. In this paper, we present Surformer v2, an enhanced multi-modal classification architecture designed to integrate visual and tactile sensory streams through a late(decision level) fusion mechanism. Building on our earlier Surformer v1 framework [1], which employed handcrafted feature extraction followed by mid-level fusion architecture with multi-head cross-attention layers, Surformer v2 integrates the feature extraction process within the model itself and shifts to late fusion. The vision branch leverages a CNN-based classifier(Efficient V-Net), while the tactile branch employs an encoder-only transformer model, allowing each modality to extract modality-specific features optimized for classification. Rather than merging feature maps, the model performs decision-level fusion by combining the output logits using a learnable weighted sum, enabling adaptive emphasis on each modality depending on data context and training dynamics. We evaluate Surformer v2 on the Touch and Go dataset [2], a multi-modal benchmark comprising surface images and corresponding tactile sensor readings. Our results demonstrate that Surformer v2 performs well, maintaining competitive inference speed, suitable for real-time robotic applications. These findings underscore the effectiveness of decision-level fusion and transformer-based tactile modeling for enhancing surface understanding in multi-modal robotic perception.

