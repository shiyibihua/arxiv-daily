---
layout: default
title: Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving
---

# Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04712" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04712v1</a>
  <a href="https://arxiv.org/pdf/2509.04712.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04712v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04712v1', 'Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhihao Zhang, Chengyang Peng, Ekim Yurtsever, Keith A. Redmill

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨æ¬¡ä¼˜ç­–ç•¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ ï¼Œæå‡è‡ªåŠ¨é©¾é©¶ç­–ç•¥è®­ç»ƒæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨é©¾é©¶` `æ¬¡ä¼˜ç­–ç•¥` `è½¯æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•` `è½¦é“å˜æ¢` `å†³ç­–æ§åˆ¶` `æ™ºèƒ½ä½“æ¢ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨é©¾é©¶æ§åˆ¶ä¸­é¢ä¸´æ ·æœ¬æ•ˆç‡ä½å’Œæ¢ç´¢å›°éš¾ç­‰æŒ‘æˆ˜ï¼Œéš¾ä»¥æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨æ¬¡ä¼˜çš„æ¼”ç¤ºç­–ç•¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œè¾…åŠ©æ¢ç´¢å¹¶åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚
3. é€šè¿‡å°†åŸºäºè§„åˆ™çš„è½¦é“å˜æ¢æ§åˆ¶å™¨ä¸SACç®—æ³•ç»“åˆï¼Œå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡é©¾é©¶æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨æ¬¡ä¼˜ç­–ç•¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“è¿›è¡Œè‡ªåŠ¨é©¾é©¶æ§åˆ¶çš„æ–¹æ³•ã€‚ç”±äºRLæ™ºèƒ½ä½“åœ¨è®­ç»ƒä¸­é¢ä¸´æ ·æœ¬æ•ˆç‡å’Œæœ‰æ•ˆæ¢ç´¢çš„æŒ‘æˆ˜ï¼Œéš¾ä»¥å‘ç°æœ€ä¼˜é©¾é©¶ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨æ¼”ç¤ºç­–ç•¥æ¥å¼•å¯¼RLé©¾é©¶æ™ºèƒ½ä½“ï¼Œè¯¥ç­–ç•¥ä¸éœ€è¦æ˜¯é«˜åº¦ä¼˜åŒ–æˆ–ä¸“å®¶çº§åˆ«çš„æ§åˆ¶å™¨ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†åŸºäºè§„åˆ™çš„è½¦é“å˜æ¢æ§åˆ¶å™¨ä¸è½¯æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆSACï¼‰ç®—æ³•ç›¸ç»“åˆï¼Œä»¥å¢å¼ºæ¢ç´¢å’Œå­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†é©¾é©¶æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é©¾é©¶åœºæ™¯ï¼Œè¿™äº›åœºæ™¯åŒæ ·å¯ä»¥ä»åŸºäºæ¼”ç¤ºçš„æŒ‡å¯¼ä¸­å—ç›Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè‡ªåŠ¨é©¾é©¶ä¸­çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒé¢ä¸´æ ·æœ¬æ•ˆç‡ä½å’Œæ¢ç´¢ç©ºé—´å¤§çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡çš„ç¯å¢ƒäº¤äº’æ‰èƒ½å­¦ä¹ åˆ°æœ‰æ•ˆçš„é©¾é©¶ç­–ç•¥ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­æ˜¯ä¸å¯è¡Œçš„ã€‚æ­¤å¤–ï¼Œæœ‰æ•ˆçš„æ¢ç´¢ç­–ç•¥å¯¹äºå‘ç°æœ€ä¼˜é©¾é©¶è¡Œä¸ºè‡³å…³é‡è¦ï¼Œä½†è®¾è®¡æœ‰æ•ˆçš„æ¢ç´¢ç­–ç•¥éå¸¸å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¬¡ä¼˜çš„æ¼”ç¤ºç­–ç•¥æ¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„æ¢ç´¢è¿‡ç¨‹ã€‚é€šè¿‡æä¾›ä¸€äº›å¯è¡Œçš„é©¾é©¶è¡Œä¸ºç¤ºä¾‹ï¼Œå¯ä»¥ç¼©å°æ¢ç´¢ç©ºé—´ï¼Œå¹¶å¸®åŠ©æ™ºèƒ½ä½“æ›´å¿«åœ°å­¦ä¹ åˆ°æœ‰ç”¨çš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºäººç±»å­¦ä¹ é©¾é©¶æ—¶ï¼Œæ•™ç»ƒä¼šæä¾›ä¸€äº›åŸºæœ¬çš„é©¾é©¶æŠ€å·§å’Œè§„åˆ™ï¼Œå¸®åŠ©å­¦å‘˜æ›´å¿«åœ°ä¸Šæ‰‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•å°†ä¸€ä¸ªåŸºäºè§„åˆ™çš„è½¦é“å˜æ¢æ§åˆ¶å™¨ä¸è½¯æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆSACï¼‰ç®—æ³•ç›¸ç»“åˆã€‚åŸºäºè§„åˆ™çš„æ§åˆ¶å™¨æä¾›æ¬¡ä¼˜çš„é©¾é©¶ç­–ç•¥ä½œä¸ºæ¼”ç¤ºï¼ŒSACç®—æ³•åˆ™è´Ÿè´£åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œå­¦ä¹ åˆ°æ›´ä¼˜çš„é©¾é©¶ç­–ç•¥ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆï¼ŒåŸºäºè§„åˆ™çš„æ§åˆ¶å™¨ç”Ÿæˆä¸€äº›é©¾é©¶è¡Œä¸ºæ•°æ®ï¼›ç„¶åï¼ŒSACç®—æ³•åˆ©ç”¨è¿™äº›æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ åˆ°ä¸€ä¸ªåˆæ­¥çš„é©¾é©¶ç­–ç•¥ï¼›æœ€åï¼ŒSACç®—æ³•ç»§ç»­ä¸ç¯å¢ƒäº¤äº’ï¼Œä¸æ–­ä¼˜åŒ–é©¾é©¶ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨æ¬¡ä¼˜ç­–ç•¥æ¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¤§é‡çš„ç¯å¢ƒäº¤äº’ï¼Œå¯ä»¥æ›´å¿«åœ°å­¦ä¹ åˆ°æœ‰æ•ˆçš„é©¾é©¶ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥é¿å…æ™ºèƒ½ä½“é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ï¼Œä»è€Œæ‰¾åˆ°æ›´ä¼˜çš„é©¾é©¶ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†è½¯æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆSACï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§off-policyçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå…·æœ‰è¾ƒå¥½çš„ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªåŸºäºè§„åˆ™çš„è½¦é“å˜æ¢æ§åˆ¶å™¨ï¼Œç”¨äºç”Ÿæˆæ¬¡ä¼˜çš„é©¾é©¶ç­–ç•¥ã€‚è¯¥æ§åˆ¶å™¨æ ¹æ®è½¦è¾†çš„é€Ÿåº¦ã€ä½ç½®å’Œå‘¨å›´è½¦è¾†çš„ä¿¡æ¯ï¼Œå†³å®šæ˜¯å¦è¿›è¡Œè½¦é“å˜æ¢ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°æœ‰æ•ˆçš„é©¾é©¶ç­–ç•¥ï¼Œå¹¶æ˜¾è‘—æé«˜é©¾é©¶æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨æ‘˜è¦ä¸­æ²¡æœ‰æ˜ç¡®ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†å¯ä»¥æ¨æ–­ï¼Œè¯¥æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œé©¾é©¶å®‰å…¨æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å†³ç­–æ§åˆ¶ç³»ç»Ÿï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚äº¤é€šåœºæ™¯ä¸‹çš„è½¦é“å˜æ¢ã€æ±‡å…¥æ±‡å‡ºç­‰é©¾é©¶ä»»åŠ¡ä¸­ã€‚é€šè¿‡åˆ©ç”¨æ¬¡ä¼˜ç­–ç•¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„è½åœ°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¼ºåŒ–å­¦ä¹ çš„æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æ— äººæœºå¯¼èˆªã€æœºæ¢°è‡‚æ“ä½œç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automated vehicle control using reinforcement learning (RL) has attracted significant attention due to its potential to learn driving policies through environment interaction. However, RL agents often face training challenges in sample efficiency and effective exploration, making it difficult to discover an optimal driving strategy. To address these issues, we propose guiding the RL driving agent with a demonstration policy that need not be a highly optimized or expert-level controller. Specifically, we integrate a rule-based lane change controller with the Soft Actor Critic (SAC) algorithm to enhance exploration and learning efficiency. Our approach demonstrates improved driving performance and can be extended to other driving scenarios that can similarly benefit from demonstration-based guidance.

