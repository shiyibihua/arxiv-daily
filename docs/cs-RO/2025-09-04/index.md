---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-09-04
---

# cs.ROï¼ˆ2025-09-04ï¼‰

ğŸ“Š å…± **19** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (12 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250904443v2-emma-scaling-mobile-manipulation-via-egocentric-human-data.html">EMMA: Scaling Mobile Manipulation via Egocentric Human Data</a></td>
  <td>EMMAï¼šåˆ©ç”¨ä»¥äººä¸ºä¸­å¿ƒçš„è§†è§‰æ•°æ®æ‰©å±•ç§»åŠ¨æ“ä½œæ¨¡ä»¿å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span> <span class="paper-tag">teleoperation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04443v2" data-paper-url="./papers/250904443v2-emma-scaling-mobile-manipulation-via-egocentric-human-data.html" onclick="toggleFavorite(this, '2509.04443v2', 'EMMA: Scaling Mobile Manipulation via Egocentric Human Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250904063v1-balancing-signal-and-variance-adaptive-offline-rl-post-training-for-.html">Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models</a></td>
  <td>æå‡ºARFMï¼Œé€šè¿‡è‡ªé€‚åº”ç¦»çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒVLA Flowæ¨¡å‹ï¼Œæå‡æœºå™¨äººæ“ä½œä»»åŠ¡ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">offline RL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04063v1" data-paper-url="./papers/250904063v1-balancing-signal-and-variance-adaptive-offline-rl-post-training-for-.html" onclick="toggleFavorite(this, '2509.04063v1', 'Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250904018v2-fpc-vla-a-vision-language-action-framework-with-a-supervisor-for-fai.html">FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction</a></td>
  <td>æå‡ºFPC-VLAæ¡†æ¶ï¼Œç”¨äºæœºå™¨äººæ“ä½œä¸­é¢„æµ‹å’Œçº æ­£å¤±è´¥</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04018v2" data-paper-url="./papers/250904018v2-fpc-vla-a-vision-language-action-framework-with-a-supervisor-for-fai.html" onclick="toggleFavorite(this, '2509.04018v2', 'FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250904069v1-solving-robotics-tasks-with-prior-demonstration-via-exploration-effi.html">Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning</a></td>
  <td>æå‡ºä¸€ç§æ¢ç´¢é«˜æ•ˆçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶DRLRï¼Œé€šè¿‡å…ˆéªŒæ¼”ç¤ºè§£å†³æœºå™¨äººä»»åŠ¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04069v1" data-paper-url="./papers/250904069v1-solving-robotics-tasks-with-prior-demonstration-via-exploration-effi.html" onclick="toggleFavorite(this, '2509.04069v1', 'Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250903889v1-reactive-in-air-clothing-manipulation-with-confidence-aware-dense-co.html">Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance</a></td>
  <td>æå‡ºåŸºäºç½®ä¿¡åº¦æ„ŸçŸ¥ç¨ å¯†å¯¹åº”å’Œè§¦è§‰åé¦ˆçš„ç©ºä¸­æœè£…æ“ä½œæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dual-arm</span> <span class="paper-tag">affordance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03889v1" data-paper-url="./papers/250903889v1-reactive-in-air-clothing-manipulation-with-confidence-aware-dense-co.html" onclick="toggleFavorite(this, '2509.03889v1', 'Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250904658v1-surformer-v2-a-multimodal-classifier-for-surface-understanding-from-.html">Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision</a></td>
  <td>Surformer v2ï¼šç”¨äºè§¦è§‰ä¸è§†è§‰è¡¨é¢ç†è§£çš„å¤šæ¨¡æ€åˆ†ç±»å™¨</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04658v1" data-paper-url="./papers/250904658v1-surformer-v2-a-multimodal-classifier-for-surface-understanding-from-.html" onclick="toggleFavorite(this, '2509.04658v1', 'Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250904441v2-dexop-a-device-for-robotic-transfer-of-dexterous-human-manipulation.html">DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation</a></td>
  <td>DEXOPï¼šä¸€ç§ç”¨äºæœºå™¨äººçµå·§æ“ä½œè¿ç§»çš„è®¾å¤‡</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous manipulation</span> <span class="paper-tag">teleoperation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04441v2" data-paper-url="./papers/250904441v2-dexop-a-device-for-robotic-transfer-of-dexterous-human-manipulation.html" onclick="toggleFavorite(this, '2509.04441v2', 'DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250903859v3-learning-multi-stage-pick-and-place-with-a-legged-mobile-manipulator.html">Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šé˜¶æ®µç§»åŠ¨æ“ä½œç­–ç•¥ï¼Œè§£å†³å››è¶³æœºå™¨äººå¤æ‚æ“ä½œä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03859v3" data-paper-url="./papers/250903859v3-learning-multi-stage-pick-and-place-with-a-legged-mobile-manipulator.html" onclick="toggleFavorite(this, '2509.03859v3', 'Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250904094v1-object-reconstruction-aware-whole-body-control-of-mobile-manipulator.html">Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators</a></td>
  <td>æå‡ºåŸºäºç›®æ ‡é‡å»ºæ„ŸçŸ¥çš„ç§»åŠ¨æœºæ¢°è‡‚å…¨èº«æ§åˆ¶æ–¹æ³•ï¼Œæå‡ä¸‰ç»´é‡å»ºæ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">whole-body control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04094v1" data-paper-url="./papers/250904094v1-object-reconstruction-aware-whole-body-control-of-mobile-manipulator.html" onclick="toggleFavorite(this, '2509.04094v1', 'Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250904076v2-keypoint-based-diffusion-for-robotic-motion-planning-on-the-nicol-ro.html">Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot</a></td>
  <td>æå‡ºåŸºäºå…³é”®ç‚¹çš„æ‰©æ•£æ¨¡å‹ï¼ŒåŠ é€ŸNICOLæœºå™¨äººè¿åŠ¨è§„åˆ’ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04076v2" data-paper-url="./papers/250904076v2-keypoint-based-diffusion-for-robotic-motion-planning-on-the-nicol-ro.html" onclick="toggleFavorite(this, '2509.04076v2', 'Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250904645v1-planning-from-point-clouds-over-continuous-actions-for-multi-object-.html">Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement</a></td>
  <td>æå‡ºSPOTï¼šä¸€ç§åŸºäºç‚¹äº‘å˜æ¢æœç´¢çš„å¤šç‰©ä½“é‡æ’åˆ—è§„åˆ’æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04645v1" data-paper-url="./papers/250904645v1-planning-from-point-clouds-over-continuous-actions-for-multi-object-.html" onclick="toggleFavorite(this, '2509.04645v1', 'Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250904119v1-lightweight-kinematic-and-static-modeling-of-cable-driven-continuum-.html">Lightweight Kinematic and Static Modeling of Cable-Driven Continuum Robots via Actuation-Space Energy Formulation</a></td>
  <td>æå‡ºè½»é‡çº§é©±åŠ¨ç©ºé—´èƒ½é‡å»ºæ¨¡æ¡†æ¶ï¼Œç”¨äºç¼†ç´¢é©±åŠ¨è¿ç»­ä½“æœºå™¨äººçš„è¿åŠ¨å­¦å’Œé™æ€å»ºæ¨¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04119v1" data-paper-url="./papers/250904119v1-lightweight-kinematic-and-static-modeling-of-cable-driven-continuum-.html" onclick="toggleFavorite(this, '2509.04119v1', 'Lightweight Kinematic and Static Modeling of Cable-Driven Continuum Robots via Actuation-Space Energy Formulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250905368v2-long-horizon-visual-imitation-learning-via-plan-and-code-reflection.html">Long-Horizon Visual Imitation Learning via Plan and Code Reflection</a></td>
  <td>æå‡ºåŸºäºè®¡åˆ’ä¸ä»£ç åæ€çš„é•¿æ—¶ç¨‹è§†è§‰æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œè§£å†³å¤æ‚åŠ¨ä½œåºåˆ—çš„æ—¶åºå’Œç©ºé—´å…³ç³»å»ºæ¨¡éš¾é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05368v2" data-paper-url="./papers/250905368v2-long-horizon-visual-imitation-learning-via-plan-and-code-reflection.html" onclick="toggleFavorite(this, '2509.05368v2', 'Long-Horizon Visual Imitation Learning via Plan and Code Reflection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250904712v1-bootstrapping-reinforcement-learning-with-sub-optimal-policies-for-a.html">Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving</a></td>
  <td>åˆ©ç”¨æ¬¡ä¼˜ç­–ç•¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ ï¼Œæå‡è‡ªåŠ¨é©¾é©¶ç­–ç•¥è®­ç»ƒæ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">SAC</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04712v1" data-paper-url="./papers/250904712v1-bootstrapping-reinforcement-learning-with-sub-optimal-policies-for-a.html" onclick="toggleFavorite(this, '2509.04712v1', 'Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250904628v1-action-chunking-with-transformers-for-image-based-spacecraft-guidanc.html">Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control</a></td>
  <td>æå‡ºåŸºäºå˜æ¢å™¨çš„åŠ¨ä½œåˆ†å—æ–¹æ³•ä»¥æå‡èˆªå¤©å™¨å¼•å¯¼ä¸æ§åˆ¶æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04628v1" data-paper-url="./papers/250904628v1-action-chunking-with-transformers-for-image-based-spacecraft-guidanc.html" onclick="toggleFavorite(this, '2509.04628v1', 'Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250904324v1-ovgrasp-open-vocabulary-grasping-assistance-via-multimodal-intent-de.html">OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</a></td>
  <td>OVGraspï¼šé€šè¿‡å¤šæ¨¡æ€æ„å›¾æ£€æµ‹å®ç°å¼€æ”¾è¯æ±‡æŠ“å–è¾…åŠ©</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04324v1" data-paper-url="./papers/250904324v1-ovgrasp-open-vocabulary-grasping-assistance-via-multimodal-intent-de.html" onclick="toggleFavorite(this, '2509.04324v1', 'OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250904016v1-odometry-calibration-and-pose-estimation-of-a-4wis4wid-mobile-wall-c.html">Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall Climbing Robot</a></td>
  <td>é’ˆå¯¹å£é¢çˆ¬è¡Œæœºå™¨äººï¼Œæå‡ºèåˆå¤šæ¨¡æ€ä¿¡æ¯çš„é‡Œç¨‹è®¡æ ‡å®šä¸ä½å§¿ä¼°è®¡æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">visual odometry</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04016v1" data-paper-url="./papers/250904016v1-odometry-calibration-and-pose-estimation-of-a-4wis4wid-mobile-wall-c.html" onclick="toggleFavorite(this, '2509.04016v1', 'Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall Climbing Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250903842v3-ingrid-intelligent-generative-robotic-design-using-large-language-mo.html">INGRID: Intelligent Generative Robotic Design Using Large Language Models</a></td>
  <td>INGRIDï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å®ç°æ™ºèƒ½ç”Ÿæˆå¼æœºå™¨äººè®¾è®¡</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03842v3" data-paper-url="./papers/250903842v3-ingrid-intelligent-generative-robotic-design-using-large-language-mo.html" onclick="toggleFavorite(this, '2509.03842v3', 'INGRID: Intelligent Generative Robotic Design Using Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250904356v1-srwtoolkit-an-open-source-wizard-of-oz-toolkit-to-create-social-robo.html">SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars</a></td>
  <td>SRWToolkitï¼šä¸€ä¸ªå¼€æºçš„ç¤¾ä¼šæœºå™¨äººåŒ–èº«å¿«é€ŸåŸå‹è®¾è®¡å·¥å…·</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04356v1" data-paper-url="./papers/250904356v1-srwtoolkit-an-open-source-wizard-of-oz-toolkit-to-create-social-robo.html" onclick="toggleFavorite(this, '2509.04356v1', 'SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)