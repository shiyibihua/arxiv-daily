---
layout: default
title: Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation
---

# Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00466" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00466v1</a>
  <a href="https://arxiv.org/pdf/2510.00466.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00466v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00466v1', 'Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Run Su, Hao Fu, Shuai Zhou, Yingao Fu

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºRTGé¢„æµ‹çš„ç¦»çº¿-åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºæå‡æœºå™¨äººç¤¾äº¤å¯¼èˆªèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººç¤¾äº¤å¯¼èˆª` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `åœ¨çº¿å¾®è°ƒ` `Return-to-Goé¢„æµ‹` `Transformer` `æ—¶ç©ºèåˆ` `åˆ†å¸ƒåç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººç¤¾äº¤å¯¼èˆªæ–¹æ³•å—é™äºè¡Œäººè¡Œä¸ºçš„ä¸ç¡®å®šæ€§ä»¥åŠè®­ç»ƒæœŸé—´æœ‰é™çš„ç¯å¢ƒäº¤äº’ï¼Œå¯¼è‡´æ¬¡ä¼˜æ¢ç´¢å’Œåˆ†å¸ƒåç§»ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºReturn-to-Goé¢„æµ‹çš„ç¦»çº¿-åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡æ—¶ç©ºèåˆæ¨¡å‹ç²¾ç¡®ä¼°è®¡RTGå€¼ï¼Œç¼“è§£åˆ†å¸ƒåç§»ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç¤¾äº¤å¯¼èˆªç¯å¢ƒä¸­å®ç°äº†æ›´é«˜çš„æˆåŠŸç‡å’Œæ›´ä½çš„ç¢°æ’ç‡ï¼Œæå‡äº†å¯¼èˆªç­–ç•¥çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¦»çº¿åˆ°åœ¨çº¿å¾®è°ƒå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºè§£å†³æœºå™¨äººç¤¾äº¤å¯¼èˆªé—®é¢˜ã€‚è¯¥ç®—æ³•å°†Return-to-Go (RTG) é¢„æµ‹é›†æˆåˆ°å› æœTransformeræ¶æ„ä¸­ã€‚é€šè¿‡è”åˆç¼–ç æ—¶é—´è¡Œäººè¿åŠ¨æ¨¡å¼å’Œç©ºé—´äººç¾¤åŠ¨æ€ï¼Œç®—æ³•ä¸­çš„æ—¶ç©ºèåˆæ¨¡å‹èƒ½å¤Ÿç²¾ç¡®åœ°å®æ—¶ä¼°è®¡RTGå€¼ã€‚è¿™ç§RTGé¢„æµ‹æ¡†æ¶é€šè¿‡å¯¹é½ç¦»çº¿ç­–ç•¥è®­ç»ƒå’Œåœ¨çº¿ç¯å¢ƒäº¤äº’æ¥ç¼“è§£åˆ†å¸ƒåç§»ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†ä¸€ç§æ··åˆç¦»çº¿-åœ¨çº¿ç»éªŒé‡‡æ ·æœºåˆ¶ï¼Œä»¥ç¨³å®šå¾®è°ƒæœŸé—´çš„ç­–ç•¥æ›´æ–°ï¼Œç¡®ä¿é¢„è®­ç»ƒçŸ¥è¯†å’Œå®æ—¶é€‚åº”çš„å¹³è¡¡é›†æˆã€‚åœ¨æ¨¡æ‹Ÿç¤¾äº¤å¯¼èˆªç¯å¢ƒä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ›´é«˜çš„æˆåŠŸç‡å’Œæ›´ä½çš„ç¢°æ’ç‡ã€‚è¿™äº›ç»“æœçªæ˜¾äº†è¯¥ç®—æ³•åœ¨å¢å¼ºå¯¼èˆªç­–ç•¥é²æ£’æ€§å’Œé€‚åº”æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºç°å®ä¸–ç•Œåº”ç”¨ä¸­æ›´å¯é å’Œè‡ªé€‚åº”çš„æœºå™¨äººå¯¼èˆªç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœºå™¨äººç¤¾äº¤å¯¼èˆªä»»åŠ¡æ—¨åœ¨ä½¿æœºå™¨äººåœ¨äººç¾¤ä¸­å®‰å…¨ã€é«˜æ•ˆåœ°å¯¼èˆªã€‚ç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé¢ä¸´ç€è¡Œäººè¡Œä¸ºé¢„æµ‹çš„ä¸ç¡®å®šæ€§ä»¥åŠç¦»çº¿è®­ç»ƒä¸åœ¨çº¿éƒ¨ç½²ä¹‹é—´çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œå¯¼è‡´å¯¼èˆªç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å¹³è¡¡ç¦»çº¿é¢„è®­ç»ƒçš„çŸ¥è¯†å’Œåœ¨çº¿ç¯å¢ƒçš„å®æ—¶é€‚åº”ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Return-to-Go (RTG) é¢„æµ‹æ¥æ¡¥æ¥ç¦»çº¿è®­ç»ƒå’Œåœ¨çº¿å¾®è°ƒä¹‹é—´çš„å·®è·ã€‚é€šè¿‡ç²¾ç¡®é¢„æµ‹RTGå€¼ï¼Œå¯ä»¥æŒ‡å¯¼æœºå™¨äººåœ¨åœ¨çº¿ç¯å¢ƒä¸­çš„æ¢ç´¢ï¼Œå¹¶ç¼“è§£åˆ†å¸ƒåç§»ã€‚åŒæ—¶ï¼Œç»“åˆç¦»çº¿å’Œåœ¨çº¿ç»éªŒï¼Œç¨³å®šç­–ç•¥æ›´æ–°ï¼Œå®ç°çŸ¥è¯†è¿ç§»å’Œå®æ—¶é€‚åº”çš„å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç®—æ³•çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†ï¼š1) åŸºäºå› æœTransformerçš„æ—¶ç©ºèåˆæ¨¡å‹ï¼Œç”¨äºå®æ—¶ä¼°è®¡RTGå€¼ï¼›2) RTGé¢„æµ‹æ¡†æ¶ï¼Œç”¨äºå¯¹é½ç¦»çº¿ç­–ç•¥è®­ç»ƒå’Œåœ¨çº¿ç¯å¢ƒäº¤äº’ï¼›3) æ··åˆç¦»çº¿-åœ¨çº¿ç»éªŒé‡‡æ ·æœºåˆ¶ï¼Œç”¨äºç¨³å®šç­–ç•¥æ›´æ–°ã€‚æ•´ä½“æµç¨‹æ˜¯é¦–å…ˆåˆ©ç”¨ç¦»çº¿æ•°æ®é¢„è®­ç»ƒç­–ç•¥ï¼Œç„¶ååˆ©ç”¨åœ¨çº¿æ•°æ®è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶åˆ©ç”¨RTGé¢„æµ‹æŒ‡å¯¼æ¢ç´¢å’Œç¼“è§£åˆ†å¸ƒåç§»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†Return-to-Go (RTG) é¢„æµ‹é›†æˆåˆ°ç¦»çº¿-åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ï¼Œå¹¶è®¾è®¡äº†æ—¶ç©ºèåˆæ¨¡å‹æ¥ç²¾ç¡®ä¼°è®¡RTGå€¼ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ç¼“è§£åˆ†å¸ƒåç§»ï¼Œå¹¶å®ç°ç¦»çº¿çŸ¥è¯†å’Œåœ¨çº¿é€‚åº”çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæ··åˆç»éªŒé‡‡æ ·æœºåˆ¶ä¹Ÿæå‡äº†ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ—¶ç©ºèåˆæ¨¡å‹é‡‡ç”¨å› æœTransformeræ¶æ„ï¼Œè”åˆç¼–ç æ—¶é—´è¡Œäººè¿åŠ¨æ¨¡å¼å’Œç©ºé—´äººç¾¤åŠ¨æ€ã€‚RTGé¢„æµ‹æ¡†æ¶é€šè¿‡æœ€å°åŒ–é¢„æµ‹RTGå€¼ä¸å®é™…RTGå€¼ä¹‹é—´çš„å·®å¼‚æ¥è®­ç»ƒã€‚æ··åˆç»éªŒé‡‡æ ·æœºåˆ¶æ ¹æ®ä¸€å®šçš„æ¯”ä¾‹ä»ç¦»çº¿å’Œåœ¨çº¿ç»éªŒæ± ä¸­é‡‡æ ·æ•°æ®ï¼Œç”¨äºç­–ç•¥æ›´æ–°ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªæ˜ç¡®è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç¤¾äº¤å¯¼èˆªç¯å¢ƒä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ›´é«˜çš„æˆåŠŸç‡å’Œæ›´ä½çš„ç¢°æ’ç‡ï¼Œè¯æ˜äº†å…¶åœ¨å¢å¼ºå¯¼èˆªç­–ç•¥é²æ£’æ€§å’Œé€‚åº”æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æœºå™¨äººä¸äººäº¤äº’çš„åœºæ™¯ï¼Œä¾‹å¦‚å•†åœºå¯¼è§ˆæœºå™¨äººã€åŒ»é™¢é…é€æœºå™¨äººã€é¤å…æœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚äººç¾¤ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œå¯ä»¥æé«˜æœåŠ¡æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒï¼Œå¹¶é™ä½å®‰å…¨é£é™©ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨æœºå™¨äººæ›´å¹¿æ³›åœ°åº”ç”¨äºæ—¥å¸¸ç”Ÿæ´»å’Œç¤¾ä¼šæœåŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline reinforcement learning (RL) has emerged as a promising framework for addressing robot social navigation challenges. However, inherent uncertainties in pedestrian behavior and limited environmental interaction during training often lead to suboptimal exploration and distributional shifts between offline training and online deployment. To overcome these limitations, this paper proposes a novel offline-to-online fine-tuning RL algorithm for robot social navigation by integrating Return-to-Go (RTG) prediction into a causal Transformer architecture. Our algorithm features a spatiotem-poral fusion model designed to precisely estimate RTG values in real-time by jointly encoding temporal pedestrian motion patterns and spatial crowd dynamics. This RTG prediction framework mitigates distribution shift by aligning offline policy training with online environmental interactions. Furthermore, a hybrid offline-online experience sampling mechanism is built to stabilize policy updates during fine-tuning, ensuring balanced integration of pre-trained knowledge and real-time adaptation. Extensive experiments in simulated social navigation environments demonstrate that our method achieves a higher success rate and lower collision rate compared to state-of-the-art baselines. These results underscore the efficacy of our algorithm in enhancing navigation policy robustness and adaptability. This work paves the way for more reliable and adaptive robotic navigation systems in real-world applications.

