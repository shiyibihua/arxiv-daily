---
layout: default
title: VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators
---

# VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00406" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00406v1</a>
  <a href="https://arxiv.org/pdf/2510.00406.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00406v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00406v1', 'VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hengtao Li, Pengxiang Ding, Runze Suo, Yihao Wang, Zirui Ge, Dongyuan Zang, Kexian Yu, Mingyang Sun, Hongyin Zhang, Donglin Wang, Weihua Su

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://vla-rft.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VLA-RFTï¼šåŸºäºä¸–ç•Œæ¨¡å‹å’ŒéªŒè¯å¥–åŠ±çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œå¼ºåŒ–å¾®è°ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•Œæ¨¡å‹` `å…·èº«æ™ºèƒ½` `æœºå™¨äºº` `æ¨¡ä»¿å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. VLAæ¨¡å‹ä¾èµ–æ¨¡ä»¿å­¦ä¹ ï¼Œæ˜“å—ç´¯ç§¯è¯¯å·®å’Œåˆ†å¸ƒåç§»å½±å“ï¼Œé²æ£’æ€§ä¸è¶³ã€‚
2. VLA-RFTåˆ©ç”¨æ•°æ®é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹ä½œä¸ºæ¨¡æ‹Ÿå™¨ï¼Œé€šè¿‡å¼ºåŒ–å¾®è°ƒæå‡ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVLA-RFTä»…éœ€å°‘é‡å¾®è°ƒæ­¥éª¤å³å¯è¶…è¶Šç›‘ç£åŸºçº¿ï¼Œå¹¶å…·æœ‰è‰¯å¥½é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹èƒ½å¤Ÿå®ç°å…·èº«å†³ç­–ï¼Œä½†ä¸¥é‡ä¾èµ–æ¨¡ä»¿å­¦ä¹ ï¼Œå¯¼è‡´ç´¯ç§¯è¯¯å·®å’Œåˆ†å¸ƒåç§»ä¸‹çš„é²æ£’æ€§å·®ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†é€šå¸¸éœ€è¦æ˜‚è´µçš„çœŸå®ä¸–ç•Œäº¤äº’æˆ–é¢ä¸´æ¨¡æ‹Ÿåˆ°çœŸå®çš„å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†VLA-RFTï¼Œä¸€ä¸ªå¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ•°æ®é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹ä½œä¸ºå¯æ§çš„æ¨¡æ‹Ÿå™¨ã€‚è¯¥æ¨¡æ‹Ÿå™¨ä»çœŸå®äº¤äº’æ•°æ®ä¸­è®­ç»ƒï¼Œé¢„æµ‹ä»¥åŠ¨ä½œä¸ºæ¡ä»¶çš„æœªæ¥è§†è§‰è§‚å¯Ÿï¼Œä»è€Œå…è®¸ä½¿ç”¨æ¥è‡ªç›®æ ‡å®ç°å‚è€ƒçš„å¯†é›†ã€è½¨è¿¹çº§å¥–åŠ±è¿›è¡Œç­–ç•¥rolloutã€‚è¿™ç§è®¾è®¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ä¸åŠ¨ä½œå¯¹é½çš„å­¦ä¹ ä¿¡å·ï¼Œå¤§å¤§é™ä½äº†æ ·æœ¬éœ€æ±‚ã€‚ç»è¿‡å°‘äº400æ­¥çš„å¾®è°ƒï¼ŒVLA-RFTè¶…è¶Šäº†å¼ºå¤§çš„ç›‘ç£åŸºçº¿ï¼Œå¹¶å®ç°äº†æ¯”åŸºäºæ¨¡æ‹Ÿå™¨çš„RLæ›´é«˜çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ‰°åŠ¨æ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œä¿æŒç¨³å®šçš„ä»»åŠ¡æ‰§è¡Œã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºä¸–ç•Œæ¨¡å‹çš„RFTæ˜¯ä¸€ç§å®ç”¨çš„åè®­ç»ƒèŒƒä¾‹ï¼Œå¯ä»¥å¢å¼ºVLAæ¨¡å‹çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVLA-RFTæ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å…·èº«å†³ç­–ä»»åŠ¡ä¸­ï¼Œç”±äºä¾èµ–æ¨¡ä»¿å­¦ä¹ è€Œå¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›å·®å’Œé²æ£’æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆéœ€è¦å¤§é‡çš„çœŸå®ä¸–ç•Œäº¤äº’æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œè¦ä¹ˆä¾èµ–äºæ¨¡æ‹Ÿå™¨ï¼Œä½†å­˜åœ¨æ¨¡æ‹Ÿåˆ°çœŸå®çš„å·®è·ï¼Œå½±å“å®é™…åº”ç”¨æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLA-RFTçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹ä½œä¸ºå¯æ§çš„æ¨¡æ‹Ÿå™¨ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹VLAæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¯¥ä¸–ç•Œæ¨¡å‹èƒ½å¤Ÿé¢„æµ‹åœ¨ç»™å®šåŠ¨ä½œåºåˆ—ä¸‹çš„æœªæ¥è§†è§‰è§‚å¯Ÿï¼Œä»è€Œå…è®¸åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œç­–ç•¥rolloutï¼Œå¹¶è·å¾—å¯†é›†çš„ã€è½¨è¿¹çº§åˆ«çš„å¥–åŠ±ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨é™ä½æ ·æœ¬å¤æ‚åº¦ï¼Œå¹¶æä¾›ä¸åŠ¨ä½œå¯¹é½çš„æœ‰æ•ˆå­¦ä¹ ä¿¡å·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLA-RFTæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **æ•°æ®é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹**ï¼šè¯¥æ¨¡å‹åŸºäºçœŸå®äº¤äº’æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç”¨äºé¢„æµ‹æœªæ¥è§†è§‰è§‚å¯Ÿã€‚2) **å¥–åŠ±å‡½æ•°**ï¼šåŸºäºç›®æ ‡å®ç°å‚è€ƒï¼Œä¸ºç­–ç•¥rolloutæä¾›å¯†é›†çš„ã€è½¨è¿¹çº§åˆ«çš„å¥–åŠ±ã€‚3) **å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼šåˆ©ç”¨å¥–åŠ±ä¿¡å·å¯¹VLAæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæå‡å…¶å†³ç­–èƒ½åŠ›ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆåˆ©ç”¨ä¸–ç•Œæ¨¡å‹è¿›è¡Œç­–ç•¥rolloutï¼Œç„¶åæ ¹æ®å¥–åŠ±å‡½æ•°è®¡ç®—å¥–åŠ±ï¼Œæœ€ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ›´æ–°VLAæ¨¡å‹çš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šVLA-RFTçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ•°æ®é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹ä¸å¼ºåŒ–å¾®è°ƒç›¸ç»“åˆï¼Œç”¨äºæå‡VLAæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ¨¡æ‹Ÿå™¨çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒVLA-RFTåˆ©ç”¨çœŸå®æ•°æ®è®­ç»ƒä¸–ç•Œæ¨¡å‹ï¼Œä»è€Œå‡å°äº†æ¨¡æ‹Ÿåˆ°çœŸå®çš„å·®è·ã€‚æ­¤å¤–ï¼ŒVLA-RFTé‡‡ç”¨å¯†é›†çš„ã€è½¨è¿¹çº§åˆ«çš„å¥–åŠ±ï¼Œèƒ½å¤Ÿæä¾›æ›´æœ‰æ•ˆçš„å­¦ä¹ ä¿¡å·ã€‚

**å…³é”®è®¾è®¡**ï¼šVLA-RFTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **ä¸–ç•Œæ¨¡å‹çš„é€‰æ‹©**ï¼šè®ºæ–‡ä¸­å…·ä½“çš„ä¸–ç•Œæ¨¡å‹ç»“æ„æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†å…¶æ•°æ®é©±åŠ¨çš„ç‰¹æ€§ã€‚2) **å¥–åŠ±å‡½æ•°çš„è®¾è®¡**ï¼šå¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦ä¸å…·ä½“ä»»åŠ¡ç›¸å…³ï¼Œæ—¨åœ¨å¼•å¯¼ç­–ç•¥æœç€ç›®æ ‡å®ç°çš„æ–¹å‘å‘å±•ã€‚3) **å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨çš„å…·ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†å…¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¯†é›†å¥–åŠ±ä¿¡å·çš„èƒ½åŠ›ã€‚4) **å¾®è°ƒæ­¥éª¤**ï¼šå®éªŒä¸­ä½¿ç”¨äº†å°‘äº400æ­¥çš„å¾®è°ƒï¼Œè¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„æ ·æœ¬æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VLA-RFTåœ¨å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œä»…ç”¨ä¸åˆ°400æ­¥çš„å¾®è°ƒå°±è¶…è¶Šäº†å¼ºå¤§çš„ç›‘ç£å­¦ä¹ åŸºçº¿ï¼Œå¹¶ä¸”æ¯”ä¼ ç»Ÿçš„åŸºäºæ¨¡æ‹Ÿå™¨çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ›´æœ‰æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒVLA-RFTåœ¨å—åˆ°æ‰°åŠ¨çš„æƒ…å†µä¸‹ä¾ç„¶èƒ½å¤Ÿä¿æŒç¨³å®šçš„ä»»åŠ¡æ‰§è¡Œï¼Œå±•ç°äº†å…¶å¼ºå¤§çš„é²æ£’æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŸºäºä¸–ç•Œæ¨¡å‹çš„å¼ºåŒ–å¾®è°ƒæ˜¯ä¸€ç§æœ‰æ•ˆçš„æå‡VLAæ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VLA-RFTå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€ç‰©ä½“æ“ä½œã€äººæœºåä½œç­‰ã€‚é€šè¿‡æå‡VLAæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œå¯ä»¥ä½¿å…¶åœ¨æ›´å¤æ‚çš„çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ç¨³å®šå¯é åœ°æ‰§è¡Œä»»åŠ¡ã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨å…·èº«æ™ºèƒ½çš„å‘å±•ï¼Œå¹¶ä¸ºæ™ºèƒ½æœºå™¨äººåœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨æä¾›æ›´å¼ºå¤§çš„æŠ€æœ¯æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.

