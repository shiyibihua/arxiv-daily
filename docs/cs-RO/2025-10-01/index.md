---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-10-01
---

# cs.ROï¼ˆ2025-10-01ï¼‰

ğŸ“Š å…± **24** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (14 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (14 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251000406v1-vla-rft-vision-language-action-reinforcement-fine-tuning-with-verifi.html">VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators</a></td>
  <td>VLA-RFTï¼šåŸºäºä¸–ç•Œæ¨¡å‹å’ŒéªŒè¯å¥–åŠ±çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œå¼ºåŒ–å¾®è°ƒ</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">imitation learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00406v1" data-paper-url="./papers/251000406v1-vla-rft-vision-language-action-reinforcement-fine-tuning-with-verifi.html" onclick="toggleFavorite(this, '2510.00406v1', 'VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251000425v1-conflict-based-search-as-a-protocol-a-multi-agent-motion-planning-pr.html">Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks</a></td>
  <td>æå‡ºåŸºäºå†²çªæœç´¢åè®®çš„å¤šæ™ºèƒ½ä½“å¼‚æ„è¿åŠ¨è§„åˆ’æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00425v1" data-paper-url="./papers/251000425v1-conflict-based-search-as-a-protocol-a-multi-agent-motion-planning-pr.html" onclick="toggleFavorite(this, '2510.00425v1', 'Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251000600v1-hybrid-training-for-vision-language-action-models.html">Hybrid Training for Vision-Language-Action Models</a></td>
  <td>æå‡ºæ··åˆè®­ç»ƒHyTæ¡†æ¶ï¼ŒåŠ é€Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¨ç†ï¼Œå…¼é¡¾æ€§èƒ½ä¸æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00600v1" data-paper-url="./papers/251000600v1-hybrid-training-for-vision-language-action-models.html" onclick="toggleFavorite(this, '2510.00600v1', 'Hybrid Training for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251000695v2-hamlet-switch-your-vision-language-action-model-into-a-history-aware.html">HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy</a></td>
  <td>HAMLETï¼šå°†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹è½¬åŒ–ä¸ºå†å²æ„ŸçŸ¥ç­–ç•¥ï¼Œæå‡æœºå™¨äººæ“ä½œæ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00695v2" data-paper-url="./papers/251000695v2-hamlet-switch-your-vision-language-action-model-into-a-history-aware.html" onclick="toggleFavorite(this, '2510.00695v2', 'HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251000682v1-shared-object-manipulation-with-a-team-of-collaborative-quadrupeds.html">Shared Object Manipulation with a Team of Collaborative Quadrupeds</a></td>
  <td>æå‡ºåŸºäºè…¿å¼æœºå™¨äººå›¢é˜Ÿçš„å…±äº«ç‰©ä½“æ“ä½œæ–¹æ³•ï¼Œè§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„ç‰©ä½“æ¬è¿é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">manipulation</span> <span class="paper-tag">loco-manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00682v1" data-paper-url="./papers/251000682v1-shared-object-manipulation-with-a-team-of-collaborative-quadrupeds.html" onclick="toggleFavorite(this, '2510.00682v1', 'Shared Object Manipulation with a Team of Collaborative Quadrupeds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251001433v1-afford2act-affordance-guided-automatic-keypoint-selection-for-genera.html">AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation</a></td>
  <td>AFFORD2ACTï¼šæå‡ºåŸºäºå¯ä¾›æ€§çš„è‡ªåŠ¨å…³é”®ç‚¹é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºé€šç”¨ä¸”è½»é‡çº§çš„æœºå™¨äººæ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">affordance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01433v1" data-paper-url="./papers/251001433v1-afford2act-affordance-guided-automatic-keypoint-selection-for-genera.html" onclick="toggleFavorite(this, '2510.01433v1', 'AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251000491v1-from-human-hands-to-robot-arms-manipulation-skills-transfer-via-traj.html">From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment</a></td>
  <td>Traj2Actionï¼šé€šè¿‡è½¨è¿¹å¯¹é½å®ç°äººæ‰‹æ“ä½œæŠ€èƒ½å‘æœºå™¨äººæ‰‹è‡‚çš„è¿ç§»</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">human-to-robot</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00491v1" data-paper-url="./papers/251000491v1-from-human-hands-to-robot-arms-manipulation-skills-transfer-via-traj.html" onclick="toggleFavorite(this, '2510.00491v1', 'From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251001357v1-safe-motion-planning-and-control-using-predictive-and-adaptive-barri.html">Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels</a></td>
  <td>æå‡ºåŸºäºé¢„æµ‹å’Œè‡ªé€‚åº”æ§åˆ¶å±éšœå‡½æ•°çš„è‡ªä¸»æ°´é¢è‰‡å®‰å…¨è¿åŠ¨è§„åˆ’æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01357v1" data-paper-url="./papers/251001357v1-safe-motion-planning-and-control-using-predictive-and-adaptive-barri.html" onclick="toggleFavorite(this, '2510.01357v1', 'Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251000814v1-rtff-random-to-target-fabric-flattening-policy-using-dual-arm-manipu.html">RTFF: Random-to-Target Fabric Flattening Policy using Dual-Arm Manipulator</a></td>
  <td>æå‡ºRTFFç­–ç•¥ï¼Œåˆ©ç”¨åŒè‡‚æœºå™¨äººå®ç°ä»»æ„è¤¶çš±ç»‡ç‰©åˆ°ç›®æ ‡å¹³æ•´çŠ¶æ€çš„å¯¹é½</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dual-arm</span> <span class="paper-tag">imitation learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00814v1" data-paper-url="./papers/251000814v1-rtff-random-to-target-fabric-flattening-policy-using-dual-arm-manipu.html" onclick="toggleFavorite(this, '2510.00814v1', 'RTFF: Random-to-Target Fabric Flattening Policy using Dual-Arm Manipulator')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251001438v2-differentiable-skill-optimisation-for-powder-manipulation-in-laborat.html">Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation</a></td>
  <td>æå‡ºåŸºäºå¯å¾®æŠ€èƒ½ä¼˜åŒ–çš„ç²‰æœ«æ“ä½œæ–¹æ³•ï¼Œç”¨äºå®éªŒå®¤è‡ªåŠ¨åŒ–ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01438v2" data-paper-url="./papers/251001438v2-differentiable-skill-optimisation-for-powder-manipulation-in-laborat.html" onclick="toggleFavorite(this, '2510.01438v2', 'Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251001023v1-prometheus-universal-open-source-mocap-based-teleoperation-system-wi.html">Prometheus: Universal, Open-Source Mocap-Based Teleoperation System with Force Feedback for Dataset Collection in Robot Learning</a></td>
  <td>Prometheusï¼šåŸºäºåŠ¨æ•å’ŒåŠ›åé¦ˆçš„é€šç”¨å¼€æºé¥æ“ä½œç³»ç»Ÿï¼Œç”¨äºæœºå™¨äººå­¦ä¹ æ•°æ®é›†é‡‡é›†</td>
  <td class="tags-cell"><span class="paper-tag">teleoperation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01023v1" data-paper-url="./papers/251001023v1-prometheus-universal-open-source-mocap-based-teleoperation-system-wi.html" onclick="toggleFavorite(this, '2510.01023v1', 'Prometheus: Universal, Open-Source Mocap-Based Teleoperation System with Force Feedback for Dataset Collection in Robot Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251000573v1-grits-a-spillage-aware-guided-diffusion-policy-for-robot-food-scoopi.html">GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks</a></td>
  <td>GRITSï¼šä¸€ç§ç”¨äºæœºå™¨äººé£Ÿç‰©èˆ€å–ä»»åŠ¡çš„é˜²æº¢å‡ºå¼•å¯¼æ‰©æ•£ç­–ç•¥</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00573v1" data-paper-url="./papers/251000573v1-grits-a-spillage-aware-guided-diffusion-policy-for-robot-food-scoopi.html" onclick="toggleFavorite(this, '2510.00573v1', 'GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251000726v1-crostata-cross-state-transition-attention-transformer-for-robotic-ma.html">CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation</a></td>
  <td>æå‡ºCross-State Transition Attention Transformerä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ‰§è¡Œå˜å¼‚é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00726v1" data-paper-url="./papers/251000726v1-crostata-cross-state-transition-attention-transformer-for-robotic-ma.html" onclick="toggleFavorite(this, '2510.00726v1', 'CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251001404v1-how-well-do-diffusion-policies-learn-kinematic-constraint-manifolds.html">How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?</a></td>
  <td>ç ”ç©¶æ‰©æ•£ç­–ç•¥å­¦ä¹ è¿åŠ¨å­¦çº¦æŸæµå½¢çš„èƒ½åŠ›ï¼Œæ­ç¤ºæ•°æ®é›†è´¨é‡å’Œå¤§å°çš„å½±å“ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">bi-manual</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01404v1" data-paper-url="./papers/251001404v1-how-well-do-diffusion-policies-learn-kinematic-constraint-manifolds.html" onclick="toggleFavorite(this, '2510.01404v1', 'How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/251000466v1-integrating-offline-pre-training-with-online-fine-tuning-a-reinforce.html">Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation</a></td>
  <td>æå‡ºåŸºäºRTGé¢„æµ‹çš„ç¦»çº¿-åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºæå‡æœºå™¨äººç¤¾äº¤å¯¼èˆªèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">offline reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00466v1" data-paper-url="./papers/251000466v1-integrating-offline-pre-training-with-online-fine-tuning-a-reinforce.html" onclick="toggleFavorite(this, '2510.00466v1', 'Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251001068v1-compose-your-policies-improving-diffusion-based-or-flow-based-robot-.html">Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition</a></td>
  <td>æå‡ºé€šç”¨ç­–ç•¥ç»„åˆï¼ˆGPCï¼‰ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æå‡æ‰©æ•£æˆ–Flowæ¨¡å‹æœºå™¨äººç­–ç•¥æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01068v1" data-paper-url="./papers/251001068v1-compose-your-policies-improving-diffusion-based-or-flow-based-robot-.html" onclick="toggleFavorite(this, '2510.01068v1', 'Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251001519v1-online-hierarchical-policy-learning-using-physics-priors-for-robot-n.html">Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments</a></td>
  <td>æå‡ºåŸºäºç‰©ç†å…ˆéªŒçš„åœ¨çº¿åˆ†å±‚ç­–ç•¥å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæœªçŸ¥ç¯å¢ƒä¸‹çš„æœºå™¨äººå¯¼èˆªã€‚</td>
  <td class="tags-cell"><span class="paper-tag">policy learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01519v1" data-paper-url="./papers/251001519v1-online-hierarchical-policy-learning-using-physics-priors-for-robot-n.html" onclick="toggleFavorite(this, '2510.01519v1', 'Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251001388v1-ventura-adapting-image-diffusion-models-for-unified-task-conditioned.html">VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation</a></td>
  <td>æå‡ºVENTURAä»¥è§£å†³æœºå™¨äººå¯¼èˆªä»»åŠ¡ä¸­çš„é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">behavior cloning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01388v1" data-paper-url="./papers/251001388v1-ventura-adapting-image-diffusion-models-for-unified-task-conditioned.html" onclick="toggleFavorite(this, '2510.01388v1', 'VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/251000441v3-seeing-through-uncertainty-robust-task-oriented-optimization-in-visu.html">Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation</a></td>
  <td>NeuROï¼šé¢å‘è§†è§‰å¯¼èˆªï¼Œé€šè¿‡é²æ£’ä¼˜åŒ–åº”å¯¹ä¸ç¡®å®šæ€§ï¼Œæå‡æ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00441v3" data-paper-url="./papers/251000441v3-seeing-through-uncertainty-robust-task-oriented-optimization-in-visu.html" onclick="toggleFavorite(this, '2510.00441v3', 'Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251001389v1-insight-inference-time-sequence-introspection-for-generating-help-tr.html">INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models</a></td>
  <td>INSIGHTï¼šæå‡ºä¸€ç§åŸºäºåºåˆ—å†…çœçš„VLAæ¨¡å‹å¸®åŠ©è§¦å‘ç”Ÿæˆæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01389v1" data-paper-url="./papers/251001389v1-insight-inference-time-sequence-introspection-for-generating-help-tr.html" onclick="toggleFavorite(this, '2510.01389v1', 'INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251000703v1-multiphysio-hrc-multimodal-physiological-signals-dataset-for-industr.html">MultiPhysio-HRC: Multimodal Physiological Signals Dataset for industrial Human-Robot Collaboration</a></td>
  <td>MultiPhysio-HRCï¼šç”¨äºå·¥ä¸šäººæœºåä½œçš„å¤šæ¨¡æ€ç”Ÿç†ä¿¡å·æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00703v1" data-paper-url="./papers/251000703v1-multiphysio-hrc-multimodal-physiological-signals-dataset-for-industr.html" onclick="toggleFavorite(this, '2510.00703v1', 'MultiPhysio-HRC: Multimodal Physiological Signals Dataset for industrial Human-Robot Collaboration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/251001483v1-vl-kng-visual-scene-understanding-for-navigation-goal-identification.html">VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs</a></td>
  <td>VL-KnGï¼šåˆ©ç”¨æ—¶ç©ºçŸ¥è¯†å›¾è°±è¿›è¡Œè§†è§‰åœºæ™¯ç†è§£ï¼Œå®ç°å¯¼èˆªç›®æ ‡è¯†åˆ«</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01483v1" data-paper-url="./papers/251001483v1-vl-kng-visual-scene-understanding-for-navigation-goal-identification.html" onclick="toggleFavorite(this, '2510.01483v1', 'VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251001348v1-kilometer-scale-gnss-denied-uav-navigation-via-heightmap-gradients-a.html">Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge</a></td>
  <td>æå‡ºåŸºäºé«˜åº¦å›¾æ¢¯åº¦çš„GNSSæ‹’æ­¢æ— äººæœºå¯¼èˆªç³»ç»Ÿï¼Œèµ¢å¾—SPRIN-DæŒ‘æˆ˜èµ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">height map</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01348v1" data-paper-url="./papers/251001348v1-kilometer-scale-gnss-denied-uav-navigation-via-heightmap-gradients-a.html" onclick="toggleFavorite(this, '2510.01348v1', 'Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251000783v1-semantic-visual-simultaneous-localization-and-mapping-a-survey-on-st.html">Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions</a></td>
  <td>ç»¼è¿°è¯­ä¹‰è§†è§‰SLAMæŠ€æœ¯ï¼Œåˆ†æç°çŠ¶ã€æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00783v1" data-paper-url="./papers/251000783v1-semantic-visual-simultaneous-localization-and-mapping-a-survey-on-st.html" onclick="toggleFavorite(this, '2510.00783v1', 'Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)