---
layout: default
title: CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding
---

# CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.04231" target="_blank" class="toolbar-btn">arXiv: 2512.04231v1</a>
    <a href="https://arxiv.org/pdf/2512.04231.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04231v1" 
            onclick="toggleFavorite(this, '2512.04231v1', 'CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhou Chen, Joe Lin, Carson Bulgin, Sathyanarayanan N. Aakur

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-03

**Â§áÊ≥®**: 20 pages. 3 figures, 4 tables. Under Review

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**CRAFT-EÔºöÁî®‰∫éÂÖ∑Ë∫´ÂèØ‰æõÊÄßÊé•Âú∞ÁöÑÁ•ûÁªèÁ¨¶Âè∑Ê°ÜÊû∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `ÂÖ∑Ë∫´Êô∫ËÉΩ` `ÂèØ‰æõÊÄß` `Á•ûÁªèÁ¨¶Âè∑` `Áü•ËØÜÂõæË∞±` `Êú∫Âô®‰∫∫` `ËßÜËßâËØ≠Ë®ÄÂØπÈΩê` `ÊäìÂèñÊé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ï‰æùËµñÈªëÁõíÊ®°ÂûãÊàñÂõ∫ÂÆöÊ†áÁ≠æÔºåÁº∫‰πèÈÄèÊòéÊÄßÂíåÂèØÊéßÊÄßÔºåÈöæ‰ª•Êª°Ë∂≥‰∫∫Êú∫‰∫§‰∫íÂ∫îÁî®ÁöÑÈúÄÊ±Ç„ÄÇ
2. CRAFT-EÁªìÂêàÁü•ËØÜÂõæË∞±„ÄÅËßÜËßâËØ≠Ë®ÄÂØπÈΩêÂíåËÉΩÈáèÊ®°ÂûãÔºåÁîüÊàêÂèØËß£ÈáäÁöÑÊé®ÁêÜË∑ØÂæÑÔºåÂπ∂ËÄÉËôëÊäìÂèñÂèØË°åÊÄß„ÄÇ
3. CRAFT-EÂú®ÈùôÊÄÅÂú∫ÊôØÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰∏îÂú®ÊÑüÁü•Âô™Â£∞‰∏ã‰øùÊåÅÈ≤ÅÊ£íÊÄßÔºåÊèê‰æõÁªÑ‰ª∂Á∫ßËØäÊñ≠„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®ÈùûÁªìÊûÑÂåñÁéØÂ¢É‰∏≠ËøêË°åÁöÑËæÖÂä©Êú∫Âô®‰∫∫‰∏ç‰ªÖÈúÄË¶ÅÁêÜËß£Áâ©‰ΩìÊòØ‰ªÄ‰πàÔºåËøòÈúÄË¶ÅÁêÜËß£ÂÆÉ‰ª¨ÂèØ‰ª•Áî®Êù•ÂÅö‰ªÄ‰πà„ÄÇËøôÈúÄË¶ÅÂ∞ÜÂü∫‰∫éËØ≠Ë®ÄÁöÑÂä®‰ΩúÊü•ËØ¢‰∏éÊó¢ËÉΩÊèê‰æõÊâÄÈúÄÂäüËÉΩÂèàËÉΩË¢´Áâ©ÁêÜÊ£ÄÁ¥¢ÁöÑÁâ©‰ΩìËøõË°åÂÖ≥ËÅî„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÈªëÁõíÊ®°ÂûãÊàñÂõ∫ÂÆöÁöÑÂèØ‰æõÊÄßÊ†áÁ≠æÔºåÈôêÂà∂‰∫ÜÈù¢Âêë‰∫∫Á±ªÂ∫îÁî®ÁöÑÈÄèÊòéÊÄß„ÄÅÂèØÊéßÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜCRAFT-EÔºå‰∏Ä‰∏™Ê®°ÂùóÂåñÁöÑÁ•ûÁªèÁ¨¶Âè∑Ê°ÜÊû∂ÔºåÂÆÉÂ∞ÜÁªìÊûÑÂåñÁöÑÂä®ËØç-Â±ûÊÄß-ÂØπË±°Áü•ËØÜÂõæ‰∏éËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêÂíåÂü∫‰∫éËÉΩÈáèÁöÑÊäìÂèñÊé®ÁêÜÁõ∏ÁªìÂêà„ÄÇËØ•Á≥ªÁªüÁîüÊàêÂèØËß£ÈáäÁöÑÊé•Âú∞Ë∑ØÂæÑÔºåÊè≠Á§∫ÂΩ±ÂìçÁâ©‰ΩìÈÄâÊã©ÁöÑÂõ†Á¥†ÔºåÂπ∂Â∞ÜÊäìÂèñÂèØË°åÊÄß‰Ωú‰∏∫ÂèØ‰æõÊÄßÊé®ÁêÜÁöÑ‰∏Ä‰∏™ÁªÑÊàêÈÉ®ÂàÜ„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´Âä®ËØç-ÂØπË±°ÂÖºÂÆπÊÄß„ÄÅÂàÜÂâ≤ÂíåÊäìÂèñÂÄôÈÄâÁöÑÁªü‰∏ÄÊ≥®ÈáäÔºåÂπ∂Âú®Áâ©ÁêÜÊú∫Âô®‰∫∫‰∏äÈÉ®ÁΩ≤‰∫ÜÂÆåÊï¥ÁöÑpipeline„ÄÇCRAFT-EÂú®ÈùôÊÄÅÂú∫ÊôØ„ÄÅÂü∫‰∫éImageNetÁöÑÂäüËÉΩÊ£ÄÁ¥¢‰ª•ÂèäÊ∂âÂèä20‰∏™Âä®ËØçÂíå39‰∏™Áâ©‰ΩìÁöÑÁúüÂÆû‰∏ñÁïåËØïÈ™å‰∏≠ÂèñÂæó‰∫ÜÊúâÁ´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂Âú®ÊÑüÁü•Âô™Â£∞‰∏ã‰øùÊåÅÁ®≥ÂÅ•ÔºåÂπ∂Êèê‰æõÈÄèÊòéÁöÑÁªÑ‰ª∂Á∫ßËØäÊñ≠„ÄÇÈÄöËøáÂ∞ÜÁ¨¶Âè∑Êé®ÁêÜ‰∏éÂÖ∑Ë∫´ÊÑüÁü•Áõ∏ÁªìÂêàÔºåCRAFT-E‰∏∫ÂèØ‰æõÊÄßÊé•Âú∞ÁöÑÁâ©‰ΩìÈÄâÊã©Êèê‰æõ‰∫Ü‰∏ÄÁßçÂèØËß£ÈáäÂíåÂèØÂÆöÂà∂ÁöÑÊõø‰ª£ÊñπÊ°àÔºåÊîØÊåÅËæÖÂä©Êú∫Âô®‰∫∫Á≥ªÁªü‰∏≠ÂÄºÂæó‰ø°ËµñÁöÑÂÜ≥Á≠ñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËæÖÂä©Êú∫Âô®‰∫∫Â¶Ç‰ΩïÂú®ÈùûÁªìÊûÑÂåñÁéØÂ¢É‰∏≠ÁêÜËß£Áâ©‰ΩìÁöÑÂäüËÉΩÔºàÂèØ‰æõÊÄßÔºâÔºåÂπ∂Ê†πÊçÆËØ≠Ë®ÄÊåá‰ª§ÈÄâÊã©ÂêàÈÄÇÁöÑÁâ©‰ΩìËøõË°åÊìç‰ΩúÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫é‰æùËµñÈªëÁõíÊ®°ÂûãÊàñÈ¢ÑÂÆö‰πâÁöÑÂèØ‰æõÊÄßÊ†áÁ≠æÔºåÁº∫‰πèÈÄèÊòéÊÄßÂíåÂèØËß£ÈáäÊÄßÔºåÈöæ‰ª•Ë∞ÉËØïÂíå‰ø°‰ªª„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÁ•ûÁªèÊñπÊ≥ïÂíåÁ¨¶Âè∑Êé®ÁêÜÁõ∏ÁªìÂêàÔºåÊûÑÂª∫‰∏Ä‰∏™Ê®°ÂùóÂåñÁöÑÁ•ûÁªèÁ¨¶Âè∑Ê°ÜÊû∂„ÄÇÈÄöËøáÁü•ËØÜÂõæË∞±Êù•Ë°®Á§∫Áâ©‰Ωì„ÄÅÂ±ûÊÄßÂíåÂä®‰Ωú‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂà©Áî®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂ∞ÜËØ≠Ë®ÄÊåá‰ª§‰∏éËßÜËßâ‰ø°ÊÅØÂØπÈΩêÔºåÂπ∂‰ΩøÁî®ËÉΩÈáèÊ®°ÂûãÊù•ËØÑ‰º∞ÊäìÂèñÁöÑÂèØË°åÊÄß„ÄÇËøôÊ†∑ÂèØ‰ª•ÁîüÊàêÂèØËß£ÈáäÁöÑÊé®ÁêÜË∑ØÂæÑÔºå‰ªéËÄåÊèêÈ´òÁ≥ªÁªüÁöÑÈÄèÊòéÊÄßÂíåÂèØÊéßÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCRAFT-EÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) Áü•ËØÜÂõæË∞±ÔºöÂ≠òÂÇ®Âä®ËØç„ÄÅÂ±ûÊÄßÂíåÂØπË±°‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ2) ËßÜËßâËØ≠Ë®ÄÂØπÈΩêÊ®°ÂùóÔºöÂ∞ÜËØ≠Ë®ÄÊåá‰ª§‰∏≠ÁöÑÂä®ËØçÂíåÂØπË±°‰∏éÂõæÂÉè‰∏≠ÁöÑËßÜËßâ‰ø°ÊÅØÂØπÈΩê„ÄÇ3) ËÉΩÈáèÊ®°ÂûãÔºöËØÑ‰º∞ÊäìÂèñÂÄôÈÄâÁöÑË¥®ÈáèÂíåÂèØË°åÊÄß„ÄÇ4) Êé®ÁêÜÂºïÊìéÔºöÊ†πÊçÆÁü•ËØÜÂõæË∞±„ÄÅËßÜËßâËØ≠Ë®ÄÂØπÈΩêÁªìÊûúÂíåÊäìÂèñÂèØË°åÊÄßÔºåÁîüÊàêÂèØËß£ÈáäÁöÑÊé•Âú∞Ë∑ØÂæÑÔºåÈÄâÊã©ÊúÄ‰Ω≥Áâ©‰Ωì„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCRAFT-EÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÁ¨¶Âè∑Êé®ÁêÜ‰∏éÂÖ∑Ë∫´ÊÑüÁü•Áõ∏ÁªìÂêàÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂèØËß£ÈáäÁöÑÁ•ûÁªèÁ¨¶Âè∑Ê°ÜÊû∂„ÄÇ‰∏éÁ´ØÂà∞Á´ØÊ®°ÂûãÁõ∏ÊØîÔºåCRAFT-EÁöÑÊé®ÁêÜËøáÁ®ãÊõ¥Âä†ÈÄèÊòéÔºåÂèØ‰ª•ËøõË°åÁªÑ‰ª∂Á∫ßÁöÑËØäÊñ≠ÂíåË∞ÉËØï„ÄÇÊ≠§Â§ñÔºåCRAFT-EÂ∞ÜÊäìÂèñÂèØË°åÊÄß‰Ωú‰∏∫ÂèØ‰æõÊÄßÊé®ÁêÜÁöÑ‰∏Ä‰∏™ÁªÑÊàêÈÉ®ÂàÜÔºåÊèêÈ´ò‰∫ÜÁâ©‰ΩìÈÄâÊã©ÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCRAFT-E‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇCLIPÔºâËøõË°åËßÜËßâËØ≠Ë®ÄÂØπÈΩê„ÄÇËÉΩÈáèÊ®°ÂûãÈááÁî®Âü∫‰∫éËÉΩÈáèÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÂ≠¶‰π†ËÉΩÈáèÂáΩÊï∞Êù•ËØÑ‰º∞ÊäìÂèñÂÄôÈÄâÁöÑË¥®Èáè„ÄÇÁü•ËØÜÂõæË∞±ÈááÁî®‰∫∫Â∑•ÊûÑÂª∫ÁöÑÊñπÂºèÔºåÂπ∂Ê†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åÊâ©Â±ï„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ËßÜËßâËØ≠Ë®ÄÂØπÈΩêÊçüÂ§±ÂíåÊäìÂèñËÉΩÈáèÊçüÂ§±„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CRAFT-EÂú®ÈùôÊÄÅÂú∫ÊôØ„ÄÅImageNet-basedÂäüËÉΩÊ£ÄÁ¥¢ÂíåÁúüÂÆû‰∏ñÁïåÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠ÂèñÂæó‰∫ÜÊúâÁ´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCRAFT-EÂú®ÊÑüÁü•Âô™Â£∞‰∏ã‰øùÊåÅÈ≤ÅÊ£íÊÄßÔºåÂπ∂ËÉΩÂ§üÊèê‰æõÈÄèÊòéÁöÑÁªÑ‰ª∂Á∫ßËØäÊñ≠„ÄÇÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠ÔºåCRAFT-EÊàêÂäüÂú∞ÂÆåÊàê‰∫ÜÊ∂âÂèä20‰∏™Âä®ËØçÂíå39‰∏™Áâ©‰ΩìÁöÑ‰ªªÂä°„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CRAFT-EÂèØÂ∫îÁî®‰∫éËæÖÂä©Êú∫Âô®‰∫∫„ÄÅÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÁ≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫ÁêÜËß£‰∫∫Á±ªÁöÑÊåá‰ª§ÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÂ∑•ÂÖ∑ÂíåÁâ©‰ΩìËøõË°åÊìç‰ΩúÔºå‰ªéËÄåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑËá™‰∏ªÊÄßÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÊûÑÂª∫Êõ¥ÂÄºÂæó‰ø°Ëµñ„ÄÅÊõ¥Êòì‰∫éÁêÜËß£ÂíåÊéßÂà∂ÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÔºå‰øÉËøõ‰∫∫Êú∫Âçè‰Ωú„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Assistive robots operating in unstructured environments must understand not only what objects are, but what they can be used for. This requires grounding language-based action queries to objects that both afford the requested function and can be physically retrieved. Existing approaches often rely on black-box models or fixed affordance labels, limiting transparency, controllability, and reliability for human-facing applications. We introduce CRAFT-E, a modular neuro-symbolic framework that composes a structured verb-property-object knowledge graph with visual-language alignment and energy-based grasp reasoning. The system generates interpretable grounding paths that expose the factors influencing object selection and incorporates grasp feasibility as an integral part of affordance inference. We further construct a benchmark dataset with unified annotations for verb-object compatibility, segmentation, and grasp candidates, and deploy the full pipeline on a physical robot. CRAFT-E achieves competitive performance in static scenes, ImageNet-based functional retrieval, and real-world trials involving 20 verbs and 39 objects. The framework remains robust under perceptual noise and provides transparent, component-level diagnostics. By coupling symbolic reasoning with embodied perception, CRAFT-E offers an interpretable and customizable alternative to end-to-end models for affordance-grounded object selection, supporting trustworthy decision-making in assistive robotic systems.

