---
layout: default
title: RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL
---

# RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.03556" target="_blank" class="toolbar-btn">arXiv: 2512.03556v1</a>
    <a href="https://arxiv.org/pdf/2512.03556.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03556v1" 
            onclick="toggleFavorite(this, '2512.03556v1', 'RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yinzhou Tang, Yu Shang, Yinuo Chen, Bingwen Wei, Xin Zhang, Shu'ang Yu, Liangzhi Shi, Chao Yu, Chen Gao, Wei Wu, Yong Li

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-03

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**RoboScape-RÔºöÈÄöËøáÁªü‰∏ÄÂ•ñÂä±-ËßÇÊµã‰∏ñÁïåÊ®°ÂûãÊèêÂçáÊú∫Âô®‰∫∫Âº∫ÂåñÂ≠¶‰π†ÁöÑÊ≥õÂåñËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫` `Âº∫ÂåñÂ≠¶‰π†` `‰∏ñÁïåÊ®°Âûã` `Ê≥õÂåñËÉΩÂäõ` `ÂÖ∑Ë∫´Êô∫ËÉΩ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†Áº∫‰πèÁªü‰∏ÄÁöÑÈÄöÁî®Â•ñÂä±‰ø°Âè∑ÔºåÈöæ‰ª•Âú®Â§öÂú∫ÊôØ‰∏≠Ê≥õÂåñÔºåÊ®°‰ªøÂ≠¶‰π†ÂàôÂÆπÊòìËøáÊãüÂêà‰∏ìÂÆ∂ËΩ®Ëøπ„ÄÇ
2. RoboScape-RÂà©Áî®‰∏ñÁïåÊ®°Âûã‰Ωú‰∏∫ÈÄöÁî®ÁéØÂ¢É‰ª£ÁêÜÔºåÈÄöËøáÂÜÖÁîüÂ•ñÂä±Êú∫Âà∂ÔºåÊèêÂçáÂº∫ÂåñÂ≠¶‰π†ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåRoboScape-RÂú®Ë∂ÖÂá∫È¢ÜÂüüÂú∫ÊôØ‰∏ãÔºåÊÄßËÉΩÊØîÂü∫Á∫øÂπ≥ÂùáÊèêÈ´ò‰∫Ü37.5%ÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂÆûÁé∞ÂèØÊ≥õÂåñÁöÑÂÖ∑Ë∫´Êô∫ËÉΩÁ≠ñÁï•‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàò„ÄÇ‰º†ÁªüÁöÑÁ≠ñÁï•Â≠¶‰π†ËåÉÂºèÔºåÂåÖÊã¨Ê®°‰ªøÂ≠¶‰π†ÔºàILÔºâÂíåÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÔºåÈÉΩÈöæ‰ª•Âú®‰∏çÂêåÁöÑÂú∫ÊôØ‰∏≠ÂüπÂÖªÊ≥õÂåñËÉΩÂäõ„ÄÇÊ®°‰ªøÂ≠¶‰π†Á≠ñÁï•ÈÄöÂ∏∏ËøáÂ∫¶ÊãüÂêàÁâπÂÆöÁöÑ‰∏ìÂÆ∂ËΩ®ËøπÔºåËÄåÂº∫ÂåñÂ≠¶‰π†ÂàôÁº∫‰πèÁªü‰∏ÄÂíåÈÄöÁî®ÁöÑÂ•ñÂä±‰ø°Âè∑ÔºåËøôÂØπ‰∫éÊúâÊïàÁöÑÂ§öÂú∫ÊôØÊ≥õÂåñËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ËÆ§‰∏∫‰∏ñÁïåÊ®°ÂûãËÉΩÂ§ü‰Ωú‰∏∫ÈÄöÁî®ÁöÑÁéØÂ¢É‰ª£ÁêÜÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÈôêÂà∂„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÁöÑ‰∏ñÁïåÊ®°Âûã‰∏ªË¶ÅÂÖ≥Ê≥®È¢ÑÊµãËßÇÊµãÁöÑËÉΩÂäõÔºå‰ªçÁÑ∂‰æùËµñ‰∫éÁâπÂÆö‰ªªÂä°ÁöÑÊâãÂ∑•ËÆæËÆ°ÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÂõ†Ê≠§Êó†Ê≥ïÊèê‰æõÁúüÊ≠£ÈÄöÁî®ÁöÑËÆ≠ÁªÉÁéØÂ¢É„ÄÇÈíàÂØπËøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜRoboScape-RÔºå‰∏Ä‰∏™Âà©Áî®‰∏ñÁïåÊ®°Âûã‰Ωú‰∏∫Âº∫ÂåñÂ≠¶‰π†ËåÉÂºè‰∏≠ÂÖ∑Ë∫´ÁéØÂ¢ÉÁöÑÈÄöÁî®‰ª£ÁêÜÁöÑÊ°ÜÊû∂„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫‰∫é‰∏ñÁïåÊ®°ÂûãÁöÑÊñ∞ÂûãÈÄöÁî®Â•ñÂä±Êú∫Âà∂ÔºåËØ•Êú∫Âà∂ÁîüÊàêÊ∫ê‰∫éÊ®°ÂûãÂØπÁúüÂÆû‰∏ñÁïåÁä∂ÊÄÅËΩ¨ÁßªÂä®ÊÄÅÁöÑÂÜÖÂú®ÁêÜËß£ÁöÑ‚ÄúÂÜÖÁîü‚ÄùÂ•ñÂä±„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåRoboScape-RÈÄöËøáÊèê‰æõÈ´òÊïàÂíåÈÄöÁî®ÁöÑËÆ≠ÁªÉÁéØÂ¢ÉÔºåÊúâÊïàÂú∞Ëß£ÂÜ≥‰∫Ü‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÔºå‰ªéËÄåÊòæËëóÊèêÈ´ò‰∫ÜÂÖ∑Ë∫´Êô∫ËÉΩÁ≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏∫Âà©Áî®‰∏ñÁïåÊ®°Âûã‰Ωú‰∏∫Âú®Á∫øËÆ≠ÁªÉÁ≠ñÁï•Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑËßÅËß£ÔºåÂπ∂‰∏îÂú®Ë∂ÖÂá∫È¢ÜÂüüÂú∫ÊôØ‰∏ãÔºåÊÄßËÉΩÊØîÂü∫Á∫øÂπ≥ÂùáÊèêÈ´ò‰∫Ü37.5%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂÖ∑Ë∫´Êô∫ËÉΩÁ≠ñÁï•Âú®‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÊ≥õÂåñÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰æùËµñ‰∫éÊâãÂ∑•ËÆæËÆ°ÁöÑ„ÄÅÁâπÂÆö‰∫é‰ªªÂä°ÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂Âú®Êú™ËßÅËøáÁöÑÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞„ÄÇÊ®°‰ªøÂ≠¶‰π†ËôΩÁÑ∂ÂèØ‰ª•Â≠¶‰π†‰∏ìÂÆ∂Á≠ñÁï•Ôºå‰ΩÜÂÆπÊòìËøáÊãüÂêàËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®‰∏ñÁïåÊ®°ÂûãÊù•Â≠¶‰π†ÁéØÂ¢ÉÁöÑÂä®ÊÄÅÁâπÊÄßÔºåÂπ∂‰ªé‰∏≠ÊèêÂèñÈÄöÁî®ÁöÑÂ•ñÂä±‰ø°Âè∑„ÄÇÈÄöËøáËÆ©Êô∫ËÉΩ‰ΩìÂú®‰∏ñÁïåÊ®°Âûã‰∏≠ËøõË°åËÆ≠ÁªÉÔºåÂèØ‰ª•ÈÅøÂÖçÂØπÁúüÂÆûÁéØÂ¢ÉÁöÑËøáÂ∫¶‰æùËµñÔºå‰ªéËÄåÊèêÈ´òÁ≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËøôÁßçÊñπÊ≥ïÁöÑÂÖ≥ÈîÆÂú®‰∫éËÆæËÆ°‰∏ÄÁßçËÉΩÂ§üÂèçÊò†ÁéØÂ¢ÉÂÜÖÂú®ËßÑÂæãÁöÑÂÜÖÁîüÂ•ñÂä±Êú∫Âà∂„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRoboScape-RÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ‰∏ñÁïåÊ®°ÂûãÔºöÁî®‰∫éÂ≠¶‰π†ÁéØÂ¢ÉÁöÑÁä∂ÊÄÅËΩ¨ÁßªÂä®ÊÄÅÔºåËÉΩÂ§üÈ¢ÑÊµãÊú™Êù•Áä∂ÊÄÅÂíåÂ•ñÂä±„ÄÇ2) ÂÜÖÁîüÂ•ñÂä±ÁîüÊàêÂô®ÔºöÂü∫‰∫é‰∏ñÁïåÊ®°ÂûãÁöÑÈ¢ÑÊµãÔºåÁîüÊàêÂèçÊò†ÁéØÂ¢ÉÂÜÖÂú®ËßÑÂæãÁöÑÂ•ñÂä±‰ø°Âè∑„ÄÇ3) Âº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰ΩìÔºöÂú®‰∏ñÁïåÊ®°Âûã‰∏≠ËøõË°åËÆ≠ÁªÉÔºå‰ª•ÊúÄÂ§ßÂåñÂÜÖÁîüÂ•ñÂä±„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÔºåÊô∫ËÉΩ‰ΩìÂú®‰∏ñÁïåÊ®°Âûã‰∏≠ÈááÂèñË°åÂä®Ôºå‰∏ñÁïåÊ®°ÂûãÈ¢ÑÊµã‰∏ã‰∏Ä‰∏™Áä∂ÊÄÅÂíåÂ•ñÂä±ÔºåÂÜÖÁîüÂ•ñÂä±ÁîüÊàêÂô®Ê†πÊçÆÈ¢ÑÊµãÁªìÊûúÁîüÊàêÂ•ñÂä±ÔºåÊô∫ËÉΩ‰ΩìÊ†πÊçÆÂ•ñÂä±Êõ¥Êñ∞Á≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÊèêÂá∫‰∫ÜÂü∫‰∫é‰∏ñÁïåÊ®°ÂûãÁöÑÂÜÖÁîüÂ•ñÂä±Êú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑÊâãÂ∑•ËÆæËÆ°ÁöÑÂ•ñÂä±ÂáΩÊï∞‰∏çÂêåÔºåÂÜÖÁîüÂ•ñÂä±ËÉΩÂ§üËá™Âä®Âú∞‰ªéÁéØÂ¢ÉÂä®ÊÄÅ‰∏≠Â≠¶‰π†Ôºå‰ªéËÄåÊèê‰æõÊõ¥ÈÄöÁî®ÂíåÈ≤ÅÊ£íÁöÑÂ•ñÂä±‰ø°Âè∑„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫ÜÂØπÁâπÂÆö‰ªªÂä°ÁöÑËøáÂ∫¶‰æùËµñÔºåÊèêÈ´ò‰∫ÜÁ≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö‰∏ñÁïåÊ®°ÂûãÈÄöÂ∏∏ÈááÁî®ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÊàñTransformerÁ≠âÊ®°ÂûãÁªìÊûÑÔºåÁî®‰∫éÂ≠¶‰π†ÁéØÂ¢ÉÁöÑÁä∂ÊÄÅË°®Á§∫ÂíåËΩ¨ÁßªÂáΩÊï∞„ÄÇÂÜÖÁîüÂ•ñÂä±ÁöÑËÆæËÆ°ÂèØ‰ª•Âü∫‰∫éÂ§öÁßçÊåáÊ†áÔºå‰æãÂ¶ÇÁä∂ÊÄÅÁöÑÂèòÂåñÂπÖÂ∫¶„ÄÅ‰∏éÁõÆÊ†áÁöÑË∑ùÁ¶ªÁ≠â„ÄÇÂº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰ΩìÂèØ‰ª•‰ΩøÁî®Â∏∏ËßÅÁöÑÁÆóÊ≥ïÔºåÂ¶ÇPPOÊàñSAC„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑ‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRoboScape-RÂú®Ë∂ÖÂá∫È¢ÜÂüüÂú∫ÊôØ‰∏ãÔºåÊÄßËÉΩÊØîÂü∫Á∫øÊñπÊ≥ïÂπ≥ÂùáÊèêÈ´ò‰∫Ü37.5%„ÄÇËøôË°®ÊòéËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òÊú∫Âô®‰∫∫Á≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ΩøÂÖ∂Âú®Êú™ËßÅËøáÁöÑÁéØÂ¢É‰∏≠‰πüËÉΩË°®Áé∞ËâØÂ•Ω„ÄÇÊ≠§Â§ñÔºåÂÆûÈ™åËøòÈ™åËØÅ‰∫ÜÂÜÖÁîüÂ•ñÂä±Êú∫Âà∂ÁöÑÊúâÊïàÊÄßÔºåËØÅÊòéÂÖ∂ËÉΩÂ§üÊèê‰æõÊõ¥ÈÄöÁî®ÂíåÈ≤ÅÊ£íÁöÑÂ•ñÂä±‰ø°Âè∑„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫‰ªªÂä°Ôºå‰æãÂ¶ÇÂØºËà™„ÄÅÊìç‰ΩúÂíåÊéßÂà∂„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫Á≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂèØ‰ª•‰ΩøÂÖ∂Âú®Êõ¥ÂπøÊ≥õÁöÑÂÆûÈôÖÂú∫ÊôØ‰∏≠ÈÉ®ÁΩ≤Ôºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÂíåÁÅæÈöæÊïëÊè¥„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑÁéØÂ¢ÉÂíå‰ªªÂä°ÔºåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥Ëá™‰∏ªÁöÑÊú∫Âô®‰∫∫Á≥ªÁªü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.

