---
layout: default
title: SEAL: Vision-Language Model-Based Safe End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling
---

# SEAL: Vision-Language Model-Based Safe End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21041" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21041v2</a>
  <a href="https://arxiv.org/pdf/2506.21041.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21041v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21041v2', 'SEAL: Vision-Language Model-Based Safe End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junwei You, Pei Li, Zhuoyu Jiang, Zilin Huang, Rui Gan, Haotian Shi, Bin Ran

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-07-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSEALæ¡†æ¶ä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„å®‰å…¨è‡ªåŠ¨é©¾é©¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `é•¿å°¾åœºæ™¯` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å®‰å…¨æ€§` `é²æ£’æ€§` `æ™ºèƒ½äº¤é€š` `ååŒæ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªåŠ¨é©¾é©¶æŠ€æœ¯åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§å—åˆ°ç¨€æœ‰å¤©æ°”åœºæ™¯çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ååŒé©¾é©¶ä¸­æ›´ä¸ºçªå‡ºã€‚
2. SEALæ¡†æ¶é€šè¿‡è§†è§‰-è¯­è¨€æ¨¡å‹å’Œè‡ªé€‚åº”å¤šæ¨¡æ€å­¦ä¹ ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é•¿å°¾åœºæ™¯ç”Ÿæˆä¸è¯„ä¼°æ–¹æ³•ï¼Œä»¥å¢å¼ºè®­ç»ƒå¤šæ ·æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSEALåœ¨æ¨ç†ã€å®‰å…¨æ€§å’Œè§„åˆ’å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡å¹…åº¦æ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨é©¾é©¶æŠ€æœ¯åœ¨ç¨€æœ‰ã€å¤šæ ·åŒ–å’Œè§†è§‰é€€åŒ–çš„å¤©æ°”åœºæ™¯ä¸‹é¢ä¸´é‡å¤§å®‰å…¨æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨è½¦è¾†ä¸åŸºç¡€è®¾æ–½ååŒæ„ŸçŸ¥çš„æƒ…å†µä¸‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SEALæ¡†æ¶ï¼ŒåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªé€‚åº”å¤šæ¨¡æ€å­¦ä¹ ï¼Œæ—¨åœ¨å¢å¼ºé•¿å°¾åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚SEALçš„ä¸‰å¤§åˆ›æ–°åŒ…æ‹¬ï¼šåˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆå’Œè¯„ä¼°é•¿å°¾åœºæ™¯çš„æç¤ºé©±åŠ¨ç®¡é“ã€é€šè¿‡åœºæ™¯å…ˆéªŒè°ƒèŠ‚è§†è§‰æµçš„é—¨æ§å¤šåœºæ™¯è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥åŠæå‡å¤šæ¨¡æ€å¯¹é½çš„å¤šä»»åŠ¡åœºæ™¯æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ç›®æ ‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSEALåœ¨å¤æ‚é©¾é©¶æ¡ä»¶ä¸‹çš„æ¨ç†ã€å®‰å…¨æ€§å’Œè§„åˆ’å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ¨åŠ¨äº†è‡ªåŠ¨é©¾é©¶çš„å®‰å…¨æ€§ã€é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶åœ¨å¤æ‚å’Œç¨€æœ‰å¤©æ°”åœºæ™¯ä¸‹çš„å®‰å…¨æ€§å’Œé²æ£’æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿å°¾åœºæ™¯æ—¶ï¼Œå¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®å’Œé€‚åº”èƒ½åŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSEALæ¡†æ¶é€šè¿‡ç»“åˆè§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªé€‚åº”å¤šæ¨¡æ€å­¦ä¹ ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„é•¿å°¾åœºæ™¯ï¼Œä»¥å¢å¼ºç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåº”å¯¹ä¸åŒçš„é©¾é©¶åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSEALçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé•¿å°¾åœºæ™¯ç”Ÿæˆä¸è¯„ä¼°ç®¡é“ã€é—¨æ§å¤šåœºæ™¯è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—å’Œå¤šä»»åŠ¡åœºæ™¯æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ç›®æ ‡ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œæå‡äº†ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSEALçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æç¤ºé©±åŠ¨çš„é•¿å°¾åœºæ™¯ç”Ÿæˆæ–¹æ³•å’Œé—¨æ§è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™äº›åˆ›æ–°ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿåœ¨å¤šå˜çš„é©¾é©¶æ¡ä»¶ä¸‹ï¼Œçµæ´»è°ƒæ•´å’Œä¼˜åŒ–ç‰¹å¾æå–ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šä»»åŠ¡å­¦ä¹ çš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡æ€å¯¹é½ï¼Œå¹¶é€šè¿‡åœºæ™¯å…ˆéªŒä¿¡æ¯è°ƒèŠ‚è§†è§‰æµï¼Œç¡®ä¿ç³»ç»Ÿåœ¨é¢å¯¹æ¨¡ç³Šæˆ–æŸåç‰¹å¾æ—¶èƒ½å¤Ÿæœ‰æ•ˆé‡æ ¡å‡†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSEALåœ¨å¤æ‚é©¾é©¶æ¡ä»¶ä¸‹çš„æ¨ç†å‡†ç¡®æ€§æå‡äº†20%ï¼Œå®‰å…¨æ€§æŒ‡æ ‡æé«˜äº†15%ï¼Œè§„åˆ’å‡†ç¡®æ€§ä¹Ÿæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSEALåœ¨å¤„ç†é•¿å°¾åœºæ™¯æ—¶å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œæ¨åŠ¨äº†è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶æ±½è½¦ä»¥åŠåŸå¸‚åŸºç¡€è®¾æ–½çš„æ™ºèƒ½åŒ–æ”¹é€ ã€‚é€šè¿‡æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§å’Œé²æ£’æ€§ï¼ŒSEALæ¡†æ¶æœ‰æœ›åœ¨æœªæ¥çš„æ™ºèƒ½äº¤é€šä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous driving technologies face significant safety challenges while operating under rare, diverse, and visually degraded weather scenarios. These challenges become more critical in cooperative settings, where vehicles and infrastructure jointly perceive and reason across complex environments. To address these issues, we propose SEAL, a vision-language model-based framework with adaptive multimodal learning for robust cooperative autonomous driving under long-tail scenarios. SEAL introduces three core innovations: (i) a prompt-driven long-tail scenario generation and evaluation pipeline that leverages foundation models to synthesize realistic long-tail conditions such as snow and fog across vehicle- and infrastructure-side views, enriching training diversity efficiently; (ii) a gated multi-scenario adaptive attention module that modulates the visual stream using scenario priors to recalibrate ambiguous or corrupted features; and (iii) a multi-task scenario-aware contrastive learning objective that improves multimodal alignment and promotes cross-scenario feature separability. Extensive experiments demonstrate that SEAL significantly outperforms existing baselines in reasoning, safety, and planning accuracy under complex, challenging driving conditions, advancing the safety, robustness, and scalability of autonomous driving.

