---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-06-26
---

# cs.ROï¼ˆ2025-06-26ï¼‰

ğŸ“Š å…± **10** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (4 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250620966v1-parallels-between-vla-model-post-training-and-human-motor-learning-p.html">Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends</a></td>
  <td>æå‡ºåè®­ç»ƒç­–ç•¥ä»¥æå‡VLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20966v1" data-paper-url="./papers/250620966v1-parallels-between-vla-model-post-training-and-human-motor-learning-p.html" onclick="toggleFavorite(this, '2506.20966v1', 'Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250621250v1-actllm-action-consistency-tuned-large-language-model.html">ACTLLM: Action Consistency Tuned Large Language Model</a></td>
  <td>æå‡ºACTLLMä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„æœºå™¨äººæ“ä½œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21250v1" data-paper-url="./papers/250621250v1-actllm-action-consistency-tuned-large-language-model.html" onclick="toggleFavorite(this, '2506.21250v1', 'ACTLLM: Action Consistency Tuned Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250621057v1-knowledge-driven-imitation-learning-enabling-generalization-across-d.html">Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions</a></td>
  <td>æå‡ºçŸ¥è¯†é©±åŠ¨æ¨¡ä»¿å­¦ä¹ ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">diffusion policy</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21057v1" data-paper-url="./papers/250621057v1-knowledge-driven-imitation-learning-enabling-generalization-across-d.html" onclick="toggleFavorite(this, '2506.21057v1', 'Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250621205v2-dynamic-risk-aware-mppi-for-mobile-robots-in-crowds-via-efficient-mo.html">Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte Carlo Approximations</a></td>
  <td>æå‡ºåŠ¨æ€é£é™©æ„ŸçŸ¥MPPIä»¥è§£å†³ç§»åŠ¨æœºå™¨äººåœ¨äººç¾¤ä¸­çš„å®‰å…¨å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21205v2" data-paper-url="./papers/250621205v2-dynamic-risk-aware-mppi-for-mobile-robots-in-crowds-via-efficient-mo.html" onclick="toggleFavorite(this, '2506.21205v2', 'Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte Carlo Approximations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>5</td>
  <td><a href="./papers/250621539v1-worldvla-towards-autoregressive-action-world-model.html">WorldVLA: Towards Autoregressive Action World Model</a></td>
  <td>æå‡ºWorldVLAä»¥è§£å†³åŠ¨ä½œç”Ÿæˆä¸å›¾åƒç†è§£çš„ç»Ÿä¸€é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21539v1" data-paper-url="./papers/250621539v1-worldvla-towards-autoregressive-action-world-model.html" onclick="toggleFavorite(this, '2506.21539v1', 'WorldVLA: Towards Autoregressive Action World Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250621732v1-experimental-investigation-of-pose-informed-reinforcement-learning-f.html">Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation</a></td>
  <td>æå‡ºåŸºäºå§¿æ€ä¿¡æ¯çš„å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³æ»‘è½¬é©±åŠ¨è§†è§‰å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21732v1" data-paper-url="./papers/250621732v1-experimental-investigation-of-pose-informed-reinforcement-learning-f.html" onclick="toggleFavorite(this, '2506.21732v1', 'Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250621041v2-seal-vision-language-model-based-safe-end-to-end-cooperative-autonom.html">SEAL: Vision-Language Model-Based Safe End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling</a></td>
  <td>æå‡ºSEALæ¡†æ¶ä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„å®‰å…¨è‡ªåŠ¨é©¾é©¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21041v2" data-paper-url="./papers/250621041v2-seal-vision-language-model-based-safe-end-to-end-cooperative-autonom.html" onclick="toggleFavorite(this, '2506.21041v2', 'SEAL: Vision-Language Model-Based Safe End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250621030v2-step-planner-constructing-cross-hierarchical-subgoal-tree-as-an-embo.html">STEP Planner: Constructing cross-hierarchical subgoal tree as an embodied long-horizon task planner</a></td>
  <td>æå‡ºSTEPè§„åˆ’å™¨ä»¥è§£å†³é•¿è¿œä»»åŠ¡è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21030v2" data-paper-url="./papers/250621030v2-step-planner-constructing-cross-hierarchical-subgoal-tree-as-an-embo.html" onclick="toggleFavorite(this, '2506.21030v2', 'STEP Planner: Constructing cross-hierarchical subgoal tree as an embodied long-horizon task planner')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250620982v1-our-coding-adventure-using-llms-to-personalise-the-narrative-of-a-ta.html">Our Coding Adventure: Using LLMs to Personalise the Narrative of a Tangible Programming Robot for Preschoolers</a></td>
  <td>æå‡ºåˆ©ç”¨LLMsä¸ªæ€§åŒ–ç¼–ç¨‹æœºå™¨äººæ•…äº‹ä»¥è§£å†³å­¦å‰æ•™è‚²æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20982v1" data-paper-url="./papers/250620982v1-our-coding-adventure-using-llms-to-personalise-the-narrative-of-a-ta.html" onclick="toggleFavorite(this, '2506.20982v1', 'Our Coding Adventure: Using LLMs to Personalise the Narrative of a Tangible Programming Robot for Preschoolers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250620969v1-thermaldiffusion-visual-to-thermal-image-to-image-translation-for-au.html">ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation</a></td>
  <td>æå‡ºThermalDiffusionä»¥è§£å†³çƒ­æˆåƒæ•°æ®ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20969v1" data-paper-url="./papers/250620969v1-thermaldiffusion-visual-to-thermal-image-to-image-translation-for-au.html" onclick="toggleFavorite(this, '2506.20969v1', 'ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)