---
layout: default
title: A Multifidelity Sim-to-Real Pipeline for Verifiable and Compositional Reinforcement Learning
---

# A Multifidelity Sim-to-Real Pipeline for Verifiable and Compositional Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.01249" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.01249v1</a>
  <a href="https://arxiv.org/pdf/2312.01249.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.01249v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.01249v1', 'A Multifidelity Sim-to-Real Pipeline for Verifiable and Compositional Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cyrus Neary, Christian Ellis, Aryaman Singh Samyal, Craig Lennon, Ufuk Topcu

**åˆ†ç±»**: cs.RO, cs.AI, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2023-12-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¤šç½®ä¿¡åº¦Sim-to-Realç®¡é“ï¼Œç”¨äºå¯éªŒè¯å’Œå¯ç»„åˆçš„å¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `Sim-to-Real` `æœºå™¨äººæ§åˆ¶` `å¤šç½®ä¿¡åº¦ä»¿çœŸ` `å¯ç»„åˆæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚æœºå™¨äººä»»åŠ¡ä¸­éš¾ä»¥ä¿è¯ç­–ç•¥çš„å¯é æ€§å’Œé€‚åº”æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä»¿çœŸç¯å¢ƒè¿ç§»åˆ°çœŸå®ç¯å¢ƒæ—¶ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§å¯ç»„åˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¹¶å®šä¹‰æ•°å­¦æ¥å£ï¼Œå®ç°å­ç­–ç•¥çš„ç‹¬ç«‹è®­ç»ƒå’ŒéªŒè¯ã€‚
3. é€šè¿‡å¤šç½®ä¿¡åº¦ä»¿çœŸç®¡é“éªŒè¯å­ç­–ç•¥æ€§èƒ½ï¼Œå¹¶æ ¹æ®ä»¿çœŸä¸ç°å®çš„å·®å¼‚è¿­ä»£ä¼˜åŒ–å­ä»»åŠ¡å’Œæ¥å£ï¼Œæœ€ç»ˆæˆåŠŸéƒ¨ç½²åœ¨æ— äººåœ°é¢æœºå™¨äººä¸Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºå¹¶å±•ç¤ºäº†ä¸€ä¸ªå¯ç»„åˆçš„æ¡†æ¶ï¼Œç”¨äºåœ¨å¤šç½®ä¿¡åº¦Sim-to-Realç®¡é“ä¸­è®­ç»ƒå’ŒéªŒè¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç³»ç»Ÿï¼Œä»¥ä¾¿åœ¨ç‰©ç†ç¡¬ä»¶ä¸Šéƒ¨ç½²å¯é ä¸”é€‚åº”æ€§å¼ºçš„RLç­–ç•¥ã€‚é€šè¿‡å°†å¤æ‚çš„æœºå™¨äººä»»åŠ¡åˆ†è§£ä¸ºç»„ä»¶å­ä»»åŠ¡ï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹é—´å®šä¹‰æ•°å­¦æ¥å£ï¼Œè¯¥æ¡†æ¶å…è®¸ç‹¬ç«‹è®­ç»ƒå’Œæµ‹è¯•ç›¸åº”çš„å­ä»»åŠ¡ç­–ç•¥ï¼ŒåŒæ—¶æä¾›å¯¹å…¶ç»„åˆäº§ç”Ÿçš„æ•´ä½“è¡Œä¸ºçš„ä¿è¯ã€‚é€šè¿‡ä½¿ç”¨å¤šç½®ä¿¡åº¦ä»¿çœŸç®¡é“éªŒè¯è¿™äº›å­ä»»åŠ¡ç­–ç•¥çš„æ€§èƒ½ï¼Œè¯¥æ¡†æ¶ä¸ä»…å¯ä»¥å®ç°é«˜æ•ˆçš„RLè®­ç»ƒï¼Œè¿˜å¯ä»¥æ ¹æ®ä»¿çœŸä¸ç°å®ä¹‹é—´å·®å¼‚å¸¦æ¥çš„æŒ‘æˆ˜æ¥æ”¹è¿›å­ä»»åŠ¡åŠå…¶æ¥å£ã€‚åœ¨ä¸€ä¸ªå®éªŒæ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨è¯¥æ¡†æ¶æ¥è®­ç»ƒå’Œéƒ¨ç½²ä¸€ä¸ªå¯ç»„åˆçš„RLç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸæˆåŠŸåœ°é©¾é©¶äº†Warthogæ— äººåœ°é¢æœºå™¨äººã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸé¢ä¸´ç€Sim-to-Realçš„æŒ‘æˆ˜ï¼Œå³åœ¨ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒçš„ç­–ç•¥éš¾ä»¥ç›´æ¥åº”ç”¨äºçœŸå®ä¸–ç•Œã€‚æ­¤å¤–ï¼Œå¯¹äºå¤æ‚çš„æœºå™¨äººä»»åŠ¡ï¼Œæ•´ä½“è®­ç»ƒéš¾åº¦å¤§ï¼Œä¸”éš¾ä»¥ä¿è¯ç­–ç•¥çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ç­–ç•¥ç»„åˆè¡Œä¸ºçš„éªŒè¯æœºåˆ¶ï¼Œéš¾ä»¥åº”å¯¹çœŸå®ç¯å¢ƒä¸­çš„ä¸ç¡®å®šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤æ‚çš„æœºå™¨äººä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªç‹¬ç«‹çš„å­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªå­ä»»åŠ¡è®­ç»ƒç›¸åº”çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚é€šè¿‡å®šä¹‰å­ä»»åŠ¡ä¹‹é—´çš„æ•°å­¦æ¥å£ï¼Œå¯ä»¥ç»„åˆè¿™äº›å­ç­–ç•¥æ¥å®Œæˆæ•´ä½“ä»»åŠ¡ã€‚åŒæ—¶ï¼Œåˆ©ç”¨å¤šç½®ä¿¡åº¦ä»¿çœŸç¯å¢ƒæ¥éªŒè¯å­ç­–ç•¥çš„æ€§èƒ½ï¼Œå¹¶æ ¹æ®ä»¿çœŸç»“æœè°ƒæ•´å­ä»»åŠ¡çš„å®šä¹‰å’Œæ¥å£ï¼Œä»è€Œæé«˜ç­–ç•¥åœ¨çœŸå®ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) ä»»åŠ¡åˆ†è§£æ¨¡å—ï¼šå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œå¹¶å®šä¹‰å­ä»»åŠ¡ä¹‹é—´çš„æ•°å­¦æ¥å£ã€‚2) å­ç­–ç•¥è®­ç»ƒæ¨¡å—ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ç‹¬ç«‹è®­ç»ƒæ¯ä¸ªå­ä»»åŠ¡çš„ç­–ç•¥ã€‚3) å¤šç½®ä¿¡åº¦ä»¿çœŸæ¨¡å—ï¼šä½¿ç”¨ä¸åŒç½®ä¿¡åº¦çš„ä»¿çœŸç¯å¢ƒæ¥éªŒè¯å­ç­–ç•¥çš„æ€§èƒ½ï¼Œå¹¶è¯„ä¼°å…¶åœ¨çœŸå®ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚4) ç­–ç•¥ç»„åˆä¸éªŒè¯æ¨¡å—ï¼šå°†è®­ç»ƒå¥½çš„å­ç­–ç•¥ç»„åˆèµ·æ¥ï¼Œå¹¶é€šè¿‡ä»¿çœŸéªŒè¯æ•´ä½“ç­–ç•¥çš„æ€§èƒ½ã€‚5) è¿­ä»£ä¼˜åŒ–æ¨¡å—ï¼šæ ¹æ®ä»¿çœŸç»“æœè°ƒæ•´å­ä»»åŠ¡çš„å®šä¹‰å’Œæ¥å£ï¼Œå¹¶é‡æ–°è®­ç»ƒå­ç­–ç•¥ï¼Œç›´åˆ°æ»¡è¶³æ€§èƒ½è¦æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå¯ç»„åˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸ç‹¬ç«‹è®­ç»ƒå’ŒéªŒè¯å­ç­–ç•¥ï¼Œå¹¶é€šè¿‡æ•°å­¦æ¥å£å°†å…¶ç»„åˆèµ·æ¥ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜åˆ©ç”¨å¤šç½®ä¿¡åº¦ä»¿çœŸç¯å¢ƒæ¥æé«˜ç­–ç•¥åœ¨çœŸå®ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¯ä»¥é™ä½è®­ç»ƒéš¾åº¦ï¼Œè¿˜å¯ä»¥æé«˜ç­–ç•¥çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å­ç­–ç•¥è®­ç»ƒæ¨¡å—ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å„ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¦‚Q-learningã€SARSAã€Actor-Criticç­‰ã€‚åœ¨å¤šç½®ä¿¡åº¦ä»¿çœŸæ¨¡å—ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„ç‰©ç†å¼•æ“å’Œç¯å¢ƒæ¨¡å‹æ¥æ¨¡æ‹ŸçœŸå®ç¯å¢ƒã€‚åœ¨ç­–ç•¥ç»„åˆä¸éªŒè¯æ¨¡å—ä¸­ï¼Œéœ€è¦å®šä¹‰åˆé€‚çš„ç»„åˆè§„åˆ™å’ŒéªŒè¯æŒ‡æ ‡ã€‚åœ¨è¿­ä»£ä¼˜åŒ–æ¨¡å—ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å„ç§ä¼˜åŒ–ç®—æ³•æ¥è°ƒæ•´å­ä»»åŠ¡çš„å®šä¹‰å’Œæ¥å£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†æ‰€æå‡ºçš„æ¡†æ¶åœ¨Warthogæ— äººåœ°é¢æœºå™¨äººä¸Šçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯ä»¥æˆåŠŸåœ°è®­ç»ƒå’Œéƒ¨ç½²ä¸€ä¸ªå¯ç»„åˆçš„RLç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆåœ°é©¾é©¶æœºå™¨äººå®Œæˆå„ç§ä»»åŠ¡ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ§åˆ¶é¢†åŸŸï¼Œä¾‹å¦‚æ— äººé©¾é©¶ã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€æœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨å¤šç½®ä¿¡åº¦ä»¿çœŸç¯å¢ƒè¿›è¡ŒéªŒè¯ï¼Œå¯ä»¥æé«˜æœºå™¨äººç­–ç•¥çš„å¯é æ€§å’Œé€‚åº”æ€§ï¼Œé™ä½å¼€å‘æˆæœ¬ï¼ŒåŠ é€Ÿæœºå™¨äººæŠ€æœ¯çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose and demonstrate a compositional framework for training and verifying reinforcement learning (RL) systems within a multifidelity sim-to-real pipeline, in order to deploy reliable and adaptable RL policies on physical hardware. By decomposing complex robotic tasks into component subtasks and defining mathematical interfaces between them, the framework allows for the independent training and testing of the corresponding subtask policies, while simultaneously providing guarantees on the overall behavior that results from their composition. By verifying the performance of these subtask policies using a multifidelity simulation pipeline, the framework not only allows for efficient RL training, but also for a refinement of the subtasks and their interfaces in response to challenges arising from discrepancies between simulation and reality. In an experimental case study we apply the framework to train and deploy a compositional RL system that successfully pilots a Warthog unmanned ground robot.

