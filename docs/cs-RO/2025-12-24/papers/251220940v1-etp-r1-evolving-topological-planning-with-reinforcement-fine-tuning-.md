---
layout: default
title: "ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments"
---

# ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20940" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20940v1</a>
  <a href="https://arxiv.org/pdf/2512.20940.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20940v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20940v1', 'ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuhao Ye, Sitong Mao, Yuxiang Cui, Xuan Yu, Shichao Zhai, Wen Chen, Shunbo Zhou, Rong Xiong, Yue Wang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-24

**å¤‡æ³¨**: 8 pages, 6 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Cepillar/ETP-R1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ETP-R1ï¼šé€šè¿‡å¼ºåŒ–å¾®è°ƒæ¼”åŒ–æ‹“æ‰‘è§„åˆ’ï¼Œè§£å†³è¿ç»­ç¯å¢ƒä¸‹çš„è§†è§‰-è¯­è¨€å¯¼èˆªé—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `è¿ç»­ç¯å¢ƒ` `æ‹“æ‰‘è§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `é¢„è®­ç»ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå›¾çš„VLN-CEæ–¹æ³•åœ¨åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®å’Œå…ˆè¿›è®­ç»ƒèŒƒå¼æ–¹é¢è½åäºåŸºäºLVLMçš„æ–¹æ³•ã€‚
2. ETP-R1é€šè¿‡æ„å»ºå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†å’Œå¼•å…¥å¼ºåŒ–å¾®è°ƒï¼Œæå‡äº†åŸºäºå›¾çš„VLN-CEæ¨¡å‹çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒETP-R1åœ¨R2R-CEå’ŒRxR-CEåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„state-of-the-artæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºETP-R1æ¡†æ¶ï¼Œæ—¨åœ¨å¼¥åˆåŸºäºå›¾çš„è§†è§‰-è¯­è¨€å¯¼èˆªåœ¨è¿ç»­ç¯å¢ƒï¼ˆVLN-CEï¼‰ä¸­çš„æ–¹æ³•ä¸åŸºäºå¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚ETP-R1å°†æ•°æ®è§„æ¨¡åŒ–å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åº”ç”¨äºåŸºäºå›¾çš„VLN-CEæ¨¡å‹ã€‚é¦–å…ˆï¼Œåˆ©ç”¨Gemini APIæ„å»ºé«˜è´¨é‡ã€å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤šæ ·åŒ–çš„ã€ä½å¹»è§‰çš„æ‹“æ‰‘è½¨è¿¹æŒ‡ä»¤ï¼Œä¸ºåŸºäºå›¾çš„ç­–ç•¥æä¾›äº†ä¸°å¯Œçš„ç›‘ç£ï¼Œä»¥å°†è¯­è¨€æ˜ å°„åˆ°æ‹“æ‰‘è·¯å¾„ã€‚é€šè¿‡ç»Ÿä¸€æ¥è‡ªR2Rå’ŒRxRä»»åŠ¡çš„æ•°æ®è¿›è¡Œè”åˆé¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥åŠ å¼ºäº†è¿™ä¸€åŸºç¡€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„è®­ç»ƒèŒƒå¼ï¼Œæœ€ç»ˆå°†é—­ç¯ã€åœ¨çº¿RFTé¦–æ¬¡åº”ç”¨äºåŸºäºå›¾çš„VLN-CEæ¨¡å‹ï¼Œå¹¶ç”±Group Relative Policy Optimization (GRPO)ç®—æ³•æä¾›æ”¯æŒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•éå¸¸æœ‰æ•ˆï¼Œåœ¨R2R-CEå’ŒRxR-CEåŸºå‡†æµ‹è¯•ä¸­éƒ½å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰-è¯­è¨€å¯¼èˆªåœ¨è¿ç»­ç¯å¢ƒï¼ˆVLN-CEï¼‰ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨è¿ç»­ç¯å¢ƒä¸­å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®ã€‚ç°æœ‰åŸºäºå›¾çš„æ–¹æ³•è™½ç„¶é€šè¿‡å°†ç¯å¢ƒæŠ½è±¡ä¸ºæ‹“æ‰‘åœ°å›¾å¹¶ç®€åŒ–åŠ¨ä½œç©ºé—´ï¼ˆä»…éœ€é€‰æ‹©èˆªç‚¹ï¼‰æé«˜äº†æ•ˆç‡ï¼Œä½†åœ¨åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®å’Œå…ˆè¿›è®­ç»ƒèŒƒå¼æ–¹é¢ä¸å¦‚åŸºäºå¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ–¹æ³•ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡åŸºäºå›¾çš„æ–¹æ³•çš„æ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿå……åˆ†åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®å’Œå…ˆè¿›è®­ç»ƒèŒƒå¼ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ•°æ®è§„æ¨¡åŒ–å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åº”ç”¨äºåŸºäºå›¾çš„VLN-CEæ¨¡å‹ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œä¸ºæ¨¡å‹æä¾›ä¸°å¯Œçš„ç›‘ç£ä¿¡æ¯ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å°†è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°æ‹“æ‰‘è·¯å¾„ã€‚ç„¶åï¼Œé€šè¿‡å¼ºåŒ–å¾®è°ƒè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨çœŸå®ç¯å¢ƒä¸­æ›´å¥½åœ°å¯¼èˆªã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šETP-R1æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) **æ•°æ®æ„å»ºé˜¶æ®µ**ï¼šåˆ©ç”¨Gemini APIç”Ÿæˆå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·åŒ–çš„ã€ä½å¹»è§‰çš„æ‹“æ‰‘è½¨è¿¹æŒ‡ä»¤ã€‚2) **é¢„è®­ç»ƒé˜¶æ®µ**ï¼šä½¿ç”¨R2Rå’ŒRxRä»»åŠ¡çš„æ•°æ®è¿›è¡Œè”åˆé¢„è®­ç»ƒï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚3) **å¼ºåŒ–å¾®è°ƒé˜¶æ®µ**ï¼šä½¿ç”¨Group Relative Policy Optimization (GRPO)ç®—æ³•è¿›è¡Œé—­ç¯ã€åœ¨çº¿å¼ºåŒ–å¾®è°ƒï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºé¦–æ¬¡å°†é—­ç¯ã€åœ¨çº¿å¼ºåŒ–å¾®è°ƒåº”ç”¨äºåŸºäºå›¾çš„VLN-CEæ¨¡å‹ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨Gemini APIæ„å»ºå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®é›†ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ï¼Œå®ƒä¸ºæ¨¡å‹æä¾›äº†ä¸°å¯Œçš„ç›‘ç£ä¿¡æ¯ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ è¯­è¨€æŒ‡ä»¤å’Œç¯å¢ƒä¹‹é—´çš„å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ„å»ºé˜¶æ®µï¼Œä½¿ç”¨Gemini APIç”Ÿæˆå¤šæ ·åŒ–çš„ã€ä½å¹»è§‰çš„æ‹“æ‰‘è½¨è¿¹æŒ‡ä»¤ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é¢„æµ‹æ­£ç¡®çš„æ‹“æ‰‘è·¯å¾„ã€‚åœ¨å¼ºåŒ–å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨Group Relative Policy Optimization (GRPO)ç®—æ³•æ¥ä¼˜åŒ–æ¨¡å‹çš„ç­–ç•¥ï¼ŒGRPOç®—æ³•é€šè¿‡æ¯”è¾ƒä¸åŒæ™ºèƒ½ä½“çš„ç­–ç•¥æ¥å‡å°‘æ–¹å·®ï¼Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20940v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20940v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20940v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

ETP-R1åœ¨R2R-CEå’ŒRxR-CEåŸºå‡†æµ‹è¯•ä¸­éƒ½å–å¾—äº†æ–°çš„state-of-the-artæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨R2R-CEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒETP-R1åœ¨æ‰€æœ‰ä¸»è¦æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœ¨RxR-CEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒETP-R1ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯å¼€å‘èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è¯­éŸ³æŒ‡ä»¤åœ¨å¤æ‚ç¯å¢ƒä¸­è‡ªä¸»å¯¼èˆªçš„æœºå™¨äººï¼Œæˆ–è€…æ„å»ºæ›´åŠ æ™ºèƒ½åŒ–çš„è™šæ‹Ÿç°å®ç¯å¢ƒï¼Œä½¿ç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ä¸è™šæ‹Ÿç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºæ™ºèƒ½å®¶å±…é¢†åŸŸï¼Œå®ç°æ›´åŠ ä¾¿æ·çš„è¯­éŸ³æ§åˆ¶å’Œè‡ªåŠ¨åŒ–æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Navigation in Continuous Environments (VLN-CE) requires an embodied agent to navigate towards target in continuous environments, following natural language instructions. While current graph-based methods offer an efficient, structured approach by abstracting the environment into a topological map and simplifying the action space to waypoint selection, they lag behind methods based on Large Vision-Language Models (LVLMs) in leveraging large-scale data and advanced training paradigms. In this paper, we try to bridge this gap by introducing ETP-R1, a framework that applies the paradigm of scaling up data and Reinforcement Fine-Tuning (RFT) to a graph-based VLN-CE model. To build a strong foundation, we first construct a high-quality, large-scale pretraining dataset using the Gemini API. This dataset consists of diverse, low-hallucination instructions for topological trajectories, providing rich supervision for our graph-based policy to map language to topological paths. This foundation is further strengthened by unifying data from both R2R and RxR tasks for joint pretraining. Building on this, we introduce a three-stage training paradigm, which culminates in the first application of closed-loop, online RFT to a graph-based VLN-CE model, powered by the Group Relative Policy Optimization (GRPO) algorithm. Extensive experiments demonstrate that our approach is highly effective, establishing new state-of-the-art performance across all major metrics on both the R2R-CE and RxR-CE benchmarks. Our code is available at https://github.com/Cepillar/ETP-R1.

