---
layout: default
title: Structured Imitation Learning of Interactive Policies through Inverse Games
---

# Structured Imitation Learning of Interactive Policies through Inverse Games

**arXiv**: [2511.12848v1](https://arxiv.org/abs/2511.12848) | [PDF](https://arxiv.org/pdf/2511.12848.pdf)

**ä½œè€…**: Max M. Sun, Todd Murphey

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17

**å¤‡æ³¨**: Presented at the "Workshop on Generative Modeling Meets Human-Robot Interaction" at Robotics: Science and Systems 2025. Workshop website: https://sites.google.com/view/gai-hri/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡é€†å‘åšå¼ˆçš„ç»“æž„åŒ–æ¨¡ä»¿å­¦ä¹ äº¤äº’ç­–ç•¥**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `äº¤äº’ç­–ç•¥` `é€†å‘åšå¼ˆ` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `ç¤¾äº¤å¯¼èˆª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨å¤šæ™ºèƒ½ä½“äº¤äº’åœºæ™¯ä¸­ï¼Œç”±äºŽè¡Œä¸ºå¤æ‚æ€§é«˜ï¼Œå­¦ä¹ äº¤äº’ç­–ç•¥é¢ä¸´æŒ‘æˆ˜ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§ç»“æž„åŒ–æ¨¡ä»¿å­¦ä¹ æ¡†æž¶ï¼Œç»“åˆç”Ÿæˆå¼å•æ™ºèƒ½ä½“ç­–ç•¥å­¦ä¹ å’Œåšå¼ˆè®ºç»“æž„ï¼Œæ˜¾å¼åˆ†ç¦»ä¸ªä½“è¡Œä¸ºå­¦ä¹ å’Œæ™ºèƒ½ä½“é—´ä¾èµ–å…³ç³»å­¦ä¹ ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨ç¤¾äº¤å¯¼èˆªä»»åŠ¡ä¸­ï¼Œä»…ä½¿ç”¨å°‘é‡æ¼”ç¤ºæ•°æ®ï¼Œå³å¯æ˜¾è‘—æå‡éžäº¤äº’ç­–ç•¥æ€§èƒ½ï¼ŒæŽ¥è¿‘çœŸå®žäº¤äº’ç­–ç•¥ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽç”Ÿæˆæ¨¡åž‹çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•æœ€è¿‘åœ¨ä»Žäººç±»æ¼”ç¤ºä¸­å­¦ä¹ é«˜å¤æ‚åº¦è¿åŠ¨æŠ€èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæžœã€‚ç„¶è€Œï¼Œåœ¨å…±äº«ç©ºé—´ä¸­ä¸Žäººç±»åè°ƒè€Œæ— éœ€æ˜¾å¼é€šä¿¡çš„äº¤äº’ç­–ç•¥çš„æ¨¡ä»¿å­¦ä¹ ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™æ˜¯å› ä¸ºå¤šæ™ºèƒ½ä½“äº¤äº’ä¸­çš„è¡Œä¸ºå¤æ‚æ€§è¿œé«˜äºŽéžäº¤äº’ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æž„åŒ–æ¨¡ä»¿å­¦ä¹ æ¡†æž¶ï¼Œç”¨äºŽäº¤äº’ç­–ç•¥ï¼Œè¯¥æ¡†æž¶å°†ç”Ÿæˆå¼å•æ™ºèƒ½ä½“ç­–ç•¥å­¦ä¹ ä¸Žçµæ´»ä¸”å¯Œæœ‰è¡¨çŽ°åŠ›çš„åšå¼ˆè®ºç»“æž„ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜Žç¡®åœ°å°†å­¦ä¹ åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä»Žå¤šæ™ºèƒ½ä½“æ¼”ç¤ºä¸­å­¦ä¹ ä¸ªä½“è¡Œä¸ºæ¨¡å¼ï¼›ç„¶åŽï¼Œæˆ‘ä»¬é€šè¿‡è§£å†³é€†å‘åšå¼ˆé—®é¢˜æ¥ç»“æž„åŒ–åœ°å­¦ä¹ æ™ºèƒ½ä½“é—´çš„ä¾èµ–å…³ç³»ã€‚åœ¨åˆæˆçš„5æ™ºèƒ½ä½“ç¤¾äº¤å¯¼èˆªä»»åŠ¡ä¸­çš„åˆæ­¥ç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æ”¹è¿›äº†éžäº¤äº’ç­–ç•¥ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨50ä¸ªæ¼”ç¤ºå°±èƒ½è¾¾åˆ°ä¸ŽçœŸå®žäº¤äº’ç­–ç•¥ç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›ç»“æžœçªæ˜¾äº†ç»“æž„åŒ–æ¨¡ä»¿å­¦ä¹ åœ¨äº¤äº’çŽ¯å¢ƒä¸­çš„æ½œåŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨å­¦ä¹ äº¤äº’ç­–ç•¥æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ™ºèƒ½ä½“çŽ¯å¢ƒä¸‹ï¼Œé¢ä¸´ç€è¡Œä¸ºå¤æ‚æ€§é«˜çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•éš¾ä»¥æ•æ‰æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„ç­–ç•¥åœ¨äº¤äº’çŽ¯å¢ƒä¸­è¡¨çŽ°ä¸ä½³ã€‚å°¤å…¶æ˜¯åœ¨æ²¡æœ‰æ˜¾å¼é€šä¿¡çš„æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“éœ€è¦é€šè¿‡è§‚å¯Ÿå…¶ä»–æ™ºèƒ½ä½“çš„è¡Œä¸ºæ¥æŽ¨æ–­å…¶æ„å›¾ï¼Œè¿™è¿›ä¸€æ­¥å¢žåŠ äº†å­¦ä¹ çš„éš¾åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†äº¤äº’ç­–ç•¥çš„å­¦ä¹ åˆ†è§£ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œå­¦ä¹ ä¸ªä½“æ™ºèƒ½ä½“çš„è¡Œä¸ºæ¨¡å¼ï¼›ç„¶åŽï¼Œå­¦ä¹ æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚é€šè¿‡è¿™ç§åˆ†è§£ï¼Œå¯ä»¥é™ä½Žå­¦ä¹ çš„å¤æ‚æ€§ï¼Œå¹¶æ›´å¥½åœ°æ•æ‰æ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’è¡Œä¸ºã€‚è®ºæ–‡åˆ©ç”¨åšå¼ˆè®ºçš„æ¡†æž¶æ¥å»ºæ¨¡æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶ä½¿ç”¨é€†å‘åšå¼ˆçš„æ–¹æ³•æ¥å­¦ä¹ è¿™äº›å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä¸ªä½“è¡Œä¸ºå­¦ä¹ é˜¶æ®µï¼šä½¿ç”¨æ ‡å‡†çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œä¾‹å¦‚è¡Œä¸ºå…‹éš†æˆ–ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ï¼Œä»Žå¤šæ™ºèƒ½ä½“æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ æ¯ä¸ªæ™ºèƒ½ä½“çš„ä¸ªä½“è¡Œä¸ºç­–ç•¥ã€‚2) æ™ºèƒ½ä½“é—´ä¾èµ–å…³ç³»å­¦ä¹ é˜¶æ®µï¼šå°†æ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’å»ºæ¨¡ä¸ºä¸€ä¸ªåšå¼ˆï¼Œå¹¶ä½¿ç”¨é€†å‘åšå¼ˆçš„æ–¹æ³•æ¥å­¦ä¹ åšå¼ˆçš„æ”¶ç›Šå‡½æ•°ã€‚æ”¶ç›Šå‡½æ•°åæ˜ äº†æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä¾‹å¦‚åˆä½œã€ç«žäº‰æˆ–åè°ƒã€‚é€šè¿‡å­¦ä¹ æ”¶ç›Šå‡½æ•°ï¼Œå¯ä»¥æŽ¨æ–­å‡ºæ¯ä¸ªæ™ºèƒ½ä½“åœ¨ä¸åŒæƒ…å†µä¸‹çš„æœ€ä½³ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽå°†ç»“æž„åŒ–çš„åšå¼ˆè®ºæ¡†æž¶å¼•å…¥åˆ°æ¨¡ä»¿å­¦ä¹ ä¸­ï¼Œä»Žè€Œèƒ½å¤Ÿæ˜¾å¼åœ°å»ºæ¨¡å’Œå­¦ä¹ æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚ä¸Žä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’è¡Œä¸ºï¼Œå¹¶å­¦ä¹ åˆ°æ›´æœ‰æ•ˆçš„äº¤äº’ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜ä½¿ç”¨äº†é€†å‘åšå¼ˆçš„æ–¹æ³•æ¥å­¦ä¹ æ”¶ç›Šå‡½æ•°ï¼Œè¿™ä½¿å¾—è¯¥æ–¹æ³•èƒ½å¤Ÿä»Žæœ‰é™çš„æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ åˆ°æ™ºèƒ½ä½“ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¸ªä½“è¡Œä¸ºå­¦ä¹ é˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨å„ç§æ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚è¡Œä¸ºå…‹éš†ã€Daggeræˆ–ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ã€‚åœ¨æ™ºèƒ½ä½“é—´ä¾èµ–å…³ç³»å­¦ä¹ é˜¶æ®µï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„åšå¼ˆæ¨¡åž‹å’Œé€†å‘åšå¼ˆç®—æ³•ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨åŠ¿åšå¼ˆæ¥å»ºæ¨¡åˆä½œè¡Œä¸ºï¼Œå¹¶ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥å­¦ä¹ æ”¶ç›Šå‡½æ•°ã€‚æŸå¤±å‡½æ•°å¯ä»¥è®¾è®¡ä¸ºçœŸå®žç­–ç•¥ä¸Žå­¦ä¹ åˆ°çš„ç­–ç•¥ä¹‹é—´çš„å·®å¼‚ï¼Œä¾‹å¦‚KLæ•£åº¦æˆ–äº¤å‰ç†µã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨åˆæˆçš„5æ™ºèƒ½ä½“ç¤¾äº¤å¯¼èˆªä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨50ä¸ªæ¼”ç¤ºæ•°æ®ï¼Œå°±æ˜¾è‘—æ”¹è¿›äº†éžäº¤äº’ç­–ç•¥ï¼Œå¹¶è¾¾åˆ°äº†ä¸ŽçœŸå®žäº¤äº’ç­–ç•¥ç›¸å½“çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜Žè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»Žæœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ åˆ°æ™ºèƒ½ä½“ä¹‹é—´çš„å¤æ‚äº¤äº’å…³ç³»ï¼Œå¹¶å­¦ä¹ åˆ°æœ‰æ•ˆçš„äº¤äº’ç­–ç•¥ã€‚å…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼ŒåŽŸæ–‡æœªç»™å‡ºæ˜Žç¡®æ•°æ®ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§éœ€è¦å¤šæ™ºèƒ½ä½“åä½œçš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººåä½œã€ç¤¾äº¤å¯¼èˆªç­‰ã€‚é€šè¿‡å­¦ä¹ äººç±»çš„äº¤äº’ç­–ç•¥ï¼Œå¯ä»¥ä½¿æ™ºèƒ½ä½“æ›´å¥½åœ°ä¸Žäººç±»æˆ–å…¶ä»–æ™ºèƒ½ä½“è¿›è¡Œåä½œï¼Œä»Žè€Œæé«˜ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºŽåˆ†æžäººç±»çš„äº¤äº’è¡Œä¸ºï¼Œä¾‹å¦‚ç†è§£äººç±»åœ¨ç¤¾äº¤åœºåˆä¸­çš„è¡Œä¸ºæ¨¡å¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.

