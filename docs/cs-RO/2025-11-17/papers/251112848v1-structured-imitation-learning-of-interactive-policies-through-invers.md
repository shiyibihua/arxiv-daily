---
layout: default
title: Structured Imitation Learning of Interactive Policies through Inverse Games
---

# Structured Imitation Learning of Interactive Policies through Inverse Games

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.12848" target="_blank" class="toolbar-btn">arXiv: 2511.12848v1</a>
    <a href="https://arxiv.org/pdf/2511.12848.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.12848v1" 
            onclick="toggleFavorite(this, '2511.12848v1', 'Structured Imitation Learning of Interactive Policies through Inverse Games')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Max M. Sun, Todd Murphey

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17

**å¤‡æ³¨**: Presented at the "Workshop on Generative Modeling Meets Human-Robot Interaction" at Robotics: Science and Systems 2025. Workshop website: https://sites.google.com/view/gai-hri/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡é€†å‘åšå¼ˆçš„ç»“æ„åŒ–æ¨¡ä»¿å­¦ä¹ äº¤äº’ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `äº¤äº’ç­–ç•¥` `é€†å‘åšå¼ˆ` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `ç¤¾äº¤å¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨å¤šæ™ºèƒ½ä½“äº¤äº’åœºæ™¯ä¸­ï¼Œç”±äºè¡Œä¸ºå¤æ‚æ€§é«˜ï¼Œå­¦ä¹ äº¤äº’ç­–ç•¥é¢ä¸´æŒ‘æˆ˜ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§ç»“æ„åŒ–æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆç”Ÿæˆå¼å•æ™ºèƒ½ä½“ç­–ç•¥å­¦ä¹ å’Œåšå¼ˆè®ºç»“æ„ï¼Œæ˜¾å¼åˆ†ç¦»ä¸ªä½“è¡Œä¸ºå­¦ä¹ å’Œæ™ºèƒ½ä½“é—´ä¾èµ–å…³ç³»å­¦ä¹ ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¤¾äº¤å¯¼èˆªä»»åŠ¡ä¸­ï¼Œä»…ä½¿ç”¨å°‘é‡æ¼”ç¤ºæ•°æ®ï¼Œå³å¯æ˜¾è‘—æå‡éäº¤äº’ç­–ç•¥æ€§èƒ½ï¼Œæ¥è¿‘çœŸå®äº¤äº’ç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºç”Ÿæˆæ¨¡å‹çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•æœ€è¿‘åœ¨ä»äººç±»æ¼”ç¤ºä¸­å­¦ä¹ é«˜å¤æ‚åº¦è¿åŠ¨æŠ€èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œåœ¨å…±äº«ç©ºé—´ä¸­ä¸äººç±»åè°ƒè€Œæ— éœ€æ˜¾å¼é€šä¿¡çš„äº¤äº’ç­–ç•¥çš„æ¨¡ä»¿å­¦ä¹ ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™æ˜¯å› ä¸ºå¤šæ™ºèƒ½ä½“äº¤äº’ä¸­çš„è¡Œä¸ºå¤æ‚æ€§è¿œé«˜äºéäº¤äº’ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºäº¤äº’ç­–ç•¥ï¼Œè¯¥æ¡†æ¶å°†ç”Ÿæˆå¼å•æ™ºèƒ½ä½“ç­–ç•¥å­¦ä¹ ä¸çµæ´»ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„åšå¼ˆè®ºç»“æ„ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜ç¡®åœ°å°†å­¦ä¹ åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä»å¤šæ™ºèƒ½ä½“æ¼”ç¤ºä¸­å­¦ä¹ ä¸ªä½“è¡Œä¸ºæ¨¡å¼ï¼›ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è§£å†³é€†å‘åšå¼ˆé—®é¢˜æ¥ç»“æ„åŒ–åœ°å­¦ä¹ æ™ºèƒ½ä½“é—´çš„ä¾èµ–å…³ç³»ã€‚åœ¨åˆæˆçš„5æ™ºèƒ½ä½“ç¤¾äº¤å¯¼èˆªä»»åŠ¡ä¸­çš„åˆæ­¥ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æ”¹è¿›äº†éäº¤äº’ç­–ç•¥ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨50ä¸ªæ¼”ç¤ºå°±èƒ½è¾¾åˆ°ä¸çœŸå®äº¤äº’ç­–ç•¥ç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ç»“æ„åŒ–æ¨¡ä»¿å­¦ä¹ åœ¨äº¤äº’ç¯å¢ƒä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨å­¦ä¹ äº¤äº’ç­–ç•¥æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸‹ï¼Œé¢ä¸´ç€è¡Œä¸ºå¤æ‚æ€§é«˜çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•éš¾ä»¥æ•æ‰æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„ç­–ç•¥åœ¨äº¤äº’ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ã€‚å°¤å…¶æ˜¯åœ¨æ²¡æœ‰æ˜¾å¼é€šä¿¡çš„æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“éœ€è¦é€šè¿‡è§‚å¯Ÿå…¶ä»–æ™ºèƒ½ä½“çš„è¡Œä¸ºæ¥æ¨æ–­å…¶æ„å›¾ï¼Œè¿™è¿›ä¸€æ­¥å¢åŠ äº†å­¦ä¹ çš„éš¾åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†äº¤äº’ç­–ç•¥çš„å­¦ä¹ åˆ†è§£ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œå­¦ä¹ ä¸ªä½“æ™ºèƒ½ä½“çš„è¡Œä¸ºæ¨¡å¼ï¼›ç„¶åï¼Œå­¦ä¹ æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚é€šè¿‡è¿™ç§åˆ†è§£ï¼Œå¯ä»¥é™ä½å­¦ä¹ çš„å¤æ‚æ€§ï¼Œå¹¶æ›´å¥½åœ°æ•æ‰æ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’è¡Œä¸ºã€‚è®ºæ–‡åˆ©ç”¨åšå¼ˆè®ºçš„æ¡†æ¶æ¥å»ºæ¨¡æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶ä½¿ç”¨é€†å‘åšå¼ˆçš„æ–¹æ³•æ¥å­¦ä¹ è¿™äº›å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä¸ªä½“è¡Œä¸ºå­¦ä¹ é˜¶æ®µï¼šä½¿ç”¨æ ‡å‡†çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œä¾‹å¦‚è¡Œä¸ºå…‹éš†æˆ–ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ï¼Œä»å¤šæ™ºèƒ½ä½“æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ æ¯ä¸ªæ™ºèƒ½ä½“çš„ä¸ªä½“è¡Œä¸ºç­–ç•¥ã€‚2) æ™ºèƒ½ä½“é—´ä¾èµ–å…³ç³»å­¦ä¹ é˜¶æ®µï¼šå°†æ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’å»ºæ¨¡ä¸ºä¸€ä¸ªåšå¼ˆï¼Œå¹¶ä½¿ç”¨é€†å‘åšå¼ˆçš„æ–¹æ³•æ¥å­¦ä¹ åšå¼ˆçš„æ”¶ç›Šå‡½æ•°ã€‚æ”¶ç›Šå‡½æ•°åæ˜ äº†æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä¾‹å¦‚åˆä½œã€ç«äº‰æˆ–åè°ƒã€‚é€šè¿‡å­¦ä¹ æ”¶ç›Šå‡½æ•°ï¼Œå¯ä»¥æ¨æ–­å‡ºæ¯ä¸ªæ™ºèƒ½ä½“åœ¨ä¸åŒæƒ…å†µä¸‹çš„æœ€ä½³ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†ç»“æ„åŒ–çš„åšå¼ˆè®ºæ¡†æ¶å¼•å…¥åˆ°æ¨¡ä»¿å­¦ä¹ ä¸­ï¼Œä»è€Œèƒ½å¤Ÿæ˜¾å¼åœ°å»ºæ¨¡å’Œå­¦ä¹ æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’è¡Œä¸ºï¼Œå¹¶å­¦ä¹ åˆ°æ›´æœ‰æ•ˆçš„äº¤äº’ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜ä½¿ç”¨äº†é€†å‘åšå¼ˆçš„æ–¹æ³•æ¥å­¦ä¹ æ”¶ç›Šå‡½æ•°ï¼Œè¿™ä½¿å¾—è¯¥æ–¹æ³•èƒ½å¤Ÿä»æœ‰é™çš„æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ åˆ°æ™ºèƒ½ä½“ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¸ªä½“è¡Œä¸ºå­¦ä¹ é˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨å„ç§æ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚è¡Œä¸ºå…‹éš†ã€Daggeræˆ–ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ã€‚åœ¨æ™ºèƒ½ä½“é—´ä¾èµ–å…³ç³»å­¦ä¹ é˜¶æ®µï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„åšå¼ˆæ¨¡å‹å’Œé€†å‘åšå¼ˆç®—æ³•ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨åŠ¿åšå¼ˆæ¥å»ºæ¨¡åˆä½œè¡Œä¸ºï¼Œå¹¶ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥å­¦ä¹ æ”¶ç›Šå‡½æ•°ã€‚æŸå¤±å‡½æ•°å¯ä»¥è®¾è®¡ä¸ºçœŸå®ç­–ç•¥ä¸å­¦ä¹ åˆ°çš„ç­–ç•¥ä¹‹é—´çš„å·®å¼‚ï¼Œä¾‹å¦‚KLæ•£åº¦æˆ–äº¤å‰ç†µã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨åˆæˆçš„5æ™ºèƒ½ä½“ç¤¾äº¤å¯¼èˆªä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨50ä¸ªæ¼”ç¤ºæ•°æ®ï¼Œå°±æ˜¾è‘—æ”¹è¿›äº†éäº¤äº’ç­–ç•¥ï¼Œå¹¶è¾¾åˆ°äº†ä¸çœŸå®äº¤äº’ç­–ç•¥ç›¸å½“çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»æœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ åˆ°æ™ºèƒ½ä½“ä¹‹é—´çš„å¤æ‚äº¤äº’å…³ç³»ï¼Œå¹¶å­¦ä¹ åˆ°æœ‰æ•ˆçš„äº¤äº’ç­–ç•¥ã€‚å…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼ŒåŸæ–‡æœªç»™å‡ºæ˜ç¡®æ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤šæ™ºèƒ½ä½“åä½œçš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººåä½œã€ç¤¾äº¤å¯¼èˆªç­‰ã€‚é€šè¿‡å­¦ä¹ äººç±»çš„äº¤äº’ç­–ç•¥ï¼Œå¯ä»¥ä½¿æ™ºèƒ½ä½“æ›´å¥½åœ°ä¸äººç±»æˆ–å…¶ä»–æ™ºèƒ½ä½“è¿›è¡Œåä½œï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºåˆ†æäººç±»çš„äº¤äº’è¡Œä¸ºï¼Œä¾‹å¦‚ç†è§£äººç±»åœ¨ç¤¾äº¤åœºåˆä¸­çš„è¡Œä¸ºæ¨¡å¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.

