---
layout: default
title: OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation
---

# OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.11479" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.11479v1</a>
  <a href="https://arxiv.org/pdf/2508.11479.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.11479v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.11479v1', 'OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tatiana Zemskova, Aleksei Staroverov, Dmitry Yudin, Aleksandr Panov

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-15

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/CognitiveAISystems/OVSegDT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOVSegDTä»¥è§£å†³å¼€æ”¾è¯æ±‡ç›®æ ‡å¯¼èˆªä¸­çš„æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡å¯¼èˆª` `å˜æ¢å™¨` `æ™ºèƒ½ä½“` `è¯­ä¹‰åˆ†æ”¯` `ç†µè‡ªé€‚åº”æŸå¤±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›®æ ‡å¯¼èˆªæ–¹æ³•åœ¨å°å‹æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³å’Œé¢‘ç¹ç¢°æ’ç­‰ä¸å®‰å…¨è¡Œä¸ºã€‚
2. OVSegDTé€šè¿‡å¼•å…¥è¯­ä¹‰åˆ†æ”¯å’Œç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶ï¼Œæä¾›ç²¾ç¡®çš„ç©ºé—´çº¿ç´¢å¹¶åŠ¨æ€å¹³è¡¡ä¿¡å·ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOVSegDTåœ¨æœªè§ç±»åˆ«ä¸Šçš„è¡¨ç°ä¸å·²è§ç±»åˆ«ç›¸å½“ï¼Œä¸”è®­ç»ƒæ ·æœ¬å¤æ‚åº¦é™ä½33%ï¼Œç¢°æ’æ¬¡æ•°å‡å°‘50%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼€æ”¾è¯æ±‡ç›®æ ‡å¯¼èˆªè¦æ±‚å…·èº«æ™ºèƒ½ä½“æ ¹æ®è‡ªç”±å½¢å¼çš„è¯­è¨€æè¿°åˆ°è¾¾ç›®æ ‡ç‰©ä½“ï¼ŒåŒ…æ‹¬è®­ç»ƒæœŸé—´æœªè§è¿‡çš„ç±»åˆ«ã€‚ç°æœ‰çš„ç«¯åˆ°ç«¯ç­–ç•¥åœ¨å°å‹æ¨¡æ‹Ÿå™¨æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆï¼Œè™½ç„¶åœ¨è®­ç»ƒåœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¸è¶³ï¼Œä¸”å¸¸å¸¸è¡¨ç°å‡ºä¸å®‰å…¨è¡Œä¸ºï¼ˆé¢‘ç¹ç¢°æ’ï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†OVSegDTï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„å˜æ¢å™¨ç­–ç•¥ï¼Œé€šè¿‡ä¸¤ä¸ªååŒç»„ä»¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç¬¬ä¸€ä¸ªç»„ä»¶æ˜¯è¯­ä¹‰åˆ†æ”¯ï¼ŒåŒ…æ‹¬ç›®æ ‡äºŒè¿›åˆ¶æ©è†œçš„ç¼–ç å™¨å’Œè¾…åŠ©åˆ†å‰²æŸå¤±å‡½æ•°ï¼Œèƒ½å¤Ÿä¸ºæ–‡æœ¬ç›®æ ‡æä¾›ç²¾ç¡®çš„ç©ºé—´çº¿ç´¢ã€‚ç¬¬äºŒä¸ªç»„ä»¶æ˜¯æå‡ºçš„ç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶ï¼Œè¿™æ˜¯ä¸€ç§æ¯æ ·æœ¬è°ƒåº¦å™¨ï¼Œæ ¹æ®ç­–ç•¥ç†µæŒç»­å¹³è¡¡æ¨¡ä»¿å’Œå¼ºåŒ–ä¿¡å·ï¼Œæ¶ˆé™¤äº†è„†å¼±çš„æ‰‹åŠ¨é˜¶æ®µåˆ‡æ¢ã€‚è¿™äº›æ”¹è¿›å°†è®­ç»ƒçš„æ ·æœ¬å¤æ‚åº¦é™ä½äº†33%ï¼Œå¹¶å°†ç¢°æ’æ¬¡æ•°å‡å°‘äº†ä¸€åŠï¼ŒåŒæ—¶ä¿æŒä½æ¨ç†æˆæœ¬ï¼ˆ130Må‚æ•°ï¼Œä»…RGBè¾“å…¥ï¼‰ã€‚åœ¨HM3D-OVONä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æœªè§ç±»åˆ«ä¸Šçš„è¡¨ç°ä¸å·²è§ç±»åˆ«ç›¸å½“ï¼Œå¹¶åœ¨æ²¡æœ‰æ·±åº¦ã€é‡Œç¨‹è®¡æˆ–å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå»ºç«‹äº†æœ€æ–°çš„ç»“æœï¼ˆéªŒè¯é›†æœªè§ç±»åˆ«çš„æˆåŠŸç‡ä¸º40.1%ï¼Œè·¯å¾„é•¿åº¦æ¯”ä¸º20.9%ï¼‰ã€‚ä»£ç å¯åœ¨https://github.com/CognitiveAISystems/OVSegDTè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡ç›®æ ‡å¯¼èˆªä¸­æ™ºèƒ½ä½“æ— æ³•æ³›åŒ–åˆ°æœªè§ç±»åˆ«çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å°å‹æ¨¡æ‹Ÿå™¨æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¯¼è‡´åœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸”å­˜åœ¨å®‰å…¨éšæ‚£ï¼Œå¦‚é¢‘ç¹ç¢°æ’ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOVSegDTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥è¯­ä¹‰åˆ†æ”¯å’Œç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶æ¥å¢å¼ºæ™ºèƒ½ä½“çš„å¯¼èˆªèƒ½åŠ›ã€‚è¯­ä¹‰åˆ†æ”¯æä¾›äº†ç›®æ ‡çš„ç²¾ç¡®ç©ºé—´ä¿¡æ¯ï¼Œè€Œç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶åˆ™åŠ¨æ€è°ƒæ•´æ¨¡ä»¿å’Œå¼ºåŒ–å­¦ä¹ ä¿¡å·çš„å¹³è¡¡ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOVSegDTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¯­ä¹‰åˆ†æ”¯å’Œç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶ã€‚è¯­ä¹‰åˆ†æ”¯è´Ÿè´£å¤„ç†ç›®æ ‡çš„äºŒè¿›åˆ¶æ©è†œï¼Œå¹¶é€šè¿‡è¾…åŠ©åˆ†å‰²æŸå¤±å‡½æ•°æ¥å¢å¼ºç›®æ ‡çš„ç©ºé—´å®šä½èƒ½åŠ›ã€‚ç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶åˆ™æ ¹æ®ç­–ç•¥çš„ç†µå€¼åŠ¨æ€è°ƒæ•´è®­ç»ƒä¿¡å·çš„æƒé‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šOVSegDTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶æœºåˆ¶ï¼Œè¿™ä¸€æœºåˆ¶èƒ½å¤Ÿæ¶ˆé™¤ä¼ ç»Ÿæ–¹æ³•ä¸­è„†å¼±çš„æ‰‹åŠ¨é˜¶æ®µåˆ‡æ¢ï¼Œæå‡äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒOVSegDTé‡‡ç”¨äº†130Må‚æ•°çš„è½»é‡çº§ç½‘ç»œç»“æ„ï¼Œè¾“å…¥ä»…ä¸ºRGBå›¾åƒã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è¾…åŠ©åˆ†å‰²æŸå¤±ï¼Œä»¥å¢å¼ºè¯­ä¹‰åˆ†æ”¯çš„æ•ˆæœï¼ŒåŒæ—¶ç†µè‡ªé€‚åº”è°ƒåˆ¶ç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¿¡å·å¹³è¡¡ã€‚æ•´ä½“è®¾è®¡æ—¨åœ¨é™ä½è®­ç»ƒå¤æ‚åº¦å¹¶æé«˜æ™ºèƒ½ä½“çš„å®‰å…¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OVSegDTåœ¨HM3D-OVONæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„å®éªŒç»“æœï¼Œæœªè§ç±»åˆ«çš„æˆåŠŸç‡è¾¾åˆ°40.1%ï¼Œè·¯å¾„é•¿åº¦æ¯”ä¸º20.9%ã€‚ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…¶è®­ç»ƒæ ·æœ¬å¤æ‚åº¦é™ä½äº†33%ï¼Œç¢°æ’æ¬¡æ•°å‡å°‘äº†ä¸€åŠï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OVSegDTåœ¨å¼€æ”¾è¯æ±‡ç›®æ ‡å¯¼èˆªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…å’Œè‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ä¸­ã€‚å…¶èƒ½å¤Ÿå¤„ç†æœªè§ç±»åˆ«çš„èƒ½åŠ›ï¼Œä½¿å¾—æ™ºèƒ½ä½“åœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­æ›´åŠ çµæ´»å’Œå®‰å…¨ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„è‡ªä¸»ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Open-vocabulary Object Goal Navigation requires an embodied agent to reach objects described by free-form language, including categories never seen during training. Existing end-to-end policies overfit small simulator datasets, achieving high success on training scenes but failing to generalize and exhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a lightweight transformer policy that tackles these issues with two synergistic components. The first component is the semantic branch, which includes an encoder for the target binary mask and an auxiliary segmentation loss function, grounding the textual goal and providing precise spatial cues. The second component consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample scheduler that continuously balances imitation and reinforcement signals according to the policy entropy, eliminating brittle manual phase switches. These additions cut the sample complexity of training by 33%, and reduce collision count in two times while keeping inference cost low (130M parameters, RGB-only input). On HM3D-OVON, our model matches the performance on unseen categories to that on seen ones and establishes state-of-the-art results (40.1% SR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language models. Code is available at https://github.com/CognitiveAISystems/OVSegDT.

