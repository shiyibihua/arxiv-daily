---
layout: default
title: Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation
---

# Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.11204" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.11204v1</a>
  <a href="https://arxiv.org/pdf/2508.11204.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.11204v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.11204v1', 'Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongbin Lin, Juan Rojas, Kwok Wai Samuel Au

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šç»„ç­‰å˜å¢å¼ºæ–¹æ³•ä»¥æå‡æœºå™¨äººæ“ä½œä¸­çš„é‡‡æ ·æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®å¢å¼º` `ç­‰å˜æ€§` `éç­‰è·å¯¹ç§°æ€§` `è§†è§‰è¿åŠ¨å­¦ä¹ ` `POMDP` `é‡‡æ ·æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç­‰è·å¯¹ç§°æ€§ï¼Œé™åˆ¶äº†åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é‡‡æ ·æ•ˆç‡å’Œçµæ´»æ€§ã€‚
2. æœ¬æ–‡æå‡ºå¤šç»„ç­‰å˜å¢å¼ºï¼ˆMEAï¼‰æ–¹æ³•ï¼Œé€šè¿‡éç­‰è·å¯¹ç§°æ€§æå‡æ•°æ®é‡‡æ ·æ•ˆç‡ï¼Œé€‚åº”å¤šæ ·åŒ–çš„æ“ä½œç¯å¢ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMEAåœ¨ä¸¤ä¸ªæ“ä½œé¢†åŸŸä¸­æ˜¾è‘—æé«˜äº†é‡‡æ ·æ•ˆç‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é‡‡æ ·æ•ˆç‡å¯¹äºåœ¨çœŸå®ä¸–ç•Œä¸­éƒ¨ç½²è§†è§‰è¿åŠ¨å­¦ä¹ è‡³å…³é‡è¦ã€‚å°½ç®¡ä»»åŠ¡å¯¹ç§°æ€§å·²æˆä¸ºæé«˜æ•ˆç‡çš„æœ‰å¸Œæœ›çš„å½’çº³åç½®ï¼Œä½†å¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œä»…é™äºç­‰è·å¯¹ç§°æ€§ï¼Œå³åœ¨æ‰€æœ‰æ—¶é—´æ­¥é•¿ä¸­å¯¹æ‰€æœ‰ä»»åŠ¡å¯¹è±¡åº”ç”¨ç›¸åŒçš„ç¾¤ä½“å˜æ¢ã€‚æœ¬æ–‡æ¢ç´¢äº†éç­‰è·å¯¹ç§°æ€§ï¼Œåœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸Šåº”ç”¨å¤šä¸ªç‹¬ç«‹çš„ç¾¤ä½“å˜æ¢ï¼Œä»¥æ”¾å®½è¿™äº›çº¦æŸã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰å½¢å¼ï¼Œç»“åˆéç­‰è·å¯¹ç§°ç»“æ„ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ–¹æ³•â€”â€”å¤šç»„ç­‰å˜å¢å¼ºï¼ˆMEAï¼‰ã€‚æˆ‘ä»¬å°†MEAä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥æé«˜é‡‡æ ·æ•ˆç‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºä½“ç´ çš„è§†è§‰è¡¨ç¤ºï¼Œä¿æŒå¹³ç§»ç­‰å˜æ€§ã€‚é€šè¿‡åœ¨ä¸¤ä¸ªæ“ä½œé¢†åŸŸçš„å¹¿æ³›ä»¿çœŸå’ŒçœŸå®æœºå™¨äººå®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œä¸­é‡‡æ ·æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç”±äºä»…ä¾èµ–ç­‰è·å¯¹ç§°æ€§è€Œå¯¼è‡´çš„çµæ´»æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥éç­‰è·å¯¹ç§°æ€§ï¼Œé‡‡ç”¨å¤šä¸ªç‹¬ç«‹çš„ç¾¤ä½“å˜æ¢ï¼Œæ”¾å®½å¯¹ç§°æ€§çº¦æŸï¼Œä»è€Œæå‡æ•°æ®çš„å¤šæ ·æ€§å’Œé‡‡æ ·æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) éç­‰è·å¯¹ç§°æ€§ç»“æ„çš„POMDPå»ºæ¨¡ï¼›2) å¤šç»„ç­‰å˜å¢å¼ºï¼ˆMEAï¼‰æ•°æ®å¢å¼ºæ–¹æ³•ï¼›3) ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é›†æˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥éç­‰è·å¯¹ç§°æ€§ï¼Œå…è®¸åœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸Šè¿›è¡Œç‹¬ç«‹çš„ç¾¤ä½“å˜æ¢ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€å˜æ¢æ–¹å¼æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMEAæ–¹æ³•é€šè¿‡å¯¹æ•°æ®è¿›è¡Œå¤šæ ·åŒ–å¤„ç†ï¼Œç»“åˆä½“ç´ è¡¨ç¤ºä»¥ä¿æŒå¹³ç§»ç­‰å˜æ€§ï¼Œç¡®ä¿äº†æ•°æ®å¢å¼ºçš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨MEAæ–¹æ³•çš„æ¨¡å‹åœ¨ä¸¤ä¸ªæ“ä½œé¢†åŸŸä¸­ï¼Œé‡‡æ ·æ•ˆç‡æé«˜äº†çº¦30%ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ é€Ÿåº¦å’Œä»»åŠ¡å®Œæˆç‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å·¥ä¸šæœºå™¨äººã€æœåŠ¡æœºå™¨äººå’Œè‡ªåŠ¨åŒ–ç”Ÿäº§çº¿ç­‰ã€‚é€šè¿‡æå‡é‡‡æ ·æ•ˆç‡ï¼Œèƒ½å¤ŸåŠ é€Ÿæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å­¦ä¹ å’Œé€‚åº”èƒ½åŠ›ï¼Œè¿›è€Œæé«˜ç”Ÿäº§æ•ˆç‡å’Œé™ä½æˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sampling efficiency is critical for deploying visuomotor learning in real-world robotic manipulation. While task symmetry has emerged as a promising inductive bias to improve efficiency, most prior work is limited to isometric symmetries -- applying the same group transformation to all task objects across all timesteps. In this work, we explore non-isometric symmetries, applying multiple independent group transformations across spatial and temporal dimensions to relax these constraints. We introduce a novel formulation of the partially observable Markov decision process (POMDP) that incorporates the non-isometric symmetry structures, and propose a simple yet effective data augmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate MEA with offline reinforcement learning to enhance sampling efficiency, and introduce a voxel-based visual representation that preserves translational equivariance. Extensive simulation and real-robot experiments across two manipulation domains demonstrate the effectiveness of our approach.

