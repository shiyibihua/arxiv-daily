---
layout: default
title: HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models
---

# HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.09928" target="_blank" class="toolbar-btn">arXiv: 2512.09928v1</a>
    <a href="https://arxiv.org/pdf/2512.09928.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.09928v1" 
            onclick="toggleFavorite(this, '2512.09928v1', 'HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Minghui Lin, Pengxiang Ding, Shu Wang, Zifeng Zhuang, Yang Liu, Xinyang Tong, Wenxuan Song, Shangke Lyu, Siteng Huang, Donglin Wang

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-10

**Â§áÊ≥®**: Project page: https://hifvla.github.io Github: https://github.com/OpenHelix-Team/HiF-VLA

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**HiF-VLAÔºöÂà©Áî®ËøêÂä®Ë°®ÂæÅËøõË°åÂèåÂêëÊó∂Â∫èÊé®ÁêÜÔºåÊèêÂçáËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÈïøÊó∂Â∫èÊìç‰ΩúËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Êú∫Âô®‰∫∫Êìç‰Ωú` `ÈïøÊó∂Á®ãËßÑÂàí` `ËøêÂä®Ë°®ÂæÅ` `Êó∂Â∫èÊé®ÁêÜ` `ÂêéËßÅ‰πãÊòé` `ËøúËßÅ` `TransformerÁΩëÁªú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. VLAÊ®°ÂûãÈÄöÂ∏∏ÂÅáËÆæÈ©¨Â∞îÂèØÂ§´ÊÄßÔºå‰ªÖ‰æùËµñÂΩìÂâçËßÇÊµãÔºåÁº∫‰πèÈïøÊó∂Á®ã‰∏ÄËá¥ÊÄßÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. HiF-VLAÂà©Áî®ËøêÂä®Ë°®ÂæÅÁºñÁ†ÅËøáÂéªÂä®ÊÄÅÂπ∂È¢ÑÊµãÊú™Êù•ËøêÂä®ÔºåÈÄöËøáÂèåÂêëÊó∂Â∫èÊé®ÁêÜÂ¢ûÂº∫Ê®°ÂûãÂØπÁéØÂ¢ÉÂèòÂåñÁöÑÁêÜËß£ÂíåÈ¢ÑÊµãËÉΩÂäõ„ÄÇ
3. HiF-VLAÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØïÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÊèêÂçáÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫HiF-VLAÔºàHindsight, Insight, and Foresight for VLAsÔºâÁöÑÁªü‰∏ÄÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËøêÂä®Ë°®ÂæÅËøõË°åÂèåÂêëÊó∂Â∫èÊé®ÁêÜÔºå‰ªéËÄåÊèêÂçáËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®ÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ„ÄÇHiF-VLAÂ∞ÜËøêÂä®ËßÜ‰∏∫‰∏ÄÁßçÊõ¥Á¥ßÂáëÂíå‰ø°ÊÅØ‰∏∞ÂØåÁöÑÊó∂Â∫è‰∏ä‰∏ãÊñáÂíå‰∏ñÁïåÂä®ÊÄÅË°®ÂæÅÔºåËÉΩÂ§üÊçïÊçâÁä∂ÊÄÅÈó¥ÁöÑÂèòÂåñÂπ∂ËøáÊª§ÈùôÊÄÅÂÉèÁ¥†Á∫ßÂô™Â£∞„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂêéËßÅ‰πãÊòéÂÖàÈ™åÁºñÁ†ÅËøáÂéªÂä®ÊÄÅÔºåÈÄöËøáËøúËßÅÊé®ÁêÜÈ¢ÑÊµãÊú™Êù•ËøêÂä®ÔºåÂπ∂ÈÄöËøáÂêéËßÅ‰πãÊòéË∞ÉËäÇÁöÑËÅîÂêà‰∏ìÂÆ∂Êï¥Âêà‰∏§ËÄÖÔºå‰ªéËÄåÂÆûÁé∞‚ÄúËæπÊÄùËÄÉËæπË°åÂä®‚ÄùÁöÑÈïøÊó∂Á®ãÊìç‰ΩúÊ®°Âºè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHiF-VLAÂú®LIBERO-LongÂíåCALVIN ABC-DÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÂº∫Â§ßÁöÑÂü∫Á∫øÔºåÂπ∂‰∏îÂú®ÂÆûÈôÖÁöÑÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊîπËøõÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÁöÑÂπøÊ≥õÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®Â§ÑÁêÜÈïøÊó∂Á®ãÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°Êó∂ÔºåÈÄöÂ∏∏ÂÅáËÆæÁéØÂ¢ÉÂÖ∑ÊúâÈ©¨Â∞îÂèØÂ§´ÊÄßÔºåÂç≥‰ªÖ‰æùËµñ‰∫éÂΩìÂâçÊó∂ÂàªÁöÑËßÇÊµãÊù•ÂÜ≥Á≠ñ„ÄÇËøôÁßçÊñπÊ≥ïÂøΩÁï•‰∫ÜÂéÜÂè≤‰ø°ÊÅØÂíåÊú™Êù•È¢ÑÊµãÔºåÂØºËá¥Ê®°ÂûãÁº∫‰πèÂØπÁéØÂ¢ÉÂä®ÊÄÅÂèòÂåñÁöÑÁêÜËß£Ôºå‰ªéËÄåÂΩ±Âìç‰∫ÜÈïøÊó∂Á®ã‰ªªÂä°ÁöÑÂÆåÊàêÊïàÊûú„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫éÊó†Ê≥ïÊúâÊïàÂà©Áî®Êó∂Â∫è‰ø°ÊÅØÔºåÂØºËá¥ÂÜ≥Á≠ñÁº∫‰πèËøûË¥ØÊÄßÂíåËøúËßÅÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöHiF-VLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËøêÂä®ËßÜ‰∏∫‰∏ÄÁßçÊõ¥Á¥ßÂáë„ÄÅ‰ø°ÊÅØÈáèÊõ¥Â§ßÁöÑÊó∂Â∫è‰∏ä‰∏ãÊñáË°®ÂæÅ„ÄÇËøêÂä®ËÉΩÂ§üÊçïÊçâÁä∂ÊÄÅÈó¥ÁöÑÂèòÂåñÔºåÂêåÊó∂ËøáÊª§ÊéâÈùôÊÄÅÁöÑÂÉèÁ¥†Á∫ßÂô™Â£∞Ôºå‰ªéËÄåÊèê‰æõÊõ¥ÊúâÊïàÁöÑÁéØÂ¢ÉÂä®ÊÄÅ‰ø°ÊÅØ„ÄÇÈÄöËøáÂØπËøáÂéªËøêÂä®ÁöÑÂõûÈ°æÔºàHindsightÔºâÂíåÂØπÊú™Êù•ËøêÂä®ÁöÑÈ¢ÑÊµãÔºàForesightÔºâÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£ÁéØÂ¢ÉÁöÑÂèòÂåñË∂ãÂäøÔºå‰ªéËÄåÂÅöÂá∫Êõ¥ÊòéÊô∫ÁöÑÂÜ≥Á≠ñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHiF-VLAÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂêéËßÅ‰πãÊòéÔºàHindsightÔºâÊ®°Âùó„ÄÅËøúËßÅÔºàForesightÔºâÊ®°ÂùóÂíåÂêéËßÅ‰πãÊòéË∞ÉËäÇÁöÑËÅîÂêà‰∏ìÂÆ∂ÔºàHindsight-modulated Joint ExpertÔºâÊ®°Âùó„ÄÇÂêéËßÅ‰πãÊòéÊ®°ÂùóÁî®‰∫éÁºñÁ†ÅËøáÂéªÁöÑËøêÂä®ËΩ®ËøπÔºåÊèê‰æõÂéÜÂè≤‰ø°ÊÅØÔºõËøúËßÅÊ®°ÂùóÁî®‰∫éÈ¢ÑÊµãÊú™Êù•ÁöÑËøêÂä®ËΩ®ËøπÔºåÊèê‰æõÊú™Êù•‰ø°ÊÅØÔºõËÅîÂêà‰∏ìÂÆ∂Ê®°ÂùóÂàôÂ∞Ü‰∏§ËÄÖÊï¥ÂêàÔºåÂπ∂Ê†πÊçÆÂêéËßÅ‰πãÊòéÊ®°ÂùóÁöÑËæìÂá∫Âä®ÊÄÅË∞ÉÊï¥ËøúËßÅÊ®°ÂùóÁöÑÊùÉÈáçÔºå‰ªéËÄåÂÆûÁé∞‚ÄúËæπÊÄùËÄÉËæπË°åÂä®‚ÄùÁöÑÊ®°Âºè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöHiF-VLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂà©Áî®ËøêÂä®Ë°®ÂæÅËøõË°åÂèåÂêëÊó∂Â∫èÊé®ÁêÜ„ÄÇ‰∏é‰º†ÁªüÁöÑ‰ªÖ‰æùËµñÂΩìÂâçËßÇÊµãÁöÑÊñπÊ≥ï‰∏çÂêåÔºåHiF-VLAÂêåÊó∂ËÄÉËôë‰∫ÜËøáÂéªÂíåÊú™Êù•ÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÂØπÁéØÂ¢ÉÂä®ÊÄÅÁöÑÁêÜËß£ÂíåÈ¢ÑÊµãËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÂêéËßÅ‰πãÊòéË∞ÉËäÇÁöÑËÅîÂêà‰∏ìÂÆ∂Ê®°ÂùóËÉΩÂ§üÂä®ÊÄÅÂú∞Ë∞ÉÊï¥‰∏çÂêå‰ø°ÊÅØÁöÑÊùÉÈáçÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏çÂêåÁöÑ‰ªªÂä°Âú∫ÊôØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöHiF-VLA‰ΩøÁî®TransformerÁΩëÁªúÊù•ÁºñÁ†ÅËøêÂä®Ë°®ÂæÅÔºåÂπ∂‰ΩøÁî®Ëá™ÁõëÁù£Â≠¶‰π†ÁöÑÊñπÂºèÊù•ËÆ≠ÁªÉËøúËßÅÊ®°Âùó„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ËøêÂä®È¢ÑÊµãÊçüÂ§±ÂíåÂä®‰ΩúÈ¢ÑÊµãÊçüÂ§±„ÄÇÂêéËßÅ‰πãÊòéË∞ÉËäÇÁöÑËÅîÂêà‰∏ìÂÆ∂Ê®°Âùó‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Âä®ÊÄÅË∞ÉÊï¥‰∏çÂêå‰ø°ÊÅØÁöÑÊùÉÈáç„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÊ†πÊçÆ‰∏çÂêåÁöÑ‰ªªÂä°Âú∫ÊôØËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

HiF-VLAÂú®LIBERO-LongÂíåCALVIN ABC-DÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÂº∫Â§ßÁöÑÂü∫Á∫øÊ®°ÂûãÔºåÂπ∂Âú®ÁúüÂÆûÁöÑÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊîπËøõ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂú®LIBERO-LongÂü∫ÂáÜÊµãËØï‰∏≠ÔºåHiF-VLAÁöÑÊàêÂäüÁéáÊèêÈ´ò‰∫ÜXX%ÔºåÂú®CALVIN ABC-DÂü∫ÂáÜÊµãËØï‰∏≠ÔºåHiF-VLAÁöÑÊàêÂäüÁéáÊèêÈ´ò‰∫ÜYY%„ÄÇÊ≠§Â§ñÔºåHiF-VLAÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠‰πüË°®Áé∞Âá∫‰∫ÜËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HiF-VLAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈïøÊó∂Á®ãËßÑÂàíÂíåÊìç‰ΩúÁöÑÊú∫Âô®‰∫∫‰ªªÂä°Ôºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÊú∫Âô®‰∫∫„ÄÅÂåªÁñóÊú∫Âô®‰∫∫Á≠â„ÄÇËØ•Á†îÁ©∂ÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂú®‰∫éÊèêÈ´ò‰∫ÜÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÊïàÁéáÂíåÂèØÈù†ÊÄßÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÂíåÂä®ÊÄÅÁöÑÁéØÂ¢É„ÄÇÊú™Êù•ÔºåHiF-VLAÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Â§öÊ®°ÊÄÅËæìÂÖ•Ôºå‰æãÂ¶ÇÁªìÂêàËØ≠Èü≥ÂíåËß¶Ëßâ‰ø°ÊÅØÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Êô∫ËÉΩÂíåÁÅµÊ¥ªÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.

