---
layout: default
title: Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation
---

# Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.10099" target="_blank" class="toolbar-btn">arXiv: 2512.10099v1</a>
    <a href="https://arxiv.org/pdf/2512.10099.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10099v1" 
            onclick="toggleFavorite(this, '2512.10099v1', 'Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Steven Caro, Stephen L. Smith

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-10

**å¤‡æ³¨**: 8 pages, 8 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/carosteven/HeRD)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHeRDï¼šä¸€ç§ç”¨äºé«˜æ•ˆéæŠ“å–æ“ä½œçš„åˆ†å±‚RL-æ‰©æ•£ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `éæŠ“å–æ“ä½œ` `å¼ºåŒ–å­¦ä¹ ` `æ‰©æ•£æ¨¡å‹` `åˆ†å±‚æ§åˆ¶` `æœºå™¨äººæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. éæŠ“å–æ“ä½œå› å…¶å¤æ‚çš„æ¥è§¦åŠ¨åŠ›å­¦å’Œé•¿ç¨‹è§„åˆ’éœ€æ±‚è€Œæå…·æŒ‘æˆ˜æ€§ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾æ•ˆç‡ä¸æ³›åŒ–æ€§ã€‚
2. HeRDé‡‡ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ -æ‰©æ•£ç­–ç•¥ï¼Œåˆ©ç”¨é«˜å±‚RLé€‰æ‹©ä¸­é—´ç›®æ ‡ï¼Œä½å±‚æ‰©æ•£æ¨¡å‹ç”Ÿæˆè½¨è¿¹ï¼Œå®ç°é«˜æ•ˆæ“ä½œã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHeRDåœ¨æˆåŠŸç‡ã€è·¯å¾„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºéæŠ“å–æ“ä½œæä¾›äº†ä¸€ç§æ–°æ€è·¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºéæŠ“å–æ“ä½œï¼ˆä¾‹å¦‚åœ¨æ‚ä¹±ç¯å¢ƒä¸­æ¨åŠ¨ç‰©ä½“ï¼‰çš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ -æ‰©æ•£ç­–ç•¥ï¼Œç§°ä¸ºHeRDã€‚ç”±äºå¤æ‚çš„æ¥è§¦åŠ¨åŠ›å­¦å’Œé•¿ç¨‹è§„åˆ’éœ€æ±‚ï¼ŒéæŠ“å–æ“ä½œæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ§åˆ¶é—®é¢˜ã€‚HeRDå°†æ¨åŠ¨ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼šé«˜å±‚ç›®æ ‡é€‰æ‹©å’Œä½å±‚è½¨è¿¹ç”Ÿæˆã€‚æˆ‘ä»¬é‡‡ç”¨é«˜å±‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“æ¥é€‰æ‹©ä¸­é—´ç©ºé—´ç›®æ ‡ï¼Œå¹¶ä½¿ç”¨ä½å±‚ç›®æ ‡æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¯è¡Œçš„ã€é«˜æ•ˆçš„è½¨è¿¹ä»¥è¾¾åˆ°è¿™äº›ç›®æ ‡ã€‚è¿™ç§æ¶æ„ç»“åˆäº†RLçš„é•¿æœŸå¥–åŠ±æœ€å¤§åŒ–è¡Œä¸ºå’Œæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨2Dä»¿çœŸç¯å¢ƒä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨æˆåŠŸç‡ã€è·¯å¾„æ•ˆç‡å’Œè·¨å¤šç§ç¯å¢ƒé…ç½®çš„æ³›åŒ–æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå…·æœ‰ç”Ÿæˆå¼ä½å±‚è§„åˆ’çš„åˆ†å±‚æ§åˆ¶æ˜¯å¯æ‰©å±•çš„ã€é¢å‘ç›®æ ‡çš„éæŠ“å–æ“ä½œçš„ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ã€‚ä»£ç ã€æ–‡æ¡£å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å·²å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³éæŠ“å–æ“ä½œä¸­ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸­æ¨åŠ¨ç‰©ä½“æ—¶ï¼Œç”±äºæ¥è§¦åŠ¨åŠ›å­¦å¤æ‚å’Œéœ€è¦é•¿ç¨‹è§„åˆ’è€Œå¯¼è‡´çš„æ§åˆ¶éš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥åœ¨æˆåŠŸç‡ã€è·¯å¾„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¨åŠ¨ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼šé«˜å±‚ç›®æ ‡é€‰æ‹©å’Œä½å±‚è½¨è¿¹ç”Ÿæˆã€‚é«˜å±‚ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥é€‰æ‹©ä¸­é—´ç›®æ ‡ï¼Œä½å±‚ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆ°è¾¾è¿™äº›ç›®æ ‡çš„è½¨è¿¹ã€‚è¿™ç§åˆ†å±‚ç»“æ„æ—¨åœ¨ç»“åˆå¼ºåŒ–å­¦ä¹ çš„é•¿æœŸè§„åˆ’èƒ½åŠ›å’Œæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆå’Œé²æ£’çš„éæŠ“å–æ“ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHeRDæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé«˜å±‚RLæ™ºèƒ½ä½“å’Œä½å±‚ç›®æ ‡æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚é«˜å±‚RLæ™ºèƒ½ä½“è´Ÿè´£æ ¹æ®å½“å‰ç¯å¢ƒçŠ¶æ€é€‰æ‹©ä¸€ä¸ªä¸­é—´ç›®æ ‡ã€‚ä½å±‚æ‰©æ•£æ¨¡å‹åˆ™æ ¹æ®é«˜å±‚é€‰æ‹©çš„ç›®æ ‡ï¼Œç”Ÿæˆä¸€æ¡ä»å½“å‰çŠ¶æ€åˆ°è¾¾è¯¥ç›®æ ‡çš„è½¨è¿¹ã€‚æ•´ä¸ªè¿‡ç¨‹é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä»¥æœ€å¤§åŒ–é•¿æœŸå¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£æ¨¡å‹ç»“åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªåˆ†å±‚æ§åˆ¶æ¡†æ¶ã€‚å¼ºåŒ–å­¦ä¹ è´Ÿè´£é«˜å±‚å†³ç­–ï¼Œæ‰©æ•£æ¨¡å‹è´Ÿè´£ä½å±‚è½¨è¿¹ç”Ÿæˆã€‚è¿™ç§ç»“åˆæ—¢åˆ©ç”¨äº†å¼ºåŒ–å­¦ä¹ çš„é•¿æœŸè§„åˆ’èƒ½åŠ›ï¼Œåˆåˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»è€Œå®ç°äº†æ›´é«˜æ•ˆå’Œé²æ£’çš„éæŠ“å–æ“ä½œã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒHeRDèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„æ¥è§¦åŠ¨åŠ›å­¦å’Œé•¿ç¨‹è§„åˆ’éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šé«˜å±‚RLæ™ºèƒ½ä½“ä½¿ç”¨æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå…·ä½“ç®—æ³•æœªçŸ¥ï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨é¼“åŠ±æ™ºèƒ½ä½“é€‰æ‹©èƒ½å¤Ÿæœ‰æ•ˆæ¨åŠ¨ç‰©ä½“åˆ°è¾¾æœ€ç»ˆç›®æ ‡çš„ä¸­é—´ç›®æ ‡ã€‚ä½å±‚æ‰©æ•£æ¨¡å‹ä½¿ç”¨ç›®æ ‡æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç»™å®šçš„ç›®æ ‡ç”Ÿæˆç›¸åº”çš„è½¨è¿¹ã€‚æ‰©æ•£æ¨¡å‹çš„å…·ä½“ç½‘ç»œç»“æ„å’Œè®­ç»ƒç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHeRDåœ¨2Dä»¿çœŸç¯å¢ƒä¸­ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒHeRDåœ¨æˆåŠŸç‡ã€è·¯å¾„æ•ˆç‡å’Œè·¨å¤šç§ç¯å¢ƒé…ç½®çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå…·æœ‰ç”Ÿæˆå¼ä½å±‚è§„åˆ’çš„åˆ†å±‚æ§åˆ¶æ˜¯å¯æ‰©å±•çš„ã€é¢å‘ç›®æ ‡çš„éæŠ“å–æ“ä½œçš„ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨åŒ–è£…é…ã€ç‰©æµåˆ†æ‹£ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨åŒ–è£…é…ä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•åœ¨æ‹¥æŒ¤çš„ç¯å¢ƒä¸­æ¨åŠ¨é›¶ä»¶åˆ°æŒ‡å®šä½ç½®ã€‚åœ¨ç‰©æµåˆ†æ‹£ä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•é«˜æ•ˆåœ°å°†åŒ…è£¹æ¨é€åˆ°ä¸åŒçš„ä¼ é€å¸¦ä¸Šã€‚è¯¥ç ”ç©¶ä¸ºå®ç°æ›´æ™ºèƒ½ã€æ›´çµæ´»çš„æœºå™¨äººæ“ä½œæä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Nonprehensile manipulation, such as pushing objects across cluttered environments, presents a challenging control problem due to complex contact dynamics and long-horizon planning requirements. In this work, we propose HeRD, a hierarchical reinforcement learning-diffusion policy that decomposes pushing tasks into two levels: high-level goal selection and low-level trajectory generation. We employ a high-level reinforcement learning (RL) agent to select intermediate spatial goals, and a low-level goal-conditioned diffusion model to generate feasible, efficient trajectories to reach them.
>   This architecture combines the long-term reward maximizing behaviour of RL with the generative capabilities of diffusion models. We evaluate our method in a 2D simulation environment and show that it outperforms the state-of-the-art baseline in success rate, path efficiency, and generalization across multiple environment configurations. Our results suggest that hierarchical control with generative low-level planning is a promising direction for scalable, goal-directed nonprehensile manipulation. Code, documentation, and trained models are available: https://github.com/carosteven/HeRD.

