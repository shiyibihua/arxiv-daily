---
layout: default
title: LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning
---

# LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16615" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16615v1</a>
  <a href="https://arxiv.org/pdf/2509.16615.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16615v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16615v1', 'LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jelle Luijkx, Runyu Ma, Zlatan AjanoviÄ‡, Jens Kober

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

**å¤‡æ³¨**: 8 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LLMå¼•å¯¼å¼ºåŒ–å­¦ä¹ ä¸­çš„ä»»åŠ¡å’Œå¯ä¾›æ€§æ¢ç´¢ï¼Œæå‡æœºå™¨äººæ“ä½œæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨äººæ“ä½œ` `å¯ä¾›æ€§` `ä»»åŠ¡è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ“ä½œä¸­é¢ä¸´æ ·æœ¬æ•ˆç‡ä½å’Œæ¢ç´¢ç©ºé—´å¤§çš„æŒ‘æˆ˜ã€‚
2. LLM-TALEåˆ©ç”¨LLMè¿›è¡Œä»»åŠ¡çº§å’Œå¯ä¾›æ€§çº§çš„è§„åˆ’ï¼Œå¼•å¯¼RLæ¢ç´¢ï¼Œæå‡å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLM-TALEåœ¨æ ·æœ¬æ•ˆç‡å’ŒæˆåŠŸç‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„sim-to-realè¿ç§»èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)æ˜¯æœºå™¨äººæ“ä½œçš„ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œä½†å®ƒå¯èƒ½å—åˆ°æ ·æœ¬æ•ˆç‡ä½çš„å½±å“ï¼Œå¹¶ä¸”éœ€è¦å¯¹å¤§å‹çŠ¶æ€-åŠ¨ä½œç©ºé—´è¿›è¡Œå¹¿æ³›çš„æ¢ç´¢ã€‚æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„å¸¸è¯†çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æ¥å¼•å¯¼æ¢ç´¢æ›´æœ‰æ„ä¹‰çš„çŠ¶æ€ã€‚ç„¶è€Œï¼ŒLLMå¯ä»¥äº§ç”Ÿåœ¨è¯­ä¹‰ä¸Šåˆç†ä½†ç‰©ç†ä¸Šä¸å¯è¡Œçš„è®¡åˆ’ï¼Œä»è€Œäº§ç”Ÿä¸å¯é çš„è¡Œä¸ºã€‚æˆ‘ä»¬å¼•å…¥äº†LLM-TALEï¼Œä¸€ä¸ªä½¿ç”¨LLMçš„è§„åˆ’æ¥ç›´æ¥å¼•å¯¼RLæ¢ç´¢çš„æ¡†æ¶ã€‚LLM-TALEé›†æˆäº†ä»»åŠ¡çº§åˆ«å’Œå¯ä¾›æ€§çº§åˆ«çš„è§„åˆ’ï¼Œé€šè¿‡å¼•å¯¼æ™ºèƒ½ä½“æ‰§è¡Œè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŠ¨ä½œæ¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚ä¸å‡è®¾LLMç”Ÿæˆçš„æœ€ä¼˜è®¡åˆ’æˆ–å¥–åŠ±çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒLLM-TALEåœ¨çº¿çº æ­£æ¬¡ä¼˜æ€§ï¼Œå¹¶æ¢ç´¢æ— éœ€äººå·¥ç›‘ç£çš„å¤šæ¨¡æ€å¯ä¾›æ€§çº§åˆ«è®¡åˆ’ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†RLåŸºå‡†æµ‹è¯•ä¸­çš„æŠ“å–å’Œæ”¾ç½®ä»»åŠ¡ä¸Šè¯„ä¼°LLM-TALEï¼Œè§‚å¯Ÿåˆ°ä¸å¼ºå¤§çš„åŸºçº¿ç›¸æ¯”ï¼Œæ ·æœ¬æ•ˆç‡å’ŒæˆåŠŸç‡éƒ½æœ‰æ‰€æé«˜ã€‚çœŸå®æœºå™¨äººå®éªŒè¡¨æ˜äº†æœ‰å¸Œæœ›çš„é›¶æ ·æœ¬sim-to-realè¿ç§»ã€‚ä»£ç å’Œè¡¥å……ææ–™å¯åœ¨https://llm-tale.github.ioè·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œç”±äºçŠ¶æ€-åŠ¨ä½œç©ºé—´å·¨å¤§ï¼Œæ¢ç´¢æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´å­¦ä¹ é€Ÿåº¦æ…¢ï¼Œéš¾ä»¥åº”ç”¨åˆ°å¤æ‚åœºæ™¯ã€‚å³ä½¿åˆ©ç”¨LLMè¿›è¡ŒæŒ‡å¯¼ï¼Œä¹Ÿå¯èƒ½ç”Ÿæˆè¯­ä¹‰åˆç†ä½†ç‰©ç†ä¸Šä¸å¯è¡Œçš„è®¡åˆ’ï¼Œå½±å“æœ€ç»ˆæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLLM-TALEçš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨LLMçš„è§„åˆ’èƒ½åŠ›ï¼Œåœ¨ä»»åŠ¡çº§åˆ«å’Œå¯ä¾›æ€§çº§åˆ«ä¸Šå¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢è¿‡ç¨‹ã€‚é€šè¿‡LLMæä¾›çš„é«˜å±‚æŒ‡ä»¤å’Œå¯¹ç‰©ä½“å¯ä¾›æ€§çš„ç†è§£ï¼Œç¼©å°æ¢ç´¢èŒƒå›´ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¿«åœ°æ‰¾åˆ°æœ‰æ„ä¹‰çš„åŠ¨ä½œåºåˆ—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLLM-TALEæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) LLMè§„åˆ’å™¨ï¼šç”Ÿæˆä»»åŠ¡çº§åˆ«çš„è®¡åˆ’ï¼ˆä¾‹å¦‚ï¼Œæ‹¿èµ·ç‰©ä½“Aï¼Œæ”¾ç½®åˆ°ä½ç½®Bï¼‰å’Œå¯ä¾›æ€§çº§åˆ«çš„è®¡åˆ’ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨æŠ“å–åŠ¨ä½œæ‹¿èµ·ç‰©ä½“Aï¼‰ã€‚2) å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼šæ ¹æ®LLMæä¾›çš„è®¡åˆ’ï¼Œåœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œå¹¶å­¦ä¹ ç­–ç•¥ã€‚3) åœ¨çº¿çº æ­£æœºåˆ¶ï¼šæ£€æµ‹LLMç”Ÿæˆçš„æ¬¡ä¼˜è®¡åˆ’ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¿®æ­£ï¼Œé¿å…å®Œå…¨ä¾èµ–LLMçš„è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šLLM-TALEçš„å…³é”®åˆ›æ–°åœ¨äºåŒæ—¶åˆ©ç”¨LLMè¿›è¡Œä»»åŠ¡çº§å’Œå¯ä¾›æ€§çº§çš„è§„åˆ’ï¼Œå¹¶å°†å…¶ä¸å¼ºåŒ–å­¦ä¹ æ¢ç´¢ç›¸ç»“åˆã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒLLM-TALEä¸å‡è®¾LLMç”Ÿæˆæœ€ä¼˜è®¡åˆ’ï¼Œè€Œæ˜¯é€šè¿‡åœ¨çº¿çº æ­£æœºåˆ¶æ¥å¤„ç†LLMçš„æ¬¡ä¼˜è¾“å‡ºï¼Œä»è€Œæé«˜äº†é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ— éœ€äººå·¥ç›‘ç£å³å¯æ¢ç´¢å¤šæ¨¡æ€çš„å¯ä¾›æ€§çº§åˆ«è®¡åˆ’ã€‚

**å…³é”®è®¾è®¡**ï¼šLLMè§„åˆ’å™¨ä½¿ç”¨é¢„è®­ç»ƒçš„LLMï¼Œé€šè¿‡promptingçš„æ–¹å¼ç”Ÿæˆè®¡åˆ’ã€‚å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å¯ä»¥ä½¿ç”¨å„ç§RLç®—æ³•ï¼Œä¾‹å¦‚PPOæˆ–SACã€‚åœ¨çº¿çº æ­£æœºåˆ¶å¯ä»¥é€šè¿‡æ¯”è¾ƒLLMè®¡åˆ’çš„é¢„æœŸç»“æœå’Œå®é™…æ‰§è¡Œç»“æœæ¥æ£€æµ‹æ¬¡ä¼˜æ€§ï¼Œå¹¶ä½¿ç”¨é¢å¤–çš„å¥–åŠ±ä¿¡å·æ¥å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ æ›´ä¼˜çš„ç­–ç•¥ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©å–å†³äºå…·ä½“çš„ä»»åŠ¡å’Œç¯å¢ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM-TALEåœ¨æŠ“å–å’Œæ”¾ç½®ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•å’Œä»…ä½¿ç”¨LLMæŒ‡å¯¼çš„æ–¹æ³•ï¼Œåœ¨æ ·æœ¬æ•ˆç‡å’ŒæˆåŠŸç‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚çœŸå®æœºå™¨äººå®éªŒä¹ŸéªŒè¯äº†LLM-TALEå…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬sim-to-realè¿ç§»èƒ½åŠ›ï¼Œè¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LLM-TALEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªåŠ¨åŒ–è£…é…ã€ç‰©æµåˆ†æ‹£ã€å®¶åº­æœåŠ¡ç­‰ã€‚é€šè¿‡ç»“åˆLLMçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä»¥åŠå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„æœºå™¨äººç³»ç»Ÿï¼Œä»è€Œæé«˜ç”Ÿäº§æ•ˆç‡å’ŒæœåŠ¡è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) is a promising approach for robotic manipulation, but it can suffer from low sample efficiency and requires extensive exploration of large state-action spaces. Recent methods leverage the commonsense knowledge and reasoning abilities of large language models (LLMs) to guide exploration toward more meaningful states. However, LLMs can produce plans that are semantically plausible yet physically infeasible, yielding unreliable behavior. We introduce LLM-TALE, a framework that uses LLMs' planning to directly steer RL exploration. LLM-TALE integrates planning at both the task level and the affordance level, improving learning efficiency by directing agents toward semantically meaningful actions. Unlike prior approaches that assume optimal LLM-generated plans or rewards, LLM-TALE corrects suboptimality online and explores multimodal affordance-level plans without human supervision. We evaluate LLM-TALE on pick-and-place tasks in standard RL benchmarks, observing improvements in both sample efficiency and success rates over strong baselines. Real-robot experiments indicate promising zero-shot sim-to-real transfer. Code and supplementary material are available at https://llm-tale.github.io.

