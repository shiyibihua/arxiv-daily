---
layout: default
title: ReSeFlow: Rectifying SE(3)-Equivariant Policy Learning Flows
---

# ReSeFlow: Rectifying SE(3)-Equivariant Policy Learning Flows

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22695" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22695v1</a>
  <a href="https://arxiv.org/pdf/2509.22695.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22695v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22695v1', 'ReSeFlow: Rectifying SE(3)-Equivariant Policy Learning Flows')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhitao Wang, Yanke Wang, Jiangtao Wen, Roberto Horowitz, Yuxing Han

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

**å¤‡æ³¨**: This work was submitted to 2026 IEEE International Conference on Robotics & Automation

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReSeFlowï¼Œä¸€ç§å¿«é€Ÿä¸”SE(3)ç­‰å˜çš„ç­–ç•¥å­¦ä¹ æ–¹æ³•ï¼Œæå‡æœºå™¨äººæ“ä½œæ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `ç­–ç•¥å­¦ä¹ ` `SE(3)ç­‰å˜æ€§` `æ‰©æ•£æ¨¡å‹` `æ ¡æ­£æµ` `è½¨è¿¹ç”Ÿæˆ` `æ•°æ®é«˜æ•ˆ` `å¿«é€Ÿæ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰SE(3)ç­‰å˜æ‰©æ•£æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œç­–ç•¥å­¦ä¹ ä¸­è¡¨ç°å‡ºæ•°æ®é«˜æ•ˆæ€§ï¼Œä½†æ¨ç†æ—¶é—´æˆæœ¬è¾ƒé«˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. ReSeFlowé€šè¿‡å¼•å…¥æ ¡æ­£æµåˆ°SE(3)æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°å¿«é€Ÿã€æµ‹åœ°ä¸€è‡´ä¸”è®¡ç®—é‡å°çš„ç­–ç•¥ç”Ÿæˆï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒReSeFlowä»…éœ€ä¸€æ­¥æ¨ç†å³å¯åœ¨ç»˜ç”»å’Œæ—‹è½¬ä¸‰è§’å½¢ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºéœ€è¦100æ­¥æ¨ç†çš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºReSeFlowï¼Œä¸€ç§æ ¡æ­£SE(3)ç­‰å˜ç­–ç•¥å­¦ä¹ æµï¼Œæ—¨åœ¨è§£å†³éç»“æ„åŒ–ç¯å¢ƒä¸­æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œé•¿æ—¶ç¨‹è½¨è¿¹ç­–ç•¥ç”Ÿæˆçš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨SE(3)ç­‰å˜æ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå®ç°æ•°æ®é«˜æ•ˆæ€§ï¼Œå¹¶å€Ÿé‰´æ ¡æ­£æµçš„æ¨ç†æ•ˆç‡ï¼Œæä¾›å¿«é€Ÿã€æµ‹åœ°ä¸€è‡´ã€è®¡ç®—é‡æœ€å°çš„ç­–ç•¥ç”Ÿæˆã€‚ReSeFlowçš„å…³é”®åœ¨äºå…¶æ‰€æœ‰ç»„ä»¶éƒ½é‡‡ç”¨SE(3)ç­‰å˜ç½‘ç»œï¼Œä»¥ä¿æŒæ—‹è½¬å’Œå¹³ç§»å¯¹ç§°æ€§ï¼Œä»è€Œåœ¨åˆšä½“è¿åŠ¨ä¸‹å®ç°é²æ£’çš„æ³›åŒ–ã€‚åœ¨æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­ï¼ŒReSeFlowä»…éœ€ä¸€æ­¥æ¨ç†å³å¯å®ç°æ¯”åŸºçº¿æ–¹æ³•æ›´å¥½çš„æ€§èƒ½å’Œæ›´ä½çš„æµ‹åœ°è·ç¦»ï¼Œåœ¨ç»˜ç”»ä»»åŠ¡ä¸­è¯¯å·®é™ä½é«˜è¾¾48.5%ï¼Œåœ¨æ—‹è½¬ä¸‰è§’å½¢ä»»åŠ¡ä¸­è¯¯å·®é™ä½21.9%ï¼ˆåŸºçº¿æ–¹æ³•ä¸º100æ­¥æ¨ç†ï¼‰ã€‚è¯¥æ–¹æ³•ç»“åˆäº†SE(3)ç­‰å˜æ€§å’Œæ ¡æ­£æµçš„ä¼˜ç‚¹ï¼Œæ¨åŠ¨äº†ç”Ÿæˆç­–ç•¥å­¦ä¹ æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ•°æ®å’Œæ¨ç†æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­ï¼Œå°¤å…¶æ˜¯åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­ï¼Œç”Ÿæˆé²æ£’ä¸”é•¿æ—¶ç¨‹è½¨è¿¹ç­–ç•¥çš„é—®é¢˜ã€‚ç°æœ‰åŸºäºSE(3)ç­‰å˜æ‰©æ•£æ¨¡å‹çš„ç­–ç•¥å­¦ä¹ æ–¹æ³•è™½ç„¶æ•°æ®æ•ˆç‡é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦æ…¢ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ ¡æ­£æµï¼ˆRectified Flowï¼‰çš„æ€æƒ³å¼•å…¥åˆ°SE(3)ç­‰å˜æ‰©æ•£æ¨¡å‹ä¸­ã€‚æ ¡æ­£æµé€šè¿‡å­¦ä¹ ä¸€ä¸ªç›´æ¥çš„æ˜ å°„å…³ç³»ï¼Œå‡å°‘äº†è¿­ä»£æ¨ç†çš„æ­¥éª¤ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ã€‚ç»“åˆSE(3)ç­‰å˜æ€§ï¼Œä¿è¯äº†æ¨¡å‹åœ¨åˆšä½“å˜æ¢ä¸‹çš„ä¸å˜æ€§ï¼Œå¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReSeFlowçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šSE(3)ç­‰å˜ç½‘ç»œå’Œæ ¡æ­£æµã€‚SE(3)ç­‰å˜ç½‘ç»œç”¨äºæå–æ„ŸçŸ¥è§‚æµ‹çš„ç‰¹å¾ï¼Œå¹¶ä¿è¯æ¨¡å‹å¯¹æ—‹è½¬å’Œå¹³ç§»çš„ç­‰å˜æ€§ã€‚æ ¡æ­£æµåˆ™å­¦ä¹ ä»å™ªå£°ç©ºé—´åˆ°ç­–ç•¥ç©ºé—´çš„ç›´æ¥æ˜ å°„ï¼Œä»è€Œå®ç°å¿«é€Ÿç­–ç•¥ç”Ÿæˆã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å­¦ä¹ å¦‚ä½•å°†å™ªå£°æ•°æ®â€œæ ¡æ­£â€ä¸ºæœ‰æ•ˆçš„ç­–ç•¥è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šReSeFlowçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ ¡æ­£æµçš„æ€æƒ³ä¸SE(3)ç­‰å˜æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆã€‚ä¸ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹éœ€è¦å¤šæ¬¡è¿­ä»£é‡‡æ ·ä¸åŒï¼ŒReSeFlowé€šè¿‡å­¦ä¹ ä¸€ä¸ªå•æ­¥æ˜ å°„ï¼Œæå¤§åœ°æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚åŒæ—¶ï¼ŒSE(3)ç­‰å˜æ€§çš„å¼•å…¥ä¿è¯äº†æ¨¡å‹åœ¨åˆšä½“å˜æ¢ä¸‹çš„é²æ£’æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”çœŸå®ä¸–ç•Œçš„å¤æ‚ç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šReSeFlowçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨SE(3)ç­‰å˜å·ç§¯ç¥ç»ç½‘ç»œæ¥æå–ç‰¹å¾ï¼Œä¿è¯æ¨¡å‹å¯¹æ—‹è½¬å’Œå¹³ç§»çš„ç­‰å˜æ€§ï¼›2) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ ¡æ­£æµå­¦ä¹ åˆ°å¹³æ»‘ä¸”æµ‹åœ°ä¸€è‡´çš„æ˜ å°„ï¼›3) é‡‡ç”¨ç‰¹å®šçš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚æ®‹å·®è¿æ¥ç­‰ï¼Œæ¥æé«˜æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ReSeFlowåœ¨æ¨¡æ‹Ÿå®éªŒä¸­è¡¨ç°å‡ºè‰²ã€‚åœ¨ç»˜ç”»ä»»åŠ¡ä¸­ï¼ŒReSeFlowä»…éœ€ä¸€æ­¥æ¨ç†å³å¯è¾¾åˆ°æ¯”åŸºçº¿æ–¹æ³•ï¼ˆ100æ­¥æ¨ç†ï¼‰ä½48.5%çš„è¯¯å·®ã€‚åœ¨æ—‹è½¬ä¸‰è§’å½¢ä»»åŠ¡ä¸­ï¼Œè¯¯å·®é™ä½äº†21.9%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒReSeFlowåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ReSeFlowåœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—æœºå™¨äººç­‰ã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆæœºå™¨äººæŠ“å–ã€è£…é…ã€å¯¼èˆªç­‰ä»»åŠ¡çš„ç­–ç•¥ï¼Œæé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚è¯¥æ–¹æ³•çš„é«˜æ•ˆæ€§å’Œé²æ£’æ€§ä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºå®æ—¶æ€§è¦æ±‚é«˜çš„åœºæ™¯ï¼Œå¹¶æœ‰æœ›æ¨åŠ¨æœºå™¨äººæŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic manipulation in unstructured environments requires the generation of robust and long-horizon trajectory-level policy with conditions of perceptual observations and benefits from the advantages of SE(3)-equivariant diffusion models that are data-efficient. However, these models suffer from the inference time costs. Inspired by the inference efficiency of rectified flows, we introduce the rectification to the SE(3)-diffusion models and propose the ReSeFlow, i.e., Rectifying SE(3)-Equivariant Policy Learning Flows, providing fast, geodesic-consistent, least-computational policy generation. Crucially, both components employ SE(3)-equivariant networks to preserve rotational and translational symmetry, enabling robust generalization under rigid-body motions. With the verification on the simulated benchmarks, we find that the proposed ReSeFlow with only one inference step can achieve better performance with lower geodesic distance than the baseline methods, achieving up to a 48.5% error reduction on the painting task and a 21.9% reduction on the rotating triangle task compared to the baseline's 100-step inference. This method takes advantages of both SE(3) equivariance and rectified flow and puts it forward for the real-world application of generative policy learning models with the data and inference efficiency.

