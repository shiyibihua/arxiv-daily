---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-10-13
---

# cs.ROï¼ˆ2025-10-13ï¼‰

ğŸ“Š å…± **27** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (18 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (18 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251011258v1-demohlm-from-one-demonstration-to-generalizable-humanoid-loco-manipu.html">DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation</a></td>
  <td>DemoHLMï¼šåŸºäºå•æ¬¡æ¼”ç¤ºå®ç°é€šç”¨äººå½¢æœºå™¨äººç§»åŠ¨æ“ä½œ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11258v1" onclick="toggleFavorite(this, '2510.11258v1', 'DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251011689v1-phys2real-fusing-vlm-priors-with-interactive-online-adaptation-for-u.html">Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation</a></td>
  <td>Phys2Realï¼šèåˆVLMå…ˆéªŒä¸äº¤äº’å¼åœ¨çº¿è‡ªé€‚åº”ï¼Œå®ç°ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„Sim-to-Realæ“ä½œ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11689v1" onclick="toggleFavorite(this, '2510.11689v1', 'Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251011542v1-navigait-navigating-dynamically-feasible-gait-libraries-using-deep-r.html">NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning</a></td>
  <td>NaviGaitï¼šåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ å¯¼èˆªåŠ¨æ€å¯è¡Œæ­¥æ€åº“ï¼Œå®ç°é²æ£’åŒè¶³è¿åŠ¨æ§åˆ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11542v1" onclick="toggleFavorite(this, '2510.11542v1', 'NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251011682v1-ego-vision-world-model-for-humanoid-contact-planning.html">Ego-Vision World Model for Humanoid Contact Planning</a></td>
  <td>æå‡ºåŸºäºè‡ªä¸­å¿ƒè§†è§‰ä¸–ç•Œæ¨¡å‹çš„ç±»äººæœºå™¨äººæ¥è§¦è§„åˆ’æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11682v1" onclick="toggleFavorite(this, '2510.11682v1', 'Ego-Vision World Model for Humanoid Contact Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251011401v1-path-and-motion-optimization-for-efficient-multi-location-inspection.html">Path and Motion Optimization for Efficient Multi-Location Inspection with Humanoid Robots</a></td>
  <td>æå‡ºä¸€ç§é«˜æ•ˆçš„äººå½¢æœºå™¨äººå¤šç‚¹å·¡æ£€è·¯å¾„ä¸è¿åŠ¨ä¼˜åŒ–æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11401v1" onclick="toggleFavorite(this, '2510.11401v1', 'Path and Motion Optimization for Efficient Multi-Location Inspection with Humanoid Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251010903v1-towards-a-unified-understanding-of-robot-manipulation-a-comprehensiv.html">Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey</a></td>
  <td>æœºå™¨äººæ“ä½œçš„ç»Ÿä¸€ç†è§£ï¼šå…¨é¢çš„ç»¼è¿°æ€§ç ”ç©¶ï¼Œæ¶µç›–æ–¹æ³•ã€ç“¶é¢ˆä¸åº”ç”¨ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10903v1" onclick="toggleFavorite(this, '2510.10903v1', 'Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251011660v2-maniagent-an-agentic-framework-for-general-robotic-manipulation.html">ManiAgent: An Agentic Framework for General Robotic Manipulation</a></td>
  <td>ManiAgentï¼šä¸€ç§ç”¨äºé€šç”¨æœºå™¨äººæ“ä½œçš„Agentæ¡†æ¶</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11660v2" onclick="toggleFavorite(this, '2510.11660v2', 'ManiAgent: An Agentic Framework for General Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251011539v2-simultaneous-calibration-of-noise-covariance-and-kinematics-for-stat.html">Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization</a></td>
  <td>æå‡ºåŒå±‚ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒæ­¥æ ‡å®šè…¿å¼æœºå™¨äººçŠ¶æ€ä¼°è®¡ä¸­çš„å™ªå£°åæ–¹å·®ä¸è¿åŠ¨å­¦å‚æ•°</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11539v2" onclick="toggleFavorite(this, '2510.11539v2', 'Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251011491v1-constraint-aware-reinforcement-learning-via-adaptive-action-scaling.html">Constraint-Aware Reinforcement Learning via Adaptive Action Scaling</a></td>
  <td>æå‡ºåŸºäºè‡ªé€‚åº”åŠ¨ä½œç¼©æ”¾çš„çº¦æŸæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡å®‰å…¨æ€§å’Œæ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11491v1" onclick="toggleFavorite(this, '2510.11491v1', 'Constraint-Aware Reinforcement Learning via Adaptive Action Scaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251011072v1-physhsi-towards-a-real-world-generalizable-and-natural-humanoid-scen.html">PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System</a></td>
  <td>æå‡ºPhysHSIä»¥è§£å†³äººå½¢æœºå™¨äººä¸çœŸå®åœºæ™¯äº¤äº’çš„æŒ‘æˆ˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11072v1" onclick="toggleFavorite(this, '2510.11072v1', 'PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251011421v1-a-modular-aiot-framework-for-low-latency-real-time-robotic-teleopera.html">A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation in Smart Cities</a></td>
  <td>æå‡ºåŸºäºAIoTçš„æ¨¡å—åŒ–ä½å»¶è¿Ÿæœºå™¨äººé¥æ“ä½œæ¡†æ¶ï¼Œç”¨äºæ™ºæ…§åŸå¸‚åº”ç”¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11421v1" onclick="toggleFavorite(this, '2510.11421v1', 'A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation in Smart Cities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251011094v1-design-and-koopman-model-predictive-control-of-a-soft-exoskeleton-ba.html">Design and Koopman Model Predictive Control of A Soft Exoskeleton Based on Origami-Inspired Pneumatic Actuator for Knee Rehabilitation</a></td>
  <td>æå‡ºåŸºäºæŠ˜çº¸æ°”åŠ¨è½¯ä½“å¤–éª¨éª¼çš„Koopmanæ¨¡å‹é¢„æµ‹æ§åˆ¶æ–¹æ³•ï¼Œç”¨äºè†å…³èŠ‚åº·å¤ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11094v1" onclick="toggleFavorite(this, '2510.11094v1', 'Design and Koopman Model Predictive Control of A Soft Exoskeleton Based on Origami-Inspired Pneumatic Actuator for Knee Rehabilitation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251010912v2-more-than-a-point-capturing-uncertainty-with-adaptive-affordance-hea.html">More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks</a></td>
  <td>RoboMAPï¼šåˆ©ç”¨è‡ªé€‚åº”å¯ä¾›æ€§çƒ­å›¾æ•è·ä¸ç¡®å®šæ€§ï¼Œæå‡æœºå™¨äººç©ºé—´å®šä½èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10912v2" onclick="toggleFavorite(this, '2510.10912v2', 'More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251011321v2-himacon-discovering-hierarchical-manipulation-concepts-from-unlabele.html">HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data</a></td>
  <td>HiMaConï¼šä»æ— æ ‡æ³¨å¤šæ¨¡æ€æ•°æ®ä¸­å‘ç°åˆ†å±‚æ“ä½œæ¦‚å¿µï¼Œæå‡æœºå™¨äººæ“ä½œæ³›åŒ–æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11321v2" onclick="toggleFavorite(this, '2510.11321v2', 'HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251011041v1-unveiling-uncertainty-aware-autonomous-cooperative-learning-based-pl.html">Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy</a></td>
  <td>æå‡ºåŸºäºä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„è‡ªä¸»ååŒå­¦ä¹ è§„åˆ’ç­–ç•¥ï¼Œæå‡å¤šè½¦äº¤äº’çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11041v1" onclick="toggleFavorite(this, '2510.11041v1', 'Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251011566v1-scoopd-learning-mixed-liquid-solid-scooping-via-sim2real-generative-.html">SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy</a></td>
  <td>SCOOP'Dï¼šé€šè¿‡Sim2Realç”Ÿæˆç­–ç•¥å­¦ä¹ æ··åˆæ¶²ä½“-å›ºä½“æŠ“å–</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11566v1" onclick="toggleFavorite(this, '2510.11566v1', 'SCOOP&#39;D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251011014v1-into-the-unknown-towards-using-generative-models-for-sampling-priors.html">Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces</a></td>
  <td>æå‡ºåŸºäºç”Ÿæˆæ¨¡å‹çš„é‡‡æ ·æ–¹æ³•ï¼Œä¸ºé…ç½®ç©ºé—´è§„åˆ’æä¾›ç¯å¢ƒä¸ç¡®å®šæ€§çš„å…ˆéªŒä¿¡æ¯ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11014v1" onclick="toggleFavorite(this, '2510.11014v1', 'Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251010893v1-an-adaptive-transition-framework-for-game-theoretic-based-takeover.html">An Adaptive Transition Framework for Game-Theoretic Based Takeover</a></td>
  <td>æå‡ºè‡ªé€‚åº”è¿‡æ¸¡ç­–ç•¥ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæ¥ç®¡é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10893v1" onclick="toggleFavorite(this, '2510.10893v1', 'An Adaptive Transition Framework for Game-Theoretic Based Takeover')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/251011103v1-a-primer-on-so3-action-representations-in-deep-reinforcement-learnin.html">A Primer on SO(3) Action Representations in Deep Reinforcement Learning</a></td>
  <td>ç ”ç©¶SO(3)ä½œç”¨è¡¨ç¤ºå¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å½±å“ï¼Œæå‡ºåŸºäºå±€éƒ¨åæ ‡ç³»åˆ‡å‘é‡çš„åŠ¨ä½œè¡¨ç¤ºæ–¹æ³•ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11103v1" onclick="toggleFavorite(this, '2510.11103v1', 'A Primer on SO(3) Action Representations in Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251011083v1-flow-matching-based-autonomous-driving-planning-with-advanced-intera.html">Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling</a></td>
  <td>Flow Plannerï¼šåŸºäºæµåŒ¹é…çš„è‡ªåŠ¨é©¾é©¶è§„åˆ’ï¼Œæå‡äº¤äº’è¡Œä¸ºå»ºæ¨¡èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11083v1" onclick="toggleFavorite(this, '2510.11083v1', 'Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251011474v2-coordinated-strategies-in-realistic-air-combat-by-hierarchical-multi.html">Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºåˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„ç©ºæˆ˜ååŒç­–ç•¥ï¼Œè§£å†³å¤æ‚ç©ºæˆ˜ç¯å¢ƒä¸‹çš„å†³ç­–éš¾é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11474v2" onclick="toggleFavorite(this, '2510.11474v2', 'Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251010960v1-game-theoretic-risk-shaped-reinforcement-learning-for-safe-autonomou.html">Game-Theoretic Risk-Shaped Reinforcement Learning for Safe Autonomous Driving</a></td>
  <td>æå‡ºæ¸¸æˆç†è®ºé£é™©å¡‘å½¢å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³å®‰å…¨è‡ªåŠ¨é©¾é©¶é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10960v1" onclick="toggleFavorite(this, '2510.10960v1', 'Game-Theoretic Risk-Shaped Reinforcement Learning for Safe Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251011036v1-xgrasp-gripper-aware-grasp-detection-with-multi-gripper-data-generat.html">XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation</a></td>
  <td>XGraspï¼šæå‡ºä¸€ç§æ”¯æŒå¤šå¤¹çˆªçš„å®æ—¶ã€å¯æ³›åŒ–æŠ“å–æ£€æµ‹æ¡†æ¶ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11036v1" onclick="toggleFavorite(this, '2510.11036v1', 'XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/251021761v1-j-ora-a-framework-and-multimodal-dataset-for-japanese-object-identif.html">J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception</a></td>
  <td>J-ORAï¼šç”¨äºæœºå™¨äººæ„ŸçŸ¥çš„æ—¥è¯­ç‰©ä½“è¯†åˆ«ã€æŒ‡ä»£å’ŒåŠ¨ä½œé¢„æµ‹çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸æ¡†æ¶</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21761v1" onclick="toggleFavorite(this, '2510.21761v1', 'J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251010865v1-grip-a-unified-framework-for-grid-based-relay-and-co-occurrence-awar.html">GRIP: A Unified Framework for Grid-Based Relay and Co-Occurrence-Aware Planning in Dynamic Environments</a></td>
  <td>GRIPï¼šåŠ¨æ€ç¯å¢ƒä¸­åŸºäºç½‘æ ¼çš„ä¸­ç»§ä¸å…±ç°æ„ŸçŸ¥ç»Ÿä¸€è§„åˆ’æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10865v1" onclick="toggleFavorite(this, '2510.10865v1', 'GRIP: A Unified Framework for Grid-Based Relay and Co-Occurrence-Aware Planning in Dynamic Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251010975v2-rover-robot-reward-model-as-test-time-verifier-for-vision-language-a.html">RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model</a></td>
  <td>RoVerï¼šæå‡ºåŸºäºå¥–åŠ±æ¨¡å‹çš„æœºå™¨äººæµ‹è¯•æ—¶éªŒè¯æ¡†æ¶ï¼Œæå‡VLAæ¨¡å‹æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10975v2" onclick="toggleFavorite(this, '2510.10975v2', 'RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>27</td>
  <td><a href="./papers/251011552v1-robot-soccer-kit-omniwheel-tracked-soccer-robots-for-education.html">Robot Soccer Kit: Omniwheel Tracked Soccer Robots for Education</a></td>
  <td>æå‡ºä¸€ç§åŸºäºå¤–éƒ¨è·Ÿè¸ªç³»ç»Ÿçš„å…¨å‘è½®è¶³çƒæœºå™¨äººæ•™è‚²å¥—ä»¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11552v1" onclick="toggleFavorite(this, '2510.11552v1', 'Robot Soccer Kit: Omniwheel Tracked Soccer Robots for Education')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)