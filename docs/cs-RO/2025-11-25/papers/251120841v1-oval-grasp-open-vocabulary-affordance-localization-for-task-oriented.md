---
layout: default
title: OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping
---

# OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.20841" target="_blank" class="toolbar-btn">arXiv: 2511.20841v1</a>
    <a href="https://arxiv.org/pdf/2511.20841.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20841v1" 
            onclick="toggleFavorite(this, '2511.20841v1', 'OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Edmond Tong, Advaith Balaji, Anthony Opipari, Stanley Lewis, Zhen Zeng, Odest Chadwicke Jenkins

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-25

**Â§áÊ≥®**: 10 pages, 7 figures, 3 tables. Presented at the 2025 International Symposium on Experimental Robotics (ISER)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://ekjt.github.io/OVAL-Grasp/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**OVAL-GraspÔºöÈù¢Âêë‰ªªÂä°ÁöÑÂºÄÊîæËØçÊ±áÊäìÂèñÊñπÊ≥ïÔºåÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÁÅµÊ¥ªÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫ÊäìÂèñ` `ÂºÄÊîæËØçÊ±á` `ÂèØ‰æõÊÄß` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `‰ªªÂä°ÂØºÂêë` `Èõ∂Ê†∑Êú¨Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÂá†‰ΩïÁöÑÊäìÂèñÊñπÊ≥ïÈöæ‰ª•Â§ÑÁêÜËßÜËßâÂÆö‰πâÁöÑÁâ©‰ΩìÈÉ®‰ª∂„ÄÅÈÅÆÊå°ÂíåÊú™ËßÅËøáÁöÑÁâ©‰ΩìÔºåÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫Âú®ÈùûÁªìÊûÑÂåñÁéØÂ¢É‰∏≠Êìç‰ΩúÁöÑÁÅµÊ¥ªÊÄß„ÄÇ
2. OVAL-GraspÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÂÆûÁé∞Èõ∂Ê†∑Êú¨ÁöÑ„ÄÅÈù¢Âêë‰ªªÂä°ÁöÑÊäìÂèñÔºå‰ΩøÊú∫Âô®‰∫∫ËÉΩÊ†πÊçÆ‰ªªÂä°ÊäìÂèñÁâ©‰ΩìÁöÑÁâπÂÆöÈÉ®‰Ωç„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåOVAL-GraspÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ËÉΩÊúâÊïàËØÜÂà´ÂíåÊäìÂèñÁõÆÊ†áÁâ©‰ΩìÈÉ®‰ΩçÔºå‰∏îÂú®ÈÅÆÊå°ÊÉÖÂÜµ‰∏ã‰ªçÂÖ∑ÊúâËâØÂ•ΩÁöÑÈ≤ÅÊ£íÊÄßÔºå‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫OVAL-GraspÁöÑÈõ∂Ê†∑Êú¨ÂºÄÊîæËØçÊ±áÊñπÊ≥ïÔºåÁî®‰∫éÂÆûÁé∞Èù¢Âêë‰ªªÂä°ÁöÑ„ÄÅÂü∫‰∫éÂèØ‰æõÊÄßÁöÑÊäìÂèñ„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂíåËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÔºå‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÊ†πÊçÆÁªôÂÆöÁöÑ‰ªªÂä°Âú®Áâ©‰ΩìÁöÑÊ≠£Á°ÆÈÉ®‰ΩçËøõË°åÊäìÂèñ„ÄÇÁªôÂÆöRGBÂõæÂÉèÂíå‰ªªÂä°ÊèèËø∞ÔºåOVAL-GraspÈ¶ñÂÖà‰ΩøÁî®LLMËØÜÂà´ÈúÄË¶ÅÊäìÂèñÊàñÈÅøÂÖçÊäìÂèñÁöÑÁâ©‰ΩìÈÉ®‰ΩçÔºåÁÑ∂Âêé‰ΩøÁî®VLMÂàÜÂâ≤Ëøô‰∫õÈÉ®‰ΩçÔºåÂπ∂ÁîüÊàêÁâ©‰Ωì‰∏äÂèØÊìç‰ΩúÂå∫ÂüüÁöÑ2DÁÉ≠Âõæ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®20‰∏™ÂÆ∂Áî®Áâ©ÂìÅÂíåÊØè‰∏™Áâ©ÂìÅ3‰∏™Áã¨Áâπ‰ªªÂä°ÁöÑÂÆûÈ™å‰∏≠Ôºå‰ºò‰∫é‰∏§ÁßçÈù¢Âêë‰ªªÂä°ÁöÑÊäìÂèñÂü∫Á∫ø„ÄÇÂú®ÁúüÂÆû‰∏ñÁïåÁöÑÂÆûÈ™å‰∏≠ÔºåOVAL-GraspÊàêÂäüËØÜÂà´ÂíåÂàÜÂâ≤Ê≠£Á°ÆÁâ©‰ΩìÈÉ®‰ΩçÁöÑÊ¶ÇÁéá‰∏∫95%ÔºåÊäìÂèñÊ≠£Á°ÆÂèØÊìç‰ΩúÂå∫ÂüüÁöÑÊ¶ÇÁéá‰∏∫78.3%„ÄÇÊ≠§Â§ñÔºåOVAL-GraspÂú®ÈÉ®ÂàÜÈÅÆÊå°ÁöÑÊÉÖÂÜµ‰∏ã‰πüËÉΩÊâæÂà∞Ê≠£Á°ÆÁöÑÁâ©‰ΩìÈÉ®‰ΩçÔºåÂú®ÊùÇ‰π±Âú∫ÊôØ‰∏≠ÁöÑÈÉ®‰ΩçÈÄâÊã©ÊàêÂäüÁéá‰∏∫80%„ÄÇËÆ∫ÊñáËøòÂ±ïÁ§∫‰∫ÜOVAL-GraspÂú®‰æùËµñËßÜËßâÁâπÂæÅËøõË°åÈÉ®‰ΩçÈÄâÊã©ÁöÑÂú∫ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÂπ∂ÈÄöËøáÊ∂àËûçÂÆûÈ™åËØÅÊòé‰∫ÜÊ®°ÂùóÂåñËÆæËÆ°ÁöÑ‰ºòÂäø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÂá†‰Ωï‰ø°ÊÅØÁöÑÊäìÂèñÊñπÊ≥ïÂú®Â§ÑÁêÜÂÖ∑ÊúâÂ§çÊùÇËßÜËßâÁâπÂæÅÁöÑÁâ©‰ΩìÈÉ®‰ª∂Êó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â≠òÂú®ÈÅÆÊå°ÊàñÈÅáÂà∞Êú™ËßÅËøáÁöÑÁâ©‰ΩìÊó∂„ÄÇËøô‰∫õÊñπÊ≥ïÈöæ‰ª•ÁêÜËß£‰ªªÂä°ÈúÄÊ±ÇÔºåÊó†Ê≥ïÊ†πÊçÆ‰ªªÂä°ÁõÆÊ†áÈÄâÊã©ÂêàÈÄÇÁöÑÊäìÂèñÈÉ®‰Ωç„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçËÉΩÂ§üÁêÜËß£‰ªªÂä°ËØ≠‰πâÂπ∂ÂÆö‰ΩçÁâ©‰Ωì‰∏äÂèØÊäìÂèñÂå∫ÂüüÁöÑÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöOVAL-GraspÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁêÜËß£‰ªªÂä°ÊèèËø∞ÔºåÂπ∂ÁªìÂêàËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂÆö‰ΩçÁâ©‰Ωì‰∏ä‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÂèØÊäìÂèñÂå∫Âüü„ÄÇÈÄöËøáÂ∞ÜËØ≠Ë®ÄÁêÜËß£ÂíåËßÜËßâÊÑüÁü•Áõ∏ÁªìÂêàÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂÆûÁé∞Èõ∂Ê†∑Êú¨ÁöÑ„ÄÅÈù¢Âêë‰ªªÂä°ÁöÑÊäìÂèñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöOVAL-GraspÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ‰ªªÂä°ÁêÜËß£Ê®°ÂùóÔºö‰ΩøÁî®LLMËß£Êûê‰ªªÂä°ÊèèËø∞ÔºåËØÜÂà´ÈúÄË¶ÅÊäìÂèñÊàñÈÅøÂÖçÊäìÂèñÁöÑÁâ©‰ΩìÈÉ®‰Ωç„ÄÇ2) ÈÉ®‰ΩçÂàÜÂâ≤Ê®°ÂùóÔºö‰ΩøÁî®VLMÂàÜÂâ≤ÂõæÂÉè‰∏≠ËØÜÂà´Âá∫ÁöÑÁâ©‰ΩìÈÉ®‰Ωç„ÄÇ3) ÁÉ≠ÂõæÁîüÊàêÊ®°ÂùóÔºöÊ†πÊçÆÂàÜÂâ≤ÁªìÊûúÁîüÊàê2DÁÉ≠ÂõæÔºåÊåáÁ§∫Áâ©‰Ωì‰∏äÂèØÊìç‰ΩúÂå∫Âüü„ÄÇ4) ÊäìÂèñÊâßË°åÊ®°ÂùóÔºöÊ†πÊçÆÁÉ≠ÂõæÈÄâÊã©ÊúÄ‰Ω≥ÊäìÂèñÁÇπÔºåÂπ∂ÊéßÂà∂Êú∫Âô®‰∫∫ÊâßË°åÊäìÂèñÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöOVAL-GraspÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜLLMÂíåVLMÁõ∏ÁªìÂêàÔºåÂÆûÁé∞Èõ∂Ê†∑Êú¨ÁöÑ„ÄÅÈù¢Âêë‰ªªÂä°ÁöÑÊäìÂèñ„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÂá†‰Ωï‰ø°ÊÅØÁöÑÊäìÂèñÊñπÊ≥ïÁõ∏ÊØîÔºåOVAL-GraspËÉΩÂ§üÁêÜËß£‰ªªÂä°ËØ≠‰πâÔºåÂπ∂Ê†πÊçÆ‰ªªÂä°ÁõÆÊ†áÈÄâÊã©ÂêàÈÄÇÁöÑÊäìÂèñÈÉ®‰Ωç„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåËÉΩÂ§üÂ§ÑÁêÜÊú™ËßÅËøáÁöÑÁâ©‰ΩìÂíåÂ§çÊùÇÁöÑÂú∫ÊôØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöOVAL-GraspÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑLLMÔºà‰æãÂ¶ÇGPT-3ÔºâËøõË°å‰ªªÂä°ÁêÜËß£ÔºåÊó†ÈúÄÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°åÂæÆË∞É„ÄÇ2) ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑVLMÔºà‰æãÂ¶ÇCLIPÔºâËøõË°åÈÉ®‰ΩçÂàÜÂâ≤ÔºåÂà©Áî®ÂÖ∂Âº∫Â§ßÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêËÉΩÂäõ„ÄÇ3) ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁÉ≠ÂõæÁöÑÊäìÂèñÁÇπÈÄâÊã©Á≠ñÁï•ÔºåËÄÉËôë‰∫ÜÊäìÂèñÁÇπÁöÑÂèØËææÊÄßÂíåÁ®≥ÂÆöÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

OVAL-GraspÂú®ÁúüÂÆû‰∏ñÁïåÁöÑÂÆûÈ™å‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊàêÂäüËØÜÂà´ÂíåÂàÜÂâ≤Ê≠£Á°ÆÁâ©‰ΩìÈÉ®‰ΩçÁöÑÊ¶ÇÁéá‰∏∫95%ÔºåÊäìÂèñÊ≠£Á°ÆÂèØÊìç‰ΩúÂå∫ÂüüÁöÑÊ¶ÇÁéá‰∏∫78.3%„ÄÇÂú®ÊùÇ‰π±Âú∫ÊôØ‰∏≠ÔºåËØ•ÊñπÊ≥ïÂú®ÈÉ®ÂàÜÈÅÆÊå°ÁöÑÊÉÖÂÜµ‰∏ã‰ªçËÉΩ‰øùÊåÅ80%ÁöÑÈÉ®‰ΩçÈÄâÊã©ÊàêÂäüÁéá„ÄÇÊ≠§Â§ñÔºåOVAL-GraspÂú®‰∏é‰∏§ÁßçÈù¢Âêë‰ªªÂä°ÁöÑÊäìÂèñÂü∫Á∫øÊñπÊ≥ïÁöÑÂØπÊØîÂÆûÈ™å‰∏≠ÔºåÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

OVAL-GraspÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÂåªÁñóËæÖÂä©Á≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂÆ∂Â∫≠ÁéØÂ¢É‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑÊåá‰ª§ÊäìÂèñÁâπÂÆöÁöÑÁâ©ÂìÅÔºåÂπ∂ÊîæÁΩÆÂà∞ÊåáÂÆöÁöÑ‰ΩçÁΩÆ„ÄÇÂú®Â∑•‰∏öËá™Âä®Âåñ‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆÁîü‰∫ß‰ªªÂä°ÊäìÂèñ‰∏çÂêåÁöÑÈõ∂‰ª∂ÔºåÂπ∂ËøõË°åÁªÑË£Ö„ÄÇÂú®ÂåªÁñóËæÖÂä©‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Â∏ÆÂä©ÂåªÁîüÊäìÂèñÊâãÊúØÂô®Ê¢∞ÔºåÊèêÈ´òÊâãÊúØÊïàÁéáÂíåÂÆâÂÖ®ÊÄß„ÄÇËØ•Á†îÁ©∂ÊúâÊúõÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> To manipulate objects in novel, unstructured environments, robots need task-oriented grasps that target object parts based on the given task. Geometry-based methods often struggle with visually defined parts, occlusions, and unseen objects. We introduce OVAL-Grasp, a zero-shot open-vocabulary approach to task-oriented, affordance based grasping that uses large-language models and vision-language models to allow a robot to grasp objects at the correct part according to a given task. Given an RGB image and a task, OVAL-Grasp identifies parts to grasp or avoid with an LLM, segments them with a VLM, and generates a 2D heatmap of actionable regions on the object. During our evaluations, we found that our method outperformed two task oriented grasping baselines on experiments with 20 household objects with 3 unique tasks for each. OVAL-Grasp successfully identifies and segments the correct object part 95% of the time and grasps the correct actionable area 78.3% of the time in real-world experiments with the Fetch mobile manipulator. Additionally, OVAL-Grasp finds correct object parts under partial occlusions, demonstrating a part selection success rate of 80% in cluttered scenes. We also demonstrate OVAL-Grasp's efficacy in scenarios that rely on visual features for part selection, and show the benefit of a modular design through our ablation experiments. Our project webpage is available at https://ekjt.github.io/OVAL-Grasp/

