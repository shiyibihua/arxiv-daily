---
layout: default
title: Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation
---

# Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation

**arXiv**: [2511.19859v1](https://arxiv.org/abs/2511.19859) | [PDF](https://arxiv.org/pdf/2511.19859.pdf)

**ä½œè€…**: Xiangkai Ma, Lekai Xing, Han Zhang, Wenzhong Li, Sanglu Lu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVITAæ¡†æž¶ï¼Œé€šè¿‡éšå¼è§†è§‰CoTç»Ÿä¸€æ„ŸçŸ¥ä¸ŽåŠ¨ä½œï¼Œæå‡æœºå™¨äººåŠ¨ä½œç”Ÿæˆèƒ½åŠ›ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `é“¾å¼æ€è€ƒ` `éšå¼è§†è§‰æŽ¨ç†` `è½¨è¿¹å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åœ¨å¤æ‚ç©ºé—´çŽ¯å¢ƒä¸­éš¾ä»¥å……åˆ†æ•æ‰åœºæ™¯ç»†èŠ‚ï¼Œæ–‡æœ¬CoTå­˜åœ¨å±€é™æ€§ï¼Œè§†è§‰å…ˆéªŒåˆ©ç”¨ä¸è¶³ã€‚
2. VITAæ¡†æž¶é€šè¿‡å­¦ä¹ è§†è§‰å’ŒåŠ¨ä½œçš„å…±äº«ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œå¹¶å¼•å…¥éšå¼è§†è§‰CoTï¼Œå®žçŽ°æ„ŸçŸ¥å’ŒåŠ¨ä½œçš„ç»Ÿä¸€å»ºæ¨¡ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒVITAåœ¨å¤šä¸ªbenchmarkä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®žä¸–ç•Œä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰é›†æˆè½¨è¿¹å¯¹é½ï¼ˆVITAï¼‰æ¡†æž¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹ä¸­è§†è§‰ä¿¡æ¯åˆ©ç”¨ä¸è¶³å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚VITAé€šè¿‡å­¦ä¹ è§†è§‰å’ŒåŠ¨ä½œçš„å…±äº«ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œå®žçŽ°æ„ŸçŸ¥å’Œè¿åŠ¨æŽ§åˆ¶çš„è”åˆå»ºæ¨¡ã€‚è¯¥æ¡†æž¶å¼•å…¥éšå¼è§†è§‰CoTï¼Œè‡ªå›žå½’åœ°ç”Ÿæˆtokenï¼Œå¹¶åŒæ—¶è§£ç ä¸ºæœªæ¥å¸§é¢„æµ‹å’Œæœºå™¨äººåŠ¨ä½œï¼Œä»Žè€Œå°†è§†è§‰åŠ¨æ€ä½œä¸ºè¿åŠ¨è§„åˆ’çš„å½’çº³åç½®ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žçŽ¯å¢ƒä¸­çš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒVITAå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨CALVINã€LIBEROå’ŒSimplerEnvä¸Šåˆ†åˆ«æ¯”çŽ°æœ‰åŸºçº¿æé«˜äº†14.5%ã€9.6%å’Œ12.1%ã€‚æ­¤å¤–ï¼ŒVITAåœ¨å…­ä¸ªçœŸå®žä¸–ç•Œä»»åŠ¡ä¸­å®žçŽ°äº†å¹³å‡80.5%çš„æˆåŠŸçŽ‡ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºé€šç”¨æœºå™¨äººæ“ä½œæ¨¡åž‹çš„æ½œåŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹çš„æœºå™¨äººæ“ä½œæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä¾èµ–äºŽChain-of-Thought (CoT) çš„æ–¹æ³•ï¼Œåœ¨å¤æ‚çŽ¯å¢ƒä¸­éš¾ä»¥å……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚æ–‡æœ¬CoTéš¾ä»¥æ•æ‰ç»†è‡´çš„åœºæ™¯ä¿¡æ¯ï¼Œè€Œç›´æŽ¥å°†è§†è§‰ä¿¡æ¯èžå…¥åŠ¨ä½œç”Ÿæˆåˆé¢ä¸´æ¨¡æ€å·®å¼‚å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œå³è§†è§‰é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆçš„ç›®æ ‡ç›¸äº’ç«žäº‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVITAçš„æ ¸å¿ƒæ€è·¯æ˜¯å­¦ä¹ ä¸€ä¸ªè§†è§‰å’ŒåŠ¨ä½œçš„å…±äº«ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œä»Žè€Œå°†è§†è§‰ä¿¡æ¯æœ‰æ•ˆåœ°èžå…¥åˆ°åŠ¨ä½œç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚é€šè¿‡éšå¼è§†è§‰CoTï¼Œæ¨¡åž‹èƒ½å¤Ÿè‡ªå›žå½’åœ°ç”Ÿæˆtokenï¼Œè¿™äº›tokenæ—¢ç”¨äºŽé¢„æµ‹æœªæ¥çš„è§†è§‰å¸§ï¼Œåˆç”¨äºŽç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚è¿™ç§è®¾è®¡å°†è§†è§‰åŠ¨æ€ä½œä¸ºè¿åŠ¨è§„åˆ’çš„å½’çº³åç½®ï¼Œä»Žè€Œæé«˜äº†åŠ¨ä½œç”Ÿæˆçš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šVITAæ¡†æž¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è§†è§‰ç¼–ç å™¨ï¼Œç”¨äºŽæå–è§†è§‰ç‰¹å¾ï¼›2) åŠ¨ä½œç¼–ç å™¨ï¼Œç”¨äºŽç¼–ç åŠ¨ä½œåºåˆ—ï¼›3) å…±äº«ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œç”¨äºŽå¯¹è§†è§‰å’ŒåŠ¨ä½œä¿¡æ¯è¿›è¡Œç»Ÿä¸€è¡¨ç¤ºï¼›4) è‡ªå›žå½’è§£ç å™¨ï¼Œç”¨äºŽç”Ÿæˆéšå¼è§†è§‰CoT tokenï¼Œå¹¶å°†å…¶è§£ç ä¸ºæœªæ¥å¸§é¢„æµ‹å’Œæœºå™¨äººåŠ¨ä½œã€‚æ•´ä¸ªæµç¨‹æ˜¯ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šVITAæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽéšå¼è§†è§‰CoTçš„å¼•å…¥ã€‚ä¸Žæ˜¾å¼åœ°ç”Ÿæˆæ–‡æœ¬å½¢å¼çš„CoTä¸åŒï¼ŒVITAé€šè¿‡è‡ªå›žå½’åœ°ç”Ÿæˆtokenï¼Œå¹¶å°†è¿™äº›tokenåŒæ—¶ç”¨äºŽè§†è§‰é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆï¼Œä»Žè€Œå®žçŽ°äº†è§†è§‰ä¿¡æ¯å’ŒåŠ¨ä½œç”Ÿæˆçš„ç´§å¯†è€¦åˆã€‚è¿™ç§éšå¼çš„æ–¹å¼é¿å…äº†æ–‡æœ¬CoTå¯èƒ½å¸¦æ¥çš„ä¿¡æ¯æŸå¤±å’Œæ­§ä¹‰ï¼Œå¹¶æé«˜äº†æ¨¡åž‹çš„æ•ˆçŽ‡å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šVITAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformeræž¶æž„ä½œä¸ºè§†è§‰å’ŒåŠ¨ä½œç¼–ç å™¨å’Œè§£ç å™¨ï¼›2) é‡‡ç”¨ç¦»æ•£å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ å…±äº«ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼›3) ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è®­ç»ƒè‡ªå›žå½’è§£ç å™¨ï¼ŒåŒæ—¶ä¼˜åŒ–æœªæ¥å¸§é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆçš„å‡†ç¡®æ€§ï¼›4) é€šè¿‡è°ƒæ•´è§†è§‰é¢„æµ‹å’ŒåŠ¨ä½œç”ŸæˆæŸå¤±çš„æƒé‡ï¼Œå¹³è¡¡ä¸¤ä¸ªç›®æ ‡ä¹‹é—´çš„ç«žäº‰å…³ç³»ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

VITAåœ¨CALVINã€LIBEROå’ŒSimplerEnvç­‰æ¨¡æ‹ŸçŽ¯å¢ƒbenchmarkä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåˆ†åˆ«æ¯”çŽ°æœ‰åŸºçº¿æé«˜äº†14.5%ã€9.6%å’Œ12.1%ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒVITAåœ¨å…­ä¸ªçœŸå®žä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å®žçŽ°äº†å¹³å‡80.5%çš„æˆåŠŸçŽ‡ï¼Œè¯æ˜Žäº†å…¶åœ¨çœŸå®žçŽ¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œå®žç”¨ä»·å€¼ã€‚è¿™äº›å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒVITAæ˜¯ç›®å‰æœ€å…ˆè¿›çš„é€šç”¨æœºå™¨äººæ“ä½œæ¨¡åž‹ä¹‹ä¸€ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

VITAæ¡†æž¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºŽå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œå¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºŽæå‡æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›ï¼Œå®žçŽ°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººæ“ä½œã€‚æœªæ¥ï¼ŒVITAå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤šæ¨¡æ€è¾“å…¥ï¼ˆå¦‚è¯­éŸ³ã€è§¦è§‰ï¼‰å’Œæ›´å¤æ‚çš„ä»»åŠ¡åœºæ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\%, 9.6\% and 12.1\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.

