---
layout: default
title: Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation
---

# Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.19859" target="_blank" class="toolbar-btn">arXiv: 2511.19859v1</a>
    <a href="https://arxiv.org/pdf/2511.19859.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19859v1" 
            onclick="toggleFavorite(this, '2511.19859v1', 'Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiangkai Ma, Lekai Xing, Han Zhang, Wenzhong Li, Sanglu Lu

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-25

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VITAÊ°ÜÊû∂ÔºåÈÄöËøáÈöêÂºèËßÜËßâCoTÁªü‰∏ÄÊÑüÁü•‰∏éÂä®‰ΩúÔºåÊèêÂçáÊú∫Âô®‰∫∫Âä®‰ΩúÁîüÊàêËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã` `ÈìæÂºèÊÄùËÄÉ` `ÈöêÂºèËßÜËßâÊé®ÁêÜ` `ËΩ®ËøπÂØπÈΩê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®Â§çÊùÇÁ©∫Èó¥ÁéØÂ¢É‰∏≠Èöæ‰ª•ÂÖÖÂàÜÊçïÊçâÂú∫ÊôØÁªÜËäÇÔºåÊñáÊú¨CoTÂ≠òÂú®Â±ÄÈôêÊÄßÔºåËßÜËßâÂÖàÈ™åÂà©Áî®‰∏çË∂≥„ÄÇ
2. VITAÊ°ÜÊû∂ÈÄöËøáÂ≠¶‰π†ËßÜËßâÂíåÂä®‰ΩúÁöÑÂÖ±‰∫´Á¶ªÊï£ÊΩúÂú®Á©∫Èó¥ÔºåÂπ∂ÂºïÂÖ•ÈöêÂºèËßÜËßâCoTÔºåÂÆûÁé∞ÊÑüÁü•ÂíåÂä®‰ΩúÁöÑÁªü‰∏ÄÂª∫Ê®°„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVITAÂú®Â§ö‰∏™benchmark‰∏äË∂ÖË∂äÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®ÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËßÜËßâÈõÜÊàêËΩ®ËøπÂØπÈΩêÔºàVITAÔºâÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°Âûã‰∏≠ËßÜËßâ‰ø°ÊÅØÂà©Áî®‰∏çË∂≥ÂíåËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÁöÑÈóÆÈ¢ò„ÄÇVITAÈÄöËøáÂ≠¶‰π†ËßÜËßâÂíåÂä®‰ΩúÁöÑÂÖ±‰∫´Á¶ªÊï£ÊΩúÂú®Á©∫Èó¥ÔºåÂÆûÁé∞ÊÑüÁü•ÂíåËøêÂä®ÊéßÂà∂ÁöÑËÅîÂêàÂª∫Ê®°„ÄÇËØ•Ê°ÜÊû∂ÂºïÂÖ•ÈöêÂºèËßÜËßâCoTÔºåËá™ÂõûÂΩíÂú∞ÁîüÊàêtokenÔºåÂπ∂ÂêåÊó∂Ëß£Á†Å‰∏∫Êú™Êù•Â∏ßÈ¢ÑÊµãÂíåÊú∫Âô®‰∫∫Âä®‰ΩúÔºå‰ªéËÄåÂ∞ÜËßÜËßâÂä®ÊÄÅ‰Ωú‰∏∫ËøêÂä®ËßÑÂàíÁöÑÂΩíÁ∫≥ÂÅèÁΩÆ„ÄÇÂú®Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåVITAÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂú®CALVIN„ÄÅLIBEROÂíåSimplerEnv‰∏äÂàÜÂà´ÊØîÁé∞ÊúâÂü∫Á∫øÊèêÈ´ò‰∫Ü14.5%„ÄÅ9.6%Âíå12.1%„ÄÇÊ≠§Â§ñÔºåVITAÂú®ÂÖ≠‰∏™ÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÂπ≥Âùá80.5%ÁöÑÊàêÂäüÁéáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂‰Ωú‰∏∫ÈÄöÁî®Êú∫Âô®‰∫∫Êìç‰ΩúÊ®°ÂûãÁöÑÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊñπÊ≥ïÔºåÁâπÂà´ÊòØÈÇ£‰∫õ‰æùËµñ‰∫éChain-of-Thought (CoT) ÁöÑÊñπÊ≥ïÔºåÂú®Â§çÊùÇÁéØÂ¢É‰∏≠Èöæ‰ª•ÂÖÖÂàÜÂà©Áî®ËßÜËßâ‰ø°ÊÅØ„ÄÇÊñáÊú¨CoTÈöæ‰ª•ÊçïÊçâÁªÜËá¥ÁöÑÂú∫ÊôØ‰ø°ÊÅØÔºåËÄåÁõ¥Êé•Â∞ÜËßÜËßâ‰ø°ÊÅØËûçÂÖ•Âä®‰ΩúÁîüÊàêÂèàÈù¢‰∏¥Ê®°ÊÄÅÂ∑ÆÂºÇÂíåËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÁöÑÈóÆÈ¢òÔºåÂç≥ËßÜËßâÈ¢ÑÊµãÂíåÂä®‰ΩúÁîüÊàêÁöÑÁõÆÊ†áÁõ∏‰∫íÁ´û‰∫â„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVITAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ≠¶‰π†‰∏Ä‰∏™ËßÜËßâÂíåÂä®‰ΩúÁöÑÂÖ±‰∫´Á¶ªÊï£ÊΩúÂú®Á©∫Èó¥Ôºå‰ªéËÄåÂ∞ÜËßÜËßâ‰ø°ÊÅØÊúâÊïàÂú∞ËûçÂÖ•Âà∞Âä®‰ΩúÁîüÊàêËøáÁ®ã‰∏≠„ÄÇÈÄöËøáÈöêÂºèËßÜËßâCoTÔºåÊ®°ÂûãËÉΩÂ§üËá™ÂõûÂΩíÂú∞ÁîüÊàêtokenÔºåËøô‰∫õtokenÊó¢Áî®‰∫éÈ¢ÑÊµãÊú™Êù•ÁöÑËßÜËßâÂ∏ßÔºåÂèàÁî®‰∫éÁîüÊàêÊú∫Âô®‰∫∫Âä®‰Ωú„ÄÇËøôÁßçËÆæËÆ°Â∞ÜËßÜËßâÂä®ÊÄÅ‰Ωú‰∏∫ËøêÂä®ËßÑÂàíÁöÑÂΩíÁ∫≥ÂÅèÁΩÆÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂä®‰ΩúÁîüÊàêÁöÑÁ®≥ÂÆöÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVITAÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËßÜËßâÁºñÁ†ÅÂô®ÔºåÁî®‰∫éÊèêÂèñËßÜËßâÁâπÂæÅÔºõ2) Âä®‰ΩúÁºñÁ†ÅÂô®ÔºåÁî®‰∫éÁºñÁ†ÅÂä®‰ΩúÂ∫èÂàóÔºõ3) ÂÖ±‰∫´Á¶ªÊï£ÊΩúÂú®Á©∫Èó¥ÔºåÁî®‰∫éÂØπËßÜËßâÂíåÂä®‰Ωú‰ø°ÊÅØËøõË°åÁªü‰∏ÄË°®Á§∫Ôºõ4) Ëá™ÂõûÂΩíËß£Á†ÅÂô®ÔºåÁî®‰∫éÁîüÊàêÈöêÂºèËßÜËßâCoT tokenÔºåÂπ∂Â∞ÜÂÖ∂Ëß£Á†Å‰∏∫Êú™Êù•Â∏ßÈ¢ÑÊµãÂíåÊú∫Âô®‰∫∫Âä®‰Ωú„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÁ´ØÂà∞Á´ØÂèØËÆ≠ÁªÉÁöÑ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVITAÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÈöêÂºèËßÜËßâCoTÁöÑÂºïÂÖ•„ÄÇ‰∏éÊòæÂºèÂú∞ÁîüÊàêÊñáÊú¨ÂΩ¢ÂºèÁöÑCoT‰∏çÂêåÔºåVITAÈÄöËøáËá™ÂõûÂΩíÂú∞ÁîüÊàêtokenÔºåÂπ∂Â∞ÜËøô‰∫õtokenÂêåÊó∂Áî®‰∫éËßÜËßâÈ¢ÑÊµãÂíåÂä®‰ΩúÁîüÊàêÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜËßÜËßâ‰ø°ÊÅØÂíåÂä®‰ΩúÁîüÊàêÁöÑÁ¥ßÂØÜËÄ¶Âêà„ÄÇËøôÁßçÈöêÂºèÁöÑÊñπÂºèÈÅøÂÖç‰∫ÜÊñáÊú¨CoTÂèØËÉΩÂ∏¶Êù•ÁöÑ‰ø°ÊÅØÊçüÂ§±ÂíåÊ≠ß‰πâÔºåÂπ∂ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVITAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®TransformerÊû∂ÊûÑ‰Ωú‰∏∫ËßÜËßâÂíåÂä®‰ΩúÁºñÁ†ÅÂô®ÂíåËß£Á†ÅÂô®Ôºõ2) ÈááÁî®Á¶ªÊï£ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÂ≠¶‰π†ÂÖ±‰∫´Á¶ªÊï£ÊΩúÂú®Á©∫Èó¥Ôºõ3) ‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ËÆ≠ÁªÉËá™ÂõûÂΩíËß£Á†ÅÂô®ÔºåÂêåÊó∂‰ºòÂåñÊú™Êù•Â∏ßÈ¢ÑÊµãÂíåÂä®‰ΩúÁîüÊàêÁöÑÂáÜÁ°ÆÊÄßÔºõ4) ÈÄöËøáË∞ÉÊï¥ËßÜËßâÈ¢ÑÊµãÂíåÂä®‰ΩúÁîüÊàêÊçüÂ§±ÁöÑÊùÉÈáçÔºåÂπ≥Ë°°‰∏§‰∏™ÁõÆÊ†á‰πãÈó¥ÁöÑÁ´û‰∫âÂÖ≥Á≥ª„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VITAÂú®CALVIN„ÄÅLIBEROÂíåSimplerEnvÁ≠âÊ®°ÊãüÁéØÂ¢Ébenchmark‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂàÜÂà´ÊØîÁé∞ÊúâÂü∫Á∫øÊèêÈ´ò‰∫Ü14.5%„ÄÅ9.6%Âíå12.1%„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåVITAÂú®ÂÖ≠‰∏™ÁúüÂÆû‰∏ñÁïåÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÂπ≥Âùá80.5%ÁöÑÊàêÂäüÁéáÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂÆûÁî®‰ª∑ÂÄº„ÄÇËøô‰∫õÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVITAÊòØÁõÆÂâçÊúÄÂÖàËøõÁöÑÈÄöÁî®Êú∫Âô®‰∫∫Êìç‰ΩúÊ®°Âûã‰πã‰∏Ä„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VITAÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°ÔºåÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÊú∫Âô®‰∫∫„ÄÅÂåªÁñóÊú∫Âô®‰∫∫Á≠â„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊÑüÁü•ÂíåÂÜ≥Á≠ñËÉΩÂäõÔºåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥Ëá™‰∏ªÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÇÊú™Êù•ÔºåVITAÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Â§öÊ®°ÊÄÅËæìÂÖ•ÔºàÂ¶ÇËØ≠Èü≥„ÄÅËß¶ËßâÔºâÂíåÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°Âú∫ÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\%, 9.6\% and 12.1\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.

