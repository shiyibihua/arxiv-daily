---
layout: default
title: ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation
---

# ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation

**arXiv**: [2511.20330v2](https://arxiv.org/abs/2511.20330) | [PDF](https://arxiv.org/pdf/2511.20330.pdf)

**ä½œè€…**: Yuhan Wu, Tiantian Wei, Shuo Wang, ZhiChao Wang, Yanyong Zhang, Daniel Cremers, Yan Xia

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25 (æ›´æ–°: 2025-11-27)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºArtiBenchå’ŒArtiBrainï¼Œç”¨äºŽè¯„ä¼°å’Œæå‡é€šç”¨è§†è§‰è¯­è¨€å¯åŠ¨å¯¹è±¡æ“ä½œèƒ½åŠ›ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å¯åŠ¨å¯¹è±¡æ“ä½œ` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æœºå™¨äººæ“ä½œ` `æ³›åŒ–èƒ½åŠ›` `å¯ä¾›æ€§å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰è¯­è¨€å’Œæ‰©æ•£æ¨¡åž‹åœ¨å¯åŠ¨å¯¹è±¡æ“ä½œä¸­æ³›åŒ–æ€§ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹éƒ¨ä»¶ã€å®žä¾‹å’Œç±»åˆ«çš„å˜åŒ–ã€‚
2. ArtiBrainæ¡†æž¶ç»“åˆVLMæŽ¨ç†å™¨åˆ†è§£ä»»åŠ¡ï¼Œä»¥åŠæ··åˆæŽ§åˆ¶å™¨å®žçŽ°ç²¾ç¡®æ“ä½œï¼Œå¹¶åˆ©ç”¨å¯ä¾›æ€§è®°å¿†åº“æå‡æ³›åŒ–èƒ½åŠ›ã€‚
3. ArtiBrainåœ¨ArtiBenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå±•çŽ°äº†æ›´å¼ºçš„é²æ£’æ€§å’Œæ³›åŒ–æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºArtiBenchï¼Œä¸€ä¸ªåŒ…å«åŽ¨æˆ¿ã€å‚¨è—å®¤ã€åŠžå…¬å®¤å’Œå·¥å…·çŽ¯å¢ƒçš„äº”çº§åŸºå‡†ï¼Œç”¨äºŽè¯„ä¼°å¯åŠ¨å¯¹è±¡æ“ä½œçš„æ³›åŒ–èƒ½åŠ›ã€‚çŽ°æœ‰åŸºäºŽè§†è§‰è¯­è¨€å’Œæ‰©æ•£æ¨¡åž‹çš„ç­–ç•¥éš¾ä»¥åœ¨éƒ¨ä»¶ã€å®žä¾‹å’Œç±»åˆ«ä¹‹é—´æ³›åŒ–ã€‚ArtiBenché€šè¿‡è·¨éƒ¨ä»¶ã€è·¨å®žä¾‹çš„å˜ä½“åˆ°é•¿æ—¶ç¨‹å¤šå¯¹è±¡ä»»åŠ¡çš„ç»“æž„åŒ–è¯„ä¼°ï¼Œæ­ç¤ºäº†å¯åŠ¨å¯¹è±¡æ“ä½œçš„æ ¸å¿ƒæ³›åŒ–æŒ‘æˆ˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡æå‡ºäº†ArtiBrainï¼Œä¸€ä¸ªç»Ÿä¸€é«˜å±‚æŽ¨ç†å’Œè‡ªé€‚åº”åº•å±‚æŽ§åˆ¶çš„æ¨¡å—åŒ–æ¡†æž¶ã€‚ArtiBrainä½¿ç”¨åŸºäºŽVLMçš„ä»»åŠ¡æŽ¨ç†å™¨ï¼ˆGPT-4.1ï¼‰åˆ†è§£å’ŒéªŒè¯å­ç›®æ ‡ï¼Œå¹¶é‡‡ç”¨æ··åˆæŽ§åˆ¶å™¨ï¼Œç»“åˆå‡ ä½•æ„ŸçŸ¥çš„å…³é”®å¸§æ‰§è¡Œå’Œå¯ä¾›æ€§å¼•å¯¼çš„æ‰©æ•£ï¼Œå®žçŽ°ç²¾ç¡®å’Œå¯è§£é‡Šçš„æ“ä½œã€‚å¯ä¾›æ€§è®°å¿†åº“ä¸æ–­ç§¯ç´¯æˆåŠŸçš„æ‰§è¡Œç‰‡æ®µï¼Œå¹¶å°†éƒ¨ä»¶çº§å¯æ“ä½œçš„å¯ä¾›æ€§ä¼ æ’­åˆ°æœªè§è¿‡çš„å¯åŠ¨éƒ¨ä»¶å’Œé…ç½®ã€‚åœ¨ArtiBenchä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒArtiBrainåœ¨é²æ£’æ€§å’Œæ³›åŒ–æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºŽæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å’ŒåŸºäºŽæ‰©æ•£çš„æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨æŽ¥æ”¶åŽå‘å¸ƒã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽè§†è§‰è¯­è¨€å’Œæ‰©æ•£æ¨¡åž‹çš„æœºå™¨äººæ“ä½œæ–¹æ³•åœ¨å¤„ç†å¯åŠ¨å¯¹è±¡æ—¶ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æ–°çš„éƒ¨ä»¶ã€å®žä¾‹å’Œç±»åˆ«ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ç¼ºä¹å¯¹å¯åŠ¨å¯¹è±¡å†…åœ¨ç»“æž„å’Œæ“ä½œé€»è¾‘çš„ç†è§£ï¼Œå¯¼è‡´åœ¨å¤æ‚åœºæ™¯å’Œé•¿æ—¶ç¨‹ä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ã€‚çŽ°æœ‰çš„è¯„ä¼°åŸºå‡†ä¹Ÿç¼ºä¹å¯¹æ³›åŒ–èƒ½åŠ›çš„ç»†ç²’åº¦è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é«˜å±‚ä»»åŠ¡æŽ¨ç†ä¸Žåº•å±‚è¿åŠ¨æŽ§åˆ¶è§£è€¦ï¼Œå¹¶å¼•å…¥å¯ä¾›æ€§å­¦ä¹ æœºåˆ¶æ¥æå‡æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡VLMè¿›è¡Œé«˜å±‚æŽ¨ç†ï¼Œåˆ†è§£ä»»åŠ¡å¹¶éªŒè¯å­ç›®æ ‡ï¼Œç¡®ä¿ä»»åŠ¡çš„é€»è¾‘æ­£ç¡®æ€§ã€‚é€šè¿‡æ··åˆæŽ§åˆ¶å™¨ï¼Œç»“åˆå‡ ä½•æ„ŸçŸ¥çš„å…³é”®å¸§æ‰§è¡Œå’Œå¯ä¾›æ€§å¼•å¯¼çš„æ‰©æ•£ï¼Œå®žçŽ°ç²¾ç¡®çš„æ“ä½œã€‚é€šè¿‡å¯ä¾›æ€§è®°å¿†åº“ï¼Œç§¯ç´¯ç»éªŒå¹¶æ³›åŒ–åˆ°æ–°çš„å¯¹è±¡å’Œåœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šArtiBrainæ¡†æž¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä»»åŠ¡æŽ¨ç†å™¨ã€æ··åˆæŽ§åˆ¶å™¨å’Œå¯ä¾›æ€§è®°å¿†åº“ã€‚ä»»åŠ¡æŽ¨ç†å™¨åŸºäºŽGPT-4.1ï¼Œè´Ÿè´£å°†é«˜å±‚ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—å­ç›®æ ‡ï¼Œå¹¶éªŒè¯å­ç›®æ ‡çš„å¯è¡Œæ€§ã€‚æ··åˆæŽ§åˆ¶å™¨ç»“åˆå‡ ä½•æ„ŸçŸ¥çš„å…³é”®å¸§æ‰§è¡Œå’Œå¯ä¾›æ€§å¼•å¯¼çš„æ‰©æ•£ï¼Œå®žçŽ°ç²¾ç¡®çš„æ“ä½œã€‚å¯ä¾›æ€§è®°å¿†åº“å­˜å‚¨æˆåŠŸçš„æ“ä½œç‰‡æ®µï¼Œå¹¶æå–éƒ¨ä»¶çº§çš„å¯ä¾›æ€§ä¿¡æ¯ï¼Œç”¨äºŽæŒ‡å¯¼æ–°çš„æ“ä½œä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šArtiBrainçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†ArtiBenchåŸºå‡†ï¼Œç”¨äºŽç»†ç²’åº¦è¯„ä¼°å¯åŠ¨å¯¹è±¡æ“ä½œçš„æ³›åŒ–èƒ½åŠ›ï¼›2) æå‡ºäº†ArtiBrainæ¡†æž¶ï¼Œå°†é«˜å±‚æŽ¨ç†ä¸Žåº•å±‚æŽ§åˆ¶è§£è€¦ï¼Œå¹¶å¼•å…¥å¯ä¾›æ€§å­¦ä¹ æœºåˆ¶ï¼›3) æå‡ºäº†æ··åˆæŽ§åˆ¶å™¨ï¼Œç»“åˆäº†å‡ ä½•æ„ŸçŸ¥å’Œå¯ä¾›æ€§å¼•å¯¼çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šä»»åŠ¡æŽ¨ç†å™¨ä½¿ç”¨GPT-4.1ï¼Œé€šè¿‡promptå·¥ç¨‹æ¥æŒ‡å¯¼ä»»åŠ¡åˆ†è§£å’ŒéªŒè¯ã€‚æ··åˆæŽ§åˆ¶å™¨ä½¿ç”¨å‡ ä½•æ„ŸçŸ¥çš„å…³é”®å¸§æ‰§è¡Œæ¥å¿«é€ŸæŽ¥è¿‘ç›®æ ‡ï¼Œç„¶åŽä½¿ç”¨å¯ä¾›æ€§å¼•å¯¼çš„æ‰©æ•£æ¥ç²¾ç»†è°ƒæ•´ã€‚å¯ä¾›æ€§è®°å¿†åº“ä½¿ç”¨å“ˆå¸Œè¡¨æ¥å­˜å‚¨æ“ä½œç‰‡æ®µï¼Œå¹¶ä½¿ç”¨ç›¸ä¼¼æ€§åº¦é‡æ¥æ£€ç´¢ç›¸å…³çš„å¯ä¾›æ€§ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è¿åŠ¨å­¦çº¦æŸæŸå¤±ã€ç¢°æ’žé¿å…æŸå¤±å’Œç›®æ ‡è¾¾æˆæŸå¤±ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

ArtiBrainåœ¨ArtiBenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è·¨éƒ¨ä»¶æ³›åŒ–ä»»åŠ¡ä¸­ï¼ŒArtiBrainçš„æˆåŠŸçŽ‡æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†20%ä»¥ä¸Šã€‚åœ¨é•¿æ—¶ç¨‹å¤šå¯¹è±¡ä»»åŠ¡ä¸­ï¼ŒArtiBrainçš„æˆåŠŸçŽ‡ä¹Ÿæ˜¾è‘—é«˜äºŽå…¶ä»–æ–¹æ³•ï¼Œè¡¨æ˜Žå…¶å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨åŒ–è£…é…ã€åŒ»ç–—æœºå™¨äººç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¶å±…ä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®ç”¨æˆ·çš„æŒ‡ä»¤ï¼Œæ“ä½œå„ç§å®¶ç”¨ç”µå™¨ï¼Œå¦‚æ‰“å¼€å†°ç®±é—¨ã€è°ƒèŠ‚çƒ¤ç®±æ¸©åº¦ç­‰ã€‚åœ¨è‡ªåŠ¨åŒ–è£…é…ä¸­ï¼Œæœºå™¨äººå¯ä»¥çµæ´»åœ°æ“ä½œå„ç§é›¶éƒ¨ä»¶ï¼Œå®Œæˆå¤æ‚çš„è£…é…ä»»åŠ¡ã€‚åœ¨åŒ»ç–—æœºå™¨äººä¸­ï¼Œæœºå™¨äººå¯ä»¥è¾…åŠ©åŒ»ç”Ÿè¿›è¡Œæ‰‹æœ¯ï¼Œæé«˜æ‰‹æœ¯çš„ç²¾åº¦å’Œæ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Interactive articulated manipulation requires long-horizon, multi-step interactions with appliances while maintaining physical consistency. Existing vision-language and diffusion-based policies struggle to generalize across parts, instances, and categories. We first introduce ArtiBench, a five-level benchmark covering kitchen, storage, office, and tool environments. ArtiBench enables structured evaluation from cross-part and cross-instance variation to long-horizon multi-object tasks, revealing the core generalization challenges of articulated object manipulation. Building on this benchmark, we propose ArtiBrain, a modular framework that unifies high-level reasoning with adaptive low-level control. ArtiBrain uses a VLM-based Task Reasoner (GPT-4.1) to decompose and validate subgoals, and employs a Hybrid Controller that combines geometry-aware keyframe execution with affordance-guided diffusion for precise and interpretable manipulation. An Affordance Memory Bank continually accumulates successful execution episodes and propagates part-level actionable affordances to unseen articulated parts and configurations. Extensive experiments on ArtiBench show that our ArtiBrain significantly outperforms state-of-the-art multimodal and diffusion-based methods in robustness and generalization. Code and dataset will be released upon acceptance.

