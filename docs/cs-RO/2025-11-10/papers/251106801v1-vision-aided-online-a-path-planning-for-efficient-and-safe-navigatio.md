---
layout: default
title: Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots
---

# Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots

**arXiv**: [2511.06801v1](https://arxiv.org/abs/2511.06801) | [PDF](https://arxiv.org/pdf/2511.06801.pdf)

**ä½œè€…**: Praveen Kumar, Tushar Sandhan

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

**å¤‡æ³¨**: 10 pages

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰è¾…åŠ©çš„åœ¨çº¿A*è·¯å¾„è§„åˆ’ï¼Œç”¨äºŽæœåŠ¡æœºå™¨äººé«˜æ•ˆå®‰å…¨å¯¼èˆª**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æœåŠ¡æœºå™¨äºº` `è·¯å¾„è§„åˆ’` `è¯­ä¹‰åˆ†å‰²` `è§†è§‰å¯¼èˆª` `A*ç®—æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸæœåŠ¡æœºå™¨äººå¯¼èˆªä¾èµ–æ¿€å…‰é›·è¾¾ï¼Œç¼ºä¹è¯­ä¹‰ç†è§£ï¼Œæ— æ³•åŒºåˆ†é‡è¦ç‰©ä½“ä¸Žæ™®é€šéšœç¢ç‰©ã€‚
2. æå‡ºè§†è§‰è¾…åŠ©çš„åœ¨çº¿A*è·¯å¾„è§„åˆ’ï¼Œé€šè¿‡è¯­ä¹‰åˆ†å‰²è¯†åˆ«è§†è§‰çº¦æŸï¼ŒæŒ‡å¯¼æœºå™¨äººå¯¼èˆªã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®žæœºå™¨äººå¹³å°ä¸Šå®žçŽ°äº†å®žæ—¶ã€é²æ£’çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¼èˆªã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„çŽ¯å¢ƒä¸­éƒ¨ç½²è‡ªä¸»æœåŠ¡æœºå™¨äººé¢ä¸´æ„ŸçŸ¥å’Œè§„åˆ’çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¯¼èˆªç³»ç»Ÿä¾èµ–æ˜‚è´µçš„æ¿€å…‰é›·è¾¾ï¼Œè™½ç„¶å‡ ä½•ç²¾åº¦é«˜ï¼Œä½†ç¼ºä¹è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ— æ³•åŒºåˆ†åŠžå…¬å®¤åœ°é¢ä¸Šçš„é‡è¦æ–‡ä»¶å’Œæ— å®³çš„åžƒåœ¾ï¼Œå°†ä¸¤è€…éƒ½è§†ä¸ºå¯ç‰©ç†ç©¿è¶Šçš„ã€‚è™½ç„¶å­˜åœ¨å…ˆè¿›çš„è¯­ä¹‰åˆ†å‰²æŠ€æœ¯ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶æœªèƒ½æˆåŠŸåœ°å°†è¿™ç§è§†è§‰æ™ºèƒ½é›†æˆåˆ°å®žæ—¶è·¯å¾„è§„åˆ’å™¨ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨ä½Žæˆæœ¬åµŒå…¥å¼ç¡¬ä»¶ä¸Šè¿è¡Œã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æž¶æ¥å¼¥åˆè¿™ä¸€å·®è·ï¼Œåœ¨ç»æµŽå®žæƒ çš„æœºå™¨äººå¹³å°ä¸Šå®žçŽ°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¼èˆªã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾§é‡äºŽè½»é‡çº§æ„ŸçŸ¥æ¨¡å—ä¸Žåœ¨çº¿A*è§„åˆ’å™¨çš„ç´§å¯†é›†æˆã€‚æ„ŸçŸ¥ç³»ç»Ÿé‡‡ç”¨è¯­ä¹‰åˆ†å‰²æ¨¡åž‹æ¥è¯†åˆ«ç”¨æˆ·å®šä¹‰çš„å¯è§†çº¦æŸï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡é‡è¦æ€§è€Œä¸æ˜¯ç‰©ç†å°ºå¯¸è¿›è¡Œå¯¼èˆªã€‚è¿™ç§é€‚åº”æ€§å…è®¸æ“ä½œå‘˜å®šä¹‰ç»™å®šä»»åŠ¡çš„å…³é”®å†…å®¹ï¼Œæ— è®ºæ˜¯åŠžå…¬å®¤ä¸­çš„æ•æ„Ÿæ–‡ä»¶è¿˜æ˜¯å·¥åŽ‚ä¸­çš„å®‰å…¨çº¿ï¼Œä»Žè€Œè§£å†³é¿å…å¯¹è±¡çš„æ¨¡ç³Šæ€§ã€‚è¿™ç§è¯­ä¹‰æ„ŸçŸ¥ä¸Žå‡ ä½•æ•°æ®æ— ç¼èžåˆã€‚è¯†åˆ«å‡ºçš„è§†è§‰çº¦æŸè¢«æŠ•å½±ä¸ºéžå‡ ä½•éšœç¢ç‰©åˆ°å…¨å±€åœ°å›¾ä¸Šï¼Œè¯¥åœ°å›¾ä»Žä¼ æ„Ÿå™¨æ•°æ®ä¸æ–­æ›´æ–°ï¼Œä»Žè€Œå®žçŽ°åœ¨éƒ¨åˆ†å·²çŸ¥å’ŒæœªçŸ¥çŽ¯å¢ƒä¸­é²æ£’å¯¼èˆªã€‚æˆ‘ä»¬é€šè¿‡é«˜ä¿çœŸæ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººå¹³å°ä¸Šçš„å¤§é‡å®žéªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æž¶ã€‚ç»“æžœè¡¨æ˜Žï¼Œè¯¥æ¡†æž¶å…·æœ‰é²æ£’çš„å®žæ—¶æ€§èƒ½ï¼Œè¯æ˜Žäº†ç»æµŽé«˜æ•ˆçš„æœºå™¨äººå¯ä»¥åœ¨å¤æ‚çš„çŽ¯å¢ƒä¸­å®‰å…¨å¯¼èˆªï¼ŒåŒæ—¶å°Šé‡ä¼ ç»Ÿè§„åˆ’å™¨æ— æ³•è¯†åˆ«çš„å…³é”®è§†è§‰çº¿ç´¢ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æœåŠ¡æœºå™¨äººå¯¼èˆªç³»ç»Ÿä¸»è¦ä¾èµ–æ¿€å…‰é›·è¾¾ç­‰ä¼ æ„Ÿå™¨è¿›è¡ŒçŽ¯å¢ƒæ„ŸçŸ¥å’Œè·¯å¾„è§„åˆ’ã€‚è¿™äº›æ–¹æ³•è™½ç„¶èƒ½å¤Ÿæä¾›ç²¾ç¡®çš„å‡ ä½•ä¿¡æ¯ï¼Œä½†ç¼ºä¹å¯¹çŽ¯å¢ƒè¯­ä¹‰ä¿¡æ¯çš„ç†è§£ï¼Œæ— æ³•åŒºåˆ†ä¸åŒç±»åž‹çš„éšœç¢ç‰©ï¼Œä¾‹å¦‚é‡è¦çš„æ–‡ä»¶å’Œåžƒåœ¾ã€‚è¿™å¯¼è‡´æœºå™¨äººå¯èƒ½å°†é‡è¦ç‰©å“è§†ä¸ºéšœç¢ç‰©è€Œé¿å¼€ï¼Œæˆ–è€…å°†å±é™©åŒºåŸŸè¯¯åˆ¤ä¸ºå¯è¡ŒåŒºåŸŸï¼Œä»Žè€Œå½±å“å¯¼èˆªæ•ˆçŽ‡å’Œå®‰å…¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰è¯­ä¹‰ä¿¡æ¯èžå…¥åˆ°è·¯å¾„è§„åˆ’è¿‡ç¨‹ä¸­ã€‚é€šè¿‡ä½¿ç”¨è¯­ä¹‰åˆ†å‰²æ¨¡åž‹è¯†åˆ«çŽ¯å¢ƒä¸­çš„å…³é”®è§†è§‰çº¦æŸï¼ˆä¾‹å¦‚ï¼Œéœ€è¦é¿å¼€çš„æ–‡ä»¶æˆ–éœ€è¦éµå¾ªçš„å®‰å…¨çº¿ï¼‰ï¼Œå¹¶å°†è¿™äº›çº¦æŸè½¬åŒ–ä¸ºéžå‡ ä½•éšœç¢ç‰©ï¼Œä»Žè€ŒæŒ‡å¯¼æœºå™¨äººè¿›è¡Œæ›´æ™ºèƒ½ã€æ›´å®‰å…¨çš„å¯¼èˆªã€‚è¿™ç§æ–¹æ³•å…è®¸æœºå™¨äººæ ¹æ®ä¸Šä¸‹æ–‡çš„é‡è¦æ€§è€Œä¸æ˜¯ç‰©ç†å°ºå¯¸æ¥åŒºåˆ†éšœç¢ç‰©ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ¡†æž¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ„ŸçŸ¥æ¨¡å—å’Œè§„åˆ’æ¨¡å—ã€‚æ„ŸçŸ¥æ¨¡å—ä½¿ç”¨è½»é‡çº§çš„è¯­ä¹‰åˆ†å‰²æ¨¡åž‹ä»Žæ‘„åƒå¤´å›¾åƒä¸­æå–è¯­ä¹‰ä¿¡æ¯ï¼Œè¯†åˆ«ç”¨æˆ·å®šä¹‰çš„å¯è§†çº¦æŸã€‚ç„¶åŽï¼Œè¿™äº›å¯è§†çº¦æŸè¢«æŠ•å½±åˆ°å…¨å±€åœ°å›¾ä¸Šï¼Œä½œä¸ºéžå‡ ä½•éšœç¢ç‰©ã€‚è§„åˆ’æ¨¡å—ä½¿ç”¨åœ¨çº¿A*ç®—æ³•ï¼Œåœ¨è€ƒè™‘å‡ ä½•éšœç¢ç‰©å’Œéžå‡ ä½•éšœç¢ç‰©çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆæœ€ä¼˜è·¯å¾„ã€‚å…¨å±€åœ°å›¾é€šè¿‡ä¼ æ„Ÿå™¨æ•°æ®ä¸æ–­æ›´æ–°ï¼Œä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„çŽ¯å¢ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽå°†è§†è§‰è¯­ä¹‰ä¿¡æ¯ä¸Žä¼ ç»Ÿçš„å‡ ä½•è·¯å¾„è§„åˆ’ç›¸ç»“åˆã€‚é€šè¿‡å°†è¯­ä¹‰åˆ†å‰²çš„ç»“æžœè½¬åŒ–ä¸ºéžå‡ ä½•éšœç¢ç‰©ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè®©æœºå™¨äººç†è§£çŽ¯å¢ƒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶æ ¹æ®è¿™äº›ä¿¡æ¯è¿›è¡Œæ›´æ™ºèƒ½çš„å¯¼èˆªã€‚ä¸Žä¼ ç»Ÿçš„åŸºäºŽå‡ ä½•ä¿¡æ¯çš„è·¯å¾„è§„åˆ’æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çŽ¯å¢ƒä¸­çš„å¯¼èˆªä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ„ŸçŸ¥æ¨¡å—é‡‡ç”¨è½»é‡çº§çš„è¯­ä¹‰åˆ†å‰²æ¨¡åž‹ï¼Œä»¥ä¿è¯å®žæ—¶æ€§ã€‚è¯­ä¹‰åˆ†å‰²æ¨¡åž‹çš„é€‰æ‹©éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ï¼Œä»¥ä¿è¯è¯†åˆ«ç²¾åº¦å’Œè®¡ç®—æ•ˆçŽ‡ã€‚å…¨å±€åœ°å›¾é‡‡ç”¨æ …æ ¼åœ°å›¾çš„å½¢å¼ï¼Œæ¯ä¸ªæ …æ ¼å­˜å‚¨å‡ ä½•éšœç¢ç‰©ä¿¡æ¯å’Œéžå‡ ä½•éšœç¢ç‰©ä¿¡æ¯ã€‚A*ç®—æ³•çš„ä»£ä»·å‡½æ•°éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè®¾è®¡ï¼Œä»¥å¹³è¡¡è·¯å¾„é•¿åº¦ã€å®‰å…¨æ€§ç­‰å› ç´ ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥è®ºæ–‡é€šè¿‡é«˜ä¿çœŸæ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººå¹³å°ä¸Šçš„å®žéªŒéªŒè¯äº†æ‰€æå‡ºæ¡†æž¶çš„æœ‰æ•ˆæ€§ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ¡†æž¶èƒ½å¤Ÿå®žæ—¶ã€é²æ£’åœ°è¯†åˆ«è§†è§‰çº¦æŸï¼Œå¹¶ç”Ÿæˆå®‰å…¨ã€é«˜æ•ˆçš„å¯¼èˆªè·¯å¾„ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†å®žæ—¶æ€§å’Œé²æ£’æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽæœåŠ¡æœºå™¨äººé¢†åŸŸï¼Œä¾‹å¦‚åŠžå…¬å®¤æ¸…æ´æœºå™¨äººã€åŒ»é™¢é…é€æœºå™¨äººã€å·¥åŽ‚å·¡æ£€æœºå™¨äººç­‰ã€‚é€šè¿‡èµ‹äºˆæœºå™¨äººä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»çŽ¯å¢ƒï¼Œä»Žè€Œæé«˜æœåŠ¡æ•ˆçŽ‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€å¢žå¼ºçŽ°å®žç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are semantically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a framework to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost-effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners.

