---
layout: default
title: Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving
---

# Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.07155" target="_blank" class="toolbar-btn">arXiv: 2511.07155v1</a>
    <a href="https://arxiv.org/pdf/2511.07155.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.07155v1" 
            onclick="toggleFavorite(this, '2511.07155v1', 'Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Thomas Steinecker, Alexander Bienemann, Denis Trescher, Thorsten Luettel, Mirko Maehlisch

**ÂàÜÁ±ª**: cs.RO, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-10

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âä®ÂäõÂ≠¶Ëß£ËÄ¶ÁöÑËΩ®ËøπÂØπÈΩêÊñπÊ≥ïÔºåÂÆûÁé∞Ëá™Âä®È©æÈ©∂RL Sim-to-RealÈõ∂Ê†∑Êú¨ËøÅÁßª**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Ëá™Âä®È©æÈ©∂` `Sim-to-Real` `ËΩ®ËøπÂØπÈΩê` `ËøêÂä®ËßÑÂàí`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÁúüÂÆûËΩ¶ËæÜÂä®ÂäõÂ≠¶Âª∫Ê®°Âõ∞ÈöæÔºåÂØºËá¥‰ªøÁúüËÆ≠ÁªÉÁöÑRLÊô∫ËÉΩ‰ΩìÈöæ‰ª•Áõ¥Êé•ËøÅÁßªÂà∞ÁúüÂÆûÁéØÂ¢É„ÄÇ
2. ÈÄöËøáÁ©∫Èó¥ÂíåÊó∂Èó¥‰∏äÁöÑËΩ®ËøπÂØπÈΩêÔºåÂ∞ÜËøêÂä®ËßÑÂàí‰∏éËΩ¶ËæÜÊéßÂà∂Ëß£ËÄ¶ÔºåÂÆûÁé∞Sim-to-RealËøÅÁßª„ÄÇ
3. Âú®ÁúüÂÆûËΩ¶ËæÜ‰∏äÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÔºåÂÆûÁé∞‰∫ÜRLËøêÂä®ËßÑÂàíÁöÑÈõ∂Ê†∑Êú¨ËøÅÁßªÔºåÊúâÊïàËß£ËÄ¶‰∫ÜÈ´ò‰ΩéÂ±ÇÊéßÂà∂„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âº∫ÂåñÂ≠¶‰π†(RL)Âú®Êú∫Âô®‰∫∫È¢ÜÂüüÂ±ïÁé∞‰∫ÜÊΩúÂäõÔºå‰ΩÜÁî±‰∫éËΩ¶ËæÜÂä®ÂäõÂ≠¶ÁöÑÂ§çÊùÇÊÄßÂíå‰ªøÁúü‰∏éÁé∞ÂÆû‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÂú®ÁúüÂÆûËΩ¶ËæÜ‰∏äÈÉ®ÁΩ≤RL‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇËΩÆËÉéÁâπÊÄß„ÄÅË∑ØÈù¢Áä∂ÂÜµ„ÄÅÁ©∫Ê∞îÂä®ÂäõÊâ∞Âä®ÂíåËΩ¶ËæÜË¥üËΩΩÁ≠âÂõ†Á¥†‰ΩøÂæóÂáÜÁ°ÆÂª∫Ê®°ÁúüÂÆû‰∏ñÁïåÂä®ÂäõÂ≠¶ÂèòÂæó‰∏çÂèØË°åÔºåËøôÈòªÁ¢ç‰∫ÜÂú®‰ªøÁúü‰∏≠ËÆ≠ÁªÉÁöÑRLÊô∫ËÉΩ‰ΩìÁöÑÁõ¥Êé•ËøÅÁßª„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ°ÜÊû∂ÔºåÈÄöËøáËôöÊãüËΩ¶ËæÜÂíåÁúüÂÆûÁ≥ªÁªü‰πãÈó¥ÁöÑÁ©∫Èó¥ÂíåÊó∂Èó¥ÂØπÈΩêÁ≠ñÁï•ÔºåÂ∞ÜËøêÂä®ËßÑÂàí‰∏éËΩ¶ËæÜÊéßÂà∂Ëß£ËÄ¶„ÄÇÈ¶ñÂÖàÔºåÂú®‰ªøÁúü‰∏≠‰ΩøÁî®ËøêÂä®Â≠¶Ëá™Ë°åËΩ¶Ê®°ÂûãËÆ≠ÁªÉRLÊô∫ËÉΩ‰Ωì‰ª•ËæìÂá∫ËøûÁª≠ÊéßÂà∂Âä®‰Ωú„ÄÇÁÑ∂ÂêéÔºåÂ∞ÜÂÖ∂Ë°å‰∏∫ÊèêÁÇºÊàêËΩ®ËøπÈ¢ÑÊµãÊô∫ËÉΩ‰ΩìÔºåÁîüÊàêÊúâÈôêËåÉÂõ¥ÁöÑËá™ËΩ¶ËΩ®ËøπÔºå‰ªéËÄåÂÆûÁé∞ËôöÊãüËΩ¶ËæÜÂíåÁúüÂÆûËΩ¶ËæÜ‰πãÈó¥ÁöÑÂêåÊ≠•„ÄÇÂú®ÈÉ®ÁΩ≤Êó∂ÔºåStanleyÊéßÂà∂Âô®ÊéßÂà∂Ê®™ÂêëÂä®ÂäõÂ≠¶ÔºåËÄåÁ∫µÂêëÂØπÈΩêÈÄöËøáËá™ÈÄÇÂ∫îÊõ¥Êñ∞Êú∫Âà∂Êù•Áª¥ÊåÅÔºå‰ª•Ë°•ÂÅøËôöÊãüËΩ®ËøπÂíåÁúüÂÆûËΩ®Ëøπ‰πãÈó¥ÁöÑÂÅèÂ∑Æ„ÄÇÊàë‰ª¨Âú®ÁúüÂÆûËΩ¶ËæÜ‰∏äÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÔºåÂπ∂ËØÅÊòéÊâÄÊèêÂá∫ÁöÑÂØπÈΩêÁ≠ñÁï•ËÉΩÂ§üÂÆûÁé∞Âü∫‰∫éRLÁöÑËøêÂä®ËßÑÂàí‰ªé‰ªøÁúüÂà∞Áé∞ÂÆûÁöÑÈ≤ÅÊ£íÈõ∂Ê†∑Êú¨ËøÅÁßªÔºåÊàêÂäüÂú∞Â∞ÜÈ´òÂ±ÇËΩ®ËøπÁîüÊàê‰∏é‰ΩéÂ±ÇËΩ¶ËæÜÊéßÂà∂Ëß£ËÄ¶„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âº∫ÂåñÂ≠¶‰π†Âú®Ëá™Âä®È©æÈ©∂È¢ÜÂüü‰∏≠ÔºåÁî±‰∫é‰ªøÁúüÁéØÂ¢É‰∏éÁúüÂÆûÁéØÂ¢ÉÂ≠òÂú®Â∑ÆÂºÇÔºåÂØºËá¥Âú®‰ªøÁúüÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÁöÑRLÊô∫ËÉΩ‰ΩìÊó†Ê≥ïÁõ¥Êé•ËøÅÁßªÂà∞ÁúüÂÆûËΩ¶ËæÜ‰∏äÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÂáÜÁ°ÆÂª∫Ê®°ÁúüÂÆûËΩ¶ËæÜÁöÑÂ§çÊùÇÂä®ÂäõÂ≠¶ÁâπÊÄßÔºå‰æãÂ¶ÇËΩÆËÉéÁâπÊÄß„ÄÅË∑ØÈù¢Áä∂ÂÜµÁ≠âÔºåËøô‰ΩøÂæóSim-to-RealÁöÑËøÅÁßªÂèòÂæóÂõ∞Èöæ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËøêÂä®ËßÑÂàí‰∏éËΩ¶ËæÜÊéßÂà∂Ëß£ËÄ¶„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈ¶ñÂÖàÂú®‰ªøÁúüÁéØÂ¢É‰∏≠‰ΩøÁî®RLËÆ≠ÁªÉ‰∏Ä‰∏™Êô∫ËÉΩ‰ΩìÔºåËØ•Êô∫ËÉΩ‰ΩìËæìÂá∫ËøûÁª≠ÁöÑÊéßÂà∂Âä®‰Ωú„ÄÇÁÑ∂ÂêéÔºåÂ∞ÜËØ•Êô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫ÊèêÁÇºÊàê‰∏Ä‰∏™ËΩ®ËøπÈ¢ÑÊµãÂô®ÔºåËØ•È¢ÑÊµãÂô®ÁîüÊàêÊúâÈôêËåÉÂõ¥ÂÜÖÁöÑËΩ¶ËæÜËΩ®Ëøπ„ÄÇÈÄöËøáÂØπÈΩêËôöÊãüËΩ¶ËæÜÂíåÁúüÂÆûËΩ¶ËæÜÁöÑËΩ®ËøπÔºåÂÆûÁé∞Sim-to-RealÁöÑËøÅÁßª„ÄÇËøôÁßçËß£ËÄ¶ÁöÑÊñπÂºèÈôç‰Ωé‰∫ÜÂØπÁ≤æÁ°ÆÂä®ÂäõÂ≠¶Ê®°ÂûãÁöÑ‰æùËµñÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜËøÅÁßªÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Âú®‰ªøÁúüÁéØÂ¢É‰∏≠‰ΩøÁî®RLËÆ≠ÁªÉËΩ®ËøπÁîüÊàêÂô®Ôºõ2) Â∞ÜËÆ≠ÁªÉÂ•ΩÁöÑËΩ®ËøπÁîüÊàêÂô®ÈÉ®ÁΩ≤Âà∞ÁúüÂÆûËΩ¶ËæÜ‰∏äÔºõ3) ‰ΩøÁî®StanleyÊéßÂà∂Âô®ÊéßÂà∂ËΩ¶ËæÜÁöÑÊ®™ÂêëËøêÂä®Ôºõ4) ‰ΩøÁî®Ëá™ÈÄÇÂ∫îÊõ¥Êñ∞Êú∫Âà∂Êù•ÂØπÈΩêËôöÊãüËΩ®ËøπÂíåÁúüÂÆûËΩ®ËøπÔºåË°•ÂÅø‰∏§ËÄÖ‰πãÈó¥ÁöÑÂÅèÂ∑Æ„ÄÇËØ•Ê°ÜÊû∂Â∞ÜÈ´òÂ±ÇËΩ®ËøπÁîüÊàê‰∏é‰ΩéÂ±ÇËΩ¶ËæÜÊéßÂà∂ÂàÜÁ¶ªÔºåÁÆÄÂåñ‰∫ÜÊéßÂà∂Á≠ñÁï•ÁöÑÂ§çÊùÇÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂä®ÂäõÂ≠¶Ëß£ËÄ¶ÁöÑËΩ®ËøπÂØπÈΩêÊñπÊ≥ï„ÄÇ‰∏é‰º†ÁªüÁöÑÁõ¥Êé•Â∞ÜRLÊô∫ËÉΩ‰ΩìÈÉ®ÁΩ≤Âà∞ÁúüÂÆûËΩ¶ËæÜ‰∏äÁöÑÊñπÊ≥ï‰∏çÂêåÔºåËØ•ÊñπÊ≥ïÈÄöËøáËΩ®ËøπÂØπÈΩêÁöÑÊñπÂºèÔºåÂ∞ÜËøêÂä®ËßÑÂàí‰∏éËΩ¶ËæÜÊéßÂà∂Ëß£ËÄ¶ÔºåÈôç‰Ωé‰∫ÜÂØπÁ≤æÁ°ÆÂä®ÂäõÂ≠¶Ê®°ÂûãÁöÑ‰æùËµñ„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®Ëá™ÈÄÇÂ∫îÊõ¥Êñ∞Êú∫Âà∂Êù•Ë°•ÂÅøËôöÊãüËΩ®ËøπÂíåÁúüÂÆûËΩ®Ëøπ‰πãÈó¥ÁöÑÂÅèÂ∑ÆÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜËøÅÁßªÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®‰ªøÁúüÁéØÂ¢É‰∏≠Ôºå‰ΩøÁî®ËøêÂä®Â≠¶Ëá™Ë°åËΩ¶Ê®°ÂûãÊù•ÁÆÄÂåñËΩ¶ËæÜÂä®ÂäõÂ≠¶„ÄÇRLÊô∫ËÉΩ‰Ωì‰ΩøÁî®ËøûÁª≠ÊéßÂà∂Âä®‰Ωú‰Ωú‰∏∫ËæìÂá∫„ÄÇËΩ®ËøπÈ¢ÑÊµãÂô®ÁîüÊàêÊúâÈôêËåÉÂõ¥ÁöÑËá™ËΩ¶ËΩ®Ëøπ„ÄÇÂú®ÁúüÂÆûËΩ¶ËæÜ‰∏äÔºå‰ΩøÁî®StanleyÊéßÂà∂Âô®ÊéßÂà∂Ê®™ÂêëËøêÂä®ÔºåËá™ÈÄÇÂ∫îÊõ¥Êñ∞Êú∫Âà∂Ê†πÊçÆËôöÊãüËΩ®ËøπÂíåÁúüÂÆûËΩ®Ëøπ‰πãÈó¥ÁöÑÂÅèÂ∑ÆÊù•Ë∞ÉÊï¥ÊéßÂà∂ÂèÇÊï∞„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁ≠âÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠Êú™ËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ËÆ∫ÊñáÂú®ÁúüÂÆûËΩ¶ËæÜ‰∏äÈ™åËØÅ‰∫ÜÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºåÂÆûÁé∞‰∫ÜÂü∫‰∫éRLÁöÑËøêÂä®ËßÑÂàí‰ªé‰ªøÁúüÂà∞Áé∞ÂÆûÁöÑÈõ∂Ê†∑Êú¨ËøÅÁßª„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞Ëß£ËÄ¶È´òÂ±ÇËΩ®ËøπÁîüÊàê‰∏é‰ΩéÂ±ÇËΩ¶ËæÜÊéßÂà∂ÔºåÂπ∂ËÉΩÂ§üÈ≤ÅÊ£íÂú∞Â∫îÂØπÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÂêÑÁßç‰∏çÁ°ÆÂÆöÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Âú®ËÆ∫Êñá‰∏≠Êú™ËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂ËΩ¶ËæÜÁöÑËøêÂä®ËßÑÂàíÂíåÊéßÂà∂ÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÂø´ÈÄüÈÉ®ÁΩ≤ÂíåÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÁöÑÂú∫ÊôØ‰∏≠„ÄÇÈÄöËøáSim-to-RealËøÅÁßªÔºåÂèØ‰ª•Èôç‰ΩéÂºÄÂèëÊàêÊú¨ÂíåÈ£éÈô©ÔºåÂä†ÈÄüËá™Âä®È©æÈ©∂ÊäÄÊúØÁöÑËêΩÂú∞„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÊú∫Âô®‰∫∫È¢ÜÂüüÔºå‰æãÂ¶ÇÊó†‰∫∫Êú∫ÂíåÁßªÂä®Êú∫Âô®‰∫∫„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.

