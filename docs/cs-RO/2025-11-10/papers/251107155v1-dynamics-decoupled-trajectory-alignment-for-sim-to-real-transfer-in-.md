---
layout: default
title: Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving
---

# Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving

**arXiv**: [2511.07155v1](https://arxiv.org/abs/2511.07155) | [PDF](https://arxiv.org/pdf/2511.07155.pdf)

**ä½œè€…**: Thomas Steinecker, Alexander Bienemann, Denis Trescher, Thorsten Luettel, Mirko Maehlisch

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨åŠ›å­¦è§£è€¦çš„è½¨è¿¹å¯¹é½æ–¹æ³•ï¼Œå®žçŽ°è‡ªåŠ¨é©¾é©¶RL Sim-to-Realé›¶æ ·æœ¬è¿ç§»**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨é©¾é©¶` `Sim-to-Real` `è½¨è¿¹å¯¹é½` `è¿åŠ¨è§„åˆ’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çœŸå®žè½¦è¾†åŠ¨åŠ›å­¦å»ºæ¨¡å›°éš¾ï¼Œå¯¼è‡´ä»¿çœŸè®­ç»ƒçš„RLæ™ºèƒ½ä½“éš¾ä»¥ç›´æŽ¥è¿ç§»åˆ°çœŸå®žçŽ¯å¢ƒã€‚
2. é€šè¿‡ç©ºé—´å’Œæ—¶é—´ä¸Šçš„è½¨è¿¹å¯¹é½ï¼Œå°†è¿åŠ¨è§„åˆ’ä¸Žè½¦è¾†æŽ§åˆ¶è§£è€¦ï¼Œå®žçŽ°Sim-to-Realè¿ç§»ã€‚
3. åœ¨çœŸå®žè½¦è¾†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå®žçŽ°äº†RLè¿åŠ¨è§„åˆ’çš„é›¶æ ·æœ¬è¿ç§»ï¼Œæœ‰æ•ˆè§£è€¦äº†é«˜ä½Žå±‚æŽ§åˆ¶ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)åœ¨æœºå™¨äººé¢†åŸŸå±•çŽ°äº†æ½œåŠ›ï¼Œä½†ç”±äºŽè½¦è¾†åŠ¨åŠ›å­¦çš„å¤æ‚æ€§å’Œä»¿çœŸä¸ŽçŽ°å®žä¹‹é—´çš„å·®å¼‚ï¼Œåœ¨çœŸå®žè½¦è¾†ä¸Šéƒ¨ç½²RLä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è½®èƒŽç‰¹æ€§ã€è·¯é¢çŠ¶å†µã€ç©ºæ°”åŠ¨åŠ›æ‰°åŠ¨å’Œè½¦è¾†è´Ÿè½½ç­‰å› ç´ ä½¿å¾—å‡†ç¡®å»ºæ¨¡çœŸå®žä¸–ç•ŒåŠ¨åŠ›å­¦å˜å¾—ä¸å¯è¡Œï¼Œè¿™é˜»ç¢äº†åœ¨ä»¿çœŸä¸­è®­ç»ƒçš„RLæ™ºèƒ½ä½“çš„ç›´æŽ¥è¿ç§»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æž¶ï¼Œé€šè¿‡è™šæ‹Ÿè½¦è¾†å’ŒçœŸå®žç³»ç»Ÿä¹‹é—´çš„ç©ºé—´å’Œæ—¶é—´å¯¹é½ç­–ç•¥ï¼Œå°†è¿åŠ¨è§„åˆ’ä¸Žè½¦è¾†æŽ§åˆ¶è§£è€¦ã€‚é¦–å…ˆï¼Œåœ¨ä»¿çœŸä¸­ä½¿ç”¨è¿åŠ¨å­¦è‡ªè¡Œè½¦æ¨¡åž‹è®­ç»ƒRLæ™ºèƒ½ä½“ä»¥è¾“å‡ºè¿žç»­æŽ§åˆ¶åŠ¨ä½œã€‚ç„¶åŽï¼Œå°†å…¶è¡Œä¸ºæç‚¼æˆè½¨è¿¹é¢„æµ‹æ™ºèƒ½ä½“ï¼Œç”Ÿæˆæœ‰é™èŒƒå›´çš„è‡ªè½¦è½¨è¿¹ï¼Œä»Žè€Œå®žçŽ°è™šæ‹Ÿè½¦è¾†å’ŒçœŸå®žè½¦è¾†ä¹‹é—´çš„åŒæ­¥ã€‚åœ¨éƒ¨ç½²æ—¶ï¼ŒStanleyæŽ§åˆ¶å™¨æŽ§åˆ¶æ¨ªå‘åŠ¨åŠ›å­¦ï¼Œè€Œçºµå‘å¯¹é½é€šè¿‡è‡ªé€‚åº”æ›´æ–°æœºåˆ¶æ¥ç»´æŒï¼Œä»¥è¡¥å¿è™šæ‹Ÿè½¨è¿¹å’ŒçœŸå®žè½¨è¿¹ä¹‹é—´çš„åå·®ã€‚æˆ‘ä»¬åœ¨çœŸå®žè½¦è¾†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå¹¶è¯æ˜Žæ‰€æå‡ºçš„å¯¹é½ç­–ç•¥èƒ½å¤Ÿå®žçŽ°åŸºäºŽRLçš„è¿åŠ¨è§„åˆ’ä»Žä»¿çœŸåˆ°çŽ°å®žçš„é²æ£’é›¶æ ·æœ¬è¿ç§»ï¼ŒæˆåŠŸåœ°å°†é«˜å±‚è½¨è¿¹ç”Ÿæˆä¸Žä½Žå±‚è½¦è¾†æŽ§åˆ¶è§£è€¦ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­ï¼Œç”±äºŽä»¿çœŸçŽ¯å¢ƒä¸ŽçœŸå®žçŽ¯å¢ƒå­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´åœ¨ä»¿çœŸçŽ¯å¢ƒä¸­è®­ç»ƒçš„RLæ™ºèƒ½ä½“æ— æ³•ç›´æŽ¥è¿ç§»åˆ°çœŸå®žè½¦è¾†ä¸Šçš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•éš¾ä»¥å‡†ç¡®å»ºæ¨¡çœŸå®žè½¦è¾†çš„å¤æ‚åŠ¨åŠ›å­¦ç‰¹æ€§ï¼Œä¾‹å¦‚è½®èƒŽç‰¹æ€§ã€è·¯é¢çŠ¶å†µç­‰ï¼Œè¿™ä½¿å¾—Sim-to-Realçš„è¿ç§»å˜å¾—å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è¿åŠ¨è§„åˆ’ä¸Žè½¦è¾†æŽ§åˆ¶è§£è€¦ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆåœ¨ä»¿çœŸçŽ¯å¢ƒä¸­ä½¿ç”¨RLè®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“è¾“å‡ºè¿žç»­çš„æŽ§åˆ¶åŠ¨ä½œã€‚ç„¶åŽï¼Œå°†è¯¥æ™ºèƒ½ä½“çš„è¡Œä¸ºæç‚¼æˆä¸€ä¸ªè½¨è¿¹é¢„æµ‹å™¨ï¼Œè¯¥é¢„æµ‹å™¨ç”Ÿæˆæœ‰é™èŒƒå›´å†…çš„è½¦è¾†è½¨è¿¹ã€‚é€šè¿‡å¯¹é½è™šæ‹Ÿè½¦è¾†å’ŒçœŸå®žè½¦è¾†çš„è½¨è¿¹ï¼Œå®žçŽ°Sim-to-Realçš„è¿ç§»ã€‚è¿™ç§è§£è€¦çš„æ–¹å¼é™ä½Žäº†å¯¹ç²¾ç¡®åŠ¨åŠ›å­¦æ¨¡åž‹çš„ä¾èµ–ï¼Œä»Žè€Œæé«˜äº†è¿ç§»çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åœ¨ä»¿çœŸçŽ¯å¢ƒä¸­ä½¿ç”¨RLè®­ç»ƒè½¨è¿¹ç”Ÿæˆå™¨ï¼›2) å°†è®­ç»ƒå¥½çš„è½¨è¿¹ç”Ÿæˆå™¨éƒ¨ç½²åˆ°çœŸå®žè½¦è¾†ä¸Šï¼›3) ä½¿ç”¨StanleyæŽ§åˆ¶å™¨æŽ§åˆ¶è½¦è¾†çš„æ¨ªå‘è¿åŠ¨ï¼›4) ä½¿ç”¨è‡ªé€‚åº”æ›´æ–°æœºåˆ¶æ¥å¯¹é½è™šæ‹Ÿè½¨è¿¹å’ŒçœŸå®žè½¨è¿¹ï¼Œè¡¥å¿ä¸¤è€…ä¹‹é—´çš„åå·®ã€‚è¯¥æ¡†æž¶å°†é«˜å±‚è½¨è¿¹ç”Ÿæˆä¸Žä½Žå±‚è½¦è¾†æŽ§åˆ¶åˆ†ç¦»ï¼Œç®€åŒ–äº†æŽ§åˆ¶ç­–ç•¥çš„å¤æ‚æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†åŠ¨åŠ›å­¦è§£è€¦çš„è½¨è¿¹å¯¹é½æ–¹æ³•ã€‚ä¸Žä¼ ç»Ÿçš„ç›´æŽ¥å°†RLæ™ºèƒ½ä½“éƒ¨ç½²åˆ°çœŸå®žè½¦è¾†ä¸Šçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡è½¨è¿¹å¯¹é½çš„æ–¹å¼ï¼Œå°†è¿åŠ¨è§„åˆ’ä¸Žè½¦è¾†æŽ§åˆ¶è§£è€¦ï¼Œé™ä½Žäº†å¯¹ç²¾ç¡®åŠ¨åŠ›å­¦æ¨¡åž‹çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è‡ªé€‚åº”æ›´æ–°æœºåˆ¶æ¥è¡¥å¿è™šæ‹Ÿè½¨è¿¹å’ŒçœŸå®žè½¨è¿¹ä¹‹é—´çš„åå·®ï¼Œè¿›ä¸€æ­¥æé«˜äº†è¿ç§»çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä»¿çœŸçŽ¯å¢ƒä¸­ï¼Œä½¿ç”¨è¿åŠ¨å­¦è‡ªè¡Œè½¦æ¨¡åž‹æ¥ç®€åŒ–è½¦è¾†åŠ¨åŠ›å­¦ã€‚RLæ™ºèƒ½ä½“ä½¿ç”¨è¿žç»­æŽ§åˆ¶åŠ¨ä½œä½œä¸ºè¾“å‡ºã€‚è½¨è¿¹é¢„æµ‹å™¨ç”Ÿæˆæœ‰é™èŒƒå›´çš„è‡ªè½¦è½¨è¿¹ã€‚åœ¨çœŸå®žè½¦è¾†ä¸Šï¼Œä½¿ç”¨StanleyæŽ§åˆ¶å™¨æŽ§åˆ¶æ¨ªå‘è¿åŠ¨ï¼Œè‡ªé€‚åº”æ›´æ–°æœºåˆ¶æ ¹æ®è™šæ‹Ÿè½¨è¿¹å’ŒçœŸå®žè½¨è¿¹ä¹‹é—´çš„åå·®æ¥è°ƒæ•´æŽ§åˆ¶å‚æ•°ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æž„ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œå±žäºŽæœªçŸ¥ä¿¡æ¯ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥è®ºæ–‡åœ¨çœŸå®žè½¦è¾†ä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œå®žçŽ°äº†åŸºäºŽRLçš„è¿åŠ¨è§„åˆ’ä»Žä»¿çœŸåˆ°çŽ°å®žçš„é›¶æ ·æœ¬è¿ç§»ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£è€¦é«˜å±‚è½¨è¿¹ç”Ÿæˆä¸Žä½Žå±‚è½¦è¾†æŽ§åˆ¶ï¼Œå¹¶èƒ½å¤Ÿé²æ£’åœ°åº”å¯¹çœŸå®žçŽ¯å¢ƒä¸­çš„å„ç§ä¸ç¡®å®šæ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œå±žäºŽæœªçŸ¥ä¿¡æ¯ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„è¿åŠ¨è§„åˆ’å’ŒæŽ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¿«é€Ÿéƒ¨ç½²å’Œé€‚åº”æ–°çŽ¯å¢ƒçš„åœºæ™¯ä¸­ã€‚é€šè¿‡Sim-to-Realè¿ç§»ï¼Œå¯ä»¥é™ä½Žå¼€å‘æˆæœ¬å’Œé£Žé™©ï¼ŒåŠ é€Ÿè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„è½åœ°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æœºå™¨äººé¢†åŸŸï¼Œä¾‹å¦‚æ— äººæœºå’Œç§»åŠ¨æœºå™¨äººã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.

