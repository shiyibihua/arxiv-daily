---
layout: default
title: Multi-Agent Reinforcement Learning for Deadlock Handling among Autonomous Mobile Robots
---

# Multi-Agent Reinforcement Learning for Deadlock Handling among Autonomous Mobile Robots

**arXiv**: [2511.07071v1](https://arxiv.org/abs/2511.07071) | [PDF](https://arxiv.org/pdf/2511.07071.pdf)

**ä½œè€…**: Marcel MÃ¼ller

**åˆ†ç±»**: cs.MA, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

**å¤‡æ³¨**: for associated repositories, see https://github.com/Nerozud/dl_reference_models and https://github.com/Nerozud/FTS_simpel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ­»é”å¤„ç†æ–¹æ³•ï¼Œæå‡AMRç‰©æµç³»ç»Ÿæ•ˆçŽ‡**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `è‡ªä¸»ç§»åŠ¨æœºå™¨äºº` `æ­»é”å¤„ç†` `å†…éƒ¨ç‰©æµ` `è·¯å¾„è§„åˆ’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰AMRç‰©æµç³»ç»Ÿæ­»é”å¤„ç†æ–¹æ³•ç¼ºä¹è‡ªé€‚åº”æ€§ï¼Œéš¾ä»¥åº”å¯¹åŠ¨æ€çŽ¯å¢ƒã€‚
2. æå‡ºåŸºäºŽMARLçš„æ­»é”å¤„ç†æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ ä¼˜åŒ–AMRçš„è·¯å¾„è§„åˆ’å’Œè¡Œä¸ºç­–ç•¥ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œåœ¨å¤æ‚çŽ¯å¢ƒä¸­ï¼ŒMARLæ–¹æ³•ä¼˜äºŽä¼ ç»Ÿè§„åˆ™æ–¹æ³•ï¼Œæå‡äº†ç³»ç»Ÿæ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬è®ºæ–‡æŽ¢è®¨äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è‡ªä¸»ç§»åŠ¨æœºå™¨äººï¼ˆAMRï¼‰å†…éƒ¨ç‰©æµç³»ç»Ÿä¸­å¤„ç†æ­»é”çš„åº”ç”¨ã€‚AMRæé«˜äº†è¿è¥çµæ´»æ€§ï¼Œä½†ä¹Ÿå¢žåŠ äº†æ­»é”çš„é£Žé™©ï¼Œä»Žè€Œé™ä½Žäº†ç³»ç»Ÿåžåé‡å’Œå¯é æ€§ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸å¿½ç•¥è§„åˆ’é˜¶æ®µçš„æ­»é”å¤„ç†ï¼Œå¹¶ä¾èµ–äºŽæ— æ³•é€‚åº”åŠ¨æ€è¿è¥æ¡ä»¶çš„åˆšæ€§æŽ§åˆ¶è§„åˆ™ã€‚ä¸ºäº†è§£å†³è¿™äº›ç¼ºç‚¹ï¼Œæœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§å°†MARLé›†æˆåˆ°ç‰©æµè§„åˆ’å’Œè¿è¥æŽ§åˆ¶ä¸­çš„ç»“æž„åŒ–æ–¹æ³•ã€‚å®ƒå¼•å…¥äº†æ˜¾å¼è€ƒè™‘æ­»é”çš„å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ï¼ˆMAPFï¼‰å‚è€ƒæ¨¡åž‹ï¼Œä»Žè€Œèƒ½å¤Ÿç³»ç»Ÿåœ°è¯„ä¼°MARLç­–ç•¥ã€‚é€šè¿‡åŸºäºŽç½‘æ ¼çš„çŽ¯å¢ƒå’Œå¤–éƒ¨ä»¿çœŸè½¯ä»¶ï¼Œè¯¥ç ”ç©¶æ¯”è¾ƒäº†ä¼ ç»Ÿçš„æ­»é”å¤„ç†ç­–ç•¥ä¸ŽåŸºäºŽMARLçš„è§£å†³æ–¹æ¡ˆï¼Œé‡ç‚¹å…³æ³¨ä¸åŒè®­ç»ƒå’Œæ‰§è¡Œæ¨¡å¼ä¸‹çš„PPOå’ŒIMPALAç®—æ³•ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼ŒåŸºäºŽMARLçš„ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯ä¸Žé›†ä¸­å¼è®­ç»ƒå’Œåˆ†æ•£å¼æ‰§è¡Œï¼ˆCTDEï¼‰ç›¸ç»“åˆæ—¶ï¼Œåœ¨å¤æ‚çš„æ‹¥å µçŽ¯å¢ƒä¸­ä¼˜äºŽåŸºäºŽè§„åˆ™çš„æ–¹æ³•ã€‚åœ¨æ›´ç®€å•çš„çŽ¯å¢ƒæˆ–å…·æœ‰å……è¶³ç©ºé—´è‡ªç”±åº¦çš„çŽ¯å¢ƒä¸­ï¼ŒåŸºäºŽè§„åˆ™çš„æ–¹æ³•ç”±äºŽå…¶è¾ƒä½Žçš„è®¡ç®—éœ€æ±‚è€Œä»ç„¶å…·æœ‰ç«žäº‰åŠ›ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒMARLä¸ºåŠ¨æ€å†…éƒ¨ç‰©æµåœºæ™¯ä¸­çš„æ­»é”å¤„ç†æä¾›äº†ä¸€ç§çµæ´»ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä½†éœ€è¦æ ¹æ®è¿è¥çŽ¯å¢ƒè¿›è¡Œä»”ç»†è°ƒæ•´ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å†…éƒ¨ç‰©æµç³»ç»Ÿä¸­ï¼Œç”±äºŽè‡ªä¸»ç§»åŠ¨æœºå™¨äººï¼ˆAMRï¼‰æ•°é‡å¢žåŠ å’ŒçŽ¯å¢ƒå¤æ‚æ€§æå‡å¯¼è‡´çš„æ­»é”é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œå¦‚é¢„å…ˆè®¾å®šçš„è§„åˆ™æˆ–é›†ä¸­å¼è·¯å¾„è§„åˆ’ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„è¿è¥çŽ¯å¢ƒï¼Œå¯¼è‡´ç³»ç»Ÿæ•ˆçŽ‡é™ä½Žå’Œå¯é æ€§ä¸‹é™ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ç¼ºä¹å­¦ä¹ å’Œè‡ªé€‚åº”èƒ½åŠ›ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†çªå‘æƒ…å†µã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¥è®­ç»ƒAMRï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªä¸»å­¦ä¹ é¿å…æ­»é”çš„ç­–ç•¥ã€‚é€šè¿‡å°†æ­»é”å¤„ç†é—®é¢˜å»ºæ¨¡ä¸ºMARLé—®é¢˜ï¼Œæ¯ä¸ªAMRä½œä¸ºä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œé€šè¿‡ä¸ŽçŽ¯å¢ƒå’Œå…¶ä»–æ™ºèƒ½ä½“çš„äº¤äº’ï¼Œå­¦ä¹ æœ€ä¼˜çš„è·¯å¾„è§„åˆ’å’Œè¡Œä¸ºç­–ç•¥ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æé«˜ç³»ç»Ÿçš„çµæ´»æ€§ã€å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè®ºæ–‡æž„å»ºäº†ä¸€ä¸ªåŸºäºŽç½‘æ ¼çš„ä»¿çœŸçŽ¯å¢ƒï¼Œç”¨äºŽè®­ç»ƒå’Œè¯„ä¼°MARLç®—æ³•ã€‚æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) çŽ¯å¢ƒå»ºæ¨¡ï¼šå°†ç‰©æµç³»ç»ŸæŠ½è±¡ä¸ºç½‘æ ¼çŽ¯å¢ƒï¼Œå®šä¹‰AMRçš„è¿åŠ¨è§„åˆ™å’Œäº¤äº’æ–¹å¼ã€‚2) æ™ºèƒ½ä½“è®¾è®¡ï¼šæ¯ä¸ªAMRé…å¤‡ä¸€ä¸ªMARLä»£ç†ï¼Œè´Ÿè´£å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚3) è®­ç»ƒæ¨¡å—ï¼šä½¿ç”¨PPOæˆ–IMPALAç­‰MARLç®—æ³•ï¼Œé€šè¿‡ä¸ŽçŽ¯å¢ƒäº¤äº’ï¼Œä¸æ–­ä¼˜åŒ–æ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚4) è¯„ä¼°æ¨¡å—ï¼šåœ¨ä¸åŒçš„åœºæ™¯ä¸‹è¯„ä¼°MARLç­–ç•¥çš„æ€§èƒ½ï¼Œå¹¶ä¸Žä¼ ç»Ÿæ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽå°†MARLåº”ç”¨äºŽAMRçš„æ­»é”å¤„ç†é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»“æž„åŒ–çš„æ–¹æ³•ï¼Œå°†MARLé›†æˆåˆ°ç‰©æµè§„åˆ’å’Œè¿è¥æŽ§åˆ¶ä¸­ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†æ˜¾å¼è€ƒè™‘æ­»é”çš„å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ï¼ˆMAPFï¼‰å‚è€ƒæ¨¡åž‹ï¼Œç”¨äºŽç³»ç»Ÿåœ°è¯„ä¼°MARLç­–ç•¥ã€‚é›†ä¸­å¼è®­ç»ƒå’Œåˆ†æ•£å¼æ‰§è¡Œï¼ˆCTDEï¼‰çš„ç­–ç•¥ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ï¼Œå®ƒå…è®¸æ™ºèƒ½ä½“åœ¨è®­ç»ƒé˜¶æ®µå…±äº«ä¿¡æ¯ï¼Œä½†åœ¨æ‰§è¡Œé˜¶æ®µç‹¬ç«‹å†³ç­–ï¼Œä»Žè€Œæé«˜äº†ç³»ç»Ÿçš„é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†PPOå’ŒIMPALAä¸¤ç§MARLç®—æ³•ï¼Œå¹¶é’ˆå¯¹æ­»é”å¤„ç†é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¥–åŠ±å‡½æ•°è®¾è®¡ï¼šå¥–åŠ±å‡½æ•°æ—¨åœ¨é¼“åŠ±AMRå®Œæˆä»»åŠ¡ï¼ŒåŒæ—¶é¿å…æ­»é”ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥è®¾ç½®è´Ÿå¥–åŠ±æ¥æƒ©ç½šç¢°æ’žæˆ–é•¿æ—¶é—´åœæ»žã€‚2) çŠ¶æ€è¡¨ç¤ºï¼šçŠ¶æ€è¡¨ç¤ºéœ€è¦åŒ…å«AMRè‡ªèº«çš„ä¿¡æ¯ï¼ˆå¦‚ä½ç½®ã€é€Ÿåº¦ï¼‰ä»¥åŠå‘¨å›´çŽ¯å¢ƒçš„ä¿¡æ¯ï¼ˆå¦‚å…¶ä»–AMRçš„ä½ç½®ã€éšœç¢ç‰©ï¼‰ã€‚3) ç½‘ç»œç»“æž„ï¼šå¯ä»¥ä½¿ç”¨å·ç§¯ç¥žç»ç½‘ç»œï¼ˆCNNï¼‰æ¥å¤„ç†ç½‘æ ¼çŽ¯å¢ƒä¸­çš„çŠ¶æ€ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨å¾ªçŽ¯ç¥žç»ç½‘ç»œï¼ˆRNNï¼‰æ¥å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ã€‚4) è¶…å‚æ•°è°ƒæ•´ï¼šéœ€è¦ä»”ç»†è°ƒæ•´PPOå’ŒIMPALAçš„è¶…å‚æ•°ï¼Œä»¥èŽ·å¾—æœ€ä½³çš„è®­ç»ƒæ•ˆæžœã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œåœ¨å¤æ‚çš„æ‹¥å µçŽ¯å¢ƒä¸­ï¼ŒåŸºäºŽMARLçš„ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯ä¸Žé›†ä¸­å¼è®­ç»ƒå’Œåˆ†æ•£å¼æ‰§è¡Œï¼ˆCTDEï¼‰ç›¸ç»“åˆæ—¶ï¼Œä¼˜äºŽä¼ ç»Ÿçš„åŸºäºŽè§„åˆ™çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒMARLæ–¹æ³•åœ¨åžåé‡å’Œæ­»é”é¿å…çŽ‡æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚åœ¨æ›´ç®€å•çš„çŽ¯å¢ƒæˆ–å…·æœ‰å……è¶³ç©ºé—´è‡ªç”±åº¦çš„çŽ¯å¢ƒä¸­ï¼ŒåŸºäºŽè§„åˆ™çš„æ–¹æ³•ç”±äºŽå…¶è¾ƒä½Žçš„è®¡ç®—éœ€æ±‚è€Œä»ç„¶å…·æœ‰ç«žäº‰åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§å†…éƒ¨ç‰©æµåœºæ™¯ï¼Œå¦‚ä»“åº“ã€å·¥åŽ‚ã€åŒ»é™¢ç­‰ï¼Œä»¥æé«˜AMRç³»ç»Ÿçš„æ•ˆçŽ‡å’Œå¯é æ€§ã€‚é€šè¿‡MARLï¼ŒAMRèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”åŠ¨æ€å˜åŒ–çš„è¿è¥çŽ¯å¢ƒï¼Œå‡å°‘æ­»é”çš„å‘ç”Ÿï¼Œä»Žè€Œæé«˜æ•´ä½“çš„ç‰©æµæ•ˆçŽ‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¦‚äº¤é€šæŽ§åˆ¶ã€æœºå™¨äººåä½œç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This dissertation explores the application of multi-agent reinforcement learning (MARL) for handling deadlocks in intralogistics systems that rely on autonomous mobile robots (AMRs). AMRs enhance operational flexibility but also increase the risk of deadlocks, which degrade system throughput and reliability. Existing approaches often neglect deadlock handling in the planning phase and rely on rigid control rules that cannot adapt to dynamic operational conditions.
>   To address these shortcomings, this work develops a structured methodology for integrating MARL into logistics planning and operational control. It introduces reference models that explicitly consider deadlock-capable multi-agent pathfinding (MAPF) problems, enabling systematic evaluation of MARL strategies. Using grid-based environments and an external simulation software, the study compares traditional deadlock handling strategies with MARL-based solutions, focusing on PPO and IMPALA algorithms under different training and execution modes.
>   Findings reveal that MARL-based strategies, particularly when combined with centralized training and decentralized execution (CTDE), outperform rule-based methods in complex, congested environments. In simpler environments or those with ample spatial freedom, rule-based methods remain competitive due to their lower computational demands. These results highlight that MARL provides a flexible and scalable solution for deadlock handling in dynamic intralogistics scenarios, but requires careful tailoring to the operational context.

