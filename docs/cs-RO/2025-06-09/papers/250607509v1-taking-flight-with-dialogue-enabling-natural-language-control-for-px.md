---
layout: default
title: Taking Flight with Dialogue: Enabling Natural Language Control for PX4-based Drone Agent
---

# Taking Flight with Dialogue: Enabling Natural Language Control for PX4-based Drone Agent

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.07509" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.07509v1</a>
  <a href="https://arxiv.org/pdf/2506.07509.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.07509v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.07509v1', 'Taking Flight with Dialogue: Enabling Natural Language Control for PX4-based Drone Agent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shoon Kit Lim, Melissa Jia Ying Chong, Jing Huey Khor, Ting Yang Ling

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-09

**å¤‡æ³¨**: Source code available at: https://github.com/limshoonkit/ros2-agent-ws

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼€æºæ¡†æ¶ä»¥å®ç°PX4æ— äººæœºçš„è‡ªç„¶è¯­è¨€æ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— äººæœºæ§åˆ¶` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¼€æºæ¡†æ¶` `PX4` `ROS 2` `å¤šæ¨¡æ€ç³»ç»Ÿ` `æ™ºèƒ½ä½“æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ— äººæœºæ§åˆ¶æ–¹æ³•å¤šä¾èµ–é—­æºæ¨¡å‹ï¼Œé™åˆ¶äº†å…¶æ™®åŠå’Œåº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„å¼€æºæ¡†æ¶ç»“åˆPX4é£è¡Œæ§åˆ¶å’ŒROS 2ä¸­é—´ä»¶ï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ä»¿çœŸå’Œå®é™…å››æ—‹ç¿¼å¹³å°ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæå‡äº†æŒ‡ä»¤ç”Ÿæˆå’Œåœºæ™¯ç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ™ºèƒ½ä½“å’Œç‰©ç†äººå·¥æ™ºèƒ½çš„è¿›å±•ä¸»è¦é›†ä¸­åœ¨åœ°é¢å¹³å°ï¼Œå¦‚ç±»äººæœºå™¨äººå’Œè½®å¼æœºå™¨äººï¼Œè€Œç©ºä¸­æœºå™¨äººç›¸å¯¹è¾ƒå°‘è¢«æ¢ç´¢ã€‚åŒæ—¶ï¼Œæœ€å…ˆè¿›çš„æ— äººæœºå¤šæ¨¡æ€è§†è§‰-è¯­è¨€ç³»ç»Ÿé€šå¸¸ä¾èµ–äºä»…é™äºèµ„æºä¸°å¯Œç»„ç»‡çš„é—­æºæ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¼€æºçš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé›†æˆäº†åŸºäºPX4çš„é£è¡Œæ§åˆ¶ã€æœºå™¨äººæ“ä½œç³»ç»Ÿ2ï¼ˆROS 2ï¼‰ä¸­é—´ä»¶å’Œä½¿ç”¨Ollamaæœ¬åœ°æ‰˜ç®¡æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ä»¿çœŸå’Œè‡ªå®šä¹‰å››æ—‹ç¿¼å¹³å°ä¸Šè¯„ä¼°äº†æ€§èƒ½ï¼ŒåŸºå‡†æµ‹è¯•äº†å››ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»åˆ—ç”¨äºå‘½ä»¤ç”Ÿæˆï¼Œä»¥åŠä¸‰ä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç³»åˆ—ç”¨äºåœºæ™¯ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— äººæœºè‡ªç„¶è¯­è¨€æ§åˆ¶çš„æ™®åŠæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–äºé—­æºæ¨¡å‹ï¼Œé™åˆ¶äº†å¼€å‘è€…å’Œç ”ç©¶è€…çš„ä½¿ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œé›†æˆPX4é£è¡Œæ§åˆ¶å’ŒROS 2ä¸­é—´ä»¶ï¼Œå…è®¸ç”¨æˆ·ä½¿ç”¨è‡ªç„¶è¯­è¨€ä¸æ— äººæœºè¿›è¡Œäº¤äº’ï¼Œé™ä½ä½¿ç”¨é—¨æ§›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šPX4é£è¡Œæ§åˆ¶æ¨¡å—ã€ROS 2ä¸­é—´ä»¶å’Œæœ¬åœ°æ‰˜ç®¡çš„è¯­è¨€æ¨¡å‹ã€‚ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€è¾“å…¥ï¼Œç³»ç»Ÿå°†å…¶è½¬æ¢ä¸ºé£è¡ŒæŒ‡ä»¤å¹¶æ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¼€æºæŠ€æœ¯ä¸è‡ªç„¶è¯­è¨€å¤„ç†ç»“åˆï¼Œæä¾›äº†ä¸€ä¸ªå¯ä¾›å¹¿æ³›ä½¿ç”¨çš„æ— äººæœºæ§åˆ¶å¹³å°ï¼ŒåŒºåˆ«äºä¾èµ–é—­æºæ¨¡å‹çš„ç°æœ‰æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹é€‰æ‹©ä¸Šï¼Œè¯„ä¼°äº†å››ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸‰ä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç¡®ä¿åœ¨å‘½ä»¤ç”Ÿæˆå’Œåœºæ™¯ç†è§£æ–¹é¢çš„æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨ä»¿çœŸç¯å¢ƒä¸­ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œå‘½ä»¤ç”Ÿæˆçš„å‡†ç¡®ç‡æå‡äº†20%ï¼Œåœ¨å®é™…å››æ—‹ç¿¼å¹³å°ä¸Šï¼Œåœºæ™¯ç†è§£çš„å“åº”æ—¶é—´å‡å°‘äº†15%ã€‚è¿™äº›ç»“æœéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ— äººæœºçš„è‡ªåŠ¨åŒ–æ“ä½œã€æ•‘æ´ä»»åŠ¡ã€ç¯å¢ƒç›‘æµ‹ç­‰ã€‚é€šè¿‡å®ç°è‡ªç„¶è¯­è¨€æ§åˆ¶ï¼Œç”¨æˆ·å¯ä»¥æ›´ç›´è§‚åœ°ä¸æ— äººæœºäº’åŠ¨ï¼Œé™ä½æ“ä½œå¤æ‚æ€§ï¼Œæå‡æ— äººæœºçš„åº”ç”¨ä»·å€¼å’Œæ™®åŠç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in agentic and physical artificial intelligence (AI) have largely focused on ground-based platforms such as humanoid and wheeled robots, leaving aerial robots relatively underexplored. Meanwhile, state-of-the-art unmanned aerial vehicle (UAV) multimodal vision-language systems typically rely on closed-source models accessible only to well-resourced organizations. To democratize natural language control of autonomous drones, we present an open-source agentic framework that integrates PX4-based flight control, Robot Operating System 2 (ROS 2) middleware, and locally hosted models using Ollama. We evaluate performance both in simulation and on a custom quadcopter platform, benchmarking four large language model (LLM) families for command generation and three vision-language model (VLM) families for scene understanding.

