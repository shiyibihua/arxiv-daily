---
layout: default
title: BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models
---

# BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.07961" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.07961v2</a>
  <a href="https://arxiv.org/pdf/2506.07961.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.07961v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.07961v2', 'BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Peiyan Li, Yixiang Chen, Hongtao Wu, Xiao Ma, Xiangnan Wu, Yan Huang, Liang Wang, Tao Kong, Tieniu Tan

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-09 (Êõ¥Êñ∞: 2025-10-14)

**Â§áÊ≥®**: NeurIPS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫BridgeVLA‰ª•Ëß£ÂÜ≥3DÊìçÊéßÂ≠¶‰π†‰∏≠ÁöÑ‰ΩéÊ†∑Êú¨ÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `3DÊìçÊéßÂ≠¶‰π†` `Âä®‰ΩúÈ¢ÑÊµã` `Ê†∑Êú¨ÊïàÁéá` `Êú∫Âô®‰∫∫ÊäÄÊúØ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®3D‰ø°Âè∑ÁöÑÂà©Áî®‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÂØºËá¥Âä®‰ΩúÈ¢ÑÊµãÁöÑÊ†∑Êú¨ÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. BridgeVLAÈÄöËøáÂ∞Ü3DËæìÂÖ•ÊäïÂΩ±Âà∞2DÂõæÂÉèÂπ∂‰ΩøÁî®2DÁÉ≠ÂõæËøõË°åÂä®‰ΩúÈ¢ÑÊµãÔºåËß£ÂÜ≥‰∫ÜËæìÂÖ•ËæìÂá∫ÂØπÈΩêÁöÑÈóÆÈ¢ò„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåBridgeVLAÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊàêÂäüÁéáÊòæËëóÊèêÈ´òÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂ÂçìË∂äÁöÑÊ†∑Êú¨ÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÊûÑÂª∫ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÊàê‰∏∫ÊúâÊïàÊú∫Âô®‰∫∫ÊìçÊéßÂ≠¶‰π†ÁöÑ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Âä®‰ΩúÈ¢ÑÊµã‰∏≠ÂæàÂ∞ëÂ∞Ü3D‰ø°Âè∑Á∫≥ÂÖ•VLMsÔºåÂπ∂Êú™ÂÖÖÂàÜÂà©Áî®3DÊï∞ÊçÆÂõ∫ÊúâÁöÑÁ©∫Èó¥ÁªìÊûÑÔºåÂØºËá¥Ê†∑Êú¨ÊïàÁéá‰Ωé‰∏ã„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜBridgeVLAÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑ3D VLAÊ®°ÂûãÔºåÈÄöËøáÂ∞Ü3DËæìÂÖ•ÊäïÂΩ±Âà∞Â§ö‰∏™2DÂõæÂÉèÔºåÁ°Æ‰øù‰∏éVLM‰∏ªÂπ≤ÁöÑËæìÂÖ•ÂØπÈΩêÔºåÂπ∂Âà©Áî®2DÁÉ≠ÂõæËøõË°åÂä®‰ΩúÈ¢ÑÊµãÔºå‰ªéËÄåÂú®‰∏ÄËá¥ÁöÑ2DÂõæÂÉèÁ©∫Èó¥ÂÜÖÁªü‰∏ÄËæìÂÖ•ÂíåËæìÂá∫„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÈ´òÊïàÊúâÊïàÂú∞Â≠¶‰π†3DÊìçÊéßÔºåÂπ∂Âú®‰∏â‰∏™Ê®°ÊãüÂü∫ÂáÜ‰∏äË∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÂü∫Á∫øÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®3DÊìçÊéßÂ≠¶‰π†‰∏≠‰ΩéÊ†∑Êú¨ÊïàÁéáÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÂÖÖÂàÜÂà©Áî®3DÊï∞ÊçÆÁöÑÁ©∫Èó¥ÁªìÊûÑÔºåÂØºËá¥Âä®‰ΩúÈ¢ÑÊµãÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöBridgeVLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞Ü3DËæìÂÖ•ÊäïÂΩ±Âà∞Â§ö‰∏™2DÂõæÂÉè‰∏≠Ôºå‰ª•Á°Æ‰øù‰∏éVLM‰∏ªÂπ≤ÁöÑËæìÂÖ•ÂØπÈΩêÔºåÂπ∂ÈÄöËøá2DÁÉ≠ÂõæËøõË°åÂä®‰ΩúÈ¢ÑÊµãÔºå‰ªéËÄåÁªü‰∏ÄËæìÂÖ•ÂíåËæìÂá∫Á©∫Èó¥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöBridgeVLAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö3DËæìÂÖ•ÊäïÂΩ±Ê®°Âùó„ÄÅ2DÁÉ≠ÂõæÁîüÊàêÊ®°ÂùóÂíå‰∏ãÊ∏∏Á≠ñÁï•Â≠¶‰π†Ê®°Âùó„ÄÇÈ¶ñÂÖàÔºå3DËæìÂÖ•Ë¢´ËΩ¨Êç¢‰∏∫Â§ö‰∏™2DÂõæÂÉèÔºåÁÑ∂ÂêéÁîüÊàêÁõ∏Â∫îÁöÑ2DÁÉ≠ÂõæÔºåÊúÄÂêéËøõË°åÁ≠ñÁï•Â≠¶‰π†‰ª•ÂÆûÁé∞Âä®‰ΩúÈ¢ÑÊµã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöBridgeVLAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ËæìÂÖ•ËæìÂá∫ÁöÑÂØπÈΩêÊú∫Âà∂ÔºåÈÄöËøáÂ∞Ü3DÊï∞ÊçÆÊúâÊïàÂú∞Êò†Â∞ÑÂà∞2DÁ©∫Èó¥ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéáÂíåÂ≠¶‰π†ÊïàÊûú„ÄÇËøô‰∏ÄËÆæËÆ°‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÊõ¥Â•ΩÂú∞Âà©Áî®‰∫Ü2DÂõæÂÉèÁöÑÁâπÂæÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÖ≥ÈîÆËÆæËÆ°‰∏äÔºåBridgeVLAÈááÁî®‰∫ÜÂèØÊâ©Â±ïÁöÑÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÔºå‰ΩøVLM‰∏ªÂπ≤ÂÖ∑Â§áÈ¢ÑÊµã2DÁÉ≠ÂõæÁöÑËÉΩÂäõÔºåÊ≠§Â§ñÔºåÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüÈíàÂØπ2DÁÉ≠ÂõæÁöÑÂáÜÁ°ÆÊÄßËøõË°å‰∫Ü‰ºòÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂÆûÈ™å‰∏≠ÔºåBridgeVLAÂú®RLBench‰∏≠Â∞ÜÂπ≥ÂùáÊàêÂäüÁéá‰ªé81.4%ÊèêÂçáËá≥88.2%ÔºåÂú®COLOSSEUM‰∏≠‰ªé56.7%ÊèêÂçáËá≥64.0%„ÄÇÂú®GemBench‰∏≠ÔºåBridgeVLAË∂ÖË∂ä‰∫ÜÊâÄÊúâÂØπÊØîÂü∫Á∫øÊñπÊ≥ï„ÄÇÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠ÔºåBridgeVLAÁöÑË°®Áé∞ÊØîÊúÄÂÖàËøõÁöÑÂü∫Á∫øÊñπÊ≥ïÂπ≥ÂùáÊèêÈ´ò32%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

BridgeVLAÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Êú∫Âô®‰∫∫ÊìçÊéß„ÄÅÊô∫ËÉΩÂà∂ÈÄ†Âíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´ò3DÊìçÊéßÂ≠¶‰π†ÁöÑÊïàÁéáÔºåËØ•Ê®°ÂûãËÉΩÂ§üÂ∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊâßË°åÂ§çÊùÇÁöÑ‰ªªÂä°ÔºåËøõËÄåÊé®Âä®Êô∫ËÉΩÊú∫Âô®‰∫∫ÊäÄÊúØÁöÑËøõÊ≠•‰∏éÊôÆÂèä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/

