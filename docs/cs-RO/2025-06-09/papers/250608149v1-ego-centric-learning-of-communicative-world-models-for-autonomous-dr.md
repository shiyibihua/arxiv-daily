---
layout: default
title: Ego-centric Learning of Communicative World Models for Autonomous Driving
---

# Ego-centric Learning of Communicative World Models for Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.08149" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.08149v1</a>
  <a href="https://arxiv.org/pdf/2506.08149.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.08149v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.08149v1', 'Ego-centric Learning of Communicative World Models for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hang Wang, Dechen Gao, Junshan Zhang

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCALLä»¥è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¿¡æ¯å…±äº«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨é©¾é©¶` `ä¿¡æ¯å…±äº«` `ç”Ÿæˆå¼AI` `ä¸–ç•Œæ¨¡å‹` `è½¨è¿¹è§„åˆ’` `æ½œåœ¨è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­é¢ä¸´éƒ¨åˆ†å¯è§‚æµ‹æ€§å’Œéå¹³ç¨³æ€§çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºCALLï¼Œé€šè¿‡ç”Ÿæˆå¼AIå’Œä¸–ç•Œæ¨¡å‹çš„æ½œåœ¨è¡¨ç¤ºï¼Œä¼˜åŒ–ä¿¡æ¯å…±äº«ï¼Œé™ä½é€šä¿¡å¼€é”€ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚
3. åœ¨CARLAå¹³å°ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒCALLæ˜¾è‘—æå‡äº†è½¨è¿¹è§„åˆ’ä»»åŠ¡çš„é¢„æµ‹å‡†ç¡®æ€§å’Œæ•´ä½“æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å¤æ‚é«˜ç»´ç¯å¢ƒï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶ï¼‰ä¸­è¿›è¡Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ—¶é¢ä¸´çš„éƒ¨åˆ†å¯è§‚æµ‹æ€§å’Œéå¹³ç¨³æ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé€šå¸¸é‡‡ç”¨ä¿¡æ¯å…±äº«ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´é€šä¿¡å¼€é”€å’Œå¯æ‰©å±•æ€§ç­‰é‡å¤§éšœç¢ã€‚æˆ‘ä»¬æå‡ºäº†CALLï¼ˆCommunicative World Modelï¼‰ï¼Œé€šè¿‡ç”Ÿæˆå¼AIä¸ä¸–ç•Œæ¨¡å‹çš„æ½œåœ¨è¡¨ç¤ºç›¸ç»“åˆï¼Œä½¿æ¯ä¸ªæ™ºèƒ½ä½“é¦–å…ˆå­¦ä¹ å…¶ä¸–ç•Œæ¨¡å‹ï¼Œå°†çŠ¶æ€å’Œæ„å›¾ç¼–ç ä¸ºä½ç»´æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶é€šè¿‡è½»é‡çº§é€šä¿¡ä¸å…¶ä»–æ™ºèƒ½ä½“å…±äº«ã€‚æ­¤å¤–ï¼Œæ™ºèƒ½ä½“åœ¨è‡ªæˆ‘ä¸­å¿ƒå­¦ä¹ çš„åŒæ—¶åˆ©ç”¨è½»é‡çº§ä¿¡æ¯å…±äº«æ¥ä¸°å¯Œå…¶ä¸–ç•Œæ¨¡å‹ï¼Œä»è€Œæé«˜é¢„æµ‹èƒ½åŠ›ä»¥æ”¹å–„è§„åˆ’ã€‚æˆ‘ä»¬åœ¨CARLAå¹³å°ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†CALLåœ¨å±€éƒ¨è½¨è¿¹è§„åˆ’ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„éƒ¨åˆ†å¯è§‚æµ‹æ€§å’Œéå¹³ç¨³æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¿¡æ¯å…±äº«æ—¶é¢ä¸´é€šä¿¡å¼€é”€å¤§å’Œå¯æ‰©å±•æ€§å·®çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„CALLæ–¹æ³•é€šè¿‡ç”Ÿæˆå¼AIå’Œä¸–ç•Œæ¨¡å‹çš„æ½œåœ¨è¡¨ç¤ºï¼Œå…è®¸æ™ºèƒ½ä½“åœ¨ä½ç»´ç©ºé—´ä¸­å…±äº«ä¿¡æ¯ï¼Œä»è€Œå‡å°‘é€šä¿¡è´Ÿæ‹…å¹¶æé«˜å­¦ä¹ æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCALLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œæ™ºèƒ½ä½“å­¦ä¹ å…¶ä¸–ç•Œæ¨¡å‹ï¼Œå°†çŠ¶æ€å’Œæ„å›¾ç¼–ç ä¸ºä½ç»´æ½œåœ¨è¡¨ç¤ºï¼›å…¶æ¬¡ï¼Œæ™ºèƒ½ä½“åœ¨è‡ªæˆ‘ä¸­å¿ƒå­¦ä¹ çš„è¿‡ç¨‹ä¸­åˆ©ç”¨è½»é‡çº§çš„ä¿¡æ¯å…±äº«æ¥ä¸°å¯Œå…¶ä¸–ç•Œæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCALLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡ä½ç»´æ½œåœ¨è¡¨ç¤ºå®ç°é«˜æ•ˆçš„ä¿¡æ¯å…±äº«ï¼Œæ˜¾è‘—é™ä½äº†é€šä¿¡å¼€é”€ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæå‡äº†æ™ºèƒ½ä½“çš„å­¦ä¹ èƒ½åŠ›å’Œé¢„æµ‹å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è½»é‡çº§çš„é€šä¿¡åè®®ï¼Œç¡®ä¿ä¿¡æ¯å…±äº«çš„é«˜æ•ˆæ€§ï¼›åŒæ—¶ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆçš„è§„åˆ’ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨CALLæ–¹æ³•çš„æ™ºèƒ½ä½“åœ¨CARLAå¹³å°ä¸Šçš„å±€éƒ¨è½¨è¿¹è§„åˆ’ä»»åŠ¡ä¸­ï¼Œé¢„æµ‹å‡†ç¡®æ€§æé«˜äº†20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ•´ä½“æ€§èƒ½æå‡æ˜¾è‘—ï¼ŒéªŒè¯äº†ä¿¡æ¯å…±äº«çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œå¤šæ™ºèƒ½ä½“åä½œä»»åŠ¡ç­‰ã€‚é€šè¿‡æé«˜æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å­¦ä¹ å’Œå†³ç­–èƒ½åŠ›ï¼ŒCALLèƒ½å¤Ÿä¸ºæœªæ¥çš„æ™ºèƒ½äº¤é€šè§£å†³æ–¹æ¡ˆæä¾›é‡è¦æ”¯æŒï¼Œæ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study multi-agent reinforcement learning (MARL) for tasks in complex high-dimensional environments, such as autonomous driving. MARL is known to suffer from the \textit{partial observability} and \textit{non-stationarity} issues. To tackle these challenges, information sharing is often employed, which however faces major hurdles in practice, including overwhelming communication overhead and scalability concerns. By making use of generative AI embodied in world model together with its latent representation, we develop {\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d Mode\underline{l}, for MARL, where 1) each agent first learns its world model that encodes its state and intention into low-dimensional latent representation with smaller memory footprint, which can be shared with other agents of interest via lightweight communication; and 2) each agent carries out ego-centric learning while exploiting lightweight information sharing to enrich her world model, and then exploits its generalization capacity to improve prediction for better planning. We characterize the gain on the prediction accuracy from the information sharing and its impact on performance gap. Extensive experiments are carried out on the challenging local trajectory planning tasks in the CARLA platform to demonstrate the performance gains of using \textit{CALL}.

