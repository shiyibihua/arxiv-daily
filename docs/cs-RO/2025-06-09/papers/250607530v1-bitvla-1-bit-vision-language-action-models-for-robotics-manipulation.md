---
layout: default
title: BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation
---

# BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.07530" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.07530v1</a>
  <a href="https://arxiv.org/pdf/2506.07530.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.07530v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.07530v1', 'BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hongyu Wang, Chuyan Xiong, Ruiping Wang, Xilin Chen

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-09

**Â§áÊ≥®**: Work in progress

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/ustcwhy/BitVLA)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫BitVLA‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÊ®°ÂûãÈÉ®ÁΩ≤ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `Êú∫Âô®‰∫∫Êìç‰Ωú` `1‰ΩçÊ®°Âûã` `Ëí∏È¶èËÆ≠ÁªÉ` `ÂÜÖÂ≠ò‰ºòÂåñ` `ËæπÁºòËÆ°ÁÆó` `Ê®°ÂûãÂéãÁº©`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®ËßÑÊ®°‰∏çÊñ≠Â¢ûÂ§ßÁöÑÊÉÖÂÜµ‰∏ãÔºåÈöæ‰ª•Âú®ËµÑÊ∫êÂèóÈôêÁöÑÊú∫Âô®‰∫∫Á≥ªÁªü‰∏äËøõË°åÊúâÊïàÈÉ®ÁΩ≤„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑBitVLAÊ®°ÂûãÈÄöËøáÂ∞ÜÂèÇÊï∞ÂéãÁº©‰∏∫‰∏âÂÖÉÁªÑÂΩ¢ÂºèÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÂç†Áî®ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ®°ÂûãÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBitVLAÂú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫‰∏éOpenVLA-OFTÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂÜÖÂ≠òÊ∂àËÄó‰ªÖ‰∏∫29.8%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÊ®°ÂûãËßÑÊ®°ÁöÑ‰∏çÊñ≠Êâ©Â§ßÁªôËµÑÊ∫êÂèóÈôêÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÁöÑÈÉ®ÁΩ≤Â∏¶Êù•‰∫ÜÈáçÂ§ßÊåëÊàò„ÄÇÂ∞ΩÁÆ°1‰ΩçÈ¢ÑËÆ≠ÁªÉÂú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊïàÁéáÊñπÈù¢Â∑≤Ë¢´ËØÅÊòéÊúâÊïàÔºå‰ΩÜÂÖ∂Âú®VLAÊ®°Âûã‰∏≠ÁöÑÂ∫îÁî®‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜBitVLAÔºåËøôÊòØÈ¶ñ‰∏™Áî®‰∫éÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑ1‰ΩçVLAÊ®°ÂûãÔºåÂÖ∂‰∏≠ÊØè‰∏™ÂèÇÊï∞‰∏∫‰∏âÂÖÉÁªÑ{-1, 0, 1}„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÂáèÂ∞ëËßÜËßâÁºñÁ†ÅÂô®ÁöÑÂÜÖÂ≠òÂç†Áî®ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËí∏È¶èÊÑüÁü•ËÆ≠ÁªÉÁ≠ñÁï•ÔºåÂ∞ÜÂÖ®Á≤æÂ∫¶ÁºñÁ†ÅÂô®ÂéãÁº©Ëá≥1.58‰ΩçÊùÉÈáç„ÄÇÂ∞ΩÁÆ°Áº∫‰πèÂ§ßËßÑÊ®°ÁöÑÊú∫Âô®‰∫∫È¢ÑËÆ≠ÁªÉÔºåBitVLAÂú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫Ü‰∏éÊúÄÂÖàËøõÊ®°ÂûãOpenVLA-OFTÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂêåÊó∂‰ªÖÊ∂àËÄó29.8%ÁöÑÂÜÖÂ≠ò„ÄÇËøô‰∫õÁªìÊûúÁ™ÅÊòæ‰∫ÜBitVLAÂú®ÂÜÖÂ≠òÂèóÈôêËæπÁºòËÆæÂ§á‰∏äÁöÑÈÉ®ÁΩ≤ÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®ËµÑÊ∫êÂèóÈôêÁöÑÊú∫Âô®‰∫∫Á≥ªÁªü‰∏≠ÈÉ®ÁΩ≤Âõ∞ÈöæÁöÑÈóÆÈ¢ò„ÄÇÈöèÁùÄÊ®°ÂûãËßÑÊ®°ÁöÑÂ¢ûÂ§ßÔºå‰º†ÁªüÊñπÊ≥ïÂú®ÂÜÖÂ≠òÂíåËÆ°ÁÆóÊïàÁéá‰∏äÈù¢‰∏¥ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑBitVLAÊ®°ÂûãÈÄöËøáÂ∞ÜÊØè‰∏™ÂèÇÊï∞ÂéãÁº©‰∏∫‰∏âÂÖÉÁªÑ{-1, 0, 1}ÔºåÂÆûÁé∞‰∫Ü1‰ΩçVLAÊ®°ÂûãÁöÑÊûÑÂª∫Ôºå‰ªéËÄåÂú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÊòæËëóÈôç‰ΩéÂÜÖÂ≠òÂç†Áî®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöBitVLAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™ËßÜËßâÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™ËØ≠Ë®Ä-Âä®‰ΩúËß£Á†ÅÂô®„ÄÇËßÜËßâÁºñÁ†ÅÂô®ÈÄöËøáËí∏È¶èÊÑüÁü•ËÆ≠ÁªÉÁ≠ñÁï•ËøõË°å‰ºòÂåñÔºå‰ΩøÁî®ÂÖ®Á≤æÂ∫¶ÁºñÁ†ÅÂô®‰Ωú‰∏∫ÊïôÂ∏àÊ®°ÂûãÊù•ÂØπÈΩêÊΩúÂú®Ë°®Á§∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöBitVLAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÈ¶ñÊ¨°Â∞Ü1‰ΩçÈ¢ÑËÆ≠ÁªÉÂ∫îÁî®‰∫éËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºå‰∏îÈÄöËøáËí∏È¶èËÆ≠ÁªÉÁ≠ñÁï•ÊúâÊïàÂéãÁº©‰∫ÜËßÜËßâÁºñÁ†ÅÂô®ÁöÑÊùÉÈáçÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÈúÄÊ±Ç„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫Ü‰∏âÂÖÉÁªÑÂèÇÊï∞Ë°®Á§∫ÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÊ≥®ÈáçÂØπÈΩêÊΩúÂú®Ë°®Á§∫ÁöÑÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂºïÂÖ•‰∫ÜÊïôÂ∏àÊ®°Âûã‰ª•ÊèêÂçáÂéãÁº©ÊïàÊûú„ÄÇÊï¥‰ΩìÊ®°ÂûãÁöÑÂÜÖÂ≠òÂç†Áî®‰ªÖ‰∏∫‰º†ÁªüÊ®°ÂûãÁöÑ29.8%„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠ÔºåBitVLAÊ®°ÂûãÁöÑÊÄßËÉΩ‰∏éÊúÄÂÖàËøõÁöÑOpenVLA-OFTÊ®°ÂûãÁõ∏ÂΩìÔºå‰∏îÂÜÖÂ≠òÊ∂àËÄó‰ªÖ‰∏∫29.8%„ÄÇËøô‰∏ÄÁªìÊûúË°®ÊòéÔºåBitVLAÂú®ËµÑÊ∫êÂèóÈôêÁöÑËæπÁºòËÆæÂ§á‰∏äÂÖ∑ÊúâÊòæËëóÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

BitVLAÊ®°ÂûãÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÊú∫Âô®‰∫∫„ÄÅËá™Âä®ÂåñÁîü‰∫ßÁ∫ø‰ª•ÂèäËæπÁºòËÆ°ÁÆóËÆæÂ§áÁ≠â„ÄÇÂÖ∂Âú®ÂÜÖÂ≠òÂèóÈôêÁéØÂ¢É‰∏≠ÁöÑÈ´òÊïàÊÄßËÉΩ‰ΩøÂÖ∂ÈÄÇÁî®‰∫éÂÆûÊó∂Êìç‰ΩúÂíåÂÜ≥Á≠ñÂú∫ÊôØÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.

