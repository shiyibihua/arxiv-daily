---
layout: default
title: Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control
---

# Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01765" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01765v1</a>
  <a href="https://arxiv.org/pdf/2509.01765.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01765v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01765v1', 'Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Skand Peri, Akhil Perincherry, Bikram Pandit, Stefan Lee

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01

**å¤‡æ³¨**: 17 pages, 6 figures. Accepted as Oral presentation at Conference on Robot Learning (CoRL) 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ— è¶…å‚æ•°çš„å¼ºåŒ–å­¦ä¹ èƒ½é‡ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºæœºå™¨äººæ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `èƒ½é‡ä¼˜åŒ–` `ç­–ç•¥æ¢¯åº¦` `å¤šä»»åŠ¡å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æœºå™¨äººæ§åˆ¶ä¸­ï¼Œéš¾ä»¥å¹³è¡¡ä»»åŠ¡æ€§èƒ½å’Œèƒ½é‡æ¶ˆè€—ï¼Œéœ€è¦æ‰‹åŠ¨è°ƒæ•´æƒé‡ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºç­–ç•¥æ¢¯åº¦æŠ•å½±çš„æ— è¶…å‚æ•°æ–¹æ³•ï¼Œåœ¨ä¸å½±å“ä»»åŠ¡æ€§èƒ½çš„å‰æä¸‹ï¼Œæœ€å°åŒ–èƒ½é‡æ¶ˆè€—ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†è¿åŠ¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œèƒ½é‡æ¶ˆè€—é™ä½äº†64%ï¼Œå¹¶åœ¨å››è¶³æœºå™¨äººä¸Šå®ç°äº†Sim2Realè¿ç§»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜æ•ˆçš„æœºå™¨äººæ§åˆ¶é€šå¸¸éœ€è¦åœ¨ä»»åŠ¡æ€§èƒ½å’Œèƒ½é‡æ¶ˆè€—ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å¼ºåŒ–å­¦ä¹ ä¸­å¸¸ç”¨çš„æ–¹æ³•æ˜¯å°†èƒ½é‡æ¶ˆè€—ç›´æ¥ä½œä¸ºå¥–åŠ±å‡½æ•°çš„ä¸€éƒ¨åˆ†è¿›è¡Œæƒ©ç½šã€‚ä½†è¿™éœ€è¦ä»”ç»†è°ƒæ•´æƒé‡ï¼Œä»¥é¿å…èƒ½é‡æœ€å°åŒ–æŸå®³ä»»åŠ¡æˆåŠŸçš„ä¸è‰¯æƒè¡¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è¶…å‚æ•°çš„æ¢¯åº¦ä¼˜åŒ–æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸å½±å“ä»»åŠ¡æ€§èƒ½çš„æƒ…å†µä¸‹æœ€å°åŒ–èƒ½é‡æ¶ˆè€—ã€‚å—å¤šä»»åŠ¡å­¦ä¹ çš„å¯å‘ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»»åŠ¡å’Œèƒ½é‡ç›®æ ‡ä¹‹é—´åº”ç”¨ç­–ç•¥æ¢¯åº¦æŠ•å½±ï¼Œä»¥æ¨å¯¼å‡ºåœ¨ä¸å½±å“ä»»åŠ¡æ€§èƒ½çš„æƒ…å†µä¸‹æœ€å°åŒ–èƒ½é‡æ¶ˆè€—çš„ç­–ç•¥æ›´æ–°ã€‚æˆ‘ä»¬åœ¨DM-Controlå’ŒHumanoidBenchçš„æ ‡å‡†è¿åŠ¨åŸºå‡†ä¸Šè¯„ä¼°äº†è¯¥æŠ€æœ¯ï¼Œç»“æœè¡¨æ˜åœ¨ä¿æŒç›¸å½“çš„ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½é‡ä½¿ç”¨é‡å‡å°‘äº†64%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨Unitree GO2å››è¶³æœºå™¨äººä¸Šè¿›è¡Œäº†å®éªŒï¼Œå±•ç¤ºäº†èƒ½é‡æ•ˆç‡ç­–ç•¥çš„Sim2Realè¿ç§»ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜“äºåœ¨æ ‡å‡†å¼ºåŒ–å­¦ä¹ æµç¨‹ä¸­å®ç°ï¼Œåªéœ€æœ€å°‘çš„ä»£ç æ›´æ”¹ï¼Œé€‚ç”¨äºä»»ä½•ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œå¹¶ä¸ºèƒ½é‡æ•ˆç‡æ§åˆ¶ç­–ç•¥æä¾›äº†ä¸€ç§æœ‰åŸåˆ™çš„å¥–åŠ±å¡‘é€ æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ§åˆ¶ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆé™ä½èƒ½é‡æ¶ˆè€—ï¼ŒåŒæ—¶ä¿è¯ä»»åŠ¡æ€§èƒ½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡åœ¨å¥–åŠ±å‡½æ•°ä¸­åŠ å…¥èƒ½é‡æ¶ˆè€—çš„æƒ©ç½šé¡¹æ¥å®ç°ï¼Œä½†è¿™ç§æ–¹æ³•éœ€è¦æ‰‹åŠ¨è°ƒæ•´æƒ©ç½šé¡¹çš„æƒé‡ï¼Œå®¹æ˜“å¯¼è‡´èƒ½é‡ä¼˜åŒ–ä¸ä»»åŠ¡ç›®æ ‡å†²çªï¼Œéš¾ä»¥æ‰¾åˆ°åˆé€‚çš„å¹³è¡¡ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æ¢¯åº¦æŠ•å½±æ€æƒ³ï¼Œå°†èƒ½é‡ä¼˜åŒ–è§†ä¸ºä¸€ä¸ªè¾…åŠ©ä»»åŠ¡ï¼Œé€šè¿‡ç­–ç•¥æ¢¯åº¦æŠ•å½±ï¼Œç¡®ä¿èƒ½é‡ä¼˜åŒ–çš„æ¢¯åº¦æ–¹å‘ä¸ä»»åŠ¡ç›®æ ‡æ¢¯åº¦æ–¹å‘ä¸å†²çªï¼Œä»è€Œåœ¨ä¸å½±å“ä»»åŠ¡æ€§èƒ½çš„å‰æä¸‹ï¼Œå°½å¯èƒ½åœ°é™ä½èƒ½é‡æ¶ˆè€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŸºäºæ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ æµç¨‹ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1ï¼‰ä½¿ç”¨ç­–ç•¥ç½‘ç»œä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†æ ·æœ¬æ•°æ®ï¼›2ï¼‰è®¡ç®—ä»»åŠ¡ç›®æ ‡çš„ç­–ç•¥æ¢¯åº¦å’Œèƒ½é‡æ¶ˆè€—çš„ç­–ç•¥æ¢¯åº¦ï¼›3ï¼‰å°†èƒ½é‡æ¶ˆè€—çš„ç­–ç•¥æ¢¯åº¦æŠ•å½±åˆ°ä¸ä»»åŠ¡ç›®æ ‡æ¢¯åº¦æ­£äº¤çš„ç©ºé—´ä¸­ï¼Œå¾—åˆ°ä¿®æ­£åçš„èƒ½é‡ä¼˜åŒ–æ¢¯åº¦ï¼›4ï¼‰ä½¿ç”¨ä¿®æ­£åçš„æ¢¯åº¦æ›´æ–°ç­–ç•¥ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºäºç­–ç•¥æ¢¯åº¦æŠ•å½±çš„èƒ½é‡ä¼˜åŒ–æ–¹æ³•ï¼Œé¿å…äº†æ‰‹åŠ¨è°ƒæ•´è¶…å‚æ•°çš„éº»çƒ¦ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¿è¯èƒ½é‡ä¼˜åŒ–ä¸ä¼šæŸå®³ä»»åŠ¡æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„å¥–åŠ±å¡‘é€ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ ç¨³å®šå’Œå¯é ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åœ¨äºç­–ç•¥æ¢¯åº¦æŠ•å½±çš„è®¡ç®—æ–¹å¼ã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾ä»»åŠ¡ç›®æ ‡çš„ç­–ç•¥æ¢¯åº¦ä¸ºg_taskï¼Œèƒ½é‡æ¶ˆè€—çš„ç­–ç•¥æ¢¯åº¦ä¸ºg_energyï¼Œåˆ™ä¿®æ­£åçš„èƒ½é‡ä¼˜åŒ–æ¢¯åº¦g_energy_proj = g_energy - (g_energy^T * g_task / ||g_task||^2) * g_taskã€‚è¿™ä¸ªå…¬å¼ä¿è¯äº†g_energy_projä¸g_taskæ­£äº¤ï¼Œå³èƒ½é‡ä¼˜åŒ–ä¸ä¼šå½±å“ä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨DM-Controlå’ŒHumanoidBenchç­‰æ ‡å‡†è¿åŠ¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒä¸åŸå§‹ç­–ç•¥ç›¸å½“çš„ä»»åŠ¡æ€§èƒ½çš„å‰æä¸‹ï¼Œå°†èƒ½é‡æ¶ˆè€—é™ä½64%ã€‚æ­¤å¤–ï¼Œåœ¨Unitree GO2å››è¶³æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è®­ç»ƒå¾—åˆ°çš„èƒ½é‡é«˜æ•ˆç­–ç•¥å¯ä»¥æˆåŠŸåœ°ä»ä»¿çœŸç¯å¢ƒè¿ç§»åˆ°çœŸå®ç¯å¢ƒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦èƒ½é‡é«˜æ•ˆæ§åˆ¶çš„æœºå™¨äººç³»ç»Ÿï¼Œä¾‹å¦‚ï¼šå››è¶³æœºå™¨äººã€æ— äººæœºã€æœºæ¢°è‡‚ç­‰ã€‚é€šè¿‡é™ä½æœºå™¨äººçš„èƒ½é‡æ¶ˆè€—ï¼Œå¯ä»¥å»¶é•¿å…¶ç»­èˆªæ—¶é—´ï¼Œé™ä½è¿è¥æˆæœ¬ï¼Œå¹¶å‡å°‘å¯¹ç¯å¢ƒçš„å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦å¹³è¡¡å¤šä¸ªç›®æ ‡çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚ï¼šåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œéœ€è¦åœ¨ä¿è¯å®‰å…¨æ€§çš„åŒæ—¶ï¼Œæé«˜è¡Œé©¶æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient robot control often requires balancing task performance with energy expenditure. A common approach in reinforcement learning (RL) is to penalize energy use directly as part of the reward function. This requires carefully tuning weight terms to avoid undesirable trade-offs where energy minimization harms task success. In this work, we propose a hyperparameter-free gradient optimization method to minimize energy expenditure without conflicting with task performance. Inspired by recent works in multitask learning, our method applies policy gradient projection between task and energy objectives to derive policy updates that minimize energy expenditure in ways that do not impact task performance. We evaluate this technique on standard locomotion benchmarks of DM-Control and HumanoidBench and demonstrate a reduction of 64% energy usage while maintaining comparable task performance. Further, we conduct experiments on a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient policies. Our method is easy to implement in standard RL pipelines with minimal code changes, is applicable to any policy gradient method, and offers a principled alternative to reward shaping for energy efficient control policies.

