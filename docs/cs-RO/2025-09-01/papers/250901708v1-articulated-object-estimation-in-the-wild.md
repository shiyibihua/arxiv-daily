---
layout: default
title: Articulated Object Estimation in the Wild
---

# Articulated Object Estimation in the Wild

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01708" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01708v1</a>
  <a href="https://arxiv.org/pdf/2509.01708.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01708v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01708v1', 'Articulated Object Estimation in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Abdelrhman Werby, Martin BÃ¼chner, Adrian RÃ¶fer, Chenguang Huang, Wolfram Burgard, Abhinav Valada

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01

**å¤‡æ³¨**: 9th Conference on Robot Learning (CoRL), 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ArtiPointï¼šæå‡ºä¸€ç§åœ¨åŠ¨æ€åœºæ™¯ä¸‹ä¼°è®¡é“°æ¥ç‰©ä½“æ¨¡å‹çš„æ¡†æ¶ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `é“°æ¥ç‰©ä½“ä¼°è®¡` `æ·±åº¦ç‚¹è·Ÿè¸ª` `å› å­å›¾ä¼˜åŒ–` `RGB-Dè§†é¢‘` `åŠ¨æ€åœºæ™¯ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é“°æ¥ç‰©ä½“ä¼°è®¡æ–¹æ³•åœ¨éçº¦æŸç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ï¼Œæ— æ³•å¤„ç†åŠ¨æ€ç›¸æœºå’Œéƒ¨åˆ†é®æŒ¡ã€‚
2. ArtiPointç»“åˆæ·±åº¦ç‚¹è·Ÿè¸ªå’Œå› å­å›¾ä¼˜åŒ–ï¼Œç›´æ¥ä»RGB-Dè§†é¢‘ä¸­ä¼°è®¡é“°æ¥éƒ¨ä»¶è½¨è¿¹å’Œé“°æ¥è½´ã€‚
3. Arti4Dæ•°æ®é›†æ˜¯é¦–ä¸ªåœºæ™¯çº§åˆ«çš„é‡å¤–é“°æ¥ç‰©ä½“äº¤äº’æ•°æ®é›†ï¼Œå®éªŒè¯æ˜ArtiPointä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç†è§£é“°æ¥ç‰©ä½“çš„3Dè¿åŠ¨å¯¹äºæœºå™¨äººåœºæ™¯ç†è§£ã€ç§»åŠ¨æ“ä½œå’Œè¿åŠ¨è§„åˆ’è‡³å…³é‡è¦ã€‚ç°æœ‰çš„é“°æ¥ä¼°è®¡æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å—æ§ç¯å¢ƒä¸­ï¼Œå‡è®¾å›ºå®šçš„ç›¸æœºè§†è§’æˆ–ç›´æ¥è§‚å¯Ÿå„ç§ç‰©ä½“çŠ¶æ€ï¼Œè¿™åœ¨æ›´çœŸå®çš„éçº¦æŸç¯å¢ƒä¸­å¾€å¾€ä¼šå¤±è´¥ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å¯ä»¥é€šè¿‡è§‚å¯Ÿä»–äººæ“çºµç‰©ä½“æ¥è½»æ¾æ¨æ–­é“°æ¥ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ArtiPointï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä¼°è®¡æ¡†æ¶ï¼Œå¯ä»¥åœ¨åŠ¨æ€ç›¸æœºè¿åŠ¨å’Œéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ä¸‹æ¨æ–­é“°æ¥ç‰©ä½“æ¨¡å‹ã€‚é€šè¿‡å°†æ·±åº¦ç‚¹è·Ÿè¸ªä¸å› å­å›¾ä¼˜åŒ–æ¡†æ¶ç›¸ç»“åˆï¼ŒArtiPointå¯ä»¥ç›´æ¥ä»åŸå§‹RGB-Dè§†é¢‘ä¸­ç¨³å¥åœ°ä¼°è®¡é“°æ¥éƒ¨ä»¶çš„è½¨è¿¹å’Œé“°æ¥è½´ã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†Arti4Dï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„é‡å¤–æ•°æ®é›†ï¼Œå®ƒæ•è·äº†åœºæ™¯çº§åˆ«çš„é“°æ¥ç‰©ä½“äº¤äº’ï¼Œå¹¶é™„å¸¦é“°æ¥æ ‡ç­¾å’Œground-truthç›¸æœºå§¿åŠ¿ã€‚æˆ‘ä»¬å°†ArtiPointä¸ä¸€ç³»åˆ—ç»å…¸çš„å’ŒåŸºäºå­¦ä¹ çš„åŸºçº¿è¿›è¡Œæ¯”è¾ƒï¼Œè¯æ˜äº†å…¶åœ¨Arti4Dä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬å°†å…¬å¼€ä»£ç å’ŒArti4Dæ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é“°æ¥ç‰©ä½“ä¼°è®¡æ–¹æ³•ä¸»è¦ä¾èµ–äºå—æ§ç¯å¢ƒï¼Œä¾‹å¦‚å›ºå®šçš„ç›¸æœºè§†è§’æˆ–å¯¹ç‰©ä½“çŠ¶æ€çš„ç›´æ¥è§‚æµ‹ã€‚è¿™äº›æ–¹æ³•åœ¨å®é™…çš„ã€éçº¦æŸçš„ç¯å¢ƒä¸­ï¼Œä¾‹å¦‚åŠ¨æ€ç›¸æœºè¿åŠ¨å’Œéƒ¨åˆ†é®æŒ¡çš„æƒ…å†µä¸‹ï¼Œå¾€å¾€ä¼šå¤±æ•ˆã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿåœ¨åŠ¨æ€åœºæ™¯å’Œéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ä¸‹å‡†ç¡®ä¼°è®¡é“°æ¥ç‰©ä½“æ¨¡å‹çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šArtiPointçš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡ä»¿äººç±»é€šè¿‡è§‚å¯Ÿç‰©ä½“æ“ä½œæ¥æ¨æ–­é“°æ¥çš„æ–¹å¼ã€‚å®ƒç»“åˆäº†æ·±åº¦ç‚¹è·Ÿè¸ªå’Œå› å­å›¾ä¼˜åŒ–ï¼Œåˆ©ç”¨RGB-Dè§†é¢‘ä¸­çš„ä¿¡æ¯æ¥ä¼°è®¡é“°æ¥éƒ¨ä»¶çš„è½¨è¿¹å’Œé“°æ¥è½´ã€‚é€šè¿‡è·Ÿè¸ªç‰©ä½“ä¸Šçš„å…³é”®ç‚¹ï¼Œå¹¶åˆ©ç”¨å› å­å›¾ä¼˜åŒ–æ¥çº¦æŸé“°æ¥è¿åŠ¨ï¼Œå¯ä»¥å®ç°å¯¹é“°æ¥ç‰©ä½“æ¨¡å‹çš„ç¨³å¥ä¼°è®¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šArtiPointæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ·±åº¦ç‚¹è·Ÿè¸ªæ¨¡å—ï¼šç”¨äºè·Ÿè¸ªRGB-Dè§†é¢‘ä¸­çš„å…³é”®ç‚¹ï¼Œè·å–å®ƒä»¬çš„3Dè½¨è¿¹ã€‚2) é“°æ¥ç»“æ„å…ˆéªŒï¼šåˆ©ç”¨å…ˆéªŒçŸ¥è¯†å¯¹é“°æ¥ç»“æ„è¿›è¡Œå»ºæ¨¡ï¼Œä¾‹å¦‚é“°æ¥ç±»å‹ï¼ˆæ—‹è½¬ã€å¹³ç§»ç­‰ï¼‰å’Œé“°æ¥è½´çš„æ–¹å‘ã€‚3) å› å­å›¾ä¼˜åŒ–æ¨¡å—ï¼šå°†ç‚¹è½¨è¿¹å’Œé“°æ¥ç»“æ„å…ˆéªŒæ•´åˆåˆ°å› å­å›¾ä¸­ï¼Œé€šè¿‡ä¼˜åŒ–å› å­å›¾æ¥ä¼°è®¡é“°æ¥éƒ¨ä»¶çš„è¿åŠ¨å‚æ•°å’Œé“°æ¥è½´ã€‚

**å…³é”®åˆ›æ–°**ï¼šArtiPointçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å°†æ·±åº¦ç‚¹è·Ÿè¸ªä¸å› å­å›¾ä¼˜åŒ–ç›¸ç»“åˆï¼Œä»è€Œèƒ½å¤Ÿåœ¨åŠ¨æ€åœºæ™¯å’Œéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ä¸‹ç¨³å¥åœ°ä¼°è®¡é“°æ¥ç‰©ä½“æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒArti4Dæ•°æ®é›†çš„å‘å¸ƒä¹Ÿä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šæ·±åº¦ç‚¹è·Ÿè¸ªæ¨¡å—ä½¿ç”¨äº†åŸºäºæ·±åº¦ä¿¡æ¯çš„ç‰¹å¾åŒ¹é…ç®—æ³•ï¼Œä»¥æé«˜è·Ÿè¸ªçš„é²æ£’æ€§ã€‚å› å­å›¾ä¼˜åŒ–æ¨¡å—ä½¿ç”¨äº†GTSAMåº“ï¼Œå¹¶è®¾è®¡äº†åˆé€‚çš„å› å­æ¥çº¦æŸé“°æ¥è¿åŠ¨ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ç‚¹è½¨è¿¹çš„é‡æŠ•å½±è¯¯å·®ã€é“°æ¥ç»“æ„å…ˆéªŒçš„çº¦æŸä»¥åŠæ­£åˆ™åŒ–é¡¹ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ArtiPointåœ¨Arti4Dæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸ä¸€ç³»åˆ—ç»å…¸çš„å’ŒåŸºäºå­¦ä¹ çš„åŸºçº¿æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArtiPointåœ¨é“°æ¥è½´ä¼°è®¡å’Œè¿åŠ¨è½¨è¿¹ä¼°è®¡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒArtiPointåœ¨é“°æ¥è½´ä¼°è®¡çš„å‡†ç¡®ç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è·Ÿè¸ªé“°æ¥éƒ¨ä»¶çš„è¿åŠ¨è½¨è¿¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ArtiPointåœ¨æœºå™¨äººæ“ä½œã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨ArtiPointæ¥ç†è§£å’Œæ“ä½œé“°æ¥ç‰©ä½“ï¼Œä»è€Œå®ç°æ›´å¤æ‚çš„ä»»åŠ¡ã€‚åœ¨AR/VRä¸­ï¼ŒArtiPointå¯ä»¥ç”¨äºåˆ›å»ºæ›´é€¼çœŸçš„äº¤äº’ä½“éªŒï¼Œä¾‹å¦‚ç”¨æˆ·å¯ä»¥è‡ªç„¶åœ°æ“çºµè™šæ‹Ÿçš„é“°æ¥ç‰©ä½“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding the 3D motion of articulated objects is essential in robotic scene understanding, mobile manipulation, and motion planning. Prior methods for articulation estimation have primarily focused on controlled settings, assuming either fixed camera viewpoints or direct observations of various object states, which tend to fail in more realistic unconstrained environments. In contrast, humans effortlessly infer articulation by watching others manipulate objects. Inspired by this, we introduce ArtiPoint, a novel estimation framework that can infer articulated object models under dynamic camera motion and partial observability. By combining deep point tracking with a factor graph optimization framework, ArtiPoint robustly estimates articulated part trajectories and articulation axes directly from raw RGB-D videos. To foster future research in this domain, we introduce Arti4D, the first ego-centric in-the-wild dataset that captures articulated object interactions at a scene level, accompanied by articulation labels and ground-truth camera poses. We benchmark ArtiPoint against a range of classical and learning-based baselines, demonstrating its superior performance on Arti4D. We make code and Arti4D publicly available at https://artipoint.cs.uni-freiburg.de.

