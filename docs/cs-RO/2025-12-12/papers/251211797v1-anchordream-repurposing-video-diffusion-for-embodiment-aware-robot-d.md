---
layout: default
title: AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis
---

# AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.11797" target="_blank" class="toolbar-btn">arXiv: 2512.11797v1</a>
    <a href="https://arxiv.org/pdf/2512.11797.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11797v1" 
            onclick="toggleFavorite(this, '2512.11797v1', 'AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Junjie Ye, Rong Xue, Basile Van Hoorick, Pavel Tokmakov, Muhammad Zubair Irshad, Yue Wang, Vitor Guizilini

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-12

**Â§áÊ≥®**: Project page: https://jay-ye.github.io/AnchorDream/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**AnchorDreamÔºöÂà©Áî®ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãËøõË°åÂÖ∑Ë∫´ÊÑüÁü•Êú∫Âô®‰∫∫Êï∞ÊçÆÂêàÊàê**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êï∞ÊçÆÂêàÊàê` `ËßÜÈ¢ëÊâ©Êï£Ê®°Âûã` `Ê®°‰ªøÂ≠¶‰π†` `ÂÖ∑Ë∫´ÊÑüÁü•` `ËøêÂä®Ê∏≤Êüì`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫Ê®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÂèóÈôê‰∫éÁúüÂÆûÊï∞ÊçÆËé∑ÂèñÊàêÊú¨È´òÊòÇÂíå‰ªøÁúüÁéØÂ¢ÉÁúüÂÆûÂ∫¶‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇ
2. AnchorDreamÈÄöËøá‰ª•Êú∫Âô®‰∫∫ËøêÂä®Ê∏≤Êüì‰∏∫Êù°‰ª∂È©±Âä®ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºåÂêàÊàêÈ´òË¥®Èáè„ÄÅÂ§öÊ†∑ÂåñÁöÑÊú∫Âô®‰∫∫Êï∞ÊçÆ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî®AnchorDreamÁîüÊàêÁöÑÊï∞ÊçÆËÉΩÊòæËëóÊèêÂçá‰∏ãÊ∏∏Á≠ñÁï•Â≠¶‰π†ÊïàÊûúÔºåÁúüÂÆûÁéØÂ¢ÉÊÄßËÉΩÊèêÂçáËøë‰∏ÄÂÄç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËßÑÊ®°ÂíåÂ§öÊ†∑ÂåñÁöÑÊú∫Âô®‰∫∫ÊºîÁ§∫Êï∞ÊçÆÊî∂ÈõÜ‰ªçÁÑ∂ÊòØÊ®°‰ªøÂ≠¶‰π†ÁöÑ‰∏ªË¶ÅÁì∂È¢àÔºåÂõ†‰∏∫ÁúüÂÆû‰∏ñÁïåÁöÑÊï∞ÊçÆËé∑ÂèñÊàêÊú¨È´òÊòÇÔºåËÄå‰ªøÁúüÂô®Êèê‰æõÁöÑÂ§öÊ†∑ÊÄßÂíåÈÄºÁúüÂ∫¶ÊúâÈôêÔºåÂ≠òÂú®ÊòéÊòæÁöÑÊ®°ÊãüÂà∞ÁúüÂÆû‰∏ñÁïåÁöÑÂ∑ÆË∑ù„ÄÇËôΩÁÑ∂ÁîüÊàêÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÊúâÂê∏ÂºïÂäõÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âè™ÊîπÂèòËßÜËßâÂ§ñËßÇËÄå‰∏çÂàõÈÄ†Êñ∞ÁöÑË°å‰∏∫ÔºåÊàñËÄÖÈÅ≠ÂèóÂÖ∑Ë∫´‰∏ç‰∏ÄËá¥ÊÄßÔºå‰ªéËÄå‰∫ßÁîü‰∏çÂêàÁêÜÁöÑËøêÂä®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜAnchorDreamÔºå‰∏ÄÁßçÂÖ∑Ë∫´ÊÑüÁü•ÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåÂÆÉÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÈáçÊñ∞Áî®‰∫éÊú∫Âô®‰∫∫Êï∞ÊçÆÂêàÊàê„ÄÇAnchorDream‰ª•Êú∫Âô®‰∫∫ËøêÂä®Ê∏≤Êüì‰∏∫Êù°‰ª∂Êù•È©±Âä®Êâ©Êï£ËøáÁ®ãÔºåÈîöÂÆöÂÖ∑Ë∫´‰ª•Èò≤Ê≠¢ÂπªËßâÔºåÂêåÊó∂ÂêàÊàê‰∏éÊú∫Âô®‰∫∫ËøêÂä®Â≠¶‰∏ÄËá¥ÁöÑÁâ©‰ΩìÂíåÁéØÂ¢É„ÄÇ‰ªéÂ∞ëÈáèÁöÑËøúÁ®ãÊìç‰ΩúÊºîÁ§∫ÂºÄÂßãÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂ∞ÜÂÖ∂Êâ©Â±ï‰∏∫Â§ßÂûã„ÄÅÂ§öÊ†∑Âåñ„ÄÅÈ´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜÔºåËÄåÊó†ÈúÄÊòæÂºèÁöÑÁéØÂ¢ÉÂª∫Ê®°„ÄÇÂÆûÈ™åË°®ÊòéÔºåÁîüÊàêÁöÑÊï∞ÊçÆËÉΩÂ§üÊåÅÁª≠ÊîπËøõ‰∏ãÊ∏∏Á≠ñÁï•Â≠¶‰π†ÔºåÂú®Ê®°ÊãüÂô®Âü∫ÂáÜÊµãËØï‰∏≠Áõ∏ÂØπÂ¢ûÁõä‰∏∫36.4%ÔºåÂú®ÁúüÂÆû‰∏ñÁïåÁ†îÁ©∂‰∏≠ÊÄßËÉΩÂá†‰πéÁøªÂÄç„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÂ∞ÜÁîüÊàê‰∏ñÁïåÊ®°ÂûãÂª∫Á´ãÂú®Êú∫Âô®‰∫∫ËøêÂä®ÁöÑÂü∫Á°Ä‰∏äÔºå‰∏∫Êâ©Â±ïÊ®°‰ªøÂ≠¶‰π†Êèê‰æõ‰∫Ü‰∏ÄÊù°ÂàáÂÆûÂèØË°åÁöÑÈÄîÂæÑ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊú∫Âô®‰∫∫Ê®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÈù¢‰∏¥Êï∞ÊçÆÁì∂È¢àÔºåÁúüÂÆûÊï∞ÊçÆÈááÈõÜÊàêÊú¨È´òÔºå‰ªøÁúüÊï∞ÊçÆÂ≠òÂú®‚Äúsim-to-real‚ÄùÂ∑ÆË∑ù„ÄÇÁîüÊàêÊ®°ÂûãËôΩÁÑ∂ÊúâÊΩúÂäõÔºå‰ΩÜË¶Å‰πàÂè™ÊîπÂèòËßÜËßâÊïàÊûúÔºåË¶Å‰πà‰∫ßÁîü‰∏çÁ¨¶ÂêàÊú∫Âô®‰∫∫ËøêÂä®Â≠¶ËßÑÂæãÁöÑÂä®‰ΩúÔºåÁº∫‰πèÂÖ∑Ë∫´ÊÑüÁü•ËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAnchorDreamÁöÑÊ†∏ÂøÉÂú®‰∫éÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºåÂπ∂‰ª•Êú∫Âô®‰∫∫ËøêÂä®Ê∏≤Êüì‰Ωú‰∏∫Êù°‰ª∂ÔºàAnchorÔºâÊù•ÂºïÂØºÊâ©Êï£ËøáÁ®ã„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãÂèØ‰ª•ÁîüÊàê‰∏éÊú∫Âô®‰∫∫ËøêÂä®Â≠¶‰∏ÄËá¥ÁöÑÂú∫ÊôØÂíåÁâ©‰ΩìÔºåÈÅøÂÖçÂπªËßâÔºå‰øùËØÅÂêàÊàêÊï∞ÊçÆÁöÑÂêàÁêÜÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAnchorDreamÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö1) ‰ΩøÁî®Â∞ëÈáè‰∫∫Â∑•ÈÅ•Êìç‰ΩúÊï∞ÊçÆ‰Ωú‰∏∫ÁßçÂ≠êÔºõ2) Â∞ÜÊú∫Âô®‰∫∫ËøêÂä®‰ø°ÊÅØÊ∏≤ÊüìÊàêÂõæÂÉèÂ∫èÂàóÔºõ3) Â∞ÜÊ∏≤ÊüìÁöÑÂõæÂÉèÂ∫èÂàó‰Ωú‰∏∫Êù°‰ª∂ËæìÂÖ•Âà∞È¢ÑËÆ≠ÁªÉÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°Âûã‰∏≠Ôºõ4) ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁîüÊàêÊñ∞ÁöÑËßÜÈ¢ëÂ∫èÂàóÔºåËøô‰∫õÂ∫èÂàóÂåÖÂê´‰∏éÊú∫Âô®‰∫∫ËøêÂä®‰∏ÄËá¥ÁöÑÂú∫ÊôØÂíåÁâ©‰Ωì„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAnchorDreamÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÊú∫Âô®‰∫∫ËøêÂä®‰ø°ÊÅØ‰Ωú‰∏∫‚ÄúÈîöÁÇπ‚ÄùËûçÂÖ•Âà∞ËßÜÈ¢ëÊâ©Êï£Ê®°Âûã‰∏≠Ôºå‰ªéËÄåÂÆûÁé∞‰∫ÜÂÖ∑Ë∫´ÊÑüÁü•ÁöÑÊú∫Âô®‰∫∫Êï∞ÊçÆÂêàÊàê„ÄÇËøô‰∏é‰ª•ÂæÄÁöÑÁîüÊàêÊ®°ÂûãÂè™ÂÖ≥Ê≥®ËßÜËßâÊïàÊûúÊàñÂøΩÁï•Êú∫Âô®‰∫∫ËøêÂä®Â≠¶Á∫¶ÊùüÁöÑÊñπÊ≥ïÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöAnchorDreamÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºåÈÅøÂÖç‰ªéÂ§¥ËÆ≠ÁªÉÁöÑÊàêÊú¨Ôºõ2) Á≤æÂøÉËÆæËÆ°ÁöÑÊú∫Âô®‰∫∫ËøêÂä®Ê∏≤ÊüìÊñπÂºèÔºåÁ°Æ‰øùËøêÂä®‰ø°ÊÅØËÉΩÂ§üÊúâÊïàÂú∞‰º†ÈÄíÁªôÊâ©Êï£Ê®°ÂûãÔºõ3) ‰ΩøÁî®ÂØπÊäóÊÄßÊçüÂ§±ÂáΩÊï∞Êù•ÊèêÈ´òÁîüÊàêÊï∞ÊçÆÁöÑÁúüÂÆûÊÑüÂíåÂ§öÊ†∑ÊÄßÔºàÂÖ∑‰ΩìÊçüÂ§±ÂáΩÊï∞ÁªÜËäÇËÆ∫Êñá‰∏≠ÂèØËÉΩÂåÖÂê´ÔºåÊ≠§Â§ÑÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®AnchorDreamÁîüÊàêÁöÑÊï∞ÊçÆËÉΩÂ§üÊòæËëóÊèêÂçá‰∏ãÊ∏∏Á≠ñÁï•Â≠¶‰π†ÁöÑÊÄßËÉΩ„ÄÇÂú®Ê®°ÊãüÂô®Âü∫ÂáÜÊµãËØï‰∏≠ÔºåÁõ∏ÂØπÂ¢ûÁõäËææÂà∞36.4%ÔºåËÄåÂú®ÁúüÂÆû‰∏ñÁïåÁöÑÁ†îÁ©∂‰∏≠ÔºåÊÄßËÉΩÂá†‰πéÁøªÂÄç„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫ÜAnchorDreamÂú®Êú∫Âô®‰∫∫Êï∞ÊçÆÂêàÊàêÊñπÈù¢ÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

AnchorDreamÂú®Êú∫Âô®‰∫∫Ê®°‰ªøÂ≠¶‰π†È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØ‰ª•Áî®‰∫éÁîüÊàêÂêÑÁßç‰ªªÂä°ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºå‰æãÂ¶ÇÁâ©‰ΩìÊäìÂèñ„ÄÅÂØºËà™„ÄÅË£ÖÈÖçÁ≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÈôç‰ΩéÊú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊàêÊú¨ÔºåÊèêÈ´òÂ≠¶‰π†ÊïàÁéáÔºåÂπ∂ÊúâÊúõÂä†ÈÄüÊú∫Âô®‰∫∫ÊäÄÊúØÂú®Â∑•‰∏ö„ÄÅÂåªÁñó„ÄÅÊúçÂä°Á≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.

