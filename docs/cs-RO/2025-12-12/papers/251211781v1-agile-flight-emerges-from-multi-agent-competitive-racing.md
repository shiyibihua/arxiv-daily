---
layout: default
title: Agile Flight Emerges from Multi-Agent Competitive Racing
---

# Agile Flight Emerges from Multi-Agent Competitive Racing

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.11781" target="_blank" class="toolbar-btn">arXiv: 2512.11781v1</a>
    <a href="https://arxiv.org/pdf/2512.11781.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11781v1" 
            onclick="toggleFavorite(this, '2512.11781v1', 'Agile Flight Emerges from Multi-Agent Competitive Racing')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Vineet Pasumarti, Lorenzo Bianchi, Antonio Loquercio

**åˆ†ç±»**: cs.RO, cs.AI, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-12-12

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Jirl-upenn/AgileFlight_MultiAgent)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åŸºäºå¤šæ™ºèƒ½ä½“ç«äº‰å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°æ— äººæœºæ•æ·é£è¡Œä¸ç­–ç•¥åšå¼ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `æ— äººæœº` `æ•æ·é£è¡Œ` `ç«äº‰å­¦ä¹ ` `ç¨€ç–å¥–åŠ±` `Sim-to-Realè¿ç§»` `ç­–ç•¥åšå¼ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äºäººä¸ºè®¾è®¡çš„å¥–åŠ±å‡½æ•°æ¥å¼•å¯¼æ— äººæœºå­¦ä¹ ç‰¹å®šè¡Œä¸ºï¼Œéš¾ä»¥é€‚åº”å¤æ‚ç¯å¢ƒå’Œå®ç°æ•æ·é£è¡Œã€‚
2. è®ºæ–‡æå‡ºåŸºäºå¤šæ™ºèƒ½ä½“ç«äº‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»…ä½¿ç”¨ç¨€ç–çš„æ¯”èµ›èƒœè´Ÿå¥–åŠ±ï¼Œä½¿æ™ºèƒ½ä½“è‡ªä¸»å­¦ä¹ é£è¡Œç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸­å‡ä¼˜äºåŸºäºå•æ™ºèƒ½ä½“è¿›å±•å¥–åŠ±çš„è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„sim-to-realè¿ç§»èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é€šè¿‡å¤šæ™ºèƒ½ä½“ç«äº‰å’Œç¨€ç–çš„é«˜çº§ç›®æ ‡ï¼ˆèµ¢å¾—æ¯”èµ›ï¼‰å‘ç°ï¼Œæ•æ·é£è¡Œï¼ˆä¾‹å¦‚ï¼Œå°†å¹³å°æ¨å‘ç‰©ç†æé™çš„é«˜é€Ÿè¿åŠ¨ï¼‰å’Œç­–ç•¥ï¼ˆä¾‹å¦‚ï¼Œè¶…è½¦æˆ–é˜»æŒ¡ï¼‰éƒ½ä»é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ™ºèƒ½ä½“ä¸­æ¶Œç°å‡ºæ¥ã€‚æˆ‘ä»¬åœ¨ä»¿çœŸå’Œç°å®ä¸–ç•Œä¸­æä¾›çš„è¯æ®è¡¨æ˜ï¼Œå½“ç¯å¢ƒçš„å¤æ‚æ€§å¢åŠ æ—¶ï¼ˆä¾‹å¦‚ï¼Œåœ¨å­˜åœ¨éšœç¢ç‰©çš„æƒ…å†µä¸‹ï¼‰ï¼Œè¿™ç§æ–¹æ³•ä¼˜äºå¸¸è§çš„èŒƒä¾‹ï¼Œå³ä½¿ç”¨è§„å®šè¡Œä¸ºçš„å¥–åŠ±ï¼ˆä¾‹å¦‚ï¼Œåœ¨èµ›é“ä¸Šçš„è¿›å±•ï¼‰æ¥å­¤ç«‹åœ°è®­ç»ƒæ™ºèƒ½ä½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ï¼Œä¸ä½¿ç”¨åŸºäºå•æ™ºèƒ½ä½“è¿›å±•çš„å¥–åŠ±è®­ç»ƒçš„ç­–ç•¥ç›¸æ¯”ï¼Œå¤šæ™ºèƒ½ä½“ç«äº‰äº§ç”Ÿçš„ç­–ç•¥èƒ½å¤Ÿæ›´å¯é åœ°è½¬ç§»åˆ°ç°å®ä¸–ç•Œï¼Œå°½ç®¡è¿™ä¸¤ç§æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„ä»¿çœŸç¯å¢ƒã€éšæœºåŒ–ç­–ç•¥å’Œç¡¬ä»¶ã€‚é™¤äº†æ”¹è¿›çš„sim-to-realè¿ç§»ä¹‹å¤–ï¼Œå¤šæ™ºèƒ½ä½“ç­–ç•¥è¿˜è¡¨ç°å‡ºä¸€å®šç¨‹åº¦çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥é€‚åº”è®­ç»ƒæ—¶æœªè§è¿‡çš„å¯¹æ‰‹ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œéµå¾ªæ•°å­—é¢†åŸŸä¸­å¤šæ™ºèƒ½ä½“ç«äº‰æ¸¸æˆçš„ä¼ ç»Ÿï¼Œè¡¨æ˜ç¨€ç–çš„ä»»åŠ¡çº§å¥–åŠ±è¶³ä»¥è®­ç»ƒèƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­è¿›è¡Œé«˜çº§ä½çº§æ§åˆ¶çš„æ™ºèƒ½ä½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ— äººæœºæ•æ·é£è¡Œæ§åˆ¶æ–¹æ³•é€šå¸¸ä¾èµ–äºç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œä¾‹å¦‚è·Ÿè¸ªé¢„å®šè½¨è¿¹æˆ–æœ€å¤§åŒ–å‰è¿›é€Ÿåº¦ã€‚è¿™äº›æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æ³›åŒ–ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„é¢†åŸŸçŸ¥è¯†æ¥è°ƒæ•´å¥–åŠ±å‡½æ•°ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éš¾ä»¥å®ç°æ™ºèƒ½ä½“ä¹‹é—´çš„ç­–ç•¥åšå¼ˆï¼Œä¾‹å¦‚è¶…è½¦å’Œé˜»æŒ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ™ºèƒ½ä½“ç«äº‰æ¥é©±åŠ¨æ— äººæœºå­¦ä¹ æ•æ·é£è¡Œå’Œç­–ç•¥åšå¼ˆã€‚é€šè¿‡è®©å¤šä¸ªæ™ºèƒ½ä½“åœ¨æ¯”èµ›ä¸­ç«äº‰ï¼Œå¹¶ä»…æä¾›ç¨€ç–çš„èƒœè´Ÿå¥–åŠ±ï¼Œæ™ºèƒ½ä½“å¯ä»¥è‡ªä¸»æ¢ç´¢å’Œå­¦ä¹ æœ€ä¼˜çš„é£è¡Œç­–ç•¥ï¼Œè€Œæ— éœ€äººä¸ºè®¾è®¡çš„å¥–åŠ±å‡½æ•°ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ›´å¥½åœ°é€‚åº”å¤æ‚ç¯å¢ƒï¼Œå¹¶å®ç°æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œå…¶ä¸­å¤šä¸ªæ— äººæœºæ™ºèƒ½ä½“åœ¨èµ›é“ä¸Šç«äº‰ã€‚æ¯ä¸ªæ™ºèƒ½ä½“éƒ½ä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚PPOï¼‰è¿›è¡Œè®­ç»ƒã€‚ç¯å¢ƒæä¾›æ— äººæœºçš„çŠ¶æ€ä¿¡æ¯ï¼ˆä¾‹å¦‚ä½ç½®ã€é€Ÿåº¦ã€å§¿æ€ï¼‰å’Œèµ›é“ä¿¡æ¯ï¼Œæ™ºèƒ½ä½“è¾“å‡ºæ§åˆ¶æŒ‡ä»¤ï¼ˆä¾‹å¦‚ç”µæœºè½¬é€Ÿï¼‰ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä½“ä»…è·å¾—ç¨€ç–çš„èƒœè´Ÿå¥–åŠ±ï¼Œå³èµ¢å¾—æ¯”èµ›çš„æ™ºèƒ½ä½“è·å¾—æ­£å¥–åŠ±ï¼Œè¾“æ‰æ¯”èµ›çš„æ™ºèƒ½ä½“è·å¾—è´Ÿå¥–åŠ±ã€‚è®­ç»ƒå®Œæˆåï¼Œæ™ºèƒ½ä½“å¯ä»¥éƒ¨ç½²åˆ°çœŸå®ç¯å¢ƒä¸­è¿›è¡Œæ¯”èµ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç«äº‰å’Œç¨€ç–å¥–åŠ±æ¥è®­ç»ƒæ— äººæœºå®ç°æ•æ·é£è¡Œå’Œç­–ç•¥åšå¼ˆã€‚ä¸ä¼ ç»Ÿçš„åŸºäºäººä¸ºè®¾è®¡å¥–åŠ±å‡½æ•°çš„å•æ™ºèƒ½ä½“è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ›´å¥½åœ°é€‚åº”å¤æ‚ç¯å¢ƒï¼Œå®ç°æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å…è®¸æ™ºèƒ½ä½“è‡ªä¸»å­¦ä¹ é£è¡Œç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿå®ç°æ™ºèƒ½ä½“ä¹‹é—´çš„ç­–ç•¥åšå¼ˆï¼Œä¾‹å¦‚è¶…è½¦å’Œé˜»æŒ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚çŠ¶æ€ç©ºé—´åŒ…æ‹¬æ— äººæœºçš„ä½ç½®ã€é€Ÿåº¦ã€å§¿æ€ã€è§’é€Ÿåº¦ä»¥åŠèµ›é“ä¿¡æ¯ã€‚åŠ¨ä½œç©ºé—´åŒ…æ‹¬å››ä¸ªç”µæœºçš„è½¬é€Ÿã€‚å¥–åŠ±å‡½æ•°æ˜¯ç¨€ç–çš„ï¼Œåªæœ‰èµ¢å¾—æ¯”èµ›çš„æ™ºèƒ½ä½“è·å¾—æ­£å¥–åŠ±ï¼Œè¾“æ‰æ¯”èµ›çš„æ™ºèƒ½ä½“è·å¾—è´Ÿå¥–åŠ±ã€‚ä¸ºäº†æé«˜sim-to-realè¿ç§»èƒ½åŠ›ï¼Œè®ºæ–‡ä½¿ç”¨äº†éšæœºåŒ–æŠ€æœ¯ï¼Œä¾‹å¦‚éšæœºåŒ–æ— äººæœºçš„è´¨é‡ã€æƒ¯æ€§çŸ©å’Œç”µæœºå‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¤šæ™ºèƒ½ä½“ç«äº‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸­å‡ä¼˜äºåŸºäºå•æ™ºèƒ½ä½“è¿›å±•å¥–åŠ±çš„è®­ç»ƒæ–¹æ³•ã€‚åœ¨ä»¿çœŸç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶å–å¾—æ›´é«˜çš„èƒœç‡ã€‚åœ¨çœŸå®ç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¥½çš„sim-to-realè¿ç§»èƒ½åŠ›ï¼Œèƒ½å¤ŸæˆåŠŸåœ°éƒ¨ç½²åˆ°çœŸå®æ— äººæœºä¸Šè¿›è¡Œæ¯”èµ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿå®ç°æ™ºèƒ½ä½“ä¹‹é—´çš„ç­–ç•¥åšå¼ˆï¼Œä¾‹å¦‚è¶…è½¦å’Œé˜»æŒ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ— äººæœºç«é€Ÿã€è‡ªä¸»å¯¼èˆªã€æœç´¢æ•‘æ´ç­‰é¢†åŸŸã€‚é€šè¿‡å¤šæ™ºèƒ½ä½“ç«äº‰å­¦ä¹ ï¼Œæ— äººæœºèƒ½å¤Ÿè‡ªä¸»é€‚åº”å¤æ‚ç¯å¢ƒï¼Œå®ç°æ•æ·é£è¡Œå’Œæ™ºèƒ½å†³ç­–ã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–æœºå™¨äººé¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè¶³çƒç­‰ï¼Œä¸ºå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿæä¾›æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.
>   Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent

