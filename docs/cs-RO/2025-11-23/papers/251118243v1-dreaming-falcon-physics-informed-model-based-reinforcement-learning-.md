---
layout: default
title: Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters
---

# Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters

**arXiv**: [2511.18243v1](https://arxiv.org/abs/2511.18243) | [PDF](https://arxiv.org/pdf/2511.18243.pdf)

**ä½œè€…**: Eashan Vytla, Bhavanishankar Kalavakolanu, Andrew Perrault, Matthew McCrink

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-23

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Dreaming Falconï¼šåŸºäºŽç‰©ç†ä¿¡æ¯çš„å››æ—‹ç¿¼é£žè¡Œå™¨æ¨¡åž‹é¢„æµ‹å¼ºåŒ–å­¦ä¹ **

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å››æ—‹ç¿¼é£žè¡Œå™¨` `æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `ç‰©ç†ä¿¡æ¯` `ä¸–ç•Œæ¨¡åž‹` `é²æ£’æŽ§åˆ¶` `è‡ªä¸»é£žè¡Œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å››æ—‹ç¿¼æŽ§åˆ¶ç®—æ³•åœ¨å¤æ‚çŽ¯å¢ƒä¸­é²æ£’æ€§ä¸è¶³ï¼ŒåŸºäºŽæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ æ˜¯æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œä½†å­˜åœ¨æ ·æœ¬æ•ˆçŽ‡å’Œæ³›åŒ–æ€§é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºŽç‰©ç†ä¿¡æ¯çš„å­¦ä¹ ä¸–ç•Œæ¨¡åž‹æ–¹æ³•ï¼Œå°†å››æ—‹ç¿¼è§†ä¸ºè‡ªç”±ä½“ï¼Œé¢„æµ‹åˆåŠ›å’ŒåŠ›çŸ©ï¼Œå¹¶ç”¨RK4ç§¯åˆ†å™¨é¢„æµ‹æœªæ¥çŠ¶æ€ã€‚
3. å®žéªŒå¯¹æ¯”äº†åŸºäºŽç‰©ç†ä¿¡æ¯å’ŒåŸºäºŽRNNçš„ä¸–ç•Œæ¨¡åž‹ï¼Œå‘çŽ°ä¸¤è€…åœ¨è®­ç»ƒé›†ä¸Šè¡¨çŽ°è‰¯å¥½ï¼Œä½†åœ¨æ–°è½¨è¿¹ä¸Šæ³›åŒ–æ€§å·®ï¼Œå¯¼è‡´ç­–ç•¥æ— æ³•æ”¶æ•›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŽ°æœ‰çš„æ— äººæœºæŽ§åˆ¶ç®—æ³•åœ¨åŠ¨æ€çŽ¯å¢ƒå’Œä¸åˆ©æ¡ä»¶ä¸‹é²æ£’æ€§ä¸è¶³ã€‚åŸºäºŽæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ (RL)åœ¨åº”å¯¹è¿™äº›æŒ‘æˆ˜æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¿æŒäº†æ ·æœ¬æ•ˆçŽ‡ã€‚æ­¤å¤–ï¼ŒDreamerå·²ç»è¯æ˜Žï¼Œå¯ä»¥ä½¿ç”¨åœ¨å›žæ”¾ç¼“å†²åŒºæ•°æ®ä¸Šè®­ç»ƒçš„å¾ªçŽ¯ä¸–ç•Œæ¨¡åž‹æ¥å®žçŽ°åœ¨çº¿çš„åŸºäºŽæ¨¡åž‹çš„RLã€‚ç„¶è€Œï¼Œç”±äºŽå…¶æ ·æœ¬æ•ˆçŽ‡ä½Žå’ŒåŠ¨åŠ›å­¦æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å·®ï¼Œå°†Dreameråº”ç”¨äºŽæ— äººæœºç³»ç»Ÿä¸€ç›´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æŽ¢ç´¢äº†ä¸€ç§åŸºäºŽç‰©ç†ä¿¡æ¯çš„å­¦ä¹ ä¸–ç•Œæ¨¡åž‹çš„æ–¹æ³•ï¼Œå¹¶æé«˜äº†ç­–ç•¥æ€§èƒ½ã€‚è¯¥ä¸–ç•Œæ¨¡åž‹å°†å››æ—‹ç¿¼é£žè¡Œå™¨è§†ä¸ºè‡ªç”±ä½“ç³»ç»Ÿï¼Œå¹¶é¢„æµ‹ä½œç”¨åœ¨å…¶ä¸Šçš„åˆåŠ›å’ŒåŠ›çŸ©ï¼Œç„¶åŽé€šè¿‡6è‡ªç”±åº¦Runge-Kuttaç§¯åˆ†å™¨(RK4)æ¥é¢„æµ‹æœªæ¥çš„çŠ¶æ€å±•å¼€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è¿™ç§åŸºäºŽç‰©ç†ä¿¡æ¯çš„æ–¹æ³•ä¸Žæ ‡å‡†çš„åŸºäºŽRNNçš„ä¸–ç•Œæ¨¡åž‹è¿›è¡Œäº†æ¯”è¾ƒã€‚è™½ç„¶è¿™ä¸¤ç§æ¨¡åž‹åœ¨è®­ç»ƒæ•°æ®ä¸Šéƒ½è¡¨çŽ°è‰¯å¥½ï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°å®ƒä»¬æ— æ³•æŽ¨å¹¿åˆ°æ–°çš„è½¨è¿¹ï¼Œå¯¼è‡´çŠ¶æ€å±•å¼€çš„å¿«é€Ÿå‘æ•£ï¼Œä»Žè€Œé˜»æ­¢äº†ç­–ç•¥æ”¶æ•›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰å››æ—‹ç¿¼é£žè¡Œå™¨çš„æŽ§åˆ¶ç®—æ³•åœ¨åŠ¨æ€çŽ¯å¢ƒå’Œæ¶åŠ£æ¡ä»¶ä¸‹è¡¨çŽ°å‡ºé²æ£’æ€§ä¸è¶³çš„é—®é¢˜ã€‚åŸºäºŽæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ è™½ç„¶æœ‰æ½œåŠ›è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†çŽ°æœ‰çš„æ–¹æ³•ï¼Œå¦‚ç›´æŽ¥ä½¿ç”¨RNNå»ºæ¨¡åŠ¨åŠ›å­¦ï¼Œå­˜åœ¨æ ·æœ¬æ•ˆçŽ‡ä½Žå’Œæ³›åŒ–èƒ½åŠ›å·®çš„ç¼ºç‚¹ï¼Œéš¾ä»¥é€‚åº”çœŸå®žä¸–ç•Œçš„å¤æ‚çŽ¯å¢ƒã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´æœ‰æ•ˆä¸”æ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„æ¨¡åž‹å­¦ä¹ æ–¹æ³•ï¼Œä»¥æå‡å››æ—‹ç¿¼é£žè¡Œå™¨çš„æŽ§åˆ¶æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç‰©ç†ä¿¡æ¯èžå…¥åˆ°ä¸–ç•Œæ¨¡åž‹çš„å­¦ä¹ ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œä¸æ˜¯ç›´æŽ¥ç”¨RNNç­‰æ¨¡åž‹æ‹ŸåˆçŠ¶æ€è½¬ç§»å‡½æ•°ï¼Œè€Œæ˜¯å°†å››æ—‹ç¿¼é£žè¡Œå™¨è§†ä¸ºä¸€ä¸ªè‡ªç”±ä½“ç³»ç»Ÿï¼Œåˆ©ç”¨å·²çŸ¥çš„ç‰©ç†è§„å¾‹ï¼ˆå¦‚ç‰›é¡¿è¿åŠ¨å®šå¾‹ï¼‰æ¥çº¦æŸæ¨¡åž‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡é¢„æµ‹ä½œç”¨åœ¨é£žè¡Œå™¨ä¸Šçš„åˆåŠ›å’ŒåŠ›çŸ©ï¼Œå¹¶ä½¿ç”¨Runge-Kuttaç§¯åˆ†å™¨è¿›è¡ŒçŠ¶æ€é¢„æµ‹ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆçŽ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŸºäºŽDreamerï¼Œä¸€ä¸ªåœ¨çº¿çš„åŸºäºŽæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚ä¸»è¦åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š1) çŽ¯å¢ƒäº¤äº’ï¼šæ™ºèƒ½ä½“ä¸ŽçŽ¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ç»éªŒæ•°æ®ï¼›2) ä¸–ç•Œæ¨¡åž‹å­¦ä¹ ï¼šä½¿ç”¨æ”¶é›†åˆ°çš„æ•°æ®è®­ç»ƒä¸–ç•Œæ¨¡åž‹ï¼Œä¸–ç•Œæ¨¡åž‹åŒ…å«ä¸€ä¸ªRNNå’Œä¸€ä¸ªç‰©ç†ä¿¡æ¯æ¨¡å—ï¼Œç”¨äºŽé¢„æµ‹æœªæ¥çš„çŠ¶æ€ï¼›3) ç­–ç•¥ä¼˜åŒ–ï¼šåŸºäºŽä¸–ç•Œæ¨¡åž‹é¢„æµ‹çš„æœªæ¥çŠ¶æ€ï¼Œä¼˜åŒ–æŽ§åˆ¶ç­–ç•¥ï¼›4) ç­–ç•¥éƒ¨ç½²ï¼šå°†ä¼˜åŒ–åŽçš„ç­–ç•¥éƒ¨ç½²åˆ°çœŸå®žçŽ¯å¢ƒä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå°†ç‰©ç†ä¿¡æ¯èžå…¥åˆ°ä¸–ç•Œæ¨¡åž‹çš„å­¦ä¹ ä¸­ã€‚ä¼ ç»Ÿçš„åŸºäºŽRNNçš„ä¸–ç•Œæ¨¡åž‹é€šå¸¸æ˜¯é»‘ç›’æ¨¡åž‹ï¼Œéš¾ä»¥ä¿è¯ç‰©ç†ä¸€è‡´æ€§ã€‚è€Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ï¼Œé€šè¿‡å°†å››æ—‹ç¿¼é£žè¡Œå™¨è§†ä¸ºè‡ªç”±ä½“ï¼Œå¹¶åˆ©ç”¨å·²çŸ¥çš„ç‰©ç†è§„å¾‹æ¥çº¦æŸæ¨¡åž‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œä»Žè€Œæé«˜äº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆçŽ‡ã€‚è¿™ç§æ–¹æ³•ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼Œå®ƒä¸ä»…ä»…æ˜¯æ‹Ÿåˆæ•°æ®ï¼Œè€Œæ˜¯åˆ©ç”¨ç‰©ç†çŸ¥è¯†æ¥æŒ‡å¯¼æ¨¡åž‹çš„å­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨6è‡ªç”±åº¦Runge-Kuttaç§¯åˆ†å™¨(RK4)æ¥é¢„æµ‹æœªæ¥çš„çŠ¶æ€ï¼Œä¿è¯äº†çŠ¶æ€é¢„æµ‹çš„ç²¾åº¦ï¼›2) ä¸–ç•Œæ¨¡åž‹é¢„æµ‹ä½œç”¨åœ¨é£žè¡Œå™¨ä¸Šçš„åˆåŠ›å’ŒåŠ›çŸ©ï¼Œè€Œä¸æ˜¯ç›´æŽ¥é¢„æµ‹çŠ¶æ€çš„å˜åŒ–é‡ï¼Œä»Žè€Œæ›´å¥½åœ°åˆ©ç”¨äº†ç‰©ç†ä¿¡æ¯ï¼›3) æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘ç‰©ç†çº¦æŸï¼Œä¾‹å¦‚ï¼Œå¯ä»¥æ·»åŠ æ­£åˆ™åŒ–é¡¹æ¥çº¦æŸæ¨¡åž‹çš„è¾“å‡ºï¼Œä½¿å…¶æ»¡è¶³ç‰©ç†è§„å¾‹ã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è®ºæ–‡å¯¹æ¯”äº†åŸºäºŽç‰©ç†ä¿¡æ¯å’ŒåŸºäºŽRNNçš„ä¸–ç•Œæ¨¡åž‹ï¼Œè™½ç„¶ä¸¤è€…åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨çŽ°è‰¯å¥½ï¼Œä½†éƒ½æ— æ³•æ³›åŒ–åˆ°æ–°çš„è½¨è¿¹ï¼Œå¯¼è‡´çŠ¶æ€é¢„æµ‹å¿«é€Ÿå‘æ•£ï¼Œç­–ç•¥æ— æ³•æ”¶æ•›ã€‚è¿™è¡¨æ˜Žä»…ä»…ä¾é æ•°æ®é©±åŠ¨çš„æ–¹æ³•éš¾ä»¥è§£å†³å››æ—‹ç¿¼é£žè¡Œå™¨çš„æŽ§åˆ¶é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥æŽ¢ç´¢æ›´æœ‰æ•ˆçš„ç‰©ç†ä¿¡æ¯èžåˆæ–¹æ³•ï¼Œæˆ–è€…ç»“åˆå…¶ä»–æŠ€æœ¯æ‰‹æ®µï¼Œå¦‚é¢†åŸŸè‡ªé€‚åº”ã€å…ƒå­¦ä¹ ç­‰ï¼Œæ¥æé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå¤šç§æ— äººæœºæŽ§åˆ¶åœºæ™¯ï¼Œä¾‹å¦‚åœ¨å¤æ‚åœ°å½¢æˆ–æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„è‡ªä¸»é£žè¡Œã€ç‰©æµé…é€ã€æœç´¢æ•‘æ´ç­‰ã€‚é€šè¿‡æé«˜æ— äººæœºæŽ§åˆ¶çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥é™ä½Žæ“ä½œéš¾åº¦å’Œé£Žé™©ï¼Œæ‹“å±•æ— äººæœºçš„åº”ç”¨èŒƒå›´ï¼Œå¹¶ä¸ºæœªæ¥çš„è‡ªä¸»é£žè¡ŒæŠ€æœ¯å‘å±•å¥ å®šåŸºç¡€ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æ€è·¯ä¹Ÿå¯æŽ¨å¹¿åˆ°å…¶ä»–æœºå™¨äººç³»ç»Ÿï¼Œä¾‹å¦‚æ°´ä¸‹æœºå™¨äººã€ç§»åŠ¨æœºå™¨äººç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Current control algorithms for aerial robots struggle with robustness in dynamic environments and adverse conditions. Model-based reinforcement learning (RL) has shown strong potential in handling these challenges while remaining sample-efficient. Additionally, Dreamer has demonstrated that online model-based RL can be achieved using a recurrent world model trained on replay buffer data. However, applying Dreamer to aerial systems has been quite challenging due to its sample inefficiency and poor generalization of dynamics models. Our work explores a physics-informed approach to world model learning and improves policy performance. The world model treats the quadcopter as a free-body system and predicts the net forces and moments acting on it, which are then passed through a 6-DOF Runge-Kutta integrator (RK4) to predict future state rollouts. In this paper, we compare this physics-informed method to a standard RNN-based world model. Although both models perform well on the training data, we observed that they fail to generalize to new trajectories, leading to rapid divergence in state rollouts, preventing policy convergence.

