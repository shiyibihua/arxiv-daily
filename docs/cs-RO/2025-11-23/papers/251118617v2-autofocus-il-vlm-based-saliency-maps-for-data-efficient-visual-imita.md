---
layout: default
title: AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations
---

# AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations

**arXiv**: [2511.18617v2](https://arxiv.org/abs/2511.18617) | [PDF](https://arxiv.org/pdf/2511.18617.pdf)

**ä½œè€…**: Litian Gong, Fatemeh Bahrani, Yutai Zhou, Amin Banayeeanzade, Jiachen Li, Erdem BÄ±yÄ±k

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-23 (æ›´æ–°: 2025-11-25)

**å¤‡æ³¨**: 8 pages, 6 figures. Code and datasets available at http://autofocus-il.github.io/

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://AutoFocus-IL.github.io/)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AutoFocus-ILï¼šåŸºäºŽVLMæ˜¾è‘—æ€§å›¾çš„æ•°æ®é«˜æ•ˆè§†è§‰æ¨¡ä»¿å­¦ä¹ ï¼Œæ— éœ€é¢å¤–äººå·¥æ ‡æ³¨**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰æ¨¡ä»¿å­¦ä¹ ` `æ˜¾è‘—æ€§æ­£åˆ™åŒ–` `è§†è§‰è¯­è¨€æ¨¡åž‹` `è¡Œä¸ºå…‹éš†` `æ•°æ®é«˜æ•ˆå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººå·¥æ ‡æ³¨æˆ–äººç±»è§†çº¿æ•°æ®è¿›è¡Œæ˜¾è‘—æ€§æ­£åˆ™åŒ–ï¼Œé™åˆ¶äº†æ•°æ®æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. AutoFocus-ILåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æ˜¾è‘—æ€§å›¾ï¼Œå¼•å¯¼ç­–ç•¥å…³æ³¨ä»»åŠ¡ç›¸å…³ç‰¹å¾ï¼ŒæŠ‘åˆ¶å¹²æ‰°ï¼Œæ— éœ€é¢å¤–äººå·¥æ ‡æ³¨ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒAutoFocus-ILåœ¨CARLAæ¨¡æ‹Ÿå™¨å’ŒçœŸå®žæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä¼˜äºŽæ ‡å‡†è¡Œä¸ºå…‹éš†å’Œä¾èµ–äººç±»ç›‘ç£çš„åŸºçº¿æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

AutoFocus-ILæ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¼•å¯¼ç­–ç•¥å…³æ³¨ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾è€Œéžå¹²æ‰°å› ç´ å’Œè™šå‡ç›¸å…³æ€§ï¼Œä»Žè€Œæé«˜è§†è§‰æ¨¡ä»¿å­¦ä¹ ä¸­çš„æ•°æ®æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚è™½ç„¶æ˜¾è‘—æ€§æ­£åˆ™åŒ–å·²æˆä¸ºå®žçŽ°è¿™ä¸€ç›®æ ‡çš„ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œä½†çŽ°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦æ˜‚è´µçš„ç›‘ç£ï¼Œä¾‹å¦‚äººç±»è§†çº¿æ•°æ®æˆ–æ‰‹åŠ¨æ˜¾è‘—æ€§æ ‡æ³¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒAutoFocus-ILåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰è‡ªåŠ¨è¯†åˆ«å’Œè·Ÿè¸ªæ¼”ç¤ºä¸­çš„å…³é”®å¯¹è±¡ï¼Œç”Ÿæˆæ—¶é—´æ˜¾è‘—æ€§å›¾ï¼Œçªå‡ºæ˜¾ç¤ºå› æžœè§†è§‰ä¿¡å·ï¼ŒåŒæ—¶æŠ‘åˆ¶å¹²æ‰°å› ç´ ã€‚ç„¶åŽï¼Œè¿™äº›å›¾ç”¨äºŽæ­£åˆ™åŒ–è¡Œä¸ºå…‹éš†ç­–ç•¥ï¼Œä»Žè€Œå¢žå¼ºè§†è§‰æ³¨æ„åŠ›å’Œä»»åŠ¡ç›¸å…³çº¿ç´¢ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚åœ¨CARLAæ¨¡æ‹Ÿå™¨å’ŒçœŸå®žæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å®žéªŒè¡¨æ˜Žï¼ŒAutoFocus-ILä¸ä»…ä¼˜äºŽæ ‡å‡†è¡Œä¸ºå…‹éš†ï¼Œè€Œä¸”è¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œè¿™äº›åŸºçº¿æ–¹æ³•å‡å®šå¯ä»¥ç‰¹æƒè®¿é—®äººç±»ç›‘ç£ï¼Œä¾‹å¦‚è§†çº¿æ•°æ®ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰æ¨¡ä»¿å­¦ä¹ æ—¨åœ¨è®©æ™ºèƒ½ä½“é€šè¿‡è§‚å¯Ÿäººç±»æˆ–å…¶ä»–æ™ºèƒ½ä½“çš„æ¼”ç¤ºæ¥å­¦ä¹ è¡Œä¸ºç­–ç•¥ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰æ–¹æ³•å®¹æ˜“å—åˆ°å¹²æ‰°å› ç´ å’Œè™šå‡ç›¸å…³æ€§çš„å½±å“ï¼Œå¯¼è‡´æ•°æ®æ•ˆçŽ‡ä½Žä¸‹å’Œæ³›åŒ–èƒ½åŠ›å·®ã€‚çŽ°æœ‰çš„æ˜¾è‘—æ€§æ­£åˆ™åŒ–æ–¹æ³•è™½ç„¶å¯ä»¥å¼•å¯¼ç­–ç•¥å…³æ³¨ä»»åŠ¡ç›¸å…³ç‰¹å¾ï¼Œä½†é€šå¸¸éœ€è¦æ˜‚è´µçš„äººå·¥æ ‡æ³¨æˆ–äººç±»è§†çº¿æ•°æ®ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAutoFocus-ILçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰çš„å¼ºå¤§èƒ½åŠ›ï¼Œè‡ªåŠ¨ç”Ÿæˆæ—¶é—´æ˜¾è‘—æ€§å›¾ï¼Œæ— éœ€é¢å¤–çš„äººå·¥æ ‡æ³¨ã€‚é€šè¿‡VLMè¯†åˆ«å’Œè·Ÿè¸ªæ¼”ç¤ºä¸­çš„å…³é”®å¯¹è±¡ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„æ˜¾è‘—æ€§å›¾ï¼Œä»Žè€Œå¼•å¯¼è¡Œä¸ºå…‹éš†ç­–ç•¥å…³æ³¨ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡å·ï¼ŒæŠ‘åˆ¶å¹²æ‰°å› ç´ ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šAutoFocus-ILçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆå¦‚CLIPï¼‰å¤„ç†æ¼”ç¤ºè§†é¢‘ï¼Œè¯†åˆ«å’Œè·Ÿè¸ªå…³é”®å¯¹è±¡ã€‚2) åŸºäºŽå¯¹è±¡è·Ÿè¸ªç»“æžœï¼Œç”Ÿæˆæ—¶é—´æ˜¾è‘—æ€§å›¾ï¼Œçªå‡ºæ˜¾ç¤ºä¸Žä»»åŠ¡ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚3) ä½¿ç”¨ç”Ÿæˆçš„æ˜¾è‘—æ€§å›¾å¯¹è¡Œä¸ºå…‹éš†ç­–ç•¥è¿›è¡Œæ­£åˆ™åŒ–ï¼Œé¼“åŠ±ç­–ç•¥å…³æ³¨æ˜¾è‘—åŒºåŸŸï¼Œå¿½ç•¥å¹²æ‰°å› ç´ ã€‚4) ä½¿ç”¨æ­£åˆ™åŒ–åŽçš„ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆæ¨¡ä»¿æ¼”ç¤ºè¡Œä¸ºçš„æ™ºèƒ½ä½“ã€‚

**å…³é”®åˆ›æ–°**ï¼šAutoFocus-ILæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºŽåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹è‡ªåŠ¨ç”Ÿæˆæ˜¾è‘—æ€§å›¾ï¼Œæ— éœ€é¢å¤–çš„äººå·¥æ ‡æ³¨ã€‚è¿™ä¸ŽçŽ°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººå·¥ç›‘ç£å½¢æˆäº†é²œæ˜Žå¯¹æ¯”ï¼Œå¤§å¤§æé«˜äº†æ•°æ®æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒAutoFocus-ILè¿˜æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ—¶é—´æ˜¾è‘—æ€§å›¾ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°æ•æ‰ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡å·ã€‚

**å…³é”®è®¾è®¡**ï¼šAutoFocus-ILçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨CLIPç­‰é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œä»¥èŽ·å¾—å¼ºå¤§çš„è§†è§‰ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚2) è®¾è®¡äº†ä¸€ç§åŸºäºŽå¯¹è±¡è·Ÿè¸ªçš„æ—¶é—´æ˜¾è‘—æ€§å›¾ç”Ÿæˆç®—æ³•ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°æ•æ‰ä»»åŠ¡ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚3) ä½¿ç”¨L1æŸå¤±æˆ–KLæ•£åº¦ç­‰æ­£åˆ™åŒ–é¡¹ï¼Œé¼“åŠ±è¡Œä¸ºå…‹éš†ç­–ç•¥å…³æ³¨æ˜¾è‘—åŒºåŸŸï¼Œå¿½ç•¥å¹²æ‰°å› ç´ ã€‚å…·ä½“å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

AutoFocus-ILåœ¨CARLAæ¨¡æ‹Ÿå™¨å’ŒçœŸå®žæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„å®žéªŒç»“æžœã€‚åœ¨CARLAä¸­ï¼ŒAutoFocus-ILä¼˜äºŽæ ‡å‡†è¡Œä¸ºå…‹éš†å’Œå…¶ä»–åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬é‚£äº›ä½¿ç”¨äººç±»è§†çº¿æ•°æ®çš„åŸºçº¿æ–¹æ³•ã€‚åœ¨çœŸå®žæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒAutoFocus-ILä¹Ÿè¡¨çŽ°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜Žäº†å…¶åœ¨å®žé™…åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒAutoFocus-ILèƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼ç­–ç•¥å…³æ³¨ä»»åŠ¡ç›¸å…³ç‰¹å¾ï¼ŒæŠ‘åˆ¶å¹²æ‰°å› ç´ ï¼Œä»Žè€Œæé«˜æ•°æ®æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

AutoFocus-ILå¯åº”ç”¨äºŽå„ç§è§†è§‰æ¨¡ä»¿å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„äººå·¥æ ‡æ³¨ï¼Œé™ä½Žäº†æ•°æ®æ”¶é›†æˆæœ¬ï¼Œæé«˜äº†æ•°æ®æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæœ‰æœ›æŽ¨åŠ¨è§†è§‰æ¨¡ä»¿å­¦ä¹ åœ¨å®žé™…åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚æœªæ¥ï¼Œå¯ä»¥æŽ¢ç´¢å°†AutoFocus-ILä¸Žå…¶ä»–æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.

