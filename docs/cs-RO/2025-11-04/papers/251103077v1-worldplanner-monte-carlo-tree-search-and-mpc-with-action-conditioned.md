---
layout: default
title: WorldPlanner: Monte Carlo Tree Search and MPC with Action-Conditioned Visual World Models
---

# WorldPlanner: Monte Carlo Tree Search and MPC with Action-Conditioned Visual World Models

**arXiv**: [2511.03077v1](https://arxiv.org/abs/2511.03077) | [PDF](https://arxiv.org/pdf/2511.03077.pdf)

**ä½œè€…**: R. Khorrambakht, Joaquim Ortiz-Haro, Joseph Amigo, Omar Mostafa, Daniel Dugas, Franziska Meier, Ludovic Righetti

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-04

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**WorldPlannerï¼šåŸºäºŽåŠ¨ä½œæ¡ä»¶è§†è§‰ä¸–ç•Œæ¨¡åž‹çš„MCTSå’ŒMPCæœºå™¨äººè§„åˆ’**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººå­¦ä¹ ` `æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `è§†è§‰ä¸–ç•Œæ¨¡åž‹` `åŠ¨ä½œæ¡ä»¶` `æ‰©æ•£æ¨¡åž‹` `æœºå™¨äººè§„åˆ’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰ä¾èµ–äººç±»æ¼”ç¤ºå­¦ä¹ ç­–ç•¥ï¼Œä½†æ³›åŒ–æ€§å·®ä¸”æ•°æ®æ”¶é›†å›°éš¾ï¼Œéœ€è¦é¢‘ç¹é‡ç½®çŽ¯å¢ƒã€‚
2. æœ¬æ–‡æå‡ºä¸€ç§åŸºäºŽæ¨¡åž‹çš„æ–¹æ¡ˆï¼Œé€šè¿‡å­¦ä¹ åŠ¨ä½œæ¡ä»¶è§†è§‰ä¸–ç•Œæ¨¡åž‹ï¼Œç»“åˆMCTSè§„åˆ’å’ŒMPCæŽ§åˆ¶ï¼Œå®žçŽ°æœºå™¨äººä»»åŠ¡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®žæœºå™¨äººä»»åŠ¡ä¸­ä¼˜äºŽBCåŸºçº¿ï¼Œè¯æ˜Žäº†è§„åˆ’åœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è§£å†³æœºå™¨äººä»ŽåŽŸå§‹æ„Ÿå®˜è¾“å…¥ç†è§£çŽ¯å¢ƒå¹¶æŽ¨ç†å…¶è¡Œä¸ºåŽæžœä»¥å®Œæˆå¤æ‚ä»»åŠ¡çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºŽæ¨¡åž‹çš„æœºå™¨äººå­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å°‘é‡æ˜“äºŽæ”¶é›†çš„éžç»“æž„åŒ–æ•°æ®æ¥å­¦ä¹ åŠ¨ä½œæ¡ä»¶è§†è§‰ä¸–ç•Œæ¨¡åž‹ã€åŸºäºŽæ‰©æ•£çš„åŠ¨ä½œé‡‡æ ·å™¨ä»¥åŠå¯é€‰çš„å¥–åŠ±æ¨¡åž‹ã€‚ä¸–ç•Œæ¨¡åž‹ä¸ŽåŠ¨ä½œé‡‡æ ·å™¨å’Œå¥–åŠ±æ¨¡åž‹ç»“åˆï¼Œé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è§„åˆ’å™¨ä¼˜åŒ–é•¿åºåˆ—åŠ¨ä½œã€‚æœ€ç»ˆçš„è§„åˆ’ç»“æžœé€šè¿‡é›¶é˜¶æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶å™¨ï¼ˆMPCï¼‰åœ¨æœºå™¨äººä¸Šæ‰§è¡Œã€‚å®žéªŒè¡¨æ˜Žï¼ŒåŠ¨ä½œé‡‡æ ·å™¨å‡è½»äº†è§„åˆ’æœŸé—´ä¸–ç•Œæ¨¡åž‹çš„å¹»è§‰é—®é¢˜ï¼Œå¹¶åœ¨ä¸‰ä¸ªçœŸå®žæœºå™¨äººä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®žéªŒç»“æžœæ”¯æŒäº†è§„åˆ’æ–¹æ³•åœ¨æ ‡å‡†æ“ä½œæµ‹è¯•çŽ¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºŽè¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰åŸºçº¿çš„å‡è®¾ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æœºå™¨äººå­¦ä¹ æ–¹æ³•ï¼Œå¦‚è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰ï¼Œä¾èµ–äºŽå¤§é‡çš„ä»»åŠ¡ç‰¹å®šçš„äººå·¥æ¼”ç¤ºæ•°æ®ï¼Œè¿™ä½¿å¾—æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”å­¦ä¹ åˆ°çš„ç­–ç•¥éš¾ä»¥æ³›åŒ–åˆ°æ–°çš„ä»»åŠ¡æˆ–çŽ¯å¢ƒã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸æ˜¯ç«¯åˆ°ç«¯çš„ï¼Œç¼ºä¹å¯¹çŽ¯å¢ƒçš„æ˜¾å¼å»ºæ¨¡å’ŒæŽ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥å¤„ç†å¤æ‚ä»»åŠ¡ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿåˆ©ç”¨å°‘é‡æ•°æ®å­¦ä¹ çŽ¯å¢ƒæ¨¡åž‹ï¼Œå¹¶è¿›è¡Œæœ‰æ•ˆè§„åˆ’çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å°‘é‡éžç»“æž„åŒ–æ•°æ®å­¦ä¹ ä¸€ä¸ªåŠ¨ä½œæ¡ä»¶è§†è§‰ä¸–ç•Œæ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹èƒ½å¤Ÿé¢„æµ‹åœ¨ç»™å®šå½“å‰çŠ¶æ€å’ŒåŠ¨ä½œçš„æƒ…å†µä¸‹ï¼ŒçŽ¯å¢ƒçš„æœªæ¥çŠ¶æ€ã€‚ç„¶åŽï¼Œåˆ©ç”¨è¯¥ä¸–ç•Œæ¨¡åž‹ï¼Œç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è§„åˆ’å™¨ï¼Œåœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­è¿›è¡Œè§„åˆ’ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„åŠ¨ä½œåºåˆ—ã€‚æœ€åŽï¼Œä½¿ç”¨æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶ï¼ˆMPCï¼‰å°†è§„åˆ’çš„åŠ¨ä½œåºåˆ—è½¬åŒ–ä¸ºå®žé™…çš„æœºå™¨äººæŽ§åˆ¶æŒ‡ä»¤ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæœºå™¨äººå¯ä»¥åœ¨æ²¡æœ‰å¤§é‡äººå·¥æ¼”ç¤ºæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ åˆ°å¤æ‚çš„ä»»åŠ¡ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•çš„æŠ€æœ¯æ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) **åŠ¨ä½œæ¡ä»¶è§†è§‰ä¸–ç•Œæ¨¡åž‹**ï¼šç”¨äºŽé¢„æµ‹ç»™å®šå½“å‰çŠ¶æ€å’ŒåŠ¨ä½œçš„æƒ…å†µä¸‹ï¼ŒçŽ¯å¢ƒçš„æœªæ¥çŠ¶æ€ã€‚2) **åŠ¨ä½œé‡‡æ ·å™¨**ï¼šç”¨äºŽåœ¨è§„åˆ’è¿‡ç¨‹ä¸­ç”Ÿæˆå€™é€‰åŠ¨ä½œã€‚3) **å¥–åŠ±æ¨¡åž‹ï¼ˆå¯é€‰ï¼‰**ï¼šç”¨äºŽè¯„ä¼°è§„åˆ’çš„åŠ¨ä½œåºåˆ—çš„è´¨é‡ã€‚4) **è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è§„åˆ’å™¨**ï¼šåˆ©ç”¨ä¸–ç•Œæ¨¡åž‹ã€åŠ¨ä½œé‡‡æ ·å™¨å’Œå¥–åŠ±æ¨¡åž‹ï¼Œåœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­è¿›è¡Œè§„åˆ’ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„åŠ¨ä½œåºåˆ—ã€‚5) **æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶ï¼ˆMPCï¼‰**ï¼šå°†è§„åˆ’çš„åŠ¨ä½œåºåˆ—è½¬åŒ–ä¸ºå®žé™…çš„æœºå™¨äººæŽ§åˆ¶æŒ‡ä»¤ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆåˆ©ç”¨å°‘é‡éžç»“æž„åŒ–æ•°æ®è®­ç»ƒä¸–ç•Œæ¨¡åž‹ã€åŠ¨ä½œé‡‡æ ·å™¨å’Œå¥–åŠ±æ¨¡åž‹ã€‚ç„¶åŽï¼Œåœ¨æ¯ä¸ªæŽ§åˆ¶å‘¨æœŸï¼Œåˆ©ç”¨MCTSè§„åˆ’å™¨åœ¨ä¸–ç•Œæ¨¡åž‹ä¸­è¿›è¡Œè§„åˆ’ï¼Œå¾—åˆ°æœ€ä¼˜çš„åŠ¨ä½œåºåˆ—ã€‚æœ€åŽï¼Œåˆ©ç”¨MPCå°†è§„åˆ’çš„åŠ¨ä½œåºåˆ—è½¬åŒ–ä¸ºå®žé™…çš„æœºå™¨äººæŽ§åˆ¶æŒ‡ä»¤ï¼ŒæŽ§åˆ¶æœºå™¨äººæ‰§è¡Œä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºŽå°†åŠ¨ä½œæ¡ä»¶è§†è§‰ä¸–ç•Œæ¨¡åž‹ä¸ŽMCTSè§„åˆ’å™¨å’ŒMPCæŽ§åˆ¶å™¨ç›¸ç»“åˆï¼Œå®žçŽ°äº†ä¸€ç§åŸºäºŽæ¨¡åž‹çš„æœºå™¨äººå­¦ä¹ æ–¹æ³•ã€‚ä¸Žä¼ ç»Ÿçš„è¡Œä¸ºå…‹éš†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¤§é‡çš„äººå·¥æ¼”ç¤ºæ•°æ®ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¼•å…¥äº†åŠ¨ä½œé‡‡æ ·å™¨ï¼Œä»¥å‡è½»è§„åˆ’è¿‡ç¨‹ä¸­ä¸–ç•Œæ¨¡åž‹çš„å¹»è§‰é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨ä½œæ¡ä»¶è§†è§‰ä¸–ç•Œæ¨¡åž‹é‡‡ç”¨æ‰©æ•£æ¨¡åž‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æœªæ¥çŠ¶æ€é¢„æµ‹ã€‚åŠ¨ä½œé‡‡æ ·å™¨åŸºäºŽæ‰©æ•£æ¨¡åž‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„å€™é€‰åŠ¨ä½œã€‚MCTSè§„åˆ’å™¨é‡‡ç”¨UCTï¼ˆUpper Confidence Bound applied to Treesï¼‰ç®—æ³•è¿›è¡ŒèŠ‚ç‚¹é€‰æ‹©ï¼Œå¹¶ä½¿ç”¨ä¸–ç•Œæ¨¡åž‹è¿›è¡ŒèŠ‚ç‚¹æ‰©å±•å’Œè¯„ä¼°ã€‚MPCæŽ§åˆ¶å™¨é‡‡ç”¨é›¶é˜¶ä¿æŒç­–ç•¥ï¼Œå°†è§„åˆ’çš„åŠ¨ä½œåºåˆ—è½¬åŒ–ä¸ºå®žé™…çš„æœºå™¨äººæŽ§åˆ¶æŒ‡ä»¤ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªçœŸå®žæœºå™¨äººä»»åŠ¡ï¼ˆåŒ…æ‹¬æŠ“å–ã€æ”¾ç½®å’Œå †å ï¼‰ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ ‡å‡†æ“ä½œæµ‹è¯•çŽ¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•æ˜Žæ˜¾ä¼˜äºŽè¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰åŸºçº¿ã€‚ä¾‹å¦‚ï¼Œåœ¨æŠ“å–ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•çš„æˆåŠŸçŽ‡æ¯”BCåŸºçº¿æé«˜äº†çº¦20%ã€‚æ­¤å¤–ï¼Œå®žéªŒè¿˜éªŒè¯äº†åŠ¨ä½œé‡‡æ ·å™¨åœ¨å‡è½»è§„åˆ’è¿‡ç¨‹ä¸­ä¸–ç•Œæ¨¡åž‹å¹»è§‰é—®é¢˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§éœ€è¦æœºå™¨äººè‡ªä¸»è§„åˆ’å’ŒæŽ§åˆ¶çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šå·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡å­¦ä¹ çŽ¯å¢ƒæ¨¡åž‹å¹¶è¿›è¡Œè§„åˆ’ï¼Œæœºå™¨äººå¯ä»¥åœ¨å¤æ‚çŽ¯å¢ƒä¸­å®Œæˆå„ç§ä»»åŠ¡ï¼Œæé«˜å·¥ä½œæ•ˆçŽ‡å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºŽè®­ç»ƒæœºå™¨äººåœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­è¿›è¡Œå­¦ä¹ ï¼Œç„¶åŽå°†å­¦ä¹ åˆ°çš„ç­–ç•¥è¿ç§»åˆ°çœŸå®žæœºå™¨äººä¸Šï¼Œä»Žè€Œé™ä½Žè®­ç»ƒæˆæœ¬å’Œé£Žé™©ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Robots must understand their environment from raw sensory inputs and reason about the consequences of their actions in it to solve complex tasks. Behavior Cloning (BC) leverages task-specific human demonstrations to learn this knowledge as end-to-end policies. However, these policies are difficult to transfer to new tasks, and generating training data is challenging because it requires careful demonstrations and frequent environment resets. In contrast to such policy-based view, in this paper we take a model-based approach where we collect a few hours of unstructured easy-to-collect play data to learn an action-conditioned visual world model, a diffusion-based action sampler, and optionally a reward model. The world model -- in combination with the action sampler and a reward model -- is then used to optimize long sequences of actions with a Monte Carlo Tree Search (MCTS) planner. The resulting plans are executed on the robot via a zeroth-order Model Predictive Controller (MPC). We show that the action sampler mitigates hallucinations of the world model during planning and validate our approach on 3 real-world robotic tasks with varying levels of planning and modeling complexity. Our experiments support the hypothesis that planning leads to a significant improvement over BC baselines on a standard manipulation test environment.

