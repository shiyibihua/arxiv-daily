---
layout: default
title: XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations
---

# XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.02776" target="_blank" class="toolbar-btn">arXiv: 2511.02776v1</a>
    <a href="https://arxiv.org/pdf/2511.02776.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.02776v1" 
            onclick="toggleFavorite(this, '2511.02776v1', 'XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Shichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao, Zhiyuan Xu, Meng Li, Qingjie Liu, Shanghang Zhang, Min Wan, Jian Tang

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-04

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://xr-1-vla.github.io/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**XR-1ÔºöÈÄöËøáÂ≠¶‰π†Áªü‰∏ÄËßÜËßâ-ËøêÂä®Ë°®ÂæÅÔºåÂÆûÁé∞ÈÄöÁî®ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Êú∫Âô®‰∫∫Â≠¶‰π†` `Áªü‰∏ÄË°®ÂæÅÂ≠¶‰π†` `Â§öÊ®°ÊÄÅËûçÂêà` `ÂºÇÊûÑÊï∞ÊçÆ` `VQ-VAE` `Êú∫Âô®‰∫∫Êìç‰Ωú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÈöæ‰ª•‰ªéÈ´òÁª¥ËßÇÊµãÁîüÊàêÁ≤æÁ°ÆÂä®‰ΩúÔºå‰∏îÈöæ‰ª•Âº•Âêà‰∏çÂêåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÂíå‰∫∫Á±ªÊºîÁ§∫Êï∞ÊçÆÈó¥ÁöÑÈ¢ÜÂüüÂ∑ÆË∑ù„ÄÇ
2. XR-1ÊèêÂá∫Áªü‰∏ÄËßÜËßâ-ËøêÂä®‰ª£Á†ÅÔºàUVMCÔºâÔºåÈÄöËøáÂèåÂàÜÊîØVQ-VAEËÅîÂêàÁºñÁ†ÅËßÜËßâÂä®ÊÄÅÂíåÊú∫Âô®‰∫∫ËøêÂä®Ôºå‰Ωú‰∏∫ËßÇÊµãÂíåÂä®‰ΩúÁöÑ‰∏≠Èó¥Ë°®Á§∫„ÄÇ
3. Âú®ÂÖ≠ÁßçÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ‰∏äËøõË°å‰∫ÜË∂ÖËøá14,000Ê¨°rolloutÁöÑÂÆûÈ™åÔºåXR-1Âú®120Â§ö‰∏™Êìç‰Ωú‰ªªÂä°‰∏≠‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÔºåÂπ∂Â±ïÁé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËßÑÊ®°Êú∫Âô®‰∫∫Êï∞ÊçÆÈõÜÂíåËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÊúÄÊñ∞ËøõÂ±ïÊé®Âä®‰∫ÜËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁöÑÁ†îÁ©∂„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑVLAÊ®°Âûã‰ªçÁÑ∂Èù¢‰∏¥‰∏§‰∏™Ê†πÊú¨ÊÄßÁöÑÊåëÊàòÔºöÔºàiÔºâ‰ªéÈ´òÁª¥ËßÇÊµã‰∏≠‰∫ßÁîüÁ≤æÁ°ÆÁöÑ‰ΩéÁ∫ßÂä®‰ΩúÔºåÔºàiiÔºâÂº•ÂêàË∑®ÂºÇÊûÑÊï∞ÊçÆÊ∫êÁöÑÈ¢ÜÂüüÂ∑ÆË∑ùÔºåÂåÖÊã¨‰∏çÂêåÁöÑÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÂíå‰∫∫Á±ªÊºîÁ§∫„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰ªéËßÜËßâÂä®ÊÄÅÊàñÊú∫Âô®‰∫∫Âä®‰Ωú‰∏≠ÁºñÁ†ÅÊΩúÂú®ÂèòÈáèÊù•ÊåáÂØºÁ≠ñÁï•Â≠¶‰π†Ôºå‰ΩÜÂÆÉ‰ª¨Êú™ËÉΩÂÖÖÂàÜÂà©Áî®Â§ßËßÑÊ®°ÂºÇÊûÑÊï∞ÊçÆÈõÜ‰∏≠Â≠òÂú®ÁöÑ‰∫íË°•Â§öÊ®°ÊÄÅÁü•ËØÜ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜX Robotic Model 1ÔºàXR-1ÔºâÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éË∑®‰∏çÂêåÊú∫Âô®‰∫∫„ÄÅ‰ªªÂä°ÂíåÁéØÂ¢ÉËøõË°åÈÄöÁî®‰∏îÂèØÊâ©Â±ïÁöÑVLAÂ≠¶‰π†ÁöÑÊñ∞Ê°ÜÊû∂„ÄÇXR-1ÂºïÂÖ•‰∫ÜÁªü‰∏ÄËßÜËßâ-ËøêÂä®‰ª£Á†ÅÔºàUVMCÔºâÔºåËøôÊòØ‰∏ÄÁßçÈÄöËøáÂèåÂàÜÊîØVQ-VAEÂ≠¶‰π†ÁöÑÁ¶ªÊï£ÊΩúÂú®Ë°®Á§∫ÔºåÂÆÉËÅîÂêàÁºñÁ†ÅËßÜËßâÂä®ÊÄÅÂíåÊú∫Âô®‰∫∫ËøêÂä®„ÄÇUVMCÈÄöËøáÔºàiÔºâÂÖÖÂΩìËßÇÊµãÂíåÂä®‰Ωú‰πãÈó¥ÁöÑ‰∏≠Èó¥Ë°®Á§∫Ôºå‰ª•ÂèäÔºàiiÔºâÂØπÈΩêÊù•Ëá™ÂºÇÊûÑÊï∞ÊçÆÊ∫êÁöÑÂ§öÊ®°ÊÄÅÂä®ÊÄÅ‰ø°ÊÅØ‰ª•ÊçïËé∑‰∫íË°•Áü•ËØÜÊù•Ëß£ÂÜ≥Ëøô‰∫õÊåëÊàò„ÄÇ‰∏∫‰∫ÜÊúâÊïàÂú∞Âà©Áî®UVMCÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏âÈò∂ÊÆµËÆ≠ÁªÉËåÉÂºèÔºöÔºàiÔºâËá™ÁõëÁù£UVMCÂ≠¶‰π†ÔºåÔºàiiÔºâÂú®Â§ßÂûãË∑®ÂΩ¢ÊÄÅÊú∫Âô®‰∫∫Êï∞ÊçÆÈõÜ‰∏äËøõË°åUVMCÂºïÂØºÁöÑÈ¢ÑËÆ≠ÁªÉÔºå‰ª•ÂèäÔºàiiiÔºâÁâπÂÆö‰∫é‰ªªÂä°ÁöÑÂêéËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÈÄöËøáÂú®ÂÖ≠Áßç‰∏çÂêåÁöÑÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ‰∏äËøõË°åÁöÑË∂ÖËøá14,000Ê¨°rolloutÁöÑÂπøÊ≥õÁúüÂÆû‰∏ñÁïåÂÆûÈ™åÈ™åËØÅ‰∫ÜXR-1ÔºåÊ∂µÁõñ‰∫Ü120Â§ö‰∏™‰∏çÂêåÁöÑÊìç‰Ωú‰ªªÂä°„ÄÇXR-1ÂßãÁªà‰ºò‰∫éÊúÄÂÖàËøõÁöÑÂü∫Á∫øÔºåÂ¶Ç$œÄ_{0.5}$Ôºå$œÄ_0$ÔºåRDTÔºåUniVLAÂíåGR00T-N1.5ÔºåÂêåÊó∂Â±ïÁ§∫‰∫ÜÂØπÊñ∞È¢ñÂØπË±°„ÄÅËÉåÊôØÂèòÂåñ„ÄÅÂπ≤Êâ∞Áâ©ÂíåÂÖâÁÖßÂèòÂåñÁöÑÂº∫Â§ßÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®Â§ÑÁêÜÈ´òÁª¥ËßÜËßâËæìÂÖ•Âπ∂ÁîüÊàêÁ≤æÁ°ÆÁöÑ‰ΩéÁ∫ßÂä®‰ΩúÊó∂Èù¢‰∏¥ÊåëÊàò„ÄÇÊ≠§Â§ñÔºåÁî±‰∫éÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÂíåÊï∞ÊçÆÊù•Ê∫êÁöÑÂ§öÊ†∑ÊÄßÔºåVLAÊ®°ÂûãÈöæ‰ª•Âú®ÂºÇÊûÑÊï∞ÊçÆ‰∏äËøõË°åÊúâÊïàËÆ≠ÁªÉÔºå‰ªéËÄåÈôêÂà∂‰∫ÜÂÖ∂ÈÄöÁî®ÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÁºñÁ†ÅËßÜËßâÂä®ÊÄÅÊàñÊú∫Âô®‰∫∫Âä®‰ΩúÁöÑÊΩúÂú®ÂèòÈáèÔºå‰ΩÜÊú™ËÉΩÂÖÖÂàÜÂà©Áî®ÂºÇÊûÑÊï∞ÊçÆÈõÜ‰∏≠Ëï¥Âê´ÁöÑ‰∫íË°•Â§öÊ®°ÊÄÅ‰ø°ÊÅØ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöXR-1ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ≠¶‰π†‰∏ÄÁßçÁªü‰∏ÄÁöÑËßÜËßâ-ËøêÂä®Ë°®ÂæÅÔºàUVMCÔºâÔºåËØ•Ë°®ÂæÅËÉΩÂ§üÂêåÊó∂ÁºñÁ†ÅËßÜËßâÂä®ÊÄÅÂíåÊú∫Âô®‰∫∫ËøêÂä®„ÄÇÈÄöËøáÂ∞ÜËßÜËßâÂíåËøêÂä®‰ø°ÊÅØÊò†Â∞ÑÂà∞ÂÖ±‰∫´ÁöÑÁ¶ªÊï£ÊΩúÂú®Á©∫Èó¥ÔºåUVMCÂèØ‰ª•‰Ωú‰∏∫ËßÇÊµãÂíåÂä®‰Ωú‰πãÈó¥ÁöÑÊ°•Ê¢ÅÔºå‰ªéËÄåÁÆÄÂåñÁ≠ñÁï•Â≠¶‰π†ËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåUVMCÁöÑËÆæËÆ°Êó®Âú®ÂØπÈΩêÊù•Ëá™‰∏çÂêåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÂíåÊï∞ÊçÆÊù•Ê∫êÁöÑÂ§öÊ®°ÊÄÅÂä®ÊÄÅ‰ø°ÊÅØÔºå‰ªéËÄå‰øÉËøõÁü•ËØÜËøÅÁßªÂíåÊ≥õÂåñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöXR-1ÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö(1) Ëá™ÁõëÁù£UVMCÂ≠¶‰π†Ôºö‰ΩøÁî®ÂèåÂàÜÊîØVQ-VAEÂ≠¶‰π†Áªü‰∏ÄÁöÑËßÜËßâ-ËøêÂä®‰ª£Á†ÅÔºåÂàÜÂà´ÁºñÁ†ÅËßÜËßâÂä®ÊÄÅÂíåÊú∫Âô®‰∫∫ËøêÂä®„ÄÇ(2) UVMCÂºïÂØºÁöÑÈ¢ÑËÆ≠ÁªÉÔºöÂú®Â§ßËßÑÊ®°Ë∑®ÂΩ¢ÊÄÅÊú∫Âô®‰∫∫Êï∞ÊçÆÈõÜ‰∏äÔºåÂà©Áî®UVMC‰Ωú‰∏∫‰∏≠Èó¥Ë°®Á§∫ËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂ≠¶‰π†ÈÄöÁî®ÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊäÄËÉΩ„ÄÇ(3) ‰ªªÂä°ÁâπÂÆöÂêéËÆ≠ÁªÉÔºöÂú®ÁâπÂÆö‰ªªÂä°‰∏äËøõË°åÂæÆË∞ÉÔºå‰ª•‰ºòÂåñÊ®°ÂûãÊÄßËÉΩ„ÄÇÊï¥‰∏™Ê°ÜÊû∂Êó®Âú®ÂÆûÁé∞Ë∑®‰∏çÂêåÊú∫Âô®‰∫∫„ÄÅ‰ªªÂä°ÂíåÁéØÂ¢ÉÁöÑÈÄöÁî®‰∏îÂèØÊâ©Â±ïÁöÑVLAÂ≠¶‰π†„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöXR-1ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÁªü‰∏ÄËßÜËßâ-ËøêÂä®‰ª£Á†ÅÔºàUVMCÔºâÔºåËøôÊòØ‰∏ÄÁßçÁ¶ªÊï£ÁöÑÊΩúÂú®Ë°®ÂæÅÔºåËÉΩÂ§üËÅîÂêàÁºñÁ†ÅËßÜËßâÂä®ÊÄÅÂíåÊú∫Âô®‰∫∫ËøêÂä®„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåUVMCËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®ÂºÇÊûÑÊï∞ÊçÆÈõÜ‰∏≠Ëï¥Âê´ÁöÑ‰∫íË°•Â§öÊ®°ÊÄÅ‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇÊ≠§Â§ñÔºå‰∏âÈò∂ÊÆµËÆ≠ÁªÉËåÉÂºè‰πü‰∏∫VLAÊ®°ÂûãÁöÑÂ≠¶‰π†Êèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöUVMCÈááÁî®ÂèåÂàÜÊîØVQ-VAEÁªìÊûÑÔºåÂàÜÂà´Â§ÑÁêÜËßÜËßâÂíåËøêÂä®‰ø°ÊÅØ„ÄÇÊØè‰∏™ÂàÜÊîØÂåÖÂê´ÁºñÁ†ÅÂô®„ÄÅÈáèÂåñÂô®ÂíåËß£Á†ÅÂô®„ÄÇÈáèÂåñÂô®Â∞ÜËøûÁª≠ÁöÑÊΩúÂú®ÂêëÈáèÊò†Â∞ÑÂà∞Á¶ªÊï£ÁöÑÁ†ÅÊú¨‰∏≠Ôºå‰ªéËÄåÂÆûÁé∞‰ø°ÊÅØÁöÑÂéãÁº©ÂíåÂØπÈΩê„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÈáçÊûÑÊçüÂ§±„ÄÅÈáèÂåñÊçüÂ§±Âíå‰∏ÄËá¥ÊÄßÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñUVMCÁöÑË°®ÂæÅËÉΩÂäõ„ÄÇÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºå‰ΩøÁî®UVMC‰Ωú‰∏∫‰∏≠Èó¥Ë°®Á§∫ÔºåÈÄöËøáÈ¢ÑÊµãÊú™Êù•ÁöÑÁä∂ÊÄÅÊàñÂä®‰ΩúÊù•Â≠¶‰π†Êú∫Âô®‰∫∫Êìç‰ΩúÊäÄËÉΩ„ÄÇÂú®ÂêéËÆ≠ÁªÉÈò∂ÊÆµÔºå‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÊàñÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÂØπÊ®°ÂûãËøõË°åÂæÆË∞É„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

XR-1Âú®ÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂú®ÂÖ≠Áßç‰∏çÂêåÁöÑÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ‰∏äËøõË°å‰∫ÜË∂ÖËøá14,000Ê¨°rolloutÔºåÊ∂µÁõñ‰∫Ü120Â§ö‰∏™‰∏çÂêåÁöÑÊìç‰Ωú‰ªªÂä°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåXR-1ÂßãÁªà‰ºò‰∫éÊúÄÂÖàËøõÁöÑÂü∫Á∫øÊ®°ÂûãÔºåÂ¶Ç$œÄ_{0.5}$Ôºå$œÄ_0$ÔºåRDTÔºåUniVLAÂíåGR00T-N1.5„ÄÇÊ≠§Â§ñÔºåXR-1ËøòÂ±ïÁ§∫‰∫ÜÂØπÊñ∞È¢ñÂØπË±°„ÄÅËÉåÊôØÂèòÂåñ„ÄÅÂπ≤Êâ∞Áâ©ÂíåÂÖâÁÖßÂèòÂåñÁöÑÂº∫Â§ßÊ≥õÂåñËÉΩÂäõÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

XR-1ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°ÔºåÂ¶ÇÁâ©‰ΩìÊäìÂèñ„ÄÅË£ÖÈÖç„ÄÅÂØºËà™Á≠â„ÄÇËØ•Ê®°ÂûãÂèØ‰ª•Â∫îÁî®‰∫éÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÂÆ∂Â∫≠ÊúçÂä°„ÄÅÂåªÁñó‰øùÂÅ•Á≠âÈ¢ÜÂüüÔºåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÂíåËá™‰∏ªÊÄß„ÄÇÊ≠§Â§ñÔºåXR-1ÁöÑÁ†îÁ©∂ÊàêÊûúËøòÂèØ‰ª•‰øÉËøõËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÂèëÂ±ïÔºå‰∏∫ÂÆûÁé∞Êõ¥ÈÄöÁî®ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÂ•†ÂÆöÂü∫Á°Ä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent progress in large-scale robotic datasets and vision-language models (VLMs) has advanced research on vision-language-action (VLA) models. However, existing VLA models still face two fundamental challenges: (i) producing precise low-level actions from high-dimensional observations, (ii) bridging domain gaps across heterogeneous data sources, including diverse robot embodiments and human demonstrations. Existing methods often encode latent variables from either visual dynamics or robotic actions to guide policy learning, but they fail to fully exploit the complementary multi-modal knowledge present in large-scale, heterogeneous datasets. In this work, we present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable VLA learning across diverse robots, tasks, and environments. XR-1 introduces the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and robotic motion. UVMC addresses these challenges by (i) serving as an intermediate representation between the observations and actions, and (ii) aligning multimodal dynamic information from heterogeneous data sources to capture complementary knowledge. To effectively exploit UVMC, we propose a three-stage training paradigm: (i) self-supervised UVMC learning, (ii) UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and (iii) task-specific post-training. We validate XR-1 through extensive real-world experiments with more than 14,000 rollouts on six different robot embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently outperforms state-of-the-art baselines such as $œÄ_{0.5}$, $œÄ_0$, RDT, UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel objects, background variations, distractors, and illumination changes. Our project is at https://xr-1-vla.github.io/.

