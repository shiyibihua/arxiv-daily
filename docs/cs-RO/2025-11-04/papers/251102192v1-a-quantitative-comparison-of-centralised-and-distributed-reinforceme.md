---
layout: default
title: A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms
---

# A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.02192" target="_blank" class="toolbar-btn">arXiv: 2511.02192v1</a>
    <a href="https://arxiv.org/pdf/2511.02192.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.02192v1" 
            onclick="toggleFavorite(this, '2511.02192v1', 'A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Linxin Hou, Qirui Wu, Zhihang Qin, Neil Banerjee, Yongxin Guo, Cecilia Laschi

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-04

**å¤‡æ³¨**: 7 pages, 4 figures, 2 tables, submitted to RoboSoft 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¯¹æ¯”é›†ä¸­å¼ä¸åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ æ§åˆ¶è½¯ä½“æœºæ¢°è‡‚ï¼Œä¸ºè½¯ä½“æœºå™¨äººæ§åˆ¶æä¾›è®¾è®¡æŒ‡å¯¼ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è½¯ä½“æœºå™¨äºº` `å¼ºåŒ–å­¦ä¹ ` `åˆ†å¸ƒå¼æ§åˆ¶` `é›†ä¸­å¼æ§åˆ¶` `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è½¯ä½“æœºå™¨äººæ§åˆ¶é¢ä¸´é«˜ç»´åº¦ã€éçº¿æ€§ç­‰æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ§åˆ¶æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹å¤æ‚ç¯å¢ƒã€‚
2. é‡‡ç”¨é›†ä¸­å¼å’Œåˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼Œå¯¹æ¯”ä¸¤ç§æ¶æ„åœ¨è½¯ä½“æœºæ¢°è‡‚æ§åˆ¶ä¸­çš„æ€§èƒ½å·®å¼‚ã€‚
3. å®éªŒè¡¨æ˜ï¼Œåˆ†å¸ƒå¼ç­–ç•¥åœ¨é«˜è‡ªç”±åº¦ä¸‹å…·æœ‰æ›´é«˜æ ·æœ¬æ•ˆç‡å’Œé²æ£’æ€§ï¼Œä½†é›†ä¸­å¼ç­–ç•¥è®­ç»ƒé€Ÿåº¦æ›´å¿«ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¯¹é›†ä¸­å¼å’Œåˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)æ¶æ„åœ¨æ§åˆ¶è½¯ä½“æœºæ¢°è‡‚æ–¹é¢çš„æ€§èƒ½è¿›è¡Œäº†å®šé‡æ¯”è¾ƒï¼Œè¯¥è½¯ä½“æœºæ¢°è‡‚åœ¨ä»¿çœŸç¯å¢ƒä¸­è¢«å»ºæ¨¡ä¸ºCosseratæ†ã€‚ä½¿ç”¨PyElasticaå’ŒOpenAI Gymæ¥å£ï¼Œæˆ‘ä»¬åœ¨ç›¸åŒçš„è®¡ç®—èµ„æºä¸‹è®­ç»ƒäº†å…¨å±€è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)æ§åˆ¶å™¨å’Œå¤šæ™ºèƒ½ä½“PPO(MAPPO)ã€‚ä¸¤ç§æ–¹æ³•éƒ½åŸºäºæ‰‹è‡‚å…·æœ‰nä¸ªå¯æ§éƒ¨åˆ†ã€‚è¯¥ç ”ç©¶ç³»ç»Ÿåœ°æ”¹å˜nï¼Œå¹¶è¯„ä¼°æ‰‹è‡‚åœ¨ä¸‰ç§åœºæ™¯ä¸­åˆ°è¾¾å›ºå®šç›®æ ‡çš„èƒ½åŠ›ï¼šé»˜è®¤åŸºçº¿æ¡ä»¶ã€ä»å¤–éƒ¨å¹²æ‰°ä¸­æ¢å¤ä»¥åŠé€‚åº”æ‰§è¡Œå™¨æ•…éšœã€‚ç”¨äºè¯„ä¼°çš„å®šé‡æŒ‡æ ‡æ˜¯å¹³å‡åŠ¨ä½œå¹…åº¦ã€å¹³å‡æœ€ç»ˆè·ç¦»ã€å¹³å‡episodeé•¿åº¦å’ŒæˆåŠŸç‡ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å¯æ§éƒ¨åˆ†æ•°é‡nâ‰¤4æ—¶ï¼Œåˆ†å¸ƒå¼ç­–ç•¥æ²¡æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨éå¸¸ç®€å•çš„ç³»ç»Ÿ(nâ‰¤2)ä¸­ï¼Œé›†ä¸­å¼ç­–ç•¥ä¼˜äºåˆ†å¸ƒå¼ç­–ç•¥ã€‚å½“nå¢åŠ åˆ°4<nâ‰¤12æ—¶ï¼Œåˆ†å¸ƒå¼ç­–ç•¥æ˜¾ç¤ºå‡ºæ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚åœ¨è¿™äº›ç³»ç»Ÿä¸­ï¼Œåˆ†å¸ƒå¼ç­–ç•¥åœ¨å±€éƒ¨å¯è§‚æµ‹æ€§ä¸‹æé«˜äº†æˆåŠŸç‡ã€å¼¹æ€§å’Œé²æ£’æ€§ï¼Œå¹¶åœ¨ç›¸åŒæ ·æœ¬é‡ä¸‹å®ç°äº†æ›´å¿«çš„æ”¶æ•›ã€‚ç„¶è€Œï¼Œé›†ä¸­å¼ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°äº†æ›´é«˜çš„æ—¶é—´æ•ˆç‡ï¼Œå› ä¸ºè®­ç»ƒç›¸åŒå¤§å°çš„æ ·æœ¬èŠ±è´¹çš„æ—¶é—´æ›´å°‘ã€‚è¿™äº›å‘ç°çªå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„è½¯ä½“æœºå™¨äººæ§åˆ¶ä¸­é›†ä¸­å¼å’Œåˆ†å¸ƒå¼ç­–ç•¥ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶ä¸ºæœªæ¥è½¯æ†çŠ¶æœºæ¢°æ‰‹ä¸­çš„sim-to-realè¿ç§»æä¾›äº†å¯æ“ä½œçš„è®¾è®¡æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è½¯ä½“æœºæ¢°è‡‚çš„æ§åˆ¶é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åœ¨é›†ä¸­å¼å’Œåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ æ¶æ„ä¹‹é—´åšå‡ºé€‰æ‹©ï¼Œä»¥å®ç°æ›´å¥½çš„æ§åˆ¶æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é«˜ç»´åº¦ã€å¤æ‚åŠ¨æ€çš„è½¯ä½“æœºå™¨äººæ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ¢ç´¢æ›´æœ‰æ•ˆçš„æ§åˆ¶ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æ¯”é›†ä¸­å¼å’Œåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ§åˆ¶è½¯ä½“æœºæ¢°è‡‚æ—¶çš„æ€§èƒ½ï¼Œæ­ç¤ºä¸¤ç§æ¶æ„çš„ä¼˜ç¼ºç‚¹ï¼Œä»è€Œä¸ºè½¯ä½“æœºå™¨äººæ§åˆ¶ç³»ç»Ÿçš„è®¾è®¡æä¾›æŒ‡å¯¼ã€‚åˆ†å¸ƒå¼æ–¹æ³•é€šè¿‡å±€éƒ¨è§‚æµ‹å’Œæ§åˆ¶ï¼Œæœ‰æœ›æé«˜é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ï¼š1) ä½¿ç”¨PyElasticaå¯¹è½¯ä½“æœºæ¢°è‡‚è¿›è¡Œå»ºæ¨¡ï¼›2) é€šè¿‡OpenAI Gymæ¥å£æ„å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼›3) åˆ†åˆ«è®­ç»ƒé›†ä¸­å¼PPOå’Œåˆ†å¸ƒå¼MAPPOæ§åˆ¶å™¨ï¼›4) åœ¨ä¸åŒåœºæ™¯ä¸‹è¯„ä¼°æ§åˆ¶å™¨çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åŸºçº¿æ¡ä»¶ã€æŠ—å¹²æ‰°å’Œå®¹é”™èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹é›†ä¸­å¼å’Œåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ åœ¨è½¯ä½“æœºå™¨äººæ§åˆ¶ä¸­çš„æ€§èƒ½è¿›è¡Œäº†å®šé‡æ¯”è¾ƒï¼Œå¹¶åˆ†æäº†ä¸åŒæ§åˆ¶å•å…ƒæ•°é‡ä¸‹ä¸¤ç§æ¶æ„çš„ä¼˜åŠ£ã€‚è¿™ä¸ºè½¯ä½“æœºå™¨äººæ§åˆ¶ç­–ç•¥çš„é€‰æ‹©æä¾›äº†ç†è®ºä¾æ®å’Œå®éªŒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Cosseratæ†æ¨¡å‹æ¨¡æ‹Ÿè½¯ä½“æœºæ¢°è‡‚ï¼›2) é‡‡ç”¨PPOå’ŒMAPPOç®—æ³•è¿›è¡Œè®­ç»ƒï¼›3) é€šè¿‡å¹³å‡åŠ¨ä½œå¹…åº¦ã€å¹³å‡æœ€ç»ˆè·ç¦»ã€å¹³å‡episodeé•¿åº¦å’ŒæˆåŠŸç‡ç­‰æŒ‡æ ‡è¯„ä¼°æ€§èƒ½ï¼›4) ç³»ç»Ÿåœ°æ”¹å˜å¯æ§éƒ¨åˆ†çš„æ•°é‡nï¼Œä»¥ç ”ç©¶å…¶å¯¹æ§åˆ¶æ€§èƒ½çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å¯æ§éƒ¨åˆ†æ•°é‡nâ‰¤4æ—¶ï¼Œé›†ä¸­å¼ç­–ç•¥åœ¨ç®€å•ç³»ç»Ÿä¸­è¡¨ç°æ›´ä¼˜ã€‚å½“4<nâ‰¤12æ—¶ï¼Œåˆ†å¸ƒå¼ç­–ç•¥å±•ç°å‡ºæ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€æˆåŠŸç‡å’Œé²æ£’æ€§ã€‚é›†ä¸­å¼ç­–ç•¥è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œä½†åˆ†å¸ƒå¼ç­–ç•¥åœ¨å¤æ‚ç³»ç»Ÿä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œä¸ºè½¯ä½“æœºå™¨äººæ§åˆ¶ç­–ç•¥é€‰æ‹©æä¾›äº†ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºåŒ»ç–—æœºå™¨äººã€åº·å¤æœºå™¨äººã€æœæ•‘æœºå™¨äººç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æŸ”æ€§å’Œé€‚åº”æ€§çš„å¤æ‚ç¯å¢ƒä¸­ã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„æ§åˆ¶æ¶æ„ï¼Œå¯ä»¥æé«˜è½¯ä½“æœºå™¨äººçš„æ“ä½œç²¾åº¦ã€é²æ£’æ€§å’Œå®‰å…¨æ€§ï¼Œä»è€Œæ›´å¥½åœ°å®Œæˆå„ç§ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents a quantitative comparison between centralised and distributed multi-agent reinforcement learning (MARL) architectures for controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical budgets. Both approaches are based on the arm having $n$ number of controlled sections. The study systematically varies $n$ and evaluates the performance of the arm to reach a fixed target in three scenarios: default baseline condition, recovery from external disturbance, and adaptation to actuator failure. Quantitative metrics used for the evaluation are mean action magnitude, mean final distance, mean episode length, and success rate. The results show that there are no significant benefits of the distributed policy when the number of controlled sections $n\le4$. In very simple systems, when $n\le2$, the centralised policy outperforms the distributed one. When $n$ increases to $4< n\le 12$, the distributed policy shows a high sample efficiency. In these systems, distributed policy promotes a stronger success rate, resilience, and robustness under local observability and yields faster convergence given the same sample size. However, centralised policies achieve much higher time efficiency during training as it takes much less time to train the same size of samples. These findings highlight the trade-offs between centralised and distributed policy in reinforcement learning-based control for soft robotic systems and provide actionable design guidance for future sim-to-real transfer in soft rod-like manipulators.

