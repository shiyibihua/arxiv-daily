---
layout: default
title: LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation
---

# LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation

**arXiv**: [2511.02239v1](https://arxiv.org/abs/2511.02239) | [PDF](https://arxiv.org/pdf/2511.02239.pdf)

**ä½œè€…**: Youngjin Hong, Houjian Yu, Mingen Li, Changhyun Choi

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-04

**å¤‡æ³¨**: Preprint. Project page: https://vla2026.github.io/LACY/

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://vla2026.github.io/LACY/)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LACYï¼šåŸºäºŽè§†è§‰-è¯­è¨€æ¨¡åž‹çš„è¯­è¨€-åŠ¨ä½œå¾ªçŽ¯ï¼Œç”¨äºŽè‡ªæå‡çš„æœºå™¨äººæ“ä½œ**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€æ¨¡åž‹` `è¯­è¨€-åŠ¨ä½œå¾ªçŽ¯` `è‡ªç›‘ç£å­¦ä¹ ` `ä¸»åŠ¨å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æœºå™¨äººæ“ä½œç­–ç•¥ç¼ºä¹ä¸Šä¸‹æ–‡ç†è§£ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ï¼Œä¸”æ— æ³•è§£é‡Šè‡ªèº«è¡Œä¸ºã€‚
2. LACYæ¡†æž¶é€šè¿‡åœ¨è§†è§‰-è¯­è¨€æ¨¡åž‹ä¸­å­¦ä¹ åŒå‘æ˜ å°„ï¼ˆL2Aå’ŒA2Lï¼‰æ¥è§£å†³æ­¤é—®é¢˜ã€‚
3. LACYé€šè¿‡è‡ªç›‘ç£å¾ªçŽ¯ç”Ÿæˆå’Œè¿‡æ»¤è®­ç»ƒæ•°æ®ï¼Œæ— éœ€é¢å¤–äººå·¥æ ‡æ³¨å³å¯æå‡æ¨¡åž‹æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†å­¦ä¹ æœºå™¨äººæ“ä½œçš„é€šç”¨ç­–ç•¥ï¼Œè¶Šæ¥è¶Šå¤šåœ°ä¾èµ–äºŽå°†è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°åŠ¨ä½œ(L2A)çš„å¤§è§„æ¨¡æ¨¡åž‹ã€‚ç„¶è€Œï¼Œè¿™ç§å•å‘èŒƒå¼äº§ç”Ÿçš„ç­–ç•¥é€šå¸¸åœ¨æ²¡æœ‰æ›´æ·±å±‚æ¬¡çš„ä¸Šä¸‹æ–‡ç†è§£çš„æƒ…å†µä¸‹æ‰§è¡Œä»»åŠ¡ï¼Œé™åˆ¶äº†å®ƒä»¬çš„æ³›åŒ–æˆ–è§£é‡Šå…¶è¡Œä¸ºçš„èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå°†åŠ¨ä½œæ˜ å°„å›žè¯­è¨€(A2L)çš„äº’è¡¥æŠ€èƒ½å¯¹äºŽå¼€å‘æ›´å…¨é¢çš„åŸºç¡€è‡³å…³é‡è¦ã€‚ä¸€ä¸ªæ—¢èƒ½è¡ŒåŠ¨åˆèƒ½è§£é‡Šå…¶è¡ŒåŠ¨çš„æ™ºèƒ½ä½“å¯ä»¥å½¢æˆæ›´ä¸°å¯Œçš„å†…éƒ¨è¡¨å¾ï¼Œå¹¶ä¸ºè‡ªç›‘ç£å­¦ä¹ è§£é”æ–°çš„èŒƒå¼ã€‚æˆ‘ä»¬å¼•å…¥äº†LACY(è¯­è¨€-åŠ¨ä½œå¾ªçŽ¯)ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æž¶ï¼Œå¯ä»¥åœ¨å•ä¸ªè§†è§‰-è¯­è¨€æ¨¡åž‹ä¸­å­¦ä¹ è¿™ç§åŒå‘æ˜ å°„ã€‚LACYåœ¨ä¸‰ä¸ªååŒä»»åŠ¡ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼šä»Žè¯­è¨€ç”Ÿæˆå‚æ•°åŒ–åŠ¨ä½œ(L2A)ï¼Œç”¨è¯­è¨€è§£é‡Šè§‚å¯Ÿåˆ°çš„åŠ¨ä½œ(A2L)ï¼Œä»¥åŠéªŒè¯ä¸¤ä¸ªè¯­è¨€æè¿°ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§(L2C)ã€‚è¿™ä½¿å¾—ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„å¾ªçŽ¯èƒ½å¤Ÿé€šè¿‡ä¸»åŠ¨å¢žå¼ºç­–ç•¥è‡ªä¸»ç”Ÿæˆå’Œè¿‡æ»¤æ–°çš„è®­ç»ƒæ•°æ®ï¼Œä»Žè€Œåœ¨æ²¡æœ‰é¢å¤–äººå·¥æ ‡ç­¾çš„æƒ…å†µä¸‹æ”¹è¿›æ¨¡åž‹ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žä¸–ç•Œä¸­çš„æŠ“å–æ”¾ç½®ä»»åŠ¡ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒLACYå¹³å‡æé«˜äº†56.46%çš„ä»»åŠ¡æˆåŠŸçŽ‡ï¼Œå¹¶ä¸ºæœºå™¨äººæ“ä½œäº§ç”Ÿäº†æ›´å¼ºå¤§çš„è¯­è¨€-åŠ¨ä½œåŸºç¡€ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽè¯­è¨€åˆ°åŠ¨ä½œ(L2A)çš„æœºå™¨äººæ“ä½œæ–¹æ³•ï¼Œé€šå¸¸ç¼ºä¹å¯¹ä»»åŠ¡ä¸Šä¸‹æ–‡çš„æ·±å…¥ç†è§£ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå¹¶ä¸”éš¾ä»¥è§£é‡Šå…¶è¡Œä¸ºã€‚è¿™ç§å•å‘æ˜ å°„å¿½ç•¥äº†åŠ¨ä½œåˆ°è¯­è¨€(A2L)çš„åé¦ˆï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“å½¢æˆæ›´ä¸°å¯Œçš„å†…éƒ¨è¡¨å¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLACYçš„æ ¸å¿ƒæ€è·¯æ˜¯æž„å»ºä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ŒåŒæ—¶å­¦ä¹ è¯­è¨€åˆ°åŠ¨ä½œ(L2A)å’ŒåŠ¨ä½œåˆ°è¯­è¨€(A2L)çš„åŒå‘æ˜ å°„ã€‚é€šè¿‡è¿™ç§åŒå‘å¾ªçŽ¯ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°ç†è§£ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼Œå¹¶æé«˜æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼ŒLACYè¿˜å¼•å…¥äº†è¯­è¨€ä¸€è‡´æ€§éªŒè¯(L2C)ä»»åŠ¡ï¼Œè¿›ä¸€æ­¥å¢žå¼ºäº†æ¨¡åž‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šLACYçš„æŠ€æœ¯æ¡†æž¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¯­è¨€åˆ°åŠ¨ä½œ(L2A)æ¨¡å—ã€åŠ¨ä½œåˆ°è¯­è¨€(A2L)æ¨¡å—å’Œè¯­è¨€ä¸€è‡´æ€§éªŒè¯(L2C)æ¨¡å—ã€‚L2Aæ¨¡å—è´Ÿè´£æ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆå‚æ•°åŒ–çš„åŠ¨ä½œåºåˆ—ã€‚A2Læ¨¡å—è´Ÿè´£æ ¹æ®è§‚å¯Ÿåˆ°çš„åŠ¨ä½œåºåˆ—ç”Ÿæˆè¯­è¨€æè¿°ã€‚L2Cæ¨¡å—è´Ÿè´£éªŒè¯ä¸¤ä¸ªè¯­è¨€æè¿°ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¿™ä¸‰ä¸ªæ¨¡å—åœ¨ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡åž‹ä¸­è¿›è¡Œè”åˆè®­ç»ƒï¼Œå½¢æˆä¸€ä¸ªè¯­è¨€-åŠ¨ä½œå¾ªçŽ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šLACYæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå…¶åŒå‘è¯­è¨€-åŠ¨ä½œå¾ªçŽ¯çš„å­¦ä¹ èŒƒå¼ã€‚ä¸Žä¼ ç»Ÿçš„å•å‘L2Aæ–¹æ³•ç›¸æ¯”ï¼ŒLACYé€šè¿‡å¼•å…¥A2Lå’ŒL2Cä»»åŠ¡ï¼Œå¢žå¼ºäº†æ¨¡åž‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLACYè¿˜æå‡ºäº†ä¸€ç§ä¸»åŠ¨å¢žå¼ºç­–ç•¥ï¼Œé€šè¿‡è‡ªä¸»ç”Ÿæˆå’Œè¿‡æ»¤æ–°çš„è®­ç»ƒæ•°æ®ï¼Œå®žçŽ°äº†æ¨¡åž‹çš„è‡ªæå‡ï¼Œæ— éœ€é¢å¤–çš„äººå·¥æ ‡æ³¨ã€‚

**å…³é”®è®¾è®¡**ï¼šLACYçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformeræž¶æž„ä½œä¸ºè§†è§‰-è¯­è¨€æ¨¡åž‹çš„åŸºç¡€ï¼Œä»¥å®žçŽ°æ›´å¥½çš„å¤šæ¨¡æ€èžåˆï¼›2) è®¾è®¡äº†å‚æ•°åŒ–çš„åŠ¨ä½œè¡¨ç¤ºï¼Œä»¥ä¾¿L2Aæ¨¡å—ç”Ÿæˆå¯æ‰§è¡Œçš„åŠ¨ä½œåºåˆ—ï¼›3) é‡‡ç”¨äº†å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°æ¥è®­ç»ƒA2Læ¨¡å—ï¼Œä»¥æé«˜è¯­è¨€æè¿°çš„å‡†ç¡®æ€§ï¼›4) ä½¿ç”¨äº†åŸºäºŽç½®ä¿¡åº¦çš„è¿‡æ»¤ç­–ç•¥æ¥é€‰æ‹©é«˜è´¨é‡çš„è‡ªç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

LACYåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žä¸–ç•Œçš„æŠ“å–æ”¾ç½®ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­ï¼ŒLACYçš„ä»»åŠ¡æˆåŠŸçŽ‡å¹³å‡æé«˜äº†56.46%ã€‚åœ¨çœŸå®žä¸–ç•ŒçŽ¯å¢ƒä¸­ï¼ŒLACYä¹Ÿè¡¨çŽ°å‡ºä¼˜äºŽåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚è¿™äº›å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒLACYæ¡†æž¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æœºå™¨äººæ“ä½œçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

LACYæ¡†æž¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºŽå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººå’ŒåŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººçš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼ŒLACYå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„çŽ¯å¢ƒï¼Œå¹¶ä¸Žäººç±»è¿›è¡Œæ›´è‡ªç„¶çš„äº¤äº’ã€‚æ­¤å¤–ï¼ŒLACYçš„è‡ªæå‡èƒ½åŠ›å¯ä»¥é™ä½Žå¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä»Žè€ŒåŠ é€Ÿæœºå™¨äººæŠ€æœ¯çš„æ™®åŠã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning generalizable policies for robotic manipulation increasingly relies on large-scale models that map language instructions to actions (L2A). However, this one-way paradigm often produces policies that execute tasks without deeper contextual understanding, limiting their ability to generalize or explain their behavior. We argue that the complementary skill of mapping actions back to language (A2L) is essential for developing more holistic grounding. An agent capable of both acting and explaining its actions can form richer internal representations and unlock new paradigms for self-supervised learning. We introduce LACY (Language-Action Cycle), a unified framework that learns such bidirectional mappings within a single vision-language model. LACY is jointly trained on three synergistic tasks: generating parameterized actions from language (L2A), explaining observed actions in language (A2L), and verifying semantic consistency between two language descriptions (L2C). This enables a self-improving cycle that autonomously generates and filters new training data through an active augmentation strategy targeting low-confidence cases, thereby improving the model without additional human labels. Experiments on pick-and-place tasks in both simulation and the real world show that LACY improves task success rates by 56.46% on average and yields more robust language-action grounding for robotic manipulation. Project page: https://vla2026.github.io/LACY/

