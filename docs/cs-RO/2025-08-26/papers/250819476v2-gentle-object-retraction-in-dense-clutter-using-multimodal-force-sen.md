---
layout: default
title: Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning
---

# Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19476" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19476v2</a>
  <a href="https://arxiv.org/pdf/2508.19476.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19476v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19476v2', 'Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dane Brouwer, Joshua Citron, Heather Nolte, Jeannette Bohg, Mark Cutkosky

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-11-30)

**å¤‡æ³¨**: Accepted in IEEE Robotics and Automation Letters (RA-L)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€åŠ›æ„ŸçŸ¥ä¸æ¨¡ä»¿å­¦ä¹ ä»¥è§£å†³å¯†é›†æ‚ç‰©ä¸­çš„ç‰©ä½“æå–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ„ŸçŸ¥` `æ¨¡ä»¿å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `åŠ›æ„ŸçŸ¥` `å¯†é›†æ‚ç‰©æå–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººåœ¨å¯†é›†æ‚ç‰©ä¸­æå–ç‰©ä½“æ—¶ï¼Œç¼ºä¹æœ‰æ•ˆçš„åŠ›æ„ŸçŸ¥æ‰‹æ®µï¼Œå¯¼è‡´æ“ä½œå¤±è´¥ç‡é«˜ã€‚
2. æœ¬æ–‡æå‡ºç»“åˆå¤šæ¨¡æ€åŠ›æ„ŸçŸ¥ä¸æ¨¡ä»¿å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»çš„è§¦è§‰å’Œè§†è§‰ç»éªŒæ¥è®­ç»ƒæœºå™¨äººã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨åŠ›æ„ŸçŸ¥çš„ç­–ç•¥åœ¨40ä¸ªæœªè§ç¯å¢ƒé…ç½®ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æˆåŠŸç‡å’Œæ›´å¿«çš„å®Œæˆæ—¶é—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ—¥å¸¸ç¯å¢ƒä¸­ï¼Œå¯†é›†çš„å¯ç§»åŠ¨ç‰©ä½“å¸¸è§äºå®¶åº­æ©±æŸœå’Œä»“åº“è´§æ¶ã€‚æœºå™¨äººåœ¨æ­¤ç±»ç¯å¢ƒä¸­å®‰å…¨æå–ç‰©ä½“é¢ä¸´æŒ‘æˆ˜ï¼Œè€Œäººç±»åˆ™ä¾é è§†è§‰å’ŒéæŠ“å–è§¦è§‰æ„ŸçŸ¥æ¥å®Œæˆæ­¤ä»»åŠ¡ã€‚æœ¬æ–‡ç ”ç©¶äº†æ¥è§¦åŠ›æ„ŸçŸ¥åœ¨è®­ç»ƒæœºå™¨äººè½»æŸ”åœ°ä»å—é™æ‚ç‰©ä¸­æå–ç‰©ä½“çš„ä½œç”¨ã€‚æˆ‘ä»¬ç»“åˆäº†å¤šç§æ„ŸçŸ¥æ–¹å¼ï¼ŒåŒ…æ‹¬â€œæ‰‹çœ¼â€è§†è§‰ã€èº«ä½“æ„ŸçŸ¥ã€éæŠ“å–ä¸‰è½´è§¦è§‰æ„ŸçŸ¥ç­‰ï¼Œåˆ©ç”¨æ¨¡ä»¿å­¦ä¹ ä»éšæœºç”Ÿæˆçš„åœºæ™¯ä¸­è®­ç»ƒç­–ç•¥ï¼Œå¹¶é€šè¿‡æ¶ˆèç ”ç©¶è¯„ä¼°åŠ›æ„ŸçŸ¥ä¿¡æ¯çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åŠ›æ„ŸçŸ¥çš„ç­–ç•¥åœ¨æˆåŠŸç‡å’Œå®Œæˆæ—¶é—´ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œæœ€ä½³ç­–ç•¥ç»“åˆäº†è§¦è§‰å’ŒåŠ›çŸ©ä¿¡æ¯ï¼Œç›¸è¾ƒäºæ— åŠ›ä¿¡æ¯çš„åŸºçº¿æå‡äº†80%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨å¯†é›†æ‚ç‰©ä¸­æå–ç‰©ä½“æ—¶çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†åŠ›æ„ŸçŸ¥çš„é‡è¦æ€§ï¼Œå¯¼è‡´æ“ä½œå¤±è´¥å’Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤šæ¨¡æ€åŠ›æ„ŸçŸ¥ï¼ŒåŒ…æ‹¬è§†è§‰ã€èº«ä½“æ„ŸçŸ¥å’Œè§¦è§‰ï¼Œç»“åˆæ¨¡ä»¿å­¦ä¹ ï¼Œæ¨¡æ‹Ÿäººç±»åœ¨å¤æ‚ç¯å¢ƒä¸­æå–ç‰©ä½“çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜æœºå™¨äººçš„æ“ä½œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ¨¡ä»¿å­¦ä¹ ç­–ç•¥è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œé€šè¿‡å¤šç§æ„ŸçŸ¥æ–¹å¼æ”¶é›†æ•°æ®ï¼Œç„¶ååˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒæœºå™¨äººç­–ç•¥ï¼Œæœ€ååœ¨æœªè§ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†å¤šç§åŠ›æ„ŸçŸ¥ä¿¡æ¯ï¼ˆå¦‚è§¦è§‰å’ŒåŠ›çŸ©ï¼‰ç»“åˆä½¿ç”¨ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å…¨é¢çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ä¸åŒæ„ŸçŸ¥ä¿¡æ¯çš„æƒé‡ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™ä½¿ç”¨äº†æ·±åº¦å·ç§¯ç½‘ç»œæ¥å¤„ç†è§†è§‰è¾“å…¥ï¼Œç»“åˆè§¦è§‰ä¿¡æ¯è¿›è¡Œå†³ç­–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨åŠ›æ„ŸçŸ¥çš„ç­–ç•¥åœ¨40ä¸ªæœªè§ç¯å¢ƒé…ç½®ä¸­ï¼ŒæˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œå®Œæˆæ—¶é—´ç¼©çŸ­ï¼Œæœ€ä½³ç­–ç•¥ç»“åˆè§¦è§‰å’ŒåŠ›çŸ©ä¿¡æ¯ï¼Œç›¸è¾ƒäºæ— åŠ›ä¿¡æ¯çš„åŸºçº¿æå‡äº†80%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®¶åº­æœåŠ¡æœºå™¨äººã€ä»“å‚¨è‡ªåŠ¨åŒ–å’Œæ•‘æ´æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç‰©ä½“æå–èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½æœºå™¨äººåœ¨æ›´å¤šåœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dense collections of movable objects are common in everyday spaces-from cabinets in a home to shelves in a warehouse. Safely retracting objects from such collections is difficult for robots, yet people do it frequently, leveraging learned experience in tandem with vision and non-prehensile tactile sensing on the sides and backs of their hands and arms. We investigate the role of contact force sensing for training robots to gently reach into constrained clutter and extract objects. The available sensing modalities are (1) "eye-in-hand" vision, (2) proprioception, (3) non-prehensile triaxial tactile sensing, (4) contact wrenches estimated from joint torques, and (5) a measure of object acquisition obtained by monitoring the vacuum line of a suction cup. We use imitation learning to train policies from a set of demonstrations on randomly generated scenes, then conduct an ablation study of wrench and tactile information. We evaluate each policy's performance across 40 unseen environment configurations. Policies employing any force sensing show fewer excessive force failures, an increased overall success rate, and faster completion times. The best performance is achieved using both tactile and wrench information, producing an 80% improvement above the baseline without force information.

