---
layout: default
title: LaVA-Man: Learning Visual Action Representations for Robot Manipulation
---

# LaVA-Man: Learning Visual Action Representations for Robot Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19391" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19391v2</a>
  <a href="https://arxiv.org/pdf/2508.19391.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19391v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19391v2', 'LaVA-Man: Learning Visual Action Representations for Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chaoran Zhu, Hengyi Wang, Yik Lung Pang, Changjae Oh

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-09-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLaVA-Manä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„è§†è§‰-æ–‡æœ¬ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰-æ–‡æœ¬ç†è§£` `è‡ªç›‘ç£å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ•°æ®é›†æ„å»º` `åŠ¨ä½œè¡¨ç¤º` `æ™ºèƒ½æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šè¿‡ä¸¤æ­¥æ³•å¤„ç†è§†è§‰è§‚å¯Ÿä¸æ–‡æœ¬æŒ‡ä»¤çš„å…³ç³»ï¼Œå¯¼è‡´æ“ä½œç²¾åº¦ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è‡ªç›‘ç£ä»»åŠ¡é‡å»ºç›®æ ‡å›¾åƒï¼Œä»è€Œå­¦ä¹ è§†è§‰-åŠ¨ä½œè¡¨ç¤ºï¼Œæ— éœ€æœºå™¨äººåŠ¨ä½œç›‘ç£ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººéªŒè¯ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-æ–‡æœ¬ç†è§£å¯¹äºè¯­è¨€å¼•å¯¼çš„æœºå™¨äººæ“ä½œè‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹æ¥æµ‹é‡ç¼–ç çš„è§†è§‰è§‚å¯Ÿä¸æ–‡æœ¬æŒ‡ä»¤ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶è®­ç»ƒæ¨¡å‹å°†è¿™ç§ç›¸ä¼¼æ€§æ˜ å°„åˆ°æœºå™¨äººåŠ¨ä½œã€‚ç„¶è€Œï¼Œè¿™ç§ä¸¤æ­¥æ³•é™åˆ¶äº†æ¨¡å‹æ•æ‰è§†è§‰è§‚å¯Ÿä¸æ–‡æœ¬æŒ‡ä»¤ä¹‹é—´çš„å…³ç³»ï¼Œå¯¼è‡´æ“ä½œä»»åŠ¡çš„ç²¾åº¦é™ä½ã€‚æœ¬æ–‡æå‡ºé€šè¿‡è‡ªç›‘ç£çš„å‰ç½®ä»»åŠ¡å­¦ä¹ è§†è§‰-æ–‡æœ¬å…³è”ï¼šåœ¨è¾“å…¥å›¾åƒå’Œæ–‡æœ¬æŒ‡ä»¤çš„æ¡ä»¶ä¸‹é‡å»ºæ©è”½çš„ç›®æ ‡å›¾åƒã€‚è¿™ä¸€æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æœºå™¨äººåŠ¨ä½œç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ è§†è§‰-åŠ¨ä½œè¡¨ç¤ºï¼Œå¹¶å¯é€šè¿‡å°‘é‡ç¤ºä¾‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†Omni-Object Pick-and-Placeæ•°æ®é›†ï¼ŒåŒ…å«180ä¸ªç‰©ä½“ç±»åˆ«å’Œ3200ä¸ªå®ä¾‹åŠå…¶å¯¹åº”çš„æ–‡æœ¬æŒ‡ä»¤ï¼Œæ”¯æŒæ¨¡å‹è·å–å¤šæ ·çš„ç‰©ä½“å…ˆéªŒå¹¶å…¨é¢è¯„ä¼°å…¶æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•ä¸­è§†è§‰è§‚å¯Ÿä¸æ–‡æœ¬æŒ‡ä»¤ä¹‹é—´å…³ç³»æ•æ‰ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„ä¸¤æ­¥æ³•é™åˆ¶äº†æ¨¡å‹çš„è¡¨ç°ï¼Œå¯¼è‡´æ“ä½œç²¾åº¦é™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è‡ªç›‘ç£çš„å‰ç½®ä»»åŠ¡æ¥å­¦ä¹ è§†è§‰-æ–‡æœ¬å…³è”ï¼Œå…·ä½“æ˜¯é‡å»ºæ©è”½çš„ç›®æ ‡å›¾åƒã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æœºå™¨äººåŠ¨ä½œç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ åˆ°æœ‰æ•ˆçš„è§†è§‰-åŠ¨ä½œè¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å›¾åƒå’Œæ–‡æœ¬æŒ‡ä»¤ï¼Œæ¨¡å‹é€šè¿‡é‡å»ºç›®æ ‡å›¾åƒæ¥å­¦ä¹ è§†è§‰-åŠ¨ä½œè¡¨ç¤ºã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è§†è§‰ç¼–ç å™¨ã€æ–‡æœ¬ç¼–ç å™¨å’Œé‡å»ºç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡è‡ªç›‘ç£å­¦ä¹ å®ç°è§†è§‰-åŠ¨ä½œè¡¨ç¤ºçš„å­¦ä¹ ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„ä¾èµ–äºç›‘ç£ä¿¡å·çš„æ–¹æ³•ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é‡å»ºä»»åŠ¡ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„èåˆï¼Œç¡®ä¿äº†ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ä¸æ•´åˆã€‚å®éªŒä¸­ä½¿ç”¨äº†å¤šæ ·çš„å‚æ•°è®¾ç½®ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€ææ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººéªŒè¯ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’ŒæœåŠ¡æœºå™¨äººç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜æœºå™¨äººå¯¹è§†è§‰å’Œæ–‡æœ¬æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æå‡å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œæ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½åŒ–åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual-textual understanding is essential for language-guided robot manipulation. Recent works leverage pre-trained vision-language models to measure the similarity between encoded visual observations and textual instructions, and then train a model to map this similarity to robot actions. However, this two-step approach limits the model to capture the relationship between visual observations and textual instructions, leading to reduced precision in manipulation tasks. We propose to learn visual-textual associations through a self-supervised pretext task: reconstructing a masked goal image conditioned on an input image and textual instructions. This formulation allows the model to learn visual-action representations without robot action supervision. The learned representations can then be fine-tuned for manipulation tasks with only a few demonstrations. We also introduce the \textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot tabletop manipulation episodes, including 180 object classes and 3,200 instances with corresponding textual instructions. This dataset enables the model to acquire diverse object priors and allows for a more comprehensive evaluation of its generalisation capability across object instances. Experimental results on the five benchmarks, including both simulated and real-robot validations, demonstrate that our method outperforms prior art.

