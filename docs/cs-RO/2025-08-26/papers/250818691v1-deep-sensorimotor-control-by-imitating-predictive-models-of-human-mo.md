---
layout: default
title: Deep Sensorimotor Control by Imitating Predictive Models of Human Motion
---

# Deep Sensorimotor Control by Imitating Predictive Models of Human Motion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18691" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18691v1</a>
  <a href="https://arxiv.org/pdf/2508.18691.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18691v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18691v1', 'Deep Sensorimotor Control by Imitating Predictive Models of Human Motion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Himanshu Gaurav Singh, Pieter Abbeel, Jitendra Malik, Antonio Loquercio

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

**å¤‡æ³¨**: Blog Post: https://hgaurav2k.github.io/trackr/

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://jirl-upenn.github.io/track_reward/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šè¿‡æ¨¡ä»¿äººç±»è¿åŠ¨é¢„æµ‹æ¨¡å‹çš„ä¼ æ„Ÿå™¨è¿åŠ¨æ§åˆ¶æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æœºå™¨äººå­¦ä¹ ` `äººç±»è¿åŠ¨é¢„æµ‹` `ä¼ æ„Ÿå™¨è¿åŠ¨æ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `é›¶æ ·æœ¬å­¦ä¹ ` `äººæœºåä½œ` `æ•°æ®é©±åŠ¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨äººç±»ä¸ç¯å¢ƒäº¤äº’æ•°æ®è¿›è¡Œæœºå™¨äººå­¦ä¹ æ—¶ï¼Œé¢ä¸´ç€æ¢¯åº¦é‡å®šå‘å’Œå¯¹æŠ—æŸå¤±çš„é™åˆ¶ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®é›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ¨¡ä»¿äººç±»è¿åŠ¨çš„é¢„æµ‹æ¨¡å‹æ¥è®­ç»ƒä¼ æ„Ÿå™¨è¿åŠ¨ç­–ç•¥ï¼Œåˆ©ç”¨äººç±»æ•°æ®çš„è¿åŠ¨é¢„æµ‹åœ¨æœºå™¨äººæ•°æ®ä¸Šè¿›è¡Œé›¶æ ·æœ¬å­¦ä¹ ã€‚
3. å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæœºå™¨äººå’Œä»»åŠ¡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰åŸºçº¿ï¼Œä¸”èƒ½å¤Ÿæ›¿ä»£å¤æ‚çš„å¥–åŠ±è®¾è®¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æœºå™¨äººä¸äººç±»ä¹‹é—´çš„ä½“ç°å·®è·ç¼©å°ï¼Œåˆ©ç”¨äººç±»ä¸ç¯å¢ƒäº’åŠ¨çš„æ•°æ®é›†è¿›è¡Œæœºå™¨äººå­¦ä¹ çš„æ–°æœºä¼šå‡ºç°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¼ æ„Ÿå™¨è¿åŠ¨ç­–ç•¥è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ¨¡ä»¿äººç±»è¿åŠ¨çš„é¢„æµ‹æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å…³é”®åœ¨äºï¼Œäººç±»å¯å‘çš„æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨çš„å…³é”®ç‚¹è¿åŠ¨ä¸ç›¸åº”çš„äººä½“å…³é”®ç‚¹è¿åŠ¨å¯†åˆ‡ç›¸ä¼¼ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåœ¨æœºå™¨äººæ•°æ®ä¸Šé›¶æ ·æœ¬ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæœªæ¥è¿åŠ¨é¢„æµ‹ã€‚æˆ‘ä»¬è®­ç»ƒä¼ æ„Ÿå™¨è¿åŠ¨ç­–ç•¥ä»¥è·Ÿè¸ªè¯¥æ¨¡å‹çš„é¢„æµ‹ï¼ŒåŸºäºè¿‡å»æœºå™¨äººçŠ¶æ€çš„å†å²ï¼ŒåŒæ—¶ä¼˜åŒ–ç›¸å¯¹ç¨€ç–çš„ä»»åŠ¡å¥–åŠ±ã€‚è¯¥æ–¹æ³•å®Œå…¨ç»•è¿‡äº†åŸºäºæ¢¯åº¦çš„è¿åŠ¨é‡å®šå‘å’Œå¯¹æŠ—æŸå¤±ï¼Œä»è€Œå…‹æœäº†ç°æœ‰æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨ç°ä»£äººç±»åœºæ™¯äº¤äº’æ•°æ®é›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§çš„é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒæœºå™¨äººå’Œä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨äººå­¦ä¹ æ–¹æ³•åœ¨åˆ©ç”¨äººç±»äº¤äº’æ•°æ®æ—¶çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯æ¢¯åº¦é‡å®šå‘å’Œå¯¹æŠ—æŸå¤±å¯¼è‡´çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ¨¡ä»¿äººç±»è¿åŠ¨çš„é¢„æµ‹æ¨¡å‹ï¼Œåˆ©ç”¨äººç±»æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨æœºå™¨äººæ•°æ®ä¸Šè¿›è¡Œé›¶æ ·æœ¬åº”ç”¨ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ä¼ æ„Ÿå™¨è¿åŠ¨ç­–ç•¥è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒå’Œç­–ç•¥ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ”¶é›†äººç±»ä¸ç¯å¢ƒäº¤äº’çš„æ•°æ®ï¼›å…¶æ¬¡ï¼Œè®­ç»ƒä¸€ä¸ªé¢„æµ‹æœªæ¥è¿åŠ¨çš„æ¨¡å‹ï¼›æœ€åï¼ŒåŸºäºè¯¥æ¨¡å‹çš„é¢„æµ‹æ¥ä¼˜åŒ–æœºå™¨äººç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå®ç°äº†é›¶æ ·æœ¬å­¦ä¹ ï¼Œä½¿å¾—è®­ç»ƒå¥½çš„æ¨¡å‹èƒ½å¤Ÿç›´æ¥åº”ç”¨äºæœºå™¨äººæ•°æ®ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„å¤æ‚æ€§å’Œå±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç›¸å¯¹ç¨€ç–çš„ä»»åŠ¡å¥–åŠ±è®¾è®¡ï¼ŒæŸå¤±å‡½æ•°åˆ™ä¸“æ³¨äºè·Ÿè¸ªé¢„æµ‹æ¨¡å‹çš„è¾“å‡ºï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºäººç±»è¿åŠ¨çš„å…³é”®ç‚¹è®¾è®¡ï¼Œç¡®ä¿äº†æœºå™¨äººè¿åŠ¨çš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªæœºå™¨äººå’Œä»»åŠ¡ä¸Šå‡æ˜¾è‘—è¶…è¶Šç°æœ‰åŸºçº¿ï¼Œæå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè·Ÿè¸ªäººç±»è¿åŠ¨æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ›¿ä»£å¤æ‚çš„å¥–åŠ±è®¾è®¡ï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æœºå™¨äººæ“æ§ã€è‡ªåŠ¨åŒ–ç”Ÿäº§å’Œäººæœºåä½œç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜æœºå™¨äººå¯¹äººç±»è¿åŠ¨çš„ç†è§£å’Œæ¨¡ä»¿èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œçµæ´»æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As the embodiment gap between a robot and a human narrows, new opportunities arise to leverage datasets of humans interacting with their surroundings for robot learning. We propose a novel technique for training sensorimotor policies with reinforcement learning by imitating predictive models of human motions. Our key insight is that the motion of keypoints on human-inspired robot end-effectors closely mirrors the motion of corresponding human body keypoints. This enables us to use a model trained to predict future motion on human data \emph{zero-shot} on robot data. We train sensorimotor policies to track the predictions of such a model, conditioned on a history of past robot states, while optimizing a relatively sparse task reward. This approach entirely bypasses gradient-based kinematic retargeting and adversarial losses, which limit existing methods from fully leveraging the scale and diversity of modern human-scene interaction datasets. Empirically, we find that our approach can work across robots and tasks, outperforming existing baselines by a large margin. In addition, we find that tracking a human motion model can substitute for carefully designed dense rewards and curricula in manipulation tasks. Code, data and qualitative results available at https://jirl-upenn.github.io/track_reward/.

