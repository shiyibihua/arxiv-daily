---
layout: default
title: MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation
---

# MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19236" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.19236v1</a>
  <a href="https://arxiv.org/pdf/2508.19236.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19236v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19236v1', 'MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, Gao Huang

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-26

**Â§áÊ≥®**: The project is available at https://shihao1895.github.io/MemoryVLA

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://shihao1895.github.io/MemoryVLA)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MemoryVLA‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÊó∂Èó¥‰∏ä‰∏ãÊñáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `ËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®` `Êó∂Èó¥‰∏ä‰∏ãÊñá` `ËÆ§Áü•ÁßëÂ≠¶` `ÈïøÊó∂Èó¥‰æùËµñ` `ÊÑüÁü•-ËÆ§Áü•ËÆ∞ÂøÜÂ∫ì` `ÂÜ≥Á≠ñËÉΩÂäõ` `Êô∫ËÉΩÊú∫Âô®‰∫∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®Ê®°ÂûãÈÄöÂ∏∏ÂøΩËßÜÊó∂Èó¥‰∏ä‰∏ãÊñáÔºåÂØºËá¥Âú®ÈïøÊó∂Èó¥‰æùËµñÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫MemoryVLAÊ°ÜÊû∂ÔºåÈÄöËøáÂ∑•‰ΩúËÆ∞ÂøÜÂíåÊÑüÁü•-ËÆ§Áü•ËÆ∞ÂøÜÂ∫ìÊù•Â§ÑÁêÜÊó∂Èó¥‰∏ä‰∏ãÊñáÔºåÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÂÜ≥Á≠ñËÉΩÂäõ„ÄÇ
3. Âú®Â§ö‰∏™Ê®°ÊãüÂíåÁé∞ÂÆû‰ªªÂä°‰∏≠ÔºåMemoryVLAÁöÑÊàêÂäüÁéáË∂ÖËøáÁé∞ÊúâÂü∫Á∫øÔºåÁâπÂà´ÊòØÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êó∂Èó¥‰∏ä‰∏ãÊñáÂØπ‰∫éÊú∫Âô®‰∫∫Êìç‰ΩúËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜ‰∏ªÊµÅÁöÑËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®ÔºàVLAÔºâÊ®°ÂûãÈÄöÂ∏∏ÂøΩËßÜËøô‰∏ÄÁÇπÔºåÂØºËá¥Âú®ÈïøÊó∂Èó¥‰æùËµñ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇËÆ§Áü•ÁßëÂ≠¶Ë°®ÊòéÔºå‰∫∫Á±ª‰æùËµñÂ∑•‰ΩúËÆ∞ÂøÜÊù•ÁºìÂÜ≤Áü≠ÊúüË°®ÂæÅ‰ª•ËøõË°åÂç≥Êó∂ÊéßÂà∂ÔºåËÄåÊµ∑È©¨Á≥ªÁªüÂàô‰øùÂ≠òËøáÂéªÁªèÈ™åÁöÑËØ¶ÁªÜÂíåËØ≠‰πâ‰ø°ÊÅØ„ÄÇÂü∫‰∫éËøô‰∫õÊú∫Âà∂ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜMemoryVLAÔºå‰∏Ä‰∏™Áî®‰∫éÈïøÊó∂Èó¥Êú∫Âô®‰∫∫Êìç‰ΩúÁöÑËÆ§Áü•-ËÆ∞ÂøÜ-Ë°åÂä®Ê°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂ∞ÜËßÇÂØüÁºñÁ†Å‰∏∫ÊÑüÁü•ÂíåËÆ§Áü•Ê†áËÆ∞ÔºåÂΩ¢ÊàêÂ∑•‰ΩúËÆ∞ÂøÜÔºåÂêåÊó∂Âª∫Á´ãÊÑüÁü•-ËÆ§Áü•ËÆ∞ÂøÜÂ∫ì‰ª•Â≠òÂÇ®‰ΩéÁ∫ßÁªÜËäÇÂíåÈ´òÁ∫ßËØ≠‰πâ„ÄÇÂ∑•‰ΩúËÆ∞ÂøÜ‰ªéËÆ∞ÂøÜÂ∫ì‰∏≠Ê£ÄÁ¥¢ÂÜ≥Á≠ñÁõ∏ÂÖ≥Êù°ÁõÆÔºåÂπ∂‰∏éÂΩìÂâçÊ†áËÆ∞Ëá™ÈÄÇÂ∫îËûçÂêàÔºåÊõ¥Êñ∞ËÆ∞ÂøÜÂ∫ì„ÄÇÈÄöËøáËøô‰∫õÊ†áËÆ∞ÔºåËÆ∞ÂøÜÊù°‰ª∂Êâ©Êï£Ë°åÂä®‰∏ìÂÆ∂ÁîüÊàêÊó∂Èó¥ÊÑüÁü•ÁöÑË°åÂä®Â∫èÂàó„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMemoryVLAÂú®150Â§ö‰∏™Ê®°ÊãüÂíåÁé∞ÂÆû‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊàêÂäüÁéáÊòæËëóÈ´ò‰∫éÁé∞ÊúâÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÊó∂Èó¥‰∏ä‰∏ãÊñáÈóÆÈ¢òÔºåÁé∞ÊúâÁöÑVLAÊ®°ÂûãÂú®Â§ÑÁêÜÈïøÊó∂Èó¥‰æùËµñ‰ªªÂä°Êó∂Ë°®Áé∞‰∏çË∂≥ÔºåÊó†Ê≥ïÊúâÊïàÂà©Áî®ÂéÜÂè≤‰ø°ÊÅØËøõË°åÂÜ≥Á≠ñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMemoryVLAÊ°ÜÊû∂ÁªìÂêà‰∫ÜËÆ§Áü•ÁßëÂ≠¶‰∏≠ÁöÑÂ∑•‰ΩúËÆ∞ÂøÜÂíåÈïøÊó∂ËÆ∞ÂøÜÊú∫Âà∂ÔºåÈÄöËøáÊÑüÁü•-ËÆ§Áü•ËÆ∞ÂøÜÂ∫ìÂ≠òÂÇ®ÂíåÊ£ÄÁ¥¢‰ø°ÊÅØÔºå‰ª•Â¢ûÂº∫Êú∫Âô®‰∫∫Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁî®‰∫éÁºñÁ†ÅËßÇÂØü‰ø°ÊÅØÔºåÊÑüÁü•-ËÆ§Áü•ËÆ∞ÂøÜÂ∫ìÁî®‰∫éÂ≠òÂÇ®‰ø°ÊÅØÔºåËÆ∞ÂøÜÊù°‰ª∂Êâ©Êï£Ë°åÂä®‰∏ìÂÆ∂Áî®‰∫éÁîüÊàêÊó∂Èó¥ÊÑüÁü•ÁöÑË°åÂä®Â∫èÂàó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMemoryVLAÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÊÑüÁü•-ËÆ§Áü•ËÆ∞ÂøÜÂ∫ìÔºåËÉΩÂ§üÊúâÊïàÊï¥ÂêàÁü≠ÊúüÂíåÈïøÊúüËÆ∞ÂøÜÔºåÊòæËëóÊèêÂçá‰∫ÜÊú∫Âô®‰∫∫Âú®ÈïøÊó∂Èó¥‰æùËµñ‰ªªÂä°‰∏≠ÁöÑÂÜ≥Á≠ñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÂ∑•‰ΩúËÆ∞ÂøÜÈÄöËøáËá™ÈÄÇÂ∫îËûçÂêàÂΩìÂâçÊ†áËÆ∞ÂíåËÆ∞ÂøÜÂ∫ì‰∏≠ÁöÑ‰ø°ÊÅØÊù•Êõ¥Êñ∞ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñËÆ∞ÂøÜÁöÑÊ£ÄÁ¥¢ÂíåÊõ¥Êñ∞ËøáÁ®ã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MemoryVLAÂú®SimperEnv-Bridge„ÄÅFractalÂíåLIBERO-5Á≠â‰ªªÂä°‰∏≠ÂàÜÂà´ÂèñÂæó‰∫Ü71.9%„ÄÅ72.7%Âíå96.5%ÁöÑÊàêÂäüÁéáÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑCogACTÂíåpi-0Âü∫Á∫øÔºåÂ∞§ÂÖ∂Âú®Bridge‰ªªÂä°‰∏äÊèêÂçá‰∫Ü14.6‰∏™ÁôæÂàÜÁÇπ„ÄÇÂú®12‰∏™Áé∞ÂÆû‰ªªÂä°‰∏≠ÔºåMemoryVLAÁöÑÊàêÂäüÁéáËææÂà∞84.0%ÔºåÈïøÊó∂Èó¥‰ªªÂä°ÁöÑË°®Áé∞ÊØîÁé∞ÊúâÂü∫Á∫øÊèêÂçá‰∫Ü26‰∏™ÁôæÂàÜÁÇπ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MemoryVLAÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®ÂåñÁîü‰∫ßÁ∫ø„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÁ≠âÈ¢ÜÂüüÔºåÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑËá™‰∏ªÂÜ≥Á≠ñËÉΩÂäõ„ÄÇÊú™Êù•ÔºåËØ•Ê°ÜÊû∂ÊúâÊúõÊé®Âä®Êõ¥È´òÂ±ÇÊ¨°ÁöÑÊô∫ËÉΩÊú∫Âô®‰∫∫ÂèëÂ±ïÔºå‰ΩøÂÖ∂Âú®Âä®ÊÄÅÂíå‰∏çÁ°ÆÂÆöÁöÑÁéØÂ¢É‰∏≠Ë°®Áé∞Êõ¥Âä†Âá∫Ëâ≤„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline. Project Page: https://shihao1895.github.io/MemoryVLA

