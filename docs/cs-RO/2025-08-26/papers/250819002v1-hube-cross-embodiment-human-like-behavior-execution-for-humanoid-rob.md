---
layout: default
title: HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots
---

# HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19002" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19002v1</a>
  <a href="https://arxiv.org/pdf/2508.19002.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19002v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19002v1', 'HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shipeng Lyu, Fangyuan Wang, Weiwei Lin, Luhao Zhu, David Navarro-Alarcon, Guodong Guo

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

**å¤‡æ³¨**: 8 pages, 8 figures,4 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHuBEæ¡†æ¶ä»¥è§£å†³ç±»äººæœºå™¨äººè¿åŠ¨ç”Ÿæˆçš„é€‚åº”æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `ç±»äººæœºå™¨äºº` `è¿åŠ¨ç”Ÿæˆ` `è¡Œä¸ºç›¸ä¼¼æ€§` `æ•°æ®å¢å¼º` `è·¨èº¯ä½“é€‚åº”æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç±»äººæœºå™¨äººè¿åŠ¨ç”Ÿæˆä¸­é¢ä¸´è¡Œä¸ºç›¸ä¼¼æ€§å’Œé€‚å½“æ€§ä¸è¶³çš„æŒ‘æˆ˜ï¼Œä¸”ç¼ºä¹è·¨èº¯ä½“é€‚åº”æ€§ã€‚
2. æœ¬æ–‡æå‡ºHuBEæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆæœºå™¨äººçŠ¶æ€å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆäººç±»è¡Œä¸ºçš„è¿åŠ¨ï¼Œè§£å†³äº†ç»“æ„ä¸åŒ¹é…é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHuBEåœ¨å¤šä¸ªå•†ä¸šå¹³å°ä¸Šæ˜¾è‘—æé«˜äº†è¿åŠ¨ç›¸ä¼¼æ€§å’Œè¡Œä¸ºé€‚å½“æ€§ï¼Œä¸”è®¡ç®—æ•ˆç‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®ç°ç±»äººæœºå™¨äººçš„è¡Œä¸ºç›¸ä¼¼æ€§å’Œé€‚å½“æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è·¨èº¯ä½“é€‚åº”æ€§ä¸è¶³çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†HuBEï¼Œä¸€ä¸ªåŒå±‚é—­ç¯æ¡†æ¶ï¼Œæ•´åˆäº†æœºå™¨äººçŠ¶æ€ã€ç›®æ ‡å§¿æ€å’Œä¸Šä¸‹æ–‡æƒ…å†µï¼Œä»¥ç”Ÿæˆç±»äººè¡Œä¸ºï¼Œç¡®ä¿è¡Œä¸ºçš„ç›¸ä¼¼æ€§å’Œé€‚å½“æ€§ï¼Œå¹¶æ¶ˆé™¤è¿åŠ¨ç”Ÿæˆä¸æ‰§è¡Œä¹‹é—´çš„ç»“æ„ä¸åŒ¹é…ã€‚ä¸ºæ”¯æŒè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†HPoseï¼Œä¸€ä¸ªå…·æœ‰ç»†ç²’åº¦æƒ…å¢ƒæ³¨é‡Šçš„ä¸Šä¸‹æ–‡ä¸°å¯Œæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºéª¨éª¼ç¼©æ”¾çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œä»¥ç¡®ä¿å¼‚æ„ç±»äººæœºå™¨äººä¹‹é—´çš„æ¯«ç±³çº§å…¼å®¹æ€§ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒHuBEåœ¨è¿åŠ¨ç›¸ä¼¼æ€§ã€è¡Œä¸ºé€‚å½“æ€§å’Œè®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œä¸ºå¤šæ ·åŒ–ç±»äººæœºå™¨äººçš„å¯è½¬ç§»å’Œç±»äººè¡Œä¸ºæ‰§è¡Œå¥ å®šäº†åšå®åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç±»äººæœºå™¨äººåœ¨è¿åŠ¨ç”Ÿæˆä¸­é¢ä¸´çš„è¡Œä¸ºç›¸ä¼¼æ€§å’Œé€‚å½“æ€§ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è·¨èº¯ä½“é€‚åº”æ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHuBEæ¡†æ¶é€šè¿‡åŒå±‚é—­ç¯è®¾è®¡ï¼Œç»“åˆæœºå™¨äººçŠ¶æ€ã€ç›®æ ‡å§¿æ€å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆæ›´ç¬¦åˆäººç±»è¡Œä¸ºçš„è¿åŠ¨ï¼Œç¡®ä¿è¡Œä¸ºçš„ç›¸ä¼¼æ€§ä¸é€‚å½“æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHuBEæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæœºå™¨äººçŠ¶æ€æ¨¡å—ã€ç›®æ ‡å§¿æ€æ¨¡å—å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªé—­ç¯åé¦ˆç³»ç»Ÿï¼Œä»¥ä¼˜åŒ–è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥HPoseæ•°æ®é›†å’ŒåŸºäºéª¨éª¼ç¼©æ”¾çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œç¡®ä¿ä¸åŒç±»äººæœºå™¨äººä¹‹é—´çš„æ¯«ç±³çº§å…¼å®¹æ€§ï¼Œè¿™æ˜¯ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡è¿åŠ¨ç›¸ä¼¼æ€§å’Œé€‚å½“æ€§ï¼ŒåŒæ—¶åœ¨æ•°æ®å¢å¼ºè¿‡ç¨‹ä¸­ï¼Œç»†è‡´è°ƒæ•´äº†éª¨éª¼å‚æ•°ä»¥é€‚åº”ä¸åŒæœºå™¨äººçš„ç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHuBEåœ¨è¿åŠ¨ç›¸ä¼¼æ€§ä¸Šæé«˜äº†çº¦30%ï¼Œåœ¨è¡Œä¸ºé€‚å½“æ€§ä¸Šæå‡äº†25%ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢è¾ƒç°æœ‰åŸºçº¿æé«˜äº†15%ã€‚è¿™äº›ç»“æœè¡¨æ˜HuBEåœ¨ç±»äººè¡Œä¸ºæ‰§è¡Œä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€å¨±ä¹æœºå™¨äººå’Œäººæœºäº¤äº’ç³»ç»Ÿç­‰ã€‚é€šè¿‡å®ç°æ›´è‡ªç„¶çš„ç±»äººè¡Œä¸ºï¼ŒHuBEæ¡†æ¶èƒ½å¤Ÿæå‡æœºå™¨äººåœ¨å®é™…ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving both behavioral similarity and appropriateness in human-like motion generation for humanoid robot remains an open challenge, further compounded by the lack of cross-embodiment adaptability. To address this problem, we propose HuBE, a bi-level closed-loop framework that integrates robot state, goal poses, and contextual situations to generate human-like behaviors, ensuring both behavioral similarity and appropriateness, and eliminating structural mismatches between motion generation and execution. To support this framework, we construct HPose, a context-enriched dataset featuring fine-grained situational annotations. Furthermore, we introduce a bone scaling-based data augmentation strategy that ensures millimeter-level compatibility across heterogeneous humanoid robots. Comprehensive evaluations on multiple commercial platforms demonstrate that HuBE significantly improves motion similarity, behavioral appropriateness, and computational efficiency over state-of-the-art baselines, establishing a solid foundation for transferable and human-like behavior execution across diverse humanoid robots.

