---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-08-26
---

# cs.ROï¼ˆ2025-08-26ï¼‰

ğŸ“Š å…± **18** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (14 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (14 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250819002v1-hube-cross-embodiment-human-like-behavior-execution-for-humanoid-rob.html">HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots</a></td>
  <td>æå‡ºHuBEæ¡†æ¶ä»¥è§£å†³ç±»äººæœºå™¨äººè¿åŠ¨ç”Ÿæˆçš„é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19002v1" data-paper-url="./papers/250819002v1-hube-cross-embodiment-human-like-behavior-execution-for-humanoid-rob.html" onclick="toggleFavorite(this, '2508.19002v1', 'HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250819236v1-memoryvla-perceptual-cognitive-memory-in-vision-language-action-mode.html">MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation</a></td>
  <td>æå‡ºMemoryVLAä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ—¶é—´ä¸Šä¸‹æ–‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19236v1" data-paper-url="./papers/250819236v1-memoryvla-perceptual-cognitive-memory-in-vision-language-action-mode.html" onclick="toggleFavorite(this, '2508.19236v1', 'MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250819153v2-quadkan-kan-enhanced-quadruped-motion-control-via-end-to-end-reinfor.html">QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning</a></td>
  <td>æå‡ºQuadKANä»¥è§£å†³å››è¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19153v2" data-paper-url="./papers/250819153v2-quadkan-kan-enhanced-quadruped-motion-control-via-end-to-end-reinfor.html" onclick="toggleFavorite(this, '2508.19153v2', 'QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250818691v1-deep-sensorimotor-control-by-imitating-predictive-models-of-human-mo.html">Deep Sensorimotor Control by Imitating Predictive Models of Human Motion</a></td>
  <td>æå‡ºé€šè¿‡æ¨¡ä»¿äººç±»è¿åŠ¨é¢„æµ‹æ¨¡å‹çš„ä¼ æ„Ÿå™¨è¿åŠ¨æ§åˆ¶æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">predictive model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18691v1" data-paper-url="./papers/250818691v1-deep-sensorimotor-control-by-imitating-predictive-models-of-human-mo.html" onclick="toggleFavorite(this, '2508.18691v1', 'Deep Sensorimotor Control by Imitating Predictive Models of Human Motion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250819191v2-autoring-imitation-learning-based-autonomous-intraocular-foreign-bod.html">AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot</a></td>
  <td>æå‡ºAutoRingä»¥è§£å†³çœ¼å†…å¼‚ç‰©å»é™¤çš„è‡ªä¸»æ“ä½œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">teleoperation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19191v2" data-paper-url="./papers/250819191v2-autoring-imitation-learning-based-autonomous-intraocular-foreign-bod.html" onclick="toggleFavorite(this, '2508.19191v2', 'AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250818627v1-integration-of-robot-and-scene-kinematics-for-sequential-mobile-mani.html">Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning</a></td>
  <td>æå‡ºé¡ºåºç§»åŠ¨æ“æ§è§„åˆ’æ¡†æ¶ä»¥è§£å†³é•¿æ—¶é—´å¤šæ­¥æ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18627v1" data-paper-url="./papers/250818627v1-integration-of-robot-and-scene-kinematics-for-sequential-mobile-mani.html" onclick="toggleFavorite(this, '2508.18627v1', 'Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250818802v3-hypertasr-hypernetwork-driven-task-aware-scene-representations-for-r.html">HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation</a></td>
  <td>æå‡ºHyperTASRä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„åœºæ™¯è¡¨ç¤ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18802v3" data-paper-url="./papers/250818802v3-hypertasr-hypernetwork-driven-task-aware-scene-representations-for-r.html" onclick="toggleFavorite(this, '2508.18802v3', 'HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250819391v2-lava-man-learning-visual-action-representations-for-robot-manipulati.html">LaVA-Man: Learning Visual Action Representations for Robot Manipulation</a></td>
  <td>æå‡ºLaVA-Manä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„è§†è§‰-æ–‡æœ¬ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19391v2" data-paper-url="./papers/250819391v2-lava-man-learning-visual-action-representations-for-robot-manipulati.html" onclick="toggleFavorite(this, '2508.19391v2', 'LaVA-Man: Learning Visual Action Representations for Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250819380v1-flipwalker-jacobs-ladder-toy-inspired-robot-for-locomotion-across-di.html">FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain</a></td>
  <td>æå‡ºFlipWalkerä»¥è§£å†³å¤æ‚åœ°å½¢ä¸‹çš„æœºå™¨äººè¿åŠ¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19380v1" data-paper-url="./papers/250819380v1-flipwalker-jacobs-ladder-toy-inspired-robot-for-locomotion-across-di.html" onclick="toggleFavorite(this, '2508.19380v1', 'FlipWalker: Jacob&#39;s Ladder toy-inspired robot for locomotion across diverse, complex terrain')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250819199v1-planning-query-guided-model-generation-for-model-based-deformable-ob.html">Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation</a></td>
  <td>æå‡ºåŸºäºè§„åˆ’æŸ¥è¯¢çš„æ¨¡å‹ç”Ÿæˆæ–¹æ³•ä»¥è§£å†³å¯å˜å½¢ç‰©ä½“æ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19199v1" data-paper-url="./papers/250819199v1-planning-query-guided-model-generation-for-model-based-deformable-ob.html" onclick="toggleFavorite(this, '2508.19199v1', 'Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250819172v3-from-tabula-rasa-to-emergent-abilities-discovering-robot-skills-via-.html">From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity</a></td>
  <td>æå‡ºURSAä»¥è§£å†³æœºå™¨äººè‡ªä¸»æŠ€èƒ½å‘ç°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">Unitree</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19172v3" data-paper-url="./papers/250819172v3-from-tabula-rasa-to-emergent-abilities-discovering-robot-skills-via-.html" onclick="toggleFavorite(this, '2508.19172v3', 'From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250819367v2-inference-of-human-derived-specifications-of-object-placement-via-de.html">Inference of Human-derived Specifications of Object Placement via Demonstration</a></td>
  <td>æå‡ºä½ç½®å¢å¼ºåŒºåŸŸè¿æ¥æ¼”ç®—ä»¥è§£å†³äººç±»ç‰©ä½“æ‘†æ”¾ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19367v2" data-paper-url="./papers/250819367v2-inference-of-human-derived-specifications-of-object-placement-via-de.html" onclick="toggleFavorite(this, '2508.19367v2', 'Inference of Human-derived Specifications of Object Placement via Demonstration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250900065v1-hybrid-perception-and-equivariant-diffusion-for-robust-multi-node-re.html">Hybrid Perception and Equivariant Diffusion for Robust Multi-Node Rebar Tying</a></td>
  <td>æå‡ºæ··åˆæ„ŸçŸ¥ä¸ç­‰å˜æ‰©æ•£æ–¹æ³•ä»¥è§£å†³å¤šèŠ‚ç‚¹é’¢ç­‹ç»‘æ‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00065v1" data-paper-url="./papers/250900065v1-hybrid-perception-and-equivariant-diffusion-for-robust-multi-node-re.html" onclick="toggleFavorite(this, '2509.00065v1', 'Hybrid Perception and Equivariant Diffusion for Robust Multi-Node Rebar Tying')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250818820v1-as2fm-enabling-statistical-model-checking-of-ros-2-systems-for-robus.html">AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy</a></td>
  <td>æå‡ºAS2FMä»¥è§£å†³ROS 2ç³»ç»Ÿçš„å½¢å¼éªŒè¯é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18820v1" data-paper-url="./papers/250818820v1-as2fm-enabling-statistical-model-checking-of-ros-2-systems-for-robus.html" onclick="toggleFavorite(this, '2508.18820v1', 'AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250900064v1-opentie-open-vocabulary-sequential-rebar-tying-system.html">OpenTie: Open-vocabulary Sequential Rebar Tying System</a></td>
  <td>æå‡ºOpenTieä»¥è§£å†³å»ºç­‘å·¥åœ°é’¢ç­‹ç»‘æ‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00064v1" data-paper-url="./papers/250900064v1-opentie-open-vocabulary-sequential-rebar-tying-system.html" onclick="toggleFavorite(this, '2509.00064v1', 'OpenTie: Open-vocabulary Sequential Rebar Tying System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250819131v2-zest-an-llm-based-zero-shot-traversability-navigation-for-unknown-en.html">ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments</a></td>
  <td>æå‡ºZeSTä»¥è§£å†³æœªçŸ¥ç¯å¢ƒä¸­çš„å¯¼èˆªå¯è¾¾æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">traversability</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19131v2" data-paper-url="./papers/250819131v2-zest-an-llm-based-zero-shot-traversability-navigation-for-unknown-en.html" onclick="toggleFavorite(this, '2508.19131v2', 'ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250819476v2-gentle-object-retraction-in-dense-clutter-using-multimodal-force-sen.html">Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning</a></td>
  <td>æå‡ºå¤šæ¨¡æ€åŠ›æ„ŸçŸ¥ä¸æ¨¡ä»¿å­¦ä¹ ä»¥è§£å†³å¯†é›†æ‚ç‰©ä¸­çš„ç‰©ä½“æå–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19476v2" data-paper-url="./papers/250819476v2-gentle-object-retraction-in-dense-clutter-using-multimodal-force-sen.html" onclick="toggleFavorite(this, '2508.19476v2', 'Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250819074v1-an-llm-powered-natural-to-robotic-language-translation-framework-wit.html">An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees</a></td>
  <td>æå‡ºè‡ªç„¶-æœºå™¨äººè¯­è¨€ç¿»è¯‘æ¡†æ¶ä»¥è§£å†³ç¼–ç¨‹é”™è¯¯é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19074v1" data-paper-url="./papers/250819074v1-an-llm-powered-natural-to-robotic-language-translation-framework-wit.html" onclick="toggleFavorite(this, '2508.19074v1', 'An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)