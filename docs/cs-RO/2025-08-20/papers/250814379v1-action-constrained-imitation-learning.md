---
layout: default
title: Action-Constrained Imitation Learning
---

# Action-Constrained Imitation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14379" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14379v1</a>
  <a href="https://arxiv.org/pdf/2508.14379.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14379v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14379v1', 'Action-Constrained Imitation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chia-Han Yeh, Tse-Sheng Nan, Risto Vuorio, Wei Hung, Hung-Yen Wu, Shao-Hua Sun, Ping-Chun Hsieh

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

**å¤‡æ³¨**: Published in ICML 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/NYCU-RL-Bandits-Lab/ACRL-Baselines)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨ä½œçº¦æŸæ¨¡ä»¿å­¦ä¹ ä»¥è§£å†³å®‰å…¨è¡Œä¸ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŠ¨ä½œçº¦æŸ` `æ¨¡ä»¿å­¦ä¹ ` `è½¨è¿¹å¯¹é½` `åŠ¨æ€æ—¶é—´è§„æ•´` `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `æœºå™¨äººæ§åˆ¶` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨åŠ¨ä½œçº¦æŸä¸‹ï¼Œä¸“å®¶ä¸æ¨¡ä»¿è€…ä¹‹é—´çš„å ç”¨åº¦é‡å­˜åœ¨ä¸åŒ¹é…ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆæœä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºDTWILï¼Œé€šè¿‡è½¨è¿¹å¯¹é½å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶è§£å†³åŠ¨ä½œçº¦æŸä¸‹çš„å­¦ä¹ é—®é¢˜ï¼Œç”Ÿæˆç¬¦åˆçº¦æŸçš„æ›¿ä»£æ•°æ®é›†ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå®éªŒè¡¨æ˜ï¼ŒDTWILç”Ÿæˆçš„æ•°æ®é›†æ˜¾è‘—æé«˜äº†å¤šä¸ªæœºå™¨äººæ§åˆ¶ä»»åŠ¡çš„æ€§èƒ½ï¼Œæ ·æœ¬æ•ˆç‡ä¼˜äºå¤šç§åŸºå‡†ç®—æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å„ç§æœºå™¨äººæ§åˆ¶å’Œèµ„æºåˆ†é…åº”ç”¨ä¸­ï¼Œæ”¿ç­–å­¦ä¹ åœ¨ç¡®ä¿å®‰å…¨è¡Œä¸ºæ–¹é¢èµ·ç€æ ¸å¿ƒä½œç”¨ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§æ–°çš„é—®é¢˜è®¾ç½®ï¼Œç§°ä¸ºåŠ¨ä½œçº¦æŸæ¨¡ä»¿å­¦ä¹ ï¼ˆACILï¼‰ï¼Œå…¶ä¸­ä¸€ä¸ªå—åŠ¨ä½œçº¦æŸçš„æ¨¡ä»¿è€…æ—¨åœ¨ä»å…·æœ‰æ›´å¤§åŠ¨ä½œç©ºé—´çš„ç¤ºèŒƒä¸“å®¶é‚£é‡Œå­¦ä¹ ã€‚ACILçš„åŸºæœ¬æŒ‘æˆ˜åœ¨äºï¼Œç”±äºåŠ¨ä½œçº¦æŸï¼Œä¸“å®¶ä¸æ¨¡ä»¿è€…ä¹‹é—´çš„å ç”¨åº¦é‡ä¸å¯é¿å…åœ°å­˜åœ¨ä¸åŒ¹é…ã€‚æˆ‘ä»¬é€šè¿‡è½¨è¿¹å¯¹é½æ¥è§£å†³è¿™ä¸€ä¸åŒ¹é…ï¼Œå¹¶æå‡ºäº†DTWILï¼Œè¯¥æ–¹æ³•ç”¨éµå¾ªç›¸ä¼¼çŠ¶æ€è½¨è¿¹çš„æ›¿ä»£æ•°æ®é›†æ›¿æ¢åŸå§‹ä¸“å®¶æ¼”ç¤ºï¼ŒåŒæ—¶éµå¾ªåŠ¨ä½œçº¦æŸã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä»DTWILç”Ÿæˆçš„æ•°æ®é›†ä¸­å­¦ä¹ æ˜¾è‘—æå‡äº†å¤šä¸ªæœºå™¨äººæ§åˆ¶ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ ·æœ¬æ•ˆç‡æ–¹é¢è¶…è¶Šäº†å¤šç§åŸºå‡†æ¨¡ä»¿å­¦ä¹ ç®—æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯åŠ¨ä½œçº¦æŸæ¨¡ä»¿å­¦ä¹ ï¼ˆACILï¼‰ï¼Œå³åœ¨åŠ¨ä½œç©ºé—´å—é™çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°ä»å…·æœ‰æ›´å¤§åŠ¨ä½œç©ºé—´çš„ä¸“å®¶å­¦ä¹ ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™ç§ä¸åŒ¹é…æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨ä¸“å®¶çš„çŸ¥è¯†ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è½¨è¿¹å¯¹é½æ¥è§£å†³ä¸“å®¶ä¸æ¨¡ä»¿è€…ä¹‹é—´çš„å ç”¨åº¦é‡ä¸åŒ¹é…é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œé‡‡ç”¨åŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆDTWï¼‰æ–¹æ³•ï¼Œå°†ä¸“å®¶çš„è½¨è¿¹ä¸ç¬¦åˆåŠ¨ä½œçº¦æŸçš„æ›¿ä»£è½¨è¿¹è¿›è¡Œå¯¹é½ï¼Œä»è€Œå®ç°æœ‰æ•ˆå­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè½¨è¿¹ç”Ÿæˆå’Œè½¨è¿¹å¯¹é½ã€‚é¦–å…ˆï¼Œé€šè¿‡æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ç”Ÿæˆç¬¦åˆåŠ¨ä½œçº¦æŸçš„æ›¿ä»£è½¨è¿¹ï¼›ç„¶åï¼Œåˆ©ç”¨DTWç®—æ³•å¯¹æ›¿ä»£è½¨è¿¹ä¸ä¸“å®¶è½¨è¿¹è¿›è¡Œå¯¹é½ï¼Œç¡®ä¿å­¦ä¹ è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä¼ é€’ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†è½¨è¿¹å¯¹é½è§†ä¸ºä¸€ä¸ªè§„åˆ’é—®é¢˜ï¼Œé€šè¿‡MPCä¸DTWç»“åˆï¼Œè§£å†³äº†åŠ¨ä½œçº¦æŸä¸‹çš„æ¨¡ä»¿å­¦ä¹ éš¾é¢˜ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†åŠ¨ä½œç©ºé—´çš„é™åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬è½¨è¿¹ç”Ÿæˆçš„æ—¶é—´æ­¥é•¿ã€MPCçš„é¢„æµ‹æ—¶åŸŸå’ŒDTWçš„è·ç¦»åº¦é‡æ–¹å¼ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†è½¨è¿¹å¯¹é½çš„ç²¾åº¦ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ›¿ä»£è½¨è¿¹èƒ½å¤Ÿæœ‰æ•ˆåæ˜ ä¸“å®¶çš„è¡Œä¸ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DTWILç”Ÿæˆçš„æ•°æ®é›†åœ¨å¤šä¸ªæœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œæ ·æœ¬æ•ˆç‡æå‡å¹…åº¦è¶…è¿‡äº†ä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªè¯¦ç»†æŠ«éœ²ï¼Œä½†ç›¸è¾ƒäºåŸºå‡†ç®—æ³•è¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€èµ„æºåˆ†é…ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å®‰å…¨æ€§ä¸æ•ˆç‡ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒACILæ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨æ€§ä¸å¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Policy learning under action constraints plays a central role in ensuring safe behaviors in various robot control and resource allocation applications. In this paper, we study a new problem setting termed Action-Constrained Imitation Learning (ACIL), where an action-constrained imitator aims to learn from a demonstrative expert with larger action space. The fundamental challenge of ACIL lies in the unavoidable mismatch of occupancy measure between the expert and the imitator caused by the action constraints. We tackle this mismatch through \textit{trajectory alignment} and propose DTWIL, which replaces the original expert demonstrations with a surrogate dataset that follows similar state trajectories while adhering to the action constraints. Specifically, we recast trajectory alignment as a planning problem and solve it via Model Predictive Control, which aligns the surrogate trajectories with the expert trajectories based on the Dynamic Time Warping (DTW) distance. Through extensive experiments, we demonstrate that learning from the dataset generated by DTWIL significantly enhances performance across multiple robot control tasks and outperforms various benchmark imitation learning algorithms in terms of sample efficiency. Our code is publicly available at https://github.com/NYCU-RL-Bandits-Lab/ACRL-Baselines.

