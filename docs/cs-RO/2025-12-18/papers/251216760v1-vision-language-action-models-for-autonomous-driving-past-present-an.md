---
layout: default
title: Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future
---

# Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16760" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16760v1</a>
  <a href="https://arxiv.org/pdf/2512.16760.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16760v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16760v1', 'Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianshuai Hu, Xiaolu Liu, Song Wang, Yiyao Zhu, Ao Liang, Lingdong Kong, Guoyang Zhao, Zeying Gong, Jun Cen, Zhiyu Huang, Xiaoshuai Hao, Linfeng Li, Hang Song, Xiangtai Li, Jun Ma, Shaojie Shen, Jianke Zhu, Dacheng Tao, Ziwei Liu, Junwei Liang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Preprint; 40 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-vla-for-ad

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°æ€§è®ºæ–‡ï¼šé¢å‘è‡ªåŠ¨é©¾é©¶çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ç ”ç©¶è¿›å±•ä¸æœªæ¥å±•æœ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `ç«¯åˆ°ç«¯å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿè‡ªåŠ¨é©¾é©¶ä¾èµ–â€œæ„ŸçŸ¥-å†³ç­–-è¡ŒåŠ¨â€æµç¨‹ï¼Œä½†å­˜åœ¨æ‰‹å·¥è®¾è®¡æ¥å£å¤±æ•ˆã€æ„ŸçŸ¥è¯¯å·®ä¼ æ’­ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚
2. è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹é€šè¿‡æ•´åˆè§†è§‰ç†è§£ã€è¯­è¨€æ¨ç†å’ŒåŠ¨ä½œè¾“å‡ºï¼Œæ—¨åœ¨å®ç°æ›´å¯è§£é‡Šã€é€šç”¨ä¸”ç¬¦åˆäººç±»ä¹ æƒ¯çš„è‡ªåŠ¨é©¾é©¶ç­–ç•¥ã€‚
3. è®ºæ–‡å¯¹VLAé¢†åŸŸè¿›è¡Œäº†ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œæ€»ç»“äº†ç°æœ‰æ–¹æ³•ã€æ•°æ®é›†å’ŒåŸºå‡†ï¼Œå¹¶æŒ‡å‡ºäº†é²æ£’æ€§ã€å¯è§£é‡Šæ€§å’ŒæŒ‡ä»¤ä¿çœŸåº¦ç­‰å…³é”®æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨é©¾é©¶é•¿æœŸä»¥æ¥ä¾èµ–äºæ¨¡å—åŒ–çš„â€œæ„ŸçŸ¥-å†³ç­–-è¡ŒåŠ¨â€æµç¨‹ï¼Œä½†æ‰‹å·¥è®¾è®¡çš„æ¥å£å’ŒåŸºäºè§„åˆ™çš„ç»„ä»¶åœ¨å¤æ‚æˆ–é•¿å°¾åœºæ™¯ä¸­ç»å¸¸å¤±æ•ˆã€‚å…¶çº§è”è®¾è®¡è¿›ä¸€æ­¥ä¼ æ’­æ„ŸçŸ¥è¯¯å·®ï¼Œé™ä½ä¸‹æ¸¸è§„åˆ’å’Œæ§åˆ¶çš„æ€§èƒ½ã€‚è§†è§‰-åŠ¨ä½œï¼ˆVAï¼‰æ¨¡å‹é€šè¿‡å­¦ä¹ ä»è§†è§‰è¾“å…¥åˆ°åŠ¨ä½œçš„ç›´æ¥æ˜ å°„æ¥è§£å†³ä¸€äº›å±€é™æ€§ï¼Œä½†å®ƒä»¬ä»ç„¶ä¸é€æ˜ï¼Œå¯¹åˆ†å¸ƒåç§»æ•æ„Ÿï¼Œå¹¶ä¸”ç¼ºä¹ç»“æ„åŒ–æ¨ç†æˆ–æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å­¦ä¹ çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¡†æ¶çš„å‡ºç°ï¼Œè¯¥æ¡†æ¶å°†æ„ŸçŸ¥ä¸åŸºäºè¯­è¨€çš„å†³ç­–ç›¸ç»“åˆã€‚é€šè¿‡ç»Ÿä¸€è§†è§‰ç†è§£ã€è¯­è¨€æ¨ç†å’Œå¯æ“ä½œçš„è¾“å‡ºï¼ŒVLAä¸ºæ›´å¯è§£é‡Šã€æ›´é€šç”¨å’Œæ›´ç¬¦åˆäººç±»ä¹ æƒ¯çš„é©¾é©¶ç­–ç•¥æä¾›äº†ä¸€æ¡é€”å¾„ã€‚æœ¬æ–‡å¯¹æ–°å…´çš„è‡ªåŠ¨é©¾é©¶VLAé¢†åŸŸè¿›è¡Œäº†ç»“æ„åŒ–æè¿°ï¼Œè¿½æº¯äº†ä»æ—©æœŸVAæ–¹æ³•åˆ°ç°ä»£VLAæ¡†æ¶çš„æ¼”å˜ï¼Œå¹¶å°†ç°æœ‰æ–¹æ³•ç»„ç»‡æˆä¸¤ç§ä¸»è¦èŒƒä¾‹ï¼šç«¯åˆ°ç«¯VLAï¼Œå®ƒåœ¨å•ä¸ªæ¨¡å‹ä¸­é›†æˆäº†æ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’ï¼›ä»¥åŠåŒç³»ç»ŸVLAï¼Œå®ƒå°†æ…¢é€Ÿå®¡è®®ï¼ˆé€šè¿‡VLMï¼‰ä¸å¿«é€Ÿã€å®‰å…¨å…³é”®çš„æ‰§è¡Œï¼ˆé€šè¿‡è§„åˆ’å™¨ï¼‰åˆ†å¼€ã€‚åœ¨è¿™äº›èŒƒä¾‹ä¸­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åŒºåˆ†äº†æ–‡æœ¬ä¸æ•°å€¼åŠ¨ä½œç”Ÿæˆå™¨ä»¥åŠæ˜¾å¼ä¸éšå¼æŒ‡å¯¼æœºåˆ¶ç­‰å­ç±»ã€‚æˆ‘ä»¬è¿˜æ€»ç»“äº†ç”¨äºè¯„ä¼°åŸºäºVLAçš„é©¾é©¶ç³»ç»Ÿçš„ä»£è¡¨æ€§æ•°æ®é›†å’ŒåŸºå‡†ï¼Œå¹¶å¼ºè°ƒäº†å…³é”®æŒ‘æˆ˜å’Œå¼€æ”¾æ–¹å‘ï¼ŒåŒ…æ‹¬é²æ£’æ€§ã€å¯è§£é‡Šæ€§å’ŒæŒ‡ä»¤ä¿çœŸåº¦ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œæ—¨åœ¨ä¸ºæ¨è¿›ä¸äººç±»å…¼å®¹çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¥ å®šè¿è´¯çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¼ ç»Ÿè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¾èµ–äºæ¨¡å—åŒ–çš„â€œæ„ŸçŸ¥-å†³ç­–-è¡ŒåŠ¨â€æµç¨‹ï¼Œå„ä¸ªæ¨¡å—ä¹‹é—´é€šè¿‡æ‰‹å·¥è®¾è®¡çš„æ¥å£è¿æ¥ã€‚è¿™ç§è®¾è®¡åœ¨å¤æ‚æˆ–é•¿å°¾åœºæ™¯ä¸‹å®¹æ˜“å¤±æ•ˆï¼Œå¹¶ä¸”æ„ŸçŸ¥æ¨¡å—çš„è¯¯å·®ä¼šé€çº§ä¼ é€’ï¼Œå½±å“ä¸‹æ¸¸çš„å†³ç­–å’Œæ§åˆ¶ã€‚æ­¤å¤–ï¼Œæ—©æœŸçš„è§†è§‰-åŠ¨ä½œ(VA)æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç›´æ¥ä»è§†è§‰è¾“å…¥é¢„æµ‹åŠ¨ä½œï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œéš¾ä»¥åº”å¯¹åˆ†å¸ƒåç§»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ€»ç»“å’Œåˆ†æè¿‘å¹´æ¥å…´èµ·çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨ã€‚VLAæ¨¡å‹é€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè¯­è¨€æ¨ç†ï¼Œä»è€Œå°†è§†è§‰æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’ŒåŠ¨ä½œæ‰§è¡Œç»Ÿä¸€èµ·æ¥ï¼Œæ—¨åœ¨æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€æ³›åŒ–èƒ½åŠ›å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚è®ºæ–‡å°†ç°æœ‰VLAæ¨¡å‹åˆ†ä¸ºç«¯åˆ°ç«¯VLAå’ŒåŒç³»ç»ŸVLAä¸¤å¤§ç±»ï¼Œå¹¶å¯¹å„ç±»æ–¹æ³•è¿›è¡Œäº†è¯¦ç»†çš„åˆ†æå’Œæ¯”è¾ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡å°†ç°æœ‰çš„VLAæ¨¡å‹åˆ†ä¸ºä»¥ä¸‹ä¸¤ç±»ï¼š
1. **ç«¯åˆ°ç«¯VLA**ï¼šè¯¥ç±»æ¨¡å‹å°†æ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ä¸­ï¼Œç›´æ¥ä»è§†è§‰è¾“å…¥å’Œè¯­è¨€æŒ‡ä»¤é¢„æµ‹è½¦è¾†çš„åŠ¨ä½œã€‚è¿™ç±»æ¨¡å‹é€šå¸¸é‡‡ç”¨Transformeræ¶æ„ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆã€‚
2. **åŒç³»ç»ŸVLA**ï¼šè¯¥ç±»æ¨¡å‹å°†å†³ç­–è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæ…¢é€Ÿå®¡è®®å’Œå¿«é€Ÿæ‰§è¡Œã€‚æ…¢é€Ÿå®¡è®®é˜¶æ®µç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è´Ÿè´£ï¼Œæ ¹æ®è§†è§‰è¾“å…¥å’Œè¯­è¨€æŒ‡ä»¤ç”Ÿæˆé«˜çº§åˆ«çš„è§„åˆ’ã€‚å¿«é€Ÿæ‰§è¡Œé˜¶æ®µç”±ä¼ ç»Ÿçš„è§„åˆ’å™¨æˆ–æ§åˆ¶å™¨è´Ÿè´£ï¼Œæ ¹æ®é«˜çº§åˆ«çš„è§„åˆ’ç”Ÿæˆå…·ä½“çš„è½¦è¾†æ§åˆ¶æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¯¹è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„VLAæ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿçš„åˆ†ç±»å’Œæ€»ç»“ï¼Œå¹¶æŒ‡å‡ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚è®ºæ–‡æå‡ºçš„åˆ†ç±»æ¡†æ¶ï¼ˆç«¯åˆ°ç«¯VLAå’ŒåŒç³»ç»ŸVLAï¼‰æœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œæ¯”è¾ƒä¸åŒçš„VLAæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ€»ç»“äº†ç”¨äºè¯„ä¼°VLAæ¨¡å‹çš„ä»£è¡¨æ€§æ•°æ®é›†å’ŒåŸºå‡†ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å‚è€ƒã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æ²¡æœ‰æå‡ºæ–°çš„æ¨¡å‹æˆ–ç®—æ³•ï¼Œè€Œæ˜¯ä¸€ç¯‡ç»¼è¿°æ€§æ–‡ç« ï¼Œå› æ­¤æ²¡æœ‰å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ã€‚ä½†æ˜¯ï¼Œè®ºæ–‡å¯¹ç°æœ‰VLAæ¨¡å‹çš„æŠ€æœ¯ç»†èŠ‚è¿›è¡Œäº†è¯¦ç»†çš„æè¿°ï¼ŒåŒ…æ‹¬ä¸åŒæ¨¡å‹çš„æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°æŒ‡æ ‡ç­‰ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16760v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16760v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16760v1/figures/fig3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡æ˜¯ä¸€ç¯‡ç»¼è¿°æ€§æ–‡ç« ï¼Œæ²¡æœ‰å…·ä½“çš„å®éªŒç»“æœã€‚ä½†æ˜¯ï¼Œè®ºæ–‡æ€»ç»“äº†ç°æœ‰VLAæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶æŒ‡å‡ºäº†ä¸åŒæ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚ä¾‹å¦‚ï¼Œä¸€äº›ç«¯åˆ°ç«¯VLAæ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸‹å–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œä½†æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼›è€ŒåŒç³»ç»ŸVLAæ¨¡å‹åˆ™åœ¨å®‰å…¨æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯¹è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚VLAæ¨¡å‹æœ‰æœ›æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ã€å¯é æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„äº¤é€šç¯å¢ƒã€‚æ­¤å¤–ï¼ŒVLAæ¨¡å‹è¿˜å¯ä»¥å®ç°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ï¼Œä¾‹å¦‚é€šè¿‡è¯­éŸ³æŒ‡ä»¤æ§åˆ¶è½¦è¾†çš„è¡Œé©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous driving has long relied on modular "Perception-Decision-Action" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.

