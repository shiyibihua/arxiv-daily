---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-12-18
---

# cs.ROï¼ˆ2025-12-18ï¼‰

ğŸ“Š å…± **12** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (8 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251216446v1-e-sds-environment-aware-see-it-do-it-sorted-automated-environment-aw.html">E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion</a></td>
  <td>E-SDSï¼šç¯å¢ƒæ„ŸçŸ¥çš„äººå½¢æœºå™¨äººå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°å¤æ‚åœ°å½¢ç¨³å¥è¡Œèµ°</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid locomotion</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16446v1" data-paper-url="./papers/251216446v1-e-sds-environment-aware-see-it-do-it-sorted-automated-environment-aw.html" onclick="toggleFavorite(this, '2512.16446v1', 'E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251216724v1-verm-leveraging-foundation-models-to-create-a-virtual-eye-for-effici.html">VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation</a></td>
  <td>VERMï¼šåˆ©ç”¨åŸºç¡€æ¨¡å‹ä¸ºæœºå™¨äººæ“ä½œæ„å»ºé«˜æ•ˆ3Dè™šæ‹Ÿè§†è§‰</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16724v1" data-paper-url="./papers/251216724v1-verm-leveraging-foundation-models-to-create-a-virtual-eye-for-effici.html" onclick="toggleFavorite(this, '2512.16724v1', 'VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251216793v1-physbrain-human-egocentric-data-as-a-bridge-from-vision-language-mod.html">PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence</a></td>
  <td>æå‡ºPhysBrainï¼Œåˆ©ç”¨äººç±»ç¬¬ä¸€è§†è§’æ•°æ®æå‡æœºå™¨äººç‰©ç†æ™ºèƒ½</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16793v1" data-paper-url="./papers/251216793v1-physbrain-human-egocentric-data-as-a-bridge-from-vision-language-mod.html" onclick="toggleFavorite(this, '2512.16793v1', 'PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251216302v1-manilong-shot-interaction-aware-one-shot-imitation-learning-for-long.html">ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation</a></td>
  <td>ManiLong-Shotï¼šäº¤äº’æ„ŸçŸ¥çš„å•æ ·æœ¬æ¨¡ä»¿å­¦ä¹ ç”¨äºé•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16302v1" data-paper-url="./papers/251216302v1-manilong-shot-interaction-aware-one-shot-imitation-learning-for-long.html" onclick="toggleFavorite(this, '2512.16302v1', 'ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251216861v1-reinforcegen-hybrid-skill-policies-with-automated-data-generation-an.html">ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning</a></td>
  <td>ReinforceGenï¼šç»“åˆè‡ªåŠ¨æ•°æ®ç”Ÿæˆä¸å¼ºåŒ–å­¦ä¹ çš„æ··åˆæŠ€èƒ½ç­–ç•¥ï¼Œè§£å†³é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion planning</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16861v1" data-paper-url="./papers/251216861v1-reinforcegen-hybrid-skill-policies-with-automated-data-generation-an.html" onclick="toggleFavorite(this, '2512.16861v1', 'ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251216896v1-sceniris-a-fast-procedural-scene-generation-framework.html">Sceniris: A Fast Procedural Scene Generation Framework</a></td>
  <td>Scenirisï¼šä¸€ç§å¿«é€Ÿç¨‹åºåŒ–åœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼ŒåŠ é€Ÿç‰©ç†AIå’Œç”Ÿæˆæ¨¡å‹å¼€å‘ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">spatial relationship</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16896v1" data-paper-url="./papers/251216896v1-sceniris-a-fast-procedural-scene-generation-framework.html" onclick="toggleFavorite(this, '2512.16896v1', 'Sceniris: A Fast Procedural Scene Generation Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251216069v1-a-task-driven-planner-in-the-loop-computational-design-framework-for.html">A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators</a></td>
  <td>æå‡ºä»»åŠ¡é©±åŠ¨çš„æ¨¡å—åŒ–æœºæ¢°è‡‚è®¡ç®—è®¾è®¡æ¡†æ¶ï¼Œå®ç°å½¢æ€ä¸è¿åŠ¨çš„ååŒä¼˜åŒ–</td>
  <td class="tags-cell"><span class="paper-tag">model predictive control</span> <span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16069v1" data-paper-url="./papers/251216069v1-a-task-driven-planner-in-the-loop-computational-design-framework-for.html" onclick="toggleFavorite(this, '2512.16069v1', 'A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251216449v1-single-view-shape-completion-for-robotic-grasping-in-clutter.html">Single-View Shape Completion for Robotic Grasping in Clutter</a></td>
  <td>æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„å•è§†è§’å½¢çŠ¶è¡¥å…¨æ–¹æ³•ï¼Œæå‡å¤æ‚åœºæ™¯ä¸‹æœºå™¨äººæŠ“å–æˆåŠŸç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16449v1" data-paper-url="./papers/251216449v1-single-view-shape-completion-for-robotic-grasping-in-clutter.html" onclick="toggleFavorite(this, '2512.16449v1', 'Single-View Shape Completion for Robotic Grasping in Clutter')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/251216760v1-vision-language-action-models-for-autonomous-driving-past-present-an.html">Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future</a></td>
  <td>ç»¼è¿°æ€§è®ºæ–‡ï¼šé¢å‘è‡ªåŠ¨é©¾é©¶çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ç ”ç©¶è¿›å±•ä¸æœªæ¥å±•æœ›</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16760v1" data-paper-url="./papers/251216760v1-vision-language-action-models-for-autonomous-driving-past-present-an.html" onclick="toggleFavorite(this, '2512.16760v1', 'Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251216881v1-polaris-scalable-real-to-sim-evaluations-for-generalist-robot-polici.html">PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies</a></td>
  <td>PolaRiSï¼šé¢å‘é€šç”¨æœºå™¨äººç­–ç•¥çš„å¯æ‰©å±•çœŸå®åˆ°ä»¿çœŸè¯„ä¼°æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16881v1" data-paper-url="./papers/251216881v1-polaris-scalable-real-to-sim-evaluations-for-generalist-robot-polici.html" onclick="toggleFavorite(this, '2512.16881v1', 'PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/251216367v1-a2visr-an-active-and-adaptive-ground-aerial-localization-system-usin.html">A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion</a></td>
  <td>æå‡ºA2VISRï¼Œä¸€ç§èåˆè§†è§‰æƒ¯æ€§å’Œå•æµ‹è·çš„ä¸»åŠ¨è‡ªé€‚åº”åœ°-ç©ºååŒå®šä½ç³»ç»Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16367v1" data-paper-url="./papers/251216367v1-a2visr-an-active-and-adaptive-ground-aerial-localization-system-usin.html" onclick="toggleFavorite(this, '2512.16367v1', 'A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251216265v1-privacy-aware-sharing-of-raw-spatial-sensor-data-for-cooperative-per.html">Privacy-Aware Sharing of Raw Spatial Sensor Data for Cooperative Perception</a></td>
  <td>æå‡ºSHARPæ¡†æ¶ï¼Œæ—¨åœ¨æœ€å°åŒ–åŸå§‹ç©ºé—´ä¼ æ„Ÿå™¨æ•°æ®å…±äº«ä¸­çš„éšç§æ³„éœ²ï¼Œä¿ƒè¿›è½¦è¾†ååŒæ„ŸçŸ¥ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16265v1" data-paper-url="./papers/251216265v1-privacy-aware-sharing-of-raw-spatial-sensor-data-for-cooperative-per.html" onclick="toggleFavorite(this, '2512.16265v1', 'Privacy-Aware Sharing of Raw Spatial Sensor Data for Cooperative Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)