---
layout: default
title: INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM
---

# INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04931" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04931v1</a>
  <a href="https://arxiv.org/pdf/2508.04931.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04931v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04931v1', 'INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jin Wang, Weijie Wang, Boyuan Deng, Heng Zhang, Rui Dai, Nikos Tsagarakis

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: Project Web: https://robo-intention.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºINTENTIONæ¡†æ¶ä»¥è§£å†³æœºå™¨äººè¿åŠ¨æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æœºå™¨äººè¿åŠ¨æ¨ç†` `è§†è§‰-è¯­è¨€æ¨¡å‹` `äº¤äº’è®°å¿†` `è‡ªä¸»æ“ä½œ` `äººç±»ç›´è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººæ§åˆ¶æ–¹æ³•ä¾èµ–äºç²¾ç¡®çš„ç‰©ç†æ¨¡å‹ï¼Œéš¾ä»¥é€‚åº”çœŸå®ä¸–ç•Œä¸­çš„å¤æ‚æ€§å’Œå˜åŒ–ã€‚
2. INTENTIONæ¡†æ¶é€šè¿‡ç»“åˆè§†è§‰-è¯­è¨€æ¨¡å‹å’Œäº¤äº’è®°å¿†ï¼Œèµ‹äºˆæœºå™¨äººè‡ªä¸»æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼Œæå‡å…¶é€‚åº”æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒINTENTIONåœ¨å¤šç§ä»»åŠ¡åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¨æ–­äº¤äº’è¡Œä¸ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„æœºå™¨äººæ§åˆ¶ä¸è§„åˆ’æ–¹æ³•ä¾èµ–äºç²¾ç¡®çš„ç‰©ç†æ¨¡å‹å’Œé¢„å®šä¹‰çš„åŠ¨ä½œåºåˆ—ï¼Œè™½ç„¶åœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­æœ‰æ•ˆï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­å¸¸å› å»ºæ¨¡ä¸å‡†ç¡®è€Œå¤±æ•ˆï¼Œä¸”éš¾ä»¥æ¨å¹¿åˆ°æ–°ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºINTENTIONæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é©±åŠ¨çš„åœºæ™¯æ¨ç†ä¸äº¤äº’é©±åŠ¨çš„è®°å¿†ï¼Œèµ‹äºˆæœºå™¨äººå­¦ä¹ çš„äº¤äº’ç›´è§‰å’Œè‡ªä¸»æ“ä½œèƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥è®°å¿†å›¾ï¼ˆMemory Graphï¼‰è®°å½•å…ˆå‰ä»»åŠ¡äº¤äº’ä¸­çš„åœºæ™¯ï¼Œä½“ç°äººç±»å¯¹ä¸åŒä»»åŠ¡çš„ç†è§£ä¸å†³ç­–ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ç›´è§‚æ„ŸçŸ¥å™¨ï¼ˆIntuitive Perceptorï¼‰ï¼Œä»è§†è§‰åœºæ™¯ä¸­æå–ç‰©ç†å…³ç³»å’Œå¯ç”¨æ€§ã€‚è¿™äº›ç»„ä»¶ä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨æ–°åœºæ™¯ä¸­æ¨æ–­é€‚å½“çš„äº¤äº’è¡Œä¸ºï¼Œè€Œæ— éœ€ä¾èµ–é‡å¤æŒ‡ä»¤ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿæœºå™¨äººæ§åˆ¶æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­çš„é€‚åº”æ€§ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹å»ºæ¨¡ä¸å‡†ç¡®å’Œæ–°ä»»åŠ¡æ—¶çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå›ºå®šçš„ç‰©ç†æ¨¡å‹å’ŒåŠ¨ä½œåºåˆ—ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚çš„ç°å®ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šINTENTIONæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å­¦ä¹ äº¤äº’ç›´è§‰å’Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œåœºæ™¯æ¨ç†ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨æ–°ç¯å¢ƒä¸­è‡ªä¸»æ¨æ–­äº¤äº’è¡Œä¸ºã€‚é€šè¿‡å¼•å…¥è®°å¿†å›¾ï¼Œæœºå™¨äººèƒ½å¤Ÿè®°å½•å’Œåˆ©ç”¨å…ˆå‰çš„ä»»åŠ¡ç»éªŒï¼Œä»è€Œå®ç°æ›´çµæ´»çš„å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šINTENTIONæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šè®°å¿†å›¾ï¼ˆMemory Graphï¼‰å’Œç›´è§‚æ„ŸçŸ¥å™¨ï¼ˆIntuitive Perceptorï¼‰ã€‚è®°å¿†å›¾ç”¨äºå­˜å‚¨å’Œç®¡ç†æœºå™¨äººä¸ç¯å¢ƒçš„äº¤äº’å†å²ï¼Œè€Œç›´è§‚æ„ŸçŸ¥å™¨åˆ™è´Ÿè´£ä»è§†è§‰è¾“å…¥ä¸­æå–ç‰©ç†å…³ç³»å’Œå¯ç”¨æ€§ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†äº¤äº’é©±åŠ¨çš„è®°å¿†æœºåˆ¶ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåƒäººç±»ä¸€æ ·ï¼Œé€šè¿‡ç»éªŒå­¦ä¹ å’Œæ¨ç†æ¥é€‚åº”æ–°ä»»åŠ¡ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„åŸºäºæ¨¡å‹çš„æ§åˆ¶æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œåè€…å¾€å¾€ç¼ºä¹çµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼Œè®°å¿†å›¾çš„æ„å»ºå’Œæ›´æ–°æœºåˆ¶æ˜¯æ ¸å¿ƒï¼Œç¡®ä¿æœºå™¨äººèƒ½å¤Ÿæœ‰æ•ˆåœ°è®°å½•å’Œåˆ©ç”¨äº¤äº’ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç›´è§‚æ„ŸçŸ¥å™¨çš„ç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æé«˜ä»è§†è§‰åœºæ™¯ä¸­æå–ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒINTENTIONæ¡†æ¶åœ¨å¤šç§ä»»åŠ¡åœºæ™¯ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå…·ä½“è€Œè¨€ï¼Œæœºå™¨äººåœ¨æ–°ä»»åŠ¡ä¸­çš„äº¤äº’è¡Œä¸ºæ¨æ–­å‡†ç¡®ç‡æé«˜äº†çº¦30%ï¼Œå¹¶ä¸”åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›æ˜¾è‘—å¢å¼ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

INTENTIONæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æœåŠ¡æœºå™¨äººã€å®¶åº­è‡ªåŠ¨åŒ–å’Œå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹ç¯å¢ƒçš„ç†è§£å’Œé€‚åº”èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ˜¾è‘—æé«˜æœºå™¨äººåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional control and planning for robotic manipulation heavily rely on precise physical models and predefined action sequences. While effective in structured environments, such approaches often fail in real-world scenarios due to modeling inaccuracies and struggle to generalize to novel tasks. In contrast, humans intuitively interact with their surroundings, demonstrating remarkable adaptability, making efficient decisions through implicit physical understanding. In this work, we propose INTENTION, a novel framework enabling robots with learned interactive intuition and autonomous manipulation in diverse scenarios, by integrating Vision-Language Models (VLMs) based scene reasoning with interaction-driven memory. We introduce Memory Graph to record scenes from previous task interactions which embodies human-like understanding and decision-making about different tasks in real world. Meanwhile, we design an Intuitive Perceptor that extracts physical relations and affordances from visual scenes. Together, these components empower robots to infer appropriate interaction behaviors in new scenes without relying on repetitive instructions. Videos: https://robo-intention.github.io

