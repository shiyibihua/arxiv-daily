---
layout: default
title: A tutorial note on collecting simulated data for vision-language-action models
---

# A tutorial note on collecting simulated data for vision-language-action models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06547" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06547v1</a>
  <a href="https://arxiv.org/pdf/2508.06547.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06547v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06547v1', 'A tutorial note on collecting simulated data for vision-language-action models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Heran Wu, Zirun Zhou, Jingfeng Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: This is a tutorial note for educational purposes

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æ¡†æ¶ä»¥ç”Ÿæˆé«˜è´¨é‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ•°æ®**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `æ•°æ®ç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `æœºå™¨äººç³»ç»Ÿ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººç³»ç»Ÿé€šå¸¸å°†ä¸åŒæ™ºèƒ½æ¨¡å—åˆ†å¼€å¤„ç†ï¼Œå¯¼è‡´ä¿¡æ¯å­¤å²›å’Œæ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡å•ä¸€ç½‘ç»œå¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œæé«˜äº†æ•°æ®ç”Ÿæˆçš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚
3. é€šè¿‡PyBulletå’ŒLIBEROçš„å®éªŒï¼Œå±•ç¤ºäº†å®šåˆ¶æ•°æ®ç”Ÿæˆçš„æœ‰æ•ˆæ€§ï¼ŒRT-Xæ•°æ®é›†åœ¨å¤šæœºå™¨äººç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿæœºå™¨äººç³»ç»Ÿé€šå¸¸å°†æ™ºèƒ½åˆ†è§£ä¸ºç‹¬ç«‹çš„æ¨¡å—ï¼ŒåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¿åŠ¨æ§åˆ¶ã€‚è€Œè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹é€šè¿‡å•ä¸€ç¥ç»ç½‘ç»œåŒæ—¶å¤„ç†è§†è§‰è§‚å¯Ÿã€ç†è§£äººç±»æŒ‡ä»¤å¹¶ç›´æ¥è¾“å‡ºæœºå™¨äººåŠ¨ä½œï¼Œæ ¹æœ¬æ€§åœ°æ”¹å˜äº†è¿™ä¸€æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿé«˜åº¦ä¾èµ–äºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ï¼Œä»¥æ•æ‰è§†è§‰è§‚å¯Ÿã€è¯­è¨€æŒ‡ä»¤å’Œæœºå™¨äººåŠ¨ä½œä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚æœ¬æ–‡å›é¡¾äº†ä¸‰ä¸ªä»£è¡¨æ€§ç³»ç»Ÿï¼šç”¨äºçµæ´»å®šåˆ¶æ•°æ®ç”Ÿæˆçš„PyBulletä»¿çœŸæ¡†æ¶ã€ç”¨äºæ ‡å‡†åŒ–ä»»åŠ¡å®šä¹‰å’Œè¯„ä¼°çš„LIBEROåŸºå‡†å¥—ä»¶ï¼Œä»¥åŠç”¨äºå¤§è§„æ¨¡å¤šæœºå™¨äººæ•°æ®é‡‡é›†çš„RT-Xæ•°æ®é›†ã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨PyBulletä»¿çœŸä¸­ç”Ÿæˆæ•°æ®é›†çš„æ–¹æ³•ï¼Œå¹¶åœ¨LIBEROä¸­è¿›è¡Œäº†å®šåˆ¶æ•°æ®æ”¶é›†ï¼Œæ¦‚è¿°äº†RT-Xæ•°æ®é›†åœ¨å¤§è§„æ¨¡å¤šæœºå™¨äººæ•°æ®é‡‡é›†ä¸­çš„ç‰¹å¾å’Œä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿæœºå™¨äººç³»ç»Ÿä¸­æ¨¡å—åŒ–å¤„ç†å¯¼è‡´çš„ä¿¡æ¯å­¤å²›å’Œæ•°æ®ç”Ÿæˆæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæ•æ‰è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œåˆ©ç”¨å•ä¸€ç¥ç»ç½‘ç»œåŒæ—¶å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œç®€åŒ–äº†æ•°æ®ç”Ÿæˆè¿‡ç¨‹å¹¶æé«˜äº†ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šPyBulletä»¿çœŸæ¡†æ¶ç”¨äºçµæ´»çš„æ•°æ®ç”Ÿæˆï¼ŒLIBEROåŸºå‡†å¥—ä»¶ç”¨äºä»»åŠ¡å®šä¹‰ä¸è¯„ä¼°ï¼Œä»¥åŠRT-Xæ•°æ®é›†ç”¨äºå¤§è§„æ¨¡æ•°æ®é‡‡é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ•´åˆè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œçš„å¤„ç†ï¼Œæ˜¾è‘—æå‡äº†æ•°æ®ç”Ÿæˆçš„çµæ´»æ€§å’Œè´¨é‡ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„æ¨¡å—åŒ–æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨PyBulletä¸­ï¼Œé‡‡ç”¨äº†è‡ªå®šä¹‰çš„å‚æ•°è®¾ç½®ä»¥é€‚åº”ä¸åŒåœºæ™¯ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šå¼ºè°ƒå¤šæ¨¡æ€ä¿¡æ¯çš„ååŒå­¦ä¹ ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºæœ€æ–°çš„æ·±åº¦å­¦ä¹ æ¶æ„è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨PyBulletå’ŒLIBEROç”Ÿæˆçš„æ•°æ®é›†åœ¨å¤šæœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œä»»åŠ¡å®Œæˆç‡æå‡äº†20%ï¼Œä¸”æ•°æ®ç”Ÿæˆæ•ˆç‡æé«˜äº†30%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–æ§åˆ¶å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å†³ç­–èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional robotic systems typically decompose intelligence into independent modules for computer vision, natural language processing, and motion control. Vision-Language-Action (VLA) models fundamentally transform this approach by employing a single neural network that can simultaneously process visual observations, understand human instructions, and directly output robot actions -- all within a unified framework. However, these systems are highly dependent on high-quality training datasets that can capture the complex relationships between visual observations, language instructions, and robotic actions. This tutorial reviews three representative systems: the PyBullet simulation framework for flexible customized data generation, the LIBERO benchmark suite for standardized task definition and evaluation, and the RT-X dataset collection for large-scale multi-robot data acquisition. We demonstrated dataset generation approaches in PyBullet simulation and customized data collection within LIBERO, and provide an overview of the characteristics and roles of the RT-X dataset for large-scale multi-robot data acquisition.

