---
layout: default
title: Improving Tactile Gesture Recognition with Optical Flow
---

# Improving Tactile Gesture Recognition with Optical Flow

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04338" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04338v1</a>
  <a href="https://arxiv.org/pdf/2508.04338.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04338v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04338v1', 'Improving Tactile Gesture Recognition with Optical Flow')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaohong Zhong, Alessandro Albini, Giammarco Caroleo, Giorgio Cannata, Perla Maiolino

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: 7 pages, 7 figures, paper accepted by the 2025 34th IEEE International Conference on Robot and Human Interactive Communication (ROMAN)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å…‰æµå¢å¼ºè§¦è§‰æ‰‹åŠ¿è¯†åˆ«çš„å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è§¦è§‰æ‰‹åŠ¿è¯†åˆ«` `å…‰æµ` `äººæœºäº¤äº’` `æœºå™¨å­¦ä¹ ` `å·ç§¯ç¥ç»ç½‘ç»œ` `åŠ¨æ€ç‰¹å¾æå–` `æœºå™¨äººæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§¦è§‰æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•ä¸»è¦ä¾èµ–äºè§¦è§‰å›¾åƒï¼Œéš¾ä»¥åŒºåˆ†æŸäº›ç›¸ä¼¼çš„æ‰‹åŠ¿ï¼Œå¯¼è‡´è¯†åˆ«å‡†ç¡®ç‡ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è®¡ç®—è§¦è§‰å›¾åƒçš„å¯†é›†å…‰æµï¼Œæ˜¾è‘—çªå‡ºæ¥è§¦åŠ¨æ€ï¼Œä»è€Œæ”¹å–„æ‰‹åŠ¿è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å…‰æµä¿¡æ¯å¢å¼ºçš„è§¦è§‰å›¾åƒè®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œæ‰‹åŠ¿åˆ†ç±»å‡†ç¡®ç‡æé«˜äº†9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§¦è§‰æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿåœ¨äººä¸æœºå™¨äººäº¤äº’ä¸­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿå®ç°ç›´è§‚çš„æ²Ÿé€šã€‚ç°æœ‰æ–‡çŒ®ä¸»è¦é€šè¿‡æœºå™¨å­¦ä¹ æŠ€æœ¯å¯¹è§¦è§‰å›¾åƒåºåˆ—è¿›è¡Œåˆ†ç±»ï¼Œä½†ä»…ä¾é è§¦è§‰å›¾åƒçš„ä¿¡æ¯ï¼ŒæŸäº›æ‰‹åŠ¿éš¾ä»¥åŒºåˆ†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—å¯†é›†å…‰æµæ¥çªå‡ºè§¦è§‰å›¾åƒä¸­çš„æ¥è§¦åŠ¨æ€ï¼Œä»è€Œæé«˜æ‰‹åŠ¿è¯†åˆ«åˆ†ç±»å™¨çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å…‰æµä¿¡æ¯å¢å¼ºçš„è§¦è§‰å›¾åƒè®­ç»ƒçš„åˆ†ç±»å™¨ç›¸æ¯”äºæ ‡å‡†è§¦è§‰å›¾åƒè®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œæ‰‹åŠ¿åˆ†ç±»å‡†ç¡®ç‡æé«˜äº†9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§¦è§‰æ‰‹åŠ¿è¯†åˆ«ä¸­ï¼Œä¾èµ–è§¦è§‰å›¾åƒå¯¼è‡´çš„æ‰‹åŠ¿åŒºåˆ†å›°éš¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç›¸ä¼¼æ‰‹åŠ¿æ—¶ï¼Œå‡†ç¡®ç‡ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰æ¥è§¦åŠ¨æ€ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¡ç®—è§¦è§‰å›¾åƒçš„å¯†é›†å…‰æµï¼Œæ¥å¢å¼ºè¾“å…¥æ•°æ®çš„åŠ¨æ€ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡ä½¿å¾—åˆ†ç±»å™¨èƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†åœ¨è§¦è§‰å›¾åƒä¸Šç›¸ä¼¼ä½†æ¥è§¦åŠ¨æ€ä¸åŒçš„æ‰‹åŠ¿ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å…‰æµè®¡ç®—ã€ç‰¹å¾æå–å’Œåˆ†ç±»å™¨è®­ç»ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå¯¹è§¦è§‰å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼Œç„¶åè®¡ç®—å…¶å…‰æµï¼Œæ¥ç€æå–ç‰¹å¾ï¼Œæœ€åä½¿ç”¨å¢å¼ºåçš„æ•°æ®è®­ç»ƒåˆ†ç±»å™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥å…‰æµä¿¡æ¯ä½œä¸ºè§¦è§‰å›¾åƒçš„è¡¥å……ç‰¹å¾ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–é™æ€è§¦è§‰å›¾åƒçš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œå…‰æµè®¡ç®—é‡‡ç”¨äº†å¯†é›†å…‰æµç®—æ³•ï¼ŒæŸå¤±å‡½æ•°ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œè®¾è®¡ï¼Œä»¥é€‚åº”å¢å¼ºåçš„è¾“å…¥ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å…‰æµä¿¡æ¯å¢å¼ºçš„è§¦è§‰å›¾åƒè®­ç»ƒçš„åˆ†ç±»å™¨ç›¸æ¯”äºæ ‡å‡†è§¦è§‰å›¾åƒè®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œæ‰‹åŠ¿åˆ†ç±»å‡†ç¡®ç‡æé«˜äº†9%ã€‚è¿™ä¸€æ˜¾è‘—æå‡å±•ç¤ºäº†å…‰æµåœ¨è§¦è§‰æ‰‹åŠ¿è¯†åˆ«ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½å®¶å±…ç­‰ã€‚é€šè¿‡æé«˜è§¦è§‰æ‰‹åŠ¿è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶çš„äº¤äº’æ–¹å¼ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œæœªæ¥å¯èƒ½åœ¨åŒ»ç–—ã€è¾…åŠ©æŠ€æœ¯ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œå¸®åŠ©æ®‹éšœäººå£«æ›´å¥½åœ°ä¸ç¯å¢ƒäº’åŠ¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tactile gesture recognition systems play a crucial role in Human-Robot Interaction (HRI) by enabling intuitive communication between humans and robots. The literature mainly addresses this problem by applying machine learning techniques to classify sequences of tactile images encoding the pressure distribution generated when executing the gestures. However, some gestures can be hard to differentiate based on the information provided by tactile images alone. In this paper, we present a simple yet effective way to improve the accuracy of a gesture recognition classifier. Our approach focuses solely on processing the tactile images used as input by the classifier. In particular, we propose to explicitly highlight the dynamics of the contact in the tactile image by computing the dense optical flow. This additional information makes it easier to distinguish between gestures that produce similar tactile images but exhibit different contact dynamics. We validate the proposed approach in a tactile gesture recognition task, showing that a classifier trained on tactile images augmented with optical flow information achieved a 9% improvement in gesture classification accuracy compared to one trained on standard tactile images.

