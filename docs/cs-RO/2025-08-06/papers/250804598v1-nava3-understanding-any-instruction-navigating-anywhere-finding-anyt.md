---
layout: default
title: $NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything
---

# $NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04598" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.04598v1</a>
  <a href="https://arxiv.org/pdf/2508.04598.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04598v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04598v1', '$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Lingfeng Zhang, Xiaoshuai Hao, Yingbo Tang, Haoxiang Fu, Xinyu Zheng, Pengwei Wang, Zhongyuan Wang, Wenbo Ding, Shanghang Zhang

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-06

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://NavigationA3.github.io/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫$NavA^3$‰ª•Ëß£ÂÜ≥Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈïøÊó∂ÂØºËà™ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂÖ∑Ë∫´ÂØºËà™` `È´òÂ±ÇÊåá‰ª§ÁêÜËß£` `Á©∫Èó¥ÊÑüÁü•` `ÂºÄÊîæËØçÊ±áÂÆö‰Ωç` `ÈïøÊó∂ÂØºËà™‰ªªÂä°` `Êú∫Âô®‰∫∫Êô∫ËÉΩ` `Â§çÊùÇÁéØÂ¢É`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂÖ∑Ë∫´ÂØºËà™ÊñπÊ≥ïÂú®ÁêÜËß£È´òÂ±ÇÊ¨°‰∫∫Á±ªÊåá‰ª§ÂíåÂºÄÊîæËØçÊ±áÂØπË±°ÂÆö‰ΩçÊñπÈù¢Â≠òÂú®ÊòæËëó‰∏çË∂≥ÔºåÈöæ‰ª•Êª°Ë∂≥Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂØºËà™ÈúÄÊ±Ç„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑ$NavA^3$Ê°ÜÊû∂ÈÄöËøáÂÖ®Â±ÄÂíåÂ±ÄÈÉ®Á≠ñÁï•ÁöÑÁªìÂêàÔºåÂà©Áî®Reasoning-VLMËß£ÊûêÊåá‰ª§Âπ∂ËøõË°åÁ©∫Èó¥ÊÑüÁü•ÂØπË±°ÂØºËà™„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå$NavA^3$Âú®ÂØºËà™ÊÄßËÉΩ‰∏äËææÂà∞‰∫ÜÂΩìÂâçÊúÄ‰ºòÊ∞¥Âπ≥ÔºåËÉΩÂ§üÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÊàêÂäüÂÆåÊàêÈïøÊó∂ÂØºËà™‰ªªÂä°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂÖ∑Ë∫´ÂØºËà™ÊòØÂÖ∑Ë∫´Êô∫ËÉΩÁöÑÂü∫Êú¨ËÉΩÂäõÔºå‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®Áâ©ÁêÜÁéØÂ¢É‰∏≠ÁßªÂä®Âíå‰∫íÂä®„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂØºËà™‰ªªÂä°‰∏ªË¶ÅÈõÜ‰∏≠Âú®È¢ÑÂÆö‰πâÁöÑÂØπË±°ÂØºËà™ÊàñÊåá‰ª§Ë∑üÈöè‰∏äÔºåËøô‰∏é‰∫∫Á±ªÂú®Â§çÊùÇÂºÄÊîæÂú∫ÊôØ‰∏≠ÁöÑÈúÄÊ±ÇÊúâÊòæËëóÂ∑ÆÂºÇ„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÈ°πÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈïøÊó∂ÂØºËà™‰ªªÂä°ÔºåË¶ÅÊ±ÇÁêÜËß£È´òÂ±ÇÊ¨°ÁöÑ‰∫∫Á±ªÊåá‰ª§Âπ∂Âú®ÁúüÂÆûÁéØÂ¢É‰∏≠ËøõË°åÁ©∫Èó¥ÊÑüÁü•ÁöÑÂØπË±°ÂØºËà™„ÄÇÁé∞ÊúâÁöÑÂÖ∑Ë∫´ÂØºËà™ÊñπÊ≥ïÂú®ÁêÜËß£È´òÂ±ÇÊåá‰ª§ÂíåÂºÄÊîæËØçÊ±áÁöÑÂØπË±°ÂÆö‰ΩçÊñπÈù¢Â≠òÂú®Â±ÄÈôê„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑ$NavA^3$Ê°ÜÊû∂ÂàÜ‰∏∫ÂÖ®Â±ÄÂíåÂ±ÄÈÉ®Á≠ñÁï•‰∏§‰∏™Èò∂ÊÆµÔºåÂà©Áî®Reasoning-VLMËß£ÊûêÈ´òÂ±ÇÊåá‰ª§Âπ∂‰∏éÂÖ®Â±Ä3DÂú∫ÊôØËßÜÂõæÁªìÂêàÔºå‰ªéËÄåÂÆûÁé∞ÁõÆÊ†áÂØπË±°ÁöÑÂØºËà™„ÄÇÂÆûÈ™åË°®ÊòéÔºå$NavA^3$Âú®ÂØºËà™ÊÄßËÉΩ‰∏äËææÂà∞‰∫ÜSOTAÊ∞¥Âπ≥ÔºåËÉΩÂ§üÊàêÂäüÂÆåÊàê‰∏çÂêåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ‰∏ãÁöÑÈïøÊó∂ÂØºËà™‰ªªÂä°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂÖ∑Ë∫´ÂØºËà™‰∏≠ÂØπÈ´òÂ±ÇÊ¨°‰∫∫Á±ªÊåá‰ª§ÁêÜËß£‰∏çË∂≥ÂèäÂºÄÊîæËØçÊ±áÂØπË±°ÂÆö‰ΩçÂõ∞ÈöæÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§çÊùÇÁéØÂ¢É‰∏≠Èöæ‰ª•ÊúâÊïàÊâßË°åÈïøÊó∂ÂØºËà™‰ªªÂä°„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**Ôºö$NavA^3$Ê°ÜÊû∂ÈÄöËøáÂàÜÂ±ÇÁ≠ñÁï•ÔºåÈ¶ñÂÖàËß£ÊûêÈ´òÂ±ÇÊåá‰ª§Âπ∂ÁªìÂêàÂÖ®Â±ÄÂú∫ÊôØ‰ø°ÊÅØÔºåÁÑ∂ÂêéÂà©Áî®ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãËøõË°åÁ≤æÁ°ÆÁöÑÂØπË±°ÂÆö‰ΩçÂíåÂØºËà™„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠Êõ¥Â•ΩÂú∞ÁêÜËß£‰ªªÂä°Âπ∂ÊâßË°åÂØºËà™„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**Ôºö$NavA^3$Ê°ÜÊû∂ÂàÜ‰∏∫‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÂÖ®Â±ÄÁ≠ñÁï•ÂíåÂ±ÄÈÉ®Á≠ñÁï•„ÄÇÂú®ÂÖ®Â±ÄÁ≠ñÁï•‰∏≠Ôºå‰ΩøÁî®Reasoning-VLMËß£Êûê‰∫∫Á±ªÊåá‰ª§Âπ∂ÁîüÊàêÂÖ®Â±ÄÂú∫ÊôØËßÜÂõæÔºõÂú®Â±ÄÈÉ®Á≠ñÁï•‰∏≠ÔºåÂà©Áî®NaviAffordÊ®°ÂûãËøõË°åÁ©∫Èó¥ÊÑüÁü•ÂØπË±°ÂÆö‰Ωç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜÈ´òÂ±ÇÊ¨°Êåá‰ª§Ëß£Êûê‰∏éÁ©∫Èó¥ÊÑüÁü•ÂØπË±°ÂØºËà™Áõ∏ÁªìÂêàÔºåÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÈïøÊó∂ÂØºËà™‰ªªÂä°Ê°ÜÊû∂ÔºåÊòæËëóÊèêÂçá‰∫ÜÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂØºËà™ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Â±ÄÈÉ®Á≠ñÁï•‰∏≠ÔºåÊî∂ÈõÜ‰∫Ü100‰∏áÊ†∑Êú¨ÁöÑÁ©∫Èó¥ÊÑüÁü•ÂØπË±°ÂèØÁî®ÊÄßÊï∞ÊçÆÔºå‰ª•ËÆ≠ÁªÉNaviAffordÊ®°ÂûãÔºàPointingVLMÔºâÔºåËØ•Ê®°ÂûãÂÖ∑Â§áÂº∫Â§ßÁöÑÂºÄÊîæËØçÊ±áÂØπË±°ÂÆö‰ΩçËÉΩÂäõÂíåÁ©∫Èó¥ÊÑèËØÜÔºåÁ°Æ‰øù‰∫ÜÁõÆÊ†áËØÜÂà´ÂíåÂØºËà™ÁöÑÁ≤æÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå$NavA^3$Âú®ÈïøÊó∂ÂØºËà™‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÂØºËà™ÊàêÂäüÁéáÊèêÈ´ò‰∫ÜXX%ÔºåÂπ∂Âú®Â§ö‰∏™ÁúüÂÆûÁéØÂ¢É‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§çÊùÇÂú∫ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

$NavA^3$ÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊô∫ËÉΩÊú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüüÔºåÊèêÂçáÊú∫Âô®‰∫∫ÁöÑËá™‰∏ªÂØºËà™ËÉΩÂäõÔºåÊª°Ë∂≥Â§çÊùÇÁéØÂ¢É‰∏ãÁöÑÂÆûÈôÖÈúÄÊ±Ç„ÄÇÊú™Êù•ÔºåËØ•Ê°ÜÊû∂ÊúâÊúõÊé®Âä®ÂÖ∑Ë∫´Êô∫ËÉΩÁöÑÂèëÂ±ïÔºå‰ΩøÊú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§öÂèòÁöÑÁé∞ÂÆû‰∏ñÁïå„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Embodied navigation is a fundamental capability of embodied intelligence, enabling robots to move and interact within physical environments. However, existing navigation tasks primarily focus on predefined object navigation or instruction following, which significantly differs from human needs in real-world scenarios involving complex, open-ended scenes. To bridge this gap, we introduce a challenging long-horizon navigation task that requires understanding high-level human instructions and performing spatial-aware object navigation in real-world environments. Existing embodied navigation methods struggle with such tasks due to their limitations in comprehending high-level human instructions and localizing objects with an open vocabulary. In this paper, we propose $NavA^3$, a hierarchical framework divided into two stages: global and local policies. In the global policy, we leverage the reasoning capabilities of Reasoning-VLM to parse high-level human instructions and integrate them with global 3D scene views. This allows us to reason and navigate to regions most likely to contain the goal object. In the local policy, we have collected a dataset of 1.0 million samples of spatial-aware object affordances to train the NaviAfford model (PointingVLM), which provides robust open-vocabulary object localization and spatial awareness for precise goal identification and navigation in complex environments. Extensive experiments demonstrate that $NavA^3$ achieves SOTA results in navigation performance and can successfully complete longhorizon navigation tasks across different robot embodiments in real-world settings, paving the way for universal embodied navigation. The dataset and code will be made available. Project website: https://NavigationA3.github.io/.

