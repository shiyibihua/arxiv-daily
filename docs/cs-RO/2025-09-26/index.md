---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-09-26
---

# cs.ROï¼ˆ2025-09-26ï¼‰

ğŸ“Š å…± **31** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (23 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (23 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250922815v1-teleoperator-aware-and-safety-critical-adaptive-nonlinear-mpc-for-sh.html">Teleoperator-Aware and Safety-Critical Adaptive Nonlinear MPC for Shared Autonomy in Obstacle Avoidance of Legged Robots</a></td>
  <td>æå‡ºè‡ªé€‚åº”éçº¿æ€§æ¨¡å‹é¢„æµ‹æ§åˆ¶ä»¥è§£å†³å››è¶³æœºå™¨äººé¿éšœé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged robot</span> <span class="paper-tag">legged locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22815v1" data-paper-url="./papers/250922815v1-teleoperator-aware-and-safety-critical-adaptive-nonlinear-mpc-for-sh.html" onclick="toggleFavorite(this, '2509.22815v1', 'Teleoperator-Aware and Safety-Critical Adaptive Nonlinear MPC for Shared Autonomy in Obstacle Avoidance of Legged Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250922441v1-underwatervla-dual-brain-vision-language-action-architecture-for-aut.html">UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation</a></td>
  <td>æå‡ºUnderwaterVLAï¼Œç”¨äºæ°´ä¸‹è‡ªä¸»å¯¼èˆªï¼Œæå‡å¤æ‚æ°´åŸŸä»»åŠ¡å®Œæˆåº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22441v1" data-paper-url="./papers/250922441v1-underwatervla-dual-brain-vision-language-action-architecture-for-aut.html" onclick="toggleFavorite(this, '2509.22441v1', 'UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250922643v1-vla-reasoner-empowering-vision-language-action-models-with-reasoning.html">VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search</a></td>
  <td>VLA-Reasonerï¼šé€šè¿‡åœ¨çº¿è’™ç‰¹å¡æ´›æ ‘æœç´¢å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22643v1" data-paper-url="./papers/250922643v1-vla-reasoner-empowering-vision-language-action-models-with-reasoning.html" onclick="toggleFavorite(this, '2509.22643v1', 'VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250922199v2-mimicdreamer-aligning-human-and-robot-demonstrations-for-scalable-vl.html">MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training</a></td>
  <td>MimicDreamerï¼šå¯¹é½äººç±»ä¸æœºå™¨äººæ¼”ç¤ºï¼Œå®ç°å¯æ‰©å±•çš„VLAæ¨¡å‹è®­ç»ƒ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dreamer</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22199v2" data-paper-url="./papers/250922199v2-mimicdreamer-aligning-human-and-robot-demonstrations-for-scalable-vl.html" onclick="toggleFavorite(this, '2509.22199v2', 'MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250921810v1-learning-multi-skill-legged-locomotion-using-conditional-adversarial.html">Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors</a></td>
  <td>æå‡ºåŸºäºæ¡ä»¶å¯¹æŠ—è¿åŠ¨å…ˆéªŒçš„å¤šæŠ€èƒ½å››è¶³æœºå™¨äººè¿åŠ¨å­¦ä¹ æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged robot</span> <span class="paper-tag">legged locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21810v1" data-paper-url="./papers/250921810v1-learning-multi-skill-legged-locomotion-using-conditional-adversarial.html" onclick="toggleFavorite(this, '2509.21810v1', 'Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250921723v2-vlbiman-vision-language-anchored-one-shot-demonstration-enables-gene.html">VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Bimanual Robotic Manipulation</a></td>
  <td>VLBiManï¼šåŸºäºè§†è§‰-è¯­è¨€é”šå®šçš„å•æ ·æœ¬ç¤ºæ•™å®ç°é€šç”¨åŒè‡‚æœºå™¨äººæ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">dual-arm</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21723v2" data-paper-url="./papers/250921723v2-vlbiman-vision-language-anchored-one-shot-demonstration-enables-gene.html" onclick="toggleFavorite(this, '2509.21723v2', 'VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Bimanual Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250921986v1-developing-vision-language-action-model-from-egocentric-videos.html">Developing Vision-Language-Action Model from Egocentric Videos</a></td>
  <td>æå‡ºåŸºäºç¬¬ä¸€è§†è§’è§†é¢‘çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">teleoperation</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21986v1" data-paper-url="./papers/250921986v1-developing-vision-language-action-model-from-egocentric-videos.html" onclick="toggleFavorite(this, '2509.21986v1', 'Developing Vision-Language-Action Model from Egocentric Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250922093v1-action-aware-dynamic-pruning-for-efficient-vision-language-action-ma.html">Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation</a></td>
  <td>æå‡ºåŠ¨ä½œæ„ŸçŸ¥åŠ¨æ€å‰ªæADPï¼Œæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22093v1" data-paper-url="./papers/250922093v1-action-aware-dynamic-pruning-for-efficient-vision-language-action-ma.html" onclick="toggleFavorite(this, '2509.22093v1', 'Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250922195v1-actions-as-language-fine-tuning-vlms-into-vlas-without-catastrophic-.html">Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting</a></td>
  <td>æå‡ºVLM2VLAä»¥è§£å†³æœºå™¨äººé¥æ§ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">teleoperation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22195v1" data-paper-url="./papers/250922195v1-actions-as-language-fine-tuning-vlms-into-vlas-without-catastrophic-.html" onclick="toggleFavorite(this, '2509.22195v1', 'Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250922578v1-egodemogen-novel-egocentric-demonstration-generation-enables-viewpoi.html">EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation</a></td>
  <td>EgoDemoGenï¼šç”Ÿæˆæ–°é¢–çš„è‡ªæˆ‘ä¸­å¿ƒè§†è§’æ¼”ç¤ºï¼Œå®ç°è§†è§’é²æ£’çš„æœºå™¨äººæ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22578v1" data-paper-url="./papers/250922578v1-egodemogen-novel-egocentric-demonstration-generation-enables-viewpoi.html" onclick="toggleFavorite(this, '2509.22578v1', 'EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250922652v1-pixel-motion-diffusion-is-what-we-need-for-robot-control.html">Pixel Motion Diffusion is What We Need for Robot Control</a></td>
  <td>DAWNï¼šåŸºäºåƒç´ è¿åŠ¨æ‰©æ•£çš„æœºå™¨äººæ§åˆ¶ç»Ÿä¸€æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion diffusion</span> <span class="paper-tag">language conditioned</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22652v1" data-paper-url="./papers/250922652v1-pixel-motion-diffusion-is-what-we-need-for-robot-control.html" onclick="toggleFavorite(this, '2509.22652v1', 'Pixel Motion Diffusion is What We Need for Robot Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250922065v1-effect-of-gait-design-on-proprioceptive-sensing-of-terrain-propertie.html">Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot</a></td>
  <td>æ­¥æ€è®¾è®¡å½±å“å››è¶³æœºå™¨äººå¯¹åœ°å½¢å±æ€§çš„æœ¬ä½“æ„Ÿå—ï¼Œæå‡ºé€‚ç”¨äºè¡Œæ˜Ÿæ¢æµ‹çš„æ­¥æ€è®¾è®¡æ–¹æ¡ˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged robot</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22065v1" data-paper-url="./papers/250922065v1-effect-of-gait-design-on-proprioceptive-sensing-of-terrain-propertie.html" onclick="toggleFavorite(this, '2509.22065v1', 'Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250922914v1-armimic-learning-robotic-manipulation-from-passive-human-demonstrati.html">ARMimic: Learning Robotic Manipulation from Passive Human Demonstrations in Augmented Reality</a></td>
  <td>ARMimicï¼šåˆ©ç”¨å¢å¼ºç°å®ä¸­çš„è¢«åŠ¨äººç±»æ¼”ç¤ºå­¦ä¹ æœºå™¨äººæ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">teleoperation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22914v1" data-paper-url="./papers/250922914v1-armimic-learning-robotic-manipulation-from-passive-human-demonstrati.html" onclick="toggleFavorite(this, '2509.22914v1', 'ARMimic: Learning Robotic Manipulation from Passive Human Demonstrations in Augmented Reality')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250922205v2-from-watch-to-imagine-steering-long-horizon-manipulation-via-human-d.html">From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment</a></td>
  <td>Super-Mimicï¼šç»“åˆäººç±»æ¼”ç¤ºä¸æœªæ¥é¢„æµ‹ï¼Œå®ç°é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡çš„é›¶æ ·æœ¬æ¨¡ä»¿å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">physically plausible</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22205v2" data-paper-url="./papers/250922205v2-from-watch-to-imagine-steering-long-horizon-manipulation-via-human-d.html" onclick="toggleFavorite(this, '2509.22205v2', 'From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250922550v3-an-intention-driven-lane-change-framework-considering-heterogeneous-.html">An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment</a></td>
  <td>æå‡ºä¸€ç§è€ƒè™‘å¼‚æ„åŠ¨æ€åˆä½œçš„æ„å›¾é©±åŠ¨å‹è½¦é“å˜æ¢æ¡†æ¶ï¼Œç”¨äºæ··åˆäº¤é€šç¯å¢ƒã€‚</td>
  <td class="tags-cell"><span class="paper-tag">model predictive control</span> <span class="paper-tag">motion planning</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22550v3" data-paper-url="./papers/250922550v3-an-intention-driven-lane-change-framework-considering-heterogeneous-.html" onclick="toggleFavorite(this, '2509.22550v3', 'An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250922801v1-towards-developing-standards-and-guidelines-for-robot-grasping-and-m.html">Towards Developing Standards and Guidelines for Robot Grasping and Manipulation Pipelines in the COMPARE Ecosystem</a></td>
  <td>COMPAREç”Ÿæ€ç³»ç»Ÿï¼šæœºå™¨äººæŠ“å–ä¸æ“ä½œæµç¨‹æ ‡å‡†åŒ–æŒ‡å—å¼€å‘</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22801v1" data-paper-url="./papers/250922801v1-towards-developing-standards-and-guidelines-for-robot-grasping-and-m.html" onclick="toggleFavorite(this, '2509.22801v1', 'Towards Developing Standards and Guidelines for Robot Grasping and Manipulation Pipelines in the COMPARE Ecosystem')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250922120v1-multi-stage-robust-nonlinear-model-predictive-control-of-a-lower-lim.html">Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot</a></td>
  <td>æå‡ºå¤šé˜¶æ®µé²æ£’éçº¿æ€§æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼Œæå‡ä¸‹è‚¢å¤–éª¨éª¼æœºå™¨äººæ§åˆ¶çš„æŠ—æ‰°åŠ¨æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22120v1" data-paper-url="./papers/250922120v1-multi-stage-robust-nonlinear-model-predictive-control-of-a-lower-lim.html" onclick="toggleFavorite(this, '2509.22120v1', 'Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250922149v1-demograsp-universal-dexterous-grasping-from-a-single-demonstration.html">DemoGrasp: Universal Dexterous Grasping from a Single Demonstration</a></td>
  <td>DemoGraspï¼šåŸºäºå•æ¬¡æ¼”ç¤ºçš„é€šç”¨çµå·§æŠ“å–æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22149v1" data-paper-url="./papers/250922149v1-demograsp-universal-dexterous-grasping-from-a-single-demonstration.html" onclick="toggleFavorite(this, '2509.22149v1', 'DemoGrasp: Universal Dexterous Grasping from a Single Demonstration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250922356v1-roboview-bias-benchmarking-visual-bias-in-embodied-agents-for-roboti.html">RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation</a></td>
  <td>RoboView-Biasï¼šé¦–ä¸ªæœºå™¨äººæ“ä½œä¸­å…·èº«æ™ºèƒ½ä½“è§†è§‰åè§è¯„æµ‹åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22356v1" data-paper-url="./papers/250922356v1-roboview-bias-benchmarking-visual-bias-in-embodied-agents-for-roboti.html" onclick="toggleFavorite(this, '2509.22356v1', 'RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250921928v1-sage-scene-graph-aware-guidance-and-execution-for-long-horizon-manip.html">SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks</a></td>
  <td>SAGEï¼šåŸºäºåœºæ™¯å›¾çš„é•¿ç¨‹æ“ä½œä»»åŠ¡å¼•å¯¼ä¸æ‰§è¡Œæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21928v1" data-paper-url="./papers/250921928v1-sage-scene-graph-aware-guidance-and-execution-for-long-horizon-manip.html" onclick="toggleFavorite(this, '2509.21928v1', 'SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250922970v2-robot-learning-from-any-images.html">Robot Learning from Any Images</a></td>
  <td>RoLAï¼šä»ä»»æ„å›¾åƒç”Ÿæˆäº¤äº’å¼ç‰©ç†æœºå™¨äººç¯å¢ƒï¼Œå®ç°å¤§è§„æ¨¡æœºå™¨äººæ•°æ®ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">sim-to-real</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22970v2" data-paper-url="./papers/250922970v2-robot-learning-from-any-images.html" onclick="toggleFavorite(this, '2509.22970v2', 'Robot Learning from Any Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250922498v1-helios-hierarchical-exploration-for-language-grounded-interaction-in.html">HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes</a></td>
  <td>HELIOSï¼šå¼€æ”¾åœºæ™¯ä¸­åŸºäºè¯­è¨€äº¤äº’çš„åˆ†å±‚æ¢ç´¢æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22498v1" data-paper-url="./papers/250922498v1-helios-hierarchical-exploration-for-language-grounded-interaction-in.html" onclick="toggleFavorite(this, '2509.22498v1', 'HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250922847v1-empart-interactive-convex-decomposition-for-converting-meshes-to-par.html">Empart: Interactive Convex Decomposition for Converting Meshes to Parts</a></td>
  <td>Empartï¼šäº¤äº’å¼å‡¸åˆ†è§£å·¥å…·ï¼Œå®ç°ç½‘æ ¼æ¨¡å‹çš„åŒºåŸŸå®šåˆ¶ç®€åŒ–ï¼Œæå‡æœºå™¨äººä»¿çœŸæ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22847v1" data-paper-url="./papers/250922847v1-empart-interactive-convex-decomposition-for-converting-meshes-to-par.html" onclick="toggleFavorite(this, '2509.22847v1', 'Empart: Interactive Convex Decomposition for Converting Meshes to Parts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/250922175v1-dhagrasp-synthesizing-affordance-aware-dual-hand-grasps-with-text-in.html">DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions</a></td>
  <td>DHAGraspï¼šæå‡ºæ–‡æœ¬å¼•å¯¼çš„åŒæ‰‹æŠ“å–ç”Ÿæˆæ–¹æ³•ï¼Œå®ç°è¯­ä¹‰æ„ŸçŸ¥çš„æŠ“å–å§¿æ€åˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">affordance</span> <span class="paper-tag">affordance-aware</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22175v1" data-paper-url="./papers/250922175v1-dhagrasp-synthesizing-affordance-aware-dual-hand-grasps-with-text-in.html" onclick="toggleFavorite(this, '2509.22175v1', 'DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250922910v1-good-weights-proactive-adaptive-dead-reckoning-fusion-for-continuous.html">Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM</a></td>
  <td>æå‡ºGood Weightsç®—æ³•ï¼Œèåˆè§†è§‰SLAMä¸é‡Œç¨‹è®¡ï¼Œæå‡å¼±çº¹ç†ç¯å¢ƒä¸‹çš„å®šä½é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22910v1" data-paper-url="./papers/250922910v1-good-weights-proactive-adaptive-dead-reckoning-fusion-for-continuous.html" onclick="toggleFavorite(this, '2509.22910v1', 'Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/250922287v1-leveraging-large-language-models-for-robot-assisted-learning-of-morp.html">Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities</a></td>
  <td>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©æœºå™¨äººä¸ºè¯­è¨€éšœç¢å¹¼å„¿æä¾›å½¢æ€ç»“æ„å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22287v1" data-paper-url="./papers/250922287v1-leveraging-large-language-models-for-robot-assisted-learning-of-morp.html" onclick="toggleFavorite(this, '2509.22287v1', 'Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250922573v1-mint-rvae-multi-cues-intention-prediction-of-human-robot-interaction.html">MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data</a></td>
  <td>MINT-RVAEï¼šåˆ©ç”¨RGBå›¾åƒçš„äººä½“å§¿æ€å’Œæƒ…æ„Ÿä¿¡æ¯è¿›è¡Œäººæœºäº¤äº’æ„å›¾é¢„æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22573v1" data-paper-url="./papers/250922573v1-mint-rvae-multi-cues-intention-prediction-of-human-robot-interaction.html" onclick="toggleFavorite(this, '2509.22573v1', 'MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/250922642v2-wow-towards-a-world-omniscient-world-model-through-embodied-interact.html">WoW: Towards a World omniscient World model Through Embodied Interaction</a></td>
  <td>WoWï¼šé€šè¿‡å…·èº«äº¤äº’æ„å»ºå…·å¤‡ç‰©ç†ç›´è§‰çš„ä¸–ç•Œæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22642v2" data-paper-url="./papers/250922642v2-wow-towards-a-world-omniscient-world-model-through-embodied-interact.html" onclick="toggleFavorite(this, '2509.22642v2', 'WoW: Towards a World omniscient World model Through Embodied Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250921961v1-flowdrive-moderated-flow-matching-with-data-balancing-for-trajectory.html">FlowDrive: moderated flow matching with data balancing for trajectory planning</a></td>
  <td>FlowDriveï¼šç»“åˆæ•°æ®å¹³è¡¡çš„é€‚åº¦æµåŒ¹é…è½¨è¿¹è§„åˆ’ï¼Œæå‡ç½•è§åœºæ™¯æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21961v1" data-paper-url="./papers/250921961v1-flowdrive-moderated-flow-matching-with-data-balancing-for-trajectory.html" onclick="toggleFavorite(this, '2509.21961v1', 'FlowDrive: moderated flow matching with data balancing for trajectory planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/250922883v1-multi-robot-allocation-for-information-gathering-in-non-uniform-spat.html">Multi-Robot Allocation for Information Gathering in Non-Uniform Spatiotemporal Environments</a></td>
  <td>æå‡ºä¸€ç§å¤šæœºå™¨äººä¿¡æ¯æ”¶é›†æ¡†æ¶ï¼Œç”¨äºè§£å†³éå‡åŒ€æ—¶ç©ºç¯å¢ƒä¸­åœºä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22883v1" data-paper-url="./papers/250922883v1-multi-robot-allocation-for-information-gathering-in-non-uniform-spat.html" onclick="toggleFavorite(this, '2509.22883v1', 'Multi-Robot Allocation for Information Gathering in Non-Uniform Spatiotemporal Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/250922434v1-an-ontology-for-unified-modeling-of-tasks-actions-environments-and-c.html">An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics</a></td>
  <td>æå‡ºOntoBOTæœ¬ä½“ï¼Œç»Ÿä¸€å»ºæ¨¡æœåŠ¡æœºå™¨äººä¸­çš„ä»»åŠ¡ã€åŠ¨ä½œã€ç¯å¢ƒå’Œèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22434v1" data-paper-url="./papers/250922434v1-an-ontology-for-unified-modeling-of-tasks-actions-environments-and-c.html" onclick="toggleFavorite(this, '2509.22434v1', 'An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)