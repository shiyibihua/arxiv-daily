---
layout: default
title: RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation
---

# RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22356" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22356v1</a>
  <a href="https://arxiv.org/pdf/2509.22356.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22356v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22356v1', 'RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Enguang Liu, Siyuan Liang, Liming Lu, Xiyu Zeng, Xiaochun Cao, Aishan Liu, Shuchao Pang

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RoboView-Biasï¼šé¦–ä¸ªæœºå™¨äººæ“ä½œä¸­å…·èº«æ™ºèƒ½ä½“è§†è§‰åè§è¯„æµ‹åŸºå‡†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½ä½“` `è§†è§‰åè§` `æœºå™¨äººæ“ä½œ` `è¯„æµ‹åŸºå‡†` `å› å­éš”ç¦»` `æ„ŸçŸ¥å…¬å¹³æ€§` `è¯­ä¹‰ grounding`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…·èº«æ™ºèƒ½ä½“è¯„æµ‹ç¼ºä¹å¯¹è§†è§‰åè§çš„ç³»ç»Ÿæ€§é‡åŒ–ï¼Œé™åˆ¶äº†å¯¹æ„ŸçŸ¥å¦‚ä½•å½±å“å†³ç­–ç¨³å®šæ€§çš„ç†è§£ã€‚
2. RoboView-Bias é‡‡ç”¨å› å­éš”ç¦»åŸåˆ™ï¼Œé€šè¿‡ç»“æ„åŒ–å˜ä½“ç”Ÿæˆå’Œæ„ŸçŸ¥å…¬å¹³æ€§éªŒè¯ï¼Œç³»ç»Ÿé‡åŒ–è§†è§‰åè§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ‰€æœ‰æ™ºèƒ½ä½“éƒ½å­˜åœ¨æ˜¾è‘—è§†è§‰åè§ï¼Œè§†è§’æ˜¯å…³é”®å› ç´ ï¼Œä¸”åè§å­˜åœ¨ä¸å¯¹ç§°è€¦åˆï¼Œè¯­ä¹‰ grounding å±‚å¯æœ‰æ•ˆç¼“è§£åè§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«æ™ºèƒ½ä½“çš„å®‰å…¨æ€§å’Œå¯é æ€§ä¾èµ–äºå‡†ç¡®ä¸”æ— åè§çš„è§†è§‰æ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„æµ‹åŸºå‡†ä¸»è¦å¼ºè°ƒæ³›åŒ–æ€§å’Œåœ¨æ‰°åŠ¨ä¸‹çš„é²æ£’æ€§ï¼Œè€Œå¯¹è§†è§‰åè§çš„ç³»ç»Ÿæ€§é‡åŒ–ä»ç„¶ä¸è¶³ã€‚è¿™ç§å·®è·é™åˆ¶äº†å¯¹æ„ŸçŸ¥å¦‚ä½•å½±å“å†³ç­–ç¨³å®šæ€§çš„æ›´æ·±å±‚ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† RoboView-Biasï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºç³»ç»Ÿæ€§é‡åŒ–æœºå™¨äººæ“ä½œä¸­è§†è§‰åè§çš„è¯„æµ‹åŸºå‡†ï¼Œéµå¾ªäº†å› å­éš”ç¦»åŸåˆ™ã€‚åˆ©ç”¨ç»“æ„åŒ–çš„å˜ä½“ç”Ÿæˆæ¡†æ¶å’Œæ„ŸçŸ¥å…¬å¹³æ€§éªŒè¯åè®®ï¼Œæˆ‘ä»¬åˆ›å»ºäº† 2127 ä¸ªä»»åŠ¡å®ä¾‹ï¼Œèƒ½å¤Ÿå¯é åœ°æµ‹é‡ç”±å•ä¸ªè§†è§‰å› ç´ åŠå…¶ç›¸äº’ä½œç”¨å¼•èµ·çš„åè§ã€‚ä½¿ç”¨è¿™ä¸ªåŸºå‡†ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸¤ç§ä¸»æµèŒƒå¼ä¸­çš„ä¸‰ä¸ªä»£è¡¨æ€§å…·èº«æ™ºèƒ½ä½“ï¼Œå¹¶æŠ¥å‘Šäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šï¼ˆiï¼‰æ‰€æœ‰æ™ºèƒ½ä½“éƒ½è¡¨ç°å‡ºæ˜¾è‘—çš„è§†è§‰åè§ï¼Œå…¶ä¸­ç›¸æœºè§†è§’æ˜¯æœ€å…³é”®çš„å› ç´ ï¼›ï¼ˆiiï¼‰æ™ºèƒ½ä½“åœ¨é«˜åº¦é¥±å’Œçš„é¢œè‰²ä¸Šå®ç°äº†æœ€é«˜çš„æˆåŠŸç‡ï¼Œè¡¨æ˜å®ƒä»¬ç»§æ‰¿äº†åº•å±‚ VLM çš„è§†è§‰åå¥½ï¼›ï¼ˆiiiï¼‰è§†è§‰åè§è¡¨ç°å‡ºå¼ºçƒˆçš„ã€ä¸å¯¹ç§°çš„è€¦åˆï¼Œè§†è§’å¼ºçƒˆåœ°æ”¾å¤§äº†ä¸é¢œè‰²ç›¸å…³çš„åè§ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†åŸºäºè¯­ä¹‰ grounding å±‚çš„ç¼“è§£ç­–ç•¥åœ¨ MOKA ä¸Šå°†è§†è§‰åè§æ˜¾è‘—é™ä½äº†çº¦ 54.5%ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯¹è§†è§‰åè§çš„ç³»ç»Ÿåˆ†ææ˜¯å¼€å‘å®‰å…¨å¯é çš„é€šç”¨å…·èº«æ™ºèƒ½ä½“çš„å…ˆå†³æ¡ä»¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å…·èº«æ™ºèƒ½ä½“åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œå…¶è§†è§‰æ„ŸçŸ¥æ¨¡å—å­˜åœ¨åè§ï¼Œå¯¼è‡´å†³ç­–ä¸ç¨³å®šå’Œå®‰å…¨æ€§é—®é¢˜ã€‚ç°æœ‰çš„è¯„æµ‹åŸºå‡†ä¸»è¦å…³æ³¨æ³›åŒ–æ€§å’Œé²æ£’æ€§ï¼Œç¼ºä¹å¯¹è§†è§‰åè§çš„ç³»ç»Ÿæ€§é‡åŒ–ï¼Œéš¾ä»¥æ·±å…¥ç†è§£æ„ŸçŸ¥å¯¹å†³ç­–çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRoboView-Bias çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å› å­éš”ç¦»çš„æ–¹å¼ï¼Œç³»ç»Ÿæ€§åœ°é‡åŒ–ä¸åŒè§†è§‰å› ç´ ï¼ˆå¦‚è§†è§’ã€é¢œè‰²ç­‰ï¼‰å¯¹å…·èº«æ™ºèƒ½ä½“å†³ç­–çš„å½±å“ã€‚é€šè¿‡æ§åˆ¶å˜é‡ï¼Œåˆ†æå•ä¸ªå› ç´ åŠå…¶ç›¸äº’ä½œç”¨å¦‚ä½•å¼•å…¥åè§ï¼Œä»è€Œä¸ºåç»­çš„åè§ç¼“è§£æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoboView-Bias åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) ç»“æ„åŒ–å˜ä½“ç”Ÿæˆæ¡†æ¶ï¼šç”¨äºç”ŸæˆåŒ…å«ä¸åŒè§†è§‰å› ç´ å˜ä½“çš„ä»»åŠ¡å®ä¾‹ï¼›2) æ„ŸçŸ¥å…¬å¹³æ€§éªŒè¯åè®®ï¼šç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨ä¸åŒè§†è§‰å› ç´ ä¸‹çš„è¡¨ç°ï¼Œå¹¶é‡åŒ–åè§ç¨‹åº¦ï¼›3) åè§ç¼“è§£ç­–ç•¥ï¼šæå‡ºåŸºäºè¯­ä¹‰ grounding å±‚çš„ç¼“è§£ç­–ç•¥ï¼Œå‡å°‘è§†è§‰åè§ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šé¦–å…ˆï¼Œåˆ©ç”¨å˜ä½“ç”Ÿæˆæ¡†æ¶åˆ›å»ºä»»åŠ¡å®ä¾‹ï¼›ç„¶åï¼Œä½¿ç”¨æ„ŸçŸ¥å…¬å¹³æ€§éªŒè¯åè®®è¯„ä¼°æ™ºèƒ½ä½“è¡¨ç°ï¼›æœ€åï¼Œåº”ç”¨åè§ç¼“è§£ç­–ç•¥ï¼Œå¹¶é‡æ–°è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šRoboView-Bias çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) é¦–æ¬¡æå‡ºé’ˆå¯¹æœºå™¨äººæ“ä½œä¸­å…·èº«æ™ºèƒ½ä½“è§†è§‰åè§çš„ç³»ç»Ÿæ€§è¯„æµ‹åŸºå‡†ï¼›2) é‡‡ç”¨å› å­éš”ç¦»åŸåˆ™ï¼Œèƒ½å¤Ÿé‡åŒ–å•ä¸ªè§†è§‰å› ç´ åŠå…¶ç›¸äº’ä½œç”¨å¯¹åè§çš„å½±å“ï¼›3) æå‡ºç»“æ„åŒ–çš„å˜ä½“ç”Ÿæˆæ¡†æ¶å’Œæ„ŸçŸ¥å…¬å¹³æ€§éªŒè¯åè®®ï¼Œå®ç°äº†åè§çš„å¯é æµ‹é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å˜ä½“ç”Ÿæˆæ¡†æ¶ä¸­ï¼Œè®ºæ–‡è®¾è®¡äº†å¤šç§è§†è§‰å› ç´ çš„å˜ä½“ï¼Œå¦‚ç›¸æœºè§†è§’ã€å…‰ç…§æ¡ä»¶ã€ç‰©ä½“é¢œè‰²ç­‰ã€‚åœ¨æ„ŸçŸ¥å…¬å¹³æ€§éªŒè¯åè®®ä¸­ï¼Œè®ºæ–‡å®šä¹‰äº†åè§çš„é‡åŒ–æŒ‡æ ‡ï¼Œå¦‚æˆåŠŸç‡å·®å¼‚ã€å…¬å¹³æ€§æŒ‡æ ‡ç­‰ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†åŸºäºè¯­ä¹‰ grounding å±‚çš„åè§ç¼“è§£ç­–ç•¥ï¼Œé€šè¿‡å°†è§†è§‰ä¿¡æ¯ä¸è¯­ä¹‰ä¿¡æ¯å¯¹é½ï¼Œå‡å°‘è§†è§‰åè§çš„å½±å“ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RoboView-Bias è¯„æµ‹ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰å—æµ‹æ™ºèƒ½ä½“éƒ½å­˜åœ¨æ˜¾è‘—çš„è§†è§‰åè§ï¼Œå…¶ä¸­ç›¸æœºè§†è§’æ˜¯æœ€å…³é”®çš„å› ç´ ã€‚æ™ºèƒ½ä½“åœ¨é«˜åº¦é¥±å’Œçš„é¢œè‰²ä¸Šè¡¨ç°æ›´å¥½ï¼Œè¡¨æ˜å…¶ç»§æ‰¿äº†åº•å±‚ VLM çš„è§†è§‰åå¥½ã€‚æå‡ºçš„åŸºäºè¯­ä¹‰ grounding å±‚çš„ç¼“è§£ç­–ç•¥åœ¨ MOKA ä¸Šå°†è§†è§‰åè§æ˜¾è‘—é™ä½äº†çº¦ 54.5%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å…·èº«æ™ºèƒ½ä½“çš„å®‰å…¨æ€§ã€å¯é æ€§å’Œå…¬å¹³æ€§ã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°è¯„ä¼°å’Œç¼“è§£è§†è§‰åè§ï¼Œå¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œå¹¶å‡å°‘å› è§†è§‰æ„ŸçŸ¥åå·®å¯¼è‡´çš„é”™è¯¯å†³ç­–ã€‚è¯¥ç ”ç©¶å¯¹å¼€å‘é€šç”¨å‹ã€å¯é çš„æœºå™¨äººç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The safety and reliability of embodied agents rely on accurate and unbiased visual perception. However, existing benchmarks mainly emphasize generalization and robustness under perturbations, while systematic quantification of visual bias remains scarce. This gap limits a deeper understanding of how perception influences decision-making stability. To address this issue, we propose RoboView-Bias, the first benchmark specifically designed to systematically quantify visual bias in robotic manipulation, following a principle of factor isolation. Leveraging a structured variant-generation framework and a perceptual-fairness validation protocol, we create 2,127 task instances that enable robust measurement of biases induced by individual visual factors and their interactions. Using this benchmark, we systematically evaluate three representative embodied agents across two prevailing paradigms and report three key findings: (i) all agents exhibit significant visual biases, with camera viewpoint being the most critical factor; (ii) agents achieve their highest success rates on highly saturated colors, indicating inherited visual preferences from underlying VLMs; and (iii) visual biases show strong, asymmetric coupling, with viewpoint strongly amplifying color-related bias. Finally, we demonstrate that a mitigation strategy based on a semantic grounding layer substantially reduces visual bias by approximately 54.5\% on MOKA. Our results highlight that systematic analysis of visual bias is a prerequisite for developing safe and reliable general-purpose embodied agents.

