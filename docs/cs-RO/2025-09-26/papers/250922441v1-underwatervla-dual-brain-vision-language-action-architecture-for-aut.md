---
layout: default
title: UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation
---

# UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22441" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22441v1</a>
  <a href="https://arxiv.org/pdf/2509.22441.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22441v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22441v1', 'UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhangyuan Wang, Yunpeng Zhu, Yuqi Yan, Xiaoyuan Tian, Xinhao Shao, Meixuan Li, Weikun Li, Guangsheng Su, Weicheng Cui, Dixia Fan

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: This paper introduces the first VLA framework for AUVs, featuring a dual-brain architecture and zero-data MPC for real-world underwater navigation

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUnderwaterVLAï¼Œç”¨äºæ°´ä¸‹è‡ªä¸»å¯¼èˆªï¼Œæå‡å¤æ‚æ°´åŸŸä»»åŠ¡å®Œæˆåº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ°´ä¸‹æœºå™¨äºº` `è‡ªä¸»å¯¼èˆª` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `åŒè„‘æ¶æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ°´ä¸‹ç¯å¢ƒå¤æ‚ï¼Œå­˜åœ¨æ°´åŠ¨åŠ›æ‰°åŠ¨ã€é€šä¿¡å¸¦å®½é™åˆ¶å’Œè§†è§‰é€€åŒ–ç­‰é—®é¢˜ï¼Œå¯¼è‡´ç°æœ‰æ°´ä¸‹å¯¼èˆªæ–¹æ³•é²æ£’æ€§ä¸è¶³ã€‚
2. UnderwaterVLAé‡‡ç”¨åŒè„‘æ¶æ„åˆ†ç¦»é«˜ä½å±‚æ§åˆ¶ï¼Œå¹¶å¼•å…¥VLAæ¨¡å‹è¿›è¡Œå¯è§£é‡Šæ¨ç†ï¼Œç»“åˆæµä½“åŠ¨åŠ›å­¦MPCè¿›è¡Œå®æ—¶è¡¥å¿ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUnderwaterVLAåœ¨è§†è§‰é€€åŒ–æ¡ä»¶ä¸‹é™ä½äº†å¯¼èˆªè¯¯å·®ï¼Œä»»åŠ¡å®Œæˆåº¦æ¯”åŸºçº¿æé«˜äº†19%åˆ°27%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUnderwaterVLAçš„æ–°å‹æ°´ä¸‹è‡ªä¸»å¯¼èˆªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å’Œå…·èº«æ™ºèƒ½ç³»ç»Ÿã€‚ç”±äºæ°´åŠ¨åŠ›æ‰°åŠ¨ã€æœ‰é™çš„é€šä¿¡å¸¦å®½ä»¥åŠæµ‘æµŠæ°´åŸŸä¸­é€€åŒ–çš„ä¼ æ„Ÿï¼Œæ°´ä¸‹ä½œä¸šä»ç„¶å¾ˆå›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰é¡¹åˆ›æ–°ã€‚é¦–å…ˆï¼ŒåŒè„‘æ¶æ„å°†é«˜å±‚ä»»åŠ¡æ¨ç†ä¸ä½å±‚ååº”æ§åˆ¶åˆ†ç¦»ï¼Œä»è€Œåœ¨é€šä¿¡å’Œè®¡ç®—çº¦æŸä¸‹å®ç°ç¨³å¥è¿è¡Œã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åº”ç”¨äºæ°´ä¸‹æœºå™¨äººï¼Œç»“åˆç»“æ„åŒ–çš„æ€ç»´é“¾æ¨ç†ä»¥å®ç°å¯è§£é‡Šçš„å†³ç­–ã€‚ç¬¬ä¸‰ï¼Œä¸€ç§åŸºäºæµä½“åŠ¨åŠ›å­¦çš„æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æ–¹æ¡ˆå®æ—¶è¡¥å¿æµä½“æ•ˆåº”ï¼Œè€Œæ— éœ€æ˜‚è´µçš„ç‰¹å®šäºä»»åŠ¡çš„è®­ç»ƒã€‚ç°åœºæµ‹è¯•çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUnderwaterVLAé™ä½äº†é€€åŒ–è§†è§‰æ¡ä»¶ä¸‹çš„å¯¼èˆªè¯¯å·®ï¼ŒåŒæ—¶æ¯”åŸºçº¿æé«˜äº†19%åˆ°27%çš„ä»»åŠ¡å®Œæˆåº¦ã€‚é€šè¿‡æœ€å¤§é™åº¦åœ°å‡å°‘å¯¹æ°´ä¸‹ç‰¹å®šè®­ç»ƒæ•°æ®çš„ä¾èµ–å¹¶æé«˜è·¨ç¯å¢ƒçš„é€‚åº”æ€§ï¼ŒUnderwaterVLAä¸ºä¸‹ä¸€ä»£æ™ºèƒ½AUVæä¾›äº†ä¸€æ¡å¯æ‰©å±•ä¸”ç»æµé«˜æ•ˆçš„è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ°´ä¸‹è‡ªä¸»å¯¼èˆªé¢ä¸´çš„ä¸»è¦é—®é¢˜æ˜¯æ°´åŠ¨åŠ›æ‰°åŠ¨ã€é€šä¿¡å¸¦å®½é™åˆ¶ä»¥åŠæµ‘æµŠæ°´åŸŸä¸­ä¼ æ„Ÿå™¨æ€§èƒ½ä¸‹é™ï¼Œè¿™äº›å› ç´ å¯¼è‡´ä¼ ç»Ÿå¯¼èˆªæ–¹æ³•åœ¨å¤æ‚æ°´ä¸‹ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œéœ€è¦å¤§é‡ç‰¹å®šç¯å¢ƒçš„æ•°æ®è®­ç»ƒï¼Œæ³›åŒ–æ€§å·®ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆçš„ä»»åŠ¡è§„åˆ’å’Œæ§åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUnderwaterVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é«˜å±‚ä»»åŠ¡æ¨ç†å’Œä½å±‚ååº”æ§åˆ¶è§£è€¦ï¼Œé€šè¿‡åŒè„‘æ¶æ„å®ç°ã€‚é«˜å±‚â€œå¤§è„‘â€è´Ÿè´£ä»»åŠ¡è§„åˆ’å’Œå†³ç­–ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹è¿›è¡Œæ¨ç†ï¼›ä½å±‚â€œå¤§è„‘â€è´Ÿè´£å®æ—¶æ§åˆ¶ï¼Œé‡‡ç”¨æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰è¡¥å¿æµä½“åŠ¨åŠ›å­¦å½±å“ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜ç³»ç»Ÿçš„é²æ£’æ€§ã€å¯è§£é‡Šæ€§å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUnderwaterVLAçš„æ•´ä½“æ¶æ„åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŒè„‘æ¶æ„ï¼šåŒ…æ‹¬é«˜å±‚ä»»åŠ¡æ¨ç†è„‘å’Œä½å±‚ååº”æ§åˆ¶è„‘ã€‚2) è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼šç”¨äºé«˜å±‚ä»»åŠ¡è§„åˆ’å’Œå†³ç­–ï¼Œé€šè¿‡é“¾å¼æ€ç»´è¿›è¡Œæ¨ç†ã€‚3) åŸºäºæµä½“åŠ¨åŠ›å­¦çš„æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ï¼šç”¨äºä½å±‚å®æ—¶æ§åˆ¶ï¼Œè¡¥å¿æ°´åŠ¨åŠ›å½±å“ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼šé¦–å…ˆï¼ŒVLAæ¨¡å‹æ ¹æ®è§†è§‰è¾“å…¥å’Œä»»åŠ¡æŒ‡ä»¤ç”Ÿæˆè¡ŒåŠ¨åºåˆ—ï¼›ç„¶åï¼ŒMPCæ ¹æ®å½“å‰çŠ¶æ€å’Œè¡ŒåŠ¨åºåˆ—ç”Ÿæˆæ§åˆ¶æŒ‡ä»¤ï¼Œé©±åŠ¨AUVæ‰§è¡Œä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šUnderwaterVLAçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) åŒè„‘æ¶æ„ï¼Œæœ‰æ•ˆåˆ†ç¦»äº†é«˜å±‚æ¨ç†å’Œä½å±‚æ§åˆ¶ï¼Œæé«˜äº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚2) é¦–æ¬¡å°†VLAæ¨¡å‹åº”ç”¨äºæ°´ä¸‹æœºå™¨äººï¼Œå®ç°äº†å¯è§£é‡Šçš„å†³ç­–è¿‡ç¨‹ã€‚3) æå‡ºäº†åŸºäºæµä½“åŠ¨åŠ›å­¦çš„MPCæ–¹æ¡ˆï¼Œæ— éœ€å¤§é‡ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ•°æ®å³å¯å®ç°å®æ—¶è¡¥å¿ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒUnderwaterVLAæ›´å…·é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šVLAæ¨¡å‹é‡‡ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹æ°´ä¸‹ç¯å¢ƒè¿›è¡Œäº†å¾®è°ƒã€‚MPCæ–¹æ¡ˆçš„å…³é”®åœ¨äºå‡†ç¡®å»ºæ¨¡æ°´åŠ¨åŠ›å½±å“ï¼Œå¹¶è®¾è®¡åˆé€‚çš„æˆæœ¬å‡½æ•°ï¼Œä»¥å®ç°ç²¾ç¡®çš„è½¨è¿¹è·Ÿè¸ªå’Œå§¿æ€æ§åˆ¶ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒUnderwaterVLAåœ¨é€€åŒ–è§†è§‰æ¡ä»¶ä¸‹æ˜¾è‘—é™ä½äº†å¯¼èˆªè¯¯å·®ï¼Œå¹¶ä¸”ä»»åŠ¡å®Œæˆåº¦æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†19%åˆ°27%ã€‚è¿™äº›ç»“æœéªŒè¯äº†UnderwaterVLAåœ¨å¤æ‚æ°´ä¸‹ç¯å¢ƒä¸­å…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚è¯¥æ¡†æ¶æœ€å¤§é™åº¦åœ°å‡å°‘äº†å¯¹æ°´ä¸‹ç‰¹å®šè®­ç»ƒæ•°æ®çš„ä¾èµ–ï¼Œå¹¶æé«˜äº†è·¨ç¯å¢ƒçš„é€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UnderwaterVLAå¯åº”ç”¨äºæ°´ä¸‹ç¯å¢ƒç›‘æµ‹ã€æ°´ä¸‹åŸºç¡€è®¾æ–½ç»´æŠ¤ã€æ°´ä¸‹æœæ•‘ã€æµ·æ´‹èµ„æºå‹˜æ¢ç­‰é¢†åŸŸã€‚è¯¥ç ”ç©¶é™ä½äº†å¯¹æ°´ä¸‹ç‰¹å®šè®­ç»ƒæ•°æ®çš„ä¾èµ–ï¼Œæé«˜äº†AUVåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§ï¼Œä¸ºä¸‹ä¸€ä»£æ™ºèƒ½æ°´ä¸‹æœºå™¨äººæä¾›äº†å¯æ‰©å±•ä¸”ç»æµé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„æœªæ¥å‘å±•å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents UnderwaterVLA, a novel framework for autonomous underwater navigation that integrates multimodal foundation models with embodied intelligence systems. Underwater operations remain difficult due to hydrodynamic disturbances, limited communication bandwidth, and degraded sensing in turbid waters. To address these challenges, we introduce three innovations. First, a dual-brain architecture decouples high-level mission reasoning from low-level reactive control, enabling robust operation under communication and computational constraints. Second, we apply Vision-Language-Action(VLA) models to underwater robotics for the first time, incorporating structured chain-of-thought reasoning for interpretable decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC) scheme compensates for fluid effects in real time without costly task-specific training. Experimental results in field tests show that UnderwaterVLA reduces navigation errors in degraded visual conditions while maintaining higher task completion by 19% to 27% over baseline. By minimizing reliance on underwater-specific training data and improving adaptability across environments, UnderwaterVLA provides a scalable and cost-effective path toward the next generation of intelligent AUVs.

