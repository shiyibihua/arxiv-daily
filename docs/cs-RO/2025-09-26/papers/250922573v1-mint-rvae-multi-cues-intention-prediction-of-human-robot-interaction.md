---
layout: default
title: MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data
---

# MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22573" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22573v1</a>
  <a href="https://arxiv.org/pdf/2509.22573.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22573v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22573v1', 'MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Farida Mohsen, Ali Safa

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MINT-RVAEï¼šåˆ©ç”¨RGBå›¾åƒçš„äººä½“å§¿æ€å’Œæƒ…æ„Ÿä¿¡æ¯è¿›è¡Œäººæœºäº¤äº’æ„å›¾é¢„æµ‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `æ„å›¾é¢„æµ‹` `RGBå›¾åƒ` `ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ` `æ•°æ®å¢å¼º` `å˜åˆ†è‡ªç¼–ç å™¨` `å§¿æ€ä¼°è®¡` `æƒ…æ„Ÿè¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„äººæœºäº¤äº’æ„å›¾é¢„æµ‹æ–¹æ³•ä¾èµ–RGB-Dç­‰å¤šæ¨¡æ€è¾“å…¥ï¼Œæˆæœ¬è¾ƒé«˜ä¸”é™åˆ¶äº†åº”ç”¨åœºæ™¯ã€‚
2. è®ºæ–‡æå‡ºMINT-RVAEæ¨¡å‹ï¼Œä»…ä½¿ç”¨RGBå›¾åƒï¼Œé€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œè§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œæå‡é¢„æµ‹ç²¾åº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äººæœºäº¤äº’æ„å›¾é¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼ŒAUROCè¾¾åˆ°0.95ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†å®ç°æœ‰æ•ˆçš„äººæœºäº¤äº’ä¸åä½œï¼Œé«˜æ•ˆåœ°æ£€æµ‹äººç±»ä¸æœºå™¨äººäº¤äº’çš„æ„å›¾è‡³å…³é‡è¦ã€‚è¿‡å»åå¹´ï¼Œæ·±åº¦å­¦ä¹ åœ¨è¯¥é¢†åŸŸå¤‡å—å…³æ³¨ï¼Œä½†ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºå¤šæ¨¡æ€è¾“å…¥ï¼Œä¾‹å¦‚RGBç»“åˆæ·±åº¦ä¿¡æ¯(RGB-D)ï¼Œå°†æ„Ÿè§‰æ•°æ®çš„æ—¶é—´åºåˆ—çª—å£åˆ†ç±»ä¸ºäº¤äº’æˆ–éäº¤äº’ã€‚ä¸æ­¤ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»…ä½¿ç”¨RGBå›¾åƒçš„æµç¨‹ï¼Œç”¨äºä»¥å¸§çº§ç²¾åº¦é¢„æµ‹äººç±»äº¤äº’æ„å›¾ï¼Œä»è€ŒåŠ å¿«æœºå™¨äººå“åº”é€Ÿåº¦å¹¶æé«˜æœåŠ¡è´¨é‡ã€‚æ„å›¾é¢„æµ‹çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯çœŸå®ä¸–ç•Œäººæœºäº¤äº’æ•°æ®é›†ä¸­å›ºæœ‰çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œè¿™ä¼šé˜»ç¢æ¨¡å‹çš„è®­ç»ƒå’Œæ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MINT-RVAEï¼Œä¸€ç§åˆæˆåºåˆ—ç”Ÿæˆæ–¹æ³•ï¼Œä»¥åŠæ–°çš„æŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºå¯¹æ ·æœ¬å¤–æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆAUROCï¼š0.95ï¼‰ï¼Œä¼˜äºå…ˆå‰çš„å·¥ä½œï¼ˆAUROCï¼š0.90-0.912ï¼‰ï¼ŒåŒæ—¶ä»…éœ€è¦RGBè¾“å…¥å¹¶æ”¯æŒç²¾ç¡®çš„å¸§èµ·å§‹é¢„æµ‹ã€‚æœ€åï¼Œä¸ºäº†æ”¯æŒæœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æˆ‘ä»¬æ–°çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äººç±»äº¤äº’æ„å›¾çš„å¸§çº§æ ‡æ³¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººæœºäº¤äº’æ„å›¾é¢„æµ‹æ–¹æ³•é€šå¸¸ä¾èµ–äºRGB-Dæ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä»…æœ‰RGBç›¸æœºçš„ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼ŒçœŸå®ä¸–ç•Œçš„äººæœºäº¤äº’æ•°æ®é›†é€šå¸¸å­˜åœ¨ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå³äº¤äº’è¡Œä¸ºçš„æ ·æœ¬è¿œå°‘äºéäº¤äº’è¡Œä¸ºçš„æ ·æœ¬ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹è®­ç»ƒåå·®ï¼Œæ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨RGBå›¾åƒä¸­çš„äººä½“å§¿æ€å’Œæƒ…æ„Ÿä¿¡æ¯æ¥é¢„æµ‹äººæœºäº¤äº’æ„å›¾ï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¥åˆæˆäº¤äº’è¡Œä¸ºçš„æ ·æœ¬ï¼Œä»è€Œç¼“è§£æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°å­¦ä¹ äº¤äº’è¡Œä¸ºçš„ç‰¹å¾ï¼Œæé«˜é¢„æµ‹ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) äººä½“å§¿æ€ä¼°è®¡æ¨¡å—ï¼Œç”¨äºä»RGBå›¾åƒä¸­æå–äººä½“å…³é”®ç‚¹ï¼›2) æƒ…æ„Ÿè¯†åˆ«æ¨¡å—ï¼Œç”¨äºè¯†åˆ«äººçš„æƒ…æ„ŸçŠ¶æ€ï¼›3) MINT-RVAEæ¨¡å—ï¼Œç”¨äºç”Ÿæˆåˆæˆçš„äº¤äº’è¡Œä¸ºåºåˆ—ï¼›4) æ„å›¾é¢„æµ‹æ¨¡å—ï¼Œç”¨äºé¢„æµ‹äººæœºäº¤äº’æ„å›¾ã€‚è¯¥æ¡†æ¶é¦–å…ˆä»RGBå›¾åƒä¸­æå–äººä½“å§¿æ€å’Œæƒ…æ„Ÿä¿¡æ¯ï¼Œç„¶ååˆ©ç”¨MINT-RVAEç”Ÿæˆæ›´å¤šçš„äº¤äº’è¡Œä¸ºæ ·æœ¬ï¼Œæœ€åå°†è¿™äº›æ ·æœ¬ç”¨äºè®­ç»ƒæ„å›¾é¢„æµ‹æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†MINT-RVAEæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜¯ä¸€ç§åŸºäºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„äº¤äº’è¡Œä¸ºåºåˆ—ã€‚ä¸ä¼ ç»Ÿçš„GANç›¸æ¯”ï¼ŒMINT-RVAEå¯ä»¥æ›´å¥½åœ°æ§åˆ¶ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨RGBå›¾åƒä½œä¸ºè¾“å…¥ï¼Œé™ä½äº†å¯¹ç¡¬ä»¶çš„è¦æ±‚ï¼Œä½¿å…¶æ›´æ˜“äºéƒ¨ç½²ã€‚

**å…³é”®è®¾è®¡**ï¼šMINT-RVAEæ¨¡å‹é‡‡ç”¨VAEçš„ç»“æ„ï¼ŒåŒ…å«ç¼–ç å™¨å’Œè§£ç å™¨ã€‚ç¼–ç å™¨å°†è¾“å…¥åºåˆ—æ˜ å°„åˆ°éšç©ºé—´ï¼Œè§£ç å™¨ä»éšç©ºé—´é‡æ„è¾“å…¥åºåˆ—ã€‚ä¸ºäº†æé«˜ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ï¼Œè®ºæ–‡å¼•å…¥äº†å¯¹æŠ—è®­ç»ƒæœºåˆ¶ï¼Œå³è®­ç»ƒä¸€ä¸ªåˆ¤åˆ«å™¨æ¥åŒºåˆ†çœŸå®æ ·æœ¬å’Œç”Ÿæˆæ ·æœ¬ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†æ–°çš„æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬é‡æ„æŸå¤±ã€KLæ•£åº¦æŸå¤±å’Œå¯¹æŠ—æŸå¤±ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡é‡‡ç”¨äº†å¤šç§è®­ç»ƒç­–ç•¥ï¼Œä¾‹å¦‚æ¢¯åº¦è£å‰ªå’Œå­¦ä¹ ç‡è¡°å‡ï¼Œä»¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡æå‡ºçš„MINT-RVAEæ¨¡å‹åœ¨äººæœºäº¤äº’æ„å›¾é¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼ŒAUROCè¾¾åˆ°0.95ï¼Œä¼˜äºå…ˆå‰çš„å·¥ä½œï¼ˆAUROCï¼š0.90-0.912ï¼‰ã€‚è¯¥æ–¹æ³•ä»…ä½¿ç”¨RGBå›¾åƒä½œä¸ºè¾“å…¥ï¼Œé™ä½äº†å¯¹ç¡¬ä»¶çš„è¦æ±‚ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å…¬å¼€äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äººç±»äº¤äº’æ„å›¾çš„å¸§çº§æ ‡æ³¨ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¾¿åˆ©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§äººæœºäº¤äº’åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¶å±…ã€æœåŠ¡æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡å‡†ç¡®é¢„æµ‹äººç±»çš„äº¤äº’æ„å›¾ï¼Œæœºå™¨äººå¯ä»¥æ›´æ™ºèƒ½åœ°å“åº”äººç±»çš„éœ€æ±‚ï¼Œæä¾›æ›´ä¼˜è´¨çš„æœåŠ¡ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›äººæœºåä½œçš„å‘å±•ï¼Œä½¿äººç±»å’Œæœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°ååŒå·¥ä½œï¼Œå…±åŒå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficiently detecting human intent to interact with ubiquitous robots is crucial for effective human-robot interaction (HRI) and collaboration. Over the past decade, deep learning has gained traction in this field, with most existing approaches relying on multimodal inputs, such as RGB combined with depth (RGB-D), to classify time-sequence windows of sensory data as interactive or non-interactive. In contrast, we propose a novel RGB-only pipeline for predicting human interaction intent with frame-level precision, enabling faster robot responses and improved service quality. A key challenge in intent prediction is the class imbalance inherent in real-world HRI datasets, which can hinder the model's training and generalization. To address this, we introduce MINT-RVAE, a synthetic sequence generation method, along with new loss functions and training strategies that enhance generalization on out-of-sample data. Our approach achieves state-of-the-art performance (AUROC: 0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB input and supporting precise frame onset prediction. Finally, to support future research, we openly release our new dataset with frame-level labeling of human interaction intent.

