---
layout: default
title: Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation
---

# Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22093" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.22093v1</a>
  <a href="https://arxiv.org/pdf/2509.22093.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22093v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22093v1', 'Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xiaohuan Pei, Yuxing Chen, Siyu Xu, Yunke Wang, Yuheng Shi, Chang Xu

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-26

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âä®‰ΩúÊÑüÁü•Âä®ÊÄÅÂâ™ÊûùADPÔºåÊèêÂçáËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Âä®ÊÄÅÂâ™Êûù` `Âä®‰ΩúÊÑüÁü•` `Ê®°ÂûãÂéãÁº©` `Êé®ÁêÜÂä†ÈÄü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºå‰∏ªË¶ÅÁî±‰∫éÂØÜÈõÜËßÜËßâtokenÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰∏îÂøΩÁï•‰∫Ü‰∏çÂêåÊìç‰ΩúÈò∂ÊÆµÁöÑÂÜó‰ΩôÂ∑ÆÂºÇ„ÄÇ
2. ADPÊ°ÜÊû∂ÈÄöËøáÂä®‰ΩúÊÑüÁü•ÁöÑËΩ®ËøπÈó®ÊéßÊú∫Âà∂ÔºåÂä®ÊÄÅË∞ÉÊï¥ËßÜËßâtokenÁöÑ‰øùÁïôÁéáÔºå‰ªéËÄåÂú®ËÆ°ÁÆóÊïàÁéáÂíåÊÑüÁü•Á≤æÂ∫¶‰πãÈó¥ÂèñÂæóÂπ≥Ë°°„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåADPÂú®Èôç‰ΩéFLOPsÂíåÊé®ÁêÜÂª∂ËøüÁöÑÂêåÊó∂Ôºå‰øùÊåÅ‰∫ÜÁîöËá≥ÊèêÂçá‰∫ÜÊìç‰ΩúÊàêÂäüÁéáÔºå‰∏∫È´òÊïàÊú∫Âô®‰∫∫Á≠ñÁï•Êèê‰æõ‰∫ÜÁÆÄÂçïÊúâÊïàÁöÑÈÄîÂæÑ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Âä®‰ΩúÊÑüÁü•Âä®ÊÄÅÂâ™Êûù(ADP)ÁöÑÂ§öÊ®°ÊÄÅÂâ™ÊûùÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÊé®ÁêÜÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïËßÇÂØüÂà∞ËßÜËßâtokenÁöÑÂÜó‰ΩôÂ∫¶Âú®Á≤óÁï•Êìç‰ΩúÈò∂ÊÆµÈ´ò‰∫éÁ≤æÁªÜÊìç‰ΩúÈò∂ÊÆµÔºåÂπ∂‰∏î‰∏éÂä®‰ΩúÂä®ÊÄÅÂØÜÂàáÁõ∏ÂÖ≥„ÄÇADPÈõÜÊàê‰∫ÜÊñáÊú¨È©±Âä®ÁöÑtokenÈÄâÊã©ÂíåÂä®‰ΩúÊÑüÁü•ÁöÑËΩ®ËøπÈó®ÊéßÊú∫Âà∂ÔºåÂà©Áî®ËøáÂéªÁöÑËøêÂä®Á™óÂè£Êù•Ë∞ÉÊï¥token‰øùÁïôÁéáÔºå‰ªéËÄåÂú®‰∏çÂêåÊìç‰ΩúÈò∂ÊÆµÂπ≥Ë°°ËÆ°ÁÆóÊïàÁéáÂíåÊÑüÁü•Á≤æÂ∫¶„ÄÇÂú®LIBEROÂ•ó‰ª∂ÂíåÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑÂÆûÈ™åË°®ÊòéÔºåADPÊòæËëóÈôç‰Ωé‰∫ÜFLOPsÂíåÂä®‰ΩúÊé®ÁêÜÂª∂ËøüÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÊàêÂäüÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÔºåÈúÄË¶ÅÂ§ÑÁêÜÈïøÊó∂Á®ãÁöÑÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂÖ∂‰∏≠ÂØπÂØÜÈõÜËßÜËßâtokenÁöÑÊ≥®ÊÑèÂäõËÆ°ÁÆóÂç†ÊçÆ‰∫Ü‰∏ªË¶ÅÁöÑËÆ°ÁÆóÊàêÊú¨„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®‰∫éÂáèÂ∞ëVLAÊ®°ÂûãÂÜÖÈÉ®ÁöÑËßÜËßâÂÜó‰ΩôÔºå‰ΩÜÂøΩÁï•‰∫ÜÊú∫Âô®‰∫∫Êìç‰Ωú‰∏çÂêåÈò∂ÊÆµËßÜËßâÂÜó‰ΩôÂ∫¶ÁöÑÂ∑ÆÂºÇÔºå‰æãÂ¶ÇÁ≤óÁï•Êìç‰ΩúÈò∂ÊÆµÁöÑÂÜó‰ΩôÂ∫¶È´ò‰∫éÁ≤æÁªÜÊìç‰ΩúÈò∂ÊÆµ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Êú∫Âô®‰∫∫Êìç‰ΩúÁöÑÂä®‰ΩúÂä®ÊÄÅ‰ø°ÊÅØÊù•ÊåáÂØºËßÜËßâtokenÁöÑÂâ™ÊûùËøáÁ®ã„ÄÇ‰ΩúËÄÖËßÇÂØüÂà∞ËßÜËßâtokenÁöÑÂÜó‰ΩôÂ∫¶‰∏éÂä®‰ΩúÂä®ÊÄÅ‰πãÈó¥Â≠òÂú®Âº∫Áõ∏ÂÖ≥ÊÄßÔºåÂõ†Ê≠§ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂä®‰ΩúÊÑüÁü•ÁöÑÂä®ÊÄÅÂâ™Êûù(ADP)Ê°ÜÊû∂„ÄÇÈÄöËøáÂàÜÊûêËøáÂéªÁöÑÂä®‰ΩúËΩ®ËøπÔºåADPÂèØ‰ª•Ëá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥ËßÜËßâtokenÁöÑ‰øùÁïôÁéáÔºå‰ªéËÄåÂú®‰øùËØÅÊÑüÁü•Á≤æÂ∫¶ÁöÑÂâçÊèê‰∏ãÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöADPÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºöÊñáÊú¨È©±Âä®ÁöÑtokenÈÄâÊã©Ê®°ÂùóÂíåÂä®‰ΩúÊÑüÁü•ÁöÑËΩ®ËøπÈó®ÊéßÊ®°Âùó„ÄÇÊñáÊú¨È©±Âä®ÁöÑtokenÈÄâÊã©Ê®°ÂùóË¥üË¥£Ê†πÊçÆÊñáÊú¨Êåá‰ª§ÈÄâÊã©‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËßÜËßâtoken„ÄÇÂä®‰ΩúÊÑüÁü•ÁöÑËΩ®ËøπÈó®ÊéßÊ®°ÂùóÂàôÂà©Áî®ËøáÂéªÁöÑÂä®‰ΩúËΩ®Ëøπ‰ø°ÊÅØÔºåÂä®ÊÄÅË∞ÉÊï¥tokenÁöÑ‰øùÁïôÁéá„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåËØ•Ê®°Âùó‰ΩøÁî®‰∏Ä‰∏™Èó®ÊéßÊú∫Âà∂ÔºåÊ†πÊçÆËøáÂéªÁöÑËøêÂä®Á™óÂè£Êù•È¢ÑÊµãÂΩìÂâçÈò∂ÊÆµÁöÑËßÜËßâÂÜó‰ΩôÂ∫¶ÔºåÂπ∂ÊçÆÊ≠§Ë∞ÉÊï¥tokenÁöÑ‰øùÁïôÊØî‰æã„ÄÇÊï¥‰∏™Ê°ÜÊû∂ÂèØ‰ª•‰Ωú‰∏∫‰∏Ä‰∏™Êèí‰ª∂ÈõÜÊàêÂà∞Áé∞ÊúâÁöÑVLAÊ®°Âûã‰∏≠„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöADPÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂä®‰ΩúÂä®ÊÄÅ‰ø°ÊÅØËûçÂÖ•Âà∞ËßÜËßâtokenÁöÑÂâ™ÊûùËøáÁ®ã‰∏≠„ÄÇ‰∏é‰ª•ÂæÄÈùôÊÄÅÊàñÂü∫‰∫éÊñáÊú¨ÁöÑÂâ™ÊûùÊñπÊ≥ï‰∏çÂêåÔºåADPËÉΩÂ§üÊ†πÊçÆÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÂÆûÈôÖÁä∂ÊÄÅÂä®ÊÄÅË∞ÉÊï¥Ââ™ÊûùÁ≠ñÁï•Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞Âπ≥Ë°°ËÆ°ÁÆóÊïàÁéáÂíåÊÑüÁü•Á≤æÂ∫¶„ÄÇËøôÁßçÂä®‰ΩúÊÑüÁü•ÁöÑÂâ™ÊûùÊñπÊ≥ïËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÂéªÈô§ÂÜó‰ΩôÁöÑËßÜËßâ‰ø°ÊÅØÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöADPÊ°ÜÊû∂‰∏≠ÁöÑÂä®‰ΩúÊÑüÁü•ËΩ®ËøπÈó®ÊéßÊ®°ÂùóÊòØÂÖ∂ÂÖ≥ÈîÆËÆæËÆ°‰πã‰∏Ä„ÄÇËØ•Ê®°Âùó‰ΩøÁî®‰∏Ä‰∏™Âæ™ÁéØÁ•ûÁªèÁΩëÁªú(RNN)ÊàñTransformerÊù•Â§ÑÁêÜËøáÂéªÁöÑÂä®‰ΩúËΩ®ËøπÔºåÂπ∂ËæìÂá∫‰∏Ä‰∏™Èó®Êéß‰ø°Âè∑ÔºåÁî®‰∫éÊéßÂà∂ËßÜËßâtokenÁöÑ‰øùÁïôÁéá„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüËá≥ÂÖ≥ÈáçË¶ÅÔºåÈÄöÂ∏∏ÂåÖÊã¨‰∏Ä‰∏™‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÊçüÂ§±ÂáΩÊï∞Ôºà‰æãÂ¶ÇÂä®‰ΩúÈ¢ÑÊµãÊçüÂ§±ÔºâÂíå‰∏Ä‰∏™Ê≠£ÂàôÂåñÈ°πÔºåÁî®‰∫éÁ∫¶ÊùüÂâ™ÊûùÁöÑÂº∫Â∫¶„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑVLAÊ®°ÂûãÂíå‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåADPÊñπÊ≥ïÂú®LIBEROÂ•ó‰ª∂ÂíåÁúüÂÆûÂú∫ÊôØ‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®OpenVLA-OFTÊ®°Âûã‰∏äÔºåADPÂÆûÁé∞‰∫Ü1.35ÂÄçÁöÑÊé®ÁêÜÈÄüÂ∫¶ÊèêÂçá„ÄÇÂêåÊó∂ÔºåÂú®OpenVLAÊ®°Âûã‰∏äÔºåADPËøòÂ∏¶Êù•‰∫Ü25.8%ÁöÑÊìç‰ΩúÊàêÂäüÁéáÊèêÂçá„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåADPËÉΩÂ§üÂú®Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÁöÑÂêåÊó∂Ôºå‰øùÊåÅÁîöËá≥ÊèêÂçáÊú∫Âô®‰∫∫ÁöÑÊìç‰ΩúÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈ´òÊïàÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂåªÁñóÊú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÈôç‰ΩéVLAÊ®°ÂûãÁöÑËÆ°ÁÆóÊàêÊú¨ÔºåÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠Êõ¥Âø´ÈÄü„ÄÅÊõ¥ÂèØÈù†Âú∞ÊâßË°å‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•‰øÉËøõÊõ¥Â§çÊùÇ„ÄÅÊõ¥Êô∫ËÉΩÁöÑÊú∫Âô®‰∫∫Á≠ñÁï•ÁöÑÂºÄÂèëÔºå‰ªéËÄåÊãìÂ±ïÊú∫Âô®‰∫∫ÁöÑÂ∫îÁî®ËåÉÂõ¥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Robotic manipulation with Vision-Language-Action models requires efficient inference over long-horizon multi-modal context, where attention to dense visual tokens dominates computational cost. Existing methods optimize inference speed by reducing visual redundancy within VLA models, but they overlook the varying redundancy across robotic manipulation stages. We observe that the visual token redundancy is higher in coarse manipulation phase than in fine-grained operations, and is strongly correlated with the action dynamic. Motivated by this observation, we propose \textbf{A}ction-aware \textbf{D}ynamic \textbf{P}runing (\textbf{ADP}), a multi-modal pruning framework that integrates text-driven token selection with action-aware trajectory gating. Our method introduces a gating mechanism that conditions the pruning signal on recent action trajectories, using past motion windows to adaptively adjust token retention ratios in accordance with dynamics, thereby balancing computational efficiency and perceptual precision across different manipulation stages. Extensive experiments on the LIBERO suites and diverse real-world scenarios demonstrate that our method significantly reduces FLOPs and action inference latency (\textit{e.g.} $1.35 \times$ speed up on OpenVLA-OFT) while maintaining competitive success rates (\textit{e.g.} 25.8\% improvements with OpenVLA) compared to baselines, thereby providing a simple plug-in path to efficient robot policies that advances the efficiency and performance frontier of robotic manipulation. Our project website is: \href{https://vla-adp.github.io/}{ADP.com}.

