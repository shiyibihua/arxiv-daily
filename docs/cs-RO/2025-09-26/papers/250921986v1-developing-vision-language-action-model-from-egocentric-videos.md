---
layout: default
title: Developing Vision-Language-Action Model from Egocentric Videos
---

# Developing Vision-Language-Action Model from Egocentric Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21986" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21986v1</a>
  <a href="https://arxiv.org/pdf/2509.21986.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21986v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21986v1', 'Developing Vision-Language-Action Model from Egocentric Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç¬¬ä¸€è§†è§’è§†é¢‘çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `ç¬¬ä¸€è§†è§’è§†é¢‘` `æœºå™¨äººæ“ä½œ` `é¢„è®­ç»ƒ` `ç‰©ä½“æ“çºµ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹è®­ç»ƒä¾èµ–æ˜‚è´µçš„æ‰‹åŠ¨é¥æ“ä½œæˆ–éœ€è¦è¯¦ç»†çš„æ‰‹éƒ¨å§¿æ€è®°å½•ç­‰è¾…åŠ©æ ‡æ³¨ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚
2. åˆ©ç”¨EgoScaleræ¡†æ¶ä»åŸå§‹ç¬¬ä¸€è§†è§’è§†é¢‘ä¸­æå–ç‰©ä½“æ“çºµè½¨è¿¹ï¼Œæ„å»ºå¤§è§„æ¨¡VLAé¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ— éœ€é¢å¤–æ ‡æ³¨ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ•°æ®é›†é¢„è®­ç»ƒçš„VLAæ¨¡å‹åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­å‡æ˜¾è‘—æå‡äº†ä»»åŠ¡æˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨ç¬¬ä¸€è§†è§’è§†é¢‘å¼€å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³VLAè®­ç»ƒä¸­å¯¹æ˜‚è´µä¸”ä¸“å®¶é©±åŠ¨çš„æ‰‹åŠ¨é¥æ“ä½œçš„ä¾èµ–é—®é¢˜ã€‚ä¸æ­¤ä¸åŒï¼Œç¬¬ä¸€è§†è§’è§†é¢‘æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ•æ‰äººç±»æ“çºµç‰©ä½“å’Œå·¥å…·çš„æ–¹å¼ï¼Œä»è€Œæä¾›ä¸°å¯Œçš„è¿åŠ¨çº¿ç´¢ã€‚è¯¥ç ”ç©¶åˆ©ç”¨EgoScaleræ¡†æ¶ä»ç¬¬ä¸€è§†è§’è§†é¢‘ä¸­æå–6DoFç‰©ä½“æ“çºµè½¨è¿¹ï¼Œæ— éœ€è¾…åŠ©è®°å½•ã€‚é€šè¿‡å°†EgoScaleråº”ç”¨äºå››ä¸ªå¤§è§„æ¨¡ç¬¬ä¸€è§†è§’è§†é¢‘æ•°æ®é›†ï¼Œå¹¶è‡ªåŠ¨ä¼˜åŒ–å™ªå£°æˆ–ä¸å®Œæ•´çš„è½¨è¿¹ï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡VLAé¢„è®­ç»ƒæ•°æ®é›†ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­ä½¿ç”¨æœ€å…ˆè¿›çš„$Ï€_0$æ¶æ„è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼šï¼ˆiï¼‰ä¸ä»å¤´å¼€å§‹è®­ç»ƒç›¸æ¯”ï¼Œåœ¨æœ¬æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒå¯å°†ä»»åŠ¡æˆåŠŸç‡æé«˜20ï¼…ä»¥ä¸Šï¼›ï¼ˆiiï¼‰æ€§èƒ½ä¸ä½¿ç”¨çœŸå®æœºå™¨äººæ•°æ®é›†æ‰€è·å¾—çš„æ€§èƒ½ç›¸å½“ï¼›ï¼ˆiiiï¼‰å°†æœ¬æ–‡æ•°æ®é›†ä¸çœŸå®æœºå™¨äººæ•°æ®ç›¸ç»“åˆå¯è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç¬¬ä¸€è§†è§’è§†é¢‘æ˜¯æ¨è¿›VLAç ”ç©¶çš„ä¸€ç§æœ‰å‰æ™¯ä¸”å¯æ‰©å±•çš„èµ„æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰è®­ç»ƒä¸­æ•°æ®è·å–æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨é¥æ“ä½œæˆ–éœ€è¦é¢å¤–çš„æ‰‹éƒ¨å§¿æ€æ ‡æ³¨ï¼Œè¿™é™åˆ¶äº†VLAæ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•ä»…åˆ©ç”¨åŸå§‹çš„ç¬¬ä¸€è§†è§’è§†é¢‘æ¥è®­ç»ƒVLAæ¨¡å‹æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¬¬ä¸€è§†è§’è§†é¢‘ä¸­è•´å«çš„ä¸°å¯Œç‰©ä½“æ“çºµä¿¡æ¯ï¼Œé€šè¿‡EgoScaleræ¡†æ¶è‡ªåŠ¨æå–ç‰©ä½“æ“çºµè½¨è¿¹ï¼Œä»è€Œæ„å»ºå¤§è§„æ¨¡çš„VLAé¢„è®­ç»ƒæ•°æ®é›†ã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ‰‹åŠ¨æ ‡æ³¨æˆ–é¢å¤–ä¼ æ„Ÿå™¨æ•°æ®çš„éœ€æ±‚ï¼Œé™ä½äº†æ•°æ®è·å–æˆæœ¬ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨EgoScalerä»ç¬¬ä¸€è§†è§’è§†é¢‘ä¸­æå–6DoFç‰©ä½“æ“çºµè½¨è¿¹ï¼›2) å¯¹æå–çš„è½¨è¿¹è¿›è¡Œè‡ªåŠ¨ä¼˜åŒ–ï¼Œå»é™¤å™ªå£°å’Œè¡¥å…¨ç¼ºå¤±æ•°æ®ï¼›3) åˆ©ç”¨ä¼˜åŒ–åçš„è½¨è¿¹æ„å»ºVLAé¢„è®­ç»ƒæ•°æ®é›†ï¼›4) ä½¿ç”¨è¯¥æ•°æ®é›†é¢„è®­ç»ƒVLAæ¨¡å‹ï¼ˆæœ¬æ–‡ä½¿ç”¨$Ï€_0$æ¶æ„ï¼‰ï¼›5) åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§å®Œå…¨åŸºäºåŸå§‹ç¬¬ä¸€è§†è§’è§†é¢‘çš„VLAæ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œæ— éœ€ä»»ä½•äººå·¥æ ‡æ³¨æˆ–é¢å¤–ä¼ æ„Ÿå™¨æ•°æ®ã€‚EgoScaleræ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨æå–ç‰©ä½“æ“çºµè½¨è¿¹ï¼Œå¹¶è¿›è¡Œè½¨è¿¹ä¼˜åŒ–ï¼Œä»è€Œæ„å»ºé«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®é›†ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—é™ä½äº†VLAæ¨¡å‹è®­ç»ƒçš„æ•°æ®è·å–æˆæœ¬ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šEgoScaleræ¡†æ¶çš„å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ï¼Œè®ºæ–‡é‡ç‚¹åœ¨äºåˆ©ç”¨è¯¥æ¡†æ¶ç”Ÿæˆçš„æ•°æ®é›†è¿›è¡ŒVLAæ¨¡å‹çš„é¢„è®­ç»ƒã€‚é¢„è®­ç»ƒé‡‡ç”¨çš„$Ï€_0$æ¶æ„çš„å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚æœªçŸ¥ã€‚è½¨è¿¹ä¼˜åŒ–ç®—æ³•çš„å…·ä½“ç»†èŠ‚ä¹ŸæœªçŸ¥ï¼Œä½†å…¶ç›®æ ‡æ˜¯å»é™¤å™ªå£°å’Œè¡¥å…¨ç¼ºå¤±æ•°æ®ï¼Œä»¥æé«˜è½¨è¿¹çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥è®ºæ–‡æå‡ºçš„æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼ŒVLAæ¨¡å‹åœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šæ¯”ä»å¤´å¼€å§‹è®­ç»ƒæé«˜äº†20%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ä¸ä½¿ç”¨çœŸå®æœºå™¨äººæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼Œå¹¶ä¸”å°†è¯¥æ•°æ®é›†ä¸çœŸå®æœºå™¨äººæ•°æ®ç»“åˆä½¿ç”¨å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨VLAæ¨¡å‹è®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šæœºå™¨äººç­‰ã€‚é€šè¿‡åˆ©ç”¨å¤§é‡çš„å…¬å¼€ç¬¬ä¸€è§†è§’è§†é¢‘æ•°æ®ï¼Œå¯ä»¥è®­ç»ƒå‡ºæ›´é€šç”¨ã€æ›´é²æ£’çš„VLAæ¨¡å‹ï¼Œä»è€Œæé«˜æœºå™¨äººçš„è‡ªä¸»æ“ä½œèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å¯¼èˆªã€æŠ“å–ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Egocentric videos capture how humans manipulate objects and tools, providing diverse motion cues for learning object manipulation. Unlike the costly, expert-driven manual teleoperation commonly used in training Vision-Language-Action models (VLAs), egocentric videos offer a scalable alternative. However, prior studies that leverage such videos for training robot policies typically rely on auxiliary annotations, such as detailed hand-pose recordings. Consequently, it remains unclear whether VLAs can be trained directly from raw egocentric videos. In this work, we address this challenge by leveraging EgoScaler, a framework that extracts 6DoF object manipulation trajectories from egocentric videos without requiring auxiliary recordings. We apply EgoScaler to four large-scale egocentric video datasets and automatically refine noisy or incomplete trajectories, thereby constructing a new large-scale dataset for VLA pre-training. Our experiments with a state-of-the-art $Ï€_0$ architecture in both simulated and real-robot environments yield three key findings: (i) pre-training on our dataset improves task success rates by over 20\% compared to training from scratch, (ii) the performance is competitive with that achieved using real-robot datasets, and (iii) combining our dataset with real-robot data yields further improvements. These results demonstrate that egocentric videos constitute a promising and scalable resource for advancing VLA research.

