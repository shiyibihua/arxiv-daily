---
layout: default
title: DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions
---

# DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22175" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22175v1</a>
  <a href="https://arxiv.org/pdf/2509.22175.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22175v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22175v1', 'DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Quanzhou Li, Zhonghua Wu, Jingbo Wang, Chen Change Loy, Bo Dai

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://quanzhou-li.github.io/DHAGrasp/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DHAGraspï¼šæå‡ºæ–‡æœ¬å¼•å¯¼çš„åŒæ‰‹æŠ“å–ç”Ÿæˆæ–¹æ³•ï¼Œå®ç°è¯­ä¹‰æ„ŸçŸ¥çš„æŠ“å–å§¿æ€åˆæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)**

**å…³é”®è¯**: `åŒæ‰‹æŠ“å–` `æŠ“å–ç”Ÿæˆ` `å¯ä¾›æ€§` `æ–‡æœ¬å¼•å¯¼` `äººæœºäº¤äº’` `æœºå™¨äººæ“ä½œ` `æ•°æ®é›†åˆæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æŠ“å–æ•°æ®é›†ä¸»è¦å…³æ³¨å•æ‰‹äº¤äº’ï¼Œç¼ºä¹è¶³å¤Ÿçš„è¯­ä¹‰ä¿¡æ¯å’ŒåŒæ‰‹æŠ“å–æ•°æ®ï¼Œé™åˆ¶äº†ç›¸å…³ç ”ç©¶ã€‚
2. DHAGraspæå‡ºäº†ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„åŒæ‰‹æŠ“å–ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨åŒæ‰‹å¯ä¾›æ€§è¡¨ç¤ºå’Œä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ï¼Œæå‡æŠ“å–å§¿æ€çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDHAGraspåœ¨æŠ“å–è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿä¸ºæœªè§è¿‡çš„ç‰©ä½“ç”Ÿæˆå¤šæ ·ä¸”åˆç†çš„åŒæ‰‹æŠ“å–å§¿æ€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”±äºæ•°æ®é›†çš„ç¨€ç¼ºæ€§ï¼Œå­¦ä¹ ç”Ÿæˆç¬¦åˆç‰©ä½“è¯­ä¹‰çš„åŒæ‰‹æŠ“å–å§¿æ€å¯¹äºé²æ£’çš„äººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œä½†ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„æŠ“å–æ•°æ®é›†ä¸»è¦é›†ä¸­äºå•æ‰‹äº¤äº’ï¼Œå¹¶ä¸”ä»…åŒ…å«æœ‰é™çš„è¯­ä¹‰éƒ¨ä»¶æ ‡æ³¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºSymOptçš„æµç¨‹ï¼Œé€šè¿‡åˆ©ç”¨ç°æœ‰çš„å•æ‰‹æ•°æ®é›†å¹¶æŒ–æ˜ç‰©ä½“å’Œæ‰‹çš„å¯¹ç§°æ€§ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒæ‰‹æŠ“å–æ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–‡æœ¬å¼•å¯¼çš„åŒæ‰‹æŠ“å–ç”Ÿæˆå™¨DHAGraspï¼Œç”¨äºä¸ºæœªè§è¿‡çš„ç‰©ä½“åˆæˆåŒæ‰‹æŠ“å–å§¿æ€ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ä¸€ç§æ–°é¢–çš„åŒæ‰‹å¯ä¾›æ€§è¡¨ç¤ºï¼Œå¹¶éµå¾ªä¸¤é˜¶æ®µè®¾è®¡ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å°è§„æ¨¡çš„åˆ†å‰²è®­ç»ƒå¯¹è±¡ä¸­å­¦ä¹ ï¼ŒåŒæ—¶æ‰©å±•åˆ°æ›´å¤§çš„æœªåˆ†å‰²æ•°æ®æ± ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”è¯­ä¹‰ä¸€è‡´çš„æŠ“å–å§¿æ€ï¼Œåœ¨æŠ“å–è´¨é‡å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ç‰©ä½“æ–¹é¢å‡ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŒæ‰‹æŠ“å–å§¿æ€ç”Ÿæˆé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºå’Œç¼ºä¹è¯­ä¹‰ä¿¡æ¯çš„æƒ…å†µä¸‹ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºå•æ‰‹æŠ“å–ï¼Œå¿½ç•¥äº†åŒæ‰‹ååŒæ“ä½œçš„å¤æ‚æ€§ï¼Œå¹¶ä¸”éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„ç‰©ä½“ã€‚æ­¤å¤–ï¼Œç°æœ‰æ•°æ®é›†çš„è¯­ä¹‰æ ‡æ³¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆæŒ‡å¯¼æŠ“å–å§¿æ€çš„ç”Ÿæˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç‰©ä½“å’Œæ‰‹çš„å¯¹ç§°æ€§ï¼Œä»ç°æœ‰çš„å•æ‰‹æŠ“å–æ•°æ®é›†ä¸­ç”Ÿæˆå¤§è§„æ¨¡çš„åŒæ‰‹æŠ“å–æ•°æ®é›†ã€‚ç„¶åï¼Œé€šè¿‡æ–‡æœ¬å¼•å¯¼çš„æ–¹å¼ï¼Œå­¦ä¹ åŒæ‰‹æŠ“å–çš„å¯ä¾›æ€§è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åˆ†å‰²å’Œæœªåˆ†å‰²çš„æ•°æ®ï¼Œæå‡æŠ“å–å§¿æ€çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDHAGraspåŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ•°æ®é›†æ„å»ºé˜¶æ®µï¼šåˆ©ç”¨SymOptæµç¨‹ï¼Œä»å•æ‰‹æŠ“å–æ•°æ®é›†ç”Ÿæˆå¤§è§„æ¨¡çš„åŒæ‰‹æŠ“å–æ•°æ®é›†ï¼Œè¯¥æµç¨‹åˆ©ç”¨ç‰©ä½“å’Œæ‰‹çš„å¯¹ç§°æ€§ï¼Œè‡ªåŠ¨ç”ŸæˆåŒæ‰‹æŠ“å–å§¿æ€ã€‚2) æŠ“å–ç”Ÿæˆé˜¶æ®µï¼šDHAGraspé¦–å…ˆå­¦ä¹ åŒæ‰‹å¯ä¾›æ€§è¡¨ç¤ºï¼Œç„¶åé‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µåœ¨å°è§„æ¨¡çš„åˆ†å‰²æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç¬¬äºŒé˜¶æ®µåœ¨å¤§è§„æ¨¡çš„æœªåˆ†å‰²æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºï¼š1) æå‡ºäº†SymOptæµç¨‹ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¤§è§„æ¨¡çš„åŒæ‰‹æŠ“å–æ•°æ®é›†ã€‚2) æå‡ºäº†åŒæ‰‹å¯ä¾›æ€§è¡¨ç¤ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰ç‰©ä½“å’Œæ‰‹ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚3) é‡‡ç”¨äº†ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åˆ†å‰²å’Œæœªåˆ†å‰²çš„æ•°æ®ï¼Œæå‡æŠ“å–å§¿æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºé˜¶æ®µï¼ŒSymOptæµç¨‹åˆ©ç”¨ç‰©ä½“å’Œæ‰‹çš„å¯¹ç§°æ€§ï¼Œè‡ªåŠ¨ç”ŸæˆåŒæ‰‹æŠ“å–å§¿æ€ï¼Œå¹¶å¯¹æŠ“å–å§¿æ€è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ä¿è¯å…¶åˆç†æ€§ã€‚åœ¨æŠ“å–ç”Ÿæˆé˜¶æ®µï¼ŒDHAGraspé‡‡ç”¨Transformerç½‘ç»œç»“æ„ï¼Œå­¦ä¹ åŒæ‰‹å¯ä¾›æ€§è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±ç”Ÿæˆè¯­ä¹‰ä¸€è‡´çš„æŠ“å–å§¿æ€ã€‚ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ä¸­ï¼Œç¬¬ä¸€é˜¶æ®µä½¿ç”¨åˆ†å‰²æ•°æ®è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨æœªåˆ†å‰²æ•°æ®è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDHAGraspåœ¨æŠ“å–è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æœªè§è¿‡çš„ç‰©ä½“ä¸Šï¼ŒDHAGraspçš„æŠ“å–æˆåŠŸç‡æ¯”æœ€å¼ºçš„åŸºçº¿æ–¹æ³•æé«˜äº†10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒDHAGraspèƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”è¯­ä¹‰ä¸€è‡´çš„æŠ“å–å§¿æ€ï¼Œèƒ½å¤Ÿæ»¡è¶³ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ“ä½œã€è™šæ‹Ÿç°å®ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒæœºå™¨äººå®Œæˆå¤æ‚çš„åŒæ‰‹æ“ä½œä»»åŠ¡ï¼Œå¦‚ç»„è£…å®¶å…·ã€çƒ¹é¥ªé£Ÿç‰©ç­‰ã€‚åœ¨è™šæ‹Ÿç°å®ä¸­ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´åŠ è‡ªç„¶å’ŒçœŸå®çš„åŒæ‰‹äº¤äº’ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›äººæœºåä½œï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning to generate dual-hand grasps that respect object semantics is essential for robust hand-object interaction but remains largely underexplored due to dataset scarcity. Existing grasp datasets predominantly focus on single-hand interactions and contain only limited semantic part annotations. To address these challenges, we introduce a pipeline, SymOpt, that constructs a large-scale dual-hand grasp dataset by leveraging existing single-hand datasets and exploiting object and hand symmetries. Building on this, we propose a text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand Affordance-aware Grasps for unseen objects. Our approach incorporates a novel dual-hand affordance representation and follows a two-stage design, which enables effective learning from a small set of segmented training objects while scaling to a much larger pool of unsegmented data. Extensive experiments demonstrate that our method produces diverse and semantically consistent grasps, outperforming strong baselines in both grasp quality and generalization to unseen objects. The project page is at https://quanzhou-li.github.io/DHAGrasp/.

