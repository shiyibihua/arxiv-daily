---
layout: default
title: Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy
---

# Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy

**arXiv**: [2511.00555v1](https://arxiv.org/abs/2511.00555) | [PDF](https://arxiv.org/pdf/2511.00555.pdf)

**ä½œè€…**: Dianye Huang, Nassir Navab, Zhongliang Jiang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-01

**å¤‡æ³¨**: Accepted by IEEE T-RO

**DOI**: [10.1109/TRO.2025.3629819](https://doi.org/10.1109/TRO.2025.3629819)

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/dianyeHuang/D3P)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºD3Pç®—æ³•ï¼Œé€šè¿‡æ·±åº¦Koopmanå¢žå¼ºæ‰©æ•£ç­–ç•¥æå‡æ¨¡ä»¿å­¦ä¹ åœ¨åˆ†å¸ƒå¤–çŠ¶æ€çš„é²æ£’æ€§**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `æ‰©æ•£æ¨¡åž‹` `æœºå™¨äººæ“ä½œ` `Koopmanç®—å­` `åˆ†å¸ƒå¤–æ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽæ‰©æ•£çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•éš¾ä»¥æ•æ‰é•¿æ—¶ä¾èµ–ï¼Œå°¤å…¶æ˜¯åœ¨èžåˆæœ¬ä½“æ„Ÿå—ä¿¡æ¯æ—¶ï¼Œæ˜“è¿‡æ‹Ÿåˆã€‚
2. D3Pç®—æ³•é€šè¿‡åŒåˆ†æ”¯æž¶æž„è§£è€¦è§†è§‰å’Œæœ¬ä½“æ„Ÿå—ä¿¡æ¯ï¼Œå¹¶å¼•å…¥Koopmanç®—å­å¢žå¼ºè§†è§‰è¡¨å¾å­¦ä¹ ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒD3Påœ¨ä»¿çœŸå’ŒçœŸå®žæœºå™¨äººä»»åŠ¡ä¸­å‡ä¼˜äºŽçŽ°æœ‰æ‰©æ•£ç­–ç•¥ï¼Œå¹³å‡æå‡è¶…è¿‡14%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦Koopmanå¢žå¼ºåŒåˆ†æ”¯æ‰©æ•£ç­–ç•¥(D3P)ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡ä»¿å­¦ä¹ åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å¯¹åˆ†å¸ƒå¤–çŠ¶æ€çš„é²æ£’æ€§ã€‚çŽ°æœ‰åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„èŒƒå¼éš¾ä»¥æ•æ‰è·¨å¤šæ­¥éª¤çš„å¼ºæ—¶é—´ä¾èµ–æ€§ï¼Œå°¤å…¶æ˜¯åœ¨èžåˆæœ¬ä½“æ„Ÿå—è¾“å…¥æ—¶ï¼Œå®¹æ˜“è¿‡åº¦æ‹Ÿåˆæœ¬ä½“æ„Ÿå—çº¿ç´¢è€Œå¿½ç•¥è§†è§‰ç‰¹å¾ã€‚D3På¼•å…¥åŒåˆ†æ”¯æž¶æž„ï¼Œè§£è€¦ä¸åŒæ„Ÿè§‰æ¨¡æ€ç»„åˆçš„ä½œç”¨ï¼šè§†è§‰åˆ†æ”¯ç¼–ç è§†è§‰è§‚æµ‹ä»¥æŒ‡ç¤ºä»»åŠ¡è¿›åº¦ï¼Œèžåˆåˆ†æ”¯æ•´åˆè§†è§‰å’Œæœ¬ä½“æ„Ÿå—è¾“å…¥ä»¥è¿›è¡Œç²¾ç¡®æ“ä½œã€‚å½“æœºå™¨äººæœªèƒ½å®Œæˆä¸­é—´ç›®æ ‡æ—¶ï¼Œç­–ç•¥å¯ä»¥åŠ¨æ€åˆ‡æ¢åˆ°è§†è§‰åˆ†æ”¯ç”Ÿæˆçš„åŠ¨ä½œå—ï¼Œæ¢å¤åˆ°å…ˆå‰è§‚å¯Ÿåˆ°çš„çŠ¶æ€å¹¶é‡æ–°å°è¯•ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒD3Pè¿˜ç»“åˆäº†æ·±åº¦Koopmanç®—å­æ¨¡å—ï¼Œä»¥å¢žå¼ºè§†è§‰è¡¨å¾å­¦ä¹ ã€‚åœ¨æŽ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ç”Ÿæˆæ¨¡åž‹çš„æµ‹è¯•æ—¶æŸå¤±ä½œä¸ºç½®ä¿¡åº¦ä¿¡å·ï¼ŒæŒ‡å¯¼æ—¶é—´é‡å çš„é¢„æµ‹åŠ¨ä½œå—çš„èšåˆï¼Œä»Žè€Œæé«˜ç­–ç•¥æ‰§è¡Œçš„å¯é æ€§ã€‚åœ¨å…­ä¸ªRLBenchæ¡Œé¢ä»»åŠ¡çš„ä»¿çœŸå®žéªŒä¸­ï¼ŒD3Pçš„æ€§èƒ½ä¼˜äºŽæœ€å…ˆè¿›çš„æ‰©æ•£ç­–ç•¥ï¼Œå¹³å‡æå‡14.6%ã€‚åœ¨ä¸‰ä¸ªçœŸå®žæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œå®žçŽ°äº†15.0%çš„æ”¹è¿›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰é•¿æ—¶æ—¶é—´ä¾èµ–å…³ç³»ï¼Œå°¤å…¶æ˜¯åœ¨èžåˆè§†è§‰å’Œæœ¬ä½“æ„Ÿå—ä¿¡æ¯æ—¶ã€‚ç­–ç•¥å®¹æ˜“è¿‡åº¦ä¾èµ–æœ¬ä½“æ„Ÿå—ä¿¡æ¯ï¼Œè€Œå¿½ç•¥è§†è§‰ä¿¡æ¯æä¾›çš„ä»»åŠ¡è¿›åº¦æŒ‡ç¤ºï¼Œå¯¼è‡´å¯¹åˆ†å¸ƒå¤–çŠ¶æ€çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œä¾‹å¦‚åœ¨æŠ“å–å¤±è´¥åŽæ— æ³•æœ‰æ•ˆæ¢å¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šD3Pçš„æ ¸å¿ƒæ€è·¯æ˜¯è§£è€¦è§†è§‰å’Œæœ¬ä½“æ„Ÿå—ä¿¡æ¯çš„ä½œç”¨ï¼Œåˆ©ç”¨è§†è§‰ä¿¡æ¯ä½œä¸ºä»»åŠ¡è¿›åº¦çš„æŒ‡ç¤ºå™¨ï¼Œæœ¬ä½“æ„Ÿå—ä¿¡æ¯ç”¨äºŽç²¾ç¡®æ“ä½œã€‚é€šè¿‡åŒåˆ†æ”¯æž¶æž„å®žçŽ°è¿™ä¸€è§£è€¦ï¼Œå¹¶åœ¨è§†è§‰åˆ†æ”¯ä¸­å¼•å…¥Koopmanç®—å­å­¦ä¹ è§†è§‰è¡¨å¾çš„åŠ¨æ€ç‰¹æ€§ï¼Œä»Žè€Œæé«˜ç­–ç•¥çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šD3Pç®—æ³•é‡‡ç”¨åŒåˆ†æ”¯æ‰©æ•£ç­–ç•¥æž¶æž„ã€‚è§†è§‰åˆ†æ”¯ä»…æŽ¥æ”¶è§†è§‰è¾“å…¥ï¼Œç”¨äºŽç”ŸæˆæŒ‡ç¤ºä»»åŠ¡è¿›åº¦çš„åŠ¨ä½œå—ï¼›èžåˆåˆ†æ”¯æŽ¥æ”¶è§†è§‰å’Œæœ¬ä½“æ„Ÿå—è¾“å…¥ï¼Œç”¨äºŽç”Ÿæˆç²¾ç¡®æ“ä½œçš„åŠ¨ä½œå—ã€‚å½“èžåˆåˆ†æ”¯æ‰§è¡Œå¤±è´¥æ—¶ï¼Œç­–ç•¥å¯ä»¥åˆ‡æ¢åˆ°è§†è§‰åˆ†æ”¯ç”Ÿæˆçš„åŠ¨ä½œå—ï¼Œå°è¯•æ¢å¤åˆ°ä¹‹å‰çš„çŠ¶æ€ã€‚æ­¤å¤–ï¼ŒD3Pè¿˜åŒ…å«ä¸€ä¸ªæ·±åº¦Koopmanç®—å­æ¨¡å—ï¼Œç”¨äºŽå­¦ä¹ è§†è§‰è¾“å…¥çš„åŠ¨æ€ç‰¹æ€§ï¼Œå¹¶æä¾›æ›´é²æ£’çš„è§†è§‰è¡¨å¾ã€‚æŽ¨ç†é˜¶æ®µï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡åž‹çš„æµ‹è¯•æ—¶æŸå¤±ä½œä¸ºç½®ä¿¡åº¦ä¿¡å·ï¼ŒæŒ‡å¯¼æ—¶é—´é‡å çš„é¢„æµ‹åŠ¨ä½œå—çš„èšåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šD3Pçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) åŒåˆ†æ”¯æž¶æž„ï¼Œè§£è€¦è§†è§‰å’Œæœ¬ä½“æ„Ÿå—ä¿¡æ¯çš„ä½œç”¨ï¼›2) å¼•å…¥æ·±åº¦Koopmanç®—å­ï¼Œå­¦ä¹ è§†è§‰è¡¨å¾çš„åŠ¨æ€ç‰¹æ€§ï¼›3) åˆ©ç”¨ç”Ÿæˆæ¨¡åž‹çš„æµ‹è¯•æ—¶æŸå¤±ä½œä¸ºç½®ä¿¡åº¦ä¿¡å·ï¼ŒæŒ‡å¯¼åŠ¨ä½œå—çš„èšåˆã€‚è¿™äº›åˆ›æ–°ä½¿å¾—D3Pèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰é•¿æ—¶ä¾èµ–å…³ç³»ï¼Œæé«˜å¯¹åˆ†å¸ƒå¤–çŠ¶æ€çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šD3Pçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åŒåˆ†æ”¯æ‰©æ•£æ¨¡åž‹çš„å…·ä½“ç½‘ç»œç»“æž„ï¼ŒåŒ…æ‹¬è§†è§‰åˆ†æ”¯å’Œèžåˆåˆ†æ”¯çš„ç½‘ç»œå±‚æ•°ã€æ¿€æ´»å‡½æ•°ç­‰ï¼›2) æ·±åº¦Koopmanç®—å­æ¨¡å—çš„å…·ä½“å®žçŽ°ï¼ŒåŒ…æ‹¬Koopmanç®—å­çš„ç»´åº¦ã€æŸå¤±å‡½æ•°ç­‰ï¼›3) æµ‹è¯•æ—¶æŸå¤±çš„å…·ä½“è®¡ç®—æ–¹å¼ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨è¯¥æŸå¤±æ¥æŒ‡å¯¼åŠ¨ä½œå—çš„èšåˆï¼›4) åŠ¨ä½œå—çš„é•¿åº¦å’Œé‡å ç¨‹åº¦ï¼Œä»¥åŠå¦‚ä½•é€‰æ‹©åˆé€‚çš„åŠ¨ä½œå—é•¿åº¦ä»¥å¹³è¡¡ç²¾åº¦å’Œæ•ˆçŽ‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

D3Pç®—æ³•åœ¨å…­ä¸ªRLBenchæ¡Œé¢ä»»åŠ¡çš„ä»¿çœŸå®žéªŒä¸­ï¼Œå¹³å‡æ€§èƒ½ä¼˜äºŽæœ€å…ˆè¿›çš„æ‰©æ•£ç­–ç•¥14.6%ã€‚åœ¨ä¸‰ä¸ªçœŸå®žæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒD3Pç®—æ³•çš„æ€§èƒ½æå‡äº†15.0%ã€‚è¿™äº›å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒD3Pç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¨¡ä»¿å­¦ä¹ åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å¯¹åˆ†å¸ƒå¤–çŠ¶æ€çš„é²æ£’æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

D3Pç®—æ³•å¯åº”ç”¨äºŽå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ã€åŠ¨æ€çš„çŽ¯å¢ƒä¸­ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥ç”¨äºŽå®¶åº­æœåŠ¡æœºå™¨äººï¼Œå¸®åŠ©æœºå™¨äººå®Œæˆå„ç§å®¶åŠ¡ä»»åŠ¡ï¼›ä¹Ÿå¯ä»¥ç”¨äºŽå·¥ä¸šæœºå™¨äººï¼Œæé«˜æœºå™¨äººåœ¨ç”Ÿäº§çº¿ä¸Šçš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•è¿˜å¯ä»¥åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚äº¤é€šçŽ¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Integrating generative models with action chunking has shown significant promise in imitation learning for robotic manipulation. However, the existing diffusion-based paradigm often struggles to capture strong temporal dependencies across multiple steps, particularly when incorporating proprioceptive input. This limitation can lead to task failures, where the policy overfits to proprioceptive cues at the expense of capturing the visually derived features of the task. To overcome this challenge, we propose the Deep Koopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a dual-branch architecture to decouple the roles of different sensory modality combinations. The visual branch encodes the visual observations to indicate task progression, while the fused branch integrates both visual and proprioceptive inputs for precise manipulation. Within this architecture, when the robot fails to accomplish intermediate goals, such as grasping a drawer handle, the policy can dynamically switch to execute action chunks generated by the visual branch, allowing recovery to previously observed states and facilitating retrial of the task. To further enhance visual representation learning, we incorporate a Deep Koopman Operator module that captures structured temporal dynamics from visual inputs. During inference, we use the test-time loss of the generative model as a confidence signal to guide the aggregation of the temporally overlapping predicted action chunks, thereby enhancing the reliability of policy execution. In simulation experiments across six RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion policy by an average of 14.6\%. On three real-world robotic manipulation tasks, it achieves a 15.0\% improvement. Code: https://github.com/dianyeHuang/D3P.

