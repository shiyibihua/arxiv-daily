---
layout: default
title: Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning
---

# Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.12531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.12531v1</a>
  <a href="https://arxiv.org/pdf/2509.12531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.12531v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.12531v1', 'Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Scott Jones, Liyou Zhou, Sebastian W. Pattinson

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é¢„è®­ç»ƒè§†è§‰è¡¨å¾æ˜¾è‘—æå‡æ¨¡å‹å¼ºåŒ–å­¦ä¹ åœ¨è§†è§‰åŸŸåç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é¢„è®­ç»ƒè§†è§‰æ¨¡å‹` `æ¨¡å‹å¼ºåŒ–å­¦ä¹ ` `è§†è§‰åŸŸåç§»` `æ³›åŒ–èƒ½åŠ›` `éƒ¨åˆ†å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ æ–¹æ³•åœ¨è§†è§‰åœºæ™¯å˜åŒ–ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œæ˜¯ç”±äºç­–ç•¥å’Œè§†è§‰ç¼–ç å™¨è”åˆè®­ç»ƒå¯¼è‡´çš„ã€‚
2. è¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ï¼ˆPVMï¼‰æ¥æå‡æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰åœ¨è§†è§‰åŸŸåç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸¥é‡è§†è§‰åŸŸåç§»ä¸‹ï¼Œç»è¿‡éƒ¨åˆ†å¾®è°ƒçš„PVMæ˜¾è‘—ä¼˜äºä»å¤´è®­ç»ƒçš„æ¨¡å‹ï¼ŒéªŒè¯äº†PVMåœ¨MBRLä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ ä¸­ï¼Œæœºå™¨äººæ™ºèƒ½ä½“çš„æ§åˆ¶ç­–ç•¥ç›´æ¥ä»è§†è§‰è¾“å…¥ä¸­è·å¾—ã€‚é€šå¸¸çš„æ–¹æ³•æ˜¯ä»å¤´å¼€å§‹è”åˆè®­ç»ƒç­–ç•¥å’Œè§†è§‰ç¼–ç å™¨ï¼Œä½†è¿™ç§æ–¹æ³•åœ¨æ–°çš„è§†è§‰åœºæ™¯å˜åŒ–ä¸‹çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ä½¿ç”¨é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ï¼ˆPVMï¼‰æ¥æŒ‡å¯¼ç­–ç•¥ç½‘ç»œå¯ä»¥æé«˜æ¨¡å‹æ— å…³å¼ºåŒ–å­¦ä¹ ï¼ˆMFRLï¼‰çš„é²æ£’æ€§ã€‚æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼ŒMBRLæ¯”MFRLå…·æœ‰æ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚ç„¶è€Œï¼Œä¸ç›´è§‰ç›¸åçš„æ˜¯ï¼Œç°æœ‰çš„å·¥ä½œå‘ç°PVMåœ¨MBRLä¸­æ•ˆæœä¸ä½³ã€‚æœ¬æ–‡ç ”ç©¶äº†PVMåœ¨MBRLä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰åŸŸåç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ä¸¥é‡åç§»çš„æƒ…å†µä¸‹ï¼ŒPVMæ¯”ä»å¤´å¼€å§‹è®­ç»ƒçš„åŸºçº¿æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚è¿›ä¸€æ­¥ç ”ç©¶äº†ä¸åŒç¨‹åº¦çš„PVMå¾®è°ƒçš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œéƒ¨åˆ†å¾®è°ƒå¯ä»¥åœ¨æœ€æç«¯çš„åˆ†å¸ƒåç§»ä¸‹ä¿æŒæœ€é«˜çš„å¹³å‡ä»»åŠ¡æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒPVMåœ¨æé«˜è§†è§‰ç­–ç•¥å­¦ä¹ çš„é²æ£’æ€§æ–¹é¢éå¸¸æˆåŠŸï¼Œä¸ºåœ¨åŸºäºæ¨¡å‹çš„æœºå™¨äººå­¦ä¹ åº”ç”¨ä¸­æ›´å¹¿æ³›åœ°é‡‡ç”¨PVMæä¾›äº†æœ‰åŠ›çš„è¯æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰åœ¨è§†è§‰åŸŸåç§»ä¸‹æ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»å¤´å¼€å§‹è®­ç»ƒè§†è§‰ç¼–ç å™¨å’Œç­–ç•¥ç½‘ç»œï¼Œå¯¼è‡´æ¨¡å‹å¯¹æ–°çš„è§†è§‰åœºæ™¯å˜åŒ–ä¸é²æ£’ã€‚å³ä½¿ä½¿ç”¨é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ï¼ˆPVMï¼‰ï¼Œåœ¨MBRLä¸­çš„æ•ˆæœä¹Ÿä¸å¦‚åœ¨æ¨¡å‹æ— å…³å¼ºåŒ–å­¦ä¹ ï¼ˆMFRLï¼‰ä¸­æ˜¾è‘—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨PVMçš„å¼ºå¤§è¡¨å¾èƒ½åŠ›ï¼Œå¹¶é€šè¿‡é€‚å½“çš„å¾®è°ƒç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”MBRLçš„ä»»åŠ¡éœ€æ±‚ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨è§†è§‰åŸŸåç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å…³é”®åœ¨äºæ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡ç‚¹ï¼Œæ—¢èƒ½åˆ©ç”¨PVMçš„å…ˆéªŒçŸ¥è¯†ï¼Œåˆèƒ½é¿å…è¿‡åº¦å¾®è°ƒå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆåˆ°ç‰¹å®šç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ï¼ˆPVMï¼‰ï¼šä½¿ç”¨åœ¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ä½œä¸ºè§†è§‰ç¼–ç å™¨çš„åˆå§‹åŒ–ã€‚2) æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰ç®—æ³•ï¼šä½¿ç”¨MBRLç®—æ³•æ¥å­¦ä¹ ç¯å¢ƒæ¨¡å‹å’Œç­–ç•¥ã€‚3) å¾®è°ƒç­–ç•¥ï¼šç ”ç©¶ä¸åŒç¨‹åº¦çš„PVMå¾®è°ƒå¯¹æ€§èƒ½çš„å½±å“ï¼ŒåŒ…æ‹¬å®Œå…¨å¾®è°ƒã€éƒ¨åˆ†å¾®è°ƒå’Œä¸å¾®è°ƒã€‚4) è¯„ä¼°æŒ‡æ ‡ï¼šä½¿ç”¨åœ¨ä¸åŒè§†è§‰åŸŸåç§»ä¸‹çš„ä»»åŠ¡æ€§èƒ½ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå‘ç°PVMåœ¨MBRLä¸­å¹¶éæ€»æ˜¯æœ‰æ•ˆï¼Œå¹¶æå‡ºäº†éƒ¨åˆ†å¾®è°ƒç­–ç•¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼Œéƒ¨åˆ†å¾®è°ƒå¯ä»¥åœ¨ä¿æŒPVMæ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œä½¿å…¶é€‚åº”ç‰¹å®šçš„MBRLä»»åŠ¡ï¼Œä»è€Œåœ¨è§†è§‰åŸŸåç§»ä¸‹è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ï¼Œä¾‹å¦‚åœ¨ImageNetç­‰å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚2) è®¾è®¡åˆé€‚çš„å¾®è°ƒç­–ç•¥ï¼Œä¾‹å¦‚åªå¾®è°ƒPVMçš„éƒ¨åˆ†å±‚ï¼Œæˆ–è€…ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒã€‚3) ä½¿ç”¨åˆé€‚çš„MBRLç®—æ³•ï¼Œä¾‹å¦‚PETSã€PILCOç­‰ã€‚4) è®¾è®¡å…·æœ‰ä¸åŒç¨‹åº¦è§†è§‰åŸŸåç§»çš„å®éªŒç¯å¢ƒï¼Œä¾‹å¦‚æ”¹å˜å…‰ç…§ã€çº¹ç†ã€èƒŒæ™¯ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å…·æœ‰ä¸¥é‡è§†è§‰åŸŸåç§»çš„åœºæ™¯ä¸­ï¼Œç»è¿‡éƒ¨åˆ†å¾®è°ƒçš„PVMæ¯”ä»å¤´å¼€å§‹è®­ç»ƒçš„åŸºçº¿æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚å…·ä½“æ¥è¯´ï¼Œéƒ¨åˆ†å¾®è°ƒçš„PVMåœ¨å¹³å‡ä»»åŠ¡æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–å¾®è°ƒç­–ç•¥ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜è§†è§‰ç­–ç•¥å­¦ä¹ é²æ£’æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºPVMåœ¨åŸºäºæ¨¡å‹çš„æœºå™¨äººå­¦ä¹ åº”ç”¨ä¸­çš„å¹¿æ³›åº”ç”¨æä¾›äº†æœ‰åŠ›çš„è¯æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æœºå™¨äººä¸è§†è§‰ç¯å¢ƒäº¤äº’çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨è§†è§‰å˜åŒ–ç¯å¢ƒä¸‹çš„é²æ£’æ€§ï¼Œå¯ä»¥é™ä½éƒ¨ç½²æˆæœ¬ï¼Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œç¯å¢ƒï¼Œä¾‹å¦‚å¤šæ¨¡æ€æ„ŸçŸ¥ã€å¤æ‚æ“ä½œç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In visuomotor policy learning, the control policy for the robotic agent is derived directly from visual inputs. The typical approach, where a policy and vision encoder are trained jointly from scratch, generalizes poorly to novel visual scene changes. Using pre-trained vision models (PVMs) to inform a policy network improves robustness in model-free reinforcement learning (MFRL). Recent developments in Model-based reinforcement learning (MBRL) suggest that MBRL is more sample-efficient than MFRL. However, counterintuitively, existing work has found PVMs to be ineffective in MBRL. Here, we investigate PVM's effectiveness in MBRL, specifically on generalization under visual domain shifts. We show that, in scenarios with severe shifts, PVMs perform much better than a baseline model trained from scratch. We further investigate the effects of varying levels of fine-tuning of PVMs. Our results show that partial fine-tuning can maintain the highest average task performance under the most extreme distribution shifts. Our results demonstrate that PVMs are highly successful in promoting robustness in visual policy learning, providing compelling evidence for their wider adoption in model-based robotic learning applications.

