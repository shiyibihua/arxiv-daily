---
layout: default
title: Robust Online Residual Refinement via Koopman-Guided Dynamics Modeling
---

# Robust Online Residual Refinement via Koopman-Guided Dynamics Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.12562" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.12562v1</a>
  <a href="https://arxiv.org/pdf/2509.12562.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.12562v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.12562v1', 'Robust Online Residual Refinement via Koopman-Guided Dynamics Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhefei Gong, Shangke Lyu, Pengxiang Ding, Wei Xiao, Donglin Wang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKORRï¼šåˆ©ç”¨Koopmanå¼•å¯¼çš„åŠ¨æ€æ¨¡å‹å®ç°é²æ£’çš„åœ¨çº¿æ®‹å·®ç­–ç•¥ä¼˜åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `æ®‹å·®ç­–ç•¥å­¦ä¹ ` `Koopmanç®—å­` `åŠ¨æ€å»ºæ¨¡` `æœºå™¨äººæ§åˆ¶` `é•¿æ—¶ç¨‹ä»»åŠ¡` `é²æ£’æ€§` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ¨¡ä»¿å­¦ä¹ åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­æ˜“ç´¯ç§¯è¯¯å·®ï¼Œç°æœ‰æ®‹å·®ç­–ç•¥å­¦ä¹ æ–¹æ³•ç¼ºä¹å…¨å±€çŠ¶æ€ç†è§£ï¼Œé™åˆ¶äº†æ³›åŒ–æ€§ã€‚
2. è®ºæ–‡æå‡ºKORRæ¡†æ¶ï¼Œåˆ©ç”¨Koopmanç®—å­å­¦ä¹ çº¿æ€§åŠ¨æ€æ¨¡å‹ï¼ŒæŒ‡å¯¼æ®‹å·®ç­–ç•¥æ›´æ–°ï¼Œå®ç°å…¨å±€ä¼˜åŒ–ã€‚
3. åœ¨æœºå™¨äººå®¶å…·ç»„è£…ä»»åŠ¡ä¸­ï¼ŒKORRç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼Œåœ¨æ€§èƒ½ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ (IL)èƒ½å¤Ÿä»æ¼”ç¤ºä¸­é«˜æ•ˆåœ°å­¦ä¹ æŠ€èƒ½ï¼Œä½†ç”±äºè¯¯å·®ç´¯ç§¯ï¼Œåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡å’Œé«˜ç²¾åº¦æ§åˆ¶æ–¹é¢è¡¨ç°ä¸ä½³ã€‚æ®‹å·®ç­–ç•¥å­¦ä¹ é€šè¿‡é—­ç¯æ ¡æ­£æ¥ä¼˜åŒ–åŸºç¡€ç­–ç•¥ï¼Œæä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„ã€æ¨¡å‹æ— å…³çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¯¹åŸºç¡€ç­–ç•¥çš„å±€éƒ¨æ ¡æ­£ï¼Œç¼ºä¹å¯¹çŠ¶æ€æ¼”åŒ–çš„å…¨å±€ç†è§£ï¼Œé™åˆ¶äº†é²æ£’æ€§å’Œå¯¹æœªè§åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºç»“åˆå…¨å±€åŠ¨æ€æ¨¡å‹æ¥æŒ‡å¯¼æ®‹å·®ç­–ç•¥æ›´æ–°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨Koopmanç®—å­ç†è®ºåœ¨å­¦ä¹ åˆ°çš„æ½œåœ¨ç©ºé—´ä¸­æ–½åŠ çº¿æ€§æ—¶ä¸å˜ç»“æ„ï¼Œä»è€Œå®ç°å¯é çš„çŠ¶æ€è½¬ç§»ï¼Œå¹¶æ”¹è¿›é•¿æ—¶ç¨‹é¢„æµ‹å’Œæœªè§ç¯å¢ƒçš„å¤–æ¨èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†KORRï¼ˆKoopmanå¼•å¯¼çš„åœ¨çº¿æ®‹å·®ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¡†æ¶ï¼Œå®ƒæ ¹æ®Koopmané¢„æµ‹çš„æ½œåœ¨çŠ¶æ€æ¥è°ƒèŠ‚æ®‹å·®æ ¡æ­£ï¼Œä»è€Œå®ç°å…¨å±€çŸ¥æƒ…å’Œç¨³å®šçš„åŠ¨ä½œä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨å„ç§æ‰°åŠ¨ä¸‹çš„é•¿æ—¶ç¨‹ã€ç²¾ç»†æœºå™¨äººå®¶å…·ç»„è£…ä»»åŠ¡ä¸Šè¯„ä¼°äº†KORRã€‚ç»“æœè¡¨æ˜ï¼Œä¸å¼ºå¤§çš„åŸºçº¿ç›¸æ¯”ï¼Œåœ¨æ€§èƒ½ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢éƒ½æœ‰æŒç»­çš„æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¿›ä¸€æ­¥çªå‡ºäº†åŸºäºKoopmançš„å»ºæ¨¡åœ¨è¿æ¥ç°ä»£å­¦ä¹ æ–¹æ³•ä¸ç»å…¸æ§åˆ¶ç†è®ºæ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ®‹å·®ç­–ç•¥å­¦ä¹ æ–¹æ³•ä¸»è¦å…³æ³¨å±€éƒ¨æ ¡æ­£ï¼Œç¼ºä¹å¯¹çŠ¶æ€æ¼”åŒ–çš„å…¨å±€ç†è§£ï¼Œå¯¼è‡´åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­è¯¯å·®ç´¯ç§¯ï¼Œé²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚ç‰¹åˆ«æ˜¯åœ¨æ¨¡ä»¿å­¦ä¹ ä¸­ï¼Œå³ä½¿åˆå§‹ç­–ç•¥è¡¨ç°è‰¯å¥½ï¼Œå¾®å°çš„è¯¯å·®ä¹Ÿä¼šéšç€æ—¶é—´æ¨ç§»è€Œæ”¾å¤§ï¼Œæœ€ç»ˆå¯¼è‡´ä»»åŠ¡å¤±è´¥ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨å…¨å±€ä¿¡æ¯æ¥æŒ‡å¯¼æ®‹å·®ç­–ç•¥çš„æ›´æ–°ï¼Œæ˜¯è§£å†³é—®é¢˜çš„å…³é”®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Koopmanç®—å­ç†è®ºå­¦ä¹ ç³»ç»Ÿçš„å…¨å±€åŠ¨æ€æ¨¡å‹ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´ä¸­æ–½åŠ çº¿æ€§æ—¶ä¸å˜ç»“æ„ã€‚é€šè¿‡Koopmanç®—å­é¢„æµ‹æœªæ¥çš„çŠ¶æ€ï¼Œå¹¶ä»¥æ­¤æŒ‡å¯¼æ®‹å·®ç­–ç•¥çš„æ›´æ–°ï¼Œä»è€Œå®ç°å…¨å±€çŸ¥æƒ…çš„åŠ¨ä½œä¼˜åŒ–ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæé«˜ç­–ç•¥çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡å’Œæœªè§ç¯å¢ƒä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKORRæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) åŸºç¡€ç­–ç•¥ï¼šæä¾›åˆå§‹çš„åŠ¨ä½œåºåˆ—ã€‚2) KoopmanåŠ¨æ€æ¨¡å‹ï¼šå­¦ä¹ ä»å½“å‰çŠ¶æ€åˆ°æœªæ¥çŠ¶æ€çš„çº¿æ€§æ˜ å°„ã€‚3) æ®‹å·®ç­–ç•¥ï¼šæ ¹æ®Koopmané¢„æµ‹çš„æ½œåœ¨çŠ¶æ€ï¼Œå¯¹åŸºç¡€ç­–ç•¥çš„åŠ¨ä½œè¿›è¡Œæ ¡æ­£ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆï¼ŒåŸºç¡€ç­–ç•¥ç”Ÿæˆä¸€ä¸ªåŠ¨ä½œï¼›ç„¶åï¼ŒKoopmanåŠ¨æ€æ¨¡å‹é¢„æµ‹æœªæ¥çš„çŠ¶æ€ï¼›æ¥ç€ï¼Œæ®‹å·®ç­–ç•¥æ ¹æ®é¢„æµ‹çš„çŠ¶æ€å¯¹åŠ¨ä½œè¿›è¡Œæ ¡æ­£ï¼›æœ€åï¼Œæ‰§è¡Œæ ¡æ­£åçš„åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†Koopmanç®—å­ç†è®ºå¼•å…¥åˆ°æ®‹å·®ç­–ç•¥å­¦ä¹ ä¸­ã€‚ä¸ä¼ ç»Ÿçš„æ®‹å·®ç­–ç•¥å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒKORRåˆ©ç”¨Koopmanç®—å­å­¦ä¹ å…¨å±€åŠ¨æ€æ¨¡å‹ï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£çŠ¶æ€çš„æ¼”åŒ–è¿‡ç¨‹ï¼Œå¹¶ä»¥æ­¤æŒ‡å¯¼æ®‹å·®ç­–ç•¥çš„æ›´æ–°ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæé«˜ç­–ç•¥çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡å’Œæœªè§ç¯å¢ƒä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šKORRçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨è‡ªç¼–ç å™¨å­¦ä¹ æ½œåœ¨ç©ºé—´ï¼Œå¹¶åœ¨è¯¥ç©ºé—´ä¸­åº”ç”¨Koopmanç®—å­ã€‚2) ä½¿ç”¨çº¿æ€§å›å½’æ¥å­¦ä¹ Koopmanç®—å­ã€‚3) æ®‹å·®ç­–ç•¥è¢«è®¾è®¡ä¸ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå…¶è¾“å…¥æ˜¯Koopmané¢„æµ‹çš„æ½œåœ¨çŠ¶æ€ï¼Œè¾“å‡ºæ˜¯å¯¹åŸºç¡€ç­–ç•¥åŠ¨ä½œçš„æ ¡æ­£é‡ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ¨¡ä»¿å­¦ä¹ æŸå¤±ã€åŠ¨æ€æ¨¡å‹é¢„æµ‹æŸå¤±å’Œæ­£åˆ™åŒ–é¡¹ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ï¼ˆå¦‚è‡ªç¼–ç å™¨å’Œæ®‹å·®ç­–ç•¥çš„ç½‘ç»œç»“æ„ã€å­¦ä¹ ç‡ã€æ­£åˆ™åŒ–ç³»æ•°ç­‰ï¼‰éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é•¿æ—¶ç¨‹æœºå™¨äººå®¶å…·ç»„è£…ä»»åŠ¡ä¸­ï¼ŒKORRæ¡†æ¶åœ¨å„ç§æ‰°åŠ¨ä¸‹å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒKORRåœ¨æˆåŠŸç‡ã€ä»»åŠ¡å®Œæˆæ—¶é—´å’Œé²æ£’æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼ŒKORRçš„æˆåŠŸç‡æ¯”æœ€ä½³åŸºçº¿æé«˜äº†10%-20%ã€‚è¿™äº›ç»“æœéªŒè¯äº†Koopmanå¼•å¯¼çš„åŠ¨æ€æ¨¡å‹åœ¨æ®‹å·®ç­–ç•¥å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é•¿æ—¶ç¨‹è§„åˆ’å’Œé«˜ç²¾åº¦æ§åˆ¶çš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šå¤æ‚ç¯å¢ƒä¸‹çš„æœºå™¨äººå¯¼èˆªã€ç²¾ç»†æ“ä½œä»»åŠ¡ï¼ˆå¦‚åŒ»ç–—æ‰‹æœ¯ã€ç”µå­å…ƒä»¶ç»„è£…ï¼‰ã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿ç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥é™ä½äººå·¥å¹²é¢„çš„éœ€æ±‚ï¼Œæé«˜ç”Ÿäº§æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„æœªæ¥å‘å±•å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation learning (IL) enables efficient skill acquisition from demonstrations but often struggles with long-horizon tasks and high-precision control due to compounding errors. Residual policy learning offers a promising, model-agnostic solution by refining a base policy through closed-loop corrections. However, existing approaches primarily focus on local corrections to the base policy, lacking a global understanding of state evolution, which limits robustness and generalization to unseen scenarios. To address this, we propose incorporating global dynamics modeling to guide residual policy updates. Specifically, we leverage Koopman operator theory to impose linear time-invariant structure in a learned latent space, enabling reliable state transitions and improved extrapolation for long-horizon prediction and unseen environments. We introduce KORR (Koopman-guided Online Residual Refinement), a simple yet effective framework that conditions residual corrections on Koopman-predicted latent states, enabling globally informed and stable action refinement. We evaluate KORR on long-horizon, fine-grained robotic furniture assembly tasks under various perturbations. Results demonstrate consistent gains in performance, robustness, and generalization over strong baselines. Our findings further highlight the potential of Koopman-based modeling to bridge modern learning methods with classical control theory.

