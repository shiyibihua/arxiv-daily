---
layout: default
title: GRATE: a Graph transformer-based deep Reinforcement learning Approach for Time-efficient autonomous robot Exploration
---

# GRATE: a Graph transformer-based deep Reinforcement learning Approach for Time-efficient autonomous robot Exploration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.12863" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.12863v1</a>
  <a href="https://arxiv.org/pdf/2509.12863.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.12863v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.12863v1', 'GRATE: a Graph transformer-based deep Reinforcement learning Approach for Time-efficient autonomous robot Exploration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haozhan Ni, Jingsong Liang, Chenyu He, Yuhong Cao, Guillaume Sartoretti

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå›¾Transformerçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•GRATEï¼Œæå‡æœºå™¨äººè‡ªä¸»æ¢ç´¢çš„æ—¶é—´æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªä¸»æœºå™¨äººæ¢ç´¢` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `å›¾Transformer` `æ—¶é—´æ•ˆç‡` `è·¯å¾„è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªä¸»æ¢ç´¢æ–¹æ³•åœ¨å›¾ç»“æ„æ•°æ®æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œä¸”é€šå¸¸åªå…³æ³¨è·ç¦»ä¼˜åŒ–ï¼Œå¿½ç•¥äº†æ—¶é—´æ•ˆç‡ã€‚
2. GRATEåˆ©ç”¨å›¾Transformeræ•è·ä¿¡æ¯å›¾çš„å±€éƒ¨ç»“æ„å’Œå…¨å±€ä¾èµ–ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨æ•´ä¸ªç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢å¹³æ»‘è·¯å¾„ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGRATEåœ¨æ¢ç´¢æ•ˆç‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè·ç¦»ä¸Šæå‡é«˜è¾¾21.5%ï¼Œæ—¶é—´ä¸Šæå‡é«˜è¾¾21.3%ï¼Œå¹¶åœ¨çœŸå®åœºæ™¯ä¸­éªŒè¯äº†æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªä¸»æœºå™¨äººæ¢ç´¢ï¼ˆAREï¼‰æ˜¯æŒ‡æœºå™¨äººè‡ªä¸»å¯¼èˆªå¹¶ç»˜åˆ¶æœªçŸ¥ç¯å¢ƒåœ°å›¾çš„è¿‡ç¨‹ã€‚æœ€è¿‘åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•é€šå¸¸å°†AREå»ºæ¨¡ä¸ºåœ¨æ— ç¢°æ’ä¿¡æ¯å›¾ä¸Šå®šä¹‰çš„åºåˆ—å†³ç­–é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å›¾ç»“æ„åŒ–æ•°æ®ä¸Šçš„æ¨ç†èƒ½åŠ›æœ‰é™ã€‚æ­¤å¤–ï¼Œç”±äºå¯¹æœºå™¨äººè¿åŠ¨çš„è€ƒè™‘ä¸è¶³ï¼Œç”±æ­¤äº§ç”Ÿçš„RLç­–ç•¥é€šå¸¸è¢«ä¼˜åŒ–ä¸ºæœ€å°åŒ–è¡Œé©¶è·ç¦»ï¼Œè€Œå¿½ç•¥äº†æ—¶é—´æ•ˆç‡ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºGRATEï¼Œä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å›¾Transformeræ¥æœ‰æ•ˆåœ°æ•è·ä¿¡æ¯å›¾çš„å±€éƒ¨ç»“æ„æ¨¡å¼å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œä»è€Œå¢å¼ºæ¨¡å‹åœ¨æ•´ä¸ªç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éƒ¨ç½²å¡å°”æ›¼æ»¤æ³¢å™¨æ¥å¹³æ»‘èˆªè·¯ç‚¹è¾“å‡ºï¼Œç¡®ä¿ç”Ÿæˆçš„è·¯å¾„åœ¨è¿åŠ¨å­¦ä¸Šæ˜¯æœºå™¨äººå¯ä»¥éµå¾ªçš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§æ¨¡æ‹ŸåŸºå‡†ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å…ˆè¿›çš„ä¼ ç»Ÿå’ŒåŸºäºå­¦ä¹ çš„åŸºçº¿è¡¨ç°å‡ºæ›´å¥½çš„æ¢ç´¢æ•ˆç‡ï¼ˆåœ¨å®Œæˆæ¢ç´¢çš„è·ç¦»ä¸Šé«˜è¾¾21.5%ï¼Œæ—¶é—´ä¸Šé«˜è¾¾21.3%ï¼‰ã€‚æˆ‘ä»¬è¿˜åœ¨çœŸå®åœºæ™¯ä¸­éªŒè¯äº†æˆ‘ä»¬çš„è§„åˆ’å™¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè‡ªä¸»æœºå™¨äººæ¢ç´¢ï¼ˆAREï¼‰æ—¨åœ¨ä½¿æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­è‡ªä¸»å¯¼èˆªå¹¶æ„å»ºåœ°å›¾ã€‚ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•é€šå¸¸å°†æ­¤é—®é¢˜å»ºæ¨¡ä¸ºåœ¨ä¿¡æ¯å›¾ä¸Šçš„åºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¤„ç†å›¾ç»“æ„æ•°æ®æ—¶æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œå¹¶ä¸”å¾€å¾€åªå…³æ³¨è·¯å¾„è·ç¦»çš„ä¼˜åŒ–ï¼Œå¿½ç•¥äº†æ—¶é—´æ•ˆç‡ï¼Œå¯¼è‡´æ¢ç´¢é€Ÿåº¦æ…¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGRATEçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å›¾Transformeræ¥å¢å¼ºæ¨¡å‹å¯¹ç¯å¢ƒçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶è€ƒè™‘æœºå™¨äººè¿åŠ¨å­¦çº¦æŸä»¥æé«˜æ—¶é—´æ•ˆç‡ã€‚å›¾Transformerèƒ½å¤Ÿæœ‰æ•ˆæ•è·ä¿¡æ¯å›¾çš„å±€éƒ¨ç»“æ„å’Œå…¨å±€ä¾èµ–å…³ç³»ï¼Œä»è€Œåšå‡ºæ›´æ˜æ™ºçš„æ¢ç´¢å†³ç­–ã€‚å¡å°”æ›¼æ»¤æ³¢å™¨çš„å¼•å…¥åˆ™ä¿è¯äº†ç”Ÿæˆè·¯å¾„çš„å¹³æ»‘æ€§å’Œå¯è¡Œæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRATEçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä¿¡æ¯å›¾æ„å»ºæ¨¡å—ï¼Œç”¨äºå°†ç¯å¢ƒä¿¡æ¯è¡¨ç¤ºä¸ºå›¾ç»“æ„ï¼›2) åŸºäºå›¾Transformerçš„ç­–ç•¥ç½‘ç»œï¼Œç”¨äºå­¦ä¹ æ¢ç´¢ç­–ç•¥ï¼›3) å¡å°”æ›¼æ»¤æ³¢å™¨ï¼Œç”¨äºå¹³æ»‘è·¯å¾„ï¼›4) å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ªç¯ï¼Œç”¨äºä¼˜åŒ–ç­–ç•¥ç½‘ç»œã€‚æœºå™¨äººæ ¹æ®ç­–ç•¥ç½‘ç»œè¾“å‡ºçš„åŠ¨ä½œé€‰æ‹©ä¸‹ä¸€ä¸ªæ¢ç´¢ç›®æ ‡ï¼Œå¹¶ä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢å¹³æ»‘è·¯å¾„ï¼Œæœ€ç»ˆå®Œæˆç¯å¢ƒæ¢ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šGRATEçš„å…³é”®åˆ›æ–°åœ¨äºå°†å›¾Transformerå¼•å…¥åˆ°è‡ªä¸»æ¢ç´¢ä»»åŠ¡ä¸­ï¼Œå¹¶ç»“åˆå¡å°”æ›¼æ»¤æ³¢å™¨è¿›è¡Œè·¯å¾„å¹³æ»‘ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå›¾çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œå›¾Transformerèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å›¾çš„å…¨å±€ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¢ç´¢æ•ˆç‡ã€‚å¡å°”æ›¼æ»¤æ³¢å™¨çš„ä½¿ç”¨åˆ™ä¿è¯äº†ç”Ÿæˆè·¯å¾„çš„è¿åŠ¨å­¦å¯è¡Œæ€§ï¼Œè¿›ä¸€æ­¥æå‡äº†æ—¶é—´æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šGRATEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å›¾Transformerçš„ç½‘ç»œç»“æ„ï¼ŒåŒ…æ‹¬å±‚æ•°ã€æ³¨æ„åŠ›å¤´æ•°ç­‰ï¼›2) å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°ï¼Œéœ€è¦å¹³è¡¡æ¢ç´¢æ•ˆç‡å’Œå®‰å…¨æ€§ï¼›3) å¡å°”æ›¼æ»¤æ³¢å™¨çš„å‚æ•°è®¾ç½®ï¼Œéœ€è¦æ ¹æ®æœºå™¨äººçš„è¿åŠ¨å­¦ç‰¹æ€§è¿›è¡Œè°ƒæ•´ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰æ›´è¯¦ç»†çš„æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGRATEåœ¨å¤šç§æ¨¡æ‹Ÿç¯å¢ƒä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æ¢ç´¢è·ç¦»ä¸Šæå‡é«˜è¾¾21.5%ï¼Œåœ¨æ¢ç´¢æ—¶é—´ä¸Šæå‡é«˜è¾¾21.3%ã€‚æ­¤å¤–ï¼ŒGRATEè¿˜åœ¨çœŸå®åœºæ™¯ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜GRATEæ˜¯ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„è‡ªä¸»æ¢ç´¢æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GRATEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºä»“åº“å·¡æ£€ã€ç¾éš¾æ•‘æ´ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜æœºå™¨äººè‡ªä¸»æ¢ç´¢çš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼ŒGRATEå¯ä»¥é™ä½äººå·¥å¹²é¢„çš„éœ€æ±‚ï¼Œæå‡å·¥ä½œæ•ˆç‡ï¼Œå¹¶åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´å¯é çš„è‡ªä¸»å¯¼èˆªã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous robot exploration (ARE) is the process of a robot autonomously navigating and mapping an unknown environment. Recent Reinforcement Learning (RL)-based approaches typically formulate ARE as a sequential decision-making problem defined on a collision-free informative graph. However, these methods often demonstrate limited reasoning ability over graph-structured data. Moreover, due to the insufficient consideration of robot motion, the resulting RL policies are generally optimized to minimize travel distance, while neglecting time efficiency. To overcome these limitations, we propose GRATE, a Deep Reinforcement Learning (DRL)-based approach that leverages a Graph Transformer to effectively capture both local structure patterns and global contextual dependencies of the informative graph, thereby enhancing the model's reasoning capability across the entire environment. In addition, we deploy a Kalman filter to smooth the waypoint outputs, ensuring that the resulting path is kinodynamically feasible for the robot to follow. Experimental results demonstrate that our method exhibits better exploration efficiency (up to 21.5% in distance and 21.3% in time to complete exploration) than state-of-the-art conventional and learning-based baselines in various simulation benchmarks. We also validate our planner in real-world scenarios.

