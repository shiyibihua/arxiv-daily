---
layout: default
title: ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation
---

# ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.12618" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.12618v1</a>
  <a href="https://arxiv.org/pdf/2509.12618.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.12618v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.12618v1', 'ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zekai Zhang, Weiye Zhu, Hewei Pan, Xiangchen Wang, Rongtao Xu, Xing Sun, Feng Zheng

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ActiveVLNï¼šåŸºäºå¤šè½®å¼ºåŒ–å­¦ä¹ çš„ä¸»åŠ¨æ¢ç´¢è§†è§‰è¯­è¨€å¯¼èˆª**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `å¼ºåŒ–å­¦ä¹ ` `ä¸»åŠ¨æ¢ç´¢` `æ¨¡ä»¿å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLNæ–¹æ³•ä¾èµ–æ¨¡ä»¿å­¦ä¹ å’ŒDAggerï¼Œæ•°æ®æ”¶é›†å’Œè®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œä¸”ç¼ºä¹ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’ã€‚
2. ActiveVLNé€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ å®ç°ä¸»åŠ¨æ¢ç´¢ï¼Œå…è®¸æ™ºèƒ½ä½“è‡ªä¸»æ”¶é›†å¤šæ ·åŒ–è½¨è¿¹å¹¶ä¼˜åŒ–å¯¼èˆªç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒActiveVLNåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨è¾ƒå°æ¨¡å‹è¾¾åˆ°äº†ä¸SOTAæ–¹æ³•ç›¸å½“çš„æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€å¯¼èˆª(VLN)ä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨å¤æ‚ç¯å¢ƒä¸­å¯¼èˆªã€‚ç°æœ‰çš„åŸºäºMLLMçš„VLNæ–¹æ³•ä¸»è¦ä¾èµ–äºæ¨¡ä»¿å­¦ä¹ (IL)ï¼Œå¹¶ç»å¸¸ä½¿ç”¨DAggerè¿›è¡Œåè®­ç»ƒä»¥å‡è½»åå˜é‡åç§»ã€‚è™½ç„¶è¿™äº›æ–¹æ³•æœ‰æ•ˆï¼Œä½†ä¼šäº§ç”Ÿå¤§é‡çš„æ•°æ®æ”¶é›†å’Œè®­ç»ƒæˆæœ¬ã€‚å¼ºåŒ–å­¦ä¹ (RL)æä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå…ˆå‰çš„VLN RLæ–¹æ³•ç¼ºä¹ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’ï¼Œå¹¶ä¸”ä¾èµ–äºä¸“å®¶è½¨è¿¹è¿›è¡Œå¥–åŠ±å¡‘é€ ï¼Œè€Œä¸æ˜¯è¿›è¡Œå¼€æ”¾å¼çš„ä¸»åŠ¨æ¢ç´¢ã€‚è¿™é™åˆ¶äº†æ™ºèƒ½ä½“å‘ç°å¤šæ ·åŒ–å’Œåˆç†çš„å¯¼èˆªè·¯çº¿çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ActiveVLNï¼Œä¸€ä¸ªVLNæ¡†æ¶ï¼Œé€šè¿‡å¤šè½®RLæ˜¾å¼åœ°å®ç°ä¸»åŠ¨æ¢ç´¢ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨ä¸€å°éƒ¨åˆ†ä¸“å®¶è½¨è¿¹è¿›è¡ŒILæ¥å¼•å¯¼æ™ºèƒ½ä½“ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ™ºèƒ½ä½“è¿­ä»£åœ°é¢„æµ‹å’Œæ‰§è¡ŒåŠ¨ä½œï¼Œè‡ªåŠ¨æ”¶é›†å¤šæ ·åŒ–çš„è½¨è¿¹ï¼Œå¹¶é€šè¿‡GRPOç›®æ ‡ä¼˜åŒ–å¤šä¸ªrolloutã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜RLæ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€æå‰åœæ­¢ç­–ç•¥æ¥ä¿®å‰ªé•¿å°¾æˆ–å¯èƒ½å¤±è´¥çš„è½¨è¿¹ï¼Œä»¥åŠé¢å¤–çš„å·¥ç¨‹ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºDAggerå’Œå…ˆå‰çš„åŸºäºRLçš„åè®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒActiveVLNåœ¨ILåŸºçº¿ä¸Šå®ç°äº†æœ€å¤§çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶åœ¨ä½¿ç”¨è¾ƒå°æ¨¡å‹çš„æƒ…å†µä¸‹è¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®å³å°†å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ï¼ˆMLLMï¼‰ï¼Œä¸»è¦ä¾èµ–äºæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ï¼Œå¹¶ä½¿ç”¨DAggerè¿›è¡Œåè®­ç»ƒä»¥è§£å†³åå˜é‡åç§»é—®é¢˜ã€‚è¿™äº›æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†éœ€è¦å¤§é‡çš„ä¸“å®¶æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ç¼ºä¹ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’ï¼Œä¾èµ–ä¸“å®¶è½¨è¿¹è¿›è¡Œå¥–åŠ±å¡‘é€ ï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“æ¢ç´¢å¤šæ ·åŒ–å¯¼èˆªè·¯çº¿çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šActiveVLNçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°ä¸»åŠ¨æ¢ç´¢ã€‚æ™ºèƒ½ä½“ä¸å†ä»…ä»…æ¨¡ä»¿ä¸“å®¶è½¨è¿¹ï¼Œè€Œæ˜¯é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œè‡ªä¸»åœ°å‘ç°å’Œå­¦ä¹ æ›´ä¼˜çš„å¯¼èˆªç­–ç•¥ã€‚é€šè¿‡è¿­ä»£åœ°é¢„æµ‹å’Œæ‰§è¡ŒåŠ¨ä½œï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªåŠ¨æ”¶é›†å¤šæ ·åŒ–çš„è½¨è¿¹ï¼Œå¹¶åˆ©ç”¨è¿™äº›è½¨è¿¹è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šActiveVLNæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨å°‘é‡ä¸“å®¶è½¨è¿¹è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼Œä»¥å¼•å¯¼æ™ºèƒ½ä½“åˆæ­¥å­¦ä¹ å¯¼èˆªç­–ç•¥ã€‚ç¬¬äºŒé˜¶æ®µï¼Œæ™ºèƒ½ä½“è¿›å…¥ä¸»åŠ¨æ¢ç´¢é˜¶æ®µï¼Œè¿­ä»£åœ°æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š1) æ™ºèƒ½ä½“æ ¹æ®å½“å‰ç­–ç•¥é¢„æµ‹å¹¶æ‰§è¡ŒåŠ¨ä½œï¼›2) æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†è½¨è¿¹æ•°æ®ï¼›3) ä½¿ç”¨æ”¶é›†åˆ°çš„è½¨è¿¹æ•°æ®ï¼Œé€šè¿‡GRPOç›®æ ‡ä¼˜åŒ–ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜å¼•å…¥äº†åŠ¨æ€æå‰åœæ­¢ç­–ç•¥ï¼Œä»¥æé«˜RLçš„æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šActiveVLNæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶ä¸»åŠ¨æ¢ç´¢æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–ä¸“å®¶è½¨è¿¹çš„VLNæ–¹æ³•ä¸åŒï¼ŒActiveVLNå…è®¸æ™ºèƒ½ä½“è‡ªä¸»åœ°ä¸ç¯å¢ƒäº¤äº’ï¼Œå‘ç°å¤šæ ·åŒ–çš„å¯¼èˆªè·¯çº¿ã€‚è¿™ç§ä¸»åŠ¨æ¢ç´¢æœºåˆ¶ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´é²æ£’ã€æ›´é€‚åº”ç¯å¢ƒå˜åŒ–çš„å¯¼èˆªç­–ç•¥ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€æå‰åœæ­¢ç­–ç•¥ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘ä¸å¿…è¦çš„è®¡ç®—ï¼Œæé«˜RLçš„æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šActiveVLNçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨GRPOï¼ˆGeneralized Policy Optimizationï¼‰ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„å¯¼èˆªç­–ç•¥ï¼›2) å¼•å…¥åŠ¨æ€æå‰åœæ­¢ç­–ç•¥ï¼Œæ ¹æ®è½¨è¿¹çš„æ—©æœŸè¡¨ç°ï¼Œåˆ¤æ–­æ˜¯å¦æå‰ç»ˆæ­¢è¯¥è½¨è¿¹çš„æ¢ç´¢ï¼Œä»è€ŒèŠ‚çœè®¡ç®—èµ„æºï¼›3) ä½¿ç”¨å°‘é‡ä¸“å®¶è½¨è¿¹è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼Œä»¥å¼•å¯¼æ™ºèƒ½ä½“åˆæ­¥å­¦ä¹ å¯¼èˆªç­–ç•¥ï¼Œé¿å…ä»é›¶å¼€å§‹æ¢ç´¢ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ActiveVLNåœ¨å®éªŒä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸åŸºäºDAggerå’Œå…ˆå‰çš„åŸºäºRLçš„åè®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒActiveVLNåœ¨ILåŸºçº¿ä¸Šå®ç°äº†æœ€å¤§çš„æ€§èƒ½æå‡ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒActiveVLNåœ¨ä½¿ç”¨è¾ƒå°æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ä¸»åŠ¨æ¢ç´¢å’Œå¤šè½®å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥æ˜¾è‘—æé«˜VLNä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ActiveVLNçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥ä½¿æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­è‡ªä¸»æ¢ç´¢å¹¶å®Œæˆå¯¼èˆªä»»åŠ¡ã€‚åœ¨è™šæ‹Ÿç°å®ä¸­ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„å¯¼èˆªä½“éªŒã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå¯ä»¥æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Vision-and-Language Navigation (VLN) task requires an agent to follow natural language instructions and navigate through complex environments. Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and often use DAgger for post-training to mitigate covariate shift. While effective, these approaches incur substantial data collection and training costs. Reinforcement learning (RL) offers a promising alternative. However, prior VLN RL methods lack dynamic interaction with the environment and depend on expert trajectories for reward shaping, rather than engaging in open-ended active exploration. This restricts the agent's ability to discover diverse and plausible navigation routes. To address these limitations, we propose ActiveVLN, a VLN framework that explicitly enables active exploration through multi-turn RL. In the first stage, a small fraction of expert trajectories is used for IL to bootstrap the agent. In the second stage, the agent iteratively predicts and executes actions, automatically collects diverse trajectories, and optimizes multiple rollouts via the GRPO objective. To further improve RL efficiency, we introduce a dynamic early-stopping strategy to prune long-tail or likely failed trajectories, along with additional engineering optimizations. Experiments show that ActiveVLN achieves the largest performance gains over IL baselines compared to both DAgger-based and prior RL-based post-training methods, while reaching competitive performance with state-of-the-art approaches despite using a smaller model. Code and data will be released soon.

