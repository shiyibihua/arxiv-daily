---
layout: default
title: APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots
---

# APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots

**arXiv**: [2511.09091v2](https://arxiv.org/abs/2511.09091) | [PDF](https://arxiv.org/pdf/2511.09091.pdf)

**ä½œè€…**: Shivam Sood, Laukik Nakhwa, Sun Ge, Yuhong Cao, Jin Cheng, Fatemah Zargarbashi, Taerim Yoon, Sungjoon Choi, Stelian Coros, Guillaume Sartoretti

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-12 (æ›´æ–°: 2025-11-19)

**å¤‡æ³¨**: This work was intended as a replacement of arXiv:2505.10022 and any subsequent updates will appear there

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://marmotlab.github.io/APEX/)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**APEXï¼šåˆ©ç”¨åŠ¨ä½œå…ˆéªŒå®žçŽ°è…¿å¼æœºå™¨äººç¨³å¥è¿åŠ¨è·Ÿè¸ªçš„é«˜æ•ˆæŽ¢ç´¢**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è…¿å¼æœºå™¨äºº` `è¿åŠ¨è·Ÿè¸ª` `å¼ºåŒ–å­¦ä¹ ` `åŠ¨ä½œå…ˆéªŒ` `ä¸“å®¶æ¼”ç¤º` `å¤šè¯„è®ºå®¶` `æœºå™¨äººæŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è…¿å¼æœºå™¨äººè¿åŠ¨è·Ÿè¸ªæ–¹æ³•ä¾èµ–å¤§é‡è°ƒå‚å’Œå‚è€ƒæ•°æ®ï¼Œé™åˆ¶äº†å…¶é€‚åº”æ€§ã€‚
2. APEXé€šè¿‡è¡°å‡åŠ¨ä½œå…ˆéªŒå¼•å¯¼å¼ºåŒ–å­¦ä¹ æŽ¢ç´¢ï¼Œå¹¶ç»“åˆå¤šè¯„è®ºå®¶æ¡†æž¶å¹³è¡¡æ€§èƒ½ä¸Žé£Žæ ¼ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒAPEXæé«˜äº†æ ·æœ¬æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€å‚è€ƒæ•°æ®å³å¯å®žçŽ°ç¨³å¥çš„è¿åŠ¨è·Ÿè¸ªã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºAPEXï¼ˆåŠ¨ä½œå…ˆéªŒå®žçŽ°é«˜æ•ˆæŽ¢ç´¢ï¼‰ï¼Œä¸€ç§å³æ’å³ç”¨çš„æ‰©å±•æ–¹æ³•ï¼Œç”¨äºŽæå‡çŽ°æœ‰è¿åŠ¨è·Ÿè¸ªç®—æ³•çš„æ€§èƒ½ã€‚APEXæ— éœ€éƒ¨ç½²æœŸé—´çš„å‚è€ƒæ•°æ®ï¼Œæé«˜äº†æ ·æœ¬æ•ˆçŽ‡ï¼Œå¹¶å‡å°‘äº†å‚æ•°è°ƒæ•´å·¥ä½œã€‚APEXé€šè¿‡ç»“åˆè¡°å‡åŠ¨ä½œå…ˆéªŒå°†ä¸“å®¶æ¼”ç¤ºç›´æŽ¥æ•´åˆåˆ°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œæœ€åˆå°†æŽ¢ç´¢åå‘äºŽä¸“å®¶æ¼”ç¤ºï¼Œä½†é€æ¸å…è®¸ç­–ç•¥ç‹¬ç«‹æŽ¢ç´¢ã€‚ç»“åˆå¤šè¯„è®ºå®¶æ¡†æž¶ï¼Œå¹³è¡¡äº†ä»»åŠ¡æ€§èƒ½å’Œè¿åŠ¨é£Žæ ¼ã€‚æ­¤å¤–ï¼ŒAPEXä½¿å•ä¸ªç­–ç•¥èƒ½å¤Ÿå­¦ä¹ å¤šæ ·åŒ–çš„è¿åŠ¨ï¼Œå¹¶åœ¨ä¸åŒåœ°å½¢å’Œé€Ÿåº¦ä¸‹è¿ç§»ç±»ä¼¼å‚è€ƒçš„é£Žæ ¼ï¼ŒåŒæ—¶å¯¹å¥–åŠ±è®¾è®¡çš„å˜åŒ–ä¿æŒé²æ£’æ€§ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿå’ŒUnitree Go2æœºå™¨äººä¸Šçš„å¤§é‡å®žéªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚APEXåˆ©ç”¨æ¼”ç¤ºæ¥æŒ‡å¯¼RLè®­ç»ƒæœŸé—´çš„æŽ¢ç´¢ï¼Œè€Œæ— éœ€å¯¹å®ƒä»¬æ–½åŠ æ˜Žç¡®çš„åå·®ï¼Œä»Žè€Œä½¿è…¿å¼æœºå™¨äººèƒ½å¤Ÿä»¥æ›´é«˜çš„ç¨³å®šæ€§ã€æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›è¿›è¡Œå­¦ä¹ ã€‚è¯¥æ–¹æ³•ä¸ºæŒ‡å¯¼é©±åŠ¨çš„RLé“ºå¹³äº†é“è·¯ï¼Œä»¥ä¿ƒè¿›ä»Žè¿åŠ¨åˆ°æ“ä½œçš„å„ç§æœºå™¨äººä»»åŠ¡ä¸­è‡ªç„¶æŠ€èƒ½çš„èŽ·å–ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è…¿å¼æœºå™¨äººè¿åŠ¨è·Ÿè¸ªæ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„å‚æ•°è°ƒæ•´ï¼Œå¹¶ä¸”åœ¨éƒ¨ç½²æ—¶ä¾èµ–å‚è€ƒæ•°æ®ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒçŽ¯å¢ƒå’Œä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•æé«˜è¿åŠ¨è·Ÿè¸ªç®—æ³•çš„æ ·æœ¬æ•ˆçŽ‡ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘å¯¹å‚è€ƒæ•°æ®çš„ä¾èµ–ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAPEXçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä¸“å®¶æ¼”ç¤ºçŸ¥è¯†èžå…¥åˆ°å¼ºåŒ–å­¦ä¹ çš„æŽ¢ç´¢è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡åŠ¨ä½œå…ˆéªŒå¼•å¯¼æ™ºèƒ½ä½“åˆæœŸæŽ¢ç´¢ï¼Œå¹¶éšç€è®­ç»ƒçš„è¿›è¡Œé€æ¸å‡å¼±å…ˆéªŒçš„å½±å“ï¼Œå…è®¸æ™ºèƒ½ä½“è‡ªä¸»æŽ¢ç´¢ã€‚è¿™ç§æ–¹å¼æ—¢èƒ½åˆ©ç”¨ä¸“å®¶çŸ¥è¯†åŠ é€Ÿå­¦ä¹ ï¼Œåˆèƒ½é¿å…è¿‡åº¦ä¾èµ–ä¸“å®¶çŸ¥è¯†è€Œé™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚åŒæ—¶ï¼Œé‡‡ç”¨å¤šè¯„è®ºå®¶æ¡†æž¶æ¥å¹³è¡¡ä»»åŠ¡æ€§èƒ½å’Œè¿åŠ¨é£Žæ ¼ï¼Œä»Žè€ŒèŽ·å¾—æ›´è‡ªç„¶å’Œé²æ£’çš„è¿åŠ¨ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šAPEXçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŠ¨ä½œå…ˆéªŒæ¨¡å—ï¼šåˆ©ç”¨ä¸“å®¶æ¼”ç¤ºæ•°æ®æž„å»ºåŠ¨ä½œå…ˆéªŒï¼Œå¹¶åœ¨è®­ç»ƒåˆæœŸå¼•å¯¼æ™ºèƒ½ä½“çš„æŽ¢ç´¢æ–¹å‘ã€‚2) å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼šé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOï¼‰è®­ç»ƒè¿åŠ¨æŽ§åˆ¶ç­–ç•¥ã€‚3) å¤šè¯„è®ºå®¶æ¨¡å—ï¼šä½¿ç”¨å¤šä¸ªè¯„è®ºå®¶ç½‘ç»œåˆ†åˆ«è¯„ä¼°ä»»åŠ¡æ€§èƒ½å’Œè¿åŠ¨é£Žæ ¼ï¼Œå¹¶ç»“åˆè¿™äº›è¯„ä¼°ç»“æžœæ¥ä¼˜åŒ–ç­–ç•¥ã€‚4) è¡°å‡æœºåˆ¶ï¼šéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ¸å‡å¼±åŠ¨ä½œå…ˆéªŒçš„å½±å“ï¼Œå…è®¸æ™ºèƒ½ä½“è‡ªä¸»æŽ¢ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šAPEXçš„å…³é”®åˆ›æ–°åœ¨äºŽå°†åŠ¨ä½œå…ˆéªŒä¸Žå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è¡°å‡æœºåˆ¶ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨åˆ©ç”¨ä¸“å®¶çŸ¥è¯†çš„åŒæ—¶ï¼Œé€æ¸æ‘†è„±å¯¹ä¸“å®¶çŸ¥è¯†çš„ä¾èµ–ï¼Œä»Žè€Œå®žçŽ°æ›´é«˜æ•ˆå’Œé²æ£’çš„æŽ¢ç´¢ã€‚æ­¤å¤–ï¼Œå¤šè¯„è®ºå®¶æ¡†æž¶çš„è®¾è®¡ä¹Ÿä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤ŸåŒæ—¶ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½å’Œè¿åŠ¨é£Žæ ¼ï¼Œä»Žè€ŒèŽ·å¾—æ›´è‡ªç„¶å’Œé€¼çœŸçš„è¿åŠ¨æ•ˆæžœã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨ä½œå…ˆéªŒé€šå¸¸ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒå»ºæ¨¡ï¼Œå…¶å‡å€¼å’Œæ–¹å·®ä»Žä¸“å®¶æ¼”ç¤ºæ•°æ®ä¸­ä¼°è®¡å¾—åˆ°ã€‚è¡°å‡æœºåˆ¶é€šå¸¸é‡‡ç”¨çº¿æ€§æˆ–æŒ‡æ•°è¡°å‡å‡½æ•°ï¼ŒæŽ§åˆ¶åŠ¨ä½œå…ˆéªŒçš„å½±å“ç¨‹åº¦ã€‚å¤šè¯„è®ºå®¶æ¡†æž¶ä¸­çš„æ¯ä¸ªè¯„è®ºå®¶ç½‘ç»œéƒ½ç‹¬ç«‹è¯„ä¼°ç­–ç•¥çš„æ€§èƒ½ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„å¥–åŠ±å‡½æ•°æ¥è¡¡é‡ä»»åŠ¡æ€§èƒ½å’Œè¿åŠ¨é£Žæ ¼ã€‚æŸå¤±å‡½æ•°é€šå¸¸åŒ…æ‹¬ä»»åŠ¡å¥–åŠ±ã€é£Žæ ¼å¥–åŠ±å’Œç­–ç•¥æ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºŽä¼˜åŒ–ç­–ç•¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

APEXåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººï¼ˆUnitree Go2ï¼‰ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒAPEXæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆçŽ‡ï¼Œå‡å°‘äº†å¯¹å‚è€ƒæ•°æ®çš„ä¾èµ–ï¼Œå¹¶å®žçŽ°äº†æ›´é²æ£’çš„è¿åŠ¨è·Ÿè¸ªã€‚ä¾‹å¦‚ï¼ŒAPEXèƒ½å¤Ÿåœ¨ä¸åŒåœ°å½¢å’Œé€Ÿåº¦ä¸‹è¿ç§»è¿åŠ¨é£Žæ ¼ï¼Œå¹¶ä¸”å¯¹å¥–åŠ±å‡½æ•°çš„å˜åŒ–å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAPEXåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨çŽ°å‡ºæ›´é«˜çš„ç¨³å®šæ€§å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

APEXæŠ€æœ¯å¯å¹¿æ³›åº”ç”¨äºŽå„ç§è…¿å¼æœºå™¨äººåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æœæ•‘ã€å·¡æ£€ã€ç‰©æµå’Œå®¶åº­æœåŠ¡ç­‰ã€‚é€šè¿‡å­¦ä¹ è‡ªç„¶ã€ç±»åŠ¨ç‰©çš„è¿åŠ¨æ–¹å¼ï¼Œæœºå™¨äººå¯ä»¥åœ¨å¤æ‚åœ°å½¢å’ŒåŠ¨æ€çŽ¯å¢ƒä¸­æ›´é«˜æ•ˆã€æ›´å®‰å…¨åœ°æ‰§è¡Œä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æŽ¨å¹¿åˆ°å…¶ä»–æœºå™¨äººä»»åŠ¡ï¼Œå¦‚æ“ä½œå’ŒæŠ“å–ï¼Œä»Žè€Œæé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: https://marmotlab.github.io/APEX/.

