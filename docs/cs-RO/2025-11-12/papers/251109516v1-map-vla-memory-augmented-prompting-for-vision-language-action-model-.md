---
layout: default
title: MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation
---

# MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation

**arXiv**: [2511.09516v1](https://arxiv.org/abs/2511.09516) | [PDF](https://arxiv.org/pdf/2511.09516.pdf)

**ä½œè€…**: Runhao Li, Wenkai Guo, Zhenyu Wu, Changyuan Wang, Haoyuan Deng, Zhenyu Weng, Yap-Peng Tan, Ziwei Wang

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-12

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MAP-VLAï¼šåˆ©ç”¨è®°å¿†å¢žå¼ºæç¤ºï¼Œæå‡VLAæ¨¡åž‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„é•¿æ—¶ä»»åŠ¡æ€§èƒ½**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `é•¿æ—¶ä»»åŠ¡` `è®°å¿†å¢žå¼º` `æç¤ºå­¦ä¹ ` `è½¨è¿¹ç›¸ä¼¼æ€§åŒ¹é…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åœ¨é•¿æ—¶æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œç”±äºŽç¼ºä¹è®°å¿†æœºåˆ¶ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨åŽ†å²ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. MAP-VLAé€šè¿‡æž„å»ºè®°å¿†åº“ï¼Œåˆ©ç”¨åŽ†å²æ¼”ç¤ºæ•°æ®ç”Ÿæˆå¯å­¦ä¹ çš„è½¯æç¤ºï¼Œå¢žå¼ºVLAæ¨¡åž‹çš„åŠ¨ä½œç”Ÿæˆèƒ½åŠ›ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMAP-VLAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººçŽ¯å¢ƒä¸­å‡æ˜¾è‘—æå‡äº†é•¿æ—¶ä»»åŠ¡çš„æ€§èƒ½ï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹åœ¨æé«˜ç«¯åˆ°ç«¯æœºå™¨äººæ“ä½œçš„é²æ£’æ€§å’Œæ³›åŒ–æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼Œç”±äºŽç¼ºä¹è®°å¿†ä¸”ä»…ä¾èµ–å³æ—¶æ„Ÿå®˜è¾“å…¥ï¼Œè¿™äº›æ¨¡åž‹åœ¨é•¿æ—¶ä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è®°å¿†å¢žå¼ºæç¤ºçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹(MAP-VLA)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æž¶ï¼Œå®ƒåˆ©ç”¨æ¥è‡ªåŽ†å²æ¼”ç¤ºçš„è®°å¿†æç¤ºæ¥å¢žå¼ºé¢„è®­ç»ƒVLAæ¨¡åž‹çš„åŠ¨ä½œç”Ÿæˆèƒ½åŠ›ï¼Œä»Žè€Œåº”å¯¹é•¿æ—¶æœºå™¨äººæ“ä½œä»»åŠ¡ã€‚ä¸ºæ­¤ï¼ŒMAP-VLAé¦–å…ˆä»ŽåŽ†å²æ¼”ç¤ºä¸­æž„å»ºä¸€ä¸ªè®°å¿†åº“ï¼Œå…¶ä¸­æ¯ä¸ªè®°å¿†å•å…ƒæ•èŽ·å…³äºŽä»»åŠ¡ç‰¹å®šé˜¶æ®µçš„ä¿¡æ¯ã€‚è¿™äº›è®°å¿†å•å…ƒè¢«å®žçŽ°ä¸ºé€šè¿‡æç¤ºè°ƒæ•´ä¼˜åŒ–çš„å¯å­¦ä¹ è½¯æç¤ºã€‚ç„¶åŽï¼Œåœ¨å®žæ—¶ä»»åŠ¡æ‰§è¡ŒæœŸé—´ï¼ŒMAP-VLAé€šè¿‡è½¨è¿¹ç›¸ä¼¼æ€§åŒ¹é…æ£€ç´¢ç›¸å…³è®°å¿†ï¼Œå¹¶å°†å…¶åŠ¨æ€é›†æˆåˆ°VLAæ¨¡åž‹ä¸­ï¼Œä»¥å¢žå¼ºåŠ¨ä½œç”Ÿæˆã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ç§æç¤ºè°ƒæ•´å’Œæ£€ç´¢å¢žå¼ºæ–¹æ³•ä½œä¸ºå†»ç»“VLAæ¨¡åž‹çš„å³æ’å³ç”¨æ¨¡å—è¿è¡Œï¼Œæä¾›äº†ä¸€ç§è½»é‡çº§ä¸”çµæ´»çš„è§£å†³æ–¹æ¡ˆæ¥æé«˜ä»»åŠ¡æ€§èƒ½ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMAP-VLAåœ¨æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†é«˜è¾¾7.0%çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œåœ¨çœŸå®žæœºå™¨äººè¯„ä¼°ä¸­å®žçŽ°äº†25.0%çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é¢„è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹åœ¨é•¿æ—¶æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³çš„é—®é¢˜ã€‚çŽ°æœ‰VLAæ¨¡åž‹ä¸»è¦ä¾èµ–äºŽå³æ—¶æ„Ÿå®˜è¾“å…¥ï¼Œç¼ºä¹å¯¹åŽ†å²ä¿¡æ¯çš„è®°å¿†å’Œåˆ©ç”¨èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨éœ€è¦é•¿æœŸè§„åˆ’å’Œå¤æ‚çŠ¶æ€è½¬ç§»çš„ä»»åŠ¡ä¸­æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åŽ†å²æ¼”ç¤ºæ•°æ®æž„å»ºè®°å¿†åº“ï¼Œå¹¶ä»Žä¸­æå–å…³é”®ä¿¡æ¯ä½œä¸ºæç¤ºï¼Œå¢žå¼ºVLAæ¨¡åž‹çš„åŠ¨ä½œç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å°†ç›¸å…³è®°å¿†åŠ¨æ€åœ°èžå…¥åˆ°VLAæ¨¡åž‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ä»»åŠ¡çŠ¶æ€ï¼Œå¹¶ç”Ÿæˆæ›´åˆç†çš„åŠ¨ä½œåºåˆ—ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMAP-VLAæ¡†æž¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼š1) è®°å¿†åº“æž„å»ºï¼šä»ŽåŽ†å²æ¼”ç¤ºæ•°æ®ä¸­æå–ä»»åŠ¡ç‰¹å®šé˜¶æ®µçš„ä¿¡æ¯ï¼Œæž„å»ºè®°å¿†å•å…ƒï¼Œå¹¶å°†å…¶è¡¨ç¤ºä¸ºå¯å­¦ä¹ çš„è½¯æç¤ºã€‚2) è®°å¿†æ£€ç´¢ï¼šåœ¨å®žæ—¶ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡è½¨è¿¹ç›¸ä¼¼æ€§åŒ¹é…ï¼Œä»Žè®°å¿†åº“ä¸­æ£€ç´¢ä¸Žå½“å‰çŠ¶æ€ç›¸å…³çš„è®°å¿†å•å…ƒã€‚3) åŠ¨ä½œç”Ÿæˆï¼šå°†æ£€ç´¢åˆ°çš„è®°å¿†å•å…ƒåŠ¨æ€åœ°é›†æˆåˆ°å†»ç»“çš„VLAæ¨¡åž‹ä¸­ï¼Œå¢žå¼ºå…¶åŠ¨ä½œç”Ÿæˆèƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šMAP-VLAçš„å…³é”®åˆ›æ–°åœ¨äºŽåˆ©ç”¨è®°å¿†å¢žå¼ºæç¤ºæ¥æå‡VLAæ¨¡åž‹åœ¨é•¿æ—¶ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMAP-VLAæ— éœ€é‡æ–°è®­ç»ƒæ•´ä¸ªVLAæ¨¡åž‹ï¼Œè€Œæ˜¯é€šè¿‡è½»é‡çº§çš„æç¤ºè°ƒæ•´å’Œæ£€ç´¢å¢žå¼ºï¼Œå³å¯æ˜¾è‘—æå‡ä»»åŠ¡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°†è®°å¿†å•å…ƒè¡¨ç¤ºä¸ºå¯å­¦ä¹ çš„è½¯æç¤ºï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ å’Œåˆ©ç”¨åŽ†å²ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®°å¿†åº“ä¸­çš„æ¯ä¸ªè®°å¿†å•å…ƒéƒ½å¯¹åº”äºŽä»»åŠ¡çš„ä¸€ä¸ªç‰¹å®šé˜¶æ®µï¼Œé€šè¿‡æç¤ºè°ƒæ•´è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æœ€å¤§ç¨‹åº¦åœ°æé«˜å…¶å¯¹åŠ¨ä½œç”Ÿæˆçš„è´¡çŒ®ã€‚è½¨è¿¹ç›¸ä¼¼æ€§åŒ¹é…é‡‡ç”¨åˆé€‚çš„è·ç¦»åº¦é‡ï¼Œä¾‹å¦‚åŠ¨æ€æ—¶é—´è§„æ•´(DTW)ï¼Œä»¥å‡†ç¡®æ£€ç´¢ç›¸å…³è®°å¿†ã€‚VLAæ¨¡åž‹é‡‡ç”¨å†»ç»“çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œé¿å…äº†é‡æ–°è®­ç»ƒçš„å¼€é”€ï¼Œå¹¶ä¿è¯äº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

MAP-VLAåœ¨æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†é«˜è¾¾7.0%çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œåœ¨çœŸå®žæœºå™¨äººè¯„ä¼°ä¸­å®žçŽ°äº†25.0%çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œæ˜¾è‘—ä¼˜äºŽå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒMAP-VLAèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨åŽ†å²ä¿¡æ¯ï¼Œæå‡VLAæ¨¡åž‹åœ¨é•¿æ—¶ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå…·æœ‰å¾ˆå¼ºçš„å®žç”¨ä»·å€¼ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

MAP-VLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºŽå„ç§éœ€è¦é•¿æœŸè§„åˆ’å’Œå¤æ‚çŠ¶æ€è½¬ç§»çš„æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚è£…é…ã€çƒ¹é¥ªã€æ¸…æ´ç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆå„ç§å®žé™…ä»»åŠ¡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.

