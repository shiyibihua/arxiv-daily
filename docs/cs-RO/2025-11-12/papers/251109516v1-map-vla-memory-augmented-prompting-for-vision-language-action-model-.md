---
layout: default
title: MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation
---

# MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.09516" target="_blank" class="toolbar-btn">arXiv: 2511.09516v1</a>
    <a href="https://arxiv.org/pdf/2511.09516.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.09516v1" 
            onclick="toggleFavorite(this, '2511.09516v1', 'MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Runhao Li, Wenkai Guo, Zhenyu Wu, Changyuan Wang, Haoyuan Deng, Zhenyu Weng, Yap-Peng Tan, Ziwei Wang

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-12

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**MAP-VLAÔºöÂà©Áî®ËÆ∞ÂøÜÂ¢ûÂº∫ÊèêÁ§∫ÔºåÊèêÂçáVLAÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÈïøÊó∂‰ªªÂä°ÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `ÈïøÊó∂‰ªªÂä°` `ËÆ∞ÂøÜÂ¢ûÂº∫` `ÊèêÁ§∫Â≠¶‰π†` `ËΩ®ËøπÁõ∏‰ººÊÄßÂåπÈÖç`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®ÈïøÊó∂Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÔºåÁî±‰∫éÁº∫‰πèËÆ∞ÂøÜÊú∫Âà∂ÔºåÈöæ‰ª•ÊúâÊïàÂà©Áî®ÂéÜÂè≤‰ø°ÊÅØÔºåÂØºËá¥ÊÄßËÉΩÂèóÈôê„ÄÇ
2. MAP-VLAÈÄöËøáÊûÑÂª∫ËÆ∞ÂøÜÂ∫ìÔºåÂà©Áî®ÂéÜÂè≤ÊºîÁ§∫Êï∞ÊçÆÁîüÊàêÂèØÂ≠¶‰π†ÁöÑËΩØÊèêÁ§∫ÔºåÂ¢ûÂº∫VLAÊ®°ÂûãÁöÑÂä®‰ΩúÁîüÊàêËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMAP-VLAÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÂùáÊòæËëóÊèêÂçá‰∫ÜÈïøÊó∂‰ªªÂä°ÁöÑÊÄßËÉΩÔºå‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®ÊèêÈ´òÁ´ØÂà∞Á´ØÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñÊÄßÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäü„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÁº∫‰πèËÆ∞ÂøÜ‰∏î‰ªÖ‰æùËµñÂç≥Êó∂ÊÑüÂÆòËæìÂÖ•ÔºåËøô‰∫õÊ®°ÂûãÂú®ÈïøÊó∂‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÂ±ÄÈôêÊÄßÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËÆ∞ÂøÜÂ¢ûÂº∫ÊèêÁ§∫ÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã(MAP-VLA)ÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂÆÉÂà©Áî®Êù•Ëá™ÂéÜÂè≤ÊºîÁ§∫ÁöÑËÆ∞ÂøÜÊèêÁ§∫Êù•Â¢ûÂº∫È¢ÑËÆ≠ÁªÉVLAÊ®°ÂûãÁöÑÂä®‰ΩúÁîüÊàêËÉΩÂäõÔºå‰ªéËÄåÂ∫îÂØπÈïøÊó∂Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°„ÄÇ‰∏∫Ê≠§ÔºåMAP-VLAÈ¶ñÂÖà‰ªéÂéÜÂè≤ÊºîÁ§∫‰∏≠ÊûÑÂª∫‰∏Ä‰∏™ËÆ∞ÂøÜÂ∫ìÔºåÂÖ∂‰∏≠ÊØè‰∏™ËÆ∞ÂøÜÂçïÂÖÉÊçïËé∑ÂÖ≥‰∫é‰ªªÂä°ÁâπÂÆöÈò∂ÊÆµÁöÑ‰ø°ÊÅØ„ÄÇËøô‰∫õËÆ∞ÂøÜÂçïÂÖÉË¢´ÂÆûÁé∞‰∏∫ÈÄöËøáÊèêÁ§∫Ë∞ÉÊï¥‰ºòÂåñÁöÑÂèØÂ≠¶‰π†ËΩØÊèêÁ§∫„ÄÇÁÑ∂ÂêéÔºåÂú®ÂÆûÊó∂‰ªªÂä°ÊâßË°åÊúüÈó¥ÔºåMAP-VLAÈÄöËøáËΩ®ËøπÁõ∏‰ººÊÄßÂåπÈÖçÊ£ÄÁ¥¢Áõ∏ÂÖ≥ËÆ∞ÂøÜÔºåÂπ∂Â∞ÜÂÖ∂Âä®ÊÄÅÈõÜÊàêÂà∞VLAÊ®°Âûã‰∏≠Ôºå‰ª•Â¢ûÂº∫Âä®‰ΩúÁîüÊàê„ÄÇÈáçË¶ÅÁöÑÊòØÔºåËøôÁßçÊèêÁ§∫Ë∞ÉÊï¥ÂíåÊ£ÄÁ¥¢Â¢ûÂº∫ÊñπÊ≥ï‰Ωú‰∏∫ÂÜªÁªìVLAÊ®°ÂûãÁöÑÂç≥ÊèíÂç≥Áî®Ê®°ÂùóËøêË°åÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ß‰∏îÁÅµÊ¥ªÁöÑËß£ÂÜ≥ÊñπÊ°àÊù•ÊèêÈ´ò‰ªªÂä°ÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMAP-VLAÂú®Ê®°ÊãüÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÈ´òËææ7.0%ÁöÑÁªùÂØπÊÄßËÉΩÊèêÂçáÔºåÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ËØÑ‰º∞‰∏≠ÂÆûÁé∞‰∫Ü25.0%ÁöÑÁªùÂØπÊÄßËÉΩÊèêÂçáÔºåË∂ÖËøá‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥È¢ÑËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®ÈïøÊó∂Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâVLAÊ®°Âûã‰∏ªË¶Å‰æùËµñ‰∫éÂç≥Êó∂ÊÑüÂÆòËæìÂÖ•ÔºåÁº∫‰πèÂØπÂéÜÂè≤‰ø°ÊÅØÁöÑËÆ∞ÂøÜÂíåÂà©Áî®ËÉΩÂäõÔºåÂØºËá¥Âú®ÈúÄË¶ÅÈïøÊúüËßÑÂàíÂíåÂ§çÊùÇÁä∂ÊÄÅËΩ¨ÁßªÁöÑ‰ªªÂä°‰∏≠ÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂéÜÂè≤ÊºîÁ§∫Êï∞ÊçÆÊûÑÂª∫ËÆ∞ÂøÜÂ∫ìÔºåÂπ∂‰ªé‰∏≠ÊèêÂèñÂÖ≥ÈîÆ‰ø°ÊÅØ‰Ωú‰∏∫ÊèêÁ§∫ÔºåÂ¢ûÂº∫VLAÊ®°ÂûãÁöÑÂä®‰ΩúÁîüÊàêËÉΩÂäõ„ÄÇÈÄöËøáÂ∞ÜÁõ∏ÂÖ≥ËÆ∞ÂøÜÂä®ÊÄÅÂú∞ËûçÂÖ•Âà∞VLAÊ®°Âûã‰∏≠Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£‰ªªÂä°Áä∂ÊÄÅÔºåÂπ∂ÁîüÊàêÊõ¥ÂêàÁêÜÁöÑÂä®‰ΩúÂ∫èÂàó„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMAP-VLAÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™Ê®°ÂùóÔºö1) ËÆ∞ÂøÜÂ∫ìÊûÑÂª∫Ôºö‰ªéÂéÜÂè≤ÊºîÁ§∫Êï∞ÊçÆ‰∏≠ÊèêÂèñ‰ªªÂä°ÁâπÂÆöÈò∂ÊÆµÁöÑ‰ø°ÊÅØÔºåÊûÑÂª∫ËÆ∞ÂøÜÂçïÂÖÉÔºåÂπ∂Â∞ÜÂÖ∂Ë°®Á§∫‰∏∫ÂèØÂ≠¶‰π†ÁöÑËΩØÊèêÁ§∫„ÄÇ2) ËÆ∞ÂøÜÊ£ÄÁ¥¢ÔºöÂú®ÂÆûÊó∂‰ªªÂä°ÊâßË°åËøáÁ®ã‰∏≠ÔºåÈÄöËøáËΩ®ËøπÁõ∏‰ººÊÄßÂåπÈÖçÔºå‰ªéËÆ∞ÂøÜÂ∫ì‰∏≠Ê£ÄÁ¥¢‰∏éÂΩìÂâçÁä∂ÊÄÅÁõ∏ÂÖ≥ÁöÑËÆ∞ÂøÜÂçïÂÖÉ„ÄÇ3) Âä®‰ΩúÁîüÊàêÔºöÂ∞ÜÊ£ÄÁ¥¢Âà∞ÁöÑËÆ∞ÂøÜÂçïÂÖÉÂä®ÊÄÅÂú∞ÈõÜÊàêÂà∞ÂÜªÁªìÁöÑVLAÊ®°Âûã‰∏≠ÔºåÂ¢ûÂº∫ÂÖ∂Âä®‰ΩúÁîüÊàêËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMAP-VLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂà©Áî®ËÆ∞ÂøÜÂ¢ûÂº∫ÊèêÁ§∫Êù•ÊèêÂçáVLAÊ®°ÂûãÂú®ÈïøÊó∂‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåMAP-VLAÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉÊï¥‰∏™VLAÊ®°ÂûãÔºåËÄåÊòØÈÄöËøáËΩªÈáèÁ∫ßÁöÑÊèêÁ§∫Ë∞ÉÊï¥ÂíåÊ£ÄÁ¥¢Â¢ûÂº∫ÔºåÂç≥ÂèØÊòæËëóÊèêÂçá‰ªªÂä°ÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂ∞ÜËÆ∞ÂøÜÂçïÂÖÉË°®Á§∫‰∏∫ÂèØÂ≠¶‰π†ÁöÑËΩØÊèêÁ§∫Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Â≠¶‰π†ÂíåÂà©Áî®ÂéÜÂè≤‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∞ÂøÜÂ∫ì‰∏≠ÁöÑÊØè‰∏™ËÆ∞ÂøÜÂçïÂÖÉÈÉΩÂØπÂ∫î‰∫é‰ªªÂä°ÁöÑ‰∏Ä‰∏™ÁâπÂÆöÈò∂ÊÆµÔºåÈÄöËøáÊèêÁ§∫Ë∞ÉÊï¥ËøõË°å‰ºòÂåñÔºå‰ª•ÊúÄÂ§ßÁ®ãÂ∫¶Âú∞ÊèêÈ´òÂÖ∂ÂØπÂä®‰ΩúÁîüÊàêÁöÑË¥°ÁåÆ„ÄÇËΩ®ËøπÁõ∏‰ººÊÄßÂåπÈÖçÈááÁî®ÂêàÈÄÇÁöÑË∑ùÁ¶ªÂ∫¶ÈáèÔºå‰æãÂ¶ÇÂä®ÊÄÅÊó∂Èó¥ËßÑÊï¥(DTW)Ôºå‰ª•ÂáÜÁ°ÆÊ£ÄÁ¥¢Áõ∏ÂÖ≥ËÆ∞ÂøÜ„ÄÇVLAÊ®°ÂûãÈááÁî®ÂÜªÁªìÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÈÅøÂÖç‰∫ÜÈáçÊñ∞ËÆ≠ÁªÉÁöÑÂºÄÈîÄÔºåÂπ∂‰øùËØÅ‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MAP-VLAÂú®Ê®°ÊãüÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÈ´òËææ7.0%ÁöÑÁªùÂØπÊÄßËÉΩÊèêÂçáÔºåÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ËØÑ‰º∞‰∏≠ÂÆûÁé∞‰∫Ü25.0%ÁöÑÁªùÂØπÊÄßËÉΩÊèêÂçáÔºåÊòæËëó‰ºò‰∫éÂΩìÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåMAP-VLAËÉΩÂ§üÊúâÊïàÂà©Áî®ÂéÜÂè≤‰ø°ÊÅØÔºåÊèêÂçáVLAÊ®°ÂûãÂú®ÈïøÊó∂‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÔºåÂÖ∑ÊúâÂæàÂº∫ÁöÑÂÆûÁî®‰ª∑ÂÄº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MAP-VLAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈïøÊúüËßÑÂàíÂíåÂ§çÊùÇÁä∂ÊÄÅËΩ¨ÁßªÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇË£ÖÈÖç„ÄÅÁÉπÈ•™„ÄÅÊ∏ÖÊ¥ÅÁ≠â„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•ÊòæËëóÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂÆåÊàêÂêÑÁßçÂÆûÈôÖ‰ªªÂä°„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÈ¢ÜÂüüÔºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.

