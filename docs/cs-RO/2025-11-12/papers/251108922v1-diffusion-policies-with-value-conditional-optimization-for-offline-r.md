---
layout: default
title: Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning
---

# Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning

**arXiv**: [2511.08922v1](https://arxiv.org/abs/2511.08922) | [PDF](https://arxiv.org/pdf/2511.08922.pdf)

**ä½œè€…**: Yunchang Ma, Tenglong Liu, Yixing Lan, Xin Yin, Changxin Zhang, Xinglong Zhang, Xin Xu

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-12

**å¤‡æ³¨**: IROS 2025

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDIVOï¼Œé€šè¿‡ä»·å€¼æ¡ä»¶ä¼˜åŒ–æ‰©æ•£ç­–ç•¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿‡ä¼°è®¡é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æ‰©æ•£æ¨¡åž‹` `ä»·å€¼æ¡ä»¶ä¼˜åŒ–` `ä¼˜åŠ¿å‡½æ•°` `ç­–ç•¥å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå€¼è¿‡ä¼°è®¡æ˜¯ç­–ç•¥æ€§èƒ½ç“¶é¢ˆï¼ŒçŽ°æœ‰æ–¹æ³•ä¿å®ˆæ€§è¿‡å¼ºï¼Œéš¾ä»¥å¹³è¡¡è¡¨è¾¾èƒ½åŠ›å’Œæ•ˆçŽ‡ã€‚
2. DIVOåˆ©ç”¨ä¼˜åŠ¿å€¼å¼•å¯¼æ‰©æ•£æ¨¡åž‹è®­ç»ƒï¼Œç²¾ç¡®å¯¹é½æ•°æ®é›†åˆ†å¸ƒï¼Œé€‰æ‹©æ€§æ‰©å±•é«˜ä¼˜åŠ¿åŠ¨ä½œè¾¹ç•Œã€‚
3. DIVOåœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œå°¤å…¶åœ¨AntMazeç­‰ç¨€ç–å¥–åŠ±çŽ¯å¢ƒä¸­ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç”±äºŽåˆ†å¸ƒå¤–(OOD)åŠ¨ä½œå¯¼è‡´çš„å€¼è¿‡ä¼°è®¡ä¸¥é‡é™åˆ¶äº†ç­–ç•¥æ€§èƒ½ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡åž‹å› å…¶å¼ºå¤§çš„åˆ†å¸ƒåŒ¹é…èƒ½åŠ›è€Œè¢«åˆ©ç”¨ï¼Œé€šè¿‡è¡Œä¸ºç­–ç•¥çº¦æŸæ¥å¼ºåˆ¶ä¿å®ˆæ€§ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰æ–¹æ³•é€šå¸¸å¯¹ä½Žè´¨é‡æ•°æ®é›†ä¸­çš„å†—ä½™åŠ¨ä½œè¿›è¡Œæ— å·®åˆ«æ­£åˆ™åŒ–ï¼Œå¯¼è‡´è¿‡åº¦ä¿å®ˆä»¥åŠæ‰©æ•£æ¨¡åž‹è¡¨è¾¾èƒ½åŠ›å’Œæ•ˆçŽ‡ä¹‹é—´çš„ä¸å¹³è¡¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³å…·æœ‰ä»·å€¼æ¡ä»¶ä¼˜åŒ–çš„æ‰©æ•£ç­–ç•¥(DIVO)ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡åž‹ç”Ÿæˆé«˜è´¨é‡ã€å¹¿æ³›è¦†ç›–çš„åˆ†å¸ƒå†…çŠ¶æ€-åŠ¨ä½œæ ·æœ¬ï¼ŒåŒæ—¶ä¿ƒè¿›æœ‰æ•ˆçš„ç­–ç•¥æ”¹è¿›ã€‚å…·ä½“æ¥è¯´ï¼ŒDIVOå¼•å…¥äº†ä¸€ç§äºŒå…ƒåŠ æƒæœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨ç¦»çº¿æ•°æ®é›†ä¸­åŠ¨ä½œçš„ä¼˜åŠ¿å€¼æ¥æŒ‡å¯¼æ‰©æ•£æ¨¡åž‹è®­ç»ƒã€‚è¿™ä½¿å¾—èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°ä¸Žæ•°æ®é›†çš„åˆ†å¸ƒå¯¹é½ï¼ŒåŒæ—¶é€‰æ‹©æ€§åœ°æ‰©å±•é«˜ä¼˜åŠ¿åŠ¨ä½œçš„è¾¹ç•Œã€‚åœ¨ç­–ç•¥æ”¹è¿›è¿‡ç¨‹ä¸­ï¼ŒDIVOåŠ¨æ€åœ°è¿‡æ»¤æ¥è‡ªæ‰©æ•£æ¨¡åž‹çš„é«˜å›žæŠ¥æ½œåŠ›åŠ¨ä½œï¼Œæœ‰æ•ˆåœ°å¼•å¯¼å­¦ä¹ åˆ°çš„ç­–ç•¥æœç€æ›´å¥½çš„æ€§èƒ½å‘å±•ã€‚è¿™ç§æ–¹æ³•åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­å®žçŽ°äº†ä¿å®ˆæ€§å’Œå¯æŽ¢ç´¢æ€§ä¹‹é—´çš„å…³é”®å¹³è¡¡ã€‚æˆ‘ä»¬åœ¨D4RLåŸºå‡†ä¸Šè¯„ä¼°äº†DIVOï¼Œå¹¶å°†å…¶ä¸Žæœ€å…ˆè¿›çš„åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒDIVOå®žçŽ°äº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨è¿åŠ¨ä»»åŠ¡ä¸­å®žçŽ°äº†å¹³å‡å›žæŠ¥çš„æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨å…·æœ‰ç¨€ç–å¥–åŠ±çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„AntMazeé¢†åŸŸä¸­ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ é¢ä¸´å€¼å‡½æ•°è¿‡ä¼°è®¡çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é›†è´¨é‡ä¸é«˜çš„æƒ…å†µä¸‹ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œå¦‚ä½¿ç”¨æ‰©æ•£æ¨¡åž‹çº¦æŸç­–ç•¥ï¼Œå®¹æ˜“å¯¹æ‰€æœ‰åŠ¨ä½œè¿›è¡Œè¿‡åº¦ä¿å®ˆçš„æ­£åˆ™åŒ–ï¼Œé™åˆ¶äº†ç­–ç•¥çš„æŽ¢ç´¢èƒ½åŠ›å’Œæ€§èƒ½æå‡ã€‚è¿™ç§ä¸€åˆ€åˆ‡çš„æ–¹æ³•å¿½ç•¥äº†æ•°æ®é›†ä¸­ä¸åŒåŠ¨ä½œçš„ä»·å€¼å·®å¼‚ï¼Œå¯¼è‡´æ¬¡ä¼˜è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDIVOçš„æ ¸å¿ƒåœ¨äºŽåˆ©ç”¨åŠ¨ä½œçš„ä¼˜åŠ¿å€¼æ¥æŒ‡å¯¼æ‰©æ•£æ¨¡åž‹çš„è®­ç»ƒï¼Œä»Žè€Œå®žçŽ°æ›´ç²¾ç»†çš„ç­–ç•¥çº¦æŸã€‚é€šè¿‡ä¼˜åŠ¿å€¼ï¼ŒDIVOèƒ½å¤ŸåŒºåˆ†æœ‰ä»·å€¼å’Œæ— ä»·å€¼çš„åŠ¨ä½œï¼Œå¹¶é€‰æ‹©æ€§åœ°å¯¹é«˜ä»·å€¼åŠ¨ä½œè¿›è¡ŒæŽ¢ç´¢ï¼Œé¿å…å¯¹æ‰€æœ‰åŠ¨ä½œè¿›è¡Œæ— å·®åˆ«çš„ä¿å®ˆçº¦æŸã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å¹³è¡¡ä¿å®ˆæ€§å’ŒæŽ¢ç´¢æ€§ï¼Œä»Žè€Œåœ¨ç¦»çº¿æ•°æ®é›†ä¸­å­¦ä¹ åˆ°æ›´å¥½çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šDIVOåŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šæ‰©æ•£æ¨¡åž‹è®­ç»ƒå’Œç­–ç•¥æ”¹è¿›ã€‚åœ¨æ‰©æ•£æ¨¡åž‹è®­ç»ƒé˜¶æ®µï¼ŒDIVOä½¿ç”¨äºŒå…ƒåŠ æƒæœºåˆ¶ï¼Œæ ¹æ®åŠ¨ä½œçš„ä¼˜åŠ¿å€¼å¯¹æ‰©æ•£æ¨¡åž‹çš„æŸå¤±å‡½æ•°è¿›è¡ŒåŠ æƒã€‚ä¼˜åŠ¿å€¼é«˜çš„åŠ¨ä½œåœ¨è®­ç»ƒä¸­èŽ·å¾—æ›´é«˜çš„æƒé‡ï¼Œä»Žè€Œå¼•å¯¼æ‰©æ•£æ¨¡åž‹æ›´å¤šåœ°å…³æ³¨è¿™äº›åŠ¨ä½œã€‚åœ¨ç­–ç•¥æ”¹è¿›é˜¶æ®µï¼ŒDIVOä»Žæ‰©æ•£æ¨¡åž‹ä¸­é‡‡æ ·åŠ¨ä½œï¼Œå¹¶æ ¹æ®å…¶æ½œåœ¨å›žæŠ¥è¿›è¡Œè¿‡æ»¤ï¼Œé€‰æ‹©æ›´æœ‰å¯èƒ½å¸¦æ¥é«˜å›žæŠ¥çš„åŠ¨ä½œæ¥æ›´æ–°ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šDIVOçš„å…³é”®åˆ›æ–°åœ¨äºŽä»·å€¼æ¡ä»¶ä¼˜åŒ–ï¼Œå³åˆ©ç”¨åŠ¨ä½œçš„ä¼˜åŠ¿å€¼æ¥æŒ‡å¯¼æ‰©æ•£æ¨¡åž‹çš„è®­ç»ƒå’Œç­–ç•¥æ”¹è¿›ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ä¸åŒï¼ŒDIVOä¸æ˜¯å¯¹æ‰€æœ‰åŠ¨ä½œè¿›è¡Œæ— å·®åˆ«çš„çº¦æŸï¼Œè€Œæ˜¯æ ¹æ®å…¶ä»·å€¼è¿›è¡Œé€‰æ‹©æ€§çš„çº¦æŸå’ŒæŽ¢ç´¢ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç¦»çº¿æ•°æ®ï¼Œå­¦ä¹ åˆ°æ›´å¥½çš„ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šDIVOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) äºŒå…ƒåŠ æƒæœºåˆ¶ï¼Œç”¨äºŽæ ¹æ®åŠ¨ä½œçš„ä¼˜åŠ¿å€¼å¯¹æ‰©æ•£æ¨¡åž‹çš„æŸå¤±å‡½æ•°è¿›è¡ŒåŠ æƒã€‚å…·ä½“æ¥è¯´ï¼Œä¼˜åŠ¿å€¼é«˜äºŽæŸä¸ªé˜ˆå€¼çš„åŠ¨ä½œè¢«èµ‹äºˆæ›´é«˜çš„æƒé‡ï¼Œè€Œä¼˜åŠ¿å€¼ä½ŽäºŽé˜ˆå€¼çš„åŠ¨ä½œåˆ™è¢«èµ‹äºˆè¾ƒä½Žçš„æƒé‡ã€‚2) åŠ¨æ€è¿‡æ»¤æœºåˆ¶ï¼Œç”¨äºŽåœ¨ç­–ç•¥æ”¹è¿›é˜¶æ®µä»Žæ‰©æ•£æ¨¡åž‹ä¸­é‡‡æ ·åŠ¨ä½œï¼Œå¹¶æ ¹æ®å…¶æ½œåœ¨å›žæŠ¥è¿›è¡Œè¿‡æ»¤ã€‚è¯¥æœºåˆ¶é€‰æ‹©æ›´æœ‰å¯èƒ½å¸¦æ¥é«˜å›žæŠ¥çš„åŠ¨ä½œæ¥æ›´æ–°ç­–ç•¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

DIVOåœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨è¿åŠ¨ä»»åŠ¡ä¸­ï¼ŒDIVOçš„å¹³å‡å›žæŠ¥æ˜¾è‘—é«˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„AntMazeçŽ¯å¢ƒä¸­ï¼ŒDIVOä¹Ÿä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œè¯æ˜Žäº†å…¶åœ¨ç¨€ç–å¥–åŠ±çŽ¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒDIVOèƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡ä¿å®ˆæ€§å’ŒæŽ¢ç´¢æ€§ï¼Œä»Žè€Œå­¦ä¹ åˆ°æ›´å¥½çš„ç­–ç•¥ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

DIVOåœ¨æœºå™¨äººæŽ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥åˆ©ç”¨ç¦»çº¿æ•°æ®å­¦ä¹ é«˜æ€§èƒ½ç­–ç•¥ï¼Œæ— éœ€åœ¨çº¿äº¤äº’ï¼Œé™ä½Žäº†å­¦ä¹ æˆæœ¬å’Œé£Žé™©ã€‚å°¤å…¶æ˜¯åœ¨æ•°æ®èŽ·å–å›°éš¾æˆ–æˆæœ¬é«˜æ˜‚çš„åœºæ™¯ä¸‹ï¼ŒDIVOçš„ä»·å€¼æ›´åŠ çªå‡ºã€‚æœªæ¥ï¼ŒDIVOå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€å…ƒå¼ºåŒ–å­¦ä¹ ç­‰é¢†åŸŸã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In offline reinforcement learning, value overestimation caused by out-of-distribution (OOD) actions significantly limits policy performance. Recently, diffusion models have been leveraged for their strong distribution-matching capabilities, enforcing conservatism through behavior policy constraints. However, existing methods often apply indiscriminate regularization to redundant actions in low-quality datasets, resulting in excessive conservatism and an imbalance between the expressiveness and efficiency of diffusion modeling. To address these issues, we propose DIffusion policies with Value-conditional Optimization (DIVO), a novel approach that leverages diffusion models to generate high-quality, broadly covered in-distribution state-action samples while facilitating efficient policy improvement. Specifically, DIVO introduces a binary-weighted mechanism that utilizes the advantage values of actions in the offline dataset to guide diffusion model training. This enables a more precise alignment with the dataset's distribution while selectively expanding the boundaries of high-advantage actions. During policy improvement, DIVO dynamically filters high-return-potential actions from the diffusion model, effectively guiding the learned policy toward better performance. This approach achieves a critical balance between conservatism and explorability in offline RL. We evaluate DIVO on the D4RL benchmark and compare it against state-of-the-art baselines. Empirical results demonstrate that DIVO achieves superior performance, delivering significant improvements in average returns across locomotion tasks and outperforming existing methods in the challenging AntMaze domain, where sparse rewards pose a major difficulty.

