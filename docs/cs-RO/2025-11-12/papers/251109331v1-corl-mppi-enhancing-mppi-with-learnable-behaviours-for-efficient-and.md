---
layout: default
title: CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance
---

# CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.09331" target="_blank" class="toolbar-btn">arXiv: 2511.09331v1</a>
    <a href="https://arxiv.org/pdf/2511.09331.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.09331v1" 
            onclick="toggleFavorite(this, '2511.09331v1', 'CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Stepan Dergachev, Artem Pshenitsyn, Aleksandr Panov, Alexey Skrynnik, Konstantin Yakovlev

**ÂàÜÁ±ª**: cs.RO, cs.MA

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-12

**Â§áÊ≥®**: The manuscript includes 9 pages, 4 figures, and 1 table

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**CoRL-MPPIÔºöËûçÂêàÂº∫ÂåñÂ≠¶‰π†‰∏éMPPIÔºåÊèêÂçáÂ§öÊú∫Âô®‰∫∫ÈÅøÈöúÊïàÁéá‰∏éÂÆâÂÖ®ÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊú∫Âô®‰∫∫Á≥ªÁªü` `Á¢∞ÊíûÈÅøÂÖç` `Ê®°ÂûãÈ¢ÑÊµãÊéßÂà∂` `Âº∫ÂåñÂ≠¶‰π†` `Ë∑ØÂæÑÁßØÂàÜ` `Âéª‰∏≠ÂøÉÂåñÊéßÂà∂` `Âêà‰ΩúÂØºËà™`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§öÊú∫Âô®‰∫∫Âéª‰∏≠ÂøÉÂåñÈÅøÈöúÈù¢‰∏¥ÊåëÊàòÔºå‰º†ÁªüMPPI‰æùËµñÈöèÊú∫ÈááÊ†∑ÔºåÊòì‰∫ßÁîüÊ¨°‰ºòËΩ®Ëøπ„ÄÇ
2. CoRL-MPPIËûçÂêàÂº∫ÂåñÂ≠¶‰π†‰∏éMPPIÔºåÂ≠¶‰π†Âêà‰ΩúÈÅøÈöúÁ≠ñÁï•ÔºåÂºïÂØºMPPIÈááÊ†∑ÂàÜÂ∏É„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåCoRL-MPPIÂú®ÂØºËà™ÊïàÁéáÂíåÂÆâÂÖ®ÊÄßÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âéª‰∏≠ÂøÉÂåñÁ¢∞ÊíûÈÅøÂÖçÊòØÂèØÊâ©Â±ïÂ§öÊú∫Âô®‰∫∫Á≥ªÁªüÁöÑÊ†∏ÂøÉÊåëÊàò„ÄÇÊ®°ÂûãÈ¢ÑÊµãË∑ØÂæÑÁßØÂàÜ(MPPI)ÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºåÂÆÉËá™ÁÑ∂ÈÄÇÁî®‰∫éÂ§ÑÁêÜ‰ªª‰ΩïÊú∫Âô®‰∫∫ËøêÂä®Ê®°ÂûãÂπ∂Êèê‰æõÂº∫Â§ßÁöÑÁêÜËÆ∫‰øùËØÅ„ÄÇÁÑ∂ËÄåÔºåÂú®ÂÆûË∑µ‰∏≠ÔºåÂü∫‰∫éMPPIÁöÑÊéßÂà∂Âô®ÂèØËÉΩÊèê‰æõÊ¨°‰ºòËΩ®ËøπÔºåÂõ†‰∏∫ÂÖ∂ÊÄßËÉΩ‰∏•Èáç‰æùËµñ‰∫éÊó†‰ø°ÊÅØÁöÑÈöèÊú∫ÈááÊ†∑„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜCoRL-MPPIÔºå‰∏ÄÁßçÂêà‰ΩúÂº∫ÂåñÂ≠¶‰π†ÂíåMPPIÁöÑÊñ∞ÂûãËûçÂêàÊñπÊ≥ïÔºå‰ª•Ëß£ÂÜ≥Ëøô‰∏ÄÂ±ÄÈôêÊÄß„ÄÇÊàë‰ª¨Âú®‰ªøÁúü‰∏≠ËÆ≠ÁªÉ‰∏Ä‰∏™Âä®‰ΩúÁ≠ñÁï•ÔºàËøë‰ºº‰∏∫Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÔºâÔºåÂ≠¶‰π†Â±ÄÈÉ®Âêà‰ΩúÈÅøÁ¢∞Ë°å‰∏∫„ÄÇÁÑ∂ÂêéÔºåÂ∞ÜÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•ÂµåÂÖ•Âà∞MPPIÊ°ÜÊû∂‰∏≠Ôºå‰ª•ÊåáÂØºÂÖ∂ÈááÊ†∑ÂàÜÂ∏ÉÔºå‰ΩøÂÖ∂ÂÅèÂêëÊõ¥Êô∫ËÉΩÂíåÂçè‰ΩúÁöÑÂä®‰Ωú„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåCoRL-MPPI‰øùÁïô‰∫ÜÂ∏∏ËßÑMPPIÁöÑÊâÄÊúâÁêÜËÆ∫‰øùËØÅ„ÄÇÊàë‰ª¨Âú®ÂØÜÈõÜÁöÑÂä®ÊÄÅ‰ªøÁúüÁéØÂ¢É‰∏≠ÔºåÈíàÂØπÊúÄÂÖàËøõÁöÑÂü∫Á∫øÔºàÂåÖÊã¨ORCA„ÄÅBVCÂíåÂ§öÊô∫ËÉΩ‰ΩìMPPIÂÆûÁé∞ÔºâËØÑ‰º∞‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ï„ÄÇÁªìÊûúË°®ÊòéÔºåCoRL-MPPIÊòæËëóÊèêÈ´ò‰∫ÜÂØºËà™ÊïàÁéáÔºàÈÄöËøáÊàêÂäüÁéáÂíåÂÆåÂ∑•Êó∂Èó¥Ë°°ÈáèÔºâÂíåÂÆâÂÖ®ÊÄßÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊïèÊç∑ËÄåÁ®≥ÂÅ•ÁöÑÂ§öÊú∫Âô®‰∫∫ÂØºËà™„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊú∫Âô®‰∫∫Á≥ªÁªü‰∏≠Âéª‰∏≠ÂøÉÂåñÁ¢∞ÊíûÈÅøÂÖçÈóÆÈ¢ò„ÄÇÁé∞ÊúâÂü∫‰∫éMPPIÁöÑÊñπÊ≥ï‰æùËµñ‰∫éÈöèÊú∫ÈááÊ†∑ÔºåÂØºËá¥Êé¢Á¥¢ÊïàÁéá‰ΩéÔºåÂÆπÊòìÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºòÔºåÊó†Ê≥ï‰øùËØÅÈ´òÊïàÂíåÂÆâÂÖ®ÁöÑÂØºËà™„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•Âú®Â§çÊùÇÂä®ÊÄÅÁéØÂ¢É‰∏≠ÂÆûÁé∞È≤ÅÊ£íÁöÑÂ§öÊú∫Âô®‰∫∫Âçè‰Ωú„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Âº∫ÂåñÂ≠¶‰π†È¢ÑËÆ≠ÁªÉ‰∏Ä‰∏™Âêà‰ΩúÈÅøÈöúÁ≠ñÁï•ÔºåÁÑ∂ÂêéÂ∞ÜËØ•Á≠ñÁï•ÂµåÂÖ•Âà∞MPPIÊ°ÜÊû∂‰∏≠Ôºå‰ª•ÊåáÂØºMPPIÁöÑÈááÊ†∑ËøáÁ®ã„ÄÇÈÄöËøáÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•Êù•ÂºïÂØºÈááÊ†∑ÔºåÂèØ‰ª•ÊèêÈ´òÈááÊ†∑ÊïàÁéáÔºåÂáèÂ∞ëÊó†ÊïàÊé¢Á¥¢Ôºå‰ªéËÄåÊõ¥Âø´Âú∞ÊâæÂà∞ÂÆâÂÖ®‰∏îÈ´òÊïàÁöÑËΩ®Ëøπ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCoRL-MPPIÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) Âêà‰ΩúÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÔºö‰ΩøÁî®Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúËÆ≠ÁªÉ‰∏Ä‰∏™Âä®‰ΩúÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•Â≠¶‰π†Â¶Ç‰ΩïÂú®Â±ÄÈÉ®ÁéØÂ¢É‰∏≠ËøõË°åÂêà‰ΩúÈÅøÈöú„ÄÇ2) MPPIÊéßÂà∂Èò∂ÊÆµÔºöÂ∞ÜÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•ÂµåÂÖ•Âà∞MPPIÊ°ÜÊû∂‰∏≠ÔºåÁî®‰∫éÊåáÂØºÈááÊ†∑ÂàÜÂ∏É„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•ËæìÂá∫‰∏Ä‰∏™Âä®‰ΩúÂª∫ËÆÆÔºåËØ•Âª∫ËÆÆË¢´Áî®Êù•Ë∞ÉÊï¥MPPIÁöÑÈááÊ†∑ÂàÜÂ∏ÉÔºå‰ΩøÂÖ∂ÂÅèÂêë‰∫éÊõ¥Êô∫ËÉΩÂíåÂçè‰ΩúÁöÑÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCoRL-MPPIÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÂíåMPPIÁõ∏ÁªìÂêàÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Â≠¶‰π†Âà∞ÁöÑÂÖàÈ™åÁü•ËØÜÊù•ÊåáÂØºMPPIÁöÑÈááÊ†∑ËøáÁ®ã„ÄÇ‰∏é‰º†ÁªüÁöÑMPPIÁõ∏ÊØîÔºåCoRL-MPPI‰∏çÂÜç‰æùËµñ‰∫éÊó†‰ø°ÊÅØÁöÑÈöèÊú∫ÈááÊ†∑ÔºåËÄåÊòØÂà©Áî®Â≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•ËøõË°åÊô∫ËÉΩÈááÊ†∑Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈááÊ†∑ÊïàÁéáÂíåÊÄßËÉΩ„ÄÇÂêåÊó∂ÔºåCoRL-MPPI‰øùÁïô‰∫ÜMPPIÁöÑÁêÜËÆ∫‰øùËØÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Âº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÔºå‰ΩøÁî®Ê∑±Â∫¶Á•ûÁªèÁΩëÁªú‰Ωú‰∏∫Âä®‰ΩúÁ≠ñÁï•ÁöÑËøë‰ºº„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÈúÄË¶ÅËÄÉËôëÁ¢∞ÊíûÈÅøÂÖç„ÄÅÁõÆÊ†áÂØºÂêëÂíåÂçè‰ΩúÁ≠âÂõ†Á¥†„ÄÇÂú®MPPIÊéßÂà∂Èò∂ÊÆµÔºåÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•ËæìÂá∫ÁöÑÂä®‰ΩúÂª∫ËÆÆË¢´Áî®Êù•Ë∞ÉÊï¥MPPIÁöÑÈááÊ†∑ÂàÜÂ∏ÉÔºå‰æãÂ¶ÇÈÄöËøáË∞ÉÊï¥ÈááÊ†∑Âô™Â£∞ÁöÑÂùáÂÄºÊàñÊñπÂ∑Æ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑÊú∫Âô®‰∫∫ËøêÂä®Ê®°ÂûãÂíåÁéØÂ¢ÉËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CoRL-MPPIÂú®ÂØÜÈõÜ„ÄÅÂä®ÊÄÅÁöÑ‰ªøÁúüÁéØÂ¢É‰∏≠ËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂπ∂‰∏éORCA„ÄÅBVCÂíåÂ§öÊô∫ËÉΩ‰ΩìMPPIÁ≠âÊúÄÂÖàËøõÁöÑÂü∫Á∫øËøõË°å‰∫ÜÊØîËæÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCoRL-MPPIÊòæËëóÊèêÈ´ò‰∫ÜÂØºËà™ÊïàÁéáÔºàÈÄöËøáÊàêÂäüÁéáÂíåÂÆåÂ∑•Êó∂Èó¥Ë°°ÈáèÔºâÂíåÂÆâÂÖ®ÊÄß„ÄÇ‰æãÂ¶ÇÔºåCoRL-MPPIÁöÑÊàêÂäüÁéáÊØî‰º†ÁªüMPPIÊèêÈ´ò‰∫ÜXX%ÔºåÂÆåÂ∑•Êó∂Èó¥Áº©Áü≠‰∫ÜYY%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CoRL-MPPIÂèØÂ∫îÁî®‰∫é‰ªìÂ∫ìÊú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊó†‰∫∫Êú∫ÁºñÈòüÁ≠âÈúÄË¶ÅÂ§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÁöÑÂú∫ÊôØ„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÈ´òÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑÂØºËà™ÊïàÁéáÂíåÂÆâÂÖ®ÊÄßÔºåÈôç‰ΩéÁ¢∞ÊíûÈ£éÈô©ÔºåÊèêÂçáÊï¥‰Ωì‰ªªÂä°ÂÆåÊàêÊïàÁéá„ÄÇÊú™Êù•ÂèØËøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑÁéØÂ¢ÉÂíå‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÂä®ÊÄÅÈöúÁ¢çÁâ©ÁéØÂ¢É„ÄÅÂºÇÊûÑÊú∫Âô®‰∫∫Á≥ªÁªüÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Decentralized collision avoidance remains a core challenge for scalable multi-robot systems. One of the promising approaches to tackle this problem is Model Predictive Path Integral (MPPI) -- a framework that is naturally suited to handle any robot motion model and provides strong theoretical guarantees. Still, in practice MPPI-based controller may provide suboptimal trajectories as its performance relies heavily on uninformed random sampling. In this work, we introduce CoRL-MPPI, a novel fusion of Cooperative Reinforcement Learning and MPPI to address this limitation. We train an action policy (approximated as deep neural network) in simulation that learns local cooperative collision avoidance behaviors. This learned policy is then embedded into the MPPI framework to guide its sampling distribution, biasing it towards more intelligent and cooperative actions. Notably, CoRL-MPPI preserves all the theoretical guarantees of regular MPPI. We evaluate our approach in dense, dynamic simulation environments against state-of-the-art baselines, including ORCA, BVC, and a multi-agent MPPI implementation. Our results demonstrate that CoRL-MPPI significantly improves navigation efficiency (measured by success rate and makespan) and safety, enabling agile and robust multi-robot navigation.

