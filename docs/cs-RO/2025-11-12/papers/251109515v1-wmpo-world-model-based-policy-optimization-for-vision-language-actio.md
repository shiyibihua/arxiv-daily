---
layout: default
title: WMPO: World Model-based Policy Optimization for Vision-Language-Action Models
---

# WMPO: World Model-based Policy Optimization for Vision-Language-Action Models

**arXiv**: [2511.09515v1](https://arxiv.org/abs/2511.09515) | [PDF](https://arxiv.org/pdf/2511.09515.pdf)

**ä½œè€…**: Fangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, Song Guo

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-12

**å¤‡æ³¨**: project website: https://wm-po.github.io

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWMPOï¼Œç”¨äºŽè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹çš„åŸºäºŽä¸–ç•Œæ¨¡åž‹çš„ç­–ç•¥ä¼˜åŒ–**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `ä¸–ç•Œæ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `æœºå™¨äººæ“ä½œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. VLAæ¨¡åž‹ä¾èµ–ä¸“å®¶æ•°æ®ï¼Œéš¾ä»¥ä»Žå¤±è´¥ä¸­å­¦ä¹ å’Œè‡ªæˆ‘çº æ­£ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚æœºå™¨äººä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. WMPOé€šè¿‡æž„å»ºåƒç´ çº§ä¸–ç•Œæ¨¡åž‹ï¼Œä½¿æ™ºèƒ½ä½“åœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œé¿å…äº†ä¸ŽçœŸå®žçŽ¯å¢ƒçš„ç›´æŽ¥äº¤äº’ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒWMPOæ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆçŽ‡å’Œæ•´ä½“æ€§èƒ½ï¼Œå¹¶å±•çŽ°å‡ºè‡ªæˆ‘çº æ­£ã€æ³›åŒ–å’Œç»ˆèº«å­¦ä¹ èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹åœ¨é€šç”¨æœºå™¨äººæ“ä½œæ–¹é¢å±•çŽ°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å¯¹ä¸“å®¶æ¼”ç¤ºçš„ä¾èµ–é™åˆ¶äº†å…¶ä»Žå¤±è´¥ä¸­å­¦ä¹ å’Œæ‰§è¡Œè‡ªæˆ‘çº æ­£çš„èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ (RL)é€šè¿‡ä¸Žç‰©ç†çŽ¯å¢ƒçš„è‡ªæˆ‘æ”¹è¿›äº¤äº’æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†å´é¢ä¸´ç€çœŸå®žæœºå™¨äººä¸Šçš„é«˜æ ·æœ¬å¤æ‚åº¦ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºŽä¸–ç•Œæ¨¡åž‹çš„ç­–ç•¥ä¼˜åŒ–(WMPO)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºŽåœ¨çº¿VLA RLçš„åŽŸåˆ™æ€§æ¡†æž¶ï¼Œæ— éœ€ä¸ŽçœŸå®žçŽ¯å¢ƒäº¤äº’ã€‚ä¸Žå¹¿æ³›ä½¿ç”¨çš„æ½œåœ¨ä¸–ç•Œæ¨¡åž‹ä¸åŒï¼ŒWMPOä¸“æ³¨äºŽåƒç´ çº§é¢„æµ‹ï¼Œä½¿â€œæƒ³è±¡â€çš„è½¨è¿¹ä¸Žä½¿ç”¨ç½‘ç»œè§„æ¨¡å›¾åƒé¢„è®­ç»ƒçš„VLAç‰¹å¾å¯¹é½ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼ŒWMPOä½¿ç­–ç•¥èƒ½å¤Ÿæ‰§è¡Œåœ¨çº¿GRPOï¼Œä»Žè€Œæä¾›æ¯”å¸¸ç”¨çš„ç¦»çº¿æ–¹æ³•æ›´å¼ºçš„æ€§èƒ½ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººçŽ¯å¢ƒä¸­çš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒWMPO (i)æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆçŽ‡ï¼Œ(ii)å®žçŽ°äº†æ›´å¼ºçš„æ•´ä½“æ€§èƒ½ï¼Œ(iii)è¡¨çŽ°å‡ºè¯¸å¦‚è‡ªæˆ‘çº æ­£ç­‰æ¶ŒçŽ°è¡Œä¸ºï¼Œä»¥åŠ(iv)å±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–å’Œç»ˆèº«å­¦ä¹ èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVLAæ¨¡åž‹è™½ç„¶åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå±•çŽ°å‡ºæ½œåŠ›ï¼Œä½†è¿‡åº¦ä¾èµ–ä¸“å®¶æ¼”ç¤ºæ•°æ®ï¼Œå¯¼è‡´å…¶éš¾ä»¥ä»Žå¤±è´¥ç»éªŒä¸­å­¦ä¹ ï¼Œç¼ºä¹è‡ªæˆ‘çº æ­£èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ è™½ç„¶å¯ä»¥é€šè¿‡ä¸ŽçŽ¯å¢ƒäº¤äº’æ¥å­¦ä¹ ï¼Œä½†æ ·æœ¬æ•ˆçŽ‡ä½Žï¼Œåœ¨çœŸå®žæœºå™¨äººä¸Šåº”ç”¨æˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨é™ä½Žæ ·æœ¬å¤æ‚åº¦çš„åŒæ—¶ï¼Œæå‡VLAæ¨¡åž‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šWMPOçš„æ ¸å¿ƒæ€è·¯æ˜¯æž„å»ºä¸€ä¸ªåŸºäºŽåƒç´ çš„ä¸–ç•Œæ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹èƒ½å¤Ÿé¢„æµ‹åœ¨ç»™å®šåŠ¨ä½œåºåˆ—ä¸‹ï¼ŒçŽ¯å¢ƒçš„æœªæ¥çŠ¶æ€ï¼ˆåƒç´ ï¼‰ã€‚é€šè¿‡åœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œæ™ºèƒ½ä½“å¯ä»¥åœ¨æ— éœ€ä¸ŽçœŸå®žçŽ¯å¢ƒäº¤äº’çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•é™ä½Žäº†æ ·æœ¬å¤æ‚åº¦ï¼Œå¹¶å…è®¸æ™ºèƒ½ä½“ä»Žæ›´å¤šæ ·åŒ–çš„ç»éªŒä¸­å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šWMPOæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) VLAç‰¹å¾æå–å™¨ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„VLAæ¨¡åž‹æå–å›¾åƒå’Œè¯­è¨€æŒ‡ä»¤çš„ç‰¹å¾ã€‚2) ä¸–ç•Œæ¨¡åž‹ï¼šä¸€ä¸ªåŸºäºŽåƒç´ çš„é¢„æµ‹æ¨¡åž‹ï¼Œç”¨äºŽé¢„æµ‹ç»™å®šçŠ¶æ€å’ŒåŠ¨ä½œåºåˆ—ä¸‹çš„æœªæ¥çŠ¶æ€ã€‚3) ç­–ç•¥ç½‘ç»œï¼šç”¨äºŽç”ŸæˆåŠ¨ä½œåºåˆ—ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–åœ¨ä¸–ç•Œæ¨¡åž‹ä¸­é¢„æµ‹çš„å¥–åŠ±ã€‚4) ç­–ç•¥ä¼˜åŒ–å™¨ï¼šä½¿ç”¨åœ¨çº¿GRPOç®—æ³•ä¼˜åŒ–ç­–ç•¥ç½‘ç»œã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œé¦–å…ˆä½¿ç”¨VLAç‰¹å¾æå–å™¨æå–çŽ¯å¢ƒçŠ¶æ€ç‰¹å¾ï¼Œç„¶åŽç­–ç•¥ç½‘ç»œæ ¹æ®çŠ¶æ€ç”ŸæˆåŠ¨ä½œï¼Œä¸–ç•Œæ¨¡åž‹é¢„æµ‹æ‰§è¡ŒåŠ¨ä½œåŽçš„ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œç­–ç•¥ä¼˜åŒ–å™¨æ ¹æ®é¢„æµ‹çš„çŠ¶æ€å’Œå¥–åŠ±ä¿¡å·æ›´æ–°ç­–ç•¥ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šWMPOçš„å…³é”®åˆ›æ–°åœ¨äºŽä½¿ç”¨åƒç´ çº§é¢„æµ‹çš„ä¸–ç•Œæ¨¡åž‹ï¼Œå¹¶ç»“åˆåœ¨çº¿GRPOç®—æ³•è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚ä¸Žä¼ ç»Ÿçš„æ½œåœ¨ä¸–ç•Œæ¨¡åž‹ç›¸æ¯”ï¼Œåƒç´ çº§é¢„æµ‹èƒ½å¤Ÿæ›´å¥½åœ°å¯¹é½â€œæƒ³è±¡â€çš„è½¨è¿¹ä¸ŽVLAç‰¹å¾ï¼Œä»Žè€Œæé«˜ç­–ç•¥å­¦ä¹ çš„æ•ˆçŽ‡å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒWMPOé‡‡ç”¨åœ¨çº¿GRPOç®—æ³•ï¼Œç›¸æ¯”äºŽå¸¸ç”¨çš„ç¦»çº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›æ›´å¼ºçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šWMPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å·ç§¯ç¥žç»ç½‘ç»œæž„å»ºåƒç´ çº§ä¸–ç•Œæ¨¡åž‹ï¼Œå¹¶é‡‡ç”¨å¯¹æŠ—ç”Ÿæˆç½‘ç»œ(GAN)æ¥æé«˜é¢„æµ‹çš„çœŸå®žæ€§ã€‚2) ä½¿ç”¨åœ¨çº¿GRPOç®—æ³•è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æŽ¢ç´¢çŠ¶æ€ç©ºé—´ï¼Œå¹¶é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚3) è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡å¹¶é¿å…ç¢°æ’žã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒWMPOåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººçŽ¯å¢ƒä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­ï¼ŒWMPOçš„æ ·æœ¬æ•ˆçŽ‡æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†æ•°å€ã€‚åœ¨çœŸå®žæœºå™¨äººçŽ¯å¢ƒä¸­ï¼ŒWMPOèƒ½å¤ŸæˆåŠŸå®Œæˆå¤æ‚çš„æ“ä½œä»»åŠ¡ï¼Œå¹¶å±•çŽ°å‡ºè‡ªæˆ‘çº æ­£å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼ŒWMPOèƒ½å¤Ÿè‡ªä¸»åœ°è°ƒæ•´æŠ“å–å§¿åŠ¿ï¼Œä»¥åº”å¯¹ç‰©ä½“ä½ç½®çš„å¾®å°å˜åŒ–ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

WMPOå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºŽå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººå’ŒåŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡åœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—é™ä½Žæœºå™¨äººçš„å¼€å‘æˆæœ¬å’Œé£Žé™©ï¼Œå¹¶æé«˜å…¶åœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒWMPOè¿˜å¯ä»¥ç”¨äºŽå¼€å‘æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿï¼Œä»Žè€Œå®žçŽ°æ›´é«˜çº§åˆ«çš„è‡ªåŠ¨åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.

