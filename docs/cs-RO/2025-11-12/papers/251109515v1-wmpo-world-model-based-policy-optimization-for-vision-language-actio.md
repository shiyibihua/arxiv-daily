---
layout: default
title: WMPO: World Model-based Policy Optimization for Vision-Language-Action Models
---

# WMPO: World Model-based Policy Optimization for Vision-Language-Action Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.09515" target="_blank" class="toolbar-btn">arXiv: 2511.09515v1</a>
    <a href="https://arxiv.org/pdf/2511.09515.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.09515v1" 
            onclick="toggleFavorite(this, '2511.09515v1', 'WMPO: World Model-based Policy Optimization for Vision-Language-Action Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Fangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, Song Guo

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-12

**Â§áÊ≥®**: project website: https://wm-po.github.io

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫WMPOÔºåÁî®‰∫éËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÂü∫‰∫é‰∏ñÁïåÊ®°ÂûãÁöÑÁ≠ñÁï•‰ºòÂåñ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã` `‰∏ñÁïåÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `Á≠ñÁï•‰ºòÂåñ` `Êú∫Âô®‰∫∫Êìç‰Ωú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. VLAÊ®°Âûã‰æùËµñ‰∏ìÂÆ∂Êï∞ÊçÆÔºåÈöæ‰ª•‰ªéÂ§±Ë¥•‰∏≠Â≠¶‰π†ÂíåËá™ÊàëÁ∫†Ê≠£ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Â§çÊùÇÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. WMPOÈÄöËøáÊûÑÂª∫ÂÉèÁ¥†Á∫ß‰∏ñÁïåÊ®°ÂûãÔºå‰ΩøÊô∫ËÉΩ‰ΩìÂú®ËôöÊãüÁéØÂ¢É‰∏≠ËøõË°åÁ≠ñÁï•‰ºòÂåñÔºåÈÅøÂÖç‰∫Ü‰∏éÁúüÂÆûÁéØÂ¢ÉÁöÑÁõ¥Êé•‰∫§‰∫í„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåWMPOÊòæËëóÊèêÂçá‰∫ÜÊ†∑Êú¨ÊïàÁéáÂíåÊï¥‰ΩìÊÄßËÉΩÔºåÂπ∂Â±ïÁé∞Âá∫Ëá™ÊàëÁ∫†Ê≠£„ÄÅÊ≥õÂåñÂíåÁªàË∫´Â≠¶‰π†ËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®ÈÄöÁî®Êú∫Âô®‰∫∫Êìç‰ΩúÊñπÈù¢Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊΩúÂäõÔºå‰ΩÜÂÆÉ‰ª¨ÂØπ‰∏ìÂÆ∂ÊºîÁ§∫ÁöÑ‰æùËµñÈôêÂà∂‰∫ÜÂÖ∂‰ªéÂ§±Ë¥•‰∏≠Â≠¶‰π†ÂíåÊâßË°åËá™ÊàëÁ∫†Ê≠£ÁöÑËÉΩÂäõ„ÄÇÂº∫ÂåñÂ≠¶‰π†(RL)ÈÄöËøá‰∏éÁâ©ÁêÜÁéØÂ¢ÉÁöÑËá™ÊàëÊîπËøõ‰∫§‰∫íÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩÜÂç¥Èù¢‰∏¥ÁùÄÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÁöÑÈ´òÊ†∑Êú¨Â§çÊùÇÂ∫¶„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂü∫‰∫é‰∏ñÁïåÊ®°ÂûãÁöÑÁ≠ñÁï•‰ºòÂåñ(WMPO)ÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éÂú®Á∫øVLA RLÁöÑÂéüÂàôÊÄßÊ°ÜÊû∂ÔºåÊó†ÈúÄ‰∏éÁúüÂÆûÁéØÂ¢É‰∫§‰∫í„ÄÇ‰∏éÂπøÊ≥õ‰ΩøÁî®ÁöÑÊΩúÂú®‰∏ñÁïåÊ®°Âûã‰∏çÂêåÔºåWMPO‰∏ìÊ≥®‰∫éÂÉèÁ¥†Á∫ßÈ¢ÑÊµãÔºå‰Ωø‚ÄúÊÉ≥Ë±°‚ÄùÁöÑËΩ®Ëøπ‰∏é‰ΩøÁî®ÁΩëÁªúËßÑÊ®°ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÁöÑVLAÁâπÂæÅÂØπÈΩê„ÄÇËá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòØÔºåWMPO‰ΩøÁ≠ñÁï•ËÉΩÂ§üÊâßË°åÂú®Á∫øGRPOÔºå‰ªéËÄåÊèê‰æõÊØîÂ∏∏Áî®ÁöÑÁ¶ªÁ∫øÊñπÊ≥ïÊõ¥Âº∫ÁöÑÊÄßËÉΩ„ÄÇÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåWMPO (i)ÊòæËëóÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéáÔºå(ii)ÂÆûÁé∞‰∫ÜÊõ¥Âº∫ÁöÑÊï¥‰ΩìÊÄßËÉΩÔºå(iii)Ë°®Áé∞Âá∫ËØ∏Â¶ÇËá™ÊàëÁ∫†Ê≠£Á≠âÊ∂åÁé∞Ë°å‰∏∫Ôºå‰ª•Âèä(iv)Â±ïÁ§∫‰∫ÜÂº∫Â§ßÁöÑÊ≥õÂåñÂíåÁªàË∫´Â≠¶‰π†ËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöVLAÊ®°ÂûãËôΩÁÑ∂Âú®Êú∫Âô®‰∫∫Êìç‰ΩúÈ¢ÜÂüüÂ±ïÁé∞Âá∫ÊΩúÂäõÔºå‰ΩÜËøáÂ∫¶‰æùËµñ‰∏ìÂÆ∂ÊºîÁ§∫Êï∞ÊçÆÔºåÂØºËá¥ÂÖ∂Èöæ‰ª•‰ªéÂ§±Ë¥•ÁªèÈ™å‰∏≠Â≠¶‰π†ÔºåÁº∫‰πèËá™ÊàëÁ∫†Ê≠£ËÉΩÂäõ„ÄÇÂº∫ÂåñÂ≠¶‰π†ËôΩÁÑ∂ÂèØ‰ª•ÈÄöËøá‰∏éÁéØÂ¢É‰∫§‰∫íÊù•Â≠¶‰π†Ôºå‰ΩÜÊ†∑Êú¨ÊïàÁéá‰ΩéÔºåÂú®ÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÂ∫îÁî®ÊàêÊú¨È´òÊòÇ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂú®Èôç‰ΩéÊ†∑Êú¨Â§çÊùÇÂ∫¶ÁöÑÂêåÊó∂ÔºåÊèêÂçáVLAÊ®°ÂûãÁöÑÂ≠¶‰π†ËÉΩÂäõÔºåÊòØÊú¨ÊñáË¶ÅËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöWMPOÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Âü∫‰∫éÂÉèÁ¥†ÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåËØ•Ê®°ÂûãËÉΩÂ§üÈ¢ÑÊµãÂú®ÁªôÂÆöÂä®‰ΩúÂ∫èÂàó‰∏ãÔºåÁéØÂ¢ÉÁöÑÊú™Êù•Áä∂ÊÄÅÔºàÂÉèÁ¥†Ôºâ„ÄÇÈÄöËøáÂú®ËôöÊãüÁéØÂ¢É‰∏≠ËøõË°åÁ≠ñÁï•‰ºòÂåñÔºåÊô∫ËÉΩ‰ΩìÂèØ‰ª•Âú®Êó†ÈúÄ‰∏éÁúüÂÆûÁéØÂ¢É‰∫§‰∫íÁöÑÊÉÖÂÜµ‰∏ãÂ≠¶‰π†„ÄÇËøôÁßçÊñπÊ≥ïÈôç‰Ωé‰∫ÜÊ†∑Êú¨Â§çÊùÇÂ∫¶ÔºåÂπ∂ÂÖÅËÆ∏Êô∫ËÉΩ‰Ωì‰ªéÊõ¥Â§öÊ†∑ÂåñÁöÑÁªèÈ™å‰∏≠Â≠¶‰π†„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöWMPOÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) VLAÁâπÂæÅÊèêÂèñÂô®Ôºö‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑVLAÊ®°ÂûãÊèêÂèñÂõæÂÉèÂíåËØ≠Ë®ÄÊåá‰ª§ÁöÑÁâπÂæÅ„ÄÇ2) ‰∏ñÁïåÊ®°ÂûãÔºö‰∏Ä‰∏™Âü∫‰∫éÂÉèÁ¥†ÁöÑÈ¢ÑÊµãÊ®°ÂûãÔºåÁî®‰∫éÈ¢ÑÊµãÁªôÂÆöÁä∂ÊÄÅÂíåÂä®‰ΩúÂ∫èÂàó‰∏ãÁöÑÊú™Êù•Áä∂ÊÄÅ„ÄÇ3) Á≠ñÁï•ÁΩëÁªúÔºöÁî®‰∫éÁîüÊàêÂä®‰ΩúÂ∫èÂàóÔºåÁõÆÊ†áÊòØÊúÄÂ§ßÂåñÂú®‰∏ñÁïåÊ®°Âûã‰∏≠È¢ÑÊµãÁöÑÂ•ñÂä±„ÄÇ4) Á≠ñÁï•‰ºòÂåñÂô®Ôºö‰ΩøÁî®Âú®Á∫øGRPOÁÆóÊ≥ï‰ºòÂåñÁ≠ñÁï•ÁΩëÁªú„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÔºåÈ¶ñÂÖà‰ΩøÁî®VLAÁâπÂæÅÊèêÂèñÂô®ÊèêÂèñÁéØÂ¢ÉÁä∂ÊÄÅÁâπÂæÅÔºåÁÑ∂ÂêéÁ≠ñÁï•ÁΩëÁªúÊ†πÊçÆÁä∂ÊÄÅÁîüÊàêÂä®‰ΩúÔºå‰∏ñÁïåÊ®°ÂûãÈ¢ÑÊµãÊâßË°åÂä®‰ΩúÂêéÁöÑ‰∏ã‰∏Ä‰∏™Áä∂ÊÄÅÔºåÁ≠ñÁï•‰ºòÂåñÂô®Ê†πÊçÆÈ¢ÑÊµãÁöÑÁä∂ÊÄÅÂíåÂ•ñÂä±‰ø°Âè∑Êõ¥Êñ∞Á≠ñÁï•ÁΩëÁªú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöWMPOÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ΩøÁî®ÂÉèÁ¥†Á∫ßÈ¢ÑÊµãÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåÂπ∂ÁªìÂêàÂú®Á∫øGRPOÁÆóÊ≥ïËøõË°åÁ≠ñÁï•‰ºòÂåñ„ÄÇ‰∏é‰º†ÁªüÁöÑÊΩúÂú®‰∏ñÁïåÊ®°ÂûãÁõ∏ÊØîÔºåÂÉèÁ¥†Á∫ßÈ¢ÑÊµãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂØπÈΩê‚ÄúÊÉ≥Ë±°‚ÄùÁöÑËΩ®Ëøπ‰∏éVLAÁâπÂæÅÔºå‰ªéËÄåÊèêÈ´òÁ≠ñÁï•Â≠¶‰π†ÁöÑÊïàÁéáÂíåÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåWMPOÈááÁî®Âú®Á∫øGRPOÁÆóÊ≥ïÔºåÁõ∏ÊØî‰∫éÂ∏∏Áî®ÁöÑÁ¶ªÁ∫øÊñπÊ≥ïÔºåËÉΩÂ§üÊèê‰æõÊõ¥Âº∫ÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöWMPOÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÊûÑÂª∫ÂÉèÁ¥†Á∫ß‰∏ñÁïåÊ®°ÂûãÔºåÂπ∂ÈááÁî®ÂØπÊäóÁîüÊàêÁΩëÁªú(GAN)Êù•ÊèêÈ´òÈ¢ÑÊµãÁöÑÁúüÂÆûÊÄß„ÄÇ2) ‰ΩøÁî®Âú®Á∫øGRPOÁÆóÊ≥ïËøõË°åÁ≠ñÁï•‰ºòÂåñÔºåËØ•ÁÆóÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞Êé¢Á¥¢Áä∂ÊÄÅÁ©∫Èó¥ÔºåÂπ∂ÈÅøÂÖçÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºò„ÄÇ3) ËÆæËÆ°ÂêàÈÄÇÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÈºìÂä±Êô∫ËÉΩ‰ΩìÂÆåÊàê‰ªªÂä°Âπ∂ÈÅøÂÖçÁ¢∞Êíû„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWMPOÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®Ê®°ÊãüÁéØÂ¢É‰∏≠ÔºåWMPOÁöÑÊ†∑Êú¨ÊïàÁéáÊØîÂü∫Á∫øÊñπÊ≥ïÊèêÈ´ò‰∫ÜÊï∞ÂÄç„ÄÇÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÔºåWMPOËÉΩÂ§üÊàêÂäüÂÆåÊàêÂ§çÊùÇÁöÑÊìç‰Ωú‰ªªÂä°ÔºåÂπ∂Â±ïÁé∞Âá∫Ëá™ÊàëÁ∫†Ê≠£ÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ‰æãÂ¶ÇÔºåWMPOËÉΩÂ§üËá™‰∏ªÂú∞Ë∞ÉÊï¥ÊäìÂèñÂßøÂäøÔºå‰ª•Â∫îÂØπÁâ©‰Ωì‰ΩçÁΩÆÁöÑÂæÆÂ∞èÂèòÂåñ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

WMPOÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÊú∫Âô®‰∫∫ÂíåÂåªÁñóÊú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÂú®ËôöÊãüÁéØÂ¢É‰∏≠ËøõË°åËÆ≠ÁªÉÔºåÂèØ‰ª•ÊòæËëóÈôç‰ΩéÊú∫Âô®‰∫∫ÁöÑÂºÄÂèëÊàêÊú¨ÂíåÈ£éÈô©ÔºåÂπ∂ÊèêÈ´òÂÖ∂Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåWMPOËøòÂèØ‰ª•Áî®‰∫éÂºÄÂèëÊõ¥Êô∫ËÉΩ„ÄÅÊõ¥Ëá™‰∏ªÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÔºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÁ∫ßÂà´ÁöÑËá™Âä®Âåñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.

