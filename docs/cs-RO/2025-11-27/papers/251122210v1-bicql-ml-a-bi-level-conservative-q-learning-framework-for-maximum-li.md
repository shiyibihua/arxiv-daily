---
layout: default
title: BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning
---

# BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.22210" target="_blank" class="toolbar-btn">arXiv: 2511.22210v1</a>
    <a href="https://arxiv.org/pdf/2511.22210.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.22210v1" 
            onclick="toggleFavorite(this, '2511.22210v1', 'BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Junsung Park

**ÂàÜÁ±ª**: cs.LG, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-27

**Â§áÊ≥®**: 8 pages, 3 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫BiCQL-MLÔºåÈÄöËøáÂèåÂ±Ç‰øùÂÆàQÂ≠¶‰π†Ëß£ÂÜ≥Á¶ªÁ∫øÈÄÜÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÂ•ñÂä±ÂáΩÊï∞ÊÅ¢Â§çÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Á¶ªÁ∫øÈÄÜÂº∫ÂåñÂ≠¶‰π†` `‰øùÂÆàQÂ≠¶‰π†` `ÂèåÂ±Ç‰ºòÂåñ` `Â•ñÂä±ÂáΩÊï∞ÊÅ¢Â§ç` `ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Á¶ªÁ∫øÈÄÜÂº∫ÂåñÂ≠¶‰π†Êó®Âú®‰ªéÂõ∫ÂÆöÊï∞ÊçÆÈõÜ‰∏≠ÊÅ¢Â§çÂ•ñÂä±ÂáΩÊï∞ÔºåÁé∞ÊúâÊñπÊ≥ïÊòìÂèóÂàÜÂ∏ÉÂ§ñÊ≥õÂåñÂΩ±Âìç„ÄÇ
2. BiCQL-MLÈÄöËøáÂèåÂ±Ç‰ºòÂåñÊ°ÜÊû∂ÔºåËÅîÂêàÂ≠¶‰π†‰øùÂÆàQÂáΩÊï∞ÂíåÂ•ñÂä±ÂáΩÊï∞ÔºåÈÅøÂÖçÊòæÂºèÁ≠ñÁï•Â≠¶‰π†„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåBiCQL-MLÂú®Â•ñÂä±ÊÅ¢Â§çÂíåÁ≠ñÁï•ÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÁ¶ªÁ∫øIRLÊñπÊ≥ïÔºåÂÖ∑ÊúâÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫BiCQL-MLÁöÑÁ≠ñÁï•Êó†ÂÖ≥ÁöÑÁ¶ªÁ∫øÈÄÜÂº∫ÂåñÂ≠¶‰π†ÔºàIRLÔºâÁÆóÊ≥ïÔºåÊó®Âú®‰ªÖÂà©Áî®Âõ∫ÂÆöÁöÑÊºîÁ§∫Êï∞ÊçÆÊÅ¢Â§çËÉΩÂ§üËß£Èáä‰∏ìÂÆ∂Ë°å‰∏∫ÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåËÄåÊó†ÈúÄ‰ªª‰ΩïÈ¢ùÂ§ñÁöÑÂú®Á∫ø‰∫§‰∫í„ÄÇBiCQL-MLÂú®‰∏Ä‰∏™ÂèåÂ±ÇÊ°ÜÊû∂‰∏≠ËÅîÂêà‰ºòÂåñÂ•ñÂä±ÂáΩÊï∞Âíå‰øùÂÆàQÂáΩÊï∞Ôºå‰ªéËÄåÈÅøÂÖç‰∫ÜÊòæÂºèÁöÑÁ≠ñÁï•Â≠¶‰π†„ÄÇËØ•ÊñπÊ≥ï‰∫§ÊõøËøõË°å‰ª•‰∏ãÊ≠•È™§ÔºöÔºàiÔºâÂú®ÂΩìÂâçÂ•ñÂä±‰∏ãÔºåÈÄöËøá‰øùÂÆàQÂ≠¶‰π†ÔºàCQLÔºâÂ≠¶‰π†‰øùÂÆàQÂáΩÊï∞ÔºõÔºàiiÔºâÊõ¥Êñ∞Â•ñÂä±ÂèÇÊï∞Ôºå‰ª•ÊúÄÂ§ßÂåñ‰∏ìÂÆ∂Ë°å‰∏∫ÁöÑÈ¢ÑÊúüQÂÄºÔºåÂêåÊó∂ÊäëÂà∂ÂØπÂàÜÂ∏ÉÂ§ñË°å‰∏∫ÁöÑËøáÂ∫¶Ê≥õÂåñ„ÄÇËØ•ËøáÁ®ãÂèØ‰ª•Ë¢´ËßÜ‰∏∫ËΩØ‰ª∑ÂÄºÂåπÈÖçÂéüÂàô‰∏ãÁöÑÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°„ÄÇÊàë‰ª¨Êèê‰æõ‰∫ÜÁêÜËÆ∫‰øùËØÅÔºåËØÅÊòéBiCQL-MLÊî∂ÊïõÂà∞‰∏Ä‰∏™Â•ñÂä±ÂáΩÊï∞ÔºåÂú®ËØ•ÂáΩÊï∞‰∏ãÔºå‰∏ìÂÆ∂Á≠ñÁï•ÊòØËΩØÊúÄ‰ºòÁöÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Ê†áÂáÜÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Âü∫ÂáÜÊµãËØï‰∏≠Ôºå‰∏éÁé∞ÊúâÁöÑÁ¶ªÁ∫øIRLÂü∫Á∫øÁõ∏ÊØîÔºåBiCQL-MLÂú®Â•ñÂä±ÊÅ¢Â§çÂíå‰∏ãÊ∏∏Á≠ñÁï•ÊÄßËÉΩÊñπÈù¢ÂùáÊúâÊâÄÊèêÈ´ò„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁ¶ªÁ∫øÈÄÜÂº∫ÂåñÂ≠¶‰π†ÔºàIRLÔºâÁöÑÁõÆÊ†áÊòØ‰ªéÈùôÊÄÅÁöÑ‰∏ìÂÆ∂ÊºîÁ§∫Êï∞ÊçÆ‰∏≠ÊÅ¢Â§çÊΩúÂú®ÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåËØ•Â•ñÂä±ÂáΩÊï∞ËÉΩÂ§üËß£Èáä‰∏ìÂÆ∂ÁöÑË°å‰∏∫„ÄÇÁé∞ÊúâÁöÑÁ¶ªÁ∫øIRLÊñπÊ≥ïÈù¢‰∏¥‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàòÔºöÂ¶Ç‰ΩïÈÅøÂÖçÂØπÂàÜÂ∏ÉÂ§ñÔºàout-of-distributionÔºâË°å‰∏∫ÁöÑËøáÂ∫¶Ê≥õÂåñÔºåÂõ†‰∏∫Á¶ªÁ∫øÊï∞ÊçÆ‰∏çÂåÖÂê´ÊâÄÊúâÂèØËÉΩÁöÑË°å‰∏∫Áä∂ÊÄÅËΩ¨ÁßªÔºåÂØºËá¥Â≠¶‰π†Âà∞ÁöÑÂ•ñÂä±ÂáΩÊï∞ÂèØËÉΩ‰∏çÂáÜÁ°ÆÔºåËøõËÄåÂΩ±Âìç‰∏ãÊ∏∏Á≠ñÁï•ÁöÑÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöBiCQL-MLÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂèåÂ±Ç‰ºòÂåñÊ°ÜÊû∂ÔºåÂêåÊó∂Â≠¶‰π†‰∏Ä‰∏™‰øùÂÆàÁöÑQÂáΩÊï∞Âíå‰∏Ä‰∏™Â•ñÂä±ÂáΩÊï∞„ÄÇ‰øùÂÆàQÂáΩÊï∞ÈÄöËøáÊÉ©ÁΩöÊú™ËßÅËøáÁöÑÁä∂ÊÄÅ-Âä®‰ΩúÂØπÁöÑQÂÄºÔºåÊù•ÊäëÂà∂ÂØπÂàÜÂ∏ÉÂ§ñË°å‰∏∫ÁöÑËøáÂ∫¶Ê≥õÂåñ„ÄÇÂ•ñÂä±ÂáΩÊï∞ÂàôÈÄöËøáÊúÄÂ§ßÂåñ‰∏ìÂÆ∂Ë°å‰∏∫ÁöÑQÂÄºÊù•Â≠¶‰π†ÔºåÂêåÊó∂ÈÅøÂÖçËøáÂ∫¶Ê≥õÂåñ„ÄÇËøôÁßçËÅîÂêà‰ºòÂåñ‰ΩøÂæóÂ•ñÂä±ÂáΩÊï∞ËÉΩÂ§üÊõ¥Â•ΩÂú∞Ëß£Èáä‰∏ìÂÆ∂Ë°å‰∏∫ÔºåÂπ∂‰∏îËÉΩÂ§üÊ≥õÂåñÂà∞Êú™ËßÅËøáÁöÑÁä∂ÊÄÅ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöBiCQL-MLÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÊòØ‰∏Ä‰∏™ÂèåÂ±Ç‰ºòÂåñËøáÁ®ã„ÄÇÂú®ÂÜÖÂ±ÇÂæ™ÁéØ‰∏≠Ôºå‰ΩøÁî®‰øùÂÆàQÂ≠¶‰π†ÔºàCQLÔºâÁÆóÊ≥ïÔºåÂú®ÂΩìÂâçÂ•ñÂä±ÂáΩÊï∞‰∏ãÂ≠¶‰π†‰∏Ä‰∏™‰øùÂÆàÁöÑQÂáΩÊï∞„ÄÇÂú®Â§ñÂ±ÇÂæ™ÁéØ‰∏≠ÔºåÊõ¥Êñ∞Â•ñÂä±ÂáΩÊï∞ÁöÑÂèÇÊï∞Ôºå‰ª•ÊúÄÂ§ßÂåñ‰∏ìÂÆ∂Ë°å‰∏∫ÁöÑÈ¢ÑÊúüQÂÄºÔºåÂêåÊó∂‰ΩøÁî®Ê≠£ÂàôÂåñÈ°πÊù•ÊÉ©ÁΩöÂØπÂàÜÂ∏ÉÂ§ñË°å‰∏∫ÁöÑËøáÂ∫¶Ê≥õÂåñ„ÄÇËøô‰∏§‰∏™Âæ™ÁéØ‰∫§ÊõøËøõË°åÔºåÁõ¥Âà∞Êî∂Êïõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöBiCQL-MLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞Ü‰øùÂÆàQÂ≠¶‰π†ÔºàCQLÔºâ‰∏éÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°Áõ∏ÁªìÂêàÔºåÂΩ¢Êàê‰∏Ä‰∏™ÂèåÂ±Ç‰ºòÂåñÊ°ÜÊû∂„ÄÇÈÄöËøáCQLÂ≠¶‰π†‰øùÂÆàQÂáΩÊï∞ÔºåÂèØ‰ª•ÊúâÊïàÂú∞ÊäëÂà∂ÂØπÂàÜÂ∏ÉÂ§ñË°å‰∏∫ÁöÑËøáÂ∫¶Ê≥õÂåñÔºå‰ªéËÄåÊèêÈ´òÂ•ñÂä±ÂáΩÊï∞ÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂêåÊó∂ÔºåÈÄöËøáÊúÄÂ§ßÂåñ‰∏ìÂÆ∂Ë°å‰∏∫ÁöÑQÂÄºÔºåÂèØ‰ª•Á°Æ‰øùÂ≠¶‰π†Âà∞ÁöÑÂ•ñÂä±ÂáΩÊï∞ËÉΩÂ§üÂæàÂ•ΩÂú∞Ëß£Èáä‰∏ìÂÆ∂Ë°å‰∏∫„ÄÇËøôÁßçÂèåÂ±Ç‰ºòÂåñÊ°ÜÊû∂ÈÅøÂÖç‰∫ÜÊòæÂºèÁöÑÁ≠ñÁï•Â≠¶‰π†Ôºå‰ªéËÄåÁÆÄÂåñ‰∫ÜÁÆóÊ≥ïÁöÑÂ§çÊùÇÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöBiCQL-MLÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®CQLÁÆóÊ≥ïÂ≠¶‰π†‰øùÂÆàQÂáΩÊï∞ÔºåCQLÈÄöËøáÂú®QÂáΩÊï∞ÁöÑÊçüÂ§±ÂáΩÊï∞‰∏≠Ê∑ªÂä†‰∏Ä‰∏™ÊÉ©ÁΩöÈ°πÔºåÊù•Èôç‰ΩéÊú™ËßÅËøáÁöÑÁä∂ÊÄÅ-Âä®‰ΩúÂØπÁöÑQÂÄº„ÄÇ2) ‰ΩøÁî®ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°Êù•Êõ¥Êñ∞Â•ñÂä±ÂáΩÊï∞ÔºåÁõÆÊ†áÊòØÊúÄÂ§ßÂåñ‰∏ìÂÆ∂Ë°å‰∏∫ÁöÑQÂÄºÔºåÂêåÊó∂‰ΩøÁî®Ê≠£ÂàôÂåñÈ°πÊù•ÊÉ©ÁΩöÂØπÂàÜÂ∏ÉÂ§ñË°å‰∏∫ÁöÑËøáÂ∫¶Ê≥õÂåñ„ÄÇ3) ÂèåÂ±Ç‰ºòÂåñÊ°ÜÊû∂ÔºåÂÜÖÂ±ÇÂæ™ÁéØÊõ¥Êñ∞QÂáΩÊï∞ÔºåÂ§ñÂ±ÇÂæ™ÁéØÊõ¥Êñ∞Â•ñÂä±ÂáΩÊï∞Ôºå‰∏§‰∏™Âæ™ÁéØ‰∫§ÊõøËøõË°åÔºåÁõ¥Âà∞Êî∂Êïõ„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞ÔºåÊ≠§Â§ÑÊú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBiCQL-MLÂú®Ê†áÂáÜÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Âü∫ÂáÜÊµãËØï‰∏≠Ôºå‰∏éÁé∞ÊúâÁöÑÁ¶ªÁ∫øIRLÂü∫Á∫øÁõ∏ÊØîÔºåÂú®Â•ñÂä±ÊÅ¢Â§çÂíå‰∏ãÊ∏∏Á≠ñÁï•ÊÄßËÉΩÊñπÈù¢ÂùáÊúâÊâÄÊèêÈ´ò„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶Êú™Áü•Ôºå‰ΩÜÊëòË¶Å‰∏≠ÊòéÁ°ÆÊåáÂá∫BiCQL-ML‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåË°®ÊòéÂÖ∂ÂÖ∑ÊúâÊòæËëóÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

BiCQL-MLÂèØÂ∫îÁî®‰∫éÂåªÁñó„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫Á≠âÈ¢ÜÂüüÔºåÂú®Ëøô‰∫õÈ¢ÜÂüü‰∏≠ÔºåÈÄöÂ∏∏Â≠òÂú®Â§ßÈáèÁöÑ‰∏ìÂÆ∂ÊºîÁ§∫Êï∞ÊçÆÔºå‰ΩÜ‰∏éÁéØÂ¢ÉÁöÑÂú®Á∫ø‰∫§‰∫íÊàêÊú¨ÂæàÈ´ò„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Âà©Áî®BiCQL-ML‰ªéÂåªÁîüÊâãÊúØÂΩïÂÉè‰∏≠Â≠¶‰π†Â•ñÂä±ÂáΩÊï∞Ôºå‰ªéËÄåËÆ≠ÁªÉÊú∫Âô®‰∫∫ËæÖÂä©ÊâãÊúØÔºõÊàñËÄÖ‰ªé‰∫∫Á±ªÈ©æÈ©∂Êï∞ÊçÆ‰∏≠Â≠¶‰π†Â•ñÂä±ÂáΩÊï∞Ôºå‰ªéËÄåÊîπËøõËá™Âä®È©æÈ©∂Á≥ªÁªüÁöÑÂÜ≥Á≠ñËÉΩÂäõ„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÈôç‰ΩéÂº∫ÂåñÂ≠¶‰π†ÁöÑÂ∫îÁî®Èó®ÊßõÔºåÂπ∂ÊèêÈ´òÂÖ∂Âú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®ÊïàÊûú„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.

