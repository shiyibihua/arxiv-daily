---
layout: default
title: BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning
---

# BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning

**arXiv**: [2511.22210v1](https://arxiv.org/abs/2511.22210) | [PDF](https://arxiv.org/pdf/2511.22210.pdf)

**ä½œè€…**: Junsung Park

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-27

**å¤‡æ³¨**: 8 pages, 3 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBiCQL-MLï¼Œé€šè¿‡åŒå±‚ä¿å®ˆQå­¦ä¹ è§£å†³ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±å‡½æ•°æ¢å¤é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ ` `ä¿å®ˆQå­¦ä¹ ` `åŒå±‚ä¼˜åŒ–` `å¥–åŠ±å‡½æ•°æ¢å¤` `æœ€å¤§ä¼¼ç„¶ä¼°è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ æ—¨åœ¨ä»Žå›ºå®šæ•°æ®é›†ä¸­æ¢å¤å¥–åŠ±å‡½æ•°ï¼ŒçŽ°æœ‰æ–¹æ³•æ˜“å—åˆ†å¸ƒå¤–æ³›åŒ–å½±å“ã€‚
2. BiCQL-MLé€šè¿‡åŒå±‚ä¼˜åŒ–æ¡†æž¶ï¼Œè”åˆå­¦ä¹ ä¿å®ˆQå‡½æ•°å’Œå¥–åŠ±å‡½æ•°ï¼Œé¿å…æ˜¾å¼ç­–ç•¥å­¦ä¹ ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒBiCQL-MLåœ¨å¥–åŠ±æ¢å¤å’Œç­–ç•¥æ€§èƒ½ä¸Šä¼˜äºŽçŽ°æœ‰ç¦»çº¿IRLæ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBiCQL-MLçš„ç­–ç•¥æ— å…³çš„ç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰ç®—æ³•ï¼Œæ—¨åœ¨ä»…åˆ©ç”¨å›ºå®šçš„æ¼”ç¤ºæ•°æ®æ¢å¤èƒ½å¤Ÿè§£é‡Šä¸“å®¶è¡Œä¸ºçš„å¥–åŠ±å‡½æ•°ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„åœ¨çº¿äº¤äº’ã€‚BiCQL-MLåœ¨ä¸€ä¸ªåŒå±‚æ¡†æž¶ä¸­è”åˆä¼˜åŒ–å¥–åŠ±å‡½æ•°å’Œä¿å®ˆQå‡½æ•°ï¼Œä»Žè€Œé¿å…äº†æ˜¾å¼çš„ç­–ç•¥å­¦ä¹ ã€‚è¯¥æ–¹æ³•äº¤æ›¿è¿›è¡Œä»¥ä¸‹æ­¥éª¤ï¼šï¼ˆiï¼‰åœ¨å½“å‰å¥–åŠ±ä¸‹ï¼Œé€šè¿‡ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰å­¦ä¹ ä¿å®ˆQå‡½æ•°ï¼›ï¼ˆiiï¼‰æ›´æ–°å¥–åŠ±å‚æ•°ï¼Œä»¥æœ€å¤§åŒ–ä¸“å®¶è¡Œä¸ºçš„é¢„æœŸQå€¼ï¼ŒåŒæ—¶æŠ‘åˆ¶å¯¹åˆ†å¸ƒå¤–è¡Œä¸ºçš„è¿‡åº¦æ³›åŒ–ã€‚è¯¥è¿‡ç¨‹å¯ä»¥è¢«è§†ä¸ºè½¯ä»·å€¼åŒ¹é…åŽŸåˆ™ä¸‹çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºä¿è¯ï¼Œè¯æ˜ŽBiCQL-MLæ”¶æ•›åˆ°ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œåœ¨è¯¥å‡½æ•°ä¸‹ï¼Œä¸“å®¶ç­–ç•¥æ˜¯è½¯æœ€ä¼˜çš„ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œåœ¨æ ‡å‡†ç¦»çº¿å¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸ŽçŽ°æœ‰çš„ç¦»çº¿IRLåŸºçº¿ç›¸æ¯”ï¼ŒBiCQL-MLåœ¨å¥–åŠ±æ¢å¤å’Œä¸‹æ¸¸ç­–ç•¥æ€§èƒ½æ–¹é¢å‡æœ‰æ‰€æé«˜ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰çš„ç›®æ ‡æ˜¯ä»Žé™æ€çš„ä¸“å®¶æ¼”ç¤ºæ•°æ®ä¸­æ¢å¤æ½œåœ¨çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å¥–åŠ±å‡½æ•°èƒ½å¤Ÿè§£é‡Šä¸“å®¶çš„è¡Œä¸ºã€‚çŽ°æœ‰çš„ç¦»çº¿IRLæ–¹æ³•é¢ä¸´ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå¦‚ä½•é¿å…å¯¹åˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰è¡Œä¸ºçš„è¿‡åº¦æ³›åŒ–ï¼Œå› ä¸ºç¦»çº¿æ•°æ®ä¸åŒ…å«æ‰€æœ‰å¯èƒ½çš„è¡Œä¸ºçŠ¶æ€è½¬ç§»ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„å¥–åŠ±å‡½æ•°å¯èƒ½ä¸å‡†ç¡®ï¼Œè¿›è€Œå½±å“ä¸‹æ¸¸ç­–ç•¥çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBiCQL-MLçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŒå±‚ä¼˜åŒ–æ¡†æž¶ï¼ŒåŒæ—¶å­¦ä¹ ä¸€ä¸ªä¿å®ˆçš„Qå‡½æ•°å’Œä¸€ä¸ªå¥–åŠ±å‡½æ•°ã€‚ä¿å®ˆQå‡½æ•°é€šè¿‡æƒ©ç½šæœªè§è¿‡çš„çŠ¶æ€-åŠ¨ä½œå¯¹çš„Qå€¼ï¼Œæ¥æŠ‘åˆ¶å¯¹åˆ†å¸ƒå¤–è¡Œä¸ºçš„è¿‡åº¦æ³›åŒ–ã€‚å¥–åŠ±å‡½æ•°åˆ™é€šè¿‡æœ€å¤§åŒ–ä¸“å®¶è¡Œä¸ºçš„Qå€¼æ¥å­¦ä¹ ï¼ŒåŒæ—¶é¿å…è¿‡åº¦æ³›åŒ–ã€‚è¿™ç§è”åˆä¼˜åŒ–ä½¿å¾—å¥–åŠ±å‡½æ•°èƒ½å¤Ÿæ›´å¥½åœ°è§£é‡Šä¸“å®¶è¡Œä¸ºï¼Œå¹¶ä¸”èƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§è¿‡çš„çŠ¶æ€ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šBiCQL-MLçš„æ•´ä½“æ¡†æž¶æ˜¯ä¸€ä¸ªåŒå±‚ä¼˜åŒ–è¿‡ç¨‹ã€‚åœ¨å†…å±‚å¾ªçŽ¯ä¸­ï¼Œä½¿ç”¨ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰ç®—æ³•ï¼Œåœ¨å½“å‰å¥–åŠ±å‡½æ•°ä¸‹å­¦ä¹ ä¸€ä¸ªä¿å®ˆçš„Qå‡½æ•°ã€‚åœ¨å¤–å±‚å¾ªçŽ¯ä¸­ï¼Œæ›´æ–°å¥–åŠ±å‡½æ•°çš„å‚æ•°ï¼Œä»¥æœ€å¤§åŒ–ä¸“å®¶è¡Œä¸ºçš„é¢„æœŸQå€¼ï¼ŒåŒæ—¶ä½¿ç”¨æ­£åˆ™åŒ–é¡¹æ¥æƒ©ç½šå¯¹åˆ†å¸ƒå¤–è¡Œä¸ºçš„è¿‡åº¦æ³›åŒ–ã€‚è¿™ä¸¤ä¸ªå¾ªçŽ¯äº¤æ›¿è¿›è¡Œï¼Œç›´åˆ°æ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šBiCQL-MLçš„å…³é”®åˆ›æ–°åœ¨äºŽå°†ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰ä¸Žæœ€å¤§ä¼¼ç„¶ä¼°è®¡ç›¸ç»“åˆï¼Œå½¢æˆä¸€ä¸ªåŒå±‚ä¼˜åŒ–æ¡†æž¶ã€‚é€šè¿‡CQLå­¦ä¹ ä¿å®ˆQå‡½æ•°ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æŠ‘åˆ¶å¯¹åˆ†å¸ƒå¤–è¡Œä¸ºçš„è¿‡åº¦æ³›åŒ–ï¼Œä»Žè€Œæé«˜å¥–åŠ±å‡½æ•°çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡æœ€å¤§åŒ–ä¸“å®¶è¡Œä¸ºçš„Qå€¼ï¼Œå¯ä»¥ç¡®ä¿å­¦ä¹ åˆ°çš„å¥–åŠ±å‡½æ•°èƒ½å¤Ÿå¾ˆå¥½åœ°è§£é‡Šä¸“å®¶è¡Œä¸ºã€‚è¿™ç§åŒå±‚ä¼˜åŒ–æ¡†æž¶é¿å…äº†æ˜¾å¼çš„ç­–ç•¥å­¦ä¹ ï¼Œä»Žè€Œç®€åŒ–äº†ç®—æ³•çš„å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šBiCQL-MLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨CQLç®—æ³•å­¦ä¹ ä¿å®ˆQå‡½æ•°ï¼ŒCQLé€šè¿‡åœ¨Qå‡½æ•°çš„æŸå¤±å‡½æ•°ä¸­æ·»åŠ ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œæ¥é™ä½Žæœªè§è¿‡çš„çŠ¶æ€-åŠ¨ä½œå¯¹çš„Qå€¼ã€‚2) ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¥æ›´æ–°å¥–åŠ±å‡½æ•°ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸“å®¶è¡Œä¸ºçš„Qå€¼ï¼ŒåŒæ—¶ä½¿ç”¨æ­£åˆ™åŒ–é¡¹æ¥æƒ©ç½šå¯¹åˆ†å¸ƒå¤–è¡Œä¸ºçš„è¿‡åº¦æ³›åŒ–ã€‚3) åŒå±‚ä¼˜åŒ–æ¡†æž¶ï¼Œå†…å±‚å¾ªçŽ¯æ›´æ–°Qå‡½æ•°ï¼Œå¤–å±‚å¾ªçŽ¯æ›´æ–°å¥–åŠ±å‡½æ•°ï¼Œä¸¤ä¸ªå¾ªçŽ¯äº¤æ›¿è¿›è¡Œï¼Œç›´åˆ°æ”¶æ•›ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œæ­¤å¤„æœªçŸ¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒBiCQL-MLåœ¨æ ‡å‡†ç¦»çº¿å¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸ŽçŽ°æœ‰çš„ç¦»çº¿IRLåŸºçº¿ç›¸æ¯”ï¼Œåœ¨å¥–åŠ±æ¢å¤å’Œä¸‹æ¸¸ç­–ç•¥æ€§èƒ½æ–¹é¢å‡æœ‰æ‰€æé«˜ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­æ˜Žç¡®æŒ‡å‡ºBiCQL-MLä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œè¡¨æ˜Žå…¶å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

BiCQL-MLå¯åº”ç”¨äºŽåŒ»ç–—ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººç­‰é¢†åŸŸï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œé€šå¸¸å­˜åœ¨å¤§é‡çš„ä¸“å®¶æ¼”ç¤ºæ•°æ®ï¼Œä½†ä¸ŽçŽ¯å¢ƒçš„åœ¨çº¿äº¤äº’æˆæœ¬å¾ˆé«˜ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åˆ©ç”¨BiCQL-MLä»ŽåŒ»ç”Ÿæ‰‹æœ¯å½•åƒä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œä»Žè€Œè®­ç»ƒæœºå™¨äººè¾…åŠ©æ‰‹æœ¯ï¼›æˆ–è€…ä»Žäººç±»é©¾é©¶æ•°æ®ä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œä»Žè€Œæ”¹è¿›è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å†³ç­–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºŽé™ä½Žå¼ºåŒ–å­¦ä¹ çš„åº”ç”¨é—¨æ§›ï¼Œå¹¶æé«˜å…¶åœ¨å®žé™…åœºæ™¯ä¸­çš„åº”ç”¨æ•ˆæžœã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.

