---
layout: default
title: Improving Robotic Manipulation Robustness via NICE Scene Surgery
---

# Improving Robotic Manipulation Robustness via NICE Scene Surgery

**arXiv**: [2511.22777v1](https://arxiv.org/abs/2511.22777) | [PDF](https://arxiv.org/pdf/2511.22777.pdf)

**ä½œè€…**: Sajjad Pakdamansavoji, Mozhgan Pourkeshavarz, Adam Sigal, Zhiyuan Li, Rui Heng Yang, Amir Rasouli

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-27

**å¤‡æ³¨**: 11 figures, 3 tables

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**NICEåœºæ™¯æ‰‹æœ¯ï¼šåˆ©ç”¨è‡ªç„¶å›¾åƒä¿®å¤å¢žå¼ºæœºå™¨äººæ“ä½œçš„é²æ£’æ€§**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰é²æ£’æ€§` `æ•°æ®å¢žå¼º` `å›¾åƒç”Ÿæˆ` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `åœºæ™¯ç†è§£` `åˆ†å¸ƒå¤–æ³›åŒ–` `æ¨¡ä»¿å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°å®žä¸–ç•Œä¸­æœºå™¨äººæ“ä½œé¢ä¸´è§†è§‰å¹²æ‰°ï¼ŒçŽ°æœ‰æ–¹æ³•éš¾ä»¥ä¿è¯ç­–ç•¥çš„é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚
2. NICEæ¡†æž¶åˆ©ç”¨å›¾åƒç”Ÿæˆå’Œå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼Œé€šè¿‡åœºæ™¯ç¼–è¾‘å¢žåŠ è§†è§‰å¤šæ ·æ€§ï¼Œç¼©å°åˆ†å¸ƒå¤–å·®è·ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒNICEæ˜¾è‘—æå‡äº†æœºå™¨äººæ“ä½œçš„å‡†ç¡®çŽ‡ã€æˆåŠŸçŽ‡å’Œå®‰å…¨æ€§ï¼Œæ— éœ€é¢å¤–æ•°æ®æˆ–æ¨¡åž‹è®­ç»ƒã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨çŽ°å®žçŽ¯å¢ƒä¸­ï¼Œè§†è§‰å¹²æ‰°ä¼šæ˜¾è‘—é™ä½Žæœºå™¨äººæ“ä½œç­–ç•¥çš„æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”å¯æ‰©å±•çš„æ¡†æž¶ï¼Œå³è‡ªç„¶åœºæ™¯ä¿®å¤å¢žå¼ºï¼ˆNICEï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨çŽ°æœ‰æ¼”ç¤ºæž„å»ºæ–°çš„ç»éªŒï¼Œå¢žåŠ è§†è§‰å¤šæ ·æ€§ï¼Œä»Žè€Œæœ€å¤§é™åº¦åœ°å‡å°‘æ¨¡ä»¿å­¦ä¹ ä¸­çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰å·®è·ã€‚NICEåˆ©ç”¨å›¾åƒç”Ÿæˆæ¡†æž¶å’Œå¤§åž‹è¯­è¨€æ¨¡åž‹æ‰§è¡Œä¸‰ç§ç¼–è¾‘æ“ä½œï¼šå¯¹è±¡æ›¿æ¢ã€é£Žæ ¼è½¬æ¢å’Œç§»é™¤å¹²æ‰°ï¼ˆéžç›®æ ‡ï¼‰å¯¹è±¡ã€‚è¿™äº›æ›´æ”¹åœ¨ä¸é˜»ç¢ç›®æ ‡å¯¹è±¡çš„æƒ…å†µä¸‹ä¿æŒç©ºé—´å…³ç³»ï¼Œå¹¶ä¿æŒåŠ¨ä½œæ ‡ç­¾çš„ä¸€è‡´æ€§ã€‚ä¸Žä»¥å¾€çš„æ–¹æ³•ä¸åŒï¼ŒNICEä¸éœ€è¦é¢å¤–çš„æ•°æ®é‡‡é›†ã€æ¨¡æ‹Ÿå™¨è®¿é—®æˆ–è‡ªå®šä¹‰æ¨¡åž‹è®­ç»ƒï¼Œä½¿å…¶æ˜“äºŽåº”ç”¨äºŽçŽ°æœ‰çš„æœºå™¨äººæ•°æ®é›†ã€‚åœ¨çœŸå®žåœºæ™¯ä¸­ï¼Œå±•ç¤ºäº†NICEåœ¨ç”Ÿæˆé€¼çœŸåœºæ™¯å¢žå¼ºæ–¹é¢çš„èƒ½åŠ›ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨NICEæ•°æ®æ¥å¾®è°ƒè§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰ä»¥è¿›è¡Œç©ºé—´å¯ä¾›æ€§é¢„æµ‹ï¼Œä»¥åŠè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥ä»¥è¿›è¡Œå¯¹è±¡æ“ä½œã€‚è¯„ä¼°è¡¨æ˜Žï¼ŒNICEæˆåŠŸåœ°æœ€å¤§é™åº¦åœ°å‡å°‘äº†OODå·®è·ï¼Œä»Žè€Œä½¿é«˜åº¦æ‚ä¹±åœºæ™¯ä¸­çš„å¯ä¾›æ€§é¢„æµ‹å‡†ç¡®çŽ‡æé«˜äº†20%ä»¥ä¸Šã€‚å¯¹äºŽæ“ä½œä»»åŠ¡ï¼Œåœ¨ä¸åŒæ•°é‡çš„å¹²æ‰°ç‰©çŽ¯å¢ƒä¸­è¿›è¡Œæµ‹è¯•æ—¶ï¼ŒæˆåŠŸçŽ‡å¹³å‡æé«˜äº†11%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æé«˜äº†è§†è§‰é²æ£’æ€§ï¼Œå°†ç›®æ ‡æ··æ·†é™ä½Žäº†6%ï¼Œå¹¶é€šè¿‡é™ä½Ž7%çš„ç¢°æ’žçŽ‡æ¥å¢žå¼ºå®‰å…¨æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³çœŸå®žåœºæ™¯ä¸­æœºå™¨äººæ“ä½œå› è§†è§‰å¹²æ‰°ç‰©å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çœŸå®žæ•°æ®æˆ–ä¾èµ–æ¨¡æ‹ŸçŽ¯å¢ƒï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥é€‚åº”å¤æ‚å¤šå˜çš„çœŸå®žçŽ¯å¢ƒï¼Œå¯¼è‡´æœºå™¨äººæ“ä½œçš„é²æ£’æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å›¾åƒä¿®å¤å’Œç”ŸæˆæŠ€æœ¯ï¼Œåœ¨çŽ°æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œåœºæ™¯å¢žå¼ºï¼Œå¢žåŠ è§†è§‰å¤šæ ·æ€§ï¼Œä»Žè€Œæé«˜æ¨¡åž‹å¯¹åˆ†å¸ƒå¤–æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¯¹åœºæ™¯è¿›è¡Œå¯¹è±¡æ›¿æ¢ã€é£Žæ ¼è½¬æ¢å’Œç§»é™¤å¹²æ‰°ç‰©ç­‰æ“ä½œï¼Œæ¨¡æ‹ŸçœŸå®žä¸–ç•Œä¸­å¯èƒ½å‡ºçŽ°çš„å„ç§è§†è§‰å¹²æ‰°ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çŽ¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šNICEæ¡†æž¶ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š1) åœºæ™¯ç†è§£ï¼šåˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹å’Œè§†è§‰æ¨¡åž‹ç†è§£åœºæ™¯å†…å®¹ï¼Œè¯†åˆ«ç›®æ ‡å¯¹è±¡å’Œå¹²æ‰°ç‰©ï¼›2) åœºæ™¯ç¼–è¾‘ï¼šåˆ©ç”¨å›¾åƒç”Ÿæˆæ¨¡åž‹å¯¹åœºæ™¯è¿›è¡Œç¼–è¾‘ï¼ŒåŒ…æ‹¬å¯¹è±¡æ›¿æ¢ã€é£Žæ ¼è½¬æ¢å’Œç§»é™¤å¹²æ‰°ç‰©ç­‰æ“ä½œï¼›3) æ•°æ®å¢žå¼ºï¼šå°†ç¼–è¾‘åŽçš„å›¾åƒæ·»åŠ åˆ°è®­ç»ƒæ•°æ®é›†ä¸­ï¼Œç”¨äºŽè®­ç»ƒæˆ–å¾®è°ƒæœºå™¨äººæ“ä½œç­–ç•¥ã€‚æ•´ä¸ªæµç¨‹æ— éœ€é¢å¤–çš„æœºå™¨äººæ•°æ®é‡‡é›†ã€æ¨¡æ‹Ÿå™¨è®¿é—®æˆ–è‡ªå®šä¹‰æ¨¡åž‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šNICEçš„å…³é”®åˆ›æ–°åœ¨äºŽåˆ©ç”¨å›¾åƒç”Ÿæˆå’Œå¤§åž‹è¯­è¨€æ¨¡åž‹è¿›è¡Œåœºæ™¯å¢žå¼ºï¼Œä»Žè€Œåœ¨ä¸å¢žåŠ æ•°æ®é‡‡é›†æˆæœ¬çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜æœºå™¨äººæ“ä½œçš„é²æ£’æ€§ã€‚ä¸Žä¼ ç»Ÿçš„æ•°æ®å¢žå¼ºæ–¹æ³•ç›¸æ¯”ï¼ŒNICEèƒ½å¤Ÿç”Ÿæˆæ›´é€¼çœŸã€æ›´å¤šæ ·åŒ–çš„åœºæ™¯ï¼Œæ›´æœ‰æ•ˆåœ°æ¨¡æ‹ŸçœŸå®žä¸–ç•Œä¸­çš„è§†è§‰å¹²æ‰°ã€‚æ­¤å¤–ï¼ŒNICEæ¡†æž¶å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯ä»¥æ–¹ä¾¿åœ°åº”ç”¨äºŽä¸åŒçš„æœºå™¨äººæ“ä½œä»»åŠ¡å’Œæ•°æ®é›†ã€‚

**å…³é”®è®¾è®¡**ï¼šNICEæ¡†æž¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼ˆå¦‚Stable Diffusionï¼‰è¿›è¡Œåœºæ™¯ç¼–è¾‘ï¼Œä¿è¯ç”Ÿæˆå›¾åƒçš„é€¼çœŸåº¦ï¼›2) åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆå¦‚GPT-3ï¼‰è¿›è¡Œåœºæ™¯ç†è§£å’Œå¯¹è±¡è¯†åˆ«ï¼Œæé«˜åœºæ™¯ç¼–è¾‘çš„å‡†ç¡®æ€§ï¼›3) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä¿è¯åœºæ™¯ç¼–è¾‘åŽçš„å›¾åƒä¸ŽåŽŸå§‹å›¾åƒåœ¨è¯­ä¹‰ä¸Šçš„ä¸€è‡´æ€§ï¼›4) é€šè¿‡æŽ§åˆ¶åœºæ™¯ç¼–è¾‘çš„å¼ºåº¦å’Œé¢‘çŽ‡ï¼Œé¿å…å¼•å…¥è¿‡å¤šçš„å™ªå£°ï¼Œå½±å“æ¨¡åž‹çš„è®­ç»ƒæ•ˆæžœã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒNICEæ¡†æž¶èƒ½å¤Ÿæ˜¾è‘—æé«˜æœºå™¨äººæ“ä½œçš„æ€§èƒ½ã€‚åœ¨å¯ä¾›æ€§é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒNICEä½¿å‡†ç¡®çŽ‡æé«˜äº†20%ä»¥ä¸Šã€‚åœ¨æ“ä½œä»»åŠ¡ä¸­ï¼ŒæˆåŠŸçŽ‡å¹³å‡æé«˜äº†11%ï¼Œç›®æ ‡æ··æ·†é™ä½Žäº†6%ï¼Œç¢°æ’žçŽ‡é™ä½Žäº†7%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒNICEèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æœºå™¨äººæ“ä½œçš„é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººæ“ä½œçš„é²æ£’æ€§å’Œå®‰å…¨æ€§ï¼Œå¯ä»¥é™ä½Žç”Ÿäº§æˆæœ¬ã€æé«˜å·¥ä½œæ•ˆçŽ‡ï¼Œå¹¶å‡å°‘äººä¸ºå¹²é¢„ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºŽæ›´å¤æ‚çš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªä¸»å¯¼èˆªã€ç›®æ ‡æœç´¢å’Œäººæœºåä½œç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.
>   Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.

