---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-06-03
---

# cs.ROï¼ˆ2025-06-03ï¼‰

ğŸ“Š å…± **19** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (10)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250602955v1-uniconflow-a-unified-constrained-generalization-framework-for-certif.html">UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models</a></td>
  <td>æå‡ºUniConFlowä»¥è§£å†³å¤šçº¦æŸè¿åŠ¨è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion planning</span> <span class="paper-tag">flow matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02955v1" data-paper-url="./papers/250602955v1-uniconflow-a-unified-constrained-generalization-framework-for-certif.html" onclick="toggleFavorite(this, '2506.02955v1', 'UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250603270v2-grounded-vision-language-interpreter-for-integrated-task-and-motion-.html">Grounded Vision-Language Interpreter for Integrated Task and Motion Planning</a></td>
  <td>æå‡ºViLaIn-TAMPä»¥è§£å†³æœºå™¨äººè§„åˆ’çš„å®‰å…¨æ€§ä¸å¯è§£é‡Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion planning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03270v2" data-paper-url="./papers/250603270v2-grounded-vision-language-interpreter-for-integrated-task-and-motion-.html" onclick="toggleFavorite(this, '2506.03270v2', 'Grounded Vision-Language Interpreter for Integrated Task and Motion Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250602507v3-aura-autonomous-upskilling-with-retrieval-augmented-agents.html">AURA: Autonomous Upskilling with Retrieval-Augmented Agents</a></td>
  <td>æå‡ºAURAæ¡†æ¶ä»¥è‡ªåŠ¨åŒ–è®¾è®¡æœºå™¨äººå¼ºåŒ–å­¦ä¹ è¯¾ç¨‹</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">humanoid locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02507v3" data-paper-url="./papers/250602507v3-aura-autonomous-upskilling-with-retrieval-augmented-agents.html" onclick="toggleFavorite(this, '2506.02507v3', 'AURA: Autonomous Upskilling with Retrieval-Augmented Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250602835v1-high-speed-control-and-navigation-for-quadrupedal-robots-on-complex-.html">High-speed control and navigation for quadrupedal robots on complex and discrete terrain</a></td>
  <td>æå‡ºå±‚æ¬¡åŒ–å¯¼èˆªç®¡é“ä»¥è§£å†³å››è¶³æœºå™¨äººé«˜é€Ÿåº¦æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged robot</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02835v1" data-paper-url="./papers/250602835v1-high-speed-control-and-navigation-for-quadrupedal-robots-on-complex-.html" onclick="toggleFavorite(this, '2506.02835v1', 'High-speed control and navigation for quadrupedal robots on complex and discrete terrain')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250602353v2-savor-skill-affordance-learning-from-visuo-haptic-perception-for-rob.html">SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for Robot-Assisted Bite Acquisition</a></td>
  <td>æå‡ºSAVORä»¥è§£å†³æœºå™¨äººè¾…åŠ©è¿›é£Ÿä¸­çš„å’¬å–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">affordance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02353v2" data-paper-url="./papers/250602353v2-savor-skill-affordance-learning-from-visuo-haptic-perception-for-rob.html" onclick="toggleFavorite(this, '2506.02353v2', 'SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for Robot-Assisted Bite Acquisition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250603362v1-robustness-aware-tool-selection-and-manipulation-planning-with-learn.html">Robustness-Aware Tool Selection and Manipulation Planning with Learned Energy-Informed Guidance</a></td>
  <td>æå‡ºä¸€ç§é²æ£’æ€§æ„è¯†çš„å·¥å…·é€‰æ‹©ä¸æ“ä½œè§„åˆ’æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03362v1" data-paper-url="./papers/250603362v1-robustness-aware-tool-selection-and-manipulation-planning-with-learn.html" onclick="toggleFavorite(this, '2506.03362v1', 'Robustness-Aware Tool Selection and Manipulation Planning with Learned Energy-Informed Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250606361v2-tactile-mnist-benchmarking-active-tactile-perception.html">Tactile MNIST: Benchmarking Active Tactile Perception</a></td>
  <td>æå‡ºTactile MNISTåŸºå‡†ä»¥è§£å†³ä¸»åŠ¨è§¦è§‰æ„ŸçŸ¥æ ‡å‡†åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06361v2" data-paper-url="./papers/250606361v2-tactile-mnist-benchmarking-active-tactile-perception.html" onclick="toggleFavorite(this, '2506.06361v2', 'Tactile MNIST: Benchmarking Active Tactile Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250602768v1-geometric-visual-servo-via-optimal-transport.html">Geometric Visual Servo Via Optimal Transport</a></td>
  <td>æå‡ºå‡ ä½•è§†è§‰ä¼ºæœæ§åˆ¶æ³•ä»¥è§£å†³æœºå™¨äººæ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02768v1" data-paper-url="./papers/250602768v1-geometric-visual-servo-via-optimal-transport.html" onclick="toggleFavorite(this, '2506.02768v1', 'Geometric Visual Servo Via Optimal Transport')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250602622v1-horus-a-mixed-reality-interface-for-managing-teams-of-mobile-robots.html">HORUS: A Mixed Reality Interface for Managing Teams of Mobile Robots</a></td>
  <td>æå‡ºHORUSä»¥è§£å†³å¤šç§»åŠ¨æœºå™¨äººç®¡ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">teleoperation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02622v1" data-paper-url="./papers/250602622v1-horus-a-mixed-reality-interface-for-managing-teams-of-mobile-robots.html" onclick="toggleFavorite(this, '2506.02622v1', 'HORUS: A Mixed Reality Interface for Managing Teams of Mobile Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250602606v4-multi-layered-autonomy-and-ai-ecologies-in-robotic-art-installations.html">Multi Layered Autonomy and AI Ecologies in Robotic Art Installations</a></td>
  <td>æå‡ºå¤šå±‚æ¬¡è‡ªä¸»æ€§ä¸AIç”Ÿæ€ä»¥æ¢è®¨æœºå™¨äººè‰ºæœ¯åˆ›ä½œä¸­çš„è´£ä»»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02606v4" data-paper-url="./papers/250602606v4-multi-layered-autonomy-and-ai-ecologies-in-robotic-art-installations.html" onclick="toggleFavorite(this, '2506.02606v4', 'Multi Layered Autonomy and AI Ecologies in Robotic Art Installations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250603046v1-eden-entorhinal-driven-egocentric-navigation-toward-robotic-deployme.html">EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment</a></td>
  <td>æå‡ºEDENæ¡†æ¶ä»¥è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ å¯¼èˆªçš„è„†å¼±æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">PPO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03046v1" data-paper-url="./papers/250603046v1-eden-entorhinal-driven-egocentric-navigation-toward-robotic-deployme.html" onclick="toggleFavorite(this, '2506.03046v1', 'EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250602618v1-rodrigues-network-for-learning-robot-actions.html">Rodrigues Network for Learning Robot Actions</a></td>
  <td>æå‡ºç¥ç»Rodriguesç®—å­ä»¥è§£å†³æœºå™¨äººåŠ¨ä½œå­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">diffusion policy</span> <span class="paper-tag">hand reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02618v1" data-paper-url="./papers/250602618v1-rodrigues-network-for-learning-robot-actions.html" onclick="toggleFavorite(this, '2506.02618v1', 'Rodrigues Network for Learning Robot Actions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250602746v1-solving-the-pod-repositioning-problem-with-deep-reinforced-adaptive-.html">Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search</a></td>
  <td>æå‡ºæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸è‡ªé€‚åº”å¤§é‚»åŸŸæœç´¢ç»“åˆçš„æ–¹æ³•è§£å†³Podé‡å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">DRL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02746v1" data-paper-url="./papers/250602746v1-solving-the-pod-repositioning-problem-with-deep-reinforced-adaptive-.html" onclick="toggleFavorite(this, '2506.02746v1', 'Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250602593v1-a-hybrid-approach-to-indoor-social-navigation-integrating-reactive-l.html">A Hybrid Approach to Indoor Social Navigation: Integrating Reactive Local Planning and Proactive Global Planning</a></td>
  <td>æå‡ºæ··åˆå¯¼èˆªæ–¹æ³•ä»¥è§£å†³å®¤å†…ç¤¾äº¤å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">DRL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02593v1" data-paper-url="./papers/250602593v1-a-hybrid-approach-to-indoor-social-navigation-integrating-reactive-l.html" onclick="toggleFavorite(this, '2506.02593v1', 'A Hybrid Approach to Indoor Social Navigation: Integrating Reactive Local Planning and Proactive Global Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250602556v2-sign-language-towards-sign-understanding-for-robot-autonomy.html">Sign Language: Towards Sign Understanding for Robot Autonomy</a></td>
  <td>æå‡ºå¯¼èˆªæ ‡å¿—ç†è§£æ–¹æ³•ä»¥æå‡æœºå™¨äººè‡ªä¸»å¯¼èˆªèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">privileged information</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02556v2" data-paper-url="./papers/250602556v2-sign-language-towards-sign-understanding-for-robot-autonomy.html" onclick="toggleFavorite(this, '2506.02556v2', 'Sign Language: Towards Sign Understanding for Robot Autonomy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250602860v1-tru-pomdp-task-planning-under-uncertainty-via-tree-of-hypotheses-and.html">Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs</a></td>
  <td>æå‡ºTru-POMDPä»¥è§£å†³å®¶åº­æœåŠ¡æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02860v1" data-paper-url="./papers/250602860v1-tru-pomdp-task-planning-under-uncertainty-via-tree-of-hypotheses-and.html" onclick="toggleFavorite(this, '2506.02860v1', 'Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250602676v1-sight-guide-a-wearable-assistive-perception-and-navigation-system-fo.html">Sight Guide: A Wearable Assistive Perception and Navigation System for the Vision Assistance Race in the Cybathlon 2024</a></td>
  <td>æå‡ºSight Guideä»¥è§£å†³è§†è§‰éšœç¢è€…å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02676v1" data-paper-url="./papers/250602676v1-sight-guide-a-wearable-assistive-perception-and-navigation-system-fo.html" onclick="toggleFavorite(this, '2506.02676v1', 'Sight Guide: A Wearable Assistive Perception and Navigation System for the Vision Assistance Race in the Cybathlon 2024')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250602373v1-olfactory-inertial-odometry-methodology-for-effective-robot-navigati.html">Olfactory Inertial Odometry: Methodology for Effective Robot Navigation by Scent</a></td>
  <td>æå‡ºå—…è§‰æƒ¯æ€§é‡Œç¨‹è®¡ä»¥è§£å†³æœºå™¨äººå—…è§‰å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VIO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02373v1" data-paper-url="./papers/250602373v1-olfactory-inertial-odometry-methodology-for-effective-robot-navigati.html" onclick="toggleFavorite(this, '2506.02373v1', 'Olfactory Inertial Odometry: Methodology for Effective Robot Navigation by Scent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250603350v1-adversarial-attacks-on-robotic-vision-language-action-models.html">Adversarial Attacks on Robotic Vision Language Action Models</a></td>
  <td>æå‡ºå¯¹æŠ—æ”»å‡»æ–¹æ³•ä»¥è§£å†³æœºå™¨äººè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹çš„è„†å¼±æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03350v1" data-paper-url="./papers/250603350v1-adversarial-attacks-on-robotic-vision-language-action-models.html" onclick="toggleFavorite(this, '2506.03350v1', 'Adversarial Attacks on Robotic Vision Language Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)