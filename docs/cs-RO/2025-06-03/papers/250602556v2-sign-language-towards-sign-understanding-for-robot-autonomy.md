---
layout: default
title: Sign Language: Towards Sign Understanding for Robot Autonomy
---

# Sign Language: Towards Sign Understanding for Robot Autonomy

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.02556" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.02556v2</a>
  <a href="https://arxiv.org/pdf/2506.02556.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.02556v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.02556v2', 'Sign Language: Towards Sign Understanding for Robot Autonomy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ayush Agrawal, Joel Loo, Nicky Zimmerman, David Hsu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-03 (æ›´æ–°: 2025-09-16)

**å¤‡æ³¨**: This work has been submitted to the IEEE for possible publication

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¼èˆªæ ‡å¿—ç†è§£æ–¹æ³•ä»¥æå‡æœºå™¨äººè‡ªä¸»å¯¼èˆªèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¯¼èˆªæ ‡å¿—ç†è§£` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æœºå™¨äººå¯¼èˆª` `åœºæ™¯ç†è§£` `å¼€æ”¾ä¸–ç•Œç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººåœ¨è§£æå¤æ‚åœºæ™¯å’Œæ ‡å¿—æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´å¯¼èˆªèƒ½åŠ›ä¸è¶³ã€‚
2. æå‡ºå¯¼èˆªæ ‡å¿—ç†è§£ä»»åŠ¡ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹è§£ææ ‡å¿—ä¸­çš„ä½ç½®ä¿¡æ¯å’Œæ–¹å‘ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–¹æ³•åœ¨å¯¼èˆªæ ‡å¿—ç†è§£ä¸Šå…·æœ‰è‰¯å¥½è¡¨ç°ï¼Œå±•ç¤ºäº†å…¶åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¼èˆªæ ‡å¿—æ˜¯äººç±»å¯»æ‰¾è·¯å¾„å’Œç†è§£åœºæ™¯çš„é‡è¦å·¥å…·ï¼Œä½†åœ¨æœºå™¨äººä¸­åº”ç”¨ä¸è¶³ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œå¯¼èˆªæ ‡å¿—èƒ½å¤Ÿç›´æ¥ç¼–ç å…³äºåŠ¨ä½œã€ç©ºé—´åŒºåŸŸå’Œå…³ç³»çš„å…³é”®ä¿¡æ¯ï¼Œä»è€Œæœ‰åŠ©äºæœºå™¨äººå¯¼èˆªå’Œåœºæ™¯ç†è§£ã€‚å°½ç®¡åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è§£ææ ‡å¿—ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä½†è¿‘å¹´æ¥è§†è§‰-è¯­è¨€æ¨¡å‹çš„è¿›å±•ä½¿å¾—è¿™ä¸€ç›®æ ‡å˜å¾—å¯è¡Œã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†å¯¼èˆªæ ‡å¿—ç†è§£ä»»åŠ¡ï¼Œè§£ææ ‡å¿—ä¸­çš„ä½ç½®å’Œæ–¹å‘ä¿¡æ¯ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼Œæ¶µç›–ä¸åŒå¤æ‚æ€§å’Œè®¾è®¡çš„æ ‡å¿—ï¼Œé€‚ç”¨äºåŒ»é™¢ã€è´­ç‰©ä¸­å¿ƒå’Œäº¤é€šæ¢çº½ç­‰å…¬å…±ç©ºé—´ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ä¸ªåŸºçº¿æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å¯¼èˆªæ ‡å¿—ç†è§£ä¸­çš„æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®é›†å·²åœ¨Githubä¸Šå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è§£æå¯¼èˆªæ ‡å¿—çš„èƒ½åŠ›ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æœ‰æ•ˆç†è§£æ ‡å¿—æ‰€ä¼ è¾¾çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¯¼èˆªæ ‡å¿—ç†è§£ä»»åŠ¡ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è§£ææ ‡å¿—ä¸­çš„ä½ç½®ä¿¡æ¯å’Œæ–¹å‘ï¼Œæ—¨åœ¨æå‡æœºå™¨äººå¯¹ç¯å¢ƒçš„ç†è§£èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æœºå™¨äººèƒ½å¤Ÿç›´æ¥åˆ©ç”¨æ ‡å¿—æ‰€ç¼–ç çš„å…³é”®ä¿¡æ¯ï¼Œä»è€Œæ”¹å–„å¯¼èˆªæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ„å»ºä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–æ ‡å¿—çš„æµ‹è¯•é›†ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼›æœ€åï¼Œé‡‡ç”¨é€‚å½“çš„è¯„ä¼°æŒ‡æ ‡å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œæµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¯¼èˆªæ ‡å¿—ç†è§£è¿™ä¸€æ–°ä»»åŠ¡ï¼Œå¹¶é€šè¿‡åŸºå‡†æµ‹è¯•é›†å’Œè¯„ä¼°æŒ‡æ ‡ä¸ºè¯¥é¢†åŸŸæä¾›äº†æ ‡å‡†åŒ–çš„å‚è€ƒã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œä¸“æ³¨äºè§£ææ ‡å¿—ä¿¡æ¯è€Œéå•çº¯çš„å›¾åƒè¯†åˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é€‚åˆçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ ‡å¿—è§£æçš„å‡†ç¡®æ€§ï¼Œå¹¶å¯¹è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç‰¹å®šçš„å‚æ•°è°ƒæ•´ï¼Œä»¥é€‚åº”å¯¼èˆªæ ‡å¿—çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–¹æ³•åœ¨å¯¼èˆªæ ‡å¿—ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€æœåŠ¡æœºå™¨äººå’Œå…¬å…±å®‰å…¨ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹å¯¼èˆªæ ‡å¿—çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªæ€§èƒ½ï¼Œè¿›è€Œæé«˜äººæœºäº¤äº’çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¤šå…¬å…±åœºæ‰€å’ŒåŠ¨æ€ç¯å¢ƒä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Navigational signs are common aids for human wayfinding and scene understanding, but are underutilized by robots. We argue that they benefit robot navigation and scene understanding, by directly encoding privileged information on actions, spatial regions, and relations. Interpreting signs in open-world settings remains a challenge owing to the complexity of scenes and signs, but recent advances in vision-language models (VLMs) make this feasible. To advance progress in this area, we introduce the task of navigational sign understanding which parses locations and associated directions from signs. We offer a benchmark for this task, proposing appropriate evaluation metrics and curating a test set capturing signs with varying complexity and design across diverse public spaces, from hospitals to shopping malls to transport hubs. We also provide a baseline approach using VLMs, and demonstrate their promise on navigational sign understanding. Code and dataset are available on Github.

