---
layout: default
title: AURA: Autonomous Upskilling with Retrieval-Augmented Agents
---

# AURA: Autonomous Upskilling with Retrieval-Augmented Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.02507" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.02507v3</a>
  <a href="https://arxiv.org/pdf/2506.02507.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.02507v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.02507v3', 'AURA: Autonomous Upskilling with Retrieval-Augmented Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alvin Zhu, Yusuke Tanaka, Andrew Goldberg, Dennis Hong

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-03 (æ›´æ–°: 2025-11-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAURAæ¡†æ¶ä»¥è‡ªåŠ¨åŒ–è®¾è®¡æœºå™¨äººå¼ºåŒ–å­¦ä¹ è¯¾ç¨‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è¯¾ç¨‹è®¾è®¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨äºº` `è‡ªåŠ¨åŒ–` `åé¦ˆæœºåˆ¶` `é¢†åŸŸéšæœºåŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹è®¾è®¡æ–¹æ³•ä¾èµ–å¤§é‡æ‰‹åŠ¨è°ƒæ•´ï¼Œæ•ˆç‡ä½ä¸”éš¾ä»¥æ‰©å±•ã€‚
2. AURAæ¡†æ¶é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆè¯¾ç¨‹ï¼Œç®€åŒ–äº†è®¾è®¡è¿‡ç¨‹å¹¶æé«˜äº†æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAURAåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡äº†å­¦ä¹ æˆåŠŸç‡å’Œé€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¾è®¡çµæ´»æœºå™¨äººçš„å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹é€šå¸¸éœ€è¦å¤§é‡æ‰‹åŠ¨è°ƒæ•´å¥–åŠ±å‡½æ•°ã€ç¯å¢ƒéšæœºåŒ–å’Œè®­ç»ƒé…ç½®ã€‚æœ¬æ–‡æå‡ºäº†AURAï¼ˆè‡ªä¸»æå‡ä¸æ£€ç´¢å¢å¼ºä»£ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¨¡å¼éªŒè¯çš„è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºå¤šé˜¶æ®µè¯¾ç¨‹çš„è‡ªä¸»è®¾è®¡è€…ã€‚AURAå°†ç”¨æˆ·æç¤ºè½¬åŒ–ä¸ºYAMLå·¥ä½œæµï¼Œç¼–ç å®Œæ•´çš„å¥–åŠ±å‡½æ•°ã€é¢†åŸŸéšæœºåŒ–ç­–ç•¥å’Œè®­ç»ƒé…ç½®ã€‚æ‰€æœ‰æ–‡ä»¶åœ¨ä½¿ç”¨GPUä¹‹å‰éƒ½ç»è¿‡é™æ€éªŒè¯ï¼Œç¡®ä¿é«˜æ•ˆå¯é çš„æ‰§è¡Œã€‚æ£€ç´¢å¢å¼ºçš„åé¦ˆå¾ªç¯ä½¿å¾—ä¸“é—¨çš„LLMä»£ç†èƒ½å¤Ÿæ ¹æ®å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­çš„å…ˆå‰è®­ç»ƒç»“æœè®¾è®¡ã€æ‰§è¡Œå’Œä¼˜åŒ–è¯¾ç¨‹é˜¶æ®µï¼Œä»è€Œå®ç°æŒç»­æ”¹è¿›ã€‚å®šé‡å®éªŒè¡¨æ˜ï¼ŒAURAåœ¨ç”ŸæˆæˆåŠŸç‡ã€ç±»äººæ­¥æ€å’Œæ“ä½œä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºLLMæŒ‡å¯¼çš„åŸºçº¿ã€‚æ¶ˆèç ”ç©¶å¼ºè°ƒäº†æ¨¡å¼éªŒè¯å’Œæ£€ç´¢å¯¹è¯¾ç¨‹è´¨é‡çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹è®¾è®¡ä¸­æ‰‹åŠ¨è°ƒæ•´ç¹çã€æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥é€‚åº”å¤æ‚çš„æœºå™¨äººä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAURAé€šè¿‡å°†ç”¨æˆ·è¾“å…¥è½¬åŒ–ä¸ºç»“æ„åŒ–çš„YAMLå·¥ä½œæµï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆè¯¾ç¨‹ï¼Œå‡å°‘äººå·¥å¹²é¢„ï¼Œæé«˜è®¾è®¡æ•ˆç‡å’Œè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAURAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç”¨æˆ·è¾“å…¥æ¨¡å—ã€YAMLç”Ÿæˆæ¨¡å—ã€é™æ€éªŒè¯æ¨¡å—å’Œåé¦ˆå¾ªç¯æ¨¡å—ã€‚ç”¨æˆ·è¾“å…¥é€šè¿‡LLMè½¬åŒ–ä¸ºè¯¾ç¨‹è®¾è®¡ï¼Œéšåè¿›è¡ŒéªŒè¯å’Œæ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šAURAçš„åˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è¯¾ç¨‹è®¾è®¡ç»“åˆï¼Œå½¢æˆè‡ªåŠ¨åŒ–çš„è®¾è®¡æµç¨‹ï¼Œæ˜¾è‘—æé«˜äº†è¯¾ç¨‹ç”Ÿæˆçš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šAURAä½¿ç”¨æ¨¡å¼éªŒè¯ç¡®ä¿ç”Ÿæˆçš„YAMLæ–‡ä»¶çš„æ­£ç¡®æ€§ï¼Œè®¾è®¡äº†é«˜æ•ˆçš„å¥–åŠ±å‡½æ•°å’Œé¢†åŸŸéšæœºåŒ–ç­–ç•¥ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚é€šè¿‡æ£€ç´¢å¢å¼ºçš„åé¦ˆæœºåˆ¶ï¼ŒAURAèƒ½å¤ŸæŒç»­ä¼˜åŒ–è¯¾ç¨‹è®¾è®¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAURAåœ¨ç”ŸæˆæˆåŠŸç‡ã€ç±»äººæ­¥æ€å’Œæ“ä½œä»»åŠ¡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„LLMæŒ‡å¯¼åŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨è¯¾ç¨‹è®¾è®¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AURAæ¡†æ¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»æœºå™¨äººã€æ™ºèƒ½åˆ¶é€ å’Œäººæœºåä½œç­‰åœºæ™¯ã€‚å…¶è‡ªåŠ¨åŒ–è¯¾ç¨‹è®¾è®¡èƒ½åŠ›å¯ä»¥å¤§å¹…åº¦é™ä½äººå·¥å¹²é¢„ï¼Œæé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¿«é€Ÿå‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Designing reinforcement learning curricula for agile robots traditionally requires extensive manual tuning of reward functions, environment randomizations, and training configurations. We introduce AURA (Autonomous Upskilling with Retrieval-Augmented Agents), a schema-validated curriculum reinforcement learning (RL) framework that leverages Large Language Models (LLMs) as autonomous designers of multi-stage curricula. AURA transforms user prompts into YAML workflows that encode full reward functions, domain randomization strategies, and training configurations. All files are statically validated before any GPU time is used, ensuring efficient and reliable execution. A retrieval-augmented feedback loop allows specialized LLM agents to design, execute, and refine curriculum stages based on prior training results stored in a vector database, enabling continual improvement over time. Quantitative experiments show that AURA consistently outperforms LLM-guided baselines in generation success rate, humanoid locomotion, and manipulation tasks. Ablation studies highlight the importance of schema validation and retrieval for curriculum quality. AURA successfully trains end-to-end policies directly from user prompts and deploys them zero-shot on a custom humanoid robot in multiple environments - capabilities that did not exist previously with manually designed controllers. By abstracting the complexity of curriculum design, AURA enables scalable and adaptive policy learning pipelines that would be complex to construct by hand. Project page: https://aura-research.org/

