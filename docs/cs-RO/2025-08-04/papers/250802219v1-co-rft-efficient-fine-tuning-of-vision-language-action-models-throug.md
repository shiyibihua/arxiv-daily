---
layout: default
title: CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning
---

# CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02219" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.02219v1</a>
  <a href="https://arxiv.org/pdf/2508.02219.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02219v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02219v1', 'CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Dongchi Huang, Zhirui Fang, Tianle Zhang, Yihang Li, Lin Zhao, Chunhe Xia

**ÂàÜÁ±ª**: cs.RO, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-04

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CO-RFT‰ª•Ëß£ÂÜ≥VLAÊ®°ÂûãÂæÆË∞É‰∏≠ÁöÑÊ†∑Êú¨ÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `Âº∫ÂåñÂ≠¶‰π†` `Ê®°‰ªøÂ≠¶‰π†` `Âä®‰ΩúÂàÜÂùó` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Ê†∑Êú¨ÊïàÁéá` `ËÆ≠ÁªÉÁ®≥ÂÆöÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉVLAÊ®°ÂûãÊó∂Èù¢‰∏¥Ê†∑Êú¨ÊïàÁéá‰Ωé„ÄÅÂä®‰ΩúÂàÜÂùóÂÖºÂÆπÊÄßÂ∑ÆÂíåËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÁ≠âÊåëÊàò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜChunked RLÊ°ÜÊû∂ÔºåÂπ∂Âú®Ê≠§Âü∫Á°Ä‰∏äÂºÄÂèë‰∫ÜCO-RFTÁÆóÊ≥ïÔºåÁªìÂêàÊ®°‰ªøÂ≠¶‰π†‰∏éÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ËøõË°åÂæÆË∞É„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCO-RFTÂú®ÊàêÂäüÁéá‰∏äÊèêÈ´ò‰∫Ü57%ÔºåÂë®ÊúüÊó∂Èó¥ÂáèÂ∞ë‰∫Ü22.3%ÔºåÂπ∂Âú®Êñ∞‰ΩçÁΩÆ‰∏äÂ±ïÁé∞Âá∫44.3%ÁöÑÊàêÂäüÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®Áé∞ÂÆû‰∏ñÁïåÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂‰∏≠Â±ïÁé∞Âá∫ÊòæËëóÊΩúÂäõÔºåÊøÄÂä±Á†îÁ©∂ËÄÖÊé¢Á¥¢ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂØπËøô‰∫õÊ®°ÂûãËøõË°åÂæÆË∞É„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî®RLÂæÆË∞ÉVLAÊ®°Âûã‰ªçÈù¢‰∏¥Ê†∑Êú¨ÊïàÁéá„ÄÅÂä®‰ΩúÂàÜÂùóÂÖºÂÆπÊÄßÂíåËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÁ≠âÊåëÊàò„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂‚Äî‚ÄîChunked RLÔºå‰∏ìÈó®ÈíàÂØπVLAÊ®°ÂûãËÆæËÆ°„ÄÇÂü∫‰∫éÊ≠§Ê°ÜÊû∂ÔºåÊèêÂá∫‰∫ÜCO-RFTÁÆóÊ≥ïÔºåÊó®Âú®ÈÄöËøáÊúâÈôêÁöÑÊºîÁ§∫Ê†∑Êú¨Ôºà30Ëá≥60‰∏™Ê†∑Êú¨ÔºâÂØπVLAÊ®°ÂûãËøõË°åÂæÆË∞É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCO-RFTÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠Ë∂ÖË∂ä‰∫Ü‰ª•ÂæÄÁöÑÁõëÁù£ÊñπÊ≥ïÔºåÊàêÂäüÁéáÊèêÈ´ò‰∫Ü57%ÔºåÂë®ÊúüÊó∂Èó¥ÂáèÂ∞ë‰∫Ü22.3%„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂú®Êú™ËßÅ‰ΩçÁΩÆ‰∏äÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑ‰ΩçÁΩÆÊ≥õÂåñËÉΩÂäõÔºåÊàêÂäüÁéáËææÂà∞‰∫Ü44.3%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÊó∂ÁöÑÊ†∑Êú¨ÊïàÁéá‰Ωé‰∏ãÂíåËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÁ≠âÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂä®‰ΩúÂàÜÂùóÊó∂Â≠òÂú®ÂÖºÂÆπÊÄß‰∏çË∂≥ÔºåÂØºËá¥ËÆ≠ÁªÉÊïàÊûú‰∏çÁêÜÊÉ≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáChunked RLÊ°ÜÊû∂ÔºåÁªìÂêàÊ®°‰ªøÂ≠¶‰π†ÂíåÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Ôºå‰ºòÂåñVLAÊ®°ÂûãÁöÑÂæÆË∞ÉËøáÁ®ã„ÄÇÈÄöËøáÂºïÂÖ•Âä®‰ΩúÂàÜÂùóÔºåÊèêÂçá‰∫ÜÊ†∑Êú¨Âà©Áî®ÊïàÁéáÂíåËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàËøõË°åÊ®°‰ªøÂ≠¶‰π†ÔºàILÔºâÔºåÂØπÊ®°ÂûãËøõË°åÂÖ®ÂèÇÊï∞ÂæÆË∞É‰ª•ÂàùÂßãÂåñÈ™®Âπ≤ÁΩëÁªúÂíåÁ≠ñÁï•ÔºõÁÑ∂ÂêéÂÆûÊñΩÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÔºåÁªìÂêàÂä®‰ΩúÂàÜÂùó‰ºòÂåñÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂ∞ÜÂä®‰ΩúÂàÜÂùóÂºïÂÖ•Âà∞Êó∂Èó¥Â∑ÆÂàÜÔºàTDÔºâÂ≠¶‰π†‰∏≠ÔºåÂΩ¢Êàê‰∫Ü‰∏ìÈó®ÈíàÂØπVLAÊ®°ÂûãÁöÑChunked RLÊ°ÜÊû∂„ÄÇËøô‰∏ÄÂàõÊñ∞ÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÖ≥ÈîÆËÆæËÆ°‰∏äÔºåCO-RFTÁÆóÊ≥ï‰ΩøÁî®‰∫ÜÊúâÈôêÁöÑÊºîÁ§∫Ê†∑Êú¨Ôºà30Ëá≥60‰∏™ÔºâÔºåÂπ∂ÈÄöËøáÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÂàùÂßãÂåñÊ®°ÂûãÔºåÈöèÂêéÈÄöËøáÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÁ≠ñÁï•ÔºåÁ°Æ‰øù‰∫ÜËÆ≠ÁªÉËøáÁ®ãÁöÑÁ®≥ÂÆöÊÄßÂíåÈ´òÊïàÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑËÆæËÆ°Âú®ËÆ∫Êñá‰∏≠ËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCO-RFTÁÆóÊ≥ïÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÊàêÂäüÁéáÊèêÈ´ò‰∫Ü57%ÔºåÂë®ÊúüÊó∂Èó¥ÂáèÂ∞ë‰∫Ü22.3%„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂú®Êú™ËßÅ‰ΩçÁΩÆÁöÑÊàêÂäüÁéáËææÂà∞‰∫Ü44.3%ÔºåÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑ‰ΩçÁΩÆÊ≥õÂåñËÉΩÂäõÔºåÊòæËëó‰ºò‰∫é‰ª•ÂæÄÁöÑÁõëÁù£Â≠¶‰π†ÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®ÂåñÁ≥ªÁªüÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠â„ÄÇÈÄöËøáÊèêÂçáVLAÊ®°ÂûãÁöÑÂæÆË∞ÉÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõÔºåCO-RFTÂèØ‰∏∫ÂÆûÈôÖÂ∫îÁî®Êèê‰æõÊõ¥‰∏∫ÁÅµÊ¥ªÂíåÈ´òÊïàÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÂú®Êõ¥Â§öÈ¢ÜÂüüÂÆûÁé∞ÂπøÊ≥õÂ∫îÁî®ÔºåÊèêÂçáÊô∫ËÉΩÁ≥ªÁªüÁöÑËá™‰∏ªÂÜ≥Á≠ñËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models demonstrate significant potential for developing generalized policies in real-world robotic control. This progress inspires researchers to explore fine-tuning these models with Reinforcement Learning (RL). However, fine-tuning VLA models with RL still faces challenges related to sample efficiency, compatibility with action chunking, and training stability. To address these challenges, we explore the fine-tuning of VLA models through offline reinforcement learning incorporating action chunking. In this work, we propose Chunked RL, a novel reinforcement learning framework specifically designed for VLA models. Within this framework, we extend temporal difference (TD) learning to incorporate action chunking, a prominent characteristic of VLA models. Building upon this framework, we propose CO-RFT, an algorithm aimed at fine-tuning VLA models using a limited set of demonstrations (30 to 60 samples). Specifically, we first conduct imitation learning (IL) with full parameter fine-tuning to initialize both the backbone and the policy. Subsequently, we implement offline RL with action chunking to optimize the pretrained policy. Our empirical results in real-world environments demonstrate that CO-RFT outperforms previous supervised methods, achieving a 57% improvement in success rate and a 22.3% reduction in cycle time. Moreover, our method exhibits robust positional generalization capabilities, attaining a success rate of 44.3% in previously unseen positions.

