---
layout: default
title: RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models
---

# RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02062" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02062v1</a>
  <a href="https://arxiv.org/pdf/2508.02062.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02062v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02062v1', 'RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, Insup Lee

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04

**å¤‡æ³¨**: Conference on Robot Learning 2025 (CoRL 2025), 17 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRICLä»¥è§£å†³VLAæ¨¡å‹ç¼ºä¹ä¸Šä¸‹æ–‡é€‚åº”æ€§çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `å¤šä»»åŠ¡å­¦ä¹ ` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„VLAæ¨¡å‹åœ¨æ–°ä»»åŠ¡å’Œæ–°ç¯å¢ƒä¸­çš„è¡¨ç°è™½å¥½ï¼Œä½†ç¼ºä¹ç”¨æˆ·å‹å¥½çš„é€‚åº”æ€§æå‡æ–¹æ³•ã€‚
2. æœ¬æ–‡æå‡ºRICLï¼Œé€šè¿‡å°‘é‡æ¼”ç¤ºæ•°æ®åæœŸæ³¨å…¥ä¸Šä¸‹æ–‡é€‚åº”æ€§ï¼Œå…è®¸æ¨¡å‹åœ¨ä¸æ›´æ–°å‚æ•°çš„æƒ…å†µä¸‹æ‰§è¡Œæ–°ä»»åŠ¡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRICLåœ¨å¤šç§æ“ä½œä»»åŠ¡ä¸­ä»…éœ€20ä¸ªæ¼”ç¤ºä¾¿èƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œä¸”åœ¨å‚æ•°æ›´æ–°æ—¶æ•ˆæœæ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šä»»åŠ¡çš„â€œè§†è§‰-è¯­è¨€-åŠ¨ä½œâ€ï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸå±•ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§ï¼Œä½†ç”¨æˆ·éœ€è¦ç®€å•çš„æ–¹æ³•æ¥æå‡å…¶æ€§èƒ½ã€‚å°½ç®¡è¯­è¨€å’Œè§†è§‰æ¨¡å‹å…·å¤‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„èƒ½åŠ›ï¼Œä½†ç»è¿‡æ¨¡ä»¿å­¦ä¹ é¢„è®­ç»ƒçš„VLAæ¨¡å‹å¹¶ä¸å…·å¤‡è¿™ç§èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åæœŸæ³¨å…¥ä¸Šä¸‹æ–‡é€‚åº”æ€§çš„æŠ€æœ¯RICLï¼Œå…è®¸ç”¨æˆ·é€šè¿‡å°‘é‡ï¼ˆ10-20ä¸ªï¼‰æ¼”ç¤ºæ¥æ•™æˆæ–°ä»»åŠ¡ã€‚ç»è¿‡RICLå¤„ç†åï¼Œæ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨è¿™äº›æ¼”ç¤ºä¸­çš„ç›¸å…³éƒ¨åˆ†è¿›è¡Œä»»åŠ¡æ‰§è¡Œï¼Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨$Ï€_{0}$-FAST VLAä¸Šåº”ç”¨RICLï¼Œå±•ç¤ºäº†åœ¨å¤šç§æ–°æ“ä½œä»»åŠ¡ä¸­ä»…éœ€20ä¸ªæ¼”ç¤ºä¾¿å¯å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”åœ¨å¯èƒ½çš„å‚æ•°æ›´æ–°æƒ…å†µä¸‹ï¼ŒRICLå¾®è°ƒè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†RICL-$Ï€_{0}$-FASTçš„ä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œä»¥ä¾¿é¦–æ¬¡å®ç°ç®€å•çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ¥å£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç»è¿‡æ¨¡ä»¿å­¦ä¹ é¢„è®­ç»ƒçš„VLAæ¨¡å‹ç¼ºä¹ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨ç”¨æˆ·æä¾›çš„å°‘é‡æ¼”ç¤ºæ¥é€‚åº”æ–°ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRICLé€šè¿‡åæœŸå¾®è°ƒå’Œå°‘é‡æ¼”ç¤ºæ•°æ®ï¼Œæ³¨å…¥ä¸Šä¸‹æ–‡é€‚åº”æ€§ï¼Œä½¿å¾—VLAæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå‚æ•°æ›´æ–°çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨æ¼”ç¤ºä¸­çš„ä¿¡æ¯æ¥æ‰§è¡Œæ–°ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRICLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹å¾®è°ƒå’Œä»»åŠ¡æ‰§è¡Œä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚ç”¨æˆ·æä¾›æ¼”ç¤ºæ•°æ®åï¼Œæ¨¡å‹é€šè¿‡ç‰¹å®šçš„å¾®è°ƒè¿‡ç¨‹æ¥å¢å¼ºå…¶ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šRICLçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åæœŸæ³¨å…¥ä¸Šä¸‹æ–‡é€‚åº”æ€§çš„èƒ½åŠ›ï¼Œå…è®¸æ¨¡å‹åœ¨ä¸æ”¹å˜å‚æ•°çš„æƒ…å†µä¸‹ï¼Œçµæ´»é€‚åº”æ–°ä»»åŠ¡ã€‚è¿™ä¸ä¼ ç»Ÿçš„éœ€è¦å¤§é‡å‚æ•°è°ƒæ•´çš„æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RICLä¸­ï¼Œå…³é”®çš„å‚æ•°è®¾ç½®åŒ…æ‹¬æ¼”ç¤ºæ•°æ®çš„æ•°é‡ï¼ˆ10-20ä¸ªï¼‰ï¼Œä»¥åŠå¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ã€‚æ¨¡å‹ç»“æ„ä¿æŒä¸å˜ï¼Œé‡ç‚¹åœ¨äºå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨ç°æœ‰çš„æ¼”ç¤ºæ•°æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRICLåœ¨å¤šç§æ–°æ“ä½œä»»åŠ¡ä¸­ï¼Œä»…éœ€20ä¸ªæ¼”ç¤ºä¾¿èƒ½å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ä»»åŠ¡æ‰§è¡Œä¸­çš„æˆåŠŸç‡å’Œæ•ˆç‡å‡æœ‰å¤§å¹…æé«˜ã€‚æ­¤å¤–ï¼Œå½“å…è®¸å¯¹ç›®æ ‡ä»»åŠ¡æ¼”ç¤ºè¿›è¡Œå‚æ•°æ›´æ–°æ—¶ï¼ŒRICLå¾®è°ƒè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„è¡¨ç°ï¼Œå±•ç°å‡ºå…¶å¼ºå¤§çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RICLæŠ€æœ¯å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æœºå™¨äººæ“ä½œã€æ™ºèƒ½å®¶å±…å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡ç®€åŒ–ç”¨æˆ·ä¸æ¨¡å‹ä¹‹é—´çš„äº¤äº’ï¼Œç”¨æˆ·å¯ä»¥æ›´è½»æ¾åœ°æ•™æˆæ–°ä»»åŠ¡ï¼Œä»è€Œæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›å’Œçµæ´»æ€§ã€‚æœªæ¥ï¼ŒRICLå¯èƒ½ä¼šæ¨åŠ¨æ›´æ™ºèƒ½çš„æœºå™¨äººç³»ç»Ÿçš„å¼€å‘ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æœåŠ¡äºäººç±»éœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-task ``vision-language-action'' (VLA) models have recently demonstrated increasing promise as generalist foundation models for robotics, achieving non-trivial performance out of the box on new tasks in new environments. However, for such models to be truly useful, an end user must have easy means to teach them to improve. For language and vision models, the emergent ability to perform in-context learning (ICL) has proven to be a versatile and highly useful interface to easily teach new tasks with no parameter finetuning. Unfortunately, VLAs pre-trained with imitation learning objectives do not naturally acquire ICL abilities. In this paper, we demonstrate that, with the right finetuning recipe and a small robot demonstration dataset, it is possible to inject in-context adaptability post hoc into such a VLA. After retraining for in-context learning (RICL), our system permits an end user to provide a small number (10-20) of demonstrations for a new task. RICL then fetches the most relevant portions of those demonstrations into the VLA context to exploit ICL, performing the new task and boosting task performance. We apply RICL to inject ICL into the $Ï€_{0}$-FAST VLA, and show that it permits large in-context improvements for a variety of new manipulation tasks with only 20 demonstrations per task, without any parameter updates. When parameter updates on the target task demonstrations is possible, RICL finetuning further boosts performance. We release code and model weights for RICL-$Ï€_{0}$-FAST alongside the paper to enable, for the first time, a simple in-context learning interface for new manipulation tasks. Website: https://ricl-vla.github.io.

