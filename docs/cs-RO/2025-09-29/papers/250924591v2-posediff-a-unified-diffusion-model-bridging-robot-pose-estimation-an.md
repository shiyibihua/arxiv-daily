---
layout: default
title: PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control
---

# PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24591" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24591v2</a>
  <a href="https://arxiv.org/pdf/2509.24591.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24591v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24591v2', 'PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haozhuo Zhang, Michele Caprio, Jing Shao, Qiang Zhang, Jian Tang, Shanghang Zhang, Wei Pan

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-10-30)

**å¤‡æ³¨**: The experimental setup and metrics lacks rigor, affecting the fairness of the comparisons

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://haozhuo-zhang.github.io/PoseDiff-project-page/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PoseDiffï¼šç»Ÿä¸€æ‰©æ•£æ¨¡å‹æ¡¥æ¥æœºå™¨äººå§¿æ€ä¼°è®¡ä¸è§†é¢‘åˆ°åŠ¨ä½œæ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `æœºå™¨äººå§¿æ€ä¼°è®¡` `è§†é¢‘åˆ°åŠ¨ä½œæ§åˆ¶` `é€†åŠ¨åŠ›å­¦` `å…·èº«æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ§åˆ¶æ–¹æ³•é€šå¸¸ä¾èµ–å¤šé˜¶æ®µæµç¨‹æˆ–è¾…åŠ©æ¨¡æ€ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œé›†æˆå›°éš¾ã€‚
2. PoseDiffåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç›´æ¥ä»å•å¼ RGBå›¾åƒé¢„æµ‹ç»“æ„åŒ–æœºå™¨äººçŠ¶æ€ï¼Œå¹¶æ‰©å±•åˆ°è§†é¢‘åˆ°åŠ¨ä½œçš„é€†åŠ¨åŠ›å­¦ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPoseDiffåœ¨å§¿æ€ä¼°è®¡å’Œç‰©ä½“æ“ä½œä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

PoseDiffæ˜¯ä¸€ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå®ƒåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å†…æ•´åˆäº†æœºå™¨äººçŠ¶æ€ä¼°è®¡å’Œæ§åˆ¶ã€‚PoseDiffçš„æ ¸å¿ƒæ˜¯å°†åŸå§‹è§†è§‰è§‚æµ‹æ˜ å°„ä¸ºç»“æ„åŒ–çš„æœºå™¨äººçŠ¶æ€ï¼ˆå¦‚3Då…³é”®ç‚¹æˆ–å…³èŠ‚è§’åº¦ï¼‰ï¼Œä»…éœ€å•å¼ RGBå›¾åƒï¼Œæ— éœ€å¤šé˜¶æ®µæµç¨‹æˆ–è¾…åŠ©æ¨¡æ€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒPoseDiffè‡ªç„¶åœ°æ‰©å±•åˆ°è§†é¢‘åˆ°åŠ¨ä½œçš„é€†åŠ¨åŠ›å­¦ï¼šé€šè¿‡ä»¥ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„ç¨€ç–è§†é¢‘å…³é”®å¸§ä¸ºæ¡ä»¶ï¼Œå®ƒé€šè¿‡é‡å å¹³å‡ç­–ç•¥ç”Ÿæˆå¹³æ»‘ä¸”è¿ç»­çš„é•¿ç¨‹åŠ¨ä½œåºåˆ—ã€‚è¿™ç§ç»Ÿä¸€çš„è®¾è®¡å®ç°äº†æ„ŸçŸ¥å’Œæ§åˆ¶çš„å¯æ‰©å±•ä¸”é«˜æ•ˆçš„é›†æˆã€‚åœ¨DREAMæ•°æ®é›†ä¸Šï¼ŒPoseDiffåœ¨å§¿æ€ä¼°è®¡æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„ç²¾åº¦å’Œå®æ—¶æ€§èƒ½ã€‚åœ¨Libero-Objectæ“ä½œä»»åŠ¡ä¸­ï¼Œå³ä½¿åœ¨ä¸¥æ ¼çš„ç¦»çº¿è®¾ç½®ä¸‹ï¼Œå®ƒä¹Ÿæ˜¾è‘—æé«˜äº†ç°æœ‰é€†åŠ¨åŠ›å­¦æ¨¡å—çš„æˆåŠŸç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒPoseDiffä¸ºå…·èº«æ™ºèƒ½ä¸­çš„æ„ŸçŸ¥ã€è§„åˆ’å’Œæ§åˆ¶ä¹‹é—´æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å‡†ç¡®å’Œé«˜æ•ˆçš„æ¡¥æ¢ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººæ§åˆ¶æ–¹æ³•é€šå¸¸éœ€è¦å¤æ‚çš„å¤šé˜¶æ®µæµç¨‹ï¼Œä¾‹å¦‚å…ˆè¿›è¡Œå§¿æ€ä¼°è®¡ï¼Œå†è¿›è¡Œè¿åŠ¨è§„åˆ’å’Œæ§åˆ¶ã€‚è¿™äº›æµç¨‹ä¸ä»…è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè€Œä¸”å®¹æ˜“å¼•å…¥è¯¯å·®ç´¯ç§¯ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ³•ä¾èµ–äºé¢å¤–çš„ä¼ æ„Ÿå™¨ä¿¡æ¯ï¼ˆå¦‚æ·±åº¦å›¾ï¼‰ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆã€å‡†ç¡®åœ°ä»è§†è§‰ä¿¡æ¯ä¸­æå–æœºå™¨äººçŠ¶æ€å¹¶è¿›è¡Œæ§åˆ¶æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPoseDiffçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå°†æœºå™¨äººçŠ¶æ€ä¼°è®¡å’Œæ§åˆ¶ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶ä¸­ã€‚é€šè¿‡å°†è§†è§‰è§‚æµ‹ä½œä¸ºæ¡ä»¶ï¼Œæ‰©æ•£æ¨¡å‹å¯ä»¥å­¦ä¹ ä»å™ªå£°åˆ°ç›®æ ‡çŠ¶æ€çš„æ˜ å°„ï¼Œä»è€Œå®ç°ä»å›¾åƒç›´æ¥é¢„æµ‹æœºå™¨äººçŠ¶æ€ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†è§†é¢‘å…³é”®å¸§ä½œä¸ºæ¡ä»¶ï¼ŒPoseDiffè¿˜å¯ä»¥ç”Ÿæˆé•¿ç¨‹åŠ¨ä½œåºåˆ—ï¼Œå®ç°è§†é¢‘åˆ°åŠ¨ä½œçš„é€†åŠ¨åŠ›å­¦æ§åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPoseDiffçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šå§¿æ€ä¼°è®¡å’Œè§†é¢‘åˆ°åŠ¨ä½œæ§åˆ¶ã€‚å¯¹äºå§¿æ€ä¼°è®¡ï¼ŒPoseDiffä»¥å•å¼ RGBå›¾åƒä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹é¢„æµ‹æœºå™¨äººçš„3Då…³é”®ç‚¹æˆ–å…³èŠ‚è§’åº¦ã€‚å¯¹äºè§†é¢‘åˆ°åŠ¨ä½œæ§åˆ¶ï¼ŒPoseDiffé¦–å…ˆåˆ©ç”¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆç¨€ç–çš„è§†é¢‘å…³é”®å¸§ï¼Œç„¶åä»¥è¿™äº›å…³é”®å¸§ä½œä¸ºæ¡ä»¶ï¼Œé€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¹³æ»‘ä¸”è¿ç»­çš„é•¿ç¨‹åŠ¨ä½œåºåˆ—ã€‚ä¸ºäº†æé«˜åŠ¨ä½œåºåˆ—çš„å¹³æ»‘æ€§ï¼ŒPoseDiffé‡‡ç”¨äº†é‡å å¹³å‡ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šPoseDiffçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„æ¡†æ¶ï¼Œå®ƒå°†æœºå™¨äººçŠ¶æ€ä¼°è®¡å’Œæ§åˆ¶æ•´åˆåˆ°ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ä¸­ã€‚ä¸ä¼ ç»Ÿçš„å¤šé˜¶æ®µæµç¨‹ç›¸æ¯”ï¼ŒPoseDiffé¿å…äº†è¯¯å·®ç´¯ç§¯ï¼Œæé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒPoseDiffè¿˜èƒ½å¤Ÿç›´æ¥ä»RGBå›¾åƒè¿›è¡Œé¢„æµ‹ï¼Œæ— éœ€é¢å¤–çš„ä¼ æ„Ÿå™¨ä¿¡æ¯ã€‚é€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œé‡å å¹³å‡ç­–ç•¥ï¼ŒPoseDiffèƒ½å¤Ÿç”Ÿæˆå¹³æ»‘ä¸”è¿ç»­çš„é•¿ç¨‹åŠ¨ä½œåºåˆ—ã€‚

**å…³é”®è®¾è®¡**ï¼šPoseDiffä½¿ç”¨äº†ä¸€ç§åŸºäºTransformerçš„æ‰©æ•£æ¨¡å‹æ¶æ„ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒPoseDiffé‡‡ç”¨äº†å™ªå£°é¢„æµ‹æŸå¤±å‡½æ•°ï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹å­¦ä¹ ä»å™ªå£°åˆ°ç›®æ ‡çŠ¶æ€çš„æ˜ å°„ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒPoseDiffè¿˜é‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚åœ¨è§†é¢‘åˆ°åŠ¨ä½œæ§åˆ¶ä¸­ï¼ŒPoseDiffçš„å…³é”®è®¾è®¡æ˜¯é‡å å¹³å‡ç­–ç•¥ï¼Œå®ƒé€šè¿‡å¯¹å¤šä¸ªé¢„æµ‹çš„åŠ¨ä½œåºåˆ—è¿›è¡Œå¹³å‡ï¼Œä»è€Œæé«˜åŠ¨ä½œåºåˆ—çš„å¹³æ»‘æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PoseDiffåœ¨DREAMæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å§¿æ€ä¼°è®¡ç²¾åº¦å’Œå®æ—¶æ€§èƒ½ã€‚åœ¨Libero-Objectæ“ä½œä»»åŠ¡ä¸­ï¼ŒPoseDiffæ˜¾è‘—æé«˜äº†æˆåŠŸç‡ï¼Œå³ä½¿åœ¨ä¸¥æ ¼çš„ç¦»çº¿è®¾ç½®ä¸‹ï¼Œä¹Ÿä¼˜äºç°æœ‰çš„é€†åŠ¨åŠ›å­¦æ¨¡å—ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸé¡¹ç‰©ä½“æ“ä½œä»»åŠ¡ä¸­ï¼ŒPoseDiffçš„æˆåŠŸç‡æ¯”æœ€ä½³åŸºçº¿æé«˜äº†15%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒPoseDiffåœ¨æœºå™¨äººæ„ŸçŸ¥å’Œæ§åˆ¶æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PoseDiffå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨å·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºå®ç°æœºå™¨äººçš„è‡ªä¸»å¯¼èˆªã€ç‰©ä½“æ“ä½œã€äººæœºåä½œç­‰ä»»åŠ¡ã€‚é€šè¿‡å°†æ„ŸçŸ¥ã€è§„åˆ’å’Œæ§åˆ¶æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ï¼ŒPoseDiffå¯ä»¥æ˜¾è‘—æé«˜æœºå™¨äººçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œåº”ç”¨èŒƒå›´ã€‚æœªæ¥ï¼ŒPoseDiffè¿˜å¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„æœºå™¨äººç³»ç»Ÿå’Œä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present PoseDiff, a conditional diffusion model that unifies robot state estimation and control within a single framework. At its core, PoseDiff maps raw visual observations into structured robot states-such as 3D keypoints or joint angles-from a single RGB image, eliminating the need for multi-stage pipelines or auxiliary modalities. Building upon this foundation, PoseDiff extends naturally to video-to-action inverse dynamics: by conditioning on sparse video keyframes generated by world models, it produces smooth and continuous long-horizon action sequences through an overlap-averaging strategy. This unified design enables scalable and efficient integration of perception and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy and real-time performance for pose estimation. On Libero-Object manipulation tasks, it substantially improves success rates over existing inverse dynamics modules, even under strict offline settings. Together, these results show that PoseDiff provides a scalable, accurate, and efficient bridge between perception, planning, and control in embodied AI. The video visualization results can be found on the project page: https://haozhuo-zhang.github.io/PoseDiff-project-page/.

