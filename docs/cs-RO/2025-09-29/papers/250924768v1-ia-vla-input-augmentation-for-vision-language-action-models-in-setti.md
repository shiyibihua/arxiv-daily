---
layout: default
title: IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks
---

# IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24768" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24768v1</a>
  <a href="https://arxiv.org/pdf/2509.24768.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24768v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24768v1', 'IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eric Hannus, Miika Malin, Tran Nguyen Le, Ville Kyrki

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: Under review for ICRA 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIA-VLAæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¢å¼ºVLAåœ¨è¯­ä¹‰å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººæ“ä½œ` `è¯­è¨€ç†è§£` `è¾“å…¥å¢å¼º` `è¯­ä¹‰å¤æ‚ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. VLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­åº”ç”¨å¹¿æ³›ï¼Œä½†å—é™äºè®¡ç®—èµ„æºï¼Œå…¶è¯­è¨€ç†è§£èƒ½åŠ›é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å¤„ç†å¤æ‚æŒ‡ä»¤ã€‚
2. IA-VLAæ¡†æ¶åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹é¢„å¤„ç†è¾“å…¥ï¼Œç”Ÿæˆæ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢å¼ºVLAå¯¹å¤æ‚è¯­ä¹‰çš„ç†è§£ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒIA-VLAåœ¨åŒ…å«è§†è§‰é‡å¤å¯¹è±¡çš„è¯­ä¹‰å¤æ‚ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æå‡äº†VLAçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ¦‚å¿µæ³›åŒ–æ–¹é¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰å·²æˆä¸ºè§£å†³æœºå™¨äººæ“ä½œé—®é¢˜çš„ä¸€ç§æ—¥ç›Šæµè¡Œçš„æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ­¤ç±»æ¨¡å‹éœ€è¦ä»¥é€‚åˆæœºå™¨äººæ§åˆ¶çš„é€Ÿç‡è¾“å‡ºåŠ¨ä½œï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ‰€èƒ½ä¾èµ–çš„è¯­è¨€æ¨¡å‹çš„å¤§å°ï¼Œå¹¶å› æ­¤é™åˆ¶äº†å®ƒä»¬çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚æ“ä½œä»»åŠ¡å¯èƒ½éœ€è¦å¤æ‚çš„è¯­è¨€æŒ‡ä»¤ï¼Œä¾‹å¦‚é€šè¿‡ç›¸å¯¹ä½ç½®è¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼Œä»¥æŒ‡å®šäººç±»æ„å›¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†IA-VLAï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¹¿æ³›çš„è¯­è¨€ç†è§£èƒ½åŠ›ä½œä¸ºé¢„å¤„ç†é˜¶æ®µï¼Œä»¥ç”Ÿæˆæ”¹è¿›çš„ä¸Šä¸‹æ–‡æ¥å¢å¼ºVLAçš„è¾“å…¥ã€‚æˆ‘ä»¬åœ¨ä¸€ç»„è¯­ä¹‰å¤æ‚çš„ä»»åŠ¡ä¸Šè¯„ä¼°äº†è¯¥æ¡†æ¶ï¼Œè¿™äº›ä»»åŠ¡åœ¨VLAæ–‡çŒ®ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œå³æ¶‰åŠè§†è§‰é‡å¤é¡¹ï¼ˆå³è§†è§‰ä¸Šæ— æ³•åŒºåˆ†çš„å¯¹è±¡ï¼‰çš„ä»»åŠ¡ã€‚ä½¿ç”¨åŒ…å«ä¸‰ç§ç±»å‹é‡å¤å¯¹è±¡åœºæ™¯çš„æ•°æ®é›†æ¥æ¯”è¾ƒåŸºçº¿VLAä¸ä¸¤ä¸ªå¢å¼ºå˜ä½“ã€‚å®éªŒè¡¨æ˜ï¼ŒVLAå—ç›Šäºå¢å¼ºæ–¹æ¡ˆï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹éœ€è¦VLAä»æ¼”ç¤ºä¸­çœ‹åˆ°çš„æ¦‚æ¦‚å¿µè¿›è¡Œæ¨æ–­çš„è¯­è¨€æŒ‡ä»¤æ—¶ã€‚ä»£ç ã€æ•°æ®é›†å’Œè§†é¢‘è§https://sites.google.com/view/ia-vlaã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰åœ¨å¤„ç†éœ€è¦å¤æ‚è¯­è¨€ç†è§£çš„ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ ¹æ®ç›¸å¯¹ä½ç½®è¯†åˆ«å¯¹è±¡æˆ–å¤„ç†è§†è§‰ä¸Šæ— æ³•åŒºåˆ†çš„é‡å¤å¯¹è±¡æ—¶ã€‚ç”±äºVLAæ¨¡å‹éœ€è¦å¿«é€Ÿè¾“å‡ºåŠ¨ä½œä»¥è¿›è¡Œæœºå™¨äººæ§åˆ¶ï¼Œå› æ­¤å…¶å†…éƒ¨è¯­è¨€æ¨¡å‹çš„è§„æ¨¡å—åˆ°é™åˆ¶ï¼Œå¯¼è‡´å…¶è¯­è¨€ç†è§£èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šIA-VLAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ã€å…·æœ‰å¼ºå¤§è¯­è¨€ç†è§£èƒ½åŠ›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥å¢å¼ºVLAçš„è¾“å…¥ã€‚VLMä½œä¸ºé¢„å¤„ç†æ­¥éª¤ï¼Œè´Ÿè´£ç†è§£å¤æ‚çš„è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶æå–æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç„¶åå°†è¿™äº›ä¿¡æ¯ä¼ é€’ç»™VLAï¼Œä»è€Œå‡è½»VLAçš„è¯­è¨€ç†è§£è´Ÿæ‹…ã€‚è¿™æ ·ï¼ŒVLAå¯ä»¥ä¸“æ³¨äºåŠ¨ä½œç”Ÿæˆï¼Œè€Œæ— éœ€æ‰¿æ‹…å¤æ‚çš„è¯­è¨€è§£æä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIA-VLAæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä¸Šä¸‹æ–‡å¢å¼ºé˜¶æ®µï¼šä½¿ç”¨å¤§å‹VLMåˆ†æè¾“å…¥å›¾åƒå’Œè¯­è¨€æŒ‡ä»¤ï¼Œç”Ÿæˆå¢å¼ºçš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚è¿™ä¸ªè¿‡ç¨‹å¯èƒ½æ¶‰åŠç›®æ ‡æ£€æµ‹ã€å…³ç³»æ¨ç†ç­‰æ“ä½œï¼Œä»¥æå–å…³é”®ä¿¡æ¯ã€‚2) åŠ¨ä½œç”Ÿæˆé˜¶æ®µï¼šå°†å¢å¼ºçš„ä¸Šä¸‹æ–‡è¡¨ç¤ºä¸åŸå§‹è¾“å…¥ä¸€èµ·è¾“å…¥åˆ°VLAæ¨¡å‹ä¸­ï¼ŒVLAæ¨¡å‹æ ¹æ®è¿™äº›ä¿¡æ¯ç”Ÿæˆç›¸åº”çš„åŠ¨ä½œæŒ‡ä»¤ã€‚VLAæ¨¡å‹å¯ä»¥æ˜¯ä»»ä½•ç°æœ‰çš„VLAæ¶æ„ï¼Œä¾‹å¦‚åŸºäºTransformerçš„æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šIA-VLAçš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤§å‹VLMä½œä¸ºVLAçš„é¢„å¤„ç†å™¨ï¼Œä»è€Œå°†è¯­è¨€ç†è§£å’ŒåŠ¨ä½œç”Ÿæˆè§£è€¦ã€‚è¿™ä½¿å¾—VLAèƒ½å¤Ÿåˆ©ç”¨å¤§å‹VLMå¼ºå¤§çš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œè€Œæ— éœ€å¢åŠ è‡ªèº«çš„æ¨¡å‹å¤æ‚åº¦ã€‚ä¸ç›´æ¥è®­ç»ƒå¤§å‹VLAæ¨¡å‹ç›¸æ¯”ï¼ŒIA-VLAæ›´å…·æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„VLMé€‰æ‹©å’Œä¸Šä¸‹æ–‡å¢å¼ºç­–ç•¥æ˜¯å…³é”®è®¾è®¡å› ç´ ã€‚è®ºæ–‡å¯èƒ½é‡‡ç”¨äº†ç‰¹å®šçš„é¢„è®­ç»ƒVLMï¼Œå¹¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡äº†ä¸Šä¸‹æ–‡å¢å¼ºæ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨VLMç”Ÿæˆç›®æ ‡å¯¹è±¡çš„æè¿°æˆ–å…³ç³»ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½ä¹Ÿéœ€è¦è€ƒè™‘å¢å¼ºä¸Šä¸‹æ–‡çš„å½±å“ï¼Œä»¥ç¡®ä¿VLAèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŒ…å«è§†è§‰é‡å¤å¯¹è±¡çš„è¯­ä¹‰å¤æ‚ä»»åŠ¡ä¸­ï¼ŒIA-VLAæ¡†æ¶æ˜¾è‘—æå‡äº†VLAçš„æ€§èƒ½ã€‚ä¸åŸºçº¿VLAæ¨¡å‹ç›¸æ¯”ï¼ŒIA-VLAåœ¨éœ€è¦æ¦‚å¿µæ³›åŒ–çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®ï¼ˆä¾‹å¦‚æˆåŠŸç‡ã€åŠ¨ä½œå‡†ç¡®ç‡ç­‰ï¼‰éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

IA-VLAæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚è¯­è¨€æŒ‡ä»¤çš„æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚åœ¨æ‚ä¹±ç¯å¢ƒä¸­æ‹£é€‰ç‰¹å®šç‰©å“ã€æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ç»„è£…å®¶å…·ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æœºå™¨äººåœ¨å¤æ‚åœºæ™¯ä¸‹çš„ä»»åŠ¡å®Œæˆèƒ½åŠ›ï¼Œå¹¶é™ä½å¯¹VLAæ¨¡å‹è‡ªèº«è¯­è¨€ç†è§£èƒ½åŠ›çš„è¦æ±‚ï¼Œä»è€Œç®€åŒ–æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒè¿‡ç¨‹ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„æœºå™¨äººåº”ç”¨é¢†åŸŸï¼Œä¾‹å¦‚æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language-action models (VLAs) have become an increasingly popular approach for addressing robot manipulation problems in recent years. However, such models need to output actions at a rate suitable for robot control, which limits the size of the language model they can be based on, and consequently, their language understanding capabilities. Manipulation tasks may require complex language instructions, such as identifying target objects by their relative positions, to specify human intention. Therefore, we introduce IA-VLA, a framework that utilizes the extensive language understanding of a large vision language model as a pre-processing stage to generate improved context to augment the input of a VLA. We evaluate the framework on a set of semantically complex tasks which have been underexplored in VLA literature, namely tasks involving visual duplicates, i.e., visually indistinguishable objects. A dataset of three types of scenes with duplicate objects is used to compare a baseline VLA against two augmented variants. The experiments show that the VLA benefits from the augmentation scheme, especially when faced with language instructions that require the VLA to extrapolate from concepts it has seen in the demonstrations. For the code, dataset, and videos, see https://sites.google.com/view/ia-vla.

