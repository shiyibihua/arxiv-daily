---
layout: default
title: SONAR: Semantic-Object Navigation with Aggregated Reasoning through a Cross-Modal Inference Paradigm
---

# SONAR: Semantic-Object Navigation with Aggregated Reasoning through a Cross-Modal Inference Paradigm

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24321" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24321v1</a>
  <a href="https://arxiv.org/pdf/2509.24321.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24321v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24321v1', 'SONAR: Semantic-Object Navigation with Aggregated Reasoning through a Cross-Modal Inference Paradigm')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yao Wang, Zhirui Sun, Wenzheng Chi, Baozhi Jia, Wenjun Xu, Jiankun Wang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SONARï¼šèåˆè·¨æ¨¡æ€æ¨ç†çš„è¯­ä¹‰å¯¹è±¡å¯¼èˆªæ–¹æ³•ï¼Œæå‡æœªçŸ¥ç¯å¢ƒé€‚åº”æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è¯­ä¹‰å¯¹è±¡å¯¼èˆª` `è§†è§‰-è¯­è¨€å¯¼èˆª` `è·¨æ¨¡æ€æ¨ç†` `è¯­ä¹‰åœ°å›¾` `æœªçŸ¥ç¯å¢ƒ` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€å¯¼èˆªæ–¹æ³•åœ¨æœªçŸ¥ç¯å¢ƒä¸­æ³›åŒ–æ€§ä¸è¶³ï¼Œæˆ–åœ¨è¯­ä¹‰ä¿¡æ¯ä¸è¶³æ—¶è¡¨ç°æ¬ ä½³ï¼Œé™åˆ¶äº†æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚
2. SONARæ–¹æ³•èåˆè¯­ä¹‰åœ°å›¾å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è·¨æ¨¡æ€æ¨ç†èšåˆä¿¡æ¯ï¼Œæå‡äº†åœ¨ä¸åŒè¯­ä¹‰çº¿ç´¢ç¯å¢ƒä¸‹çš„å¯¼èˆªé²æ£’æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSONARåœ¨MP3Dæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸç‡å’ŒSPLæå‡ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSONARçš„è¯­ä¹‰å¯¹è±¡å¯¼èˆªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è·¨æ¨¡æ€æ¨ç†è¿›è¡Œèšåˆæ¨ç†ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å—åŒ–æ–¹æ³•æ³›åŒ–æ€§å·®ä»¥åŠåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–¹æ³•åœ¨è¯­ä¹‰çº¿ç´¢å¼±æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼ŒSONARé›†æˆäº†åŸºäºè¯­ä¹‰åœ°å›¾çš„ç›®æ ‡é¢„æµ‹æ¨¡å—å’ŒåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„å€¼åœ°å›¾æ¨¡å—ï¼Œä»è€Œåœ¨å…·æœ‰ä¸åŒè¯­ä¹‰çº¿ç´¢å¼ºåº¦çš„æœªçŸ¥ç¯å¢ƒä¸­å®ç°æ›´é²æ£’çš„å¯¼èˆªï¼Œå¹¶æœ‰æ•ˆå¹³è¡¡äº†æ³›åŒ–èƒ½åŠ›å’Œåœºæ™¯é€‚åº”æ€§ã€‚åœ¨ç›®æ ‡å®šä½æ–¹é¢ï¼Œæå‡ºäº†ä¸€ç§å°†å¤šå°ºåº¦è¯­ä¹‰åœ°å›¾ä¸ç½®ä¿¡åº¦åœ°å›¾ç›¸ç»“åˆçš„ç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘ç›®æ ‡å¯¹è±¡çš„é”™è¯¯æ£€æµ‹ã€‚åœ¨Gazeboæ¨¡æ‹Ÿå™¨ä¸­ä½¿ç”¨æœ€å…·æŒ‘æˆ˜æ€§çš„Matterport 3Dï¼ˆMP3Dï¼‰æ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSONARçš„æˆåŠŸç‡ä¸º38.4%ï¼ŒSPLä¸º17.7%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºæ¨¡å—åŒ–çš„è§†è§‰-è¯­è¨€å¯¼èˆªæ–¹æ³•ä¾èµ–äºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚è€ŒåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–¹æ³•åœ¨è¯­ä¹‰çº¿ç´¢è¾ƒå¼±æ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨æœªçŸ¥ç¯å¢ƒä¸­ï¼Œåˆ©ç”¨æœ‰é™çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå®ç°é²æ£’çš„å¯¼èˆªæ˜¯æœ¬æ–‡è¦è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSONARçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åŸºäºè¯­ä¹‰åœ°å›¾çš„ç›®æ ‡é¢„æµ‹æ¨¡å—ä¸åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„å€¼åœ°å›¾æ¨¡å—è¿›è¡Œèåˆï¼Œåˆ©ç”¨å„è‡ªçš„ä¼˜åŠ¿äº’è¡¥ã€‚è¯­ä¹‰åœ°å›¾æä¾›ç²¾ç¡®çš„ç›®æ ‡ä½ç½®ä¿¡æ¯ï¼Œè€Œè§†è§‰-è¯­è¨€æ¨¡å‹æä¾›æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡è·¨æ¨¡æ€æ¨ç†ï¼Œå°†ä¸¤ç§ä¿¡æ¯æºè¿›è¡Œèšåˆï¼Œä»è€Œåœ¨ä¸åŒè¯­ä¹‰çº¿ç´¢å¼ºåº¦çš„ç¯å¢ƒä¸­å®ç°æ›´é²æ£’çš„å¯¼èˆªã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSONARçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¯­ä¹‰åœ°å›¾ç›®æ ‡é¢„æµ‹æ¨¡å—å’Œè§†è§‰-è¯­è¨€æ¨¡å‹å€¼åœ°å›¾æ¨¡å—ã€‚è¯­ä¹‰åœ°å›¾æ¨¡å—é¦–å…ˆæ„å»ºç¯å¢ƒçš„è¯­ä¹‰åœ°å›¾ï¼Œç„¶åé¢„æµ‹ç›®æ ‡å¯¹è±¡çš„ä½ç½®ã€‚è§†è§‰-è¯­è¨€æ¨¡å‹æ¨¡å—åˆ™æ ¹æ®è§†è§‰è¾“å…¥å’Œè¯­è¨€æŒ‡ä»¤ï¼Œç”Ÿæˆå€¼åœ°å›¾ï¼Œè¡¨ç¤ºæ¯ä¸ªä½ç½®çš„å¯¼èˆªä»·å€¼ã€‚æœ€åï¼Œé€šè¿‡èšåˆä¸¤ä¸ªæ¨¡å—çš„è¾“å‡ºï¼Œé€‰æ‹©æœ€ä½³çš„å¯¼èˆªåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šSONARçš„å…³é”®åˆ›æ–°åœ¨äºè·¨æ¨¡æ€æ¨ç†çš„èšåˆæ–¹æ³•ã€‚å®ƒä¸æ˜¯ç®€å•åœ°å°†ä¸¤ä¸ªæ¨¡å—çš„ç»“æœè¿›è¡ŒåŠ æƒå¹³å‡ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ç§æ›´å¤æ‚çš„æ–¹å¼è¿›è¡Œèåˆï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨ä¸¤ç§ä¿¡æ¯æºçš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæå‡ºçš„å¤šå°ºåº¦è¯­ä¹‰åœ°å›¾ä¸ç½®ä¿¡åº¦åœ°å›¾ç›¸ç»“åˆçš„ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†ç›®æ ‡å¯¹è±¡çš„é”™è¯¯æ£€æµ‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯­ä¹‰åœ°å›¾æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†å¤šå°ºåº¦è¯­ä¹‰åœ°å›¾ï¼Œä»¥é€‚åº”ä¸åŒå¤§å°çš„ç›®æ ‡å¯¹è±¡ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ç½®ä¿¡åº¦åœ°å›¾ï¼Œç”¨äºè¿‡æ»¤æ‰ä½ç½®ä¿¡åº¦çš„ç›®æ ‡æ£€æµ‹ç»“æœã€‚åœ¨è§†è§‰-è¯­è¨€æ¨¡å‹æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹å¯¼èˆªä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒã€‚èšåˆæ¨¡å—ä½¿ç”¨äº†ä¸€ç§å¯å­¦ä¹ çš„èåˆç­–ç•¥ï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„ç¯å¢ƒå’ŒæŒ‡ä»¤ï¼ŒåŠ¨æ€åœ°è°ƒæ•´ä¸¤ä¸ªæ¨¡å—çš„æƒé‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SONARåœ¨Matterport 3Dæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒSONARçš„æˆåŠŸç‡ä¸º38.4%ï¼ŒSPLä¸º17.7%ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒSONARåœ¨æˆåŠŸç‡å’ŒSPLä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SONARæ–¹æ³•å¯åº”ç”¨äºå®¶åº­æœåŠ¡æœºå™¨äººã€ç‰©æµæœºå™¨äººã€å®‰é˜²å·¡é€»æœºå™¨äººç­‰é¢†åŸŸï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚çš„æœªçŸ¥ç¯å¢ƒä¸­æ ¹æ®äººç±»æŒ‡ä»¤å®Œæˆå¯¼èˆªä»»åŠ¡ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡æœºå™¨äººçš„è‡ªä¸»æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ï¼Œä½¿å…¶æ›´å¥½åœ°æœåŠ¡äºäººç±»ç”Ÿæ´»ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding human instructions and accomplishing Vision-Language Navigation tasks in unknown environments is essential for robots. However, existing modular approaches heavily rely on the quality of training data and often exhibit poor generalization. Vision-Language Model based methods, while demonstrating strong generalization capabilities, tend to perform unsatisfactorily when semantic cues are weak. To address these issues, this paper proposes SONAR, an aggregated reasoning approach through a cross modal paradigm. The proposed method integrates a semantic map based target prediction module with a Vision-Language Model based value map module, enabling more robust navigation in unknown environments with varying levels of semantic cues, and effectively balancing generalization ability with scene adaptability. In terms of target localization, we propose a strategy that integrates multi-scale semantic maps with confidence maps, aiming to mitigate false detections of target objects. We conducted an evaluation of the SONAR within the Gazebo simulator, leveraging the most challenging Matterport 3D (MP3D) dataset as the experimental benchmark. Experimental results demonstrate that SONAR achieves a success rate of 38.4% and an SPL of 17.7%.

