---
layout: default
title: SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation
---

# SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25358" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25358v3</a>
  <a href="https://arxiv.org/pdf/2509.25358.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25358v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25358v3', 'SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qianzhong Chen, Justin Yu, Mac Schwager, Pieter Abbeel, Yide Shentu, Philipp Wu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-10-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSARMï¼šç”¨äºé•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œçš„é˜¶æ®µæ„ŸçŸ¥å¥–åŠ±å»ºæ¨¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¥–åŠ±å»ºæ¨¡` `æ¨¡ä»¿å­¦ä¹ ` `é•¿æ—¶ç¨‹ä»»åŠ¡` `è¡Œä¸ºå…‹éš†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººå­¦ä¹ æ–¹æ³•åœ¨é•¿æ—¶ç¨‹ã€æ¥è§¦å¯†é›†å‹æ“ä½œä¸­ï¼Œå› æ¼”ç¤ºæ•°æ®è´¨é‡å‚å·®ä¸é½è€Œé¢ä¸´æŒ‘æˆ˜ã€‚
2. æå‡ºé˜¶æ®µæ„ŸçŸ¥çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡è”åˆé¢„æµ‹ä»»åŠ¡é˜¶æ®µå’Œç»†ç²’åº¦è¿›åº¦ï¼Œä»è‡ªç„¶è¯­è¨€æ³¨é‡Šä¸­è‡ªåŠ¨ç”Ÿæˆå¥–åŠ±æ ‡ç­¾ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Tæ¤æŠ˜å ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿè¡Œä¸ºå…‹éš†ï¼ŒéªŒè¯äº†å¥–åŠ±å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡æœºå™¨äººå­¦ä¹ åœ¨æ•´åˆæ„ŸçŸ¥ã€æ§åˆ¶å’Œè¯­è¨€ç†è§£æ–¹é¢å±•ç°äº†æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒåœ¨é•¿æ—¶ç¨‹ã€æ¥è§¦å¯†é›†å‹æ“ä½œï¼ˆå¦‚å¯å˜å½¢ç‰©ä½“å¤„ç†ï¼‰ä¸­è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºæ¼”ç¤ºè´¨é‡ä¸ä¸€è‡´ã€‚å¥–åŠ±å»ºæ¨¡æä¾›äº†ä¸€ä¸ªè‡ªç„¶çš„è§£å†³æ–¹æ¡ˆï¼šé€šè¿‡æä¾›æœ‰å®é™…æ„ä¹‰çš„è¿›åº¦ä¿¡å·ï¼Œå®ƒå°†å˜ˆæ‚çš„æ¼”ç¤ºè½¬åŒ–ä¸ºç¨³å®šçš„ç›‘ç£ï¼Œä»è€Œæ¨å¹¿åˆ°ä¸åŒçš„è½¨è¿¹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé˜¶æ®µæ„ŸçŸ¥çš„ã€åŸºäºè§†é¢‘çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è”åˆé¢„æµ‹é«˜çº§ä»»åŠ¡é˜¶æ®µå’Œç»†ç²’åº¦è¿›åº¦ã€‚å¥–åŠ±æ ‡ç­¾è‡ªåŠ¨ä»è‡ªç„¶è¯­è¨€å­ä»»åŠ¡æ³¨é‡Šä¸­å¯¼å‡ºï¼Œç¡®ä¿åœ¨å¯å˜é•¿åº¦çš„æ¼”ç¤ºä¸­ä¸€è‡´çš„è¿›åº¦ä¼°è®¡ã€‚è¿™ç§è®¾è®¡å…‹æœäº†å¸§ç´¢å¼•æ ‡ç­¾çš„å±€é™æ€§ï¼Œåè€…åœ¨æŠ˜å Tæ¤ç­‰å¯å˜æŒç»­æ—¶é—´çš„ä»»åŠ¡ä¸­å¤±æ•ˆã€‚æˆ‘ä»¬çš„å¥–åŠ±æ¨¡å‹å±•ç¤ºäº†å¯¹å˜å¼‚æ€§çš„é²æ£’æ€§ã€å¯¹åˆ†å¸ƒå¤–è®¾ç½®çš„æ³›åŒ–èƒ½åŠ›ä»¥åŠå¯¹ç­–ç•¥è®­ç»ƒçš„å¼ºå¤§æ•ˆç”¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†å¥–åŠ±å¯¹é½è¡Œä¸ºå…‹éš†ï¼ˆRA-BCï¼‰ï¼Œå®ƒè¿‡æ»¤é«˜è´¨é‡æ•°æ®å¹¶é€šè¿‡å¥–åŠ±é‡æ–°åŠ æƒæ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œä»…å¥–åŠ±æ¨¡å‹åœ¨éªŒè¯å’ŒçœŸå®æœºå™¨äººéƒ¨ç½²ä¸­ä¼˜äºåŸºçº¿ã€‚é›†æˆåˆ°RA-BCåï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»å±•å¹³çŠ¶æ€æŠ˜å Tæ¤æ—¶è¾¾åˆ°83%çš„æˆåŠŸç‡ï¼Œä»è¤¶çš±çŠ¶æ€æŠ˜å Tæ¤æ—¶è¾¾åˆ°67%çš„æˆåŠŸç‡â€”â€”è¿œè¿œè¶…è¿‡äº†æ™®é€šè¡Œä¸ºå…‹éš†ï¼Œåè€…ä»…è¾¾åˆ°8%å’Œ0%çš„æˆåŠŸç‡ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†å¥–åŠ±å»ºæ¨¡æ˜¯é•¿æ—¶ç¨‹æ“ä½œä¸­å¯æ‰©å±•ã€æ³¨é‡Šé«˜æ•ˆå’Œé²æ£’æ¨¡ä»¿å­¦ä¹ çš„å…³é”®æ¨åŠ¨å› ç´ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œç”±äºæ¼”ç¤ºæ•°æ®è´¨é‡ä¸ä¸€è‡´å¯¼è‡´æ¨¡ä»¿å­¦ä¹ æ•ˆæœä¸ä½³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚è¡Œä¸ºå…‹éš†ï¼Œç›´æ¥ä»æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ ç­–ç•¥ï¼Œä½†å½“æ¼”ç¤ºæ•°æ®åŒ…å«å™ªå£°æˆ–è´¨é‡ä¸é«˜æ—¶ï¼Œå­¦ä¹ åˆ°çš„ç­–ç•¥æ€§èƒ½ä¼šå—åˆ°é™åˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨å¯å˜å½¢ç‰©ä½“å¤„ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­ã€‚å¸§ç´¢å¼•æ ‡ç­¾åœ¨å¯å˜æŒç»­æ—¶é—´çš„ä»»åŠ¡ä¸­å¤±æ•ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¥–åŠ±å»ºæ¨¡ï¼Œå°†å˜ˆæ‚çš„æ¼”ç¤ºæ•°æ®è½¬åŒ–ä¸ºæ›´ç¨³å®šã€æ›´å…·æ³›åŒ–èƒ½åŠ›çš„ç›‘ç£ä¿¡å·ã€‚é€šè¿‡å­¦ä¹ ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°èƒ½å¤Ÿè¯„ä¼°æœºå™¨äººæ“ä½œçš„è¿›åº¦å’Œè´¨é‡ï¼Œä»è€Œä¸ºç­–ç•¥å­¦ä¹ æä¾›æ›´å¯é çš„æŒ‡å¯¼ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†ä»»åŠ¡çš„é˜¶æ®µæ€§ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ ä»»åŠ¡çš„å®Œæˆæƒ…å†µã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šé˜¶æ®µæ„ŸçŸ¥å¥–åŠ±æ¨¡å‹å’Œå¥–åŠ±å¯¹é½è¡Œä¸ºå…‹éš†ï¼ˆRA-BCï¼‰ã€‚é˜¶æ®µæ„ŸçŸ¥å¥–åŠ±æ¨¡å‹åŸºäºè§†é¢‘è¾“å…¥ï¼Œè”åˆé¢„æµ‹ä»»åŠ¡çš„é˜¶æ®µå’Œç»†ç²’åº¦è¿›åº¦ï¼Œå¹¶è¾“å‡ºå¥–åŠ±å€¼ã€‚RA-BCåˆ©ç”¨å¥–åŠ±æ¨¡å‹å¯¹æ¼”ç¤ºæ•°æ®è¿›è¡Œè¿‡æ»¤å’Œé‡åŠ æƒï¼Œé€‰æ‹©é«˜è´¨é‡çš„æ ·æœ¬ç”¨äºè¡Œä¸ºå…‹éš†è®­ç»ƒã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ï¼ˆå¸¦è‡ªç„¶è¯­è¨€å­ä»»åŠ¡æ³¨é‡Šçš„æ¼”ç¤ºï¼‰ï¼Œå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œä»¥åŠä½¿ç”¨RA-BCè¿›è¡Œç­–ç•¥å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé˜¶æ®µæ„ŸçŸ¥çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸ä»…é¢„æµ‹æ•´ä½“è¿›åº¦ï¼Œè¿˜æ˜¾å¼åœ°å»ºæ¨¡ä»»åŠ¡çš„é˜¶æ®µã€‚è¿™ä½¿å¾—å¥–åŠ±å‡½æ•°èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ ä»»åŠ¡çš„å®Œæˆæƒ…å†µï¼Œå¹¶å¯¹ä¸åŒé˜¶æ®µçš„è¿›å±•è¿›è¡ŒåŒºåˆ†ã€‚æ­¤å¤–ï¼Œè‡ªåŠ¨ä»è‡ªç„¶è¯­è¨€å­ä»»åŠ¡æ³¨é‡Šä¸­å¯¼å‡ºå¥–åŠ±æ ‡ç­¾ï¼Œé¿å…äº†æ‰‹åŠ¨æ ‡æ³¨çš„ç¹çå’Œä¸ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå¥–åŠ±æ¨¡å‹é‡‡ç”¨åŸºäºè§†é¢‘çš„è¾“å…¥ï¼Œä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œæå–è§†è§‰ç‰¹å¾ï¼Œç„¶åä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œå»ºæ¨¡æ—¶é—´ä¾èµ–æ€§ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é˜¶æ®µåˆ†ç±»æŸå¤±å’Œè¿›åº¦å›å½’æŸå¤±ã€‚RA-BCä½¿ç”¨å¥–åŠ±æ¨¡å‹è¾“å‡ºçš„å¥–åŠ±å€¼ä½œä¸ºæ ·æœ¬æƒé‡ï¼Œå¯¹é«˜è´¨é‡çš„æ ·æœ¬èµ‹äºˆæ›´é«˜çš„æƒé‡ï¼Œä»è€Œæé«˜è¡Œä¸ºå…‹éš†çš„æ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä½†æ­¤å¤„æœªæä¾›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å¥–åŠ±æ¨¡å‹åœ¨éªŒè¯å’ŒçœŸå®æœºå™¨äººéƒ¨ç½²ä¸­å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚åœ¨Tæ¤æŠ˜å ä»»åŠ¡ä¸­ï¼Œé›†æˆåˆ°RA-BCåï¼Œè¯¥æ–¹æ³•ä»å±•å¹³çŠ¶æ€æŠ˜å Tæ¤æ—¶è¾¾åˆ°83%çš„æˆåŠŸç‡ï¼Œä»è¤¶çš±çŠ¶æ€æŠ˜å Tæ¤æ—¶è¾¾åˆ°67%çš„æˆåŠŸç‡ï¼Œè€Œæ™®é€šè¡Œä¸ºå…‹éš†ä»…è¾¾åˆ°8%å’Œ0%çš„æˆåŠŸç‡ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šå®¶åº­æœåŠ¡æœºå™¨äººæ‰§è¡Œå®¶åŠ¡ã€å·¥ä¸šæœºå™¨äººè¿›è¡Œå¤æ‚è£…é…ã€åŒ»ç–—æœºå™¨äººè¾…åŠ©æ‰‹æœ¯ç­‰ã€‚é€šè¿‡å¥–åŠ±å»ºæ¨¡ï¼Œå¯ä»¥é™ä½å¯¹é«˜è´¨é‡æ¼”ç¤ºæ•°æ®çš„éœ€æ±‚ï¼Œæé«˜æœºå™¨äººå­¦ä¹ çš„æ•ˆç‡å’Œé²æ£’æ€§ï¼Œä»è€ŒåŠ é€Ÿæœºå™¨äººåœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large-scale robot learning has recently shown promise for enabling robots to perform complex tasks by integrating perception, control, and language understanding. Yet, it struggles with long-horizon, contact-rich manipulation such as deformable object handling, where demonstration quality is inconsistent. Reward modeling offers a natural solution: by providing grounded progress signals, it transforms noisy demonstrations into stable supervision that generalizes across diverse trajectories. We introduce a stage-aware, video-based reward modeling framework that jointly predicts high-level task stages and fine-grained progress. Reward labels are automatically derived from natural language subtask annotations, ensuring consistent progress estimation across variable-length demonstrations. This design overcomes frame-index labeling, which fails in variable-duration tasks like folding a T-shirt. Our reward model demonstrates robustness to variability, generalization to out-of-distribution settings, and strong utility for policy training. Building on it, we propose Reward-Aligned Behavior Cloning (RA-BC), which filters high-quality data and reweights samples by reward. Experiments show the reward model alone outperforms baselines on validation and real robot rollouts. Integrated into RA-BC, our approach achieves 83% success on folding T-shirts from the flattened state and 67% from the crumpled state -- far surpassing vanilla behavior cloning, which attains only 8% and 0% success. Overall, our results highlight reward modeling as a key enabler for scalable, annotation-efficient, and robust imitation learning in long-horizon manipulation.

