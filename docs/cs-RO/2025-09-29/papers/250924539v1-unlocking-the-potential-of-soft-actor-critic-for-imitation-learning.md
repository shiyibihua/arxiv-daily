---
layout: default
title: Unlocking the Potential of Soft Actor-Critic for Imitation Learning
---

# Unlocking the Potential of Soft Actor-Critic for Imitation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24539" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24539v1</a>
  <a href="https://arxiv.org/pdf/2509.24539.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24539v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24539v1', 'Unlocking the Potential of Soft Actor-Critic for Imitation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nayari Marie Lessa, Melya Boukheddimi, Frank Kirchner

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAMP+SACæ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œæå‡å››è¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶çš„æ•°æ®æ•ˆç‡ä¸æ³›åŒ–æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `è½¯æ¼”å‘˜-è¯„è®ºå®¶` `å¯¹æŠ—è¿åŠ¨å…ˆéªŒ` `æœºå™¨äººè¿åŠ¨æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¸»è¦ä¾èµ–PPOç­‰åœ¨ç­–ç•¥ç®—æ³•ï¼Œæ ·æœ¬æ•ˆç‡å’Œç­–ç•¥æ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. è®ºæ–‡æå‡ºAMP+SACæ¡†æ¶ï¼Œåˆ©ç”¨ç¦»ç­–ç•¥å­¦ä¹ å’Œç†µæ­£åˆ™åŒ–æ¢ç´¢ï¼Œæå‡æ•°æ®æ•ˆç‡å’Œé²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAMP+SACåœ¨å››è¶³æ­¥æ€æ¨¡ä»¿å­¦ä¹ ä¸­ï¼Œç›¸æ¯”AMP+PPOï¼Œå®ç°äº†æ›´é«˜çš„æ¨¡ä»¿å¥–åŠ±ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¯¹æŠ—è¿åŠ¨å…ˆéªŒï¼ˆAMPï¼‰ä¸ç¦»ç­–ç•¥è½¯æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆSACï¼‰ç®—æ³•ï¼Œæ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•ï¼ˆä¸»è¦ä¾èµ–è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–PPOï¼‰åœ¨æ ·æœ¬æ•ˆç‡å’Œç­–ç•¥æ³›åŒ–æ–¹é¢çš„å±€é™æ€§ã€‚é€šè¿‡åˆ©ç”¨å›æ”¾é©±åŠ¨å­¦ä¹ å’Œç†µæ­£åˆ™åŒ–æ¢ç´¢ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°æ›´è‡ªç„¶çš„è¡Œä¸ºå’Œä»»åŠ¡æ‰§è¡Œï¼ŒåŒæ—¶æé«˜æ•°æ®æ•ˆç‡å’Œé²æ£’æ€§ã€‚åœ¨æ¶‰åŠå¤šä¸ªå‚è€ƒè¿åŠ¨å’Œä¸åŒåœ°å½¢çš„å››è¶³æ­¥æ€å®éªŒä¸­ï¼Œç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ï¼ˆAMP+SACï¼‰ä¸ä»…ä¿æŒäº†ç¨³å®šçš„ä»»åŠ¡æ‰§è¡Œï¼Œè€Œä¸”ç›¸æ¯”å¹¿æ³›ä½¿ç”¨çš„AMP+PPOæ–¹æ³•ï¼Œè·å¾—äº†æ›´é«˜çš„æ¨¡ä»¿å¥–åŠ±ã€‚è¿™äº›å‘ç°çªå‡ºäº†ç¦»ç­–ç•¥ILå…¬å¼åœ¨æ¨è¿›æœºå™¨äººè¿åŠ¨ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åº”ç”¨äºæœºå™¨äººè¿åŠ¨æ§åˆ¶æ—¶ï¼Œé€šå¸¸é‡‡ç”¨Proximal Policy Optimization (PPO)ç­‰åœ¨ç­–ç•¥ç®—æ³•ã€‚è¿™äº›ç®—æ³•è™½ç„¶ä¿è¯äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œä½†æ ·æœ¬æ•ˆç‡è¾ƒä½ï¼Œéœ€è¦å¤§é‡çš„ä¸“å®¶æ•°æ®æ‰èƒ½è®­ç»ƒå‡ºæœ‰æ•ˆçš„ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒPPOçš„ç­–ç•¥æ³›åŒ–èƒ½åŠ›ä¹Ÿå­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥é€‚åº”å¤æ‚å¤šå˜çš„ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†Adversarial Motion Priors (AMP)ä¸Soft Actor-Critic (SAC)ç®—æ³•ç›¸ç»“åˆã€‚AMPæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è¿åŠ¨å…ˆéªŒè¡¨ç¤ºæ–¹æ³•ï¼Œè€ŒSACä½œä¸ºä¸€ç§ç¦»ç­–ç•¥ç®—æ³•ï¼Œå…·æœ‰æ›´é«˜çš„æ ·æœ¬æ•ˆç‡å’Œæ¢ç´¢èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ï¼Œå¯ä»¥å…‹æœPPOçš„å±€é™æ€§ï¼Œæé«˜æ¨¡ä»¿å­¦ä¹ çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šä¸“å®¶æ•°æ®æ”¶é›†æ¨¡å—ã€AMPè¿åŠ¨å…ˆéªŒæ¨¡å—å’ŒSACç­–ç•¥å­¦ä¹ æ¨¡å—ã€‚é¦–å…ˆï¼Œä»ä¸“å®¶æ¼”ç¤ºä¸­æ”¶é›†è¿åŠ¨æ•°æ®ã€‚ç„¶åï¼Œåˆ©ç”¨AMPå­¦ä¹ è¿åŠ¨å…ˆéªŒï¼Œç”Ÿæˆåˆ¤åˆ«å™¨ï¼Œç”¨äºåŒºåˆ†æ¨¡ä»¿ç­–ç•¥ç”Ÿæˆçš„è¿åŠ¨å’Œä¸“å®¶è¿åŠ¨ã€‚æœ€åï¼Œä½¿ç”¨SACç®—æ³•è®­ç»ƒç­–ç•¥ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆèƒ½å¤Ÿæ¬ºéª—åˆ¤åˆ«å™¨çš„è¿åŠ¨ï¼Œä»è€Œå®ç°å¯¹ä¸“å®¶è¿åŠ¨çš„æ¨¡ä»¿ã€‚SACç®—æ³•ä½¿ç”¨å›æ”¾ç¼“å†²åŒºå­˜å‚¨ç»éªŒï¼Œå¹¶åˆ©ç”¨ç†µæ­£åˆ™åŒ–é¼“åŠ±æ¢ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†AMPä¸SACç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„AMP+PPOæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨SACçš„ç¦»ç­–ç•¥å­¦ä¹ èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒSACçš„ç†µæ­£åˆ™åŒ–æœºåˆ¶æœ‰åŠ©äºç­–ç•¥æ¢ç´¢ï¼Œä»è€Œæé«˜ç­–ç•¥çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬è´¨åŒºåˆ«åœ¨äºä»åœ¨ç­–ç•¥å­¦ä¹ è½¬å‘äº†ç¦»ç­–ç•¥å­¦ä¹ ï¼Œä»è€Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šAMPä½¿ç”¨å¯¹æŠ—å­¦ä¹ çš„æ–¹å¼ï¼Œè®­ç»ƒä¸€ä¸ªåˆ¤åˆ«å™¨æ¥åŒºåˆ†æœºå™¨äººäº§ç”Ÿçš„åŠ¨ä½œå’Œä¸“å®¶åŠ¨ä½œã€‚SACç®—æ³•ä¸­çš„å¥–åŠ±å‡½æ•°ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸€éƒ¨åˆ†æ˜¯æ¨¡ä»¿å¥–åŠ±ï¼Œç”¨äºé¼“åŠ±ç­–ç•¥ç”Ÿæˆä¸ä¸“å®¶æ•°æ®ç›¸ä¼¼çš„è¿åŠ¨ï¼›å¦ä¸€éƒ¨åˆ†æ˜¯ç†µå¥–åŠ±ï¼Œç”¨äºé¼“åŠ±ç­–ç•¥è¿›è¡Œæ¢ç´¢ã€‚Actorå’ŒCriticç½‘ç»œé€šå¸¸é‡‡ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ç»“æ„ã€‚å…³é”®å‚æ•°åŒ…æ‹¬å­¦ä¹ ç‡ã€å›æ”¾ç¼“å†²åŒºå¤§å°ã€ç†µæ­£åˆ™åŒ–ç³»æ•°ç­‰ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬Actorçš„ç­–ç•¥æŸå¤±ã€Criticçš„Qå€¼æŸå¤±å’Œåˆ¤åˆ«å™¨çš„å¯¹æŠ—æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„AMP+SACæ¡†æ¶åœ¨å››è¶³æ­¥æ€æ¨¡ä»¿å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œç›¸æ¯”äºå¹¿æ³›ä½¿ç”¨çš„AMP+PPOæ–¹æ³•ï¼Œå–å¾—äº†æ›´é«˜çš„æ¨¡ä»¿å¥–åŠ±ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å¤šä¸ªå‚è€ƒè¿åŠ¨å’Œä¸åŒåœ°å½¢çš„æµ‹è¯•ä¸­ï¼ŒAMP+SACèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ¨¡ä»¿ä¸“å®¶è¿åŠ¨ï¼Œå¹¶ä¿æŒç¨³å®šçš„ä»»åŠ¡æ‰§è¡Œã€‚è¿™éªŒè¯äº†ç¦»ç­–ç•¥æ¨¡ä»¿å­¦ä¹ åœ¨æœºå™¨äººè¿åŠ¨æ§åˆ¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººè¿åŠ¨æ§åˆ¶é¢†åŸŸï¼Œä¾‹å¦‚å››è¶³æœºå™¨äººã€äººå½¢æœºå™¨äººç­‰ã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ ï¼Œæœºå™¨äººå¯ä»¥å­¦ä¹ åˆ°å¤æ‚çš„è¿åŠ¨æŠ€èƒ½ï¼Œä»è€Œåœ¨æœç´¢æ•‘æ´ã€ç‰©æµè¿è¾“ã€åŒ»ç–—åº·å¤ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæ¸¸æˆAIã€è™šæ‹Ÿè§’è‰²æ§åˆ¶ç­‰é¢†åŸŸï¼Œæå‡è™šæ‹Ÿè§’è‰²çš„è‡ªç„¶æ€§å’Œæ™ºèƒ½æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning-based methods have enabled robots to acquire bio-inspired movements with increasing levels of naturalness and adaptability. Among these, Imitation Learning (IL) has proven effective in transferring complex motion patterns from animals to robotic systems. However, current state-of-the-art frameworks predominantly rely on Proximal Policy Optimization (PPO), an on-policy algorithm that prioritizes stability over sample efficiency and policy generalization. This paper proposes a novel IL framework that combines Adversarial Motion Priors (AMP) with the off-policy Soft Actor-Critic (SAC) algorithm to overcome these limitations. This integration leverages replay-driven learning and entropy-regularized exploration, enabling naturalistic behavior and task execution, improving data efficiency and robustness. We evaluate the proposed approach (AMP+SAC) on quadruped gaits involving multiple reference motions and diverse terrains. Experimental results demonstrate that the proposed framework not only maintains stable task execution but also achieves higher imitation rewards compared to the widely used AMP+PPO method. These findings highlight the potential of an off-policy IL formulation for advancing motion generation in robotics.

