---
layout: default
title: Curriculum Imitation Learning of Distributed Multi-Robot Policies
---

# Curriculum Imitation Learning of Distributed Multi-Robot Policies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25097" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25097v2</a>
  <a href="https://arxiv.org/pdf/2509.25097.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25097v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25097v2', 'Curriculum Imitation Learning of Distributed Multi-Robot Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: JesÃºs Roche, Eduardo SebastiÃ¡n, Eduardo Montijano

**åˆ†ç±»**: cs.RO, cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-10-01)

**å¤‡æ³¨**: Accepted and presented at the Eight Iberian Robotics Conference, 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè¯¾ç¨‹å­¦ä¹ çš„åˆ†å¸ƒå¼å¤šæœºå™¨äººç­–ç•¥æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œæå‡é•¿æœŸåè°ƒèƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `å¤šæœºå™¨äººç³»ç»Ÿ` `æ¨¡ä»¿å­¦ä¹ ` `è¯¾ç¨‹å­¦ä¹ ` `åˆ†å¸ƒå¼æ§åˆ¶` `æ„ŸçŸ¥ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæœºå™¨äººç³»ç»Ÿæ§åˆ¶ç­–ç•¥å­¦ä¹ é¢ä¸´é•¿æœŸåè°ƒå›°éš¾å’ŒçœŸå®è®­ç»ƒæ•°æ®åŒ®ä¹çš„æŒ‘æˆ˜ã€‚
2. æå‡ºä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡é€æ­¥å¢åŠ ä¸“å®¶è½¨è¿¹é•¿åº¦ï¼Œæå‡é•¿æœŸè¡Œä¸ºçš„å‡†ç¡®æ€§å’Œå­¦ä¹ ç¨³å®šæ€§ã€‚
3. å¼•å…¥æ„ŸçŸ¥ä¼°è®¡æ–¹æ³•ï¼Œä»…åˆ©ç”¨å…¨å±€çŠ¶æ€æ¼”ç¤ºè¿‘ä¼¼æœºå™¨äººè‡ªæˆ‘ä¸­å¿ƒæ„ŸçŸ¥ï¼Œå¢å¼ºç­–ç•¥çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å¤šæœºå™¨äººç³»ç»Ÿï¼ˆMRSï¼‰æ§åˆ¶ç­–ç•¥å­¦ä¹ ä¸­é•¿æœŸåè°ƒå›°éš¾å’Œéš¾ä»¥è·å–çœŸå®è®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ã€‚é¦–å…ˆï¼Œæ”¹å˜äº†è¯¾ç¨‹å­¦ä¹ åœ¨MRSä¸­çš„ä¼ ç»Ÿè§’è‰²ï¼Œä¸å†ä¾§é‡äºæœºå™¨äººæ•°é‡çš„å¯æ‰©å±•æ€§ï¼Œè€Œæ˜¯ä¸“æ³¨äºæé«˜é•¿æœŸåè°ƒèƒ½åŠ›ã€‚æå‡ºäº†ä¸€ç§è¯¾ç¨‹ç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¢åŠ ä¸“å®¶è½¨è¿¹çš„é•¿åº¦ï¼Œä»è€Œç¨³å®šå­¦ä¹ å¹¶æé«˜é•¿æœŸè¡Œä¸ºçš„å‡†ç¡®æ€§ã€‚å…¶æ¬¡ï¼Œå¼•å…¥äº†ä¸€ç§ä»…ä½¿ç”¨ç¬¬ä¸‰äººç§°å…¨å±€çŠ¶æ€æ¼”ç¤ºæ¥è¿‘ä¼¼æ¯ä¸ªæœºå™¨äººçš„è‡ªæˆ‘ä¸­å¿ƒæ„ŸçŸ¥çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿‡æ»¤é‚»å±…ã€è½¬æ¢å‚è€ƒç³»å’Œæ¨¡æ‹Ÿè½¦è½½ä¼ æ„Ÿå™¨å¯å˜æ€§ï¼Œå°†ç†æƒ³åŒ–çš„è½¨è¿¹è½¬æ¢ä¸ºå±€éƒ¨å¯ç”¨çš„è§‚æµ‹ã€‚æœ€åï¼Œå°†è¿™ä¸¤ç§æ–¹æ³•é›†æˆåˆ°ä¸€ç§ç‰©ç†ä¿¡æ¯æŠ€æœ¯ä¸­ï¼Œä»¥ä»è§‚æµ‹ä¸­ç”Ÿæˆå¯æ‰©å±•çš„åˆ†å¸ƒå¼ç­–ç•¥ã€‚åœ¨å…·æœ‰ä¸åŒå›¢é˜Ÿè§„æ¨¡å’Œå™ªå£°æ°´å¹³çš„ä¸¤ä¸ªä»»åŠ¡ä¸­è¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥è¯¾ç¨‹æé«˜äº†é•¿æœŸå‡†ç¡®æ€§ï¼Œè€Œæ„ŸçŸ¥ä¼°è®¡æ–¹æ³•äº§ç”Ÿäº†å¯¹çœŸå®ä¸ç¡®å®šæ€§å…·æœ‰é²æ£’æ€§çš„ç­–ç•¥ã€‚æ€»ä¹‹ï¼Œè¿™äº›ç­–ç•¥èƒ½å¤Ÿä»å…¨å±€æ¼”ç¤ºä¸­å­¦ä¹ é²æ£’çš„åˆ†å¸ƒå¼æ§åˆ¶å™¨ï¼Œå³ä½¿åœ¨æ²¡æœ‰ä¸“å®¶åŠ¨ä½œæˆ–è½¦è½½æµ‹é‡çš„æƒ…å†µä¸‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæœºå™¨äººç³»ç»Ÿæ§åˆ¶ç­–ç•¥å­¦ä¹ é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯é•¿æœŸåè°ƒé—®é¢˜ï¼Œå³å¦‚ä½•è®©å¤šä¸ªæœºå™¨äººåœ¨è¾ƒé•¿æ—¶é—´èŒƒå›´å†…ååŒå®Œæˆä»»åŠ¡ï¼›äºŒæ˜¯éš¾ä»¥è·å–çœŸå®çš„è®­ç»ƒæ•°æ®ï¼ŒçœŸå®ç¯å¢ƒä¸­çš„æ•°æ®å¾€å¾€åŒ…å«å™ªå£°å’Œä¸ç¡®å®šæ€§ï¼Œè€Œä»¿çœŸæ•°æ®åˆéš¾ä»¥å®Œå…¨æ¨¡æ‹ŸçœŸå®ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆè¯¾ç¨‹å­¦ä¹ å’Œæ„ŸçŸ¥ä¼°è®¡ï¼Œä»å…¨å±€æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ é²æ£’çš„åˆ†å¸ƒå¼æ§åˆ¶ç­–ç•¥ã€‚è¯¾ç¨‹å­¦ä¹ ç”¨äºè§£å†³é•¿æœŸåè°ƒé—®é¢˜ï¼Œæ„ŸçŸ¥ä¼°è®¡ç”¨äºè§£å†³æ•°æ®çœŸå®æ€§é—®é¢˜ã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ ï¼Œé¿å…äº†å¼ºåŒ–å­¦ä¹ ä¸­æ¢ç´¢çš„å›°éš¾ï¼Œæé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å…¨å±€çŠ¶æ€æ¼”ç¤ºæ•°æ®æ”¶é›†ï¼›2) æ„ŸçŸ¥ä¼°è®¡æ¨¡å—ï¼Œå°†å…¨å±€çŠ¶æ€è½¬æ¢ä¸ºæ¯ä¸ªæœºå™¨äººçš„å±€éƒ¨è§‚æµ‹ï¼›3) è¯¾ç¨‹å­¦ä¹ æ¨¡å—ï¼Œé€æ­¥å¢åŠ ä¸“å®¶è½¨è¿¹çš„é•¿åº¦ï¼›4) ç­–ç•¥å­¦ä¹ æ¨¡å—ï¼Œåˆ©ç”¨æ¨¡ä»¿å­¦ä¹ ç®—æ³•è®­ç»ƒåˆ†å¸ƒå¼æ§åˆ¶ç­–ç•¥ã€‚æ•´ä¸ªæµç¨‹ä»å…¨å±€æ¼”ç¤ºæ•°æ®å¼€å§‹ï¼Œç»è¿‡æ„ŸçŸ¥ä¼°è®¡å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œæœ€ç»ˆå¾—åˆ°å¯ç”¨äºå®é™…ç¯å¢ƒçš„åˆ†å¸ƒå¼æ§åˆ¶ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä¸¤ä¸ªæ–¹é¢ï¼šä¸€æ˜¯è¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„åº”ç”¨ï¼Œä¸åŒäºä»¥å¾€ä¾§é‡äºæœºå™¨äººæ•°é‡æ‰©å±•çš„è¯¾ç¨‹å­¦ä¹ ï¼Œæœ¬æ–‡ä¾§é‡äºæå‡é•¿æœŸåè°ƒèƒ½åŠ›ï¼›äºŒæ˜¯æ„ŸçŸ¥ä¼°è®¡æ–¹æ³•ï¼Œä»…ä½¿ç”¨å…¨å±€çŠ¶æ€æ¼”ç¤ºæ¥è¿‘ä¼¼æ¯ä¸ªæœºå™¨äººçš„è‡ªæˆ‘ä¸­å¿ƒæ„ŸçŸ¥ï¼Œé¿å…äº†å¯¹ä¸“å®¶åŠ¨ä½œæˆ–è½¦è½½æµ‹é‡çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„å…³é”®åœ¨äºå¦‚ä½•è®¾è®¡è¯¾ç¨‹éš¾åº¦ï¼Œæœ¬æ–‡é‡‡ç”¨é€æ­¥å¢åŠ ä¸“å®¶è½¨è¿¹é•¿åº¦çš„æ–¹å¼ã€‚æ„ŸçŸ¥ä¼°è®¡æ–¹æ³•çš„å…³é”®åœ¨äºå¦‚ä½•è¿‡æ»¤é‚»å±…ã€è½¬æ¢å‚è€ƒç³»å’Œæ¨¡æ‹Ÿè½¦è½½ä¼ æ„Ÿå™¨å¯å˜æ€§ï¼Œä»¥ç”Ÿæˆæ›´çœŸå®çš„å±€éƒ¨è§‚æµ‹ã€‚ç­–ç•¥å­¦ä¹ æ¨¡å—å¯ä»¥ä½¿ç”¨å„ç§æ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚è¡Œä¸ºå…‹éš†æˆ–Daggerã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—æé«˜é•¿æœŸè¡Œä¸ºçš„å‡†ç¡®æ€§ï¼Œæ„ŸçŸ¥ä¼°è®¡æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå¯¹çœŸå®ä¸ç¡®å®šæ€§å…·æœ‰é²æ£’æ€§çš„ç­–ç•¥ã€‚åœ¨ä¸¤ä¸ªä¸åŒçš„ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•éƒ½å–å¾—äº†ä¼˜äºåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤šæœºå™¨äººååŒå®Œæˆä»»åŠ¡çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šä»“åº“ç‰©æµã€ç¾éš¾æ•‘æ´ã€ç¯å¢ƒç›‘æµ‹ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ ï¼Œå¯ä»¥å¿«é€Ÿéƒ¨ç½²å¤šæœºå™¨äººç³»ç»Ÿï¼Œé™ä½å¼€å‘æˆæœ¬ï¼Œæé«˜ç³»ç»Ÿæ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œç¯å¢ƒï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„å¤šæœºå™¨äººç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning control policies for multi-robot systems (MRS) remains a major challenge due to long-term coordination and the difficulty of obtaining realistic training data. In this work, we address both limitations within an imitation learning framework. First, we shift the typical role of Curriculum Learning in MRS, from scalability with the number of robots, to focus on improving long-term coordination. We propose a curriculum strategy that gradually increases the length of expert trajectories during training, stabilizing learning and enhancing the accuracy of long-term behaviors. Second, we introduce a method to approximate the egocentric perception of each robot using only third-person global state demonstrations. Our approach transforms idealized trajectories into locally available observations by filtering neighbors, converting reference frames, and simulating onboard sensor variability. Both contributions are integrated into a physics-informed technique to produce scalable, distributed policies from observations. We conduct experiments across two tasks with varying team sizes and noise levels. Results show that our curriculum improves long-term accuracy, while our perceptual estimation method yields policies that are robust to realistic uncertainty. Together, these strategies enable the learning of robust, distributed controllers from global demonstrations, even in the absence of expert actions or onboard measurements.

