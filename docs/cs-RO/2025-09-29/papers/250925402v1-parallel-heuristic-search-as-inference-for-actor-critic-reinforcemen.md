---
layout: default
title: Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models
---

# Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25402" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25402v1</a>
  <a href="https://arxiv.org/pdf/2509.25402.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25402v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25402v1', 'Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanlan Yang, Itamar Mishani, Luca Pivetti, Zachary Kingston, Maxim Likhachev

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: Submitted for Publication

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPACHSç®—æ³•ï¼Œåˆ©ç”¨Actor-Criticæ¨¡å‹è¿›è¡Œé«˜æ•ˆå¹¶è¡Œå¯å‘å¼æœç´¢ï¼Œæå‡æœºå™¨äººæ“ä½œä»»åŠ¡æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Actor-Critic` `å¼ºåŒ–å­¦ä¹ ` `å¯å‘å¼æœç´¢` `æœºå™¨äººæ“ä½œ` `å¹¶è¡Œè®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Actor-Criticæ¨¡å‹éƒ¨ç½²ç­–ç•¥é€šå¸¸ä¾èµ–ç®€å•çš„actorç­–ç•¥rolloutï¼Œå¿½ç•¥äº†criticç½‘ç»œçš„ä»·å€¼ï¼Œé™åˆ¶äº†æ€§èƒ½ã€‚
2. PACHSç®—æ³•åˆ©ç”¨actorç”ŸæˆåŠ¨ä½œï¼Œcriticè¯„ä¼°cost-to-goï¼ŒæŒ‡å¯¼å¹¶è¡Œæœ€ä½³ä¼˜å…ˆæœç´¢ï¼Œå®ç°æ›´é«˜æ•ˆçš„æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPACHSåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬æ— ç¢°æ’è¿åŠ¨è§„åˆ’å’ŒéæŠ“å–å¼æ¨ç‰©ç­‰å¤æ‚äº¤äº’ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Actor-Criticæ¨¡å‹æ˜¯ä¸€ç±»æ— éœ€æ¨¡å‹çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå·²åœ¨å„ç§æœºå™¨äººå­¦ä¹ ä»»åŠ¡ä¸­å±•ç°å‡ºæœ‰æ•ˆæ€§ã€‚è™½ç„¶å¤§é‡ç ”ç©¶é›†ä¸­åœ¨æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ•°æ®é‡‡æ ·æ•ˆç‡ä¸Šï¼Œä½†å¤§å¤šæ•°éƒ¨ç½²ç­–ç•¥ä»ç„¶ç›¸å¯¹ç®€å•ï¼Œé€šå¸¸ä¾èµ–äºç›´æ¥çš„actorç­–ç•¥rolloutã€‚ä¸æ­¤ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†PACHSï¼ˆå¹¶è¡ŒActor-Criticå¯å‘å¼æœç´¢ï¼‰ï¼Œä¸€ç§é«˜æ•ˆçš„å¹¶è¡Œæœ€ä½³ä¼˜å…ˆæœç´¢ç®—æ³•ï¼Œç”¨äºæ¨ç†ï¼Œå®ƒåˆ©ç”¨äº†actor-criticæ¶æ„çš„ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šactorç½‘ç»œç”ŸæˆåŠ¨ä½œï¼Œè€Œcriticç½‘ç»œæä¾›cost-to-goä¼°è®¡æ¥æŒ‡å¯¼æœç´¢ã€‚æœç´¢ä¸­é‡‡ç”¨äº†ä¸¤ä¸ªå±‚æ¬¡çš„å¹¶è¡Œæ€§â€”â€”åŠ¨ä½œå’Œcost-to-goä¼°è®¡åˆ†åˆ«ç”±actorå’Œcriticç½‘ç»œæ‰¹é‡ç”Ÿæˆï¼Œå¹¶ä¸”å›¾æ‰©å±•åˆ†å¸ƒåœ¨å¤šä¸ªçº¿ç¨‹ä¸­ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ— ç¢°æ’è¿åŠ¨è§„åˆ’å’Œå¯Œå«æ¥è§¦çš„äº¤äº’ï¼Œä¾‹å¦‚éæŠ“å–å¼æ¨ç‰©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œå¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®­ç»ƒå¥½çš„Actor-Criticæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç®€å•çš„actorç­–ç•¥rolloutï¼Œå¿½ç•¥äº†criticç½‘ç»œæä¾›çš„ä»·å€¼ä¼°è®¡ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„åŠ¨ä½œé€‰æ‹©å’Œè¾ƒä½çš„æ•ˆç‡ã€‚å°¤å…¶æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼Œè¿™ç§ç®€å•ç­–ç•¥éš¾ä»¥åº”å¯¹å„ç§æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç¢°æ’é¿å…å’Œæ¥è§¦åŠ›æ§åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†Actor-Criticæ¨¡å‹ä¸å¯å‘å¼æœç´¢ç›¸ç»“åˆï¼Œåˆ©ç”¨actorç½‘ç»œç”Ÿæˆå€™é€‰åŠ¨ä½œï¼Œå¹¶ä½¿ç”¨criticç½‘ç»œè¯„ä¼°è¿™äº›åŠ¨ä½œçš„cost-to-goï¼Œä»è€ŒæŒ‡å¯¼æœç´¢è¿‡ç¨‹ã€‚é€šè¿‡å¹¶è¡ŒåŒ–æœç´¢è¿‡ç¨‹ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°æ¢ç´¢çŠ¶æ€ç©ºé—´ï¼Œæ‰¾åˆ°æ›´ä¼˜çš„åŠ¨ä½œåºåˆ—ã€‚è¿™ç§æ–¹æ³•å……åˆ†åˆ©ç”¨äº†Actor-Criticæ¨¡å‹çš„ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPACHSç®—æ³•çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) Actorç½‘ç»œï¼šç”Ÿæˆå€™é€‰åŠ¨ä½œï¼›2) Criticç½‘ç»œï¼šè¯„ä¼°çŠ¶æ€çš„cost-to-goï¼›3) å¹¶è¡Œæœ€ä½³ä¼˜å…ˆæœç´¢ï¼šåˆ©ç”¨actorå’Œcriticçš„è¾“å‡ºï¼Œåœ¨çŠ¶æ€ç©ºé—´ä¸­è¿›è¡Œæœç´¢ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„åŠ¨ä½œåºåˆ—ï¼›4) å¹¶è¡ŒåŒ–æœºåˆ¶ï¼šåŒ…æ‹¬åŠ¨ä½œå’Œcost-to-goçš„æ‰¹é‡ç”Ÿæˆï¼Œä»¥åŠå›¾æ‰©å±•çš„å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ã€‚ç®—æ³•é¦–å…ˆä»åˆå§‹çŠ¶æ€å¼€å§‹ï¼Œåˆ©ç”¨actorç½‘ç»œç”Ÿæˆä¸€ç»„å€™é€‰åŠ¨ä½œï¼Œç„¶åä½¿ç”¨criticç½‘ç»œè¯„ä¼°è¿™äº›åŠ¨ä½œçš„cost-to-goã€‚æ ¹æ®cost-to-goçš„å€¼ï¼Œé€‰æ‹©æœ€æœ‰å¸Œæœ›çš„çŠ¶æ€è¿›è¡Œæ‰©å±•ï¼Œé‡å¤æ­¤è¿‡ç¨‹ç›´åˆ°æ‰¾åˆ°ç›®æ ‡çŠ¶æ€æˆ–è¾¾åˆ°æœç´¢é™åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šPACHSç®—æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå°†Actor-Criticæ¨¡å‹ä¸å¹¶è¡Œå¯å‘å¼æœç´¢ç›¸ç»“åˆï¼Œå……åˆ†åˆ©ç”¨äº†actorå’Œcriticç½‘ç»œçš„ä¼˜åŠ¿ã€‚ä¸ä¼ ç»Ÿçš„actorç­–ç•¥rolloutç›¸æ¯”ï¼ŒPACHSç®—æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢çŠ¶æ€ç©ºé—´ï¼Œæ‰¾åˆ°æ›´ä¼˜çš„åŠ¨ä½œåºåˆ—ã€‚æ­¤å¤–ï¼ŒPACHSç®—æ³•é‡‡ç”¨äº†ä¸¤å±‚å¹¶è¡ŒåŒ–æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æé«˜äº†æœç´¢æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šPACHSç®—æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) Actorå’ŒCriticç½‘ç»œçš„ç»“æ„ï¼šæ ¹æ®å…·ä½“çš„ä»»åŠ¡é€‰æ‹©åˆé€‚çš„ç½‘ç»œç»“æ„ï¼›2) Cost-to-goçš„è¯„ä¼°æ–¹å¼ï¼šå¯ä»¥ä½¿ç”¨criticç½‘ç»œçš„è¾“å‡ºç›´æ¥ä½œä¸ºcost-to-goï¼Œä¹Ÿå¯ä»¥è¿›è¡Œä¸€äº›è°ƒæ•´å’Œä¼˜åŒ–ï¼›3) æœç´¢ç®—æ³•çš„å‚æ•°è®¾ç½®ï¼šä¾‹å¦‚æœç´¢æ·±åº¦ã€åˆ†æ”¯å› å­ç­‰ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼›4) å¹¶è¡ŒåŒ–ç­–ç•¥ï¼šéœ€è¦ä»”ç»†è®¾è®¡å¹¶è¡ŒåŒ–æœºåˆ¶ï¼Œä»¥é¿å…çº¿ç¨‹å†²çªå’Œèµ„æºç«äº‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPACHSç®—æ³•åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ— ç¢°æ’è¿åŠ¨è§„åˆ’ä»»åŠ¡ä¸­ï¼ŒPACHSç®—æ³•èƒ½å¤Ÿæ‰¾åˆ°æ›´çŸ­çš„è·¯å¾„ï¼Œå¹¶å‡å°‘ç¢°æ’çš„å‘ç”Ÿã€‚åœ¨éæŠ“å–å¼æ¨ç‰©ä»»åŠ¡ä¸­ï¼ŒPACHSç®—æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ§åˆ¶ç‰©ä½“çš„è¿åŠ¨è½¨è¿¹ï¼Œå¹¶æé«˜ä»»åŠ¡çš„æˆåŠŸç‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†PACHSç›¸å¯¹äºä¼ ç»Ÿæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PACHSç®—æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šå·¥ä¸šè‡ªåŠ¨åŒ–ã€ç‰©æµåˆ†æ‹£ã€åŒ»ç–—æ‰‹æœ¯ã€å®¶åº­æœåŠ¡ç­‰ã€‚è¯¥ç®—æ³•å¯ä»¥æé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒã€‚æ­¤å¤–ï¼ŒPACHSç®—æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚ï¼šæ¸¸æˆAIã€è·¯å¾„è§„åˆ’ã€èµ„æºè°ƒåº¦ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Actor-Critic models are a class of model-free deep reinforcement learning (RL) algorithms that have demonstrated effectiveness across various robot learning tasks. While considerable research has focused on improving training stability and data sampling efficiency, most deployment strategies have remained relatively simplistic, typically relying on direct actor policy rollouts. In contrast, we propose \pachs{} (\textit{P}arallel \textit{A}ctor-\textit{C}ritic \textit{H}euristic \textit{S}earch), an efficient parallel best-first search algorithm for inference that leverages both components of the actor-critic architecture: the actor network generates actions, while the critic network provides cost-to-go estimates to guide the search. Two levels of parallelism are employed within the search -- actions and cost-to-go estimates are generated in batches by the actor and critic networks respectively, and graph expansion is distributed across multiple threads. We demonstrate the effectiveness of our approach in robotic manipulation tasks, including collision-free motion planning and contact-rich interactions such as non-prehensile pushing. Visit p-achs.github.io for demonstrations and examples.

