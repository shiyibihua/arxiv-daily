---
layout: default
title: Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning
---

# Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24313" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24313v1</a>
  <a href="https://arxiv.org/pdf/2509.24313.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24313v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24313v1', 'Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Korbinian Moller, Roland Stroop, Mattia Piccinini, Alexander Langmann, Johannes Betz

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: 8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TUM-AVS/Learning-to-Sample)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ å¼•å¯¼é‡‡æ ·çš„è¿åŠ¨è§„åˆ’æ–¹æ³•ï¼Œæå‡è‡ªåŠ¨é©¾é©¶åœ¨å¤æ‚åŸå¸‚ç¯å¢ƒä¸­çš„å†³ç­–æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è¿åŠ¨è§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `é‡‡æ ·æ–¹æ³•` `ä¸–ç•Œæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åœ¨å¤æ‚åŸå¸‚ç¯å¢ƒä¸­ï¼Œä¼ ç»ŸåŸºäºé‡‡æ ·çš„è¿åŠ¨è§„åˆ’æ–¹æ³•å› å‡åŒ€æˆ–å¯å‘å¼é‡‡æ ·äº§ç”Ÿå¤§é‡ä¸å¯è¡Œæˆ–ä¸ç›¸å…³çš„è½¨è¿¹ï¼Œæ•ˆç‡è¾ƒä½ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ··åˆæ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å¼•å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œä½¿é‡‡æ ·é›†ä¸­åœ¨æ›´æœ‰å¯èƒ½äº§ç”Ÿå¯è¡Œè½¨è¿¹çš„åŒºåŸŸã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†é‡‡æ ·æ•°é‡å’Œè¿è¡Œæ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†è§„åˆ’è´¨é‡ï¼Œæå‡äº†è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å†³ç­–æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œç”¨äºè§£å†³è‡ªåŠ¨é©¾é©¶ä¸­åŸºäºé‡‡æ ·çš„è¿åŠ¨è§„åˆ’æ–¹æ³•åœ¨å¤æ‚åŸå¸‚ç¯å¢ƒä¸­æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“å¼•å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œä½¿å…¶é›†ä¸­åœ¨æ›´æœ‰å¯èƒ½äº§ç”Ÿå¯è¡Œè½¨è¿¹çš„åŠ¨ä½œç©ºé—´åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒè½¨è¿¹ç”Ÿæˆå’Œè¯„ä¼°çš„è§£ææ€§å’Œå¯éªŒè¯æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºå¯è§£ç æ·±åº¦é›†ç¼–ç å™¨çš„ä¸–ç•Œæ¨¡å‹ï¼ˆWMï¼‰ï¼Œèƒ½å¤Ÿå¤„ç†å¯å˜æ•°é‡çš„äº¤é€šå‚ä¸è€…å¹¶é‡å»ºæ½œåœ¨è¡¨ç¤ºã€‚åœ¨CommonRoadä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå‡å°‘é«˜è¾¾99%çš„é‡‡æ ·éœ€æ±‚ï¼Œå¹¶å‡å°‘é«˜è¾¾84%çš„è¿è¡Œæ—¶é—´ï¼ŒåŒæ—¶ä¿æŒè§„åˆ’è´¨é‡ï¼ˆæˆåŠŸç‡å’Œæ— ç¢°æ’ç‡ï¼‰ã€‚è¿™äº›æ”¹è¿›ä½¿å¾—è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨åŸå¸‚ç¯å¢ƒä¸­èƒ½å¤Ÿæ›´å¿«ã€æ›´å¯é åœ°åšå‡ºå†³ç­–ï¼Œä»è€Œåœ¨å®é™…çº¦æŸä¸‹å®ç°æ›´å®‰å…¨ã€æ›´çµæ•çš„å¯¼èˆªã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåœ¨è‡ªåŠ¨é©¾é©¶çš„è¿åŠ¨è§„åˆ’ä¸­ï¼ŒåŸºäºé‡‡æ ·çš„ç®—æ³•åœ¨å¤æ‚åŸå¸‚ç¯å¢ƒä¸­é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å‡åŒ€é‡‡æ ·æˆ–å¯å‘å¼é‡‡æ ·ç­–ç•¥å¾€å¾€ä¼šç”Ÿæˆå¤§é‡æ— æ•ˆæˆ–ä¸ç›¸å…³çš„è½¨è¿¹ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹å’Œè§„åˆ’æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨ä¿è¯è§„åˆ’è´¨é‡çš„åŒæ—¶ï¼Œå¿«é€Ÿæ‰¾åˆ°å¯è¡Œçš„è½¨è¿¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªRLæ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°å“ªäº›åŒºåŸŸçš„é‡‡æ ·æ›´æœ‰å¯èƒ½äº§ç”Ÿå¯è¡Œçš„è½¨è¿¹ã€‚è¿™æ ·ï¼Œé‡‡æ ·è¿‡ç¨‹ä¸å†æ˜¯ç›²ç›®çš„ï¼Œè€Œæ˜¯æœ‰é’ˆå¯¹æ€§çš„ï¼Œä»è€Œæé«˜äº†é‡‡æ ·æ•ˆç‡å’Œè§„åˆ’é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é‡‡ç”¨æ··åˆæ¡†æ¶ï¼ŒåŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŸºäºå¯è§£ç æ·±åº¦é›†ç¼–ç å™¨çš„ä¸–ç•Œæ¨¡å‹ï¼ˆWMï¼‰ï¼Œç”¨äºå¯¹ç¯å¢ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¤„ç†å¯å˜æ•°é‡çš„äº¤é€šå‚ä¸è€…ï¼Œå¹¶ç”Ÿæˆç¯å¢ƒçš„æ½œåœ¨è¡¨ç¤ºï¼›2) å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œæ ¹æ®ä¸–ç•Œæ¨¡å‹çš„è¾“å‡ºï¼ŒæŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œé€‰æ‹©æ›´æœ‰å¯èƒ½äº§ç”Ÿå¯è¡Œè½¨è¿¹çš„åŠ¨ä½œï¼›3) è½¨è¿¹ç”Ÿæˆå™¨ï¼Œæ ¹æ®RLæ™ºèƒ½ä½“é€‰æ‹©çš„åŠ¨ä½œï¼Œç”Ÿæˆè½¨è¿¹ï¼›4) è½¨è¿¹è¯„ä¼°å™¨ï¼Œä½¿ç”¨ç¡®å®šæ€§çš„å¯è¡Œæ€§æ£€æŸ¥å’Œæˆæœ¬å‡½æ•°ï¼Œè¯„ä¼°è½¨è¿¹çš„è´¨é‡ï¼Œå¹¶é€‰æ‹©æœ€ä¼˜è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥åˆ°é‡‡æ ·è¿‡ç¨‹ä¸­ã€‚ä¸ä¼ ç»Ÿçš„å‡åŒ€é‡‡æ ·æˆ–å¯å‘å¼é‡‡æ ·ç›¸æ¯”ï¼ŒRLå¼•å¯¼çš„é‡‡æ ·èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢åŠ¨ä½œç©ºé—´ï¼Œæ‰¾åˆ°å¯è¡Œçš„è½¨è¿¹ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¯è§£ç æ·±åº¦é›†ç¼–ç å™¨æ„å»ºä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†åŠ¨æ€å˜åŒ–çš„äº¤é€šç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šä¸–ç•Œæ¨¡å‹ä½¿ç”¨æ·±åº¦é›†ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå¤„ç†å¯å˜æ•°é‡çš„äº¤é€šå‚ä¸è€…ã€‚å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ç”Ÿæˆå¯è¡Œè½¨è¿¹çš„æ¦‚ç‡ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œé¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢æ–°çš„åŒºåŸŸï¼ŒåŒæ—¶åˆ©ç”¨å·²çŸ¥çš„å¯è¡ŒåŒºåŸŸã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CommonRoadä»¿çœŸç¯å¢ƒä¸­èƒ½å¤Ÿæ˜¾è‘—æå‡è¿åŠ¨è§„åˆ’çš„æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå‡å°‘é«˜è¾¾99%çš„é‡‡æ ·éœ€æ±‚ï¼Œå¹¶å‡å°‘é«˜è¾¾84%çš„è¿è¡Œæ—¶é—´ï¼ŒåŒæ—¶ä¿æŒè§„åˆ’è´¨é‡ï¼ˆæˆåŠŸç‡å’Œæ— ç¢°æ’ç‡ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å¤æ‚åŸå¸‚ç¯å¢ƒä¸­çš„è¿åŠ¨è§„åˆ’é—®é¢˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„è¿åŠ¨è§„åˆ’å’Œå†³ç­–æ§åˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åŸå¸‚äº¤é€šç¯å¢ƒä¸­ã€‚é€šè¿‡æé«˜è§„åˆ’æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œå¯ä»¥æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¯é æ€§å’Œç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æœºå™¨äººè¿åŠ¨è§„åˆ’é¢†åŸŸï¼Œä¾‹å¦‚æ— äººæœºå¯¼èˆªå’Œå·¥ä¸šæœºå™¨äººè·¯å¾„è§„åˆ’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sampling-based motion planning is a well-established approach in autonomous driving, valued for its modularity and analytical tractability. In complex urban scenarios, however, uniform or heuristic sampling often produces many infeasible or irrelevant trajectories. We address this limitation with a hybrid framework that learns where to sample while keeping trajectory generation and evaluation fully analytical and verifiable. A reinforcement learning (RL) agent guides the sampling process toward regions of the action space likely to yield feasible trajectories, while evaluation and final selection remains governed by deterministic feasibility checks and cost functions. We couple the RL sampler with a world model (WM) based on a decodable deep set encoder, enabling both variable numbers of traffic participants and reconstructable latent representations. The approach is evaluated in the CommonRoad simulation environment, showing up to 99% fewer required samples and a runtime reduction of up to 84% while maintaining planning quality in terms of success and collision-free rates. These improvements lead to faster, more reliable decision-making for autonomous vehicles in urban environments, achieving safer and more responsive navigation under real-world constraints. Code and trained artifacts are publicly available at: https://github.com/TUM-AVS/Learning-to-Sample

