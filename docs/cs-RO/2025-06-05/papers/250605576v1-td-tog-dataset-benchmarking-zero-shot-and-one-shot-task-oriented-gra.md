---
layout: default
title: TD-TOG Dataset: Benchmarking Zero-Shot and One-Shot Task-Oriented Grasping for Object Generalization
---

# TD-TOG Dataset: Benchmarking Zero-Shot and One-Shot Task-Oriented Grasping for Object Generalization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05576" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05576v1</a>
  <a href="https://arxiv.org/pdf/2506.05576.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05576v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05576v1', 'TD-TOG Dataset: Benchmarking Zero-Shot and One-Shot Task-Oriented Grasping for Object Generalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Valerija Holomjova, Jamie Grech, Dewei Yi, Bruno Yun, Andrew Starkey, Pascal MeiÃŸner

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTD-TOGæ•°æ®é›†ä»¥è§£å†³TOGæ•°æ®ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `ä»»åŠ¡å¯¼å‘æŠ“å–` `é›¶-shotå­¦ä¹ ` `ä¸€-shotå­¦ä¹ ` `æœºå™¨äººæŠ€æœ¯` `æ•°æ®é›†æ„å»º` `ç‰©ä½“è¯†åˆ«` `å¯ç”¨æ€§è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰TOGæ•°æ®é›†æ•°é‡ä¸è¶³ï¼Œä¸”å¤šä¸ºåˆæˆæ•°æ®ï¼Œæ ‡æ³¨è´¨é‡ä½ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. æå‡ºTD-TOGæ•°æ®é›†ï¼ŒåŒ…å«ä¸°å¯Œçš„çœŸå®åœºæ™¯å’Œå…¨é¢çš„æ ‡æ³¨ï¼Œæ”¯æŒTOGè§£å†³æ–¹æ¡ˆçš„è®­ç»ƒä¸è¯„ä¼°ã€‚
3. Binary-TOGæ¡†æ¶å®ç°äº†68.9%çš„æŠ“å–å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†é›¶-shotå’Œä¸€-shotå­¦ä¹ åœ¨ç‰©ä½“æ³›åŒ–ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»»åŠ¡å¯¼å‘æŠ“å–ï¼ˆTOGï¼‰æ˜¯æœºå™¨äººæ‰§è¡Œä»»åŠ¡çš„å…³é”®æ­¥éª¤ï¼Œæ¶‰åŠé¢„æµ‹ç›®æ ‡ç‰©ä½“çš„æŠ“å–åŒºåŸŸä»¥ä¾¿äºå®Œæˆç‰¹å®šä»»åŠ¡ã€‚ç°æœ‰çš„TOGæ•°æ®é›†æ•°é‡æœ‰é™ï¼Œä¸”å¤šä¸ºåˆæˆæ•°æ®æˆ–å­˜åœ¨æ ‡æ³¨ä¼ªå½±ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Top-down Task-oriented Graspingï¼ˆTD-TOGï¼‰æ•°æ®é›†ï¼ŒåŒ…å«1,449ä¸ªçœŸå®ä¸–ç•Œçš„RGB-Dåœºæ™¯ï¼Œæ¶µç›–30ä¸ªç‰©ä½“ç±»åˆ«å’Œ120ä¸ªå­ç±»åˆ«ï¼Œæä¾›æ‰‹å·¥æ ‡æ³¨çš„ç‰©ä½“æ©ç ã€å¯ç”¨æ€§å’Œå¹³é¢çŸ©å½¢æŠ“å–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†Binary-TOGæ¡†æ¶ï¼Œåˆ©ç”¨é›¶-shotå­¦ä¹ è¿›è¡Œç‰©ä½“è¯†åˆ«å’Œä¸€-shotå­¦ä¹ è¿›è¡Œå¯ç”¨æ€§è¯†åˆ«ï¼Œåœ¨å¤šç‰©ä½“åœºæ™¯ä¸­å®ç°äº†68.9%çš„ä»»åŠ¡å¯¼å‘æŠ“å–å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰TOGæ•°æ®é›†ç¨€ç¼ºå’Œæ ‡æ³¨è´¨é‡ä½çš„é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºTD-TOGæ•°æ®é›†ï¼ŒåŒ…å«ä¸°å¯Œçš„çœŸå®åœºæ™¯å’Œå…¨é¢çš„æ ‡æ³¨ï¼Œæ”¯æŒTOGè§£å†³æ–¹æ¡ˆçš„è®­ç»ƒä¸è¯„ä¼°ï¼ŒåŒæ—¶å¼•å…¥Binary-TOGæ¡†æ¶ï¼Œç»“åˆé›¶-shotå’Œä¸€-shotå­¦ä¹ ä»¥æé«˜ç‰©ä½“è¯†åˆ«å’Œå¯ç”¨æ€§è¯†åˆ«çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTD-TOGæ•°æ®é›†ç”±1,449ä¸ªRGB-Dåœºæ™¯ç»„æˆï¼ŒåŒ…å«30ä¸ªç‰©ä½“ç±»åˆ«å’Œ120ä¸ªå­ç±»åˆ«ã€‚Binary-TOGæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé›¶-shotç‰©ä½“è¯†åˆ«å’Œä¸€-shotå¯ç”¨æ€§è¯†åˆ«ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºè¿›è¡Œç‰©ä½“è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šTD-TOGæ•°æ®é›†çš„æå‡ºå¡«è¡¥äº†TOGæ•°æ®é›†çš„ç©ºç™½ï¼ŒBinary-TOGæ¡†æ¶é€šè¿‡é›¶-shotå­¦ä¹ æ¶ˆé™¤äº†å¯¹è§†è§‰å‚è€ƒçš„ä¾èµ–ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤šç‰©ä½“åœºæ™¯ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šBinary-TOGæ¡†æ¶çš„è®¾è®¡åŒ…æ‹¬ä½¿ç”¨æ–‡æœ¬æç¤ºè¿›è¡Œç‰©ä½“è¯†åˆ«ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æŠ“å–å‡†ç¡®æ€§ï¼Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ä»¥æ”¯æŒé«˜æ•ˆçš„å­¦ä¹ å’Œæ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šç‰©ä½“åœºæ™¯ä¸­ï¼ŒBinary-TOGæ¡†æ¶å®ç°äº†68.9%çš„ä»»åŠ¡å¯¼å‘æŠ“å–å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨é›¶-shotå’Œä¸€-shotå­¦ä¹ æ–¹é¢çš„ä¼˜åŠ¿ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTD-TOGæ•°æ®é›†çš„å¼•å…¥æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œæ™ºèƒ½å®¶å±…ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æŠ“å–å’Œæ“ä½œèƒ½åŠ›ã€‚æœªæ¥ï¼ŒTD-TOGæ•°æ®é›†å’ŒBinary-TOGæ¡†æ¶å°†ä¸ºæœºå™¨äººè‡ªä¸»å­¦ä¹ å’Œé€‚åº”æ–°ç‰©ä½“æä¾›é‡è¦æ”¯æŒï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Task-oriented grasping (TOG) is an essential preliminary step for robotic task execution, which involves predicting grasps on regions of target objects that facilitate intended tasks. Existing literature reveals there is a limited availability of TOG datasets for training and benchmarking despite large demand, which are often synthetic or have artifacts in mask annotations that hinder model performance. Moreover, TOG solutions often require affordance masks, grasps, and object masks for training, however, existing datasets typically provide only a subset of these annotations. To address these limitations, we introduce the Top-down Task-oriented Grasping (TD-TOG) dataset, designed to train and evaluate TOG solutions. TD-TOG comprises 1,449 real-world RGB-D scenes including 30 object categories and 120 subcategories, with hand-annotated object masks, affordances, and planar rectangular grasps. It also features a test set for a novel challenge that assesses a TOG solution's ability to distinguish between object subcategories. To contribute to the demand for TOG solutions that can adapt and manipulate previously unseen objects without re-training, we propose a novel TOG framework, Binary-TOG. Binary-TOG uses zero-shot for object recognition, and one-shot learning for affordance recognition. Zero-shot learning enables Binary-TOG to identify objects in multi-object scenes through textual prompts, eliminating the need for visual references. In multi-object settings, Binary-TOG achieves an average task-oriented grasp accuracy of 68.9%. Lastly, this paper contributes a comparative analysis between one-shot and zero-shot learning for object generalization in TOG to be used in the development of future TOG solutions.

