---
layout: default
title: Embodied Navigation Foundation Model
---

# Embodied Navigation Foundation Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.12129" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.12129v2</a>
  <a href="https://arxiv.org/pdf/2509.12129.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.12129v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.12129v2', 'Embodied Navigation Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiazhao Zhang, Anqi Li, Yunpeng Qi, Minghan Li, Jiahang Liu, Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li, Yuxin Fan, Wenjun Li, Zhibo Chen, Fei Gao, Qi Wu, Zhizheng Zhang, He Wang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-15 (æ›´æ–°: 2025-09-16)

**å¤‡æ³¨**: Project Page: https://pku-epic.github.io/NavFoM-Web/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨å…·èº«ã€è·¨ä»»åŠ¡çš„å¯¼èˆªåŸºç¡€æ¨¡å‹NavFoMï¼Œæå‡å…·èº«æ™ºèƒ½å¯¼èˆªçš„æ³›åŒ–èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `å¯¼èˆªåŸºç¡€æ¨¡å‹` `è·¨å…·èº«` `è·¨ä»»åŠ¡` `è§†è§‰è¯­è¨€å¯¼èˆª` `Transformer` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å…·èº«å¯¼èˆªä¸­çš„æ³›åŒ–èƒ½åŠ›å—é™äºç‰¹å®šä»»åŠ¡å’Œå…·èº«æ¶æ„ï¼Œéš¾ä»¥é€‚åº”å¤æ‚ç¯å¢ƒã€‚
2. NavFoMé€šè¿‡ç»Ÿä¸€æ¶æ„å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œå¹¶å¼•å…¥æ ‡è¯†ç¬¦tokenåµŒå…¥ç›¸æœºè§†å›¾å’Œæ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæå‡æ³›åŒ–æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒNavFoMåœ¨å¤šä¸ªå¯¼èˆªä»»åŠ¡å’Œå…·èº«æœºå™¨äººä¸Šå–å¾—äº†é¢†å…ˆæ€§èƒ½ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼Œå¹¶éªŒè¯äº†å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨å…·èº«å’Œè·¨ä»»åŠ¡çš„å¯¼èˆªåŸºç¡€æ¨¡å‹(NavFoM)ï¼Œè¯¥æ¨¡å‹åœ¨åŒ…å«å››è¶³æœºå™¨äººã€æ— äººæœºã€è½®å¼æœºå™¨äººå’Œè½¦è¾†çš„å…«ç™¾ä¸‡å¯¼èˆªæ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–äº†è§†è§‰è¯­è¨€å¯¼èˆªã€ç‰©ä½“æœç´¢ã€ç›®æ ‡è·Ÿè¸ªå’Œè‡ªåŠ¨é©¾é©¶ç­‰å¤šç§ä»»åŠ¡ã€‚NavFoMé‡‡ç”¨ç»Ÿä¸€çš„æ¶æ„ï¼Œå¤„ç†æ¥è‡ªä¸åŒç›¸æœºé…ç½®å’Œå¯¼èˆªèŒƒå›´çš„å¤šæ¨¡æ€å¯¼èˆªè¾“å…¥ã€‚ä¸ºäº†é€‚åº”ä¸åŒçš„ç›¸æœºè®¾ç½®å’Œæ—¶é—´èŒƒå›´ï¼ŒNavFoMé›†æˆäº†æ ‡è¯†ç¬¦tokenï¼ŒåµŒå…¥äº†å…·èº«æœºå™¨äººçš„ç›¸æœºè§†å›¾ä¿¡æ¯å’Œä»»åŠ¡çš„æ—¶é—´ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ»¡è¶³å®é™…éƒ¨ç½²çš„éœ€æ±‚ï¼ŒNavFoMåœ¨æœ‰é™çš„tokené•¿åº¦é¢„ç®—ä¸‹ï¼Œä½¿ç”¨åŠ¨æ€è°ƒæ•´çš„é‡‡æ ·ç­–ç•¥æ¥æ§åˆ¶æ‰€æœ‰è§‚å¯Ÿtokenã€‚åœ¨å…¬å…±åŸºå‡†ä¸Šçš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªå¯¼èˆªä»»åŠ¡å’Œå…·èº«æœºå™¨äººä¸Šå®ç°äº†æœ€å…ˆè¿›æˆ–æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè€Œæ— éœ€ç‰¹å®šäºä»»åŠ¡çš„å¾®è°ƒã€‚é¢å¤–çš„çœŸå®ä¸–ç•Œå®éªŒè¿›ä¸€æ­¥è¯å®äº†è¯¥æ–¹æ³•çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›å’Œå®é™…é€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å…·èº«å¯¼èˆªæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œåœ¨æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™æ€§ã€‚å®ƒä»¬é€šå¸¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡å’Œç‰¹å®šçš„å…·èº«æœºå™¨äººæ¶æ„è¿›è¡Œä¼˜åŒ–ï¼Œéš¾ä»¥é€‚åº”ä¸åŒç±»å‹çš„æœºå™¨äººï¼ˆå¦‚å››è¶³æœºå™¨äººã€æ— äººæœºã€è½®å¼æœºå™¨äººç­‰ï¼‰å’Œä¸åŒçš„å¯¼èˆªä»»åŠ¡ï¼ˆå¦‚è§†è§‰è¯­è¨€å¯¼èˆªã€ç‰©ä½“æœç´¢ã€ç›®æ ‡è·Ÿè¸ªç­‰ï¼‰ã€‚è¿™ç§ç¼ºä¹é€šç”¨æ€§çš„é—®é¢˜é™åˆ¶äº†å…·èº«æ™ºèƒ½åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNavFoMçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªé€šç”¨çš„å¯¼èˆªåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å¤§è§„æ¨¡çš„å¤šæ ·åŒ–æ•°æ®è®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„å…·èº«æœºå™¨äººå’Œå¯¼èˆªä»»åŠ¡ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç»Ÿä¸€çš„æ¶æ„æ¥å¤„ç†æ¥è‡ªä¸åŒç›¸æœºé…ç½®å’Œå¯¼èˆªèŒƒå›´çš„å¤šæ¨¡æ€å¯¼èˆªè¾“å…¥ï¼Œå¹¶é€šè¿‡å¼•å…¥æ ‡è¯†ç¬¦tokenæ¥åµŒå…¥ç›¸æœºè§†å›¾ä¿¡æ¯å’Œä»»åŠ¡çš„æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNavFoMçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€è¾“å…¥å¤„ç†æ¨¡å—ï¼šè´Ÿè´£å¤„ç†æ¥è‡ªä¸åŒä¼ æ„Ÿå™¨ï¼ˆå¦‚æ‘„åƒå¤´ã€æ¿€å…‰é›·è¾¾ç­‰ï¼‰çš„è¾“å…¥æ•°æ®ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç»Ÿä¸€çš„è¡¨ç¤ºå½¢å¼ã€‚2) æ ‡è¯†ç¬¦TokenåµŒå…¥æ¨¡å—ï¼šå°†ç›¸æœºè§†å›¾ä¿¡æ¯å’Œä»»åŠ¡çš„æ—¶é—´ä¸Šä¸‹æ–‡ç¼–ç ä¸ºæ ‡è¯†ç¬¦tokenï¼Œå¹¶å°†å…¶ä¸è¾“å…¥æ•°æ®èåˆã€‚3) Transformerç¼–ç å™¨ï¼šä½¿ç”¨Transformerç¼–ç å™¨å¯¹èåˆåçš„æ•°æ®è¿›è¡Œç¼–ç ï¼Œæå–ç‰¹å¾ã€‚4) åŠ¨æ€Tokené‡‡æ ·æ¨¡å—ï¼šåœ¨æœ‰é™çš„tokené•¿åº¦é¢„ç®—ä¸‹ï¼ŒåŠ¨æ€è°ƒæ•´é‡‡æ ·ç­–ç•¥ï¼Œé€‰æ‹©é‡è¦çš„è§‚å¯Ÿtokenã€‚5) å¯¼èˆªç­–ç•¥è¾“å‡ºæ¨¡å—ï¼šæ ¹æ®ç¼–ç åçš„ç‰¹å¾ï¼Œè¾“å‡ºå¯¼èˆªç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šNavFoMçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) è·¨å…·èº«å’Œè·¨ä»»åŠ¡çš„ç»Ÿä¸€æ¶æ„ï¼šèƒ½å¤Ÿå¤„ç†ä¸åŒç±»å‹çš„æœºå™¨äººå’Œå¯¼èˆªä»»åŠ¡ã€‚2) æ ‡è¯†ç¬¦tokenåµŒå…¥ï¼šèƒ½å¤ŸåµŒå…¥ç›¸æœºè§†å›¾ä¿¡æ¯å’Œä»»åŠ¡çš„æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚3) åŠ¨æ€Tokené‡‡æ ·ï¼šèƒ½å¤Ÿåœ¨æœ‰é™çš„tokené•¿åº¦é¢„ç®—ä¸‹ï¼Œé€‰æ‹©é‡è¦çš„è§‚å¯Ÿtokenï¼Œæé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šNavFoMçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformerä½œä¸ºæ ¸å¿ƒç¼–ç å™¨ï¼Œä»¥æ•æ‰è¾“å…¥æ•°æ®ä¹‹é—´çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚2) è®¾è®¡äº†ä¸“é—¨çš„æ ‡è¯†ç¬¦tokenï¼Œç”¨äºç¼–ç ç›¸æœºè§†å›¾ä¿¡æ¯å’Œä»»åŠ¡çš„æ—¶é—´ä¸Šä¸‹æ–‡ã€‚3) é‡‡ç”¨åŠ¨æ€Tokené‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®tokençš„é‡è¦æ€§åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¦‚ç‡ã€‚4) ä½¿ç”¨å¤§è§„æ¨¡çš„å¤šæ ·åŒ–æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

NavFoMåœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›æˆ–æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰è¯­è¨€å¯¼èˆªä»»åŠ¡ä¸­ï¼ŒNavFoMçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¹ŸéªŒè¯äº†NavFoMçš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨æ½œåŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

NavFoMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€ç‰©æµé…é€ã€å®¶åº­æœåŠ¡æœºå™¨äººã€å®‰é˜²å·¡æ£€ç­‰é¢†åŸŸã€‚é€šè¿‡é¢„è®­ç»ƒçš„NavFoMï¼Œå¯ä»¥å¿«é€Ÿéƒ¨ç½²åˆ°æ–°çš„æœºå™¨äººå¹³å°å’Œå¯¼èˆªä»»åŠ¡ä¸­ï¼Œé™ä½å¼€å‘æˆæœ¬ï¼ŒåŠ é€Ÿå…·èº«æ™ºèƒ½çš„è½åœ°ã€‚æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢NavFoMåœ¨æ›´å¤æ‚ç¯å¢ƒå’Œä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä¾‹å¦‚ç¾éš¾æ•‘æ´ã€åŒ»ç–—è¾…åŠ©ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Navigation is a fundamental capability in embodied AI, representing the intelligence required to perceive and interact within physical environments following language instructions. Despite significant progress in large Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance on general vision-language tasks, their generalization ability in embodied navigation remains largely confined to narrow task settings and embodiment-specific architectures. In this work, we introduce a cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained on eight million navigation samples that encompass quadrupeds, drones, wheeled robots, and vehicles, and spanning diverse tasks such as vision-and-language navigation, object searching, target tracking, and autonomous driving. NavFoM employs a unified architecture that processes multimodal navigation inputs from varying camera configurations and navigation horizons. To accommodate diverse camera setups and temporal horizons, NavFoM incorporates identifier tokens that embed camera view information of embodiments and the temporal context of tasks. Furthermore, to meet the demands of real-world deployment, NavFoM controls all observation tokens using a dynamically adjusted sampling strategy under a limited token length budget. Extensive evaluations on public benchmarks demonstrate that our model achieves state-of-the-art or highly competitive performance across multiple navigation tasks and embodiments without requiring task-specific fine-tuning. Additional real-world experiments further confirm the strong generalization capability and practical applicability of our approach.

