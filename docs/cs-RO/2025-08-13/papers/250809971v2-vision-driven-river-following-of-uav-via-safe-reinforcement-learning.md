---
layout: default
title: Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model
---

# Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09971" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09971v2</a>
  <a href="https://arxiv.org/pdf/2508.09971.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09971v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09971v2', 'Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihan Wang, Nina Mahmoudian

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-09-30)

**å¤‡æ³¨**: Submitted to Robotics and Autonomous Systems (RAS) journal

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®‰å…¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³æ— äººæœºæ²³æµè·Ÿéšé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ— äººæœº` `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `æ²³æµè·Ÿéš` `è¯­ä¹‰åŠ¨æ€æ¨¡å‹` `è¾¹é™…å¢ç›Šä¼˜åŠ¿ä¼°è®¡` `ç¯å¢ƒç›‘æµ‹` `è‡ªä¸»å¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚æ²³æµç¯å¢ƒä¸­è¿›è¡Œå®‰å…¨å¯¼èˆªæ—¶ï¼Œé¢ä¸´GPSä¿¡å·ä¸å¯é å’Œå†å²ä¾èµ–å¥–åŠ±çš„æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºäº†è¾¹é™…å¢ç›Šä¼˜åŠ¿ä¼°è®¡ã€è¯­ä¹‰åŠ¨æ€æ¨¡å‹å’Œçº¦æŸæ¼”å‘˜åŠ¨æ€ä¼°è®¡å™¨ï¼Œæ—¨åœ¨ä¼˜åŒ–å®‰å…¨å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMGAEåœ¨æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„åŸºäºè¯„è®ºè€…çš„æ–¹æ³•ï¼ŒSDMæä¾›æ›´å‡†ç¡®çš„çŸ­æœŸçŠ¶æ€é¢„æµ‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºè§†è§‰çš„æ— äººæœºè‡ªä¸»æ²³æµè·Ÿéšå¯¹äºæ•‘æ´ã€ç›‘è§†å’Œç¯å¢ƒç›‘æµ‹ç­‰åº”ç”¨è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨GPSä¿¡å·ä¸å¯é çš„å¯†é›†æ²³æµç¯å¢ƒä¸­ã€‚è¿™äº›å®‰å…¨å…³é”®çš„å¯¼èˆªä»»åŠ¡å¿…é¡»åœ¨ä¼˜åŒ–æ€§èƒ½çš„åŒæ—¶æ»¡è¶³ä¸¥æ ¼çš„å®‰å…¨çº¦æŸã€‚æ­¤å¤–ï¼Œæ²³æµè·Ÿéšçš„å¥–åŠ±æœ¬è´¨ä¸Šæ˜¯å†å²ä¾èµ–çš„ï¼ˆéé©¬å°”å¯å¤«ï¼‰ï¼Œè¿™ä½¿å¾—æ ‡å‡†çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ ï¼ˆSafeRLï¼‰é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸‰ä¸ªè´¡çŒ®ï¼šé¦–å…ˆï¼Œå¼•å…¥è¾¹é™…å¢ç›Šä¼˜åŠ¿ä¼°è®¡ï¼ˆMGAEï¼‰ï¼Œé€šè¿‡ä½¿ç”¨å†å²å›æŠ¥çš„æ»‘åŠ¨çª—å£åŸºçº¿æ¥ä¼˜åŒ–å¥–åŠ±ä¼˜åŠ¿å‡½æ•°ã€‚å…¶æ¬¡ï¼Œå¼€å‘äº†åŸºäºæ°´ä½“è¯­ä¹‰æ©ç çš„è¯­ä¹‰åŠ¨æ€æ¨¡å‹ï¼ˆSDMï¼‰ï¼Œæä¾›æ›´å…·å¯è§£é‡Šæ€§å’Œæ•°æ®æ•ˆç‡çš„çŸ­æœŸé¢„æµ‹ã€‚æœ€åï¼Œæå‡ºäº†çº¦æŸæ¼”å‘˜åŠ¨æ€ä¼°è®¡å™¨ï¼ˆCADEï¼‰æ¶æ„ï¼Œå°†æ¼”å‘˜ã€æˆæœ¬ä¼°è®¡å™¨å’ŒSDMé›†æˆåœ¨ä¸€èµ·ï¼Œå½¢æˆåŸºäºæ¨¡å‹çš„SafeRLæ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— äººæœºåœ¨å¤æ‚æ²³æµç¯å¢ƒä¸­è¿›è¡Œå®‰å…¨è‡ªä¸»å¯¼èˆªçš„é—®é¢˜ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å†å²ä¾èµ–å¥–åŠ±å’Œå®‰å…¨çº¦æŸæ—¶å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨GPSä¿¡å·ä¸å¯é çš„æƒ…å†µä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥è¾¹é™…å¢ç›Šä¼˜åŠ¿ä¼°è®¡ï¼ˆMGAEï¼‰å’Œè¯­ä¹‰åŠ¨æ€æ¨¡å‹ï¼ˆSDMï¼‰ï¼Œæ¥ä¼˜åŒ–å¥–åŠ±ä¼°è®¡å’ŒçŠ¶æ€é¢„æµ‹ï¼Œä»è€Œæé«˜å®‰å…¨å¼ºåŒ–å­¦ä¹ çš„æ•ˆæœã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”éé©¬å°”å¯å¤«åŠ¨æ€ï¼Œå¹¶å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¾¹é™…å¢ç›Šä¼˜åŠ¿ä¼°è®¡ï¼ˆMGAEï¼‰ã€è¯­ä¹‰åŠ¨æ€æ¨¡å‹ï¼ˆSDMï¼‰å’Œçº¦æŸæ¼”å‘˜åŠ¨æ€ä¼°è®¡å™¨ï¼ˆCADEï¼‰ã€‚MGAEç”¨äºä¼˜åŒ–å¥–åŠ±ä¼˜åŠ¿ï¼ŒSDMç”¨äºçŸ­æœŸçŠ¶æ€é¢„æµ‹ï¼Œè€ŒCADEåˆ™æ•´åˆäº†æ¼”å‘˜å’Œæˆæœ¬ä¼°è®¡å™¨ä»¥å®ç°å®‰å…¨å¼ºåŒ–å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†MGAEå’ŒSDMï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å†å²ä¾èµ–æ€§å’Œå®‰å…¨çº¦æŸã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒMGAEé€šè¿‡æ»‘åŠ¨çª—å£åŸºçº¿ä¼˜åŒ–å¥–åŠ±ä¼°è®¡ï¼ŒSDMåˆ™æä¾›äº†æ›´é«˜çš„æ•°æ®æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMGAEä½¿ç”¨å†å²å›æŠ¥çš„æ»‘åŠ¨çª—å£ä½œä¸ºåŸºçº¿ï¼ŒSDMåŸºäºæ°´ä½“è¯­ä¹‰æ©ç è¿›è¡ŒçŠ¶æ€é¢„æµ‹ï¼ŒCADEåˆ™é€šè¿‡Lagrangianæ–¹æ³•åœ¨è®­ç»ƒä¸­å®ç°å¥–åŠ±ä¸å®‰å…¨çš„â€œè½¯â€å¹³è¡¡ï¼ŒåŒæ—¶åœ¨æ¨ç†æ—¶æ–½åŠ â€œç¡¬â€åŠ¨ä½œè¦†ç›–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMGAEåœ¨æ”¶æ•›é€Ÿåº¦ä¸Šæ¯”ä¼ ç»Ÿçš„è¯„è®ºè€…æ–¹æ³•ï¼ˆå¦‚å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰æ›´å¿«ï¼Œä¸”æ€§èƒ½ä¼˜è¶Šã€‚SDMæä¾›çš„çŸ­æœŸçŠ¶æ€é¢„æµ‹å‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼Œä½¿å¾—æˆæœ¬ä¼°è®¡å™¨èƒ½å¤Ÿæ›´å¥½åœ°é¢„æµ‹æ½œåœ¨çš„å®‰å…¨è¿è§„æƒ…å†µã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ— äººæœºåœ¨æ•‘æ´ã€ç¯å¢ƒç›‘æµ‹å’Œç›‘è§†ç­‰ä»»åŠ¡ä¸­çš„è‡ªä¸»å¯¼èˆªã€‚é€šè¿‡æé«˜åœ¨å¤æ‚æ²³æµç¯å¢ƒä¸­çš„å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œç ”ç©¶æˆæœå°†ä¸ºæ— äººæœºæŠ€æœ¯çš„å®é™…åº”ç”¨æä¾›é‡è¦æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-driven autonomous river following by Unmanned Aerial Vehicles is critical for applications such as rescue, surveillance, and environmental monitoring, particularly in dense riverine environments where GPS signals are unreliable. These safety-critical navigation tasks must satisfy hard safety constraints while optimizing performance. Moreover, the reward in river following is inherently history-dependent (non-Markovian) by which river segment has already been visited, making it challenging for standard safe Reinforcement Learning (SafeRL). To address these gaps, we propose three contributions. First, we introduce Marginal Gain Advantage Estimation, which refines the reward advantage function by using a sliding window baseline computed from historical episodic returns, aligning the advantage estimate with non-Markovian dynamics. Second, we develop a Semantic Dynamics Model based on patchified water semantic masks offering more interpretable and data-efficient short-term prediction of future observations compared to latent vision dynamics models. Third, we present the Constrained Actor Dynamics Estimator architecture, which integrates the actor, cost estimator, and SDM for cost advantage estimation to form a model-based SafeRL framework. Simulation results demonstrate that MGAE achieves faster convergence and superior performance over traditional critic-based methods like Generalized Advantage Estimation. SDM provides more accurate short-term state predictions that enable the cost estimator to better predict potential violations. Overall, CADE effectively integrates safety regulation into model-based RL, with the Lagrangian approach providing a "soft" balance between reward and safety during training, while the safety layer enhances inference by imposing a "hard" action overlay.

