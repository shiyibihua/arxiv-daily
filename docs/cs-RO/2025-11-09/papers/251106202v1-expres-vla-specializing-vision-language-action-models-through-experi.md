---
layout: default
title: ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval
---

# ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval

**arXiv**: [2511.06202v1](https://arxiv.org/abs/2511.06202) | [PDF](https://arxiv.org/pdf/2511.06202.pdf)

**ä½œè€…**: Shahram Najam Syed, Yatharth Ahuja, Arthur Jakobsson, Jeff Ichnowski

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-09

**å¤‡æ³¨**: 10 pages, 5 figures, submitted to ICRA 2026. Equal contribution by first two authors

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ExpReS-VLAï¼šé€šè¿‡ç»éªŒå›žæ”¾ä¸Žæ£€ç´¢å®žçŽ°VLAæ¨¡åž‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„é«˜æ•ˆç‰¹åŒ–**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `ç»éªŒå›žæ”¾` `ç»éªŒæ£€ç´¢` `æœºå™¨äººæ“ä½œ` `æ¨¡åž‹ç‰¹åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­æ³›åŒ–èƒ½åŠ›å¼ºï¼Œä½†éš¾ä»¥é«˜æ•ˆé€‚åº”æ–°çŽ¯å¢ƒï¼Œåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨çŽ°ä¸ç¨³å®šã€‚
2. ExpReS-VLAé€šè¿‡ç»éªŒå›žæ”¾ä¸Žæ£€ç´¢ï¼Œåˆ©ç”¨ç´§å‡‘ç‰¹å¾è¡¨ç¤ºå­˜å‚¨ç»éªŒï¼Œå¹¶å¼•å…¥é˜ˆå€¼æ··åˆå¯¹æ¯”æŸå¤±ï¼Œæå‡æ¨¡åž‹ç‰¹åŒ–èƒ½åŠ›ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒExpReS-VLAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººä»»åŠ¡ä¸­å‡æ˜¾è‘—æå‡æˆåŠŸçŽ‡ï¼Œä¸”é€‚åº”é€Ÿåº¦å¿«ï¼Œå…·æœ‰å®žé™…éƒ¨ç½²ä»·å€¼ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºExpReS-VLAï¼Œä¸€ç§é€šè¿‡ç»éªŒå›žæ”¾å’Œæ£€ç´¢æ¥ç‰¹åŒ–é¢„è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼Œä»Žè€Œæå‡æ¨¡åž‹åœ¨ç‰¹å®šéƒ¨ç½²çŽ¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•å­˜å‚¨æ¥è‡ªå†»ç»“è§†è§‰éª¨å¹²ç½‘ç»œçš„ç´§å‡‘ç‰¹å¾è¡¨ç¤ºï¼Œè€ŒéžåŽŸå§‹å›¾åƒ-åŠ¨ä½œå¯¹ï¼Œä»Žè€Œå°†å†…å­˜ä½¿ç”¨é‡å‡å°‘çº¦97%ã€‚åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³çš„åŽ†å²ç»éªŒï¼Œå¹¶åˆ©ç”¨æ£€ç´¢åˆ°çš„ç»éªŒæŒ‡å¯¼æ¨¡åž‹é€‚åº”ï¼ŒåŒæ—¶ä¼˜å…ˆå›žæ”¾æˆåŠŸçš„è½¨è¿¹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†é˜ˆå€¼æ··åˆå¯¹æ¯”æŸå¤±ï¼Œä»Žè€Œèƒ½å¤Ÿä»ŽæˆåŠŸå’Œå¤±è´¥çš„å°è¯•ä¸­è¿›è¡Œå­¦ä¹ ã€‚åœ¨LIBEROæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­ï¼ŒExpReS-VLAåœ¨ç©ºé—´æŽ¨ç†ä»»åŠ¡ä¸Šçš„æˆåŠŸçŽ‡ä»Ž82.6%æé«˜åˆ°93.1%ï¼Œåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸Šçš„æˆåŠŸçŽ‡ä»Ž61%æé«˜åˆ°72.3%ã€‚åœ¨åŒ…å«äº”ä¸ªæ“ä½œä»»åŠ¡çš„ç‰©ç†æœºå™¨äººå®žéªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å·²è§å’Œæœªè§åœºæ™¯ä¸­å‡è¾¾åˆ°98%çš„æˆåŠŸçŽ‡ï¼Œè€Œæœ´ç´ å¾®è°ƒçš„æˆåŠŸçŽ‡åˆ†åˆ«ä¸º84.7%å’Œ32%ã€‚ä½¿ç”¨å•ä¸ªRTX 5090 GPUï¼Œé€šè¿‡12ä¸ªæ¼”ç¤ºè¿›è¡Œé€‚åº”ä»…éœ€31ç§’ï¼Œä½¿å¾—è¯¥æ–¹æ³•åœ¨å®žé™…æœºå™¨äººéƒ¨ç½²ä¸­å…·æœ‰å¯è¡Œæ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹åœ¨ç‰¹å®šæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œéš¾ä»¥é«˜æ•ˆé€‚åº”æ–°çŽ¯å¢ƒçš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¾®è°ƒï¼Œä½†å®¹æ˜“å‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œå¯¼è‡´æ¨¡åž‹åœ¨å·²å­¦ä¹ ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œç›´æŽ¥å­˜å‚¨åŽŸå§‹å›¾åƒ-åŠ¨ä½œå¯¹è¿›è¡Œç»éªŒå›žæ”¾ä¼šæ¶ˆè€—å¤§é‡å†…å­˜ï¼Œé™åˆ¶äº†å…¶åœ¨å®žé™…æœºå™¨äººåº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»éªŒå›žæ”¾å’Œæ£€ç´¢ï¼Œç»“åˆç´§å‡‘çš„ç‰¹å¾è¡¨ç¤ºï¼Œæ¥æŒ‡å¯¼VLAæ¨¡åž‹åœ¨æ–°çŽ¯å¢ƒä¸­çš„é€‚åº”ã€‚é€šè¿‡å­˜å‚¨åŽ†å²ç»éªŒï¼Œæ¨¡åž‹å¯ä»¥é¿å…ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢åˆ°çš„ç›¸å…³ç»éªŒæ¥åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå¼•å…¥é˜ˆå€¼æ··åˆå¯¹æ¯”æŸå¤±ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿä»ŽæˆåŠŸå’Œå¤±è´¥çš„å°è¯•ä¸­å­¦ä¹ ï¼Œè¿›ä¸€æ­¥æå‡å­¦ä¹ æ•ˆçŽ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šExpReS-VLAçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç‰¹å¾æå–ï¼šä½¿ç”¨å†»ç»“çš„è§†è§‰éª¨å¹²ç½‘ç»œæå–å›¾åƒçš„ç´§å‡‘ç‰¹å¾è¡¨ç¤ºã€‚2) ç»éªŒå­˜å‚¨ï¼šå°†æå–çš„ç‰¹å¾è¡¨ç¤ºå’Œå¯¹åº”çš„åŠ¨ä½œå­˜å‚¨åˆ°ç»éªŒæ± ä¸­ã€‚3) ç»éªŒæ£€ç´¢ï¼šåœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢ä¸Žå½“å‰çŠ¶æ€ç›¸å…³çš„åŽ†å²ç»éªŒã€‚4) ç­–ç•¥æ›´æ–°ï¼šåˆ©ç”¨æ£€ç´¢åˆ°çš„ç»éªŒå’Œä¼˜å…ˆç»éªŒå›žæ”¾æ¥æ›´æ–°VLAæ¨¡åž‹çš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šExpReS-VLAçš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºŽï¼š1) ä½¿ç”¨ç´§å‡‘çš„ç‰¹å¾è¡¨ç¤ºæ¥å­˜å‚¨ç»éªŒï¼Œæ˜¾è‘—é™ä½Žäº†å†…å­˜æ¶ˆè€—ã€‚2) å¼•å…¥äº†é˜ˆå€¼æ··åˆå¯¹æ¯”æŸå¤±ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿä»ŽæˆåŠŸå’Œå¤±è´¥çš„å°è¯•ä¸­å­¦ä¹ ã€‚3) ç»“åˆç»éªŒå›žæ”¾å’Œæ£€ç´¢ï¼Œå®žçŽ°äº†é«˜æ•ˆçš„æ¨¡åž‹ç‰¹åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç‰¹å¾æå–æ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰éª¨å¹²ç½‘ç»œï¼Œå¹¶å°†å…¶å‚æ•°å†»ç»“ï¼Œä»¥é¿å…ç¾éš¾æ€§é—å¿˜ã€‚åœ¨ç»éªŒæ£€ç´¢æ–¹é¢ï¼Œä½¿ç”¨äº†ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¡¡é‡çŠ¶æ€ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚åœ¨é˜ˆå€¼æ··åˆå¯¹æ¯”æŸå¤±æ–¹é¢ï¼Œè®ºæ–‡è®¾ç½®äº†ä¸€ä¸ªé˜ˆå€¼ï¼Œç”¨äºŽåŒºåˆ†æˆåŠŸå’Œå¤±è´¥çš„å°è¯•ï¼Œå¹¶åˆ†åˆ«è®¡ç®—å¯¹æ¯”æŸå¤±ã€‚å…·ä½“è€Œè¨€ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡å…è®¸æ¨¡åž‹åŒºåˆ†ç›¸ä¼¼çš„æˆåŠŸè½¨è¿¹å’Œä¸åŒçš„å¤±è´¥è½¨è¿¹ï¼Œä»Žè€Œæ›´å¥½åœ°å­¦ä¹ ç­–ç•¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

ExpReS-VLAåœ¨LIBEROæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­ï¼Œç©ºé—´æŽ¨ç†ä»»åŠ¡æˆåŠŸçŽ‡æå‡è‡³93.1%ï¼Œé•¿æ—¶ç¨‹ä»»åŠ¡æå‡è‡³72.3%ã€‚åœ¨çœŸå®žæœºå™¨äººå®žéªŒä¸­ï¼Œå·²è§å’Œæœªè§åœºæ™¯çš„æˆåŠŸçŽ‡å‡è¾¾åˆ°98%ï¼Œè¿œé«˜äºŽæœ´ç´ å¾®è°ƒçš„84.7%å’Œ32%ã€‚é€‚åº”è¿‡ç¨‹ä»…éœ€31ç§’ï¼Œè¯æ˜Žäº†å…¶åœ¨å®žé™…æœºå™¨äººéƒ¨ç½²ä¸­çš„é«˜æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

ExpReS-VLAé€‚ç”¨äºŽå„ç§éœ€è¦æœºå™¨äººå¿«é€Ÿé€‚åº”æ–°çŽ¯å¢ƒçš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½åˆ¶é€ ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Œé™ä½Žéƒ¨ç½²æˆæœ¬ï¼ŒåŠ é€Ÿæœºå™¨äººæŠ€æœ¯çš„å•†ä¸šåŒ–è½åœ°ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’ŒçŽ¯å¢ƒï¼Œä¾‹å¦‚å¤šæœºå™¨äººåä½œã€åŠ¨æ€çŽ¯å¢ƒç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.

