---
layout: default
title: ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval
---

# ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.06202" target="_blank" class="toolbar-btn">arXiv: 2511.06202v1</a>
    <a href="https://arxiv.org/pdf/2511.06202.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.06202v1" 
            onclick="toggleFavorite(this, '2511.06202v1', 'ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Shahram Najam Syed, Yatharth Ahuja, Arthur Jakobsson, Jeff Ichnowski

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-09

**Â§áÊ≥®**: 10 pages, 5 figures, submitted to ICRA 2026. Equal contribution by first two authors

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ExpReS-VLAÔºöÈÄöËøáÁªèÈ™åÂõûÊîæ‰∏éÊ£ÄÁ¥¢ÂÆûÁé∞VLAÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÁöÑÈ´òÊïàÁâπÂåñ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã` `ÁªèÈ™åÂõûÊîæ` `ÁªèÈ™åÊ£ÄÁ¥¢` `Êú∫Âô®‰∫∫Êìç‰Ωú` `Ê®°ÂûãÁâπÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Ê≥õÂåñËÉΩÂäõÂº∫Ôºå‰ΩÜÈöæ‰ª•È´òÊïàÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÔºåÂú®ÁâπÂÆö‰ªªÂä°‰∏äË°®Áé∞‰∏çÁ®≥ÂÆö„ÄÇ
2. ExpReS-VLAÈÄöËøáÁªèÈ™åÂõûÊîæ‰∏éÊ£ÄÁ¥¢ÔºåÂà©Áî®Á¥ßÂáëÁâπÂæÅË°®Á§∫Â≠òÂÇ®ÁªèÈ™åÔºåÂπ∂ÂºïÂÖ•ÈòàÂÄºÊ∑∑ÂêàÂØπÊØîÊçüÂ§±ÔºåÊèêÂçáÊ®°ÂûãÁâπÂåñËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåExpReS-VLAÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂùáÊòæËëóÊèêÂçáÊàêÂäüÁéáÔºå‰∏îÈÄÇÂ∫îÈÄüÂ∫¶Âø´ÔºåÂÖ∑ÊúâÂÆûÈôÖÈÉ®ÁΩ≤‰ª∑ÂÄº„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫ExpReS-VLAÔºå‰∏ÄÁßçÈÄöËøáÁªèÈ™åÂõûÊîæÂíåÊ£ÄÁ¥¢Êù•ÁâπÂåñÈ¢ÑËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁöÑÊñπÊ≥ïÔºåÊó®Âú®Èò≤Ê≠¢ÁÅæÈöæÊÄßÈÅóÂøòÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÂú®ÁâπÂÆöÈÉ®ÁΩ≤ÁéØÂ¢É‰∏ãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÂ≠òÂÇ®Êù•Ëá™ÂÜªÁªìËßÜËßâÈ™®Âπ≤ÁΩëÁªúÁöÑÁ¥ßÂáëÁâπÂæÅË°®Á§∫ÔºåËÄåÈùûÂéüÂßãÂõæÂÉè-Âä®‰ΩúÂØπÔºå‰ªéËÄåÂ∞ÜÂÜÖÂ≠ò‰ΩøÁî®ÈáèÂáèÂ∞ëÁ∫¶97%„ÄÇÂú®ÈÉ®ÁΩ≤ËøáÁ®ã‰∏≠ÔºåÈÄöËøá‰ΩôÂº¶Áõ∏‰ººÂ∫¶Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÁöÑÂéÜÂè≤ÁªèÈ™åÔºåÂπ∂Âà©Áî®Ê£ÄÁ¥¢Âà∞ÁöÑÁªèÈ™åÊåáÂØºÊ®°ÂûãÈÄÇÂ∫îÔºåÂêåÊó∂‰ºòÂÖàÂõûÊîæÊàêÂäüÁöÑËΩ®Ëøπ„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòÂºïÂÖ•‰∫ÜÈòàÂÄºÊ∑∑ÂêàÂØπÊØîÊçüÂ§±Ôºå‰ªéËÄåËÉΩÂ§ü‰ªéÊàêÂäüÂíåÂ§±Ë¥•ÁöÑÂ∞ùËØï‰∏≠ËøõË°åÂ≠¶‰π†„ÄÇÂú®LIBEROÊ®°ÊãüÂü∫ÂáÜÊµãËØï‰∏≠ÔºåExpReS-VLAÂú®Á©∫Èó¥Êé®ÁêÜ‰ªªÂä°‰∏äÁöÑÊàêÂäüÁéá‰ªé82.6%ÊèêÈ´òÂà∞93.1%ÔºåÂú®ÈïøÊó∂Á®ã‰ªªÂä°‰∏äÁöÑÊàêÂäüÁéá‰ªé61%ÊèêÈ´òÂà∞72.3%„ÄÇÂú®ÂåÖÂê´‰∫î‰∏™Êìç‰Ωú‰ªªÂä°ÁöÑÁâ©ÁêÜÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠ÔºåËØ•ÊñπÊ≥ïÂú®Â∑≤ËßÅÂíåÊú™ËßÅÂú∫ÊôØ‰∏≠ÂùáËææÂà∞98%ÁöÑÊàêÂäüÁéáÔºåËÄåÊú¥Á¥†ÂæÆË∞ÉÁöÑÊàêÂäüÁéáÂàÜÂà´‰∏∫84.7%Âíå32%„ÄÇ‰ΩøÁî®Âçï‰∏™RTX 5090 GPUÔºåÈÄöËøá12‰∏™ÊºîÁ§∫ËøõË°åÈÄÇÂ∫î‰ªÖÈúÄ31ÁßíÔºå‰ΩøÂæóËØ•ÊñπÊ≥ïÂú®ÂÆûÈôÖÊú∫Âô®‰∫∫ÈÉ®ÁΩ≤‰∏≠ÂÖ∑ÊúâÂèØË°åÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®ÁâπÂÆöÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÔºåÈöæ‰ª•È´òÊïàÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®ÂæÆË∞ÉÔºå‰ΩÜÂÆπÊòìÂèëÁîüÁÅæÈöæÊÄßÈÅóÂøòÔºåÂØºËá¥Ê®°ÂûãÂú®Â∑≤Â≠¶‰π†‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåÁõ¥Êé•Â≠òÂÇ®ÂéüÂßãÂõæÂÉè-Âä®‰ΩúÂØπËøõË°åÁªèÈ™åÂõûÊîæ‰ºöÊ∂àËÄóÂ§ßÈáèÂÜÖÂ≠òÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÊú∫Âô®‰∫∫Â∫îÁî®‰∏≠ÁöÑÂèØË°åÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁªèÈ™åÂõûÊîæÂíåÊ£ÄÁ¥¢ÔºåÁªìÂêàÁ¥ßÂáëÁöÑÁâπÂæÅË°®Á§∫ÔºåÊù•ÊåáÂØºVLAÊ®°ÂûãÂú®Êñ∞ÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫î„ÄÇÈÄöËøáÂ≠òÂÇ®ÂéÜÂè≤ÁªèÈ™åÔºåÊ®°ÂûãÂèØ‰ª•ÈÅøÂÖçÁÅæÈöæÊÄßÈÅóÂøòÔºåÂπ∂Âà©Áî®Ê£ÄÁ¥¢Âà∞ÁöÑÁõ∏ÂÖ≥ÁªèÈ™åÊù•Âä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåÂºïÂÖ•ÈòàÂÄºÊ∑∑ÂêàÂØπÊØîÊçüÂ§±Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§ü‰ªéÊàêÂäüÂíåÂ§±Ë¥•ÁöÑÂ∞ùËØï‰∏≠Â≠¶‰π†ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÂ≠¶‰π†ÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöExpReS-VLAÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÁâπÂæÅÊèêÂèñÔºö‰ΩøÁî®ÂÜªÁªìÁöÑËßÜËßâÈ™®Âπ≤ÁΩëÁªúÊèêÂèñÂõæÂÉèÁöÑÁ¥ßÂáëÁâπÂæÅË°®Á§∫„ÄÇ2) ÁªèÈ™åÂ≠òÂÇ®ÔºöÂ∞ÜÊèêÂèñÁöÑÁâπÂæÅË°®Á§∫ÂíåÂØπÂ∫îÁöÑÂä®‰ΩúÂ≠òÂÇ®Âà∞ÁªèÈ™åÊ±†‰∏≠„ÄÇ3) ÁªèÈ™åÊ£ÄÁ¥¢ÔºöÂú®ÈÉ®ÁΩ≤ËøáÁ®ã‰∏≠Ôºå‰ΩøÁî®‰ΩôÂº¶Áõ∏‰ººÂ∫¶Ê£ÄÁ¥¢‰∏éÂΩìÂâçÁä∂ÊÄÅÁõ∏ÂÖ≥ÁöÑÂéÜÂè≤ÁªèÈ™å„ÄÇ4) Á≠ñÁï•Êõ¥Êñ∞ÔºöÂà©Áî®Ê£ÄÁ¥¢Âà∞ÁöÑÁªèÈ™åÂíå‰ºòÂÖàÁªèÈ™åÂõûÊîæÊù•Êõ¥Êñ∞VLAÊ®°ÂûãÁöÑÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöExpReS-VLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞ÁÇπÂú®‰∫éÔºö1) ‰ΩøÁî®Á¥ßÂáëÁöÑÁâπÂæÅË°®Á§∫Êù•Â≠òÂÇ®ÁªèÈ™åÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÊ∂àËÄó„ÄÇ2) ÂºïÂÖ•‰∫ÜÈòàÂÄºÊ∑∑ÂêàÂØπÊØîÊçüÂ§±Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§ü‰ªéÊàêÂäüÂíåÂ§±Ë¥•ÁöÑÂ∞ùËØï‰∏≠Â≠¶‰π†„ÄÇ3) ÁªìÂêàÁªèÈ™åÂõûÊîæÂíåÊ£ÄÁ¥¢ÔºåÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑÊ®°ÂûãÁâπÂåñ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁâπÂæÅÊèêÂèñÊñπÈù¢ÔºåËÆ∫Êñá‰ΩøÁî®‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÈ™®Âπ≤ÁΩëÁªúÔºåÂπ∂Â∞ÜÂÖ∂ÂèÇÊï∞ÂÜªÁªìÔºå‰ª•ÈÅøÂÖçÁÅæÈöæÊÄßÈÅóÂøò„ÄÇÂú®ÁªèÈ™åÊ£ÄÁ¥¢ÊñπÈù¢Ôºå‰ΩøÁî®‰∫Ü‰ΩôÂº¶Áõ∏‰ººÂ∫¶Êù•Ë°°ÈáèÁä∂ÊÄÅ‰πãÈó¥ÁöÑÁõ∏‰ººÊÄß„ÄÇÂú®ÈòàÂÄºÊ∑∑ÂêàÂØπÊØîÊçüÂ§±ÊñπÈù¢ÔºåËÆ∫ÊñáËÆæÁΩÆ‰∫Ü‰∏Ä‰∏™ÈòàÂÄºÔºåÁî®‰∫éÂå∫ÂàÜÊàêÂäüÂíåÂ§±Ë¥•ÁöÑÂ∞ùËØïÔºåÂπ∂ÂàÜÂà´ËÆ°ÁÆóÂØπÊØîÊçüÂ§±„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÂÖÅËÆ∏Ê®°ÂûãÂå∫ÂàÜÁõ∏‰ººÁöÑÊàêÂäüËΩ®ËøπÂíå‰∏çÂêåÁöÑÂ§±Ë¥•ËΩ®ËøπÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Â≠¶‰π†Á≠ñÁï•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ExpReS-VLAÂú®LIBEROÊ®°ÊãüÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÁ©∫Èó¥Êé®ÁêÜ‰ªªÂä°ÊàêÂäüÁéáÊèêÂçáËá≥93.1%ÔºåÈïøÊó∂Á®ã‰ªªÂä°ÊèêÂçáËá≥72.3%„ÄÇÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠ÔºåÂ∑≤ËßÅÂíåÊú™ËßÅÂú∫ÊôØÁöÑÊàêÂäüÁéáÂùáËææÂà∞98%ÔºåËøúÈ´ò‰∫éÊú¥Á¥†ÂæÆË∞ÉÁöÑ84.7%Âíå32%„ÄÇÈÄÇÂ∫îËøáÁ®ã‰ªÖÈúÄ31ÁßíÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÊú∫Âô®‰∫∫ÈÉ®ÁΩ≤‰∏≠ÁöÑÈ´òÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ExpReS-VLAÈÄÇÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÊú∫Âô®‰∫∫Âø´ÈÄüÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÊô∫ËÉΩÂà∂ÈÄ†„ÄÅÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂåªÁñóËæÖÂä©Êú∫Âô®‰∫∫Á≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊòæËëóÊèêÂçáÊú∫Âô®‰∫∫Âú®ÁâπÂÆö‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄßÔºåÈôç‰ΩéÈÉ®ÁΩ≤ÊàêÊú¨ÔºåÂä†ÈÄüÊú∫Âô®‰∫∫ÊäÄÊúØÁöÑÂïÜ‰∏öÂåñËêΩÂú∞„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑ‰ªªÂä°ÂíåÁéØÂ¢ÉÔºå‰æãÂ¶ÇÂ§öÊú∫Âô®‰∫∫Âçè‰Ωú„ÄÅÂä®ÊÄÅÁéØÂ¢ÉÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.

