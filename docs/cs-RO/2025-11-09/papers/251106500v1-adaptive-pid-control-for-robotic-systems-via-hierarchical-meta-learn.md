---
layout: default
title: Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation
---

# Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.06500" target="_blank" class="toolbar-btn">arXiv: 2511.06500v1</a>
    <a href="https://arxiv.org/pdf/2511.06500.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.06500v1" 
            onclick="toggleFavorite(this, '2511.06500v1', 'Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: JiaHao Wu, ShengWen Yu

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-09

**Â§áÊ≥®**: 21 pages,12 tables, 6 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂ±ÇÁ∫ßÂÖÉÂ≠¶‰π†‰∏éÂº∫ÂåñÂ≠¶‰π†ÁöÑËá™ÈÄÇÂ∫îPIDÊéßÂà∂Ê°ÜÊû∂ÔºåÊèêÂçáÊú∫Âô®‰∫∫Á≥ªÁªüÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `PIDÊéßÂà∂` `ÂÖÉÂ≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `Êï∞ÊçÆÂ¢ûÂº∫` `Êú∫Âô®‰∫∫ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüPIDÊéßÂà∂Âô®Âú®Êú∫Âô®‰∫∫È¢ÜÂüüÂ∫îÁî®ÂπøÊ≥õÔºå‰ΩÜÈíàÂØπ‰∏çÂêåÊú∫Âô®‰∫∫Âπ≥Âè∞ÊâãÂä®Ë∞ÉÂèÇËÄóÊó∂‰∏î‰æùËµñ‰∏ìÂÆ∂ÁªèÈ™å„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçÂ±ÇÁ∫ßÊéßÂà∂Ê°ÜÊû∂ÔºåÂà©Áî®ÂÖÉÂ≠¶‰π†ÂàùÂßãÂåñPIDÂèÇÊï∞ÔºåÂÜçÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËøõË°åÂú®Á∫øËá™ÈÄÇÂ∫îË∞ÉÊï¥„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Franka PandaÊú∫Ê¢∞ËáÇ‰∏äÊÄßËÉΩÊèêÂçáÊòæËëóÔºåÂπ∂Êè≠Á§∫‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÊïàÊûúÂèóÂÖÉÂ≠¶‰π†Âü∫Á∫øË¥®ÈáèÂΩ±ÂìçÁöÑ‚Äú‰ºòÂåñÂ§©Ëä±ÊùøÊïàÂ∫î‚Äù„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ±ÇÁ∫ßÊéßÂà∂Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÁî®‰∫éPIDÂèÇÊï∞ÂàùÂßãÂåñÁöÑÂÖÉÂ≠¶‰π†ÂíåÁî®‰∫éÂú®Á∫øËá™ÈÄÇÂ∫îÁöÑÂº∫ÂåñÂ≠¶‰π†(RL)„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ê†∑Êú¨ÊïàÁéáÈóÆÈ¢òÔºåÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁâ©ÁêÜÁöÑÊï∞ÊçÆÂ¢ûÂº∫Á≠ñÁï•ÔºåÈÄöËøáÁ≥ªÁªüÂú∞Êâ∞Âä®Áâ©ÁêÜÂèÇÊï∞Êù•ÁîüÊàêËôöÊãüÊú∫Âô®‰∫∫ÈÖçÁΩÆÔºå‰ªéËÄåÂú®ÊúâÈôêÁöÑÁúüÂÆûÊú∫Âô®‰∫∫Êï∞ÊçÆ‰∏ãÂÆûÁé∞ÊúâÊïàÁöÑÂÖÉÂ≠¶‰π†„ÄÇËØ•ÊñπÊ≥ïÂú®Franka PandaÊú∫Ê¢∞ËáÇ(9Ëá™Áî±Â∫¶)ÂíåLaikagoÂõõË∂≥Êú∫Âô®‰∫∫(12Ëá™Áî±Â∫¶)‰∏§‰∏™ÂºÇÊûÑÂπ≥Âè∞‰∏äËøõË°å‰∫ÜËØÑ‰º∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Franka Panda‰∏äÂÆûÁé∞‰∫Ü16.6%ÁöÑÂπ≥ÂùáÊîπËøõ(6.26¬∞ MAE)ÔºåÂú®È´òË¥üËΩΩÂÖ≥ËäÇ(J2)‰∏äËé∑Âæó‰∫ÜÊòæËëóÁöÑÂ¢ûÁõä(‰ªé12.36¬∞ÊîπËøõÂà∞2.42¬∞ÔºåÊèêÂçá80.4%)„ÄÇÈáçË¶ÅÁöÑÊòØÔºåËøôÈ°πÂ∑•‰ΩúÂèëÁé∞‰∫Ü‚Äú‰ºòÂåñÂ§©Ëä±ÊùøÊïàÂ∫î‚ÄùÔºöÂΩìÂÖÉÂ≠¶‰π†Ë°®Áé∞Âá∫Â±ÄÈÉ®È´òËØØÂ∑ÆÂÖ≥ËäÇÊó∂ÔºåRLÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊîπËøõÔºå‰ΩÜÂΩìÂü∫Á∫øÊÄßËÉΩÂùáÂåÄËâØÂ•ΩÊó∂ÔºåRLÊ≤°ÊúâÊèê‰æõ‰ªª‰ΩïÂ•ΩÂ§Ñ(0.0%)ÔºåÊ≠£Â¶ÇÂú®Laikago‰∏≠ËßÇÂØüÂà∞ÁöÑÈÇ£Ê†∑„ÄÇËØ•ÊñπÊ≥ïÂú®Êâ∞Âä®‰∏ãË°®Áé∞Âá∫È≤ÅÊ£íÁöÑÊÄßËÉΩ(ÂèÇÊï∞‰∏çÁ°ÆÂÆöÊÄßÔºö+19.2%ÔºåÊó†Êâ∞Âä®Ôºö+16.6%ÔºåÂπ≥ÂùáÔºö+10.0%)Ôºå‰∏î‰ªÖÈúÄ10ÂàÜÈíüÁöÑËÆ≠ÁªÉÊó∂Èó¥„ÄÇË∑®100‰∏™ÈöèÊú∫ÂàùÂßãÂåñÁöÑÂ§öÁßçÂ≠êÂàÜÊûêËØÅÂÆû‰∫ÜÁ®≥ÂÆöÁöÑÊÄßËÉΩ(Âπ≥Âùá4.81+/-1.64%)„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåRLÁöÑÊúâÊïàÊÄßÈ´òÂ∫¶‰æùËµñ‰∫éÂÖÉÂ≠¶‰π†Âü∫Á∫øÁöÑË¥®ÈáèÂíåËØØÂ∑ÆÂàÜÂ∏ÉÔºå‰∏∫Â±ÇÁ∫ßÊéßÂà∂Á≥ªÁªüÁöÑËÆæËÆ°Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÊåáÂØº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Á≥ªÁªü‰∏≠PIDÊéßÂà∂Âô®ÂèÇÊï∞Êï¥ÂÆöÁöÑÈöæÈ¢ò„ÄÇ‰º†ÁªüÁöÑÊâãÂä®Ë∞ÉÂèÇÊñπÊ≥ïËÄóÊó∂Ë¥πÂäõÔºå‰∏îÈúÄË¶Å‰∏∞ÂØåÁöÑÈ¢ÜÂüüÁü•ËØÜ„ÄÇÁé∞ÊúâÁöÑËá™Âä®ÂåñË∞ÉÂèÇÊñπÊ≥ïÔºåÂ¶ÇÂº∫ÂåñÂ≠¶‰π†ÔºåÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑÊ†∑Êú¨Êï∞ÊçÆÔºåÈöæ‰ª•Âú®ÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÁõ¥Êé•Â∫îÁî®„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂú®ÊúâÈôêÁöÑÁúüÂÆûÊú∫Âô®‰∫∫Êï∞ÊçÆ‰∏ãÔºåÂø´ÈÄüÊúâÊïàÂú∞Êï¥ÂÆöPIDÂèÇÊï∞ÔºåÊòØÊú¨ÊñáË¶ÅËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÁªìÂêàÂÖÉÂ≠¶‰π†ÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑ‰ºòÂäøÔºåÊûÑÂª∫‰∏Ä‰∏™Â±ÇÁ∫ßÊéßÂà∂Ê°ÜÊû∂„ÄÇÈ¶ñÂÖàÔºåÂà©Áî®ÂÖÉÂ≠¶‰π†‰ªéÂ∞ëÈáèÊï∞ÊçÆ‰∏≠Â≠¶‰π†PIDÂèÇÊï∞ÁöÑÂàùÂßãÂåñÁ≠ñÁï•Ôºå‰ΩøÂæóPIDÊéßÂà∂Âô®ËÉΩÂ§üÂø´ÈÄüÈÄÇÂ∫î‰∏çÂêåÁöÑÊú∫Âô®‰∫∫ÈÖçÁΩÆ„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÂØπPIDÂèÇÊï∞ËøõË°åÂú®Á∫øËá™ÈÄÇÂ∫îË∞ÉÊï¥ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÊéßÂà∂ÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜËß£ÂÜ≥Ê†∑Êú¨ÊïàÁéáÈóÆÈ¢òÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁâ©ÁêÜÁöÑÊï∞ÊçÆÂ¢ûÂº∫Á≠ñÁï•ÔºåÈÄöËøáÊ®°Êãü‰∏çÂêåÁöÑÊú∫Âô®‰∫∫Áâ©ÁêÜÂèÇÊï∞ÔºåÁîüÊàêÂ§ßÈáèÁöÑËôöÊãüÊï∞ÊçÆÔºå‰ªéËÄåÂä†ÈÄüÂÖÉÂ≠¶‰π†ÁöÑËÆ≠ÁªÉ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Â±ÇÁ∫ßÊéßÂà∂Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) **Áâ©ÁêÜÂèÇÊï∞Êâ∞Âä®Ê®°Âùó**ÔºöÈÄöËøáÈöèÊú∫Êâ∞Âä®Êú∫Âô®‰∫∫ÁöÑÁâ©ÁêÜÂèÇÊï∞ÔºàÂ¶ÇË¥®Èáè„ÄÅÊë©Êì¶Á≥ªÊï∞Á≠âÔºâÔºåÁîüÊàê‰∏çÂêåÁöÑËôöÊãüÊú∫Âô®‰∫∫ÈÖçÁΩÆ„ÄÇ2) **ÂÖÉÂ≠¶‰π†Ê®°Âùó**ÔºöÂà©Áî®ÁîüÊàêÁöÑÂ§ßÈáèËôöÊãüÊï∞ÊçÆÔºåËÆ≠ÁªÉ‰∏Ä‰∏™ÂÖÉÂ≠¶‰π†Ê®°ÂûãÔºåËØ•Ê®°ÂûãËÉΩÂ§üÊ†πÊçÆÊú∫Âô®‰∫∫ÈÖçÁΩÆÔºåÂø´ÈÄüÂàùÂßãÂåñPIDÂèÇÊï∞„ÄÇ3) **Âº∫ÂåñÂ≠¶‰π†Ê®°Âùó**ÔºöÂà©Áî®ÁúüÂÆûÊú∫Âô®‰∫∫Êï∞ÊçÆÔºåËÆ≠ÁªÉ‰∏Ä‰∏™Âº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰ΩìÔºåËØ•Êô∫ËÉΩ‰ΩìËÉΩÂ§üÊ†πÊçÆÂΩìÂâçÁä∂ÊÄÅÔºåÂØπPIDÂèÇÊï∞ËøõË°åÂú®Á∫øËá™ÈÄÇÂ∫îË∞ÉÊï¥„ÄÇ4) **PIDÊéßÂà∂Ê®°Âùó**ÔºöÂà©Áî®Êï¥ÂÆöÂêéÁöÑPIDÂèÇÊï∞ÔºåÊéßÂà∂Êú∫Âô®‰∫∫ÊâßË°å‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁâ©ÁêÜÁöÑÊï∞ÊçÆÂ¢ûÂº∫Á≠ñÁï•ÔºåÊúâÊïàÂú∞Ëß£ÂÜ≥‰∫ÜÊ†∑Êú¨ÊïàÁéáÈóÆÈ¢ò„ÄÇ2) ÁªìÂêàÂÖÉÂ≠¶‰π†ÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â±ÇÁ∫ßÊéßÂà∂Ê°ÜÊû∂ÔºåÂÖÖÂàÜÂà©Áî®‰∫Ü‰∏§ÁßçÊñπÊ≥ïÁöÑ‰ºòÂäø„ÄÇ3) ÂèëÁé∞‰∫Ü‚Äú‰ºòÂåñÂ§©Ëä±ÊùøÊïàÂ∫î‚ÄùÔºåÊè≠Á§∫‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÊïàÊûúÂèóÂÖÉÂ≠¶‰π†Âü∫Á∫øË¥®ÈáèÁöÑÂΩ±ÂìçÔºå‰∏∫Â±ÇÁ∫ßÊéßÂà∂Á≥ªÁªüÁöÑËÆæËÆ°Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÊåáÂØº„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÖÉÂ≠¶‰π†Ê®°Âùó‰∏≠Ôºå‰ΩøÁî®‰∫ÜModel-Agnostic Meta-Learning (MAML)ÁÆóÊ≥ïÔºåÂ≠¶‰π†PIDÂèÇÊï∞ÁöÑÂàùÂßãÂåñÁ≠ñÁï•„ÄÇÂú®Âº∫ÂåñÂ≠¶‰π†Ê®°Âùó‰∏≠Ôºå‰ΩøÁî®‰∫ÜProximal Policy Optimization (PPO)ÁÆóÊ≥ïÔºåÂØπPIDÂèÇÊï∞ËøõË°åÂú®Á∫øËá™ÈÄÇÂ∫îË∞ÉÊï¥„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ËÄÉËôë‰∫Ü‰ΩçÁΩÆËØØÂ∑ÆÂíåÊéßÂà∂ÂäõÁü©Ôºå‰ª•‰øùËØÅÊéßÂà∂Á≤æÂ∫¶ÂíåÁ®≥ÂÆöÊÄß„ÄÇÁΩëÁªúÁªìÊûÑÈááÁî®‰∫ÜÂ§öÂ±ÇÊÑüÁü•Êú∫ÔºàMLPÔºâÔºåËæìÂÖ•‰∏∫Êú∫Âô®‰∫∫Áä∂ÊÄÅÔºåËæìÂá∫‰∏∫PIDÂèÇÊï∞ÁöÑË∞ÉÊï¥Èáè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Franka PandaÊú∫Ê¢∞ËáÇ‰∏äÂÆûÁé∞‰∫Ü16.6%ÁöÑÂπ≥ÂùáÊîπËøõ(6.26¬∞ MAE)ÔºåÂú®È´òË¥üËΩΩÂÖ≥ËäÇ(J2)‰∏äËé∑Âæó‰∫ÜÊòæËëóÁöÑÂ¢ûÁõä(‰ªé12.36¬∞ÊîπËøõÂà∞2.42¬∞ÔºåÊèêÂçá80.4%)„ÄÇÂú®Êâ∞Âä®‰∏ãË°®Áé∞Âá∫È≤ÅÊ£íÁöÑÊÄßËÉΩ(ÂèÇÊï∞‰∏çÁ°ÆÂÆöÊÄßÔºö+19.2%ÔºåÊó†Êâ∞Âä®Ôºö+16.6%ÔºåÂπ≥ÂùáÔºö+10.0%)Ôºå‰∏î‰ªÖÈúÄ10ÂàÜÈíüÁöÑËÆ≠ÁªÉÊó∂Èó¥„ÄÇË∑®100‰∏™ÈöèÊú∫ÂàùÂßãÂåñÁöÑÂ§öÁßçÂ≠êÂàÜÊûêËØÅÂÆû‰∫ÜÁ®≥ÂÆöÁöÑÊÄßËÉΩ(Âπ≥Âùá4.81+/-1.64%)„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂ∑•‰∏öÊú∫Âô®‰∫∫„ÄÅÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂõõË∂≥Êú∫Âô®‰∫∫Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáËØ•ÊñπÊ≥ïÔºåÂèØ‰ª•ÊòæËëóÈôç‰ΩéPIDÊéßÂà∂Âô®ÂèÇÊï∞Êï¥ÂÆöÁöÑÊó∂Èó¥ÂíåÊàêÊú¨ÔºåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÊéßÂà∂ÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊé®ÂπøÂà∞Êõ¥Â§çÊùÇÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÂíåÊéßÂà∂‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÂ§öÊú∫Âô®‰∫∫ÂçèÂêåÊéßÂà∂„ÄÅËá™‰∏ªÂØºËà™Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Proportional-Integral-Derivative (PID) controllers remain the predominant choice in industrial robotics due to their simplicity and reliability. However, manual tuning of PID parameters for diverse robotic platforms is time-consuming and requires extensive domain expertise. This paper presents a novel hierarchical control framework that combines meta-learning for PID initialization and reinforcement learning (RL) for online adaptation. To address the sample efficiency challenge, a \textit{physics-based data augmentation} strategy is introduced that generates virtual robot configurations by systematically perturbing physical parameters, enabling effective meta-learning with limited real robot data. The proposed approach is evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the proposed method achieves 16.6\% average improvement on Franka Panda (6.26¬∞ MAE), with exceptional gains in high-load joints (J2: 80.4\% improvement from 12.36¬∞ to 2.42¬∞). Critically, this work discovers the \textit{optimization ceiling effect}: RL achieves dramatic improvements when meta-learning exhibits localized high-error joints, but provides no benefit (0.0\%) when baseline performance is uniformly strong, as observed in Laikago. The method demonstrates robust performance under disturbances (parameter uncertainty: +19.2\%, no disturbance: +16.6\%, average: +10.0\%) with only 10 minutes of training time. Multi-seed analysis across 100 random initializations confirms stable performance (4.81+/-1.64\% average). These results establish that RL effectiveness is highly dependent on meta-learning baseline quality and error distribution, providing important design guidance for hierarchical control systems.

