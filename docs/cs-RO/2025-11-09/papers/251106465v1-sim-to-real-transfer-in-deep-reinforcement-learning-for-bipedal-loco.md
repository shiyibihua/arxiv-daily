---
layout: default
title: Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion
---

# Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.06465" target="_blank" class="toolbar-btn">arXiv: 2511.06465v1</a>
    <a href="https://arxiv.org/pdf/2511.06465.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.06465v1" 
            onclick="toggleFavorite(this, '2511.06465v1', 'Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Lingfan Bao, Tianhu Peng, Chengxu Zhou

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-09

**Â§áÊ≥®**: Sim-to-real for bipedal locomotion chapter

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÈíàÂØπÂèåË∂≥Êú∫Âô®‰∫∫Ê≠•ÊÄÅÔºåÊèêÂá∫Âü∫‰∫éÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÁöÑSim-to-RealËøÅÁßªÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Sim-to-RealËøÅÁßª` `Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†` `ÂèåË∂≥Êú∫Âô®‰∫∫` `Ê≠•ÊÄÅÊéßÂà∂` `È≤ÅÊ£íÊÄßËÆ≠ÁªÉ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÂèåË∂≥Êú∫Âô®‰∫∫Ê≠•ÊÄÅÊéßÂà∂‰∏≠Èù¢‰∏¥‰ªøÁúü‰∏éÁé∞ÂÆûÁéØÂ¢ÉÂ∑ÆÂºÇÂ∏¶Êù•ÁöÑÊåëÊàòÔºåÂç≥‚Äú‰ªøÁúüËØÖÂíí‚Äù„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫‰∏ÄÁßçÁªìÂêàÊ®°Âûã‰ºòÂåñÂíåÁ≠ñÁï•Âº∫ÂåñÁöÑSim-to-RealËøÅÁßªÊñπÊ≥ïÔºåÊèêÈ´òÁ≠ñÁï•Âú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ
3. ËØ•ÊñπÊ≥ïÈÄöËøáÊèêÈ´ò‰ªøÁúüÂô®Á≤æÂ∫¶ÂíåËÆ≠ÁªÉÁ≠ñÁï•ÂØπÊ®°ÂûãËØØÂ∑ÆÁöÑÂÆπÂøçÂ∫¶ÔºåÂÆûÁé∞ÊúâÊïàÁöÑSim-to-RealËøÅÁßª„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Á´†Êé¢ËÆ®‰∫ÜÂèåË∂≥Êú∫Âô®‰∫∫Ê≠•ÊÄÅ‰∏≠Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºàDRLÔºâÁöÑ‰ªøÁúüÂà∞Áé∞ÂÆûÔºàsim-to-realÔºâËøÅÁßªËøô‰∏ÄÂÖ≥ÈîÆÊåëÊàò„ÄÇÂú®Â∞ÜÈóÆÈ¢òÁΩÆ‰∫éÂêÑÁßçÊéßÂà∂Êû∂ÊûÑÁöÑËÉåÊôØ‰∏ãÂêéÔºåÊàë‰ª¨ÈÄöËøáÂàÜÊûêsim-to-realÂ∑ÆË∑ùÁöÑ‰∏ªË¶ÅÊù•Ê∫êÊù•ÂâñÊûê‚Äú‰ªøÁúüËØÖÂíí‚ÄùÔºåËøô‰∫õÊù•Ê∫êÂåÖÊã¨ÔºöÊú∫Âô®‰∫∫Âä®ÂäõÂ≠¶„ÄÅÊé•Ëß¶Âª∫Ê®°„ÄÅÁä∂ÊÄÅ‰º∞ËÆ°ÂíåÊï∞ÂÄºÊ±ÇËß£Âô®„ÄÇÂú®Ê≠§ËØäÊñ≠ÁöÑÂü∫Á°Ä‰∏äÔºåÊàë‰ª¨Âõ¥Áªï‰∏§Áßç‰∫íË°•ÁöÑÁêÜÂøµÊûÑÂª∫Ëß£ÂÜ≥ÊñπÊ°à„ÄÇÁ¨¨‰∏ÄÁßçÊòØÈÄöËøá‰ª•Ê®°Âûã‰∏∫‰∏≠ÂøÉÁöÑÁ≠ñÁï•Êù•Áº©Â∞èÂ∑ÆË∑ùÔºåËøô‰∫õÁ≠ñÁï•Á≥ªÁªüÂú∞ÊèêÈ´ò‰∫ÜÊ®°ÊãüÂô®ÁöÑÁâ©ÁêÜ‰øùÁúüÂ∫¶„ÄÇÁ¨¨‰∫åÁßçÊòØÂº∫ÂåñÁ≠ñÁï•ÔºåËøôÊòØ‰∏ÄÁßç‰∫íË°•ÁöÑÊñπÊ≥ïÔºåÂÆÉ‰ΩøÁî®‰ªøÁúü‰∏≠ÁöÑÈ≤ÅÊ£íÊÄßËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤ÂêéÁöÑÈÄÇÂ∫îÊù•‰ΩøÁ≠ñÁï•Êú¨Ë∫´ÂØπÊ®°Âûã‰∏çÂáÜÁ°ÆÂÖ∑ÊúâÂºπÊÄß„ÄÇÊú¨Á´†ÊúÄÂêéÂ∞ÜËøô‰∫õÁêÜÂøµÁªºÂêà‰∏∫‰∏Ä‰∏™ÊàòÁï•Ê°ÜÊû∂Ôºå‰∏∫ÂºÄÂèëÂíåËØÑ‰º∞Á®≥ÂÅ•ÁöÑsim-to-realËß£ÂÜ≥ÊñπÊ°àÊèê‰æõÊ∏ÖÊô∞ÁöÑË∑ØÁ∫øÂõæ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂèåË∂≥Êú∫Âô®‰∫∫Ê≠•ÊÄÅÊéßÂà∂‰∏≠ÔºåÁî±‰∫é‰ªøÁúüÁéØÂ¢É‰∏éÁúüÂÆûÁéØÂ¢ÉÂ≠òÂú®Â∑ÆÂºÇÔºåÂØºËá¥Âú®‰ªøÁúüÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÁöÑÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Èöæ‰ª•Áõ¥Êé•ËøÅÁßªÂà∞ÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÈöæ‰ª•ÂÖãÊúçÊú∫Âô®‰∫∫Âä®ÂäõÂ≠¶„ÄÅÊé•Ëß¶Âª∫Ê®°„ÄÅÁä∂ÊÄÅ‰º∞ËÆ°ÂíåÊï∞ÂÄºÊ±ÇËß£Âô®Á≠âÊñπÈù¢ÁöÑÂ∑ÆÂºÇÔºåÂØºËá¥Á≠ñÁï•ÊÄßËÉΩ‰∏ãÈôçÁîöËá≥Â§±Êïà„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂèåÁÆ°ÈΩê‰∏ãÔºå‰∏ÄÊñπÈù¢Â∞ΩÂèØËÉΩÁº©Â∞è‰ªøÁúüÁéØÂ¢É‰∏éÁúüÂÆûÁéØÂ¢ÉÁöÑÂ∑ÆË∑ùÔºåÂè¶‰∏ÄÊñπÈù¢Â¢ûÂº∫Á≠ñÁï•Êú¨Ë∫´ÁöÑÈ≤ÅÊ£íÊÄßÔºå‰ΩøÂÖ∂ËÉΩÂ§üÈÄÇÂ∫îÁéØÂ¢ÉÂ∑ÆÂºÇ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøá‰ºòÂåñ‰ªøÁúüÊ®°ÂûãÊèêÈ´òÂÖ∂Áâ©ÁêÜ‰øùÁúüÂ∫¶ÔºåÂêåÊó∂Âú®‰ªøÁúüÁéØÂ¢É‰∏≠ËøõË°åÈ≤ÅÊ£íÊÄßËÆ≠ÁªÉÔºå‰ΩøÁ≠ñÁï•ÂØπÊ®°ÂûãËØØÂ∑ÆÂÖ∑Êúâ‰∏ÄÂÆöÁöÑÂÆπÂøçËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËøòËÄÉËôëÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ËøõË°åÈÉ®ÁΩ≤ÂêéÁöÑÈÄÇÂ∫îÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÁ≠ñÁï•ÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöÊ®°Âûã‰ºòÂåñÂíåÁ≠ñÁï•Âº∫Âåñ„ÄÇÊ®°Âûã‰ºòÂåñÈÉ®ÂàÜÊó®Âú®ÊèêÈ´ò‰ªøÁúüÂô®ÁöÑÁâ©ÁêÜ‰øùÁúüÂ∫¶Ôºå‰æãÂ¶ÇÈÄöËøáÁ≥ªÁªüËæ®ËØÜÊñπÊ≥ïÁ≤æÁ°ÆÂª∫Ê®°Êú∫Âô®‰∫∫Âä®ÂäõÂ≠¶ÂèÇÊï∞ÔºåÊîπËøõÊé•Ëß¶Ê®°Âûã‰ª•Êõ¥ÁúüÂÆûÂú∞Ê®°ÊãüÂú∞Èù¢Âèç‰ΩúÁî®ÂäõÁ≠â„ÄÇÁ≠ñÁï•Âº∫ÂåñÈÉ®ÂàÜÂàôÂåÖÊã¨Âú®‰ªøÁúüÁéØÂ¢É‰∏≠ËøõË°åÈ≤ÅÊ£íÊÄßËÆ≠ÁªÉÔºå‰æãÂ¶ÇÈÄöËøáÊ∑ªÂä†Âô™Â£∞„ÄÅÊîπÂèòÁéØÂ¢ÉÂèÇÊï∞Á≠âÊñπÂºèÔºå‰ΩøÁ≠ñÁï•ÂØπÁéØÂ¢ÉÂèòÂåñÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËøòÂèØ‰ª•Âú®ÁúüÂÆûÁéØÂ¢É‰∏≠ËøõË°åÂæÆË∞ÉÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÁ≠ñÁï•ÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÊ®°Âûã‰ºòÂåñÂíåÁ≠ñÁï•Âº∫ÂåñÁõ∏ÁªìÂêàÔºåÂΩ¢Êàê‰∏Ä‰∏™ÂÆåÊï¥ÁöÑSim-to-RealËøÅÁßªÊ°ÜÊû∂„ÄÇ‰∏é‰ª•ÂæÄ‰æßÈáç‰∫éÂçï‰∏ÄÊñπÈù¢ÁöÑÁ†îÁ©∂‰∏çÂêåÔºåËØ•ÊñπÊ≥ïÂêåÊó∂ËÄÉËôë‰∫Ü‰ªøÁúüÁéØÂ¢ÉÁöÑÂáÜÁ°ÆÊÄßÂíåÁ≠ñÁï•ÁöÑÈ≤ÅÊ£íÊÄßÔºå‰ªéËÄåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Ëß£ÂÜ≥Sim-to-RealËøÅÁßªÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÊàòÁï•Ê°ÜÊû∂Ôºå‰∏∫ÂºÄÂèëÂíåËØÑ‰º∞Á®≥ÂÅ•ÁöÑsim-to-realËß£ÂÜ≥ÊñπÊ°àÊèê‰æõÊ∏ÖÊô∞ÁöÑË∑ØÁ∫øÂõæ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠Ê∂âÂèäÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö(1) ‰ªøÁúüÊ®°ÂûãÁöÑÂèÇÊï∞Ëæ®ËØÜÊñπÊ≥ïÔºåÁî®‰∫éÊèêÈ´ò‰ªøÁúüÂô®ÁöÑÁâ©ÁêÜ‰øùÁúüÂ∫¶Ôºõ(2) È≤ÅÊ£íÊÄßËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰æãÂ¶Ç‰ΩøÁî®ÂØπÊäóËÆ≠ÁªÉÊàñÂüüÈöèÊú∫ÂåñÁ≠âÊñπÊ≥ïÔºå‰ΩøÁ≠ñÁï•ÂØπÁéØÂ¢ÉÂèòÂåñÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÈÄÇÂ∫îËÉΩÂäõÔºõ(3) ÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÂæÆË∞ÉÁ≠ñÁï•Ôºå‰æãÂ¶Ç‰ΩøÁî®Âú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÊàñËá™ÈÄÇÂ∫îÊéßÂà∂Á≠âÊñπÊ≥ïÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÁ≠ñÁï•ÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆ„ÄÅÊçüÂ§±ÂáΩÊï∞„ÄÅÁΩëÁªúÁªìÊûÑÁ≠âÊäÄÊúØÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠Êú™ËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Áî±‰∫éËÆ∫Êñá‰∏∫ÁªºËø∞ÊÄßË¥®ÔºåÂπ∂Êú™Êèê‰æõÂÖ∑‰ΩìÁöÑÂÆûÈ™åÁªìÊûú„ÄÇÂÖ∂‰∫ÆÁÇπÂú®‰∫éÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂÆåÊï¥ÁöÑSim-to-RealËøÅÁßªÊ°ÜÊû∂ÔºåÂπ∂ÁªôÂá∫‰∫ÜÊ∏ÖÊô∞ÁöÑË∑ØÁ∫øÂõæÔºå‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫ÜÊåáÂØº„ÄÇÊú™Êù•ÁöÑÁ†îÁ©∂ÂèØ‰ª•Âü∫‰∫éËØ•Ê°ÜÊû∂ÔºåÊé¢Á¥¢Êõ¥ÊúâÊïàÁöÑÊ®°Âûã‰ºòÂåñÂíåÁ≠ñÁï•Âº∫ÂåñÊñπÊ≥ïÔºåÂπ∂Âú®ÁúüÂÆûÊú∫Âô®‰∫∫Âπ≥Âè∞‰∏äËøõË°åÈ™åËØÅ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßçÂèåË∂≥Êú∫Âô®‰∫∫Â∫îÁî®Âú∫ÊôØÔºå‰æãÂ¶ÇÁÅæÈöæÊïëÊè¥„ÄÅÁâ©ÊµÅËøêËæì„ÄÅÂÆ∂Â∫≠ÊúçÂä°Á≠â„ÄÇÈÄöËøáÈôç‰ΩéSim-to-RealËøÅÁßªÁöÑÈöæÂ∫¶ÔºåÂèØ‰ª•Âä†ÈÄüÂèåË∂≥Êú∫Âô®‰∫∫ÁöÑÁ†îÂèëÂíåÈÉ®ÁΩ≤Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÂ§öÂèòÁöÑÁúüÂÆûÁéØÂ¢ÉÔºå‰ªéËÄåÂèëÊå•Êõ¥Â§ßÁöÑ‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation'' by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator's physical fidelity. The second is to harden the policy, a complementary approach that uses in-simulation robustness training and post-deployment adaptation to make the policy inherently resilient to model inaccuracies. The chapter concludes by synthesizing these philosophies into a strategic framework, providing a clear roadmap for developing and evaluating robust sim-to-real solutions.

