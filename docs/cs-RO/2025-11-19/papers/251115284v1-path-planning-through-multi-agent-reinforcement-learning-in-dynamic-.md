---
layout: default
title: Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments
---

# Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.15284" target="_blank" class="toolbar-btn">arXiv: 2511.15284v1</a>
    <a href="https://arxiv.org/pdf/2511.15284.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.15284v1" 
            onclick="toggleFavorite(this, '2511.15284v1', 'Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jonas De Maeyer, Hossein Yarahmadi, Moharram Challenger

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-19

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÁöÑÂä®ÊÄÅÁéØÂ¢ÉË∑ØÂæÑËßÑÂàíÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ë∑ØÂæÑËßÑÂàí` `Âº∫ÂåñÂ≠¶‰π†` `Â§öÊô∫ËÉΩ‰Ωì` `Âä®ÊÄÅÁéØÂ¢É` `ËÅîÈÇ¶Â≠¶‰π†` `Êú∫Âô®‰∫∫ÂØºËà™` `Êô∫ËÉΩ‰∫§ÈÄö`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂä®ÊÄÅÁéØÂ¢ÉË∑ØÂæÑËßÑÂàíÊñπÊ≥ïÈöæ‰ª•Â∫îÂØπÁéØÂ¢ÉÁöÑÂ§çÊùÇÊÄßÂíå‰∏çÁ°ÆÂÆöÊÄßÔºåÂÖ®Â±ÄËßÑÂàíÂô®ËÆ°ÁÆóÈáèÂ§ßÔºåÈöæ‰ª•Êâ©Â±ï„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçÂå∫ÂüüÊÑüÁü•ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂ∞ÜÁéØÂ¢ÉÂàÜËß£‰∏∫Â±ÄÈÉ®Âå∫ÂüüÔºåÂà©Áî®ÂàÜÂ∏ÉÂºèÊô∫ËÉΩ‰ΩìËøõË°åÂ±ÄÈÉ®ÈÄÇÂ∫î„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËÅîÈÇ¶Â≠¶‰π†Âèò‰Ωì‰ºò‰∫éÂçïÊô∫ËÉΩ‰ΩìÔºåÊÄßËÉΩÊé•ËøëA* OracleÔºåÂπ∂ÂÖ∑ÊúâÊõ¥Áü≠ÁöÑÈÄÇÂ∫îÊó∂Èó¥ÂíåËâØÂ•ΩÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®Êô∫ËÉΩ‰∫§ÈÄöÂíåÊú∫Âô®‰∫∫È¢ÜÂüüÔºåÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑË∑ØÂæÑËßÑÂàíÊòØ‰∏Ä‰∏™Ê†πÊú¨ÊÄßÁöÑÊåëÊàòÔºåÂÖ∂‰∏≠ÈöúÁ¢çÁâ©ÂíåÊù°‰ª∂ÈöèÊó∂Èó¥ÂèòÂåñÔºåÂºïÂÖ•‰∏çÁ°ÆÂÆöÊÄßÂπ∂ÈúÄË¶ÅÊåÅÁª≠ÈÄÇÂ∫î„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÂÅáËÆæÂÆåÂÖ®ÁöÑÁéØÂ¢É‰∏çÂèØÈ¢ÑÊµãÊÄßÊàñ‰æùËµñ‰∫éÂÖ®Â±ÄËßÑÂàíÂô®ÔºåËøô‰∫õÂÅáËÆæÈôêÂà∂‰∫ÜÂú®ÂÆûÈôÖÁéØÂ¢É‰∏≠ÁöÑÂèØÊâ©Â±ïÊÄßÂíåÂÆûÈôÖÈÉ®ÁΩ≤„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑ„ÄÅÂå∫ÂüüÊÑüÁü•ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊ°ÜÊû∂ÔºåÁî®‰∫éÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑË∑ØÂæÑËßÑÂàí„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂü∫‰∫éÁéØÂ¢ÉÂèòÂåñÈÄöÂ∏∏Â±ÄÈôê‰∫éÊúâÁïåÂå∫ÂüüÂÜÖÁöÑËßÇÂØü„ÄÇ‰∏∫‰∫ÜÂà©Áî®Ëøô‰∏ÄÁÇπÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÁéØÂ¢ÉÁöÑÂ±ÇÊ¨°ÂàÜËß£ÔºåÂπ∂ÈÉ®ÁΩ≤ÂàÜÂ∏ÉÂºèRLÊô∫ËÉΩ‰ΩìÔºå‰ª•Âú®Êú¨Âú∞ÈÄÇÂ∫îÂèòÂåñ„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ≠êÁéØÂ¢ÉÊàêÂäüÁéáÁöÑÈáçËÆ≠ÁªÉÊú∫Âà∂Ôºå‰ª•Á°ÆÂÆö‰ΩïÊó∂ÈúÄË¶ÅÁ≠ñÁï•Êõ¥Êñ∞„ÄÇÊàë‰ª¨Êé¢Á¥¢‰∫Ü‰∏§ÁßçËÆ≠ÁªÉËåÉÂºèÔºöÂçïÊô∫ËÉΩ‰ΩìQÂ≠¶‰π†ÂíåÂ§öÊô∫ËÉΩ‰ΩìËÅîÈÇ¶QÂ≠¶‰π†ÔºåÂÖ∂‰∏≠Êú¨Âú∞QË°®ÂÆöÊúüËÅöÂêà‰ª•Âä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇ‰∏éÂÖàÂâçÁöÑÂ∑•‰Ωú‰∏çÂêåÔºåÊàë‰ª¨Âú®Êõ¥Áé∞ÂÆûÁöÑËÆæÁΩÆ‰∏≠ËØÑ‰º∞Êàë‰ª¨ÁöÑÊñπÊ≥ïÔºåÂÖ∂‰∏≠Â≠òÂú®Â§ö‰∏™ÂêåÊó∂ÂèëÁîüÁöÑÈöúÁ¢çÁâ©ÂèòÂåñÂíåÈöæÂ∫¶Á∫ßÂà´Â¢ûÂä†„ÄÇÁªìÊûúË°®ÊòéÔºåËÅîÈÇ¶Âèò‰ΩìÂßãÁªà‰ºò‰∫éÂÖ∂ÂçïÊô∫ËÉΩ‰ΩìÂØπÂ∫îÁâ©ÔºåÂπ∂‰∏îÂú®‰øùÊåÅÊõ¥Áü≠ÁöÑÈÄÇÂ∫îÊó∂Èó¥ÂíåÂº∫Â§ßÁöÑÂèØÊâ©Â±ïÊÄßÁöÑÂêåÊó∂ÔºåÊé•ËøëA* OracleÁöÑÊÄßËÉΩ„ÄÇËôΩÁÑ∂ÂàùÂßãËÆ≠ÁªÉÂú®Â§ßÁéØÂ¢É‰∏≠‰ªçÁÑ∂ËÄóÊó∂Ôºå‰ΩÜÊàë‰ª¨ÁöÑÂàÜÊï£ÂºèÊ°ÜÊû∂Ê∂àÈô§‰∫ÜÂØπÂÖ®Â±ÄËßÑÂàíÂô®ÁöÑÈúÄÊ±ÇÔºåÂπ∂‰∏∫Êú™Êù•‰ΩøÁî®Ê∑±Â∫¶RLÂíåÁÅµÊ¥ªÁöÑÁéØÂ¢ÉÂàÜËß£ËøõË°åÊîπËøõÂ•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âä®ÊÄÅÁéØÂ¢É‰∏≠Ë∑ØÂæÑËßÑÂàíÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂ¶ÇÂÖ®Â±ÄËßÑÂàíÂô®Âú®ÁéØÂ¢ÉÂèòÂåñÈ¢ëÁπÅÊó∂ËÆ°ÁÆó‰ª£‰ª∑È´òÊòÇÔºåÈöæ‰ª•ÂÆûÊó∂ÈÄÇÂ∫îÔºõËÄåÂü∫‰∫éÂçïÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÁöÑÊñπÊ≥ïÈöæ‰ª•Êâ©Â±ïÂà∞Â§çÊùÇÁéØÂ¢ÉÔºåÊ≥õÂåñËÉΩÂäõÊúâÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÁéØÂ¢ÉËøõË°åÂå∫ÂüüÂàÜËß£ÔºåÊØè‰∏™Âå∫ÂüüÈÉ®ÁΩ≤‰∏Ä‰∏™Âº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰ΩìÔºåÊô∫ËÉΩ‰ΩìÂè™ÂÖ≥Ê≥®Â±ÄÈÉ®ÁéØÂ¢ÉÁöÑÂèòÂåñÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÊèêÈ´ò‰∫ÜÈÄÇÂ∫îÊÄß„ÄÇÂêåÊó∂ÔºåÈááÁî®ËÅîÈÇ¶Â≠¶‰π†ÁöÑÊñπÂºèÔºåËÆ©ÂêÑ‰∏™Êô∫ËÉΩ‰ΩìÂÖ±‰∫´Â≠¶‰π†ÁªèÈ™åÔºåÂä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨ÁéØÂ¢ÉÂàÜËß£Ê®°Âùó„ÄÅÂ±ÄÈÉ®Êô∫ËÉΩ‰ΩìÂ≠¶‰π†Ê®°ÂùóÂíåËÅîÈÇ¶Â≠¶‰π†Ê®°Âùó„ÄÇÁéØÂ¢ÉÂàÜËß£Ê®°ÂùóÂ∞ÜÁéØÂ¢ÉÂàíÂàÜ‰∏∫Â§ö‰∏™Â≠êÂå∫ÂüüÔºõÂ±ÄÈÉ®Êô∫ËÉΩ‰ΩìÂ≠¶‰π†Ê®°Âùó‰ΩøÁî®Q-learningÁÆóÊ≥ïËÆ≠ÁªÉÊØè‰∏™Â≠êÂå∫ÂüüÁöÑÊô∫ËÉΩ‰ΩìÔºõËÅîÈÇ¶Â≠¶‰π†Ê®°ÂùóÂÆöÊúüËÅöÂêàÂêÑ‰∏™Êô∫ËÉΩ‰ΩìÁöÑQË°®ÔºåÊõ¥Êñ∞ÂÖ®Â±ÄÁ≠ñÁï•„ÄÇÂΩìÂ≠êÁéØÂ¢ÉÊàêÂäüÁéá‰Ωé‰∫éÈòàÂÄºÊó∂ÔºåËß¶ÂèëÈáçËÆ≠ÁªÉÊú∫Âà∂„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂå∫ÂüüÊÑüÁü•ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂ∞ÜÂÖ®Â±ÄËßÑÂàíÈóÆÈ¢òÂàÜËß£‰∏∫Â§ö‰∏™Â±ÄÈÉ®ËßÑÂàíÈóÆÈ¢òÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÊèêÈ´ò‰∫ÜÈÄÇÂ∫îÊÄß„ÄÇÂêåÊó∂ÔºåÈááÁî®ËÅîÈÇ¶Â≠¶‰π†ÁöÑÊñπÂºèÔºåÂä†ÈÄü‰∫ÜÂ≠¶‰π†ËøáÁ®ãÔºåÊèêÈ´ò‰∫ÜÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÈááÁî®‰∫ÜQ-learningÁÆóÊ≥ï‰Ωú‰∏∫Â±ÄÈÉ®Êô∫ËÉΩ‰ΩìÁöÑÂ≠¶‰π†ÁÆóÊ≥ïÔºåÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°‰∏∫Âà∞ËææÁõÆÊ†áÂ•ñÂä±1ÔºåÁ¢∞ÊíûÊÉ©ÁΩö-1ÔºåÂÖ∂‰ªñÊÉÖÂÜµ‰∏∫-0.01„ÄÇËÅîÈÇ¶Â≠¶‰π†ÈááÁî®ÁÆÄÂçïÁöÑÂπ≥ÂùáËÅöÂêàÊñπÂºèÔºåÂÆöÊúüÂ∞ÜÂêÑ‰∏™Êô∫ËÉΩ‰ΩìÁöÑQË°®ËøõË°åÂπ≥Âùá„ÄÇÈáçËÆ≠ÁªÉÊú∫Âà∂Âü∫‰∫éÂ≠êÁéØÂ¢ÉÊàêÂäüÁéáÔºåÂΩìÊàêÂäüÁéá‰Ωé‰∫éÈòàÂÄºÊó∂ÔºåËß¶ÂèëÈáçËÆ≠ÁªÉ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËÅîÈÇ¶QÂ≠¶‰π†Âèò‰ΩìÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠Ë∑ØÂæÑËßÑÂàí‰ªªÂä°‰∏≠ÔºåÊÄßËÉΩÊòæËëó‰ºò‰∫éÂçïÊô∫ËÉΩ‰ΩìQÂ≠¶‰π†ÔºåÂπ∂‰∏îÊé•ËøëA* OracleÁöÑÊÄßËÉΩ„ÄÇÂú®Â§ö‰∏™ÂêåÊó∂ÂèëÁîüÁöÑÈöúÁ¢çÁâ©ÂèòÂåñÂíåÈöæÂ∫¶Á∫ßÂà´Â¢ûÂä†ÁöÑÂú∫ÊôØ‰∏ãÔºåËÅîÈÇ¶QÂ≠¶‰π†Ë°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂÖ∑ÊúâËâØÂ•ΩÁöÑÂèØÊâ©Â±ïÊÄßÔºåÂèØ‰ª•Â∫îÁî®‰∫éÊõ¥Â§ßËßÑÊ®°ÁöÑÁéØÂ¢É„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩ‰∫§ÈÄöÁ≥ªÁªü„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÊ∏∏ÊàèAIÁ≠âÈ¢ÜÂüü„ÄÇÂú®Êô∫ËÉΩ‰∫§ÈÄöÁ≥ªÁªü‰∏≠ÔºåÂèØ‰ª•Áî®‰∫éËΩ¶ËæÜÁöÑËá™Âä®È©æÈ©∂ÂíåË∑ØÂæÑËßÑÂàíÔºåÊèêÈ´ò‰∫§ÈÄöÊïàÁéáÂíåÂÆâÂÖ®ÊÄß„ÄÇÂú®Êú∫Âô®‰∫∫ÂØºËà™‰∏≠ÔºåÂèØ‰ª•Áî®‰∫éÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑËá™‰∏ªÂØºËà™ÂíåÈÅøÈöú„ÄÇÂú®Ê∏∏ÊàèAI‰∏≠ÔºåÂèØ‰ª•Áî®‰∫éÊ∏∏ÊàèËßíËâ≤ÁöÑÊô∫ËÉΩË°å‰∏∫ÂÜ≥Á≠ñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.

