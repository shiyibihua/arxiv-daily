---
layout: default
title: Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments
---

# Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments

**arXiv**: [2511.15284v1](https://arxiv.org/abs/2511.15284) | [PDF](https://arxiv.org/pdf/2511.15284.pdf)

**ä½œè€…**: Jonas De Maeyer, Hossein Yarahmadi, Moharram Challenger

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºŽå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€çŽ¯å¢ƒè·¯å¾„è§„åˆ’æ–¹æ³•**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è·¯å¾„è§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ™ºèƒ½ä½“` `åŠ¨æ€çŽ¯å¢ƒ` `è”é‚¦å­¦ä¹ ` `æœºå™¨äººå¯¼èˆª` `æ™ºèƒ½äº¤é€š`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŠ¨æ€çŽ¯å¢ƒè·¯å¾„è§„åˆ’æ–¹æ³•éš¾ä»¥åº”å¯¹çŽ¯å¢ƒçš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ï¼Œå…¨å±€è§„åˆ’å™¨è®¡ç®—é‡å¤§ï¼Œéš¾ä»¥æ‰©å±•ã€‚
2. æå‡ºä¸€ç§åŒºåŸŸæ„ŸçŸ¥çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œå°†çŽ¯å¢ƒåˆ†è§£ä¸ºå±€éƒ¨åŒºåŸŸï¼Œåˆ©ç”¨åˆ†å¸ƒå¼æ™ºèƒ½ä½“è¿›è¡Œå±€éƒ¨é€‚åº”ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè”é‚¦å­¦ä¹ å˜ä½“ä¼˜äºŽå•æ™ºèƒ½ä½“ï¼Œæ€§èƒ½æŽ¥è¿‘A* Oracleï¼Œå¹¶å…·æœ‰æ›´çŸ­çš„é€‚åº”æ—¶é—´å’Œè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ™ºèƒ½äº¤é€šå’Œæœºå™¨äººé¢†åŸŸï¼ŒåŠ¨æ€çŽ¯å¢ƒä¸­çš„è·¯å¾„è§„åˆ’æ˜¯ä¸€ä¸ªæ ¹æœ¬æ€§çš„æŒ‘æˆ˜ï¼Œå…¶ä¸­éšœç¢ç‰©å’Œæ¡ä»¶éšæ—¶é—´å˜åŒ–ï¼Œå¼•å…¥ä¸ç¡®å®šæ€§å¹¶éœ€è¦æŒç»­é€‚åº”ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾å®Œå…¨çš„çŽ¯å¢ƒä¸å¯é¢„æµ‹æ€§æˆ–ä¾èµ–äºŽå…¨å±€è§„åˆ’å™¨ï¼Œè¿™äº›å‡è®¾é™åˆ¶äº†åœ¨å®žé™…çŽ¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§å’Œå®žé™…éƒ¨ç½²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„ã€åŒºåŸŸæ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æž¶ï¼Œç”¨äºŽåŠ¨æ€çŽ¯å¢ƒä¸­çš„è·¯å¾„è§„åˆ’ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºŽçŽ¯å¢ƒå˜åŒ–é€šå¸¸å±€é™äºŽæœ‰ç•ŒåŒºåŸŸå†…çš„è§‚å¯Ÿã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†çŽ¯å¢ƒçš„å±‚æ¬¡åˆ†è§£ï¼Œå¹¶éƒ¨ç½²åˆ†å¸ƒå¼RLæ™ºèƒ½ä½“ï¼Œä»¥åœ¨æœ¬åœ°é€‚åº”å˜åŒ–ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºŽå­çŽ¯å¢ƒæˆåŠŸçŽ‡çš„é‡è®­ç»ƒæœºåˆ¶ï¼Œä»¥ç¡®å®šä½•æ—¶éœ€è¦ç­–ç•¥æ›´æ–°ã€‚æˆ‘ä»¬æŽ¢ç´¢äº†ä¸¤ç§è®­ç»ƒèŒƒå¼ï¼šå•æ™ºèƒ½ä½“Qå­¦ä¹ å’Œå¤šæ™ºèƒ½ä½“è”é‚¦Qå­¦ä¹ ï¼Œå…¶ä¸­æœ¬åœ°Qè¡¨å®šæœŸèšåˆä»¥åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚ä¸Žå…ˆå‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬åœ¨æ›´çŽ°å®žçš„è®¾ç½®ä¸­è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå…¶ä¸­å­˜åœ¨å¤šä¸ªåŒæ—¶å‘ç”Ÿçš„éšœç¢ç‰©å˜åŒ–å’Œéš¾åº¦çº§åˆ«å¢žåŠ ã€‚ç»“æžœè¡¨æ˜Žï¼Œè”é‚¦å˜ä½“å§‹ç»ˆä¼˜äºŽå…¶å•æ™ºèƒ½ä½“å¯¹åº”ç‰©ï¼Œå¹¶ä¸”åœ¨ä¿æŒæ›´çŸ­çš„é€‚åº”æ—¶é—´å’Œå¼ºå¤§çš„å¯æ‰©å±•æ€§çš„åŒæ—¶ï¼ŒæŽ¥è¿‘A* Oracleçš„æ€§èƒ½ã€‚è™½ç„¶åˆå§‹è®­ç»ƒåœ¨å¤§çŽ¯å¢ƒä¸­ä»ç„¶è€—æ—¶ï¼Œä½†æˆ‘ä»¬çš„åˆ†æ•£å¼æ¡†æž¶æ¶ˆé™¤äº†å¯¹å…¨å±€è§„åˆ’å™¨çš„éœ€æ±‚ï¼Œå¹¶ä¸ºæœªæ¥ä½¿ç”¨æ·±åº¦RLå’Œçµæ´»çš„çŽ¯å¢ƒåˆ†è§£è¿›è¡Œæ”¹è¿›å¥ å®šäº†åŸºç¡€ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŠ¨æ€çŽ¯å¢ƒä¸­è·¯å¾„è§„åˆ’é—®é¢˜ï¼ŒçŽ°æœ‰æ–¹æ³•å¦‚å…¨å±€è§„åˆ’å™¨åœ¨çŽ¯å¢ƒå˜åŒ–é¢‘ç¹æ—¶è®¡ç®—ä»£ä»·é«˜æ˜‚ï¼Œéš¾ä»¥å®žæ—¶é€‚åº”ï¼›è€ŒåŸºäºŽå•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•éš¾ä»¥æ‰©å±•åˆ°å¤æ‚çŽ¯å¢ƒï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†çŽ¯å¢ƒè¿›è¡ŒåŒºåŸŸåˆ†è§£ï¼Œæ¯ä¸ªåŒºåŸŸéƒ¨ç½²ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œæ™ºèƒ½ä½“åªå…³æ³¨å±€éƒ¨çŽ¯å¢ƒçš„å˜åŒ–ï¼Œä»Žè€Œé™ä½Žäº†è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜äº†é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨è”é‚¦å­¦ä¹ çš„æ–¹å¼ï¼Œè®©å„ä¸ªæ™ºèƒ½ä½“å…±äº«å­¦ä¹ ç»éªŒï¼ŒåŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…æ‹¬çŽ¯å¢ƒåˆ†è§£æ¨¡å—ã€å±€éƒ¨æ™ºèƒ½ä½“å­¦ä¹ æ¨¡å—å’Œè”é‚¦å­¦ä¹ æ¨¡å—ã€‚çŽ¯å¢ƒåˆ†è§£æ¨¡å—å°†çŽ¯å¢ƒåˆ’åˆ†ä¸ºå¤šä¸ªå­åŒºåŸŸï¼›å±€éƒ¨æ™ºèƒ½ä½“å­¦ä¹ æ¨¡å—ä½¿ç”¨Q-learningç®—æ³•è®­ç»ƒæ¯ä¸ªå­åŒºåŸŸçš„æ™ºèƒ½ä½“ï¼›è”é‚¦å­¦ä¹ æ¨¡å—å®šæœŸèšåˆå„ä¸ªæ™ºèƒ½ä½“çš„Qè¡¨ï¼Œæ›´æ–°å…¨å±€ç­–ç•¥ã€‚å½“å­çŽ¯å¢ƒæˆåŠŸçŽ‡ä½ŽäºŽé˜ˆå€¼æ—¶ï¼Œè§¦å‘é‡è®­ç»ƒæœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†åŒºåŸŸæ„ŸçŸ¥çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œå°†å…¨å±€è§„åˆ’é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå±€éƒ¨è§„åˆ’é—®é¢˜ï¼Œé™ä½Žäº†è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜äº†é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨è”é‚¦å­¦ä¹ çš„æ–¹å¼ï¼ŒåŠ é€Ÿäº†å­¦ä¹ è¿‡ç¨‹ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡é‡‡ç”¨äº†Q-learningç®—æ³•ä½œä¸ºå±€éƒ¨æ™ºèƒ½ä½“çš„å­¦ä¹ ç®—æ³•ï¼Œå¥–åŠ±å‡½æ•°è®¾è®¡ä¸ºåˆ°è¾¾ç›®æ ‡å¥–åŠ±1ï¼Œç¢°æ’žæƒ©ç½š-1ï¼Œå…¶ä»–æƒ…å†µä¸º-0.01ã€‚è”é‚¦å­¦ä¹ é‡‡ç”¨ç®€å•çš„å¹³å‡èšåˆæ–¹å¼ï¼Œå®šæœŸå°†å„ä¸ªæ™ºèƒ½ä½“çš„Qè¡¨è¿›è¡Œå¹³å‡ã€‚é‡è®­ç»ƒæœºåˆ¶åŸºäºŽå­çŽ¯å¢ƒæˆåŠŸçŽ‡ï¼Œå½“æˆåŠŸçŽ‡ä½ŽäºŽé˜ˆå€¼æ—¶ï¼Œè§¦å‘é‡è®­ç»ƒã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè”é‚¦Qå­¦ä¹ å˜ä½“åœ¨åŠ¨æ€çŽ¯å¢ƒä¸­è·¯å¾„è§„åˆ’ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºŽå•æ™ºèƒ½ä½“Qå­¦ä¹ ï¼Œå¹¶ä¸”æŽ¥è¿‘A* Oracleçš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªåŒæ—¶å‘ç”Ÿçš„éšœç¢ç‰©å˜åŒ–å’Œéš¾åº¦çº§åˆ«å¢žåŠ çš„åœºæ™¯ä¸‹ï¼Œè”é‚¦Qå­¦ä¹ è¡¨çŽ°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯ä»¥åº”ç”¨äºŽæ›´å¤§è§„æ¨¡çš„çŽ¯å¢ƒã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæ™ºèƒ½äº¤é€šç³»ç»Ÿã€æœºå™¨äººå¯¼èˆªã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­ï¼Œå¯ä»¥ç”¨äºŽè½¦è¾†çš„è‡ªåŠ¨é©¾é©¶å’Œè·¯å¾„è§„åˆ’ï¼Œæé«˜äº¤é€šæ•ˆçŽ‡å’Œå®‰å…¨æ€§ã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥ç”¨äºŽæœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªå’Œé¿éšœã€‚åœ¨æ¸¸æˆAIä¸­ï¼Œå¯ä»¥ç”¨äºŽæ¸¸æˆè§’è‰²çš„æ™ºèƒ½è¡Œä¸ºå†³ç­–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.

