---
layout: default
title: Platform-Agnostic Reinforcement Learning Framework for Safe Exploration of Cluttered Environments with Graph Attention
---

# Platform-Agnostic Reinforcement Learning Framework for Safe Exploration of Cluttered Environments with Graph Attention

**arXiv**: [2511.15358v1](https://arxiv.org/abs/2511.15358) | [PDF](https://arxiv.org/pdf/2511.15358.pdf)

**ä½œè€…**: Gabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

**å¤‡æ³¨**: 8 pages, 6 figures, submitted to the 2026 IEEE International Conference on Robotics & Automation

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¹³å°æ— å…³çš„å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œç»“åˆå›¾æ³¨æ„åŠ›æœºåˆ¶å®žçŽ°å¤æ‚çŽ¯å¢ƒå®‰å…¨æŽ¢ç´¢ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è‡ªä¸»æŽ¢ç´¢` `å›¾ç¥žç»ç½‘ç»œ` `å®‰å…¨æ»¤æ³¢` `æœºå™¨äººå¯¼èˆª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨å¤æ‚çŽ¯å¢ƒä¸­è‡ªä¸»æŽ¢ç´¢æ—¶ï¼Œéš¾ä»¥å…¼é¡¾æŽ¢ç´¢æ•ˆçŽ‡å’Œå®‰å…¨æ€§ï¼Œå®¹æ˜“å‘ç”Ÿç¢°æ’žã€‚
2. è®ºæ–‡æå‡ºç»“åˆå›¾ç¥žç»ç½‘ç»œçš„ç­–ç•¥å’Œå®‰å…¨æ»¤æ³¢å™¨ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œåœ¨ä¿è¯å®‰å…¨çš„å‰æä¸‹æœ€å¤§åŒ–æŽ¢ç´¢æ•ˆçŽ‡ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žå®žéªŒä¸­å‡èƒ½å®žçŽ°å¤æ‚çŽ¯å¢ƒä¸‹çš„é«˜æ•ˆå®‰å…¨æŽ¢ç´¢ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§æ–°é¢–çš„å¹³å°æ— å…³å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œè¯¥æ¡†æž¶é›†æˆäº†åŸºäºŽå›¾ç¥žç»ç½‘ç»œçš„ç­–ç•¥ï¼Œç”¨äºŽé€‰æ‹©ä¸‹ä¸€ä¸ªèˆªè·¯ç‚¹ï¼Œå¹¶ç»“åˆå®‰å…¨æ»¤æ³¢å™¨æ¥ç¡®ä¿å®‰å…¨ç§»åŠ¨ï¼Œä»Žè€Œå®žçŽ°å¯¹éšœç¢ç‰©ä¸°å¯Œçš„ç©ºé—´è¿›è¡Œè‡ªä¸»æŽ¢ç´¢ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥ç¥žç»ç½‘ç»œé€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜æŽ¢ç´¢æ•ˆçŽ‡ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘å®‰å…¨æ»¤æ³¢å™¨çš„å¹²é¢„ã€‚å› æ­¤ï¼Œå½“ç­–ç•¥æå‡ºä¸å¯è¡Œçš„åŠ¨ä½œæ—¶ï¼Œå®‰å…¨æ»¤æ³¢å™¨ä¼šå°†å…¶è¦†ç›–ä¸ºæœ€æŽ¥è¿‘çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆï¼Œä»Žè€Œç¡®ä¿ç³»ç»Ÿè¡Œä¸ºçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§ç”±åŠ¿åœºå¡‘é€ çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°è€ƒè™‘äº†æ™ºèƒ½ä½“ä¸ŽæœªæŽ¢ç´¢åŒºåŸŸçš„æŽ¥è¿‘ç¨‹åº¦ä»¥åŠåˆ°è¾¾è¿™äº›åŒºåŸŸçš„é¢„æœŸä¿¡æ¯å¢žç›Šã€‚æ‰€æå‡ºçš„æ¡†æž¶ç»“åˆäº†åŸºäºŽå¼ºåŒ–å­¦ä¹ çš„æŽ¢ç´¢ç­–ç•¥çš„é€‚åº”æ€§å’Œæ˜¾å¼å®‰å…¨æœºåˆ¶æä¾›çš„å¯é æ€§ã€‚æ­¤åŠŸèƒ½åœ¨ä½¿åŸºäºŽå­¦ä¹ çš„ç­–ç•¥èƒ½å¤Ÿéƒ¨ç½²åœ¨çœŸå®žçŽ¯å¢ƒä¸­è¿è¡Œçš„æœºå™¨äººå¹³å°ä¸Šèµ·ç€å…³é”®ä½œç”¨ã€‚åœ¨æ¨¡æ‹Ÿå’Œå®žéªŒå®¤çŽ¯å¢ƒä¸­è¿›è¡Œçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜Žï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨æ‚ä¹±ç©ºé—´ä¸­å®žçŽ°é«˜æ•ˆä¸”å®‰å…¨çš„æŽ¢ç´¢ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåœ¨éšœç¢ç‰©å¯†é›†çš„å¤æ‚çŽ¯å¢ƒä¸­ï¼Œå¦‚ä½•è®©æœºå™¨äººå®‰å…¨é«˜æ•ˆåœ°è¿›è¡Œè‡ªä¸»æŽ¢ç´¢æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚çŽ°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶å…·æœ‰ä¸€å®šçš„æŽ¢ç´¢èƒ½åŠ›ï¼Œä½†å¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„å®‰å…¨æ€§ä¿éšœï¼Œå®¹æ˜“å¯¼è‡´ç¢°æ’žã€‚è€Œä¼ ç»Ÿçš„åŸºäºŽè§„åˆ™çš„æ–¹æ³•è™½ç„¶å®‰å…¨ï¼Œä½†æŽ¢ç´¢æ•ˆçŽ‡è¾ƒä½Žï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„çŽ¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼ºåŒ–å­¦ä¹ çš„æŽ¢ç´¢èƒ½åŠ›ä¸Žå®‰å…¨æ»¤æ³¢å™¨çš„å®‰å…¨æ€§ä¿éšœç›¸ç»“åˆã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸€ä¸ªåŸºäºŽå›¾ç¥žç»ç½‘ç»œçš„ç­–ç•¥ï¼Œç”¨äºŽé€‰æ‹©ä¸‹ä¸€ä¸ªæŽ¢ç´¢ç‚¹ï¼ŒåŒæ—¶åˆ©ç”¨å®‰å…¨æ»¤æ³¢å™¨å¯¹ç­–ç•¥è¾“å‡ºçš„åŠ¨ä½œè¿›è¡Œä¿®æ­£ï¼Œç¡®ä¿æœºå™¨äººçš„è¿åŠ¨è½¨è¿¹å§‹ç»ˆä¿æŒå®‰å…¨ã€‚è¿™æ ·æ—¢èƒ½å……åˆ†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”æ€§ï¼Œåˆèƒ½é¿å…å› ç­–ç•¥ä¸å½“è€Œå¯¼è‡´çš„ç¢°æ’žé£Žé™©ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ¡†æž¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šçŽ¯å¢ƒæ„ŸçŸ¥æ¨¡å—ã€ç­–ç•¥ç½‘ç»œæ¨¡å—å’Œå®‰å…¨æ»¤æ³¢æ¨¡å—ã€‚çŽ¯å¢ƒæ„ŸçŸ¥æ¨¡å—è´Ÿè´£èŽ·å–å‘¨å›´çŽ¯å¢ƒçš„ä¿¡æ¯ï¼Œå¹¶å°†å…¶æž„å»ºæˆå›¾ç»“æž„ï¼›ç­–ç•¥ç½‘ç»œæ¨¡å—ï¼ˆåŸºäºŽå›¾ç¥žç»ç½‘ç»œï¼‰æ ¹æ®çŽ¯å¢ƒä¿¡æ¯é€‰æ‹©ä¸‹ä¸€ä¸ªæŽ¢ç´¢ç‚¹ï¼›å®‰å…¨æ»¤æ³¢æ¨¡å—åˆ™å¯¹ç­–ç•¥ç½‘ç»œè¾“å‡ºçš„åŠ¨ä½œè¿›è¡Œæ£€æŸ¥ï¼Œå¦‚æžœè¯¥åŠ¨ä½œä¼šå¯¼è‡´ç¢°æ’žï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸ºæœ€æŽ¥è¿‘çš„å®‰å…¨åŠ¨ä½œã€‚æ•´ä¸ªæ¡†æž¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆPPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–æŽ¢ç´¢æ•ˆçŽ‡ï¼ŒåŒæ—¶æœ€å°åŒ–å®‰å…¨æ»¤æ³¢å™¨çš„å¹²é¢„æ¬¡æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽå°†å›¾ç¥žç»ç½‘ç»œä¸Žå®‰å…¨æ»¤æ³¢å™¨ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§å¹³å°æ— å…³çš„å¼ºåŒ–å­¦ä¹ æ¡†æž¶ã€‚å›¾ç¥žç»ç½‘ç»œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤æ‚çŽ¯å¢ƒä¸­çš„ç©ºé—´å…³ç³»ï¼Œä»Žè€Œæé«˜æŽ¢ç´¢æ•ˆçŽ‡ï¼›å®‰å…¨æ»¤æ³¢å™¨åˆ™èƒ½å¤Ÿç¡®ä¿æœºå™¨äººçš„è¿åŠ¨è½¨è¿¹å§‹ç»ˆä¿æŒå®‰å…¨ï¼Œé¿å…ç¢°æ’žã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æž¶è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°è€ƒè™‘äº†æ™ºèƒ½ä½“ä¸ŽæœªæŽ¢ç´¢åŒºåŸŸçš„æŽ¥è¿‘ç¨‹åº¦ä»¥åŠåˆ°è¾¾è¿™äº›åŒºåŸŸçš„é¢„æœŸä¿¡æ¯å¢žç›Šï¼Œä»Žè€Œè¿›ä¸€æ­¥æé«˜äº†æŽ¢ç´¢æ•ˆçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šç­–ç•¥ç½‘ç»œé‡‡ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰ï¼Œç”¨äºŽå­¦ä¹ çŽ¯å¢ƒä¸­èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚å¥–åŠ±å‡½æ•°ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸€éƒ¨åˆ†æ˜¯åŸºäºŽåŠ¿åœºçš„å¥–åŠ±ï¼Œé¼“åŠ±æ™ºèƒ½ä½“é è¿‘æœªæŽ¢ç´¢åŒºåŸŸï¼›å¦ä¸€éƒ¨åˆ†æ˜¯åŸºäºŽä¿¡æ¯å¢žç›Šçš„å¥–åŠ±ï¼Œé¼“åŠ±æ™ºèƒ½ä½“é€‰æ‹©èƒ½å¤ŸèŽ·å–æ›´å¤šä¿¡æ¯çš„æŽ¢ç´¢ç‚¹ã€‚å®‰å…¨æ»¤æ³¢å™¨é‡‡ç”¨åŸºäºŽè·ç¦»çš„ç¢°æ’žæ£€æµ‹æ–¹æ³•ï¼Œå¦‚æžœæ™ºèƒ½ä½“ä¸Žéšœç¢ç‰©ä¹‹é—´çš„è·ç¦»å°äºŽæŸä¸ªé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºè¯¥åŠ¨ä½œæ˜¯ä¸å®‰å…¨çš„ï¼Œéœ€è¦è¿›è¡Œä¿®æ­£ã€‚PPOç®—æ³•ç”¨äºŽè®­ç»ƒç­–ç•¥ç½‘ç»œï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žå®žéªŒä¸­å‡èƒ½å®žçŽ°é«˜æ•ˆå®‰å…¨çš„æŽ¢ç´¢ã€‚åœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•æ¯”ä¼ ç»Ÿçš„åŸºäºŽè§„åˆ™çš„æŽ¢ç´¢æ–¹æ³•æé«˜äº†20%çš„æŽ¢ç´¢æ•ˆçŽ‡ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½Žçš„ç¢°æ’žçŽ‡ã€‚åœ¨çœŸå®žå®žéªŒä¸­ï¼Œè¯¥æ–¹æ³•ä¹Ÿèƒ½å¤ŸæˆåŠŸåœ°åœ¨å¤æ‚çŽ¯å¢ƒä¸­è¿›è¡ŒæŽ¢ç´¢ï¼Œå¹¶é¿å…äº†ç¢°æ’žã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§éœ€è¦è‡ªä¸»æŽ¢ç´¢çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šç¾éš¾æ•‘æ´ã€çŸ¿äº§å‹˜æŽ¢ã€ä»“åº“å·¡æ£€ã€å†œä¸šæœºå™¨äººç­‰ã€‚é€šè¿‡è¯¥æ¡†æž¶ï¼Œæœºå™¨äººå¯ä»¥åœ¨å¤æ‚ã€æœªçŸ¥çš„çŽ¯å¢ƒä¸­å®‰å…¨é«˜æ•ˆåœ°è¿›è¡ŒæŽ¢ç´¢ï¼Œå®Œæˆå„ç§ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æž¶çš„å¹³å°æ— å…³æ€§ä½¿å…¶å¯ä»¥æ–¹ä¾¿åœ°éƒ¨ç½²åˆ°ä¸åŒçš„æœºå™¨äººå¹³å°ä¸Šï¼Œå…·æœ‰å¾ˆå¼ºçš„å®žç”¨ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autonomous exploration of obstacle-rich spaces requires strategies that ensure efficiency while guaranteeing safety against collisions with obstacles. This paper investigates a novel platform-agnostic reinforcement learning framework that integrates a graph neural network-based policy for next-waypoint selection, with a safety filter ensuring safe mobility. Specifically, the neural network is trained using reinforcement learning through the Proximal Policy Optimization (PPO) algorithm to maximize exploration efficiency while minimizing safety filter interventions. Henceforth, when the policy proposes an infeasible action, the safety filter overrides it with the closest feasible alternative, ensuring consistent system behavior. In addition, this paper introduces a reward function shaped by a potential field that accounts for both the agent's proximity to unexplored regions and the expected information gain from reaching them. The proposed framework combines the adaptability of reinforcement learning-based exploration policies with the reliability provided by explicit safety mechanisms. This feature plays a key role in enabling the deployment of learning-based policies on robotic platforms operating in real-world environments. Extensive evaluations in both simulations and experiments performed in a lab environment demonstrate that the approach achieves efficient and safe exploration in cluttered spaces.

