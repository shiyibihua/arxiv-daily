---
layout: default
title: SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models
---

# SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models

**arXiv**: [2511.15605v2](https://arxiv.org/abs/2511.15605) | [PDF](https://arxiv.org/pdf/2511.15605.pdf)

**ä½œè€…**: Senyu Fei, Siyin Wang, Li Ji, Ao Li, Shiduo Zhang, Liming Liu, Jinlong Hou, Jingjing Gong, Xianzhong Zhao, Xipeng Qiu

**åˆ†ç±»**: cs.RO, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19 (æ›´æ–°: 2025-11-30)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSRPOï¼Œåˆ©ç”¨è‡ªå‚ç…§ç­–ç•¥ä¼˜åŒ–è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ï¼Œè§£å†³å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `è‡ªå‚ç…§ç­–ç•¥ä¼˜åŒ–` `å¥–åŠ±ç¨€ç–` `ä¸–ç•Œæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹ä¾èµ–ä¸“å®¶æ¼”ç¤ºï¼Œå­˜åœ¨æ¼”ç¤ºåå·®ï¼Œä¸”å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å¥–åŠ±ç¨€ç–ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆçŽ‡ä½Žä¸‹ã€‚
2. SRPOåˆ©ç”¨æ¨¡åž‹è‡ªèº«ç”Ÿæˆçš„æˆåŠŸè½¨è¿¹ä½œä¸ºå‚ç…§ï¼Œä¸ºå¤±è´¥è½¨è¿¹åˆ†é…åŸºäºŽè¿›åº¦çš„å¥–åŠ±ï¼Œæ— éœ€é¢å¤–ç›‘ç£ã€‚
3. SRPOä½¿ç”¨ä¸–ç•Œæ¨¡åž‹çš„æ½œåœ¨ç©ºé—´ç¼–ç æ¥è¡¡é‡è¡Œä¸ºè¿›åº¦ï¼Œå®žçŽ°è·¨çŽ¯å¢ƒçš„æ³›åŒ–ï¼Œå¹¶åœ¨LIBEROåŸºå‡†ä¸Šå–å¾—æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹åœ¨æœºå™¨äººæ“ä½œæ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½†è¿‡åº¦ä¾èµ–ä¸“å®¶æ¼”ç¤ºï¼Œå¯¼è‡´æ¼”ç¤ºåå·®å¹¶é™åˆ¶æ€§èƒ½ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯å…‹æœè¿™äº›é™åˆ¶çš„å…³é”®åŽè®­ç»ƒç­–ç•¥ï¼Œç„¶è€Œï¼ŒçŽ°æœ‰çš„VLA-RLæ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºŽç¾¤ä½“çš„ä¼˜åŒ–æ–¹æ³•ï¼Œéƒ½å—åˆ°ä¸¥é‡å¥–åŠ±ç¨€ç–æ€§çš„é˜»ç¢ã€‚ä¾èµ–äºŒå…ƒæˆåŠŸæŒ‡æ ‡æµªè´¹äº†å¤±è´¥è½¨è¿¹ä¸­çš„å®è´µä¿¡æ¯ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆçŽ‡ä½Žä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªå‚ç…§ç­–ç•¥ä¼˜åŒ–ï¼ˆSRPOï¼‰ï¼Œä¸€ç§æ–°é¢–çš„VLA-RLæ¡†æž¶ã€‚SRPOé€šè¿‡åˆ©ç”¨æ¨¡åž‹è‡ªèº«åœ¨å½“å‰è®­ç»ƒæ‰¹æ¬¡ä¸­ç”Ÿæˆçš„æˆåŠŸè½¨è¿¹ä½œä¸ºè‡ªå‚ç…§ï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ¼”ç¤ºæˆ–æ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹çš„éœ€æ±‚ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿä¸ºå¤±è´¥çš„å°è¯•åˆ†é…ä¸€ä¸ªåŸºäºŽè¿›åº¦çš„å¥–åŠ±ã€‚ä¸€ä¸ªæ ¸å¿ƒåˆ›æ–°æ˜¯ä½¿ç”¨æ½œåœ¨ä¸–ç•Œè¡¨å¾æ¥ç¨³å¥åœ°è¡¡é‡è¡Œä¸ºè¿›åº¦ã€‚æˆ‘ä»¬åˆ©ç”¨æ¥è‡ªä¸–ç•Œæ¨¡åž‹çš„æ½œåœ¨ç©ºé—´çš„åŽ‹ç¼©ã€å¯è½¬ç§»çš„ç¼–ç ï¼Œè€Œä¸æ˜¯ä¾èµ–åŽŸå§‹åƒç´ æˆ–éœ€è¦ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒã€‚è¿™äº›è¡¨å¾è‡ªç„¶åœ°æ•æ‰äº†è·¨çŽ¯å¢ƒçš„è¿›åº¦æ¨¡å¼ï¼Œä»Žè€Œå®žçŽ°å‡†ç¡®ã€é€šç”¨çš„è½¨è¿¹æ¯”è¾ƒã€‚åœ¨LIBEROåŸºå‡†ä¸Šçš„ç»éªŒè¯„ä¼°è¯æ˜Žäº†SRPOçš„æ•ˆçŽ‡å’Œæœ‰æ•ˆæ€§ã€‚ä»Ž48.9%çš„æˆåŠŸçŽ‡çš„ç›‘ç£åŸºçº¿å¼€å§‹ï¼ŒSRPOä»…åœ¨200ä¸ªRLæ­¥éª¤ä¸­å°±å®žçŽ°äº†99.2%çš„æœ€æ–°æˆåŠŸçŽ‡ï¼Œåœ¨æ²¡æœ‰ä»»ä½•é¢å¤–ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œç›¸å¯¹æé«˜äº†103%ã€‚æ­¤å¤–ï¼ŒSRPOæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„é²æ£’æ€§ï¼Œåœ¨LIBERO-PlusåŸºå‡†ä¸Šå®žçŽ°äº†167%çš„æ€§èƒ½æå‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œç”±äºŽå¥–åŠ±ç¨€ç–æ€§å¯¼è‡´çš„è®­ç»ƒæ•ˆçŽ‡ä½Žä¸‹çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ä¾èµ–äºŒå…ƒæˆåŠŸ/å¤±è´¥ä¿¡å·ï¼Œå¿½ç•¥äº†å¤±è´¥è½¨è¿¹ä¸­åŒ…å«çš„è¿›åº¦ä¿¡æ¯ï¼Œæˆ–è€…éœ€è¦é¢å¤–çš„äººå·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œå¢žåŠ äº†å¤æ‚æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSRPOçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡åž‹è‡ªèº«äº§ç”Ÿçš„æˆåŠŸè½¨è¿¹ä½œä¸ºå‚ç…§ï¼Œæ¥è¯„ä¼°å¤±è´¥è½¨è¿¹çš„è¿›åº¦ã€‚é€šè¿‡æ¯”è¾ƒå¤±è´¥è½¨è¿¹ä¸ŽæˆåŠŸè½¨è¿¹åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„ç›¸ä¼¼åº¦ï¼Œå¯ä»¥æŽ¨æ–­å‡ºå¤±è´¥è½¨è¿¹çš„å®Œæˆç¨‹åº¦ï¼Œä»Žè€Œèµ‹äºˆå…¶ä¸€ä¸ªè¿žç»­çš„ã€åŸºäºŽè¿›åº¦çš„å¥–åŠ±ã€‚è¿™ç§è‡ªå‚ç…§çš„æ–¹å¼é¿å…äº†å¯¹å¤–éƒ¨æ¼”ç¤ºæˆ–äººå·¥å¥–åŠ±å·¥ç¨‹çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šSRPOçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) VLAæ¨¡åž‹åˆå§‹åŒ–ï¼šä½¿ç”¨ç›‘ç£å­¦ä¹ åœ¨ä¸“å®¶æ•°æ®é›†ä¸Šé¢„è®­ç»ƒVLAæ¨¡åž‹ã€‚2) è½¨è¿¹ç”Ÿæˆï¼šä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆä¸€æ‰¹è½¨è¿¹ã€‚3) è‡ªå‚ç…§é€‰æ‹©ï¼šä»Žå½“å‰æ‰¹æ¬¡ä¸­é€‰æ‹©æˆåŠŸçš„è½¨è¿¹ä½œä¸ºå‚ç…§ã€‚4) æ½œåœ¨ç©ºé—´ç¼–ç ï¼šä½¿ç”¨ä¸–ç•Œæ¨¡åž‹å°†è½¨è¿¹ç¼–ç åˆ°æ½œåœ¨ç©ºé—´ã€‚5) å¥–åŠ±è®¡ç®—ï¼šè®¡ç®—å¤±è´¥è½¨è¿¹ä¸ŽæˆåŠŸè½¨è¿¹åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„ç›¸ä¼¼åº¦ï¼Œä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚6) ç­–ç•¥æ›´æ–°ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOï¼‰æ›´æ–°ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šSRPOæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽå…¶è‡ªå‚ç…§çš„å¥–åŠ±ç”Ÿæˆæœºåˆ¶ã€‚ä¸Žä¼ ç»Ÿçš„ä¾èµ–äºŒå…ƒå¥–åŠ±æˆ–äººå·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°çš„æ–¹æ³•ä¸åŒï¼ŒSRPOåˆ©ç”¨æ¨¡åž‹è‡ªèº«ç”Ÿæˆçš„æˆåŠŸè½¨è¿¹ä½œä¸ºå‚ç…§ï¼Œè‡ªåŠ¨åœ°ä¸ºå¤±è´¥è½¨è¿¹èµ‹äºˆåŸºäºŽè¿›åº¦çš„å¥–åŠ±ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ä¸–ç•Œæ¨¡åž‹çš„æ½œåœ¨ç©ºé—´ç¼–ç ï¼Œä½¿å¾—å¥–åŠ±ä¿¡å·æ›´åŠ é²æ£’å’Œæ³›åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šSRPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä¸–ç•Œæ¨¡åž‹çš„é€‰æ‹©ï¼šè®ºæ–‡ä½¿ç”¨é¢„è®­ç»ƒçš„ä¸–ç•Œæ¨¡åž‹ï¼Œä»¥èŽ·å¾—é«˜è´¨é‡çš„æ½œåœ¨ç©ºé—´ç¼–ç ã€‚2) ç›¸ä¼¼åº¦åº¦é‡ï¼šè®ºæ–‡ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¡¡é‡å¤±è´¥è½¨è¿¹ä¸ŽæˆåŠŸè½¨è¿¹åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„ç›¸ä¼¼åº¦ã€‚3) å¥–åŠ±ç¼©æ”¾ï¼šè®ºæ–‡å¯¹å¥–åŠ±ä¿¡å·è¿›è¡Œç¼©æ”¾ï¼Œä»¥æŽ§åˆ¶å…¶å¤§å°å’Œæ–¹å·®ã€‚4) ç­–ç•¥æ›´æ–°ç®—æ³•ï¼šè®ºæ–‡ä½¿ç”¨PPOç®—æ³•æ¥æ›´æ–°ç­–ç•¥ï¼Œå¹¶è°ƒæ•´äº†PPOçš„è¶…å‚æ•°ä»¥é€‚åº”è‡ªå‚ç…§å¥–åŠ±çš„ç‰¹ç‚¹ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

SRPOåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»Ž48.9%çš„ç›‘ç£å­¦ä¹ åŸºçº¿æˆåŠŸçŽ‡æå‡è‡³99.2%ï¼Œä»…éœ€200æ­¥å¼ºåŒ–å­¦ä¹ ï¼Œç›¸å¯¹æå‡103%ã€‚åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„LIBERO-PlusåŸºå‡†ä¸Šï¼ŒSRPOå®žçŽ°äº†167%çš„æ€§èƒ½æå‡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒSRPOèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å¥–åŠ±ç¨€ç–é—®é¢˜ï¼Œå¹¶æ˜¾è‘—æé«˜VLAæ¨¡åž‹çš„æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

SRPOå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºŽæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚é€šè¿‡å‡å°‘å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œé™ä½Žäº†æ¨¡åž‹è®­ç»ƒçš„æˆæœ¬ï¼Œå¹¶æé«˜äº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•æœ‰æœ›æŽ¨åŠ¨æœºå™¨äººæ™ºèƒ½çš„è¿›ä¸€æ­¥å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„çŽ¯å¢ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.

