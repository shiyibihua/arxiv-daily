---
layout: default
title: Fault Tolerant Control of a Quadcopter using Reinforcement Learning
---

# Fault Tolerant Control of a Quadcopter using Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07707" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07707v1</a>
  <a href="https://arxiv.org/pdf/2509.07707.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07707v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07707v1', 'Fault Tolerant Control of a Quadcopter using Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Muzaffar Habib, Adnan Maqsood, Adnan Fayyaz ud Din

**åˆ†ç±»**: cs.RO, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

**å¤‡æ³¨**: e-ISSN: 1946-3901, ISSN: 1946-3855, https://www.sae.org/publications/technical-papers/content/01-18-01-0006/

**æœŸåˆŠ**: SAE International Journal of Aerospace-V134-1EJ, 2025

**DOI**: [10.4271/01-18-01-0006](https://doi.org/10.4271/01-18-01-0006)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å››æ—‹ç¿¼å®¹é”™æ§åˆ¶æ¡†æ¶ï¼Œæå‡å•æ¡¨å¤±æ•ˆä¸‹çš„é²æ£’æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å››æ—‹ç¿¼` `å®¹é”™æ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€è§„åˆ’` `æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦` `æ— äººæœº` `æ•…éšœè¯Šæ–­` `é²æ£’æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å››æ—‹ç¿¼æ§åˆ¶æ–¹æ³•åœ¨å•æ¡¨å¤±æ•ˆç­‰çªå‘æƒ…å†µä¸‹é²æ£’æ€§ä¸è¶³ï¼Œéš¾ä»¥ä¿è¯é£è¡Œå®‰å…¨å’Œæœ‰æ•ˆè½½è·ã€‚
2. é‡‡ç”¨åŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰å’Œæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDDPGï¼‰ä¸¤ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡å››æ—‹ç¿¼åœ¨å•æ¡¨å¤±æ•ˆåçš„å®¹é”™æ§åˆ¶èƒ½åŠ›ã€‚
3. é€šè¿‡MATLABä»¿çœŸéªŒè¯ï¼Œæ”¹è¿›çš„DPå’ŒDDPGç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å•æ¡¨å¤±æ•ˆï¼Œå¹¶åœ¨ä¸åŒåˆå§‹æ¡ä»¶ä¸‹ä¿æŒæœŸæœ›é«˜åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ§åˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å››æ—‹ç¿¼çš„å®‰å…¨æ€§å’Œé²æ£’æ€§ï¼Œç‰¹åˆ«å…³æ³¨é£è¡Œä¸­å•æ¡¨å¤±æ•ˆçš„å®¹é”™èƒ½åŠ›ã€‚é’ˆå¯¹å››æ—‹ç¿¼ä¿æŒæœŸæœ›é«˜åº¦ä»¥ä¿éšœç¡¬ä»¶å’Œæœ‰æ•ˆè½½è·å®‰å…¨çš„å…³é”®éœ€æ±‚ï¼Œæœ¬ç ”ç©¶æ¢ç´¢äº†ä¸¤ç§RLæ–¹æ³•ï¼šåŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰å’Œæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDDPGï¼‰ï¼Œä»¥å…‹æœå››æ—‹ç¿¼æ—‹ç¿¼å¤±æ•ˆå¸¦æ¥çš„æŒ‘æˆ˜ã€‚DPæ˜¯ä¸€ç§åŸºäºæ¨¡å‹çš„æ–¹æ³•ï¼Œå°½ç®¡è®¡ç®—é‡å¤§ï¼Œä½†å…·æœ‰æ”¶æ•›ä¿è¯ï¼›è€ŒDDPGæ˜¯ä¸€ç§æ— æ¨¡å‹æŠ€æœ¯ï¼Œè®¡ç®—é€Ÿåº¦å¿«ï¼Œä½†å¯¹è§£çš„æŒç»­æ—¶é—´æœ‰é™åˆ¶ã€‚ç ”ç©¶çš„æŒ‘æˆ˜åœ¨äºåœ¨é«˜ç»´åº¦å’ŒåŠ¨ä½œç©ºé—´ä¸Šè®­ç»ƒRLç®—æ³•ã€‚é€šè¿‡å¯¹ç°æœ‰DPå’ŒDDPGç®—æ³•çš„ä¿®æ”¹ï¼Œæ§åˆ¶å™¨ä¸ä»…èƒ½å¤Ÿå¤„ç†å¤§å‹è¿ç»­çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ï¼Œè€Œä¸”èƒ½å¤Ÿåœ¨é£è¡Œä¸­èºæ—‹æ¡¨å¤±æ•ˆåè¾¾åˆ°æœŸæœ›çŠ¶æ€ã€‚ä¸ºäº†éªŒè¯æ‰€æå‡ºçš„æ§åˆ¶æ¡†æ¶çš„é²æ£’æ€§ï¼Œåœ¨MATLABç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›çš„ä»¿çœŸï¼Œæ¶µç›–äº†å„ç§åˆå§‹æ¡ä»¶ï¼Œçªå‡ºäº†å…¶åœ¨ä»»åŠ¡å…³é”®å‹å››æ—‹ç¿¼åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚å¯¹ä¸¤ç§RLç®—æ³•åŠå…¶åœ¨æ•…éšœèˆªç©ºç³»ç»Ÿä¸­çš„åº”ç”¨æ½œåŠ›è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å››æ—‹ç¿¼é£è¡Œå™¨åœ¨å•æ—‹ç¿¼å¤±æ•ˆæƒ…å†µä¸‹çš„å®¹é”™æ§åˆ¶é—®é¢˜ã€‚ç°æœ‰æ§åˆ¶æ–¹æ³•åœ¨é¢å¯¹æ­¤ç±»çªå‘æ•…éšœæ—¶ï¼Œå¾€å¾€éš¾ä»¥ç»´æŒé£è¡Œå™¨çš„ç¨³å®šæ€§å’ŒæœŸæœ›é«˜åº¦ï¼Œä»è€Œå¯èƒ½å¯¼è‡´ç¡¬ä»¶æŸåæˆ–ä»»åŠ¡å¤±è´¥ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–ç²¾ç¡®çš„ç³»ç»Ÿæ¨¡å‹ï¼Œè€Œæ—‹ç¿¼å¤±æ•ˆåçš„æ¨¡å‹éš¾ä»¥å‡†ç¡®è·å–ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½å¤Ÿè‡ªé€‚åº”å­¦ä¹ å¹¶è¿›è¡Œæœ‰æ•ˆæ§åˆ¶çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•ï¼Œä½¿å››æ—‹ç¿¼æ§åˆ¶å™¨èƒ½å¤Ÿé€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’å­¦ä¹ ï¼Œåœ¨å•æ—‹ç¿¼å¤±æ•ˆåè‡ªåŠ¨è°ƒæ•´æ§åˆ¶ç­–ç•¥ï¼Œä»è€Œæ¢å¤å¹¶ä¿æŒæœŸæœ›çš„é£è¡ŒçŠ¶æ€ã€‚é€‰æ‹©å¼ºåŒ–å­¦ä¹ æ˜¯å› ä¸ºå…¶å…·æœ‰æ— éœ€ç²¾ç¡®æ¨¡å‹ã€èƒ½å¤Ÿå¤„ç†å¤æ‚éçº¿æ€§ç³»ç»Ÿçš„ä¼˜ç‚¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) å››æ—‹ç¿¼åŠ¨åŠ›å­¦å»ºæ¨¡ï¼Œç”¨äºä»¿çœŸç¯å¢ƒçš„æ„å»ºï¼›2) å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©ä¸æ”¹è¿›ï¼ŒåŒ…æ‹¬åŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰å’Œæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDDPGï¼‰ï¼›3) å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼Œç”¨äºå¼•å¯¼RLç®—æ³•å­¦ä¹ æœŸæœ›çš„æ§åˆ¶ç­–ç•¥ï¼›4) ä»¿çœŸç¯å¢ƒçš„æ­å»ºä¸è®­ç»ƒï¼Œä»¥åŠæ€§èƒ½è¯„ä¼°ã€‚DPä½œä¸ºä¸€ç§åŸºäºæ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨äºæä¾›æ”¶æ•›ä¿è¯ï¼Œè€ŒDDPGä½œä¸ºä¸€ç§æ— æ¨¡å‹æ–¹æ³•ï¼Œç”¨äºåŠ é€Ÿè®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºé’ˆå¯¹å››æ—‹ç¿¼å•æ—‹ç¿¼å¤±æ•ˆåœºæ™¯ï¼Œå¯¹ä¼ ç»Ÿçš„DPå’ŒDDPGç®—æ³•è¿›è¡Œäº†æ”¹è¿›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”é«˜ç»´åº¦è¿ç»­çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†åˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Œå¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ åœ¨æ•…éšœå‘ç”Ÿåå¿«é€Ÿæ¢å¤åˆ°æœŸæœ›çŠ¶æ€çš„æ§åˆ¶ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œå®ƒç›´æ¥å½±å“ç€RLç®—æ³•çš„å­¦ä¹ æ•ˆæœã€‚å¥–åŠ±å‡½æ•°é€šå¸¸åŒ…å«å¤šä¸ªéƒ¨åˆ†ï¼Œä¾‹å¦‚ä¸æœŸæœ›é«˜åº¦çš„åå·®ã€æ§åˆ¶è¾“å…¥çš„æƒ©ç½šé¡¹ç­‰ã€‚æ­¤å¤–ï¼Œå¯¹äºDDPGç®—æ³•ï¼Œç½‘ç»œç»“æ„çš„é€‰æ‹©å’Œè¶…å‚æ•°çš„è°ƒæ•´ä¹Ÿéœ€è¦ä»”ç»†è€ƒè™‘ï¼Œä»¥ä¿è¯ç®—æ³•çš„æ”¶æ•›æ€§å’Œæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

é€šè¿‡MATLABä»¿çœŸå®éªŒï¼ŒéªŒè¯äº†æ‰€æå‡ºçš„åŸºäºDPå’ŒDDPGçš„å®¹é”™æ§åˆ¶æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ”¹è¿›åçš„DPå’ŒDDPGç®—æ³•èƒ½å¤Ÿåœ¨å•æ—‹ç¿¼å¤±æ•ˆåï¼Œå¿«é€Ÿæ¢å¤å››æ—‹ç¿¼çš„æœŸæœ›é«˜åº¦ï¼Œå¹¶åœ¨ä¸åŒåˆå§‹æ¡ä»¶ä¸‹ä¿æŒç¨³å®šé£è¡Œã€‚ä¸¤ç§ç®—æ³•å„æœ‰ä¼˜åŠ£ï¼ŒDPç®—æ³•å…·æœ‰æ”¶æ•›ä¿è¯ï¼Œä½†è®¡ç®—é‡è¾ƒå¤§ï¼›DDPGç®—æ³•è®¡ç®—é€Ÿåº¦å¿«ï¼Œä½†å¯¹è¶…å‚æ•°æ•æ„Ÿã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ— äººæœºç‰©æµã€å·¡æ£€ã€æœæ•‘ç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜å¯é æ€§å’Œå®‰å…¨æ€§çš„ä»»åŠ¡ä¸­ã€‚é€šè¿‡æé«˜å››æ—‹ç¿¼é£è¡Œå™¨åœ¨çªå‘æ•…éšœä¸‹çš„å®¹é”™èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆé™ä½äº‹æ•…é£é™©ï¼Œä¿éšœä»»åŠ¡é¡ºåˆ©å®Œæˆï¼Œå¹¶æ‰©å±•å››æ—‹ç¿¼çš„åº”ç”¨èŒƒå›´ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„å¤šæ—‹ç¿¼é£è¡Œå™¨å’Œæœºå™¨äººç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This study presents a novel reinforcement learning (RL)-based control framework aimed at enhancing the safety and robustness of the quadcopter, with a specific focus on resilience to in-flight one propeller failure. Addressing the critical need of a robust control strategy for maintaining a desired altitude for the quadcopter to safe the hardware and the payload in physical applications. The proposed framework investigates two RL methodologies Dynamic Programming (DP) and Deep Deterministic Policy Gradient (DDPG), to overcome the challenges posed by the rotor failure mechanism of the quadcopter. DP, a model-based approach, is leveraged for its convergence guarantees, despite high computational demands, whereas DDPG, a model-free technique, facilitates rapid computation but with constraints on solution duration. The research challenge arises from training RL algorithms on large dimensions and action domains. With modifications to the existing DP and DDPG algorithms, the controllers were trained not only to cater for large continuous state and action domain and also achieve a desired state after an inflight propeller failure. To verify the robustness of the proposed control framework, extensive simulations were conducted in a MATLAB environment across various initial conditions and underscoring its viability for mission-critical quadcopter applications. A comparative analysis was performed between both RL algorithms and their potential for applications in faulty aerial systems.

