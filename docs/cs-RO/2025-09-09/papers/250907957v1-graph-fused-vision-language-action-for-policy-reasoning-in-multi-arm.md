---
layout: default
title: Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation
---

# Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07957" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07957v1</a>
  <a href="https://arxiv.org/pdf/2509.07957.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07957v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07957v1', 'Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shunlei Li, Longsen Gao, Jiuwen Cao, Yingbai Hu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

**å¤‡æ³¨**: This paper is submitted to IEEE IROS 2025 Workshop AIR4S

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGraph-Fused VLAæ¡†æ¶ï¼Œè§£å†³åŒè‡‚æœºå™¨äººä»äººç±»æ¼”ç¤ºä¸­å­¦ä¹ å¤æ‚æ“ä½œç­–ç•¥çš„é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒè‡‚æœºå™¨äºº` `æ¨¡ä»¿å­¦ä¹ ` `è§†è§‰è¯­è¨€åŠ¨ä½œ` `åœºæ™¯å›¾` `ä»»åŠ¡è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿæœºå™¨äººæŠ€èƒ½å­¦ä¹ ä¾èµ–ä½çº§è½¨è¿¹å¤åˆ¶ï¼Œéš¾ä»¥æ³›åŒ–åˆ°ä¸åŒç‰©ä½“ã€å¸ƒå±€å’Œæœºå™¨äººé…ç½®ã€‚
2. GF-VLAæ¡†æ¶é€šè¿‡ä¿¡æ¯è®ºæå–å…³é”®äº¤äº’çº¿ç´¢ï¼Œæ„å»ºåœºæ™¯å›¾ï¼Œå¹¶ç»“åˆè¯­è¨€æ¡ä»¶Transformerç”Ÿæˆåˆ†å±‚è¡Œä¸ºæ ‘ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGF-VLAåœ¨åŒè‡‚æœºå™¨äººä»»åŠ¡ä¸­å®ç°äº†é«˜å‡†ç¡®ç‡å’ŒæˆåŠŸç‡ï¼Œå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGraph-Fused Vision-Language-Action (GF-VLA) çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿åŒè‡‚æœºå™¨äººç³»ç»Ÿèƒ½å¤Ÿç›´æ¥ä»RGB-Däººç±»æ¼”ç¤ºä¸­è¿›è¡Œä»»åŠ¡çº§æ¨ç†å’Œæ‰§è¡Œã€‚GF-VLAé‡‡ç”¨ä¿¡æ¯è®ºæ–¹æ³•æå–ä»»åŠ¡ç›¸å…³çº¿ç´¢ï¼Œé€‰æ‹©æ€§åœ°çªå‡ºå…³é”®çš„æ‰‹-ç‰©å’Œç‰©-ç‰©äº¤äº’ã€‚è¿™äº›çº¿ç´¢è¢«æ„å»ºæˆæ—¶é—´æ’åºçš„åœºæ™¯å›¾ï¼Œç„¶åä¸è¯­è¨€æ¡ä»¶Transformeré›†æˆï¼Œä»¥ç”Ÿæˆåˆ†å±‚è¡Œä¸ºæ ‘å’Œå¯è§£é‡Šçš„ç¬›å¡å°”è¿åŠ¨åŸè¯­ã€‚ä¸ºäº†æé«˜åŒæ‰‹åŠ¨æ‰§è¡Œçš„æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨è‡‚åˆ†é…ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥è‡ªåŠ¨ç¡®å®šå¤¹å…·åˆ†é…ï¼Œè€Œæ— éœ€æ˜¾å¼çš„å‡ ä½•å»ºæ¨¡ã€‚åœ¨æ¶‰åŠç¬¦å·ç»“æ„æ„å»ºå’Œç©ºé—´æ³›åŒ–çš„å››ä¸ªåŒè‡‚ç§¯æœ¨ç»„è£…åŸºå‡†ä¸ŠéªŒè¯äº†GF-VLAã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è¡¨ç¤ºå®ç°äº†è¶…è¿‡95%çš„å›¾å‡†ç¡®ç‡å’Œ93%çš„å­ä»»åŠ¡åˆ†å‰²å‡†ç¡®ç‡ï¼Œä½¿è¯­è¨€-åŠ¨ä½œè§„åˆ’å™¨èƒ½å¤Ÿç”Ÿæˆé²æ£’ã€å¯è§£é‡Šçš„ä»»åŠ¡ç­–ç•¥ã€‚å½“éƒ¨ç½²åœ¨åŒè‡‚æœºå™¨äººä¸Šæ—¶ï¼Œè¿™äº›ç­–ç•¥åœ¨å †å ã€å­—æ¯å½¢æˆå’Œå‡ ä½•é‡æ„ä»»åŠ¡ä¸­å®ç°äº†94%çš„æŠ“å–å¯é æ€§ã€89%çš„æ”¾ç½®å‡†ç¡®æ€§å’Œ90%çš„æ€»ä½“ä»»åŠ¡æˆåŠŸç‡ï¼Œè¯æ˜äº†åœ¨å„ç§ç©ºé—´å’Œè¯­ä¹‰å˜åŒ–ä¸‹çš„å¼ºå¤§æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººä»äººç±»æ¼”ç¤ºä¸­å­¦ä¹ æŠ€èƒ½çš„æ–¹æ³•ï¼Œé€šå¸¸ä¾èµ–äºä½å±‚æ¬¡çš„è½¨è¿¹å¤ç°ï¼Œè¿™å¯¼è‡´äº†åœ¨é¢å¯¹ä¸åŒçš„ç‰©ä½“ã€ç©ºé—´å¸ƒå±€ä»¥åŠæœºå™¨äººé…ç½®æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚å°¤å…¶æ˜¯åœ¨åŒè‡‚æœºå™¨äººæ“ä½œä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è¿›è¡Œä»»åŠ¡åˆ†è§£ã€åŠ¨ä½œè§„åˆ’ä»¥åŠåŒè‡‚ååŒæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰ä¿¡æ¯ã€è¯­è¨€ä¿¡æ¯å’ŒåŠ¨ä½œä¿¡æ¯èåˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ï¼Œåˆ©ç”¨ä¿¡æ¯è®ºæ–¹æ³•æå–ä»»åŠ¡ç›¸å…³çš„å…³é”®çº¿ç´¢ï¼Œå¹¶å°†å…¶è¡¨ç¤ºä¸ºæ—¶åºåœºæ™¯å›¾ã€‚ç„¶åï¼Œåˆ©ç”¨è¯­è¨€æ¡ä»¶Transformerå°†åœºæ™¯å›¾è½¬åŒ–ä¸ºåˆ†å±‚çš„è¡Œä¸ºæ ‘å’Œå¯è§£é‡Šçš„è¿åŠ¨åŸè¯­ï¼Œä»è€Œå®ç°ä»»åŠ¡çº§çš„æ¨ç†å’Œæ‰§è¡Œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGF-VLAæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†è§‰ä¿¡æ¯æå–æ¨¡å—ï¼Œåˆ©ç”¨RGB-Då›¾åƒæå–åœºæ™¯ä¸­çš„ç‰©ä½“å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼›2) ä¿¡æ¯è®ºçº¿ç´¢æå–æ¨¡å—ï¼Œé€šè¿‡ä¿¡æ¯è®ºæ–¹æ³•é€‰æ‹©æ€§åœ°çªå‡ºå…³é”®çš„æ‰‹-ç‰©å’Œç‰©-ç‰©äº¤äº’ï¼›3) åœºæ™¯å›¾æ„å»ºæ¨¡å—ï¼Œå°†æå–çš„çº¿ç´¢æ„å»ºæˆæ—¶é—´æ’åºçš„åœºæ™¯å›¾ï¼›4) è¯­è¨€æ¡ä»¶Transformeræ¨¡å—ï¼Œå°†åœºæ™¯å›¾å’Œè¯­è¨€æŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆåˆ†å±‚çš„è¡Œä¸ºæ ‘å’Œè¿åŠ¨åŸè¯­ï¼›5) è·¨è‡‚åˆ†é…ç­–ç•¥æ¨¡å—ï¼Œè‡ªåŠ¨ç¡®å®šå¤¹å…·åˆ†é…ï¼Œæ— éœ€æ˜¾å¼çš„å‡ ä½•å»ºæ¨¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§åŸºäºä¿¡æ¯è®ºçš„çº¿ç´¢æå–æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æå–ä»»åŠ¡ç›¸å…³çš„å…³é”®äº¤äº’ä¿¡æ¯ï¼›2) å°†è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¿¡æ¯èåˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ï¼Œå®ç°äº†ä»»åŠ¡çº§çš„æ¨ç†å’Œæ‰§è¡Œï¼›3) æå‡ºäº†ä¸€ç§è·¨è‡‚åˆ†é…ç­–ç•¥ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç¡®å®šå¤¹å…·åˆ†é…ï¼Œæé«˜äº†åŒè‡‚æ“ä½œçš„æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†Transformerç½‘ç»œè¿›è¡Œåºåˆ—å»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨è¯­è¨€æŒ‡ä»¤ä½œä¸ºæ¡ä»¶æ¥æŒ‡å¯¼è¡Œä¸ºæ ‘çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œä¿¡æ¯è®ºçº¿ç´¢æå–æ¨¡å—çš„å…·ä½“å®ç°ç»†èŠ‚ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨çš„ä¿¡æ¯è®ºæŒ‡æ ‡ã€é˜ˆå€¼è®¾ç½®ç­‰ï¼‰ä»¥åŠè·¨è‡‚åˆ†é…ç­–ç•¥çš„å…·ä½“ç®—æ³•æ˜¯å…³é”®çš„è®¾è®¡ç»†èŠ‚ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæåŠã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGF-VLAåœ¨å››ä¸ªåŒè‡‚ç§¯æœ¨ç»„è£…åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚è¯¥æ–¹æ³•å®ç°äº†è¶…è¿‡95%çš„å›¾å‡†ç¡®ç‡å’Œ93%çš„å­ä»»åŠ¡åˆ†å‰²å‡†ç¡®ç‡ã€‚åœ¨å®é™…çš„åŒè‡‚æœºå™¨äººéƒ¨ç½²ä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†94%çš„æŠ“å–å¯é æ€§ã€89%çš„æ”¾ç½®å‡†ç¡®æ€§å’Œ90%çš„æ€»ä½“ä»»åŠ¡æˆåŠŸç‡ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒç©ºé—´å’Œè¯­ä¹‰å˜åŒ–ä¸‹çš„å¼ºå¤§æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨åŒ–è£…é…ã€æ™ºèƒ½åˆ¶é€ ã€åŒ»ç–—æœºå™¨äººç­‰é¢†åŸŸã€‚é€šè¿‡å­¦ä¹ äººç±»çš„æ“ä½œæ¼”ç¤ºï¼Œæœºå™¨äººèƒ½å¤Ÿå®Œæˆå¤æ‚çš„è£…é…ä»»åŠ¡ï¼Œæé«˜ç”Ÿäº§æ•ˆç‡å’Œçµæ´»æ€§ã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼Œè¯¥æŠ€æœ¯å¯ç”¨äºè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œæ‰‹æœ¯æ“ä½œï¼Œæé«˜æ‰‹æœ¯ç²¾åº¦å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›å®ç°æ›´é«˜çº§åˆ«çš„è‡ªä¸»æ“ä½œå’Œäººæœºåä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Acquiring dexterous robotic skills from human video demonstrations remains a significant challenge, largely due to conventional reliance on low-level trajectory replication, which often fails to generalize across varying objects, spatial layouts, and manipulator configurations. To address this limitation, we introduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB-D human demonstrations. GF-VLA employs an information-theoretic approach to extract task-relevant cues, selectively highlighting critical hand-object and object-object interactions. These cues are structured into temporally ordered scene graphs, which are subsequently integrated with a language-conditioned transformer to produce hierarchical behavior trees and interpretable Cartesian motion primitives. To enhance efficiency in bimanual execution, we propose a cross-arm allocation strategy that autonomously determines gripper assignment without requiring explicit geometric modeling. We validate GF-VLA on four dual-arm block assembly benchmarks involving symbolic structure construction and spatial generalization. Empirical results demonstrate that the proposed representation achieves over 95% graph accuracy and 93% subtask segmentation, enabling the language-action planner to generate robust, interpretable task policies. When deployed on a dual-arm robot, these policies attain 94% grasp reliability, 89% placement accuracy, and 90% overall task success across stacking, letter-formation, and geometric reconfiguration tasks, evidencing strong generalization and robustness under diverse spatial and semantic variations.

