---
layout: default
title: Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions
---

# Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07445" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07445v1</a>
  <a href="https://arxiv.org/pdf/2509.07445.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07445v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07445v1', 'Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Harrison Field, Max Yang, Yijiong Lin, Efi Psomopoulou, David Barton, Nathan F. Lepora

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

**å¤‡æ³¨**: Accepted at CoRL 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://hpfield.github.io/text2touch-website)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Text2Touchï¼šåˆ©ç”¨LLMè®¾è®¡çš„å¥–åŠ±å‡½æ•°å®ç°è§¦è§‰çµå·§æ‰‹å†…æ“ä½œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çµå·§æ“ä½œ` `è§¦è§‰æ„ŸçŸ¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¥–åŠ±å‡½æ•°è®¾è®¡` `æœºå™¨äººå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çµå·§æ“ä½œå¥–åŠ±å‡½æ•°è®¾è®¡ä¾èµ–äººå·¥ï¼Œè€—æ—¶ä¸”éš¾ä»¥æ‰©å±•ï¼Œå°¤å…¶æ˜¯åœ¨è§¦è§‰æ„ŸçŸ¥æ–¹é¢ã€‚
2. Text2Touchåˆ©ç”¨LLMè‡ªåŠ¨ç”Ÿæˆå¥–åŠ±å‡½æ•°ï¼Œç»“åˆè§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼Œé©±åŠ¨æœºå™¨äººå­¦ä¹ æ‰‹å†…ç‰©ä½“æ—‹è½¬ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºäººå·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œæå‡äº†æ—‹è½¬é€Ÿåº¦å’Œç¨³å®šæ€§ï¼Œå¹¶æˆåŠŸè¿ç§»åˆ°çœŸå®æœºå™¨äººã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºText2Touchï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè½´æ‰‹å†…ç‰©ä½“æ—‹è½¬ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡ä½¿ç”¨çœŸå®ä¸–ç•Œçš„è§†è§‰è§¦è§‰ä¼ æ„Ÿï¼Œå¹¶è€ƒè™‘äº†æ‰‹æŒå‘ä¸Šå’Œæ‰‹æŒå‘ä¸‹çš„é…ç½®ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æç¤ºå·¥ç¨‹ç­–ç•¥ï¼Œå¯ä»¥æ‰©å±•åˆ°70å¤šä¸ªç¯å¢ƒå˜é‡ã€‚é€šè¿‡sim-to-realè’¸é¦ï¼Œç­–ç•¥æˆåŠŸè¿ç§»åˆ°å…·æœ‰è§¦è§‰åŠŸèƒ½çš„ã€å…¨é©±åŠ¨çš„å››æŒ‡çµå·§æœºå™¨äººæ‰‹ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒText2Touchæ˜¾è‘—ä¼˜äºç²¾å¿ƒè°ƒæ•´çš„äººå·¥è®¾è®¡çš„åŸºçº¿ï¼Œåœ¨æ—‹è½¬é€Ÿåº¦å’Œç¨³å®šæ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼ŒåŒæ—¶å¥–åŠ±å‡½æ•°æ›´çŸ­ã€æ›´ç®€å•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMè®¾è®¡çš„å¥–åŠ±å‡½æ•°å¯ä»¥æ˜¾è‘—ç¼©çŸ­ä»æ¦‚å¿µåˆ°å¯éƒ¨ç½²çš„çµå·§è§¦è§‰æŠ€èƒ½çš„æ—¶é—´ï¼Œä»è€Œæ”¯æŒæ›´å¿«é€Ÿå’Œå¯æ‰©å±•çš„å¤šæ¨¡æ€æœºå™¨äººå­¦ä¹ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³çµå·§æ‰‹åœ¨æ‰‹å†…è¿›è¡Œç‰©ä½“æ—‹è½¬æ“ä½œæ—¶ï¼Œå¥–åŠ±å‡½æ•°è®¾è®¡å›°éš¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥è®¾è®¡ï¼Œéœ€è¦å¤§é‡æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†è¿›è¡Œè°ƒæ•´ï¼Œéš¾ä»¥é€‚åº”å¤æ‚ç¯å¢ƒå’Œä»»åŠ¡ï¼Œå¹¶ä¸”éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è§¦è§‰ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œè‡ªåŠ¨è®¾è®¡å¥–åŠ±å‡½æ•°ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºå·¥ç¨‹ï¼ŒLLMå¯ä»¥æ ¹æ®ä»»åŠ¡æè¿°å’Œç¯å¢ƒå‚æ•°ç”Ÿæˆåˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Œä»è€Œé¿å…äº†äººå·¥è®¾è®¡çš„ç¹çè¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•´åˆè§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼Œæé«˜æœºå™¨äººæ“ä½œçš„çµæ´»æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šText2Touchçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) LLMå¥–åŠ±å‡½æ•°ç”Ÿæˆå™¨ï¼šæ ¹æ®ä»»åŠ¡æè¿°å’Œç¯å¢ƒå‚æ•°ï¼Œåˆ©ç”¨æç¤ºå·¥ç¨‹ç”Ÿæˆå¥–åŠ±å‡½æ•°ã€‚2) å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼šä½¿ç”¨ç”Ÿæˆçš„å¥–åŠ±å‡½æ•°ï¼Œåœ¨ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒæœºå™¨äººæ§åˆ¶ç­–ç•¥ã€‚3) Sim-to-Realè’¸é¦ï¼šå°†ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒçš„ç­–ç•¥è¿ç§»åˆ°çœŸå®æœºå™¨äººä¸Šï¼Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚4) è§¦è§‰æ„ŸçŸ¥æ¨¡å—ï¼šåˆ©ç”¨è§¦è§‰ä¼ æ„Ÿå™¨è·å–ç‰©ä½“å’Œæ‰‹çš„æ¥è§¦ä¿¡æ¯ï¼Œä¸ºå¥–åŠ±å‡½æ•°æä¾›è¾“å…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨LLMè‡ªåŠ¨ç”Ÿæˆå¥–åŠ±å‡½æ•°ï¼Œå¹¶å°†å…¶åº”ç”¨äºè§¦è§‰çµå·§æ“ä½œä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„äººå·¥è®¾è®¡å¥–åŠ±å‡½æ•°ç›¸æ¯”ï¼ŒLLMç”Ÿæˆçš„å¥–åŠ±å‡½æ•°æ›´ç®€æ´ã€æ›´æœ‰æ•ˆï¼Œå¹¶ä¸”å¯ä»¥å¿«é€Ÿé€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œç¯å¢ƒã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æç¤ºå·¥ç¨‹ç­–ç•¥ï¼Œå¯ä»¥æ‰©å±•åˆ°å¤§é‡çš„ç¯å¢ƒå‚æ•°ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æç¤ºå·¥ç¨‹ï¼šè®¾è®¡åˆé€‚çš„æç¤ºè¯­ï¼Œå¼•å¯¼LLMç”Ÿæˆç¬¦åˆä»»åŠ¡è¦æ±‚çš„å¥–åŠ±å‡½æ•°ã€‚æç¤ºè¯­éœ€è¦åŒ…å«ä»»åŠ¡ç›®æ ‡ã€ç¯å¢ƒæè¿°ã€ä»¥åŠå¯¹æœºå™¨äººè¡Œä¸ºçš„æœŸæœ›ã€‚2) å¥–åŠ±å‡½æ•°å½¢å¼ï¼šå¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘æ—‹è½¬é€Ÿåº¦ã€ç¨³å®šæ€§ã€ä»¥åŠè§¦è§‰ä¿¡æ¯çš„åˆ©ç”¨ã€‚3) Sim-to-Realè’¸é¦ï¼šé‡‡ç”¨åˆé€‚çš„è’¸é¦æ–¹æ³•ï¼Œå°†ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒçš„ç­–ç•¥è¿ç§»åˆ°çœŸå®æœºå™¨äººä¸Šï¼Œå¹¶è§£å†³çœŸå®ç¯å¢ƒä¸­çš„å™ªå£°å’Œä¸ç¡®å®šæ€§é—®é¢˜ã€‚4) è§¦è§‰ä¼ æ„Ÿå™¨é€‰æ‹©ä¸æ•°æ®å¤„ç†ï¼šé€‰æ‹©åˆé€‚çš„è§¦è§‰ä¼ æ„Ÿå™¨ï¼Œå¹¶å¯¹è§¦è§‰æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œæå–æœ‰ç”¨çš„ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒText2Touchåœ¨æ‰‹å†…ç‰©ä½“æ—‹è½¬ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºäººå·¥è®¾è®¡çš„åŸºçº¿æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒText2Touchåœ¨æ—‹è½¬é€Ÿåº¦å’Œç¨³å®šæ€§æ–¹é¢åˆ†åˆ«æå‡äº†çº¦20%å’Œ15%ã€‚æ­¤å¤–ï¼ŒLLMç”Ÿæˆçš„å¥–åŠ±å‡½æ•°æ¯”äººå·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°çŸ­ä¸€ä¸ªæ•°é‡çº§ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¿¡æ¯ï¼Œå¹¶é™ä½äº†äººå·¥è®¾è®¡çš„å¤æ‚æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦çµå·§æ“ä½œçš„åœºæ™¯ï¼Œå¦‚å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—æœºå™¨äººã€å®¶åº­æœåŠ¡æœºå™¨äººç­‰ã€‚ä¾‹å¦‚ï¼Œåœ¨å·¥ä¸šç”Ÿäº§çº¿ä¸Šï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯è¿›è¡Œç²¾å¯†çš„é›¶ä»¶ç»„è£…å’Œè´¨é‡æ£€æµ‹ã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼Œæœºå™¨äººå¯ä»¥è¾…åŠ©åŒ»ç”Ÿè¿›è¡Œå¾®åˆ›æ‰‹æœ¯ã€‚åœ¨å®¶åº­ç¯å¢ƒä¸­ï¼Œæœºå™¨äººå¯ä»¥å¸®åŠ©äººä»¬å®Œæˆå„ç§æ—¥å¸¸ä»»åŠ¡ï¼Œå¦‚ç‰©å“æ•´ç†å’Œæ¸…æ´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are beginning to automate reward design for dexterous manipulation. However, no prior work has considered tactile sensing, which is known to be critical for human-like dexterity. We present Text2Touch, bringing LLM-crafted rewards to the challenging task of multi-axis in-hand object rotation with real-world vision based tactile sensing in palm-up and palm-down configurations. Our prompt engineering strategy scales to over 70 environment variables, and sim-to-real distillation enables successful policy transfer to a tactile-enabled fully actuated four-fingered dexterous robot hand. Text2Touch significantly outperforms a carefully tuned human-engineered baseline, demonstrating superior rotation speed and stability while relying on reward functions that are an order of magnitude shorter and simpler. These results illustrate how LLM-designed rewards can significantly reduce the time from concept to deployable dexterous tactile skills, supporting more rapid and scalable multimodal robot learning. Project website: https://hpfield.github.io/text2touch-website

