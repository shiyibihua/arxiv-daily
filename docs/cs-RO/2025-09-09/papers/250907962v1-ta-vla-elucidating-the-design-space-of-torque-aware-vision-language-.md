---
layout: default
title: TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models
---

# TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07962" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07962v1</a>
  <a href="https://arxiv.org/pdf/2509.07962.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07962v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07962v1', 'TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zongzheng Zhang, Haobo Xu, Zhuo Yang, Chenghao Yue, Zehao Lin, Huan-ang Gao, Ziwei Wang, Hao Zhao

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

**å¤‡æ³¨**: Accepted to CoRL 2025, project page: \url{https://zzongzheng0918.github.io/Torque-Aware-VLA.github.io/}

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ‰­çŸ©æ„ŸçŸ¥è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(TA-VLA)ï¼Œæå‡æœºå™¨äººæ“ä½œä¸­åŠ›è§‰åé¦ˆçš„åˆ©ç”¨ç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººæ“ä½œ` `åŠ›è§‰åé¦ˆ` `æ‰­çŸ©æ„ŸçŸ¥` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹éš¾ä»¥æœ‰æ•ˆæ•´åˆåŠ›è§‰åé¦ˆï¼ˆå¦‚æ‰­çŸ©ï¼‰ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. é€šè¿‡ç³»ç»Ÿç ”ç©¶æ‰­çŸ©ä¿¡å·åœ¨VLAæ¶æ„ä¸­çš„é›†æˆæ–¹å¼ï¼Œæå‡ºæ‰­çŸ©æ„ŸçŸ¥VLAæ¨¡å‹(TA-VLA)ã€‚
3. å®éªŒè¡¨æ˜ï¼Œåœ¨è§£ç å™¨ä¸­å¼•å…¥æ‰­çŸ©é€‚é…å™¨ï¼Œå¹¶é¢„æµ‹æ‰­çŸ©ä½œä¸ºè¾…åŠ©è¾“å‡ºï¼Œèƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šæœºå™¨äººæ“ä½œä»»åŠ¡éœ€è¦æ„ŸçŸ¥å’Œå“åº”åŠ›ä¿¡å·ï¼Œä¾‹å¦‚æ‰­çŸ©ï¼Œä»¥è¯„ä¼°ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆå¹¶å®ç°é—­ç¯æ§åˆ¶ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ç¼ºä¹æ•´åˆè¿™ç§ç»†å¾®ç‰©ç†åé¦ˆçš„èƒ½åŠ›ã€‚æœ¬æ–‡æ¢ç´¢äº†æ‰­çŸ©æ„ŸçŸ¥VLAæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶å°†æ‰­çŸ©ä¿¡å·æ•´åˆåˆ°ç°æœ‰VLAæ¶æ„ä¸­çš„è®¾è®¡ç©ºé—´æ¥å¼¥åˆè¿™ä¸€å·®è·ã€‚æˆ‘ä»¬è¯†åˆ«å¹¶è¯„ä¼°äº†å‡ ç§ç­–ç•¥ï¼Œå¾—å‡ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ã€‚é¦–å…ˆï¼Œå°†æ‰­çŸ©é€‚é…å™¨å¼•å…¥è§£ç å™¨å§‹ç»ˆä¼˜äºå°†å…¶æ’å…¥ç¼–ç å™¨ã€‚å…¶æ¬¡ï¼Œå—è‡ªåŠ¨é©¾é©¶ä¸­è”åˆé¢„æµ‹å’Œè§„åˆ’èŒƒå¼çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºé¢„æµ‹æ‰­çŸ©ä½œä¸ºè¾…åŠ©è¾“å‡ºï¼Œè¿™è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚è¿™ç§ç­–ç•¥é¼“åŠ±æ¨¡å‹æ„å»ºä¸€ä¸ªç‰©ç†ä¸Šæ‰æ ¹çš„äº¤äº’åŠ¨åŠ›å­¦å†…éƒ¨è¡¨ç¤ºã€‚é€šè¿‡ä¸°å¯Œçš„æ¥è§¦æ“ä½œåŸºå‡†çš„å¹¿æ³›å®šé‡å’Œå®šæ€§å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤„ç†éœ€è¦ç²¾ç»†åŠ›è§‰åé¦ˆçš„æœºå™¨äººæ“ä½œä»»åŠ¡æ—¶å­˜åœ¨ä¸è¶³ã€‚è¿™äº›ä»»åŠ¡é€šå¸¸ä¾èµ–äºæ‰­çŸ©ç­‰åŠ›ä¿¡å·æ¥åˆ¤æ–­ä»»åŠ¡å®Œæˆæƒ…å†µå’Œå®ç°é—­ç¯æ§åˆ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹æ— æ³•æœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›ç‰©ç†ä¿¡å·ï¼Œå¯¼è‡´æ“ä½œç²¾åº¦å’Œé²æ£’æ€§ä¸‹é™ã€‚å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨è§£å†³VLAæ¨¡å‹ç¼ºä¹åŠ›è§‰æ„ŸçŸ¥èƒ½åŠ›çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å°†æ‰­çŸ©ä¿¡å·æ˜¾å¼åœ°æ•´åˆåˆ°VLAæ¨¡å‹çš„æ¶æ„ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å’Œåˆ©ç”¨è¿™äº›ä¿¡å·æ¥æé«˜æ“ä½œæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡æ¢ç´¢äº†ä¸åŒçš„æ‰­çŸ©ä¿¡å·é›†æˆç­–ç•¥ï¼Œå¹¶å‘ç°å°†æ‰­çŸ©ä¿¡æ¯èå…¥è§£ç å™¨ä»¥åŠé¢„æµ‹æ‰­çŸ©ä½œä¸ºè¾…åŠ©ä»»åŠ¡èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•é¼“åŠ±æ¨¡å‹å­¦ä¹ ç‰©ç†ä¸Šåˆç†çš„äº¤äº’åŠ¨åŠ›å­¦è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTA-VLAæ¨¡å‹çš„æ•´ä½“æ¡†æ¶åŸºäºç°æœ‰çš„VLAæ¶æ„ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼•å…¥äº†æ‰­çŸ©æ„ŸçŸ¥æ¨¡å—ã€‚è¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) è§†è§‰ç¼–ç å™¨ï¼šç”¨äºæå–å›¾åƒç‰¹å¾ï¼›2) è¯­è¨€ç¼–ç å™¨ï¼šç”¨äºæå–è¯­è¨€æŒ‡ä»¤ç‰¹å¾ï¼›3) æ‰­çŸ©é€‚é…å™¨ï¼šç”¨äºå°†æ‰­çŸ©ä¿¡å·èå…¥è§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼›4) åŠ¨ä½œè§£ç å™¨ï¼šç”¨äºç”Ÿæˆæœºå™¨äººåŠ¨ä½œï¼›5) æ‰­çŸ©é¢„æµ‹æ¨¡å—ï¼šç”¨äºé¢„æµ‹æ‰­çŸ©ä¿¡å·ä½œä¸ºè¾…åŠ©è¾“å‡ºã€‚è®ºæ–‡é‡ç‚¹ç ”ç©¶äº†æ‰­çŸ©é€‚é…å™¨çš„ä½ç½®ï¼ˆç¼–ç å™¨ vs è§£ç å™¨ï¼‰å’Œæ‰­çŸ©é¢„æµ‹æ¨¡å—çš„ä½œç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç³»ç»Ÿåœ°æ¢ç´¢äº†æ‰­çŸ©ä¿¡å·åœ¨VLAæ¨¡å‹ä¸­çš„é›†æˆæ–¹å¼ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„é›†æˆç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡å‘ç°å°†æ‰­çŸ©é€‚é…å™¨æ”¾ç½®åœ¨è§£ç å™¨ä¸­èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æ‰­çŸ©ä¿¡æ¯ï¼Œå¹¶ä¸”é€šè¿‡é¢„æµ‹æ‰­çŸ©ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œå¯ä»¥æé«˜æ¨¡å‹å¯¹äº¤äº’åŠ¨åŠ›å­¦çš„ç†è§£ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é€šè¿‡å¤§é‡çš„å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ‰­çŸ©é€‚é…å™¨çš„ä½ç½®ï¼šå®éªŒå¯¹æ¯”äº†å°†æ‰­çŸ©é€‚é…å™¨æ”¾ç½®åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­çš„æ€§èƒ½å·®å¼‚ï¼Œå‘ç°æ”¾ç½®åœ¨è§£ç å™¨ä¸­æ•ˆæœæ›´å¥½ã€‚2) æ‰­çŸ©é¢„æµ‹æ¨¡å—ï¼šé€šè¿‡æ·»åŠ ä¸€ä¸ªé¢å¤–çš„æŸå¤±å‡½æ•°æ¥é¼“åŠ±æ¨¡å‹é¢„æµ‹æ‰­çŸ©ä¿¡å·ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹äº¤äº’åŠ¨åŠ›å­¦çš„ç†è§£ã€‚3) æŸå¤±å‡½æ•°ï¼šä½¿ç”¨äº†åŠ¨ä½œé¢„æµ‹æŸå¤±å’Œæ‰­çŸ©é¢„æµ‹æŸå¤±çš„åŠ æƒå’Œä½œä¸ºæ€»æŸå¤±å‡½æ•°ï¼Œé€šè¿‡è°ƒæ•´æƒé‡æ¥å¹³è¡¡ä¸¤ä¸ªä»»åŠ¡çš„é‡è¦æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°†æ‰­çŸ©é€‚é…å™¨å¼•å…¥è§£ç å™¨å§‹ç»ˆä¼˜äºå°†å…¶æ’å…¥ç¼–ç å™¨ã€‚æ­¤å¤–ï¼Œé€šè¿‡é¢„æµ‹æ‰­çŸ©ä½œä¸ºè¾…åŠ©è¾“å‡ºï¼Œæ¨¡å‹æ€§èƒ½å¾—åˆ°è¿›ä¸€æ­¥æå‡ã€‚åœ¨æ¥è§¦æ“ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTA-VLAæ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰VLAæ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦ç²¾ç»†åŠ›è§‰æ§åˆ¶çš„æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚è£…é…ã€æŠ“å–ã€æ‰“ç£¨ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹åŠ›è§‰åé¦ˆçš„åˆ©ç”¨ç‡ï¼Œå¯ä»¥æé«˜æ“ä½œçš„ç²¾åº¦ã€ç¨³å®šæ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºè‡ªåŠ¨åŒ–ç”Ÿäº§çº¿ã€åŒ»ç–—æœºå™¨äººã€æœåŠ¡æœºå™¨äººç­‰é¢†åŸŸï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´å¯é çš„æœºå™¨äººæ“ä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings.

