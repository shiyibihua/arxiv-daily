---
layout: default
title: Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning
---

# Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08126" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08126v1</a>
  <a href="https://arxiv.org/pdf/2509.08126.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08126v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08126v1', 'Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

**å¤‡æ³¨**: Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid Robots

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå±æ€§çš„å¯¹è±¡å®šä½ä¸æœºå™¨äººæŠ“å–æ¡†æ¶OGRGï¼Œè§£å†³å¤æ‚åœºæ™¯ä¸‹çš„è¯­è¨€æŒ‡å®šæŠ“å–é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æœºå™¨äººæŠ“å–` `è‡ªç„¶è¯­è¨€ç†è§£` `è§†è§‰è¯­è¨€èåˆ` `ç©ºé—´æ¨ç†` `å¼±ç›‘ç£å­¦ä¹ ` `å¯¹è±¡å®šä½` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†å¼€æ”¾å¼è¯­è¨€è¡¨è¾¾ï¼Œä¸”é€šå¸¸å‡è®¾ç›®æ ‡å¯¹è±¡æ˜¯å”¯ä¸€çš„ï¼Œå¿½ç•¥äº†é‡å¤å®ä¾‹ï¼Œé™åˆ¶äº†åº”ç”¨åœºæ™¯ã€‚
2. OGRGæ¡†æ¶é€šè¿‡åŒå‘è§†è§‰-è¯­è¨€èåˆå’Œæ·±åº¦ä¿¡æ¯é›†æˆï¼Œå¢å¼ºäº†å‡ ä½•æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæå‡äº†å®šä½å’ŒæŠ“å–æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒOGRGåœ¨å®šä½ç²¾åº¦å’ŒæŠ“å–æˆåŠŸç‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººå®éªŒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå±æ€§çš„å¯¹è±¡å®šä½ä¸æœºå™¨äººæŠ“å–æ¡†æ¶ï¼ˆOGRGï¼‰ï¼Œæ—¨åœ¨è§£å†³äººæœºäº¤äº’ä¸­é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡å®šæŠ“å–å¯¹è±¡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè§£æå¼€æ”¾å¼çš„è¯­è¨€è¡¨è¾¾ï¼Œå¹¶è¿›è¡Œç©ºé—´æ¨ç†ï¼Œä»è€Œåœ¨åŒ…å«é‡å¤å¯¹è±¡å®ä¾‹çš„åœºæ™¯ä¸­å®šä½ç›®æ ‡å¯¹è±¡å¹¶é¢„æµ‹å¹³é¢æŠ“å–å§¿æ€ã€‚OGRGåœ¨ä¸¤ç§è®¾ç½®ä¸‹è¿›è¡Œäº†ç ”ç©¶ï¼š(1)åƒç´ çº§å…¨ç›‘ç£ä¸‹çš„å‚è€ƒæŠ“å–åˆæˆï¼ˆRGSï¼‰ï¼›(2)ä»…ä½¿ç”¨å•åƒç´ æŠ“å–æ ‡æ³¨çš„å¼±ç›‘ç£å­¦ä¹ ä¸‹çš„å‚è€ƒæŠ“å–å¯ä¾›æ€§ï¼ˆRGAï¼‰ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬åŒå‘è§†è§‰-è¯­è¨€èåˆæ¨¡å—ä»¥åŠæ·±åº¦ä¿¡æ¯é›†æˆä»¥å¢å¼ºå‡ ä½•æ¨ç†ï¼Œä»è€Œæé«˜å®šä½å’ŒæŠ“å–æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOGRGåœ¨å…·æœ‰å¤šæ ·åŒ–ç©ºé—´è¯­è¨€æŒ‡ä»¤çš„æ¡Œé¢åœºæ™¯ä¸­ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ–¹æ³•ã€‚åœ¨RGSä¸­ï¼Œå®ƒåœ¨å•ä¸ªNVIDIA RTX 2080 Ti GPUä¸Šä»¥17.59 FPSè¿è¡Œï¼Œä½¿å…¶èƒ½å¤Ÿæ½œåœ¨åœ°ç”¨äºé—­ç¯æˆ–å¤šå¯¹è±¡é¡ºåºæŠ“å–ï¼ŒåŒæ—¶æä¾›ä¼˜äºæ‰€æœ‰è€ƒè™‘çš„åŸºçº¿çš„å®šä½å’ŒæŠ“å–é¢„æµ‹ç²¾åº¦ã€‚åœ¨å¼±ç›‘ç£RGAè®¾ç½®ä¸‹ï¼ŒOGRGåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººè¯•éªŒä¸­ä¹Ÿè¶…è¿‡äº†åŸºçº¿æŠ“å–æˆåŠŸç‡ï¼Œçªå‡ºäº†å…¶ç©ºé—´æ¨ç†è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤æŠ“å–ç‰¹å®šå¯¹è±¡çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨åœºæ™¯ä¸­å­˜åœ¨å¤šä¸ªç›¸åŒå¯¹è±¡å®ä¾‹çš„æƒ…å†µä¸‹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯†é›†çš„åƒç´ çº§æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”éš¾ä»¥å¤„ç†å¤æ‚çš„ã€å¼€æ”¾å¼çš„è¯­è¨€æŒ‡ä»¤ï¼Œä»¥åŠç¼ºä¹å¯¹åœºæ™¯ä¸­å¯¹è±¡é—´ç©ºé—´å…³ç³»çš„æœ‰æ•ˆæ¨ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰å±æ€§å’Œç©ºé—´æ¨ç†æ¥ç²¾ç¡®åœ°å®šä½ç›®æ ‡å¯¹è±¡ï¼Œå¹¶é¢„æµ‹åˆé€‚çš„æŠ“å–å§¿æ€ã€‚é€šè¿‡èåˆè§†è§‰ä¿¡æ¯å’Œè¯­è¨€ä¿¡æ¯ï¼Œå¹¶ç»“åˆæ·±åº¦ä¿¡æ¯è¿›è¡Œå‡ ä½•æ¨ç†ï¼Œä»è€Œå…‹æœç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œå¼€æ”¾å¼è¯­è¨€æŒ‡ä»¤æ–¹é¢çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOGRGæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š(1)è§†è§‰ç‰¹å¾æå–æ¨¡å—ï¼Œç”¨äºæå–åœºæ™¯çš„è§†è§‰ç‰¹å¾ï¼›(2)è¯­è¨€ç‰¹å¾æå–æ¨¡å—ï¼Œç”¨äºæå–è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„è¯­ä¹‰ç‰¹å¾ï¼›(3)åŒå‘è§†è§‰-è¯­è¨€èåˆæ¨¡å—ï¼Œç”¨äºå°†è§†è§‰ç‰¹å¾å’Œè¯­è¨€ç‰¹å¾è¿›è¡Œèåˆï¼Œä»è€Œå®ç°è§†è§‰ä¿¡æ¯å’Œè¯­è¨€ä¿¡æ¯çš„æœ‰æ•ˆäº¤äº’ï¼›(4)ç©ºé—´æ¨ç†æ¨¡å—ï¼Œåˆ©ç”¨æ·±åº¦ä¿¡æ¯è¿›è¡Œå‡ ä½•æ¨ç†ï¼Œå¢å¼ºå¯¹åœºæ™¯ä¸­å¯¹è±¡é—´ç©ºé—´å…³ç³»çš„ç†è§£ï¼›(5)æŠ“å–å§¿æ€é¢„æµ‹æ¨¡å—ï¼Œç”¨äºé¢„æµ‹ç›®æ ‡å¯¹è±¡çš„æŠ“å–å§¿æ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š(1)æå‡ºäº†åŒå‘è§†è§‰-è¯­è¨€èåˆæ¨¡å—ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°èåˆè§†è§‰ä¿¡æ¯å’Œè¯­è¨€ä¿¡æ¯ï¼›(2)é›†æˆäº†æ·±åº¦ä¿¡æ¯ï¼Œå¢å¼ºäº†å‡ ä½•æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†å®šä½å’ŒæŠ“å–æ€§èƒ½ï¼›(3)æå‡ºäº†å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä»…ä½¿ç”¨å•åƒç´ æŠ“å–æ ‡æ³¨ï¼Œé™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOGRGèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„åœºæ™¯å’Œå¼€æ”¾å¼çš„è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶ä¸”å…·æœ‰æ›´é«˜çš„å®šä½ç²¾åº¦å’ŒæŠ“å–æˆåŠŸç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåŒå‘è§†è§‰-è¯­è¨€èåˆæ¨¡å—é‡‡ç”¨Transformerç»“æ„ï¼Œå…è®¸è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ç›¸äº’å½±å“å’Œå¢å¼ºã€‚ç©ºé—´æ¨ç†æ¨¡å—åˆ©ç”¨æ·±åº¦å›¾è®¡ç®—ç‚¹äº‘ï¼Œå¹¶ä½¿ç”¨PointNetæå–å‡ ä½•ç‰¹å¾ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬å®šä½æŸå¤±å’ŒæŠ“å–æŸå¤±ï¼Œå…¶ä¸­å®šä½æŸå¤±é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼ŒæŠ“å–æŸå¤±é‡‡ç”¨Smooth L1æŸå¤±ã€‚åœ¨å¼±ç›‘ç£å­¦ä¹ ä¸­ï¼Œé‡‡ç”¨æœ€å¤§åŒ–æŠ“å–æˆåŠŸç‡çš„ç­–ç•¥è¿›è¡Œè®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOGRGåœ¨RGSè®¾ç½®ä¸‹ï¼Œåœ¨å•ä¸ªNVIDIA RTX 2080 Ti GPUä¸Šä»¥17.59 FPSè¿è¡Œï¼ŒåŒæ—¶åœ¨å®šä½ç²¾åº¦å’ŒæŠ“å–é¢„æµ‹ç²¾åº¦ä¸Šå‡ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚åœ¨å¼±ç›‘ç£RGAè®¾ç½®ä¸‹ï¼ŒOGRGåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººè¯•éªŒä¸­ä¹Ÿè¶…è¿‡äº†åŸºçº¿æŠ“å–æˆåŠŸç‡ï¼ŒéªŒè¯äº†å…¶ç©ºé—´æ¨ç†è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨çœŸå®æœºå™¨äººå®éªŒä¸­ï¼ŒOGRGçš„æŠ“å–æˆåŠŸç‡æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†æ˜¾è‘—æ¯”ä¾‹ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½åˆ¶é€ ã€å®¶åº­æœåŠ¡æœºå™¨äººã€ä»“å‚¨ç‰©æµç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½åˆ¶é€ ä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®å·¥äººçš„è¯­éŸ³æŒ‡ä»¤æŠ“å–ç‰¹å®šçš„é›¶ä»¶è¿›è¡Œç»„è£…ï¼›åœ¨å®¶åº­æœåŠ¡ä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®ç”¨æˆ·çš„è¯­è¨€æŒ‡ä»¤æŠ“å–ç‰©å“ï¼Œå¸®åŠ©ç”¨æˆ·å®Œæˆå®¶åŠ¡ï¼›åœ¨ä»“å‚¨ç‰©æµä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®æŒ‡ä»¤æŠ“å–è´§ç‰©è¿›è¡Œåˆ†æ‹£å’Œæ¬è¿ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæ•ˆç‡ï¼Œå®ç°æ›´æ™ºèƒ½åŒ–çš„æœºå™¨äººåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Enabling robots to grasp objects specified through natural language is essential for effective human-robot interaction, yet it remains a significant challenge. Existing approaches often struggle with open-form language expressions and typically assume unambiguous target objects without duplicates. Moreover, they frequently rely on costly, dense pixel-wise annotations for both object grounding and grasp configuration. We present Attribute-based Object Grounding and Robotic Grasping (OGRG), a novel framework that interprets open-form language expressions and performs spatial reasoning to ground target objects and predict planar grasp poses, even in scenes containing duplicated object instances. We investigate OGRG in two settings: (1) Referring Grasp Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp Affordance (RGA) using weakly supervised learning with only single-pixel grasp annotations. Key contributions include a bi-directional vision-language fusion module and the integration of depth information to enhance geometric reasoning, improving both grounding and grasping performance. Experiment results show that OGRG outperforms strong baselines in tabletop scenes with diverse spatial language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX 2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential grasping, while delivering superior grounding and grasp prediction accuracy compared to all the baselines considered. Under the weakly supervised RGA setting, OGRG also surpasses baseline grasp-success rates in both simulation and real-robot trials, underscoring the effectiveness of its spatial reasoning design. Project page: https://z.umn.edu/ogrg

