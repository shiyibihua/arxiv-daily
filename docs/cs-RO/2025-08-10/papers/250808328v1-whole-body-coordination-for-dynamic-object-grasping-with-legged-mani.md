---
layout: default
title: Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators
---

# Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08328" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08328v1</a>
  <a href="https://arxiv.org/pdf/2508.08328.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08328v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08328v1', 'Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qiwei Liang, Boyang Cai, Rongyi He, Hui Li, Tao Teng, Haihan Duan, Changxin Huang, Runhao Zeng

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDQ-Netä»¥è§£å†³åŠ¨æ€ç‰©ä½“æŠ“å–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŠ¨æ€æŠ“å–` `å››è¶³æœºå™¨äºº` `æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶` `åŸºå‡†è¯„ä¼°` `è¿åŠ¨è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºé™æ€ç‰©ä½“æŠ“å–ï¼Œå¿½è§†äº†åŠ¨æ€ç›®æ ‡çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†åœ¨åŠ¨æ€åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. æå‡ºDQ-BenchåŸºå‡†å’ŒDQ-Netæ¡†æ¶ï¼Œé€šè¿‡æ•™å¸ˆç½‘ç»œå’Œå­¦ç”Ÿç½‘ç»œçš„ååŒå·¥ä½œï¼Œæ¨æ–­æŠ“å–é…ç½®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDQ-Netåœ¨åŠ¨æ€ç‰©ä½“æŠ“å–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸç‡å’Œå“åº”é€Ÿåº¦æ˜¾è‘—æé«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å››è¶³æœºå™¨äººé…å¤‡æ“æ§å™¨åœ¨åŠ¨æ€ç¯å¢ƒä¸­å…·å¤‡å¼ºå¤§çš„ç§»åŠ¨æ€§å’Œé€‚åº”æ€§ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºé™æ€ç‰©ä½“æŠ“å–ï¼Œå¿½è§†äº†åŠ¨æ€ç›®æ ‡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†DQ-Benchï¼Œä¸€ä¸ªç³»ç»Ÿè¯„ä¼°åŠ¨æ€æŠ“å–çš„æ–°åŸºå‡†ï¼Œæ¶µç›–ä¸åŒç‰©ä½“è¿åŠ¨ã€é€Ÿåº¦ã€é«˜åº¦ã€ç±»å‹å’Œåœ°å½¢å¤æ‚æ€§ã€‚åŸºäºæ­¤åŸºå‡†ï¼Œæå‡ºäº†DQ-Netï¼Œä¸€ä¸ªç´§å‡‘çš„æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œæ—¨åœ¨ä»æœ‰é™çš„æ„ŸçŸ¥çº¿ç´¢ä¸­æ¨æ–­æŠ“å–é…ç½®ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒDQ-Netåœ¨å¤šä¸ªä»»åŠ¡è®¾ç½®ä¸­å®ç°äº†åŠ¨æ€ç‰©ä½“æŠ“å–çš„é²æ£’æ€§ï¼Œæ˜¾è‘—è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å››è¶³æœºå™¨äººåœ¨åŠ¨æ€ç¯å¢ƒä¸­æŠ“å–åŠ¨æ€ç‰©ä½“çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºé™æ€ç‰©ä½“æŠ“å–ï¼Œç¼ºä¹å¯¹åŠ¨æ€ç›®æ ‡çš„æœ‰æ•ˆå¤„ç†ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºDQ-BenchåŸºå‡†å’ŒDQ-Netæ¡†æ¶ï¼Œåˆ©ç”¨æ•™å¸ˆç½‘ç»œæä¾›çš„ç‰¹æƒä¿¡æ¯æ¥å…¨é¢å»ºæ¨¡ç›®æ ‡çš„å‡ ä½•ç‰¹æ€§å’ŒåŠ¨æ€è¿åŠ¨ç‰¹å¾ï¼ŒåŒæ—¶è®¾è®¡è½»é‡çº§çš„å­¦ç”Ÿç½‘ç»œä»¥å®ç°é—­ç¯åŠ¨ä½œè¾“å‡ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDQ-Netç”±æ•™å¸ˆç½‘ç»œå’Œå­¦ç”Ÿç½‘ç»œç»„æˆã€‚æ•™å¸ˆç½‘ç»œä½¿ç”¨ç‰¹æƒä¿¡æ¯è¿›è¡Œè®­ç»ƒï¼Œæ•´åˆæŠ“å–èåˆæ¨¡å—ä»¥æä¾›è¿åŠ¨è§„åˆ’çš„æŒ‡å¯¼ï¼›å­¦ç”Ÿç½‘ç»œåˆ™ä¾èµ–ç›®æ ‡æ©ç ã€æ·±åº¦å›¾å’Œè‡ªæˆ‘æ„ŸçŸ¥çŠ¶æ€è¿›è¡ŒåŒè§†è§’æ—¶é—´å»ºæ¨¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDQ-Benchä½œä¸ºæ–°åŸºå‡†ç³»ç»Ÿåœ°è¯„ä¼°åŠ¨æ€æŠ“å–ï¼ŒDQ-Neté€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶æœ‰æ•ˆæ¨æ–­æŠ“å–é…ç½®ï¼Œæ˜¾è‘—æå‡äº†åŠ¨æ€ç‰©ä½“æŠ“å–çš„æˆåŠŸç‡å’Œå“åº”é€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•™å¸ˆç½‘ç»œæ•´åˆäº†é™æ€å‡ ä½•å±æ€§å’ŒåŠ¨æ€è¿åŠ¨ç‰¹å¾ï¼Œå­¦ç”Ÿç½‘ç»œåˆ™è®¾è®¡ä¸ºè½»é‡çº§ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–ç‰¹æƒæ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„æŠ“å–å†³ç­–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨DQ-BenchåŸºå‡†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDQ-Netåœ¨åŠ¨æ€ç‰©ä½“æŠ“å–ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒæˆåŠŸç‡å’Œå“åº”é€Ÿåº¦å‡æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œä½†æå‡å¹…åº¦æ˜æ˜¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç‰©æµåˆ†æ‹£ã€äººä¸æœºå™¨äººåä½œç­‰åŠ¨æ€åœºæ™¯ã€‚é€šè¿‡æå‡å››è¶³æœºå™¨äººåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æŠ“å–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Quadrupedal robots with manipulators offer strong mobility and adaptability for grasping in unstructured, dynamic environments through coordinated whole-body control. However, existing research has predominantly focused on static-object grasping, neglecting the challenges posed by dynamic targets and thus limiting applicability in dynamic scenarios such as logistics sorting and human-robot collaboration. To address this, we introduce DQ-Bench, a new benchmark that systematically evaluates dynamic grasping across varying object motions, velocities, heights, object types, and terrain complexities, along with comprehensive evaluation metrics. Building upon this benchmark, we propose DQ-Net, a compact teacher-student framework designed to infer grasp configurations from limited perceptual cues. During training, the teacher network leverages privileged information to holistically model both the static geometric properties and dynamic motion characteristics of the target, and integrates a grasp fusion module to deliver robust guidance for motion planning. Concurrently, we design a lightweight student network that performs dual-viewpoint temporal modeling using only the target mask, depth map, and proprioceptive state, enabling closed-loop action outputs without reliance on privileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Net achieves robust dynamic objects grasping across multiple task settings, substantially outperforming baseline methods in both success rate and responsiveness.

