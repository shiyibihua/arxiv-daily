---
layout: default
title: 3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction
---

# 3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07182" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07182v1</a>
  <a href="https://arxiv.org/pdf/2508.07182.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07182v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07182v1', '3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuesong Li, Lars Petersson, Vivien Rolland

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º3Dé«˜æ–¯è¡¨ç¤ºä¸è¿åŠ¨è½¨è¿¹åœºä»¥è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `åŠ¨æ€åœºæ™¯é‡å»º` `è¿åŠ¨è½¨è¿¹` `3Dé«˜æ–¯è¡¨ç¤º` `å•ç›®è§†é¢‘` `æ–°è§†è§’åˆæˆ` `ç¥ç»è¾å°„åœº` `æœºå™¨äººè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯é‡å»ºä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¦‚ä½•æœ‰æ•ˆå¤„ç†å¤æ‚ç‰©ä½“è¿åŠ¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆ3Dé«˜æ–¯ç‚¹äº‘ä¸è¿åŠ¨è½¨è¿¹åœºçš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿç²¾ç¡®ä¼˜åŒ–åŠ¨æ€ç‰©ä½“çš„è¿åŠ¨è½¨è¿¹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ–°è§†è§’åˆæˆå’Œè¿åŠ¨è½¨è¿¹æ¢å¤ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œè¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹ä»å•ç›®è§†é¢‘ä¸­è¿›è¡ŒåŠ¨æ€åœºæ™¯çš„æ–°è§†è§’åˆæˆå’Œè¿åŠ¨é‡å»ºè¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ã€‚å°½ç®¡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰åœ¨é™æ€åœºæ™¯æ¸²æŸ“ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å°†å…¶æ‰©å±•åˆ°åŠ¨æ€åœºæ™¯é‡å»ºä»ç„¶é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†3DGSä¸è¿åŠ¨è½¨è¿¹åœºï¼Œèƒ½å¤Ÿç²¾ç¡®å¤„ç†å¤æ‚çš„ç‰©ä½“è¿åŠ¨ï¼Œå¹¶å®ç°ç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨è½¨è¿¹ã€‚é€šè¿‡å°†åŠ¨æ€ç‰©ä½“ä¸é™æ€èƒŒæ™¯è§£è€¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç´§å‡‘åœ°ä¼˜åŒ–äº†è¿åŠ¨è½¨è¿¹åœºï¼Œé‡‡ç”¨æ—¶é—´ä¸å˜çš„è¿åŠ¨ç³»æ•°å’Œå…±äº«çš„è¿åŠ¨è½¨è¿¹åŸºæ¥æ•æ‰å¤æ‚çš„è¿åŠ¨æ¨¡å¼ï¼ŒåŒæ—¶é™ä½ä¼˜åŒ–å¤æ‚æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–°è§†è§’åˆæˆå’Œå•ç›®è§†é¢‘çš„è¿åŠ¨è½¨è¿¹æ¢å¤æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ¨åŠ¨äº†åŠ¨æ€åœºæ™¯é‡å»ºçš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»å•ç›®è§†é¢‘ä¸­è¿›è¡ŒåŠ¨æ€åœºæ™¯é‡å»ºçš„éš¾é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç‰©ä½“è¿åŠ¨æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥å®ç°ç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨è½¨è¿¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç»“åˆ3Dé«˜æ–¯ç‚¹äº‘å’Œè¿åŠ¨è½¨è¿¹åœºï¼Œè§£è€¦åŠ¨æ€ç‰©ä½“ä¸é™æ€èƒŒæ™¯ï¼Œä»è€Œä¼˜åŒ–è¿åŠ¨è½¨è¿¹åœºï¼Œæ•æ‰å¤æ‚è¿åŠ¨æ¨¡å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€åŠ¨æ€ç‰©ä½“ä¸é™æ€èƒŒæ™¯çš„è§£è€¦ã€è¿åŠ¨è½¨è¿¹åœºçš„ä¼˜åŒ–ä»¥åŠæ–°è§†è§’åˆæˆç­‰ä¸»è¦æ¨¡å—ï¼Œç¡®ä¿äº†é«˜æ•ˆçš„åŠ¨æ€åœºæ™¯é‡å»ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ—¶é—´ä¸å˜çš„è¿åŠ¨ç³»æ•°å’Œå…±äº«çš„è¿åŠ¨è½¨è¿¹åŸºï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„åŠ¨æ€å¤„ç†æ–¹å¼å½¢æˆäº†æ˜¾è‘—åŒºåˆ«ï¼Œæå‡äº†è¿åŠ¨è½¨è¿¹çš„å‡†ç¡®æ€§å’Œä¼˜åŒ–æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæˆ‘ä»¬è®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡åŠ¨æ€ä¸é™æ€éƒ¨åˆ†çš„ä¼˜åŒ–ï¼ŒåŒæ—¶é‡‡ç”¨äº†é€‚åº”æ€§ç½‘ç»œç»“æ„ä»¥æé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæˆ‘ä»¬æœ‰æ•ˆé™ä½äº†ä¼˜åŒ–çš„å¤æ‚æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æ–°è§†è§’åˆæˆå’Œè¿åŠ¨è½¨è¿¹æ¢å¤æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œåˆæˆè´¨é‡æå‡äº†çº¦15%ï¼Œè¿åŠ¨è½¨è¿¹æ¢å¤ç²¾åº¦æé«˜äº†20%ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯é‡å»ºä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æœºå™¨äººè§†è§‰ã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡å®ç°é«˜è´¨é‡çš„åŠ¨æ€åœºæ™¯é‡å»ºï¼Œå¯ä»¥æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œä»¥åŠå¢å¼ºç°å®åº”ç”¨ä¸­çš„ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper addresses the challenge of novel-view synthesis and motion reconstruction of dynamic scenes from monocular video, which is critical for many robotic applications. Although Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering static scenes, extending them to reconstruct dynamic scenes remains challenging. In this work, we introduce a novel approach that combines 3DGS with a motion trajectory field, enabling precise handling of complex object motions and achieving physically plausible motion trajectories. By decoupling dynamic objects from static background, our method compactly optimizes the motion trajectory field. The approach incorporates time-invariant motion coefficients and shared motion trajectory bases to capture intricate motion patterns while minimizing optimization complexity. Extensive experiments demonstrate that our approach achieves state-of-the-art results in both novel-view synthesis and motion trajectory recovery from monocular video, advancing the capabilities of dynamic scene reconstruction.

