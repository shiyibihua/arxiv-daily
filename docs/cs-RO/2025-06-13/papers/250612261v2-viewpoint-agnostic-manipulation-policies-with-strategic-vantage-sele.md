---
layout: default
title: Viewpoint-Agnostic Manipulation Policies with Strategic Vantage Selection
---

# Viewpoint-Agnostic Manipulation Policies with Strategic Vantage Selection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12261" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12261v2</a>
  <a href="https://arxiv.org/pdf/2506.12261.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12261v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12261v2', 'Viewpoint-Agnostic Manipulation Policies with Strategic Vantage Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sreevishakh Vasudevan, Som Sagar, Ransalu Senanayake

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-10-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVantageæ¡†æ¶ä»¥è§£å†³è§†è§’å˜åŒ–ä¸‹çš„æ“æ§ç­–ç•¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§’æ— å…³æ“æ§` `ä¿¡æ¯å¢ç›Šä¼˜åŒ–` `ç­–ç•¥å¾®è°ƒ` `æœºå™¨äººæ“æ§` `åŠ¨æ€ç›¸æœºè®¾ç½®` `æ·±åº¦å­¦ä¹ ` `è§†è§‰ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰æ“æ§ç­–ç•¥é€šå¸¸åœ¨å•ä¸€è§†è§’ä¸‹è®­ç»ƒï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­è§†è§’å˜åŒ–æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚
2. Vantageæ¡†æ¶é€šè¿‡ä¼˜åŒ–ç›¸æœºä½ç½®çš„é€‰æ‹©ï¼Œå‡å°‘äº†å¯¹éšæœºè§†è§’èšåˆçš„ä¾èµ–ï¼Œä»è€Œæé«˜äº†ç­–ç•¥çš„è§†è§’æ— å…³æ€§ã€‚
3. åœ¨å¤šç§æ“æ§ä»»åŠ¡ä¸­ï¼ŒVantageåœ¨ä»…éœ€å°‘é‡å¾®è°ƒæ­¥éª¤çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸç‡æé«˜äº†25%ï¼Œå¹¶åœ¨åŠ¨æ€ç›¸æœºè®¾ç½®ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”±äºåŸºäºè§†è§‰çš„æ“æ§ç­–ç•¥é€šå¸¸æ˜¯åœ¨å•ä¸€è§†è§’ä¸‹è®­ç»ƒçš„ï¼Œå› æ­¤åœ¨éƒ¨ç½²æ—¶è§†è§’å˜åŒ–ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ç®€å•åœ°ä»å¤šä¸ªéšæœºè§†è§’èšåˆç¤ºä¾‹ä¸ä»…æˆæœ¬é«˜ï¼Œè€Œä¸”ä¼šå› è§†è§‰å¤šæ ·æ€§è¿‡å¤§è€Œå¯¼è‡´å­¦ä¹ ä¸ç¨³å®šã€‚æœ¬æ–‡æå‡ºäº†Vantageï¼Œä¸€ä¸ªè§†è§’é€‰æ‹©æ¡†æ¶ï¼Œé€šè¿‡åœ¨ä¸€å°ç»„æˆ˜ç•¥æ€§è®¾ç½®çš„ç›¸æœºä½ç½®ä¸Šå¾®è°ƒä»»ä½•é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥è¯±å¯¼è§†è§’æ— å…³çš„è¡Œä¸ºã€‚Vantageå°†ç›¸æœºæ”¾ç½®è§†ä¸ºä¸€ä¸ªä¿¡æ¯å¢ç›Šä¼˜åŒ–é—®é¢˜ï¼Œå¹³è¡¡äº†æ–°è§†è§’çš„æ¢ç´¢ä¸æœ‰å‰æ™¯è§†è§’çš„åˆ©ç”¨ï¼ŒåŒæ—¶æä¾›äº†å…³äºæ”¶æ•›æ€§å’Œé²æ£’æ€§çš„ç†è®ºä¿è¯ã€‚å®éªŒè¡¨æ˜ï¼ŒVantageåœ¨è§†è§’å˜åŒ–ä¸‹çš„æˆåŠŸç‡æ˜¾è‘—æé«˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºäºè§†è§‰çš„æ“æ§ç­–ç•¥åœ¨è§†è§’å˜åŒ–æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤šè§†è§’æ•°æ®èšåˆæ—¶ï¼Œå¾€å¾€å¯¼è‡´å­¦ä¹ ä¸ç¨³å®šï¼Œä¸”æˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVantageæ¡†æ¶é€šè¿‡å°†ç›¸æœºæ”¾ç½®è§†ä¸ºä¿¡æ¯å¢ç›Šä¼˜åŒ–é—®é¢˜ï¼Œé€‰æ‹©ä¸€å°ç»„æˆ˜ç•¥æ€§ç›¸æœºä½ç½®è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå®ç°è§†è§’æ— å…³çš„æ“æ§ç­–ç•¥ã€‚è¯¥æ–¹æ³•é¿å…äº†ä¼ ç»Ÿçš„æš´åŠ›æœç´¢ï¼Œå¹³è¡¡äº†æ–°è§†è§’çš„æ¢ç´¢ä¸å·²æœ‰è§†è§’çš„åˆ©ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVantageçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç›¸æœºä½ç½®é€‰æ‹©æ¨¡å—å’Œç­–ç•¥å¾®è°ƒæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡ä¿¡æ¯å¢ç›Šä¼˜åŒ–é€‰æ‹©ç›¸æœºä½ç½®ï¼Œç„¶ååœ¨è¿™äº›ä½ç½®ä¸Šå¾®è°ƒé¢„è®­ç»ƒçš„æ“æ§ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šVantageçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å°†ç›¸æœºæ”¾ç½®é—®é¢˜è½¬åŒ–ä¸ºä¿¡æ¯å¢ç›Šä¼˜åŒ–ï¼Œæä¾›äº†ç†è®ºä¸Šçš„æ”¶æ•›æ€§å’Œé²æ£’æ€§ä¿è¯ã€‚è¿™ä¸ä¼ ç»Ÿçš„éšæœºè§†è§’èšåˆæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Vantageä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ç›¸æœºä½ç½®çš„é€‰æ‹©ç­–ç•¥å’Œå¾®è°ƒæ­¥éª¤çš„æ•°é‡ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œè€ƒè™‘äº†è§†è§’å˜åŒ–å¯¹ç­–ç•¥æ€§èƒ½çš„å½±å“ï¼Œç¡®ä¿å¾®è°ƒè¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚ç­–ç•¥ç½‘ç»œç»“æ„åˆ™åŸºäºç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€‚åº”æ€§å¼ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVantageåœ¨å¤šç§æ“æ§ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºå›ºå®šã€ç½‘æ ¼æˆ–éšæœºæ•°æ®é€‰æ‹©ç­–ç•¥ï¼ŒæˆåŠŸç‡æé«˜äº†25%ã€‚åœ¨åŠ¨æ€ç›¸æœºè®¾ç½®ä¸‹ï¼ŒVantageå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“æ§ã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿å’Œæ™ºèƒ½å®¶å±…ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜æ“æ§ç­–ç•¥åœ¨ä¸åŒè§†è§’ä¸‹çš„é²æ£’æ€§ï¼ŒVantageæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Since vision-based manipulation policies are typically trained from data gathered from a single viewpoint, their performance drops when the view changes during deployment. Naively aggregating demonstrations from numerous random views is not only costly but also known to destabilize learning, as excessive visual diversity acts as noise. We present Vantage, a viewpoint selection framework to fine-tune any pre-trained policy on a small, strategically set of camera poses to induce viewpoint-agnostic behavior. Instead of relying on costly brute-force search over viewpoints, Vantage formulates camera placement as an information gain optimization problem in a continuous space. This approach balances exploration of novel poses with exploitation of promising ones, while also providing theoretical guarantees about convergence and robustness. Across manipulation tasks and policy families, Vantage consistently improves success under viewpoint shifts compared to fixed, grid, or random data selection strategies with only a handful of fine-tuning steps. Experiments conducted on simulated and real-world setups show that Vantage increases the task success rate by 25% for diffusion policies, and yields robust gains in dynamic-camera settings.

