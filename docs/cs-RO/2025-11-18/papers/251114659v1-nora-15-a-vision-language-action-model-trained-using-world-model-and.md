---
layout: default
title: NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards
---

# NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards

**arXiv**: [2511.14659v1](https://arxiv.org/abs/2511.14659) | [PDF](https://arxiv.org/pdf/2511.14659.pdf)

**ä½œè€…**: Chia-Yu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

**å¤‡æ³¨**: https://declare-lab.github.io/nora-1.5

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**NORA-1.5ï¼šåŸºäºŽä¸–ç•Œæ¨¡åž‹å’ŒåŠ¨ä½œåå¥½å¥–åŠ±è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ï¼Œæå‡å…·èº«æ™ºèƒ½ä½“çš„å¯é æ€§ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `å…·èº«æ™ºèƒ½` `ä¸–ç•Œæ¨¡åž‹` `åå¥½å­¦ä¹ ` `ç›´æŽ¥åå¥½ä¼˜åŒ–` `æµåŒ¹é…` `æœºå™¨äºº`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨å¯é æ€§å’Œæ³›åŒ–æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒçŽ¯å¢ƒæˆ–çœŸå®žä¸–ç•Œéƒ¨ç½²æ—¶ã€‚
2. NORA-1.5é€šè¿‡å¢žåŠ åŸºäºŽæµåŒ¹é…çš„åŠ¨ä½œä¸“å®¶ï¼Œå¹¶ç»“åˆä¸–ç•Œæ¨¡åž‹å’ŒåŠ¨ä½œåå¥½å¥–åŠ±è¿›è¡Œè®­ç»ƒï¼Œæå‡æ¨¡åž‹æ€§èƒ½ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œå¥–åŠ±é©±åŠ¨çš„åŽè®­ç»ƒèƒ½å¤ŸæŒç»­æé«˜æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººçŽ¯å¢ƒä¸­çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜VLAæ¨¡åž‹çš„å¯é æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†NORA-1.5ï¼Œä¸€ä¸ªè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹ï¼Œå®ƒåŸºäºŽé¢„è®­ç»ƒçš„NORAéª¨å¹²ç½‘ç»œï¼Œå¹¶å¢žåŠ äº†ä¸€ä¸ªåŸºäºŽæµåŒ¹é…çš„åŠ¨ä½œä¸“å®¶ã€‚è¿™ç§æž¶æž„ä¸Šçš„å¢žå¼ºæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œä½¿NORA-1.5åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽNORAå’Œå‡ ç§æœ€å…ˆè¿›çš„VLAæ¨¡åž‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é²æ£’æ€§å’Œä»»åŠ¡æˆåŠŸçŽ‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—å¥–åŠ±æ¨¡åž‹ï¼Œç”¨äºŽå¯¹VLAç­–ç•¥è¿›è¡ŒåŽè®­ç»ƒã€‚æˆ‘ä»¬çš„å¥–åŠ±ç»“åˆäº†ï¼ˆiï¼‰ä¸€ä¸ªåŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡åž‹ï¼ˆWMï¼‰ï¼Œç”¨äºŽè¯„ä¼°ç”Ÿæˆçš„åŠ¨ä½œæ˜¯å¦å¯¼å‘æœŸæœ›çš„ç›®æ ‡ï¼Œä»¥åŠï¼ˆiiï¼‰ä¸€ä¸ªåç¦»çœŸå®žå€¼çš„å¯å‘å¼æ–¹æ³•ï¼Œç”¨äºŽåŒºåˆ†å¥½åŠ¨ä½œå’ŒååŠ¨ä½œã€‚åˆ©ç”¨è¿™äº›å¥–åŠ±ä¿¡å·ï¼Œæˆ‘ä»¬æž„å»ºäº†åå¥½æ•°æ®é›†ï¼Œå¹¶é€šè¿‡ç›´æŽ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½¿NORA-1.5é€‚åº”ç›®æ ‡çŽ¯å¢ƒã€‚å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜Žï¼Œå¥–åŠ±é©±åŠ¨çš„åŽè®­ç»ƒèƒ½å¤ŸæŒç»­æé«˜æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººçŽ¯å¢ƒä¸­çš„æ€§èƒ½ï¼Œé€šè¿‡ç®€å•è€Œæœ‰æ•ˆçš„å¥–åŠ±æ¨¡åž‹æ˜¾è‘—æé«˜VLAæ¨¡åž‹çš„å¯é æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼ŒNORA-1.5å’Œå¥–åŠ±å¼•å¯¼çš„åŽè®­ç»ƒæ˜¯å®žçŽ°æ›´å¯é çš„ã€é€‚ç”¨äºŽçœŸå®žä¸–ç•Œéƒ¨ç½²çš„å…·èº«æ™ºèƒ½ä½“çš„å¯è¡Œé€”å¾„ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨çœŸå®žä¸–ç•Œéƒ¨ç½²æ—¶å¯é æ€§å’Œæ³›åŒ–æ€§ä¸è¶³çš„é—®é¢˜ã€‚çŽ°æœ‰VLAæ¨¡åž‹åœ¨é¢å¯¹ä¸åŒçŽ¯å¢ƒå’Œå…·èº«æ™ºèƒ½ä½“æ—¶ï¼Œéš¾ä»¥ä¿è¯ä»»åŠ¡çš„æˆåŠŸçŽ‡å’Œç¨³å®šæ€§ï¼Œé™åˆ¶äº†å…¶åœ¨å®žé™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¢žå¼ºæ¨¡åž‹æž¶æž„å’Œå¼•å…¥å¥–åŠ±é©±åŠ¨çš„åŽè®­ç»ƒæ¥æé«˜VLAæ¨¡åž‹çš„å¯é æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆé€šè¿‡æ·»åŠ åŸºäºŽæµåŒ¹é…çš„åŠ¨ä½œä¸“å®¶æ¥æå‡æ¨¡åž‹çš„åŸºç¡€æ€§èƒ½ï¼Œç„¶åŽåˆ©ç”¨ä¸–ç•Œæ¨¡åž‹å’ŒåŠ¨ä½œåå¥½å¥–åŠ±æ¥æŒ‡å¯¼æ¨¡åž‹çš„ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ç›®æ ‡çŽ¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šNORA-1.5çš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) é¢„è®­ç»ƒçš„NORAéª¨å¹²ç½‘ç»œï¼›2) åŸºäºŽæµåŒ¹é…çš„åŠ¨ä½œä¸“å®¶ï¼Œç”¨äºŽç”ŸæˆåŠ¨ä½œï¼›3) åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡åž‹ï¼ˆWMï¼‰ï¼Œç”¨äºŽè¯„ä¼°åŠ¨ä½œçš„æœ‰æ•ˆæ€§ï¼›4) åç¦»çœŸå®žå€¼çš„å¯å‘å¼æ–¹æ³•ï¼Œç”¨äºŽåŒºåˆ†å¥½ååŠ¨ä½œï¼›5) ç›´æŽ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®—æ³•ï¼Œç”¨äºŽæ ¹æ®å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ¨¡åž‹ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽç»“åˆäº†æ¨¡åž‹æž¶æž„å¢žå¼ºå’Œå¥–åŠ±é©±åŠ¨çš„åŽè®­ç»ƒã€‚é€šè¿‡æ·»åŠ åŠ¨ä½œä¸“å®¶ï¼Œæ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°ç”ŸæˆåŠ¨ä½œï¼›é€šè¿‡ä¸–ç•Œæ¨¡åž‹å’ŒåŠ¨ä½œåå¥½å¥–åŠ±ï¼Œæ¨¡åž‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´æœ‰æ•ˆçš„ç­–ç•¥ã€‚è¿™ç§ç»“åˆä½¿å¾—NORA-1.5åœ¨å¯é æ€§å’Œæ³›åŒ–æ€§æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æµåŒ¹é…æ–¹æ³•è®­ç»ƒåŠ¨ä½œä¸“å®¶ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´æµç•…è‡ªç„¶çš„åŠ¨ä½œï¼›2) è®¾è®¡åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡åž‹ï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹åŠ¨ä½œå¯¹çŽ¯å¢ƒçš„å½±å“ï¼›3) è®¾è®¡åç¦»çœŸå®žå€¼çš„å¯å‘å¼å¥–åŠ±ï¼Œç”¨äºŽåŒºåˆ†å¥½ååŠ¨ä½œï¼›4) ä½¿ç”¨ç›´æŽ¥åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œæ ¹æ®å¥–åŠ±ä¿¡å·ç›´æŽ¥ä¼˜åŒ–æ¨¡åž‹ç­–ç•¥ï¼Œé¿å…äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€äº›é—®é¢˜ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

NORA-1.5åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡æ·»åŠ åŠ¨ä½œä¸“å®¶å’Œè¿›è¡Œå¥–åŠ±é©±åŠ¨çš„åŽè®­ç»ƒï¼ŒNORA-1.5åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†NORAå’Œå…¶ä»–æœ€å…ˆè¿›çš„VLAæ¨¡åž‹ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œå¥–åŠ±é©±åŠ¨çš„åŽè®­ç»ƒèƒ½å¤ŸæŒç»­æé«˜æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜VLAæ¨¡åž‹çš„å¯é æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§éœ€è¦å…·èº«æ™ºèƒ½ä½“çš„å®žé™…åœºæ™¯ï¼Œå¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æé«˜VLAæ¨¡åž‹çš„å¯é æ€§å’Œæ³›åŒ–æ€§ï¼Œå¯ä»¥ä½¿è¿™äº›æ™ºèƒ½ä½“æ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„çŽ¯å¢ƒï¼Œå®Œæˆå„ç§ä»»åŠ¡ï¼Œä»Žè€Œæå‡ç”Ÿäº§æ•ˆçŽ‡å’ŒæœåŠ¡è´¨é‡ï¼Œå…·æœ‰é‡è¦çš„å®žé™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.

