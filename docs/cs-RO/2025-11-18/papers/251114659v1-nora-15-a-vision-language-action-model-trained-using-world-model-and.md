---
layout: default
title: NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards
---

# NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.14659" target="_blank" class="toolbar-btn">arXiv: 2511.14659v1</a>
    <a href="https://arxiv.org/pdf/2511.14659.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14659v1" 
            onclick="toggleFavorite(this, '2511.14659v1', 'NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Chia-Yu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-18

**Â§áÊ≥®**: https://declare-lab.github.io/nora-1.5

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**NORA-1.5ÔºöÂü∫‰∫é‰∏ñÁïåÊ®°ÂûãÂíåÂä®‰ΩúÂÅèÂ•ΩÂ•ñÂä±ËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºåÊèêÂçáÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑÂèØÈù†ÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `ÂÖ∑Ë∫´Êô∫ËÉΩ` `‰∏ñÁïåÊ®°Âûã` `ÂÅèÂ•ΩÂ≠¶‰π†` `Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ` `ÊµÅÂåπÈÖç` `Êú∫Âô®‰∫∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®ÂèØÈù†ÊÄßÂíåÊ≥õÂåñÊÄßÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏çÂêåÁéØÂ¢ÉÊàñÁúüÂÆû‰∏ñÁïåÈÉ®ÁΩ≤Êó∂„ÄÇ
2. NORA-1.5ÈÄöËøáÂ¢ûÂä†Âü∫‰∫éÊµÅÂåπÈÖçÁöÑÂä®‰Ωú‰∏ìÂÆ∂ÔºåÂπ∂ÁªìÂêà‰∏ñÁïåÊ®°ÂûãÂíåÂä®‰ΩúÂÅèÂ•ΩÂ•ñÂä±ËøõË°åËÆ≠ÁªÉÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂ•ñÂä±È©±Âä®ÁöÑÂêéËÆ≠ÁªÉËÉΩÂ§üÊåÅÁª≠ÊèêÈ´òÊ®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÁöÑÊÄßËÉΩÔºåÊòæËëóÊèêÈ´òVLAÊ®°ÂûãÁöÑÂèØÈù†ÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Êñá‰ªãÁªç‰∫ÜNORA-1.5Ôºå‰∏Ä‰∏™ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÔºåÂÆÉÂü∫‰∫éÈ¢ÑËÆ≠ÁªÉÁöÑNORAÈ™®Âπ≤ÁΩëÁªúÔºåÂπ∂Â¢ûÂä†‰∫Ü‰∏Ä‰∏™Âü∫‰∫éÊµÅÂåπÈÖçÁöÑÂä®‰Ωú‰∏ìÂÆ∂„ÄÇËøôÁßçÊû∂ÊûÑ‰∏äÁöÑÂ¢ûÂº∫ÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºå‰ΩøNORA-1.5Âú®Ê®°ÊãüÂíåÁúüÂÆû‰∏ñÁïåÁöÑÂü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫éNORAÂíåÂá†ÁßçÊúÄÂÖàËøõÁöÑVLAÊ®°Âûã„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÊèêÈ´òÈ≤ÅÊ£íÊÄßÂíå‰ªªÂä°ÊàêÂäüÁéáÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÂ•óÂ•ñÂä±Ê®°ÂûãÔºåÁî®‰∫éÂØπVLAÁ≠ñÁï•ËøõË°åÂêéËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÁöÑÂ•ñÂä±ÁªìÂêà‰∫ÜÔºàiÔºâ‰∏Ä‰∏™Âä®‰ΩúÊù°‰ª∂‰∏ñÁïåÊ®°ÂûãÔºàWMÔºâÔºåÁî®‰∫éËØÑ‰º∞ÁîüÊàêÁöÑÂä®‰ΩúÊòØÂê¶ÂØºÂêëÊúüÊúõÁöÑÁõÆÊ†áÔºå‰ª•ÂèäÔºàiiÔºâ‰∏Ä‰∏™ÂÅèÁ¶ªÁúüÂÆûÂÄºÁöÑÂêØÂèëÂºèÊñπÊ≥ïÔºåÁî®‰∫éÂå∫ÂàÜÂ•ΩÂä®‰ΩúÂíåÂùèÂä®‰Ωú„ÄÇÂà©Áî®Ëøô‰∫õÂ•ñÂä±‰ø°Âè∑ÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜÂÅèÂ•ΩÊï∞ÊçÆÈõÜÔºåÂπ∂ÈÄöËøáÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâ‰ΩøNORA-1.5ÈÄÇÂ∫îÁõÆÊ†áÁéØÂ¢É„ÄÇÂπøÊ≥õÁöÑËØÑ‰º∞Ë°®ÊòéÔºåÂ•ñÂä±È©±Âä®ÁöÑÂêéËÆ≠ÁªÉËÉΩÂ§üÊåÅÁª≠ÊèêÈ´òÊ®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÁöÑÊÄßËÉΩÔºåÈÄöËøáÁÆÄÂçïËÄåÊúâÊïàÁöÑÂ•ñÂä±Ê®°ÂûãÊòæËëóÊèêÈ´òVLAÊ®°ÂûãÁöÑÂèØÈù†ÊÄß„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåNORA-1.5ÂíåÂ•ñÂä±ÂºïÂØºÁöÑÂêéËÆ≠ÁªÉÊòØÂÆûÁé∞Êõ¥ÂèØÈù†ÁöÑ„ÄÅÈÄÇÁî®‰∫éÁúüÂÆû‰∏ñÁïåÈÉ®ÁΩ≤ÁöÑÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑÂèØË°åÈÄîÂæÑ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®ÁúüÂÆû‰∏ñÁïåÈÉ®ÁΩ≤Êó∂ÂèØÈù†ÊÄßÂíåÊ≥õÂåñÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâVLAÊ®°ÂûãÂú®Èù¢ÂØπ‰∏çÂêåÁéØÂ¢ÉÂíåÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÊó∂ÔºåÈöæ‰ª•‰øùËØÅ‰ªªÂä°ÁöÑÊàêÂäüÁéáÂíåÁ®≥ÂÆöÊÄßÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂ¢ûÂº∫Ê®°ÂûãÊû∂ÊûÑÂíåÂºïÂÖ•Â•ñÂä±È©±Âä®ÁöÑÂêéËÆ≠ÁªÉÊù•ÊèêÈ´òVLAÊ®°ÂûãÁöÑÂèØÈù†ÊÄß„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈ¶ñÂÖàÈÄöËøáÊ∑ªÂä†Âü∫‰∫éÊµÅÂåπÈÖçÁöÑÂä®‰Ωú‰∏ìÂÆ∂Êù•ÊèêÂçáÊ®°ÂûãÁöÑÂü∫Á°ÄÊÄßËÉΩÔºåÁÑ∂ÂêéÂà©Áî®‰∏ñÁïåÊ®°ÂûãÂíåÂä®‰ΩúÂÅèÂ•ΩÂ•ñÂä±Êù•ÊåáÂØºÊ®°ÂûãÁöÑÁ≠ñÁï•‰ºòÂåñÔºå‰ΩøÂÖ∂Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îÁõÆÊ†áÁéØÂ¢É„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöNORA-1.5ÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) È¢ÑËÆ≠ÁªÉÁöÑNORAÈ™®Âπ≤ÁΩëÁªúÔºõ2) Âü∫‰∫éÊµÅÂåπÈÖçÁöÑÂä®‰Ωú‰∏ìÂÆ∂ÔºåÁî®‰∫éÁîüÊàêÂä®‰ΩúÔºõ3) Âä®‰ΩúÊù°‰ª∂‰∏ñÁïåÊ®°ÂûãÔºàWMÔºâÔºåÁî®‰∫éËØÑ‰º∞Âä®‰ΩúÁöÑÊúâÊïàÊÄßÔºõ4) ÂÅèÁ¶ªÁúüÂÆûÂÄºÁöÑÂêØÂèëÂºèÊñπÊ≥ïÔºåÁî®‰∫éÂå∫ÂàÜÂ•ΩÂùèÂä®‰ΩúÔºõ5) Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÁÆóÊ≥ïÔºåÁî®‰∫éÊ†πÊçÆÂ•ñÂä±‰ø°Âè∑‰ºòÂåñÊ®°ÂûãÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫ÜÊ®°ÂûãÊû∂ÊûÑÂ¢ûÂº∫ÂíåÂ•ñÂä±È©±Âä®ÁöÑÂêéËÆ≠ÁªÉ„ÄÇÈÄöËøáÊ∑ªÂä†Âä®‰Ωú‰∏ìÂÆ∂ÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁîüÊàêÂä®‰ΩúÔºõÈÄöËøá‰∏ñÁïåÊ®°ÂûãÂíåÂä®‰ΩúÂÅèÂ•ΩÂ•ñÂä±ÔºåÊ®°ÂûãËÉΩÂ§üÂ≠¶‰π†Âà∞Êõ¥ÊúâÊïàÁöÑÁ≠ñÁï•„ÄÇËøôÁßçÁªìÂêà‰ΩøÂæóNORA-1.5Âú®ÂèØÈù†ÊÄßÂíåÊ≥õÂåñÊÄßÊñπÈù¢ÈÉΩÂèñÂæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®ÊµÅÂåπÈÖçÊñπÊ≥ïËÆ≠ÁªÉÂä®‰Ωú‰∏ìÂÆ∂Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÁîüÊàêÊõ¥ÊµÅÁïÖËá™ÁÑ∂ÁöÑÂä®‰ΩúÔºõ2) ËÆæËÆ°Âä®‰ΩúÊù°‰ª∂‰∏ñÁïåÊ®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂáÜÁ°ÆÈ¢ÑÊµãÂä®‰ΩúÂØπÁéØÂ¢ÉÁöÑÂΩ±ÂìçÔºõ3) ËÆæËÆ°ÂÅèÁ¶ªÁúüÂÆûÂÄºÁöÑÂêØÂèëÂºèÂ•ñÂä±ÔºåÁî®‰∫éÂå∫ÂàÜÂ•ΩÂùèÂä®‰ΩúÔºõ4) ‰ΩøÁî®Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÁÆóÊ≥ïÔºåÊ†πÊçÆÂ•ñÂä±‰ø°Âè∑Áõ¥Êé•‰ºòÂåñÊ®°ÂûãÁ≠ñÁï•ÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑ‰∏Ä‰∫õÈóÆÈ¢ò„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

NORA-1.5Âú®Ê®°ÊãüÂíåÁúüÂÆû‰∏ñÁïåÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÈÄöËøáÊ∑ªÂä†Âä®‰Ωú‰∏ìÂÆ∂ÂíåËøõË°åÂ•ñÂä±È©±Âä®ÁöÑÂêéËÆ≠ÁªÉÔºåNORA-1.5Âú®Â§ö‰∏™‰ªªÂä°‰∏äË∂ÖË∂ä‰∫ÜNORAÂíåÂÖ∂‰ªñÊúÄÂÖàËøõÁöÑVLAÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ•ñÂä±È©±Âä®ÁöÑÂêéËÆ≠ÁªÉËÉΩÂ§üÊåÅÁª≠ÊèêÈ´òÊÄßËÉΩÔºåÊòæËëóÊèêÈ´òVLAÊ®°ÂûãÁöÑÂèØÈù†ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑÂÆûÈôÖÂú∫ÊôØÔºåÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®Âåñ„ÄÅËá™Âä®È©æÈ©∂Á≠â„ÄÇÈÄöËøáÊèêÈ´òVLAÊ®°ÂûãÁöÑÂèØÈù†ÊÄßÂíåÊ≥õÂåñÊÄßÔºåÂèØ‰ª•‰ΩøËøô‰∫õÊô∫ËÉΩ‰ΩìÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÂ§öÂèòÁöÑÁéØÂ¢ÉÔºåÂÆåÊàêÂêÑÁßç‰ªªÂä°Ôºå‰ªéËÄåÊèêÂçáÁîü‰∫ßÊïàÁéáÂíåÊúçÂä°Ë¥®ÈáèÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂπøÈòîÁöÑÂèëÂ±ïÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.

