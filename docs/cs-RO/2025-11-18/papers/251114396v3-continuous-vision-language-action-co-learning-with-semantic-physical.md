---
layout: default
title: Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning
---

# Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.14396" target="_blank" class="toolbar-btn">arXiv: 2511.14396v3</a>
    <a href="https://arxiv.org/pdf/2511.14396.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14396v3" 
            onclick="toggleFavorite(this, '2511.14396v3', 'Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiuxiu Qi, Yu Yang, Jiannong Cao, Luyao Bai, Chongshan Fan, Chengtai Cao, Hongpeng Wang

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-18 (Êõ¥Êñ∞: 2025-12-12)

**Â§áÊ≥®**: Accepted at AAAI 2026, the Project website is available at https://qhemu.github.io/CCoL/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CCoLÊ°ÜÊû∂ÔºåÈÄöËøáËØ≠‰πâ-Áâ©ÁêÜÂØπÈΩêÁöÑËøûÁª≠ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÂçèÂêåÂ≠¶‰π†ÊèêÂçáË°å‰∏∫ÂÖãÈöÜÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `Ë°å‰∏∫ÂÖãÈöÜ` `ËßÜËßâËØ≠Ë®ÄÂä®‰Ωú` `ÂçèÂêåÂ≠¶‰π†` `ËØ≠‰πâÁâ©ÁêÜÂØπÈΩê` `Êú∫Âô®‰∫∫Êìç‰Ωú` `‰∫∫Êú∫‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ë°å‰∏∫ÂÖãÈöÜÈù¢‰∏¥Â∫èÂàóÂä®‰ΩúÂÜ≥Á≠ñ‰∏≠ÁöÑÁ¥ØÁßØËØØÂ∑ÆÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Áâ©ÁêÜ‰∏çËøûÁª≠ÊÄßÂíåËØ≠‰πâ-Áâ©ÁêÜ‰∏çÂØπÈΩêÊñπÈù¢Â≠òÂú®‰∏çË∂≥„ÄÇ
2. CCoLÊ°ÜÊû∂ÈÄöËøáËøûÁª≠ÂçèÂêåÂ≠¶‰π†ËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÊú¨‰ΩìÊÑüÂèó‰ø°ÊÅØÔºåÂπ∂Âà©Áî®ÂèåÂêë‰∫§ÂèâÊ≥®ÊÑèÂäõÂÆûÁé∞ËØ≠‰πâ-Áâ©ÁêÜÂØπÈΩêÔºå‰ªéËÄåÁîüÊàêÊõ¥È≤ÅÊ£íÁöÑÂä®‰ΩúËΩ®Ëøπ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCCoLÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®ÂèåÊâãÂä®ÊèíÂÖ•Á≠âÂ§çÊùÇ‰ªªÂä°‰∏≠Ë°®Áé∞Á™ÅÂá∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ËøûÁª≠ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÂçèÂêåÂ≠¶‰π†‰∏éËØ≠‰πâ-Áâ©ÁêÜÂØπÈΩêÔºàCCoLÔºâÁöÑÊñ∞ÂûãË°å‰∏∫ÂÖãÈöÜÔºàBCÔºâÊ°ÜÊû∂ÔºåÊó®Âú®Á°Æ‰øùÊó∂Èó¥‰∏ä‰∏ÄËá¥ÁöÑÊâßË°åÂíåÁªÜÁ≤íÂ∫¶ÁöÑËØ≠‰πâÂØπÈΩê„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÊú¨‰ΩìÊÑüÂèóËæìÂÖ•Ôºà‰æãÂ¶ÇÔºåÊú∫Âô®‰∫∫ÂÜÖÈÉ®Áä∂ÊÄÅÔºâÁöÑËøûÁª≠ÂçèÂêåÂ≠¶‰π†ÔºåÁîüÊàêÈ≤ÅÊ£í‰∏îÂπ≥ÊªëÁöÑÂä®‰ΩúÊâßË°åËΩ®Ëøπ„ÄÇÂêåÊó∂ÔºåÈÄöËøáÂèåÂêë‰∫§ÂèâÊ≥®ÊÑèÂäõÂ∞ÜËØ≠Ë®ÄËØ≠‰πâÈîöÂÆöÂà∞ËßÜËßâËøêÂä®Ë°®Á§∫ÔºåÂ≠¶‰π†Áî®‰∫éÂä®‰ΩúÁîüÊàêÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÊàêÂäüÂÖãÊúç‰∫ÜËØ≠‰πâ-Áâ©ÁêÜ‰∏çÂØπÈΩêÁöÑÈóÆÈ¢ò„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåCCoLÂú®‰∏â‰∏™Ê®°ÊãüÂ•ó‰ª∂‰∏≠ÂÆûÁé∞‰∫ÜÂπ≥Âùá8.0%ÁöÑÁõ∏ÂØπÊîπËøõÔºåÂú®‰∫∫Â∑•ÊºîÁ§∫ÁöÑÂèåÊâãÂä®ÊèíÂÖ•‰ªªÂä°‰∏≠Ëé∑Âæó‰∫ÜÈ´òËææ19.2%ÁöÑÁõ∏ÂØπÂ¢ûÁõä„ÄÇÂú®7Ëá™Áî±Â∫¶Êú∫Âô®‰∫∫‰∏äÁöÑÁúüÂÆû‰∏ñÁïåÊµãËØïËøõ‰∏ÄÊ≠•ËØÅÂÆû‰∫ÜCCoLÂú®Êú™ËßÅÂíåÂòàÊùÇÁâ©‰ΩìÁä∂ÊÄÅ‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöË°å‰∏∫ÂÖãÈöÜÔºàBCÔºâÊó®Âú®Ê®°‰ªø‰∫∫Á±ªÊºîÁ§∫Â≠¶‰π†ÊéßÂà∂Á≠ñÁï•Ôºå‰ΩÜÁî±‰∫éÂ∫èÂàóÂä®‰ΩúÂÜ≥Á≠ñ‰∏≠ÁöÑÁ¥ØÁßØËØØÂ∑ÆÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•Ëß£ÂÜ≥Áâ©ÁêÜ‰∏çËøûÁª≠ÊÄßÂíåËØ≠‰πâ-Áâ©ÁêÜ‰∏çÂØπÈΩêÈóÆÈ¢òÔºå‰ΩøÂæóÂä®‰ΩúÂÖãÈöÜ‰∏çÂáÜÁ°ÆÔºåÊâßË°åÊñ≠Êñ≠Áª≠Áª≠„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCCoLÁöÑÊ†∏ÂøÉÂú®‰∫éÈÄöËøáËøûÁª≠ÂçèÂêåÂ≠¶‰π†ËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÊú¨‰ΩìÊÑüÂèó‰ø°ÊÅØÔºåÁîüÊàêÂπ≥ÊªëÁöÑÂä®‰ΩúËΩ®ËøπÔºåÂπ∂Âà©Áî®ËØ≠‰πâ-Áâ©ÁêÜÂØπÈΩêÊù•ÂÖãÊúçËØ≠‰πâÈ∏øÊ≤ü„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ËØ≠Ë®ÄÊåá‰ª§ÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫Á≤æÁ°ÆÁöÑÊú∫Âô®‰∫∫Âä®‰Ωú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCCoLÊ°ÜÊû∂ÂåÖÂê´ËßÜËßâÁºñÁ†ÅÂô®„ÄÅËØ≠Ë®ÄÁºñÁ†ÅÂô®ÂíåÂä®‰ΩúÁîüÊàêÂô®„ÄÇËßÜËßâÁºñÁ†ÅÂô®Â§ÑÁêÜÂõæÂÉèËæìÂÖ•ÔºåËØ≠Ë®ÄÁºñÁ†ÅÂô®Â§ÑÁêÜËØ≠Ë®ÄÊåá‰ª§ÔºåÂä®‰ΩúÁîüÊàêÂô®ÂàôÊ†πÊçÆËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØÁîüÊàêÂä®‰ΩúÂ∫èÂàó„ÄÇÂÖ≥ÈîÆÂú®‰∫éÔºåËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÊú¨‰ΩìÊÑüÂèó‰ø°ÊÅØÂú®Êï¥‰∏™ËøáÁ®ã‰∏≠ËøõË°åËøûÁª≠ÂçèÂêåÂ≠¶‰π†Ôºå‰ª•Á°Æ‰øùÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåËøò‰ΩøÁî®‰∫ÜÂèåÂêë‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•ÂÆûÁé∞ËØ≠‰πâ-Áâ©ÁêÜÂØπÈΩê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCCoLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ËøûÁª≠ÂçèÂêåÂ≠¶‰π†ÂíåËØ≠‰πâ-Áâ©ÁêÜÂØπÈΩêÊú∫Âà∂„ÄÇ‰º†ÁªüÁöÑBCÊñπÊ≥ïÈÄöÂ∏∏Áã¨Á´ãÂ§ÑÁêÜËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØÔºåËÄåCCoLÂàôÂ∞ÜÂÆÉ‰ª¨Êï¥ÂêàÂà∞‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂ≠¶‰π†Ê°ÜÊû∂‰∏≠„ÄÇÈÄöËøáÂèåÂêë‰∫§ÂèâÊ≥®ÊÑèÂäõÔºåÊ®°ÂûãËÉΩÂ§üÂ≠¶‰π†Âà∞ËØ≠Ë®ÄËØ≠‰πâ‰∏éËßÜËßâËøêÂä®Ë°®Á§∫‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£ËØ≠Ë®ÄÊåá‰ª§Âπ∂ÁîüÊàêÁõ∏Â∫îÁöÑÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCCoL‰ΩøÁî®‰∫ÜTransformerÊû∂ÊûÑÊù•ÂÆûÁé∞ÂèåÂêë‰∫§ÂèâÊ≥®ÊÑèÂäõ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨Ë°å‰∏∫ÂÖãÈöÜÊçüÂ§±„ÄÅÂπ≥ÊªëÊçüÂ§±ÂíåÂØπÈΩêÊçüÂ§±„ÄÇË°å‰∏∫ÂÖãÈöÜÊçüÂ§±Áî®‰∫éÊ®°‰ªø‰∫∫Á±ªÊºîÁ§∫ÔºåÂπ≥ÊªëÊçüÂ§±Áî®‰∫éÁîüÊàêÂπ≥ÊªëÁöÑÂä®‰ΩúËΩ®ËøπÔºåÂØπÈΩêÊçüÂ§±Áî®‰∫é‰øÉËøõËØ≠‰πâ-Áâ©ÁêÜÂØπÈΩê„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÔºàÂ¶ÇTransformerÂ±ÇÊï∞„ÄÅÊ≥®ÊÑèÂäõÂ§¥Êï∞Á≠âÔºâÊ†πÊçÆÂÆûÈ™åÁªìÊûúËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CCoLÂú®‰∏â‰∏™Ê®°ÊãüÂ•ó‰ª∂‰∏≠ÂÆûÁé∞‰∫ÜÂπ≥Âùá8.0%ÁöÑÁõ∏ÂØπÊîπËøõÔºåÂú®‰∫∫Â∑•ÊºîÁ§∫ÁöÑÂèåÊâãÂä®ÊèíÂÖ•‰ªªÂä°‰∏≠Ëé∑Âæó‰∫ÜÈ´òËææ19.2%ÁöÑÁõ∏ÂØπÂ¢ûÁõä„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCCoLËÉΩÂ§üÁîüÊàêÊõ¥È≤ÅÊ£í„ÄÅÊõ¥Âπ≥ÊªëÁöÑÂä®‰ΩúËΩ®ËøπÔºåÂπ∂Êõ¥Â•ΩÂú∞Ê≥õÂåñÂà∞Êú™ËßÅËøáÁöÑÁâ©‰ΩìÁä∂ÊÄÅ„ÄÇÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰πüÈ™åËØÅ‰∫ÜCCoLÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CCoLÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶Å‰∫∫Êú∫Âçè‰ΩúÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇË£ÖÈÖç„ÄÅÊäìÂèñ„ÄÅÊîæÁΩÆÁ≠â„ÄÇËØ•ÊäÄÊúØËÉΩÂ§üÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏ãÁöÑÊìç‰ΩúËÉΩÂäõÔºåÂπ∂Èôç‰ΩéÂØπ‰∫∫Â∑•Á§∫ÊïôÊï∞ÊçÆÁöÑ‰æùËµñ„ÄÇÊú™Êù•ÔºåCCoLÊúâÊúõÂ∫îÁî®‰∫éÊô∫ËÉΩÂà∂ÈÄ†„ÄÅÂåªÁñóÊú∫Âô®‰∫∫„ÄÅÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫Á≠âÈ¢ÜÂüüÔºåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥È´òÊïàÁöÑ‰∫∫Êú∫‰∫§‰∫í„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.

