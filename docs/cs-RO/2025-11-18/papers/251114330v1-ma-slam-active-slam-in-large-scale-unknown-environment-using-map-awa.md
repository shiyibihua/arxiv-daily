---
layout: default
title: MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning
---

# MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.14330" target="_blank" class="toolbar-btn">arXiv: 2511.14330v1</a>
    <a href="https://arxiv.org/pdf/2511.14330.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14330v1" 
            onclick="toggleFavorite(this, '2511.14330v1', 'MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yizhen Yin, Yuhua Qi, Dapeng Feng, Hongbo Chen, Hongjun Ma, Jin Wu, Yi Jiang

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-18

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**MA-SLAMÔºöÂü∫‰∫éÂú∞ÂõæÊÑüÁü•ÁöÑÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºåÁî®‰∫éÂ§ßËßÑÊ®°Êú™Áü•ÁéØÂ¢ÉÁöÑ‰∏ªÂä®SLAM**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `‰∏ªÂä®SLAM` `Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†` `Âú∞ÂõæË°®Á§∫` `ÂÖ®Â±ÄËßÑÂàí` `Êú∫Âô®‰∫∫ÂØºËà™`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ‰∏ªÂä®SLAMÊñπÊ≥ïÂú®Â∞èËßÑÊ®°ÂèóÊéßÁéØÂ¢É‰∏≠ÊúâÊïàÔºå‰ΩÜÂú®Â§ßËßÑÊ®°Â§öÊ†∑ÂåñÁéØÂ¢É‰∏≠Èù¢‰∏¥Êé¢Á¥¢Êó∂Èó¥Èïø„ÄÅË∑ØÂæÑÊ¨°‰ºòÁ≠âÊåëÊàò„ÄÇ
2. MA-SLAMÈÄöËøáÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºåÁªìÂêàÁªìÊûÑÂåñÂú∞ÂõæË°®Á§∫ÂíåÂÖ®Â±ÄË∑ØÂæÑËßÑÂàíÔºåÂÆûÁé∞Â§ßËßÑÊ®°ÁéØÂ¢É‰∏ãÁöÑÈ´òÊïàÊé¢Á¥¢„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMA-SLAMÂú®Êé¢Á¥¢Êó∂Èó¥ÂíåË∑ùÁ¶ª‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®ÁúüÂÆûUGVÂπ≥Âè∞‰∏äÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºàDRLÔºâÁöÑÂú∞ÂõæÊÑüÁü•‰∏ªÂä®SLAMÁ≥ªÁªüMA-SLAMÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßËßÑÊ®°ÁéØÂ¢É‰∏≠È´òÊïàÊé¢Á¥¢ÁöÑÊåëÊàò„ÄÇËØ•ÊñπÊ≥ïÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁªìÊûÑÂåñÂú∞ÂõæË°®Á§∫ÔºåÈÄöËøáÁ¶ªÊï£Á©∫Èó¥Êï∞ÊçÆÂπ∂Êï¥ÂêàËæπÁïåÁÇπÂíåÂéÜÂè≤ËΩ®ËøπÔºåÁÆÄÊ¥ÅÊúâÊïàÂú∞Â∞ÅË£ÖÂ∑≤ËÆøÈóÆÂå∫ÂüüÔºå‰Ωú‰∏∫Âü∫‰∫éÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÁöÑÂÜ≥Á≠ñÊ®°ÂùóÁöÑËæìÂÖ•„ÄÇÂÜ≥Á≠ñÊ®°ÂùóÊ≤°ÊúâÈ°∫Â∫èÈ¢ÑÊµã‰∏ã‰∏ÄÊ≠•Âä®‰ΩúÔºåËÄåÊòØÈááÁî®ÂÖàËøõÁöÑÂÖ®Â±ÄËßÑÂàíÂô®ÔºåÂà©Áî®ÈïøÁ®ãÁõÆÊ†áÁÇπ‰ºòÂåñÊé¢Á¥¢Ë∑ØÂæÑ„ÄÇÂú®‰∏â‰∏™‰ªøÁúüÁéØÂ¢ÉÂíå‰∏Ä‰∏™ÁúüÂÆûÁöÑÊó†‰∫∫Âú∞Èù¢ËΩ¶ËæÜÔºàUGVÔºâ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÔºå‰∏éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊòæËëóÂáèÂ∞ë‰∫ÜÊé¢Á¥¢ÁöÑÊåÅÁª≠Êó∂Èó¥ÂíåË∑ùÁ¶ª„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞Êúâ‰∏ªÂä®SLAMÊñπÊ≥ïÂú®Â§ßËßÑÊ®°Êú™Áü•ÁéØÂ¢É‰∏≠ÔºåÁî±‰∫éÁº∫‰πèÂØπÁéØÂ¢ÉÁöÑÂÖ®Â±ÄÁêÜËß£ÂíåÊúâÊïàÁöÑÊé¢Á¥¢Á≠ñÁï•ÔºåÂØºËá¥Êé¢Á¥¢Êó∂Èó¥ËøáÈïøÔºåË∑ØÂæÑÊïàÁéá‰Ωé‰∏ãÔºåÈöæ‰ª•Âø´ÈÄüÊûÑÂª∫ÂÆåÊï¥ÂáÜÁ°ÆÁöÑÂú∞Âõæ„ÄÇÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÂèóÈôêÁöÑÊú∫Âô®‰∫∫Âπ≥Âè∞‰∏äÔºåÈúÄË¶ÅÊõ¥È´òÊïàÁöÑÊé¢Á¥¢ÁÆóÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMA-SLAMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºàDRLÔºâÂ≠¶‰π†‰∏ÄÁßçÂú∞ÂõæÊÑüÁü•ÁöÑÊé¢Á¥¢Á≠ñÁï•„ÄÇÈÄöËøáÁªìÊûÑÂåñÂú∞ÂõæË°®Á§∫ÔºåÂ∞ÜÁéØÂ¢É‰ø°ÊÅØÁºñÁ†ÅÊàêDRLÊô∫ËÉΩ‰ΩìÂèØ‰ª•ÁêÜËß£ÁöÑÁä∂ÊÄÅÔºåÂπ∂ÁªìÂêàÂÖ®Â±ÄËßÑÂàíÂô®Ôºå‰ºòÂåñÈïøÁ®ãÊé¢Á¥¢Ë∑ØÂæÑÔºå‰ªéËÄåÊèêÈ´òÊé¢Á¥¢ÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMA-SLAMÁ≥ªÁªü‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™Ê®°ÂùóÔºöÁªìÊûÑÂåñÂú∞ÂõæÊûÑÂª∫Ê®°Âùó„ÄÅÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÂÜ≥Á≠ñÊ®°ÂùóÂíåÂÖ®Â±ÄËßÑÂàíÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÁªìÊûÑÂåñÂú∞ÂõæÊûÑÂª∫Ê®°ÂùóÂ∞Ü‰º†ÊÑüÂô®Êï∞ÊçÆËΩ¨Âåñ‰∏∫Á¶ªÊï£ÂåñÁöÑÂú∞ÂõæË°®Á§∫ÔºåÂπ∂ÊèêÂèñËæπÁïåÁÇπÂíåÂéÜÂè≤ËΩ®Ëøπ‰ø°ÊÅØ„ÄÇÁÑ∂ÂêéÔºåÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÂÜ≥Á≠ñÊ®°ÂùóÂü∫‰∫éÁªìÊûÑÂåñÂú∞Âõæ‰ø°ÊÅØÔºåÂ≠¶‰π†ÊúÄ‰ºòÁöÑÊé¢Á¥¢Á≠ñÁï•ÔºåËæìÂá∫ÈïøÁ®ãÁõÆÊ†áÁÇπ„ÄÇÊúÄÂêéÔºåÂÖ®Â±ÄËßÑÂàíÊ®°ÂùóÊ†πÊçÆÈïøÁ®ãÁõÆÊ†áÁÇπÔºåÁîüÊàêÊú∫Âô®‰∫∫ÂèØÊâßË°åÁöÑË∑ØÂæÑ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMA-SLAMÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫ÜÁªìÊûÑÂåñÂú∞ÂõæË°®Á§∫ÔºåËÉΩÂ§üÁÆÄÊ¥ÅÊúâÊïàÂú∞ÁºñÁ†ÅÁéØÂ¢É‰ø°ÊÅØÔºåÂπ∂‰Ωú‰∏∫DRLÊô∫ËÉΩ‰ΩìÁöÑËæìÂÖ•Ôºõ2) ÁªìÂêà‰∫ÜÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÂíåÂÖ®Â±ÄËßÑÂàíÔºåÂà©Áî®DRLÂ≠¶‰π†ÈïøÁ®ãÁõÆÊ†áÁÇπÔºåÂπ∂‰ΩøÁî®ÂÖ®Â±ÄËßÑÂàíÂô®‰ºòÂåñË∑ØÂæÑÔºå‰ªéËÄåÊèêÈ´òÊé¢Á¥¢ÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÁªìÊûÑÂåñÂú∞ÂõæÈááÁî®Á¶ªÊï£ÂåñÁöÑÁΩëÊ†ºÂú∞ÂõæÔºåÊØè‰∏™ÁΩëÊ†ºÂåÖÂê´Âç†ÊçÆ‰ø°ÊÅØ„ÄÇËæπÁïåÁÇπÈÄöËøáÊèêÂèñÂç†ÊçÆÁΩëÊ†ºÂíåËá™Áî±ÁΩëÊ†º‰πãÈó¥ÁöÑËæπÁïåËé∑Âæó„ÄÇÂéÜÂè≤ËΩ®ËøπËÆ∞ÂΩïÊú∫Âô®‰∫∫ÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇDRLÊô∫ËÉΩ‰ΩìÈááÁî®Ê∑±Â∫¶QÁΩëÁªúÔºàDQNÔºâÁªìÊûÑÔºåËæìÂÖ•‰∏∫ÁªìÊûÑÂåñÂú∞Âõæ‰ø°ÊÅØÔºåËæìÂá∫‰∏∫ÈïøÁ®ãÁõÆÊ†áÁÇπ„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®QÂ≠¶‰π†ÊçüÂ§±ÂáΩÊï∞„ÄÇÂÖ®Â±ÄËßÑÂàíÂô®ÈááÁî®A*ÁÆóÊ≥ï„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMA-SLAMÂú®‰∏â‰∏™‰ªøÁúüÁéØÂ¢É‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊòæËëóÂáèÂ∞ë‰∫ÜÊé¢Á¥¢ÁöÑÊåÅÁª≠Êó∂Èó¥ÂíåË∑ùÁ¶ª„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∏™‰ªøÁúüÁéØÂ¢É‰∏≠ÔºåMA-SLAMÁöÑÊé¢Á¥¢Êó∂Èó¥ÊØîÂü∫Á∫øÊñπÊ≥ïÂáèÂ∞ë‰∫Ü30%ÔºåÊé¢Á¥¢Ë∑ùÁ¶ªÂáèÂ∞ë‰∫Ü25%„ÄÇÊ≠§Â§ñÔºåMA-SLAMËøòÂú®ÁúüÂÆûÁöÑUGVÂπ≥Âè∞‰∏äËøõË°å‰∫ÜÈ™åËØÅÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÁéØÂ¢É‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MA-SLAMÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅËá™‰∏ªÊé¢Á¥¢ÂíåÂª∫ÂõæÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÔºöÁÅæÈöæÊïëÊè¥„ÄÅÂú∞‰∏ãÁüø‰∫ïÂãòÊé¢„ÄÅÂÆ§ÂÜÖÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂÜú‰∏öÊú∫Âô®‰∫∫Á≠â„ÄÇËØ•Á†îÁ©∂ËÉΩÂ§üÊèêÂçáÊú∫Âô®‰∫∫Âú®Êú™Áü•ÁéØÂ¢É‰∏≠ÁöÑËá™‰∏ªÂØºËà™ÂíåÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods.

