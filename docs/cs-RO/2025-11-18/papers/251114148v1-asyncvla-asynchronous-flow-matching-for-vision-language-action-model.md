---
layout: default
title: AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models
---

# AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models

**arXiv**: [2511.14148v1](https://arxiv.org/abs/2511.14148) | [PDF](https://arxiv.org/pdf/2511.14148.pdf)

**ä½œè€…**: Yuhua Jiang, Shuang Cheng, Yan Ding, Feifei Gao, Biqing Qi

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/YuhuaJiang2002/AsyncVLA)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AsyncVLAï¼šé¢å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹çš„å¼‚æ­¥æµåŒ¹é…ï¼Œæå‡é•¿æ—¶ä»»åŠ¡çš„ç¨³å®šæ€§å’Œè‡ªçº é”™èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æµåŒ¹é…` `å¼‚æ­¥ç”Ÿæˆ` `æœºå™¨äººæ“ä½œ` `è‡ªçº é”™`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸVLAæ¨¡åž‹é‡‡ç”¨åŒæ­¥æµåŒ¹é…ï¼Œç¼ºä¹åŠ¨ä½œä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œå¼‚æ­¥çº é”™ï¼Œå¯¼è‡´é•¿æ—¶ä»»åŠ¡ä¸­å®¹æ˜“å‡ºé”™ã€‚
2. AsyncVLAé€šè¿‡å¼‚æ­¥æµåŒ¹é…ï¼Œå¼•å…¥æ—¶é—´çµæ´»æ€§å’ŒåŠ¨ä½œä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œå®žçŽ°åŠ¨ä½œç”Ÿæˆä¸­çš„è‡ªçº é”™ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒAsyncVLAåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šè¡¨çŽ°å‡ºæ•°æ®æ•ˆçŽ‡å’Œè‡ªçº é”™èƒ½åŠ›ï¼Œè¾¾åˆ°SOTAæ°´å¹³ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹å·²æˆä¸ºæž„å»ºé€šç”¨æœºå™¨äººçš„å¼ºå¤§èŒƒä¾‹ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„VLAæ¨¡åž‹é€šè¿‡æµåŒ¹é…(FM)ç”ŸæˆåŠ¨ä½œï¼Œé€šå¸¸ä¾èµ–äºŽåˆšæ€§å’Œç»Ÿä¸€çš„æ—¶é—´è¡¨ï¼Œå³åŒæ­¥FM(SFM)ã€‚ç”±äºŽç¼ºä¹åŠ¨ä½œä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œå¼‚æ­¥è‡ªçº é”™èƒ½åŠ›ï¼ŒSFMåœ¨é•¿æ—¶ä»»åŠ¡ä¸­å˜å¾—ä¸ç¨³å®šï¼Œå•ä¸ªåŠ¨ä½œé”™è¯¯å¯èƒ½å¯¼è‡´æ•´ä½“å¤±è´¥ã€‚æœ¬æ–‡æå‡ºäº†å¼‚æ­¥æµåŒ¹é…VLA(AsyncVLA)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æž¶ï¼Œå®ƒåœ¨å¼‚æ­¥FM(AFM)ä¸­å¼•å…¥äº†æ—¶é—´çµæ´»æ€§ï¼Œå¹¶å®žçŽ°äº†åŠ¨ä½œç”Ÿæˆä¸­çš„è‡ªçº é”™ã€‚AsyncVLAé€šè¿‡ä»¥å…·æœ‰åŠ¨ä½œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„éžå‡åŒ€æ—¶é—´è¡¨ç”ŸæˆåŠ¨ä½œtokenï¼Œæ‰“ç ´äº†VLAæ¨¡åž‹ä¸­vanilla SFMçš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†ç½®ä¿¡åº¦è¯„ä¼°å™¨æ¥æå–åˆå§‹ç”ŸæˆåŠ¨ä½œçš„ç½®ä¿¡åº¦ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿåœ¨æ‰§è¡Œå‰é€‰æ‹©æ€§åœ°ç»†åŒ–ä¸å‡†ç¡®çš„åŠ¨ä½œtokenã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†SFMå’ŒAFMçš„ç»Ÿä¸€è®­ç»ƒç¨‹åºï¼Œä½¿å•ä¸ªæ¨¡åž‹åŒæ—¶å…·å¤‡ä¸¤ç§æ¨¡å¼ï¼Œä»Žè€Œæé«˜KV-cacheçš„åˆ©ç”¨çŽ‡ã€‚åœ¨æœºå™¨äººæ“ä½œåŸºå‡†ä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒAsyncVLAå…·æœ‰æ•°æ®æ•ˆçŽ‡å’Œè‡ªçº é”™èƒ½åŠ›ã€‚ç”±äºŽAFMä¸­çš„å¼‚æ­¥ç”Ÿæˆï¼ŒAsyncVLAåœ¨é€šç”¨å…·èº«è¯„ä¼°ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æžœã€‚ä»£ç å¯åœ¨https://github.com/YuhuaJiang2002/AsyncVLAèŽ·å–ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹åœ¨ç”ŸæˆåŠ¨ä½œæ—¶ï¼Œé€šå¸¸é‡‡ç”¨åŒæ­¥æµåŒ¹é…(SFM)ï¼Œå³æŒ‰ç…§å›ºå®šçš„æ—¶é—´æ­¥ç”ŸæˆåŠ¨ä½œåºåˆ—ã€‚è¿™ç§æ–¹æ³•ç¼ºä¹å¯¹åŠ¨ä½œä¸Šä¸‹æ–‡çš„æ„ŸçŸ¥ï¼Œå¹¶ä¸”æ— æ³•è¿›è¡Œå¼‚æ­¥çš„è‡ªæˆ‘çº æ­£ã€‚å› æ­¤ï¼Œåœ¨é•¿æ—¶ä»»åŠ¡ä¸­ï¼Œä¸€æ—¦å‡ºçŽ°åŠ¨ä½œé”™è¯¯ï¼Œå°±å®¹æ˜“ç´¯ç§¯å¹¶å¯¼è‡´ä»»åŠ¡å¤±è´¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAsyncVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥å¼‚æ­¥æµåŒ¹é…(AFM)ï¼Œå…è®¸æ¨¡åž‹ä»¥éžå‡åŒ€çš„æ—¶é—´è¡¨ç”ŸæˆåŠ¨ä½œtokenã€‚é€šè¿‡åŠ¨ä½œä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œæ¨¡åž‹å¯ä»¥åŠ¨æ€åœ°è°ƒæ•´åŠ¨ä½œç”Ÿæˆçš„æ—¶é—´æ­¥é•¿ï¼Œå¹¶åœ¨å¿…è¦æ—¶å¯¹ä¸å‡†ç¡®çš„åŠ¨ä½œè¿›è¡Œä¿®æ­£ï¼Œä»Žè€Œæé«˜æ¨¡åž‹åœ¨é•¿æ—¶ä»»åŠ¡ä¸­çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šAsyncVLAæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å¼‚æ­¥æµåŒ¹é…æ¨¡å—ï¼šè´Ÿè´£ä»¥éžå‡åŒ€çš„æ—¶é—´è¡¨ç”ŸæˆåŠ¨ä½œtokenã€‚2) ç½®ä¿¡åº¦è¯„ä¼°å™¨ï¼šç”¨äºŽè¯„ä¼°ç”ŸæˆåŠ¨ä½œçš„ç½®ä¿¡åº¦ï¼Œå¹¶é€‰æ‹©æ€§åœ°å¯¹ä½Žç½®ä¿¡åº¦çš„åŠ¨ä½œè¿›è¡Œä¿®æ­£ã€‚3) ç»Ÿä¸€è®­ç»ƒç¨‹åºï¼šæ”¯æŒSFMå’ŒAFMä¸¤ç§æ¨¡å¼çš„è”åˆè®­ç»ƒï¼Œæé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’ŒKV-cacheçš„åˆ©ç”¨çŽ‡ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œæ¨¡åž‹é¦–å…ˆæ ¹æ®è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œé€šè¿‡å¼‚æ­¥æµåŒ¹é…ç”Ÿæˆåˆå§‹åŠ¨ä½œåºåˆ—ã€‚ç„¶åŽï¼Œç½®ä¿¡åº¦è¯„ä¼°å™¨è¯„ä¼°æ¯ä¸ªåŠ¨ä½œçš„ç½®ä¿¡åº¦ï¼Œå¹¶å¯¹ä½Žç½®ä¿¡åº¦çš„åŠ¨ä½œè¿›è¡Œè¿­ä»£ä¿®æ­£ã€‚æœ€åŽï¼Œæ¨¡åž‹è¾“å‡ºä¿®æ­£åŽçš„åŠ¨ä½œåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šAsyncVLAæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽå¼•å…¥äº†å¼‚æ­¥æµåŒ¹é…(AFM)ï¼Œæ‰“ç ´äº†ä¼ ç»ŸVLAæ¨¡åž‹ä¸­åŒæ­¥æµåŒ¹é…(SFM)çš„é™åˆ¶ã€‚AFMå…è®¸æ¨¡åž‹ä»¥éžå‡åŒ€çš„æ—¶é—´è¡¨ç”ŸæˆåŠ¨ä½œtokenï¼Œå¹¶æ ¹æ®åŠ¨ä½œä¸Šä¸‹æ–‡è¿›è¡ŒåŠ¨æ€è°ƒæ•´å’Œè‡ªæˆ‘çº æ­£ï¼Œä»Žè€Œæé«˜äº†æ¨¡åž‹åœ¨é•¿æ—¶ä»»åŠ¡ä¸­çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šAsyncVLAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) éžå‡åŒ€æ—¶é—´è¡¨çš„ç”Ÿæˆç­–ç•¥ï¼šæ ¹æ®åŠ¨ä½œä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ—¶é—´æ­¥é•¿ã€‚2) ç½®ä¿¡åº¦è¯„ä¼°å™¨çš„è®¾è®¡ï¼šé‡‡ç”¨ç¥žç»ç½‘ç»œé¢„æµ‹åŠ¨ä½œçš„ç½®ä¿¡åº¦ã€‚3) ç»Ÿä¸€è®­ç»ƒç¨‹åºçš„è®¾è®¡ï¼šé€šè¿‡å…±äº«å‚æ•°å’ŒæŸå¤±å‡½æ•°ï¼Œå®žçŽ°SFMå’ŒAFMçš„è”åˆè®­ç»ƒã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æµåŒ¹é…æŸå¤±å’Œç½®ä¿¡åº¦é¢„æµ‹æŸå¤±ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

AsyncVLAåœ¨æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒAsyncVLAåœ¨æ•°æ®æ•ˆçŽ‡å’Œè‡ªçº é”™èƒ½åŠ›æ–¹é¢å‡ä¼˜äºŽä¼ ç»Ÿçš„åŒæ­¥æµåŒ¹é…æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒAsyncVLAåœ¨å¤šä¸ªé•¿æ—¶ä»»åŠ¡ä¸Šçš„æˆåŠŸçŽ‡æé«˜äº†XX%ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°çº æ­£åˆå§‹åŠ¨ä½œä¸­çš„é”™è¯¯ï¼Œä»Žè€Œé¿å…ä»»åŠ¡å¤±è´¥ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

AsyncVLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºŽå„ç§éœ€è¦é•¿æ—¶é—´è§„åˆ’å’Œç¨³å®šæ‰§è¡Œçš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ï¼ŒAsyncVLAæœ‰æœ›æŽ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•å’Œåº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.

