---
layout: default
title: Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control
---

# Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control

**arXiv**: [2512.00050v1](https://arxiv.org/abs/2512.00050) | [PDF](https://arxiv.org/pdf/2512.00050.pdf)

**ä½œè€…**: Suzie Kim

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

**å¤‡æ³¨**: Master's thesis, Korea University, 2025. arXiv admin note: substantial text overlap with arXiv:2507.13171

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽéšå¼ç¥žç»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºŽäººæœºåä½œæœºå™¨äººæŽ§åˆ¶**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººæœºäº¤äº’` `è„‘ç”µä¿¡å·` `éšå¼åé¦ˆ` `æœºå™¨äººæŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨ç¨€ç–å¥–åŠ±ä¸‹è¡¨çŽ°ä¸ä½³ï¼Œä¾èµ–äººå·¥è®¾è®¡çš„å¤æ‚å¥–åŠ±å‡½æ•°ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æå‡ºRLIHFæ¡†æž¶ï¼Œåˆ©ç”¨è„‘ç”µä¿¡å·ï¼ˆErrPï¼‰ä½œä¸ºéšå¼åé¦ˆï¼Œæ— éœ€ç”¨æˆ·æ˜¾å¼å¹²é¢„ï¼Œå®žçŽ°è¿žç»­è¯„ä¼°ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒåŸºäºŽEEGåé¦ˆè®­ç»ƒçš„æ™ºèƒ½ä½“åœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸­è¾¾åˆ°ä¸Žå¯†é›†å¥–åŠ±è®­ç»ƒæ™ºèƒ½ä½“ç›¸å½“çš„æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç¨€ç–å¥–åŠ±çŽ¯å¢ƒä¸‹éš¾ä»¥å­¦ä¹ æœ‰æ•ˆçš„ç­–ç•¥ï¼Œé€šå¸¸éœ€è¦æ‰‹åŠ¨è®¾è®¡å¤æ‚çš„ã€ç‰¹å®šäºŽä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»Žäººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åº”è¿è€Œç”Ÿï¼Œå®ƒç”¨äººç±»æä¾›çš„è¯„ä¼°ä¿¡å·æ¥è¡¥å……æ‰‹å·¥è®¾è®¡çš„å¥–åŠ±ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°çŽ°æœ‰çš„RLHFæ–¹æ³•ä¾èµ–äºŽæ˜¾å¼åé¦ˆæœºåˆ¶ï¼Œä¾‹å¦‚æŒ‰é’®æŒ‰ä¸‹æˆ–åå¥½æ ‡ç­¾ï¼Œè¿™ä¼šä¸­æ–­è‡ªç„¶äº¤äº’è¿‡ç¨‹å¹¶ç»™ç”¨æˆ·å¸¦æ¥å·¨å¤§çš„è®¤çŸ¥è´Ÿæ‹…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»Žéšå¼äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLIHFï¼‰çš„æ¡†æž¶ï¼Œè¯¥æ¡†æž¶åˆ©ç”¨éžä¾µå…¥æ€§è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¿¡å·ï¼Œç‰¹åˆ«æ˜¯é”™è¯¯ç›¸å…³ç”µä½ï¼ˆErrPï¼‰ï¼Œæ¥æä¾›è¿žç»­çš„ã€éšå¼çš„åé¦ˆï¼Œè€Œæ— éœ€æ˜¾å¼çš„ç”¨æˆ·å¹²é¢„ã€‚è¯¥æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒçš„è§£ç å™¨å°†åŽŸå§‹EEGä¿¡å·è½¬æ¢ä¸ºæ¦‚çŽ‡å¥–åŠ±åˆ†é‡ï¼Œä»Žè€Œå³ä½¿åœ¨å­˜åœ¨ç¨€ç–å¤–éƒ¨å¥–åŠ±çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®žçŽ°æœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨åŸºäºŽMuJoCoç‰©ç†å¼•æ“Žçš„ä»¿çœŸçŽ¯å¢ƒä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä½¿ç”¨Kinova Gen2æœºæ¢°è‡‚æ‰§è¡Œå¤æ‚çš„æŠ“å–å’Œæ”¾ç½®ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡éœ€è¦é¿å¼€éšœç¢ç‰©åŒæ—¶æ“ä½œç›®æ ‡å¯¹è±¡ã€‚ç»“æžœè¡¨æ˜Žï¼Œä½¿ç”¨è§£ç åŽçš„EEGåé¦ˆè®­ç»ƒçš„æ™ºèƒ½ä½“å®žçŽ°äº†ä¸Žä½¿ç”¨å¯†é›†ã€æ‰‹åŠ¨è®¾è®¡çš„å¥–åŠ±è®­ç»ƒçš„æ™ºèƒ½ä½“ç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›å‘çŽ°éªŒè¯äº†ä½¿ç”¨éšå¼ç¥žç»åé¦ˆåœ¨äº¤äº’å¼æœºå™¨äººæŠ€æœ¯ä¸­å®žçŽ°å¯æ‰©å±•ä¸”ä¸Žäººç±»å¯¹é½çš„å¼ºåŒ–å­¦ä¹ çš„æ½œåŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç¨€ç–å¥–åŠ±çŽ¯å¢ƒä¸‹éš¾ä»¥æœ‰æ•ˆå­¦ä¹ ï¼Œéœ€è¦äººå·¥è®¾è®¡å¤æ‚çš„å¥–åŠ±å‡½æ•°ã€‚è€ŒçŽ°æœ‰RLHFæ–¹æ³•ä¾èµ–æ˜¾å¼åé¦ˆï¼Œä¸­æ–­äººæœºäº¤äº’ï¼Œå¢žåŠ ç”¨æˆ·è®¤çŸ¥è´Ÿæ‹…ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿåˆ©ç”¨éšå¼åé¦ˆä¿¡å·è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥å®žçŽ°æ›´è‡ªç„¶ã€é«˜æ•ˆçš„äººæœºåä½œã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨éžä¾µå…¥å¼è„‘ç”µä¿¡å·ï¼ˆEEGï¼‰ä¸­çš„é”™è¯¯ç›¸å…³ç”µä½ï¼ˆErrPï¼‰ä½œä¸ºéšå¼åé¦ˆä¿¡å·ï¼Œä»£æ›¿æ˜¾å¼çš„ç”¨æˆ·è¾“å…¥ã€‚é€šè¿‡è§£ç ErrPä¿¡å·ï¼Œå°†å…¶è½¬åŒ–ä¸ºæ¦‚çŽ‡å¥–åŠ±åˆ†é‡ï¼Œä»Žè€Œåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å‡å°‘ç”¨æˆ·è®¤çŸ¥è´Ÿæ‹…ï¼Œå¹¶å®žçŽ°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥RLIHFæ¡†æž¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) æœºå™¨äººçŽ¯å¢ƒï¼ˆMuJoCoä»¿çœŸï¼‰ï¼›2) æ™ºèƒ½ä½“ï¼ˆåŸºäºŽå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼‰ï¼›3) EEGä¿¡å·é‡‡é›†è®¾å¤‡ï¼›4) é¢„è®­ç»ƒçš„EEGè§£ç å™¨ï¼ˆå°†EEGä¿¡å·è½¬æ¢ä¸ºæ¦‚çŽ‡å¥–åŠ±ï¼‰ï¼›5) å¥–åŠ±å‡½æ•°ï¼ˆç»“åˆå¤–éƒ¨å¥–åŠ±å’Œè§£ç åŽçš„EEGå¥–åŠ±ï¼‰ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šæ™ºèƒ½ä½“åœ¨çŽ¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œï¼Œç”¨æˆ·è§‚å¯Ÿå¹¶äº§ç”ŸErrPä¿¡å·ï¼ŒEEGè§£ç å™¨å°†ErrPä¿¡å·è½¬æ¢ä¸ºå¥–åŠ±ï¼Œæ™ºèƒ½ä½“æ ¹æ®å¥–åŠ±æ›´æ–°ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºŽåˆ©ç”¨éšå¼ç¥žç»åé¦ˆï¼ˆErrPï¼‰ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ã€‚ä¸Žä¼ ç»Ÿçš„æ˜¾å¼åé¦ˆæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€ç”¨æˆ·ä¸»åŠ¨æä¾›åé¦ˆï¼Œä»Žè€Œå‡å°‘äº†ç”¨æˆ·è®¤çŸ¥è´Ÿæ‹…ï¼Œå®žçŽ°äº†æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚æ­¤å¤–ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è§£ç å™¨å°†EEGä¿¡å·è½¬æ¢ä¸ºæ¦‚çŽ‡å¥–åŠ±ï¼Œæé«˜äº†å¥–åŠ±ä¿¡å·çš„å¯é æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šä½¿ç”¨äº†é¢„è®­ç»ƒçš„EEGè§£ç å™¨ï¼Œè¯¥è§£ç å™¨å°†åŽŸå§‹EEGä¿¡å·æ˜ å°„åˆ°æ¦‚çŽ‡å¥–åŠ±åˆ†é‡ã€‚å…·ä½“æ¥è¯´ï¼Œè§£ç å™¨å¯èƒ½æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œç”¨äºŽåŒºåˆ†â€œæ­£ç¡®â€å’Œâ€œé”™è¯¯â€çš„è„‘ç”µä¿¡å·æ¨¡å¼ã€‚å¥–åŠ±å‡½æ•°å°†å¤–éƒ¨å¥–åŠ±ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰ä¸Žè§£ç åŽçš„EEGå¥–åŠ±ç›¸ç»“åˆï¼Œå½¢æˆæœ€ç»ˆçš„å¥–åŠ±ä¿¡å·ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå…·ä½“ç®—æ³•æœªçŸ¥ï¼ŒåŽŸæ–‡æœªæåŠï¼‰åˆ©ç”¨è¯¥å¥–åŠ±ä¿¡å·æ¥æ›´æ–°æ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚Kinova Gen2æœºæ¢°è‡‚åœ¨MuJoCoçŽ¯å¢ƒä¸­æ‰§è¡ŒæŠ“å–å’Œæ”¾ç½®ä»»åŠ¡ï¼Œéœ€è¦é¿å¼€éšœç¢ç‰©ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œä½¿ç”¨è§£ç åŽçš„EEGåé¦ˆè®­ç»ƒçš„æ™ºèƒ½ä½“åœ¨MuJoCoä»¿çœŸçŽ¯å¢ƒä¸­ï¼Œæ‰§è¡Œå¤æ‚çš„æŠ“å–å’Œæ”¾ç½®ä»»åŠ¡æ—¶ï¼Œå…¶æ€§èƒ½ä¸Žä½¿ç”¨å¯†é›†ã€æ‰‹åŠ¨è®¾è®¡çš„å¥–åŠ±å‡½æ•°è®­ç»ƒçš„æ™ºèƒ½ä½“ç›¸å½“ã€‚è¿™è¡¨æ˜Žï¼Œå³ä½¿åœ¨æ²¡æœ‰æ˜¾å¼å¥–åŠ±ä¿¡å·çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨éšå¼ç¥žç»åé¦ˆä¹Ÿèƒ½æœ‰æ•ˆåœ°è®­ç»ƒæ™ºèƒ½ä½“ï¼Œå®žçŽ°ä¸Žäººç±»å¯¹é½çš„æœºå™¨äººæŽ§åˆ¶ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽäººæœºåä½œæœºå™¨äººã€åº·å¤æœºå™¨äººã€è¾…åŠ©è®¾å¤‡ç­‰é¢†åŸŸã€‚é€šè¿‡åˆ©ç”¨ç”¨æˆ·çš„éšå¼ç¥žç»åé¦ˆï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œå¹¶åšå‡ºç›¸åº”çš„è°ƒæ•´ï¼Œä»Žè€Œæé«˜äººæœºäº¤äº’çš„æ•ˆçŽ‡å’Œè‡ªç„¶æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºŽæ›´å¹¿æ³›çš„é¢†åŸŸï¼Œä¾‹å¦‚æ™ºèƒ½å®¶å±…ã€è™šæ‹ŸçŽ°å®žç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Conventional reinforcement learning (RL) approaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, reinforcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, enabling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.

