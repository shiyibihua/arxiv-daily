---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-06-11
---

# cs.ROï¼ˆ2025-06-11ï¼‰

ğŸ“Š å…± **22** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (15 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250609384v1-analyzing-key-objectives-in-human-to-robot-retargeting-for-dexterous.html">Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation</a></td>
  <td>æå‡ºå…¨é¢çš„é‡å®šå‘ç›®æ ‡ä»¥è§£å†³äººæœºçµå·§æ“ä½œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous manipulation</span> <span class="paper-tag">teleoperation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09384v1" data-paper-url="./papers/250609384v1-analyzing-key-objectives-in-human-to-robot-retargeting-for-dexterous.html" onclick="toggleFavorite(this, '2506.09384v1', 'Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250609366v1-skillblender-towards-versatile-humanoid-whole-body-loco-manipulation.html">SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending</a></td>
  <td>æå‡ºSkillBlenderä»¥è§£å†³äººå½¢æœºå™¨äººå¤šä»»åŠ¡æ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">whole-body control</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09366v1" data-paper-url="./papers/250609366v1-skillblender-towards-versatile-humanoid-whole-body-loco-manipulation.html" onclick="toggleFavorite(this, '2506.09366v1', 'SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250609979v2-locomotion-on-constrained-footholds-via-layered-architectures-and-mo.html">Locomotion on Constrained Footholds via Layered Architectures and Model Predictive Control</a></td>
  <td>æå‡ºåˆ†å±‚æ¶æ„ä¸æ¨¡å‹é¢„æµ‹æ§åˆ¶ä»¥è§£å†³è…¿éƒ¨æœºå™¨äººè¿åŠ¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged locomotion</span> <span class="paper-tag">humanoid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09979v2" data-paper-url="./papers/250609979v2-locomotion-on-constrained-footholds-via-layered-architectures-and-mo.html" onclick="toggleFavorite(this, '2506.09979v2', 'Locomotion on Constrained Footholds via Layered Architectures and Model Predictive Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250609588v1-attention-based-map-encoding-for-learning-generalized-legged-locomot.html">Attention-Based Map Encoding for Learning Generalized Legged Locomotion</a></td>
  <td>æå‡ºåŸºäºæ³¨æ„åŠ›çš„åœ°å›¾ç¼–ç ä»¥è§£å†³è…¿éƒ¨æœºå™¨äººé€šç”¨è¿åŠ¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged robot</span> <span class="paper-tag">legged locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09588v1" data-paper-url="./papers/250609588v1-attention-based-map-encoding-for-learning-generalized-legged-locomot.html" onclick="toggleFavorite(this, '2506.09588v1', 'Attention-Based Map Encoding for Learning Generalized Legged Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250609383v2-bipedal-balance-control-with-whole-body-musculoskeletal-standing-and.html">Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations</a></td>
  <td>æå‡ºåˆ†å±‚æ§åˆ¶ç®¡é“ä»¥æ¨¡æ‹Ÿäººç±»å¹³è¡¡è¡Œä¸º</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">bipedal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09383v2" data-paper-url="./papers/250609383v2-bipedal-balance-control-with-whole-body-musculoskeletal-standing-and.html" onclick="toggleFavorite(this, '2506.09383v2', 'Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250609937v2-safe-multitask-failure-detection-for-vision-language-action-models.html">SAFE: Multitask Failure Detection for Vision-Language-Action Models</a></td>
  <td>æå‡ºSAFEä»¥è§£å†³å¤šä»»åŠ¡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å¤±è´¥æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09937v2" data-paper-url="./papers/250609937v2-safe-multitask-failure-detection-for-vision-language-action-models.html" onclick="toggleFavorite(this, '2506.09937v2', 'SAFE: Multitask Failure Detection for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250609422v1-time-unified-diffusion-policy-with-action-discrimination-for-robotic.html">Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation</a></td>
  <td>æå‡ºæ—¶é—´ç»Ÿä¸€æ‰©æ•£ç­–ç•¥ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„å®æ—¶å“åº”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09422v1" data-paper-url="./papers/250609422v1-time-unified-diffusion-policy-with-action-discrimination-for-robotic.html" onclick="toggleFavorite(this, '2506.09422v1', 'Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250609406v1-scoop-and-toss-dynamic-object-collection-for-quadrupedal-systems.html">Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems</a></td>
  <td>æå‡ºåŠ¨æ€ç‰©ä½“æ”¶é›†æ¡†æ¶ä»¥è§£å†³å››è¶³æœºå™¨äººæ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09406v1" data-paper-url="./papers/250609406v1-scoop-and-toss-dynamic-object-collection-for-quadrupedal-systems.html" onclick="toggleFavorite(this, '2506.09406v1', 'Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250609990v1-chain-of-action-trajectory-autoregressive-modeling-for-robotic-manip.html">Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation</a></td>
  <td>æå‡ºChain-of-Actionä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„è½¨è¿¹ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09990v1" data-paper-url="./papers/250609990v1-chain-of-action-trajectory-autoregressive-modeling-for-robotic-manip.html" onclick="toggleFavorite(this, '2506.09990v1', 'Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250609491v1-dcirnet-depth-completion-with-iterative-refinement-for-dexterous-gra.html">DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects</a></td>
  <td>æå‡ºDCIRNetä»¥è§£å†³é€æ˜å’Œåå°„ç‰©ä½“çš„æ·±åº¦è¡¥å…¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09491v1" data-paper-url="./papers/250609491v1-dcirnet-depth-completion-with-iterative-refinement-for-dexterous-gra.html" onclick="toggleFavorite(this, '2506.09491v1', 'DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250609859v2-hierarchical-learning-enhanced-mpc-for-safe-crowd-navigation-with-he.html">Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints</a></td>
  <td>æå‡ºå±‚æ¬¡å­¦ä¹ å¢å¼ºçš„MPCä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„å®‰å…¨äººç¾¤å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09859v2" data-paper-url="./papers/250609859v2-hierarchical-learning-enhanced-mpc-for-safe-crowd-navigation-with-he.html" onclick="toggleFavorite(this, '2506.09859v2', 'Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250609552v1-enhancing-human-robot-collaboration-a-sim2real-domain-adaptation-alg.html">Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments</a></td>
  <td>æå‡ºSim2Realé¢†åŸŸé€‚åº”ç®—æ³•ä»¥å¢å¼ºäººæœºåä½œä¸­çš„ç‚¹äº‘åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09552v1" data-paper-url="./papers/250609552v1-enhancing-human-robot-collaboration-a-sim2real-domain-adaptation-alg.html" onclick="toggleFavorite(this, '2506.09552v1', 'Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250610106v1-one-for-all-llm-based-heterogeneous-mission-planning-in-precision-ag.html">One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture</a></td>
  <td>æå‡ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼‚æ„æœºå™¨äººä»»åŠ¡è§„åˆ’ç³»ç»Ÿä»¥è§£å†³ç²¾å‡†å†œä¸šä¸­çš„å¤æ‚æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10106v1" data-paper-url="./papers/250610106v1-one-for-all-llm-based-heterogeneous-mission-planning-in-precision-ag.html" onclick="toggleFavorite(this, '2506.10106v1', 'One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250609548v2-tightly-coupled-lidar-imu-leg-odometry-with-online-learned-leg-kinem.html">Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information</a></td>
  <td>æå‡ºç´§è€¦åˆLiDAR-IMU-è…¿éƒ¨é‡Œç¨‹è®¡ä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09548v2" data-paper-url="./papers/250609548v2-tightly-coupled-lidar-imu-leg-odometry-with-online-learned-leg-kinem.html" onclick="toggleFavorite(this, '2506.09548v2', 'Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250609494v1-advances-on-affordable-hardware-platforms-for-human-demonstration-ac.html">Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications</a></td>
  <td>æå‡ºä½æˆæœ¬æ‰‹æŒæŠ“å–å™¨ä»¥æå‡å†œä¸šåœºæ™¯ä¸­çš„ç¤ºèŒƒé‡‡é›†æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09494v1" data-paper-url="./papers/250609494v1-advances-on-affordable-hardware-platforms-for-human-demonstration-ac.html" onclick="toggleFavorite(this, '2506.09494v1', 'Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250609623v1-analytic-task-scheduler-recursive-least-squares-based-method-for-con.html">Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models</a></td>
  <td>æå‡ºåˆ†æä»»åŠ¡è°ƒåº¦å™¨ä»¥è§£å†³æŒç»­å­¦ä¹ ä¸­çš„é—å¿˜é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09623v1" data-paper-url="./papers/250609623v1-analytic-task-scheduler-recursive-least-squares-based-method-for-con.html" onclick="toggleFavorite(this, '2506.09623v1', 'Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250610172v1-a-navigation-framework-utilizing-vision-language-models.html">A Navigation Framework Utilizing Vision-Language Models</a></td>
  <td>æå‡ºæ¨¡å—åŒ–å¯¼èˆªæ¡†æ¶ä»¥è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªæŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span> <span class="paper-tag">VLN</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10172v1" data-paper-url="./papers/250610172v1-a-navigation-framework-utilizing-vision-language-models.html" onclick="toggleFavorite(this, '2506.10172v1', 'A Navigation Framework Utilizing Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250610098v3-estimating-the-joint-probability-of-scenario-parameters-with-gaussia.html">Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models</a></td>
  <td>æå‡ºé«˜æ–¯æ··åˆCopulaæ¨¡å‹ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶åœºæ™¯å‚æ•°è”åˆæ¦‚ç‡ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10098v3" data-paper-url="./papers/250610098v3-estimating-the-joint-probability-of-scenario-parameters-with-gaussia.html" onclick="toggleFavorite(this, '2506.10098v3', 'Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250610093v1-leveraging-llms-for-mission-planning-in-precision-agriculture.html">Leveraging LLMs for Mission Planning in Precision Agriculture</a></td>
  <td>æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡è§„åˆ’ç³»ç»Ÿä»¥è§£å†³ç²¾å‡†å†œä¸šä¸­çš„æœºå™¨äººä»»åŠ¡åˆ†é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10093v1" data-paper-url="./papers/250610093v1-leveraging-llms-for-mission-planning-in-precision-agriculture.html" onclick="toggleFavorite(this, '2506.10093v1', 'Leveraging LLMs for Mission Planning in Precision Agriculture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250609581v1-integrating-quantized-llms-into-robotics-systems-as-edge-ai-to-lever.html">Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities</a></td>
  <td>æå‡ºllama_rosä»¥è§£å†³æœºå™¨äººç³»ç»Ÿä¸­çš„è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09581v1" data-paper-url="./papers/250609581v1-integrating-quantized-llms-into-robotics-systems-as-edge-ai-to-lever.html" onclick="toggleFavorite(this, '2506.09581v1', 'Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250609930v1-from-intention-to-execution-probing-the-generalization-boundaries-of.html">From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models</a></td>
  <td>æå‡ºç»Ÿä¸€è¯„ä¼°å¥—ä»¶ä»¥è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09930v1" data-paper-url="./papers/250609930v1-from-intention-to-execution-probing-the-generalization-boundaries-of.html" onclick="toggleFavorite(this, '2506.09930v1', 'From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250609583v1-vault-a-mobile-mapping-system-for-ros-2-based-autonomous-robots.html">VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots</a></td>
  <td>æå‡ºVAULTä»¥è§£å†³è‡ªä¸»æœºå™¨äººå®¤å¤–å®šä½ä¸æ˜ å°„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span> <span class="paper-tag">VIO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09583v1" data-paper-url="./papers/250609583v1-vault-a-mobile-mapping-system-for-ros-2-based-autonomous-robots.html" onclick="toggleFavorite(this, '2506.09583v1', 'VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)