---
layout: default
title: From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models
---

# From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09930" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.09930v1</a>
  <a href="https://arxiv.org/pdf/2506.09930.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09930v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09930v1', 'From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Irving Fang, Juexiao Zhang, Shengbang Tong, Chen Feng

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-11

**Â§áÊ≥®**: Under review

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://ai4ce.github.io/INT-ACT/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Áªü‰∏ÄËØÑ‰º∞Â•ó‰ª∂‰ª•Ëß£ÂÜ≥ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Ê≥õÂåñËÉΩÂäõ` `ËØÑ‰º∞Â•ó‰ª∂` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `‰ªªÂä°ËÆæËÆ°` `Êô∫ËÉΩÊú∫Âô®‰∫∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÂΩìÂâçÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãËØÑ‰º∞‰∏çË∂≥ÔºåÁº∫‰πèÊúâÊïàÁöÑËØ≠Ë®ÄÊåá‰ª§ÂíåÂ§öÊ†∑ÂåñÁöÑ‰ªªÂä°ÔºåÈôêÂà∂‰∫ÜÂÖ∂Ê≥õÂåñËÉΩÂäõÁöÑÁ†îÁ©∂„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´50‰∏™Ê®°Êãü‰ªªÂä°ÁöÑÁªü‰∏ÄËØÑ‰º∞Â•ó‰ª∂ÔºåÊó®Âú®Á≥ªÁªüÊÄßÂú∞ËØÑ‰º∞VLAÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°VLAÊ®°ÂûãÂú®ÊÑüÁü•ÁêÜËß£‰∏äË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Âä®‰ΩúÊâßË°å‰∏äÂ≠òÂú®ÊòæËëó‰∏çË∂≥Ôºå‰∏îÂæÆË∞ÉÂèØËÉΩÂâäÂº±ÂÖ∂Êé®ÁêÜËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁõ∏ËæÉ‰∫é‰º†ÁªüÊ®°‰ªøÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫È¢ÜÂüüÂÖ∑ÊúâÊõ¥ÂπøÊ≥õÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÂØπVLAÁöÑËØÑ‰º∞‰ªçÊòæ‰∏çË∂≥ÔºåÁº∫‰πèÊúâÊïàÁöÑËØ≠Ë®ÄÊåá‰ª§ÂíåÂ§öÊ†∑ÂåñÁöÑËØÑ‰º∞‰ªªÂä°„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´50‰∏™Ê®°Êãü‰ªªÂä°ÁöÑÁªü‰∏ÄËØÑ‰º∞Â•ó‰ª∂ÔºåÊ∂µÁõñËØ≠Ë®ÄÊåá‰ª§„ÄÅËßÜËßâÂíåÁâ©‰ΩìÁ≠â10‰∏™Â≠êÁ±ªÂà´„ÄÇÈÄöËøáÂØπÂ§öÁßçÊúÄÂÖàËøõÁöÑVLAÊû∂ÊûÑËøõË°åÁ≥ªÁªüËØÑ‰º∞ÔºåÂèëÁé∞Â∞ΩÁÆ°VLMÈ™®Âπ≤ÁΩëÁªúËµã‰∫àVLAÂº∫Â§ßÁöÑÊÑüÁü•ÁêÜËß£ÂíåÈ´òÊ∞¥Âπ≥ËßÑÂàíËÉΩÂäõÔºå‰ΩÜÂú®Èù¢ÂØπÂàÜÂ∏ÉÂ§ñËßÇÂØüÊó∂ÔºåÊîøÁ≠ñÁöÑÂä®‰ΩúÊâßË°åÂç¥Â∏∏Â∏∏‰∏çÂ§üÁ≤æÁ°Æ„ÄÇÊàë‰ª¨ÂèëÂ∏É‰∫ÜËØ•‰ªªÂä°Â•ó‰ª∂ÂíåËØÑ‰º∞‰ª£Á†ÅÔºå‰ª•Êé®Âä®Êú™Êù•VLAÁ†îÁ©∂ÁöÑÊ†áÂáÜÂåñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂΩìÂâçËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºàVLAÔºâËØÑ‰º∞‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®Áº∫‰πèËØ≠Ë®ÄÊåá‰ª§ÂíåÂ§öÊ†∑Âåñ‰ªªÂä°ÁöÑÊÉÖÂÜµ‰∏ãÔºåÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàËØÑ‰º∞Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰∏Ä‰∏™Áªü‰∏ÄÁöÑËØÑ‰º∞Â•ó‰ª∂ÔºåÂåÖÂê´50‰∏™Ê®°Êãü‰ªªÂä°ÔºåË¶ÜÁõñËØ≠Ë®ÄÊåá‰ª§„ÄÅËßÜËßâÂíåÁâ©‰ΩìÁ≠âÂ§ö‰∏™Áª¥Â∫¶Ôºå‰ª•Á≥ªÁªüÊÄßÂú∞ËØÑ‰º∞VLAÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØÑ‰º∞Â•ó‰ª∂ÂàÜ‰∏∫10‰∏™Â≠êÁ±ªÂà´ÔºåÊØè‰∏™Á±ªÂà´ÂåÖÂê´‰∏çÂêåÁöÑ‰ªªÂä°ÔºåÊó®Âú®ÂÖ®Èù¢ËÄÉÂØüVLAÊ®°ÂûãÂú®Â§öÁßçÂú∫ÊôØ‰∏ãÁöÑË°®Áé∞„ÄÇÈÄöËøáÂØπÂ§öÁßçVLAÊû∂ÊûÑËøõË°åËØÑ‰º∞ÔºåÂàÜÊûêÂÖ∂Âú®ÊÑüÁü•ÂíåÂä®‰ΩúÊâßË°å‰∏äÁöÑËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨Á†îÁ©∂ÁöÑÂàõÊñ∞Âú®‰∫éÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Ê†áÂáÜÂåñÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÂ°´Ë°•‰∫ÜÁé∞ÊúâVLAËØÑ‰º∞ÁöÑÁ©∫ÁôΩÔºåÁâπÂà´ÊòØÂú®ËØ≠Ë®ÄÊåá‰ª§‰∏éÂä®‰ΩúÊâßË°å‰πãÈó¥ÁöÑÂÖ≥ËÅîÊÄßÁ†îÁ©∂‰∏ä„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËØÑ‰º∞ËøáÁ®ã‰∏≠ÔºåÈááÁî®‰∫ÜÂ§öÁßç‰ªªÂä°ËÆæËÆ°ÔºåÁ°Æ‰øù‰ªªÂä°ÁöÑÂ§öÊ†∑ÊÄßÂíåÊåëÊàòÊÄßÔºåÂêåÊó∂ÂÖ≥Ê≥®Ê®°ÂûãÂú®Èù¢ÂØπÂàÜÂ∏ÉÂ§ñËßÇÂØüÊó∂ÁöÑË°®Áé∞Ôºå‰ª•Êè≠Á§∫ÂÖ∂Ê≥õÂåñËÉΩÂäõÁöÑÁúüÂÆûËæπÁïå„ÄÇÂÆûÈ™å‰∏≠ËøòËÄÉËôë‰∫ÜÂæÆË∞ÉÂØπÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂ∞ΩÁÆ°VLAÊ®°ÂûãÂú®ÊÑüÁü•ÁêÜËß£ÂíåÈ´òÂ±ÇËßÑÂàí‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Èù¢ÂØπÂàÜÂ∏ÉÂ§ñËßÇÂØüÊó∂ÔºåÂä®‰ΩúÊâßË°åÁöÑÂáÜÁ°ÆÊÄßÊòæËëó‰∏ãÈôç„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊ®°ÂûãÂú®Êüê‰∫õ‰ªªÂä°‰∏äÁöÑË°®Áé∞‰∏éÂü∫Á∫øÁõ∏ÊØîÊèêÂçá‰∫Ü20%Ôºå‰ΩÜÂú®Âä®‰ΩúÊâßË°åÁöÑÁ≤æÁ°ÆÊÄß‰∏ä‰ªçÂ≠òÂú®ÊòéÊòæ‰∏çË∂≥„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®ÂåñÁ≥ªÁªüÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠â„ÄÇÈÄöËøáÊèêÂçáVLAÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Êô∫ËÉΩÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫ÔºåÊé®Âä®Êô∫ËÉΩÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, "generalist" robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLM's generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/

