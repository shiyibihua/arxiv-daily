---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-11-03
---

# cs.ROï¼ˆ2025-11-03ï¼‰

ğŸ“Š å…± **25** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (17 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (7)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (17 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251101774v1-mobius-a-multi-modal-bipedal-robot-that-can-walk-crawl-climb-and-rol.html">MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll</a></td>
  <td>MOBIUSï¼šä¸€ç§å¯æ­¥è¡Œã€çˆ¬è¡Œã€æ”€çˆ¬å’Œæ»šåŠ¨çš„å¤šæ¨¡æ€åŒè¶³æœºå™¨äºº</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01774v1" onclick="toggleFavorite(this, '2511.01774v1', 'MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251101177v2-scaling-cross-embodiment-world-models-for-dexterous-manipulation.html">Scaling Cross-Embodiment World Models for Dexterous Manipulation</a></td>
  <td>æå‡ºåŸºäºç²’å­ä½ç§»çš„è·¨å½¢æ€ä¸–ç•Œæ¨¡å‹ï¼Œå®ç°çµå·§æ“ä½œçš„æ³›åŒ–ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01177v2" onclick="toggleFavorite(this, '2511.01177v2', 'Scaling Cross-Embodiment World Models for Dexterous Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251101770v1-lightweight-learning-from-actuation-space-demonstrations-via-flow-ma.html">Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping</a></td>
  <td>æå‡ºåŸºäºæµåŒ¹é…çš„è½»é‡çº§é©±åŠ¨ç©ºé—´å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå…¨èº«è½¯ä½“æœºå™¨äººæŠ“å–</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01770v1" onclick="toggleFavorite(this, '2511.01770v1', 'Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251101224v1-embodiment-transfer-learning-for-vision-language-action-models.html">Embodiment Transfer Learning for Vision-Language-Action Models</a></td>
  <td>æå‡ºET-VLAæ¡†æ¶ï¼Œé€šè¿‡å…·èº«è¿ç§»å­¦ä¹ æå‡VLAæ¨¡å‹åœ¨å¤šæœºå™¨äººåä½œä¸­çš„æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01224v1" onclick="toggleFavorite(this, '2511.01224v1', 'Embodiment Transfer Learning for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251101520v1-phy-tac-toward-human-like-grasping-via-physics-conditioned-tactile-g.html">Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals</a></td>
  <td>æå‡ºåŸºäºç‰©ç†çº¦æŸè§¦è§‰ç›®æ ‡çš„åŠ›æœ€ä¼˜ç¨³å®šæŠ“å–æ–¹æ³•Phy-Tac</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01520v1" onclick="toggleFavorite(this, '2511.01520v1', 'Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251101791v1-gendexhand-generative-simulation-for-dexterous-hands.html">GenDexHand: Generative Simulation for Dexterous Hands</a></td>
  <td>GenDexHandï¼šé¢å‘çµå·§æ‰‹çš„ç”Ÿæˆå¼ä»¿çœŸï¼Œè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01791v1" onclick="toggleFavorite(this, '2511.01791v1', 'GenDexHand: Generative Simulation for Dexterous Hands')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251200024v1-learning-from-watching-scalable-extraction-of-manipulation-trajector.html">Learning from Watching: Scalable Extraction of Manipulation Trajectories from Human Videos</a></td>
  <td>æå‡ºä¸€ç§åŸºäºè§†é¢‘ç†è§£å’Œç‚¹è¿½è¸ªçš„æ“çºµè½¨è¿¹æå–æ–¹æ³•ï¼Œç”¨äºä»äººç±»è§†é¢‘ä¸­å­¦ä¹ </td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.00024v1" onclick="toggleFavorite(this, '2512.00024v1', 'Learning from Watching: Scalable Extraction of Manipulation Trajectories from Human Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251102015v1-stein-based-optimization-of-sampling-distributions-in-model-predicti.html">Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control</a></td>
  <td>æå‡ºåŸºäºSteinå˜åˆ†æ¢¯åº¦ä¸‹é™çš„MPPIæ§åˆ¶ï¼Œä¼˜åŒ–é‡‡æ ·åˆ†å¸ƒä»¥æå‡è½¨è¿¹è§„åˆ’æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.02015v1" onclick="toggleFavorite(this, '2511.02015v1', 'Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251101476v1-mo-segman-rearrangement-planning-framework-for-multi-objective-seque.html">MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments</a></td>
  <td>MO-SeGManï¼šé¢å‘çº¦æŸç¯å¢ƒçš„å¤šç›®æ ‡åºåˆ—å¼•å¯¼æ“ä½œé‡æ’è§„åˆ’æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01476v1" onclick="toggleFavorite(this, '2511.01476v1', 'MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251101472v1-aermani-vlm-structured-prompting-and-reasoning-for-aerial-manipulati.html">AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models</a></td>
  <td>AERMANI-VLMï¼šåŸºäºç»“æ„åŒ–æç¤ºå’Œæ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ— äººæœºæ“ä½œä¸­çš„åº”ç”¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01472v1" onclick="toggleFavorite(this, '2511.01472v1', 'AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251101276v1-contact-map-transfer-with-conditional-diffusion-model-for-generaliza.html">Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation</a></td>
  <td>æå‡ºåŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ¥è§¦å›¾ä¼ é€’æ–¹æ³•ï¼Œå®ç°é€šç”¨çµå·§æŠ“å–ç”Ÿæˆã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01276v1" onclick="toggleFavorite(this, '2511.01276v1', 'Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251101999v1-trace-textual-reasoning-for-affordance-coordinate-extraction.html">TRACE: Textual Reasoning for Affordance Coordinate Extraction</a></td>
  <td>TRACEï¼šåˆ©ç”¨æ–‡æœ¬æ¨ç†æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„ç©ºé—´å®šä½ç²¾åº¦</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01999v1" onclick="toggleFavorite(this, '2511.01999v1', 'TRACE: Textual Reasoning for Affordance Coordinate Extraction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251101383v1-carli-v-camera-radar-lidar-point-wise-3d-velocity-estimation.html">CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation</a></td>
  <td>CaRLi-Vï¼šæå‡ºç›¸æœº-é›·è¾¾-æ¿€å…‰é›·è¾¾èåˆçš„ç‚¹äº‘çº§ä¸‰ç»´é€Ÿåº¦ä¼°è®¡æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01383v1" onclick="toggleFavorite(this, '2511.01383v1', 'CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251101331v2-robustvla-robustness-aware-reinforcement-post-training-for-vision-la.html">RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models</a></td>
  <td>RobustVLAï¼šé¢å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„é²æ£’æ€§å¼ºåŒ–åè®­ç»ƒ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01331v2" onclick="toggleFavorite(this, '2511.01331v2', 'RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251101294v2-kinematify-open-vocabulary-synthesis-of-high-dof-articulated-objects.html">Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects</a></td>
  <td>Kinematifyï¼šå¼€æ”¾è¯æ±‡é«˜è‡ªç”±åº¦é“°æ¥ç‰©ä½“è‡ªåŠ¨åˆæˆæ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01294v2" onclick="toggleFavorite(this, '2511.01294v2', 'Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251101347v1-design-and-development-of-an-electronics-free-earthworm-robot.html">Design and development of an electronics-free earthworm robot</a></td>
  <td>æå‡ºä¸€ç§æ— éœ€ç”µå­å…ƒä»¶çš„è •è™«æœºå™¨äººï¼Œé€‚ç”¨äºå—é™å’Œéç»“æ„åŒ–ç¯å¢ƒã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01347v1" onclick="toggleFavorite(this, '2511.01347v1', 'Design and development of an electronics-free earthworm robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251101272v1-design-and-fabrication-of-origami-inspired-knitted-fabrics-for-soft-.html">Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics</a></td>
  <td>æå‡ºä¸€ç§åŸºäºé’ˆç»‡é¢æ–™çš„æŠ˜çº¸ç»“æ„è½¯ä½“æœºå™¨äººè®¾è®¡ä¸åˆ¶é€ æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01272v1" onclick="toggleFavorite(this, '2511.01272v1', 'Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/251101186v1-lidar-vggt-cross-modal-coarse-to-fine-fusion-for-globally-consistent.html">LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping</a></td>
  <td>æå‡ºLiDAR-VGGTï¼Œé€šè¿‡è·¨æ¨¡æ€èåˆå®ç°å…¨å±€ä¸€è‡´å’Œåº¦é‡å°ºåº¦ç¨ å¯†åœ°å›¾é‡å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01186v1" onclick="toggleFavorite(this, '2511.01186v1', 'LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251102036v1-turbomap-gpu-accelerated-local-mapping-for-visual-slam.html">TurboMap: GPU-Accelerated Local Mapping for Visual SLAM</a></td>
  <td>TurboMapï¼šé¢å‘è§†è§‰SLAMçš„GPUåŠ é€Ÿå±€éƒ¨åœ°å›¾æ„å»ºæ¨¡å—</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.02036v1" onclick="toggleFavorite(this, '2511.02036v1', 'TurboMap: GPU-Accelerated Local Mapping for Visual SLAM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251101493v2-floor-plan-guided-visual-navigation-incorporating-depth-and-directio.html">Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues</a></td>
  <td>æå‡ºGlocDiffï¼Œèåˆæ¥¼å±‚å¹³é¢å›¾ä¸æ·±åº¦ä¿¡æ¯çš„è§†è§‰å¯¼èˆªæ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01493v2" onclick="toggleFavorite(this, '2511.01493v2', 'Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251101379v1-cm-liuw-odometry-robust-and-high-precision-lidar-inertial-uwb-wheel-.html">CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels</a></td>
  <td>CM-LIUW-Odometryï¼šé¢å‘æç«¯é€€åŒ–ç…¤çŸ¿å··é“çš„é²æ£’é«˜ç²¾åº¦æ¿€å…‰-æƒ¯æ€§-UWB-è½®é€Ÿé‡Œç¨‹è®¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01379v1" onclick="toggleFavorite(this, '2511.01379v1', 'CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251101219v2-tackling-the-kidnapped-robot-problem-via-sparse-feasible-hypothesis-.html">Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference</a></td>
  <td>æå‡ºåŸºäºç¨€ç–å¯è¡Œå‡è®¾é‡‡æ ·å’Œå¯é æ‰¹å¤„ç†å¤šé˜¶æ®µæ¨ç†çš„æ¡†æ¶ï¼Œè§£å†³æœºå™¨äººé‡å®šä½é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01219v2" onclick="toggleFavorite(this, '2511.01219v2', 'Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251101797v1-hybrid-neural-network-based-indoor-localisation-system-for-mobile-ro.html">Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator</a></td>
  <td>æå‡ºä¸€ç§åŸºäºæ··åˆç¥ç»ç½‘ç»œçš„å®¤å†…å®šä½ç³»ç»Ÿï¼Œåˆ©ç”¨CSIæ•°æ®ä¸ºç§»åŠ¨æœºå™¨äººå®ç°ç²¾å‡†å®šä½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01797v1" onclick="toggleFavorite(this, '2511.01797v1', 'Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251101369v1-lateral-velocity-model-for-vehicle-parking-applications.html">Lateral Velocity Model for Vehicle Parking Applications</a></td>
  <td>æå‡ºåŸºäºå®è½¦æ•°æ®çš„æ¨ªå‘é€Ÿåº¦æ¨¡å‹ï¼Œæå‡è‡ªåŠ¨æ³Šè½¦å®šä½ç²¾åº¦</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01369v1" onclick="toggleFavorite(this, '2511.01369v1', 'Lateral Velocity Model for Vehicle Parking Applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/251101407v1-foldpath-end-to-end-object-centric-motion-generation-via-modulated-i.html">FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths</a></td>
  <td>FoldPathï¼šé€šè¿‡è°ƒåˆ¶éšå¼è·¯å¾„å®ç°ç«¯åˆ°ç«¯é¢å‘å¯¹è±¡çš„è¿åŠ¨ç”Ÿæˆ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01407v1" onclick="toggleFavorite(this, '2511.01407v1', 'FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)