---
layout: default
title: TRACE: Textual Reasoning for Affordance Coordinate Extraction
---

# TRACE: Textual Reasoning for Affordance Coordinate Extraction

**arXiv**: [2511.01999v1](https://arxiv.org/abs/2511.01999) | [PDF](https://arxiv.org/pdf/2511.01999.pdf)

**ä½œè€…**: Sangyun Park, Jin Kim, Yuchen Cui, Matthew S. Brown

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-03

**å¤‡æ³¨**: ICCV 2025. *Equal contribution. â€ Corresponding author

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/jink-ucla/TRACE)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TRACEï¼šåˆ©ç”¨æ–‡æœ¬æŽ¨ç†æå‡è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„ç©ºé—´å®šä½ç²¾åº¦**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æœºå™¨äººæ“ä½œ` `æ–‡æœ¬æŽ¨ç†é“¾` `ç©ºé—´æŽ¨ç†` `å¯ä¾›æ€§é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹éš¾ä»¥å°†é«˜çº§æŒ‡ä»¤è½¬åŒ–ä¸ºæœºå™¨äººæ“ä½œæ‰€éœ€çš„ç²¾ç¡®ç©ºé—´å¯ä¾›æ€§ï¼Œä¸”è§†è§‰æ€ç»´é“¾æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚
2. TRACEæ–¹æ³•é€šè¿‡å¼•å…¥æ–‡æœ¬æŽ¨ç†é“¾(CoR)ï¼Œä½¿æ¨¡åž‹åœ¨è¡ŒåŠ¨å‰å¤–éƒ¨åŒ–ç©ºé—´æŽ¨ç†ï¼Œä»Žè€Œæé«˜å®šä½ç²¾åº¦ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒTRACEæ¨¡åž‹åœ¨Where2PlaceåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æ–‡æœ¬æŽ¨ç†é“¾çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡åž‹(VLM)éš¾ä»¥å°†é«˜å±‚æŒ‡ä»¤è½¬åŒ–ä¸ºæœºå™¨äººæ“ä½œæ‰€éœ€çš„ç²¾ç¡®ç©ºé—´å¯ä¾›æ€§ã€‚è™½ç„¶å­˜åœ¨è§†è§‰æ€ç»´é“¾(CoT)æ–¹æ³•ï¼Œä½†è®¡ç®—æˆæœ¬é€šå¸¸å¾ˆé«˜ã€‚æœ¬æ–‡æå‡ºTRACEï¼ˆç”¨äºŽå¯ä¾›æ€§åæ ‡æå–çš„æ–‡æœ¬æŽ¨ç†ï¼‰ï¼Œä¸€ç§å°†æ–‡æœ¬æŽ¨ç†é“¾(CoR)é›†æˆåˆ°å¯ä¾›æ€§é¢„æµ‹è¿‡ç¨‹ä¸­çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥æ–¹æ³•åˆ›å»ºäº†TRACEæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è‡ªä¸»æµç¨‹ç”Ÿæˆçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå°†æŒ‡ä»¤ä¸Žæ˜¾å¼æ–‡æœ¬ç†ç”±é…å¯¹ã€‚é€šè¿‡åœ¨æ­¤æ•°æ®ä¸Šå¾®è°ƒVLMï¼Œæˆ‘ä»¬çš„æ¨¡åž‹å­¦ä¼šäº†åœ¨è¡ŒåŠ¨å‰å¤–éƒ¨åŒ–å…¶ç©ºé—´æŽ¨ç†ã€‚å®žéªŒè¡¨æ˜Žï¼ŒTRACEè°ƒä¼˜çš„æ¨¡åž‹å®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä¸»è¦çš„Where2Place (W2P)åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°48.1%çš„å‡†ç¡®çŽ‡ï¼ˆç›¸å¯¹æå‡9.6%ï¼‰ï¼Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„W2P(h)å­é›†ä¸­è¾¾åˆ°55.0%ã€‚å…³é”®çš„æ˜¯ï¼Œä¸€é¡¹æ¶ˆèžç ”ç©¶è¡¨æ˜Žï¼Œæ€§èƒ½ä¸Žä½¿ç”¨çš„æŽ¨ç†æ•°æ®é‡ç›´æŽ¥ç›¸å…³ï¼Œè¯å®žäº†CoRçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå¯¹æ¨¡åž‹æ³¨æ„åŠ›å›¾çš„åˆ†æžæ­ç¤ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„æŽ¨ç†è¿‡ç¨‹ï¼Œå…¶ä¸­ç„¦ç‚¹åœ¨æŽ¨ç†æ­¥éª¤ä¸­åŠ¨æ€è½¬ç§»ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜Žï¼Œè®­ç»ƒVLMç”Ÿæˆæ–‡æœ¬CoRæ˜¯æé«˜åŸºäºŽVLMçš„æœºå™¨äººæŽ§åˆ¶çš„ç²¾åº¦ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§çš„æœ‰æ•ˆä¸”ç¨³å¥çš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨https://github.com/jink-ucla/TRACE èŽ·å¾—ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œéš¾ä»¥ç²¾ç¡®ç†è§£é«˜å±‚æŒ‡ä»¤å¹¶è½¬åŒ–ä¸ºå…·ä½“çš„ç©ºé—´åæ ‡ï¼Œå¯¼è‡´æ“ä½œå¤±è´¥ã€‚çŽ°æœ‰çš„è§†è§‰æ€ç»´é“¾æ–¹æ³•è™½ç„¶å¯ä»¥ä¸€å®šç¨‹åº¦è§£å†³è¯¥é—®é¢˜ï¼Œä½†è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥å®žé™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTRACEçš„æ ¸å¿ƒåœ¨äºŽåˆ©ç”¨æ–‡æœ¬æŽ¨ç†é“¾(CoR)æ¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡åž‹è¿›è¡Œç©ºé—´æŽ¨ç†ã€‚é€šè¿‡è®©æ¨¡åž‹å…ˆç”Ÿæˆä¸€æ®µæ–‡æœ¬æè¿°å…¶æŽ¨ç†è¿‡ç¨‹ï¼Œå†æ ¹æ®æŽ¨ç†ç»“æžœé¢„æµ‹ç›®æ ‡åæ ‡ï¼Œä»Žè€Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¿™ç§æ–¹æ³•å€Ÿé‰´äº†äººç±»è§£å†³é—®é¢˜çš„æ€è·¯ï¼Œå³å…ˆæ€è€ƒå†è¡ŒåŠ¨ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šTRACEæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®é›†æž„å»ºï¼šä½¿ç”¨è‡ªä¸»æµç¨‹ç”Ÿæˆå¤§è§„æ¨¡çš„TRACEæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æŒ‡ä»¤ã€æ–‡æœ¬æŽ¨ç†é“¾å’Œå¯¹åº”çš„ç›®æ ‡åæ ‡ã€‚2) æ¨¡åž‹å¾®è°ƒï¼šåœ¨TRACEæ•°æ®é›†ä¸Šå¾®è°ƒè§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œä½¿å…¶å­¦ä¹ ç”Ÿæˆæ–‡æœ¬æŽ¨ç†é“¾å¹¶é¢„æµ‹ç›®æ ‡åæ ‡ã€‚3) æŽ¨ç†è¿‡ç¨‹ï¼šç»™å®šæŒ‡ä»¤ï¼Œæ¨¡åž‹é¦–å…ˆç”Ÿæˆæ–‡æœ¬æŽ¨ç†é“¾ï¼Œç„¶åŽæ ¹æ®æŽ¨ç†é“¾é¢„æµ‹ç›®æ ‡åæ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šTRACEçš„å…³é”®åˆ›æ–°åœ¨äºŽå°†æ–‡æœ¬æŽ¨ç†é“¾å¼•å…¥åˆ°è§†è§‰è¯­è¨€æ¨¡åž‹çš„ç©ºé—´æŽ¨ç†è¿‡ç¨‹ä¸­ã€‚ä¸Žä¼ ç»Ÿçš„è§†è§‰æ€ç»´é“¾æ–¹æ³•ç›¸æ¯”ï¼ŒTRACEæ–¹æ³•æ›´åŠ é«˜æ•ˆï¼Œå¹¶ä¸”å¯ä»¥æä¾›æ›´æ¸…æ™°çš„æŽ¨ç†è¿‡ç¨‹ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼ŒTRACEæ•°æ®é›†çš„æž„å»ºä¹Ÿä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†å®è´µçš„æ•°æ®èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šTRACEæ•°æ®é›†çš„æž„å»ºé‡‡ç”¨äº†è‡ªä¸»æµç¨‹ï¼Œä¿è¯äº†æ•°æ®çš„è§„æ¨¡å’Œè´¨é‡ã€‚åœ¨æ¨¡åž‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡åž‹ç”Ÿæˆæ–‡æœ¬æŽ¨ç†é“¾ï¼Œå¹¶ä½¿ç”¨L1æŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡åž‹é¢„æµ‹ç›®æ ‡åæ ‡ã€‚æ¨¡åž‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¹Ÿè¢«ç”¨äºŽåˆ†æžæŽ¨ç†è¿‡ç¨‹ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„å¯è§£é‡Šæ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

TRACEæ¨¡åž‹åœ¨Where2Place (W2P)åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå‡†ç¡®çŽ‡è¾¾åˆ°48.1%ï¼Œç›¸å¯¹æå‡9.6%ã€‚åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„W2P(h)å­é›†ä¸­ï¼Œå‡†ç¡®çŽ‡è¾¾åˆ°55.0%ã€‚æ¶ˆèžå®žéªŒè¡¨æ˜Žï¼Œæ€§èƒ½ä¸Žä½¿ç”¨çš„æŽ¨ç†æ•°æ®é‡ç›´æŽ¥ç›¸å…³ï¼Œè¯å®žäº†æ–‡æœ¬æŽ¨ç†é“¾çš„æœ‰æ•ˆæ€§ã€‚æ³¨æ„åŠ›å›¾åˆ†æžæ­ç¤ºäº†æ¨¡åž‹çš„å¯è§£é‡ŠæŽ¨ç†è¿‡ç¨‹ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

TRACEæ–¹æ³•å¯åº”ç”¨äºŽå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–ã€æ”¾ç½®å’Œç»„è£…ã€‚é€šè¿‡æé«˜æœºå™¨äººå¯¹æŒ‡ä»¤çš„ç†è§£å’Œç©ºé—´æŽ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®žçŽ°æ›´æ™ºèƒ½ã€æ›´çµæ´»çš„è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºŽè™šæ‹ŸçŽ°å®žå’Œå¢žå¼ºçŽ°å®žç­‰é¢†åŸŸï¼Œæé«˜ç”¨æˆ·ä¸Žè™šæ‹ŸçŽ¯å¢ƒçš„äº¤äº’ä½“éªŒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) struggle to translate high-level instructions into the precise spatial affordances required for robotic manipulation. While visual Chain-of-Thought (CoT) methods exist, they are often computationally intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance Coordinate Extraction), a novel methodology that integrates a textual Chain of Reasoning (CoR) into the affordance prediction process. We use this methodology to create the TRACE dataset, a large-scale collection created via an autonomous pipeline that pairs instructions with explicit textual rationales. By fine-tuning a VLM on this data, our model learns to externalize its spatial reasoning before acting. Our experiments show that our TRACE-tuned model achieves state-of-the-art performance, reaching 48.1% accuracy on the primary Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more challenging W2P(h) subset. Crucially, an ablation study demonstrates that performance scales directly with the amount of reasoning data used, confirming the CoR's effectiveness. Furthermore, analysis of the model's attention maps reveals an interpretable reasoning process where focus shifts dynamically across reasoning steps. This work shows that training VLMs to generate a textual CoR is an effective and robust strategy for enhancing the precision, reliability, and interpretability of VLM-based robot control. Our dataset and code are available at https://github.com/jink-ucla/TRACE

