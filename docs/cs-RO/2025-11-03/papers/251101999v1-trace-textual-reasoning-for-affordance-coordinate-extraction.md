---
layout: default
title: TRACE: Textual Reasoning for Affordance Coordinate Extraction
---

# TRACE: Textual Reasoning for Affordance Coordinate Extraction

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.01999" target="_blank" class="toolbar-btn">arXiv: 2511.01999v1</a>
    <a href="https://arxiv.org/pdf/2511.01999.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01999v1" 
            onclick="toggleFavorite(this, '2511.01999v1', 'TRACE: Textual Reasoning for Affordance Coordinate Extraction')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Sangyun Park, Jin Kim, Yuchen Cui, Matthew S. Brown

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-03

**Â§áÊ≥®**: ICCV 2025. *Equal contribution. ‚Ä†Corresponding author

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/jink-ucla/TRACE)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**TRACEÔºöÂà©Áî®ÊñáÊú¨Êé®ÁêÜÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÁ©∫Èó¥ÂÆö‰ΩçÁ≤æÂ∫¶**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Êú∫Âô®‰∫∫Êìç‰Ωú` `ÊñáÊú¨Êé®ÁêÜÈìæ` `Á©∫Èó¥Êé®ÁêÜ` `ÂèØ‰æõÊÄßÈ¢ÑÊµã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÈöæ‰ª•Â∞ÜÈ´òÁ∫ßÊåá‰ª§ËΩ¨Âåñ‰∏∫Êú∫Âô®‰∫∫Êìç‰ΩúÊâÄÈúÄÁöÑÁ≤æÁ°ÆÁ©∫Èó¥ÂèØ‰æõÊÄßÔºå‰∏îËßÜËßâÊÄùÁª¥ÈìæÊñπÊ≥ïËÆ°ÁÆóÊàêÊú¨È´òÊòÇ„ÄÇ
2. TRACEÊñπÊ≥ïÈÄöËøáÂºïÂÖ•ÊñáÊú¨Êé®ÁêÜÈìæ(CoR)Ôºå‰ΩøÊ®°ÂûãÂú®Ë°åÂä®ÂâçÂ§ñÈÉ®ÂåñÁ©∫Èó¥Êé®ÁêÜÔºå‰ªéËÄåÊèêÈ´òÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåTRACEÊ®°ÂûãÂú®Where2PlaceÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÈ™åËØÅ‰∫ÜÊñáÊú¨Êé®ÁêÜÈìæÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâËØ≠Ë®ÄÊ®°Âûã(VLM)Èöæ‰ª•Â∞ÜÈ´òÂ±ÇÊåá‰ª§ËΩ¨Âåñ‰∏∫Êú∫Âô®‰∫∫Êìç‰ΩúÊâÄÈúÄÁöÑÁ≤æÁ°ÆÁ©∫Èó¥ÂèØ‰æõÊÄß„ÄÇËôΩÁÑ∂Â≠òÂú®ËßÜËßâÊÄùÁª¥Èìæ(CoT)ÊñπÊ≥ïÔºå‰ΩÜËÆ°ÁÆóÊàêÊú¨ÈÄöÂ∏∏ÂæàÈ´ò„ÄÇÊú¨ÊñáÊèêÂá∫TRACEÔºàÁî®‰∫éÂèØ‰æõÊÄßÂùêÊ†áÊèêÂèñÁöÑÊñáÊú¨Êé®ÁêÜÔºâÔºå‰∏ÄÁßçÂ∞ÜÊñáÊú¨Êé®ÁêÜÈìæ(CoR)ÈõÜÊàêÂà∞ÂèØ‰æõÊÄßÈ¢ÑÊµãËøáÁ®ã‰∏≠ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÊàë‰ª¨‰ΩøÁî®ËØ•ÊñπÊ≥ïÂàõÂª∫‰∫ÜTRACEÊï∞ÊçÆÈõÜÔºåËøôÊòØ‰∏Ä‰∏™ÈÄöËøáËá™‰∏ªÊµÅÁ®ãÁîüÊàêÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂ∞ÜÊåá‰ª§‰∏éÊòæÂºèÊñáÊú¨ÁêÜÁî±ÈÖçÂØπ„ÄÇÈÄöËøáÂú®Ê≠§Êï∞ÊçÆ‰∏äÂæÆË∞ÉVLMÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂ≠¶‰ºö‰∫ÜÂú®Ë°åÂä®ÂâçÂ§ñÈÉ®ÂåñÂÖ∂Á©∫Èó¥Êé®ÁêÜ„ÄÇÂÆûÈ™åË°®ÊòéÔºåTRACEË∞É‰ºòÁöÑÊ®°ÂûãÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂú®‰∏ªË¶ÅÁöÑWhere2Place (W2P)Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞48.1%ÁöÑÂáÜÁ°ÆÁéáÔºàÁõ∏ÂØπÊèêÂçá9.6%ÔºâÔºåÂú®Êõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑW2P(h)Â≠êÈõÜ‰∏≠ËææÂà∞55.0%„ÄÇÂÖ≥ÈîÆÁöÑÊòØÔºå‰∏ÄÈ°πÊ∂àËûçÁ†îÁ©∂Ë°®ÊòéÔºåÊÄßËÉΩ‰∏é‰ΩøÁî®ÁöÑÊé®ÁêÜÊï∞ÊçÆÈáèÁõ¥Êé•Áõ∏ÂÖ≥ÔºåËØÅÂÆû‰∫ÜCoRÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåÂØπÊ®°ÂûãÊ≥®ÊÑèÂäõÂõæÁöÑÂàÜÊûêÊè≠Á§∫‰∫Ü‰∏Ä‰∏™ÂèØËß£ÈáäÁöÑÊé®ÁêÜËøáÁ®ãÔºåÂÖ∂‰∏≠ÁÑ¶ÁÇπÂú®Êé®ÁêÜÊ≠•È™§‰∏≠Âä®ÊÄÅËΩ¨Áßª„ÄÇËøôÈ°πÂ∑•‰ΩúË°®ÊòéÔºåËÆ≠ÁªÉVLMÁîüÊàêÊñáÊú¨CoRÊòØÊèêÈ´òÂü∫‰∫éVLMÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂ÁöÑÁ≤æÂ∫¶„ÄÅÂèØÈù†ÊÄßÂíåÂèØËß£ÈáäÊÄßÁöÑÊúâÊïà‰∏îÁ®≥ÂÅ•ÁöÑÁ≠ñÁï•„ÄÇÊàë‰ª¨ÁöÑÊï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÂèØÂú®https://github.com/jink-ucla/TRACE Ëé∑Âæó„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÔºåÈöæ‰ª•Á≤æÁ°ÆÁêÜËß£È´òÂ±ÇÊåá‰ª§Âπ∂ËΩ¨Âåñ‰∏∫ÂÖ∑‰ΩìÁöÑÁ©∫Èó¥ÂùêÊ†áÔºåÂØºËá¥Êìç‰ΩúÂ§±Ë¥•„ÄÇÁé∞ÊúâÁöÑËßÜËßâÊÄùÁª¥ÈìæÊñπÊ≥ïËôΩÁÑ∂ÂèØ‰ª•‰∏ÄÂÆöÁ®ãÂ∫¶Ëß£ÂÜ≥ËØ•ÈóÆÈ¢òÔºå‰ΩÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºåÈöæ‰ª•ÂÆûÈôÖÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTRACEÁöÑÊ†∏ÂøÉÂú®‰∫éÂà©Áî®ÊñáÊú¨Êé®ÁêÜÈìæ(CoR)Êù•ÂºïÂØºËßÜËßâËØ≠Ë®ÄÊ®°ÂûãËøõË°åÁ©∫Èó¥Êé®ÁêÜ„ÄÇÈÄöËøáËÆ©Ê®°ÂûãÂÖàÁîüÊàê‰∏ÄÊÆµÊñáÊú¨ÊèèËø∞ÂÖ∂Êé®ÁêÜËøáÁ®ãÔºåÂÜçÊ†πÊçÆÊé®ÁêÜÁªìÊûúÈ¢ÑÊµãÁõÆÊ†áÂùêÊ†áÔºå‰ªéËÄåÊèêÈ´òÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇËøôÁßçÊñπÊ≥ïÂÄüÈâ¥‰∫Ü‰∫∫Á±ªËß£ÂÜ≥ÈóÆÈ¢òÁöÑÊÄùË∑ØÔºåÂç≥ÂÖàÊÄùËÄÉÂÜçË°åÂä®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTRACEÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) Êï∞ÊçÆÈõÜÊûÑÂª∫Ôºö‰ΩøÁî®Ëá™‰∏ªÊµÅÁ®ãÁîüÊàêÂ§ßËßÑÊ®°ÁöÑTRACEÊï∞ÊçÆÈõÜÔºåËØ•Êï∞ÊçÆÈõÜÂåÖÂê´Êåá‰ª§„ÄÅÊñáÊú¨Êé®ÁêÜÈìæÂíåÂØπÂ∫îÁöÑÁõÆÊ†áÂùêÊ†á„ÄÇ2) Ê®°ÂûãÂæÆË∞ÉÔºöÂú®TRACEÊï∞ÊçÆÈõÜ‰∏äÂæÆË∞ÉËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÂÖ∂Â≠¶‰π†ÁîüÊàêÊñáÊú¨Êé®ÁêÜÈìæÂπ∂È¢ÑÊµãÁõÆÊ†áÂùêÊ†á„ÄÇ3) Êé®ÁêÜËøáÁ®ãÔºöÁªôÂÆöÊåá‰ª§ÔºåÊ®°ÂûãÈ¶ñÂÖàÁîüÊàêÊñáÊú¨Êé®ÁêÜÈìæÔºåÁÑ∂ÂêéÊ†πÊçÆÊé®ÁêÜÈìæÈ¢ÑÊµãÁõÆÊ†áÂùêÊ†á„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTRACEÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÊñáÊú¨Êé®ÁêÜÈìæÂºïÂÖ•Âà∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥Êé®ÁêÜËøáÁ®ã‰∏≠„ÄÇ‰∏é‰º†ÁªüÁöÑËßÜËßâÊÄùÁª¥ÈìæÊñπÊ≥ïÁõ∏ÊØîÔºåTRACEÊñπÊ≥ïÊõ¥Âä†È´òÊïàÔºåÂπ∂‰∏îÂèØ‰ª•Êèê‰æõÊõ¥Ê∏ÖÊô∞ÁöÑÊé®ÁêÜËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄß„ÄÇÊ≠§Â§ñÔºåTRACEÊï∞ÊçÆÈõÜÁöÑÊûÑÂª∫‰πü‰∏∫ËØ•È¢ÜÂüüÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÂÆùË¥µÁöÑÊï∞ÊçÆËµÑÊ∫ê„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöTRACEÊï∞ÊçÆÈõÜÁöÑÊûÑÂª∫ÈááÁî®‰∫ÜËá™‰∏ªÊµÅÁ®ãÔºå‰øùËØÅ‰∫ÜÊï∞ÊçÆÁöÑËßÑÊ®°ÂíåË¥®Èáè„ÄÇÂú®Ê®°ÂûãÂæÆË∞ÉËøáÁ®ã‰∏≠Ôºå‰ΩøÁî®‰∫Ü‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Êù•ËÆ≠ÁªÉÊ®°ÂûãÁîüÊàêÊñáÊú¨Êé®ÁêÜÈìæÔºåÂπ∂‰ΩøÁî®L1ÊçüÂ§±ÂáΩÊï∞Êù•ËÆ≠ÁªÉÊ®°ÂûãÈ¢ÑÊµãÁõÆÊ†áÂùêÊ†á„ÄÇÊ®°ÂûãÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂‰πüË¢´Áî®‰∫éÂàÜÊûêÊé®ÁêÜËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

TRACEÊ®°ÂûãÂú®Where2Place (W2P)Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂáÜÁ°ÆÁéáËææÂà∞48.1%ÔºåÁõ∏ÂØπÊèêÂçá9.6%„ÄÇÂú®Êõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑW2P(h)Â≠êÈõÜ‰∏≠ÔºåÂáÜÁ°ÆÁéáËææÂà∞55.0%„ÄÇÊ∂àËûçÂÆûÈ™åË°®ÊòéÔºåÊÄßËÉΩ‰∏é‰ΩøÁî®ÁöÑÊé®ÁêÜÊï∞ÊçÆÈáèÁõ¥Êé•Áõ∏ÂÖ≥ÔºåËØÅÂÆû‰∫ÜÊñáÊú¨Êé®ÁêÜÈìæÁöÑÊúâÊïàÊÄß„ÄÇÊ≥®ÊÑèÂäõÂõæÂàÜÊûêÊè≠Á§∫‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊé®ÁêÜËøáÁ®ã„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

TRACEÊñπÊ≥ïÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇÁâ©‰ΩìÊäìÂèñ„ÄÅÊîæÁΩÆÂíåÁªÑË£Ö„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫ÂØπÊåá‰ª§ÁöÑÁêÜËß£ÂíåÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÁÅµÊ¥ªÁöÑËá™Âä®ÂåñÁîü‰∫ßÁ∫ø„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÔºåÊèêÈ´òÁî®Êà∑‰∏éËôöÊãüÁéØÂ¢ÉÁöÑ‰∫§‰∫í‰ΩìÈ™å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language Models (VLMs) struggle to translate high-level instructions into the precise spatial affordances required for robotic manipulation. While visual Chain-of-Thought (CoT) methods exist, they are often computationally intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance Coordinate Extraction), a novel methodology that integrates a textual Chain of Reasoning (CoR) into the affordance prediction process. We use this methodology to create the TRACE dataset, a large-scale collection created via an autonomous pipeline that pairs instructions with explicit textual rationales. By fine-tuning a VLM on this data, our model learns to externalize its spatial reasoning before acting. Our experiments show that our TRACE-tuned model achieves state-of-the-art performance, reaching 48.1% accuracy on the primary Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more challenging W2P(h) subset. Crucially, an ablation study demonstrates that performance scales directly with the amount of reasoning data used, confirming the CoR's effectiveness. Furthermore, analysis of the model's attention maps reveals an interpretable reasoning process where focus shifts dynamically across reasoning steps. This work shows that training VLMs to generate a textual CoR is an effective and robust strategy for enhancing the precision, reliability, and interpretability of VLM-based robot control. Our dataset and code are available at https://github.com/jink-ucla/TRACE

