---
layout: default
title: Learning from Watching: Scalable Extraction of Manipulation Trajectories from Human Videos
---

# Learning from Watching: Scalable Extraction of Manipulation Trajectories from Human Videos

**arXiv**: [2512.00024v1](https://arxiv.org/abs/2512.00024) | [PDF](https://arxiv.org/pdf/2512.00024.pdf)

**ä½œè€…**: X. Hu, G. Ye

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-03

**å¤‡æ³¨**: Accepted to RSS 2025 Workshop

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºŽè§†é¢‘ç†è§£å’Œç‚¹è¿½è¸ªçš„æ“çºµè½¨è¿¹æå–æ–¹æ³•ï¼Œç”¨äºŽä»Žäººç±»è§†é¢‘ä¸­å­¦ä¹ **

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æœºå™¨äººå­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `è§†é¢‘ç†è§£` `ç‚¹è¿½è¸ª` `æ“çºµè½¨è¿¹æå–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æœºå™¨äººå­¦ä¹ æ–¹æ³•ä¾èµ–æ˜‚è´µçš„æœºå™¨äººå¹³å°å’Œäººå·¥æ ‡æ³¨ï¼Œé™åˆ¶äº†æ•°æ®è§„æ¨¡ã€‚
2. è¯¥æ–¹æ³•ç»“åˆè§†é¢‘ç†è§£æ¨¡åž‹å’Œç‚¹è¿½è¸ªæŠ€æœ¯ï¼Œä»Žäººç±»æ“çºµè§†é¢‘ä¸­æå–å…³é”®ç‚¹è½¨è¿¹ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½å‡†ç¡®è¿½è¸ªå…³é”®ç‚¹ï¼Œä¸ºæœºå™¨äººå­¦ä¹ æä¾›å¤§è§„æ¨¡æ•°æ®ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è®­ç»ƒå¤§è§„æ¨¡æœºå™¨äººæ¨¡åž‹ï¼Œæ”¶é›†é«˜è´¨é‡æ•°æ®é€šå¸¸ä¾èµ–äºŽçœŸå®žçš„æœºå™¨äººå¹³å°ï¼Œæ— è®ºæ˜¯é¥æ“ä½œè¿˜æ˜¯è„šæœ¬æ¼”ç¤ºï¼Œè¿™éƒ½éžå¸¸è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†æ‰©å±•æ•°æ®æ”¶é›†ï¼Œè®¸å¤šç ”ç©¶äººå‘˜è½¬å‘åˆ©ç”¨åœ¨çº¿å¯èŽ·å¾—çš„äººç±»æ“çºµè§†é¢‘ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ‰‹éƒ¨æ£€æµ‹æˆ–ç‰©ä½“å§¿æ€ä¼°è®¡ä¸Šï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è¿™äº›è§†é¢‘ä¸­è•´å«çš„ä¸°å¯Œäº¤äº’çº¿ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç”¨äºŽè§†é¢‘ç†è§£çš„å¤§åž‹åŸºç¡€æ¨¡åž‹å’Œç‚¹è¿½è¸ªæŠ€æœ¯ï¼Œä»¥æå–æ“çºµè¿‡ç¨‹ä¸­æ‰€æœ‰ä»»åŠ¡ç›¸å…³å…³é”®ç‚¹çš„å¯†é›†è½¨è¿¹ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿæ›´å…¨é¢åœ°åˆ©ç”¨äº’è”ç½‘è§„æ¨¡çš„äººç±»æ¼”ç¤ºè§†é¢‘ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å‡†ç¡®åœ°è·Ÿè¸ªæ•´ä¸ªæ“çºµè¿‡ç¨‹ä¸­çš„å…³é”®ç‚¹ï¼Œä¸ºæ›´å…·å¯æ‰©å±•æ€§å’Œæ•°æ®æ•ˆçŽ‡çš„æœºå™¨äººå­¦ä¹ é“ºå¹³äº†é“è·¯ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æœºå™¨äººå­¦ä¹ çš„æ•°æ®æ”¶é›†æ–¹æ³•ï¼Œå¦‚é¥æ“ä½œå’Œè„šæœ¬æ¼”ç¤ºï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚è™½ç„¶å¯ä»¥åˆ©ç”¨äº’è”ç½‘ä¸Šçš„äººç±»æ“çºµè§†é¢‘ï¼Œä½†çŽ°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºŽæ‰‹éƒ¨æ£€æµ‹æˆ–ç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨è§†é¢‘ä¸­è•´å«çš„ä¸°å¯Œäº¤äº’ä¿¡æ¯ï¼Œä¾‹å¦‚å…³é”®ç‚¹çš„è¿åŠ¨è½¨è¿¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§åž‹è§†é¢‘ç†è§£æ¨¡åž‹æ¥è¯†åˆ«è§†é¢‘ä¸­çš„å…³é”®ç‚¹ï¼Œå¹¶ä½¿ç”¨ç‚¹è¿½è¸ªæŠ€æœ¯æ¥è·Ÿè¸ªè¿™äº›å…³é”®ç‚¹åœ¨æ•´ä¸ªæ“çºµè¿‡ç¨‹ä¸­çš„è¿åŠ¨è½¨è¿¹ã€‚é€šè¿‡æå–è¿™äº›å¯†é›†çš„å…³é”®ç‚¹è½¨è¿¹ï¼Œå¯ä»¥æ›´å…¨é¢åœ°ç†è§£äººç±»çš„æ“çºµè¡Œä¸ºï¼Œä»Žè€Œä¸ºæœºå™¨äººå­¦ä¹ æä¾›æ›´ä¸°å¯Œçš„æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•çš„æŠ€æœ¯æ¡†æž¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§åž‹è§†é¢‘ç†è§£æ¨¡åž‹ï¼ˆå…·ä½“æ¨¡åž‹æœªçŸ¥ï¼‰æ¥æ£€æµ‹å’Œè¯†åˆ«è§†é¢‘ä¸­çš„å…³é”®ç‚¹ã€‚è¿™äº›å…³é”®ç‚¹æ˜¯ä¸Žæ“çºµä»»åŠ¡ç›¸å…³çš„ï¼Œä¾‹å¦‚ç‰©ä½“ä¸Šçš„ç‰¹å®šä½ç½®æˆ–æ‰‹éƒ¨çš„å…³èŠ‚ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨ç‚¹è¿½è¸ªæŠ€æœ¯æ¥è·Ÿè¸ªè¿™äº›å…³é”®ç‚¹åœ¨è§†é¢‘å¸§ä¹‹é—´çš„è¿åŠ¨è½¨è¿¹ã€‚é€šè¿‡è¿žæŽ¥è¿™äº›è½¨è¿¹ï¼Œå¯ä»¥èŽ·å¾—å…³é”®ç‚¹åœ¨æ•´ä¸ªæ“çºµè¿‡ç¨‹ä¸­çš„å¯†é›†è¿åŠ¨ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽå°†å¤§åž‹è§†é¢‘ç†è§£æ¨¡åž‹ä¸Žç‚¹è¿½è¸ªæŠ€æœ¯ç›¸ç»“åˆï¼Œä»Žè€Œèƒ½å¤Ÿä»Žäººç±»æ“çºµè§†é¢‘ä¸­æå–å‡ºå¯†é›†çš„å…³é”®ç‚¹è½¨è¿¹ã€‚è¿™ä¸ŽçŽ°æœ‰æ–¹æ³•åªå…³æ³¨æ‰‹éƒ¨æ£€æµ‹æˆ–ç‰©ä½“å§¿æ€ä¼°è®¡å½¢æˆäº†é²œæ˜Žå¯¹æ¯”ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°åˆ©ç”¨è§†é¢‘ä¸­çš„ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†è¯´æ˜Žå…³é”®å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æž„ç­‰æŠ€æœ¯ç»†èŠ‚ã€‚ä½†æ˜¯ï¼Œå¯ä»¥æŽ¨æµ‹ï¼Œè§†é¢‘ç†è§£æ¨¡åž‹çš„é€‰æ‹©å’Œè®­ç»ƒï¼Œä»¥åŠç‚¹è¿½è¸ªç®—æ³•çš„é€‰æ‹©å’Œå‚æ•°è°ƒæ•´ï¼Œéƒ½ä¼šå¯¹æœ€ç»ˆçš„è½¨è¿¹æå–æ•ˆæžœäº§ç”Ÿé‡è¦å½±å“ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æž„æœªçŸ¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è®ºæ–‡å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®åœ°è·Ÿè¸ªæ•´ä¸ªæ“çºµè¿‡ç¨‹ä¸­çš„å…³é”®ç‚¹ã€‚è™½ç„¶è®ºæ–‡ä¸­æ²¡æœ‰æä¾›å…·ä½“çš„æ€§èƒ½æ•°æ®æˆ–å¯¹æ¯”åŸºçº¿ï¼Œä½†å¼ºè°ƒäº†è¯¥æ–¹æ³•ä¸ºæ›´å…·å¯æ‰©å±•æ€§å’Œæ•°æ®æ•ˆçŽ‡çš„æœºå™¨äººå­¦ä¹ é“ºå¹³äº†é“è·¯ã€‚å…·ä½“çš„æå‡å¹…åº¦æœªçŸ¥ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæœºå™¨äººæ¨¡ä»¿å­¦ä¹ ã€æœºå™¨äººæŠ€èƒ½å­¦ä¹ ã€äººæœºåä½œç­‰é¢†åŸŸã€‚é€šè¿‡ä»Žå¤§é‡äººç±»æ“çºµè§†é¢‘ä¸­å­¦ä¹ ï¼Œæœºå™¨äººå¯ä»¥æ›´é«˜æ•ˆåœ°æŽŒæ¡å„ç§æ“ä½œæŠ€èƒ½ï¼Œä»Žè€Œé™ä½Žæœºå™¨äººå¼€å‘çš„æˆæœ¬å’Œæ—¶é—´ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºŽåˆ†æžäººç±»è¡Œä¸ºï¼Œä¾‹å¦‚è¿åŠ¨åˆ†æžå’Œåº·å¤è®­ç»ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Collecting high-quality data for training large-scale robotic models typically relies on real robot platforms, which is labor-intensive and costly, whether via teleoperation or scripted demonstrations. To scale data collection, many researchers have turned to leveraging human manipulation videos available online. However, current methods predominantly focus on hand detection or object pose estimation, failing to fully exploit the rich interaction cues embedded in these videos. In this work, we propose a novel approach that combines large foundation models for video understanding with point tracking techniques to extract dense trajectories of all task-relevant keypoints during manipulation. This enables more comprehensive utilization of Internet-scale human demonstration videos. Experimental results demonstrate that our method can accurately track keypoints throughout the entire manipulation process, paving the way for more scalable and data-efficient robot learning.

