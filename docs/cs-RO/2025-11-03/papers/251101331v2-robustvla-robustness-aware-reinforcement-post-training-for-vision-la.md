---
layout: default
title: RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models
---

# RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.01331" target="_blank" class="toolbar-btn">arXiv: 2511.01331v2</a>
    <a href="https://arxiv.org/pdf/2511.01331.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01331v2" 
            onclick="toggleFavorite(this, '2511.01331v2', 'RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Hongyin Zhang, Shuo Zhang, Junxi Jin, Qixin Zeng, Runze Li, Donglin Wang

**ÂàÜÁ±ª**: cs.RO, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-03 (Êõ¥Êñ∞: 2025-12-01)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**RobustVLAÔºöÈù¢ÂêëËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂº∫ÂåñÂêéËÆ≠ÁªÉ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `È≤ÅÊ£íÊÄß` `ÂêéËÆ≠ÁªÉ` `ÈõÖÂèØÊØîÊ≠£ÂàôÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®ÂÆûÈôÖÊú∫Âô®‰∫∫ÈÉ®ÁΩ≤‰∏≠ÔºåÈù¢ÂØπËßÇÊµãÂô™Â£∞ÂíåÂä®‰ΩúÊâ∞Âä®Á≠âÈóÆÈ¢òÊó∂ÔºåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÈ≤ÅÊ£íÊÄßËæÉÂ∑Æ„ÄÇ
2. RobustVLAÈÄöËøáÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÂêéËÆ≠ÁªÉÔºåÊòæÂºèÂú∞ÊèêÂçáVLAÊ®°ÂûãÂØπÁéØÂ¢É‰∏çÁ°ÆÂÆöÊÄßÁöÑÈ≤ÅÊ£íÊÄßÔºåÊ†∏ÂøÉÂú®‰∫éÈõÖÂèØÊØîÊ≠£ÂàôÂåñÂíåÂπ≥ÊªëÊÄßÊ≠£ÂàôÂåñ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåRobustVLAÂú®Â§öÁßçÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÊòæËëóÊèêÂçá‰∫ÜVLAÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÂèØÈù†ÊÄßÔºå‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂèóÁõä‰∫éÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÈ¢ÑËÆ≠ÁªÉÔºåÂ∑≤Êàê‰∏∫Êú∫Âô®‰∫∫Êìç‰ΩúÈ¢ÜÂüüÂº∫Â§ßÁöÑÈÄöÁî®Á≠ñÁï•„ÄÇÁÑ∂ËÄåÔºåÂú®ÂàÜÂ∏ÉÂ§ñÁöÑÈÉ®ÁΩ≤‰∏≠ÔºåÁî±‰∫é‰∏çÂèØÈÅøÂÖçÁöÑÊâ∞Âä®ÔºàÂ¶ÇËßÇÊµãÂô™Â£∞„ÄÅ‰º†ÊÑüÂô®ËØØÂ∑ÆÊàñÊâßË°åÊâ∞Âä®ÔºâÊôÆÈÅçÂ≠òÂú®ÔºåÂÆÉ‰ª¨ÈÄöÂ∏∏Êó†Ê≥ïÂèØÈù†Âú∞Ê≥õÂåñ„ÄÇËôΩÁÑ∂ÊúÄËøëÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†(RL)ÁöÑÂêéËÆ≠ÁªÉ‰∏∫Ë∞ÉÊï¥È¢ÑËÆ≠ÁªÉVLAÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏ÄÁßçÂÆûÁî®ÈÄîÂæÑÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂº∫Ë∞ÉÂ•ñÂä±ÊúÄÂ§ßÂåñÔºåËÄåÂøΩÁï•‰∫ÜÂØπÁéØÂ¢É‰∏çÁ°ÆÂÆöÊÄßÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜRobustVLAÔºå‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑÂú®Á∫øRLÂêéËÆ≠ÁªÉÊñπÊ≥ïÔºåÊó®Âú®ÊòæÂºèÂú∞Â¢ûÂº∫VLAÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÈÄöËøáÁ≥ªÁªüÁöÑÈ≤ÅÊ£íÊÄßÂàÜÊûêÔºåÊàë‰ª¨Á°ÆÂÆö‰∫Ü‰∏§‰∏™ÂÖ≥ÈîÆÁöÑÊ≠£ÂàôÂåñÈ°πÔºöÈõÖÂèØÊØîÊ≠£ÂàôÂåñÔºåÁî®‰∫éÂáèËΩªÂØπËßÇÊµãÂô™Â£∞ÁöÑÊïèÊÑüÊÄßÔºõÂπ≥ÊªëÊÄßÊ≠£ÂàôÂåñÔºåÁî®‰∫éÁ®≥ÂÆöÂä®‰ΩúÊâ∞Âä®‰∏ãÁöÑÁ≠ñÁï•„ÄÇÂú®ÂêÑÁßçÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠ÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåRobustVLAÂú®È≤ÅÊ£íÊÄßÂíåÂèØÈù†ÊÄßÊñπÈù¢ÊòæËëó‰ºò‰∫éÂÖàÂâçÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÂº∫Ë∞É‰∫Ü‰ª•ÂéüÂàôÊÄßÁöÑÈ≤ÅÊ£íÊÄß‰∏∫ÂØºÂêëÁöÑRLÂêéËÆ≠ÁªÉ‰Ωú‰∏∫ÊèêÈ´òVLAÊ®°ÂûãÂèØÈù†ÊÄßÂíåÈ≤ÅÊ£íÊÄßÁöÑÂÖ≥ÈîÆÊ≠•È™§ÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöVLAÊ®°ÂûãÂú®ÂÆûÈôÖÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÔºåÂÆπÊòìÂèóÂà∞ËßÇÊµãÂô™Â£∞„ÄÅ‰º†ÊÑüÂô®ËØØÂ∑ÆÂíåÂä®‰ΩúÊâ∞Âä®Á≠âÂõ†Á¥†ÁöÑÂΩ±ÂìçÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôçÁîöËá≥Â§±Êïà„ÄÇÁé∞ÊúâÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂêéËÆ≠ÁªÉÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®Â•ñÂä±ÊúÄÂ§ßÂåñÔºåÂøΩÁï•‰∫ÜÂØπÁéØÂ¢É‰∏çÁ°ÆÂÆöÊÄßÁöÑÈ≤ÅÊ£íÊÄßÔºåÊó†Ê≥ïÊúâÊïàËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöRobustVLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂêéËÆ≠ÁªÉÔºåÊòæÂºèÂú∞ÊèêÂçáVLAÊ®°ÂûãÂØπÁéØÂ¢É‰∏çÁ°ÆÂÆöÊÄßÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáÂºïÂÖ•ÈõÖÂèØÊØîÊ≠£ÂàôÂåñÂíåÂπ≥ÊªëÊÄßÊ≠£ÂàôÂåñÔºåÂàÜÂà´Èôç‰ΩéÊ®°ÂûãÂØπËßÇÊµãÂô™Â£∞ÁöÑÊïèÊÑüÊÄßÂíåÁ®≥ÂÆöÂä®‰ΩúÊâ∞Âä®‰∏ãÁöÑÁ≠ñÁï•„ÄÇËøôÁßçÊñπÊ≥ïÊó®Âú®‰ΩøÊ®°ÂûãÂú®Èù¢ÂØπÂêÑÁßçÊâ∞Âä®Êó∂Ôºå‰ªçËÉΩ‰øùÊåÅËæÉÂ•ΩÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRobustVLAÈááÁî®Âú®Á∫øÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ËøõË°åÂêéËÆ≠ÁªÉ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) VLAÊ®°ÂûãÔºö‰Ωú‰∏∫Á≠ñÁï•ÁΩëÁªúÔºåÊé•Êî∂ËßÜËßâÂíåËØ≠Ë®ÄËæìÂÖ•ÔºåËæìÂá∫Âä®‰ΩúÊåá‰ª§„ÄÇ2) Âº∫ÂåñÂ≠¶‰π†ÁéØÂ¢ÉÔºöÊ®°ÊãüÁúüÂÆûÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÁéØÂ¢ÉÔºåÂåÖÊã¨ÂêÑÁßçÊâ∞Âä®„ÄÇ3) Â•ñÂä±ÂáΩÊï∞ÔºöÁî®‰∫éËØÑ‰º∞VLAÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ4) ÈõÖÂèØÊØîÊ≠£ÂàôÂåñÊ®°ÂùóÔºöËÆ°ÁÆóÁ≠ñÁï•ÁΩëÁªúËæìÂá∫ÂØπËæìÂÖ•ÁöÑÈõÖÂèØÊØîÁü©ÈòµÔºåÂπ∂ËøõË°åÊ≠£ÂàôÂåñ„ÄÇ5) Âπ≥ÊªëÊÄßÊ≠£ÂàôÂåñÊ®°ÂùóÔºöÂØπËøûÁª≠Êó∂ÂàªÁöÑÂä®‰ΩúËæìÂá∫ËøõË°åÂπ≥ÊªëÊÄßÁ∫¶Êùü„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRobustVLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏§ÁßçÊñ∞ÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ïÔºöÈõÖÂèØÊØîÊ≠£ÂàôÂåñÂíåÂπ≥ÊªëÊÄßÊ≠£ÂàôÂåñ„ÄÇÈõÖÂèØÊØîÊ≠£ÂàôÂåñÈÄöËøáÁ∫¶ÊùüÁ≠ñÁï•ÁΩëÁªúËæìÂá∫ÂØπËæìÂÖ•ÁöÑÊïèÊÑüÊÄßÔºåÈôç‰Ωé‰∫ÜÊ®°ÂûãÂØπËßÇÊµãÂô™Â£∞ÁöÑ‰æùËµñ„ÄÇÂπ≥ÊªëÊÄßÊ≠£ÂàôÂåñÈÄöËøáÁ∫¶ÊùüËøûÁª≠Êó∂ÂàªÁöÑÂä®‰ΩúËæìÂá∫ÔºåÁ®≥ÂÆö‰∫ÜÂä®‰ΩúÊâ∞Âä®‰∏ãÁöÑÁ≠ñÁï•„ÄÇËøô‰∏§ÁßçÊ≠£ÂàôÂåñÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞ÊèêÂçáVLAÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÈõÖÂèØÊØîÊ≠£ÂàôÂåñÊçüÂ§±ÂáΩÊï∞‰∏∫Á≠ñÁï•ÁΩëÁªúËæìÂá∫ÂØπËæìÂÖ•ÁöÑÈõÖÂèØÊØîÁü©ÈòµÁöÑFrobeniusËåÉÊï∞„ÄÇÂπ≥ÊªëÊÄßÊ≠£ÂàôÂåñÊçüÂ§±ÂáΩÊï∞‰∏∫ËøûÁª≠Êó∂ÂàªÂä®‰ΩúËæìÂá∫ÁöÑÂ∑ÆÁöÑÂπ≥Êñπ„ÄÇËøô‰∏§‰∏™ÊçüÂ§±ÂáΩÊï∞‰∏éÂ•ñÂä±ÂáΩÊï∞ÁªìÂêàÔºåÂÖ±Âêå‰ºòÂåñVLAÊ®°Âûã„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÔºàÂ¶ÇÊ≠£ÂàôÂåñÁ≥ªÊï∞ÔºâÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRobustVLAÂú®Â§ö‰∏™Êú∫Âô®‰∫∫Êìç‰ΩúÁéØÂ¢É‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®Â≠òÂú®ËßÇÊµãÂô™Â£∞ÂíåÂä®‰ΩúÊâ∞Âä®ÁöÑÁéØÂ¢É‰∏≠ÔºåRobustVLAÁöÑÊàêÂäüÁéáÊØîÂü∫Á∫øÊñπÊ≥ïÊèêÈ´ò‰∫Ü15%-20%„ÄÇÊ≠§Â§ñÔºåRobustVLAËøòË°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåËÉΩÂ§üÂú®Êú™ËßÅËøáÁöÑÁéØÂ¢É‰∏≠‰øùÊåÅËæÉÈ´òÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

RobustVLAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÊèêÂçáÂêÑÁßçÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°ÁöÑÂèØÈù†ÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºå‰æãÂ¶ÇÔºöÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂåªÁñóÊú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÊèêÈ´òVLAÊ®°ÂûãÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îËÉΩÂäõÔºåÂèØ‰ª•Èôç‰ΩéÈÉ®ÁΩ≤ÊàêÊú¨ÔºåÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÔºåÂπ∂Êâ©Â±ïÊú∫Âô®‰∫∫ÁöÑÂ∫îÁî®ËåÉÂõ¥„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂà∂ÈÄ†Á≠âÈ¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models have recently emerged as powerful general-purpose policies for robotic manipulation, benefiting from large-scale multi-modal pre-training. However, they often fail to generalize reliably in out-of-distribution deployments, where unavoidable disturbances such as observation noise, sensor errors, or actuation perturbations become prevalent. While recent Reinforcement Learning (RL)-based post-training provides a practical means to adapt pre-trained VLA models, existing methods mainly emphasize reward maximization and overlook robustness to environmental uncertainty. In this work, we introduce RobustVLA, a lightweight online RL post-training method designed to explicitly enhance the resilience of VLA models. Through a systematic robustness analysis, we identify two key regularizations: Jacobian regularization, which mitigates sensitivity to observation noise, and smoothness regularization, which stabilizes policies under action perturbations. Extensive experiments across diverse robotic environments demonstrate that RobustVLA significantly outperforms prior state-of-the-art methods in robustness and reliability. Our results highlight the importance of principled robustness-aware RL post-training as a key step toward improving the reliability and robustness of VLA models.

