---
layout: default
title: Embodiment Transfer Learning for Vision-Language-Action Models
---

# Embodiment Transfer Learning for Vision-Language-Action Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.01224" target="_blank" class="toolbar-btn">arXiv: 2511.01224v1</a>
    <a href="https://arxiv.org/pdf/2511.01224.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01224v1" 
            onclick="toggleFavorite(this, '2511.01224v1', 'Embodiment Transfer Learning for Vision-Language-Action Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Chengmeng Li, Yaxin Peng

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-03

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ET-VLAÊ°ÜÊû∂ÔºåÈÄöËøáÂÖ∑Ë∫´ËøÅÁßªÂ≠¶‰π†ÊèêÂçáVLAÊ®°ÂûãÂú®Â§öÊú∫Âô®‰∫∫Âçè‰Ωú‰∏≠ÁöÑÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)**

**ÂÖ≥ÈîÆËØç**: `ÂÖ∑Ë∫´Êô∫ËÉΩ` `ËøÅÁßªÂ≠¶‰π†` `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Â§öÊú∫Âô®‰∫∫Âçè‰Ωú` `ÂêàÊàêÊï∞ÊçÆ` `Êú∫Âô®‰∫∫Â≠¶‰π†` `ÂõæÁ•ûÁªèÁΩëÁªú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®Â§öÊú∫Âô®‰∫∫Âçè‰Ωú‰ªªÂä°‰∏≠Ë°®Áé∞‰∏çË∂≥ÔºåÈöæ‰ª•ÊúâÊïàÂå∫ÂàÜÂíåÂà©Áî®‰∏çÂêåÊú∫Âô®‰∫∫ÁöÑÂäüËÉΩ„ÄÇ
2. ET-VLAÈÄöËøáÂêàÊàêÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉÂíåÂÖ∑Ë∫´ÂõæÊÄùÁª¥Ôºå‰ΩøÊ®°ÂûãËÉΩÊõ¥Â•ΩÈÄÇÂ∫îÊñ∞ÂÖ∑Ë∫´Âπ∂ÁêÜËß£Â§öÊú∫Âô®‰∫∫ËßíËâ≤„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåET-VLAÂú®ÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáË∂ÖËøá53.2%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®Êú∫Âô®‰∫∫Â≠¶‰π†È¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåËÉΩÂ§üÂà©Áî®Â§ßËßÑÊ®°„ÄÅË∑®ÂÖ∑Ë∫´Êï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÂπ∂ÈíàÂØπÁâπÂÆöÊú∫Âô®‰∫∫ËøõË°åÂæÆË∞É„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÊúÄÂÖàËøõÁöÑËá™ÂõûÂΩíVLAÊ®°ÂûãÂú®Â§öÊú∫Âô®‰∫∫Âçè‰ΩúÊñπÈù¢Ë°®Áé∞‰∏ç‰Ω≥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖ∑Ë∫´ËøÅÁßªÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁß∞‰∏∫ET-VLAÔºåÁî®‰∫éÈ´òÊïà‰∏îÊúâÊïàÂú∞Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑVLAÊ®°ÂûãËøÅÁßªÂà∞Â§öÊú∫Âô®‰∫∫Âú∫ÊôØ„ÄÇET-VLAÁöÑÊ†∏ÂøÉÊòØÂêàÊàêÊåÅÁª≠È¢ÑËÆ≠ÁªÉ(SCP)ÔºåÂÆÉ‰ΩøÁî®ÂêàÊàêÁîüÊàêÁöÑÊï∞ÊçÆÊù•È¢ÑÁÉ≠Ê®°ÂûãÔºå‰ΩøÂÖ∂ÈÄÇÂ∫îÊñ∞ÁöÑÂÖ∑Ë∫´Ôºå‰ªéËÄåÈÅøÂÖç‰∫ÜÂØπÁúüÂÆû‰∫∫Á±ªÊºîÁ§∫ÁöÑÈúÄÊ±ÇÔºåÂπ∂Èôç‰Ωé‰∫ÜÊï∞ÊçÆÊî∂ÈõÜÊàêÊú¨„ÄÇSCP‰ΩøÊ®°ÂûãËÉΩÂ§üÂ≠¶‰π†Ê≠£Á°ÆÁöÑÂä®‰ΩúÂíåÁ≤æÁ°ÆÁöÑÂä®‰ΩútokenÊï∞Èáè„ÄÇÂú®SCP‰πãÂêéÔºåÊ®°ÂûãÂú®ÁõÆÊ†áÂÖ∑Ë∫´Êï∞ÊçÆ‰∏äËøõË°åÂæÆË∞É„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÊèêÈ´òÊ®°ÂûãÂú®Â§öÂÖ∑Ë∫´ÁéØÂ¢É‰∏ãÁöÑÊÄßËÉΩÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖ∑Ë∫´ÂõæÊÄùÁª¥ÊäÄÊúØÔºåËØ•ÊäÄÊúØÂ∞ÜÊØè‰∏™Â≠ê‰ªªÂä°Ë°®Á§∫‰∏∫‰∏Ä‰∏™ËäÇÁÇπÔºå‰ΩøVLAÊ®°ÂûãËÉΩÂ§üÂú®‰ªªÂä°ÊâßË°åÊúüÈó¥Âå∫ÂàÜÊØè‰∏™ÂÖ∑Ë∫´ÁöÑÂäüËÉΩÂíåËßíËâ≤„ÄÇÊú¨Êñá‰ª•ÂèåËáÇÊú∫Âô®‰∫∫‰∏∫‰æãÔºåÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÂú®Â§öÊú∫Âô®‰∫∫Âú∫ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂú®Ê®°ÊãüÂü∫ÂáÜÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑET-VLAÂú®ÂÖ≠‰∏™ÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏äÁöÑÊÄßËÉΩË∂ÖËøáOpenVLA 53.2%„ÄÇÊàë‰ª¨Â∞ÜÂºÄÊ∫êÊâÄÊúâ‰ª£Á†ÅÔºå‰ª•ÊîØÊåÅÁ§æÂå∫Êé®ËøõVLAÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Â≠¶‰π†‰∏≠ÁöÑÂèëÂ±ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®ÂçïÊú∫Âô®‰∫∫‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂΩìÂ∫îÁî®‰∫éÂ§öÊú∫Âô®‰∫∫Âçè‰ΩúÂú∫ÊôØÊó∂ÔºåÈù¢‰∏¥ÁùÄÊåëÊàò„ÄÇ‰∏ªË¶ÅÁóõÁÇπÂú®‰∫éÊ®°ÂûãÈöæ‰ª•Âå∫ÂàÜ‰∏çÂêåÊú∫Âô®‰∫∫ÁöÑÂäüËÉΩÂíåËßíËâ≤ÔºåÂØºËá¥Âçè‰ΩúÊïàÁéá‰Ωé‰∏ãÔºåÁîöËá≥‰ªªÂä°Â§±Ë¥•„ÄÇÊ≠§Â§ñÔºåÈíàÂØπÊØè‰∏™Êñ∞ÁöÑÂ§öÊú∫Âô®‰∫∫Á≥ªÁªüÔºåÈÉΩÈúÄË¶ÅÂ§ßÈáèÁöÑÁúüÂÆûÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÊàêÊú¨È´òÊòÇ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöET-VLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂÖ∑Ë∫´ËøÅÁßªÂ≠¶‰π†ÔºåÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑVLAÊ®°ÂûãÈ´òÊïàÂú∞ËøÅÁßªÂà∞Â§öÊú∫Âô®‰∫∫Á≥ªÁªü„ÄÇÈÄöËøáÂêàÊàêÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉ(SCP)Êù•Ê®°Êãü‰∏çÂêåÊú∫Âô®‰∫∫ÁöÑÂÖ∑Ë∫´ÁâπÊÄßÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂø´ÈÄüÈÄÇÂ∫îÊñ∞ÁöÑÊú∫Âô®‰∫∫„ÄÇÂêåÊó∂ÔºåÂºïÂÖ•ÂÖ∑Ë∫´ÂõæÊÄùÁª¥(Embodied Graph-of-Thought)ÊäÄÊúØÔºåÂ∞Ü‰ªªÂä°ÂàÜËß£‰∏∫Â≠ê‰ªªÂä°ÔºåÂπ∂‰∏∫ÊØè‰∏™Êú∫Âô®‰∫∫ÂàÜÈÖçÁâπÂÆöÁöÑËßíËâ≤Ôºå‰ªéËÄåÊèêÈ´òÂçè‰ΩúÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöET-VLAÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºöÂêàÊàêÊåÅÁª≠È¢ÑËÆ≠ÁªÉ(SCP)ÂíåÁõÆÊ†áÂÖ∑Ë∫´ÂæÆË∞É„ÄÇÂú®SCPÈò∂ÊÆµÔºåÂà©Áî®ÂêàÊàêÊï∞ÊçÆÁîüÊàêÂô®ÔºåÊ®°Êãü‰∏çÂêåÊú∫Âô®‰∫∫ÁöÑËßÜËßâÂíåÂä®‰ΩúÁ©∫Èó¥ÔºåÂØπÈ¢ÑËÆ≠ÁªÉÁöÑVLAÊ®°ÂûãËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇÂú®ÁõÆÊ†áÂÖ∑Ë∫´ÂæÆË∞ÉÈò∂ÊÆµÔºå‰ΩøÁî®Â∞ëÈáèÁúüÂÆûÊï∞ÊçÆÂØπÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂ÈÄÇÂ∫îÁâπÂÆöÁöÑÂ§öÊú∫Âô®‰∫∫Á≥ªÁªü„ÄÇÊ≠§Â§ñÔºåÂú®‰ªªÂä°ÊâßË°åËøáÁ®ã‰∏≠Ôºå‰ΩøÁî®ÂÖ∑Ë∫´ÂõæÊÄùÁª¥ÊäÄÊúØÔºåÂ∞Ü‰ªªÂä°ÂàÜËß£‰∏∫Â≠ê‰ªªÂä°ÔºåÂπ∂‰∏∫ÊØè‰∏™Êú∫Âô®‰∫∫ÂàÜÈÖçÁâπÂÆöÁöÑËßíËâ≤„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöET-VLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ª•‰∏ã‰∏§ÁÇπÔºö‰∏ÄÊòØÊèêÂá∫‰∫ÜÂêàÊàêÊåÅÁª≠È¢ÑËÆ≠ÁªÉ(SCP)ÊñπÊ≥ïÔºåÂà©Áî®ÂêàÊàêÊï∞ÊçÆÊù•È¢ÑÁÉ≠Ê®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂø´ÈÄüÈÄÇÂ∫îÊñ∞ÁöÑÂÖ∑Ë∫´ÔºåÈôç‰Ωé‰∫ÜÊï∞ÊçÆÊî∂ÈõÜÊàêÊú¨„ÄÇ‰∫åÊòØÂºïÂÖ•‰∫ÜÂÖ∑Ë∫´ÂõæÊÄùÁª¥(Embodied Graph-of-Thought)ÊäÄÊúØÔºåÂ∞Ü‰ªªÂä°ÂàÜËß£‰∏∫Â≠ê‰ªªÂä°ÔºåÂπ∂‰∏∫ÊØè‰∏™Êú∫Âô®‰∫∫ÂàÜÈÖçÁâπÂÆöÁöÑËßíËâ≤Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂçè‰ΩúÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSCPÈò∂ÊÆµÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂêàÊàêÊï∞ÊçÆÁöÑÁîüÊàêÊñπÂºè„ÄÇËÆ∫Êñá‰ΩøÁî®Á®ãÂ∫èÂåñÁîüÊàêÊñπÊ≥ïÔºåÊ®°Êãü‰∏çÂêåÊú∫Âô®‰∫∫ÁöÑËßÜËßâÂíåÂä®‰ΩúÁ©∫Èó¥„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáÈöèÊú∫ÊîπÂèòÊú∫Âô®‰∫∫ÁöÑÂΩ¢Áä∂„ÄÅÈ¢úËâ≤ÂíåÂÖ≥ËäÇËßíÂ∫¶ÔºåÁîüÊàê‰∏çÂêåÁöÑËßÜËßâÂõæÂÉè„ÄÇÂêåÊó∂ÔºåÈÄöËøáÈöèÊú∫ÁªÑÂêà‰∏çÂêåÁöÑÂä®‰ΩúÂ∫èÂàóÔºåÁîüÊàê‰∏çÂêåÁöÑÂä®‰ΩúÊåá‰ª§„ÄÇÂú®ÂÖ∑Ë∫´ÂõæÊÄùÁª¥ÊñπÈù¢ÔºåËÆ∫Êñá‰ΩøÁî®ÂõæÁ•ûÁªèÁΩëÁªúÊù•Âª∫Ê®°Â≠ê‰ªªÂä°‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºåÂπ∂‰∏∫ÊØè‰∏™Êú∫Âô®‰∫∫ÂàÜÈÖç‰∏Ä‰∏™ËäÇÁÇπÔºåË°®Á§∫ÂÖ∂Âú®Â≠ê‰ªªÂä°‰∏≠ÁöÑËßíËâ≤„ÄÇÊçüÂ§±ÂáΩÊï∞ÊñπÈù¢Ôºå‰ΩøÁî®‰∫Ü‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÊ®°ÂûãÁöÑÂä®‰ΩúÈ¢ÑÊµãËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåET-VLAÂú®ÂÖ≠‰∏™ÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏äÁöÑÊÄßËÉΩË∂ÖËøáOpenVLA 53.2%„ÄÇÂú®Ê®°ÊãüÁéØÂ¢É‰∏≠ÔºåET-VLA‰πüÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫ÜET-VLAÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄßÔºåË°®ÊòéÂÖ∂ËÉΩÂ§üÊòæËëóÊèêÈ´òVLAÊ®°ÂûãÂú®Â§öÊú∫Âô®‰∫∫Âçè‰Ωú‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂ§öÊú∫Âô®‰∫∫Âçè‰ΩúÂú∫ÊôØÔºå‰æãÂ¶ÇÔºöËá™Âä®ÂåñË£ÖÈÖçÁ∫ø„ÄÅÂçèÂêåÊê¨Ëøê„ÄÅÂåªÁñóÊâãÊúØÁ≠â„ÄÇÈÄöËøáET-VLAÊ°ÜÊû∂ÔºåÂèØ‰ª•Âø´ÈÄüÈÉ®ÁΩ≤Êñ∞ÁöÑÂ§öÊú∫Âô®‰∫∫Á≥ªÁªüÔºåÈôç‰ΩéÂºÄÂèëÊàêÊú¨ÔºåÊèêÈ´òÁîü‰∫ßÊïàÁéá„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éÊõ¥Â§çÊùÇÁöÑÊú∫Âô®‰∫∫Âçè‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇÔºöÁÅæÈöæÊïëÊè¥„ÄÅÂ§™Á©∫Êé¢Á¥¢Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-language-action (VLA) models have significantly advanced robotic learning, enabling training on large-scale, cross-embodiment data and fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs struggle with multi-robot collaboration. We introduce embodiment transfer learning, denoted as ET-VLA, a novel framework for efficient and effective transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic Continued Pretraining (SCP), which uses synthetically generated data to warm up the model for the new embodiment, bypassing the need for real human demonstrations and reducing data collection costs. SCP enables the model to learn correct actions and precise action token numbers. Following SCP, the model is fine-tuned on target embodiment data. To further enhance the model performance on multi-embodiment, we present the Embodied Graph-of-Thought technique, a novel approach that formulates each sub-task as a node, that allows the VLA model to distinguish the functionalities and roles of each embodiment during task execution. Our work considers bimanual robots, a simple version of multi-robot to verify our approaches. We validate the effectiveness of our method on both simulation benchmarks and real robots covering three different bimanual embodiments. In particular, our proposed ET-VLA \space can outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all codes to support the community in advancing VLA models for robot learning.

