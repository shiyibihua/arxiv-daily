---
layout: default
title: Embodiment Transfer Learning for Vision-Language-Action Models
---

# Embodiment Transfer Learning for Vision-Language-Action Models

**arXiv**: [2511.01224v1](https://arxiv.org/abs/2511.01224) | [PDF](https://arxiv.org/pdf/2511.01224.pdf)

**ä½œè€…**: Chengmeng Li, Yaxin Peng

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-03

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºET-VLAæ¡†æž¶ï¼Œé€šè¿‡å…·èº«è¿ç§»å­¦ä¹ æå‡VLAæ¨¡åž‹åœ¨å¤šæœºå™¨äººåä½œä¸­çš„æ€§èƒ½**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `è¿ç§»å­¦ä¹ ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `å¤šæœºå™¨äººåä½œ` `åˆæˆæ•°æ®` `æœºå™¨äººå­¦ä¹ ` `å›¾ç¥žç»ç½‘ç»œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åœ¨å¤šæœºå™¨äººåä½œä»»åŠ¡ä¸­è¡¨çŽ°ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆåŒºåˆ†å’Œåˆ©ç”¨ä¸åŒæœºå™¨äººçš„åŠŸèƒ½ã€‚
2. ET-VLAé€šè¿‡åˆæˆæ•°æ®é¢„è®­ç»ƒå’Œå…·èº«å›¾æ€ç»´ï¼Œä½¿æ¨¡åž‹èƒ½æ›´å¥½é€‚åº”æ–°å…·èº«å¹¶ç†è§£å¤šæœºå™¨äººè§’è‰²ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒET-VLAåœ¨çœŸå®žæœºå™¨äººä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½æå‡è¶…è¿‡53.2%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹åœ¨æœºå™¨äººå­¦ä¹ é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿåˆ©ç”¨å¤§è§„æ¨¡ã€è·¨å…·èº«æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶é’ˆå¯¹ç‰¹å®šæœºå™¨äººè¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œç›®å‰æœ€å…ˆè¿›çš„è‡ªå›žå½’VLAæ¨¡åž‹åœ¨å¤šæœºå™¨äººåä½œæ–¹é¢è¡¨çŽ°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·èº«è¿ç§»å­¦ä¹ æ¡†æž¶ï¼Œç§°ä¸ºET-VLAï¼Œç”¨äºŽé«˜æ•ˆä¸”æœ‰æ•ˆåœ°å°†é¢„è®­ç»ƒçš„VLAæ¨¡åž‹è¿ç§»åˆ°å¤šæœºå™¨äººåœºæ™¯ã€‚ET-VLAçš„æ ¸å¿ƒæ˜¯åˆæˆæŒç»­é¢„è®­ç»ƒ(SCP)ï¼Œå®ƒä½¿ç”¨åˆæˆç”Ÿæˆçš„æ•°æ®æ¥é¢„çƒ­æ¨¡åž‹ï¼Œä½¿å…¶é€‚åº”æ–°çš„å…·èº«ï¼Œä»Žè€Œé¿å…äº†å¯¹çœŸå®žäººç±»æ¼”ç¤ºçš„éœ€æ±‚ï¼Œå¹¶é™ä½Žäº†æ•°æ®æ”¶é›†æˆæœ¬ã€‚SCPä½¿æ¨¡åž‹èƒ½å¤Ÿå­¦ä¹ æ­£ç¡®çš„åŠ¨ä½œå’Œç²¾ç¡®çš„åŠ¨ä½œtokenæ•°é‡ã€‚åœ¨SCPä¹‹åŽï¼Œæ¨¡åž‹åœ¨ç›®æ ‡å…·èº«æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡åž‹åœ¨å¤šå…·èº«çŽ¯å¢ƒä¸‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·èº«å›¾æ€ç»´æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†æ¯ä¸ªå­ä»»åŠ¡è¡¨ç¤ºä¸ºä¸€ä¸ªèŠ‚ç‚¹ï¼Œä½¿VLAæ¨¡åž‹èƒ½å¤Ÿåœ¨ä»»åŠ¡æ‰§è¡ŒæœŸé—´åŒºåˆ†æ¯ä¸ªå…·èº«çš„åŠŸèƒ½å’Œè§’è‰²ã€‚æœ¬æ–‡ä»¥åŒè‡‚æœºå™¨äººä¸ºä¾‹ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šæœºå™¨äººåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨æ¨¡æ‹ŸåŸºå‡†å’ŒçœŸå®žæœºå™¨äººä¸Šçš„å®žéªŒç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬æå‡ºçš„ET-VLAåœ¨å…­ä¸ªçœŸå®žä¸–ç•Œä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡OpenVLA 53.2%ã€‚æˆ‘ä»¬å°†å¼€æºæ‰€æœ‰ä»£ç ï¼Œä»¥æ”¯æŒç¤¾åŒºæŽ¨è¿›VLAæ¨¡åž‹åœ¨æœºå™¨äººå­¦ä¹ ä¸­çš„å‘å±•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹åœ¨å•æœºå™¨äººä»»åŠ¡ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œä½†å½“åº”ç”¨äºŽå¤šæœºå™¨äººåä½œåœºæ™¯æ—¶ï¼Œé¢ä¸´ç€æŒ‘æˆ˜ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºŽæ¨¡åž‹éš¾ä»¥åŒºåˆ†ä¸åŒæœºå™¨äººçš„åŠŸèƒ½å’Œè§’è‰²ï¼Œå¯¼è‡´åä½œæ•ˆçŽ‡ä½Žä¸‹ï¼Œç”šè‡³ä»»åŠ¡å¤±è´¥ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹æ¯ä¸ªæ–°çš„å¤šæœºå™¨äººç³»ç»Ÿï¼Œéƒ½éœ€è¦å¤§é‡çš„çœŸå®žæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šET-VLAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å…·èº«è¿ç§»å­¦ä¹ ï¼Œå°†é¢„è®­ç»ƒçš„VLAæ¨¡åž‹é«˜æ•ˆåœ°è¿ç§»åˆ°å¤šæœºå™¨äººç³»ç»Ÿã€‚é€šè¿‡åˆæˆæ•°æ®é¢„è®­ç»ƒ(SCP)æ¥æ¨¡æ‹Ÿä¸åŒæœºå™¨äººçš„å…·èº«ç‰¹æ€§ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°çš„æœºå™¨äººã€‚åŒæ—¶ï¼Œå¼•å…¥å…·èº«å›¾æ€ç»´(Embodied Graph-of-Thought)æŠ€æœ¯ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªæœºå™¨äººåˆ†é…ç‰¹å®šçš„è§’è‰²ï¼Œä»Žè€Œæé«˜åä½œæ•ˆçŽ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šET-VLAæ¡†æž¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šåˆæˆæŒç»­é¢„è®­ç»ƒ(SCP)å’Œç›®æ ‡å…·èº«å¾®è°ƒã€‚åœ¨SCPé˜¶æ®µï¼Œåˆ©ç”¨åˆæˆæ•°æ®ç”Ÿæˆå™¨ï¼Œæ¨¡æ‹Ÿä¸åŒæœºå™¨äººçš„è§†è§‰å’ŒåŠ¨ä½œç©ºé—´ï¼Œå¯¹é¢„è®­ç»ƒçš„VLAæ¨¡åž‹è¿›è¡Œé¢„è®­ç»ƒã€‚åœ¨ç›®æ ‡å…·èº«å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨å°‘é‡çœŸå®žæ•°æ®å¯¹æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”ç‰¹å®šçš„å¤šæœºå™¨äººç³»ç»Ÿã€‚æ­¤å¤–ï¼Œåœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å…·èº«å›¾æ€ç»´æŠ€æœ¯ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªæœºå™¨äººåˆ†é…ç‰¹å®šçš„è§’è‰²ã€‚

**å…³é”®åˆ›æ–°**ï¼šET-VLAçš„å…³é”®åˆ›æ–°åœ¨äºŽä»¥ä¸‹ä¸¤ç‚¹ï¼šä¸€æ˜¯æå‡ºäº†åˆæˆæŒç»­é¢„è®­ç»ƒ(SCP)æ–¹æ³•ï¼Œåˆ©ç”¨åˆæˆæ•°æ®æ¥é¢„çƒ­æ¨¡åž‹ï¼Œä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°çš„å…·èº«ï¼Œé™ä½Žäº†æ•°æ®æ”¶é›†æˆæœ¬ã€‚äºŒæ˜¯å¼•å…¥äº†å…·èº«å›¾æ€ç»´(Embodied Graph-of-Thought)æŠ€æœ¯ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªæœºå™¨äººåˆ†é…ç‰¹å®šçš„è§’è‰²ï¼Œä»Žè€Œæé«˜äº†åä½œæ•ˆçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šSCPé˜¶æ®µçš„å…³é”®è®¾è®¡åœ¨äºŽåˆæˆæ•°æ®çš„ç”Ÿæˆæ–¹å¼ã€‚è®ºæ–‡ä½¿ç”¨ç¨‹åºåŒ–ç”Ÿæˆæ–¹æ³•ï¼Œæ¨¡æ‹Ÿä¸åŒæœºå™¨äººçš„è§†è§‰å’ŒåŠ¨ä½œç©ºé—´ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡éšæœºæ”¹å˜æœºå™¨äººçš„å½¢çŠ¶ã€é¢œè‰²å’Œå…³èŠ‚è§’åº¦ï¼Œç”Ÿæˆä¸åŒçš„è§†è§‰å›¾åƒã€‚åŒæ—¶ï¼Œé€šè¿‡éšæœºç»„åˆä¸åŒçš„åŠ¨ä½œåºåˆ—ï¼Œç”Ÿæˆä¸åŒçš„åŠ¨ä½œæŒ‡ä»¤ã€‚åœ¨å…·èº«å›¾æ€ç»´æ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨å›¾ç¥žç»ç½‘ç»œæ¥å»ºæ¨¡å­ä»»åŠ¡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶ä¸ºæ¯ä¸ªæœºå™¨äººåˆ†é…ä¸€ä¸ªèŠ‚ç‚¹ï¼Œè¡¨ç¤ºå…¶åœ¨å­ä»»åŠ¡ä¸­çš„è§’è‰²ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡åž‹çš„åŠ¨ä½œé¢„æµ‹èƒ½åŠ›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒET-VLAåœ¨å…­ä¸ªçœŸå®žä¸–ç•Œä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡OpenVLA 53.2%ã€‚åœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­ï¼ŒET-VLAä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æžœéªŒè¯äº†ET-VLAæ¡†æž¶çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜Žå…¶èƒ½å¤Ÿæ˜¾è‘—æé«˜VLAæ¨¡åž‹åœ¨å¤šæœºå™¨äººåä½œä¸­çš„æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽå¤šæœºå™¨äººåä½œåœºæ™¯ï¼Œä¾‹å¦‚ï¼šè‡ªåŠ¨åŒ–è£…é…çº¿ã€ååŒæ¬è¿ã€åŒ»ç–—æ‰‹æœ¯ç­‰ã€‚é€šè¿‡ET-VLAæ¡†æž¶ï¼Œå¯ä»¥å¿«é€Ÿéƒ¨ç½²æ–°çš„å¤šæœºå™¨äººç³»ç»Ÿï¼Œé™ä½Žå¼€å‘æˆæœ¬ï¼Œæé«˜ç”Ÿäº§æ•ˆçŽ‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºŽæ›´å¤æ‚çš„æœºå™¨äººåä½œä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šç¾éš¾æ•‘æ´ã€å¤ªç©ºæŽ¢ç´¢ç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language-action (VLA) models have significantly advanced robotic learning, enabling training on large-scale, cross-embodiment data and fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs struggle with multi-robot collaboration. We introduce embodiment transfer learning, denoted as ET-VLA, a novel framework for efficient and effective transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic Continued Pretraining (SCP), which uses synthetically generated data to warm up the model for the new embodiment, bypassing the need for real human demonstrations and reducing data collection costs. SCP enables the model to learn correct actions and precise action token numbers. Following SCP, the model is fine-tuned on target embodiment data. To further enhance the model performance on multi-embodiment, we present the Embodied Graph-of-Thought technique, a novel approach that formulates each sub-task as a node, that allows the VLA model to distinguish the functionalities and roles of each embodiment during task execution. Our work considers bimanual robots, a simple version of multi-robot to verify our approaches. We validate the effectiveness of our method on both simulation benchmarks and real robots covering three different bimanual embodiments. In particular, our proposed ET-VLA \space can outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all codes to support the community in advancing VLA models for robot learning.

