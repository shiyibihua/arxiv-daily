---
layout: default
title: Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments
---

# Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments

**arXiv**: [2512.00915v1](https://arxiv.org/abs/2512.00915) | [PDF](https://arxiv.org/pdf/2512.00915.pdf)

**ä½œè€…**: Junwoo Chang, Minwoo Park, Joohwan Seo, Roberto Horowitz, Jongmin Lee, Jongeun Choi

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

**å¤‡æ³¨**: 27 pages, 10 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéƒ¨åˆ†ç­‰å˜å¼ºåŒ–å­¦ä¹ ï¼Œè§£å†³å¯¹ç§°ç ´ç¼ºçŽ¯å¢ƒä¸‹çš„æ³›åŒ–é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç¾¤å¯¹ç§°æ€§` `ç­‰å˜æ€§` `å¯¹ç§°ç ´ç¼º` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°å®žçŽ¯å¢ƒä¸­çš„å¯¹ç§°æ€§é€šå¸¸æ˜¯å±€éƒ¨è€Œéžå…¨å±€çš„ï¼Œè¿™å¯¼è‡´ä¼ ç»Ÿç¾¤ä¸å˜å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¯¹ç§°ç ´ç¼ºåŒºåŸŸäº§ç”Ÿè¯¯å·®å¹¶æ‰©æ•£ã€‚
2. è®ºæ–‡æå‡ºéƒ¨åˆ†ç¾¤ä¸å˜MDP (PI-MDP) æ¡†æž¶ï¼Œæ ¹æ®å¯¹ç§°æ€§ä¿æŒæƒ…å†µé€‰æ‹©æ€§åœ°åº”ç”¨ç¾¤ä¸å˜æˆ–æ ‡å‡†è´å°”æ›¼å¤‡ä»½ï¼Œå‡å°‘è¯¯å·®ä¼ æ’­ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼Œæå‡ºçš„ PE-DQN å’Œ PE-SAC ç®—æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºŽåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†é€‰æ‹©æ€§å¯¹ç§°æ€§åˆ©ç”¨çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¾¤å¯¹ç§°æ€§ä¸ºå¼ºåŒ–å­¦ä¹ (RL)æä¾›äº†ä¸€ç§å¼ºå¤§çš„å½’çº³åç½®ï¼Œé€šè¿‡ç¾¤ä¸å˜é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)å®žçŽ°è·¨å¯¹ç§°çŠ¶æ€å’ŒåŠ¨ä½œçš„æœ‰æ•ˆæ³›åŒ–ã€‚ç„¶è€Œï¼ŒçŽ°å®žçŽ¯å¢ƒå‡ ä¹Žä»Žæœªå®žçŽ°å®Œå…¨ç¾¤ä¸å˜çš„MDPï¼›åŠ¨åŠ›å­¦ã€é©±åŠ¨é™åˆ¶å’Œå¥–åŠ±è®¾è®¡é€šå¸¸ä¼šæ‰“ç ´å¯¹ç§°æ€§ï¼Œè€Œä¸”é€šå¸¸åªæ˜¯å±€éƒ¨æ‰“ç ´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æžœé‡‡ç”¨ç¾¤ä¸å˜çš„è´å°”æ›¼å¤‡ä»½ï¼Œå±€éƒ¨å¯¹ç§°æ€§ç ´ç¼ºä¼šå¼•å…¥è¯¯å·®ï¼Œå¹¶åœ¨æ•´ä¸ªçŠ¶æ€-åŠ¨ä½œç©ºé—´ä¸­ä¼ æ’­ï¼Œå¯¼è‡´å…¨å±€ä»·å€¼ä¼°è®¡è¯¯å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†éƒ¨åˆ†ç¾¤ä¸å˜MDP(PI-MDP)ï¼Œå®ƒæ ¹æ®å¯¹ç§°æ€§ä¿æŒçš„ä½ç½®é€‰æ‹©æ€§åœ°åº”ç”¨ç¾¤ä¸å˜æˆ–æ ‡å‡†è´å°”æ›¼å¤‡ä»½ã€‚è¯¥æ¡†æž¶å‡è½»äº†å±€éƒ¨å¯¹ç§°æ€§ç ´ç¼ºå¸¦æ¥çš„è¯¯å·®ä¼ æ’­ï¼ŒåŒæ—¶ä¿æŒäº†ç­‰å˜çš„ä¼˜åŠ¿ï¼Œä»Žè€Œæé«˜äº†æ ·æœ¬æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ­¤æ¡†æž¶çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†å®žç”¨çš„RLç®—æ³•â€”â€”ç”¨äºŽç¦»æ•£æŽ§åˆ¶çš„Partially Equivariant (PE)-DQNå’Œç”¨äºŽè¿žç»­æŽ§åˆ¶çš„PE-SACâ€”â€”å®ƒä»¬ç»“åˆäº†ç­‰å˜çš„ä¼˜åŠ¿å’Œå¯¹å¯¹ç§°æ€§ç ´ç¼ºçš„é²æ£’æ€§ã€‚åœ¨Grid-Worldã€è¿åŠ¨å’Œæ“ä½œåŸºå‡†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒPE-DQNå’ŒPE-SACæ˜Žæ˜¾ä¼˜äºŽåŸºçº¿ï¼Œçªå‡ºäº†é€‰æ‹©æ€§å¯¹ç§°æ€§åˆ©ç”¨å¯¹äºŽé²æ£’å’Œæ ·æœ¬é«˜æ•ˆRLçš„é‡è¦æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽç¾¤å¯¹ç§°æ€§çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºŽå®Œå…¨ç¾¤ä¸å˜çš„MDPå‡è®¾ï¼Œä½†åœ¨å®žé™…çŽ¯å¢ƒä¸­ï¼Œç”±äºŽåŠ¨åŠ›å­¦ã€åŠ¨ä½œé™åˆ¶æˆ–å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼Œå¯¹ç§°æ€§å¾€å¾€æ˜¯å±€éƒ¨ç ´ç¼ºçš„ã€‚è¿™ç§å±€éƒ¨å¯¹ç§°æ€§ç ´ç¼ºä¼šå¯¼è‡´ä»·å€¼ä¼°è®¡è¯¯å·®åœ¨æ•´ä¸ªçŠ¶æ€ç©ºé—´ä¼ æ’­ï¼Œé™ä½Žç®—æ³•çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯åªåœ¨å¯¹ç§°æ€§ä¿æŒçš„åŒºåŸŸåˆ©ç”¨ç¾¤ä¸å˜æ€§ï¼Œè€Œåœ¨å¯¹ç§°æ€§ç ´ç¼ºçš„åŒºåŸŸä½¿ç”¨æ ‡å‡†çš„è´å°”æ›¼å¤‡ä»½ã€‚é€šè¿‡è¿™ç§é€‰æ‹©æ€§çš„å¯¹ç§°æ€§åˆ©ç”¨ï¼Œå¯ä»¥é¿å…è¯¯å·®ä»Žå¯¹ç§°æ€§ç ´ç¼ºåŒºåŸŸä¼ æ’­åˆ°æ•´ä¸ªçŠ¶æ€ç©ºé—´ï¼Œä»Žè€Œæé«˜ä»·å€¼ä¼°è®¡çš„å‡†ç¡®æ€§å’Œç®—æ³•çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè®ºæ–‡æå‡ºäº†éƒ¨åˆ†ç¾¤ä¸å˜MDP (PI-MDP) æ¡†æž¶ã€‚è¯¥æ¡†æž¶åŒ…å«ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼š1) å¯¹ç§°æ€§æ£€æµ‹å™¨ï¼Œç”¨äºŽåˆ¤æ–­å½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹æ˜¯å¦æ»¡è¶³ç¾¤ä¸å˜æ€§ï¼›2) é€‰æ‹©æ€§è´å°”æ›¼å¤‡ä»½ï¼Œæ ¹æ®å¯¹ç§°æ€§æ£€æµ‹å™¨çš„ç»“æžœï¼Œé€‰æ‹©ä½¿ç”¨ç¾¤ä¸å˜è´å°”æ›¼å¤‡ä»½æˆ–æ ‡å‡†è´å°”æ›¼å¤‡ä»½æ¥æ›´æ–°ä»·å€¼å‡½æ•°ã€‚åŸºäºŽ PI-MDP æ¡†æž¶ï¼Œè®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº† PE-DQN å’Œ PE-SAC ä¸¤ç§å…·ä½“çš„ç®—æ³•ï¼Œåˆ†åˆ«ç”¨äºŽç¦»æ•£å’Œè¿žç»­æŽ§åˆ¶ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†éƒ¨åˆ†ç¾¤ä¸å˜MDP (PI-MDP) çš„æ¦‚å¿µï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„ç®—æ³•æ¡†æž¶ã€‚ä¸Žä¼ ç»Ÿçš„ç¾¤ä¸å˜å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒPI-MDP èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å®žé™…çŽ¯å¢ƒä¸­æ™®éå­˜åœ¨çš„å±€éƒ¨å¯¹ç§°æ€§ç ´ç¼ºçŽ°è±¡ï¼Œä»Žè€Œæé«˜ç®—æ³•çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šPE-DQN å’Œ PE-SAC ç®—æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç¥žç»ç½‘ç»œæ¥è¿‘ä¼¼å¯¹ç§°æ€§æ£€æµ‹å™¨ï¼Œé€šè¿‡è®­ç»ƒæ¥å­¦ä¹ çŠ¶æ€-åŠ¨ä½œå¯¹çš„å¯¹ç§°æ€§ï¼›2) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±å¯¹ç§°æ€§æ£€æµ‹å™¨è¾“å‡ºå‡†ç¡®çš„å¯¹ç§°æ€§åˆ¤æ–­ç»“æžœï¼›3) åœ¨ç¾¤ä¸å˜è´å°”æ›¼å¤‡ä»½ä¸­ä½¿ç”¨åˆé€‚çš„ç¾¤è¡¨ç¤ºï¼Œä»¥ä¿è¯ä»·å€¼å‡½æ•°çš„ç­‰å˜æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œåœ¨ Grid-Worldã€locomotion å’Œ manipulation ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPE-DQN å’Œ PE-SAC ç®—æ³•å‡æ˜¾è‘—ä¼˜äºŽåŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ locomotion ä»»åŠ¡ä¸­ï¼ŒPE-SAC ç®—æ³•çš„æ€§èƒ½æ¯” SAC ç®—æ³•æé«˜äº† 20% ä»¥ä¸Šï¼Œè¯æ˜Žäº†éƒ¨åˆ†ç­‰å˜å¼ºåŒ–å­¦ä¹ åœ¨å¯¹ç§°ç ´ç¼ºçŽ¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæœºå™¨äººæŽ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨è¿™äº›é¢†åŸŸä¸­å­˜åœ¨éƒ¨åˆ†å¯¹ç§°æ€§çš„åœºæ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œæœºå™¨äººçš„æŸäº›å…³èŠ‚å¯èƒ½å…·æœ‰æ—‹è½¬å¯¹ç§°æ€§ï¼Œè€Œå…¶ä»–å…³èŠ‚åˆ™å—åˆ°ç‰©ç†é™åˆ¶ã€‚é€šè¿‡åˆ©ç”¨éƒ¨åˆ†ç­‰å˜å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥æé«˜æœºå™¨äººåœ¨è¿™äº›ä»»åŠ¡ä¸­çš„å­¦ä¹ æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»Žè€Œé™ä½Žå¼€å‘æˆæœ¬å’Œæé«˜ç³»ç»Ÿæ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.

