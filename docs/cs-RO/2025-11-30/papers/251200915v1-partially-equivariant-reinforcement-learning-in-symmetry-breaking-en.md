---
layout: default
title: Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments
---

# Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.00915" target="_blank" class="toolbar-btn">arXiv: 2512.00915v1</a>
    <a href="https://arxiv.org/pdf/2512.00915.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00915v1" 
            onclick="toggleFavorite(this, '2512.00915v1', 'Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Junwoo Chang, Minwoo Park, Joohwan Seo, Roberto Horowitz, Jongmin Lee, Jongeun Choi

**ÂàÜÁ±ª**: cs.LG, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-30

**Â§áÊ≥®**: 27 pages, 10 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÈÉ®ÂàÜÁ≠âÂèòÂº∫ÂåñÂ≠¶‰π†ÔºåËß£ÂÜ≥ÂØπÁß∞Á†¥Áº∫ÁéØÂ¢É‰∏ãÁöÑÊ≥õÂåñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Áæ§ÂØπÁß∞ÊÄß` `Á≠âÂèòÊÄß` `ÂØπÁß∞Á†¥Áº∫` `Ê≥õÂåñËÉΩÂäõ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÂÆûÁéØÂ¢É‰∏≠ÁöÑÂØπÁß∞ÊÄßÈÄöÂ∏∏ÊòØÂ±ÄÈÉ®ËÄåÈùûÂÖ®Â±ÄÁöÑÔºåËøôÂØºËá¥‰º†ÁªüÁæ§‰∏çÂèòÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÂØπÁß∞Á†¥Áº∫Âå∫Âüü‰∫ßÁîüËØØÂ∑ÆÂπ∂Êâ©Êï£„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÈÉ®ÂàÜÁæ§‰∏çÂèòMDP (PI-MDP) Ê°ÜÊû∂ÔºåÊ†πÊçÆÂØπÁß∞ÊÄß‰øùÊåÅÊÉÖÂÜµÈÄâÊã©ÊÄßÂú∞Â∫îÁî®Áæ§‰∏çÂèòÊàñÊ†áÂáÜË¥ùÂ∞îÊõºÂ§á‰ªΩÔºåÂáèÂ∞ëËØØÂ∑Æ‰º†Êí≠„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊèêÂá∫ÁöÑ PE-DQN Âíå PE-SAC ÁÆóÊ≥ïÂú®Â§ö‰∏™‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÈÄâÊã©ÊÄßÂØπÁß∞ÊÄßÂà©Áî®ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áæ§ÂØπÁß∞ÊÄß‰∏∫Âº∫ÂåñÂ≠¶‰π†(RL)Êèê‰æõ‰∫Ü‰∏ÄÁßçÂº∫Â§ßÁöÑÂΩíÁ∫≥ÂÅèÁΩÆÔºåÈÄöËøáÁæ§‰∏çÂèòÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã(MDP)ÂÆûÁé∞Ë∑®ÂØπÁß∞Áä∂ÊÄÅÂíåÂä®‰ΩúÁöÑÊúâÊïàÊ≥õÂåñ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÂÆûÁéØÂ¢ÉÂá†‰πé‰ªéÊú™ÂÆûÁé∞ÂÆåÂÖ®Áæ§‰∏çÂèòÁöÑMDPÔºõÂä®ÂäõÂ≠¶„ÄÅÈ©±Âä®ÈôêÂà∂ÂíåÂ•ñÂä±ËÆæËÆ°ÈÄöÂ∏∏‰ºöÊâìÁ†¥ÂØπÁß∞ÊÄßÔºåËÄå‰∏îÈÄöÂ∏∏Âè™ÊòØÂ±ÄÈÉ®ÊâìÁ†¥„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÂ¶ÇÊûúÈááÁî®Áæ§‰∏çÂèòÁöÑË¥ùÂ∞îÊõºÂ§á‰ªΩÔºåÂ±ÄÈÉ®ÂØπÁß∞ÊÄßÁ†¥Áº∫‰ºöÂºïÂÖ•ËØØÂ∑ÆÔºåÂπ∂Âú®Êï¥‰∏™Áä∂ÊÄÅ-Âä®‰ΩúÁ©∫Èó¥‰∏≠‰º†Êí≠ÔºåÂØºËá¥ÂÖ®Â±Ä‰ª∑ÂÄº‰º∞ËÆ°ËØØÂ∑Æ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÈÉ®ÂàÜÁæ§‰∏çÂèòMDP(PI-MDP)ÔºåÂÆÉÊ†πÊçÆÂØπÁß∞ÊÄß‰øùÊåÅÁöÑ‰ΩçÁΩÆÈÄâÊã©ÊÄßÂú∞Â∫îÁî®Áæ§‰∏çÂèòÊàñÊ†áÂáÜË¥ùÂ∞îÊõºÂ§á‰ªΩ„ÄÇËØ•Ê°ÜÊû∂ÂáèËΩª‰∫ÜÂ±ÄÈÉ®ÂØπÁß∞ÊÄßÁ†¥Áº∫Â∏¶Êù•ÁöÑËØØÂ∑Æ‰º†Êí≠ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁ≠âÂèòÁöÑ‰ºòÂäøÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÂú®Ê≠§Ê°ÜÊû∂ÁöÑÂü∫Á°Ä‰∏äÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂÆûÁî®ÁöÑRLÁÆóÊ≥ï‚Äî‚ÄîÁî®‰∫éÁ¶ªÊï£ÊéßÂà∂ÁöÑPartially Equivariant (PE)-DQNÂíåÁî®‰∫éËøûÁª≠ÊéßÂà∂ÁöÑPE-SAC‚Äî‚ÄîÂÆÉ‰ª¨ÁªìÂêà‰∫ÜÁ≠âÂèòÁöÑ‰ºòÂäøÂíåÂØπÂØπÁß∞ÊÄßÁ†¥Áº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂú®Grid-World„ÄÅËøêÂä®ÂíåÊìç‰ΩúÂü∫ÂáÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåPE-DQNÂíåPE-SACÊòéÊòæ‰ºò‰∫éÂü∫Á∫øÔºåÁ™ÅÂá∫‰∫ÜÈÄâÊã©ÊÄßÂØπÁß∞ÊÄßÂà©Áî®ÂØπ‰∫éÈ≤ÅÊ£íÂíåÊ†∑Êú¨È´òÊïàRLÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÁæ§ÂØπÁß∞ÊÄßÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰æùËµñ‰∫éÂÆåÂÖ®Áæ§‰∏çÂèòÁöÑMDPÂÅáËÆæÔºå‰ΩÜÂú®ÂÆûÈôÖÁéØÂ¢É‰∏≠ÔºåÁî±‰∫éÂä®ÂäõÂ≠¶„ÄÅÂä®‰ΩúÈôêÂà∂ÊàñÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºåÂØπÁß∞ÊÄßÂæÄÂæÄÊòØÂ±ÄÈÉ®Á†¥Áº∫ÁöÑ„ÄÇËøôÁßçÂ±ÄÈÉ®ÂØπÁß∞ÊÄßÁ†¥Áº∫‰ºöÂØºËá¥‰ª∑ÂÄº‰º∞ËÆ°ËØØÂ∑ÆÂú®Êï¥‰∏™Áä∂ÊÄÅÁ©∫Èó¥‰º†Êí≠ÔºåÈôç‰ΩéÁÆóÊ≥ïÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂè™Âú®ÂØπÁß∞ÊÄß‰øùÊåÅÁöÑÂå∫ÂüüÂà©Áî®Áæ§‰∏çÂèòÊÄßÔºåËÄåÂú®ÂØπÁß∞ÊÄßÁ†¥Áº∫ÁöÑÂå∫Âüü‰ΩøÁî®Ê†áÂáÜÁöÑË¥ùÂ∞îÊõºÂ§á‰ªΩ„ÄÇÈÄöËøáËøôÁßçÈÄâÊã©ÊÄßÁöÑÂØπÁß∞ÊÄßÂà©Áî®ÔºåÂèØ‰ª•ÈÅøÂÖçËØØÂ∑Æ‰ªéÂØπÁß∞ÊÄßÁ†¥Áº∫Âå∫Âüü‰º†Êí≠Âà∞Êï¥‰∏™Áä∂ÊÄÅÁ©∫Èó¥Ôºå‰ªéËÄåÊèêÈ´ò‰ª∑ÂÄº‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÁÆóÊ≥ïÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫ÜÈÉ®ÂàÜÁæ§‰∏çÂèòMDP (PI-MDP) Ê°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏§‰∏™ÂÖ≥ÈîÆÈÉ®ÂàÜÔºö1) ÂØπÁß∞ÊÄßÊ£ÄÊµãÂô®ÔºåÁî®‰∫éÂà§Êñ≠ÂΩìÂâçÁä∂ÊÄÅ-Âä®‰ΩúÂØπÊòØÂê¶Êª°Ë∂≥Áæ§‰∏çÂèòÊÄßÔºõ2) ÈÄâÊã©ÊÄßË¥ùÂ∞îÊõºÂ§á‰ªΩÔºåÊ†πÊçÆÂØπÁß∞ÊÄßÊ£ÄÊµãÂô®ÁöÑÁªìÊûúÔºåÈÄâÊã©‰ΩøÁî®Áæ§‰∏çÂèòË¥ùÂ∞îÊõºÂ§á‰ªΩÊàñÊ†áÂáÜË¥ùÂ∞îÊõºÂ§á‰ªΩÊù•Êõ¥Êñ∞‰ª∑ÂÄºÂáΩÊï∞„ÄÇÂü∫‰∫é PI-MDP Ê°ÜÊû∂ÔºåËÆ∫ÊñáËøõ‰∏ÄÊ≠•ÊèêÂá∫‰∫Ü PE-DQN Âíå PE-SAC ‰∏§ÁßçÂÖ∑‰ΩìÁöÑÁÆóÊ≥ïÔºåÂàÜÂà´Áî®‰∫éÁ¶ªÊï£ÂíåËøûÁª≠ÊéßÂà∂‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÈÉ®ÂàÜÁæ§‰∏çÂèòMDP (PI-MDP) ÁöÑÊ¶ÇÂøµÔºåÂπ∂ËÆæËÆ°‰∫ÜÁõ∏Â∫îÁöÑÁÆóÊ≥ïÊ°ÜÊû∂„ÄÇ‰∏é‰º†ÁªüÁöÑÁæ§‰∏çÂèòÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåPI-MDP ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂÆûÈôÖÁéØÂ¢É‰∏≠ÊôÆÈÅçÂ≠òÂú®ÁöÑÂ±ÄÈÉ®ÂØπÁß∞ÊÄßÁ†¥Áº∫Áé∞Ë±°Ôºå‰ªéËÄåÊèêÈ´òÁÆóÊ≥ïÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöPE-DQN Âíå PE-SAC ÁÆóÊ≥ïÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Á•ûÁªèÁΩëÁªúÊù•Ëøë‰ººÂØπÁß∞ÊÄßÊ£ÄÊµãÂô®ÔºåÈÄöËøáËÆ≠ÁªÉÊù•Â≠¶‰π†Áä∂ÊÄÅ-Âä®‰ΩúÂØπÁöÑÂØπÁß∞ÊÄßÔºõ2) ËÆæËÆ°ÂêàÈÄÇÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÈºìÂä±ÂØπÁß∞ÊÄßÊ£ÄÊµãÂô®ËæìÂá∫ÂáÜÁ°ÆÁöÑÂØπÁß∞ÊÄßÂà§Êñ≠ÁªìÊûúÔºõ3) Âú®Áæ§‰∏çÂèòË¥ùÂ∞îÊõºÂ§á‰ªΩ‰∏≠‰ΩøÁî®ÂêàÈÄÇÁöÑÁæ§Ë°®Á§∫Ôºå‰ª•‰øùËØÅ‰ª∑ÂÄºÂáΩÊï∞ÁöÑÁ≠âÂèòÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú® Grid-World„ÄÅlocomotion Âíå manipulation Á≠âÂ§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåPE-DQN Âíå PE-SAC ÁÆóÊ≥ïÂùáÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú® locomotion ‰ªªÂä°‰∏≠ÔºåPE-SAC ÁÆóÊ≥ïÁöÑÊÄßËÉΩÊØî SAC ÁÆóÊ≥ïÊèêÈ´ò‰∫Ü 20% ‰ª•‰∏äÔºåËØÅÊòé‰∫ÜÈÉ®ÂàÜÁ≠âÂèòÂº∫ÂåñÂ≠¶‰π†Âú®ÂØπÁß∞Á†¥Áº∫ÁéØÂ¢É‰∏ãÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÅÊ∏∏ÊàèAI„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÔºåÂ∞§ÂÖ∂ÊòØÂú®Ëøô‰∫õÈ¢ÜÂüü‰∏≠Â≠òÂú®ÈÉ®ÂàÜÂØπÁß∞ÊÄßÁöÑÂú∫ÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÔºåÊú∫Âô®‰∫∫ÁöÑÊüê‰∫õÂÖ≥ËäÇÂèØËÉΩÂÖ∑ÊúâÊóãËΩ¨ÂØπÁß∞ÊÄßÔºåËÄåÂÖ∂‰ªñÂÖ≥ËäÇÂàôÂèóÂà∞Áâ©ÁêÜÈôêÂà∂„ÄÇÈÄöËøáÂà©Áî®ÈÉ®ÂàÜÁ≠âÂèòÂº∫ÂåñÂ≠¶‰π†ÔºåÂèØ‰ª•ÊèêÈ´òÊú∫Âô®‰∫∫Âú®Ëøô‰∫õ‰ªªÂä°‰∏≠ÁöÑÂ≠¶‰π†ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõÔºå‰ªéËÄåÈôç‰ΩéÂºÄÂèëÊàêÊú¨ÂíåÊèêÈ´òÁ≥ªÁªüÊÄßËÉΩ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.

