---
layout: default
title: Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration
---

# Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration

**arXiv**: [2512.00797v1](https://arxiv.org/abs/2512.00797) | [PDF](https://arxiv.org/pdf/2512.00797.pdf)

**ä½œè€…**: Nan Sun, Bo Mao, Yongchang Li, Chenxu Wang, Di Guo, Huaping Liu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

**å¤‡æ³¨**: 21 pages, 16 figures, 4 tables

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**InteractGenï¼šå°†å•ä½“æ¨¡åž‹è½¬åŒ–ä¸ºå…·èº«å¤šæ™ºèƒ½ä½“æž¶æž„ï¼Œä¿ƒè¿›äººæœºåä½œ**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `äººæœºåä½œ` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `åŸºåº§æ¨¡åž‹` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `æœåŠ¡æœºå™¨äºº`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºåº§æ¨¡åž‹åœ¨æœºå™¨äººåº”ç”¨ä¸­å­˜åœ¨å•ä½“å‡è®¾ä¸Žå®žé™…ä»»åŠ¡çš„åˆ†å¸ƒå¼åŠ¨æ€æ€§ä¸åŒ¹é…çš„é—®é¢˜ã€‚
2. InteractGenæå‡ºäº†ä¸€ç§åŸºäºŽLLMçš„å¤šæ™ºèƒ½ä½“æ¡†æž¶ï¼Œå°†æœºå™¨äººæ™ºèƒ½åˆ†è§£ä¸ºå¤šä¸ªä¸“ä¸šæ™ºèƒ½ä½“ååŒå·¥ä½œã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒInteractGenæé«˜äº†ä»»åŠ¡æˆåŠŸçŽ‡ã€é€‚åº”æ€§å’Œäººæœºåä½œèƒ½åŠ›ï¼ŒéªŒè¯äº†å¤šæ™ºèƒ½ä½“æž¶æž„çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰ï¼ŒåŸºåº§æ¨¡åž‹å·²æˆä¸ºç»Ÿä¸€æœºå™¨äººæ„ŸçŸ¥å’Œè§„åˆ’çš„æ ¸å¿ƒï¼Œä½†å®žé™…éƒ¨ç½²ä¸­ï¼Œå…¶å•ä½“å‡è®¾ï¼ˆå³å•ä¸ªæ¨¡åž‹å¯ä»¥å¤„ç†æ‰€æœ‰è®¤çŸ¥åŠŸèƒ½ï¼‰ä¸Žå®žé™…æœåŠ¡å·¥ä½œæµç¨‹çš„åˆ†å¸ƒå¼ã€åŠ¨æ€ç‰¹æ€§ä¸åŒ¹é…ã€‚è§†è§‰-è¯­è¨€æ¨¡åž‹å…·æœ‰å¼ºå¤§çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä½†ç¼ºä¹å…·èº«æ„ŸçŸ¥çš„åŠ¨ä½œèƒ½åŠ›ï¼Œå¹¶ä¸”ä¾èµ–äºŽæ‰‹å·¥è®¾è®¡çš„æŠ€èƒ½ã€‚è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥èƒ½å¤Ÿå®žçŽ°ååº”å¼æ“ä½œï¼Œä½†åœ¨ä¸åŒå…·èº«ä¹‹é—´è¡¨çŽ°è„†å¼±ï¼Œå‡ ä½•åŸºç¡€è–„å¼±ï¼Œå¹¶ä¸”ç¼ºä¹ä¸»åŠ¨åä½œæœºåˆ¶ã€‚è¿™äº›å±€é™æ€§è¡¨æ˜Žï¼Œä»…æ‰©å±•å•ä¸ªæ¨¡åž‹æ— æ³•ä¸ºåœ¨äººç¾¤çŽ¯å¢ƒä¸­è¿è¡Œçš„æœåŠ¡æœºå™¨äººæä¾›å¯é çš„è‡ªä¸»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†InteractGenï¼Œè¿™æ˜¯ä¸€ä¸ªç”±LLMé©±åŠ¨çš„å¤šæ™ºèƒ½ä½“æ¡†æž¶ï¼Œå®ƒå°†æœºå™¨äººæ™ºèƒ½åˆ†è§£ä¸ºä¸“é—¨çš„æ™ºèƒ½ä½“ï¼Œç”¨äºŽæŒç»­æ„ŸçŸ¥ã€ä¾èµ–æ„ŸçŸ¥è§„åˆ’ã€å†³ç­–å’ŒéªŒè¯ã€å¤±è´¥åæ€ä»¥åŠåŠ¨æ€çš„äººå·¥å§”æ‰˜ï¼Œå°†åŸºåº§æ¨¡åž‹è§†ä¸ºé—­çŽ¯é›†ä½“ä¸­çš„å—æŽ§ç»„ä»¶ã€‚InteractGenéƒ¨ç½²åœ¨å¼‚æž„æœºå™¨äººå›¢é˜Ÿä¸Šï¼Œå¹¶åœ¨ä¸ºæœŸä¸‰ä¸ªæœˆçš„å¼€æ”¾ä½¿ç”¨ç ”ç©¶ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œæé«˜äº†ä»»åŠ¡æˆåŠŸçŽ‡ã€é€‚åº”æ€§å’Œäººæœºåä½œèƒ½åŠ›ï¼Œè¯æ˜Žäº†å¤šæ™ºèƒ½ä½“ç¼–æŽ’æ¯”è¿›ä¸€æ­¥æ‰©å±•ç‹¬ç«‹æ¨¡åž‹æ›´å¯è¡Œï¼Œèƒ½å¤Ÿå®žçŽ°å…·æœ‰ç¤¾ä¼šåŸºç¡€çš„æœåŠ¡è‡ªä¸»æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœåŠ¡æœºå™¨äººé¢†åŸŸä¸­ï¼ŒçŽ°æœ‰åŸºåº§æ¨¡åž‹ï¼ˆå¦‚è§†è§‰-è¯­è¨€æ¨¡åž‹å’Œè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥ï¼‰åœ¨å®žé™…äººæœºåä½œåœºæ™¯ä¸‹çš„å±€é™æ€§ã€‚è¿™äº›æ¨¡åž‹è¦ä¹ˆç¼ºä¹å…·èº«æ„ŸçŸ¥èƒ½åŠ›å’Œä¸»åŠ¨åä½œæœºåˆ¶ï¼Œè¦ä¹ˆåœ¨ä¸åŒæœºå™¨äººä¹‹é—´æ³›åŒ–èƒ½åŠ›å·®ï¼Œæ— æ³•å¯é åœ°å®Œæˆå¤æ‚ä»»åŠ¡ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽå®ƒä»¬ä¾èµ–äºŽå•ä½“æ¨¡åž‹ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„çŽ¯å¢ƒå’Œä»»åŠ¡éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å•ä½“åŸºåº§æ¨¡åž‹è½¬åŒ–ä¸ºå¤šæ™ºèƒ½ä½“æž¶æž„ã€‚é€šè¿‡å°†æœºå™¨äººæ™ºèƒ½åˆ†è§£ä¸ºå¤šä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“è´Ÿè´£ä¸åŒçš„è®¤çŸ¥åŠŸèƒ½ï¼ˆå¦‚æ„ŸçŸ¥ã€è§„åˆ’ã€å†³ç­–ç­‰ï¼‰ï¼Œå¹¶åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰è¿›è¡Œåè°ƒå’ŒæŽ§åˆ¶ï¼Œä»Žè€Œå®žçŽ°æ›´çµæ´»ã€é²æ£’å’Œå¯æ‰©å±•çš„æœºå™¨äººç³»ç»Ÿã€‚è¿™ç§è®¾è®¡å€Ÿé‰´äº†åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ€æƒ³ï¼Œå…è®¸å„ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹è¿è¡Œå’ŒååŒå·¥ä½œï¼Œæé«˜äº†ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šInteractGenæ¡†æž¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) æŒç»­æ„ŸçŸ¥æ™ºèƒ½ä½“ï¼šè´Ÿè´£æŒç»­æ„ŸçŸ¥çŽ¯å¢ƒä¿¡æ¯ã€‚2) ä¾èµ–æ„ŸçŸ¥è§„åˆ’æ™ºèƒ½ä½“ï¼šæ ¹æ®ä»»åŠ¡ç›®æ ‡å’ŒçŽ¯å¢ƒä¿¡æ¯è¿›è¡Œè§„åˆ’ã€‚3) å†³ç­–å’ŒéªŒè¯æ™ºèƒ½ä½“ï¼šåšå‡ºå†³ç­–å¹¶éªŒè¯å…¶å¯è¡Œæ€§ã€‚4) å¤±è´¥åæ€æ™ºèƒ½ä½“ï¼šåœ¨ä»»åŠ¡å¤±è´¥æ—¶è¿›è¡Œåæ€å’Œè°ƒæ•´ã€‚5) åŠ¨æ€äººå·¥å§”æ‰˜æ™ºèƒ½ä½“ï¼šå…è®¸äººç±»ä»‹å…¥å¹¶å§”æ‰˜ä»»åŠ¡ã€‚è¿™äº›æ™ºèƒ½ä½“é€šè¿‡LLMè¿›è¡Œåè°ƒå’Œé€šä¿¡ï¼Œå½¢æˆä¸€ä¸ªé—­çŽ¯æŽ§åˆ¶ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå°†LLMä½œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ä¸­å¤®åè°ƒå™¨ã€‚LLMä¸ä»…å¯ä»¥ç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè¿˜å¯ä»¥æŽ¨ç†ä»»åŠ¡ä¾èµ–å…³ç³»ï¼Œå¹¶æ ¹æ®çŽ¯å¢ƒå˜åŒ–åŠ¨æ€è°ƒæ•´æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ‰‹å·¥è®¾è®¡å¤æ‚çš„æŽ§åˆ¶ç­–ç•¥ï¼Œæé«˜äº†ç³»ç»Ÿçš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼ŒInteractGenä¸å†ä¾èµ–äºŽå•ä¸ªæ¨¡åž‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œè€Œæ˜¯é€šè¿‡å¤šä¸ªæ™ºèƒ½ä½“çš„ååŒå·¥ä½œæ¥å®žçŽ°å¤æ‚ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†æè¿°å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æž„ç­‰æŠ€æœ¯ç»†èŠ‚ã€‚ä½†å¯ä»¥æŽ¨æ–­ï¼ŒLLMçš„é€‰æ‹©å’Œå¾®è°ƒã€æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡åè®®ã€ä»¥åŠä»»åŠ¡åˆ†è§£ç­–ç•¥æ˜¯å…³é”®çš„è®¾è®¡å› ç´ ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨äººç±»çš„åé¦ˆå’ŒæŒ‡å¯¼ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

InteractGenåœ¨ä¸ºæœŸä¸‰ä¸ªæœˆçš„å¼€æ”¾ä½¿ç”¨ç ”ç©¶ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æžœè¡¨æ˜Žï¼Œè¯¥æ¡†æž¶æé«˜äº†ä»»åŠ¡æˆåŠŸçŽ‡ã€é€‚åº”æ€§å’Œäººæœºåä½œèƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œä½†ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œå¤šæ™ºèƒ½ä½“ç¼–æŽ’æ¯”è¿›ä¸€æ­¥æ‰©å±•ç‹¬ç«‹æ¨¡åž‹æ›´å¯è¡Œï¼Œèƒ½å¤Ÿå®žçŽ°å…·æœ‰ç¤¾ä¼šåŸºç¡€çš„æœåŠ¡è‡ªä¸»æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§äººæœºåä½œåœºæ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡ã€åŒ»ç–—æŠ¤ç†ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚é€šè¿‡å°†æœºå™¨äººæ™ºèƒ½åˆ†è§£ä¸ºå¤šä¸ªæ™ºèƒ½ä½“ï¼Œå¹¶åˆ©ç”¨LLMè¿›è¡Œåè°ƒï¼Œå¯ä»¥å®žçŽ°æ›´å®‰å…¨ã€é«˜æ•ˆå’Œå¯é çš„äººæœºåä½œã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æŽ¨åŠ¨æœåŠ¡æœºå™¨äººåœ¨å®žé™…ç”Ÿæ´»ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæé«˜äººä»¬çš„ç”Ÿæ´»è´¨é‡å’Œå·¥ä½œæ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Foundation models have become central to unifying perception and planning in robotics, yet real-world deployment exposes a mismatch between their monolithic assumption that a single model can handle all cognitive functions and the distributed, dynamic nature of practical service workflows. Vision-language models offer strong semantic understanding but lack embodiment-aware action capabilities while relying on hand-crafted skills. Vision-Language-Action policies enable reactive manipulation but remain brittle across embodiments, weak in geometric grounding, and devoid of proactive collaboration mechanisms. These limitations indicate that scaling a single model alone cannot deliver reliable autonomy for service robots operating in human-populated settings. To address this gap, we present InteractGen, an LLM-powered multi-agent framework that decomposes robot intelligence into specialized agents for continuous perception, dependency-aware planning, decision and verification, failure reflection, and dynamic human delegation, treating foundation models as regulated components within a closed-loop collective. Deployed on a heterogeneous robot team and evaluated in a three-month open-use study, InteractGen improves task success, adaptability, and human-robot collaboration, providing evidence that multi-agent orchestration offers a more feasible path toward socially grounded service autonomy than further scaling standalone models.

