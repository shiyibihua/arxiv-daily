---
layout: default
title: TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos
---

# TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos

**arXiv**: [2511.21690v1](https://arxiv.org/abs/2511.21690) | [PDF](https://arxiv.org/pdf/2511.21690.pdf)

**ä½œè€…**: Seungjae Lee, Yoonkyo Jung, Inkook Chun, Yao-Chih Lee, Zikui Cai, Hongjia Huang, Aayush Talreja, Tan Dat Dao, Yongyuan Liang, Jia-Bin Huang, Furong Huang

**åˆ†ç±»**: cs.RO, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-26

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TraceGenï¼šé€šè¿‡3Dè½¨è¿¹ç©ºé—´çš„ä¸–ç•Œå»ºæ¨¡å®žçŽ°è·¨å…·èº«è§†é¢‘å­¦ä¹ **

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æœºå™¨äººå­¦ä¹ ` `è·¨å…·èº«å­¦ä¹ ` `ä¸–ç•Œæ¨¡åž‹` `è½¨è¿¹é¢„æµ‹` `è¿ç§»å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•éš¾ä»¥åˆ©ç”¨æ¥è‡ªä¸åŒå…·èº«ï¼ˆäººç±»ã€å…¶ä»–æœºå™¨äººï¼‰çš„è§†é¢‘è¿›è¡Œæœºå™¨äººå­¦ä¹ ï¼Œå› ä¸ºå…·èº«ã€ç›¸æœºå’ŒçŽ¯å¢ƒçš„å·®å¼‚é˜»ç¢äº†ç›´æŽ¥ä½¿ç”¨ã€‚
2. TraceGené€šè¿‡å¼•å…¥3Dè½¨è¿¹ç©ºé—´ä½œä¸ºç»Ÿä¸€è¡¨ç¤ºï¼Œå°†å¼‚æž„è§†é¢‘è½¬æ¢ä¸ºä¸€è‡´çš„è½¨è¿¹ï¼Œä»Žè€ŒæŠ½è±¡æŽ‰å¤–è§‚å·®å¼‚ï¼Œä¿ç•™å‡ ä½•ç»“æž„ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒTraceGenä»…éœ€å°‘é‡ç›®æ ‡æœºå™¨äººè§†é¢‘å³å¯å®žçŽ°é«˜æ•ˆè¿ç§»å­¦ä¹ ï¼Œå¹¶åœ¨çœŸå®žæœºå™¨äººä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆåŠŸï¼ŒæŽ¨ç†é€Ÿåº¦è¿œè¶…çŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•TraceGenï¼Œé€šè¿‡ç»Ÿä¸€çš„ç¬¦å·è¡¨ç¤ºâ€”â€”åœºæ™¯çº§è½¨è¿¹çš„ç´§å‡‘3Dâ€œè½¨è¿¹ç©ºé—´â€ï¼Œå®žçŽ°ä»Žè·¨å…·èº«ã€è·¨çŽ¯å¢ƒå’Œè·¨ä»»åŠ¡è§†é¢‘ä¸­å­¦ä¹ æ–°çš„æœºå™¨äººä»»åŠ¡ã€‚TraceGenæ˜¯ä¸€ä¸ªä¸–ç•Œæ¨¡åž‹ï¼Œå®ƒåœ¨è½¨è¿¹ç©ºé—´è€Œéžåƒç´ ç©ºé—´ä¸­é¢„æµ‹æœªæ¥è¿åŠ¨ï¼ŒæŠ½è±¡æŽ‰å¤–è§‚ï¼ŒåŒæ—¶ä¿ç•™æ“ä½œæ‰€éœ€çš„å‡ ä½•ç»“æž„ã€‚ä¸ºäº†å¤§è§„æ¨¡è®­ç»ƒTraceGenï¼Œå¼€å‘äº†TraceForgeæ•°æ®ç®¡é“ï¼Œå°†å¼‚æž„çš„äººç±»å’Œæœºå™¨äººè§†é¢‘è½¬æ¢ä¸ºä¸€è‡´çš„3Dè½¨è¿¹ï¼Œç”ŸæˆåŒ…å«12.3ä¸‡ä¸ªè§†é¢‘å’Œ180ä¸‡ä¸ªè§‚å¯Ÿ-è½¨è¿¹-è¯­è¨€ä¸‰å…ƒç»„çš„è¯­æ–™åº“ã€‚é¢„è®­ç»ƒçš„TraceGenäº§ç”Ÿäº†ä¸€ä¸ªå¯è¿ç§»çš„3Dè¿åŠ¨å…ˆéªŒï¼Œèƒ½å¤Ÿé«˜æ•ˆé€‚åº”ï¼šä»…ç”¨äº”ä¸ªç›®æ ‡æœºå™¨äººè§†é¢‘ï¼ŒTraceGenåœ¨å››ä¸ªä»»åŠ¡ä¸­è¾¾åˆ°80%çš„æˆåŠŸçŽ‡ï¼ŒåŒæ—¶æä¾›æ¯”æœ€å…ˆè¿›çš„åŸºäºŽè§†é¢‘çš„ä¸–ç•Œæ¨¡åž‹å¿«50-600å€çš„æŽ¨ç†é€Ÿåº¦ã€‚åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æƒ…å†µä¸‹ï¼Œä»…æœ‰äº”ä¸ªåœ¨æ‰‹æŒç”µè¯ä¸Šæ•èŽ·çš„æœªæ ¡å‡†çš„äººç±»æ¼”ç¤ºè§†é¢‘å¯ç”¨æ—¶ï¼Œå®ƒä»ç„¶å¯ä»¥åœ¨çœŸå®žæœºå™¨äººä¸Šè¾¾åˆ°67.5%çš„æˆåŠŸçŽ‡ï¼Œçªæ˜¾äº†TraceGenåœ¨ä¸ä¾èµ–å¯¹è±¡æ£€æµ‹å™¨æˆ–ç¹é‡çš„åƒç´ ç©ºé—´ç”Ÿæˆçš„æƒ…å†µä¸‹è·¨å…·èº«é€‚åº”çš„èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æœºå™¨äººå­¦ä¹ æ–¹æ³•éš¾ä»¥ç›´æŽ¥åˆ©ç”¨å¤§é‡å­˜åœ¨çš„è·¨å…·èº«ï¼ˆä¾‹å¦‚ï¼Œäººç±»å’Œä¸åŒæœºå™¨äººï¼‰è§†é¢‘æ•°æ®ã€‚è¿™æ˜¯å› ä¸ºä¸åŒå…·èº«ã€ç›¸æœºè§†è§’ä»¥åŠçŽ¯å¢ƒçš„å·®å¼‚å¯¼è‡´äº†æ•°æ®åˆ†å¸ƒçš„å·¨å¤§å·®å¼‚ï¼Œä½¿å¾—ç›´æŽ¥çš„åƒç´ ç©ºé—´å­¦ä¹ å˜å¾—å›°éš¾ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽå¤§é‡çš„ç›®æ ‡æœºå™¨äººæ•°æ®æˆ–å¤æ‚çš„åƒç´ çº§ç”Ÿæˆæ¨¡åž‹ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTraceGençš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†é¢‘æ•°æ®æŠ•å½±åˆ°ä¸€ä¸ªç»Ÿä¸€çš„3Dè½¨è¿¹ç©ºé—´ä¸­ï¼Œä»Žè€ŒæŠ½è±¡æŽ‰å¤–è§‚ä¿¡æ¯ï¼Œä¿ç•™å…³é”®çš„å‡ ä½•ç»“æž„ã€‚é€šè¿‡åœ¨è¿™ä¸ªè½¨è¿¹ç©ºé—´ä¸­è¿›è¡Œè¿åŠ¨é¢„æµ‹ï¼Œæ¨¡åž‹å¯ä»¥å­¦ä¹ åˆ°ä¸Žå…·èº«æ— å…³çš„è¿åŠ¨å…ˆéªŒçŸ¥è¯†ï¼Œä»Žè€Œå®žçŽ°è·¨å…·èº«çš„è¿ç§»å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æŽ¥åœ¨åƒç´ ç©ºé—´è¿›è¡Œå­¦ä¹ ï¼Œé™ä½Žäº†è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šTraceGençš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šTraceForgeæ•°æ®ç®¡é“å’ŒTraceGenä¸–ç•Œæ¨¡åž‹ã€‚TraceForgeè´Ÿè´£å°†å¼‚æž„çš„è§†é¢‘æ•°æ®è½¬æ¢ä¸º3Dè½¨è¿¹ï¼Œç”Ÿæˆå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®é›†ã€‚TraceGenåˆ™æ˜¯ä¸€ä¸ªåŸºäºŽTransformerçš„åºåˆ—é¢„æµ‹æ¨¡åž‹ï¼Œå®ƒä»¥è½¨è¿¹åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹æœªæ¥çš„è½¨è¿¹ã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€è½¨è¿¹æå–ã€æ¨¡åž‹è®­ç»ƒå’Œè¿åŠ¨è§„åˆ’ç­‰æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šTraceGençš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†3Dè½¨è¿¹ç©ºé—´ä½œä¸ºç»Ÿä¸€çš„è¡¨ç¤ºå½¢å¼ï¼Œä»¥åŠTraceForgeæ•°æ®ç®¡é“ç”¨äºŽç”Ÿæˆå¤§è§„æ¨¡çš„è·¨å…·èº«è®­ç»ƒæ•°æ®ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTraceGenä¸éœ€è¦ä¾èµ–äºŽå¯¹è±¡æ£€æµ‹å™¨æˆ–å¤æ‚çš„åƒç´ ç©ºé—´ç”Ÿæˆæ¨¡åž‹ï¼Œè€Œæ˜¯ç›´æŽ¥åœ¨è½¨è¿¹ç©ºé—´ä¸­è¿›è¡Œå­¦ä¹ ï¼Œä»Žè€Œé™ä½Žäº†è®¡ç®—æˆæœ¬ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šTraceGenä½¿ç”¨Transformeræž¶æž„è¿›è¡Œè½¨è¿¹é¢„æµ‹ï¼ŒæŸå¤±å‡½æ•°åŒ…æ‹¬è½¨è¿¹é¢„æµ‹æŸå¤±å’Œè¯­è¨€æè¿°æŸå¤±ã€‚TraceForgeæ•°æ®ç®¡é“åˆ©ç”¨SfMï¼ˆStructure from Motionï¼‰æŠ€æœ¯ä»Žè§†é¢‘ä¸­é‡å»º3Dåœºæ™¯ï¼Œå¹¶æå–è½¨è¿¹ã€‚ä¸ºäº†å¤„ç†ä¸åŒè§†é¢‘çš„å°ºåº¦å’Œè§†è§’å·®å¼‚ï¼ŒTraceForgeè¿˜é‡‡ç”¨äº†æ•°æ®å¢žå¼ºæŠ€æœ¯ï¼Œä¾‹å¦‚éšæœºç¼©æ”¾å’Œæ—‹è½¬ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒTraceGenåœ¨å››ä¸ªæœºå™¨äººä»»åŠ¡ä¸­ä»…ä½¿ç”¨äº”ä¸ªç›®æ ‡æœºå™¨äººè§†é¢‘å³å¯è¾¾åˆ°80%çš„æˆåŠŸçŽ‡ï¼Œå¹¶ä¸”æŽ¨ç†é€Ÿåº¦æ¯”æœ€å…ˆè¿›çš„åŸºäºŽè§†é¢‘çš„ä¸–ç•Œæ¨¡åž‹å¿«50-600å€ã€‚åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æƒ…å†µä¸‹ï¼Œä»…ä½¿ç”¨äº”ä¸ªæœªæ ¡å‡†çš„äººç±»æ¼”ç¤ºè§†é¢‘ï¼ŒTraceGenä»ç„¶å¯ä»¥åœ¨çœŸå®žæœºå™¨äººä¸Šè¾¾åˆ°67.5%çš„æˆåŠŸçŽ‡ï¼Œè¯æ˜Žäº†å…¶å¼ºå¤§çš„è·¨å…·èº«è¿ç§»èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

TraceGenå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å¯ä»¥ç”¨äºŽæœºå™¨äººæ¨¡ä»¿å­¦ä¹ ã€äººæœºåä½œã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡åˆ©ç”¨å¤§é‡çš„è·¨å…·èº«è§†é¢‘æ•°æ®ï¼ŒTraceGenå¯ä»¥å¸®åŠ©æœºå™¨äººå¿«é€Ÿå­¦ä¹ æ–°çš„æŠ€èƒ½ï¼Œå¹¶é€‚åº”ä¸åŒçš„çŽ¯å¢ƒã€‚æ­¤å¤–ï¼ŒTraceGenè¿˜å¯ä»¥ç”¨äºŽç”Ÿæˆé€¼çœŸçš„æœºå™¨äººåŠ¨ç”»ï¼Œä»¥åŠè¿›è¡Œè™šæ‹ŸçŽ°å®žå’Œå¢žå¼ºçŽ°å®žåº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.

