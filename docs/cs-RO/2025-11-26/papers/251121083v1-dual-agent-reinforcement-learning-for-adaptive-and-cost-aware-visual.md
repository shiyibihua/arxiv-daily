---
layout: default
title: Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry
---

# Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry

**arXiv**: [2511.21083v1](https://arxiv.org/abs/2511.21083) | [PDF](https://arxiv.org/pdf/2511.21083.pdf)

**ä½œè€…**: Feiyang Pan, Shenghe Zheng, Chunyan Yin, Guangbin Dou

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-26

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽåŒæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”ã€ä½Žæˆæœ¬è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡` `å¼ºåŒ–å­¦ä¹ ` `è‡ªä¸»å¯¼èˆª` `èµ„æºå—é™å¹³å°` `è‡ªé€‚åº”ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VIOæ–¹æ³•åœ¨ç²¾åº¦å’Œè®¡ç®—æ•ˆçŽ‡ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œä¼˜åŒ–æ–¹æ³•ç²¾åº¦é«˜ä½†è®¡ç®—é‡å¤§ï¼Œæ»¤æ³¢æ–¹æ³•æ•ˆçŽ‡é«˜ä½†æ˜“æ¼‚ç§»ã€‚
2. æå‡ºä¸€ç§åŸºäºŽåŒæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„VIOæ¡†æž¶ï¼Œè‡ªé€‚åº”åœ°æŽ§åˆ¶è§†è§‰å‰ç«¯çš„è¿è¡Œå’Œä¿¡æ¯èžåˆï¼Œä»¥é™ä½Žè®¡ç®—æˆæœ¬ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨ç²¾åº¦ã€æ•ˆçŽ‡å’Œå†…å­˜å ç”¨æ–¹é¢å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œå¹¶åœ¨EuRoC MAVå’ŒTUM-VIæ•°æ®é›†ä¸ŠéªŒè¯äº†æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡(VIO)æ˜¯å®žçŽ°é²æ£’çš„è‡ªè¿åŠ¨ä¼°è®¡çš„å…³é”®ç»„ä»¶ï¼Œä¸ºæœºå™¨äººè‡ªä¸»å¯¼èˆªå’Œå¢žå¼ºçŽ°å®žçš„å®žæ—¶6è‡ªç”±åº¦è·Ÿè¸ªç­‰åŸºç¡€èƒ½åŠ›æä¾›æ”¯æŒã€‚çŽ°æœ‰æ–¹æ³•é¢ä¸´ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„æƒè¡¡ï¼šåŸºäºŽæ»¤æ³¢çš„æ–¹æ³•æ•ˆçŽ‡é«˜ä½†å®¹æ˜“æ¼‚ç§»ï¼Œè€ŒåŸºäºŽä¼˜åŒ–çš„æ–¹æ³•è™½ç„¶å‡†ç¡®ï¼Œä½†ä¾èµ–äºŽè®¡ç®—é‡å·¨å¤§çš„è§†è§‰æƒ¯æ€§æ†ç»‘è°ƒæ•´(VIBA)ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„å¹³å°ä¸Šè¿è¡Œã€‚æœ¬æ–‡æ—¨åœ¨å‡å°‘VIBAçš„è°ƒç”¨é¢‘çŽ‡å’Œå¼ºåº¦ï¼Œè€Œä¸æ˜¯å®Œå…¨ç§»é™¤å®ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†çŽ°ä»£VIOä¸­çš„ä¸¤ä¸ªå…³é”®è®¾è®¡é€‰æ‹©ï¼ˆä½•æ—¶è¿è¡Œè§†è§‰å‰ç«¯ä»¥åŠå¯¹å®ƒçš„è¾“å‡ºä¿¡ä»»åº¦ï¼‰è½¬åŒ–ä¸ºåºåˆ—å†³ç­–é—®é¢˜ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§å¼ºåŒ–å­¦ä¹ (RL)æ™ºèƒ½ä½“è§£å†³å®ƒä»¬ã€‚æœ¬æ–‡æ¡†æž¶å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„åŒç®¡é½ä¸‹çš„RLç­–ç•¥ï¼Œä½œä¸ºæ ¸å¿ƒè´¡çŒ®ï¼š(1)ä¸€ä¸ªé€‰æ‹©æ™ºèƒ½ä½“ï¼Œä»…åŸºäºŽé«˜é¢‘IMUæ•°æ®æ™ºèƒ½åœ°æŽ§åˆ¶æ•´ä¸ªVOæµç¨‹ï¼›(2)ä¸€ä¸ªå¤åˆèžåˆæ™ºèƒ½ä½“ï¼Œé¦–å…ˆé€šè¿‡ç›‘ç£ç½‘ç»œä¼°è®¡ä¸€ä¸ªé²æ£’çš„é€Ÿåº¦çŠ¶æ€ï¼Œç„¶åŽé€šè¿‡RLç­–ç•¥è‡ªé€‚åº”åœ°èžåˆå®Œæ•´çš„(p, v, q)çŠ¶æ€ã€‚åœ¨EuRoC MAVå’ŒTUM-VIæ•°æ®é›†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œåœ¨ç»Ÿä¸€è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•æ¯”ä¹‹å‰çš„åŸºäºŽGPUçš„VO/VIOç³»ç»Ÿå®žçŽ°äº†æ›´æœ‰åˆ©çš„ç²¾åº¦-æ•ˆçŽ‡-å†…å­˜æƒè¡¡ï¼šåœ¨è¿è¡Œé€Ÿåº¦æé«˜1.77å€å¹¶ä½¿ç”¨æ›´å°‘GPUå†…å­˜çš„åŒæ—¶ï¼ŒèŽ·å¾—äº†æœ€ä½³çš„å¹³å‡ATEã€‚ä¸Žç»å…¸çš„åŸºäºŽä¼˜åŒ–çš„VIOç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæœ‰ç«žäº‰åŠ›çš„è½¨è¿¹ç²¾åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½Žäº†è®¡ç®—è´Ÿè½½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰VIOæ–¹æ³•éœ€è¦åœ¨ç²¾åº¦å’Œè®¡ç®—æ•ˆçŽ‡ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚åŸºäºŽä¼˜åŒ–çš„æ–¹æ³•ï¼Œå¦‚VIBAï¼Œè™½ç„¶èƒ½æä¾›é«˜ç²¾åº¦çš„ä½å§¿ä¼°è®¡ï¼Œä½†è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„ç§»åŠ¨å¹³å°ä¸Šå®žæ—¶è¿è¡Œã€‚åŸºäºŽæ»¤æ³¢çš„æ–¹æ³•è™½ç„¶è®¡ç®—æ•ˆçŽ‡é«˜ï¼Œä½†ç²¾åº¦ç›¸å¯¹è¾ƒä½Žï¼Œå®¹æ˜“äº§ç”Ÿæ¼‚ç§»ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹ï¼Œé™ä½ŽVIOçš„è®¡ç®—æˆæœ¬ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†VIOä¸­çš„ä¸¤ä¸ªå…³é”®å†³ç­–è¿‡ç¨‹ï¼Œå³â€œä½•æ—¶è¿è¡Œè§†è§‰å‰ç«¯â€å’Œâ€œå¦‚ä½•èžåˆè§†è§‰å’Œæƒ¯æ€§ä¿¡æ¯â€ï¼Œå»ºæ¨¡ä¸ºåºåˆ—å†³ç­–é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (RL)æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚é€šè¿‡æ™ºèƒ½åœ°æŽ§åˆ¶è§†è§‰å‰ç«¯çš„è¿è¡Œé¢‘çŽ‡å’Œè‡ªé€‚åº”åœ°èžåˆè§†è§‰å’Œæƒ¯æ€§ä¿¡æ¯ï¼Œå¯ä»¥åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½Žè®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥VIOæ¡†æž¶åŒ…å«ä¸¤ä¸ªä¸»è¦çš„RLæ™ºèƒ½ä½“ï¼šé€‰æ‹©æ™ºèƒ½ä½“(Select Agent)å’Œèžåˆæ™ºèƒ½ä½“(Fusion Agent)ã€‚é€‰æ‹©æ™ºèƒ½ä½“åŸºäºŽé«˜é¢‘IMUæ•°æ®ï¼Œå†³å®šæ˜¯å¦è¿è¡Œè§†è§‰å‰ç«¯ã€‚èžåˆæ™ºèƒ½ä½“é¦–å…ˆé€šè¿‡ä¸€ä¸ªç›‘ç£ç½‘ç»œä¼°è®¡é²æ£’çš„é€Ÿåº¦çŠ¶æ€ï¼Œç„¶åŽä½¿ç”¨RLç­–ç•¥è‡ªé€‚åº”åœ°èžåˆè§†è§‰ä¿¡æ¯å’Œæƒ¯æ€§ä¿¡æ¯ï¼Œå¾—åˆ°æœ€ç»ˆçš„ä½å§¿ä¼°è®¡ã€‚æ•´ä¸ªæ¡†æž¶å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªè‡ªé€‚åº”çš„VIOç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®çŽ¯å¢ƒå’Œè‡ªèº«çŠ¶æ€åŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºçš„ä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽå°†VIOä¸­çš„å†³ç­–è¿‡ç¨‹å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚ä¸Žä¼ ç»Ÿçš„VIOæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´åŠ æ™ºèƒ½åœ°æŽ§åˆ¶è®¡ç®—èµ„æºçš„åˆ†é…ï¼Œä»Žè€Œåœ¨ç²¾åº¦å’Œæ•ˆçŽ‡ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚åŒæ™ºèƒ½ä½“çš„è®¾è®¡ä¹Ÿæ˜¯ä¸€ä¸ªåˆ›æ–°ï¼Œé€‰æ‹©æ™ºèƒ½ä½“è´Ÿè´£æŽ§åˆ¶è§†è§‰å‰ç«¯çš„è¿è¡Œï¼Œèžåˆæ™ºèƒ½ä½“è´Ÿè´£ä¿¡æ¯èžåˆï¼Œä¸¤ä¸ªæ™ºèƒ½ä½“ååŒå·¥ä½œï¼Œå…±åŒä¼˜åŒ–VIOçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šé€‰æ‹©æ™ºèƒ½ä½“çš„è¾“å…¥æ˜¯é«˜é¢‘IMUæ•°æ®ï¼Œè¾“å‡ºæ˜¯æ˜¯å¦è¿è¡Œè§†è§‰å‰ç«¯çš„å†³ç­–ã€‚èžåˆæ™ºèƒ½ä½“åŒ…å«ä¸€ä¸ªç›‘ç£ç½‘ç»œå’Œä¸€ä¸ªRLç­–ç•¥ã€‚ç›‘ç£ç½‘ç»œç”¨äºŽä¼°è®¡é²æ£’çš„é€Ÿåº¦çŠ¶æ€ï¼ŒRLç­–ç•¥ç”¨äºŽè‡ªé€‚åº”åœ°èžåˆè§†è§‰ä¿¡æ¯å’Œæƒ¯æ€§ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘ç²¾åº¦å’Œè®¡ç®—æˆæœ¬ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦é¼“åŠ±æ™ºèƒ½ä½“åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹ï¼Œå°½å¯èƒ½åœ°å‡å°‘è®¡ç®—é‡ã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å®žé™…åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨EuRoC MAVå’ŒTUM-VIæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸Žä¹‹å‰çš„åŸºäºŽGPUçš„VO/VIOç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è¿è¡Œé€Ÿåº¦æé«˜1.77å€å¹¶ä½¿ç”¨æ›´å°‘GPUå†…å­˜çš„åŒæ—¶ï¼ŒèŽ·å¾—äº†æœ€ä½³çš„å¹³å‡ATEã€‚ä¸Žç»å…¸çš„åŸºäºŽä¼˜åŒ–çš„VIOç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæœ‰ç«žäº‰åŠ›çš„è½¨è¿¹ç²¾åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½Žäº†è®¡ç®—è´Ÿè½½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæœºå™¨äººè‡ªä¸»å¯¼èˆªã€å¢žå¼ºçŽ°å®žã€æ— äººæœºç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½ŽVIOçš„è®¡ç®—æˆæœ¬ï¼Œå¯ä»¥ä½¿è¿™äº›åº”ç”¨åœ¨èµ„æºå—é™çš„ç§»åŠ¨å¹³å°ä¸Šè¿è¡Œï¼Œä»Žè€Œæ‰©å±•å…¶åº”ç”¨èŒƒå›´ã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”çš„VIOç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®çŽ¯å¢ƒå’Œè‡ªèº«çŠ¶æ€åŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºçš„ä½¿ç”¨ï¼Œä»Žè€Œæé«˜ç³»ç»Ÿçš„é²æ£’æ€§å’Œå¯é æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual-Inertial Odometry (VIO) is a critical component for robust ego-motion estimation, enabling foundational capabilities such as autonomous navigation in robotics and real-time 6-DoF tracking for augmented reality. Existing methods face a well-known trade-off: filter-based approaches are efficient but prone to drift, while optimization-based methods, though accurate, rely on computationally prohibitive Visual-Inertial Bundle Adjustment (VIBA) that is difficult to run on resource-constrained platforms. Rather than removing VIBA altogether, we aim to reduce how often and how heavily it must be invoked. To this end, we cast two key design choices in modern VIO, when to run the visual frontend and how strongly to trust its output, as sequential decision problems, and solve them with lightweight reinforcement learning (RL) agents. Our framework introduces a lightweight, dual-pronged RL policy that serves as our core contribution: (1) a Select Agent intelligently gates the entire VO pipeline based only on high-frequency IMU data; and (2) a composite Fusion Agent that first estimates a robust velocity state via a supervised network, before an RL policy adaptively fuses the full (p, v, q) state. Experiments on the EuRoC MAV and TUM-VI datasets show that, in our unified evaluation, the proposed method achieves a more favorable accuracy-efficiency-memory trade-off than prior GPU-based VO/VIO systems: it attains the best average ATE while running up to 1.77 times faster and using less GPU memory. Compared to classical optimization-based VIO systems, our approach maintains competitive trajectory accuracy while substantially reducing computational load.

