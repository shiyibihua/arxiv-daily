---
layout: default
title: Predictive Safety Shield for Dyna-Q Reinforcement Learning
---

# Predictive Safety Shield for Dyna-Q Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.21531" target="_blank" class="toolbar-btn">arXiv: 2511.21531v1</a>
    <a href="https://arxiv.org/pdf/2511.21531.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.21531v1" 
            onclick="toggleFavorite(this, '2511.21531v1', 'Predictive Safety Shield for Dyna-Q Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jin Pin, Krasowski Hanna, Vanneaux Elena

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO, eess.SY

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-26

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÈ¢ÑÊµãÁöÑÂÆâÂÖ®ÁõæÔºåÊèêÂçáDyna-QÂº∫ÂåñÂ≠¶‰π†Âú®Á¶ªÊï£Á©∫Èó¥ÁöÑÂÆâÂÖ®ÊÄßÂíåÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `ÂÆâÂÖ®Áõæ` `Ê®°ÂûãÈ¢ÑÊµã` `Dyna-Q` `ÂÆâÂÖ®ÊÄß` `Á¶ªÊï£Á©∫Èó¥` `Êú∫Âô®‰∫∫ÂØºËà™`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂÆâÂÖ®ÁõæÊñπÊ≥ï‰æùËµñÈöèÊú∫ÊäΩÊ†∑ÊàñÂõ∫ÂÆöÊéßÂà∂Âô®ÔºåÂøΩÁï•‰∫ÜÂÆâÂÖ®Âä®‰ΩúÂØπÊú™Êù•ÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåÈôêÂà∂‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÁöÑÂ∫îÁî®„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçÈ¢ÑÊµãÂÆâÂÖ®ÁõæÔºåÈÄöËøáÁéØÂ¢ÉÊ®°ÂûãÁöÑÂÆâÂÖ®Ê®°ÊãüËøõË°åÂÆâÂÖ®È¢ÑÊµãÔºåÂπ∂Â±ÄÈÉ®Êõ¥Êñ∞QÂáΩÊï∞Ôºå‰ªéËÄå‰ºòÂåñÂÆâÂÖ®Âä®‰ΩúÁöÑÈÄâÊã©„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÁΩëÊ†º‰∏ñÁïå‰∏≠Âç≥‰Ωø‰ΩøÁî®Áü≠È¢ÑÊµãËåÉÂõ¥‰πüËÉΩÊâæÂà∞ÊúÄ‰ºòË∑ØÂæÑÔºå‰∏îÂØπÊ®°Êãü‰∏éÁé∞ÂÆûÁöÑÂàÜÂ∏ÉÂÅèÁßªÂÖ∑ÊúâÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏∫Âº∫ÂåñÂ≠¶‰π†Êèê‰æõÂÆâÂÖ®‰øùÈöúÊòØÂÆûÁé∞ÂÖ∂Âú®Áé∞ÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠Â∫îÁî®ÁöÑÂÖ≥ÈîÆÊåëÊàò„ÄÇÂÆâÂÖ®ÁõæÊâ©Â±ï‰∫ÜÊ†áÂáÜÂº∫ÂåñÂ≠¶‰π†ÔºåÂÆûÁé∞‰∫ÜÁ°¨ÊÄßÂÆâÂÖ®‰øùËØÅ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂÆâÂÖ®ÁõæÈÄöÂ∏∏‰ΩøÁî®ÂÆâÂÖ®Âä®‰ΩúÁöÑÈöèÊú∫ÊäΩÊ†∑ÊàñÂõ∫ÂÆöÁöÑÂõûÈÄÄÊéßÂà∂Âô®ÔºåÂõ†Ê≠§ÂøΩÁï•‰∫Ü‰∏çÂêåÂÆâÂÖ®Âä®‰ΩúÂØπÊú™Êù•ÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁî®‰∫éÁ¶ªÊï£Á©∫Èó¥‰∏≠Âü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰ΩìÁöÑÈ¢ÑÊµãÂÆâÂÖ®Áõæ„ÄÇÊàë‰ª¨ÁöÑÂÆâÂÖ®ÁõæÂü∫‰∫éÂÆâÂÖ®È¢ÑÊµãÂ±ÄÈÉ®Êõ¥Êñ∞QÂáΩÊï∞ÔºåËøô‰∫õÈ¢ÑÊµãÊ∫ê‰∫éÁéØÂ¢ÉÊ®°ÂûãÁöÑÂÆâÂÖ®Ê®°Êãü„ÄÇËøôÁßçÂ±èËîΩÊñπÊ≥ïÂú®‰øùÊåÅÁ°¨ÊÄßÂÆâÂÖ®‰øùËØÅÁöÑÂêåÊó∂ÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇÂú®ÁΩëÊ†º‰∏ñÁïåÁéØÂ¢É‰∏≠ÁöÑÂÆûÈ™åË°®ÊòéÔºåÂç≥‰ΩøÊòØÁü≠ÁöÑÈ¢ÑÊµãËåÉÂõ¥‰πüË∂≥‰ª•ËØÜÂà´ÊúÄ‰Ω≥Ë∑ØÂæÑ„ÄÇÊàë‰ª¨ËßÇÂØüÂà∞ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂØπÂàÜÂ∏ÉÂÅèÁßªÔºà‰æãÂ¶ÇÔºåÊ®°ÊãüÂíåÁé∞ÂÆû‰πãÈó¥ÔºâÂÖ∑ÊúâÈ≤ÅÊ£íÊÄßÔºåËÄåÊó†ÈúÄÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂº∫ÂåñÂ≠¶‰π†Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Èù¢‰∏¥ÂÆâÂÖ®ÈóÆÈ¢òÔºåÈúÄË¶Å‰øùËØÅÊô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫ÂßãÁªàÂ§Ñ‰∫éÂÆâÂÖ®Áä∂ÊÄÅ„ÄÇÁé∞ÊúâÁöÑÂÆâÂÖ®ÁõæÊñπÊ≥ïÔºåÂ¶ÇÈöèÊú∫ÈááÊ†∑ÂÆâÂÖ®Âä®‰ΩúÊàñ‰ΩøÁî®Âõ∫ÂÆöÂõûÈÄÄÁ≠ñÁï•ÔºåËôΩÁÑ∂ËÉΩ‰øùËØÅÂÆâÂÖ®ÊÄßÔºå‰ΩÜÂæÄÂæÄ‰ºöÁâ∫Áâ≤ÊÄßËÉΩÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Ê≤°ÊúâÂÖÖÂàÜËÄÉËôë‰∏çÂêåÂÆâÂÖ®Âä®‰ΩúÂØπÊú™Êù•ÂõûÊä•ÁöÑÂΩ±Âìç„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂú®‰øùËØÅÂÆâÂÖ®ÊÄßÁöÑÂâçÊèê‰∏ãÔºåÊèêÂçáÂº∫ÂåñÂ≠¶‰π†ÁöÑÊÄßËÉΩÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÁéØÂ¢ÉÊ®°ÂûãËøõË°åÈ¢ÑÊµãÔºå‰ªéËÄåÈÄâÊã©Êõ¥‰ºòÁöÑÂÆâÂÖ®Âä®‰Ωú„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáÂØπÁéØÂ¢ÉÊ®°ÂûãËøõË°åÂÆâÂÖ®Ê®°ÊãüÔºåÈ¢ÑÊµã‰∏çÂêåÂÆâÂÖ®Âä®‰ΩúÁöÑÊú™Êù•Áä∂ÊÄÅÂíåÂõûÊä•ÔºåÂπ∂Âü∫‰∫éËøô‰∫õÈ¢ÑÊµãÊù•Â±ÄÈÉ®Êõ¥Êñ∞QÂáΩÊï∞„ÄÇËøôÊ†∑ÔºåÂÆâÂÖ®Áõæ‰∏ç‰ªÖËÉΩ‰øùËØÅÂÆâÂÖ®ÊÄßÔºåËøòËÉΩÈÄâÊã©Êõ¥ÊúâÂà©‰∫éÈïøÊúüÂõûÊä•ÁöÑÂä®‰ΩúÔºå‰ªéËÄåÊèêÂçáÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) Âü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰ΩìÔºå‰ΩøÁî®Dyna-QÁÆóÊ≥ïËøõË°åÂ≠¶‰π†Ôºõ2) ÁéØÂ¢ÉÊ®°ÂûãÔºåÁî®‰∫éÊ®°ÊãüÁéØÂ¢ÉÁöÑÂä®ÊÄÅÂèòÂåñÔºõ3) ÂÆâÂÖ®ÁõæÔºåË¥üË¥£Âà§Êñ≠ÂΩìÂâçÂä®‰ΩúÊòØÂê¶ÂÆâÂÖ®ÔºåÂπ∂ÈÄâÊã©ÂÆâÂÖ®Âä®‰ΩúÔºõ4) È¢ÑÊµãÊ®°ÂùóÔºåÂü∫‰∫éÁéØÂ¢ÉÊ®°ÂûãËøõË°åÂÆâÂÖ®È¢ÑÊµãÔºåËØÑ‰º∞‰∏çÂêåÂÆâÂÖ®Âä®‰ΩúÁöÑÊú™Êù•ÂõûÊä•Ôºõ5) QÂáΩÊï∞Êõ¥Êñ∞Ê®°ÂùóÔºåÊ†πÊçÆÈ¢ÑÊµãÁªìÊûúÂ±ÄÈÉ®Êõ¥Êñ∞QÂáΩÊï∞„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöÊô∫ËÉΩ‰ΩìÊ†πÊçÆÂΩìÂâçÁä∂ÊÄÅÈÄâÊã©Âä®‰ΩúÔºåÂÆâÂÖ®ÁõæÂà§Êñ≠Âä®‰ΩúÊòØÂê¶ÂÆâÂÖ®ÔºåÂ¶ÇÊûúÂÆâÂÖ®ÂàôÊâßË°åÔºåÂê¶Âàô‰ΩøÁî®È¢ÑÊµãÊ®°ÂùóÈÄâÊã©Êõ¥‰ºòÁöÑÂÆâÂÖ®Âä®‰ΩúÔºåÂπ∂Êõ¥Êñ∞QÂáΩÊï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÈ¢ÑÊµãÂºïÂÖ•ÂÆâÂÖ®Áõæ‰∏≠„ÄÇ‰º†ÁªüÁöÑÂÆâÂÖ®ÁõæÂè™ÂÖ≥Ê≥®ÂΩìÂâçÂä®‰ΩúÁöÑÂÆâÂÖ®ÊÄßÔºåËÄåÂøΩÁï•‰∫ÜÊú™Êù•ÂõûÊä•„ÄÇÈÄöËøá‰ΩøÁî®ÁéØÂ¢ÉÊ®°ÂûãËøõË°åÈ¢ÑÊµãÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üËØÑ‰º∞‰∏çÂêåÂÆâÂÖ®Âä®‰ΩúÁöÑÈïøÊúüÂΩ±ÂìçÔºå‰ªéËÄåÈÄâÊã©Êõ¥ÊúâÂà©‰∫éÈïøÊúüÂõûÊä•ÁöÑÂä®‰Ωú„ÄÇËøôÁßçÈ¢ÑÊµãËÉΩÂäõ‰ΩøÂæóÂÆâÂÖ®Áõæ‰∏ç‰ªÖËÉΩ‰øùËØÅÂÆâÂÖ®ÊÄßÔºåËøòËÉΩÊèêÂçáÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËØ•ÊñπÊ≥ïÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Dyna-QÁÆóÊ≥ïËøõË°åÂ≠¶‰π†ÔºåDyna-QÁÆóÊ≥ïÊòØ‰∏ÄÁßçÂü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÈÄÇÂêà‰∫éÁ¶ªÊï£Á©∫Èó¥Ôºõ2) ‰ΩøÁî®ÁéØÂ¢ÉÊ®°ÂûãËøõË°åÂÆâÂÖ®È¢ÑÊµãÔºåÁéØÂ¢ÉÊ®°ÂûãÂèØ‰ª•ÊòØÂ≠¶‰π†ÂæóÂà∞ÁöÑÔºå‰πüÂèØ‰ª•ÊòØÈ¢ÑÂÖàÂÆö‰πâÁöÑÔºõ3) Â±ÄÈÉ®Êõ¥Êñ∞QÂáΩÊï∞ÔºåÂè™Êõ¥Êñ∞‰∏éÂÆâÂÖ®È¢ÑÊµãÁõ∏ÂÖ≥ÁöÑQÂÄºÔºåÈÅøÂÖçÂΩ±ÂìçÂÖ∂‰ªñQÂÄºÁöÑÂáÜÁ°ÆÊÄßÔºõ4) È¢ÑÊµãËåÉÂõ¥ÁöÑÈÄâÊã©ÔºåÈ¢ÑÊµãËåÉÂõ¥Ë∂äÈïøÔºåÈ¢ÑÊµãË∂äÂáÜÁ°ÆÔºå‰ΩÜËÆ°ÁÆóÊàêÊú¨‰πüË∂äÈ´òÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÈóÆÈ¢òËøõË°åÊùÉË°°„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÁΩëÊ†º‰∏ñÁïåÁéØÂ¢É‰∏≠ËÉΩÂ§üÊúâÊïàÂú∞ÊèêÂçáÂº∫ÂåñÂ≠¶‰π†ÁöÑÊÄßËÉΩÔºåÂêåÊó∂‰øùËØÅÂÆâÂÖ®ÊÄß„ÄÇÂç≥‰Ωø‰ΩøÁî®ËæÉÁü≠ÁöÑÈ¢ÑÊµãËåÉÂõ¥ÔºåËØ•ÊñπÊ≥ï‰πüËÉΩÊâæÂà∞ÊúÄ‰ºòË∑ØÂæÑ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂØπÊ®°Êãü‰∏éÁé∞ÂÆû‰πãÈó¥ÁöÑÂàÜÂ∏ÉÂÅèÁßªÂÖ∑ÊúâÈ≤ÅÊ£íÊÄßÔºåÊó†ÈúÄÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂú®Êüê‰∫õÂÆûÈ™å‰∏≠ÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üËææÂà∞‰∏éÊó†ÂÆâÂÖ®ÁõæÁöÑDyna-QÁÆóÊ≥ïÁõ∏ËøëÁöÑÊÄßËÉΩÔºåÂêåÊó∂‰øùËØÅ‰∫Ü100%ÁöÑÂÆâÂÖ®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊ∏∏ÊàèAIÁ≠âÈ¢ÜÂüüÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÂØπÂÆâÂÖ®ÊÄßË¶ÅÊ±ÇËæÉÈ´òÁöÑÂú∫ÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫ÂØºËà™‰∏≠ÔºåÂèØ‰ª•Âà©Áî®ËØ•ÊñπÊ≥ï‰øùËØÅÊú∫Âô®‰∫∫Âú®ÈÅøÂºÄÈöúÁ¢çÁâ©ÁöÑÂêåÊó∂ÔºåÂ∞ΩÂèØËÉΩÂø´Âú∞Âà∞ËææÁõÆÊ†á‰ΩçÁΩÆ„ÄÇÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåÂèØ‰ª•‰øùËØÅËΩ¶ËæÜÂú®Ë°åÈ©∂ËøáÁ®ã‰∏≠ÂßãÁªàÂ§Ñ‰∫éÂÆâÂÖ®Áä∂ÊÄÅÔºåÈÅøÂÖçÂèëÁîü‰∫§ÈÄö‰∫ãÊïÖ„ÄÇËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éÊ∏∏ÊàèAI‰∏≠Ôºå‰ΩøAIÂú®‰øùËØÅÊ∏∏ÊàèËßÑÂàôÁöÑÂâçÊèê‰∏ãÔºåÂÅöÂá∫Êõ¥Êô∫ËÉΩÁöÑÂÜ≥Á≠ñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Obtaining safety guarantees for reinforcement learning is a major challenge to achieve applicability for real-world tasks. Safety shields extend standard reinforcement learning and achieve hard safety guarantees. However, existing safety shields commonly use random sampling of safe actions or a fixed fallback controller, therefore disregarding future performance implications of different safe actions. In this work, we propose a predictive safety shield for model-based reinforcement learning agents in discrete space. Our safety shield updates the Q-function locally based on safe predictions, which originate from a safe simulation of the environment model. This shielding approach improves performance while maintaining hard safety guarantees. Our experiments on gridworld environments demonstrate that even short prediction horizons can be sufficient to identify the optimal path. We observe that our approach is robust to distribution shifts, e.g., between simulation and reality, without requiring additional training.

