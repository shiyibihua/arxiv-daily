---
layout: default
title: HALO: Human Preference Aligned Offline Reward Learning for Robot Navigation
---

# HALO: Human Preference Aligned Offline Reward Learning for Robot Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01539" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01539v1</a>
  <a href="https://arxiv.org/pdf/2508.01539.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01539v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01539v1', 'HALO: Human Preference Aligned Offline Reward Learning for Robot Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gershom Seneviratne, Jianyu An, Sahire Ellahy, Kasun Weerakoon, Mohamed Bashir Elnoor, Jonathan Deepak Kannan, Amogha Thalihalla Sunil, Dinesh Manocha

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHALOä»¥è§£å†³æœºå™¨äººå¯¼èˆªä¸­çš„äººç±»åå¥½å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `ç¦»çº¿å¥–åŠ±å­¦ä¹ ` `äººç±»åå¥½å¯¹é½` `æœºå™¨äººå¯¼èˆª` `ä¸“å®¶è½¨è¿¹` `Boltzmannåˆ†å¸ƒ` `Plackett-LuceæŸå¤±` `æ™ºèƒ½å†³ç­–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººå¯¼èˆªæ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨äººç±»çš„ç›´è§‰å’Œåå¥½ï¼Œå¯¼è‡´å¯¼èˆªæ€§èƒ½ä¸è¶³ã€‚
2. HALOé€šè¿‡ç¦»çº¿å­¦ä¹ äººç±»åå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œå°†ä¸“å®¶è½¨è¿¹ä¸ç”¨æˆ·åé¦ˆç»“åˆï¼Œæå‡å¯¼èˆªå†³ç­–çš„æ™ºèƒ½æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHALOåœ¨æˆåŠŸç‡ä¸Šæé«˜äº†è‡³å°‘33.3%ï¼Œå¹¶åœ¨è½¨è¿¹é•¿åº¦å’ŒFrechetè·ç¦»ä¸Šå‡æœ‰æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„ç¦»çº¿å¥–åŠ±å­¦ä¹ ç®—æ³•HALOï¼Œè¯¥ç®—æ³•å°†äººç±»åœ¨å¯¼èˆªä¸­çš„ç›´è§‰é‡åŒ–ä¸ºåŸºäºè§†è§‰çš„å¥–åŠ±å‡½æ•°ã€‚HALOä»ç¦»çº¿æ•°æ®ä¸­å­¦ä¹ å¥–åŠ±æ¨¡å‹ï¼Œåˆ©ç”¨ç§»åŠ¨æœºå™¨äººæ”¶é›†çš„ä¸“å®¶è½¨è¿¹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå›´ç»•å‚è€ƒåŠ¨ä½œå‡åŒ€é‡‡æ ·åŠ¨ä½œï¼Œå¹¶ä½¿ç”¨åŸºäºç”¨æˆ·åé¦ˆçš„åå¥½åˆ†æ•°å¯¹å…¶è¿›è¡Œæ’åã€‚é€šè¿‡Plackett-LuceæŸå¤±å‡½æ•°è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä»¥ä¸è¿™äº›æ’ååå¥½å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒHALOåœ¨å¤šç§åœºæ™¯ä¸‹çš„å®é™…éƒ¨ç½²ä¸­ï¼Œè®­ç»ƒå‡ºçš„ç­–ç•¥åœ¨æœªè§ç¯å¢ƒä¸­æœ‰æ•ˆæ³›åŒ–ï¼Œå¹¶åœ¨æˆåŠŸç‡ã€è½¨è¿¹é•¿åº¦å’ŒFrechetè·ç¦»ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººå¯¼èˆªä¸­å¦‚ä½•æœ‰æ•ˆæ•´åˆäººç±»åå¥½ä¸ç›´è§‰çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨äººç±»åé¦ˆæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´å¯¼èˆªæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHALOé€šè¿‡ç¦»çº¿æ•°æ®å­¦ä¹ å¥–åŠ±æ¨¡å‹ï¼Œåˆ©ç”¨ä¸“å®¶è½¨è¿¹å’Œç”¨æˆ·åé¦ˆæ¥é‡åŒ–äººç±»çš„å¯¼èˆªåå¥½ï¼Œä»è€Œä¼˜åŒ–æœºå™¨äººå¯¼èˆªç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHALOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€åŠ¨ä½œé‡‡æ ·ã€åå¥½æ’åå’Œå¥–åŠ±æ¨¡å‹è®­ç»ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†ä¸“å®¶è½¨è¿¹ï¼Œç„¶åå›´ç»•å‚è€ƒåŠ¨ä½œè¿›è¡Œå‡åŒ€é‡‡æ ·ï¼Œæ¥ç€æ ¹æ®ç”¨æˆ·åé¦ˆè¿›è¡Œåå¥½æ’åï¼Œæœ€åé€šè¿‡Plackett-LuceæŸå¤±å‡½æ•°è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šHALOçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†äººç±»åå¥½ä¸å¥–åŠ±å­¦ä¹ ç»“åˆï¼Œé€šè¿‡Boltzmannåˆ†å¸ƒå’Œç”¨æˆ·åé¦ˆå®ç°äº†æ›´ä¸ºç²¾å‡†çš„å¯¼èˆªå†³ç­–ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŠ¨ä½œçš„å‡åŒ€é‡‡æ ·å’ŒåŸºäºç”¨æˆ·åé¦ˆçš„åå¥½è¯„åˆ†æ˜¯å…³é”®è®¾è®¡ï¼ŒPlackett-LuceæŸå¤±å‡½æ•°ç”¨äºä¼˜åŒ–å¥–åŠ±æ¨¡å‹ï¼Œä½¿å…¶æ›´å¥½åœ°å¯¹é½äººç±»åå¥½ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HALOåœ¨å®é™…éƒ¨ç½²ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºç°æœ‰çš„è§†è§‰å¯¼èˆªæ–¹æ³•ï¼Œå…¶æˆåŠŸç‡æé«˜äº†è‡³å°‘33.3%ï¼Œè½¨è¿¹é•¿åº¦å‡å°‘äº†12.9%ï¼ŒFrechetè·ç¦»é™ä½äº†26.6%ã€‚è¿™äº›ç»“æœè¡¨æ˜HALOåœ¨å¤šæ ·åŒ–åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HALOçš„ç ”ç©¶æˆæœåœ¨æœºå™¨äººå¯¼èˆªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªèƒ½åŠ›ã€‚å…¶æ–¹æ³•ä¸ä»…é€‚ç”¨äºç§»åŠ¨æœºå™¨äººï¼Œè¿˜å¯æ‰©å±•åˆ°å…¶ä»–éœ€è¦äººæœºäº¤äº’çš„æ™ºèƒ½ç³»ç»Ÿä¸­ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½å¯¼èˆªæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we introduce HALO, a novel Offline Reward Learning algorithm that quantifies human intuition in navigation into a vision-based reward function for robot navigation. HALO learns a reward model from offline data, leveraging expert trajectories collected from mobile robots. During training, actions are uniformly sampled around a reference action and ranked using preference scores derived from a Boltzmann distribution centered on the preferred action, and shaped based on binary user feedback to intuitive navigation queries. The reward model is trained via the Plackett-Luce loss to align with these ranked preferences. To demonstrate the effectiveness of HALO, we deploy its reward model in two downstream applications: (i) an offline learned policy trained directly on the HALO-derived rewards, and (ii) a model-predictive-control (MPC) based planner that incorporates the HALO reward as an additional cost term. This showcases the versatility of HALO across both learning-based and classical navigation frameworks. Our real-world deployments on a Clearpath Husky across diverse scenarios demonstrate that policies trained with HALO generalize effectively to unseen environments and hardware setups not present in the training data. HALO outperforms state-of-the-art vision-based navigation methods, achieving at least a 33.3% improvement in success rate, a 12.9% reduction in normalized trajectory length, and a 26.6% reduction in Frechet distance compared to human expert trajectories.

