---
layout: default
title: Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface
---

# Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19402" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.19402v1</a>
  <a href="https://arxiv.org/pdf/2512.19402.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19402v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19402v1', 'Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yujie Zhao, Hongwei Fan, Di Chen, Shengcong Chen, Liliang Chen, Xiaoqi Li, Guanghui Ren, Hao Dong

**åˆ†ç±»**: cs.RO, cs.CV, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Real2Edit2Realï¼šé€šè¿‡3Dæ§åˆ¶ç•Œé¢ç”Ÿæˆæœºå™¨äººæ“ä½œæ¼”ç¤ºæ•°æ®ï¼Œæå‡æ•°æ®æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººå­¦ä¹ ` `æ•°æ®å¢å¼º` `3Dé‡å»º` `è§†é¢‘ç”Ÿæˆ` `æ“ä½œä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æœºå™¨äººå­¦ä¹ ä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä½†æ”¶é›†å¤šæ ·åŒ–çš„æ“ä½œæ¼”ç¤ºæ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†ç­–ç•¥çš„é²æ£’æ€§ã€‚
2. Real2Edit2Realæ¡†æ¶é€šè¿‡3Dæ§åˆ¶ç•Œé¢ï¼Œåˆ©ç”¨3Dç¼–è¾‘å’Œå¤šæ¡ä»¶è§†é¢‘ç”Ÿæˆï¼Œä»å°‘é‡çœŸå®æ•°æ®ç”Ÿæˆå¤§é‡å¢å¼ºæ•°æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ¡†æ¶ç”Ÿæˆçš„æ•°æ®è®­ç»ƒçš„ç­–ç•¥ï¼Œåœ¨æ•°æ®æ•ˆç‡ä¸Šæ¯”ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒçš„ç­–ç•¥æå‡äº†10-50å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†å‡å°‘æœºå™¨äººå­¦ä¹ ä¸­é‡å¤çš„æ•°æ®æ”¶é›†ï¼Œç‰¹åˆ«æ˜¯æ“ä½œä»»åŠ¡ä¸­çš„ç©ºé—´æ³›åŒ–é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Real2Edit2Realæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡3Då¯ç¼–è¾‘æ€§ä¸2Dè§†è§‰æ•°æ®æ¡¥æ¥ï¼Œåˆ©ç”¨3Dæ§åˆ¶ç•Œé¢ç”Ÿæˆæ–°çš„æ¼”ç¤ºæ•°æ®ã€‚è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨åº¦é‡å°ºåº¦çš„3Dé‡å»ºæ¨¡å‹ä»å¤šè§†è§’RGBè§‚æµ‹ä¸­é‡å»ºåœºæ™¯å‡ ä½•ã€‚åŸºäºé‡å»ºçš„å‡ ä½•ä½“ï¼Œå¯¹ç‚¹äº‘è¿›è¡Œæ·±åº¦å¯é çš„3Dç¼–è¾‘ï¼Œç”Ÿæˆæ–°çš„æ“ä½œè½¨è¿¹ï¼ŒåŒæ—¶å‡ ä½•æ ¡æ­£æœºå™¨äººå§¿æ€ä»¥æ¢å¤ç‰©ç†ä¸€è‡´çš„æ·±åº¦ï¼Œä½œä¸ºåˆæˆæ–°æ¼”ç¤ºçš„å¯é æ¡ä»¶ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ä¸ªä»¥æ·±åº¦ä¸ºä¸»è¦æ§åˆ¶ä¿¡å·ï¼Œç»“åˆåŠ¨ä½œã€è¾¹ç¼˜å’Œå°„çº¿å›¾çš„å¤šæ¡ä»¶è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åˆæˆç©ºé—´å¢å¼ºçš„å¤šè§†è§’æ“ä½œè§†é¢‘ã€‚åœ¨å››ä¸ªçœŸå®æ“ä½œä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä»…ç”¨1-5ä¸ªæºæ¼”ç¤ºæ•°æ®ç”Ÿæˆçš„è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥åŒ¹é…ç”šè‡³è¶…è¿‡ç”¨50ä¸ªçœŸå®æ¼”ç¤ºæ•°æ®è®­ç»ƒçš„ç­–ç•¥ï¼Œæ•°æ®æ•ˆç‡æé«˜äº†10-50å€ã€‚æ­¤å¤–ï¼Œé«˜åº¦å’Œçº¹ç†ç¼–è¾‘çš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œè¡¨æ˜å…¶æœ‰æ½œåŠ›ä½œä¸ºç»Ÿä¸€çš„æ•°æ®ç”Ÿæˆæ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººå­¦ä¹ æ–¹æ³•ä¾èµ–å¤§é‡çœŸå®æ¼”ç¤ºæ•°æ®ï¼Œå°¤å…¶æ˜¯åœ¨æ“ä½œä»»åŠ¡ä¸­ï¼Œæ”¶é›†å…·æœ‰ç©ºé—´æ³›åŒ–èƒ½åŠ›çš„æ¼”ç¤ºæ•°æ®æˆæœ¬å¾ˆé«˜ã€‚è¿™é™åˆ¶äº†ç­–ç•¥çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨å°‘é‡çœŸå®æ•°æ®ç”Ÿæˆé«˜è´¨é‡çš„å¢å¼ºæ•°æ®ï¼Œæ˜¯æœ¬è®ºæ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡3Då¯ç¼–è¾‘æ€§å°†2Dè§†è§‰æ•°æ®ä¸3Dæ§åˆ¶ç•Œé¢è¿æ¥èµ·æ¥ï¼Œä»è€Œå®ç°å¯¹æœºå™¨äººæ“ä½œè½¨è¿¹çš„ç¼–è¾‘å’Œç”Ÿæˆã€‚é€šè¿‡åœ¨3Dç©ºé—´ä¸­ç¼–è¾‘åœºæ™¯å‡ ä½•å’Œæœºå™¨äººå§¿æ€ï¼Œå¯ä»¥ç”Ÿæˆæ–°çš„ã€ç‰©ç†ä¸Šåˆç†çš„è½¨è¿¹ï¼Œå¹¶åˆ©ç”¨è¿™äº›è½¨è¿¹åˆæˆæ–°çš„è§†è§‰æ¼”ç¤ºæ•°æ®ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥åœ¨åƒç´ ç©ºé—´è¿›è¡Œç¼–è¾‘ï¼Œä»è€Œä¿è¯äº†ç”Ÿæˆæ•°æ®çš„ç‰©ç†ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReal2Edit2Realæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **3Dé‡å»ºæ¨¡å—**ï¼šä»å¤šè§†è§’RGBå›¾åƒé‡å»ºåœºæ™¯çš„3Då‡ ä½•ç»“æ„ï¼Œå¾—åˆ°åº¦é‡å°ºåº¦çš„ç‚¹äº‘æ¨¡å‹ã€‚2) **3Dç¼–è¾‘æ¨¡å—**ï¼šåœ¨é‡å»ºçš„ç‚¹äº‘ä¸Šè¿›è¡Œæ·±åº¦å¯é çš„3Dç¼–è¾‘ï¼ŒåŒ…æ‹¬æ”¹å˜ç‰©ä½“çš„ä½ç½®ã€å½¢çŠ¶ç­‰ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´æœºå™¨äººå§¿æ€ï¼Œä»¥ä¿è¯ç‰©ç†ä¸€è‡´æ€§ã€‚3) **å¤šæ¡ä»¶è§†é¢‘ç”Ÿæˆæ¨¡å—**ï¼šåŸºäºç¼–è¾‘åçš„3Dåœºæ™¯å’Œæœºå™¨äººå§¿æ€ï¼Œç”Ÿæˆæ–°çš„å¤šè§†è§’æ“ä½œè§†é¢‘ã€‚è¯¥æ¨¡å—ä»¥æ·±åº¦å›¾ä¸ºä¸»è¦æ§åˆ¶ä¿¡å·ï¼ŒåŒæ—¶ç»“åˆåŠ¨ä½œã€è¾¹ç¼˜å’Œå°„çº¿å›¾ç­‰ä¿¡æ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†3Dç¼–è¾‘å¼•å…¥åˆ°æœºå™¨äººæ¼”ç¤ºæ•°æ®çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå›¾åƒçš„å¢å¼ºæ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•åœ¨3Dç©ºé—´ä¸­è¿›è¡Œç¼–è¾‘ï¼Œå¯ä»¥æ›´å¥½åœ°ä¿è¯ç”Ÿæˆæ•°æ®çš„ç‰©ç†åˆç†æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºçš„å¤šæ¡ä»¶è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ·±åº¦ä¿¡æ¯å’Œå…¶ä»–è¾…åŠ©ä¿¡æ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¤šè§†è§’è§†é¢‘ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨3Dç¼–è¾‘æ¨¡å—ä¸­ï¼Œè®ºæ–‡é‡‡ç”¨äº†æ·±åº¦å¯é çš„ç¼–è¾‘æ–¹æ³•ï¼Œå³åœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­å§‹ç»ˆä¿æŒæ·±åº¦ä¿¡æ¯çš„ç‰©ç†ä¸€è‡´æ€§ã€‚åœ¨å¤šæ¡ä»¶è§†é¢‘ç”Ÿæˆæ¨¡å—ä¸­ï¼Œè®ºæ–‡ä½¿ç”¨äº†æ·±åº¦å›¾ä½œä¸ºä¸»è¦æ§åˆ¶ä¿¡å·ï¼Œå¹¶ç»“åˆåŠ¨ä½œã€è¾¹ç¼˜å’Œå°„çº¿å›¾ç­‰ä¿¡æ¯ï¼Œä»¥æé«˜ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæ˜ç¡®æåŠã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Real2Edit2Realæ¡†æ¶ç”Ÿæˆçš„æ•°æ®è®­ç»ƒçš„æœºå™¨äººç­–ç•¥ï¼Œåœ¨å››ä¸ªçœŸå®æ“ä½œä»»åŠ¡ä¸Šï¼Œä»…ä½¿ç”¨1-5ä¸ªçœŸå®æ¼”ç¤ºæ•°æ®ï¼Œå°±èƒ½è¾¾åˆ°ç”šè‡³è¶…è¿‡ä½¿ç”¨50ä¸ªçœŸå®æ¼”ç¤ºæ•°æ®è®­ç»ƒçš„ç­–ç•¥çš„æ€§èƒ½ã€‚æ•°æ®æ•ˆç‡æå‡äº†10-50å€ï¼Œæ˜¾è‘—é™ä½äº†æ•°æ®æ”¶é›†æˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡çš„æ•°æ®å¢å¼ºï¼Œä¾‹å¦‚è£…é…ã€æŠ“å–ã€æ”¾ç½®ç­‰ã€‚é€šè¿‡å°‘é‡çœŸå®æ•°æ®å³å¯ç”Ÿæˆå¤§é‡è®­ç»ƒæ•°æ®ï¼Œé™ä½äº†æœºå™¨äººå­¦ä¹ çš„æˆæœ¬ï¼ŒåŠ é€Ÿäº†æœºå™¨äººæŠ€æœ¯çš„è½åœ°ã€‚è¯¥æ¡†æ¶è¿˜å…·æœ‰ä¸€å®šçš„é€šç”¨æ€§ï¼Œå¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦æ•°æ®å¢å¼ºçš„é¢†åŸŸï¼Œä¾‹å¦‚è™šæ‹Ÿç°å®ã€æ¸¸æˆç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.

