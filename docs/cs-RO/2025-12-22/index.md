---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-12-22
---

# cs.ROï¼ˆ2025-12-22ï¼‰

ğŸ“Š å…± **20** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (12)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251218987v1-affordance-rag-hierarchical-multimodal-retrieval-with-affordance-awa.html">Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation</a></td>
  <td>Affordance RAGï¼šç”¨äºç§»åŠ¨æ“ä½œçš„å…·èº«è®°å¿†åˆ†å±‚å¤šæ¨¡æ€æ£€ç´¢</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span> <span class="paper-tag">open-vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.18987v1" data-paper-url="./papers/251218987v1-affordance-rag-hierarchical-multimodal-retrieval-with-affordance-awa.html" onclick="toggleFavorite(this, '2512.18987v1', 'Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251218938v1-a-framework-for-deploying-learning-based-quadruped-loco-manipulation.html">A Framework for Deploying Learning-based Quadruped Loco-Manipulation</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å››è¶³æœºå™¨äººçµå·§æ“ä½œéƒ¨ç½²æ¡†æ¶ï¼Œè§£å†³ä»¿çœŸåˆ°ç°å®è¿ç§»éš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">whole-body control</span> <span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.18938v1" data-paper-url="./papers/251218938v1-a-framework-for-deploying-learning-based-quadruped-loco-manipulation.html" onclick="toggleFavorite(this, '2512.18938v1', 'A Framework for Deploying Learning-based Quadruped Loco-Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251219390v1-twinaligner-visual-dynamic-alignment-empowers-physics-aware-real2sim.html">TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation</a></td>
  <td>TwinAlignerï¼šé€šè¿‡è§†è§‰-åŠ¨åŠ›å­¦å¯¹é½å®ç°ç‰©ç†æ„ŸçŸ¥çš„Real2Sim2Realæœºå™¨äººæ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">sim2real</span> <span class="paper-tag">real2sim</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19390v1" data-paper-url="./papers/251219390v1-twinaligner-visual-dynamic-alignment-empowers-physics-aware-real2sim.html" onclick="toggleFavorite(this, '2512.19390v1', 'TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251219043v1-egm-efficiently-learning-general-motion-tracking-policy-for-high-dyn.html">EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control</a></td>
  <td>EGMï¼šé«˜æ•ˆå­¦ä¹ é€šç”¨è¿åŠ¨è·Ÿè¸ªç­–ç•¥ï¼Œç”¨äºé«˜åŠ¨æ€äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">whole-body control</span> <span class="paper-tag">motion tracking</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19043v1" data-paper-url="./papers/251219043v1-egm-efficiently-learning-general-motion-tracking-policy-for-high-dyn.html" onclick="toggleFavorite(this, '2512.19043v1', 'EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251219562v1-realm-a-real-to-sim-validated-benchmark-for-generalization-in-roboti.html">REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation</a></td>
  <td>REALMï¼šç”¨äºæœºå™¨äººæ“ä½œæ³›åŒ–èƒ½åŠ›çš„çœŸå®-æ¨¡æ‹ŸéªŒè¯åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19562v1" data-paper-url="./papers/251219562v1-realm-a-real-to-sim-validated-benchmark-for-generalization-in-roboti.html" onclick="toggleFavorite(this, '2512.19562v1', 'REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251219148v1-a-flexible-field-based-policy-learning-framework-for-diverse-robotic.html">A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors</a></td>
  <td>æå‡ºåŸºäºåœºä¿¡æ¯çš„æŸ”æ€§ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œå®ç°è·¨æœºå™¨äººå’Œä¼ æ„Ÿå™¨çš„æ“ä½œæŠ€èƒ½æ³›åŒ–</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">teleoperation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19148v1" data-paper-url="./papers/251219148v1-a-flexible-field-based-policy-learning-framework-for-diverse-robotic.html" onclick="toggleFavorite(this, '2512.19148v1', 'A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251219583v1-learning-generalizable-hand-object-tracking-from-synthetic-demonstra.html">Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations</a></td>
  <td>æå‡ºHOP+HOTæ¡†æ¶ï¼Œä»…ç”¨åˆæˆæ•°æ®å­¦ä¹ é€šç”¨æ‰‹-ç‰©è·Ÿè¸ªæ§åˆ¶å™¨</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">dexterous manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19583v1" data-paper-url="./papers/251219583v1-learning-generalizable-hand-object-tracking-from-synthetic-demonstra.html" onclick="toggleFavorite(this, '2512.19583v1', 'Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251219576v1-lelar-the-first-in-orbit-demonstration-of-an-ai-based-satellite-atti.html">LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</a></td>
  <td>LeLaRé¦–æ¬¡åœ¨è½¨æ¼”ç¤ºåŸºäºAIçš„å«æ˜Ÿå§¿æ€æ§åˆ¶å™¨ï¼Œå…‹æœSim2Realéš¾é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19576v1" data-paper-url="./papers/251219576v1-lelar-the-first-in-orbit-demonstration-of-an-ai-based-satellite-atti.html" onclick="toggleFavorite(this, '2512.19576v1', 'LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251219564v1-results-of-the-2024-commonroad-motion-planning-competition-for-auton.html">Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles</a></td>
  <td>CommonRoadè‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’ç«èµ›ï¼šæ ‡å‡†åŒ–è¯„ä¼°å¤æ‚äº¤é€šåœºæ™¯ä¸‹çš„è§„åˆ’ç®—æ³•</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19564v1" data-paper-url="./papers/251219564v1-results-of-the-2024-commonroad-motion-planning-competition-for-auton.html" onclick="toggleFavorite(this, '2512.19564v1', 'Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251219347v1-omp-one-step-meanflow-policy-with-directional-alignment.html">OMP: One-step Meanflow Policy with Directional Alignment</a></td>
  <td>æå‡ºOMPï¼šä¸€ç§å•æ­¥MeanFlowç­–ç•¥ï¼Œé€šè¿‡æ–¹å‘å¯¹é½æå‡æœºå™¨äººæ“ä½œæ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19347v1" data-paper-url="./papers/251219347v1-omp-one-step-meanflow-policy-with-directional-alignment.html" onclick="toggleFavorite(this, '2512.19347v1', 'OMP: One-step Meanflow Policy with Directional Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251219269v1-translating-flow-to-policy-via-hindsight-online-imitation.html">Translating Flow to Policy via Hindsight Online Imitation</a></td>
  <td>æå‡ºHinFlowï¼Œé€šè¿‡å›æº¯åœ¨çº¿æ¨¡ä»¿å­¦ä¹ å°†é«˜å±‚è§„åˆ’è½¬åŒ–ä¸ºæœºå™¨äººç­–ç•¥</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">cross-embodiment</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19269v1" data-paper-url="./papers/251219269v1-translating-flow-to-policy-via-hindsight-online-imitation.html" onclick="toggleFavorite(this, '2512.19269v1', 'Translating Flow to Policy via Hindsight Online Imitation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251219402v1-real2edit2real-generating-robotic-demonstrations-via-a-3d-control-in.html">Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</a></td>
  <td>Real2Edit2Realï¼šé€šè¿‡3Dæ§åˆ¶ç•Œé¢ç”Ÿæˆæœºå™¨äººæ“ä½œæ¼”ç¤ºæ•°æ®ï¼Œæå‡æ•°æ®æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19402v1" data-paper-url="./papers/251219402v1-real2edit2real-generating-robotic-demonstrations-via-a-3d-control-in.html" onclick="toggleFavorite(this, '2512.19402v1', 'Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/251219083v1-codrone-autonomous-drone-navigation-assisted-by-edge-and-cloud-found.html">CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models</a></td>
  <td>CoDroneï¼šè¾¹ç¼˜äº‘ååŒï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹å¢å¼ºæ— äººæœºè‡ªä¸»å¯¼èˆªèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19083v1" data-paper-url="./papers/251219083v1-codrone-autonomous-drone-navigation-assisted-by-edge-and-cloud-found.html" onclick="toggleFavorite(this, '2512.19083v1', 'CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251219133v1-worldrft-latent-world-model-planning-with-reinforcement-fine-tuning-.html">WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</a></td>
  <td>WorldRFTï¼šé€šè¿‡å¼ºåŒ–å¾®è°ƒçš„æ½œåœ¨ä¸–ç•Œæ¨¡å‹è§„åˆ’ï¼Œæå‡è‡ªåŠ¨é©¾é©¶å®‰å…¨æ€§ä¸è§„åˆ’èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">world model</span> <span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19133v1" data-paper-url="./papers/251219133v1-worldrft-latent-world-model-planning-with-reinforcement-fine-tuning-.html" onclick="toggleFavorite(this, '2512.19133v1', 'WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251218988v1-dtccl-disengagement-triggered-contrastive-continual-learning-for-aut.html">DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners</a></td>
  <td>æå‡ºDTCCLæ¡†æ¶ï¼Œé€šè¿‡è„±ç¦»äº‹ä»¶è§¦å‘çš„å¯¹æ¯”æŒç»­å­¦ä¹ æå‡è‡ªåŠ¨é©¾é©¶å·´å£«è§„åˆ’ç­–ç•¥ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.18988v1" data-paper-url="./papers/251218988v1-dtccl-disengagement-triggered-contrastive-continual-learning-for-aut.html" onclick="toggleFavorite(this, '2512.18988v1', 'DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/251219024v1-indooruav-benchmarking-vision-language-uav-navigation-in-continuous-.html">IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments</a></td>
  <td>IndoorUAVï¼šæå‡ºå®¤å†…æ— äººæœºè§†è§‰-è¯­è¨€å¯¼èˆªåŸºå‡†ä¸æ–¹æ³•ï¼Œå¡«è¡¥ç›¸å…³ç ”ç©¶ç©ºç™½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span> <span class="paper-tag">VLA</span> <span class="paper-tag">VLN</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19024v1" data-paper-url="./papers/251219024v1-indooruav-benchmarking-vision-language-uav-navigation-in-continuous-.html" onclick="toggleFavorite(this, '2512.19024v1', 'IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251219010v1-palpaid-multimodal-pneumatic-tactile-sensor-for-tissue-palpation.html">PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation</a></td>
  <td>PalpAidï¼šç”¨äºç»„ç»‡è§¦è¯Šçš„å¤šæ¨¡æ€æ°”åŠ¨è§¦è§‰ä¼ æ„Ÿå™¨</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19010v1" data-paper-url="./papers/251219010v1-palpaid-multimodal-pneumatic-tactile-sensor-for-tissue-palpation.html" onclick="toggleFavorite(this, '2512.19010v1', 'PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251219453v1-map-avr-a-meta-action-planner-for-agents-leveraging-vision-language-.html">MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation</a></td>
  <td>MaP-AVRï¼šç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œä¸ºæœºå™¨äººæå‡ºå…ƒåŠ¨ä½œè§„åˆ’å™¨</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19453v1" data-paper-url="./papers/251219453v1-map-avr-a-meta-action-planner-for-agents-leveraging-vision-language-.html" onclick="toggleFavorite(this, '2512.19453v1', 'MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/251219629v1-logoplanner-localization-grounded-navigation-policy-with-metric-awar.html">LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</a></td>
  <td>LoGoPlannerï¼šåŸºäºåº¦é‡è§†è§‰å‡ ä½•çš„å®šä½å¼•å¯¼ç«¯åˆ°ç«¯å¯¼èˆªç­–ç•¥</td>
  <td class="tags-cell"><span class="paper-tag">cross-embodiment</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19629v1" data-paper-url="./papers/251219629v1-logoplanner-localization-grounded-navigation-policy-with-metric-awar.html" onclick="toggleFavorite(this, '2512.19629v1', 'LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251219178v1-vision-language-policy-model-for-dynamic-robot-task-planning.html">Vision-Language-Policy Model for Dynamic Robot Task Planning</a></td>
  <td>æå‡ºåŸºäºè§†è§‰-è¯­è¨€-ç­–ç•¥æ¨¡å‹çš„åŠ¨æ€æœºå™¨äººä»»åŠ¡è§„åˆ’æ–¹æ³•ï¼Œæå‡å¤æ‚ç¯å¢ƒä¸‹çš„è‡ªä¸»æ‰§è¡Œèƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">cross-embodiment</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19178v1" data-paper-url="./papers/251219178v1-vision-language-policy-model-for-dynamic-robot-task-planning.html" onclick="toggleFavorite(this, '2512.19178v1', 'Vision-Language-Policy Model for Dynamic Robot Task Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)