---
layout: default
title: CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human
---

# CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14889" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14889v1</a>
  <a href="https://arxiv.org/pdf/2509.14889.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14889v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14889v1', 'CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nan Sun, Yongchang Li, Chenxu Wang, Huiying Li, Huaping Liu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: 8 pages, 5 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CollabVLAï¼šæå‡ºè‡ªåæ€çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œå®ç°äººæœºååŒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `è‡ªåæ€` `äººæœºåä½œ` `æ‰©æ•£æ¨¡å‹` `æ··åˆä¸“å®¶` `è§†è§‰è¯­è¨€æ¨¡å‹` `æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰å­˜åœ¨é¢†åŸŸè¿‡æ‹Ÿåˆã€æ¨ç†è¿‡ç¨‹ä¸å¯è§£é‡Šä»¥åŠä¾èµ–é«˜å»¶è¿Ÿçš„ç”Ÿæˆæ¨¡å‹çš„ä¸è¶³ã€‚
2. CollabVLAé›†æˆäº†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„åæ€æ¨ç†å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„åŠ¨ä½œç”Ÿæˆï¼Œå¹¶é‡‡ç”¨æ··åˆä¸“å®¶è®¾è®¡ï¼Œæå‡äº†æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCollabVLAåœ¨æ—¶é—´æ•ˆç‡å’ŒDreamè®¡æ•°ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶æé«˜äº†æˆåŠŸç‡å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†CollabVLAï¼Œä¸€ä¸ªè‡ªåæ€çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œå°†æ ‡å‡†çš„è§†è§‰è¿åŠ¨ç­–ç•¥è½¬å˜ä¸ºåä½œåŠ©æ‰‹ã€‚CollabVLAé€šè¿‡åœ¨æ··åˆä¸“å®¶è®¾è®¡ä¸‹ï¼Œé›†æˆåŸºäºVLMçš„åæ€æ¨ç†å’ŒåŸºäºæ‰©æ•£çš„åŠ¨ä½œç”Ÿæˆï¼Œè§£å†³äº†ç°æœ‰VLAçš„å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬é¢†åŸŸè¿‡æ‹Ÿåˆã€ä¸å¯è§£é‡Šçš„æ¨ç†ä»¥åŠè¾…åŠ©ç”Ÿæˆæ¨¡å‹çš„é«˜å»¶è¿Ÿã€‚é€šè¿‡åŠ¨ä½œ grounding å’Œåæ€è°ƒä¼˜çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼ŒCollabVLAæ”¯æŒæ˜¾å¼çš„è‡ªæˆ‘åæ€ï¼Œå¹¶åœ¨é¢å¯¹ä¸ç¡®å®šæ€§æˆ–é‡å¤å¤±è´¥æ—¶ä¸»åŠ¨å¾æ±‚äººç±»æŒ‡å¯¼ã€‚ä¸ç”Ÿæˆå¼æ™ºèƒ½ä½“ç›¸æ¯”ï¼ŒCollabVLAå°†æ ‡å‡†åŒ–æ—¶é—´ç¼©çŸ­äº†çº¦2å€ï¼ŒDreamè®¡æ•°å‡å°‘äº†çº¦4å€ï¼Œå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ã€æ”¹è¿›çš„å¯è§£é‡Šæ€§ä»¥åŠå¹³è¡¡çš„ä½å»¶è¿Ÿã€‚è¿™é¡¹å·¥ä½œæœç€å°†VLAä»ä¸é€æ˜çš„æ§åˆ¶å™¨è½¬å˜ä¸ºçœŸæ­£èƒ½å¤Ÿæ¨ç†ã€è¡ŒåŠ¨å¹¶ä¸äººç±»åä½œçš„è¾…åŠ©æ™ºèƒ½ä½“è¿ˆå‡ºäº†å¼€åˆ›æ€§çš„ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼šä¸€æ˜¯é¢†åŸŸè¿‡æ‹Ÿåˆï¼Œæ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°æ–°çš„ç¯å¢ƒï¼›äºŒæ˜¯æ¨ç†è¿‡ç¨‹ä¸é€æ˜ï¼Œéš¾ä»¥ç†è§£æ¨¡å‹åšå‡ºå†³ç­–çš„åŸå› ï¼›ä¸‰æ˜¯ä¾èµ–è¾…åŠ©ç”Ÿæˆæ¨¡å‹ï¼Œå¯¼è‡´å»¶è¿Ÿè¾ƒé«˜ï¼Œå½±å“å®æ—¶æ€§ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†VLAåœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCollabVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†VLAè½¬å˜ä¸ºä¸€ä¸ªèƒ½å¤Ÿä¸äººç±»åä½œçš„æ™ºèƒ½åŠ©æ‰‹ã€‚ä¸ºæ­¤ï¼Œæ¨¡å‹éœ€è¦å…·å¤‡è‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨é‡åˆ°å›°éš¾æ—¶ä¸»åŠ¨å¯»æ±‚äººç±»çš„æŒ‡å¯¼ã€‚é€šè¿‡ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ‰©æ•£æ¨¡å‹çš„åŠ¨ä½œç”Ÿæˆèƒ½åŠ›ï¼ŒCollabVLAèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£å†³å¤æ‚ä»»åŠ¡ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„å†³ç­–è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCollabVLAçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰æ„ŸçŸ¥æ¨¡å—ï¼Œç”¨äºä»ç¯å¢ƒä¸­æå–è§†è§‰ä¿¡æ¯ï¼›2) è¯­è¨€ç†è§£æ¨¡å—ï¼Œç”¨äºç†è§£äººç±»çš„æŒ‡ä»¤å’Œåé¦ˆï¼›3) åæ€æ¨ç†æ¨¡å—ï¼ŒåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œåˆ†æå½“å‰çŠ¶æ€å¹¶è¯„ä¼°è¡ŒåŠ¨çš„æœ‰æ•ˆæ€§ï¼›4) åŠ¨ä½œç”Ÿæˆæ¨¡å—ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹ç”Ÿæˆå…·ä½“çš„åŠ¨ä½œæŒ‡ä»¤ï¼›5) æ··åˆä¸“å®¶æ¨¡å—ï¼Œç”¨äºæ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åˆé€‚çš„ä¸“å®¶æ¨¡å—è¿›è¡Œå¤„ç†ã€‚è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè¿›è¡ŒåŠ¨ä½œ groundingï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£åŠ¨ä½œä¸ç¯å¢ƒä¹‹é—´çš„å…³ç³»ï¼›ç„¶åè¿›è¡Œåæ€è°ƒä¼˜ï¼Œä½¿æ¨¡å‹å…·å¤‡è‡ªæˆ‘åæ€å’Œå¯»æ±‚äººç±»æŒ‡å¯¼çš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šCollabVLAæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå…¶è‡ªåæ€èƒ½åŠ›ã€‚é€šè¿‡é›†æˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒCollabVLAèƒ½å¤Ÿå¯¹è‡ªèº«çš„è¡Œä¸ºè¿›è¡Œè¯„ä¼°ï¼Œå¹¶åœ¨é‡åˆ°å›°éš¾æ—¶ä¸»åŠ¨å¯»æ±‚äººç±»çš„å¸®åŠ©ã€‚è¿™ç§è‡ªåæ€èƒ½åŠ›ä½¿å¾—CollabVLAèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚ç¯å¢ƒï¼Œå¹¶æä¾›æ›´å¯é çš„å†³ç­–ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCollabVLAä¸å†æ˜¯ä¸€ä¸ªç®€å•çš„æ§åˆ¶å™¨ï¼Œè€Œæ˜¯ä¸€ä¸ªèƒ½å¤Ÿä¸äººç±»åä½œçš„æ™ºèƒ½åŠ©æ‰‹ã€‚

**å…³é”®è®¾è®¡**ï¼šCollabVLAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ··åˆä¸“å®¶æ¨¡å—çš„è®¾è®¡ï¼Œç”¨äºæ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åˆé€‚çš„ä¸“å®¶æ¨¡å—ï¼›2) åŸºäºæ‰©æ•£æ¨¡å‹çš„åŠ¨ä½œç”Ÿæˆæ¨¡å—ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´è‡ªç„¶ã€æ›´æµç•…çš„åŠ¨ä½œï¼›3) ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé¦–å…ˆè¿›è¡ŒåŠ¨ä½œ groundingï¼Œç„¶åè¿›è¡Œåæ€è°ƒä¼˜ï¼›4) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç”¨äºé¼“åŠ±æ¨¡å‹è¿›è¡Œè‡ªæˆ‘åæ€å’Œå¯»æ±‚äººç±»æŒ‡å¯¼ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCollabVLAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç”Ÿæˆå¼æ™ºèƒ½ä½“ç›¸æ¯”ï¼ŒCollabVLAå°†æ ‡å‡†åŒ–æ—¶é—´ç¼©çŸ­äº†çº¦2å€ï¼ŒDreamè®¡æ•°å‡å°‘äº†çº¦4å€ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„æˆåŠŸç‡å’Œæ”¹è¿›çš„å¯è§£é‡Šæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒCollabVLAåœ¨æ•ˆç‡ã€å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CollabVLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼šæ™ºèƒ½å®¶å±…åŠ©æ‰‹ã€å·¥ä¸šæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚å®ƒå¯ä»¥å¸®åŠ©äººç±»å®Œæˆå„ç§å¤æ‚ä»»åŠ¡ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œç”Ÿæ´»è´¨é‡ã€‚é€šè¿‡ä¸äººç±»çš„åä½œï¼ŒCollabVLAå¯ä»¥ä¸æ–­å­¦ä¹ å’Œæ”¹è¿›ï¼Œæœ€ç»ˆæˆä¸ºä¸€ä¸ªçœŸæ­£æ™ºèƒ½çš„åŠ©æ‰‹ã€‚æœªæ¥ï¼ŒCollabVLAæœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œä¸ºäººç±»å¸¦æ¥æ›´å¤§çš„ä¾¿åˆ©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we present CollabVLA, a self-reflective vision-language-action framework that transforms a standard visuomotor policy into a collaborative assistant. CollabVLA tackles key limitations of prior VLAs, including domain overfitting, non-interpretable reasoning, and the high latency of auxiliary generative models, by integrating VLM-based reflective reasoning with diffusion-based action generation under a mixture-of-experts design. Through a two-stage training recipe of action grounding and reflection tuning, it supports explicit self-reflection and proactively solicits human guidance when confronted with uncertainty or repeated failure. It cuts normalized Time by ~2x and Dream counts by ~4x vs. generative agents, achieving higher success rates, improved interpretability, and balanced low latency compared with existing methods. This work takes a pioneering step toward shifting VLAs from opaque controllers to genuinely assistive agents capable of reasoning, acting, and collaborating with humans.

