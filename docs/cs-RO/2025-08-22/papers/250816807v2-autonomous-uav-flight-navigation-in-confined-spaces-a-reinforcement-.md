---
layout: default
title: Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach
---

# Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16807" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.16807v2</a>
  <a href="https://arxiv.org/pdf/2508.16807.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16807v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16807v2', 'Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Marco S. Tayar, Lucas K. de Oliveira, Felipe Andrade G. Tommaselli, Juliano D. Negri, Thiago H. Segreto, Ricardo V. Godoy, Marcelo Becker

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-08-22 (æ›´æ–°: 2025-10-11)

**æœŸåˆŠ**: 2025 Latin American Robotics Symposium (LARS)

**DOI**: [10.1109/LARS69345.2025.11273007](https://doi.org/10.1109/LARS69345.2025.11273007)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ— äººæœºè‡ªä¸»é£è¡Œå¯¼èˆªæ–¹æ³•ä»¥è§£å†³ç‹­å°ç©ºé—´ä¸­çš„ç¢°æ’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ— äººæœºå¯¼èˆª` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–` `è½¯æ¼”å‘˜è¯„è®ºå®¶` `å®‰å…¨å…³é”®ä»»åŠ¡` `ç¢°æ’é¿å…` `å·¥ä¸šæ£€æŸ¥` `ç¨‹åºåŒ–ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ— äººæœºå¯¼èˆªæ–¹æ³•åœ¨ç‹­å°ç©ºé—´ä¸­é¢ä¸´ç¢°æ’é£é™©ï¼Œç¼ºä¹æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ¯”è¾ƒPPOå’ŒSACä¸¤ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ¢ç´¢åœ¨é«˜ç²¾åº¦å¯¼èˆªä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPPOèƒ½å¤Ÿç¨³å®šå­¦ä¹ åˆ°å®Œæ•´çš„æ— ç¢°æ’ç­–ç•¥ï¼Œè€ŒSACåˆ™æœªèƒ½æˆåŠŸå®Œæˆä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶é’ˆå¯¹æ— äººæœºåœ¨ç‹­å°å·¥ä¸šåŸºç¡€è®¾æ–½ï¼ˆå¦‚é€šé£ç®¡é“ï¼‰ä¸­çš„è‡ªä¸»æ£€æŸ¥ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å¯¼èˆªç­–ç•¥ã€‚ç ”ç©¶æ¯”è¾ƒäº†ä¸¤ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œè½¯æ¼”å‘˜è¯„è®ºå®¶ï¼ˆSACï¼‰ï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬åœ¨é«˜ç²¾åº¦é£è¡Œä¸­çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒPPOèƒ½å¤Ÿç¨³å®šå­¦ä¹ åˆ°æ— ç¢°æ’çš„å¯¼èˆªç­–ç•¥ï¼Œè€ŒSACåˆ™æœªèƒ½æ‰¾åˆ°å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œæœ€ç»ˆæ”¶æ•›åˆ°æ¬¡ä¼˜ç­–ç•¥ã€‚è¿™ä¸€ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å®‰å…¨å…³é”®çš„å¯¼èˆªä»»åŠ¡ä¸­ï¼Œç¨³å®šæ”¶æ•›çš„ç­–ç•¥æ¯”æ ·æœ¬æ•ˆç‡æ›´ä¸ºé‡è¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æ— äººæœºåœ¨ç‹­å°ç©ºé—´ä¸­è‡ªä¸»é£è¡Œå¯¼èˆªæ—¶çš„ç¢°æ’é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é«˜é£é™©ç¯å¢ƒä¸­å¾€å¾€æ— æ³•ä¿è¯ç¨³å®šæ€§å’Œå®‰å…¨æ€§ï¼Œå¯¼è‡´å¯¼èˆªå¤±è´¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡æ¯”è¾ƒä¸¤ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆPPOå’ŒSACï¼‰ï¼Œæ¢è®¨åœ¨é«˜ç²¾åº¦å’Œå®‰å…¨å…³é”®çš„å¯¼èˆªä»»åŠ¡ä¸­ï¼Œå“ªç§ç®—æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ åˆ°æ— ç¢°æ’çš„å¯¼èˆªç­–ç•¥ã€‚é€‰æ‹©PPOä½œä¸ºå¯¹æ¯”çš„åŸå› åœ¨äºå…¶åœ¨è®­ç»ƒç¨³å®šæ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä½¿ç”¨é«˜ä¿çœŸæ¨¡æ‹Ÿå™¨ç”Ÿæˆç¨‹åºåŒ–çš„é€šé£ç®¡é“ç¯å¢ƒï¼ŒPPOå’ŒSACç®—æ³•åˆ†åˆ«åœ¨æ­¤ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ç¯å¢ƒå»ºæ¨¡ã€ç®—æ³•è®­ç»ƒã€ç­–ç•¥è¯„ä¼°ç­‰é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†PPOå’ŒSACåœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¼ºè°ƒäº†åœ¨å®‰å…¨å…³é”®ä»»åŠ¡ä¸­ï¼Œç¨³å®šæ”¶æ•›çš„é‡è¦æ€§ã€‚ä¸ä»¥å¾€ç ”ç©¶ä¸åŒï¼Œæœ¬ç ”ç©¶æä¾›äº†æ˜ç¡®çš„å®è¯æ•°æ®æ”¯æŒè¿™ä¸€è§‚ç‚¹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼ŒPPOé‡‡ç”¨äº†ç‰¹å®šçš„è¶…å‚æ•°è®¾ç½®ä»¥ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œè€ŒSACåˆ™ä¾§é‡äºæ ·æœ¬æ•ˆç‡ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿæœ‰æ‰€ä¸åŒï¼ŒPPOä½¿ç”¨äº†å‰ªåˆ‡æŸå¤±å‡½æ•°ä»¥é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPPOç®—æ³•åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æˆåŠŸå­¦ä¹ åˆ°ç¨³å®šçš„æ— ç¢°æ’ç­–ç•¥ï¼Œèƒ½å¤Ÿå®Œæˆæ•´ä¸ªè¯¾ç¨‹ï¼Œè€ŒSACç®—æ³•åˆ™æœªèƒ½æ‰¾åˆ°å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œä»…åœ¨åˆå§‹é˜¶æ®µè¡¨ç°è‰¯å¥½ï¼Œæœ€ç»ˆæ”¶æ•›åˆ°æ¬¡ä¼˜ç­–ç•¥ã€‚è¿™è¡¨æ˜PPOåœ¨é«˜ç²¾åº¦å¯¼èˆªä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨å®‰å…¨å…³é”®çš„åº”ç”¨åœºæ™¯ä¸­ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æˆæœå¯å¹¿æ³›åº”ç”¨äºå·¥ä¸šé¢†åŸŸçš„æ— äººæœºè‡ªä¸»æ£€æŸ¥ï¼Œå°¤å…¶æ˜¯åœ¨ç‹­å°å’Œå¤æ‚ç¯å¢ƒä¸­ï¼Œå¦‚é€šé£ç®¡é“ã€ä»“åº“å’Œå…¶ä»–å·¥ä¸šè®¾æ–½ã€‚é€šè¿‡æé«˜æ— äººæœºçš„å¯¼èˆªç²¾åº¦å’Œå®‰å…¨æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½äººå·¥æ£€æŸ¥çš„é£é™©å’Œæˆæœ¬ï¼Œæå‡å·¥ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œå¦‚ç¾åæ•‘æ´å’Œç¯å¢ƒç›‘æµ‹ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous UAV inspection of confined industrial infrastructure, such as ventilation ducts, demands robust navigation policies where collisions are unacceptable. While Deep Reinforcement Learning (DRL) offers a powerful paradigm for developing such policies, it presents a critical trade-off between on-policy and off-policy algorithms. Off-policy methods promise high sample efficiency, a vital trait for minimizing costly and unsafe real-world fine-tuning. In contrast, on-policy methods often exhibit greater training stability, which is essential for reliable convergence in hazard-dense environments. This paper directly investigates this trade-off by comparing a leading on-policy algorithm, Proximal Policy Optimization (PPO), against an off-policy counterpart, Soft Actor-Critic (SAC), for precision flight in procedurally generated ducts within a high-fidelity simulator. Our results show that PPO consistently learned a stable, collision-free policy that completed the entire course. In contrast, SAC failed to find a complete solution, converging to a suboptimal policy that navigated only the initial segments before failure. This work provides evidence that for high-precision, safety-critical navigation tasks, the reliable convergence of a well-established on-policy method can be more decisive than the nominal sample efficiency of an off-policy algorithm.

