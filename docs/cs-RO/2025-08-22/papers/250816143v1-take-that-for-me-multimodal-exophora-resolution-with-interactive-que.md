---
layout: default
title: Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions
---

# Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16143" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.16143v1</a>
  <a href="https://arxiv.org/pdf/2508.16143.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16143v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16143v1', 'Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Akira Oyama, Shoichi Hasegawa, Akira Taniguchi, Yoshinobu Hagiwara, Tadahiro Taniguchi

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-22

**Â§áÊ≥®**: See website at https://emergentsystemlabstudent.github.io/MIEL/. Accepted at IEEE RO-MAN 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://emergentsystemlabstudent.github.io/MIEL/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â§öÊ®°ÊÄÅ‰∫§‰∫íÂºèÂ§ñÊåá‰ª£Ëß£ÊûêÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥Ê®°Á≥äÊåá‰ª§ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ñÊåá‰ª£Ëß£Êûê` `Â§öÊ®°ÊÄÅ‰∫§‰∫í` `Â£∞Èü≥Ê∫êÂÆö‰Ωç` `ËØ≠‰πâÊò†Â∞Ñ` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Êú∫Âô®‰∫∫ÊäÄÊúØ` `Áî®Êà∑‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§ñÊåá‰ª£Ëß£ÊûêÊñπÊ≥ïÂú®Áî®Êà∑ÊàñÂØπË±°‰∏çÂèØËßÅÁöÑÊÉÖÂÜµ‰∏ãÔºåÊó†Ê≥ïÊúâÊïàÁêÜËß£Ê®°Á≥äÁöÑËØ≠Ë®ÄÊåá‰ª§ÔºåÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑMIELÊ°ÜÊû∂ÈÄöËøáÁªìÂêàÂ£∞Èü≥Ê∫êÂÆö‰Ωç„ÄÅËØ≠‰πâÊò†Â∞ÑÂíå‰∫§‰∫íÂºèÊèêÈóÆÔºåËÉΩÂ§üÂú®Áî®Êà∑‰∏çÂèØËßÅÊó∂‰ªçÁÑ∂ÂáÜÁ°ÆËß£ÊûêÊåá‰ª§„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMIELÂú®Áî®Êà∑ÂèØËßÅÊó∂ÊÄßËÉΩÊèêÂçáÁ∫¶1.3ÂÄçÔºåÂú®Áî®Êà∑‰∏çÂèØËßÅÊó∂ÊèêÂçáËææ2.0ÂÄçÔºåÊòæËëó‰ºò‰∫é‰º†ÁªüÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êó•Â∏∏ÁîüÊ¥ªÊîØÊåÅÊú∫Âô®‰∫∫ÂøÖÈ°ªËÉΩÂ§üÁêÜËß£Ê®°Á≥äÁöÑËØ≠Ë®ÄÊåá‰ª§Ôºå‰æãÂ¶Ç‚ÄúÊääÈÇ£‰∏™ÊùØÂ≠êÁªôÊàë‚ÄùÔºåÂç≥‰ΩøÂØπË±°ÊàñÁî®Êà∑‰∏çÂú®Êú∫Âô®‰∫∫ÁöÑËßÜÈáéÂÜÖ„ÄÇÁé∞ÊúâÁöÑÂ§ñÊåá‰ª£Ëß£ÊûêÊñπÊ≥ï‰∏ªË¶Å‰æùËµñËßÜËßâÊï∞ÊçÆÔºåÂõ†Ê≠§Âú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÔºåÂΩìÂØπË±°ÊàñÁî®Êà∑‰∏çÂèØËßÅÊó∂ÔºåÊïàÊûú‰∏ç‰Ω≥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÊ®°ÊÄÅ‰∫§‰∫íÂºèÂ§ñÊåá‰ª£Ëß£ÊûêÊ°ÜÊû∂ÔºàMIELÔºâÔºåËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÂ£∞Èü≥Ê∫êÂÆö‰Ωç„ÄÅËØ≠‰πâÊò†Â∞Ñ„ÄÅËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂíåÂü∫‰∫éGPT-4oÁöÑ‰∫§‰∫íÂºèÊèêÈóÆ„ÄÇÈÄöËøáÊûÑÂª∫ÁéØÂ¢ÉÁöÑËØ≠‰πâÂú∞ÂõæÂíåÂà©Áî®Áî®Êà∑ÁöÑÈ™®È™ºÊï∞ÊçÆÊù•‰º∞ËÆ°ÂÄôÈÄâÂØπË±°ÔºåMIELËÉΩÂ§üÊúâÊïàÂú∞ËØÜÂà´Áî®Êà∑ÁöÑÊâãÂäøÂíåÊåáÂêëÊñπÂêë„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂΩìÁî®Êà∑ÂèØËßÅÊó∂ÔºåÊÄßËÉΩÊèêÂçáÁ∫¶1.3ÂÄçÔºåËÄåÂΩìÁî®Êà∑‰∏çÂèØËßÅÊó∂ÔºåÊèêÂçáËææ2.0ÂÄç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êó•Â∏∏ÁîüÊ¥ªÊîØÊåÅÊú∫Âô®‰∫∫Âú®Áî®Êà∑ÊàñÂØπË±°‰∏çÂèØËßÅÊó∂ÔºåÊó†Ê≥ïÊúâÊïàËß£ÊûêÊ®°Á≥äËØ≠Ë®ÄÊåá‰ª§ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñËßÜËßâ‰ø°ÊÅØÔºåÂØºËá¥Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMIELÊ°ÜÊû∂ÈÄöËøáÁªìÂêàÂ£∞Èü≥Ê∫êÂÆö‰Ωç„ÄÅËØ≠‰πâÊò†Â∞ÑÂíå‰∫§‰∫íÂºèÊèêÈóÆÔºåËÉΩÂ§üÂú®Áî®Êà∑‰∏çÂèØËßÅÊó∂‰ªçÁÑ∂ÂáÜÁ°ÆËØÜÂà´Êåá‰ª§„ÄÇËØ•ËÆæËÆ°Êó®Âú®ÊèêÈ´òÊú∫Âô®‰∫∫ÂØπÊ®°Á≥äÊåá‰ª§ÁöÑÁêÜËß£ËÉΩÂäõÔºåÂ¢ûÂº∫ÂÖ∂Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMIELÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Âõõ‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËØ≠‰πâÂú∞ÂõæÊûÑÂª∫Ôºå2) Áî®Êà∑ÂÆö‰Ωç‰∏éÂÄôÈÄâÂØπË±°‰º∞ËÆ°Ôºå3) Â£∞Èü≥Ê∫êÂÆö‰ΩçÔºå4) ‰∫§‰∫íÂºèÊèêÈóÆ„ÄÇÈÄöËøáËøô‰∫õÊ®°ÂùóÁöÑÂçèÂêåÂ∑•‰ΩúÔºåÊú∫Âô®‰∫∫ËÉΩÂ§üÊúâÊïàËß£ÊûêÊ®°Á≥äÊåá‰ª§„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMIELÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÁªìÂêàÂ£∞Èü≥Ê∫êÂÆö‰Ωç‰∏é‰∫§‰∫íÂºèÊèêÈóÆÔºåËÉΩÂ§üÂú®Áî®Êà∑‰∏çÂèØËßÅÁöÑÊÉÖÂÜµ‰∏ã‰∏ªÂä®‰∏éÁî®Êà∑‰∫íÂä®ÔºåÊèêÂá∫ÊæÑÊ∏ÖÊÄßÈóÆÈ¢òÔºå‰ªéËÄåÊèêÈ´òËß£ÊûêÁöÑÂáÜÁ°ÆÊÄß„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†Áªü‰æùËµñËßÜËßâÁöÑËß£ÊûêÊñπÊ≥ïÊú¨Ë¥®‰∏ä‰∏çÂêå„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊäÄÊúØÁªÜËäÇ‰∏äÔºåMIEL‰ΩøÁî®‰∫ÜÁî®Êà∑ÁöÑÈ™®È™ºÊï∞ÊçÆËøõË°åÂØπË±°‰º∞ËÆ°ÔºåÂπ∂ÈÄöËøáGPT-4oÁîüÊàêÊæÑÊ∏ÖÊÄßÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåÂ£∞Èü≥Ê∫êÂÆö‰ΩçÊäÄÊúØÁöÑÂ∫îÁî®‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®Áî®Êà∑‰∏çÂú®ËßÜÈáéÂÜÖÊó∂Ôºå‰æùÁÑ∂ËÉΩÂ§üÂáÜÁ°ÆËØÜÂà´Áî®Êà∑ÁöÑÊâãÂäøÂíåÊåáÂêëÊñπÂêë„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMIELÂú®Áî®Êà∑ÂèØËßÅÊó∂ÁöÑÊÄßËÉΩÊèêÂçáÁ∫¶1.3ÂÄçÔºåËÄåÂú®Áî®Êà∑‰∏çÂèØËßÅÊó∂ÁöÑÊèêÂçáËææ2.0ÂÄçÔºåÊòæËëó‰ºò‰∫éÊú™‰ΩøÁî®Â£∞Èü≥Ê∫êÂÆö‰ΩçÂíå‰∫§‰∫íÂºèÊèêÈóÆÁöÑ‰º†ÁªüÊñπÊ≥ï„ÄÇËøô‰∏ÄÁªìÊûúÈ™åËØÅ‰∫ÜMIELÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂåªÁñóËæÖÂä©Êú∫Âô®‰∫∫‰ª•ÂèäÊô∫ËÉΩÂÆ∂Â±ÖÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫ÂØπÊ®°Á≥äÊåá‰ª§ÁöÑÁêÜËß£ËÉΩÂäõÔºåMIELËÉΩÂ§üÊòæËëóÊèêÂçáÁî®Êà∑‰ΩìÈ™åÔºåÊé®Âä®Êú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂÆûÈôÖÂ∫îÁî®ÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÁ§æ‰ºö‰ª∑ÂÄºÂíåÂΩ±ÂìçÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Daily life support robots must interpret ambiguous verbal instructions involving demonstratives such as ``Bring me that cup,'' even when objects or users are out of the robot's view. Existing approaches to exophora resolution primarily rely on visual data and thus fail in real-world scenarios where the object or user is not visible. We propose Multimodal Interactive Exophora resolution with user Localization (MIEL), which is a multimodal exophora resolution framework leveraging sound source localization (SSL), semantic mapping, visual-language models (VLMs), and interactive questioning with GPT-4o. Our approach first constructs a semantic map of the environment and estimates candidate objects from a linguistic query with the user's skeletal data. SSL is utilized to orient the robot toward users who are initially outside its visual field, enabling accurate identification of user gestures and pointing directions. When ambiguities remain, the robot proactively interacts with the user, employing GPT-4o to formulate clarifying questions. Experiments in a real-world environment showed results that were approximately 1.3 times better when the user was visible to the robot and 2.0 times better when the user was not visible to the robot, compared to the methods without SSL and interactive questioning. The project website is https://emergentsystemlabstudent.github.io/MIEL/.

