---
layout: default
title: OG-VLA: Orthographic Image Generation for 3D-Aware Vision-Language Action Model
---

# OG-VLA: Orthographic Image Generation for 3D-Aware Vision-Language Action Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.01196" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.01196v2</a>
  <a href="https://arxiv.org/pdf/2506.01196.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.01196v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.01196v2', 'OG-VLA: Orthographic Image Generation for 3D-Aware Vision-Language Action Model')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ishika Singh, Ankit Goyal, Stan Birchfield, Dieter Fox, Animesh Garg, Valts Blukis

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-01 (Êõ¥Êñ∞: 2025-11-18)

**Â§áÊ≥®**: 13 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫OG-VLA‰ª•Ëß£ÂÜ≥3DÊÑüÁü•‰∏éËØ≠Ë®ÄÊåá‰ª§Êò†Â∞ÑÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `3DÊÑüÁü•` `Êú∫Âô®‰∫∫Êìç‰Ωú` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `ÁÇπ‰∫ëÊ∏≤Êüì` `Ê≥õÂåñËÉΩÂäõ` `Êô∫ËÉΩÁ≥ªÁªü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑ3DÊÑüÁü•Êú∫Âô®‰∫∫Á≠ñÁï•Âú®Èù¢ÂØπÊú™ËßÅÊåá‰ª§ÂíåÂú∫ÊôØÊó∂Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®ËåÉÂõ¥„ÄÇ
2. OG-VLAÈÄöËøáÂ∞ÜËæìÂÖ•ËßÇÂØüÂèçÊäïÂΩ±‰∏∫ÁÇπ‰∫ëÂπ∂‰ªéÊ≠£‰∫§ËßÜÂõæÊ∏≤ÊüìÔºåÁªìÂêàËßÜËßâÂíåËØ≠Ë®ÄÊ®°ÂûãÔºåÊèêÂçá‰∫Ü3DÊÑüÁü•Á≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
3. Âú®ArnoldÂíåColosseumÂü∫ÂáÜÊµãËØï‰∏≠ÔºåOG-VLAÂ±ïÁ§∫‰∫ÜË∂ÖËøá40%ÁöÑÁõ∏ÂØπÊèêÂçáÔºåÂêåÊó∂Âú®Â∑≤ËßÅÁéØÂ¢É‰∏≠‰øùÊåÅ‰∫ÜÁ®≥ÂÅ•ÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êàë‰ª¨‰ªãÁªç‰∫ÜOG-VLAÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊû∂ÊûÑÂíåÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁªìÂêà‰∫ÜËßÜËßâËØ≠Ë®ÄË°åÂä®Ê®°ÂûãÔºàVLAÔºâÁöÑÊ≥õÂåñËÉΩÂäõ‰∏é3DÊÑüÁü•Á≠ñÁï•ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÂ∞ÜËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§‰∏éRGBDËßÇÂØüÊò†Â∞ÑÂà∞ÂáÜÈùôÊÄÅÊú∫Âô®‰∫∫Âä®‰ΩúÁöÑÊåëÊàò„ÄÇÂ∞ΩÁÆ°3DÊÑüÁü•Êú∫Âô®‰∫∫Á≠ñÁï•Âú®Á≤æÁ°ÆÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Èù¢ÂØπÊú™ËßÅÊåá‰ª§„ÄÅÂú∫ÊôØÂíåÁâ©‰ΩìÊó∂Âç¥Èöæ‰ª•Ê≥õÂåñ„ÄÇÁõ∏ÂèçÔºåVLAÂú®Êåá‰ª§ÂíåÂú∫ÊôØÁöÑÊ≥õÂåñ‰∏äË°®Áé∞‰ºòÂºÇÔºå‰ΩÜÂØπÁõ∏Êú∫ÂíåÊú∫Âô®‰∫∫ÂßøÊÄÅÂèòÂåñÊïèÊÑü„ÄÇÊàë‰ª¨Âà©Áî®ËØ≠Ë®ÄÂíåËßÜËßâÂü∫Á°ÄÊ®°Âûã‰∏≠ÂµåÂÖ•ÁöÑÂÖàÈ™åÁü•ËØÜÊù•ÊèêÈ´ò3DÊÑüÁü•ÂÖ≥ÈîÆÂ∏ßÁ≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇOG-VLAÂ∞ÜÊù•Ëá™‰∏çÂêåËßÜËßíÁöÑËæìÂÖ•ËßÇÂØüÂèçÊäïÂΩ±‰∏∫ÁÇπ‰∫ëÔºåÁÑ∂Âêé‰ªéËßÑËåÉÊ≠£‰∫§ËßÜÂõæÊ∏≤ÊüìÔºåÁ°Æ‰øùËæìÂÖ•ËßÜÂõæÁöÑ‰∏çÂèòÊÄßÂíåËæìÂÖ•ËæìÂá∫Á©∫Èó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåÂú®ArnoldÂíåColosseumÂü∫ÂáÜ‰∏äÔºåOG-VLAÂú®Êú™ËßÅÁéØÂ¢É‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõËææÂà∞‰∫ÜÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºåÁõ∏ÂØπÊèêÂçáË∂ÖËøá40%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â∞ÜËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§‰∏éRGBDËßÇÂØüÊò†Â∞ÑÂà∞ÂáÜÈùôÊÄÅÊú∫Âô®‰∫∫Âä®‰ΩúÁöÑÊåëÊàò„ÄÇÁé∞ÊúâÁöÑ3DÊÑüÁü•Êú∫Âô®‰∫∫Á≠ñÁï•Âú®Êú™ËßÅÊåá‰ª§ÂíåÂú∫ÊôØÁöÑÊ≥õÂåñËÉΩÂäõ‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåËÄåËßÜËßâËØ≠Ë®ÄË°åÂä®Ê®°ÂûãÂú®Êåá‰ª§Ê≥õÂåñ‰∏äË°®Áé∞‰ºòÂºÇ‰ΩÜÂØπÁéØÂ¢ÉÂèòÂåñÊïèÊÑü„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöOG-VLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂ∞ÜËæìÂÖ•ËßÇÂØüÂèçÊäïÂΩ±‰∏∫ÁÇπ‰∫ëÂπ∂‰ªéËßÑËåÉÊ≠£‰∫§ËßÜÂõæÊ∏≤ÊüìÔºåÂà©Áî®ËØ≠Ë®ÄÂíåËßÜËßâÊ®°ÂûãÁöÑÂÖàÈ™åÁü•ËØÜÊù•ÊèêÂçá3DÊÑüÁü•Á≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËøôÁßçËÆæËÆ°Á°Æ‰øù‰∫ÜËæìÂÖ•ËßÜÂõæÁöÑ‰∏çÂèòÊÄßÂíåËæìÂÖ•ËæìÂá∫Á©∫Èó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöOG-VLAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Â§ö‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÔºåÂ∞ÜRGBDËßÇÂØüÂèçÊäïÂΩ±‰∏∫ÁÇπ‰∫ëÔºõÂÖ∂Ê¨°Ôºå‰ªéËßÑËåÉÊ≠£‰∫§ËßÜÂõæÊ∏≤ÊüìËøô‰∫õÁÇπ‰∫ëÔºõÁÑ∂ÂêéÔºå‰ΩøÁî®ËßÜËßâÈ™®Âπ≤ÁΩëÁªú„ÄÅ‰∏Ä‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂíåÂõæÂÉèÊâ©Êï£Ê®°ÂûãÁîüÊàêÁºñÁ†ÅÊú´Á´ØÊâßË°åÂô®‰∏ã‰∏Ä‰∏™‰ΩçÁΩÆÂíåÊñπÂêëÁöÑÂõæÂÉè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöOG-VLAÁöÑÊúÄÈáçË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Â∞Ü3DÊÑüÁü•‰∏éËØ≠Ë®ÄÊ®°ÂûãÊúâÊïàÁªìÂêàÔºåÂÖãÊúç‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂú®Ê≥õÂåñËÉΩÂäõÂíåÁéØÂ¢ÉÈÄÇÂ∫îÊÄß‰∏äÁöÑ‰∏çË∂≥„ÄÇËøôÁßçÊñπÊ≥ïÂú®ËæìÂÖ•ËßÜÂõæ‰∏çÂèòÊÄßÂíåËæìÂá∫‰∏ÄËá¥ÊÄßÊñπÈù¢ÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåOG-VLAÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÁîüÊàêÂõæÂÉèÁöÑË¥®ÈáèÔºåÂπ∂ÈÄöËøáË∞ÉÊï¥ÁΩëÁªúÁªìÊûÑ‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑËæìÂÖ•ËßÜËßíÂíåÁéØÂ¢ÉÂèòÂåñÔºåÁ°Æ‰øù‰∫ÜÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

OG-VLAÂú®ArnoldÂíåColosseumÂü∫ÂáÜÊµãËØï‰∏≠Â±ïÁ§∫‰∫ÜË∂ÖËøá40%ÁöÑÁõ∏ÂØπÊèêÂçáÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Êú™ËßÅÁéØÂ¢É‰∏≠ÁöÑ‰ºòË∂äÊ≥õÂåñËÉΩÂäõÔºåÂêåÊó∂Âú®Â∑≤ËßÅÁéØÂ¢É‰∏≠‰øùÊåÅ‰∫ÜÁ®≥ÂÅ•ÁöÑÊÄßËÉΩÔºåÊòæÁ§∫Âá∫ËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫Êìç‰Ωú„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÂíåËá™Âä®ÂåñÂà∂ÈÄ†Á≠â„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫ÂØπËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÁöÑÁêÜËß£ÂíåÊâßË°åËÉΩÂäõÔºåOG-VLAÂèØ‰ª•Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥È´òÊïàÁöÑ‰ªªÂä°ÊâßË°åÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and one or more RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA unprojects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/

