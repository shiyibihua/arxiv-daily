---
layout: default
title: ViPRA: Video Prediction for Robot Actions
---

# ViPRA: Video Prediction for Robot Actions

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.07732" target="_blank" class="toolbar-btn">arXiv: 2511.07732v1</a>
    <a href="https://arxiv.org/pdf/2511.07732.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.07732v1" 
            onclick="toggleFavorite(this, '2511.07732v1', 'ViPRA: Video Prediction for Robot Actions')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Sandeep Routray, Hengkai Pan, Unnat Jain, Shikhar Bahl, Deepak Pathak

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.CL, cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-11

**Â§áÊ≥®**: Website: https://vipra-project.github.io

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ViPRAÔºöÂà©Áî®ËßÜÈ¢ëÈ¢ÑÊµãÊ®°ÂûãÂ≠¶‰π†Êú∫Âô®‰∫∫Âä®‰ΩúÊéßÂà∂Á≠ñÁï•**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÈ¢ÑÊµã` `Êú∫Âô®‰∫∫Âä®‰Ωú` `Êó†ÁõëÁù£Â≠¶‰π†` `ÊΩúÂú®Âä®‰Ωú` `ËøûÁª≠ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫Â≠¶‰π†ÊñπÊ≥ï‰æùËµñÂ§ßÈáèÂ∏¶Ê†áÁ≠æÊï∞ÊçÆÔºåËÄå‰∫∫Á±ªÊàñÈÅ•Êìç‰ΩúÊú∫Âô®‰∫∫ÁöÑËßÜÈ¢ëÁº∫‰πèÂä®‰ΩúÊ†áÁ≠æÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®„ÄÇ
2. ViPRAÈÄöËøáËßÜÈ¢ëÈ¢ÑÊµãÊ®°ÂûãÂ≠¶‰π†Âú∫ÊôØÂä®ÊÄÅÁöÑÊΩúÂú®Âä®‰ΩúË°®Á§∫ÔºåÂπ∂Âà©Áî®ÂàÜÂùóÊµÅÂåπÈÖçËß£Á†ÅÂô®ÁîüÊàêÊú∫Âô®‰∫∫ÊéßÂà∂Êåá‰ª§„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåViPRAÂú®SIMPLERÂü∫ÂáÜÂíåÁúüÂÆûÊìç‰Ωú‰ªªÂä°‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ËßÜÈ¢ëÈ¢ÑÊµãÊú∫Âô®‰∫∫Âä®‰ΩúÔºàViPRAÔºâÁöÑÈ¢ÑËÆ≠ÁªÉ-ÂæÆË∞ÉÊ°ÜÊû∂ÔºåÊó®Âú®‰ªéÊó†Âä®‰ΩúÊ†áÁ≠æÁöÑËßÜÈ¢ë‰∏≠Â≠¶‰π†ËøûÁª≠ÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂Á≠ñÁï•„ÄÇËØ•ÊñπÊ≥ïËÆ≠ÁªÉ‰∏Ä‰∏™ËßÜÈ¢ë-ËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÈ¢ÑÊµãÊú™Êù•ÁöÑËßÜËßâËßÇÊµãÂíå‰ª•ËøêÂä®‰∏∫‰∏≠ÂøÉÁöÑÊΩúÂú®Âä®‰ΩúÔºåËøô‰∫õÊΩúÂú®Âä®‰Ωú‰Ωú‰∏∫Âú∫ÊôØÂä®ÊÄÅÁöÑ‰∏≠Èó¥Ë°®Á§∫„ÄÇÂà©Áî®ÊÑüÁü•ÊçüÂ§±ÂíåÂÖâÊµÅ‰∏ÄËá¥ÊÄßÊù•ËÆ≠ÁªÉËøô‰∫õÊΩúÂú®Âä®‰ΩúÔºåÁ°Æ‰øùÂÖ∂ÂèçÊò†Áâ©ÁêÜ‰∏äÂêàÁêÜÁöÑË°å‰∏∫„ÄÇÂú®‰∏ãÊ∏∏ÊéßÂà∂‰∏≠ÔºåÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂàÜÂùóÊµÅÂåπÈÖçËß£Á†ÅÂô®Ôºå‰ªÖ‰ΩøÁî®100Âà∞200‰∏™ÈÅ•Êìç‰ΩúÊºîÁ§∫ÔºåÂ∞ÜÊΩúÂú®Âä®‰ΩúÊò†Â∞ÑÂà∞Êú∫Âô®‰∫∫ÁâπÂÆöÁöÑËøûÁª≠Âä®‰ΩúÂ∫èÂàó„ÄÇËØ•ÊñπÊ≥ïÈÅøÂÖç‰∫ÜÊòÇË¥µÁöÑÂä®‰ΩúÊ†áÊ≥®ÔºåÊîØÊåÅË∑®‰∏çÂêåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÁöÑÊ≥õÂåñÔºåÂπ∂ÈÄöËøáÂàÜÂùóÂä®‰ΩúËß£Á†ÅÂÆûÁé∞È´òËææ22 HzÁöÑÂπ≥Êªë„ÄÅÈ´òÈ¢ëËøûÁª≠ÊéßÂà∂„ÄÇ‰∏éÂÖàÂâçÂ∞ÜÈ¢ÑËÆ≠ÁªÉËßÜ‰∏∫Ëá™ÂõûÂΩíÁ≠ñÁï•Â≠¶‰π†ÁöÑÊΩúÂú®Âä®‰ΩúÂ∑•‰Ωú‰∏çÂêåÔºåViPRAÊòæÂºèÂú∞Âª∫Ê®°‰∫ÜÂèòÂåñÁöÑÂÜÖÂÆπÂíåÊñπÂºè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ï‰ºò‰∫éÂº∫Â§ßÁöÑÂü∫Á∫øÊñπÊ≥ïÔºåÂú®SIMPLERÂü∫ÂáÜÊµãËØï‰∏≠Ëé∑Âæó‰∫Ü16%ÁöÑÊèêÂçáÔºåÂú®ÁúüÂÆû‰∏ñÁïåÊìç‰Ωú‰ªªÂä°‰∏≠Ëé∑Âæó‰∫Ü13%ÁöÑÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊú∫Âô®‰∫∫Â≠¶‰π†ÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑÂ∏¶Ê†áÁ≠æÊï∞ÊçÆÔºåËÄåËé∑ÂèñËøô‰∫õÊï∞ÊçÆÊàêÊú¨È´òÊòÇ„ÄÇÂè¶‰∏ÄÊñπÈù¢Ôºå‰∫íËÅîÁΩë‰∏äÂ≠òÂú®Â§ßÈáèÁöÑÊó†Ê†áÁ≠æËßÜÈ¢ëÔºå‰æãÂ¶Ç‰∫∫Á±ªÊìç‰ΩúÊàñÈÅ•Êìç‰ΩúÊú∫Âô®‰∫∫ÁöÑËßÜÈ¢ëÔºåËøô‰∫õËßÜÈ¢ëËï¥Âê´‰∏∞ÂØåÁöÑÁâ©ÁêÜ‰∫§‰∫í‰ø°ÊÅØÔºå‰ΩÜÁî±‰∫éÁº∫‰πèÂä®‰ΩúÊ†áÁ≠æÔºåÈöæ‰ª•Áõ¥Êé•Áî®‰∫éÊú∫Âô®‰∫∫Â≠¶‰π†„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂà©Áî®Ëøô‰∫õÊó†Ê†áÁ≠æËßÜÈ¢ëÊù•Â≠¶‰π†ÊúâÊïàÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂Á≠ñÁï•ÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöViPRAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËßÜÈ¢ëÈ¢ÑÊµãÊ®°ÂûãÂ≠¶‰π†Âú∫ÊôØÂä®ÊÄÅÁöÑÊΩúÂú®Âä®‰ΩúË°®Á§∫„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåËØ•ÊñπÊ≥ïËÆ≠ÁªÉ‰∏Ä‰∏™ËßÜÈ¢ë-ËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÈ¢ÑÊµãÊú™Êù•ÁöÑËßÜËßâËßÇÊµãÂíå‰ª•ËøêÂä®‰∏∫‰∏≠ÂøÉÁöÑÊΩúÂú®Âä®‰Ωú„ÄÇËøô‰∫õÊΩúÂú®Âä®‰Ωú‰Ωú‰∏∫Âú∫ÊôØÂä®ÊÄÅÁöÑ‰∏≠Èó¥Ë°®Á§∫ÔºåËÉΩÂ§üÊçïÊçâËßÜÈ¢ë‰∏≠ÁöÑÁâ©ÁêÜ‰∫§‰∫í‰ø°ÊÅØÔºå‰ªéËÄåÈÅøÂÖç‰∫ÜÁõ¥Êé•È¢ÑÊµãÂä®‰ΩúÁöÑÂõ∞Èöæ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöViPRAÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨È¢ÑËÆ≠ÁªÉÂíåÂæÆË∞É‰∏§‰∏™Èò∂ÊÆµ„ÄÇÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºå‰ΩøÁî®Â§ßÈáèÁöÑÊó†Ê†áÁ≠æËßÜÈ¢ëËÆ≠ÁªÉ‰∏Ä‰∏™ËßÜÈ¢ë-ËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÈ¢ÑÊµãÊú™Êù•ÁöÑËßÜËßâËßÇÊµãÂíåÊΩúÂú®Âä®‰Ωú„ÄÇÂú®ÂæÆË∞ÉÈò∂ÊÆµÔºå‰ΩøÁî®Â∞ëÈáèÁöÑÂ∏¶Ê†áÁ≠æÊï∞ÊçÆÔºà‰æãÂ¶Ç100-200‰∏™ÈÅ•Êìç‰ΩúÊºîÁ§∫ÔºâËÆ≠ÁªÉ‰∏Ä‰∏™ÂàÜÂùóÊµÅÂåπÈÖçËß£Á†ÅÂô®ÔºåÂ∞ÜÊΩúÂú®Âä®‰ΩúÊò†Â∞ÑÂà∞Êú∫Âô®‰∫∫ÁâπÂÆöÁöÑËøûÁª≠Âä®‰ΩúÂ∫èÂàó„ÄÇËØ•Ëß£Á†ÅÂô®Â∞ÜÊΩúÂú®Âä®‰ΩúÂ∫èÂàóÂàÜÂâ≤ÊàêÂ§ö‰∏™chunkÔºåÁÑ∂ÂêéÂàÜÂà´È¢ÑÊµãÊØè‰∏™chunkÂØπÂ∫îÁöÑÊú∫Âô®‰∫∫Âä®‰ΩúÂ∫èÂàó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöViPRAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊòæÂºèÂú∞Âª∫Ê®°‰∫ÜÂèòÂåñÁöÑÂÜÖÂÆπÂíåÊñπÂºè„ÄÇ‰∏éÂÖàÂâçÂ∞ÜÈ¢ÑËÆ≠ÁªÉËßÜ‰∏∫Ëá™ÂõûÂΩíÁ≠ñÁï•Â≠¶‰π†ÁöÑÊΩúÂú®Âä®‰ΩúÂ∑•‰Ωú‰∏çÂêåÔºåViPRA‰∏ç‰ªÖÈ¢ÑÊµã‰∫ÜÊú™Êù•ÁöÑËßÜËßâËßÇÊµãÔºàÂç≥ÂèòÂåñÁöÑÂÜÖÂÆπÔºâÔºåËøòÈ¢ÑÊµã‰∫Ü‰ª•ËøêÂä®‰∏∫‰∏≠ÂøÉÁöÑÊΩúÂú®Âä®‰ΩúÔºàÂç≥ÂèòÂåñÁöÑÊñπÂºèÔºâ„ÄÇËøôÁßçÊòæÂºèÁöÑÂª∫Ê®°ÊñπÂºè‰ΩøÂæóViPRAËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâËßÜÈ¢ë‰∏≠ÁöÑÁâ©ÁêÜ‰∫§‰∫í‰ø°ÊÅØÔºå‰ªéËÄåÂ≠¶‰π†Âà∞Êõ¥ÊúâÊïàÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂Á≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöViPRAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢Ôºö1) ‰ΩøÁî®ÊÑüÁü•ÊçüÂ§±ÂíåÂÖâÊµÅ‰∏ÄËá¥ÊÄßÊù•ËÆ≠ÁªÉÊΩúÂú®Âä®‰ΩúÔºåÁ°Æ‰øùÂÖ∂ÂèçÊò†Áâ©ÁêÜ‰∏äÂêàÁêÜÁöÑË°å‰∏∫„ÄÇ2) ÂºïÂÖ•ÂàÜÂùóÊµÅÂåπÈÖçËß£Á†ÅÂô®ÔºåÂ∞ÜÊΩúÂú®Âä®‰ΩúÊò†Â∞ÑÂà∞Êú∫Âô®‰∫∫ÁâπÂÆöÁöÑËøûÁª≠Âä®‰ΩúÂ∫èÂàóÔºåÂÆûÁé∞È´òÈ¢ëËøûÁª≠ÊéßÂà∂„ÄÇ3) ‰ΩøÁî®ËßÜÈ¢ë-ËØ≠Ë®ÄÊ®°ÂûãÔºåÂ∞ÜËßÜËßâ‰ø°ÊÅØÂíåËØ≠Ë®Ä‰ø°ÊÅØÁªìÂêàËµ∑Êù•ÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ViPRAÂú®SIMPLERÂü∫ÂáÜÊµãËØï‰∏≠Ëé∑Âæó‰∫Ü16%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂú®ÁúüÂÆû‰∏ñÁïåÊìç‰Ωú‰ªªÂä°‰∏≠Ëé∑Âæó‰∫Ü13%ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇËøô‰∫õÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåViPRAËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Êó†Ê†áÁ≠æËßÜÈ¢ëÊï∞ÊçÆÂ≠¶‰π†Êú∫Âô®‰∫∫ÊéßÂà∂Á≠ñÁï•ÔºåÂπ∂‰∏îÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåViPRAÈÄöËøáÂàÜÂùóÂä®‰ΩúËß£Á†ÅÂÆûÁé∞‰∫ÜÈ´òËææ22 HzÁöÑÂπ≥Êªë„ÄÅÈ´òÈ¢ëËøûÁª≠ÊéßÂà∂„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ViPRAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÊú∫Âô®‰∫∫„ÄÅÂåªÁñóÊú∫Âô®‰∫∫Á≠â„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•Âà©Áî®Â§ßÈáèÁöÑÊó†Ê†áÁ≠æËßÜÈ¢ëÊï∞ÊçÆÔºåÈôç‰ΩéÊú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊàêÊú¨ÔºåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇÊ≠§Â§ñÔºåViPRAËøòÂèØ‰ª•Â∫îÁî®‰∫éËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÁ≠âÈ¢ÜÂüüÔºåÁîüÊàêÊõ¥Âä†ÈÄºÁúüÁöÑÂä®ÁîªÊïàÊûú„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io

