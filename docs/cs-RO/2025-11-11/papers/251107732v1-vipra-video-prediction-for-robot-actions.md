---
layout: default
title: ViPRA: Video Prediction for Robot Actions
---

# ViPRA: Video Prediction for Robot Actions

**arXiv**: [2511.07732v1](https://arxiv.org/abs/2511.07732) | [PDF](https://arxiv.org/pdf/2511.07732.pdf)

**ä½œè€…**: Sandeep Routray, Hengkai Pan, Unnat Jain, Shikhar Bahl, Deepak Pathak

**åˆ†ç±»**: cs.RO, cs.AI, cs.CL, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

**å¤‡æ³¨**: Website: https://vipra-project.github.io

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ViPRAï¼šåˆ©ç”¨è§†é¢‘é¢„æµ‹æ¨¡åž‹å­¦ä¹ æœºå™¨äººåŠ¨ä½œæŽ§åˆ¶ç­–ç•¥**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `è§†é¢‘é¢„æµ‹` `æœºå™¨äººåŠ¨ä½œ` `æ— ç›‘ç£å­¦ä¹ ` `æ½œåœ¨åŠ¨ä½œ` `è¿žç»­æŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æœºå™¨äººå­¦ä¹ æ–¹æ³•ä¾èµ–å¤§é‡å¸¦æ ‡ç­¾æ•°æ®ï¼Œè€Œäººç±»æˆ–é¥æ“ä½œæœºå™¨äººçš„è§†é¢‘ç¼ºä¹åŠ¨ä½œæ ‡ç­¾ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. ViPRAé€šè¿‡è§†é¢‘é¢„æµ‹æ¨¡åž‹å­¦ä¹ åœºæ™¯åŠ¨æ€çš„æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨åˆ†å—æµåŒ¹é…è§£ç å™¨ç”Ÿæˆæœºå™¨äººæŽ§åˆ¶æŒ‡ä»¤ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒViPRAåœ¨SIMPLERåŸºå‡†å’ŒçœŸå®žæ“ä½œä»»åŠ¡ä¸­å‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå®žçŽ°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè§†é¢‘é¢„æµ‹æœºå™¨äººåŠ¨ä½œï¼ˆViPRAï¼‰çš„é¢„è®­ç»ƒ-å¾®è°ƒæ¡†æž¶ï¼Œæ—¨åœ¨ä»Žæ— åŠ¨ä½œæ ‡ç­¾çš„è§†é¢‘ä¸­å­¦ä¹ è¿žç»­çš„æœºå™¨äººæŽ§åˆ¶ç­–ç•¥ã€‚è¯¥æ–¹æ³•è®­ç»ƒä¸€ä¸ªè§†é¢‘-è¯­è¨€æ¨¡åž‹ï¼Œä½¿å…¶èƒ½å¤Ÿé¢„æµ‹æœªæ¥çš„è§†è§‰è§‚æµ‹å’Œä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„æ½œåœ¨åŠ¨ä½œï¼Œè¿™äº›æ½œåœ¨åŠ¨ä½œä½œä¸ºåœºæ™¯åŠ¨æ€çš„ä¸­é—´è¡¨ç¤ºã€‚åˆ©ç”¨æ„ŸçŸ¥æŸå¤±å’Œå…‰æµä¸€è‡´æ€§æ¥è®­ç»ƒè¿™äº›æ½œåœ¨åŠ¨ä½œï¼Œç¡®ä¿å…¶åæ˜ ç‰©ç†ä¸Šåˆç†çš„è¡Œä¸ºã€‚åœ¨ä¸‹æ¸¸æŽ§åˆ¶ä¸­ï¼Œå¼•å…¥äº†ä¸€ç§åˆ†å—æµåŒ¹é…è§£ç å™¨ï¼Œä»…ä½¿ç”¨100åˆ°200ä¸ªé¥æ“ä½œæ¼”ç¤ºï¼Œå°†æ½œåœ¨åŠ¨ä½œæ˜ å°„åˆ°æœºå™¨äººç‰¹å®šçš„è¿žç»­åŠ¨ä½œåºåˆ—ã€‚è¯¥æ–¹æ³•é¿å…äº†æ˜‚è´µçš„åŠ¨ä½œæ ‡æ³¨ï¼Œæ”¯æŒè·¨ä¸åŒæœºå™¨äººå½¢æ€çš„æ³›åŒ–ï¼Œå¹¶é€šè¿‡åˆ†å—åŠ¨ä½œè§£ç å®žçŽ°é«˜è¾¾22 Hzçš„å¹³æ»‘ã€é«˜é¢‘è¿žç»­æŽ§åˆ¶ã€‚ä¸Žå…ˆå‰å°†é¢„è®­ç»ƒè§†ä¸ºè‡ªå›žå½’ç­–ç•¥å­¦ä¹ çš„æ½œåœ¨åŠ¨ä½œå·¥ä½œä¸åŒï¼ŒViPRAæ˜¾å¼åœ°å»ºæ¨¡äº†å˜åŒ–çš„å†…å®¹å’Œæ–¹å¼ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•ä¼˜äºŽå¼ºå¤§çš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨SIMPLERåŸºå‡†æµ‹è¯•ä¸­èŽ·å¾—äº†16%çš„æå‡ï¼Œåœ¨çœŸå®žä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­èŽ·å¾—äº†13%çš„æå‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æœºå™¨äººå­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„å¸¦æ ‡ç­¾æ•°æ®ï¼Œè€ŒèŽ·å–è¿™äº›æ•°æ®æˆæœ¬é«˜æ˜‚ã€‚å¦ä¸€æ–¹é¢ï¼Œäº’è”ç½‘ä¸Šå­˜åœ¨å¤§é‡çš„æ— æ ‡ç­¾è§†é¢‘ï¼Œä¾‹å¦‚äººç±»æ“ä½œæˆ–é¥æ“ä½œæœºå™¨äººçš„è§†é¢‘ï¼Œè¿™äº›è§†é¢‘è•´å«ä¸°å¯Œçš„ç‰©ç†äº¤äº’ä¿¡æ¯ï¼Œä½†ç”±äºŽç¼ºä¹åŠ¨ä½œæ ‡ç­¾ï¼Œéš¾ä»¥ç›´æŽ¥ç”¨äºŽæœºå™¨äººå­¦ä¹ ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨è¿™äº›æ— æ ‡ç­¾è§†é¢‘æ¥å­¦ä¹ æœ‰æ•ˆçš„æœºå™¨äººæŽ§åˆ¶ç­–ç•¥æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šViPRAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†é¢‘é¢„æµ‹æ¨¡åž‹å­¦ä¹ åœºæ™¯åŠ¨æ€çš„æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•è®­ç»ƒä¸€ä¸ªè§†é¢‘-è¯­è¨€æ¨¡åž‹ï¼Œä½¿å…¶èƒ½å¤Ÿé¢„æµ‹æœªæ¥çš„è§†è§‰è§‚æµ‹å’Œä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„æ½œåœ¨åŠ¨ä½œã€‚è¿™äº›æ½œåœ¨åŠ¨ä½œä½œä¸ºåœºæ™¯åŠ¨æ€çš„ä¸­é—´è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ•æ‰è§†é¢‘ä¸­çš„ç‰©ç†äº¤äº’ä¿¡æ¯ï¼Œä»Žè€Œé¿å…äº†ç›´æŽ¥é¢„æµ‹åŠ¨ä½œçš„å›°éš¾ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šViPRAçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨å¤§é‡çš„æ— æ ‡ç­¾è§†é¢‘è®­ç»ƒä¸€ä¸ªè§†é¢‘-è¯­è¨€æ¨¡åž‹ï¼Œä½¿å…¶èƒ½å¤Ÿé¢„æµ‹æœªæ¥çš„è§†è§‰è§‚æµ‹å’Œæ½œåœ¨åŠ¨ä½œã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨å°‘é‡çš„å¸¦æ ‡ç­¾æ•°æ®ï¼ˆä¾‹å¦‚100-200ä¸ªé¥æ“ä½œæ¼”ç¤ºï¼‰è®­ç»ƒä¸€ä¸ªåˆ†å—æµåŒ¹é…è§£ç å™¨ï¼Œå°†æ½œåœ¨åŠ¨ä½œæ˜ å°„åˆ°æœºå™¨äººç‰¹å®šçš„è¿žç»­åŠ¨ä½œåºåˆ—ã€‚è¯¥è§£ç å™¨å°†æ½œåœ¨åŠ¨ä½œåºåˆ—åˆ†å‰²æˆå¤šä¸ªchunkï¼Œç„¶åŽåˆ†åˆ«é¢„æµ‹æ¯ä¸ªchunkå¯¹åº”çš„æœºå™¨äººåŠ¨ä½œåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šViPRAçš„å…³é”®åˆ›æ–°åœ¨äºŽæ˜¾å¼åœ°å»ºæ¨¡äº†å˜åŒ–çš„å†…å®¹å’Œæ–¹å¼ã€‚ä¸Žå…ˆå‰å°†é¢„è®­ç»ƒè§†ä¸ºè‡ªå›žå½’ç­–ç•¥å­¦ä¹ çš„æ½œåœ¨åŠ¨ä½œå·¥ä½œä¸åŒï¼ŒViPRAä¸ä»…é¢„æµ‹äº†æœªæ¥çš„è§†è§‰è§‚æµ‹ï¼ˆå³å˜åŒ–çš„å†…å®¹ï¼‰ï¼Œè¿˜é¢„æµ‹äº†ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„æ½œåœ¨åŠ¨ä½œï¼ˆå³å˜åŒ–çš„æ–¹å¼ï¼‰ã€‚è¿™ç§æ˜¾å¼çš„å»ºæ¨¡æ–¹å¼ä½¿å¾—ViPRAèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è§†é¢‘ä¸­çš„ç‰©ç†äº¤äº’ä¿¡æ¯ï¼Œä»Žè€Œå­¦ä¹ åˆ°æ›´æœ‰æ•ˆçš„æœºå™¨äººæŽ§åˆ¶ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šViPRAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) ä½¿ç”¨æ„ŸçŸ¥æŸå¤±å’Œå…‰æµä¸€è‡´æ€§æ¥è®­ç»ƒæ½œåœ¨åŠ¨ä½œï¼Œç¡®ä¿å…¶åæ˜ ç‰©ç†ä¸Šåˆç†çš„è¡Œä¸ºã€‚2) å¼•å…¥åˆ†å—æµåŒ¹é…è§£ç å™¨ï¼Œå°†æ½œåœ¨åŠ¨ä½œæ˜ å°„åˆ°æœºå™¨äººç‰¹å®šçš„è¿žç»­åŠ¨ä½œåºåˆ—ï¼Œå®žçŽ°é«˜é¢‘è¿žç»­æŽ§åˆ¶ã€‚3) ä½¿ç”¨è§†é¢‘-è¯­è¨€æ¨¡åž‹ï¼Œå°†è§†è§‰ä¿¡æ¯å’Œè¯­è¨€ä¿¡æ¯ç»“åˆèµ·æ¥ï¼Œæé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

ViPRAåœ¨SIMPLERåŸºå‡†æµ‹è¯•ä¸­èŽ·å¾—äº†16%çš„æ€§èƒ½æå‡ï¼Œåœ¨çœŸå®žä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­èŽ·å¾—äº†13%çš„æ€§èƒ½æå‡ã€‚è¿™äº›å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒViPRAèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ— æ ‡ç­¾è§†é¢‘æ•°æ®å­¦ä¹ æœºå™¨äººæŽ§åˆ¶ç­–ç•¥ï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒViPRAé€šè¿‡åˆ†å—åŠ¨ä½œè§£ç å®žçŽ°äº†é«˜è¾¾22 Hzçš„å¹³æ»‘ã€é«˜é¢‘è¿žç»­æŽ§åˆ¶ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

ViPRAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥åˆ©ç”¨å¤§é‡çš„æ— æ ‡ç­¾è§†é¢‘æ•°æ®ï¼Œé™ä½Žæœºå™¨äººå­¦ä¹ çš„æˆæœ¬ï¼Œæé«˜æœºå™¨äººçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æ­¤å¤–ï¼ŒViPRAè¿˜å¯ä»¥åº”ç”¨äºŽè™šæ‹ŸçŽ°å®žã€æ¸¸æˆç­‰é¢†åŸŸï¼Œç”Ÿæˆæ›´åŠ é€¼çœŸçš„åŠ¨ç”»æ•ˆæžœã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io

