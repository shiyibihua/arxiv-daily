---
layout: default
title: Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks
---

# Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.08086" target="_blank" class="toolbar-btn">arXiv: 2511.08086v2</a>
    <a href="https://arxiv.org/pdf/2511.08086.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08086v2" 
            onclick="toggleFavorite(this, '2511.08086v2', 'Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Muthukumar Pandaram, Jakob Hollenstein, David Drexel, Samuele Tosatto, Antonio RodrÃ­guez-SÃ¡nchez, Justus Piater

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11 (æ›´æ–°: 2025-11-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºæœºå™¨äººå¼ºåŒ–å­¦ä¹ ç¯å¢ƒåŠ¨æ€ç¨€ç–æ€§çš„æŒ‘æˆ˜ä¸ç‰¹æ€§ï¼Œä¸ºä¸–ç•Œæ¨¡å‹å­¦ä¹ æä¾›æ–°è§†è§’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸–ç•Œæ¨¡å‹` `åŠ¨æ€ç¨€ç–æ€§` `æœºå™¨äººå¼ºåŒ–å­¦ä¹ ` `MuJoCo` `å½’çº³åç½®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä¸–ç•Œæ¨¡å‹å­¦ä¹ æ–¹æ³•å¸¸ä¾èµ–äºåŠ¨æ€ç³»ç»Ÿçš„ç¨€ç–æ€§å‡è®¾ï¼Œä½†ç¼ºä¹å¯¹çœŸå®æœºå™¨äººç¯å¢ƒåŠ¨æ€ç‰¹æ€§çš„æ·±å…¥åˆ†æã€‚
2. æœ¬ç ”ç©¶é€šè¿‡åˆ†æ MuJoCo Playground ç¯å¢ƒï¼Œæ­ç¤ºäº†åŠ¨æ€ç¨€ç–æ€§å¹¶éå…¨å±€æ€§è´¨ï¼Œè€Œæ˜¯å±€éƒ¨ä¸”çŠ¶æ€ç›¸å…³çš„ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåº”è®¾è®¡æ›´ç¬¦åˆçœŸå®ç¯å¢ƒåŠ¨æ€ç‰¹æ€§çš„å½’çº³åç½®ï¼Œä»¥æå‡ä¸–ç•Œæ¨¡å‹å­¦ä¹ çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ è¯„ä¼°äº†æœºå™¨äººå¼ºåŒ–å­¦ä¹ ä¸­ä¸–ç•Œæ¨¡å‹å­¦ä¹ çš„å¸¸è§ç¨€ç–æ€§å‡è®¾ã€‚é€šè¿‡åˆ†æ MuJoCo Playground åŸºå‡†æµ‹è¯•å¥—ä»¶ä¸­çœŸå®ç¯å¢ƒçš„åŠ¨æ€ç‰¹æ€§ï¼Œæ¢ç©¶äº†ç¯å¢ƒåŠ¨æ€çš„å› æœå›¾æ˜¯å¦ç¨€ç–ã€è¿™ç§ç¨€ç–æ€§æ˜¯å¦ä¾èµ–äºçŠ¶æ€ï¼Œä»¥åŠå±€éƒ¨ç³»ç»ŸåŠ¨æ€æ˜¯å¦ç¨€ç–å˜åŒ–ã€‚ç»“æœè¡¨æ˜ï¼Œå…¨å±€ç¨€ç–æ€§å¾ˆå°‘è§ï¼Œä»»åŠ¡è¡¨ç°å‡ºå±€éƒ¨ã€çŠ¶æ€ç›¸å…³çš„ç¨€ç–æ€§ï¼Œå¹¶ä¸”è¿™ç§ç¨€ç–æ€§å‘ˆç°å‡ºç‹¬ç‰¹çš„ç»“æ„ï¼Œä¾‹å¦‚åœ¨æ¥è§¦äº‹ä»¶æœŸé—´ä»¥æ—¶é—´å±€éƒ¨åŒ–çš„ç°‡å½¢å¼å‡ºç°ï¼Œå¹¶å½±å“çŠ¶æ€ç»´åº¦çš„ç‰¹å®šå­é›†ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†åŠ¨æ€å­¦ä¹ ä¸­å¸¸è§çš„ç¨€ç–æ€§å…ˆéªŒå‡è®¾ï¼Œå¼ºè°ƒéœ€è¦åæ˜ çœŸå®ä¸–ç•ŒåŠ¨æ€çš„çŠ¶æ€ç›¸å…³ç¨€ç–æ€§ç»“æ„çš„å½’çº³åç½®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ä¸–ç•Œæ¨¡å‹å­¦ä¹ æ–¹æ³•é€šå¸¸å‡è®¾ç¯å¢ƒåŠ¨æ€å…·æœ‰å…¨å±€ç¨€ç–æ€§æˆ–æ—¶é—´ç¨€ç–æ€§ï¼Œå³æœªæ¥çš„çŠ¶æ€å˜é‡ä»…ä¾èµ–äºå½“å‰çŠ¶æ€å˜é‡çš„ä¸€ä¸ªå°å­é›†ï¼Œæˆ–è€…å±€éƒ¨åŠ¨æ€å˜åŒ–ç¨€ç–ä¸”çªç„¶ã€‚ç„¶è€Œï¼Œè¿™äº›å‡è®¾æ˜¯å¦é€‚ç”¨äºçœŸå®çš„æœºå™¨äººå¼ºåŒ–å­¦ä¹ ç¯å¢ƒå°šä¸æ˜ç¡®ã€‚ç°æœ‰æ–¹æ³•å¯èƒ½å› ä¸ºä½¿ç”¨äº†ä¸åˆé€‚çš„ç¨€ç–æ€§å…ˆéªŒè€Œé™åˆ¶äº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†æçœŸå®æœºå™¨äººå¼ºåŒ–å­¦ä¹ ç¯å¢ƒçš„åŠ¨æ€ç‰¹æ€§ï¼ŒéªŒè¯ç°æœ‰ç¨€ç–æ€§å‡è®¾çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºçœŸå®ç¯å¢ƒåŠ¨æ€çš„ç¨€ç–æ€§ç»“æ„ã€‚é€šè¿‡å¯¹ MuJoCo Playground ç¯å¢ƒçš„ ground-truth åŠ¨æ€è¿›è¡Œåˆ†æï¼Œæ¢ç©¶ç¯å¢ƒåŠ¨æ€çš„å› æœå›¾æ˜¯å¦ç¨€ç–ï¼Œè¿™ç§ç¨€ç–æ€§æ˜¯å¦ä¾èµ–äºçŠ¶æ€ï¼Œä»¥åŠå±€éƒ¨ç³»ç»ŸåŠ¨æ€æ˜¯å¦ç¨€ç–å˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) é€‰æ‹© MuJoCo Playground åŸºå‡†æµ‹è¯•å¥—ä»¶ä¸­çš„å¤šä¸ªæœºå™¨äººå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼›2) è·å–æ¯ä¸ªç¯å¢ƒçš„ ground-truth åŠ¨æ€æ•°æ®ï¼›3) åˆ†æç¯å¢ƒåŠ¨æ€çš„å› æœå›¾ï¼Œè¯„ä¼°å…¶ç¨€ç–æ€§ï¼›4) åˆ†æç¨€ç–æ€§ä¸çŠ¶æ€çš„ä¾èµ–å…³ç³»ï¼›5) åˆ†æå±€éƒ¨ç³»ç»ŸåŠ¨æ€çš„å˜åŒ–æƒ…å†µï¼Œè¯„ä¼°å…¶æ—¶é—´ç¨€ç–æ€§ã€‚ç ”ç©¶ä½¿ç”¨äº†ç°æœ‰çš„å›¾åˆ†æå’Œæ—¶é—´åºåˆ—åˆ†ææ–¹æ³•æ¥å®Œæˆä¸Šè¿°æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹æœºå™¨äººå¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­åŠ¨æ€ç¨€ç–æ€§çš„æ·±å…¥åˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œå…¨å±€ç¨€ç–æ€§å¾ˆå°‘è§ï¼Œä»»åŠ¡è¡¨ç°å‡ºå±€éƒ¨ã€çŠ¶æ€ç›¸å…³çš„ç¨€ç–æ€§ï¼Œå¹¶ä¸”è¿™ç§ç¨€ç–æ€§å‘ˆç°å‡ºç‹¬ç‰¹çš„ç»“æ„ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†åŠ¨æ€å­¦ä¹ ä¸­å¸¸è§çš„ç¨€ç–æ€§å…ˆéªŒå‡è®¾ï¼Œä¸ºè®¾è®¡æ›´ç¬¦åˆçœŸå®ç¯å¢ƒåŠ¨æ€ç‰¹æ€§çš„å½’çº³åç½®æä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šæœ¬ç ”ç©¶çš„å…³é”®è®¾è®¡åœ¨äºé€‰æ‹© MuJoCo Playground ä½œä¸ºåˆ†æå¯¹è±¡ï¼Œå› ä¸ºå®ƒæä¾›äº† ground-truth çš„åŠ¨æ€ä¿¡æ¯ï¼Œé¿å…äº†ä»æ•°æ®ä¸­å­¦ä¹ åŠ¨æ€æ¨¡å‹å¸¦æ¥çš„è¯¯å·®ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ä½¿ç”¨äº†å¤šç§åˆ†ææ–¹æ³•æ¥è¯„ä¼°ç¨€ç–æ€§ï¼ŒåŒ…æ‹¬å›¾åˆ†æã€çŠ¶æ€ä¾èµ–æ€§åˆ†æå’Œæ—¶é—´åºåˆ—åˆ†æã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºæ‰€ä½¿ç”¨çš„åˆ†ææ–¹æ³•ï¼Œè®ºæ–‡ä¸­æœªè¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå…¨å±€ç¨€ç–æ€§åœ¨ MuJoCo Playground ç¯å¢ƒä¸­å¾ˆå°‘è§ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯å±€éƒ¨ã€çŠ¶æ€ç›¸å…³çš„ç¨€ç–æ€§ã€‚è¿™ç§ç¨€ç–æ€§å‘ˆç°å‡ºç‹¬ç‰¹çš„ç»“æ„ï¼Œä¾‹å¦‚åœ¨æ¥è§¦äº‹ä»¶æœŸé—´ä»¥æ—¶é—´å±€éƒ¨åŒ–çš„ç°‡å½¢å¼å‡ºç°ï¼Œå¹¶å½±å“çŠ¶æ€ç»´åº¦çš„ç‰¹å®šå­é›†ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†ç°æœ‰æ–¹æ³•ä¸­å¸¸ç”¨çš„å…¨å±€ç¨€ç–æ€§å‡è®¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¼ºåŒ–å­¦ä¹ é¢†åŸŸï¼Œé€šè¿‡è®¾è®¡æ›´ç¬¦åˆçœŸå®ç¯å¢ƒåŠ¨æ€ç‰¹æ€§çš„å½’çº³åç½®ï¼Œæå‡ä¸–ç•Œæ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥è®¾è®¡çŠ¶æ€ç›¸å…³çš„ç¨€ç–è¿æ¥ç½‘ç»œï¼Œæˆ–è€…ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥åŠ¨æ€åœ°é€‰æ‹©é‡è¦çš„çŠ¶æ€å˜é‡ã€‚è¿™æœ‰åŠ©äºæœºå™¨äººæ›´å¥½åœ°ç†è§£å’Œé¢„æµ‹ç¯å¢ƒåŠ¨æ€ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ§åˆ¶å’Œå†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The use of learned dynamics models, also known as world models, can improve the sample efficiency of reinforcement learning. Recent work suggests that the underlying causal graphs of such dynamics models are sparsely connected, with each of the future state variables depending only on a small subset of the current state variables, and that learning may therefore benefit from sparsity priors. Similarly, temporal sparsity, i.e. sparsely and abruptly changing local dynamics, has also been proposed as a useful inductive bias.
>   In this work, we critically examine these assumptions by analyzing ground-truth dynamics from a set of robotic reinforcement learning environments in the MuJoCo Playground benchmark suite, aiming to determine whether the proposed notions of state and temporal sparsity actually tend to hold in typical reinforcement learning tasks.
>   We study (i) whether the causal graphs of environment dynamics are sparse, (ii) whether such sparsity is state-dependent, and (iii) whether local system dynamics change sparsely.
>   Our results indicate that global sparsity is rare, but instead the tasks show local, state-dependent sparsity in their dynamics and this sparsity exhibits distinct structures, appearing in temporally localized clusters (e.g., during contact events) and affecting specific subsets of state dimensions. These findings challenge common sparsity prior assumptions in dynamics learning, emphasizing the need for grounded inductive biases that reflect the state-dependent sparsity structure of real-world dynamics.

