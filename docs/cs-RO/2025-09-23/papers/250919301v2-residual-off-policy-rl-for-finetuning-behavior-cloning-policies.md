---
layout: default
title: Residual Off-Policy RL for Finetuning Behavior Cloning Policies
---

# Residual Off-Policy RL for Finetuning Behavior Cloning Policies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19301" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19301v2</a>
  <a href="https://arxiv.org/pdf/2509.19301.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19301v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19301v2', 'Residual Off-Policy RL for Finetuning Behavior Cloning Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lars Ankile, Zhenyu Jiang, Rocky Duan, Guanya Shi, Pieter Abbeel, Anusha Nagabandi

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-23 (æ›´æ–°: 2025-09-25)

**å¤‡æ³¨**: Project website: https://residual-offpolicy-rl.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ®‹å·®ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå¾®è°ƒè¡Œä¸ºå…‹éš†ç­–ç•¥ï¼Œå®ç°é«˜è‡ªç”±åº¦æœºå™¨äººçµå·§æ“ä½œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ®‹å·®å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `è¡Œä¸ºå…‹éš†` `æœºå™¨äººæ“ä½œ` `ç¦»çº¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¡Œä¸ºå…‹éš†æ–¹æ³•ä¾èµ–é«˜è´¨é‡äººå·¥æ•°æ®ï¼Œä¸”æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œç¦»çº¿æ•°æ®åˆ©ç”¨ç‡å­˜åœ¨ç“¶é¢ˆã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§æ®‹å·®å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨è¡Œä¸ºå…‹éš†ç­–ç•¥ä½œä¸ºåŸºç¡€ï¼Œé€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ å­¦ä¹ æ®‹å·®æ ¡æ­£ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»…éœ€ç¨€ç–å¥–åŠ±ä¿¡å·ï¼Œå³å¯æœ‰æ•ˆæå‡é«˜è‡ªç”±åº¦æœºå™¨äººçš„æ“ä½œç­–ç•¥ï¼Œå¹¶åœ¨çœŸå®äººå½¢æœºå™¨äººä¸Šå–å¾—æˆåŠŸã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¡Œä¸ºå…‹éš†(BC)çš„æœ€æ–°è¿›å±•å·²ç»å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è§†è§‰è¿åŠ¨æ§åˆ¶ç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å—åˆ°äººç±»æ¼”ç¤ºè´¨é‡ã€æ•°æ®æ”¶é›†æ‰€éœ€çš„äººå·¥ä»¥åŠç¦»çº¿æ•°æ®æ”¶ç›Šé€’å‡çš„é™åˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ (RL)é€šè¿‡ä¸ç¯å¢ƒçš„è‡ªä¸»äº¤äº’æ¥è®­ç»ƒæ™ºèƒ½ä½“ï¼Œå¹¶åœ¨å„ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼Œç”±äºæ ·æœ¬æ•ˆç‡ä½ã€å®‰å…¨é—®é¢˜ä»¥åŠåœ¨é«˜è‡ªç”±åº¦(DoF)ç³»ç»Ÿçš„é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­ä»ç¨€ç–å¥–åŠ±ä¸­å­¦ä¹ çš„éš¾åº¦ï¼Œç›´æ¥åœ¨çœŸå®æœºå™¨äººä¸Šè®­ç»ƒRLç­–ç•¥ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡æ®‹å·®å­¦ä¹ æ¡†æ¶ç»“åˆBCå’ŒRLä¼˜åŠ¿çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨BCç­–ç•¥ä½œä¸ºé»‘ç›’åŸºç¡€ï¼Œå¹¶é€šè¿‡æ ·æœ¬é«˜æ•ˆçš„ç¦»çº¿RLå­¦ä¹ è½»é‡çº§çš„æ¯æ­¥æ®‹å·®æ ¡æ­£ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦ç¨€ç–çš„äºŒå…ƒå¥–åŠ±ä¿¡å·ï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆåœ°æ”¹è¿›æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä¸­é«˜è‡ªç”±åº¦ç³»ç»Ÿçš„æ“ä½œç­–ç•¥ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç¬¬ä¸€ä¸ªåœ¨å…·æœ‰çµå·§æ‰‹çš„äººå½¢æœºå™¨äººä¸ŠæˆåŠŸè¿›è¡ŒçœŸå®ä¸–ç•ŒRLè®­ç»ƒçš„æ¡ˆä¾‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜åœ¨å„ç§åŸºäºè§†è§‰çš„ä»»åŠ¡ä¸­å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºåœ¨çœŸå®ä¸–ç•Œä¸­éƒ¨ç½²RLæä¾›äº†ä¸€æ¡å¯è¡Œçš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è¡Œä¸ºå…‹éš†æ–¹æ³•ä¾èµ–äºé«˜è´¨é‡çš„äººå·¥ç¤ºæ•™æ•°æ®ï¼Œæ•°æ®æ”¶é›†æˆæœ¬é«˜ï¼Œä¸”ç¦»çº¿æ•°æ®çš„åˆ©ç”¨å­˜åœ¨æ”¶ç›Šé€’å‡çš„é—®é¢˜ã€‚ç›´æ¥åœ¨çœŸå®æœºå™¨äººä¸Šè®­ç»ƒå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé¢ä¸´æ ·æœ¬æ•ˆç‡ä½ã€å®‰å…¨é—®é¢˜ä»¥åŠéš¾ä»¥ä»ç¨€ç–å¥–åŠ±ä¸­å­¦ä¹ é•¿æ—¶ç¨‹ä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜è‡ªç”±åº¦æœºå™¨äººä¸Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆè¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ çš„ä¼˜ç‚¹ï¼Œåˆ©ç”¨è¡Œä¸ºå…‹éš†ç­–ç•¥ä½œä¸ºåŸºç¡€ç­–ç•¥ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ ä¸€ä¸ªæ®‹å·®ç­–ç•¥ï¼Œå¯¹è¡Œä¸ºå…‹éš†ç­–ç•¥çš„è¾“å‡ºè¿›è¡Œä¿®æ­£ã€‚è¿™æ ·æ—¢å¯ä»¥åˆ©ç”¨è¡Œä¸ºå…‹éš†ç­–ç•¥çš„å…ˆéªŒçŸ¥è¯†ï¼Œåˆå¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥è¿›ä¸€æ­¥æå‡ç­–ç•¥çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†å¯¹æ ·æœ¬æ•ˆç‡çš„è¦æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šè¡Œä¸ºå…‹éš†ç­–ç•¥å’Œæ®‹å·®å¼ºåŒ–å­¦ä¹ æ¨¡å—ã€‚è¡Œä¸ºå…‹éš†ç­–ç•¥ä½œä¸ºåŸºç¡€ç­–ç•¥ï¼Œç›´æ¥ä»ç¦»çº¿æ•°æ®ä¸­å­¦ä¹ ã€‚æ®‹å·®å¼ºåŒ–å­¦ä¹ æ¨¡å—åˆ™å­¦ä¹ ä¸€ä¸ªæ®‹å·®ç­–ç•¥ï¼Œè¯¥ç­–ç•¥çš„è¾“å‡ºä¸è¡Œä¸ºå…‹éš†ç­–ç•¥çš„è¾“å‡ºç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆçš„åŠ¨ä½œã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹æ˜¯ç¦»çº¿çš„ï¼Œå³æ®‹å·®å¼ºåŒ–å­¦ä¹ æ¨¡å—çš„è®­ç»ƒä¸ä¾èµ–äºä¸ç¯å¢ƒçš„åœ¨çº¿äº¤äº’ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ®‹å·®å­¦ä¹ çš„æ€æƒ³å¼•å…¥åˆ°è¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ çš„ç»“åˆä¸­ã€‚é€šè¿‡å­¦ä¹ æ®‹å·®ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨è¡Œä¸ºå…‹éš†ç­–ç•¥çš„å…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶é¿å…äº†ç›´æ¥ä»å¤´å¼€å§‹è®­ç»ƒå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å›°éš¾ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åªéœ€è¦ç¨€ç–çš„äºŒå…ƒå¥–åŠ±ä¿¡å·ï¼Œé™ä½äº†å¯¹å¥–åŠ±å‡½æ•°è®¾è®¡çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šæ®‹å·®å¼ºåŒ–å­¦ä¹ æ¨¡å—ä½¿ç”¨off-policyçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚SACæˆ–è€…TD3ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œé€šå¸¸é‡‡ç”¨ç¨€ç–çš„äºŒå…ƒå¥–åŠ±ï¼Œä¾‹å¦‚æˆåŠŸå®Œæˆä»»åŠ¡åˆ™å¥–åŠ±1ï¼Œå¦åˆ™å¥–åŠ±0ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œæ®‹å·®ç­–ç•¥é€šå¸¸é‡‡ç”¨è½»é‡çº§çš„ç½‘ç»œç»“æ„ï¼Œä»¥ä¿è¯æ ·æœ¬æ•ˆç‡ã€‚è¡Œä¸ºå…‹éš†ç­–ç•¥å¯ä»¥ä½¿ç”¨ä»»ä½•ç°æœ‰çš„è¡Œä¸ºå…‹éš†æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚BCã€GAILç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡åœ¨çœŸå®äººå½¢æœºå™¨äººä¸ŠæˆåŠŸè¿›è¡Œäº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ®ä½œè€…æ‰€çŸ¥æ˜¯é¦–æ¬¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§åŸºäºè§†è§‰çš„ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åªéœ€è¦ç¨€ç–çš„äºŒå…ƒå¥–åŠ±ä¿¡å·ã€‚ä¸ç›´æ¥ä½¿ç”¨è¡Œä¸ºå…‹éš†ç­–ç•¥ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡ç­–ç•¥çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜ç²¾åº¦å’Œå¤æ‚æ“ä½œçš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—æ‰‹æœ¯ã€å®¶åº­æœåŠ¡ç­‰ã€‚é€šè¿‡ç»“åˆè¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥é™ä½æœºå™¨äººéƒ¨ç½²çš„æˆæœ¬å’Œéš¾åº¦ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”çœŸå®ä¸–ç•Œçš„å¤æ‚ç¯å¢ƒã€‚è¯¥æ–¹æ³•åœ¨äººå½¢æœºå™¨äººä¸Šçš„æˆåŠŸåº”ç”¨ï¼Œä¹Ÿä¸ºæœªæ¥äººå½¢æœºå™¨äººçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from offline data. In comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems.
>   We present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL. We demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands. Our results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world.

