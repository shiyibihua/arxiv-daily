---
layout: default
title: VGGT-DP: Generalizable Robot Control via Vision Foundation Models
---

# VGGT-DP: Generalizable Robot Control via Vision Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18778" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18778v1</a>
  <a href="https://arxiv.org/pdf/2509.18778.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18778v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18778v1', 'VGGT-DP: Generalizable Robot Control via Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shijia Ge, Yinxin Zhang, Shuzhao Xie, Weixiang Zhang, Mingcai Zhou, Zhi Wang

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-23

**å¤‡æ³¨**: submitted to AAAI 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVGGT-DPï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æå‡æœºå™¨äººæ“ä½œæŠ€èƒ½çš„æ³›åŒ–æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ§åˆ¶` `è§†è§‰æ¨¡ä»¿å­¦ä¹ ` `è§†è§‰åŸºç¡€æ¨¡å‹` `3Dæ„ŸçŸ¥` `å‡ ä½•å…ˆéªŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•å¿½ç•¥è§†è§‰ç¼–ç å™¨çš„ç»“æ„å’Œèƒ½åŠ›ï¼Œå¯¼è‡´ç©ºé—´ç†è§£å’Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. VGGT-DPç»“åˆé¢„è®­ç»ƒ3Dæ„ŸçŸ¥æ¨¡å‹çš„å‡ ä½•å…ˆéªŒå’Œæœ¬ä½“æ„Ÿå—åé¦ˆï¼Œæå‡ç©ºé—´å®šä½å’Œé—­ç¯æ§åˆ¶ã€‚
3. é€šè¿‡ä»¤ç‰Œé‡ç”¨å’Œå‰ªæï¼Œé™ä½æ¨ç†å»¶è¿Ÿï¼Œå¢å¼ºç­–ç•¥é²æ£’æ€§ï¼Œå¹¶åœ¨MetaWorldä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVGGT-DPçš„è§†è§‰è¿åŠ¨ç­–ç•¥æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æœºå™¨äººæ“ä½œæŠ€èƒ½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç­–ç•¥è®¾è®¡ï¼Œå¿½ç•¥äº†è§†è§‰ç¼–ç å™¨çš„ç»“æ„å’Œèƒ½åŠ›ï¼Œé™åˆ¶äº†ç©ºé—´ç†è§£å’Œæ³›åŒ–ã€‚å—ç”Ÿç‰©è§†è§‰ç³»ç»Ÿçš„å¯å‘ï¼ŒVGGT-DPç»“åˆäº†é¢„è®­ç»ƒ3Dæ„ŸçŸ¥æ¨¡å‹æä¾›çš„å‡ ä½•å…ˆéªŒçŸ¥è¯†å’Œæœ¬ä½“æ„Ÿå—åé¦ˆï¼Œä»¥å®ç°é²æ£’æ§åˆ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨Visual Geometry Grounded Transformer (VGGT) ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æœ¬ä½“æ„Ÿå—å¼•å¯¼çš„è§†è§‰å­¦ä¹ ç­–ç•¥ï¼Œå°†æ„ŸçŸ¥ä¸å†…éƒ¨æœºå™¨äººçŠ¶æ€å¯¹é½ï¼Œä»è€Œæ”¹å–„ç©ºé—´å®šä½å’Œé—­ç¯æ§åˆ¶ã€‚ä¸ºäº†é™ä½æ¨ç†å»¶è¿Ÿï¼Œè®¾è®¡äº†ä¸€ç§é€å¸§ä»¤ç‰Œé‡ç”¨æœºåˆ¶ï¼Œå°†å¤šè§†å›¾ä»¤ç‰Œå‹ç¼©æˆé«˜æ•ˆçš„ç©ºé—´è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œåº”ç”¨éšæœºä»¤ç‰Œå‰ªææ¥å¢å¼ºç­–ç•¥çš„é²æ£’æ€§å¹¶å‡å°‘è¿‡æ‹Ÿåˆã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„MetaWorldä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVGGT-DPæ˜¾è‘—ä¼˜äºDPå’ŒDP3ç­‰å¼ºå¤§çš„åŸºçº¿ï¼Œå°¤å…¶æ˜¯åœ¨ç²¾åº¦è¦æ±‚é«˜å’Œé•¿æ—¶ç¨‹åœºæ™¯ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œæŠ€èƒ½å­¦ä¹ ä¸­ï¼Œä¸»è¦å…³æ³¨ç­–ç•¥è®¾è®¡ï¼Œä½†å¿½ç•¥äº†è§†è§‰ç¼–ç å™¨çš„ç»“æ„å’Œèƒ½åŠ›ï¼Œå¯¼è‡´æœºå™¨äººå¯¹ç¯å¢ƒçš„ç©ºé—´ç†è§£ä¸è¶³ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚å°¤å…¶æ˜¯åœ¨ç²¾åº¦è¦æ±‚é«˜å’Œé•¿æ—¶ç¨‹ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ç“¶é¢ˆæ›´åŠ æ˜æ˜¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šå—ç”Ÿç‰©è§†è§‰ç³»ç»ŸåŒæ—¶ä¾èµ–è§†è§‰å’Œæœ¬ä½“æ„Ÿå—ä¿¡æ¯çš„å¯å‘ï¼Œè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é¢„è®­ç»ƒçš„3Dæ„ŸçŸ¥æ¨¡å‹æä¾›çš„å‡ ä½•å…ˆéªŒçŸ¥è¯†ä¸æœºå™¨äººçš„æœ¬ä½“æ„Ÿå—åé¦ˆç›¸ç»“åˆï¼Œä»è€Œå¢å¼ºæœºå™¨äººå¯¹ç¯å¢ƒçš„ç†è§£å’Œå®šä½èƒ½åŠ›ï¼Œæå‡æ§åˆ¶ç­–ç•¥çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVGGT-DPæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) Visual Geometry Grounded Transformer (VGGT) ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œç”¨äºæå–å›¾åƒç‰¹å¾å’Œå‡ ä½•ä¿¡æ¯ï¼›2) æœ¬ä½“æ„Ÿå—å¼•å¯¼çš„è§†è§‰å­¦ä¹ ç­–ç•¥ï¼Œç”¨äºå°†è§†è§‰æ„ŸçŸ¥ä¸æœºå™¨äººçš„å†…éƒ¨çŠ¶æ€å¯¹é½ï¼›3) é€å¸§ä»¤ç‰Œé‡ç”¨æœºåˆ¶ï¼Œç”¨äºé™ä½æ¨ç†å»¶è¿Ÿï¼›4) éšæœºä»¤ç‰Œå‰ªæï¼Œç”¨äºå¢å¼ºç­–ç•¥çš„é²æ£’æ€§ã€‚æ•´ä½“æµç¨‹æ˜¯ä»å¤šè§†è§’å›¾åƒä¸­æå–è§†è§‰ç‰¹å¾ï¼Œç»“åˆæœ¬ä½“æ„Ÿå—ä¿¡æ¯ï¼Œé€šè¿‡ç­–ç•¥ç½‘ç»œè¾“å‡ºæ§åˆ¶æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†é¢„è®­ç»ƒçš„3Dæ„ŸçŸ¥æ¨¡å‹VGGTä¸æœ¬ä½“æ„Ÿå—åé¦ˆç›¸ç»“åˆï¼Œç”¨äºæœºå™¨äººæ§åˆ¶ã€‚è¿™ç§ç»“åˆæ–¹å¼èƒ½å¤Ÿæœ‰æ•ˆæå‡æœºå™¨äººå¯¹ç¯å¢ƒçš„ç†è§£å’Œå®šä½èƒ½åŠ›ï¼Œä»è€Œæé«˜æ§åˆ¶ç­–ç•¥çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œé€å¸§ä»¤ç‰Œé‡ç”¨å’Œéšæœºä»¤ç‰Œå‰ªæä¹Ÿæ˜¯é‡è¦çš„åˆ›æ–°ç‚¹ï¼Œèƒ½å¤Ÿé™ä½æ¨ç†å»¶è¿Ÿå¹¶å¢å¼ºç­–ç•¥çš„é²æ£’æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVGGT-DPæ›´åŠ æ³¨é‡åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æä¾›çš„å…ˆéªŒçŸ¥è¯†ï¼Œè€Œéä»…ä»…ä¾èµ–ç­–ç•¥è®¾è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šVGGTä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œè´Ÿè´£æå–å›¾åƒç‰¹å¾å’Œå‡ ä½•ä¿¡æ¯ã€‚æœ¬ä½“æ„Ÿå—å¼•å¯¼çš„è§†è§‰å­¦ä¹ ç­–ç•¥é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¯¹é½è§†è§‰æ„ŸçŸ¥å’Œæœºå™¨äººå†…éƒ¨çŠ¶æ€ã€‚é€å¸§ä»¤ç‰Œé‡ç”¨æœºåˆ¶é€šè¿‡å‹ç¼©å¤šè§†å›¾ä»¤ç‰Œæ¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚éšæœºä»¤ç‰Œå‰ªæé€šè¿‡éšæœºç§»é™¤éƒ¨åˆ†ä»¤ç‰Œæ¥å¢å¼ºç­–ç•¥çš„é²æ£’æ€§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVGGT-DPåœ¨MetaWorldä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºDPå’ŒDP3ç­‰åŸºçº¿æ–¹æ³•ã€‚å°¤å…¶æ˜¯åœ¨ç²¾åº¦è¦æ±‚é«˜å’Œé•¿æ—¶ç¨‹åœºæ™¯ä¸­ï¼ŒVGGT-DPçš„æ€§èƒ½æå‡æ›´ä¸ºæ˜æ˜¾ã€‚å…·ä½“çš„æ•°æ®æŒ‡æ ‡ï¼ˆä¾‹å¦‚æˆåŠŸç‡ã€å¹³å‡å¥–åŠ±ç­‰ï¼‰åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†å±•ç¤ºï¼ˆæœªçŸ¥ï¼‰ã€‚è¿™äº›ç»“æœéªŒè¯äº†VGGT-DPåœ¨æå‡æœºå™¨äººæ“ä½œæŠ€èƒ½æ³›åŒ–æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜ç²¾åº¦å’Œé•¿æ—¶ç¨‹æ“ä½œçš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—æ‰‹æœ¯ã€å®¶åº­æœåŠ¡ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººçš„ç©ºé—´ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒï¼Œä»è€Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤šç±»å‹çš„æœºå™¨äººå’Œæ›´å¤æ‚çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders, limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.

