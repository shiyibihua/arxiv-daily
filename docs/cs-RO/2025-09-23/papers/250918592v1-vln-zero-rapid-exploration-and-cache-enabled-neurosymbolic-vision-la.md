---
layout: default
title: VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation
---

# VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18592" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18592v1</a>
  <a href="https://arxiv.org/pdf/2509.18592.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18592v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18592v1', 'VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Neel P. Bhatt, Yunhao Yang, Rohan Siva, Pranay Samineni, Daniel Milan, Zhangyang Wang, Ufuk Topcu

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-23

**å¤‡æ³¨**: Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://vln-zero.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VLN-Zeroï¼šé¢å‘æœºå™¨äººå¯¼èˆªé›¶æ ·æœ¬è¿ç§»çš„å¿«é€Ÿæ¢ç´¢ä¸ç¼“å­˜ç¥ç»ç¬¦å·è§†è§‰è¯­è¨€è§„åˆ’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `é›¶æ ·æœ¬å­¦ä¹ ` `æœºå™¨äººå¯¼èˆª` `ç¥ç»ç¬¦å·æ¨ç†` `åœºæ™¯å›¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äºè¯¦å°½çš„æ¢ç´¢æˆ–åˆšæ€§çš„å¯¼èˆªç­–ç•¥ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æ–°ç¯å¢ƒï¼Œé™åˆ¶äº†ç°å®ä¸–ç•Œè‡ªä¸»æ€§çš„å¯æ‰©å±•æ€§ã€‚
2. VLN-Zeroåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºç¬¦å·åœºæ™¯å›¾ï¼Œç»“åˆç¥ç»ç¬¦å·è§„åˆ’å’Œç¼“å­˜æœºåˆ¶ï¼Œå®ç°é«˜æ•ˆçš„é›¶æ ·æœ¬å¯¼èˆªã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVLN-Zeroåœ¨æˆåŠŸç‡ã€å¯¼èˆªæ—¶é—´å’ŒVLMè°ƒç”¨æ¬¡æ•°ä¸Šå‡ä¼˜äºç°æœ‰é›¶æ ·æœ¬æ¨¡å‹å’Œå¤šæ•°å¾®è°ƒåŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

VLN-Zeroæ˜¯ä¸€ä¸ªåŒé˜¶æ®µè§†è§‰è¯­è¨€å¯¼èˆªæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æœªçŸ¥ç¯å¢ƒä¸­çš„å¿«é€Ÿé€‚åº”ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é«˜æ•ˆæ„å»ºç¬¦å·åœºæ™¯å›¾ï¼Œå¹¶å®ç°é›¶æ ·æœ¬ç¥ç»ç¬¦å·å¯¼èˆªã€‚åœ¨æ¢ç´¢é˜¶æ®µï¼Œç»“æ„åŒ–æç¤ºå¼•å¯¼VLMè¿›è¡Œä¿¡æ¯ä¸°å¯Œä¸”å¤šæ ·åŒ–çš„è½¨è¿¹æœç´¢ï¼Œä»è€Œç”Ÿæˆç´§å‡‘çš„åœºæ™¯å›¾è¡¨ç¤ºã€‚åœ¨éƒ¨ç½²é˜¶æ®µï¼Œç¥ç»ç¬¦å·è§„åˆ’å™¨åŸºäºåœºæ™¯å›¾å’Œç¯å¢ƒè§‚æµ‹è¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆå¯æ‰§è¡Œçš„è®¡åˆ’ï¼ŒåŒæ—¶ç¼“å­˜æ‰§è¡Œæ¨¡å—é€šè¿‡é‡ç”¨å…ˆå‰è®¡ç®—çš„ä»»åŠ¡-ä½ç½®è½¨è¿¹æ¥åŠ é€Ÿé€‚åº”ã€‚é€šè¿‡ç»“åˆå¿«é€Ÿæ¢ç´¢ã€ç¬¦å·æ¨ç†å’Œç¼“å­˜æ‰§è¡Œï¼Œè¯¥æ¡†æ¶å…‹æœäº†ç°æœ‰è§†è§‰è¯­è¨€å¯¼èˆªæ–¹æ³•çš„è®¡ç®—ä½æ•ˆå’Œæ³›åŒ–æ€§å·®çš„é—®é¢˜ï¼Œä»è€Œåœ¨æœªçŸ¥ç¯å¢ƒä¸­å®ç°é²æ£’ä¸”å¯æ‰©å±•çš„å†³ç­–ã€‚VLN-Zeroåœ¨å„ç§ç¯å¢ƒä¸­å®ç°äº†æ¯”æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ¨¡å‹é«˜2å€çš„æˆåŠŸç‡ï¼Œä¼˜äºå¤§å¤šæ•°å¾®è°ƒåŸºçº¿ï¼Œå¹¶ä»¥ä¸€åŠçš„æ—¶é—´å’Œå¹³å‡å‡å°‘55%çš„VLMè°ƒç”¨æ¬¡æ•°åˆ°è¾¾ç›®æ ‡ä½ç½®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€å¯¼èˆªæ–¹æ³•åœ¨æœªçŸ¥ç¯å¢ƒä¸­æ³›åŒ–èƒ½åŠ›å·®ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„æ¢ç´¢æˆ–å¾®è°ƒã€‚è¿™äº›æ–¹æ³•è®¡ç®—æ•ˆç‡ä½ï¼Œéš¾ä»¥é€‚åº”çœŸå®ä¸–ç•Œçš„å¤æ‚ç¯å¢ƒã€‚è®ºæ–‡æ—¨åœ¨è§£å†³é›¶æ ·æœ¬æ¡ä»¶ä¸‹çš„æœºå™¨äººå¯¼èˆªé—®é¢˜ï¼Œå³åœ¨æ²¡æœ‰é¢„å…ˆè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä½¿æœºå™¨äººåœ¨æ–°ç¯å¢ƒä¸­å¿«é€Ÿã€æœ‰æ•ˆåœ°å®Œæˆå¯¼èˆªä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLN-Zeroçš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€ç¬¦å·æ¨ç†çš„è§„åˆ’èƒ½åŠ›ä»¥åŠç¼“å­˜æœºåˆ¶çš„åŠ é€Ÿèƒ½åŠ›ã€‚é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºç¯å¢ƒçš„ç¬¦å·è¡¨ç¤ºï¼Œåˆ©ç”¨ç¥ç»ç¬¦å·è§„åˆ’å™¨è¿›è¡Œæ¨ç†ï¼Œå¹¶åˆ©ç”¨ç¼“å­˜æœºåˆ¶é‡ç”¨å·²çŸ¥çš„è½¨è¿¹ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é›¶æ ·æœ¬å¯¼èˆªã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å…‹æœä¼ ç»Ÿæ–¹æ³•åœ¨æ³›åŒ–æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLN-Zeroæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šæ¢ç´¢é˜¶æ®µå’Œéƒ¨ç½²é˜¶æ®µã€‚åœ¨æ¢ç´¢é˜¶æ®µï¼Œä½¿ç”¨ç»“æ„åŒ–æç¤ºå¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹æ¢ç´¢ç¯å¢ƒï¼Œæ„å»ºåœºæ™¯å›¾ã€‚åœ¨éƒ¨ç½²é˜¶æ®µï¼Œç¥ç»ç¬¦å·è§„åˆ’å™¨åŸºäºåœºæ™¯å›¾å’Œç¯å¢ƒè§‚æµ‹ç”Ÿæˆå¯¼èˆªè®¡åˆ’ï¼Œå¹¶ä½¿ç”¨ç¼“å­˜æ‰§è¡Œæ¨¡å—åŠ é€Ÿæ‰§è¡Œã€‚æ•´ä½“æµç¨‹æ˜¯ä»è§†è§‰æ„ŸçŸ¥åˆ°ç¬¦å·æ¨ç†ï¼Œå†åˆ°åŠ¨ä½œæ‰§è¡Œçš„å¾ªç¯è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šVLN-Zeroçš„å…³é”®åˆ›æ–°åœ¨äºç»“åˆäº†å¿«é€Ÿæ¢ç´¢ã€ç¬¦å·æ¨ç†å’Œç¼“å­˜æ‰§è¡Œã€‚å¿«é€Ÿæ¢ç´¢åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹é«˜æ•ˆæ„å»ºåœºæ™¯å›¾ï¼Œç¬¦å·æ¨ç†åˆ©ç”¨ç¥ç»ç¬¦å·è§„åˆ’å™¨è¿›è¡Œå…¨å±€è§„åˆ’ï¼Œç¼“å­˜æ‰§è¡Œé€šè¿‡é‡ç”¨å·²çŸ¥çš„è½¨è¿¹åŠ é€Ÿé€‚åº”ã€‚è¿™ç§ç»“åˆå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ³›åŒ–æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„å±€é™æ€§ï¼Œå®ç°äº†æ›´é²æ£’å’Œå¯æ‰©å±•çš„é›¶æ ·æœ¬å¯¼èˆªã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¢ç´¢é˜¶æ®µï¼Œä½¿ç”¨ç»“æ„åŒ–æç¤ºï¼ˆstructured promptsï¼‰å¼•å¯¼VLMè¿›è¡Œæ¢ç´¢ï¼Œä¾‹å¦‚â€œå¯»æ‰¾åŒ…å«Xçš„æˆ¿é—´â€ã€‚åœºæ™¯å›¾çš„æ„å»ºæ–¹å¼æ˜¯åŸºäºVLMå¯¹ç¯å¢ƒçš„ç†è§£ï¼ŒèŠ‚ç‚¹è¡¨ç¤ºä½ç½®ï¼Œè¾¹è¡¨ç¤ºä½ç½®ä¹‹é—´çš„å¯è¾¾æ€§ã€‚ç¥ç»ç¬¦å·è§„åˆ’å™¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–å¯¼èˆªæˆåŠŸç‡ã€‚ç¼“å­˜æ‰§è¡Œæ¨¡å—ç»´æŠ¤ä¸€ä¸ªä»»åŠ¡-ä½ç½®è½¨è¿¹çš„ç¼“å­˜ï¼Œå½“é‡åˆ°ç›¸ä¼¼çš„ä»»åŠ¡å’Œä½ç½®æ—¶ï¼Œç›´æ¥é‡ç”¨ç¼“å­˜ä¸­çš„è½¨è¿¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VLN-Zeroåœ¨å¤šä¸ªå¯¼èˆªç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜å…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¸æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ¨¡å‹ç›¸æ¯”ï¼ŒVLN-Zeroçš„æˆåŠŸç‡æé«˜äº†2å€ï¼Œå¹¶ä¸”ä¼˜äºå¤§å¤šæ•°å¾®è°ƒåŸºçº¿ã€‚æ­¤å¤–ï¼ŒVLN-Zeroåœ¨å¯¼èˆªæ—¶é—´ä¸Šç¼©çŸ­äº†ä¸€åŠï¼Œå¹³å‡å‡å°‘äº†55%çš„VLMè°ƒç”¨æ¬¡æ•°ï¼Œè¡¨æ˜å…¶å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VLN-Zeroå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨å®¶åº­æœåŠ¡æœºå™¨äººã€ä»“åº“ç‰©æµæœºå™¨äººã€æœç´¢æ•‘æ´æœºå™¨äººç­‰é¢†åŸŸã€‚è¯¥ç ”ç©¶èƒ½å¤Ÿä½¿æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­å¿«é€Ÿé€‚åº”å¹¶å®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œé™ä½äº†éƒ¨ç½²æˆæœ¬å’Œç»´æŠ¤éš¾åº¦ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œç¯å¢ƒï¼Œä¾‹å¦‚å¤šæœºå™¨äººååŒå¯¼èˆªã€åŠ¨æ€ç¯å¢ƒä¸‹çš„å¯¼èˆªç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task-location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments. Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/.

