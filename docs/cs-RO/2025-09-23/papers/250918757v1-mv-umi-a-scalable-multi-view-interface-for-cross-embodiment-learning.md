---
layout: default
title: MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning
---

# MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18757" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18757v1</a>
  <a href="https://arxiv.org/pdf/2509.18757.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18757v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18757v1', 'MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Omar Rayyan, John Abanes, Mahmoud Hafez, Anthony Tzes, Fares Abu-Dakka

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-23

**å¤‡æ³¨**: For project website and videos, see https https://mv-umi.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MV-UMIï¼šç”¨äºè·¨å…·èº«å­¦ä¹ çš„å¯æ‰©å±•å¤šè§†è§’äº¤äº’ç•Œé¢**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `å¤šè§†è§’å­¦ä¹ ` `è·¨å…·èº«å­¦ä¹ ` `æ‰‹æŒå¤¹çˆª` `æ•°æ®å¢å¼º` `é¢†åŸŸè‡ªé€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡ä»¿å­¦ä¹ ä¾èµ–ç‰¹å®šæœºå™¨äººå½¢æ€çš„æ•°æ®é›†ï¼Œæ”¶é›†æˆæœ¬é«˜ä¸”ç¼ºä¹æ³›åŒ–æ€§ã€‚
2. MV-UMIæ¡†æ¶èåˆç¬¬ä¸‰äººç§°è§†è§’ä¸ç¬¬ä¸€äººç§°è§†è§’ï¼Œå¼¥è¡¥åœºæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMV-UMIåœ¨éœ€è¦åœºæ™¯ç†è§£çš„ä»»åŠ¡ä¸­æ€§èƒ½æå‡çº¦47%ï¼Œæ‰©å±•äº†ä»»åŠ¡èŒƒå›´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æ¥ï¼Œæ¨¡ä»¿å­¦ä¹ åœ¨å¼€å‘é²æ£’çš„æœºå™¨äººæ“ä½œç­–ç•¥æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§æ½œåŠ›å–å†³äºå¤šæ ·åŒ–ã€é«˜è´¨é‡æ•°æ®é›†çš„å¯ç”¨æ€§ï¼Œè€Œè¿™äº›æ•°æ®é›†çš„æ”¶é›†ä¸ä»…å…·æœ‰æŒ‘æˆ˜æ€§å’Œé«˜æˆæœ¬ï¼Œè€Œä¸”é€šå¸¸å±€é™äºç‰¹å®šçš„æœºå™¨äººå½¢æ€ã€‚ä¾¿æºå¼æ‰‹æŒå¤¹çˆªæœ€è¿‘ä½œä¸ºä¼ ç»Ÿæœºå™¨äººé¥æ“ä½œæ–¹æ³•çš„ç›´è§‚ä¸”å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆå‡ºç°ï¼Œç”¨äºæ•°æ®æ”¶é›†ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»…ä»…ä¾èµ–äºç¬¬ä¸€äººç§°è§†è§’çš„æ‰‹è…•æ‘„åƒå¤´ï¼Œè¿™é€šå¸¸é™åˆ¶äº†å¯¹è¶³å¤Ÿåœºæ™¯ä¸Šä¸‹æ–‡çš„æ•æ‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MV-UMIï¼ˆå¤šè§†è§’é€šç”¨æ“ä½œç•Œé¢ï¼‰ï¼Œè¯¥æ¡†æ¶é›†æˆäº†ç¬¬ä¸‰äººç§°è§†è§’å’Œè‡ªæˆ‘ä¸­å¿ƒæ‘„åƒå¤´ï¼Œä»¥å…‹æœè¿™ä¸€é™åˆ¶ã€‚è¿™ç§é›†æˆå‡è½»äº†äººç±»æ¼”ç¤ºå’Œæœºå™¨äººéƒ¨ç½²ä¹‹é—´çš„é¢†åŸŸè½¬ç§»ï¼Œä¿ç•™äº†æ‰‹æŒæ•°æ®æ”¶é›†è®¾å¤‡çš„è·¨å…·èº«ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœï¼ŒåŒ…æ‹¬ä¸€é¡¹æ¶ˆèç ”ç©¶ï¼Œè¡¨æ˜æˆ‘ä»¬çš„MV-UMIæ¡†æ¶åœ¨éœ€è¦å¹¿æ³›åœºæ™¯ç†è§£çš„å­ä»»åŠ¡ä¸­ï¼Œåœ¨3ä¸ªä»»åŠ¡ä¸­æé«˜äº†çº¦47%çš„æ€§èƒ½ï¼Œè¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰©å±•å¯ä»¥ä½¿ç”¨æ‰‹æŒå¤¹çˆªç³»ç»Ÿå­¦ä¹ çš„å¯è¡Œæ“ä½œä»»åŠ¡èŒƒå›´æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ä¼šæŸå®³æ­¤ç±»ç³»ç»Ÿå›ºæœ‰çš„è·¨å…·èº«ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¾èµ–äºç‰¹å®šæœºå™¨äººå½¢æ€çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†çš„æ”¶é›†æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ³›åŒ–åˆ°å…¶ä»–æœºå™¨äººå½¢æ€ã€‚æ‰‹æŒå¤¹çˆªä½œä¸ºä¸€ç§æ•°æ®æ”¶é›†æ–¹å¼ï¼Œè™½ç„¶å…·æœ‰è·¨å…·èº«ä¼˜åŠ¿ï¼Œä½†ä»…ä¾èµ–ç¬¬ä¸€äººç§°è§†è§’ï¼Œå¯¼è‡´åœºæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œé™åˆ¶äº†å¯å­¦ä¹ çš„ä»»åŠ¡èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMV-UMIçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡èåˆç¬¬ä¸‰äººç§°è§†è§’æ¥å¢å¼ºæ‰‹æŒå¤¹çˆªçš„æ•°æ®æ”¶é›†èƒ½åŠ›ï¼Œä»è€Œå¼¥è¡¥ç¬¬ä¸€äººç§°è§†è§’çš„å±€é™æ€§ã€‚é€šè¿‡æä¾›æ›´å…¨é¢çš„åœºæ™¯ä¿¡æ¯ï¼Œå‡å°‘äººç±»æ¼”ç¤ºå’Œæœºå™¨äººéƒ¨ç½²ä¹‹é—´çš„é¢†åŸŸå·®å¼‚ï¼Œæå‡æ¨¡ä»¿å­¦ä¹ çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMV-UMIæ¡†æ¶åŒ…å«ä¸€ä¸ªæ‰‹æŒå¤¹çˆªï¼Œä¸€ä¸ªè…•éƒ¨å®‰è£…çš„ç¬¬ä¸€äººç§°æ‘„åƒå¤´ï¼Œä»¥åŠä¸€ä¸ªé¢å¤–çš„ç¬¬ä¸‰äººç§°æ‘„åƒå¤´ã€‚æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶è®°å½•æ¥è‡ªä¸¤ä¸ªæ‘„åƒå¤´çš„è§†é¢‘æµã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨èåˆåçš„å¤šè§†è§’æ•°æ®è®­ç»ƒæ¨¡ä»¿å­¦ä¹ æ¨¡å‹ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ¨¡ä»¿å­¦ä¹ æµç¨‹ä¸­ï¼Œæ— éœ€å¯¹åº•å±‚ç®—æ³•è¿›è¡Œé‡å¤§ä¿®æ”¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMV-UMIçš„å…³é”®åˆ›æ–°åœ¨äºå¤šè§†è§’èåˆçš„æ•°æ®é‡‡é›†æ–¹å¼ã€‚é€šè¿‡ç»“åˆç¬¬ä¸€äººç§°å’Œç¬¬ä¸‰äººç§°è§†è§’ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ•æ‰æ›´ä¸°å¯Œçš„åœºæ™¯ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨¡ä»¿å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä¿ç•™äº†æ‰‹æŒå¤¹çˆªçš„è·¨å…·èº«ä¼˜åŠ¿ï¼ŒåŒæ—¶æ‰©å±•äº†å¯å­¦ä¹ çš„ä»»åŠ¡èŒƒå›´ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†æè¿°å…·ä½“çš„ç½‘ç»œç»“æ„æˆ–æŸå¤±å‡½æ•°ï¼Œä½†å¼ºè°ƒäº†å¤šè§†è§’æ•°æ®èåˆçš„é‡è¦æ€§ã€‚ç¬¬ä¸‰äººç§°æ‘„åƒå¤´çš„å…·ä½“å‚æ•°è®¾ç½®ï¼ˆä¾‹å¦‚ï¼Œä½ç½®ã€è§’åº¦ã€åˆ†è¾¨ç‡ï¼‰å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚æœªæ¥çš„å·¥ä½œå¯ä»¥æ¢ç´¢ä¸åŒçš„å¤šè§†è§’èåˆç­–ç•¥ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥åŠ¨æ€åœ°åŠ æƒä¸åŒè§†è§’çš„ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMV-UMIæ¡†æ¶åœ¨éœ€è¦å¹¿æ³›åœºæ™¯ç†è§£çš„å­ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æå‡çº¦47%ã€‚æ¶ˆèå®éªŒéªŒè¯äº†ç¬¬ä¸‰äººç§°è§†è§’å¯¹æ€§èƒ½æå‡çš„è´¡çŒ®ã€‚è¯¥æ¡†æ¶åœ¨ä¸‰ä¸ªä¸åŒçš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯æ˜äº†å…¶é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMV-UMIèƒ½å¤Ÿæ˜¾è‘—æ‰©å±•å¯ä»¥ä½¿ç”¨æ‰‹æŒå¤¹çˆªç³»ç»Ÿå­¦ä¹ çš„å¯è¡Œæ“ä½œä»»åŠ¡èŒƒå›´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MV-UMIæ¡†æ¶å¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¹¿æ³›åœºæ™¯ç†è§£çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚è¯¥æ¡†æ¶é™ä½äº†æ•°æ®æ”¶é›†çš„æˆæœ¬å’Œéš¾åº¦ï¼Œä¿ƒè¿›äº†æ¨¡ä»¿å­¦ä¹ åœ¨æœºå™¨äººé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå¹¶ä¸ºå¼€å‘æ›´æ™ºèƒ½ã€æ›´çµæ´»çš„æœºå™¨äººç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in imitation learning have shown great promise for developing robust robot manipulation policies from demonstrations. However, this promise is contingent on the availability of diverse, high-quality datasets, which are not only challenging and costly to collect but are often constrained to a specific robot embodiment. Portable handheld grippers have recently emerged as intuitive and scalable alternatives to traditional robotic teleoperation methods for data collection. However, their reliance solely on first-person view wrist-mounted cameras often creates limitations in capturing sufficient scene contexts. In this paper, we present MV-UMI (Multi-View Universal Manipulation Interface), a framework that integrates a third-person perspective with the egocentric camera to overcome this limitation. This integration mitigates domain shifts between human demonstration and robot deployment, preserving the cross-embodiment advantages of handheld data-collection devices. Our experimental results, including an ablation study, demonstrate that our MV-UMI framework improves performance in sub-tasks requiring broad scene understanding by approximately 47% across 3 tasks, confirming the effectiveness of our approach in expanding the range of feasible manipulation tasks that can be learned using handheld gripper systems, without compromising the cross-embodiment advantages inherent to such systems.

