---
layout: default
title: SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration
---

# SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19292" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19292v1</a>
  <a href="https://arxiv.org/pdf/2509.19292.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19292v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19292v1', 'SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Jin, Jun Lv, Han Xue, Wendi Chen, Chuan Wen, Cewu Lu

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-23

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://ericjin2002.github.io/SOE)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SOEï¼šåŸºäºæµå½¢æ¢ç´¢çš„æœºå™¨äººç­–ç•¥è‡ªæå‡ï¼Œæå‡é‡‡æ ·æ•ˆç‡ä¸å®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººç­–ç•¥å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æµå½¢å­¦ä¹ ` `æ¢ç´¢ç­–ç•¥` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººç­–ç•¥æ¢ç´¢æ–¹æ³•ä¾èµ–éšæœºæ‰°åŠ¨ï¼Œå­˜åœ¨ä¸å®‰å…¨ã€è¡Œä¸ºä¸ç¨³å®šç­‰é—®é¢˜ï¼Œé™åˆ¶äº†æ¢ç´¢æ•ˆç‡ã€‚
2. SOEé€šè¿‡å­¦ä¹ ä»»åŠ¡ç›¸å…³å› ç´ çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶å°†æ¢ç´¢é™åˆ¶åœ¨æœ‰æ•ˆåŠ¨ä½œæµå½¢ä¸Šï¼Œä¿è¯æ¢ç´¢çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSOEåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡å’Œæ ·æœ¬æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ™ºèƒ½ä½“é€šè¿‡ç§¯ææ¢ç´¢ç¯å¢ƒæ¥ä¸æ–­æå‡è‡ªèº«èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºåŠ¨ä½œæ¨¡å¼å´©æºƒï¼Œæœºå™¨äººç­–ç•¥å¸¸å¸¸ç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢èƒ½åŠ›ã€‚ç°æœ‰çš„é¼“åŠ±æ¢ç´¢æ–¹æ³•é€šå¸¸ä¾èµ–äºéšæœºæ‰°åŠ¨ï¼Œè¿™æ—¢ä¸å®‰å…¨ï¼Œä¹Ÿä¼šå¯¼è‡´ä¸ç¨³å®šå’Œä¸è§„å¾‹çš„è¡Œä¸ºï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºæµå½¢æ¢ç´¢çš„è‡ªæå‡ï¼ˆSOEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¢å¼ºäº†æœºå™¨äººæ“ä½œä¸­çš„ç­–ç•¥æ¢ç´¢å’Œæ”¹è¿›ã€‚SOEå­¦ä¹ ä»»åŠ¡ç›¸å…³å› ç´ çš„ç´§å‡‘æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶å°†æ¢ç´¢é™åˆ¶åœ¨æœ‰æ•ˆåŠ¨ä½œçš„æµå½¢ä¸Šï¼Œä»è€Œç¡®ä¿å®‰å…¨æ€§ã€å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§ã€‚å®ƒå¯ä»¥æ— ç¼åœ°ä¸ä»»æ„ç­–ç•¥æ¨¡å‹é›†æˆï¼Œä½œä¸ºä¸€ä¸ªæ’ä»¶æ¨¡å—ï¼Œåœ¨ä¸é™ä½åŸºæœ¬ç­–ç•¥æ€§èƒ½çš„æƒ…å†µä¸‹å¢å¼ºæ¢ç´¢ã€‚æ­¤å¤–ï¼Œç»“æ„åŒ–çš„æ½œåœ¨ç©ºé—´æ”¯æŒäººå·¥å¼•å¯¼çš„æ¢ç´¢ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ•ˆç‡å’Œå¯æ§æ€§ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSOEå§‹ç»ˆä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„ä»»åŠ¡æˆåŠŸç‡ã€æ›´å¹³æ»‘å’Œæ›´å®‰å…¨çš„æ¢ç´¢ï¼Œä»¥åŠå“è¶Šçš„æ ·æœ¬æ•ˆç‡ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†æµå½¢æ¢ç´¢ä½œä¸ºä¸€ç§æ ·æœ¬é«˜æ•ˆç­–ç•¥è‡ªæå‡çš„åŸåˆ™æ€§æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººç­–ç•¥å­¦ä¹ æ–¹æ³•ï¼Œå°¤å…¶æ˜¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåœ¨æ¢ç´¢ç¯å¢ƒæ—¶é€šå¸¸é‡‡ç”¨éšæœºæ‰°åŠ¨çš„æ–¹å¼ã€‚è¿™ç§æ–¹å¼å®¹æ˜“å¯¼è‡´æœºå™¨äººæ‰§è¡Œä¸å®‰å…¨çš„åŠ¨ä½œï¼Œäº§ç”Ÿä¸ç¨³å®šçš„è¡Œä¸ºï¼Œå¹¶ä¸”æ¢ç´¢æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨é‡‡æ ·æ•°æ®è¿›è¡Œç­–ç•¥æå‡ã€‚å› æ­¤ï¼Œå¦‚ä½•å®‰å…¨ã€é«˜æ•ˆåœ°å¼•å¯¼æœºå™¨äººè¿›è¡Œæ¢ç´¢æ˜¯äºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSOEçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¢ç´¢é™åˆ¶åœ¨æœ‰æ•ˆåŠ¨ä½œçš„æµå½¢ä¸Šã€‚é€šè¿‡å­¦ä¹ ä¸€ä¸ªä½ç»´çš„æ½œåœ¨ç©ºé—´æ¥è¡¨ç¤ºä»»åŠ¡ç›¸å…³çš„å› ç´ ï¼Œå¹¶åœ¨è¯¥æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¢ç´¢ï¼Œè§£ç å›åŠ¨ä½œç©ºé—´æ—¶ï¼Œä¿è¯ç”Ÿæˆçš„åŠ¨ä½œæ˜¯å®‰å…¨ä¸”æœ‰æ•ˆçš„ã€‚è¿™ç§â€œåœ¨æµå½¢ä¸Šæ¢ç´¢â€çš„æ–¹å¼ï¼Œé¿å…äº†éšæœºæ‰°åŠ¨å¸¦æ¥çš„é—®é¢˜ï¼Œæé«˜äº†æ¢ç´¢çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSOEæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) ç­–ç•¥ç½‘ç»œï¼šè´Ÿè´£ç”ŸæˆåŸºç¡€çš„åŠ¨ä½œç­–ç•¥ã€‚2) æ½œåœ¨ç©ºé—´ç¼–ç å™¨ï¼šå°†çŠ¶æ€ä¿¡æ¯ç¼–ç åˆ°ä½ç»´çš„æ½œåœ¨ç©ºé—´ä¸­ã€‚3) æ½œåœ¨ç©ºé—´æ¢ç´¢æ¨¡å—ï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¢ç´¢ï¼Œç”Ÿæˆæ¢ç´¢å‘é‡ã€‚4) åŠ¨ä½œè§£ç å™¨ï¼šå°†æ½œåœ¨ç©ºé—´çš„æ¢ç´¢å‘é‡è§£ç ä¸ºåŠ¨ä½œç©ºé—´ä¸­çš„åŠ¨ä½œæ‰°åŠ¨ã€‚5) ç­–ç•¥èåˆæ¨¡å—ï¼šå°†åŸºç¡€ç­–ç•¥å’ŒåŠ¨ä½œæ‰°åŠ¨è¿›è¡Œèåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„æ¢ç´¢ç­–ç•¥ã€‚æ•´ä¸ªæ¡†æ¶å¯ä»¥ä½œä¸ºä¸€ä¸ªæ’ä»¶é›†æˆåˆ°ç°æœ‰çš„ç­–ç•¥å­¦ä¹ ç®—æ³•ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šSOEçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†â€œåœ¨æµå½¢ä¸Šæ¢ç´¢â€çš„æ€æƒ³ï¼Œå¹¶å°†å…¶åº”ç”¨äºæœºå™¨äººç­–ç•¥å­¦ä¹ ä¸­ã€‚ä¸ä¼ ç»Ÿçš„éšæœºæ‰°åŠ¨æ–¹æ³•ç›¸æ¯”ï¼ŒSOEèƒ½å¤Ÿå­¦ä¹ ä»»åŠ¡ç›¸å…³çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶å°†æ¢ç´¢é™åˆ¶åœ¨è¯¥ç©ºé—´ä¸­ï¼Œä»è€Œä¿è¯äº†æ¢ç´¢çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒSOEè¿˜æ”¯æŒäººå·¥å¼•å¯¼çš„æ¢ç´¢ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¢ç´¢çš„æ•ˆç‡å’Œå¯æ§æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šSOEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ æ½œåœ¨ç©ºé—´ï¼Œä¿è¯æ½œåœ¨ç©ºé—´çš„è¿ç»­æ€§å’Œå®Œæ•´æ€§ã€‚2) è®¾è®¡äº†ä¸“é—¨çš„æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ½œåœ¨ç©ºé—´èƒ½å¤Ÿæ•æ‰ä»»åŠ¡ç›¸å…³çš„å› ç´ ã€‚3) é‡‡ç”¨å¯¹æŠ—è®­ç»ƒçš„æ–¹å¼ï¼Œä¿è¯è§£ç åçš„åŠ¨ä½œæ‰°åŠ¨æ˜¯æœ‰æ•ˆçš„ã€‚4) ç­–ç•¥èåˆæ¨¡å—é‡‡ç”¨å¯å­¦ä¹ çš„æƒé‡ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´åŸºç¡€ç­–ç•¥å’Œæ¢ç´¢ç­–ç•¥çš„æ¯”ä¾‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SOEåœ¨å¤šä¸ªæ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬Reachã€Pushã€Pick & Placeç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSOEåœ¨ä»»åŠ¡æˆåŠŸç‡ã€æ¢ç´¢æ•ˆç‡å’Œå®‰å…¨æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„æ¢ç´¢æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨Pick & Placeä»»åŠ¡ä¸­ï¼ŒSOEçš„æˆåŠŸç‡æ¯”SAC-augmentedé«˜å‡º15%ï¼Œå¹¶ä¸”æ¢ç´¢è¿‡ç¨‹æ›´åŠ å¹³æ»‘å’Œå®‰å…¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SOEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æŠ“å–ã€è£…é…ã€å¯¼èˆªç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜æœºå™¨äººç­–ç•¥å­¦ä¹ çš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œé™ä½äº†æœºå™¨äººéƒ¨ç½²çš„æˆæœ¬ã€‚æœªæ¥ï¼ŒSOEè¿˜å¯ä»¥åº”ç”¨äºæ›´å¤æ‚çš„æœºå™¨äººä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚äººæœºåä½œã€è‡ªä¸»æ¢ç´¢ç­‰ï¼Œæ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Intelligent agents progress by continually refining their capabilities through actively exploring environments. Yet robot policies often lack sufficient exploration capability due to action mode collapse. Existing methods that encourage exploration typically rely on random perturbations, which are unsafe and induce unstable, erratic behaviors, thereby limiting their effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a framework that enhances policy exploration and improvement in robotic manipulation. SOE learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, ensuring safety, diversity, and effectiveness. It can be seamlessly integrated with arbitrary policy models as a plug-in module, augmenting exploration without degrading the base policy performance. Moreover, the structured latent space enables human-guided exploration, further improving efficiency and controllability. Extensive experiments in both simulation and real-world tasks demonstrate that SOE consistently outperforms prior methods, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency. These results establish on-manifold exploration as a principled approach to sample-efficient policy self-improvement. Project website: https://ericjin2002.github.io/SOE

