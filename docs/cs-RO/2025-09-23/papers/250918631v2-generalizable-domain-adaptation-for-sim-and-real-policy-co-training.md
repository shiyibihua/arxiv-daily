---
layout: default
title: Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training
---

# Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18631" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18631v2</a>
  <a href="https://arxiv.org/pdf/2509.18631.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18631v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18631v2', 'Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuo Cheng, Liqian Ma, Zhenyang Chen, Ajay Mandlekar, Caelan Garrett, Danfei Xu

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-23 (æ›´æ–°: 2025-09-24)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæœ€ä¼˜ä¼ è¾“çš„Sim-to-Realç­–ç•¥ååŒè®­ç»ƒæ¡†æ¶ï¼Œæå‡æœºå™¨äººæ“ä½œæ³›åŒ–æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `é¢†åŸŸè‡ªé€‚åº”` `Sim-to-Real` `è¡Œä¸ºå…‹éš†` `æœ€ä¼˜ä¼ è¾“` `ååŒè®­ç»ƒ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œæ¼”ç¤ºæ•°æ®è·å–æˆæœ¬é«˜æ˜‚ï¼Œè€Œç›´æ¥ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒç­–ç•¥å­˜åœ¨Sim-to-Realçš„é¢†åŸŸå·®å¼‚ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºæœ€ä¼˜ä¼ è¾“çš„ååŒè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å¯¹é½æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®çš„è§‚æµ‹-åŠ¨ä½œè”åˆåˆ†å¸ƒï¼Œå­¦ä¹ é¢†åŸŸä¸å˜çš„ç‰¹å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåˆ©ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œæ˜¾è‘—æå‡çœŸå®ä¸–ç•Œæ“ä½œæˆåŠŸç‡ï¼Œå¹¶å…·å¤‡ä¸€å®šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¨¡æ‹Ÿ-çœŸå®ååŒè®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå­¦ä¹ å¯æ³›åŒ–çš„æœºå™¨äººæ“ä½œç­–ç•¥ã€‚è¯¥æ¡†æ¶ä¸»è¦åˆ©ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œä»…éœ€å°‘é‡çœŸå®ä¸–ç•Œæ¼”ç¤ºã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å­¦ä¹ ä¸€ä¸ªé¢†åŸŸä¸å˜ä¸”ä¸ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾ç©ºé—´ã€‚å…³é”®åœ¨äºå¯¹é½è·¨é¢†åŸŸçš„è§‚æµ‹åŠå…¶å¯¹åº”åŠ¨ä½œçš„è”åˆåˆ†å¸ƒï¼Œè¿™æ¯”ä»…å¯¹é½è§‚æµ‹çš„è¾¹ç¼˜åˆ†å¸ƒæä¾›äº†æ›´ä¸°å¯Œçš„ä¿¡å·ã€‚é€šè¿‡åœ¨ååŒè®­ç»ƒæ¡†æ¶ä¸­åµŒå…¥å—æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰å¯å‘çš„æŸå¤±ï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°éå¹³è¡¡OTæ¡†æ¶ä»¥å¤„ç†æ¨¡æ‹Ÿæ•°æ®ä¸°å¯Œå’ŒçœŸå®æ•°æ®æœ‰é™ä¹‹é—´çš„ä¸å¹³è¡¡ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ“ä½œä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œè¡¨æ˜å…¶å¯ä»¥åˆ©ç”¨ä¸°å¯Œçš„æ¨¡æ‹Ÿæ•°æ®ï¼Œåœ¨çœŸå®ä¸–ç•Œçš„æˆåŠŸç‡ä¸Šæé«˜é«˜è¾¾30%ï¼Œç”šè‡³å¯ä»¥æ³›åŒ–åˆ°ä»…åœ¨æ¨¡æ‹Ÿä¸­çœ‹åˆ°çš„åœºæ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè¡Œä¸ºå…‹éš†çš„æœºå™¨äººæ“ä½œæ–¹æ³•ä¾èµ–å¤§é‡çœŸå®æ•°æ®ï¼Œè·å–æˆæœ¬é«˜ã€‚åˆ©ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œè®­ç»ƒå¯ä»¥é™ä½æˆæœ¬ï¼Œä½†ç”±äºæ¨¡æ‹Ÿç¯å¢ƒä¸çœŸå®ç¯å¢ƒå­˜åœ¨å·®å¼‚ï¼ˆSim-to-Real gapï¼‰ï¼Œå¯¼è‡´ç­–ç•¥åœ¨çœŸå®ç¯å¢ƒä¸­çš„æ€§èƒ½ä¸‹é™ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨æœ‰é™çš„çœŸå®æ•°æ®å’Œå¤§é‡çš„æ¨¡æ‹Ÿæ•°æ®ï¼Œè®­ç»ƒå‡ºå…·æœ‰è‰¯å¥½æ³›åŒ–èƒ½åŠ›çš„æœºå™¨äººæ“ä½œç­–ç•¥æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å­¦ä¹ é¢†åŸŸä¸å˜çš„ç‰¹å¾è¡¨ç¤ºï¼Œç¼©å°æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ç¯å¢ƒä¹‹é—´çš„å·®è·ã€‚ä¸ä»¥å¾€ä»…å¯¹é½è§‚æµ‹çš„è¾¹ç¼˜åˆ†å¸ƒçš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡æå‡ºå¯¹é½è§‚æµ‹å’ŒåŠ¨ä½œçš„è”åˆåˆ†å¸ƒï¼Œè®¤ä¸ºåŠ¨ä½œä¿¡æ¯èƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œçš„é¢†åŸŸå¯¹é½ä¿¡å·ã€‚é€šè¿‡æœ€å°åŒ–æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®åœ¨è”åˆåˆ†å¸ƒä¸Šçš„å·®å¼‚ï¼Œå¯ä»¥å­¦ä¹ åˆ°æ›´å…·æ³›åŒ–æ€§çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶æ˜¯ä¸€ä¸ªSim-and-RealååŒè®­ç»ƒæµç¨‹ã€‚é¦–å…ˆï¼Œåˆ†åˆ«ä»æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ç¯å¢ƒä¸­æ”¶é›†æ•°æ®ã€‚ç„¶åï¼Œåˆ©ç”¨ä¸€ä¸ªå…±äº«çš„ç¥ç»ç½‘ç»œæå–è§‚æµ‹å’ŒåŠ¨ä½œçš„ç‰¹å¾è¡¨ç¤ºã€‚æ¥ç€ï¼Œä½¿ç”¨ä¸€ä¸ªåŸºäºæœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰çš„æŸå¤±å‡½æ•°æ¥å¯¹é½æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®çš„è”åˆåˆ†å¸ƒã€‚æœ€åï¼Œä½¿ç”¨è¡Œä¸ºå…‹éš†æŸå¤±æ¥è®­ç»ƒç­–ç•¥ã€‚æ•´ä¸ªæµç¨‹è¿­ä»£è¿›è¡Œï¼Œä¸æ–­ä¼˜åŒ–ç‰¹å¾æå–å™¨å’Œç­–ç•¥ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºäºæœ€ä¼˜ä¼ è¾“çš„è”åˆåˆ†å¸ƒå¯¹é½æ–¹æ³•ã€‚ä¼ ç»Ÿçš„é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•é€šå¸¸åªå…³æ³¨è§‚æµ‹çš„è¾¹ç¼˜åˆ†å¸ƒå¯¹é½ï¼Œå¿½ç•¥äº†åŠ¨ä½œä¿¡æ¯ã€‚æœ¬æ–‡é€šè¿‡å¯¹é½è§‚æµ‹å’ŒåŠ¨ä½œçš„è”åˆåˆ†å¸ƒï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ç¼©å°é¢†åŸŸå·®å¼‚ï¼Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®é‡ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œè®ºæ–‡è¿˜æå‡ºäº†éå¹³è¡¡æœ€ä¼˜ä¼ è¾“æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†åŸºäºç¥ç»ç½‘ç»œçš„ç‰¹å¾æå–å™¨ï¼Œå°†è§‚æµ‹å’ŒåŠ¨ä½œæ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ã€‚æœ€ä¼˜ä¼ è¾“æŸå¤±å‡½æ•°é‡‡ç”¨Sinkhornè·ç¦»è¿›è¡Œè®¡ç®—ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚ä¸ºäº†å¤„ç†æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œä½¿ç”¨äº†éå¹³è¡¡æœ€ä¼˜ä¼ è¾“ï¼Œå…è®¸æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®åœ¨åˆ†å¸ƒå¯¹é½æ—¶å­˜åœ¨ä¸€å®šçš„è´¨é‡å·®å¼‚ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°åŒ…æ‹¬è¡Œä¸ºå…‹éš†æŸå¤±å’Œæœ€ä¼˜ä¼ è¾“æŸå¤±ï¼Œé€šè¿‡è°ƒæ•´æƒé‡æ¥å¹³è¡¡ä¸¤ä¸ªæŸå¤±å‡½æ•°çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨çœŸå®ä¸–ç•Œçš„æ“ä½œæˆåŠŸç‡ä¸Šï¼Œç›¸æ¯”äºåŸºçº¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ€é«˜æå‡äº†30%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿæ³›åŒ–åˆ°ä»…åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è§è¿‡çš„åœºæ™¯ï¼Œè¯æ˜äº†å…¶è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–ã€è£…é…ã€å¯¼èˆªç­‰ã€‚é€šè¿‡åˆ©ç”¨å»‰ä»·çš„æ¨¡æ‹Ÿæ•°æ®å’Œå°‘é‡çœŸå®æ•°æ®ï¼Œå¯ä»¥å¿«é€Ÿè®­ç»ƒå‡ºé€‚ç”¨äºçœŸå®ç¯å¢ƒçš„æœºå™¨äººç­–ç•¥ï¼Œé™ä½äº†æœºå™¨äººéƒ¨ç½²çš„æˆæœ¬å’Œéš¾åº¦ã€‚è¯¥æ–¹æ³•åœ¨å·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡æœºå™¨äººç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to achieve up to a 30% improvement in the real-world success rate and even generalize to scenarios seen only in simulation.

