---
layout: default
title: LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA
---

# LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18576" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18576v1</a>
  <a href="https://arxiv.org/pdf/2509.18576.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18576v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18576v1', 'LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeyi Kang, Liang He, Yanxin Zhang, Zuheng Ming, Kaixing Zhao

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§è·¨æ¨¡æ€Mambaformerï¼ˆLCMFï¼‰ï¼Œç”¨äºå…·èº«æœºå™¨äººVQAä»»åŠ¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«æœºå™¨äºº` `è§†è§‰é—®ç­”` `å¤šæ¨¡æ€èåˆ` `Mambaæ¶æ„` `è·¨æ¨¡æ€æ³¨æ„åŠ›` `å‚æ•°å…±äº«` `è½»é‡åŒ–æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆå¼‚æ„æ•°æ®ï¼Œä¸”åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹è®¡ç®—æ•ˆç‡ä½ï¼Œé™åˆ¶äº†å…·èº«æ™ºèƒ½çš„å‘å±•ã€‚
2. æå‡ºLCMFæ¡†æ¶ï¼Œé€šè¿‡å¤šå±‚è·¨æ¨¡æ€å‚æ•°å…±äº«æœºåˆ¶çš„Mambaæ¨¡å—ï¼Œå®ç°å¼‚æ„æ¨¡æ€é«˜æ•ˆèåˆå’Œè¯­ä¹‰å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLCMFåœ¨VQAä»»åŠ¡ä¸­å‡†ç¡®ç‡è¾¾74.29%ï¼ŒFLOPsæ¯”åŸºçº¿å¹³å‡æ°´å¹³é™ä½4.35å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶é’ˆå¯¹å…·èº«æ™ºèƒ½ä¸­å¤šæ¨¡æ€è¯­ä¹‰å­¦ä¹ é¢ä¸´çš„å¼‚æ„æ•°æ®èåˆå’Œèµ„æºå—é™ç¯å¢ƒä¸‹çš„è®¡ç®—æ•ˆç‡é—®é¢˜ï¼Œæå‡ºäº†è½»é‡çº§LCMFçº§è”æ³¨æ„åŠ›æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å¤šå±‚è·¨æ¨¡æ€å‚æ•°å…±äº«æœºåˆ¶å¼•å…¥Mambaæ¨¡å—ï¼Œç»“åˆCross-Attentionå’Œé€‰æ‹©æ€§å‚æ•°å…±äº«çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„ä¼˜åŠ¿ï¼Œå®ç°äº†å¼‚æ„æ¨¡æ€çš„é«˜æ•ˆèåˆå’Œè¯­ä¹‰äº’è¡¥å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLCMFåœ¨VQAä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€åŸºçº¿ï¼Œå‡†ç¡®ç‡è¾¾åˆ°74.29%ï¼Œå¹¶åœ¨EQAè§†é¢‘ä»»åŠ¡ä¸­è¾¾åˆ°äº†å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼ˆLLM Agentsï¼‰åˆ†å¸ƒé›†ç¾¤ä¸­å…·æœ‰ç«äº‰åŠ›çš„ä¸­ç­‰æ°´å¹³æ€§èƒ½ã€‚å…¶è½»é‡åŒ–è®¾è®¡å®ç°äº†ç›¸å¯¹äºå¯æ¯”åŸºçº¿å¹³å‡æ°´å¹³4.35å€çš„FLOPså‡å°‘ï¼ŒåŒæ—¶ä»…ä½¿ç”¨166.51Mï¼ˆå›¾åƒ-æ–‡æœ¬ï¼‰å’Œ219Mï¼ˆè§†é¢‘-æ–‡æœ¬ï¼‰å‚æ•°ï¼Œä¸ºèµ„æºå—é™åœºæ™¯ä¸‹çš„äººæœºäº¤äº’ï¼ˆHRIï¼‰åº”ç”¨æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å…·æœ‰å¼ºå¤§çš„å¤šæ¨¡æ€å†³ç­–æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å…·èº«æœºå™¨äººè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­ï¼Œå¦‚ä½•é«˜æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚å›¾åƒã€æ–‡æœ¬ã€è§†é¢‘ï¼‰ï¼Œå¹¶åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æœºå™¨äººå¹³å°ä¸Šå®ç°é«˜æ€§èƒ½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸è®¡ç®—é‡å¤§ï¼Œéš¾ä»¥éƒ¨ç½²åœ¨èµ„æºå—é™çš„æœºå™¨äººä¸Šï¼Œä¸”å¤šæ¨¡æ€ä¿¡æ¯èåˆæ•ˆç‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†Mambaæ¶æ„ä¸è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥å¤šå±‚å‚æ•°å…±äº«ç­–ç•¥ï¼Œä»è€Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦ã€‚Mambaæ¶æ„æ“…é•¿åºåˆ—å»ºæ¨¡ï¼Œè·¨æ¨¡æ€æ³¨æ„åŠ›åˆ™èƒ½æœ‰æ•ˆèåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚å‚æ•°å…±äº«è¿›ä¸€æ­¥å‡å°‘äº†æ¨¡å‹å‚æ•°é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLCMFæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚è§†è§‰Transformerå’Œæ–‡æœ¬ç¼–ç å™¨ï¼‰æå–å›¾åƒã€æ–‡æœ¬æˆ–è§†é¢‘çš„ç‰¹å¾ã€‚ç„¶åï¼Œå°†è¿™äº›ç‰¹å¾è¾“å…¥åˆ°LCMFæ¨¡å—ä¸­è¿›è¡Œè·¨æ¨¡æ€èåˆã€‚LCMFæ¨¡å—çš„æ ¸å¿ƒæ˜¯Mambaæ¨¡å—ï¼Œå…¶ä¸­é›†æˆäº†è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œå¤šå±‚å‚æ•°å…±äº«ç­–ç•¥ã€‚æœ€åï¼Œé€šè¿‡ä¸€ä¸ªé¢„æµ‹å¤´è¾“å‡ºç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†Mambaæ¶æ„ä¸è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆï¼Œå¹¶æå‡ºäº†å¤šå±‚å‚æ•°å…±äº«ç­–ç•¥ã€‚Mambaæ¶æ„æ“…é•¿åºåˆ—å»ºæ¨¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶åˆ™èƒ½å¤Ÿè‡ªé€‚åº”åœ°èåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚å¤šå±‚å‚æ•°å…±äº«ç­–ç•¥åˆ™æ˜¾è‘—é™ä½äº†æ¨¡å‹çš„å‚æ•°é‡ï¼Œä½¿å…¶æ›´é€‚åˆéƒ¨ç½²åœ¨èµ„æºå—é™çš„æœºå™¨äººå¹³å°ä¸Šã€‚

**å…³é”®è®¾è®¡**ï¼šLCMFæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é€‰æ‹©æ€§å‚æ•°å…±äº«çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œåœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å…±äº«éƒ¨åˆ†å‚æ•°ï¼Œå‡å°‘æ¨¡å‹å‚æ•°é‡ï¼›2) é‡‡ç”¨Cross-Attentionæœºåˆ¶ï¼Œå®ç°ä¸åŒæ¨¡æ€ç‰¹å¾ä¹‹é—´çš„æœ‰æ•ˆäº¤äº’ï¼›3) é€šè¿‡çº§è”å¤šä¸ªLCMFæ¨¡å—ï¼Œé€æ­¥æå‡æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼›4) æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨äºè®­ç»ƒVQAæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLCMFåœ¨VQAä»»åŠ¡ä¸­å–å¾—äº†74.29%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€åŸºçº¿ã€‚åŒæ—¶ï¼ŒLCMFçš„FLOPsç›¸å¯¹äºå¯æ¯”åŸºçº¿å¹³å‡æ°´å¹³é™ä½äº†4.35å€ï¼Œå‚æ•°é‡ä»…ä¸º166.51Mï¼ˆå›¾åƒ-æ–‡æœ¬ï¼‰å’Œ219Mï¼ˆè§†é¢‘-æ–‡æœ¬ï¼‰ã€‚æ­¤å¤–ï¼ŒLCMFåœ¨EQAè§†é¢‘ä»»åŠ¡ä¸­ä¹Ÿè¾¾åˆ°äº†å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼ˆLLM Agentsï¼‰åˆ†å¸ƒé›†ç¾¤ä¸­å…·æœ‰ç«äº‰åŠ›çš„ä¸­ç­‰æ°´å¹³æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶å¼ºå¤§çš„å¤šæ¨¡æ€å†³ç­–æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§äººæœºäº¤äº’åœºæ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šå·¡æ£€æœºå™¨äººã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰ã€‚é€šè¿‡é«˜æ•ˆçš„å¤šæ¨¡æ€ä¿¡æ¯èåˆå’Œå†³ç­–èƒ½åŠ›ï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£äººç±»æŒ‡ä»¤ï¼Œæ„ŸçŸ¥å‘¨å›´ç¯å¢ƒï¼Œå¹¶åšå‡ºæ™ºèƒ½å†³ç­–ï¼Œä»è€Œæå‡äººæœºäº¤äº’çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨å…·èº«æ™ºèƒ½åœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.

