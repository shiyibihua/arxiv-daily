---
layout: default
title: OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation
---

# OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19480" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19480v1</a>
  <a href="https://arxiv.org/pdf/2509.19480.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19480v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19480v1', 'OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Noriaki Hirose, Catherine Glossop, Dhruv Shah, Sergey Levine

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-23

**å¤‡æ³¨**: 9 pages, 7 figures, 6 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniVLAï¼šç”¨äºæœºå™¨äººå¯¼èˆªçš„é€šç”¨æ¨¡æ€è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººå¯¼èˆª` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `é€šç”¨æ¨¡æ€` `éšæœºæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººå¯¼èˆªç­–ç•¥å¤§å¤šåŸºäºå•ä¸€æ¨¡æ€è®­ç»ƒï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„é€‚åº”æ€§ï¼Œéš¾ä»¥çµæ´»ç»„åˆè¯­è¨€ã€åæ ‡æˆ–è§†è§‰å‚è€ƒç­‰ç›®æ ‡ã€‚
2. OmniVLAåˆ©ç”¨é«˜å®¹é‡VLAéª¨å¹²ç½‘ç»œï¼Œé€šè¿‡éšæœºæ¨¡æ€èåˆç­–ç•¥ï¼Œç»“åˆ2Då§¿åŠ¿ã€å›¾åƒå’Œè‡ªç„¶è¯­è¨€ç­‰å¤šç§æ¨¡æ€è¿›è¡Œè®­ç»ƒã€‚
3. OmniVLAåœ¨æœªè§ç¯å¢ƒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹æ¨¡æ€ç¼ºå¤±å…·æœ‰é²æ£’æ€§ï¼Œå¹¶èƒ½ç†è§£æ–°çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä¼˜äºå•æ¨¡æ€åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæœºå™¨äººåŸºç¡€æ¨¡å‹çš„è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ”¯æŒåŸºäºè§†è§‰çš„å¯¼èˆªçš„é€šç”¨æ¨¡æ€ç›®æ ‡æ¡ä»¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é«˜å®¹é‡çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰éª¨å¹²ç½‘ç»œï¼Œå¹¶é€šè¿‡éšæœºæ¨¡æ€èåˆç­–ç•¥ï¼Œä½¿ç”¨ä¸‰ç§ä¸»è¦ç›®æ ‡æ¨¡æ€ï¼ˆ2Då§¿åŠ¿ã€è‡ªæˆ‘ä¸­å¿ƒå›¾åƒå’Œè‡ªç„¶è¯­è¨€ï¼‰åŠå…¶ç»„åˆè¿›è¡Œè®­ç»ƒã€‚è¿™ç§è®¾è®¡ä¸ä»…æ‰©å±•äº†å¯ç”¨æ•°æ®é›†çš„èŒƒå›´ï¼Œè¿˜é¼“åŠ±ç­–ç•¥å‘å±•æ›´ä¸°å¯Œçš„å‡ ä½•ã€è¯­ä¹‰å’Œè§†è§‰è¡¨ç¤ºã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹OmniVLAï¼Œå®ç°äº†å¯¹æœªè§ç¯å¢ƒçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹ç¨€ç¼ºæ¨¡æ€çš„é²æ£’æ€§ï¼Œä»¥åŠéµå¾ªæ–°çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒOmniVLAåœ¨å„ç§æ¨¡æ€ä¸Šä¼˜äºä¸“ä¸šåŸºçº¿ï¼Œå¹¶ä¸ºå¾®è°ƒåˆ°æ–°çš„æ¨¡æ€å’Œä»»åŠ¡æä¾›äº†çµæ´»çš„åŸºç¡€ã€‚OmniVLAä¸ºå¹¿æ³›é€šç”¨å’Œçµæ´»çš„å¯¼èˆªç­–ç•¥ä»¥åŠæ„å»ºé€šç”¨æ¨¡æ€æœºå™¨äººåŸºç¡€æ¨¡å‹çš„å¯æ‰©å±•è·¯å¾„æä¾›äº†ä¸€ä¸ªæ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººå¯¼èˆªç­–ç•¥é€šå¸¸é’ˆå¯¹å•ä¸€æ¨¡æ€ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚ä»…ä¾èµ–å›¾åƒã€è¯­è¨€æˆ–åæ ‡ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨çœŸå®ä¸–ç•Œä¸­çš„åº”ç”¨ï¼Œå› ä¸ºäººç±»é€šå¸¸å¯ä»¥çµæ´»åœ°ç»„åˆå¤šç§æ¨¡æ€çš„ä¿¡æ¯æ¥æŒ‡å¯¼å¯¼èˆªã€‚å› æ­¤ï¼Œå¦‚ä½•æ„å»ºä¸€ä¸ªèƒ½å¤Ÿç†è§£å’Œèåˆå¤šç§æ¨¡æ€ç›®æ ‡ä¿¡æ¯çš„é€šç”¨å¯¼èˆªæ¨¡å‹æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿæ¥å—å¤šç§æ¨¡æ€è¾“å…¥ï¼ˆåŒ…æ‹¬2Då§¿åŠ¿ã€è‡ªæˆ‘ä¸­å¿ƒå›¾åƒå’Œè‡ªç„¶è¯­è¨€ï¼‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ã€‚é€šè¿‡éšæœºæ¨¡æ€èåˆç­–ç•¥ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³è”æ€§ï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®å¯ç”¨çš„æ¨¡æ€ä¿¡æ¯è¿›è¡Œå¯¼èˆªã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”çœŸå®ä¸–ç•Œçš„å¤æ‚åœºæ™¯ï¼Œå¹¶æé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniVLAæ¨¡å‹é‡‡ç”¨ä¸€ä¸ªé«˜å®¹é‡çš„VLAéª¨å¹²ç½‘ç»œã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æ¥æ”¶2Då§¿åŠ¿ã€è‡ªæˆ‘ä¸­å¿ƒå›¾åƒå’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªéšæœºæ¨¡æ€èåˆæ¨¡å—å°†è¿™äº›ä¿¡æ¯èåˆåœ¨ä¸€èµ·ã€‚èåˆåçš„ä¿¡æ¯è¢«è¾“å…¥åˆ°ç­–ç•¥ç½‘ç»œä¸­ï¼Œç­–ç•¥ç½‘ç»œè¾“å‡ºæœºå™¨äººçš„åŠ¨ä½œæŒ‡ä»¤ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šOmniVLAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é€šç”¨æ¨¡æ€ç›®æ ‡æ¡ä»¶è®¾è®¡å’Œéšæœºæ¨¡æ€èåˆç­–ç•¥ã€‚ä¸ä»¥å¾€çš„å•æ¨¡æ€å¯¼èˆªæ¨¡å‹ä¸åŒï¼ŒOmniVLAèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šç§æ¨¡æ€çš„è¾“å…¥ï¼Œå¹¶å­¦ä¹ åˆ°å®ƒä»¬ä¹‹é—´çš„å…³è”æ€§ã€‚éšæœºæ¨¡æ€èåˆç­–ç•¥é¼“åŠ±æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„è¡¨ç¤ºï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒæ¨¡æ€ä¿¡æ¯ç¼ºå¤±çš„æƒ…å†µã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½œè€…é‡‡ç”¨äº†éšæœºæ¨¡æ€èåˆç­–ç•¥ï¼Œå³éšæœºé€‰æ‹©è¾“å…¥æ¨¡æ€çš„ç»„åˆã€‚è¿™ç§ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°é˜²æ­¢æ¨¡å‹è¿‡åº¦ä¾èµ–äºå•ä¸€æ¨¡æ€ï¼Œå¹¶é¼“åŠ±æ¨¡å‹å­¦ä¹ æ›´é€šç”¨çš„è¡¨ç¤ºã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦å¹³è¡¡ä¸åŒæ¨¡æ€ä¹‹é—´çš„è´¡çŒ®ï¼Œå¹¶ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°æ‰§è¡Œå¯¼èˆªä»»åŠ¡ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OmniVLAåœ¨æœªè§ç¯å¢ƒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºå•æ¨¡æ€åŸºçº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniVLAèƒ½å¤ŸæˆåŠŸåœ°éµå¾ªæ–°çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶ä¸”å¯¹æ¨¡æ€ç¼ºå¤±å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”ç»“æœå¯åœ¨è®ºæ–‡çš„å®éªŒéƒ¨åˆ†æ‰¾åˆ°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OmniVLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå®¶åº­æœåŠ¡æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€æ— äººæœºç­‰é¢†åŸŸã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç†è§£å¤šç§å½¢å¼çš„ç›®æ ‡æŒ‡ä»¤ï¼Œå¹¶é€‚åº”ä¸åŒçš„ç¯å¢ƒæ¡ä»¶ï¼Œä»è€Œæé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼ŒOmniVLAå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤šçš„æ¨¡æ€å’Œä»»åŠ¡ï¼Œæˆä¸ºé€šç”¨æœºå™¨äººå¹³å°çš„åŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humans can flexibly interpret and compose different goal specifications, such as language instructions, spatial coordinates, or visual references, when navigating to a destination. In contrast, most existing robotic navigation policies are trained on a single modality, limiting their adaptability to real-world scenarios where different forms of goal specification are natural and complementary. In this work, we present a training framework for robotic foundation models that enables omni-modal goal conditioning for vision-based navigation. Our approach leverages a high-capacity vision-language-action (VLA) backbone and trains with three primary goal modalities: 2D poses, egocentric images, and natural language, as well as their combinations, through a randomized modality fusion strategy. This design not only expands the pool of usable datasets but also encourages the policy to develop richer geometric, semantic, and visual representations. The resulting model, OmniVLA, achieves strong generalization to unseen environments, robustness to scarce modalities, and the ability to follow novel natural language instructions. We demonstrate that OmniVLA outperforms specialist baselines across modalities and offers a flexible foundation for fine-tuning to new modalities and tasks. We believe OmniVLA provides a step toward broadly generalizable and flexible navigation policies, and a scalable path for building omni-modal robotic foundation models. We present videos showcasing OmniVLA performance and will release its checkpoints and training code on our project page.

