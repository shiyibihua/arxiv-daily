---
layout: default
title: Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning
---

# Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19573" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19573v1</a>
  <a href="https://arxiv.org/pdf/2509.19573.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19573v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19573v1', 'Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zachary Olkin, Kejun Li, William D. Compton, Aaron D. Ames

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-23

**å¤‡æ³¨**: Submitted to ICRA 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ§åˆ¶Lyapunovå‡½æ•°å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ç°äººå½¢æœºå™¨äººç¨³å®šå¥”è·‘**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `äººå½¢æœºå™¨äºº` `å¼ºåŒ–å­¦ä¹ ` `æ§åˆ¶Lyapunovå‡½æ•°` `è¿åŠ¨æ§åˆ¶` `å¥”è·‘` `éçº¿æ€§æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. äººå½¢æœºå™¨äººå¥”è·‘ç­‰é«˜åŠ¨æ€è¿åŠ¨æ§åˆ¶é¢ä¸´é²æ£’æ€§å’Œç²¾ç¡®æ€§æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥åº”å¯¹éçº¿æ€§æ··åˆåŠ¨åŠ›å­¦ã€‚
2. è®ºæ–‡æå‡ºCLF-RLæ–¹æ³•ï¼Œå°†æ§åˆ¶Lyapunovå‡½æ•°å’Œä¼˜åŒ–è½¨è¿¹èå…¥å¼ºåŒ–å­¦ä¹ ï¼Œå¼•å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œä¿è¯ç¨³å®šæ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨è·‘æ­¥æœºå’Œæˆ·å¤–ç¯å¢ƒä¸­ç¨³å®šå¥”è·‘ï¼Œå¹¶å¯¹æ‰°åŠ¨å…·æœ‰é²æ£’æ€§ï¼Œå®ç°å…¨å±€å‚è€ƒè·Ÿè¸ªã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è®©äººå½¢æœºå™¨äººå®ç°é«˜åŠ¨æ€è¡Œä¸ºï¼Œä¾‹å¦‚å¥”è·‘ï¼Œéœ€è¦è®¾è®¡æ—¢é²æ£’åˆç²¾ç¡®çš„æ§åˆ¶å™¨ï¼Œè¿™éå¸¸å›°éš¾ã€‚ç»å…¸æ§åˆ¶æ–¹æ³•ä¸ºè¿™ç±»ç³»ç»Ÿçš„ç¨³å®šæ€§æä¾›äº†å®è´µçš„è§è§£ï¼Œä½†ä¸ºéçº¿æ€§æ··åˆåŠ¨åŠ›å­¦ç³»ç»Ÿåˆæˆå®æ—¶æ§åˆ¶å™¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å› å…¶å¤„ç†å¤æ‚åŠ¨åŠ›å­¦ç³»ç»Ÿçš„èƒ½åŠ›è€Œåœ¨è¿åŠ¨æ§åˆ¶é¢†åŸŸå¹¿å—æ¬¢è¿ã€‚æœ¬æ–‡å°†éçº¿æ€§æ§åˆ¶ç†è®ºä¸­çš„æ§åˆ¶Lyapunovå‡½æ•°ï¼ˆCLFï¼‰ä»¥åŠä¼˜åŒ–çš„åŠ¨æ€å‚è€ƒè½¨è¿¹åµŒå…¥åˆ°å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»¥å¡‘é€ å¥–åŠ±å‡½æ•°ã€‚è¿™ç§CLF-RLæ–¹æ³•æ— éœ€æ‰‹å·¥è®¾è®¡å’Œè°ƒæ•´å¯å‘å¼å¥–åŠ±é¡¹ï¼ŒåŒæ—¶é¼“åŠ±å¯è¯æ˜çš„ç¨³å®šæ€§ï¼Œå¹¶æä¾›æœ‰æ„ä¹‰çš„ä¸­é—´å¥–åŠ±æ¥æŒ‡å¯¼å­¦ä¹ ã€‚é€šè¿‡å°†ç­–ç•¥å­¦ä¹ å»ºç«‹åœ¨åŠ¨æ€å¯è¡Œçš„è½¨è¿¹ä¸Šï¼Œæ‰©å±•äº†æœºå™¨äººçš„åŠ¨æ€èƒ½åŠ›ï¼Œå¹¶å®ç°äº†åŒ…æ‹¬é£è¡Œå’Œå•æ”¯æ’‘é˜¶æ®µçš„å¥”è·‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨è·‘æ­¥æœºå’Œæˆ·å¤–ç¯å¢ƒä¸­éƒ½èƒ½å¯é è¿è¡Œï¼Œå¯¹èº¯å¹²å’Œè„šéƒ¨çš„æ‰°åŠ¨å…·æœ‰é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œå®ƒä»…ä½¿ç”¨æ¿è½½ä¼ æ„Ÿå™¨å³å¯å®ç°ç²¾ç¡®çš„å…¨å±€å‚è€ƒè·Ÿè¸ªï¼Œè¿™æ˜¯å°†è¿™äº›åŠ¨æ€è¿åŠ¨é›†æˆåˆ°å®Œæ•´è‡ªä¸»ç³»ç»Ÿä¸­çš„å…³é”®ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³äººå½¢æœºå™¨äººå¥”è·‘æ§åˆ¶é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨ä¿è¯ç¨³å®šæ€§çš„åŒæ—¶å®ç°é«˜åŠ¨æ€çš„å¥”è·‘è¿åŠ¨ã€‚ä¼ ç»Ÿæ§åˆ¶æ–¹æ³•éš¾ä»¥å¤„ç†å¤æ‚çš„éçº¿æ€§æ··åˆåŠ¨åŠ›å­¦ï¼Œè€Œçº¯ç²¹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•éœ€è¦æ‰‹åŠ¨è®¾è®¡å’Œè°ƒæ•´å¤æ‚çš„å¥–åŠ±å‡½æ•°ï¼Œç¼ºä¹ç†è®ºä¿è¯ï¼Œä¸”è®­ç»ƒæ•ˆç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†éçº¿æ€§æ§åˆ¶ç†è®ºä¸­çš„æ§åˆ¶Lyapunovå‡½æ•°ï¼ˆCLFï¼‰èå…¥åˆ°å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°è®¾è®¡ä¸­ã€‚CLFèƒ½å¤Ÿæä¾›ç³»ç»Ÿç¨³å®šæ€§çš„åº¦é‡ï¼Œå°†å…¶ä½œä¸ºå¥–åŠ±å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œå¯ä»¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ ç®—æ³•å­¦ä¹ åˆ°ç¨³å®šçš„æ§åˆ¶ç­–ç•¥ã€‚åŒæ—¶ï¼Œåˆ©ç”¨ä¼˜åŒ–çš„åŠ¨æ€å‚è€ƒè½¨è¿¹ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ æä¾›ä¸€ä¸ªåŠ¨æ€å¯è¡Œçš„ç›®æ ‡ï¼ŒåŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) åŠ¨æ€å‚è€ƒè½¨è¿¹ç”Ÿæˆå™¨ï¼Œç”¨äºç”ŸæˆæœŸæœ›çš„å¥”è·‘è½¨è¿¹ï¼›2) åŸºäºCLFçš„å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œå°†CLFå€¼ä½œä¸ºå¥–åŠ±çš„ä¸€éƒ¨åˆ†ï¼Œé¼“åŠ±ç­–ç•¥çš„ç¨³å®šæ€§ï¼›3) å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ï¼Œä½¿ç”¨è®¾è®¡çš„å¥–åŠ±å‡½æ•°è®­ç»ƒæ§åˆ¶ç­–ç•¥ã€‚è®­ç»ƒå®Œæˆåï¼Œå°†ç­–ç•¥éƒ¨ç½²åˆ°äººå½¢æœºå™¨äººä¸Šè¿›è¡Œå¥”è·‘å®éªŒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ§åˆ¶ç†è®ºä¸­çš„CLFä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±å‡½æ•°è®¾è®¡æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„å¯å‘å¼å¥–åŠ±å‡½æ•°ç›¸æ¯”ï¼ŒCLFèƒ½å¤Ÿæä¾›ç³»ç»Ÿç¨³å®šæ€§çš„ç†è®ºä¿è¯ï¼Œé¿å…äº†æ‰‹åŠ¨è°ƒæ•´å¥–åŠ±å‡½æ•°çš„ç¹çè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œç»“åˆä¼˜åŒ–çš„åŠ¨æ€å‚è€ƒè½¨è¿¹ï¼Œèƒ½å¤ŸåŠ é€Ÿå¼ºåŒ–å­¦ä¹ çš„æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶æé«˜ç­–ç•¥çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼ŒCLFçš„é€‰æ‹©è‡³å…³é‡è¦ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„æœºå™¨äººåŠ¨åŠ›å­¦æ¨¡å‹è¿›è¡Œè®¾è®¡ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡åŒ…æ‹¬CLFé¡¹ã€è½¨è¿¹è·Ÿè¸ªé¡¹å’ŒåŠ¨ä½œæƒ©ç½šé¡¹ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯ä»¥é€‰æ‹©å¸¸è§çš„ç®—æ³•ï¼Œå¦‚PPOæˆ–SACã€‚åŠ¨æ€å‚è€ƒè½¨è¿¹çš„ç”Ÿæˆå¯ä»¥ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚ç›´æ¥æ­é…æ³•æˆ–å¤šé¡¹å¼æ’å€¼ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å®éªŒç»“æœè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è®­ç»ƒå¾—åˆ°çš„ç­–ç•¥èƒ½å¤Ÿåœ¨è·‘æ­¥æœºå’Œæˆ·å¤–ç¯å¢ƒä¸­ç¨³å®šå¥”è·‘ï¼Œå¹¶å¯¹èº¯å¹²å’Œè„šéƒ¨çš„æ‰°åŠ¨å…·æœ‰é²æ£’æ€§ã€‚æœºå™¨äººèƒ½å¤Ÿä»…ä½¿ç”¨æ¿è½½ä¼ æ„Ÿå™¨å®ç°ç²¾ç¡®çš„å…¨å±€å‚è€ƒè·Ÿè¸ªã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›ï¼Œå¹¶è·å¾—æ›´é«˜çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äººå½¢æœºå™¨äººå¥”è·‘æ§åˆ¶æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºäººå½¢æœºå™¨äººçš„è¿åŠ¨æ§åˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡ï¼Œä¾‹å¦‚æœæ•‘ã€å·¡æ£€å’Œè¾…åŠ©åŒ»ç–—ã€‚é€šè¿‡æé«˜äººå½¢æœºå™¨äººçš„è¿åŠ¨èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œå¯ä»¥æ‰©å±•å…¶åº”ç”¨èŒƒå›´ï¼Œä½¿å…¶åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥ä½œç”¨ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´é«˜çº§çš„è‡ªä¸»å¯¼èˆªå’Œäººæœºåä½œä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving highly dynamic behaviors on humanoid robots, such as running, requires controllers that are both robust and precise, and hence difficult to design. Classical control methods offer valuable insight into how such systems can stabilize themselves, but synthesizing real-time controllers for nonlinear and hybrid dynamics remains challenging. Recently, reinforcement learning (RL) has gained popularity for locomotion control due to its ability to handle these complex dynamics. In this work, we embed ideas from nonlinear control theory, specifically control Lyapunov functions (CLFs), along with optimized dynamic reference trajectories into the reinforcement learning training process to shape the reward. This approach, CLF-RL, eliminates the need to handcraft and tune heuristic reward terms, while simultaneously encouraging certifiable stability and providing meaningful intermediate rewards to guide learning. By grounding policy learning in dynamically feasible trajectories, we expand the robot's dynamic capabilities and enable running that includes both flight and single support phases. The resulting policy operates reliably on a treadmill and in outdoor environments, demonstrating robustness to disturbances applied to the torso and feet. Moreover, it achieves accurate global reference tracking utilizing only on-board sensors, making a critical step toward integrating these dynamic motions into a full autonomy stack.

