---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-09-23
---

# cs.ROï¼ˆ2025-09-23ï¼‰

ğŸ“Š å…± **45** ç¯‡è®ºæ–‡
 | ğŸ”— **8** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (36 ğŸ”—6)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (36 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250919545v1-romoco-robotic-motion-control-toolbox-for-reduced-order-model-based-.html">RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots</a></td>
  <td>RoMoCoï¼šç”¨äºåŒè¶³å’Œäººå½¢æœºå™¨äººåŸºäºé™é˜¶æ¨¡å‹çš„è¿åŠ¨æ§åˆ¶å·¥å…·ç®±</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">bipedal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19545v1" data-paper-url="./papers/250919545v1-romoco-robotic-motion-control-toolbox-for-reduced-order-model-based-.html" onclick="toggleFavorite(this, '2509.19545v1', 'RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250918953v1-eva-vla-evaluating-vision-language-action-models-robustness-under-re.html">Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations</a></td>
  <td>Eva-VLAï¼šè¯„ä¼°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨çœŸå®ç‰©ç†å˜åŒ–ä¸‹çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18953v1" data-paper-url="./papers/250918953v1-eva-vla-evaluating-vision-language-action-models-robustness-under-re.html" onclick="toggleFavorite(this, '2509.18953v1', 'Eva-VLA: Evaluating Vision-Language-Action Models&#39; Robustness Under Real-World Physical Variations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250919571v1-agentic-scene-policies-unifying-space-semantics-and-affordances-for-.html">Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action</a></td>
  <td>æå‡ºAgentic Scene Policiesä»¥è§£å†³å¤æ‚æŒ‡ä»¤ä¸‹çš„æœºå™¨äººåŠ¨ä½œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion planning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19571v1" data-paper-url="./papers/250919571v1-agentic-scene-policies-unifying-space-semantics-and-affordances-for-.html" onclick="toggleFavorite(this, '2509.19571v1', 'Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250919023v1-reduced-order-model-guided-reinforcement-learning-for-demonstration-.html">Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion</a></td>
  <td>æå‡ºåŸºäºé™é˜¶æ¨¡å‹å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ç°æ— éœ€æ¼”ç¤ºçš„äººå½¢æœºå™¨äººè¿åŠ¨æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid locomotion</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19023v1" data-paper-url="./papers/250919023v1-reduced-order-model-guided-reinforcement-learning-for-demonstration-.html" onclick="toggleFavorite(this, '2509.19023v1', 'Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250918757v1-mv-umi-a-scalable-multi-view-interface-for-cross-embodiment-learning.html">MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning</a></td>
  <td>MV-UMIï¼šç”¨äºè·¨å…·èº«å­¦ä¹ çš„å¯æ‰©å±•å¤šè§†è§’äº¤äº’ç•Œé¢</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">teleoperation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18757v1" data-paper-url="./papers/250918757v1-mv-umi-a-scalable-multi-view-interface-for-cross-embodiment-learning.html" onclick="toggleFavorite(this, '2509.18757v1', 'MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250919012v3-pure-vision-language-action-vla-models-a-comprehensive-survey.html">Pure Vision Language Action (VLA) Models: A Comprehensive Survey</a></td>
  <td>VLAæ¨¡å‹ç»¼è¿°ï¼šå°†è§†è§‰è¯­è¨€æ¨¡å‹ä»åºåˆ—ç”Ÿæˆå™¨è½¬å˜ä¸ºæœºå™¨äººæ§åˆ¶çš„ä¸»åŠ¨Agent</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19012v3" data-paper-url="./papers/250919012v3-pure-vision-language-action-vla-models-a-comprehensive-survey.html" onclick="toggleFavorite(this, '2509.19012v3', 'Pure Vision Language Action (VLA) Models: A Comprehensive Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250919080v1-world4rl-diffusion-world-models-for-policy-refinement-with-reinforce.html">World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation</a></td>
  <td>World4RLï¼šåˆ©ç”¨æ‰©æ•£ä¸–ç•Œæ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ æ”¹è¿›æœºå™¨äººæ“ä½œç­–ç•¥</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">sim-to-real</span> <span class="paper-tag">reinforcement learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19080v1" data-paper-url="./papers/250919080v1-world4rl-diffusion-world-models-for-policy-refinement-with-reinforce.html" onclick="toggleFavorite(this, '2509.19080v1', 'World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250918778v1-vggt-dp-generalizable-robot-control-via-vision-foundation-models.html">VGGT-DP: Generalizable Robot Control via Vision Foundation Models</a></td>
  <td>æå‡ºVGGT-DPï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æå‡æœºå™¨äººæ“ä½œæŠ€èƒ½çš„æ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">VGGT</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18778v1" data-paper-url="./papers/250918778v1-vggt-dp-generalizable-robot-control-via-vision-foundation-models.html" onclick="toggleFavorite(this, '2509.18778v1', 'VGGT-DP: Generalizable Robot Control via Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250919626v1-egobridge-domain-adaptation-for-generalizable-imitation-from-egocent.html">EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data</a></td>
  <td>EgoBridgeï¼šåˆ©ç”¨é¢†åŸŸè‡ªé€‚åº”å®ç°ä»ç¬¬ä¸€è§†è§’äººç±»æ•°æ®ä¸­æ³›åŒ–æ¨¡ä»¿å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">bimanual manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19626v1" data-paper-url="./papers/250919626v1-egobridge-domain-adaptation-for-generalizable-imitation-from-egocent.html" onclick="toggleFavorite(this, '2509.19626v1', 'EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250919047v1-manipforce-force-guided-policy-learning-with-frequency-aware-represe.html">ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation</a></td>
  <td>ManipForceï¼šæå‡ºåŠ›å¼•å¯¼çš„ç­–ç•¥å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ¥è§¦å¼æ“ä½œä»»åŠ¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19047v1" data-paper-url="./papers/250919047v1-manipforce-force-guided-policy-learning-with-frequency-aware-represe.html" onclick="toggleFavorite(this, '2509.19047v1', 'ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250919573v1-chasing-stability-humanoid-running-via-control-lyapunov-function-gui.html">Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºæ§åˆ¶Lyapunovå‡½æ•°å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ç°äººå½¢æœºå™¨äººç¨³å®šå¥”è·‘</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19573v1" data-paper-url="./papers/250919573v1-chasing-stability-humanoid-running-via-control-lyapunov-function-gui.html" onclick="toggleFavorite(this, '2509.19573v1', 'Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250918865v1-bi-vla-bilateral-control-based-imitation-learning-via-vision-languag.html">Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation</a></td>
  <td>æå‡ºBi-VLAï¼Œé€šè¿‡è§†è§‰-è¯­è¨€èåˆçš„æ¨¡ä»¿å­¦ä¹ ï¼Œè§£å†³æœºå™¨äººå•æ¨¡å‹å¤šä»»åŠ¡åŠ¨ä½œç”Ÿæˆé—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18865v1" data-paper-url="./papers/250918865v1-bi-vla-bilateral-control-based-imitation-learning-via-vision-languag.html" onclick="toggleFavorite(this, '2509.18865v1', 'Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250918610v1-singer-an-onboard-generalist-vision-language-navigation-policy-for-d.html">SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones</a></td>
  <td>SINGERï¼šä¸€ç§ç”¨äºæ— äººæœºçš„é€šç”¨è§†è§‰-è¯­è¨€å¯¼èˆªç­–ç•¥ï¼Œä»…ä½¿ç”¨æœºè½½ä¼ æ„Ÿå™¨ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">sim-to-real</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18610v1" data-paper-url="./papers/250918610v1-singer-an-onboard-generalist-vision-language-navigation-policy-for-d.html" onclick="toggleFavorite(this, '2509.18610v1', 'SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250919102v1-funcanon-learning-pose-aware-action-primitives-via-functional-object.html">FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation</a></td>
  <td>FUNCanonï¼šé€šè¿‡åŠŸèƒ½å¯¹è±¡è§„èŒƒåŒ–å­¦ä¹ å§¿æ€æ„ŸçŸ¥åŠ¨ä½œåŸè¯­ï¼Œå®ç°é€šç”¨æœºå™¨äººæ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">sim2real</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19102v1" data-paper-url="./papers/250919102v1-funcanon-learning-pose-aware-action-primitives-via-functional-object.html" onclick="toggleFavorite(this, '2509.19102v1', 'FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250919301v2-residual-off-policy-rl-for-finetuning-behavior-cloning-policies.html">Residual Off-Policy RL for Finetuning Behavior Cloning Policies</a></td>
  <td>æå‡ºæ®‹å·®ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå¾®è°ƒè¡Œä¸ºå…‹éš†ç­–ç•¥ï¼Œå®ç°é«˜è‡ªç”±åº¦æœºå™¨äººçµå·§æ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19301v2" data-paper-url="./papers/250919301v2-residual-off-policy-rl-for-finetuning-behavior-cloning-policies.html" onclick="toggleFavorite(this, '2509.19301v2', 'Residual Off-Policy RL for Finetuning Behavior Cloning Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250919261v1-imitation-guided-bimanual-planning-for-stable-manipulation-under-cha.html">Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces</a></td>
  <td>æå‡ºæ¨¡ä»¿å¼•å¯¼çš„åŒè‡‚è§„åˆ’æ¡†æ¶ï¼Œè§£å†³åŠ¨æ€ç¯å¢ƒä¸‹ç¨³å®šæ“ä½œçš„æŠ“å–è¿‡æ¸¡é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19261v1" data-paper-url="./papers/250919261v1-imitation-guided-bimanual-planning-for-stable-manipulation-under-cha.html" onclick="toggleFavorite(this, '2509.19261v1', 'Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250918609v1-pie-perception-and-interaction-enhanced-end-to-end-motion-planning-f.html">PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving</a></td>
  <td>PIEï¼šé¢å‘è‡ªåŠ¨é©¾é©¶ï¼Œæå‡ºæ„ŸçŸ¥äº¤äº’å¢å¼ºçš„ç«¯åˆ°ç«¯è¿åŠ¨è§„åˆ’æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">Mamba</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18609v1" data-paper-url="./papers/250918609v1-pie-perception-and-interaction-enhanced-end-to-end-motion-planning-f.html" onclick="toggleFavorite(this, '2509.18609v1', 'PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250919454v1-ropa-synthetic-robot-pose-generation-for-rgb-d-bimanual-data-augment.html">ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation</a></td>
  <td>ROPAï¼šç”¨äºRGB-DåŒè‡‚æ“ä½œæ•°æ®å¢å¼ºçš„åˆæˆæœºå™¨äººå§¿æ€ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">bimanual manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19454v1" data-paper-url="./papers/250919454v1-ropa-synthetic-robot-pose-generation-for-rgb-d-bimanual-data-augment.html" onclick="toggleFavorite(this, '2509.19454v1', 'ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250919521v1-a-bimanual-gesture-interface-for-ros-based-mobile-manipulators-using.html">A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion</a></td>
  <td>æå‡ºåŸºäºTinyMLå’Œä¼ æ„Ÿå™¨èåˆçš„åŒæ‰‹åŠ¨åŠ¿æ¥å£ï¼Œç”¨äºROSç§»åŠ¨æœºæ¢°è‡‚æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19521v1" data-paper-url="./papers/250919521v1-a-bimanual-gesture-interface-for-ros-based-mobile-manipulators-using.html" onclick="toggleFavorite(this, '2509.19521v1', 'A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250918676v1-3d-flow-diffusion-policy-visuomotor-policy-learning-via-generating-f.html">3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space</a></td>
  <td>æå‡º3D FDPï¼Œé€šè¿‡ç”Ÿæˆ3Dç©ºé—´ä¸­çš„Flowå­¦ä¹ é€šç”¨æœºå™¨äººæ“ä½œç­–ç•¥ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18676v1" data-paper-url="./papers/250918676v1-3d-flow-diffusion-policy-visuomotor-policy-learning-via-generating-f.html" onclick="toggleFavorite(this, '2509.18676v1', '3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250919142v1-bigraspformer-end-to-end-bimanual-grasp-transformer.html">BiGraspFormer: End-to-End Bimanual Grasp Transformer</a></td>
  <td>BiGraspFormerï¼šç«¯åˆ°ç«¯åŒè‡‚æŠ“å–Transformerç½‘ç»œï¼Œè§£å†³å¤æ‚ç‰©ä½“æ“ä½œä¸­çš„åè°ƒé—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">bimanual manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19142v1" data-paper-url="./papers/250919142v1-bigraspformer-end-to-end-bimanual-grasp-transformer.html" onclick="toggleFavorite(this, '2509.19142v1', 'BiGraspFormer: End-to-End Bimanual Grasp Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250918937v1-lang2morph-language-driven-morphological-design-of-robotic-hands.html">Lang2Morph: Language-Driven Morphological Design of Robotic Hands</a></td>
  <td>Lang2Morphï¼šæå‡ºä¸€ç§åŸºäºè¯­è¨€é©±åŠ¨çš„æœºå™¨äººæ‰‹éƒ¨å½¢æ€è‡ªåŠ¨è®¾è®¡æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">human-object interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18937v1" data-paper-url="./papers/250918937v1-lang2morph-language-driven-morphological-design-of-robotic-hands.html" onclick="toggleFavorite(this, '2509.18937v1', 'Lang2Morph: Language-Driven Morphological Design of Robotic Hands')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250918648v4-spidr-a-simple-approach-for-zero-shot-safety-in-sim-to-real-transfer.html">SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer</a></td>
  <td>SPiDRï¼šä¸€ç§åŸºäºæ‚²è§‚åŸŸéšæœºåŒ–çš„ç®€å•é›¶æ ·æœ¬å®‰å…¨Sim-to-Realè¿ç§»æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">domain randomization</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18648v4" data-paper-url="./papers/250918648v4-spidr-a-simple-approach-for-zero-shot-safety-in-sim-to-real-transfer.html" onclick="toggleFavorite(this, '2509.18648v4', 'SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250918506v1-spatial-envelope-mpc-high-performance-driving-without-a-reference.html">Spatial Envelope MPC: High Performance Driving without a Reference</a></td>
  <td>æå‡ºåŸºäºç©ºé—´åŒ…ç»œçš„MPCæ¡†æ¶ï¼Œæ— éœ€å‚è€ƒè½¨è¿¹å®ç°é«˜æ€§èƒ½è‡ªåŠ¨é©¾é©¶ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18506v1" data-paper-url="./papers/250918506v1-spatial-envelope-mpc-high-performance-driving-without-a-reference.html" onclick="toggleFavorite(this, '2509.18506v1', 'Spatial Envelope MPC: High Performance Driving without a Reference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250918597v2-growing-with-your-embodied-agent-a-human-in-the-loop-lifelong-code-g.html">Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills</a></td>
  <td>æå‡ºäººæœºåä½œçš„ç»ˆèº«ä»£ç ç”Ÿæˆæ¡†æ¶ï¼Œæå‡é•¿æ—¶ç¨‹æ“ä½œæŠ€èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18597v2" data-paper-url="./papers/250918597v2-growing-with-your-embodied-agent-a-human-in-the-loop-lifelong-code-g.html" onclick="toggleFavorite(this, '2509.18597v2', 'Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250918644v2-do-you-need-proprioceptive-states-in-visuomotor-policies.html">Do You Need Proprioceptive States in Visuomotor Policies?</a></td>
  <td>æå‡ºState-freeç­–ç•¥ï¼Œè§£å†³åŸºäºæ¨¡ä»¿å­¦ä¹ çš„æœºå™¨äººæ“ä½œä¸­å¯¹æœ¬ä½“æ„Ÿå—çŠ¶æ€çš„è¿‡åº¦ä¾èµ–é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">whole-body manipulation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18644v2" data-paper-url="./papers/250918644v2-do-you-need-proprioceptive-states-in-visuomotor-policies.html" onclick="toggleFavorite(this, '2509.18644v2', 'Do You Need Proprioceptive States in Visuomotor Policies?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250918760v1-guaranteed-robust-nonlinear-mpc-via-disturbance-feedback.html">Guaranteed Robust Nonlinear MPC via Disturbance Feedback</a></td>
  <td>æå‡ºåŸºäºæ‰°åŠ¨åé¦ˆçš„é²æ£’éçº¿æ€§MPCï¼Œä¿éšœæœºå™¨äººå®‰å…¨çº¦æŸä¸ç¨³å®šæ€§</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18760v1" data-paper-url="./papers/250918760v1-guaranteed-robust-nonlinear-mpc-via-disturbance-feedback.html" onclick="toggleFavorite(this, '2509.18760v1', 'Guaranteed Robust Nonlinear MPC via Disturbance Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250918671v1-n2m-bridging-navigation-and-manipulation-by-learning-pose-preference.html">N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout</a></td>
  <td>æå‡ºN2Mä»¥è§£å†³ç§»åŠ¨æ“ä½œä¸­å¯¼èˆªä¸æ“ä½œä¸ä¸€è‡´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18671v1" data-paper-url="./papers/250918671v1-n2m-bridging-navigation-and-manipulation-by-learning-pose-preference.html" onclick="toggleFavorite(this, '2509.18671v1', 'N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250918830v1-dexskin-high-coverage-conformable-robotic-skin-for-learning-contact-.html">DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation</a></td>
  <td>æå‡ºDexSkinä»¥è§£å†³æœºå™¨äººè§¦è§‰æ„ŸçŸ¥ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18830v1" data-paper-url="./papers/250918830v1-dexskin-high-coverage-conformable-robotic-skin-for-learning-contact-.html" onclick="toggleFavorite(this, '2509.18830v1', 'DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250918666v1-distributionally-robust-safe-motion-planning-with-contextual-informa.html">Distributionally Robust Safe Motion Planning with Contextual Information</a></td>
  <td>æå‡ºä¸€ç§åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯çš„åˆ†å¸ƒé²æ£’å®‰å…¨è¿åŠ¨è§„åˆ’æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„é¿éšœé—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18666v1" data-paper-url="./papers/250918666v1-distributionally-robust-safe-motion-planning-with-contextual-informa.html" onclick="toggleFavorite(this, '2509.18666v1', 'Distributionally Robust Safe Motion Planning with Contextual Information')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250919169v1-magiclaw-a-dual-use-vision-based-soft-gripper-for-bridging-the-human.html">MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap</a></td>
  <td>æå‡ºMagiClawä»¥è§£å†³äººç±»ç¤ºèŒƒä¸æœºå™¨äººæ‰§è¡Œä¹‹é—´çš„é¢†åŸŸå·®è·é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">teleoperation</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19169v1" data-paper-url="./papers/250919169v1-magiclaw-a-dual-use-vision-based-soft-gripper-for-bridging-the-human.html" onclick="toggleFavorite(this, '2509.19169v1', 'MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250919105v2-spectral-signature-mapping-from-rgb-imagery-for-terrain-aware-naviga.html">Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation</a></td>
  <td>æå‡ºRS-Netï¼Œåˆ©ç”¨RGBå›¾åƒé¢„æµ‹å…‰è°±ç‰¹å¾ï¼Œå®ç°åœ°å½¢æ„ŸçŸ¥å¯¼èˆª</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">MPC</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19105v2" data-paper-url="./papers/250919105v2-spectral-signature-mapping-from-rgb-imagery-for-terrain-aware-naviga.html" onclick="toggleFavorite(this, '2509.19105v2', 'Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250918631v2-generalizable-domain-adaptation-for-sim-and-real-policy-co-training.html">Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training</a></td>
  <td>æå‡ºåŸºäºæœ€ä¼˜ä¼ è¾“çš„Sim-to-Realç­–ç•¥ååŒè®­ç»ƒæ¡†æ¶ï¼Œæå‡æœºå™¨äººæ“ä½œæ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">behavior cloning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18631v2" data-paper-url="./papers/250918631v2-generalizable-domain-adaptation-for-sim-and-real-policy-co-training.html" onclick="toggleFavorite(this, '2509.18631v2', 'Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250919610v1-look-as-you-leap-planning-simultaneous-motion-and-perception-for-hig.html">Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots</a></td>
  <td>æå‡ºåŸºäºç¥ç»ä»£ç†æ¨¡å‹çš„GPUå¹¶è¡Œæ„ŸçŸ¥è¯„åˆ†å¼•å¯¼æ¦‚ç‡è·¯çº¿å›¾è§„åˆ’å™¨ï¼Œè§£å†³é«˜è‡ªç”±åº¦æœºå™¨äººè¿åŠ¨æ„ŸçŸ¥ååŒè§„åˆ’é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19610v1" data-paper-url="./papers/250919610v1-look-as-you-leap-planning-simultaneous-motion-and-perception-for-hig.html" onclick="toggleFavorite(this, '2509.19610v1', 'Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250919473v1-crater-observing-bio-inspired-rolling-articulator-cobra.html">Crater Observing Bio-inspired Rolling Articulator (COBRA)</a></td>
  <td>COBRAï¼šä¸€ç§ç”¨äºæœˆçƒé™¨çŸ³å‘æ¢ç´¢çš„ä»¿ç”Ÿæ»šåŠ¨å…³èŠ‚æœºå™¨äºº</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19473v1" data-paper-url="./papers/250919473v1-crater-observing-bio-inspired-rolling-articulator-cobra.html" onclick="toggleFavorite(this, '2509.19473v1', 'Crater Observing Bio-inspired Rolling Articulator (COBRA)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250919292v1-soe-sample-efficient-robot-policy-self-improvement-via-on-manifold-e.html">SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration</a></td>
  <td>SOEï¼šåŸºäºæµå½¢æ¢ç´¢çš„æœºå™¨äººç­–ç•¥è‡ªæå‡ï¼Œæå‡é‡‡æ ·æ•ˆç‡ä¸å®‰å…¨æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19292v1" data-paper-url="./papers/250919292v1-soe-sample-efficient-robot-policy-self-improvement-via-on-manifold-e.html" onclick="toggleFavorite(this, '2509.19292v1', 'SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>37</td>
  <td><a href="./papers/250918576v1-lcmf-lightweight-cross-modality-mambaformer-for-embodied-robotics-vq.html">LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA</a></td>
  <td>æå‡ºè½»é‡çº§è·¨æ¨¡æ€Mambaformerï¼ˆLCMFï¼‰ï¼Œç”¨äºå…·èº«æœºå™¨äººVQAä»»åŠ¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18576v1" data-paper-url="./papers/250918576v1-lcmf-lightweight-cross-modality-mambaformer-for-embodied-robotics-vq.html" onclick="toggleFavorite(this, '2509.18576v1', 'LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250918608v2-end-to-end-crop-row-navigation-via-lidar-based-deep-reinforcement-le.html">End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºLiDARå’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç«¯åˆ°ç«¯ä½œç‰©è¡Œå¯¼èˆªæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18608v2" data-paper-url="./papers/250918608v2-end-to-end-crop-row-navigation-via-lidar-based-deep-reinforcement-le.html" onclick="toggleFavorite(this, '2509.18608v2', 'End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250919460v1-self-evolved-imitation-learning-in-simulated-world.html">Self-evolved Imitation Learning in Simulated World</a></td>
  <td>æå‡ºè‡ªè¿›åŒ–æ¨¡ä»¿å­¦ä¹ æ¡†æ¶SEILï¼Œè§£å†³å°‘æ ·æœ¬æ¨¡ä»¿å­¦ä¹ ä¸­ä¸“å®¶æ•°æ®åŒ®ä¹é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">generalist agent</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19460v1" data-paper-url="./papers/250919460v1-self-evolved-imitation-learning-in-simulated-world.html" onclick="toggleFavorite(this, '2509.19460v1', 'Self-evolved Imitation Learning in Simulated World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250919525v1-real-time-reinforcement-learning-for-dynamic-tasks-with-a-parallel-s.html">Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot</a></td>
  <td>æå‡ºåŸºäºè¯¾ç¨‹å­¦ä¹ çš„æœ€å¤§æ‰©æ•£å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°è½¯ä½“æœºå™¨äººåœ¨åŠ¨æ€ä»»åŠ¡ä¸­çš„å®æ—¶å•æ¬¡éƒ¨ç½²æ§åˆ¶ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">curriculum learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19525v1" data-paper-url="./papers/250919525v1-real-time-reinforcement-learning-for-dynamic-tasks-with-a-parallel-s.html" onclick="toggleFavorite(this, '2509.19525v1', 'Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250918686v1-query-centric-diffusion-policy-for-generalizable-robotic-assembly.html">Query-Centric Diffusion Policy for Generalizable Robotic Assembly</a></td>
  <td>æå‡ºQuery-centric Diffusion Policyï¼Œè§£å†³æœºå™¨äººè£…é…ä¸­é«˜å±‚è§„åˆ’ä¸åº•å±‚æ§åˆ¶çš„é¸¿æ²Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18686v1" data-paper-url="./papers/250918686v1-query-centric-diffusion-policy-for-generalizable-robotic-assembly.html" onclick="toggleFavorite(this, '2509.18686v1', 'Query-Centric Diffusion Policy for Generalizable Robotic Assembly')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>42</td>
  <td><a href="./papers/250918592v1-vln-zero-rapid-exploration-and-cache-enabled-neurosymbolic-vision-la.html">VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation</a></td>
  <td>VLN-Zeroï¼šé¢å‘æœºå™¨äººå¯¼èˆªé›¶æ ·æœ¬è¿ç§»çš„å¿«é€Ÿæ¢ç´¢ä¸ç¼“å­˜ç¥ç»ç¬¦å·è§†è§‰è¯­è¨€è§„åˆ’</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span> <span class="paper-tag">zero-shot transfer</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18592v1" data-paper-url="./papers/250918592v1-vln-zero-rapid-exploration-and-cache-enabled-neurosymbolic-vision-la.html" onclick="toggleFavorite(this, '2509.18592v1', 'VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250919168v1-a-multimodal-stochastic-planning-approach-for-navigation-and-multi-r.html">A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination</a></td>
  <td>æå‡ºä¸€ç§å¤šæ¨¡æ€éšæœºè§„åˆ’æ–¹æ³•ï¼Œç”¨äºå¯¼èˆªå’Œå¤šæœºå™¨äººåè°ƒã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19168v1" data-paper-url="./papers/250919168v1-a-multimodal-stochastic-planning-approach-for-navigation-and-multi-r.html" onclick="toggleFavorite(this, '2509.19168v1', 'A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>44</td>
  <td><a href="./papers/250919480v1-omnivla-an-omni-modal-vision-language-action-model-for-robot-navigat.html">OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation</a></td>
  <td>OmniVLAï¼šç”¨äºæœºå™¨äººå¯¼èˆªçš„é€šç”¨æ¨¡æ€è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19480v1" data-paper-url="./papers/250919480v1-omnivla-an-omni-modal-vision-language-action-model-for-robot-navigat.html" onclick="toggleFavorite(this, '2509.19480v1', 'OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>45</td>
  <td><a href="./papers/250919579v1-terra-hierarchical-terrain-aware-3d-scene-graph-for-task-agnostic-ou.html">Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping</a></td>
  <td>Terraï¼šé¢å‘ä»»åŠ¡æ— å…³æˆ·å¤–å»ºå›¾çš„åˆ†å±‚åœ°å½¢æ„ŸçŸ¥3Dåœºæ™¯å›¾</td>
  <td class="tags-cell"><span class="paper-tag">traversability</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.19579v1" data-paper-url="./papers/250919579v1-terra-hierarchical-terrain-aware-3d-scene-graph-for-task-agnostic-ou.html" onclick="toggleFavorite(this, '2509.19579v1', 'Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)