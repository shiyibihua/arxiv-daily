---
layout: default
title: Human-assisted Robotic Policy Refinement via Action Preference Optimization
---

# Human-assisted Robotic Policy Refinement via Action Preference Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.07127" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.07127v3</a>
  <a href="https://arxiv.org/pdf/2506.07127.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.07127v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.07127v3', 'Human-assisted Robotic Policy Refinement via Action Preference Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Wenke Xia, Yichu Yang, Hongtao Wu, Xiao Ma, Tao Kong, Di Hu

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-08 (Êõ¥Êñ∞: 2025-10-30)

**Â§áÊ≥®**: Accepted By NeurIPS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/GeWu-Lab/Action-Preference-Optimization)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∫∫Êú∫Âçè‰ΩúÁöÑÂä®‰ΩúÂÅèÂ•Ω‰ºòÂåñ‰ª•ÊèêÂçáÊú∫Âô®‰∫∫Á≠ñÁï•Á≤æÂ∫¶**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰∫∫Êú∫Âçè‰Ωú` `Âä®‰ΩúÂÅèÂ•Ω‰ºòÂåñ` `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `Êú∫Âô®‰∫∫Â≠¶‰π†` `Âä®ÊÄÅÁéØÂ¢É` `Á≠ñÁï•‰ºòÂåñ` `Â§±Ë¥•‰øÆÊ≠£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑVLAÊ®°Âûã‰æùËµñÁ¶ªÁ∫ø‰∏ìÂÆ∂ÊºîÁ§∫ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂêéÊúü‰ºòÂåñËÉΩÂäõÔºåÂØºËá¥Êú∫Âô®‰∫∫Âú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞‰∏çÁ®≥ÂÆö„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑÂä®‰ΩúÂÅèÂ•Ω‰ºòÂåñÔºàAPOÔºâÊñπÊ≥ïÔºåÈÄöËøá‰∫∫Êú∫Âçè‰ΩúÊî∂ÈõÜ‰∫§‰∫íÊï∞ÊçÆÔºåÂà©Áî®ÂÅèÂ•ΩÂØπÈΩêÊù•‰ºòÂåñVLAÊ®°ÂûãÔºåÊèêÂçáÂÖ∂ÈÄÇÂ∫îÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåAPOÊñπÊ≥ïÂú®Ê®°ÊãüÂíåÁúüÂÆûÂú∫ÊôØ‰∏≠ÂùáË°®Áé∞Âá∫‰ºòË∂äÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄßÔºåÁõ∏ËæÉ‰∫é‰º†ÁªüÊñπÊ≥ïÊúâÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âª∫Á´ã‰∏Ä‰∏™ÂèØÈù†‰∏îÂèØËø≠‰ª£‰ºòÂåñÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÂØπ‰∫éÂÆûÈôÖÂ∫îÁî®Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÂ∞ΩÁÆ°ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãË¢´ÂπøÊ≥õËÆ§ÂèØ‰∏∫Êú∫Âô®‰∫∫ÈÉ®ÁΩ≤ÁöÑÂü∫Á°ÄÊ®°ÂûãÔºå‰ΩÜÂÖ∂‰æùËµñÁ¶ªÁ∫ø‰∏ìÂÆ∂ÊºîÁ§∫ÁöÑÁâπÊÄßÈôêÂà∂‰∫ÜÂêéÊúü‰ºòÂåñËÉΩÂäõ„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÂä®‰ΩúÂÅèÂ•Ω‰ºòÂåñÔºàAPOÔºâÊñπÊ≥ïÔºåÈÄöËøá‰∏éÁéØÂ¢ÉÁöÑ‰∫§‰∫íÊî∂ÈõÜ‰∫∫Á±ªËæÖÂä©ÁöÑÂÅèÂ•ΩÂØπÈΩêÔºå‰ªéËÄå‰ºòÂåñVLAÊ®°Âûã„ÄÇËØ•ÊñπÊ≥ï‰ª•‰∫∫Êú∫Âçè‰ΩúÊ°ÜÊû∂‰∏∫Âü∫Á°ÄÔºåËøõË°åÂèØÈù†ÁöÑÂ§±Ë¥•‰øÆÊ≠£Âíå‰∫§‰∫íËΩ®ËøπÊî∂ÈõÜ„ÄÇAPOÂºïÂÖ•‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫îÈáçÂä†ÊùÉÁÆóÊ≥ïÔºåÂà©Áî®Êù•Ëá™‰∫§‰∫íÁöÑ‰∫åÂÖÉÂèØÂèñÊÄß‰ø°Âè∑ÔºåÊúâÊïàÊäëÂà∂Â§±Ë¥•ÂÄæÂêëÁöÑÂä®‰ΩúÂπ∂Â¢ûÂº∫Á∫†Ê≠£Âä®‰ΩúÁöÑÈÄÇÂ∫îÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®Â§öÁßçÊìç‰Ωú‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫‰ºòË∂äÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâVLAÊ®°ÂûãÂú®ÂêéÊúü‰ºòÂåñ‰∏≠ÁöÑÂ±ÄÈôêÊÄßÔºåÂ∞§ÂÖ∂ÊòØÁî±‰∫é‰æùËµñÁ¶ªÁ∫ø‰∏ìÂÆ∂ÊºîÁ§∫ËÄåÂØºËá¥ÁöÑÈÄÇÂ∫îÊÄß‰∏çË∂≥ÂíåÂ§±Ë¥•‰øÆÊ≠£ËÉΩÂäõÂº±ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫Âä®‰ΩúÂÅèÂ•Ω‰ºòÂåñÔºàAPOÔºâÊñπÊ≥ïÔºåÈÄöËøá‰∫∫Êú∫Âçè‰ΩúÊî∂ÈõÜ‰∫§‰∫íÊï∞ÊçÆÔºåÂà©Áî®Ëøô‰∫õÊï∞ÊçÆËøõË°åÂÅèÂ•ΩÂØπÈΩêÔºå‰ªéËÄå‰ºòÂåñVLAÊ®°ÂûãÁöÑÂÜ≥Á≠ñËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂº∫Ë∞É‰∫∫Á±ªÂú®Êú∫Âô®‰∫∫Â≠¶‰π†ËøáÁ®ã‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåÁâπÂà´ÊòØÂú®Â§±Ë¥•‰øÆÊ≠£ÂíåÁ≠ñÁï•Ë∞ÉÊï¥ÊñπÈù¢„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAPOÊñπÊ≥ïÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∫∫Êú∫Âçè‰ΩúÊ°ÜÊû∂„ÄÅ‰∫§‰∫íËΩ®ËøπÊî∂ÈõÜÊ®°ÂùóÂíåËá™ÈÄÇÂ∫îÈáçÂä†ÊùÉÁÆóÊ≥ï„ÄÇÈ¶ñÂÖàÔºåÈÄöËøá‰∫∫Êú∫Âçè‰ΩúËøõË°å‰∫§‰∫íÔºåÊî∂ÈõÜËΩ®ËøπÊï∞ÊçÆÔºõÁÑ∂ÂêéÔºåÂà©Áî®Ëøô‰∫õÊï∞ÊçÆËøõË°åÂÅèÂ•Ω‰ºòÂåñÔºåÊúÄÂêéÈÄöËøáËá™ÈÄÇÂ∫îÈáçÂä†ÊùÉÁÆóÊ≥ïË∞ÉÊï¥Ê®°ÂûãÁöÑÂÜ≥Á≠ñÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAPOÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜËá™ÈÄÇÂ∫îÈáçÂä†ÊùÉÁÆóÊ≥ïÔºåËØ•ÁÆóÊ≥ïÂà©Áî®Êù•Ëá™‰∫§‰∫íÁöÑ‰∫åÂÖÉÂèØÂèñÊÄß‰ø°Âè∑ÔºåËÉΩÂ§üÊúâÊïàÊäëÂà∂Â§±Ë¥•ÂÄæÂêëÁöÑÂä®‰ΩúÂπ∂Â¢ûÂº∫Á∫†Ê≠£Âä®‰ΩúÁöÑÈÄÇÂ∫îÊÄß„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÁ¶ªÁ∫øÊï∞ÊçÆÁöÑ‰ºòÂåñÊñπÊ≥ïÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÖ≥ÈîÆËÆæËÆ°ÊñπÈù¢ÔºåAPOÈááÁî®‰∫Ü‰∫åÂÖÉÂèØÂèñÊÄß‰ø°Âè∑‰Ωú‰∏∫‰ºòÂåñÁöÑÂü∫Á°ÄÔºåËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•Âπ≥Ë°°ÊàêÂäü‰∏éÂ§±Ë¥•ÁöÑÂä®‰ΩúÊùÉÈáçÔºåÂêåÊó∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äËøõË°å‰∫ÜË∞ÉÊï¥Ôºå‰ª•ÈÄÇÂ∫îÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÂèòÂåñ„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAPOÊñπÊ≥ïÂú®Â§öÁßçÊìç‰Ωú‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫‰ºòË∂äÁöÑÊÄßËÉΩÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÊàêÂäüÁéáÊèêÂçá‰∫Ü20%‰ª•‰∏äÔºå‰∏îÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÊòæËëóÂ¢ûÂº∫ÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÂíåÊô∫ËÉΩÂÆ∂Â±ÖÁ≠âÂú∫ÊôØ„ÄÇÈÄöËøá‰∫∫Êú∫Âçè‰Ωú‰ºòÂåñÊú∫Âô®‰∫∫ÂÜ≥Á≠ñËÉΩÂäõÔºåÂèØ‰ª•ÊòæËëóÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÂíåÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Establishing a reliable and iteratively refined robotic system is essential for deploying real-world applications. While Vision-Language-Action (VLA) models are widely recognized as the foundation model for such robotic deployment, their reliance on offline expert demonstrations critically limits their capacity for post-deployment refinement. To mitigate this limitation, we introduce Action Preference Optimization (APO), a method designed to refine VLA models by human-assisted preference alignment gathered through interaction with environments. This method begins with a human-robot collaboration framework for reliable failure correction and interaction trajectory collection through human intervention. However, directly leveraging these interaction trajectories for preference optimization is non-trivial due to the challenges of irreversible robotic actions and token distribution mismatch. To solve this, APO proposes an adaptive reweighting algorithm with binary desirability signals derived from interaction, empowering VLA models effectively suppress failure-prone actions while enhancing corrective action adaptation. Ultimately, APO equips VLA models with the crucial capability to learn from failure, paving the way for their iterative refinement and reliable deployment in dynamic environments. The experiments conducted in simulation and real-world scenarios prove superior generalization and robustness of our human-assisted framework across a variety of manipulation tasks. We believe this work could bring insights for efficient and stable optimization of VLA models through human-robot collaboration. The code and dataset are released at https://github.com/GeWu-Lab/Action-Preference-Optimization

