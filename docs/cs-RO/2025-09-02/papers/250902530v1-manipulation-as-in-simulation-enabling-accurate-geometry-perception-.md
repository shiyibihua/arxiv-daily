---
layout: default
title: Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots
---

# Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02530" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02530v1</a>
  <a href="https://arxiv.org/pdf/2509.02530.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02530v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02530v1', 'Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minghuan Liu, Zhengbang Zhu, Xiaoshen Han, Peng Hu, Haotong Lin, Xinyao Li, Jingxiao Chen, Jiafeng Xu, Yichu Yang, Yunfeng Lin, Xinghang Li, Yong Yu, Weinan Zhang, Tao Kong, Bingyi Kang

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

**å¤‡æ³¨**: 32 pages, 18 figures, project page: https://manipulation-as-in-simulation.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç›¸æœºæ·±åº¦æ¨¡å‹ï¼ˆCDMï¼‰ï¼Œæå‡æœºå™¨äººæ“ä½œä¸­æ·±åº¦æ„ŸçŸ¥çš„å‡†ç¡®æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `æ·±åº¦æ„ŸçŸ¥` `æ·±åº¦ç›¸æœº` `æ¨¡æ‹Ÿåˆ°çœŸå®` `ç›¸æœºæ·±åº¦æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ“ä½œä¾èµ–2Dè§†è§‰ï¼Œæ³›åŒ–æ€§å·®ï¼Œè€Œæ·±åº¦ç›¸æœºç²¾åº¦ä¸è¶³ï¼Œé™åˆ¶äº†3Då‡ ä½•ä¿¡æ¯çš„åº”ç”¨ã€‚
2. æå‡ºç›¸æœºæ·±åº¦æ¨¡å‹ï¼ˆCDMï¼‰ï¼Œé€šè¿‡æ¨¡æ‹Ÿå™ªå£°æ¨¡å¼ç”Ÿæˆé«˜è´¨é‡æ•°æ®ï¼Œæå‡æ·±åº¦é¢„æµ‹ç²¾åº¦ã€‚
3. å®éªŒè¯æ˜ï¼ŒCDMä½¿æ¨¡æ‹Ÿè®­ç»ƒçš„ç­–ç•¥èƒ½ç›´æ¥åº”ç”¨äºçœŸå®æœºå™¨äººï¼Œå®Œæˆå¤æ‚æ“ä½œä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£æœºå™¨äººæ“ä½œä¸»è¦ä¾èµ–äº2Då½©è‰²å›¾åƒè¿›è¡ŒæŠ€èƒ½å­¦ä¹ ï¼Œä½†æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚äººç±»æ›´å¤šåœ°ä¾èµ–è·ç¦»ã€å¤§å°å’Œå½¢çŠ¶ç­‰ç‰©ç†å±æ€§ä¸ç‰©ä½“äº¤äº’ã€‚è™½ç„¶æ·±åº¦ç›¸æœºå¯ä»¥æä¾›3Då‡ ä½•ä¿¡æ¯ï¼Œä½†å…¶ç²¾åº¦æœ‰é™ä¸”æ˜“å—å™ªå£°å½±å“ã€‚æœ¬æ–‡æå‡ºäº†ç›¸æœºæ·±åº¦æ¨¡å‹ï¼ˆCDMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ’ä»¶ï¼Œå¯ç”¨äºæ—¥å¸¸ä½¿ç”¨çš„æ·±åº¦ç›¸æœºï¼Œå®ƒä»¥RGBå›¾åƒå’ŒåŸå§‹æ·±åº¦ä¿¡å·ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå»å™ªçš„ã€å‡†ç¡®çš„åº¦é‡æ·±åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç¥ç»æ•°æ®å¼•æ“ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ·±åº¦ç›¸æœºçš„å™ªå£°æ¨¡å¼ï¼Œä»æ¨¡æ‹Ÿä¸­ç”Ÿæˆé«˜è´¨é‡çš„é…å¯¹æ•°æ®ã€‚ç»“æœè¡¨æ˜ï¼ŒCDMåœ¨æ·±åº¦é¢„æµ‹æ–¹é¢å®ç°äº†æ¥è¿‘æ¨¡æ‹Ÿæ°´å¹³çš„ç²¾åº¦ï¼Œæœ‰æ•ˆåœ°å¼¥åˆäº†æ¨¡æ‹Ÿåˆ°çœŸå®çš„å·®è·ã€‚å®éªŒé¦–æ¬¡è¯æ˜ï¼Œåœ¨åŸå§‹æ¨¡æ‹Ÿæ·±åº¦ä¸Šè®­ç»ƒçš„ç­–ç•¥ï¼Œæ— éœ€æ·»åŠ å™ªå£°æˆ–è¿›è¡ŒçœŸå®ä¸–ç•Œçš„å¾®è°ƒï¼Œå³å¯æ— ç¼åœ°æ³›åŒ–åˆ°çœŸå®ä¸–ç•Œçš„æœºå™¨äººä¸Šï¼Œå®Œæˆæ¶‰åŠé“°æ¥ã€åå°„å’Œç»†é•¿ç‰©ä½“çš„ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿ç¨‹ä»»åŠ¡ï¼Œä¸”æ€§èƒ½å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å‘ç°èƒ½å¤Ÿæ¿€å‘æœªæ¥åœ¨é€šç”¨æœºå™¨äººç­–ç•¥ä¸­åˆ©ç”¨æ¨¡æ‹Ÿæ•°æ®å’Œ3Dä¿¡æ¯çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººæ“ä½œä¸»è¦ä¾èµ–2Då½©è‰²å›¾åƒï¼Œç¼ºä¹å¯¹ç‰©ä½“3Då‡ ä½•ä¿¡æ¯çš„å‡†ç¡®æ„ŸçŸ¥ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚æ·±åº¦ç›¸æœºè™½ç„¶å¯ä»¥æä¾›3Dä¿¡æ¯ï¼Œä½†å…¶å›ºæœ‰çš„å™ªå£°å’Œç²¾åº¦é™åˆ¶ï¼Œä½¿å¾—ç›´æ¥ä½¿ç”¨æ·±åº¦æ•°æ®è¿›è¡Œæœºå™¨äººæ§åˆ¶å˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œéœ€è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•æé«˜æ·±åº¦ç›¸æœºçš„ç²¾åº¦ï¼Œä½¿å…¶èƒ½å¤Ÿä¸ºæœºå™¨äººæä¾›å¯é çš„3Då‡ ä½•ä¿¡æ¯ï¼Œä»è€Œæå‡æ“ä½œæŠ€èƒ½çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡æ‹Ÿç¯å¢ƒç”Ÿæˆå¤§é‡å¸¦æœ‰å™ªå£°çš„æ·±åº¦æ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªæ·±åº¦ç›¸æœºæ¨¡å‹ï¼ˆCDMï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿä»çœŸå®æ·±åº¦ç›¸æœºè·å–çš„RGBå›¾åƒå’ŒåŸå§‹æ·±åº¦ä¿¡å·ä¸­ï¼Œé¢„æµ‹å‡ºå‡†ç¡®çš„ã€å»å™ªçš„åº¦é‡æ·±åº¦ã€‚é€šè¿‡æ¨¡æ‹ŸçœŸå®æ·±åº¦ç›¸æœºçš„å™ªå£°æ¨¡å¼ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¼¥åˆæ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ç¯å¢ƒä¹‹é—´çš„å·®è·ï¼Œä»è€Œå®ç°ä»æ¨¡æ‹Ÿåˆ°çœŸå®çš„è¿ç§»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šç¥ç»æ•°æ®å¼•æ“å’Œç›¸æœºæ·±åº¦æ¨¡å‹ï¼ˆCDMï¼‰ã€‚ç¥ç»æ•°æ®å¼•æ“è´Ÿè´£åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ç”Ÿæˆé«˜è´¨é‡çš„é…å¯¹æ•°æ®ï¼ŒåŒ…æ‹¬RGBå›¾åƒå’Œå¸¦æœ‰å™ªå£°çš„æ·±åº¦å›¾åƒã€‚CDMæ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œä»¥RGBå›¾åƒå’ŒåŸå§‹æ·±åº¦ä¿¡å·ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå»å™ªçš„ã€å‡†ç¡®çš„åº¦é‡æ·±åº¦ã€‚è®­ç»ƒå¥½çš„CDMå¯ä»¥ä½œä¸ºä¸€ä¸ªæ’ä»¶ï¼Œç›´æ¥åº”ç”¨äºçœŸå®ä¸–ç•Œçš„æ·±åº¦ç›¸æœºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨ç¥ç»æ•°æ®å¼•æ“æ¨¡æ‹Ÿæ·±åº¦ç›¸æœºçš„å™ªå£°æ¨¡å¼ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥åœ¨çœŸå®ä¸–ç•Œä¸­æ”¶é›†å¤§é‡æ•°æ®çš„å›°éš¾ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°å¼¥åˆæ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ç¯å¢ƒä¹‹é—´çš„å·®è·ã€‚æ­¤å¤–ï¼ŒCDMçš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†æ·±åº¦ç›¸æœºçš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å»é™¤å™ªå£°å¹¶æé«˜æ·±åº¦é¢„æµ‹çš„ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šç¥ç»æ•°æ®å¼•æ“é€šè¿‡å¯¹æ·±åº¦ç›¸æœºå™ªå£°è¿›è¡Œå»ºæ¨¡ï¼Œä¾‹å¦‚é«˜æ–¯å™ªå£°ã€æ¤’ç›å™ªå£°ç­‰ï¼Œå¹¶è°ƒæ•´å™ªå£°çš„å¼ºåº¦å’Œåˆ†å¸ƒï¼Œç”Ÿæˆé€¼çœŸçš„æ¨¡æ‹Ÿæ•°æ®ã€‚CDMé‡‡ç”¨äº†ä¸€ç§å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ï¼Œåˆ©ç”¨RGBå›¾åƒæä¾›çº¹ç†ä¿¡æ¯ï¼Œå¹¶ç»“åˆåŸå§‹æ·±åº¦ä¿¡å·è¿›è¡Œæ·±åº¦é¢„æµ‹ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ·±åº¦é¢„æµ‹çš„å‡æ–¹è¯¯å·®å’Œä¸€äº›æ­£åˆ™åŒ–é¡¹ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“çš„æ·±åº¦ç›¸æœºå‹å·å’Œåº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCDMèƒ½å¤Ÿæ˜¾è‘—æé«˜æ·±åº¦é¢„æµ‹çš„ç²¾åº¦ï¼Œä½¿å…¶æ¥è¿‘æ¨¡æ‹Ÿæ°´å¹³ã€‚ä½¿ç”¨CDMåï¼Œåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è®­ç»ƒçš„æœºå™¨äººç­–ç•¥å¯ä»¥ç›´æ¥åº”ç”¨äºçœŸå®æœºå™¨äººï¼Œå®Œæˆæ¶‰åŠé“°æ¥ã€åå°„å’Œç»†é•¿ç‰©ä½“çš„å¤æ‚æ“ä½œä»»åŠ¡ï¼Œä¸”æ€§èƒ½å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚è¿™è¡¨æ˜CDMæœ‰æ•ˆåœ°å¼¥åˆäº†æ¨¡æ‹Ÿåˆ°çœŸå®çš„å·®è·ï¼Œä¸ºåˆ©ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæœºå™¨äººè®­ç»ƒæä¾›äº†æ–°çš„é€”å¾„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜æ·±åº¦æ„ŸçŸ¥çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥ä½¿æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°ç†è§£å’Œæ“ä½œç‰©ä½“ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´å¯é çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œåœ¨å·¥ä¸šæœºå™¨äººé¢†åŸŸï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯å®ç°æ›´ç²¾ç¡®çš„è£…é…å’Œæ£€æµ‹ï¼›åœ¨æœåŠ¡æœºå™¨äººé¢†åŸŸï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶çš„äº¤äº’å’Œæ›´çµæ´»çš„æ“ä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern robotic manipulation primarily relies on visual observations in a 2D color space for skill learning but suffers from poor generalization. In contrast, humans, living in a 3D world, depend more on physical properties-such as distance, size, and shape-than on texture when interacting with objects. Since such 3D geometric information can be acquired from widely available depth cameras, it appears feasible to endow robots with similar perceptual capabilities. Our pilot study found that using depth cameras for manipulation is challenging, primarily due to their limited accuracy and susceptibility to various types of noise. In this work, we propose Camera Depth Models (CDMs) as a simple plugin on daily-use depth cameras, which take RGB images and raw depth signals as input and output denoised, accurate metric depth. To achieve this, we develop a neural data engine that generates high-quality paired data from simulation by modeling a depth camera's noise pattern. Our results show that CDMs achieve nearly simulation-level accuracy in depth prediction, effectively bridging the sim-to-real gap for manipulation tasks. Notably, our experiments demonstrate, for the first time, that a policy trained on raw simulated depth, without the need for adding noise or real-world fine-tuning, generalizes seamlessly to real-world robots on two challenging long-horizon tasks involving articulated, reflective, and slender objects, with little to no performance degradation. We hope our findings will inspire future research in utilizing simulation data and 3D information in general robot policies.

