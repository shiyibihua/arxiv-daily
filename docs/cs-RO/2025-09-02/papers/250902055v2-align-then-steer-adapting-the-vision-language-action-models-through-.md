---
layout: default
title: "Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance"
---

# Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02055" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02055v2</a>
  <a href="https://arxiv.org/pdf/2509.02055.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02055v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02055v2', 'Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Zhang, Chenwei Wang, Ouyang Lu, Yuan Zhao, Yunfei Ge, Zhenglong Sun, Xiu Li, Chi Zhang, Chenjia Bai, Xuelong Li

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02 (æ›´æ–°: 2025-09-05)

**å¤‡æ³¨**: The first three authors contributed equally

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Align-Then-stEeræ¡†æ¶é€šè¿‡ç»Ÿä¸€æ½œåœ¨ç©ºé—´æŒ‡å¯¼ï¼Œå®ç°VLAæ¨¡å‹åœ¨æœºå™¨äººä»»åŠ¡ä¸Šçš„é«˜æ•ˆè¿ç§»ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººæ“ä½œ` `è¿ç§»å­¦ä¹ ` `å˜åˆ†è‡ªç¼–ç å™¨` `æ‰©æ•£æ¨¡å‹` `åŠ¨ä½œç©ºé—´å¯¹é½` `æ½œåœ¨ç©ºé—´` `æ¨¡å‹å¼•å¯¼`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨é€‚åº”æ–°æœºå™¨äººå½¢æ€æˆ–ä»»åŠ¡æ—¶ï¼Œé¢ä¸´åŠ¨ä½œåˆ†å¸ƒä¸åŒ¹é…çš„æŒ‘æˆ˜ï¼Œéœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºè¿›è¡Œå¾®è°ƒã€‚
2. ATEæ¡†æ¶é€šè¿‡æ„å»ºç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´å¯¹é½åŠ¨ä½œç©ºé—´ï¼Œå¹¶åˆ©ç”¨å¼•å¯¼æœºåˆ¶åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å°†æ¨¡å‹è¾“å‡ºåˆ†å¸ƒæ¨å‘ç›®æ ‡åŸŸã€‚
3. å®éªŒè¡¨æ˜ï¼ŒATEåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å‡æ˜¾è‘—æå‡äº†VLAæ¨¡å‹åœ¨è·¨å½¢æ€å’Œè·¨ä»»åŠ¡æ“ä½œä¸­çš„æˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAlign-Then-stEerï¼ˆATEï¼‰çš„å…¨æ–°ã€æ•°æ®é«˜æ•ˆä¸”å³æ’å³ç”¨çš„è‡ªé€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é€‚åº”æ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººå½¢æ€æˆ–ä»»åŠ¡ä¸é¢„è®­ç»ƒæ•°æ®å­˜åœ¨å·®å¼‚æ—¶ã€‚ATEé€šè¿‡æ„å»ºç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´æ¥å¯¹é½ä¸åŒçš„åŠ¨ä½œç©ºé—´ï¼Œå…¶ä¸­å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å—åˆ°åå‘KLæ•£åº¦çš„çº¦æŸï¼Œå°†è‡ªé€‚åº”åŠ¨ä½œåµŒå…¥åˆ°é¢„è®­ç»ƒåŠ¨ä½œæ½œåœ¨åˆ†å¸ƒçš„æ¨¡å¼ä¸­ã€‚éšåï¼Œé€šè¿‡å¼•å¯¼æœºåˆ¶åœ¨å¾®è°ƒæœŸé—´æ§åˆ¶åŸºäºæ‰©æ•£æˆ–æµçš„VLAç”Ÿæˆè¿‡ç¨‹ï¼Œå°†æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ¨å‘ç›®æ ‡åŸŸã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„è·¨å½¢æ€å’Œè·¨ä»»åŠ¡æ“ä½œä¸­è¿›è¡Œäº†å¤§é‡å®éªŒã€‚ä¸ç›´æ¥å¾®è°ƒVLAæ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿä¸­å°†å¹³å‡å¤šä»»åŠ¡æˆåŠŸç‡æé«˜äº†é«˜è¾¾9.8ï¼…ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œçš„è·¨å½¢æ€è®¾ç½®ä¸­å®ç°äº†æƒŠäººçš„32ï¼…çš„æˆåŠŸç‡æå‡ã€‚è¯¥å·¥ä½œæå‡ºäº†ä¸€ç§é€šç”¨ä¸”è½»é‡çº§çš„è§£å†³æ–¹æ¡ˆï¼Œå¤§å¤§æé«˜äº†å°†VLAæ¨¡å‹éƒ¨ç½²åˆ°æ–°çš„æœºå™¨äººå¹³å°å’Œä»»åŠ¡çš„å®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVLAæ¨¡å‹åœ¨é¢„è®­ç»ƒæ•°æ®ä¸å®é™…æœºå™¨äººä»»åŠ¡å­˜åœ¨å·®å¼‚æ—¶ï¼ŒåŠ¨ä½œç©ºé—´ä¸åŒ¹é…ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™ï¼Œéœ€è¦å¤§é‡æ•°æ®è¿›è¡Œå¾®è°ƒæ‰èƒ½é€‚åº”æ–°çš„æœºå™¨äººå½¢æ€æˆ–ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ç›´æ¥å¾®è°ƒï¼Œæ•ˆç‡ä½ä¸‹ï¼Œä¸”å®¹æ˜“è¿‡æ‹Ÿåˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è§£è€¦å¯¹é½ï¼ˆAlignï¼‰å’Œå¼•å¯¼ï¼ˆSteerï¼‰ä¸¤ä¸ªè¿‡ç¨‹ã€‚é¦–å…ˆï¼Œé€šè¿‡ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´å¯¹é½ä¸åŒæœºå™¨äººå½¢æ€æˆ–ä»»åŠ¡çš„åŠ¨ä½œç©ºé—´ï¼Œå°†å®ƒä»¬æ˜ å°„åˆ°é¢„è®­ç»ƒåŠ¨ä½œçš„æ½œåœ¨åˆ†å¸ƒä¸­ã€‚ç„¶åï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨å¼•å¯¼æœºåˆ¶ï¼Œå°†æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ¨å‘ç›®æ ‡åŸŸï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è¿ç§»å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šATEæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šå¯¹é½é˜¶æ®µå’Œå¼•å¯¼é˜¶æ®µã€‚åœ¨å¯¹é½é˜¶æ®µï¼Œä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ ä¸€ä¸ªç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ï¼Œå°†ä¸åŒåŠ¨ä½œç©ºé—´æ˜ å°„åˆ°è¯¥ç©ºé—´ä¸­ã€‚VAEçš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–é‡æ„è¯¯å·®å’Œåå‘KLæ•£åº¦ï¼Œç¡®ä¿æ½œåœ¨ç©ºé—´èƒ½å¤Ÿæ•æ‰åˆ°é¢„è®­ç»ƒåŠ¨ä½œçš„åˆ†å¸ƒã€‚åœ¨å¼•å¯¼é˜¶æ®µï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹æˆ–æµæ¨¡å‹ç”ŸæˆåŠ¨ä½œï¼Œå¹¶ä½¿ç”¨å¼•å¯¼æœºåˆ¶å°†ç”Ÿæˆè¿‡ç¨‹æ¨å‘ç›®æ ‡åŸŸã€‚å¼•å¯¼æœºåˆ¶é€šè¿‡è®¡ç®—ç›®æ ‡åŸŸçš„æ¢¯åº¦ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°ç”Ÿæˆè¿‡ç¨‹çš„å™ªå£°ä¸­ï¼Œä»è€Œå½±å“ç”Ÿæˆç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šATEçš„å…³é”®åˆ›æ–°åœ¨äºå°†åŠ¨ä½œç©ºé—´å¯¹é½å’Œæ¨¡å‹å¼•å¯¼è§£è€¦ï¼Œå¹¶ä½¿ç”¨ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ä½œä¸ºæ¡¥æ¢ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥å¾®è°ƒå¸¦æ¥çš„æ•°æ®ä¾èµ–æ€§é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä½¿ç”¨åå‘KLæ•£åº¦çº¦æŸVAEçš„æ½œåœ¨ç©ºé—´ï¼Œç¡®ä¿å…¶èƒ½å¤Ÿæ•æ‰åˆ°é¢„è®­ç»ƒåŠ¨ä½œçš„åˆ†å¸ƒï¼Œä»è€Œæé«˜äº†å¯¹é½çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¯¹é½é˜¶æ®µï¼ŒVAEçš„ç¼–ç å™¨å’Œè§£ç å™¨é‡‡ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ç»“æ„ã€‚åå‘KLæ•£åº¦çš„æƒé‡æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚åœ¨å¼•å¯¼é˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„æ‰©æ•£æ¨¡å‹æˆ–æµæ¨¡å‹ã€‚å¼•å¯¼å¼ºåº¦ä¹Ÿæ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†DDPMä½œä¸ºæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨Classifier-Free Guidanceè¿›è¡Œå¼•å¯¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒATEæ¡†æ¶åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼ŒATEå°†å¹³å‡å¤šä»»åŠ¡æˆåŠŸç‡æé«˜äº†é«˜è¾¾9.8ï¼…ã€‚åœ¨çœŸå®ä¸–ç•Œçš„è·¨å½¢æ€è®¾ç½®ä¸­ï¼ŒATEå®ç°äº†æƒŠäººçš„32ï¼…çš„æˆåŠŸç‡æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒATEæ˜¯ä¸€ç§æœ‰æ•ˆä¸”é€šç”¨çš„VLAæ¨¡å‹è‡ªé€‚åº”æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ“ä½œé¢†åŸŸï¼Œä¾‹å¦‚å·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡ATEæ¡†æ¶ï¼Œå¯ä»¥å¿«é€Ÿå°†VLAæ¨¡å‹éƒ¨ç½²åˆ°æ–°çš„æœºå™¨äººå¹³å°å’Œä»»åŠ¡ä¸­ï¼Œé™ä½å¼€å‘æˆæœ¬ï¼Œæé«˜æœºå™¨äººæ™ºèƒ½åŒ–æ°´å¹³ã€‚è¯¥æ–¹æ³•è¿˜æœ‰æ½œåŠ›åº”ç”¨äºå…¶ä»–è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒç”Ÿæˆã€æ–‡æœ¬ç¼–è¾‘ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models pre-trained on large, diverse datasets show remarkable potential for general-purpose robotic manipulation. However, a primary bottleneck remains in adapting these models to downstream tasks, especially when the robot's embodiment or the task itself differs from the pre-training data. This discrepancy leads to a significant mismatch in action distributions, demanding extensive data and compute for effective fine-tuning. To address this challenge, we introduce \textbf{Align-Then-stEer (\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation framework. \texttt{ATE} first aligns disparate action spaces by constructing a unified latent space, where a variational autoencoder constrained by reverse KL divergence embeds adaptation actions into modes of the pre-training action latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's generation process during fine-tuning via a guidance mechanism that pushes the model's output distribution towards the target domain. We conduct extensive experiments on cross-embodiment and cross-task manipulation in both simulation and real world. Compared to direct fine-tuning of representative VLAs, our method improves the average multi-task success rate by up to \textbf{9.8\% } in simulation and achieves a striking \textbf{32\% success rate gain} in a real-world cross-embodiment setting. Our work presents a general and lightweight solution that greatly enhances the practicality of deploying VLA models to new robotic platforms and tasks.

