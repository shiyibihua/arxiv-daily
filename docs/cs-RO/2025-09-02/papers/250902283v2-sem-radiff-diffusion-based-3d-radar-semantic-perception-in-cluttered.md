---
layout: default
title: Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments
---

# Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02283" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02283v2</a>
  <a href="https://arxiv.org/pdf/2509.02283.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02283v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02283v2', 'Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruibin Zhang, Fei Gao

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02 (æ›´æ–°: 2025-09-03)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé›·è¾¾çš„3Dç¯å¢ƒæ„ŸçŸ¥æ¡†æ¶ä»¥è§£å†³å†œä¸šç¯å¢ƒä¸­çš„æ„ŸçŸ¥æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `é›·è¾¾æ„ŸçŸ¥` `3Dç¯å¢ƒæ„ŸçŸ¥` `å†œä¸šæœºå™¨äºº` `æ‰©æ•£æ¨¡å‹` `ç¨€ç–ç½‘ç»œ` `è¯­ä¹‰åˆ†å‰²` `ä¿¡å·å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å…‰å­¦ä¼ æ„Ÿå™¨åœ¨å†œä¸šç¯å¢ƒä¸­å®¹æ˜“å—åˆ°é®æŒ¡å’Œæ±¡æŸ“ï¼Œå¯¼è‡´æ„ŸçŸ¥æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé›·è¾¾çš„3Dç¯å¢ƒæ„ŸçŸ¥æ¡†æ¶ï¼ŒåŒ…å«ä¿¡å·å¢å¼ºã€æ‰©æ•£æ¨¡å‹å­¦ä¹ å’Œç¨€ç–3Dç½‘ç»œå¤„ç†ç­‰æ¨¡å—ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç»“æ„å’Œè¯­ä¹‰é¢„æµ‹ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸”è®¡ç®—å’Œå†…å­˜æˆæœ¬æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‡†ç¡®ä¸”ç¨³å¥çš„ç¯å¢ƒæ„ŸçŸ¥å¯¹äºæœºå™¨äººè‡ªä¸»å¯¼èˆªè‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å…‰å­¦ä¼ æ„Ÿå™¨ï¼ˆå¦‚ç›¸æœºã€æ¿€å…‰é›·è¾¾ï¼‰ï¼Œä½†åœ¨è§†è§‰é®æŒ¡æƒ…å†µä¸‹æ€§èƒ½ä¼šä¸‹é™ã€‚æœ¬æ–‡èšç„¦äºå†œä¸šåœºæ™¯ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé›·è¾¾çš„3Dç¯å¢ƒæ„ŸçŸ¥æ¡†æ¶ï¼Œåˆ©ç”¨é›·è¾¾çš„å¼ºç©¿é€èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ä»¥å®ç°å¯†é›†ä¸”å‡†ç¡®çš„è¯­ä¹‰æ„ŸçŸ¥ã€‚é€šè¿‡åœ¨çœŸå®å†œä¸šåœºæ™¯ä¸­æ”¶é›†çš„æ•°æ®è¿›è¡Œå¹¿æ³›çš„åŸºå‡†æ¯”è¾ƒå’Œå®éªŒè¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç»“æ„å’Œè¯­ä¹‰é¢„æµ‹æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—å’Œå†…å­˜æˆæœ¬åˆ†åˆ«é™ä½äº†51.3%å’Œ27.5%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å†œä¸šç¯å¢ƒä¸­æœºå™¨äººæ„ŸçŸ¥çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§é—®é¢˜ï¼Œç°æœ‰å…‰å­¦ä¼ æ„Ÿå™¨åœ¨å¤æ‚ç¯å¢ƒä¸­å®¹æ˜“å—åˆ°é®æŒ¡å’Œæ±¡æŸ“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨é›·è¾¾çš„å¼ºç©¿é€èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„3Dç¯å¢ƒæ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åœ¨å¤æ‚å†œä¸šåœºæ™¯ä¸­çš„æ„ŸçŸ¥ç²¾åº¦å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š1) å¹¶è¡Œå¸§ç´¯ç§¯ä»¥å¢å¼ºé›·è¾¾åŸå§‹æ•°æ®çš„ä¿¡å™ªæ¯”ï¼›2) åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å±‚å­¦ä¹ æ¡†æ¶ï¼Œè¿‡æ»¤é›·è¾¾å‰¯ç“£ä¼ªå½±å¹¶ç”Ÿæˆç»†ç²’åº¦çš„3Dè¯­ä¹‰ç‚¹äº‘ï¼›3) ä¸“é—¨è®¾è®¡çš„ç¨€ç–3Dç½‘ç»œï¼Œä¼˜åŒ–å¤„ç†å¤§è§„æ¨¡é›·è¾¾åŸå§‹æ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†æ‰©æ•£æ¨¡å‹ä¸ç¨€ç–3Dç½‘ç»œï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é›·è¾¾æ•°æ®ä¸­çš„å™ªå£°å’Œä¼ªå½±ï¼Œç”Ÿæˆé«˜è´¨é‡çš„3Dè¯­ä¹‰ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è¯­ä¹‰åˆ†å‰²å’Œç»“æ„é‡å»ºï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†ç¨€ç–å¤„ç†æœºåˆ¶ï¼Œä»¥é€‚åº”å¤§è§„æ¨¡æ•°æ®çš„å¤„ç†éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ç»“æ„å’Œè¯­ä¹‰é¢„æµ‹æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œè®¡ç®—å’Œå†…å­˜æˆæœ¬åˆ†åˆ«é™ä½äº†51.3%å’Œ27.5%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®Œæ•´é‡å»ºå’Œå‡†ç¡®åˆ†ç±»è–„ç»“æ„ï¼Œå¦‚æ†å’Œç”µçº¿ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¯†é›†å’Œå‡†ç¡®çš„3Dé›·è¾¾æ„ŸçŸ¥ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†œä¸šæœºå™¨äººã€æ— äººæœºç›‘æµ‹å’Œç¯å¢ƒç›‘æµ‹ç­‰ã€‚é€šè¿‡æé«˜åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å†œä¸šä½œä¸šä¸­çš„è‡ªä¸»å¯¼èˆªå’Œå†³ç­–èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate and robust environmental perception is crucial for robot autonomous navigation. While current methods typically adopt optical sensors (e.g., camera, LiDAR) as primary sensing modalities, their susceptibility to visual occlusion often leads to degraded performance or complete system failure. In this paper, we focus on agricultural scenarios where robots are exposed to the risk of onboard sensor contamination. Leveraging radar's strong penetration capability, we introduce a radar-based 3D environmental perception framework as a viable alternative. It comprises three core modules designed for dense and accurate semantic perception: 1) Parallel frame accumulation to enhance signal-to-noise ratio of radar raw data. 2) A diffusion model-based hierarchical learning framework that first filters radar sidelobe artifacts then generates fine-grained 3D semantic point clouds. 3) A specifically designed sparse 3D network optimized for processing large-scale radar raw data. We conducted extensive benchmark comparisons and experimental evaluations on a self-built dataset collected in real-world agricultural field scenes. Results demonstrate that our method achieves superior structural and semantic prediction performance compared to existing methods, while simultaneously reducing computational and memory costs by 51.3% and 27.5%, respectively. Furthermore, our approach achieves complete reconstruction and accurate classification of thin structures such as poles and wires-which existing methods struggle to perceive-highlighting its potential for dense and accurate 3D radar perception.

