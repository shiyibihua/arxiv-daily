---
layout: default
title: Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception
---

# Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02324" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.02324v1</a>
  <a href="https://arxiv.org/pdf/2509.02324.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02324v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02324v1', 'Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Changshi Zhou, Haichuan Xu, Ningquan Gu, Zhipeng Wang, Bin Cheng, Pengpeng Zhang, Yanchao Dong, Mitsuhiro Hayashibe, Yanmin Zhou, Bin He

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-02

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éLLMËßÑÂàíÂíåËßÜËßâÊÑüÁü•ÁöÑËØ≠Ë®ÄÂºïÂØºÈïøÊó∂Á®ãÊìç‰ΩúÊ°ÜÊû∂ÔºåËß£ÂÜ≥ÂèØÂèòÂΩ¢Áâ©‰ΩìÊìç‰ΩúÈöæÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËØ≠Ë®ÄÂºïÂØºÊìç‰Ωú` `ÈïøÊó∂Á®ãËßÑÂàí` `ÂèØÂèòÂΩ¢Áâ©‰ΩìÊìç‰Ωú` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Êú∫Âô®‰∫∫` `Â∏ÉÊñôÊäòÂè†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÂèØÂèòÂΩ¢Áâ©‰ΩìÊìç‰ΩúÂõ†ÂÖ∂È´òËá™Áî±Â∫¶„ÄÅÂ§çÊùÇÂä®ÂäõÂ≠¶ÂíåÁ≤æÁ°ÆËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêÈúÄÊ±ÇËÄåÊûÅÂÖ∑ÊåëÊàò„ÄÇ
2. ËØ•ÊñπÊ≥ïÊèêÂá∫‰∏Ä‰∏™Áªü‰∏ÄÊ°ÜÊû∂ÔºåÁªìÂêàLLMËßÑÂàíÂô®„ÄÅVLMÊÑüÁü•Á≥ªÁªüÂíå‰ªªÂä°ÊâßË°åÊ®°ÂùóÔºåÂÆûÁé∞ËØ≠Ë®ÄÂºïÂØºÁöÑÈïøÊó∂Á®ãÊìç‰Ωú„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÁéØÂ¢É‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊäÄÊúØÔºåÂ±ïÁé∞‰∫ÜÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÈíàÂØπÂèØÂèòÂΩ¢Áâ©‰ΩìÁöÑËØ≠Ë®ÄÂºïÂØºÈïøÊó∂Á®ãÊìç‰ΩúÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÊ°ÜÊû∂ÔºåËØ•‰ªªÂä°Âõ†È´òËá™Áî±Â∫¶„ÄÅÂ§çÊùÇÂä®ÂäõÂ≠¶‰ª•ÂèäÁ≤æÁ°ÆÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêËÄåÊûÅÂÖ∑ÊåëÊàò„ÄÇÁ†îÁ©∂ËÅöÁÑ¶‰∫éÂ§öÊ≠•È™§ÁöÑÂ∏ÉÊñôÊäòÂè†ÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑÂèØÂèòÂΩ¢Áâ©‰ΩìÊìç‰Ωú‰ªªÂä°ÔºåÈúÄË¶ÅÁªìÊûÑÂåñÁöÑÈïøÊó∂Á®ãËßÑÂàíÂíåÁ≤æÁªÜÁöÑËßÜËßâÊÑüÁü•„ÄÇËØ•Ê°ÜÊû∂ÈõÜÊàê‰∫ÜÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËßÑÂàíÂô®„ÄÅÂü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÊÑüÁü•Á≥ªÁªüÂíå‰ªªÂä°ÊâßË°åÊ®°Âùó„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåLLMËßÑÂàíÂô®Â∞ÜÈ´òÂ±ÇËØ≠Ë®ÄÊåá‰ª§ÂàÜËß£‰∏∫‰ΩéÂ±ÇÂä®‰ΩúÂéüËØ≠ÔºåÂº•Âêà‰∫ÜËØ≠‰πâ-ÊâßË°åÁöÑÂ∑ÆË∑ùÔºåÂØπÈΩê‰∫ÜÊÑüÁü•‰∏éÂä®‰ΩúÔºåÂπ∂Â¢ûÂº∫‰∫ÜÊ≥õÂåñËÉΩÂäõ„ÄÇVLMÊÑüÁü•Ê®°ÂùóÈááÁî®SigLIP2È©±Âä®ÁöÑÊû∂ÊûÑÔºåÂÖ∑ÊúâÂèåÂêë‰∫§ÂèâÊ≥®ÊÑèÂäõËûçÂêàÊú∫Âà∂ÂíåÊùÉÈáçÂàÜËß£‰ΩéÁß©ÈÄÇÂ∫îÔºàDoRAÔºâÂæÆË∞ÉÔºå‰ª•ÂÆûÁé∞ËØ≠Ë®ÄÊù°‰ª∂‰∏ãÁöÑÁ≤æÁªÜËßÜËßâÂÆö‰Ωç„ÄÇÂú®Ê®°ÊãüÂíåÁúüÂÆû‰∏ñÁïåÁöÑÂÆûÈ™åÂùáÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÂú®Ê®°Êãü‰∏≠ÔºåËØ•ÊñπÊ≥ïÂú®Â∑≤ËßÅÊåá‰ª§„ÄÅÊú™ËßÅÊåá‰ª§ÂíåÊú™ËßÅ‰ªªÂä°‰∏äÂàÜÂà´‰ºò‰∫éÊúÄÂÖàËøõÁöÑÂü∫Á∫ø2.23„ÄÅ1.87Âíå33.3„ÄÇÂú®ÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÔºåËØ•ÊñπÊ≥ïËÉΩÂ§ü‰ªéËØ≠Ë®ÄÊåá‰ª§‰∏≠Á®≥ÂÅ•Âú∞ÊâßË°åË∑®Â§öÁßçÂ∏ÉÊñôÊùêÊñôÂíåÈÖçÁΩÆÁöÑÂ§öÊ≠•È™§ÊäòÂè†Â∫èÂàóÔºåÂ±ïÁ§∫‰∫ÜÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂº∫Â§ßÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËØ≠Ë®ÄÂºïÂØº‰∏ãÁöÑÂèØÂèòÂΩ¢Áâ©‰ΩìÔºàÁâπÂà´ÊòØÂ∏ÉÊñôÔºâÈïøÊó∂Á®ãÊìç‰ΩúÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•Â§ÑÁêÜÂ∏ÉÊñôÁöÑÈ´òËá™Áî±Â∫¶„ÄÅÂ§çÊùÇÂä®ÂäõÂ≠¶‰ª•ÂèäËØ≠Ë®ÄÊåá‰ª§Âà∞ÂÖ∑‰ΩìÂä®‰ΩúÁöÑÊò†Â∞ÑÔºåÂØºËá¥Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÊÑüÁü•ÂíåËßÑÂàí‰∏äÂ≠òÂú®ËÑ±ËäÇÔºåÈöæ‰ª•ÂÆûÁé∞Á≤æÁ°ÆÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËøõË°åÈ´òÂ±ÇËßÑÂàíÔºåÂ∞ÜËØ≠Ë®ÄÊåá‰ª§ÂàÜËß£‰∏∫ÂèØÊâßË°åÁöÑÂä®‰ΩúÂ∫èÂàóÔºåÂπ∂ÁªìÂêàËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËøõË°åÁ≤æÁªÜÁöÑËßÜËßâÊÑüÁü•Ôºå‰ªéËÄåÂÆûÁé∞ËØ≠Ë®ÄÂºïÂØº‰∏ãÁöÑÈïøÊó∂Á®ãÊìç‰Ωú„ÄÇÈÄöËøáLLMÂº•ÂêàËØ≠‰πâÂíåÊâßË°å‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºåÂπ∂ÂØπÈΩêÊÑüÁü•ÂíåÂä®‰Ωú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Âü∫‰∫éLLMÁöÑËßÑÂàíÂô®ÔºöË¥üË¥£Â∞ÜÈ´òÂ±ÇËØ≠Ë®ÄÊåá‰ª§ÂàÜËß£‰∏∫‰ΩéÂ±ÇÂä®‰ΩúÂéüËØ≠Â∫èÂàó„ÄÇ2) Âü∫‰∫éVLMÁöÑÊÑüÁü•Ê®°ÂùóÔºöË¥üË¥£Ê†πÊçÆËØ≠Ë®ÄÊåá‰ª§ËøõË°åËßÜËßâÂÆö‰ΩçÔºåËØÜÂà´Â∏ÉÊñôÁöÑÁä∂ÊÄÅÂíåÂÖ≥ÈîÆÁÇπ„ÄÇ3) ‰ªªÂä°ÊâßË°åÊ®°ÂùóÔºöË¥üË¥£ÊâßË°åËßÑÂàíÂô®ÁîüÊàêÁöÑÂä®‰ΩúÂ∫èÂàóÔºåÂÆåÊàêÂ∏ÉÊñôÊäòÂè†‰ªªÂä°„ÄÇVLMÊÑüÁü•Ê®°ÂùóÈááÁî®SigLIP2È©±Âä®ÁöÑÊû∂ÊûÑÔºåÂπ∂‰ΩøÁî®ÂèåÂêë‰∫§ÂèâÊ≥®ÊÑèÂäõËûçÂêàÊú∫Âà∂ÂíåDoRAÂæÆË∞É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜLLMÂºïÂÖ•Âà∞ÂèØÂèòÂΩ¢Áâ©‰ΩìÊìç‰ΩúÁöÑËßÑÂàí‰∏≠ÔºåÂà©Áî®LLMÂº∫Â§ßÁöÑËØ≠Ë®ÄÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõÔºåÂÆûÁé∞‰∫Ü‰ªéÈ´òÂ±ÇËØ≠Ë®ÄÊåá‰ª§Âà∞‰ΩéÂ±ÇÂä®‰ΩúÂéüËØ≠ÁöÑÊúâÊïàÊò†Â∞Ñ„ÄÇÊ≠§Â§ñÔºåVLMÊÑüÁü•Ê®°ÂùóÈááÁî®ÂèåÂêë‰∫§ÂèâÊ≥®ÊÑèÂäõËûçÂêàÊú∫Âà∂ÂíåDoRAÂæÆË∞ÉÔºåÊèêÈ´ò‰∫ÜËßÜËßâÂÆö‰ΩçÁöÑÁ≤æÂ∫¶ÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVLMÊÑüÁü•Ê®°ÂùóÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®SigLIP2‰Ωú‰∏∫ËßÜËßâÁºñÁ†ÅÂô®Ôºå‰ª•Ëé∑ÂæóÊõ¥Âº∫ÁöÑËßÜËßâË°®ÂæÅËÉΩÂäõ„ÄÇ2) ÈááÁî®ÂèåÂêë‰∫§ÂèâÊ≥®ÊÑèÂäõËûçÂêàÊú∫Âà∂ÔºåÂ∞ÜËØ≠Ë®Ä‰ø°ÊÅØÂíåËßÜËßâ‰ø°ÊÅØËøõË°åÊúâÊïàËûçÂêà„ÄÇ3) ‰ΩøÁî®DoRAÂæÆË∞ÉÔºåÂú®‰øùÊåÅÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇLLMËßÑÂàíÂô®‰ΩøÁî®Prompt EngineeringÊù•ÊåáÂØºLLMÁîüÊàêÂêàÈÄÇÁöÑÂä®‰ΩúÂ∫èÂàó„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÊñπÊ≥ïÂú®Ê®°ÊãüÁéØÂ¢É‰∏≠ÔºåÂú®Â∑≤ËßÅÊåá‰ª§„ÄÅÊú™ËßÅÊåá‰ª§ÂíåÊú™ËßÅ‰ªªÂä°‰∏äÂàÜÂà´‰ºò‰∫éÊúÄÂÖàËøõÁöÑÂü∫Á∫ø2.23„ÄÅ1.87Âíå33.3„ÄÇÂú®ÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÔºåËØ•ÊñπÊ≥ïËÉΩÂ§ü‰ªéËØ≠Ë®ÄÊåá‰ª§‰∏≠Á®≥ÂÅ•Âú∞ÊâßË°åË∑®Â§öÁßçÂ∏ÉÊñôÊùêÊñôÂíåÈÖçÁΩÆÁöÑÂ§öÊ≠•È™§ÊäòÂè†Â∫èÂàóÔºåÂ±ïÁ§∫‰∫ÜÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂÆûÈôÖÂ∫îÁî®ÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËá™Âä®ÂåñÊúçË£ÖÂà∂ÈÄ†„ÄÅÂÆ∂Áî®Êú∫Âô®‰∫∫„ÄÅÂåªÁñóÊä§ÁêÜÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®ÊúçË£ÖÂà∂ÈÄ†‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆËÆæËÆ°ÂõæÁ∫∏Ëá™Âä®ÂÆåÊàêÂ∏ÉÊñôÁöÑË£ÅÂâ™ÂíåÁºùÁ∫´„ÄÇÂú®ÂÆ∂Áî®Êú∫Âô®‰∫∫È¢ÜÂüüÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑËØ≠Ë®ÄÊåá‰ª§ÂÆåÊàêË°£Áâ©ÁöÑÊï¥ÁêÜÂíåÊäòÂè†„ÄÇÂú®ÂåªÁñóÊä§ÁêÜÈ¢ÜÂüüÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•ËæÖÂä©ÂåªÊä§‰∫∫ÂëòËøõË°åÊâãÊúØÊàñÂ∫∑Â§çËÆ≠ÁªÉ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Language-guided long-horizon manipulation of deformable objects presents significant challenges due to high degrees of freedom, complex dynamics, and the need for accurate vision-language grounding. In this work, we focus on multi-step cloth folding, a representative deformable-object manipulation task that requires both structured long-horizon planning and fine-grained visual perception. To this end, we propose a unified framework that integrates a Large Language Model (LLM)-based planner, a Vision-Language Model (VLM)-based perception system, and a task execution module. Specifically, the LLM-based planner decomposes high-level language instructions into low-level action primitives, bridging the semantic-execution gap, aligning perception with action, and enhancing generalization. The VLM-based perception module employs a SigLIP2-driven architecture with a bidirectional cross-attention fusion mechanism and weight-decomposed low-rank adaptation (DoRA) fine-tuning to achieve language-conditioned fine-grained visual grounding. Experiments in both simulation and real-world settings demonstrate the method's effectiveness. In simulation, it outperforms state-of-the-art baselines by 2.23, 1.87, and 33.3 on seen instructions, unseen instructions, and unseen tasks, respectively. On a real robot, it robustly executes multi-step folding sequences from language instructions across diverse cloth materials and configurations, demonstrating strong generalization in practical scenarios. Project page: https://language-guided.netlify.app/

