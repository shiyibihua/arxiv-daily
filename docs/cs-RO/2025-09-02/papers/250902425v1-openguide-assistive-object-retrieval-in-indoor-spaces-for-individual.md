---
layout: default
title: OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments
---

# OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02425" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02425v1</a>
  <a href="https://arxiv.org/pdf/2509.02425.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02425v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02425v1', 'OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Xu, Qianwei Wang, Vineet Kamat, Carol Menassa

**åˆ†ç±»**: cs.RO, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

**å¤‡æ³¨**: 32 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OpenGuideï¼šé¢å‘è§†éšœäººå£«çš„å®¤å†…è¾…åŠ©ç‰©ä½“æ£€ç´¢æœºå™¨äººç³»ç»Ÿ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¾…åŠ©` `æœºå™¨äººå¯¼èˆª` `è§†è§‰-è¯­è¨€æ¨¡å‹` `POMDP` `å®¤å†…åœºæ™¯` `ç‰©ä½“æ£€ç´¢` `è§†éšœäººå£«` `è‡ªä¸»æ¢ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å®¤å†…è¾…åŠ©æŠ€æœ¯ç¼ºä¹åœ¨å¤æ‚ç¯å¢ƒä¸­é«˜æ•ˆæœç´¢å¤šä¸ªç›®æ ‡ç‰©ä½“çš„èƒ½åŠ›ï¼Œå¯¹è§†éšœäººå£«æ„æˆæŒ‘æˆ˜ã€‚
2. OpenGuideç»“åˆè‡ªç„¶è¯­è¨€ç†è§£ã€è§†è§‰-è¯­è¨€æ¨¡å‹ã€å‰æ²¿æ¢ç´¢å’ŒPOMDPè§„åˆ’ï¼Œå®ç°å¤šç›®æ ‡ç‰©ä½“çš„è‡ªé€‚åº”æœç´¢ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenGuideåœ¨ä»»åŠ¡æˆåŠŸç‡å’Œæœç´¢æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºè¾…åŠ©ç”Ÿæ´»æä¾›æ–°æ–¹æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®¤å†…ç¯å¢ƒï¼Œå¦‚å®¶åº­å’ŒåŠå…¬å®¤ï¼Œé€šå¸¸å¸ƒå±€å¤æ‚ä¸”æ‚ä¹±ï¼Œè¿™ç»™ç›²äººæˆ–è§†éšœäººå£«å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ‰§è¡Œæ¶‰åŠå®šä½å’Œæ”¶é›†å¤šä¸ªç‰©ä½“çš„ä»»åŠ¡æ—¶ã€‚è™½ç„¶è®¸å¤šç°æœ‰çš„è¾…åŠ©æŠ€æœ¯ä¾§é‡äºåŸºæœ¬çš„å¯¼èˆªæˆ–é¿éšœï¼Œä½†å¾ˆå°‘æœ‰ç³»ç»Ÿèƒ½å¤Ÿåœ¨çœŸå®ã€éƒ¨åˆ†å¯è§‚å¯Ÿçš„ç¯å¢ƒä¸­æä¾›å¯æ‰©å±•ä¸”é«˜æ•ˆçš„å¤šç‰©ä½“æœç´¢èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬ä»‹ç»äº†OpenGuideï¼Œè¿™æ˜¯ä¸€ç§è¾…åŠ©ç§»åŠ¨æœºå™¨äººç³»ç»Ÿï¼Œå®ƒç»“åˆäº†è‡ªç„¶è¯­è¨€ç†è§£ä¸è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLMï¼‰ã€åŸºäºå‰æ²¿çš„æ¢ç´¢å’Œéƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰è§„åˆ’å™¨ã€‚OpenGuideè§£é‡Šå¼€æ”¾è¯æ±‡è¯·æ±‚ï¼Œæ¨ç†ç‰©ä½“-åœºæ™¯å…³ç³»ï¼Œå¹¶è‡ªé€‚åº”åœ°å¯¼èˆªå’Œå®šä½æ–°ç¯å¢ƒä¸­çš„å¤šä¸ªç›®æ ‡ç‰©å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»·å€¼è¡°å‡å’Œä¿¡å¿µç©ºé—´æ¨ç†ï¼Œå®ç°äº†ä»æ¼æ£€ä¸­çš„ç¨³å¥æ¢å¤ï¼Œä»è€Œæé«˜äº†æ¢ç´¢å’Œç‰©ä½“å®šä½çš„æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­éªŒè¯äº†OpenGuideï¼Œè¯æ˜äº†å…¶åœ¨ä»»åŠ¡æˆåŠŸç‡å’Œæœç´¢æ•ˆç‡æ–¹é¢ç›¸å¯¹äºå…ˆå‰æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚è¿™é¡¹å·¥ä½œä¸ºè¾…åŠ©ç”Ÿæ´»ç¯å¢ƒä¸­å¯æ‰©å±•çš„ã€ä»¥äººä¸ºæœ¬çš„æœºå™¨äººè¾…åŠ©å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†éšœäººå£«åœ¨å¤æ‚å®¤å†…ç¯å¢ƒä¸­éš¾ä»¥é«˜æ•ˆå®šä½å’Œæ£€ç´¢å¤šä¸ªç›®æ ‡ç‰©ä½“çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾§é‡äºå¯¼èˆªæˆ–é¿éšœï¼Œç¼ºä¹å¯æ‰©å±•çš„å¤šç‰©ä½“æœç´¢èƒ½åŠ›ï¼Œå®¹æ˜“å—åˆ°ç¯å¢ƒå™ªå£°å’Œç‰©ä½“æ¼æ£€çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOpenGuideçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªç„¶è¯­è¨€ç†è§£ç”¨æˆ·éœ€æ±‚ï¼Œç»“åˆè§†è§‰-è¯­è¨€æ¨¡å‹ç†è§£åœºæ™¯å’Œç‰©ä½“å…³ç³»ï¼Œå¹¶é€šè¿‡åŸºäºå‰æ²¿çš„æ¢ç´¢å’ŒPOMDPè§„åˆ’ï¼Œå®ç°å¯¹å¤šä¸ªç›®æ ‡ç‰©ä½“çš„è‡ªé€‚åº”æœç´¢å’Œå®šä½ã€‚è¿™ç§ç»“åˆä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿä»æ¼æ£€ä¸­æ¢å¤ï¼Œå¹¶æ ¹æ®ç¯å¢ƒä¿¡æ¯è°ƒæ•´æœç´¢ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOpenGuideç³»ç»ŸåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å—ï¼Œç”¨äºè§£æç”¨æˆ·è¯·æ±‚ï¼›2) è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç”¨äºç†è§£åœºæ™¯å’Œç‰©ä½“å…³ç³»ï¼›3) åŸºäºå‰æ²¿çš„æ¢ç´¢æ¨¡å—ï¼Œç”¨äºè‡ªä¸»æ¢ç´¢æœªçŸ¥åŒºåŸŸï¼›4) POMDPè§„åˆ’å™¨ï¼Œç”¨äºåˆ¶å®šæœ€ä¼˜çš„å¯¼èˆªå’Œæœç´¢ç­–ç•¥ã€‚ç³»ç»Ÿé¦–å…ˆè§£æç”¨æˆ·è¯·æ±‚ï¼Œç„¶ååˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹è¯†åˆ«æ½œåœ¨çš„ç›®æ ‡ç‰©ä½“ä½ç½®ï¼Œæ¥ç€é€šè¿‡å‰æ²¿æ¢ç´¢æ¨¡å—æ¢ç´¢æœªçŸ¥åŒºåŸŸï¼Œæœ€åä½¿ç”¨POMDPè§„åˆ’å™¨åˆ¶å®šæœ€ä¼˜çš„æœç´¢è·¯å¾„ã€‚

**å…³é”®åˆ›æ–°**ï¼šOpenGuideçš„å…³é”®åˆ›æ–°åœ¨äºå°†è§†è§‰-è¯­è¨€æ¨¡å‹ä¸POMDPè§„åˆ’å™¨ç›¸ç»“åˆï¼Œå®ç°äº†åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿç¯å¢ƒä¸‹çš„é²æ£’å¤šç‰©ä½“æœç´¢ã€‚é€šè¿‡ä»·å€¼è¡°å‡å’Œä¿¡å¿µç©ºé—´æ¨ç†ï¼Œç³»ç»Ÿèƒ½å¤Ÿä»æ¼æ£€ä¸­æ¢å¤ï¼Œå¹¶æ ¹æ®ç¯å¢ƒä¿¡æ¯åŠ¨æ€è°ƒæ•´æœç´¢ç­–ç•¥ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜é‡‡ç”¨äº†åŸºäºå‰æ²¿çš„æ¢ç´¢æ–¹æ³•ï¼Œæé«˜äº†æœç´¢æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šOpenGuideä½¿ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹å®¤å†…ç‰©ä½“æœç´¢ä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒã€‚POMDPè§„åˆ’å™¨é‡‡ç”¨äº†å€¼è¿­ä»£ç®—æ³•ï¼Œå¹¶ä½¿ç”¨ä»·å€¼è¡°å‡æ¥å¤„ç†æ¼æ£€æƒ…å†µã€‚åŸºäºå‰æ²¿çš„æ¢ç´¢æ¨¡å—é‡‡ç”¨äº†è´ªå©ªç­–ç•¥ï¼Œä¼˜å…ˆæ¢ç´¢æœªçŸ¥çš„åŒºåŸŸã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OpenGuideåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­å‡å–å¾—äº†æ˜¾è‘—æˆæœã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOpenGuideåœ¨ä»»åŠ¡æˆåŠŸç‡æ–¹é¢æå‡äº†XX%ï¼ˆå…·ä½“æ•°æ®è¯·æŸ¥é˜…åŸæ–‡ï¼‰ï¼Œæœç´¢æ•ˆç‡æå‡äº†YY%ï¼ˆå…·ä½“æ•°æ®è¯·æŸ¥é˜…åŸæ–‡ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenGuideèƒ½å¤Ÿæœ‰æ•ˆåœ°å®šä½å’Œæ£€ç´¢å¤šä¸ªç›®æ ‡ç‰©ä½“ï¼Œå¹¶ä»æ¼æ£€ä¸­ç¨³å¥æ¢å¤ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OpenGuideå¯åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€å…»è€é™¢ã€åŒ»é™¢ç­‰è¾…åŠ©ç”Ÿæ´»ç¯å¢ƒï¼Œå¸®åŠ©è§†éšœäººå£«è‡ªä¸»å®Œæˆç‰©å“æ£€ç´¢ä»»åŠ¡ï¼Œæé«˜ç”Ÿæ´»è´¨é‡å’Œç‹¬ç«‹æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œå¦‚ä»“åº“ç®¡ç†ã€æ™ºèƒ½å®‰é˜²ç­‰ï¼Œå®ç°æ›´å¹¿æ³›çš„æœºå™¨äººè¾…åŠ©åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Indoor built environments like homes and offices often present complex and cluttered layouts that pose significant challenges for individuals who are blind or visually impaired, especially when performing tasks that involve locating and gathering multiple objects. While many existing assistive technologies focus on basic navigation or obstacle avoidance, few systems provide scalable and efficient multi-object search capabilities in real-world, partially observable settings. To address this gap, we introduce OpenGuide, an assistive mobile robot system that combines natural language understanding with vision-language foundation models (VLM), frontier-based exploration, and a Partially Observable Markov Decision Process (POMDP) planner. OpenGuide interprets open-vocabulary requests, reasons about object-scene relationships, and adaptively navigates and localizes multiple target items in novel environments. Our approach enables robust recovery from missed detections through value decay and belief-space reasoning, resulting in more effective exploration and object localization. We validate OpenGuide in simulated and real-world experiments, demonstrating substantial improvements in task success rate and search efficiency over prior methods. This work establishes a foundation for scalable, human-centered robotic assistance in assisted living environments.

