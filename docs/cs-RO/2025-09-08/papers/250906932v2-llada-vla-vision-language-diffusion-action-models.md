---
layout: default
title: LLaDA-VLA: Vision Language Diffusion Action Models
---

# LLaDA-VLA: Vision Language Diffusion Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.06932" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.06932v2</a>
  <a href="https://arxiv.org/pdf/2509.06932.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.06932v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.06932v2', 'LLaDA-VLA: Vision Language Diffusion Action Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yuqing Wen, Hebei Li, Kefan Gu, Yucheng Zhao, Tiancai Wang, Xiaoyan Sun

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-08 (Êõ¥Êñ∞: 2025-09-10)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LLaDA-VLA‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÂª∫Ê®°ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Êú∫Âô®‰∫∫Êìç‰Ωú` `Êâ©Êï£Ê®°Âûã` `Âä®‰ΩúÂª∫Ê®°` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Á≠ñÁï•Â≠¶‰π†` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠Â∫îÁî®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Á≠ñÁï•Â≠¶‰π†ÊñπÈù¢Â≠òÂú®ÊåëÊàò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑLLaDA-VLAÊ®°ÂûãÈÄöËøáÂºïÂÖ•ÁâπÊÆäÊ†áËÆ∞ÂàÜÁ±ªÂíåÂàÜÂ±ÇËß£Á†ÅÁ≠ñÁï•ÔºåÊúâÊïàÂú∞ÈÄÇÂ∫î‰∫ÜÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÈúÄÊ±Ç„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLLaDA-VLAÂú®‰ªøÁúüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ÂùáÊòæËëó‰ºò‰∫éÂΩìÂâçÊúÄÂÖàËøõÁöÑËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄËá™ÂõûÂΩíËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºåËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÔºàVLAÔºâÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÂ∫îÁî®ÂºïËµ∑‰∫ÜË∂äÊù•Ë∂äÂ§öÁöÑÂÖ≥Ê≥®„ÄÇËøëÊúüÔºåÊé©ËîΩÊâ©Êï£Ê®°Âûã‰Ωú‰∏∫‰∏ÄÁßç‰∏éËá™ÂõûÂΩíÊ®°Âûã‰∏çÂêåÁöÑËåÉÂºèÔºåÂ∑≤Âú®ÊñáÊú¨ÁîüÊàêÂíåÂ§öÊ®°ÊÄÅÂ∫îÁî®‰∏≠Â±ïÁé∞Âá∫Á´û‰∫âÂäõ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜLLaDA-VLAÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Âü∫‰∫éÈ¢ÑËÆ≠ÁªÉÊâ©Êï£Ê®°ÂûãÁöÑËßÜËßâ-ËØ≠Ë®Ä-Êâ©Êï£-Âä®‰ΩúÊ®°ÂûãÔºåÊó®Âú®ÊúâÊïàÈÄÇÂ∫îÊú∫Âô®‰∫∫È¢ÜÂüü„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏§È°πÂÖ≥ÈîÆËÆæËÆ°Ôºö‰∏ÄÊòØÂ±ÄÈÉ®ÁâπÊÆäÊ†áËÆ∞ÂàÜÁ±ªÁ≠ñÁï•Ôºå‰∫åÊòØÂàÜÂ±ÇÂä®‰ΩúÁªìÊûÑËß£Á†ÅÁ≠ñÁï•„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåLLaDA-VLAÂú®‰ªøÁúüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõVLA„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â¶Ç‰ΩïÂ∞ÜÊâ©Êï£Ê®°ÂûãÊúâÊïàÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÂª∫Ê®°ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Á≠ñÁï•Â≠¶‰π†‰∏äÂ≠òÂú®ÈÄÇÂ∫îÊÄß‰∏çË∂≥ÂíåÂ§çÊùÇÊÄßÈ´òÁöÑÁóõÁÇπ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLLaDA-VLAÈÄöËøáÂºïÂÖ•Â±ÄÈÉ®ÁâπÊÆäÊ†áËÆ∞ÂàÜÁ±ªÂíåÂàÜÂ±ÇÂä®‰ΩúÁªìÊûÑËß£Á†ÅÔºåÈôç‰Ωé‰∫ÜÊ®°ÂûãÈÄÇÂ∫îÈöæÂ∫¶Âπ∂ÊèêÈ´ò‰∫ÜÂä®‰ΩúÂ∫èÂàóÁöÑËß£Á†ÅÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê®°ÂûãÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨È¢ÑËÆ≠ÁªÉÁöÑÊâ©Êï£Ê®°Âûã‰Ωú‰∏∫Âü∫Á°ÄÔºåÁªìÂêàÁâπÊÆäÊ†áËÆ∞ÂàÜÁ±ªÂíåÂàÜÂ±ÇËß£Á†ÅÊ®°ÂùóÔºåÂΩ¢Êàê‰∏Ä‰∏™ÂÆåÊï¥ÁöÑËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÁîüÊàêÊµÅÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLLaDA-VLAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Â±ÄÈÉ®ÁâπÊÆäÊ†áËÆ∞ÂàÜÁ±ªÁ≠ñÁï•ÂíåÂàÜÂ±ÇËß£Á†ÅÁ≠ñÁï•ÔºåËøô‰∏é‰º†ÁªüÁöÑÂÖ®ËØçÊ±áÂàÜÁ±ªÂíåÁ∫øÊÄßËß£Á†ÅÊñπÂºèÊúâÊú¨Ë¥®Âå∫Âà´ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÈÄÇÂ∫îÊÄßÂíåÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÊÆäÂä®‰ΩúÊ†áËÆ∞Êõø‰ª£ÂÖ®ËØçÊ±áÂàÜÁ±ªÔºåÂáèÂ∞ë‰∫ÜÂàÜÁ±ªÂ§çÊùÇÂ∫¶ÔºõÂêåÊó∂ÔºåÂàÜÂ±ÇËß£Á†ÅÁ≠ñÁï•ËÄÉËôë‰∫ÜÂä®‰ΩúÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºå‰ºòÂåñ‰∫ÜÂä®‰ΩúÂ∫èÂàóÁöÑÁîüÊàêËøáÁ®ã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLLaDA-VLAÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂùáÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÔºåÂÖ∑‰ΩìË°®Áé∞‰∏∫Âú®‰ªøÁúüÁéØÂ¢É‰∏≠ÊÄßËÉΩÊèêÂçáË∂ÖËøá20%ÔºåÂú®ÁúüÂÆûÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÊèêÂçáÂπÖÂ∫¶ËææÂà∞15%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LLaDA-VLAÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰ΩúÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÂ§çÊùÇÂä®‰ΩúÂ∫èÂàóÁîüÊàêÁöÑÂú∫ÊôØÔºåÂ¶ÇÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÊúçÂä°Êú∫Âô®‰∫∫Âíå‰∫∫Êú∫Âçè‰ΩúÁ≠â„ÄÇÂÖ∂ÊúâÊïàÁöÑÁ≠ñÁï•Â≠¶‰π†ËÉΩÂäõÂ∞ÜÊé®Âä®Êú∫Âô®‰∫∫Âú®Â§öÊ®°ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÊô∫ËÉΩÂåñÂèëÂ±ïÔºåÊèêÂçáÊìç‰ΩúÊïàÁéáÂíåÁÅµÊ¥ªÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The rapid progress of auto-regressive vision-language models (VLMs) has inspired growing interest in vision-language-action models (VLA) for robotic manipulation. Recently, masked diffusion models, a paradigm distinct from autoregressive models, have begun to demonstrate competitive performance in text generation and multimodal applications, leading to the development of a series of diffusion-based VLMs (d-VLMs). However, leveraging such models for robot policy learning remains largely unexplored. In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to robotic domain, we introduce two key designs: (1) a localized special-token classification strategy that replaces full-vocabulary classification with special action token classification, reducing adaptation difficulty; (2) a hierarchical action-structured decoding strategy that decodes action sequences hierarchically considering the dependencies within and across actions. Extensive experiments demonstrate that LLaDA-VLA significantly outperforms state-of-the-art VLAs on both simulation and real-world robots.

