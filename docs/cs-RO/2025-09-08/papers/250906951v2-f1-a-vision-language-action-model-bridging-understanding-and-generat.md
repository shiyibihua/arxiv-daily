---
layout: default
title: F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions
---

# F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.06951" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.06951v2</a>
  <a href="https://arxiv.org/pdf/2509.06951.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.06951v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.06951v2', 'F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, Jiangmiao Pang

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-08 (æ›´æ–°: 2025-09-09)

**å¤‡æ³¨**: Homepage: https://aopolin-lv.github.io/F1-VLA/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**F1æ¨¡å‹ï¼šé€šè¿‡è§†è§‰é¢„æµ‹è¿æ¥ç†è§£ã€ç”Ÿæˆä¸åŠ¨ä½œï¼Œæå‡å…·èº«æ™ºèƒ½ä»»åŠ¡æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `è§†è§‰é¢„æµ‹` `å…·èº«æ™ºèƒ½` `Transformer` `é€†åŠ¨åŠ›å­¦` `é•¿æœŸè§„åˆ’` `åŠ¨æ€ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¡¨ç°å‡ºçŸ­è§†è¡Œä¸ºå’Œé²æ£’æ€§ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆæ‰§è¡Œè¯­è¨€æ¡ä»¶ä»»åŠ¡ã€‚
2. F1æ¨¡å‹é€šè¿‡è§†è§‰é¢„æµ‹ç”Ÿæˆæ˜¾å¼è§„åˆ’ç›®æ ‡ï¼Œå°†åŠ¨ä½œç”Ÿæˆè½¬åŒ–ä¸ºé¢„æµ‹å¼•å¯¼çš„é€†åŠ¨åŠ›å­¦é—®é¢˜ã€‚
3. F1æ¨¡å‹åœ¨åŒ…å«å¤§é‡ä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œä¸‰é˜¶æ®µè®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†ä»»åŠ¡æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºF1çš„é¢„è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…·èº«AIä¸­åŠ¨æ€è§†è§‰ç¯å¢ƒä¸‹è¯­è¨€æ¡ä»¶ä»»åŠ¡çš„æ‰§è¡Œé—®é¢˜ã€‚ç°æœ‰VLAæ¨¡å‹ä¸»è¦é‡‡ç”¨ååº”å¼çš„çŠ¶æ€-åŠ¨ä½œæ˜ å°„ï¼Œå¯¼è‡´çŸ­è§†è¡Œä¸ºå’Œåœ¨åŠ¨æ€åœºæ™¯ä¸­çš„é²æ£’æ€§è¾ƒå·®ã€‚F1é›†æˆäº†è§†è§‰é¢„æµ‹ç”Ÿæˆåˆ°å†³ç­–æµç¨‹ä¸­ï¼Œé‡‡ç”¨æ··åˆTransformeræ¶æ„ï¼ŒåŒ…å«æ„ŸçŸ¥ã€é¢„æµ‹ç”Ÿæˆå’Œæ§åˆ¶ç­‰ä¸“ç”¨æ¨¡å—ï¼Œä»è€Œè¿æ¥ç†è§£ã€ç”Ÿæˆå’ŒåŠ¨ä½œã€‚F1çš„æ ¸å¿ƒæ˜¯ä¸‹ä¸€å°ºåº¦é¢„æµ‹æœºåˆ¶ï¼Œåˆæˆç›®æ ‡æ¡ä»¶ä¸‹çš„è§†è§‰é¢„æµ‹ä½œä¸ºæ˜¾å¼è§„åˆ’ç›®æ ‡ã€‚é€šè¿‡é¢„æµ‹æœªæ¥å¯èƒ½çš„è§†è§‰çŠ¶æ€ï¼ŒF1å°†åŠ¨ä½œç”Ÿæˆé‡æ„ä¸ºé¢„æµ‹å¼•å¯¼çš„é€†åŠ¨åŠ›å­¦é—®é¢˜ï¼Œä»è€Œå®ç°éšå¼åœ°è¾¾æˆè§†è§‰ç›®æ ‡çš„åŠ¨ä½œã€‚ä¸ºäº†èµ‹äºˆF1é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œåœ¨ä¸€ä¸ªåŒ…å«è¶…è¿‡33ä¸‡æ¡è½¨è¿¹å’Œ136ä¸ªä¸åŒä»»åŠ¡çš„å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥è®­ç»ƒæ–¹æ¡ˆå¢å¼ºäº†æ¨¡å—åŒ–æ¨ç†ï¼Œå¹¶ä½¿æ¨¡å‹å…·å¤‡å¯è¿ç§»çš„è§†è§‰é¢„æµ‹èƒ½åŠ›ï¼Œè¿™å¯¹äºå¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒè‡³å…³é‡è¦ã€‚åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡å’Œæ¨¡æ‹ŸåŸºå‡†ä¸Šçš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼ŒF1å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ä»»åŠ¡æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ‰§è¡Œè¯­è¨€æ¡ä»¶ä»»åŠ¡æ—¶ï¼Œä¸»è¦ä¾èµ–ååº”å¼çš„çŠ¶æ€-åŠ¨ä½œæ˜ å°„ï¼Œç¼ºä¹å¯¹æœªæ¥çŠ¶æ€çš„é¢„æµ‹å’Œè§„åˆ’ï¼Œå¯¼è‡´çŸ­è§†è¡Œä¸ºï¼Œéš¾ä»¥åº”å¯¹å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒï¼Œé²æ£’æ€§è¾ƒå·®ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿè¿›è¡Œé•¿æœŸè§„åˆ’å¹¶é€‚åº”åŠ¨æ€ç¯å¢ƒçš„VLAæ¨¡å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šF1æ¨¡å‹çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰é¢„æµ‹èå…¥åˆ°å†³ç­–è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡é¢„æµ‹æœªæ¥å¯èƒ½çš„è§†è§‰çŠ¶æ€ä½œä¸ºæ˜¾å¼çš„è§„åˆ’ç›®æ ‡ï¼Œä»è€ŒæŒ‡å¯¼åŠ¨ä½œçš„ç”Ÿæˆã€‚è¿™ç§æ–¹æ³•å°†åŠ¨ä½œç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªé¢„æµ‹å¼•å¯¼çš„é€†åŠ¨åŠ›å­¦é—®é¢˜ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆéšå¼åœ°è¾¾æˆè§†è§‰ç›®æ ‡çš„åŠ¨ä½œï¼Œä»è€Œå®ç°é•¿æœŸè§„åˆ’å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šF1æ¨¡å‹é‡‡ç”¨æ··åˆTransformeræ¶æ„ï¼ŒåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ„ŸçŸ¥æ¨¡å—ã€é¢„æµ‹ç”Ÿæˆæ¨¡å—å’Œæ§åˆ¶æ¨¡å—ã€‚æ„ŸçŸ¥æ¨¡å—è´Ÿè´£ä»è§†è§‰è¾“å…¥ä¸­æå–ç‰¹å¾ï¼›é¢„æµ‹ç”Ÿæˆæ¨¡å—åŸºäºæ„ŸçŸ¥ç‰¹å¾å’Œç›®æ ‡æ¡ä»¶ï¼Œé¢„æµ‹æœªæ¥å¯èƒ½çš„è§†è§‰çŠ¶æ€ï¼›æ§åˆ¶æ¨¡å—åˆ™æ ¹æ®é¢„æµ‹çš„æœªæ¥çŠ¶æ€å’Œå½“å‰çŠ¶æ€ï¼Œç”Ÿæˆç›¸åº”çš„åŠ¨ä½œã€‚æ•´ä¸ªæµç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼šè§†è§‰è¾“å…¥ -> æ„ŸçŸ¥ -> ç›®æ ‡æ¡ä»¶ -> è§†è§‰é¢„æµ‹ -> æ§åˆ¶ -> åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šF1æ¨¡å‹çš„å…³é”®åˆ›æ–°åœ¨äºå…¶è§†è§‰é¢„æµ‹æœºåˆ¶å’Œä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆã€‚è§†è§‰é¢„æµ‹æœºåˆ¶é€šè¿‡é¢„æµ‹æœªæ¥çŠ¶æ€ï¼Œä¸ºåŠ¨ä½œç”Ÿæˆæä¾›æ˜¾å¼ç›®æ ‡ï¼Œä»è€Œå®ç°é•¿æœŸè§„åˆ’ã€‚ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆåˆ™å¢å¼ºäº†æ¨¡å‹çš„æ¨¡å—åŒ–æ¨ç†èƒ½åŠ›å’Œå¯è¿ç§»çš„è§†è§‰é¢„æµ‹èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šF1æ¨¡å‹ä½¿ç”¨äº†ä¸‹ä¸€å°ºåº¦é¢„æµ‹æœºåˆ¶ï¼Œå…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ã€‚ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆåŒ…æ‹¬ï¼šç¬¬ä¸€é˜¶æ®µï¼Œé¢„è®­ç»ƒæ„ŸçŸ¥æ¨¡å—å’Œé¢„æµ‹ç”Ÿæˆæ¨¡å—ï¼›ç¬¬äºŒé˜¶æ®µï¼Œå¾®è°ƒæ•´ä¸ªæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæœ‰æ•ˆçš„åŠ¨ä½œï¼›ç¬¬ä¸‰é˜¶æ®µï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œå¯èƒ½åŒ…æ‹¬é¢„æµ‹æŸå¤±ã€åŠ¨ä½œæŸå¤±å’Œå¥–åŠ±æŸå¤±ç­‰ï¼Œå…·ä½“ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

F1æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡å’Œæ¨¡æ‹ŸåŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒF1æ¨¡å‹å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ä»»åŠ¡æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æå‡ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å±•ç¤ºï¼Œè¯æ˜äº†F1æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

F1æ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€ç‰©ä½“æ“ä½œã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡èµ‹äºˆæœºå™¨äººæ›´å¼ºçš„è§†è§‰é¢„æµ‹å’Œè§„åˆ’èƒ½åŠ›ï¼ŒF1æ¨¡å‹å¯ä»¥å¸®åŠ©æœºå™¨äººåœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­æ›´å¥½åœ°å®Œæˆä»»åŠ¡ï¼Œæé«˜å…¶è‡ªä¸»æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæœªæ¥å‘å±•æ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.

