---
layout: default
title: Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance
---

# Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.11173" target="_blank" class="toolbar-btn">arXiv: 2512.11173v1</a>
    <a href="https://arxiv.org/pdf/2512.11173.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11173v1" 
            onclick="toggleFavorite(this, '2512.11173v1', 'Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Tzu-Hsien Lee, Fidan Mahmudova, Karthik Desingh

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-11

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://rpm-lab-umn.github.io/category-level-last-meter-nav/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂçïÂÆû‰æãRGBÂõæÂÉèÊ®°‰ªøÂ≠¶‰π†ÁöÑÁ±ªÂà´Á∫ßÊú´Á´ØÂØºËà™ÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Êú´Á´ØÂØºËà™` `Ê®°‰ªøÂ≠¶‰π†` `Á±ªÂà´Á∫ßÊ≥õÂåñ` `ÁßªÂä®Êìç‰Ωú` `RGBÂõæÂÉè` `Êú∫Âô®‰∫∫ÂÆö‰Ωç` `ËØ≠Ë®ÄÈ©±Âä®ÂàÜÂâ≤`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éRGBÁöÑÂØºËà™Á≥ªÁªüÁ≤æÂ∫¶‰∏çË∂≥ÔºåÈöæ‰ª•Êª°Ë∂≥ÁßªÂä®Êìç‰Ωú‰∏≠Á≤æÁ°ÆÂÆö‰ΩçÁöÑÈúÄÊ±ÇÔºåÂØºËá¥Êìç‰ΩúÁ≠ñÁï•ÊâßË°åÂ§±Ë¥•„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÁâ©‰Ωì‰∏≠ÂøÉÁöÑÊ®°‰ªøÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂà©Áî®RGBÂõæÂÉè„ÄÅÊñáÊú¨ÊèêÁ§∫ÂíåÁ©∫Èó¥ÂæóÂàÜÁü©ÈòµËß£Á†ÅÂô®ÂÆûÁé∞Êú´Á´ØÂØºËà™„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êú™ËßÅËøáÁöÑÁâ©‰ΩìÂÆû‰æã‰∏äÂÆûÁé∞‰∫ÜËæÉÈ´òÁöÑËæπÁºòÂØπÈΩêÂíåÁâ©‰ΩìÂØπÈΩêÊàêÂäüÁéáÔºåÊó†ÈúÄÊ∑±Â∫¶‰ø°ÊÅØÊàñÂú∞ÂõæÂÖàÈ™å„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈù¢ÂêëÊú´Á´ØÂØºËà™ÁöÑ„ÄÅ‰ª•Áâ©‰Ωì‰∏∫‰∏≠ÂøÉÁöÑÊ®°‰ªøÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®‰ΩøÂõõË∂≥ÁßªÂä®Êú∫Ê¢∞ËáÇ‰ªÖ‰ΩøÁî®ÊùøËΩΩÊëÑÂÉèÂ§¥ÁöÑRGBÂõæÂÉèËßÇÊµãÔºåÂç≥ÂèØÂÆûÁé∞Êìç‰ΩúÂ∞±Áª™ÁöÑÁ≤æÁ°ÆÂÆö‰Ωç„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÂØºËà™Á≠ñÁï•Âª∫Á´ãÂú®‰∏â‰∏™ËæìÂÖ•‰πã‰∏äÔºöÁõÆÊ†áÂõæÂÉè„ÄÅÊù•Ëá™ÊùøËΩΩÊëÑÂÉèÂ§¥ÁöÑÂ§öËßÜËßíRGBËßÇÊµã‰ª•ÂèäÊåáÂÆöÁõÆÊ†áÁâ©‰ΩìÁöÑÊñáÊú¨ÊèêÁ§∫„ÄÇÁÑ∂ÂêéÔºå‰∏Ä‰∏™ËØ≠Ë®ÄÈ©±Âä®ÁöÑÂàÜÂâ≤Ê®°ÂùóÂíå‰∏Ä‰∏™Á©∫Èó¥ÂæóÂàÜÁü©ÈòµËß£Á†ÅÂô®Êèê‰æõÊòæÂºèÁöÑÁâ©‰ΩìÂÆö‰ΩçÂíåÁõ∏ÂØπÂßøÊÄÅÊé®ÁêÜ„ÄÇËØ•Á≥ªÁªü‰ΩøÁî®Êù•Ëá™Á±ªÂà´‰∏≠Âçï‰∏™Áâ©‰ΩìÂÆû‰æãÁöÑÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÔºåÊ≥õÂåñÂà∞ÂÖ∑ÊúâÊåëÊàòÊÄßÂÖâÁÖßÂíåËÉåÊôØÊù°‰ª∂ÁöÑ‰∏çÂêåÁéØÂ¢É‰∏≠Êú™ËßÅËøáÁöÑÁâ©‰ΩìÂÆû‰æã„ÄÇ‰∏∫‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåÂºïÂÖ•‰∫Ü‰∏§‰∏™ÊåáÊ†áÔºö‰ΩøÁî®ÁúüÂÆûÊñπÂêëÁöÑËæπÁºòÂØπÈΩêÊåáÊ†áÔºå‰ª•ÂèäËØÑ‰º∞Êú∫Âô®‰∫∫ËßÜËßâ‰∏ä‰∏éÁõÆÊ†áÂØπÈΩêÁ®ãÂ∫¶ÁöÑÁâ©‰ΩìÂØπÈΩêÊåáÊ†á„ÄÇÁªìÊûúË°®ÊòéÔºåËØ•Á≠ñÁï•Âú®Êú™ËßÅËøáÁöÑÁõÆÊ†áÁâ©‰ΩìÂÆö‰Ωç‰∏≠ÔºåËæπÁºòÂØπÈΩêÊàêÂäüÁéá‰∏∫73.47%ÔºåÁâ©‰ΩìÂØπÈΩêÊàêÂäüÁéá‰∏∫96.94%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÁßªÂä®Êú∫Ê¢∞ËáÇÊú´Á´ØÂØºËà™ÁöÑÁ≤æÁ°ÆÂÆö‰ΩçÈóÆÈ¢ò„ÄÇÁé∞ÊúâÂü∫‰∫éRGBÁöÑÂØºËà™Á≥ªÁªüÈÄöÂ∏∏Âè™ËÉΩÊèê‰æõÁ±≥Á∫ßÁ≤æÂ∫¶ÔºåÊó†Ê≥ïÊª°Ë∂≥ÂêéÁª≠Êìç‰ΩúÊâÄÈúÄÁöÑÁ≤æÁ°Æ‰ΩçÁΩÆÔºåÂØºËá¥Êìç‰ΩúÁ≠ñÁï•Êó†Ê≥ïÂú®ËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂàÜÂ∏ÉÂÜÖÊâßË°åÔºå‰ªéËÄåÂØºËá¥Â§±Ë¥•„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Ê®°‰ªøÂ≠¶‰π†ÔºåËÆ©Êú∫Âô®‰∫∫Â≠¶‰π†Â¶Ç‰Ωï‰ªÖÈÄöËøáRGBÂõæÂÉèËßÇÊµãÂíåÊñáÊú¨ÊèêÁ§∫ÔºåÂ∞ÜËá™Ë∫´ÂÆö‰ΩçÂà∞ÁõÆÊ†áÁâ©‰ΩìÈôÑËøëÔºåËææÂà∞Êìç‰ΩúÂ∞±Áª™ÁöÑÁä∂ÊÄÅ„ÄÇÈÄöËøáÂ≠¶‰π†Âçï‰∏™Áâ©‰ΩìÂÆû‰æãÁöÑÊï∞ÊçÆÔºåÂÆûÁé∞ÂØπÊï¥‰∏™Áâ©‰ΩìÁ±ªÂà´ÁöÑÊ≥õÂåñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Êé•Êî∂ÁõÆÊ†áÂõæÂÉè„ÄÅÂ§öËßÜËßíRGBËßÇÊµãÂíåÊñáÊú¨ÊèêÁ§∫‰Ωú‰∏∫ËæìÂÖ•Ôºõ2) ‰ΩøÁî®ËØ≠Ë®ÄÈ©±Âä®ÁöÑÂàÜÂâ≤Ê®°ÂùóËøõË°åÁâ©‰ΩìÂàÜÂâ≤ÔºåÊèêÂèñÁõÆÊ†áÁâ©‰ΩìÔºõ3) ‰ΩøÁî®Á©∫Èó¥ÂæóÂàÜÁü©ÈòµËß£Á†ÅÂô®ËøõË°åÁõ∏ÂØπÂßøÊÄÅÊé®ÁêÜÔºå‰º∞ËÆ°Êú∫Âô®‰∫∫‰∏éÁõÆÊ†áÁâ©‰Ωì‰πãÈó¥ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ªÔºõ4) Ê†πÊçÆ‰º∞ËÆ°ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ªÔºåÊéßÂà∂Êú∫Âô®‰∫∫ËøõË°åÂØºËà™„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂÆûÁé∞‰∫ÜÁ±ªÂà´Á∫ßÂà´ÁöÑÊú´Á´ØÂØºËà™ÔºåÂç≥‰ªÖ‰ΩøÁî®Âçï‰∏™Áâ©‰ΩìÂÆû‰æãÁöÑÊï∞ÊçÆÔºåÂ∞±ËÉΩÊ≥õÂåñÂà∞Âêå‰∏ÄÁ±ªÂà´‰∏ãÁöÑÂÖ∂‰ªñÊú™ËßÅËøáÁöÑÁâ©‰ΩìÂÆû‰æã„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ï‰ªÖ‰æùËµñRGBÂõæÂÉèÂíåÊñáÊú¨ÊèêÁ§∫ÔºåÊó†ÈúÄÊ∑±Â∫¶‰ø°ÊÅØ„ÄÅÊøÄÂÖâÈõ∑ËææÊàñÂú∞ÂõæÂÖàÈ™åÔºåÈôç‰Ωé‰∫ÜÁ≥ªÁªüÁöÑÂ§çÊùÇÊÄßÂíåÊàêÊú¨„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ËØ≠Ë®ÄÈ©±Âä®ÁöÑÂàÜÂâ≤Ê®°ÂùóÔºåÁî®‰∫é‰ªéRGBÂõæÂÉè‰∏≠ÂàÜÂâ≤Âá∫ÁõÆÊ†áÁâ©‰ΩìÔºõ2) Á©∫Èó¥ÂæóÂàÜÁü©ÈòµËß£Á†ÅÂô®ÔºåÁî®‰∫é‰º∞ËÆ°Êú∫Âô®‰∫∫‰∏éÁõÆÊ†áÁâ©‰Ωì‰πãÈó¥ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ªÔºõ3) ËæπÁºòÂØπÈΩêÂíåÁâ©‰ΩìÂØπÈΩê‰∏§‰∏™ËØÑ‰º∞ÊåáÊ†áÔºåÁî®‰∫éËØÑ‰º∞ÂØºËà™Á≠ñÁï•ÁöÑÊÄßËÉΩ„ÄÇÊçüÂ§±ÂáΩÊï∞Êú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êú™ËßÅËøáÁöÑÁõÆÊ†áÁâ©‰ΩìÂÆö‰Ωç‰∏≠ÔºåËæπÁºòÂØπÈΩêÊàêÂäüÁéá‰∏∫73.47%ÔºåÁâ©‰ΩìÂØπÈΩêÊàêÂäüÁéá‰∏∫96.94%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂú®Á±ªÂà´Á∫ßÂà´‰∏äÂÆûÁé∞Á≤æÁ°ÆÁöÑÊú´Á´ØÂØºËà™Ôºå‰∏îÊó†ÈúÄÊ∑±Â∫¶‰ø°ÊÅØ„ÄÅÊøÄÂÖâÈõ∑ËææÊàñÂú∞ÂõæÂÖàÈ™å„ÄÇËØ•ÊñπÊ≥ï‰∏∫Áªü‰∏ÄÁöÑÁßªÂä®Êìç‰ΩúÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÈÄîÂæÑ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÁ≤æÁ°ÆÂÆö‰ΩçÁöÑÁßªÂä®Êìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇÔºöÂú®ÂÆ∂Â∫≠ÁéØÂ¢É‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆÊåá‰ª§Â∞ÜËá™Ë∫´ÂÆö‰ΩçÂà∞ÁâπÂÆöÂÆ∂ÂÖ∑ÈôÑËøëÔºå‰ª•‰æøËøõË°åÊ∏ÖÊ¥Å„ÄÅÁª¥‰øÆÁ≠âÊìç‰ΩúÔºõÂú®Â∑•‰∏öÁéØÂ¢É‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Á≤æÁ°ÆÂÆö‰ΩçÂà∞Áîü‰∫ßÁ∫ø‰∏äÁöÑÁâπÂÆöÈÉ®‰ª∂ÈôÑËøëÔºå‰ª•‰æøËøõË°åÁªÑË£Ö„ÄÅÊ£ÄÊµãÁ≠âÊìç‰Ωú„ÄÇËØ•Á†îÁ©∂‰∏∫ÂÆûÁé∞ÈÄöÁî®ÁßªÂä®Êìç‰ΩúÊú∫Âô®‰∫∫Â•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/

