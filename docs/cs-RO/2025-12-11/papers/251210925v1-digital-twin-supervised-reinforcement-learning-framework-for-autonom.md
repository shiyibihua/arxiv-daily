---
layout: default
title: Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation
---

# Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.10925" target="_blank" class="toolbar-btn">arXiv: 2512.10925v1</a>
    <a href="https://arxiv.org/pdf/2512.10925.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10925v1" 
            onclick="toggleFavorite(this, '2512.10925v1', 'Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zamirddine Mari, Mohamad Motasem Nawaf, Pierre Drap

**ÂàÜÁ±ª**: cs.LG, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-11

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÊï∞Â≠óÂ≠™ÁîüÁõëÁù£Âº∫ÂåñÂ≠¶‰π†ÁöÑÊ∞¥‰∏ãËá™‰∏ªÂØºËà™Ê°ÜÊû∂ÔºåÊèêÂçáÂ§çÊùÇÁéØÂ¢ÉÈÄÇÂ∫îÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Ê∞¥‰∏ãËá™‰∏ªÂØºËà™` `Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†` `Êï∞Â≠óÂ≠™Áîü` `ËøëÁ´ØÁ≠ñÁï•‰ºòÂåñ` `Ê∞¥‰∏ãÊú∫Âô®‰∫∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê∞¥‰∏ãËá™‰∏ªÂØºËà™Èù¢‰∏¥GPSÁº∫Â§±„ÄÅ‰ΩéËÉΩËßÅÂ∫¶Á≠âÊåëÊàòÔºå‰º†ÁªüÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂ∫îÂØπÂ§çÊùÇÁéØÂ¢É„ÄÇ
2. Âà©Áî®PPOÁÆóÊ≥ïÔºåÁªìÂêàËôöÊãüÁéØÂ¢É‰ø°ÊÅØÂíåÂ∞ÑÁ∫øÊäïÂ∞ÑÔºåÊûÑÂª∫Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÔºåÊèêÂçáÂØºËà™ÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰ªøÁúüÂíåÁúüÂÆûÊ∞¥‰∏ãÁéØÂ¢É‰∏≠Âùá‰ºò‰∫éDWAÔºåÂπ∂ÂÖ∑Â§áËâØÂ•ΩÁöÑËøÅÁßªËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÈíàÂØπÊ∞¥‰∏ãÁéØÂ¢ÉËá™‰∏ªÂØºËà™ÈöæÈ¢òÔºåÂ¶ÇGPSÁº∫Â§±„ÄÅ‰ΩéËÉΩËßÅÂ∫¶ÂíåÊ∞¥‰∏ãÈöúÁ¢çÁâ©ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËøëÁ´ØÁ≠ñÁï•‰ºòÂåñÔºàPPOÔºâÁÆóÊ≥ïÁöÑÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ÁõÆÊ†áÂØºÂêëÂØºËà™‰ø°ÊÅØ„ÄÅËôöÊãüÂç†ÊçÆÊ†ÖÊ†ºÂíåÊ≤øÊìç‰ΩúÂå∫ÂüüËæπÁïåÁöÑÂ∞ÑÁ∫øÊäïÂ∞ÑÊûÑÂª∫ËßÇÊµãÁ©∫Èó¥„ÄÇÈÄöËøáÂú®ÈÄºÁúüÁöÑ‰ªøÁúüÁéØÂ¢É‰∏≠ËØÑ‰º∞ÔºåÂπ∂Â∞ÜÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•‰∏éÂ∏∏Áî®ÁöÑÂä®ÊÄÅÁ™óÂè£Ê≥ïÔºàDWAÔºâËøõË°åÊØîËæÉÔºåÁªìÊûúË°®ÊòéÔºåPPOÁ≠ñÁï•Âú®È´òÂ∫¶ÊùÇ‰π±ÁöÑÁéØÂ¢É‰∏≠ÂßãÁªà‰ºò‰∫éDWAÔºåËøô‰∏ªË¶ÅÂΩíÂäü‰∫éÂÖ∂Êõ¥Â•ΩÁöÑÂ±ÄÈÉ®ÈÄÇÂ∫îÊÄßÂíåÊõ¥Â∞ëÁöÑÁ¢∞Êíû„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊï∞Â≠óÂ≠™ÁîüÁõëÁù£‰∏ãÁöÑÁúüÂÆûBlueROV2ÂÆûÈ™åÔºåÈ™åËØÅ‰∫ÜÂ≠¶‰π†Âà∞ÁöÑË°å‰∏∫‰ªé‰ªøÁúüÂà∞Áé∞ÂÆû‰∏ñÁïåÁöÑËøÅÁßªËÉΩÂäõÔºåËØÅÂÆû‰∫ÜÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Âú®Ê∞¥‰∏ãÊú∫Âô®‰∫∫Ëá™‰∏ªÂØºËà™‰∏≠ÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊ∞¥‰∏ãËá™‰∏ªÂØºËà™ÁöÑ‰∏ªË¶ÅÈóÆÈ¢òÂú®‰∫éÁº∫‰πèÂèØÈù†ÁöÑÂÆö‰Ωç‰ø°ÊÅØÔºàGPS‰∏çÂèØÁî®ÔºâÔºåÊ∞¥‰∏ãÁéØÂ¢ÉÁöÑ‰ΩéËÉΩËßÅÂ∫¶Ôºå‰ª•ÂèäÂ§çÊùÇÁéØÂ¢É‰∏≠Â≠òÂú®ÁöÑÊ∞¥‰∏ãÈöúÁ¢çÁâ©„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇDWAÔºåÂú®È´òÂ∫¶Âä®ÊÄÅÂíåÂ§çÊùÇÁöÑÁéØÂ¢É‰∏≠ÂÆπÊòìÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºòÔºåÂØºËá¥ÂØºËà™ÊïàÁéáÈôç‰ΩéÊàñÁ¢∞ÊíûÈ£éÈô©Â¢ûÂä†„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºàDRLÔºâÊù•Â≠¶‰π†‰∏Ä‰∏™ËÉΩÂ§üÈÄÇÂ∫îÂ§çÊùÇÊ∞¥‰∏ãÁéØÂ¢ÉÁöÑÂØºËà™Á≠ñÁï•„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºåÊô∫ËÉΩ‰ΩìÂèØ‰ª•‰ªé‰∏éÁéØÂ¢ÉÁöÑ‰∫§‰∫í‰∏≠Â≠¶‰π†Ôºå‰ªéËÄåÊâæÂà∞ÊúÄ‰ºòÁöÑÂØºËà™Ë∑ØÂæÑÔºåÈÅøÂÖçÈöúÁ¢çÁâ©ÔºåÂπ∂ÊúÄÁªàËææÂà∞ÁõÆÊ†á„ÄÇÊï∞Â≠óÂ≠™ÁîüÁöÑÂºïÂÖ•ÔºåÈôç‰Ωé‰∫ÜÁúüÂÆûÁéØÂ¢ÉÂÆûÈ™åÁöÑÈ£éÈô©„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ‰ªøÁúüÁéØÂ¢ÉÔºöÁî®‰∫éËÆ≠ÁªÉÂº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰ΩìÔºåÊèê‰æõÈÄºÁúüÁöÑÊ∞¥‰∏ãÁéØÂ¢ÉÊ®°Êãü„ÄÇ2) Âº∫ÂåñÂ≠¶‰π†Ê®°ÂùóÔºö‰ΩøÁî®PPOÁÆóÊ≥ïËÆ≠ÁªÉÂØºËà™Á≠ñÁï•„ÄÇ3) ËßÇÊµãÁ©∫Èó¥ÊûÑÂª∫ÔºöÁªìÂêàÁõÆÊ†áÂØºÂêëÂØºËà™‰ø°ÊÅØ„ÄÅËôöÊãüÂç†ÊçÆÊ†ÖÊ†ºÂíåÂ∞ÑÁ∫øÊäïÂ∞ÑÔºå‰∏∫Êô∫ËÉΩ‰ΩìÊèê‰æõ‰∏∞ÂØåÁöÑÁéØÂ¢É‰ø°ÊÅØ„ÄÇ4) Êï∞Â≠óÂ≠™ÁîüÁõëÁù£ÔºöÂà©Áî®Êï∞Â≠óÂ≠™ÁîüÊäÄÊúØÔºåÂú®ËôöÊãüÁéØÂ¢É‰∏≠È™åËØÅÂíå‰ºòÂåñÁ≠ñÁï•ÔºåÂáèÂ∞ëÁúüÂÆûÁéØÂ¢ÉÂÆûÈ™åÁöÑÈ£éÈô©„ÄÇ5) ÁúüÂÆûÊ∞¥‰∏ãÊú∫Âô®‰∫∫ÂÆûÈ™åÔºöÂ∞ÜËÆ≠ÁªÉÂ•ΩÁöÑÁ≠ñÁï•ÈÉ®ÁΩ≤Âà∞ÁúüÂÆûÁöÑBlueROV2Ê∞¥‰∏ãÊú∫Âô®‰∫∫‰∏äËøõË°åÈ™åËØÅ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†‰∏éÊï∞Â≠óÂ≠™ÁîüÊäÄÊúØÁõ∏ÁªìÂêàÔºåÁî®‰∫éËß£ÂÜ≥Ê∞¥‰∏ãËá™‰∏ªÂØºËà™ÈóÆÈ¢ò„ÄÇ‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ËøõË°åÂ§ßÈáèÁöÑÂÆûÈ™åÔºåËøôÂØπ‰∫éÊ∞¥‰∏ãÊú∫Âô®‰∫∫Êù•ËØ¥ÊòØÂç±Èô©‰∏îÊòÇË¥µÁöÑ„ÄÇÈÄöËøáÊï∞Â≠óÂ≠™ÁîüÊäÄÊúØÔºåÂèØ‰ª•Âú®ËôöÊãüÁéØÂ¢É‰∏≠ËøõË°åÁ≠ñÁï•ÁöÑËÆ≠ÁªÉÂíåÈ™åËØÅÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜÁúüÂÆûÁéØÂ¢ÉÂÆûÈ™åÁöÑÈ£éÈô©ÂíåÊàêÊú¨„ÄÇÊ≠§Â§ñÔºåÁªìÂêàÂ§öÁßçÁéØÂ¢É‰ø°ÊÅØÊûÑÂª∫ËßÇÊµãÁ©∫Èó¥ÔºåÊèêÂçá‰∫ÜÊô∫ËÉΩ‰ΩìÂØπÁéØÂ¢ÉÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠‰ΩøÁî®‰∫ÜPPOÁÆóÊ≥ï‰Ωú‰∏∫Âº∫ÂåñÂ≠¶‰π†ÁöÑÊ†∏ÂøÉÁÆóÊ≥ï„ÄÇËßÇÊµãÁ©∫Èó¥Áî±‰∏âÈÉ®ÂàÜÁªÑÊàêÔºöÁõÆÊ†áÂØºÂêëÂØºËà™‰ø°ÊÅØÔºàÁõÆÊ†áÊñπÂêëÂíåË∑ùÁ¶ªÔºâ„ÄÅËôöÊãüÂç†ÊçÆÊ†ÖÊ†ºÔºàÂë®Âõ¥ÁéØÂ¢ÉÁöÑÂ±ÄÈÉ®Âú∞ÂõæÔºâÂíåÂ∞ÑÁ∫øÊäïÂ∞ÑÔºàÊ≤øÊìç‰ΩúÂå∫ÂüüËæπÁïåÁöÑË∑ùÁ¶ª‰ø°ÊÅØÔºâ„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°Êó®Âú®ÈºìÂä±Êô∫ËÉΩ‰ΩìÊúùÁùÄÁõÆÊ†áÂâçËøõÔºåÂêåÊó∂ÈÅøÂÖçÁ¢∞Êíû„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ËÆ∫Êñá‰∏≠Ê≤°ÊúâËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éPPOÁöÑÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Âú®È´òÂ∫¶ÊùÇ‰π±ÁöÑÊ∞¥‰∏ãÁéØÂ¢É‰∏≠ÔºåÂØºËà™ÊÄßËÉΩÊòéÊòæ‰ºò‰∫é‰º†ÁªüÁöÑDWAÁÆóÊ≥ï„ÄÇPPOÁ≠ñÁï•ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ±ÄÈÉ®ÁéØÂ¢ÉÂèòÂåñÔºåÂáèÂ∞ëÁ¢∞ÊíûÊ¨°Êï∞ÔºåÂπ∂ÊàêÂäüÂú∞Â∞ÜÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•‰ªé‰ªøÁúüÁéØÂ¢ÉËøÅÁßªÂà∞ÁúüÂÆûÁöÑBlueROV2Ê∞¥‰∏ãÊú∫Âô®‰∫∫‰∏ä„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÊú™Áü•Ôºå‰ΩÜÊï¥‰ΩìË°®Áé∞‰ºò‰∫éDWA„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊ∞¥‰∏ãÁéØÂ¢ÉÁõëÊµã„ÄÅÊ∞¥‰∏ãËµÑÊ∫êÂãòÊé¢„ÄÅÊ∞¥‰∏ãÂü∫Á°ÄËÆæÊñΩÁª¥Êä§„ÄÅÊ∞¥‰∏ãÊêúÊïëÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáËá™‰∏ªÂØºËà™ÔºåÊ∞¥‰∏ãÊú∫Âô®‰∫∫ÂèØ‰ª•Êõ¥È´òÊïà„ÄÅÊõ¥ÂÆâÂÖ®Âú∞ÂÆåÊàêÂêÑÁßçÊ∞¥‰∏ã‰ªªÂä°ÔºåÈôç‰Ωé‰∫∫Â∑•Êìç‰ΩúÁöÑÈ£éÈô©ÂíåÊàêÊú¨ÔºåÊèêÈ´ò‰Ωú‰∏öÊïàÁéá„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõËøõ‰∏ÄÊ≠•Êé®ÂπøÂà∞ÂÖ∂‰ªñÁ±ªÂûãÁöÑÊú∫Âô®‰∫∫ÂíåÂ§çÊùÇÁéØÂ¢É„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

