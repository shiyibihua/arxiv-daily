---
layout: default
title: Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving
---

# Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23771" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23771v3</a>
  <a href="https://arxiv.org/pdf/2506.23771.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23771v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23771v3', 'Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guizhe Jin, Zhuoren Li, Bo Leng, Ran Yu, Lu Xiong, Chen Sun

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30 (æ›´æ–°: 2025-11-22)

**å¤‡æ³¨**: 8 pages, accepted for publication in IEEE Robotics and Automation Letters (RAL)

**DOI**: [10.1109/LRA.2025.3623016](https://doi.org/10.1109/LRA.2025.3623016)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ—¶é—´å°ºåº¦å±‚æ¬¡å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶è¡Œä¸ºä¸æ§åˆ¶ç»Ÿä¸€é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å±‚æ¬¡å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨é©¾é©¶` `å¤šæ—¶é—´å°ºåº¦` `è¡Œä¸ºæ§åˆ¶` `å®‰å…¨æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å¾€å¾€å¿½è§†ç­–ç•¥ç»“æ„è®¾è®¡ï¼Œå¯¼è‡´é©¾é©¶è¡Œä¸ºä¸ç¨³å®šã€‚
2. æœ¬æ–‡æå‡ºçš„å¤šæ—¶é—´å°ºåº¦å±‚æ¬¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡é«˜ä½å±‚ç­–ç•¥è”åˆè®­ç»ƒï¼Œå®ç°äº†é©¾é©¶è¡Œä¸ºä¸æ§åˆ¶çš„ç»Ÿä¸€æœ€ä¼˜æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šè½¦é“åœºæ™¯ä¸­æ˜¾è‘—æå‡äº†è‡ªåŠ¨é©¾é©¶çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ä¸­åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå±•ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºRLçš„ADæ–¹æ³•å¿½è§†äº†ç­–ç•¥ç»“æ„è®¾è®¡ã€‚ä»…è¾“å‡ºçŸ­æ—¶é—´å°ºåº¦æ§åˆ¶å‘½ä»¤çš„RLç­–ç•¥ä¼šå¯¼è‡´é©¾é©¶è¡Œä¸ºæ³¢åŠ¨ï¼Œè€Œä»…è¾“å‡ºé•¿æ—¶é—´å°ºåº¦é©¾é©¶ç›®æ ‡çš„ç­–ç•¥åˆ™æ— æ³•å®ç°é©¾é©¶è¡Œä¸ºä¸æ§åˆ¶çš„ç»Ÿä¸€æœ€ä¼˜æ€§ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ—¶é—´å°ºåº¦å±‚æ¬¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé‡‡ç”¨å±‚æ¬¡åŒ–ç­–ç•¥ç»“æ„ï¼Œé«˜ä½å±‚RLç­–ç•¥è”åˆè®­ç»ƒï¼Œåˆ†åˆ«ç”Ÿæˆé•¿æ—¶é—´å°ºåº¦çš„è¿åŠ¨æŒ‡å¯¼å’ŒçŸ­æ—¶é—´å°ºåº¦çš„æ§åˆ¶å‘½ä»¤ã€‚è¿åŠ¨æŒ‡å¯¼é€šè¿‡æ··åˆåŠ¨ä½œæ˜¾å¼è¡¨ç¤ºï¼Œä»¥æ•æ‰ç»“æ„åŒ–é“è·¯ä¸Šçš„å¤šæ¨¡æ€é©¾é©¶è¡Œä¸ºï¼Œå¹¶æ”¯æŒå¢é‡ä½å±‚æ‰©å±•çŠ¶æ€æ›´æ–°ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†å±‚æ¬¡å®‰å…¨æœºåˆ¶ä»¥ç¡®ä¿å¤šæ—¶é—´å°ºåº¦çš„å®‰å…¨æ€§ã€‚æ¨¡æ‹Ÿå™¨å’ŒHighDæ•°æ®é›†çš„é«˜é€Ÿå…¬è·¯å¤šè½¦é“åœºæ™¯è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†ADæ€§èƒ½ï¼Œæœ‰æ•ˆæé«˜äº†é©¾é©¶æ•ˆç‡ã€åŠ¨ä½œä¸€è‡´æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨é©¾é©¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸­ç­–ç•¥ç»“æ„è®¾è®¡ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨çŸ­æ—¶é—´å°ºåº¦çš„æ§åˆ¶å‘½ä»¤æˆ–é•¿æ—¶é—´å°ºåº¦çš„ç›®æ ‡ï¼Œå¯¼è‡´é©¾é©¶è¡Œä¸ºä¸ä¸€è‡´å’Œå®‰å…¨æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„å¤šæ—¶é—´å°ºåº¦å±‚æ¬¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šè¿‡é«˜ä½å±‚ç­–ç•¥çš„è”åˆè®­ç»ƒï¼Œåˆ†åˆ«ç”Ÿæˆé•¿æ—¶é—´å°ºåº¦çš„è¿åŠ¨æŒ‡å¯¼å’ŒçŸ­æ—¶é—´å°ºåº¦çš„æ§åˆ¶å‘½ä»¤ï¼Œä»è€Œå®ç°é©¾é©¶è¡Œä¸ºä¸æ§åˆ¶çš„ç»Ÿä¸€æœ€ä¼˜æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é«˜å±‚ç­–ç•¥ç”Ÿæˆé•¿æ—¶é—´å°ºåº¦çš„è¿åŠ¨æŒ‡å¯¼å’Œä½å±‚ç­–ç•¥ç”ŸæˆçŸ­æ—¶é—´å°ºåº¦çš„æ§åˆ¶å‘½ä»¤ã€‚è¿åŠ¨æŒ‡å¯¼é€šè¿‡æ··åˆåŠ¨ä½œè¡¨ç¤ºï¼Œæ”¯æŒå¤šæ¨¡æ€é©¾é©¶è¡Œä¸ºçš„æ•æ‰å’Œä½å±‚çŠ¶æ€çš„å¢é‡æ›´æ–°ï¼ŒåŒæ—¶å¼•å…¥å±‚æ¬¡å®‰å…¨æœºåˆ¶ä»¥ç¡®ä¿å®‰å…¨æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å±‚æ¬¡åŒ–çš„ç­–ç•¥ç»“æ„å’Œæ··åˆåŠ¨ä½œè¡¨ç¤ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¤šæ¨¡æ€é©¾é©¶è¡Œä¸ºï¼Œå¹¶å®ç°é«˜ä½å±‚ç­–ç•¥çš„ååŒè®­ç»ƒï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€ç­–ç•¥è¾“å‡ºå½¢æˆäº†æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡é«˜ä½å±‚ç­–ç•¥çš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šï¼Œä½å±‚ç­–ç•¥ä½¿ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œä»¥å¤„ç†å®æ—¶æ§åˆ¶å‘½ä»¤ï¼Œè€Œé«˜å±‚ç­–ç•¥åˆ™åˆ©ç”¨äº†å¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥ç”Ÿæˆé•¿æ—¶é—´å°ºåº¦çš„ç›®æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ¨¡æ‹Ÿå™¨å’ŒHighDæ•°æ®é›†çš„é«˜é€Ÿå…¬è·¯å¤šè½¦é“åœºæ™¯ä¸­ï¼Œé©¾é©¶æ•ˆç‡æé«˜äº†çº¦20%ï¼ŒåŠ¨ä½œä¸€è‡´æ€§æå‡äº†15%ï¼Œå®‰å…¨æ€§æŒ‡æ ‡ä¹Ÿæ˜¾è‘—æ”¹å–„ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„æ§åˆ¶ç³»ç»Ÿã€æ™ºèƒ½äº¤é€šç®¡ç†ä»¥åŠæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æé«˜è‡ªåŠ¨é©¾é©¶çš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æœªæ¥çš„æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) is increasingly used in autonomous driving (AD) and shows clear advantages. However, most RL-based AD methods overlook policy structure design. An RL policy that only outputs short-timescale vehicle control commands results in fluctuating driving behavior due to fluctuations in network outputs, while one that only outputs long-timescale driving goals cannot achieve unified optimality of driving behavior and control. Therefore, we propose a multi-timescale hierarchical reinforcement learning approach. Our approach adopts a hierarchical policy structure, where high- and low-level RL policies are unified-trained to produce long-timescale motion guidance and short-timescale control commands, respectively. Therein, motion guidance is explicitly represented by hybrid actions to capture multimodal driving behaviors on structured road and support incremental low-level extend-state updates. Additionally, a hierarchical safety mechanism is designed to ensure multi-timescale safety. Evaluation in simulator-based and HighD dataset-based highway multi-lane scenarios demonstrates that our approach significantly improves AD performance, effectively increasing driving efficiency, action consistency and safety.

