---
layout: default
title: Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems
---

# Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00268" class="toolbar-btn" target="_blank">üìÑ arXiv: 2507.00268v1</a>
  <a href="https://arxiv.org/pdf/2507.00268.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00268v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00268v1', 'Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Oren Fivel, Matan Rudman, Kobi Cohen

**ÂàÜÁ±ª**: cs.RO, cs.AI, eess.SY

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-30

**Â§áÊ≥®**: 27 pages, 10 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÊéßÂà∂‰ºòÂåñÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†‰ª•Ëß£ÂÜ≥ÊâßË°å‰∏çÂåπÈÖçÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†` `ÊéßÂà∂‰ºòÂåñ` `ÊâßË°å‰∏çÂåπÈÖç` `Êô∫ËÉΩ‰ª£ÁêÜ` `Êú∫Âô®‰∫∫ÊäÄÊúØ` `Êú∫Áîµ‰∏Ä‰ΩìÂåñ` `Á≥ªÁªüÈ≤ÅÊ£íÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂÅáËÆæÂä®‰ΩúÊâßË°åÂÆåÁæéÔºåÊú™ËÄÉËôëÊâßË°å‰∏çÂåπÈÖçÂ∏¶Êù•ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåÂΩ±Âìç‰∫ÜÂÆûÈôÖÂ∫îÁî®ÁöÑÊÄßËÉΩ„ÄÇ
2. Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊéßÂà∂‰ºòÂåñÁöÑDRLÊ°ÜÊû∂ÔºåÈÄöËøá‰∏§Èò∂ÊÆµËøáÁ®ãÂª∫Ê®°Âπ∂Ë°•ÂÅøÂä®‰ΩúÊâßË°å‰∏çÂåπÈÖçÔºåÂ¢ûÂº∫‰∫ÜÁ≥ªÁªüÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ
3. Âú®‰∫î‰∏™ÂºÄÊ∫êÊú∫Ê¢∞‰ªøÁúüÁéØÂ¢É‰∏≠ËØÑ‰º∞ËØ•Ê°ÜÊû∂ÔºåÁªìÊûúÊòæÁ§∫ÂÖ∂Âú®Èù¢ÂØπ‰∏çÁ°ÆÂÆöÊÄßÊó∂Ë°®Áé∞Âá∫Ëâ≤ÔºåÊèê‰æõ‰∫ÜÈ´òÊïàÁöÑÊéßÂà∂Ëß£ÂÜ≥ÊñπÊ°à„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºàDRLÔºâÂ∑≤Êàê‰∏∫Êú∫Âô®Â≠¶‰π†Âíå‰∫∫Â∑•Êô∫ËÉΩ‰∏≠Â§çÊùÇÂÜ≥Á≠ñÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºå‰º†ÁªüÊñπÊ≥ïÈÄöÂ∏∏ÂÅáËÆæÂÆåÁæéÁöÑÂä®‰ΩúÊâßË°åÔºåÂøΩËßÜ‰∫Ü‰ª£ÁêÜÈÄâÊã©ÁöÑÂä®‰Ωú‰∏éÂÆûÈôÖÁ≥ªÁªüÂìçÂ∫î‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÂú®Êú∫Âô®‰∫∫„ÄÅÊú∫Áîµ‰∏Ä‰ΩìÂåñÂíåÈÄö‰ø°ÁΩëÁªúÁ≠âÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÁî±‰∫éÁ≥ªÁªüÂä®ÊÄÅ„ÄÅÁ°¨‰ª∂ÈôêÂà∂ÂíåÂª∂ËøüÁ≠âÂõ†Á¥†ÂØºËá¥ÁöÑÊâßË°å‰∏çÂåπÈÖç‰ºöÊòæËëóÈôç‰ΩéÊÄßËÉΩ„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊéßÂà∂‰ºòÂåñDRLÊ°ÜÊû∂ÔºåÊòéÁ°ÆÂª∫Ê®°Âπ∂Ë°•ÂÅøÂä®‰ΩúÊâßË°å‰∏çÂåπÈÖçÔºåÂª∫Á´ã‰∫Ü‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑ‰∏§Èò∂ÊÆµËøáÁ®ãÔºöÁ°ÆÂÆöÊúüÊúõÂä®‰ΩúÂíåÈÄâÊã©ÈÄÇÂΩìÁöÑÊéßÂà∂‰ø°Âè∑‰ª•Á°Æ‰øùÊ≠£Á°ÆÊâßË°å„ÄÇÈÄöËøáÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ËÄÉËôëËøô‰∫õÂõ†Á¥†ÔºåAI‰ª£ÁêÜËÉΩÂ§ü‰ºòÂåñÊúüÊúõÂä®‰ΩúÔºå‰ªéËÄåÊèêÈ´òÂú®Áé∞ÂÆû‰∏ñÁïå‰∏çÁ°ÆÂÆöÊÄß‰∏ãÁöÑÂÜ≥Á≠ñÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨Á†îÁ©∂Êó®Âú®Ëß£ÂÜ≥Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†‰∏≠Âä®‰ΩúÊâßË°å‰∏çÂåπÈÖçÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÂ§ÑÁêÜÂõ†Á≥ªÁªüÂä®ÊÄÅÂíåÁ°¨‰ª∂ÈôêÂà∂ÂØºËá¥ÁöÑÊâßË°åËØØÂ∑Æ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÁöÑÊ°ÜÊû∂ÈÄöËøáÊòéÁ°ÆÂª∫Ê®°ÊâßË°å‰∏çÂåπÈÖçÔºåÈááÁî®‰∏§Èò∂ÊÆµËøáÁ®ãÊù•‰ºòÂåñÂä®‰ΩúÈÄâÊã©ÂíåÊéßÂà∂‰ø°Âè∑Ôºå‰ªéËÄåÊèêÈ´òÂÜ≥Á≠ñÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµÁ°ÆÂÆöÊúüÊúõÂä®‰ΩúÔºåÁ¨¨‰∫åÈò∂ÊÆµÈÄâÊã©ÈÄÇÂΩìÁöÑÊéßÂà∂‰ø°Âè∑‰ª•Á°Æ‰øùÂä®‰ΩúÁöÑÊ≠£Á°ÆÊâßË°å„ÄÇËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂêåÊó∂ËÄÉËôëÂä®‰Ωú‰∏çÂåπÈÖçÂíåÊéßÂà∂Âô®‰øÆÊ≠£„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•Á†îÁ©∂ÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜÊâßË°å‰∏çÂåπÈÖçÁöÑÂª∫Ê®°‰∏éË°•ÂÅøÊú∫Âà∂Á∫≥ÂÖ•DRLËÆ≠ÁªÉËøáÁ®ãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊô∫ËÉΩ‰ª£ÁêÜÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÈíàÂØπÂä®‰Ωú‰∏çÂåπÈÖçÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÊéßÂà∂‰ø°Âè∑‰ºòÂåñÁ≠ñÁï•ÔºåÁ°Æ‰øù‰ª£ÁêÜÂú®ËÆ≠ÁªÉÊó∂ËÉΩÂ§üÊúâÊïàÂ∫îÂØπÊâßË°åËØØÂ∑Æ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ÂÆûÈ™å‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÈ™åËØÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®‰∫î‰∏™ÂºÄÊ∫êÊú∫Ê¢∞‰ªøÁúüÁéØÂ¢É‰∏≠ËøõË°åÁöÑÂÆûÈ™åË°®ÊòéÔºåÊèêÂá∫ÁöÑÊéßÂà∂‰ºòÂåñDRLÊ°ÜÊû∂Âú®Èù¢ÂØπÊâßË°å‰∏çÂåπÈÖçÊó∂ÔºåÂÜ≥Á≠ñÊúâÊïàÊÄßÊòæËëóÊèêÈ´òÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåÁõ∏ËæÉ‰∫é‰º†ÁªüÊñπÊ≥ïË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊ°ÜÊû∂ÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫„ÄÅÊú∫Áîµ‰∏Ä‰ΩìÂåñÂíåÈÄö‰ø°ÁΩëÁªúÁ≠âÈ¢ÜÂüüÔºåËÉΩÂ§üÊúâÊïàÊèêÂçáÊô∫ËÉΩÁ≥ªÁªüÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂÜ≥Á≠ñËÉΩÂäõÂíåÊâßË°åÁ≤æÂ∫¶„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊé®Âä®Êô∫ËÉΩ‰ª£ÁêÜÂú®Âä®ÊÄÅÂíå‰∏çÁ°ÆÂÆöÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®ÔºåÊèêÂçáÂ∑•Á®ãÂÆûË∑µÁöÑÊïàÁéáÂíåÂèØÈù†ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Deep reinforcement learning (DRL) has become a powerful tool for complex decision-making in machine learning and AI. However, traditional methods often assume perfect action execution, overlooking the uncertainties and deviations between an agent's selected actions and the actual system response. In real-world applications, such as robotics, mechatronics, and communication networks, execution mismatches arising from system dynamics, hardware constraints, and latency can significantly degrade performance. This work advances AI by developing a novel control-optimized DRL framework that explicitly models and compensates for action execution mismatches, a challenge largely overlooked in existing methods. Our approach establishes a structured two-stage process: determining the desired action and selecting the appropriate control signal to ensure proper execution. It trains the agent while accounting for action mismatches and controller corrections. By incorporating these factors into the training process, the AI agent optimizes the desired action with respect to both the actual control signal and the intended outcome, explicitly considering execution errors. This approach enhances robustness, ensuring that decision-making remains effective under real-world uncertainties. Our approach offers a substantial advancement for engineering practice by bridging the gap between idealized learning and real-world implementation. It equips intelligent agents operating in engineering environments with the ability to anticipate and adjust for actuation errors and system disturbances during training. We evaluate the framework in five widely used open-source mechanical simulation environments we restructured and developed to reflect real-world operating conditions, showcasing its robustness against uncertainties and offering a highly practical and efficient solution for control-oriented applications.

