---
layout: default
title: VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model
---

# VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09372" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.09372v2</a>
  <a href="https://arxiv.org/pdf/2509.09372.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09372v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09372v2', 'VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, Siteng Huang, Yifan Tang, Wenhui Wang, Ru Zhang, Jianyi Liu, Donglin Wang

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-11 (Êõ¥Êñ∞: 2025-09-22)

**Â§áÊ≥®**: 28 pages; Project page: https://vla-adapter.github.io/; Github: https://github.com/OpenHelix-Team/VLA-Adapter; HuggingFace: https://huggingface.co/VLA-Adapter

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://vla-adapter.github.io/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VLA-AdapterÔºö‰∏ÄÁßçÈ´òÊïàÁöÑÂæÆÂûãËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãËåÉÂºè**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Bridge Attention` `ËΩªÈáèÁ∫ßÊ®°Âûã` `ËøÅÁßªÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°Âûã‰æùËµñÂ§ßËßÑÊ®°VLMÈ¢ÑËÆ≠ÁªÉÔºåÊàêÊú¨È´òÊòÇÔºåÂ¶Ç‰ΩïÈôç‰ΩéÂØπÂ§ßÂûãVLMÁöÑ‰æùËµñÊòØÊ†∏ÂøÉÈóÆÈ¢ò„ÄÇ
2. VLA-AdapterÈÄöËøáÂàÜÊûêVLÊù°‰ª∂ÁöÑÈáçË¶ÅÊÄßÔºåËÆæËÆ°Â∏¶Bridge AttentionÁöÑËΩªÈáèÁ∫ßÁ≠ñÁï•Ê®°ÂùóÔºåËá™‰∏ªÊ≥®ÂÖ•ÊúÄ‰ºòÊù°‰ª∂„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVLA-Adapter‰ªÖÁî®0.5BÂèÇÊï∞È™®Âπ≤ÁΩëÁªúÂç≥ÂèØËææÂà∞SOTAÊÄßËÉΩÔºå‰∏îÊé®ÁêÜÈÄüÂ∫¶Âø´ÔºåÂçïGPU 8Â∞èÊó∂Âç≥ÂèØËÆ≠ÁªÉÂÆåÊàê„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÈÄöÂ∏∏ÈÄöËøáÂú®Êú∫Âô®‰∫∫Êï∞ÊçÆ‰∏äÈ¢ÑËÆ≠ÁªÉÂ§ßËßÑÊ®°ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)Êù•Âº•ÂêàÊÑüÁü•ÂíåÂä®‰ΩúÁ©∫Èó¥‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇËôΩÁÑ∂ËøôÁßçÊñπÊ≥ïÊûÅÂ§ßÂú∞ÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºå‰ΩÜ‰πüÂ∏¶Êù•‰∫ÜÂ∑®Â§ßÁöÑËÆ≠ÁªÉÊàêÊú¨„ÄÇÊú¨ÊñáÁ†îÁ©∂‰∫ÜÂ¶Ç‰ΩïÊúâÊïàÂú∞Â∞ÜËßÜËßâ-ËØ≠Ë®Ä(VL)Ë°®Á§∫Ê°•Êé•Âà∞Âä®‰Ωú(A)„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜVLA-AdapterÔºåËøôÊòØ‰∏ÄÁßçÊó®Âú®ÂáèÂ∞ëVLAÊ®°ÂûãÂØπÂ§ßÂûãVLMÂíåÂπøÊ≥õÈ¢ÑËÆ≠ÁªÉÁöÑ‰æùËµñÁöÑÊñ∞ËåÉÂºè„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨È¶ñÂÖàÁ≥ªÁªüÂú∞ÂàÜÊûê‰∫ÜÂêÑÁßçVLÊù°‰ª∂ÁöÑÊúâÊïàÊÄßÔºåÂπ∂ÊèêÂá∫‰∫ÜÂÖ≥‰∫éÂì™‰∫õÊù°‰ª∂ÂØπ‰∫éÊ°•Êé•ÊÑüÁü•ÂíåÂä®‰ΩúÁ©∫Èó¥Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑÂÖ≥ÈîÆÂèëÁé∞„ÄÇÂü∫‰∫éËøô‰∫õËßÅËß£ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â∏¶ÊúâBridge AttentionÁöÑËΩªÈáèÁ∫ßÁ≠ñÁï•Ê®°ÂùóÔºåËØ•Ê®°ÂùóËá™‰∏ªÂú∞Â∞ÜÊúÄ‰Ω≥Êù°‰ª∂Ê≥®ÂÖ•Âà∞Âä®‰ΩúÁ©∫Èó¥‰∏≠„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ªÖ‰ΩøÁî®0.5BÂèÇÊï∞ÁöÑÈ™®Âπ≤ÁΩëÁªúÂç≥ÂèØÂÆûÁé∞È´òÊÄßËÉΩÔºåËÄåÊó†ÈúÄ‰ªª‰ΩïÊú∫Âô®‰∫∫Êï∞ÊçÆÈ¢ÑËÆ≠ÁªÉ„ÄÇÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫Âü∫ÂáÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåVLA-Adapter‰∏ç‰ªÖÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÊ∞¥Âπ≥ÔºåËÄå‰∏îËøòÊèê‰æõ‰∫ÜËøÑ‰ªä‰∏∫Ê≠¢ÊúÄÂø´ÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇÊ≠§Â§ñÔºåÁî±‰∫éÊâÄÊèêÂá∫ÁöÑÂÖàËøõÊ°•Êé•ËåÉÂºèÔºåVLA-AdapterËÉΩÂ§üÂú®Âçï‰∏™Ê∂àË¥πÁ∫ßGPU‰∏ä‰ªÖÁî®8Â∞èÊó∂ËÆ≠ÁªÉ‰∏Ä‰∏™Âº∫Â§ßÁöÑVLAÊ®°ÂûãÔºåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜÈÉ®ÁΩ≤VLAÊ®°ÂûãÁöÑÈó®Êßõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöVLAÊ®°ÂûãÊó®Âú®ËøûÊé•ËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÂä®‰ΩúÁ©∫Èó¥ÔºåÂÆûÁé∞Êú∫Âô®‰∫∫Á≠âÊô∫ËÉΩ‰ΩìÁöÑËá™‰∏ªÂÜ≥Á≠ñ„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÂ§ßËßÑÊ®°ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÈ¢ÑËÆ≠ÁªÉÔºåËøôÂ∏¶Êù•‰∫ÜÂ∑®Â§ßÁöÑËÆ°ÁÆóÂíåÊï∞ÊçÆÊàêÊú¨ÔºåÈôêÂà∂‰∫ÜVLAÊ®°ÂûãÁöÑÈÉ®ÁΩ≤ÂíåÂ∫îÁî®„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÈôç‰ΩéVLAÊ®°ÂûãÂØπÂ§ßËßÑÊ®°VLMÁöÑ‰æùËµñÔºåÂêåÊó∂‰øùÊåÅÁîöËá≥ÊèêÂçáÊÄßËÉΩÔºåÊòØÊú¨ÊñáË¶ÅËß£ÂÜ≥ÁöÑÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫éÈ¢ÑËÆ≠ÁªÉÊàêÊú¨È´òÔºåÊ®°Âûã‰ΩìÁßØÂ§ßÔºåÊé®ÁêÜÈÄüÂ∫¶ÊÖ¢„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂàÜÊûê‰∏çÂêåËßÜËßâ-ËØ≠Ë®ÄÔºàVLÔºâÊù°‰ª∂ÂØπÂä®‰ΩúÁ©∫Èó¥ÁöÑÂΩ±ÂìçÔºåÊâæÂá∫ÊúÄÊúâÊïàÁöÑÊù°‰ª∂ÔºåÂπ∂ËÆæËÆ°‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÁ≠ñÁï•Ê®°ÂùóÔºåÂ∞ÜËøô‰∫õÂÖ≥ÈîÆÊù°‰ª∂Ëá™‰∏ªÂú∞Ê≥®ÂÖ•Âà∞Âä®‰ΩúÁ©∫Èó¥‰∏≠„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫ÜÂØπÊï¥‰∏™VLMËøõË°åÂæÆË∞ÉÊàñÈáçÊñ∞ËÆ≠ÁªÉÔºå‰ªéËÄåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÂíåÊï∞ÊçÆÈúÄÊ±Ç„ÄÇÊ†∏ÂøÉÂú®‰∫éÊâæÂà∞VL‰ø°ÊÅØÂà∞ActionÁöÑÊúâÊïàÊ°•Ê¢Å„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVLA-AdapterÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËßÜËßâÁºñÁ†ÅÂô®ÔºöÁî®‰∫éÊèêÂèñËßÜËßâÁâπÂæÅ„ÄÇ2) ËØ≠Ë®ÄÁºñÁ†ÅÂô®ÔºöÁî®‰∫éÊèêÂèñËØ≠Ë®ÄÊåá‰ª§ÁöÑÁâπÂæÅ„ÄÇ3) Á≠ñÁï•Ê®°ÂùóÔºöÂåÖÂê´Bridge AttentionÊú∫Âà∂ÔºåÁî®‰∫éËûçÂêàËßÜËßâÂíåËØ≠Ë®ÄÁâπÂæÅÔºåÂπ∂ÁîüÊàêÂä®‰ΩúÊåá‰ª§„ÄÇËØ•Ê®°ÂùóÊòØVLA-AdapterÁöÑÊ†∏ÂøÉÔºåË¥üË¥£Â∞ÜVLË°®Á§∫Ê°•Êé•Âà∞Âä®‰ΩúÁ©∫Èó¥„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÔºöËæìÂÖ•ËßÜËßâ‰ø°ÊÅØÂíåËØ≠Ë®ÄÊåá‰ª§ÔºåÂàÜÂà´ÈÄöËøáÁºñÁ†ÅÂô®ÊèêÂèñÁâπÂæÅÔºåÁÑ∂ÂêéÈÄöËøáÁ≠ñÁï•Ê®°ÂùóËûçÂêàÁâπÂæÅÂπ∂ÁîüÊàêÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVLA-AdapterÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éBridge AttentionÊú∫Âà∂ÂíåËΩªÈáèÁ∫ßÁ≠ñÁï•Ê®°ÂùóÁöÑËÆæËÆ°„ÄÇBridge AttentionËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞ÈÄâÊã©ÂíåËûçÂêàÊúÄÁõ∏ÂÖ≥ÁöÑËßÜËßâÂíåËØ≠Ë®ÄÁâπÂæÅÔºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞ÊåáÂØºÂä®‰ΩúÁîüÊàê„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåVLA-Adapter‰∏çÈúÄË¶ÅÂØπÊï¥‰∏™VLMËøõË°åÂæÆË∞ÉÔºåËÄåÊòØÈÄöËøá‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÁ≠ñÁï•Ê®°ÂùóÊù•ÂÆûÁé∞VLÂà∞AÁöÑÊ°•Êé•ÔºåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇÊú¨Ë¥®Âå∫Âà´Âú®‰∫é‰ªé‰æùËµñÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉËΩ¨Âèò‰∏∫È´òÊïàÁöÑÊù°‰ª∂Ê≥®ÂÖ•„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöBridge AttentionÊú∫Âà∂ÊòØÂÖ≥ÈîÆËÆæËÆ°‰πã‰∏ÄÔºåÂÆÉÈÄöËøáÂ≠¶‰π†Ê≥®ÊÑèÂäõÊùÉÈáçÊù•Á°ÆÂÆöÂì™‰∫õËßÜËßâÂíåËØ≠Ë®ÄÁâπÂæÅÂØπÂä®‰ΩúÁîüÊàêÊúÄÈáçË¶Å„ÄÇÁ≠ñÁï•Ê®°ÂùóÈááÁî®ËΩªÈáèÁ∫ßÁΩëÁªúÁªìÊûÑÔºå‰ª•ÂáèÂ∞ëËÆ°ÁÆóÈáèÂíåÂèÇÊï∞Èáè„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüËá≥ÂÖ≥ÈáçË¶ÅÔºåÈúÄË¶ÅÂπ≥Ë°°Âä®‰ΩúÁöÑÂáÜÁ°ÆÊÄßÂíåÂπ≥ÊªëÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÔºàÂ¶ÇÊ≥®ÊÑèÂäõÂ§¥ÁöÑÊï∞Èáè„ÄÅÁΩëÁªúÂ±ÇÊï∞Á≠âÔºâÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇËÆ∫Êñá‰∏≠ÊèêÂà∞‰ΩøÁî®0.5BÂèÇÊï∞ÁöÑÈ™®Âπ≤ÁΩëÁªúÔºåÂπ∂Âú®Âçï‰∏™Ê∂àË¥πÁ∫ßGPU‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VLA-AdapterÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫Âü∫ÂáÜÊµãËØï‰∏≠ÂùáÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°Âûã‰ªÖ‰ΩøÁî®0.5BÂèÇÊï∞ÁöÑÈ™®Âπ≤ÁΩëÁªúÔºåÊó†ÈúÄ‰ªª‰ΩïÊú∫Âô®‰∫∫Êï∞ÊçÆÈ¢ÑËÆ≠ÁªÉ„ÄÇÂú®Êé®ÁêÜÈÄüÂ∫¶ÊñπÈù¢ÔºåVLA-Adapter‰πüËææÂà∞‰∫ÜÁõÆÂâçÊä•ÈÅìÁöÑÊúÄÂø´Ê∞¥Âπ≥„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåËØ•Ê®°ÂûãÂèØ‰ª•Âú®Âçï‰∏™Ê∂àË¥πÁ∫ßGPU‰∏ä‰ªÖÁî®8Â∞èÊó∂ÂÆåÊàêËÆ≠ÁªÉÔºåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜVLAÊ®°ÂûãÁöÑËÆ≠ÁªÉÈó®Êßõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VLA-AdapterÂú®Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÁêÜËß£‰∫∫Á±ªÁöÑÊåá‰ª§ÔºåÂπ∂Ê†πÊçÆËßÜËßâ‰ø°ÊÅØÂÅöÂá∫Áõ∏Â∫îÁöÑÂä®‰Ωú„ÄÇÁî±‰∫éÂÖ∂ËÆ≠ÁªÉÊàêÊú¨‰Ωé„ÄÅÊé®ÁêÜÈÄüÂ∫¶Âø´ÔºåVLA-AdapterÊúâÊúõÂä†ÈÄüVLAÊ®°ÂûãÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÈÉ®ÁΩ≤ÂíåÂ∫îÁî®ÔºåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÁÅµÊ¥ªÁöÑ‰∫∫Êú∫‰∫§‰∫í„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/.

