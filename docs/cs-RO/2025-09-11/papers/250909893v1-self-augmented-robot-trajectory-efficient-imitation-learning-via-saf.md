---
layout: default
title: Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision
---

# Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09893" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09893v1</a>
  <a href="https://arxiv.org/pdf/2509.09893.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09893v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09893v1', 'Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanbit Oh, Masaki Murooka, Tomohiro Motoda, Ryoichi Nakajo, Yukiyasu Domae

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SARTï¼šé€šè¿‡å®‰å…¨è‡ªå¢å¼ºçš„æœºå™¨äººè½¨è¿¹å­¦ä¹ ï¼Œæå‡æ¨¡ä»¿å­¦ä¹ æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `æœºå™¨äººè½¨è¿¹` `æ•°æ®å¢å¼º` `å®‰å…¨æ¢ç´¢` `æœºå™¨äººæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ ä¾èµ–å¤§é‡äººå·¥æ¼”ç¤ºæˆ–éšæœºæ¢ç´¢ï¼Œæ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œå°¤å…¶åœ¨å®‰å…¨è¦æ±‚é«˜çš„ä»»åŠ¡ä¸­ã€‚
2. SARTæ¡†æ¶é€šè¿‡å•æ¬¡äººå·¥æ¼”ç¤ºå’Œåç»­çš„æœºå™¨äººè‡ªä¸»å®‰å…¨å¢å¼ºï¼Œæœ‰æ•ˆæ‰©å±•æ•°æ®é›†ï¼Œæå‡å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSARTåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æé«˜äº†ç­–ç•¥çš„æˆåŠŸç‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ æ˜¯è®­ç»ƒæœºå™¨äººæ™ºèƒ½ä½“çš„æœ‰æ•ˆèŒƒå¼ã€‚ç„¶è€Œï¼Œæ ‡å‡†æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ•°æ®ï¼Œé€šè¿‡å¤šæ¬¡æ¼”ç¤ºæˆ–éšæœºæ¢ç´¢æ¥ç¡®ä¿å¯é æ€§èƒ½ã€‚è™½ç„¶æ¢ç´¢å‡å°‘äº†äººå·¥å¹²é¢„ï¼Œä½†ç¼ºä¹å®‰å…¨ä¿è¯ï¼Œå®¹æ˜“å‘ç”Ÿç¢°æ’ï¼Œå°¤å…¶æ˜¯åœ¨é—´éš™å—é™çš„ä»»åŠ¡ä¸­ï¼ˆå¦‚æ’å­”ï¼‰ã€‚è¿™éœ€è¦æ‰‹åŠ¨é‡ç½®ç¯å¢ƒï¼Œå¢åŠ äººå·¥è´Ÿæ‹…ã€‚æœ¬ç ”ç©¶æå‡ºäº†è‡ªå¢å¼ºæœºå™¨äººè½¨è¿¹ï¼ˆSARTï¼‰æ¡†æ¶ï¼Œä»…éœ€å•æ¬¡äººå·¥æ¼”ç¤ºå³å¯è¿›è¡Œç­–ç•¥å­¦ä¹ ï¼Œå¹¶é€šè¿‡è‡ªä¸»å¢å¼ºå®‰å…¨åœ°æ‰©å±•æ•°æ®é›†ã€‚SARTåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰å•æ¬¡äººå·¥ç¤ºæ•™ï¼Œæä¾›ä¸€æ¬¡æ¼”ç¤ºï¼Œå¹¶æ ‡æ³¨å…³é”®è·¯å¾„ç‚¹å‘¨å›´çš„ç²¾åº¦è¾¹ç•Œï¼ˆè¡¨ç¤ºä¸ºçƒä½“ï¼‰ï¼Œç„¶åé‡ç½®ä¸€æ¬¡ç¯å¢ƒï¼›ï¼ˆ2ï¼‰æœºå™¨äººè‡ªå¢å¼ºï¼Œæœºå™¨äººåœ¨è¿™äº›è¾¹ç•Œå†…ç”Ÿæˆå¤šæ ·ä¸”æ— ç¢°æ’çš„è½¨è¿¹ï¼Œå¹¶é‡æ–°è¿æ¥åˆ°åŸå§‹æ¼”ç¤ºã€‚è¿™ç§è®¾è®¡é€šè¿‡æœ€å°åŒ–äººå·¥å¹²é¢„æ¥æé«˜æ•°æ®æ”¶é›†æ•ˆç‡ï¼ŒåŒæ—¶ç¡®ä¿å®‰å…¨ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ“ä½œä»»åŠ¡ä¸­çš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼ŒSARTæ¯”ä»…åœ¨äººå·¥æ”¶é›†çš„æ¼”ç¤ºä¸Šè®­ç»ƒçš„ç­–ç•¥å®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ¨¡ä»¿å­¦ä¹ ä¸­æ•°æ®æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡äººå·¥æ¼”ç¤ºæˆ–ä¸å®‰å…¨çš„éšæœºæ¢ç´¢ã€‚äººå·¥æ¼”ç¤ºæˆæœ¬é«˜ï¼Œè€Œéšæœºæ¢ç´¢å¯èƒ½å¯¼è‡´ç¢°æ’ï¼Œéœ€è¦é¢‘ç¹çš„äººå·¥å¹²é¢„é‡ç½®ç¯å¢ƒï¼Œå¢åŠ äº†å­¦ä¹ éš¾åº¦å’Œæ—¶é—´æˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å•æ¬¡äººå·¥æ¼”ç¤ºä½œä¸ºåŸºç¡€ï¼Œé€šè¿‡æœºå™¨äººè‡ªä¸»ç”Ÿæˆå®‰å…¨ä¸”å¤šæ ·åŒ–çš„è½¨è¿¹æ¥æ‰©å……æ•°æ®é›†ã€‚å…³é”®åœ¨äºé™åˆ¶æœºå™¨äººæ¢ç´¢çš„èŒƒå›´ï¼Œä½¿å…¶åœ¨äººå·¥æ¼”ç¤ºçš„å…³é”®è·¯å¾„ç‚¹é™„è¿‘è¿›è¡Œå®‰å…¨æ¢ç´¢ï¼Œé¿å…ç¢°æ’ï¼ŒåŒæ—¶ä¿è¯ç”Ÿæˆè½¨è¿¹çš„å¤šæ ·æ€§ï¼Œä»è€Œæé«˜ç­–ç•¥å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSARTæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) äººå·¥ç¤ºæ•™é˜¶æ®µï¼šäººå·¥æä¾›ä¸€æ¬¡æ¼”ç¤ºè½¨è¿¹ï¼Œå¹¶å¯¹å…³é”®è·¯å¾„ç‚¹è¿›è¡Œæ ‡æ³¨ï¼Œç¡®å®šç²¾åº¦è¾¹ç•Œï¼ˆçƒä½“ï¼‰ã€‚2) æœºå™¨äººè‡ªå¢å¼ºé˜¶æ®µï¼šæœºå™¨äººæ ¹æ®äººå·¥ç¤ºæ•™çš„ç²¾åº¦è¾¹ç•Œï¼Œç”Ÿæˆå¤šæ ·ä¸”æ— ç¢°æ’çš„è½¨è¿¹ï¼Œå¹¶å°†è¿™äº›è½¨è¿¹é‡æ–°è¿æ¥åˆ°åŸå§‹æ¼”ç¤ºè½¨è¿¹ä¸Šï¼Œä»è€Œæ‰©å……æ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šSARTçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå®‰å…¨è‡ªå¢å¼ºçš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†äººå·¥ç¤ºæ•™çš„å¼•å¯¼æ€§å’Œæœºå™¨äººè‡ªä¸»æ¢ç´¢çš„æ•ˆç‡ã€‚é€šè¿‡äººå·¥æ ‡æ³¨çš„ç²¾åº¦è¾¹ç•Œï¼Œé™åˆ¶äº†æœºå™¨äººçš„æ¢ç´¢ç©ºé—´ï¼Œä¿è¯äº†å®‰å…¨æ€§ï¼ŒåŒæ—¶å…è®¸æœºå™¨äººåœ¨è¾¹ç•Œå†…ç”Ÿæˆå¤šæ ·åŒ–çš„è½¨è¿¹ï¼Œæé«˜äº†æ•°æ®æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSARTæ˜¾è‘—å‡å°‘äº†äººå·¥å¹²é¢„çš„éœ€æ±‚ï¼Œå¹¶æé«˜äº†å­¦ä¹ çš„å®‰å…¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šç²¾åº¦è¾¹ç•Œçš„è®¾è®¡æ˜¯å…³é”®ã€‚è®ºæ–‡ä½¿ç”¨çƒä½“æ¥è¡¨ç¤ºå…³é”®è·¯å¾„ç‚¹å‘¨å›´çš„ç²¾åº¦èŒƒå›´ï¼Œçƒä½“çš„å¤§å°å¯ä»¥æ ¹æ®ä»»åŠ¡çš„ç²¾åº¦è¦æ±‚è¿›è¡Œè°ƒæ•´ã€‚æœºå™¨äººç”Ÿæˆè½¨è¿¹æ—¶ï¼Œéœ€è¦ä¿è¯è½¨è¿¹åœ¨çƒä½“å†…éƒ¨ï¼Œå¹¶ä¸”ä¸åŸå§‹æ¼”ç¤ºè½¨è¿¹å¹³æ»‘è¿æ¥ã€‚å…·ä½“çš„è½¨è¿¹ç”Ÿæˆæ–¹æ³•å¯èƒ½æ¶‰åŠè¿åŠ¨è§„åˆ’ç®—æ³•æˆ–ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥ç¡®ä¿è½¨è¿¹çš„å¹³æ»‘æ€§å’Œæ— ç¢°æ’æ€§ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½åŒ…æ‹¬è½¨è¿¹çš„å¹³æ»‘æ€§æŸå¤±ã€ä¸åŸå§‹æ¼”ç¤ºè½¨è¿¹çš„ç›¸ä¼¼æ€§æŸå¤±ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜SARTæ˜¾è‘—æé«˜äº†ç­–ç•¥çš„æˆåŠŸç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ’å­”ä»»åŠ¡ä¸­ï¼ŒSARTä»…ä½¿ç”¨å•æ¬¡äººå·¥æ¼”ç¤ºå³å¯è¾¾åˆ°è¿œé«˜äºä»…ä½¿ç”¨äººå·¥æ¼”ç¤ºè®­ç»ƒçš„ç­–ç•¥çš„æˆåŠŸç‡ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œè¯æ˜äº†SARTåœ¨æ•°æ®æ•ˆç‡å’Œå®‰å…¨æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SARTæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦é«˜ç²¾åº¦å’Œå®‰å…¨æ€§çš„æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚è£…é…ã€ç„Šæ¥ã€åŒ»ç–—æ‰‹æœ¯ç­‰ã€‚é€šè¿‡å‡å°‘äººå·¥ç¤ºæ•™çš„éœ€æ±‚ï¼Œé™ä½äº†æœºå™¨äººéƒ¨ç½²çš„æˆæœ¬å’Œéš¾åº¦ã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦å®‰å…¨æ¢ç´¢çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ— äººæœºå¯¼èˆªç­‰ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation learning is a promising paradigm for training robot agents; however, standard approaches typically require substantial data acquisition -- via numerous demonstrations or random exploration -- to ensure reliable performance. Although exploration reduces human effort, it lacks safety guarantees and often results in frequent collisions -- particularly in clearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual environmental resets and imposing additional human burden. This study proposes Self-Augmented Robot Trajectory (SART), a framework that enables policy learning from a single human demonstration, while safely expanding the dataset through autonomous augmentation. SART consists of two stages: (1) human teaching only once, where a single demonstration is provided and precision boundaries -- represented as spheres around key waypoints -- are annotated, followed by one environment reset; (2) robot self-augmentation, where the robot generates diverse, collision-free trajectories within these boundaries and reconnects to the original demonstration. This design improves the data collection efficiency by minimizing human effort while ensuring safety. Extensive evaluations in simulation and real-world manipulation tasks show that SART achieves substantially higher success rates than policies trained solely on human-collected demonstrations. Video results available at https://sites.google.com/view/sart-il .

