---
layout: default
title: OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning
---

# OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09332" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09332v2</a>
  <a href="https://arxiv.org/pdf/2509.09332.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09332v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09332v2', 'OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan

**åˆ†ç±»**: cs.RO, cs.AI, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11 (æ›´æ–°: 2025-09-12)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniEVAï¼šé€šè¿‡ä»»åŠ¡è‡ªé€‚åº”3Dæ„ŸçŸ¥å’Œå…·èº«è®¤çŸ¥å®ç°é€šç”¨å…·èº«è§„åˆ’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `3Dæ„ŸçŸ¥` `ä»»åŠ¡è§„åˆ’` `å…·èº«æ¨ç†` `æœºå™¨äºº` `å‡ ä½•é€‚åº”æ€§` `å…·èº«çº¦æŸ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºMLLMçš„å…·èº«æ™ºèƒ½ç³»ç»Ÿåœ¨å‡ ä½•é€‚åº”æ€§å’Œå…·èº«çº¦æŸæ–¹é¢å­˜åœ¨å·®è·ï¼Œéš¾ä»¥é€‚åº”å¤šæ ·åŒ–çš„ç©ºé—´éœ€æ±‚å’Œç‰©ç†é™åˆ¶ã€‚
2. OmniEVAé€šè¿‡ä»»åŠ¡è‡ªé€‚åº”3Dæ„ŸçŸ¥å’Œå…·èº«æ„ŸçŸ¥æ¨ç†æ¡†æ¶ï¼Œå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„3Dä¿¡æ¯èåˆå’Œè€ƒè™‘æœºå™¨äººç‰©ç†çº¦æŸçš„è§„åˆ’å†³ç­–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniEVAåœ¨é€šç”¨å…·èº«æ¨ç†æ€§èƒ½æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼Œå¹¶åœ¨å„ç§ä¸‹æ¸¸åœºæ™¯ä¸­å±•ç°å‡ºå¼ºå¤§çš„è§„åˆ’èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå…·èº«æ™ºèƒ½å¼€è¾Ÿäº†æ–°çš„æœºé‡ï¼Œå®ç°äº†å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œäº¤äº’ï¼Œä»¥åŠè¿ç»­çš„ç©ºé—´å†³ç­–ã€‚ç„¶è€Œï¼Œç›®å‰åŸºäºMLLMçš„å…·èº«ç³»ç»Ÿé¢ä¸´ä¸¤ä¸ªå…³é”®é™åˆ¶ã€‚é¦–å…ˆæ˜¯å‡ ä½•é€‚åº”æ€§å·®è·ï¼šä»…åœ¨2Dè¾“å…¥ä¸Šè®­ç»ƒæˆ–é‡‡ç”¨ç¡¬ç¼–ç 3Då‡ ä½•æ³¨å…¥çš„æ¨¡å‹ï¼Œè¦ä¹ˆç©ºé—´ä¿¡æ¯ä¸è¶³ï¼Œè¦ä¹ˆ2Dæ³›åŒ–èƒ½åŠ›å—é™ï¼Œå¯¼è‡´åœ¨å…·æœ‰ä¸åŒç©ºé—´éœ€æ±‚çš„ä»»åŠ¡ä¸­é€‚åº”æ€§è¾ƒå·®ã€‚å…¶æ¬¡æ˜¯å…·èº«çº¦æŸå·®è·ï¼šå…ˆå‰çš„å·¥ä½œå¸¸å¸¸å¿½ç•¥çœŸå®æœºå™¨äººçš„ç‰©ç†çº¦æŸå’Œèƒ½åŠ›ï¼Œå¯¼è‡´ç†è®ºä¸Šæœ‰æ•ˆä½†å®é™…ä¸Šä¸å¯è¡Œçš„ä»»åŠ¡è®¡åˆ’ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniEVAâ€”â€”ä¸€ç§å…·èº«é€šç”¨è§„åˆ’å™¨ï¼Œé€šè¿‡ä¸¤é¡¹å…³é”®åˆ›æ–°å®ç°é«˜çº§å…·èº«æ¨ç†å’Œä»»åŠ¡è§„åˆ’ï¼šï¼ˆ1ï¼‰ä»»åŠ¡è‡ªé€‚åº”3Dæ„ŸçŸ¥æœºåˆ¶ï¼Œå¼•å…¥é—¨æ§è·¯ç”±å™¨æ¥æ‰§è¡ŒåŸºäºä¸Šä¸‹æ–‡éœ€æ±‚çš„æ˜¾å¼é€‰æ‹©æ€§3Dèåˆï¼Œä»è€Œå®ç°é’ˆå¯¹ä¸åŒå…·èº«ä»»åŠ¡çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥3Dæ„ŸçŸ¥ã€‚ï¼ˆ2ï¼‰å…·èº«æ„ŸçŸ¥æ¨ç†æ¡†æ¶ï¼Œå°†ä»»åŠ¡ç›®æ ‡å’Œå…·èº«çº¦æŸè”åˆçº³å…¥æ¨ç†å¾ªç¯ï¼Œä»è€Œäº§ç”Ÿæ—¢æœ‰ç›®æ ‡å¯¼å‘åˆå¯æ‰§è¡Œçš„è§„åˆ’å†³ç­–ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniEVAä¸ä»…å®ç°äº†æœ€å…ˆè¿›çš„é€šç”¨å…·èº«æ¨ç†æ€§èƒ½ï¼Œè€Œä¸”åœ¨å„ç§ä¸‹æ¸¸åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚å¯¹ä¸€å¥—æå‡ºçš„å…·èº«åŸºå‡†ï¼ˆåŒ…æ‹¬åŸå§‹ä»»åŠ¡å’Œå¤åˆä»»åŠ¡ï¼‰çš„è¯„ä¼°è¯å®äº†å…¶ç¨³å¥å’Œé€šç”¨çš„è§„åˆ’èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å…·èº«æ™ºèƒ½ç³»ç»Ÿï¼Œåœ¨å¤„ç†ä¸åŒç©ºé—´å‡ ä½•ç»“æ„çš„ä»»åŠ¡æ—¶ï¼Œç”±äºç¼ºä¹è¶³å¤Ÿçš„3Dç©ºé—´ä¿¡æ¯æˆ–å—é™äº2Dæ³›åŒ–èƒ½åŠ›ï¼Œå¯¼è‡´å‡ ä½•é€‚åº”æ€§ä¸è¶³ã€‚åŒæ—¶ï¼Œè¿™äº›ç³»ç»Ÿå¾€å¾€å¿½ç•¥äº†çœŸå®æœºå™¨äººçš„ç‰©ç†çº¦æŸï¼Œç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’åœ¨å®é™…ä¸­éš¾ä»¥æ‰§è¡Œã€‚å› æ­¤ï¼Œéœ€è¦è§£å†³å¦‚ä½•åœ¨å…·èº«æ™ºèƒ½ç³»ç»Ÿä¸­å®ç°ä»»åŠ¡è‡ªé€‚åº”çš„3Dæ„ŸçŸ¥å’Œå…·èº«çº¦æŸæ¨ç†çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOmniEVAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä»»åŠ¡è‡ªé€‚åº”çš„3Dä¿¡æ¯èåˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸åŒçš„ä»»åŠ¡éœ€æ±‚é€‰æ‹©æ€§åœ°åˆ©ç”¨3Dä¿¡æ¯ï¼Œä»è€Œæé«˜å‡ ä½•é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œå°†ä»»åŠ¡ç›®æ ‡å’Œå…·èº«çº¦æŸè”åˆçº³å…¥æ¨ç†å¾ªç¯ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’æ—¢èƒ½å®ç°ç›®æ ‡ï¼Œåˆç¬¦åˆæœºå™¨äººçš„ç‰©ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniEVAçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä»»åŠ¡è‡ªé€‚åº”3Dæ„ŸçŸ¥æ¨¡å—å’Œå…·èº«æ„ŸçŸ¥æ¨ç†æ¨¡å—ã€‚ä»»åŠ¡è‡ªé€‚åº”3Dæ„ŸçŸ¥æ¨¡å—è´Ÿè´£ä»å¤šæ¨¡æ€è¾“å…¥ä¸­æå–3Dä¿¡æ¯ï¼Œå¹¶æ ¹æ®ä»»åŠ¡éœ€æ±‚è¿›è¡Œé€‰æ‹©æ€§èåˆã€‚å…·èº«æ„ŸçŸ¥æ¨ç†æ¨¡å—åˆ™åˆ©ç”¨èåˆåçš„3Dä¿¡æ¯ã€ä»»åŠ¡ç›®æ ‡å’Œå…·èº«çº¦æŸè¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆå¯æ‰§è¡Œçš„ä»»åŠ¡è®¡åˆ’ã€‚æ•´ä¸ªæµç¨‹æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œä¸æ–­ä¼˜åŒ–ä»»åŠ¡è®¡åˆ’ï¼Œç›´åˆ°æ»¡è¶³æ‰€æœ‰çº¦æŸæ¡ä»¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šOmniEVAçš„å…³é”®åˆ›æ–°åœ¨äºä»»åŠ¡è‡ªé€‚åº”3Dæ„ŸçŸ¥æœºåˆ¶å’Œå…·èº«æ„ŸçŸ¥æ¨ç†æ¡†æ¶ã€‚ä»»åŠ¡è‡ªé€‚åº”3Dæ„ŸçŸ¥æœºåˆ¶é€šè¿‡é—¨æ§è·¯ç”±å™¨å®ç°å¯¹3Dä¿¡æ¯çš„é€‰æ‹©æ€§èåˆï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡éœ€æ±‚åŠ¨æ€è°ƒæ•´å¯¹ä¸åŒ3Dä¿¡æ¯çš„å…³æ³¨ç¨‹åº¦ã€‚å…·èº«æ„ŸçŸ¥æ¨ç†æ¡†æ¶åˆ™å°†ä»»åŠ¡ç›®æ ‡å’Œå…·èº«çº¦æŸè”åˆå»ºæ¨¡ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’æ—¢æœ‰ç›®æ ‡å¯¼å‘ï¼Œåˆç¬¦åˆå®é™…çš„ç‰©ç†é™åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šä»»åŠ¡è‡ªé€‚åº”3Dæ„ŸçŸ¥æ¨¡å—ä¸­çš„é—¨æ§è·¯ç”±å™¨é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯è®¡ç®—æ¯ä¸ª3Dç‰¹å¾çš„é‡è¦æ€§æƒé‡ï¼Œå¹¶åˆ©ç”¨è¿™äº›æƒé‡å¯¹3Dç‰¹å¾è¿›è¡ŒåŠ æƒèåˆã€‚å…·èº«æ„ŸçŸ¥æ¨ç†æ¡†æ¶ä¸­ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒæ¨¡å‹ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡åŒæ—¶è€ƒè™‘äº†ä»»åŠ¡ç›®æ ‡çš„å®Œæˆåº¦å’Œå…·èº«çº¦æŸçš„æ»¡è¶³ç¨‹åº¦ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æ ¹æ®ä¸åŒçš„ä»»åŠ¡åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniEVAåœ¨å¤šä¸ªå…·èº«æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤åˆä»»åŠ¡çš„è§„åˆ’æˆåŠŸç‡ä¸Šï¼ŒOmniEVAç›¸æ¯”äºç°æœ‰æ–¹æ³•æå‡äº†æ˜¾è‘—çš„ç™¾åˆ†æ¯”ï¼ˆå…·ä½“æ•°å€¼éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ï¼‰ã€‚æ­¤å¤–ï¼ŒOmniEVAåœ¨ä¸åŒç±»å‹çš„æœºå™¨äººå¹³å°ä¸Šä¹Ÿå±•ç°å‡ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶é²æ£’æ€§å’Œé€šç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OmniEVAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£ç¯å¢ƒï¼Œæ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶ä¸äººç±»è¿›è¡Œæ›´è‡ªç„¶çš„äº¤äº’ã€‚æœªæ¥ï¼ŒOmniEVAæœ‰æœ›æˆä¸ºé€šç”¨å…·èº«æ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œæ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible. To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io

