---
layout: default
title: ObjectReact: Learning Object-Relative Control for Visual Navigation
---

# ObjectReact: Learning Object-Relative Control for Visual Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09594" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09594v1</a>
  <a href="https://arxiv.org/pdf/2509.09594.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09594v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09594v1', 'ObjectReact: Learning Object-Relative Control for Visual Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: CoRL 2025; 23 pages including appendix

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºObjectReactä»¥è§£å†³è§†è§‰å¯¼èˆªä¸­çš„æ§åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `è§†è§‰å¯¼èˆª` `ç‰©ä½“ç›¸å¯¹æ§åˆ¶` `æ‹“æ‰‘åœ°å›¾` `å±€éƒ¨æ§åˆ¶å™¨` `è·¯å¾„è§„åˆ’` `æœºå™¨äººå¯¼èˆª` `ç©ºé—´ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾åƒç›¸å¯¹æ–¹æ³•åœ¨æ§åˆ¶é¢„æµ‹ä¸­å—é™äºä»£ç†çš„å§¿æ€å’Œä½“ç°ï¼Œéš¾ä»¥é€‚åº”å¤šæ ·åŒ–çš„å¯¼èˆªåœºæ™¯ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç‰©ä½“ç›¸å¯¹æ§åˆ¶çš„å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç‰©ä½“ä½œä¸ºåœ°å›¾çš„å±æ€§ï¼Œæä¾›äº†æ›´å…·ä¸å˜æ€§çš„ä¸–ç•Œè¡¨ç¤ºã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒObjectReactåœ¨å¤šç§å¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ä¼ æ„Ÿå™¨é«˜åº¦å˜åŒ–å’Œåå‘å¯¼èˆªç­‰æŒ‘æˆ˜ä¸‹ï¼Œå…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½¿ç”¨å•ä¸€ç›¸æœºå’Œæ‹“æ‰‘åœ°å›¾è¿›è¡Œè§†è§‰å¯¼èˆªå·²æˆä¸ºä¸€ç§å¸å¼•äººçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„å›¾åƒç›¸å¯¹æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„â€œç‰©ä½“ç›¸å¯¹â€æ§åˆ¶å­¦ä¹ èŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–å…ˆå‰ç»éªŒçš„æƒ…å†µä¸‹æ¢ç´¢æ–°è·¯å¾„ï¼Œå¹¶å°†æ§åˆ¶é¢„æµ‹ä¸å›¾åƒåŒ¹é…é—®é¢˜è§£è€¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›¸å¯¹3Dåœºæ™¯å›¾çš„æ‹“æ‰‘åœ°å›¾è¡¨ç¤ºï¼Œåˆ©ç”¨æ›´å…·ä¿¡æ¯é‡çš„ç‰©ä½“çº§å…¨å±€è·¯å¾„è§„åˆ’æˆæœ¬æ¥è®­ç»ƒå±€éƒ¨æ§åˆ¶å™¨ObjectReactã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒä¼ æ„Ÿå™¨é«˜åº¦å’Œå¤šç§å¯¼èˆªä»»åŠ¡ä¸­ä¼˜äºå›¾åƒç›¸å¯¹æ§åˆ¶ï¼Œä¸”åœ¨çœŸå®å®¤å†…ç¯å¢ƒä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰å¯¼èˆªæ–¹æ³•ä¸­ï¼Œå›¾åƒç›¸å¯¹æ§åˆ¶åœ¨ä¸åŒç¯å¢ƒå’Œå§¿æ€ä¸‹çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ–°è·¯å¾„æ¢ç´¢å’Œæ§åˆ¶é¢„æµ‹çš„å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„â€œç‰©ä½“ç›¸å¯¹â€æ§åˆ¶å­¦ä¹ èŒƒå¼ï¼Œåˆ©ç”¨ç‰©ä½“ä½œä¸ºåœ°å›¾çš„å±æ€§ï¼Œæä¾›ä¸ä¾èµ–äºä»£ç†å§¿æ€çš„ä¸–ç•Œè¡¨ç¤ºï¼Œä»è€Œå®ç°æ›´é«˜çš„æ§åˆ¶çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªç›¸å¯¹3Dåœºæ™¯å›¾çš„æ‹“æ‰‘åœ°å›¾è¡¨ç¤ºï¼Œç»“åˆâ€œWayObject Costmapâ€è¿›è¡Œå±€éƒ¨æ§åˆ¶å™¨ObjectReactçš„è®­ç»ƒï¼Œé¿å…äº†å¯¹RGBè¾“å…¥çš„ä¾èµ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ§åˆ¶é¢„æµ‹ä¸å›¾åƒåŒ¹é…é—®é¢˜è§£è€¦ï¼Œå…è®¸åœ¨ä¸åŒçš„ç¯å¢ƒå’Œä»»åŠ¡ä¸­å®ç°é«˜ä¸å˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è·¨ä½“ç°çš„éƒ¨ç½²ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç‰©ä½“çº§è·¯å¾„è§„åˆ’æˆæœ¬ï¼Œå¹¶è®¾è®¡äº†é€‚åº”ä¸åŒä¼ æ„Ÿå™¨é«˜åº¦çš„ç½‘ç»œç»“æ„ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒObjectReactåœ¨å¤šç§å¯¼èˆªä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿå›¾åƒç›¸å¯¹æ§åˆ¶æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚åœ¨ä¼ æ„Ÿå™¨é«˜åº¦å˜åŒ–çš„æƒ…å†µä¸‹ï¼ŒObjectReactçš„æ§åˆ¶ç²¾åº¦æé«˜äº†çº¦20%ï¼Œå¹¶ä¸”åœ¨åå‘å¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…ç³»ç»Ÿå’Œå¢å¼ºç°å®ç­‰ã€‚é€šè¿‡å®ç°æ›´çµæ´»çš„è§†è§‰å¯¼èˆªï¼ŒObjectReactå¯ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­æé«˜æœºå™¨äººå’Œè®¾å¤‡çš„è‡ªä¸»æ€§å’Œæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½è®¾å¤‡åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an "image-relative" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning "object-relative" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a "relative" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed "ObjectReact", conditioned directly on a high-level "WayObject Costmap" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/

