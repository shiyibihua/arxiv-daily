---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-09-11
---

# cs.ROï¼ˆ2025-09-11ï¼‰

ğŸ“Š å…± **19** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (12 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250909106v2-lipm-guided-reinforcement-learning-for-stable-and-perceptive-locomot.html">LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots</a></td>
  <td>æå‡ºåŸºäºLIPMå¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ç°åŒè¶³æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸­çš„ç¨³å®šæ„ŸçŸ¥è¿åŠ¨</td>
  <td class="tags-cell"><span class="paper-tag">bipedal</span> <span class="paper-tag">biped</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09106v2" data-paper-url="./papers/250909106v2-lipm-guided-reinforcement-learning-for-stable-and-perceptive-locomot.html" onclick="toggleFavorite(this, '2509.09106v2', 'LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250909674v1-simplevla-rl-scaling-vla-training-via-reinforcement-learning.html">SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</a></td>
  <td>SimpleVLA-RLï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æ‰©å±•VLAæ¨¡å‹è®­ç»ƒï¼Œæå‡æœºå™¨äººæ“ä½œæ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">vision-language-action</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09674v1" data-paper-url="./papers/250909674v1-simplevla-rl-scaling-vla-training-via-reinforcement-learning.html" onclick="toggleFavorite(this, '2509.09674v1', 'SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250909769v1-mimicdroid-in-context-learning-for-humanoid-robot-manipulation-from-.html">MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos</a></td>
  <td>MimicDroidï¼šåˆ©ç”¨äººç±»æ¸¸æˆè§†é¢‘è¿›è¡Œç±»äººæœºå™¨äººæ“ä½œçš„ä¸Šä¸‹æ–‡å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09769v1" data-paper-url="./papers/250909769v1-mimicdroid-in-context-learning-for-humanoid-robot-manipulation-from-.html" onclick="toggleFavorite(this, '2509.09769v1', 'MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250909484v1-bagit-an-adaptive-dual-arm-manipulation-of-fabric-bags-for-object-ba.html">BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging</a></td>
  <td>æå‡ºè‡ªé€‚åº”åŒè‡‚æ“ä½œç­–ç•¥ï¼Œè§£å†³æŸ”æ€§è¢‹è£…ç‰©ä½“çš„è‡ªåŠ¨åŒ–è£…è¢‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dual-arm</span> <span class="paper-tag">MPC</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09484v1" data-paper-url="./papers/250909484v1-bagit-an-adaptive-dual-arm-manipulation-of-fabric-bags-for-object-ba.html" onclick="toggleFavorite(this, '2509.09484v1', 'BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250909671v1-dexplore-scalable-neural-control-for-dexterous-manipulation-from-ref.html">Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration</a></td>
  <td>Dexploreï¼šåŸºäºå‚è€ƒèŒƒå›´æ¢ç´¢çš„å¯æ‰©å±•ç¥ç»æ§åˆ¶ï¼Œç”¨äºçµå·§æ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09671v1" data-paper-url="./papers/250909671v1-dexplore-scalable-neural-control-for-dexterous-manipulation-from-ref.html" onclick="toggleFavorite(this, '2509.09671v1', 'Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250909364v1-agiloped-agile-open-source-humanoid-robot-for-research.html">AGILOped: Agile Open-Source Humanoid Robot for Research</a></td>
  <td>AGILOpedï¼šä¸€æ¬¾é«˜æ€§èƒ½ã€ä½æˆæœ¬ã€å¼€æºçš„äººå½¢æœºå™¨äººå¹³å°</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09364v1" data-paper-url="./papers/250909364v1-agiloped-agile-open-source-humanoid-robot-for-research.html" onclick="toggleFavorite(this, '2509.09364v1', 'AGILOped: Agile Open-Source Humanoid Robot for Research')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250909141v1-aeos-active-environment-aware-optimal-scanning-control-for-uav-lidar.html">AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes</a></td>
  <td>æå‡ºAEOSï¼Œä¸€ç§ç¯å¢ƒæ„ŸçŸ¥çš„ä¸»åŠ¨æ‰«ææ§åˆ¶æ–¹æ³•ï¼Œæå‡å¤æ‚åœºæ™¯ä¸‹æ— äººæœºæ¿€å…‰é›·è¾¾æƒ¯æ€§é‡Œç¨‹è®¡ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09141v1" data-paper-url="./papers/250909141v1-aeos-active-environment-aware-optimal-scanning-control-for-uav-lidar.html" onclick="toggleFavorite(this, '2509.09141v1', 'AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250909893v1-self-augmented-robot-trajectory-efficient-imitation-learning-via-saf.html">Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision</a></td>
  <td>SARTï¼šé€šè¿‡å®‰å…¨è‡ªå¢å¼ºçš„æœºå™¨äººè½¨è¿¹å­¦ä¹ ï¼Œæå‡æ¨¡ä»¿å­¦ä¹ æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09893v1" data-paper-url="./papers/250909893v1-self-augmented-robot-trajectory-efficient-imitation-learning-via-saf.html" onclick="toggleFavorite(this, '2509.09893v1', 'Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250909283v1-renet-fault-tolerant-motion-control-for-quadruped-robots-via-redunda.html">RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse</a></td>
  <td>RENetï¼šåŸºäºå†—ä½™ä¼°è®¡å™¨ç½‘ç»œçš„å››è¶³æœºå™¨äººå®¹é”™è¿åŠ¨æ§åˆ¶ï¼Œè§£å†³è§†è§‰å¤±æ•ˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09283v1" data-paper-url="./papers/250909283v1-renet-fault-tolerant-motion-control-for-quadruped-robots-via-redunda.html" onclick="toggleFavorite(this, '2509.09283v1', 'RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250909074v2-koopmotion-learning-almost-divergence-free-koopman-flow-fields-for-m.html">KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning</a></td>
  <td>KoopMotionï¼šå­¦ä¹ è¿‘ä¼¼æ— æ•£åº¦çš„Koopmanæµåœºç”¨äºè¿åŠ¨è§„åˆ’ï¼Œå®ç°è½¨è¿¹æ”¶æ•›ä¸è·Ÿè¸ªã€‚</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09074v2" data-paper-url="./papers/250909074v2-koopmotion-learning-almost-divergence-free-koopman-flow-fields-for-m.html" onclick="toggleFavorite(this, '2509.09074v2', 'KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250909613v2-mofu-development-of-a-morphing-fluffy-unit-with-expansion-and-contra.html">MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements</a></td>
  <td>æå‡ºå¯è†¨èƒ€æ”¶ç¼©çš„MOFUæœºå™¨äººï¼Œæ¢ç´¢å½¢å˜è¿åŠ¨å¯¹æ‹ŸäººåŒ–æ„ŸçŸ¥çš„å½±å“</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09613v2" data-paper-url="./papers/250909613v2-mofu-development-of-a-morphing-fluffy-unit-with-expansion-and-contra.html" onclick="toggleFavorite(this, '2509.09613v2', 'MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250909546v1-a-neuromorphic-incipient-slip-detection-system-using-papillae-morpho.html">A Neuromorphic Incipient Slip Detection System using Papillae Morphology</a></td>
  <td>æå‡ºä¸€ç§åŸºäºä¹³çªå½¢æ€çš„ç¥ç»å½¢æ€åˆå§‹æ»‘åŠ¨æ£€æµ‹ç³»ç»Ÿï¼Œç”¨äºæå‡æœºå™¨äººæ“ä½œå®‰å…¨æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09546v1" data-paper-url="./papers/250909546v1-a-neuromorphic-incipient-slip-detection-system-using-papillae-morpho.html" onclick="toggleFavorite(this, '2509.09546v1', 'A Neuromorphic Incipient Slip Detection System using Papillae Morphology')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250909372v2-vla-adapter-an-effective-paradigm-for-tiny-scale-vision-language-act.html">VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model</a></td>
  <td>VLA-Adapterï¼šä¸€ç§é«˜æ•ˆçš„å¾®å‹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹èŒƒå¼</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09372v2" data-paper-url="./papers/250909372v2-vla-adapter-an-effective-paradigm-for-tiny-scale-vision-language-act.html" onclick="toggleFavorite(this, '2509.09372v2', 'VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250910570v1-large-foundation-models-for-trajectory-prediction-in-autonomous-driv.html">Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey</a></td>
  <td>ç»¼è¿°æ€§è®ºæ–‡ï¼šåˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹è§£å†³è‡ªåŠ¨é©¾é©¶è½¨è¿¹é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10570v1" data-paper-url="./papers/250910570v1-large-foundation-models-for-trajectory-prediction-in-autonomous-driv.html" onclick="toggleFavorite(this, '2509.10570v1', 'Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250909805v1-mimo-grows-simulating-body-and-sensory-development-in-a-multimodal-i.html">MIMo grows! Simulating body and sensory development in a multimodal infant model</a></td>
  <td>MIMo v2ï¼šæ„å»ºå¯æ¨¡æ‹Ÿèº«ä½“å’Œæ„Ÿè§‰å‘è‚²çš„å¤šæ¨¡æ€å©´å„¿æ¨¡å‹ï¼Œä¿ƒè¿›æ—©æœŸå‘å±•ç ”ç©¶ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09805v1" data-paper-url="./papers/250909805v1-mimo-grows-simulating-body-and-sensory-development-in-a-multimodal-i.html" onclick="toggleFavorite(this, '2509.09805v1', 'MIMo grows! Simulating body and sensory development in a multimodal infant model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250909332v2-omnieva-embodied-versatile-planner-via-task-adaptive-3d-grounded-and.html">OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</a></td>
  <td>OmniEVAï¼šé€šè¿‡ä»»åŠ¡è‡ªé€‚åº”3Dæ„ŸçŸ¥å’Œå…·èº«è®¤çŸ¥å®ç°é€šç”¨å…·èº«è§„åˆ’</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09332v2" data-paper-url="./papers/250909332v2-omnieva-embodied-versatile-planner-via-task-adaptive-3d-grounded-and.html" onclick="toggleFavorite(this, '2509.09332v2', 'OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250925200v1-when-and-how-to-express-empathy-in-human-robot-interaction-scenarios.html">When and How to Express Empathy in Human-Robot Interaction Scenarios</a></td>
  <td>æå‡ºwhEEæ¡†æ¶ï¼Œä½¿ç¤¾äº¤æœºå™¨äººå…·å¤‡æƒ…å¢ƒæ„ŸçŸ¥çš„å…±æƒ…è¡¨è¾¾èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25200v1" data-paper-url="./papers/250925200v1-when-and-how-to-express-empathy-in-human-robot-interaction-scenarios.html" onclick="toggleFavorite(this, '2509.25200v1', 'When and How to Express Empathy in Human-Robot Interaction Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250909509v2-smapper-a-multi-modal-data-acquisition-platform-for-slam-benchmarkin.html">SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking</a></td>
  <td>SMapperï¼šç”¨äºSLAMåŸºå‡†æµ‹è¯•çš„å¤šæ¨¡æ€æ•°æ®é‡‡é›†å¹³å°</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09509v2" data-paper-url="./papers/250909509v2-smapper-a-multi-modal-data-acquisition-platform-for-slam-benchmarkin.html" onclick="toggleFavorite(this, '2509.09509v2', 'SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250909594v1-objectreact-learning-object-relative-control-for-visual-navigation.html">ObjectReact: Learning Object-Relative Control for Visual Navigation</a></td>
  <td>æå‡ºObjectReactä»¥è§£å†³è§†è§‰å¯¼èˆªä¸­çš„æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">cross-embodiment</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09594v1" data-paper-url="./papers/250909594v1-objectreact-learning-object-relative-control-for-visual-navigation.html" onclick="toggleFavorite(this, '2509.09594v1', 'ObjectReact: Learning Object-Relative Control for Visual Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)