---
layout: default
title: Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains
---

# Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.06486" target="_blank" class="toolbar-btn">arXiv: 2512.06486v2</a>
    <a href="https://arxiv.org/pdf/2512.06486.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.06486v2" 
            onclick="toggleFavorite(this, '2512.06486v2', 'Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Wanru Gong, Xinyi Zheng, Yuan Hui, Zhongjun Li, Weiqiang Wang, Xiaoqing Zhu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-06 (æ›´æ–°: 2025-12-13)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç†µæ§åˆ¶çš„å†…åœ¨åŠ¨æœºå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæå‡å››è¶³æœºå™¨äººå¤æ‚åœ°å½¢è¿åŠ¨èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å››è¶³æœºå™¨äºº` `å¼ºåŒ–å­¦ä¹ ` `å†…åœ¨åŠ¨æœº` `ç†µæ§åˆ¶` `å¤æ‚åœ°å½¢` `è¿åŠ¨æ§åˆ¶` `æœºå™¨äºº locomotion`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å››è¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶ä¸­æ˜“é™·å…¥æ—©ç†Ÿæ”¶æ•›ï¼Œå¯¼è‡´æ¬¡ä¼˜è¿åŠ¨ç­–ç•¥å’Œä»»åŠ¡æ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºECIMç®—æ³•ï¼Œç»“åˆç†µæ§åˆ¶å’Œå†…åœ¨åŠ¨æœºï¼Œé¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢æœªçŸ¥çŠ¶æ€ï¼Œé¿å…è¿‡æ—©æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒECIMåœ¨å¤šç§å¤æ‚åœ°å½¢ä¸‹æ˜¾è‘—æå‡äº†å››è¶³æœºå™¨äººçš„è¿åŠ¨æ€§èƒ½ï¼Œé™ä½äº†èƒ½é‡æ¶ˆè€—å’Œå…³èŠ‚å‹åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç†µæ§åˆ¶å†…åœ¨åŠ¨æœºï¼ˆECIMï¼‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³å››è¶³æœºå™¨äººè¿åŠ¨ç­–ç•¥è®­ç»ƒä¸­å¸¸è§çš„æ—©ç†Ÿæ”¶æ•›é—®é¢˜ã€‚ä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç³»åˆ—ç®—æ³•ä¸åŒï¼ŒECIMé€šè¿‡ç»“åˆå†…åœ¨åŠ¨æœºå’Œè‡ªé€‚åº”æ¢ç´¢æ¥å‡å°‘æ—©ç†Ÿæ”¶æ•›ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨Isaac Gymçš„å…­ç§åœ°å½¢ç±»åˆ«ï¼ˆå‘ä¸Šæ–œå¡ã€å‘ä¸‹æ–œå¡ã€ä¸å¹³å¦ç²—ç³™åœ°å½¢ã€ä¸Šå‡æ¥¼æ¢¯ã€ä¸‹é™æ¥¼æ¢¯å’Œå¹³å¦åœ°é¢ï¼‰ä¸­ï¼ŒECIMå§‹ç»ˆä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œä»»åŠ¡å¥–åŠ±æé«˜äº†4-12%ï¼Œèº«ä½“ä¿¯ä»°æŒ¯è¡å³°å€¼é™ä½äº†23-29%ï¼Œå…³èŠ‚åŠ é€Ÿåº¦é™ä½äº†20-32%ï¼Œå…³èŠ‚æ‰­çŸ©æ¶ˆè€—é™ä½äº†11-20%ã€‚ECIMé€šè¿‡ç»“åˆç†µæ§åˆ¶å’Œå†…åœ¨åŠ¨æœºæ§åˆ¶ï¼Œåœ¨ä¸åŒåœ°å½¢ä¸­å®ç°äº†æ›´å¥½çš„å››è¶³è¿åŠ¨ç¨³å®šæ€§ï¼ŒåŒæ—¶é™ä½äº†èƒ½é‡æ¶ˆè€—ï¼Œä½¿å…¶æˆä¸ºå¤æ‚æœºå™¨äººæ§åˆ¶ä»»åŠ¡çš„å®ç”¨é€‰æ‹©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºPPOçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨è®­ç»ƒå››è¶³æœºå™¨äººè¿åŠ¨ç­–ç•¥æ—¶ï¼Œå®¹æ˜“å‡ºç°æ—©ç†Ÿæ”¶æ•›çš„é—®é¢˜ã€‚è¿™æ„å‘³ç€æ™ºèƒ½ä½“åœ¨æ¢ç´¢åˆ°å…¨å±€æœ€ä¼˜ç­–ç•¥ä¹‹å‰ï¼Œå°±é™·å…¥äº†å±€éƒ¨æœ€ä¼˜è§£ï¼Œå¯¼è‡´æœ€ç»ˆå­¦ä¹ åˆ°çš„è¿åŠ¨ç­–ç•¥å¹¶éæœ€ä¼˜ï¼Œä»è€Œé™åˆ¶äº†æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸‹çš„è¿åŠ¨èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æœ‰æ•ˆçš„æ¢ç´¢æœºåˆ¶ï¼Œéš¾ä»¥è·³å‡ºå±€éƒ¨æœ€ä¼˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ç†µæ§åˆ¶çš„å†…åœ¨åŠ¨æœºæœºåˆ¶ã€‚ç†µæ§åˆ¶ç”¨äºé¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢æœªçŸ¥çš„çŠ¶æ€ç©ºé—´ï¼Œé¿å…è¿‡æ—©æ”¶æ•›ã€‚å†…åœ¨åŠ¨æœºåˆ™ä¸ºæ™ºèƒ½ä½“æä¾›é¢å¤–çš„å¥–åŠ±ä¿¡å·ï¼Œä¿ƒä½¿å…¶ä¸»åŠ¨æ¢ç´¢ç¯å¢ƒï¼Œå­¦ä¹ æ›´é²æ£’çš„è¿åŠ¨ç­–ç•¥ã€‚é€šè¿‡å°†ä¸¤è€…ç»“åˆï¼ŒECIMç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä»è€Œé¿å…æ—©ç†Ÿæ”¶æ•›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šECIMç®—æ³•çš„æ•´ä½“æ¡†æ¶ä»ç„¶åŸºäºActor-Criticæ¶æ„ï¼Œç±»ä¼¼äºPPOã€‚ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) Actorç½‘ç»œï¼Œç”¨äºç”ŸæˆåŠ¨ä½œç­–ç•¥ï¼›2) Criticç½‘ç»œï¼Œç”¨äºè¯„ä¼°çŠ¶æ€ä»·å€¼ï¼›3) ç†µå¥–åŠ±æ¨¡å—ï¼Œæ ¹æ®å½“å‰ç­–ç•¥çš„ç†µå€¼ï¼Œç»™äºˆæ™ºèƒ½ä½“é¢å¤–çš„å¥–åŠ±ï¼Œé¼“åŠ±æ¢ç´¢ï¼›4) å†…åœ¨åŠ¨æœºå¥–åŠ±æ¨¡å—ï¼Œæ ¹æ®æ™ºèƒ½ä½“å¯¹ç¯å¢ƒçš„é¢„æµ‹è¯¯å·®ï¼Œç»™äºˆæ™ºèƒ½ä½“é¢å¤–çš„å¥–åŠ±ï¼Œé¼“åŠ±æ¢ç´¢æœªçŸ¥çŠ¶æ€ã€‚è¿™äº›æ¨¡å—å…±åŒä½œç”¨ï¼ŒæŒ‡å¯¼æ™ºèƒ½ä½“å­¦ä¹ æœ€ä¼˜è¿åŠ¨ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šECIMç®—æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå°†ç†µæ§åˆ¶å’Œå†…åœ¨åŠ¨æœºç›¸ç»“åˆï¼Œå¹¶å°†å…¶åº”ç”¨äºå››è¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶ã€‚ä¸ä¼ ç»Ÿçš„PPOç®—æ³•ç›¸æ¯”ï¼ŒECIMç®—æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é¿å…æ—©ç†Ÿæ”¶æ•›ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´é²æ£’ã€æ›´é«˜æ•ˆçš„è¿åŠ¨ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒECIMç®—æ³•è¿˜é‡‡ç”¨äº†è‡ªé€‚åº”çš„æ¢ç´¢ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®ç¯å¢ƒçš„å¤æ‚ç¨‹åº¦åŠ¨æ€è°ƒæ•´æ¢ç´¢åŠ›åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šECIMç®—æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç†µå¥–åŠ±çš„è®¾è®¡ï¼Œé€šå¸¸ä½¿ç”¨ç­–ç•¥åˆ†å¸ƒçš„ç†µä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä¾‹å¦‚ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒçš„æ–¹å·®æˆ–softmaxè¾“å‡ºçš„ç†µï¼›2) å†…åœ¨åŠ¨æœºå¥–åŠ±çš„è®¾è®¡ï¼Œé€šå¸¸åŸºäºé¢„æµ‹è¯¯å·®ï¼Œä¾‹å¦‚ä½¿ç”¨å‰å‘æ¨¡å‹çš„é¢„æµ‹è¯¯å·®æˆ–çŠ¶æ€è¡¨å¾çš„é‡æ„è¯¯å·®ï¼›3) Actorå’ŒCriticç½‘ç»œçš„ç»“æ„ï¼Œé€šå¸¸ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºæˆ–å¾ªç¯ç¥ç»ç½‘ç»œï¼›4) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼ŒåŒ…æ‹¬ç­–ç•¥æ¢¯åº¦æŸå¤±ã€ä»·å€¼å‡½æ•°æŸå¤±ã€ç†µå¥–åŠ±æŸå¤±å’Œå†…åœ¨åŠ¨æœºå¥–åŠ±æŸå¤±ã€‚è¿™äº›è®¾è®¡å…±åŒå†³å®šäº†ECIMç®—æ³•çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒECIMç®—æ³•åœ¨å…­ç§å¤æ‚åœ°å½¢ä¸­å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚ä»»åŠ¡å¥–åŠ±å¹³å‡æé«˜äº†4-12%ï¼Œèº«ä½“ä¿¯ä»°æŒ¯è¡å³°å€¼é™ä½äº†23-29%ï¼Œå…³èŠ‚åŠ é€Ÿåº¦é™ä½äº†20-32%ï¼Œå…³èŠ‚æ‰­çŸ©æ¶ˆè€—é™ä½äº†11-20%ã€‚è¿™äº›æ•°æ®è¡¨æ˜ï¼ŒECIMç®—æ³•ä¸ä»…æå‡äº†æœºå™¨äººçš„è¿åŠ¨æ€§èƒ½ï¼Œè¿˜é™ä½äº†èƒ½é‡æ¶ˆè€—å’Œå…³èŠ‚å‹åŠ›ï¼Œä½¿å…¶æ›´å…·å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦å››è¶³æœºå™¨äººè¿›è¡Œå¤æ‚åœ°å½¢è¿åŠ¨çš„åœºæ™¯ï¼Œä¾‹å¦‚æœæ•‘ã€å‹˜æ¢ã€ç‰©æµå’Œå·¡æ£€ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººçš„è¿åŠ¨èƒ½åŠ›å’Œç¨³å®šæ€§ï¼Œå¯ä»¥ä½¿å…¶åœ¨æ¶åŠ£ç¯å¢ƒä¸‹æ‰§è¡Œä»»åŠ¡ï¼Œé™ä½äººå‘˜é£é™©ï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æœºå™¨äººï¼Œä¾‹å¦‚äººå½¢æœºå™¨äººå’Œè½®å¼æœºå™¨äººã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning is the basis of both biological and artificial systems when it comes to mimicking intelligent behaviors. From the classical PPO (Proximal Policy Optimization), there is a series of deep reinforcement learning algorithms which are widely used in training locomotion policies for quadrupedal robots because of their stability and sample efficiency. However, among all these variants, experiments and simulations often converge prematurely, leading to suboptimal locomotion and reduced task performance. Therefore, in this paper, we introduce Entropy-Controlled Intrinsic Motivation (ECIM), an entropy-based reinforcement learning algorithm in contrast with the PPO series, that can reduce premature convergence by combining intrinsic motivation with adaptive exploration.
>   For experiments, in order to parallel with other baselines, we chose to apply it in Isaac Gym across six terrain categories: upward slopes, downward slopes, uneven rough terrain, ascending stairs, descending stairs, and flat ground as widely used. For comparison, our experiments consistently achieve better performance: task rewards increase by 4--12%, peak body pitch oscillation is reduced by 23--29%, joint acceleration decreases by 20--32%, and joint torque consumption declines by 11--20%. Overall, our model ECIM, by combining entropy control and intrinsic motivation control, achieves better results in stability across different terrains for quadrupedal locomotion, and at the same time reduces energetic cost and makes it a practical choice for complex robotic control tasks.

