---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-09-17
---

# cs.ROï¼ˆ2025-09-17ï¼‰

ğŸ“Š å…± **34** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (23 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (23 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250913780v1-behavior-foundation-model-for-humanoid-robots.html">Behavior Foundation Model for Humanoid Robots</a></td>
  <td>æå‡ºäººå½¢æœºå™¨äººè¡Œä¸ºåŸºç¡€æ¨¡å‹ï¼Œæå‡é€šç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">humanoid control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13780v1" data-paper-url="./papers/250913780v1-behavior-foundation-model-for-humanoid-robots.html" onclick="toggleFavorite(this, '2509.13780v1', 'Behavior Foundation Model for Humanoid Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250914349v1-levr-a-modular-vr-teleoperation-framework-for-imitation-learning-in-.html">LeVR: A Modular VR Teleoperation Framework for Imitation Learning in Dexterous Manipulation</a></td>
  <td>LeVRï¼šç”¨äºçµå·§æ“ä½œæ¨¡ä»¿å­¦ä¹ çš„æ¨¡å—åŒ–VRé¥æ“ä½œæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">dexterous manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14349v1" data-paper-url="./papers/250914349v1-levr-a-modular-vr-teleoperation-framework-for-imitation-learning-in-.html" onclick="toggleFavorite(this, '2509.14349v1', 'LeVR: A Modular VR Teleoperation Framework for Imitation Learning in Dexterous Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250913737v1-dynamic-adaptive-legged-locomotion-policy-via-decoupling-reaction-fo.html">Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control</a></td>
  <td>æå‡ºè§£è€¦ååº”åŠ›æ§åˆ¶ä¸æ­¥æ€æ§åˆ¶çš„åŠ¨æ€è‡ªé€‚åº”è…¿è¶³è¿åŠ¨ç­–ç•¥ï¼Œæå‡æ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">legged locomotion</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">locomotion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13737v1" data-paper-url="./papers/250913737v1-dynamic-adaptive-legged-locomotion-policy-via-decoupling-reaction-fo.html" onclick="toggleFavorite(this, '2509.13737v1', 'Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250914380v2-craft-coaching-reinforcement-learning-autonomously-using-foundation-.html">CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</a></td>
  <td>CRAFTï¼šåˆ©ç”¨å…·èº«æ™ºèƒ½è‡ªä¸»æŒ‡å¯¼å¤šæœºå™¨äººå¼ºåŒ–å­¦ä¹ ï¼Œè§£å†³å¤æ‚åä½œä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14380v2" data-paper-url="./papers/250914380v2-craft-coaching-reinforcement-learning-autonomously-using-foundation-.html" onclick="toggleFavorite(this, '2509.14380v2', 'CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250914138v1-seqvla-sequential-task-execution-for-long-horizon-manipulation-with-.html">SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model</a></td>
  <td>SeqVLAï¼šç”¨äºé•¿æ—¶ç¨‹æ“ä½œçš„å…·æœ‰å®Œæˆæ„ŸçŸ¥èƒ½åŠ›çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œè§£å†³åºåˆ—ä»»åŠ¡æ‰§è¡Œé—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14138v1" data-paper-url="./papers/250914138v1-seqvla-sequential-task-execution-for-long-horizon-manipulation-with-.html" onclick="toggleFavorite(this, '2509.14138v1', 'SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250914353v3-dreamcontrol-human-inspired-whole-body-humanoid-control-for-scene-in.html">DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion</a></td>
  <td>DreamControlï¼šé€šè¿‡å¼•å¯¼æ‰©æ•£å®ç°å—äººç±»å¯å‘çš„å…¨èº«äººå½¢æœºå™¨äººåœºæ™¯äº¤äº’æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid control</span> <span class="paper-tag">sim-to-real</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14353v3" data-paper-url="./papers/250914353v3-dreamcontrol-human-inspired-whole-body-humanoid-control-for-scene-in.html" onclick="toggleFavorite(this, '2509.14353v3', 'DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250914010v1-whole-body-motion-control-of-an-omnidirectional-wheel-legged-mobile-.html">Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization</a></td>
  <td>æå‡ºä¸€ç§æ¥è§¦æ„ŸçŸ¥çš„å…¨èº«åŠ¨æ€ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºå…¨å‘è½®è…¿å¼ç§»åŠ¨æœºæ¢°è‡‚çš„è¿åŠ¨æ§åˆ¶ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged robot</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14010v1" data-paper-url="./papers/250914010v1-whole-body-motion-control-of-an-omnidirectional-wheel-legged-mobile-.html" onclick="toggleFavorite(this, '2509.14010v1', 'Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250913774v1-dual-actor-fine-tuning-of-vla-models-a-talk-and-tweak-human-in-the-l.html">Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach</a></td>
  <td>æå‡ºäººæœºåä½œçš„åŒæ¼”å‘˜å¾®è°ƒæ¡†æ¶ä»¥æå‡VLAæ¨¡å‹çš„ä»»åŠ¡è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13774v1" data-paper-url="./papers/250913774v1-dual-actor-fine-tuning-of-vla-models-a-talk-and-tweak-human-in-the-l.html" onclick="toggleFavorite(this, '2509.13774v1', 'Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250914143v1-claw-a-vision-language-action-framework-for-weight-aware-robotic-gra.html">CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping</a></td>
  <td>CLAWï¼šä¸€ç§ç”¨äºé‡é‡æ„ŸçŸ¥æœºå™¨äººæŠ“å–çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dual-arm</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14143v1" data-paper-url="./papers/250914143v1-claw-a-vision-language-action-framework-for-weight-aware-robotic-gra.html" onclick="toggleFavorite(this, '2509.14143v1', 'CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250913839v1-pre-manipulation-alignment-prediction-with-parallel-deep-state-space.html">Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models</a></td>
  <td>æå‡ºå¹¶è¡Œæ·±åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹ä¸Transformerçš„é¢„æ“ä½œå¯¹é½é¢„æµ‹æ–¹æ³•ï¼Œæå‡æœºå™¨äººæ“ä½œæˆåŠŸç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13839v1" data-paper-url="./papers/250913839v1-pre-manipulation-alignment-prediction-with-parallel-deep-state-space.html" onclick="toggleFavorite(this, '2509.13839v1', 'Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250913733v3-fsr-vln-fast-and-slow-reasoning-for-vision-language-navigation-with-.html">FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph</a></td>
  <td>æå‡ºFSR-VLNï¼Œç»“åˆåˆ†å±‚å¤šæ¨¡æ€åœºæ™¯å›¾ä¸å¿«æ…¢æ¨ç†ï¼Œæå‡è§†è§‰è¯­è¨€å¯¼èˆªæ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">Unitree</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13733v3" data-paper-url="./papers/250913733v3-fsr-vln-fast-and-slow-reasoning-for-vision-language-navigation-with-.html" onclick="toggleFavorite(this, '2509.13733v3', 'FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250913903v1-physicalagent-towards-general-cognitive-robotics-with-foundation-wor.html">PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models</a></td>
  <td>PhysicalAgentï¼šåŸºäºä¸–ç•Œæ¨¡å‹çš„é€šç”¨è®¤çŸ¥æœºå™¨äººæ¡†æ¶ï¼Œå®ç°è¿­ä»£æ¨ç†å’Œé—­ç¯æ‰§è¡Œã€‚</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13903v1" data-paper-url="./papers/250913903v1-physicalagent-towards-general-cognitive-robotics-with-foundation-wor.html" onclick="toggleFavorite(this, '2509.13903v1', 'PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250914342v2-multi-quadruped-cooperative-object-transport-learning-decentralized-.html">Multi-Quadruped Cooperative Object Transport: Learning Decentralized Pinch-Lift-Move</a></td>
  <td>æå‡ºä¸€ç§åˆ†æ•£å¼ç­–ç•¥ï¼Œå®ç°å¤šè¶³æœºå™¨äººååŒæ¬è¿æ— æŠ“å–ç‰©ä½“</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">sim2real</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14342v2" data-paper-url="./papers/250914342v2-multi-quadruped-cooperative-object-transport-learning-decentralized-.html" onclick="toggleFavorite(this, '2509.14342v2', 'Multi-Quadruped Cooperative Object Transport: Learning Decentralized Pinch-Lift-Move')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250914063v1-language-conditioning-improves-accuracy-of-aircraft-goal-prediction-.html">Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace</a></td>
  <td>æå‡ºè¯­è¨€æ¡ä»¶çº¦æŸçš„é£æœºç›®æ ‡é¢„æµ‹æ¡†æ¶ï¼Œæå‡éå¡”å°ç©ºåŸŸè‡ªä¸»é£è¡Œå®‰å…¨æ€§</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14063v1" data-paper-url="./papers/250914063v1-language-conditioning-improves-accuracy-of-aircraft-goal-prediction-.html" onclick="toggleFavorite(this, '2509.14063v1', 'Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250913731v1-reinforcement-learning-for-robotic-insertion-of-flexible-cables-in-i.html">Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ å’ŒåŸºç¡€æ¨¡å‹çš„æŸ”æ€§ç”µç¼†æœºå™¨äººæ’å…¥æ–¹æ³•ï¼Œå®ç°é›¶æ ·æœ¬éƒ¨ç½²ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13731v1" data-paper-url="./papers/250913731v1-reinforcement-learning-for-robotic-insertion-of-flexible-cables-in-i.html" onclick="toggleFavorite(this, '2509.13731v1', 'Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250914082v2-flightdiffusion-revolutionising-autonomous-drone-training-with-diffu.html">FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video</a></td>
  <td>FlightDiffusionï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆFPVè§†é¢‘ï¼Œé©æ–°æ— äººæœºè‡ªä¸»è®­ç»ƒ</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14082v2" data-paper-url="./papers/250914082v2-flightdiffusion-revolutionising-autonomous-drone-training-with-diffu.html" onclick="toggleFavorite(this, '2509.14082v2', 'FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250913949v1-share-rl-structured-interactive-reinforcement-learning-for-contact-r.html">SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks</a></td>
  <td>SHaRe-RLï¼šé¢å‘é«˜æŸ”æ€§å·¥ä¸šè£…é…çš„ç»“æ„åŒ–äº¤äº’å¼å¼ºåŒ–å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13949v1" data-paper-url="./papers/250913949v1-share-rl-structured-interactive-reinforcement-learning-for-contact-r.html" onclick="toggleFavorite(this, '2509.13949v1', 'SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250913998v2-flexible-and-foldable-workspace-analysis-and-object-manipulation-usi.html">Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array</a></td>
  <td>æå‡ºä¸€ç§åŸºäºè½¯æ€§äº’è¿æŠ˜çº¸ç»“æ„çš„æŸ”æ€§åˆ†å¸ƒå¼æ“ä½œå™¨ï¼Œç”¨äºæå‡ç‰©ä½“æ“ä½œèƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13998v2" data-paper-url="./papers/250913998v2-flexible-and-foldable-workspace-analysis-and-object-manipulation-usi.html" onclick="toggleFavorite(this, '2509.13998v2', 'Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250913882v1-repulsive-trajectory-modification-and-conflict-resolution-for-effici.html">Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning</a></td>
  <td>æå‡ºåŸºäºæ–¥åŠ›è½¨è¿¹ä¿®æ­£çš„å¤šæœºæ¢°è‡‚è¿åŠ¨è§„åˆ’æ–¹æ³•ï¼Œæå‡æ•ˆç‡å¹¶è§£å†³å†²çª</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13882v1" data-paper-url="./papers/250913882v1-repulsive-trajectory-modification-and-conflict-resolution-for-effici.html" onclick="toggleFavorite(this, '2509.13882v1', 'Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250913833v3-track-any-motions-under-any-disturbances.html">Track Any Motions under Any Disturbances</a></td>
  <td>Any2Trackï¼šå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°äººå½¢æœºå™¨äººå¤æ‚è¿åŠ¨å’ŒæŠ—æ‰°åŠ¨è¿½è¸ª</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">sim2real</span> <span class="paper-tag">Unitree</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13833v3" data-paper-url="./papers/250913833v3-track-any-motions-under-any-disturbances.html" onclick="toggleFavorite(this, '2509.13833v3', 'Track Any Motions under Any Disturbances')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250914383v1-rlbind-adversarial-invariant-cross-modal-alignment-for-unified-robus.html">RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings</a></td>
  <td>RLBindï¼šå¯¹æŠ—ä¸å˜è·¨æ¨¡æ€å¯¹é½ï¼Œç”¨äºç»Ÿä¸€é²æ£’åµŒå…¥</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">zero-shot transfer</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14383v1" data-paper-url="./papers/250914383v1-rlbind-adversarial-invariant-cross-modal-alignment-for-unified-robus.html" onclick="toggleFavorite(this, '2509.14383v1', 'RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250913692v1-hgacnet-hierarchical-graph-attention-network-for-cross-modal-point-c.html">HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion</a></td>
  <td>HGACNetï¼šç”¨äºè·¨æ¨¡æ€ç‚¹äº‘è¡¥å…¨çš„åˆ†å±‚å›¾æ³¨æ„åŠ›ç½‘ç»œ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13692v1" data-paper-url="./papers/250913692v1-hgacnet-hierarchical-graph-attention-network-for-cross-modal-point-c.html" onclick="toggleFavorite(this, '2509.13692v1', 'HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250913771v1-cdflow-generative-gradient-flows-for-configuration-space-distance-fi.html">CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs</a></td>
  <td>CDFlowï¼šåˆ©ç”¨ç¥ç»ODEç”Ÿæˆé…ç½®ç©ºé—´è·ç¦»åœºçš„æ¢¯åº¦æµï¼Œæå‡é«˜è‡ªç”±åº¦æœºå™¨äººè¿åŠ¨è§„åˆ’æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13771v1" data-paper-url="./papers/250913771v1-cdflow-generative-gradient-flows-for-configuration-space-distance-fi.html" onclick="toggleFavorite(this, '2509.13771v1', 'CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/250914421v1-perception-integrated-safety-critical-control-via-analytic-collision.html">Perception-Integrated Safety Critical Control via Analytic Collision Cone Barrier Functions on 3D Gaussian Splatting</a></td>
  <td>æå‡ºåŸºäº3Dé«˜æ–¯æº…å°„åˆ†æç¢°æ’é”¥çš„æ„ŸçŸ¥é›†æˆå®‰å…¨æ§åˆ¶æ–¹æ³•ï¼Œç”¨äºæœºå™¨äººå®‰å…¨å¯¼èˆªã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14421v1" data-paper-url="./papers/250914421v1-perception-integrated-safety-critical-control-via-analytic-collision.html" onclick="toggleFavorite(this, '2509.14421v1', 'Perception-Integrated Safety Critical Control via Analytic Collision Cone Barrier Functions on 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250914191v2-mcgs-slam-a-multi-camera-slam-framework-using-gaussian-splatting-for.html">MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping</a></td>
  <td>MCGS-SLAMï¼šåŸºäºé«˜æ–¯æº…å°„çš„å¤šç›¸æœºSLAMæ¡†æ¶ï¼Œå®ç°é«˜ä¿çœŸåœ°å›¾æ„å»º</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14191v2" data-paper-url="./papers/250914191v2-mcgs-slam-a-multi-camera-slam-framework-using-gaussian-splatting-for.html" onclick="toggleFavorite(this, '2509.14191v2', 'MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250913972v2-bim-informed-visual-slam-for-construction-monitoring.html">BIM Informed Visual SLAM for Construction Monitoring</a></td>
  <td>æå‡ºBIMä¿¡æ¯é©±åŠ¨çš„è§†è§‰SLAMä»¥è§£å†³å»ºç­‘ç›‘æµ‹ä¸­çš„å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13972v2" data-paper-url="./papers/250913972v2-bim-informed-visual-slam-for-construction-monitoring.html" onclick="toggleFavorite(this, '2509.13972v2', 'BIM Informed Visual SLAM for Construction Monitoring')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250913649v1-barometer-aided-attitude-estimation.html">Barometer-Aided Attitude Estimation</a></td>
  <td>æå‡ºæ°”å‹è®¡è¾…åŠ©çš„å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œè§£å†³GNSSæ‹’æ­¢ç¯å¢ƒä¸‹çš„å€¾æ–œè§’ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual odometry</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13649v1" data-paper-url="./papers/250913649v1-barometer-aided-attitude-estimation.html" onclick="toggleFavorite(this, '2509.13649v1', 'Barometer-Aided Attitude Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/250913956v1-seg-parking-towards-safe-efficient-and-generalizable-autonomous-park.html">SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning</a></td>
  <td>æå‡ºSEG-Parkingï¼Œé€šè¿‡ç«¯åˆ°ç«¯ç¦»çº¿å¼ºåŒ–å­¦ä¹ å®ç°å®‰å…¨ã€é«˜æ•ˆå’Œæ³›åŒ–çš„è‡ªåŠ¨æ³Šè½¦</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">offline RL</span> <span class="paper-tag">offline reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13956v1" data-paper-url="./papers/250913956v1-seg-parking-towards-safe-efficient-and-generalizable-autonomous-park.html" onclick="toggleFavorite(this, '2509.13956v1', 'SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250913736v1-motion-adaptation-across-users-and-tasks-for-exoskeletons-via-meta-l.html">Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning</a></td>
  <td>æå‡ºåŸºäºå…ƒå­¦ä¹ çš„å¤–éª¨éª¼è¿åŠ¨è‡ªé€‚åº”æ–¹æ³•ï¼Œæå‡ç”¨æˆ·å’Œä»»åŠ¡æ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">motion adaptation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.13736v1" data-paper-url="./papers/250913736v1-motion-adaptation-across-users-and-tasks-for-exoskeletons-via-meta-l.html" onclick="toggleFavorite(this, '2509.13736v1', 'Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250914159v1-mimic-d-multi-modal-imitation-for-multi-agent-coordination-with-dece.html">MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies</a></td>
  <td>æå‡ºMIMIC-Dï¼Œåˆ©ç”¨å»ä¸­å¿ƒåŒ–æ‰©æ•£ç­–ç•¥å®ç°å¤šæ™ºèƒ½ä½“å¤šæ¨¡æ€æ¨¡ä»¿å­¦ä¹ ä¸ååŒ</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14159v1" data-paper-url="./papers/250914159v1-mimic-d-multi-modal-imitation-for-multi-agent-coordination-with-dece.html" onclick="toggleFavorite(this, '2509.14159v1', 'MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250914295v4-aegis-automated-error-generation-and-attribution-for-multi-agent-sys.html">Aegis: Automated Error Generation and Attribution for Multi-Agent Systems</a></td>
  <td>Aegisï¼šç”¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è‡ªåŠ¨åŒ–é”™è¯¯ç”Ÿæˆä¸å½’å› æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14295v4" data-paper-url="./papers/250914295v4-aegis-automated-error-generation-and-attribution-for-multi-agent-sys.html" onclick="toggleFavorite(this, '2509.14295v4', 'Aegis: Automated Error Generation and Attribution for Multi-Agent Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>32</td>
  <td><a href="./papers/250914117v3-geoaware-vla-implicit-geometry-aware-vision-language-action-model.html">GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model</a></td>
  <td>GeoAware-VLAï¼šåˆ©ç”¨å‡ ä½•å…ˆéªŒæå‡VLAæ¨¡å‹è§†è§’æ³›åŒ–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14117v3" data-paper-url="./papers/250914117v3-geoaware-vla-implicit-geometry-aware-vision-language-action-model.html" onclick="toggleFavorite(this, '2509.14117v3', 'GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250914412v1-gestos-advanced-hand-gesture-interpretation-via-large-language-model.html">GestOS: Advanced Hand Gesture Interpretation via Large Language Models to control Any Type of Robot</a></td>
  <td>GestOSï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°æ‰‹åŠ¿å¯¹å¼‚æ„æœºå™¨äººå›¢é˜Ÿçš„é«˜çº§æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14412v1" data-paper-url="./papers/250914412v1-gestos-advanced-hand-gesture-interpretation-via-large-language-model.html" onclick="toggleFavorite(this, '2509.14412v1', 'GestOS: Advanced Hand Gesture Interpretation via Large Language Models to control Any Type of Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>34</td>
  <td><a href="./papers/250914147v2-stabletracker-learning-to-stably-track-target-via-differentiable-sim.html">StableTracker: Learning to Stably Track Target via Differentiable Simulation</a></td>
  <td>æå‡ºStableTrackerä»¥è§£å†³FPVç›®æ ‡è·Ÿè¸ªä¸­çš„ç¨³å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">differentiable simulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14147v2" data-paper-url="./papers/250914147v2-stabletracker-learning-to-stably-track-target-via-differentiable-sim.html" onclick="toggleFavorite(this, '2509.14147v2', 'StableTracker: Learning to Stably Track Target via Differentiable Simulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)