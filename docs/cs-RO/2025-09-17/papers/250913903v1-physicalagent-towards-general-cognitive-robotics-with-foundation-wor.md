---
layout: default
title: PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models
---

# PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13903" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13903v1</a>
  <a href="https://arxiv.org/pdf/2509.13903.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13903v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13903v1', 'PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Artem Lykov, Jeffrin Sam, Hung Khang Nguyen, Vladislav Kozlovskiy, Yara Mahmoud, Valerii Serpiva, Miguel Altamirano Cabrera, Mikhail Konenkov, Dzmitry Tsetserukou

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

**å¤‡æ³¨**: submitted to IEEE conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PhysicalAgentï¼šåŸºäºä¸–ç•Œæ¨¡å‹çš„é€šç”¨è®¤çŸ¥æœºå™¨äººæ¡†æ¶ï¼Œå®ç°è¿­ä»£æ¨ç†å’Œé—­ç¯æ‰§è¡Œã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `ä¸–ç•Œæ¨¡å‹` `è§†é¢‘ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `è¿­ä»£æ¨ç†` `é—­ç¯æ‰§è¡Œ` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•åœ¨æ³›åŒ–æ€§å’Œé²æ£’æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ç¯å¢ƒå’Œä»»åŠ¡ã€‚
2. PhysicalAgenté€šè¿‡ç”Ÿæˆè§†é¢‘æ¼”ç¤ºã€é—­ç¯æ‰§è¡Œå’Œè¿­ä»£é‡æ–°è§„åˆ’ï¼Œå®ç°äº†æ›´å¼ºçš„æ³›åŒ–å’Œå®¹é”™èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPhysicalAgentåœ¨å¤šç§æœºå™¨äººå¹³å°å’Œæ„ŸçŸ¥æ¨¡æ€ä¸‹ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒæˆåŠŸç‡é«˜è¾¾83%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPhysicalAgentçš„æœºå™¨äººæ“ä½œæ¡†æ¶ï¼Œå®ƒé›†æˆäº†è¿­ä»£æ¨ç†ã€åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†é¢‘ç”Ÿæˆå’Œé—­ç¯æ‰§è¡Œã€‚ç»™å®šæ–‡æœ¬æŒ‡ä»¤ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆå€™é€‰è½¨è¿¹çš„çŸ­è§†é¢‘æ¼”ç¤ºï¼Œåœ¨æœºå™¨äººä¸Šæ‰§è¡Œè¿™äº›è½¨è¿¹ï¼Œå¹¶æ ¹æ®å¤±è´¥æƒ…å†µè¿­ä»£åœ°é‡æ–°è§„åˆ’ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿä»æ‰§è¡Œé”™è¯¯ä¸­ç¨³å¥åœ°æ¢å¤ã€‚æˆ‘ä»¬åœ¨å¤šç§æ„ŸçŸ¥æ¨¡æ€ï¼ˆè‡ªæˆ‘ä¸­å¿ƒã€ç¬¬ä¸‰äººç§°å’Œæ¨¡æ‹Ÿï¼‰å’Œæœºå™¨äººå¹³å°ï¼ˆåŒè‡‚UR3ã€Unitree G1äººå½¢æœºå™¨äººã€æ¨¡æ‹ŸGR1ï¼‰ä¸Šè¯„ä¼°äº†PhysicalAgentï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„ç‰¹å®šä»»åŠ¡åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨äººç±»ç†Ÿæ‚‰çš„ä»»åŠ¡ä¸­æˆåŠŸç‡é«˜è¾¾83%ã€‚å®é™…è¯•éªŒè¡¨æ˜ï¼Œé¦–æ¬¡å°è¯•çš„æˆåŠŸç‡æœ‰é™ï¼ˆ20-30%ï¼‰ï¼Œä½†è¿­ä»£çº æ­£å°†æ‰€æœ‰å¹³å°çš„æ€»ä½“æˆåŠŸç‡æé«˜åˆ°80%ã€‚è¿™äº›ç»“æœçªå‡ºäº†åŸºäºè§†é¢‘çš„ç”Ÿæˆæ¨ç†åœ¨é€šç”¨æœºå™¨äººæ“ä½œä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒäº†è¿­ä»£æ‰§è¡Œå¯¹äºä»åˆå§‹å¤±è´¥ä¸­æ¢å¤çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºå¯æ‰©å±•ã€å¯é€‚åº”å’Œé²æ£’çš„æœºå™¨äººæ§åˆ¶é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›å·®ï¼Œéš¾ä»¥é€‚åº”æ–°çš„ç¯å¢ƒå’Œä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°çš„é”™è¯¯éš¾ä»¥çº æ­£ï¼Œå¯¼è‡´ä»»åŠ¡å¤±è´¥ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´é€šç”¨ã€æ›´é²æ£’çš„æœºå™¨äººæ“ä½œæ¡†æ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPhysicalAgentçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸–ç•Œæ¨¡å‹è¿›è¡Œè§†é¢‘ç”Ÿæˆï¼Œæ¨¡æ‹Ÿæœºå™¨äººæ‰§è¡Œè½¨è¿¹ï¼Œå¹¶ç»“åˆè¿­ä»£æ¨ç†å’Œé—­ç¯æ‰§è¡Œï¼Œä¸æ–­ä¼˜åŒ–è½¨è¿¹ï¼Œä»è€Œæé«˜ä»»åŠ¡æˆåŠŸç‡ã€‚é€šè¿‡è§†é¢‘ç”Ÿæˆï¼Œå¯ä»¥å­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„ç¯å¢ƒä¿¡æ¯å’ŒåŠ¨ä½œæ¨¡å¼ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚è¿­ä»£æ¨ç†å’Œé—­ç¯æ‰§è¡Œåˆ™å¯ä»¥åŠæ—¶çº æ­£é”™è¯¯ï¼Œæé«˜é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPhysicalAgentçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ–‡æœ¬æŒ‡ä»¤è¾“å…¥ï¼›2) åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†é¢‘ç”Ÿæˆå™¨ï¼Œç”Ÿæˆå€™é€‰è½¨è¿¹çš„è§†é¢‘æ¼”ç¤ºï¼›3) æœºå™¨äººæ‰§è¡Œå™¨ï¼Œæ‰§è¡Œç”Ÿæˆçš„è½¨è¿¹ï¼›4) çŠ¶æ€æ„ŸçŸ¥æ¨¡å—ï¼Œæ„ŸçŸ¥æœºå™¨äººå’Œç¯å¢ƒçš„çŠ¶æ€ï¼›5) è¿­ä»£æ¨ç†æ¨¡å—ï¼Œæ ¹æ®çŠ¶æ€æ„ŸçŸ¥ç»“æœå’Œä»»åŠ¡ç›®æ ‡ï¼Œé‡æ–°è§„åˆ’è½¨è¿¹ã€‚æ•´ä¸ªæµç¨‹æ˜¯ä¸€ä¸ªé—­ç¯ç³»ç»Ÿï¼Œä¸æ–­è¿­ä»£ä¼˜åŒ–ï¼Œç›´åˆ°ä»»åŠ¡æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šPhysicalAgentçš„å…³é”®åˆ›æ–°åœ¨äºå°†è§†é¢‘ç”Ÿæˆã€è¿­ä»£æ¨ç†å’Œé—­ç¯æ‰§è¡Œé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ã€‚è§†é¢‘ç”Ÿæˆæ¨¡å—åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å€™é€‰è½¨è¿¹ï¼Œä¸ºåç»­çš„æ¨ç†å’Œæ‰§è¡Œæä¾›åŸºç¡€ã€‚è¿­ä»£æ¨ç†æ¨¡å—åˆ™å¯ä»¥æ ¹æ®å®é™…æ‰§è¡Œæƒ…å†µï¼Œä¸æ–­ä¼˜åŒ–è½¨è¿¹ï¼Œæé«˜ä»»åŠ¡æˆåŠŸç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè§†é¢‘ç”Ÿæˆæ¨¡å—é‡‡ç”¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å­¦ä¹ å¤§é‡çš„æœºå™¨äººæ“ä½œè§†é¢‘æ•°æ®ï¼Œç”Ÿæˆé€¼çœŸçš„å€™é€‰è½¨è¿¹ã€‚è¿­ä»£æ¨ç†æ¨¡å—é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ ¹æ®çŠ¶æ€æ„ŸçŸ¥ç»“æœå’Œä»»åŠ¡ç›®æ ‡ï¼Œå­¦ä¹ æœ€ä¼˜çš„è½¨è¿¹è§„åˆ’ç­–ç•¥ã€‚é—­ç¯æ‰§è¡Œæ¨¡å—åˆ™é‡‡ç”¨PIDæ§åˆ¶ç®—æ³•ï¼Œå®ç°ç²¾ç¡®çš„æœºå™¨äººè¿åŠ¨æ§åˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PhysicalAgentåœ¨å¤šä¸ªæœºå™¨äººå¹³å°å’Œæ„ŸçŸ¥æ¨¡æ€ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨äººç±»ç†Ÿæ‚‰çš„ä»»åŠ¡ä¸­æˆåŠŸç‡é«˜è¾¾83%ã€‚å®é™…è¯•éªŒè¡¨æ˜ï¼Œé¦–æ¬¡å°è¯•çš„æˆåŠŸç‡æœ‰é™ï¼ˆ20-30%ï¼‰ï¼Œä½†è¿­ä»£çº æ­£å°†æ‰€æœ‰å¹³å°çš„æ€»ä½“æˆåŠŸç‡æé«˜åˆ°80%ã€‚è¿™äº›ç»“æœéªŒè¯äº†PhysicalAgentçš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PhysicalAgentå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡ã€åŒ»ç–—åº·å¤ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨å·¥ä¸šè‡ªåŠ¨åŒ–ä¸­ï¼Œå¯ä»¥åˆ©ç”¨PhysicalAgentæ§åˆ¶æœºå™¨äººå®Œæˆå¤æ‚çš„è£…é…ä»»åŠ¡ï¼›åœ¨å®¶åº­æœåŠ¡ä¸­ï¼Œå¯ä»¥åˆ©ç”¨PhysicalAgentæ§åˆ¶æœºå™¨äººå®Œæˆå®¶åŠ¡åŠ³åŠ¨ï¼›åœ¨åŒ»ç–—åº·å¤ä¸­ï¼Œå¯ä»¥åˆ©ç”¨PhysicalAgentè¾…åŠ©æ‚£è€…è¿›è¡Œåº·å¤è®­ç»ƒã€‚è¯¥ç ”ç©¶ä¸ºé€šç”¨æœºå™¨äººæ“ä½œç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce PhysicalAgent, an agentic framework for robotic manipulation that integrates iterative reasoning, diffusion-based video generation, and closed-loop execution. Given a textual instruction, our method generates short video demonstrations of candidate trajectories, executes them on the robot, and iteratively re-plans in response to failures. This approach enables robust recovery from execution errors. We evaluate PhysicalAgent across multiple perceptual modalities (egocentric, third-person, and simulated) and robotic embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing against state-of-the-art task-specific baselines. Experiments demonstrate that our method consistently outperforms prior approaches, achieving up to 83% success on human-familiar tasks. Physical trials reveal that first-attempt success is limited (20-30%), yet iterative correction increases overall success to 80% across platforms. These results highlight the potential of video-based generative reasoning for general-purpose robotic manipulation and underscore the importance of iterative execution for recovering from initial failures. Our framework paves the way for scalable, adaptable, and robust robot control.

