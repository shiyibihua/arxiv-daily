---
layout: default
title: HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion
---

# HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13692" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13692v1</a>
  <a href="https://arxiv.org/pdf/2509.13692.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13692v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13692v1', 'HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yadan Zeng, Jiadong Zhou, Xiaohan Li, I-Ming Chen

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

**å¤‡æ³¨**: 9 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HGACNetï¼šç”¨äºè·¨æ¨¡æ€ç‚¹äº‘è¡¥å…¨çš„åˆ†å±‚å›¾æ³¨æ„åŠ›ç½‘ç»œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `ç‚¹äº‘è¡¥å…¨` `è·¨æ¨¡æ€èåˆ` `å›¾æ³¨æ„åŠ›ç½‘ç»œ` `æœºå™¨äººæ„ŸçŸ¥` `ä¸‰ç»´é‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†å› é®æŒ¡å’Œä¼ æ„Ÿå™¨é™åˆ¶å¯¼è‡´çš„ç‚¹äº‘ä¸å®Œæ•´é—®é¢˜ï¼Œå½±å“ä¸‹æ¸¸ä»»åŠ¡ã€‚
2. HGACNeté€šè¿‡åˆ†å±‚å›¾æ³¨æ„åŠ›ç¼–ç å‡ ä½•ç‰¹å¾ï¼Œå¹¶èåˆå›¾åƒå¼•å¯¼å…ˆéªŒï¼Œå®ç°æ›´ç²¾ç¡®çš„ç‚¹äº‘è¡¥å…¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHGACNetåœ¨ShapeNet-ViPCå’ŒYCB-Completeæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶é€‚ç”¨äºå®é™…æœºå™¨äººæ“ä½œã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç‚¹äº‘è¡¥å…¨å¯¹äºæœºå™¨äººæ„ŸçŸ¥ã€ç‰©ä½“é‡å»ºä»¥åŠæ”¯æŒæŠ“å–è§„åˆ’ã€é¿éšœå’Œæ“ä½œç­‰ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±è‡ªé®æŒ¡å’Œä¼ æ„Ÿå™¨é™åˆ¶å¯¼è‡´çš„ä¸å®Œæ•´å‡ ä½•å½¢çŠ¶ä¼šæ˜¾è‘—é™ä½ä¸‹æ¸¸æ¨ç†å’Œäº¤äº’çš„æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†HGACNetï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚ç¼–ç 3Då‡ ä½•ç‰¹å¾å¹¶å°†å…¶ä¸æ¥è‡ªå•è§†è§’RGBå›¾åƒçš„å›¾åƒå¼•å¯¼å…ˆéªŒèåˆï¼Œæ¥é‡å»ºå•ä¸ªç‰©ä½“çš„å®Œæ•´ç‚¹äº‘ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åˆ†å±‚å›¾æ³¨æ„åŠ›ï¼ˆHGAï¼‰ç¼–ç å™¨ï¼Œå®ƒé€šè¿‡åŸºäºå›¾æ³¨æ„åŠ›çš„ä¸‹é‡‡æ ·è‡ªé€‚åº”åœ°é€‰æ‹©å…³é”®å±€éƒ¨ç‚¹ï¼Œå¹¶é€æ­¥ç»†åŒ–åˆ†å±‚å‡ ä½•ç‰¹å¾ï¼Œä»¥æ›´å¥½åœ°æ•è·ç»“æ„è¿ç»­æ€§å’Œç©ºé—´å…³ç³»ã€‚ä¸ºäº†åŠ å¼ºè·¨æ¨¡æ€äº¤äº’ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦è·¨æ¨¡æ€èåˆï¼ˆMSCFï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨åˆ†å±‚å‡ ä½•ç‰¹å¾å’Œç»“æ„åŒ–è§†è§‰è¡¨ç¤ºä¹‹é—´æ‰§è¡ŒåŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾å¯¹é½ï¼Œä»è€Œä¸ºè¡¥å…¨æä¾›ç»†ç²’åº¦çš„è¯­ä¹‰æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹æ¯”æŸå¤±ï¼ˆC-Lossï¼‰æ¥æ˜¾å¼åœ°å¯¹é½è·¨æ¨¡æ€çš„ç‰¹å¾åˆ†å¸ƒï¼Œä»è€Œæé«˜æ¨¡æ€å·®å¼‚ä¸‹çš„è¡¥å…¨ä¿çœŸåº¦ã€‚æœ€åï¼Œåœ¨ShapeNet-ViPCåŸºå‡†å’ŒYCB-Completeæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¯å®äº†HGACNetçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶æœ€å…ˆè¿›çš„æ€§èƒ½ä»¥åŠåœ¨ç°å®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å¼ºå¤§é€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç”±äºè‡ªé®æŒ¡å’Œä¼ æ„Ÿå™¨é™åˆ¶å¯¼è‡´çš„ç‚¹äº‘æ•°æ®ä¸å®Œæ•´é—®é¢˜ã€‚ç°æœ‰ç‚¹äº‘è¡¥å…¨æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨å›¾åƒä¿¡æ¯ï¼Œå¹¶ä¸”åœ¨å¤„ç†å¤æ‚å‡ ä½•ç»“æ„æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´è¡¥å…¨ç»“æœçš„ç²¾åº¦å’Œå®Œæ•´æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å•è§†è§’RGBå›¾åƒä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼ŒæŒ‡å¯¼ç‚¹äº‘çš„è¡¥å…¨è¿‡ç¨‹ã€‚é€šè¿‡åˆ†å±‚å›¾æ³¨æ„åŠ›ç½‘ç»œæå–ç‚¹äº‘çš„å‡ ä½•ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å¤šå°ºåº¦è·¨æ¨¡æ€èåˆæ¨¡å—å°†å‡ ä½•ç‰¹å¾ä¸è§†è§‰ç‰¹å¾å¯¹é½ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®å’Œå®Œæ•´çš„ç‚¹äº‘è¡¥å…¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHGACNetçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åˆ†å±‚å›¾æ³¨æ„åŠ›ï¼ˆHGAï¼‰ç¼–ç å™¨ï¼šç”¨äºæå–ç‚¹äº‘çš„åˆ†å±‚å‡ ä½•ç‰¹å¾ã€‚2) å¤šå°ºåº¦è·¨æ¨¡æ€èåˆï¼ˆMSCFï¼‰æ¨¡å—ï¼šç”¨äºèåˆå‡ ä½•ç‰¹å¾å’Œè§†è§‰ç‰¹å¾ã€‚3) è§£ç å™¨ï¼šç”¨äºé‡å»ºå®Œæ•´çš„ç‚¹äº‘ã€‚4) å¯¹æ¯”æŸå¤±ï¼ˆC-Lossï¼‰ï¼šç”¨äºå¯¹é½è·¨æ¨¡æ€çš„ç‰¹å¾åˆ†å¸ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†åˆ†å±‚å›¾æ³¨æ„åŠ›ï¼ˆHGAï¼‰ç¼–ç å™¨ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°é€‰æ‹©å…³é”®å±€éƒ¨ç‚¹ï¼Œå¹¶é€æ­¥ç»†åŒ–åˆ†å±‚å‡ ä½•ç‰¹å¾ã€‚2) è®¾è®¡äº†å¤šå°ºåº¦è·¨æ¨¡æ€èåˆï¼ˆMSCFï¼‰æ¨¡å—ï¼Œèƒ½å¤Ÿæ‰§è¡ŒåŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾å¯¹é½ï¼Œä»è€Œä¸ºè¡¥å…¨æä¾›ç»†ç²’åº¦çš„è¯­ä¹‰æŒ‡å¯¼ã€‚3) æå‡ºäº†å¯¹æ¯”æŸå¤±ï¼ˆC-Lossï¼‰ï¼Œèƒ½å¤Ÿæ˜¾å¼åœ°å¯¹é½è·¨æ¨¡æ€çš„ç‰¹å¾åˆ†å¸ƒï¼Œä»è€Œæé«˜è¡¥å…¨ä¿çœŸåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šHGAç¼–ç å™¨ä½¿ç”¨å›¾æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œä¸‹é‡‡æ ·ï¼Œé€‰æ‹©å…³é”®ç‚¹å¹¶èšåˆå±€éƒ¨ä¿¡æ¯ã€‚MSCFæ¨¡å—é‡‡ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å¯¹é½ä¸åŒæ¨¡æ€çš„ç‰¹å¾ã€‚C-Lossé€šè¿‡æœ€å°åŒ–è·¨æ¨¡æ€ç‰¹å¾åˆ†å¸ƒçš„è·ç¦»ï¼Œæé«˜ç‰¹å¾çš„åˆ¤åˆ«æ€§ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ç‚¹äº‘é‡å»ºæŸå¤±å’Œå¯¹æ¯”æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HGACNetåœ¨ShapeNet-ViPCå’ŒYCB-Completeæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHGACNetèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¥å…¨ç‚¹äº‘ï¼Œå¹¶ä¸”åœ¨å¤„ç†å¤æ‚å‡ ä½•ç»“æ„æ—¶è¡¨ç°å‡ºè‰²ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒHGACNetåœ¨è¡¥å…¨ç²¾åº¦å’Œå®Œæ•´æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ„ŸçŸ¥ã€ç‰©ä½“é‡å»ºã€å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚åœ¨æœºå™¨äººé¢†åŸŸï¼Œå®ƒå¯ä»¥æé«˜æœºå™¨äººå¯¹ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæ”¯æŒæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚æŠ“å–ã€æ“ä½œå’Œå¯¼èˆªã€‚åœ¨AR/VRé¢†åŸŸï¼Œå®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ›´é€¼çœŸçš„3Dæ¨¡å‹ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Point cloud completion is essential for robotic perception, object reconstruction and supporting downstream tasks like grasp planning, obstacle avoidance, and manipulation. However, incomplete geometry caused by self-occlusion and sensor limitations can significantly degrade downstream reasoning and interaction. To address these challenges, we propose HGACNet, a novel framework that reconstructs complete point clouds of individual objects by hierarchically encoding 3D geometric features and fusing them with image-guided priors from a single-view RGB image. At the core of our approach, the Hierarchical Graph Attention (HGA) encoder adaptively selects critical local points through graph attention-based downsampling and progressively refines hierarchical geometric features to better capture structural continuity and spatial relationships. To strengthen cross-modal interaction, we further design a Multi-Scale Cross-Modal Fusion (MSCF) module that performs attention-based feature alignment between hierarchical geometric features and structured visual representations, enabling fine-grained semantic guidance for completion. In addition, we proposed the contrastive loss (C-Loss) to explicitly align the feature distributions across modalities, improving completion fidelity under modality discrepancy. Finally, extensive experiments conducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset confirm the effectiveness of HGACNet, demonstrating state-of-the-art performance as well as strong applicability in real-world robotic manipulation tasks.

