---
layout: default
title: CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping
---

# CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14143" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14143v1</a>
  <a href="https://arxiv.org/pdf/2509.14143.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14143v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14143v1', 'CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zijian An, Ran Yang, Yiming Feng, Lifeng Zhou

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

**å¤‡æ³¨**: 8 pages, 5 figures, 1 table

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CLAWï¼šä¸€ç§ç”¨äºé‡é‡æ„ŸçŸ¥æœºå™¨äººæŠ“å–çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæŠ“å–` `è§†è§‰è¯­è¨€åŠ¨ä½œ` `é‡é‡æ„ŸçŸ¥` `æ¡ä»¶æ§åˆ¶` `CLIPæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹éš¾ä»¥æ»¡è¶³ç²¾ç¡®çš„ä»»åŠ¡çº¦æŸï¼Œä¾‹å¦‚åŸºäºæ•°å€¼é˜ˆå€¼åœæ­¢ï¼Œå› ä¸ºç¼ºä¹æ˜¾å¼æ¡ä»¶ç›‘æ§æœºåˆ¶ã€‚
2. CLAWæ¡†æ¶è§£è€¦äº†æ¡ä»¶è¯„ä¼°å’ŒåŠ¨ä½œç”Ÿæˆï¼Œåˆ©ç”¨CLIPæ¨¡å‹ç”Ÿæˆæç¤ºï¼ŒæŒ‡å¯¼åŸºäºæµçš„VLAç­–ç•¥ç”ŸæˆåŠ¨ä½œã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCLAWåœ¨å•å¯¹è±¡æŠ“å–å’Œæ··åˆå¯¹è±¡ä»»åŠ¡ä¸­ï¼Œèƒ½å¯é æ‰§è¡Œé‡é‡æ„ŸçŸ¥è¡Œä¸ºï¼Œä¼˜äºåŸºçº¿æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹æœ€è¿‘ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æœºå™¨äººæ§åˆ¶èŒƒä¾‹å‡ºç°ï¼Œå®ƒèƒ½å¤Ÿå®ç°å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤èå…¥è§†è§‰è¿åŠ¨åŠ¨ä½œçš„ç«¯åˆ°ç«¯ç­–ç•¥ã€‚ç„¶è€Œï¼Œå½“å‰çš„VLAæ¨¡å‹é€šå¸¸éš¾ä»¥æ»¡è¶³ç²¾ç¡®çš„ä»»åŠ¡çº¦æŸï¼Œä¾‹å¦‚åŸºäºæ•°å€¼é˜ˆå€¼åœæ­¢ï¼Œå› ä¸ºå®ƒä»¬çš„è§‚å¯Ÿåˆ°åŠ¨ä½œçš„æ˜ å°„æ˜¯ç”±è®­ç»ƒæ•°æ®éšå¼å¡‘é€ çš„ï¼Œå¹¶ä¸”ç¼ºä¹ç”¨äºæ¡ä»¶ç›‘æ§çš„æ˜¾å¼æœºåˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CLAWï¼ˆCLIP-Language-Action for Weightï¼‰ï¼Œä¸€ä¸ªå°†æ¡ä»¶è¯„ä¼°ä¸åŠ¨ä½œç”Ÿæˆè§£è€¦çš„æ¡†æ¶ã€‚CLAWåˆ©ç”¨å¾®è°ƒçš„CLIPæ¨¡å‹ä½œä¸ºè½»é‡çº§çš„æç¤ºç”Ÿæˆå™¨ï¼Œå®ƒæŒç»­ç›‘æ§ç§¤çš„æ•°å­—è¯»æ•°ï¼Œå¹¶åŸºäºç‰¹å®šäºä»»åŠ¡çš„é‡é‡é˜ˆå€¼ç”Ÿæˆç¦»æ•£æŒ‡ä»¤ã€‚è¿™äº›æç¤ºéšåè¢«$Ï€_0$ï¼ˆä¸€ä¸ªåŸºäºæµçš„VLAç­–ç•¥ï¼‰ä½¿ç”¨ï¼Œè¯¥ç­–ç•¥å°†æç¤ºä¸å¤šè§†è§’ç›¸æœºè§‚å¯Ÿç»“æœé›†æˆï¼Œä»¥äº§ç”Ÿè¿ç»­çš„æœºå™¨äººåŠ¨ä½œã€‚è¿™ç§è®¾è®¡ä½¿CLAWèƒ½å¤Ÿå°†ç¬¦å·é‡é‡æ¨ç†ä¸é«˜é¢‘è§†è§‰è¿åŠ¨æ§åˆ¶ç›¸ç»“åˆã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå®éªŒè£…ç½®ä¸ŠéªŒè¯äº†CLAWï¼šå•å¯¹è±¡æŠ“å–å’Œéœ€è¦åŒè‡‚æ“ä½œçš„æ··åˆå¯¹è±¡ä»»åŠ¡ã€‚åœ¨æ‰€æœ‰æ¡ä»¶ä¸‹ï¼ŒCLAWéƒ½èƒ½å¯é åœ°æ‰§è¡Œé‡é‡æ„ŸçŸ¥è¡Œä¸ºï¼Œå¹¶ä¸”ä¼˜äºåŸå§‹$Ï€_0$å’Œå¾®è°ƒçš„$Ï€_0$æ¨¡å‹ã€‚æˆ‘ä»¬å·²å°†è§†é¢‘ä½œä¸ºè¡¥å……ææ–™ä¸Šä¼ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä¸­éš¾ä»¥æ»¡è¶³ç²¾ç¡®ä»»åŠ¡çº¦æŸçš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å½“ä»»åŠ¡éœ€è¦åŸºäºæ•°å€¼é˜ˆå€¼ï¼ˆä¾‹å¦‚é‡é‡ï¼‰è¿›è¡Œåœæ­¢æ—¶ã€‚ç°æœ‰VLAæ¨¡å‹çš„è§‚å¯Ÿåˆ°åŠ¨ä½œçš„æ˜ å°„æ˜¯éšå¼çš„ï¼Œç¼ºä¹æ˜¾å¼çš„æ¡ä»¶ç›‘æ§æœºåˆ¶ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®æ‰§è¡Œæ­¤ç±»ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¡ä»¶è¯„ä¼°ä¸åŠ¨ä½œç”Ÿæˆè§£è€¦ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„æç¤ºç”Ÿæˆå™¨ï¼ˆåŸºäºCLIPæ¨¡å‹ï¼‰æ¥æŒç»­ç›‘æ§ä»»åŠ¡ç›¸å…³çš„æ¡ä»¶ï¼ˆä¾‹å¦‚ç§¤çš„è¯»æ•°ï¼‰ï¼Œå¹¶ç”Ÿæˆç¦»æ•£çš„æŒ‡ä»¤ï¼ˆæç¤ºï¼‰ã€‚ç„¶åï¼Œè¿™äº›æç¤ºè¢«é›†æˆåˆ°VLAç­–ç•¥ä¸­ï¼Œä»¥æŒ‡å¯¼æœºå™¨äººåŠ¨ä½œçš„ç”Ÿæˆã€‚è¿™ç§è§£è€¦çš„è®¾è®¡å…è®¸ç³»ç»Ÿåˆ†åˆ«å¤„ç†ç¬¦å·æ¨ç†ï¼ˆæ¡ä»¶è¯„ä¼°ï¼‰å’Œé«˜é¢‘è§†è§‰è¿åŠ¨æ§åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLAWæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) å¾®è°ƒçš„CLIPæ¨¡å‹ï¼šä½œä¸ºè½»é‡çº§çš„æç¤ºç”Ÿæˆå™¨ï¼Œæ ¹æ®æ•°å­—ç§¤çš„è¯»æ•°å’Œé¢„å®šä¹‰çš„é‡é‡é˜ˆå€¼ç”Ÿæˆç¦»æ•£çš„æŒ‡ä»¤ã€‚2) åŸºäºæµçš„VLAç­–ç•¥($Ï€_0$)ï¼šå°†CLIPæ¨¡å‹ç”Ÿæˆçš„æç¤ºä¸å¤šè§†è§’ç›¸æœºè§‚å¯Ÿç»“æœç›¸ç»“åˆï¼Œç”Ÿæˆè¿ç»­çš„æœºå™¨äººåŠ¨ä½œã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šç›¸æœºæ•æ‰å›¾åƒ -> CLIPæ¨¡å‹æ ¹æ®å›¾åƒå’Œé‡é‡é˜ˆå€¼ç”Ÿæˆæç¤º -> VLAç­–ç•¥æ ¹æ®å›¾åƒå’Œæç¤ºç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†æ¡ä»¶è¯„ä¼°ä¸åŠ¨ä½œç”Ÿæˆè§£è€¦ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯VLAæ¨¡å‹ä¸åŒï¼ŒCLAWä½¿ç”¨ä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å—ï¼ˆCLIPæ¨¡å‹ï¼‰æ¥å¤„ç†æ¡ä»¶è¯„ä¼°ï¼Œå¹¶å°†è¯„ä¼°ç»“æœä»¥æç¤ºçš„å½¢å¼ä¼ é€’ç»™VLAç­–ç•¥ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†éœ€è¦ç²¾ç¡®æ¡ä»¶ç›‘æ§çš„ä»»åŠ¡ï¼Œå¹¶ä¸”å¯ä»¥æ›´å®¹æ˜“åœ°ä¿®æ”¹å’Œè°ƒæ•´æ¡ä»¶è¯„ä¼°çš„é€»è¾‘ã€‚

**å…³é”®è®¾è®¡**ï¼šCLIPæ¨¡å‹é€šè¿‡å¾®è°ƒæ¥é€‚åº”ç‰¹å®šçš„ä»»åŠ¡å’Œé‡é‡é˜ˆå€¼ã€‚VLAç­–ç•¥($Ï€_0$)é‡‡ç”¨åŸºäºæµçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆè¿ç»­çš„æœºå™¨äººåŠ¨ä½œã€‚æç¤ºçš„è®¾è®¡éœ€è¦ä»”ç»†è€ƒè™‘ï¼Œä»¥ç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆåœ°æŒ‡å¯¼VLAç­–ç•¥çš„åŠ¨ä½œç”Ÿæˆã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦å¹³è¡¡åŠ¨ä½œçš„å‡†ç¡®æ€§å’Œå¹³æ»‘æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCLAWåœ¨å•å¯¹è±¡æŠ“å–å’Œæ··åˆå¯¹è±¡ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¯é åœ°æ‰§è¡Œé‡é‡æ„ŸçŸ¥è¡Œä¸ºã€‚CLAWæ˜¾è‘—ä¼˜äºåŸå§‹çš„$Ï€_0$æ¨¡å‹å’Œå¾®è°ƒçš„$Ï€_0$æ¨¡å‹ï¼Œè¯æ˜äº†è§£è€¦æ¡ä»¶è¯„ä¼°å’ŒåŠ¨ä½œç”Ÿæˆç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®åœ¨è®ºæ–‡çš„å®éªŒéƒ¨åˆ†æœ‰è¯¦ç»†å±•ç¤ºï¼ŒåŒ…æ‹¬æˆåŠŸç‡ã€ç²¾åº¦ç­‰æŒ‡æ ‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CLAWæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨è‡ªåŠ¨åŒ–è£…é…ã€ç‰©æµåˆ†æ‹£ã€åŒ»ç–—æœºå™¨äººç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºéœ€è¦ç²¾ç¡®é‡é‡æ§åˆ¶çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è¯å“åˆ†è£…ã€é£Ÿå“é…æ–™ã€ç²¾å¯†ä»ªå™¨ç»„è£…ç­‰ã€‚é€šè¿‡ç»“åˆè§†è§‰ä¿¡æ¯å’Œè¯­è¨€æŒ‡ä»¤ï¼ŒCLAWå¯ä»¥å®ç°æ›´åŠ çµæ´»å’Œæ™ºèƒ½çš„æœºå™¨äººæ§åˆ¶ï¼Œæé«˜ç”Ÿäº§æ•ˆç‡å’Œäº§å“è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ¡ä»¶ç›‘æ§ï¼Œä¾‹å¦‚æ¸©åº¦ã€å‹åŠ›ã€ä½ç½®ç­‰ï¼Œä»è€Œå®ç°æ›´åŠ é€šç”¨çš„æœºå™¨äººæ§åˆ¶ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language-action (VLA) models have recently emerged as a promising paradigm for robotic control, enabling end-to-end policies that ground natural language instructions into visuomotor actions. However, current VLAs often struggle to satisfy precise task constraints, such as stopping based on numeric thresholds, since their observation-to-action mappings are implicitly shaped by training data and lack explicit mechanisms for condition monitoring. In this work, we propose CLAW (CLIP-Language-Action for Weight), a framework that decouples condition evaluation from action generation. CLAW leverages a fine-tuned CLIP model as a lightweight prompt generator, which continuously monitors the digital readout of a scale and produces discrete directives based on task-specific weight thresholds. These prompts are then consumed by $Ï€_0$, a flow-based VLA policy, which integrates the prompts with multi-view camera observations to produce continuous robot actions. This design enables CLAW to combine symbolic weight reasoning with high-frequency visuomotor control. We validate CLAW on three experimental setups: single-object grasping and mixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW reliably executes weight-aware behaviors and outperforms both raw-$Ï€_0$ and fine-tuned $Ï€_0$ models. We have uploaded the videos as supplementary materials.

