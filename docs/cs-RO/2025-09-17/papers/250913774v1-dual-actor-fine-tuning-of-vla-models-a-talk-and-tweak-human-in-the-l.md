---
layout: default
title: Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach
---

# Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13774" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13774v1</a>
  <a href="https://arxiv.org/pdf/2509.13774.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13774v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13774v1', 'Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Piaopiao Jin, Qi Wang, Guokang Sun, Ziwen Cai, Pinjia He, Yangwei You

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäººæœºåä½œçš„åŒæ¼”å‘˜å¾®è°ƒæ¡†æ¶ä»¥æå‡VLAæ¨¡å‹çš„ä»»åŠ¡è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `äººæœºåä½œ` `å¼ºåŒ–å­¦ä¹ ` `å¤šä»»åŠ¡å­¦ä¹ ` `æœºå™¨äººè®­ç»ƒ` `æ•°æ®ç”Ÿæˆ` `å®æ—¶åé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„VLAæ¨¡å‹åœ¨å¤æ‚çš„ç°å®ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®è´¨é‡å—é™çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•éš¾ä»¥æ»¡è¶³éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§äººæœºåä½œçš„åŒæ¼”å‘˜å¾®è°ƒæ¡†æ¶ï¼Œç»“åˆä¸»æ¼”å‘˜çš„å¤šä»»åŠ¡æ€§èƒ½ä¸ç²¾ç»†æ¼”å‘˜çš„æ½œåœ¨ç©ºé—´é€‚åº”èƒ½åŠ›ï¼Œåˆ©ç”¨äººç±»åé¦ˆç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰é¡¹ä»»åŠ¡ä¸­å®ç°äº†100%çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­ä¿æŒ50%çš„æˆåŠŸç‡ï¼Œä¸”åœ¨å¤šæœºå™¨äººè®­ç»ƒä¸­æ•ˆç‡æå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚çš„ç°å®ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„äººæœºåä½œåŒæ¼”å‘˜å¾®è°ƒæ¡†æ¶ï¼Œé›†æˆäº†ä¸€ä¸ªä¸»æ¼”å‘˜ç”¨äºå¤šä»»åŠ¡æ€§èƒ½çš„ç¨³å¥æ€§å’Œä¸€ä¸ªç²¾ç»†æ¼”å‘˜ç”¨äºæ½œåœ¨ç©ºé—´çš„é€‚åº”ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„â€œè°ˆè¯ä¸è°ƒæ•´â€æ–¹æ¡ˆï¼Œå°†äººç±»çš„ä¿®æ­£è½¬åŒ–ä¸ºè¯­ä¹‰åŒ–çš„è¯­è¨€æŒ‡ä»¤ï¼Œä»è€Œç”Ÿæˆæ–°çš„æ•°æ®é›†ç”¨äºç­–ç•¥å­¦ä¹ ã€‚åœ¨å®é™…çš„å¤šä»»åŠ¡å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨101åˆ†é’Ÿçš„åœ¨çº¿å¾®è°ƒå†…å®ç°äº†ä¸‰é¡¹ä»»åŠ¡çš„100%æˆåŠŸç‡ï¼Œå¹¶åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­ä¿æŒäº†12æ¬¡è¿ç»­æ“ä½œçš„50%æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨å¤šæœºå™¨äººè®­ç»ƒä¸­æœ‰æ•ˆæ‰©å±•ï¼Œä½¿ç”¨åŒæœºå™¨äººæ—¶æ•ˆç‡æå‡å¯è¾¾2å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³VLAæ¨¡å‹åœ¨å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®è´¨é‡ä¸é«˜çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•æ•ˆæœæœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§äººæœºåä½œçš„åŒæ¼”å‘˜å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡ä¸»æ¼”å‘˜å’Œç²¾ç»†æ¼”å‘˜çš„ååŒå·¥ä½œï¼Œåˆ©ç”¨äººç±»çš„å®æ—¶åé¦ˆæ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸»æ¼”å‘˜è´Ÿè´£æ‰§è¡Œå¤šä»»åŠ¡æ“ä½œï¼Œç²¾ç»†æ¼”å‘˜åˆ™è¿›è¡Œæ½œåœ¨ç©ºé—´çš„è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæ¡†æ¶ä¸­å¼•å…¥äº†â€œè°ˆè¯ä¸è°ƒæ•´â€æœºåˆ¶ï¼Œå°†äººç±»çš„ä¿®æ­£è½¬åŒ–ä¸ºè¯­è¨€æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†äººç±»åé¦ˆè½¬åŒ–ä¸ºè¯­ä¹‰åŒ–çš„è¯­è¨€æŒ‡ä»¤ï¼Œä»è€Œç”Ÿæˆæ–°çš„æ•°æ®é›†ç”¨äºç­–ç•¥å­¦ä¹ ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ˜¾è‘—ä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è½»é‡çº§çš„è¯­è¨€å¤„ç†æ¨¡å—ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†äººç±»åé¦ˆçš„æƒé‡ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹äººç±»æŒ‡ä»¤çš„é€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰é¡¹ä»»åŠ¡ä¸­å®ç°äº†100%çš„æˆåŠŸç‡ï¼Œä¸”åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­ä¿æŒ50%çš„æˆåŠŸç‡ã€‚ä½¿ç”¨åŒæœºå™¨äººè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ•ˆç‡æå‡å¯è¾¾2å€ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å¤šæœºå™¨äººåä½œä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿å’Œäººæœºåä½œç³»ç»Ÿã€‚é€šè¿‡æå‡VLAæ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å®é™…æ“ä½œä¸­æä¾›æ›´é«˜çš„çµæ´»æ€§å’Œæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language-action (VLA) models demonstrate strong generalization in robotic manipulation but face challenges in complex, real-world tasks. While supervised fine-tuning with demonstrations is constrained by data quality, reinforcement learning (RL) offers a promising alternative. We propose a human-in-the-loop dual-actor fine-tuning framework grounded in RL. The framework integrates a primary actor for robust multi-task performance with a refinement actor for latent-space adaptation. Beyond standard physical interventions, we introduce a lightweight talk-and-tweak scheme that converts human corrections into semantically grounded language commands, thereby generating a new dataset for policy learning. In real-world multi-task experiments, our approach achieves 100% success across three tasks within 101 minutes of online fine-tuning. For long-horizon tasks, it sustains a 50% success rate over 12 consecutive operations. Furthermore, the framework scales effectively to multi-robot training, achieving up to a 2 times improvement in efficiency when using dual robots. The experiment videos are available at https://sites.google.com/view/hil-daft/.

