---
layout: default
title: FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph
---

# FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13733" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13733v3</a>
  <a href="https://arxiv.org/pdf/2509.13733.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13733v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13733v3', 'FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaolin Zhou, Tingyang Xiao, Liu Liu, Yucheng Wang, Maiyue Chen, Xinrui Meng, Xinjie Wang, Wei Feng, Wei Sui, Zhizhong Su

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17 (æ›´æ–°: 2025-11-25)

**å¤‡æ³¨**: Demo video are available at https://horizonrobotics.github.io/robot_lab/fsr-vln/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFSR-VLNï¼Œç»“åˆåˆ†å±‚å¤šæ¨¡æ€åœºæ™¯å›¾ä¸å¿«æ…¢æ¨ç†ï¼Œæå‡è§†è§‰è¯­è¨€å¯¼èˆªæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `æœºå™¨äººå¯¼èˆª` `åˆ†å±‚åœºæ™¯å›¾` `å¿«æ…¢æ¨ç†` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€å¯¼èˆªæ–¹æ³•åœ¨é•¿è·ç¦»ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¯¼è‡´æˆåŠŸç‡ä½ä¸”æ¨ç†å»¶è¿Ÿé«˜ï¼Œå°¤å…¶æ˜¯åœ¨é•¿è·ç¦»å¯¼èˆªä»»åŠ¡ä¸­ã€‚
2. FSR-VLNç»“åˆåˆ†å±‚å¤šæ¨¡æ€åœºæ™¯å›¾(HMSG)ä¸å¿«æ…¢å¯¼èˆªæ¨ç†(FSR)ï¼Œå®ç°ä»ç²—åˆ°ç»†çš„æ¸è¿›å¼ç›®æ ‡æ£€ç´¢ä¸é€‰æ‹©ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFSR-VLNåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå“åº”æ—¶é—´æ˜¾è‘—é™ä½ï¼Œå¹¶æˆåŠŸé›†æˆåˆ°äººå½¢æœºå™¨äººå¹³å°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€å¯¼èˆª(VLN)æ˜¯æœºå™¨äººç³»ç»Ÿä¸­çš„ä¸€é¡¹åŸºç¡€æŒ‘æˆ˜ï¼Œåœ¨ç°å®ç¯å¢ƒä¸­éƒ¨ç½²å…·èº«æ™ºèƒ½ä½“å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨é•¿è·ç¦»ç©ºé—´æ¨ç†ä¸Šçš„å±€é™æ€§ï¼Œä»¥åŠæˆåŠŸç‡ä½ã€æ¨ç†å»¶è¿Ÿé«˜ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FSR-VLNï¼Œä¸€ä¸ªç»“åˆåˆ†å±‚å¤šæ¨¡æ€åœºæ™¯å›¾(HMSG)ä¸å¿«æ…¢å¯¼èˆªæ¨ç†(FSR)çš„è§†è§‰è¯­è¨€å¯¼èˆªç³»ç»Ÿã€‚HMSGæä¾›äº†ä¸€ç§å¤šæ¨¡æ€åœ°å›¾è¡¨ç¤ºï¼Œæ”¯æŒä»ç²—ç²’åº¦çš„æˆ¿é—´çº§å®šä½åˆ°ç»†ç²’åº¦çš„ç›®æ ‡è§†å›¾å’Œç‰©ä½“è¯†åˆ«çš„æ¸è¿›å¼æ£€ç´¢ã€‚åŸºäºHMSGï¼ŒFSRé¦–å…ˆæ‰§è¡Œå¿«é€ŸåŒ¹é…ä»¥é«˜æ•ˆåœ°é€‰æ‹©å€™é€‰æˆ¿é—´ã€è§†å›¾å’Œç‰©ä½“ï¼Œç„¶ååº”ç”¨VLMé©±åŠ¨çš„ç»†åŒ–æ¥è¿›è¡Œæœ€ç»ˆç›®æ ‡é€‰æ‹©ã€‚åœ¨ç”±äººå½¢æœºå™¨äººæ”¶é›†çš„å››ä¸ªç»¼åˆå®¤å†…æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†FSR-VLNï¼Œè¿™äº›æ•°æ®é›†åŒ…å«87æ¡æ¶µç›–å„ç§ç‰©ä½“ç±»åˆ«çš„æŒ‡ä»¤ã€‚FSR-VLNåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½å®ç°äº†æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½ï¼Œé€šè¿‡æ£€ç´¢æˆåŠŸç‡(RSR)è¡¡é‡ï¼Œå¹¶ä¸”é€šè¿‡ä»…åœ¨å¿«é€Ÿç›´è§‰å¤±è´¥æ—¶æ¿€æ´»æ…¢é€Ÿæ¨ç†ï¼Œä¸åŸºäºVLMçš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å·¡è§†è§†é¢‘ä¸Šçš„å“åº”æ—¶é—´å‡å°‘äº†82%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†FSR-VLNä¸Unitree-G1äººå½¢æœºå™¨äººä¸Šçš„è¯­éŸ³äº¤äº’ã€è§„åˆ’å’Œæ§åˆ¶æ¨¡å—é›†æˆï¼Œä»è€Œå®ç°è‡ªç„¶è¯­è¨€äº¤äº’å’Œå®æ—¶å¯¼èˆªã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€å¯¼èˆªæ–¹æ³•åœ¨å¤„ç†é•¿è·ç¦»ã€å¤æ‚æŒ‡ä»¤æ—¶ï¼Œç”±äºç¼ºä¹æœ‰æ•ˆçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´å¯¼èˆªæˆåŠŸç‡ä½ï¼Œä¸”è®¡ç®—å¤æ‚åº¦é«˜ï¼Œæ¨ç†é€Ÿåº¦æ…¢ã€‚å°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®å®šä½ç›®æ ‡ç‰©ä½“æˆ–ä½ç½®æ—¶ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œé‡‡ç”¨â€œå¿«æ…¢æ¨ç†â€æœºåˆ¶ã€‚é¦–å…ˆé€šè¿‡å¿«é€ŸåŒ¹é…è¿…é€Ÿç¼©å°æœç´¢èŒƒå›´ï¼Œç„¶ååˆ©ç”¨æ›´ç²¾ç»†çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œç²¾ç¡®åˆ¤æ–­ã€‚åŒæ—¶ï¼Œå¼•å…¥åˆ†å±‚å¤šæ¨¡æ€åœºæ™¯å›¾ï¼Œå°†ç¯å¢ƒä¿¡æ¯ç»„ç»‡æˆå¤šç²’åº¦çš„ç»“æ„ï¼Œæ–¹ä¾¿å¿«é€Ÿæ£€ç´¢å’Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFSR-VLNç³»ç»Ÿä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šåˆ†å±‚å¤šæ¨¡æ€åœºæ™¯å›¾(HMSG)å’Œå¿«æ…¢å¯¼èˆªæ¨ç†(FSR)ã€‚HMSGè´Ÿè´£æ„å»ºç¯å¢ƒçš„å¤šæ¨¡æ€åœ°å›¾è¡¨ç¤ºï¼ŒåŒ…æ‹¬æˆ¿é—´çº§ã€è§†å›¾çº§å’Œç‰©ä½“çº§çš„ä¿¡æ¯ã€‚FSRæ¨¡å—é¦–å…ˆåˆ©ç”¨å¿«é€ŸåŒ¹é…ç®—æ³•åœ¨HMSGä¸­ç­›é€‰å€™é€‰ç›®æ ‡ï¼Œç„¶åä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLM)å¯¹å€™é€‰ç›®æ ‡è¿›è¡Œç»†åŒ–å’Œæ’åºï¼Œæœ€ç»ˆé€‰æ‹©æœ€ä½³ç›®æ ‡ã€‚æ•´ä¸ªæµç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼šæŒ‡ä»¤è¾“å…¥ -> HMSGæ£€ç´¢ï¼ˆå¿«é€ŸåŒ¹é…ï¼‰-> VLMç»†åŒ–ï¼ˆæ…¢é€Ÿæ¨ç†ï¼‰-> å¯¼èˆªå†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ç»“åˆåˆ†å±‚åœºæ™¯å›¾å’Œå¿«æ…¢æ¨ç†çš„å¯¼èˆªæ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯æ–¹æ³•æˆ–å•ä¸€æ¨ç†æ¨¡å¼ç›¸æ¯”ï¼ŒFSR-VLNèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç¯å¢ƒä¿¡æ¯ï¼Œå¹¶åœ¨ä¿è¯å‡†ç¡®æ€§çš„å‰æä¸‹æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ã€‚åˆ†å±‚åœºæ™¯å›¾çš„è®¾è®¡ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿä»ç²—åˆ°ç»†åœ°è¿›è¡Œç›®æ ‡æ£€ç´¢ï¼Œè€Œå¿«æ…¢æ¨ç†æœºåˆ¶åˆ™å…è®¸ç³»ç»Ÿæ ¹æ®ä»»åŠ¡çš„å¤æ‚ç¨‹åº¦åŠ¨æ€è°ƒæ•´æ¨ç†ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šHMSGé‡‡ç”¨å¤šå±‚ç»“æ„ï¼Œæ¯ä¸€å±‚å¯¹åº”ä¸åŒçš„ç²’åº¦çº§åˆ«ï¼ˆæˆ¿é—´ã€è§†å›¾ã€ç‰©ä½“ï¼‰ã€‚æ¯ä¸€å±‚éƒ½åŒ…å«è§†è§‰ã€è¯­è¨€å’Œç©ºé—´ä¿¡æ¯ã€‚å¿«é€ŸåŒ¹é…ç®—æ³•åŸºäºç®€å•çš„ç›¸ä¼¼åº¦åº¦é‡ï¼Œä¾‹å¦‚ä½™å¼¦ç›¸ä¼¼åº¦æˆ–æ¬§æ°è·ç¦»ã€‚VLMç»†åŒ–é˜¶æ®µä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚CLIPæˆ–ALIGNï¼Œå¯¹å€™é€‰ç›®æ ‡è¿›è¡Œæ’åºã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨å¹³è¡¡å¯¼èˆªçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥é‡‡ç”¨äº¤å‰ç†µæŸå¤±æˆ–æ’åºæŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FSR-VLNåœ¨å››ä¸ªå®¤å†…æ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œæ£€ç´¢æˆåŠŸç‡(RSR)æ˜¾è‘—æå‡ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä¸åŸºäºVLMçš„æ–¹æ³•ç›¸æ¯”ï¼ŒFSR-VLNçš„å“åº”æ—¶é—´å‡å°‘äº†82%ï¼Œè¿™è¡¨æ˜å…¶å¿«æ…¢æ¨ç†æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œæå‡å¯¼èˆªæ•ˆç‡ã€‚åœ¨Unitree-G1äººå½¢æœºå™¨äººä¸Šçš„é›†æˆéªŒè¯äº†è¯¥æ–¹æ³•çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FSR-VLNåœ¨æœºå™¨äººå¯¼èˆªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€ä»“åº“ç‰©æµæœºå™¨äººã€å®‰é˜²å·¡é€»æœºå™¨äººç­‰ã€‚è¯¥æŠ€æœ¯å¯ä»¥å¸®åŠ©æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆåœ°å®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¯¹äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿå…·æœ‰é‡è¦çš„ç†è®ºä»·å€¼å’Œå®è·µæ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation.

