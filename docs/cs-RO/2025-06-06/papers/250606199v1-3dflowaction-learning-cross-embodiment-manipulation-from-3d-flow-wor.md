---
layout: default
title: 3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model
---

# 3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06199" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.06199v1</a>
  <a href="https://arxiv.org/pdf/2506.06199.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06199v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06199v1', '3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hongyan Zhi, Peihao Chen, Siyuan Zhou, Yubo Dong, Quanxi Wu, Lei Han, Mingkui Tan

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫3DFlowAction‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫ÊìçÊéßÊäÄËÉΩÂ≠¶‰π†ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫ÊìçÊéß` `3DÂÖâÊµÅ` `Ë∑®ÂÆû‰ΩìÈÄÇÂ∫î` `Âä®‰ΩúËßÑÂàí` `Êï∞ÊçÆÈõÜÂêàÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫ÊìçÊéßÊñπÊ≥ïÁº∫‰πèÁªü‰∏ÄÂíåÂº∫ÂÅ•ÁöÑÂä®‰ΩúË°®Á§∫ÔºåÈôêÂà∂‰∫ÜÂú®Â§öÊ†∑Âú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄöËøá3DÊµÅÂä®‰∏ñÁïåÊ®°ÂûãÂ≠¶‰π†ÊìçÊéßÊäÄËÉΩÔºåÂà©Áî®‰∫∫Á±ªÂíåÊú∫Âô®‰∫∫Êï∞ÊçÆÈ¢ÑÊµãÁâ©‰ΩìËøêÂä®„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®Â§öÊ†∑ÂåñÁöÑÊìçÊéß‰ªªÂä°‰∏≠ÂÖ∑ÊúâÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÂíåË∑®ÂÆû‰ΩìÈÄÇÂ∫îÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÊìçÊéß‰∏ÄÁõ¥ÊòØÊú∫Âô®‰∫∫Èù¢‰∏¥ÁöÑÊåëÊàòÔºåËÄå‰∫∫Á±ªËÉΩÂ§üËΩªÊùæ‰∏éÁâ©‰ΩìËøõË°åÂ§çÊùÇ‰∫§‰∫í„ÄÇÁé∞ÊúâÁöÑÊú∫Âô®‰∫∫Êï∞ÊçÆÈõÜÁº∫‰πèÁªü‰∏ÄÊÄßÔºåÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫Âú®Â§öÊ†∑Âú∫ÊôØ‰∏≠ÁöÑÊìçÊéßÊäÄËÉΩÂ≠¶‰π†„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç3DÊµÅÂä®‰∏ñÁïåÊ®°ÂûãÔºåÈÄöËøáÂàÜÊûê‰∫∫Á±ªÂíåÊú∫Âô®‰∫∫ÊìçÊéßÊï∞ÊçÆÔºåÈ¢ÑÊµãÁâ©‰ΩìÂú®3DÁ©∫Èó¥‰∏≠ÁöÑÊú™Êù•ËøêÂä®Ôºå‰ªéËÄåÊåáÂØºÊìçÊéßËßÑÂàí„ÄÇÊàë‰ª¨ÂêàÊàê‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ManiFlow-110kÁöÑÂ§ßËßÑÊ®°3DÂÖâÊµÅÊï∞ÊçÆÈõÜÔºåÂπ∂Âà©Áî®ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂ≠¶‰π†ÊìçÊéßÁâ©ÁêÜÔºåÁîüÊàêÂü∫‰∫éËØ≠Ë®ÄÊåá‰ª§ÁöÑ3DÂÖâÊµÅËΩ®Ëøπ„ÄÇÊúÄÁªàÔºåÁªìÂêà‰ºòÂåñÁ≠ñÁï•ÔºåÊú¨ÊñáÂÆûÁé∞‰∫ÜË∑®ÂÆû‰ΩìÈÄÇÂ∫îÁöÑÂº∫Â§ßÊìçÊéßËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫ÊìçÊéßÊäÄËÉΩÂ≠¶‰π†‰∏≠Áº∫‰πèÁªü‰∏ÄÊï∞ÊçÆÈõÜÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âú®ÁÆÄÂçïÂú∫ÊôØ‰∏≠ËÆ∞ÂΩï‰∏çÂêåÁöÑÊú∫Âô®‰∫∫Âä®‰ΩúÔºåÂØºËá¥Êú∫Âô®‰∫∫Êó†Ê≥ïÂ≠¶‰π†Âà∞Ë∑®Âú∫ÊôØÁöÑÊìçÊéßÊäÄËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáÊûÑÂª∫3DÊµÅÂä®‰∏ñÁïåÊ®°ÂûãÔºåÁêÜËß£Áâ©‰ΩìÂú®3DÁ©∫Èó¥‰∏≠ÁöÑËøêÂä®Ôºå‰ªéËÄåÊåáÂØºÊú∫Âô®‰∫∫ËøõË°åÊìçÊéß„ÄÇËØ•Ê®°ÂûãÊòØ‰∏éÂÖ∑‰ΩìÂÆû‰ΩìÊó†ÂÖ≥ÁöÑÔºåÈÄÇÁî®‰∫é‰∏çÂêåÁöÑÊú∫Âô®‰∫∫Âíå‰∫∫Á±ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈõÜÁöÑÂêàÊàê„ÄÅ3DÂÖâÊµÅÊ®°ÂûãÁöÑÂ≠¶‰π†ÂíåÂä®‰ΩúËßÑÂàí„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÁßªÂä®Áâ©‰ΩìËá™Âä®Ê£ÄÊµãÁÆ°ÈÅìÂêàÊàêManiFlow-110kÊï∞ÊçÆÈõÜÔºõÁÑ∂ÂêéÔºåÂà©Áî®ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂ≠¶‰π†ÊìçÊéßÁâ©ÁêÜÔºõÊúÄÂêéÔºåÁªìÂêàÁîüÊàêÁöÑ3DÂÖâÊµÅËøõË°åÂä®‰ΩúËßÑÂàí„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÊµÅÂºïÂØºÊ∏≤ÊüìÊú∫Âà∂ÔºåËÉΩÂ§üÊ†πÊçÆÁîüÊàêÁöÑ3DÂÖâÊµÅÂíåËØ≠Ë®ÄÊåá‰ª§ËØÑ‰º∞Âä®‰ΩúÁöÑÂêàÁêÜÊÄß„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÂæóÊú∫Âô®‰∫∫ÂÖ∑Â§áÈó≠ÁéØËßÑÂàíËÉΩÂäõÔºåÊòæËëóÊèêÂçá‰∫ÜÊìçÊéßÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊäÄÊúØÁªÜËäÇ‰∏äÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÂÖâÊµÅÈ¢ÑÊµãÔºåÂπ∂ËÆæËÆ°‰∫ÜÈÄÇÂ∫îÊÄßÂº∫ÁöÑÁΩëÁªúÁªìÊûÑÔºå‰ª•Á°Æ‰øùÊ®°ÂûãÂú®Â§öÊ†∑ÂåñÂú∫ÊôØ‰∏≠ÁöÑË°®Áé∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêÊñπÊ≥ïÂú®Â§öÁßçÊú∫Âô®‰∫∫ÊìçÊéß‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊ≥õÂåñËÉΩÂäõÂº∫ÔºåË∑®ÂÆû‰ΩìÈÄÇÂ∫îÊÄßËâØÂ•Ω„ÄÇ‰∏éÂü∫Á∫øÊñπÊ≥ïÁõ∏ÊØîÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåÈ™åËØÅ‰∫ÜÊ®°ÂûãÁöÑÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂÆ∂Â±Ö„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÂíåÊúçÂä°Êú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊìçÊéßËÉΩÂäõÔºåËÉΩÂ§üÂÆûÁé∞Êõ¥È´òÊïàÁöÑÁâ©‰ΩìÂ§ÑÁêÜÂíå‰∫∫Êú∫Âçè‰ΩúÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÁöÑÂπøÊ≥õÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Manipulation has long been a challenging task for robots, while humans can effortlessly perform complex interactions with objects, such as hanging a cup on the mug rack. A key reason is the lack of a large and uniform dataset for teaching robots manipulation skills. Current robot datasets often record robot action in different action spaces within a simple scene. This hinders the robot to learn a unified and robust action representation for different robots within diverse scenes. Observing how humans understand a manipulation task, we find that understanding how the objects should move in the 3D space is a critical clue for guiding actions. This clue is embodiment-agnostic and suitable for both humans and different robots. Motivated by this, we aim to learn a 3D flow world model from both human and robot manipulation data. This model predicts the future movement of the interacting objects in 3D space, guiding action planning for manipulation. Specifically, we synthesize a large-scale 3D optical flow dataset, named ManiFlow-110k, through a moving object auto-detect pipeline. A video diffusion-based world model then learns manipulation physics from these data, generating 3D optical flow trajectories conditioned on language instructions. With the generated 3D object optical flow, we propose a flow-guided rendering mechanism, which renders the predicted final state and leverages GPT-4o to assess whether the predicted flow aligns with the task description. This equips the robot with a closed-loop planning ability. Finally, we consider the predicted 3D optical flow as constraints for an optimization policy to determine a chunk of robot actions for manipulation. Extensive experiments demonstrate strong generalization across diverse robotic manipulation tasks and reliable cross-embodiment adaptation without hardware-specific training.

