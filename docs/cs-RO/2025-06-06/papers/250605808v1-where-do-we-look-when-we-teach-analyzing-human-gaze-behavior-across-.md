---
layout: default
title: Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning
---

# Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05808" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05808v1</a>
  <a href="https://arxiv.org/pdf/2506.05808.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05808v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05808v1', 'Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yutaro Ishida, Takamitsu Matsubara, Takayuki Kanai, Kazuhiro Shintani, Hiroshi Bito

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®éªŒæ¡†æ¶åˆ†ææ•™å­¦ä¸­äººç±»æ³¨è§†è¡Œä¸ºä»¥æå‡æ¨¡ä»¿å­¦ä¹ æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `äººç±»æ³¨è§†è¡Œä¸º` `æœºå™¨äººå­¦ä¹ ` `ç¤ºèŒƒè®¾å¤‡` `ä»»åŠ¡æˆåŠŸç‡` `æ•°æ®æ”¶é›†` `è®¤çŸ¥æŠ€èƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¾èµ–å¤§é‡ç¤ºèŒƒæ•°æ®ï¼Œå¯¼è‡´æˆæœ¬é«˜ä¸”æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºä¸€ä¸ªå®éªŒæ¡†æ¶ï¼Œç³»ç»Ÿåˆ†æä¸åŒç¤ºèŒƒè®¾å¤‡ä¸‹çš„ç¤ºèŒƒè€…æ³¨è§†è¡Œä¸ºï¼Œä»¥æå–ä»»åŠ¡ç›¸å…³çº¿ç´¢ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è‡ªç„¶è¡Œä¸ºæ•æ‰è®¾å¤‡çš„æ•°æ®æ˜¾è‘—æé«˜äº†ä»»åŠ¡æˆåŠŸç‡ï¼Œæå‡å¹…åº¦è¾¾åˆ°50%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ åœ¨è·å–å¯æ³›åŒ–ç­–ç•¥æ—¶é€šå¸¸éœ€è¦å¤§é‡ç¤ºèŒƒæ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºé€šè¿‡åˆ†æäººç±»ç¤ºèŒƒè€…çš„æ³¨è§†è¡Œä¸ºæ¥æå–ä»»åŠ¡ç›¸å…³çº¿ç´¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡æ‹Ÿæœºå™¨äººä½“ç°æˆ–è§†è§‰æ¡ä»¶çš„è®¾å¤‡ä¼šå‰Šå¼±ç¤ºèŒƒè€…æå–ä»»åŠ¡çº¿ç´¢çš„èƒ½åŠ›ï¼Œä¸”è¿™ç§å‰Šå¼±ç¨‹åº¦ä¸æ¨¡æ‹Ÿçš„ç¨‹åº¦æœ‰å…³ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ•æ‰è‡ªç„¶äººç±»è¡Œä¸ºçš„è®¾å¤‡æ”¶é›†çš„æ³¨è§†æ•°æ®ï¼Œèƒ½å°†ç­–ç•¥çš„ä»»åŠ¡æˆåŠŸç‡ä»18.8%æå‡è‡³68.8%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨¡ä»¿å­¦ä¹ ä¸­ç¤ºèŒƒæ•°æ®æ”¶é›†çš„é«˜æˆæœ¬é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æå–ä»»åŠ¡ç›¸å…³çº¿ç´¢æ—¶é¢ä¸´è®¾å¤‡å½±å“æ³¨è§†è¡Œä¸ºçš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®¾è®¡å®éªŒæ¡†æ¶ï¼Œç³»ç»Ÿåˆ†æä¸åŒç¤ºèŒƒè®¾å¤‡å¯¹ç¤ºèŒƒè€…æ³¨è§†è¡Œä¸ºçš„å½±å“ï¼Œä»è€Œä¼˜åŒ–æ•°æ®æ”¶é›†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åŒ…æ‹¬å¤šä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œé€‰æ‹©ä¸åŒçš„ç¤ºèŒƒè®¾å¤‡ï¼›å…¶æ¬¡ï¼Œè®°å½•ç¤ºèŒƒè€…çš„æ³¨è§†è¡Œä¸ºï¼›æœ€åï¼Œåˆ†ææ³¨è§†æ•°æ®ä¸ä»»åŠ¡æˆåŠŸç‡ä¹‹é—´çš„å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°æ¢è®¨äº†è®¾å¤‡æ¨¡æ‹Ÿå¯¹äººç±»æ³¨è§†è¡Œä¸ºçš„å½±å“ï¼Œæ­ç¤ºäº†è®¾å¤‡è®¾è®¡ä¸å­¦ä¹ æ•ˆæœä¹‹é—´çš„å…³è”ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­é‡‡ç”¨äº†å¤šç§è®¾å¤‡ï¼Œè®¾ç½®äº†ä¸åŒçš„è§†è§‰æ¡ä»¶ï¼Œå¹¶ä½¿ç”¨ç»Ÿè®¡åˆ†ææ–¹æ³•è¯„ä¼°æ³¨è§†è¡Œä¸ºä¸ä»»åŠ¡æˆåŠŸç‡çš„å…³ç³»ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒå®Œæ•´è®ºæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è‡ªç„¶è¡Œä¸ºæ•æ‰è®¾å¤‡æ”¶é›†çš„æ³¨è§†æ•°æ®ï¼Œä»»åŠ¡æˆåŠŸç‡ä»18.8%æ˜¾è‘—æå‡è‡³68.8%ï¼Œè¡¨æ˜è®¾å¤‡è®¾è®¡å¯¹æ¨¡ä»¿å­¦ä¹ çš„å½±å“æ˜¾è‘—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå­¦ä¹ ã€æ•™è‚²æŠ€æœ¯å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡ä¼˜åŒ–ç¤ºèŒƒæ•°æ®çš„æ”¶é›†æ–¹å¼ï¼Œå¯ä»¥åœ¨å¤šç§åœºæ™¯ä¸­æå‡æœºå™¨å­¦ä¹ çš„æ•ˆç‡å’Œæ•ˆæœï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation learning for acquiring generalizable policies often requires a large volume of demonstration data, making the process significantly costly. One promising strategy to address this challenge is to leverage the cognitive and decision-making skills of human demonstrators with strong generalization capability, particularly by extracting task-relevant cues from their gaze behavior. However, imitation learning typically involves humans collecting data using demonstration devices that emulate a robot's embodiment and visual condition. This raises the question of how such devices influence gaze behavior. We propose an experimental framework that systematically analyzes demonstrators' gaze behavior across a spectrum of demonstration devices. Our experimental results indicate that devices emulating (1) a robot's embodiment or (2) visual condition impair demonstrators' capability to extract task-relevant cues via gaze behavior, with the extent of impairment depending on the degree of emulation. Additionally, gaze data collected using devices that capture natural human behavior improves the policy's task success rate from 18.8% to 68.8% under environmental shifts.

