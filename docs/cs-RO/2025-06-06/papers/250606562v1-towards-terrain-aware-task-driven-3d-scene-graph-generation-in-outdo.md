---
layout: default
title: Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor Environments
---

# Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06562" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06562v1</a>
  <a href="https://arxiv.org/pdf/2506.06562.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06562v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06562v1', 'Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chad R Samuelson, Timothy W McLain, Joshua G Mangelson

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: Presented at the 2025 IEEE ICRA Workshop on Field Robotics

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœ°å½¢æ„ŸçŸ¥ä»»åŠ¡é©±åŠ¨çš„æˆ·å¤–ä¸‰ç»´åœºæ™¯å›¾ç”Ÿæˆæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `ä¸‰ç»´åœºæ™¯å›¾` `æˆ·å¤–ç¯å¢ƒ` `æœºå™¨äººå†³ç­–` `è¯­ä¹‰ç†è§£` `ç‚¹äº‘ç”Ÿæˆ` `é«˜å±‚æ¬¡æ¨ç†` `è‡ªä¸»å¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸‰ç»´åœºæ™¯è¡¨ç¤ºæ–¹æ³•åœ¨é«˜å±‚æ¬¡æ¨ç†å’Œå†³ç­–æ”¯æŒæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æˆ·å¤–ç¯å¢ƒä¸­ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç”Ÿæˆä»»åŠ¡æ— å…³çš„åº¦é‡è¯­ä¹‰ç‚¹äº‘ï¼Œå¹¶å¯¹å®¤å†…3DSGç”ŸæˆæŠ€æœ¯è¿›è¡Œäº†é€‚åº”æ€§ä¿®æ”¹ï¼Œä»¥é€‚åº”æˆ·å¤–åœºæ™¯ã€‚
3. åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ·å¤–3DSGsçš„ç”Ÿæˆæ˜¯å¯è¡Œçš„ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåœ¨ä»·å€¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜æ°´å¹³çš„è‡ªä¸»æ“ä½œä¾èµ–äºæœºå™¨äººæ„å»ºè¶³å¤Ÿè¡¨è¾¾ç¯å¢ƒçš„æ¨¡å‹ã€‚ä¼ ç»Ÿçš„ä¸‰ç»´åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå¦‚ç‚¹äº‘å’Œå ç”¨ç½‘æ ¼ï¼Œè™½ç„¶æä¾›äº†è¯¦ç»†çš„å‡ ä½•ä¿¡æ¯ï¼Œä½†ç¼ºä¹é«˜å±‚æ¬¡æ¨ç†æ‰€éœ€çš„ç»“æ„åŒ–è¯­ä¹‰ç»„ç»‡ã€‚ä¸‰ç»´åœºæ™¯å›¾ï¼ˆ3DSGsï¼‰é€šè¿‡å°†å‡ ä½•ã€æ‹“æ‰‘å’Œè¯­ä¹‰å…³ç³»æ•´åˆåˆ°å¤šå±‚å›¾å½¢è¡¨ç¤ºä¸­ï¼Œè§£å†³äº†è¿™ä¸€å±€é™æ€§ã€‚æœ¬æ–‡æ¢è®¨äº†3DSGsåœ¨æˆ·å¤–ç¯å¢ƒä¸­çš„æ„å»ºå’Œåº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºå¤§å‹æˆ·å¤–åœºæ™¯çš„ä»»åŠ¡æ— å…³çš„åº¦é‡è¯­ä¹‰ç‚¹äº‘ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶å¯¹ç°æœ‰çš„å®¤å†…3DSGç”ŸæˆæŠ€æœ¯è¿›è¡Œäº†ä¿®æ”¹ï¼Œä»¥é€‚åº”æˆ·å¤–åº”ç”¨ã€‚åˆæ­¥çš„å®šæ€§ç»“æœå±•ç¤ºäº†æˆ·å¤–3DSGsçš„å¯è¡Œæ€§ï¼Œå¹¶å¼ºè°ƒäº†å…¶åœ¨å®é™…åœºåœ°æœºå™¨äººåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿä¸‰ç»´åœºæ™¯è¡¨ç¤ºåœ¨æˆ·å¤–ç¯å¢ƒä¸­ç¼ºä¹ç»“æ„åŒ–è¯­ä¹‰ç»„ç»‡çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºå®¤å†…åœºæ™¯ï¼Œæ— æ³•æœ‰æ•ˆæ”¯æŒæˆ·å¤–å¤æ‚ç¯å¢ƒçš„é«˜å±‚æ¬¡æ¨ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§æ–°çš„ç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆåº¦é‡å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œåˆ›å»ºé€‚ç”¨äºå¤§å‹æˆ·å¤–ç¯å¢ƒçš„ä¸‰ç»´åœºæ™¯å›¾ï¼Œå¢å¼ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€ç‚¹äº‘ç”Ÿæˆã€è¯­ä¹‰æ ‡æ³¨å’Œå›¾å½¢æ„å»ºå››ä¸ªä¸»è¦æ¨¡å—ï¼Œç¡®ä¿ç”Ÿæˆçš„3DSGèƒ½å¤Ÿåæ˜ ç¯å¢ƒçš„å‡ ä½•å’Œè¯­ä¹‰ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºå°†å®¤å†…3DSGç”ŸæˆæŠ€æœ¯è°ƒæ•´ä¸ºé€‚åº”æˆ·å¤–ç¯å¢ƒï¼Œé¦–æ¬¡å®ç°äº†åœ¨å¤§è§„æ¨¡æˆ·å¤–åœºæ™¯ä¸­ç”Ÿæˆé«˜æ•ˆçš„ä¸‰ç»´åœºæ™¯å›¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„ç‚¹äº‘å¯†åº¦å’Œè¯­ä¹‰æ ‡ç­¾ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯çš„å¹³è¡¡ï¼Œç¡®ä¿ç”Ÿæˆçš„3DSGå…·æœ‰è‰¯å¥½çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æˆ·å¤–3DSGç”Ÿæˆæ–¹æ³•åœ¨å¤šä¸ªåœºæ™¯ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœï¼Œåˆæ­¥å®šæ€§åˆ†ææ˜¾ç¤ºå…¶åœ¨ç¯å¢ƒç†è§£å’Œä»»åŠ¡æ‰§è¡Œä¸­çš„æœ‰æ•ˆæ€§ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å°šæœªå…¬å¼€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»å¯¼èˆªã€ç¯å¢ƒç›‘æµ‹å’Œç¾åè¯„ä¼°ç­‰ï¼Œèƒ½å¤Ÿä¸ºæœºå™¨äººåœ¨å¤æ‚æˆ·å¤–ç¯å¢ƒä¸­çš„ä»»åŠ¡æ‰§è¡Œæä¾›æ›´ä¸ºç²¾å‡†çš„ç¯å¢ƒç†è§£å’Œå†³ç­–æ”¯æŒã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„æˆç†Ÿï¼Œæˆ·å¤–3DSGsæœ‰æœ›åœ¨å®é™…åœºåœ°æœºå™¨äººåº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> High-level autonomous operations depend on a robot's ability to construct a sufficiently expressive model of its environment. Traditional three-dimensional (3D) scene representations, such as point clouds and occupancy grids, provide detailed geometric information but lack the structured, semantic organization needed for high-level reasoning. 3D scene graphs (3DSGs) address this limitation by integrating geometric, topological, and semantic relationships into a multi-level graph-based representation. By capturing hierarchical abstractions of objects and spatial layouts, 3DSGs enable robots to reason about environments in a structured manner, improving context-aware decision-making and adaptive planning. Although most recent work has focused on indoor 3DSGs, this paper investigates their construction and utility in outdoor environments. We present a method for generating a task-agnostic metric-semantic point cloud for large outdoor settings and propose modifications to existing indoor 3DSG generation techniques for outdoor applicability. Our preliminary qualitative results demonstrate the feasibility of outdoor 3DSGs and highlight their potential for future deployment in real-world field robotic applications.

