---
layout: default
title: Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation via End-to-End Reinforcement Learning
---

# Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation via End-to-End Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05997" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05997v2</a>
  <a href="https://arxiv.org/pdf/2506.05997.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05997v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05997v2', 'Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation via End-to-End Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fan Yang, Per Frivik, David Hoeller, Chen Wang, Cesar Cadena, Marco Hutter

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-09-04)

**å¤‡æ³¨**: 22 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç©ºé—´å¢å¼ºé€’å½’è®°å¿†ä»¥è§£å†³é•¿è·ç¦»æ— åœ°å›¾å¯¼èˆªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è·ç¦»å¯¼èˆª` `æ— åœ°å›¾å¯¼èˆª` `ç©ºé—´è®°å¿†` `é€’å½’ç¥ç»ç½‘ç»œ` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººå¯¼èˆª` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¼èˆªæ–¹æ³•åœ¨ç©ºé—´è®°å¿†èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆæ•´åˆæ¥è‡ªä¸åŒè§†è§’çš„åºåˆ—è§‚å¯Ÿã€‚
2. æœ¬æ–‡æå‡ºç©ºé—´å¢å¼ºé€’å½’å•å…ƒï¼ˆSRUsï¼‰ï¼Œé€šè¿‡æ”¹è¿›RNNç»“æ„æ¥å¢å¼ºç©ºé—´è®°å¿†èƒ½åŠ›ï¼Œå¹¶ç»“åˆæ³¨æ„åŠ›æœºåˆ¶å®ç°é•¿è·ç¦»æ— åœ°å›¾å¯¼èˆªã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨SRUè®°å¿†çš„æ–¹æ³•åœ¨é•¿è·ç¦»å¯¼èˆªä¸Šæ¯”ä¾èµ–æ˜¾å¼æ˜ å°„å’Œå †å å†å²è§‚å¯Ÿçš„åŸºçº¿åˆ†åˆ«æå‡29.6%å’Œ105.0%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæœºå™¨äººå¯¼èˆªé¢†åŸŸç‰¹åˆ«æ˜¯ç«¯åˆ°ç«¯å­¦ä¹ æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›å±•æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒæˆåŠŸå¯¼èˆªä»ä¾èµ–äºä¸¤ä¸ªå…³é”®èƒ½åŠ›ï¼šæ˜ å°„å’Œè§„åˆ’ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ˜¾å¼æ˜ å°„ç®¡é“å°†è‡ªæˆ‘ä¸­å¿ƒè§‚å¯Ÿæ³¨å†Œåˆ°ä¸€è‡´çš„åœ°å›¾ä¸­ï¼Œè€Œç«¯åˆ°ç«¯å­¦ä¹ åˆ™é€šè¿‡é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰éšå¼å®ç°ã€‚ç°æœ‰æ¶æ„å¦‚LSTMå’ŒGRUèƒ½å¤Ÿæ•æ‰æ—¶é—´ä¾èµ–æ€§ï¼Œä½†åœ¨ç©ºé—´è®°å¿†æ–¹é¢å­˜åœ¨å…³é”®é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºç©ºé—´å¢å¼ºé€’å½’å•å…ƒï¼ˆSRUsï¼‰ï¼Œå¯¹ç°æœ‰RNNè¿›è¡Œæœ‰æ•ˆæ”¹è¿›ï¼Œå¢å¼ºç©ºé—´è®°å¿†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰RNNç›¸æ¯”ï¼Œé•¿è·ç¦»å¯¼èˆªæ•´ä½“æå‡23.5%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰é€’å½’ç¥ç»ç½‘ç»œåœ¨é•¿è·ç¦»å¯¼èˆªä¸­ç©ºé—´è®°å¿†ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–æ˜¾å¼æ˜ å°„ï¼Œæ— æ³•æœ‰æ•ˆæ•´åˆä¸åŒè§†è§’çš„è§‚å¯Ÿæ•°æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç©ºé—´å¢å¼ºé€’å½’å•å…ƒï¼ˆSRUsï¼‰ï¼Œé€šè¿‡å¯¹RNNçš„ç®€å•æœ‰æ•ˆä¿®æ”¹ï¼Œå¢å¼ºå…¶ç©ºé—´è®°å¿†èƒ½åŠ›ï¼Œä»¥æ”¯æŒæ›´å¥½çš„è§„åˆ’å’Œå¯¼èˆªã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬SRUså’ŒåŸºäºæ³¨æ„åŠ›çš„ç½‘ç»œç»“æ„ï¼Œåˆ©ç”¨å•ä¸ªå‰å‘ç«‹ä½“ç›¸æœºè¿›è¡Œé•¿è·ç¦»å¯¼èˆªã€‚é€šè¿‡æ­£åˆ™åŒ–æŠ€æœ¯ä¿ƒè¿›ç«¯åˆ°ç«¯çš„é€’å½’è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šSRUsæ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼Œæ˜¾è‘—æå‡äº†ç©ºé—´è®°å¿†èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿçš„LSTMå’ŒGRUæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ç©ºé—´ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸­ï¼ŒSRUsé€šè¿‡è°ƒæ•´è®°å¿†å•å…ƒçš„è®¾è®¡ï¼Œç»“åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¼˜åŒ–äº†å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„è®­ç»ƒå’Œå¯¼èˆªæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨SRUè®°å¿†çš„å¯¼èˆªæ–¹æ³•åœ¨é•¿è·ç¦»å¯¼èˆªä»»åŠ¡ä¸­æ•´ä½“æå‡23.5%ã€‚ä¸ä¾èµ–æ˜¾å¼æ˜ å°„å’Œå †å å†å²è§‚å¯Ÿçš„åŸºçº¿ç›¸æ¯”ï¼Œåˆ†åˆ«æå‡äº†29.6%å’Œ105.0%ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»æœºå™¨äººå¯¼èˆªã€æ— äººé©¾é©¶æ±½è½¦ä»¥åŠæ™ºèƒ½å®¶å±…ç­‰åœºæ™¯ã€‚é€šè¿‡å¢å¼ºç©ºé—´è®°å¿†èƒ½åŠ›ï¼Œæœºå™¨äººèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°è¿›è¡Œå¯¼èˆªå’Œå†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in robot navigation, particularly with end-to-end learning approaches such as reinforcement learning (RL), have demonstrated strong performance. However, successful navigation still depends on two key capabilities: mapping and planning (explicitly or implicitly). Classical approaches rely on explicit mapping pipelines to register egocentric observations into a coherent map. In contrast, end-to-end learning often achieves this implicitly -- through recurrent neural networks (RNNs) that fuse current and historical observations into a latent space for planning. While existing architectures, such as LSTM and GRU, can capture temporal dependencies, our findings reveal a critical limitation: their inability to effectively perform spatial memorization. This capability is essential for integrating sequential observations from varying perspectives to build spatial representations that support planning. To address this, we propose Spatially-Enhanced Recurrent Units (SRUs) -- a simple yet effective modification to existing RNNs -- that enhance spatial memorization. We further introduce an attention-based network architecture integrated with SRUs, enabling long-range mapless navigation using a single forward-facing stereo camera. We also employ regularization techniques to facilitate robust end-to-end recurrent training via RL. Experimental results show 23.5% overall improvement in long-range navigation compared to existing RNNs. With SRU memory, our method outperforms RL baselines -- one relying on explicit mapping and the other on stacked historical observations -- by 29.6% and 105.0%, respectively, across diverse environments requiring long-horizon mapping and memorization. Finally, we address the sim-to-real gap by leveraging large-scale pretraining on synthetic depth data, enabling zero-shot transfer for deployment across diverse and complex real-world environments.

