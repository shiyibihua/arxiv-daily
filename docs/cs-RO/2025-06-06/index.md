---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-06-06
---

# cs.ROï¼ˆ2025-06-06ï¼‰

ğŸ“Š å…± **16** ç¯‡è®ºæ–‡


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (8)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250606199v1-3dflowaction-learning-cross-embodiment-manipulation-from-3d-flow-wor.html">3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model</a></td>
  <td>æå‡º3DFlowActionä»¥è§£å†³æœºå™¨äººæ“æ§æŠ€èƒ½å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">world model</span> <span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06199v1" data-paper-url="./papers/250606199v1-3dflowaction-learning-cross-embodiment-manipulation-from-3d-flow-wor.html" onclick="toggleFavorite(this, '2506.06199v1', '3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250606221v2-biassemble-learning-collaborative-affordance-for-bimanual-geometric-.html">BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly</a></td>
  <td>æå‡ºBiAssembleä»¥è§£å†³åŒæ‰‹å‡ ä½•è£…é…ä¸­çš„åä½œèƒ½åŠ›é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">affordance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06221v2" data-paper-url="./papers/250606221v2-biassemble-learning-collaborative-affordance-for-bimanual-geometric-.html" onclick="toggleFavorite(this, '2506.06221v2', 'BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250605997v2-spatially-enhanced-recurrent-memory-for-long-range-mapless-navigatio.html">Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation via End-to-End Reinforcement Learning</a></td>
  <td>æå‡ºç©ºé—´å¢å¼ºé€’å½’è®°å¿†ä»¥è§£å†³é•¿è·ç¦»æ— åœ°å›¾å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05997v2" data-paper-url="./papers/250605997v2-spatially-enhanced-recurrent-memory-for-long-range-mapless-navigatio.html" onclick="toggleFavorite(this, '2506.05997v2', 'Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation via End-to-End Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250606535v3-maplegrasp-mask-guided-feature-pooling-for-language-driven-efficient.html">MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping</a></td>
  <td>æå‡ºMapleGraspä»¥è§£å†³è¯­è¨€é©±åŠ¨çš„æœºå™¨äººæŠ“å–æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06535v3" data-paper-url="./papers/250606535v3-maplegrasp-mask-guided-feature-pooling-for-language-driven-efficient.html" onclick="toggleFavorite(this, '2506.06535v3', 'MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250605714v1-advancement-and-field-evaluation-of-a-dual-arm-apple-harvesting-robo.html">Advancement and Field Evaluation of a Dual-arm Apple Harvesting Robot</a></td>
  <td>æå‡ºåŒè‡‚è‹¹æœé‡‡æ‘˜æœºå™¨äººä»¥è§£å†³äººå·¥é‡‡æ‘˜æ•ˆç‡ä½ä¸‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">dual-arm</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05714v1" data-paper-url="./papers/250605714v1-advancement-and-field-evaluation-of-a-dual-arm-apple-harvesting-robo.html" onclick="toggleFavorite(this, '2506.05714v1', 'Advancement and Field Evaluation of a Dual-arm Apple Harvesting Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250606196v1-bridging-perception-and-action-spatially-grounded-mid-level-represen.html">Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization</a></td>
  <td>æå‡ºç©ºé—´åŸºç¡€çš„ä¸­å±‚è¡¨ç¤ºä»¥æå‡æœºå™¨äººä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">bimanual manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06196v1" data-paper-url="./papers/250606196v1-bridging-perception-and-action-spatially-grounded-mid-level-represen.html" onclick="toggleFavorite(this, '2506.06196v1', 'Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250606567v1-nesypack-a-neuro-symbolic-framework-for-bimanual-logistics-packing.html">NeSyPack: A Neuro-Symbolic Framework for Bimanual Logistics Packing</a></td>
  <td>æå‡ºNeSyPackæ¡†æ¶ä»¥è§£å†³åŒæ‰‹ç‰©æµæ‰“åŒ…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">bi-manual</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06567v1" data-paper-url="./papers/250606567v1-nesypack-a-neuro-symbolic-framework-for-bimanual-logistics-packing.html" onclick="toggleFavorite(this, '2506.06567v1', 'NeSyPack: A Neuro-Symbolic Framework for Bimanual Logistics Packing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250606136v2-uav-ugv-cooperative-trajectory-optimization-and-task-allocation-for-.html">UAV-UGV Cooperative Trajectory Optimization and Task Allocation for Medical Rescue Tasks in Post-Disaster Environments</a></td>
  <td>æå‡ºæ— äººæœºä¸åœ°é¢æ— äººè½¦åä½œä¼˜åŒ–æ–¹æ¡ˆä»¥è§£å†³ç¾ååŒ»ç–—æ•‘æ´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">trajectory optimization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06136v2" data-paper-url="./papers/250606136v2-uav-ugv-cooperative-trajectory-optimization-and-task-allocation-for-.html" onclick="toggleFavorite(this, '2506.06136v2', 'UAV-UGV Cooperative Trajectory Optimization and Task Allocation for Medical Rescue Tasks in Post-Disaster Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/250606205v1-astra-toward-general-purpose-mobile-robots-via-hierarchical-multimod.html">Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal Learning</a></td>
  <td>æå‡ºAstraä»¥è§£å†³å¤æ‚å®¤å†…ç¯å¢ƒä¸‹ç§»åŠ¨æœºå™¨äººå¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06205v1" data-paper-url="./papers/250606205v1-astra-toward-general-purpose-mobile-robots-via-hierarchical-multimod.html" onclick="toggleFavorite(this, '2506.06205v1', 'Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250606077v1-self-driving-algorithm-for-an-active-four-wheel-drive-racecar.html">Self driving algorithm for an active four wheel drive racecar</a></td>
  <td>æå‡ºæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ä»¥ä¼˜åŒ–å››è½®é©±åŠ¨èµ›è½¦çš„è‡ªåŠ¨é©¾é©¶æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">DRL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06077v1" data-paper-url="./papers/250606077v1-self-driving-algorithm-for-an-active-four-wheel-drive-racecar.html" onclick="toggleFavorite(this, '2506.06077v1', 'Self driving algorithm for an active four wheel drive racecar')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250606072v3-beast-efficient-tokenization-of-b-splines-encoded-action-sequences-f.html">BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning</a></td>
  <td>æå‡ºBEASTä»¥é«˜æ•ˆç¼–ç Bæ ·æ¡åŠ¨ä½œåºåˆ—ç”¨äºæ¨¡ä»¿å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06072v3" data-paper-url="./papers/250606072v3-beast-efficient-tokenization-of-b-splines-encoded-action-sequences-f.html" onclick="toggleFavorite(this, '2506.06072v3', 'BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250605808v1-where-do-we-look-when-we-teach-analyzing-human-gaze-behavior-across-.html">Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning</a></td>
  <td>æå‡ºå®éªŒæ¡†æ¶åˆ†ææ•™å­¦ä¸­äººç±»æ³¨è§†è¡Œä¸ºä»¥æå‡æ¨¡ä»¿å­¦ä¹ æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05808v1" data-paper-url="./papers/250605808v1-where-do-we-look-when-we-teach-analyzing-human-gaze-behavior-across-.html" onclick="toggleFavorite(this, '2506.05808v1', 'Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250605896v1-object-navigation-with-structure-semantic-reasoning-based-multi-leve.html">Object Navigation with Structure-Semantic Reasoning-Based Multi-level Map and Multimodal Decision-Making LLM</a></td>
  <td>æå‡ºåŸºäºç»“æ„-è¯­ä¹‰æ¨ç†çš„å¤šå±‚æ¬¡åœ°å›¾ä¸å¤šæ¨¡æ€å†³ç­–çš„ä¸»åŠ¨ç‰©ä½“å¯¼èˆªæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05896v1" data-paper-url="./papers/250605896v1-object-navigation-with-structure-semantic-reasoning-based-multi-leve.html" onclick="toggleFavorite(this, '2506.05896v1', 'Object Navigation with Structure-Semantic Reasoning-Based Multi-level Map and Multimodal Decision-Making LLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250606570v2-from-perception-logs-to-failure-modes-language-driven-semantic-clust.html">From Perception Logs to Failure Modes: Language-Driven Semantic Clustering of Failures for Robot Safety</a></td>
  <td>æå‡ºä¸€ç§åŸºäºè¯­è¨€é©±åŠ¨çš„è¯­ä¹‰èšç±»æ–¹æ³•ä»¥æå‡æœºå™¨äººå®‰å…¨æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06570v2" data-paper-url="./papers/250606570v2-from-perception-logs-to-failure-modes-language-driven-semantic-clust.html" onclick="toggleFavorite(this, '2506.06570v2', 'From Perception Logs to Failure Modes: Language-Driven Semantic Clustering of Failures for Robot Safety')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250605648v1-a-modular-haptic-display-with-reconfigurable-signals-for-personalize.html">A Modular Haptic Display with Reconfigurable Signals for Personalized Information Transfer</a></td>
  <td>æå‡ºå¯é‡æ„è§¦è§‰æ˜¾ç¤ºç³»ç»Ÿä»¥å®ç°ä¸ªæ€§åŒ–ä¿¡æ¯ä¼ é€’</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05648v1" data-paper-url="./papers/250605648v1-a-modular-haptic-display-with-reconfigurable-signals-for-personalize.html" onclick="toggleFavorite(this, '2506.05648v1', 'A Modular Haptic Display with Reconfigurable Signals for Personalized Information Transfer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250606562v1-towards-terrain-aware-task-driven-3d-scene-graph-generation-in-outdo.html">Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor Environments</a></td>
  <td>æå‡ºåœ°å½¢æ„ŸçŸ¥ä»»åŠ¡é©±åŠ¨çš„æˆ·å¤–ä¸‰ç»´åœºæ™¯å›¾ç”Ÿæˆæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">occupancy grid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06562v1" data-paper-url="./papers/250606562v1-towards-terrain-aware-task-driven-3d-scene-graph-generation-in-outdo.html" onclick="toggleFavorite(this, '2506.06562v1', 'Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)