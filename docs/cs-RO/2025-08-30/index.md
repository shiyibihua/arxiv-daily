---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-08-30
---

# cs.ROï¼ˆ2025-08-30ï¼‰

ğŸ“Š å…± **15** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250900576v1-galaxea-open-world-dataset-and-g0-dual-system-vla-model.html">Galaxea Open-World Dataset and G0 Dual-System VLA Model</a></td>
  <td>æå‡ºGalaxeaæ•°æ®é›†ä¸G0åŒç³»ç»ŸVLAæ¨¡å‹ä»¥æå‡æœºå™¨äººå¤šæ¨¡æ€è§„åˆ’èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span> <span class="paper-tag">cross-embodiment</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00576v1" data-paper-url="./papers/250900576v1-galaxea-open-world-dataset-and-g0-dual-system-vla-model.html" onclick="toggleFavorite(this, '2509.00576v1', 'Galaxea Open-World Dataset and G0 Dual-System VLA Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250900317v1-a-framework-for-task-and-motion-planning-based-on-expanding-andor-gr.html">A Framework for Task and Motion Planning based on Expanding AND/OR Graphs</a></td>
  <td>æå‡ºåŸºäºæ‰©å±•AND/ORå›¾çš„ä»»åŠ¡ä¸è¿åŠ¨è§„åˆ’æ¡†æ¶ä»¥åº”å¯¹æœºå™¨äººè‡ªä¸»æ€§æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">task and motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00317v1" data-paper-url="./papers/250900317v1-a-framework-for-task-and-motion-planning-based-on-expanding-andor-gr.html" onclick="toggleFavorite(this, '2509.00317v1', 'A Framework for Task and Motion Planning based on Expanding AND/OR Graphs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250900574v1-learning-dolly-in-filming-from-demonstration-using-a-ground-based-ro.html">Learning Dolly-In Filming From Demonstration Using a Ground-Based Robot</a></td>
  <td>æå‡ºåŸºäºç¤ºèŒƒå­¦ä¹ çš„è‡ªåŠ¨åŒ–æ‹æ‘„æ–¹æ³•ä»¥è§£å†³ç”µå½±æ‹æ‘„ä¸­çš„æ§åˆ¶éš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">teleoperation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">PPO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00574v1" data-paper-url="./papers/250900574v1-learning-dolly-in-filming-from-demonstration-using-a-ground-based-ro.html" onclick="toggleFavorite(this, '2509.00574v1', 'Learning Dolly-In Filming From Demonstration Using a Ground-Based Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250900361v1-generative-visual-foresight-meets-task-agnostic-pose-estimation-in-r.html">Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-Top Manipulation</a></td>
  <td>æå‡ºGVF-TAPEä»¥è§£å†³æœºå™¨äººå¤šä»»åŠ¡æ“ä½œä¸­çš„æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00361v1" data-paper-url="./papers/250900361v1-generative-visual-foresight-meets-task-agnostic-pose-estimation-in-r.html" onclick="toggleFavorite(this, '2509.00361v1', 'Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-Top Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250900499v1-neuralsvcd-for-efficient-swept-volume-collision-detection.html">NeuralSVCD for Efficient Swept Volume Collision Detection</a></td>
  <td>æå‡ºNeuralSVCDä»¥è§£å†³é«˜æ•ˆçš„æ‰«æ ä½“ç¢°æ’æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion planning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00499v1" data-paper-url="./papers/250900499v1-neuralsvcd-for-efficient-swept-volume-collision-detection.html" onclick="toggleFavorite(this, '2509.00499v1', 'NeuralSVCD for Efficient Swept Volume Collision Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250900491v1-extended-diffeomorphism-for-real-time-motion-replication-in-workspac.html">Extended Diffeomorphism for Real-Time Motion Replication in Workspaces with Different Spatial Arrangements</a></td>
  <td>æå‡ºæ‰©å±•å¾®åˆ†åŒèƒšä»¥è§£å†³æœºå™¨äººå·¥ä½œç©ºé—´å¸ƒå±€å·®å¼‚é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">dual-arm</span> <span class="paper-tag">teleoperation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00491v1" data-paper-url="./papers/250900491v1-extended-diffeomorphism-for-real-time-motion-replication-in-workspac.html" onclick="toggleFavorite(this, '2509.00491v1', 'Extended Diffeomorphism for Real-Time Motion Replication in Workspaces with Different Spatial Arrangements')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250900310v2-tref-6-inferring-task-relevant-frames-from-a-single-demonstration-fo.html">TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization</a></td>
  <td>æå‡ºTReF-6ä»¥è§£å†³æœºå™¨äººå•æ¬¡ç¤ºèŒƒæ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00310v2" data-paper-url="./papers/250900310v2-tref-6-inferring-task-relevant-frames-from-a-single-demonstration-fo.html" onclick="toggleFavorite(this, '2509.00310v2', 'TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250900319v1-contact-aided-navigation-of-flexible-robotic-endoscope-using-deep-re.html">Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach</a></td>
  <td>æå‡ºåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ¥è§¦è¾…åŠ©å¯¼èˆªç­–ç•¥ä»¥è§£å†³æŸ”æ€§æœºå™¨äººå†…çª¥é•œåœ¨åŠ¨æ€èƒƒä¸­çš„å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">DRL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00319v1" data-paper-url="./papers/250900319v1-contact-aided-navigation-of-flexible-robotic-endoscope-using-deep-re.html" onclick="toggleFavorite(this, '2509.00319v1', 'Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250900571v1-gray-box-computed-torque-control-for-differential-drive-mobile-robot.html">Gray-Box Computed Torque Control for Differential-Drive Mobile Robot Tracking</a></td>
  <td>æå‡ºç°ç®±è®¡ç®—åŠ›çŸ©æ§åˆ¶ä»¥è§£å†³ç§»åŠ¨æœºå™¨äººè·Ÿè¸ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">DRL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00571v1" data-paper-url="./papers/250900571v1-gray-box-computed-torque-control-for-differential-drive-mobile-robot.html" onclick="toggleFavorite(this, '2509.00571v1', 'Gray-Box Computed Torque Control for Differential-Drive Mobile Robot Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250900329v1-jacobian-exploratory-dual-phase-reinforcement-learning-for-dynamic-e.html">Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots</a></td>
  <td>æå‡ºJacobianæ¢ç´¢åŒç›¸å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³å¯å˜å½¢è¿ç»­æœºå™¨äººå¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">PPO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00329v1" data-paper-url="./papers/250900329v1-jacobian-exploratory-dual-phase-reinforcement-learning-for-dynamic-e.html" onclick="toggleFavorite(this, '2509.00329v1', 'Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250900564v1-reinforcement-learning-of-dolly-in-filming-using-a-ground-based-robo.html">Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot</a></td>
  <td>æå‡ºå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è§£å†³åœ°é¢æœºå™¨äººè‡ªåŠ¨æ‹æ‘„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00564v1" data-paper-url="./papers/250900564v1-reinforcement-learning-of-dolly-in-filming-using-a-ground-based-robo.html" onclick="toggleFavorite(this, '2509.00564v1', 'Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250900328v1-mechanistic-interpretability-for-steering-vision-language-action-mod.html">Mechanistic interpretability for steering vision-language-action models</a></td>
  <td>æå‡ºæœºåˆ¶å¯è§£é‡Šæ€§æ¡†æ¶ä»¥å¼•å¯¼è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00328v1" data-paper-url="./papers/250900328v1-mechanistic-interpretability-for-steering-vision-language-action-mod.html" onclick="toggleFavorite(this, '2509.00328v1', 'Mechanistic interpretability for steering vision-language-action models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250900465v1-embodied-spatial-intelligence-from-implicit-scene-modeling-to-spatia.html">Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning</a></td>
  <td>æå‡ºå…·èº«ç©ºé—´æ™ºèƒ½ä»¥è§£å†³æœºå™¨äººç†è§£ä¸è¡ŒåŠ¨çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00465v1" data-paper-url="./papers/250900465v1-embodied-spatial-intelligence-from-implicit-scene-modeling-to-spatia.html" onclick="toggleFavorite(this, '2509.00465v1', 'Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250900433v1-ags-accelerating-3d-gaussian-splatting-slam-via-codec-assisted-frame.html">AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection</a></td>
  <td>æå‡ºAGSæ¡†æ¶ä»¥åŠ é€Ÿ3Dé«˜æ–¯ç‚¹äº‘SLAMç³»ç»Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00433v1" data-paper-url="./papers/250900433v1-ags-accelerating-3d-gaussian-splatting-slam-via-codec-assisted-frame.html" onclick="toggleFavorite(this, '2509.00433v1', 'AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250900570v1-conceptbot-enhancing-robots-autonomy-through-task-decomposition-with.html">ConceptBot: Enhancing Robot's Autonomy through Task Decomposition with Large Language Models and Knowledge Graph</a></td>
  <td>æå‡ºConceptBotä»¥è§£å†³æœºå™¨äººè‡ªä¸»æ€§ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00570v1" data-paper-url="./papers/250900570v1-conceptbot-enhancing-robots-autonomy-through-task-decomposition-with.html" onclick="toggleFavorite(this, '2509.00570v1', 'ConceptBot: Enhancing Robot&#39;s Autonomy through Task Decomposition with Large Language Models and Knowledge Graph')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)