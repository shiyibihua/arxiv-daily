---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-06-27
---

# cs.ROï¼ˆ2025-06-27ï¼‰

ğŸ“Š å…± **8** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250621853v3-skill-nav-enhanced-navigation-with-versatile-quadrupedal-locomotion-.html">Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via Waypoint Interface</a></td>
  <td>æå‡ºSkill-Navä»¥è§£å†³å››è¶³æœºå™¨äººå¯¼èˆªä¸è¿åŠ¨æŠ€èƒ½æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">locomotion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21853v3" data-paper-url="./papers/250621853v3-skill-nav-enhanced-navigation-with-versatile-quadrupedal-locomotion-.html" onclick="toggleFavorite(this, '2506.21853v3', 'Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via Waypoint Interface')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250621982v2-a-milp-based-solution-to-multi-agent-motion-planning-and-collision-a.html">A MILP-Based Solution to Multi-Agent Motion Planning and Collision Avoidance in Constrained Environments</a></td>
  <td>æå‡ºMILPæ–¹æ³•ä»¥è§£å†³å¤šæ™ºèƒ½ä½“è¿åŠ¨è§„åˆ’ä¸ç¢°æ’é¿å…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21982v2" data-paper-url="./papers/250621982v2-a-milp-based-solution-to-multi-agent-motion-planning-and-collision-a.html" onclick="toggleFavorite(this, '2506.21982v2', 'A MILP-Based Solution to Multi-Agent Motion Planning and Collision Avoidance in Constrained Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250622593v1-pixels-to-graph-real-time-integration-of-building-information-models.html">Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding</a></td>
  <td>æå‡ºPixels-to-Graphæ–¹æ³•ä»¥è§£å†³äººæœºåä½œä¸­çš„ç¯å¢ƒç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22593v1" data-paper-url="./papers/250622593v1-pixels-to-graph-real-time-integration-of-building-information-models.html" onclick="toggleFavorite(this, '2506.22593v1', 'Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250622087v2-an-introduction-to-zero-order-optimization-techniques-for-robotics.html">An Introduction to Zero-Order Optimization Techniques for Robotics</a></td>
  <td>æå‡ºé›¶é˜¶ä¼˜åŒ–æŠ€æœ¯ä»¥è§£å†³æœºå™¨äººè½¨è¿¹ä¼˜åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">trajectory optimization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22087v2" data-paper-url="./papers/250622087v2-an-introduction-to-zero-order-optimization-techniques-for-robotics.html" onclick="toggleFavorite(this, '2506.22087v2', 'An Introduction to Zero-Order Optimization Techniques for Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>5</td>
  <td><a href="./papers/250622364v1-robotic-multimodal-data-acquisition-for-in-field-deep-learning-estim.html">Robotic Multimodal Data Acquisition for In-Field Deep Learning Estimation of Cover Crop Biomass</a></td>
  <td>æå‡ºå¤šæ¨¡æ€ä¼ æ„Ÿå™¨ç³»ç»Ÿä»¥ä¼˜åŒ–è¦†ç›–ä½œç‰©ç”Ÿç‰©é‡ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22364v1" data-paper-url="./papers/250622364v1-robotic-multimodal-data-acquisition-for-in-field-deep-learning-estim.html" onclick="toggleFavorite(this, '2506.22364v1', 'Robotic Multimodal Data Acquisition for In-Field Deep Learning Estimation of Cover Crop Biomass')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250622116v1-evaluating-pointing-gestures-for-target-selection-in-human-robot-col.html">Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration</a></td>
  <td>æå‡ºä¸€ç§æ–°æ–¹æ³•ä»¥æé«˜äººæœºåä½œä¸­çš„æŒ‡å‘æ‰‹åŠ¿ç›®æ ‡é€‰æ‹©ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22116v1" data-paper-url="./papers/250622116v1-evaluating-pointing-gestures-for-target-selection-in-human-robot-col.html" onclick="toggleFavorite(this, '2506.22116v1', 'Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250622028v1-lmpvc-and-policy-bank-adaptive-voice-control-for-industrial-robots-w.html">LMPVC and Policy Bank: Adaptive voice control for industrial robots with code generating LLMs and reusable Pythonic policies</a></td>
  <td>æå‡ºLMPVCä¸ç­–ç•¥åº“ä»¥è§£å†³å·¥ä¸šæœºå™¨äººè¯­éŸ³æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22028v1" data-paper-url="./papers/250622028v1-lmpvc-and-policy-bank-adaptive-voice-control-for-industrial-robots-w.html" onclick="toggleFavorite(this, '2506.22028v1', 'LMPVC and Policy Bank: Adaptive voice control for industrial robots with code generating LLMs and reusable Pythonic policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250621860v1-embodied-domain-adaptation-for-object-detection.html">Embodied Domain Adaptation for Object Detection</a></td>
  <td>æå‡ºæºæ— å…³é¢†åŸŸé€‚åº”æ–¹æ³•ä»¥è§£å†³å®¤å†…ç‰©ä½“æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21860v1" data-paper-url="./papers/250621860v1-embodied-domain-adaptation-for-object-detection.html" onclick="toggleFavorite(this, '2506.21860v1', 'Embodied Domain Adaptation for Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)