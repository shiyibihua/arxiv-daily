---
layout: default
title: Embodied Domain Adaptation for Object Detection
---

# Embodied Domain Adaptation for Object Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21860" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21860v1</a>
  <a href="https://arxiv.org/pdf/2506.21860.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21860v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21860v1', 'Embodied Domain Adaptation for Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiangyu Shi, Yanyuan Qiao, Lingqiao Liu, Feras Dayoub

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: Accepted by IROS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæºæ— å…³é¢†åŸŸé€‚åº”æ–¹æ³•ä»¥è§£å†³å®¤å†…ç‰©ä½“æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æºæ— å…³é¢†åŸŸé€‚åº”` `ç‰©ä½“æ£€æµ‹` `å¼€æ”¾è¯æ±‡æ£€æµ‹` `å¯¹æ¯”å­¦ä¹ ` `ç§»åŠ¨æœºå™¨äºº` `åŠ¨æ€ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é—­é›†ç‰©ä½“æ£€æµ‹æ–¹æ³•åœ¨åº”å¯¹çœŸå®ç¯å¢ƒä¸­çš„å¤šæ ·åŒ–ç‰©ä½“å’ŒåŠ¨æ€æ¡ä»¶æ—¶è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æºæ— å…³é¢†åŸŸé€‚åº”æ–¹æ³•ï¼Œåˆ©ç”¨æ—¶é—´èšç±»å’Œå¯¹æ¯”å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œåœ¨ä¸ä¾èµ–æºæ•°æ®çš„æƒ…å†µä¸‹ä¼˜åŒ–ç‰©ä½“æ£€æµ‹æ¨¡å‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½ä¸Šæ˜¾è‘—æå‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”å®¤å†…ç¯å¢ƒçš„åŠ¨æ€å˜åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç§»åŠ¨æœºå™¨äººä¾èµ–ç‰©ä½“æ£€æµ‹å™¨è¿›è¡Œå®¤å†…ç¯å¢ƒçš„æ„ŸçŸ¥å’Œç‰©ä½“å®šä½ã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„é—­é›†æ–¹æ³•åœ¨çœŸå®å®¶åº­å’Œå®éªŒå®¤ä¸­é¢ä¸´å¤šæ ·åŒ–ç‰©ä½“å’ŒåŠ¨æ€æ¡ä»¶çš„æŒ‘æˆ˜ã€‚å¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹ï¼ˆOVODï¼‰è™½ç„¶è¶…è¶Šäº†å›ºå®šæ ‡ç­¾ï¼Œä½†åœ¨å®¤å†…ç¯å¢ƒçš„é¢†åŸŸè½¬ç§»ä¸­ä»ç„¶å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æºæ— å…³é¢†åŸŸé€‚åº”ï¼ˆSFDAï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸è®¿é—®æºæ•°æ®çš„æƒ…å†µä¸‹è°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡æ—¶é—´èšç±»ç²¾ç‚¼ä¼ªæ ‡ç­¾ï¼Œé‡‡ç”¨å¤šå°ºåº¦é˜ˆå€¼èåˆï¼Œå¹¶åº”ç”¨å¯¹æ¯”å­¦ä¹ çš„å¹³å‡æ•™å¸ˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„ç‰©ä½“æ£€æµ‹çš„å…·èº«é¢†åŸŸé€‚åº”ï¼ˆEDAODï¼‰åŸºå‡†è¯„ä¼°äº†åœ¨ç…§æ˜ã€å¸ƒå±€å’Œç‰©ä½“å¤šæ ·æ€§ç­‰é¡ºåºå˜åŒ–ä¸‹çš„é€‚åº”æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶èƒ½çµæ´»é€‚åº”åŠ¨æ€å®¤å†…æ¡ä»¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç§»åŠ¨æœºå™¨äººåœ¨åŠ¨æ€å®¤å†…ç¯å¢ƒä¸­ç‰©ä½“æ£€æµ‹çš„é¢†åŸŸé€‚åº”é—®é¢˜ã€‚ç°æœ‰çš„é—­é›†æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–ç‰©ä½“å’Œç¯å¢ƒå˜åŒ–ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æºæ— å…³é¢†åŸŸé€‚åº”ï¼ˆSFDAï¼‰æ–¹æ³•é€šè¿‡ä¸ä¾èµ–æºæ•°æ®çš„æ–¹å¼ï¼Œåˆ©ç”¨æ—¶é—´èšç±»å’Œå¯¹æ¯”å­¦ä¹ æ¥ä¼˜åŒ–ä¼ªæ ‡ç­¾ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨æ–°ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¼ªæ ‡ç­¾ç”Ÿæˆã€æ—¶é—´èšç±»ã€å¯¹æ¯”å­¦ä¹ å’Œå¤šå°ºåº¦é˜ˆå€¼èåˆç­‰æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡æ—¶é—´èšç±»ç²¾ç‚¼ä¼ªæ ‡ç­¾ï¼Œç„¶åç»“åˆå¯¹æ¯”å­¦ä¹ å’Œå¹³å‡æ•™å¸ˆæ¡†æ¶è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæœ€åè¿›è¡Œå¤šå°ºåº¦èåˆä»¥æé«˜æ£€æµ‹ç²¾åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†æºæ— å…³çš„é¢†åŸŸé€‚åº”ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æºæ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„æ¨¡å‹è°ƒæ•´ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–æºæ•°æ®çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†æ—¶é—´èšç±»ç®—æ³•æ¥ç²¾ç‚¼ä¼ªæ ‡ç­¾ï¼Œè®¾è®¡äº†å¤šå°ºåº¦é˜ˆå€¼èåˆç­–ç•¥ï¼Œå¹¶åœ¨å¯¹æ¯”å­¦ä¹ ä¸­å¼•å…¥äº†å¹³å‡æ•™å¸ˆæ¡†æ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨é›¶æ ·æœ¬æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œæœ‰æ•ˆéªŒè¯äº†æ–¹æ³•åœ¨åŠ¨æ€å®¤å†…ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€æœåŠ¡æœºå™¨äººå’Œå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡ç‰©ä½“æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé€‚åº”æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªå’Œä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mobile robots rely on object detectors for perception and object localization in indoor environments. However, standard closed-set methods struggle to handle the diverse objects and dynamic conditions encountered in real homes and labs. Open-vocabulary object detection (OVOD), driven by Vision Language Models (VLMs), extends beyond fixed labels but still struggles with domain shifts in indoor environments. We introduce a Source-Free Domain Adaptation (SFDA) approach that adapts a pre-trained model without accessing source data. We refine pseudo labels via temporal clustering, employ multi-scale threshold fusion, and apply a Mean Teacher framework with contrastive learning. Our Embodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates adaptation under sequential changes in lighting, layout, and object diversity. Our experiments show significant gains in zero-shot detection performance and flexible adaptation to dynamic indoor conditions.

