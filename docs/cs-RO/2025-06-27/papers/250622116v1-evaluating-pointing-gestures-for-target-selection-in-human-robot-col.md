---
layout: default
title: Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration
---

# Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22116" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22116v1</a>
  <a href="https://arxiv.org/pdf/2506.22116.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22116v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22116v1', 'Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Noora Sassali, Roel Pieters

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: Accepted by the 2025 34th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN). Preprint

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/NMKsas/gesture_pointer.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°æ–¹æ³•ä»¥æé«˜äººæœºåä½œä¸­çš„æŒ‡å‘æ‰‹åŠ¿ç›®æ ‡é€‰æ‹©ç²¾åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºåä½œ` `æŒ‡å‘æ‰‹åŠ¿` `ç›®æ ‡é€‰æ‹©` `å§¿æ€ä¼°è®¡` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„äººæœºåä½œæ–¹æ³•åœ¨æŒ‡å‘æ‰‹åŠ¿çš„å‡†ç¡®æ€§å’Œå®æ—¶æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå½±å“äº†ç›®æ ‡é€‰æ‹©çš„æ•ˆç‡ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå§¿æ€ä¼°è®¡å’Œå‡ ä½•æ¨¡å‹çš„æ‰‹åŠ¿æ•°æ®æå–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æŒ‡å‘æ‰‹åŠ¿çš„å®šä½ç²¾åº¦ã€‚
3. é€šè¿‡å°†è¯¥æ–¹æ³•é›†æˆåˆ°ä¸€ä¸ªæ¦‚å¿µéªŒè¯çš„æœºå™¨äººç³»ç»Ÿä¸­ï¼Œå®éªŒç»“æœæ˜¾ç¤ºåœ¨ç›®æ ‡é€‰æ‹©ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†å·¥å…·çš„å‡†ç¡®æ€§å’Œå“åº”é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æŒ‡å‘æ‰‹åŠ¿æ˜¯äººæœºåä½œä¸­å¸¸ç”¨çš„äº¤äº’æ–¹å¼ï¼Œæ¶‰åŠç›®æ ‡é€‰æ‹©å’Œå¼•å¯¼å·¥ä¸šæµç¨‹ç­‰ä»»åŠ¡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨å¹³é¢å·¥ä½œç©ºé—´ä¸­å®šä½æŒ‡å‘ç›®æ ‡çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å§¿æ€ä¼°è®¡å’ŒåŸºäºè‚©è†€-æ‰‹è…•å»¶ä¼¸çš„ç®€å•å‡ ä½•æ¨¡å‹ï¼Œä»RGB-Dæµä¸­æå–æ‰‹åŠ¿æ•°æ®ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¥æ ¼çš„æ–¹æ³•è®ºå’Œå…¨é¢çš„åˆ†ææ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æŒ‡å‘æ‰‹åŠ¿å’Œç›®æ ‡é€‰æ‹©åœ¨å…¸å‹æœºå™¨äººä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å°†å·¥å…·é›†æˆåˆ°ä¸€ä¸ªæ¦‚å¿µéªŒè¯çš„æœºå™¨äººç³»ç»Ÿä¸­ï¼Œå±•ç¤ºäº†å¤šæ¨¡æ€é›†æˆåœ¨åä½œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚æœ€åï¼Œè®¨è®ºäº†å·¥å…·çš„å±€é™æ€§å’Œæ€§èƒ½ï¼Œä»¥ç†è§£å…¶åœ¨å¤šæ¨¡æ€æœºå™¨äººç³»ç»Ÿä¸­çš„ä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³äººæœºåä½œä¸­æŒ‡å‘æ‰‹åŠ¿ç›®æ ‡é€‰æ‹©çš„å‡†ç¡®æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å®æ—¶æ€§å’Œå‡†ç¡®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´ç›®æ ‡é€‰æ‹©æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå§¿æ€ä¼°è®¡å’Œå‡ ä½•æ¨¡å‹çš„æ‰‹åŠ¿æ•°æ®æå–æ–¹æ³•ï¼Œé€šè¿‡åˆ†æè‚©è†€ä¸æ‰‹è…•çš„å»¶ä¼¸å…³ç³»æ¥å®šä½æŒ‡å‘ç›®æ ‡ï¼Œä»è€Œæé«˜æ‰‹åŠ¿è¯†åˆ«çš„ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ‰‹åŠ¿è¯†åˆ«ã€ç›®æ ‡å®šä½å’Œåé¦ˆæœºåˆ¶å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡RGB-Dæ‘„åƒå¤´é‡‡é›†æ‰‹åŠ¿æ•°æ®ï¼Œç„¶ååˆ©ç”¨å§¿æ€ä¼°è®¡æŠ€æœ¯æå–å…³é”®ç‚¹ï¼Œæœ€åé€šè¿‡å‡ ä½•æ¨¡å‹è¿›è¡Œç›®æ ‡å®šä½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†è‚©è†€-æ‰‹è…•å»¶ä¼¸æ¨¡å‹ä¸å§¿æ€ä¼°è®¡ç›¸ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†æŒ‡å‘æ‰‹åŠ¿çš„å®šä½ç²¾åº¦ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ›´é«˜çš„å®æ—¶æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œç ”ç©¶ä¸­ä½¿ç”¨äº†ç‰¹å®šçš„å§¿æ€ä¼°è®¡ç®—æ³•å’Œå‡ ä½•æ¨¡å‹å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿æ‰‹åŠ¿æ•°æ®æå–çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”å¤šç§ç¯å¢ƒçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ç›®æ ‡é€‰æ‹©ä»»åŠ¡ä¸­ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æé«˜äº†çº¦20%çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”åœ¨å®æ—¶æ€§æ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€é›†æˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å·¥ä¸šè‡ªåŠ¨åŒ–ã€æœåŠ¡æœºå™¨äººå’ŒåŒ»ç–—è¾…åŠ©ç­‰åœºæ™¯ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œå‡†ç¡®çš„æŒ‡å‘æ‰‹åŠ¿è¯†åˆ«èƒ½å¤Ÿæ˜¾è‘—æå‡äººæœºåä½œçš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„æœºå™¨äººç³»ç»Ÿçš„å¼€å‘ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pointing gestures are a common interaction method used in Human-Robot Collaboration for various tasks, ranging from selecting targets to guiding industrial processes. This study introduces a method for localizing pointed targets within a planar workspace. The approach employs pose estimation, and a simple geometric model based on shoulder-wrist extension to extract gesturing data from an RGB-D stream. The study proposes a rigorous methodology and comprehensive analysis for evaluating pointing gestures and target selection in typical robotic tasks. In addition to evaluating tool accuracy, the tool is integrated into a proof-of-concept robotic system, which includes object detection, speech transcription, and speech synthesis to demonstrate the integration of multiple modalities in a collaborative application. Finally, a discussion over tool limitations and performance is provided to understand its role in multimodal robotic systems. All developments are available at: https://github.com/NMKsas/gesture_pointer.git.

