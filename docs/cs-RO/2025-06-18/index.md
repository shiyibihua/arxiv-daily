---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-06-18
---

# cs.ROï¼ˆ2025-06-18ï¼‰

ğŸ“Š å…± **22** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (13 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250615146v1-tact-humanoid-whole-body-contact-manipulation-through-deep-imitation.html">TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality</a></td>
  <td>æå‡ºTACTä»¥è§£å†³ç±»äººæœºå™¨äººå…¨èº«æ¥è§¦æ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">humanoid control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15146v1" data-paper-url="./papers/250615146v1-tact-humanoid-whole-body-contact-manipulation-through-deep-imitation.html" onclick="toggleFavorite(this, '2506.15146v1', 'TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250615132v1-booster-gym-an-end-to-end-reinforcement-learning-framework-for-human.html">Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion</a></td>
  <td>æå‡ºBooster Gymæ¡†æ¶ä»¥è§£å†³äººå½¢æœºå™¨äººè¿åŠ¨æ”¿ç­–è½¬ç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">locomotion</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15132v1" data-paper-url="./papers/250615132v1-booster-gym-an-end-to-end-reinforcement-learning-framework-for-human.html" onclick="toggleFavorite(this, '2506.15132v1', 'Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250615847v1-safemimic-towards-safe-and-autonomous-human-to-robot-imitation-for-m.html">SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation</a></td>
  <td>æå‡ºSafeMimicä»¥è§£å†³æœºå™¨äººæ¨¡ä»¿äººç±»æ“ä½œçš„å®‰å…¨æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15847v1" data-paper-url="./papers/250615847v1-safemimic-towards-safe-and-autonomous-human-to-robot-imitation-for-m.html" onclick="toggleFavorite(this, '2506.15847v1', 'SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250615157v1-robust-instant-policy-leveraging-students-t-regression-model-for-rob.html">Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation</a></td>
  <td>æå‡ºé²æ£’å³æ—¶ç­–ç•¥ä»¥è§£å†³æœºå™¨äººæ¨¡ä»¿å­¦ä¹ ä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15157v1" data-paper-url="./papers/250615157v1-robust-instant-policy-leveraging-students-t-regression-model-for-rob.html" onclick="toggleFavorite(this, '2506.15157v1', 'Robust Instant Policy: Leveraging Student&#39;s t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250615666v1-vision-in-action-learning-active-perception-from-human-demonstration.html">Vision in Action: Learning Active Perception from Human Demonstrations</a></td>
  <td>æå‡ºViAç³»ç»Ÿä»¥è§£å†³åŒæ‰‹æœºå™¨äººæ“ä½œä¸­çš„ä¸»åŠ¨æ„ŸçŸ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">bimanual manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15666v1" data-paper-url="./papers/250615666v1-vision-in-action-learning-active-perception-from-human-demonstration.html" onclick="toggleFavorite(this, '2506.15666v1', 'Vision in Action: Learning Active Perception from Human Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250615380v1-efficient-navigation-among-movable-obstacles-using-a-mobile-manipula.html">Efficient Navigation Among Movable Obstacles using a Mobile Manipulator via Hierarchical Policy Learning</a></td>
  <td>æå‡ºå±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å¯ç§»åŠ¨éšœç¢ç‰©å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15380v1" data-paper-url="./papers/250615380v1-efficient-navigation-among-movable-obstacles-using-a-mobile-manipula.html" onclick="toggleFavorite(this, '2506.15380v1', 'Efficient Navigation Among Movable Obstacles using a Mobile Manipulator via Hierarchical Policy Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250615249v3-context-aware-deep-lagrangian-networks-for-model-predictive-control.html">Context-Aware Deep Lagrangian Networks for Model Predictive Control</a></td>
  <td>æå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥æ·±åº¦æ‹‰æ ¼æœ—æ—¥ç½‘ç»œä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸­çš„æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15249v3" data-paper-url="./papers/250615249v3-context-aware-deep-lagrangian-networks-for-model-predictive-control.html" onclick="toggleFavorite(this, '2506.15249v3', 'Context-Aware Deep Lagrangian Networks for Model Predictive Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250615865v1-improving-robotic-manipulation-techniques-for-object-pose-estimation.html">Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples</a></td>
  <td>æå‡ºè§¦è§‰ä¼ æ„ŸæŠ€æœ¯ä»¥è§£å†³æœºå™¨äººæŠ“å–ä¸­çš„ä½ç½®ä¸ç¡®å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15865v1" data-paper-url="./papers/250615865v1-improving-robotic-manipulation-techniques-for-object-pose-estimation.html" onclick="toggleFavorite(this, '2506.15865v1', 'Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250615150v1-human-locomotion-implicit-modeling-based-real-time-gait-phase-estima.html">Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation</a></td>
  <td>æå‡ºåŸºäºéšå¼å»ºæ¨¡çš„å®æ—¶æ­¥æ€ç›¸ä½ä¼°è®¡æ–¹æ³•ä»¥è§£å†³ç²¾ç¡®é€‚åº”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span> <span class="paper-tag">MAE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15150v1" data-paper-url="./papers/250615150v1-human-locomotion-implicit-modeling-based-real-time-gait-phase-estima.html" onclick="toggleFavorite(this, '2506.15150v1', 'Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250615680v2-particle-grid-neural-dynamics-for-learning-deformable-object-models-.html">Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</a></td>
  <td>æå‡ºç²’å­-ç½‘æ ¼ç¥ç»åŠ¨åŠ›å­¦ä»¥è§£å†³å¯å˜å½¢ç‰©ä½“å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15680v2" data-paper-url="./papers/250615680v2-particle-grid-neural-dynamics-for-learning-deformable-object-models-.html" onclick="toggleFavorite(this, '2506.15680v2', 'Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250615788v1-robust-control-for-multi-legged-elongate-robots-in-noisy-environment.html">Robust control for multi-legged elongate robots in noisy environments</a></td>
  <td>æå‡ºä¸€ç§æ–°èŒƒå¼ä»¥å¢å¼ºå¤šè¶³å»¶ä¼¸æœºå™¨äººåœ¨å™ªå£°ç¯å¢ƒä¸­çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15788v1" data-paper-url="./papers/250615788v1-robust-control-for-multi-legged-elongate-robots-in-noisy-environment.html" onclick="toggleFavorite(this, '2506.15788v1', 'Robust control for multi-legged elongate robots in noisy environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250615539v2-aerial-grasping-via-maximizing-delta-arm-workspace-utilization.html">Aerial Grasping via Maximizing Delta-Arm Workspace Utilization</a></td>
  <td>æå‡ºä¸€ç§æ–°è§„åˆ’æ¡†æ¶ä»¥æœ€å¤§åŒ–æ— äººæœºæŠ“å–çš„å·¥ä½œç©ºé—´åˆ©ç”¨ç‡</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15539v2" data-paper-url="./papers/250615539v2-aerial-grasping-via-maximizing-delta-arm-workspace-utilization.html" onclick="toggleFavorite(this, '2506.15539v2', 'Aerial Grasping via Maximizing Delta-Arm Workspace Utilization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250615087v1-3d-vision-tactile-reconstruction-from-infrared-and-visible-images-fo.html">3D Vision-tactile Reconstruction from Infrared and Visible Images for Robotic Fine-grained Tactile Perception</a></td>
  <td>æå‡ºGelSplitter3Dä»¥è§£å†³ä¼ ç»Ÿå¹³é¢è§¦è§‰ä¼ æ„Ÿå™¨åœ¨æ›²é¢é‡å»ºä¸­çš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15087v1" data-paper-url="./papers/250615087v1-3d-vision-tactile-reconstruction-from-infrared-and-visible-images-fo.html" onclick="toggleFavorite(this, '2506.15087v1', '3D Vision-tactile Reconstruction from Infrared and Visible Images for Robotic Fine-grained Tactile Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250615450v1-surfaav-design-and-implementation-of-a-novel-multimodal-surfing-aqua.html">SurfAAV: Design and Implementation of a Novel Multimodal Surfing Aquatic-Aerial Vehicle</a></td>
  <td>æå‡ºSurfAAVä»¥è§£å†³æ°´ä¸‹ã€è¡¨é¢å’Œç©ºä¸­è¿åŠ¨æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15450v1" data-paper-url="./papers/250615450v1-surfaav-design-and-implementation-of-a-novel-multimodal-surfing-aqua.html" onclick="toggleFavorite(this, '2506.15450v1', 'SurfAAV: Design and Implementation of a Novel Multimodal Surfing Aquatic-Aerial Vehicle')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250615293v1-designing-intent-a-multimodal-framework-for-human-robot-cooperation-.html">Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ¡†æ¶ä»¥è§£å†³äººæœºåä½œä¸­çš„æ„å›¾æ²Ÿé€šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15293v1" data-paper-url="./papers/250615293v1-designing-intent-a-multimodal-framework-for-human-robot-cooperation-.html" onclick="toggleFavorite(this, '2506.15293v1', 'Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250615828v2-context-matters-relaxing-goals-with-llms-for-feasible-3d-scene-plann.html">Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning</a></td>
  <td>æå‡ºContextMattersæ¡†æ¶ä»¥è§£å†³3Dåœºæ™¯è§„åˆ’ä¸­çš„å¯è¡Œæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15828v2" data-paper-url="./papers/250615828v2-context-matters-relaxing-goals-with-llms-for-feasible-3d-scene-plann.html" onclick="toggleFavorite(this, '2506.15828v2', 'Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250615175v2-sherloc-synchronized-heterogeneous-radar-place-recognition-for-cross.html">SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization</a></td>
  <td>æå‡ºSHeRLocä»¥è§£å†³å¼‚æ„é›·è¾¾è·¨æ¨¡æ€å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15175v2" data-paper-url="./papers/250615175v2-sherloc-synchronized-heterogeneous-radar-place-recognition-for-cross.html" onclick="toggleFavorite(this, '2506.15175v2', 'SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250615096v1-dynavlm-zero-shot-vision-language-navigation-system-with-dynamic-vie.html">DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory</a></td>
  <td>æå‡ºDyNaVLMä»¥è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„åŠ¨æ€è§†è§’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15096v1" data-paper-url="./papers/250615096v1-dynavlm-zero-shot-vision-language-navigation-system-with-dynamic-vie.html" onclick="toggleFavorite(this, '2506.15096v1', 'DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250615402v1-mcoo-slam-a-multi-camera-omnidirectional-object-slam-system.html">MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System</a></td>
  <td>æå‡ºMCOO-SLAMä»¥è§£å†³ä¼ ç»ŸSLAMåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15402v1" data-paper-url="./papers/250615402v1-mcoo-slam-a-multi-camera-omnidirectional-object-slam-system.html" onclick="toggleFavorite(this, '2506.15402v1', 'MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250615607v2-grim-task-oriented-grasping-with-conditioning-on-generative-examples.html">GRIM: Task-Oriented Grasping with Conditioning on Generative Examples</a></td>
  <td>æå‡ºGRIMæ¡†æ¶ä»¥è§£å†³ä»»åŠ¡å¯¼å‘æŠ“å–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">affordance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15607v2" data-paper-url="./papers/250615607v2-grim-task-oriented-grasping-with-conditioning-on-generative-examples.html" onclick="toggleFavorite(this, '2506.15607v2', 'GRIM: Task-Oriented Grasping with Conditioning on Generative Examples')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250615518v1-real-time-initialization-of-unknown-anchors-for-uwb-aided-navigation.html">Real-Time Initialization of Unknown Anchors for UWB-aided Navigation</a></td>
  <td>æå‡ºå®æ—¶åˆå§‹åŒ–æœªçŸ¥UWBé”šç‚¹çš„æ–¹æ³•ä»¥è§£å†³å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VIO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15518v1" data-paper-url="./papers/250615518v1-real-time-initialization-of-unknown-anchors-for-uwb-aided-navigation.html" onclick="toggleFavorite(this, '2506.15518v1', 'Real-Time Initialization of Unknown Anchors for UWB-aided Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250615799v2-steering-your-diffusion-policy-with-latent-space-reinforcement-learn.html">Steering Your Diffusion Policy with Latent Space Reinforcement Learning</a></td>
  <td>æå‡ºæ‰©æ•£æ”¿ç­–å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³è¡Œä¸ºå…‹éš†é€‚åº”æ€§ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15799v2" data-paper-url="./papers/250615799v2-steering-your-diffusion-policy-with-latent-space-reinforcement-learn.html" onclick="toggleFavorite(this, '2506.15799v2', 'Steering Your Diffusion Policy with Latent Space Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)