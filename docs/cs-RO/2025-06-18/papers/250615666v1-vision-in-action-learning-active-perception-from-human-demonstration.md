---
layout: default
title: Vision in Action: Learning Active Perception from Human Demonstrations
---

# Vision in Action: Learning Active Perception from Human Demonstrations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15666" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15666v1</a>
  <a href="https://arxiv.org/pdf/2506.15666.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15666v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15666v1', 'Vision in Action: Learning Active Perception from Human Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoyu Xiong, Xiaomeng Xu, Jimmy Wu, Yifan Hou, Jeannette Bohg, Shuran Song

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViAç³»ç»Ÿä»¥è§£å†³åŒæ‰‹æœºå™¨äººæ“ä½œä¸­çš„ä¸»åŠ¨æ„ŸçŸ¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `ä¸»åŠ¨æ„ŸçŸ¥` `åŒæ‰‹æœºå™¨äºº` `è™šæ‹Ÿç°å®` `äººç±»ç¤ºèŒƒ` `è§†è§‰è¿åŠ¨ç­–ç•¥` `æœºå™¨äººæ“ä½œ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººæ“ä½œç³»ç»Ÿåœ¨ä¸»åŠ¨æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆåº”å¯¹å¤æ‚çš„åŒæ‰‹æ“ä½œä»»åŠ¡ã€‚
2. ViAç³»ç»Ÿé€šè¿‡å­¦ä¹ äººç±»ç¤ºèŒƒä¸­çš„ä¸»åŠ¨æ„ŸçŸ¥ç­–ç•¥ï¼Œç»“åˆè™šæ‹Ÿç°å®æŠ€æœ¯ï¼Œæå‡äº†æœºå™¨äººçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒViAåœ¨å¤„ç†è§†è§‰é®æŒ¡çš„å¤šé˜¶æ®µåŒæ‰‹æ“ä½œä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿ç³»ç»Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†Vision in Actionï¼ˆViAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŒæ‰‹æœºå™¨äººæ“ä½œçš„ä¸»åŠ¨æ„ŸçŸ¥ç³»ç»Ÿã€‚ViAç›´æ¥ä»äººç±»ç¤ºèŒƒä¸­å­¦ä¹ ä¸ä»»åŠ¡ç›¸å…³çš„ä¸»åŠ¨æ„ŸçŸ¥ç­–ç•¥ï¼ˆå¦‚æœç´¢ã€è·Ÿè¸ªå’Œèšç„¦ï¼‰ã€‚åœ¨ç¡¬ä»¶æ–¹é¢ï¼ŒViAé‡‡ç”¨äº†ç®€å•è€Œæœ‰æ•ˆçš„6è‡ªç”±åº¦æœºå™¨äººé¢ˆéƒ¨ï¼Œä»¥å®ç°çµæ´»çš„äººç±»å¤´éƒ¨è¿åŠ¨ã€‚ä¸ºäº†æ•æ‰äººç±»çš„ä¸»åŠ¨æ„ŸçŸ¥ç­–ç•¥ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºè™šæ‹Ÿç°å®çš„é¥æ“ä½œæ¥å£ï¼Œåˆ›å»ºäº†æœºå™¨äººä¸äººç±»æ“ä½œå‘˜ä¹‹é—´çš„å…±äº«è§‚å¯Ÿç©ºé—´ã€‚è¯¥æ¥å£é€šè¿‡ä½¿ç”¨ä¸­é—´3Dåœºæ™¯è¡¨ç¤ºæ¥ç¼“è§£å› æœºå™¨äººç‰©ç†è¿åŠ¨å»¶è¿Ÿå¼•èµ·çš„è™šæ‹Ÿç°å®è¿åŠ¨ç—…ï¼Œä½¿æ“ä½œå‘˜èƒ½å¤Ÿå®æ—¶æ¸²æŸ“è§†å›¾ï¼ŒåŒæ—¶å¼‚æ­¥æ›´æ–°åœºæ™¯ã€‚æ‰€æœ‰è¿™äº›è®¾è®¡å…ƒç´ å…±åŒä¿ƒè¿›äº†å¯¹ä¸‰ç§å¤æ‚çš„å¤šé˜¶æ®µåŒæ‰‹æ“ä½œä»»åŠ¡çš„ç¨³å¥è§†è§‰è¿åŠ¨ç­–ç•¥çš„å­¦ä¹ ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿ç³»ç»Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åŒæ‰‹æœºå™¨äººæ“ä½œä¸­çš„ä¸»åŠ¨æ„ŸçŸ¥é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸­éš¾ä»¥æœ‰æ•ˆåº”å¯¹è§†è§‰é®æŒ¡å’ŒåŠ¨æ€ç¯å¢ƒçš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šViAç³»ç»Ÿé€šè¿‡ä»äººç±»ç¤ºèŒƒä¸­å­¦ä¹ ä¸»åŠ¨æ„ŸçŸ¥ç­–ç•¥ï¼Œç»“åˆè™šæ‹Ÿç°å®æŠ€æœ¯ï¼Œåˆ›å»ºå…±äº«è§‚å¯Ÿç©ºé—´ä»¥æå‡æœºå™¨äººæ„ŸçŸ¥èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šViAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ª6è‡ªç”±åº¦çš„æœºå™¨äººé¢ˆéƒ¨ã€ä¸€ä¸ªåŸºäºè™šæ‹Ÿç°å®çš„é¥æ“ä½œæ¥å£å’Œä¸€ä¸ªç”¨äºå­¦ä¹ è§†è§‰è¿åŠ¨ç­–ç•¥çš„æ·±åº¦å­¦ä¹ æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šViAçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶é€šè¿‡è™šæ‹Ÿç°å®æ¥å£å®ç°äº†äººç±»ä¸æœºå™¨äººä¹‹é—´çš„å®æ—¶äº’åŠ¨ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„å»¶è¿Ÿé—®é¢˜ï¼Œä»è€Œæé«˜äº†å­¦ä¹ æ•ˆç‡å’Œæ„ŸçŸ¥èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä¸­é—´3Dåœºæ™¯è¡¨ç¤ºæ¥å‡å°‘å»¶è¿Ÿï¼Œå¹¶ä½¿ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è§†è§‰è¿åŠ¨ç­–ç•¥çš„å­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒViAåœ¨ä¸‰ç§å¤æ‚çš„å¤šé˜¶æ®µåŒæ‰‹æ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿ç³»ç»Ÿï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°æ˜¾è‘—çš„XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œæœ‰æ•ˆåº”å¯¹äº†è§†è§‰é®æŒ¡ç­‰æŒ‘æˆ˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œäººæœºåä½œç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä¸»åŠ¨æ„ŸçŸ¥èƒ½åŠ›ï¼ŒViAç³»ç»Ÿèƒ½å¤Ÿåœ¨å®é™…æ“ä½œä¸­å®ç°æ›´é«˜çš„çµæ´»æ€§å’Œæ•ˆç‡ï¼Œæ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Vision in Action (ViA), an active perception system for bimanual robot manipulation. ViA learns task-relevant active perceptual strategies (e.g., searching, tracking, and focusing) directly from human demonstrations. On the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to enable flexible, human-like head movements. To capture human active perception strategies, we design a VR-based teleoperation interface that creates a shared observation space between the robot and the human operator. To mitigate VR motion sickness caused by latency in the robot's physical movements, the interface uses an intermediate 3D scene representation, enabling real-time view rendering on the operator side while asynchronously updating the scene with the robot's latest observations. Together, these design elements enable the learning of robust visuomotor policies for three complex, multi-stage bimanual manipulation tasks involving visual occlusions, significantly outperforming baseline systems.

