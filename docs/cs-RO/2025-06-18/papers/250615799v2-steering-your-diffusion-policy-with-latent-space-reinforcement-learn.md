---
layout: default
title: Steering Your Diffusion Policy with Latent Space Reinforcement Learning
---

# Steering Your Diffusion Policy with Latent Space Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15799" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15799v2</a>
  <a href="https://arxiv.org/pdf/2506.15799.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15799v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15799v2', 'Steering Your Diffusion Policy with Latent Space Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrew Wagenmaker, Mitsuhiko Nakamoto, Yunchu Zhang, Seohong Park, Waleed Yagoub, Anusha Nagabandi, Abhishek Gupta, Sergey Levine

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18 (æ›´æ–°: 2025-06-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ‰©æ•£æ”¿ç­–å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³è¡Œä¸ºå…‹éš†é€‚åº”æ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¡Œä¸ºå…‹éš†` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `è‡ªä¸»é€‚åº”` `æ ·æœ¬æ•ˆç‡` `æ‰©æ•£æ”¿ç­–` `é»‘ç®±è®¿é—®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¡Œä¸ºå…‹éš†æ–¹æ³•åœ¨æ–°ç¯å¢ƒä¸­çš„é€‚åº”æ€§ä¸è¶³ï¼Œé€šå¸¸éœ€è¦é¢å¤–çš„äººç±»ç¤ºèŒƒæ¥æå‡æ€§èƒ½ï¼Œè¿‡ç¨‹ç¹çä¸”è€—æ—¶ã€‚
2. æå‡ºçš„DSRLæ–¹æ³•é€šè¿‡åœ¨æ½œåœ¨å™ªå£°ç©ºé—´ä¸Šè¿è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¿«é€Ÿé€‚åº”BCè®­ç»ƒçš„ç­–ç•¥ï¼Œé¿å…äº†å¯¹åŸºç¡€ç­–ç•¥æƒé‡çš„ä¿®æ”¹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDSRLåœ¨å¤šä¸ªæ¨¡æ‹ŸåŸºå‡†å’Œç°å®ä¸–ç•Œä»»åŠ¡ä¸­è¡¨ç°å‡ºé«˜æ ·æœ¬æ•ˆç‡å’Œæ˜¾è‘—çš„ç­–ç•¥æ”¹è¿›æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»äººç±»ç¤ºèŒƒä¸­å­¦ä¹ çš„æœºå™¨äººæ§åˆ¶ç­–ç•¥åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œåœ¨åˆå§‹è¡¨ç°ä¸ä½³çš„æƒ…å†µä¸‹ï¼ŒåŸºäºè¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰å­¦ä¹ çš„ç­–ç•¥é€šå¸¸éœ€è¦æ”¶é›†é¢å¤–çš„äººç±»ç¤ºèŒƒä»¥è¿›ä¸€æ­¥æ”¹å–„å…¶è¡Œä¸ºï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æœ‰æœ›å®ç°è‡ªä¸»åœ¨çº¿ç­–ç•¥æ”¹è¿›ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡æ ·æœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨æ½œåœ¨å™ªå£°ç©ºé—´ä¸Šè¿›è¡Œæ‰©æ•£ç­–ç•¥è°ƒæ•´çš„æ–¹æ³•ï¼ˆDSRLï¼‰ï¼Œä»¥å®ç°BCè®­ç»ƒç­–ç•¥çš„å¿«é€Ÿè‡ªä¸»é€‚åº”ã€‚DSRLå…·æœ‰é«˜æ ·æœ¬æ•ˆç‡ï¼Œä»…éœ€å¯¹BCç­–ç•¥è¿›è¡Œé»‘ç®±è®¿é—®ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å®ç°ç°å®ä¸–ç•Œä¸­çš„è‡ªä¸»æ”¿ç­–æ”¹è¿›ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹ŸåŸºå‡†ã€ç°å®ä¸–ç•Œæœºå™¨äººä»»åŠ¡ä»¥åŠé€‚åº”é¢„è®­ç»ƒé€šç”¨ç­–ç•¥ä¸Šå±•ç¤ºäº†DSRLçš„æ ·æœ¬æ•ˆç‡å’Œæœ‰æ•ˆæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¡Œä¸ºå…‹éš†ç­–ç•¥åœ¨æ–°ç¯å¢ƒä¸­é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡äººç±»ç¤ºèŒƒä»¥æå‡æ€§èƒ½ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„DSRLæ–¹æ³•é€šè¿‡åœ¨æ½œåœ¨å™ªå£°ç©ºé—´ä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¿«é€Ÿè°ƒæ•´BCç­–ç•¥ï¼Œé¿å…äº†å¯¹åŸºç¡€ç­–ç•¥çš„ç›´æ¥ä¿®æ”¹ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è‡ªä¸»é€‚åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDSRLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œåˆ©ç”¨ç°æœ‰çš„BCç­–ç•¥ç”Ÿæˆåˆå§‹æ§åˆ¶è¡Œä¸ºï¼›å…¶æ¬¡ï¼Œåœ¨æ½œåœ¨å™ªå£°ç©ºé—´ä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥ä¼˜åŒ–ç­–ç•¥ï¼›æœ€åï¼Œå°†ä¼˜åŒ–åçš„ç­–ç•¥åº”ç”¨äºå®é™…ä»»åŠ¡ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šDSRLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶é«˜æ ·æœ¬æ•ˆç‡å’Œå¯¹BCç­–ç•¥çš„é»‘ç®±è®¿é—®èƒ½åŠ›ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹äººç±»ç¤ºèŒƒçš„ä¾èµ–ï¼Œä¸ä¼ ç»ŸRLæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°å®ç°ç­–ç•¥æ”¹è¿›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DSRLä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§çš„è¶…å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDSRLåœ¨å¤šä¸ªæ¨¡æ‹ŸåŸºå‡†å’Œç°å®ä¸–ç•Œä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ ·æœ¬æ•ˆç‡æé«˜äº†50%ä»¥ä¸Šï¼Œä¸”åœ¨é€‚åº”é¢„è®­ç»ƒç­–ç•¥æ—¶è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»é€‚åº”èƒ½åŠ›ï¼Œé™ä½äººç±»ç¤ºèŒƒçš„éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic control policies learned from human demonstrations have achieved impressive results in many real-world applications. However, in scenarios where initial performance is not satisfactory, as is often the case in novel open-world settings, such behavioral cloning (BC)-learned policies typically require collecting additional human demonstrations to further improve their behavior -- an expensive and time-consuming process. In contrast, reinforcement learning (RL) holds the promise of enabling autonomous online policy improvement, but often falls short of achieving this due to the large number of samples it typically requires. In this work we take steps towards enabling fast autonomous adaptation of BC-trained policies via efficient real-world RL. Focusing in particular on diffusion policies -- a state-of-the-art BC methodology -- we propose diffusion steering via reinforcement learning (DSRL): adapting the BC policy by running RL over its latent-noise space. We show that DSRL is highly sample efficient, requires only black-box access to the BC policy, and enables effective real-world autonomous policy improvement. Furthermore, DSRL avoids many of the challenges associated with finetuning diffusion policies, obviating the need to modify the weights of the base policy at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks, and for adapting pretrained generalist policies, illustrating its sample efficiency and effective performance at real-world policy improvement.

