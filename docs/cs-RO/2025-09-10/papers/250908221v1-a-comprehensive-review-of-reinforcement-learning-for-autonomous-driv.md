---
layout: default
title: A Comprehensive Review of Reinforcement Learning for Autonomous Driving in the CARLA Simulator
---

# A Comprehensive Review of Reinforcement Learning for Autonomous Driving in the CARLA Simulator

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08221" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08221v1</a>
  <a href="https://arxiv.org/pdf/2509.08221.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08221v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08221v1', 'A Comprehensive Review of Reinforcement Learning for Autonomous Driving in the CARLA Simulator')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Elahe Delavari, Feeza Khan Khanzada, Jaerock Kwon

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°ï¼šCARLAæ¨¡æ‹Ÿå™¨ä¸­åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶ç ”ç©¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `å¼ºåŒ–å­¦ä¹ ` `CARLAæ¨¡æ‹Ÿå™¨` `æ·±åº¦å­¦ä¹ ` `ç»¼è¿°` `æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹åŸºå¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶ç ”ç©¶ç¼ºä¹ç³»ç»Ÿæ€§çš„åˆ†æå’Œè¯„ä¼°ï¼Œé˜»ç¢äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚
2. æœ¬ç»¼è¿°é€šè¿‡åˆ†æå¤§é‡åŸºäºCARLAæ¨¡æ‹Ÿå™¨çš„è®ºæ–‡ï¼Œå¯¹å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„åˆ†ç±»å’Œæ€»ç»“ã€‚
3. è¯¥ç ”ç©¶æ€»ç»“äº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å¼ºåŒ–å­¦ä¹ (RL)ä½œä¸ºä¸€ç§æ•°æ®é©±åŠ¨çš„å†³ç­–æ¡†æ¶ï¼Œè¿‘å¹´æ¥åœ¨è‡ªåŠ¨é©¾é©¶ç ”ç©¶ä¸­å¤‡å—é’çã€‚ç„¶è€Œï¼Œç›®å‰å¯¹äºè¿™äº›ç®—æ³•å¦‚ä½•è¢«åº”ç”¨ã€åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°ï¼Œä»ç„¶ç¼ºä¹æ¸…æ™°çš„è®¤è¯†ã€‚æœ¬ç»¼è¿°æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œç³»ç»Ÿåœ°åˆ†æäº†çº¦100ç¯‡åœ¨å¼€æºCARLAæ¨¡æ‹Ÿå™¨ä¸­è®­ç»ƒã€æµ‹è¯•æˆ–éªŒè¯RLç­–ç•¥çš„åŒè¡Œè¯„å®¡è®ºæ–‡ã€‚æˆ‘ä»¬é¦–å…ˆæŒ‰ç…§ç®—æ³•å®¶æ—ï¼ˆæ— æ¨¡å‹ã€åŸºäºæ¨¡å‹ã€åˆ†å±‚å’Œæ··åˆï¼‰å¯¹æ–‡çŒ®è¿›è¡Œåˆ†ç±»ï¼Œå¹¶é‡åŒ–å®ƒä»¬çš„æµè¡Œç¨‹åº¦ï¼Œå¼ºè°ƒè¶…è¿‡80%çš„ç°æœ‰ç ”ç©¶ä»ç„¶ä¾èµ–äºDQNã€PPOå’ŒSACç­‰æ— æ¨¡å‹æ–¹æ³•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è§£é‡Šäº†ä¸åŒç ”ç©¶ä¸­é‡‡ç”¨çš„å¤šæ ·åŒ–çš„çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±è®¾è®¡ï¼Œé˜è¿°äº†ä¼ æ„Ÿå™¨æ¨¡æ€ï¼ˆRGBã€LiDARã€BEVã€è¯­ä¹‰åœ°å›¾å’ŒCARLAè¿åŠ¨å­¦çŠ¶æ€ï¼‰ã€æ§åˆ¶æŠ½è±¡ï¼ˆç¦»æ•£ä¸è¿ç»­ï¼‰å’Œå¥–åŠ±å¡‘é€ å¦‚ä½•åœ¨å„ç§æ–‡çŒ®ä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åˆ—å‡ºCARLAåŸºå‡†æµ‹è¯•ä¸­æœ€å¸¸ç”¨çš„æŒ‡æ ‡ï¼ˆæˆåŠŸç‡ã€ç¢°æ’ç‡ã€è½¦é“åç¦»ã€é©¾é©¶åˆ†æ•°ï¼‰ä»¥åŠåŸé•‡ã€åœºæ™¯å’Œäº¤é€šé…ç½®ï¼Œæ•´åˆäº†è¯„ä¼°ä½“ç³»ã€‚æˆ‘ä»¬å°†ç¨€ç–å¥–åŠ±ã€sim-to-realè¿ç§»ã€å®‰å…¨ä¿è¯å’Œæœ‰é™çš„è¡Œä¸ºå¤šæ ·æ€§ç­‰æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜æç‚¼ä¸ºä¸€ç³»åˆ—å¼€æ”¾çš„ç ”ç©¶é—®é¢˜ï¼Œå¹¶æ¦‚è¿°äº†åŸºäºæ¨¡å‹çš„RLã€å…ƒå­¦ä¹ å’Œæ›´ä¸°å¯Œçš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿç­‰æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚é€šè¿‡æä¾›ç»Ÿä¸€çš„åˆ†ç±»ã€é‡åŒ–ç»Ÿè®¡å’Œå¯¹å±€é™æ€§çš„æ‰¹åˆ¤æ€§è®¨è®ºï¼Œæœ¬ç»¼è¿°æ—¨åœ¨ä¸ºæ–°æ‰‹æä¾›å‚è€ƒï¼Œå¹¶ä¸ºæ¨è¿›åŸºäºRLçš„è‡ªåŠ¨é©¾é©¶èµ°å‘å®é™…éƒ¨ç½²æä¾›è·¯çº¿å›¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶ç ”ç©¶ï¼Œåœ¨ç®—æ³•é€‰æ‹©ã€çŠ¶æ€åŠ¨ä½œç©ºé—´è®¾è®¡ã€å¥–åŠ±å‡½æ•°è®¾è®¡ä»¥åŠè¯„ä¼°æŒ‡æ ‡ç­‰æ–¹é¢å­˜åœ¨å¤šæ ·æ€§ï¼Œç¼ºä¹ç»Ÿä¸€çš„æ ‡å‡†å’Œæ·±å…¥çš„åˆ†æã€‚æ­¤å¤–ï¼Œç¨€ç–å¥–åŠ±ã€sim-to-realè¿ç§»ã€å®‰å…¨ä¿è¯å’Œè¡Œä¸ºå¤šæ ·æ€§ç­‰é—®é¢˜ä»ç„¶æ˜¯è¯¥é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç»¼è¿°çš„æ ¸å¿ƒæ€è·¯æ˜¯å¯¹ç°æœ‰æ–‡çŒ®è¿›è¡Œç³»ç»Ÿæ€§çš„åˆ†ç±»ã€æ€»ç»“å’Œåˆ†æï¼Œä»è€Œæ­ç¤ºå½“å‰ç ”ç©¶çš„ç°çŠ¶ã€æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚é€šè¿‡é‡åŒ–ç»Ÿè®¡ä¸åŒç®—æ³•çš„ä½¿ç”¨é¢‘ç‡ã€çŠ¶æ€åŠ¨ä½œç©ºé—´çš„è®¾è®¡æ–¹æ³•ã€å¥–åŠ±å‡½æ•°çš„æ„å»ºæ–¹å¼ä»¥åŠè¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›å…¨é¢çš„å‚è€ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç»¼è¿°çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) æ–‡çŒ®æ”¶é›†ï¼šæ”¶é›†äº†å¤§é‡åŸºäºCARLAæ¨¡æ‹Ÿå™¨çš„è‡ªåŠ¨é©¾é©¶å¼ºåŒ–å­¦ä¹ è®ºæ–‡ã€‚2) åˆ†ç±»ï¼šæŒ‰ç…§ç®—æ³•å®¶æ—ï¼ˆæ— æ¨¡å‹ã€åŸºäºæ¨¡å‹ã€åˆ†å±‚å’Œæ··åˆï¼‰å¯¹æ–‡çŒ®è¿›è¡Œåˆ†ç±»ã€‚3) åˆ†æï¼šå¯¹ä¸åŒç®—æ³•çš„çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±è®¾è®¡è¿›è¡Œåˆ†æã€‚4) è¯„ä¼°ï¼šæ€»ç»“å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•åœºæ™¯ã€‚5) æ€»ç»“ï¼šæç‚¼å‡ºå½“å‰ç ”ç©¶çš„æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç»¼è¿°çš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç³»ç»Ÿæ€§å’Œå…¨é¢æ€§ã€‚å®ƒä¸ä»…å¯¹ç°æœ‰æ–‡çŒ®è¿›è¡Œäº†åˆ†ç±»å’Œæ€»ç»“ï¼Œè¿˜æ·±å…¥åˆ†æäº†ä¸åŒç®—æ³•çš„è®¾è®¡ç»†èŠ‚å’Œè¯„ä¼°æ–¹æ³•ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æŒ‘æˆ˜å’Œæ–¹å‘ã€‚è¿™ç§å…¨é¢çš„åˆ†æä¸ºç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥ç»¼è¿°çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç®—æ³•åˆ†ç±»ï¼šå°†å¼ºåŒ–å­¦ä¹ ç®—æ³•åˆ†ä¸ºæ— æ¨¡å‹ã€åŸºäºæ¨¡å‹ã€åˆ†å±‚å’Œæ··åˆç­‰ç±»åˆ«ï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜äº†è§£ä¸åŒç®—æ³•çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚2) çŠ¶æ€åŠ¨ä½œç©ºé—´åˆ†æï¼šå¯¹ä¸åŒç ”ç©¶ä¸­é‡‡ç”¨çš„çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´è®¾è®¡è¿›è¡Œåˆ†æï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜é€‰æ‹©åˆé€‚çš„çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ã€‚3) å¥–åŠ±å‡½æ•°åˆ†æï¼šå¯¹ä¸åŒç ”ç©¶ä¸­é‡‡ç”¨çš„å¥–åŠ±å‡½æ•°è®¾è®¡è¿›è¡Œåˆ†æï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°ã€‚4) è¯„ä¼°æŒ‡æ ‡æ€»ç»“ï¼šæ€»ç»“å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜è¯„ä¼°ç®—æ³•çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°ç»Ÿè®¡äº†è¶…è¿‡80%çš„ç°æœ‰ç ”ç©¶ä»ç„¶ä¾èµ–äºDQNã€PPOå’ŒSACç­‰æ— æ¨¡å‹æ–¹æ³•ï¼Œæ­ç¤ºäº†å½“å‰ç ”ç©¶çš„ç°çŠ¶ã€‚åŒæ—¶ï¼Œè¯¥ç»¼è¿°è¿˜æ€»ç»“äº†å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆæˆåŠŸç‡ã€ç¢°æ’ç‡ã€è½¦é“åç¦»ã€é©¾é©¶åˆ†æ•°ï¼‰å’ŒåŸºå‡†æµ‹è¯•åœºæ™¯ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†è¯„ä¼°ç®—æ³•æ€§èƒ½çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç»¼è¿°çš„ç ”ç©¶æˆæœå¯ä»¥åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ç®—æ³•çš„å¼€å‘å’Œè¯„ä¼°ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›å‚è€ƒå’ŒæŒ‡å¯¼ã€‚é€šè¿‡äº†è§£ç°æœ‰æ–¹æ³•çš„ä¼˜ç¼ºç‚¹å’Œæœªæ¥å‘å±•æ–¹å‘ï¼Œå¯ä»¥åŠ é€Ÿè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„ç ”å‘è¿›ç¨‹ï¼Œå¹¶æœ€ç»ˆå®ç°å®‰å…¨å¯é çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous-driving research has recently embraced deep Reinforcement Learning (RL) as a promising framework for data-driven decision making, yet a clear picture of how these algorithms are currently employed, benchmarked and evaluated is still missing. This survey fills that gap by systematically analysing around 100 peer-reviewed papers that train, test or validate RL policies inside the open-source CARLA simulator. We first categorize the literature by algorithmic family model-free, model-based, hierarchical, and hybrid and quantify their prevalence, highlighting that more than 80% of existing studies still rely on model-free methods such as DQN, PPO and SAC. Next, we explain the diverse state, action and reward formulations adopted across works, illustrating how choices of sensor modality (RGB, LiDAR, BEV, semantic maps, and carla kinematics states), control abstraction (discrete vs. continuous) and reward shaping are used across various literature. We also consolidate the evaluation landscape by listing the most common metrics (success rate, collision rate, lane deviation, driving score) and the towns, scenarios and traffic configurations used in CARLA benchmarks. Persistent challenges including sparse rewards, sim-to-real transfer, safety guarantees and limited behaviour diversity are distilled into a set of open research questions, and promising directions such as model-based RL, meta-learning and richer multi-agent simulations are outlined. By providing a unified taxonomy, quantitative statistics and a critical discussion of limitations, this review aims to serve both as a reference for newcomers and as a roadmap for advancing RL-based autonomous driving toward real-world deployment.

