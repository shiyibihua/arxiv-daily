---
layout: default
title: Symmetry-Guided Multi-Agent Inverse Reinforcement Learning
---

# Symmetry-Guided Multi-Agent Inverse Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08257" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08257v2</a>
  <a href="https://arxiv.org/pdf/2509.08257.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08257v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08257v2', 'Symmetry-Guided Multi-Agent Inverse Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongkai Tian, Yirong Qi, Xin Yu, Wenjun Wu, Jie Luo

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10 (æ›´æ–°: 2025-09-11)

**å¤‡æ³¨**: 8pages, 6 figures. Accepted for publication in the Proceedings of the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025) as oral presentation

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹ç§°å¼•å¯¼çš„å¤šæ™ºèƒ½ä½“é€†å¼ºåŒ–å­¦ä¹ ä»¥æå‡æ ·æœ¬æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é€†å¼ºåŒ–å­¦ä¹ ` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `æ ·æœ¬æ•ˆç‡` `å¯¹ç§°æ€§` `æœºå™¨äººæŠ€æœ¯` `ç­–ç•¥ä¼˜åŒ–` `å¥–åŠ±å‡½æ•°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­é¢ä¸´æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜ï¼Œæ”¶é›†ä¸“å®¶ç¤ºèŒƒçš„æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯¹ç§°æ€§æ¥æ¢å¤æ›´å‡†ç¡®çš„å¥–åŠ±å‡½æ•°ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªå¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨ç‰©ç†å¤šæœºå™¨äººç³»ç»Ÿä¸­çš„éªŒè¯è¿›ä¸€æ­¥è¯æ˜äº†å…¶å®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æœºå™¨äººç³»ç»Ÿä¸­ï¼Œå¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ä¾èµ–äºé¢„å®šä¹‰å¥–åŠ±å‡½æ•°çš„åˆç†æ€§ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨è®¾è®¡çš„å¥–åŠ±å‡½æ•°å¸¸å¸¸å› ä¸å‡†ç¡®è€Œå¯¼è‡´ç­–ç•¥å¤±è´¥ã€‚é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰é€šè¿‡ä»ä¸“å®¶ç¤ºèŒƒä¸­æ¨æ–­éšå«å¥–åŠ±å‡½æ•°æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–å¤§é‡ä¸“å®¶ç¤ºèŒƒï¼Œé™åˆ¶äº†å…¶åœ¨å¤šæœºå™¨äººç³»ç»Ÿä¸­çš„å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„å¯¹ç§°æ€§ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå¹¶åœ¨å¤šä¸ªå¤æ‚ä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œè¿›ä¸€æ­¥åœ¨ç‰©ç†å¤šæœºå™¨äººç³»ç»Ÿä¸­å±•ç¤ºäº†æ–¹æ³•çš„å®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“é€†å¼ºåŒ–å­¦ä¹ ä¸­æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤§é‡ä¸“å®¶ç¤ºèŒƒï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­éš¾ä»¥éƒ¨ç½²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„å¯¹ç§°æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ¢å¤å¥–åŠ±å‡½æ•°ï¼Œä»è€Œå‡å°‘å¯¹ä¸“å®¶ç¤ºèŒƒçš„éœ€æ±‚ï¼Œæé«˜æ ·æœ¬æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶é›†æˆäº†å¯¹ç§°æ€§ä¸ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¯¹æŠ—æ€§é€†å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¸»è¦åŒ…æ‹¬å¯¹ç§°æ€§åˆ†ææ¨¡å—ã€å¥–åŠ±å‡½æ•°æ¢å¤æ¨¡å—å’Œç­–ç•¥ä¼˜åŒ–æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¯¹ç§°æ€§å¼•å…¥å¥–åŠ±å‡½æ•°çš„æ¢å¤è¿‡ç¨‹ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå‡å°‘äº†å¯¹ä¸“å®¶ç¤ºèŒƒçš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†è‡ªé€‚åº”å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–æŠ€æœ¯ï¼›æŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†å¯¹ç§°æ€§çº¦æŸï¼›ç½‘ç»œç»“æ„ä¸Šï¼Œä½¿ç”¨äº†å¤šå±‚æ„ŸçŸ¥æœºä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¡†æ¶åœ¨å¤šä¸ªå¤æ‚ä»»åŠ¡ä¸­ç›¸æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œåœ¨ç‰©ç†å¤šæœºå™¨äººç³»ç»Ÿä¸­çš„éªŒè¯è¿›ä¸€æ­¥è¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæœºå™¨äººåä½œã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜æ ·æœ¬æ•ˆç‡ï¼Œèƒ½å¤Ÿé™ä½ä¸“å®¶ç¤ºèŒƒçš„æ”¶é›†æˆæœ¬ï¼Œä»è€ŒåŠ é€Ÿå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å®é™…éƒ¨ç½²å’Œåº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In robotic systems, the performance of reinforcement learning depends on the rationality of predefined reward functions. However, manually designed reward functions often lead to policy failures due to inaccuracies. Inverse Reinforcement Learning (IRL) addresses this problem by inferring implicit reward functions from expert demonstrations. Nevertheless, existing methods rely heavily on large amounts of expert demonstrations to accurately recover the reward function. The high cost of collecting expert demonstrations in robotic applications, particularly in multi-robot systems, severely hinders the practical deployment of IRL. Consequently, improving sample efficiency has emerged as a critical challenge in multi-agent inverse reinforcement learning (MIRL). Inspired by the symmetry inherent in multi-agent systems, this work theoretically demonstrates that leveraging symmetry enables the recovery of more accurate reward functions. Building upon this insight, we propose a universal framework that integrates symmetry into existing multi-agent adversarial IRL algorithms, thereby significantly enhancing sample efficiency. Experimental results from multiple challenging tasks have demonstrated the effectiveness of this framework. Further validation in physical multi-robot systems has shown the practicality of our method.

