---
layout: default
title: SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation
---

# SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08757" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.08757v1</a>
  <a href="https://arxiv.org/pdf/2509.08757.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08757v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08757v1', 'SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Michael J. Munje, Chen Tang, Shuijing Liu, Zichao Hu, Yifeng Zhu, Jiaxun Cui, Garrett Warnell, Joydeep Biswas, Peter Stone

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-10

**Â§áÊ≥®**: Conference on Robot Learning (CoRL) 2025 Project site: https://larg.github.io/socialnav-sub

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://larg.github.io/socialnav-sub)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SocialNav-SUBÂü∫ÂáÜÔºåËØÑ‰º∞VLMÂú®Á§æ‰∫§Êú∫Âô®‰∫∫ÂØºËà™Âú∫ÊôØÁêÜËß£‰∏≠ÁöÑËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±ÂÖ´ÔºöÁâ©ÁêÜÂä®Áîª (Physics-based Animation)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Á§æ‰∫§Êú∫Âô®‰∫∫ÂØºËà™` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Âú∫ÊôØÁêÜËß£` `ËßÜËßâÈóÆÁ≠î` `Âü∫ÂáÜÊï∞ÊçÆÈõÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLMÂú®Á§æ‰∫§Êú∫Âô®‰∫∫ÂØºËà™Âú∫ÊôØÁêÜËß£ÊñπÈù¢ËÉΩÂäõ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Á©∫Èó¥„ÄÅÊó∂Á©∫ÂíåÁ§æ‰ºöÊé®ÁêÜÊñπÈù¢„ÄÇ
2. ÊèêÂá∫SocialNav-SUBÂü∫ÂáÜÔºåÂåÖÂê´VQAÊï∞ÊçÆÈõÜÔºåÁî®‰∫éÁ≥ªÁªüËØÑ‰º∞VLMÂú®Á§æ‰∫§ÂØºËà™Âú∫ÊôØ‰∏≠ÁöÑË°®Áé∞„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÁé∞ÊúâVLMÂú®Á§æ‰∫§Âú∫ÊôØÁêÜËß£ÊñπÈù¢‰ªçÊúâÂ∑ÆË∑ùÔºåÊÄßËÉΩ‰Ωé‰∫é‰∫∫Á±ªÂíåÂü∫‰∫éËßÑÂàôÁöÑÂü∫Á∫ø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜÁ§æ‰∫§ÂØºËà™Âú∫ÊôØÁêÜËß£Âü∫ÂáÜ(SocialNav-SUB)ÔºåËøôÊòØ‰∏Ä‰∏™ËßÜËßâÈóÆÁ≠î(VQA)Êï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞ËßÜËßâËØ≠Ë®ÄÊ®°Âûã(VLM)Âú®ÁúüÂÆûÁ§æ‰∫§Êú∫Âô®‰∫∫ÂØºËà™Âú∫ÊôØ‰∏≠ÁöÑÂú∫ÊôØÁêÜËß£ËÉΩÂäõ„ÄÇSocialNav-SUBÊèê‰æõ‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éËØÑ‰º∞VLMÂú®ÈúÄË¶ÅÁ©∫Èó¥„ÄÅÊó∂Á©∫ÂíåÁ§æ‰ºöÊé®ÁêÜÁöÑVQA‰ªªÂä°‰∏≠Ôºå‰∏é‰∫∫Á±ªÂíåÂü∫‰∫éËßÑÂàôÁöÑÂü∫Á∫øÁõ∏ÊØîÁöÑË°®Áé∞„ÄÇÈÄöËøáÂØπÊúÄÂÖàËøõÁöÑVLMËøõË°åÂÆûÈ™åÔºåÂèëÁé∞ËôΩÁÑ∂ÊÄßËÉΩÊúÄ‰Ω≥ÁöÑVLMÂú®‰∏é‰∫∫Á±ªÁ≠îÊ°àËææÊàê‰∏ÄËá¥ÊñπÈù¢ÂèñÂæó‰∫Ü‰ª§‰∫∫ÈºìËàûÁöÑÊ¶ÇÁéáÔºå‰ΩÜÂÖ∂ÊÄßËÉΩ‰ªçÁÑ∂‰Ωé‰∫éÊõ¥ÁÆÄÂçïÁöÑÂü∫‰∫éËßÑÂàôÁöÑÊñπÊ≥ïÂíå‰∫∫Á±ªÂÖ±ËØÜÂü∫Á∫øÔºåËøôË°®ÊòéÂΩìÂâçVLMÂú®Á§æ‰∫§Âú∫ÊôØÁêÜËß£ÊñπÈù¢Â≠òÂú®ÂÖ≥ÈîÆÂ∑ÆË∑ù„ÄÇËØ•Âü∫ÂáÜ‰∏∫Ëøõ‰∏ÄÊ≠•Á†îÁ©∂Á§æ‰∫§Êú∫Âô®‰∫∫ÂØºËà™ÁöÑÂü∫Á°ÄÊ®°ÂûãÂ•†ÂÆö‰∫ÜÂü∫Á°ÄÔºåÂπ∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™Ê°ÜÊû∂Êù•Êé¢Á¥¢Â¶Ç‰ΩïÂÆöÂà∂VLM‰ª•Êª°Ë∂≥ÁúüÂÆû‰∏ñÁïåÁöÑÁ§æ‰∫§Êú∫Âô®‰∫∫ÂØºËà™ÈúÄÊ±Ç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Á§æ‰∫§Êú∫Âô®‰∫∫ÂØºËà™‰∏≠ÔºåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂØπÂ§çÊùÇÁ§æ‰∫§Âú∫ÊôØÁêÜËß£‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÁ≥ªÁªüÊÄßÁöÑËØÑ‰º∞ÔºåÊó†Ê≥ïÂáÜÁ°ÆË°°ÈáèVLMÂú®Á©∫Èó¥„ÄÅÊó∂Á©∫ÂíåÁ§æ‰ºöÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõÔºåËøôÂØπ‰∫éÂÆâÂÖ®ÂíåÁ¨¶ÂêàÁ§æ‰ºöËßÑËåÉÁöÑÊú∫Âô®‰∫∫ÂØºËà™Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞VLMÂú®Á§æ‰∫§ÂØºËà™Âú∫ÊôØÁêÜËß£ËÉΩÂäõÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜSocialNav-SUB„ÄÇÈÄöËøáËÆæËÆ°‰∏ÄÁ≥ªÂàóÈúÄË¶ÅÁ©∫Èó¥„ÄÅÊó∂Á©∫ÂíåÁ§æ‰ºöÊé®ÁêÜÁöÑËßÜËßâÈóÆÁ≠îÔºàVQAÔºâ‰ªªÂä°ÔºåÊù•Á≥ªÁªüÊÄßÂú∞ËØÑ‰º∞VLMÁöÑÊÄßËÉΩÔºåÂπ∂‰∏é‰∫∫Á±ªÂíåÂü∫‰∫éËßÑÂàôÁöÑÂü∫Á∫øËøõË°åÊØîËæÉ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSocialNav-SUBÂü∫ÂáÜÂåÖÂê´‰∏Ä‰∏™VQAÊï∞ÊçÆÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ÁúüÂÆûÁ§æ‰∫§Êú∫Âô®‰∫∫ÂØºËà™Âú∫ÊôØÁöÑÂõæÂÉèÂíåÁõ∏ÂÖ≥ÈóÆÈ¢ò„ÄÇËØÑ‰º∞ÊµÅÁ®ãÂåÖÊã¨Ôºö1) VLMÊé•Êî∂ÂõæÂÉèÂíåÈóÆÈ¢ò‰Ωú‰∏∫ËæìÂÖ•Ôºõ2) VLMÁîüÊàêÁ≠îÊ°àÔºõ3) Â∞ÜVLMÁöÑÁ≠îÊ°à‰∏é‰∫∫Á±ªÁ≠îÊ°àÂíåÂü∫‰∫éËßÑÂàôÁöÑÂü∫Á∫øÁ≠îÊ°àËøõË°åÊØîËæÉÔºåËÆ°ÁÆóÂáÜÁ°ÆÁéáÁ≠âÊåáÊ†á„ÄÇËØ•Ê°ÜÊû∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂπ≥Âè∞ÔºåÁî®‰∫éËØÑ‰º∞‰∏çÂêåVLMÂú®Á§æ‰∫§ÂØºËà™Âú∫ÊôØÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊûÑÂª∫‰∫Ü‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπÁ§æ‰∫§Êú∫Âô®‰∫∫ÂØºËà™Âú∫ÊôØÁêÜËß£ÁöÑVQAÂü∫ÂáÜÊï∞ÊçÆÈõÜSocialNav-SUB„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´ÈúÄË¶ÅÁ©∫Èó¥„ÄÅÊó∂Á©∫ÂíåÁ§æ‰ºöÊé®ÁêÜÁöÑÂ§çÊùÇÂú∫ÊôØÔºåËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞ËØÑ‰º∞VLMÂú®Á§æ‰∫§ÁéØÂ¢É‰∏≠ÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSocialNav-SUBÊèê‰æõ‰∫Ü‰∏Ä‰∏™Êõ¥ÂÖ∑ÊåëÊàòÊÄßÂíåÁé∞ÂÆûÊÑè‰πâÁöÑËØÑ‰º∞Âπ≥Âè∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSocialNav-SUBÊï∞ÊçÆÈõÜÂåÖÂê´Â§öÁßçÁ±ªÂûãÁöÑVQAÈóÆÈ¢òÔºå‰æãÂ¶ÇÔºöÁ©∫Èó¥ÂÖ≥Á≥ªÊé®ÁêÜÔºà‰æãÂ¶ÇÔºå‚ÄúAÂú®BÁöÑÂ∑¶ËæπÂêóÔºü‚ÄùÔºâ„ÄÅÊó∂Á©∫ÂÖ≥Á≥ªÊé®ÁêÜÔºà‰æãÂ¶ÇÔºå‚ÄúAÂú®B‰πãÂâçÁªèËøáCÂêóÔºü‚ÄùÔºâÂíåÁ§æ‰ºöÂÖ≥Á≥ªÊé®ÁêÜÔºà‰æãÂ¶ÇÔºå‚ÄúAÁöÑÊÑèÂõæÊòØ‰ªÄ‰πàÔºü‚ÄùÔºâ„ÄÇÊï∞ÊçÆÈõÜÁöÑÊûÑÂª∫ËøáÁ®ã‰∏≠ÔºåÈááÁî®‰∫Ü‰∏•Ê†ºÁöÑÊ†áÊ≥®ËßÑËåÉÔºå‰ª•‰øùËØÅÊï∞ÊçÆË¥®Èáè„ÄÇËØÑ‰º∞ÊåáÊ†áÂåÖÊã¨ÂáÜÁ°ÆÁéá„ÄÅF1-scoreÁ≠â„ÄÇËÆ∫ÊñáËøòËÆæËÆ°‰∫ÜÂü∫‰∫éËßÑÂàôÁöÑÂü∫Á∫øÊñπÊ≥ïÔºåÁî®‰∫é‰∏éVLMËøõË°åÊØîËæÉ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËôΩÁÑ∂ÊúÄÂÖàËøõÁöÑVLMÂú®‰∏é‰∫∫Á±ªÁ≠îÊ°àËææÊàê‰∏ÄËá¥ÊñπÈù¢ÂèñÂæó‰∫Ü‰∏ÄÂÆöÁöÑËøõÂ±ïÔºå‰ΩÜÂÖ∂ÊÄßËÉΩ‰ªçÁÑ∂‰Ωé‰∫éÂü∫‰∫éËßÑÂàôÁöÑÂü∫Á∫øÂíå‰∫∫Á±ªÂÖ±ËØÜ„ÄÇ‰æãÂ¶ÇÔºåÊúÄ‰Ω≥VLMÁöÑÂáÜÁ°ÆÁéáÁ∫¶‰∏∫X%ÔºåËÄåÂü∫‰∫éËßÑÂàôÁöÑÂü∫Á∫øÂáÜÁ°ÆÁéáÁ∫¶‰∏∫Y%Ôºå‰∫∫Á±ªÂÖ±ËØÜÂáÜÁ°ÆÁéáÁ∫¶‰∏∫Z%„ÄÇËøôË°®ÊòéÂΩìÂâçVLMÂú®Á§æ‰∫§Âú∫ÊôØÁêÜËß£ÊñπÈù¢‰ªçÂ≠òÂú®ÊòæËëóÂ∑ÆË∑ùÔºåÈúÄË¶ÅËøõ‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊèêÂçáÁ§æ‰∫§Êú∫Âô®‰∫∫Âú®Â§çÊùÇÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÂØºËà™ËÉΩÂäõÔºå‰æãÂ¶ÇÂú®ÂåªÈô¢„ÄÅÂïÜÂú∫Á≠âÂÖ¨ÂÖ±Âú∫ÊâÄ‰∏∫‰∫∫Á±ªÊèê‰æõÂºïÂØºÂíåÊúçÂä°„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫ÂØπ‰∫∫Á±ªÊÑèÂõæÂíåË°å‰∏∫ÁöÑÁêÜËß£ÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥ÂÆâÂÖ®„ÄÅÊõ¥Ëá™ÁÑ∂ÁöÑÊú∫Âô®‰∫∫‰∏é‰∫∫Á±ªÁöÑ‰∫§‰∫íÔºå‰øÉËøõ‰∫∫Êú∫Âçè‰Ωú„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding. Recent Vision-Language Models (VLMs) exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding-capabilities that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can accurately understand complex social navigation scenes (e.g., inferring the spatial-temporal relations among agents and human intentions), which is essential for safe and socially compliant robot navigation. While some recent works have explored the use of VLMs in social robot navigation, no existing work systematically evaluates their ability to meet these necessary conditions. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms simpler rule-based approach and human consensus baselines, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. An overview of this paper along with the code and data can be found at https://larg.github.io/socialnav-sub .

