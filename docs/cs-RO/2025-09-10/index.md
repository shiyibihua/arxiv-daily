---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-09-10
---

# cs.ROï¼ˆ2025-09-10ï¼‰

ğŸ“Š å…± **17** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (9 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250908522v2-robomatch-a-unified-mobile-manipulation-teleoperation-platform-with-.html">RoboMatch: A Unified Mobile-Manipulation Teleoperation Platform with Auto-Matching Network Architecture for Long-Horizon Tasks</a></td>
  <td>RoboMatchï¼šé¢å‘é•¿æ—¶ç¨‹ä»»åŠ¡çš„ç»Ÿä¸€ç§»åŠ¨æ“ä½œé¥æ“ä½œå¹³å°ä¸è‡ªåŒ¹é…ç½‘ç»œæ¶æ„</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span> <span class="paper-tag">dual-arm</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08522v2" data-paper-url="./papers/250908522v2-robomatch-a-unified-mobile-manipulation-teleoperation-platform-with-.html" onclick="toggleFavorite(this, '2509.08522v2', 'RoboMatch: A Unified Mobile-Manipulation Teleoperation Platform with Auto-Matching Network Architecture for Long-Horizon Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250908813v1-calib3r-a-3d-foundation-model-for-multi-camera-to-robot-calibration-.html">Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and 3D Metric-Scaled Scene Reconstruction</a></td>
  <td>Calib3Rï¼šåŸºäº3DåŸºç¡€æ¨¡å‹çš„å¤šç›¸æœº-æœºå™¨äººè”åˆæ ‡å®šä¸å°ºåº¦é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">scene reconstruction</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08813v1" data-paper-url="./papers/250908813v1-calib3r-a-3d-foundation-model-for-multi-camera-to-robot-calibration-.html" onclick="toggleFavorite(this, '2509.08813v1', 'Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and 3D Metric-Scaled Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250908221v1-a-comprehensive-review-of-reinforcement-learning-for-autonomous-driv.html">A Comprehensive Review of Reinforcement Learning for Autonomous Driving in the CARLA Simulator</a></td>
  <td>ç»¼è¿°ï¼šCARLAæ¨¡æ‹Ÿå™¨ä¸­åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶ç ”ç©¶</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08221v1" data-paper-url="./papers/250908221v1-a-comprehensive-review-of-reinforcement-learning-for-autonomous-driv.html" onclick="toggleFavorite(this, '2509.08221v1', 'A Comprehensive Review of Reinforcement Learning for Autonomous Driving in the CARLA Simulator')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250908435v1-pegasusflow-parallel-rolling-denoising-score-sampling-for-robot-diff.html">PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching</a></td>
  <td>PegasusFlowï¼šç”¨äºæœºå™¨äººæ‰©æ•£è§„åˆ’å™¨æµåŒ¹é…çš„å¹¶è¡Œæ»šåŠ¨å»å™ªåˆ†æ•°é‡‡æ ·</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span> <span class="paper-tag">trajectory optimization</span> <span class="paper-tag">motion planning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08435v1" data-paper-url="./papers/250908435v1-pegasusflow-parallel-rolling-denoising-score-sampling-for-robot-diff.html" onclick="toggleFavorite(this, '2509.08435v1', 'PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250908226v1-input-gated-bilateral-teleoperation-an-easy-to-implement-force-feedb.html">Input-gated Bilateral Teleoperation: An Easy-to-implement Force Feedback Teleoperation Method for Low-cost Hardware</a></td>
  <td>æå‡ºä¸€ç§æ˜“äºå®ç°çš„åŠ›åé¦ˆé¥æ“ä½œæ–¹æ³•ï¼Œé€‚ç”¨äºä½æˆæœ¬ç¡¬ä»¶</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">teleoperation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08226v1" data-paper-url="./papers/250908226v1-input-gated-bilateral-teleoperation-an-easy-to-implement-force-feedb.html" onclick="toggleFavorite(this, '2509.08226v1', 'Input-gated Bilateral Teleoperation: An Easy-to-implement Force Feedback Teleoperation Method for Low-cost Hardware')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250908775v2-joint-model-based-model-free-diffusion-for-planning-with-constraints.html">Joint Model-based Model-free Diffusion for Planning with Constraints</a></td>
  <td>æå‡ºJM2Dæ¡†æ¶ï¼Œé€šè¿‡è”åˆæ‰©æ•£æ¨¡å‹å®ç°å¸¦çº¦æŸçš„æœºå™¨äººè¿åŠ¨è§„åˆ’ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion planning</span> <span class="paper-tag">offline RL</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08775v2" data-paper-url="./papers/250908775v2-joint-model-based-model-free-diffusion-for-planning-with-constraints.html" onclick="toggleFavorite(this, '2509.08775v2', 'Joint Model-based Model-free Diffusion for Planning with Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250908354v1-grasp-like-humans-learning-generalizable-multi-fingered-grasping-fro.html">Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration</a></td>
  <td>æå‡ºåŸºäºè§¦è§‰-è¿åŠ¨æ„ŸçŸ¥èåˆçš„æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œå®ç°æœºå™¨äººé€šç”¨å¤šæŒ‡çµå·§æŠ“å–</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous manipulation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08354v1" data-paper-url="./papers/250908354v1-grasp-like-humans-learning-generalizable-multi-fingered-grasping-fro.html" onclick="toggleFavorite(this, '2509.08354v1', 'Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250908495v1-clap-clustering-to-localize-across-n-possibilities-a-simple-robust-g.html">CLAP: Clustering to Localize Across n Possibilities, A Simple, Robust Geometric Approach in the Presence of Symmetries</a></td>
  <td>CLAPï¼šä¸€ç§åŸºäºèšç±»çš„é²æ£’å‡ ä½•å®šä½æ–¹æ³•ï¼Œè§£å†³å¯¹ç§°ç¯å¢ƒä¸‹çš„å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08495v1" data-paper-url="./papers/250908495v1-clap-clustering-to-localize-across-n-possibilities-a-simple-robust-g.html" onclick="toggleFavorite(this, '2509.08495v1', 'CLAP: Clustering to Localize Across n Possibilities, A Simple, Robust Geometric Approach in the Presence of Symmetries')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250908460v1-dual-stage-safe-herding-framework-for-adversarial-attacker-in-dynami.html">Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic Environment</a></td>
  <td>æå‡ºåŒé˜¶æ®µå®‰å…¨å¼•å¯¼æ¡†æ¶ï¼Œè§£å†³åŠ¨æ€ç¯å¢ƒä¸­é˜²å¾¡æœºå™¨äººå¼•å¯¼å¯¹æŠ—æ™ºèƒ½ä½“é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08460v1" data-paper-url="./papers/250908460v1-dual-stage-safe-herding-framework-for-adversarial-attacker-in-dynami.html" onclick="toggleFavorite(this, '2509.08460v1', 'Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic Environment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250908757v1-socialnav-sub-benchmarking-vlms-for-scene-understanding-in-social-ro.html">SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation</a></td>
  <td>æå‡ºSocialNav-SUBåŸºå‡†ï¼Œè¯„ä¼°VLMåœ¨ç¤¾äº¤æœºå™¨äººå¯¼èˆªåœºæ™¯ç†è§£ä¸­çš„èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08757v1" data-paper-url="./papers/250908757v1-socialnav-sub-benchmarking-vlms-for-scene-understanding-in-social-ro.html" onclick="toggleFavorite(this, '2509.08757v1', 'SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250908235v1-deep-visual-odometry-for-stereo-event-cameras.html">Deep Visual Odometry for Stereo Event Cameras</a></td>
  <td>æå‡ºStereo-DEVOï¼Œä¸€ç§ç”¨äºç«‹ä½“äº‹ä»¶ç›¸æœºçš„æ·±åº¦è§†è§‰é‡Œç¨‹è®¡ï¼Œæå‡äº†åœ¨HDRç¯å¢ƒä¸‹çš„ä½å§¿ä¼°è®¡ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">visual odometry</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08235v1" data-paper-url="./papers/250908235v1-deep-visual-odometry-for-stereo-event-cameras.html" onclick="toggleFavorite(this, '2509.08235v1', 'Deep Visual Odometry for Stereo Event Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250908699v1-tango-traversability-aware-navigation-with-local-metric-control-for-.html">TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</a></td>
  <td>TANGOï¼šåŸºäºå¯é€šè¡Œæ€§æ„ŸçŸ¥å’Œå±€éƒ¨åº¦é‡æ§åˆ¶çš„æ‹“æ‰‘ç›®æ ‡å¯¼èˆª</td>
  <td class="tags-cell"><span class="paper-tag">monocular depth</span> <span class="paper-tag">traversability</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08699v1" data-paper-url="./papers/250908699v1-tango-traversability-aware-navigation-with-local-metric-control-for-.html" onclick="toggleFavorite(this, '2509.08699v1', 'TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250908333v1-good-deep-features-to-track-self-supervised-feature-extraction-and-t.html">Good Deep Features to Track: Self-Supervised Feature Extraction and Tracking in Visual Odometry</a></td>
  <td>æå‡ºè‡ªç›‘ç£ç‰¹å¾æå–ä¸è·Ÿè¸ªæ–¹æ³•ï¼Œæå‡è§†è§‰é‡Œç¨‹è®¡åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">visual odometry</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08333v1" data-paper-url="./papers/250908333v1-good-deep-features-to-track-self-supervised-feature-extraction-and-t.html" onclick="toggleFavorite(this, '2509.08333v1', 'Good Deep Features to Track: Self-Supervised Feature Extraction and Tracking in Visual Odometry')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250908820v1-robochemist-long-horizon-and-safety-compliant-robotic-chemical-exper.html">RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation</a></td>
  <td>RoboChemistï¼šé¢å‘é•¿æœŸä»»åŠ¡å’Œå®‰å…¨åˆè§„çš„æœºå™¨äººåŒ–å­¦å®éªŒæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span> <span class="paper-tag">VoxPoser</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08820v1" data-paper-url="./papers/250908820v1-robochemist-long-horizon-and-safety-compliant-robotic-chemical-exper.html" onclick="toggleFavorite(this, '2509.08820v1', 'RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250908302v1-foundation-models-for-autonomous-driving-perception-a-survey-through.html">Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities</a></td>
  <td>ç»¼è¿°ï¼šè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ä¸­çš„åŸºç¡€æ¨¡å‹åŠå…¶æ ¸å¿ƒèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08302v1" data-paper-url="./papers/250908302v1-foundation-models-for-autonomous-driving-perception-a-survey-through.html" onclick="toggleFavorite(this, '2509.08302v1', 'Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250908257v2-symmetry-guided-multi-agent-inverse-reinforcement-learning.html">Symmetry-Guided Multi-Agent Inverse Reinforcement Learning</a></td>
  <td>æå‡ºå¯¹ç§°å¼•å¯¼çš„å¤šæ™ºèƒ½ä½“é€†å¼ºåŒ–å­¦ä¹ ä»¥æå‡æ ·æœ¬æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">inverse reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08257v2" data-paper-url="./papers/250908257v2-symmetry-guided-multi-agent-inverse-reinforcement-learning.html" onclick="toggleFavorite(this, '2509.08257v2', 'Symmetry-Guided Multi-Agent Inverse Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250908241v1-sample-efficient-online-control-policy-learning-with-real-time-recur.html">Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates</a></td>
  <td>æå‡ºé€’å½’Koopmanå­¦ä¹ (RKL)ï¼Œå®ç°é«˜æ ·æœ¬æ•ˆç‡çš„åœ¨çº¿æ§åˆ¶ç­–ç•¥å­¦ä¹ ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08241v1" data-paper-url="./papers/250908241v1-sample-efficient-online-control-policy-learning-with-real-time-recur.html" onclick="toggleFavorite(this, '2509.08241v1', 'Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)