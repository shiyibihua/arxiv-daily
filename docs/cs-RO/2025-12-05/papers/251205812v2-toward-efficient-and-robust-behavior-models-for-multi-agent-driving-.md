---
layout: default
title: Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation
---

# Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.05812" target="_blank" class="toolbar-btn">arXiv: 2512.05812v2</a>
    <a href="https://arxiv.org/pdf/2512.05812.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.05812v2" 
            onclick="toggleFavorite(this, '2512.05812v2', 'Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Fabian Konstantinidis, Moritz Sackmann, Ulrich Hofmann, Christoph Stiller

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-05 (Êõ¥Êñ∞: 2025-12-10)

**Â§áÊ≥®**: This work has been submitted to the IEEE for possible publication

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ÄÁßçÈ´òÊïàÈ≤ÅÊ£íÁöÑÂ§öÊô∫ËÉΩ‰ΩìÈ©æÈ©∂Ë°å‰∏∫Ê®°ÂûãÔºåÁî®‰∫éÈ©æÈ©∂Ê®°Êãü„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü` `È©æÈ©∂Ê®°Êãü` `Ë°å‰∏∫Ê®°Âûã` `ÈÄÜÂº∫ÂåñÂ≠¶‰π†` `Âú∫ÊôØË°®Á§∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈ©æÈ©∂Ê®°ÊãüË°å‰∏∫Ê®°ÂûãÂú®ËÆ°ÁÆóÊïàÁéáÂíåÁúüÂÆûÊÄß‰πãÈó¥Èöæ‰ª•Âπ≥Ë°°ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§öÊô∫ËÉΩ‰ΩìÂú∫ÊôØ‰∏ã„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫‰∏ÄÁßç‰ª•ÂÆû‰æã‰∏∫‰∏≠ÂøÉÁöÑÂú∫ÊôØË°®Á§∫ÂíåÂØπÁß∞‰∏ä‰∏ãÊñáÁºñÁ†ÅÂô®ÔºåÁªìÂêàÂØπÊäóÈÄÜÂº∫ÂåñÂ≠¶‰π†ÔºåÊèêÂçáÊ®°ÂûãÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂáèÂ∞ëËÆ≠ÁªÉÂíåÊé®ÁêÜÊó∂Èó¥ÁöÑÂêåÊó∂ÔºåÊèêÈ´ò‰∫Ü‰ΩçÁΩÆÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄßÔºå‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂèØÊâ©Â±ïÁöÑÂ§öÊô∫ËÉΩ‰ΩìÈ©æÈ©∂Ê®°ÊãüÈúÄË¶ÅÊó¢ÁúüÂÆûÂèàÂÖ∑ÊúâËÆ°ÁÆóÊïàÁéáÁöÑË°å‰∏∫Ê®°Âûã„ÄÇÊú¨ÊñáÈÄöËøá‰ºòÂåñÊéßÂà∂ÂêÑ‰∏™‰∫§ÈÄöÂèÇ‰∏éËÄÖÁöÑË°å‰∏∫Ê®°ÂûãÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜÊèêÈ´òÊïàÁéáÔºåÊàë‰ª¨ÈááÁî®‰∫Ü‰∏ÄÁßç‰ª•ÂÆû‰æã‰∏∫‰∏≠ÂøÉÁöÑÂú∫ÊôØË°®Á§∫ÔºåÂÖ∂‰∏≠ÊØè‰∏™‰∫§ÈÄöÂèÇ‰∏éËÄÖÂíåÂú∞ÂõæÂÖÉÁ¥†ÈÉΩÂú®ÂÖ∂Ëá™Â∑±ÁöÑÂ±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏≠Âª∫Ê®°„ÄÇËøôÁßçËÆæËÆ°ÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑ„ÄÅËßÜÁÇπ‰∏çÂèòÁöÑÂú∫ÊôØÁºñÁ†ÅÔºåÂπ∂ÂÖÅËÆ∏ÈùôÊÄÅÂú∞ÂõæÊ†áËÆ∞Âú®Ê®°ÊãüÊ≠•È™§‰∏≠ÈáçÂ§ç‰ΩøÁî®„ÄÇ‰∏∫‰∫ÜÊ®°Êãü‰∫§‰∫íÔºåÊàë‰ª¨ÈááÁî®‰∫Ü‰∏ÄÁßç‰ª•Êü•ËØ¢‰∏∫‰∏≠ÂøÉÁöÑÂØπÁß∞‰∏ä‰∏ãÊñáÁºñÁ†ÅÂô®ÔºåËØ•ÁºñÁ†ÅÂô®ÂÖ∑ÊúâÂ±ÄÈÉ®Â∏ß‰πãÈó¥ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÁºñÁ†Å„ÄÇÊàë‰ª¨‰ΩøÁî®ÂØπÊäóÈÄÜÂº∫ÂåñÂ≠¶‰π†Êù•Â≠¶‰π†Ë°å‰∏∫Ê®°ÂûãÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫îÂ•ñÂä±ËΩ¨Êç¢ÔºåËØ•ËΩ¨Êç¢ÂèØ‰ª•Âú®ËÆ≠ÁªÉÊúüÈó¥Ëá™Âä®Âπ≥Ë°°È≤ÅÊ£íÊÄßÂíåÁúüÂÆûÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÂú∞ÈöèÁùÄtokenÊï∞ÈáèËøõË°åÊâ©Â±ïÔºå‰ªéËÄåÊòæËëóÂáèÂ∞ëËÆ≠ÁªÉÂíåÊé®ÁêÜÊó∂Èó¥ÔºåÂêåÊó∂Âú®‰ΩçÁΩÆÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢‰ºò‰∫éÂá†Áßç‰ª•agent‰∏∫‰∏≠ÂøÉÁöÑÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§öÊô∫ËÉΩ‰ΩìÈ©æÈ©∂Ê®°ÊãüË°å‰∏∫Ê®°ÂûãÈù¢‰∏¥ÁùÄËÆ°ÁÆóÊïàÁéáÂíåÁúüÂÆûÊÄß‰πãÈó¥ÁöÑÊùÉË°°ÈóÆÈ¢ò„ÄÇ‰º†ÁªüÁöÑ‰ª•Agent‰∏∫‰∏≠ÂøÉÁöÑÊ®°ÂûãÂú®Â§ÑÁêÜÂ§ßÈáè‰∫§ÈÄöÂèÇ‰∏éËÄÖÊó∂ËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºåÈöæ‰ª•Êâ©Â±ï„ÄÇÊ≠§Â§ñÔºåÂ¶Ç‰Ωï‰øùËØÅÊ®°ÂûãÂú®Â§çÊùÇ‰∫§ÈÄöÂú∫ÊôØ‰∏ãÁöÑÈ≤ÅÊ£íÊÄß‰πüÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØ‰ª•ÂÆû‰æã‰∏∫‰∏≠ÂøÉË°®Á§∫Âú∫ÊôØÔºåÂπ∂Âà©Áî®Â±ÄÈÉ®ÂùêÊ†áÁ≥ªÊù•ÁºñÁ†Å‰∫§ÈÄöÂèÇ‰∏éËÄÖÂíåÂú∞ÂõæÂÖÉÁ¥†„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÂÆûÁé∞ËßÜÁÇπ‰∏çÂèòÁöÑÂú∫ÊôØÁºñÁ†ÅÔºåÂπ∂ÂÖÅËÆ∏ÈùôÊÄÅÂú∞ÂõæÊ†áËÆ∞Âú®Ê®°ÊãüÊ≠•È™§‰∏≠ÈáçÂ§ç‰ΩøÁî®Ôºå‰ªéËÄåÊèêÈ´òËÆ°ÁÆóÊïàÁéá„ÄÇÂêåÊó∂ÔºåÈááÁî®ÂØπÁß∞‰∏ä‰∏ãÊñáÁºñÁ†ÅÂô®Êù•Âª∫Ê®°Êô∫ËÉΩ‰Ωì‰πãÈó¥ÁöÑ‰∫§‰∫íÔºåÂπ∂‰ΩøÁî®ÂØπÊäóÈÄÜÂº∫ÂåñÂ≠¶‰π†Êù•Â≠¶‰π†Ë°å‰∏∫Ê®°Âûã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÂÆû‰æã‰∏≠ÂøÉÂú∫ÊôØË°®Á§∫ÔºöÂ∞ÜÊØè‰∏™‰∫§ÈÄöÂèÇ‰∏éËÄÖÂíåÂú∞ÂõæÂÖÉÁ¥†ËΩ¨Êç¢Âà∞ÂÖ∂Ëá™Ë∫´ÁöÑÂ±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏≠„ÄÇ2) ÂØπÁß∞‰∏ä‰∏ãÊñáÁºñÁ†ÅÂô®Ôºö‰ΩøÁî®Áõ∏ÂØπ‰ΩçÁΩÆÁºñÁ†ÅÊù•Âª∫Ê®°Â±ÄÈÉ®Â∏ß‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂπ∂ÁºñÁ†ÅÊô∫ËÉΩ‰Ωì‰πãÈó¥ÁöÑ‰∫§‰∫í„ÄÇ3) ÂØπÊäóÈÄÜÂº∫ÂåñÂ≠¶‰π†Ôºö‰ΩøÁî®Âà§Âà´Âô®Êù•Âå∫ÂàÜÁúüÂÆûËΩ®ËøπÂíåÊ®°ÊãüËΩ®ËøπÔºåÂπ∂‰ΩøÁî®ÁîüÊàêÂô®Êù•Â≠¶‰π†Ë°å‰∏∫Ê®°Âûã„ÄÇ4) Ëá™ÈÄÇÂ∫îÂ•ñÂä±ËΩ¨Êç¢ÔºöËá™Âä®Âπ≥Ë°°ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÈ≤ÅÊ£íÊÄßÂíåÁúüÂÆûÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫é‰ª•ÂÆû‰æã‰∏∫‰∏≠ÂøÉÁöÑÂú∫ÊôØË°®Á§∫ÂíåÂØπÁß∞‰∏ä‰∏ãÊñáÁºñÁ†ÅÂô®ÁöÑÁªìÂêà„ÄÇ‰º†ÁªüÁöÑAgent‰∏≠ÂøÉÊñπÊ≥ïÈúÄË¶ÅÂØπÊØè‰∏™AgentÂçïÁã¨ËøõË°åËÆ°ÁÆóÔºåËÄåÂÆû‰æã‰∏≠ÂøÉÊñπÊ≥ïÂèØ‰ª•ÂÖ±‰∫´ËÆ°ÁÆóËµÑÊ∫êÔºå‰ªéËÄåÊèêÈ´òÊïàÁéá„ÄÇÂØπÁß∞‰∏ä‰∏ãÊñáÁºñÁ†ÅÂô®ËÉΩÂ§üÊúâÊïàÂú∞Âª∫Ê®°Êô∫ËÉΩ‰Ωì‰πãÈó¥ÁöÑ‰∫§‰∫íÔºåÂπ∂‰øùËØÅÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÈááÁî®ÂØπÊäóÈÄÜÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁîüÊàêÂô®‰ΩøÁî®Á•ûÁªèÁΩëÁªúÊù•È¢ÑÊµãÊô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫ÔºåÂà§Âà´Âô®Áî®‰∫éÂå∫ÂàÜÁúüÂÆûËΩ®ËøπÂíåÊ®°ÊãüËΩ®Ëøπ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÁîüÊàêÂô®ÊçüÂ§±ÂíåÂà§Âà´Âô®ÊçüÂ§±ÔºåÂπ∂ÈÄöËøáËá™ÈÄÇÂ∫îÂ•ñÂä±ËΩ¨Êç¢Êù•Âπ≥Ë°°È≤ÅÊ£íÊÄßÂíåÁúüÂÆûÊÄß„ÄÇÂÖ∑‰ΩìÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®tokenÊï∞ÈáèÊâ©Â±ïÊó∂Ë°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊïàÁéáÔºåÊòæËëóÂáèÂ∞ë‰∫ÜËÆ≠ÁªÉÂíåÊé®ÁêÜÊó∂Èó¥„ÄÇÂú®‰ΩçÁΩÆÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢ÔºåËØ•ÊñπÊ≥ï‰ºò‰∫éÂá†Áßç‰ª•Agent‰∏∫‰∏≠ÂøÉÁöÑÂü∫Á∫øÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊèêÂçáÊï∞ÊçÆÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÂ±ïÁ§∫ÔºåËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂ËΩ¶ËæÜÁöÑÊµãËØï‰∏éÈ™åËØÅ„ÄÅ‰∫§ÈÄöÊµÅ‰ªøÁúü‰∏é‰ºòÂåñ„ÄÅ‰ª•ÂèäÈ©æÈ©∂ÂëòË°å‰∏∫ÂàÜÊûêÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊûÑÂª∫È´òÊïà‰∏îÈ≤ÅÊ£íÁöÑÈ©æÈ©∂Ê®°ÊãüÁéØÂ¢ÉÔºåÂèØ‰ª•Âä†ÈÄüËá™Âä®È©æÈ©∂ÁÆóÊ≥ïÁöÑÂºÄÂèëÂíåÈ™åËØÅÔºåÊèêÈ´ò‰∫§ÈÄöÁ≥ªÁªüÁöÑÊïàÁéáÂíåÂÆâÂÖ®ÊÄßÔºåÂπ∂‰∏∫È©æÈ©∂ÂëòËæÖÂä©Á≥ªÁªüÁöÑËÆæËÆ°Êèê‰æõÊï∞ÊçÆÊîØÊåÅ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness.

