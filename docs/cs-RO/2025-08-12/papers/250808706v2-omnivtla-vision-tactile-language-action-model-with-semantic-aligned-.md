---
layout: default
title: OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing
---

# OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08706" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08706v2</a>
  <a href="https://arxiv.org/pdf/2508.08706.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08706v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08706v2', 'OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengxue Cheng, Yiqian Zhang, Wenkang Zhang, Haoyu Li, Keyu Wang, Li Song, Hengdi Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12 (æ›´æ–°: 2025-08-22)

**å¤‡æ³¨**: 15 pages, 7 figures, 8 tables. ObjTac dataset: https://readerek.github.io/Objtac.github.io

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://readerek.github.io/Objtac.github.io)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniVTLAä»¥è§£å†³è§¦è§‰æ„ŸçŸ¥åœ¨æœºå™¨äººæ“ä½œä¸­çš„ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `è§¦è§‰æ„ŸçŸ¥` `æœºå™¨äººæ“ä½œ` `å¤šæ¨¡æ€èåˆ` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨è§¦è§‰æ„ŸçŸ¥æ–¹é¢ä¸¥é‡ä¸è¶³ï¼Œå¯¼è‡´åœ¨æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚
2. æå‡ºOmniVTLAæ¶æ„ï¼Œç»“åˆåŒè·¯å¾„è§¦è§‰ç¼–ç å™¨å’ŒObjTacæ•°æ®é›†ï¼Œæå‡è§¦è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniVTLAåœ¨æŠ“å–ä»»åŠ¡ä¸­æˆåŠŸç‡æé«˜è‡³96.9%ï¼Œæ¯”åŸºçº¿é«˜å‡º21.9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºäºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„ç ”ç©¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨è§¦è§‰æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†OmniVTLAï¼Œä¸€ä¸ªç»“åˆè§¦è§‰æ„ŸçŸ¥çš„åˆ›æ–°æ¶æ„ã€‚å…¶ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šæ„å»ºäº†åŒè·¯å¾„è§¦è§‰ç¼–ç å™¨æ¡†æ¶ï¼Œå¢å¼ºäº†ä¸åŒç±»å‹è§¦è§‰ä¼ æ„Ÿå™¨çš„æ„ŸçŸ¥èƒ½åŠ›ï¼›å¼•å…¥äº†ObjTacæ•°æ®é›†ï¼Œæ¶µç›–56ç§ç‰©ä½“çš„æ–‡æœ¬ã€è§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼›è®­ç»ƒäº†ä¸€ä¸ªè¯­ä¹‰å¯¹é½çš„è§¦è§‰ç¼–ç å™¨ï¼Œæå‡äº†OmniVTLAçš„åˆå§‹åŒ–æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniVTLAåœ¨æŠ“å–å’Œæ”¾ç½®ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æˆåŠŸç‡å’Œä»»åŠ¡å®Œæˆæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨è§¦è§‰æ„ŸçŸ¥æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®æ¥è§¦çš„ä»»åŠ¡ä¸­ï¼Œå¯¼è‡´å…¶æ€§èƒ½å—åˆ°é™åˆ¶ã€‚è§¦è§‰ä¼ æ„Ÿå™¨çš„å¼‚è´¨æ€§å’Œæ•°æ®è·å–çš„å›°éš¾ä½¿å¾—ç°æœ‰æ¨¡å‹æ— æ³•æœ‰æ•ˆåˆ©ç”¨è§¦è§‰ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºOmniVTLAï¼Œé€šè¿‡å¼•å…¥åŒè·¯å¾„è§¦è§‰ç¼–ç å™¨æ¡†æ¶ï¼Œç»“åˆè§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼Œæå‡æœºå™¨äººåœ¨å¤æ‚æ“ä½œä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªè¯­ä¹‰å¯¹é½çš„è§¦è§‰ç¼–ç å™¨ï¼ŒOmniVTLAèƒ½å¤Ÿæ›´å¥½åœ°æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniVTLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€ä¸ªæ˜¯é¢„è®­ç»ƒçš„è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰ï¼Œç”¨äºå¤„ç†è§†è§‰ä¿¡æ¯ï¼›å¦ä¸€ä¸ªæ˜¯è¯­ä¹‰å¯¹é½çš„è§¦è§‰ViTï¼ˆSA-ViTï¼‰ï¼Œç”¨äºå¤„ç†è§¦è§‰ä¿¡æ¯ã€‚è¿™ä¸¤ä¸ªæ¨¡å—é€šè¿‡åŒè·¯å¾„ç»“æ„è¿›è¡Œä¿¡æ¯èåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šOmniVTLAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŒè·¯å¾„è§¦è§‰ç¼–ç å™¨æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆæ¥è‡ªä¸åŒç±»å‹è§¦è§‰ä¼ æ„Ÿå™¨çš„ä¿¡æ¯ï¼Œå¹¶é€šè¿‡ObjTacæ•°æ®é›†çš„æ”¯æŒï¼Œæå‡äº†è§¦è§‰æ„ŸçŸ¥çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆæ•ˆæœï¼ŒåŒæ—¶åœ¨è§¦è§‰ç¼–ç å™¨çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨äº†ObjTacæ•°æ®é›†ä¸­çš„ä¸°å¯Œæ ·æœ¬ï¼Œç¡®ä¿äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniVTLAåœ¨æŠ“å–ä»»åŠ¡ä¸­æˆåŠŸç‡è¾¾åˆ°96.9%ï¼Œæ¯”ç°æœ‰åŸºçº¿æé«˜21.9%ï¼›åœ¨çµå·§æ‰‹æ“ä½œä¸­æˆåŠŸç‡è¾¾åˆ°100%ï¼Œæ¯”åŸºçº¿æé«˜6.2%ã€‚æ­¤å¤–ï¼ŒOmniVTLAåœ¨ä»»åŠ¡å®Œæˆæ—¶é—´å’Œè½¨è¿¹å¹³æ»‘æ€§æ–¹é¢ä¹Ÿæ˜¾è‘—ä¼˜äºç°æœ‰VLAæ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OmniVTLAçš„ç ”ç©¶æˆæœåœ¨æœºå™¨äººæ“ä½œã€è‡ªåŠ¨åŒ–åˆ¶é€ å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡è§¦è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´ç²¾å‡†åœ°æ‰§è¡Œå¤æ‚çš„æ“ä½œä»»åŠ¡ï¼Œè¿›è€Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œéšç€è§¦è§‰ä¼ æ„Ÿå™¨æŠ€æœ¯çš„å‘å±•ï¼ŒOmniVTLAæœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent vision-language-action (VLA) models build upon vision-language foundations, and have achieved promising results and exhibit the possibility of task generalization in robot manipulation. However, due to the heterogeneity of tactile sensors and the difficulty of acquiring tactile data, current VLA models significantly overlook the importance of tactile perception and fail in contact-rich tasks. To address this issue, this paper proposes OmniVTLA, a novel architecture involving tactile sensing. Specifically, our contributions are threefold. First, our OmniVTLA features a dual-path tactile encoder framework. This framework enhances tactile perception across diverse vision-based and force-based tactile sensors by using a pretrained vision transformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we introduce ObjTac, a comprehensive force-based tactile dataset capturing textual, visual, and tactile information for 56 objects across 10 categories. With 135K tri-modal samples, ObjTac supplements existing visuo-tactile datasets. Third, leveraging this dataset, we train a semantically-aligned tactile encoder to learn a unified tactile representation, serving as a better initialization for OmniVTLA. Real-world experiments demonstrate substantial improvements over state-of-the-art VLA baselines, achieving 96.9% success rates with grippers, (21.9% higher over baseline) and 100% success rates with dexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides, OmniVTLA significantly reduces task completion time and generates smoother trajectories through tactile sensing compared to existing VLA. Our ObjTac dataset can be found at https://readerek.github.io/Objtac.github.io

