---
layout: default
title: Rational Inverse Reasoning
---

# Rational Inverse Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08983" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08983v1</a>
  <a href="https://arxiv.org/pdf/2508.08983.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08983v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08983v1', 'Rational Inverse Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ben Zandonati, TomÃ¡s Lozano-PÃ©rez, Leslie Pack Kaelbling

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç†æ€§é€†æ¨æ¨ç†æ¡†æ¶ä»¥è§£å†³æœºå™¨äººæ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `é€†æ¨æ¨ç†` `å°‘é‡å­¦ä¹ ` `è´å¶æ–¯ç¨‹åºå½’çº³` `å±‚æ¬¡ç”Ÿæˆæ¨¡å‹` `æœºå™¨äººå­¦ä¹ ` `ä»»åŠ¡æ³›åŒ–` `è§†è§‰-è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººç³»ç»Ÿåœ¨æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé€šå¸¸éœ€è¦å¤§é‡ç¤ºä¾‹æ‰èƒ½å­¦ä¹ æ–°ä»»åŠ¡ã€‚
2. è®ºæ–‡æå‡ºç†æ€§é€†æ¨æ¨ç†ï¼ˆRIRï¼‰æ¡†æ¶ï¼Œé€šè¿‡å±‚æ¬¡ç”Ÿæˆæ¨¡å‹æ¨æ–­æ½œåœ¨çš„ä»»åŠ¡ç¨‹åºï¼Œæå‡æœºå™¨äººå­¦ä¹ æ•ˆç‡ã€‚
3. RIRåœ¨ä¸€ç³»åˆ—è¿ç»­æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä»…æœ‰ä¸€ä¸ªç¤ºèŒƒçš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆçš„ä»»åŠ¡æ³›åŒ–ï¼Œè¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»èƒ½å¤Ÿé€šè¿‡å•ä¸€çš„ä¸å®Œç¾ç¤ºèŒƒè¿…é€Ÿæ¨å¹¿åˆ°ä¸åŒçš„é—®é¢˜è®¾ç½®ï¼Œè€Œæœºå™¨äººé€šå¸¸éœ€è¦æ•°ç™¾ä¸ªç¤ºä¾‹ï¼Œä¸”åœ¨è®­ç»ƒæ¡ä»¶ä¹‹å¤–çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚æœ¬æ–‡æå‡ºç†æ€§é€†æ¨æ¨ç†ï¼ˆRIRï¼‰æ¡†æ¶ï¼Œé€šè¿‡å±‚æ¬¡ç”Ÿæˆæ¨¡å‹æ¨æ–­æ½œåœ¨ç¨‹åºï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é™åˆ¶ã€‚RIRå°†å°‘é‡æ¨¡ä»¿è§†ä¸ºè´å¶æ–¯ç¨‹åºå½’çº³ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹è¿­ä»£æå‡ºç»“æ„åŒ–çš„ä»»åŠ¡å‡è®¾ï¼Œå¹¶é€šè¿‡è§„åˆ’è€…åœ¨ç¯æ¨ç†æ–¹æ¡ˆå¯¹æ¯ä¸ªå‡è®¾è¿›è¡Œè¯„åˆ†ã€‚å®éªŒè¡¨æ˜ï¼ŒRIRåœ¨è¿ç»­æ“ä½œä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿåœ¨ä»…æœ‰ä¸€ä¸ªç¤ºèŒƒçš„æƒ…å†µä¸‹æ¨æ–­å‡ºé¢„æœŸçš„ä»»åŠ¡ç»“æ„å¹¶åœ¨æ–°ç¯å¢ƒä¸­æ³›åŒ–ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨å°‘é‡ç¤ºèŒƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤§é‡ç¤ºä¾‹ï¼Œéš¾ä»¥æœ‰æ•ˆæ¨æ–­æ½œåœ¨çš„ä»»åŠ¡ç»“æ„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRIRæ¡†æ¶é€šè¿‡å±‚æ¬¡ç”Ÿæˆæ¨¡å‹æ¨æ–­æ½œåœ¨ç¨‹åºï¼Œå°†å°‘é‡æ¨¡ä»¿è§†ä¸ºè´å¶æ–¯ç¨‹åºå½’çº³ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å’Œè§„åˆ’è€…åœ¨ç¯æ¨ç†ç›¸ç»“åˆçš„æ–¹å¼ï¼Œæå‡æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRIRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§‰-è¯­è¨€æ¨¡å‹ç”¨äºç”Ÿæˆä»»åŠ¡å‡è®¾ï¼Œè§„åˆ’è€…åœ¨ç¯æ¨ç†ç”¨äºè¯„åˆ†å’Œé€‰æ‹©æœ€ä½³å‡è®¾ã€‚è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£è¿‡ç¨‹ä¸æ–­ä¼˜åŒ–ä»»åŠ¡ç»“æ„çš„æ¨æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šRIRçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å°‘é‡æ¨¡ä»¿å­¦ä¹ è½¬åŒ–ä¸ºè´å¶æ–¯ç¨‹åºå½’çº³ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¨æ–­å‡ºé«˜å±‚æ¬¡çš„ä»»åŠ¡ç»“æ„ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨æ–°ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šRIRè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å‡è®¾è¯„åˆ†ï¼Œå¹¶é‡‡ç”¨äº†å±‚æ¬¡åŒ–çš„ç½‘ç»œç»“æ„ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRIRåœ¨ä¸€ç³»åˆ—è¿ç»­æ“ä½œä»»åŠ¡ä¸­ï¼Œèƒ½å¤Ÿåœ¨ä»…æœ‰ä¸€ä¸ªç¤ºèŒƒçš„æƒ…å†µä¸‹æˆåŠŸæ¨æ–­ä»»åŠ¡ç»“æ„ï¼Œå¹¶åœ¨æ–°ç¯å¢ƒä¸­æ³›åŒ–ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åŸºçº¿ï¼Œå±•ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€è‡ªåŠ¨åŒ–åˆ¶é€ ã€æ™ºèƒ½å®¶å±…ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨æ–°ç¯å¢ƒä¸­çš„å­¦ä¹ å’Œé€‚åº”èƒ½åŠ›ï¼ŒRIRæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æé«˜æœºå™¨äººåœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œçµæ´»æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humans can observe a single, imperfect demonstration and immediately generalize to very different problem settings. Robots, in contrast, often require hundreds of examples and still struggle to generalize beyond the training conditions. We argue that this limitation arises from the inability to recover the latent explanations that underpin intelligent behavior, and that these explanations can take the form of structured programs consisting of high-level goals, sub-task decomposition, and execution constraints. In this work, we introduce Rational Inverse Reasoning (RIR), a framework for inferring these latent programs through a hierarchical generative model of behavior. RIR frames few-shot imitation as Bayesian program induction: a vision-language model iteratively proposes structured symbolic task hypotheses, while a planner-in-the-loop inference scheme scores each by the likelihood of the observed demonstration under that hypothesis. This loop yields a posterior over concise, executable programs. We evaluate RIR on a suite of continuous manipulation tasks designed to test one-shot and few-shot generalization across variations in object pose, count, geometry, and layout. With as little as one demonstration, RIR infers the intended task structure and generalizes to novel settings, outperforming state-of-the-art vision-language model baselines.

