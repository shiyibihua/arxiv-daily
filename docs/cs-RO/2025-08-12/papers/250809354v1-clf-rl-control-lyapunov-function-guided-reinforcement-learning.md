---
layout: default
title: CLF-RL: Control Lyapunov Function Guided Reinforcement Learning
---

# CLF-RL: Control Lyapunov Function Guided Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09354" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09354v1</a>
  <a href="https://arxiv.org/pdf/2508.09354.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09354v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09354v1', 'CLF-RL: Control Lyapunov Function Guided Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kejun Li, Zachary Olkin, Yisong Yue, Aaron D. Ames

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: 8 pages; 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLF-RLä»¥è§£å†³å¼ºåŒ–å­¦ä¹ åœ¨åŒè¶³æœºå™¨äººæ§åˆ¶ä¸­çš„å¥–åŠ±è®¾è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ§åˆ¶æé›…æ™®è¯ºå¤«å‡½æ•°` `åŒè¶³æœºå™¨äºº` `è¿åŠ¨è§„åˆ’` `å¥–åŠ±è®¾è®¡` `æ¨¡å‹åŸºæ–¹æ³•` `é²æ£’æ€§æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åŒè¶³æœºå™¨äººæ§åˆ¶ä¸­é¢ä¸´å¥–åŠ±è®¾è®¡å¤æ‚å’Œå¯¹ç›®æ ‡æ•æ„Ÿçš„é—®é¢˜ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ§åˆ¶æé›…æ™®è¯ºå¤«å‡½æ•°çš„å¥–åŠ±å¡‘å½¢æ¡†æ¶ï¼Œé€šè¿‡æ¨¡å‹ç”Ÿæˆå‚è€ƒè½¨è¿¹æ¥æŒ‡å¯¼ç­–ç•¥å­¦ä¹ ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCLF-RLåœ¨ä»¿çœŸå’Œå®é™…åº”ç”¨ä¸­ç›¸è¾ƒäºåŸºçº¿RLç­–ç•¥å…·æœ‰æ˜¾è‘—çš„é²æ£’æ€§æå‡å’Œæ›´å¥½çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ç”ŸæˆåŒè¶³æœºå™¨äººç¨³å¥çš„è¿åŠ¨ç­–ç•¥æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å¸¸å¸¸é¢ä¸´å¥–åŠ±è®¾è®¡ç¹çå’Œå¯¹ç›®æ ‡å½¢çŠ¶æ•æ„Ÿçš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„å¥–åŠ±å¡‘å½¢æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºæ¨¡å‹çš„è½¨è¿¹ç”Ÿæˆå’Œæ§åˆ¶æé›…æ™®è¯ºå¤«å‡½æ•°ï¼ˆCLFsï¼‰æ¥æŒ‡å¯¼ç­–ç•¥å­¦ä¹ ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§åŸºäºæ¨¡å‹çš„è§„åˆ’å™¨æ¥ç”Ÿæˆå‚è€ƒè½¨è¿¹ï¼šä¸€ç§æ˜¯ç”¨äºé€Ÿåº¦æ¡ä»¶è¿åŠ¨è§„åˆ’çš„ç®€åŒ–çº¿æ€§å€’ç«‹æ‘†ï¼ˆLIPï¼‰æ¨¡å‹ï¼Œå¦ä¸€ç§æ˜¯åŸºäºæ··åˆé›¶åŠ¨æ€ï¼ˆHZDï¼‰çš„å…¨é˜¶åŠ¨æ€é¢„è®¡ç®—æ­¥æ€åº“ã€‚è¿™äº›è§„åˆ’å™¨å®šä¹‰äº†æœŸæœ›çš„æœ«ç«¯æ‰§è¡Œå™¨å’Œå…³èŠ‚è½¨è¿¹ï¼Œç”¨äºæ„å»ºåŸºäºCLFçš„å¥–åŠ±ï¼Œæƒ©ç½šè·Ÿè¸ªè¯¯å·®å¹¶é¼“åŠ±å¿«é€Ÿæ”¶æ•›ã€‚è¯¥æ–¹æ³•åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨å‚è€ƒè½¨è¿¹å’ŒCLFå¡‘å½¢ï¼Œéƒ¨ç½²æ—¶åˆ™ç”Ÿæˆè½»é‡çº§ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ä»¿çœŸå’ŒUnitree G1æœºå™¨äººä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ åœ¨åŒè¶³æœºå™¨äººæ§åˆ¶ä¸­å¥–åŠ±è®¾è®¡ç¹çå’Œå¯¹ç›®æ ‡æ•æ„Ÿçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥ç”Ÿæˆæœ‰æ•ˆçš„è¿åŠ¨ç­–ç•¥ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„CLF-RLæ–¹æ³•é€šè¿‡ç»“åˆæ§åˆ¶æé›…æ™®è¯ºå¤«å‡½æ•°å’ŒåŸºäºæ¨¡å‹çš„è½¨è¿¹ç”Ÿæˆï¼Œæä¾›äº†ä¸€ç§ç»“æ„åŒ–çš„å¥–åŠ±å¡‘å½¢æ¡†æ¶ï¼Œä»¥æŒ‡å¯¼ç­–ç•¥å­¦ä¹ å¹¶æé«˜å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯åŸºäºç®€åŒ–çº¿æ€§å€’ç«‹æ‘†æ¨¡å‹çš„é€Ÿåº¦æ¡ä»¶è¿åŠ¨è§„åˆ’ï¼ŒäºŒæ˜¯åŸºäºæ··åˆé›¶åŠ¨æ€çš„é¢„è®¡ç®—æ­¥æ€åº“ã€‚è¿™äº›æ¨¡å—ç”ŸæˆæœŸæœ›çš„è½¨è¿¹ï¼Œè¿›è€Œæ„å»ºCLFå¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ§åˆ¶æé›…æ™®è¯ºå¤«å‡½æ•°åº”ç”¨äºå¥–åŠ±è®¾è®¡ä¸­ï¼Œæä¾›äº†æœ‰æ•ˆçš„ä¸­é—´å¥–åŠ±ï¼Œæ˜¾è‘—æ”¹å–„äº†å­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚ä¸ä¼ ç»Ÿçš„è·Ÿè¸ªå¥–åŠ±æ–¹æ³•ç›¸æ¯”ï¼ŒCLF-RLåœ¨å¥–åŠ±è®¾è®¡ä¸Šæ›´å…·ç»“æ„æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCLFå¥–åŠ±å‡½æ•°æƒ©ç½šè·Ÿè¸ªè¯¯å·®å¹¶é¼“åŠ±å¿«é€Ÿæ”¶æ•›ï¼Œç¡®ä¿äº†åœ¨è®­ç»ƒæœŸé—´çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå‚è€ƒè½¨è¿¹çš„ç”Ÿæˆå’ŒCLFå¡‘å½¢ä»…åœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨ï¼Œç¡®ä¿äº†éƒ¨ç½²æ—¶ç­–ç•¥çš„è½»é‡åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLF-RLåœ¨Unitree G1æœºå™¨äººä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿RLç­–ç•¥ï¼Œé²æ£’æ€§æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œå¹¶ä¸”åœ¨ç»å…¸è·Ÿè¸ªå¥–åŠ±çš„RLæ–¹æ³•ä¸­è¡¨ç°æ›´ä½³ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒè¶³æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶å’Œäººæœºåä½œç­‰åœºæ™¯ã€‚é€šè¿‡æä¾›æ›´ç¨³å¥çš„è¿åŠ¨æ§åˆ¶ç­–ç•¥ï¼ŒCLF-RLèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„è¿åŠ¨è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has shown promise in generating robust locomotion policies for bipedal robots, but often suffers from tedious reward design and sensitivity to poorly shaped objectives. In this work, we propose a structured reward shaping framework that leverages model-based trajectory generation and control Lyapunov functions (CLFs) to guide policy learning. We explore two model-based planners for generating reference trajectories: a reduced-order linear inverted pendulum (LIP) model for velocity-conditioned motion planning, and a precomputed gait library based on hybrid zero dynamics (HZD) using full-order dynamics. These planners define desired end-effector and joint trajectories, which are used to construct CLF-based rewards that penalize tracking error and encourage rapid convergence. This formulation provides meaningful intermediate rewards, and is straightforward to implement once a reference is available. Both the reference trajectories and CLF shaping are used only during training, resulting in a lightweight policy at deployment. We validate our method both in simulation and through extensive real-world experiments on a Unitree G1 robot. CLF-RL demonstrates significantly improved robustness relative to the baseline RL policy and better performance than a classic tracking reward RL formulation.

