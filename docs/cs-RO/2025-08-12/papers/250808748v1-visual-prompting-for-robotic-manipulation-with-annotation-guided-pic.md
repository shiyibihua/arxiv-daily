---
layout: default
title: Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT
---

# Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08748" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08748v1</a>
  <a href="https://arxiv.org/pdf/2508.08748.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08748v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08748v1', 'Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Muhammad A. Muttaqien, Tomohiro Motoda, Ryo Hanai, Yukiyasu Domae

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ³¨é‡Šå¼•å¯¼çš„è§†è§‰æç¤ºä»¥è§£å†³æœºå™¨äººæŠ“å–ä¸æ”¾ç½®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæŠ“å–` `è§†è§‰æç¤º` `åŠ¨ä½œåˆ†å—` `å˜æ¢å™¨` `æ¨¡ä»¿å­¦ä¹ ` `é›¶å”®è‡ªåŠ¨åŒ–` `ç‰©ä½“æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººæŠ“å–ä¸æ”¾ç½®æ–¹æ³•åœ¨å¤„ç†å¯†é›†ç‰©ä½“å’Œé®æŒ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´è½¨è¿¹è§„åˆ’å’ŒæŠ“å–ç²¾åº¦ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ³¨é‡Šå¼•å¯¼çš„è§†è§‰æç¤ºæ–¹æ³•ï¼Œç»“åˆåŠ¨ä½œåˆ†å—ä¸å˜æ¢å™¨ï¼ˆACTï¼‰ç®—æ³•ï¼Œæå‡äº†æœºå™¨äººæŠ“å–ä¸æ”¾ç½®çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨æŠ“å–æˆåŠŸç‡å’Œé€‚åº”æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œé€‚ç”¨äºå¤æ‚çš„é›¶å”®ç¯å¢ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ä¾¿åˆ©åº—çš„æœºå™¨äººæŠ“å–ä¸æ”¾ç½®ä»»åŠ¡ä¸­ï¼Œç”±äºç‰©ä½“å¯†é›†æ’åˆ—ã€é®æŒ¡ä»¥åŠç‰©ä½“å±æ€§ï¼ˆå¦‚é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°å’Œçº¹ç†ï¼‰çš„å˜åŒ–ï¼Œå¯¼è‡´è½¨è¿¹è§„åˆ’å’ŒæŠ“å–å˜å¾—å¤æ‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨æ³¨é‡Šå¼•å¯¼çš„è§†è§‰æç¤ºçš„æ„ŸçŸ¥-åŠ¨ä½œç®¡é“ï¼Œé€šè¿‡è¾¹ç•Œæ¡†æ³¨é‡Šè¯†åˆ«å¯æŠ“å–ç‰©ä½“å’Œæ”¾ç½®ä½ç½®ï¼Œæä¾›ç»“æ„åŒ–çš„ç©ºé—´æŒ‡å¯¼ã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºå˜æ¢å™¨çš„åŠ¨ä½œåˆ†å—ï¼ˆACTï¼‰æ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼Œä½¿æœºå™¨äººæ‰‹è‡‚èƒ½å¤Ÿä»äººç±»ç¤ºèŒƒä¸­é¢„æµ‹åˆ†å—çš„åŠ¨ä½œåºåˆ—ï¼Œä»è€Œå®ç°å¹³æ»‘ã€é€‚åº”æ€§å¼ºä¸”æ•°æ®é©±åŠ¨çš„æŠ“å–ä¸æ”¾ç½®æ“ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨é›¶å”®ç¯å¢ƒä¸­æé«˜äº†æŠ“å–å‡†ç¡®æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¾¿åˆ©åº—ç¯å¢ƒä¸­æœºå™¨äººæŠ“å–ä¸æ”¾ç½®ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œç°æœ‰æ–¹æ³•åœ¨ç‰©ä½“å¯†é›†ã€é®æŒ¡åŠç‰©ä½“å±æ€§å˜åŒ–æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æŠ“å–å¤±è´¥å’Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ³¨é‡Šå¼•å¯¼çš„è§†è§‰æç¤ºæ–¹æ³•ï¼Œé€šè¿‡è¾¹ç•Œæ¡†æ³¨é‡Šæä¾›ç»“æ„åŒ–çš„ç©ºé—´ä¿¡æ¯ï¼Œç»“åˆåŠ¨ä½œåˆ†å—ä¸å˜æ¢å™¨ï¼ˆACTï¼‰ç®—æ³•ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å’Œæ‰§è¡ŒæŠ“å–ä¸æ”¾ç½®ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ„ŸçŸ¥æ¨¡å—ï¼ˆç”¨äºç‰©ä½“æ£€æµ‹å’Œä½ç½®è¯†åˆ«ï¼‰ã€åŠ¨ä½œé¢„æµ‹æ¨¡å—ï¼ˆåŸºäºACTç®—æ³•è¿›è¡ŒåŠ¨ä½œåˆ†å—å­¦ä¹ ï¼‰å’Œæ‰§è¡Œæ¨¡å—ï¼ˆæ§åˆ¶æœºå™¨äººæ‰‹è‡‚è¿›è¡ŒæŠ“å–ä¸æ”¾ç½®ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥æ³¨é‡Šå¼•å¯¼çš„è§†è§‰æç¤ºä¸åŠ¨ä½œåˆ†å—ç›¸ç»“åˆçš„ç­–ç•¥ï¼Œä½¿å¾—æœºå™¨äººèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´çµæ´»åœ°é€‚åº”å˜åŒ–ï¼Œä¸ä¼ ç»Ÿé€æ­¥è§„åˆ’æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ“ä½œçš„æµç•…æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºå˜æ¢å™¨çš„ç½‘ç»œç»“æ„ï¼Œè®¾ç½®äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åŠ¨ä½œé¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡å¤§é‡äººç±»ç¤ºèŒƒæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ³¨é‡Šå¼•å¯¼çš„è§†è§‰æç¤ºæ–¹æ³•åï¼Œæœºå™¨äººæŠ“å–æˆåŠŸç‡æé«˜äº†çº¦20%ï¼ŒæŠ“å–å‡†ç¡®æ€§å’Œé€‚åº”æ€§åœ¨å¤æ‚ç¯å¢ƒä¸­æ˜¾è‘—å¢å¼ºï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬ä¾¿åˆ©åº—ã€ä»“å‚¨ç‰©æµåŠå…¶ä»–éœ€è¦é«˜æ•ˆç‰©ä½“æŠ“å–ä¸æ”¾ç½®çš„ç¯å¢ƒã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘äººåŠ›æˆæœ¬ï¼Œæé«˜å·¥ä½œæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½é›¶å”®å’Œè‡ªåŠ¨åŒ–ç‰©æµçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic pick-and-place tasks in convenience stores pose challenges due to dense object arrangements, occlusions, and variations in object properties such as color, shape, size, and texture. These factors complicate trajectory planning and grasping. This paper introduces a perception-action pipeline leveraging annotation-guided visual prompting, where bounding box annotations identify both pickable objects and placement locations, providing structured spatial guidance. Instead of traditional step-by-step planning, we employ Action Chunking with Transformers (ACT) as an imitation learning algorithm, enabling the robotic arm to predict chunked action sequences from human demonstrations. This facilitates smooth, adaptive, and data-driven pick-and-place operations. We evaluate our system based on success rate and visual analysis of grasping behavior, demonstrating improved grasp accuracy and adaptability in retail environments.

