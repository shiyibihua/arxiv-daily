---
layout: default
title: GeoVLA: Empowering 3D Representations in Vision-Language-Action Models
---

# GeoVLA: Empowering 3D Representations in Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09071" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09071v2</a>
  <a href="https://arxiv.org/pdf/2508.09071.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09071v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09071v2', 'GeoVLA: Empowering 3D Representations in Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, Jiale Cao

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12 (æ›´æ–°: 2025-08-13)

**å¤‡æ³¨**: The project is visible at https://linsun449.github.io/GeoVLA/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGeoVLAä»¥è§£å†³VLAæ¨¡å‹åœ¨3Dä¿¡æ¯æ•´åˆä¸­çš„ä¸è¶³**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `3Då‡ ä½•ä¿¡æ¯` `æœºå™¨äººæ“ä½œ` `å¤šæ¨¡æ€èåˆ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„VLAæ¨¡å‹ä¸»è¦ä¾èµ–2Dè§†è§‰è¾“å…¥ï¼Œç¼ºä¹å¯¹3Då‡ ä½•ä¿¡æ¯çš„åˆ©ç”¨ï¼Œå¯¼è‡´ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ä¸è¶³ã€‚
2. GeoVLAæ¡†æ¶é€šè¿‡æ•´åˆ3Dä¿¡æ¯ï¼Œä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å’Œå®šåˆ¶çš„ç‚¹ç¼–ç å™¨ç”Ÿæˆèåˆçš„è§†è§‰-è¯­è¨€å’Œ3Då‡ ä½•åµŒå…¥ã€‚
3. åœ¨LIBEROå’ŒManiSkill2æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGeoVLAå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨çœŸå®ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä½œä¸ºä¸€ç§æ–°å…´æ–¹æ³•ï¼Œèƒ½å¤Ÿä½¿æœºå™¨äººéµå¾ªè¯­è¨€æŒ‡ä»¤å¹¶é¢„æµ‹ç›¸åº”åŠ¨ä½œã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLAæ¨¡å‹ä¸»è¦ä¾èµ–äº2Dè§†è§‰è¾“å…¥ï¼Œå¿½è§†äº†3Dç‰©ç†ä¸–ç•Œä¸­çš„ä¸°å¯Œå‡ ä½•ä¿¡æ¯ï¼Œé™åˆ¶äº†å…¶ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚æœ¬æ–‡æå‡ºäº†GeoVLAï¼Œä¸€ä¸ªæ–°é¢–çš„VLAæ¡†æ¶ï¼Œæœ‰æ•ˆæ•´åˆ3Dä¿¡æ¯ä»¥æå‡æœºå™¨äººæ“ä½œèƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å¤„ç†å›¾åƒå’Œè¯­è¨€æŒ‡ä»¤ï¼Œæå–èåˆçš„è§†è§‰-è¯­è¨€åµŒå…¥ã€‚åŒæ—¶ï¼Œå°†æ·±åº¦å›¾è½¬æ¢ä¸ºç‚¹äº‘ï¼Œå¹¶é‡‡ç”¨å®šåˆ¶çš„ç‚¹ç¼–ç å™¨ç”Ÿæˆç‹¬ç«‹çš„3Då‡ ä½•åµŒå…¥ã€‚è¿™äº›åµŒå…¥è¢«è¿æ¥å¹¶ç”±æˆ‘ä»¬æå‡ºçš„ç©ºé—´æ„ŸçŸ¥åŠ¨ä½œä¸“å®¶å¤„ç†ï¼Œç”Ÿæˆç²¾ç¡®çš„åŠ¨ä½œåºåˆ—ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„å¹¿æ³›å®éªŒï¼ŒGeoVLAå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤„ç†3Dä¿¡æ¯æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å…¶å¯¹ç©ºé—´æ„ŸçŸ¥å’Œé€‚åº”æ€§çš„é™åˆ¶ã€‚ç°æœ‰æ¨¡å‹ä¸»è¦ä¾èµ–äº2Dè§†è§‰è¾“å…¥ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨3Då‡ ä½•ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGeoVLAé€šè¿‡å¼•å…¥3Dä¿¡æ¯ï¼Œç»“åˆè§†è§‰-è¯­è¨€æ¨¡å‹å’Œå®šåˆ¶çš„ç‚¹ç¼–ç å™¨ï¼Œç”Ÿæˆèåˆçš„è§†è§‰-è¯­è¨€åµŒå…¥å’Œç‹¬ç«‹çš„3Då‡ ä½•åµŒå…¥ï¼Œä»è€Œæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGeoVLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”¨äºå¤„ç†å›¾åƒå’Œè¯­è¨€æŒ‡ä»¤ï¼Œç”Ÿæˆè§†è§‰-è¯­è¨€åµŒå…¥ï¼›ç‚¹åµŒå…¥ç½‘ç»œç”¨äºå°†æ·±åº¦å›¾è½¬æ¢ä¸ºç‚¹äº‘å¹¶ç”Ÿæˆ3Då‡ ä½•åµŒå…¥ã€‚éšåï¼Œè¿™äº›åµŒå…¥è¢«è¿æ¥å¹¶è¾“å…¥åˆ°3Då¢å¼ºåŠ¨ä½œä¸“å®¶ä¸­ï¼Œç”Ÿæˆç²¾ç¡®çš„åŠ¨ä½œåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šGeoVLAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æœ‰æ•ˆæ•´åˆäº†3Då‡ ä½•ä¿¡æ¯ä¸è§†è§‰-è¯­è¨€ä¿¡æ¯ï¼Œæå‡ºäº†3Då¢å¼ºåŠ¨ä½œä¸“å®¶ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä½¿ç”¨äº†å®šåˆ¶çš„ç‚¹ç¼–ç å™¨æ¥å¤„ç†ç‚¹äº‘æ•°æ®ï¼Œç¡®ä¿ç”Ÿæˆçš„3Då‡ ä½•åµŒå…¥èƒ½å¤Ÿç‹¬ç«‹äºè§†è§‰-è¯­è¨€åµŒå…¥ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆï¼Œä»¥ä¼˜åŒ–åŠ¨ä½œåºåˆ—çš„ç”Ÿæˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GeoVLAåœ¨LIBEROå’ŒManiSkill2æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç°å‡ºåœ¨çœŸå®ä»»åŠ¡ä¸­çš„æ˜¾è‘—é²æ£’æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é«˜åº¦é€‚åº”æ€§ã€å°ºåº¦æ„ŸçŸ¥å’Œè§†è§’ä¸å˜æ€§æ–¹é¢ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GeoVLAçš„ç ”ç©¶æˆæœåœ¨æœºå™¨äººæ“ä½œã€è‡ªåŠ¨åŒ–åˆ¶é€ å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹3Dç¯å¢ƒçš„ç†è§£å’Œé€‚åº”èƒ½åŠ›ï¼ŒGeoVLAèƒ½å¤Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­å®ç°æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have emerged as a promising approach for enabling robots to follow language instructions and predict corresponding actions. However, current VLA models mainly rely on 2D visual inputs, neglecting the rich geometric information in the 3D physical world, which limits their spatial awareness and adaptability. In this paper, we present GeoVLA, a novel VLA framework that effectively integrates 3D information to advance robotic manipulation. It uses a vision-language model (VLM) to process images and language instructions,extracting fused vision-language embeddings. In parallel, it converts depth maps into point clouds and employs a customized point encoder, called Point Embedding Network, to generate 3D geometric embeddings independently. These produced embeddings are then concatenated and processed by our proposed spatial-aware action expert, called 3D-enhanced Action Expert, which combines information from different sensor modalities to produce precise action sequences. Through extensive experiments in both simulation and real-world environments, GeoVLA demonstrates superior performance and robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2 simulation benchmarks and shows remarkable robustness in real-world tasks requiring height adaptability, scale awareness and viewpoint invariance.

