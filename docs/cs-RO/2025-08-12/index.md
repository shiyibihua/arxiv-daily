---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-08-12
---

# cs.ROï¼ˆ2025-08-12ï¼‰

ğŸ“Š å…± **10** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (9 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250808896v4-towards-affordance-aware-robotic-dexterous-grasping-with-human-like-.html">Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors</a></td>
  <td>æå‡ºAffordDexä»¥è§£å†³æœºå™¨äººçµå·§æŠ“å–ä¸­çš„äººç±»å§¿æ€ä¸åŠŸèƒ½é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">teacher-student</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08896v4" data-paper-url="./papers/250808896v4-towards-affordance-aware-robotic-dexterous-grasping-with-human-like-.html" onclick="toggleFavorite(this, '2508.08896v4', 'Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250809354v1-clf-rl-control-lyapunov-function-guided-reinforcement-learning.html">CLF-RL: Control Lyapunov Function Guided Reinforcement Learning</a></td>
  <td>æå‡ºCLF-RLä»¥è§£å†³å¼ºåŒ–å­¦ä¹ åœ¨åŒè¶³æœºå™¨äººæ§åˆ¶ä¸­çš„å¥–åŠ±è®¾è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">bipedal</span> <span class="paper-tag">biped</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09354v1" data-paper-url="./papers/250809354v1-clf-rl-control-lyapunov-function-guided-reinforcement-learning.html" onclick="toggleFavorite(this, '2508.09354v1', 'CLF-RL: Control Lyapunov Function Guided Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250809071v2-geovla-empowering-3d-representations-in-vision-language-action-model.html">GeoVLA: Empowering 3D Representations in Vision-Language-Action Models</a></td>
  <td>æå‡ºGeoVLAä»¥è§£å†³VLAæ¨¡å‹åœ¨3Dä¿¡æ¯æ•´åˆä¸­çš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09071v2" data-paper-url="./papers/250809071v2-geovla-empowering-3d-representations-in-vision-language-action-model.html" onclick="toggleFavorite(this, '2508.09071v2', 'GeoVLA: Empowering 3D Representations in Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250808624v2-communication-efficient-robotic-mixed-reality-with-gaussian-splattin.html">Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization</a></td>
  <td>æå‡ºé«˜æ–¯å–·æº…è·¨å±‚ä¼˜åŒ–ä»¥è§£å†³æœºå™¨äººæ··åˆç°å®ä¸­çš„ä½é€šä¿¡æˆæœ¬é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08624v2" data-paper-url="./papers/250808624v2-communication-efficient-robotic-mixed-reality-with-gaussian-splattin.html" onclick="toggleFavorite(this, '2508.08624v2', 'Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250808707v1-towards-safe-imitation-learning-via-potential-field-guided-flow-matc.html">Towards Safe Imitation Learning via Potential Field-Guided Flow Matching</a></td>
  <td>æå‡ºæ½œåœºå¼•å¯¼æµåŒ¹é…ç­–ç•¥ä»¥è§£å†³æ¨¡ä»¿å­¦ä¹ ä¸­çš„å®‰å…¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">flow matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08707v1" data-paper-url="./papers/250808707v1-towards-safe-imitation-learning-via-potential-field-guided-flow-matc.html" onclick="toggleFavorite(this, '2508.08707v1', 'Towards Safe Imitation Learning via Potential Field-Guided Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250808982v1-unsupervised-skill-discovery-as-exploration-for-learning-agile-locom.html">Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion</a></td>
  <td>æå‡ºSDAXæ¡†æ¶ä»¥è§£å†³è…¿éƒ¨æœºå™¨äººçµæ´»è¿åŠ¨å­¦ä¹ ä¸­çš„æ¢ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged robot</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08982v1" data-paper-url="./papers/250808982v1-unsupervised-skill-discovery-as-exploration-for-learning-agile-locom.html" onclick="toggleFavorite(this, '2508.08982v1', 'Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250808706v2-omnivtla-vision-tactile-language-action-model-with-semantic-aligned-.html">OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing</a></td>
  <td>æå‡ºOmniVTLAä»¥è§£å†³è§¦è§‰æ„ŸçŸ¥åœ¨æœºå™¨äººæ“ä½œä¸­çš„ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">vision-language-action</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08706v2" data-paper-url="./papers/250808706v2-omnivtla-vision-tactile-language-action-model-with-semantic-aligned-.html" onclick="toggleFavorite(this, '2508.08706v2', 'OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250808748v1-visual-prompting-for-robotic-manipulation-with-annotation-guided-pic.html">Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT</a></td>
  <td>æå‡ºåŸºäºæ³¨é‡Šå¼•å¯¼çš„è§†è§‰æç¤ºä»¥è§£å†³æœºå™¨äººæŠ“å–ä¸æ”¾ç½®é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08748v1" data-paper-url="./papers/250808748v1-visual-prompting-for-robotic-manipulation-with-annotation-guided-pic.html" onclick="toggleFavorite(this, '2508.08748v1', 'Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250808983v1-rational-inverse-reasoning.html">Rational Inverse Reasoning</a></td>
  <td>æå‡ºç†æ€§é€†æ¨æ¨ç†æ¡†æ¶ä»¥è§£å†³æœºå™¨äººæ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08983v1" data-paper-url="./papers/250808983v1-rational-inverse-reasoning.html" onclick="toggleFavorite(this, '2508.08983v1', 'Rational Inverse Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250808574v2-deepfleet-multi-agent-foundation-models-for-mobile-robots.html">DeepFleet: Multi-Agent Foundation Models for Mobile Robots</a></td>
  <td>æå‡ºDeepFleetä»¥æ”¯æŒå¤§è§„æ¨¡ç§»åŠ¨æœºå™¨äººé˜Ÿä¼çš„åè°ƒä¸è§„åˆ’</td>
  <td class="tags-cell"><span class="paper-tag">decision transformer</span> <span class="paper-tag">spatial relationship</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08574v2" data-paper-url="./papers/250808574v2-deepfleet-multi-agent-foundation-models-for-mobile-robots.html" onclick="toggleFavorite(this, '2508.08574v2', 'DeepFleet: Multi-Agent Foundation Models for Mobile Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)