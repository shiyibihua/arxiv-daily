---
layout: default
title: Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control
---

# Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23650" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.23650v1</a>
  <a href="https://arxiv.org/pdf/2512.23650.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23650v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23650v1', 'Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RoboPerformï¼šæå‡ºä¸€ç§åŸºäºéŸ³é¢‘æ§åˆ¶çš„æ‹Ÿäººæœºå™¨äººè‡ªç”±è¿åŠ¨ç”Ÿæˆæ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `éŸ³é¢‘æ§åˆ¶` `æ‹Ÿäººæœºå™¨äºº` `è¿åŠ¨ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `äººæœºäº¤äº’` `é£æ ¼è¿ç§»` `ResMoE` `ä½å»¶è¿Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‹Ÿäººæœºå™¨äººç¼ºä¹è¡¨ç°åŠ›ï¼Œé€šå¸¸å±€é™äºé¢„å®šä¹‰çš„åŠ¨ä½œæˆ–ç¨€ç–çš„æŒ‡ä»¤ï¼Œéš¾ä»¥å®ç°å³å…´è¡¨æ¼”ã€‚
2. RoboPerformå°†éŸ³é¢‘ä½œä¸ºéšå¼é£æ ¼ä¿¡å·ï¼Œé¿å…äº†æ˜¾å¼çš„è¿åŠ¨é‡å»ºï¼Œä»è€Œé™ä½äº†å»¶è¿Ÿå¹¶æé«˜äº†ä¿çœŸåº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRoboPerformåœ¨ç‰©ç†åˆç†æ€§å’ŒéŸ³é¢‘å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®éŸ³é¢‘è¿›è¡Œèˆè¹ˆå’Œæ‰‹åŠ¿è¡¨æ¼”ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºRoboPerformï¼Œé¦–ä¸ªç»Ÿä¸€çš„éŸ³é¢‘åˆ°è¿åŠ¨æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»éŸ³é¢‘ç”ŸæˆéŸ³ä¹é©±åŠ¨çš„èˆè¹ˆå’Œè¯­éŸ³é©±åŠ¨çš„ä¼´éšæ‰‹åŠ¿ã€‚è¯¥æ¡†æ¶éµå¾ªâ€œè¿åŠ¨=å†…å®¹+é£æ ¼â€çš„æ ¸å¿ƒåŸåˆ™ï¼Œå°†éŸ³é¢‘è§†ä¸ºéšå¼çš„é£æ ¼ä¿¡å·ï¼Œæ— éœ€æ˜¾å¼çš„è¿åŠ¨é‡å»ºã€‚RoboPerformé›†æˆäº†ResMoEæ•™å¸ˆç­–ç•¥ä»¥é€‚åº”ä¸åŒçš„è¿åŠ¨æ¨¡å¼ï¼Œä»¥åŠåŸºäºæ‰©æ•£çš„studentç­–ç•¥ä»¥æ³¨å…¥éŸ³é¢‘é£æ ¼ã€‚è¿™ç§æ— éœ€é‡å®šå‘çš„è®¾è®¡ç¡®ä¿äº†ä½å»¶è¿Ÿå’Œé«˜ä¿çœŸåº¦ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼ŒRoboPerformåœ¨ç‰©ç†åˆç†æ€§å’ŒéŸ³é¢‘å¯¹é½æ–¹é¢å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼ŒæˆåŠŸåœ°å°†æœºå™¨äººè½¬å˜ä¸ºèƒ½å¤Ÿå¯¹éŸ³é¢‘åšå‡ºååº”çš„è¡¨æ¼”è€…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•ä¾èµ–äºæ˜¾å¼çš„è¿åŠ¨é‡å»ºï¼Œå°†éŸ³é¢‘è½¬æ¢ä¸ºè¿åŠ¨å†é‡å®šå‘åˆ°æœºå™¨äººï¼Œå¯¼è‡´çº§è”è¯¯å·®ã€é«˜å»¶è¿Ÿä»¥åŠå£°å­¦-é©±åŠ¨æ˜ å°„çš„è„±èŠ‚ã€‚å› æ­¤ï¼Œå¦‚ä½•ç›´æ¥ä»éŸ³é¢‘ç”Ÿæˆé«˜è´¨é‡ã€ä½å»¶è¿Ÿçš„æœºå™¨äººè¿åŠ¨æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRoboPerformçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†éŸ³é¢‘è§†ä¸ºä¸€ç§éšå¼çš„é£æ ¼ä¿¡å·ï¼Œå¹¶ç›´æ¥å°†å…¶æ³¨å…¥åˆ°è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œé¿å…äº†ä¸­é—´çš„è¿åŠ¨é‡å»ºæ­¥éª¤ã€‚è¿™ç§æ–¹æ³•åŸºäºâ€œè¿åŠ¨=å†…å®¹+é£æ ¼â€çš„åŸåˆ™ï¼Œè®¤ä¸ºéŸ³é¢‘å¯ä»¥æä¾›è¿åŠ¨çš„é£æ ¼ä¿¡æ¯ï¼Œè€Œæ— éœ€æ˜¾å¼åœ°å®šä¹‰è¿åŠ¨çš„ç»†èŠ‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoboPerformæ¡†æ¶åŒ…å«ä¸€ä¸ªResMoEæ•™å¸ˆç­–ç•¥å’Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„studentç­–ç•¥ã€‚ResMoEæ•™å¸ˆç­–ç•¥ç”¨äºå­¦ä¹ å„ç§è¿åŠ¨æ¨¡å¼ï¼Œä»è€Œé€‚åº”ä¸åŒçš„è¿åŠ¨å†…å®¹ã€‚åŸºäºæ‰©æ•£çš„studentç­–ç•¥åˆ™è´Ÿè´£å°†éŸ³é¢‘é£æ ¼æ³¨å…¥åˆ°è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚æ•´ä¸ªæ¡†æ¶æ— éœ€è¿åŠ¨é‡å®šå‘ï¼Œç›´æ¥ç”Ÿæˆæœºå™¨äººçš„è¿åŠ¨æ§åˆ¶ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šRoboPerformçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ— éœ€æ˜¾å¼è¿åŠ¨é‡å»ºçš„æ¶æ„ã€‚é€šè¿‡å°†éŸ³é¢‘ä½œä¸ºéšå¼é£æ ¼ä¿¡å·ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé£æ ¼æ³¨å…¥ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€ä½å»¶è¿Ÿçš„æœºå™¨äººè¿åŠ¨ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„çº§è”è¯¯å·®å’Œå»¶è¿Ÿé—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šResMoEæ•™å¸ˆç­–ç•¥ä½¿ç”¨æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰æ¥å­¦ä¹ ä¸åŒçš„è¿åŠ¨æ¨¡å¼ã€‚åŸºäºæ‰©æ•£çš„studentç­–ç•¥ä½¿ç”¨æ‰©æ•£æ¨¡å‹å°†éŸ³é¢‘ç‰¹å¾èå…¥åˆ°è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è¿åŠ¨å¹³æ»‘æ€§æŸå¤±ã€éŸ³é¢‘å¯¹é½æŸå¤±å’Œç‰©ç†åˆç†æ€§æŸå¤±ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨æ—¢è‡ªç„¶åˆç¬¦åˆç‰©ç†è§„å¾‹ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23650v1/x2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23650v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23650v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

RoboPerformåœ¨å®éªŒä¸­è¡¨ç°å‡ºè‰¯å¥½çš„ç‰©ç†åˆç†æ€§å’ŒéŸ³é¢‘å¯¹é½æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRoboPerformèƒ½å¤Ÿç”Ÿæˆæ›´è‡ªç„¶ã€æ›´æµç•…çš„æœºå™¨äººè¿åŠ¨ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°ä¸éŸ³é¢‘åŒæ­¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboPerformæˆåŠŸåœ°å°†æœºå™¨äººè½¬å˜ä¸ºèƒ½å¤Ÿå¯¹éŸ³é¢‘åšå‡ºååº”çš„è¡¨æ¼”è€…ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RoboPerformå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬å¨±ä¹æœºå™¨äººã€äººæœºäº¤äº’ã€åº·å¤è®­ç»ƒç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®éŸ³ä¹è¿›è¡Œèˆè¹ˆè¡¨æ¼”ï¼Œæˆ–è€…æ ¹æ®è¯­éŸ³è¿›è¡Œä¼´éšæ‰‹åŠ¿ï¼Œä»è€Œæé«˜äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œè¶£å‘³æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºåº·å¤è®­ç»ƒï¼Œé€šè¿‡éŸ³ä¹æˆ–è¯­éŸ³å¼•å¯¼æ‚£è€…è¿›è¡Œè¿åŠ¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humans intuitively move to sound, but current humanoid robots lack expressive improvisational capabilities, confined to predefined motions or sparse commands. Generating motion from audio and then retargeting it to robots relies on explicit motion reconstruction, leading to cascaded errors, high latency, and disjointed acoustic-actuation mapping. We propose RoboPerform, the first unified audio-to-locomotion framework that can directly generate music-driven dance and speech-driven co-speech gestures from audio. Guided by the core principle of "motion = content + style", the framework treats audio as implicit style signals and eliminates the need for explicit motion reconstruction. RoboPerform integrates a ResMoE teacher policy for adapting to diverse motion patterns and a diffusion-based student policy for audio style injection. This retargeting-free design ensures low latency and high fidelity. Experimental validation shows that RoboPerform achieves promising results in physical plausibility and audio alignment, successfully transforming robots into responsive performers capable of reacting to audio.

