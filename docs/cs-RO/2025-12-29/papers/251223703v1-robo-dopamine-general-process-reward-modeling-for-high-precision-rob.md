---
layout: default
title: "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation"
---

# Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23703" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.23703v1</a>
  <a href="https://arxiv.org/pdf/2512.23703.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23703v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23703v1', 'Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huajie Tan, Sixiang Chen, Yijie Xu, Zixiao Wang, Yuheng Ji, Cheng Chi, Yaoxu Lyu, Zhongxia Zhao, Xiansheng Chen, Peterson Co, Shaoxuan Xie, Guocai Yao, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-29

**å¤‡æ³¨**: 27 pages, 11 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDopamine-Rewardä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„å¥–åŠ±å‡½æ•°è®¾è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¿‡ç¨‹å¥–åŠ±æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `å¤šè§†è§’è¾“å…¥` `å¥–åŠ±å¡‘é€ ` `ç­–ç•¥å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­é¢ä¸´å¥–åŠ±è¯„ä¼°ä¸å¯é å’Œç†è®ºåŸºç¡€ä¸æ‰å®çš„æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºDopamine-Rewardï¼Œé€šè¿‡å¤šè§†è§’è¾“å…¥å­¦ä¹ å…·æœ‰æ­¥æ€æ„è¯†çš„é€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGRMåœ¨å¥–åŠ±è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒDopamine-RLåœ¨ç­–ç•¥å­¦ä¹ æ•ˆç‡ä¸Šæ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºç°å®æœºå™¨äººæ—¶ï¼Œè®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°æ˜¯ä¸»è¦éšœç¢ã€‚å°½ç®¡åŸºäºå­¦ä¹ çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œä½†å®ƒä»¬é€šå¸¸å—åˆ°å¥–åŠ±æ¨¡å‹ç¼ºä¹æ­¥æ€æ„è¯†å’Œä¾èµ–å•è§†è§’æ„ŸçŸ¥çš„é™åˆ¶ï¼Œå¯¼è‡´å¯¹ç»†ç²’åº¦æ“ä½œè¿›å±•çš„è¯„ä¼°ä¸å¯é ã€‚æ­¤å¤–ï¼Œå¥–åŠ±å¡‘é€ ç¨‹åºåœ¨ç†è®ºä¸Šä¸å¤Ÿä¸¥è°¨ï¼Œå¸¸å¸¸è¯±å¯¼å‡ºè¯¯å¯¼æ€§çš„è¯­ä¹‰é™·é˜±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•Dopamine-Rewardï¼Œæ—¨åœ¨ä»å¤šè§†è§’è¾“å…¥ä¸­å­¦ä¹ é€šç”¨çš„ã€å…·æœ‰æ­¥æ€æ„è¯†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚æ ¸å¿ƒæ˜¯æˆ‘ä»¬çš„é€šç”¨å¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰ï¼Œå…¶è®­ç»ƒåŸºäºè¶…è¿‡3400å°æ—¶çš„æ•°æ®é›†ï¼Œåˆ©ç”¨é€æ­¥å¥–åŠ±ç¦»æ•£åŒ–å’Œå¤šè§†è§’å¥–åŠ±èåˆæ¥å…‹æœæ„ŸçŸ¥é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRMåœ¨å¥–åŠ±è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œè€ŒåŸºäºGRMçš„Dopamine-RLæ˜¾è‘—æé«˜äº†ç­–ç•¥å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨ç°å®æœºå™¨äººæ“ä½œä¸­ï¼Œç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¥–åŠ±å‡½æ•°è®¾è®¡ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¥–åŠ±æ¨¡å‹ç¼ºä¹æ­¥æ€æ„è¯†å’Œä¾èµ–å•ä¸€è§†è§’æ„ŸçŸ¥çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºDopamine-Rewardæ–¹æ³•ï¼Œé€šè¿‡å¤šè§†è§’è¾“å…¥æ¥å­¦ä¹ é€šç”¨çš„ã€å…·æœ‰æ­¥æ€æ„è¯†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œæé«˜å¥–åŠ±è¯„ä¼°çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é€šç”¨å¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰å’ŒDopamine-RLæ¡†æ¶ã€‚GRMé€šè¿‡é€æ­¥å¥–åŠ±ç¦»æ•£åŒ–å’Œå¤šè§†è§’å¥–åŠ±èåˆæ¥å®ç°ç»“æ„åŒ–ç†è§£ï¼Œè€ŒDopamine-RLåˆ™é‡‡ç”¨ç†è®ºä¸Šä¸¥è°¨çš„ç­–ç•¥ä¸å˜å¥–åŠ±å¡‘é€ æ–¹æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†é€æ­¥å¥–åŠ±ç¦»æ•£åŒ–å’Œå¤šè§†è§’å¥–åŠ±èåˆï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„æ„ŸçŸ¥é™åˆ¶ï¼Œå¹¶é€šè¿‡ç†è®ºä¸Šä¸¥è°¨çš„å¥–åŠ±å¡‘é€ é¿å…äº†è¯­ä¹‰é™·é˜±ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GRMä¸­ï¼Œä½¿ç”¨äº†å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼ˆ3400å°æ—¶ä»¥ä¸Šï¼‰ï¼Œå¹¶è®¾è®¡äº†é€‚å½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23703v1/x2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23703v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23703v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRMåœ¨å¥–åŠ±è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒDopamine-RLåœ¨ç­–ç•¥å­¦ä¹ æ•ˆç‡ä¸Šæ˜¾è‘—æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶ï¼ŒDopamine-RLèƒ½å¤Ÿåœ¨ä»…150æ¬¡åœ¨çº¿å›åˆå†…å°†æˆåŠŸç‡ä»æ¥è¿‘é›¶æå‡è‡³95%ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å·¥ä¸šæœºå™¨äººã€æœåŠ¡æœºå™¨äººå’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œç²¾åº¦å’Œæ•ˆç‡ã€‚æœªæ¥ï¼ŒDopamine-Rewardæ–¹æ³•å¯èƒ½ä¼šæ¨åŠ¨æ›´å¹¿æ³›çš„æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ï¼Œä½¿å…¶åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io

