---
layout: default
title: Latent Action Pretraining Through World Modeling
---

# Latent Action Pretraining Through World Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18428" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18428v1</a>
  <a href="https://arxiv.org/pdf/2509.18428.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18428v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18428v1', 'Latent Action Pretraining Through World Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bahey Tharwat, Yara Nasser, Ali Abouzeid, Ian Reid

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLAWMï¼Œé€šè¿‡ä¸–ç•Œå»ºæ¨¡è¿›è¡Œæ½œåœ¨åŠ¨ä½œé¢„è®­ç»ƒï¼Œæå‡æœºå™¨äººæ“ä½œä»»åŠ¡æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `æ¨¡ä»¿å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `ä¸–ç•Œå»ºæ¨¡` `æ½œåœ¨åŠ¨ä½œè¡¨ç¤º` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `é¢„è®­ç»ƒ` `æ— æ ‡ç­¾æ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹ä¾èµ–å¤§è§„æ¨¡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œæ¨¡å‹ä½“ç§¯å¤§ï¼Œéš¾ä»¥åœ¨çœŸå®åœºæ™¯éƒ¨ç½²ã€‚
2. LAWMæ¡†æ¶é€šè¿‡ä¸–ç•Œå»ºæ¨¡ï¼Œä»æ— æ ‡ç­¾è§†é¢‘ä¸­å­¦ä¹ æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºï¼Œå®ç°è‡ªç›‘ç£é¢„è®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLAWMåœ¨LIBEROåŸºå‡†å’ŒçœŸå®ç¯å¢ƒä¸­ï¼Œæ€§èƒ½ä¼˜äºå…¶ä»–é¢„è®­ç»ƒæ–¹æ³•ï¼Œä¸”æ›´é«˜æ•ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨å­¦ä¹ éµå¾ªè¯­è¨€æŒ‡ä»¤çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚æœ€å…ˆè¿›çš„VLAæ¨¡å‹ï¼Œå¦‚OpenVLAå’Œ$Ï€_{0}$ï¼Œæ˜¯åœ¨é€šè¿‡é¥æ“ä½œæ”¶é›†çš„å¤§è§„æ¨¡ã€æ‰‹åŠ¨æ ‡è®°çš„åŠ¨ä½œæ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€‚æœ€è¿‘çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬LAPAå’Œvilla-Xï¼Œå¼•å…¥äº†æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºï¼Œé€šè¿‡å¯¹å¸§ä¹‹é—´çš„æŠ½è±¡è§†è§‰å˜åŒ–è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œèƒ½å¤Ÿåœ¨æœªæ ‡è®°çš„æ•°æ®é›†ä¸Šè¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒã€‚å°½ç®¡è¿™äº›æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç»“æœï¼Œä½†å®ƒä»¬åºå¤§çš„æ¨¡å‹å°ºå¯¸ä½¿å¾—åœ¨ç°å®ä¸–ç•Œä¸­çš„éƒ¨ç½²å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LAWMï¼Œä¸€ä¸ªæ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸–ç•Œå»ºæ¨¡ä»æœªæ ‡è®°çš„è§†é¢‘æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºï¼Œä»¥è‡ªç›‘ç£çš„æ–¹å¼é¢„è®­ç»ƒæ¨¡ä»¿å­¦ä¹ æ¨¡å‹ã€‚è¿™äº›è§†é¢‘å¯ä»¥æ¥è‡ªæœºå™¨äººå½•åƒæˆ–äººç±»ä½¿ç”¨æ—¥å¸¸ç‰©å“æ‰§è¡ŒåŠ¨ä½œçš„è§†é¢‘ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ—¨åœ¨æœ‰æ•ˆåœ°è·¨ä»»åŠ¡ã€ç¯å¢ƒå’Œå®ä½“è¿›è¡Œè¿ç§»ã€‚åœ¨LIBEROåŸºå‡†æµ‹è¯•å’ŒçœŸå®ç¯å¢ƒä¸­ï¼Œå®ƒä¼˜äºä½¿ç”¨çœŸå®æœºå™¨äººåŠ¨ä½œè®­ç»ƒçš„æ¨¡å‹å’Œç±»ä¼¼çš„é¢„è®­ç»ƒæ–¹æ³•ï¼ŒåŒæ—¶å¯¹äºç°å®ä¸–ç•Œç¯å¢ƒæ¥è¯´æ•ˆç‡æ›´é«˜ä¸”æ›´å®ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹ä¾èµ–äºå¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„åŠ¨ä½œæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹çš„æ¨¡å‹å°ºå¯¸é€šå¸¸å¾ˆå¤§ï¼Œç»™åœ¨èµ„æºå—é™çš„çœŸå®ä¸–ç•Œæœºå™¨äººåº”ç”¨ä¸­çš„éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®è¿›è¡Œæœ‰æ•ˆé¢„è®­ç»ƒï¼Œå¹¶ç”Ÿæˆç´§å‡‘æ¨¡å‹çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLAWMçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸–ç•Œå»ºæ¨¡å­¦ä¹ æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼ŒLAWMåˆ©ç”¨æ— æ ‡ç­¾è§†é¢‘æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œæœºå™¨äººå½•åƒæˆ–äººç±»æ“ä½œè§†é¢‘ï¼‰æ¥å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿé¢„æµ‹è§†é¢‘å¸§ä¹‹é—´æŠ½è±¡è§†è§‰å˜åŒ–çš„æ¨¡å‹ã€‚è¿™ç§æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºå¯ä»¥æ•æ‰åˆ°åŠ¨ä½œçš„æœ¬è´¨ï¼Œè€Œæ— éœ€ä¾èµ–äºå…·ä½“çš„åŠ¨ä½œæ ‡ç­¾ã€‚é€šè¿‡å­¦ä¹ è¿™ç§è¡¨ç¤ºï¼ŒLAWMå¯ä»¥ä¸ºä¸‹æ¸¸çš„æ¨¡ä»¿å­¦ä¹ ä»»åŠ¡æä¾›ä¸€ä¸ªè‰¯å¥½çš„åˆå§‹åŒ–ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLAWMæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è§†é¢‘ç¼–ç å™¨ï¼šå°†åŸå§‹è§†é¢‘å¸§ç¼–ç æˆè§†è§‰ç‰¹å¾å‘é‡ã€‚2) æ½œåœ¨åŠ¨ä½œç¼–ç å™¨ï¼šå°†è¿ç»­çš„è§†è§‰ç‰¹å¾å‘é‡ç¼–ç æˆæ½œåœ¨åŠ¨ä½œè¡¨ç¤ºã€‚3) ä¸–ç•Œæ¨¡å‹ï¼šåˆ©ç”¨æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºé¢„æµ‹ä¸‹ä¸€å¸§çš„è§†è§‰ç‰¹å¾å‘é‡ã€‚4) æŸå¤±å‡½æ•°ï¼šç”¨äºè®­ç»ƒä¸–ç•Œæ¨¡å‹ï¼Œé¼“åŠ±æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºèƒ½å¤Ÿå‡†ç¡®åœ°é¢„æµ‹æœªæ¥çš„è§†è§‰å˜åŒ–ã€‚æ•´ä¸ªæ¡†æ¶ä»¥è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šLAWMçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ¨¡å‹æ— å…³æ€§ä»¥åŠåˆ©ç”¨ä¸–ç•Œæ¨¡å‹è¿›è¡Œæ½œåœ¨åŠ¨ä½œé¢„è®­ç»ƒã€‚ä¸ä»¥å¾€ä¾èµ–ç‰¹å®šæ¨¡å‹ç»“æ„çš„é¢„è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒLAWMå¯ä»¥ä¸å„ç§æ¨¡ä»¿å­¦ä¹ æ¨¡å‹ç»“åˆä½¿ç”¨ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸–ç•Œå»ºæ¨¡ï¼ŒLAWMèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å…·æ³›åŒ–èƒ½åŠ›çš„æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºï¼Œä»è€Œæé«˜äº†è·¨ä»»åŠ¡ã€ç¯å¢ƒå’Œå®ä½“çš„è¿ç§»èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šLAWMçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°æ¥è®­ç»ƒæ½œåœ¨åŠ¨ä½œç¼–ç å™¨ï¼Œé¼“åŠ±ç›¸ä¼¼åŠ¨ä½œçš„æ½œåœ¨è¡¨ç¤ºæ›´åŠ æ¥è¿‘ã€‚2) ä½¿ç”¨Transformerç½‘ç»œæ¥å»ºæ¨¡è§†é¢‘å¸§ä¹‹é—´çš„æ—¶åºå…³ç³»ã€‚3) ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨(VAE)æ¥å­¦ä¹ æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LAWMåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†ä½¿ç”¨çœŸå®æœºå™¨äººåŠ¨ä½œè®­ç»ƒçš„æ¨¡å‹ä»¥åŠå…¶ä»–é¢„è®­ç»ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œåœ¨çœŸå®ä¸–ç•Œæœºå™¨äººå®éªŒä¸­ï¼ŒLAWMä¹Ÿè¡¨ç°å‡ºäº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚é‡è¦çš„æ˜¯ï¼ŒLAWMåœ¨æ€§èƒ½æå‡çš„åŒæ—¶ï¼Œä¿æŒäº†è¾ƒé«˜çš„æ•ˆç‡ï¼Œä½¿å…¶æ›´é€‚åˆåœ¨èµ„æºå—é™çš„æœºå™¨äººå¹³å°ä¸Šéƒ¨ç½²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LAWMæ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–ã€è£…é…ã€å¯¼èˆªç­‰ã€‚é€šè¿‡åˆ©ç”¨å¤§é‡çš„æ— æ ‡ç­¾è§†é¢‘æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼ŒåŠ é€Ÿæœºå™¨äººå­¦ä¹ è¿‡ç¨‹ã€‚æ­¤å¤–ï¼ŒLAWMçš„æ¨¡å‹æ— å…³æ€§ä½¿å…¶èƒ½å¤Ÿä¸å„ç§æœºå™¨äººå¹³å°å’Œæ§åˆ¶ç®—æ³•ç»“åˆä½¿ç”¨ï¼Œå…·æœ‰å¾ˆé«˜çš„çµæ´»æ€§å’Œå®ç”¨ä»·å€¼ã€‚æœªæ¥ï¼ŒLAWMæœ‰æœ›æ¨åŠ¨æœºå™¨äººæŠ€æœ¯åœ¨å·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡ã€åŒ»ç–—ä¿å¥ç­‰é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and $Ï€_{0}$, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging. In this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.

