---
layout: default
title: AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback
---

# AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18384" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18384v1</a>
  <a href="https://arxiv.org/pdf/2509.18384.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18384v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18384v1', 'AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunhao Yang, Junyuan Hong, Gabriel Jacob Perin, Zhiwen Fan, Li Yin, Zhangyang Wang, Ufuk Topcu

**åˆ†ç±»**: cs.RO, cs.FL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AD-VFï¼šåŸºäºLLMè‡ªåŠ¨å¾®åˆ†ä¸å½¢å¼åŒ–åé¦ˆçš„å…å¾®è°ƒæœºå™¨äººè§„åˆ’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨äººè§„åˆ’` `å½¢å¼åŒ–éªŒè¯` `è‡ªåŠ¨å¾®åˆ†` `å…å¾®è°ƒ` `æç¤ºå·¥ç¨‹` `è§„èŒƒä¾ä»æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨æœºå™¨äººè§„åˆ’ä¸­éš¾ä»¥æ»¡è¶³å®‰å…¨å’Œç›‘ç®¡çº¦æŸï¼Œä¸”ä¾èµ–äººå·¥æ ‡æ³¨æˆ–èµ„æºå¯†é›†å‹å¾®è°ƒã€‚
2. LAD-VFåˆ©ç”¨å½¢å¼åŒ–éªŒè¯åé¦ˆï¼Œé€šè¿‡LLMè‡ªåŠ¨å¾®åˆ†è¿­ä»£ä¼˜åŒ–æç¤ºï¼Œæ— éœ€å¾®è°ƒæ¨¡å‹å‚æ•°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLAD-VFæ˜¾è‘—æå‡äº†æœºå™¨äººå¯¼èˆªå’Œæ“ä½œä»»åŠ¡çš„è§„èŒƒä¾ä»æ€§ï¼ŒæˆåŠŸç‡ä»60%æå‡è‡³90%ä»¥ä¸Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬åŒ–ä¸ºæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„å¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’ã€‚ç„¶è€Œï¼Œåœ¨ç‰©ç†ä¸–ç•Œä¸­éƒ¨ç½²LLMé©±åŠ¨çš„è§„åˆ’éœ€è¦ä¸¥æ ¼éµå®ˆå®‰å…¨å’Œç›‘ç®¡çº¦æŸï¼Œè€Œå½“å‰æ¨¡å‹ç”±äºå¹»è§‰æˆ–å¼±å¯¹é½ï¼Œç»å¸¸è¿åè¿™äº›çº¦æŸã€‚ä¼ ç»Ÿçš„æ•°æ®é©±åŠ¨å¯¹é½æ–¹æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œéœ€è¦æ˜‚è´µçš„äººå·¥æ ‡æ³¨ï¼Œè€Œæœ€è¿‘çš„å½¢å¼åŒ–åé¦ˆæ–¹æ³•ä»ç„¶ä¾èµ–äºèµ„æºå¯†é›†å‹çš„å¾®è°ƒã€‚æœ¬æ–‡æå‡ºLAD-VFï¼Œä¸€ä¸ªå…å¾®è°ƒæ¡†æ¶ï¼Œåˆ©ç”¨å½¢å¼åŒ–éªŒè¯åé¦ˆè¿›è¡Œè‡ªåŠ¨æç¤ºå·¥ç¨‹ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªå½¢å¼åŒ–éªŒè¯ä¿¡æ¯æ–‡æœ¬æŸå¤±ï¼Œå¹¶å°†å…¶ä¸LLM-AutoDiffé›†æˆï¼ŒLAD-VFè¿­ä»£åœ°æ”¹è¿›æç¤ºï¼Œè€Œä¸æ˜¯æ¨¡å‹å‚æ•°ã€‚è¿™å¸¦æ¥äº†ä¸‰ä¸ªå…³é”®å¥½å¤„ï¼šï¼ˆiï¼‰æ— éœ€å¾®è°ƒçš„å¯æ‰©å±•é€‚åº”æ€§ï¼›ï¼ˆiiï¼‰ä¸æ¨¡å—åŒ–LLMæ¶æ„çš„å…¼å®¹æ€§ï¼›ï¼ˆiiiï¼‰é€šè¿‡å¯å®¡è®¡æç¤ºå®ç°å¯è§£é‡Šçš„æ”¹è¿›ã€‚åœ¨æœºå™¨äººå¯¼èˆªå’Œæ“ä½œä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒLAD-VFæ˜¾è‘—æé«˜äº†è§„èŒƒä¾ä»æ€§ï¼Œå°†æˆåŠŸç‡ä»60%æé«˜åˆ°90%ä»¥ä¸Šã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ºå¯ä¿¡èµ–çš„ã€ç»è¿‡å½¢å¼åŒ–éªŒè¯çš„LLMé©±åŠ¨æ§åˆ¶ç³»ç»Ÿæä¾›äº†ä¸€æ¡å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰åŸºäºLLMçš„æœºå™¨äººè§„åˆ’æ–¹æ³•ï¼Œåœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´å®‰å…¨æ€§å’Œåˆè§„æ€§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæˆæœ¬é«˜æ˜‚ï¼›è¦ä¹ˆè™½ç„¶å¼•å…¥äº†å½¢å¼åŒ–éªŒè¯åé¦ˆï¼Œä½†ä»ç„¶éœ€è¦è€—è´¹å¤§é‡è®¡ç®—èµ„æºè¿›è¡Œå¾®è°ƒï¼Œéš¾ä»¥å¿«é€Ÿé€‚åº”æ–°çš„çº¦æŸæ¡ä»¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLAD-VFçš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨å½¢å¼åŒ–éªŒè¯çš„ç»“æœï¼Œè‡ªåŠ¨ä¼˜åŒ–LLMçš„æç¤ºï¼ˆPromptï¼‰ï¼Œè€Œéç›´æ¥å¾®è°ƒLLMçš„å‚æ•°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥é¿å…å¾®è°ƒå¸¦æ¥çš„è®¡ç®—å¼€é”€å’Œæ½œåœ¨çš„è¿‡æ‹Ÿåˆé£é™©ï¼ŒåŒæ—¶ä¿æŒLLMçš„é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚å½¢å¼åŒ–éªŒè¯æä¾›äº†ä¸€ç§æ˜ç¡®çš„åé¦ˆä¿¡å·ï¼ŒæŒ‡å¯¼æç¤ºçš„æ”¹è¿›æ–¹å‘ï¼Œä»è€Œæé«˜è§„åˆ’ç»“æœçš„å®‰å…¨æ€§ä¸åˆè§„æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLAD-VFæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) LLMè§„åˆ’å™¨ï¼šæ¥æ”¶è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œç”Ÿæˆåˆæ­¥çš„æœºå™¨äººè¡ŒåŠ¨è®¡åˆ’ï¼›2) å½¢å¼åŒ–éªŒè¯å™¨ï¼šå¯¹è¡ŒåŠ¨è®¡åˆ’è¿›è¡ŒéªŒè¯ï¼Œåˆ¤æ–­å…¶æ˜¯å¦æ»¡è¶³é¢„è®¾çš„å®‰å…¨å’Œåˆè§„çº¦æŸï¼›3) LLMè‡ªåŠ¨å¾®åˆ†æ¨¡å—ï¼ˆLLM-AutoDiffï¼‰ï¼šæ ¹æ®å½¢å¼åŒ–éªŒè¯çš„ç»“æœï¼Œè®¡ç®—æç¤ºçš„æ¢¯åº¦ï¼›4) æç¤ºä¼˜åŒ–å™¨ï¼šæ ¹æ®æ¢¯åº¦ä¿¡æ¯ï¼Œè¿­ä»£æ›´æ–°æç¤ºï¼Œç›´åˆ°æ»¡è¶³çº¦æŸæ¡ä»¶æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚æ•´ä¸ªæµç¨‹æ˜¯ä¸€ä¸ªé—­ç¯åé¦ˆç³»ç»Ÿï¼Œé€šè¿‡ä¸æ–­ä¼˜åŒ–æç¤ºï¼Œæé«˜LLMè§„åˆ’å™¨çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šLAD-VFæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å½¢å¼åŒ–éªŒè¯åé¦ˆä¸LLMè‡ªåŠ¨å¾®åˆ†ç›¸ç»“åˆï¼Œå®ç°å…å¾®è°ƒçš„æç¤ºä¼˜åŒ–ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒLAD-VFæ— éœ€æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬å’Œæ•°æ®éœ€æ±‚ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¼˜åŒ–æç¤ºè€Œéæ¨¡å‹æœ¬èº«ï¼Œå¯ä»¥æ›´å®¹æ˜“åœ°ç†è§£å’Œè§£é‡ŠLLMçš„è¡Œä¸ºï¼Œæé«˜äº†ç³»ç»Ÿçš„å¯ä¿¡åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šLAD-VFçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å½¢å¼åŒ–éªŒè¯ä¿¡æ¯æ–‡æœ¬æŸå¤±ï¼šè¯¥æŸå¤±å‡½æ•°ç”¨äºè¡¡é‡å½“å‰æç¤ºç”Ÿæˆçš„è®¡åˆ’ä¸çº¦æŸæ¡ä»¶ä¹‹é—´çš„å·®è·ï¼ŒæŒ‡å¯¼æç¤ºçš„ä¼˜åŒ–æ–¹å‘ï¼›2) LLM-AutoDiffï¼šåˆ©ç”¨LLMçš„è‡ªåŠ¨å¾®åˆ†èƒ½åŠ›ï¼Œè®¡ç®—æŸå¤±å‡½æ•°å…³äºæç¤ºçš„æ¢¯åº¦ï¼›3) æç¤ºçš„è¡¨ç¤ºæ–¹å¼ï¼šæç¤ºçš„è®¾è®¡éœ€è¦èƒ½å¤Ÿè¢«LLMç†è§£å’Œæ‰§è¡Œï¼ŒåŒæ—¶ä¹Ÿè¦æ–¹ä¾¿è¿›è¡Œæ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–ã€‚å…·ä½“çš„æç¤ºç»“æ„å’Œä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLAD-VFåœ¨æœºå™¨äººå¯¼èˆªå’Œæ“ä½œä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†è§„èŒƒä¾ä»æ€§ï¼Œå°†æˆåŠŸç‡ä»60%æé«˜åˆ°90%ä»¥ä¸Šã€‚ä¸éœ€è¦å¾®è°ƒçš„æ–¹æ³•ç›¸æ¯”ï¼ŒLAD-VFåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬å’Œæ•°æ®éœ€æ±‚ï¼Œå±•ç°äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LAD-VFå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰é¢†åŸŸã€‚å®ƒèƒ½å¤Ÿå¸®åŠ©LLMé©±åŠ¨çš„æ§åˆ¶ç³»ç»Ÿæ›´å¥½åœ°æ»¡è¶³å®‰å…¨å’Œåˆè§„çº¦æŸï¼Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿åˆ°å…¶ä»–éœ€è¦å½¢å¼åŒ–éªŒè¯çš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚ä»£ç ç”Ÿæˆã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) can translate natural language instructions into executable action plans for robotics, autonomous driving, and other domains. Yet, deploying LLM-driven planning in the physical world demands strict adherence to safety and regulatory constraints, which current models often violate due to hallucination or weak alignment. Traditional data-driven alignment methods, such as Direct Preference Optimization (DPO), require costly human labeling, while recent formal-feedback approaches still depend on resource-intensive fine-tuning. In this paper, we propose LAD-VF, a fine-tuning-free framework that leverages formal verification feedback for automated prompt engineering. By introducing a formal-verification-informed text loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts rather than model parameters. This yields three key benefits: (i) scalable adaptation without fine-tuning; (ii) compatibility with modular LLM architectures; and (iii) interpretable refinement via auditable prompts. Experiments in robot navigation and manipulation tasks demonstrate that LAD-VF substantially enhances specification compliance, improving success rates from 60% to over 90%. Our method thus presents a scalable and interpretable pathway toward trustworthy, formally-verified LLM-driven control systems.

