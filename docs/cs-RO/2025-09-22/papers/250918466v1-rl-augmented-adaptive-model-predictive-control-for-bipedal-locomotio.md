---
layout: default
title: RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain
---

# RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18466" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18466v1</a>
  <a href="https://arxiv.org/pdf/2509.18466.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18466v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18466v1', 'RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junnosuke Kamohara, Feiyang Wu, Chinmayee Wamorkar, Seth Hutchinson, Ye Zhao

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ å¢å¼ºçš„è‡ªé€‚åº”æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼Œç”¨äºåŒè¶³æœºå™¨äººå¤æ‚åœ°å½¢è¡Œèµ°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŒè¶³æœºå™¨äºº` `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `å¤æ‚åœ°å½¢` `è‡ªé€‚åº”æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸMPCåœ¨å¤æ‚åœ°å½¢ä¸­åŒè¶³è¡Œèµ°é¢ä¸´å»ºæ¨¡éš¾é¢˜ï¼Œéš¾ä»¥åº”å¯¹åœ°å½¢äº¤äº’ã€‚
2. åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¢å¼ºMPCï¼Œè‡ªé€‚åº”è°ƒæ•´ç³»ç»ŸåŠ¨åŠ›å­¦ã€æ‘†åŠ¨è…¿æ§åˆ¶å’Œæ­¥é¢‘ã€‚
3. åœ¨å¤šç§å¤æ‚åœ°å½¢ä»¿çœŸä¸­éªŒè¯ï¼Œç›¸æ¯”ä¼ ç»ŸMPCå’ŒRLï¼Œæ˜¾è‘—æå‡äº†é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡å‹é¢„æµ‹æ§åˆ¶(MPC)åœ¨äººå½¢åŒè¶³è¿åŠ¨ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼›ç„¶è€Œï¼Œç”±äºéš¾ä»¥å¯¹åœ°å½¢äº¤äº’è¿›è¡Œå»ºæ¨¡ï¼Œå…¶åœ¨ç²—ç³™å’Œæ¹¿æ»‘åœ°å½¢ç­‰å¤æ‚ç¯å¢ƒä¸­çš„é€‚ç”¨æ€§å—åˆ°é™åˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ (RL)åœ¨è®­ç»ƒå„ç§åœ°å½¢ä¸Šçš„é²æ£’è¿åŠ¨ç­–ç•¥æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å®ƒç¼ºä¹çº¦æŸæ»¡è¶³çš„ä¿è¯ï¼Œå¹¶ä¸”é€šå¸¸éœ€è¦å¤§é‡çš„å¥–åŠ±å¡‘é€ ã€‚æœ€è¿‘ç»“åˆMPCå’ŒRLçš„åŠªåŠ›æ˜¾ç¤ºäº†ä¸¤è€…ä¼˜åŠ¿äº’è¡¥çš„å¸Œæœ›ï¼Œä½†å®ƒä»¬ä¸»è¦å±€é™äºå¹³å¦åœ°å½¢æˆ–å››è¶³æœºå™¨äººã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹ç²—ç³™å’Œæ¹¿æ»‘åœ°å½¢ä¸Šçš„åŒè¶³è¿åŠ¨é‡èº«å®šåˆ¶çš„RLå¢å¼ºMPCæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å‚æ•°åŒ–äº†åŸºäºå•åˆšä½“åŠ¨åŠ›å­¦çš„MPCçš„ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šç³»ç»ŸåŠ¨åŠ›å­¦ã€æ‘†åŠ¨è…¿æ§åˆ¶å™¨å’Œæ­¥é¢‘ã€‚æˆ‘ä»¬é€šè¿‡åœ¨NVIDIA IsaacLabä¸­å¯¹åŒè¶³æœºå™¨äººåœ¨å„ç§åœ°å½¢ï¼ˆåŒ…æ‹¬æ¥¼æ¢¯ã€å«è„šçŸ³å’Œä½æ‘©æ“¦è¡¨é¢ï¼‰ä¸Šçš„ä»¿çœŸæ¥éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿MPCå’ŒRLç›¸æ¯”ï¼Œæˆ‘ä»¬çš„RLå¢å¼ºMPCæ¡†æ¶äº§ç”Ÿäº†æ›´å…·é€‚åº”æ€§å’Œé²æ£’æ€§çš„è¡Œä¸ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŒè¶³æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ï¼ˆå¦‚ç²—ç³™ã€æ¹¿æ»‘åœ°å½¢ï¼‰ä¸Šçš„ç¨³å®šè¡Œèµ°é—®é¢˜ã€‚ä¼ ç»ŸMPCæ–¹æ³•ä¾èµ–äºç²¾ç¡®çš„ç¯å¢ƒæ¨¡å‹ï¼Œä½†åœ¨å¤æ‚åœ°å½¢ä¸­ï¼Œç²¾ç¡®å»ºæ¨¡åœ°å½¢äº¤äº’éå¸¸å›°éš¾ï¼Œå¯¼è‡´MPCæ€§èƒ½ä¸‹é™ã€‚å¦ä¸€æ–¹é¢ï¼Œçº¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶å¯ä»¥å­¦ä¹ åˆ°é²æ£’çš„ç­–ç•¥ï¼Œä½†ç¼ºä¹çº¦æŸä¿è¯ï¼Œä¸”éœ€è¦å¤§é‡çš„å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œè®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼ºåŒ–å­¦ä¹ ä¸æ¨¡å‹é¢„æµ‹æ§åˆ¶ç›¸ç»“åˆï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥åœ¨çº¿è‡ªé€‚åº”åœ°è°ƒæ•´MPCçš„å…³é”®å‚æ•°ï¼Œä»è€Œæé«˜MPCåœ¨å¤æ‚åœ°å½¢ä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå¼ºåŒ–å­¦ä¹ è´Ÿè´£å­¦ä¹ ç³»ç»ŸåŠ¨åŠ›å­¦ã€æ‘†åŠ¨è…¿æ§åˆ¶å™¨å’Œæ­¥é¢‘çš„å‚æ•°ï¼Œä½¿å¾—MPCèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„åœ°å½¢æ¡ä»¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é‡‡ç”¨RLå¢å¼ºçš„MPCæ¡†æ¶ã€‚é¦–å…ˆï¼Œä½¿ç”¨MPCè¿›è¡Œè¿åŠ¨è§„åˆ’å’Œæ§åˆ¶ï¼Œä½†MPCçš„å…³é”®å‚æ•°ï¼ˆç³»ç»ŸåŠ¨åŠ›å­¦ã€æ‘†åŠ¨è…¿æ§åˆ¶å™¨å’Œæ­¥é¢‘ï¼‰ä¸æ˜¯å›ºå®šçš„ï¼Œè€Œæ˜¯ç”±ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç­–ç•¥ç½‘ç»œåŠ¨æ€è°ƒæ•´ã€‚å¼ºåŒ–å­¦ä¹ ç­–ç•¥ç½‘ç»œæ¥æ”¶æœºå™¨äººçš„çŠ¶æ€ä¿¡æ¯ï¼ˆå¦‚ä½ç½®ã€é€Ÿåº¦ã€å§¿æ€ç­‰ï¼‰ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºMPCå‚æ•°çš„è°ƒæ•´é‡ã€‚ç„¶åï¼Œå°†è°ƒæ•´åçš„å‚æ•°ä¼ é€’ç»™MPCï¼ŒMPCæ ¹æ®è¿™äº›å‚æ•°è¿›è¡Œè¿åŠ¨è§„åˆ’å’Œæ§åˆ¶ã€‚æ•´ä¸ªè¿‡ç¨‹åœ¨ä¸€ä¸ªå¾ªç¯ä¸­è¿›è¡Œï¼Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ç½‘ç»œä¸æ–­å­¦ä¹ ï¼Œä»¥ä¼˜åŒ–MPCçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å¼ºåŒ–å­¦ä¹ ä¸MPCæ·±åº¦èåˆï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥åœ¨çº¿è‡ªé€‚åº”åœ°è°ƒæ•´MPCçš„å…³é”®å‚æ•°ã€‚ä¸ä¼ ç»Ÿçš„MPCæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦ç²¾ç¡®çš„ç¯å¢ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚åœ°å½¢ã€‚ä¸çº¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†MPCçš„çº¦æŸä¿è¯ï¼Œæé«˜äº†ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é’ˆå¯¹åŒè¶³æœºå™¨äººè¿›è¡Œäº†ä¸“é—¨è®¾è®¡ï¼Œè€ƒè™‘äº†åŒè¶³è¡Œèµ°çš„ç‰¹æ®Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ç½‘ç»œé‡‡ç”¨Actor-Criticç»“æ„ï¼ŒActorç½‘ç»œè´Ÿè´£è¾“å‡ºMPCå‚æ•°çš„è°ƒæ•´é‡ï¼ŒCriticç½‘ç»œè´Ÿè´£è¯„ä¼°å½“å‰ç­–ç•¥çš„æ€§èƒ½ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘æœºå™¨äººçš„è¡Œèµ°é€Ÿåº¦ã€ç¨³å®šæ€§ã€èƒ½é‡æ¶ˆè€—ç­‰å› ç´ ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼Œè®ºæ–‡é‡‡ç”¨äº†æ¨¡ä»¿å­¦ä¹ å’Œè¯¾ç¨‹å­¦ä¹ ç­‰æŠ€æœ¯ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥RLå¢å¼ºMPCæ¡†æ¶åœ¨å„ç§å¤æ‚åœ°å½¢ï¼ˆåŒ…æ‹¬æ¥¼æ¢¯ã€å«è„šçŸ³å’Œä½æ‘©æ“¦è¡¨é¢ï¼‰ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¸åŸºçº¿MPCå’ŒRLç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿæ›´å…·é€‚åº”æ€§å’Œé²æ£’æ€§çš„è¡Œèµ°è¡Œä¸ºã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦å¼ºè°ƒäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦åœ¨å¤æ‚åœ°å½¢ä¸­è¡Œèµ°çš„åŒè¶³æœºå™¨äººï¼Œä¾‹å¦‚æœæ•‘æœºå™¨äººã€å·¡æ£€æœºå™¨äººã€ä»¥åŠç”¨äºç‰©æµå’Œå»ºç­‘è¡Œä¸šçš„æœºå™¨äººã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ï¼Œå¯ä»¥æ‰©å±•æœºå™¨äººçš„åº”ç”¨èŒƒå›´ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å‘æŒ¥ä½œç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æœºå™¨äººï¼Œä¾‹å¦‚å››è¶³æœºå™¨äººå’Œè½®å¼æœºå™¨äººã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions. In contrast, reinforcement learning (RL) has achieved notable success in training robust locomotion policies over diverse terrain, yet it lacks guarantees of constraint satisfaction and often requires substantial reward shaping. Recent efforts in combining MPC and RL have shown promise of taking the best of both worlds, but they are primarily restricted to flat terrain or quadrupedal robots. In this work, we propose an RL-augmented MPC framework tailored for bipedal locomotion over rough and slippery terrain. Our method parametrizes three key components of single-rigid-body-dynamics-based MPC: system dynamics, swing leg controller, and gait frequency. We validate our approach through bipedal robot simulations in NVIDIA IsaacLab across various terrains, including stairs, stepping stones, and low-friction surfaces. Experimental results demonstrate that our RL-augmented MPC framework produces significantly more adaptive and robust behaviors compared to baseline MPC and RL.

