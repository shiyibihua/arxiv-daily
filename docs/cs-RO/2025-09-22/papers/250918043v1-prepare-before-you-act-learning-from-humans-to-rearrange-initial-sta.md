---
layout: default
title: Prepare Before You Act: Learning From Humans to Rearrange Initial States
---

# Prepare Before You Act: Learning From Humans to Rearrange Initial States

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18043" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18043v1</a>
  <a href="https://arxiv.org/pdf/2509.18043.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18043v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18043v1', 'Prepare Before You Act: Learning From Humans to Rearrange Initial States')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yinlong Dai, Andre Keyser, Dylan P. Losey

**åˆ†ç±»**: cs.RO, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReSETï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ äººç±»é¢„å¤„ç†ç¯å¢ƒï¼Œæå‡æœºå™¨äººæ“ä½œä»»åŠ¡çš„æ³›åŒ–æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `ç¯å¢ƒé¢„å¤„ç†` `æ³›åŒ–èƒ½åŠ›` `äººç±»è¡Œä¸ºæ¨¡ä»¿`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ¨¡ä»¿å­¦ä¹ åœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸­é¢ä¸´æ³›åŒ–æ€§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨åˆå§‹çŠ¶æ€åˆ†å¸ƒåç§»æ—¶è¡¨ç°ä¸ä½³ã€‚
2. ReSETç®—æ³•æ¨¡ä»¿äººç±»åœ¨æ“ä½œå‰çš„ç¯å¢ƒé¢„å¤„ç†è¡Œä¸ºï¼Œé€šè¿‡é‡æ„åœºæ™¯æ¥æå‡ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒReSETåœ¨ç›¸åŒè®­ç»ƒæ•°æ®ä¸‹ï¼Œèƒ½æ˜¾è‘—æå‡æœºå™¨äººæ“ä½œä»»åŠ¡çš„é²æ£’æ€§ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ (IL)åœ¨å„ç§æ“ä½œä»»åŠ¡ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œå½“é¢ä¸´åˆ†å¸ƒå¤–çš„è§‚å¯Ÿæ—¶ï¼Œä¾‹å¦‚å½“ç›®æ ‡å¯¹è±¡å¤„äºå…ˆå‰æœªè§çš„ä½ç½®æˆ–è¢«å…¶ä»–å¯¹è±¡é®æŒ¡æ—¶ï¼ŒILç­–ç•¥å¸¸å¸¸ä¼šé‡åˆ°å›°éš¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½“å‰çš„ILæ–¹æ³•éœ€è¦å¤§é‡çš„æ¼”ç¤ºæ‰èƒ½è¾¾åˆ°é²æ£’å’Œå¯æ³›åŒ–çš„è¡Œä¸ºã€‚ä½†æ˜¯ï¼Œå½“äººç±»é¢å¯¹è¿™äº›éå…¸å‹çš„åˆå§‹çŠ¶æ€æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šé‡æ–°æ’åˆ—ç¯å¢ƒï¼Œä»¥ä¾¿æ›´å¥½åœ°æ‰§è¡Œä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªäººå¯èƒ½ä¼šæ—‹è½¬å’–å•¡æ¯ï¼Œä»¥ä¾¿æ›´å®¹æ˜“æŠ“ä½æŠŠæ‰‹ï¼Œæˆ–è€…æ¨å¼€ä¸€ä¸ªç›’å­ï¼Œä»¥ä¾¿ä»–ä»¬å¯ä»¥ç›´æ¥æŠ“ä½ä»–ä»¬çš„ç›®æ ‡å¯¹è±¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯•å›¾è®©æœºå™¨äººå­¦ä¹ è€…å…·å¤‡åŒæ ·çš„èƒ½åŠ›ï¼šä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨æ‰§è¡Œå…¶ç»™å®šç­–ç•¥ä¹‹å‰å‡†å¤‡ç¯å¢ƒã€‚æˆ‘ä»¬æå‡ºReSETï¼Œä¸€ç§ç®—æ³•ï¼Œå®ƒæ¥å—åˆå§‹çŠ¶æ€ï¼ˆè¿™äº›çŠ¶æ€åœ¨ç­–ç•¥çš„åˆ†å¸ƒä¹‹å¤–ï¼‰ï¼Œå¹¶è‡ªä¸»åœ°ä¿®æ”¹å¯¹è±¡å§¿åŠ¿ï¼Œä½¿é‡æ„åçš„åœºæ™¯ç±»ä¼¼äºè®­ç»ƒæ•°æ®ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ä¸ªä¸¤æ­¥è¿‡ç¨‹ï¼ˆåœ¨å±•å¼€ç»™å®šç­–ç•¥ä¹‹å‰é‡æ–°æ’åˆ—ç¯å¢ƒï¼‰å‡å°‘äº†æ³›åŒ–å·®è·ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬çš„ReSETç®—æ³•å°†ä¸åŠ¨ä½œæ— å…³çš„äººç±»è§†é¢‘ä¸ä¸ä»»åŠ¡æ— å…³çš„é¥æ“ä½œæ•°æ®ç›¸ç»“åˆï¼Œä»¥ i) å†³å®šä½•æ—¶ä¿®æ”¹åœºæ™¯ï¼Œii) é¢„æµ‹äººç±»ä¼šé‡‡å–å“ªäº›ç®€åŒ–åŠ¨ä½œï¼Œä»¥åŠ iii) å°†è¿™äº›é¢„æµ‹æ˜ å°„åˆ°æœºå™¨äººåŠ¨ä½œåŸè¯­ã€‚ä¸æ‰©æ•£ç­–ç•¥ã€VLAså’Œå…¶ä»–åŸºçº¿çš„æ¯”è¾ƒè¡¨æ˜ï¼Œä½¿ç”¨ReSETå‡†å¤‡ç¯å¢ƒèƒ½å¤Ÿä»¥ç›¸åŒçš„æ€»è®­ç»ƒæ•°æ®å®ç°æ›´å¼ºå¤§çš„ä»»åŠ¡æ‰§è¡Œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸åŒçš„åˆå§‹çŠ¶æ€æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ä¾‹å¦‚ï¼Œç›®æ ‡ç‰©ä½“ä½ç½®å¼‚å¸¸æˆ–è¢«é®æŒ¡æ—¶ï¼Œæœºå™¨äººéš¾ä»¥æˆåŠŸå®Œæˆä»»åŠ¡ã€‚éœ€è¦å¤§é‡çš„é¢å¤–è®­ç»ƒæ•°æ®æ‰èƒ½æå‡é²æ£’æ€§ï¼Œä½†æˆæœ¬å¾ˆé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ¨¡ä»¿äººç±»åœ¨æ‰§è¡Œä»»åŠ¡å‰ä¼šå…ˆå¯¹ç¯å¢ƒè¿›è¡Œé¢„å¤„ç†ï¼Œä¾‹å¦‚è°ƒæ•´ç‰©ä½“ä½ç½®ï¼Œä½¿å…¶æ›´æ˜“äºæ“ä½œã€‚ReSETç®—æ³•æ—¨åœ¨è®©æœºå™¨äººå­¦ä¹ è¿™ç§é¢„å¤„ç†èƒ½åŠ›ï¼Œå°†åˆå§‹çŠ¶æ€è°ƒæ•´åˆ°æ›´æ¥è¿‘è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜åç»­ç­–ç•¥çš„æˆåŠŸç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReSETåŒ…å«ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š1) å†³å®šä½•æ—¶éœ€è¦ä¿®æ”¹åœºæ™¯ï¼›2) é¢„æµ‹äººç±»ä¼šé‡‡å–çš„ç®€åŒ–åŠ¨ä½œï¼›3) å°†é¢„æµ‹çš„åŠ¨ä½œæ˜ å°„åˆ°æœºå™¨äººçš„åŠ¨ä½œåŸè¯­ã€‚ç®—æ³•åˆ©ç”¨åŠ¨ä½œæ— å…³çš„äººç±»è§†é¢‘å’Œä»»åŠ¡æ— å…³çš„é¥æ“ä½œæ•°æ®è¿›è¡Œå­¦ä¹ ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œå½“è¾“å…¥ä¸€ä¸ªåˆå§‹çŠ¶æ€æ—¶ï¼ŒReSETé¦–å…ˆåˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œç¯å¢ƒé‡æ„ã€‚å¦‚æœéœ€è¦ï¼Œåˆ™é¢„æµ‹äººç±»ä¼šé‡‡å–çš„åŠ¨ä½œï¼Œå¹¶å°†è¿™äº›åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººå¯ä»¥æ‰§è¡Œçš„åŠ¨ä½œï¼Œä»è€Œæ”¹å˜ç¯å¢ƒçŠ¶æ€ã€‚æœ€åï¼Œæ‰§è¡Œé¢„å…ˆè®­ç»ƒå¥½çš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šReSETçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ç¯å¢ƒé¢„å¤„ç†çš„æ¦‚å¿µå¼•å…¥åˆ°æ¨¡ä»¿å­¦ä¹ ä¸­ï¼Œé€šè¿‡æ¨¡ä»¿äººç±»çš„è¡Œä¸ºæ¥æ”¹å–„åˆå§‹çŠ¶æ€ï¼Œä»è€Œæå‡ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›´æ¥å­¦ä¹ æ“ä½œç­–ç•¥ä¸åŒï¼ŒReSETå­¦ä¹ çš„æ˜¯å¦‚ä½•æ”¹å˜ç¯å¢ƒï¼Œä½¿å…¶æ›´é€‚åˆæ‰§è¡Œç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šReSETä½¿ç”¨åŠ¨ä½œæ— å…³çš„äººç±»è§†é¢‘æ¥å­¦ä¹ äººç±»å¦‚ä½•é¢„å¤„ç†ç¯å¢ƒã€‚åŒæ—¶ï¼Œåˆ©ç”¨ä»»åŠ¡æ— å…³çš„é¥æ“ä½œæ•°æ®æ¥å­¦ä¹ å¦‚ä½•å°†äººç±»çš„åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººçš„åŠ¨ä½œåŸè¯­ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒReSETç®—æ³•åœ¨ç¯å¢ƒé¢„å¤„ç†åï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººæ“ä½œä»»åŠ¡çš„æˆåŠŸç‡ã€‚ä¸æ‰©æ•£ç­–ç•¥ã€VLAsç­‰åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒReSETåœ¨ç›¸åŒè®­ç»ƒæ•°æ®é‡ä¸‹ï¼Œèƒ½å¤Ÿå®ç°æ›´é²æ£’çš„ä»»åŠ¡æ‰§è¡Œã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®åœ¨æ‘˜è¦ä¸­æœªç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ReSETç®—æ³•å¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ã€éç»“æ„åŒ–çš„ç¯å¢ƒä¸­ã€‚ä¾‹å¦‚ï¼Œåœ¨å®¶åº­æœåŠ¡æœºå™¨äººä¸­ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººæ•´ç†ç‰©å“ã€è°ƒæ•´ç‰©ä½“ä½ç½®ï¼Œä½¿å…¶æ›´å®¹æ˜“æŠ“å–å’Œä½¿ç”¨ã€‚åœ¨å·¥ä¸šè‡ªåŠ¨åŒ–é¢†åŸŸï¼Œå¯ä»¥ç”¨äºå¤„ç†ç”Ÿäº§çº¿ä¸Šä½ç½®ä¸ç¡®å®šçš„é›¶ä»¶ï¼Œæé«˜ç”Ÿäº§æ•ˆç‡ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡æœºå™¨äººçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹çœŸå®ä¸–ç•Œçš„æŒ‘æˆ˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation learning (IL) has proven effective across a wide range of manipulation tasks. However, IL policies often struggle when faced with out-of-distribution observations; for instance, when the target object is in a previously unseen position or occluded by other objects. In these cases, extensive demonstrations are needed for current IL methods to reach robust and generalizable behaviors. But when humans are faced with these sorts of atypical initial states, we often rearrange the environment for more favorable task execution. For example, a person might rotate a coffee cup so that it is easier to grasp the handle, or push a box out of the way so they can directly grasp their target object. In this work we seek to equip robot learners with the same capability: enabling robots to prepare the environment before executing their given policy. We propose ReSET, an algorithm that takes initial states -- which are outside the policy's distribution -- and autonomously modifies object poses so that the restructured scene is similar to training data. Theoretically, we show that this two step process (rearranging the environment before rolling out the given policy) reduces the generalization gap. Practically, our ReSET algorithm combines action-agnostic human videos with task-agnostic teleoperation data to i) decide when to modify the scene, ii) predict what simplifying actions a human would take, and iii) map those predictions into robot action primitives. Comparisons with diffusion policies, VLAs, and other baselines show that using ReSET to prepare the environment enables more robust task execution with equal amounts of total training data. See videos at our project website: https://reset2025paper.github.io/

