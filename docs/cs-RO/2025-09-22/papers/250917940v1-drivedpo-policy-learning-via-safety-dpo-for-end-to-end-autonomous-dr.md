---
layout: default
title: DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving
---

# DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.17940" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.17940v1</a>
  <a href="https://arxiv.org/pdf/2509.17940.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.17940v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.17940v1', 'DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuyao Shang, Yuntao Chen, Yuqi Wang, Yingyan Li, Zhaoxiang Zhang

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-22

**å¤‡æ³¨**: NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DriveDPOï¼šé¢å‘ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶çš„å®‰å…¨DPOç­–ç•¥å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶` `ç›´æ¥åå¥½ä¼˜åŒ–` `å®‰å…¨ç­–ç•¥å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `è§„åˆ™é©±åŠ¨` `è½¨è¿¹ä¼˜åŒ–` `NAVSIM` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ¨¡ä»¿å­¦ä¹ åœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­é¢ä¸´å®‰å…¨æŒ‘æˆ˜ï¼Œéš¾ä»¥åŒºåˆ†ç±»äººä½†æ½œåœ¨å±é™©çš„è½¨è¿¹ã€‚
2. DriveDPOé€šè¿‡æç‚¼äººç±»æ¨¡ä»¿å’Œè§„åˆ™å®‰å…¨åˆ†æ•°ï¼Œè¿›è¡Œç›´æ¥ç­–ç•¥ä¼˜åŒ–ï¼Œå®ç°å®‰å…¨é©¾é©¶ã€‚
3. åœ¨NAVSIMåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDriveDPOè¾¾åˆ°äº†90.0çš„PDMSï¼Œå¹¶åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´å®‰å…¨å¯é çš„é©¾é©¶è¡Œä¸ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶é€šè¿‡ç›´æ¥ä»åŸå§‹æ„ŸçŸ¥è¾“å…¥é¢„æµ‹æœªæ¥è½¨è¿¹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç»•è¿‡äº†ä¼ ç»Ÿçš„æ¨¡å—åŒ–æµç¨‹ã€‚ç„¶è€Œï¼Œä¸»æµçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•å­˜åœ¨ä¸¥é‡çš„å®‰å…¨é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•åŒºåˆ†çœ‹èµ·æ¥åƒäººç±»ä½†å¯èƒ½ä¸å®‰å…¨çš„è½¨è¿¹ã€‚ä¸€äº›æœ€è¿‘çš„æ–¹æ³•è¯•å›¾é€šè¿‡å›å½’å¤šä¸ªè§„åˆ™é©±åŠ¨çš„åˆ†æ•°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å°†ç›‘ç£ä¸ç­–ç•¥ä¼˜åŒ–åˆ†ç¦»ï¼Œå¯¼è‡´æ¬¡ä¼˜æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DriveDPOï¼Œä¸€ä¸ªå®‰å…¨ç›´æ¥åå¥½ä¼˜åŒ–ç­–ç•¥å­¦ä¹ æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»äººç±»æ¨¡ä»¿ç›¸ä¼¼æ€§å’ŒåŸºäºè§„åˆ™çš„å®‰å…¨åˆ†æ•°ä¸­æç‚¼å‡ºä¸€ä¸ªç»Ÿä¸€çš„ç­–ç•¥åˆ†å¸ƒï¼Œç”¨äºç›´æ¥ç­–ç•¥ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¿­ä»£çš„ç›´æ¥åå¥½ä¼˜åŒ–é˜¶æ®µï¼Œå°†å…¶è¡¨è¿°ä¸ºè½¨è¿¹çº§åˆ«çš„åå¥½å¯¹é½ã€‚åœ¨NAVSIMåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDriveDPOå®ç°äº†90.0çš„æœ€æ–°PDMSã€‚æ­¤å¤–ï¼Œå„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­çš„å®šæ€§ç»“æœçªå‡ºäº†DriveDPOäº§ç”Ÿæ›´å®‰å…¨å’Œæ›´å¯é çš„é©¾é©¶è¡Œä¸ºçš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ¨¡ä»¿å­¦ä¹ çš„æ–¹æ³•ï¼Œåœ¨å®‰å…¨æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚å®ƒä»¬éš¾ä»¥åŒºåˆ†æ¨¡ä»¿äººç±»é©¾é©¶è¡Œä¸ºä½†å®é™…ä¸Šä¸å®‰å…¨çš„è½¨è¿¹ï¼Œå¯¼è‡´æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚è™½ç„¶ä¸€äº›æ–¹æ³•å°è¯•å¼•å…¥è§„åˆ™é©±åŠ¨çš„å®‰å…¨è¯„åˆ†ï¼Œä½†è¿™äº›è¯„åˆ†ä¸ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹æ˜¯åˆ†ç¦»çš„ï¼Œå¯¼è‡´æ€§èƒ½æ¬¡ä¼˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDriveDPOçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†äººç±»é©¾é©¶è¡Œä¸ºçš„ç›¸ä¼¼æ€§å’Œè§„åˆ™é©±åŠ¨çš„å®‰å…¨è¯„åˆ†ç»“åˆèµ·æ¥ï¼Œæç‚¼å‡ºä¸€ä¸ªç»Ÿä¸€çš„ç­–ç•¥åˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimization, DPOï¼‰æ–¹æ³•æ¥ä¼˜åŒ–ç­–ç•¥ã€‚é€šè¿‡ç›´æ¥å¯¹è½¨è¿¹çº§åˆ«çš„åå¥½è¿›è¡Œå¯¹é½ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç›‘ç£ä¿¡å·ä¸ç­–ç•¥ä¼˜åŒ–åˆ†ç¦»çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDriveDPOæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œä»äººç±»æ¨¡ä»¿ç›¸ä¼¼æ€§å’Œè§„åˆ™å®‰å…¨åˆ†æ•°ä¸­æç‚¼å‡ºä¸€ä¸ªç»Ÿä¸€çš„ç­–ç•¥åˆ†å¸ƒã€‚ç„¶åï¼Œä½¿ç”¨è¿­ä»£çš„DPOé˜¶æ®µï¼Œå°†ç­–ç•¥ä¸è½¨è¿¹çº§åˆ«çš„åå¥½å¯¹é½ã€‚æ•´ä¸ªæ¡†æ¶ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œç›´æ¥ä»åŸå§‹æ„ŸçŸ¥è¾“å…¥é¢„æµ‹æœªæ¥è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šDriveDPOçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨å®‰å…¨ç›´æ¥åå¥½ä¼˜åŒ–ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œå°†äººç±»é©¾é©¶è¡Œä¸ºçš„æ¨¡ä»¿å’Œè§„åˆ™é©±åŠ¨çš„å®‰å…¨çº¦æŸé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„ä¼˜åŒ–è¿‡ç¨‹ä¸­ã€‚é€šè¿‡ç›´æ¥ä¼˜åŒ–ç­–ç•¥ä»¥ç¬¦åˆäººç±»åå¥½å’Œå®‰å…¨è§„åˆ™ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç›‘ç£ä¿¡å·ä¸ç­–ç•¥ä¼˜åŒ–åˆ†ç¦»çš„é—®é¢˜ï¼Œä»è€Œæé«˜äº†é©¾é©¶çš„å®‰å…¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šDriveDPOä½¿ç”¨DPOæŸå¤±å‡½æ•°æ¥å¯¹é½è½¨è¿¹çº§åˆ«çš„åå¥½ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡æ¯”è¾ƒä¸åŒè½¨è¿¹çš„å¾—åˆ†ï¼Œé¼“åŠ±æ¨¡å‹é€‰æ‹©æ›´å®‰å…¨ã€æ›´ç¬¦åˆäººç±»é©¾é©¶ä¹ æƒ¯çš„è½¨è¿¹ã€‚æ­¤å¤–ï¼ŒDriveDPOé‡‡ç”¨è¿­ä»£çš„ä¼˜åŒ–ç­–ç•¥ï¼Œé€æ­¥æé«˜ç­–ç•¥çš„æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ åˆ°å®‰å…¨é©¾é©¶ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DriveDPOåœ¨NAVSIMåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼ŒPDMSï¼ˆPercentage of Driving Miles Safelyï¼‰è¾¾åˆ°äº†90.0ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³ã€‚å®šæ€§ç»“æœè¡¨æ˜ï¼ŒDriveDPOåœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­èƒ½å¤Ÿäº§ç”Ÿæ›´å®‰å…¨ã€æ›´å¯é çš„é©¾é©¶è¡Œä¸ºï¼Œä¾‹å¦‚åœ¨äº¤é€šæ‹¥å µã€è¡Œäººç©¿è¶Šå’Œæ¶åŠ£å¤©æ°”ç­‰æƒ…å†µä¸‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DriveDPOæŠ€æœ¯å¯åº”ç”¨äºå„ç§è‡ªåŠ¨é©¾é©¶åœºæ™¯ï¼ŒåŒ…æ‹¬åŸå¸‚é“è·¯ã€é«˜é€Ÿå…¬è·¯å’Œè¶Šé‡ç¯å¢ƒã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ï¼Œå‡å°‘äº‹æ•…é£é™©ï¼Œå¹¶æå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼ŒDriveDPOè¿˜å¯ä»¥ç”¨äºè®­ç»ƒæ›´å®‰å…¨ã€æ›´å¯é çš„æœºå™¨äººå’Œæ— äººæœºï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> End-to-end autonomous driving has substantially progressed by directly predicting future trajectories from raw perception inputs, which bypasses traditional modular pipelines. However, mainstream methods trained via imitation learning suffer from critical safety limitations, as they fail to distinguish between trajectories that appear human-like but are potentially unsafe. Some recent approaches attempt to address this by regressing multiple rule-driven scores but decoupling supervision from policy optimization, resulting in suboptimal performance. To tackle these challenges, we propose DriveDPO, a Safety Direct Preference Optimization Policy Learning framework. First, we distill a unified policy distribution from human imitation similarity and rule-based safety scores for direct policy optimization. Further, we introduce an iterative Direct Preference Optimization stage formulated as trajectory-level preference alignment. Extensive experiments on the NAVSIM benchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of 90.0. Furthermore, qualitative results across diverse challenging scenarios highlight DriveDPO's ability to produce safer and more reliable driving behaviors.

