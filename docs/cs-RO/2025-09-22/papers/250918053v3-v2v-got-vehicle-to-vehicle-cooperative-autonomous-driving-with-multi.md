---
layout: default
title: V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts
---

# V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18053" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18053v3</a>
  <a href="https://arxiv.org/pdf/2509.18053.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18053v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18053v3', 'V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hsu-kuang Chiu, Ryo Hachiuma, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen, Stephen F. Smith

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-22 (æ›´æ–°: 2025-09-25)

**å¤‡æ³¨**: Our project website: https://eddyhkchiu.github.io/v2vgot.github.io/

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://eddyhkchiu.github.io/v2vgot.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºV2V-GoTï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œå›¾æ¨ç†è§£å†³V2VååŒè‡ªåŠ¨é©¾é©¶ä¸­çš„é®æŒ¡é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `V2VååŒé©¾é©¶` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å›¾æ¨ç†` `é®æŒ¡æ„ŸçŸ¥` `è‡ªåŠ¨é©¾é©¶` `ååŒæ„ŸçŸ¥` `è½¦è¾†äº’è”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨ä¼ æ„Ÿå™¨è¢«é®æŒ¡æ—¶å­˜åœ¨å®‰å…¨éšæ‚£ï¼ŒV2VååŒé©¾é©¶æ˜¯æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚
2. è®ºæ–‡æå‡ºV2V-GoTæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œå›¾æ¨ç†è¿›è¡ŒååŒæ„ŸçŸ¥å’Œè§„åˆ’ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒV2V-GoTåœ¨ååŒæ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰æœ€å…ˆè¿›çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨é“è·¯ä¸Šè¢«å¤§å‹ç‰©ä½“é®æŒ¡å±€éƒ¨ä¼ æ„Ÿå™¨æ—¶ï¼Œå¯èƒ½ä¼šé¢ä¸´å®‰å…¨ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…ã€‚è½¦è¾†åˆ°è½¦è¾†ï¼ˆV2Vï¼‰ååŒè‡ªåŠ¨é©¾é©¶è¢«æå‡ºä½œä¸ºè§£å†³æ­¤é—®é¢˜çš„ä¸€ç§æ‰‹æ®µã€‚æœ€è¿‘å¼•å…¥çš„ä¸€ç§ååŒè‡ªåŠ¨é©¾é©¶æ¡†æ¶è¿›ä¸€æ­¥é‡‡ç”¨äº†ä¸€ç§ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ–¹æ³•ï¼Œä»¥æ•´åˆååŒæ„ŸçŸ¥å’Œè§„åˆ’è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå°½ç®¡å°†å›¾æ¨ç†åº”ç”¨äºMLLMå…·æœ‰æ½œåœ¨çš„å¥½å¤„ï¼Œä½†ä¹‹å‰çš„ååŒè‡ªåŠ¨é©¾é©¶ç ”ç©¶å¹¶æœªè€ƒè™‘è¿™ä¸€æƒ³æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨ä¸ºåŸºäºMLLMçš„ååŒè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„æ–°çš„å›¾æ¨ç†æ¡†æ¶ã€‚æˆ‘ä»¬çš„å›¾æ¨ç†åŒ…æ‹¬æˆ‘ä»¬æå‡ºçš„é®æŒ¡æ„ŸçŸ¥æ„ŸçŸ¥å’Œè§„åˆ’æ„ŸçŸ¥é¢„æµ‹çš„æ–°é¢–æƒ³æ³•ã€‚æˆ‘ä»¬æ•´ç†äº†V2V-GoT-QAæ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†V2V-GoTæ¨¡å‹ï¼Œç”¨äºè®­ç»ƒå’Œæµ‹è¯•ååŒé©¾é©¶å›¾æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ååŒæ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ä»»åŠ¡ä¸­ä¼˜äºå…¶ä»–åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³V2VååŒè‡ªåŠ¨é©¾é©¶ä¸­ï¼Œç”±äºè½¦è¾†ä¼ æ„Ÿå™¨è¢«é®æŒ¡è€Œå¯¼è‡´çš„å®‰å…¨é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶åˆ©ç”¨MLLMè¿›è¡ŒååŒæ„ŸçŸ¥å’Œè§„åˆ’ï¼Œä½†ç¼ºä¹æœ‰æ•ˆçš„æ¨ç†æœºåˆ¶ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤šè½¦è¾†çš„ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨é®æŒ¡çš„æƒ…å†µä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥å›¾æ¨ç†ï¼ˆGraph-of-Thoughts, GoTï¼‰æœºåˆ¶ï¼Œç»“åˆMLLMï¼Œæ„å»ºä¸€ä¸ªæ›´å¼ºå¤§çš„ååŒè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿã€‚é€šè¿‡GoTï¼Œç³»ç»Ÿå¯ä»¥è¿›è¡Œæ›´æ·±å…¥çš„æ¨ç†å’Œå†³ç­–ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†é®æŒ¡ç­‰å¤æ‚æƒ…å†µã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ¨¡æ‹Ÿäººç±»é©¾é©¶å‘˜åœ¨ååŒé©¾é©¶ä¸­çš„æ€è€ƒè¿‡ç¨‹ï¼Œå³ç»¼åˆè€ƒè™‘å¤šä¸ªè½¦è¾†çš„æ„ŸçŸ¥ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œæ¨ç†å’Œé¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šV2V-GoTæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€è¾“å…¥ï¼šæ¥æ”¶æ¥è‡ªå¤šä¸ªè½¦è¾†çš„æ„ŸçŸ¥æ•°æ®ï¼ŒåŒ…æ‹¬å›¾åƒã€æ¿€å…‰é›·è¾¾ç­‰ï¼›2) MLLMç¼–ç å™¨ï¼šåˆ©ç”¨MLLMå¯¹å¤šæ¨¡æ€æ•°æ®è¿›è¡Œç¼–ç ï¼Œæå–ç‰¹å¾ï¼›3) å›¾æ„å»ºï¼šåŸºäºè½¦è¾†ä¹‹é—´çš„å…³ç³»ï¼ˆä¾‹å¦‚è·ç¦»ã€ç›¸å¯¹ä½ç½®ï¼‰æ„å»ºå›¾ç»“æ„ï¼›4) å›¾æ¨ç†ï¼šåœ¨å›¾ä¸Šè¿›è¡Œæ¨ç†ï¼Œä¾‹å¦‚é®æŒ¡æ„ŸçŸ¥æ„ŸçŸ¥å’Œè§„åˆ’æ„ŸçŸ¥é¢„æµ‹ï¼›5) å†³ç­–è¾“å‡ºï¼šæ ¹æ®æ¨ç†ç»“æœï¼Œç”ŸæˆååŒé©¾é©¶å†³ç­–ï¼Œä¾‹å¦‚åŠ é€Ÿã€å‡é€Ÿã€å˜é“ç­‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å›¾æ¨ç†ï¼ˆGoTï¼‰å¼•å…¥åˆ°MLLM-basedçš„V2VååŒè‡ªåŠ¨é©¾é©¶ä¸­ã€‚å…·ä½“è€Œè¨€ï¼Œæå‡ºäº†é®æŒ¡æ„ŸçŸ¥æ„ŸçŸ¥å’Œè§„åˆ’æ„ŸçŸ¥é¢„æµ‹ä¸¤ç§æ–°çš„å›¾æ¨ç†ç­–ç•¥ã€‚é®æŒ¡æ„ŸçŸ¥æ„ŸçŸ¥æ—¨åœ¨åˆ©ç”¨å…¶ä»–è½¦è¾†çš„è§†è§’æ¥å¼¥è¡¥è‡ªèº«è½¦è¾†çš„æ„ŸçŸ¥ç›²åŒºã€‚è§„åˆ’æ„ŸçŸ¥é¢„æµ‹åˆ™è€ƒè™‘äº†å…¶ä»–è½¦è¾†çš„è§„åˆ’æ„å›¾ï¼Œä»è€Œæ›´å‡†ç¡®åœ°é¢„æµ‹å…¶æœªæ¥è¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æå‡ºäº†V2V-GoT-QAæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œæµ‹è¯•ååŒé©¾é©¶å›¾æ¨ç†ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆä¾‹å¦‚MLLMçš„å…·ä½“é€‰æ‹©ã€å›¾æ¨ç†ç®—æ³•ã€æŸå¤±å‡½æ•°ç­‰ï¼‰åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæ˜ç¡®æåŠã€‚è¿™äº›ç»†èŠ‚å¯¹äºå¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶è‡³å…³é‡è¦ï¼Œéœ€è¦æŸ¥é˜…åŸæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒV2V-GoTåœ¨ååŒæ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ä»»åŠ¡ä¸­å‡ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œéœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚V2V-GoT-QAæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºç›¸å…³ç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæé«˜è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸‹çš„å®‰å…¨æ€§ï¼Œå°¤å…¶æ˜¯åœ¨åŸå¸‚é“è·¯ã€äº¤å‰è·¯å£ç­‰å®¹æ˜“å‘ç”Ÿé®æŒ¡çš„åœºæ™¯ã€‚é€šè¿‡è½¦è¾†é—´çš„ååŒæ„ŸçŸ¥å’Œæ¨ç†ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘äº‹æ•…é£é™©ï¼Œæå‡äº¤é€šæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨å¹¿åˆ°è½¦è·¯ååŒç³»ç»Ÿï¼Œå®ç°æ›´é«˜çº§åˆ«çš„è‡ªåŠ¨é©¾é©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current state-of-the-art autonomous vehicles could face safety-critical situations when their local sensors are occluded by large nearby objects on the road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed as a means of addressing this problem, and one recently introduced framework for cooperative autonomous driving has further adopted an approach that incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative perception and planning processes. However, despite the potential benefit of applying graph-of-thoughts reasoning to the MLLM, this idea has not been considered by previous cooperative autonomous driving research. In this paper, we propose a novel graph-of-thoughts framework specifically designed for MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our proposed novel ideas of occlusion-aware perception and planning-aware prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for training and testing the cooperative driving graph-of-thoughts. Our experimental results show that our method outperforms other baselines in cooperative perception, prediction, and planning tasks. Our project website: https://eddyhkchiu.github.io/v2vgot.github.io/ .

