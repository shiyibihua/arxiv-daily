---
layout: default
title: M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer
---

# M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18005" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18005v1</a>
  <a href="https://arxiv.org/pdf/2509.18005.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18005v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18005v1', 'M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanxin Zhang, Liang He, Zeyi Kang, Zuheng Ming, Kaixing Zhao

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-22

**å¤‡æ³¨**: 8 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM3ETï¼šä¸€ç§é«˜æ•ˆçš„åŸºäºå¤šæ¨¡æ€Mambaå¢å¼ºTransformerçš„æœºå™¨äººè§†è§‰-è¯­è¨€å­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æœºå™¨äººè§†è§‰` `Mamba` `Transformer` `è§†è§‰é—®ç­”` `æ¨¡å‹è½»é‡åŒ–` `è‡ªé€‚åº”æ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æœºå™¨äººè§†è§‰-è¯­è¨€å­¦ä¹ ä¸­ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨æ–‡æœ¬æ¨¡æ€ï¼Œä¸”è®¡ç®—é‡å¤§ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚
2. M3ETé€šè¿‡å¼•å…¥Mambaæ¨¡å—å’Œè¯­ä¹‰è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¼˜åŒ–ç‰¹å¾èåˆå’Œæ¨¡æ€é‡å»ºï¼Œå®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒM3ETåœ¨ä¿æŒVQAä»»åŠ¡å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å‚æ•°é‡ï¼Œå¹¶æå‡äº†æ¨ç†é€Ÿåº¦ï¼Œæ›´é€‚åˆç§»åŠ¨å¹³å°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å­¦ä¹ åœ¨æœºå™¨äººè§†è§‰å’Œä¿¡æ¯èåˆä¸­å˜å¾—è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£å¤æ‚ç¯å¢ƒä¸­äººç±»è¡Œä¸ºæ–¹é¢ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨æ–‡æœ¬æ¨¡æ€ï¼Œä¾èµ–äºæœ‰ç›‘ç£çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™é™åˆ¶äº†åœ¨æ— ç›‘ç£æœºå™¨äººç¯å¢ƒä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨æ˜¾è‘—æ¨¡æ€æŸå¤±çš„æƒ…å†µä¸‹ï¼Œè¯­ä¹‰ä¿¡æ¯çš„æå–ã€‚è¿™äº›æ–¹æ³•ä¹Ÿå¾€å¾€è®¡ç®—å¯†é›†ï¼Œå¯¼è‡´å®é™…åº”ç”¨ä¸­èµ„æºæ¶ˆè€—é«˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€Mambaå¢å¼ºTransformerï¼ˆM3ETï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§æ¨¡å‹ï¼Œä¸“ä¸ºé«˜æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ è€Œè®¾è®¡ï¼Œå°¤å…¶æ˜¯åœ¨ç§»åŠ¨å¹³å°ä¸Šã€‚é€šè¿‡ç»“åˆMambaæ¨¡å—å’ŒåŸºäºè¯­ä¹‰çš„è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼ŒM3ETä¼˜åŒ–äº†ç‰¹å¾èåˆã€å¯¹é½å’Œæ¨¡æ€é‡å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒM3ETæé«˜äº†è·¨ä»»åŠ¡æ€§èƒ½ï¼Œé¢„è®­ç»ƒæ¨ç†é€Ÿåº¦æé«˜äº†2.3å€ã€‚ç‰¹åˆ«æ˜¯ï¼ŒM3ETçš„æ ¸å¿ƒVQAä»»åŠ¡å‡†ç¡®ç‡ä¿æŒåœ¨0.74ï¼Œè€Œæ¨¡å‹å‚æ•°é‡å‡å°‘äº†0.67ã€‚è™½ç„¶EQAä»»åŠ¡çš„æ€§èƒ½æœ‰é™ï¼Œä½†M3ETçš„è½»é‡çº§è®¾è®¡ä½¿å…¶éå¸¸é€‚åˆéƒ¨ç½²åœ¨èµ„æºå—é™çš„æœºå™¨äººå¹³å°ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººè§†è§‰-è¯­è¨€å­¦ä¹ ä¸­ï¼Œç°æœ‰æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨æ–‡æœ¬æ¨¡æ€ï¼Œä¸”è®¡ç®—é‡å¤§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–æœ‰ç›‘ç£é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨æ— ç›‘ç£æœºå™¨äººç¯å¢ƒä¸­è¯­ä¹‰æå–èƒ½åŠ›å—é™ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨æ˜¾è‘—æ¨¡æ€æŸå¤±çš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œé«˜è®¡ç®—é‡ä½¿å¾—è¿™äº›æ–¹æ³•éš¾ä»¥åœ¨èµ„æºå—é™çš„æœºå™¨äººå¹³å°ä¸Šéƒ¨ç½²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ä¸ªè½»é‡çº§ä¸”é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°èåˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œå¹¶åœ¨èµ„æºå—é™çš„å¹³å°ä¸Šè¿è¡Œã€‚é€šè¿‡å¼•å…¥Mambaæ¨¡å—å’Œè¯­ä¹‰è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå–å’Œå¯¹é½ä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼Œä»è€Œæé«˜æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM3ETæ¨¡å‹åŸºäºTransformeræ¶æ„ï¼Œå¹¶å¼•å…¥äº†Mambaæ¨¡å—å’Œè¯­ä¹‰è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œä½¿ç”¨è§†è§‰å’Œè¯­è¨€ç¼–ç å™¨æå–ç‰¹å¾ï¼›ç„¶åï¼Œé€šè¿‡Mambaæ¨¡å—è¿›è¡Œåºåˆ—å»ºæ¨¡ï¼Œæ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»ï¼›æ¥ç€ï¼Œä½¿ç”¨è¯­ä¹‰è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç‰¹å¾èåˆï¼Œå¹¶è¿›è¡Œæ¨¡æ€é‡å»ºï¼›æœ€åï¼Œé€šè¿‡ä»»åŠ¡ç›¸å…³çš„å¤´éƒ¨è¿›è¡Œé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šM3ETçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å°†Mambaæ¨¡å—å¼•å…¥å¤šæ¨¡æ€å­¦ä¹ ï¼Œæé«˜äº†åºåˆ—å»ºæ¨¡çš„æ•ˆç‡ï¼›2) æå‡ºäº†è¯­ä¹‰è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®è¯­ä¹‰ä¿¡æ¯åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œæ›´å¥½åœ°èåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ã€‚è¿™ä¸ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ä¸åŒï¼Œåè€…é€šå¸¸æ˜¯é™æ€çš„æˆ–åŸºäºç®€å•çš„ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—ã€‚

**å…³é”®è®¾è®¡**ï¼šMambaæ¨¡å—çš„å…·ä½“é…ç½®ï¼ˆä¾‹å¦‚ï¼ŒçŠ¶æ€ç©ºé—´ç»´åº¦ã€é€‰æ‹©æœºåˆ¶ç­‰ï¼‰éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚è¯­ä¹‰è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶çš„è®¾è®¡åŒ…æ‹¬å¦‚ä½•è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–æ›´å¤æ‚çš„åº¦é‡å­¦ä¹ æ–¹æ³•ï¼‰ï¼Œä»¥åŠå¦‚ä½•å°†è¯­ä¹‰ä¿¡æ¯èå…¥æ³¨æ„åŠ›æƒé‡ä¸­ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘æ¨¡æ€é‡å»ºçš„æŸå¤±ï¼Œä»¥åŠç‰¹å®šä»»åŠ¡çš„æŸå¤±ï¼ˆä¾‹å¦‚ï¼ŒVQAä»»åŠ¡çš„åˆ†ç±»æŸå¤±ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

M3ETåœ¨VQAä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨ä¿æŒ0.74çš„å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ¨¡å‹å‚æ•°é‡å‡å°‘äº†0.67ã€‚æ­¤å¤–ï¼ŒM3ETçš„é¢„è®­ç»ƒæ¨ç†é€Ÿåº¦æé«˜äº†2.3å€ï¼Œä½¿å…¶æ›´é€‚åˆåœ¨èµ„æºå—é™çš„æœºå™¨äººå¹³å°ä¸Šéƒ¨ç½²ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒM3ETåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

M3ETé€‚ç”¨äºèµ„æºå—é™çš„æœºå™¨äººå¹³å°ï¼Œå¯åº”ç”¨äºå®¶åº­æœåŠ¡æœºå™¨äººã€ç§»åŠ¨å·¡æ£€æœºå™¨äººç­‰åœºæ™¯ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæå‡æœºå™¨äººå¯¹å¤æ‚ç¯å¢ƒçš„ç†è§£èƒ½åŠ›ï¼Œä½¿å…¶æ›´å¥½åœ°æ‰§è¡Œè§†è§‰é—®ç­”ã€ç¯å¢ƒæ„ŸçŸ¥ç­‰ä»»åŠ¡ã€‚æœªæ¥ï¼ŒM3ETæœ‰æœ›æ¨åŠ¨æœºå™¨äººæ™ºèƒ½åŒ–æ°´å¹³çš„æå‡ï¼Œä½¿å…¶åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, multimodal learning has become essential in robotic vision and information fusion, especially for understanding human behavior in complex environments. However, current methods struggle to fully leverage the textual modality, relying on supervised pretrained models, which limits semantic extraction in unsupervised robotic environments, particularly with significant modality loss. These methods also tend to be computationally intensive, leading to high resource consumption in real-world applications. To address these challenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a lightweight model designed for efficient multimodal learning, particularly on mobile platforms. By incorporating the Mamba module and a semantic-based adaptive attention mechanism, M3ET optimizes feature fusion, alignment, and modality reconstruction. Our experiments show that M3ET improves cross-task performance, with a 2.3 times increase in pretraining inference speed. In particular, the core VQA task accuracy of M3ET remains at 0.74, while the model's parameter count is reduced by 0.67. Although performance on the EQA task is limited, M3ET's lightweight design makes it well suited for deployment on resource-constrained robotic platforms.

