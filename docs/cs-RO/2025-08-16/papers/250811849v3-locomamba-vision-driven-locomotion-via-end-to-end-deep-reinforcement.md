---
layout: default
title: LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba
---

# LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.11849" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.11849v3</a>
  <a href="https://arxiv.org/pdf/2508.11849.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.11849v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.11849v3', 'LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yinuo Wang, Gavin Tao

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV, eess.IV, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-08-16 (æ›´æ–°: 2025-12-14)

**å¤‡æ³¨**: 14 pages. This paper has been published in Advanced Engineering Informatics. Please cite the journal version: DOI: 10.1016/j.aei.2025.104230

**æœŸåˆŠ**: Advanced Engineering Informatics, Vol. 70, Art. no. 104230 (2026)

**DOI**: [10.1016/j.aei.2025.104230](https://doi.org/10.1016/j.aei.2025.104230)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLocoMambaä»¥è§£å†³è§†è§‰é©±åŠ¨çš„è¿åŠ¨æ§åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰é©±åŠ¨` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `è·¨æ¨¡æ€èåˆ` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `è¿åŠ¨æ§åˆ¶` `æœºå™¨äººå¯¼èˆª` `è®­ç»ƒæ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¿åŠ¨æ§åˆ¶é¢ä¸´é•¿ç¨‹ä¾èµ–æ•æ‰å’Œè®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚
2. LocoMambaé€šè¿‡é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹å’ŒMambaå±‚å®ç°é«˜æ•ˆçš„åºåˆ—å»ºæ¨¡ï¼Œæå‡äº†çŠ¶æ€è¡¨ç¤ºå’Œè®­ç»ƒæ•ˆç‡ã€‚
3. åœ¨æŒ‘æˆ˜æ€§æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼ŒLocoMambaç›¸æ¯”äºç°æœ‰åŸºçº¿å–å¾—äº†æ›´é«˜çš„å›æŠ¥å’ŒæˆåŠŸç‡ï¼Œä¸”ç¢°æ’æ¬¡æ•°æ›´å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†LocoMambaï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹çš„è§†è§‰é©±åŠ¨è·¨æ¨¡æ€æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç‰¹åˆ«åˆ©ç”¨Mambaå®ç°è¿‘çº¿æ€§æ—¶é—´åºåˆ—å»ºæ¨¡ï¼Œæœ‰æ•ˆæ•æ‰é•¿ç¨‹ä¾èµ–ï¼Œå¹¶æ”¯æŒæ›´é•¿åºåˆ—çš„é«˜æ•ˆè®­ç»ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å¤šå±‚æ„ŸçŸ¥æœºåµŒå…¥æœ¬ä½“çŠ¶æ€ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œå¯¹æ·±åº¦å›¾åƒè¿›è¡Œåˆ†å—ï¼Œç”Ÿæˆç´§å‡‘çš„tokenä»¥æ”¹å–„çŠ¶æ€è¡¨ç¤ºã€‚å…¶æ¬¡ï¼Œå †å çš„Mambaå±‚é€šè¿‡è¿‘çº¿æ€§æ—¶é—´çš„é€‰æ‹©æ€§æ‰«æèåˆè¿™äº›tokenï¼Œé™ä½å»¶è¿Ÿå’Œå†…å­˜å ç”¨ï¼Œä¿æŒå¯¹tokené•¿åº¦å’Œå›¾åƒåˆ†è¾¨ç‡çš„é²æ£’æ€§ï¼Œå¹¶æä¾›ä¸€ç§å½’çº³åç½®ä»¥å‡è½»è¿‡æ‹Ÿåˆã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨åœ°å½¢å’Œå¤–è§‚éšæœºåŒ–ä»¥åŠéšœç¢ç‰©å¯†åº¦è¯¾ç¨‹ä¸‹ï¼Œä½¿ç”¨ç´§å‡‘çš„çŠ¶æ€ä¸­å¿ƒå¥–åŠ±å¯¹ç­–ç•¥è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œå¹³è¡¡è¿›å±•ã€å¹³æ»‘æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œè§†è§‰é©±åŠ¨çš„è¿åŠ¨æ§åˆ¶æ—¶ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•æ‰é•¿ç¨‹ä¾èµ–å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLocoMambaçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹å’ŒMambaå±‚ï¼Œé€šè¿‡é«˜æ•ˆçš„åºåˆ—å»ºæ¨¡æ¥æå‡çŠ¶æ€è¡¨ç¤ºå’Œè®­ç»ƒæ•ˆç‡ï¼Œä»è€Œæ›´å¥½åœ°åº”å¯¹å¤æ‚ç¯å¢ƒä¸­çš„è¿åŠ¨æ§åˆ¶ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLocoMambaçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯é€šè¿‡å¤šå±‚æ„ŸçŸ¥æœºåµŒå…¥æœ¬ä½“çŠ¶æ€ï¼Œæ¥ç€ä½¿ç”¨è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œå¯¹æ·±åº¦å›¾åƒè¿›è¡Œåˆ†å—ï¼Œæœ€åé€šè¿‡å †å çš„Mambaå±‚è¿›è¡Œtokençš„èåˆå’Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šLocoMambaçš„æœ€é‡è¦åˆ›æ–°åœ¨äºå…¶é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹å’ŒMambaå±‚çš„è®¾è®¡ï¼Œä½¿å¾—åºåˆ—å»ºæ¨¡çš„æ—¶é—´å¤æ‚åº¦æ¥è¿‘çº¿æ€§ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç´§å‡‘çš„çŠ¶æ€ä¸­å¿ƒå¥–åŠ±æœºåˆ¶ï¼Œå¹³è¡¡äº†è¿›å±•ã€å¹³æ»‘æ€§å’Œå®‰å…¨æ€§ï¼ŒåŒæ—¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†åœ°å½¢å’Œå¤–è§‚çš„éšæœºåŒ–ï¼Œä»¥åŠéšœç¢ç‰©å¯†åº¦çš„è¯¾ç¨‹è®¾ç½®ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒLocoMambaåœ¨å…·æœ‰é™æ€å’ŒåŠ¨æ€éšœç¢ç‰©ä»¥åŠä¸å¹³å¦åœ°å½¢çš„æŒ‘æˆ˜æ€§æ¨¡æ‹Ÿç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸æ¯”äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œå…¶å›æŠ¥å’ŒæˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œç¢°æ’æ¬¡æ•°å‡å°‘ï¼Œä¸”åœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹æ”¶æ•›æ‰€éœ€çš„æ›´æ–°æ¬¡æ•°æ›´å°‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LocoMambaçš„ç ”ç©¶æˆæœåœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½ç§»åŠ¨è®¾å¤‡ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜è¿åŠ¨æ§åˆ¶çš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„è‡ªä¸»å†³ç­–ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce LocoMamba, a vision-driven cross-modal DRL framework built on selective state-space models, specifically leveraging Mamba, that achieves near-linear-time sequence modeling, effectively captures long-range dependencies, and enables efficient training with longer sequences. First, we embed proprioceptive states with a multilayer perceptron and patchify depth images with a lightweight convolutional neural network, producing compact tokens that improve state representation. Second, stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint, remaining robust to token length and image resolution, and providing an inductive bias that mitigates overfitting. Third, we train the policy end-to-end with Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum, using a compact state-centric reward that balances progress, smoothness, and safety. We evaluate our method in challenging simulated environments with static and moving obstacles as well as uneven terrain. Compared with state-of-the-art baselines, our method achieves higher returns and success rates with fewer collisions, exhibits stronger generalization to unseen terrains and obstacle densities, and improves training efficiency by converging in fewer updates under the same compute budget.

