---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-09-25
---

# cs.ROï¼ˆ2025-09-25ï¼‰

ğŸ“Š å…± **34** ç¯‡è®ºæ–‡


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (24)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (24 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250921006v1-anywherevla-language-conditioned-exploration-and-mobile-manipulation.html">AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</a></td>
  <td>AnywhereVLAï¼šé¢å‘æœªè§ç¯å¢ƒçš„è¯­è¨€æ¡ä»¶ç§»åŠ¨æ“ä½œæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span> <span class="paper-tag">reachability-aware</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21006v1" data-paper-url="./papers/250921006v1-anywherevla-language-conditioned-exploration-and-mobile-manipulation.html" onclick="toggleFavorite(this, '2509.21006v1', 'AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250921231v1-seec-stable-end-effector-control-with-model-enhanced-residual-learni.html">SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation</a></td>
  <td>æå‡ºSEECæ¡†æ¶ï¼Œé€šè¿‡æ¨¡å‹å¢å¼ºæ®‹å·®å­¦ä¹ å®ç°äººå‹æœºå™¨äººç¨³å®šæœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">bipedal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21231v1" data-paper-url="./papers/250921231v1-seec-stable-end-effector-control-with-model-enhanced-residual-learni.html" onclick="toggleFavorite(this, '2509.21231v1', 'SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250920696v1-run-residual-policy-for-natural-humanoid-locomotion.html">RuN: Residual Policy for Natural Humanoid Locomotion</a></td>
  <td>æå‡ºRuNï¼šä¸€ç§æ®‹å·®ç­–ç•¥ï¼Œç”¨äºå®ç°è‡ªç„¶çš„äººå½¢æœºå™¨äººè¿åŠ¨æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">humanoid locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20696v1" data-paper-url="./papers/250920696v1-run-residual-policy-for-natural-humanoid-locomotion.html" onclick="toggleFavorite(this, '2509.20696v1', 'RuN: Residual Policy for Natural Humanoid Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250920703v1-joint-flow-trajectory-optimization-for-feasible-robot-motion-generat.html">Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations</a></td>
  <td>æå‡ºJoint Flow Trajectory Optimizationæ¡†æ¶ï¼Œè§£å†³è§†é¢‘æ¼”ç¤ºå­¦ä¹ ä¸­çš„æœºå™¨äººè¿åŠ¨ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">trajectory optimization</span> <span class="paper-tag">teleoperation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20703v1" data-paper-url="./papers/250920703v1-joint-flow-trajectory-optimization-for-feasible-robot-motion-generat.html" onclick="toggleFavorite(this, '2509.20703v1', 'Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250920717v1-robotdancing-residual-action-reinforcement-learning-enables-robust-l.html">RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking</a></td>
  <td>æå‡ºåŸºäºæ®‹å·®åŠ¨ä½œå¼ºåŒ–å­¦ä¹ çš„RobotDancingæ¡†æ¶ï¼Œå®ç°é²æ£’çš„äººå½¢æœºå™¨äººé•¿æ—¶ç¨‹è¿åŠ¨è·Ÿè¸ªã€‚</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">sim-to-real</span> <span class="paper-tag">Unitree</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20717v1" data-paper-url="./papers/250920717v1-robotdancing-residual-action-reinforcement-learning-enables-robust-l.html" onclick="toggleFavorite(this, '2509.20717v1', 'RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250921045v1-mpc-based-deep-reinforcement-learning-method-for-space-robotic-contr.html">MPC-based Deep Reinforcement Learning Method for Space Robotic Control with Fuel Sloshing Mitigation</a></td>
  <td>æå‡ºåŸºäºMPCçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæŠ‘åˆ¶ç‡ƒæ–™æ™ƒåŠ¨ç©ºé—´æœºå™¨äººæ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21045v1" data-paper-url="./papers/250921045v1-mpc-based-deep-reinforcement-learning-method-for-space-robotic-contr.html" onclick="toggleFavorite(this, '2509.21045v1', 'MPC-based Deep Reinforcement Learning Method for Space Robotic Control with Fuel Sloshing Mitigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250921020v1-multi-robot-vision-based-task-and-motion-planning-for-ev-battery-dis.html">Multi-Robot Vision-Based Task and Motion Planning for EV Battery Disassembly and Sorting</a></td>
  <td>æå‡ºå¤šæœºå™¨äººè§†è§‰ä»»åŠ¡ä¸è¿åŠ¨è§„åˆ’æ¡†æ¶ï¼Œç”¨äºç”µåŠ¨æ±½è½¦ç”µæ± æ‹†å¸ä¸åˆ†æ‹£</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">task and motion planning</span> <span class="paper-tag">TAMP</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21020v1" data-paper-url="./papers/250921020v1-multi-robot-vision-based-task-and-motion-planning-for-ev-battery-dis.html" onclick="toggleFavorite(this, '2509.21020v1', 'Multi-Robot Vision-Based Task and Motion Planning for EV Battery Disassembly and Sorting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250921690v2-towards-versatile-humanoid-table-tennis-unified-reinforcement-learni.html">Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation</a></td>
  <td>æå‡ºåŸºäºé¢„æµ‹å¢å¼ºçš„ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°é€šç”¨äººå½¢æœºå™¨äººä¹’ä¹“çƒ</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21690v2" data-paper-url="./papers/250921690v2-towards-versatile-humanoid-table-tennis-unified-reinforcement-learni.html" onclick="toggleFavorite(this, '2509.21690v2', 'Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250921256v1-binomap-learning-category-level-bimanual-non-prehensile-manipulation.html">BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation Primitives</a></td>
  <td>æå‡ºBiNoMaPï¼Œå­¦ä¹ ç±»åˆ«çº§åŒè‡‚éæŠ“å–æ“ä½œåŸè¯­ï¼Œè§£å†³æœºå™¨äººæ“ä½œæ³›åŒ–æ€§é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">dual-arm</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21256v1" data-paper-url="./papers/250921256v1-binomap-learning-category-level-bimanual-non-prehensile-manipulation.html" onclick="toggleFavorite(this, '2509.21256v1', 'BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation Primitives')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250921243v1-retovla-reusing-register-tokens-for-spatial-reasoning-in-vision-lang.html">RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models</a></td>
  <td>RetoVLAï¼šé€šè¿‡å¤ç”¨Register Tokenså¢å¼ºVLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21243v1" data-paper-url="./papers/250921243v1-retovla-reusing-register-tokens-for-spatial-reasoning-in-vision-lang.html" onclick="toggleFavorite(this, '2509.21243v1', 'RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250920646v1-suction-leap-hand-suction-cups-on-a-multi-fingered-hand-enable-embod.html">Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enable Embodied Dexterity and In-Hand Teleoperation</a></td>
  <td>æå‡ºåŸºäºå¸ç›˜çš„å¤šæŒ‡çµå·§æ‰‹SLeap Handï¼Œå®ç°è¶…è¶Šäººæ‰‹çš„çµå·§æ“ä½œä¸é¥æ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">in-hand manipulation</span> <span class="paper-tag">sim-to-real</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20646v1" data-paper-url="./papers/250920646v1-suction-leap-hand-suction-cups-on-a-multi-fingered-hand-enable-embod.html" onclick="toggleFavorite(this, '2509.20646v1', 'Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enable Embodied Dexterity and In-Hand Teleoperation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250921107v1-cross-modal-instructions-for-robot-motion-generation.html">Cross-Modal Instructions for Robot Motion Generation</a></td>
  <td>æå‡ºCrossInstructæ¡†æ¶ï¼Œåˆ©ç”¨è·¨æ¨¡æ€æŒ‡ä»¤ç”Ÿæˆæœºå™¨äººè¿åŠ¨è½¨è¿¹</td>
  <td class="tags-cell"><span class="paper-tag">teleoperation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21107v1" data-paper-url="./papers/250921107v1-cross-modal-instructions-for-robot-motion-generation.html" onclick="toggleFavorite(this, '2509.21107v1', 'Cross-Modal Instructions for Robot Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250920635v2-learning-terrain-specialized-policies-for-adaptive-locomotion-in-cha.html">Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments</a></td>
  <td>æå‡ºåœ°å½¢ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸çš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæå‡å¤æ‚ç¯å¢ƒä¸‹çš„æœºå™¨äººè¿åŠ¨èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20635v2" data-paper-url="./papers/250920635v2-learning-terrain-specialized-policies-for-adaptive-locomotion-in-cha.html" onclick="toggleFavorite(this, '2509.20635v2', 'Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250921571v1-autonomous-uav-quadruped-docking-in-complex-terrains-via-active-post.html">Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control</a></td>
  <td>æå‡ºä¸€ç§ä¸»åŠ¨å§¿æ€å¯¹é½å’Œçº¦æŸæ„ŸçŸ¥æ§åˆ¶çš„æ— äººæœº-å››è¶³æœºå™¨äººå¤æ‚åœ°å½¢è‡ªä¸»å¯¹æ¥æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21571v1" data-paper-url="./papers/250921571v1-autonomous-uav-quadruped-docking-in-complex-terrains-via-active-post.html" onclick="toggleFavorite(this, '2509.21571v1', 'Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250920841v1-imaginationpolicy-towards-generalizable-precise-and-reliable-end-to-.html">ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation</a></td>
  <td>æå‡ºåŸºäºè¿åŠ¨å¯¼å‘å…³é”®ç‚¹é“¾çš„æœºå™¨äººæ“ä½œç«¯åˆ°ç«¯ç­–ç•¥ï¼Œæå‡æ³›åŒ–æ€§ä¸ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20841v1" data-paper-url="./papers/250920841v1-imaginationpolicy-towards-generalizable-precise-and-reliable-end-to-.html" onclick="toggleFavorite(this, '2509.20841v1', 'ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250921145v2-dagdiff-guiding-dual-arm-grasp-diffusion-to-stable-and-collision-fre.html">DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free Grasps</a></td>
  <td>DAGDiffï¼šå¼•å¯¼åŒè‡‚æŠ“å–æ‰©æ•£å®ç°ç¨³å®šæ— ç¢°æ’çš„æŠ“å–</td>
  <td class="tags-cell"><span class="paper-tag">dual-arm</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21145v2" data-paper-url="./papers/250921145v2-dagdiff-guiding-dual-arm-grasp-diffusion-to-stable-and-collision-fre.html" onclick="toggleFavorite(this, '2509.21145v2', 'DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free Grasps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250920917v1-efficient-differentiable-contact-model-with-long-range-influence.html">Efficient Differentiable Contact Model with Long-range Influence</a></td>
  <td>æå‡ºé«˜æ•ˆå¯å¾®é•¿ç¨‹æ¥è§¦æ¨¡å‹ï¼Œæå‡æœºå™¨äººæ§åˆ¶ä¸ä¼˜åŒ–</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span> <span class="paper-tag">manipulation</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20917v1" data-paper-url="./papers/250920917v1-efficient-differentiable-contact-model-with-long-range-influence.html" onclick="toggleFavorite(this, '2509.20917v1', 'Efficient Differentiable Contact Model with Long-range Influence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250920656v1-eeg-driven-ar-robot-system-for-zero-touch-grasping-manipulation.html">EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation</a></td>
  <td>æå‡ºåŸºäºè„‘ç”µçš„AR-æœºå™¨äººç³»ç»Ÿï¼Œå®ç°é›¶æ¥è§¦æŠ“å–æ“ä½œï¼Œæå‡è¿åŠ¨éšœç¢äººå£«äººæœºäº¤äº’èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20656v1" data-paper-url="./papers/250920656v1-eeg-driven-ar-robot-system-for-zero-touch-grasping-manipulation.html" onclick="toggleFavorite(this, '2509.20656v1', 'EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250920653v2-cyber-racing-coach-a-haptic-shared-control-framework-for-teaching-ad.html">Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills</a></td>
  <td>æå‡ºè§¦è§‰å…±äº«æ§åˆ¶æ¡†æ¶ï¼Œç”¨äºæå‡é©¾é©¶å‘˜é«˜é˜¶é©¾é©¶æŠ€èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">shared control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20653v2" data-paper-url="./papers/250920653v2-cyber-racing-coach-a-haptic-shared-control-framework-for-teaching-ad.html" onclick="toggleFavorite(this, '2509.20653v2', 'Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250921496v1-wall-inspector-quadrotor-control-in-wall-proximity-through-model-com.html">Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation</a></td>
  <td>æå‡ºåŸºäºå¸åŠ›è¡¥å¿æ¨¡å‹é¢„æµ‹æ§åˆ¶çš„å››æ—‹ç¿¼è¿‘å¢™ç¨³å®šæ§åˆ¶æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">MAE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21496v1" data-paper-url="./papers/250921496v1-wall-inspector-quadrotor-control-in-wall-proximity-through-model-com.html" onclick="toggleFavorite(this, '2509.21496v1', 'Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250920739v1-slam-free-visual-navigation-with-hierarchical-vision-language-percep.html">SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning</a></td>
  <td>æå‡ºä¸€ç§åŸºäºè§†è§‰è¯­è¨€åˆ†å±‚æ„ŸçŸ¥å’Œç²—ç»†ç²’åº¦è¯­ä¹‰æ‹“æ‰‘è§„åˆ’çš„æ— SLAMè§†è§‰å¯¼èˆªæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20739v1" data-paper-url="./papers/250920739v1-slam-free-visual-navigation-with-hierarchical-vision-language-percep.html" onclick="toggleFavorite(this, '2509.20739v1', 'SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250921664v1-generating-stable-placements-via-physics-guided-diffusion-models.html">Generating Stable Placements via Physics-guided Diffusion Models</a></td>
  <td>æå‡ºåŸºäºç‰©ç†å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ”¾ç½®ç”Ÿæˆæ–¹æ³•ï¼Œæå‡æœºå™¨äººæ“ä½œçš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">penetration</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21664v1" data-paper-url="./papers/250921664v1-generating-stable-placements-via-physics-guided-diffusion-models.html" onclick="toggleFavorite(this, '2509.21664v1', 'Generating Stable Placements via Physics-guided Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250921242v1-fsglove-an-inertial-based-hand-tracking-system-with-shape-aware-cali.html">FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration</a></td>
  <td>FSGloveï¼šä¸€ç§åŸºäºæƒ¯æ€§ä¼ æ„Ÿçš„ã€å…·æœ‰å½¢çŠ¶æ„ŸçŸ¥æ ¡å‡†çš„æ‰‹éƒ¨è¿½è¸ªç³»ç»Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">MANO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21242v1" data-paper-url="./papers/250921242v1-fsglove-an-inertial-based-hand-tracking-system-with-shape-aware-cali.html" onclick="toggleFavorite(this, '2509.21242v1', 'FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250921445v1-developing-a-mono-actuated-compliant-geogami-robot.html">Developing a Mono-Actuated Compliant GeoGami Robot</a></td>
  <td>æå‡ºä¸€ç§å•é©±åŠ¨æŸ”æ€§GeoGamiæœºå™¨äººï¼Œç”¨äºå½¢çŠ¶å˜æ¢å’Œç§»åŠ¨</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21445v1" data-paper-url="./papers/250921445v1-developing-a-mono-actuated-compliant-geogami-robot.html" onclick="toggleFavorite(this, '2509.21445v1', 'Developing a Mono-Actuated Compliant GeoGami Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250920681v1-efficient-construction-of-implicit-surface-models-from-a-single-imag.html">Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation</a></td>
  <td>FINSï¼šä¸€ç§åŸºäºå•å¼ å›¾åƒå¿«é€Ÿæ„å»ºéšå¼è¡¨é¢æ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨äºæœºå™¨äººè¿åŠ¨ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">implicit representation</span> <span class="paper-tag">motion generation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20681v1" data-paper-url="./papers/250920681v1-efficient-construction-of-implicit-surface-models-from-a-single-imag.html" onclick="toggleFavorite(this, '2509.20681v1', 'Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250920709v1-digital-twin-guided-robot-path-planning-a-beta-bernoulli-fusion-with.html">Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor</a></td>
  <td>æå‡ºåŸºäºæ•°å­—å­ªç”Ÿçš„æœºå™¨äººè·¯å¾„è§„åˆ’æ–¹æ³•ï¼ŒèåˆLLMè¯­ä¹‰ç†è§£æå‡è·¯å¾„å®‰å…¨æ€§</td>
  <td class="tags-cell"><span class="paper-tag">semantic map</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20709v1" data-paper-url="./papers/250920709v1-digital-twin-guided-robot-path-planning-a-beta-bernoulli-fusion-with.html" onclick="toggleFavorite(this, '2509.20709v1', 'Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250921602v1-real-time-indoor-object-slam-with-llm-enhanced-priors.html">Real-Time Indoor Object SLAM with LLM-Enhanced Priors</a></td>
  <td>åˆ©ç”¨LLMå¢å¼ºå…ˆéªŒçŸ¥è¯†ï¼Œå®ç°å®æ—¶å®¤å†…ç‰©ä½“SLAM</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21602v1" data-paper-url="./papers/250921602v1-real-time-indoor-object-slam-with-llm-enhanced-priors.html" onclick="toggleFavorite(this, '2509.21602v1', 'Real-Time Indoor Object SLAM with LLM-Enhanced Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250920839v1-semsight-probabilistic-birds-eye-view-prediction-of-multi-level-scen.html">SemSight: Probabilistic Bird's-Eye-View Prediction of Multi-Level Scene Semantics for Navigation</a></td>
  <td>SemSightï¼šç”¨äºå¯¼èˆªçš„å¤šå±‚æ¬¡åœºæ™¯è¯­ä¹‰æ¦‚ç‡é¸Ÿç°å›¾é¢„æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">semantic map</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20839v1" data-paper-url="./papers/250920839v1-semsight-probabilistic-birds-eye-view-prediction-of-multi-level-scen.html" onclick="toggleFavorite(this, '2509.20839v1', 'SemSight: Probabilistic Bird&#39;s-Eye-View Prediction of Multi-Level Scene Semantics for Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/250921143v2-automotive-env-benchmarking-multimodal-agents-in-vehicle-interface-s.html">Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems</a></td>
  <td>æå‡º Automotive-ENV åŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°è½¦è½½ç•Œé¢ç³»ç»Ÿä¸­å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21143v2" data-paper-url="./papers/250921143v2-automotive-env-benchmarking-multimodal-agents-in-vehicle-interface-s.html" onclick="toggleFavorite(this, '2509.21143v2', 'Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250921543v1-plan2evolve-llm-self-evolution-for-improved-planning-capability-via-.html">Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation</a></td>
  <td>Plan2Evolveï¼šé€šè¿‡è‡ªåŠ¨é¢†åŸŸç”Ÿæˆå®ç°LLMè‡ªè¿›åŒ–ï¼Œæå‡è§„åˆ’èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21543v1" data-paper-url="./papers/250921543v1-plan2evolve-llm-self-evolution-for-improved-planning-capability-via-.html" onclick="toggleFavorite(this, '2509.21543v1', 'Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250920705v1-building-information-models-to-robot-ready-site-digital-twins-bim2rd.html">Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework</a></td>
  <td>BIM2RDTæ¡†æ¶ï¼šåˆ©ç”¨Agentic AIæ„å»ºæœºå™¨äººå¯ç”¨çš„å®‰å…¨å·¥åœ°æ•°å­—å­ªç”Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20705v1" data-paper-url="./papers/250920705v1-building-information-models-to-robot-ready-site-digital-twins-bim2rd.html" onclick="toggleFavorite(this, '2509.20705v1', 'Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>32</td>
  <td><a href="./papers/250921073v1-normalizing-flows-are-capable-visuomotor-policy-learning-models.html">Normalizing Flows are Capable Visuomotor Policy Learning Models</a></td>
  <td>æå‡ºåŸºäºNormalizing Flowsçš„è§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ æ¨¡å‹ï¼Œæå‡æ¨ç†é€Ÿåº¦å’Œç½®ä¿¡åº¦è¯„ä¼°ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">policy learning</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21073v1" data-paper-url="./papers/250921073v1-normalizing-flows-are-capable-visuomotor-policy-learning-models.html" onclick="toggleFavorite(this, '2509.21073v1', 'Normalizing Flows are Capable Visuomotor Policy Learning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250920766v2-leveraging-temporally-extended-behavior-sharing-for-multi-task-reinf.html">Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning</a></td>
  <td>æå‡ºMT-LÃ©vyï¼Œç»“åˆè¡Œä¸ºå…±äº«ä¸æ—¶åºæ‰©å±•æ¢ç´¢ï¼Œæå‡å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººé¢†åŸŸçš„æ ·æœ¬æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.20766v2" data-paper-url="./papers/250920766v2-leveraging-temporally-extended-behavior-sharing-for-multi-task-reinf.html" onclick="toggleFavorite(this, '2509.20766v2', 'Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>34</td>
  <td><a href="./papers/250921281v1-taxonomy-aware-dynamic-motion-generation-on-hyperbolic-manifolds.html">Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds</a></td>
  <td>æå‡ºGPHDMä»¥è§£å†³æœºå™¨äººè¿åŠ¨ç”Ÿæˆä¸­çš„å±‚æ¬¡ç»“æ„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21281v1" data-paper-url="./papers/250921281v1-taxonomy-aware-dynamic-motion-generation-on-hyperbolic-manifolds.html" onclick="toggleFavorite(this, '2509.21281v1', 'Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)