---
layout: default
title: EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation
---

# EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20656" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20656v1</a>
  <a href="https://arxiv.org/pdf/2509.20656.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20656v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20656v1', 'EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junzhe Wang, Jiarui Xie, Pengfei Hao, Zheng Li, Yi Cai

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**å¤‡æ³¨**: 8 pages, 14 figures, submitted to ICRA 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè„‘ç”µçš„AR-æœºå™¨äººç³»ç»Ÿï¼Œå®ç°é›¶æ¥è§¦æŠ“å–æ“ä½œï¼Œæå‡è¿åŠ¨éšœç¢äººå£«äººæœºäº¤äº’èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è„‘æœºæ¥å£` `å¢å¼ºç°å®` `æœºå™¨äººæ§åˆ¶` `è¿åŠ¨æƒ³è±¡` `è¾…åŠ©æœºå™¨äºº` `äººæœºäº¤äº’` `é›¶æ¥è§¦æ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è„‘æœºæ¥å£-æœºå™¨äººç³»ç»Ÿé¢ä¸´è„‘ç”µä¿¡å·å™ªå£°å¤§ã€ç›®æ ‡é€‰æ‹©ä¸çµæ´»ä»¥åŠç¼ºä¹é—­ç¯éªŒè¯ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨è¾…åŠ©åœºæ™¯ä¸­çš„å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºä¸€ç§é—­ç¯BCI-AR-Robotç³»ç»Ÿï¼Œåˆ©ç”¨è¿åŠ¨æƒ³è±¡è„‘ç”µä¿¡å·è§£ç ã€å¢å¼ºç°å®ç¥ç»åé¦ˆå’Œæœºå™¨äººæŠ“å–ï¼Œå®ç°é›¶æ¥è§¦æ“ä½œã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨è¿åŠ¨æƒ³è±¡è®­ç»ƒå‡†ç¡®ç‡ã€ä¿¡æ¯ä¼ è¾“é€Ÿç‡å’Œé—­ç¯æŠ“å–æˆåŠŸç‡æ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†ARåé¦ˆå¯¹è„‘ç”µæ§åˆ¶çš„ç¨³å®šä½œç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é—­ç¯è„‘æœºæ¥å£ï¼ˆBCIï¼‰-å¢å¼ºç°å®ï¼ˆARï¼‰-æœºå™¨äººç³»ç»Ÿï¼Œç”¨äºé›¶æ¥è§¦æ“ä½œã€‚è¯¥ç³»ç»Ÿé›†æˆäº†åŸºäºè¿åŠ¨æƒ³è±¡ï¼ˆMIï¼‰çš„è„‘ç”µä¿¡å·è§£ç ã€ARç¥ç»åé¦ˆå’Œæœºå™¨äººæŠ“å–ã€‚ä½¿ç”¨14é€šé“è„‘ç”µå¤´ç›”è¿›è¡Œä¸ªæ€§åŒ–çš„MIæ ¡å‡†ï¼ŒåŸºäºæ™ºèƒ½æ‰‹æœºçš„ARç•Œé¢æ”¯æŒå¤šç›®æ ‡å¯¼èˆªï¼Œå¹¶æä¾›æ–¹å‘ä¸€è‡´çš„åé¦ˆä»¥å¢å¼ºç¨³å®šæ€§ã€‚æœºå™¨äººæ‰‹è‡‚ç»“åˆå†³ç­–è¾“å‡ºå’ŒåŸºäºè§†è§‰çš„å§¿æ€ä¼°è®¡ï¼Œå®ç°è‡ªä¸»æŠ“å–ã€‚å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼šMIè®­ç»ƒè¾¾åˆ°93.1%çš„å‡†ç¡®ç‡ï¼Œå¹³å‡ä¿¡æ¯ä¼ è¾“é€Ÿç‡ï¼ˆITRï¼‰ä¸º14.8 bit/minï¼›ARç¥ç»åé¦ˆæ˜¾è‘—æé«˜äº†æŒç»­æ§åˆ¶èƒ½åŠ›ï¼ˆSCI = 0.210ï¼‰ï¼Œå¹¶å®ç°äº†æœ€é«˜çš„ITRï¼ˆ21.3 bit/minï¼‰ï¼Œä¼˜äºé™æ€ã€ä¼ªåé¦ˆå’Œæ— ARåŸºçº¿ï¼›é—­ç¯æŠ“å–æˆåŠŸç‡è¾¾åˆ°97.2%ï¼Œå…·æœ‰è‰¯å¥½çš„æ•ˆç‡å’Œç”¨æˆ·æ§åˆ¶æ„Ÿã€‚ç»“æœè¡¨æ˜ï¼ŒARåé¦ˆæ˜¾è‘—ç¨³å®šäº†åŸºäºè„‘ç”µçš„æ§åˆ¶ï¼Œæ‰€æå‡ºçš„æ¡†æ¶å®ç°äº†é²æ£’çš„é›¶æ¥è§¦æŠ“å–ï¼Œæ¨åŠ¨äº†è¾…åŠ©æœºå™¨äººåº”ç”¨å’Œæœªæ¥äººæœºäº¤äº’æ¨¡å¼çš„å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¿åŠ¨éšœç¢äººå£«åœ¨äººæœºäº¤äº’ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³å¦‚ä½•åˆ©ç”¨è„‘æœºæ¥å£æŠ€æœ¯å®ç°å¯¹æœºå™¨äººçš„ç²¾ç¡®æ§åˆ¶ï¼Œä»è€Œå®Œæˆè¯¸å¦‚æŠ“å–ç­‰æ“ä½œã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºè„‘ç”µä¿¡å·çš„å™ªå£°å¹²æ‰°ã€ç›®æ ‡é€‰æ‹©çš„å±€é™æ€§ä»¥åŠç¼ºä¹æœ‰æ•ˆçš„åé¦ˆæœºåˆ¶ï¼Œå¯¼è‡´æ§åˆ¶ä¸ç¨³å®šä¸”æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆå¢å¼ºç°å®ï¼ˆARï¼‰æŠ€æœ¯ï¼Œä¸ºç”¨æˆ·æä¾›å®æ—¶çš„è§†è§‰åé¦ˆï¼Œä»è€Œå¢å¼ºè„‘æœºæ¥å£æ§åˆ¶çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡ARç•Œé¢ï¼Œç”¨æˆ·å¯ä»¥æ›´ç›´è§‚åœ°äº†è§£æœºå™¨äººçš„çŠ¶æ€å’Œç›®æ ‡ä½ç½®ï¼Œä»è€Œæ›´å¥½åœ°è°ƒæ•´è¿åŠ¨æƒ³è±¡ï¼Œæé«˜æ§åˆ¶æ•ˆæœã€‚åŒæ—¶ï¼Œç»“åˆè§†è§‰ä¼ºæœæŠ€æœ¯ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿè‡ªä¸»å®ŒæˆæŠ“å–åŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç³»ç»Ÿä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼š1) åŸºäºè¿åŠ¨æƒ³è±¡ï¼ˆMIï¼‰çš„è„‘ç”µä¿¡å·è§£ç æ¨¡å—ï¼Œç”¨äºè¯†åˆ«ç”¨æˆ·çš„æ§åˆ¶æ„å›¾ï¼›2) åŸºäºæ™ºèƒ½æ‰‹æœºçš„ARç¥ç»åé¦ˆæ¨¡å—ï¼Œä¸ºç”¨æˆ·æä¾›å®æ—¶çš„è§†è§‰åé¦ˆï¼Œå¢å¼ºæ§åˆ¶ç¨³å®šæ€§ï¼›3) æœºå™¨äººæŠ“å–æ¨¡å—ï¼Œç»“åˆå†³ç­–è¾“å‡ºå’ŒåŸºäºè§†è§‰çš„å§¿æ€ä¼°è®¡ï¼Œå®ç°è‡ªä¸»æŠ“å–ã€‚æ•´ä¸ªç³»ç»Ÿæ„æˆä¸€ä¸ªé—­ç¯æ§åˆ¶ç³»ç»Ÿï¼Œç”¨æˆ·é€šè¿‡è„‘ç”µä¿¡å·æ§åˆ¶æœºå™¨äººï¼Œæœºå™¨äººæ‰§è¡ŒåŠ¨ä½œåï¼Œé€šè¿‡ARç•Œé¢å°†ç»“æœåé¦ˆç»™ç”¨æˆ·ï¼Œç”¨æˆ·æ ¹æ®åé¦ˆè°ƒæ•´æ§åˆ¶ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¢å¼ºç°å®æŠ€æœ¯å¼•å…¥åˆ°è„‘æœºæ¥å£-æœºå™¨äººæ§åˆ¶ç³»ç»Ÿä¸­ï¼Œåˆ©ç”¨ARç•Œé¢ä¸ºç”¨æˆ·æä¾›å®æ—¶çš„ã€æ–¹å‘ä¸€è‡´çš„è§†è§‰åé¦ˆï¼Œæ˜¾è‘—æé«˜äº†è„‘ç”µæ§åˆ¶çš„ç¨³å®šæ€§å’Œä¿¡æ¯ä¼ è¾“é€Ÿç‡ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜å®ç°äº†åŸºäºè§†è§‰çš„è‡ªä¸»æŠ“å–ï¼Œå‡å°‘äº†äººå·¥å¹²é¢„ï¼Œæé«˜äº†æ“ä½œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è„‘ç”µä¿¡å·è§£ç æ–¹é¢ï¼Œé‡‡ç”¨äº†14é€šé“è„‘ç”µå¤´ç›”è¿›è¡Œæ•°æ®é‡‡é›†ï¼Œå¹¶é’ˆå¯¹æ¯ä¸ªç”¨æˆ·è¿›è¡Œä¸ªæ€§åŒ–çš„MIæ ¡å‡†ã€‚åœ¨ARç¥ç»åé¦ˆæ–¹é¢ï¼Œè®¾è®¡äº†æ–¹å‘ä¸€è‡´çš„è§†è§‰æç¤ºï¼Œå¼•å¯¼ç”¨æˆ·è¿›è¡Œè¿åŠ¨æƒ³è±¡ã€‚åœ¨æœºå™¨äººæŠ“å–æ–¹é¢ï¼Œç»“åˆäº†å†³ç­–è¾“å‡ºå’Œè§†è§‰ä¼ºæœæŠ€æœ¯ï¼Œå®ç°äº†è‡ªä¸»æŠ“å–ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šä¸ªæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿åŠ¨æƒ³è±¡è®­ç»ƒçš„å‡†ç¡®ç‡è¾¾åˆ°93.1%ï¼Œå¹³å‡ä¿¡æ¯ä¼ è¾“é€Ÿç‡ï¼ˆITRï¼‰ä¸º14.8 bit/minã€‚ARç¥ç»åé¦ˆæ˜¾è‘—æé«˜äº†æŒç»­æ§åˆ¶èƒ½åŠ›ï¼ˆSCI = 0.210ï¼‰ï¼Œå¹¶å®ç°äº†æœ€é«˜çš„ITRï¼ˆ21.3 bit/minï¼‰ï¼Œä¼˜äºé™æ€ã€ä¼ªåé¦ˆå’Œæ— ARåŸºçº¿ã€‚é—­ç¯æŠ“å–æˆåŠŸç‡è¾¾åˆ°97.2%ï¼Œè¡¨æ˜è¯¥ç³»ç»Ÿå…·æœ‰è‰¯å¥½çš„é²æ£’æ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¾…åŠ©æœºå™¨äººé¢†åŸŸï¼Œå¸®åŠ©è¿åŠ¨éšœç¢äººå£«å®Œæˆæ—¥å¸¸ç”Ÿæ´»ä¸­çš„å„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚å–ç‰©ã€è¿›é£Ÿç­‰ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯åº”ç”¨äºè¿œç¨‹æ“ä½œã€å±é™©ç¯å¢ƒä½œä¸šç­‰é¢†åŸŸï¼Œæé«˜æ“ä½œçš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œéšç€è„‘æœºæ¥å£æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œè¯¥ç³»ç»Ÿæœ‰æœ›æˆä¸ºä¸€ç§é‡è¦çš„è¾…åŠ©å·¥å…·ï¼Œæ”¹å–„äººä»¬çš„ç”Ÿæ´»è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reliable brain-computer interface (BCI) control of robots provides an intuitive and accessible means of human-robot interaction, particularly valuable for individuals with motor impairments. However, existing BCI-Robot systems face major limitations: electroencephalography (EEG) signals are noisy and unstable, target selection is often predefined and inflexible, and most studies remain restricted to simulation without closed-loop validation. These issues hinder real-world deployment in assistive scenarios. To address them, we propose a closed-loop BCI-AR-Robot system that integrates motor imagery (MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic grasping for zero-touch operation. A 14-channel EEG headset enabled individualized MI calibration, a smartphone-based AR interface supported multi-target navigation with direction-congruent feedback to enhance stability, and the robotic arm combined decision outputs with vision-based pose estimation for autonomous grasping. Experiments are conducted to validate the framework: MI training achieved 93.1 percent accuracy with an average information transfer rate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained control (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with static, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2 percent success rate with good efficiency and strong user-reported control. These results show that AR feedback substantially stabilizes EEG-based control and that the proposed framework enables robust zero-touch grasping, advancing assistive robotic applications and future modes of human-robot interaction.

