---
layout: default
title: Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning
---

# Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20766" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20766v2</a>
  <a href="https://arxiv.org/pdf/2509.20766.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20766v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20766v2', 'Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gawon Lee, Daesol Cho, H. Jin Kim

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25 (æ›´æ–°: 2025-09-29)

**å¤‡æ³¨**: Accepted for publication in the proceedings of the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMT-LÃ©vyï¼Œç»“åˆè¡Œä¸ºå…±äº«ä¸æ—¶åºæ‰©å±•æ¢ç´¢ï¼Œæå‡å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººé¢†åŸŸçš„æ ·æœ¬æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººå­¦ä¹ ` `è¡Œä¸ºå…±äº«` `æ—¶åºæ‰©å±•æ¢ç´¢` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æœºå™¨äººå¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ é¢ä¸´æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. MT-LÃ©vyç»“åˆè·¨ä»»åŠ¡è¡Œä¸ºå…±äº«ä¸æ—¶åºæ‰©å±•æ¢ç´¢ï¼Œå¼•å¯¼æ™ºèƒ½ä½“æ›´é«˜æ•ˆåœ°æ¢ç´¢å…³é”®çŠ¶æ€ã€‚
3. å®éªŒè¯æ˜MT-LÃ©vyæ˜¾è‘—æå‡äº†æ¢ç´¢æ•ˆç‡å’Œæ ·æœ¬åˆ©ç”¨ç‡ï¼Œä¸ºæœºå™¨äººå¤šä»»åŠ¡å­¦ä¹ æä¾›äº†æ–°æ€è·¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ (MTRL)é€šè¿‡åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè®­ç»ƒæ™ºèƒ½ä½“ï¼Œå®ç°çŸ¥è¯†å…±äº«ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ”¶é›†å¤šæ ·åŒ–ä»»åŠ¡æ•°æ®çš„æˆæœ¬é«˜æ˜‚ï¼Œå°†MTRLåº”ç”¨äºæœºå™¨äººé¢†åŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¢ç´¢ç­–ç•¥MT-LÃ©vyï¼Œå®ƒç»“åˆäº†è·¨ä»»åŠ¡çš„è¡Œä¸ºå…±äº«å’Œå—LÃ©vyé£è¡Œå¯å‘çš„æ—¶åºæ‰©å±•æ¢ç´¢ï¼Œä»è€Œæé«˜äº†MTRLç¯å¢ƒä¸­çš„æ ·æœ¬æ•ˆç‡ã€‚MT-LÃ©vyåˆ©ç”¨åœ¨ç›¸å…³ä»»åŠ¡ä¸Šè®­ç»ƒçš„ç­–ç•¥æ¥å¼•å¯¼æ¢ç´¢åˆ°å…³é”®çŠ¶æ€ï¼ŒåŒæ—¶æ ¹æ®ä»»åŠ¡æˆåŠŸç‡åŠ¨æ€è°ƒæ•´æ¢ç´¢æ°´å¹³ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¦†ç›–çŠ¶æ€ç©ºé—´ï¼Œå³ä½¿åœ¨å¤æ‚çš„æœºå™¨äººç¯å¢ƒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMT-LÃ©vyæ˜¾è‘—æé«˜äº†æ¢ç´¢å’Œæ ·æœ¬æ•ˆç‡ï¼Œå¹¶é€šè¿‡å®šé‡å’Œå®šæ€§åˆ†æå¾—åˆ°äº†æ”¯æŒã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥çªå‡ºäº†æ¯ä¸ªç»„æˆéƒ¨åˆ†çš„è´¡çŒ®ï¼Œè¡¨æ˜å°†è¡Œä¸ºå…±äº«ä¸è‡ªé€‚åº”æ¢ç´¢ç­–ç•¥ç›¸ç»“åˆå¯ä»¥æ˜¾è‘—æé«˜MTRLåœ¨æœºå™¨äººåº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººé¢†åŸŸçš„åº”ç”¨å—é™äºæ•°æ®æ”¶é›†çš„é«˜æˆæœ¬ï¼Œå¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥å……åˆ†æ¢ç´¢çŠ¶æ€ç©ºé—´ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨ä»»åŠ¡é—´æœ‰æ•ˆå…±äº«çŸ¥è¯†ï¼Œå¹¶ä¸”ç¼ºä¹é’ˆå¯¹æœºå™¨äººç¯å¢ƒçš„æœ‰æ•ˆæ¢ç´¢ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMT-LÃ©vyçš„æ ¸å¿ƒåœ¨äºç»“åˆè·¨ä»»åŠ¡çš„è¡Œä¸ºå…±äº«å’Œå—LÃ©vyé£è¡Œå¯å‘çš„æ—¶åºæ‰©å±•æ¢ç´¢ã€‚é€šè¿‡åˆ©ç”¨ç›¸å…³ä»»åŠ¡çš„ç­–ç•¥æ¥æŒ‡å¯¼æ¢ç´¢ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¿«åœ°æ‰¾åˆ°æœ‰ä»·å€¼çš„çŠ¶æ€ã€‚åŒæ—¶ï¼ŒåŠ¨æ€è°ƒæ•´æ¢ç´¢æ°´å¹³ï¼Œæ ¹æ®ä»»åŠ¡æˆåŠŸç‡è‡ªé€‚åº”åœ°è¿›è¡Œæ¢ç´¢ï¼Œé¿å…æ— æ•ˆæ¢ç´¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMT-LÃ©vyçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŸºäºç›¸å…³ä»»åŠ¡ç­–ç•¥çš„è¡Œä¸ºå…±äº«æ¨¡å—ï¼Œç”¨äºå¼•å¯¼æ¢ç´¢æ–¹å‘ï¼›2) å—LÃ©vyé£è¡Œå¯å‘çš„æ—¶åºæ‰©å±•æ¢ç´¢æ¨¡å—ï¼Œç”¨äºåœ¨æ—¶é—´ç»´åº¦ä¸Šæ‰©å±•æ¢ç´¢èŒƒå›´ï¼›3) è‡ªé€‚åº”æ¢ç´¢æ°´å¹³è°ƒæ•´æ¨¡å—ï¼Œæ ¹æ®ä»»åŠ¡æˆåŠŸç‡åŠ¨æ€è°ƒæ•´æ¢ç´¢å¼ºåº¦ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œå®ç°é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´è¦†ç›–ã€‚

**å…³é”®åˆ›æ–°**ï¼šMT-LÃ©vyçš„å…³é”®åˆ›æ–°åœ¨äºå°†è¡Œä¸ºå…±äº«ä¸æ—¶åºæ‰©å±•æ¢ç´¢ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”æ¢ç´¢æ°´å¹³è°ƒæ•´æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„æ¢ç´¢ç­–ç•¥ç›¸æ¯”ï¼ŒMT-LÃ©vyèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å·²æœ‰çš„çŸ¥è¯†ï¼Œå¹¶æ ¹æ®ä»»åŠ¡çš„è¿›å±•æƒ…å†µåŠ¨æ€è°ƒæ•´æ¢ç´¢ç­–ç•¥ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šMT-LÃ©vyçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•é€‰æ‹©ç›¸å…³ä»»åŠ¡çš„ç­–ç•¥è¿›è¡Œè¡Œä¸ºå…±äº«ï¼›2) å¦‚ä½•è®¾è®¡æ—¶åºæ‰©å±•æ¢ç´¢çš„ç­–ç•¥ï¼Œä¾‹å¦‚LÃ©vyé£è¡Œçš„æ­¥é•¿å’Œæ–¹å‘ï¼›3) å¦‚ä½•å®šä¹‰ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹¶å°†å…¶ç”¨äºè‡ªé€‚åº”è°ƒæ•´æ¢ç´¢æ°´å¹³ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„éœ€è¦æ ¹æ®å…·ä½“çš„æœºå™¨äººä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMT-LÃ©vyåœ¨å¤šä¸ªæœºå™¨äººä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ¢ç´¢æ•ˆç‡å’Œæ ·æœ¬æ•ˆç‡ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒMT-LÃ©vyèƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶å–å¾—æ›´é«˜çš„ç´¯ç§¯å¥–åŠ±ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†è¡Œä¸ºå…±äº«å’Œè‡ªé€‚åº”æ¢ç´¢ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MT-LÃ©vyå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ï¼Œå¯ä»¥é™ä½æœºå™¨äººå­¦ä¹ çš„æˆæœ¬ï¼ŒåŠ é€Ÿæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚è¯¥ç ”ç©¶å¯¹äºæ¨åŠ¨æœºå™¨äººæ™ºèƒ½åŒ–å’Œè‡ªåŠ¨åŒ–å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-task reinforcement learning (MTRL) offers a promising approach to improve sample efficiency and generalization by training agents across multiple tasks, enabling knowledge sharing between them. However, applying MTRL to robotics remains challenging due to the high cost of collecting diverse task data. To address this, we propose MT-LÃ©vy, a novel exploration strategy that enhances sample efficiency in MTRL environments by combining behavior sharing across tasks with temporally extended exploration inspired by LÃ©vy flight. MT-LÃ©vy leverages policies trained on related tasks to guide exploration towards key states, while dynamically adjusting exploration levels based on task success ratios. This approach enables more efficient state-space coverage, even in complex robotics environments. Empirical results demonstrate that MT-LÃ©vy significantly improves exploration and sample efficiency, supported by quantitative and qualitative analyses. Ablation studies further highlight the contribution of each component, showing that combining behavior sharing with adaptive exploration strategies can significantly improve the practicality of MTRL in robotics applications.

