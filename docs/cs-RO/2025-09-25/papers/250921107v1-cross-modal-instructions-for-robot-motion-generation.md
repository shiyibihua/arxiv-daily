---
layout: default
title: Cross-Modal Instructions for Robot Motion Generation
---

# Cross-Modal Instructions for Robot Motion Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21107" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.21107v1</a>
  <a href="https://arxiv.org/pdf/2509.21107.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21107v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21107v1', 'Cross-Modal Instructions for Robot Motion Generation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: William Barron, Xiaoxiang Dong, Matthew Johnson-Roberson, Weiming Zhi

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-25

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CrossInstructÊ°ÜÊû∂ÔºåÂà©Áî®Ë∑®Ê®°ÊÄÅÊåá‰ª§ÁîüÊàêÊú∫Âô®‰∫∫ËøêÂä®ËΩ®Ëøπ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫ËøêÂä®ÁîüÊàê` `Ë∑®Ê®°ÊÄÅÂ≠¶‰π†` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÊú∫Âô®‰∫∫Ë°å‰∏∫Â≠¶‰π†‰æùËµñ‰∫éËÄóÊó∂ÁöÑÊï∞ÊçÆÈááÈõÜÔºåÂ¶ÇÈÅ•Êìç‰ΩúÊàñÁâ©ÁêÜÂºïÂØºÔºåÈöæ‰ª•Êâ©Â±ï„ÄÇ
2. CrossInstructÊ°ÜÊû∂Âà©Áî®Ë∑®Ê®°ÊÄÅÊåá‰ª§ÔºàÊñáÊú¨Ê†áÁ≠æÁ≠âÔºâ‰Ωú‰∏∫ËøêÂä®ÊºîÁ§∫ÔºåÈôç‰Ωé‰∫ÜÊï∞ÊçÆÊî∂ÈõÜÁöÑÈöæÂ∫¶„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåCrossInstructËÉΩÁîüÊàêÂèØÊ≥õÂåñÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫ÔºåÂπ∂‰∏∫Âº∫ÂåñÂ≠¶‰π†Êèê‰æõÊúâÊïàÁöÑÂàùÂßãÂåñ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫Â≠¶‰π†ËåÉÂºèÔºö‰ªéË∑®Ê®°ÊÄÅÊåá‰ª§‰∏≠Â≠¶‰π†„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Á≤óÁï•ÁöÑÊ†áÊ≥®ÔºàÂåÖÂê´Ëá™Áî±ÊñáÊú¨Ê†áÁ≠æÔºâ‰Ωú‰∏∫ËøêÂä®ÊºîÁ§∫ÔºåÊõø‰ª£‰º†ÁªüÁöÑÈÅ•Êìç‰ΩúÊàñÁâ©ÁêÜÂºïÂØº„ÄÇËÆ∫ÊñáÊèêÂá∫‰∫ÜCrossInstructÊ°ÜÊû∂ÔºåÂ∞ÜË∑®Ê®°ÊÄÅÊåá‰ª§‰Ωú‰∏∫‰∏ä‰∏ãÊñáËæìÂÖ•Âà∞Âü∫Á°ÄËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)‰∏≠„ÄÇVLMËø≠‰ª£Êü•ËØ¢‰∏Ä‰∏™ËæÉÂ∞èÁöÑ„ÄÅÂæÆË∞ÉÂêéÁöÑÊ®°ÂûãÔºåÂπ∂ÂêàÊàêÂ§ö‰∏™2DËßÜÂõæ‰∏äÁöÑÊúüÊúõËøêÂä®„ÄÇËøô‰∫õËßÜÂõæÈöèÂêéË¢´ËûçÂêà‰∏∫Êú∫Âô®‰∫∫Â∑•‰ΩúÁ©∫Èó¥‰∏≠3DËøêÂä®ËΩ®ËøπÁöÑËøûË¥ØÂàÜÂ∏É„ÄÇÈÄöËøáÁªìÂêàÂ§ßÂûãVLMÁöÑÊé®ÁêÜËÉΩÂäõÂíåÁ≤æÁªÜÁöÑÊåáÂêëÊ®°ÂûãÔºåCrossInstructÁîüÊàêÂèØÊâßË°åÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫ÔºåËøô‰∫õË°å‰∏∫ÂèØ‰ª•Ê≥õÂåñÂà∞ÊúâÈôêÊåá‰ª§Á§∫‰æãÁöÑÁéØÂ¢É‰πãÂ§ñ„ÄÇËÆ∫ÊñáËøòÂºïÂÖ•‰∫Ü‰∏Ä‰∏™‰∏ãÊ∏∏Âº∫ÂåñÂ≠¶‰π†ÊµÅÁ®ãÔºåÂà©Áî®CrossInstructÁöÑËæìÂá∫Êù•È´òÊïàÂú∞Â≠¶‰π†ÂÆåÊàêÁ≤æÁªÜ‰ªªÂä°ÁöÑÁ≠ñÁï•„ÄÇÂú®Âü∫ÂáÜ‰ªøÁúü‰ªªÂä°ÂíåÁúüÂÆûÁ°¨‰ª∂‰∏äÁöÑÂÆûÈ™åËØÑ‰º∞Ë°®ÊòéÔºåCrossInstructÊó†ÈúÄÈ¢ùÂ§ñÂæÆË∞ÉÂç≥ÂèØÊúâÊïàÂ∑•‰ΩúÔºåÂπ∂‰∏∫ÂêéÁª≠ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊîπËøõÁöÑÁ≠ñÁï•Êèê‰æõ‰∫ÜÂº∫Â§ßÁöÑÂàùÂßãÂåñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊú∫Âô®‰∫∫Ë°å‰∏∫Â≠¶‰π†ÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫é‰∫∫Â∑•Á§∫ÊïôÔºå‰æãÂ¶ÇÈÅ•Êìç‰ΩúÊàñÁâ©ÁêÜÂºïÂØº„ÄÇËøô‰∫õÊñπÊ≥ïÊï∞ÊçÆÈááÈõÜÊàêÊú¨È´òÊòÇÔºåÈöæ‰ª•Êâ©Â±ïÂà∞Â§çÊùÇÁöÑ‰ªªÂä°ÂíåÁéØÂ¢É„ÄÇÊ≠§Â§ñÔºåÂ¶Ç‰ΩïÂà©Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÁ≠âË∑®Ê®°ÊÄÅ‰ø°ÊÅØÊù•ÊåáÂØºÊú∫Âô®‰∫∫ËøêÂä®‰πüÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÂº∫Â§ßÊé®ÁêÜËÉΩÂäõÔºåÂ∞ÜÁ≤óÁï•ÁöÑË∑®Ê®°ÊÄÅÊåá‰ª§Ôºà‰æãÂ¶ÇÊñáÊú¨ÊèèËø∞ÔºâËΩ¨Âåñ‰∏∫Êú∫Âô®‰∫∫ÂèØ‰ª•ÁêÜËß£ÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇÈÄöËøáÂ∞ÜÊåá‰ª§‰Ωú‰∏∫‰∏ä‰∏ãÊñá‰ø°ÊÅØËæìÂÖ•VLMÔºåÂπ∂ÁªìÂêà‰∏Ä‰∏™Á≤æÁªÜÁöÑÊåáÂêëÊ®°ÂûãÔºåÂèØ‰ª•ÁîüÊàêÊõ¥ÂÖ∑Ê≥õÂåñËÉΩÂäõÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCrossInstructÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) **Ë∑®Ê®°ÊÄÅÊåá‰ª§ÁºñÁ†ÅÂô®**ÔºöÂ∞ÜÊñáÊú¨Ê†áÁ≠æÁ≠âË∑®Ê®°ÊÄÅÊåá‰ª§ÁºñÁ†Å‰∏∫ÂêëÈáèË°®Á§∫„ÄÇ2) **ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ**ÔºöÊé•Êî∂ÁºñÁ†ÅÂêéÁöÑÊåá‰ª§ÂíåÂΩìÂâçÁéØÂ¢ÉÁöÑËßÜËßâ‰ø°ÊÅØÔºåÊé®ÁêÜÂá∫ÊúüÊúõÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇ3) **ÊåáÂêëÊ®°Âûã**Ôºö‰∏Ä‰∏™Â∞èÂûã„ÄÅÂæÆË∞ÉÂêéÁöÑÊ®°ÂûãÔºåÁî®‰∫éÁ≤æÁ°ÆÂÆö‰ΩçÁõÆÊ†áÁâ©‰ΩìÔºåËæÖÂä©VLMÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇ4) **ËøêÂä®ËΩ®ËøπËûçÂêàÊ®°Âùó**ÔºöÂ∞ÜÂ§ö‰∏™2DËßÜÂõæ‰∏äÁöÑËøêÂä®ËΩ®ËøπËûçÂêà‰∏∫3DÁ©∫Èó¥‰∏≠ÁöÑËøûË¥ØÂàÜÂ∏É„ÄÇ5) **Âº∫ÂåñÂ≠¶‰π†Ê®°Âùó**ÔºöÂà©Áî®CrossInstructÁîüÊàêÁöÑËΩ®Ëøπ‰Ωú‰∏∫ÂàùÂßãÂåñÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ëøõ‰∏ÄÊ≠•‰ºòÂåñÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂà©Áî®Â§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊù•ÁêÜËß£Ë∑®Ê®°ÊÄÅÊåá‰ª§ÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫Êú∫Âô®‰∫∫ËøêÂä®„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÁ§∫ÊïôÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåCrossInstructÈôç‰Ωé‰∫ÜÊï∞ÊçÆÈááÈõÜÁöÑÈöæÂ∫¶ÔºåÂπ∂ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÁªìÂêàVLMÂíåÁ≤æÁªÜÁöÑÊåáÂêëÊ®°ÂûãÔºåÂèØ‰ª•ÁîüÊàêÊõ¥ÂáÜÁ°Æ„ÄÅÊõ¥ÂèØÊéßÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVLMÁöÑÈÄâÊã©Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÈúÄË¶ÅÈÄâÊã©ÂÖ∑ÊúâÂº∫Â§ßÊé®ÁêÜËÉΩÂäõÂíåÊ≥õÂåñËÉΩÂäõÁöÑÊ®°Âûã„ÄÇÊåáÂêëÊ®°ÂûãÁöÑËÆ≠ÁªÉÈúÄË¶ÅÂ§ßÈáèÁöÑÊ†áÊ≥®Êï∞ÊçÆÔºåÂèØ‰ª•‰ΩøÁî®ÂêàÊàêÊï∞ÊçÆÊàñ‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆ„ÄÇËøêÂä®ËΩ®ËøπËûçÂêàÊ®°ÂùóÁöÑËÆæËÆ°ÈúÄË¶ÅËÄÉËôë‰∏çÂêåËßÜÂõæ‰πãÈó¥ÁöÑÂá†‰ΩïÂÖ≥Á≥ªÔºåÂèØ‰ª•‰ΩøÁî®Âç°Â∞îÊõºÊª§Ê≥¢Á≠âÊñπÊ≥ï„ÄÇÂº∫ÂåñÂ≠¶‰π†Ê®°ÂùóÁöÑÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°ÈúÄË¶Å‰∏é‰ªªÂä°ÁõÆÊ†áÁõ∏ÂåπÈÖç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CrossInstructÂú®‰ªøÁúüÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠ÈÉΩÂèñÂæó‰∫ÜÊòæËëóÊàêÊûú„ÄÇÂú®‰ªøÁúüÁéØÂ¢É‰∏≠ÔºåCrossInstructËÉΩÂ§üÊàêÂäüÂÆåÊàêÂêÑÁßçÊìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇÊäìÂèñ„ÄÅÊîæÁΩÆÂíåÁßªÂä®Áâ©‰Ωì„ÄÇÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠ÔºåCrossInstructÊó†ÈúÄÈ¢ùÂ§ñÂæÆË∞ÉÂç≥ÂèØÁîüÊàêÂèØÊâßË°åÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫ÔºåÂπ∂‰∏∫ÂêéÁª≠ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊîπËøõÁöÑÁ≠ñÁï•Êèê‰æõ‰∫ÜÂº∫Â§ßÁöÑÂàùÂßãÂåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCrossInstructÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫Ëá™Âä®ÂåñÂú∫ÊôØÔºå‰æãÂ¶ÇÔºöÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑËØ≠Èü≥Êåá‰ª§ÂÆåÊàêÂÆ∂Âä°ÔºõÂ∑•‰∏öÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆÊñáÊú¨ÊèèËø∞ÊâßË°åË£ÖÈÖç‰ªªÂä°ÔºõÂåªÁñóÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆÂåªÁîüÁöÑÊåáÁ§∫ËøõË°åÊâãÊúØÊìç‰Ωú„ÄÇËØ•ÊñπÊ≥ïÈôç‰Ωé‰∫ÜÊú∫Âô®‰∫∫ÁºñÁ®ãÁöÑÈó®ÊßõÔºå‰ΩøÈùû‰∏ì‰∏ö‰∫∫Âëò‰πüËÉΩËΩªÊùæÂú∞ÊéßÂà∂Êú∫Âô®‰∫∫„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Teaching robots novel behaviors typically requires motion demonstrations via teleoperation or kinaesthetic teaching, that is, physically guiding the robot. While recent work has explored using human sketches to specify desired behaviors, data collection remains cumbersome, and demonstration datasets are difficult to scale. In this paper, we introduce an alternative paradigm, Learning from Cross-Modal Instructions, where robots are shaped by demonstrations in the form of rough annotations, which can contain free-form text labels, and are used in lieu of physical motion. We introduce the CrossInstruct framework, which integrates cross-modal instructions as examples into the context input to a foundational vision-language model (VLM). The VLM then iteratively queries a smaller, fine-tuned model, and synthesizes the desired motion over multiple 2D views. These are then subsequently fused into a coherent distribution over 3D motion trajectories in the robot's workspace. By incorporating the reasoning of the large VLM with a fine-grained pointing model, CrossInstruct produces executable robot behaviors that generalize beyond the environment of in the limited set of instruction examples. We then introduce a downstream reinforcement learning pipeline that leverages CrossInstruct outputs to efficiently learn policies to complete fine-grained tasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and real hardware, demonstrating effectiveness without additional fine-tuning and providing a strong initialization for policies subsequently refined via reinforcement learning.

