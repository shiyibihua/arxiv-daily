---
layout: default
title: Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation
---

# Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21543" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21543v1</a>
  <a href="https://arxiv.org/pdf/2509.21543.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21543v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21543v1', 'Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinbang Huang, Zhiyuan Li, Zhanguang Zhang, Xingyue Quan, Jianye Hao, Yingxue Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**å¤‡æ³¨**: 25 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Plan2Evolveï¼šé€šè¿‡è‡ªåŠ¨é¢†åŸŸç”Ÿæˆå®ç°LLMè‡ªè¿›åŒ–ï¼Œæå‡è§„åˆ’èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨äººä»»åŠ¡è§„åˆ’` `è‡ªåŠ¨è§„åˆ’é¢†åŸŸç”Ÿæˆ` `æ€ç»´é“¾` `è‡ªè¿›åŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å°†LLMç”Ÿæˆçš„è§„åˆ’é¢†åŸŸè§†ä¸ºæœç´¢å·¥å…·ï¼Œå¿½ç•¥äº†å…¶ä½œä¸ºå¯æ‰©å±•æ¨ç†æ•°æ®æ¥æºçš„æ½œåŠ›ã€‚
2. Plan2Evolveæ¡†æ¶åˆ©ç”¨LLMè‡ªç”Ÿæˆè§„åˆ’é¢†åŸŸï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ‰©å±•çš„CoTè½¨è¿¹ï¼Œå¯¹é½ç¬¦å·è§„åˆ’å’Œè‡ªç„¶è¯­è¨€æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æå‡LLMçš„è§„åˆ’æˆåŠŸç‡ã€è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é™ä½æ¨ç†æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡é›†æˆç¬¦å·æœç´¢çš„è‡ªåŠ¨è§„åˆ’é¢†åŸŸç”Ÿæˆã€‚ç„¶è€Œï¼Œå…ˆå‰çš„æ–¹æ³•ä¸»è¦å°†è¿™äº›é¢†åŸŸè§†ä¸ºæœç´¢å·¥å…·ï¼Œè€Œå¯¹å…¶ä½œä¸ºå¯æ‰©å±•çš„æ¨ç†æ•°æ®æ¥æºçš„æ½œåŠ›å…³æ³¨ä¸è¶³ã€‚ä¸æ­¤åŒæ—¶ï¼Œæ¨ç†LLMsçš„è¿›å±•å—åˆ°æ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£çš„æ¨åŠ¨ï¼Œä½†å…¶åœ¨æœºå™¨äººé¢†åŸŸçš„åº”ç”¨ä»ç„¶ä¾èµ–äºæ˜‚è´µçš„äººå·¥ç­–åˆ’æ•°æ®é›†ã€‚æˆ‘ä»¬æå‡ºäº†Plan2Evolveï¼Œä¸€ä¸ªLLMè‡ªè¿›åŒ–æ¡†æ¶ï¼Œå…¶ä¸­åŸºç¡€æ¨¡å‹ç”Ÿæˆè§„åˆ’é¢†åŸŸï¼Œä½œä¸ºäº§ç”Ÿç¬¦å·é—®é¢˜-è®¡åˆ’å¯¹ä½œä¸ºæ¨ç†è½¨è¿¹çš„å¼•æ“ã€‚è¿™äº›å¯¹éšåé€šè¿‡ç›¸åŒçš„æ¨¡å‹ï¼Œå€ŸåŠ©è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œè¢«è½¬æ¢ä¸ºæ‰©å±•çš„CoTè½¨è¿¹ï¼Œä»è€Œæ˜¾å¼åœ°å°†ç¬¦å·è§„åˆ’ç»“æ„ä¸è‡ªç„¶è¯­è¨€æ¨ç†å¯¹é½ã€‚ç”±æ­¤äº§ç”Ÿçš„æ•°æ®è¶…è¶Šäº†æ¨¡å‹å›ºæœ‰çš„è§„åˆ’èƒ½åŠ›ï¼Œä»è€Œèƒ½å¤Ÿè¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªè§„åˆ’å¢å¼ºçš„LLMï¼Œè¯¥LLMå…·æœ‰æ›´é«˜çš„è§„åˆ’æˆåŠŸç‡ã€æ›´å¼ºçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›å’Œæ›´ä½çš„æ¨ç†æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³LLMåœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸­ï¼Œä¾èµ–äººå·¥æ ‡æ³¨CoTæ•°æ®è¿›è¡Œæ¨ç†èƒ½åŠ›æå‡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†LLMç”Ÿæˆçš„è§„åˆ’é¢†åŸŸä»…ä½œä¸ºæœç´¢å·¥å…·ï¼Œå¿½ç•¥äº†å…¶ä½œä¸ºå¤§è§„æ¨¡æ¨ç†æ•°æ®æ¥æºçš„æ½œåŠ›ï¼Œä¸”äººå·¥æ ‡æ³¨CoTæ•°æ®æˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMè‡ªèº«ç”Ÿæˆè§„åˆ’é¢†åŸŸï¼Œå¹¶åŸºäºæ­¤è‡ªåŠ¨ç”Ÿæˆé—®é¢˜-è®¡åˆ’å¯¹ï¼Œå†å°†è¿™äº›é—®é¢˜-è®¡åˆ’å¯¹è½¬åŒ–ä¸ºæ‰©å±•çš„CoTè½¨è¿¹ï¼Œä»è€Œå®ç°LLMçš„è‡ªè¿›åŒ–ï¼Œæå‡å…¶è§„åˆ’èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥é¿å…å¯¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPlan2Evolveæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **è§„åˆ’é¢†åŸŸç”Ÿæˆ**ï¼šåŸºç¡€LLMç”Ÿæˆè§„åˆ’é¢†åŸŸæè¿°ï¼›2) **é—®é¢˜-è®¡åˆ’å¯¹ç”Ÿæˆ**ï¼šåˆ©ç”¨ç”Ÿæˆçš„è§„åˆ’é¢†åŸŸä½œä¸ºå¼•æ“ï¼Œç”Ÿæˆç¬¦å·é—®é¢˜-è®¡åˆ’å¯¹ï¼›3) **CoTè½¨è¿¹ç”Ÿæˆ**ï¼šä½¿ç”¨LLMå°†é—®é¢˜-è®¡åˆ’å¯¹è½¬åŒ–ä¸ºåŒ…å«è‡ªç„¶è¯­è¨€è§£é‡Šçš„æ‰©å±•CoTè½¨è¿¹ï¼›4) **æ¨¡å‹å¾®è°ƒ**ï¼šä½¿ç”¨ç”Ÿæˆçš„CoTæ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œæå‡å…¶è§„åˆ’èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªLLMè‡ªè¿›åŒ–çš„æ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆè§„åˆ’é¢†åŸŸå’ŒCoTè½¨è¿¹ï¼Œå®ç°äº†LLMè§„åˆ’èƒ½åŠ›çš„è‡ªæˆ‘æå‡ï¼Œæ‘†è„±äº†å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPlan2Evolveèƒ½å¤Ÿæ›´é«˜æ•ˆã€æ›´ç»æµåœ°æå‡LLMçš„è§„åˆ’èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šå¦‚ä½•è®¾è®¡æç¤ºè¯ï¼Œå¼•å¯¼LLMç”Ÿæˆé«˜è´¨é‡çš„è§„åˆ’é¢†åŸŸæè¿°ï¼›å¦‚ä½•å°†ç¬¦å·é—®é¢˜-è®¡åˆ’å¯¹è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°çš„CoTè½¨è¿¹ï¼›ä»¥åŠå¦‚ä½•é€‰æ‹©åˆé€‚çš„å¾®è°ƒç­–ç•¥ï¼Œä»¥æœ€å¤§ç¨‹åº¦åœ°æå‡LLMçš„è§„åˆ’èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°ã€ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPlan2Evolveèƒ½å¤Ÿæ˜¾è‘—æå‡LLMçš„è§„åˆ’æˆåŠŸç‡å’Œè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæœºå™¨äººä»»åŠ¡è§„åˆ’æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”é™ä½äº†æ¨ç†æˆæœ¬ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡çš„å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Plan2Evolveå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºæœºå™¨äººä»»åŠ¡è§„åˆ’ã€æ¸¸æˆAIã€æ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡LLMåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å†³ç­–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œä»»åŠ¡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„é¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have recently shown strong potential in robotic task planning, particularly through automatic planning domain generation that integrates symbolic search. Prior approaches, however, have largely treated these domains as search utilities, with limited attention to their potential as scalable sources of reasoning data. At the same time, progress in reasoning LLMs has been driven by chain-of-thought (CoT) supervision, whose application in robotics remains dependent on costly, human-curated datasets. We propose Plan2Evolve, an LLM self-evolving framework in which the base model generates planning domains that serve as engines for producing symbolic problem-plan pairs as reasoning traces. These pairs are then transformed into extended CoT trajectories by the same model through natural-language explanations, thereby explicitly aligning symbolic planning structures with natural language reasoning. The resulting data extend beyond the model's intrinsic planning capacity, enabling model fine-tuning that yields a planning-enhanced LLM with improved planning success, stronger cross-task generalization, and reduced inference costs.

