---
layout: default
title: RuN: Residual Policy for Natural Humanoid Locomotion
---

# RuN: Residual Policy for Natural Humanoid Locomotion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20696" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20696v1</a>
  <a href="https://arxiv.org/pdf/2509.20696.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20696v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20696v1', 'RuN: Residual Policy for Natural Humanoid Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qingpeng Li, Chengrui Zhu, Yanming Wu, Xin Yuan, Zhen Zhang, Jian Yang, Yong Liu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRuNï¼šä¸€ç§æ®‹å·®ç­–ç•¥ï¼Œç”¨äºå®ç°è‡ªç„¶çš„äººå½¢æœºå™¨äººè¿åŠ¨æ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `äººå½¢æœºå™¨äºº` `å¼ºåŒ–å­¦ä¹ ` `è¿åŠ¨æ§åˆ¶` `æ®‹å·®å­¦ä¹ ` `æ­¥æ€ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥è®©äººå½¢æœºå™¨äººå®ç°è‡ªç„¶æµç•…çš„æ­¥æ€åˆ‡æ¢ï¼Œå°¤å…¶æ˜¯åœ¨è¡Œèµ°å’Œè·‘æ­¥ä¹‹é—´å¹³æ»‘è¿‡æ¸¡ï¼Œå› ä¸ºéœ€è¦å•ä¸ªç­–ç•¥åŒæ—¶å¤„ç†å¤šç§ä»»åŠ¡ã€‚
2. RuNçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è¿åŠ¨æ§åˆ¶ä»»åŠ¡è§£è€¦ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è¿åŠ¨ç”Ÿæˆå™¨æä¾›è¿åŠ¨å…ˆéªŒï¼Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥å­¦ä¹ æ®‹å·®æ ¡æ­£ï¼Œä»è€Œç®€åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚
3. åœ¨Unitree G1æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRuNåœ¨0-2.5m/sçš„é€Ÿåº¦èŒƒå›´å†…å®ç°äº†ç¨³å®šè‡ªç„¶çš„æ­¥æ€åˆ‡æ¢ï¼Œå¹¶åœ¨è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRuNçš„è§£è€¦æ®‹å·®å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿äººå½¢æœºå™¨äººèƒ½å¤Ÿåœ¨å¹¿æ³›çš„é€Ÿåº¦èŒƒå›´å†…å®ç°è‡ªç„¶å’ŒåŠ¨æ€çš„è¿åŠ¨ï¼ŒåŒ…æ‹¬ä»è¡Œèµ°å¹³æ»‘è¿‡æ¸¡åˆ°è·‘æ­¥ã€‚ç°æœ‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦ç­–ç•¥ç›´æ¥è·Ÿè¸ªå‚è€ƒè¿åŠ¨ï¼Œè¿«ä½¿å•ä¸ªç­–ç•¥åŒæ—¶å­¦ä¹ è¿åŠ¨æ¨¡ä»¿ã€é€Ÿåº¦è·Ÿè¸ªå’Œç¨³å®šæ€§ç»´æŒã€‚RuNé€šè¿‡å°†é¢„è®­ç»ƒçš„æ¡ä»¶è¿åŠ¨ç”Ÿæˆå™¨ï¼ˆæä¾›è¿åŠ¨å­¦ä¸Šè‡ªç„¶çš„è¿åŠ¨å…ˆéªŒï¼‰ä¸å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ˆå­¦ä¹ è½»é‡çº§çš„æ®‹å·®æ ¡æ­£ä»¥å¤„ç†åŠ¨åŠ›å­¦äº¤äº’ï¼‰ç›¸ç»“åˆæ¥åˆ†è§£æ§åˆ¶ä»»åŠ¡ã€‚åœ¨Unitree G1äººå½¢æœºå™¨äººä¸Šçš„ä»¿çœŸå’ŒçœŸå®å®éªŒè¡¨æ˜ï¼ŒRuNåœ¨å¹¿æ³›çš„é€Ÿåº¦èŒƒå›´ï¼ˆ0-2.5 m/sï¼‰å†…å®ç°äº†ç¨³å®šã€è‡ªç„¶çš„æ­¥æ€å’Œå¹³æ»‘çš„è¡Œèµ°-è·‘æ­¥è¿‡æ¸¡ï¼Œåœ¨è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½æ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„äººå½¢æœºå™¨äººè¿åŠ¨æ§åˆ¶æ–¹æ³•ï¼Œé€šå¸¸éœ€è¦ç­–ç•¥ç½‘ç»œç›´æ¥æ¨¡ä»¿å‚è€ƒè¿åŠ¨ï¼Œè¿™ä½¿å¾—å•ä¸ªç­–ç•¥éœ€è¦åŒæ—¶å­¦ä¹ è¿åŠ¨æ¨¡ä»¿ã€é€Ÿåº¦è·Ÿè¸ªå’Œå¹³è¡¡ç»´æŒç­‰å¤šé‡ä»»åŠ¡ã€‚è¿™ç§è€¦åˆçš„æ–¹å¼å¯¼è‡´è®­ç»ƒå›°éš¾ï¼Œéš¾ä»¥å®ç°è‡ªç„¶æµç•…çš„æ­¥æ€åˆ‡æ¢ï¼Œå°¤å…¶æ˜¯åœ¨è¡Œèµ°å’Œè·‘æ­¥ä¹‹é—´è¿›è¡Œå¹³æ»‘è¿‡æ¸¡æ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRuNçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤æ‚çš„è¿åŠ¨æ§åˆ¶ä»»åŠ¡è§£è€¦ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šä¸€ä¸ªé¢„è®­ç»ƒçš„æ¡ä»¶è¿åŠ¨ç”Ÿæˆå™¨å’Œä¸€ä¸ªæ®‹å·®å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚è¿åŠ¨ç”Ÿæˆå™¨è´Ÿè´£æä¾›ä¸€ä¸ªè¿åŠ¨å­¦ä¸Šè‡ªç„¶çš„è¿åŠ¨å…ˆéªŒï¼Œè€Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥åˆ™è´Ÿè´£å­¦ä¹ ä¸€ä¸ªè½»é‡çº§çš„æ®‹å·®æ ¡æ­£ï¼Œä»¥å¤„ç†åŠ¨åŠ›å­¦äº¤äº’å’Œç¯å¢ƒå˜åŒ–ã€‚é€šè¿‡è¿™ç§è§£è€¦ï¼Œå¯ä»¥æ˜¾è‘—é™ä½å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å­¦ä¹ éš¾åº¦ï¼Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRuNçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ¡ä»¶è¿åŠ¨ç”Ÿæˆå™¨ï¼ˆConditional Motion Generatorï¼‰å’Œæ®‹å·®å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ˆResidual Reinforcement Learning Policyï¼‰ã€‚é¦–å…ˆï¼Œæ¡ä»¶è¿åŠ¨ç”Ÿæˆå™¨æ ¹æ®æœŸæœ›çš„é€Ÿåº¦ç­‰æ¡ä»¶ç”Ÿæˆä¸€ä¸ªåŸºç¡€è¿åŠ¨åºåˆ—ã€‚ç„¶åï¼Œæ®‹å·®å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¥æ”¶å½“å‰çŠ¶æ€å’ŒåŸºç¡€è¿åŠ¨åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªæ®‹å·®æ ¡æ­£é‡ã€‚æœ€ç»ˆçš„æ§åˆ¶æŒ‡ä»¤æ˜¯åŸºç¡€è¿åŠ¨åºåˆ—å’Œæ®‹å·®æ ¡æ­£é‡çš„å åŠ ã€‚

**å…³é”®åˆ›æ–°**ï¼šRuNçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è§£è€¦çš„æ®‹å·®å­¦ä¹ æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒRuNå°†è¿åŠ¨æ§åˆ¶ä»»åŠ¡åˆ†è§£ä¸ºè¿åŠ¨å…ˆéªŒç”Ÿæˆå’Œæ®‹å·®æ ¡æ­£ä¸¤ä¸ªéƒ¨åˆ†ï¼Œæ˜¾è‘—é™ä½äº†å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å­¦ä¹ éš¾åº¦ã€‚æ­¤å¤–ï¼ŒRuNé‡‡ç”¨è½»é‡çº§çš„æ®‹å·®æ ¡æ­£ç­–ç•¥ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†åŠ¨åŠ›å­¦äº¤äº’å’Œç¯å¢ƒå˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¡ä»¶è¿åŠ¨ç”Ÿæˆå™¨å¯ä»¥ä½¿ç”¨å„ç§è¿åŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œä¾‹å¦‚è¿åŠ¨æ•æ‰æ•°æ®é©±åŠ¨çš„æ–¹æ³•æˆ–å‚æ•°åŒ–çš„è¿åŠ¨æ¨¡å‹ã€‚æ®‹å·®å¼ºåŒ–å­¦ä¹ ç­–ç•¥é€šå¸¸é‡‡ç”¨Actor-Criticç»“æ„ï¼Œå…¶ä¸­Actorç½‘ç»œè¾“å‡ºæ®‹å·®æ ¡æ­£é‡ï¼ŒCriticç½‘ç»œè¯„ä¼°å½“å‰çŠ¶æ€çš„ä»·å€¼ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬å¥–åŠ±å‡½æ•°ã€çŠ¶æ€è·Ÿè¸ªè¯¯å·®å’ŒåŠ¨ä½œæ­£åˆ™åŒ–é¡¹ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦é¼“åŠ±æœºå™¨äººä¿æŒå¹³è¡¡ã€è·Ÿè¸ªæœŸæœ›é€Ÿåº¦å’Œæ‰§è¡Œè‡ªç„¶è¿åŠ¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRuNåœ¨Unitree G1äººå½¢æœºå™¨äººä¸Šå®ç°äº†ç¨³å®šã€è‡ªç„¶çš„æ­¥æ€å’Œå¹³æ»‘çš„è¡Œèµ°-è·‘æ­¥è¿‡æ¸¡ï¼Œé€Ÿåº¦èŒƒå›´ä¸º0-2.5 m/sã€‚ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒRuNåœ¨è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚ä¾‹å¦‚ï¼ŒRuNèƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°ç¨³å®šçš„æ­¥æ€ï¼Œå¹¶ä¸”åœ¨ç›¸åŒè®­ç»ƒæ—¶é—´å†…ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜çš„é€Ÿåº¦å’Œæ›´è‡ªç„¶çš„è¿åŠ¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RuNæŠ€æœ¯å¯åº”ç”¨äºå„ç§äººå½¢æœºå™¨äººåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æœæ•‘ã€ç‰©æµã€åº·å¤è®­ç»ƒå’Œå¨±ä¹ç­‰ã€‚é€šè¿‡å®ç°è‡ªç„¶æµç•…çš„è¿åŠ¨æ§åˆ¶ï¼ŒRuNå¯ä»¥æé«˜äººå½¢æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæ“ä½œèƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æœåŠ¡äºäººç±»ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æœºå™¨äººï¼Œä¾‹å¦‚å››è¶³æœºå™¨äººå’ŒåŒè‡‚æœºå™¨äººã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Enabling humanoid robots to achieve natural and dynamic locomotion across a wide range of speeds, including smooth transitions from walking to running, presents a significant challenge. Existing deep reinforcement learning methods typically require the policy to directly track a reference motion, forcing a single policy to simultaneously learn motion imitation, velocity tracking, and stability maintenance. To address this, we introduce RuN, a novel decoupled residual learning framework. RuN decomposes the control task by pairing a pre-trained Conditional Motion Generator, which provides a kinematically natural motion prior, with a reinforcement learning policy that learns a lightweight residual correction to handle dynamical interactions. Experiments in simulation and reality on the Unitree G1 humanoid robot demonstrate that RuN achieves stable, natural gaits and smooth walk-run transitions across a broad velocity range (0-2.5 m/s), outperforming state-of-the-art methods in both training efficiency and final performance.

