---
layout: default
title: "RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence"
---

# RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24653" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24653v1</a>
  <a href="https://arxiv.org/pdf/2512.24653.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24653v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24653v1', 'RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengkai Hou, Kun Wu, Jiaming Liu, Zhengping Che, Di Wu, Fei Liao, Guangrun Li, Jingyang He, Qiuxuan Feng, Zhao Jin, Chenyang Gu, Zhuoyang Liu, Nuowei Han, Xiangju Mi, Yaoxu Lv, Yankai Fu, Gaole Dai, Langzhe Gu, Tao Li, Yuheng Zhang, Yixue Zhang, Xinhua Wang, Shichao Fan, Meng Li, Zhen Zhao, Ning Liu, Zhiyuan Xu, Pei Ren, Junjie Ji, Haonan Liu, Kuan Cheng, Shanghang Zhang, Jian Tang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RoboMIND 2.0ï¼šç”¨äºé€šç”¨å…·èº«æ™ºèƒ½çš„å¤šæ¨¡æ€åŒè‡‚ç§»åŠ¨æ“ä½œæ•°æ®é›†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å…·èº«æ™ºèƒ½` `å¤šæ¨¡æ€å­¦ä¹ ` `æ•°æ®é›†` `æ¨¡ä»¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•å—é™äºå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œæ¼”ç¤ºæ•°æ®åŒ®ä¹ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. RoboMIND 2.0é€šè¿‡æ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå¹¶ç»“åˆæ•°å­—å­ªç”Ÿï¼Œä¿ƒè¿›å…·èº«æ™ºèƒ½çš„sim-to-realè¿ç§»ã€‚
3. æå‡ºçš„MIND-2ç³»ç»Ÿï¼Œåˆ©ç”¨åˆ†å±‚åŒç³»ç»Ÿæ¡†æ¶ï¼Œå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬åŒ–ä¸ºæœºå™¨äººå¯æ‰§è¡Œçš„åŠ¨ä½œã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†RoboMIND 2.0ï¼Œä¸€ä¸ªå…¨é¢çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡31ä¸‡æ¡åŒè‡‚æ“ä½œè½¨è¿¹ï¼Œæ¶µç›–å…­ç§ä¸åŒçš„æœºå™¨äººå½¢æ€å’Œ739ä¸ªå¤æ‚ä»»åŠ¡ã€‚ä¸ºäº†æ”¯æŒæ¥è§¦ä¸°å¯Œçš„å’Œç©ºé—´æ‰©å±•çš„ä»»åŠ¡ç ”ç©¶ï¼Œè¯¥æ•°æ®é›†è¿˜åŒ…å«äº†1.2ä¸‡ä¸ªè§¦è§‰å¢å¼ºçš„ç‰‡æ®µå’Œ2ä¸‡ä¸ªç§»åŠ¨æ“ä½œè½¨è¿¹ã€‚ä¸ºäº†è¡¥å……ç‰©ç†æ•°æ®ï¼Œä½œè€…æ„å»ºäº†çœŸå®ä¸–ç•Œç¯å¢ƒçš„é«˜ä¿çœŸæ•°å­—å­ªç”Ÿï¼Œå¹¶å‘å¸ƒäº†é¢å¤–çš„2ä¸‡æ¡è½¨è¿¹çš„æ¨¡æ‹Ÿæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›é²æ£’çš„sim-to-realè¿ç§»ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨RoboMIND 2.0çš„æ½œåŠ›ï¼Œä½œè€…æå‡ºäº†MIND-2ç³»ç»Ÿï¼Œä¸€ä¸ªé€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„åˆ†å±‚åŒç³»ç»Ÿæ¡†æ¶ã€‚MIND-2é›†æˆäº†é«˜å±‚è¯­ä¹‰è§„åˆ’å™¨(MIND-2-VLM)ï¼Œå°†æŠ½è±¡çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ†è§£ä¸ºå…·ä½“çš„å­ç›®æ ‡ï¼Œä»¥åŠä¸€ä¸ªä½å±‚è§†è§‰-è¯­è¨€-åŠ¨ä½œæ‰§è¡Œå™¨(MIND-2-VLA)ï¼Œç”Ÿæˆç²¾ç¡®çš„ã€æ„ŸçŸ¥è‡ªèº«çŠ¶æ€çš„è¿åŠ¨åŠ¨ä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•éš¾ä»¥åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­æ³›åŒ–åˆ°é•¿æ—¶ç¨‹åŒè‡‚ä»»åŠ¡å’Œç§»åŠ¨æ“ä½œï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œæ¼”ç¤ºæ•°æ®ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡å’Œç©ºé—´æ‰©å±•çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ¨¡æ€çš„æœºå™¨äººæ“ä½œæ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿç¯å¢ƒçš„æ•°æ®ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªåˆ†å±‚æ§åˆ¶æ¡†æ¶ï¼Œå°†é«˜çº§è¯­ä¹‰æŒ‡ä»¤è½¬åŒ–ä¸ºä½çº§è¿åŠ¨åŠ¨ä½œã€‚é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ§åˆ¶ç­–ç•¥ï¼Œæé«˜æœºå™¨äººçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMIND-2ç³»ç»Ÿé‡‡ç”¨åˆ†å±‚åŒç³»ç»Ÿæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šMIND-2-VLMï¼ˆé«˜å±‚è¯­ä¹‰è§„åˆ’å™¨ï¼‰å’ŒMIND-2-VLAï¼ˆä½å±‚è§†è§‰-è¯­è¨€-åŠ¨ä½œæ‰§è¡Œå™¨ï¼‰ã€‚MIND-2-VLMè´Ÿè´£å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ†è§£ä¸ºå…·ä½“çš„å­ç›®æ ‡ï¼ŒMIND-2-VLAè´Ÿè´£æ ¹æ®è§†è§‰ä¿¡æ¯ã€è¯­è¨€æŒ‡ä»¤å’Œè‡ªèº«çŠ¶æ€ç”Ÿæˆç²¾ç¡®çš„è¿åŠ¨åŠ¨ä½œã€‚æ•´ä¸ªç³»ç»Ÿé€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†å¤§è§„æ¨¡ã€å¤šæ¨¡æ€çš„RoboMIND 2.0æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿç¯å¢ƒçš„æ•°æ®ï¼Œå¹¶æ¶µç›–å¤šç§æœºå™¨äººå½¢æ€å’Œå¤æ‚ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæå‡ºçš„MIND-2ç³»ç»Ÿé‡‡ç”¨åˆ†å±‚åŒç³»ç»Ÿæ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†é«˜çº§è¯­ä¹‰æŒ‡ä»¤è½¬åŒ–ä¸ºä½çº§è¿åŠ¨åŠ¨ä½œã€‚

**å…³é”®è®¾è®¡**ï¼šMIND-2-VLMå¯èƒ½é‡‡ç”¨äº†Transformerç­‰æ¨¡å‹ï¼Œç”¨äºç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¹¶ç”Ÿæˆå­ç›®æ ‡åºåˆ—ã€‚MIND-2-VLAå¯èƒ½é‡‡ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå°†è§†è§‰ä¿¡æ¯ã€è¯­è¨€æŒ‡ä»¤å’Œè‡ªèº«çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆè¿åŠ¨åŠ¨ä½œã€‚ç¦»çº¿å¼ºåŒ–å­¦ä¹ å¯èƒ½é‡‡ç”¨äº†DDPGã€SACç­‰ç®—æ³•ï¼Œç”¨äºä¼˜åŒ–MIND-2-VLAçš„æ§åˆ¶ç­–ç•¥ã€‚å…·ä½“å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RoboMIND 2.0æ•°æ®é›†åŒ…å«è¶…è¿‡31ä¸‡æ¡åŒè‡‚æ“ä½œè½¨è¿¹ï¼Œæ¶µç›–å…­ç§ä¸åŒçš„æœºå™¨äººå½¢æ€å’Œ739ä¸ªå¤æ‚ä»»åŠ¡ã€‚è¯¥æ•°æ®é›†è¿˜åŒ…å«äº†1.2ä¸‡ä¸ªè§¦è§‰å¢å¼ºçš„ç‰‡æ®µå’Œ2ä¸‡ä¸ªç§»åŠ¨æ“ä½œè½¨è¿¹ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æ„å»ºäº†çœŸå®ä¸–ç•Œç¯å¢ƒçš„é«˜ä¿çœŸæ•°å­—å­ªç”Ÿï¼Œå¹¶å‘å¸ƒäº†é¢å¤–çš„2ä¸‡æ¡è½¨è¿¹çš„æ¨¡æ‹Ÿæ•°æ®é›†ã€‚MIND-2ç³»ç»Ÿçš„å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒå…¶é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ è¿›è¡Œäº†ä¼˜åŒ–ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡ã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©ç­‰ã€‚é€šè¿‡åˆ©ç”¨RoboMIND 2.0æ•°æ®é›†å’ŒMIND-2ç³»ç»Ÿï¼Œå¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººåº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤šæœºå™¨äººå½¢æ€å’Œä»»åŠ¡ç±»å‹ï¼Œå¹¶æ¢ç´¢æ›´æœ‰æ•ˆçš„æ§åˆ¶ç­–ç•¥å’Œå­¦ä¹ ç®—æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited. To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12K tactile-enhanced episodes and 20K mobile manipulation trajectories. Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer. To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system frame-work optimized via offline reinforcement learning. MIND-2 integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions.

