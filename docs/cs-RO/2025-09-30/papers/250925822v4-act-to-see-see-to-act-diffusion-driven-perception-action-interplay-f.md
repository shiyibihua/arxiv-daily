---
layout: default
title: Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies
---

# Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25822" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25822v4</a>
  <a href="https://arxiv.org/pdf/2509.25822.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25822v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25822v4', 'Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jing Wang, Weiting Peng, Jing Tang, Zeyu Gong, Xihua Wang, Bo Tao, Li Cheng

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-11-11)

**å¤‡æ³¨**: 39th Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAction-Guided Diffusion Policyï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹é©±åŠ¨çš„æ„ŸçŸ¥-åŠ¨ä½œäº¤äº’å®ç°è‡ªé€‚åº”ç­–ç•¥ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `æ‰©æ•£æ¨¡å‹` `æ„ŸçŸ¥-åŠ¨ä½œäº¤äº’` `è‡ªé€‚åº”ç­–ç•¥` `æœºå™¨äººæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åˆ†ç¦»æ„ŸçŸ¥å’ŒåŠ¨ä½œï¼Œå¿½ç•¥äº†ä¸¤è€…ä¹‹é—´çš„å› æœäº’æƒ å…³ç³»ï¼Œé™åˆ¶äº†ç­–ç•¥çš„è‡ªé€‚åº”æ€§ã€‚
2. DP-AGé€šè¿‡åŠ¨ä½œå¼•å¯¼çš„æ‰©æ•£è¿‡ç¨‹ï¼Œæ˜¾å¼å»ºæ¨¡æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„åŠ¨æ€äº¤äº’ï¼Œå®ç°ç»Ÿä¸€çš„è¡¨å¾å­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDP-AGåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†ç­–ç•¥çš„è‡ªé€‚åº”èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAction-Guided Diffusion Policy (DP-AG) çš„ç»Ÿä¸€è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æ˜¾å¼åœ°å»ºæ¨¡æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„åŠ¨æ€äº¤äº’ï¼Œè¿™ç§äº¤äº’æ˜¯äººç±»å®ç°è‡ªé€‚åº”è¡Œä¸ºçš„å…³é”®ã€‚DP-AGé€šè¿‡å˜åˆ†æ¨æ–­å°†æ½œåœ¨è§‚æµ‹ç¼–ç ä¸ºé«˜æ–¯åéªŒï¼Œå¹¶ä½¿ç”¨åŠ¨ä½œå¼•å¯¼çš„éšæœºå¾®åˆ†æ–¹ç¨‹(SDE)æ¼”åŒ–è¿™äº›æ½œåœ¨å˜é‡ã€‚æ‰©æ•£ç­–ç•¥å™ªå£°é¢„æµ‹çš„Vector-Jacobian Product (VJP)ä½œä¸ºä¸€ç§ç»“æ„åŒ–çš„éšæœºåŠ›ï¼Œé©±åŠ¨æ½œåœ¨å˜é‡çš„æ›´æ–°ã€‚ä¸ºäº†ä¿ƒè¿›æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„åŒå‘å­¦ä¹ ï¼Œå¼•å…¥äº†å¾ªç¯ä¸€è‡´çš„å¯¹æ¯”æŸå¤±ï¼Œå°†å™ªå£°é¢„æµ‹å™¨çš„æ¢¯åº¦æµç»„ç»‡æˆä¸€ä¸ªè¿è´¯çš„æ„ŸçŸ¥-åŠ¨ä½œå¾ªç¯ï¼Œä»è€Œåœ¨æ½œåœ¨å˜é‡æ›´æ–°å’ŒåŠ¨ä½œä¼˜åŒ–ä¸­å¼ºåˆ¶æ‰§è¡Œç›¸äº’ä¸€è‡´çš„è½¬æ¢ã€‚ç†è®ºä¸Šï¼Œæ¨å¯¼äº†åŠ¨ä½œå¼•å¯¼SDEçš„å˜åˆ†ä¸‹ç•Œï¼Œå¹¶è¯æ˜äº†å¯¹æ¯”ç›®æ ‡å¢å¼ºäº†æ½œåœ¨å˜é‡å’ŒåŠ¨ä½œè½¨è¿¹çš„è¿ç»­æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDP-AGåœ¨æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œçš„UR5æ“ä½œä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚DP-AGä¸ºå¼¥åˆç”Ÿç‰©é€‚åº”æ€§å’Œäººå·¥æ™ºèƒ½ç­–ç•¥å­¦ä¹ ä¹‹é—´çš„å·®è·æä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•é€šå¸¸å°†æ„ŸçŸ¥å’ŒåŠ¨ä½œè§£è€¦ï¼Œå¿½ç•¥äº†æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„ç›¸äº’å½±å“ã€‚è¿™ç§è§£è€¦å¯¼è‡´ç­–ç•¥éš¾ä»¥é€‚åº”ç¯å¢ƒå˜åŒ–ï¼Œç¼ºä¹ç”Ÿç‰©æ™ºèƒ½çš„è‡ªé€‚åº”èƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæ˜¾å¼å»ºæ¨¡æ„ŸçŸ¥å’ŒåŠ¨ä½œäº¤äº’çš„ç­–ç•¥å­¦ä¹ æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDP-AGçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ‰©æ•£æ¨¡å‹æ¥å»ºæ¨¡æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„åŠ¨æ€äº¤äº’ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨åŠ¨ä½œå¼•å¯¼çš„éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰æ¥æ¼”åŒ–æ½œåœ¨è§‚æµ‹ï¼Œå¹¶å°†æ‰©æ•£ç­–ç•¥çš„å™ªå£°é¢„æµ‹ä½œä¸ºé©±åŠ¨æ½œåœ¨å˜é‡æ›´æ–°çš„ç»“æ„åŒ–éšæœºåŠ›ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ„ŸçŸ¥å¯ä»¥å½±å“åŠ¨ä½œï¼Œåä¹‹äº¦ç„¶ï¼Œä»è€Œå®ç°åŒå‘çš„äº¤äº’å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDP-AGçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç¼–ç å™¨ï¼šå°†è§‚æµ‹ç¼–ç ä¸ºæ½œåœ¨å˜é‡çš„é«˜æ–¯åéªŒåˆ†å¸ƒã€‚2) åŠ¨ä½œå¼•å¯¼çš„SDEï¼šä½¿ç”¨åŠ¨ä½œä½œä¸ºæ¡ä»¶ï¼Œé€šè¿‡SDEæ¼”åŒ–æ½œåœ¨å˜é‡ã€‚3) æ‰©æ•£ç­–ç•¥ï¼šé¢„æµ‹SDEä¸­çš„å™ªå£°ï¼Œå¹¶åˆ©ç”¨å…¶Vector-Jacobian Product (VJP)ä½œä¸ºé©±åŠ¨æ½œåœ¨å˜é‡æ›´æ–°çš„åŠ›ã€‚4) å¾ªç¯ä¸€è‡´å¯¹æ¯”æŸå¤±ï¼šä¿ƒè¿›æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„åŒå‘å­¦ä¹ ï¼Œç¡®ä¿æ½œåœ¨å˜é‡æ›´æ–°å’ŒåŠ¨ä½œä¼˜åŒ–çš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šDP-AGæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå®ƒæ˜¾å¼åœ°å»ºæ¨¡äº†æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„åŠ¨æ€äº¤äº’ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒDP-AGä¸æ˜¯ç®€å•åœ°å°†æ„ŸçŸ¥ä½œä¸ºåŠ¨ä½œçš„è¾“å…¥ï¼Œè€Œæ˜¯é€šè¿‡æ‰©æ•£æ¨¡å‹å»ºç«‹äº†ä¸€ä¸ªåŒå‘çš„åé¦ˆå›è·¯ï¼Œä½¿å¾—æ„ŸçŸ¥å’ŒåŠ¨ä½œå¯ä»¥ç›¸äº’å½±å“ã€ç›¸äº’ä¿ƒè¿›ã€‚æ­¤å¤–ï¼Œå¾ªç¯ä¸€è‡´å¯¹æ¯”æŸå¤±çš„å¼•å…¥è¿›ä¸€æ­¥å¢å¼ºäº†æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šDP-AGçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åŠ¨ä½œå¼•å¯¼çš„SDEï¼šSDEçš„æ¼‚ç§»é¡¹å’Œæ‰©æ•£é¡¹éƒ½ä¾èµ–äºåŠ¨ä½œï¼Œä»è€Œå®ç°åŠ¨ä½œå¯¹æ½œåœ¨å˜é‡æ¼”åŒ–çš„å¼•å¯¼ã€‚2) æ‰©æ•£ç­–ç•¥çš„VJPï¼šVJPæä¾›äº†å…³äºç­–ç•¥å¯¹æ½œåœ¨å˜é‡çš„æ•æ„Ÿæ€§ä¿¡æ¯ï¼Œå¯ä»¥ä½œä¸ºä¸€ç§ç»“æ„åŒ–çš„éšæœºåŠ›æ¥é©±åŠ¨æ½œåœ¨å˜é‡çš„æ›´æ–°ã€‚3) å¾ªç¯ä¸€è‡´å¯¹æ¯”æŸå¤±ï¼šè¯¥æŸå¤±é¼“åŠ±åœ¨æ½œåœ¨ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ä¸­è¿›è¡Œä¸€è‡´çš„è½¬æ¢ï¼Œä»è€Œä¿ƒè¿›æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„åŒå‘å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DP-AGåœ¨æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œçš„UR5æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸæ¨¡æ‹Ÿä»»åŠ¡ä¸­ï¼ŒDP-AGçš„æˆåŠŸç‡æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†15%ã€‚åœ¨çœŸå®æœºå™¨äººä»»åŠ¡ä¸­ï¼ŒDP-AGä¹Ÿè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œèƒ½å¤ŸæˆåŠŸå®Œæˆå¤æ‚çš„æ“ä½œä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è‡ªé€‚åº”ç­–ç•¥çš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å¤æ‚ç¯å¢ƒä¸‹çš„ç‰©ä½“æ“ä½œã€è‡ªä¸»å¯¼èˆªå’Œäººæœºåä½œã€‚é€šè¿‡æ˜¾å¼å»ºæ¨¡æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„äº¤äº’ï¼Œå¯ä»¥æé«˜æœºå™¨äººçš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹çœŸå®ä¸–ç•Œä¸­çš„ä¸ç¡®å®šæ€§å’Œå˜åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æœ‰æ½œåŠ›åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆAIç­‰é¢†åŸŸï¼Œæå‡æ™ºèƒ½ä½“çš„äº¤äº’èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing imitation learning methods decouple perception and action, which overlooks the causal reciprocity between sensory representations and action execution that humans naturally leverage for adaptive behaviors. To bridge this gap, we introduce Action-Guided Diffusion Policy (DP-AG), a unified representation learning that explicitly models a dynamic interplay between perception and action through probabilistic latent dynamics. DP-AG encodes latent observations into a Gaussian posterior via variational inference and evolves them using an action-guided SDE, where the Vector-Jacobian Product (VJP) of the diffusion policy's noise predictions serves as a structured stochastic force driving latent updates. To promote bidirectional learning between perception and action, we introduce a cycle-consistent contrastive loss that organizes the gradient flow of the noise predictor into a coherent perception-action loop, enforcing mutually consistent transitions in both latent updates and action refinements. Theoretically, we derive a variational lower bound for the action-guided SDE, and prove that the contrastive objective enhances continuity in both latent and action trajectories. Empirically, DP-AG significantly outperforms state-of-the-art methods across simulation benchmarks and real-world UR5 manipulation tasks. As a result, our DP-AG offers a promising step toward bridging biological adaptability and artificial policy learning.

