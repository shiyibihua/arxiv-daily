---
layout: default
title: SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling
---

# SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25756" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25756v2</a>
  <a href="https://arxiv.org/pdf/2509.25756.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25756v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25756v2', 'SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yixian Zhang, Shu'ang Yu, Tonghe Zhang, Mo Guang, Haojia Hui, Kaiwen Long, Yu Wang, Chao Yu, Wenbo Ding

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAC Flowç®—æ³•ï¼Œé€šè¿‡é€Ÿåº¦é‡å‚æ•°åŒ–åºåˆ—å»ºæ¨¡å®ç°Flow-Basedç­–ç•¥é«˜æ•ˆå¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æµæ¨¡å‹` `è¿ç»­æ§åˆ¶` `æœºå™¨äººæ“ä½œ` `åºåˆ—å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŸºäºæµçš„ç­–ç•¥åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­è®­ç»ƒå›°éš¾ï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºå¤šæ­¥åŠ¨ä½œé‡‡æ ·å¯¼è‡´çš„æ¢¯åº¦ä¸ç¨³å®šé—®é¢˜ã€‚
2. è®ºæ–‡é€šè¿‡é€Ÿåº¦é‡å‚æ•°åŒ–ï¼Œå°†æµå±•å¼€ç­‰ä»·äºæ®‹å·®å¾ªç¯è®¡ç®—ï¼Œå¹¶å€Ÿé‰´ç°ä»£åºåˆ—æ¨¡å‹è®¾è®¡äº†Flow-Gå’ŒFlow-Tä¸¤ç§ç¨³å®šæ¶æ„ã€‚
3. æå‡ºçš„SAC Flowç®—æ³•åœ¨è¿ç»­æ§åˆ¶å’Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œæ— éœ€é¢å¤–çš„ç­–ç•¥è’¸é¦ç­‰æŠ€å·§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨off-policyå¼ºåŒ–å­¦ä¹ è®­ç»ƒåŸºäºæµçš„ç­–ç•¥æ—¶ï¼Œç”±äºå¤šæ­¥åŠ¨ä½œé‡‡æ ·è¿‡ç¨‹ä¸­çš„æ¢¯åº¦é—®é¢˜è€Œå¯¼è‡´çš„ä¸ç¨³å®šæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæµçš„å±•å¼€åœ¨ä»£æ•°ä¸Šç­‰ä»·äºæ®‹å·®å¾ªç¯è®¡ç®—ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°ä¸RNNç›¸åŒçš„æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡åˆ©ç”¨ç°ä»£åºåˆ—æ¨¡å‹çš„åŸç†å¯¹é€Ÿåº¦ç½‘ç»œè¿›è¡Œé‡å‚æ•°åŒ–ï¼Œå¼•å…¥äº†ä¸¤ç§ç¨³å®šçš„æ¶æ„ï¼šFlow-Gï¼ˆç»“åˆé—¨æ§é€Ÿåº¦ï¼‰å’ŒFlow-Tï¼ˆåˆ©ç”¨è§£ç é€Ÿåº¦ï¼‰ã€‚ç„¶åï¼Œå¼€å‘äº†ä¸€ç§åŸºäºSACçš„å®ç”¨ç®—æ³•ï¼Œé€šè¿‡å™ªå£°å¢å¼ºçš„å±•å¼€ï¼Œä¿ƒè¿›è¿™äº›ç­–ç•¥çš„ç›´æ¥ç«¯åˆ°ç«¯è®­ç»ƒã€‚è¯¥æ–¹æ³•æ”¯æŒä»å¤´å¼€å§‹å’Œç¦»çº¿åˆ°åœ¨çº¿çš„å­¦ä¹ ï¼Œå¹¶åœ¨è¿ç»­æ§åˆ¶å’Œæœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ— éœ€ç­–ç•¥è’¸é¦æˆ–æ›¿ä»£ç›®æ ‡ç­‰å¸¸è§æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºæµçš„ç­–ç•¥åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­è®­ç»ƒæ—¶ï¼Œç”±äºå¤šæ­¥åŠ¨ä½œé‡‡æ ·è¿‡ç¨‹ä¸­çš„æ¢¯åº¦é—®é¢˜ï¼Œè®­ç»ƒè¿‡ç¨‹éå¸¸ä¸ç¨³å®šã€‚è¿™ç§ä¸ç¨³å®šæ€§æºäºæµçš„å±•å¼€åœ¨ä»£æ•°ä¸Šç­‰ä»·äºæ®‹å·®å¾ªç¯è®¡ç®—ï¼Œå› æ­¤å®¹æ˜“å—åˆ°ä¸RNNç›¸åŒçš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„å½±å“ã€‚ç°æœ‰çš„è§£å†³æ–¹æ³•é€šå¸¸éœ€è¦ç­–ç•¥è’¸é¦æˆ–æ›¿ä»£ç›®æ ‡ç­‰æŠ€å·§ï¼Œå¢åŠ äº†è®­ç»ƒçš„å¤æ‚æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹é€Ÿåº¦ç½‘ç»œè¿›è¡Œé‡å‚æ•°åŒ–ï¼Œå€Ÿé‰´ç°ä»£åºåˆ—æ¨¡å‹çš„æ€æƒ³ï¼Œè®¾è®¡å‡ºæ›´ç¨³å®šçš„ç½‘ç»œæ¶æ„ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡å°†æµçš„å±•å¼€è§†ä¸ºä¸€ä¸ªæ®‹å·®å¾ªç¯è®¡ç®—è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨é—¨æ§æœºåˆ¶å’Œè§£ç é€Ÿåº¦ç­‰æŠ€æœ¯æ¥ç¼“è§£æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å®ç°å¯¹åŸºäºæµçš„ç­–ç•¥çš„ç›´æ¥ç«¯åˆ°ç«¯è®­ç»ƒï¼Œé¿å…äº†é¢å¤–çš„ç­–ç•¥è’¸é¦æˆ–æ›¿ä»£ç›®æ ‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŸºäºSoft Actor-Critic (SAC) ç®—æ³•ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ã€‚ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) é€Ÿåº¦ç½‘ç»œï¼šä½¿ç”¨é‡å‚æ•°åŒ–çš„é€Ÿåº¦ç½‘ç»œï¼ˆFlow-Gæˆ–Flow-Tï¼‰æ¥ç”ŸæˆåŠ¨ä½œï¼›2) å™ªå£°å¢å¼ºçš„å±•å¼€ï¼šåœ¨æµçš„å±•å¼€è¿‡ç¨‹ä¸­å¼•å…¥å™ªå£°ï¼Œä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ï¼›3) SACæŸå¤±å‡½æ•°ï¼šä½¿ç”¨æ ‡å‡†çš„SACæŸå¤±å‡½æ•°æ¥è®­ç»ƒç­–ç•¥å’Œä»·å€¼å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¯¹é€Ÿåº¦ç½‘ç»œçš„é‡å‚æ•°åŒ–ï¼Œä»¥åŠç”±æ­¤äº§ç”Ÿçš„Flow-Gå’ŒFlow-Tä¸¤ç§ç¨³å®šæ¶æ„ã€‚è¿™ç§é‡å‚æ•°åŒ–å€Ÿé‰´äº†ç°ä»£åºåˆ—æ¨¡å‹çš„æ€æƒ³ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œä½¿å¾—å¯ä»¥ç›´æ¥ç«¯åˆ°ç«¯åœ°è®­ç»ƒåŸºäºæµçš„ç­–ç•¥ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€ç­–ç•¥è’¸é¦æˆ–æ›¿ä»£ç›®æ ‡ï¼Œç®€åŒ–äº†è®­ç»ƒæµç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šFlow-Gæ¶æ„ä½¿ç”¨é—¨æ§æœºåˆ¶æ¥æ§åˆ¶é€Ÿåº¦çš„æ›´æ–°ï¼Œç±»ä¼¼äºGRUæˆ–LSTMä¸­çš„é—¨æ§å•å…ƒã€‚Flow-Tæ¶æ„åˆ™ä½¿ç”¨ä¸€ä¸ªè§£ç å™¨ç½‘ç»œæ¥è§£ç é€Ÿåº¦ï¼Œä»è€Œé¿å…äº†ç›´æ¥çš„å¾ªç¯ä¾èµ–ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡ä½¿ç”¨äº†å™ªå£°å¢å¼ºçš„å±•å¼€ï¼Œå³åœ¨æµçš„å±•å¼€è¿‡ç¨‹ä¸­å¼•å…¥éšæœºå™ªå£°ï¼Œä»¥æé«˜è®­ç»ƒçš„é²æ£’æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ä¸æ ‡å‡†çš„SACç®—æ³•ç±»ä¼¼ï¼Œä½†é’ˆå¯¹Flow-Gå’ŒFlow-Tæ¶æ„è¿›è¡Œäº†å¾®è°ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„SAC Flowç®—æ³•åœ¨å¤šä¸ªè¿ç»­æ§åˆ¶å’Œæœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒSAC Flowçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰çš„SOTAç®—æ³•ï¼Œå¹¶ä¸”æ— éœ€ç­–ç•¥è’¸é¦æˆ–æ›¿ä»£ç›®æ ‡ç­‰æŠ€å·§ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†Flow-Gå’ŒFlow-Tä¸¤ç§æ¶æ„çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå™ªå£°å¢å¼ºå±•å¼€å¯¹è®­ç»ƒç¨³å®šæ€§çš„æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚ç­–ç•¥çš„è¿ç»­æ§åˆ¶å’Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æœºå™¨äººæŠ“å–ã€å¯¼èˆªã€è£…é…ç­‰ã€‚é€šè¿‡æé«˜ç­–ç•¥è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ï¼Œå¯ä»¥é™ä½æœºå™¨äººå¼€å‘çš„æˆæœ¬å’Œæ—¶é—´ï¼Œå¹¶æé«˜æœºå™¨äººçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦åºåˆ—å»ºæ¨¡çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training expressive flow-based policies with off-policy reinforcement learning is notoriously unstable due to gradient pathologies in the multi-step action sampling process. We trace this instability to a fundamental connection: the flow rollout is algebraically equivalent to a residual recurrent computation, making it susceptible to the same vanishing and exploding gradients as RNNs. To address this, we reparameterize the velocity network using principles from modern sequential models, introducing two stable architectures: Flow-G, which incorporates a gated velocity, and Flow-T, which utilizes a decoded velocity. We then develop a practical SAC-based algorithm, enabled by a noise-augmented rollout, that facilitates direct end-to-end training of these policies. Our approach supports both from-scratch and offline-to-online learning and achieves state-of-the-art performance on continuous control and robotic manipulation benchmarks, eliminating the need for common workarounds like policy distillation or surrogate objectives.

