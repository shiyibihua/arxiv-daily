---
layout: default
title: Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning
---

# Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00329" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00329v1</a>
  <a href="https://arxiv.org/pdf/2510.00329.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00329v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00329v1', 'Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sarmad Mehrdad, Maxime Sabbah, Vincent Bonnet, Ludovic Righetti

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 8 pages, 4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæœ€å°è§‚æµ‹é€†å¼ºåŒ–å­¦ä¹ çš„äººä½“æ‰‹è‡‚è¿åŠ¨æœ€ä¼˜æ€§å»ºæ¨¡æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é€†å¼ºåŒ–å­¦ä¹ ` `è¿åŠ¨å»ºæ¨¡` `äººä½“è¿åŠ¨` `æœ€å°è§‚æµ‹` `è¿åŠ¨æ§åˆ¶` `äººå½¢æœºå™¨äºº` `ç”Ÿç‰©åŠ›å­¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿé€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å»ºæ¨¡äººç±»è¿åŠ¨æ—¶éœ€è¦å¤§é‡æ¼”ç¤ºæ•°æ®ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ…¢ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºMO-IRLæ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ä»£ä»·æƒé‡ï¼Œæ˜¾è‘—å‡å°‘äº†æ‰€éœ€æ¼”ç¤ºæ•°æ®é‡ï¼Œå¹¶åŠ é€Ÿäº†æ”¶æ•›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé¢„æµ‹äººä½“æ‰‹è‡‚è¿åŠ¨è½¨è¿¹ï¼Œå¹¶æ­ç¤ºè¿åŠ¨æ§åˆ¶ä¸­çš„åŠ¨æ€ä»£ä»·ç»“æ„ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æœ€å°è§‚æµ‹é€†å¼ºåŒ–å­¦ä¹ ï¼ˆMO-IRLï¼‰åœ¨å»ºæ¨¡å’Œé¢„æµ‹å…·æœ‰æ—¶å˜ä»£ä»·æƒé‡çš„äººä½“æ‰‹è‡‚åˆ°è¾¾è¿åŠ¨ä¸­çš„åº”ç”¨ã€‚ä½¿ç”¨å¹³é¢äºŒè¿æ†ç”Ÿç‰©åŠ›å­¦æ¨¡å‹å’Œé«˜åˆ†è¾¨ç‡è¿åŠ¨æ•æ‰æ•°æ®ï¼Œé’ˆå¯¹å—è¯•è€…æ‰§è¡Œçš„æŒ‡å‘ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªè½¨è¿¹åˆ†å‰²æˆå¤šä¸ªé˜¶æ®µï¼Œå¹¶å­¦ä¹ ç‰¹å®šé˜¶æ®µçš„ä¸ƒä¸ªå€™é€‰ä»£ä»·å‡½æ•°çš„ç»„åˆã€‚MO-IRLé€šè¿‡åœ¨æœ€å¤§ç†µé€†å¼ºåŒ–å­¦ä¹ å…¬å¼ä¸­ç¼©æ”¾è§‚å¯Ÿåˆ°çš„å’Œç”Ÿæˆçš„è½¨è¿¹æ¥è¿­ä»£åœ°ç»†åŒ–ä»£ä»·æƒé‡ï¼Œä¸ç»å…¸çš„é€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œå¤§å¤§å‡å°‘äº†æ‰€éœ€çš„æ¼”ç¤ºæ¬¡æ•°å’Œæ”¶æ•›æ—¶é—´ã€‚åœ¨æ¯ä¸ªå§¿åŠ¿ä¸Šè®­ç»ƒåæ¬¡è¯•éªŒï¼Œå¯¹äºå…­æ®µå’Œå…«æ®µæƒé‡åˆ’åˆ†ï¼Œå¹³å‡å…³èŠ‚è§’åº¦å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰åˆ†åˆ«ä¸º6.4åº¦å’Œ5.6åº¦ï¼Œè€Œä½¿ç”¨å•ä¸ªé™æ€æƒé‡æ—¶ä¸º10.4åº¦ã€‚å¯¹å‰©ä½™è¯•éªŒçš„äº¤å‰éªŒè¯ï¼Œä»¥åŠé¦–æ¬¡å¯¹æœªè§å—è¯•è€…çš„20æ¬¡è¯•éªŒè¿›è¡Œå—è¯•è€…é—´éªŒè¯ï¼Œéƒ½æ˜¾ç¤ºå‡ºç›¸å½“çš„é¢„æµ‹ç²¾åº¦ï¼Œçº¦ä¸º8åº¦RMSEï¼Œè¡¨æ˜äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚å­¦ä¹ åˆ°çš„æƒé‡å¼ºè°ƒè¿åŠ¨å¼€å§‹å’Œç»“æŸæœŸé—´çš„å…³èŠ‚åŠ é€Ÿåº¦æœ€å°åŒ–ï¼Œè¿™ä¸ç”Ÿç‰©è¿åŠ¨ä¸­è§‚å¯Ÿåˆ°çš„å¹³æ»‘æ€§åŸåˆ™ä¸€è‡´ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMO-IRLå¯ä»¥æœ‰æ•ˆåœ°æ­ç¤ºäººç±»è¿åŠ¨æ§åˆ¶ä¸­åŠ¨æ€çš„ã€ä¸å—è¯•è€…æ— å…³çš„ä»£ä»·ç»“æ„ï¼Œå¹¶å…·æœ‰åœ¨äººå½¢æœºå™¨äººä¸­çš„æ½œåœ¨åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•é«˜æ•ˆåœ°ä»å°‘é‡äººä½“è¿åŠ¨æ•°æ®ä¸­å­¦ä¹ åˆ°èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹è¿åŠ¨è½¨è¿¹ï¼Œå¹¶æ­ç¤ºæ½œåœ¨è¿åŠ¨æ§åˆ¶æœºåˆ¶çš„é—®é¢˜ã€‚ç°æœ‰é€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡çš„æ¼”ç¤ºæ•°æ®ï¼Œè®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥åº”ç”¨äºå¤æ‚çš„äººä½“è¿åŠ¨å»ºæ¨¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æœ€å°è§‚æµ‹é€†å¼ºåŒ–å­¦ä¹ ï¼ˆMO-IRLï¼‰ï¼Œé€šè¿‡è¿­ä»£åœ°ä¼˜åŒ–ä»£ä»·æƒé‡ï¼Œä½¿å¾—æ¨¡å‹ç”Ÿæˆçš„è½¨è¿¹ä¸è§‚å¯Ÿåˆ°çš„è½¨è¿¹å°½å¯èƒ½ä¸€è‡´ã€‚MO-IRLé€šè¿‡ç¼©æ”¾è§‚å¯Ÿåˆ°çš„å’Œç”Ÿæˆçš„è½¨è¿¹ï¼Œåœ¨æœ€å¤§ç†µé€†å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸‹è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œå‡å°‘äº†å¯¹å¤§é‡æ•°æ®çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) æ•°æ®é‡‡é›†ï¼šä½¿ç”¨è¿åŠ¨æ•æ‰ç³»ç»Ÿè®°å½•äººä½“æ‰‹è‡‚è¿åŠ¨è½¨è¿¹ï¼›2) è½¨è¿¹åˆ†å‰²ï¼šå°†è½¨è¿¹åˆ†å‰²æˆå¤šä¸ªé˜¶æ®µï¼›3) ç‰¹å¾æå–ï¼šæå–æ¯ä¸ªé˜¶æ®µçš„è¿åŠ¨å­¦ç‰¹å¾ï¼›4) MO-IRLä¼˜åŒ–ï¼šä½¿ç”¨MO-IRLç®—æ³•è¿­ä»£ä¼˜åŒ–æ¯ä¸ªé˜¶æ®µçš„ä»£ä»·æƒé‡ï¼›5) è½¨è¿¹é¢„æµ‹ï¼šä½¿ç”¨å­¦ä¹ åˆ°çš„ä»£ä»·æƒé‡é¢„æµ‹æ–°çš„è¿åŠ¨è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºMO-IRLç®—æ³•çš„åº”ç”¨ï¼Œå®ƒé€šè¿‡è¿­ä»£åœ°ç¼©æ”¾è½¨è¿¹ï¼Œå‡å°‘äº†å¯¹å¤§é‡æ•°æ®çš„éœ€æ±‚ï¼Œå¹¶åŠ é€Ÿäº†æ”¶æ•›ã€‚æ­¤å¤–ï¼Œå°†è½¨è¿¹åˆ†å‰²æˆå¤šä¸ªé˜¶æ®µï¼Œå¹¶ä¸ºæ¯ä¸ªé˜¶æ®µå­¦ä¹ ä¸åŒçš„ä»£ä»·æƒé‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¿åŠ¨è¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šä»£ä»·å‡½æ•°ç”±ä¸ƒä¸ªå€™é€‰ä»£ä»·å‡½æ•°ç»„æˆï¼ŒåŒ…æ‹¬å…³èŠ‚è§’åº¦ã€å…³èŠ‚é€Ÿåº¦å’Œå…³èŠ‚åŠ é€Ÿåº¦çš„æœ€å°åŒ–ã€‚ä½¿ç”¨æœ€å¤§ç†µé€†å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–ä»£ä»·æƒé‡æ¥æœ€å¤§åŒ–è§‚å¯Ÿåˆ°çš„è½¨è¿¹çš„æ¦‚ç‡ã€‚é‡‡ç”¨Root Mean Squared Error (RMSE) ä½œä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œè¡¡é‡é¢„æµ‹è½¨è¿¹ä¸çœŸå®è½¨è¿¹ä¹‹é—´çš„å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MO-IRLæ–¹æ³•ï¼Œåœ¨æ¯ä¸ªå§¿åŠ¿ä¸Šè®­ç»ƒåæ¬¡è¯•éªŒï¼Œå¯¹äºå…­æ®µå’Œå…«æ®µæƒé‡åˆ’åˆ†ï¼Œå¹³å‡å…³èŠ‚è§’åº¦å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰åˆ†åˆ«ä¸º6.4åº¦å’Œ5.6åº¦ï¼Œè€Œä½¿ç”¨å•ä¸ªé™æ€æƒé‡æ—¶ä¸º10.4åº¦ã€‚å¯¹æœªè§å—è¯•è€…çš„20æ¬¡è¯•éªŒè¿›è¡Œå—è¯•è€…é—´éªŒè¯ï¼Œé¢„æµ‹ç²¾åº¦çº¦ä¸º8åº¦RMSEï¼Œè¡¨æ˜äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºäººå½¢æœºå™¨äººè¿åŠ¨æ§åˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨¡ä»¿äººç±»çš„è‡ªç„¶è¿åŠ¨æ–¹å¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ç”¨äºåº·å¤è®­ç»ƒï¼Œé€šè¿‡åˆ†ææ‚£è€…çš„è¿åŠ¨è½¨è¿¹ï¼Œè¯„ä¼°å…¶è¿åŠ¨èƒ½åŠ›ï¼Œå¹¶åˆ¶å®šä¸ªæ€§åŒ–çš„åº·å¤è®¡åˆ’ã€‚è¯¥ç ”ç©¶å¯¹äºç†è§£äººç±»è¿åŠ¨æ§åˆ¶æœºåˆ¶ï¼Œä»¥åŠå¼€å‘æ›´æ™ºèƒ½ã€æ›´äººæ€§åŒ–çš„æœºå™¨äººç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper investigates the application of Minimal Observation Inverse Reinforcement Learning (MO-IRL) to model and predict human arm-reaching movements with time-varying cost weights. Using a planar two-link biomechanical model and high-resolution motion-capture data from subjects performing a pointing task, we segment each trajectory into multiple phases and learn phase-specific combinations of seven candidate cost functions. MO-IRL iteratively refines cost weights by scaling observed and generated trajectories in the maximum entropy IRL formulation, greatly reducing the number of required demonstrations and convergence time compared to classical IRL approaches. Training on ten trials per posture yields average joint-angle Root Mean Squared Errors (RMSE) of 6.4 deg and 5.6 deg for six- and eight-segment weight divisions, respectively, versus 10.4 deg using a single static weight. Cross-validation on remaining trials and, for the first time, inter-subject validation on an unseen subject's 20 trials, demonstrates comparable predictive accuracy, around 8 deg RMSE, indicating robust generalization. Learned weights emphasize joint acceleration minimization during movement onset and termination, aligning with smoothness principles observed in biological motion. These results suggest that MO-IRL can efficiently uncover dynamic, subject-independent cost structures underlying human motor control, with potential applications for humanoid robots.

