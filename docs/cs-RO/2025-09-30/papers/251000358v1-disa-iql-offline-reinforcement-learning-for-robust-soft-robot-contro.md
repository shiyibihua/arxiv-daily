---
layout: default
title: DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts
---

# DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00358" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00358v1</a>
  <a href="https://arxiv.org/pdf/2510.00358.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00358v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00358v1', 'DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Linjin He, Xinda Qi, Dong Chen, Zhaojian Li, Xiaobo Tan

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DiSA-IQLï¼šé¢å‘åˆ†å¸ƒåç§»ä¸‹æŸ”æ€§æœºå™¨äººé²æ£’æ§åˆ¶çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æŸ”æ€§æœºå™¨äººæ§åˆ¶` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `åˆ†å¸ƒåç§»` `é²æ£’æ€§` `éšå¼Qå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æŸ”æ€§æœºå™¨äººæ§åˆ¶é¢ä¸´éçº¿æ€§åŠ¨åŠ›å­¦æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–ç®€åŒ–å‡è®¾ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚
2. DiSA-IQLé€šè¿‡æƒ©ç½šä¸å¯é çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œå‡è½»åˆ†å¸ƒåç§»çš„å½±å“ï¼Œä»è€Œæå‡æŸ”æ€§æœºå™¨äººçš„æ§åˆ¶é²æ£’æ€§ã€‚
3. ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒDiSA-IQLåœ¨ç›®æ ‡åˆ°è¾¾ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºBCã€CQLå’ŒIQLç­‰åŸºçº¿æ¨¡å‹ï¼ŒæˆåŠŸç‡æ›´é«˜ï¼Œè½¨è¿¹æ›´å¹³æ»‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æŸ”æ€§è›‡å½¢æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­å±•ç°å‡ºå“è¶Šçš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œä½†å…¶æ§åˆ¶å› é«˜åº¦éçº¿æ€§åŠ¨åŠ›å­¦è€Œå……æ»¡æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºæ¨¡å‹å’Œä»¿ç”Ÿæ§åˆ¶å™¨ä¾èµ–äºç®€åŒ–çš„å‡è®¾ï¼Œé™åˆ¶äº†æ€§èƒ½ã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æœ€è¿‘æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç”±äºä»£ä»·é«˜æ˜‚ä¸”å¯èƒ½é€ æˆæŸå®³çš„çœŸå®ä¸–ç•Œäº¤äº’ï¼Œåœ¨çº¿è®­ç»ƒé€šå¸¸ä¸åˆ‡å®é™…ã€‚ç¦»çº¿å¼ºåŒ–å­¦ä¹ é€šè¿‡åˆ©ç”¨é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†æä¾›äº†ä¸€ç§æ›´å®‰å…¨çš„é€‰æ‹©ï¼Œä½†å®ƒå—åˆ°åˆ†å¸ƒåç§»çš„å½±å“ï¼Œè¿™é™ä½äº†å¯¹æœªè§åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å¸ƒåç§»æ„ŸçŸ¥éšå¼Qå­¦ä¹ ï¼ˆDiSA-IQLï¼‰ï¼Œå®ƒæ˜¯IQLçš„æ‰©å±•ï¼Œé€šè¿‡æƒ©ç½šä¸å¯é çš„çŠ¶æ€-åŠ¨ä½œå¯¹æ¥å‡è½»åˆ†å¸ƒåç§»ï¼Œä»è€Œç»“åˆäº†é²æ£’æ€§è°ƒèŠ‚ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§è®¾ç½®ä¸‹çš„ç›®æ ‡åˆ°è¾¾ä»»åŠ¡ä¸­è¯„ä¼°äº†DiSA-IQLï¼šåŒåˆ†å¸ƒå’Œå¼‚åˆ†å¸ƒè¯„ä¼°ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒDiSA-IQLå§‹ç»ˆä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰ã€ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰å’ŒåŸå§‹IQLï¼Œå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ã€æ›´å¹³æ»‘çš„è½¨è¿¹å’Œæ›´é«˜çš„é²æ£’æ€§ã€‚ä»£ç å·²å¼€æºï¼Œä»¥æ”¯æŒå¯é‡å¤æ€§å¹¶ä¿ƒè¿›æŸ”æ€§æœºå™¨äººæ§åˆ¶ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æŸ”æ€§æœºå™¨äººåœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç”±äºè®­ç»ƒæ•°æ®ä¸å®é™…ç¯å¢ƒå­˜åœ¨åˆ†å¸ƒåç§»ï¼Œå¯¼è‡´æ§åˆ¶ç­–ç•¥æ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚è¡Œä¸ºå…‹éš†(BC)ã€ä¿å®ˆQå­¦ä¹ (CQL)å’Œéšå¼Qå­¦ä¹ (IQL)ï¼Œåœ¨åˆ†å¸ƒåç§»ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæ— æ³•ä¿è¯æŸ”æ€§æœºå™¨äººçš„å®‰å…¨å¯é è¿è¡Œã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¯†åˆ«å¹¶æƒ©ç½šé‚£äº›åœ¨è®­ç»ƒæ•°æ®åˆ†å¸ƒä¹‹å¤–çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œä»è€Œæé«˜ç­–ç•¥çš„é²æ£’æ€§ã€‚DiSA-IQLåœ¨IQLçš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†åˆ†å¸ƒåç§»æ„ŸçŸ¥æœºåˆ¶ï¼Œä½¿å¾—ç­–ç•¥å­¦ä¹ è¿‡ç¨‹æ›´åŠ å…³æ³¨é‚£äº›åœ¨ä¸åŒåˆ†å¸ƒä¸‹éƒ½è¡¨ç°è‰¯å¥½çš„çŠ¶æ€-åŠ¨ä½œï¼Œä»è€Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDiSA-IQLçš„æ•´ä½“æ¡†æ¶åŸºäºIQLï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) ç¦»çº¿æ•°æ®é›†ï¼šåŒ…å«æŸ”æ€§æœºå™¨äººåœ¨ä¸åŒç¯å¢ƒä¸‹çš„çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±æ•°æ®ã€‚2) Qå‡½æ•°å­¦ä¹ ï¼šä½¿ç”¨ç¦»çº¿æ•°æ®å­¦ä¹ çŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°Q(s, a)ã€‚3) ç­–ç•¥æå–ï¼šåŸºäºå­¦ä¹ åˆ°çš„Qå‡½æ•°ï¼Œæå–æœ€ä¼˜ç­–ç•¥ã€‚4) é²æ£’æ€§è°ƒåˆ¶ï¼šé€šè¿‡æƒ©ç½šä¸å¯é çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œè°ƒæ•´Qå‡½æ•°å’Œç­–ç•¥ï¼Œæé«˜é²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šDiSA-IQLçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åˆ†å¸ƒåç§»æ„ŸçŸ¥æœºåˆ¶ã€‚è¯¥æœºåˆ¶é€šè¿‡æŸç§æ–¹å¼ï¼ˆè®ºæ–‡ä¸­æœªæ˜ç¡®è¯´æ˜å…·ä½“å®ç°ï¼Œæ ‡è®°ä¸ºæœªçŸ¥ï¼‰è¯„ä¼°çŠ¶æ€-åŠ¨ä½œå¯¹çš„å¯é æ€§ï¼Œå¹¶å¯¹ä¸å¯é çš„çŠ¶æ€-åŠ¨ä½œå¯¹è¿›è¡Œæƒ©ç½šã€‚è¿™ç§æƒ©ç½šæœºåˆ¶ä½¿å¾—ç­–ç•¥å­¦ä¹ è¿‡ç¨‹æ›´åŠ å…³æ³¨é‚£äº›åœ¨ä¸åŒåˆ†å¸ƒä¸‹éƒ½è¡¨ç°è‰¯å¥½çš„çŠ¶æ€-åŠ¨ä½œï¼Œä»è€Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒDiSA-IQLæ˜¾å¼åœ°è€ƒè™‘äº†åˆ†å¸ƒåç§»çš„å½±å“ï¼Œå¹¶é‡‡å–æªæ–½å‡è½»è¿™ç§å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æœªè¯¦ç»†æè¿°é²æ£’æ€§è°ƒåˆ¶çš„å…·ä½“å®ç°ç»†èŠ‚ï¼ŒåŒ…æ‹¬å¦‚ä½•å®šä¹‰å’Œè®¡ç®—çŠ¶æ€-åŠ¨ä½œå¯¹çš„å¯é æ€§ï¼Œä»¥åŠå¦‚ä½•è®¾è®¡æƒ©ç½šå‡½æ•°ã€‚è¿™äº›ç»†èŠ‚å¯¹äºç†è§£å’Œå¤ç°DiSA-IQLè‡³å…³é‡è¦ï¼Œä½†ç›®å‰ä¿¡æ¯ä¸è¶³ã€‚IQLæœ¬èº«çš„å…³é”®è®¾è®¡åŒ…æ‹¬éšå¼ç­–ç•¥æå–å’Œä¿å®ˆçš„ä»·å€¼ä¼°è®¡ï¼Œè¿™äº›è®¾è®¡åœ¨DiSA-IQLä¸­å¾—ä»¥ä¿ç•™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DiSA-IQLåœ¨ä»¿çœŸå®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨åŒåˆ†å¸ƒå’Œå¼‚åˆ†å¸ƒä¸¤ç§è®¾ç½®ä¸‹ï¼Œå‡ä¼˜äºBCã€CQLå’ŒIQLç­‰åŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒDiSA-IQLåœ¨ç›®æ ‡åˆ°è¾¾ä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ï¼Œç”Ÿæˆäº†æ›´å¹³æ»‘çš„è½¨è¿¹ï¼Œå¹¶å±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDiSA-IQLèƒ½å¤Ÿæœ‰æ•ˆå‡è½»åˆ†å¸ƒåç§»çš„å½±å“ï¼Œæé«˜æŸ”æ€§æœºå™¨äººçš„æ§åˆ¶æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DiSA-IQLå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å®‰å…¨å¯é æ§åˆ¶çš„æŸ”æ€§æœºå™¨äººé¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»ç–—æ‰‹æœ¯æœºå™¨äººã€ç¾éš¾æ•‘æ´æœºå™¨äººã€ä»¥åŠå¤æ‚ç¯å¢ƒä¸‹çš„å·¥ä¸šæ£€æµ‹æœºå™¨äººç­‰åœºæ™¯ä¸­ï¼ŒDiSA-IQLå¯ä»¥åˆ©ç”¨é¢„å…ˆæ”¶é›†çš„æ•°æ®ï¼Œè®­ç»ƒå‡ºé²æ£’çš„æ§åˆ¶ç­–ç•¥ï¼Œä»è€Œä¿è¯æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸‹çš„å®‰å…¨å¯é è¿è¡Œã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨æŸ”æ€§æœºå™¨äººåœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Soft snake robots offer remarkable flexibility and adaptability in complex environments, yet their control remains challenging due to highly nonlinear dynamics. Existing model-based and bio-inspired controllers rely on simplified assumptions that limit performance. Deep reinforcement learning (DRL) has recently emerged as a promising alternative, but online training is often impractical because of costly and potentially damaging real-world interactions. Offline RL provides a safer option by leveraging pre-collected datasets, but it suffers from distribution shift, which degrades generalization to unseen scenarios. To overcome this challenge, we propose DiSA-IQL (Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that incorporates robustness modulation by penalizing unreliable state-action pairs to mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks across two settings: in-distribution and out-of-distribution evaluation. Simulation results show that DiSA-IQL consistently outperforms baseline models, including Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla IQL, achieving higher success rates, smoother trajectories, and improved robustness. The codes are open-sourced to support reproducibility and to facilitate further research in offline RL for soft robot control.

