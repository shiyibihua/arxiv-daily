---
layout: default
title: MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation
---

# MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26642" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.26642v1</a>
  <a href="https://arxiv.org/pdf/2509.26642.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26642v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26642v1', 'MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhuoyang Liu, Jiaming Liu, Jiadong Xu, Nuowei Han, Chenyang Gu, Hao Chen, Kaichen Zhou, Renrui Zhang, Kai Chin Hsieh, Kun Wu, Zhengping Che, Jian Tang, Shanghang Zhang

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MLAÂ§öÊÑüÂÆòËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºåÂ¢ûÂº∫Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠Â§öÊ®°ÊÄÅÁêÜËß£‰∏éÈ¢ÑÊµãËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊÑüÂÆòËûçÂêà` `Êú∫Âô®‰∫∫Êìç‰Ωú` `ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã` `Áâ©ÁêÜ‰∏ñÁïåÂª∫Ê®°` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÂøΩÁï•‰∫ÜÂØπÊú∫Âô®‰∫∫ÁâπÂÆöÂ§öÊÑüÂÆò‰ø°ÊÅØÁöÑÂÖ®Èù¢ÁêÜËß£ÔºåÈôêÂà∂‰∫ÜÂ§çÊùÇÊìç‰ΩúÁöÑÊÄßËÉΩ„ÄÇ
2. MLAÊ®°ÂûãÈÄöËøáÊó†ÁºñÁ†ÅÂô®Â§öÊ®°ÊÄÅÂØπÈΩêÊñπÊ°àÂíåÊú™Êù•Â§öÊÑüÂÆòÁîüÊàêÂêéËÆ≠ÁªÉÁ≠ñÁï•ÔºåÂ¢ûÂº∫‰∫ÜÂØπÁâ©ÁêÜ‰∏ñÁïåÁöÑÁêÜËß£ÂíåÈ¢ÑÊµãËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMLAÊ®°ÂûãÂú®Â§çÊùÇÊìç‰Ωú‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®Êú™ËßÅÈÖçÁΩÆ‰∏≠Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã(VLA)ÈÄöËøáÁªßÊâøËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)Âπ∂Â≠¶‰π†Âä®‰ΩúÁîüÊàêÔºåÂ∑≤Âú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫Ê≥õÂåñËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâVLAÊ®°Âûã‰∏ªË¶ÅÂÖ≥Ê≥®ËßÜËßâÂíåËØ≠Ë®ÄÁöÑËß£Èáä‰ª•ÁîüÊàêÂä®‰ΩúÔºåÂøΩÁï•‰∫ÜÊú∫Âô®‰∫∫‰∏éÁ©∫Èó¥Áâ©ÁêÜ‰∏ñÁïåÁöÑÊÑüÁü•Âíå‰∫§‰∫í„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§öÊÑüÂÆòËØ≠Ë®Ä-Âä®‰Ωú(MLA)Ê®°ÂûãÔºåÂÆÉÂçèÂêåÊÑüÁü•ÂºÇÊûÑÊÑüÂÆòÊ®°ÊÄÅÔºåÂπ∂È¢ÑÊµãÊú™Êù•ÁöÑÂ§öÊÑüÂÆòÁõÆÊ†áÔºå‰ª•‰øÉËøõÁâ©ÁêÜ‰∏ñÁïåÂª∫Ê®°„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰∏∫‰∫ÜÂ¢ûÂº∫ÊÑüÁü•Ë°®ÂæÅÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÁºñÁ†ÅÂô®ÁöÑÂ§öÊ®°ÊÄÅÂØπÈΩêÊñπÊ°àÔºåÂàõÊñ∞ÊÄßÂú∞Â∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊú¨Ë∫´ÈáçÊñ∞Áî®‰ΩúÊÑüÁü•Ê®°ÂùóÔºåÈÄöËøá‰ΩçÁΩÆÂØπÂ∫îÁõ¥Êé•Ëß£Èáä2DÂõæÂÉè„ÄÅ3DÁÇπ‰∫ëÂíåËß¶Ëßâtoken„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•Â¢ûÂº∫MLAÂØπÁâ©ÁêÜÂä®ÊÄÅÁöÑÁêÜËß£ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊú™Êù•Â§öÊÑüÂÆòÁîüÊàêÂêéËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰ΩøMLAËÉΩÂ§üÊé®ÁêÜËØ≠‰πâ„ÄÅÂá†‰ΩïÂíå‰∫§‰∫í‰ø°ÊÅØÔºå‰∏∫Âä®‰ΩúÁîüÊàêÊèê‰æõÊõ¥Âº∫Â§ßÁöÑÊù°‰ª∂„ÄÇÂú®ËØÑ‰º∞‰∏≠ÔºåMLAÊ®°ÂûãÂú®Â§çÊùÇ„ÄÅÊé•Ëß¶‰∏∞ÂØåÁöÑÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ÔºåÂàÜÂà´‰ºò‰∫éÂÖàÂâçÁöÑÊúÄÂÖàËøõÁöÑ2DÂíå3D VLAÊñπÊ≥ï12%Âíå24%ÔºåÂêåÊó∂‰πüÂ±ïÁ§∫‰∫ÜÂØπÊú™ËßÅÈÖçÁΩÆÁöÑÊîπËøõÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã(VLA)Âú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Ôºå‰∏ªË¶Å‰æùËµñËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØÁîüÊàêÂä®‰ΩúÔºåËÄåÂøΩÁï•‰∫ÜÊú∫Âô®‰∫∫‰∏éÁâ©ÁêÜ‰∏ñÁïåÁöÑ‰∫§‰∫íÊÑüÁü•ÔºåÁâπÂà´ÊòØÂ§öÊÑüÂÆò‰ø°ÊÅØÁöÑËûçÂêàÂíåÂà©Áî®„ÄÇËøôÂØºËá¥Ê®°ÂûãÂú®Â§çÊùÇ„ÄÅÊé•Ëß¶‰∏∞ÂØåÁöÑ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÊ≥õÂåñËÉΩÂäõÂèóÈôê„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂª∫Ê®°Áâ©ÁêÜ‰∏ñÁïåÁöÑÂä®ÊÄÅÂèòÂåñÂíåÂ§öÊ®°ÊÄÅ‰ø°ÊÅØ‰πãÈó¥ÁöÑÂÖ≥ËÅî„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)‰Ωú‰∏∫Â§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÁªü‰∏ÄÂ§ÑÁêÜÊ°ÜÊû∂ÔºåÈÄöËøáÊó†ÁºñÁ†ÅÂô®ÁöÑÂØπÈΩêÊñπÂºèÔºåÂ∞ÜËßÜËßâ(2DÂõæÂÉè„ÄÅ3DÁÇπ‰∫ë)ÂíåËß¶Ëßâ‰ø°ÊÅØÁõ¥Êé•Êò†Â∞ÑÂà∞LLMÁöÑtokenÁ©∫Èó¥‰∏≠„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊú™Êù•Â§öÊÑüÂÆòÁîüÊàê‰ªªÂä°ÔºåËÆ©Ê®°ÂûãÂ≠¶‰π†È¢ÑÊµãÊú™Êù•ÁöÑÊÑüÂÆòÁä∂ÊÄÅÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£Áâ©ÁêÜ‰∏ñÁïåÁöÑÂä®ÊÄÅÂèòÂåñÔºå‰∏∫Âä®‰ΩúÁîüÊàêÊèê‰æõÊõ¥È≤ÅÊ£íÁöÑÊù°‰ª∂„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMLAÊ®°Âûã‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºöÂ§öÊ®°ÊÄÅÊÑüÁü•Ê®°ÂùóÂíåÊú™Êù•Â§öÊÑüÂÆòÁîüÊàêÊ®°Âùó„ÄÇÂ§öÊ®°ÊÄÅÊÑüÁü•Ê®°ÂùóË¥üË¥£Â∞Ü2DÂõæÂÉè„ÄÅ3DÁÇπ‰∫ëÂíåËß¶Ëßâ‰ø°ÊÅØÈÄöËøá‰ΩçÁΩÆÂØπÂ∫îÂÖ≥Á≥ªÂØπÈΩêÂà∞LLMÁöÑtokenÁ©∫Èó¥„ÄÇÊú™Êù•Â§öÊÑüÂÆòÁîüÊàêÊ®°ÂùóÂàôÂà©Áî®LLMÈ¢ÑÊµãÊú™Êù•ÁöÑÂ§öÊÑüÂÆòÁä∂ÊÄÅÔºåÂåÖÊã¨ËßÜËßâ„ÄÅËß¶ËßâÁ≠â‰ø°ÊÅØ„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÔºåÈ¶ñÂÖàÈÄöËøáÂ§öÊ®°ÊÄÅÊÑüÁü•Ê®°ÂùóÊèêÂèñÂ§öÊ®°ÊÄÅÁâπÂæÅÔºåÁÑ∂ÂêéÂà©Áî®Êú™Êù•Â§öÊÑüÂÆòÁîüÊàêÊ®°ÂùóÈ¢ÑÊµãÊú™Êù•Áä∂ÊÄÅÔºåÊúÄÂêéÂü∫‰∫éÈ¢ÑÊµãÁöÑÁä∂ÊÄÅÁîüÊàêÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ª•‰∏ã‰∏§ÁÇπÔºö‰∏ÄÊòØÊèêÂá∫‰∫ÜÊó†ÁºñÁ†ÅÂô®ÁöÑÂ§öÊ®°ÊÄÅÂØπÈΩêÊñπÊ°àÔºåÁõ¥Êé•Âà©Áî®LLM‰Ωú‰∏∫ÊÑüÁü•Ê®°ÂùóÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÁºñÁ†ÅÂô®ÂèØËÉΩÂ∏¶Êù•ÁöÑ‰ø°ÊÅØÊçüÂ§±Ôºõ‰∫åÊòØËÆæËÆ°‰∫ÜÊú™Êù•Â§öÊÑüÂÆòÁîüÊàêÂêéËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊé®ÁêÜÁâ©ÁêÜ‰∏ñÁïåÁöÑÂä®ÊÄÅÂèòÂåñÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÈ¢ÑÊµãÁéØÂ¢É„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Â§öÊ®°ÊÄÅÂØπÈΩêÊñπÈù¢Ôºå‰ΩøÁî®‰∫Ü‰ΩçÁΩÆÁºñÁ†ÅÂ∞Ü‰∏çÂêåÊ®°ÊÄÅÁöÑ‰ø°ÊÅØÂØπÈΩêÂà∞LLMÁöÑtokenÁ©∫Èó¥„ÄÇÂú®Êú™Êù•Â§öÊÑüÂÆòÁîüÊàêÊñπÈù¢ÔºåÈááÁî®‰∫ÜËá™ÂõûÂΩíÁöÑÊñπÂºèÈ¢ÑÊµãÊú™Êù•ÁöÑÂ§öÊÑüÂÆòÁä∂ÊÄÅÔºåÂπ∂‰ΩøÁî®‰∫Ü‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ËøõË°åËÆ≠ÁªÉ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå2DÂõæÂÉèÂíå3DÁÇπ‰∫ëÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÊ®°ÂûãÊèêÂèñÁâπÂæÅÔºåÁÑ∂ÂêéÈÄöËøáÁ∫øÊÄßÂ±ÇÊò†Â∞ÑÂà∞LLMÁöÑtokenÁ©∫Èó¥„ÄÇËß¶Ëßâ‰ø°ÊÅØÂàôÁõ¥Êé•ÈÄöËøáembeddingÂ±ÇÊò†Â∞ÑÂà∞tokenÁ©∫Èó¥„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºå‰ΩøÁî®‰∫Üteacher forcingÁ≠ñÁï•ÔºåÂç≥‰ΩøÁî®ÁúüÂÆûÁöÑÂéÜÂè≤Áä∂ÊÄÅ‰Ωú‰∏∫ËæìÂÖ•Êù•È¢ÑÊµãÊú™Êù•ÁöÑÁä∂ÊÄÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MLAÊ®°ÂûãÂú®ÁúüÂÆû‰∏ñÁïåÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®Â§çÊùÇ„ÄÅÊé•Ëß¶‰∏∞ÂØåÁöÑ‰ªªÂä°‰∏≠ÔºåMLAÊ®°ÂûãÂàÜÂà´‰ºò‰∫éÂÖàÂâçÁöÑÊúÄÂÖàËøõÁöÑ2DÂíå3D VLAÊñπÊ≥ï12%Âíå24%„ÄÇÊ≠§Â§ñÔºåMLAÊ®°ÂûãÂú®Êú™ËßÅËøáÁöÑÈÖçÁΩÆ‰∏≠‰πüË°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MLAÊ®°ÂûãÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂ§çÊùÇÊìç‰ΩúÂíåÁ≤æÁªÜÊéßÂà∂ÁöÑÊú∫Âô®‰∫∫‰ªªÂä°ÔºåÂ¶ÇË£ÖÈÖç„ÄÅÊäìÂèñ„ÄÅÊìç‰ΩúÂ∑•ÂÖ∑Á≠â„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üÊèêÂçáÊú∫Âô®‰∫∫Âú®Êú™Áü•ÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåÈôç‰ΩéÂØπ‰∫∫Â∑•Á§∫ÊïôÁöÑ‰æùËµñÔºå‰ªéËÄåÂä†ÈÄüÊú∫Âô®‰∫∫Âú®Â∑•‰∏ö„ÄÅÂåªÁñó„ÄÅÊúçÂä°Á≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLA's understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: https://sites.google.com/view/open-mla

