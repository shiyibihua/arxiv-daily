---
layout: default
title: dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought
---

# dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25681" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25681v1</a>
  <a href="https://arxiv.org/pdf/2509.25681.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25681v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25681v1', 'dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junjie Wen, Minjie Zhu, Jiaming Liu, Zhiyuan Liu, Yicun Yang, Linfeng Zhang, Shanghang Zhang, Yichen Zhu, Yi Xu

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: technique report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºdVLAï¼šåŸºäºæ‰©æ•£æ¨¡å‹å’Œå¤šæ¨¡æ€CoTçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæœºå™¨äººæ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `æ‰©æ•£æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `æ€ç»´é“¾` `è·¨æ¨¡æ€æ¨ç†` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨è·¨æ¨¡æ€æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ä»»åŠ¡å’Œæ–°ç¯å¢ƒã€‚
2. dVLAé‡‡ç”¨æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆå¤šæ¨¡æ€æ€ç»´é“¾ï¼Œç»Ÿä¸€ä¼˜åŒ–è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œï¼Œæå‡è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒdVLAåœ¨LIBEROåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸­å‡å–å¾—SOTAæ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹dVLAï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡æ€æ€ç»´é“¾åœ¨å•ä¸ªç³»ç»Ÿä¸­ç»Ÿä¸€è§†è§‰æ„ŸçŸ¥ã€è¯­è¨€æ¨ç†å’Œæœºå™¨äººæ§åˆ¶ã€‚dVLAåœ¨å•ä¸€æ‰©æ•£ç›®æ ‡ä¸‹è”åˆä¼˜åŒ–æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’ŒåŠ¨ä½œï¼Œä»è€Œå®ç°æ›´å¼ºçš„è·¨æ¨¡æ€æ¨ç†å’Œå¯¹æ–°æŒ‡ä»¤å’Œå¯¹è±¡çš„æ›´å¥½æ³›åŒ–ã€‚ä¸ºäº†å®é™…éƒ¨ç½²ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆå‰ç¼€æ³¨æ„åŠ›æ©ç å’ŒKVç¼“å­˜ä¸¤ç§åŠ é€Ÿç­–ç•¥æ¥ç¼“è§£æ¨ç†å»¶è¿Ÿï¼Œä»è€Œåœ¨æµ‹è¯•æ—¶æ¨ç†ä¸­å®ç°é«˜è¾¾æ•°å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä¸­è¯„ä¼°äº†dVLAï¼šåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡æˆåŠŸç‡ä¸º96.4%ï¼Œå§‹ç»ˆä¼˜äºç¦»æ•£å’Œè¿ç»­åŠ¨ä½œç­–ç•¥ï¼›åœ¨çœŸå®çš„Frankaæœºå™¨äººä¸Šï¼Œå®ƒæˆåŠŸåœ°å®Œæˆäº†ä¸€ç³»åˆ—ä¸åŒçš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬éœ€è¦å¤šæ­¥éª¤è§„åˆ’çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–™ç®±æ‹£é€‰ä»»åŠ¡ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„çœŸå®ä¸–ç•Œæ€§èƒ½ã€‚æ€»ä¹‹ï¼Œè¿™äº›ç»“æœå¼ºè°ƒäº†ç»Ÿä¸€æ‰©æ•£æ¡†æ¶åœ¨å®ç”¨ã€é«˜æ€§èƒ½VLAæœºå™¨äººæŠ€æœ¯ä¸­çš„å‰æ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚æœºå™¨äººä»»åŠ¡æ—¶ï¼Œé¢ä¸´è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ä¸è¶³å’Œæ³›åŒ–æ€§å·®çš„é—®é¢˜ã€‚å°¤å…¶æ˜¯åœ¨éœ€è¦å¤šæ­¥éª¤è§„åˆ’å’Œå¤„ç†æ–°é¢–å¯¹è±¡çš„åœºæ™¯ä¸‹ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•´åˆè§†è§‰ä¿¡æ¯ã€è¯­è¨€æŒ‡ä»¤å’ŒåŠ¨ä½œæ§åˆ¶ï¼Œå¯¼è‡´ä»»åŠ¡æˆåŠŸç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šdVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›å’Œå¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œå°†è§†è§‰æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’ŒåŠ¨ä½œç”Ÿæˆç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶ä¸­ã€‚é€šè¿‡è”åˆä¼˜åŒ–è¿™ä¸‰ä¸ªæ¨¡æ€ï¼ŒdVLAèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æŒ‡ä»¤æ„å›¾ï¼Œå¹¶ç”Ÿæˆåˆç†çš„åŠ¨ä½œåºåˆ—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šdVLAçš„æ•´ä½“æ¶æ„åŒ…å«è§†è§‰ç¼–ç å™¨ã€è¯­è¨€ç¼–ç å™¨ã€åŠ¨ä½œè§£ç å™¨ä»¥åŠä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ¨¡å‹ã€‚è§†è§‰ç¼–ç å™¨æå–å›¾åƒç‰¹å¾ï¼Œè¯­è¨€ç¼–ç å™¨å¤„ç†æ–‡æœ¬æŒ‡ä»¤ï¼Œç„¶åå°†è¿™äº›ç‰¹å¾è¾“å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥å»å™ªçš„è¿‡ç¨‹ï¼Œç”Ÿæˆæœ€ç»ˆçš„åŠ¨ä½œåºåˆ—ã€‚å¤šæ¨¡æ€CoTæ¨¡å—åˆ™åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å…¥ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡éœ€æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šdVLAçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ‰©æ•£æ¨¡å‹å’Œå¤šæ¨¡æ€CoTç›¸ç»“åˆï¼Œå®ç°è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œçš„ç»Ÿä¸€å»ºæ¨¡ã€‚ä¸ä¼ ç»Ÿçš„VLAæ¨¡å‹ç›¸æ¯”ï¼ŒdVLAèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è·¨æ¨¡æ€ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Œä»è€Œæé«˜ä»»åŠ¡æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸¤ç§åŠ é€Ÿç­–ç•¥ï¼šå‰ç¼€æ³¨æ„åŠ›æ©ç å’ŒKVç¼“å­˜ï¼Œä»¥é™ä½æ¨ç†å»¶è¿Ÿã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ‰©æ•£æ¨¡å‹æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†DDPMï¼ˆDenoising Diffusion Probabilistic Modelsï¼‰ä½œä¸ºåŸºç¡€æ¡†æ¶ï¼Œå¹¶é’ˆå¯¹VLAä»»åŠ¡è¿›è¡Œäº†æ”¹è¿›ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ‰©æ•£æŸå¤±å’ŒåŠ¨ä½œé¢„æµ‹æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºäº†åŠ é€Ÿæ¨ç†ï¼Œè®ºæ–‡ä½¿ç”¨äº†å‰ç¼€æ³¨æ„åŠ›æ©ç ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„è®¡ç®—ï¼Œå¹¶åˆ©ç”¨KVç¼“å­˜å­˜å‚¨ä¸­é—´ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

dVLAåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†96.4%çš„å¹³å‡æˆåŠŸç‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç¦»æ•£å’Œè¿ç»­åŠ¨ä½œç­–ç•¥ï¼Œè¾¾åˆ°äº†SOTAæ°´å¹³ã€‚åœ¨çœŸå®çš„Frankaæœºå™¨äººä¸Šï¼ŒdVLAæˆåŠŸå®Œæˆäº†åŒ…æ‹¬æ–™ç®±æ‹£é€‰åœ¨å†…çš„å¤šç§å¤æ‚ä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶åœ¨çœŸå®ç¯å¢ƒä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥å‰ç¼€æ³¨æ„åŠ›æ©ç å’ŒKVç¼“å­˜ï¼ŒdVLAçš„æ¨ç†é€Ÿåº¦å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

dVLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå„ç§æœºå™¨äººä»»åŠ¡ï¼Œå¦‚å®¶åº­æœåŠ¡ã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©ç­‰ã€‚é€šè¿‡ç†è§£äººç±»æŒ‡ä»¤å’Œæ„ŸçŸ¥ç¯å¢ƒï¼ŒdVLAèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼Œæé«˜ç”Ÿäº§æ•ˆç‡å’ŒæœåŠ¡è´¨é‡ã€‚æœªæ¥ï¼ŒdVLAæœ‰æœ›æˆä¸ºé€šç”¨æœºå™¨äººæ§åˆ¶å¹³å°ï¼Œèµ‹èƒ½å„è¡Œå„ä¸šã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models are emerging as a next-generation paradigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages a multimodal chain-of-thought to unify visual perception, language reasoning, and robotic control in a single system. dVLA jointly optimizes perception, language understanding, and action under a single diffusion objective, enabling stronger cross-modal reasoning and better generalization to novel instructions and objects. For practical deployment, we mitigate inference latency by incorporating two acceleration strategies, a prefix attention mask and KV caching, yielding up to around times speedup at test-time inference. We evaluate dVLA in both simulation and the real world: on the LIBERO benchmark, it achieves state-of-the-art performance with a 96.4% average success rate, consistently surpassing both discrete and continuous action policies; on a real Franka robot, it succeeds across a diverse task suite, including a challenging bin-picking task that requires multi-step planning, demonstrating robust real-world performance. Together, these results underscore the promise of unified diffusion frameworks for practical, high-performance VLA robotics.

