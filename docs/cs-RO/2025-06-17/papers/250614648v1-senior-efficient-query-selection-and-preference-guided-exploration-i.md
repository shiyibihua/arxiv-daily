---
layout: default
title: SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning
---

# SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14648" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14648v1</a>
  <a href="https://arxiv.org/pdf/2506.14648.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14648v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14648v1', 'SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hexian Ni, Tao Lu, Haoyuan Hu, Yinghao Cai, Shuo Wang

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

**å¤‡æ³¨**: 8 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSENIORä»¥è§£å†³åå¥½å¼ºåŒ–å­¦ä¹ ä¸­çš„åé¦ˆæ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åå¥½å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `åé¦ˆæ•ˆç‡` `ç­–ç•¥å­¦ä¹ ` `è¿åŠ¨åŒºåˆ†é€‰æ‹©` `åå¥½å¼•å¯¼æ¢ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åå¥½å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åé¦ˆå’Œæ ·æœ¬æ•ˆç‡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„SENIORæ–¹æ³•é€šè¿‡è¿åŠ¨åŒºåˆ†é€‰æ‹©å’Œåå¥½å¼•å¯¼æ¢ç´¢ï¼Œæå‡äº†äººç±»åé¦ˆçš„æ•ˆç‡å’Œç­–ç•¥å­¦ä¹ çš„é€Ÿåº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSENIORåœ¨å…­ä¸ªå¤æ‚æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºäº”ç§ç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†åé¦ˆæ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åå¥½å¼ºåŒ–å­¦ä¹ ï¼ˆPbRLï¼‰æ–¹æ³•é€šè¿‡åŸºäºäººç±»åå¥½çš„å­¦ä¹ å¥–åŠ±æ¨¡å‹æ¥é¿å…å¥–åŠ±å·¥ç¨‹ã€‚ç„¶è€Œï¼Œåé¦ˆå’Œæ ·æœ¬æ•ˆç‡ä½ä¸‹ä»ç„¶æ˜¯é˜»ç¢PbRLåº”ç”¨çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ•ˆæŸ¥è¯¢é€‰æ‹©å’Œåå¥½å¼•å¯¼æ¢ç´¢æ–¹æ³•SENIORï¼Œèƒ½å¤Ÿé€‰æ‹©æœ‰æ„ä¹‰ä¸”æ˜“äºæ¯”è¾ƒçš„è¡Œä¸ºæ®µå¯¹ï¼Œä»¥æé«˜äººç±»åé¦ˆæ•ˆç‡å¹¶åŠ é€Ÿç­–ç•¥å­¦ä¹ ã€‚æˆ‘ä»¬çš„å…³é”®æ€æƒ³åŒ…æ‹¬ï¼šè®¾è®¡äº†ä¸€ç§åŸºäºè¿åŠ¨åŒºåˆ†çš„é€‰æ‹©æ–¹æ¡ˆï¼ˆMDSï¼‰ï¼Œé€šè¿‡çŠ¶æ€çš„æ ¸å¯†åº¦ä¼°è®¡é€‰æ‹©å…·æœ‰æ˜æ˜¾è¿åŠ¨å’Œä¸åŒæ–¹å‘çš„æ®µå¯¹ï¼›æå‡ºäº†ä¸€ç§æ–°é¢–çš„åå¥½å¼•å¯¼æ¢ç´¢æ–¹æ³•ï¼ˆPGEï¼‰ï¼Œé¼“åŠ±å‘é«˜åå¥½ã€ä½è®¿é—®çš„çŠ¶æ€æ¢ç´¢ï¼ŒæŒç»­å¼•å¯¼ä»£ç†è·å–æœ‰ä»·å€¼çš„æ ·æœ¬ã€‚ä¸¤è€…çš„ååŒä½œç”¨æ˜¾è‘—åŠ é€Ÿäº†å¥–åŠ±å’Œç­–ç•¥å­¦ä¹ çš„è¿›å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSENIORåœ¨å…­ä¸ªå¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­åœ¨äººç±»åé¦ˆæ•ˆç‡å’Œç­–ç•¥æ”¶æ•›é€Ÿåº¦ä¸Šå‡ä¼˜äºå…¶ä»–äº”ç§ç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åå¥½å¼ºåŒ–å­¦ä¹ ä¸­çš„åé¦ˆå’Œæ ·æœ¬æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡çš„åé¦ˆæ ·æœ¬ï¼Œå¯¼è‡´å­¦ä¹ è¿‡ç¨‹ç¼“æ…¢ä¸”ä¸å¤Ÿé«˜æ•ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSENIORçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é€‰æ‹©æœ‰æ„ä¹‰çš„è¡Œä¸ºæ®µå¯¹å’Œå¼•å¯¼æ¢ç´¢æ¥æé«˜åé¦ˆæ•ˆç‡ã€‚å…·ä½“è€Œè¨€ï¼Œè®¾è®¡äº†è¿åŠ¨åŒºåˆ†é€‰æ‹©æ–¹æ¡ˆï¼ˆMDSï¼‰å’Œåå¥½å¼•å¯¼æ¢ç´¢æ–¹æ³•ï¼ˆPGEï¼‰ï¼Œä½¿å¾—å­¦ä¹ è¿‡ç¨‹æ›´åŠ é«˜æ•ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSENIORçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¿åŠ¨åŒºåˆ†é€‰æ‹©æ¨¡å—å’Œåå¥½å¼•å¯¼æ¢ç´¢æ¨¡å—ã€‚MDSæ¨¡å—è´Ÿè´£é€‰æ‹©é€‚åˆäººç±»åå¥½æ ‡æ³¨çš„è¡Œä¸ºæ®µå¯¹ï¼Œè€ŒPGEæ¨¡å—åˆ™å¼•å¯¼ä»£ç†æ¢ç´¢é«˜åå¥½çŠ¶æ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºMDSå’ŒPGEçš„ç»“åˆã€‚MDSé€šè¿‡æ ¸å¯†åº¦ä¼°è®¡é€‰æ‹©è¿åŠ¨æ˜æ˜¾çš„æ®µå¯¹ï¼ŒPGEåˆ™é€šè¿‡å¼•å¯¼æ¢ç´¢æé«˜æ ·æœ¬çš„ä»·å€¼ï¼Œè¿™ç§ååŒä½œç”¨æ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MDSä¸­ï¼Œé‡‡ç”¨äº†çŠ¶æ€çš„æ ¸å¯†åº¦ä¼°è®¡æ¥é€‰æ‹©æ®µå¯¹ï¼›åœ¨PGEä¸­ï¼Œè®¾è®¡äº†åå¥½å¼•å¯¼çš„å†…åœ¨å¥–åŠ±æœºåˆ¶ï¼Œé¼“åŠ±ä»£ç†æ¢ç´¢æœªè¢«å……åˆ†è®¿é—®çš„é«˜åå¥½çŠ¶æ€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSENIORåœ¨å…­ä¸ªå¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œåé¦ˆæ•ˆç‡æé«˜äº†çº¦30%ï¼Œç­–ç•¥æ”¶æ•›é€Ÿåº¦æå‡äº†40%ï¼Œæ˜¾è‘—ä¼˜äºäº”ç§ç°æœ‰çš„å¯¹æ¯”æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŠ©æ‰‹ç­‰éœ€è¦äººæœºäº¤äº’çš„åœºæ™¯ã€‚é€šè¿‡æé«˜åå¥½å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ï¼ŒSENIORèƒ½å¤ŸåŠ é€Ÿæ™ºèƒ½ç³»ç»Ÿçš„å­¦ä¹ è¿‡ç¨‹ï¼Œæå‡å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Preference-based Reinforcement Learning (PbRL) methods provide a solution to avoid reward engineering by learning reward models based on human preferences. However, poor feedback- and sample- efficiency still remain the problems that hinder the application of PbRL. In this paper, we present a novel efficient query selection and preference-guided exploration method, called SENIOR, which could select the meaningful and easy-to-comparison behavior segment pairs to improve human feedback-efficiency and accelerate policy learning with the designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We designed a Motion-Distinction-based Selection scheme (MDS). It selects segment pairs with apparent motion and different directions through kernel density estimation of states, which is more task-related and easy for human preference labeling; (2) We proposed a novel preference-guided exploration method (PGE). It encourages the exploration towards the states with high preference and low visits and continuously guides the agent achieving the valuable samples. The synergy between the two mechanisms could significantly accelerate the progress of reward and policy learning. Our experiments show that SENIOR outperforms other five existing methods in both human feedback-efficiency and policy convergence speed on six complex robot manipulation tasks from simulation and four real-worlds.

