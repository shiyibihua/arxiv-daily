---
layout: default
title: Latent Action Diffusion for Cross-Embodiment Manipulation
---

# Latent Action Diffusion for Cross-Embodiment Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14608" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14608v3</a>
  <a href="https://arxiv.org/pdf/2506.14608.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14608v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14608v3', 'Latent Action Diffusion for Cross-Embodiment Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Erik Bauer, Elvis Nava, Robert K. Katzschmann

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17 (æ›´æ–°: 2025-10-03)

**å¤‡æ³¨**: 15 pages, 7 figures, website: https://mimicrobotics.github.io/lad/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ½œåœ¨åŠ¨ä½œæ‰©æ•£æ–¹æ³•ä»¥è§£å†³è·¨å½¢æ€æ“æ§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æœºå™¨äººæ“æ§` `è·¨å½¢æ€å­¦ä¹ ` `æ½œåœ¨åŠ¨ä½œç©ºé—´` `æŠ€èƒ½è½¬ç§»` `å¤šæœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç«¯åˆ°ç«¯å­¦ä¹ æ–¹æ³•åœ¨æœºå™¨äººæ“æ§ä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºå’ŒåŠ¨ä½œç©ºé—´å¼‚è´¨æ€§çš„é—®é¢˜ï¼Œé™åˆ¶äº†è·¨å½¢æ€å­¦ä¹ å’ŒæŠ€èƒ½è½¬ç§»ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡åœ¨æ½œåœ¨åŠ¨ä½œç©ºé—´ä¸­å­¦ä¹ æ‰©æ•£ç­–ç•¥ï¼Œç»Ÿä¸€ä¸åŒæœ«ç«¯æ‰§è¡Œå™¨çš„åŠ¨ä½œï¼Œè§£å†³è·¨å½¢æ€æ“æ§çš„æŒ‘æˆ˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨å•ä¸€ç­–ç•¥è¿›è¡Œå¤šæœºå™¨äººæ§åˆ¶ï¼Œæ“æ§æˆåŠŸç‡æé«˜äº†25.3%ï¼Œæœ‰æ•ˆå®ç°äº†æŠ€èƒ½è½¬ç§»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç«¯åˆ°ç«¯å­¦ä¹ ä½œä¸ºä¸€ç§å¼ºå¤§çš„æœºå™¨äººæ“æ§èŒƒå¼ï¼Œå…¶æœ‰æ•ˆæ€§å—åˆ°æ•°æ®ç¨€ç¼ºå’Œæœºå™¨äººå½¢æ€é—´åŠ¨ä½œç©ºé—´å¼‚è´¨æ€§çš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡åœ¨æ½œåœ¨åŠ¨ä½œç©ºé—´ä¸­å­¦ä¹ æ‰©æ•£ç­–ç•¥çš„æ–¹æ³•ï¼Œç»Ÿä¸€ä¸åŒæœ«ç«¯æ‰§è¡Œå™¨çš„åŠ¨ä½œã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä¸ºäººå½¢æœºå™¨äººæ‰‹ã€äººæ‰‹å’Œå¹¶è¡Œå¤¹å…·å­¦ä¹ è¯­ä¹‰å¯¹é½çš„æ½œåœ¨åŠ¨ä½œç©ºé—´ï¼Œå¹¶é€šè¿‡åœ¨ä¸åŒæœ«ç«¯æ‰§è¡Œå™¨çš„æ“æ§æ•°æ®ä¸Šå…±åŒè®­ç»ƒï¼Œåˆ©ç”¨å•ä¸€ç­–ç•¥å®ç°å¤šæœºå™¨äººæ§åˆ¶ï¼ŒæˆåŠŸæå‡æ“æ§æˆåŠŸç‡è¾¾25.3%ã€‚è¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å¯¹æ¯ç§æ–°æœºå™¨äººå½¢æ€çš„å¹¿æ³›æ•°æ®æ”¶é›†éœ€æ±‚ï¼ŒåŠ é€Ÿäº†è·¨å½¢æ€çš„æ³›åŒ–ï¼Œä¿ƒè¿›äº†æ›´å¯æ‰©å±•å’Œé«˜æ•ˆçš„æœºå™¨äººå­¦ä¹ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“æ§ä¸­ç”±äºä¸åŒæœ«ç«¯æ‰§è¡Œå™¨ä¹‹é—´åŠ¨ä½œç©ºé—´å¼‚è´¨æ€§å¯¼è‡´çš„è·¨å½¢æ€å­¦ä¹ å’ŒæŠ€èƒ½è½¬ç§»å›°éš¾ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥æœ‰æ•ˆå®ç°å¤šæœºå™¨äººåä½œã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åœ¨æ½œåœ¨åŠ¨ä½œç©ºé—´ä¸­å­¦ä¹ æ‰©æ•£ç­–ç•¥çš„æ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰å¯¹é½çš„æ½œåœ¨åŠ¨ä½œç©ºé—´æ¥ç»Ÿä¸€ä¸åŒæœ«ç«¯æ‰§è¡Œå™¨çš„åŠ¨ä½œï¼Œä»è€Œå®ç°è·¨å½¢æ€çš„æ“æ§èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯é€šè¿‡å¯¹æ¯”æŸå¤±è®­ç»ƒçš„ç¼–ç å™¨ï¼Œç”¨äºå­¦ä¹ æ½œåœ¨åŠ¨ä½œç©ºé—´ï¼›å…¶æ¬¡æ˜¯åˆ©ç”¨è¯¥æ½œåœ¨åŠ¨ä½œç©ºé—´è¿›è¡Œå¤šæœºå™¨äººæ“æ§ç­–ç•¥çš„å…±åŒè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†æ½œåœ¨è·¨å½¢æ€ç­–ç•¥çš„æ¦‚å¿µï¼ŒæˆåŠŸåœ°å°†ä¸åŒåŠ¨ä½œç©ºé—´ç»Ÿä¸€ä¸ºä¸€ä¸ªæ½œåœ¨ç©ºé—´ï¼Œä»è€Œå®ç°äº†æœ‰æ•ˆçš„æŠ€èƒ½è½¬ç§»å’Œå¤šæœºå™¨äººæ§åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†å¯¹æ¯”æŸå¤±å‡½æ•°æ¥è®­ç»ƒç¼–ç å™¨ï¼Œç¡®ä¿æ½œåœ¨åŠ¨ä½œç©ºé—´çš„è¯­ä¹‰å¯¹é½ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†é€‚åº”ä¸åŒæœ«ç«¯æ‰§è¡Œå™¨çš„ç­–ç•¥å…±äº«æœºåˆ¶ï¼Œä»¥æé«˜æ“æ§çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆ©ç”¨æå‡ºçš„æ½œåœ¨åŠ¨ä½œæ‰©æ•£ç­–ç•¥è¿›è¡Œå¤šæœºå™¨äººæ§åˆ¶ï¼Œæ“æ§æˆåŠŸç‡æé«˜äº†25.3%ã€‚è¿™ä¸€æå‡è¡¨æ˜ï¼Œå°½ç®¡å­˜åœ¨æ˜¾è‘—çš„å½¢æ€å·®å¼‚ï¼ŒæŠ€èƒ½è½¬ç§»ä¾ç„¶æˆåŠŸï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæœºå™¨äººåä½œã€æ™ºèƒ½åˆ¶é€ å’ŒæœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡ç»Ÿä¸€ä¸åŒæœºå™¨äººå½¢æ€çš„æ“æ§ç­–ç•¥ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›å’Œå·¥ä½œæ•ˆç‡ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> End-to-end learning is emerging as a powerful paradigm for robotic manipulation, but its effectiveness is limited by data scarcity and the heterogeneity of action spaces across robot embodiments. In particular, diverse action spaces across different end-effectors create barriers for cross-embodiment learning and skill transfer. We address this challenge through diffusion policies learned in a latent action space that unifies diverse end-effector actions. We first show that we can learn a semantically aligned latent action space for anthropomorphic robotic hands, a human hand, and a parallel jaw gripper using encoders trained with a contrastive loss. Second, we show that by using our proposed latent action space for co-training on manipulation data from different end-effectors, we can utilize a single policy for multi-robot control and obtain up to 25.3% improved manipulation success rates, indicating successful skill transfer despite a significant embodiment gap. Our approach using latent cross-embodiment policies presents a new method to unify different action spaces across embodiments, enabling efficient multi-robot control and data sharing across robot setups. This unified representation significantly reduces the need for extensive data collection for each new robot morphology, accelerates generalization across embodiments, and ultimately facilitates more scalable and efficient robotic learning.

