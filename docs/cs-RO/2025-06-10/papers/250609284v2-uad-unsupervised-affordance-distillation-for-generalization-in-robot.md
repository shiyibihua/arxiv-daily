---
layout: default
title: UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation
---

# UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09284" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09284v2</a>
  <a href="https://arxiv.org/pdf/2506.09284.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09284v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09284v2', 'UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yihe Tang, Wenlong Huang, Yingke Wang, Chengshu Li, Roy Yuan, Ruohan Zhang, Jiajun Wu, Li Fei-Fei

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10 (æ›´æ–°: 2025-08-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— ç›‘ç£çš„å¯ä¾›æ€§è’¸é¦æ–¹æ³•ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— ç›‘ç£å­¦ä¹ ` `å¯ä¾›æ€§è’¸é¦` `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰å¯ä¾›æ€§é¢„æµ‹æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨æ ‡æ³¨æ•°æ®ï¼Œé™åˆ¶äº†å…¶åœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. UADé€šè¿‡æ— ç›‘ç£æ–¹å¼ä»åŸºç¡€æ¨¡å‹ä¸­æå–å¯ä¾›æ€§çŸ¥è¯†ï¼Œè‡ªåŠ¨ç”Ÿæˆ<æŒ‡ä»¤, è§†è§‰å¯ä¾›æ€§>å¯¹ï¼Œé¿å…äº†æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ã€‚
3. UADåœ¨çœŸå®åœºæ™¯ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œç»è¿‡å°‘é‡æ¼”ç¤ºè®­ç»ƒåï¼Œèƒ½å¤Ÿé€‚åº”æœªè§è¿‡çš„ç‰©ä½“å’Œä»»åŠ¡æŒ‡ä»¤ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç†è§£ç»†ç²’åº¦çš„ç‰©ä½“å¯ä¾›æ€§å¯¹äºæœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­æ‰§è¡Œå¼€æ”¾å¼ä»»åŠ¡æŒ‡ä»¤è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰å¯ä¾›æ€§é¢„æµ‹æ–¹æ³•å¾€å¾€ä¾èµ–äºæ‰‹åŠ¨æ ‡æ³¨çš„æ•°æ®æˆ–ä»…åœ¨é¢„å®šä¹‰ä»»åŠ¡é›†ä¸Šè¿›è¡Œã€‚æˆ‘ä»¬æå‡ºäº†UADï¼ˆæ— ç›‘ç£å¯ä¾›æ€§è’¸é¦ï¼‰ï¼Œä¸€ç§ä»åŸºç¡€æ¨¡å‹ä¸­æå–å¯ä¾›æ€§çŸ¥è¯†çš„æ–¹æ³•ï¼Œæ— éœ€ä»»ä½•æ‰‹åŠ¨æ ‡æ³¨ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰æ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ï¼ŒUADè‡ªåŠ¨æ ‡æ³¨äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”Ÿæˆè¯¦ç»†çš„<æŒ‡ä»¤, è§†è§‰å¯ä¾›æ€§>å¯¹ã€‚å°½ç®¡ä»…åœ¨æ¨¡æ‹Ÿä¸­ä½¿ç”¨æ¸²æŸ“å¯¹è±¡è¿›è¡Œè®­ç»ƒï¼ŒUADåœ¨çœŸå®åœºæ™¯å’Œå„ç§äººç±»æ´»åŠ¨ä¸­å±•ç°äº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚ä½¿ç”¨UADæä¾›çš„å¯ä¾›æ€§ä½œä¸ºè§‚å¯Ÿç©ºé—´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ï¼Œç»è¿‡ä»…10æ¬¡æ¼”ç¤ºè®­ç»ƒåï¼Œèƒ½å¤Ÿå¯¹æœªè§è¿‡çš„ç‰©ä½“å®ä¾‹ã€ç‰©ä½“ç±»åˆ«ç”šè‡³ä»»åŠ¡æŒ‡ä»¤çš„å˜åŒ–è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­æ‰§è¡Œå¼€æ”¾å¼ä»»åŠ¡æ—¶å¯¹ç‰©ä½“å¯ä¾›æ€§çš„ç†è§£é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨æ ‡æ³¨æ•°æ®ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›å’Œåº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUADé€šè¿‡æ— ç›‘ç£çš„æ–¹å¼ï¼Œä»åŸºç¡€æ¨¡å‹ä¸­æå–å¯ä¾›æ€§çŸ¥è¯†ï¼Œè‡ªåŠ¨ç”Ÿæˆå¯ä¾›æ€§æ ‡æ³¨ï¼Œé¿å…äº†å¯¹æ‰‹åŠ¨æ ‡æ³¨çš„ä¾èµ–ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†å¤§å‹è§†è§‰æ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUADçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŸºç¡€æ¨¡å‹å’Œä»»åŠ¡æ¡ä»¶è§£ç å™¨ã€‚åŸºç¡€æ¨¡å‹è´Ÿè´£æå–ç‰¹å¾ï¼Œè€Œä»»åŠ¡æ¡ä»¶è§£ç å™¨åˆ™åœ¨å†»ç»“çš„ç‰¹å¾ä¸Šè¿›è¡Œè½»é‡çº§è®­ç»ƒï¼Œä»¥ç”Ÿæˆä»»åŠ¡ç›¸å…³çš„å¯ä¾›æ€§è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šUADçš„ä¸»è¦åˆ›æ–°åœ¨äºæ— ç›‘ç£çš„å¯ä¾›æ€§è’¸é¦è¿‡ç¨‹ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ²¡æœ‰æ‰‹åŠ¨æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„å¯ä¾›æ€§æ•°æ®ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒUADä¸å†ä¾èµ–äºé¢„å®šä¹‰çš„ä»»åŠ¡é›†å’Œæ‰‹åŠ¨æ ‡æ³¨ã€‚

**å…³é”®è®¾è®¡**ï¼šUADé‡‡ç”¨äº†è½»é‡çº§çš„ä»»åŠ¡æ¡ä»¶è§£ç å™¨æ¶æ„ï¼Œç»“åˆäº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¯ä¾›æ€§é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒUADåœ¨çœŸå®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œç»è¿‡ä»…10æ¬¡æ¼”ç¤ºè®­ç»ƒåï¼Œæ¨¡ä»¿å­¦ä¹ ç­–ç•¥èƒ½å¤ŸæˆåŠŸé€‚åº”æœªè§è¿‡çš„ç‰©ä½“å®ä¾‹å’Œä»»åŠ¡æŒ‡ä»¤ï¼Œå±•ç¤ºå‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒUADåœ¨å¤šç§ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿å’Œäººæœºäº¤äº’ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººå¯¹ç‰©ä½“å¯ä¾›æ€§çš„ç†è§£èƒ½åŠ›ï¼ŒUADèƒ½å¤Ÿä½¿æœºå™¨äººåœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°æ‰§è¡Œä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding fine-grained object affordances is imperative for robots to manipulate objects in unstructured environments given open-ended task instructions. However, existing methods of visual affordance predictions often rely on manually annotated data or conditions only on a predefined set of tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for distilling affordance knowledge from foundation models into a task-conditioned affordance model without any manual annotations. By leveraging the complementary strengths of large vision models and vision-language models, UAD automatically annotates a large-scale dataset with detailed $<$instruction, visual affordance$>$ pairs. Training only a lightweight task-conditioned decoder atop frozen features, UAD exhibits notable generalization to in-the-wild robotic scenes and to various human activities, despite only being trained on rendered objects in simulation. Using affordance provided by UAD as the observation space, we show an imitation learning policy that demonstrates promising generalization to unseen object instances, object categories, and even variations in task instructions after training on as few as 10 demonstrations. Project website: https://unsup-affordance.github.io/

