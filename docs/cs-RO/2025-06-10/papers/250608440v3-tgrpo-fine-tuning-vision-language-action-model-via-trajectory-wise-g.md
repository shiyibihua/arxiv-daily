---
layout: default
title: TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization
---

# TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.08440" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.08440v3</a>
  <a href="https://arxiv.org/pdf/2506.08440.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.08440v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.08440v3', 'TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zengjue Chen, Runliang Niu, He Kong, Qi Wang, Qianli Xing, Zipei Fan

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-10 (Êõ¥Êñ∞: 2025-09-27)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TGRPO‰ª•Ëß£ÂÜ≥VLAÊ®°ÂûãÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `Âº∫ÂåñÂ≠¶‰π†` `ËΩ®Ëøπ‰ºòÂåñ` `Á®†ÂØÜÂ•ñÂä±` `Êú∫Âô®‰∫∫‰ªªÂä°` `Ëá™ÈÄÇÂ∫îÁ≠ñÁï•` `‰ªªÂä°ÂàÜÊûê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑVLAÊ®°ÂûãËÆ≠ÁªÉÊñπÊ≥ï‰æùËµñÊâãÂä®Á§∫ËåÉÔºåÈöæ‰ª•Â∫îÂØπÂ§çÊùÇÁéØÂ¢É‰∏≠ÁöÑOODÂú∫ÊôØÂíåÊâßË°åÂÅèÂ∑Æ„ÄÇ
2. TGRPOÈÄöËøáÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂà©Áî®‰ªªÂä°ÂàÜÊûêËá™Âä®ÁîüÊàêÁ®†ÂØÜÂ•ñÂä±ÔºåÈááÁî®Áæ§‰ΩìÁ≠ñÁï•ÂáèÂ∞ëÊñπÂ∑Æ„ÄÇ
3. Âú®LIBEROÂü∫ÂáÜ‰∏äÔºåTGRPOÁöÑÊàêÂäüÁéáËææÂà∞80.7%ÔºåË∂ÖË∂ä‰∫ÜSFTÂíåÂÖ∂‰ªñRLÂêéËÆ≠ÁªÉÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®Â§öÁßçÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠Â±ïÁé∞‰∫ÜÂº∫Â§ßÁöÑË∑®Âú∫ÊôØÊ≥õÂåñËÉΩÂäõÔºå‰ΩÜÂÖ∂ËÆ≠ÁªÉ‰∏ªË¶Å‰æùËµñ‰∫éÊâãÂä®Êî∂ÈõÜÁöÑÊàêÂäüÁ§∫ËåÉÔºåÈöæ‰ª•ÈÄÇÂ∫îÂ§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂàÜÂ∏ÉÂ§ñÔºàOODÔºâÂú∫ÊôØÊàñÊâßË°åÂÅèÂ∑Æ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÂü∫‰∫éËΩ®ËøπÁöÑÁæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàTGRPOÔºâÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁöÑ‰ªªÂä°ÂàÜÊûêËá™Âä®ÊûÑÂª∫Á®†ÂØÜÂ•ñÂä±ÂáΩÊï∞Ôºå‰ªéËÄåÂä†ÈÄüÊî∂ÊïõÂπ∂ÊîπÂñÑ‰ø°Áî®ÂàÜÈÖç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTGRPOÂú®LIBEROÂü∫ÂáÜÁöÑÂõõ‰∏™‰ªªÂä°Á±ªÂà´‰∏äÂÆûÁé∞‰∫Ü80.7%ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÔºåËæÉÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊèêÈ´ò‰∫Ü4.2%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥VLAÊ®°ÂûãÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÊâãÂä®Á§∫ËåÉÔºåÈöæ‰ª•Â§ÑÁêÜOODÂú∫ÊôØÂíåÊâßË°åÂÅèÂ∑ÆÔºåÂØºËá¥ËÆ≠ÁªÉÊïàÁéá‰Ωé‰∏ãÂíåÊÄßËÉΩ‰∏çÁ®≥ÂÆö„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTGRPOÈÄöËøáÂºïÂÖ•Âú®Á∫øÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁöÑ‰ªªÂä°ÂàÜÊûêÊù•Ëá™Âä®ÊûÑÂª∫Á®†ÂØÜÂ•ñÂä±ÂáΩÊï∞Ôºå‰ªéËÄåÊèê‰æõÁªÜÁ≤íÂ∫¶ÂèçÈ¶àÔºåÂä†ÈÄüÊ®°ÂûãÊî∂ÊïõÂπ∂ÊîπÂñÑ‰ø°Áî®ÂàÜÈÖç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTGRPOÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰ªªÂä°ÂàÜÊûêÊ®°Âùó„ÄÅÁ®†ÂØÜÂ•ñÂä±ÊûÑÂª∫Ê®°ÂùóÂíåÁæ§‰ΩìÁ≠ñÁï•‰ºòÂåñÊ®°ÂùóÔºåÈááÁî®Âπ∂Ë°åÈááÊ†∑ÂíåÂΩí‰∏ÄÂåñÂ§ö‰∏™ËΩ®ËøπÔºåÂáèÂ∞ëÊñπÂ∑Æ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTGRPOÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÁæ§‰ΩìÁ≠ñÁï•ÁöÑÂºïÂÖ•ÔºåÈÄöËøáÁõ∏ÂØπÊØîËæÉÂ§ö‰∏™ËΩ®ËøπÊù•Èôç‰ΩéÊñπÂ∑ÆÔºåÂπ∂ÁªìÂêàËΩ®ËøπÁ∫ßÂíåÊ≠•Á∫ßÁöÑ‰ºòÂäø‰º∞ËÆ°ÔºåÊçïÊçâÂÖ®Â±ÄÂíåÂ±ÄÈÉ®‰ºòÂåñ‰ø°Âè∑ÔºåÈÅøÂÖç‰æùËµñ‰ª∑ÂÄºÁΩëÁªú„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåTGRPOÈááÁî®‰∫ÜËá™ÈÄÇÂ∫îÁöÑÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°ÔºåÂπ∂Âú®ÊçüÂ§±ÂáΩÊï∞‰∏≠ÂºïÂÖ•‰∫ÜËΩ®ËøπÁ∫ßÂíåÊ≠•Á∫ßÁöÑ‰ºòÂäø‰º∞ËÆ°ÔºåÁ°Æ‰øùÊ®°ÂûãÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠ÁöÑÁ®≥ÂÆöÊÄßÂíåÊî∂ÊïõÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

TGRPOÂú®LIBEROÂü∫ÂáÜÁöÑÂõõ‰∏™‰ªªÂä°Á±ªÂà´‰∏äÂÆûÁé∞‰∫Ü80.7%ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÔºåËæÉÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊèêÈ´ò‰∫Ü4.2%„ÄÇÊ≠§Â§ñÔºåTGRPOÁöÑË°®Áé∞‰ºò‰∫éÂÖ∂‰ªñ‰ª£Ë°®ÊÄßÁöÑÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂêéËÆ≠ÁªÉÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄßÂíå‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

TGRPOÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Êú∫Âô®‰∫∫‰ªªÂä°„ÄÅÊô∫ËÉΩÂä©ÊâãÂíåËá™Âä®ÂåñÊéßÂà∂Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òVLAÊ®°ÂûãÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊîØÊåÅËá™‰∏ªÂÜ≥Á≠ñÂíå‰∫∫Êú∫Âçè‰ΩúÔºåÊé®Âä®Êô∫ËÉΩÊú∫Âô®‰∫∫ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇÊú™Êù•ÔºåTGRPOÊúâÊúõÂú®Êõ¥Â§öÂÆûÈôÖÂú∫ÊôØ‰∏≠ÂÆûÁé∞È´òÊïàÁöÑ‰ªªÂä°ÊâßË°å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Visual-Language-Action (VLA) models have demonstrated strong cross-scenario generalization capabilities in various robotic tasks through large-scale pre-training and task-specific fine-tuning. However, their training paradigm mainly relies on manually collected successful demonstrations, making it difficult to adapt to complex environments when encountering out-of-distribution (OOD) scenarios or execution biases. While Reinforcement Learning (RL) provides a closed-loop optimization framework via active trial-and-error mechanism, it suffers from sparse rewards, high variance, and unstable optimization in long-horizon robotic tasks. To address these limitations, we propose Trajectory-based Group Relative Policy Optimization (TGRPO), an online RL-based training framework for VLA models. TGRPO leverages task analysis generated by a large language model to automatically construct dense reward functions, providing fine-grained feedback to accelerate convergence and improve credit assignment. The core of our method is a group-based strategy that samples and normalizes multiple trajectories in parallel, reducing variance through relative comparison. By integrating trajectory-level and step-level advantage estimation, TGRPO captures both global and local optimization signals without relying on a value network. Experiments on four task categories of the LIBERO benchmark demonstrate that TGRPO achieves an average success rate of 80.7\%, which is 4.2\% higher than that of Supervised Fine-Tuning (SFT) and outperforms other representative RL-based post-training methods.

