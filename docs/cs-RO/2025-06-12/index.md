---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-06-12
---

# cs.ROï¼ˆ2025-06-12ï¼‰

ğŸ“Š å…± **12** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (9 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250612095v1-doublyaware-dual-planning-and-policy-awareness-for-temporal-differen.html">DoublyAware: Dual Planning and Policy Awareness for Temporal Difference Learning in Humanoid Locomotion</a></td>
  <td>æå‡ºDoublyAwareä»¥è§£å†³äººå½¢æœºå™¨äººè¿åŠ¨ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid locomotion</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.12095v1" data-paper-url="./papers/250612095v1-doublyaware-dual-planning-and-policy-awareness-for-temporal-differen.html" onclick="toggleFavorite(this, '2506.12095v1', 'DoublyAware: Dual Planning and Policy Awareness for Temporal Difference Learning in Humanoid Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250610966v1-genmanip-llm-driven-simulation-for-generalizable-instruction-followi.html">GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation</a></td>
  <td>æå‡ºGenManipä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10966v1" data-paper-url="./papers/250610966v1-genmanip-llm-driven-simulation-for-generalizable-instruction-followi.html" onclick="toggleFavorite(this, '2506.10966v1', 'GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250610826v2-rationalvla-a-rational-vision-language-action-model-with-dual-system.html">RationalVLA: A Rational Vision-Language-Action Model with Dual System</a></td>
  <td>æå‡ºRationalVLAä»¥è§£å†³æœºå™¨äººå¯¹è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç†è§£ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">language conditioned</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10826v2" data-paper-url="./papers/250610826v2-rationalvla-a-rational-vision-language-action-model-with-dual-system.html" onclick="toggleFavorite(this, '2506.10826v2', 'RationalVLA: A Rational Vision-Language-Action Model with Dual System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250611261v1-gondola-grounded-vision-language-planning-for-generalizable-robotic-.html">Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation</a></td>
  <td>æå‡ºGondolaä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11261v1" data-paper-url="./papers/250611261v1-gondola-grounded-vision-language-planning-for-generalizable-robotic-.html" onclick="toggleFavorite(this, '2506.11261v1', 'Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250610359v1-demonstrating-multi-suction-item-picking-at-scale-via-multi-modal-le.html">Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success</a></td>
  <td>é€šè¿‡å¤šæ¨¡æ€å­¦ä¹ æå‡æœºå™¨äººå¤šå¸åŠ›ç‰©å“æ‹¾å–æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10359v1" data-paper-url="./papers/250610359v1-demonstrating-multi-suction-item-picking-at-scale-via-multi-modal-le.html" onclick="toggleFavorite(this, '2506.10359v1', 'Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250610968v2-eye-robot-learning-to-look-to-act-with-a-bc-rl-perception-action-loo.html">Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop</a></td>
  <td>æå‡ºEyeRobotä»¥è§£å†³æœºå™¨äººæ‰‹çœ¼åè°ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10968v2" data-paper-url="./papers/250610968v2-eye-robot-learning-to-look-to-act-with-a-bc-rl-perception-action-loo.html" onclick="toggleFavorite(this, '2506.10968v2', 'Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250610923v1-vib2move-in-hand-object-reconfiguration-via-fingertip-micro-vibratio.html">Vib2Move: In-Hand Object Reconfiguration via Fingertip Micro-Vibrations</a></td>
  <td>æå‡ºVib2Moveä»¥è§£å†³æ‰‹ä¸­ç‰©ä½“é‡æ„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10923v1" data-paper-url="./papers/250610923v1-vib2move-in-hand-object-reconfiguration-via-fingertip-micro-vibratio.html" onclick="toggleFavorite(this, '2506.10923v1', 'Vib2Move: In-Hand Object Reconfiguration via Fingertip Micro-Vibrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250610787v1-in-hand-object-pose-estimation-via-visual-tactile-fusion.html">In-Hand Object Pose Estimation via Visual-Tactile Fusion</a></td>
  <td>æå‡ºè§†è§‰-è§¦è§‰èåˆæ–¹æ³•ä»¥è§£å†³æ‰‹ä¸­ç‰©ä½“å§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10787v1" data-paper-url="./papers/250610787v1-in-hand-object-pose-estimation-via-visual-tactile-fusion.html" onclick="toggleFavorite(this, '2506.10787v1', 'In-Hand Object Pose Estimation via Visual-Tactile Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250610252v1-a-novel-feedforward-youla-parameterization-method-for-avoiding-local.html">A Novel Feedforward Youla Parameterization Method for Avoiding Local Minima in Stereo Image Based Visual Servoing Control</a></td>
  <td>æå‡ºæ–°å‹å‰é¦ˆYoulaå‚æ•°åŒ–æ–¹æ³•ä»¥é¿å…ç«‹ä½“å›¾åƒè§†è§‰ä¼ºæœæ§åˆ¶ä¸­çš„å±€éƒ¨æå°å€¼é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10252v1" data-paper-url="./papers/250610252v1-a-novel-feedforward-youla-parameterization-method-for-avoiding-local.html" onclick="toggleFavorite(this, '2506.10252v1', 'A Novel Feedforward Youla Parameterization Method for Avoiding Local Minima in Stereo Image Based Visual Servoing Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250610600v2-embodiedgen-towards-a-generative-3d-world-engine-for-embodied-intell.html">EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence</a></td>
  <td>æå‡ºEmbodiedGenä»¥è§£å†³ä¼ ç»Ÿ3Dèµ„äº§ç”Ÿæˆæˆæœ¬é«˜å’ŒçœŸå®æ€§ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10600v2" data-paper-url="./papers/250610600v2-embodiedgen-towards-a-generative-3d-world-engine-for-embodied-intell.html" onclick="toggleFavorite(this, '2506.10600v2', 'EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250610462v1-are-we-generalizing-from-the-exception-an-in-the-wild-study-on-group.html">Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions</a></td>
  <td>ç ”ç©¶ç¾¤ä½“é€‚åº”æ€§å¯¹è¯è®¾è®¡ä»¥æå‡äººæœºäº¤äº’æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10462v1" data-paper-url="./papers/250610462v1-are-we-generalizing-from-the-exception-an-in-the-wild-study-on-group.html" onclick="toggleFavorite(this, '2506.10462v1', 'Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250610756v1-grounded-vision-language-navigation-for-uavs-with-open-vocabulary-go.html">Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding</a></td>
  <td>æå‡ºVLFlyæ¡†æ¶ä»¥è§£å†³æ— äººæœºçš„è¯­è¨€å¼•å¯¼å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10756v1" data-paper-url="./papers/250610756v1-grounded-vision-language-navigation-for-uavs-with-open-vocabulary-go.html" onclick="toggleFavorite(this, '2506.10756v1', 'Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)