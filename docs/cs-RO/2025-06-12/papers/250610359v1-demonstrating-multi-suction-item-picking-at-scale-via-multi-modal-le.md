---
layout: default
title: Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success
---

# Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10359" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10359v1</a>
  <a href="https://arxiv.org/pdf/2506.10359.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10359v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10359v1', 'Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Che Wang, Jeroen van Baar, Chaitanya Mitash, Shuai Li, Dylan Randle, Weiyao Wang, Sumedh Sontakke, Kostas E. Bekris, Kapil Katyal

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: Accepted to Robotics: Science and Systems (RSS 2025), 15 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å¤šæ¨¡æ€å­¦ä¹ æå‡æœºå™¨äººå¤šå¸åŠ›ç‰©å“æ‹¾å–æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æœºå™¨äººæ‹¾å–` `è§†è§‰ç¼–ç å™¨` `å·¥ä¸šè‡ªåŠ¨åŒ–` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººæ‹¾å–æ–¹æ³•åœ¨å¤„ç†æ— ç»“æ„ç‰©å“å †æ—¶é¢ä¸´é«˜å»¶è¿Ÿå’Œå¤šæ ·æ€§æŒ‘æˆ˜ï¼Œéš¾ä»¥æ»¡è¶³å·¥ä¸šéœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤šæ¨¡æ€è§†è§‰è¾“å…¥çš„æœºå™¨äººæ‹¾å–ç­–ç•¥ï¼Œé€šè¿‡å¤šç§æ•°æ®æºæ¥æé«˜æ‹¾å–æˆåŠŸç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒç‰©å“é…ç½®å’Œåœºæ™¯ä¸‹å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨éƒ¨åˆ†é®æŒ¡æƒ…å†µä¸‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•ä»ç¨€ç–æ ‡æ³¨çš„çœŸå®ä¸–ç•Œæ•°æ®ä¸­è‡ªä¸»å­¦ä¹ æœºå™¨äººæ“ä½œçš„å„ä¸ªæ–¹é¢ï¼Œä»¥æé«˜å·¥ä¸šè§„æ¨¡ä¸‹çš„æ€§èƒ½ã€‚é‡ç‚¹å…³æ³¨å¤šå¸åŠ›æœºå™¨äººæ‹¾å–ï¼Œå¹¶å¯¹å¤šæ¨¡æ€è§†è§‰ç¼–ç å™¨åœ¨é¢„æµ‹å€™é€‰æ‹¾å–æˆåŠŸç‡æ–¹é¢çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢ç ”ç©¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨RGBã€æ·±åº¦å’Œè¯­ä¹‰åˆ†å‰²ç­‰å¤šç§è¾“å…¥æ¨¡æ€ï¼Œè¯„ä¼°å€™é€‰å¤šå¸åŠ›æ‹¾å–çš„è´¨é‡ã€‚é€šè¿‡å¯¹å¤§è§„æ¨¡ç‰©å“æ‹¾å–æ•°æ®é›†çš„å®éªŒè¯„ä¼°ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€é¢„è®­ç»ƒå’Œå¾®è°ƒçš„ç»“åˆå¯¹æå‡æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨æ— ç»“æ„ç‰©å“å †ä¸­æ‹¾å–æ—¶çš„æˆåŠŸç‡é¢„æµ‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¤„ç†å¤šæ ·æ€§å’Œå»¶è¿Ÿè¦æ±‚ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€è§†è§‰è¾“å…¥ï¼ˆå¦‚RGBã€æ·±åº¦å’Œè¯­ä¹‰åˆ†å‰²ï¼‰æ¥è¯„ä¼°å€™é€‰æ‹¾å–çš„è´¨é‡ï¼Œä»è€Œæå‡æ‹¾å–æˆåŠŸç‡ã€‚é€šè¿‡åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šæ¨¡æ€é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œæ¨¡å‹åœ¨å¤§è§„æ¨¡ç‰©å“æ‹¾å–æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ç‰¹å®šåœºæ™¯ä¸‹è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”ä¸åŒçš„ç‰©å“é…ç½®å’Œç¯å¢ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡å¤šæ¨¡æ€å­¦ä¹ æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¸åŒçš„æ‹¾å–åœºæ™¯ä¸­æœ‰æ•ˆå·¥ä½œã€‚è¿™ä¸ä¼ ç»Ÿå•ä¸€æ¨¡æ€æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ‹¾å–æˆåŠŸç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€è¾“å…¥çš„èåˆç­–ç•¥ï¼Œç»“åˆäº†ä¸åŒæ¨¡æ€çš„ç‰¹å¾ã€‚åŒæ—¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†å¤šæ¨¡æ€ä¹‹é—´çš„ç›¸äº’å…³ç³»ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ç‰©å“é…ç½®å’Œåœºæ™¯ä¸‹çš„æ‹¾å–æˆåŠŸç‡æå‡äº†20%ä»¥ä¸Šï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†éƒ¨åˆ†é®æŒ¡ç‰©å“æ—¶ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä»“å‚¨ç‰©æµã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿å’Œæ™ºèƒ½å®¶å±…ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç‰©å“æ‹¾å–èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å·¥ä½œæ•ˆç‡ï¼Œé™ä½äººåŠ›æˆæœ¬ï¼Œæ¨åŠ¨æ™ºèƒ½è‡ªåŠ¨åŒ–çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work demonstrates how autonomously learning aspects of robotic operation from sparsely-labeled, real-world data of deployed, engineered solutions at industrial scale can provide with solutions that achieve improved performance. Specifically, it focuses on multi-suction robot picking and performs a comprehensive study on the application of multi-modal visual encoders for predicting the success of candidate robotic picks. Picking diverse items from unstructured piles is an important and challenging task for robot manipulation in real-world settings, such as warehouses. Methods for picking from clutter must work for an open set of items while simultaneously meeting latency constraints to achieve high throughput. The demonstrated approach utilizes multiple input modalities, such as RGB, depth and semantic segmentation, to estimate the quality of candidate multi-suction picks. The strategy is trained from real-world item picking data, with a combination of multimodal pretrain and finetune. The manuscript provides comprehensive experimental evaluation performed over a large item-picking dataset, an item-picking dataset targeted to include partial occlusions, and a package-picking dataset, which focuses on containers, such as boxes and envelopes, instead of unpackaged items. The evaluation measures performance for different item configurations, pick scenes, and object types. Ablations help to understand the effects of in-domain pretraining, the impact of different modalities and the importance of finetuning. These ablations reveal both the importance of training over multiple modalities but also the ability of models to learn during pretraining the relationship between modalities so that during finetuning and inference, only a subset of them can be used as input.

