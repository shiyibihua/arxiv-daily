---
layout: default
title: General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting
---

# General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17462" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17462v2</a>
  <a href="https://arxiv.org/pdf/2506.17462.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17462v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17462v2', 'General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bernard Lange, Anil Yildiz, Mansur Arief, Shehryar Khattak, Mykel Kochenderfer, Georgios Georgakis

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-10-17)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºARNAæ¡†æ¶ä»¥è§£å†³æœªçŸ¥ç¯å¢ƒä¸­çš„é€šç”¨å¯¼èˆªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é€šç”¨å¯¼èˆª` `è§†è§‰è¯­è¨€æ¨¡å‹` `æœºå™¨äººæŠ€æœ¯` `è‡ªä¸»å†³ç­–` `å¤šæ¨¡æ€è¾“å…¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººå¯¼èˆªç³»ç»Ÿé€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„ç¥ç»ç½‘ç»œï¼Œç¼ºä¹åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„ARNAæ¡†æ¶é€šè¿‡ç»“åˆLVLMä¸ç°ä»£æœºå™¨äººå·¥å…·åº“ï¼Œä½¿ä»£ç†äººèƒ½å¤Ÿè‡ªä¸»å®šä¹‰å’Œæ‰§è¡Œä»»åŠ¡ç‰¹å®šçš„å·¥ä½œæµã€‚
3. åœ¨Habitat Labçš„å®éªŒä¸­ï¼ŒARNAåœ¨HM-EQAåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œä¸ºæœªçŸ¥ç¯å¢ƒå¼€å‘é€šç”¨å¯¼èˆªç­–ç•¥ä»ç„¶æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚ç°æœ‰ç³»ç»Ÿé€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„ç¥ç»ç½‘ç»œå’Œå›ºå®šçš„ä¿¡æ¯æµï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§ã€‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€šè¿‡åµŒå…¥ç±»äººçŸ¥è¯†ä¸ºæ¨ç†å’Œè§„åˆ’æä¾›äº†æœ‰å¸Œæœ›çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ä¹‹å‰çš„LVLM-æœºå™¨äººé›†æˆä¸»è¦ä¾èµ–äºé¢„å…ˆæ˜ å°„çš„ç©ºé—´å’Œç¡¬ç¼–ç çš„è¡¨ç¤ºã€‚æœ¬æ–‡æå‡ºäº†Agentic Robotic Navigation Architectureï¼ˆARNAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œèµ‹äºˆåŸºäºLVLMçš„ä»£ç†äººä¸€å¥—æ¥è‡ªç°ä»£æœºå™¨äººæ ˆçš„æ„ŸçŸ¥ã€æ¨ç†å’Œå¯¼èˆªå·¥å…·ã€‚ARNAåœ¨Habitat Labçš„HM-EQAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„EQAç‰¹å®šæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¹¿æ³›å¯¼èˆªæŒ‘æˆ˜ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„é€šç”¨å¯¼èˆªé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„ç¥ç»ç½‘ç»œå’Œå›ºå®šçš„ä¿¡æ¯æµï¼Œé™åˆ¶äº†å…¶åœ¨æ–°ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šARNAæ¡†æ¶é€šè¿‡ç»“åˆå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ä¸ç°ä»£æœºå™¨äººæŠ€æœ¯ï¼Œèµ‹äºˆä»£ç†äººè‡ªä¸»å®šä¹‰å’Œæ‰§è¡Œä»»åŠ¡ç‰¹å®šå·¥ä½œæµçš„èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„å¯¼èˆªå’Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šARNAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ„ŸçŸ¥ã€æ¨ç†å’Œå¯¼èˆªæ¨¡å—ï¼Œä»£ç†äººåœ¨è¿è¡Œæ—¶é€šè¿‡æŸ¥è¯¢è¿™äº›æ¨¡å—æ¥å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„å¯¼èˆªåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šARNAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä»£ç†äººèƒ½å¤Ÿåœ¨æœªæ˜ å°„çš„ç¯å¢ƒä¸­è‡ªä¸»æ‰§è¡Œä»»åŠ¡ï¼Œçªç ´äº†ä»¥å¾€ä¾èµ–äºç¡¬ç¼–ç å’Œé¢„æ˜ å°„ç©ºé—´çš„é™åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šARNAçš„è®¾è®¡åŒ…æ‹¬æ¨¡å—åŒ–çš„æ„ŸçŸ¥å’Œæ¨ç†å·¥å…·ï¼Œå…è®¸ä»£ç†äººæ ¹æ®å®æ—¶è¾“å…¥åŠ¨æ€è°ƒæ•´å¯¼èˆªç­–ç•¥ï¼Œå…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†è®¨è®ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Habitat Labçš„HM-EQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒARNAæ¡†æ¶çš„è¡¨ç°è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„EQAç‰¹å®šæ–¹æ³•ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“æ•°æ®åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†å¯¹æ¯”ï¼Œè¯æ˜äº†å…¶åœ¨å¤šç§å¯¼èˆªæŒ‘æˆ˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ã€æ— äººæœºå¯¼èˆªç­‰ã€‚ARNAæ¡†æ¶çš„çµæ´»æ€§å’Œé€šç”¨æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”å„ç§å¤æ‚ç¯å¢ƒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Developing general-purpose navigation policies for unknown environments remains a core challenge in robotics. Most existing systems rely on task-specific neural networks and fixed information flows, limiting their generalizability. Large Vision-Language Models (LVLMs) offer a promising alternative by embedding human-like knowledge for reasoning and planning, but prior LVLM-robot integrations have largely depended on pre-mapped spaces, hard-coded representations, and rigid control logic. We introduce the Agentic Robotic Navigation Architecture (ARNA), a general-purpose framework that equips an LVLM-based agent with a library of perception, reasoning, and navigation tools drawn from modern robotic stacks. At runtime, the agent autonomously defines and executes task-specific workflows that iteratively query modules, reason over multimodal inputs, and select navigation actions. This agentic formulation enables robust navigation and reasoning in previously unmapped environments, offering a new perspective on robotic stack design. Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA outperforms state-of-the-art EQA-specific approaches. Qualitative results on RxR and custom tasks further demonstrate its ability to generalize across a broad range of navigation challenges.

