---
layout: default
title: Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving
---

# Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving

**arXiv**: [2511.12751v1](https://arxiv.org/abs/2511.12751) | [PDF](https://arxiv.org/pdf/2511.12751.pdf)

**ä½œè€…**: Timur Anvar, Jeffrey Chen, Yuyan Wang, Rohan Chandra

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢ç´¢LLMè¾…åŠ©å¼ºåŒ–å­¦ä¹ åœ¨åˆ†æ•£å¼è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ï¼šå¥–åŠ±å¡‘é€ çš„æ¡ˆä¾‹ç ”ç©¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¥–åŠ±å¡‘é€ ` `åˆ†æ•£å¼æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­ä¾èµ–äººå·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œéš¾ä»¥æ•æ‰çœŸå®ä¸–ç•Œçš„è¯­ä¹‰å’Œç¤¾ä¼šå¤æ‚æ€§ã€‚
2. æœ¬ç ”ç©¶æ¢ç´¢åˆ©ç”¨å°å‹æœ¬åœ°LLMé€šè¿‡å¥–åŠ±å¡‘é€ æ¥è¾…åŠ©å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨æå‡è‡ªåŠ¨é©¾é©¶ç­–ç•¥çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMè¾…åŠ©çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶èƒ½æé«˜æˆåŠŸç‡ï¼Œä½†å­˜åœ¨ä¿å®ˆåå·®å’Œæ¨¡å‹ä¾èµ–æ€§ï¼Œæ•ˆç‡ä½äºçº¯å¼ºåŒ–å­¦ä¹ ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼Œå¦‚æ‹¥æŒ¤çš„é«˜é€Ÿå…¬è·¯å’Œè½¦è¾†æ±‡å…¥åœºæ™¯ä¸­ï¼Œè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å¯¼èˆªä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸€ä¸ªå…³é”®é™åˆ¶åœ¨äºå…¶å¯¹è‰¯å¥½å®šä¹‰çš„å¥–åŠ±å‡½æ•°çš„ä¾èµ–ï¼Œè¿™äº›å‡½æ•°é€šå¸¸æ— æ³•æ•æ‰å„ç§åˆ†å¸ƒå¤–æƒ…å†µçš„å®Œæ•´è¯­ä¹‰å’Œç¤¾ä¼šå¤æ‚æ€§ã€‚å› æ­¤ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶æ¢ç´¢ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ›¿ä»£æˆ–è¡¥å……RLï¼Œä»¥è¿›è¡Œç›´æ¥è§„åˆ’å’Œæ§åˆ¶ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿæ¨ç†ä¸°å¯Œçš„è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼ŒLLMä¹Ÿå­˜åœ¨æ˜¾è‘—çš„ç¼ºç‚¹ï¼šåœ¨é›¶æ ·æœ¬å®‰å…¨å…³é”®è®¾ç½®ä¸­å¯èƒ½ä¸ç¨³å®šï¼Œäº§ç”Ÿä¸ä¸€è‡´çš„è¾“å‡ºï¼Œå¹¶ä¸”é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„APIè°ƒç”¨å’Œç½‘ç»œå»¶è¿Ÿã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬ç ”ç©¶å°å‹ã€æœ¬åœ°éƒ¨ç½²çš„LLMï¼ˆ<14Bå‚æ•°ï¼‰æ˜¯å¦å¯ä»¥é€šè¿‡å¥–åŠ±å¡‘é€ è€Œéç›´æ¥æ§åˆ¶æ¥æœ‰æ„ä¹‰åœ°æ”¯æŒè‡ªåŠ¨é«˜é€Ÿå…¬è·¯é©¾é©¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæ¯”è¾ƒäº†çº¯RLã€çº¯LLMå’Œæ··åˆæ–¹æ³•ï¼Œå…¶ä¸­LLMé€šè¿‡åœ¨è®­ç»ƒæœŸé—´å¯¹çŠ¶æ€-åŠ¨ä½œè½¬æ¢è¿›è¡Œè¯„åˆ†æ¥å¢å¼ºRLå¥–åŠ±ï¼Œè€Œæ ‡å‡†RLç­–ç•¥åœ¨æµ‹è¯•æ—¶æ‰§è¡Œã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œçº¯RLæ™ºèƒ½ä½“ä»¥åˆç†çš„æ•ˆç‡å®ç°äº†ä¸­ç­‰çš„æˆåŠŸç‡ï¼ˆ73-89%ï¼‰ï¼Œçº¯LLMæ™ºèƒ½ä½“å¯ä»¥è¾¾åˆ°æ›´é«˜çš„æˆåŠŸç‡ï¼ˆé«˜è¾¾94%ï¼‰ï¼Œä½†é€Ÿåº¦æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œè€Œæ··åˆæ–¹æ³•å§‹ç»ˆä»‹äºè¿™äº›æç«¯ä¹‹é—´ã€‚å…³é”®çš„æ˜¯ï¼Œå°½ç®¡æœ‰æ˜ç¡®çš„æ•ˆç‡æŒ‡ä»¤ï¼Œå—LLMå½±å“çš„æ–¹æ³•è¡¨ç°å‡ºç³»ç»Ÿçš„ä¿å®ˆåå·®ï¼Œå¹¶å…·æœ‰æ˜¾è‘—çš„æ¨¡å‹ä¾èµ–æ€§å¯å˜æ€§ï¼Œè¿™çªå‡ºäº†å½“å‰å°å‹LLMåœ¨å®‰å…¨å…³é”®æ§åˆ¶ä»»åŠ¡ä¸­çš„é‡è¦å±€é™æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºäººå·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œéš¾ä»¥é€‚åº”å¤æ‚äº¤é€šåœºæ™¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå¥–åŠ±å‡½æ•°éš¾ä»¥æ•æ‰çœŸå®ä¸–ç•Œé©¾é©¶çš„è¯­ä¹‰å’Œç¤¾ä¼šè§„åˆ™ï¼Œå¯¼è‡´ç­–ç•¥æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å°å‹æœ¬åœ°éƒ¨ç½²çš„LLMï¼Œé€šè¿‡å¥–åŠ±å¡‘é€ æ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°ã€‚LLMèƒ½å¤Ÿç†è§£æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œä¸ºå¼ºåŒ–å­¦ä¹ æä¾›æ›´å‡†ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ æ›´ç¬¦åˆäººç±»é©¾é©¶ä¹ æƒ¯çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šçº¯RLæ™ºèƒ½ä½“ã€çº¯LLMæ™ºèƒ½ä½“å’Œæ··åˆæ™ºèƒ½ä½“ã€‚æ··åˆæ™ºèƒ½ä½“åœ¨è®­ç»ƒé˜¶æ®µåˆ©ç”¨LLMå¯¹çŠ¶æ€-åŠ¨ä½œè½¬æ¢è¿›è¡Œè¯„åˆ†ï¼Œä»¥æ­¤å¢å¼ºRLçš„å¥–åŠ±å‡½æ•°ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œæ··åˆæ™ºèƒ½ä½“ä½¿ç”¨è®­ç»ƒå¥½çš„RLç­–ç•¥è¿›è¡Œæ§åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†LLMå¼•å…¥åˆ°å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å¡‘é€ è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨LLMçš„è¯­ä¹‰ç†è§£èƒ½åŠ›æ¥æ”¹å–„å¥–åŠ±å‡½æ•°çš„è´¨é‡ã€‚ä¸ç›´æ¥ä½¿ç”¨LLMè¿›è¡Œæ§åˆ¶ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨åˆ©ç”¨LLMçš„ä¼˜åŠ¿ï¼ŒåŒæ—¶é¿å…å…¶åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„ä¸ç¨³å®šæ€§å’Œé«˜å»¶è¿Ÿé—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†å°äº14Bå‚æ•°çš„å°å‹LLMï¼Œä»¥ä¿è¯æœ¬åœ°éƒ¨ç½²å’Œä½å»¶è¿Ÿã€‚LLMçš„è¾“å‡ºè¢«ç”¨äºè°ƒæ•´å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°ï¼Œå…·ä½“çš„è°ƒæ•´æ–¹å¼å’Œæƒé‡æ˜¯å…³é”®çš„è®¾è®¡å‚æ•°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†æ˜ç¡®çš„æ•ˆç‡æŒ‡ä»¤ï¼Œè¯•å›¾å¼•å¯¼LLMç”Ÿæˆæ›´é«˜æ•ˆçš„é©¾é©¶ç­–ç•¥ï¼Œä½†å®éªŒç»“æœè¡¨æ˜æ•ˆæœæœ‰é™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œçº¯RLæ™ºèƒ½ä½“æˆåŠŸç‡åœ¨73-89%ä¹‹é—´ï¼Œçº¯LLMæ™ºèƒ½ä½“æˆåŠŸç‡é«˜è¾¾94%ï¼Œä½†é€Ÿåº¦æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æ··åˆæ–¹æ³•ä»‹äºä¸¤è€…ä¹‹é—´ï¼Œä½†è¡¨ç°å‡ºä¿å®ˆåå·®å’Œæ¨¡å‹ä¾èµ–æ€§ã€‚å°½ç®¡æœ‰æ˜ç¡®çš„æ•ˆç‡æŒ‡ä»¤ï¼ŒLLMè¾…åŠ©çš„æ–¹æ³•åœ¨æ•ˆç‡æ–¹é¢ä»ä¸å¦‚çº¯RLæ–¹æ³•ï¼Œè¡¨æ˜å½“å‰å°å‹LLMåœ¨å®‰å…¨å…³é”®æ§åˆ¶ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å†³ç­–æ§åˆ¶ç³»ç»Ÿï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚å’ŒåŠ¨æ€çš„äº¤é€šç¯å¢ƒä¸­ã€‚é€šè¿‡LLMè¾…åŠ©çš„å¥–åŠ±å¡‘é€ ï¼Œæœ‰æœ›æå‡è‡ªåŠ¨é©¾é©¶ç­–ç•¥çš„å®‰å…¨æ€§ã€æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–éœ€è¦å¤æ‚å¥–åŠ±å‡½æ•°è®¾è®¡çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªå’Œæ¸¸æˆAIã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.

