---
layout: default
title: RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation
---

# RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.12436" target="_blank" class="toolbar-btn">arXiv: 2511.12436v1</a>
    <a href="https://arxiv.org/pdf/2511.12436.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.12436v1" 
            onclick="toggleFavorite(this, '2511.12436v1', 'RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiaoshuai Hao, Yingbo Tang, Lingfeng Zhang, Yanbiao Ma, Yunfeng Diao, Ziyu Jia, Wenbo Ding, Hangjun Ye, Long Chen

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**RoboAfford++Ôºö‰∏Ä‰∏™ÁîüÊàêÂºèAIÂ¢ûÂº∫ÁöÑÂ§öÊ®°ÊÄÅÂèØ‰æõÊÄßÂ≠¶‰π†Êï∞ÊçÆÈõÜÔºåÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰ΩúÂíåÂØºËà™**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `Êú∫Âô®‰∫∫ÂØºËà™` `ÂèØ‰æõÊÄßÂ≠¶‰π†` `Â§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜ` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®È´òÁ∫ß‰ªªÂä°ËßÑÂàíÂíåÂú∫ÊôØÁêÜËß£ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Êé®Êñ≠Áâ©ÁêÜ‰∫§‰∫íÁöÑÂèØÊìç‰Ωú‰ΩçÁΩÆÔºàÂ¶ÇÊäìÂèñÁÇπÂíåÊîæÁΩÆÂå∫ÂüüÔºâÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄß„ÄÇ
2. RoboAfford++Êï∞ÊçÆÈõÜÈÄöËøáÁîüÊàêÂºèAIÂ¢ûÂº∫ÔºåÊèê‰æõ‰∫ÜÁªÜÁ≤íÂ∫¶ÁöÑÁâ©‰ΩìÂíåÁ©∫Èó¥ÂèØ‰æõÊÄßÊ†áÊ≥®ÔºåÊó®Âú®Âº•Ë°•Áé∞ÊúâÊï∞ÊçÆÈõÜÂú®ÂèØ‰æõÊÄß‰ø°ÊÅØÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®RoboAfford++‰∏äÂæÆË∞ÉËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂèØ‰ª•ÊòæËëóÊèêÂçáÂÖ∂ÂØπÁâ©‰ΩìÂíåÁ©∫Èó¥ÂèØ‰æõÊÄßÁöÑÊé®ÁêÜËÉΩÂäõÔºåËØÅÊòé‰∫ÜÊï∞ÊçÆÈõÜÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜRoboAfford++Ôºå‰∏Ä‰∏™ÁîüÊàêÂºèAIÂ¢ûÂº∫ÁöÑÊï∞ÊçÆÈõÜÔºåÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰ΩúÂíåÂØºËà™‰∏≠ÁöÑÂ§öÊ®°ÊÄÅÂèØ‰æõÊÄßÂ≠¶‰π†„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´869,987Âº†ÂõæÂÉèÔºåÂπ∂ÈÖçÊúâ200‰∏á‰∏™ÈóÆÁ≠îÔºàQAÔºâÊ†áÊ≥®ÔºåÊ∂µÁõñ‰∏â‰∏™ÂÖ≥ÈîÆ‰ªªÂä°ÔºöÂü∫‰∫éÂ±ûÊÄßÂíåÁ©∫Èó¥ÂÖ≥Á≥ªËØÜÂà´ÁõÆÊ†áÁâ©‰ΩìÁöÑÁâ©‰ΩìÂèØ‰æõÊÄßËØÜÂà´ÔºõÁ≤æÁ°ÆÂÆö‰ΩçÁî®‰∫éÊìç‰ΩúÁöÑÂäüËÉΩÈÉ®‰ª∂ÁöÑÁâ©‰ΩìÂèØ‰æõÊÄßÈ¢ÑÊµãÔºõ‰ª•ÂèäËØÜÂà´Áî®‰∫éÁâ©‰ΩìÊîæÁΩÆÂíåÊú∫Âô®‰∫∫ÂØºËà™ÁöÑËá™Áî±Á©∫Èó¥ÁöÑÁ©∫Èó¥ÂèØ‰æõÊÄßÂÆö‰Ωç„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòÊèêÂá∫‰∫ÜRoboAfford-EvalÔºå‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞ÁúüÂÆûÂú∫ÊôØ‰∏≠ÂèØ‰æõÊÄßÊÑüÁü•È¢ÑÊµãÁöÑÁªºÂêàÂü∫ÂáÜÔºåÂåÖÂê´338‰∏™Á≤æÂøÉÊ†áÊ≥®ÁöÑÊ†∑Êú¨ÔºåÊ∂µÁõñ‰∏äËø∞‰∏â‰∏™‰ªªÂä°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ÂèØ‰æõÊÄßÂ≠¶‰π†ÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåËÄåÂü∫‰∫éRoboAfford++Êï∞ÊçÆÈõÜÁöÑÂæÆË∞ÉÂèØ‰ª•ÊòæËëóÊèêÈ´òÂÆÉ‰ª¨ÂØπÁâ©‰ΩìÂíåÁ©∫Èó¥ÂèØ‰æõÊÄßÁöÑÊé®ÁêÜËÉΩÂäõÔºåÈ™åËØÅ‰∫ÜÊï∞ÊçÆÈõÜÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®È´òÁ∫ß‰ªªÂä°ËßÑÂàíÂíåÂú∫ÊôØÁêÜËß£ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁº∫‰πèÂØπÁâ©‰ΩìÂíåÁ©∫Èó¥ÂèØ‰æõÊÄßÁöÑÁªÜÁ≤íÂ∫¶ÁêÜËß£ÔºåÂØºËá¥Êó†Ê≥ïÂáÜÁ°ÆÊé®Êñ≠Áâ©ÁêÜ‰∫§‰∫íÁöÑÂèØÊìç‰Ωú‰ΩçÁΩÆÔºå‰æãÂ¶ÇÂäüËÉΩÊÄßÁöÑÊäìÂèñÁÇπÂíåÂÖÅËÆ∏ÊîæÁΩÆÁöÑÂå∫Âüü„ÄÇÁé∞ÊúâËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁº∫‰πèÂØπÁâ©‰ΩìÂíåÁ©∫Èó¥ÂèØ‰æõÊÄßÁöÑÁªÜÁ≤íÂ∫¶Ê†áÊ≥®ÔºåÊòØÈÄ†ÊàêËøô‰∏ÄÈóÆÈ¢òÁöÑ‰∏ªË¶ÅÂéüÂõ†„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Â§ßËßÑÊ®°„ÄÅÈ´òË¥®ÈáèÁöÑÂ§öÊ®°ÊÄÅÂèØ‰æõÊÄßÊï∞ÊçÆÈõÜRoboAfford++ÔºåÂπ∂Âà©Áî®ÁîüÊàêÂºèAIÊäÄÊúØÂ¢ûÂº∫Êï∞ÊçÆÈõÜÁöÑÊ†áÊ≥®Ë¥®ÈáèÂíåÊï∞Èáè„ÄÇÈÄöËøáÂú®RoboAfford++‰∏äÂæÆË∞ÉVLMsÔºåÂèØ‰ª•ÊòæËëóÊèêÂçáÂÖ∂ÂØπÁâ©‰ΩìÂíåÁ©∫Èó¥ÂèØ‰æõÊÄßÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰ªéËÄåÊèêÈ´òÊú∫Âô®‰∫∫Âú®Êìç‰ΩúÂíåÂØºËà™‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRoboAfford++Êï∞ÊçÆÈõÜÊ∂µÁõñ‰∏â‰∏™ÂÖ≥ÈîÆ‰ªªÂä°ÔºöÁâ©‰ΩìÂèØ‰æõÊÄßËØÜÂà´ÔºàObject Affordance RecognitionÔºâ„ÄÅÁâ©‰ΩìÂèØ‰æõÊÄßÈ¢ÑÊµãÔºàObject Affordance PredictionÔºâÂíåÁ©∫Èó¥ÂèØ‰æõÊÄßÂÆö‰ΩçÔºàSpatial Affordance LocalizationÔºâ„ÄÇÊï∞ÊçÆÈõÜÂåÖÂê´ÂõæÂÉèÂíåÈóÆÁ≠îÔºàQAÔºâÊ†áÊ≥®„ÄÇÊ≠§Â§ñÔºåËøòÊèêÂá∫‰∫ÜRoboAfford-EvalÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑÂèØ‰æõÊÄßÊÑüÁü•È¢ÑÊµãËÉΩÂäõ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Êï∞ÊçÆÊî∂ÈõÜ„ÄÅÊ†áÊ≥®„ÄÅÁîüÊàêÂºèAIÂ¢ûÂº∫„ÄÅÊ®°ÂûãËÆ≠ÁªÉÂíåËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°„ÄÅÂ§öÊ®°ÊÄÅÁöÑÂèØ‰æõÊÄßÊï∞ÊçÆÈõÜRoboAfford++ÔºåÂ°´Ë°•‰∫ÜÁé∞ÊúâÊï∞ÊçÆÈõÜÂú®ÂèØ‰æõÊÄß‰ø°ÊÅØÊñπÈù¢ÁöÑÁ©∫ÁôΩ„ÄÇ2) Âà©Áî®ÁîüÊàêÂºèAIÊäÄÊúØÂ¢ûÂº∫Êï∞ÊçÆÈõÜÁöÑÊ†áÊ≥®Ë¥®ÈáèÂíåÊï∞ÈáèÔºåÊèêÈ´ò‰∫ÜÊï∞ÊçÆÁöÑÂèØÁî®ÊÄß„ÄÇ3) ÊèêÂá∫‰∫ÜRoboAfford-EvalÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑÂèØ‰æõÊÄßÊÑüÁü•È¢ÑÊµãËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÊèêÂçáVLMsÂØπÁâ©‰ΩìÂíåÁ©∫Èó¥ÂèØ‰æõÊÄßÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊï∞ÊçÆÈõÜÂåÖÂê´869,987Âº†ÂõæÂÉèÂíå200‰∏á‰∏™ÈóÆÁ≠îÊ†áÊ≥®ÔºåÊ∂µÁõñÁâ©‰ΩìÂ±ûÊÄß„ÄÅÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÅÂäüËÉΩÈÉ®‰ª∂ÂíåËá™Áî±Á©∫Èó¥Á≠â‰ø°ÊÅØ„ÄÇRoboAfford-EvalÂü∫ÂáÜÂåÖÂê´338‰∏™Á≤æÂøÉÊ†áÊ≥®ÁöÑÊ†∑Êú¨ÔºåÊ∂µÁõñ‰∏äËø∞‰∏â‰∏™‰ªªÂä°„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆ„ÄÅÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁ≠âÊäÄÊúØÁªÜËäÇÊú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜÊèèËø∞ÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®RoboAfford++Êï∞ÊçÆÈõÜ‰∏äÂæÆË∞ÉÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂèØ‰ª•ÊòæËëóÊèêÈ´òÂÖ∂ÂØπÁâ©‰ΩìÂíåÁ©∫Èó¥ÂèØ‰æõÊÄßÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Êú™Âú®ÊëòË¶Å‰∏≠ÁªôÂá∫ÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ‰ΩÜÊï¥‰ΩìÂÆûÈ™åÁªìÊûúÈ™åËØÅ‰∫ÜRoboAfford++Êï∞ÊçÆÈõÜÁöÑÊúâÊïàÊÄßÔºåÂπ∂Ë°®ÊòéÂÖ∂ËÉΩÂ§üÊúâÊïàÊèêÂçáVLMsÂú®ÂèØ‰æõÊÄßÂ≠¶‰π†ÊñπÈù¢ÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫ÂØπÁéØÂ¢ÉÁöÑÁêÜËß£Âíå‰∫§‰∫íËÉΩÂäõÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥È´òÊïàÁöÑËá™Âä®Âåñ‰ªªÂä°Ôºå‰æãÂ¶ÇÁâ©‰ΩìÊäìÂèñ„ÄÅÊîæÁΩÆ„ÄÅÂØºËà™Á≠â„ÄÇÊú™Êù•ÔºåËØ•Êï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÂèØ‰ª•‰øÉËøõÊú∫Âô®‰∫∫ÂèØ‰æõÊÄßÂ≠¶‰π†È¢ÜÂüüÁöÑÂèëÂ±ïÔºåÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.

