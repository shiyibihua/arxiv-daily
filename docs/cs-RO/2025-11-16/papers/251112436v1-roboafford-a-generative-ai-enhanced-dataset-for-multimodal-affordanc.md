---
layout: default
title: RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation
---

# RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation

**arXiv**: [2511.12436v1](https://arxiv.org/abs/2511.12436) | [PDF](https://arxiv.org/pdf/2511.12436.pdf)

**ä½œè€…**: Xiaoshuai Hao, Yingbo Tang, Lingfeng Zhang, Yanbiao Ma, Yunfeng Diao, Ziyu Jia, Wenbo Ding, Hangjun Ye, Long Chen

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RoboAfford++ï¼šä¸€ä¸ªç”Ÿæˆå¼AIå¢žå¼ºçš„å¤šæ¨¡æ€å¯ä¾›æ€§å­¦ä¹ æ•°æ®é›†ï¼Œç”¨äºŽæœºå™¨äººæ“ä½œå’Œå¯¼èˆª**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `æœºå™¨äººå¯¼èˆª` `å¯ä¾›æ€§å­¦ä¹ ` `å¤šæ¨¡æ€æ•°æ®é›†` `è§†è§‰è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨é«˜çº§ä»»åŠ¡è§„åˆ’å’Œåœºæ™¯ç†è§£æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½†åœ¨æŽ¨æ–­ç‰©ç†äº¤äº’çš„å¯æ“ä½œä½ç½®ï¼ˆå¦‚æŠ“å–ç‚¹å’Œæ”¾ç½®åŒºåŸŸï¼‰æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚
2. RoboAfford++æ•°æ®é›†é€šè¿‡ç”Ÿæˆå¼AIå¢žå¼ºï¼Œæä¾›äº†ç»†ç²’åº¦çš„ç‰©ä½“å’Œç©ºé—´å¯ä¾›æ€§æ ‡æ³¨ï¼Œæ—¨åœ¨å¼¥è¡¥çŽ°æœ‰æ•°æ®é›†åœ¨å¯ä¾›æ€§ä¿¡æ¯æ–¹é¢çš„ä¸è¶³ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼Œåœ¨RoboAfford++ä¸Šå¾®è°ƒè§†è§‰è¯­è¨€æ¨¡åž‹å¯ä»¥æ˜¾è‘—æå‡å…¶å¯¹ç‰©ä½“å’Œç©ºé—´å¯ä¾›æ€§çš„æŽ¨ç†èƒ½åŠ›ï¼Œè¯æ˜Žäº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†RoboAfford++ï¼Œä¸€ä¸ªç”Ÿæˆå¼AIå¢žå¼ºçš„æ•°æ®é›†ï¼Œç”¨äºŽæœºå™¨äººæ“ä½œå’Œå¯¼èˆªä¸­çš„å¤šæ¨¡æ€å¯ä¾›æ€§å­¦ä¹ ã€‚è¯¥æ•°æ®é›†åŒ…å«869,987å¼ å›¾åƒï¼Œå¹¶é…æœ‰200ä¸‡ä¸ªé—®ç­”ï¼ˆQAï¼‰æ ‡æ³¨ï¼Œæ¶µç›–ä¸‰ä¸ªå…³é”®ä»»åŠ¡ï¼šåŸºäºŽå±žæ€§å’Œç©ºé—´å…³ç³»è¯†åˆ«ç›®æ ‡ç‰©ä½“çš„ç‰©ä½“å¯ä¾›æ€§è¯†åˆ«ï¼›ç²¾ç¡®å®šä½ç”¨äºŽæ“ä½œçš„åŠŸèƒ½éƒ¨ä»¶çš„ç‰©ä½“å¯ä¾›æ€§é¢„æµ‹ï¼›ä»¥åŠè¯†åˆ«ç”¨äºŽç‰©ä½“æ”¾ç½®å’Œæœºå™¨äººå¯¼èˆªçš„è‡ªç”±ç©ºé—´çš„ç©ºé—´å¯ä¾›æ€§å®šä½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†RoboAfford-Evalï¼Œä¸€ä¸ªç”¨äºŽè¯„ä¼°çœŸå®žåœºæ™¯ä¸­å¯ä¾›æ€§æ„ŸçŸ¥é¢„æµ‹çš„ç»¼åˆåŸºå‡†ï¼ŒåŒ…å«338ä¸ªç²¾å¿ƒæ ‡æ³¨çš„æ ·æœ¬ï¼Œæ¶µç›–ä¸Šè¿°ä¸‰ä¸ªä»»åŠ¡ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒçŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMsï¼‰åœ¨å¯ä¾›æ€§å­¦ä¹ æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè€ŒåŸºäºŽRoboAfford++æ•°æ®é›†çš„å¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜å®ƒä»¬å¯¹ç‰©ä½“å’Œç©ºé—´å¯ä¾›æ€§çš„æŽ¨ç†èƒ½åŠ›ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMsï¼‰åœ¨é«˜çº§ä»»åŠ¡è§„åˆ’å’Œåœºæ™¯ç†è§£æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½†ç¼ºä¹å¯¹ç‰©ä½“å’Œç©ºé—´å¯ä¾›æ€§çš„ç»†ç²’åº¦ç†è§£ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®æŽ¨æ–­ç‰©ç†äº¤äº’çš„å¯æ“ä½œä½ç½®ï¼Œä¾‹å¦‚åŠŸèƒ½æ€§çš„æŠ“å–ç‚¹å’Œå…è®¸æ”¾ç½®çš„åŒºåŸŸã€‚çŽ°æœ‰è®­ç»ƒæ•°æ®é›†ç¼ºä¹å¯¹ç‰©ä½“å’Œç©ºé—´å¯ä¾›æ€§çš„ç»†ç²’åº¦æ ‡æ³¨ï¼Œæ˜¯é€ æˆè¿™ä¸€é—®é¢˜çš„ä¸»è¦åŽŸå› ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æž„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å¤šæ¨¡æ€å¯ä¾›æ€§æ•°æ®é›†RoboAfford++ï¼Œå¹¶åˆ©ç”¨ç”Ÿæˆå¼AIæŠ€æœ¯å¢žå¼ºæ•°æ®é›†çš„æ ‡æ³¨è´¨é‡å’Œæ•°é‡ã€‚é€šè¿‡åœ¨RoboAfford++ä¸Šå¾®è°ƒVLMsï¼Œå¯ä»¥æ˜¾è‘—æå‡å…¶å¯¹ç‰©ä½“å’Œç©ºé—´å¯ä¾›æ€§çš„æŽ¨ç†èƒ½åŠ›ï¼Œä»Žè€Œæé«˜æœºå™¨äººåœ¨æ“ä½œå’Œå¯¼èˆªä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šRoboAfford++æ•°æ®é›†æ¶µç›–ä¸‰ä¸ªå…³é”®ä»»åŠ¡ï¼šç‰©ä½“å¯ä¾›æ€§è¯†åˆ«ï¼ˆObject Affordance Recognitionï¼‰ã€ç‰©ä½“å¯ä¾›æ€§é¢„æµ‹ï¼ˆObject Affordance Predictionï¼‰å’Œç©ºé—´å¯ä¾›æ€§å®šä½ï¼ˆSpatial Affordance Localizationï¼‰ã€‚æ•°æ®é›†åŒ…å«å›¾åƒå’Œé—®ç­”ï¼ˆQAï¼‰æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†RoboAfford-EvalåŸºå‡†ï¼Œç”¨äºŽè¯„ä¼°æ¨¡åž‹åœ¨çœŸå®žåœºæ™¯ä¸­çš„å¯ä¾›æ€§æ„ŸçŸ¥é¢„æµ‹èƒ½åŠ›ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ ‡æ³¨ã€ç”Ÿæˆå¼AIå¢žå¼ºã€æ¨¡åž‹è®­ç»ƒå’Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ¨¡æ€çš„å¯ä¾›æ€§æ•°æ®é›†RoboAfford++ï¼Œå¡«è¡¥äº†çŽ°æœ‰æ•°æ®é›†åœ¨å¯ä¾›æ€§ä¿¡æ¯æ–¹é¢çš„ç©ºç™½ã€‚2) åˆ©ç”¨ç”Ÿæˆå¼AIæŠ€æœ¯å¢žå¼ºæ•°æ®é›†çš„æ ‡æ³¨è´¨é‡å’Œæ•°é‡ï¼Œæé«˜äº†æ•°æ®çš„å¯ç”¨æ€§ã€‚3) æå‡ºäº†RoboAfford-EvalåŸºå‡†ï¼Œç”¨äºŽè¯„ä¼°æ¨¡åž‹åœ¨çœŸå®žåœºæ™¯ä¸­çš„å¯ä¾›æ€§æ„ŸçŸ¥é¢„æµ‹èƒ½åŠ›ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå‡VLMså¯¹ç‰©ä½“å’Œç©ºé—´å¯ä¾›æ€§çš„ç†è§£å’ŒæŽ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†åŒ…å«869,987å¼ å›¾åƒå’Œ200ä¸‡ä¸ªé—®ç­”æ ‡æ³¨ï¼Œæ¶µç›–ç‰©ä½“å±žæ€§ã€ç©ºé—´å…³ç³»ã€åŠŸèƒ½éƒ¨ä»¶å’Œè‡ªç”±ç©ºé—´ç­‰ä¿¡æ¯ã€‚RoboAfford-EvalåŸºå‡†åŒ…å«338ä¸ªç²¾å¿ƒæ ‡æ³¨çš„æ ·æœ¬ï¼Œæ¶µç›–ä¸Šè¿°ä¸‰ä¸ªä»»åŠ¡ã€‚å…·ä½“å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æž„ç­‰æŠ€æœ¯ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†æè¿°ï¼Œå±žäºŽæœªçŸ¥ä¿¡æ¯ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œåœ¨RoboAfford++æ•°æ®é›†ä¸Šå¾®è°ƒçŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMsï¼‰å¯ä»¥æ˜¾è‘—æé«˜å…¶å¯¹ç‰©ä½“å’Œç©ºé—´å¯ä¾›æ€§çš„æŽ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œå±žäºŽæœªçŸ¥ä¿¡æ¯ã€‚ä½†æ•´ä½“å®žéªŒç»“æžœéªŒè¯äº†RoboAfford++æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜Žå…¶èƒ½å¤Ÿæœ‰æ•ˆæå‡VLMsåœ¨å¯ä¾›æ€§å­¦ä¹ æ–¹é¢çš„æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹çŽ¯å¢ƒçš„ç†è§£å’Œäº¤äº’èƒ½åŠ›ï¼Œå¯ä»¥å®žçŽ°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–ã€æ”¾ç½®ã€å¯¼èˆªç­‰ã€‚æœªæ¥ï¼Œè¯¥æ•°æ®é›†å’ŒåŸºå‡†å¯ä»¥ä¿ƒè¿›æœºå™¨äººå¯ä¾›æ€§å­¦ä¹ é¢†åŸŸçš„å‘å±•ï¼ŒæŽ¨åŠ¨æœºå™¨äººæŠ€æœ¯åœ¨å®žé™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.

