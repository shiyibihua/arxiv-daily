---
layout: default
title: ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models
---

# ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16211" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16211v1</a>
  <a href="https://arxiv.org/pdf/2506.16211.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16211v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16211v1', 'ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Puhao Li, Yingying Wu, Ziheng Xi, Wanlin Li, Yuzhe Huang, Zhiyuan Zhang, Yinghan Chen, Jianan Wang, Song-Chun Zhu, Tengyu Liu, Siyuan Huang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: Website: https://controlvla.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºControlVLAä»¥è§£å†³å°‘é‡ç¤ºä¾‹ä¸‹çš„æœºå™¨äººæ“ä½œé€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°‘é‡ç¤ºä¾‹å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `å¯¹è±¡ä¸­å¿ƒè¡¨ç¤º` `å¾®è°ƒæŠ€æœ¯` `ControlNet`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å°‘é‡ç¤ºä¾‹æœºå™¨äººæ“ä½œæ–¹æ³•åœ¨æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œä¸”ç¼ºä¹å¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”èƒ½åŠ›ã€‚
2. ControlVLAé€šè¿‡é›¶åˆå§‹åŒ–æŠ•å½±å±‚ï¼Œå°†é¢„è®­ç»ƒçš„æ“ä½œç­–ç•¥ä¸å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºç›¸ç»“åˆï¼Œå®ç°é«˜æ•ˆçš„å¾®è°ƒã€‚
3. åœ¨å…­ä¸ªä¸åŒçš„çœŸå®ä»»åŠ¡ä¸­ï¼ŒControlVLAåœ¨ä»…éœ€10-20ä¸ªç¤ºä¾‹çš„æƒ…å†µä¸‹è¾¾åˆ°äº†76.7%çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—æå‡äº†æ“ä½œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å­¦ä¹ ç°å®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¯ç”¨ç¤ºä¾‹æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç°æœ‰çš„å°‘é‡ç¤ºä¾‹æ“ä½œæ–¹æ³•é€šå¸¸ä¾èµ–äºæ¨¡æ‹Ÿå¢å¼ºæ•°æ®æˆ–é¢„æ„å»ºæ¨¡å—ï¼Œå¦‚æŠ“å–å’Œå§¿æ€ä¼°è®¡ï¼Œè¿™äº›æ–¹æ³•åœ¨æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´å­˜åœ¨å·®è·ä¸”ç¼ºä¹æ‰©å±•æ€§ã€‚å°½ç®¡å¤§è§„æ¨¡æ¨¡ä»¿é¢„è®­ç»ƒæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸­å°†è¿™äº›é€šç”¨ç­–ç•¥é€‚åº”äºç‰¹å®šä»»åŠ¡ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ControlVLAï¼Œä¸€ä¸ªé€šè¿‡ControlNeté£æ ¼æ¶æ„å°†é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¡¨ç¤ºç›¸ç»“åˆçš„æ¡†æ¶ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¾®è°ƒã€‚åœ¨å…­ä¸ªä¸åŒä»»åŠ¡çš„çœŸå®å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»…éœ€10-20ä¸ªç¤ºä¾‹çš„æƒ…å†µä¸‹å®ç°äº†76.7%çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•æ‰€éœ€çš„100å¤šä¸ªç¤ºä¾‹ã€‚é¢å¤–å®éªŒè¡¨æ˜ControlVLAåœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„æ‰©å±•æ€§å’Œå¯¹æœªè§å¯¹è±¡åŠèƒŒæ™¯çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨æœ‰é™ç¤ºä¾‹ä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é€‚åº”äºç‰¹å®šçš„æœºå™¨äººæ“ä½œä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡ç¤ºä¾‹ï¼Œå¯¼è‡´åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹çš„é€‚åº”æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šControlVLAçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é›¶åˆå§‹åŒ–ä¸€ç»„æŠ•å½±å±‚ï¼Œä½¿å…¶èƒ½å¤Ÿé€æ­¥é€‚åº”é¢„è®­ç»ƒçš„æ“ä½œç­–ç•¥ï¼Œä»è€Œå¼•å…¥å¯¹è±¡ä¸­å¿ƒæ¡ä»¶è€Œä¸è¦†ç›–å·²æœ‰çŸ¥è¯†ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹åœ¨å°‘é‡ç¤ºä¾‹ä¸‹çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šControlVLAé‡‡ç”¨ControlNeté£æ ¼çš„æ¶æ„ï¼Œä¸»è¦åŒ…æ‹¬é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å’Œå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„ç»“åˆã€‚è¯¥æ¡†æ¶é€šè¿‡é€æ­¥å¾®è°ƒæŠ•å½±å±‚ï¼Œå®ç°å¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šControlVLAçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶é›¶åˆå§‹åŒ–çš„æŠ•å½±å±‚è®¾è®¡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸ä¸¢å¤±é¢„è®­ç»ƒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œçµæ´»é€‚åº”æ–°çš„æ“ä½œä»»åŠ¡ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ç›´æ¥é‡è®­ç»ƒæ–¹å¼å½¢æˆäº†æ˜æ˜¾å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒControlVLAçš„æŠ•å½±å±‚é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¯¹è±¡ç‰¹å¾ï¼Œå¹¶ä¿æŒå¯¹é¢„è®­ç»ƒç­–ç•¥çš„ä¾èµ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å…­ä¸ªä¸åŒçš„çœŸå®ä»»åŠ¡ä¸­ï¼ŒControlVLAåœ¨ä»…éœ€10-20ä¸ªç¤ºä¾‹çš„æƒ…å†µä¸‹å®ç°äº†76.7%çš„æˆåŠŸç‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æ‰€éœ€çš„100å¤šä¸ªç¤ºä¾‹ï¼Œæå‡å¹…åº¦æ˜¾è‘—ã€‚è¿™ä¸€ç»“æœå±•ç¤ºäº†ControlVLAåœ¨å°‘é‡ç¤ºä¾‹å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ControlVLAçš„ç ”ç©¶æˆæœåœ¨æœºå™¨äººæ“ä½œã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨åŒ–ç”Ÿäº§ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡å‡å°‘å¯¹ç¤ºä¾‹çš„ä¾èµ–ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŠ é€Ÿæœºå™¨äººåœ¨æ–°ç¯å¢ƒä¸­çš„å­¦ä¹ å’Œé€‚åº”ï¼Œæå‡å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„æœºå™¨äººç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning real-world robotic manipulation is challenging, particularly when limited demonstrations are available. Existing methods for few-shot manipulation often rely on simulation-augmented data or pre-built modules like grasping and pose estimation, which struggle with sim-to-real gaps and lack extensibility. While large-scale imitation pre-training shows promise, adapting these general-purpose policies to specific tasks in data-scarce settings remains unexplored. To achieve this, we propose ControlVLA, a novel framework that bridges pre-trained VLA models with object-centric representations via a ControlNet-style architecture for efficient fine-tuning. Specifically, to introduce object-centric conditions without overwriting prior knowledge, ControlVLA zero-initializes a set of projection layers, allowing them to gradually adapt the pre-trained manipulation policies. In real-world experiments across 6 diverse tasks, including pouring cubes and folding clothes, our method achieves a 76.7% success rate while requiring only 10-20 demonstrations -- a significant improvement over traditional approaches that require more than 100 demonstrations to achieve comparable success. Additional experiments highlight ControlVLA's extensibility to long-horizon tasks and robustness to unseen objects and backgrounds.

