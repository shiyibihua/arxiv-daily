---
layout: default
title: CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity
---

# CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16652" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16652v1</a>
  <a href="https://arxiv.org/pdf/2506.16652.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16652v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16652v1', 'CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guang Yin, Yitong Li, Yixuan Wang, Dale McConachie, Paarth Shah, Kunimatsu Hashimoto, Huan Zhang, Katherine Liu, Yunzhu Li

**åˆ†ç±»**: cs.RO, cs.CV, cs.LG, cs.SE

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: Accepted to Robotics: Science and Systems (RSS) 2025. The first three authors contributed equally. Project Page: https://robopil.github.io/code-diffuser/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCodeDiffuserä»¥è§£å†³è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ¨¡ç³Šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è‡ªç„¶è¯­è¨€å¤„ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ¨¡ç³Šæ€§è§£æ` `ä»»åŠ¡ç‰¹å®šä»£ç ` `å¤šæ¨¡æ€èåˆ` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¡ä»¶ç­–ç•¥åœ¨å¤„ç†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ—¶ï¼Œå¸¸å› ç¼ºä¹æ¨¡å—åŒ–å’Œå¯è§£é‡Šæ€§è€Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå¯æ‰§è¡Œä»£ç æ¥è§£ææ¨¡ç³Šçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å¤æ‚æ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†å¯¹è¯­è¨€å’Œç¯å¢ƒå˜åŒ–çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å¸¸å¸¸å­˜åœ¨æ¨¡ç³Šæ€§å’Œä¸ç¡®å®šæ€§ã€‚ä¾‹å¦‚ï¼ŒæŒ‡ä»¤â€œæŠŠæ¯å­æŒ‚åœ¨æ¯æ¶ä¸Šâ€å¯èƒ½æ¶‰åŠå¤šä¸ªæœ‰æ•ˆåŠ¨ä½œã€‚ç°æœ‰çš„è¯­è¨€æ¡ä»¶ç­–ç•¥é€šå¸¸ä¾èµ–äºç«¯åˆ°ç«¯æ¨¡å‹ï¼Œéš¾ä»¥å¤„ç†é«˜å±‚è¯­ä¹‰ç†è§£ä¸ä½å±‚åŠ¨ä½œç”Ÿæˆçš„æ¨¡å—åŒ–å’Œå¯è§£é‡Šæ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æœºå™¨äººæ“ä½œæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§£æè‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­çš„æŠ½è±¡æ¦‚å¿µï¼Œå¹¶ç”Ÿæˆä»»åŠ¡ç‰¹å®šä»£ç ï¼Œä½œä¸ºå¯è§£é‡Šå’Œå¯æ‰§è¡Œçš„ä¸­é—´è¡¨ç¤ºã€‚ç”Ÿæˆçš„ä»£ç ä¸æ„ŸçŸ¥æ¨¡å—æ¥å£ï¼Œç»“åˆç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ç”Ÿæˆ3Dæ³¨æ„åŠ›å›¾ï¼Œæœ‰æ•ˆè§£å†³æŒ‡ä»¤ä¸­çš„æ¨¡ç³Šæ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œè¯†åˆ«äº†å½“å‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•çš„å…³é”®å±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤„ç†è¯­è¨€æ¨¡ç³Šæ€§ã€æ¥è§¦ä¸°å¯Œçš„æ“ä½œå’Œå¤šç‰©ä½“äº¤äº’æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨æœºå™¨äººæ“ä½œä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç«¯åˆ°ç«¯æ¨¡å‹ï¼Œç¼ºä¹å¯¹é«˜å±‚è¯­ä¹‰å’Œä½å±‚åŠ¨ä½œç”Ÿæˆçš„æœ‰æ•ˆåˆ†ç¦»ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§£æè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶ç”Ÿæˆå¯æ‰§è¡Œçš„ä»»åŠ¡ç‰¹å®šä»£ç ï¼Œä»¥æé«˜æŒ‡ä»¤çš„å¯è§£é‡Šæ€§å’Œæ‰§è¡Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè‡ªç„¶è¯­è¨€è§£ææ¨¡å—ã€ä»£ç ç”Ÿæˆæ¨¡å—å’Œæ„ŸçŸ¥æ¨¡å—ã€‚è‡ªç„¶è¯­è¨€è§£ææ¨¡å—è´Ÿè´£ç†è§£æŒ‡ä»¤ï¼Œä»£ç ç”Ÿæˆæ¨¡å—å°†æŒ‡ä»¤è½¬åŒ–ä¸ºå¯æ‰§è¡Œä»£ç ï¼Œæ„ŸçŸ¥æ¨¡å—åˆ™ç”Ÿæˆ3Dæ³¨æ„åŠ›å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ç”Ÿæˆä¸­é—´ä»£ç æ¥è§£å†³æŒ‡ä»¤æ¨¡ç³Šæ€§ï¼Œè¿™ç§æ–¹æ³•ä¸ç°æœ‰çš„ç«¯åˆ°ç«¯æ¨¡å‹æœ‰æœ¬è´¨åŒºåˆ«ï¼Œå¢å¼ºäº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œæ¨¡å—åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜å¯¹ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯çš„æ•´åˆèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCodeDiffuseråœ¨å¤„ç†è¯­è¨€æ¨¡ç³Šæ€§å’Œå¤æ‚æ“ä½œä»»åŠ¡æ—¶ï¼Œç›¸è¾ƒäºç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç‰©ä½“äº¤äº’å’Œæ¥è§¦ä¸°å¯Œçš„æ“ä½œåœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’ŒæœåŠ¡æœºå™¨äººç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜æœºå™¨äººå¯¹è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡äººæœºäº¤äº’çš„æ•ˆç‡å’Œçµæ´»æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„æ™ºèƒ½æœºå™¨äººåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Natural language instructions for robotic manipulation tasks often exhibit ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug tree" may involve multiple valid actions if there are several mugs and branches to choose from. Existing language-conditioned policies typically rely on end-to-end models that jointly handle high-level semantic understanding and low-level action generation, which can result in suboptimal performance due to their lack of modularity and interpretability. To address these challenges, we introduce a novel robotic manipulation framework that can accomplish tasks specified by potentially ambiguous natural language. This framework employs a Vision-Language Model (VLM) to interpret abstract concepts in natural language instructions and generates task-specific code - an interpretable and executable intermediate representation. The generated code interfaces with the perception module to produce 3D attention maps that highlight task-relevant regions by integrating spatial and semantic information, effectively resolving ambiguities in instructions. Through extensive experiments, we identify key limitations of current imitation learning methods, such as poor adaptation to language and environmental variations. We show that our approach excels across challenging manipulation tasks involving language ambiguity, contact-rich manipulation, and multi-object interactions.

