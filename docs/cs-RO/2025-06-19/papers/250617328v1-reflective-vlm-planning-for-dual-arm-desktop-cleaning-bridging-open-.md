---
layout: default
title: Reflective VLM Planning for Dual-Arm Desktop Cleaning: Bridging Open-Vocabulary Perception and Precise Manipulation
---

# Reflective VLM Planning for Dual-Arm Desktop Cleaning: Bridging Open-Vocabulary Perception and Precise Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17328" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17328v1</a>
  <a href="https://arxiv.org/pdf/2506.17328.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17328v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17328v1', 'Reflective VLM Planning for Dual-Arm Desktop Cleaning: Bridging Open-Vocabulary Perception and Precise Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yufan Liu, Yi Wu, Gweneth Ge, Haoliang Cheng, Rui Liu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåå°„å¼è§†è§‰è¯­è¨€æ¨¡å‹è§„åˆ’ä»¥è§£å†³åŒè‡‚æ¡Œé¢æ¸…æ´é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `åŒè‡‚æœºå™¨äºº` `æ¡Œé¢æ¸…æ´` `å¼€æ”¾è¯æ±‡è¯†åˆ«` `ç»“æ„åŒ–è®°å¿†` `å®æ—¶æ§åˆ¶` `æ“ä½œåºåˆ—ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¼‚è´¨åƒåœ¾æ—¶ï¼Œå¾€å¾€ç¼ºä¹å¼€æ”¾è¯æ±‡è¯†åˆ«å’Œç²¾ç¡®æ“ä½œçš„èƒ½åŠ›ï¼Œå¯¼è‡´æ¸…æ´æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„å±‚æ¬¡åŒ–æ¡†æ¶ç»“åˆäº†åå°„å¼VLMè§„åˆ’å’ŒåŒè‡‚æ‰§è¡Œï¼Œé€šè¿‡ç»“æ„åŒ–åœºæ™¯è¡¨ç¤ºæ¥æå‡æ“ä½œçš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç³»ç»Ÿåœ¨æ¨¡æ‹Ÿåœºæ™¯ä¸­å®ç°äº†87.2%çš„ä»»åŠ¡å®Œæˆç‡ï¼Œè¾ƒé™æ€VLMå’Œå•è‡‚åŸºçº¿åˆ†åˆ«æé«˜äº†28.8%å’Œ36.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¡Œé¢æ¸…æ´éœ€è¦å¯¹å¼‚è´¨åƒåœ¾è¿›è¡Œå¼€æ”¾è¯æ±‡è¯†åˆ«å’Œç²¾ç¡®æ“ä½œã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–æ¡†æ¶ï¼Œå°†åå°„å¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§„åˆ’ä¸åŒè‡‚æ‰§è¡Œç»“åˆï¼Œé€šè¿‡ç»“æ„åŒ–åœºæ™¯è¡¨ç¤ºå®ç°ã€‚Grounded-SAM2ç”¨äºå¼€æ”¾è¯æ±‡æ£€æµ‹ï¼Œè€Œå¢å¼ºè®°å¿†çš„VLMç”Ÿæˆã€è¯„ä¼°å’Œä¿®è®¢æ“ä½œåºåˆ—ã€‚è¿™äº›åºåˆ—è¢«è½¬æ¢ä¸ºäº”ç§åŸè¯­çš„å‚æ•°è½¨è¿¹ï¼Œç”±åè°ƒçš„Frankaè‡‚æ‰§è¡Œã€‚åœ¨æ¨¡æ‹Ÿåœºæ™¯ä¸­è¯„ä¼°ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå®ç°äº†87.2%çš„ä»»åŠ¡å®Œæˆç‡ï¼Œç›¸è¾ƒäºé™æ€VLMæé«˜äº†28.8%ï¼Œç›¸è¾ƒäºå•è‡‚åŸºçº¿æé«˜äº†36.2%ã€‚ç»“æ„åŒ–è®°å¿†é›†æˆå¯¹äºç¨³å¥ã€å¯æ³›åŒ–çš„æ“ä½œè‡³å…³é‡è¦ï¼ŒåŒæ—¶ä¿æŒå®æ—¶æ§åˆ¶æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¡Œé¢æ¸…æ´ä¸­å¯¹å¼‚è´¨åƒåœ¾çš„å¼€æ”¾è¯æ±‡è¯†åˆ«å’Œç²¾ç¡®æ“ä½œçš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ¸…æ´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥åå°„å¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§„åˆ’ä¸åŒè‡‚æ‰§è¡Œçš„ç»“åˆï¼Œåˆ©ç”¨ç»“æ„åŒ–åœºæ™¯è¡¨ç¤ºæ¥æå‡æ“ä½œçš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ï¼Œè¿›è€Œå®ç°é«˜æ•ˆçš„æ¡Œé¢æ¸…æ´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼ŒGrounded-SAM2ç”¨äºå¼€æ”¾è¯æ±‡æ£€æµ‹ï¼›å…¶æ¬¡ï¼Œå¢å¼ºè®°å¿†çš„VLMè´Ÿè´£ç”Ÿæˆã€è¯„ä¼°å’Œä¿®è®¢æ“ä½œåºåˆ—ï¼›æœ€åï¼Œè¿™äº›åºåˆ—è¢«è½¬æ¢ä¸ºå‚æ•°è½¨è¿¹ï¼Œç”±åè°ƒçš„Frankaè‡‚æ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºç»“æ„åŒ–è®°å¿†çš„é›†æˆï¼Œä½¿å¾—æ“ä½œæ›´åŠ ç¨³å¥å’Œå¯æ³›åŒ–ï¼ŒåŒæ—¶ä¿æŒå®æ—¶æ§åˆ¶æ€§èƒ½ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„é™æ€VLMæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†å¢å¼ºè®°å¿†æœºåˆ¶ä»¥æ”¯æŒVLMçš„æ“ä½œåºåˆ—ç”Ÿæˆï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è½¨è¿¹ç”Ÿæˆè¿‡ç¨‹ï¼Œç¡®ä¿äº”ç§åŸè¯­çš„é«˜æ•ˆæ‰§è¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç³»ç»Ÿåœ¨æ¨¡æ‹Ÿåœºæ™¯ä¸­å®ç°äº†87.2%çš„ä»»åŠ¡å®Œæˆç‡ï¼Œè¾ƒé™æ€VLMæé«˜äº†28.8%ï¼Œè¾ƒå•è‡‚åŸºçº¿æé«˜äº†36.2%ã€‚è¿™ä¸€æ˜¾è‘—æå‡è¡¨æ˜äº†ç»“æ„åŒ–è®°å¿†é›†æˆåœ¨æ“ä½œç¨³å¥æ€§å’Œå®æ—¶æ§åˆ¶ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®¶åº­è‡ªåŠ¨åŒ–ã€æœåŠ¡æœºå™¨äººå’Œå·¥ä¸šæ¸…æ´ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¸…æ´æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½å®¶å±…å’ŒæœåŠ¡æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Desktop cleaning demands open-vocabulary recognition and precise manipulation for heterogeneous debris. We propose a hierarchical framework integrating reflective Vision-Language Model (VLM) planning with dual-arm execution via structured scene representation. Grounded-SAM2 facilitates open-vocabulary detection, while a memory-augmented VLM generates, critiques, and revises manipulation sequences. These sequences are converted into parametric trajectories for five primitives executed by coordinated Franka arms. Evaluated in simulated scenarios, our system achieving 87.2% task completion, a 28.8% improvement over static VLM and 36.2% over single-arm baselines. Structured memory integration proves crucial for robust, generalizable manipulation while maintaining real-time control performance.

