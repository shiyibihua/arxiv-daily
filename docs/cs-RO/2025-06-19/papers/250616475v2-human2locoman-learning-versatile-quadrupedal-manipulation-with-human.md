---
layout: default
title: Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining
---

# Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16475" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16475v2</a>
  <a href="https://arxiv.org/pdf/2506.16475.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16475v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16475v2', 'Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yaru Niu, Yunzhe Zhang, Mingyang Yu, Changyi Lin, Chenhao Li, Yikai Wang, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Zhenzhen Li, Jonathan Francis, Bingqing Chen, Jie Tan, Ding Zhao

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-07-07)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨ä½“ç°æ¨¡ä»¿å­¦ä¹ ç³»ç»Ÿä»¥è§£å†³å››è¶³æœºå™¨äººæ“æ§èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `å››è¶³æœºå™¨äºº` `æ¨¡ä»¿å­¦ä¹ ` `æ“æ§èƒ½åŠ›` `æ•°æ®é›†æ„å»º` `è·¨ä½“ç°å­¦ä¹ ` `é¥æ“ä½œ` `å®¶åº­ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å››è¶³æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“æ§èƒ½åŠ›ä¸è¶³ï¼Œç¼ºä¹æœ‰æ•ˆçš„è‡ªä¸»æ“æ§æŠ€èƒ½ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è·¨ä½“ç°æ¨¡ä»¿å­¦ä¹ ç³»ç»Ÿï¼Œåˆ©ç”¨äººç±»å’Œæœºå™¨äººæ•°æ®çš„ç»“åˆï¼Œæ„å»ºäº†ç»Ÿä¸€çš„è§‚å¯Ÿä¸åŠ¨ä½œç©ºé—´ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å…­ä¸ªçœŸå®ä¸–ç•Œæ“æ§ä»»åŠ¡ä¸­ï¼Œæ•´ä½“æˆåŠŸç‡æå‡41.9%ï¼Œåœ¨OODè®¾ç½®ä¸‹æå‡79.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å››è¶³æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¿åŠ¨èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†åœ¨å¯æ‰©å±•æ€§æ–¹é¢å®ç°è‡ªä¸»å¤šåŠŸèƒ½æ“æ§æŠ€èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è·¨ä½“ç°æ¨¡ä»¿å­¦ä¹ ç³»ç»Ÿï¼Œåˆ©ç”¨ä»äººç±»å’ŒLocoManï¼ˆä¸€ä¸ªé…å¤‡å¤šç§æ“æ§æ¨¡å¼çš„å››è¶³æœºå™¨äººï¼‰æ”¶é›†çš„æ•°æ®ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€å’Œæ¨¡å—åŒ–çš„äººæœºè§‚å¯Ÿä¸åŠ¨ä½œç©ºé—´çš„é¥æ“ä½œå’Œæ•°æ®æ”¶é›†ç®¡é“ã€‚ä¸ºæœ‰æ•ˆåˆ©ç”¨æ”¶é›†çš„æ•°æ®ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ¨¡å—åŒ–æ¶æ„ï¼Œæ”¯æŒä¸åŒä½“ç°é—´çš„ç»“æ„åŒ–æ¨¡æ€å¯¹é½æ•°æ®çš„å…±åŒè®­ç»ƒå’Œé¢„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªLocoManæœºå™¨äººæ“æ§æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§å®¶åº­ä»»åŠ¡ï¼Œå¹¶éªŒè¯äº†ç³»ç»Ÿåœ¨å…­ä¸ªçœŸå®ä¸–ç•Œæ“æ§ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæ•´ä½“æˆåŠŸç‡æå‡41.9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å››è¶³æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­è‡ªä¸»æ“æ§èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ“æ§æŠ€èƒ½çš„å¯æ‰©å±•æ€§å’Œæœ‰æ•ˆæ€§ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†æœºå™¨äººçš„åº”ç”¨æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§è·¨ä½“ç°æ¨¡ä»¿å­¦ä¹ ç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆäººç±»å’ŒLocoManæœºå™¨äººçš„æ•°æ®ï¼Œæ„å»ºç»Ÿä¸€çš„è§‚å¯Ÿä¸åŠ¨ä½œç©ºé—´ï¼Œä»è€Œæå‡æœºå™¨äººçš„æ“æ§èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¥æ“ä½œå’Œæ•°æ®æ”¶é›†ç®¡é“ï¼Œæ¨¡å—åŒ–è®¾è®¡ä½¿å¾—äººç±»ä¸æœºå™¨äººä¹‹é—´çš„è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´å¾—ä»¥ç»Ÿä¸€ã€‚ç³»ç»Ÿæ”¯æŒä¸åŒä½“ç°é—´çš„å…±åŒè®­ç»ƒå’Œé¢„è®­ç»ƒï¼Œæå‡äº†æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡æ„å»ºäº†LocoManæœºå™¨äººæ“æ§æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†é«˜æ•ˆçš„æ¨¡å—åŒ–æ¶æ„ï¼Œæ”¯æŒè·¨ä½“ç°çš„æ•°æ®å¯¹é½ä¸è®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†æ“æ§æŠ€èƒ½çš„å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†æ¨¡å—åŒ–çš„ç½‘ç»œç»“æ„ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥é€‚åº”ä¸åŒæ“æ§ä»»åŠ¡çš„éœ€æ±‚ï¼Œç¡®ä¿äº†åœ¨ä»…ä½¿ç”¨ä¸€åŠæœºå™¨äººæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»èƒ½å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç³»ç»Ÿåœ¨å…­ä¸ªçœŸå®ä¸–ç•Œæ“æ§ä»»åŠ¡ä¸­ï¼Œæ•´ä½“æˆåŠŸç‡æå‡41.9%ï¼Œåœ¨OODè®¾ç½®ä¸‹æˆåŠŸç‡æå‡è¾¾79.7%ã€‚é¢„è®­ç»ƒä½¿ç”¨äººç±»æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸç‡æå‡38.6%ï¼Œåœ¨OODè®¾ç½®ä¸‹æ›´æ˜¯è¾¾åˆ°82.7%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œæ•‘æ´ä»»åŠ¡ç­‰ã€‚é€šè¿‡æå‡å››è¶³æœºå™¨äººçš„æ“æ§èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Quadrupedal robots have demonstrated impressive locomotion capabilities in complex environments, but equipping them with autonomous versatile manipulation skills in a scalable way remains a significant challenge. In this work, we introduce a cross-embodiment imitation learning system for quadrupedal manipulation, leveraging data collected from both humans and LocoMan, a quadruped equipped with multiple manipulation modes. Specifically, we develop a teleoperation and data collection pipeline, which unifies and modularizes the observation and action spaces of the human and the robot. To effectively leverage the collected data, we propose an efficient modularized architecture that supports co-training and pretraining on structured modality-aligned data across different embodiments. Additionally, we construct the first manipulation dataset for the LocoMan robot, covering various household tasks in both unimanual and bimanual modes, supplemented by a corresponding human dataset. We validate our system on six real-world manipulation tasks, where it achieves an average success rate improvement of 41.9% overall and 79.7% under out-of-distribution (OOD) settings compared to the baseline. Pretraining with human data contributes a 38.6% success rate improvement overall and 82.7% under OOD settings, enabling consistently better performance with only half the amount of robot data. Our code, hardware, and data are open-sourced at: https://human2bots.github.io.

