---
layout: default
title: Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration
---

# Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04040" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04040v1</a>
  <a href="https://arxiv.org/pdf/2506.04040.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04040v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04040v1', 'Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengdong Wu, Sven Kirchner, Nils Purschke, Alois C. Knoll

**åˆ†ç±»**: cs.RO, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**å¤‡æ³¨**: 8 pages; Accepted for publication at the 36th IEEE Intelligent Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è‡ªä¸»è½¦è¾†æ¨ªå‘æ§åˆ¶æ–¹æ³•ä»¥åº”å¯¹æ¨¡å‹ä¸å®Œå–„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªä¸»é©¾é©¶` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `PIDæ§åˆ¶` `è½¦è¾†æ§åˆ¶` `æ™ºèƒ½äº¤é€š` `æ§åˆ¶ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªä¸»é©¾é©¶æ§åˆ¶æ–¹æ³•åœ¨è½¦è¾†æ¨¡å‹ä¸å®Œå–„æ—¶ï¼Œå®¹æ˜“å¯¼è‡´æ§åˆ¶æ€§èƒ½ä¸‹é™ï¼Œå½±å“é©¾é©¶å®‰å…¨æ€§å’Œèˆ’é€‚æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆMPC-PIDå’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ§åˆ¶å™¨ï¼Œåˆ©ç”¨åœ¨çº¿ä¿¡æ¯æå‡æ§åˆ¶æ€§èƒ½ï¼Œç¡®ä¿è½¦è¾†è¾¾åˆ°é¢„æœŸä½ç½®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ§åˆ¶å™¨åœ¨è½¦è¾†ä¿¡æ¯ä¸å®Œæ•´çš„æƒ…å†µä¸‹ä»èƒ½æœ‰æ•ˆå·¥ä½œï¼Œä¸”DRLçš„è®­ç»ƒè¿‡ç¨‹å¾—åˆ°äº†ç¨³å®šåŒ–ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ªå‘æ§åˆ¶æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è‡ªä¸»é©¾é©¶ä¸­ç”±äºæµ‹é‡è¯¯å·®å’Œæ¨¡å‹ç®€åŒ–å¯¼è‡´çš„è½¦è¾†æ§åˆ¶ä¸å‡†ç¡®é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¼ ç»Ÿçš„æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰å’ŒPIDæ§åˆ¶å™¨ï¼Œåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰éƒ¨åˆ†åœ¨çº¿è·å–MPC-PIDçš„åé¦ˆä¿¡æ¯ã€‚é€šè¿‡åœ¨CARLAä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ§åˆ¶å™¨åœ¨è½¦è¾†ä¿¡æ¯ä¸å®Œæ•´çš„æƒ…å†µä¸‹ä»èƒ½å®ç°èˆ’é€‚ã€é«˜æ•ˆå’Œç¨³å¥çš„æ§åˆ¶æ€§èƒ½ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥è‡ªä¸»é©¾é©¶ç³»ç»Ÿçš„å¼€å‘å’Œé›†æˆæä¾›äº†æ½œåœ¨çš„ç®€åŒ–æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è‡ªä¸»é©¾é©¶ä¸­ç”±äºè½¦è¾†æ¨¡å‹ä¸å®Œå–„ï¼ˆå¦‚æµ‹é‡è¯¯å·®å’Œç®€åŒ–æ¨¡å‹ï¼‰å¯¼è‡´çš„æ¨ªå‘æ§åˆ¶é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹è¿™äº›ä¸ç¡®å®šæ€§æ—¶ï¼Œæ§åˆ¶æ€§èƒ½å¾€å¾€æ— æ³•ä¿è¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ§åˆ¶å™¨ç»“åˆäº†ä¼ ç»Ÿçš„æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰å’ŒPIDæ§åˆ¶å™¨ï¼Œä½œä¸ºåŸºç¡€éƒ¨åˆ†ï¼ŒåŒæ—¶å¼•å…¥æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ¥åˆ©ç”¨MPC-PIDçš„å®æ—¶åé¦ˆä¿¡æ¯ï¼Œä»è€Œæå‡æ§åˆ¶çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬MPC-PIDæ§åˆ¶å™¨å’ŒDRLæ¨¡å—ã€‚MPC-PIDè´Ÿè´£åŸºç¡€æ§åˆ¶ï¼Œè€ŒDRLæ¨¡å—åˆ™é€šè¿‡åœ¨çº¿å­¦ä¹ ä¸æ–­ä¼˜åŒ–æ§åˆ¶ç­–ç•¥ï¼Œç¡®ä¿åœ¨ä¿¡æ¯ä¸å®Œæ•´çš„æƒ…å†µä¸‹ä»èƒ½æœ‰æ•ˆæ§åˆ¶è½¦è¾†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸ä¼ ç»Ÿæ§åˆ¶æ–¹æ³•ç›¸ç»“åˆï¼Œåˆ©ç”¨MPC-PIDçš„åé¦ˆä¿¡æ¯æ¥å¢å¼ºDRLçš„å­¦ä¹ è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•åœ¨æ§åˆ¶æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ä¸Šå‡ä¼˜äºä¼ ç»Ÿå•ä¸€æ§åˆ¶ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMPC-PIDæ§åˆ¶å™¨çš„å‚æ•°è®¾ç½®ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å“åº”é€Ÿåº¦ã€‚åŒæ—¶ï¼ŒDRLæ¨¡å—é‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´å¥½åœ°é€‚åº”ä¸åŒçš„é©¾é©¶åœºæ™¯å’Œè½¦è¾†çŠ¶æ€ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒç­–ç•¥ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å’Œæ§åˆ¶ç²¾åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ§åˆ¶å™¨åœ¨è½¦è¾†ä¿¡æ¯ä¸å®Œæ•´çš„æƒ…å†µä¸‹ï¼Œä»èƒ½å®ç°é«˜è¾¾95%çš„ç›®æ ‡ä½ç½®åˆ°è¾¾ç‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ§åˆ¶æ–¹æ³•æå‡äº†çº¦20%çš„æ§åˆ¶ç¨³å®šæ€§ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æå‡æ§åˆ¶ç²¾åº¦å’Œç¨³å®šæ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜è‡ªä¸»é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨å•†ä¸šåŒ–è‡ªåŠ¨é©¾é©¶è§£å†³æ–¹æ¡ˆä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The controller is one of the most important modules in the autonomous driving pipeline, ensuring the vehicle reaches its desired position. In this work, a reinforcement learning based lateral control approach, despite the imperfections in the vehicle models due to measurement errors and simplifications, is presented. Our approach ensures comfortable, efficient, and robust control performance considering the interface between controlling and other modules. The controller consists of the conventional Model Predictive Control (MPC)-PID part as the basis and the demonstrator, and the Deep Reinforcement Learning (DRL) part which leverages the online information from the MPC-PID part. The controller's performance is evaluated in CARLA using the ground truth of the waypoints as inputs. Experimental results demonstrate the effectiveness of the controller when vehicle information is incomplete, and the training of DRL can be stabilized with the demonstration part. These findings highlight the potential to reduce development and integration efforts for autonomous driving pipelines in the future.

