---
layout: default
title: RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics
---

# RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04308" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04308v3</a>
  <a href="https://arxiv.org/pdf/2506.04308.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04308v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04308v3', 'RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, Shanghang Zhang

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04 (æ›´æ–°: 2025-10-25)

**å¤‡æ³¨**: Accepted by NeurIPS 2025. Project page: https://zhoues.github.io/RoboRefer/

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://zhoues.github.io/RoboRefer)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoboReferä»¥è§£å†³æœºå™¨äººç©ºé—´æŒ‡ç§°ä¸æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `ç©ºé—´æŒ‡ç§°` `è§†è§‰è¯­è¨€æ¨¡å‹` `ä¸‰ç»´ç†è§£` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæŠ€æœ¯` `å¤šæ­¥æ¨ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç†è§£å¤æ‚ä¸‰ç»´åœºæ™¯å’ŒåŠ¨æ€æ¨ç†æŒ‡ç¤ºä½ç½®æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³æœºå™¨äººäº¤äº’éœ€æ±‚ã€‚
2. æå‡ºRoboReferï¼Œé€šè¿‡é›†æˆæ·±åº¦ç¼–ç å™¨å’Œå¼ºåŒ–å¾®è°ƒï¼Œæå‡ç©ºé—´ç†è§£å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRoboReferåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šä¸ªåŸºçº¿æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç©ºé—´æŒ‡ç§°æ˜¯å…·èº«æœºå™¨äººä¸ä¸‰ç»´ç‰©ç†ä¸–ç•Œäº’åŠ¨çš„åŸºæœ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯å¼ºå¤§çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç°æœ‰æ–¹æ³•ä»æ— æ³•å‡†ç¡®ç†è§£å¤æ‚çš„ä¸‰ç»´åœºæ™¯å¹¶åŠ¨æ€æ¨ç†æŒ‡ç¤ºä½ç½®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºRoboReferï¼Œä¸€ä¸ªå…·æœ‰ä¸‰ç»´æ„ŸçŸ¥èƒ½åŠ›çš„VLMï¼Œé€šè¿‡é›†æˆè§£è€¦çš„æ·±åº¦ç¼–ç å™¨å®ç°ç²¾ç¡®çš„ç©ºé—´ç†è§£ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ¨è¿›å¤šæ­¥ç©ºé—´æ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†RefSpatialï¼Œä¸€ä¸ªåŒ…å«2000ä¸‡ä¸ªé—®ç­”å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–31ç§ç©ºé—´å…³ç³»ï¼Œæ”¯æŒå¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡ç›‘ç£å¾®è°ƒçš„RoboReferåœ¨ç©ºé—´ç†è§£ä¸Šè¾¾åˆ°äº†89.6%çš„æˆåŠŸç‡ï¼Œç»è¿‡å¼ºåŒ–å¾®è°ƒçš„RoboReferåœ¨RefSpatial-Benchä¸Šè¶…è¶Šæ‰€æœ‰åŸºçº¿ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”Gemini-2.5-Proé«˜å‡º17.4%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨å¤æ‚ä¸‰ç»´åœºæ™¯ä¸­è¿›è¡Œç©ºé—´æŒ‡ç§°å’Œæ¨ç†çš„èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç†è§£å’Œæ¨ç†æ–¹é¢çš„å±€é™æ€§å¯¼è‡´æœºå™¨äººæ— æ³•æœ‰æ•ˆæ‰§è¡Œä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRoboReferé€šè¿‡é›†æˆè§£è€¦çš„æ·±åº¦ç¼–ç å™¨å®ç°ç²¾ç¡®çš„ç©ºé—´ç†è§£ï¼Œå¹¶é‡‡ç”¨å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ¥æå‡å¤šæ­¥ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œæ—¨åœ¨å¢å¼ºæœºå™¨äººå¯¹ç¯å¢ƒçš„ç†è§£å’Œäº’åŠ¨èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoboReferçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œæ·±åº¦ç¼–ç å™¨çš„è®­ç»ƒï¼Œä»¥å®ç°ç©ºé—´ç†è§£ï¼›å…¶æ¬¡æ˜¯é€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¿›è¡Œå¤šæ­¥æ¨ç†çš„è®­ç»ƒï¼Œä½¿ç”¨é’ˆå¯¹ç©ºé—´æŒ‡ç§°ä»»åŠ¡çš„åº¦é‡æ•æ„Ÿè¿‡ç¨‹å¥–åŠ±å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šRoboReferçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†RefSpatialæ•°æ®é›†ï¼ŒåŒ…å«2000ä¸‡ä¸ªé—®ç­”å¯¹å’Œ31ç§ç©ºé—´å…³ç³»ï¼Œæ”¯æŒå¤æ‚çš„æ¨ç†è¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå¥–åŠ±æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–ç©ºé—´æŒ‡ç§°ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒæ—¶ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šæ­¥æ¨ç†çš„å¤æ‚æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡ç›‘ç£å¾®è°ƒçš„RoboReferåœ¨ç©ºé—´ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†89.6%çš„æˆåŠŸç‡ï¼Œè€Œç»è¿‡å¼ºåŒ–å¾®è°ƒçš„RoboReferåœ¨RefSpatial-Benchä¸Šè¶…è¶Šæ‰€æœ‰åŸºçº¿ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”Gemini-2.5-Proé«˜å‡º17.4%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RoboReferçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æœåŠ¡æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹ç¯å¢ƒçš„ç†è§£èƒ½åŠ›ï¼ŒRoboReferèƒ½å¤Ÿæ›´å¥½åœ°æ‰§è¡Œå¤æ‚çš„åŠ¨æ€ä»»åŠ¡ï¼Œæ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes. Please see the project page at https://zhoues.github.io/RoboRefer.

