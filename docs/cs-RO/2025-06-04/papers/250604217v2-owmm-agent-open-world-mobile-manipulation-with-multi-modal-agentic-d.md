---
layout: default
title: OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis
---

# OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04217" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04217v2</a>
  <a href="https://arxiv.org/pdf/2506.04217.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04217v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04217v2', 'OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junting Chen, Haotian Liang, Lingxiao Du, Weiyun Wang, Mengkang Hu, Yao Mu, Wenhai Wang, Jifeng Dai, Ping Luo, Wenqi Shao, Lin Shao

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04 (æ›´æ–°: 2025-06-21)

**å¤‡æ³¨**: 9 pages of main content, 19 pages in total

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HHYHRHY/OWMM-Agent)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOWMM-Agentä»¥è§£å†³å¼€æ”¾ä¸–ç•Œç§»åŠ¨æ“æ§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼€æ”¾ä¸–ç•Œç§»åŠ¨æ“æ§` `å¤šæ¨¡æ€ä»£ç†` `æœºå™¨äººæ§åˆ¶` `åœºæ™¯ç†è§£` `æŒ‡ä»¤å¾®è°ƒ` `æ•°æ®åˆæˆ` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼€æ”¾ä¸–ç•Œç§»åŠ¨æ“æ§ä»»åŠ¡é¢ä¸´æ³›åŒ–èƒ½åŠ›ä¸è¶³å’Œå¤æ‚å†³ç­–ä¸æ§åˆ¶æ•´åˆçš„æŒ‘æˆ˜ã€‚
2. æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€ä»£ç†æ¶æ„ï¼Œé€šè¿‡ç»´æŠ¤åœºæ™¯å¸§å’Œä»£ç†çŠ¶æ€æ¥ä¼˜åŒ–å†³ç­–è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å‡½æ•°è°ƒç”¨å®ç°æœºå™¨äººæ§åˆ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOWMM-VLMæ¨¡å‹åœ¨ä¸GPT-4oç­‰å…¶ä»–åŸºç¡€æ¨¡å‹çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å¼ºå¤§çš„é›¶-shotæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¯¼èˆªã€æ“æ§å’Œè§†è§‰æ¨¡å‹çš„å¿«é€Ÿè¿›å±•ï¼Œç§»åŠ¨æ“æ§å™¨åœ¨è®¸å¤šä¸“ä¸šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå¼€æ”¾ä¸–ç•Œç§»åŠ¨æ“æ§ï¼ˆOWMMï¼‰ä»»åŠ¡ä¾ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¯¹å¼€æ”¾å¼æŒ‡ä»¤å’Œç¯å¢ƒè¿›è¡Œæ³›åŒ–çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å¤æ‚æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€ä»£ç†æ¶æ„ï¼Œè¯¥æ¶æ„ç»´æŠ¤å¤šè§†è§’åœºæ™¯å¸§å’Œä»£ç†çŠ¶æ€ä»¥è¿›è¡Œå†³ç­–ï¼Œå¹¶é€šè¿‡å‡½æ•°è°ƒç”¨æ§åˆ¶æœºå™¨äººã€‚æ­¤å¤–ï¼Œé’ˆå¯¹é¢†åŸŸè½¬ç§»å¸¦æ¥çš„å¹»è§‰é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ä»£ç†æ•°æ®åˆæˆç®¡é“ï¼Œä»¥é€‚åº”VLMæ¨¡å‹å¹¶è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„OWMM-VLMæ¨¡å‹åœ¨å…¨çƒåœºæ™¯ç†è§£ã€æœºå™¨äººçŠ¶æ€è·Ÿè¸ªå’Œå¤šæ¨¡æ€åŠ¨ä½œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼€æ”¾ä¸–ç•Œç§»åŠ¨æ“æ§ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³å’Œé«˜ä½å±‚å†³ç­–æ•´åˆçš„å¤æ‚æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¼€æ”¾å¼æŒ‡ä»¤å’Œç¯å¢ƒæ—¶è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥å®ç°æœ‰æ•ˆçš„æœºå™¨äººæ§åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€ä»£ç†æ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶ç»´æŠ¤å¤šè§†è§’åœºæ™¯ä¿¡æ¯å’Œä»£ç†çŠ¶æ€ï¼Œä»è€Œä¼˜åŒ–å†³ç­–è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å‡½æ•°è°ƒç”¨å®ç°å¯¹æœºå™¨äººçš„æ§åˆ¶ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¤æ‚ç¯å¢ƒå’ŒæŒ‡ä»¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šæ¨¡æ€è¾“å…¥æ¨¡å—ã€å†³ç­–æ¨¡å—å’Œæ§åˆ¶æ¨¡å—ã€‚å¤šæ¨¡æ€è¾“å…¥æ¨¡å—è´Ÿè´£æ¥æ”¶å’Œå¤„ç†æ¥è‡ªä¸åŒè§†è§’çš„åœºæ™¯ä¿¡æ¯ï¼Œå†³ç­–æ¨¡å—åŸºäºè¿™äº›ä¿¡æ¯å’Œä»£ç†çŠ¶æ€è¿›è¡Œå†³ç­–ï¼Œæ§åˆ¶æ¨¡å—åˆ™é€šè¿‡å‡½æ•°è°ƒç”¨å®ç°å¯¹æœºå™¨äººçš„å…·ä½“æ“ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†ä»£ç†æ•°æ®åˆæˆç®¡é“ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”VLMæ¨¡å‹å¹¶è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä½¿å…¶æˆä¸ºä¸“é—¨é’ˆå¯¹ç§»åŠ¨æ“æ§å™¨çš„åŸºç¡€æ¨¡å‹ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å…¨çƒåœºæ™¯ç†è§£å’Œå¤šæ¨¡æ€åŠ¨ä½œç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¤šæ¨¡æ€è¾“å…¥çš„èåˆæ•ˆæœï¼Œå¹¶é€šè¿‡ç²¾ç»†è°ƒæ•´ç½‘ç»œç»“æ„æ¥æå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOWMM-VLMæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºGPT-4oç­‰åŸºç¡€æ¨¡å‹ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„é›¶-shotæ³›åŒ–èƒ½åŠ›ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€æœåŠ¡æœºå™¨äººå’Œå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚é€šè¿‡æå‡ç§»åŠ¨æ“æ§å™¨åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼ŒOWMM-Agentæœ‰æœ›åœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜æ•ˆçš„ä»»åŠ¡æ‰§è¡Œï¼Œæ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„å‘å±•ä¸æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid progress of navigation, manipulation, and vision models has made mobile manipulators capable in many specialized tasks. However, the open-world mobile manipulation (OWMM) task remains a challenge due to the need for generalization to open-ended instructions and environments, as well as the systematic complexity to integrate high-level decision making with low-level robot control based on both global scene understanding and current agent state. To address this complexity, we propose a novel multi-modal agent architecture that maintains multi-view scene frames and agent states for decision-making and controls the robot by function calling. A second challenge is the hallucination from domain shift. To enhance the agent performance, we further introduce an agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our task domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM as the first dedicated foundation model for mobile manipulators with global scene understanding, robot state tracking, and multi-modal action generation in a unified model. Through experiments, we demonstrate that our model achieves SOTA performance compared to other foundation models including GPT-4o and strong zero-shot generalization in real world. The project page is at https://github.com/HHYHRHY/OWMM-Agent

