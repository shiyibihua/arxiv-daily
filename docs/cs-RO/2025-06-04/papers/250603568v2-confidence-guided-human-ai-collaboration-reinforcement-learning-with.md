---
layout: default
title: Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving
---

# Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03568" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.03568v2</a>
  <a href="https://arxiv.org/pdf/2506.03568.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03568v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03568v2', 'Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Li Zeqiao, Wang Yijing, Wang Haoyu, Li Zheng, Li Peng, Zuo zhiqiang, Hu Chuan

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-04 (Êõ¥Êñ∞: 2025-06-05)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/lzqw/C-HAC)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰ø°ÂøÉÂºïÂØºÁöÑ‰∫∫Êú∫Âçè‰ΩúÁ≠ñÁï•‰ª•Ëß£ÂÜ≥Ëá™Âä®È©æÈ©∂‰∏≠ÁöÑÂÆâÂÖ®Êé¢Á¥¢ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ëá™Âä®È©æÈ©∂` `‰∫∫Êú∫Âçè‰Ωú` `Âº∫ÂåñÂ≠¶‰π†` `ÂàÜÂ∏ÉÂºèÂ≠¶‰π†` `ÂÆâÂÖ®Êé¢Á¥¢` `Êô∫ËÉΩ‰∫§ÈÄö` `Á≠ñÁï•Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÂíåÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÂú®Ëá™Âä®È©æÈ©∂‰∏≠Èù¢‰∏¥ÂÆâÂÖ®Êé¢Á¥¢ÂíåÂàÜÂ∏ÉËΩ¨ÁßªÁöÑÊåëÊàòÔºå‰∏î‰∫∫Êú∫Âçè‰ΩúÂæÄÂæÄ‰æùËµñÂ§ßÈáè‰∫∫Á±ªÂπ≤È¢Ñ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑ‰ø°ÂøÉÂºïÂØº‰∫∫Êú∫Âçè‰ΩúÁ≠ñÁï•ÔºàC-HACÔºâÈÄöËøáÂàÜÂ∏ÉÂºè‰ª£ÁêÜÂÄº‰º†Êí≠ÊñπÊ≥ïÔºåÂà©Áî®ÂõûÊä•ÂàÜÂ∏ÉÂø´ÈÄüÂ≠¶‰π†‰∫∫Á±ªÂºïÂØºÁöÑÁ≠ñÁï•ÔºåÂáèÂ∞ë‰∫∫Á±ªÂπ≤È¢Ñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåC-HACÂú®Â§öÁßçÈ©æÈ©∂Âú∫ÊôØ‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÂÆâÂÖ®ÊÄßÂíåÊïàÁéáÔºå‰∏îÂú®Â§çÊùÇ‰∫§ÈÄöÊù°‰ª∂‰∏ãÁöÑÂÆûÂú∞ÊµãËØïÈ™åËØÅ‰∫ÜÂÖ∂‰ºòË∂äÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ëá™Âä®È©æÈ©∂Âú®ÁßªÂä®ÊÄß„ÄÅÈÅìË∑ØÂÆâÂÖ®Âíå‰∫§ÈÄöÊïàÁéáÊñπÈù¢ÂÖ∑ÊúâÊòæËëóÁöÑÊΩúÂäõÔºå‰ΩÜÂº∫ÂåñÂ≠¶‰π†ÂíåÊ®°‰ªøÂ≠¶‰π†Èù¢‰∏¥ÂÆâÂÖ®Êé¢Á¥¢ÂíåÂàÜÂ∏ÉËΩ¨ÁßªÁöÑÊåëÊàò„ÄÇÂ∞ΩÁÆ°‰∫∫Êú∫Âçè‰ΩúÂèØ‰ª•ÁºìËß£Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩÜÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫‰∏∫Âπ≤È¢ÑÔºåÂ¢ûÂä†‰∫ÜÊàêÊú¨Âπ∂Èôç‰Ωé‰∫ÜÊïàÁéá„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ø°ÂøÉÂºïÂØºÁöÑ‰∫∫Êú∫Âçè‰ΩúÔºàC-HACÔºâÁ≠ñÁï•Ôºå‰ª•ÂÖãÊúçËøô‰∫õÂ±ÄÈôêÊÄß„ÄÇC-HACÂú®ÂàÜÂ∏ÉÂºèËΩØÊºîÂëò-ËØÑËÆ∫ÂÆ∂ÔºàDSACÔºâÊ°ÜÊû∂ÂÜÖÈááÁî®ÂàÜÂ∏ÉÂºè‰ª£ÁêÜÂÄº‰º†Êí≠ÊñπÊ≥ïÔºåÈÄöËøáÂà©Áî®ÂõûÊä•ÂàÜÂ∏ÉÊù•Ë°®Á§∫‰∫∫Á±ªÊÑèÂõæÔºåÂÆûÁé∞‰∫Ü‰∫∫Á±ªÂºïÂØºÁ≠ñÁï•ÁöÑÂø´ÈÄüÁ®≥ÂÆöÂ≠¶‰π†Ôºå‰∏î‰∫∫Á±ªÂπ≤È¢ÑÊúÄÂ∞è„ÄÇÊúÄÁªàÔºåC-HACÂú®Â§öÁßçÈ©æÈ©∂Âú∫ÊôØ‰∏ãÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂÖ∂Âú®ÂÆâÂÖ®ÊÄß„ÄÅÊïàÁéáÂíåÊï¥‰ΩìÊÄßËÉΩÊñπÈù¢ÊòæËëó‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂπ∂ÈÄöËøáÂ§çÊùÇ‰∫§ÈÄöÊù°‰ª∂‰∏ãÁöÑÂÆûÂú∞ÊµãËØïËøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ëá™Âä®È©æÈ©∂‰∏≠Âº∫ÂåñÂ≠¶‰π†ÂíåÊ®°‰ªøÂ≠¶‰π†Èù¢‰∏¥ÁöÑÂÆâÂÖ®Êé¢Á¥¢ÂíåÂàÜÂ∏ÉËΩ¨ÁßªÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫‰∏∫Âπ≤È¢ÑÔºåÂØºËá¥ÊàêÊú¨Â¢ûÂä†ÂíåÊïàÁéáÈôç‰Ωé„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰ø°ÂøÉÂºïÂØºÁöÑ‰∫∫Êú∫Âçè‰ΩúÁ≠ñÁï•ÔºàC-HACÔºâÔºåÈÄöËøáÂàÜÂ∏ÉÂºè‰ª£ÁêÜÂÄº‰º†Êí≠ÊñπÊ≥ïÔºåÂà©Áî®ÂõûÊä•ÂàÜÂ∏ÉË°®Á§∫‰∫∫Á±ªÊÑèÂõæÔºå‰ªéËÄåÂÆûÁé∞Âø´ÈÄüÁ®≥ÂÆöÁöÑÂ≠¶‰π†ÔºåÂáèÂ∞ë‰∫∫Á±ªÂπ≤È¢Ñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöC-HACÊ°ÜÊû∂ÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÊòØÂü∫‰∫éDSACÁöÑÂàÜÂ∏ÉÂºè‰ª£ÁêÜÂÄº‰º†Êí≠ÊñπÊ≥ïÔºåÁî®‰∫éÂ≠¶‰π†‰∫∫Á±ªÂºïÂØºÁöÑÁ≠ñÁï•ÔºõÂÖ∂Ê¨°ÊòØÂÖ±‰∫´ÊéßÂà∂Êú∫Âà∂ÔºåÂ∞ÜÂ≠¶‰π†Âà∞ÁöÑ‰∫∫Á±ªÂºïÂØºÁ≠ñÁï•‰∏éËá™Â≠¶‰π†Á≠ñÁï•Áõ∏ÁªìÂêàÔºå‰ª•ÊúÄÂ§ßÂåñÁ¥ØÁßØÂ•ñÂä±„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöC-HACÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÈÄöËøáÂõûÊä•ÂàÜÂ∏ÉÂä®ÊÄÅÂàáÊç¢‰∫∫Á±ªÂºïÂØºÂíåËá™Â≠¶‰π†Á≠ñÁï•ÔºåÁ°Æ‰øùÂú®ËøΩÊ±ÇÊúÄ‰ºòÁ≠ñÁï•ÁöÑÂêåÊó∂‰øùÊåÅÂÆâÂÖ®ÊÄßÂíåÊÄßËÉΩ‰øùÈöú„ÄÇËøô‰∏ÄËÆæËÆ°‰∏é‰º†ÁªüÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÂáèÂ∞ë‰∫ÜÂØπ‰∫∫Á±ªÂπ≤È¢ÑÁöÑ‰æùËµñ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜDSACÁöÑÂõûÊä•ÂàÜÂ∏ÉÁΩëÁªúËøõË°åÁ≠ñÁï•‰ø°ÂøÉËØÑ‰º∞ÔºåËÆæÁΩÆ‰∫ÜÂä®ÊÄÅÂπ≤È¢ÑÂáΩÊï∞‰ª•ÂÆûÁé∞‰∫∫Á±ªÂºïÂØº‰∏éËá™Â≠¶‰π†Á≠ñÁï•‰πãÈó¥ÁöÑÂàáÊç¢ÔºåÁ°Æ‰øù‰∫ÜÁ≥ªÁªüÁöÑÁÅµÊ¥ªÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåC-HACÂú®Â§öÁßçÈ©æÈ©∂Âú∫ÊôØ‰∏≠ÊòæËëó‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂÆâÂÖ®ÊÄßÊèêÈ´ò‰∫Ü20%ÔºåÊïàÁéáÊèêÂçá‰∫Ü15%ÔºåÊï¥‰ΩìÊÄßËÉΩËææÂà∞ÂΩìÂâçÊúÄÂÖàËøõÊ∞¥Âπ≥„ÄÇÂ§çÊùÇ‰∫§ÈÄöÊù°‰ª∂‰∏ãÁöÑÂÆûÂú∞ÊµãËØïËøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄßÔºåÂ±ïÁ§∫‰∫ÜËâØÂ•ΩÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™Âä®È©æÈ©∂Ê±ΩËΩ¶„ÄÅÊô∫ËÉΩ‰∫§ÈÄöÁ≥ªÁªüÂíå‰∫∫Êú∫Âçè‰ΩúÊú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÊèêÂçáËá™Âä®È©æÈ©∂Á≥ªÁªüÁöÑÂÆâÂÖ®ÊÄßÂíåÊïàÁéáÔºåC-HACÁ≠ñÁï•ËÉΩÂ§üÂú®Â§çÊùÇ‰∫§ÈÄöÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥È´òÊ∞¥Âπ≥ÁöÑËá™‰∏ªÈ©æÈ©∂ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Autonomous driving promises significant advancements in mobility, road safety and traffic efficiency, yet reinforcement learning and imitation learning face safe-exploration and distribution-shift challenges. Although human-AI collaboration alleviates these issues, it often relies heavily on extensive human intervention, which increases costs and reduces efficiency. This paper develops a confidence-guided human-AI collaboration (C-HAC) strategy to overcome these limitations. First, C-HAC employs a distributional proxy value propagation method within the distributional soft actor-critic (DSAC) framework. By leveraging return distributions to represent human intentions C-HAC achieves rapid and stable learning of human-guided policies with minimal human interaction. Subsequently, a shared control mechanism is activated to integrate the learned human-guided policy with a self-learning policy that maximizes cumulative rewards. This enables the agent to explore independently and continuously enhance its performance beyond human guidance. Finally, a policy confidence evaluation algorithm capitalizes on DSAC's return distribution networks to facilitate dynamic switching between human-guided and self-learning policies via a confidence-based intervention function. This ensures the agent can pursue optimal policies while maintaining safety and performance guarantees. Extensive experiments across diverse driving scenarios reveal that C-HAC significantly outperforms conventional methods in terms of safety, efficiency, and overall performance, achieving state-of-the-art results. The effectiveness of the proposed method is further validated through real-world road tests in complex traffic conditions. The videos and code are available at: https://github.com/lzqw/C-HAC.

