---
layout: default
title: Object-centric 3D Motion Field for Robot Learning from Human Videos
---

# Object-centric 3D Motion Field for Robot Learning from Human Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04227" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04227v1</a>
  <a href="https://arxiv.org/pdf/2506.04227.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04227v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04227v1', 'Object-centric 3D Motion Field for Robot Learning from Human Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhao-Heng Yin, Sherry Yang, Pieter Abbeel

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**å¤‡æ³¨**: Project: https://zhaohengyin.github.io/3DMF

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹è±¡ä¸­å¿ƒçš„3Dè¿åŠ¨åœºä»¥è§£å†³æœºå™¨äººä»äººç±»è§†é¢‘å­¦ä¹ çš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æœºå™¨äººå­¦ä¹ ` `3Dè¿åŠ¨åœº` `è§†é¢‘ç†è§£` `åŠ¨ä½œè¡¨ç¤º` `å»å™ªä¼°è®¡` `ç­–ç•¥æ³›åŒ–` `è·¨ä½“ç°è½¬ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä»è§†é¢‘ä¸­æå–åŠ¨ä½œçŸ¥è¯†æ—¶é¢ä¸´å»ºæ¨¡å¤æ‚æ€§å’Œä¿¡æ¯ä¸¢å¤±ç­‰æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºä½¿ç”¨å¯¹è±¡ä¸­å¿ƒçš„3Dè¿åŠ¨åœºæ¥è¡¨ç¤ºåŠ¨ä½œï¼Œå¹¶è®¾è®¡äº†å»å™ªä¼°è®¡å™¨å’Œå¯†é›†é¢„æµ‹æ¶æ„ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–¹æ³•åœ¨3Dè¿åŠ¨ä¼°è®¡è¯¯å·®ä¸Šé™ä½è¶…è¿‡50%ï¼ŒæˆåŠŸç‡è¾¾åˆ°55%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»äººç±»è§†é¢‘ä¸­å­¦ä¹ æœºå™¨äººæ§åˆ¶ç­–ç•¥æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æœºå™¨äººå­¦ä¹ æ‰©å±•æ–¹å‘ã€‚ç„¶è€Œï¼Œå¦‚ä½•ä»è§†é¢‘ä¸­æå–åŠ¨ä½œçŸ¥è¯†ä»¥è¿›è¡Œç­–ç•¥å­¦ä¹ ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŠ¨ä½œè¡¨ç¤ºæ–¹æ³•å¦‚è§†é¢‘å¸§ã€åƒç´ æµå’Œç‚¹äº‘æµå­˜åœ¨å»ºæ¨¡å¤æ‚æ€§æˆ–ä¿¡æ¯ä¸¢å¤±ç­‰å›ºæœ‰å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨å¯¹è±¡ä¸­å¿ƒçš„3Dè¿åŠ¨åœºæ¥è¡¨ç¤ºåŠ¨ä½œï¼Œå¹¶å±•ç¤ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶æ¥ä»è§†é¢‘ä¸­æå–è¿™ç§è¡¨ç¤ºä»¥å®ç°é›¶-shotæ§åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªæ–°é¢–çš„ç»„ä»¶ï¼šä¸€æ˜¯è®­ç»ƒä¸€ä¸ªâ€œå»å™ªâ€3Dè¿åŠ¨åœºä¼°è®¡å™¨çš„è®­ç»ƒç®¡é“ï¼Œä»¥ç¨³å¥åœ°ä»äººç±»è§†é¢‘ä¸­æå–ç»†è‡´çš„å¯¹è±¡3Dè¿åŠ¨ï¼›äºŒæ˜¯ä¸€ä¸ªå¯†é›†çš„å¯¹è±¡ä¸­å¿ƒ3Dè¿åŠ¨åœºé¢„æµ‹æ¶æ„ï¼Œä¿ƒè¿›è·¨ä½“ç°è½¬ç§»å’Œç­–ç•¥å¯¹èƒŒæ™¯çš„æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œè®¾ç½®ä¸­å°†3Dè¿åŠ¨ä¼°è®¡è¯¯å·®é™ä½äº†50%ä»¥ä¸Šï¼ŒæˆåŠŸç‡è¾¾åˆ°55%ï¼Œè€Œå…ˆå‰æ–¹æ³•çš„æˆåŠŸç‡ä½äº10%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»äººç±»è§†é¢‘ä¸­æå–æœ‰æ•ˆåŠ¨ä½œè¡¨ç¤ºä»¥è¿›è¡Œæœºå™¨äººæ§åˆ¶ç­–ç•¥å­¦ä¹ çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚è§†é¢‘å¸§å’Œç‚¹äº‘æµå­˜åœ¨å»ºæ¨¡å¤æ‚æ€§å’Œä¿¡æ¯ä¸¢å¤±çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºä½¿ç”¨å¯¹è±¡ä¸­å¿ƒçš„3Dè¿åŠ¨åœºä½œä¸ºåŠ¨ä½œè¡¨ç¤ºï¼Œé€šè¿‡å»å™ªä¼°è®¡å™¨æå–ç»†è‡´çš„å¯¹è±¡è¿åŠ¨ï¼Œå¢å¼ºç­–ç•¥å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯â€œå»å™ªâ€3Dè¿åŠ¨åœºä¼°è®¡å™¨ï¼ŒäºŒæ˜¯å¯†é›†çš„å¯¹è±¡ä¸­å¿ƒ3Dè¿åŠ¨åœºé¢„æµ‹æ¶æ„ï¼ŒäºŒè€…ååŒå·¥ä½œä»¥å®ç°é«˜æ•ˆçš„åŠ¨ä½œè¡¨ç¤ºæå–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å»å™ªä¼°è®¡å™¨å’Œå¯†é›†é¢„æµ‹æ¶æ„ï¼Œå‰è€…æé«˜äº†è¿åŠ¨æå–çš„é²æ£’æ€§ï¼Œåè€…ä¿ƒè¿›äº†è·¨ä½“ç°è½¬ç§»å’Œç­–ç•¥æ³›åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å»å™ªä¼°è®¡å™¨ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è¿åŠ¨åœºçš„ç²¾åº¦ï¼›å¯†é›†é¢„æµ‹æ¶æ„åˆ™é€šè¿‡å¤šå±‚ç½‘ç»œç»“æ„å®ç°å¯¹å¤æ‚èƒŒæ™¯çš„é€‚åº”æ€§ã€‚å®éªŒä¸­è¿˜å¯¹å‚æ•°è¿›è¡Œäº†ç»†è‡´è°ƒä¼˜ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•åœ¨3Dè¿åŠ¨ä¼°è®¡è¯¯å·®ä¸Šé™ä½è¶…è¿‡50%ï¼ŒæˆåŠŸç‡è¾¾åˆ°55%ï¼Œè€Œå…ˆå‰æ–¹æ³•çš„æˆåŠŸç‡ä»…ä¸º10%ä»¥ä¸‹ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç»†ç²’åº¦æ“ä½œæŠ€èƒ½çš„å­¦ä¹ ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€è‡ªåŠ¨åŒ–åˆ¶é€ å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡ä»äººç±»è§†é¢‘ä¸­å­¦ä¹ ï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´çµæ´»åœ°é€‚åº”å¤æ‚ç¯å¢ƒï¼Œæå‡å…¶è‡ªä¸»æ“ä½œèƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½åœ¨å®¶åº­æœåŠ¡ã€åŒ»ç–—è¾…åŠ©ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning robot control policies from human videos is a promising direction for scaling up robot learning. However, how to extract action knowledge (or action representations) from videos for policy learning remains a key challenge. Existing action representations such as video frames, pixelflow, and pointcloud flow have inherent limitations such as modeling complexity or loss of information. In this paper, we propose to use object-centric 3D motion field to represent actions for robot learning from human videos, and present a novel framework for extracting this representation from videos for zero-shot control. We introduce two novel components in its implementation. First, a novel training pipeline for training a ''denoising'' 3D motion field estimator to extract fine object 3D motions from human videos with noisy depth robustly. Second, a dense object-centric 3D motion field prediction architecture that favors both cross-embodiment transfer and policy generalization to background. We evaluate the system in real world setups. Experiments show that our method reduces 3D motion estimation error by over 50% compared to the latest method, achieve 55% average success rate in diverse tasks where prior approaches fail~($\lesssim 10$\%), and can even acquire fine-grained manipulation skills like insertion.

