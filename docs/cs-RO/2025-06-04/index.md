---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-06-04
---

# cs.ROï¼ˆ2025-06-04ï¼‰

ğŸ“Š å…± **20** ç¯‡è®ºæ–‡
 | ğŸ”— **7** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (13 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250604040v1-autonomous-vehicle-lateral-control-using-deep-reinforcement-learning.html">Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration</a></td>
  <td>æå‡ºåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è‡ªä¸»è½¦è¾†æ¨ªå‘æ§åˆ¶æ–¹æ³•ä»¥åº”å¯¹æ¨¡å‹ä¸å®Œå–„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04040v1" data-paper-url="./papers/250604040v1-autonomous-vehicle-lateral-control-using-deep-reinforcement-learning.html" onclick="toggleFavorite(this, '2506.04040v1', 'Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250603856v1-phase-based-nonlinear-model-predictive-control-for-humanoid-walking-.html">Phase-based Nonlinear Model Predictive Control for Humanoid Walking Stabilization with Single and Double Support Time Adjustments</a></td>
  <td>æå‡ºåŸºäºç›¸ä½çš„éçº¿æ€§æ¨¡å‹é¢„æµ‹æ§åˆ¶ä»¥ä¼˜åŒ–äººå½¢æœºå™¨äººæ­¥æ€ç¨³å®šæ€§</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">MPC</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03856v1" data-paper-url="./papers/250603856v1-phase-based-nonlinear-model-predictive-control-for-humanoid-walking-.html" onclick="toggleFavorite(this, '2506.03856v1', 'Phase-based Nonlinear Model Predictive Control for Humanoid Walking Stabilization with Single and Double Support Time Adjustments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250604217v2-owmm-agent-open-world-mobile-manipulation-with-multi-modal-agentic-d.html">OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis</a></td>
  <td>æå‡ºOWMM-Agentä»¥è§£å†³å¼€æ”¾ä¸–ç•Œç§»åŠ¨æ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span> <span class="paper-tag">scene understanding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04217v2" data-paper-url="./papers/250604217v2-owmm-agent-open-world-mobile-manipulation-with-multi-modal-agentic-d.html" onclick="toggleFavorite(this, '2506.04217v2', 'OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250603574v1-switchvla-execution-aware-task-switching-for-vision-language-action-.html">SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models</a></td>
  <td>æå‡ºSwitchVLAä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„ä»»åŠ¡åˆ‡æ¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03574v1" data-paper-url="./papers/250603574v1-switchvla-execution-aware-task-switching-for-vision-language-action-.html" onclick="toggleFavorite(this, '2506.03574v1', 'SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250604120v2-splatting-physical-scenes-end-to-end-real-to-sim-from-imperfect-robo.html">Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot Data</a></td>
  <td>æå‡ºä¸€ç§æ–°æ¡†æ¶ä»¥è§£å†³æœºå™¨äººæ•°æ®ä¸å®Œç¾å¸¦æ¥çš„çœŸå®åˆ°ä»¿çœŸé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">bi-manual</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04120v2" data-paper-url="./papers/250604120v2-splatting-physical-scenes-end-to-end-real-to-sim-from-imperfect-robo.html" onclick="toggleFavorite(this, '2506.04120v2', 'Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250603760v1-understanding-physical-properties-of-unseen-deformable-objects-by-le.html">Understanding Physical Properties of Unseen Deformable Objects by Leveraging Large Language Models and Robot Actions</a></td>
  <td>æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ä»¥ç†è§£æœªè§å˜å½¢ç‰©ä½“çš„ç‰©ç†å±æ€§</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">task and motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03760v1" data-paper-url="./papers/250603760v1-understanding-physical-properties-of-unseen-deformable-objects-by-le.html" onclick="toggleFavorite(this, '2506.03760v1', 'Understanding Physical Properties of Unseen Deformable Objects by Leveraging Large Language Models and Robot Actions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250604227v1-object-centric-3d-motion-field-for-robot-learning-from-human-videos.html">Object-centric 3D Motion Field for Robot Learning from Human Videos</a></td>
  <td>æå‡ºå¯¹è±¡ä¸­å¿ƒçš„3Dè¿åŠ¨åœºä»¥è§£å†³æœºå™¨äººä»äººç±»è§†é¢‘å­¦ä¹ çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">cross-embodiment</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04227v1" data-paper-url="./papers/250604227v1-object-centric-3d-motion-field-for-robot-learning-from-human-videos.html" onclick="toggleFavorite(this, '2506.04227v1', 'Object-centric 3D Motion Field for Robot Learning from Human Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250604147v4-slac-simulation-pretrained-latent-action-space-for-whole-body-real-w.html">SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL</a></td>
  <td>æå‡ºSLACä»¥è§£å†³é«˜è‡ªç”±åº¦æœºå™¨äººæ§åˆ¶çš„æ ·æœ¬æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span> <span class="paper-tag">bi-manual</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04147v4" data-paper-url="./papers/250604147v4-slac-simulation-pretrained-latent-action-space-for-whole-body-real-w.html" onclick="toggleFavorite(this, '2506.04147v4', 'SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250603568v2-confidence-guided-human-ai-collaboration-reinforcement-learning-with.html">Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving</a></td>
  <td>æå‡ºä¿¡å¿ƒå¼•å¯¼çš„äººæœºåä½œç­–ç•¥ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„å®‰å…¨æ¢ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">shared control</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">imitation learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03568v2" data-paper-url="./papers/250603568v2-confidence-guided-human-ai-collaboration-reinforcement-learning-with.html" onclick="toggleFavorite(this, '2506.03568v2', 'Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250603982v1-a-bi-level-optimization-method-for-redundant-dual-arm-minimum-time-p.html">A Bi-Level Optimization Method for Redundant Dual-Arm Minimum Time Problems</a></td>
  <td>æå‡ºåŒå±‚ä¼˜åŒ–æ–¹æ³•ä»¥è§£å†³å†—ä½™åŒè‡‚æœºå™¨äººæœ€å°æ—¶é—´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">dual-arm</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03982v1" data-paper-url="./papers/250603982v1-a-bi-level-optimization-method-for-redundant-dual-arm-minimum-time-p.html" onclick="toggleFavorite(this, '2506.03982v1', 'A Bi-Level Optimization Method for Redundant Dual-Arm Minimum Time Problems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250603896v2-flip-flowability-informed-powder-weighing.html">FLIP: Flowability-Informed Powder Weighing</a></td>
  <td>æå‡ºFLIPæ¡†æ¶ä»¥è§£å†³ç²‰æœ«ç§°é‡ä¸­çš„è‡ªé€‚åº”è‡ªåŠ¨åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">curriculum learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03896v2" data-paper-url="./papers/250603896v2-flip-flowability-informed-powder-weighing.html" onclick="toggleFavorite(this, '2506.03896v2', 'FLIP: Flowability-Informed Powder Weighing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250603863v2-star-learning-diverse-robot-skill-abstractions-through-rotation-augm.html">STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization</a></td>
  <td>æå‡ºSTARæ¡†æ¶ä»¥è§£å†³æœºå™¨äººæŠ€èƒ½æŠ½è±¡ä¸­çš„ä»£ç æœ¬å´©æºƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">VQ-VAE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03863v2" data-paper-url="./papers/250603863v2-star-learning-diverse-robot-skill-abstractions-through-rotation-augm.html" onclick="toggleFavorite(this, '2506.03863v2', 'STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250604308v3-roborefer-towards-spatial-referring-with-reasoning-in-vision-languag.html">RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics</a></td>
  <td>æå‡ºRoboReferä»¥è§£å†³æœºå™¨äººç©ºé—´æŒ‡ç§°ä¸æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04308v3" data-paper-url="./papers/250604308v3-roborefer-towards-spatial-referring-with-reasoning-in-vision-languag.html" onclick="toggleFavorite(this, '2506.04308v3', 'RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250604362v2-learning-smooth-state-dependent-traversability-from-dense-point-clou.html">Learning Smooth State-Dependent Traversability from Dense Point Clouds</a></td>
  <td>æå‡ºSPARTAä»¥è§£å†³è¶Šé‡è‡ªä¸»é©¾é©¶ä¸­çš„åœ°å½¢å¯é€šè¡Œæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">traversability</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04362v2" data-paper-url="./papers/250604362v2-learning-smooth-state-dependent-traversability-from-dense-point-clou.html" onclick="toggleFavorite(this, '2506.04362v2', 'Learning Smooth State-Dependent Traversability from Dense Point Clouds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250604359v3-cuvslam-cuda-accelerated-visual-odometry-and-mapping.html">cuVSLAM: CUDA accelerated visual odometry and mapping</a></td>
  <td>æå‡ºcuVSLAMä»¥è§£å†³è‡ªä¸»æœºå™¨äººå®šä½ä¸åœ°å›¾æ„å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual odometry</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04359v3" data-paper-url="./papers/250604359v3-cuvslam-cuda-accelerated-visual-odometry-and-mapping.html" onclick="toggleFavorite(this, '2506.04359v3', 'cuVSLAM: CUDA accelerated visual odometry and mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250604218v2-pseudo-simulation-for-autonomous-driving.html">Pseudo-Simulation for Autonomous Driving</a></td>
  <td>æå‡ºä¼ªä»¿çœŸä»¥è§£å†³è‡ªåŠ¨é©¾é©¶è¯„ä¼°ä¸­çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04218v2" data-paper-url="./papers/250604218v2-pseudo-simulation-for-autonomous-driving.html" onclick="toggleFavorite(this, '2506.04218v2', 'Pseudo-Simulation for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250604505v1-sgn-cirl-scene-graph-based-navigation-with-curriculum-imitation-and-.html">SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning</a></td>
  <td>æå‡ºSGN-CIRLæ¡†æ¶ä»¥è§£å†³æ— åœ°å›¾æœºå™¨äººå¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">curriculum learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04505v1" data-paper-url="./papers/250604505v1-sgn-cirl-scene-graph-based-navigation-with-curriculum-imitation-and-.html" onclick="toggleFavorite(this, '2506.04505v1', 'SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250603516v1-semnav-a-model-based-planner-for-zero-shot-object-goal-navigation-us.html">SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models</a></td>
  <td>æå‡ºSemNavä»¥è§£å†³é›¶-shotç›®æ ‡ç‰©ä½“å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03516v1" data-paper-url="./papers/250603516v1-semnav-a-model-based-planner-for-zero-shot-object-goal-navigation-us.html" onclick="toggleFavorite(this, '2506.03516v1', 'SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250604404v1-a-framework-leveraging-large-language-models-for-autonomous-uav-cont.html">A Framework Leveraging Large Language Models for Autonomous UAV Control in Flying Networks</a></td>
  <td>æå‡ºFLUCæ¡†æ¶ä»¥å®ç°æ— äººæœºåœ¨é£è¡Œç½‘ç»œä¸­çš„è‡ªä¸»æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04404v1" data-paper-url="./papers/250604404v1-a-framework-leveraging-large-language-models-for-autonomous-uav-cont.html" onclick="toggleFavorite(this, '2506.04404v1', 'A Framework Leveraging Large Language Models for Autonomous UAV Control in Flying Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250603834v4-care-enhancing-safety-of-visual-navigation-through-collision-avoidan.html">CARE: Enhancing Safety of Visual Navigation through Collision Avoidance via Repulsive Estimation</a></td>
  <td>æå‡ºCAREä»¥è§£å†³è§†è§‰å¯¼èˆªä¸­çš„ç¢°æ’é¿å…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03834v4" data-paper-url="./papers/250603834v4-care-enhancing-safety-of-visual-navigation-through-collision-avoidan.html" onclick="toggleFavorite(this, '2506.03834v4', 'CARE: Enhancing Safety of Visual Navigation through Collision Avoidance via Repulsive Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)