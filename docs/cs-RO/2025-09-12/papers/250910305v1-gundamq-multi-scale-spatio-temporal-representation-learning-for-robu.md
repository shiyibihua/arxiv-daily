---
layout: default
title: GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning
---

# GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10305" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10305v1</a>
  <a href="https://arxiv.org/pdf/2509.10305.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10305v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10305v1', 'GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yutong Shen, Ruizhe Xia, Bokai Yan, Shunqi zhang, Pengrui Xiang, Sicheng He, Yixin Xu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

**å¤‡æ³¨**: 6 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GundamQï¼šå¤šå°ºåº¦æ—¶ç©ºè¡¨å¾å­¦ä¹ æå‡æœºå™¨äººé²æ£’è·¯å¾„è§„åˆ’èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `æœºå™¨äººè·¯å¾„è§„åˆ’` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `æ—¶ç©ºè¡¨å¾å­¦ä¹ ` `å¤šå°ºåº¦å»ºæ¨¡` `åŠ¨æ€ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è·¯å¾„è§„åˆ’æ–¹æ³•éš¾ä»¥å……åˆ†å»ºæ¨¡å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§ï¼Œå¯¼è‡´åŠ¨æ€ç¯å¢ƒé€‚åº”æ€§ä¸è¶³ã€‚
2. GundamQé€šè¿‡åˆ†å±‚æå–å¤šç²’åº¦ç©ºé—´ç‰¹å¾å’Œå¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§ï¼Œæå‡åŠ¨æ€ç¯å¢ƒä¸‹çš„æ„ŸçŸ¥ç²¾åº¦ã€‚
3. GundamQé‡‡ç”¨è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–æ¨¡å—ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œå¹¶é€šè¿‡çº¦æŸç­–ç•¥æ›´æ–°ä¼˜åŒ–è·¯å¾„å¹³æ»‘æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­ï¼Œæœºå™¨äººè·¯å¾„è§„åˆ’éœ€è¦ç²¾ç¡®çš„æ—¶ç©ºç¯å¢ƒç†è§£ä»¥åŠåœ¨éƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹çš„é²æ£’å†³ç­–ã€‚ç„¶è€Œï¼Œç›®å‰åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è·¯å¾„è§„åˆ’æ–¹æ³•é¢ä¸´ä¸¤ä¸ªæ ¹æœ¬é™åˆ¶ï¼šï¼ˆ1ï¼‰å¯¹å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§çš„å»ºæ¨¡ä¸è¶³ï¼Œå¯¼è‡´åœ¨åŠ¨æ€åœºæ™¯ä¸­çš„é€‚åº”æ€§æ¬ ä½³ï¼›ï¼ˆ2ï¼‰æ¢ç´¢-åˆ©ç”¨å¹³è¡¡æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´è·¯å¾„è´¨é‡ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GundamQï¼šä¸€ç§ç”¨äºæœºå™¨äººè·¯å¾„è§„åˆ’çš„å¤šå°ºåº¦æ—¶ç©ºQç½‘ç»œã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šï¼ˆiï¼‰æ—¶ç©ºæ„ŸçŸ¥æ¨¡å—ï¼Œå®ƒåˆ†å±‚æå–å¤šç²’åº¦ç©ºé—´ç‰¹å¾å’Œä»ç¬æ—¶åˆ°æ‰©å±•æ—¶é—´èŒƒå›´çš„å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§ï¼Œä»è€Œæé«˜åŠ¨æ€ç¯å¢ƒä¸­çš„æ„ŸçŸ¥ç²¾åº¦ï¼›ï¼ˆiiï¼‰è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–æ¨¡å—ï¼Œå®ƒåœ¨è®­ç»ƒæœŸé—´å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼ŒåŒæ—¶é€šè¿‡çº¦æŸç­–ç•¥æ›´æ–°æ¥ä¼˜åŒ–å¹³æ»‘æ€§å’Œç¢°æ’æ¦‚ç‡ã€‚åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼ŒGundamQåœ¨æˆåŠŸç‡æ–¹é¢å®ç°äº†15.3ï¼…çš„æå‡ï¼Œåœ¨æ•´ä½“è·¯å¾„è´¨é‡æ–¹é¢å®ç°äº†21.7ï¼…çš„æå‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­æœºå™¨äººè·¯å¾„è§„åˆ’çš„é—®é¢˜ã€‚ç°æœ‰åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•åœ¨å»ºæ¨¡å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§å’Œå¹³è¡¡æ¢ç´¢-åˆ©ç”¨æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´è·¯å¾„è§„åˆ’çš„æˆåŠŸç‡å’Œè´¨é‡ä¸é«˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹ç¯å¢ƒçš„åŠ¨æ€å˜åŒ–ï¼Œå¹¶ä¸”å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤šå°ºåº¦æ—¶ç©ºQç½‘ç»œï¼Œé€šè¿‡åˆ†å±‚æå–ç©ºé—´ç‰¹å¾å’Œå¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§æ¥å¢å¼ºå¯¹åŠ¨æ€ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–æ–¹æ³•æ¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä»è€Œæé«˜è·¯å¾„è§„åˆ’çš„æˆåŠŸç‡å’Œè´¨é‡ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒé€‚åº”æ€§å’Œæ¢ç´¢æ•ˆç‡æ–¹é¢çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGundamQæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ—¶ç©ºæ„ŸçŸ¥æ¨¡å—å’Œè‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–æ¨¡å—ã€‚æ—¶ç©ºæ„ŸçŸ¥æ¨¡å—è´Ÿè´£ä»ç¯å¢ƒä¸­æå–å¤šç²’åº¦ç©ºé—´ç‰¹å¾å’Œå¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§ï¼Œä¸ºåç»­çš„å†³ç­–æä¾›ä¿¡æ¯ã€‚è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–æ¨¡å—åˆ™åŸºäºæå–çš„ç‰¹å¾ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–æœºå™¨äººçš„ç­–ç•¥ï¼Œå¹¶å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä»¥è·å¾—é«˜è´¨é‡çš„è·¯å¾„ã€‚æ•´ä½“æµç¨‹æ˜¯ä»ç¯å¢ƒè¾“å…¥åˆ°ç‰¹å¾æå–ï¼Œå†åˆ°ç­–ç•¥ä¼˜åŒ–å’Œè·¯å¾„ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå¤šå°ºåº¦æ—¶ç©ºQç½‘ç»œï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å»ºæ¨¡ç¯å¢ƒä¸­çš„å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§ï¼Œä»è€Œæé«˜å¯¹åŠ¨æ€ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–æ¨¡å—èƒ½å¤Ÿå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ï¼Œå¹¶ä¼˜åŒ–è·¯å¾„çš„å¹³æ»‘æ€§å’Œå®‰å…¨æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGundamQåœ¨åŠ¨æ€ç¯å¢ƒé€‚åº”æ€§å’Œè·¯å¾„è´¨é‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šæ—¶ç©ºæ„ŸçŸ¥æ¨¡å—å¯èƒ½é‡‡ç”¨äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„ç»„åˆï¼ŒCNNç”¨äºæå–ç©ºé—´ç‰¹å¾ï¼ŒRNNç”¨äºå»ºæ¨¡æ—¶é—´ä¾èµ–æ€§ã€‚è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–æ¨¡å—å¯èƒ½é‡‡ç”¨äº†Trust Region Policy Optimization (TRPO) æˆ– Proximal Policy Optimization (PPO) ç­‰ç®—æ³•ï¼Œå¹¶å¼•å…¥äº†çº¦æŸæ¡ä»¶æ¥ä¿è¯è·¯å¾„çš„å¹³æ»‘æ€§å’Œå®‰å…¨æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬è·¯å¾„é•¿åº¦ã€ç¢°æ’æƒ©ç½šå’Œç­–ç•¥æ›´æ–°çš„çº¦æŸé¡¹ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGundamQåœ¨åŠ¨æ€ç¯å¢ƒä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒGundamQåœ¨æˆåŠŸç‡æ–¹é¢æé«˜äº†15.3%ï¼Œåœ¨æ•´ä½“è·¯å¾„è´¨é‡æ–¹é¢æé«˜äº†21.7%ã€‚è¿™äº›æ•°æ®è¡¨æ˜ï¼ŒGundamQèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åº”å¯¹åŠ¨æ€ç¯å¢ƒä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶ç”Ÿæˆæ›´é«˜è´¨é‡çš„è·¯å¾„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GundamQå¯åº”ç”¨äºå„ç§éœ€è¦åœ¨åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­è¿›è¡Œè·¯å¾„è§„åˆ’çš„æœºå™¨äººåº”ç”¨ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€ç‰©æµé…é€ã€ä»“å‚¨æœºå™¨äººã€æœæ•‘æœºå™¨äººç­‰ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªèƒ½åŠ›ï¼Œé™ä½äº‹æ•…é£é™©ï¼Œæé«˜å·¥ä½œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„å¸‚åœºå‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In dynamic and uncertain environments, robotic path planning demands accurate spatiotemporal environment understanding combined with robust decision-making under partial observability. However, current deep reinforcement learning-based path planning methods face two fundamental limitations: (1) insufficient modeling of multi-scale temporal dependencies, resulting in suboptimal adaptability in dynamic scenarios, and (2) inefficient exploration-exploitation balance, leading to degraded path quality. To address these challenges, we propose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path Planning. The framework comprises two key modules: (i) the Spatiotemporal Perception module, which hierarchically extracts multi-granularity spatial features and multi-scale temporal dependencies ranging from instantaneous to extended time horizons, thereby improving perception accuracy in dynamic environments; and (ii) the Adaptive Policy Optimization module, which balances exploration and exploitation during training while optimizing for smoothness and collision probability through constrained policy updates. Experiments in dynamic environments demonstrate that GundamQ achieves a 15.3\% improvement in success rate and a 21.7\% increase in overall path quality, significantly outperforming existing state-of-the-art methods.

