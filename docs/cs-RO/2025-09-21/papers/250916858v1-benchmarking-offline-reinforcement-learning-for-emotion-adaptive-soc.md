---
layout: default
title: Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics
---

# Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16858" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16858v1</a>
  <a href="https://arxiv.org/pdf/2509.16858.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16858v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16858v1', 'Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Soon Jynn Chu, Raju Gottumukkala, Alan Barhorst

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-21

**å¤‡æ³¨**: Submitted to conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æƒ…æ„Ÿè‡ªé€‚åº”ç¤¾äº¤æœºå™¨äººåŸºå‡†æµ‹è¯•æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æƒ…æ„Ÿè‡ªé€‚åº”` `ç¤¾äº¤æœºå™¨äºº` `äººæœºäº¤äº’` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åœ¨çº¿å¼ºåŒ–å­¦ä¹ åœ¨ç¤¾äº¤æœºå™¨äººæƒ…æ„Ÿé€‚åº”æ–¹é¢é¢ä¸´æ•°æ®æ”¶é›†éš¾å’Œè¡Œä¸ºå®‰å…¨é£é™©ã€‚
2. åˆ©ç”¨é¢„æ”¶é›†æ•°æ®ï¼Œé‡‡ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ç°æƒ…æ„Ÿè‡ªé€‚åº”ç¤¾äº¤æœºå™¨äººã€‚
3. åœ¨äººæœºæ¸¸æˆåœºæ™¯ä¸­ï¼ŒBCQå’ŒCQLç®—æ³•åœ¨æ•°æ®ç¨€ç–æƒ…å†µä¸‹è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨æƒ…æ„Ÿè‡ªé€‚åº”ç¤¾äº¤æœºå™¨äººä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚å’Œå­˜åœ¨ä¸å®‰å…¨è¡Œä¸ºé£é™©çš„é—®é¢˜ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ¶æ„ï¼Œé›†æˆäº†å¤šæ¨¡æ€æ„ŸçŸ¥ä¸è¯†åˆ«ã€å†³ç­–åˆ¶å®šå’Œè‡ªé€‚åº”å“åº”ã€‚é€šè¿‡åœ¨äººæœºæ¸¸æˆåœºæ™¯ä¸­æ”¶é›†çš„æœ‰é™æ•°æ®é›†ï¼Œå»ºç«‹äº†ä¸€ä¸ªç”¨äºæ¯”è¾ƒç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„åŸºå‡†ï¼Œè¿™äº›ç®—æ³•ä¸éœ€è¦åœ¨çº¿ç¯å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBCQå’ŒCQLç®—æ³•å¯¹æ•°æ®ç¨€ç–æ€§æ›´å…·é²æ£’æ€§ï¼Œä¸NFQã€DQNå’ŒDDQNç›¸æ¯”ï¼Œå®ç°äº†æ›´é«˜çš„çŠ¶æ€-åŠ¨ä½œä»·å€¼ã€‚è¿™é¡¹å·¥ä½œä¸ºæƒ…æ„Ÿè‡ªé€‚åº”æœºå™¨äººé¢†åŸŸçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºæœªæ¥åœ¨çœŸå®äººæœºäº¤äº’ç¯å¢ƒä¸­çš„éƒ¨ç½²æä¾›äº†ä¿¡æ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¤¾äº¤æœºå™¨äººå¦‚ä½•æœ‰æ•ˆä¸”å®‰å…¨åœ°å­¦ä¹ å¯¹äººç±»æƒ…æ„Ÿåšå‡ºé€‚å½“ååº”çš„é—®é¢˜ã€‚ä¼ ç»Ÿåœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç¤¾äº¤æœºå™¨äººé¢†åŸŸåº”ç”¨å—é™ï¼Œä¸»è¦ç—›ç‚¹åœ¨äºä¸äººç±»äº¤äº’è¿›è¡Œæ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­å¯èƒ½äº§ç”Ÿä¸å®‰å…¨æˆ–ä¸æ°å½“çš„è¡Œä¸ºï¼Œå½±å“ç”¨æˆ·ä½“éªŒå’Œä¿¡ä»»åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå³ä»…ä½¿ç”¨é¢„å…ˆæ”¶é›†å¥½çš„æ•°æ®é›†æ¥è®­ç»ƒæœºå™¨äººï¼Œæ— éœ€åœ¨çº¿äº¤äº’ã€‚è¿™æ ·å¯ä»¥é¿å…åœ¨çº¿æ¢ç´¢å¸¦æ¥çš„é£é™©å’Œæˆæœ¬ï¼ŒåŒæ—¶ä½¿æœºå™¨äººèƒ½å¤Ÿå­¦ä¹ åˆ°æƒ…æ„Ÿè‡ªé€‚åº”çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºçš„ç³»ç»Ÿæ¶æ„åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€æ„ŸçŸ¥ä¸è¯†åˆ«æ¨¡å—ï¼Œç”¨äºæ„ŸçŸ¥äººç±»çš„æƒ…æ„ŸçŠ¶æ€ï¼›2) å†³ç­–åˆ¶å®šæ¨¡å—ï¼ŒåŸºäºç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ ¹æ®æ„ŸçŸ¥åˆ°çš„æƒ…æ„ŸçŠ¶æ€é€‰æ‹©åˆé€‚çš„æœºå™¨äººè¡Œä¸ºï¼›3) è‡ªé€‚åº”å“åº”æ¨¡å—ï¼Œæ‰§è¡Œæœºå™¨äººè¡Œä¸ºï¼Œå¹¶å°†å…¶åé¦ˆç»™äººç±»ã€‚æ•´ä½“æµç¨‹æ˜¯ä»å¤šæ¨¡æ€æ•°æ®ä¸­æå–æƒ…æ„Ÿç‰¹å¾ï¼Œç„¶åè¾“å…¥åˆ°ç¦»çº¿è®­ç»ƒå¥½çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ä¸­ï¼Œè¾“å‡ºç›¸åº”çš„æœºå™¨äººåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†ç¦»çº¿å¼ºåŒ–å­¦ä¹ åº”ç”¨äºæƒ…æ„Ÿè‡ªé€‚åº”ç¤¾äº¤æœºå™¨äººï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°ä¸åŒç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ æ— éœ€åœ¨çº¿æ¢ç´¢ï¼Œæ›´åŠ å®‰å…¨å’Œé«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ªäººæœºæ¸¸æˆåœºæ™¯çš„æ•°æ®é›†è¿›è¡Œå®éªŒã€‚å¯¹æ¯”äº†å¤šç§ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ŒåŒ…æ‹¬BCQã€CQLã€NFQã€DQNå’ŒDDQNã€‚å®éªŒä¸­ï¼ŒçŠ¶æ€ç©ºé—´åŒ…æ‹¬äººç±»çš„æƒ…æ„ŸçŠ¶æ€ï¼ŒåŠ¨ä½œç©ºé—´åŒ…æ‹¬æœºå™¨äººçš„å„ç§è¡Œä¸ºã€‚è®ºæ–‡å…³æ³¨ä¸åŒç®—æ³•åœ¨æ•°æ®ç¨€ç–æƒ…å†µä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä½¿ç”¨çŠ¶æ€-åŠ¨ä½œä»·å€¼ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœ‰é™çš„äººæœºæ¸¸æˆæ•°æ®é›†ä¸Šï¼ŒBCQå’ŒCQLç®—æ³•åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç–çš„æƒ…å†µä¸‹ã€‚ç›¸è¾ƒäºNFQã€DQNå’ŒDDQNç­‰ç®—æ³•ï¼ŒBCQå’ŒCQLèƒ½å¤Ÿå®ç°æ›´é«˜çš„çŠ¶æ€-åŠ¨ä½œä»·å€¼ï¼Œè¡¨æ˜å®ƒä»¬èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ åˆ°æƒ…æ„Ÿè‡ªé€‚åº”çš„ç­–ç•¥ã€‚è¿™äº›ç»“æœä¸ºé€‰æ‹©åˆé€‚çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§äººæœºäº¤äº’åœºæ™¯ï¼Œä¾‹å¦‚ï¼šå¯¹è¯æœºå™¨äººï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„æƒ…ç»ªè°ƒæ•´å¯¹è¯ç­–ç•¥ï¼›æ•™è‚²ä¼™ä¼´ï¼Œæ ¹æ®å­¦ç”Ÿçš„æƒ…ç»ªçŠ¶æ€æä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ è¾…å¯¼ï¼›ä»¥åŠä¸ªäººåŠ©ç†ï¼Œæ ¹æ®ç”¨æˆ·çš„æƒ…ç»ªéœ€æ±‚æä¾›æ›´è´´å¿ƒçš„æœåŠ¡ã€‚é€šè¿‡æé«˜ç¤¾äº¤æœºå™¨äººçš„æƒ…æ„Ÿé€‚åº”èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·ä½“éªŒï¼Œå»ºç«‹æ›´å¼ºçš„ä¿¡ä»»å…³ç³»ï¼Œå¹¶ä¿ƒè¿›äººæœºåä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The ability of social robots to respond to human emotions is crucial for building trust and acceptance in human-robot collaborative environments. However, developing such capabilities through online reinforcement learning is sometimes impractical due to the prohibitive cost of data collection and the risk of generating unsafe behaviors. In this paper, we study the use of offline reinforcement learning as a practical and efficient alternative. This technique uses pre-collected data to enable emotion-adaptive social robots. We present a system architecture that integrates multimodal sensing and recognition, decision-making, and adaptive responses. Using a limited dataset from a human-robot game-playing scenario, we establish a benchmark for comparing offline reinforcement learning algorithms that do not require an online environment. Our results show that BCQ and CQL are more robust to data sparsity, achieving higher state-action values compared to NFQ, DQN, and DDQN. This work establishes a foundation for benchmarking offline RL in emotion-adaptive robotics and informs future deployment in real-world HRI. Our findings provide empirical insight into the performance of offline reinforcement learning algorithms in data-constrained HRI. This work establishes a foundation for benchmarking offline RL in emotion-adaptive robotics and informs its future deployment in real-world HRI, such as in conversational agents, educational partners, and personal assistants, require reliable emotional responsiveness.

