---
layout: default
title: Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning
---

# Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning

**arXiv**: [2512.14057v1](https://arxiv.org/abs/2512.14057) | [PDF](https://arxiv.org/pdf/2512.14057.pdf)

**ä½œè€…**: Amir M. Soufi Enayati, Homayoun Honari, Homayoun Najjaran

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCRAFTæ¨¡åž‹ï¼Œé€šè¿‡æ— åŠ¨ä½œTransformerç¼–ç å™¨-è§£ç å™¨å®žçŽ°ä»»åŠ¡è¡¨ç¤ºï¼Œä»¥è§£å†³å…ƒå¼ºåŒ–å­¦ä¹ ä¸­ä»»åŠ¡æŽ¨æ–­ä¸Žç­–ç•¥ä¼˜åŒ–çš„è€¦åˆé—®é¢˜ã€‚**

**å…³é”®è¯**: `å…ƒå¼ºåŒ–å­¦ä¹ ` `ä»»åŠ¡è¡¨ç¤ºå­¦ä¹ ` `Transformerç¼–ç å™¨-è§£ç å™¨` `æ— åŠ¨ä½œæŽ¨æ–­` `æœºå™¨äººæŽ§åˆ¶` `æ³›åŒ–èƒ½åŠ›` `ä¿¡å¿µæ¨¡åž‹` `æ‘Šé”€å˜åˆ†æŽ¨æ–­`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å…ƒå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–åŠ¨ä½œä¿¡æ¯è¿›è¡Œä»»åŠ¡æŽ¨æ–­ï¼Œå¯¼è‡´ä»»åŠ¡è¡¨ç¤ºä¸Žç­–ç•¥ä¼˜åŒ–ç´§å¯†è€¦åˆï¼Œé™åˆ¶äº†æ³›åŒ–èƒ½åŠ›ã€‚
2. CRAFTæ¨¡åž‹ä»…ä½¿ç”¨çŠ¶æ€å’Œå¥–åŠ±åºåˆ—ï¼Œé€šè¿‡Transformerç¼–ç å™¨-è§£ç å™¨æŽ¨æ–­ä»»åŠ¡è¡¨ç¤ºï¼Œå®žçŽ°ä»»åŠ¡æŽ¨æ–­ä¸Žç­–ç•¥ä¼˜åŒ–çš„è§£è€¦ã€‚
3. åœ¨MetaWorld ML-10åŸºå‡†ä¸Šï¼ŒCRAFTç›¸æ¯”åŸºçº¿æ–¹æ³•å±•çŽ°å‡ºæ›´å¿«çš„é€‚åº”é€Ÿåº¦ã€æ›´å¥½çš„æ³›åŒ–æ€§èƒ½å’Œæ›´æœ‰æ•ˆçš„æŽ¢ç´¢èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½¿æœºå™¨äººèƒ½åœ¨ä¸ç¡®å®šçŽ¯å¢ƒä¸­æ“ä½œï¼Œä½†æ ‡å‡†æ–¹æ³•å¸¸éš¾ä»¥æ³›åŒ–åˆ°æœªè§ä»»åŠ¡ã€‚ä¸Šä¸‹æ–‡è‡ªé€‚åº”å…ƒå¼ºåŒ–å­¦ä¹ é€šè¿‡ä»»åŠ¡è¡¨ç¤ºæ¥åº”å¯¹è¿™äº›é™åˆ¶ï¼Œä½†å®ƒä»¬å¤§å¤šä¾èµ–ç»éªŒä¸­çš„å®Œæ•´åŠ¨ä½œä¿¡æ¯ï¼Œä½¿ä»»åŠ¡æŽ¨æ–­ä¸Žç‰¹å®šç­–ç•¥ç´§å¯†è€¦åˆã€‚æœ¬æ–‡ä»‹ç»äº†Context Representation via Action Free Transformer encoder decoderï¼ˆCRAFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¿¡å¿µæ¨¡åž‹ï¼Œä»…ä»ŽçŠ¶æ€å’Œå¥–åŠ±åºåˆ—æŽ¨æ–­ä»»åŠ¡è¡¨ç¤ºã€‚é€šè¿‡æ¶ˆé™¤å¯¹åŠ¨ä½œçš„ä¾èµ–ï¼ŒCRAFTå°†ä»»åŠ¡æŽ¨æ–­ä¸Žç­–ç•¥ä¼˜åŒ–è§£è€¦ï¼Œæ”¯æŒæ¨¡å—åŒ–è®­ç»ƒï¼Œå¹¶åˆ©ç”¨æ‘Šé”€å˜åˆ†æŽ¨æ–­è¿›è¡Œå¯æ‰©å±•çš„ä¿¡å¿µæ›´æ–°ã€‚è¯¥æ¨¡åž‹åŸºäºŽå¸¦æœ‰æ—‹è½¬ä½ç½®åµŒå…¥çš„Transformerç¼–ç å™¨-è§£ç å™¨æž„å»ºï¼Œèƒ½æ•æ‰é•¿ç¨‹æ—¶é—´ä¾èµ–ï¼Œå¹¶ç¨³å¥ç¼–ç å‚æ•°åŒ–å’Œéžå‚æ•°åŒ–ä»»åŠ¡å˜åŒ–ã€‚åœ¨MetaWorld ML-10æœºå™¨äººæ“ä½œåŸºå‡†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œä¸Žä¸Šä¸‹æ–‡è‡ªé€‚åº”å…ƒRLåŸºçº¿ç›¸æ¯”ï¼ŒCRAFTå®žçŽ°äº†æ›´å¿«çš„é€‚åº”ã€æ›´å¥½çš„æ³›åŒ–å’Œæ›´æœ‰æ•ˆçš„æŽ¢ç´¢ã€‚è¿™äº›å‘çŽ°çªæ˜¾äº†æ— åŠ¨ä½œæŽ¨æ–­ä½œä¸ºæœºå™¨äººæŽ§åˆ¶ä¸­å¯æ‰©å±•RLåŸºç¡€çš„æ½œåŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

CRAFTçš„æ•´ä½“æ¡†æž¶æ˜¯ä¸€ä¸ªåŸºäºŽTransformerç¼–ç å™¨-è§£ç å™¨çš„ä¿¡å¿µæ¨¡åž‹ï¼Œç”¨äºŽä»ŽçŠ¶æ€å’Œå¥–åŠ±åºåˆ—æŽ¨æ–­ä»»åŠ¡è¡¨ç¤ºã€‚å…³é”®æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ï¼šé‡‡ç”¨æ— åŠ¨ä½œè¾“å…¥è®¾è®¡ï¼Œä»…ä¾èµ–çŠ¶æ€å’Œå¥–åŠ±åºåˆ—ï¼›ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥å¢žå¼ºä½ç½®ç¼–ç èƒ½åŠ›ï¼›ç»“åˆæ‘Šé”€å˜åˆ†æŽ¨æ–­è¿›è¡Œå¯æ‰©å±•çš„ä¿¡å¿µæ›´æ–°ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œä¼ ç»Ÿä¸Šä¸‹æ–‡è‡ªé€‚åº”å…ƒRLæ–¹æ³•éœ€è¦å®Œæ•´åŠ¨ä½œä¿¡æ¯ï¼Œè€ŒCRAFTé€šè¿‡ç§»é™¤åŠ¨ä½œä¾èµ–ï¼Œå®žçŽ°äº†ä»»åŠ¡æŽ¨æ–­ä¸Žç­–ç•¥ä¼˜åŒ–çš„è§£è€¦ï¼Œæ”¯æŒæ¨¡å—åŒ–è®­ç»ƒï¼Œå¹¶èƒ½å¤„ç†å‚æ•°åŒ–å’Œéžå‚æ•°åŒ–ä»»åŠ¡å˜åŒ–ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨MetaWorld ML-10æœºå™¨äººæ“ä½œåŸºå‡†å®žéªŒä¸­ï¼ŒCRAFTç›¸æ¯”ä¸Šä¸‹æ–‡è‡ªé€‚åº”å…ƒRLåŸºçº¿æ–¹æ³•ï¼Œå®žçŽ°äº†æ›´å¿«çš„ä»»åŠ¡é€‚åº”é€Ÿåº¦ã€æ›´é«˜çš„æ³›åŒ–æ€§èƒ½ï¼ˆæå‡å…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ï¼Œä»¥åŠæ›´æœ‰æ•ˆçš„æŽ¢ç´¢ç­–ç•¥ï¼Œçªæ˜¾äº†æ— åŠ¨ä½œæŽ¨æ–­åœ¨æå‡å…ƒå¼ºåŒ–å­¦ä¹ å¯æ‰©å±•æ€§å’Œæ•ˆçŽ‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸»è¦åº”ç”¨äºŽæœºå™¨äººæŽ§åˆ¶é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡å’Œæ³›åŒ–åˆ°æœªè§åœºæ™¯çš„å…ƒå¼ºåŒ–å­¦ä¹ çŽ¯å¢ƒä¸­ã€‚æ½œåœ¨åº”ç”¨åŒ…æ‹¬å·¥ä¸šè‡ªåŠ¨åŒ–ã€æœåŠ¡æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ç­‰ï¼Œå…¶ä¸­æœºå™¨äººéœ€åœ¨åŠ¨æ€å’Œä¸ç¡®å®šçŽ¯å¢ƒä¸­é«˜æ•ˆå­¦ä¹ å’Œæ“ä½œã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning (RL) enables robots to operate in uncertain environments, but standard approaches often struggle with poor generalization to unseen tasks. Context-adaptive meta reinforcement learning addresses these limitations by conditioning on the task representation, yet they mostly rely on complete action information in the experience making task inference tightly coupled to a specific policy. This paper introduces Context Representation via Action Free Transformer encoder decoder (CRAFT), a belief model that infers task representations solely from sequences of states and rewards. By removing the dependence on actions, CRAFT decouples task inference from policy optimization, supports modular training, and leverages amortized variational inference for scalable belief updates. Built on a transformer encoder decoder with rotary positional embeddings, the model captures long range temporal dependencies and robustly encodes both parametric and non-parametric task variations. Experiments on the MetaWorld ML-10 robotic manipulation benchmark show that CRAFT achieves faster adaptation, improved generalization, and more effective exploration compared to context adaptive meta--RL baselines. These findings highlight the potential of action-free inference as a foundation for scalable RL in robotic control.

