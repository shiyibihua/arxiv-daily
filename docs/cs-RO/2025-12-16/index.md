---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-12-16
---

# cs.ROï¼ˆ2025-12-16ï¼‰

ğŸ“Š å…± **28** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (20 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (20 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251214031v1-sample-efficient-robot-skill-learning-for-construction-tasks-benchma.html">Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model</a></td>
  <td>å¯¹æ¯”VLAæ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ï¼Œæå‡å»ºç­‘æœºå™¨äººæ“ä½œæŠ€èƒ½å¹¶å®ç°é«˜æ•ˆæ ·æœ¬åˆ©ç”¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14031v1" onclick="toggleFavorite(this, '2512.14031v1', 'Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251214666v1-evolve-vla-test-time-training-from-environment-feedback-for-vision-l.html">EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</a></td>
  <td>EVOLVE-VLAï¼šé¢å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ç¯å¢ƒåé¦ˆæµ‹è¯•æ—¶è®­ç»ƒ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14666v1" onclick="toggleFavorite(this, '2512.14666v1', 'EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251214689v1-chip-adaptive-compliance-for-humanoid-control-through-hindsight-pert.html">CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation</a></td>
  <td>æå‡ºCHIPè‡ªé€‚åº”æŸ”é¡ºæ§åˆ¶ï¼Œæå‡äººå½¢æœºå™¨äººåŠ›æ“ä½œä»»åŠ¡æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14689v1" onclick="toggleFavorite(this, '2512.14689v1', 'CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251214270v1-cafe-television-a-coarse-to-fine-teleoperation-system-with-immersive.html">CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics</a></td>
  <td>CaFe-TeleVisionï¼šé¢å‘äººæœºå·¥æ•ˆå¢å¼ºçš„ç²—ç»†ç²’åº¦é¥æ“ä½œç³»ç»Ÿä¸æ²‰æµ¸å¼æƒ…å¢ƒå¯è§†åŒ–</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14270v1" onclick="toggleFavorite(this, '2512.14270v1', 'CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251214111v1-interactive-motion-planning-for-human-robot-collaboration-based-on-h.html">Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field</a></td>
  <td>æå‡ºåŸºäºäººæœºåä½œæ„å‹ç©ºé—´äººä½“å·¥å­¦åœºçš„äº¤äº’å¼æœºå™¨äººè¿åŠ¨è§„åˆ’æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14111v1" onclick="toggleFavorite(this, '2512.14111v1', 'Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251214411v1-synthetic-data-pipelines-for-adaptive-mission-ready-militarized-huma.html">Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids</a></td>
  <td>Omniaæå‡ºä¸€ç§åŸºäºåˆæˆæ•°æ®çš„æµç¨‹ï¼ŒåŠ é€Ÿå†›ç”¨äººå½¢æœºå™¨äººçš„è®­ç»ƒä¸éƒ¨ç½²ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14411v1" onclick="toggleFavorite(this, '2512.14411v1', 'Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251214057v1-context-representation-via-action-free-transformer-encoder-decoder-f.html">Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning</a></td>
  <td>æå‡ºCRAFTï¼šä¸€ç§åŸºäºæ— åŠ¨ä½œTransformerçš„å…ƒå¼ºåŒ–å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºæ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14057v1" onclick="toggleFavorite(this, '2512.14057v1', 'Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251214350v1-fine-tuning-of-neural-network-approximate-mpc-without-retraining-via.html">Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization</a></td>
  <td>æå‡ºåŸºäºè´å¶æ–¯ä¼˜åŒ–çš„ç¥ç»è¿‘ä¼¼MPCå¾®è°ƒæ–¹æ³•ï¼Œæ— éœ€é‡æ–°è®­ç»ƒç½‘ç»œã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14350v1" onclick="toggleFavorite(this, '2512.14350v1', 'Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251214206v1-trajectory-tracking-for-multi-manipulator-systems-in-constrained-env.html">Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments</a></td>
  <td>æå‡ºå¤šé€Ÿç‡è§„åˆ’ä¸æ§åˆ¶æ¡†æ¶ï¼Œè§£å†³çº¦æŸç¯å¢ƒä¸‹å¤šæœºæ¢°è‡‚ç³»ç»Ÿçš„è½¨è¿¹è·Ÿè¸ªé—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14206v1" onclick="toggleFavorite(this, '2512.14206v1', 'Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251214689v1-chip-adaptive-compliance-for-humanoid-control-through-hindsight-pert.html">CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation</a></td>
  <td>CHIPï¼šé€šè¿‡åè§ä¹‹æ˜æ‰°åŠ¨å®ç°äººå‹æœºå™¨äººè‡ªé€‚åº”æŸ”é¡ºæ§åˆ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14689v1" onclick="toggleFavorite(this, '2512.14689v1', 'CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251214270v1-cafe-television-a-coarse-to-fine-teleoperation-system-with-immersive.html">CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics</a></td>
  <td>CaFe-TeleVisionï¼šåŸºäºç²—ç»†ç²’åº¦æ§åˆ¶ä¸æ²‰æµ¸å¼å¯è§†åŒ–çš„äººå½¢æœºå™¨äººé¥æ“ä½œç³»ç»Ÿï¼Œæå‡äººæœºå·¥æ•ˆ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14270v1" onclick="toggleFavorite(this, '2512.14270v1', 'CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251214111v1-interactive-motion-planning-for-human-robot-collaboration-based-on-h.html">Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field</a></td>
  <td>æå‡ºåŸºäºäººæœºåä½œé…ç½®ç©ºé—´äººä½“å·¥å­¦åœºçš„äº¤äº’å¼è¿åŠ¨è§„åˆ’æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14111v1" onclick="toggleFavorite(this, '2512.14111v1', 'Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251214411v1-synthetic-data-pipelines-for-adaptive-mission-ready-militarized-huma.html">Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids</a></td>
  <td>Omniaæå‡ºä¸€ç§åŸºäºåˆæˆæ•°æ®çš„ç®¡çº¿ï¼ŒåŠ é€Ÿå†›ç”¨äººå½¢æœºå™¨äººçš„è®­ç»ƒä¸éƒ¨ç½²ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14411v1" onclick="toggleFavorite(this, '2512.14411v1', 'Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251214057v1-context-representation-via-action-free-transformer-encoder-decoder-f.html">Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning</a></td>
  <td>æå‡ºCRAFTï¼šä¸€ç§åŸºäºæ— åŠ¨ä½œTransformerçš„å…ƒå¼ºåŒ–å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºæ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14057v1" onclick="toggleFavorite(this, '2512.14057v1', 'Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251214031v1-sample-efficient-robot-skill-learning-for-construction-tasks-benchma.html">Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model</a></td>
  <td>å¯¹æ¯”VLAæ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ï¼Œæå‡å»ºç­‘æœºå™¨äººæ“ä½œæŠ€èƒ½å¹¶å®ç°é«˜æ•ˆæ ·æœ¬åˆ©ç”¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14031v1" onclick="toggleFavorite(this, '2512.14031v1', 'Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251214350v1-fine-tuning-of-neural-network-approximate-mpc-without-retraining-via.html">Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization</a></td>
  <td>æå‡ºåŸºäºè´å¶æ–¯ä¼˜åŒ–çš„AMPCè°ƒå‚æ–¹æ³•ï¼Œæ— éœ€é‡è®­ç»ƒç¥ç»ç½‘ç»œ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14350v1" onclick="toggleFavorite(this, '2512.14350v1', 'Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251214206v1-trajectory-tracking-for-multi-manipulator-systems-in-constrained-env.html">Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments</a></td>
  <td>æå‡ºå¤šé€Ÿç‡è§„åˆ’ä¸æ§åˆ¶æ¡†æ¶ï¼Œè§£å†³çº¦æŸç¯å¢ƒä¸‹å¤šæœºæ¢°è‡‚ç³»ç»Ÿçš„è½¨è¿¹è·Ÿè¸ªé—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14206v1" onclick="toggleFavorite(this, '2512.14206v1', 'Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251213974v1-autonomous-construction-site-safety-inspection-using-mobile-robots-a.html">Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline</a></td>
  <td>æå‡ºåŸºäºå¤šå±‚VLM-LLMæµæ°´çº¿çš„ç§»åŠ¨æœºå™¨äººè‡ªä¸»å»ºç­‘å·¥åœ°å®‰å…¨å·¡æ£€æ–¹æ¡ˆ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13974v1" onclick="toggleFavorite(this, '2512.13974v1', 'Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251214666v1-evolve-vla-test-time-training-from-environment-feedback-for-vision-l.html">EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</a></td>
  <td>EVOLVE-VLAï¼šåŸºäºç¯å¢ƒåé¦ˆçš„VLAæ¨¡å‹æµ‹è¯•æ—¶è®­ç»ƒæ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14666v1" onclick="toggleFavorite(this, '2512.14666v1', 'EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251213981v1-impact-of-robot-facial-audio-expressions-on-human-robot-trust-dynami.html">Impact of Robot Facial-Audio Expressions on Human Robot Trust Dynamics and Trust Repair</a></td>
  <td>ç ”ç©¶æœºå™¨äººé¢éƒ¨-éŸ³é¢‘è¡¨æƒ…å¯¹äººæœºä¿¡ä»»åŠ¨æ€åŠä¿®å¤çš„å½±å“ï¼Œé¢å‘å»ºç­‘è¡Œä¸šäººæœºåä½œã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13981v1" onclick="toggleFavorite(this, '2512.13981v1', 'Impact of Robot Facial-Audio Expressions on Human Robot Trust Dynamics and Trust Repair')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251214189v1-super-a-framework-for-sensitivity-based-uncertainty-aware-performanc.html">SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry</a></td>
  <td>SUPERï¼šåŸºäºæ•æ„Ÿåº¦çš„è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡æ€§èƒ½ä¸é£é™©è¯„ä¼°æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14189v1" onclick="toggleFavorite(this, '2512.14189v1', 'SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251214428v1-odyssey-an-automotive-lidar-inertial-odometry-dataset-for-gnss-denie.html">Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations</a></td>
  <td>Odysseyï¼šä¸ºGNSSæ‹’æ­¢ç¯å¢ƒæä¾›é«˜ç²¾åº¦æ¿€å…‰é›·è¾¾æƒ¯æ€§é‡Œç¨‹è®¡æ•°æ®é›†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14428v1" onclick="toggleFavorite(this, '2512.14428v1', 'Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251214340v1-field-evaluation-and-optimization-of-a-lightweight-lidar-based-uav-n.html">Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments</a></td>
  <td>æå‡ºä¸€ç§è½»é‡çº§æ¿€å…‰é›·è¾¾æ— äººæœºå¯¼èˆªç³»ç»Ÿï¼Œå¹¶ä¼˜åŒ–å…¶åœ¨ç¨ å¯†åŒ—æ–¹æ£®æ—ç¯å¢ƒä¸­çš„æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14340v1" onclick="toggleFavorite(this, '2512.14340v1', 'Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251214046v1-e-navi-environmental-adaptive-navigation-for-uavs-on-resource-constr.html">E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms</a></td>
  <td>E-Naviï¼šé¢å‘èµ„æºå—é™å¹³å°ï¼Œç¯å¢ƒè‡ªé€‚åº”æ— äººæœºå¯¼èˆªç³»ç»Ÿ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14046v1" onclick="toggleFavorite(this, '2512.14046v1', 'E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251214001v1-claim-camera-lidar-alignment-with-intensity-and-monodepth.html">CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth</a></td>
  <td>æå‡ºCLAIMï¼šä¸€ç§åˆ©ç”¨å•ç›®æ·±åº¦å’Œå¼ºåº¦ä¿¡æ¯çš„ç›¸æœº-æ¿€å…‰é›·è¾¾æ ‡å®šæ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14001v1" onclick="toggleFavorite(this, '2512.14001v1', 'CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251214189v1-super-a-framework-for-sensitivity-based-uncertainty-aware-performanc.html">SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry</a></td>
  <td>SUPERï¼šåŸºäºæ•æ„Ÿåº¦çš„è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡æ€§èƒ½ä¸é£é™©è¯„ä¼°æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14189v1" onclick="toggleFavorite(this, '2512.14189v1', 'SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251214428v1-odyssey-an-automotive-lidar-inertial-odometry-dataset-for-gnss-denie.html">Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations</a></td>
  <td>Odysseyï¼šé¢å‘GNSSæ‹’æ­¢ç¯å¢ƒçš„è½¦è½½æ¿€å…‰é›·è¾¾-æƒ¯æ€§é‡Œç¨‹è®¡æ•°æ®é›†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14428v1" onclick="toggleFavorite(this, '2512.14428v1', 'Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/251214001v1-claim-camera-lidar-alignment-with-intensity-and-monodepth.html">CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth</a></td>
  <td>CLAIMï¼šæå‡ºä¸€ç§åŸºäºå¼ºåº¦å’Œå•ç›®æ·±åº¦ä¿¡æ¯çš„ç›¸æœº-æ¿€å…‰é›·è¾¾æ ‡å®šæ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14001v1" onclick="toggleFavorite(this, '2512.14001v1', 'CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)