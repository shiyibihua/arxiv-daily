---
layout: default
title: Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning
---

# Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.16330" target="_blank" class="toolbar-btn">arXiv: 2511.16330v1</a>
    <a href="https://arxiv.org/pdf/2511.16330.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16330v1" 
            onclick="toggleFavorite(this, '2511.16330v1', 'Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Shreyas Kumar, Ravi Prakash

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-20

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫C-GMSÊ°ÜÊû∂ÔºåÈÄöËøáËÆ§ËØÅÂº∫ÂåñÂ≠¶‰π†ÂÆûÁé∞ÂÆâÂÖ®‰∏î‰ºòÂåñÁöÑÂèòÈòªÊäóÊéßÂà∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `ÂèòÈòªÊäóÊéßÂà∂` `Êú∫Âô®‰∫∫ÊéßÂà∂` `ÂÆâÂÖ®ÊÄß` `LyapunovÁ®≥ÂÆöÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫ÂèòÈòªÊäóÊéßÂà∂‰∏≠Â≠òÂú®‰∏çÁ®≥ÂÆöÊÄß‰∏é‰∏çÂÆâÂÖ®Êé¢Á¥¢ÁöÑÈ£éÈô©ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈòªÊäóÂ¢ûÁõäÈöèÊó∂Èó¥ÂèòÂåñÊó∂„ÄÇ
2. C-GMSÊ°ÜÊû∂ÈÄöËøáÂú®Á®≥ÂÆöÂ¢ûÁõäÊµÅÂΩ¢‰∏äÈááÊ†∑Á≠ñÁï•Ôºå‰ªéÊ†πÊú¨‰∏ä‰øùËØÅ‰∫ÜÁ≠ñÁï•ÁöÑLyapunovÁ®≥ÂÆöÊÄßÂíåÊâßË°åÂô®ÁöÑÂèØË°åÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåC-GMSÂú®‰ªøÁúüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÂùáË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊÄßËÉΩÔºå‰∏∫Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑËá™‰∏ª‰∫§‰∫íÊèê‰æõ‰∫Ü‰øùÈöú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ËÆ§ËØÅÈ´òÊñØÊµÅÂΩ¢ÈááÊ†∑ÔºàC-GMSÔºâÁöÑËΩ®Ëøπ‰∏≠ÂøÉÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁî®‰∫éÂ≠¶‰π†Âä®ÊÄÅËøêÂä®ÂéüËØ≠ÔºàDMPÔºâÂíåÂèòÈòªÊäóÊéßÂà∂ÔºàVICÔºâÁõ∏ÁªìÂêàÁöÑÁ≠ñÁï•ÔºåÂêåÊó∂‰øùËØÅLyapunovÁ®≥ÂÆöÊÄß‰ª•ÂèäÊâßË°åÂô®ÂèØË°åÊÄß„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÁ≠ñÁï•Êé¢Á¥¢ÈáçÊñ∞ÂÆö‰πâ‰∏∫‰ªéÊï∞Â≠¶ÂÆö‰πâÁöÑÁ®≥ÂÆöÂ¢ûÁõäË∞ÉÂ∫¶ÊµÅÂΩ¢‰∏≠ÈááÊ†∑„ÄÇËøôÁ°Æ‰øù‰∫ÜÊØè‰∏™Á≠ñÁï•rolloutÈÉΩÊòØÁ®≥ÂÆö‰∏îÁâ©ÁêÜ‰∏äÂèØÂÆûÁé∞ÁöÑÔºå‰ªéËÄåÊó†ÈúÄÂ•ñÂä±ÊÉ©ÁΩöÊàñ‰∫ãÂêéÈ™åËØÅ„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáÊèê‰æõ‰∫ÜÁêÜËÆ∫‰øùËØÅÔºåÂç≥‰ΩøÂú®Â≠òÂú®ÊúâÁïåÊ®°ÂûãËØØÂ∑ÆÂíåÈÉ®ÁΩ≤Êó∂Â≠òÂú®‰∏çÁ°ÆÂÆöÊÄßÁöÑÊÉÖÂÜµ‰∏ãÔºåËØ•ÊñπÊ≥ï‰πüËÉΩÁ°Æ‰øùÊúâÁïåÁöÑË∑üË∏™ËØØÂ∑Æ„ÄÇÈÄöËøá‰ªøÁúüÈ™åËØÅ‰∫ÜC-GMSÁöÑÊúâÊïàÊÄßÔºåÂπ∂Âú®ÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÈ™åËØÅ‰∫ÜÂÖ∂ÂäüÊïàÔºå‰∏∫Â§çÊùÇÁéØÂ¢É‰∏≠ÂèØÈù†ÁöÑËá™‰∏ª‰∫§‰∫íÈì∫Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®Êú∫Âô®‰∫∫ÂèòÈòªÊäóÊéßÂà∂‰∏≠ÔºåÁî±‰∫éÈòªÊäóÂ¢ûÁõäÁöÑÊó∂ÂèòÁâπÊÄßÔºåÂÆπÊòìÂØºËá¥Á≥ªÁªü‰∏çÁ®≥ÂÆöÂíå‰∏çÂÆâÂÖ®ÁöÑÊé¢Á¥¢„ÄÇ‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÈÄöËøáÂ•ñÂä±ÂáΩÊï∞Êàñ‰∫ãÂêéÈ™åËØÅÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩÜËøô‰∫õÊñπÊ≥ïÂπ∂‰∏çËÉΩ‰ªéÊ†πÊú¨‰∏ä‰øùËØÅÁ≥ªÁªüÁöÑÂÆâÂÖ®ÊÄß„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçËÉΩÂ§ü‰øùËØÅÁ≥ªÁªüÁ®≥ÂÆöÊÄßÂíåÂÆâÂÖ®ÊÄßÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÁ≠ñÁï•Êé¢Á¥¢ÈôêÂà∂Âú®‰∏Ä‰∏™Êï∞Â≠¶ÂÆö‰πâÁöÑÊµÅÂΩ¢‰∏äÔºåËØ•ÊµÅÂΩ¢‰∏äÁöÑÊâÄÊúâÁ≠ñÁï•ÈÉΩÊª°Ë∂≥LyapunovÁ®≥ÂÆöÊÄßÊù°‰ª∂ÂíåÊâßË°åÂô®ÂèØË°åÊÄßÊù°‰ª∂„ÄÇÈÄöËøáÂú®Ëøô‰∏™ÊµÅÂΩ¢‰∏äËøõË°åÈááÊ†∑ÔºåÂèØ‰ª•‰øùËØÅÊâÄÊúârolloutÁöÑÁ≠ñÁï•ÈÉΩÊòØÁ®≥ÂÆö‰∏îÁâ©ÁêÜ‰∏äÂèØÂÆûÁé∞ÁöÑÔºå‰ªéËÄåÈÅøÂÖç‰∫Ü‰∏çÂÆâÂÖ®Êé¢Á¥¢Âíå‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢ò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöC-GMSÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) Âü∫‰∫éÂä®ÊÄÅËøêÂä®ÂéüËØ≠ÔºàDMPÔºâÁöÑËøêÂä®ËßÑÂàíÊ®°ÂùóÔºõ2) Âü∫‰∫éÂèòÈòªÊäóÊéßÂà∂ÔºàVICÔºâÁöÑÂäõÊéßÂà∂Ê®°ÂùóÔºõ3) ËÆ§ËØÅÈ´òÊñØÊµÅÂΩ¢ÈááÊ†∑ÔºàC-GMSÔºâÊ®°ÂùóÔºåÁî®‰∫éÁîüÊàêÁ®≥ÂÆöÁöÑÈòªÊäóÂ¢ûÁõäË∞ÉÂ∫¶Ôºõ4) Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÁî®‰∫é‰ºòÂåñDMPÂíåVICÁöÑÂèÇÊï∞„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöÈ¶ñÂÖà‰ΩøÁî®DMPÁîüÊàêËøêÂä®ËΩ®ËøπÔºåÁÑ∂Âêé‰ΩøÁî®VICËøõË°åÂäõÊéßÂà∂ÔºåC-GMSÊ®°ÂùóÁîüÊàêÁ®≥ÂÆöÁöÑÈòªÊäóÂ¢ûÁõäÔºåÊúÄÂêé‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï‰ºòÂåñÊï¥‰∏™Á≥ªÁªüÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöC-GMSÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÁ≠ñÁï•Êé¢Á¥¢ÈôêÂà∂Âú®‰∏Ä‰∏™Á®≥ÂÆöÂ¢ûÁõäÊµÅÂΩ¢‰∏ä„ÄÇ‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰∏çÂêåÔºåC-GMS‰∏çÊòØÈÄöËøáÂ•ñÂä±ÂáΩÊï∞Êàñ‰∫ãÂêéÈ™åËØÅÊù•‰øùËØÅÁ≥ªÁªüÁöÑÂÆâÂÖ®ÊÄßÔºåËÄåÊòØÈÄöËøáÂú®ËÆæËÆ°‰∏ä‰øùËØÅÊâÄÊúâÁ≠ñÁï•ÈÉΩÊòØÁ®≥ÂÆöÁöÑ„ÄÇËøôÁßçÊñπÊ≥ïÂèØ‰ª•ÈÅøÂÖç‰∏çÂÆâÂÖ®Êé¢Á¥¢Âíå‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢òÔºå‰ªéËÄåÊèêÈ´òÁ≥ªÁªüÁöÑÂÆâÂÖ®ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèê‰æõ‰∫ÜÁêÜËÆ∫‰øùËØÅÔºåÂç≥‰ΩøÂú®Â≠òÂú®ÊúâÁïåÊ®°ÂûãËØØÂ∑ÆÂíåÈÉ®ÁΩ≤Êó∂Â≠òÂú®‰∏çÁ°ÆÂÆöÊÄßÁöÑÊÉÖÂÜµ‰∏ãÔºåËØ•ÊñπÊ≥ï‰πüËÉΩÁ°Æ‰øùÊúâÁïåÁöÑË∑üË∏™ËØØÂ∑Æ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöC-GMSÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) LyapunovÁ®≥ÂÆöÊÄßÊù°‰ª∂ÁöÑÊï∞Â≠¶Ë°®ËææÔºõ2) Á®≥ÂÆöÂ¢ûÁõäÊµÅÂΩ¢ÁöÑÂÆö‰πâÂíåÈááÊ†∑ÊñπÊ≥ïÔºõ3) Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÈÄâÊã©ÂíåÂèÇÊï∞Ë∞ÉÊï¥„ÄÇËÆ∫Êñá‰ΩøÁî®È´òÊñØËøáÁ®ãÊù•Ë°®Á§∫ÈòªÊäóÂ¢ûÁõäÔºåÂπ∂ÈÄöËøá‰ºòÂåñÈ´òÊñØËøáÁ®ãÁöÑÂèÇÊï∞Êù•‰øùËØÅLyapunovÁ®≥ÂÆöÊÄßÊù°‰ª∂„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøò‰ΩøÁî®‰∫Ü‰ø°‰ªªÂüüÁ≠ñÁï•‰ºòÂåñÔºàTRPOÔºâÁÆóÊ≥ïÊù•‰ºòÂåñDMPÂíåVICÁöÑÂèÇÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåC-GMSÂú®‰ªøÁúüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÂùáË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇÂú®‰ªøÁúüÁéØÂ¢É‰∏≠ÔºåC-GMSËÉΩÂ§üÂ≠¶‰π†Âà∞Á®≥ÂÆöÁöÑÈòªÊäóÂ¢ûÁõäË∞ÉÂ∫¶Ôºå‰ªéËÄåÂÆûÁé∞Á≤æÁ°ÆÁöÑÂäõÊéßÂà∂ÂíåËøêÂä®Ë∑üË∏™„ÄÇÂú®ÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÔºåC-GMSËÉΩÂ§üÂÆâÂÖ®Âú∞‰∏éÁéØÂ¢ÉËøõË°å‰∫§‰∫íÔºåÈÅøÂÖç‰∫Ü‰∏çÂÆâÂÖ®Êé¢Á¥¢Âíå‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÊú™Áü•Ôºå‰ΩÜËÆ∫ÊñáÂº∫Ë∞É‰∫ÜÂÖ∂Á®≥ÂÆöÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂÆâÂÖ®ÂèØÈù†‰∫∫Êú∫‰∫§‰∫íÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÔºöÂçè‰ΩúÊú∫Âô®‰∫∫„ÄÅÂåªÁñóÊú∫Âô®‰∫∫„ÄÅÂ∫∑Â§çÊú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøá‰øùËØÅÊú∫Âô®‰∫∫ÁöÑÁ®≥ÂÆöÊÄßÂíåÂÆâÂÖ®ÊÄßÔºåÂèØ‰ª•ÊèêÈ´ò‰∫∫Êú∫‰∫§‰∫íÁöÑÊïàÁéáÂíåÂÆâÂÖ®ÊÄßÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Â§çÊùÇ„ÄÅÊõ¥Êô∫ËÉΩÁöÑÊú∫Âô®‰∫∫Â∫îÁî®„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éÊõ¥ÂπøÊ≥õÁöÑÊú∫Âô®‰∫∫È¢ÜÂüüÔºå‰æãÂ¶ÇÔºöËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂà∂ÈÄ†Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments.

