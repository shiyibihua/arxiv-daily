---
layout: default
title: Implicit Constraint-Aware Off-Policy Correction for Offline Reinforcement Learning
---

# Implicit Constraint-Aware Off-Policy Correction for Offline Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14058" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14058v1</a>
  <a href="https://arxiv.org/pdf/2506.14058.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14058v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14058v1', 'Implicit Constraint-Aware Off-Policy Correction for Offline Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ali Baheri

**åˆ†ç±»**: eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéšå¼çº¦æŸæ„ŸçŸ¥çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ ¡æ­£æ–¹æ³•ä»¥è§£å†³ä»·å€¼è¿‡é«˜ä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `è´å°”æ›¼æ›´æ–°` `çº¦æŸæ„ŸçŸ¥` `ä»·å€¼ä¼°è®¡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¤„ç†ä»·å€¼è¿‡é«˜ä¼°è®¡å’Œè¿åé¢†åŸŸçŸ¥è¯†æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå½±å“äº†ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§éšå¼çº¦æŸæ„ŸçŸ¥çš„ç¦»çº¿ç­–ç•¥æ ¡æ­£æ–¹æ³•ï¼Œé€šè¿‡åœ¨è´å°”æ›¼æ›´æ–°ä¸­åµŒå…¥ç»“æ„å…ˆéªŒæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚
3. åœ¨åˆæˆçš„Bid-Clickæ‹å–å®éªŒä¸­ï¼Œæœ¬æ–‡æ–¹æ³•æ¶ˆé™¤äº†å•è°ƒæ€§è¿åï¼Œå¹¶åœ¨å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¯¹æ¯”ç®—æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä»…ä¾èµ–è®°å½•çš„äº¤äº’æ•°æ®è¿›è¡Œç­–ç•¥æ”¹è¿›ï¼Œä½†ç°æœ‰ç®—æ³•æ˜“å—åˆ°ä»·å€¼è¿‡é«˜ä¼°è®¡å’Œé¢†åŸŸçŸ¥è¯†è¿åï¼ˆå¦‚å•è°ƒæ€§æˆ–å…‰æ»‘æ€§ï¼‰çš„å½±å“ã€‚æœ¬æ–‡æå‡ºéšå¼çº¦æŸæ„ŸçŸ¥çš„ç¦»çº¿ç­–ç•¥æ ¡æ­£æ¡†æ¶ï¼Œå°†ç»“æ„å…ˆéªŒç›´æ¥åµŒå…¥æ¯ä¸ªè´å°”æ›¼æ›´æ–°ä¸­ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†æœ€ä¼˜è´å°”æ›¼ç®—å­ä¸å‡¸çº¦æŸé›†ä¸Šçš„è¿‘ç«¯æŠ•å½±ç›¸ç»“åˆï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ç®—å­ï¼Œè¯¥ç®—å­ä¿æŒ$Î³$-æ”¶ç¼©æ€§ï¼Œå…·æœ‰å”¯ä¸€çš„å›ºå®šç‚¹ï¼Œå¹¶ä¸¥æ ¼æ‰§è¡Œè§„å®šçš„ç»“æ„ã€‚åœ¨åˆæˆçš„Bid-Clickæ‹å–å®éªŒä¸­ï¼Œæœ¬æ–‡æ–¹æ³•æ¶ˆé™¤äº†æ‰€æœ‰å•è°ƒæ€§è¿åï¼Œå¹¶åœ¨å›æŠ¥ã€é—æ†¾å’Œæ ·æœ¬æ•ˆç‡ä¸Šè¶…è¶Šäº†ä¿å®ˆQå­¦ä¹ å’Œéšå¼Qå­¦ä¹ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­å­˜åœ¨çš„ä»·å€¼è¿‡é«˜ä¼°è®¡å’Œé¢†åŸŸçŸ¥è¯†è¿åçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¿™æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´ç­–ç•¥æ”¹è¿›æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºéšå¼çº¦æŸæ„ŸçŸ¥çš„ç¦»çº¿ç­–ç•¥æ ¡æ­£æ¡†æ¶ï¼Œé€šè¿‡å°†ç»“æ„å…ˆéªŒåµŒå…¥è´å°”æ›¼æ›´æ–°ï¼Œç»“åˆæœ€ä¼˜è´å°”æ›¼ç®—å­ä¸å‡¸çº¦æŸé›†çš„è¿‘ç«¯æŠ•å½±ï¼Œç¡®ä¿æ›´æ–°è¿‡ç¨‹ç¬¦åˆé¢„è®¾çš„ç»“æ„è¦æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è´å°”æ›¼ç®—å­çš„æ„å»ºã€è¿‘ç«¯æŠ•å½±çš„å®ç°å’Œä¼˜åŒ–å±‚çš„è®¾è®¡ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è´å°”æ›¼æ›´æ–°æ¨¡å—ã€çº¦æŸæŠ•å½±æ¨¡å—å’Œæ¢¯åº¦è®¡ç®—æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ç»“æ„å…ˆéªŒç›´æ¥åµŒå…¥è´å°”æ›¼æ›´æ–°ä¸­ï¼Œä½¿å¾—æ–°çš„ç®—å­ä¸ä»…ä¿æŒ$Î³$-æ”¶ç¼©æ€§ï¼Œè¿˜èƒ½ä¸¥æ ¼éµå¾ªå•è°ƒæ€§ç­‰çº¦æŸï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ä¼˜åŒ–å±‚çš„å¯å¾®æ€§ï¼Œä½¿å¾—åœ¨æ·±åº¦å‡½æ•°é€¼è¿‘å™¨ä¸­è®¡ç®—æ¢¯åº¦çš„æˆæœ¬ä¸éšå¼Qå­¦ä¹ ç›¸å½“ï¼ŒåŒæ—¶ç¡®ä¿æŠ•å½±è¿‡ç¨‹çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨åˆæˆçš„Bid-Clickæ‹å–å®éªŒä¸­ï¼Œæœ¬æ–‡æ–¹æ³•æˆåŠŸæ¶ˆé™¤äº†æ‰€æœ‰å•è°ƒæ€§è¿åï¼Œä¸”åœ¨å›æŠ¥ã€é—æ†¾å’Œæ ·æœ¬æ•ˆç‡ä¸Šå‡è¶…è¶Šäº†ä¿å®ˆQå­¦ä¹ å’Œéšå¼Qå­¦ä¹ ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åœ¨çº¿å¹¿å‘Šç«ä»·ã€æ¨èç³»ç»Ÿå’Œå…¶ä»–éœ€è¦ä»å†å²æ•°æ®ä¸­å­¦ä¹ çš„å†³ç­–ç³»ç»Ÿã€‚é€šè¿‡æ¶ˆé™¤ä»·å€¼è¿‡é«˜ä¼°è®¡å’Œéµå¾ªé¢†åŸŸçŸ¥è¯†ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜ç­–ç•¥çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline reinforcement learning promises policy improvement from logged interaction data alone, yet state-of-the-art algorithms remain vulnerable to value over-estimation and to violations of domain knowledge such as monotonicity or smoothness. We introduce implicit constraint-aware off-policy correction, a framework that embeds structural priors directly inside every Bellman update. The key idea is to compose the optimal Bellman operator with a proximal projection on a convex constraint set, which produces a new operator that (i) remains a $Î³$-contraction, (ii) possesses a unique fixed point, and (iii) enforces the prescribed structure exactly. A differentiable optimization layer solves the projection; implicit differentiation supplies gradients for deep function approximators at a cost comparable to implicit Q-learning. On a synthetic Bid-Click auction -- where the true value is provably monotone in the bid -- our method eliminates all monotonicity violations and outperforms conservative Q-learning and implicit Q-learning in return, regret, and sample efficiency.

