---
layout: default
title: RL-Guided MPC for Autonomous Greenhouse Control
---

# RL-Guided MPC for Autonomous Greenhouse Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13278" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13278v1</a>
  <a href="https://arxiv.org/pdf/2506.13278.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13278v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13278v1', 'RL-Guided MPC for Autonomous Greenhouse Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Salim Msaad, Murray Harraway, Robert D. McAllister

**åˆ†ç±»**: eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRLå¼•å¯¼çš„MPCä»¥ä¼˜åŒ–è‡ªä¸»æ¸©å®¤æ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¸©å®¤æ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `è‡ªä¸»ç³»ç»Ÿ` `ç»æµä¼˜åŒ–` `ä¸ç¡®å®šæ€§å¤„ç†` `æ™ºèƒ½å†œä¸š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¸©å®¤æ§åˆ¶æ–¹æ³•åœ¨å¤„ç†ä¸ç¡®å®šæ€§å’Œä¼˜åŒ–ç»æµæ•ˆç›Šæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆè¿è¥ã€‚
2. æœ¬æ–‡æå‡ºçš„RLå¼•å¯¼çš„MPCæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆRLå’ŒMPCçš„ä¼˜åŠ¿ï¼Œä¼˜åŒ–äº†æ¸©å®¤æ§åˆ¶çš„ç»ˆç«¯æˆæœ¬å’Œçº¦æŸæ¡ä»¶ã€‚
3. ä»¿çœŸç»“æœæ˜¾ç¤ºï¼ŒRLå¼•å¯¼çš„MPCåœ¨ç¡®å®šæ€§å’Œä¸ç¡®å®šæ€§ç¯å¢ƒä¸­å‡è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„RLå’ŒMPCæ–¹æ³•ï¼Œæå‡äº†æ§åˆ¶æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¸©å®¤çš„é«˜æ•ˆè¿è¥å¯¹äºæé«˜ä½œç‰©äº§é‡å’Œé™ä½èƒ½æºæˆæœ¬è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§æ§åˆ¶ç­–ç•¥ï¼Œå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ç›¸ç»“åˆï¼Œä»¥ä¼˜åŒ–è‡ªä¸»æ¸©å®¤çš„ç»æµæ•ˆç›Šã€‚ä»¥å¾€ç ”ç©¶åˆ†åˆ«æ¢è®¨äº†RLå’ŒMPCåœ¨æ¸©å®¤æ§åˆ¶ä¸­çš„åº”ç”¨ï¼Œæˆ–å°†MPCä½œä¸ºRLä»£ç†çš„å‡½æ•°é€¼è¿‘å™¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†RLå¼•å¯¼çš„MPCæ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒRLç­–ç•¥æ¥æ„å»ºMPCä¼˜åŒ–é—®é¢˜çš„ç»ˆç«¯æˆæœ¬å’Œç»ˆç«¯åŒºåŸŸçº¦æŸã€‚è¯¥æ–¹æ³•åˆ©ç”¨RLå¤„ç†ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ä¸MPCçš„åœ¨çº¿ä¼˜åŒ–ç›¸ç»“åˆï¼Œä»¥æé«˜æ•´ä½“æ§åˆ¶æ€§èƒ½ã€‚é€šè¿‡æ•°å€¼ä»¿çœŸå°†RLå¼•å¯¼çš„MPCä¸MPCå’ŒRLè¿›è¡Œäº†æ¯”è¾ƒï¼Œç»“æœè¡¨æ˜åœ¨ç¡®å®šæ€§å’Œä¸ç¡®å®šæ€§ç¯å¢ƒä¸­ï¼ŒRLå¼•å¯¼çš„MPCå‡ä¼˜äºRLå’ŒMPCï¼Œä¸”é¢„æµ‹æ—¶é—´æ›´çŸ­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¸©å®¤æ§åˆ¶æ–¹æ³•åœ¨å¤„ç†ä¸ç¡®å®šæ€§å’Œä¼˜åŒ–ç»æµæ•ˆç›Šæ–¹é¢çš„ä¸è¶³ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å®ç°é«˜æ•ˆçš„æ¸©å®¤è¿è¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„RLå¼•å¯¼çš„MPCæ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒå¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¥æ„å»ºMPCçš„ç»ˆç«¯æˆæœ¬å’Œçº¦æŸï¼Œä»è€Œç»“åˆRLçš„é€‚åº”æ€§ä¸MPCçš„ä¼˜åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬RLç­–ç•¥çš„è®­ç»ƒé˜¶æ®µå’ŒMPCä¼˜åŒ–é˜¶æ®µï¼ŒRLç­–ç•¥ç”¨äºç”Ÿæˆç»ˆç«¯æˆæœ¬å’Œçº¦æŸæ¡ä»¶ï¼ŒMPCåˆ™è¿›è¡Œå®æ—¶ä¼˜åŒ–æ§åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šRLå¼•å¯¼çš„MPCæ¡†æ¶æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒå°†RLä¸MPCæœ‰æ•ˆç»“åˆï¼Œå…‹æœäº†ä»¥å¾€æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡äº†æ§åˆ¶çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒRLç­–ç•¥çš„è®­ç»ƒé‡‡ç”¨äº†ç‰¹å®šçš„å¥–åŠ±å‡½æ•°ï¼ŒMPCçš„ä¼˜åŒ–åˆ™åŸºäºå®æ—¶è·å–çš„ç¯å¢ƒçŠ¶æ€ï¼Œç¡®ä¿äº†æ§åˆ¶å†³ç­–çš„åŠæ—¶æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRLå¼•å¯¼çš„MPCåœ¨ä¸¤ç§ç¯å¢ƒä¸‹å‡ä¼˜äºä¼ ç»Ÿçš„RLå’ŒMPCæ–¹æ³•ã€‚åœ¨ç¡®å®šæ€§ç¯å¢ƒä¸­ï¼ŒRLå¼•å¯¼çš„MPCçš„æ§åˆ¶æ€§èƒ½æå‡äº†çº¦15%ï¼Œè€Œåœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸­ï¼Œæå‡å¹…åº¦æ›´æ˜¯è¾¾åˆ°20%ã€‚æ­¤å¤–ï¼ŒRLå¼•å¯¼çš„MPCèƒ½å¤Ÿå®ç°æ›´çŸ­çš„é¢„æµ‹æ—¶é—´ï¼Œæå‡äº†å®æ—¶æ§åˆ¶çš„æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†œä¸šè‡ªåŠ¨åŒ–ã€æ™ºèƒ½æ¸©å®¤ç®¡ç†å’Œç¯å¢ƒæ§åˆ¶ç³»ç»Ÿã€‚é€šè¿‡ä¼˜åŒ–æ¸©å®¤çš„æ§åˆ¶ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ä½œç‰©çš„äº§é‡å’Œè´¨é‡ï¼ŒåŒæ—¶é™ä½èƒ½æºæ¶ˆè€—ï¼Œå…·æœ‰é‡è¦çš„ç»æµå’Œç¯å¢ƒä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„æ™ºèƒ½å†œä¸šè§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The efficient operation of greenhouses is essential for enhancing crop yield while minimizing energy costs. This paper investigates a control strategy that integrates Reinforcement Learning (RL) and Model Predictive Control (MPC) to optimize economic benefits in autonomous greenhouses. Previous research has explored the use of RL and MPC for greenhouse control individually, or by using MPC as the function approximator for the RL agent. This study introduces the RL-Guided MPC framework, where a RL policy is trained and then used to construct a terminal cost and terminal region constraint for the MPC optimization problem. This approach leverages the ability to handle uncertainties of RL with MPC's online optimization to improve overall control performance. The RL-Guided MPC framework is compared with both MPC and RL via numerical simulations. Two scenarios are considered: a deterministic environment and an uncertain environment. Simulation results demonstrate that, in both environments, RL-Guided MPC outperforms both RL and MPC with shorter prediction horizons.

