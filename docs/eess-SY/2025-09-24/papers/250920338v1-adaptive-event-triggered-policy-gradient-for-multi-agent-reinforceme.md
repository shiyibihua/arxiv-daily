---
layout: default
title: Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning
---

# Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20338" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20338v1</a>
  <a href="https://arxiv.org/pdf/2509.20338.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20338v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20338v1', 'Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Umer Siddique, Abhinav Sinha, Yongcan Cao

**åˆ†ç±»**: eess.SY, cs.AI, cs.MA, math.DS

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºET-MAPGå’ŒAET-MAPGï¼Œè§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­è®¡ç®—å’Œé€šä¿¡å¼€é”€å¤§çš„é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `äº‹ä»¶è§¦å‘æ§åˆ¶` `ç­–ç•¥æ¢¯åº¦` `æ³¨æ„åŠ›æœºåˆ¶` `é€šä¿¡ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¾èµ–å›ºå®šæ—¶é—´é—´éš”é‡‡æ ·å’Œé€šä¿¡ï¼Œè®¡ç®—å’Œé€šä¿¡å¼€é”€å¤§ã€‚
2. ET-MAPGè”åˆå­¦ä¹ æ§åˆ¶ç­–ç•¥å’Œäº‹ä»¶è§¦å‘ç­–ç•¥ï¼ŒAET-MAPGé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–æ™ºèƒ½ä½“é—´é€šä¿¡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿè½½å’Œé€šä¿¡å¼€é”€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº‹ä»¶è§¦å‘çš„å¤šæ™ºèƒ½ä½“ç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ET-MAPGï¼Œè¯¥æ¡†æ¶è”åˆå­¦ä¹ æ™ºèƒ½ä½“çš„æ§åˆ¶ç­–ç•¥å’Œäº‹ä»¶è§¦å‘ç­–ç•¥ã€‚ä¸è§£è€¦è¿™ä¸¤ç§æœºåˆ¶çš„å…ˆå‰å·¥ä½œä¸åŒï¼ŒET-MAPGå°†å®ƒä»¬é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œä½¿æ™ºèƒ½ä½“ä¸ä»…èƒ½å­¦ä¹ é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ï¼Œè¿˜èƒ½å­¦ä¹ ä½•æ—¶æ‰§è¡Œè¯¥è¡ŒåŠ¨ã€‚å¯¹äºæ™ºèƒ½ä½“é—´å­˜åœ¨é€šä¿¡çš„åœºæ™¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†AET-MAPGï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„å˜ä½“ï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥å­¦ä¹ é€‰æ‹©æ€§çš„é€šä¿¡æ¨¡å¼ã€‚AET-MAPGä½¿æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿç¡®å®šä½•æ—¶è§¦å‘è¡ŒåŠ¨ï¼Œè¿˜èƒ½ç¡®å®šä¸è°é€šä¿¡ä»¥åŠäº¤æ¢ä»€ä¹ˆä¿¡æ¯ï¼Œä»è€Œä¼˜åŒ–åä½œã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½å¯ä»¥ä¸ä»»ä½•ç­–ç•¥æ¢¯åº¦MARLç®—æ³•é›†æˆã€‚åœ¨å„ç§MARLåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°ä¸æœ€å…ˆè¿›çš„ã€æ—¶é—´è§¦å‘çš„åŸºçº¿ç›¸å½“çš„æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿè½½å’Œé€šä¿¡å¼€é”€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¼ ç»Ÿå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸é‡‡ç”¨æ—¶é—´è§¦å‘æœºåˆ¶ï¼Œå³æ™ºèƒ½ä½“ä»¥å›ºå®šçš„æ—¶é—´é—´éš”é‡‡æ ·åŠ¨ä½œå¹¶è¿›è¡Œé€šä¿¡ã€‚è¿™ç§æ–¹æ³•çš„ç¼ºç‚¹åœ¨äºï¼Œæ— è®ºç¯å¢ƒçŠ¶æ€æ˜¯å¦å‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œæ™ºèƒ½ä½“éƒ½ä¼šè¿›è¡Œè®¡ç®—å’Œé€šä¿¡ï¼Œå¯¼è‡´å¤§é‡çš„è®¡ç®—èµ„æºå’Œé€šä¿¡å¸¦å®½è¢«æµªè´¹ã€‚å› æ­¤ï¼Œå¦‚ä½•é™ä½å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„è®¡ç®—å’Œé€šä¿¡å¼€é”€æ˜¯ä¸€ä¸ªé‡è¦çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥äº‹ä»¶è§¦å‘æœºåˆ¶ï¼Œè®©æ™ºèƒ½ä½“æ ¹æ®ç¯å¢ƒçŠ¶æ€çš„å˜åŒ–è‡ªé€‚åº”åœ°å†³å®šä½•æ—¶æ‰§è¡ŒåŠ¨ä½œå’Œè¿›è¡Œé€šä¿¡ã€‚å…·ä½“æ¥è¯´ï¼Œæ™ºèƒ½ä½“å­¦ä¹ ä¸€ä¸ªäº‹ä»¶è§¦å‘ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å†³å®šäº†åœ¨ä»€ä¹ˆæƒ…å†µä¸‹è§¦å‘åŠ¨ä½œçš„æ‰§è¡Œå’Œä¿¡æ¯çš„é€šä¿¡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ™ºèƒ½ä½“å¯ä»¥åœ¨ç¯å¢ƒçŠ¶æ€å˜åŒ–è¾ƒå°æ—¶å‡å°‘è®¡ç®—å’Œé€šä¿¡ï¼Œä»è€Œé™ä½æ•´ä½“å¼€é”€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šET-MAPGæ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šæ§åˆ¶ç­–ç•¥æ¨¡å—å’Œäº‹ä»¶è§¦å‘ç­–ç•¥æ¨¡å—ã€‚æ§åˆ¶ç­–ç•¥æ¨¡å—è´Ÿè´£å­¦ä¹ æ™ºèƒ½ä½“çš„æœ€ä¼˜åŠ¨ä½œï¼Œäº‹ä»¶è§¦å‘ç­–ç•¥æ¨¡å—è´Ÿè´£å†³å®šä½•æ—¶è§¦å‘åŠ¨ä½œçš„æ‰§è¡Œã€‚AET-MAPGåœ¨ET-MAPGçš„åŸºç¡€ä¸Šå¢åŠ äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå­¦ä¹ æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡æ¨¡å¼ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œæ™ºèƒ½ä½“é¦–å…ˆæ ¹æ®å½“å‰çŠ¶æ€å’Œäº‹ä»¶è§¦å‘ç­–ç•¥åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡ŒåŠ¨ä½œæˆ–é€šä¿¡ã€‚å¦‚æœéœ€è¦ï¼Œåˆ™æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œæˆ–é€šä¿¡ï¼Œå¹¶æ›´æ–°æ§åˆ¶ç­–ç•¥å’Œäº‹ä»¶è§¦å‘ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†äº‹ä»¶è§¦å‘æœºåˆ¶ä¸å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œå¹¶æå‡ºäº†ET-MAPGå’ŒAET-MAPGä¸¤ç§ç®—æ³•ã€‚ä¸ä»¥å¾€å°†æ§åˆ¶ç­–ç•¥å’Œäº‹ä»¶è§¦å‘ç­–ç•¥è§£è€¦çš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡å°†å®ƒä»¬é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„å­¦ä¹ æ¡†æ¶ä¸­ï¼Œä½¿å¾—æ™ºèƒ½ä½“å¯ä»¥åŒæ—¶å­¦ä¹ å¦‚ä½•è¡ŒåŠ¨å’Œä½•æ—¶è¡ŒåŠ¨ã€‚æ­¤å¤–ï¼ŒAET-MAPGé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡æ¨¡å¼ï¼Œè¿›ä¸€æ­¥æé«˜äº†åä½œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šET-MAPGå’ŒAET-MAPGå¯ä»¥ä¸ä»»ä½•ç­–ç•¥æ¢¯åº¦MARLç®—æ³•é›†æˆã€‚äº‹ä»¶è§¦å‘ç­–ç•¥é€šå¸¸ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå‚æ•°åŒ–ï¼Œå…¶è¾“å…¥æ˜¯æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚æµ‹ï¼Œè¾“å‡ºæ˜¯è§¦å‘æ¦‚ç‡ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ§åˆ¶ç­–ç•¥çš„æŸå¤±å’Œäº‹ä»¶è§¦å‘ç­–ç•¥çš„æŸå¤±ï¼Œå…¶ä¸­äº‹ä»¶è§¦å‘ç­–ç•¥çš„æŸå¤±æ—¨åœ¨é¼“åŠ±æ™ºèƒ½ä½“åœ¨å¿…è¦æ—¶æ‰è§¦å‘åŠ¨ä½œå’Œé€šä¿¡ã€‚AET-MAPGä¸­çš„æ³¨æ„åŠ›æœºåˆ¶é‡‡ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå­¦ä¹ æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒET-MAPGå’ŒAET-MAPGåœ¨å¤šä¸ªMARLåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¸æœ€å…ˆè¿›çš„æ—¶é—´è§¦å‘æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿè½½å’Œé€šä¿¡å¼€é”€ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œè®¡ç®—é‡å’Œé€šä¿¡é‡é™ä½äº†20%-50%ï¼Œè€Œæ€§èƒ½æ²¡æœ‰æ˜æ˜¾ä¸‹é™ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººé›†ç¾¤æ§åˆ¶ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿã€æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½è®¡ç®—å’Œé€šä¿¡å¼€é”€ï¼Œå¯ä»¥æé«˜ç³»ç»Ÿçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”æ›´å¤æ‚çš„ç¯å¢ƒå’Œæ›´å¤§è§„æ¨¡çš„æ™ºèƒ½ä½“ç¾¤ä½“ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸Šéƒ¨ç½²å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Conventional multi-agent reinforcement learning (MARL) methods rely on time-triggered execution, where agents sample and communicate actions at fixed intervals. This approach is often computationally expensive and communication-intensive. To address this limitation, we propose ET-MAPG (Event-Triggered Multi-Agent Policy Gradient reinforcement learning), a framework that jointly learns an agent's control policy and its event-triggering policy. Unlike prior work that decouples these mechanisms, ET-MAPG integrates them into a unified learning process, enabling agents to learn not only what action to take but also when to execute it. For scenarios with inter-agent communication, we introduce AET-MAPG, an attention-based variant that leverages a self-attention mechanism to learn selective communication patterns. AET-MAPG empowers agents to determine not only when to trigger an action but also with whom to communicate and what information to exchange, thereby optimizing coordination. Both methods can be integrated with any policy gradient MARL algorithm. Extensive experiments across diverse MARL benchmarks demonstrate that our approaches achieve performance comparable to state-of-the-art, time-triggered baselines while significantly reducing both computational load and communication overhead.

