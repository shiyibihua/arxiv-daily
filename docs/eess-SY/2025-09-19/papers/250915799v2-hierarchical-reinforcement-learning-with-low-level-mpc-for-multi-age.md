---
layout: default
title: Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control
---

# Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15799" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15799v2</a>
  <a href="https://arxiv.org/pdf/2509.15799.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15799v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15799v2', 'Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Max Studt, Georg Schildbach

**åˆ†ç±»**: eess.SY, cs.AI, cs.RO, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19 (æ›´æ–°: 2025-10-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåˆ†å±‚å¼ºåŒ–å­¦ä¹ ä¸ä½å±‚MPCçš„å¤šæ™ºèƒ½ä½“æ§åˆ¶æ–¹æ³•ï¼Œæå‡å®‰å…¨æ€§ä¸ååŒæ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åˆ†å±‚å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å®‰å…¨æ§åˆ¶` `ååŒæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç«¯åˆ°ç«¯å­¦ä¹ åœ¨å¤æ‚ç¯å¢ƒä¸­æ ·æœ¬æ•ˆç‡ä½ï¼ŒåŸºäºæ¨¡å‹çš„æ–¹æ³•æ³›åŒ–èƒ½åŠ›å¼±ï¼Œéš¾ä»¥å®ç°å®‰å…¨ååŒæ§åˆ¶ã€‚
2. é‡‡ç”¨åˆ†å±‚æ¡†æ¶ï¼Œé«˜å±‚å¼ºåŒ–å­¦ä¹ è¿›è¡Œæˆ˜æœ¯å†³ç­–ï¼Œä½å±‚æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ä¿è¯å®‰å…¨è¿åŠ¨ã€‚
3. åœ¨æ•é£Ÿè€…-çŒç‰©ç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¥–åŠ±ã€å®‰å…¨æ€§å’Œä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰ç«¯åˆ°ç«¯å’ŒåŸºäºå±è”½çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åŠ¨æ€ã€çº¦æŸä¸°å¯Œçš„ç¯å¢ƒä¸­å®ç°å®‰å…¨å’ŒååŒè¡Œä¸ºä»ç„¶æ˜¯åŸºäºå­¦ä¹ çš„æ§åˆ¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚çº¯ç²¹çš„ç«¯åˆ°ç«¯å­¦ä¹ é€šå¸¸å­˜åœ¨æ ·æœ¬æ•ˆç‡ä½å’Œå¯é æ€§æœ‰é™çš„é—®é¢˜ï¼Œè€ŒåŸºäºæ¨¡å‹çš„æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰çš„å‚è€ƒï¼Œå¹¶ä¸”éš¾ä»¥æ³›åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œæˆ˜æœ¯å†³ç­–ä¸é€šè¿‡æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰è¿›è¡Œä½å±‚æ‰§è¡Œç›¸ç»“åˆã€‚å¯¹äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œè¿™æ„å‘³ç€é«˜å±‚ç­–ç•¥ä»ç»“æ„åŒ–çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ä¸­é€‰æ‹©æŠ½è±¡ç›®æ ‡ï¼Œè€ŒMPCç¡®ä¿åŠ¨æ€å¯è¡Œå’Œå®‰å…¨çš„è¿åŠ¨ã€‚åœ¨æ•é£Ÿè€…-çŒç‰©åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¥–åŠ±ã€å®‰å…¨æ€§å’Œä¸€è‡´æ€§æ–¹é¢ä¼˜äºç«¯åˆ°ç«¯å’ŒåŸºäºå±è”½çš„RLåŸºçº¿ï¼Œçªå‡ºäº†å°†ç»“æ„åŒ–å­¦ä¹ ä¸åŸºäºæ¨¡å‹çš„æ§åˆ¶ç›¸ç»“åˆçš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œåœ¨åŠ¨æ€ã€çº¦æŸä¸°å¯Œçš„ç¯å¢ƒä¸­å®ç°å®‰å…¨å’ŒååŒæ§åˆ¶çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ï¼Œæ ·æœ¬æ•ˆç‡ä½ä¸‹ä¸”å¯é æ€§æœ‰é™ï¼›è€ŒåŸºäºæ¨¡å‹çš„æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰çš„å‚è€ƒè½¨è¿¹ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­ä¿è¯æ™ºèƒ½ä½“çš„å®‰å…¨æ€§å’ŒååŒæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ç›¸ç»“åˆï¼Œæ„å»ºä¸€ä¸ªåˆ†å±‚æ§åˆ¶æ¡†æ¶ã€‚é«˜å±‚RLè´Ÿè´£æˆ˜æœ¯å†³ç­–ï¼Œé€‰æ‹©æŠ½è±¡ç›®æ ‡ï¼›ä½å±‚MPCè´Ÿè´£æ‰§è¡Œï¼Œä¿è¯åŠ¨æ€å¯è¡Œæ€§å’Œå®‰å…¨æ€§ã€‚è¿™ç§åˆ†å±‚ç»“æ„èƒ½å¤Ÿç»“åˆRLçš„æ³›åŒ–èƒ½åŠ›å’ŒMPCçš„ç²¾ç¡®æ§åˆ¶èƒ½åŠ›ï¼Œä»è€Œåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°å®‰å…¨å’ŒååŒçš„æ§åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé«˜å±‚ç­–ç•¥å’Œä½å±‚MPCã€‚é«˜å±‚ç­–ç•¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»ç»“æ„åŒ–çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ä¸­é€‰æ‹©æŠ½è±¡ç›®æ ‡ã€‚ä½å±‚MPCæ¥æ”¶é«˜å±‚ç­–ç•¥çš„ç›®æ ‡ï¼Œå¹¶ç”Ÿæˆæ»¡è¶³åŠ¨æ€çº¦æŸå’Œå®‰å…¨çº¦æŸçš„æ§åˆ¶æŒ‡ä»¤ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šç¯å¢ƒçŠ¶æ€è¾“å…¥é«˜å±‚RLç­–ç•¥ï¼Œè¾“å‡ºç›®æ ‡ç‚¹ï¼Œç›®æ ‡ç‚¹è¾“å…¥ä½å±‚MPCï¼ŒMPCè¾“å‡ºæ§åˆ¶æŒ‡ä»¤ä½œç”¨äºç¯å¢ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å¼ºåŒ–å­¦ä¹ å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶æœ‰æœºç»“åˆï¼Œå½¢æˆä¸€ä¸ªåˆ†å±‚æ§åˆ¶æ¡†æ¶ã€‚è¿™ç§åˆ†å±‚ç»“æ„èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨RLçš„æ³›åŒ–èƒ½åŠ›å’ŒMPCçš„ç²¾ç¡®æ§åˆ¶èƒ½åŠ›ï¼Œä»è€Œåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°å®‰å…¨å’ŒååŒçš„æ§åˆ¶ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ ·æœ¬æ•ˆç‡å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦é¢„å®šä¹‰çš„å‚è€ƒè½¨è¿¹ï¼Œèƒ½å¤Ÿé€‚åº”æ›´å¤æ‚çš„ç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šé«˜å±‚RLç­–ç•¥ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œè¡¨ç¤ºï¼Œé‡‡ç”¨åˆé€‚çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOæˆ–SACï¼‰è¿›è¡Œè®­ç»ƒã€‚ä½å±‚MPCä½¿ç”¨ç²¾ç¡®çš„åŠ¨åŠ›å­¦æ¨¡å‹å’Œçº¦æŸæ¡ä»¶ï¼Œé€šè¿‡æ±‚è§£ä¼˜åŒ–é—®é¢˜ç”Ÿæˆæ§åˆ¶æŒ‡ä»¤ã€‚ROIçš„è®¾è®¡éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œä»¥ä¿è¯é«˜å±‚ç­–ç•¥èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„æŠ½è±¡ç›®æ ‡ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦åŒæ—¶è€ƒè™‘å¥–åŠ±ã€å®‰å…¨æ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ•é£Ÿè€…-çŒç‰©åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¥–åŠ±ã€å®‰å…¨æ€§å’Œä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºå±è”½çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°é¿å…ç¢°æ’å’Œä¿æŒååŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€æ— äººæœºé›†ç¾¤æ§åˆ¶ç­‰é¢†åŸŸã€‚é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ çš„å†³ç­–èƒ½åŠ›å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶çš„ç²¾ç¡®æ‰§è¡Œï¼Œå¯ä»¥æå‡æ™ºèƒ½ä½“åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„å®‰å…¨æ€§ã€å¯é æ€§å’ŒååŒæ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving safe and coordinated behavior in dynamic, constraint-rich environments remains a major challenge for learning-based control. Pure end-to-end learning often suffers from poor sample efficiency and limited reliability, while model-based methods depend on predefined references and struggle to generalize. We propose a hierarchical framework that combines tactical decision-making via reinforcement learning (RL) with low-level execution through Model Predictive Control (MPC). For the case of multi-agent systems this means that high-level policies select abstract targets from structured regions of interest (ROIs), while MPC ensures dynamically feasible and safe motion. Tested on a predator-prey benchmark, our approach outperforms end-to-end and shielding-based RL baselines in terms of reward, safety, and consistency, underscoring the benefits of combining structured learning with model-based control.

