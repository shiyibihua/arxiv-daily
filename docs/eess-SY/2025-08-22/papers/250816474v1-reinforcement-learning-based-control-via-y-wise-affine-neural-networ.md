---
layout: default
title: Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)
---

# Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16474" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.16474v1</a>
  <a href="https://arxiv.org/pdf/2508.16474.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16474v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16474v1', 'Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Austin Braniff, Yuhe Tian

**ÂàÜÁ±ª**: eess.SY, cs.LG, math.OC

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-22

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éY-wise‰ªøÂ∞ÑÁ•ûÁªèÁΩëÁªúÁöÑÂº∫ÂåñÂ≠¶‰π†ÊéßÂà∂ÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `‰ªøÂ∞ÑÁ•ûÁªèÁΩëÁªú` `ÊéßÂà∂ÁÆóÊ≥ï` `ÈùûÁ∫øÊÄßÁ≥ªÁªü` `ÂÆâÂÖ®ÊÄß` `Â§öÂèÇÊï∞ÊéßÂà∂` `Âú®Á∫øÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®Â§çÊùÇÈùûÁ∫øÊÄßÁ≥ªÁªüÊéßÂà∂‰∏≠Èù¢‰∏¥Êî∂ÊïõÈÄüÂ∫¶ÊÖ¢ÂíåÂÆâÂÖ®ÊÄß‰∏çË∂≥ÁöÑÊåëÊàò„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÈÄöËøáY-wise‰ªøÂ∞ÑÁ•ûÁªèÁΩëÁªúÂàùÂßãÂåñÂº∫ÂåñÂ≠¶‰π†ÁΩëÁªúÔºå‰ª•ÁªìÂêàÁ∫øÊÄßÊúÄ‰ºòÊéßÂà∂ÁöÑ‰ºòÂäøÔºåÊèêÂçáÂ≠¶‰π†ÊïàÁéáÂíåÂÆâÂÖ®ÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåYANN-RLÂú®Â§πÊåÅÊëÜÂíåÂÆâÂÖ®ÂÖ≥ÈîÆÂåñÂ≠¶ÂèçÂ∫îÁ≥ªÁªü‰∏≠ÊòæËëó‰ºò‰∫éÁé∞‰ª£Ê∑±Â∫¶Á°ÆÂÆöÊÄßÁ≠ñÁï•Ê¢ØÂ∫¶ÁÆóÊ≥ïÔºåÂ∞§ÂÖ∂Âú®ÂÆâÂÖ®Á∫¶Êùü‰∏ãË°®Áé∞Á™ÅÂá∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éY-wise‰ªøÂ∞ÑÁ•ûÁªèÁΩëÁªúÔºàYANNsÔºâÁöÑÊñ∞ÂûãÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁÆóÊ≥ï„ÄÇYANNsËÉΩÂ§üÁ≤æÁ°ÆË°®Á§∫‰ªªÊÑèËæìÂÖ•ÂíåËæìÂá∫Áª¥Â∫¶ÁöÑÂ∑≤Áü•ÂàÜÊÆµ‰ªøÂ∞ÑÂáΩÊï∞ÔºåÈÄÇÁî®‰∫éÂ§öÂèÇÊï∞Á∫øÊÄßÊ®°ÂûãÈ¢ÑÊµãÊéßÂà∂ÁöÑÊòæÂºèËß£„ÄÇÈÄöËøáÂ∞ÜYANNsÁî®‰∫éÂàùÂßãÂåñRLÁöÑÊºîÂëòÂíåËØÑËÆ∫ÂÆ∂ÁΩëÁªúÔºåYANN-RLÊéßÂà∂ÁÆóÊ≥ïËÉΩÂ§ü‰ª•Á∫øÊÄßÊúÄ‰ºòÊéßÂà∂ÁöÑ‰ø°ÂøÉÂºÄÂßã„ÄÇYANN-ÊºîÂëòÈÄöËøáÁ¶ªÁ∫øËÆ°ÁÆóËé∑ÂæóÁöÑÂ§öÂèÇÊï∞ÊéßÂà∂Ëß£ËøõË°åÂàùÂßãÂåñÔºåËÄåYANN-ËØÑËÆ∫ÂÆ∂ÂàôË°®Á§∫Á∫øÊÄßÁ≥ªÁªüÁöÑÁä∂ÊÄÅ-Âä®‰ΩúÂÄºÂáΩÊï∞ÂíåÊúÄ‰ºòÊéßÂà∂ÈóÆÈ¢ò‰∏≠ÁöÑÂ•ñÂä±ÂáΩÊï∞„ÄÇÈ¢ùÂ§ñÁöÑÁΩëÁªúÂ±ÇË¢´Ê≥®ÂÖ•‰ª•Êâ©Â±ïYANNs‰ª•ÈÄÇÂ∫îÈùûÁ∫øÊÄßË°®ËææÔºåËÉΩÂ§üÂú®Á∫øËÆ≠ÁªÉ‰ª•Áõ¥Êé•‰∏éÂ§çÊùÇÈùûÁ∫øÊÄßÁ≥ªÁªü‰∫§‰∫í„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåYANN-RLÂú®ËÄÉËôëÂÆâÂÖ®Á∫¶ÊùüÊó∂ÊòæËëó‰ºò‰∫éÁé∞‰ª£RLÁÆóÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§πÊåÅÊëÜÂíåÂÆâÂÖ®ÂÖ≥ÈîÆÂåñÂ≠¶ÂèçÂ∫îÁ≥ªÁªüÁöÑÂ∫îÁî®‰∏≠„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨Á†îÁ©∂Êó®Âú®Ëß£ÂÜ≥‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†Âú®Â§çÊùÇÈùûÁ∫øÊÄßÁ≥ªÁªüÊéßÂà∂‰∏≠ÁöÑÊî∂ÊïõÈÄüÂ∫¶ÊÖ¢ÂíåÂÆâÂÖ®ÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄ‰æùËµñ‰∫éÊ∑±Â∫¶Â≠¶‰π†ÔºåÂØºËá¥Âú®ÂÆâÂÖ®ÂÖ≥ÈîÆÂ∫îÁî®‰∏≠ÁöÑÊÄßËÉΩ‰∏çÁ®≥ÂÆö„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Y-wise‰ªøÂ∞ÑÁ•ûÁªèÁΩëÁªúÔºàYANNsÔºâÊù•ÂàùÂßãÂåñÂº∫ÂåñÂ≠¶‰π†ÁöÑÊºîÂëòÂíåËØÑËÆ∫ÂÆ∂ÁΩëÁªúÔºå‰ªéËÄåÁªìÂêàÁ∫øÊÄßÊúÄ‰ºòÊéßÂà∂ÁöÑ‰ºòÂäøÔºåÊèê‰æõÊõ¥Âø´ÁöÑÊî∂ÊïõÂíåÊõ¥È´òÁöÑÂÆâÂÖ®ÊÄß„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåYANN-RLÁÆóÊ≥ïËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÊúâÊïàÂ≠¶‰π†„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨YANN-ÊºîÂëòÂíåYANN-ËØÑËÆ∫ÂÆ∂‰∏§‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇYANN-ÊºîÂëòÈÄöËøáÁ¶ªÁ∫øËÆ°ÁÆóÁöÑÂ§öÂèÇÊï∞ÊéßÂà∂Ëß£ËøõË°åÂàùÂßãÂåñÔºåËÄåYANN-ËØÑËÆ∫ÂÆ∂ÂàôË°®Á§∫Áä∂ÊÄÅ-Âä®‰ΩúÂÄºÂáΩÊï∞ÂíåÂ•ñÂä±ÂáΩÊï∞„ÄÇÈ¢ùÂ§ñÁöÑÁΩëÁªúÂ±ÇÁî®‰∫éÊâ©Â±ïYANNs‰ª•ÈÄÇÂ∫îÈùûÁ∫øÊÄßË°®ËææÔºåÊîØÊåÅÂú®Á∫øËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éYANNsÁöÑÂºïÂÖ•Ôºå‰ΩøÂæóÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïËÉΩÂ§üÂú®ÂàùÂßãÈò∂ÊÆµÂ∞±ÂÖ∑Â§áÁ∫øÊÄßÊúÄ‰ºòÊéßÂà∂ÁöÑ‰ø°ÂøÉÔºå‰ªéËÄåÂä†ÈÄüÂ≠¶‰π†ËøáÁ®ãÂπ∂ÊèêÈ´òÂÆâÂÖ®ÊÄß„ÄÇËøô‰∏é‰º†ÁªüÊñπÊ≥ïÁöÑÈöèÊú∫ÂàùÂßãÂåñÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁΩëÁªúÁªìÊûÑ‰∏äÔºåYANNsËÉΩÂ§üÁ≤æÁ°ÆË°®Á§∫ÂàÜÊÆµ‰ªøÂ∞ÑÂáΩÊï∞ÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏∫ÊúÄÂ∞èÂåñÁä∂ÊÄÅ-Âä®‰ΩúÂÄºÂáΩÊï∞ÁöÑËØØÂ∑Æ„ÄÇÊ≠§Â§ñÔºåÁΩëÁªúÂ±ÇÁöÑÊâ©Â±ïËÆæËÆ°ÂÖÅËÆ∏YANNsÂú®‰∏éÂ§çÊùÇÈùûÁ∫øÊÄßÁ≥ªÁªü‰∫§‰∫íÊó∂ËøõË°åÂú®Á∫øÂ≠¶‰π†ÔºåÊèêÂçá‰∫ÜÁÆóÊ≥ïÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåYANN-RLÂú®Â§πÊåÅÊëÜÂíåÂÆâÂÖ®ÂÖ≥ÈîÆÂåñÂ≠¶ÂèçÂ∫îÁ≥ªÁªü‰∏≠ÁöÑË°®Áé∞ÊòæËëó‰ºò‰∫éÁé∞‰ª£Ê∑±Â∫¶Á°ÆÂÆöÊÄßÁ≠ñÁï•Ê¢ØÂ∫¶ÁÆóÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØÂú®ËÄÉËôëÂÆâÂÖ®Á∫¶ÊùüÊó∂ÔºåÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%„ÄÇËøô‰∏ÄÁªìÊûúÈ™åËØÅ‰∫ÜYANN-RLÂú®Â§çÊùÇÈùûÁ∫øÊÄßÊéßÂà∂‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂà∂ÈÄ†Á≠âÂÆâÂÖ®ÂÖ≥ÈîÆÁ≥ªÁªü„ÄÇÈÄöËøáÁªìÂêàÁ∫øÊÄßÊúÄ‰ºòÊéßÂà∂ÁöÑ‰ºòÂäøÔºåYANN-RLÁÆóÊ≥ïËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆûÁé∞È´òÊïà„ÄÅÂÆâÂÖ®ÁöÑÊéßÂà∂Á≠ñÁï•ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This work presents a novel reinforcement learning (RL) algorithm based on Y-wise Affine Neural Networks (YANNs). YANNs provide an interpretable neural network which can exactly represent known piecewise affine functions of arbitrary input and output dimensions defined on any amount of polytopic subdomains. One representative application of YANNs is to reformulate explicit solutions of multi-parametric linear model predictive control. Built on this, we propose the use of YANNs to initialize RL actor and critic networks, which enables the resulting YANN-RL control algorithm to start with the confidence of linear optimal control. The YANN-actor is initialized by representing the multi-parametric control solutions obtained via offline computation using an approximated linear system model. The YANN-critic represents the explicit form of the state-action value function for the linear system and the reward function as the objective in an optimal control problem (OCP). Additional network layers are injected to extend YANNs for nonlinear expressions, which can be trained online by directly interacting with the true complex nonlinear system. In this way, both the policy and state-value functions exactly represent a linear OCP initially and are able to eventually learn the solution of a general nonlinear OCP. Continuous policy improvement is also implemented to provide heuristic confidence that the linear OCP solution serves as an effective lower bound to the performance of RL policy. The YANN-RL algorithm is demonstrated on a clipped pendulum and a safety-critical chemical-reactive system. Our results show that YANN-RL significantly outperforms the modern RL algorithm using deep deterministic policy gradient, especially when considering safety constraints.

