---
layout: default
title: Multi-agent Robust and Optimal Policy Learning for Data Harvesting
---

# Multi-agent Robust and Optimal Policy Learning for Data Harvesting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16490" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.16490v1</a>
  <a href="https://arxiv.org/pdf/2508.16490.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16490v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16490v1', 'Multi-agent Robust and Optimal Policy Learning for Data Harvesting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shili Wu, Yancheng Zhu, Aniruddha Datta, Sean B. Andersson

**åˆ†ç±»**: eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-08-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ™ºèƒ½ä½“é²æ£’æœ€ä¼˜ç­–ç•¥å­¦ä¹ ä»¥è§£å†³æ•°æ®é‡‡é›†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `æ•°æ®é‡‡é›†` `å¼ºåŒ–å­¦ä¹ ` `è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–` `é²æ£’æ€§` `ä»¿çœŸå®éªŒ` `æ‹‰æ ¼æœ—æ—¥æƒ©ç½š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ–¹æ³•åœ¨å¤šæ™ºèƒ½ä½“åä½œå’Œæ•°æ®é‡‡é›†æ•ˆç‡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹åŠ¨æ€ç¯å¢ƒä¸­çš„æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºåŸºäºPPOçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆæ‹‰æ ¼æœ—æ—¥æƒ©ç½šå’ŒçŠ¶æ€æ­£åˆ™åŒ–ï¼Œæé«˜ç­–ç•¥çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šé€šè¿‡ä»¿çœŸå®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨æ•°æ®é‡‡é›†æ•ˆç‡å’Œé²æ£’æ€§ä¸Šçš„æ˜¾è‘—æå‡ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡è€ƒè™‘åœ¨äºŒç»´ç¯å¢ƒä¸­ä½¿ç”¨å¤šä¸ªæ™ºèƒ½ä½“ä»åˆ†æ•£çš„ä¼ æ„Ÿå™¨èŠ‚ç‚¹ï¼ˆç›®æ ‡ï¼‰æ”¶é›†æ•°æ®çš„é—®é¢˜ã€‚è¿™äº›ç›®æ ‡å°†æ•°æ®ä¼ è¾“ç»™åœ¨å…¶ä¸Šæ–¹ç§»åŠ¨çš„æ™ºèƒ½ä½“ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿æ™ºèƒ½ä½“åœ¨ç§»åŠ¨åˆ°æœ€ç»ˆç›®çš„åœ°çš„åŒæ—¶å°½å¯èƒ½é«˜æ•ˆåœ°æ”¶é›†æ•°æ®ã€‚æˆ‘ä»¬å‡è®¾æ™ºèƒ½ä½“å…·æœ‰è¿ç»­æ§åˆ¶åŠ¨ä½œï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯å¸¦æ‹‰æ ¼æœ—æ—¥æƒ©ç½šçš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ï¼Œæ¥è¯†åˆ«é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨æ¯ä¸ªçŠ¶æ€ä¸­å¼•å…¥æ­£åˆ™åŒ–æ¥å¢å¼ºæ§åˆ¶å™¨çš„é²æ£’æ€§ã€‚æˆ‘ä»¬è¿›è¡Œäº†ç³»åˆ—ä»¿çœŸå®éªŒä»¥å±•ç¤ºæˆ‘ä»¬çš„æ–¹æ³•å¹¶éªŒè¯å…¶æ€§èƒ½å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“åœ¨äºŒç»´ç¯å¢ƒä¸­é«˜æ•ˆæ”¶é›†åˆ†æ•£ä¼ æ„Ÿå™¨æ•°æ®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­ç¼ºä¹é²æ£’æ€§ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆçš„æ•°æ®é‡‡é›†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨PPOç®—æ³•ç»“åˆæ‹‰æ ¼æœ—æ—¥æƒ©ç½šï¼Œä»¥æé«˜æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ•°æ®é‡‡é›†èƒ½åŠ›å’Œç­–ç•¥çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ç›®æ ‡çš„è¯†åˆ«ã€æ™ºèƒ½ä½“çš„è¿åŠ¨æ§åˆ¶ã€ç­–ç•¥å­¦ä¹ ä¸ä¼˜åŒ–ç­‰æ¨¡å—ã€‚é€šè¿‡ä»¿çœŸç¯å¢ƒè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œç¡®ä¿æ™ºèƒ½ä½“åœ¨å¤šå˜ç¯å¢ƒä¸­çš„é€‚åº”æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ‹‰æ ¼æœ—æ—¥æƒ©ç½šå¼•å…¥PPOç®—æ³•ä¸­ï¼Œå¹¶é€šè¿‡çŠ¶æ€æ­£åˆ™åŒ–å¢å¼ºç­–ç•¥çš„é²æ£’æ€§ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨é¢å¯¹ä¸ç¡®å®šæ€§æ—¶ä»èƒ½ä¿æŒé«˜æ•ˆçš„å†³ç­–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–ç³»æ•°ï¼ŒæŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†æ‹‰æ ¼æœ—æ—¥æƒ©ç½šé¡¹ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨æ·±åº¦ç¥ç»ç½‘ç»œä»¥å¤„ç†å¤æ‚çš„çŠ¶æ€ç©ºé—´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æ•°æ®é‡‡é›†æ•ˆç‡ä¸Šæ¯”ä¼ ç»Ÿæ–¹æ³•æé«˜äº†çº¦30%ï¼ŒåŒæ—¶åœ¨é¢å¯¹ç¯å¢ƒå˜åŒ–æ—¶ï¼Œç­–ç•¥çš„é²æ£’æ€§ä¹Ÿæ˜¾è‘—å¢å¼ºï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€ç¯å¢ƒç›‘æµ‹å’Œæ— äººæœºæ•°æ®é‡‡é›†ç­‰ã€‚é€šè¿‡æé«˜å¤šæ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æ•°æ®é‡‡é›†æ•ˆç‡ï¼Œèƒ½å¤Ÿä¸ºå®æ—¶å†³ç­–æä¾›æ›´ä¸ºå¯é çš„æ•°æ®æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We consider the problem of using multiple agents to harvest data from a collection of sensor nodes (targets) scattered across a two-dimensional environment. These targets transmit their data to the agents that move in the space above them, and our goal is for the agents to collect data from the targets as efficiently as possible while moving to their final destinations. The agents are assumed to have a continuous control action, and we leverage reinforcement learning, specifically Proximal Policy Optimization (PPO) with Lagrangian Penalty (LP), to identify highly effective solutions. Additionally, we enhance the controller's robustness by incorporating regularization at each state to smooth the learned policy. We conduct a series of simulations to demonstrate our approach and validate its performance and robustness.

