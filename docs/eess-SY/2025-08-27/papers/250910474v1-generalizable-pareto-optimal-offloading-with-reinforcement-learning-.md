---
layout: default
title: Generalizable Pareto-Optimal Offloading with Reinforcement Learning in Mobile Edge Computing
---

# Generalizable Pareto-Optimal Offloading with Reinforcement Learning in Mobile Edge Computing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10474" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.10474v1</a>
  <a href="https://arxiv.org/pdf/2509.10474.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10474v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10474v1', 'Generalizable Pareto-Optimal Offloading with Reinforcement Learning in Mobile Edge Computing')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ning Yang, Junrui Wen, Meng Zhang, Ming Tang

**ÂàÜÁ±ª**: eess.SY

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-27

**Â§áÊ≥®**: 28 pages including appendix, 7 figures, 2 tables, accepted to IEEE Transactions on Services Computing

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/gracefulning/Generalizable-Pareto-Optimal-Offloading-with-Reinforcement-Learning-in-Mobile-Edge-Computing)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÈÄöÁî®Â∏ïÁ¥ØÊâòÊúÄ‰ºòÂç∏ËΩΩÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥ÁßªÂä®ËæπÁºòËÆ°ÁÆóÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÁßªÂä®ËæπÁºòËÆ°ÁÆó` `Â§öÁõÆÊ†á‰ºòÂåñ` `Âº∫ÂåñÂ≠¶‰π†` `‰ªªÂä°Âç∏ËΩΩ` `Ê∑±Â∫¶Â≠¶‰π†` `ËÉΩÊïà` `Âª∂Ëøü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂçïÁõÆÊ†áË∞ÉÂ∫¶ÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜÁßªÂä®ËæπÁºòËÆ°ÁÆó‰∏≠Â§öÁõÆÊ†á‰ºòÂåñÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÅèÂ•ΩÊú™Áü•ÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ
2. Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÈÄöÁî®Â§öÁõÆÊ†áÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÁöÑÂç∏ËΩΩÊ°ÜÊû∂ÔºåËÉΩÂ§üÈÄÇÂ∫î‰∏çÂêåÁöÑMECÁ≥ªÁªüÂπ∂È´òÊïàË∞ÉÂ∫¶‰ªªÂä°„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêGMORLÊñπÊ°àÂú®Ë∂Ö‰ΩìÁßØ‰∏äËæÉÂü∫Á∫øÊèêÂçá‰∫Ü121.0%ÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊîπËøõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁßªÂä®ËæπÁºòËÆ°ÁÆóÔºàMECÔºâÊòØ‰∏ã‰∏Ä‰ª£ÁßªÂä®ÁΩëÁªúÂ∫îÁî®ÁöÑÈáçË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºåÈúÄÂπ≥Ë°°Âª∂ËøüÂíåËÉΩÊïàÁ≠âÂ§öÁßçÊÄßËÉΩÊåáÊ†á„ÄÇÁÑ∂ËÄåÔºå‰º†ÁªüÁöÑÂçïÁõÆÊ†áË∞ÉÂ∫¶ÊñπÊ°àÊó†Ê≥ïÁõ¥Êé•Â∫îÁî®‰∫éÂÆûÈôÖÁ≥ªÁªüÔºåÂõ†‰∏∫‰∏çÂêåÁõÆÊ†áÁöÑÊùÉÈáçÂæÄÂæÄÊú™Áü•ÊàñÈöæ‰ª•ÊèêÂâçÊåáÂÆö„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÁõÆÊ†áÂç∏ËΩΩÈóÆÈ¢òÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊúÄÂ∞èÂåñÈ¢ÑÊúüÁöÑÈïøÊúüËÉΩËÄóÂíåÂª∂ËøüÔºåÂêåÊó∂ËÄÉËôëÊú™Áü•ÁöÑÂÅèÂ•Ω„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÈÄöÁî®Â§öÁõÆÊ†áÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºàGMORLÔºâÁöÑ‰ªªÂä°Âç∏ËΩΩÊ°ÜÊû∂ÔºåÈááÁî®Á¶ªÊï£ËΩØÊºîÂëò-ËØÑËÆ∫ÂÆ∂ÔºàDiscrete-SACÔºâÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Âçï‰∏ÄÁ≠ñÁï•Ê®°ÂûãÈ´òÊïàË∞ÉÂ∫¶‰ªªÂä°ÔºåÈÄÇÂ∫î‰∏çÂêåÁöÑMECÁ≥ªÁªü„ÄÇÈÄöËøáÂºïÂÖ•Áõ¥ÊñπÂõæÁä∂ÊÄÅÁºñÁ†Å„ÄÅÂ§çÊùÇÁöÑÂ•ñÂä±ÂáΩÊï∞ÂíåÊñ∞È¢ñÁöÑÁ•ûÁªèÁΩëÁªúÊû∂ÊûÑÔºåÊàë‰ª¨ÁöÑGMORLÊñπÊ°àÂú®Ë∂Ö‰ΩìÁßØ‰∏äÊèêÂçá‰∫ÜÈ´òËææ121.0%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨Á†îÁ©∂Êó®Âú®Ëß£ÂÜ≥ÁßªÂä®ËæπÁºòËÆ°ÁÆó‰∏≠ÁöÑÂ§öÁõÆÊ†áÂç∏ËΩΩÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏çÂêåÁõÆÊ†áÊùÉÈáçÊú™Áü•ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰º†ÁªüÊñπÊ≥ïÈöæ‰ª•ÈÄÇÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫Âü∫‰∫éÈÄöÁî®Â§öÁõÆÊ†áÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÂçï‰∏ÄÁ≠ñÁï•Ê®°ÂûãÈÄÇÂ∫î‰∏çÂêåÁöÑÂÅèÂ•ΩÂíåMECÁ≥ªÁªüÔºåÂÆûÁé∞È´òÊïàÁöÑ‰ªªÂä°Ë∞ÉÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Áä∂ÊÄÅÁºñÁ†ÅÊ®°Âùó„ÄÅÂ•ñÂä±ËÆ°ÁÆóÊ®°ÂùóÂíåÁ≠ñÁï•Â≠¶‰π†Ê®°Âùó„ÄÇÁä∂ÊÄÅÁºñÁ†Å‰ΩøÁî®Áõ¥ÊñπÂõæÊñπÊ≥ïÔºåÂ•ñÂä±ÂáΩÊï∞Á≤æÁ°ÆËÆ°ÁÆóÂª∂ËøüÂíåËÉΩËÄóÁöÑÊïàÁî®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂºïÂÖ•‰∫ÜÁõ¥ÊñπÂõæÁä∂ÊÄÅÁºñÁ†ÅÂíåÂ§çÊùÇÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÂ§öÁõÆÊ†á‰ºòÂåñÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÈááÁî®Á¶ªÊï£ËΩØÊºîÂëò-ËØÑËÆ∫ÂÆ∂ÔºàDiscrete-SACÔºâÊñπÊ≥ïÔºåËÆæËÆ°‰∫ÜÈÄÇÂ∫îÊÄßÂº∫ÁöÑÁ•ûÁªèÁΩëÁªúÊû∂ÊûÑÔºå‰ºòÂåñ‰∫Ü‰ªªÂä°Ë∞ÉÂ∫¶ÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Âú®ÂÆûÈ™å‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÈ™åËØÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêGMORLÊñπÊ°àÂú®Ë∂Ö‰ΩìÁßØ‰∏äËæÉÂü∫Á∫øÊèêÂçá‰∫Ü121.0%ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂ§öÁõÆÊ†á‰ºòÂåñÁöÑÊïàÊûúÔºåËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÁßªÂä®ËæπÁºòËÆ°ÁÆóÈ¢ÜÂüüÔºåÂ∞§ÂÖ∂ÊòØÂú®Êô∫ËÉΩÊâãÊú∫„ÄÅÁâ©ËÅîÁΩëËÆæÂ§áÂíåËæπÁºòÊúçÂä°Âô®Á≠âÂú∫ÊôØ‰∏≠„ÄÇÈÄöËøá‰ºòÂåñ‰ªªÂä°Âç∏ËΩΩÁ≠ñÁï•ÔºåÂèØ‰ª•ÊúâÊïàÊèêÂçáÁ≥ªÁªüÁöÑËÉΩÊïàÂíåÂìçÂ∫îÈÄüÂ∫¶ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Mobile edge computing (MEC) is essential for next-generation mobile network applications that prioritize various performance metrics, including delays and energy efficiency. However, conventional single-objective scheduling solutions cannot be directly applied to practical systems in which the preferences (i.e., the weights of different objectives) are often unknown or challenging to specify in advance. In this study, we formulate a multi-objective offloading problem for MEC with multiple edges to minimize the sum of expected long-term energy consumption and delay while considering unknown preferences. To address the challenge of unknown preferences and the potentially diverse MEC systems, we propose a generalizable multi-objective (deep) reinforcement learning (GMORL)-based tasks offloading framework, which employs the Discrete Soft Actor-Critic (Discrete-SAC) method. Our method uses a single policy model to efficiently schedule tasks based on varying preferences and adapt to heterogeneous MEC systems with different CPU frequencies and server quantities. Under the proposed framework, we introduce a histogram-based state encoding method for constructing features for multiple edges in MEC systems, a sophisticated reward function for accurately computing the utilities of delay and energy consumption, and a novel neural network architecture for improving generalization. Simulation results demonstrate that our proposed GMORL scheme enhances the hypervolume of the Pareto front by up to $121.0\%$ compared to benchmarks. Our code are avavilable at https://github.com/gracefulning/Generalizable-Pareto-Optimal-Offloading-with-Reinforcement-Learning-in-Mobile-Edge-Computing

