---
layout: default
title: Computationally efficient Gauss-Newton reinforcement learning for model predictive control
---

# Computationally efficient Gauss-Newton reinforcement learning for model predictive control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02441" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02441v1</a>
  <a href="https://arxiv.org/pdf/2508.02441.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02441v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02441v1', 'Computationally efficient Gauss-Newton reinforcement learning for model predictive control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dean Brandner, Sebastien Gros, Sergio Lucia

**åˆ†ç±»**: eess.SY, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04

**å¤‡æ³¨**: 14 pages, 8 figures, submitted to Elsevier

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆçš„Gauss-Newtonå¼ºåŒ–å­¦ä¹ ä»¥ä¼˜åŒ–æ¨¡å‹é¢„æµ‹æ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `Gauss-Newton` `ç­–ç•¥ä¼˜åŒ–` `åŠ¨æ€ç³»ç»Ÿ` `æ•°æ®æ•ˆç‡` `æ”¶æ•›é€Ÿåº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¤šä¾èµ–ä¸€é˜¶æ›´æ–°ï¼Œæ”¶æ•›é€Ÿåº¦æ…¢ä¸”åœ¨è§£å†³MPCæ—¶æ•ˆç‡ä½ä¸‹ã€‚
2. æå‡ºGauss-Newtonè¿‘ä¼¼ç­–ç•¥Hessianï¼Œæ¶ˆé™¤å¯¹äºŒé˜¶å¯¼æ•°çš„éœ€æ±‚ï¼Œå®ç°è¶…çº¿æ€§æ”¶æ•›ã€‚
3. åœ¨CSTRå®éªŒä¸­ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„å¿«é€Ÿæ”¶æ•›å’Œæ•°æ®æ•ˆç‡ä¼˜äºç°æœ‰ä¸€é˜¶æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰å› å…¶å¯è§£é‡Šæ€§å’Œå¤„ç†çº¦æŸçš„èƒ½åŠ›è€Œå¹¿æ³›åº”ç”¨äºè¿‡ç¨‹æ§åˆ¶ã€‚ä½œä¸ºå¼ºåŒ–å­¦ä¹ ä¸­çš„å‚æ•°åŒ–ç­–ç•¥ï¼ŒMPCç›¸æ¯”äºç¥ç»ç½‘ç»œç­‰é»‘ç®±ç­–ç•¥å…·æœ‰è¾ƒå¼ºçš„åˆå§‹æ€§èƒ½å’Œè¾ƒä½çš„æ•°æ®éœ€æ±‚ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºä¸€é˜¶æ›´æ–°ï¼Œè™½ç„¶åœ¨å¤§å‚æ•°ç©ºé—´ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†æ”¶æ•›é€Ÿåº¦é€šå¸¸ä»…ä¸ºçº¿æ€§ï¼Œå¯¼è‡´åœ¨æ¯æ¬¡ç­–ç•¥æ›´æ–°éƒ½éœ€è§£å†³æœ€ä¼˜æ§åˆ¶é—®é¢˜æ—¶æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§Gauss-Newtonè¿‘ä¼¼çš„ç¡®å®šæ€§ç­–ç•¥Hessianï¼Œæ¶ˆé™¤äº†å¯¹äºŒé˜¶ç­–ç•¥å¯¼æ•°çš„éœ€æ±‚ï¼Œä»è€Œå®ç°è¶…çº¿æ€§æ”¶æ•›å¹¶ä¿æŒè¾ƒä½çš„è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºåŠ¨é‡çš„Hessianå¹³å‡æ–¹æ¡ˆï¼Œä»¥æé«˜åœ¨å™ªå£°ä¼°è®¡ä¸‹çš„è®­ç»ƒç¨³å®šæ€§ã€‚é€šè¿‡åœ¨éçº¿æ€§è¿ç»­æ…æ‹Œç½ååº”å™¨ï¼ˆCSTRï¼‰ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦å’Œæ•°æ®æ•ˆç‡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„ä¸€é˜¶æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ¨¡å‹é¢„æµ‹æ§åˆ¶ä¸­çš„ä½æ•ˆé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¯æ¬¡ç­–ç•¥æ›´æ–°éœ€è¦è§£å†³æœ€ä¼˜æ§åˆ¶é—®é¢˜æ—¶çš„è®¡ç®—å¼€é”€å’Œæ”¶æ•›é€Ÿåº¦ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥Gauss-Newtonè¿‘ä¼¼ï¼Œé¿å…äº†å¯¹äºŒé˜¶ç­–ç•¥å¯¼æ•°çš„è®¡ç®—ï¼Œä»è€Œå®ç°äº†æ›´é«˜æ•ˆçš„ç­–ç•¥æ›´æ–°ï¼Œæå‡äº†æ”¶æ•›é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬ç­–ç•¥æ›´æ–°çš„Gauss-Newtonè¿‘ä¼¼è®¡ç®—ã€åŠ¨é‡Hessianå¹³å‡æ–¹æ¡ˆä»¥åŠåœ¨éçº¿æ€§CSTRç¯å¢ƒä¸­çš„åº”ç”¨ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç­–ç•¥è¯„ä¼°ã€Hessianè®¡ç®—å’Œæ›´æ–°æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡Gauss-Newtonè¿‘ä¼¼å®ç°äº†å¯¹äºŒé˜¶å¯¼æ•°çš„æ— éœ€æ±‚ï¼Œä»è€Œä½¿å¾—ç­–ç•¥æ›´æ–°èƒ½å¤Ÿå®ç°è¶…çº¿æ€§æ”¶æ•›ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŠ¨é‡å¹³å‡ç­–ç•¥ä»¥å¢å¼ºè®­ç»ƒçš„ç¨³å®šæ€§ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šåˆ™è€ƒè™‘äº†å™ªå£°å½±å“ï¼Œç¡®ä¿åœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çš„æœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨CSTRå®éªŒä¸­å®ç°äº†æ¯”æœ€å…ˆè¿›çš„ä¸€é˜¶æ–¹æ³•æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œæ•°æ®æ•ˆç‡æ˜¾è‘—æé«˜ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å·¥ä¸šè¿‡ç¨‹æ§åˆ¶ã€æœºå™¨äººæ§åˆ¶ä»¥åŠè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç³»ç»Ÿçš„æ§åˆ¶æ•ˆç‡å’Œç¨³å®šæ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¤æ‚çš„åŠ¨æ€ç³»ç»Ÿä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½æ§åˆ¶æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model predictive control (MPC) is widely used in process control due to its interpretability and ability to handle constraints. As a parametric policy in reinforcement learning (RL), MPC offers strong initial performance and low data requirements compared to black-box policies like neural networks. However, most RL methods rely on first-order updates, which scale well to large parameter spaces but converge at most linearly, making them inefficient when each policy update requires solving an optimal control problem, as is the case with MPC. While MPC policies are typically sparsely parameterized and thus amenable to second-order approaches, existing second-order methods demand second-order policy derivatives, which can be computationally and memory-wise intractable.
>   This work introduces a Gauss-Newton approximation of the deterministic policy Hessian that eliminates the need for second-order policy derivatives, enabling superlinear convergence with minimal computational overhead. To further improve robustness, we propose a momentum-based Hessian averaging scheme for stable training under noisy estimates. We demonstrate the effectiveness of the approach on a nonlinear continuously stirred tank reactor (CSTR), showing faster convergence and improved data efficiency over state-of-the-art first-order methods.

