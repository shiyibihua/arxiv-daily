---
layout: default
title: MM-LMPC: Multi-Modal Learning Model Predictive Control via Bandit-Based Mode Selection
---

# MM-LMPC: Multi-Modal Learning Model Predictive Control via Bandit-Based Mode Selection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00410" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00410v1</a>
  <a href="https://arxiv.org/pdf/2510.00410.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00410v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00410v1', 'MM-LMPC: Multi-Modal Learning Model Predictive Control via Bandit-Based Mode Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wataru Hashimoto, Kazumune Hashimoto

**åˆ†ç±»**: eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**å¤‡æ³¨**: This paper is submitted to 2026 American Control Conference (ACC)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºBanditæ¨¡å¼é€‰æ‹©çš„å¤šæ¨¡æ€å­¦ä¹ æ¨¡å‹é¢„æµ‹æ§åˆ¶(MM-LMPC)ï¼Œè§£å†³LMPCæ¢ç´¢ä¸è¶³é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `è¿­ä»£å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `Banditç®—æ³•` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸLMPCä¾èµ–åˆå§‹è½¨è¿¹ï¼Œæ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œå¯¼è‡´çŠ¶æ€ç©ºé—´æ¢ç´¢ä¸è¶³ã€‚
2. MM-LMPCå°†è½¨è¿¹èšç±»ä¸ºå¤šä¸ªæ¨¡å¼ï¼Œç»´æŠ¤æ¨¡å¼ç‰¹å®šçš„ç»ˆç«¯é›†å’Œä»·å€¼å‡½æ•°ã€‚
3. é‡‡ç”¨åŸºäºBanditçš„å…ƒæ§åˆ¶å™¨å¹³è¡¡æ¨¡å¼æ¢ç´¢ä¸åˆ©ç”¨ï¼Œæå‡å…¨å±€å¯»ä¼˜èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å­¦ä¹ æ¨¡å‹é¢„æµ‹æ§åˆ¶(LMPC)é€šè¿‡åˆ©ç”¨å…ˆå‰æ‰§è¡Œçš„æ•°æ®æ¥æé«˜è¿­ä»£ä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒLMPCä»è¿‡å»çš„è½¨è¿¹æ„å»ºä¸€ä¸ªé‡‡æ ·çš„å®‰å…¨é›†ï¼Œå¹¶å°†å…¶ç”¨ä½œç»ˆç«¯çº¦æŸï¼Œç»ˆç«¯ä»£ä»·ç”±ç›¸åº”çš„cost-to-goç»™å‡ºã€‚è™½ç„¶æœ‰æ•ˆï¼Œä½†LMPCä¸¥é‡ä¾èµ–äºåˆå§‹è½¨è¿¹ï¼šå…·æœ‰é«˜cost-to-goçš„çŠ¶æ€å¾ˆå°‘è¢«é€‰æ‹©ä¸ºåæœŸè¿­ä»£ä¸­çš„ç»ˆç«¯å€™é€‰ï¼Œå¯¼è‡´éƒ¨åˆ†çŠ¶æ€ç©ºé—´æœªè¢«æ¢ç´¢ï¼Œå¹¶å¯èƒ½é”™è¿‡æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼Œåœ¨å…·æœ‰ä¸¤æ¡å¯èƒ½è·¯å¾„çš„reach-avoidä»»åŠ¡ä¸­ï¼ŒLMPCå¯èƒ½ä¸æ–­æ”¹è¿›æœ€åˆè¾ƒçŸ­çš„è·¯å¾„ï¼Œè€Œå¿½ç•¥äº†å¯èƒ½å¯¼è‡´å…¨å±€æ›´å¥½è§£å†³æ–¹æ¡ˆçš„æ›¿ä»£è·¯å¾„ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€LMPC(MM-LMPC)ï¼Œå®ƒå°†è¿‡å»çš„è½¨è¿¹èšç±»æˆæ¨¡å¼ï¼Œå¹¶ç»´æŠ¤ç‰¹å®šäºæ¨¡å¼çš„ç»ˆç«¯é›†å’Œä»·å€¼å‡½æ•°ã€‚ä¸€ä¸ªåŸºäºBanditçš„å…ƒæ§åˆ¶å™¨ï¼Œé‡‡ç”¨ä¸‹ç½®ä¿¡ç•Œ(LCB)ç­–ç•¥ï¼Œå¹³è¡¡äº†è·¨æ¨¡å¼çš„æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä»è€Œèƒ½å¤Ÿç³»ç»Ÿåœ°æ”¹è¿›æ‰€æœ‰æ¨¡å¼ã€‚è¿™ä½¿å¾—MM-LMPCèƒ½å¤Ÿé€ƒè„±é«˜ä»£ä»·çš„å±€éƒ¨æœ€ä¼˜è§£ï¼Œå¹¶å‘ç°å…¨å±€æ›´ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å»ºç«‹äº†é€’å½’å¯è¡Œæ€§ã€é—­ç¯ç¨³å®šæ€§ã€æ¸è¿‘æ”¶æ•›åˆ°æœ€ä½³æ¨¡å¼ä»¥åŠå¯¹æ•°åæ‚”ç•Œã€‚åœ¨é¿éšœä»»åŠ¡ä¸Šçš„ä»¿çœŸéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æ€§èƒ½æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šLMPCåœ¨è¿­ä»£å­¦ä¹ æ§åˆ¶ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ€§èƒ½é«˜åº¦ä¾èµ–äºåˆå§‹è½¨è¿¹ã€‚å¦‚æœåˆå§‹è½¨è¿¹è´¨é‡ä¸é«˜ï¼ŒLMPCå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œæ— æ³•å……åˆ†æ¢ç´¢çŠ¶æ€ç©ºé—´ï¼Œä»è€Œé”™å¤±å…¨å±€æœ€ä¼˜è§£ã€‚ä¾‹å¦‚ï¼Œåœ¨å­˜åœ¨å¤šæ¡å¯è¡Œè·¯å¾„çš„ä»»åŠ¡ä¸­ï¼ŒLMPCå¯èƒ½è¿‡åº¦ä¼˜åŒ–åˆå§‹é€‰æ‹©çš„è·¯å¾„ï¼Œè€Œå¿½ç•¥äº†å…¶ä»–æ½œåœ¨çš„æ›´ä¼˜è·¯å¾„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMM-LMPCçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è¿‡å»çš„è½¨è¿¹æ•°æ®èšç±»æˆå¤šä¸ªä¸åŒçš„â€œæ¨¡å¼â€ï¼Œæ¯ä¸ªæ¨¡å¼ä»£è¡¨ä¸€ç§ä¸åŒçš„è¡Œä¸ºç­–ç•¥æˆ–è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ç»´æŠ¤æ¯ä¸ªæ¨¡å¼ç‰¹å®šçš„ç»ˆç«¯é›†å’Œä»·å€¼å‡½æ•°ï¼ŒMM-LMPCèƒ½å¤ŸåŒæ—¶æ¢ç´¢å¤šä¸ªæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œé¿å…è¿‡æ—©åœ°æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMM-LMPCçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è½¨è¿¹èšç±»ï¼šä½¿ç”¨èšç±»ç®—æ³•ï¼ˆå¦‚k-meansï¼‰å°†å†å²è½¨è¿¹æ•°æ®åˆ’åˆ†æˆå¤šä¸ªæ¨¡å¼ã€‚2) æ¨¡å¼ç‰¹å®šçš„ç»ˆç«¯é›†å’Œä»·å€¼å‡½æ•°ï¼šä¸ºæ¯ä¸ªæ¨¡å¼ç»´æŠ¤ä¸€ä¸ªç»ˆç«¯é›†ï¼Œè¯¥é›†åˆåŒ…å«è¯¥æ¨¡å¼ä¸‹è¡¨ç°è‰¯å¥½çš„çŠ¶æ€ã€‚åŒæ—¶ï¼Œä¸ºæ¯ä¸ªæ¨¡å¼å­¦ä¹ ä¸€ä¸ªä»·å€¼å‡½æ•°ï¼Œç”¨äºè¯„ä¼°è¯¥æ¨¡å¼ä¸‹çŠ¶æ€çš„ä¼˜åŠ£ã€‚3) åŸºäºBanditçš„å…ƒæ§åˆ¶å™¨ï¼šä½¿ç”¨Banditç®—æ³•ï¼ˆå¦‚UCBæˆ–LCBï¼‰æ¥é€‰æ‹©åœ¨å½“å‰è¿­ä»£ä¸­åº”è¯¥æ¢ç´¢å“ªä¸ªæ¨¡å¼ã€‚å…ƒæ§åˆ¶å™¨æ ¹æ®æ¯ä¸ªæ¨¡å¼çš„æ€§èƒ½å’Œæ¢ç´¢ç¨‹åº¦ï¼Œå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä»è€Œç³»ç»Ÿåœ°æ”¹è¿›æ‰€æœ‰æ¨¡å¼ã€‚4) æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼šåŸºäºé€‰å®šçš„æ¨¡å¼ï¼Œä½¿ç”¨æ¨¡å‹é¢„æµ‹æ§åˆ¶ç”Ÿæˆæ§åˆ¶åºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šMM-LMPCçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºBanditçš„å…ƒæ§åˆ¶å™¨æ¥ç®¡ç†å¤šä¸ªæ¨¡å¼çš„æ¢ç´¢å’Œåˆ©ç”¨ã€‚ä¸ä¼ ç»Ÿçš„LMPCåªå…³æ³¨å•ä¸€è½¨è¿¹ä¸åŒï¼ŒMM-LMPCèƒ½å¤ŸåŒæ—¶æ¢ç´¢å¤šä¸ªæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚æ­¤å¤–ï¼ŒåŸºäºBanditçš„å…ƒæ§åˆ¶å™¨èƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªæ¨¡å¼çš„æ¢ç´¢æ¦‚ç‡ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å†å²æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šMM-LMPCçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨k-meansç®—æ³•è¿›è¡Œè½¨è¿¹èšç±»ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„kå€¼ã€‚2) ä½¿ç”¨ä¸‹ç½®ä¿¡ç•Œ(LCB)ç­–ç•¥ä½œä¸ºBanditç®—æ³•ï¼Œå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ã€‚LCBç­–ç•¥æ ¹æ®æ¯ä¸ªæ¨¡å¼çš„å¹³å‡å¥–åŠ±å’Œæ¢ç´¢æ¬¡æ•°ï¼Œè®¡ç®—ä¸€ä¸ªä¸‹ç½®ä¿¡ç•Œï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€ä½ä¸‹ç½®ä¿¡ç•Œçš„æ¨¡å¼è¿›è¡Œæ¢ç´¢ã€‚3) ç»ˆç«¯é›†çš„æ„å»ºæ–¹å¼ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥é€‰æ‹©cost-to-goä½äºæŸä¸ªé˜ˆå€¼çš„çŠ¶æ€ä½œä¸ºç»ˆç«¯é›†ã€‚4) ä»·å€¼å‡½æ•°çš„å­¦ä¹ æ–¹æ³•ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ä»·å€¼å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨é¿éšœä»»åŠ¡çš„ä»¿çœŸå®éªŒä¸­ï¼ŒMM-LMPCèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢ä¸åŒçš„è·¯å¾„ï¼Œå¹¶æ‰¾åˆ°æ¯”ä¼ ç»ŸLMPCæ›´ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMM-LMPCèƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ä¼˜è§£ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„é²æ£’æ€§ã€‚è®ºæ–‡è¿˜æä¾›äº†é€’å½’å¯è¡Œæ€§ã€é—­ç¯ç¨³å®šæ€§ã€æ¸è¿‘æ”¶æ•›åˆ°æœ€ä½³æ¨¡å¼ä»¥åŠå¯¹æ•°åæ‚”ç•Œçš„ç†è®ºè¯æ˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MM-LMPCé€‚ç”¨äºéœ€è¦è¿­ä»£å­¦ä¹ å’Œä¼˜åŒ–çš„æœºå™¨äººæ§åˆ¶ä»»åŠ¡ï¼Œä¾‹å¦‚å¤æ‚ç¯å¢ƒä¸‹çš„è·¯å¾„è§„åˆ’ã€é¿éšœã€æŠ“å–ç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€å·¥ä¸šæœºå™¨äººã€æœåŠ¡æœºå™¨äººç­‰é¢†åŸŸï¼Œæé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ï¼Œé™ä½äººå·¥å¹²é¢„çš„éœ€æ±‚ã€‚æœªæ¥ï¼Œå¯ä»¥å°†MM-LMPCä¸å…¶ä»–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚å¼ºåŒ–å­¦ä¹ ï¼‰ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡å…¶æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning Model Predictive Control (LMPC) improves performance on iterative tasks by leveraging data from previous executions. At each iteration, LMPC constructs a sampled safe set from past trajectories and uses it as a terminal constraint, with a terminal cost given by the corresponding cost-to-go. While effective, LMPC heavily depends on the initial trajectories: states with high cost-to-go are rarely selected as terminal candidates in later iterations, leaving parts of the state space unexplored and potentially missing better solutions. For example, in a reach-avoid task with two possible routes, LMPC may keep refining the initially shorter path while neglecting the alternative path that could lead to a globally better solution. To overcome this limitation, we propose Multi-Modal LMPC (MM-LMPC), which clusters past trajectories into modes and maintains mode-specific terminal sets and value functions. A bandit-based meta-controller with a Lower Confidence Bound (LCB) policy balances exploration and exploitation across modes, enabling systematic refinement of all modes. This allows MM-LMPC to escape high-cost local optima and discover globally superior solutions. We establish recursive feasibility, closed-loop stability, asymptotic convergence to the best mode, and a logarithmic regret bound. Simulations on obstacle-avoidance tasks validate the performance improvements of the proposed method.

