---
layout: default
title: Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods
---

# Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.02841" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.02841v2</a>
  <a href="https://arxiv.org/pdf/2506.02841.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.02841v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.02841v2', 'Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tom Danino, Nahum Shimkin

**åˆ†ç±»**: eess.SY, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-03 (æ›´æ–°: 2025-06-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEnsemble-MIXä»¥è§£å†³å¤šæ™ºèƒ½ä½“RLæ ·æœ¬æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `æ ·æœ¬æ•ˆç‡` `é›†æˆå­¦ä¹ ` `é€‰æ‹©æ€§æ¢ç´¢` `é›†ä¸­è¯„è®ºå‘˜` `å»ä¸­å¿ƒåŒ–å­¦ä¹ ` `TDç®—æ³•` `é«˜ä¸ç¡®å®šæ€§çŠ¶æ€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ”¶æ•›æ—¶éœ€è¦å¤§é‡çš„ç¯å¢ƒäº¤äº’ï¼Œæ¢ç´¢å¤§è§„æ¨¡è”åˆåŠ¨ä½œç©ºé—´çš„å›°éš¾åŠ å‰§äº†è¿™ä¸€é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆåˆ†è§£é›†ä¸­è¯„è®ºå‘˜ä¸å»ä¸­å¿ƒåŒ–é›†æˆå­¦ä¹ çš„æ–°ç®—æ³•ï¼Œåˆ©ç”¨é›†æˆå³°åº¦è¿›è¡Œé€‰æ‹©æ€§æ¢ç´¢ä»¥å¼•å¯¼å­¦ä¹ ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†MARLåŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä½†é€šå¸¸éœ€è¦æ¯”å•æ™ºèƒ½ä½“æ–¹æ³•æ›´å¤šçš„ç¯å¢ƒäº¤äº’æ‰èƒ½æ”¶æ•›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼Œç»“åˆäº†åˆ†è§£çš„é›†ä¸­å¼è¯„è®ºå‘˜ä¸å»ä¸­å¿ƒåŒ–çš„é›†æˆå­¦ä¹ ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬åˆ©ç”¨é›†æˆå³°åº¦çš„é€‰æ‹©æ€§æ¢ç´¢æ–¹æ³•ï¼Œæ‰©å±•äº†å…¨çƒåˆ†è§£è¯„è®ºå‘˜ï¼Œå¹¶é€šè¿‡å¤šæ ·æ€§æ­£åˆ™åŒ–çš„ä¸ªä½“è¯„è®ºå‘˜é›†æˆæ¥å¼•å¯¼æ¢ç´¢é«˜ä¸ç¡®å®šæ€§çŠ¶æ€å’ŒåŠ¨ä½œã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ–°å‹æˆªæ–­TD($Î»$)ç®—æ³•è®­ç»ƒé›†ä¸­è¯„è®ºå‘˜ï¼Œä»¥æé«˜æ ·æœ¬æ•ˆç‡ï¼Œå¹¶åœ¨æ¼”å‘˜ä¾§é€‚åº”æ··åˆæ ·æœ¬æ–¹æ³•ï¼Œå¹³è¡¡ç¨³å®šæ€§ä¸æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†MARLåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­æ ·æœ¬æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ”¶æ•›æ—¶é€šå¸¸éœ€è¦æ›´å¤šçš„ç¯å¢ƒäº¤äº’ï¼Œä¸”åœ¨å¤§è§„æ¨¡è”åˆåŠ¨ä½œç©ºé—´ä¸­æ¢ç´¢å›°éš¾ï¼Œå¯¼è‡´é«˜æ–¹å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„Ensemble-MIXç®—æ³•ç»“åˆäº†åˆ†è§£çš„é›†ä¸­å¼è¯„è®ºå‘˜ä¸å»ä¸­å¿ƒåŒ–çš„é›†æˆå­¦ä¹ ï¼Œåˆ©ç”¨é›†æˆå³°åº¦è¿›è¡Œé€‰æ‹©æ€§æ¢ç´¢ï¼Œä»¥å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ä¸­çš„é«˜ä¸ç¡®å®šæ€§çŠ¶æ€å’ŒåŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªåˆ†è§£çš„é›†ä¸­è¯„è®ºå‘˜å’Œå¤šä¸ªå»ä¸­å¿ƒåŒ–çš„ä¸ªä½“è¯„è®ºå‘˜ã€‚é›†ä¸­è¯„è®ºå‘˜é€šè¿‡æ–°å‹æˆªæ–­TD($Î»$)ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸ªä½“è¯„è®ºå‘˜åˆ™é€šè¿‡å¤šæ ·æ€§æ­£åˆ™åŒ–è¿›è¡Œé›†æˆã€‚æ¼”å‘˜ä¾§é‡‡ç”¨æ··åˆæ ·æœ¬æ–¹æ³•ï¼Œç»“åˆäº†åœ¨æ”¿ç­–å’Œç¦»æ”¿ç­–æŸå¤±å‡½æ•°çš„è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†é›†æˆå³°åº¦ä½œä¸ºé€‰æ‹©æ€§æ¢ç´¢çš„ä¾æ®ï¼Œå¹¶é€šè¿‡æˆªæ–­TD($Î»$)ç®—æ³•æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºæ›´æœ‰æ•ˆçš„æ¢ç´¢ç­–ç•¥å’Œæ›´ä½çš„æ–¹å·®ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬é›†æˆè¯„è®ºå‘˜çš„å¤šæ ·æ€§æ­£åˆ™åŒ–ã€é€‰æ‹©æ€§æ¢ç´¢æœºåˆ¶çš„å®ç°ï¼Œä»¥åŠæ··åˆæ ·æœ¬æ–¹æ³•çš„å…·ä½“åº”ç”¨ï¼Œç¡®ä¿äº†ç®—æ³•åœ¨ç¨³å®šæ€§ä¸æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œç®—æ³•èƒ½å¤Ÿåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEnsemble-MIXåœ¨å¤šä¸ªæ ‡å‡†MARLåŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œå°¤å…¶åœ¨SMAC IIåœ°å›¾ä¸Šè¡¨ç°çªå‡ºï¼Œæ ·æœ¬æ•ˆç‡æ˜¾è‘—æé«˜ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šæ™ºèƒ½ä½“å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººåä½œã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œå¤šæ™ºèƒ½ä½“æ¸¸æˆç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ ·æœ¬æ•ˆç‡ï¼Œèƒ½å¤ŸåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œé™ä½èµ„æºæ¶ˆè€—ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜çš„æ•ˆèƒ½å’Œæ›´ä½çš„æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-agent reinforcement learning (MARL) methods have achieved state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms typically require significantly more environment interactions than their single-agent counterparts to converge, a problem exacerbated by the difficulty in exploring over a large joint action space and the high variance intrinsic to MARL environments. To tackle these issues, we propose a novel algorithm that combines a decomposed centralized critic with decentralized ensemble learning, incorporating several key contributions. The main component in our scheme is a selective exploration method that leverages ensemble kurtosis. We extend the global decomposed critic with a diversity-regularized ensemble of individual critics and utilize its excess kurtosis to guide exploration toward high-uncertainty states and actions. To improve sample efficiency, we train the centralized critic with a novel truncated variation of the TD($Î»$) algorithm, enabling efficient off-policy learning with reduced variance. On the actor side, our suggested algorithm adapts the mixed samples approach to MARL, mixing on-policy and off-policy loss functions for training the actors. This approach balances between stability and efficiency and outperforms purely off-policy learning. The evaluation shows our method outperforms state-of-the-art baselines on standard MARL benchmarks, including a variety of SMAC II maps.

