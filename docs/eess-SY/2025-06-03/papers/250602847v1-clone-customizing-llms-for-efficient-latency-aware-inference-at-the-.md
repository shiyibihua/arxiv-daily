---
layout: default
title: CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the Edge
---

# CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the Edge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.02847" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.02847v1</a>
  <a href="https://arxiv.org/pdf/2506.02847.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.02847v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.02847v1', 'CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the Edge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chunlin Tian, Xinpeng Qin, Kahou Tam, Li Li, Zijian Wang, Yuanzhe Zhao, Minglei Zhang, Chengzhong Xu

**åˆ†ç±»**: cs.AR, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-06-03

**å¤‡æ³¨**: Accepted by USENIX ATC 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLONEä»¥è§£å†³è¾¹ç¼˜è®¾å¤‡ä¸ŠLLMæ¨ç†å»¶è¿Ÿä¸èƒ½è€—é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¾¹ç¼˜è®¡ç®—` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç®—æ³•ç¡¬ä»¶ååŒ` `èƒ½é‡ä¼˜åŒ–` `å®æ—¶æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¾¹ç¼˜è®¾å¤‡åœ¨å­˜å‚¨ã€åŠŸè€—å’Œé‡é‡ä¸Šå­˜åœ¨é™åˆ¶ï¼Œå¯¼è‡´LLMåº”ç”¨éƒ¨ç½²é¢ä¸´å»¶è¿Ÿä¸èƒ½è€—çš„å¹³è¡¡æŒ‘æˆ˜ã€‚
2. CLONEé€šè¿‡ç®—æ³•ä¸ç¡¬ä»¶çš„ååŒè®¾è®¡ï¼Œä¼˜åŒ–äº†LLMåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶æ¨ç†æ€§èƒ½ä¸èƒ½æ•ˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLONEåœ¨ä¸¤ä¸ªè¾¹ç¼˜å¹³å°ä¸Šå®ç°äº†æ¨ç†åŠ é€Ÿ11.92å€ï¼Œèƒ½è€—é™ä½7.36å€ï¼Œä¸”ç”Ÿæˆè´¨é‡ä¿æŒé«˜æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹äºå¿«é€Ÿå“åº”å’Œæ•°æ®éšç§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¾¹ç¼˜è®¾å¤‡çš„å­˜å‚¨ã€é‡é‡å’ŒåŠŸè€—é™åˆ¶ä½¿å¾—LLMåº”ç”¨çš„éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡é¦–å…ˆé‡åŒ–äº†åœ¨ç°æœ‰è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²LLMçš„å›°éš¾ï¼Œå¹¶æå‡ºäº†CLONEï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ¨¡å‹å’Œç³»ç»Ÿå±‚é¢è¿›è¡Œæ·±å…¥ç®—æ³•-ç¡¬ä»¶ååŒè®¾è®¡çš„æ–¹æ³•ï¼Œæ™ºèƒ½æ•´åˆå®æ—¶èƒ½é‡ä¼˜åŒ–ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„é€šç”¨æ€§ã€‚ä¸ºäº†æœ€å¤§åŒ–è¿™äº›ç®—æ³•åœ¨å§‹ç»ˆåœ¨çº¿å’Œä¸­é—´è¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸­çš„ååŒæ•ˆç›Šï¼Œæˆ‘ä»¬ä¸“æ³¨äº28nmå¯æ‰©å±•ç¡¬ä»¶åŠ é€Ÿå™¨ç³»ç»Ÿã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªç°æˆçš„è¾¹ç¼˜å¹³å°ä¸Šå®ç°å¹¶å¹¿æ³›è¯„ä¼°äº†CLONEï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒCLONEæœ‰æ•ˆåŠ é€Ÿæ¨ç†è¿‡ç¨‹è¾¾11.92å€ï¼ŒåŒæ—¶èŠ‚èƒ½è¾¾7.36å€ï¼Œå¹¶ä¿æŒé«˜ç”Ÿæˆè´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶é¢ä¸´çš„å»¶è¿Ÿå’Œèƒ½è€—é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éš¾ä»¥å®ç°é«˜æ•ˆæ¨ç†ï¼Œå¯¼è‡´å“åº”æ—¶é—´å»¶é•¿å’Œèƒ½é‡æ¶ˆè€—å¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCLONEçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç®—æ³•ä¸ç¡¬ä»¶çš„æ·±åº¦ååŒè®¾è®¡ï¼Œç»“åˆå®æ—¶èƒ½é‡ä¼˜åŒ–ç­–ç•¥ï¼Œç¡®ä¿åœ¨ä¿è¯æ¨ç†ç²¾åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½å»¶è¿Ÿå’Œèƒ½è€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLONEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹ä¼˜åŒ–æ¨¡å—å’Œç¡¬ä»¶åŠ é€Ÿæ¨¡å—ã€‚æ¨¡å‹ä¼˜åŒ–æ¨¡å—è´Ÿè´£è°ƒæ•´LLMçš„ç»“æ„å’Œå‚æ•°ä»¥é€‚åº”è¾¹ç¼˜è®¾å¤‡çš„ç‰¹æ€§ï¼Œè€Œç¡¬ä»¶åŠ é€Ÿæ¨¡å—åˆ™åˆ©ç”¨28nmæŠ€æœ¯å®ç°é«˜æ•ˆçš„è®¡ç®—èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šCLONEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç®—æ³•ä¸ç¡¬ä»¶çš„è”åˆè®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨è¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸­å®ç°å®æ—¶æ¨ç†å’Œèƒ½é‡ä¼˜åŒ–çš„æœ€ä½³å¹³è¡¡ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€ä¼˜åŒ–æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCLONEé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥é€‚åº”è¾¹ç¼˜è®¾å¤‡çš„è®¡ç®—èƒ½åŠ›ï¼Œå¹¶é€šè¿‡åŠ¨æ€è°ƒæ•´æ¨¡å‹å‚æ•°æ¥å®ç°èƒ½æ•ˆçš„æœ€å¤§åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCLONEåœ¨ä¸¤ä¸ªè¾¹ç¼˜å¹³å°ä¸Šå®ç°äº†æ¨ç†é€Ÿåº¦æå‡è‡³11.92å€ï¼Œèƒ½è€—é™ä½è‡³7.36å€ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨è¾¹ç¼˜è®¡ç®—ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CLONEçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæ™ºèƒ½æ‰‹æœºã€ç‰©è”ç½‘è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—å¹³å°ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæå‡è¿™äº›è®¾å¤‡ä¸ŠLLMåº”ç”¨çš„å“åº”é€Ÿåº¦å’Œèƒ½æ•ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deploying large language models (LLMs) on edge devices is crucial for delivering fast responses and ensuring data privacy. However, the limited storage, weight, and power of edge devices make it difficult to deploy LLM-powered applications. These devices must balance latency requirements with energy consumption and model accuracy. In this paper, we first quantify the challenges of deploying LLMs on off-the-shelf edge devices and then we present CLONE, an in-depth algorithm-hardware co-design at both the model- and system-level that intelligently integrates real-time, energy optimization while maintaining robust generality. In order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize in a 28nm scalable hardware accelerator system. We implement and extensively evaluate CLONE on two off-the-shelf edge platforms. Experiments show that CLONE effectively accelerates the inference process up to 11.92x, and saves energy up to 7.36x, while maintaining high-generation.

