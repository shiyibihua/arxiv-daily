---
layout: default
title: Compositional shield synthesis for safe reinforcement learning in partial observability
---

# Compositional shield synthesis for safe reinforcement learning in partial observability

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.12085" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.12085v1</a>
  <a href="https://arxiv.org/pdf/2509.12085.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.12085v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.12085v1', 'Compositional shield synthesis for safe reinforcement learning in partial observability')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Steven Carr, Georgios Bakirtzis, Ufuk Topcu

**åˆ†ç±»**: eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§ç»„åˆå¼å®‰å…¨ç›¾ç‰Œåˆæˆæ–¹æ³•ï¼Œç”¨äºéƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `éƒ¨åˆ†å¯è§‚æµ‹æ€§` `å®‰å…¨ç›¾ç‰Œ` `ç»„åˆå¼åˆæˆ` `POMDP`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­æ˜“è¿›å…¥ä¸å®‰å…¨çŠ¶æ€ï¼Œä¼ ç»Ÿæ•´ä½“å®‰å…¨ç›¾ç‰Œåˆæˆè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚
2. è®ºæ–‡æå‡ºç»„åˆå¼å®‰å…¨ç›¾ç‰Œåˆæˆæ–¹æ³•ï¼Œå°†å®‰å…¨éœ€æ±‚åˆ†è§£ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œæå‡äº†å¯æ‰©å±•æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯å®‰å…¨æ€§çš„åŒæ—¶ï¼Œèƒ½ä½¿æ™ºèƒ½ä½“æ”¶æ•›åˆ°æ›´é«˜çš„æœŸæœ›å¥–åŠ±ï¼Œå¹¶å‡å°‘è®­ç»ƒæ¬¡æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•æ§åˆ¶çš„æ™ºèƒ½ä½“ç»å¸¸ä¼šè¿›å…¥ä¸å®‰å…¨çŠ¶æ€ï¼Œå°¤å…¶æ˜¯åœ¨ä¸ç¡®å®šå’Œéƒ¨åˆ†å¯è§‚æµ‹çš„ç¯å¢ƒä¸­ã€‚éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ä¸ºç ”ç©¶è¿™ç§æœ‰é™æ„ŸçŸ¥åœºæ™¯æä¾›äº†ä¸€ä¸ªè‡ªç„¶çš„ç¯å¢ƒã€‚å®‰å…¨ç›¾ç‰Œé€šè¿‡è¿‡æ»¤ä¸è‰¯åŠ¨ä½œæ¥ç¡®ä¿å®‰å…¨RLï¼Œä»è€Œåœ¨æ™ºèƒ½ä½“çš„ç­–ç•¥ä¸­ä¿æŒå®‰å…¨è¦æ±‚ã€‚ç„¶è€Œï¼Œåœ¨å¤æ‚çš„éƒ¨ç½²åœºæ™¯ä¸­ï¼Œæ•´ä½“å®‰å…¨ç›¾ç‰Œçš„åˆæˆåœ¨è®¡ç®—ä¸Šæ˜¯æ˜‚è´µçš„ã€‚æœ¬æ–‡æå‡ºé€šè¿‡å¯¹å®‰å…¨è¦æ±‚è¿›è¡Œåˆ†éƒ¨åˆ†å»ºæ¨¡æ¥ç»„åˆåˆæˆå®‰å…¨ç›¾ç‰Œï¼Œä»è€Œæé«˜å¯æ‰©å±•æ€§ã€‚ç‰¹åˆ«åœ°ï¼Œä½¿ç”¨RLç®—æ³•çš„POMDPå½¢å¼çš„é—®é¢˜å…¬å¼è¡¨æ˜ï¼Œé…å¤‡ç»„åˆå¼å®‰å…¨ç›¾ç‰Œçš„RLæ™ºèƒ½ä½“ï¼Œé™¤äº†å®‰å…¨ä¹‹å¤–ï¼Œè¿˜èƒ½æ”¶æ•›åˆ°æ›´é«˜çš„æœŸæœ›å¥–åŠ±å€¼ã€‚é€šè¿‡ä½¿ç”¨å­é—®é¢˜å…¬å¼ï¼Œæˆ‘ä»¬ä¿ç•™å¹¶æé«˜äº†å®‰å…¨ç›¾ç‰Œæ™ºèƒ½ä½“çš„èƒ½åŠ›ï¼Œä½¿å…¶æ¯”æœªå®‰å…¨ç›¾ç‰Œçš„æ™ºèƒ½ä½“éœ€è¦æ›´å°‘çš„è®­ç»ƒæ¬¡æ•°ï¼Œå°¤å…¶æ˜¯åœ¨ç¨€ç–å¥–åŠ±è®¾ç½®ä¸­ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å‘ç°ç»„åˆå¼å®‰å…¨ç›¾ç‰Œåˆæˆå…è®¸RLæ™ºèƒ½ä½“åœ¨æ¯”å…¶ä»–æœ€å…ˆè¿›çš„åŸºäºæ¨¡å‹çš„æ–¹æ³•å¤§ä¸¤ä¸ªæ•°é‡çº§çš„ç¯å¢ƒä¸­ä¿æŒå®‰å…¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­éš¾ä»¥ä¿è¯å®‰å…¨æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ•´ä½“å®‰å…¨ç›¾ç‰Œåˆæˆæ–¹æ³•åœ¨è®¡ç®—ä¸Šæˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡ç¯å¢ƒï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ•´ä½“çš„å®‰å…¨éœ€æ±‚åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ï¼Œé’ˆå¯¹æ¯ä¸ªå­é—®é¢˜åˆ†åˆ«åˆæˆå®‰å…¨ç›¾ç‰Œï¼Œç„¶åå°†è¿™äº›ç›¾ç‰Œç»„åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªç»„åˆå¼çš„å®‰å…¨ç›¾ç‰Œã€‚è¿™ç§åˆ†è§£é™ä½äº†æ¯ä¸ªå­é—®é¢˜çš„å¤æ‚åº¦ï¼Œä»è€Œæé«˜äº†åˆæˆæ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚é€šè¿‡ç»„åˆå¤šä¸ªå­ç›¾ç‰Œï¼Œå¯ä»¥ä¿è¯æ•´ä½“çš„å®‰å…¨çº¦æŸã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) å°†æ•´ä½“å®‰å…¨éœ€æ±‚åˆ†è§£ä¸ºå¤šä¸ªå­éœ€æ±‚ï¼›2) é’ˆå¯¹æ¯ä¸ªå­éœ€æ±‚ï¼Œå»ºç«‹å¯¹åº”çš„POMDPå­é—®é¢˜æ¨¡å‹ï¼›3) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒæ¯ä¸ªå­é—®é¢˜çš„å®‰å…¨ç›¾ç‰Œï¼›4) å°†å„ä¸ªå­ç›¾ç‰Œç»„åˆæˆä¸€ä¸ªæ•´ä½“çš„ç»„åˆå¼å®‰å…¨ç›¾ç‰Œï¼›5) å°†ç»„åˆå¼å®‰å…¨ç›¾ç‰Œé›†æˆåˆ°å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„ç­–ç•¥ä¸­ï¼Œç”¨äºè¿‡æ»¤ä¸å®‰å…¨åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ç»„åˆå¼å®‰å…¨ç›¾ç‰Œåˆæˆæ–¹æ³•ï¼Œé€šè¿‡åˆ†è§£å®‰å…¨éœ€æ±‚æ¥é™ä½åˆæˆçš„å¤æ‚åº¦ï¼Œä»è€Œæé«˜äº†å¯æ‰©å±•æ€§ã€‚ä¸ä¼ ç»Ÿçš„æ•´ä½“å®‰å…¨ç›¾ç‰Œåˆæˆæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡ã€æ›´å¤æ‚çš„ç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿåˆ©ç”¨å­é—®é¢˜ä¹‹é—´çš„ç‹¬ç«‹æ€§ï¼Œå¹¶è¡Œåˆæˆå¤šä¸ªå­ç›¾ç‰Œï¼Œè¿›ä¸€æ­¥æé«˜æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼Œå­é—®é¢˜çš„åˆ’åˆ†æ–¹å¼ã€å­ç›¾ç‰Œçš„ç»„åˆç­–ç•¥ä»¥åŠå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©æ˜¯å…³é”®çš„è®¾è®¡è¦ç´ ã€‚å­é—®é¢˜çš„åˆ’åˆ†éœ€è¦ä¿è¯æ¯ä¸ªå­é—®é¢˜éƒ½å…·æœ‰ä¸€å®šçš„ç‹¬ç«‹æ€§ï¼ŒåŒæ—¶åˆè¦èƒ½å¤Ÿè¦†ç›–æ•´ä½“çš„å®‰å…¨éœ€æ±‚ã€‚å­ç›¾ç‰Œçš„ç»„åˆç­–ç•¥éœ€è¦ä¿è¯ç»„åˆåçš„ç›¾ç‰Œèƒ½å¤Ÿæ»¡è¶³æ•´ä½“çš„å®‰å…¨çº¦æŸã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©éœ€è¦è€ƒè™‘ç®—æ³•çš„æ”¶æ•›é€Ÿåº¦ã€ç¨³å®šæ€§å’Œå¯¹éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒçš„é€‚åº”æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„æ•´ä½“å®‰å…¨ç›¾ç‰Œåˆæˆæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡çš„ç¯å¢ƒï¼ˆä¸¤ä¸ªæ•°é‡çº§ï¼‰ã€‚æ­¤å¤–ï¼Œé…å¤‡ç»„åˆå¼å®‰å…¨ç›¾ç‰Œçš„RLæ™ºèƒ½ä½“ï¼Œåœ¨ä¿è¯å®‰å…¨æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ”¶æ•›åˆ°æ›´é«˜çš„æœŸæœ›å¥–åŠ±å€¼ï¼Œå¹¶ä¸”éœ€è¦æ›´å°‘çš„è®­ç»ƒæ¬¡æ•°ï¼Œå°¤å…¶æ˜¯åœ¨ç¨€ç–å¥–åŠ±è®¾ç½®ä¸­ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰é¢†åŸŸï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨ä¸ç¡®å®šå’Œéƒ¨åˆ†å¯è§‚æµ‹çš„ç¯å¢ƒä¸­å®‰å…¨åœ°æ‰§è¡Œä»»åŠ¡ã€‚é€šè¿‡ä½¿ç”¨ç»„åˆå¼å®‰å…¨ç›¾ç‰Œï¼Œå¯ä»¥æœ‰æ•ˆåœ°é˜²æ­¢æ™ºèƒ½ä½“è¿›å…¥å±é™©çŠ¶æ€ï¼Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºé™ä½å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿå…¶åœ¨å®é™…åœºæ™¯ä¸­çš„éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Agents controlled by the output of reinforcement learning (RL) algorithms often transition to unsafe states, particularly in uncertain and partially observable environments. Partially observable Markov decision processes (POMDPs) provide a natural setting for studying such scenarios with limited sensing. Shields filter undesirable actions to ensure safe RL by preserving safety requirements in the agents' policy. However, synthesizing holistic shields is computationally expensive in complex deployment scenarios. We propose the compositional synthesis of shields by modeling safety requirements by parts, thereby improving scalability. In particular, problem formulations in the form of POMDPs using RL algorithms illustrate that an RL agent equipped with the resulting compositional shielding, beyond being safe, converges to higher values of expected reward. By using subproblem formulations, we preserve and improve the ability of shielded agents to require fewer training episodes than unshielded agents, especially in sparse-reward settings. Concretely, we find that compositional shield synthesis allows an RL agent to remain safe in environments two orders of magnitude larger than other state-of-the-art model-based approaches.

