---
layout: default
title: Multi-Task Lifelong Reinforcement Learning for Wireless Sensor Networks
---

# Multi-Task Lifelong Reinforcement Learning for Wireless Sensor Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16254" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16254v2</a>
  <a href="https://arxiv.org/pdf/2506.16254.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16254v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16254v2', 'Multi-Task Lifelong Reinforcement Learning for Wireless Sensor Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hossein Mohammadi Firouzjaei, Rafaela Scaciota, Sumudu Samarakoon

**åˆ†ç±»**: eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-06-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¤šä»»åŠ¡ç»ˆèº«å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥ä¼˜åŒ–æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œ` `ç»ˆèº«å¼ºåŒ–å­¦ä¹ ` `è‡ªé€‚åº”æ§åˆ¶` `èƒ½é‡ä¼˜åŒ–` `åŠ¨æ€ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­éš¾ä»¥æœ‰æ•ˆé€‚åº”ï¼Œå¯¼è‡´èƒ½é‡æ¶ˆè€—å’Œé˜Ÿåˆ—ç¨³å®šæ€§é—®é¢˜ã€‚
2. æå‡ºçš„è‡ªé€‚åº”æ§åˆ¶ç­–ç•¥ç»“åˆç»ˆèº«å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡çŸ¥è¯†è¿ç§»ä¼˜åŒ–æ•°æ®ä¼ è¾“å’Œèƒ½é‡æ”¶é›†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é€‚åº”æ€§å’Œèƒ½æ•ˆæ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡å¹…åº¦æ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åŠ¨æ€å’Œä¸å¯é¢„æµ‹çš„ç¯å¢ƒä¸­ï¼Œæé«˜æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œï¼ˆWSNï¼‰çš„å¯æŒç»­æ€§å’Œæ•ˆç‡éœ€è¦è‡ªé€‚åº”çš„é€šä¿¡å’Œèƒ½é‡æ”¶é›†ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªé€‚åº”æ§åˆ¶ç­–ç•¥ï¼Œæ—¨åœ¨ä¼˜åŒ–æ•°æ®ä¼ è¾“å’Œèƒ½é‡æ”¶é›†ï¼Œä»¥æœ€å°åŒ–æ•´ä½“èƒ½è€—ï¼ŒåŒæ—¶ç¡®ä¿é˜Ÿåˆ—ç¨³å®šæ€§å’Œèƒ½é‡å­˜å‚¨çº¦æŸã€‚é€šè¿‡ç»ˆèº«å¼ºåŒ–å­¦ä¹ çš„æ¦‚å¿µï¼Œå°†ç¯å¢ƒç‰¹å®šçš„çŸ¥è¯†è¿ç§»åˆ°æ–°æ¡ä»¶ä¸­ï¼Œä»è€Œå®ç°é€‚åº”æ€§ã€‚ä¸ä¸¤ç§åŸºçº¿æ¡†æ¶ï¼ˆåŸºäºLyapunovçš„ä¼˜åŒ–å’Œç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹ ï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿå¿«é€Ÿé€‚åº”ç¯å¢ƒå˜åŒ–ï¼Œæ€§èƒ½æ¥è¿‘æœ€ä¼˜ï¼Œé€Ÿåº¦æ¯”å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¿«çº¦30%ï¼Œæ¯”Lyapunovæ–¹æ³•å¿«çº¦60%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œåœ¨åŠ¨æ€ç¯å¢ƒä¸­èƒ½é‡æ¶ˆè€—å’Œé˜Ÿåˆ—ç¨³å®šæ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹ç¯å¢ƒå˜åŒ–æ—¶ï¼Œé€‚åº”æ€§ä¸è¶³ï¼Œå¯¼è‡´èƒ½é‡åˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ§åˆ¶ç­–ç•¥ï¼Œé€šè¿‡ç»ˆèº«å¼ºåŒ–å­¦ä¹ å®ç°ç¯å¢ƒçŸ¥è¯†çš„è¿ç§»ï¼Œä»è€Œä¼˜åŒ–æ•°æ®ä¼ è¾“å’Œèƒ½é‡æ”¶é›†ç­–ç•¥ï¼Œä»¥é™ä½æ•´ä½“èƒ½è€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç¯å¢ƒçŠ¶æ€æ„ŸçŸ¥æ¨¡å—ã€æ•°æ®ä¼ è¾“ä¼˜åŒ–æ¨¡å—å’Œèƒ½é‡æ”¶é›†ç­–ç•¥æ¨¡å—ã€‚é€šè¿‡å®æ—¶ç›‘æµ‹ç¯å¢ƒå˜åŒ–ï¼ŒåŠ¨æ€è°ƒæ•´ä¼ è¾“å’Œæ”¶é›†ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ç»ˆèº«å¼ºåŒ–å­¦ä¹ åº”ç”¨äºæ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œçš„è‡ªé€‚åº”æ§åˆ¶ä¸­ï¼Œåˆ©ç”¨çŸ¥è¯†è¿ç§»å®ç°å¿«é€Ÿé€‚åº”ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´çš„å­¦ä¹ ç‡å’ŒæŠ˜æ‰£å› å­ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»¼åˆè€ƒè™‘èƒ½é‡æ¶ˆè€—å’Œé˜Ÿåˆ—ç¨³å®šæ€§çš„å¤šç›®æ ‡ä¼˜åŒ–å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨é€‚åº”æ€§å’Œèƒ½æ•ˆæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½æ¯”å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¿«çº¦30%ï¼Œæ¯”åŸºäºLyapunovçš„ä¼˜åŒ–æ–¹æ³•å¿«çº¦60%ã€‚è¿™äº›ç»“æœéªŒè¯äº†çŸ¥è¯†è¿ç§»åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æ™ºèƒ½åŸå¸‚ã€ç¯å¢ƒç›‘æµ‹å’Œå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œçš„èƒ½æ•ˆå’Œç¨³å®šæ€§ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œå¯é æ€§ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Enhancing the sustainability and efficiency of wireless sensor networks (WSN) in dynamic and unpredictable environments requires adaptive communication and energy harvesting strategies. We propose a novel adaptive control strategy for WSNs that optimizes data transmission and EH to minimize overall energy consumption while ensuring queue stability and energy storing constraints under dynamic environmental conditions. The notion of adaptability therein is achieved by transferring the known environment-specific knowledge to new conditions resorting to the lifelong reinforcement learning concepts. We evaluate our proposed method against two baseline frameworks: Lyapunov-based optimization, and policy-gradient reinforcement learning (RL). Simulation results demonstrate that our approach rapidly adapts to changing environmental conditions by leveraging transferable knowledge, achieving near-optimal performance approximately $30\%$ faster than the RL method and $60\%$ faster than the Lyapunov-based approach. The implementation is available at our GitHub repository for reproducibility purposes [1].

