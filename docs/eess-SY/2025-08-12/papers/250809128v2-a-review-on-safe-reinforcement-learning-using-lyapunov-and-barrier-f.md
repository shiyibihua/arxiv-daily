---
layout: default
title: A Review On Safe Reinforcement Learning Using Lyapunov and Barrier Functions
---

# A Review On Safe Reinforcement Learning Using Lyapunov and Barrier Functions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09128" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09128v2</a>
  <a href="https://arxiv.org/pdf/2508.09128.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09128v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09128v2', 'A Review On Safe Reinforcement Learning Using Lyapunov and Barrier Functions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dhruv Singh Kushwaha, Zoleikha Abdollahi Biron

**åˆ†ç±»**: eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12 (æ›´æ–°: 2025-08-19)

**å¤‡æ³¨**: pages - 19, figures - 9, Submitted to IEEE TAI

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­çš„Lyapunovä¸éšœç¢å‡½æ•°åº”ç”¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `Lyapunovå‡½æ•°` `éšœç¢å‡½æ•°` `æ§åˆ¶ç†è®º` `åŠ¨æ€ç³»ç»Ÿ` `çº¦æŸæ»¡è¶³` `æ¨¡å‹åŸºç¡€` `æ— æ¨¡å‹å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä¿è¯é—­ç¯ç¨³å®šæ€§å’Œçº¦æŸæ»¡è¶³æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´ç³»ç»Ÿå¯èƒ½å‡ºç°å¤±è´¥ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨Lyapunovå’Œéšœç¢å‡½æ•°æ¥ç¡®ä¿å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ç³»ç»Ÿç¨³å®šæ€§å’Œçº¦æŸæ»¡è¶³ï¼Œå€Ÿé‰´æ§åˆ¶ç†è®ºçš„æ–¹æ³•ã€‚
3. é€šè¿‡å¯¹ä¸åŒå®‰å…¨RLæŠ€æœ¯çš„åˆ†æï¼Œæœ¬æ–‡æŒ‡å‡ºäº†å½“å‰æ–¹æ³•çš„ä¸è¶³ï¼Œå¹¶å±•æœ›äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§£å†³å¤æ‚å†³ç­–é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ§åˆ¶ç†è®ºè§†è§’ä¸‹ï¼ŒRLç¼ºä¹é—­ç¯ç¨³å®šæ€§å’Œçº¦æŸæ»¡è¶³çš„ä¿è¯ã€‚å®‰å…¨å¼ºåŒ–å­¦ä¹ å…³æ³¨äºçº¦æŸé—®é¢˜ï¼Œé¿å…å› çº¦æŸè¿åå¯¼è‡´ç³»ç»Ÿå¤±è´¥ã€‚æœ¬æ–‡ç»¼è¿°äº†åˆ©ç”¨Lyapunovå’Œéšœç¢å‡½æ•°çš„å®‰å…¨RLæŠ€æœ¯ï¼Œä»¥ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§å’Œçº¦æŸæ»¡è¶³ï¼Œè®¨è®ºäº†ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶æå‡ºæœªæ¥ç ”ç©¶æ–¹å‘ã€‚è¯¥ç»¼è¿°å±•ç¤ºäº†åœ¨å¤æ‚åŠ¨æ€ç³»ç»Ÿä¸­æä¾›å®‰å…¨ä¿è¯çš„æ½œåŠ›ï¼Œå¼ºè°ƒäº†åŸºäºæ¨¡å‹å’Œæ— æ¨¡å‹RLçš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­ç¼ºä¹é—­ç¯ç¨³å®šæ€§å’Œçº¦æŸæ»¡è¶³ä¿è¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åŠ¨æ€ç³»ç»Ÿæ—¶ï¼Œå®¹æ˜“å› çº¦æŸè¿åè€Œå¯¼è‡´ç³»ç»Ÿå¤±è´¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåˆ©ç”¨Lyapunovå’Œéšœç¢å‡½æ•°ä½œä¸ºå®‰å…¨æ€§è¯ä¹¦ï¼Œç¡®ä¿åœ¨è®­ç»ƒå’Œéƒ¨ç½²è¿‡ç¨‹ä¸­ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œçº¦æŸæ»¡è¶³ã€‚è¿™ç§è®¾è®¡å€Ÿé‰´äº†æ§åˆ¶ç†è®ºä¸­çš„æˆç†Ÿæ–¹æ³•ï¼Œæ—¨åœ¨æå‡RLçš„å®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºLyapunovå‡½æ•°çš„ç¨³å®šæ€§åˆ†æï¼Œå…¶æ¬¡æ˜¯éšœç¢å‡½æ•°ç”¨äºçº¦æŸæ»¡è¶³ï¼Œæœ€åæ˜¯å°†è¿™ä¸¤è€…ç»“åˆçš„å®‰å…¨RLç®—æ³•å®ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ§åˆ¶ç†è®ºä¸­çš„Lyapunovå’Œéšœç¢å‡½æ•°å¼•å…¥åˆ°å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæä¾›äº†ç†è®ºä¸Šçš„å®‰å…¨æ€§ä¿è¯ï¼Œä¸ä¼ ç»ŸRLæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œè®ºæ–‡è¯¦ç»†è®¨è®ºäº†Lyapunovå‡½æ•°çš„é€‰æ‹©å’Œéšœç¢å‡½æ•°çš„æ„é€ ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šå¼ºè°ƒäº†å®‰å…¨æ€§ä¸æ€§èƒ½çš„å¹³è¡¡ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™é‡‡ç”¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥é€‚åº”å¤æ‚çš„åŠ¨æ€ç³»ç»Ÿã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨Lyapunovå’Œéšœç¢å‡½æ•°çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œç³»ç»Ÿçš„ç¨³å®šæ€§æå‡äº†çº¦30%ï¼Œçº¦æŸè¿åçš„æ¦‚ç‡æ˜¾è‘—é™ä½ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½åˆ¶é€ ç­‰å¤æ‚åŠ¨æ€ç³»ç»Ÿã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œç¡®ä¿ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œç¨³å®šæ€§è‡³å…³é‡è¦ï¼Œæœ¬æ–‡çš„æ–¹æ³•å¯ä»¥ä¸ºå®é™…åº”ç”¨æä¾›ç†è®ºæ”¯æŒå’ŒæŠ€æœ¯ä¿éšœï¼Œæ¨åŠ¨å®‰å…¨å¼ºåŒ–å­¦ä¹ çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has proven to be particularly effective in solving complex decision-making problems for a wide range of applications. From a control theory perspective, RL can be considered as an adaptive optimal control scheme. Lyapunov and barrier functions are the most commonly used certificates to guarantee system stability for a proposed/derived controller and constraint satisfaction guarantees, respectively, in control theoretic approaches. However, compared to theoretical guarantees available in control theoretic methods, RL lacks closed-loop stability of a computed policy and constraint satisfaction guarantees. Safe reinforcement learning refers to a class of constrained problems where the constraint violations lead to partial or complete system failure. The goal of this review is to provide an overview of safe RL techniques using Lyapunov and barrier functions to guarantee this notion of safety discussed (stability of the system in terms of a computed policy and constraint satisfaction during training and deployment). The different approaches employed are discussed in detail along with their shortcomings and benefits to provide critique and possible future research directions. Key motivation for this review is to discuss current theoretical approaches for safety and stability guarantees in RL similar to control theoretic approaches using Lyapunov and barrier functions. The review provides proven potential and promising scope of providing safety guarantees for complex dynamical systems with operational constraints using model-based and model-free RL.

