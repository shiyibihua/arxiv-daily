---
layout: default
title: Bridging Continuous-time LQR and Reinforcement Learning via Gradient Flow of the Bellman Error
---

# Bridging Continuous-time LQR and Reinforcement Learning via Gradient Flow of the Bellman Error

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09685" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09685v1</a>
  <a href="https://arxiv.org/pdf/2506.09685.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09685v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09685v1', 'Bridging Continuous-time LQR and Reinforcement Learning via Gradient Flow of the Bellman Error')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Armin GieÃŸler, Albertus Johannes Malan, SÃ¶ren Hohmann

**åˆ†ç±»**: eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: submitted to Conference on Decision and Control

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°æ–¹æ³•é€šè¿‡æ¢¯åº¦æµè®¡ç®—æœ€ä¼˜åé¦ˆå¢ç›Š**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨` `è´å°”æ›¼è¯¯å·®` `æ¢¯åº¦æµ` `å¼ºåŒ–å­¦ä¹ ` `å“ˆå¯†é¡¿-é›…å¯æ¯”-è´å°”æ›¼æ–¹ç¨‹` `ç¨³å®šæ€§åˆ†æ` `æ§åˆ¶ç³»ç»Ÿ` `åé¦ˆå¢ç›Š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LQRæ–¹æ³•åœ¨å¤„ç†æ— é™æ—¶åŸŸé—®é¢˜æ—¶å­˜åœ¨æ¬¡ä¼˜æ€§å’Œè®¡ç®—å¤æ‚åº¦é«˜çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è¿ç»­æ—¶é—´è´å°”æ›¼è¯¯å·®å’Œæ¢¯åº¦æµæ¥è®¡ç®—æœ€ä¼˜åé¦ˆå¢ç›Šçš„æ–°æ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»¿çœŸä¸­è¡¨ç°ä¼˜è¶Šï¼Œèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆç¨³å®šçš„åé¦ˆç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡å¸¸å¾®åˆ†æ–¹ç¨‹è®¡ç®—æ— é™æ—¶åŸŸçº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨ï¼ˆLQRï¼‰é—®é¢˜çš„æœ€ä¼˜åé¦ˆå¢ç›Šã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è¿ç»­æ—¶é—´è´å°”æ›¼è¯¯å·®ï¼Œè¯¥è¯¯å·®æºè‡ªå“ˆå¯†é¡¿-é›…å¯æ¯”-è´å°”æ›¼ï¼ˆHJBï¼‰æ–¹ç¨‹ï¼Œé‡åŒ–äº†ç¨³å®šç­–ç•¥çš„æ¬¡ä¼˜æ€§ï¼Œå¹¶ä»¥åé¦ˆå¢ç›Šä¸ºå‚æ•°ã€‚æˆ‘ä»¬åˆ†æäº†å…¶æ€§è´¨ï¼ŒåŒ…æ‹¬æœ‰æ•ˆåŸŸã€å…‰æ»‘æ€§å’Œå¼ºè¿«æ€§ï¼Œå¹¶è¯æ˜äº†åœ¨ç¨³å®šåŒºåŸŸå†…å­˜åœ¨å”¯ä¸€çš„é©»ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å¯¼äº†è´å°”æ›¼è¯¯å·®çš„é—­å¼æ¢¯åº¦è¡¨è¾¾å¼ï¼Œå½¢æˆäº†ä¸€ä¸ªæ¢¯åº¦æµï¼Œæ”¶æ•›åˆ°æœ€ä¼˜åé¦ˆï¼Œå¹¶ç”Ÿæˆä»…åŒ…å«ç¨³å®šåé¦ˆç­–ç•¥çš„å”¯ä¸€è½¨è¿¹ã€‚æ­¤ç ”ç©¶è¿˜é€šè¿‡å°†ä»£æ•°é‡Œå¡ææ–¹ç¨‹ï¼ˆAREï¼‰çš„æ¬¡ä¼˜æ€§é‡æ–°å®šä¹‰ä¸ºè´å°”æ›¼è¯¯å·®ï¼Œå»ºç«‹äº†LQRç†è®ºä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¹‹é—´çš„æœ‰è¶£è”ç³»ã€‚æˆ‘ä»¬åœ¨ä»¿çœŸä¸­éªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå¹¶ä¸ç°æœ‰æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— é™æ—¶åŸŸçº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨ï¼ˆLQRï¼‰é—®é¢˜ä¸­çš„æœ€ä¼˜åé¦ˆå¢ç›Šè®¡ç®—ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ¬¡ä¼˜æ€§æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¿ç»­æ—¶é—´è´å°”æ›¼è¯¯å·®ï¼Œè¯¥è¯¯å·®é€šè¿‡å“ˆå¯†é¡¿-é›…å¯æ¯”-è´å°”æ›¼æ–¹ç¨‹æ¨å¯¼è€Œæ¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé‡åŒ–ç­–ç•¥çš„æ¬¡ä¼˜æ€§ï¼Œå¹¶é€šè¿‡æ¢¯åº¦æµæ–¹æ³•æ”¶æ•›åˆ°æœ€ä¼˜åé¦ˆå¢ç›Šã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬å®šä¹‰è¿ç»­æ—¶é—´è´å°”æ›¼è¯¯å·®ã€åˆ†æå…¶æ€§è´¨ã€æ¨å¯¼é—­å¼æ¢¯åº¦è¡¨è¾¾å¼ä»¥åŠé€šè¿‡æ¢¯åº¦æµç”Ÿæˆç¨³å®šåé¦ˆç­–ç•¥çš„è¿‡ç¨‹ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è´å°”æ›¼è¯¯å·®çš„æ„å»ºã€æ¢¯åº¦æµçš„å®ç°å’Œç¨³å®šæ€§åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†ä»£æ•°é‡Œå¡ææ–¹ç¨‹çš„æ¬¡ä¼˜æ€§é‡æ–°å®šä¹‰ä¸ºè´å°”æ›¼è¯¯å·®ï¼Œå¹¶é€šè¿‡å¼•å…¥çŠ¶æ€æ— å…³çš„å½¢å¼å’Œåˆ©ç”¨æé›…æ™®è¯ºå¤«æ–¹ç¨‹æ¥å…‹æœæ— é™æ—¶åŸŸçš„æŒ‘æˆ˜ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨äºè´å°”æ›¼è¯¯å·®çš„æœ‰æ•ˆåŸŸã€å…‰æ»‘æ€§å’Œå¼ºè¿«æ€§ï¼Œç¡®ä¿åœ¨ç¨³å®šåŒºåŸŸå†…å­˜åœ¨å”¯ä¸€çš„é©»ç‚¹ï¼Œå¹¶é€šè¿‡é—­å¼æ¢¯åº¦è¡¨è¾¾å¼å®ç°é«˜æ•ˆçš„åé¦ˆå¢ç›Šè®¡ç®—ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿ç®—æ³•çš„æ”¶æ•›æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨ä»¿çœŸä¸­æˆåŠŸç”Ÿæˆäº†ç¨³å®šçš„åé¦ˆç­–ç•¥ï¼Œç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šæ¬¡å®éªŒä¸­æ”¶æ•›é€Ÿåº¦åŠ å¿«ï¼Œä¸”åœ¨ç¨³å®šæ€§æ–¹é¢è¡¨ç°å‡ºæ›´ä¼˜çš„æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨æ§åˆ¶ç³»ç»Ÿã€æœºå™¨äººå¯¼èˆªå’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿç­‰ã€‚é€šè¿‡æä¾›ä¸€ç§é«˜æ•ˆçš„æœ€ä¼˜åé¦ˆå¢ç›Šè®¡ç®—æ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—æå‡è¿™äº›ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå“åº”é€Ÿåº¦ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we present a novel method for computing the optimal feedback gain of the infinite-horizon Linear Quadratic Regulator (LQR) problem via an ordinary differential equation. We introduce a novel continuous-time Bellman error, derived from the Hamilton-Jacobi-Bellman (HJB) equation, which quantifies the suboptimality of stabilizing policies and is parametrized in terms of the feedback gain. We analyze its properties, including its effective domain, smoothness, coerciveness and show the existence of a unique stationary point within the stability region. Furthermore, we derive a closed-form gradient expression of the Bellman error that induces a gradient flow. This converges to the optimal feedback and generates a unique trajectory which exclusively comprises stabilizing feedback policies. Additionally, this work advances interesting connections between LQR theory and Reinforcement Learning (RL) by redefining suboptimality of the Algebraic Riccati Equation (ARE) as a Bellman error, adapting a state-independent formulation, and leveraging Lyapunov equations to overcome the infinite-horizon challenge. We validate our method in a simulation and compare it to the state of the art.

