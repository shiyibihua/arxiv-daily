---
layout: default
title: Off Policy Lyapunov Stability in Reinforcement Learning
---

# Off Policy Lyapunov Stability in Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09863" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09863v1</a>
  <a href="https://arxiv.org/pdf/2509.09863.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09863v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09863v1', 'Off Policy Lyapunov Stability in Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sarvan Gill, Daniela Constantinescu

**åˆ†ç±»**: eess.SY, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: Conference on Robot Learning (CORL) 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§Off-Policy Lyapunovå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡ç¨³å®šæ€§å’Œæ•°æ®æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `Lyapunovç¨³å®šæ€§` `Off-Policyå­¦ä¹ ` `ç¨³å®šæ€§ä¿è¯` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ éš¾ä»¥ä¿è¯ç¨³å®šæ€§ï¼Œè€ŒåŸºäºOn-Policy Lyapunovå‡½æ•°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ ·æœ¬æ•ˆç‡ä½ã€‚
2. è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§Off-Policy Lyapunovå‡½æ•°å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç¨³å®šæ€§å’Œæ•°æ®åˆ©ç”¨ç‡ã€‚
3. é€šè¿‡åœ¨å€’ç«‹æ‘†å’Œå››æ—‹ç¿¼é£è¡Œå™¨ä¸Šçš„å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡Soft Actor Criticå’ŒProximal Policy Optimizationç®—æ³•æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç¼ºä¹æä¾›ç¨³å®šä¿è¯çš„èƒ½åŠ›ã€‚æœ€è¿‘çš„ä¸€äº›ç®—æ³•é€šè¿‡å­¦ä¹ Lyapunovå‡½æ•°å’Œæ§åˆ¶ç­–ç•¥æ¥ç¡®ä¿å­¦ä¹ çš„ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œç”±äºå…¶On-Policyçš„ç‰¹æ€§ï¼Œå½“å‰è‡ªå­¦ä¹ çš„Lyapunovå‡½æ•°æ ·æœ¬æ•ˆç‡è¾ƒä½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§Off-Policyå­¦ä¹ Lyapunovå‡½æ•°çš„æ–¹æ³•ï¼Œå¹¶å°†æå‡ºçš„Off-Policy Lyapunovå‡½æ•°èå…¥åˆ°è½¯æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆSoft Actor Criticï¼‰å’Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆProximal Policy Optimizationï¼‰ç®—æ³•ä¸­ï¼Œä¸ºå®ƒä»¬æä¾›æ•°æ®é«˜æ•ˆçš„ç¨³å®šæ€§è¯æ˜ã€‚å€’ç«‹æ‘†å’Œå››æ—‹ç¿¼çš„ä»¿çœŸç»“æœè¡¨æ˜ï¼Œå½“ä½¿ç”¨æ‰€æå‡ºçš„Off-Policy Lyapunovå‡½æ•°æ—¶ï¼Œè¿™ä¸¤ç§ç®—æ³•çš„æ€§èƒ½å¾—åˆ°äº†æé«˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•é€šå¸¸ç¼ºä¹ç¨³å®šæ€§ä¿è¯ï¼Œå®¹æ˜“åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°éœ‡è¡æˆ–å‘æ•£ã€‚è™½ç„¶ä¸€äº›ç ”ç©¶é€šè¿‡å­¦ä¹ Lyapunovå‡½æ•°æ¥ç¡®ä¿ç¨³å®šæ€§ï¼Œä½†ç°æœ‰çš„æ–¹æ³•å¤§å¤šæ˜¯On-Policyçš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬åªèƒ½åˆ©ç”¨å½“å‰ç­–ç•¥äº§ç”Ÿçš„æ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œå¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ï¼Œè®­ç»ƒé€Ÿåº¦æ…¢ã€‚å› æ­¤ï¼Œå¦‚ä½•æé«˜Lyapunovå‡½æ•°å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ï¼Œä»è€Œæå‡å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç¨³å®šæ€§å’Œè®­ç»ƒé€Ÿåº¦ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†Lyapunovå‡½æ•°çš„å­¦ä¹ è¿‡ç¨‹ä»On-Policyè½¬å˜ä¸ºOff-Policyã€‚è¿™æ„å‘³ç€å¯ä»¥åˆ©ç”¨è¿‡å»ç­–ç•¥äº§ç”Ÿçš„æ•°æ®æ¥å­¦ä¹ Lyapunovå‡½æ•°ï¼Œä»è€Œå¤§å¤§æé«˜æ ·æœ¬æ•ˆç‡ã€‚é€šè¿‡åˆ©ç”¨ç»éªŒå›æ”¾æ± ä¸­çš„æ•°æ®ï¼Œå¯ä»¥æ›´å……åˆ†åœ°åˆ©ç”¨ç¯å¢ƒäº¤äº’ä¿¡æ¯ï¼ŒåŠ é€ŸLyapunovå‡½æ•°çš„æ”¶æ•›ï¼Œå¹¶æœ€ç»ˆæå‡å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•å°†Off-Policy Lyapunovå‡½æ•°å­¦ä¹ æ¨¡å—é›†æˆåˆ°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¡†æ¶ä¸­ï¼Œä¾‹å¦‚Soft Actor Critic (SAC) å’Œ Proximal Policy Optimization (PPO)ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸ç¯å¢ƒäº¤äº’ï¼Œå¹¶å°†æ•°æ®å­˜å‚¨åˆ°ç»éªŒå›æ”¾æ± ä¸­ï¼›2) ä»ç»éªŒå›æ”¾æ± ä¸­é‡‡æ ·æ•°æ®ï¼Œç”¨äºæ›´æ–°ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œï¼›3) ä½¿ç”¨é‡‡æ ·çš„æ•°æ®ï¼Œé€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°æ›´æ–°Lyapunovå‡½æ•°ï¼›4) å°†Lyapunovå‡½æ•°ä½œä¸ºæ­£åˆ™é¡¹æˆ–çº¦æŸæ¡ä»¶ï¼Œå¼•å¯¼ç­–ç•¥å­¦ä¹ ï¼Œç¡®ä¿ç¨³å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†Off-Policy Lyapunovå‡½æ•°å­¦ä¹ æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„On-Policyæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨å†å²æ•°æ®ï¼Œæ˜¾è‘—æé«˜æ ·æœ¬æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æä¾›äº†ä¸€ç§å°†Lyapunovå‡½æ•°é›†æˆåˆ°ç°æœ‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­çš„é€šç”¨æ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿæ–¹ä¾¿åœ°åº”ç”¨äºä¸åŒçš„ç®—æ³•å’Œä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼š Lyapunovå‡½æ•°çš„å…·ä½“å½¢å¼å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ã€‚è®ºæ–‡ä¸­å¯èƒ½é‡‡ç”¨äº†æŸç§ç¥ç»ç½‘ç»œç»“æ„æ¥è¡¨ç¤ºLyapunovå‡½æ•°ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿Lyapunovå‡½æ•°æ»¡è¶³Lyapunovç¨³å®šæ€§æ¡ä»¶ã€‚æŸå¤±å‡½æ•°å¯èƒ½åŒ…å«ä»¥ä¸‹å‡ é¡¹ï¼šLyapunovå‡½æ•°å€¼å¤§äºé›¶çš„çº¦æŸã€Lyapunovå‡½æ•°æ²¿è½¨è¿¹çš„å¯¼æ•°å°äºé›¶çš„çº¦æŸï¼Œä»¥åŠä¸€äº›æ­£åˆ™åŒ–é¡¹ï¼Œä»¥é˜²æ­¢Lyapunovå‡½æ•°è¿‡äºå¤æ‚ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ï¼ˆä¾‹å¦‚å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ã€æ­£åˆ™åŒ–ç³»æ•°ç­‰ï¼‰éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°†æå‡ºçš„Off-Policy Lyapunovå‡½æ•°é›†æˆåˆ°SACå’ŒPPOç®—æ³•ä¸­ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ç®—æ³•çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚åœ¨å€’ç«‹æ‘†å’Œå››æ—‹ç¿¼é£è¡Œå™¨ç­‰ä»¿çœŸç¯å¢ƒä¸­ï¼Œä½¿ç”¨Off-Policy Lyapunovå‡½æ•°çš„ç®—æ³•èƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„é²æ£’æ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­æ˜ç¡®æŒ‡å‡ºâ€œimproved performanceâ€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦é«˜ç¨³å®šæ€§çš„æ§åˆ¶ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€é£è¡Œå™¨æ§åˆ¶ç­‰ã€‚é€šè¿‡æä¾›ç¨³å®šæ€§ä¿è¯ï¼Œå¯ä»¥é™ä½ç³»ç»Ÿå‘ç”Ÿæ•…éšœçš„é£é™©ï¼Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åŠ é€Ÿå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ï¼Œé™ä½å¯¹æ ·æœ¬æ•°æ®çš„éœ€æ±‚ï¼Œä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºæ›´å¤æ‚çš„å®é™…åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional reinforcement learning lacks the ability to provide stability guarantees. More recent algorithms learn Lyapunov functions alongside the control policies to ensure stable learning. However, the current self-learned Lyapunov functions are sample inefficient due to their on-policy nature. This paper introduces a method for learning Lyapunov functions off-policy and incorporates the proposed off-policy Lyapunov function into the Soft Actor Critic and Proximal Policy Optimization algorithms to provide them with a data efficient stability certificate. Simulations of an inverted pendulum and a quadrotor illustrate the improved performance of the two algorithms when endowed with the proposed off-policy Lyapunov function.

