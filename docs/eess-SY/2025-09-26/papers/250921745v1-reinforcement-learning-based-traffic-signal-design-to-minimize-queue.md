---
layout: default
title: Reinforcement Learning Based Traffic Signal Design to Minimize Queue Lengths
---

# Reinforcement Learning Based Traffic Signal Design to Minimize Queue Lengths

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21745" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21745v1</a>
  <a href="https://arxiv.org/pdf/2509.21745.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21745v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21745v1', 'Reinforcement Learning Based Traffic Signal Design to Minimize Queue Lengths')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anirud Nandakumar, Chayan Banerjee, Lelitha Devi Vanajakshi

**åˆ†ç±»**: eess.SY, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„äº¤é€šä¿¡å·ç¯ä¼˜åŒ–æ–¹æ³•ï¼Œæœ€å°åŒ–è½¦è¾†æ’é˜Ÿé•¿åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `äº¤é€šä¿¡å·æ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–` `è½¦è¾†æ’é˜Ÿé•¿åº¦` `äº¤é€šä»¿çœŸ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿäº¤é€šä¿¡å·æ§åˆ¶æ–¹æ³•éš¾ä»¥é€‚åº”åŠ¨æ€äº¤é€šå˜åŒ–ï¼Œå¯¼è‡´æ‹¥å µå’Œå»¶è¯¯ã€‚
2. åˆ©ç”¨å¼ºåŒ–å­¦ä¹ PPOç®—æ³•ï¼Œç»“åˆå¤šç§çŠ¶æ€è¡¨ç¤ºæ–¹æ³•ï¼Œæœ€å°åŒ–äº¤é€šè·¯å£çš„è½¦è¾†æ’é˜Ÿé•¿åº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨SUMOæ¨¡æ‹Ÿå™¨ä¸­ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹³å‡æ’é˜Ÿé•¿åº¦å‡å°‘çº¦29%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜æ•ˆçš„äº¤é€šä¿¡å·æ§åˆ¶ï¼ˆTSCï¼‰å¯¹äºå‡å°‘æ‹¥å µã€å»¶è¯¯ã€æ±¡æŸ“å’Œç¡®ä¿é“è·¯å®‰å…¨è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚å›ºå®šé…æ—¶å’Œæ„Ÿåº”æ§åˆ¶ï¼‰éš¾ä»¥åº”å¯¹åŠ¨æ€äº¤é€šæ¨¡å¼ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è‡ªé€‚åº”TSCæ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•ï¼Œä»¥æœ€å°åŒ–æ‰€æœ‰ä¿¡å·ç›¸ä½ä¸Šçš„æ€»æ’é˜Ÿé•¿åº¦ã€‚é€šè¿‡æ‰©å±•çŠ¶æ€ç©ºé—´ã€è‡ªç¼–ç å™¨è¡¨ç¤ºå’Œå—K-Planeså¯å‘çš„è¡¨ç¤ºç­‰å¤šç§çŠ¶æ€è¡¨ç¤ºæ–¹æ³•ï¼Œè§£å†³äº†ä¸ºRLæ§åˆ¶å™¨æœ‰æ•ˆè¡¨ç¤ºé«˜åº¦éšæœºäº¤é€šçŠ¶å†µçš„æŒ‘æˆ˜ã€‚è¯¥ç®—æ³•å·²åœ¨SUMOäº¤é€šæ¨¡æ‹Ÿå™¨ä¸­å®ç°ï¼Œå¹¶åœ¨å‡å°‘æ’é˜Ÿé•¿åº¦æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œå…¶ä»–åŸºäºRLçš„æ–¹æ³•ã€‚æœ€ä½³é…ç½®æ¯”ä¼ ç»Ÿçš„éŸ¦ä¼¯æ–¯ç‰¹æ–¹æ³•å‡å°‘äº†çº¦29%çš„å¹³å‡æ’é˜Ÿé•¿åº¦ã€‚æ­¤å¤–ï¼Œå¯¹æ›¿ä»£å¥–åŠ±å‡½æ•°çš„æ¯”è¾ƒè¯„ä¼°è¯æ˜äº†æ‰€æå‡ºçš„åŸºäºé˜Ÿåˆ—çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å¯æ‰©å±•å’Œè‡ªé€‚åº”çš„åŸå¸‚äº¤é€šç®¡ç†çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸå¸‚äº¤é€šä¿¡å·æ§åˆ¶é—®é¢˜ï¼Œç›®æ ‡æ˜¯å‡å°‘è½¦è¾†æ’é˜Ÿé•¿åº¦ï¼Œæé«˜äº¤é€šæ•ˆç‡ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚å›ºå®šé…æ—¶å’Œæ„Ÿåº”æ§åˆ¶ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹åŠ¨æ€å˜åŒ–çš„äº¤é€šæµé‡ï¼Œå¯¼è‡´äº¤é€šæ‹¥å µå’Œå»¶è¯¯ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†é«˜ç»´ã€éšæœºçš„äº¤é€šçŠ¶æ€ç©ºé—´æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡ä¸äº¤é€šç¯å¢ƒçš„äº¤äº’å­¦ä¹ ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´äº¤é€šä¿¡å·ç¯çš„é…æ—¶æ–¹æ¡ˆï¼Œä»è€Œæœ€å°åŒ–è½¦è¾†æ’é˜Ÿé•¿åº¦ã€‚é€šè¿‡è®¾è®¡æœ‰æ•ˆçš„çŠ¶æ€è¡¨ç¤ºå’Œå¥–åŠ±å‡½æ•°ï¼Œä½¿å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–äº¤é€šçŠ¶å†µã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) äº¤é€šç¯å¢ƒæ¨¡æ‹Ÿå™¨ï¼ˆSUMOï¼‰ï¼šç”¨äºæ¨¡æ‹ŸçœŸå®çš„äº¤é€šç¯å¢ƒï¼Œæä¾›è½¦è¾†æµé‡ã€ä½ç½®ç­‰ä¿¡æ¯ã€‚2) å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼šä½¿ç”¨PPOç®—æ³•ï¼Œæ ¹æ®å½“å‰äº¤é€šçŠ¶æ€é€‰æ‹©åˆé€‚çš„ä¿¡å·ç¯é…æ—¶æ–¹æ¡ˆã€‚3) çŠ¶æ€è¡¨ç¤ºæ¨¡å—ï¼šå°†äº¤é€šç¯å¢ƒä¿¡æ¯è½¬æ¢ä¸ºæ™ºèƒ½ä½“å¯ä»¥ç†è§£çš„çŠ¶æ€å‘é‡ï¼ŒåŒ…æ‹¬æ‰©å±•çŠ¶æ€ç©ºé—´ã€è‡ªç¼–ç å™¨è¡¨ç¤ºå’ŒK-Planesè¡¨ç¤ºã€‚4) å¥–åŠ±å‡½æ•°æ¨¡å—ï¼šæ ¹æ®è½¦è¾†æ’é˜Ÿé•¿åº¦è®¡ç®—å¥–åŠ±å€¼ï¼Œå¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ä¼˜åŒ–äº¤é€šæµé‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å¤šç§çŠ¶æ€è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥æ›´æœ‰æ•ˆåœ°è¡¨ç¤ºå¤æ‚çš„äº¤é€šçŠ¶æ€ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„å•ä¸€çŠ¶æ€è¡¨ç¤ºï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿæ•æ‰åˆ°æ›´å¤šçš„äº¤é€šä¿¡æ¯ï¼Œæé«˜å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†ä¸åŒçš„å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œä»¥æ›´å¥½åœ°å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ä¼˜åŒ–ç›®æ ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†PPOç®—æ³•ä½œä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•å…·æœ‰è¾ƒå¥½çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚çŠ¶æ€è¡¨ç¤ºæ–¹é¢ï¼Œé‡‡ç”¨äº†æ‰©å±•çŠ¶æ€ç©ºé—´ï¼ˆåŒ…æ‹¬è½¦è¾†ä½ç½®ã€é€Ÿåº¦ç­‰ä¿¡æ¯ï¼‰ã€è‡ªç¼–ç å™¨è¡¨ç¤ºï¼ˆç”¨äºé™ç»´ï¼‰å’ŒK-Planesè¡¨ç¤ºï¼ˆç”¨äºæ•æ‰äº¤é€šæ¨¡å¼ï¼‰ã€‚å¥–åŠ±å‡½æ•°è®¾è®¡ä¸ºè´Ÿçš„è½¦è¾†æ’é˜Ÿé•¿åº¦ï¼Œé¼“åŠ±æ™ºèƒ½ä½“å‡å°‘æ’é˜Ÿã€‚PPOç®—æ³•çš„å…·ä½“å‚æ•°è®¾ç½®ï¼ˆå¦‚å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ç­‰ï¼‰éœ€è¦æ ¹æ®å…·ä½“äº¤é€šåœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„äº¤é€šä¿¡å·æ§åˆ¶æ–¹æ³•åœ¨SUMOæ¨¡æ‹Ÿå™¨ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ä¼ ç»Ÿçš„éŸ¦ä¼¯æ–¯ç‰¹æ–¹æ³•ç›¸æ¯”ï¼Œæœ€ä½³é…ç½®èƒ½å¤Ÿå‡å°‘çº¦29%çš„å¹³å‡æ’é˜Ÿé•¿åº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹æ¯”ä¸åŒçš„çŠ¶æ€è¡¨ç¤ºå’Œå¥–åŠ±å‡½æ•°è®¾è®¡ï¼ŒéªŒè¯äº†æ‰€æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºåŸå¸‚äº¤é€šç®¡ç†ç³»ç»Ÿï¼Œå®ç°äº¤é€šä¿¡å·ç¯çš„è‡ªé€‚åº”ä¼˜åŒ–ï¼Œå‡å°‘äº¤é€šæ‹¥å µã€é™ä½è½¦è¾†æ’æ”¾ã€æé«˜å‡ºè¡Œæ•ˆç‡ã€‚é€šè¿‡ä¸å®æ—¶äº¤é€šæ•°æ®çš„ç»“åˆï¼Œå¯ä»¥å®ç°æ›´åŠ æ™ºèƒ½åŒ–çš„äº¤é€šæ§åˆ¶ï¼Œæå‡åŸå¸‚äº¤é€šçš„æ•´ä½“è¿è¡Œæ•ˆç‡å’ŒæœåŠ¡æ°´å¹³ã€‚æœªæ¥å¯æ‰©å±•åˆ°åŒºåŸŸäº¤é€šååŒæ§åˆ¶ï¼Œè§£å†³æ›´å¤§èŒƒå›´çš„äº¤é€šæ‹¥å µé—®é¢˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient traffic signal control (TSC) is crucial for reducing congestion, travel delays, pollution, and for ensuring road safety. Traditional approaches, such as fixed signal control and actuated control, often struggle to handle dynamic traffic patterns. In this study, we propose a novel adaptive TSC framework that leverages Reinforcement Learning (RL), using the Proximal Policy Optimization (PPO) algorithm, to minimize total queue lengths across all signal phases. The challenge of efficiently representing highly stochastic traffic conditions for an RL controller is addressed through multiple state representations, including an expanded state space, an autoencoder representation, and a K-Planes-inspired representation. The proposed algorithm has been implemented using the Simulation of Urban Mobility (SUMO) traffic simulator and demonstrates superior performance over both traditional methods and other conventional RL-based approaches in reducing queue lengths. The best performing configuration achieves an approximately 29% reduction in average queue lengths compared to the traditional Webster method. Furthermore, comparative evaluation of alternative reward formulations demonstrates the effectiveness of the proposed queue-based approach, showcasing the potential for scalable and adaptive urban traffic management.

