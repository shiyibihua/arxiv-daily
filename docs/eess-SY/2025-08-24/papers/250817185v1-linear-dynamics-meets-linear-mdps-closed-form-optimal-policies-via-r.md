---
layout: default
title: Linear Dynamics meets Linear MDPs: Closed-Form Optimal Policies via Reinforcement Learning
---

# Linear Dynamics meets Linear MDPs: Closed-Form Optimal Policies via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17185" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17185v1</a>
  <a href="https://arxiv.org/pdf/2508.17185.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17185v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17185v1', 'Linear Dynamics meets Linear MDPs: Closed-Form Optimal Policies via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Abed AlRahman Al Makdah, Oliver Kosut, Lalitha Sankar, Shaofeng Zou

**åˆ†ç±»**: math.OC, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§ç»“åˆçº¿æ€§åŠ¨æ€ä¸çº¿æ€§MDPçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥ä¼˜åŒ–æ§åˆ¶ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çº¿æ€§åŠ¨æ€ç³»ç»Ÿ` `çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `å¼ºåŒ–å­¦ä¹ ` `æ§åˆ¶ç†è®º` `æœ€ä¼˜æ§åˆ¶ç­–ç•¥` `éšæœºå»ºæ¨¡` `æ ·æœ¬å¤æ‚åº¦åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹å¤æ‚çš„éšæœºç¯å¢ƒæ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆå»ºæ¨¡å’Œä¼˜åŒ–æ§åˆ¶ç­–ç•¥ï¼Œå°¤å…¶æ˜¯åœ¨è½¬ç§»æ¦‚ç‡æœªçŸ¥çš„æƒ…å†µä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆçº¿æ€§åŠ¨æ€ç³»ç»Ÿä¸çº¿æ€§MDPï¼Œèƒ½å¤Ÿåœ¨ä¸ä¼°è®¡è½¬ç§»æ¦‚ç‡çš„æƒ…å†µä¸‹ç›´æ¥æ”¹è¿›ç­–ç•¥ã€‚
3. é€šè¿‡æ•°å€¼ç¤ºä¾‹ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨éƒ¨åˆ†å·²çŸ¥éšæœºåŠ¨æ€ä¸‹å­¦ä¹ æœ€ä¼˜æ§åˆ¶ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šåº”ç”¨åœºæ™¯ï¼ˆå¦‚ç”µåŠ›ç³»ç»Ÿã€æœºå™¨äººå’Œç»æµå­¦ï¼‰æ¶‰åŠä¸éšæœºä¸”éš¾ä»¥å»ºæ¨¡çš„ç¯å¢ƒäº’åŠ¨çš„åŠ¨æ€ç³»ç»Ÿã€‚æœ¬æ–‡é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ§åˆ¶æ­¤ç±»ç³»ç»Ÿï¼Œè€ƒè™‘ä¸€ä¸ªç¡®å®šæ€§ã€ç¦»æ•£æ—¶é—´ã€çº¿æ€§ã€æ—¶é—´ä¸å˜çš„åŠ¨æ€ç³»ç»Ÿï¼Œå¹¶ä¸å…·æœ‰æœªçŸ¥è½¬ç§»æ ¸çš„ç‰¹å¾åŸºç¡€çº¿æ€§é©¬å°”å¯å¤«è¿‡ç¨‹ç›¸ç»“åˆã€‚ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ç§æ§åˆ¶ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–ç³»ç»ŸçŠ¶æ€ã€é©¬å°”å¯å¤«è¿‡ç¨‹å’Œæ§åˆ¶è¾“å…¥çš„äºŒæ¬¡æˆæœ¬ã€‚é€šè¿‡ç»“åˆç»å…¸çº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨ï¼ˆLQRï¼‰å’Œçº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰çš„ç‰¹ç‚¹ï¼Œæœ¬æ–‡æ¨å¯¼å‡ºæœ€ä¼˜çŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°å’Œç›¸åº”çš„æœ€ä¼˜ç­–ç•¥çš„æ˜¾å¼å‚æ•°å½¢å¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æœªçŸ¥è½¬ç§»æ¦‚ç‡çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä¼˜åŒ–æ§åˆ¶ç­–ç•¥çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚çš„éšæœºç¯å¢ƒä¸­éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡ï¼Œå¯¼è‡´æ§åˆ¶æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆçº¿æ€§åŠ¨æ€ç³»ç»Ÿä¸çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæ¨å¯¼å‡ºæœ€ä¼˜çŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°çš„æ˜¾å¼å‚æ•°å½¢å¼ï¼Œä»è€Œå®ç°ç›´æ¥çš„ç­–ç•¥æ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡ã€ç‰¹å¾æå–ã€ç­–ç•¥å­¦ä¹ å’Œç¨³å®šæ€§åˆ†æç­‰ä¸»è¦æ¨¡å—ã€‚é€šè¿‡æ§åˆ¶ç†è®ºå·¥å…·ï¼Œç¡®ä¿å­¦ä¹ åˆ°çš„ç­–ç•¥çš„ç¨³å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†ç»å…¸LQRä¸çº¿æ€§MDPæ¡†æ¶ç›¸ç»“åˆï¼Œä¿ç•™äº†LQRçš„å®ç°ç®€ä¾¿æ€§ï¼ŒåŒæ—¶å¼•å…¥äº†çº¿æ€§MDPçš„å¤æ‚éšæœºå»ºæ¨¡èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å¾åŸºç¡€çš„çº¿æ€§é©¬å°”å¯å¤«è¿‡ç¨‹ï¼Œä¼˜åŒ–äº†äºŒæ¬¡æˆæœ¬å‡½æ•°ï¼Œå¹¶è¿›è¡Œäº†æ ·æœ¬å¤æ‚åº¦åˆ†æï¼Œä»¥ç¡®ä¿æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨æ–‡ä¸­è¯¦ç»†é˜è¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å­¦ä¹ æœ€ä¼˜æ§åˆ¶ç­–ç•¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå°¤å…¶åœ¨éƒ¨åˆ†å·²çŸ¥éšæœºåŠ¨æ€ä¸‹çš„åº”ç”¨æ•ˆæœæ˜¾è‘—ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„æ½œåœ¨åº”ç”¨é¢†åŸŸï¼ŒåŒ…æ‹¬ç”µåŠ›ç³»ç»Ÿçš„åŠ¨æ€è°ƒèŠ‚ã€æœºå™¨äººæ§åˆ¶ä¸­çš„å†³ç­–ä¼˜åŒ–ä»¥åŠç»æµå­¦ä¸­çš„èµ„æºåˆ†é…é—®é¢˜ã€‚é€šè¿‡æä¾›ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ§åˆ¶ï¼Œæå‡ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œç¨³å®šæ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Many applications -- including power systems, robotics, and economics -- involve a dynamical system interacting with a stochastic and hard-to-model environment. We adopt a reinforcement learning approach to control such systems. Specifically, we consider a deterministic, discrete-time, linear, time-invariant dynamical system coupled with a feature-based linear Markov process with an unknown transition kernel. The objective is to learn a control policy that optimizes a quadratic cost over the system state, the Markov process, and the control input. Leveraging both components of the system, we derive an explicit parametric form for the optimal state-action value function and the corresponding optimal policy. Our model is distinct in combining aspects of both classical Linear Quadratic Regulator (LQR) and linear Markov decision process (MDP) frameworks. This combination retains the implementation simplicity of LQR, while allowing for sophisticated stochastic modeling afforded by linear MDPs, without estimating the transition probabilities, thereby enabling direct policy improvement. We use tools from control theory to provide theoretical guarantees on the stability of the system under the learned policy and provide a sample complexity analysis for its convergence to the optimal policy. We illustrate our results via a numerical example that demonstrates the effectiveness of our approach in learning the optimal control policy under partially known stochastic dynamics.

