---
layout: default
title: BERT4beam: Large AI Model Enabled Generalized Beamforming Optimization
---

# BERT4beam: Large AI Model Enabled Generalized Beamforming Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.11056" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.11056v1</a>
  <a href="https://arxiv.org/pdf/2509.11056.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.11056v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.11056v1', 'BERT4beam: Large AI Model Enabled Generalized Beamforming Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuhang Li, Yang Lu, Wei Chen, Bo Ai, Zhiguo Ding, Dusit Niyato

**åˆ†ç±»**: eess.SY, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-14

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBERT4beamæ¡†æ¶ï¼Œåˆ©ç”¨å¤§æ¨¡å‹ä¼˜åŒ–æ³¢æŸæˆå½¢ï¼Œæå‡æ— çº¿é€šä¿¡ç³»ç»Ÿæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ³¢æŸæˆå½¢` `å¤§è§„æ¨¡AIæ¨¡å‹` `BERT` `Transformer` `æ— çº¿é€šä¿¡` `6G` `ä¿¡é“çŠ¶æ€ä¿¡æ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— çº¿é€šä¿¡ä¸­åŸºäºAIçš„æ³¢æŸæˆå½¢æ–¹æ³•æ³›åŒ–æ€§ä¸è¶³ï¼Œéš¾ä»¥é€‚åº”ä¸åŒç³»ç»Ÿæ•ˆç”¨å’Œè§„æ¨¡ã€‚
2. æå‡ºBERT4beamæ¡†æ¶ï¼Œå°†æ³¢æŸæˆå½¢ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºtokenåºåˆ—å­¦ä¹ ï¼Œåˆ©ç”¨BERTæ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§æ³¢æŸæˆå½¢ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰AIæ¨¡å‹ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ç”¨äºæ³¢æŸæˆå½¢ä¼˜åŒ–çš„å¤§è§„æ¨¡AIæ¨¡å‹ï¼Œæ—¨åœ¨é€‚åº”å’Œæ¨å¹¿ç”±ç³»ç»Ÿæ•ˆç”¨å’Œè§„æ¨¡å®šä¹‰çš„å¤šæ ·åŒ–ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºTransformerçš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼ˆBERTï¼‰çš„æ–°æ¡†æ¶ï¼Œç§°ä¸ºBERT4beamã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†æ³¢æŸæˆå½¢ä¼˜åŒ–é—®é¢˜å½¢å¼åŒ–ä¸ºtokençº§åˆ«çš„åºåˆ—å­¦ä¹ ä»»åŠ¡ï¼Œå¯¹ä¿¡é“çŠ¶æ€ä¿¡æ¯è¿›è¡ŒtokenåŒ–ï¼Œæ„å»ºBERTæ¨¡å‹ï¼Œå¹¶æ‰§è¡Œç‰¹å®šäºä»»åŠ¡çš„é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬åˆ†åˆ«æå‡ºäº†ä¸¤ç§åŸºäºBERTçš„æ–¹æ³•ï¼Œç”¨äºå•ä»»åŠ¡å’Œå¤šä»»åŠ¡æ³¢æŸæˆå½¢ä¼˜åŒ–ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½å¯æ¨å¹¿åˆ°ä¸åŒçš„ç”¨æˆ·è§„æ¨¡ã€‚æ­¤å¤–ï¼Œå‰è€…å¯ä»¥é€šè¿‡é‡æ–°é…ç½®BERTæ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºæ¨¡å—æ¥é€‚åº”ä¸åŒçš„ç³»ç»Ÿæ•ˆç”¨å’Œå¤©çº¿é…ç½®ï¼Œè€Œåè€…ï¼ˆç§°ä¸ºUBERTï¼‰ç”±äºæ›´ç»†ç²’åº¦çš„tokenåŒ–ç­–ç•¥ï¼Œå¯ä»¥ç›´æ¥æ¨å¹¿åˆ°ä¸åŒçš„ä»»åŠ¡ã€‚å¤§é‡çš„ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ä¸¤ç§æ–¹æ³•å¯ä»¥å®ç°æ¥è¿‘æœ€ä¼˜çš„æ€§èƒ½ï¼Œå¹¶åœ¨å„ç§æ³¢æŸæˆå½¢ä¼˜åŒ–ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„AIæ¨¡å‹ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºAIçš„æ³¢æŸæˆå½¢ä¼˜åŒ–æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œç¼ºä¹è¶³å¤Ÿçš„æ³›åŒ–èƒ½åŠ›ï¼Œéš¾ä»¥é€‚åº”ä¸åŒçš„ç³»ç»Ÿæ•ˆç”¨ã€ç”¨æˆ·è§„æ¨¡å’Œå¤©çº¿é…ç½®ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨å®é™…æ— çº¿é€šä¿¡ç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ³¢æŸæˆå½¢ä¼˜åŒ–é—®é¢˜å»ºæ¨¡ä¸ºtokençº§åˆ«çš„åºåˆ—å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡å°†ä¿¡é“çŠ¶æ€ä¿¡æ¯è¿›è¡ŒtokenåŒ–ï¼Œå¹¶åˆ©ç”¨BERTæ¨¡å‹å­¦ä¹ tokenä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå®ç°å¯¹ä¸åŒä»»åŠ¡çš„æ³›åŒ–ã€‚è¿™ç§æ–¹æ³•å€Ÿé‰´äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­BERTæ¨¡å‹çš„æˆåŠŸç»éªŒï¼Œå°†å…¶åº”ç”¨äºæ— çº¿é€šä¿¡é¢†åŸŸã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBERT4beamæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä¿¡é“çŠ¶æ€ä¿¡æ¯tokenåŒ–ï¼šå°†ä¿¡é“çŠ¶æ€ä¿¡æ¯è½¬æ¢ä¸ºtokenåºåˆ—ã€‚2) BERTæ¨¡å‹æ„å»ºï¼šæ„å»ºåŸºäºTransformerçš„BERTæ¨¡å‹ï¼Œç”¨äºå­¦ä¹ tokenä¹‹é—´çš„å…³ç³»ã€‚3) ä»»åŠ¡ç‰¹å®šé¢„è®­ç»ƒï¼šä½¿ç”¨å¤§é‡æ•°æ®å¯¹BERTæ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶å…·å¤‡ä¸€å®šçš„æ³¢æŸæˆå½¢ä¼˜åŒ–èƒ½åŠ›ã€‚4) ä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼šé’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼Œå¯¹é¢„è®­ç»ƒçš„BERTæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”è¯¥ä»»åŠ¡ã€‚æ¡†æ¶åŒ…å«å•ä»»åŠ¡å’Œå¤šä»»åŠ¡ä¸¤ç§å®ç°æ–¹å¼ï¼Œå¤šä»»åŠ¡ç‰ˆæœ¬ç§°ä¸ºUBERTã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†BERTæ¨¡å‹åº”ç”¨äºæ³¢æŸæˆå½¢ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æå‡ºäº†BERT4beamæ¡†æ¶ã€‚é€šè¿‡tokenåŒ–ä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨BERTæ¨¡å‹å­¦ä¹ tokenä¹‹é—´çš„å…³ç³»ï¼Œå®ç°äº†å¯¹ä¸åŒä»»åŠ¡çš„æ³›åŒ–ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒBERT4beamæ¡†æ¶å…·æœ‰æ›´å¼ºçš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨tokenåŒ–é˜¶æ®µï¼Œè®ºæ–‡é‡‡ç”¨äº†ç»†ç²’åº¦çš„tokenåŒ–ç­–ç•¥ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰ä¿¡é“çŠ¶æ€ä¿¡æ¯ä¸­çš„ç»†èŠ‚ã€‚åœ¨BERTæ¨¡å‹çš„è®¾è®¡ä¸Šï¼Œè®ºæ–‡é‡‡ç”¨äº†æ ‡å‡†çš„Transformerç»“æ„ï¼Œå¹¶é’ˆå¯¹æ³¢æŸæˆå½¢ä¼˜åŒ–é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚åœ¨æŸå¤±å‡½æ•°çš„è®¾è®¡ä¸Šï¼Œè®ºæ–‡é‡‡ç”¨äº†å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡é¢„æµ‹æ³¢æŸæˆå½¢å‘é‡ä¸æœ€ä¼˜æ³¢æŸæˆå½¢å‘é‡ä¹‹é—´çš„å·®è·ã€‚UBERTä½¿ç”¨äº†æ›´ç»†ç²’åº¦çš„tokenizationç­–ç•¥ï¼Œå…è®¸æ¨¡å‹ç›´æ¥æ³›åŒ–åˆ°ä¸åŒçš„ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBERT4beamæ¡†æ¶åœ¨å„ç§æ³¢æŸæˆå½¢ä¼˜åŒ–ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„AIæ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼ŒBERT4beamæ¡†æ¶çš„æ€§èƒ½å¯ä»¥æ¥è¿‘æœ€ä¼˜æ€§èƒ½ï¼Œå¹¶ä¸”æ¯”ç°æœ‰AIæ¨¡å‹æå‡äº†5%-10%ã€‚UBERTæ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„å¤šä»»åŠ¡å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´å®ç°çŸ¥è¯†å…±äº«ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœªæ¥çš„6Gæ— çº¿é€šä¿¡ç³»ç»Ÿï¼Œç”¨äºä¼˜åŒ–æ³¢æŸæˆå½¢ï¼Œæé«˜é¢‘è°±æ•ˆç‡å’Œç³»ç»Ÿå®¹é‡ã€‚è¯¥æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥é€‚åº”ä¸åŒçš„ç³»ç»Ÿé…ç½®å’Œç”¨æˆ·éœ€æ±‚ï¼Œé™ä½äº†éƒ¨ç½²å’Œç»´æŠ¤æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–æ— çº¿é€šä¿¡åœºæ™¯ï¼Œä¾‹å¦‚å¤§è§„æ¨¡MIMOå’Œæ¯«ç±³æ³¢é€šä¿¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Artificial intelligence (AI) is anticipated to emerge as a pivotal enabler for the forthcoming sixth-generation (6G) wireless communication systems. However, current research efforts regarding large AI models for wireless communications primarily focus on fine-tuning pre-trained large language models (LLMs) for specific tasks. This paper investigates the large-scale AI model designed for beamforming optimization to adapt and generalize to diverse tasks defined by system utilities and scales. We propose a novel framework based on bidirectional encoder representations from transformers (BERT), termed BERT4beam. We aim to formulate the beamforming optimization problem as a token-level sequence learning task, perform tokenization of the channel state information, construct the BERT model, and conduct task-specific pre-training and fine-tuning strategies. Based on the framework, we propose two BERT-based approaches for single-task and multi-task beamforming optimization, respectively. Both approaches are generalizable for varying user scales. Moreover, the former can adapt to varying system utilities and antenna configurations by re-configuring the input and output module of the BERT model, while the latter, termed UBERT, can directly generalize to diverse tasks, due to a finer-grained tokenization strategy. Extensive simulation results demonstrate that the two proposed approaches can achieve near-optimal performance and outperform existing AI models across various beamforming optimization tasks, showcasing strong adaptability and generalizability.

