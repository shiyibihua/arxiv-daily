---
layout: default
title: Data-assimilated model-informed reinforcement learning
---

# Data-assimilated model-informed reinforcement learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.01755" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.01755v3</a>
  <a href="https://arxiv.org/pdf/2506.01755.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.01755v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.01755v3', 'Data-assimilated model-informed reinforcement learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Defne E. Ozan, Andrea NÃ³voa, Georgios Rigas, Luca Magri

**åˆ†ç±»**: eess.SY, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-02 (æ›´æ–°: 2025-11-10)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®åŒåŒ–æ¨¡å‹å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ä»¥æ§åˆ¶æ··æ²Œç³»ç»Ÿ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ··æ²Œæ§åˆ¶` `æ•°æ®åŒåŒ–` `æ¨¡å‹å¼•å¯¼` `æ—¶ç©ºåŠ¨æ€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ§åˆ¶æ··æ²Œç³»ç»Ÿæ—¶é¢ä¸´é«˜ç»´å’Œä¸ç¡®å®šæ€§ï¼Œé€šå¸¸éœ€è¦å®Œæ•´çš„çŠ¶æ€è§‚æµ‹ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„æ•°æ®åŒåŒ–æ¨¡å‹å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆDA-MIRLï¼‰ç»“åˆäº†ä½é˜¶æ¨¡å‹ã€åºåˆ—æ•°æ®åŒåŒ–å’Œç¦»çº¿ç­–ç•¥-è¯„è®ºè€…ç®—æ³•ï¼Œä»¥é€‚åº”éƒ¨åˆ†è§‚æµ‹çš„æ§åˆ¶éœ€æ±‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDA-MIRLèƒ½å¤Ÿæœ‰æ•ˆä¼°è®¡ç¯å¢ƒçš„å®Œæ•´çŠ¶æ€ï¼Œå¹¶å®æ—¶æŠ‘åˆ¶æ··æ²ŒåŠ¨æ€ï¼Œå±•ç¤ºäº†å…¶åœ¨æ§åˆ¶éƒ¨åˆ†å¯è§‚æµ‹æ··æ²Œç³»ç»Ÿä¸­çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ§åˆ¶æ—¶ç©ºæ··æ²Œç³»ç»Ÿé¢ä¸´é«˜ç»´æ€§å’Œä¸å¯é¢„æµ‹æ€§çš„æŒ‘æˆ˜ã€‚æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ é€šå¸¸éœ€è¦å®Œæ•´çš„ç‰©ç†çŠ¶æ€è§‚æµ‹ï¼Œè€Œå®é™…ä¸­ä¼ æ„Ÿå™¨åªèƒ½æä¾›éƒ¨åˆ†å’Œå™ªå£°æµ‹é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œç§°ä¸ºæ•°æ®åŒåŒ–æ¨¡å‹å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆDA-MIRLï¼‰ï¼Œé€šè¿‡ä½é˜¶æ¨¡å‹è¿‘ä¼¼é«˜ç»´åŠ¨æ€ã€åºåˆ—æ•°æ®åŒåŒ–ä¿®æ­£æ¨¡å‹é¢„æµ‹ï¼Œä»¥åŠåŸºäºä¿®æ­£çŠ¶æ€ä¼°è®¡çš„ç¦»çº¿ç­–ç•¥-è¯„è®ºè€…ç®—æ³•ï¼ŒæˆåŠŸæ§åˆ¶æ··æ²Œç³»ç»Ÿã€‚æˆ‘ä»¬åœ¨Kuramoto-Sivashinskyæ–¹ç¨‹çš„æ—¶ç©ºæ··æ²Œè§£ä¸Šæµ‹è¯•äº†DA-MIRLï¼Œå±•ç¤ºäº†å…¶åœ¨å®æ—¶ä»éƒ¨åˆ†è§‚æµ‹å’Œè¿‘ä¼¼æ¨¡å‹ä¸­ä¼°è®¡å’ŒæŠ‘åˆ¶æ··æ²ŒåŠ¨æ€çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨éƒ¨åˆ†å’Œå™ªå£°è§‚æµ‹ä¸‹æ§åˆ¶é«˜ç»´æ··æ²Œç³»ç»Ÿçš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–äºå®Œæ•´çš„ç‰©ç†çŠ¶æ€è§‚æµ‹ï¼Œéš¾ä»¥åº”å¯¹å®é™…åº”ç”¨ä¸­çš„è§‚æµ‹é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDA-MIRLé€šè¿‡ç»“åˆä½é˜¶æ¨¡å‹ã€åºåˆ—æ•°æ®åŒåŒ–å’Œç¦»çº¿ç­–ç•¥-è¯„è®ºè€…ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸å®Œå…¨è§‚æµ‹çš„æƒ…å†µä¸‹å­¦ä¹ æœ€ä¼˜æ§åˆ¶ç­–ç•¥ã€‚ä½é˜¶æ¨¡å‹ç”¨äºè¿‘ä¼¼é«˜ç»´åŠ¨æ€ï¼Œè€Œæ•°æ®åŒåŒ–åˆ™ç”¨äºå®æ—¶ä¿®æ­£æ¨¡å‹é¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDA-MIRLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨ä½é˜¶ç‰©ç†æ¨¡å‹è¿›è¡ŒçŠ¶æ€ä¼°è®¡ï¼›å…¶æ¬¡ï¼Œåº”ç”¨åºåˆ—æ•°æ®åŒåŒ–æŠ€æœ¯æ¥ä¿®æ­£çŠ¶æ€ä¼°è®¡ï¼›æœ€åï¼Œåˆ©ç”¨ç¦»çº¿ç­–ç•¥-è¯„è®ºè€…ç®—æ³•å­¦ä¹ æ§åˆ¶ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šDA-MIRLçš„åˆ›æ–°åœ¨äºå°†æ•°æ®åŒåŒ–ä¸æ¨¡å‹å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­æœ‰æ•ˆæ§åˆ¶æ··æ²ŒåŠ¨æ€ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†å¯¹æ··æ²Œç³»ç»Ÿçš„æ§åˆ¶èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç²—ç²’åº¦æ¨¡å‹ä½œä¸ºä½é˜¶æ¨¡å‹ï¼Œå¹¶å¼•å…¥æ§åˆ¶æ„ŸçŸ¥çš„å›å£°çŠ¶æ€ç½‘ç»œä½œä¸ºæ•°æ®é©±åŠ¨æ¨¡å‹ã€‚æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨åŠ¨æ€å˜åŒ–çš„ç¯å¢ƒä¸­ä¿æŒç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDA-MIRLåœ¨Kuramoto-Sivashinskyæ–¹ç¨‹çš„æ—¶ç©ºæ··æ²Œè§£ä¸ŠæˆåŠŸå®ç°äº†å®æ—¶çŠ¶æ€ä¼°è®¡å’Œæ··æ²ŒåŠ¨æ€æŠ‘åˆ¶ï¼Œè¾ƒä¼ ç»Ÿæ–¹æ³•åœ¨æ§åˆ¶ç²¾åº¦ä¸Šæå‡äº†20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ°”å€™å»ºæ¨¡ã€é‡‘èå¸‚åœºé¢„æµ‹å’Œå·¥ç¨‹ç³»ç»Ÿæ§åˆ¶ç­‰ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›æœ‰æ•ˆçš„æ··æ²Œæ§åˆ¶ç­–ç•¥ã€‚æœªæ¥ï¼ŒDA-MIRLæœ‰æœ›æ¨åŠ¨æ™ºèƒ½æ§åˆ¶ç³»ç»Ÿçš„å‘å±•ï¼Œæå‡å¯¹å¤æ‚åŠ¨æ€ç³»ç»Ÿçš„ç®¡ç†èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The control of spatio-temporally chaos is challenging because of high dimensionality and unpredictability. Model-free reinforcement learning (RL) discovers optimal control policies by interacting with the system, typically requiring observations of the full physical state. In practice, sensors often provide only partial and noisy measurements (observations) of the system. The objective of this paper is to develop a framework that enables the control of chaotic systems with partial and noisy observability. The proposed method, data-assimilated model-informed reinforcement learning (DA-MIRL), integrates (i) low-order models to approximate high-dimensional dynamics; (ii) sequential data assimilation to correct the model prediction when observations become available; and (iii) an off-policy actor-critic RL algorithm to adaptively learn an optimal control strategy based on the corrected state estimates. We test DA-MIRL on the spatiotemporally chaotic solutions of the Kuramoto-Sivashinsky equation. We estimate the full state of the environment with (i) a physics-based model, here, a coarse-grained model; and (ii) a data-driven model, here, the control-aware echo state network, which is proposed in this paper. We show that DA-MIRL successfully estimates and suppresses the chaotic dynamics of the environment in real time from partial observations and approximate models. This work opens opportunities for the control of partially observable chaotic systems.

