---
layout: default
title: Activation Quantization of Vision Encoders Needs Prefixing Registers
---

# Activation Quantization of Vision Encoders Needs Prefixing Registers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04547" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.04547v3</a>
  <a href="https://arxiv.org/pdf/2510.04547.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04547v3" onclick="toggleFavorite(this, '2510.04547v3', 'Activation Quantization of Vision Encoders Needs Prefixing Registers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seunghyeon Kim, Jinho Kim, Taesun Yeom, Wonpyo Park, Kyuyeun Kim, Jaeho Lee

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-06 (æ›´æ–°: 2025-11-28)

**å¤‡æ³¨**: 19 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRegCacheï¼Œé€šè¿‡å‰ç¼€å¯„å­˜å™¨å®ç°è§†è§‰ç¼–ç å™¨æ¿€æ´»é‡åŒ–çš„æ— è®­ç»ƒä¼˜åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰ç¼–ç å™¨` `é‡åŒ–` `æ¿€æ´»å€¼` `å¼‚å¸¸å€¼æŠ‘åˆ¶` `å‰ç¼€å¯„å­˜å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰ç¼–ç å™¨é‡åŒ–é¢ä¸´å¤§è§„æ¨¡æ¿€æ´»å¼‚å¸¸å€¼çš„æŒ‘æˆ˜ï¼Œå³ä½¿åœ¨8ä½ç²¾åº¦ä¸‹ä¹Ÿéš¾ä»¥ä¿è¯æ€§èƒ½ã€‚
2. RegCacheé€šè¿‡å¼•å…¥å‰ç¼€tokenæ¥æŠ‘åˆ¶æ¿€æ´»å¼‚å¸¸å€¼ï¼Œæ— éœ€è®­ç»ƒï¼Œå¯ä½œä¸ºæ’ä»¶é›†æˆåˆ°ç°æœ‰é‡åŒ–æµç¨‹ä¸­ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRegCacheèƒ½æœ‰æ•ˆæå‡é‡åŒ–åè§†è§‰ç¼–ç å™¨çš„ç²¾åº¦ï¼Œé€‚ç”¨äºæ–‡æœ¬ç›‘ç£å’Œè‡ªç›‘ç£æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºTransformerçš„è§†è§‰ç¼–ç å™¨ï¼Œå¦‚CLIPï¼Œåœ¨å¤šæ¨¡æ€æ™ºèƒ½ä¸­è‡³å…³é‡è¦ï¼Œé©±åŠ¨ç€ä»è‡ªä¸»ç½‘ç»œä»£ç†åˆ°æœºå™¨äººæ§åˆ¶ç­‰åº”ç”¨ã€‚ç”±äºè¿™äº›åº”ç”¨é€šå¸¸éœ€è¦å¯¹æµ·é‡è§†è§‰æ•°æ®è¿›è¡Œå®æ—¶å¤„ç†ï¼Œå› æ­¤é™ä½è§†è§‰ç¼–ç å™¨çš„æ¨ç†æˆæœ¬è‡³å…³é‡è¦ã€‚é‡åŒ–æä¾›äº†ä¸€æ¡å¯è¡Œçš„é€”å¾„ï¼Œä½†ç”±äºå¤§è§„æ¨¡æ¿€æ´»ï¼ˆå³å¼‚å¸¸å€¼ï¼‰ï¼Œå³ä½¿åœ¨8ä½ç²¾åº¦ä¸‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºRegCacheï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„ç®—æ³•ï¼Œç”¨äºç¼“è§£å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ä¸­çš„å¼‚å¸¸å€¼ï¼Œå¹¶ä½œä¸ºä¸€ä¸ªæ’ä»¶æ¨¡å—åº”ç”¨äºå…¶ä»–é‡åŒ–æ–¹æ³•ä¹‹ä¸Šã€‚RegCacheå°†æ˜“äºå‡ºç°å¼‚å¸¸å€¼ä½†è¯­ä¹‰ä¸Šæ— æ„ä¹‰çš„å‰ç¼€tokenå¼•å…¥ç›®æ ‡è§†è§‰ç¼–ç å™¨ï¼Œä»è€Œé˜²æ­¢å…¶ä»–tokenå‡ºç°å¼‚å¸¸å€¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è§†è§‰ç¼–ç å™¨ä¸­çš„å¼‚å¸¸å€¼ä¸è¯­è¨€æ¨¡å‹ä¸­çš„å¼‚å¸¸å€¼è¡¨ç°ä¸åŒï¼Œè¿™ä¿ƒä½¿äº†ä¸¤é¡¹æŠ€æœ¯åˆ›æ–°ï¼šä¸­é—´å±‚å‰ç¼€å’Œtokenåˆ é™¤ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆæé«˜æ–‡æœ¬ç›‘ç£å’Œè‡ªç›‘ç£è§†è§‰ç¼–ç å™¨ä¸­é‡åŒ–æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰ç¼–ç å™¨é‡åŒ–è¿‡ç¨‹ä¸­ï¼Œç”±äºæ¿€æ´»å€¼ä¸­å­˜åœ¨å¤§é‡å¼‚å¸¸å€¼ï¼ˆoutliersï¼‰è€Œå¯¼è‡´çš„ç²¾åº¦ä¸‹é™é—®é¢˜ã€‚ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡è§†è§‰ç¼–ç å™¨æ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆæŠ‘åˆ¶è¿™äº›å¼‚å¸¸å€¼ï¼Œå¯¼è‡´é‡åŒ–åçš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—é™ä½ã€‚å°¤å…¶æ˜¯åœ¨ä½æ¯”ç‰¹é‡åŒ–çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªé—®é¢˜æ›´åŠ çªå‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥â€œå‰ç¼€å¯„å­˜å™¨â€ï¼ˆPrefixing Registersï¼‰ï¼Œå³åœ¨è§†è§‰ç¼–ç å™¨çš„è¾“å…¥tokenåºåˆ—ä¸­æ·»åŠ ä¸€äº›ç‰¹æ®Šçš„ã€è¯­ä¹‰ä¸Šæ— æ„ä¹‰çš„tokenã€‚è¿™äº›tokençš„è®¾è®¡ç›®çš„æ˜¯å¸å¼•å¹¶éš”ç¦»æ¿€æ´»å€¼ä¸­çš„å¼‚å¸¸å€¼ï¼Œä»è€Œé˜²æ­¢å…¶ä»–å…·æœ‰å®é™…è¯­ä¹‰ä¿¡æ¯çš„tokenå—åˆ°å¼‚å¸¸å€¼çš„å½±å“ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æŠ‘åˆ¶é‡åŒ–è¿‡ç¨‹ä¸­çš„ç²¾åº¦æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRegCacheä½œä¸ºä¸€ä¸ªæ’ä»¶æ¨¡å—ï¼Œå¯ä»¥æ·»åŠ åˆ°ç°æœ‰çš„è§†è§‰ç¼–ç å™¨é‡åŒ–æµç¨‹ä¸­ã€‚å…¶ä¸»è¦æ­¥éª¤åŒ…æ‹¬ï¼š1ï¼‰åœ¨è¾“å…¥tokenåºåˆ—ä¸­æ·»åŠ å‰ç¼€tokenï¼›2ï¼‰åœ¨è§†è§‰ç¼–ç å™¨çš„ä¸­é—´å±‚ï¼ˆè€Œéä»…åœ¨è¾“å…¥å±‚ï¼‰æ·»åŠ å‰ç¼€tokenï¼Œä»¥æ›´å¥½åœ°æ•è·å’Œéš”ç¦»å¼‚å¸¸å€¼ï¼›3ï¼‰å¯¹æ·»åŠ çš„å‰ç¼€tokenè¿›è¡Œé€‰æ‹©æ€§åˆ é™¤ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚æ•´ä¸ªæ¡†æ¶æ— éœ€é¢å¤–çš„è®­ç»ƒï¼Œå¯ä»¥ç›´æ¥åº”ç”¨äºé¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1ï¼‰æå‡ºäº†å‰ç¼€å¯„å­˜å™¨çš„æ¦‚å¿µï¼Œé€šè¿‡å¼•å…¥è¯­ä¹‰ä¸Šæ— æ„ä¹‰çš„tokenæ¥æŠ‘åˆ¶æ¿€æ´»å¼‚å¸¸å€¼ï¼›2ï¼‰è§‚å¯Ÿåˆ°è§†è§‰ç¼–ç å™¨ä¸­çš„å¼‚å¸¸å€¼è¡Œä¸ºä¸è¯­è¨€æ¨¡å‹ä¸åŒï¼Œå› æ­¤æå‡ºäº†ä¸­é—´å±‚å‰ç¼€å’Œtokenåˆ é™¤ç­–ç•¥ï¼Œæ›´æœ‰æ•ˆåœ°å¤„ç†è§†è§‰ç¼–ç å™¨çš„é‡åŒ–é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…·ä½“å®ç°ä¸Šï¼Œå‰ç¼€tokençš„æ•°é‡æ˜¯ä¸€ä¸ªé‡è¦çš„å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„æ¨¡å‹å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚è®ºæ–‡ä¸­è¿˜æåˆ°ï¼Œåœ¨ä¸­é—´å±‚æ·»åŠ å‰ç¼€tokenå¯ä»¥æ›´å¥½åœ°æ•è·å’Œéš”ç¦»å¼‚å¸¸å€¼ï¼Œè¿™ä¸åœ¨è¾“å…¥å±‚æ·»åŠ å‰ç¼€tokençš„æ•ˆæœä¸åŒã€‚æ­¤å¤–ï¼Œtokenåˆ é™¤ç­–ç•¥ä¹Ÿæ˜¯ä¸€ä¸ªå…³é”®çš„è®¾è®¡ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„æ²¡æœ‰æ”¹å˜ï¼ŒRegCacheä½œä¸ºä¸€ä¸ªæ’ä»¶ï¼Œä¸»è¦ä½œç”¨äºè¾“å…¥tokenåºåˆ—çš„å¤„ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRegCacheèƒ½å¤Ÿæ˜¾è‘—æé«˜é‡åŒ–åè§†è§‰ç¼–ç å™¨çš„ç²¾åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨CLIPæ¨¡å‹ä¸Šï¼Œä½¿ç”¨RegCacheåï¼Œé‡åŒ–æ¨¡å‹çš„ç²¾åº¦æå‡äº†å¤šä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼ŒRegCacheåœ¨æ–‡æœ¬ç›‘ç£å’Œè‡ªç›‘ç£è§†è§‰ç¼–ç å™¨ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰çš„é‡åŒ–æµç¨‹ä¸­ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RegCacheæŠ€æœ¯å¯å¹¿æ³›åº”ç”¨äºå„ç§ä¾èµ–è§†è§‰ç¼–ç å™¨çš„å¤šæ¨¡æ€æ™ºèƒ½åº”ç”¨ä¸­ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶ã€å›¾åƒæœç´¢ã€è§†é¢‘åˆ†æç­‰ã€‚é€šè¿‡é™ä½è§†è§‰ç¼–ç å™¨çš„æ¨ç†æˆæœ¬ï¼Œå¯ä»¥å®ç°æ›´é«˜æ•ˆã€æ›´å®æ—¶çš„è§†è§‰ä¿¡æ¯å¤„ç†ï¼Œä»è€Œæå‡ç›¸å…³åº”ç”¨çš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚è¯¥æŠ€æœ¯è¿˜æœ‰åŠ©äºåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²å¤æ‚çš„è§†è§‰æ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\textit{RegCache}$, a training-free algorithm that mitigates outliers in large-scale pretrained vision encoders and serves as a plug-in module that can be applied on top of other quantization methods. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.

