---
layout: default
title: Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning
---

# Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20605" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20605v1</a>
  <a href="https://arxiv.org/pdf/2512.20605.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20605v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20605v1', 'Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherre, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise AgÃ¼era y Arcas, Alexander Meulemans, JoÃ£o Sacramento

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå†…éƒ¨å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨è‡ªå›å½’æ¨¡å‹ä¸­çš„æ—¶é—´æŠ½è±¡å®ç°åˆ†å±‚å¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªå›å½’æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `åˆ†å±‚å¼ºåŒ–å­¦ä¹ ` `æ—¶é—´æŠ½è±¡` `å†…éƒ¨å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨è‡ªå›å½’æ¨¡å‹ä¸­é€tokené‡‡æ ·åŠ¨ä½œæ•ˆç‡ä½ï¼Œå°¤å…¶åœ¨å¥–åŠ±ç¨€ç–æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚
2. æå‡ºå†…éƒ¨å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨é«˜é˜¶æ¨¡å‹ä¸­å­¦ä¹ æ—¶é—´æŠ½è±¡åŠ¨ä½œï¼Œæ§åˆ¶åº•å±‚è‡ªå›å½’æ¨¡å‹çš„æ¿€æ´»ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå‹ç¼©é•¿åºåˆ—åŠ¨ä½œï¼Œä»ç¨€ç–å¥–åŠ±ä¸­å­¦ä¹ ï¼Œä¼˜äºæ ‡å‡†å¼ºåŒ–å­¦ä¹ ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨è‡ªå›å½’æ¨¡å‹å†…éƒ¨è¡¨ç¤ºä¸­è¿›è¡ŒåŠ¨ä½œå’Œæ¢ç´¢çš„æ–¹æ³•ï¼Œä»¥å…‹æœä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¸­token-by-tokené‡‡æ ·å¯¼è‡´çš„å­¦ä¹ æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¥–åŠ±ç¨€ç–çš„æƒ…å†µä¸‹ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªé«˜é˜¶éå› æœåºåˆ—æ¨¡å‹ï¼Œå…¶è¾“å‡ºæ§åˆ¶åŸºç¡€è‡ªå›å½’æ¨¡å‹çš„æ®‹å·®æµæ¿€æ´»ï¼Œä»è€Œå‘ç°æ—¶é—´æŠ½è±¡åŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥é«˜é˜¶æ¨¡å‹èƒ½å¤Ÿå°†é•¿æ¿€æ´»åºåˆ—å—å‹ç¼©åˆ°å†…éƒ¨æ§åˆ¶å™¨ä¸Šï¼Œæ¯ä¸ªæ§åˆ¶å™¨æ‰§è¡Œä¸€ç³»åˆ—è¡Œä¸ºä¸Šæœ‰æ„ä¹‰çš„åŠ¨ä½œï¼Œè¿™äº›åŠ¨ä½œåœ¨é•¿æ—¶é—´å°ºåº¦ä¸Šå±•å¼€ï¼Œå¹¶ä¼´éšä¸€ä¸ªå­¦ä¹ åˆ°çš„ç»ˆæ­¢æ¡ä»¶ã€‚é€šè¿‡ç›´æ¥å†…éƒ¨æ§åˆ¶å™¨å¼ºåŒ–ï¼Œå³â€œå†…éƒ¨å¼ºåŒ–å­¦ä¹ â€ï¼Œå¯ä»¥åœ¨æ ‡å‡†å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¤±è´¥çš„æƒ…å†µä¸‹ä»ç¨€ç–å¥–åŠ±ä¸­å­¦ä¹ ã€‚è¯¥ç ”ç©¶è¡¨æ˜äº†åœ¨è‡ªå›å½’æ¨¡å‹ä¸­è¿›è¡Œæ½œåœ¨åŠ¨ä½œç”Ÿæˆå’Œå¼ºåŒ–çš„ä¼˜åŠ¿ï¼Œå¹¶è®¤ä¸ºå†…éƒ¨å¼ºåŒ–å­¦ä¹ æ˜¯å®ç°åŸºç¡€æ¨¡å‹ä¸­åˆ†å±‚å¼ºåŒ–å­¦ä¹ çš„ä¸€ç§æœ‰å‰æ™¯çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è§„æ¨¡è‡ªå›å½’æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ—¶ï¼Œé€šå¸¸é‡‡ç”¨é€tokenç”ŸæˆåŠ¨ä½œçš„æ–¹å¼è¿›è¡Œæ¢ç´¢ã€‚è¿™ç§æ–¹å¼åœ¨å¥–åŠ±ç¨€ç–çš„ç¯å¢ƒä¸‹æ•ˆç‡æä½ï¼Œå› ä¸ºæ¨¡å‹éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´æ‰èƒ½æ¢ç´¢åˆ°æœ‰æ„ä¹‰çš„åŠ¨ä½œåºåˆ—ã€‚å› æ­¤ï¼Œå¦‚ä½•æé«˜è‡ªå›å½’æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢æ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨å¥–åŠ±ç¨€ç–çš„ç¯å¢ƒä¸‹ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨è‡ªå›å½’æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºä¸­è¿›è¡ŒåŠ¨ä½œå’Œæ¢ç´¢ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨è¾“å‡ºç©ºé—´ä¸­é€tokenç”ŸæˆåŠ¨ä½œã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªé«˜é˜¶éå› æœåºåˆ—æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å­¦ä¹ æ§åˆ¶åº•å±‚è‡ªå›å½’æ¨¡å‹çš„æ®‹å·®æµæ¿€æ´»ï¼Œä»è€Œå®ç°å¯¹æ—¶é—´æŠ½è±¡åŠ¨ä½œçš„å»ºæ¨¡ã€‚è¿™æ ·ï¼Œé«˜é˜¶æ¨¡å‹å¯ä»¥ä¸€æ¬¡æ€§ç”Ÿæˆä¸€ä¸ªåŠ¨ä½œåºåˆ—ï¼Œè€Œä¸æ˜¯é€tokenç”Ÿæˆï¼Œä»è€Œæé«˜äº†æ¢ç´¢æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šä¸€ä¸ªåŸºç¡€çš„è‡ªå›å½’æ¨¡å‹å’Œä¸€ä¸ªé«˜é˜¶éå› æœåºåˆ—æ¨¡å‹ã€‚åŸºç¡€è‡ªå›å½’æ¨¡å‹è´Ÿè´£ç”Ÿæˆåº•å±‚çš„åŠ¨ä½œåºåˆ—ï¼Œè€Œé«˜é˜¶æ¨¡å‹åˆ™è´Ÿè´£ç”Ÿæˆæ§åˆ¶ä¿¡å·ï¼Œè¿™äº›æ§åˆ¶ä¿¡å·ä½œç”¨äºåŸºç¡€æ¨¡å‹çš„æ®‹å·®æµæ¿€æ´»ï¼Œä»è€Œå½±å“åŸºç¡€æ¨¡å‹çš„è¡Œä¸ºã€‚åœ¨é«˜é˜¶æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç›´æ¥å¯¹é«˜é˜¶æ¨¡å‹çš„æ§åˆ¶å™¨è¿›è¡Œå¼ºåŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´æœ‰åˆ©äºä»»åŠ¡å®Œæˆçš„æ§åˆ¶ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†â€œå†…éƒ¨å¼ºåŒ–å­¦ä¹ â€çš„æ¦‚å¿µï¼Œå³ç›´æ¥åœ¨è‡ªå›å½’æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒï¼Œå†…éƒ¨å¼ºåŒ–å­¦ä¹ ä¸éœ€è¦ç›´æ¥å¯¹è¾“å‡ºç©ºé—´ä¸­çš„åŠ¨ä½œè¿›è¡Œå¼ºåŒ–ï¼Œè€Œæ˜¯é€šè¿‡å¯¹å†…éƒ¨æ§åˆ¶å™¨çš„å¼ºåŒ–ï¼Œé—´æ¥åœ°å½±å“æ¨¡å‹çš„è¡Œä¸ºã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è‡ªå›å½’æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šé«˜é˜¶æ¨¡å‹é‡‡ç”¨éå› æœåºåˆ—æ¨¡å‹ï¼Œå…è®¸æ¨¡å‹åœ¨ç”Ÿæˆæ§åˆ¶ä¿¡å·æ—¶è€ƒè™‘æœªæ¥çš„ä¿¡æ¯ã€‚é«˜é˜¶æ¨¡å‹çš„æŸå¤±å‡½æ•°åŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†æ˜¯æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ§åˆ¶å™¨çš„è¡Œä¸ºï¼›å¦ä¸€éƒ¨åˆ†æ˜¯æ­£åˆ™åŒ–æŸå¤±ï¼Œç”¨äºçº¦æŸæ§åˆ¶å™¨çš„è¾“å‡ºï¼Œä½¿å…¶æ›´åŠ å¹³æ»‘å’Œå¯è§£é‡Šã€‚æ­¤å¤–ï¼Œé«˜é˜¶æ¨¡å‹è¿˜å­¦ä¹ ä¸€ä¸ªç»ˆæ­¢æ¡ä»¶ï¼Œç”¨äºåˆ¤æ–­ä½•æ—¶åœæ­¢å½“å‰æ§åˆ¶å™¨çš„æ‰§è¡Œï¼Œä»è€Œå®ç°å¯¹åŠ¨ä½œåºåˆ—çš„åˆ†å‰²ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20605v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20605v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20605v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç½‘æ ¼ä¸–ç•Œå’ŒMuJoCoä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ æ—¶é—´æŠ½è±¡åŠ¨ä½œï¼Œå¹¶åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œå†…éƒ¨å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¹¶åœ¨æŸäº›ä»»åŠ¡ä¸­å–å¾—æ›´é«˜çš„å¥–åŠ±ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é•¿æ—¶é—´è§„åˆ’å’Œç¨€ç–å¥–åŠ±çš„ç¯å¢ƒä¸‹ã€‚é€šè¿‡å­¦ä¹ æ—¶é—´æŠ½è±¡åŠ¨ä½œï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ™ºèƒ½ä½“çš„å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„ç¯å¢ƒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„é¢†åŸŸï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.

