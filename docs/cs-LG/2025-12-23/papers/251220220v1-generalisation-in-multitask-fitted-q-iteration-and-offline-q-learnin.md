---
layout: default
title: Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning
---

# Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20220" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20220v1</a>
  <a href="https://arxiv.org/pdf/2512.20220.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20220v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20220v1', 'Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kausthubh Manda, Raghuram Bharadwaj Diddigi

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

**å¤‡æ³¨**: 18 pages (9 pages + Appendix and references), this is version 1

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šä»»åŠ¡ç¦»çº¿Qå­¦ä¹ æ–¹æ³•ä»¥æå‡ç»Ÿè®¡æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šä»»åŠ¡å­¦ä¹ ` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `Qå­¦ä¹ ` `è´å°”æ›¼è¯¯å·®` `å…±äº«è¡¨ç¤º` `ç»Ÿè®¡æ•ˆç‡` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å…±äº«ä½ç§©è¡¨ç¤ºæ—¶ï¼Œå¸¸å¸¸é¢ä¸´ç»Ÿè®¡æ•ˆç‡ä½å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡æ‹ŸåˆQè¿­ä»£çš„æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–è´å°”æ›¼è¯¯å·®æ¥è”åˆå­¦ä¹ å…±äº«è¡¨ç¤ºå’Œä»»åŠ¡ç‰¹å®šçš„ä»·å€¼å‡½æ•°ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè·¨ä»»åŠ¡æ•°æ®æ± åŒ–æ˜¾è‘—æé«˜äº†ä»·å€¼å‡½æ•°çš„ä¼°è®¡ç²¾åº¦ï¼Œå¹¶ä¸”åœ¨æ–°ä»»åŠ¡ä¸­é‡ç”¨è¡¨ç¤ºå¯ä»¥é™ä½å­¦ä¹ å¤æ‚æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¤šä»»åŠ¡ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šä¸ªä»»åŠ¡å…±äº«ä½ç§©åŠ¨ä½œä»·å€¼å‡½æ•°è¡¨ç¤ºçš„æƒ…å†µä¸‹ã€‚å­¦ä¹ è€…åˆ©ç”¨å›ºå®šçš„æ•°æ®é›†è¿›è¡Œå­¦ä¹ ï¼Œæ—¨åœ¨é€šè¿‡å…±äº«ç»“æ„æé«˜ç»Ÿè®¡æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ†æäº†ä¸€ç§å¤šä»»åŠ¡æ‹ŸåˆQè¿­ä»£çš„å˜ä½“ï¼Œé€šè¿‡åœ¨ç¦»çº¿æ•°æ®ä¸Šæœ€å°åŒ–è´å°”æ›¼è¯¯å·®ï¼Œè”åˆå­¦ä¹ å…±äº«è¡¨ç¤ºå’Œä»»åŠ¡ç‰¹å®šçš„ä»·å€¼å‡½æ•°ã€‚åœ¨æ ‡å‡†çš„å¯å®ç°æ€§å’Œè¦†ç›–å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬å»ºç«‹äº†å­¦ä¹ çš„ä»·å€¼å‡½æ•°çš„æœ‰é™æ ·æœ¬æ³›åŒ–ä¿è¯ï¼Œæ˜ç¡®äº†è·¨ä»»åŠ¡æ•°æ®æ± åŒ–å¦‚ä½•æ”¹å–„ä¼°è®¡ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è€ƒè™‘äº†ä¸€ä¸ªä¸‹æ¸¸ç¦»çº¿è®¾ç½®ï¼Œå…¶ä¸­æ–°ä»»åŠ¡å…±äº«ä¸ä¸Šæ¸¸ä»»åŠ¡ç›¸åŒçš„åŸºç¡€è¡¨ç¤ºï¼Œç ”ç©¶äº†åœ¨å¤šä»»åŠ¡é˜¶æ®µå­¦ä¹ çš„è¡¨ç¤ºå¦‚ä½•å½±å“æ–°ä»»åŠ¡çš„ä»·å€¼ä¼°è®¡ï¼Œæ˜¾ç¤ºå‡ºç›¸è¾ƒäºä»å¤´å­¦ä¹ å¯ä»¥é™ä½ä¸‹æ¸¸å­¦ä¹ çš„æœ‰æ•ˆå¤æ‚æ€§ã€‚æˆ‘ä»¬çš„ç»“æœé˜æ˜äº†å…±äº«è¡¨ç¤ºåœ¨å¤šä»»åŠ¡ç¦»çº¿Qå­¦ä¹ ä¸­çš„ä½œç”¨ï¼Œå¹¶æä¾›äº†ç†è®ºè§è§£ï¼Œè¯´æ˜ä½•æ—¶ä»¥åŠå¦‚ä½•åˆ©ç”¨å¤šä»»åŠ¡ç»“æ„æ”¹å–„æ— æ¨¡å‹ã€åŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šä»»åŠ¡ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å…±äº«ä½ç§©è¡¨ç¤ºæ¥æå‡ç»Ÿè®¡æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šä¸ªç›¸å…³ä»»åŠ¡æ—¶ï¼Œå¾€å¾€ç¼ºä¹æœ‰æ•ˆçš„ç»“æ„åŒ–å­¦ä¹ ç­–ç•¥ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡æ‹ŸåˆQè¿­ä»£çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨ç¦»çº¿æ•°æ®ä¸Šæœ€å°åŒ–è´å°”æ›¼è¯¯å·®ï¼Œè”åˆå­¦ä¹ å…±äº«è¡¨ç¤ºå’Œä»»åŠ¡ç‰¹å®šçš„ä»·å€¼å‡½æ•°ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå……åˆ†åˆ©ç”¨å¤šä¸ªä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œæé«˜å­¦ä¹ çš„ç»Ÿè®¡æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å…±äº«è¡¨ç¤ºå­¦ä¹ å’Œä»»åŠ¡ç‰¹å®šä»·å€¼å‡½æ•°å­¦ä¹ ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ”¶é›†å¤šä¸ªç›¸å…³ä»»åŠ¡çš„ç¦»çº¿æ•°æ®ï¼›ç„¶åï¼Œé€šè¿‡æ‹ŸåˆQè¿­ä»£ç®—æ³•å­¦ä¹ å…±äº«è¡¨ç¤ºï¼›æœ€åï¼Œé’ˆå¯¹æ¯ä¸ªä»»åŠ¡ä¼˜åŒ–å…¶ç‰¹å®šçš„ä»·å€¼å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå»ºç«‹äº†æœ‰é™æ ·æœ¬æ³›åŒ–ä¿è¯ï¼Œæ˜ç¡®äº†è·¨ä»»åŠ¡æ•°æ®æ± åŒ–å¦‚ä½•æ”¹å–„ä¼°è®¡ç²¾åº¦ã€‚è¿™ä¸€ç†è®ºæ¡†æ¶ä¸ä¼ ç»Ÿçš„å•ä»»åŠ¡å­¦ä¹ æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ä»»åŠ¡é—´çš„å…±äº«ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡‡ç”¨è´å°”æ›¼è¯¯å·®æœ€å°åŒ–ç­–ç•¥ï¼›åœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œè®¾è®¡äº†å…±äº«å±‚å’Œä»»åŠ¡ç‰¹å®šå±‚çš„ç»„åˆï¼Œä»¥ä¾¿æœ‰æ•ˆæ•æ‰ä»»åŠ¡é—´çš„å…±æ€§ä¸å·®å¼‚ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20220v1/T-Scaling.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20220v1/n-scaling.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20220v1/H_scaling.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†ä»·å€¼å‡½æ•°çš„ä¼°è®¡ç²¾åº¦ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æ ·æœ¬æ•°é‡ä¸ºnTæ—¶ï¼Œä¼°è®¡è¯¯å·®ä¾èµ–äº$1/	ext{sqrt}(nT)$ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜æ˜¾æå‡ã€‚æ­¤å¤–ï¼Œæ–°ä»»åŠ¡çš„å­¦ä¹ å¤æ‚æ€§é™ä½ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€ä¸ªæ€§åŒ–æ¨èç­‰å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯ã€‚é€šè¿‡æé«˜å¤šä»»åŠ¡å­¦ä¹ çš„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜çš„æ€§èƒ½å’Œæ›´ä½çš„æˆæœ¬ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.

