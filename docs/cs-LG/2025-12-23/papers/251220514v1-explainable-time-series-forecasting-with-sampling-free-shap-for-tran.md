---
layout: default
title: Explainable time-series forecasting with sampling-free SHAP for Transformers
---

# Explainable time-series forecasting with sampling-free SHAP for Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20514" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20514v1</a>
  <a href="https://arxiv.org/pdf/2512.20514.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20514v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20514v1', 'Explainable time-series forecasting with sampling-free SHAP for Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Matthias Hertel, Sebastian PÃ¼tz, Ralf Mikut, Veit Hagenmeyer, Benjamin SchÃ¤fer

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSHAPformerï¼Œä¸€ç§åŸºäºTransformerçš„å¿«é€Ÿã€æ— é‡‡æ ·çš„å¯è§£é‡Šæ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—é¢„æµ‹` `å¯è§£é‡ŠAI` `Transformer` `SHAPå€¼` `æ³¨æ„åŠ›æœºåˆ¶` `ç”µåŠ›è´Ÿè·é¢„æµ‹` `æ— é‡‡æ ·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•ç¼ºä¹é«˜æ•ˆçš„å¯è§£é‡Šæ€§å·¥å…·ï¼ŒSHAPç­‰æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ï¼Œä¸”å¸¸å‡è®¾ç‰¹å¾ç‹¬ç«‹æ€§ã€‚
2. SHAPformeråˆ©ç”¨Transformeræ¶æ„å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡ç‰¹å¾å­é›†è¿›è¡Œé¢„æµ‹ï¼Œæ— éœ€é‡‡æ ·å³å¯å¿«é€Ÿç”Ÿæˆè§£é‡Šã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSHAPformeråœ¨åˆæˆæ•°æ®ä¸Šè§£é‡Šå‡†ç¡®ï¼Œåœ¨ç”µåŠ›è´Ÿè·æ•°æ®ä¸Šé¢„æµ‹æ€§èƒ½ä¼˜å¼‚ï¼Œå¹¶èƒ½æä¾›æœ‰æ„ä¹‰çš„æ´è§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ—¶é—´åºåˆ—é¢„æµ‹åœ¨è®¸å¤šé¢†åŸŸå¯¹äºè§„åˆ’å’Œå†³ç­–è‡³å…³é‡è¦ã€‚å¯è§£é‡Šæ€§æ˜¯å»ºç«‹ç”¨æˆ·ä¿¡ä»»å’Œæ»¡è¶³é€æ˜åº¦è¦æ±‚çš„å…³é”®ã€‚Shapley Additive Explanations (SHAP) æ˜¯ä¸€ç§æµè¡Œçš„å¯è§£é‡ŠAIæ¡†æ¶ï¼Œä½†å®ƒç¼ºä¹é’ˆå¯¹æ—¶é—´åºåˆ—çš„æœ‰æ•ˆå®ç°ï¼Œå¹¶ä¸”åœ¨å¯¹åäº‹å®è¿›è¡Œé‡‡æ ·æ—¶é€šå¸¸å‡è®¾ç‰¹å¾ç‹¬ç«‹æ€§ã€‚æˆ‘ä»¬ä»‹ç» SHAPformerï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Transformer æ¶æ„çš„å‡†ç¡®ã€å¿«é€Ÿä¸”æ— é‡‡æ ·çš„å¯è§£é‡Šæ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ã€‚å®ƒåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ“ä½œæ¥è¿›è¡ŒåŸºäºç‰¹å¾å­é›†çš„é¢„æµ‹ã€‚SHAPformer åœ¨ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å†…ç”Ÿæˆè§£é‡Šï¼Œæ¯” SHAP Permutation Explainer å¿«å‡ ä¸ªæ•°é‡çº§ã€‚åœ¨å…·æœ‰çœŸå®è§£é‡Šçš„åˆæˆæ•°æ®ä¸Šï¼ŒSHAPformer æä¾›çš„è§£é‡Šä¸æ•°æ®ä¸€è‡´ã€‚åº”ç”¨äºçœŸå®ä¸–ç•Œçš„ç”µåŠ›è´Ÿè·æ•°æ®ï¼Œå®ƒå®ç°äº†æœ‰ç«äº‰åŠ›çš„é¢„æµ‹æ€§èƒ½ï¼Œå¹¶æä¾›äº†æœ‰æ„ä¹‰çš„å±€éƒ¨å’Œå…¨å±€è§è§£ï¼Œä¾‹å¦‚å°†è¿‡å»çš„è´Ÿè·è¯†åˆ«ä¸ºå…³é”®é¢„æµ‹å› å­ï¼Œå¹¶æ­ç¤ºäº†åœ£è¯èŠ‚æœŸé—´ç‹¬ç‰¹çš„æ¨¡å‹è¡Œä¸ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹çš„å¯è§£é‡Šæ€§é—®é¢˜ã€‚ç°æœ‰çš„SHAPæ–¹æ³•åœ¨æ—¶é—´åºåˆ—æ•°æ®ä¸Šè®¡ç®—æ•ˆç‡ä½ï¼Œå¹¶ä¸”é€šå¸¸å‡è®¾ç‰¹å¾ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œè¿™ä¸æ—¶é—´åºåˆ—æ•°æ®çš„å®é™…æƒ…å†µä¸ç¬¦ï¼Œå¯¼è‡´è§£é‡Šä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Transformeræ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡æ“çºµæ³¨æ„åŠ›æƒé‡æ¥æ¨¡æ‹Ÿç‰¹å¾å­é›†çš„å½±å“ï¼Œä»è€Œå®ç°æ— é‡‡æ ·çš„SHAPå€¼è®¡ç®—ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¼ ç»ŸSHAPæ–¹æ³•ä¸­éœ€è¦å¤§é‡é‡‡æ ·çš„è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSHAPformeråŸºäºTransformeræ¶æ„ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼šè¾“å…¥åµŒå…¥å±‚ã€Transformerç¼–ç å™¨å±‚ã€æ³¨æ„åŠ›æ“çºµå±‚å’Œé¢„æµ‹è¾“å‡ºå±‚ã€‚è¾“å…¥åµŒå…¥å±‚å°†æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸ºåµŒå…¥å‘é‡ã€‚Transformerç¼–ç å™¨å±‚å­¦ä¹ æ—¶é—´åºåˆ—çš„ç‰¹å¾è¡¨ç¤ºã€‚æ³¨æ„åŠ›æ“çºµå±‚é€šè¿‡è°ƒæ•´æ³¨æ„åŠ›æƒé‡æ¥æ¨¡æ‹Ÿç‰¹å¾å­é›†çš„å½±å“ã€‚é¢„æµ‹è¾“å‡ºå±‚åŸºäºç‰¹å¾è¡¨ç¤ºè¿›è¡Œé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šSHAPformeræœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå…¶æ— é‡‡æ ·çš„SHAPå€¼è®¡ç®—æ–¹æ³•ã€‚é€šè¿‡ç›´æ¥æ“çºµTransformerçš„æ³¨æ„åŠ›æƒé‡ï¼Œé¿å…äº†ä¼ ç»ŸSHAPæ–¹æ³•ä¸­éœ€è¦å¤§é‡é‡‡æ ·æ¥ä¼°è®¡æ¡ä»¶æœŸæœ›çš„é—®é¢˜ï¼Œä»è€Œå®ç°äº†å¿«é€Ÿä¸”å‡†ç¡®çš„å¯è§£é‡Šæ€§åˆ†æã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSHAPformerä¸éœ€è¦è¿›è¡Œæ˜‚è´µçš„é‡‡æ ·è¿‡ç¨‹ï¼Œå› æ­¤è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚

**å…³é”®è®¾è®¡**ï¼šSHAPformerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) ä½¿ç”¨æ ‡å‡†çš„Transformerç¼–ç å™¨ç»“æ„æ¥å­¦ä¹ æ—¶é—´åºåˆ—çš„ç‰¹å¾è¡¨ç¤ºï¼›(2) è®¾è®¡äº†ä¸€ç§æ³¨æ„åŠ›æ“çºµæœºåˆ¶ï¼Œé€šè¿‡å±è”½æˆ–è°ƒæ•´æ³¨æ„åŠ›æƒé‡æ¥æ¨¡æ‹Ÿç‰¹å¾å­é›†çš„å½±å“ï¼›(3) ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä½œä¸ºé¢„æµ‹æŸå¤±å‡½æ•°ï¼Œå¹¶é‡‡ç”¨Adamä¼˜åŒ–å™¨è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚å…·ä½“çš„æ³¨æ„åŠ›æ“çºµç­–ç•¥å’Œæƒé‡è°ƒæ•´æ–¹æ³•åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20514v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20514v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20514v1/x5.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

SHAPformeråœ¨åˆæˆæ•°æ®ä¸Šèƒ½å¤Ÿå‡†ç¡®åœ°æ¢å¤ground truthè§£é‡Šï¼Œè¯æ˜äº†å…¶è§£é‡Šçš„å¯é æ€§ã€‚åœ¨çœŸå®ä¸–ç•Œçš„ç”µåŠ›è´Ÿè·æ•°æ®ä¸Šï¼ŒSHAPformerå®ç°äº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“çš„é¢„æµ‹æ€§èƒ½ï¼Œå¹¶ä¸”è§£é‡Šé€Ÿåº¦æ¯”SHAP Permutation Explainerå¿«å‡ ä¸ªæ•°é‡çº§ã€‚æ­¤å¤–ï¼ŒSHAPformerè¿˜æ­ç¤ºäº†ç”µåŠ›è´Ÿè·é¢„æµ‹ä¸­çš„ä¸€äº›æœ‰è¶£ç°è±¡ï¼Œä¾‹å¦‚è¿‡å»è´Ÿè·æ˜¯å…³é”®é¢„æµ‹å› å­ï¼Œä»¥åŠåœ£è¯èŠ‚æœŸé—´æ¨¡å‹è¡Œä¸ºçš„ç‰¹æ®Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SHAPformerå¯åº”ç”¨äºå„ç§éœ€è¦å¯è§£é‡Šæ—¶é—´åºåˆ—é¢„æµ‹çš„é¢†åŸŸï¼Œä¾‹å¦‚ç”µåŠ›è´Ÿè·é¢„æµ‹ã€é‡‘èå¸‚åœºåˆ†æã€ä¾›åº”é“¾ç®¡ç†å’ŒåŒ»ç–—å¥åº·ç›‘æµ‹ã€‚é€šè¿‡æä¾›å¯¹é¢„æµ‹ç»“æœçš„è§£é‡Šï¼ŒSHAPformerå¯ä»¥å¸®åŠ©ç”¨æˆ·ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œå»ºç«‹ä¿¡ä»»ï¼Œå¹¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚æ­¤å¤–ï¼ŒSHAPformerè¿˜å¯ä»¥ç”¨äºè¯†åˆ«å…³é”®çš„å½±å“å› ç´ ï¼Œä»è€Œä¼˜åŒ–èµ„æºåˆ†é…å’Œæ”¹è¿›ä¸šåŠ¡æµç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Time-series forecasts are essential for planning and decision-making in many domains. Explainability is key to building user trust and meeting transparency requirements. Shapley Additive Explanations (SHAP) is a popular explainable AI framework, but it lacks efficient implementations for time series and often assumes feature independence when sampling counterfactuals. We introduce SHAPformer, an accurate, fast and sampling-free explainable time-series forecasting model based on the Transformer architecture. It leverages attention manipulation to make predictions based on feature subsets. SHAPformer generates explanations in under one second, several orders of magnitude faster than the SHAP Permutation Explainer. On synthetic data with ground truth explanations, SHAPformer provides explanations that are true to the data. Applied to real-world electrical load data, it achieves competitive predictive performance and delivers meaningful local and global insights, such as identifying the past load as the key predictor and revealing a distinct model behavior during the Christmas period.

