---
layout: default
title: Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering
---

# Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20115" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20115v1</a>
  <a href="https://arxiv.org/pdf/2512.20115.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20115v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20115v1', 'Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuanhao Chen, Qi Liu, Pengbin Chen, Zhongjian Qiao, Yanjie Li

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ ·æœ¬è¿‡æ»¤çš„ç­–ç•¥çº¦æŸç¦»çº¿æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡æ ·æœ¬æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥çº¦æŸ` `æ ·æœ¬è¿‡æ»¤` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `åˆ†å¸ƒåç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¦»çº¿å¼ºåŒ–å­¦ä¹ å—åˆ†å¸ƒåç§»é—®é¢˜å›°æ‰°ï¼Œç­–ç•¥çº¦æŸæ–¹æ³•è™½èƒ½ç¼“è§£ï¼Œä½†æ˜“å—æ•°æ®é›†ä¸­ä½å›æŠ¥æ ·æœ¬çš„å½±å“ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ ·æœ¬è¿‡æ»¤æ–¹æ³•ï¼Œé€šè¿‡è¯„ä¼°è½¬ç§»æ ·æœ¬çš„åˆ†æ•°ï¼Œç­›é€‰å‡ºé«˜è´¨é‡æ ·æœ¬ç”¨äºè®­ç»ƒï¼Œæå‡å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•å’ŒåŸºå‡†ä»»åŠ¡ä¸­ï¼Œå‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ—¨åœ¨åˆ©ç”¨ç»™å®šçš„é™æ€æ•°æ®é›†å­¦ä¹ ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–é¢„æœŸå›æŠ¥ã€‚ç„¶è€Œï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ é¢ä¸´åˆ†å¸ƒåç§»é—®é¢˜ã€‚ç­–ç•¥çº¦æŸç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¢«æå‡ºä»¥è§£å†³è¯¥é—®é¢˜ã€‚åœ¨ç­–ç•¥çº¦æŸç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ï¼Œç¡®ä¿å­¦ä¹ åˆ°çš„ç­–ç•¥ä¸è¡Œä¸ºç­–ç•¥ä¹‹é—´çš„å·®å¼‚åœ¨ç»™å®šé˜ˆå€¼å†…éå¸¸é‡è¦ã€‚å› æ­¤ï¼Œå­¦ä¹ åˆ°çš„ç­–ç•¥ä¸¥é‡ä¾èµ–äºè¡Œä¸ºç­–ç•¥çš„è´¨é‡ã€‚ç„¶è€Œï¼Œç°æœ‰ç­–ç•¥çº¦æŸæ–¹æ³•å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šå¦‚æœæ•°æ®é›†åŒ…å«è®¸å¤šä½å›æŠ¥çš„è½¬ç§»æ ·æœ¬ï¼Œå­¦ä¹ åˆ°çš„ç­–ç•¥å°†åŒ…å«æ¬¡ä¼˜å‚è€ƒç­–ç•¥ï¼Œå¯¼è‡´å­¦ä¹ é€Ÿåº¦æ…¢ã€æ ·æœ¬æ•ˆç‡ä½å’Œæ€§èƒ½å·®ã€‚æœ¬æ–‡è¡¨æ˜ï¼Œç­–ç•¥çº¦æŸç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ä½¿ç”¨çš„æ‰€æœ‰æ•°æ®é›†è½¬ç§»æ ·æœ¬çš„é‡‡æ ·æ–¹æ³•å¯ä»¥æ”¹è¿›ã€‚æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ ·æœ¬è¿‡æ»¤æ–¹æ³•ï¼Œä»¥æé«˜æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡æ•°æ®é›†ä¸­episodeçš„å¹³å‡å¥–åŠ±å’Œå¹³å‡æŠ˜æ‰£å¥–åŠ±æ¥è¯„ä¼°è½¬ç§»æ ·æœ¬çš„åˆ†æ•°ï¼Œå¹¶æå–é«˜åˆ†æ•°çš„è½¬ç§»æ ·æœ¬ã€‚å…¶æ¬¡ï¼Œé«˜åˆ†æ•°çš„è½¬ç§»æ ·æœ¬ç”¨äºè®­ç»ƒç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚æˆ‘ä»¬åœ¨ä¸€äº›ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•å’ŒåŸºå‡†ä»»åŠ¡ä¸­éªŒè¯äº†æ‰€æå‡ºçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç­–ç•¥çº¦æŸæ–¹æ³•æ—¨åœ¨è§£å†³åˆ†å¸ƒåç§»é—®é¢˜ï¼Œä½†å½“ç¦»çº¿æ•°æ®é›†åŒ…å«å¤§é‡ä½å›æŠ¥çš„è½¬ç§»æ ·æœ¬æ—¶ï¼Œå­¦ä¹ åˆ°çš„ç­–ç•¥ä¼šå—åˆ°æ¬¡ä¼˜è¡Œä¸ºç­–ç•¥çš„é™åˆ¶ï¼Œå¯¼è‡´å­¦ä¹ é€Ÿåº¦æ…¢ã€æ ·æœ¬æ•ˆç‡ä½ï¼Œæœ€ç»ˆæ€§èƒ½ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•æ²¡æœ‰æœ‰æ•ˆåŒºåˆ†å’Œåˆ©ç”¨é«˜è´¨é‡æ ·æœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒåœ¨äºé€šè¿‡æ ·æœ¬è¿‡æ»¤ï¼Œä¼˜å…ˆé€‰æ‹©é«˜è´¨é‡ï¼ˆé«˜å›æŠ¥ï¼‰çš„è½¬ç§»æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚è¿™æ ·å¯ä»¥å‡å°‘ä½è´¨é‡æ ·æœ¬å¯¹ç­–ç•¥å­¦ä¹ çš„è´Ÿé¢å½±å“ï¼Œä½¿å­¦ä¹ è¿‡ç¨‹æ›´å…³æ³¨æœ‰ä»·å€¼çš„ç»éªŒï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ ·æœ¬è¯„åˆ†ä¸è¿‡æ»¤ï¼šå¯¹ç¦»çº¿æ•°æ®é›†ä¸­çš„æ¯ä¸ªè½¬ç§»æ ·æœ¬è¿›è¡Œè¯„åˆ†ï¼Œè¯„åˆ†ä¾æ®æ˜¯åŒ…å«è¯¥æ ·æœ¬çš„episodeçš„å¹³å‡å¥–åŠ±å’Œå¹³å‡æŠ˜æ‰£å¥–åŠ±ã€‚ç„¶åï¼Œæ ¹æ®è®¾å®šçš„é˜ˆå€¼ï¼Œç­›é€‰å‡ºé«˜åˆ†æ•°çš„è½¬ç§»æ ·æœ¬ã€‚2) ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼šä½¿ç”¨è¿‡æ»¤åçš„é«˜è´¨é‡æ ·æœ¬ï¼Œè®­ç»ƒç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚BCQã€CQLç­‰ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ç®€å•æœ‰æ•ˆçš„æ ·æœ¬è¿‡æ»¤æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤ŸåŒºåˆ†ç¦»çº¿æ•°æ®é›†ä¸­çš„é«˜è´¨é‡å’Œä½è´¨é‡æ ·æœ¬ï¼Œå¹¶ä¼˜å…ˆåˆ©ç”¨é«˜è´¨é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚ä¸ç›´æ¥ä½¿ç”¨æ‰€æœ‰æ ·æœ¬çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®ï¼Œé¿å…äº†ä½è´¨é‡æ ·æœ¬çš„å¹²æ‰°ã€‚

**å…³é”®è®¾è®¡**ï¼šæ ·æœ¬è¯„åˆ†å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ã€‚è®ºæ–‡é‡‡ç”¨episodeçš„å¹³å‡å¥–åŠ±å’Œå¹³å‡æŠ˜æ‰£å¥–åŠ±ä½œä¸ºè¯„åˆ†æ ‡å‡†ï¼Œè¿™èƒ½å¤Ÿåæ˜ è½¬ç§»æ ·æœ¬çš„é•¿æœŸä»·å€¼ã€‚æ­¤å¤–ï¼Œè¿‡æ»¤é˜ˆå€¼çš„é€‰æ‹©ä¹Ÿä¼šå½±å“æœ€ç»ˆæ€§èƒ½ï¼Œéœ€è¦åœ¨å®é™…åº”ç”¨ä¸­è¿›è¡Œè°ƒæ•´ã€‚æ²¡æœ‰æ¶‰åŠç‰¹å®šçš„ç½‘ç»œç»“æ„æˆ–æŸå¤±å‡½æ•°ï¼Œè€Œæ˜¯å°†è¯¥æ–¹æ³•åº”ç”¨äºç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20115v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20115v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20115v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ ·æœ¬è¿‡æ»¤æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„ç­–ç•¥çº¦æŸç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå°†æ€§èƒ½æå‡è¶…è¿‡10%ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¨èç³»ç»Ÿç­‰ã€‚å°¤å…¶æ˜¯åœ¨æ•°æ®è´¨é‡å‚å·®ä¸é½çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡å­¦ä¹ æ•ˆç‡å’Œç­–ç•¥æ€§èƒ½ã€‚é€šè¿‡è¿‡æ»¤ä½è´¨é‡æ•°æ®ï¼Œå¯ä»¥é™ä½æ•°æ®æ”¶é›†æˆæœ¬ï¼Œå¹¶åŠ é€Ÿç­–ç•¥è¿­ä»£è¿‡ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected return using a given static dataset of transitions. However, offline RL faces the distribution shift problem. The policy constraint offline RL method is proposed to solve the distribution shift problem. During the policy constraint offline RL training, it is important to ensure the difference between the learned policy and behavior policy within a given threshold. Thus, the learned policy heavily relies on the quality of the behavior policy. However, a problem exists in existing policy constraint methods: if the dataset contains many low-reward transitions, the learned will be contained with a suboptimal reference policy, leading to slow learning speed, low sample efficiency, and inferior performances. This paper shows that the sampling method in policy constraint offline RL that uses all the transitions in the dataset can be improved. A simple but efficient sample filtering method is proposed to improve the sample efficiency and the final performance. First, we evaluate the score of the transitions by average reward and average discounted reward of episodes in the dataset and extract the transition samples of high scores. Second, the high-score transition samples are used to train the offline RL algorithms. We verify the proposed method in a series of offline RL algorithms and benchmark tasks. Experimental results show that the proposed method outperforms baselines.

