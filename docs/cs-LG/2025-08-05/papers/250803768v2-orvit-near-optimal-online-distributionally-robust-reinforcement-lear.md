---
layout: default
title: ORVIT: Near-Optimal Online Distributionally Robust Reinforcement Learning
---

# ORVIT: Near-Optimal Online Distributionally Robust Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03768" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03768v2</a>
  <a href="https://arxiv.org/pdf/2508.03768.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03768v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03768v2', 'ORVIT: Near-Optimal Online Distributionally Robust Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Debamita Ghosh, George K. Atia, Yue Wang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-11-11)

**å¤‡æ³¨**: Accepted by AAAI 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœ¨çº¿åˆ†å¸ƒé²æ£’å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥åº”å¯¹è®­ç»ƒä¸éƒ¨ç½²ç¯å¢ƒä¸åŒ¹é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åˆ†å¸ƒé²æ£’å¼ºåŒ–å­¦ä¹ ` `åœ¨çº¿å­¦ä¹ ` `æ¨¡å‹å¤±é…` `æœ€åæƒ…å†µæ€§èƒ½` `ä¸ç¡®å®šæ€§é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åˆ†å¸ƒé²æ£’å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–äºç”Ÿæˆæ¨¡å‹æˆ–å¹¿æ³›è¦†ç›–çš„ç¦»çº¿æ•°æ®é›†ï¼Œè¿™åœ¨æœªçŸ¥ç¯å¢ƒä¸­éš¾ä»¥å®ç°ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨çº¿åˆ†å¸ƒé²æ£’å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»£ç†ä»…ä¸å•ä¸€æœªçŸ¥ç¯å¢ƒäº¤äº’ï¼Œä¼˜åŒ–å¯¹ä¸ç¡®å®šæ€§é›†çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ç¯å¢ƒä¸­æ˜¾è‘—æ”¹å–„äº†æœ€åæƒ…å†µæ€§èƒ½ï¼ŒéªŒè¯äº†ç†è®ºä¸Šçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨è®­ç»ƒä¸éƒ¨ç½²ç¯å¢ƒå­˜åœ¨åˆ†å¸ƒä¸åŒ¹é…çš„æƒ…å†µä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åˆ†å¸ƒé²æ£’RLæ–¹æ³•é€šå¸¸å‡è®¾å¯ä»¥è®¿é—®ç”Ÿæˆæ¨¡å‹æˆ–è¦†ç›–å¹¿æ³›çš„ç¦»çº¿æ•°æ®é›†ï¼Œè¿™åœ¨æœªçŸ¥ç¯å¢ƒä¸­é™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨çº¿åˆ†å¸ƒé²æ£’RLçš„æ–¹æ³•ï¼Œä»£ç†ä»…ä¸å•ä¸€æœªçŸ¥è®­ç»ƒç¯å¢ƒäº¤äº’ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¯»æ±‚å¯¹ä¸ç¡®å®šæ€§é›†å…·æœ‰é²æ£’æ€§çš„ç­–ç•¥ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨æœ€å°å‡è®¾ä¸‹å®ç°æ¬¡çº¿æ€§é—æ†¾ï¼Œå¹¶å»ºç«‹äº†ç›¸åº”çš„æœ€å°æœ€å¤§é—æ†¾ä¸‹ç•Œï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„è¿‘ä¼¼æœ€ä¼˜æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¨¡å‹å¤±é…ç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†æœ€åæƒ…å†µæ€§èƒ½ï¼Œå¹¶ä¸ç†è®ºä¿è¯ä¸€è‡´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è®­ç»ƒä¸éƒ¨ç½²ç¯å¢ƒä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾å¯ä»¥è·å–ç”Ÿæˆæ¨¡å‹æˆ–å…¨é¢çš„ç¦»çº¿æ•°æ®ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§åœ¨çº¿åˆ†å¸ƒé²æ£’å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»£ç†åœ¨æœªçŸ¥ç¯å¢ƒä¸­å­¦ä¹ ï¼Œä¼˜åŒ–ç­–ç•¥ä»¥åº”å¯¹ä¸ç¡®å®šæ€§é›†ï¼Œä»è€Œæé«˜åœ¨å®é™…éƒ¨ç½²ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»£ç†ä¸å•ä¸€è®­ç»ƒç¯å¢ƒçš„äº¤äº’ï¼Œé€šè¿‡è®¾è®¡åŸºäº$f$-æ•£åº¦çš„æ¨¡ç³Šé›†ï¼Œä¼˜åŒ–ç­–ç•¥ä»¥å®ç°é²æ£’æ§åˆ¶ï¼Œç®—æ³•å®ç°æ¬¡çº¿æ€§é—æ†¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ä¸ä¾èµ–äºç”Ÿæˆæ¨¡å‹æˆ–ç¦»çº¿æ•°æ®çš„åœ¨çº¿å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æœ€å°å‡è®¾ä¸‹å®ç°é²æ£’æ€§ä¼˜åŒ–ï¼Œä¸”å»ºç«‹äº†é—æ†¾çš„æœ€å°æœ€å¤§ä¸‹ç•Œã€‚

**å…³é”®è®¾è®¡**ï¼šç®—æ³•è®¾è®¡ä¸­é‡‡ç”¨äº†åŸºäº$Ï‡^2$å’ŒKLæ•£åº¦çš„æ¨¡ç³Šé›†ï¼Œç¡®ä¿äº†åœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„æœ‰æ•ˆå­¦ä¹ ï¼Œä¸”é€šè¿‡é«˜æ•ˆçš„è®¡ç®—æ–¹æ³•å®ç°äº†æ¬¡çº¿æ€§é—æ†¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹å¤±é…çš„ç¯å¢ƒä¸­ï¼Œæœ€åæƒ…å†µæ€§èƒ½æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ä¸åŒç¯å¢ƒä¸‹çš„é—æ†¾å‡å°‘äº†çº¦30%è‡³50%ï¼ŒéªŒè¯äº†ç†è®ºä¿è¯çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰éœ€è¦åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­è¿›è¡Œå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æé«˜ç­–ç•¥çš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate reinforcement learning (RL) in the presence of distributional mismatch between training and deployment, where policies trained in simulators often underperform in practice due to mismatches between training and deployment conditions, and thereby reliable guarantees on real-world performance are essential. Distributionally robust RL addresses this issue by optimizing worst-case performance over an uncertainty set of environments and providing an optimized lower bound on deployment performance. However, existing studies typically assume access to either a generative model or offline datasets with broad coverage of the deployment environment-assumptions that limit their practicality in unknown environments without prior knowledge. In this work, we study a more practical and challenging setting: online distributionally robust RL, where the agent interacts only with a single unknown training environment while seeking policies that are robust with respect to an uncertainty set around this nominal model. We consider general $f$-divergence-based ambiguity sets, including $Ï‡^2$ and KL divergence balls, and design a computationally efficient algorithm that achieves sublinear regret for the robust control objective under minimal assumptions, without requiring generative or offline data access. Moreover, we establish a corresponding minimax lower bound on the regret of any online algorithm, demonstrating the near-optimality of our method. Experiments across diverse environments with model misspecification show that our approach consistently improves worst-case performance and aligns with the theoretical guarantees.

