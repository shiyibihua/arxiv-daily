---
layout: default
title: VRPRM: Process Reward Modeling via Visual Reasoning
---

# VRPRM: Process Reward Modeling via Visual Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03556" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03556v2</a>
  <a href="https://arxiv.org/pdf/2508.03556.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03556v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03556v2', 'VRPRM: Process Reward Modeling via Visual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinquan Chen, Bangwei Liu, Xuhong Wang, Yingchun Wang, Chaochao Lu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-08-28)

**å¤‡æ³¨**: 13 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVRPRMä»¥è§£å†³PRMåœ¨é•¿è¿œæ¨ç†ä¸­çš„ä¸è¶³**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¿‡ç¨‹å¥–åŠ±æ¨¡å‹` `è§†è§‰æ¨ç†` `é•¿è¿œæ¨ç†` `é“¾å¼æ€ç»´` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨é•¿è¿œæ¨ç†å’Œæ·±åº¦æ€è€ƒèƒ½åŠ›ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºVRPRMï¼Œé€šè¿‡è§†è§‰æ¨ç†æ¥å¢å¼ºPRMçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†é«˜æ•ˆçš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥é™ä½æ•°æ®æ ‡æ³¨æˆæœ¬ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVRPRMåœ¨ä½¿ç”¨è¾ƒå°‘æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…¶åœ¨æ•°æ®åˆ©ç”¨æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åè®­ç»ƒä¸­å¹¿æ³›åº”ç”¨ï¼Œèƒ½å¤Ÿå¯¹ç”Ÿæˆå†…å®¹çš„æ¨ç†æ­¥éª¤è¿›è¡Œç»†è‡´è¯„ä¼°ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°PRMç¼ºä¹é•¿è¿œæ¨ç†å’Œæ·±åº¦æ€è€ƒèƒ½åŠ›ã€‚è™½ç„¶å·²æœ‰å°‘æ•°ç ”ç©¶å°è¯•å°†é“¾å¼æ€ç»´èƒ½åŠ›å¼•å…¥PRMï¼Œä½†CoT-PRMæ•°æ®çš„æ ‡æ³¨æˆæœ¬è¿‡é«˜ï¼Œéš¾ä»¥åœ¨å„ç§ä»»åŠ¡ä¸­å‘æŒ¥ç¨³å®šä½œç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†VRPRMï¼Œé€šè¿‡è§†è§‰æ¨ç†è¿›è¡Œè¿‡ç¨‹å¥–åŠ±å»ºæ¨¡ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§é«˜æ•ˆçš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ä»…3.6Kçš„CoT-PRM SFTæ•°æ®å’Œ50Kçš„éCoT PRM RLè®­ç»ƒæ•°æ®ï¼ŒVRPRMçš„æ€§èƒ½è¶…è¶Šäº†æ•°æ®é‡è¾¾åˆ°400Kçš„éæ€è€ƒPRMï¼Œåœ¨BoNå®éªŒä¸­ç›¸è¾ƒåŸºç¡€æ¨¡å‹å®ç°äº†é«˜è¾¾118%çš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨é•¿è¿œæ¨ç†å’Œæ·±åº¦æ€è€ƒèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸”æ•°æ®æ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„VRPRMé€šè¿‡å¼•å…¥è§†è§‰æ¨ç†çš„æœºåˆ¶ï¼Œå¢å¼ºäº†PRMçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡è®¾è®¡é«˜æ•ˆçš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé™ä½äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæå‡äº†æ¨¡å‹çš„æ¨ç†è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVRPRMçš„æ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºä½¿ç”¨å°‘é‡çš„CoT-PRM SFTæ•°æ®è¿›è¡Œåˆæ­¥è®­ç»ƒï¼Œç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨å¤§é‡çš„éCoT PRM RLæ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šVRPRMçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆè§†è§‰æ¨ç†ä¸è¿‡ç¨‹å¥–åŠ±å»ºæ¨¡ï¼Œçªç ´äº†ä¼ ç»ŸPRMåœ¨æ¨ç†æ·±åº¦å’Œé•¿è¿œæ€è€ƒä¸Šçš„å±€é™ã€‚è¿™ç§ç»“åˆä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡æ¨ç†è´¨é‡ä¸æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”è§†è§‰ä¿¡æ¯çš„å¤„ç†éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVRPRMåœ¨ä»…ä½¿ç”¨3.6Kçš„CoT-PRM SFTæ•°æ®å’Œ50Kçš„éCoT PRM RLè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¶…è¶Šäº†æ•°æ®é‡è¾¾åˆ°400Kçš„éæ€è€ƒPRMã€‚åœ¨BoNå®éªŒä¸­ï¼ŒVRPRMå®ç°äº†é«˜è¾¾118%çš„ç›¸å¯¹æ€§èƒ½æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ•°æ®åˆ©ç”¨æ•ˆç‡å’Œæ¨ç†èƒ½åŠ›ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œæœºå™¨äººå†³ç­–ç­‰ã€‚é€šè¿‡æå‡è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒVRPRMèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­æä¾›æ›´é«˜è´¨é‡çš„è¾“å‡ºï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Process Reward Model (PRM) is widely used in the post-training of Large Language Model (LLM) because it can perform fine-grained evaluation of the reasoning steps of generated content. However, most PRMs lack long-term reasoning and deep thinking capabilities. On the other hand, although a few works have tried to introduce Chain-of-Thought capability into PRMs, the annotation cost of CoT-PRM data is too expensive to play a stable role in various tasks. To address the above challenges, we propose VRPRM, a process reward model via visual reasoning, and design an efficient two-stage training strategy. Experimental results show that using only 3.6K CoT-PRM SFT data and 50K non-CoT PRM RL training data, VRPRM can surpass the non-thinking PRM with a total data volume of 400K and achieved a relative performance improvement of up to 118\% over the base model in the BoN experiment. This result confirms that the proposed combined training strategy can achieve higher quality reasoning capabilities at a lower data annotation cost, thus providing a new paradigm for PRM training with more efficient data utilization.

