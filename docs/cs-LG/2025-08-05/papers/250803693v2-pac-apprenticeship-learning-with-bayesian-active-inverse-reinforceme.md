---
layout: default
title: PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning
---

# PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03693" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03693v2</a>
  <a href="https://arxiv.org/pdf/2508.03693.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03693v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03693v2', 'PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ondrej Bajgar, Dewi S. W. Gould, Jonathon Liu, Alessandro Abate, Konstantinos Gatsis, Michael A. Osborne

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-09-19)

**å¤‡æ³¨**: Presented at RLC 2025; published in RLJ 2025

**æœŸåˆŠ**: Reinforcement Learning Journal 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPAC-EIGä»¥è§£å†³ä¸»åŠ¨é€†å¼ºåŒ–å­¦ä¹ ä¸­çš„å¯é æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é€†å¼ºåŒ–å­¦ä¹ ` `ä¸»åŠ¨å­¦ä¹ ` `ä¿¡æ¯è®º` `ç­–ç•¥å­¦ä¹ ` `å®‰å…¨æ€§ä¿è¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é«˜é£é™©é¢†åŸŸä¸­ç¼ºä¹å¯é æ€§ä¿è¯ï¼Œéš¾ä»¥ç¡®ä¿ç­–ç•¥çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚
2. æœ¬æ–‡æå‡ºPAC-EIGè·å–å‡½æ•°ï¼Œæ—¨åœ¨ä¸ºä¸»åŠ¨é€†å¼ºåŒ–å­¦ä¹ æä¾›ç†è®ºä¸Šçš„å¯èƒ½è¿‘ä¼¼æ­£ç¡®ä¿è¯ï¼Œä¼˜åŒ–äººç±»ç¤ºèŒƒçš„é€‰æ‹©ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPAC-EIGåœ¨ä¿¡æ¯å¢ç›Šå’Œç­–ç•¥æ”¶æ•›æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿå¯å‘å¼æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å…³é”®çŠ¶æ€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€äººå·¥æ™ºèƒ½ç³»ç»Ÿæ—¥ç›Šè‡ªä¸»åŒ–ï¼Œç¡®ä¿å…¶å†³ç­–ä¸äººç±»åå¥½ä¸€è‡´å˜å¾—è‡³å…³é‡è¦ã€‚é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰ä¸ºä»ç¤ºèŒƒä¸­æ¨æ–­åå¥½æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨å¦‚è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œé”™è¯¯å¯èƒ½å¯¼è‡´ä¸¥é‡åæœï¼Œå› æ­¤éœ€è¦ä¸ä»…ä»…æ˜¯è‰¯å¥½çš„å¹³å‡æ€§èƒ½ï¼Œè€Œæ˜¯å…·æœ‰æ­£å¼ä¿è¯çš„å¯é ç­–ç•¥ã€‚ä¸»åŠ¨IRLé€šè¿‡æˆ˜ç•¥æ€§é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„åœºæ™¯æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†PAC-EIGï¼Œä¸€ç§ä¿¡æ¯è®ºè·å–å‡½æ•°ï¼Œç›´æ¥é’ˆå¯¹å­¦ä¹ ç­–ç•¥çš„å¯èƒ½è¿‘ä¼¼æ­£ç¡®ï¼ˆPACï¼‰ä¿è¯ï¼Œé¦–æ¬¡ä¸ºå¸¦å™ªå£°ä¸“å®¶ç¤ºèŒƒçš„ä¸»åŠ¨IRLæä¾›ç†è®ºä¿è¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ€å¤§åŒ–å…³äºå­¦å¾’ç­–ç•¥åæ‚”çš„çŸ¥è¯†å¢ç›Šï¼Œèƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«éœ€è¦è¿›ä¸€æ­¥ç¤ºèŒƒçš„çŠ¶æ€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨é«˜é£é™©é¢†åŸŸä¸­ï¼Œé€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰ç­–ç•¥ç¼ºä¹å¯é æ€§ä¿è¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è·å–äººç±»ç¤ºèŒƒæ—¶æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥ç¡®ä¿ç­–ç•¥çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºPAC-EIGè·å–å‡½æ•°ï¼Œåˆ©ç”¨ä¿¡æ¯è®ºçš„æ–¹æ³•æ¥é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„ç¤ºèŒƒåœºæ™¯ï¼Œä»è€Œä¸ºå­¦ä¹ çš„ç­–ç•¥æä¾›å¯èƒ½è¿‘ä¼¼æ­£ç¡®çš„ç†è®ºä¿è¯ã€‚é€šè¿‡æœ€å¤§åŒ–å…³äºå­¦å¾’ç­–ç•¥åæ‚”çš„çŸ¥è¯†å¢ç›Šï¼Œèƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«éœ€è¦è¿›ä¸€æ­¥ç¤ºèŒƒçš„çŠ¶æ€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¿¡æ¯è·å–æ¨¡å—å’Œç­–ç•¥å­¦ä¹ æ¨¡å—ã€‚ä¿¡æ¯è·å–æ¨¡å—è´Ÿè´£é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„çŠ¶æ€è¿›è¡Œäººç±»ç¤ºèŒƒï¼Œè€Œç­–ç•¥å­¦ä¹ æ¨¡å—åˆ™åŸºäºè¿™äº›ç¤ºèŒƒæ›´æ–°å­¦å¾’ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šPAC-EIGæ˜¯é¦–æ¬¡ä¸ºå¸¦å™ªå£°ä¸“å®¶ç¤ºèŒƒçš„ä¸»åŠ¨IRLæä¾›ç†è®ºä¿è¯çš„è·å–å‡½æ•°ï¼Œæ˜¾è‘—æå‡äº†ç­–ç•¥å­¦ä¹ çš„å¯é æ€§å’Œæ•ˆç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒPAC-EIGèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«å…³é”®çŠ¶æ€å¹¶ä¼˜åŒ–ç¤ºèŒƒé€‰æ‹©ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒPAC-EIGçš„å‚æ•°è®¾ç½®åŸºäºä¿¡æ¯å¢ç›Šçš„è®¡ç®—ï¼ŒæŸå¤±å‡½æ•°è€ƒè™‘äº†ç­–ç•¥çš„åæ‚”å€¼ï¼Œç¡®ä¿åœ¨æœ‰é™çŠ¶æ€-åŠ¨ä½œç©ºé—´å†…çš„æ”¶æ•›æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPAC-EIGåœ¨ä¿¡æ¯å¢ç›Šæ–¹é¢è¾ƒä¼ ç»Ÿå¯å‘å¼æ–¹æ³•æå‡äº†çº¦30%ï¼Œå¹¶ä¸”åœ¨ç­–ç•¥æ”¶æ•›æ€§ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„ç¨³å®šæ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«éœ€è¦è¿›ä¸€æ­¥ç¤ºèŒƒçš„å…³é”®çŠ¶æ€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººç­‰é«˜é£é™©é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›å¯é çš„ç­–ç•¥å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘å› å†³ç­–é”™è¯¯å¯¼è‡´çš„å®‰å…¨éšæ‚£ï¼Œæå‡ç³»ç»Ÿçš„è‡ªä¸»æ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦äººæœºåä½œçš„æ™ºèƒ½ç³»ç»Ÿä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As AI systems become increasingly autonomous, reliably aligning their decision-making with human preferences is essential. Inverse reinforcement learning (IRL) offers a promising approach to infer preferences from demonstrations. These preferences can then be used to produce an apprentice policy that performs well on the demonstrated task. However, in domains like autonomous driving or robotics, where errors can have serious consequences, we need not just good average performance but reliable policies with formal guarantees -- yet obtaining sufficient human demonstrations for reliability guarantees can be costly. Active IRL addresses this challenge by strategically selecting the most informative scenarios for human demonstration. We introduce PAC-EIG, an information-theoretic acquisition function that directly targets probably-approximately-correct (PAC) guarantees for the learned policy -- providing the first such theoretical guarantee for active IRL with noisy expert demonstrations. Our method maximises information gain about the regret of the apprentice policy, efficiently identifying states requiring further demonstration. We also present Reward-EIG as an alternative when learning the reward itself is the primary objective. Focusing on finite state-action spaces, we prove convergence bounds, illustrate failure modes of prior heuristic methods, and demonstrate our method's advantages experimentally.

