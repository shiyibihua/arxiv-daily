---
layout: default
title: Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies
---

# Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03194" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.03194v1</a>
  <a href="https://arxiv.org/pdf/2508.03194.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03194v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03194v1', 'Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yi Ma, Hongyao Tang, Chenjun Xiao, Yaodong Yang, Wei Wei, Jianye Hao, Jiye Liang

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-05

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Êï∞ÊçÆ„ÄÅÁΩëÁªú‰∏éËÆ≠ÁªÉÈ¢ÑÁÆóÁ≠ñÁï•‰ª•ÊèêÂçáÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÂÜ≥Á≠ñËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†` `Êâ©Â±ïÊ≥ïÂàô` `Êï∞ÊçÆÊïàÁéá` `ÁΩëÁªúÊû∂ÊûÑ` `ËÆ≠ÁªÉÈ¢ÑÁÆó` `Ëá™‰∏ªÂÜ≥Á≠ñ` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Ëá™Âä®È©æÈ©∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºàDRLÔºâÂú®ÂÜ≥Á≠ñÂà∂ÂÆö‰∏≠Â∞öÊú™ÂÖÖÂàÜÂà©Áî®Êâ©Â±ïÊ≥ïÂàôÔºåÂØºËá¥ÊÄßËÉΩÊèêÂçáÊΩúÂäõÊú™Ë¢´ÊåñÊéò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄöËøá‰ºòÂåñÊï∞ÊçÆÊïàÁéá„ÄÅÂ¢ûÂº∫ÁΩëÁªúÊû∂ÊûÑÂíåÊèêÂçáËÆ≠ÁªÉÈ¢ÑÁÆóÊù•ÂÆûÁé∞DRLÁöÑÊâ©Â±ïÔºåÁ≥ªÁªüÂàÜÊûê‰∫Ü‰∏âÁßçÁ≠ñÁï•ÁöÑÂçèÂêå‰ΩúÁî®„ÄÇ
3. Á†îÁ©∂Ë°®ÊòéÔºåÈááÁî®ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÂíåÈ´òÈáçÊîæÊØîÁ≠âÁ≠ñÁï•ÂèØ‰ª•ÊòæËëóÊèêÈ´òËÆ≠ÁªÉÊïàÁéáÂíåÊî∂ÊïõÈÄüÂ∫¶ÔºåÊé®Âä®DRLÂú®Â§ö‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÂíåËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊâ©Â±ïÊé®Âä®‰∫ÜÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊòæËëóËøõÂ±ïÔºåÂ∞§ÂÖ∂ÊòØÂú®ËÆ°ÁÆóÊú∫ËßÜËßâÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüü„ÄÇËøô‰∏ÄËøõÂ±ïÂü∫‰∫éÊâ©Â±ïÊ≥ïÂàôÔºåË°®ÊòéÊâ©Â±ïÊ®°ÂûãÂèÇÊï∞ÂíåËÆ≠ÁªÉÊï∞ÊçÆÂèØ‰ª•ÊèêÂçáÂ≠¶‰π†ÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÊâ©Â±ïÊ≥ïÂàôÂú®Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºàDRLÔºâ‰∏≠ÁöÑÂ∫îÁî®‰ªçÁõ∏ÂØπÊú™Ë¢´Êé¢Á¥¢„ÄÇÊú¨ÊñáÁ≥ªÁªüÂàÜÊûê‰∫ÜÊï∞ÊçÆ„ÄÅÁΩëÁªúÂíåËÆ≠ÁªÉÈ¢ÑÁÆó‰∏â‰∏™Áª¥Â∫¶ÁöÑÊâ©Â±ïÁ≠ñÁï•ÔºåÊé¢ËÆ®‰∫ÜÊï∞ÊçÆÊïàÁéá‰ºòÂåñ„ÄÅÁΩëÁªúÊû∂ÊûÑÂ¢ûÂº∫ÂèäËÆ≠ÁªÉÊïàÁéáÊèêÂçáÁöÑÂÖ≥Á≥ªÔºåÂº∫Ë∞É‰∫ÜÂú®ÂÜ≥Á≠ñÂà∂ÂÆö‰∏≠Âπ≥Ë°°ÂèØÊâ©Â±ïÊÄß‰∏éËÆ°ÁÆóÊïàÁéáÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂‰∏∫Êú™Êù•Á†îÁ©∂Êèê‰æõ‰∫ÜÊñπÂêë„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†‰∏≠Êâ©Â±ïÊ≥ïÂàôÂ∫îÁî®‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Êï∞ÊçÆÂà©Áî®ÂíåËÆ≠ÁªÉÊïàÁéá‰∏äÂ≠òÂú®ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÁ≥ªÁªüÂàÜÊûêÊï∞ÊçÆ„ÄÅÁΩëÁªúÂíåËÆ≠ÁªÉÈ¢ÑÁÆóÁöÑÊâ©Â±ïÁ≠ñÁï•ÔºåÊèêÂá∫‰ºòÂåñÊñπÊ°à‰ª•ÊèêÂçáDRLÁöÑÂÜ≥Á≠ñËÉΩÂäõÔºåÂº∫Ë∞É‰∏âËÄÖÁöÑÂçèÂêå‰ΩúÁî®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÊâ©Â±ï„ÄÅÁΩëÁªúÊâ©Â±ïÂíåËÆ≠ÁªÉÈ¢ÑÁÆóÊâ©Â±ï‰∏â‰∏™Ê®°ÂùóÔºåÂàÜÂà´ÈíàÂØπÊï∞ÊçÆÊïàÁéá„ÄÅÊ®°ÂûãË°®ËææËÉΩÂäõÂíåËÆ≠ÁªÉÊïàÁéáËøõË°å‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊèêÂá∫‰∫ÜÁªìÂêàÂπ∂Ë°åÈááÊ†∑„ÄÅÊï∞ÊçÆÁîüÊàê„ÄÅÁΩëÁªúÊû∂ÊûÑÂ¢ûÂº∫ÔºàÂ¶ÇÂçï‰ΩìÊâ©Â±ï„ÄÅÈõÜÊàêÂíåMoEÊñπÊ≥ïÔºâÂèäÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÁöÑÁªºÂêàÁ≠ñÁï•ÔºåÊòæËëóÊèêÂçá‰∫ÜDRLÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Êï∞ÊçÆÊâ©Â±ï‰∏≠ÔºåÈááÁî®È´òÈáçÊîæÊØîÂíåÂ§ßÊâπÈáèËÆ≠ÁªÉÔºõÁΩëÁªúÊâ©Â±ï‰∏≠ÔºåÊé¢Á¥¢‰∫ÜÂ§öÁßçÊû∂ÊûÑËÆæËÆ°ÔºõËÆ≠ÁªÉÈ¢ÑÁÆóÊâ©Â±ïÂàôÂÖ≥Ê≥®‰∫éÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÁöÑÊïàÁéáÂíåÊî∂ÊïõÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈááÁî®Êú¨ÊñáÊèêÂá∫ÁöÑÊâ©Â±ïÁ≠ñÁï•ÂêéÔºåDRLÂú®Â§ö‰∏™‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÊèêÂçáÊòæËëó„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∫õÂÜ≥Á≠ñ‰ªªÂä°‰∏≠ÔºåÊ®°ÂûãÁöÑÊî∂ÊïõÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü30%ÔºåÂπ∂‰∏îÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÊâ©Â±ïÊ≥ïÂàôÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂ÂíåÂ§ßËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉÁ≠â„ÄÇÈÄöËøá‰ºòÂåñDRLÁöÑÂÜ≥Á≠ñËÉΩÂäõÔºåÂèØ‰ª•Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥È´òÊïàÁöÑËá™‰∏ªÂÜ≥Á≠ñÔºåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇÊú™Êù•ÔºåÈöèÁùÄÊäÄÊúØÁöÑËøõÊ≠•ÔºåÂèØËÉΩ‰ºöÂú®Êõ¥Â§öÂÆûÈôÖÂ∫îÁî®‰∏≠Â±ïÁé∞Âá∫ÊòæËëó‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In recent years, the expansion of neural network models and training data has driven remarkable progress in deep learning, particularly in computer vision and natural language processing. This advancement is underpinned by the concept of Scaling Laws, which demonstrates that scaling model parameters and training data enhances learning performance. While these fields have witnessed breakthroughs, such as the development of large language models like GPT-4 and advanced vision models like Midjourney, the application of scaling laws in deep reinforcement learning (DRL) remains relatively unexplored. Despite its potential to improve performance, the integration of scaling laws into DRL for decision making has not been fully realized. This review addresses this gap by systematically analyzing scaling strategies in three dimensions: data, network, and training budget. In data scaling, we explore methods to optimize data efficiency through parallel sampling and data generation, examining the relationship between data volume and learning outcomes. For network scaling, we investigate architectural enhancements, including monolithic expansions, ensemble and MoE methods, and agent number scaling techniques, which collectively enhance model expressivity while posing unique computational challenges. Lastly, in training budget scaling, we evaluate the impact of distributed training, high replay ratios, large batch sizes, and auxiliary training on training efficiency and convergence. By synthesizing these strategies, this review not only highlights their synergistic roles in advancing DRL for decision making but also provides a roadmap for future research. We emphasize the importance of balancing scalability with computational efficiency and outline promising directions for leveraging scaling to unlock the full potential of DRL in various tasks such as robot control, autonomous driving and LLM training.

