---
layout: default
title: Cross-Model Semantics in Representation Learning
---

# Cross-Model Semantics in Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03649" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03649v1</a>
  <a href="https://arxiv.org/pdf/2508.03649.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03649v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03649v1', 'Cross-Model Semantics in Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Saleh Nikooroo, Thomas Engel

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“æ„çº¦æŸä»¥æå‡æ·±åº¦ç½‘ç»œè¡¨ç¤ºçš„è·¨æ¨¡å‹å…¼å®¹æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¡¨ç¤ºå­¦ä¹ ` `ç»“æ„çº¦æŸ` `æ¨¡å‹è’¸é¦` `ç‰¹å¾äº’æ“ä½œæ€§` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºå¯¹æ¶æ„é€‰æ‹©æ•æ„Ÿï¼Œå¯¼è‡´ä¸åŒæ¨¡å‹é—´çš„è¡¨ç¤ºä¸ç¨³å®šå’Œéš¾ä»¥è¿ç§»ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¼•å…¥ç»“æ„çº¦æŸï¼Œå¦‚çº¿æ€§å¡‘å½¢ç®—å­å’Œçº æ­£è·¯å¾„ï¼Œæ¥æå‡ä¸åŒæ¶æ„é—´çš„è¡¨ç¤ºå…¼å®¹æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç»“æ„è§„å¾‹æ€§èƒ½å¤Ÿåœ¨æ¶æ„å˜åŒ–ä¸‹ä¿æŒæ›´ç¨³å®šçš„è¡¨ç¤ºå‡ ä½•ï¼Œæå‡äº†æ¨¡å‹é—´ç‰¹å¾çš„äº’æ“ä½œæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ç½‘ç»œå­¦ä¹ çš„å†…éƒ¨è¡¨ç¤ºå¾€å¾€å¯¹æ¶æ„ç‰¹å®šé€‰æ‹©æ•æ„Ÿï¼Œè¿™å¼•å‘äº†å¯¹ä¸åŒæ¨¡å‹é—´å­¦ä¹ ç»“æ„çš„ç¨³å®šæ€§ã€å¯¹é½æ€§å’Œå¯è¿ç§»æ€§çš„è´¨ç–‘ã€‚æœ¬æ–‡ç ”ç©¶äº†çº¿æ€§å¡‘å½¢ç®—å­å’Œçº æ­£è·¯å¾„ç­‰ç»“æ„çº¦æŸå¦‚ä½•å½±å“ä¸åŒæ¶æ„é—´å†…éƒ¨è¡¨ç¤ºçš„å…¼å®¹æ€§ã€‚åŸºäºå…ˆå‰å…³äºç»“æ„å˜æ¢å’Œæ”¶æ•›çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶æ¥æµ‹é‡å’Œåˆ†æå…·æœ‰ä¸åŒä½†ç›¸å…³æ¶æ„å…ˆéªŒçš„ç½‘ç»œé—´çš„è¡¨ç¤ºå¯¹é½ã€‚é€šè¿‡ç†è®ºæ´å¯Ÿã€å®è¯æ¢æµ‹å’Œæ§åˆ¶è½¬ç§»å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ç»“æ„è§„å¾‹æ€§åœ¨æ¶æ„å˜åŒ–ä¸‹èƒ½è¯±å¯¼æ›´ç¨³å®šçš„è¡¨ç¤ºå‡ ä½•ã€‚è¿™è¡¨æ˜æŸäº›å½¢å¼çš„å½’çº³åå·®ä¸ä»…æ”¯æŒæ¨¡å‹å†…éƒ¨çš„æ³›åŒ–ï¼Œè¿˜èƒ½æ”¹å–„æ¨¡å‹é—´å­¦ä¹ ç‰¹å¾çš„äº’æ“ä½œæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†è¡¨ç¤ºå¯è¿ç§»æ€§å¯¹æ¨¡å‹è’¸é¦ã€æ¨¡å—åŒ–å­¦ä¹ å’Œç¨³å¥å­¦ä¹ ç³»ç»Ÿè®¾è®¡çš„å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦ç½‘ç»œå†…éƒ¨è¡¨ç¤ºå¯¹æ¶æ„ç‰¹å®šé€‰æ‹©æ•æ„Ÿçš„é—®é¢˜ï¼Œå¯¼è‡´ä¸åŒæ¨¡å‹é—´çš„è¡¨ç¤ºä¸ç¨³å®šå’Œè¿ç§»å›°éš¾ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†è¿™ç§æ¶æ„é—´çš„å…¼å®¹æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ç»“æ„çº¦æŸï¼Œé€šè¿‡çº¿æ€§å¡‘å½¢ç®—å­å’Œçº æ­£è·¯å¾„æ¥å¢å¼ºä¸åŒæ¶æ„é—´çš„è¡¨ç¤ºå¯¹é½æ€§ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨é€šè¿‡ç»“æ„è§„å¾‹æ€§æ¥æé«˜è¡¨ç¤ºçš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç†è®ºåˆ†æã€å®è¯æ¢æµ‹å’Œæ§åˆ¶è½¬ç§»å®éªŒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡ç†è®ºåˆ†æç¡®å®šç»“æ„çº¦æŸçš„å½±å“ï¼›å…¶æ¬¡ï¼Œè¿›è¡Œå®è¯æ¢æµ‹ä»¥éªŒè¯ç†è®ºï¼›æœ€åï¼Œé€šè¿‡æ§åˆ¶è½¬ç§»å®éªŒè¯„ä¼°ä¸åŒæ¶æ„é—´çš„è¡¨ç¤ºå…¼å®¹æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶æ¥æµ‹é‡å’Œåˆ†æä¸åŒæ¶æ„é—´çš„è¡¨ç¤ºå¯¹é½æ€§ï¼Œå¼ºè°ƒäº†ç»“æ„è§„å¾‹æ€§åœ¨è¡¨ç¤ºå‡ ä½•ç¨³å®šæ€§ä¸­çš„ä½œç”¨ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…³æ³¨ç»“æ„çº¦æŸçš„å¼•å…¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œè®ºæ–‡é‡‡ç”¨äº†ç‰¹å®šçš„çº¿æ€§å¡‘å½¢ç®—å­å’Œçº æ­£è·¯å¾„è®¾è®¡ï¼Œä»¥ç¡®ä¿ç»“æ„çº¦æŸçš„æœ‰æ•ˆæ€§ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†è¡¨ç¤ºå¯¹é½çš„éœ€æ±‚ï¼Œç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¼˜åŒ–è¡¨ç¤ºçš„å…¼å®¹æ€§ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œé‡‡ç”¨äº†å…·æœ‰ä¸åŒæ¶æ„å…ˆéªŒçš„æ¨¡å‹è¿›è¡Œå®éªŒï¼Œä»¥éªŒè¯æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨ç»“æ„çº¦æŸçš„æ¨¡å‹åœ¨ä¸åŒæ¶æ„é—´çš„è¡¨ç¤ºå¯¹é½æ€§æ˜¾è‘—æé«˜ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œè¡¨ç¤ºçš„ç¨³å®šæ€§æå‡äº†çº¦20%ã€‚è¿™ä¸€å‘ç°ä¸ºæ¨¡å‹é—´çš„ç‰¹å¾äº’æ“ä½œæ€§æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¨¡å‹è’¸é¦ã€æ¨¡å—åŒ–å­¦ä¹ å’Œç¨³å¥å­¦ä¹ ç³»ç»Ÿè®¾è®¡ã€‚é€šè¿‡æå‡ä¸åŒæ¨¡å‹é—´çš„è¡¨ç¤ºå…¼å®¹æ€§ï¼Œå¯ä»¥åœ¨å¤šç§ä»»åŠ¡ä¸­å®ç°æ›´é«˜æ•ˆçš„çŸ¥è¯†è½¬ç§»å’Œç‰¹å¾å…±äº«ï¼Œä»è€Œæ¨åŠ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½æå‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The internal representations learned by deep networks are often sensitive to architecture-specific choices, raising questions about the stability, alignment, and transferability of learned structure across models. In this paper, we investigate how structural constraints--such as linear shaping operators and corrective paths--affect the compatibility of internal representations across different architectures. Building on the insights from prior studies on structured transformations and convergence, we develop a framework for measuring and analyzing representational alignment across networks with distinct but related architectural priors. Through a combination of theoretical insights, empirical probes, and controlled transfer experiments, we demonstrate that structural regularities induce representational geometry that is more stable under architectural variation. This suggests that certain forms of inductive bias not only support generalization within a model, but also improve the interoperability of learned features across models. We conclude with a discussion on the implications of representational transferability for model distillation, modular learning, and the principled design of robust learning systems.

