---
layout: default
title: A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models
---

# A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09155" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09155v1</a>
  <a href="https://arxiv.org/pdf/2508.09155.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09155v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09155v1', 'A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenkai Wang, Hongcan Guo, Zheqi Lv, Shengyu Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

**å¤‡æ³¨**: 17 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAdaPOä»¥è§£å†³å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹è‡ªæˆ‘è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæˆ‘è¯„ä¼°` `å¤šæ¨¡æ€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `è‡ªé€‚åº”å¥–åŠ±` `åŠ¨æ€æ­£åˆ™åŒ–` `å¯¹è¯ç³»ç»Ÿ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨¡å‹è‡ªæˆ‘è¯„ä¼°ä¸­å­˜åœ¨å›ºå®šå¥–åŠ±æœºåˆ¶å¯¼è‡´çš„å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œå½±å“æ¨¡å‹ç¨³å®šæ€§ã€‚
2. æœ¬æ–‡æå‡ºAdaPOæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´è®­ç»ƒç›®æ ‡ï¼Œç»“åˆè‡ªé€‚åº”å¥–åŠ±æ¨¡å‹å’ŒåŠ¨æ€KLæ­£åˆ™åŒ–æœºåˆ¶ï¼Œè§£å†³äº†å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚
3. åœ¨8ä¸ªåŸºå‡†æµ‹è¯•å’Œå¤šç§æ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAdaPOæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†å’Œè‡ªæˆ‘è¯„ä¼°èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªæˆ‘è¯„ä¼°æ˜¯å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­å®ç°è‡ªæˆ‘æ”¹è¿›çš„å…³é”®èƒ½åŠ›ï¼Œä½†åœ¨åŸºç¡€æ¨¡å‹ä¸­å‡ ä¹ç¼ºå¤±ã€‚ç°æœ‰ç ”ç©¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡è‡ªæˆ‘è¯„ä¼°èƒ½åŠ›ï¼Œä½†å›ºå®šçš„å¥–åŠ±æœºåˆ¶åœ¨ä¼˜åŒ–å¤šä¸ªè®­ç»ƒç›®æ ‡æ—¶å®¹æ˜“å¯¼è‡´å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œä»è€Œå¼•å‘æ¨¡å‹å´©æºƒã€‚æœ¬æ–‡æå‡ºäº†AdaPOï¼Œä¸€ä¸ªåœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å½“å‰è®­ç»ƒçŠ¶æ€å®æ—¶è‡ªé€‚åº”è°ƒæ•´æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒç›®æ ‡ã€‚å…·ä½“è€Œè¨€ï¼ŒAdaPOå¼•å…¥äº†è‡ªé€‚åº”å¥–åŠ±æ¨¡å‹ï¼ˆARMï¼‰å’Œå¥–åŠ±æ„ŸçŸ¥åŠ¨æ€KLæ­£åˆ™åŒ–æœºåˆ¶ï¼Œä»¥ç¼“è§£å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†ç›´æ¥æ¨ç†å’Œè‡ªæˆ‘è¯„ä¼°èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹åœ¨è‡ªæˆ‘è¯„ä¼°ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å›ºå®šå¥–åŠ±æœºåˆ¶å¯¼è‡´çš„å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œè¿™ä¼šå½±å“æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºAdaPOæ¡†æ¶ï¼Œé€šè¿‡å®æ—¶è‡ªé€‚åº”è°ƒæ•´è®­ç»ƒç›®æ ‡ï¼Œåˆ©ç”¨è‡ªé€‚åº”å¥–åŠ±æ¨¡å‹ï¼ˆARMï¼‰å’ŒåŠ¨æ€KLæ­£åˆ™åŒ–æœºåˆ¶ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„è‡ªæˆ‘è¯„ä¼°èƒ½åŠ›ï¼Œé¿å…å›ºå®šå¥–åŠ±å¸¦æ¥çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAdaPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè‡ªé€‚åº”å¥–åŠ±æ¨¡å‹ï¼ˆARMï¼‰ç”¨äºè¯„ä¼°ä»»åŠ¡çš„è®­ç»ƒçŠ¶æ€ï¼Œä»¥åŠå¥–åŠ±æ„ŸçŸ¥åŠ¨æ€KLæ­£åˆ™åŒ–æœºåˆ¶ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´æƒ©ç½šç³»æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è‡ªé€‚åº”å¥–åŠ±æ¨¡å‹å’ŒåŠ¨æ€KLæ­£åˆ™åŒ–æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸åŒå¤šè½®å¯¹è¯çš„æƒ…å†µåŠ¨æ€è°ƒæ•´å­¦ä¹ é‡ç‚¹ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„å›ºå®šæƒ©ç½šé—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒARMé€šè¿‡åˆ†ææ¨¡å‹ç”Ÿæˆçš„å¤šè½®è½¨è¿¹çš„è¡¨ç°æ¥è¯„ä¼°è®­ç»ƒçŠ¶æ€ï¼Œè€ŒåŠ¨æ€KLæ­£åˆ™åŒ–åˆ™ç”¨åŠ¨æ€ç³»æ•°æ›¿ä»£å›ºå®šæƒ©ç½šï¼Œä¾æ®ä¸åŒå¤šè½®æƒ…å†µçš„å¥–åŠ±å·®è·è¿›è¡Œè°ƒèŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdaPOåœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è‡ªæˆ‘è¯„ä¼°èƒ½åŠ›ï¼Œç›´æ¥æ¨ç†æ€§èƒ½æé«˜äº†15%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€å¯¹è¯ç³»ç»Ÿå’Œæ•™è‚²æœºå™¨äººç­‰ï¼Œèƒ½å¤Ÿæå‡å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚å¯¹è¯åœºæ™¯ä¸­çš„è‡ªæˆ‘è¯„ä¼°å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿›è€Œæé«˜ç”¨æˆ·äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„å¯¹è¯ç³»ç»Ÿå‘å±•ï¼Œä¿ƒè¿›äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Self-evaluation, a model's ability to assess the correctness of its own output, is crucial for Large Multimodal Models (LMMs) to achieve self-improvement in multi-turn conversations, yet largely absent in foundation models. Recent work has employed reinforcement learning (RL) to enhance self-evaluation; however, its fixed reward mechanism suffers from reward hacking when optimizing multiple training objectives, leading to model collapse. In this paper we propose AdaPO, an online reinforcement learning framework capable of adaptively adjusting training objective in real time according to the current training state for each task. Specifically, to mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's training state from the distribution of model generated multi-turn trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty with dynamic coefficients which is modulated by the reward gap between different multi-turn situations. Notably, our method automatically and smoothly adjusts its learning focus based on sub-tasks' training progress without manual intervention. Extensive experiments over 8 benchmarks and various models show that our method significantly enhances both direct reasoning and self-evaluation capability. We will release our code to contribute to the community.

