---
layout: default
title: HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation
---

# HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03104" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03104v2</a>
  <a href="https://arxiv.org/pdf/2508.03104.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03104v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03104v2', 'HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mengting Pan, Fan Li, Xiaoyang Wang, Wenjie Zhang, Xuemin Lin

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-08-10)

**å¤‡æ³¨**: 12 pages, 18 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHiTeCæ¡†æ¶ä»¥è§£å†³æ–‡æœ¬å±æ€§è¶…å›¾çš„å¯¹æ¯”å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `è¶…å›¾å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `æ–‡æœ¬å±æ€§` `è¯­ä¹‰å¢å¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ–‡æœ¬å±æ€§è¶…å›¾æ—¶ï¼Œå¿½è§†äº†æ–‡æœ¬ä¿¡æ¯ä¸è¶…å›¾ç»“æ„ä¹‹é—´çš„å…³ç³»ï¼Œå¯¼è‡´è¡¨ç¤ºæ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„HiTeCæ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µçš„å±‚æ¬¡å¯¹æ¯”å­¦ä¹ å’Œè¯­ä¹‰æ„ŸçŸ¥å¢å¼ºç­–ç•¥ï¼Œæå‡äº†è‡ªç›‘ç£å­¦ä¹ çš„æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHiTeCåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†è¡¨ç¤ºå­¦ä¹ çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰å·²æˆä¸ºè‡ªç›‘ç£è¶…å›¾å­¦ä¹ çš„ä¸»æµèŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜‚è´µæ ‡ç­¾çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„è¶…å›¾èŠ‚ç‚¹å®ä½“é€šå¸¸ä¸ä¸°å¯Œçš„æ–‡æœ¬ä¿¡æ¯ç›¸å…³ï¼Œè¿™åœ¨ä»¥å¾€çš„ç ”ç©¶ä¸­è¢«å¿½è§†ã€‚ç›´æ¥å°†ç°æœ‰çš„åŸºäºCLçš„æ–¹æ³•åº”ç”¨äºæ–‡æœ¬å±æ€§è¶…å›¾ï¼ˆTAHGsï¼‰å­˜åœ¨ä¸‰å¤§å…³é”®å±€é™æ€§ï¼šä¸€æ˜¯å¸¸ç”¨çš„å›¾æ— å…³æ–‡æœ¬ç¼–ç å™¨å¿½ç•¥äº†æ–‡æœ¬å†…å®¹ä¸è¶…å›¾æ‹“æ‰‘ä¹‹é—´çš„å…³è”ï¼Œå¯¼è‡´è¡¨ç¤ºæ•ˆæœä¸ä½³ï¼›äºŒæ˜¯ä¾èµ–éšæœºæ•°æ®å¢å¼ºå¼•å…¥å™ªå£°ï¼Œå‰Šå¼±äº†å¯¹æ¯”ç›®æ ‡ï¼›ä¸‰æ˜¯ä¸»è¦å…³æ³¨èŠ‚ç‚¹å’Œè¶…è¾¹çº§åˆ«çš„å¯¹æ¯”ä¿¡å·ï¼Œé™åˆ¶äº†æ•æ‰é•¿è·ç¦»ä¾èµ–çš„èƒ½åŠ›ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†HiTeCï¼Œä¸€ä¸ªå…·æœ‰è¯­ä¹‰æ„ŸçŸ¥å¢å¼ºçš„ä¸¤é˜¶æ®µå±‚æ¬¡å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°TAHGsçš„å¯æ‰©å±•å’Œæœ‰æ•ˆçš„è‡ªç›‘ç£å­¦ä¹ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨æ–‡æœ¬å±æ€§è¶…å›¾ï¼ˆTAHGsï¼‰ä¸­åº”ç”¨æ—¶çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å›¾æ— å…³æ–‡æœ¬ç¼–ç å™¨å¯¼è‡´çš„è¡¨ç¤ºä¸ä½³ã€éšæœºæ•°æ®å¢å¼ºå¼•å…¥çš„å™ªå£°ä»¥åŠå¯¹æ¯”ä¿¡å·çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHiTeCæ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µçš„å­¦ä¹ ç­–ç•¥ï¼Œé¦–å…ˆå¯¹æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œç»“æ„æ„ŸçŸ¥çš„å¯¹æ¯”é¢„è®­ç»ƒï¼Œç„¶åå¼•å…¥è¯­ä¹‰æ„ŸçŸ¥çš„å¢å¼ºç­–ç•¥ï¼Œä»¥ç”Ÿæˆæ›´å…·ä¿¡æ¯é‡çš„è§†å›¾ï¼Œä»è€Œæå‡è¡¨ç¤ºå­¦ä¹ çš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHiTeCçš„æ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯æ–‡æœ¬ç¼–ç å™¨çš„é¢„è®­ç»ƒï¼Œé‡‡ç”¨ç»“æ„æ„ŸçŸ¥çš„å¯¹æ¯”ç›®æ ‡ï¼›ç¬¬äºŒé˜¶æ®µåˆ™å¼•å…¥ä¸¤ç§è¯­ä¹‰æ„ŸçŸ¥çš„å¢å¼ºç­–ç•¥ï¼ŒåŒ…æ‹¬æç¤ºå¢å¼ºæ–‡æœ¬å¢å¼ºå’Œè¯­ä¹‰æ„ŸçŸ¥è¶…è¾¹ä¸¢å¼ƒï¼Œä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„è§†å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šHiTeCçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä¸¤é˜¶æ®µè®¾è®¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£è€¦æ–‡æœ¬ç¼–ç å™¨çš„é¢„è®­ç»ƒä¸è¶…å›¾å¯¹æ¯”å­¦ä¹ ï¼Œæå‡äº†å¯æ‰©å±•æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è¡¨ç¤ºè´¨é‡çš„é«˜æ°´å¹³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°æ–¹é¢ï¼ŒHiTeCæå‡ºäº†ä¸€ç§å¤šå°ºåº¦å¯¹æ¯”æŸå¤±ï¼Œæ‰©å±•äº†ç°æœ‰ç›®æ ‡ï¼Œç»“åˆ$s$-walkåŸºç¡€çš„å­å›¾çº§åˆ«å¯¹æ¯”ï¼Œä»¥æ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHiTeCåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šç›¸è¾ƒäºç°æœ‰æ–¹æ³•æå‡äº†è¡¨ç¤ºå­¦ä¹ æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šå‡†ç¡®ç‡æé«˜äº†10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HiTeCæ¡†æ¶åœ¨ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿå’ŒçŸ¥è¯†å›¾è°±ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨æ–‡æœ¬ä¿¡æ¯ä¸è¶…å›¾ç»“æ„çš„å…³ç³»ï¼ŒHiTeCèƒ½å¤Ÿæå‡ä¿¡æ¯æ£€ç´¢ã€ç”¨æˆ·è¡Œä¸ºé¢„æµ‹ç­‰ä»»åŠ¡çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Contrastive learning (CL) has become a dominant paradigm for self-supervised hypergraph learning, enabling effective training without costly labels. However, node entities in real-world hypergraphs are often associated with rich textual information, which is overlooked in prior works. Directly applying existing CL-based methods to such text-attributed hypergraphs (TAHGs) leads to three key limitations: (1) The common use of graph-agnostic text encoders overlooks the correlations between textual content and hypergraph topology, resulting in suboptimal representations. (2) Their reliance on random data augmentations introduces noise and weakens the contrastive objective. (3) The primary focus on node- and hyperedge-level contrastive signals limits the ability to capture long-range dependencies, which is essential for expressive representation learning. Although HyperBERT pioneers CL on TAHGs, its co-training paradigm suffers from poor scalability. To fill the research gap, we introduce HiTeC, a two-stage hierarchical contrastive learning framework with semantic-aware augmentation for scalable and effective self-supervised learning on TAHGs. In the first stage, we pre-train the text encoder with a structure-aware contrastive objective to overcome the graph-agnostic nature of conventional methods. In the second stage, we introduce two semantic-aware augmentation strategies, including prompt-enhanced text augmentation and semantic-aware hyperedge drop, to facilitate informative view generation. Furthermore, we propose a multi-scale contrastive loss that extends existing objectives with an $s$-walk-based subgraph-level contrast to better capture long-range dependencies. By decoupling text encoder pretraining from hypergraph contrastive learning, this two-stage design enhances scalability without compromising representation quality. Extensive experiments confirm the effectiveness of HiTeC.

