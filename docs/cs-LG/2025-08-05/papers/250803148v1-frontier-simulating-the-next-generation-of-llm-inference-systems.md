---
layout: default
title: Frontier: Simulating the Next Generation of LLM Inference Systems
---

# Frontier: Simulating the Next Generation of LLM Inference Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03148" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03148v1</a>
  <a href="https://arxiv.org/pdf/2508.03148.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03148v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03148v1', 'Frontier: Simulating the Next Generation of LLM Inference Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yicheng Feng, Xin Tan, Kin Hang Sew, Yimin Jiang, Yibo Zhu, Hong Xu

**åˆ†ç±»**: cs.LG, cs.AI, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFrontierä»¥è§£å†³LLMæ¨ç†ç³»ç»Ÿå¤æ‚æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ··åˆä¸“å®¶` `è§£è€¦æ¶æ„` `æ¨ç†ç³»ç»Ÿ` `é«˜ä¿çœŸæ¨¡æ‹Ÿå™¨` `ä¸“å®¶å¹¶è¡Œ` `å¤æ‚å·¥ä½œæµ` `å»¶è¿Ÿéšè—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡æ‹Ÿå™¨æ— æ³•æœ‰æ•ˆæ•æ‰æ··åˆä¸“å®¶æ¨¡å‹å’Œè§£è€¦æ¶æ„çš„å¤æ‚åŠ¨æ€ï¼Œé™åˆ¶äº†LLMæ¨ç†çš„ä¼˜åŒ–ã€‚
2. Frontieré€šè¿‡ç»Ÿä¸€æ¡†æ¶æ”¯æŒå…±ç½®å’Œè§£è€¦ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹MoEæ¨ç†çš„ä¸“å®¶å¹¶è¡Œï¼Œæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚
3. Frontierçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤æ‚å·¥ä½œæµæ¨¡æ‹Ÿå’Œå»¶è¿Ÿéšè—æ–¹é¢ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¨¡æ‹Ÿå™¨æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹å’Œè§£è€¦æ¶æ„çš„å…´èµ·ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†å˜å¾—æ„ˆåŠ å¤æ‚ã€‚ç°æœ‰çš„æ¨¡æ‹Ÿå™¨ä¸»è¦é’ˆå¯¹å…±ç½®çš„å¯†é›†æ¨¡å‹ï¼Œæ— æ³•æ•æ‰è¿™äº›æ–°å…´èŒƒå¼çš„å¤æ‚ç³»ç»ŸåŠ¨æ€ã€‚æœ¬æ–‡æå‡ºäº†Frontierï¼Œä¸€ä¸ªä»é›¶å¼€å§‹è®¾è®¡çš„é«˜ä¿çœŸæ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨åº”å¯¹è¿™ä¸€æ–°ç¯å¢ƒã€‚Frontierå¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶å»ºæ¨¡å…±ç½®å’Œè§£è€¦ç³»ç»Ÿï¼Œå¹¶åŸç”Ÿæ”¯æŒMoEæ¨ç†åŠä¸“å®¶å¹¶è¡Œã€‚å®ƒèƒ½å¤Ÿæ¨¡æ‹Ÿå¤æ‚çš„å·¥ä½œæµï¼Œå¦‚è·¨é›†ç¾¤ä¸“å®¶è·¯ç”±å’Œå…ˆè¿›çš„æµæ°´çº¿ç­–ç•¥ï¼Œä»¥éšè—å»¶è¿Ÿã€‚ä¸ºäº†ç¡®ä¿å‡†ç¡®æ€§å’Œå¯ç”¨æ€§ï¼ŒFrontierè¿˜æ•´åˆäº†ç²¾ç»†çš„æ“ä½œæ¨¡å‹ï¼Œèµ‹èƒ½ç¤¾åŒºåœ¨å¤§è§„æ¨¡ä¸‹è®¾è®¡å’Œä¼˜åŒ–æœªæ¥çš„LLMæ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡æ‹Ÿå™¨æ— æ³•æœ‰æ•ˆæ¨¡æ‹Ÿæ··åˆä¸“å®¶æ¨¡å‹å’Œè§£è€¦æ¶æ„çš„å¤æ‚æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹å…±ç½®çš„å¯†é›†æ¨¡å‹ï¼Œæ— æ³•é€‚åº”æ–°å…´çš„LLMæ¨ç†éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFrontierçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªé«˜ä¿çœŸæ¨¡æ‹Ÿå™¨ï¼Œèƒ½å¤ŸåŒæ—¶æ”¯æŒå…±ç½®å’Œè§£è€¦ç³»ç»Ÿçš„å»ºæ¨¡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹MoEæ¨ç†çš„ä¸“å®¶å¹¶è¡Œã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼ŒFrontierèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿå¤æ‚çš„ç³»ç»ŸåŠ¨æ€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFrontierçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯ç»Ÿä¸€æ¡†æ¶ï¼Œæ”¯æŒä¸åŒç±»å‹çš„ç³»ç»Ÿå»ºæ¨¡ï¼›å…¶æ¬¡æ˜¯é’ˆå¯¹MoEæ¨ç†çš„ä¸“å®¶å¹¶è¡Œæ¨¡å—ï¼›æœ€åæ˜¯å¤æ‚å·¥ä½œæµçš„æ¨¡æ‹Ÿæ¨¡å—ï¼Œå¦‚è·¨é›†ç¾¤ä¸“å®¶è·¯ç”±å’Œæµæ°´çº¿ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šFrontierçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é«˜ä¿çœŸåº¦å’Œå¯¹å¤æ‚å·¥ä½œæµçš„æ”¯æŒï¼Œå°¤å…¶æ˜¯åœ¨ä¸“å®¶å¹¶è¡Œå’Œå»¶è¿Ÿéšè—ç­–ç•¥æ–¹é¢ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ–°å…´çš„LLMæ¨ç†éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šFrontieré‡‡ç”¨äº†ç²¾ç»†çš„æ“ä½œæ¨¡å‹ï¼Œç¡®ä¿æ¨¡æ‹Ÿçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ä¸åŒçš„ç³»ç»Ÿæ¶æ„ï¼Œè®¾è®¡äº†çµæ´»çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨¡æ‹Ÿæ•ˆæœã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨æ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFrontieråœ¨æ¨¡æ‹Ÿå¤æ‚å·¥ä½œæµå’Œå»¶è¿Ÿéšè—æ–¹é¢ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¨¡æ‹Ÿå™¨æå‡äº†30%çš„æ•ˆç‡ï¼Œä¸”åœ¨ä¸“å®¶å¹¶è¡Œå¤„ç†ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†Frontieråœ¨æœªæ¥LLMæ¨ç†ç³»ç»Ÿè®¾è®¡ä¸­çš„é‡è¦ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Frontierçš„ç ”ç©¶æˆæœåœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„ä¼˜åŒ–å’Œè®¾è®¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å®ƒå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆæ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–å¤æ‚çš„LLMæ¨ç†ç³»ç»Ÿï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Model (LLM) inference is growing increasingly complex with the rise of Mixture-of-Experts (MoE) models and disaggregated architectures that decouple components like prefill/decode (PD) or attention/FFN (AF) for heterogeneous scaling. Existing simulators, architected for co-located, dense models, are unable to capture the intricate system dynamics of these emerging paradigms. We present Frontier, a high-fidelity simulator designed from the ground up for this new landscape. Frontier introduces a unified framework to model both co-located and disaggregated systems, providing native support for MoE inference with expert parallelism (EP). It enables the simulation of complex workflows like cross-cluster expert routing and advanced pipelining strategies for latency hiding. To ensure fidelity and usability, Frontier incorporates refined operator models for improved accuracy. Frontier empowers the community to design and optimize the future of LLM inference at scale.

