---
layout: default
title: GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control
---

# GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03772" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03772v5</a>
  <a href="https://arxiv.org/pdf/2508.03772.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03772v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03772v5', 'GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Marco Simoni, Aleksandar Fontana, Giulio Rossolini, Andrea Saracino, Paolo Mori

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-12-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGTPOä»¥è§£å†³GRPOè®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–` `è®­ç»ƒç¨³å®šæ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç†µè¿‡æ»¤` `è´Ÿæ›´æ–°` `ç­–ç•¥å´©æºƒ` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„GRPOæ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨ä¸ç¨³å®šæ€§å’Œæ”¶æ•›æ€§å·®çš„é—®é¢˜ï¼Œå½±å“äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚
2. GTPOé€šè¿‡è·³è¿‡è´Ÿæ›´æ–°å¹¶æ”¾å¤§æ­£æ›´æ–°æ¥è§£å†³çŸ›ç›¾æ¢¯åº¦é—®é¢˜ï¼ŒåŒæ—¶è¿‡æ»¤é«˜ç†µå®Œæˆä»¥é˜²æ­¢ç­–ç•¥å´©æºƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGTPOåœ¨GSM8Kã€MATHç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½ï¼Œä½†å…¶æ€§èƒ½å¸¸å¸¸å—åˆ°è®­ç»ƒä¸ç¨³å®šæ€§å’Œæ”¶æ•›ä¸ä½³çš„é™åˆ¶ã€‚æœ¬æ–‡è¯†åˆ«å¹¶åˆ†æäº†GRPOçš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯ä»¤ç‰Œçº§æƒ©ç½šï¼Œå¯¼è‡´ä¸åŒå“åº”ä¸­å…±äº«çš„æœ‰ä»·å€¼ä»¤ç‰Œæ”¶åˆ°çŸ›ç›¾çš„åé¦ˆä¿¡å·ï¼›äºŒæ˜¯ç­–ç•¥å´©æºƒï¼Œè´Ÿå¥–åŠ±çš„å®Œæˆå¯èƒ½ä¼šæƒ©ç½šè‡ªä¿¡çš„å“åº”ï¼Œè¿›è€Œä½¿æ¨¡å‹å†³ç­–åå‘ä¸å¤ªå¯èƒ½çš„ä»¤ç‰Œã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GTPOï¼ˆåŸºäºè½¨è¿¹çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œé€šè¿‡è·³è¿‡è´Ÿæ›´æ–°å¹¶æ”¾å¤§æ­£æ›´æ–°æ¥é˜²æ­¢æœ‰ä»·å€¼ä»¤ç‰Œä¸Šçš„çŸ›ç›¾æ¢¯åº¦ï¼ŒåŒæ—¶è¿‡æ»¤æ‰ç†µè¶…è¿‡å¯è¯æ˜é˜ˆå€¼çš„å®Œæˆï¼Œä»¥é˜²æ­¢ç­–ç•¥å´©æºƒã€‚ä¸GRPOä¸åŒï¼ŒGTPOä¸ä¾èµ–äºKLæ•£åº¦æ­£åˆ™åŒ–ï¼Œæ¶ˆé™¤äº†è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹å‚è€ƒæ¨¡å‹çš„éœ€æ±‚ï¼ŒåŒæ—¶ç¡®ä¿äº†æ›´å¤§çš„è®­ç»ƒç¨³å®šæ€§å’Œæ”¹è¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³GRPOåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä»¤ç‰Œçº§æƒ©ç½šå¯¼è‡´çš„çŸ›ç›¾æ¢¯åº¦æ›´æ–°ï¼Œä»¥åŠè´Ÿå¥–åŠ±å®Œæˆå¼•èµ·çš„ç­–ç•¥å´©æºƒã€‚è¿™äº›é—®é¢˜ä¼šé™ä½æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGTPOçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è·³è¿‡å¯¹æœ‰ä»·å€¼ä»¤ç‰Œçš„è´Ÿæ›´æ–°æ¥é¿å…çŸ›ç›¾æ¢¯åº¦ï¼ŒåŒæ—¶é€šè¿‡ç†µè¿‡æ»¤æœºåˆ¶é˜²æ­¢ç­–ç•¥å´©æºƒã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å¢å¼ºæ¨¡å‹å¯¹æœ‰ä»·å€¼ä¿¡æ¯çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGTPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯å¯¹æœ‰ä»·å€¼ä»¤ç‰Œçš„æ­£æ›´æ–°æ”¾å¤§ï¼ŒäºŒæ˜¯ç†µè¿‡æ»¤æœºåˆ¶ã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¨¡å—çš„ç»“åˆï¼ŒGTPOèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒç¨³å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šGTPOçš„ä¸»è¦åˆ›æ–°åœ¨äºä¸ä¾èµ–KLæ•£åº¦æ­£åˆ™åŒ–ï¼Œæ¶ˆé™¤äº†å¯¹å‚è€ƒæ¨¡å‹çš„éœ€æ±‚ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´åŠ ç®€åŒ–ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GTPOä¸­ï¼Œè´Ÿæ›´æ–°è¢«è·³è¿‡ï¼Œæ­£æ›´æ–°è¢«æ”¾å¤§ï¼Œç†µè¿‡æ»¤çš„é˜ˆå€¼ç»è¿‡ç†è®ºè¯æ˜ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šå‡ºç°ç­–ç•¥å´©æºƒçš„ç°è±¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGTPOåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆå¦‚GSM8Kã€MATHã€AIME 2024ã€AIME 2025å’ŒAMC 2023ï¼‰ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè®­ç»ƒç¨³å®šæ€§æ˜æ˜¾å¢å¼ºï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œä½†ç›¸è¾ƒäºGRPOæœ‰æ˜æ˜¾æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GTPOçš„ç ”ç©¶æˆæœåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œå¯¹é½ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜ç¨³å®šæ€§å’Œé«˜æ€§èƒ½çš„ä»»åŠ¡ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½é—®ç­”ç­‰é¢†åŸŸã€‚æœªæ¥ï¼ŒGTPOå¯èƒ½ä¼šæ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒæ–¹æ³•çš„å‘å±•ï¼Œæå‡äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¯é æ€§å’Œæ™ºèƒ½æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Group Relative Policy Optimization (GRPO) is a promising policy-based approach for Large Language Model alignment, yet its performance is often limited by training instability and suboptimal convergence. In this paper, we identify and analyze two main GRPO issues: (i) the token-level penalization, where valuable tokens shared across different responses receive contradictory feedback signals, leading to conflicting gradient updates that can reduce their likelihood; and (ii) the policy collapse, where negatively rewarded completions may penalize confident responses and shift model decisions toward unlikely tokens, destabilizing training process. To address these issues we introduce GTPO (Group-relative Trajectory-based Policy Optimization), which prevents conflicting gradients on valuable tokens by skipping negative updates while amplifying positive ones and filters out completions whose entropy exceeds a provable threshold, to prevent policy collapse. Unlike GRPO, GTPO does not rely on KL-divergence regularization, eliminating the need for a reference model during training, while still ensuring greater training stability and improved performance, as validated through multiple experiments on GSM8K, MATH, AIME 2024, AIME 2025 and AMC 2023.

