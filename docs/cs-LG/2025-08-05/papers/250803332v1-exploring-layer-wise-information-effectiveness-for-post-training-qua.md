---
layout: default
title: Exploring Layer-wise Information Effectiveness for Post-Training Quantization in Small Language Models
---

# Exploring Layer-wise Information Effectiveness for Post-Training Quantization in Small Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03332" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03332v1</a>
  <a href="https://arxiv.org/pdf/2508.03332.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03332v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03332v1', 'Exploring Layer-wise Information Effectiveness for Post-Training Quantization in Small Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: He Xiao, Qingyao Yang, Dirui Xie, Wendong Xu, Wenyong Zhou, Haobo Liu, Zhengwu Liu, Ngai Wong

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

**å¤‡æ³¨**: low-bit quantization

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLieQæ¡†æ¶ä»¥è§£å†³å°å‹è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒé‡åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åè®­ç»ƒé‡åŒ–` `å°å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `å±‚çº§è¯Šæ–­` `æ¯”ç‰¹å®½åº¦åˆ†é…` `è¾¹ç¼˜è®¡ç®—` `èµ„æºä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨2-3æ¯”ç‰¹ç²¾åº¦ä¸‹ä¼šä¸¥é‡é™ä½å‡†ç¡®æ€§ï¼Œéš¾ä»¥æ»¡è¶³å°å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨éœ€æ±‚ã€‚
2. LieQæ¡†æ¶é€šè¿‡å¼•å…¥å±‚çº§è¯Šæ–­æŒ‡æ ‡ï¼Œè‡ªåŠ¨åˆ†é…æ¯”ç‰¹å®½åº¦ï¼Œé¿å…äº†æ¢¯åº¦æ›´æ–°ï¼Œä»è€Œåœ¨ä½æ¯”ç‰¹å‹ç¼©ä¸‹ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚
3. åœ¨å¤šä¸ªé›¶-shotæ¨ç†ä»»åŠ¡ä¸­ï¼ŒLieQåœ¨å‹ç¼©-å‡†ç¡®æ€§æƒè¡¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†å°å‹è¯­è¨€æ¨¡å‹çš„å®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸å­˜åœ¨è¿‡åº¦é…ç½®çš„é—®é¢˜ï¼Œè®¸å¤šå±‚çš„è´¡çŒ®æœ‰é™ï¼Œå´å ç”¨äº†å¤§é‡å†…å­˜å’Œèƒ½é‡ã€‚æœ¬æ–‡æå‡ºäº†LieQï¼Œä¸€ä¸ªåŸºäºæŒ‡æ ‡é©±åŠ¨çš„åè®­ç»ƒé‡åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨æä½æ¯”ç‰¹å‹ç¼©ä¸‹ä¿æŒå°å‹æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸‰ç§äº’è¡¥çš„å±‚çº§è¯Šæ–­æŒ‡æ ‡ï¼Œæ­ç¤ºäº†å±‚ä¹‹é—´çš„å…¸å‹åˆ†å·¥ï¼Œä»è€Œå®ç°è‡ªåŠ¨æ¯”ç‰¹å®½åº¦åˆ†é…ã€‚LieQåœ¨Qwen3-4Bæ¨¡å‹ä¸Šä»¥2.05æ¯”ç‰¹é‡åŒ–æ¢å¤äº†95.9%çš„FP16åŸºçº¿æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸”åœ¨LLaMA3.2-3Bä¸Šä»¥2.07æ¯”ç‰¹ç²¾åº¦ä¿æŒäº†98.2%çš„åŸºçº¿å‡†ç¡®æ€§ï¼ŒåŒæ—¶å®ç°äº†4å€çš„å†…å­˜å‡å°‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹åœ¨æä½æ¯”ç‰¹å‹ç¼©ä¸‹ä¿æŒå‡†ç¡®æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä½æ¯”ç‰¹ç²¾åº¦ä¸‹é€šå¸¸ä¼šå¯¼è‡´æ˜¾è‘—çš„å‡†ç¡®æ€§ä¸‹é™ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLieQæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¼•å…¥ä¸‰ç§å±‚çº§è¯Šæ–­æŒ‡æ ‡ï¼ˆå›°æƒ‘åº¦ä¸‹é™ã€è¡¨ç¤ºç´§å‡‘æ€§å’ŒTop-kèƒ½é‡å¢ç›Šï¼‰ï¼Œæ­ç¤ºå±‚ä¹‹é—´çš„åˆ†å·¥ï¼Œä»è€Œå®ç°è‡ªåŠ¨çš„æ¯”ç‰¹å®½åº¦åˆ†é…ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„æ¢¯åº¦æ›´æ–°è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLieQçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå±‚çº§è¯Šæ–­æ¨¡å—ã€æ¯”ç‰¹å®½åº¦åˆ†é…æ¨¡å—å’Œé‡åŒ–æ¨¡å—ã€‚å±‚çº§è¯Šæ–­æ¨¡å—è´Ÿè´£è®¡ç®—å„å±‚çš„æ€§èƒ½æŒ‡æ ‡ï¼Œæ¯”ç‰¹å®½åº¦åˆ†é…æ¨¡å—æ ¹æ®è¯Šæ–­ç»“æœè‡ªåŠ¨åˆ†é…æ¯”ç‰¹å®½åº¦ï¼Œé‡åŒ–æ¨¡å—åˆ™æ‰§è¡Œå®é™…çš„æ¨¡å‹é‡åŒ–æ“ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šLieQçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æŒ‡æ ‡é©±åŠ¨çš„é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡å±‚çº§è¯Šæ–­å®ç°äº†æ¯”ç‰¹å®½åº¦çš„è‡ªåŠ¨åˆ†é…ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†ä½æ¯”ç‰¹é‡åŒ–ä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLieQä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å‹ç¼©ç‡ä¸å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨æ¯”ç‰¹å®½åº¦åˆ†é…æ—¶è€ƒè™‘äº†å„å±‚çš„ç‰¹å¾é‡è¦æ€§ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨é‡åŒ–åçš„æ€§èƒ½ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LieQåœ¨Qwen3-4Bæ¨¡å‹ä¸Šä»¥2.05æ¯”ç‰¹é‡åŒ–æ¢å¤äº†95.9%çš„FP16åŸºçº¿æ€§èƒ½ï¼Œè¶…è¶Šäº†GPTQå’ŒAWQï¼Œåˆ†åˆ«æé«˜äº†19.7%å’Œ18.1%ã€‚åœ¨LLaMA3.2-3Bæ¨¡å‹ä¸Šï¼ŒLieQä»¥2.07æ¯”ç‰¹ç²¾åº¦ä¿æŒäº†98.2%çš„åŸºçº¿å‡†ç¡®æ€§ï¼ŒåŒæ—¶å®ç°äº†4å€çš„å†…å­˜å‡å°‘ï¼Œå±•ç°äº†ä¼˜è¶Šçš„å‹ç¼©-å‡†ç¡®æ€§æƒè¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡å’Œç§»åŠ¨è®¾å¤‡ä¸Šçš„å°å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²ã€‚é€šè¿‡æœ‰æ•ˆçš„åè®­ç»ƒé‡åŒ–ï¼ŒLieQèƒ½å¤Ÿåœ¨ä¿æŒé«˜å‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œæ¨åŠ¨äº†å°å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models with billions of parameters are often over-provisioned: many layers contribute little unique information yet dominate the memory and energy footprint during inference. We present LieQ, a metric-driven post-training quantization framework that addresses the critical challenge of maintaining accuracy in sub-7B models under extreme low-bit compression. Our method introduces three complementary layer-wise diagnostics-Perplexity Drop, Representational Compactness, and Top-k Energy Gain -that reveal a canonical division of labour across layers, enabling automatic bit-width allocation without gradient updates. Unlike existing approaches that suffer severe accuracy degradation at 2-3 bits precision, LieQ achieves state-of-the-art compression-accuracy trade-offs: on Qwen3-4B, it recovers 95.9% of FP16 baseline performance at 2.05-bit quantization, outperforming GPTQ by 19.7% and AWQ by 18.1% on average across seven zero-shot reasoning tasks. Applied to LLaMA3.2-3B, LieQ maintains 98.2% of baseline accuracy at 2.07-bit precision while enabling 4x memory reduction, establishing new paradigms for deploying small language models on resource-constrained edge devices.

