---
layout: default
title: Understanding protein function with a multimodal retrieval-augmented foundation model
---

# Understanding protein function with a multimodal retrieval-augmented foundation model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04724" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04724v1</a>
  <a href="https://arxiv.org/pdf/2508.04724.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04724v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04724v1', 'Understanding protein function with a multimodal retrieval-augmented foundation model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Timothy Fei Truong, Tristan Bepler

**åˆ†ç±»**: q-bio.QM, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPoET-2ä»¥è§£å†³è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹çš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è›‹ç™½è´¨è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å»ºæ¨¡` `æ£€ç´¢å¢å¼º` `å˜ä½“æ•ˆåº”é¢„æµ‹` `ç”Ÿç‰©ä¿¡æ¯å­¦` `ç»“æ„é¢„æµ‹` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹åœ¨çªå˜ç†è§£å’ŒåŠŸèƒ½é¢„æµ‹çš„è¡¨ç¤ºè´¨é‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºçš„PoET-2æ¨¡å‹é€šè¿‡å¤šæ¨¡æ€å’Œæ£€ç´¢å¢å¼ºçš„æ–¹æ³•ï¼Œç»“åˆè¿›åŒ–çº¦æŸå’Œç»“æ„ä¿¡æ¯ï¼Œæå‡è›‹ç™½è´¨åºåˆ—çš„ç”Ÿæˆèƒ½åŠ›ã€‚
3. PoET-2åœ¨é›¶æ ·æœ¬å˜ä½“æ•ˆåº”é¢„æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å°æ•°æ®é›†ä¸Šï¼Œå…¶åµŒå…¥æ•ˆæœè¶…è¶Šäº†ä»¥å¾€æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰é€šè¿‡å­¦ä¹ æ•°äº¿æ¡è‡ªç„¶è›‹ç™½è´¨åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒï¼Œé€æ¸å…·å¤‡è›‹ç™½è´¨ç†è§£å’Œè®¾è®¡èƒ½åŠ›ã€‚å°½ç®¡ç°æœ‰æ¨¡å‹åœ¨ç»“æ„é¢„æµ‹ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨çªå˜ç†è§£å’ŒåŠŸèƒ½é¢„æµ‹çš„è¡¨ç¤ºè´¨é‡ä¸Šä»å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†PoET-2ï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€ã€æ£€ç´¢å¢å¼ºçš„è›‹ç™½è´¨åŸºç¡€æ¨¡å‹ï¼Œç»“åˆäº†å®¶æ—ç‰¹å®šçš„è¿›åŒ–çº¦æŸå’Œå¯é€‰çš„ç»“æ„æ¡ä»¶ï¼Œä»¥å­¦ä¹ è›‹ç™½è´¨åºåˆ—çš„ç”Ÿæˆåˆ†å¸ƒã€‚PoET-2é‡‡ç”¨äº†å±‚æ¬¡åŒ–çš„å˜æ¢å™¨ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå¤„ç†åºåˆ—ä¸Šä¸‹æ–‡é¡ºåºçš„ç­‰å˜æ€§ï¼Œå¹¶å…·å¤‡å› æœå’Œæ©è”½è¯­è¨€å»ºæ¨¡ç›®æ ‡çš„åŒè§£ç å™¨æ¶æ„ï¼Œæ”¯æŒå®Œå…¨ç”Ÿæˆå’ŒåŒå‘è¡¨ç¤ºå­¦ä¹ æ¨¡å¼ã€‚PoET-2åœ¨é›¶æ ·æœ¬å˜ä½“æ•ˆåº”é¢„æµ‹ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤„ç†å¤šçªå˜å’ŒæŒ‘æˆ˜æ€§æ’å…¥ç¼ºå¤±çªå˜æ—¶è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è›‹ç™½è´¨è¯­è¨€æ¨¡å‹åœ¨çªå˜ç†è§£å’ŒåŠŸèƒ½é¢„æµ‹ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å°æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPoET-2é€šè¿‡ç»“åˆå¤šæ¨¡æ€ä¿¡æ¯å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œåˆ©ç”¨å®¶æ—ç‰¹å®šçš„è¿›åŒ–çº¦æŸå’Œç»“æ„æ¡ä»¶æ¥å­¦ä¹ è›‹ç™½è´¨åºåˆ—çš„ç”Ÿæˆåˆ†å¸ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPoET-2çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å±‚æ¬¡åŒ–çš„å˜æ¢å™¨ç¼–ç å™¨å’ŒåŒè§£ç å™¨ï¼Œæ”¯æŒå› æœå’Œæ©è”½è¯­è¨€å»ºæ¨¡ç›®æ ‡ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆå’ŒåŒå‘è¡¨ç¤ºå­¦ä¹ æ¨¡å¼ä¸‹è¿ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šPoET-2çš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ£€ç´¢å¢å¼ºçš„å¤šæ¨¡æ€å»ºæ¨¡æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç»“åˆè¿›åŒ–ä¿¡æ¯ä¸ç»“æ„ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼ŒPoET-2é‡‡ç”¨äº†å±‚æ¬¡åŒ–çš„å˜æ¢å™¨ç¼–ç å™¨ï¼Œè®¾è®¡äº†åŒè§£ç å™¨æ¶æ„ï¼Œå¹¶ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ çš„æ•ˆæœã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PoET-2åœ¨é›¶æ ·æœ¬å˜ä½“æ•ˆåº”é¢„æµ‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤„ç†å¤šçªå˜å’Œæ’å…¥ç¼ºå¤±çªå˜æ—¶è¡¨ç°ä¼˜å¼‚ã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼Œå…¶åœ¨å°æ•°æ®é›†ä¸Šçš„åµŒå…¥æ•ˆæœæ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†æ£€ç´¢å¢å¼ºå’Œå¤šæ¨¡æ€å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PoET-2æ¨¡å‹åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ã€è¯ç‰©è®¾è®¡å’ŒåŸºå› å·¥ç¨‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤ŸåŠ é€Ÿæ–°è¯çš„ç ”å‘å’Œè›‹ç™½è´¨å·¥ç¨‹çš„è¿›å±•ï¼Œæ¨åŠ¨ç”Ÿç‰©æŠ€æœ¯çš„åˆ›æ–°ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Protein language models (PLMs) learn probability distributions over natural protein sequences. By learning from hundreds of millions of natural protein sequences, protein understanding and design capabilities emerge. Recent works have shown that scaling these models improves structure prediction, but does not seem to improve mutation understanding and representation quality for protein function prediction. We introduce PoET-2, a multimodal, retrieval-augmented protein foundation model that incorporates in-context learning of family-specific evolutionary constraints with optional structure conditioning to learn generative distributions over protein sequences. PoET-2 uses a hierarchical transformer encoder that is equivariant to sequence context ordering and a dual decoder architecture with both causal and masked language modeling objectives, allowing PoET-2 to operate in both fully generative and bidirectional representation learning modes. PoET-2 achieves state-of-the-art performance on zero-shot variant effect prediction, excelling at scoring variants with multiple mutations and challenging indel mutations. In supervised settings, PoET-2 embeddings outperform previous methods for learning sequence-function relationships, especially with small datasets. This work highlights the benefits of combining retrieval augmentation with multimodal, family-centric modeling for advancing protein foundation models.

