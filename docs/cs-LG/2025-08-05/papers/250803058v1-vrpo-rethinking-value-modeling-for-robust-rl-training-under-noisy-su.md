---
layout: default
title: VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision
---

# VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03058" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03058v1</a>
  <a href="https://arxiv.org/pdf/2508.03058.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03058v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03058v1', 'VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dingwei Zhu, Shihan Dou, Zhiheng Xi, Senjie Jin, Guoqiang Zhang, Jiazheng Zhang, Junjie Ye, Mingxu Chai, Enyu Zhou, Ming Zhang, Caishuang Huang, Yunke Zhang, Yuran Wang, Tao Gui

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVRPOä»¥è§£å†³å™ªå£°ç›‘ç£ä¸‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ä»·å€¼æ¨¡å‹` `å™ªå£°ç›‘ç£` `PPO` `RLHF` `ç­–ç•¥ä¼˜åŒ–` `å¤šè½®å¯¹è¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å™ªå£°å¥–åŠ±æ—¶ï¼Œå¾€å¾€å¯¼è‡´ç­–ç•¥ä¸ç¨³å®šå’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­ã€‚
2. æœ¬æ–‡æå‡ºVRPOæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–ä»·å€¼æ¨¡å‹çš„èƒ½åŠ›ï¼Œåˆ©ç”¨è¾…åŠ©æŸå¤±å’Œå˜åˆ†ä¿¡æ¯ç“¶é¢ˆæ¥æé«˜ä¼˜åŠ¿ä¼°è®¡çš„å¯é æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVRPOåœ¨å¤šç§ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„PPOå’ŒGRPOæ–¹æ³•ï¼Œå±•ç¤ºäº†ä»·å€¼æ¨¡å‹çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åœ¨ç°å®ç¯å¢ƒä¸­å¸¸å¸¸å—åˆ°å™ªå£°æˆ–ä¸å®Œç¾å¥–åŠ±ç›‘ç£çš„å½±å“ï¼Œè¿™å‰Šå¼±äº†ç­–ç•¥çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¥–åŠ±å»å™ªæˆ–è¿‡æ»¤ä¸è‰¯æ•°æ®ï¼Œä½†å¾€å¾€å¿½è§†äº†ä»·å€¼æ¨¡å‹åœ¨ç­–ç•¥ä¼˜åŒ–ä¸­çš„å…³é”®ä½œç”¨ã€‚æœ¬æ–‡æå‡ºVRPOï¼Œä¸€ä¸ªä»¥ä»·å€¼ä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨å™ªå£°ç›‘ç£ä¸‹å®ç°ç¨³å¥çš„PPOè®­ç»ƒã€‚VRPOç»“åˆäº†ä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡ï¼šä¸€ä¸ªç”±å†»ç»“è¯­è¨€æ¨¡å‹å¼•å¯¼çš„è¾…åŠ©æŸå¤±ï¼Œä»¥åŠä¸€ä¸ªå˜åˆ†ä¿¡æ¯ç“¶é¢ˆã€‚è¿™äº›æœºåˆ¶å¢å¼ºäº†ä»·å€¼æ¨¡å‹è¿‡æ»¤å™ªå£°å’Œæ•æ‰å…³é”®å­—çš„èƒ½åŠ›ï¼Œä½¿å…¶ä»è¢«åŠ¨é¢„æµ‹è€…è½¬å˜ä¸ºå™ªå£°çš„ä¸»åŠ¨è°ƒèŠ‚è€…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVRPOåœ¨æ•°å­¦æ¨ç†ã€ç§‘å­¦é—®ç­”å’Œå¤šè½®å¯¹è¯ä»»åŠ¡ä¸­ï¼Œå‡ä¼˜äºPPOå’ŒGRPOåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å™ªå£°ç›‘ç£ä¸‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ï¼Œä»·å€¼æ¨¡å‹å¯¹ç­–ç•¥ä¼˜åŒ–çš„é‡è¦æ€§è¢«å¿½è§†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¥–åŠ±å»å™ªï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨ä»·å€¼æ¨¡å‹çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVRPOæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¢å¼ºä»·å€¼æ¨¡å‹çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°è¿‡æ»¤å™ªå£°å¹¶æ•æ‰å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæé«˜ä¼˜åŠ¿ä¼°è®¡çš„å¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVRPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯åŸºäºå†»ç»“è¯­è¨€æ¨¡å‹çš„è¾…åŠ©æŸå¤±ï¼ŒäºŒæ˜¯å˜åˆ†ä¿¡æ¯ç“¶é¢ˆã€‚è¿™ä¸¤ä¸ªæ¨¡å—å…±åŒä½œç”¨ï¼Œæå‡äº†ä»·å€¼æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†ä»·å€¼æ¨¡å‹è½¬å˜ä¸ºå™ªå£°çš„ä¸»åŠ¨è°ƒèŠ‚è€…ï¼Œè€Œä¸ä»…ä»…æ˜¯è¢«åŠ¨çš„é¢„æµ‹è€…ã€‚è¿™ä¸€è½¬å˜ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å™ªå£°ä¿¡å·ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè¾…åŠ©æŸå¤±é€šè¿‡ç†µå’Œå›°æƒ‘åº¦å¼•å¯¼ï¼Œç¡®ä¿ä»·å€¼æ¨¡å‹èƒ½å¤Ÿå…³æ³¨é‡è¦ä¿¡æ¯ã€‚åŒæ—¶ï¼Œå˜åˆ†ä¿¡æ¯ç“¶é¢ˆçš„å¼•å…¥ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä¿¡æ¯æ—¶æ›´åŠ ç¨³å¥ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVRPOåœ¨æ•°å­¦æ¨ç†ã€ç§‘å­¦é—®ç­”å’Œå¤šè½®å¯¹è¯ä»»åŠ¡ä¸­ï¼Œå‡æ˜¾è‘—ä¼˜äºPPOå’ŒGRPOåŸºçº¿ï¼Œæå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šã€‚è¿™è¡¨æ˜ä»·å€¼æ¨¡å‹åœ¨RLHFä¸­çš„é‡è¦æ€§ä¸å®¹å¿½è§†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„å­¦ä¹ èƒ½åŠ›ï¼ŒVRPOèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´ç¨³å®šå’Œå¯é çš„ç­–ç•¥ä¼˜åŒ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning from Human Feedback (RLHF) often suffers from noisy or imperfect reward supervision in real-world settings, which undermines policy stability and generalization. Such noise may cause models to lose attention on key words during advantage estimation. While prior work focuses on reward denoising or filtering poor data, it often overlooks the critical role of the value model in policy optimization. In this work, we show that a strong value model is essential for mitigating noise by absorbing unstable signals and enabling more reliable advantage estimation. We propose VRPO, a value-centric framework for robust PPO training under noisy supervision. VRPO combines two core designs: (1) an auxiliary loss guided by entropy and perplexity from a frozen language model, and (2) a variational information bottleneck. These mechanisms enhance the value model's ability to filter out noise and capture key words from the context during advantage estimation, transforming it from a passive predictor into an active regulator of noise. Experiments on math reasoning, science QA, and multi-turn dialogue, under both rule-based and model-based noisy rewards, show that VRPO consistently outperforms PPO and GRPO baselines. Our findings underscore the often-overlooked importance of the value model in RLHF and offer a principled and practical approach to robust policy optimization in noisy real-world environments.

