---
layout: default
title: Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning
---

# Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03501" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03501v2</a>
  <a href="https://arxiv.org/pdf/2508.03501.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03501v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03501v2', 'Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexander Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Sergey Abramov, Andrei Andriushchenko, Filipp Fisin, Sergei Skvortsov, Boris Yangel

**åˆ†ç±»**: cs.LG, cs.CL, cs.SE

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-10-10)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è§£å†³è½¯ä»¶å·¥ç¨‹ä¸­çš„å¤šè½®äº¤äº’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤šè½®äº¤äº’` `è½¯ä»¶å·¥ç¨‹` `æ‹’ç»å¾®è°ƒ` `æ‰§è¡Œåé¦ˆ` `åŠ¨æ€ç­–ç•¥ä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¤šé›†ä¸­äºå•è½®äº¤äº’ï¼Œç¼ºä¹å¯¹å¤šè½®çŠ¶æ€åé¦ˆçš„å¤„ç†ï¼Œå¯¼è‡´åœ¨è½¯ä»¶å·¥ç¨‹ç­‰å¤æ‚é¢†åŸŸçš„åº”ç”¨æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡æ‹’ç»å¾®è°ƒå’ŒåŒæ­¥å¼ºåŒ–å­¦ä¹ ç®¡é“ï¼Œåˆ©ç”¨æ‰§è¡Œåé¦ˆæ¥è®­ç»ƒèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šè½®äº¤äº’çš„ä»£ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨SWE-benchå’ŒSWE-rebenchä¸Šè¾¾åˆ°äº†ç«äº‰æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ä¸­ï¼Œä¸»è¦é›†ä¸­äºå•è½®é—®é¢˜ï¼Œå¦‚æ•°å­¦æ¨ç†æˆ–å•æ¬¡ä»£ç ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›é—®é¢˜å¯ä»¥è§†ä¸ºå¤šè½®é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„ç®€åŒ–æƒ…å†µï¼Œä¸è½¯ä»¶å·¥ç¨‹ç­‰çœŸå®ä¸–ç•Œé¢†åŸŸçš„å¤šè½®äº¤äº’éœ€æ±‚ç›¸æ‚–ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å±•ç¤ºäº†RLåœ¨è¿™ä¸€å¹¿æ³›åœºæ™¯ä¸­çš„æˆåŠŸåº”ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡æ‰§è¡Œåé¦ˆè¿›è¡Œæ‹’ç»å¾®è°ƒï¼ˆRFTï¼‰ï¼Œè®­ç»ƒç­–ç•¥æœ‰æ•ˆåœ°éµå¾ªæŒ‡ä»¤å’Œæ ¼å¼ï¼Œéšåé‡‡ç”¨åŒæ­¥RLç®¡é“è¿›è¡Œè¿­ä»£æ”¹è¿›ã€‚å°†è¯¥ç®¡é“åº”ç”¨äºQwen2.5-72B-Instructï¼Œæˆ‘ä»¬åœ¨SWE-bench VerifiedåŸºå‡†ä¸Šçš„Pass@1ä»11%æå‡è‡³39%ï¼Œæ˜¾è‘—ä¼˜äº20%çš„RFTåŸºçº¿ã€‚æœ€ç»ˆä»£ç†åœ¨SWE-rebenchçš„Mayå’ŒJuneåˆ†å‰²ä¸Šåˆ†åˆ«è¾¾åˆ°äº†35%å’Œ31%çš„Pass@1ï¼Œè¡¨ç°å‡ºè‰²ï¼Œç”šè‡³ä¸æ›´å¤§æ¨¡å‹å¦‚DeepSeek-V3-0324æˆ–Qwen3-235B-A22Bç›¸ç«äº‰ï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸ºè®­ç»ƒå¤šè½®äº¤äº’ä»»åŠ¡çš„èƒ½åŠ›ä»£ç†æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸå¤šè½®äº¤äº’ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹æœ‰æ•ˆçš„çŠ¶æ€åé¦ˆæœºåˆ¶ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºå•è½®é—®é¢˜ï¼Œæ— æ³•é€‚åº”å¤æ‚çš„å¤šè½®äº¤äº’åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ‹’ç»å¾®è°ƒï¼ˆRFTï¼‰ç»“åˆæ‰§è¡Œåé¦ˆï¼Œè®­ç»ƒæ¨¡å‹éµå¾ªæŒ‡ä»¤å’Œæ ¼å¼ï¼Œéšåä½¿ç”¨åŒæ­¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®¡é“è¿›è¡Œè¿­ä»£æ”¹è¿›ï¼Œä»¥æå‡æ¨¡å‹åœ¨å¤šè½®äº¤äº’ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯æ‹’ç»å¾®è°ƒï¼Œåˆ©ç”¨æ‰§è¡Œåé¦ˆä¼˜åŒ–ç­–ç•¥ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯åŒæ­¥RLç®¡é“ï¼Œé‡‡ç”¨DAPOï¼ˆDynamic Action Policy Optimizationï¼‰è¿›è¡ŒæŒç»­æ”¹è¿›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ æœ‰æ•ˆåº”ç”¨äºå¤šè½®äº¤äº’åœºæ™¯ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ‰§è¡Œåé¦ˆçš„æ‹’ç»å¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œæ ¼å¼åŒ–æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æŒ‡ä»¤éµå¾ªçš„å‡†ç¡®æ€§ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè°ƒæ•´ï¼Œä»¥é€‚åº”å¤šè½®äº¤äº’çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨SWE-bench VerifiedåŸºå‡†ä¸Šçš„Pass@1ä»11%æå‡è‡³39%ï¼Œåœ¨SWE-rebenchçš„Mayå’ŒJuneåˆ†å‰²ä¸Šåˆ†åˆ«è¾¾åˆ°äº†35%å’Œ31%çš„Pass@1ï¼Œè¡¨ç°ä¼˜äº20%çš„RFTåŸºçº¿ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å·¥ç¨‹ä¸­çš„è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆã€è°ƒè¯•å’Œä¼˜åŒ–ç­‰ä»»åŠ¡ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å¤šè½®äº¤äº’ä¸­çš„è¡¨ç°ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å¼€å‘æ•ˆç‡ï¼Œé™ä½äººåŠ›æˆæœ¬ï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Research on applications of reinforcement learning (RL) to large language models has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn Markov decision processes (MDPs), this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation. To bridge this gap, we demonstrate the successful application of RL to this general regime. Our methodology begins with rejection fine-tuning (RFT) using execution feedback to train a policy to follow instructions and formatting effectively, followed by a synchronous RL pipeline using DAPO for iterative improvement. Applying this pipeline to Qwen2.5-72B-Instruct, we increase its Pass@1 on the SWE-bench Verified benchmark from 11% to 39%, substantially improving upon the 20% RFT baseline. On the May and June splits of SWE-rebench, the resulting agent achieves Pass@1 of 35% and 31% respectively, competitive with even larger models such as DeepSeek-V3-0324 or Qwen3-235B-A22B, demonstrating that our methodology offers a practical approach for training capable agents for multi-turn interactive tasks using open-weight models.

