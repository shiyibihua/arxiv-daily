---
layout: default
title: MoKA: Mixture of Kronecker Adapters
---

# MoKA: Mixture of Kronecker Adapters

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03527" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03527v1</a>
  <a href="https://arxiv.org/pdf/2508.03527.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03527v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03527v1', 'MoKA: Mixture of Kronecker Adapters')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammadreza Sadeghi, Mahsa Ghazvini Nejad, MirHamed Jafarzadeh Asl, Yu Gu, Yuanhao Yu, Masoud Asgharian, Vahid Partovi Nia

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoKAä»¥è§£å†³ä½ç§©é€‚é…å™¨è¡¨è¾¾èƒ½åŠ›ä¸è¶³çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å…‹ç½—å†…å…‹é€‚é…å™¨` `é—¨æ§æœºåˆ¶` `ä½ç§©é€‚é…å™¨` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å‹ç¼©` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä½ç§©é€‚é…å™¨åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šå—åˆ°ç§©çº¦æŸçš„é™åˆ¶ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å¤æ‚ä»»åŠ¡ã€‚
2. æœ¬æ–‡æå‡ºçš„MoKAé€šè¿‡æ··åˆå…‹ç½—å†…å…‹ç§¯å»ºæ¨¡æƒé‡æ›´æ–°ï¼Œç»“åˆé—¨æ§æœºåˆ¶æå‡é€‚åº”æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMoKAåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶ŠPEFTåŸºçº¿ï¼Œå‚æ•°å¯è®­ç»ƒæ€§å‡å°‘27å€ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å¯¹äºé™ä½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®¡ç®—å¼€é”€è‡³å…³é‡è¦ã€‚ä½ç§©é€‚é…å™¨è™½ç„¶èƒ½æœ‰æ•ˆæ§åˆ¶å‚æ•°è§„æ¨¡ï¼Œä½†ç”±äºç§©çº¦æŸï¼Œå…¶è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œå½±å“å¤æ‚ä»»åŠ¡çš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†æ··åˆå…‹ç½—å†…å…‹é€‚é…å™¨ï¼ˆMoKAï¼‰ï¼Œé€šè¿‡å°†æƒé‡æ›´æ–°å»ºæ¨¡ä¸ºå…‹ç½—å†…å…‹ç§¯çš„æ··åˆï¼Œå…‹æœäº†è¿™ä¸€é™åˆ¶ã€‚MoKAé‡‡ç”¨é—¨æ§æœºåˆ¶è¯„ä¼°æ¯ä¸ªå…‹ç½—å†…å…‹å› å­çš„ç›¸å¯¹é‡è¦æ€§ï¼Œå¢å¼ºäº†é€‚åº”æ€§ã€‚åŒæ—¶ï¼ŒMoKAå®ç°äº†ç§©çµæ´»æ€§ï¼Œåœ¨å‚æ•°æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´æä¾›äº†æ›´å¥½çš„å¹³è¡¡ã€‚é€šè¿‡æ ‡å‡†çŸ©é˜µè¿ç®—é‡æ„å…‹ç½—å†…å…‹è®¡ç®—ï¼Œç¡®ä¿äº†ç¡¬ä»¶æ•ˆç‡ï¼Œä¾¿äºåœ¨GPUä¼˜åŒ–ç¡¬ä»¶ä¸Šéƒ¨ç½²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoKAåœ¨æŒ‡ä»¤è°ƒä¼˜å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸­è¶…è¶Šäº†PEFTåŸºçº¿ï¼Œå¹¶å°†å¯è®­ç»ƒå‚æ•°å‡å°‘äº†å¤šè¾¾27å€ï¼Œè¾¾åˆ°äº†æ€§èƒ½ä¸å‚æ•°æ•ˆç‡çš„æœ€ä½³å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä½ç§©é€‚é…å™¨åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨è¾¾èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç”±äºç§©çº¦æŸï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ¨¡å‹çš„æ½œåŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoKAé€šè¿‡å°†æƒé‡æ›´æ–°å»ºæ¨¡ä¸ºå…‹ç½—å†…å…‹ç§¯çš„æ··åˆï¼Œç»“åˆé—¨æ§æœºåˆ¶æ¥è¯„ä¼°å„ä¸ªå…‹ç½—å†…å…‹å› å­çš„ç›¸å¯¹é‡è¦æ€§ï¼Œä»è€Œå¢å¼ºé€‚åº”æ€§å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoKAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æƒé‡æ›´æ–°çš„æ··åˆå»ºæ¨¡ã€é—¨æ§æœºåˆ¶å’Œæ ‡å‡†çŸ©é˜µè¿ç®—çš„é‡æ„ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬å…‹ç½—å†…å…‹é€‚é…å™¨ã€é—¨æ§ç½‘ç»œå’Œä¼˜åŒ–ç®—æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoKAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†æ··åˆå…‹ç½—å†…å…‹ç§¯çš„æ¦‚å¿µï¼Œæ‰“ç ´äº†ä¼ ç»Ÿä½ç§©é€‚é…å™¨çš„é™åˆ¶ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒMoKAé‡‡ç”¨äº†çµæ´»çš„ç§©è®¾ç½®å’Œé—¨æ§æœºåˆ¶ï¼Œç¡®ä¿äº†æ¯ä¸ªå…‹ç½—å†…å…‹å› å­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ ‡å‡†çŸ©é˜µè¿ç®—çš„é‡æ„ï¼Œæå‡äº†è®¡ç®—æ•ˆç‡ï¼Œä¾¿äºåœ¨GPUä¸Šé«˜æ•ˆè¿è¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒMoKAåœ¨æŒ‡ä»¤è°ƒä¼˜å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„PEFTåŸºçº¿ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚å…·ä½“è€Œè¨€ï¼ŒMoKAå°†å¯è®­ç»ƒå‚æ•°å‡å°‘äº†å¤šè¾¾27å€ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå±•ç°äº†å…¶åœ¨å‚æ•°æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´çš„ä¼˜è¶Šå¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MoKAçš„ç ”ç©¶æˆæœåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„å‚æ•°åˆ©ç”¨å’Œå‡ºè‰²çš„æ€§èƒ½è¡¨ç°ï¼Œèƒ½å¤Ÿä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®é™…éƒ¨ç½²æä¾›æ›´ä¸ºçµæ´»å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Parameter-efficient fine-tuning (PEFT) is essential for reducing the computational overhead of large language models (LLMs). Low-rank family adapters are commonly used to control the parameter size efficiently while maintaining the generative power of LLMs. However, their limited expressiveness due to the rank constraint often restricts their performance on complex tasks. We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker adapters that addresses this limitation by modeling weight updates as a mixture of Kronecker products. Our proposed adapter leverages a gating mechanism that measures the importance of each Kronecker factor, enabling more expressive adaptation. Moreover, MoKA enables a rank flexibility that provides a better trade-off between parameter efficiency and accuracy. To ensure hardware efficiency, we reformulate Kronecker computations using standard matrix operations, allowing seamless deployment on GPU-optimized hardware. We conduct extensive experiments on instruction-tuning and commonsense reasoning tasks using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not only outperforms PEFT baselines, but also reduces the number of trainable parameters up to 27x, achieving state-of-the-art trade-offs between performance and parameter efficiency.

