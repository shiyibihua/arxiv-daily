---
layout: default
title: Reinforcement Learning in MDPs with Information-Ordered Policies
---

# Reinforcement Learning in MDPs with Information-Ordered Policies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03904" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03904v1</a>
  <a href="https://arxiv.org/pdf/2508.03904.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03904v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03904v1', 'Reinforcement Learning in MDPs with Information-Ordered Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhongjun Zhang, Shipra Agrawal, Ilan Lobel, Sean R. Sinclair, Christina Lee Yu

**åˆ†ç±»**: stat.ML, cs.LG, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

**å¤‡æ³¨**: 57 pages, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¿¡æ¯æœ‰åºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä»¥ä¼˜åŒ–MDPs**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `éƒ¨åˆ†é¡ºåº` `è¿ç­¹å­¦` `åº“å­˜æ§åˆ¶` `æ’é˜Ÿç³»ç»Ÿ` `åäº‹å®æ¨æ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†æ— é™æ—¶é—´èŒƒå›´çš„MDPsæ—¶ï¼Œå¾€å¾€é¢ä¸´é«˜ç»´çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å¸¦æ¥çš„æ€§èƒ½ç“¶é¢ˆã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºéƒ¨åˆ†é¡ºåºçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œåˆ©ç”¨ç­–ç•¥é—´çš„å¯æ¯”è¾ƒæ€§æ¥å‡å°‘ç¯å¢ƒäº¤äº’ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚
3. é€šè¿‡åœ¨å¤šä¸ªè¿ç­¹å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œç®—æ³•å±•ç¤ºäº†æ–°çš„ç†è®ºä¿è¯å’Œå®è¯ç»“æœï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œä¸”ä¸ä¾èµ–äºé¢å¤–å‡è®¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‘¨æœŸçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé’ˆå¯¹æ— é™æ—¶é—´èŒƒå›´å†…çš„å¹³å‡æˆæœ¬é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPsï¼‰ï¼Œåˆ©ç”¨ç­–ç•¥ç±»ä¸Šçš„éƒ¨åˆ†é¡ºåºç»“æ„ã€‚åœ¨è¯¥ç»“æ„ä¸­ï¼Œå¦‚æœåœ¨ç­–ç•¥Ï€ä¸‹æ”¶é›†çš„æ•°æ®å¯ä»¥ç”¨äºä¼°è®¡ç­–ç•¥Ï€'çš„æ€§èƒ½ï¼Œåˆ™æœ‰Ï€' â‰¤ Ï€ï¼Œä»è€Œå®ç°äº†åœ¨ä¸å¢åŠ ç¯å¢ƒäº¤äº’çš„æƒ…å†µä¸‹è¿›è¡Œåäº‹å®æ¨æ–­ã€‚åŸºäºè¿™ä¸€éƒ¨åˆ†é¡ºåºï¼Œç®—æ³•çš„é—æ†¾ç•Œé™ä¸ºO(âˆš(w log(\|Î˜\|) T))ï¼Œå…¶ä¸­wä¸ºéƒ¨åˆ†é¡ºåºçš„å®½åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥ç•Œé™ä¸çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„å¤§å°æ— å…³ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™äº›éƒ¨åˆ†é¡ºåºåœ¨è¿ç­¹å­¦å¤šä¸ªé¢†åŸŸçš„é€‚ç”¨æ€§ï¼ŒåŒ…æ‹¬åº“å­˜æ§åˆ¶å’Œæ’é˜Ÿç³»ç»Ÿï¼Œå¹¶åœ¨æ¯ä¸ªé¢†åŸŸåº”ç”¨äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå¾—åˆ°äº†æ–°çš„ç†è®ºä¿è¯å’Œå¼ºæœ‰åŠ›çš„å®è¯ç»“æœï¼Œè€Œæ— éœ€å¯¹åº“å­˜æ¨¡å‹çš„å‡¸æ€§æˆ–æ’é˜Ÿæ¨¡å‹çš„åˆ°è¾¾ç‡ç»“æ„æ–½åŠ é¢å¤–å‡è®¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æ— é™æ—¶é—´èŒƒå›´å†…çš„å¹³å‡æˆæœ¬é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPsï¼‰ä¸­ï¼Œç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é«˜ç»´çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä¸‹çš„æ€§èƒ½ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„ç¯å¢ƒäº¤äº’ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„ç®—æ³•åˆ©ç”¨ç­–ç•¥ç±»ä¸Šçš„éƒ¨åˆ†é¡ºåºç»“æ„ï¼Œä½¿å¾—åœ¨æŸä¸€ç­–ç•¥ä¸‹æ”¶é›†çš„æ•°æ®å¯ä»¥ç”¨äºä¼°è®¡å…¶ä»–ç­–ç•¥çš„æ€§èƒ½ï¼Œä»è€Œå®ç°åäº‹å®æ¨æ–­ï¼Œå‡å°‘äº†å¯¹ç¯å¢ƒäº¤äº’çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç®—æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ›´æ–°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’æ”¶é›†æ•°æ®ï¼›ç„¶åï¼Œåˆ©ç”¨éƒ¨åˆ†é¡ºåºç»“æ„è¿›è¡Œç­–ç•¥æ€§èƒ½çš„ä¼°è®¡ï¼›æœ€åï¼Œæ ¹æ®ä¼°è®¡ç»“æœæ›´æ–°ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†éƒ¨åˆ†é¡ºåºçš„æ¦‚å¿µï¼Œä½¿å¾—ä¸åŒç­–ç•¥ä¹‹é—´çš„æ€§èƒ½å¯ä»¥è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œå®ç°äº†æ›´é«˜æ•ˆçš„å­¦ä¹ å’Œå†³ç­–è¿‡ç¨‹ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†å¯¹ç¯å¢ƒäº¤äº’çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬éƒ¨åˆ†é¡ºåºçš„å®½åº¦wï¼Œä»¥åŠç­–ç•¥è¯„ä¼°æ—¶ä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€‚ç®—æ³•ä¸è¦æ±‚çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„ç‰¹å®šç»“æ„ï¼Œå¢å¼ºäº†å…¶é€‚ç”¨æ€§å’Œçµæ´»æ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸­å¯èƒ½æœ‰æ‰€ä¸åŒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®—æ³•åœ¨å¤šä¸ªè¿ç­¹å­¦é—®é¢˜ä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œé—æ†¾ç•Œé™ä¸ºO(âˆš(w log(\|Î˜\|) T))ï¼Œä¸”ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼Œå°¤å…¶åœ¨é«˜ç»´çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä¸‹è¡¨ç°å‡ºè‰²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åº“å­˜æ§åˆ¶ã€æ’é˜Ÿç³»ç»Ÿç­‰è¿ç­¹å­¦ç›¸å…³é—®é¢˜ã€‚é€šè¿‡ä¼˜åŒ–å†³ç­–è¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨å®é™…åœºæ™¯ä¸­æ˜¾è‘—æå‡èµ„æºåˆ©ç”¨æ•ˆç‡å’Œé™ä½æˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose an epoch-based reinforcement learning algorithm for infinite-horizon average-cost Markov decision processes (MDPs) that leverages a partial order over a policy class. In this structure, $Ï€' \leq Ï€$ if data collected under $Ï€$ can be used to estimate the performance of $Ï€'$, enabling counterfactual inference without additional environment interaction. Leveraging this partial order, we show that our algorithm achieves a regret bound of $O(\sqrt{w \log(\|Î˜\|) T})$, where $w$ is the width of the partial order. Notably, the bound is independent of the state and action space sizes. We illustrate the applicability of these partial orders in many domains in operations research, including inventory control and queuing systems. For each, we apply our framework to that problem, yielding new theoretical guarantees and strong empirical results without imposing extra assumptions such as convexity in the inventory model or specialized arrival-rate structure in the queuing model.

