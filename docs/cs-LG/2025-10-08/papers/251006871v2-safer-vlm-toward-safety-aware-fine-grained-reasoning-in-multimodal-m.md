---
layout: default
title: SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models
---

# SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.06871" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.06871v2</a>
  <a href="https://arxiv.org/pdf/2510.06871.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06871v2" onclick="toggleFavorite(this, '2510.06871v2', 'SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huahui Yi, Kun Wang, Qiankun Li, Miao Yu, Liang Lin, Gongli Xi, Hao Wu, Xuming Hu, Kang Li, Yang Liu

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08 (æ›´æ–°: 2025-10-09)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HarveyYi/SaFeR-VLM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SaFeR-VLMï¼šé¢å‘å®‰å…¨çš„å¤šæ¨¡æ€æ¨¡å‹ç»†ç²’åº¦æ¨ç†æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `å®‰å…¨æ€§` `å¼ºåŒ–å­¦ä¹ ` `å®‰å…¨å¯¹é½` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLRMsåœ¨å®‰å…¨æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ï¼Œä¸”ç¼ºä¹å¯¹æ¨ç†è¿‡ç¨‹çš„çº¦æŸã€‚
2. SaFeR-VLMé€šè¿‡å®‰å…¨å¯¹é½çš„å¼ºåŒ–å­¦ä¹ ï¼Œå°†å®‰å…¨æ€§åµŒå…¥å¤šæ¨¡æ€æ¨ç†ï¼Œä¸»åŠ¨é©±åŠ¨å®‰å…¨æ¨ç†ã€‚
3. SaFeR-VLMåœ¨å®‰å…¨æ€§å’Œhelpfulnessä¸Šè¶…è¶Šäº†åŒç­‰è§„æ¨¡å’Œæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œä¸”ä»£ç å·²å¼€æºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹æ¨ç†æ¨¡å‹(MLRMs)åœ¨è·¨æ¨¡æ€æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸åœ¨å¯¹æŠ—æ€§æˆ–ä¸å®‰å…¨æç¤ºä¸‹æ”¾å¤§å®‰å…¨é£é™©ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ¨ç†ç¨â€ã€‚ç°æœ‰çš„é˜²å¾¡æªæ–½ä¸»è¦åœ¨è¾“å‡ºå±‚é¢èµ·ä½œç”¨ï¼Œæ²¡æœ‰çº¦æŸæ¨ç†è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹æš´éœ²äºéšæ€§é£é™©ã€‚æœ¬æ–‡æå‡ºäº†SaFeR-VLMï¼Œä¸€ä¸ªå®‰å…¨å¯¹é½çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå°†å®‰å…¨æ€§ç›´æ¥åµŒå…¥åˆ°å¤šæ¨¡æ€æ¨ç†ä¸­ã€‚è¯¥æ¡†æ¶é›†æˆäº†å››ä¸ªç»„æˆéƒ¨åˆ†ï¼š(I) QI-Safe-10Kï¼Œä¸€ä¸ªå¼ºè°ƒå®‰å…¨å…³é”®å’Œæ¨ç†æ•æ„Ÿæ¡ˆä¾‹çš„ç²¾é€‰æ•°æ®é›†ï¼›(II) å®‰å…¨æ„ŸçŸ¥rolloutï¼Œå…¶ä¸­ä¸å®‰å…¨çš„ç”Ÿæˆç»å†åæ€å’Œçº æ­£ï¼Œè€Œä¸æ˜¯è¢«ä¸¢å¼ƒï¼›(III) ç»“æ„åŒ–å¥–åŠ±å»ºæ¨¡ï¼Œå…·æœ‰å¤šç»´åŠ æƒæ ‡å‡†å’Œå¯¹å¹»è§‰å’ŒçŸ›ç›¾çš„æ˜¾å¼æƒ©ç½šï¼›(IV) GRPOä¼˜åŒ–ï¼Œå®ƒå¼ºåŒ–å®‰å…¨å’Œçº æ­£çš„è½¨è¿¹ã€‚è¿™ç§ç»Ÿä¸€çš„è®¾è®¡å°†å®‰å…¨æ€§ä»è¢«åŠ¨ä¿éšœè½¬å˜ä¸ºæ¨ç†çš„ä¸»åŠ¨é©±åŠ¨åŠ›ï¼Œä»è€Œå®ç°å¯æ‰©å±•å’Œå¯æ³›åŒ–çš„å®‰å…¨æ„ŸçŸ¥æ¨ç†ã€‚SaFeR-VLMè¿›ä¸€æ­¥å±•ç¤ºäº†å¯¹æ˜¾æ€§å’Œéšæ€§é£é™©çš„é²æ£’æ€§ï¼Œæ”¯æŒè¶…è¶Šè¡¨é¢è¿‡æ»¤çš„åŠ¨æ€å’Œå¯è§£é‡Šçš„å®‰å…¨å†³ç­–ã€‚SaFeR-VLM-3Båœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®‰å…¨æ€§å’Œæœ‰ç”¨æ€§çš„å¹³å‡æ€§èƒ½åˆ†åˆ«è¾¾åˆ°70.13å’Œ78.97ï¼Œè¶…è¿‡äº†åŒç­‰è§„æ¨¡å’Œå¤§äº10å€è§„æ¨¡çš„æ¨¡å‹ï¼Œå¦‚Skywork-R1V3-38Bã€Qwen2.5VL-72Bå’ŒGLM4.5V-106Bã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSaFeR-VLM-7Bå—ç›Šäºå…¶è§„æ¨¡çš„å¢åŠ ï¼Œåœ¨å®‰å…¨æŒ‡æ ‡ä¸Šåˆ†åˆ«è¶…è¿‡GPT-5-miniå’ŒGemini-2.5-Flash 6.47å’Œ16.76ä¸ªç‚¹ï¼Œå¹¶ä¸”åœ¨helpfulnessæ€§èƒ½ä¸Šæ²¡æœ‰ä»»ä½•ä¸‹é™ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨https://github.com/HarveyYi/SaFeR-VLMè·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆMLRMsï¼‰åœ¨é¢å¯¹å¯¹æŠ—æ€§æˆ–ä¸å®‰å…¨æç¤ºæ—¶ï¼Œå®¹æ˜“äº§ç”Ÿä¸å®‰å…¨è¾“å‡ºçš„é—®é¢˜ï¼Œå³æ‰€è°“çš„â€œæ¨ç†ç¨â€ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è¾“å‡ºå±‚é¢çš„è¿‡æ»¤ï¼Œç¼ºä¹å¯¹æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„çº¦æŸï¼Œå¯¼è‡´æ¨¡å‹å®¹æ˜“å—åˆ°éšæ€§é£é™©çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å®‰å…¨æ€§èå…¥åˆ°æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè€Œä¸æ˜¯ä»…ä»…åœ¨è¾“å‡ºå±‚é¢è¿›è¡Œè¿‡æ»¤ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿè¯†åˆ«å¹¶çº æ­£ä¸å®‰å…¨çš„è¡Œä¸ºï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“å®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSaFeR-VLMæ¡†æ¶åŒ…å«å››ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š1) QI-Safe-10Kæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹çš„å®‰å…¨æ€§ï¼›2) å®‰å…¨æ„ŸçŸ¥rolloutï¼Œå…è®¸æ¨¡å‹åœ¨ç”Ÿæˆä¸å®‰å…¨å†…å®¹åè¿›è¡Œåæ€å’Œçº æ­£ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¸¢å¼ƒï¼›3) ç»“æ„åŒ–å¥–åŠ±å»ºæ¨¡ï¼Œä½¿ç”¨å¤šç»´åŠ æƒæ ‡å‡†ï¼Œå¹¶å¯¹å¹»è§‰å’ŒçŸ›ç›¾è¿›è¡Œæ˜¾å¼æƒ©ç½šï¼›4) GRPOä¼˜åŒ–ï¼Œå¼ºåŒ–å®‰å…¨å’Œçº æ­£åçš„è½¨è¿¹ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆå®‰å…¨çš„å†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šSaFeR-VLMçš„å…³é”®åˆ›æ–°åœ¨äºå°†å®‰å…¨æ€§ä»è¢«åŠ¨çš„é˜²å¾¡è½¬å˜ä¸ºä¸»åŠ¨çš„é©±åŠ¨åŠ›ã€‚é€šè¿‡å®‰å…¨æ„ŸçŸ¥rolloutå’Œç»“æ„åŒ–å¥–åŠ±å»ºæ¨¡ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å­¦ä¹ å®‰å…¨ç­–ç•¥ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼ŒGRPOä¼˜åŒ–è¿›ä¸€æ­¥å¼ºåŒ–äº†æ¨¡å‹çš„å®‰å…¨è¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šQI-Safe-10Kæ•°æ®é›†åŒ…å«å®‰å…¨å…³é”®å’Œæ¨ç†æ•æ„Ÿçš„æ¡ˆä¾‹ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹çš„å®‰å…¨æ„è¯†ã€‚å®‰å…¨æ„ŸçŸ¥rolloutå…è®¸æ¨¡å‹åœ¨ç”Ÿæˆä¸å®‰å…¨å†…å®¹åè¿›è¡Œåæ€å’Œçº æ­£ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¸¢å¼ƒã€‚ç»“æ„åŒ–å¥–åŠ±å»ºæ¨¡ä½¿ç”¨å¤šç»´åŠ æƒæ ‡å‡†ï¼Œå¹¶å¯¹å¹»è§‰å’ŒçŸ›ç›¾è¿›è¡Œæ˜¾å¼æƒ©ç½šï¼Œä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚GRPOä¼˜åŒ–ä½¿ç”¨Proximal Policy Optimization (PPO) çš„å˜ä½“ï¼Œå¼ºåŒ–å®‰å…¨å’Œçº æ­£åçš„è½¨è¿¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SaFeR-VLM-3Båœ¨å®‰å…¨æ€§å’Œhelpfulnessä¸Šè¶…è¶Šäº†åŒç­‰è§„æ¨¡å’Œå¤§äº10å€è§„æ¨¡çš„æ¨¡å‹ï¼Œå¦‚Skywork-R1V3-38Bã€Qwen2.5VL-72Bå’ŒGLM4.5V-106Bã€‚SaFeR-VLM-7Båœ¨å®‰å…¨æŒ‡æ ‡ä¸Šåˆ†åˆ«è¶…è¿‡GPT-5-miniå’ŒGemini-2.5-Flash 6.47å’Œ16.76ä¸ªç‚¹ï¼Œä¸”helpfulnessæ€§èƒ½æ²¡æœ‰ä¸‹é™ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SaFeR-VLMå¯åº”ç”¨äºå„ç§éœ€è¦å®‰å…¨ä¿éšœçš„å¤šæ¨¡æ€åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹åœ¨å¤æ‚æ¨ç†è¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§ï¼Œå¯ä»¥é™ä½æ½œåœ¨é£é™©ï¼Œæå‡ç”¨æˆ·ä¿¡ä»»åº¦ï¼Œå¹¶ä¿ƒè¿›å¤šæ¨¡æ€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal reasoning but often amplify safety risks under adversarial or unsafe prompts, a phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at the output level and do not constrain the reasoning process, leaving models exposed to implicit risks. In this paper, we propose SaFeR-VLM, a safety-aligned reinforcement learning framework that embeds safety directly into multimodal reasoning. The framework integrates four components: (I) QI-Safe-10K, a curated dataset emphasizing safety-critical and reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations undergo reflection and correction instead of being discarded; (III) structured reward modeling with multi-dimensional weighted criteria and explicit penalties for hallucinations and contradictions; and (IV) GRPO optimization, which reinforces both safe and corrected trajectories. This unified design shifts safety from a passive safeguard to an active driver of reasoning, enabling scalable and generalizable safety-aware reasoning. SaFeR-VLM further demonstrates robustness against both explicit and implicit risks, supporting dynamic and interpretable safety decisions beyond surface-level filtering. SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and helpfulness across six benchmarks, surpassing both same-scale and $>10\times$ larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B. Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points respectively on safety metrics, achieving this improvement without any degradation in helpfulness performance. Our codes are available at https://github.com/HarveyYi/SaFeR-VLM.

