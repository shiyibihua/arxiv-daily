---
layout: default
title: ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL
---

# ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07151" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07151v1</a>
  <a href="https://arxiv.org/pdf/2510.07151.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07151v1" onclick="toggleFavorite(this, '2510.07151v1', 'ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Egor Cherepanov, Alexey K. Kovalev, Aleksandr I. Panov

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08

**å¤‡æ³¨**: 22 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ELMURï¼šåˆ©ç”¨å¤–éƒ¨å±‚è®°å¿†å’Œæ›´æ–°/é‡å†™æœºåˆ¶ï¼Œè§£å†³é•¿æ—¶ç¨‹å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `é•¿æ—¶ç¨‹å¼ºåŒ–å­¦ä¹ ` `å¤–éƒ¨è®°å¿†` `Transformer` `å±‚å±€éƒ¨è®°å¿†` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨é•¿æœŸå†å²ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨éƒ¨åˆ†å¯è§‚æµ‹å’Œé•¿æ—¶ç¨‹ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚
2. ELMURé€šè¿‡å¼•å…¥å±‚å±€éƒ¨å¤–éƒ¨è®°å¿†ï¼Œå¹¶ç»“åˆæ›´æ–°/é‡å†™æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹é•¿æœŸä¾èµ–å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒELMURåœ¨å¤šä¸ªå¤æ‚ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶åœ¨é•¿æ—¶ç¨‹å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºELMURï¼ˆExternal Layer Memory with Update/Rewriteï¼‰çš„Transformeræ¶æ„ï¼Œç”¨äºè§£å†³éƒ¨åˆ†å¯è§‚æµ‹å’Œé•¿æ—¶ç¨‹å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚ELMURé‡‡ç”¨ç»“æ„åŒ–çš„å¤–éƒ¨è®°å¿†ï¼Œæ¯ä¸€å±‚éƒ½ç»´æŠ¤è®°å¿†åµŒå…¥ï¼Œå¹¶é€šè¿‡åŒå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¸è®°å¿†åµŒå…¥äº¤äº’ï¼Œç„¶åä½¿ç”¨æœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼ˆLRUï¼‰çš„è®°å¿†æ¨¡å—ï¼Œé€šè¿‡æ›¿æ¢æˆ–å‡¸æ··åˆçš„æ–¹å¼æ›´æ–°è®°å¿†ã€‚ELMURèƒ½å¤Ÿå°†æœ‰æ•ˆæ—¶ç¨‹æ‰©å±•åˆ°æ³¨æ„åŠ›çª—å£çš„10ä¸‡å€ï¼Œå¹¶åœ¨ä¸€ä¸ªèµ°å»Šé•¿åº¦é«˜è¾¾ç™¾ä¸‡æ­¥çš„åˆæˆTå‹è¿·å®«ä»»åŠ¡ä¸­å®ç°äº†100%çš„æˆåŠŸç‡ã€‚åœ¨POPGymä¸­ï¼ŒELMURåœ¨è¶…è¿‡ä¸€åŠçš„ä»»åŠ¡ä¸­ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚åœ¨MIKASA-Roboç¨€ç–å¥–åŠ±çš„è§†è§‰æ“ä½œä»»åŠ¡ä¸­ï¼ŒELMURçš„æ€§èƒ½å‡ ä¹æ˜¯å¼ºå¤§åŸºçº¿æ¨¡å‹çš„ä¸¤å€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–çš„å±‚å±€éƒ¨å¤–éƒ¨è®°å¿†ä¸ºéƒ¨åˆ†å¯è§‚æµ‹ä¸‹çš„å†³ç­–æä¾›äº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°å®ä¸–ç•Œçš„æœºå™¨äººéœ€è¦åœ¨éƒ¨åˆ†å¯è§‚æµ‹å’Œé•¿æ—¶ç¨‹çš„ç¯å¢ƒä¸­è¡ŒåŠ¨ï¼Œå…³é”®çº¿ç´¢å¯èƒ½åœ¨å½±å“å†³ç­–ä¹‹å‰å¾ˆä¹…å°±å‡ºç°ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°ä»£æ–¹æ³•ä»…ä¾èµ–äºç¬æ—¶ä¿¡æ¯ï¼Œè€Œæ²¡æœ‰ç»“åˆè¿‡å»çš„ç»éªŒã€‚æ ‡å‡†çš„å¾ªç¯ç¥ç»ç½‘ç»œæˆ–Transformeræ¨¡å‹éš¾ä»¥ä¿ç•™å’Œåˆ©ç”¨é•¿æœŸä¾èµ–å…³ç³»ï¼šä¸Šä¸‹æ–‡çª—å£ä¼šæˆªæ–­å†å²ä¿¡æ¯ï¼Œè€Œç®€å•çš„è®°å¿†æ‰©å±•æ–¹æ³•åœ¨è§„æ¨¡å’Œç¨€ç–æ€§æ–¹é¢è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šELMURçš„æ ¸å¿ƒæ€è·¯æ˜¯ä¸ºTransformerçš„æ¯ä¸€å±‚å¼•å…¥ä¸€ä¸ªç»“æ„åŒ–çš„å¤–éƒ¨è®°å¿†æ¨¡å—ã€‚è¯¥æ¨¡å—å…è®¸æ¯ä¸€å±‚å­˜å‚¨å’Œæ£€ç´¢ä¸è¯¥å±‚ç›¸å…³çš„é•¿æœŸä¿¡æ¯ï¼Œä»è€Œç¼“è§£äº†ä¼ ç»ŸTransformerçš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ã€‚é€šè¿‡å±‚å±€éƒ¨è®°å¿†ï¼Œæ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°å­¦ä¹ å’Œåˆ©ç”¨ä¸åŒæ—¶é—´å°ºåº¦ä¸Šçš„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šELMURåŸºäºTransformeræ¶æ„ï¼Œå¹¶åœ¨æ¯ä¸€å±‚æ·»åŠ äº†å¤–éƒ¨è®°å¿†æ¨¡å—ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1) è¾“å…¥ç»è¿‡Transformerå±‚å¤„ç†ï¼›2) æ¯ä¸€å±‚é€šè¿‡åŒå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¸å¤–éƒ¨è®°å¿†äº¤äº’ï¼Œæå–ç›¸å…³ä¿¡æ¯ï¼›3) ä½¿ç”¨LRUè®°å¿†æ¨¡å—æ›´æ–°å¤–éƒ¨è®°å¿†ï¼Œå¯ä»¥é€‰æ‹©æ›¿æ¢æˆ–å‡¸æ··åˆçš„æ–¹å¼ã€‚è¿™ç§ç»“æ„ä½¿å¾—æ¯ä¸€å±‚éƒ½èƒ½ç»´æŠ¤è‡ªå·±çš„è®°å¿†è¡¨ç¤ºï¼Œå¹¶æ ¹æ®å½“å‰è¾“å…¥åŠ¨æ€æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šELMURçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»“æ„åŒ–çš„å±‚å±€éƒ¨å¤–éƒ¨è®°å¿†ã€‚ä¸å…¨å±€å…±äº«çš„å¤–éƒ¨è®°å¿†ç›¸æ¯”ï¼Œå±‚å±€éƒ¨è®°å¿†å…è®¸æ¯ä¸€å±‚å­¦ä¹ å’Œç»´æŠ¤ç‰¹å®šäºè¯¥å±‚çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†è®°å¿†çš„åˆ©ç”¨æ•ˆç‡å’Œæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä½¿ç”¨LRUæœºåˆ¶è¿›è¡Œè®°å¿†æ›´æ–°ï¼Œä¿è¯äº†è®°å¿†æ¨¡å—èƒ½å¤Ÿå­˜å‚¨æœ€è¿‘æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šELMURçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åŒå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºåœ¨æ¯ä¸€å±‚æå–ä¸å½“å‰è¾“å…¥ç›¸å…³çš„è®°å¿†ä¿¡æ¯ï¼›2) LRUè®°å¿†æ¨¡å—ï¼Œç”¨äºæ›´æ–°å¤–éƒ¨è®°å¿†ï¼Œå¯ä»¥é€‰æ‹©æ›¿æ¢æˆ–å‡¸æ··åˆçš„æ–¹å¼ï¼›3) æ¯ä¸€å±‚ç»´æŠ¤ç‹¬ç«‹çš„è®°å¿†åµŒå…¥ï¼Œå…è®¸æ¨¡å‹å­¦ä¹ ä¸åŒæ—¶é—´å°ºåº¦ä¸Šçš„ä¿¡æ¯ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæ˜ç¡®æåŠã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ELMURåœ¨åˆæˆTå‹è¿·å®«ä»»åŠ¡ä¸­ï¼Œèµ°å»Šé•¿åº¦é«˜è¾¾ç™¾ä¸‡æ­¥æ—¶ï¼Œå®ç°äº†100%çš„æˆåŠŸç‡ï¼Œæœ‰æ•ˆæ—¶ç¨‹æ‰©å±•åˆ°æ³¨æ„åŠ›çª—å£çš„10ä¸‡å€ã€‚åœ¨POPGymä¸­ï¼ŒELMURåœ¨è¶…è¿‡ä¸€åŠçš„ä»»åŠ¡ä¸­ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚åœ¨MIKASA-Roboç¨€ç–å¥–åŠ±çš„è§†è§‰æ“ä½œä»»åŠ¡ä¸­ï¼ŒELMURçš„æ€§èƒ½å‡ ä¹æ˜¯å¼ºå¤§åŸºçº¿æ¨¡å‹çš„ä¸¤å€ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒELMURåœ¨é•¿æ—¶ç¨‹å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ELMURé€‚ç”¨äºéœ€è¦é•¿æœŸè®°å¿†å’Œæ¨ç†çš„æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­å¯¼èˆªçš„æœºå™¨äººéœ€è¦è®°ä½ä¹‹å‰çš„è·¯å¾„å’Œé‡åˆ°çš„éšœç¢ç‰©ï¼›åœ¨æ¸¸æˆä¸­ï¼ŒAIéœ€è¦è®°ä½ç©å®¶çš„è¡Œä¸ºæ¨¡å¼å’Œæ¸¸æˆçŠ¶æ€ã€‚ELMURçš„å‡ºç°ï¼Œæœ‰æœ›æå‡è¿™äº›åº”ç”¨åœºæ™¯ä¸­æ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-world robotic agents must act under partial observability and long horizons, where key cues may appear long before they affect decision making. However, most modern approaches rely solely on instantaneous information, without incorporating insights from the past. Standard recurrent or transformer models struggle with retaining and leveraging long-term dependencies: context windows truncate history, while naive memory extensions fail under scale and sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a transformer architecture with structured external memory. Each layer maintains memory embeddings, interacts with them via bidirectional cross-attention, and updates them through an Least Recently Used (LRU) memory module using replacement or convex blending. ELMUR extends effective horizons up to 100,000 times beyond the attention window and achieves a 100% success rate on a synthetic T-Maze task with corridors up to one million steps. In POPGym, it outperforms baselines on more than half of the tasks. On MIKASA-Robo sparse-reward manipulation tasks with visual observations, it nearly doubles the performance of strong baselines. These results demonstrate that structured, layer-local external memory offers a simple and scalable approach to decision making under partial observability.

