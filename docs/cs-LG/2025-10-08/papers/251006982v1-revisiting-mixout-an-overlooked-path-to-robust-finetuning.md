---
layout: default
title: Revisiting Mixout: An Overlooked Path to Robust Finetuning
---

# Revisiting Mixout: An Overlooked Path to Robust Finetuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.06982" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.06982v1</a>
  <a href="https://arxiv.org/pdf/2510.06982.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06982v1" onclick="toggleFavorite(this, '2510.06982v1', 'Revisiting Mixout: An Overlooked Path to Robust Finetuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, Marco Pedersoli

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGMixoutï¼Œé€šè¿‡è‡ªé€‚åº”æƒé‡æ··åˆæå‡å¾®è°ƒæ¨¡å‹åœ¨åˆ†å¸ƒåç§»ä¸‹çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰åŸºç¡€æ¨¡å‹` `å¾®è°ƒ` `é²æ£’æ€§` `åˆ†å¸ƒåç§»` `Mixout` `æ­£åˆ™åŒ–` `æŒ‡æ•°ç§»åŠ¨å¹³å‡` `è‡ªé€‚åº”å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¾®è°ƒè§†è§‰æ¨¡å‹åœ¨æå‡ç‰¹å®šé¢†åŸŸæ€§èƒ½çš„åŒæ—¶ï¼Œå¾€å¾€ä¼šé™ä½æ¨¡å‹åœ¨æ•°æ®åˆ†å¸ƒå‘ç”Ÿå˜åŒ–æ—¶çš„é²æ£’æ€§ã€‚
2. GMixouté€šè¿‡åŠ¨æ€è°ƒæ•´æƒé‡æ··åˆæ¯”ä¾‹ï¼Œåˆ©ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡å¿«ç…§ä½œä¸ºé”šç‚¹ï¼Œå¹¶æ˜¾å¼æ§åˆ¶é‡é‡‡æ ·é¢‘ç‡ï¼Œä»è€Œæå‡æ¨¡å‹é²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGMixoutåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒäºç°æœ‰å¾®è°ƒæ–¹æ³•ï¼Œåœ¨åŸŸå†…ç²¾åº¦å’Œåˆ†å¸ƒåç§»é²æ£’æ€§ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒè§†è§‰åŸºç¡€æ¨¡å‹é€šå¸¸èƒ½æå‡åœ¨åŸŸç²¾åº¦ï¼Œä½†ä¼šç‰ºç‰²åˆ†å¸ƒåç§»ä¸‹çš„é²æ£’æ€§ã€‚æœ¬æ–‡é‡æ–°å®¡è§†Mixoutï¼Œä¸€ç§éšæœºæ­£åˆ™åŒ–æ–¹æ³•ï¼Œå®ƒé—´æ­‡æ€§åœ°ç”¨é¢„è®­ç»ƒæƒé‡æ›¿æ¢å¾®è°ƒæƒé‡ï¼Œå¹¶å°†å…¶è§†ä¸ºå•æ¬¡è¿è¡Œã€æƒé‡å…±äº«çš„éšå¼é›†æˆã€‚è¿™ç§è§†è§’æ­ç¤ºäº†æ§åˆ¶é²æ£’æ€§çš„ä¸‰ä¸ªå…³é”®å› ç´ ï¼šæ©ç é”šç‚¹ã€é‡é‡‡æ ·é¢‘ç‡å’Œæ©ç ç¨€ç–æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†GMixoutï¼Œå®ƒ(i)ç”¨è®­ç»ƒæœŸé—´è‡ªé€‚åº”çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡å¿«ç…§æ›¿æ¢å›ºå®šé”šç‚¹ï¼Œä»¥åŠ(ii)é€šè¿‡æ˜¾å¼çš„é‡é‡‡æ ·é¢‘ç‡è¶…å‚æ•°æ¥è°ƒèŠ‚æ©ç å‘¨æœŸã€‚æˆ‘ä»¬çš„ç¨€ç–æ ¸å®ç°ä»…æ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œä¸”æ²¡æœ‰æ¨ç†æ—¶å¼€é”€ï¼Œä»è€Œå¯ä»¥åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨æ¶µç›–åå˜é‡åç§»ã€æŸåå’Œç±»åˆ«ä¸å¹³è¡¡çš„åŸºå‡†æµ‹è¯•ï¼ˆImageNet / ImageNet-LTã€DomainNetã€iWildCamå’ŒCIFAR100-Cï¼‰ä¸Šï¼ŒGMixoutå§‹ç»ˆä¼˜äºé›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹¶åœ¨åˆ†å¸ƒåç§»ä¸‹è¶…è¶Šæ¨¡å‹é›†æˆå’Œå¼ºå¤§çš„å‚æ•°é«˜æ•ˆå¾®è°ƒåŸºçº¿ï¼ŒåŒæ—¶æå‡äº†åœ¨åŸŸç²¾åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰åŸºç¡€æ¨¡å‹çš„å¾®è°ƒæ–¹æ³•ï¼Œè™½ç„¶èƒ½åœ¨ç›®æ ‡é¢†åŸŸå–å¾—ä¸é”™çš„ç²¾åº¦ï¼Œä½†å½“æµ‹è¯•æ•°æ®ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸ä¸€è‡´æ—¶ï¼ˆå³å‘ç”Ÿåˆ†å¸ƒåç§»ï¼‰ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™ï¼Œé²æ£’æ€§ä¸è¶³ã€‚ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å’Œæ¨¡å‹é›†æˆæ–¹æ³•åœ¨åˆ†å¸ƒåç§»ä¸‹ä»å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ”¹è¿›Mixoutæ­£åˆ™åŒ–æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡æ¨¡å‹åœ¨ç›®æ ‡é¢†åŸŸçš„ç²¾åº¦å’Œåœ¨åˆ†å¸ƒåç§»ä¸‹çš„é²æ£’æ€§ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”çš„æƒé‡æ··åˆç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œæå‡é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGMixoutçš„æ ¸å¿ƒåœ¨äºæ”¹è¿›äº†Mixoutçš„æƒé‡æ··åˆç­–ç•¥ã€‚MixoutåŸæœ¬ä½¿ç”¨å›ºå®šçš„é¢„è®­ç»ƒæƒé‡ä½œä¸ºé”šç‚¹ï¼Œè€ŒGMixoutä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰çš„æƒé‡å¿«ç…§ä½œä¸ºé”šç‚¹ï¼Œè¿™ä¸ªEMAå¿«ç…§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­æ›´æ–°ï¼Œä»è€Œå®ç°è‡ªé€‚åº”çš„æƒé‡æ··åˆã€‚æ­¤å¤–ï¼ŒGMixoutè¿˜å¼•å…¥äº†ä¸€ä¸ªæ˜¾å¼çš„é‡é‡‡æ ·é¢‘ç‡è¶…å‚æ•°ï¼Œç”¨äºæ§åˆ¶æ©ç çš„æ›´æ–°å‘¨æœŸã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨GMixoutè¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šGMixoutçš„å…³é”®åˆ›æ–°åœ¨äºï¼š(1) ä½¿ç”¨EMAæƒé‡å¿«ç…§ä½œä¸ºMixoutçš„é”šç‚¹ï¼Œä½¿å¾—æƒé‡æ··åˆæ›´åŠ è‡ªé€‚åº”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›(2) å¼•å…¥æ˜¾å¼çš„é‡é‡‡æ ·é¢‘ç‡è¶…å‚æ•°ï¼Œç”¨äºæ§åˆ¶æ©ç çš„æ›´æ–°å‘¨æœŸï¼Œä»è€Œæ›´å¥½åœ°è°ƒèŠ‚æ­£åˆ™åŒ–å¼ºåº¦ã€‚ä¸ä¼ ç»ŸMixoutç›¸æ¯”ï¼ŒGMixoutçš„é”šç‚¹æ˜¯åŠ¨æ€å˜åŒ–çš„ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¾®è°ƒè¿‡ç¨‹ä¸­çš„æƒé‡å˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šGMixoutçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) EMAå¿«ç…§çš„æ›´æ–°ç‡ï¼šæ§åˆ¶EMAå¿«ç…§å¯¹å½“å‰æ¨¡å‹æƒé‡çš„å­¦ä¹ é€Ÿåº¦ï¼›(2) é‡é‡‡æ ·é¢‘ç‡ï¼šæ§åˆ¶æ©ç çš„æ›´æ–°é¢‘ç‡ï¼Œå½±å“æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼›(3) ç¨€ç–æ ¸å®ç°ï¼šä¸ºäº†é™ä½è®¡ç®—æˆæœ¬ï¼ŒGMixouté‡‡ç”¨ç¨€ç–æ ¸å®ç°ï¼Œåªæ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œä»è€Œå¯ä»¥åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿›è¡Œè®­ç»ƒã€‚æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGMixoutåœ¨ImageNetã€DomainNetã€iWildCamå’ŒCIFAR100-Cç­‰æ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒäºé›¶æ ·æœ¬æ€§èƒ½å’Œç°æœ‰çš„å¾®è°ƒæ–¹æ³•ï¼ˆåŒ…æ‹¬Model Soupså’Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼‰ï¼Œåœ¨åŸŸå†…ç²¾åº¦å’Œåˆ†å¸ƒåç§»é²æ£’æ€§ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒGMixoutçš„æ€§èƒ½æå‡è¶…è¿‡äº†5%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GMixoutå¯åº”ç”¨äºå„ç§éœ€è¦å¯¹è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®åˆ†å¸ƒå¯èƒ½å‘ç”Ÿåç§»çš„æƒ…å†µä¸‹ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å›¾åƒåˆ†æã€é¥æ„Ÿå›¾åƒåˆ†æç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œé™ä½æ¨¡å‹å¤±æ•ˆçš„é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Finetuning vision foundation models often improves in-domain accuracy but comes at the cost of robustness under distribution shift. We revisit Mixout, a stochastic regularizer that intermittently replaces finetuned weights with their pretrained reference, through the lens of a single-run, weight-sharing implicit ensemble. This perspective reveals three key levers that govern robustness: the \emph{masking anchor}, \emph{resampling frequency}, and \emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i) replaces the fixed anchor with an exponential moving-average snapshot that adapts during training, and (ii) regulates masking period via an explicit resampling-frequency hyperparameter. Our sparse-kernel implementation updates only a small fraction of parameters with no inference-time overhead, enabling training on consumer-grade GPUs. Experiments on benchmarks covering covariate shift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet, iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy beyond zero-shot performance while surpassing both Model Soups and strong parameter-efficient finetuning baselines under distribution shift.

