---
layout: default
title: A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning
---

# A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10512" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10512v1</a>
  <a href="https://arxiv.org/pdf/2509.10512.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10512v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10512v1', 'A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaxing Cao, Yuzhou Gao, Jiwei Huang

**åˆ†ç±»**: cs.LG, cs.GT, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03

**å¤‡æ³¨**: Accepted at CollaborateCom 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé¢å‘æœåŠ¡çš„è‡ªé€‚åº”åˆ†å±‚æ¿€åŠ±æœºåˆ¶ï¼Œè§£å†³è”é‚¦å­¦ä¹ ä¸­æ•°æ®åŒ®ä¹é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `æ¿€åŠ±æœºåˆ¶` `Stackelbergåšå¼ˆ` `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `æ•°æ®è´¡çŒ®` `æœåŠ¡å¯¼å‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è”é‚¦å­¦ä¹ é¢ä¸´æ•°æ®åŒ®ä¹çš„æŒ‘æˆ˜ï¼Œå½±å“æ¨¡å‹è®­ç»ƒæ•ˆæœï¼Œéœ€è¦æ¿€åŠ±æœºåˆ¶å¸å¼•æ›´å¤šæ•°æ®è´¡çŒ®è€…ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§è‡ªé€‚åº”æ¿€åŠ±æœºåˆ¶ï¼Œé€šè¿‡Stackelbergåšå¼ˆå’Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼Œä¼˜åŒ–å„å‚ä¸æ–¹æ•ˆç”¨ã€‚
3. å®éªŒéªŒè¯äº†è¯¥æœºåˆ¶çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿç¨³å®šå‚ä¸è€…ç­–ç•¥ï¼Œå¹¶æå‡è”é‚¦å­¦ä¹ çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸­æ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢å‘æœåŠ¡çš„è‡ªé€‚åº”æ¿€åŠ±æœºåˆ¶ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä»»åŠ¡å‘å¸ƒè€…ï¼ˆTPï¼‰ã€æœ¬åœ°æ¨¡å‹æ‰€æœ‰è€…ï¼ˆLMOï¼‰å’Œæ•°æ®æ”¶é›†è€…ï¼ˆworkersï¼‰çš„æ•ˆç”¨ã€‚å…·ä½“è€Œè¨€ï¼Œè®ºæ–‡åœ¨LMOå’ŒTPä¹‹é—´å»ºç«‹äº†ä¸€ä¸ªStackelbergåšå¼ˆæ¨¡å‹ï¼ŒTPä½œä¸ºé¢†å¯¼è€…ï¼ŒLMOä½œä¸ºè·Ÿéšè€…ï¼Œå¹¶æ¨å¯¼å‡ºè§£æçº³ä»€å‡è¡¡è§£ä»¥æœ€å¤§åŒ–ä»–ä»¬çš„æ•ˆç”¨ã€‚LMOå’Œworkersä¹‹é—´çš„äº¤äº’è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMAMDPï¼‰ï¼Œå¹¶é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ç¡®å®šæœ€ä¼˜ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”æœç´¢æœ€ä¼˜ç­–ç•¥ç®—æ³•ï¼ˆASOSAï¼‰æ¥ç¨³å®šæ¯ä¸ªå‚ä¸è€…çš„ç­–ç•¥å¹¶è§£å†³è€¦åˆé—®é¢˜ã€‚å¤§é‡çš„æ•°å€¼å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè”é‚¦å­¦ä¹ ä¸­ï¼Œç”±äºå‚ä¸è€…æ•°æ®æœ‰é™æˆ–ä¸æ„¿å…±äº«ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒæ•ˆæœå—é™ã€‚ç°æœ‰æ¿€åŠ±æœºåˆ¶å¯èƒ½æ— æ³•æœ‰æ•ˆåè°ƒä»»åŠ¡å‘å¸ƒè€…ã€æœ¬åœ°æ¨¡å‹æ‰€æœ‰è€…å’Œæ•°æ®æ”¶é›†è€…ä¹‹é—´çš„åˆ©ç›Šï¼Œå¯¼è‡´æ•°æ®è´¡çŒ®ä¸è¶³ï¼Œå½±å“è”é‚¦å­¦ä¹ çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä»æœåŠ¡å¯¼å‘çš„è§’åº¦å‡ºå‘ï¼Œè®¾è®¡ä¸€ç§è‡ªé€‚åº”çš„åˆ†å±‚æ¿€åŠ±æœºåˆ¶ã€‚é€šè¿‡Stackelbergåšå¼ˆåè°ƒä»»åŠ¡å‘å¸ƒè€…å’Œæœ¬åœ°æ¨¡å‹æ‰€æœ‰è€…çš„åˆ©ç›Šï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æœ¬åœ°æ¨¡å‹æ‰€æœ‰è€…å’Œæ•°æ®æ”¶é›†è€…ä¹‹é—´çš„äº¤äº’ï¼Œä»è€Œæœ€å¤§åŒ–æ‰€æœ‰å‚ä¸è€…çš„æ•ˆç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦å‚ä¸è€…ï¼šä»»åŠ¡å‘å¸ƒè€…ï¼ˆTPï¼‰ã€æœ¬åœ°æ¨¡å‹æ‰€æœ‰è€…ï¼ˆLMOï¼‰å’Œæ•°æ®æ”¶é›†è€…ï¼ˆworkersï¼‰ã€‚TPå’ŒLMOä¹‹é—´é€šè¿‡Stackelbergåšå¼ˆè¿›è¡Œäº¤äº’ï¼ŒTPä½œä¸ºé¢†å¯¼è€…è®¾å®šæ¿€åŠ±ç­–ç•¥ï¼ŒLMOä½œä¸ºè·Ÿéšè€…æ ¹æ®TPçš„ç­–ç•¥é€‰æ‹©å‚ä¸ç¨‹åº¦ã€‚LMOå’Œworkersä¹‹é—´é€šè¿‡å¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹è¿›è¡Œäº¤äº’ï¼ŒLMOæ ¹æ®workersçš„æ•°æ®è´¡çŒ®æƒ…å†µç»™äºˆå¥–åŠ±ã€‚æ•´ä¸ªè¿‡ç¨‹é€šè¿‡è‡ªé€‚åº”æœç´¢æœ€ä¼˜ç­–ç•¥ç®—æ³•ï¼ˆASOSAï¼‰è¿›è¡Œåè°ƒï¼Œä»¥ç¨³å®šå„å‚ä¸è€…çš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†Stackelbergåšå¼ˆå’Œå¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç›¸ç»“åˆï¼Œæ„å»ºäº†ä¸€ä¸ªåˆ†å±‚çš„æ¿€åŠ±æœºåˆ¶ã€‚è¿™ç§åˆ†å±‚ç»“æ„èƒ½å¤Ÿæ›´å¥½åœ°æ¨¡æ‹Ÿè”é‚¦å­¦ä¹ ä¸­ä¸åŒå‚ä¸è€…ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œå¹¶å®ç°æ›´æœ‰æ•ˆçš„æ¿€åŠ±ã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”æœç´¢æœ€ä¼˜ç­–ç•¥ç®—æ³•ï¼ˆASOSAï¼‰èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å‚ä¸è€…ç­–ç•¥ä¹‹é—´çš„è€¦åˆé—®é¢˜ï¼Œæé«˜æ¿€åŠ±æœºåˆ¶çš„ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šStackelbergåšå¼ˆä¸­ï¼ŒTPçš„æ•ˆç”¨å‡½æ•°è€ƒè™‘äº†æ¨¡å‹è®­ç»ƒçš„æ”¶ç›Šå’Œæ¿€åŠ±æˆæœ¬ï¼ŒLMOçš„æ•ˆç”¨å‡½æ•°è€ƒè™‘äº†æ¨¡å‹è®­ç»ƒçš„æ”¶ç›Šå’Œæ•°æ®æ”¶é›†çš„æˆæœ¬ã€‚å¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­ï¼ŒçŠ¶æ€ç©ºé—´åŒ…æ‹¬workersçš„æ•°æ®è´¡çŒ®æƒ…å†µï¼ŒåŠ¨ä½œç©ºé—´åŒ…æ‹¬LMOçš„å¥–åŠ±ç­–ç•¥ï¼Œå¥–åŠ±å‡½æ•°è€ƒè™‘äº†LMOçš„æ•ˆç”¨ã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ç”¨äºå­¦ä¹ LMOçš„æœ€ä¼˜å¥–åŠ±ç­–ç•¥ã€‚ASOSAç®—æ³•é€šè¿‡è¿­ä»£æœç´¢ï¼Œé€æ­¥è°ƒæ•´å„å‚ä¸è€…çš„ç­–ç•¥ï¼Œç›´åˆ°è¾¾åˆ°çº³ä»€å‡è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡æ•°å€¼å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜ä»»åŠ¡å‘å¸ƒè€…ã€æœ¬åœ°æ¨¡å‹æ‰€æœ‰è€…å’Œæ•°æ®æ”¶é›†è€…çš„æ•ˆç”¨ã€‚ä¸ä¼ ç»Ÿçš„æ¿€åŠ±æœºåˆ¶ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¿€åŠ±æ•°æ®è´¡çŒ®ï¼Œä»è€Œæé«˜è”é‚¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œä½†æ‘˜è¦ä¸­æœªæ˜ç¡®æåŠã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è”é‚¦å­¦ä¹ çš„åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—å¥åº·ã€é‡‘èé£æ§ã€æ™ºèƒ½äº¤é€šç­‰ã€‚é€šè¿‡æœ‰æ•ˆçš„æ¿€åŠ±æœºåˆ¶ï¼Œå¯ä»¥å¸å¼•æ›´å¤šçš„æ•°æ®è´¡çŒ®è€…å‚ä¸è”é‚¦å­¦ä¹ ï¼Œä»è€Œæé«˜æ¨¡å‹è®­ç»ƒçš„æ•ˆæœï¼Œå¹¶ä¿ƒè¿›ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„è”é‚¦å­¦ä¹ åœºæ™¯ï¼Œä¾‹å¦‚å¼‚æ„æ•°æ®å’Œéç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, federated learning (FL) has emerged as a novel framework for distributed model training. In FL, the task publisher (TP) releases tasks, and local model owners (LMOs) use their local data to train models. Sometimes, FL suffers from the lack of training data, and thus workers are recruited for gathering data. To this end, this paper proposes an adaptive incentive mechanism from a service-oriented perspective, with the objective of maximizing the utilities of TP, LMOs and workers. Specifically, a Stackelberg game is theoretically established between the LMOs and TP, positioning TP as the leader and the LMOs as followers. An analytical Nash equilibrium solution is derived to maximize their utilities. The interaction between LMOs and workers is formulated by a multi-agent Markov decision process (MAMDP), with the optimal strategy identified via deep reinforcement learning (DRL). Additionally, an Adaptively Searching the Optimal Strategy Algorithm (ASOSA) is designed to stabilize the strategies of each participant and solve the coupling problems. Extensive numerical experiments are conducted to validate the efficacy of the proposed method.

