---
layout: default
title: A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games
---

# A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03682" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03682v1</a>
  <a href="https://arxiv.org/pdf/2509.03682.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03682v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03682v1', 'A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengyang Li, Qijin Ji, Xinghong Ling, Quan Liu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03

**å¤‡æ³¨**: IEEE Transactions on Games, 2025

**DOI**: [10.1109/TG.2025.3588809](https://doi.org/10.1109/TG.2025.3588809)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åœ¨è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨ä¸æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `è§†é¢‘æ¸¸æˆAI` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `è‡ªåšå¼ˆ` `å›¢é˜Ÿåä½œ` `éå¹³ç¨³ç¯å¢ƒ` `éƒ¨åˆ†å¯è§‚æµ‹æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MARLæ–¹æ³•åœ¨è§†é¢‘æ¸¸æˆä¸­é¢ä¸´éå¹³ç¨³ç¯å¢ƒã€éƒ¨åˆ†å¯è§‚æµ‹ã€ç¨€ç–å¥–åŠ±ç­‰æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨æ•ˆæœã€‚
2. è¯¥ç»¼è¿°æ—¨åœ¨å…¨é¢åˆ†æMARLåœ¨ä¸åŒç±»å‹è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨ï¼Œå¹¶æ¢è®¨è§£å†³ä¸Šè¿°æŒ‘æˆ˜çš„ç­–ç•¥ã€‚
3. è®ºæ–‡åˆ†æäº†MARLåœ¨å„ç±»æ¸¸æˆä¸­çš„æˆåŠŸæ¡ˆä¾‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ¸¸æˆå¤æ‚åº¦è¯„ä¼°æ–¹æ³•ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)çš„æœ€æ–°è¿›å±•è¡¨æ˜å…¶åœ¨ç°ä»£æ¸¸æˆä¸­å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚ä»åŸºç¡€å·¥ä½œåˆ°é‡Œç¨‹ç¢‘å¼çš„æˆå°±ï¼Œå¦‚æ˜Ÿé™…äº‰éœ¸IIä¸­çš„AlphaStarå’ŒDota 2ä¸­çš„OpenAI Fiveï¼ŒMARLå·²ç»è¯æ˜äº†é€šè¿‡è‡ªåšå¼ˆã€ç›‘ç£å­¦ä¹ å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„æ¸¸æˆç¯å¢ƒä¸­å®ç°è¶…äººçš„æ€§èƒ½ã€‚éšç€å…¶å½±å“åŠ›çš„å¢é•¿ï¼Œå…¨é¢çš„ç»¼è¿°å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨å…¨é¢è€ƒå¯ŸMARLåœ¨è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨ï¼Œä»å›åˆåˆ¶åŒæ™ºèƒ½ä½“æ¸¸æˆåˆ°å®æ—¶å¤šæ™ºèƒ½ä½“è§†é¢‘æ¸¸æˆï¼ŒåŒ…æ‹¬ä½“è‚²æ¸¸æˆã€ç¬¬ä¸€äººç§°å°„å‡»(FPS)æ¸¸æˆã€å®æ—¶æˆ˜ç•¥(RTS)æ¸¸æˆå’Œå¤šäººåœ¨çº¿æˆ˜æ–—ç«æŠ€åœº(MOBA)æ¸¸æˆç­‰æµè¡Œç±»å‹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†MARLåœ¨è§†é¢‘æ¸¸æˆä¸­é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éå¹³ç¨³æ€§ã€éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ã€ç¨€ç–å¥–åŠ±ã€å›¢é˜Ÿåè°ƒå’Œå¯æ‰©å±•æ€§ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†åœ¨ç«ç®­è”ç›Ÿã€æˆ‘çš„ä¸–ç•Œã€é›·ç¥ä¹‹é”¤IIIç«æŠ€åœºã€æ˜Ÿé™…äº‰éœ¸IIã€Dota 2ã€ç‹è€…è£è€€ç­‰æ¸¸æˆä¸­çš„æˆåŠŸåº”ç”¨ã€‚æœ¬æ–‡æ·±å…¥äº†è§£äº†MARLåœ¨è§†é¢‘æ¸¸æˆAIç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§ä¼°è®¡æ¸¸æˆå¤æ‚æ€§çš„æ–°æ–¹æ³•ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æ¨è¿›MARLåŠå…¶åœ¨æ¸¸æˆå¼€å‘ä¸­çš„åº”ç”¨ï¼Œä»è€Œæ¿€å‘è¿™ä¸ªå¿«é€Ÿå‘å±•é¢†åŸŸçš„è¿›ä¸€æ­¥åˆ›æ–°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åœ¨è§†é¢‘æ¸¸æˆä¸­é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œä¾‹å¦‚ç¯å¢ƒçš„éå¹³ç¨³æ€§ï¼ˆnon-stationarityï¼‰ï¼Œç”±äºå…¶ä»–æ™ºèƒ½ä½“çš„ç­–ç•¥ä¹Ÿåœ¨ä¸æ–­å˜åŒ–ï¼›éƒ¨åˆ†å¯è§‚æµ‹æ€§ï¼ˆpartial observabilityï¼‰ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“åªèƒ½è§‚å¯Ÿåˆ°ç¯å¢ƒçš„ä¸€éƒ¨åˆ†ä¿¡æ¯ï¼›ç¨€ç–å¥–åŠ±ï¼ˆsparse rewardsï¼‰ï¼Œæ™ºèƒ½ä½“å¾ˆéš¾è·å¾—æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ï¼›å›¢é˜Ÿåè°ƒï¼ˆteam coordinationï¼‰ï¼Œæ™ºèƒ½ä½“ä¹‹é—´éœ€è¦ååŒåˆä½œæ‰èƒ½å®Œæˆä»»åŠ¡ï¼›ä»¥åŠå¯æ‰©å±•æ€§ï¼ˆscalabilityï¼‰ï¼Œå½“æ™ºèƒ½ä½“æ•°é‡å¢åŠ æ—¶ï¼Œç®—æ³•çš„è®¡ç®—å¤æ‚åº¦ä¼šæ€¥å‰§ä¸Šå‡ã€‚è¿™äº›é—®é¢˜ä½¿å¾—è®­ç»ƒå‡ºèƒ½å¤Ÿåœ¨å¤æ‚æ¸¸æˆä¸­è¡¨ç°è‰¯å¥½çš„MARLæ™ºèƒ½ä½“å˜å¾—éå¸¸å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥ç»¼è¿°çš„æ ¸å¿ƒæ€è·¯æ˜¯ç³»ç»Ÿæ€§åœ°æ¢³ç†MARLåœ¨ä¸åŒç±»å‹è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨ç°çŠ¶ï¼Œåˆ†æç°æœ‰æ–¹æ³•åœ¨è§£å†³ä¸Šè¿°æŒ‘æˆ˜æ—¶æ‰€é‡‡ç”¨çš„ç­–ç•¥ï¼Œå¹¶æ€»ç»“æˆåŠŸæ¡ˆä¾‹çš„ç»éªŒã€‚é€šè¿‡å¯¹ä¸åŒæ¸¸æˆç±»å‹å’ŒMARLç®—æ³•çš„å¯¹æ¯”åˆ†æï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€ä¸ªå…¨é¢çš„è§†è§’ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£MARLåœ¨è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨æ½œåŠ›å’Œå±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç»¼è¿°çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼šé¦–å…ˆï¼Œä»‹ç»MARLçš„åŸºç¡€çŸ¥è¯†å’Œå¸¸ç”¨ç®—æ³•ï¼›å…¶æ¬¡ï¼ŒæŒ‰ç…§æ¸¸æˆç±»å‹ï¼ˆå¦‚ä½“è‚²æ¸¸æˆã€FPSæ¸¸æˆã€RTSæ¸¸æˆã€MOBAæ¸¸æˆç­‰ï¼‰åˆ†ç±»ï¼Œåˆ†åˆ«ä»‹ç»MARLåœ¨è¿™äº›æ¸¸æˆä¸­çš„åº”ç”¨æ¡ˆä¾‹ï¼›ç„¶åï¼Œåˆ†æMARLåœ¨è§†é¢‘æ¸¸æˆä¸­é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶è®¨è®ºç°æœ‰æ–¹æ³•å¦‚ä½•åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼›æœ€åï¼Œæå‡ºä¸€ç§æ–°çš„æ¸¸æˆå¤æ‚åº¦è¯„ä¼°æ–¹æ³•ï¼Œå¹¶å±•æœ›æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç»¼è¿°çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶å…¨é¢æ€§å’Œç³»ç»Ÿæ€§ã€‚å®ƒä¸ä»…æ¶µç›–äº†MARLåœ¨å„ç§ç±»å‹è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨ï¼Œè¿˜æ·±å…¥åˆ†æäº†MARLé¢ä¸´çš„æŒ‘æˆ˜å’Œç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ¸¸æˆå¤æ‚åº¦è¯„ä¼°æ–¹æ³•ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ–°çš„å·¥å…·æ¥è¯„ä¼°ä¸åŒæ¸¸æˆçš„éš¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥ç»¼è¿°çš„å…³é”®è®¾è®¡åœ¨äºå…¶ç»“æ„åŒ–çš„ç»„ç»‡æ–¹å¼ã€‚é€šè¿‡æŒ‰ç…§æ¸¸æˆç±»å‹å’ŒæŒ‘æˆ˜ç±»å‹è¿›è¡Œåˆ†ç±»ï¼Œä½¿å¾—è¯»è€…å¯ä»¥å¿«é€Ÿæ‰¾åˆ°è‡ªå·±æ„Ÿå…´è¶£çš„å†…å®¹ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜åŒ…å«äº†å¤§é‡çš„å‚è€ƒæ–‡çŒ®ï¼Œæ–¹ä¾¿è¯»è€…è¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°æ€»ç»“äº†MARLåœ¨ã€Šç«ç®­è”ç›Ÿã€‹ã€ã€Šæˆ‘çš„ä¸–ç•Œã€‹ã€ã€Šé›·ç¥ä¹‹é”¤IIIç«æŠ€åœºã€‹ã€ã€Šæ˜Ÿé™…äº‰éœ¸IIã€‹ã€ã€ŠDota 2ã€‹ã€ã€Šç‹è€…è£è€€ã€‹ç­‰æ¸¸æˆä¸­çš„æˆåŠŸåº”ç”¨æ¡ˆä¾‹ï¼Œå±•ç¤ºäº†MARLåœ¨ä¸åŒæ¸¸æˆç±»å‹ä¸­å–å¾—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ¸¸æˆå¤æ‚åº¦è¯„ä¼°æ–¹æ³•ï¼Œä¸ºMARLç®—æ³•çš„è®¾è®¡å’Œé€‰æ‹©æä¾›äº†å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ¸¸æˆAIå¼€å‘ï¼Œæå‡æ¸¸æˆæ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›å’Œåä½œæ°´å¹³ï¼Œåˆ›é€ æ›´å…·æŒ‘æˆ˜æ€§å’Œè¶£å‘³æ€§çš„æ¸¸æˆä½“éªŒã€‚æ­¤å¤–ï¼ŒMARLæŠ€æœ¯åœ¨æ¸¸æˆä¸­çš„åº”ç”¨ç»éªŒï¼Œå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–å¤šæ™ºèƒ½ä½“åä½œåœºæ™¯ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººååŒã€äº¤é€šè°ƒåº¦ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in multi-agent reinforcement learning (MARL) have demonstrated its application potential in modern games. Beginning with foundational work and progressing to landmark achievements such as AlphaStar in StarCraft II and OpenAI Five in Dota 2, MARL has proven capable of achieving superhuman performance across diverse game environments through techniques like self-play, supervised learning, and deep reinforcement learning. With its growing impact, a comprehensive review has become increasingly important in this field. This paper aims to provide a thorough examination of MARL's application from turn-based two-agent games to real-time multi-agent video games including popular genres such as Sports games, First-Person Shooter (FPS) games, Real-Time Strategy (RTS) games and Multiplayer Online Battle Arena (MOBA) games. We further analyze critical challenges posed by MARL in video games, including nonstationary, partial observability, sparse rewards, team coordination, and scalability, and highlight successful implementations in games like Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2, Honor of Kings, etc. This paper offers insights into MARL in video game AI systems, proposes a novel method to estimate game complexity, and suggests future research directions to advance MARL and its applications in game development, inspiring further innovation in this rapidly evolving field.

