---
layout: default
title: LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence
---

# LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03505" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03505v2</a>
  <a href="https://arxiv.org/pdf/2509.03505.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03505v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03505v2', 'LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingxuan Zhang, Gang Ren, Han Yu, Hao Yuan, Hui Wang, Jiansheng Li, Jiayun Wu, Lang Mo, Li Mao, Mingchao Hao, Ningbo Dai, Renzhe Xu, Shuyang Li, Tianyang Zhang, Yue He, Yuanrui Wang, Yunjia Zhang, Zijing Xu, Dongzhe Li, Fang Gao, Hao Zou, Jiandong Liu, Jiashuo Liu, Jiawei Xu, Kaijie Cheng, Kehan Li, Linjun Zhou, Qing Li, Shaohua Fan, Xiaoyu Lin, Xinyan Han, Xuanyue Li, Yan Lu, Yuan Xue, Yuanyuan Jiang, Zimu Wang, Zhenlei Wang, Peng Cui

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03 (æ›´æ–°: 2025-11-07)

**å¤‡æ³¨**: 61 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LimiXï¼šé‡Šæ”¾ç»“æ„åŒ–æ•°æ®å»ºæ¨¡èƒ½åŠ›ï¼Œèµ‹èƒ½é€šç”¨æ™ºèƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»“æ„åŒ–æ•°æ®å»ºæ¨¡` `è¡¨æ ¼æ•°æ®` `è”åˆåˆ†å¸ƒ` `æ¡ä»¶é¢„æµ‹` `é¢„è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®æ—¶ï¼Œé€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡æ¨¡å‹å’Œè®­ç»ƒç­–ç•¥ï¼Œç¼ºä¹é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚
2. LimiXå°†ç»“æ„åŒ–æ•°æ®å»ºæ¨¡ä¸ºå˜é‡å’Œç¼ºå¤±å€¼çš„è”åˆåˆ†å¸ƒï¼Œé€šè¿‡æ¡ä»¶é¢„æµ‹æ¥å¤„ç†å„ç§ä»»åŠ¡ï¼Œå®ç°ç»Ÿä¸€å»ºæ¨¡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLimiXåœ¨å¤šä¸ªç»“æ„åŒ–æ•°æ®åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†ç±»ã€å›å½’å’Œç¼ºå¤±å€¼æ’è¡¥ç­‰ä»»åŠ¡ä¸Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºLimiX-16Må’ŒLimiX-2Mï¼Œä½œä¸ºå¤§å‹ç»“æ„åŒ–æ•°æ®æ¨¡å‹ï¼ˆLDMï¼‰çš„ä¸¤ä¸ªå®ä¾‹ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹å°†ç»“æ„åŒ–æ•°æ®è§†ä¸ºå˜é‡å’Œç¼ºå¤±å€¼çš„è”åˆåˆ†å¸ƒï¼Œä»è€Œèƒ½å¤Ÿé€šè¿‡å•ä¸ªæ¨¡å‹ï¼ŒåŸºäºæŸ¥è¯¢çš„æ¡ä»¶é¢„æµ‹æ¥å¤„ç†å„ç§è¡¨æ ¼ä»»åŠ¡ã€‚å®ƒä»¬ä½¿ç”¨å¸¦æƒ…æ™¯æ¡ä»¶ç›®æ ‡çš„episodic masked joint-distribution modelingè¿›è¡Œé¢„è®­ç»ƒï¼Œæ”¯æŒåœ¨æ¨ç†æ—¶è¿›è¡Œå¿«é€Ÿã€å…è®­ç»ƒçš„é€‚åº”ã€‚æˆ‘ä»¬åœ¨11ä¸ªå¤§å‹ç»“æ„åŒ–æ•°æ®åŸºå‡†ä¸Šè¯„ä¼°LimiXæ¨¡å‹ï¼Œè¿™äº›åŸºå‡†æ¶µç›–äº†æ ·æœ¬å¤§å°ã€ç‰¹å¾ç»´åº¦ã€ç±»åˆ«æ•°é‡ã€ç±»åˆ«ä¸æ•°å€¼ç‰¹å¾æ¯”ç‡ã€ç¼ºå¤±å€¼å’Œæ ·æœ¬ä¸ç‰¹å¾æ¯”ç‡ç­‰å¹¿æ³›èŒƒå›´ã€‚LimiX-16Må§‹ç»ˆè¶…è¶Šå¼ºå¤§çš„åŸºçº¿ï¼Œåœ¨åˆ†ç±»ã€å›å½’ã€ç¼ºå¤±å€¼æ’è¡¥å’Œæ•°æ®ç”Ÿæˆç­‰å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œé€šå¸¸æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶é¿å…äº†ç‰¹å®šäºä»»åŠ¡çš„æ¶æ„æˆ–æ¯ä¸ªä»»åŠ¡çš„å®šåˆ¶è®­ç»ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLimiX-2Måœ¨ä¸¥æ ¼çš„è®¡ç®—å’Œå†…å­˜é¢„ç®—ä¸‹ä¹Ÿèƒ½æä¾›å¼ºå¤§çš„ç»“æœã€‚æˆ‘ä»¬è¿˜æå‡ºäº†LDMçš„ç¬¬ä¸€ä¸ªç¼©æ”¾å®šå¾‹ç ”ç©¶ï¼Œæ­ç¤ºäº†æ•°æ®å’Œæ¨¡å‹ç¼©æ”¾å¦‚ä½•å…±åŒå½±å“ä¸‹æ¸¸æ€§èƒ½ï¼Œå¹¶ä¸ºè¡¨æ ¼åŸºç¡€å»ºæ¨¡æä¾›å®šé‡æŒ‡å¯¼ã€‚æ‰€æœ‰LimiXæ¨¡å‹å‡ä»¥Apache 2.0åè®®å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®æ—¶ï¼Œé€šå¸¸éœ€è¦é’ˆå¯¹ä¸åŒä»»åŠ¡è®¾è®¡ä¸åŒçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹ï¼Œç¼ºä¹é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œå¯¹äºåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®ï¼Œéœ€è¦è¿›è¡Œä¸“é—¨çš„å¤„ç†æˆ–æ’è¡¥ï¼Œå¢åŠ äº†å»ºæ¨¡çš„å¤æ‚æ€§ã€‚è¿™äº›ç—›ç‚¹é™åˆ¶äº†ç»“æ„åŒ–æ•°æ®æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLimiXçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç»“æ„åŒ–æ•°æ®è§†ä¸ºä¸€ä¸ªå˜é‡å’Œç¼ºå¤±å€¼çš„è”åˆæ¦‚ç‡åˆ†å¸ƒã€‚é€šè¿‡å­¦ä¹ è¿™ä¸ªè”åˆåˆ†å¸ƒï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®ç»™å®šçš„æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼ŒæŸäº›å˜é‡çš„å€¼ï¼‰æ¥é¢„æµ‹å…¶ä»–å˜é‡çš„å€¼æˆ–ç¼ºå¤±æƒ…å†µã€‚è¿™ç§æ–¹æ³•å°†å„ç§è¡¨æ ¼ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ã€å›å½’ã€ç¼ºå¤±å€¼æ’è¡¥ç­‰ï¼‰ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶ä¸‹ï¼Œé¿å…äº†ä¸ºæ¯ä¸ªä»»åŠ¡å•ç‹¬è®¾è®¡æ¨¡å‹çš„éœ€è¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLimiXçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬é¢„è®­ç»ƒå’Œæ¨ç†ä¸¤ä¸ªé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹ä½¿ç”¨masked joint-distribution modelingå­¦ä¹ ç»“æ„åŒ–æ•°æ®çš„è”åˆåˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹éšæœºmaskæ‰ä¸€äº›å˜é‡çš„å€¼ï¼Œç„¶åå°è¯•æ ¹æ®å‰©ä½™çš„å˜é‡æ¥é¢„æµ‹è¢«maskæ‰çš„å€¼ã€‚é¢„è®­ç»ƒé‡‡ç”¨episodic, context-conditional objectiveï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„ä¸Šä¸‹æ–‡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢æ¡ä»¶ï¼Œè¿›è¡Œæ¡ä»¶é¢„æµ‹ï¼Œä»è€Œå®Œæˆå„ç§è¡¨æ ¼ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šLimiXæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå…¶å°†ç»“æ„åŒ–æ•°æ®å»ºæ¨¡ä¸ºè”åˆåˆ†å¸ƒï¼Œå¹¶é€šè¿‡æ¡ä»¶é¢„æµ‹æ¥ç»Ÿä¸€å¤„ç†å„ç§è¡¨æ ¼ä»»åŠ¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLimiXé¿å…äº†ä¸ºæ¯ä¸ªä»»åŠ¡å•ç‹¬è®¾è®¡æ¨¡å‹ï¼Œæé«˜äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚æ­¤å¤–ï¼ŒLimiXçš„é¢„è®­ç»ƒæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ ç»“æ„åŒ–æ•°æ®çš„å†…åœ¨ç»“æ„ï¼Œæé«˜äº†æ¨¡å‹çš„é¢„æµ‹ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šLimiXçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformeræ¶æ„ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œä»¥æ•æ‰å˜é‡ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼›2) é‡‡ç”¨masked joint-distribution modelingè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ ç»“æ„åŒ–æ•°æ®çš„è”åˆåˆ†å¸ƒï¼›3) è®¾è®¡episodic, context-conditional objectiveï¼Œä»¥æé«˜æ¨¡å‹çš„é€‚åº”æ€§ï¼›4) é€šè¿‡ç¼©æ”¾å®éªŒç ”ç©¶æ•°æ®å’Œæ¨¡å‹å¤§å°å¯¹æ€§èƒ½çš„å½±å“ï¼Œä¸ºè¡¨æ ¼åŸºç¡€å»ºæ¨¡æä¾›æŒ‡å¯¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LimiX-16Måœ¨11ä¸ªå¤§å‹ç»“æ„åŒ–æ•°æ®åŸºå‡†ä¸Šå§‹ç»ˆè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œåœ¨åˆ†ç±»ã€å›å½’ã€ç¼ºå¤±å€¼æ’è¡¥å’Œæ•°æ®ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒLimiXçš„æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡10%ã€‚æ­¤å¤–ï¼ŒLimiX-2Måœ¨è®¡ç®—å’Œå†…å­˜èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œä»ç„¶èƒ½å¤Ÿå–å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LimiXåœ¨é‡‘èé£æ§ã€åŒ»ç–—è¯Šæ–­ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºé¢„æµ‹å®¢æˆ·ä¿¡ç”¨é£é™©ã€è¾…åŠ©åŒ»ç”Ÿè¿›è¡Œç–¾ç—…è¯Šæ–­ã€æé«˜æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚é€šè¿‡ç»Ÿä¸€çš„ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æ¡†æ¶ï¼ŒLimiXå¯ä»¥é™ä½æ¨¡å‹å¼€å‘å’Œç»´æŠ¤æˆæœ¬ï¼Œæé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼Œä»è€Œä¸ºå„è¡Œä¸šå¸¦æ¥å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We argue that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX-16M and LimiX-2M, two instantiations of our large structured-data models (LDMs). Both models treat structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. They are pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, supporting rapid, training-free adaptation at inference. We evaluate LimiX models across 11 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. LimiX-16M consistently surpasses strong baselines, as shown in Figure 1 and Figure 2. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. Notably, LimiX-2M delivers strong results under tight compute and memory budgets. We also present the first scaling law study for LDMs, revealing how data and model scaling jointly influence downstream performance and offering quantitative guidance for tabular foundation modeling. All LimiX models are publicly accessible under Apache 2.0.

