---
layout: default
title: Population-aware Online Mirror Descent for Mean-Field Games with Common Noise by Deep Reinforcement Learning
---

# Population-aware Online Mirror Descent for Mean-Field Games with Common Noise by Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03030" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03030v1</a>
  <a href="https://arxiv.org/pdf/2509.03030.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03030v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03030v1', 'Population-aware Online Mirror Descent for Mean-Field Games with Common Noise by Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zida Wu, Mathieu Lauriere, Matthieu Geist, Olivier Pietquin, Ankur Mehta

**åˆ†ç±»**: cs.LG, cs.MA, cs.RO, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03

**å¤‡æ³¨**: 2025 IEEE 64rd Conference on Decision and Control (CDC)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„Population-aware Online Mirror Descentç®—æ³•ï¼Œè§£å†³å¸¦å…±åŒå™ªå£°çš„Mean-Field Gamesé—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å‡åœºåšå¼ˆ` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `åœ¨çº¿é•œåƒä¸‹é™` `å…±åŒå™ªå£°` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `çº³ä»€å‡è¡¡` `Munchausen RL`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸMFGæ–¹æ³•åœ¨åˆå§‹åˆ†å¸ƒæœªçŸ¥æˆ–å­˜åœ¨å…±åŒå™ªå£°æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å­¦ä¹ çº³ä»€å‡è¡¡ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºMunchausen RLå’ŒOnline Mirror Descentçš„DRLç®—æ³•ï¼Œæ— éœ€å†å²é‡‡æ ·ï¼Œé€‚åº”ä¸åŒåˆå§‹åˆ†å¸ƒå’Œå™ªå£°ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨æ”¶æ•›æ€§ä¸Šä¼˜äºç°æœ‰ç®—æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨å…±åŒå™ªå£°æ—¶ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ç®—æ³•ï¼Œç”¨äºåœ¨å‡åœºåšå¼ˆï¼ˆMFGï¼‰ä¸­å­¦ä¹ äººå£ç›¸å…³çš„çº³ä»€å‡è¡¡ï¼Œå°¤å…¶æ˜¯åœ¨åˆå§‹åˆ†å¸ƒæœªçŸ¥æˆ–äººå£å—åˆ°å…±åŒå™ªå£°å½±å“çš„æƒ…å†µä¸‹ã€‚è¯¥ç®—æ³•å—åˆ°Munchausen RLå’ŒOnline Mirror Descentçš„å¯å‘ï¼Œæ— éœ€å¹³å‡æˆ–å†å²é‡‡æ ·ã€‚æ‰€å¾—åˆ°çš„ç­–ç•¥èƒ½å¤Ÿé€‚åº”å„ç§åˆå§‹åˆ†å¸ƒå’Œå…±åŒå™ªå£°æºã€‚é€šè¿‡åœ¨ä¸ƒä¸ªå…¸å‹ä¾‹å­ä¸Šçš„æ•°å€¼å®éªŒï¼Œè¯æ˜äº†è¯¥ç®—æ³•ç›¸æ¯”äºæœ€å…ˆè¿›çš„ç®—æ³•ï¼ˆç‰¹åˆ«æ˜¯ç”¨äºäººå£ç›¸å…³ç­–ç•¥çš„Fictitious Playçš„DRLç‰ˆæœ¬ï¼‰å…·æœ‰æ›´ä¼˜è¶Šçš„æ”¶æ•›ç‰¹æ€§ã€‚åœ¨å­˜åœ¨å…±åŒå™ªå£°çš„æƒ…å†µä¸‹çš„æ€§èƒ½çªæ˜¾äº†è¯¥æ–¹æ³•çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³Mean-Field Gamesï¼ˆMFGï¼‰ä¸­ï¼Œå½“åˆå§‹åˆ†å¸ƒæœªçŸ¥æˆ–å­˜åœ¨å…±åŒå™ªå£°æ—¶ï¼Œå­¦ä¹ äººå£ä¾èµ–çš„çº³ä»€å‡è¡¡çš„éš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚Fictitious Playï¼Œåœ¨å¤„ç†å¤æ‚ç¯å¢ƒå’Œé«˜ç»´çŠ¶æ€ç©ºé—´æ—¶ï¼Œæ”¶æ•›é€Ÿåº¦æ…¢ï¼Œä¸”å¯¹å™ªå£°æ•æ„Ÿã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ³•ä¾èµ–äºå¯¹å†å²æ•°æ®çš„å¹³å‡æˆ–é‡‡æ ·ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†Online Mirror Descentï¼ˆOMDï¼‰çš„æ€æƒ³èå…¥åˆ°æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ¡†æ¶ä¸­ï¼Œå¹¶ç»“åˆMunchausen RLçš„ä¼˜åŠ¿ï¼Œä»è€Œå®ç°å¯¹äººå£åˆ†å¸ƒå˜åŒ–çš„å¿«é€Ÿé€‚åº”å’Œå¯¹å…±åŒå™ªå£°çš„é²æ£’æ€§ã€‚OMDèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿½è¸ªæœ€ä¼˜ç­–ç•¥ï¼Œè€ŒMunchausen RLåˆ™é€šè¿‡å¼•å…¥å¥–åŠ±æŠ˜æ‰£ï¼Œé¼“åŠ±æ¢ç´¢ï¼Œé¿å…è¿‡æ—©æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç®—æ³•çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç¯å¢ƒäº¤äº’æ¨¡å—ï¼šæ™ºèƒ½ä½“ä¸MFGç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œæ”¶é›†çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ç­‰æ•°æ®ã€‚2) ç­–ç•¥ç½‘ç»œï¼šä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¡¨ç¤ºæ™ºèƒ½ä½“çš„ç­–ç•¥ï¼Œæ ¹æ®å½“å‰çŠ¶æ€è¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒã€‚3) ä»·å€¼ç½‘ç»œï¼šä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œä¼°è®¡çŠ¶æ€çš„ä»·å€¼å‡½æ•°ï¼Œç”¨äºæŒ‡å¯¼ç­–ç•¥çš„æ›´æ–°ã€‚4) Online Mirror Descentæ›´æ–°æ¨¡å—ï¼šæ ¹æ®ç¯å¢ƒåé¦ˆå’Œä»·å€¼ç½‘ç»œçš„ä¼°è®¡ï¼Œä½¿ç”¨OMDç®—æ³•æ›´æ–°ç­–ç•¥ç½‘ç»œã€‚5) Munchausenå¥–åŠ±è°ƒæ•´æ¨¡å—ï¼šå¯¹ç¯å¢ƒå¥–åŠ±è¿›è¡Œè°ƒæ•´ï¼Œé¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢æœªçŸ¥çš„çŠ¶æ€ç©ºé—´ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†Online Mirror Descentä¸Munchausen RLç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§Population-awareçš„DRLç®—æ³•ã€‚ä¸ä¼ ç»Ÿçš„Fictitious Playç­‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç®—æ³•æ— éœ€å¯¹å†å²æ•°æ®è¿›è¡Œå¹³å‡æˆ–é‡‡æ ·ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°é€‚åº”äººå£åˆ†å¸ƒçš„å˜åŒ–ã€‚æ­¤å¤–ï¼ŒMunchausen RLçš„å¼•å…¥å¢å¼ºäº†ç®—æ³•çš„æ¢ç´¢èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹å…±åŒå™ªå£°çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œå‡é‡‡ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå…·ä½“ç»“æ„æ ¹æ®å…·ä½“MFGé—®é¢˜çš„å¤æ‚ç¨‹åº¦è¿›è¡Œè°ƒæ•´ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ç­–ç•¥æ¢¯åº¦æŸå¤±å’Œä»·å€¼å‡½æ•°æŸå¤±ï¼Œç”¨äºä¼˜åŒ–ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œã€‚Online Mirror Descentçš„æ›´æ–°è§„åˆ™é‡‡ç”¨æŒ‡æ•°åŠ æƒå¹³å‡ï¼Œä»¥å¹³æ»‘ç­–ç•¥çš„æ›´æ–°è¿‡ç¨‹ã€‚Munchausenå¥–åŠ±çš„æŠ˜æ‰£å› å­æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“é—®é¢˜è¿›è¡Œè°ƒæ•´ï¼Œä»¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨ä¸ƒä¸ªå…¸å‹MFGä¾‹å­ä¸­å‡å–å¾—äº†ä¼˜äºç°æœ‰ç®—æ³•çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨å…±åŒå™ªå£°çš„æƒ…å†µä¸‹ï¼Œè¯¥ç®—æ³•çš„æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§æ˜æ˜¾ä¼˜äºDRLç‰ˆæœ¬çš„Fictitious Playã€‚åœ¨æŸäº›ä¾‹å­ä¸­ï¼Œè¯¥ç®—æ³•çš„æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡20%ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥ç®—æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºäº¤é€šæµé‡ä¼˜åŒ–ã€ç”µåŠ›èµ„æºåˆ†é…ã€é‡‘èå¸‚åœºå»ºæ¨¡ã€ç¤¾äº¤ç½‘ç»œæ§åˆ¶ç­‰å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚é€šè¿‡å­¦ä¹ äººå£ç›¸å…³çš„çº³ä»€å‡è¡¡ï¼Œå¯ä»¥è®¾è®¡æ›´æœ‰æ•ˆçš„æ§åˆ¶ç­–ç•¥ï¼Œæé«˜ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿åˆ°æ›´å¤æ‚çš„MFGåœºæ™¯ï¼Œä¾‹å¦‚å…·æœ‰å¼‚æ„æ™ºèƒ½ä½“å’Œéçº¿æ€§åŠ¨æ€çš„ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mean Field Games (MFGs) offer a powerful framework for studying large-scale multi-agent systems. Yet, learning Nash equilibria in MFGs remains a challenging problem, particularly when the initial distribution is unknown or when the population is subject to common noise. In this paper, we introduce an efficient deep reinforcement learning (DRL) algorithm designed to achieve population-dependent Nash equilibria without relying on averaging or historical sampling, inspired by Munchausen RL and Online Mirror Descent. The resulting policy is adaptable to various initial distributions and sources of common noise. Through numerical experiments on seven canonical examples, we demonstrate that our algorithm exhibits superior convergence properties compared to state-of-the-art algorithms, particularly a DRL version of Fictitious Play for population-dependent policies. The performance in the presence of common noise underscores the robustness and adaptability of our approach.

