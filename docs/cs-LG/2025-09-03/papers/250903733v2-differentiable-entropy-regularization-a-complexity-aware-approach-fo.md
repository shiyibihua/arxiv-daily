---
layout: default
title: Differentiable Entropy Regularization: A Complexity-Aware Approach for Neural Optimization
---

# Differentiable Entropy Regularization: A Complexity-Aware Approach for Neural Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03733" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03733v2</a>
  <a href="https://arxiv.org/pdf/2509.03733.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03733v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03733v2', 'Differentiable Entropy Regularization: A Complexity-Aware Approach for Neural Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03 (æ›´æ–°: 2025-11-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯å¾®ç†µæ­£åˆ™åŒ–æ–¹æ³•ï¼Œé€šè¿‡é™ä½è¡¨å¾å¤æ‚åº¦æå‡ç¥ç»ç½‘ç»œæ•ˆç‡ä¸é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯å¾®ç†µæ­£åˆ™åŒ–` `è¡¨å¾å¤æ‚åº¦` `ç¥ç»ç½‘ç»œä¼˜åŒ–` `æ¨¡å‹å‹ç¼©` `é²æ£’æ€§` `è®¡ç®—å‡ ä½•` `ç¨€ç–æ€§` `Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¥ç»ç½‘ç»œä¼˜åŒ–æ–¹æ³•é€šå¸¸ä¾§é‡äºæ¶æ„ä¿®æ”¹æˆ–è¾“å‡ºåˆ†å¸ƒï¼Œå¿½ç•¥äº†æ¨¡å‹å†…éƒ¨è¡¨å¾çš„å¤æ‚åº¦ï¼Œå¯¼è‡´æ•ˆç‡æå‡å—é™ã€‚
2. æœ¬æ–‡æå‡ºå¯å¾®ç†µæ­£åˆ™åŒ–æ–¹æ³•ï¼Œç›´æ¥æœ€å°åŒ–è¡¨å¾å¤æ‚åº¦ï¼Œé€šè¿‡æ§åˆ¶ç®—æ³•è¿è¡Œæ—¶é—´ï¼Œå®ç°æ•ˆç‡å’Œé²æ£’æ€§çš„è”åˆä¼˜åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—å‡ ä½•ã€è§†è§‰Transformerå’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå‡èƒ½æœ‰æ•ˆæå‡æ•ˆç‡ï¼ŒåŒæ—¶å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯å¾®çš„èŒƒå›´åˆ†å‰²ç†µè¿‘ä¼¼æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ¥è‡ªè®¡ç®—å‡ ä½•çš„å¤æ‚åº¦åº¦é‡ï¼Œå¯ä»¥ç›´æ¥çº¦æŸç®—æ³•çš„è¿è¡Œæ—¶é—´ã€‚ä¸æ¶æ„ä¿®æ”¹ä¸åŒï¼Œè¯¥æ–¹æ³•æ˜¯ä¸€ç§äº’è¡¥çš„æ­£åˆ™åŒ–å™¨ï¼Œä¸ç°æœ‰ä¼˜åŒ–æ–¹æ³•ç»“åˆä½¿ç”¨æ—¶ï¼Œå¯ä»¥æä¾›æ­£äº¤çš„æ•ˆç‡æå‡ã€‚åœ¨è®¡ç®—å‡ ä½•ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ç†è®ºä¿è¯ï¼Œåœ¨å‡¸åŒ…å’Œä¸‰è§’å‰–åˆ†ä¸Šå®ç°äº†4-5å€çš„åŠ é€Ÿï¼Œè¯¯å·®å°äº0.2%ã€‚åœ¨ImageNet-1Kä¸Šï¼Œä½¿ç”¨ViT-Baseï¼Œç†µæ­£åˆ™åŒ–åœ¨80%ç¨€ç–åº¦ä¸‹å®ç°äº†80.1%çš„top-1å‡†ç¡®ç‡ï¼ˆ1.60å€çš„ç‹¬ç«‹åŠ é€Ÿï¼‰ï¼Œä¸FlashAttentionç»“åˆä½¿ç”¨æ—¶ï¼Œå®ç°äº†2.07å€çš„åŠ é€Ÿï¼Œè€Œå•ç‹¬ä½¿ç”¨FlashAttentionæ—¶ä¸º1.63å€ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLaMA-2 7Bã€Mistral-7Bã€Phi-2ï¼‰ä¸Šï¼Œæˆ‘ä»¬åœ¨70-75%çš„ç¨€ç–åº¦ä¸‹å®ç°äº†1.48-1.60å€çš„æ¨ç†åŠ é€Ÿï¼Œè´¨é‡ä¸‹é™æå°ï¼ˆROUGE-Lä¸‹é™0.3-0.4ç‚¹ï¼Œå›°æƒ‘åº¦å¢åŠ 0.9ï¼‰ã€‚ä¸é’ˆå¯¹è¾“å‡ºåˆ†å¸ƒçš„å…ˆå‰æ­£åˆ™åŒ–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬ç›´æ¥æœ€å°åŒ–è¡¨å¾å¤æ‚åº¦ï¼Œé€šè¿‡è¯­ä¹‰ç»“æ„åŒ–çš„ç¨€ç–æ¨¡å¼ï¼Œæ—¢æé«˜äº†æ•ˆç‡ï¼Œåˆæé«˜äº†é²æ£’æ€§ï¼ˆIoU 0.73 vs 0.41ï¼ŒCIFAR-100-C mCE 48.7 vs 55.4ï¼‰ã€‚è¯¥æ–¹æ³•åœ¨å‡ ä½•å’Œè§†è§‰Transformerä¸Šçš„ä¼˜åŠ¿æœ€ä¸ºæ˜æ˜¾ï¼Œåœ¨LLMä¸Šä¹Ÿæœ‰é€‚åº¦ä½†å¯è¡¡é‡çš„æ”¶ç›Šï¼Œè¡¨æ˜å¤æ‚åº¦æ­£åˆ™åŒ–ä¸ºè”åˆæ•ˆç‡-é²æ£’æ€§ä¼˜åŒ–æä¾›äº†ä¸€æ¡æœ‰åŸåˆ™çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç¥ç»ç½‘ç»œä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚å‰ªæå’Œé‡åŒ–ï¼Œé€šå¸¸å…³æ³¨æ¨¡å‹å‚æ•°çš„ç¨€ç–æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œä½†å¿½ç•¥äº†æ¨¡å‹å†…éƒ¨è¡¨å¾çš„å¤æ‚åº¦ã€‚è¿™ç§å¿½ç•¥å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨æ•ˆç‡æå‡çš„åŒæ—¶ï¼Œé²æ£’æ€§ä¸‹é™ï¼Œå¹¶ä¸”éš¾ä»¥æ‰¾åˆ°æ•ˆç‡å’Œé²æ£’æ€§ä¹‹é—´çš„å¹³è¡¡ç‚¹ã€‚ç°æœ‰æ­£åˆ™åŒ–æ–¹æ³•é€šå¸¸é’ˆå¯¹è¾“å‡ºåˆ†å¸ƒï¼Œé—´æ¥å½±å“è¡¨å¾å¤æ‚åº¦ï¼Œæ•ˆæœæœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥å¯¹æ¨¡å‹å†…éƒ¨è¡¨å¾çš„å¤æ‚åº¦è¿›è¡Œæ­£åˆ™åŒ–ï¼Œé€šè¿‡æœ€å°åŒ–è¡¨å¾çš„ç†µæ¥é™ä½æ¨¡å‹çš„å¤æ‚åº¦ã€‚ä½œè€…è®¤ä¸ºï¼Œé™ä½è¡¨å¾å¤æ‚åº¦å¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—é‡ï¼Œä»è€Œå®ç°æ•ˆç‡å’Œé²æ£’æ€§çš„åŒé‡æå‡ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºæ‰¾åˆ°ä¸€ç§å¯å¾®çš„ç†µåº¦é‡ï¼Œä»¥ä¾¿èƒ½å¤Ÿé€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1ï¼‰é€‰æ‹©éœ€è¦æ­£åˆ™åŒ–çš„æ¨¡å‹å±‚ï¼›2ï¼‰è®¡ç®—è¯¥å±‚è¾“å‡ºè¡¨å¾çš„èŒƒå›´åˆ†å‰²ç†µï¼ˆRange-Partition Entropyï¼ŒRPEï¼‰ï¼›3ï¼‰ä½¿ç”¨å¯å¾®è¿‘ä¼¼æ–¹æ³•è®¡ç®—RPEçš„æ¢¯åº¦ï¼›4ï¼‰å°†RPEä½œä¸ºæ­£åˆ™åŒ–é¡¹æ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­ï¼›5ï¼‰ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚æ•´ä½“æµç¨‹æ˜¯åœ¨æ ‡å‡†ç¥ç»ç½‘ç»œè®­ç»ƒæµç¨‹ä¸­åŠ å…¥ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œè¯¥æ­£åˆ™åŒ–é¡¹ç›´æ¥çº¦æŸæ¨¡å‹å†…éƒ¨è¡¨å¾çš„å¤æ‚åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å¯å¾®çš„èŒƒå›´åˆ†å‰²ç†µï¼ˆRPEï¼‰è¿‘ä¼¼æ–¹æ³•ã€‚ä¼ ç»Ÿçš„RPEè®¡ç®—å¤æ‚åº¦é«˜ï¼Œä¸”ä¸å¯å¾®ï¼Œæ— æ³•ç›´æ¥ç”¨äºç¥ç»ç½‘ç»œçš„ä¼˜åŒ–ã€‚ä½œè€…é€šè¿‡å¼•å…¥å¯å¾®çš„è¿‘ä¼¼è®¡ç®—æ–¹æ³•ï¼Œä½¿å¾—RPEå¯ä»¥ä½œä¸ºæ­£åˆ™åŒ–é¡¹æ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œå¹¶é€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œè¯¥æ–¹æ³•ç›´æ¥é’ˆå¯¹è¡¨å¾å¤æ‚åº¦è¿›è¡Œæ­£åˆ™åŒ–ï¼Œè€Œä¸æ˜¯é—´æ¥é€šè¿‡è¾“å‡ºåˆ†å¸ƒè¿›è¡Œçº¦æŸã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰èŒƒå›´åˆ†å‰²ç†µçš„è®¡ç®—æ–¹å¼ï¼Œä½œè€…é‡‡ç”¨äº†ä¸€ç§åŸºäºè®¡ç®—å‡ ä½•çš„èŒƒå›´åˆ†å‰²æ–¹æ³•æ¥åº¦é‡è¡¨å¾çš„å¤æ‚åº¦ï¼›2ï¼‰å¯å¾®è¿‘ä¼¼æ–¹æ³•çš„é€‰æ‹©ï¼Œä½œè€…é€‰æ‹©äº†ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆè¿‘ä¼¼RPEä¸”æ˜“äºè®¡ç®—æ¢¯åº¦çš„æ–¹æ³•ï¼›3ï¼‰æ­£åˆ™åŒ–ç³»æ•°çš„é€‰æ‹©ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ¨¡å‹è¿›è¡Œè°ƒæ•´ï¼Œä»¥å¹³è¡¡æ•ˆç‡å’Œé²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨ImageNet-1Kä¸Šï¼Œä½¿ç”¨ViT-Baseï¼Œç†µæ­£åˆ™åŒ–åœ¨80%ç¨€ç–åº¦ä¸‹å®ç°äº†80.1%çš„top-1å‡†ç¡®ç‡ï¼Œç›¸æ¯”åŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½åŸºæœ¬ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†1.48-1.60å€çš„æ¨ç†åŠ é€Ÿã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œåœ¨CIFAR-100-Cä¸Šçš„mCEæŒ‡æ ‡é™ä½äº†6.7ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆå’Œé²æ£’çš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œå¯ä»¥å‡å°‘è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæé«˜æ¨ç†é€Ÿåº¦ï¼Œå¹¶å¢å¼ºæ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ï¼Œå¯ä»¥å¸®åŠ©éƒ¨ç½²æ›´é«˜æ•ˆã€æ›´å¯é çš„AIåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce the first differentiable approximation of range-partition entropy, a complexity measure from computational geometry that directly bounds algorithmic runtime. Unlike architectural modifications, our method is a complementary regularizer that provides orthogonal efficiency gains when combined with existing optimizations. We establish theoretical guarantees in computational geometry, achieving 4--5$\times$ provable speedups on convex hull and triangulation with $<$0.2\% error. On ImageNet-1K with ViT-Base, entropy regularization achieves 80.1\% top-1 accuracy at 80\% sparsity (1.60$\times$ standalone speedup), and when combined with FlashAttention yields 2.07$\times$ speedup versus 1.63$\times$ for FlashAttention alone. On large language models (LLaMA-2 7B, Mistral-7B, Phi-2), we achieve 1.48--1.60$\times$ inference speedups at 70--75\% sparsity with minimal quality degradation (ROUGE-L drops of 0.3--0.4 points, perplexity increase of 0.9). Unlike prior regularization methods that target output distributions, we directly minimize representation complexity, yielding both efficiency gains and improved robustness through semantically structured sparsity patterns (IoU 0.73 vs 0.41 for magnitude pruning, CIFAR-100-C mCE 48.7 vs 55.4). Benefits are strongest for geometry and vision transformers, with more modest but measurable gains on LLMs, demonstrating that complexity regularization offers a principled pathway to joint efficiency-robustness optimization.

