---
layout: default
title: Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs
---

# Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13727" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13727v1</a>
  <a href="https://arxiv.org/pdf/2506.13727.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13727v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13727v1', 'Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Reduan Achtibat, Patrick Kahardipraja, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

**å¤‡æ³¨**: Work in progress (10 pages manuscript, 3 pages references, 12 pages appendix)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/erfanhatefi/SparC3)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå½’å› å¼•å¯¼çš„å‰ªææ–¹æ³•ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `å½’å› å¼•å¯¼å‰ªæ` `å±‚æ¬¡ç›¸å…³ä¼ æ’­` `æ¨¡å‹ä¿®æ­£` `ç”µè·¯å‘ç°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§è¯­è¨€æ¨¡å‹çš„åºå¤§å‚æ•°é‡ä½¿å…¶åœ¨å†…å­˜å’Œè®¡ç®—å—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå±‚æ¬¡ç›¸å…³ä¼ æ’­çš„å½’å› å¼•å¯¼å‰ªææ–¹æ³•ï¼Œæ—¨åœ¨è¯†åˆ«å’Œç§»é™¤ä¸ç›¸å…³çš„æ¨¡å‹ç»„ä»¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‹ç¼©æ¨¡å‹çš„åŒæ—¶ï¼Œæ€§èƒ½æŸå¤±æå°ï¼Œä¸”æœ‰æ•ˆå‘ç°å¹¶ä¿®æ­£äº†æ¨¡å‹ä¸­çš„è™šå‡è¡Œä¸ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å½“ä»Šè®¸å¤šäººå·¥æ™ºèƒ½åº”ç”¨ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œä½†å…¶åºå¤§çš„å‚æ•°é‡åœ¨å†…å­˜å’Œè®¡ç®—å—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚è¿‘æœŸçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ç ”ç©¶è¡¨æ˜ï¼Œå½’å› æ–¹æ³•ä¸ä»…å¯ä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œè¿˜èƒ½é€šè¿‡è¯†åˆ«å’Œç§»é™¤ä¸æ¨ç†æ— å…³çš„ç»„ä»¶æ¥å®ç°æ¨¡å‹å‹ç¼©ã€‚æœ¬æ–‡åˆ©ç”¨å±‚æ¬¡ç›¸å…³ä¼ æ’­ï¼ˆLRPï¼‰æ–¹æ³•è¿›è¡ŒLLMsçš„å½’å› å¼•å¯¼å‰ªæï¼Œæ‰©å±•äº†å…¶åœ¨è§†è§‰æ¨¡å‹ä¸­çš„ç»“æ„åŒ–å‰ªæåº”ç”¨è‡³LLMsçš„éç»“æ„åŒ–å‰ªæï¼Œæ˜¾è‘—å‡å°‘æ¨¡å‹å¤§å°ä¸”æ€§èƒ½æŸå¤±æœ€å°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æå–ä»»åŠ¡ç›¸å…³å­å›¾ï¼ˆå³â€œç”µè·¯â€ï¼‰æ–¹é¢å°¤å…¶æœ‰æ•ˆï¼Œè¿™äº›ç”µè·¯å¯ä»¥è¡¨ç¤ºæ ¸å¿ƒåŠŸèƒ½ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ¨¡å‹ä¿®æ­£æŠ€æœ¯ï¼Œé€šè¿‡é€‰æ‹©æ€§ç§»é™¤å¯¼è‡´è™šå‡è¡Œä¸ºçš„ç”µè·¯æ¥æé«˜æ¨¡å‹çš„å®‰å…¨æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¯¹Llamaå’ŒOPTæ¨¡å‹çš„å¹¿æ³›å®éªŒå±•ç¤ºäº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå±€é™æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å†…å­˜å’Œè®¡ç®—å—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ¨¡å‹å‹ç¼©å’Œå¯è§£é‡Šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨å±‚æ¬¡ç›¸å…³ä¼ æ’­ï¼ˆLRPï¼‰è¿›è¡Œå½’å› å¼•å¯¼å‰ªæï¼Œè¯†åˆ«å¹¶ç§»é™¤ä¸æ¨ç†æ— å…³çš„æ¨¡å‹ç»„ä»¶ï¼Œä»è€Œå®ç°æ¨¡å‹çš„æœ‰æ•ˆå‹ç¼©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå½’å› å¼•å¯¼å‰ªææ¨¡å—ã€ä»»åŠ¡ç›¸å…³ç”µè·¯æå–æ¨¡å—å’Œæ¨¡å‹ä¿®æ­£æ¨¡å—ï¼Œåˆ†åˆ«è´Ÿè´£æ¨¡å‹å‹ç¼©ã€æ ¸å¿ƒåŠŸèƒ½è¯†åˆ«å’Œè™šå‡è¡Œä¸ºä¿®æ­£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†LRPæ–¹æ³•æ‰©å±•è‡³LLMsçš„éç»“æ„åŒ–å‰ªæï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹å‹ç¼©çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‰ªæè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„é˜ˆå€¼è®¾ç½®æ¥å†³å®šå“ªäº›å‚æ•°åº”è¢«ç§»é™¤ï¼ŒåŒæ—¶è®¾è®¡äº†æŸå¤±å‡½æ•°ä»¥å¹³è¡¡å‹ç¼©ç‡ä¸æ€§èƒ½æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨å½’å› å¼•å¯¼å‰ªææ–¹æ³•åï¼Œæ¨¡å‹å¤§å°å‡å°‘äº†çº¦30%ï¼Œè€Œæ€§èƒ½æŸå¤±ä¿æŒåœ¨5%ä»¥å†…ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹è™šå‡è¡Œä¸ºçš„ä¿®æ­£æŠ€æœ¯æœ‰æ•ˆé™ä½äº†æ¨¡å‹è¾“å‡ºçš„æœ‰å®³å†…å®¹ï¼Œæå‡äº†æ¨¡å‹çš„å®‰å…¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–éƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼Œå¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ç¯å¢ƒã€‚é€šè¿‡æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­é™ä½è®¡ç®—æˆæœ¬å’Œé£é™©ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are central to many contemporary AI applications, yet their extensive parameter counts pose significant challenges for deployment in memory- and compute-constrained environments. Recent works in eXplainable AI (XAI), particularly on attribution methods, suggest that interpretability can also enable model compression by identifying and removing components irrelevant to inference. In this paper, we leverage Layer-wise Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs. While LRP has shown promise in structured pruning for vision models, we extend it to unstructured pruning in LLMs and demonstrate that it can substantially reduce model size with minimal performance loss. Our method is especially effective in extracting task-relevant subgraphs -- so-called ``circuits'' -- which can represent core functions (e.g., indirect object identification). Building on this, we introduce a technique for model correction, by selectively removing circuits responsible for spurious behaviors (e.g., toxic outputs). All in all, we gather these techniques as a uniform holistic framework and showcase its effectiveness and limitations through extensive experiments for compression, circuit discovery and model correction on Llama and OPT models, highlighting its potential for improving both model efficiency and safety. Our code is publicly available at https://github.com/erfanhatefi/SparC3.

