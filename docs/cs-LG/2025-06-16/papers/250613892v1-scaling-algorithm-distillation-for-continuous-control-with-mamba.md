---
layout: default
title: Scaling Algorithm Distillation for Continuous Control with Mamba
---

# Scaling Algorithm Distillation for Continuous Control with Mamba

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13892" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13892v1</a>
  <a href="https://arxiv.org/pdf/2506.13892.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13892v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13892v1', 'Scaling Algorithm Distillation for Continuous Control with Mamba')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Samuel Beaussant, Mehdi Mounsif

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMambaä»¥è§£å†³ç®—æ³•è’¸é¦åœ¨è¿ç»­æ§åˆ¶ä¸­çš„æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç®—æ³•è’¸é¦` `è¿ç»­æ§åˆ¶` `é€‰æ‹©æ€§ç»“æ„` `é•¿åºåˆ—å»ºæ¨¡` `å…ƒå¼ºåŒ–å­¦ä¹ ` `è‡ªå›å½’æ¨¡å‹` `å˜æ¢å™¨æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç®—æ³•è’¸é¦æ–¹æ³•åœ¨å¤„ç†é•¿æ—¶é—´åºåˆ—æ—¶å—åˆ°å˜æ¢å™¨æ¨¡å‹çš„å¹³æ–¹å¤æ‚åº¦é™åˆ¶ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚
2. æœ¬æ–‡æå‡ºMambaæ¨¡å‹ï¼Œåˆ©ç”¨é€‰æ‹©æ€§ç»“æ„çŠ¶æ€ç©ºé—´åºåˆ—ï¼ˆS6ï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨é•¿åºåˆ—å»ºæ¨¡ä¸­å®ç°çº¿æ€§æ‰©å±•ï¼Œæå‡ç®—æ³•è’¸é¦æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMambaåœ¨å››ä¸ªå¤æ‚çš„è¿ç»­å…ƒå¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­è¡¨ç°ä¼˜è¶Šï¼Œä¸”åœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹çš„ICRLæ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç®—æ³•è’¸é¦ï¼ˆADï¼‰æœ€è¿‘è¢«æå‡ºä½œä¸ºä¸€ç§é€šè¿‡å› æœå˜æ¢å™¨æ¨¡å‹è‡ªå›å½’åœ°å»ºæ¨¡è·¨æƒ…èŠ‚è®­ç»ƒå†å²çš„æ–°æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºæ³¨æ„åŠ›æœºåˆ¶å¼•å‘çš„å®é™…é™åˆ¶ï¼Œå®éªŒå—åˆ°å˜æ¢å™¨çš„å¹³æ–¹å¤æ‚åº¦çš„ç“¶é¢ˆï¼Œé™åˆ¶åœ¨ç®€å•çš„ç¦»æ•£ç¯å¢ƒå’ŒçŸ­æ—¶é—´èŒƒå›´å†…ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨æœ€è¿‘æå‡ºçš„é€‰æ‹©æ€§ç»“æ„çŠ¶æ€ç©ºé—´åºåˆ—ï¼ˆS6ï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨é•¿åºåˆ—å»ºæ¨¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨åºåˆ—é•¿åº¦ä¸Šçº¿æ€§æ‰©å±•ã€‚é€šè¿‡å››ä¸ªå¤æ‚çš„è¿ç»­å…ƒå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäºS6å±‚æ„å»ºçš„Mambaæ¨¡å‹åœ¨ADä»»åŠ¡ä¸Šç›¸è¾ƒäºå˜æ¢å™¨æ¨¡å‹çš„æ•´ä½“ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå°†ADæ‰©å±•åˆ°éå¸¸é•¿çš„ä¸Šä¸‹æ–‡å¯ä»¥æé«˜ICRLæ€§èƒ½ï¼Œä½¿å…¶åœ¨ä¸æœ€å…ˆè¿›çš„åœ¨çº¿å…ƒRLåŸºçº¿ç«äº‰æ—¶æ›´å…·ç«äº‰åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç®—æ³•è’¸é¦ï¼ˆADï¼‰æ–¹æ³•åœ¨é•¿æ—¶é—´åºåˆ—å»ºæ¨¡ä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œå°¤å…¶æ˜¯å˜æ¢å™¨æ¨¡å‹çš„å¹³æ–¹å¤æ‚åº¦é™åˆ¶å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºMambaæ¨¡å‹ï¼ŒåŸºäºé€‰æ‹©æ€§ç»“æ„çŠ¶æ€ç©ºé—´åºåˆ—ï¼ˆS6ï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨é•¿åºåˆ—å»ºæ¨¡ä¸­å®ç°çº¿æ€§æ‰©å±•ï¼Œä»è€Œæé«˜ç®—æ³•è’¸é¦çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMambaæ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªS6å±‚ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿æ—¶é—´åºåˆ—æ•°æ®ã€‚æ¨¡å‹é€šè¿‡è‡ªå›å½’æ–¹å¼å»ºæ¨¡è·¨æƒ…èŠ‚çš„è®­ç»ƒå†å²ï¼Œä¼˜åŒ–äº†ä¿¡æ¯çš„ä¼ é€’å’Œåˆ©ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šMambaæ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥S6æ¨¡å‹ï¼Œçªç ´äº†ä¼ ç»Ÿå˜æ¢å™¨æ¨¡å‹çš„å¤æ‚åº¦é™åˆ¶ï¼Œä½¿å¾—åœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹çš„ç®—æ³•è’¸é¦æˆä¸ºå¯èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒS6å±‚çš„é€‰æ‹©æ€§ç»“æ„å…è®¸æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶ä¿æŒçº¿æ€§å¤æ‚åº¦ï¼Œæ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥ç»è¿‡ä¼˜åŒ–ï¼Œä»¥é€‚åº”è¿ç»­æ§åˆ¶ä»»åŠ¡çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMambaæ¨¡å‹åœ¨å››ä¸ªå¤æ‚çš„è¿ç»­å…ƒå¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­è¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿå˜æ¢å™¨æ¨¡å‹ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨é•¿ä¸Šä¸‹æ–‡çš„ICRLä»»åŠ¡ä¸­ï¼ŒMambaçš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„åœ¨çº¿å…ƒRLåŸºçº¿ç›¸ç«äº‰ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰éœ€è¦å¤„ç†å¤æ‚å†³ç­–å’Œé•¿æ—¶é—´åºåˆ—çš„åœºæ™¯ã€‚é€šè¿‡æé«˜ç®—æ³•è’¸é¦çš„æ•ˆç‡ï¼ŒMambaæ¨¡å‹èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ å’Œå†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.

