---
layout: default
title: Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models
---

# Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13923" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13923v2</a>
  <a href="https://arxiv.org/pdf/2506.13923.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13923v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13923v2', 'Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vaskar Nath, Elaine Lau, Anisha Gunjal, Manasi Sharma, Nikhil Baharte, Sean Hendryx

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-16 (æ›´æ–°: 2025-06-20)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”å¼•å¯¼æ–¹æ³•ä»¥åŠ é€Ÿæ¨ç†æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªé€‚åº”å¼•å¯¼` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†æ¨¡å‹` `èƒ½åŠ›æå‡` `è‡ªè’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†æ¨¡å‹åœ¨è§£å†³æ–°é—®é¢˜æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å¤æ‚åº¦ä»»åŠ¡ä¸­ã€‚
2. è®ºæ–‡æå‡ºçš„$	ext{Guide}$æ–¹æ³•é€šè¿‡è‡ªé€‚åº”å¼•å¯¼æ¨¡å‹å­¦ä¹ æ–°é—®é¢˜ï¼Œç»“åˆè‡ªç„¶è¯­è¨€æç¤ºä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œ$	ext{Guide}$åœ¨7Bå’Œ32Bå‚æ•°æ¨¡å‹ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†æœ€å¤š4%çš„å®å¹³å‡æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†é€šè¿‡å¯éªŒè¯å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„æ¨ç†æ¨¡å‹å¦‚ä½•å­¦ä¹ è§£å†³æ–°é—®é¢˜çš„è¿‡ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼Œå¼ºåŒ–å­¦ä¹ é©±åŠ¨æ€§èƒ½çš„ä¸»è¦æ–¹å¼æœ‰ä¸¤ç§ï¼šä¸€æ˜¯å°†pass@$k$å‹ç¼©ä¸ºpass@1ï¼ŒäºŒæ˜¯é€šè¿‡â€œèƒ½åŠ›æå‡â€ä½¿æ¨¡å‹èƒ½å¤Ÿè§£å†³ä¹‹å‰æ— æ³•è§£å†³çš„æ–°é—®é¢˜ã€‚å°½ç®¡èƒ½åŠ›æå‡åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­æ™®éå­˜åœ¨ï¼Œä½†å­¦ä¹ æ–°é—®é¢˜ä¸»è¦ä¾èµ–è‡ªè’¸é¦ã€‚æˆ‘ä»¬åœ¨è¶…è¿‡500,000ä¸ªæ•°å­¦ã€ç§‘å­¦å’Œä»£ç é¢†åŸŸçš„æ¨ç†é—®é¢˜ä¸ŠéªŒè¯äº†è¿™äº›å‘ç°ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åœ¨çº¿è®­ç»ƒç®—æ³•$	ext{Guide}$ï¼Œè¯¥ç®—æ³•é€šè¿‡è‡ªé€‚åº”åœ°å°†æç¤ºèå…¥æ¨¡å‹ä¸Šä¸‹æ–‡ä¸­æ¥ä¼˜åŒ–ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†$	ext{Guide}$åœ¨ä¸åŒå‚æ•°è§„æ¨¡æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æé«˜äº†é€šç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³æ¨ç†æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹æ–°é—®é¢˜çš„å­¦ä¹ èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é«˜å¤æ‚åº¦ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹æœªè§è¿‡çš„æƒ…å¢ƒä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªé€‚åº”å¼•å¯¼ï¼ˆ$	ext{Guide}$ï¼‰å°†è‡ªç„¶è¯­è¨€æç¤ºèå…¥æ¨¡å‹ä¸Šä¸‹æ–‡ä¸­ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è§£å†³æ–°é—®é¢˜æ—¶æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å·²æœ‰çŸ¥è¯†ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œè§£å†³èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹çš„è‡ªè’¸é¦è¿‡ç¨‹ã€å¼•å¯¼æç¤ºçš„é›†æˆä»¥åŠé‡è¦æ€§é‡‡æ ·çš„è°ƒæ•´ã€‚å…·ä½“æµç¨‹ä¸ºï¼šé¦–å…ˆè¯†åˆ«æ‰€æœ‰åˆå§‹é”™è¯¯çš„å›åˆï¼Œç„¶åå°†æç¤ºä¿¡æ¯èå…¥ä¸Šä¸‹æ–‡ä¸­ï¼Œæœ€åä¼˜åŒ–ç­–ç•¥ä»¥é€‚åº”æ²¡æœ‰æç¤ºçš„æƒ…å¢ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†è‡ªé€‚åº”å¼•å¯¼æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹åœ¨é¢å¯¹æ–°é—®é¢˜æ—¶èƒ½å¤ŸåŠ¨æ€è°ƒæ•´å­¦ä¹ ç­–ç•¥ï¼Œä¸ä¼ ç»Ÿçš„é™æ€å­¦ä¹ æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œ$	ext{Guide}$æ–¹æ³•é’ˆå¯¹ä¸åŒæ¨¡å‹è§„æ¨¡ï¼ˆå¦‚7Bå’Œ32Bå‚æ•°ï¼‰è¿›è¡Œäº†ä¼˜åŒ–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†å¼•å¯¼æç¤ºçš„å½±å“ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†è‡ªè’¸é¦æœºåˆ¶ä»¥æå‡å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨$	ext{Guide}$æ–¹æ³•çš„7Bå’Œ32Bå‚æ•°æ¨¡å‹åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æé«˜äº†æœ€å¤š4%çš„å®å¹³å‡æ€§èƒ½ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç§‘å­¦è®¡ç®—å’Œç¼–ç¨‹è¾…åŠ©ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©æ¨ç†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­æ›´å¥½åœ°ç†è§£å’Œè§£å†³é—®é¢˜ã€‚æœªæ¥ï¼Œè¿™ç§è‡ªé€‚åº”å¼•å¯¼æ–¹æ³•å¯èƒ½ä¼šåœ¨æ›´å¹¿æ³›çš„äººå·¥æ™ºèƒ½åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæå‡æ¨¡å‹çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance in two main ways: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B parameters on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\text{Guide}$ -- a new class of online training algorithms. $\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.

