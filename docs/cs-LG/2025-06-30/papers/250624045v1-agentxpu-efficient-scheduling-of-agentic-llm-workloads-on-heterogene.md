---
layout: default
title: Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC
---

# Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.24045" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.24045v1</a>
  <a href="https://arxiv.org/pdf/2506.24045.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.24045v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.24045v1', 'Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinming Wei, Jiahao Zhang, Haoran Li, Jiayu Chen, Rui Qu, Maoliang Li, Xiang Chen, Guojie Luo

**åˆ†ç±»**: cs.DC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAgent.xpuä»¥é«˜æ•ˆè°ƒåº¦å¼‚æ„SoCä¸Šçš„æ™ºèƒ½LLMå·¥ä½œè´Ÿè½½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ™ºèƒ½è¯­è¨€æ¨¡å‹` `å¼‚æ„è®¡ç®—` `ä»»åŠ¡è°ƒåº¦` `ä½å»¶è¿Ÿå“åº”` `ååé‡ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è®¾å¤‡ç«¯LLMå¼•æ“æ— æ³•æœ‰æ•ˆç®¡ç†ååº”æ€§å’Œä¸»åŠ¨æ€§ä»»åŠ¡çš„å¹¶å‘è¯·æ±‚ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚
2. Agent.xpué€šè¿‡æ„å»ºå¼‚æ„æ‰§è¡Œå›¾å’Œåœ¨çº¿è°ƒåº¦å™¨ï¼Œä¼˜åŒ–äº†ååº”æ€§å’Œä¸»åŠ¨æ€§ä»»åŠ¡çš„è°ƒåº¦ç­–ç•¥ã€‚
3. åœ¨Intel Core Ultra SoCä¸Šï¼ŒAgent.xpuåœ¨ååº”æ€§ä»»åŠ¡ä¸Šé™ä½äº†4.6å€å»¶è¿Ÿï¼Œå¹¶åœ¨ä¸»åŠ¨ä»»åŠ¡ä¸Šæé«˜äº†1.6åˆ°6.8å€çš„ååé‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ™ºèƒ½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸ªäººè®¾å¤‡ä¸Šçš„æ™®åŠï¼Œå‡ºç°äº†ä¸€ç±»æ–°å·¥ä½œè´Ÿè½½ï¼Œå…¶ç‰¹ç‚¹æ˜¯ç›®æ ‡çš„äºŒåˆ†æ€§ã€‚ç”¨æˆ·å‘èµ·çš„ååº”æ€§ä»»åŠ¡éœ€è¦å³æ—¶ã€ä½å»¶è¿Ÿçš„å“åº”ï¼Œè€Œä¸»åŠ¨ä»»åŠ¡åˆ™åœ¨åå°è¿è¡Œï¼Œä¼˜å…ˆè€ƒè™‘ååé‡ã€‚ç°æœ‰çš„è®¾å¤‡ç«¯LLMå¼•æ“è®¾è®¡ç”¨äºå­¤ç«‹æ¨ç†ï¼Œæ— æ³•æœ‰æ•ˆç®¡ç†è¿™äº›åœ¨æ¶ˆè´¹çº§å¼‚æ„ç³»ç»ŸèŠ¯ç‰‡ï¼ˆSoCï¼‰ä¸Šå¹¶å‘ä¸”ç›¸äº’å†²çªçš„è¯·æ±‚ã€‚æœ¬æ–‡æå‡ºäº†Agent.xpuï¼Œä¸€ä¸ªé«˜æ•ˆçš„æœåŠ¡ç³»ç»Ÿï¼Œæ—¨åœ¨å†…å­˜ç»Ÿä¸€çš„å¼‚æ„SoCä¸Šå¤„ç†æ™ºèƒ½LLMå·¥ä½œè´Ÿè½½ã€‚é€šè¿‡ä¸“é—¨çš„ç¦»çº¿åˆ†æï¼ŒAgent.xpué¦–å…ˆæ„å»ºå¼‚æ„æ‰§è¡Œå›¾ï¼Œèåˆå’Œåˆ†å—æ¨¡å‹å†…æ ¸ï¼Œä»¥å®ç°åŸºäºäº²å’Œæ€§çš„å¼¹æ€§åŠ é€Ÿå™¨æ˜ å°„å’Œé¢„æµ‹å†…æ ¸æ³¨é‡Šã€‚åœ¨è¿è¡Œæ—¶ï¼Œå…¶åœ¨çº¿è°ƒåº¦å™¨æ”¯æŒç»†ç²’åº¦çš„å†…æ ¸çº§æŠ¢å ï¼Œä»¥ä¿è¯ååº”æ€§ä»»åŠ¡çš„å“åº”æ€§ã€‚ä¸ºäº†æœ€å¤§åŒ–SoCåˆ©ç”¨ç‡ï¼Œé‡‡ç”¨äº†åŸºäºæ¾å¼›çš„å†…æ ¸å›å¡«ç­–ç•¥ï¼Œæœºä¼šæ€§åœ°é™„åŠ ä¸»åŠ¨ä»»åŠ¡ï¼Œå¹¶é€šè¿‡å¸¦å®½æ„ŸçŸ¥è°ƒåº¦æ¥ç¼“è§£NPUä¸iGPUä¹‹é—´çš„ç«äº‰ã€‚å¯¹Intel Core Ultra SoCçš„è¯„ä¼°è¡¨æ˜ï¼ŒAgent.xpuåœ¨ååº”æ€§ä»»åŠ¡ä¸Šå®ç°äº†4.6å€çš„å»¶è¿Ÿé™ä½ï¼Œå¹¶åœ¨ä¸»åŠ¨ä»»åŠ¡ä¸Šç»´æŒäº†1.6åˆ°6.8å€çš„ååé‡æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æ¶ˆè´¹çº§å¼‚æ„SoCä¸Šï¼Œæ™ºèƒ½LLMå·¥ä½œè´Ÿè½½ä¸­ååº”æ€§ä»»åŠ¡ä¸ä¸»åŠ¨æ€§ä»»åŠ¡å¹¶å‘è°ƒåº¦çš„æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆç®¡ç†è¿™ç±»ä»»åŠ¡çš„å†²çªï¼Œå¯¼è‡´å“åº”æ€§å’Œååé‡çš„ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAgent.xpuçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æ„å»ºå¼‚æ„æ‰§è¡Œå›¾å’Œå®æ–½ç»†ç²’åº¦çš„å†…æ ¸çº§æŠ¢å ï¼Œæ¥ä¼˜åŒ–ååº”æ€§å’Œä¸»åŠ¨æ€§ä»»åŠ¡çš„è°ƒåº¦ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAgent.xpuçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç¦»çº¿åˆ†ææ¨¡å—ã€å¼‚æ„æ‰§è¡Œå›¾æ„å»ºæ¨¡å—ã€åœ¨çº¿è°ƒåº¦å™¨å’Œä»»åŠ¡è°ƒåº¦ç­–ç•¥ã€‚ç¦»çº¿åˆ†æç”¨äºç”Ÿæˆæ‰§è¡Œå›¾ï¼Œåœ¨çº¿è°ƒåº¦å™¨åˆ™è´Ÿè´£åŠ¨æ€è°ƒåº¦ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åŸºäºäº²å’Œæ€§çš„å¼¹æ€§åŠ é€Ÿå™¨æ˜ å°„å’Œå¸¦å®½æ„ŸçŸ¥è°ƒåº¦ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†ååº”æ€§ä»»åŠ¡çš„å“åº”æ€§å’Œä¸»åŠ¨æ€§ä»»åŠ¡çš„ååé‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒAgent.xpué‡‡ç”¨äº†æ¾å¼›æ„ŸçŸ¥çš„å†…æ ¸å›å¡«ç­–ç•¥ï¼Œå¹¶å®ç°äº†åœ¨çº¿è°ƒåº¦å™¨çš„ç»†ç²’åº¦å†…æ ¸çº§æŠ¢å ï¼Œä»¥ç¡®ä¿ååº”æ€§ä»»åŠ¡çš„åŠæ—¶å“åº”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAgent.xpuåœ¨ååº”æ€§ä»»åŠ¡ä¸Šå®ç°äº†4.6å€çš„å»¶è¿Ÿé™ä½ï¼ŒåŒæ—¶åœ¨ä¸»åŠ¨ä»»åŠ¡ä¸Šç»´æŒäº†1.6åˆ°6.8å€çš„ååé‡æå‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ¨ç†å¼•æ“ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¼ºå¤§æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æ‰‹æœºã€è¾¹ç¼˜è®¡ç®—è®¾å¤‡å’Œå…¶ä»–ä¸ªäººæ™ºèƒ½è®¾å¤‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç”¨æˆ·ä½“éªŒï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¿«é€Ÿå“åº”çš„åº”ç”¨åœºæ™¯ä¸­ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹å’Œå®æ—¶ç¿»è¯‘ç­‰ã€‚æœªæ¥ï¼ŒAgent.xpuå¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šæ™ºèƒ½åº”ç”¨çš„æ™®åŠï¼Œæå‡è®¾å¤‡çš„è®¡ç®—æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The proliferation of agentic Large Language Models (LLMs) on personal devices introduces a new class of workloads characterized by a dichotomy of objectives. Reactive tasks, initiated by users, demand immediate, low-latency responses, while proactive tasks operate invisibly and prioritize throughput. Existing on-device LLM engines, designed for isolated inferences, fail to efficiently manage these concurrent and conflicting requests on consumer-grade heterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces Agent.xpu, an efficient serving system for agentic LLM workloads on memory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu first constructs a heterogeneous execution graph, which fuses and chunks model kernels for affinity-guided, elastic accelerator mapping with predictive kernel annotation. At runtime, its online scheduler enables fine-grained, kernel-level preemption to guarantee the responsiveness of reactive tasks. To maximize SoC utilization, it adopts slack-aware kernel backfill to opportunistically append proactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware dispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves 4.6$\times$ lower latency for reactive tasks and sustains 1.6$\times$-6.8$\times$ higher throughput for proactive tasks compared to state-of-the-art inference engines.

