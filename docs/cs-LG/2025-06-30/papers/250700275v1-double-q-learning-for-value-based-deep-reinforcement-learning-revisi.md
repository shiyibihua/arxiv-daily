---
layout: default
title: Double Q-learning for Value-based Deep Reinforcement Learning, Revisited
---

# Double Q-learning for Value-based Deep Reinforcement Learning, Revisited

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00275" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00275v1</a>
  <a href="https://arxiv.org/pdf/2507.00275.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00275v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00275v1', 'Double Q-learning for Value-based Deep Reinforcement Learning, Revisited')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Prabhat Nagarajan, Martha White, Marlos C. Machado

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: 44 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ·±åº¦åŒQå­¦ä¹ ä»¥è§£å†³Qå­¦ä¹ è¿‡åº¦ä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `åŒQå­¦ä¹ ` `è¿‡åº¦ä¼°è®¡` `æ·±åº¦Qç½‘ç»œ` `Atariæ¸¸æˆ` `æ™ºèƒ½ä½“å­¦ä¹ ` `ç»éªŒå›æ”¾` `å°æ‰¹é‡é‡‡æ ·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒæ·±åº¦Qç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†åˆ©ç”¨ä¸¤ä¸ªQå‡½æ•°ï¼Œå¯¼è‡´è¿‡åº¦ä¼°è®¡é—®é¢˜ä¾ç„¶å­˜åœ¨ã€‚
2. æœ¬æ–‡æå‡ºæ·±åº¦åŒQå­¦ä¹ ï¼ˆDDQLï¼‰ï¼Œé€šè¿‡è®­ç»ƒä¸¤ä¸ªç›¸äº’å¼•å¯¼çš„Qå‡½æ•°æ¥å‡å°‘è¿‡åº¦ä¼°è®¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDDQLåœ¨57ä¸ªAtari 2600æ¸¸æˆä¸­è¡¨ç°ä¼˜äºåŒæ·±åº¦Qç½‘ç»œï¼Œä¸”æ— éœ€é¢å¤–çš„è¶…å‚æ•°è®¾ç½®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‡åº¦ä¼°è®¡åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æ™®éå­˜åœ¨ï¼Œå°¤å…¶æ˜¯åœ¨Qå­¦ä¹ ä¸­ã€‚åŒQå­¦ä¹ ç®—æ³•æ—¨åœ¨é€šè¿‡è®­ç»ƒä¸¤ä¸ªQå‡½æ•°æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä»è€Œå»ç›¸å…³åŒ–åŠ¨ä½œé€‰æ‹©å’ŒåŠ¨ä½œè¯„ä¼°ã€‚å°½ç®¡åŒQå­¦ä¹ å·²è¢«é€‚é…åˆ°æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä½†ç°æœ‰çš„åŒæ·±åº¦Qç½‘ç»œï¼ˆDouble DQNï¼‰å¹¶æœªå……åˆ†åˆ©ç”¨ä¸¤ä¸ªQå‡½æ•°çš„è®­ç»ƒã€‚æœ¬æ–‡æå‡ºæ·±åº¦åŒQå­¦ä¹ ï¼ˆDDQLï¼‰ï¼Œæ—¨åœ¨æ¢è®¨å…¶æ˜¯å¦èƒ½å‡å°‘è¿‡åº¦ä¼°è®¡å¹¶åœ¨æ€§èƒ½ä¸Šä¼˜äºåŒæ·±åº¦Qç½‘ç»œã€‚ç ”ç©¶è¡¨æ˜ï¼ŒDDQLåœ¨57ä¸ªAtari 2600æ¸¸æˆä¸­è¡¨ç°ä¼˜äºåŒæ·±åº¦Qç½‘ç»œï¼Œä¸”æ— éœ€é¢å¤–çš„è¶…å‚æ•°è®¾ç½®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­Qå­¦ä¹ ç®—æ³•çš„è¿‡åº¦ä¼°è®¡é—®é¢˜ã€‚ç°æœ‰çš„åŒæ·±åº¦Qç½‘ç»œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨ä¸¤ä¸ªQå‡½æ•°çš„è®­ç»ƒï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ·±åº¦åŒQå­¦ä¹ ï¼ˆDDQLï¼‰é€šè¿‡åŒæ—¶è®­ç»ƒä¸¤ä¸ªQå‡½æ•°ï¼Œä½¿å¾—åŠ¨ä½œé€‰æ‹©ä¸åŠ¨ä½œè¯„ä¼°ç›¸äº’å»ç›¸å…³ï¼Œä»è€Œå‡å°‘è¿‡åº¦ä¼°è®¡ç°è±¡ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°åŠ¨ä½œçš„ä»·å€¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDDQLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªQç½‘ç»œï¼Œè¿™ä¸¤ä¸ªç½‘ç»œç›¸äº’å¼•å¯¼è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ç»éªŒå›æ”¾æœºåˆ¶æ¥æé«˜æ ·æœ¬æ•ˆç‡ï¼Œå¹¶é‡‡ç”¨å°æ‰¹é‡é‡‡æ ·ç­–ç•¥æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šDDQLçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¯¹åŒQå­¦ä¹ æ ¸å¿ƒæ€æƒ³çš„å…¨é¢é€‚é…ï¼Œå……åˆ†åˆ©ç”¨ä¸¤ä¸ªQå‡½æ•°çš„è®­ç»ƒï¼Œè€Œä¸ä»…ä»…æ˜¯æ¾æ•£çš„é€‚é…ã€‚è¿™ä¸€è®¾è®¡æ˜¾è‘—æ”¹å–„äº†Qå€¼çš„ä¼°è®¡ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šDDQLåœ¨ç½‘ç»œç»“æ„ä¸Šé‡‡ç”¨äº†æ ‡å‡†çš„æ·±åº¦Qç½‘ç»œæ¶æ„ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†ä¸¤ä¸ªQå‡½æ•°çš„äº¤äº’å½±å“ã€‚å…³é”®å‚æ•°è®¾ç½®ä¸Šï¼ŒDDQLä¸éœ€è¦é¢å¤–çš„è¶…å‚æ•°ï¼Œç®€åŒ–äº†æ¨¡å‹çš„è°ƒä¼˜è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ·±åº¦åŒQå­¦ä¹ åœ¨57ä¸ªAtari 2600æ¸¸æˆä¸­çš„è¡¨ç°ä¼˜äºåŒæ·±åº¦Qç½‘ç»œï¼Œæ•´ä½“æ€§èƒ½æå‡æ˜¾è‘—ï¼Œä¸”ä¸éœ€è¦é¢å¤–çš„è¶…å‚æ•°è®¾ç½®ã€‚è¿™ä¸€å‘ç°éªŒè¯äº†DDQLåœ¨å‡å°‘è¿‡åº¦ä¼°è®¡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå…·æœ‰é‡è¦çš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

æ·±åº¦åŒQå­¦ä¹ ï¼ˆDDQLï¼‰åœ¨æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡å‡å°‘è¿‡åº¦ä¼°è®¡ï¼ŒDDQLèƒ½å¤Ÿæé«˜æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ å’Œæ›´ä¼˜çš„æ€§èƒ½è¡¨ç°ã€‚æœªæ¥ï¼ŒDDQLçš„æ€æƒ³ä¹Ÿå¯èƒ½è¢«æ‰©å±•åˆ°å…¶ä»–å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Overestimation is pervasive in reinforcement learning (RL), including in Q-learning, which forms the algorithmic basis for many value-based deep RL algorithms. Double Q-learning is an algorithm introduced to address Q-learning's overestimation by training two Q-functions and using both to de-correlate action-selection and action-evaluation in bootstrap targets. Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks (DQN), Double Q-learning was adapted to deep RL in the form of Double DQN. However, Double DQN only loosely adapts Double Q-learning, forgoing the training of two different Q-functions that bootstrap off one another. In this paper, we study algorithms that adapt this core idea of Double Q-learning for value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our aim is to understand whether DDQL exhibits less overestimation than Double DQN and whether performant instantiations of DDQL exist. We answer both questions affirmatively, demonstrating that DDQL reduces overestimation and outperforms Double DQN in aggregate across 57 Atari 2600 games, without requiring additional hyperparameters. We also study several aspects of DDQL, including its network architecture, replay ratio, and minibatch sampling strategy.

