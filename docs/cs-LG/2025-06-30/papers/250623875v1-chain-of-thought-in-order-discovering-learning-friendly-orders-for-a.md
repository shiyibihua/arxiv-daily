---
layout: default
title: Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic
---

# Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23875" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23875v1</a>
  <a href="https://arxiv.org/pdf/2506.23875.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23875v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23875v1', 'Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuta Sato, Kazuhiko Kawamoto, Hiroshi Kera

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: 14 pages, 10 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå­¦ä¹ å‹å¥½çš„é¡ºåºä»¥ä¼˜åŒ–Transformerçš„ç®—æœ¯æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Transformer` `ç®—æœ¯æ¨ç†` `è¾“å…¥é¡ºåº` `å­¦ä¹ å‹å¥½` `å±‚æ¬¡åŒ–æ–¹æ³•` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç®—æœ¯æ¨ç†æ—¶ï¼Œæœªå……åˆ†è€ƒè™‘æ¨ç†æ­¥éª¤çš„é¡ºåºå¯¹å­¦ä¹ æ•ˆæœçš„å½±å“ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡é‡æ–°æ’åºè§£ç å™¨è¾“å…¥ä»¤ç‰Œï¼Œå½¢æˆé€‚åˆå­¦ä¹ çš„é¡ºåºï¼Œä»¥ä¼˜åŒ–Transformerçš„ç®—æœ¯å­¦ä¹ è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªå¯¹é¡ºåºæ•æ„Ÿçš„ç®—æœ¯ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨ä¹˜æ³•ä»»åŠ¡ä¸­æ¢å¤äº†æœ‰æ•ˆçš„åå‘æ•°å­—é¡ºåºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“¾å¼æ€ç»´æ˜¯Transformeræ¨¡å‹ä¸­é€æ­¥æ¨ç†çš„åŸºç¡€ï¼Œæ¨ç†æ­¥éª¤çš„é¡ºåºå¯¹éš¾åº¦æœ‰é‡è¦å½±å“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œå³é‡æ–°æ’åºè§£ç å™¨è¾“å…¥ä»¤ç‰Œï¼Œä»¥å½¢æˆé€‚åˆå­¦ä¹ çš„é¡ºåºï¼Œä»è€Œå¸®åŠ©Transformerå­¦ä¹ ç®—æœ¯ä»»åŠ¡ã€‚ç ”ç©¶é¦–å…ˆåœ¨ä¸åŒé¡ºåºæ’åˆ—çš„ç›®æ ‡åºåˆ—æ··åˆä¸Šè®­ç»ƒTransformerï¼Œç„¶åè¯†åˆ«å‡ºåœ¨æ—©æœŸé˜¶æ®µæŸå¤±å¿«é€Ÿä¸‹é™çš„è‰¯æ€§é¡ºåºã€‚ç”±äºæœç´¢ç©ºé—´éšç€åºåˆ—é•¿åº¦å‘ˆé˜¶ä¹˜å¢é•¿ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„å±‚æ¬¡åŒ–æ–¹æ³•è¿›è¡Œå—é—´å’Œå—å†…çš„é‡æ–°æ’åºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»æ•°åäº¿ä¸ªå€™é€‰ä¸­è¯†åˆ«å‡ºå­¦ä¹ å‹å¥½çš„é¡ºåºï¼Œå°¤å…¶åœ¨ä¹˜æ³•ä»»åŠ¡ä¸­æ¢å¤äº†å…ˆå‰ç ”ç©¶ä¸­æŠ¥å‘Šçš„åå‘æ•°å­—é¡ºåºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³Transformeråœ¨ç®—æœ¯æ¨ç†ä¸­å› è¾“å…¥é¡ºåºä¸å½“å¯¼è‡´çš„å­¦ä¹ å›°éš¾ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œåˆ©ç”¨å­¦ä¹ å‹å¥½çš„è¾“å…¥é¡ºåºï¼Œå½±å“äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é‡æ–°æ’åºè§£ç å™¨è¾“å…¥ä»¤ç‰Œï¼Œå½¢æˆé€‚åˆå­¦ä¹ çš„é¡ºåºã€‚è¯¥è®¾è®¡æ—¨åœ¨é€šè¿‡ä¼˜åŒ–è¾“å…¥é¡ºåºæ¥åŠ é€Ÿæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œé™ä½æ¨ç†éš¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆåœ¨ä¸åŒé¡ºåºçš„ç›®æ ‡åºåˆ—ä¸Šè®­ç»ƒTransformerï¼Œç„¶åé€šè¿‡æŸå¤±ä¸‹é™é€Ÿåº¦è¯†åˆ«è‰¯æ€§é¡ºåºã€‚é‡‡ç”¨å±‚æ¬¡åŒ–æ–¹æ³•è¿›è¡Œå—é—´å’Œå—å†…çš„é‡æ–°æ’åºï¼Œä»¥åº”å¯¹æœç´¢ç©ºé—´çš„æŒ‡æ•°å¢é•¿ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ä¸¤é˜¶æ®µå±‚æ¬¡åŒ–æ’åºæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ•°åäº¿ä¸ªå€™é€‰ä¸­è¯†åˆ«å‡ºå­¦ä¹ å‹å¥½çš„é¡ºåºã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„éšæœºæˆ–å›ºå®šé¡ºåºè¾“å…¥æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œæ—©åœç­–ç•¥ä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸ºå…³æ³¨æ—©æœŸé˜¶æ®µçš„æŸå¤±å˜åŒ–ï¼Œä»¥ä¾¿å¿«é€Ÿè¯†åˆ«æœ‰æ•ˆé¡ºåºã€‚ç½‘ç»œç»“æ„ä¸Šï¼ŒTransformerçš„è§£ç å™¨éƒ¨åˆ†è¿›è¡Œäº†é€‚å½“çš„è°ƒæ•´ï¼Œä»¥æ”¯æŒè¾“å…¥é¡ºåºçš„åŠ¨æ€å˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨å››ä¸ªç®—æœ¯ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨ä¹˜æ³•ä»»åŠ¡ä¸­ï¼ŒæˆåŠŸæ¢å¤äº†åå‘æ•°å­—é¡ºåºï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ¨ç†å‡†ç¡®æ€§ã€‚å…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€æŸ¥é˜…åŸæ–‡ä»¥è·å–æ›´å¤šæ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿä»¥åŠä»»ä½•éœ€è¦åŸºäºæ¨ç†çš„è‡ªåŠ¨åŒ–ç®—æœ¯è®¡ç®—ä»»åŠ¡ã€‚é€šè¿‡ä¼˜åŒ–è¾“å…¥é¡ºåºï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨ç®—æœ¯æ¨ç†ä¸­çš„è¡¨ç°ï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The chain of thought is fundamental in Transformers, which is to perform step-by-step reasoning. Besides what intermediate steps work, the order of these steps critically affects the difficulty of the reasoning. This study addresses a novel task of unraveling chain of thought - reordering decoder input tokens to a learning-friendly sequence for Transformers to learn arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture of target sequences arranged in different orders and then identifies benign orders as those with fast loss drops in the early stage. As the search space grows factorially with sequence length, we propose a two-stage hierarchical approach for inter- and intra-block reordering. Experiments on four order-sensitive arithmetic tasks show that our method identifies a learning-friendly order out of a few billion candidates. Notably, on the multiplication task, it recovered the reverse-digit order reported in prior studies.

