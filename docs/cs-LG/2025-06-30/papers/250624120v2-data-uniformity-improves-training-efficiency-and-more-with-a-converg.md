---
layout: default
title: Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime
---

# Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.24120" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.24120v2</a>
  <a href="https://arxiv.org/pdf/2506.24120.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.24120v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.24120v2', 'Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqing Wang, Shangding Gu

**åˆ†ç±»**: cs.LG, cs.AI, math.OC, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30 (æ›´æ–°: 2025-09-29)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/SafeRL-Lab/data-uniformity)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®å‡åŒ€æ€§é€‰æ‹©ä»¥æå‡è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®é€‰æ‹©` `è®­ç»ƒæ•ˆç‡` `ç¥ç»ç½‘ç»œ` `å‡åŒ€åˆ†å¸ƒ` `æ”¶æ•›æ¡†æ¶` `æ·±åº¦å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°æ®é€‰æ‹©æ–¹æ³•é€šå¸¸ä¾èµ–äºä»»åŠ¡ç‰¹æ€§ï¼Œç¼ºä¹é€šç”¨æ€§ï¼Œå¯¼è‡´åœ¨å¤æ‚ä»»åŠ¡ä¸­æ€§èƒ½æå‡æœ‰é™ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡é€‰æ‹©å‡åŒ€åˆ†å¸ƒçš„æ•°æ®æ¥æé«˜è®­ç»ƒæ•ˆç‡ï¼Œå»ºç«‹äº†ä¸€ä¸ªæ–°çš„æ”¶æ•›æ¡†æ¶ï¼Œé€‚ç”¨äºå¤šç§ç¥ç»ç½‘ç»œæ¶æ„ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€å¤§åŒ–æ•°æ®ç‚¹ä¹‹é—´çš„æˆå¯¹è·ç¦»æ˜¾è‘—åŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®é€‰æ‹©åœ¨æ•°æ®é©±åŠ¨å†³ç­–ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œé€šå¸¸ä¾èµ–äºå…·ä½“ä»»åŠ¡ã€‚å°½ç®¡æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§å·²è¢«å¹¿æ³›ç ”ç©¶å¹¶è¢«è®¤ä¸ºèƒ½æå‡æ¨¡å‹æ€§èƒ½ï¼Œä½†æ˜¯å¦å­˜åœ¨å…¶ä»–å®šé‡å’Œé€šç”¨çš„æ•°æ®é€‰æ‹©åŸåˆ™ä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡å±•ç¤ºäº†é€‰æ‹©æ›´å‡åŒ€åˆ†å¸ƒçš„æ•°æ®å¯ä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶å¢å¼ºæ€§èƒ½ã€‚æˆ‘ä»¬è¯æ˜äº†æ›´å‡åŒ€çš„åˆ†å¸ƒå¯¼è‡´æ•°æ®ç‚¹ä¹‹é—´çš„æœ€å°æˆå¯¹è·ç¦»å¢å¤§ï¼Œå¹¶ä¸”è¾ƒå°çš„æœ€å°æˆå¯¹è·ç¦»ä¼šå‡ç¼“æ¢¯åº¦ä¸‹é™çš„è®­ç»ƒåŠ¨æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç†è®ºä¸Šè¡¨æ˜ï¼Œç¥ç»ç½‘ç»œçš„è¿‘ä¼¼è¯¯å·®éšç€æœ€å°æˆå¯¹è·ç¦»çš„å¢åŠ è€Œå‡å°‘ã€‚æˆ‘ä»¬çš„åˆ†æå¼•å…¥äº†ä¸€ä¸ªè¶…è¶Šç¥ç»åˆ‡çº¿æ ¸ï¼ˆNTKï¼‰èŒƒç•´çš„æ”¶æ•›æ¡†æ¶ï¼Œé€‚ç”¨äºåŒ…æ‹¬å˜æ¢å™¨åœ¨å†…çš„å¹¿æ³›æ¶æ„ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å¤šç§è®¾ç½®ä¸‹è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœè¡¨æ˜é€šè¿‡æœ€å¤§åŒ–æˆå¯¹è·ç¦»é€‰æ‹©æ•°æ®æ˜¾è‘—åŠ é€Ÿè®­ç»ƒå¹¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šå®ç°äº†å¯æ¯”æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ•°æ®é€‰æ‹©åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ä¸è¶³é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„ç‰¹æ€§ï¼Œå¯¼è‡´æ€§èƒ½æå‡ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€‰æ‹©å‡åŒ€åˆ†å¸ƒçš„æ•°æ®ï¼Œä»¥å¢å¤§æ•°æ®ç‚¹ä¹‹é—´çš„æœ€å°æˆå¯¹è·ç¦»ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œè¯æ˜äº†æœ€å°æˆå¯¹è·ç¦»ä¸è®­ç»ƒåŠ¨æ€å’Œè¿‘ä¼¼è¯¯å·®ä¹‹é—´çš„å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªè¶…è¶Šç¥ç»åˆ‡çº¿æ ¸ï¼ˆNTKï¼‰èŒƒç•´çš„æ”¶æ•›æ¡†æ¶ï¼Œé€‚ç”¨äºå¤šç§ç¥ç»ç½‘ç»œæ¶æ„ï¼ŒåŒ…æ‹¬å˜æ¢å™¨ã€‚è¯¥æ¡†æ¶ä¸è¦æ±‚Lipschitzå…‰æ»‘æ€§ï¼Œæä¾›äº†å¯¹æ®‹å·®è¿æ¥å’Œå‡½æ•°ç»„åˆçš„ç†è®ºæ”¯æŒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†é€šè¿‡å‡åŒ€é€‰æ‹©æ•°æ®æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹çš„ç†è®ºæ¡†æ¶ï¼Œæ˜ç¡®äº†æœ€å°æˆå¯¹è·ç¦»å¯¹è®­ç»ƒåŠ¨æ€çš„å½±å“ï¼Œè¿™åœ¨ç°æœ‰ç ”ç©¶ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé€‰æ‹©æ•°æ®æ—¶æœ€å¤§åŒ–æˆå¯¹è·ç¦»çš„ç­–ç•¥è¢«åº”ç”¨äºä¸åŒçš„ä¼˜åŒ–ç­–ç•¥ã€æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•°æ®é›†ï¼Œç¡®ä¿äº†å®éªŒç»“æœçš„å¹¿æ³›é€‚ç”¨æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡é€‰æ‹©å‡åŒ€åˆ†å¸ƒçš„æ•°æ®ï¼Œè®­ç»ƒé€Ÿåº¦æ˜¾è‘—åŠ å¿«ï¼Œä¸”åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†ä¸åŸºçº¿æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œæœ€å¤§åŒ–æˆå¯¹è·ç¦»çš„ç­–ç•¥åœ¨å¤šç§æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæå‡å¹…åº¦æ˜æ˜¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰æ•°æ®é©±åŠ¨çš„ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†å¤æ‚æ•°æ®é›†çš„åœºæ™¯ä¸­ã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Data selection plays a crucial role in data-driven decision-making, including in large language models (LLMs), and is typically task-dependent. Properties such as data quality and diversity have been extensively studied and are known to enhance model performance. However, it remains unclear whether there exist other quantitative and general principles of data selection that can consistently improve performance, especially for complicated tasks. In this paper, we demonstrate that selecting more uniformly distributed data can improve training efficiency while enhancing performance. Specifically, we establish that more uniform (less biased) distribution leads to a larger minimum pairwise distance between data points, denoted by $h_{\min}$, and prove that a smaller $h_{\min}$ can slow down the training dynamics of gradient descent (GD). Moreover, we theoretically show that the approximation error of neural networks decreases as $h_{\min}$ increases. Our analysis introduces a convergence framework for GD beyond the Neural Tangent Kernel (NTK) regime, applicable to a broad class of architectures, including transformers, without requiring Lipschitz smoothness. This framework further provides theoretical justification for the use of residual connection and function composition in deep neural architectures. In the end, we conduct comprehensive experiments for supervised fine-tuning across various settings, including different optimization strategies, model sizes, and training datasets. The results consistently demonstrate that selecting data by maximizing pairwise distance significantly accelerates training and achieves comparable or better performance in LLMs across diverse datasets. Code and Datasets are available at the link: https://github.com/SafeRL-Lab/data-uniformity.

