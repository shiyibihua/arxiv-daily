---
layout: default
title: Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP
---

# Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01630" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01630v2</a>
  <a href="https://arxiv.org/pdf/2509.01630.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01630v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01630v2', 'Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bingheng Wang, Yichao Gao, Tianchen Sun, Lin Zhao

**åˆ†ç±»**: cs.LG, cs.MA, cs.RO, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01 (æ›´æ–°: 2025-09-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºL2Cæ¡†æ¶ï¼Œé€šè¿‡å¯å¾®ADMM-DDPå®ç°åˆ†å¸ƒå¼å…ƒè½¨è¿¹ä¼˜åŒ–ï¼Œè§£å†³å¤šæ™ºèƒ½ä½“ååŒé—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `åˆ†å¸ƒå¼ä¼˜åŒ–` `å…ƒå­¦ä¹ ` `è½¨è¿¹ä¼˜åŒ–` `ADMM-DDP` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºADMM-DDPçš„åˆ†å¸ƒå¼è½¨è¿¹ä¼˜åŒ–æ–¹æ³•éœ€è¦å¤§é‡è°ƒå‚ï¼Œéš¾ä»¥å¹³è¡¡å±€éƒ¨ä»»åŠ¡æ€§èƒ½å’Œå…¨å±€ååŒã€‚
2. L2Cæ¡†æ¶é€šè¿‡å…ƒå­¦ä¹ æ™ºèƒ½ä½“è¶…å‚æ•°ï¼Œè‡ªé€‚åº”ä¸åŒä»»åŠ¡å’Œæ™ºèƒ½ä½“é…ç½®ï¼Œå®ç°é«˜æ•ˆçš„åˆ†å¸ƒå¼ååŒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒL2Cåœ¨å¤æ‚ä»»åŠ¡ä¸­ç”ŸæˆåŠ¨æ€å¯è¡Œè½¨è¿¹ï¼Œå¹¶èƒ½é€‚åº”ä¸åŒå›¢é˜Ÿè§„æ¨¡ï¼Œæ¢¯åº¦è®¡ç®—é€Ÿåº¦æå‡88%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLearning to Coordinate (L2C)çš„é€šç”¨æ¡†æ¶ï¼Œç”¨äºå…ƒå­¦ä¹ è¶…å‚æ•°ï¼Œè¿™äº›è¶…å‚æ•°ç”±è½»é‡çº§çš„æ™ºèƒ½ä½“ç¥ç»ç½‘ç»œå»ºæ¨¡ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œæ™ºèƒ½ä½“é…ç½®ã€‚L2Cä»¥åˆ†å¸ƒå¼æ–¹å¼ç«¯åˆ°ç«¯åœ°é€šè¿‡ADMM-DDPæµç¨‹è¿›è¡Œå¾®åˆ†ã€‚å®ƒè¿˜é€šè¿‡é‡ç”¨DDPç»„ä»¶ï¼ˆå¦‚Riccatié€’å½’å’Œåé¦ˆå¢ç›Šï¼‰æ¥å®ç°é«˜æ•ˆçš„å…ƒæ¢¯åº¦è®¡ç®—ã€‚è¿™äº›æ¢¯åº¦å¯¹åº”äºåˆ†å¸ƒå¼çŸ©é˜µå€¼LQRé—®é¢˜çš„æœ€ä¼˜è§£ï¼Œè¿™äº›é—®é¢˜é€šè¿‡è¾…åŠ©ADMMæ¡†æ¶åœ¨æ™ºèƒ½ä½“ä¹‹é—´è¿›è¡Œåè°ƒï¼Œè¯¥æ¡†æ¶åœ¨æ¸©å’Œçš„å‡è®¾ä¸‹å˜ä¸ºå‡¸ä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡æˆªæ–­è¿­ä»£å’Œå…ƒå­¦ä¹ ADMMæƒ©ç½šå‚æ•°ï¼ˆé’ˆå¯¹å¿«é€Ÿæ®‹å·®å‡å°‘è¿›è¡Œäº†ä¼˜åŒ–ï¼‰è¿›ä¸€æ­¥åŠ é€Ÿäº†è®­ç»ƒï¼Œå¹¶å…·æœ‰å¯è¯æ˜çš„Lipschitzæœ‰ç•Œæ¢¯åº¦è¯¯å·®ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åˆä½œç©ºä¸­è¿è¾“ä»»åŠ¡ä¸­ï¼ŒL2Cä½¿ç”¨IsaacSIMåœ¨é«˜ä¿çœŸä»¿çœŸä¸­ç”ŸæˆåŠ¨æ€å¯è¡Œçš„è½¨è¿¹ï¼Œé‡æ–°é…ç½®å››æ—‹ç¿¼é£è¡Œå™¨ç¼–é˜Ÿä»¥åœ¨ç‹­å°ç©ºé—´ä¸­å®‰å…¨åœ°è¿›è¡Œ6è‡ªç”±åº¦è´Ÿè½½æ“ä½œï¼Œå¹¶ç¨³å¥åœ°é€‚åº”ä¸åŒçš„å›¢é˜Ÿè§„æ¨¡å’Œä»»åŠ¡æ¡ä»¶ï¼ŒåŒæ—¶å®ç°äº†æ¯”æœ€å…ˆè¿›æ–¹æ³•å¿«è¾¾88%çš„æ¢¯åº¦è®¡ç®—é€Ÿåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºADMM-DDPçš„åˆ†å¸ƒå¼è½¨è¿¹ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨å¤šæ™ºèƒ½ä½“ååŒç³»ç»Ÿä¸­ï¼Œéœ€è¦æ‰‹åŠ¨è°ƒæ•´å¤§é‡çš„è¶…å‚æ•°ï¼Œè¿™äº›è¶…å‚æ•°å…±åŒæ§åˆ¶ç€å±€éƒ¨ä»»åŠ¡çš„æ€§èƒ½å’Œå…¨å±€çš„ååŒæ•ˆæœã€‚æ‰‹åŠ¨è°ƒå‚è¿‡ç¨‹è€—æ—¶ä¸”ä½æ•ˆï¼Œéš¾ä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œæ™ºèƒ½ä½“é…ç½®ã€‚å› æ­¤ï¼Œå¦‚ä½•è‡ªåŠ¨åœ°å­¦ä¹ è¿™äº›è¶…å‚æ•°ï¼Œä»¥å®ç°é«˜æ•ˆçš„åˆ†å¸ƒå¼ååŒï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šL2Cçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å…ƒå­¦ä¹ çš„æ€æƒ³ï¼Œå°†è¶…å‚æ•°çš„å­¦ä¹ è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡æ„å»ºè½»é‡çº§çš„æ™ºèƒ½ä½“ç¥ç»ç½‘ç»œæ¥è¡¨ç¤ºè¿™äº›è¶…å‚æ•°ï¼Œå¹¶åˆ©ç”¨å¯å¾®åˆ†çš„ADMM-DDPæµç¨‹ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å…ƒå­¦ä¹ ã€‚è¿™æ ·ï¼Œå°±å¯ä»¥é€šè¿‡æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•ï¼Œè‡ªåŠ¨åœ°å­¦ä¹ åˆ°é€‚åº”ä¸åŒä»»åŠ¡å’Œæ™ºèƒ½ä½“é…ç½®çš„è¶…å‚æ•°ï¼Œä»è€Œæé«˜åˆ†å¸ƒå¼ååŒçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šL2Cçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŸºäºADMM-DDPçš„åˆ†å¸ƒå¼è½¨è¿¹ä¼˜åŒ–æ¨¡å—ï¼Œè´Ÿè´£ç”Ÿæˆæ¯ä¸ªæ™ºèƒ½ä½“çš„å±€éƒ¨è½¨è¿¹ï¼›2) è½»é‡çº§çš„æ™ºèƒ½ä½“ç¥ç»ç½‘ç»œï¼Œç”¨äºå»ºæ¨¡è¶…å‚æ•°ï¼›3) å…ƒå­¦ä¹ ä¼˜åŒ–å™¨ï¼Œç”¨äºæ›´æ–°æ™ºèƒ½ä½“ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚æ•´ä¸ªæµç¨‹æ˜¯ç«¯åˆ°ç«¯å¯å¾®åˆ†çš„ï¼Œå¯ä»¥é€šè¿‡åå‘ä¼ æ’­ç®—æ³•è®¡ç®—æ¢¯åº¦ï¼Œå¹¶åˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°æ™ºèƒ½ä½“ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚æ­¤å¤–ï¼Œä¸ºäº†åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼ŒL2Cè¿˜é‡‡ç”¨äº†æˆªæ–­è¿­ä»£å’Œå…ƒå­¦ä¹ ADMMæƒ©ç½šå‚æ•°ç­‰æŠ€æœ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šL2Cæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å…ƒå­¦ä¹ çš„æ€æƒ³å¼•å…¥åˆ°åˆ†å¸ƒå¼è½¨è¿¹ä¼˜åŒ–ä¸­ï¼Œå®ç°äº†è¶…å‚æ•°çš„è‡ªåŠ¨å­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ‰‹åŠ¨è°ƒå‚çš„æ–¹æ³•ç›¸æ¯”ï¼ŒL2Cå¯ä»¥è‡ªåŠ¨åœ°å­¦ä¹ åˆ°é€‚åº”ä¸åŒä»»åŠ¡å’Œæ™ºèƒ½ä½“é…ç½®çš„è¶…å‚æ•°ï¼Œä»è€Œæé«˜äº†åˆ†å¸ƒå¼ååŒçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒL2Cè¿˜é€šè¿‡é‡ç”¨DDPç»„ä»¶ï¼ˆå¦‚Riccatié€’å½’å’Œåé¦ˆå¢ç›Šï¼‰æ¥å®ç°é«˜æ•ˆçš„å…ƒæ¢¯åº¦è®¡ç®—ï¼Œè¿›ä¸€æ­¥æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šL2Cçš„å…³é”®è®¾è®¡åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) æ™ºèƒ½ä½“ç¥ç»ç½‘ç»œçš„ç»“æ„è®¾è®¡ï¼Œéœ€è¦ä¿è¯å…¶è½»é‡çº§å’Œè¡¨è¾¾èƒ½åŠ›ï¼›2) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œéœ€è¦èƒ½å¤Ÿåæ˜ å±€éƒ¨ä»»åŠ¡çš„æ€§èƒ½å’Œå…¨å±€çš„ååŒæ•ˆæœï¼›3) å…ƒå­¦ä¹ ä¼˜åŒ–å™¨çš„é€‰æ‹©ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°æ›´æ–°æ™ºèƒ½ä½“ç¥ç»ç½‘ç»œçš„å‚æ•°ï¼›4) ADMMæƒ©ç½šå‚æ•°çš„å…ƒå­¦ä¹ ç­–ç•¥ï¼Œéœ€è¦èƒ½å¤ŸåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹å¹¶ä¿è¯æ”¶æ•›æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒL2Cåœ¨åˆä½œç©ºä¸­è¿è¾“ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”ŸæˆåŠ¨æ€å¯è¡Œçš„è½¨è¿¹ï¼Œå¹¶èƒ½é€‚åº”ä¸åŒçš„å›¢é˜Ÿè§„æ¨¡å’Œä»»åŠ¡æ¡ä»¶ã€‚ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒL2Cçš„æ¢¯åº¦è®¡ç®—é€Ÿåº¦æé«˜äº†é«˜è¾¾88%ã€‚æ­¤å¤–ï¼ŒL2Cè¿˜èƒ½å¤Ÿé‡æ–°é…ç½®å››æ—‹ç¿¼é£è¡Œå™¨ç¼–é˜Ÿï¼Œä»¥åœ¨ç‹­å°ç©ºé—´ä¸­å®‰å…¨åœ°è¿›è¡Œ6è‡ªç”±åº¦è´Ÿè½½æ“ä½œã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

L2Cæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå¤šæœºå™¨äººååŒã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½äº¤é€šç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨å¤šæœºå™¨äººååŒæ¬è¿ä»»åŠ¡ä¸­ï¼ŒL2Cå¯ä»¥è‡ªåŠ¨å­¦ä¹ æ¯ä¸ªæœºå™¨äººçš„æ§åˆ¶å‚æ•°ï¼Œå®ç°é«˜æ•ˆçš„ååŒæ¬è¿ã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼ŒL2Cå¯ä»¥ç”¨äºä¼˜åŒ–è½¦è¾†çš„è¡Œé©¶è½¨è¿¹ï¼Œæé«˜è¡Œé©¶çš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚åœ¨æ™ºèƒ½äº¤é€šé¢†åŸŸï¼ŒL2Cå¯ä»¥ç”¨äºä¼˜åŒ–äº¤é€šä¿¡å·ç¯çš„é…æ—¶ï¼Œç¼“è§£äº¤é€šæ‹¥å µã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Distributed trajectory optimization via ADMM-DDP is a powerful approach for coordinating multi-agent systems, but it requires extensive tuning of tightly coupled hyperparameters that jointly govern local task performance and global coordination. In this paper, we propose Learning to Coordinate (L2C), a general framework that meta-learns these hyperparameters, modeled by lightweight agent-wise neural networks, to adapt across diverse tasks and agent configurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in a distributed manner. It also enables efficient meta-gradient computation by reusing DDP components such as Riccati recursions and feedback gains. These gradients correspond to the optimal solutions of distributed matrix-valued LQR problems, coordinated across agents via an auxiliary ADMM framework that becomes convex under mild assumptions. Training is further accelerated by truncating iterations and meta-learning ADMM penalty parameters optimized for rapid residual reduction, with provable Lipschitz-bounded gradient errors. On a challenging cooperative aerial transport task, L2C generates dynamically feasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures quadrotor formations for safe 6-DoF load manipulation in tight spaces, and adapts robustly to varying team sizes and task conditions, while achieving up to $88\%$ faster gradient computation than state-of-the-art methods.

