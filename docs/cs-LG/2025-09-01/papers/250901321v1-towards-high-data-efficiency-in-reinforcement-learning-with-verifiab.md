---
layout: default
title: Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward
---

# Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01321" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01321v1</a>
  <a href="https://arxiv.org/pdf/2509.01321.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01321v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01321v1', 'Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinyu Tang, Zhenduo Zhang, Yurou Liu, Wayne Xin Zhao, Zujie Wen, Zhiqiang Zhang, Jun Zhou

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DEPOï¼šé¢å‘å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ çš„é«˜æ•°æ®æ•ˆç‡ç­–ç•¥ä¼˜åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®æ•ˆç‡` `ç­–ç•¥ä¼˜åŒ–` `å¯éªŒè¯å¥–åŠ±` `ç¦»çº¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ‰©å±•æ—¶é¢ä¸´æ•°æ®æ•ˆç‡ä½ã€è®­ç»ƒæˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚
2. DEPOé€šè¿‡ç»“åˆä¼˜åŒ–çš„ç¦»çº¿æ•°æ®é€‰æ‹©ï¼ˆåŸºäºå¤šæ ·æ€§ã€å½±å“åŠ›å’Œéš¾åº¦ï¼‰å’Œåœ¨çº¿æ•°æ®é€‰æ‹©ï¼ˆåŸºäºå¯æ¢ç´¢æ€§ï¼‰æ¥æé«˜æ•°æ®æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDEPOåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä»…ä½¿ç”¨20%çš„æ•°æ®å³å¯è¾¾åˆ°ç”šè‡³è¶…è¿‡å…¨æ•°æ®è®­ç»ƒçš„æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹åˆ©ç”¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ (RLVR)æ¥æå‡æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ‰©å±•è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„rolloutè®¡ç®—å’Œå¤§å‹æ•°æ®é›†ï¼Œå¯¼è‡´é«˜è®­ç»ƒæˆæœ¬å’Œä½æ•°æ®æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DEPOï¼Œä¸€ç§æ•°æ®é«˜æ•ˆçš„ç­–ç•¥ä¼˜åŒ–æµç¨‹ï¼Œå®ƒç»“åˆäº†ç¦»çº¿å’Œåœ¨çº¿æ•°æ®é€‰æ‹©çš„ä¼˜åŒ–ç­–ç•¥ã€‚åœ¨ç¦»çº¿é˜¶æ®µï¼Œæˆ‘ä»¬åŸºäºå¤šæ ·æ€§ã€å½±å“åŠ›å’Œé€‚å½“çš„éš¾åº¦ï¼Œç­›é€‰å‡ºé«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬å­é›†ã€‚åœ¨åœ¨çº¿RLVRè®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ ·æœ¬çº§åˆ«çš„å¯æ¢ç´¢æ€§æŒ‡æ ‡ï¼Œä»¥åŠ¨æ€è¿‡æ»¤æ¢ç´¢æ½œåŠ›ä½çš„æ ·æœ¬ï¼Œä»è€Œæ˜¾è‘—é™ä½rolloutè®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºæœªå……åˆ†æ¢ç´¢çš„æ ·æœ¬å¼•å…¥äº†å›æ”¾æœºåˆ¶ï¼Œä»¥ç¡®ä¿å……åˆ†çš„è®­ç»ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„æœ€ç»ˆæ”¶æ•›æ€§èƒ½ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDEPOåœ¨ç¦»çº¿å’Œåœ¨çº¿æ•°æ®é€‰æ‹©åœºæ™¯ä¸­å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…ä½¿ç”¨20%çš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AIME24ä¸Šçš„é€Ÿåº¦æé«˜äº†1.85å€ï¼Œåœ¨AIME25ä¸Šçš„é€Ÿåº¦æé«˜äº†1.66å€ï¼Œè€ŒGRPOåˆ™ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåœ¨æå‡å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ—¶ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå¯¼è‡´è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œæ•°æ®åˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚å°¤å…¶æ˜¯åœ¨çº¿å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œå¤§é‡çš„rolloutè®¡ç®—æ¶ˆè€—äº†å¤§é‡çš„èµ„æºï¼Œè€Œå¾ˆå¤šrolloutæ ·æœ¬çš„æ¢ç´¢ä»·å€¼å¹¶ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDEPOçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¼˜åŒ–æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œåœ¨ç¦»çº¿å’Œåœ¨çº¿é˜¶æ®µéƒ½é€‰æ‹©æ›´æœ‰ä»·å€¼çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä»è€Œæé«˜æ•°æ®æ•ˆç‡ï¼Œé™ä½è®­ç»ƒæˆæœ¬ã€‚ç¦»çº¿é˜¶æ®µé€‰æ‹©å…·æœ‰ä»£è¡¨æ€§å’Œéš¾åº¦çš„æ ·æœ¬ï¼Œåœ¨çº¿é˜¶æ®µåˆ™å…³æ³¨å…·æœ‰æ¢ç´¢æ½œåŠ›çš„æ ·æœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDEPOåŒ…å«ç¦»çº¿æ•°æ®é€‰æ‹©å’Œåœ¨çº¿æ•°æ®é€‰æ‹©ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚ç¦»çº¿é˜¶æ®µï¼Œé¦–å…ˆä»åŸå§‹æ•°æ®é›†ä¸­ç­›é€‰å‡ºå…·æœ‰å¤šæ ·æ€§ã€å½±å“åŠ›å’Œé€‚å½“éš¾åº¦çš„æ ·æœ¬å­é›†ã€‚åœ¨çº¿é˜¶æ®µï¼Œå¼•å…¥æ ·æœ¬çº§åˆ«çš„å¯æ¢ç´¢æ€§æŒ‡æ ‡ï¼ŒåŠ¨æ€è¿‡æ»¤æ¢ç´¢æ½œåŠ›ä½çš„æ ·æœ¬ï¼Œå¹¶å¯¹æœªå……åˆ†æ¢ç´¢çš„æ ·æœ¬è¿›è¡Œå›æ”¾ã€‚æ•´ä¸ªæµç¨‹æ—¨åœ¨å‡å°‘ä¸å¿…è¦çš„rolloutè®¡ç®—ï¼Œå¹¶ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå……åˆ†å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šDEPOçš„å…³é”®åˆ›æ–°åœ¨äºç»“åˆäº†ç¦»çº¿å’Œåœ¨çº¿çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå¹¶æå‡ºäº†æ ·æœ¬çº§åˆ«çš„å¯æ¢ç´¢æ€§æŒ‡æ ‡ã€‚ç¦»çº¿æ•°æ®é€‰æ‹©ä¾§é‡äºæ•°æ®çš„ä»£è¡¨æ€§å’Œéš¾åº¦ï¼Œè€Œåœ¨çº¿æ•°æ®é€‰æ‹©åˆ™ä¾§é‡äºæ•°æ®çš„æ¢ç´¢æ½œåŠ›ã€‚è¿™ç§ç»“åˆä½¿å¾—DEPOèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šç¦»çº¿æ•°æ®é€‰æ‹©ä¸­ï¼Œä½¿ç”¨äº†å¤šæ ·æ€§ã€å½±å“åŠ›å’Œéš¾åº¦ä¸‰ä¸ªæŒ‡æ ‡æ¥è¯„ä¼°æ ·æœ¬çš„ä»·å€¼ã€‚åœ¨çº¿æ•°æ®é€‰æ‹©ä¸­ï¼Œå¯æ¢ç´¢æ€§æŒ‡æ ‡çš„å…·ä½“è®¡ç®—æ–¹æ³•æœªçŸ¥ã€‚æ­¤å¤–ï¼ŒDEPOè¿˜å¼•å…¥äº†å›æ”¾æœºåˆ¶ï¼Œç”¨äºå­˜å‚¨å’Œé‡æ–°è®­ç»ƒæœªå……åˆ†æ¢ç´¢çš„æ ·æœ¬ï¼Œä»¥ä¿è¯æ¨¡å‹çš„æ”¶æ•›æ€§èƒ½ã€‚å…·ä½“çš„å›æ”¾ç­–ç•¥å’Œå‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DEPOåœ¨äº”ä¸ªæ¨ç†åŸºå‡†ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨AIME24å’ŒAIME25æ•°æ®é›†ä¸Šï¼Œä»…ä½¿ç”¨20%çš„è®­ç»ƒæ•°æ®ï¼ŒDEPOå°±åˆ†åˆ«å®ç°äº†1.85å€å’Œ1.66å€çš„åŠ é€Ÿï¼Œè¶…è¿‡äº†ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒçš„GRPOæ¨¡å‹ã€‚è¿™è¡¨æ˜DEPOåœ¨æ•°æ®æ•ˆç‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DEPOå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•°æ®æ•ˆç‡çš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™æˆ–æ•°æ®è·å–æˆæœ¬é«˜æ˜‚çš„æƒ…å†µä¸‹ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸï¼Œå¸®åŠ©æ¨¡å‹æ›´å¿«åœ°å­¦ä¹ å’Œé€‚åº”ç¯å¢ƒï¼Œé™ä½å¼€å‘æˆæœ¬ï¼ŒåŠ é€Ÿäº§å“è¿­ä»£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large reasoning models have leveraged reinforcement learning with verifiable rewards (RLVR) to improve reasoning capabilities. However, scaling these methods typically requires extensive rollout computation and large datasets, leading to high training costs and low data efficiency. To mitigate this issue, we propose DEPO, a Data-Efficient Policy Optimization pipeline that combines optimized strategies for both offline and online data selection. In the offline phase, we curate a high-quality subset of training samples based on diversity, influence, and appropriate difficulty. During online RLVR training, we introduce a sample-level explorability metric to dynamically filter samples with low exploration potential, thereby reducing substantial rollout computational costs. Furthermore, we incorporate a replay mechanism for under-explored samples to ensure adequate training, which enhances the model's final convergence performance. Experiments across five reasoning benchmarks show that DEPO consistently outperforms existing methods in both offline and online data selection scenarios. Notably, using only 20% of the training data, our approach achieves a 1.85 times speed-up on AIME24 and a 1.66 times speed-up on AIME25 compared to GRPO trained on the full dataset.

