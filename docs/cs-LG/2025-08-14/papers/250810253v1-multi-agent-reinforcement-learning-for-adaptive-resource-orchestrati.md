---
layout: default
title: Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters
---

# Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10253" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10253v1</a>
  <a href="https://arxiv.org/pdf/2508.10253.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10253v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10253v1', 'Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guanzi Yao, Heyao Liu, Linyan Dai

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-14

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”èµ„æºè°ƒåº¦æ–¹æ³•ä»¥åº”å¯¹äº‘åŸç”Ÿé›†ç¾¤çš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `èµ„æºè°ƒåº¦` `äº‘åŸç”Ÿ` `å¥–åŠ±å¡‘å½¢` `å¼‚æ„è§’è‰²å»ºæ¨¡` `ç³»ç»Ÿç¨³å®šæ€§` `è°ƒåº¦ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰äº‘åŸç”Ÿæ•°æ®åº“ç³»ç»Ÿé¢ä¸´èµ„æºåŠ¨æ€æ€§é«˜å’Œè°ƒåº¦å¤æ‚æ€§å¤§çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´èµ„æºåˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚
2. æå‡ºäº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”èµ„æºè°ƒåº¦æ–¹æ³•ï¼Œé€šè¿‡å¼‚æ„è§’è‰²å»ºæ¨¡å’Œå¥–åŠ±å¡‘å½¢æœºåˆ¶æå‡è°ƒåº¦æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå…³é”®æŒ‡æ ‡ä¸Šè¶…è¶Šä¼ ç»Ÿæ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨é«˜å¹¶å‘å’Œå¤æ‚ä¾èµ–å…³ç³»ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹äº‘åŸç”Ÿæ•°æ®åº“ç³»ç»Ÿä¸­èµ„æºåŠ¨æ€æ€§é«˜å’Œè°ƒåº¦å¤æ‚æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”èµ„æºè°ƒåº¦æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å¼‚æ„è§’è‰²çš„æ™ºèƒ½ä½“å»ºæ¨¡æœºåˆ¶ï¼Œä½¿ä¸åŒèµ„æºå®ä½“ï¼ˆå¦‚è®¡ç®—èŠ‚ç‚¹ã€å­˜å‚¨èŠ‚ç‚¹å’Œè°ƒåº¦å™¨ï¼‰èƒ½å¤Ÿé‡‡ç”¨ä¸åŒçš„ç­–ç•¥è¡¨ç¤ºï¼Œåæ˜ å„è‡ªçš„åŠŸèƒ½è´£ä»»å’Œå±€éƒ¨ç¯å¢ƒç‰¹å¾ã€‚è®¾è®¡çš„å¥–åŠ±å¡‘å½¢æœºåˆ¶å°†å±€éƒ¨è§‚å¯Ÿä¸å…¨å±€åé¦ˆç»“åˆï¼Œå‡è½»äº†å› çŠ¶æ€è§‚å¯Ÿä¸å®Œæ•´å¯¼è‡´çš„ç­–ç•¥å­¦ä¹ åå·®ã€‚é€šè¿‡å®æ—¶çš„å±€éƒ¨æ€§èƒ½ä¿¡å·ä¸å…¨å±€ç³»ç»Ÿä»·å€¼ä¼°è®¡çš„ç»“åˆï¼Œæå‡äº†æ™ºèƒ½ä½“é—´çš„åè°ƒæ€§å’Œç­–ç•¥æ”¶æ•›çš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èµ„æºåˆ©ç”¨ç‡ã€è°ƒåº¦å»¶è¿Ÿã€ç­–ç•¥æ”¶æ•›é€Ÿåº¦ã€ç³»ç»Ÿç¨³å®šæ€§å’Œå…¬å¹³æ€§ç­‰å¤šä¸ªå…³é”®æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³äº‘åŸç”Ÿé›†ç¾¤ä¸­èµ„æºè°ƒåº¦çš„é«˜åŠ¨æ€æ€§å’Œå¤æ‚æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåº”å¯¹ä¸åŒèµ„æºå®ä½“çš„å¤šæ ·æ€§å’Œç¯å¢ƒå˜åŒ–ï¼Œå¯¼è‡´è°ƒåº¦æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”èµ„æºè°ƒåº¦æ–¹æ³•ï¼Œé‡‡ç”¨å¼‚æ„è§’è‰²çš„æ™ºèƒ½ä½“å»ºæ¨¡æœºåˆ¶ï¼Œä½¿ä¸åŒèµ„æºå®ä½“èƒ½å¤Ÿæ ¹æ®å…¶ç‰¹æ€§å’ŒåŠŸèƒ½é‡‡ç”¨ä¸åŒçš„ç­–ç•¥è¡¨ç¤ºï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”åŠ¨æ€ç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“è´Ÿè´£ç‰¹å®šçš„èµ„æºå®ä½“ï¼Œå¦‚è®¡ç®—èŠ‚ç‚¹ã€å­˜å‚¨èŠ‚ç‚¹å’Œè°ƒåº¦å™¨ã€‚é€šè¿‡å±€éƒ¨è§‚å¯Ÿå’Œå…¨å±€åé¦ˆçš„ç»“åˆï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ å’Œåè°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å¼‚æ„è§’è‰²çš„æ™ºèƒ½ä½“å»ºæ¨¡æœºåˆ¶å’Œå¥–åŠ±å¡‘å½¢æœºåˆ¶ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€è§’è‰²å»ºæ¨¡å’Œç®€å•å¥–åŠ±è®¾è®¡å½¢æˆäº†æ˜¾è‘—åŒºåˆ«ï¼Œæå‡äº†è°ƒåº¦çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’ŒåŠ¨æ€å¥–åŠ±å‡½æ•°ï¼Œä»¥é€‚åº”ä¸åŒåœºæ™¯ä¸‹çš„è°ƒåº¦éœ€æ±‚ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œä½¿ç”¨äº†æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰æ¥å®ç°ç­–ç•¥å­¦ä¹ ï¼Œç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿå¿«é€Ÿæ”¶æ•›å¹¶é€‚åº”ç¯å¢ƒå˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨èµ„æºåˆ©ç”¨ç‡ã€è°ƒåº¦å»¶è¿Ÿã€ç­–ç•¥æ”¶æ•›é€Ÿåº¦ç­‰æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°15%-30%ã€‚åœ¨å¤šä¸ªå®éªŒåœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§å’Œç¨³å®šæ€§ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äº‘è®¡ç®—èµ„æºç®¡ç†ã€æ•°æ®åº“è°ƒåº¦ä¼˜åŒ–å’Œå¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿçš„èµ„æºè°ƒåº¦ã€‚å…¶å®é™…ä»·å€¼åœ¨äºæå‡èµ„æºåˆ©ç”¨æ•ˆç‡å’Œç³»ç»Ÿç¨³å®šæ€§ï¼Œæœªæ¥å¯èƒ½å¯¹äº‘æœåŠ¡æä¾›å•†å’Œä¼ä¸šçº§åº”ç”¨äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper addresses the challenges of high resource dynamism and scheduling complexity in cloud-native database systems. It proposes an adaptive resource orchestration method based on multi-agent reinforcement learning. The method introduces a heterogeneous role-based agent modeling mechanism. This allows different resource entities, such as compute nodes, storage nodes, and schedulers, to adopt distinct policy representations. These agents are better able to reflect diverse functional responsibilities and local environmental characteristics within the system. A reward-shaping mechanism is designed to integrate local observations with global feedback. This helps mitigate policy learning bias caused by incomplete state observations. By combining real-time local performance signals with global system value estimation, the mechanism improves coordination among agents and enhances policy convergence stability. A unified multi-agent training framework is developed and evaluated on a representative production scheduling dataset. Experimental results show that the proposed method outperforms traditional approaches across multiple key metrics. These include resource utilization, scheduling latency, policy convergence speed, system stability, and fairness. The results demonstrate strong generalization and practical utility. Across various experimental scenarios, the method proves effective in handling orchestration tasks with high concurrency, high-dimensional state spaces, and complex dependency relationships. This confirms its advantages in real-world, large-scale scheduling environments.

