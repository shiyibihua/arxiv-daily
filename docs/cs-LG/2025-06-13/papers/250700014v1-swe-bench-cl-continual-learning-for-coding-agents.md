---
layout: default
title: SWE-Bench-CL: Continual Learning for Coding Agents
---

# SWE-Bench-CL: Continual Learning for Coding Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00014" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00014v1</a>
  <a href="https://arxiv.org/pdf/2507.00014.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00014v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00014v1', 'SWE-Bench-CL: Continual Learning for Coding Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thomas Joshi, Shayan Chowdhury, Fatih Uysal

**åˆ†ç±»**: cs.LG, cs.AI, cs.SE

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/thomasjoshi/agents-never-forget)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSWE-Bench-CLä»¥è§£å†³æŒç»­å­¦ä¹ ä¸­çš„çŸ¥è¯†é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `ä»£ç ç”Ÿæˆ` `çŸ¥è¯†è½¬ç§»` `GitHubé—®é¢˜` `AIä»£ç†` `è½¯ä»¶å·¥ç¨‹` `è¯„ä¼°æ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é™æ€ä»£ç ç”Ÿæˆæ¨¡å‹åœ¨é¢å¯¹æŒç»­å˜åŒ–çš„å¼€å‘ç¯å¢ƒæ—¶ï¼Œå®¹æ˜“å‡ºç°çŸ¥è¯†é—å¿˜å’Œé€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚
2. SWE-Bench-CLé€šè¿‡æ„å»ºåŸºäºæ—¶é—´åºåˆ—çš„GitHubé—®é¢˜æ•°æ®é›†ï¼Œè¯„ä¼°ä»£ç†çš„æŒç»­å­¦ä¹ èƒ½åŠ›ï¼Œè§£å†³äº†çŸ¥è¯†è½¬ç§»å’Œé—å¿˜çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è®°å¿†å¢å¼ºçš„ä»£ç†åœ¨å¤šä¸ªPythonä»£ç åº“ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ä½çš„é—å¿˜ç‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é™æ€ä»£ç ç”ŸæˆåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ç°å®è½¯ä»¶å¼€å‘æ˜¯ä¸€ä¸ªä¸æ–­æ¼”å˜çš„é—®é¢˜ã€ä¿®å¤å’ŒåŠŸèƒ½è¯·æ±‚çš„è¿ç»­æµã€‚æˆ‘ä»¬å¼•å…¥äº†SWE-Bench-CLï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºOpenAIå’ŒPrinceton-NLPäº2024å¹´æ¨å‡ºçš„äººç±»éªŒè¯çš„SWE-Bench Verifiedæ•°æ®é›†æ„å»ºçš„æŒç»­å­¦ä¹ åŸºå‡†ã€‚é€šè¿‡å°†GitHubé—®é¢˜ç»„ç»‡æˆåæ˜ è‡ªç„¶ä»£ç åº“æ¼”å˜çš„æ—¶é—´é¡ºåºåºåˆ—ï¼ŒSWE-Bench-CLä½¿å¾—ç›´æ¥è¯„ä¼°ä»£ç†åœ¨ç§¯ç´¯ç»éªŒã€è·¨ä»»åŠ¡è½¬ç§»çŸ¥è¯†å’ŒæŠµæŠ—ç¾éš¾æ€§é—å¿˜æ–¹é¢çš„èƒ½åŠ›æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬è¿˜è¡¥å……äº†å¯¹ä»»åŠ¡é—´ç»“æ„ç›¸ä¼¼æ€§å’Œä¸Šä¸‹æ–‡æ•æ„Ÿæ€§çš„åˆæ­¥åˆ†æï¼Œä»¥åŠå¢å¼ºäº†FAISSæ”¯æŒçš„è¯­ä¹‰è®°å¿†æ¨¡å—çš„LangGraphäº’åŠ¨è¯„ä¼°æ¡†æ¶ï¼Œå¹¶æä¾›äº†ä¸€å¥—ä¸“é—¨çš„æŒç»­å­¦ä¹ æŒ‡æ ‡ï¼Œä»¥æ•æ‰ç¨³å®šæ€§ä¸å¯å¡‘æ€§ä¹‹é—´çš„æƒè¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨æŒç»­å­¦ä¹ ç¯å¢ƒä¸­ï¼Œä»£ç†åœ¨é¢å¯¹ä¸æ–­å˜åŒ–çš„å¼€å‘éœ€æ±‚æ—¶å®¹æ˜“å‡ºç°çš„çŸ¥è¯†é—å¿˜é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸‹çš„é€‚åº”æ€§ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆç§¯ç´¯å’Œè½¬ç§»çŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºSWE-Bench-CLåŸºå‡†ï¼Œç»„ç»‡GitHubé—®é¢˜ä¸ºæ—¶é—´åºåˆ—ï¼Œç›´æ¥è¯„ä¼°ä»£ç†çš„å­¦ä¹ èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ä¸ªç»¼åˆçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ä»£ç†çš„é€‚åº”æ€§å’ŒçŸ¥è¯†ä¿æŒèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€LangGraphäº’åŠ¨è¯„ä¼°æ¡†æ¶å’ŒFAISSè¯­ä¹‰è®°å¿†æ¨¡å—ã€‚æ•°æ®é›†é€šè¿‡æ—¶é—´åºåˆ—ç»„ç»‡ï¼Œè¯„ä¼°æ¡†æ¶åˆ™ç»“åˆäº†å¤šç§æŒç»­å­¦ä¹ æŒ‡æ ‡ï¼Œå½¢æˆå®Œæ•´çš„è¯„ä¼°æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ—¶é—´åºåˆ—æ•°æ®é›†å’Œç»¼åˆè¯„ä¼°æŒ‡æ ‡ï¼Œä½¿å¾—ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å­¦ä¹ èƒ½åŠ›å¾—ä»¥å…¨é¢è¯„ä¼°ï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»Ÿé™æ€è¯„ä¼°æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æŒç»­å­¦ä¹ æŒ‡æ ‡ï¼Œå¦‚å¹³å‡å‡†ç¡®ç‡ã€é—å¿˜ç‡å’Œè½¬ç§»æ•ˆç‡ç­‰ï¼Œä»¥å…¨é¢æ•æ‰ä»£ç†çš„å­¦ä¹ è¡¨ç°ï¼Œå¹¶é€šè¿‡LangGraphå’ŒFAISSæ¨¡å—å¢å¼ºäº†è¯­ä¹‰è®°å¿†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è®°å¿†å¢å¼ºçš„ä»£ç†åœ¨å¤šä¸ªPythonä»£ç åº“ä¸­å¹³å‡å‡†ç¡®ç‡æé«˜äº†15%ï¼Œé—å¿˜ç‡é™ä½äº†20%ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSWE-Bench-CLæä¾›äº†æ›´ä¸ºå…¨é¢çš„è¯„ä¼°ï¼ŒéªŒè¯äº†å…¶åœ¨æŒç»­å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å·¥ç¨‹ä¸­çš„è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆã€æ™ºèƒ½è°ƒè¯•å’ŒæŒç»­é›†æˆç­‰ã€‚é€šè¿‡æé«˜AIä»£ç†çš„é€‚åº”æ€§å’ŒçŸ¥è¯†ä¿æŒèƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è½¯ä»¶å¼€å‘çš„æ•ˆç‡å’Œè´¨é‡ï¼Œæœªæ¥å¯èƒ½å¯¹è½¯ä»¶å¼€å‘æµç¨‹äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have achieved impressive results on static code-generation benchmarks, but real-world software development unfolds as a continuous stream of evolving issues, fixes, and feature requests. We introduce SWE-Bench-CL, a novel continual learning benchmark built on the human-verified SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By organizing GitHub issues into chronologically ordered sequences that reflect natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist catastrophic forgetting. We complement the dataset with (i) a preliminary analysis of inter-task structural similarity and contextual sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module, and (iii) a suite of specialized continual learning metrics -- including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized Composite Continual Learning Score and CL-F-beta score -- to capture the stability-plasticity trade-off. We outline a rigorous experimental protocol comparing memory-enabled and memory-disabled agents across diverse Python repositories. All code and data are publicly available at https://github.com/thomasjoshi/agents-never-forget, providing the community with a reproducible platform for developing more adaptive and robust AI agents in software engineering.

