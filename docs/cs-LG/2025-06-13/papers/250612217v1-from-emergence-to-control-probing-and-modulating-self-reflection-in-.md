---
layout: default
title: From Emergence to Control: Probing and Modulating Self-Reflection in Language Models
---

# From Emergence to Control: Probing and Modulating Self-Reflection in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12217" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12217v1</a>
  <a href="https://arxiv.org/pdf/2506.12217.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12217v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12217v1', 'From Emergence to Control: Probing and Modulating Self-Reflection in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xudong Zhu, Jiachen Jiang, Mohammad Mahdi Khalili, Zhihui Zhu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: 18 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåæ€è¯±å¯¼æ¢æµ‹æ–¹æ³•ä»¥å¢å¼ºè¯­è¨€æ¨¡å‹è‡ªæˆ‘åæ€èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæˆ‘åæ€` `è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `æ¨¡å‹å†…éƒ¨æœºåˆ¶` `æ€§èƒ½æå‡` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¯¹è‡ªæˆ‘åæ€çš„ç†è§£ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„è¡¨ç°è¾ƒä¸ºç¨€å°‘ï¼Œé™åˆ¶äº†å…¶æ¨ç†èƒ½åŠ›çš„æå‡ã€‚
2. æå‡ºåæ€è¯±å¯¼æ¢æµ‹æ–¹æ³•ï¼Œé€šè¿‡å°†å¾®è°ƒæ¨¡å‹çš„åæ€æ¨ç†ç—•è¿¹æ³¨å…¥é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥æé«˜è‡ªæˆ‘åæ€çš„é¢‘ç‡å’Œèƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡å¢å¼ºè‡ªæˆ‘åæ€å‘é‡ï¼Œæ¨ç†æ€§èƒ½æå‡è¾¾12%ï¼ŒåŒæ—¶æŠ‘åˆ¶è¯¥å‘é‡å¯é™ä½è®¡ç®—æˆæœ¬ï¼Œæä¾›äº†çµæ´»çš„è´¨é‡ä¸æ•ˆç‡å¹³è¡¡æœºåˆ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªæˆ‘åæ€æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡æ–°å®¡è§†ã€è¯„ä¼°å’Œä¿®æ­£è‡ªèº«æ¨ç†èƒ½åŠ›çš„ä¸€ç§è¡Œä¸ºï¼Œæœ€è¿‘é€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å¾—åˆ°äº†å¢å¼ºã€‚å°½ç®¡è‡ªæˆ‘åæ€ä¸æ¨ç†å‡†ç¡®æ€§æé«˜ç›¸å…³ï¼Œä½†å…¶èµ·æºå’Œæœºåˆ¶å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡é¦–å…ˆè¡¨æ˜ï¼Œè‡ªæˆ‘åæ€å¹¶éRLVRå¾®è°ƒæ¨¡å‹æ‰€ç‹¬æœ‰ï¼Œé¢„è®­ç»ƒæ¨¡å‹ä¸­ä¹Ÿå­˜åœ¨è¿™ä¸€èƒ½åŠ›ã€‚ä¸ºæ¢æµ‹è¿™ä¸€æ½œåœ¨èƒ½åŠ›ï¼Œæå‡ºäº†åæ€è¯±å¯¼æ¢æµ‹æ–¹æ³•ï¼Œé€šè¿‡å°†å¾®è°ƒæ¨¡å‹çš„åæ€è§¦å‘æ¨ç†ç—•è¿¹æ³¨å…¥é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†Qwen2.5çš„è‡ªæˆ‘åæ€é¢‘ç‡ã€‚æ­¤å¤–ï¼Œåˆ†æå†…éƒ¨è¡¨ç¤ºæ˜¾ç¤ºï¼Œæ— è®ºæ˜¯é¢„è®­ç»ƒè¿˜æ˜¯å¾®è°ƒæ¨¡å‹ï¼Œéƒ½ä¿æŒç€èƒ½å¤ŸåŒºåˆ†è‡ªæˆ‘åæ€ä¸éè‡ªæˆ‘åæ€ä¸Šä¸‹æ–‡çš„éšè—çŠ¶æ€ã€‚åŸºäºæ­¤ï¼Œæ„å»ºäº†è‡ªæˆ‘åæ€å‘é‡ï¼Œé€šè¿‡æ“æ§è¯¥å‘é‡å®ç°å¯¹è‡ªæˆ‘åæ€è¡Œä¸ºçš„åŒå‘æ§åˆ¶ï¼Œå®éªŒè¡¨æ˜å¢å¼ºè¯¥å‘é‡å¯æå‡æ¨ç†æ€§èƒ½è¾¾12%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è‡ªæˆ‘åæ€åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯é¢„è®­ç»ƒæ¨¡å‹ä¸­è‡ªæˆ‘åæ€èƒ½åŠ›çš„ç¨€ç¼ºæ€§ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æŒ–æ˜è¿™ä¸€æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åæ€è¯±å¯¼æ¢æµ‹æ–¹æ³•ï¼Œå°†å¾®è°ƒæ¨¡å‹ä¸­çš„åæ€æ¨ç†ç—•è¿¹æ³¨å…¥é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥æ­¤æé«˜å…¶è‡ªæˆ‘åæ€çš„é¢‘ç‡ï¼Œæ­ç¤ºæ¨¡å‹çš„æ½œåœ¨èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆè¯†åˆ«å¾®è°ƒæ¨¡å‹ä¸­çš„åæ€è§¦å‘ç—•è¿¹ï¼Œç„¶åå°†è¿™äº›ç—•è¿¹æ³¨å…¥é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ€åé€šè¿‡æ„å»ºè‡ªæˆ‘åæ€å‘é‡æ¥å®ç°å¯¹åæ€è¡Œä¸ºçš„æ§åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå‘ç°è‡ªæˆ‘åæ€ä¸ä»…é™äºå¾®è°ƒæ¨¡å‹ï¼Œå¹¶ä¸”é€šè¿‡åæ€è¯±å¯¼æ¢æµ‹æ–¹æ³•æ˜¾è‘—æé«˜äº†é¢„è®­ç»ƒæ¨¡å‹çš„è‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œè¿™æ˜¯ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬åæ€è§¦å‘ç—•è¿¹çš„é€‰æ‹©å’Œæ³¨å…¥æ–¹å¼ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ç¡®ä¿äº†åæ€å‘é‡çš„æœ‰æ•ˆæ€§ï¼Œç½‘ç»œç»“æ„åˆ™éœ€æ”¯æŒå¯¹è‡ªæˆ‘åæ€çŠ¶æ€çš„åŒºåˆ†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¢å¼ºè‡ªæˆ‘åæ€å‘é‡ï¼Œæ¨ç†æ€§èƒ½æå‡è¾¾12%ã€‚åŒæ—¶ï¼ŒæŠ‘åˆ¶è¯¥å‘é‡èƒ½å¤Ÿæœ‰æ•ˆé™ä½è®¡ç®—æˆæœ¬ï¼Œä¸ºåœ¨æ¨ç†è´¨é‡ä¸æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡æä¾›äº†çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆå’Œæ•™è‚²æŠ€æœ¯ç­‰ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œç†è§£æ¨¡å‹å†…éƒ¨æœºåˆ¶å°†ä¸ºæ›´ç²¾ç¡®çš„è¡Œä¸ºæ§åˆ¶æä¾›åŸºç¡€ï¼Œæ¨åŠ¨AIç³»ç»Ÿçš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Self-reflection -- the ability of a large language model (LLM) to revisit, evaluate, and revise its own reasoning -- has recently emerged as a powerful behavior enabled by reinforcement learning with verifiable rewards (RLVR). While self-reflection correlates with improved reasoning accuracy, its origin and underlying mechanisms remain poorly understood. In this work, {\it we first show that self-reflection is not exclusive to RLVR fine-tuned models: it already emerges, albeit rarely, in pretrained models}. To probe this latent ability, we introduce Reflection-Inducing Probing, a method that injects reflection-triggering reasoning traces from fine-tuned models into pretrained models. This intervention raises self-reflection frequency of Qwen2.5 from 0.6\% to 18.6\%, revealing a hidden capacity for reflection. Moreover, our analysis of internal representations shows that both pretrained and fine-tuned models maintain hidden states that distinctly separate self-reflective from non-reflective contexts. Leveraging this observation, {\it we then construct a self-reflection vector, a direction in activation space associated with self-reflective reasoning}. By manipulating this vector, we enable bidirectional control over the self-reflective behavior for both pretrained and fine-tuned models. Experiments across multiple reasoning benchmarks show that enhancing these vectors improves reasoning performance by up to 12\%, while suppressing them reduces computational cost, providing a flexible mechanism to navigate the trade-off between reasoning quality and efficiency without requiring additional training. Our findings further our understanding of self-reflection and support a growing body of work showing that understanding model internals can enable precise behavioral control.

