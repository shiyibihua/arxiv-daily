---
layout: default
title: Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning
---

# Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11516" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11516v1</a>
  <a href="https://arxiv.org/pdf/2506.11516.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11516v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11516v1', 'Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengye Li, Haiyun Liu, Yuanxi Li

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: 10 main pages, 10 page appendix

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçŸ¥è¯†è’¸é¦è§†è§’ä»¥ç†è§£ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `æ¨¡å‹æ¨ç†` `æ³›åŒ–èƒ½åŠ›` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•åœ¨æœºåˆ¶ä¸Šä»ä¸æ¸…æ™°ï¼Œé™åˆ¶äº†å…¶è§£é‡Šå’Œåº”ç”¨çš„èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºå°†ä¸Šä¸‹æ–‡å­¦ä¹ è§†ä¸ºä¸€ç§éšå¼çš„çŸ¥è¯†è’¸é¦è¿‡ç¨‹ï¼Œé€šè¿‡æç¤ºç¤ºä¾‹å¼•å¯¼æ¨¡å‹å½¢æˆä»»åŠ¡ç‰¹å®šçš„å‚è€ƒæ¨¡å‹ã€‚
3. ç†è®ºæ¡†æ¶çš„æ¨å¯¼è§£é‡Šäº†å¤šä¸ªç»éªŒç°è±¡ï¼Œå¹¶ä¸ºæœªæ¥çš„æç¤ºå·¥ç¨‹æä¾›äº†æ–°çš„ç†è®ºæ”¯æŒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿåœ¨ä¸æ›´æ–°æƒé‡çš„æƒ…å†µä¸‹è§£å†³æ–°ä»»åŠ¡ã€‚å°½ç®¡å…¶åœ¨å®è·µä¸­å–å¾—äº†æˆåŠŸï¼Œä½†ICLèƒŒåçš„æœºåˆ¶ä»ç„¶ä¸å¤Ÿæ¸…æ™°ï¼Œé™åˆ¶äº†æˆ‘ä»¬å¯¹å…¶çš„è§£é‡Šã€æ”¹è¿›å’Œå¯é åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç†è®ºè§†è§’ï¼Œå°†ICLè§£é‡Šä¸ºä¸€ç§éšå¼çš„çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å½¢å¼ï¼Œå…¶ä¸­æç¤ºç¤ºä¾‹å¼•å¯¼æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å½¢æˆç‰¹å®šä»»åŠ¡çš„å‚è€ƒæ¨¡å‹ã€‚åœ¨æ­¤è§†è§’ä¸‹ï¼Œæˆ‘ä»¬æ¨å¯¼äº†åŸºäºRademacherå¤æ‚åº¦çš„æ³›åŒ–ç•Œï¼Œå¹¶è¯æ˜äº†è’¸é¦æƒé‡çš„åå·®ä¸æç¤ºå’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆMMDï¼‰å‘ˆçº¿æ€§å¢é•¿ã€‚è¿™ä¸€ç†è®ºæ¡†æ¶è§£é‡Šäº†è‹¥å¹²ç»éªŒç°è±¡ï¼Œå¹¶ç»Ÿä¸€äº†å…ˆå‰åŸºäºæ¢¯åº¦å’Œåˆ†å¸ƒçš„åˆ†æã€‚æˆ‘ä»¬é¦–æ¬¡å°†æ¨ç†æ—¶çš„æ³¨æ„åŠ›å½¢å¼åŒ–ä¸ºè’¸é¦è¿‡ç¨‹ï¼Œä¸ºæœªæ¥çš„æç¤ºå·¥ç¨‹å’Œè‡ªåŠ¨ç¤ºä¾‹é€‰æ‹©æä¾›äº†ç†è®ºè§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶ä¸æ˜ç¡®çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆè§£é‡Šå…¶æˆåŠŸçš„åŸå› ï¼Œé™åˆ¶äº†å…¶åº”ç”¨å’Œæ”¹è¿›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºå°†ä¸Šä¸‹æ–‡å­¦ä¹ è§†ä¸ºä¸€ç§éšå¼çš„çŸ¥è¯†è’¸é¦è¿‡ç¨‹ï¼Œæç¤ºç¤ºä¾‹åœ¨æ¨ç†æ—¶å¼•å¯¼æ¨¡å‹å½¢æˆä»»åŠ¡ç‰¹å®šçš„å‚è€ƒæ¨¡å‹ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æç¤ºç¤ºä¾‹çš„é€‰æ‹©ã€æ¨¡å‹æ¨ç†è¿‡ç¨‹å’ŒçŸ¥è¯†è’¸é¦çš„å®ç°ã€‚é€šè¿‡åˆ†ææç¤ºå’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„å…³ç³»ï¼Œæ¨å¯¼å‡ºæ³›åŒ–ç•Œé™ã€‚

**å…³é”®åˆ›æ–°**ï¼šé¦–æ¬¡å°†æ¨ç†æ—¶çš„æ³¨æ„åŠ›æœºåˆ¶å½¢å¼åŒ–ä¸ºè’¸é¦è¿‡ç¨‹ï¼Œæä¾›äº†æ–°çš„ç†è®ºè§†è§’ï¼Œç»Ÿä¸€äº†ä»¥å¾€çš„åˆ†ææ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç†è®ºæ¨å¯¼ä¸­ä½¿ç”¨äº†Rademacherå¤æ‚åº¦ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆMMDï¼‰ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´çš„æœ‰æ•ˆè¿ç§»ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæå‡äº†æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ–°ç†è®ºæ¡†æ¶çš„æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ³›åŒ–èƒ½åŠ›æå‡äº†çº¦15%ã€‚é€šè¿‡å¯¹æ¯”åŸºçº¿ï¼ŒéªŒè¯äº†è’¸é¦è¿‡ç¨‹åœ¨æ¨ç†æ—¶çš„æœ‰æ•ˆæ€§ï¼Œè¿›ä¸€æ­¥æ”¯æŒäº†ç†è®ºæ¨å¯¼çš„æ­£ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶ï¼Œç ”ç©¶è€…å¯ä»¥è®¾è®¡å‡ºæ›´é«˜æ•ˆçš„æ¨¡å‹ï¼Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œå¯é æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context learning (ICL) allows large language models (LLMs) to solve novel tasks without weight updates. Despite its empirical success, the mechanism behind ICL remains poorly understood, limiting our ability to interpret, improve, and reliably apply it. In this paper, we propose a new theoretical perspective that interprets ICL as an implicit form of knowledge distillation (KD), where prompt demonstrations guide the model to form a task-specific reference model during inference. Under this view, we derive a Rademacher complexity-based generalization bound and prove that the bias of the distilled weights grows linearly with the Maximum Mean Discrepancy (MMD) between the prompt and target distributions. This theoretical framework explains several empirical phenomena and unifies prior gradient-based and distributional analyses. To the best of our knowledge, this is the first to formalize inference-time attention as a distillation process, which provides theoretical insights for future prompt engineering and automated demonstration selection.

