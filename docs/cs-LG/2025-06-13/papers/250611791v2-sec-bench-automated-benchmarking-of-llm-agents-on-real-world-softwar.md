---
layout: default
title: SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks
---

# SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11791" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11791v2</a>
  <a href="https://arxiv.org/pdf/2506.11791.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11791v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11791v2', 'SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hwiwon Lee, Ziqi Zhang, Hanxiao Lu, Lingming Zhang

**åˆ†ç±»**: cs.LG, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-10-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSEC-benchä»¥è§£å†³LLMä»£ç†åœ¨è½¯ä»¶å®‰å…¨ä»»åŠ¡ä¸­çš„è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è½¯ä»¶å®‰å…¨` `è‡ªåŠ¨åŒ–è¯„ä¼°` `æ¼æ´æ£€æµ‹` `è¡¥ä¸ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMä»£ç†è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–åˆæˆæ•°æ®ï¼Œæ— æ³•çœŸå®åæ˜ è½¯ä»¶å®‰å…¨å·¥ç¨‹ä¸­çš„å¤æ‚æ€§å’ŒæŒ‘æˆ˜ã€‚
2. SEC-benchæ¡†æ¶é€šè¿‡è‡ªåŠ¨æ„å»ºä»£ç åº“å’Œé‡ç°æ¼æ´ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°LLMä»£ç†åœ¨çœŸå®å®‰å…¨ä»»åŠ¡ä¸­çš„èƒ½åŠ›çš„æ–¹æ³•ã€‚
3. åœ¨ä½¿ç”¨SEC-benchè¿›è¡Œçš„è¯„ä¼°ä¸­ï¼ŒLLMä»£ç†åœ¨æ¦‚å¿µéªŒè¯ç”Ÿæˆå’Œæ¼æ´ä¿®è¡¥ä»»åŠ¡ä¸­çš„æˆåŠŸç‡åˆ†åˆ«ä»…ä¸º18.0%å’Œ34.0%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¿›è¡Œä¸¥æ ¼çš„å®‰å…¨è¯„ä¼°å¯¹äºç¡®ä¿å…¶åœ¨è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­çš„å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–äºåˆæˆæŒ‘æˆ˜æˆ–ç®€åŒ–çš„æ¼æ´æ•°æ®é›†ï¼Œæ— æ³•æ•æ‰å®‰å…¨å·¥ç¨‹å¸ˆåœ¨å®é™…å·¥ä½œä¸­é‡åˆ°çš„å¤æ‚æ€§å’Œæ¨¡ç³Šæ€§ã€‚æˆ‘ä»¬æå‡ºSEC-benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMä»£ç†åœ¨çœŸå®å®‰å…¨å·¥ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚SEC-benché‡‡ç”¨ä¸€ç§æ–°é¢–çš„å¤šä»£ç†æ¡†æ¶ï¼Œè‡ªåŠ¨æ„å»ºä»£ç åº“ï¼Œé‡ç°æ¼æ´ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„è¡¥ä¸ä»¥è¿›è¡Œå¯é è¯„ä¼°ã€‚ä½¿ç”¨SEC-benchï¼Œæˆ‘ä»¬å®æ–½äº†ä¸¤ä¸ªå…³é”®çš„è½¯ä»¶å®‰å…¨ä»»åŠ¡ï¼Œç»“æœæ˜¾ç¤ºç°æœ‰LLMä»£ç ä»£ç†åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMä»£ç†åœ¨è½¯ä»¶å®‰å…¨ä»»åŠ¡è¯„ä¼°ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹çœŸå®ä¸–ç•Œå¤æ‚æ€§å’Œæ¨¡ç³Šæ€§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºåˆæˆæ•°æ®ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°LLMåœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºSEC-benchï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨çœŸå®çš„å®‰å…¨å·¥ç¨‹ä»»åŠ¡ä¸­è¯„ä¼°LLMä»£ç†çš„èƒ½åŠ›ã€‚é€šè¿‡è‡ªåŠ¨æ„å»ºä»£ç åº“å’Œé‡ç°æ¼æ´ï¼ŒSEC-benchæä¾›äº†ä¸€ä¸ªå¯é çš„è¯„ä¼°ç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSEC-benchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šè‡ªåŠ¨æ„å»ºä»£ç åº“ã€æ¼æ´é‡ç°ã€ç”Ÿæˆé«˜è´¨é‡è¡¥ä¸å’Œè¯„ä¼°LLMä»£ç†çš„èƒ½åŠ›ã€‚æ¯ä¸ªæ¨¡å—ååŒå·¥ä½œï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSEC-benchçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å®Œå…¨è‡ªåŠ¨åŒ–çš„ç‰¹æ€§å’Œå¤šä»£ç†æ¡†æ¶è®¾è®¡ï¼Œä½¿å¾—è¯„ä¼°è¿‡ç¨‹é«˜æ•ˆä¸”å¯é‡å¤ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ‰‹åŠ¨è¯„ä¼°æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨äºé«˜è´¨é‡æ¼æ´æ•°æ®é›†çš„ç”Ÿæˆï¼Œç¡®ä¿æ¯ä¸ªå®ä¾‹çš„æˆæœ¬ä»…ä¸º0.87ç¾å…ƒã€‚æ­¤å¤–ï¼Œè¯„ä¼°è¿‡ç¨‹ä¸­é‡‡ç”¨äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯é æ€§å’Œå¯æ¯”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨SEC-benchçš„è¯„ä¼°ä¸­ï¼Œç°æœ‰çš„LLMä»£ç ä»£ç†åœ¨æ¦‚å¿µéªŒè¯ç”Ÿæˆä»»åŠ¡ä¸­çš„æˆåŠŸç‡ä»…ä¸º18.0%ï¼Œè€Œåœ¨æ¼æ´ä¿®è¡¥ä»»åŠ¡ä¸­ä¸º34.0%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå½“å‰æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­ä»å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›çš„å¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SEC-benchçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è½¯ä»¶å®‰å…¨é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©å¼€å‘è€…å’Œå®‰å…¨å·¥ç¨‹å¸ˆæ›´å¥½åœ°è¯„ä¼°å’Œé€‰æ‹©LLMä»£ç†ï¼Œä»è€Œæé«˜è½¯ä»¶å¼€å‘è¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸçš„è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Rigorous security-focused evaluation of large language model (LLM) agents is imperative for establishing trust in their safe deployment throughout the software development lifecycle. However, existing benchmarks largely rely on synthetic challenges or simplified vulnerability datasets that fail to capture the complexity and ambiguity encountered by security engineers in practice. We introduce SEC-bench, the first fully automated benchmarking framework for evaluating LLM agents on authentic security engineering tasks. SEC-bench employs a novel multi-agent scaffold that automatically constructs code repositories with harnesses, reproduces vulnerabilities in isolated environments, and generates gold patches for reliable evaluation. Our framework automatically creates high-quality software vulnerability datasets with reproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench, we implement two critical software security tasks to rigorously evaluate LLM agents' capabilities: proof-of-concept (PoC) generation and vulnerability patching. A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset. These results highlight the crucial steps needed toward developing LLM agents that are more practical, intelligent, and autonomous for security engineering.

