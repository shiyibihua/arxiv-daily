---
layout: default
title: LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model
---

# LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11402" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11402v2</a>
  <a href="https://arxiv.org/pdf/2506.11402.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11402v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11402v2', 'LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Marcel Mateos Salles, Praney Goyal, Pradyut Sekhsaria, Hai Huang, Randall Balestriero

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-10-01)

**å¤‡æ³¨**: 46 pages, 17 figures, 26 tables. Submitted for publication. for associated blog post, see https://pradyut3501.github.io/lora-spur-corr/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºLoRAæ¨¡å‹åœ¨å¾®è°ƒä¸­æ˜“å—çŸ­è·¯æ”»å‡»çš„è„†å¼±æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©é€‚åº”` `çŸ­è·¯æ”»å‡»` `è™šå‡æ ‡è®°æ³¨å…¥` `æ¨¡å‹å®‰å…¨æ€§` `æ•°æ®è´¨é‡` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¾®è°ƒæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LoRAå¾®è°ƒæ–¹æ³•åœ¨é«˜æ•ˆæ€§ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å´å¼•å…¥äº†çŸ­è·¯æ”»å‡»çš„è„†å¼±æ€§ï¼Œå½±å“æ¨¡å‹çš„å®‰å…¨æ€§ã€‚
2. æå‡ºæ— ç¼è™šå‡æ ‡è®°æ³¨å…¥ï¼ˆSSTIï¼‰æ–¹æ³•ï¼Œä¸“æ³¨äºå•ä¸ªä¸æ ‡ç­¾è™šå‡ç›¸å…³çš„æ ‡è®°ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„è„†å¼±æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„æ•°æ®æ¸…ç†å·¥å…·æ— æ³•æœ‰æ•ˆé˜²èŒƒè™šå‡æ ‡è®°çš„å½±å“ï¼Œæå‡ºäº†æ–°çš„æ•°æ®è´¨é‡å’Œå®‰å…¨æ€§é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸ç”¨äºå¤šç§åº”ç”¨åœºæ™¯ï¼Œå¹¶é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°é«˜æ•ˆçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶è¡¨æ˜ï¼ŒLoRAæ–¹æ³•å®é™…ä¸Šå¼•å…¥äº†çŸ­è·¯è„†å¼±æ€§ï¼Œä¸”è¶Šæ˜¯èµ„æºé«˜æ•ˆçš„LoRAè®¾ç½®ï¼Œå¾®è°ƒæ¨¡å‹è¶Šå®¹æ˜“å—åˆ°æ”»å‡»ã€‚æˆ‘ä»¬æå‡ºäº†æ— ç¼è™šå‡æ ‡è®°æ³¨å…¥ï¼ˆSSTIï¼‰æ–¹æ³•ï¼Œå‘ç°LoRAåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯¹ä¸ä¸‹æ¸¸æ ‡ç­¾è™šå‡ç›¸å…³çš„å•ä¸ªæ ‡è®°è¿‡äºæ•æ„Ÿã€‚é€šè¿‡åœ¨å¾®è°ƒæœŸé—´æ³¨å…¥è¿™äº›è™šå‡æ ‡è®°ï¼Œå¯ä»¥åœ¨æµ‹è¯•æ—¶æŒ‰éœ€æ“æ§æ¨¡å‹çš„é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„æ•°æ®æ£€æŸ¥å’Œé¢„å¤„ç†å·¥å…·æ— æ³•æœ‰æ•ˆæ¸…ç†æ•°æ®é›†ï¼Œä»è€Œå¼•å‘äº†å¯¹æ•°æ®è´¨é‡å’Œäººå·¥æ™ºèƒ½å®‰å…¨çš„æ–°æ‹…å¿§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³LoRAå¾®è°ƒæ¨¡å‹åœ¨é¢å¯¹çŸ­è·¯æ”»å‡»æ—¶çš„è„†å¼±æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨é«˜æ•ˆæ€§ä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†å´æœªèƒ½è€ƒè™‘åˆ°æ¨¡å‹å®‰å…¨æ€§çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹æ˜“å—æ”»å‡»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ— ç¼è™šå‡æ ‡è®°æ³¨å…¥ï¼ˆSSTIï¼‰æ–¹æ³•ï¼Œä¸“æ³¨äºè¯†åˆ«å’Œåˆ©ç”¨ä¸ä¸‹æ¸¸æ ‡ç­¾è™šå‡ç›¸å…³çš„å•ä¸ªæ ‡è®°ï¼Œä»è€Œè¯„ä¼°å’Œå±•ç¤ºLoRAæ¨¡å‹çš„è„†å¼±æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ­ç¤ºåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ½œåœ¨çš„å®‰å…¨éšæ‚£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆé€šè¿‡LoRAå¾®è°ƒæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ³¨å…¥è™šå‡æ ‡è®°ï¼Œæœ€åè¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é›†å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€è™šå‡æ ‡è®°æ³¨å…¥å’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºè¯†åˆ«å‡ºLoRAå¾®è°ƒæ¨¡å‹å¯¹è™šå‡æ ‡è®°çš„æ•æ„Ÿæ€§ï¼Œå¹¶é€šè¿‡SSTIæ–¹æ³•å±•ç¤ºäº†è¿™ä¸€ç°è±¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ•°æ®è´¨é‡é—®é¢˜åŠå…¶å¯¹æ¨¡å‹å®‰å…¨æ€§çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œç ”ç©¶è€…è®¾ç½®äº†ä¸åŒçš„è™šå‡æ ‡è®°æ³¨å…¥ç­–ç•¥ï¼Œå¹¶è¯„ä¼°äº†å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨é¢å¯¹è™šå‡æ ‡è®°æ—¶çš„ååº”è¢«å‡†ç¡®æ•æ‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„æ£€æŸ¥å’Œé¢„å¤„ç†å·¥å…·æ— æ³•æœ‰æ•ˆæ¸…ç†æ•°æ®é›†ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†ç ”ç©¶çš„å‡è®¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨LoRAå¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ³¨å…¥è™šå‡æ ‡è®°åï¼Œæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ï¼Œä¸”ç°æœ‰çš„æ•°æ®æ£€æŸ¥å·¥å…·æ— æ³•æœ‰æ•ˆè¯†åˆ«å’Œæ¸…ç†è¿™äº›è™šå‡æ ‡è®°ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æ•°æ®è´¨é‡å¯¹æ¨¡å‹å®‰å…¨æ€§çš„å…³é”®å½±å“ï¼Œæå‡ºäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œä»»ä½•ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚é€šè¿‡æé«˜å¯¹å¾®è°ƒæ¨¡å‹è„†å¼±æ€§çš„è®¤è¯†ï¼Œç ”ç©¶å¯ä»¥æ¨åŠ¨æ›´å®‰å…¨çš„æ¨¡å‹è®¾è®¡å’Œæ•°æ®å¤„ç†æ–¹æ³•ï¼Œä»è€Œæå‡äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ•´ä½“å®‰å…¨æ€§å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are commonly finetuned for a variety of use cases and domains. A common approach is to leverage Low-Rank Adaptation (LoRA) -- known to provide strong performance at low resource costs. In this study, we demonstrate that LoRA actually opens the door to short-cut vulnerabilities -- and the more resource efficient is the LoRA setup, the more vulnerable will be the finetuned model to aggressive attacks. To measure that vulnerability, we introduce Seamless Spurious Token Injection (SSTI), where we find that LoRA exclusively focuses on even just a single token that is spuriously correlated with downstream labels. In short, injection of that spurious token during finetuning ensure that the model's prediction at test-time can be manipulated on-demand. We conducted experiments across model families and datasets to evaluate the impact of SSTI during LoRA finetuning while providing possible mitigations. Our experiments conclude that none of the existing checkers and preprocessors can sanitize a dataset raising new concerns for data quality and AI safety.

