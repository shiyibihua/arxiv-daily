---
layout: default
title: EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction
---

# EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12015" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.12015v1</a>
  <a href="https://arxiv.org/pdf/2506.12015.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12015v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12015v1', 'EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hsi-Che Lin, Yu-Chu Yu, Kai-Po Chang, Yu-Chiang Frank Wang

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-13

**Â§áÊ≥®**: Under review. Project page: https://hsi-che-lin.github.io/EMLoC/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EMLoCÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥Â§ßÊ®°ÂûãÂæÆË∞ÉÁöÑÂÜÖÂ≠òÂºÄÈîÄÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê®°ÂûãÂæÆË∞É` `ÂÜÖÂ≠òÊïàÁéá` `LoRA` `‰ªøÁúüÂô®` `Â•áÂºÇÂÄºÂàÜËß£` `Ê∑±Â∫¶Â≠¶‰π†` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `ËÆ°ÁÆóÊú∫ËßÜËßâ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ßËßÑÊ®°Âü∫Á°ÄÊ®°ÂûãÂæÆË∞É‰∏≠Èù¢‰∏¥ÊòæËëóÁöÑÂÜÖÂ≠òÂºÄÈîÄÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®È¢ÜÂüüÁâπÂÆö‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. EMLoCÊ°ÜÊû∂ÈÄöËøáÊûÑÂª∫ËΩªÈáèÁ∫ß‰ªøÁúüÂô®ÂíåLoRA‰øÆÊ≠£ÔºåÂÆûÁé∞‰∫ÜÂú®Êé®ÁêÜÂÜÖÂ≠òÈ¢ÑÁÆóÂÜÖÁöÑÈ´òÊïàÂæÆË∞É„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåEMLoCÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåËÉΩÂ§üÂú®Âçï‰∏™GPU‰∏äÈ´òÊïàÂæÆË∞ÉÂ§ßÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂºÄÊ∫êÂü∫Á°ÄÊ®°ÂûãÁöÑÂø´ÈÄüÂèëÂ±ï‰ΩøÂÖ∂Âú®Â§ö‰∏™È¢ÜÂüüÂÖ∑Â§áÂº∫Â§ßÁöÑÈÄöÁî®ËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂØπ‰∫éÂ§ßËßÑÊ®°Âü∫Á°ÄÊ®°ÂûãËøõË°åÈ¢ÜÂüüÁâπÂÆöÊàñ‰∏™ÊÄßÂåñ‰ªªÂä°ÁöÑÂæÆË∞ÉÔºåÂõ†ÂÖ∂ÊòæËëóÁöÑÂÜÖÂ≠òÂºÄÈîÄËÄåÂèòÂæóÊûÅ‰∏∫ÊòÇË¥µ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜEMLoCÔºå‰∏Ä‰∏™Âü∫‰∫é‰ªøÁúüÂô®ÁöÑÂÜÖÂ≠òÈ´òÊïàÂæÆË∞ÉÊ°ÜÊû∂ÔºåÁªìÂêàLoRA‰øÆÊ≠£ÔºåËÉΩÂ§üÂú®‰∏éÊé®ÁêÜÁõ∏ÂêåÁöÑÂÜÖÂ≠òÈ¢ÑÁÆóÂÜÖËøõË°åÊ®°ÂûãÂæÆË∞É„ÄÇEMLoCÈÄöËøáÂØπÂ∞èÂûã‰∏ãÊ∏∏Ê†°ÂáÜÈõÜËøõË°åÊøÄÊ¥ªÊÑüÁü•ÁöÑÂ•áÂºÇÂÄºÂàÜËß£ÔºàSVDÔºâÊûÑÂª∫‰ªªÂä°ÁâπÂÆöÁöÑËΩªÈáèÁ∫ß‰ªøÁúüÂô®ÔºåÈöèÂêéÂú®ËØ•‰ªøÁúüÂô®‰∏äÈÄöËøáLoRAËøõË°åÂæÆË∞É„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ÂéüÂßãÊ®°Âûã‰∏éÂéãÁº©‰ªøÁúüÂô®‰πãÈó¥ÁöÑÈîô‰ΩçÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑË°•ÂÅøÁÆóÊ≥ïÔºå‰ª•‰øÆÊ≠£ÂæÆË∞ÉÂêéÁöÑLoRAÊ®°ÂùóÔºå‰ªéËÄåÂèØ‰ª•ÂêàÂπ∂Âà∞ÂéüÂßãÊ®°Âûã‰∏≠ËøõË°åÊé®ÁêÜ„ÄÇEMLoCÊîØÊåÅÁÅµÊ¥ªÁöÑÂéãÁº©ÊØîÂíåÊ†áÂáÜËÆ≠ÁªÉÊµÅÁ®ãÔºåÈÄÇÂ∫îÂπøÊ≥õÁöÑÂ∫îÁî®Âú∫ÊôØ„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåEMLoCÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜÂíåÊ®°ÊÄÅ‰∏äË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂü∫Á∫øÔºå‰∏îÂú®‰∏çËøõË°åÈáèÂåñÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂú®Âçï‰∏™24GBÊ∂àË¥πÁ∫ßGPU‰∏äÂæÆË∞É38BÊ®°ÂûãÔºå‰∏∫‰∏™‰ΩìÁî®Êà∑Â∏¶Êù•‰∫ÜÈ´òÊïà‰∏îÂÆûÁî®ÁöÑÊ®°ÂûãÈÄÇÂ∫îËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËßÑÊ®°Âü∫Á°ÄÊ®°ÂûãÂú®È¢ÜÂüüÁâπÂÆö‰ªªÂä°ÂæÆË∞ÉÊó∂ÁöÑÂÜÖÂ≠òÂºÄÈîÄÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÂÜÖÂ≠òÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØË°åÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEMLoCÈÄöËøáÊûÑÂª∫ËΩªÈáèÁ∫ß‰ªøÁúüÂô®ÔºåÁªìÂêàLoRAÊäÄÊúØÔºåÂú®ÂÜÖÂ≠òÈ¢ÑÁÆóÂÜÖÂÆûÁé∞È´òÊïàÂæÆË∞É„ÄÇËØ•ËÆæËÆ°‰ΩøÂæóÂæÆË∞ÉËøáÁ®ã‰∏çÂÜç‰æùËµñ‰∫éÂéüÂßãÊ®°ÂûãÁöÑÂÖ®ÈÉ®ÂèÇÊï∞Ôºå‰ªéËÄåÈôç‰ΩéÂÜÖÂ≠òÈúÄÊ±Ç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEMLoCÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÔºå‰ΩøÁî®ÊøÄÊ¥ªÊÑüÁü•ÁöÑÂ•áÂºÇÂÄºÂàÜËß£ÔºàSVDÔºâÊûÑÂª∫ËΩªÈáèÁ∫ß‰ªøÁúüÂô®ÔºõÂÖ∂Ê¨°ÔºåÂú®ËØ•‰ªøÁúüÂô®‰∏äËøõË°åLoRAÂæÆË∞ÉÔºõÊúÄÂêéÔºåÂ∫îÁî®Ë°•ÂÅøÁÆóÊ≥ï‰øÆÊ≠£ÂæÆË∞ÉÂêéÁöÑLoRAÊ®°ÂùóÔºå‰ª•‰æøÂ∞ÜÂÖ∂ÂêàÂπ∂Âà∞ÂéüÂßãÊ®°Âûã‰∏≠ËøõË°åÊé®ÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEMLoCÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑË°•ÂÅøÁÆóÊ≥ïÔºåËß£ÂÜ≥‰∫ÜÂéüÂßãÊ®°Âûã‰∏éÂéãÁº©‰ªøÁúüÂô®‰πãÈó¥ÁöÑÈîô‰ΩçÈóÆÈ¢ò„ÄÇËøô‰∏ÄÂàõÊñ∞‰ΩøÂæóÂæÆË∞ÉÂêéÁöÑÊ®°ÂûãËÉΩÂ§üÊúâÊïàÂú∞‰∏éÂéüÂßãÊ®°ÂûãÂÖºÂÆπ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåEMLoCÂÖÅËÆ∏ÁÅµÊ¥ªÁöÑÂéãÁº©ÊØîÔºåÂπ∂ÈááÁî®Ê†áÂáÜÁöÑËÆ≠ÁªÉÊµÅÁ®ã„ÄÇÊ≠§Â§ñÔºåÊøÄÊ¥ªÊÑüÁü•ÁöÑSVDÂíåLoRAÁöÑÁªìÂêàÊòØÂÖ∂Ê†∏ÂøÉÊäÄÊúØÁªÜËäÇ‰πã‰∏ÄÔºåÁ°Æ‰øù‰∫ÜÂæÆË∞ÉËøáÁ®ãÁöÑÈ´òÊïàÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EMLoCÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÂü∫Á∫øÊñπÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏çËøõË°åÈáèÂåñÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂú®Âçï‰∏™24GBÁöÑÊ∂àË¥πÁ∫ßGPU‰∏äÊàêÂäüÂæÆË∞É38BÂèÇÊï∞ÁöÑÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂÜÖÂ≠òÊïàÁéáÂíåÊÄßËÉΩ‰∏äÁöÑÊòæËëóÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EMLoCÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÈúÄË¶ÅÂø´ÈÄüÈÄÇÂ∫îÂ§ßËßÑÊ®°Âü∫Á°ÄÊ®°ÂûãÁöÑÈ¢ÜÂüüÔºåÂ¶ÇËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅËÆ°ÁÆóÊú∫ËßÜËßâÂíå‰∏™ÊÄßÂåñÊé®ËçêÁ≥ªÁªü„ÄÇÂÖ∂È´òÊïàÁöÑÂæÆË∞ÉËÉΩÂäõ‰ΩøÂæó‰∏≠Â∞èÂûã‰ºÅ‰∏öÂíå‰∏™‰∫∫ÂºÄÂèëËÄÖËÉΩÂ§üÂú®ÊúâÈôêÁöÑËµÑÊ∫ê‰∏ãÔºåÂà©Áî®Â§ßÊ®°ÂûãËøõË°åÁâπÂÆö‰ªªÂä°ÁöÑ‰ºòÂåñÔºåÊé®Âä®‰∫ÜAIÊäÄÊúØÁöÑÊôÆÂèä‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.

