---
layout: default
title: RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer
---

# RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11465" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11465v1</a>
  <a href="https://arxiv.org/pdf/2506.11465.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11465v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11465v1', 'RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haotian Ni, Yake Wei, Hang Liu, Gong Chen, Chong Peng, Hao Lin, Di Hu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: Accepted by ICML 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/GeWu-Lab/RollingQ_ICML2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRollingQä»¥è§£å†³å¤šæ¨¡æ€Transformerä¸­çš„åˆä½œåŠ¨æ€é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `Transformer` `æ³¨æ„åŠ›æœºåˆ¶` `ä¿¡æ¯èåˆ` `åŠ¨æ€é€‚åº”æ€§` `Rolling Query` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªæ³¨æ„åŠ›æ¨¡å‹åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­å­˜åœ¨åŠ¨æ€é€‚åº”æ€§å‡å¼±çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹åå‘æŸä¸€æ¨¡æ€ã€‚
2. è®ºæ–‡æå‡ºçš„RollingQæ–¹æ³•é€šè¿‡æ—‹è½¬æŸ¥è¯¢æ¥å¹³è¡¡æ³¨æ„åŠ›åˆ†é…ï¼Œæ‰“ç ´è‡ªæˆ‘å¼ºåŒ–å¾ªç¯ï¼Œæ¢å¤æ¨¡æ€é—´çš„åˆä½œåŠ¨æ€ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRollingQæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€Transformerçš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šç§åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å­¦ä¹ åœ¨æœ‰æ•ˆèåˆæ¥è‡ªä¸åŒæ¨¡æ€çš„ä¿¡æ¯æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡æ€è´¨é‡å› æ ·æœ¬è€Œå¼‚çš„æƒ…å†µä¸‹ã€‚åŠ¨æ€èåˆç­–ç•¥ï¼Œå¦‚Transformerä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨æ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹å¾è‡ªé€‚åº”åœ°å¼ºè°ƒæ¨¡æ€ã€‚ç„¶è€Œï¼Œç»è¿‡å¤§é‡ç²¾å¿ƒè®¾è®¡çš„å®éªŒï¼Œæˆ‘ä»¬æ„å¤–åœ°è§‚å¯Ÿåˆ°å¹¿æ³›ä½¿ç”¨çš„è‡ªæ³¨æ„åŠ›æ¨¡å‹çš„åŠ¨æ€é€‚åº”æ€§å‡å¼±ï¼Œæ¨¡å‹å€¾å‘äºä¼˜å…ˆé€‰æ‹©æŸä¸€æ¨¡æ€ï¼Œå¯¼è‡´è‡ªæˆ‘å¼ºåŒ–å¾ªç¯ï¼Œé€æ¸è¿‡åº¦å¼ºè°ƒåçˆ±çš„æ¨¡æ€ï¼Œæ‰©å¤§äº†æ¨¡æ€é—´æ³¨æ„åŠ›é”®çš„åˆ†å¸ƒå·®è·ã€‚ä¸ºæ¢å¤é€‚åº”æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•Rolling Queryï¼ˆRollingQï¼‰ï¼Œé€šè¿‡æ—‹è½¬æŸ¥è¯¢æ¥å¹³è¡¡æ³¨æ„åŠ›åˆ†é…ï¼Œæ‰“ç ´è‡ªæˆ‘å¼ºåŒ–å¾ªç¯ï¼Œå‡è½»é”®åˆ†å¸ƒå·®è·ã€‚å¤§é‡å®éªŒéªŒè¯äº†RollingQçš„æœ‰æ•ˆæ€§ï¼Œæ¢å¤åˆä½œåŠ¨æ€å¯¹å¢å¼ºå¹¿æ³›éƒ¨ç½²çš„å¤šæ¨¡æ€Transformerçš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¤šæ¨¡æ€Transformerä¸­åŠ¨æ€é€‚åº”æ€§å‡å¼±ï¼Œå¯¼è‡´æ¨¡å‹åå‘æŸä¸€æ¨¡æ€ï¼Œå½±å“ä¿¡æ¯èåˆæ•ˆæœã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ¨¡æ€è´¨é‡å·®å¼‚æ—¶è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨æ‰€æœ‰æ¨¡æ€çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯æå‡ºRolling Queryï¼ˆRollingQï¼‰ï¼Œé€šè¿‡æ—‹è½¬æŸ¥è¯¢æ¥å¹³è¡¡ä¸åŒæ¨¡æ€çš„æ³¨æ„åŠ›åˆ†é…ï¼Œæ‰“ç ´è‡ªæˆ‘å¼ºåŒ–å¾ªç¯ï¼Œä»è€Œæ¢å¤æ¨¡æ€é—´çš„åˆä½œåŠ¨æ€ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å‡è½»æ¨¡æ€é—´çš„æ³¨æ„åŠ›é”®åˆ†å¸ƒå·®è·ï¼Œå¢å¼ºæ¨¡å‹çš„é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å¤šæ¨¡æ€æ•°æ®ï¼Œé€šè¿‡RollingQæ¨¡å—åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›åˆ†é…ï¼Œæœ€ç»ˆè¾“å‡ºèåˆåçš„ç‰¹å¾è¡¨ç¤ºã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€RollingQæ³¨æ„åŠ›æœºåˆ¶å’Œç‰¹å¾èåˆå±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºRollingQæ–¹æ³•çš„æå‡ºï¼Œå®ƒé€šè¿‡æ—‹è½¬æŸ¥è¯¢æœºåˆ¶æœ‰æ•ˆåœ°æ‰“ç ´äº†è‡ªæˆ‘å¼ºåŒ–å¾ªç¯ï¼Œä¸ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ¨¡æ€é—´çš„è´¨é‡å·®å¼‚ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬æŸ¥è¯¢æ—‹è½¬çš„å…·ä½“å®ç°æ–¹å¼ã€æ³¨æ„åŠ›åˆ†é…çš„ç­–ç•¥ä»¥åŠæŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°å„æ¨¡æ€çš„é‡è¦æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨RollingQåï¼Œå¤šæ¨¡æ€Transformeråœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡æ€è´¨é‡å·®å¼‚æ˜æ˜¾çš„åœºæ™¯ä¸­ï¼Œæ³¨æ„åŠ›åˆ†é…çš„å‡è¡¡æ€§å¾—åˆ°äº†æœ‰æ•ˆæ”¹å–„ï¼Œæå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€è§†é¢‘ç†è§£ã€å›¾åƒä¸æ–‡æœ¬çš„è”åˆå­¦ä¹ ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€Transformerçš„æ€§èƒ½ï¼ŒRollingQèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ›´å¥½åœ°å¤„ç†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯èåˆï¼Œæé«˜ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal learning faces challenges in effectively fusing information from diverse modalities, especially when modality quality varies across samples. Dynamic fusion strategies, such as attention mechanism in Transformers, aim to address such challenge by adaptively emphasizing modalities based on the characteristics of input data. However, through amounts of carefully designed experiments, we surprisingly observed that the dynamic adaptability of widely-used self-attention models diminishes. Model tends to prefer one modality regardless of data characteristics. This bias triggers a self-reinforcing cycle that progressively overemphasizes the favored modality, widening the distribution gap in attention keys across modalities and deactivating attention mechanism's dynamic properties. To revive adaptability, we propose a simple yet effective method Rolling Query (RollingQ), which balances attention allocation by rotating the query to break the self-reinforcing cycle and mitigate the key distribution gap. Extensive experiments on various multimodal scenarios validate the effectiveness of RollingQ and the restoration of cooperation dynamics is pivotal for enhancing the broader capabilities of widely deployed multimodal Transformers. The source code is available at https://github.com/GeWu-Lab/RollingQ_ICML2025.

