---
layout: default
title: LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment
---

# LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11480" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11480v3</a>
  <a href="https://arxiv.org/pdf/2506.11480.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11480v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11480v3', 'LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shipeng Li, Shikun Li, Zhiqin Yang, Xinghua Zhang, Gaode Chen, Xiaobo Xia, Hengyu Liu, Zhe Peng

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-07-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLearnAlignä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­çš„æ•°æ®é€‰æ‹©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `æ•°æ®é€‰æ‹©` `æ¢¯åº¦å¯¹é½` `æ¨ç†èƒ½åŠ›` `æ•°æ®æ•ˆç‡` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡ä¸­é¢ä¸´æ•°æ®æ•ˆç‡ä½ä¸‹çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºçš„LearnAlignæ–¹æ³•é€šè¿‡æ™ºèƒ½é€‰æ‹©å¯å­¦ä¹ çš„æ¨ç†æ•°æ®ï¼Œä¼˜åŒ–äº†åè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®ä½¿ç”¨æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLearnAlignåœ¨GSM8KåŸºå‡†ä¸Šå°†æ•°æ®éœ€æ±‚å‡å°‘è‡³1000ä¸ªæ•°æ®ç‚¹ï¼ŒåŒæ—¶æ€§èƒ½è¾¾åˆ°77.53%ï¼Œä¼˜äºå…¨æ•°æ®é›†çš„77.04%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„å…³é”®æŠ€æœ¯ï¼Œä½†å…¶æ•°æ®æ•ˆç‡ä½ä¸‹ä»æ˜¯ä¸»è¦ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€å…³é”®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¢¯åº¦å¯¹é½çš„æ™ºèƒ½æ•°æ®é€‰æ‹©æ–¹æ³•LearnAlignï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºRLåè®­ç»ƒæ™ºèƒ½é€‰æ‹©å¯å­¦ä¹ å’Œå…·æœ‰ä»£è¡¨æ€§çš„æ¨ç†è®­ç»ƒæ•°æ®ã€‚é€šè¿‡å¼•å…¥åŸºäºæˆåŠŸç‡çš„æ•°æ®å¯å­¦ä¹ æ€§ï¼Œå…‹æœäº†æ¢¯åº¦èŒƒæ•°ä¸­çš„å“åº”é•¿åº¦åå·®é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ•°æ®éœ€æ±‚ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šä»…æœ‰è½»å¾®ä¸‹é™ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æ›´ä½³ã€‚è¯¥ç ”ç©¶ä¸ºæ•°æ®é«˜æ•ˆçš„RLåè®­ç»ƒæä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥ä¼˜åŒ–æ¨ç†æ•°æ®é€‰æ‹©çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­é¢ä¸´çš„æ•°æ®æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒæ—¶é€šå¸¸ä¾èµ–äºå¤§é‡æ•°æ®ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œè®­ç»ƒæ—¶é—´å»¶é•¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLearnAligné€šè¿‡å¼•å…¥åŸºäºæˆåŠŸç‡çš„æ•°æ®å¯å­¦ä¹ æ€§ï¼Œæ™ºèƒ½é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œæé«˜æ•°æ®ä½¿ç”¨æ•ˆç‡ï¼Œå‡å°‘å¯¹æ•°æ®é‡çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é€‰æ‹©æ¨¡å—å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ã€‚æ•°æ®é€‰æ‹©æ¨¡å—æ ¹æ®æ¢¯åº¦å¯¹é½å’ŒæˆåŠŸç‡è¯„ä¼°æ•°æ®çš„å¯å­¦ä¹ æ€§ï¼Œéšåå°†é€‰å‡ºçš„æ•°æ®ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šLearnAlignçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºæˆåŠŸç‡çš„å­¦ä¹ æ½œåŠ›è¯„ä¼°æœºåˆ¶ï¼Œè¿™ä¸€æœºåˆ¶æœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­å“åº”é•¿åº¦åå·®çš„é—®é¢˜ï¼Œä½¿å¾—æ•°æ®é€‰æ‹©æ›´åŠ ç²¾å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒLearnAligné‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¢¯åº¦å¯¹é½ï¼ŒåŒæ—¶åœ¨æ•°æ®é€‰æ‹©è¿‡ç¨‹ä¸­è®¾ç½®äº†é˜ˆå€¼ï¼Œä»¥ç¡®ä¿é€‰å‡ºçš„æ•°æ®åœ¨æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰è¾ƒé«˜çš„ä»£è¡¨æ€§å’Œå­¦ä¹ ä»·å€¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLearnAlignåœ¨GSM8KåŸºå‡†ä¸Šå°†æ•°æ®éœ€æ±‚å‡å°‘è‡³1000ä¸ªæ•°æ®ç‚¹ï¼ŒåŒæ—¶æ€§èƒ½è¾¾åˆ°77.53%ï¼Œä¼˜äºå…¨æ•°æ®é›†çš„77.04%ã€‚è¿™ä¸€ç»“æœæ˜¾ç¤ºäº†LearnAlignåœ¨æ•°æ®é€‰æ‹©æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ï¼ŒéªŒè¯äº†å…¶åœ¨å¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èå’Œç§‘å­¦ç ”ç©¶ç­‰éœ€è¦æ¨ç†èƒ½åŠ›çš„åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®é€‰æ‹©ï¼ŒLearnAlignå¯ä»¥æ˜¾è‘—æé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ã€‚æœªæ¥ï¼Œéšç€æ›´å¤šä¼˜åŒ–ç­–ç•¥çš„æå‡ºï¼ŒLearnAlignæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„é¢†åŸŸä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has become a key technique for enhancing LLMs' reasoning abilities, yet its data inefficiency remains a major bottleneck. To address this critical yet challenging issue, we present a novel gradient-alignment-based method, named LearnAlign, which intelligently selects the learnable and representative training reasoning data for RL post-training. To overcome the issue of response-length bias in gradient norms, we introduce the data learnability based on the success rate, which can indicate the learning potential of each data point. Experiments across three mathematical reasoning benchmarks demonstrate that our method significantly reduces training data requirements while achieving minor performance degradation or even improving performance compared to full-data training. For example, it reduces data requirements by up to 1,000 data points with better performance (77.53%) than that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show its effectiveness in the staged RL setting. This work provides valuable insights into data-efficient RL post-training and establishes a foundation for future research in optimizing reasoning data selection. To facilitate future work, we will release code.

