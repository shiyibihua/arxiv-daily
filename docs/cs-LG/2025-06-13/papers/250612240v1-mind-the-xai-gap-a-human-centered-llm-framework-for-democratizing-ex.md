---
layout: default
title: Mind the XAI Gap: A Human-Centered LLM Framework for Democratizing Explainable AI
---

# Mind the XAI Gap: A Human-Centered LLM Framework for Democratizing Explainable AI

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12240" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12240v1</a>
  <a href="https://arxiv.org/pdf/2506.12240.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12240v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12240v1', 'Mind the XAI Gap: A Human-Centered LLM Framework for Democratizing Explainable AI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eva Paraschou, Ioannis Arapakis, Sofia Yfantidou, Sebastian Macaluso, Athena Vakali

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: Accepted for publication at The 3rd World Conference on eXplainable Artificial Intelligence. This version corresponds to the camera-ready manuscript submitted to the conference proceedings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäººæœ¬ä¸­å¿ƒçš„LLMæ¡†æ¶ä»¥è§£å†³å¯è§£é‡ŠAIçš„é€æ˜æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯è§£é‡ŠAI` `å¤§å‹è¯­è¨€æ¨¡å‹` `äººæœ¬ä¸­å¿ƒ` `é€æ˜æ€§` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `ç”¨æˆ·ç ”ç©¶` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯è§£é‡ŠAIæ–¹æ³•å¤šé¢å‘ä¸“å®¶ï¼Œéä¸“å®¶éš¾ä»¥ç†è§£ï¼Œå¯¼è‡´é€æ˜æ€§ä¸è¶³ã€‚
2. æå‡ºä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæä¾›é€‚åˆä¸“å®¶å’Œéä¸“å®¶çš„è§£é‡Šã€‚
3. é€šè¿‡åŸºå‡†æµ‹è¯•å’Œç”¨æˆ·ç ”ç©¶ï¼ŒéªŒè¯äº†æ¡†æ¶çš„é«˜è´¨é‡å†…å®¹å’Œå¯¹éä¸“å®¶çš„å‹å¥½æ€§ï¼ŒSpearmanç›¸å…³ç³»æ•°è¾¾åˆ°0.92ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è¿…é€Ÿèå…¥å…³é”®å†³ç­–ç³»ç»Ÿï¼Œä½†å…¶åŸºç¡€çš„â€œé»‘ç®±â€æ¨¡å‹éœ€è¦å¯è§£é‡ŠAIï¼ˆXAIï¼‰è§£å†³æ–¹æ¡ˆä»¥å¢å¼ºé€æ˜æ€§ï¼Œç°æœ‰æ–¹æ¡ˆå¤šé¢å‘ä¸“å®¶ï¼Œéä¸“å®¶éš¾ä»¥ç†è§£ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªé¢†åŸŸã€æ¨¡å‹å’Œè§£é‡Šæ— å…³çš„é€šç”¨æ¡†æ¶ï¼Œç¡®ä¿é€æ˜æ€§å’Œäººæœ¬ä¸­å¿ƒçš„è§£é‡Šï¼Œé€‚åº”ä¸“å®¶å’Œéä¸“å®¶çš„éœ€æ±‚ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå°†é¢†åŸŸå’Œå¯è§£é‡Šæ€§ç›¸å…³çš„çŸ¥è¯†ä¼ é€’ç»™LLMsã€‚é€šè¿‡ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ¡†æ¶çš„é«˜å†…å®¹è´¨é‡å’Œå¯¹éä¸“å®¶çš„å‹å¥½æ€§ï¼Œå»ºç«‹äº†ä¿¡ä»»äºLLMsä½œä¸ºäººæœ¬ä¸­å¿ƒå¯è§£é‡ŠAIçš„æ¨åŠ¨è€…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯è§£é‡ŠAIæ–¹æ³•å¯¹éä¸“å®¶çš„ç†è§£éšœç¢ï¼Œç°æœ‰æ–¹æ³•å¤šä¸ºä¸“å®¶å¯¼å‘ï¼Œç¼ºä¹äººæœ¬ä¸­å¿ƒçš„é€æ˜æ€§å’Œå¯ç†è§£æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ä¸ªé¢†åŸŸã€æ¨¡å‹å’Œè§£é‡Šæ— å…³çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå°†ç›¸å…³çŸ¥è¯†èå…¥æ¨¡å‹ä¸­ï¼Œä»¥ä¾¿ä¸ºä¸åŒèƒŒæ™¯çš„ç”¨æˆ·æä¾›é€‚å½“çš„è§£é‡Šã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ¡†æ¶åŒ…æ‹¬ç»“æ„åŒ–æç¤ºå’Œç³»ç»Ÿè®¾ç½®ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€å“åº”ä¸­æä¾›éä¸“å®¶å¯ç†è§£çš„è§£é‡Šå’Œä¸“å®¶æ‰€éœ€çš„æŠ€æœ¯ä¿¡æ¯ï¼Œç¡®ä¿ç¬¦åˆé¢†åŸŸå’Œå¯è§£é‡Šæ€§åŸåˆ™ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ¡†æ¶çš„é€šç”¨æ€§å’Œå¯é‡å¤æ€§ï¼Œèƒ½å¤ŸåŒæ—¶æ»¡è¶³ä¸“å®¶å’Œéä¸“å®¶çš„éœ€æ±‚ï¼Œæ˜¾è‘—æå‡äº†å¯è§£é‡ŠAIçš„é€æ˜æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¡†æ¶é€šè¿‡ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•å»ºç«‹äº†çœŸå®çš„ä¸Šä¸‹æ–‡â€œåŒä¹‰è¯åº“â€ï¼Œå¹¶åœ¨è¶…è¿‡40ç§æ•°æ®ã€æ¨¡å‹å’ŒXAIç»„åˆä¸­è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿äº†é«˜è´¨é‡çš„å†…å®¹å’Œç”¨æˆ·å‹å¥½æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¡†æ¶çš„å†…å®¹è´¨é‡ä¸çœŸå®è§£é‡Šä¹‹é—´çš„Spearmanç›¸å…³ç³»æ•°è¾¾åˆ°0.92ï¼Œè¡¨æ˜é«˜ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œéä¸“å®¶ç”¨æˆ·å¯¹æ¡†æ¶æä¾›çš„è§£é‡Šæ„Ÿåˆ°æ›´æ˜“ç†è§£å’Œå‹å¥½ï¼Œæ˜¾è‘—æå‡äº†è§£é‡Šçš„å¯è§£é‡Šæ€§å’Œäººæ€§åŒ–ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å†³ç­–ã€é‡‘èåˆ†æå’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©éä¸“å®¶ç”¨æˆ·ç†è§£AIå†³ç­–è¿‡ç¨‹ï¼Œæå‡å¯¹AIç³»ç»Ÿçš„ä¿¡ä»»å’Œæ¥å—åº¦ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯èƒ½åœ¨å„ç±»éœ€è¦é€æ˜æ€§å’Œå¯è§£é‡Šæ€§çš„AIåº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Artificial Intelligence (AI) is rapidly embedded in critical decision-making systems, however their foundational ``black-box'' models require eXplainable AI (XAI) solutions to enhance transparency, which are mostly oriented to experts, making no sense to non-experts. Alarming evidence about AI's unprecedented human values risks brings forward the imperative need for transparent human-centered XAI solutions. In this work, we introduce a domain-, model-, explanation-agnostic, generalizable and reproducible framework that ensures both transparency and human-centered explanations tailored to the needs of both experts and non-experts. The framework leverages Large Language Models (LLMs) and employs in-context learning to convey domain- and explainability-relevant contextual knowledge into LLMs. Through its structured prompt and system setting, our framework encapsulates in one response explanations understandable by non-experts and technical information to experts, all grounded in domain and explainability principles. To demonstrate the effectiveness of our framework, we establish a ground-truth contextual ``thesaurus'' through a rigorous benchmarking with over 40 data, model, and XAI combinations for an explainable clustering analysis of a well-being scenario. Through a comprehensive quality and human-friendliness evaluation of our framework's explanations, we prove high content quality through strong correlations with ground-truth explanations (Spearman rank correlation=0.92) and improved interpretability and human-friendliness to non-experts through a user study (N=56). Our overall evaluation confirms trust in LLMs as HCXAI enablers, as our framework bridges the above Gaps by delivering (i) high-quality technical explanations aligned with foundational XAI methods and (ii) clear, efficient, and interpretable human-centered explanations for non-experts.

