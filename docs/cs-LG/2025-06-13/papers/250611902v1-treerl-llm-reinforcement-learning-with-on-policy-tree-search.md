---
layout: default
title: TreeRL: LLM Reinforcement Learning with On-Policy Tree Search
---

# TreeRL: LLM Reinforcement Learning with On-Policy Tree Search

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11902" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11902v1</a>
  <a href="https://arxiv.org/pdf/2506.11902.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11902v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11902v1', 'TreeRL: LLM Reinforcement Learning with On-Policy Tree Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, Yuxiao Dong

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: Accepted to ACL 2025 main conference

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/THUDM/TreeRL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTreeRLæ¡†æ¶ä»¥è§£å†³ä¼ ç»ŸRLæ–¹æ³•çš„æ¢ç´¢ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ ‘æœç´¢` `ä¸­é—´ç›‘ç£` `æ¨ç†ä»»åŠ¡` `æ¨¡å‹è®­ç»ƒ` `ä»£ç ç”Ÿæˆ` `æ•°å­¦æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸­æ¢ç´¢èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºTreeRLæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆåœ¨çº¿æ ‘æœç´¢å’Œä¸­é—´ç›‘ç£ï¼Œæå‡äº†RLè®­ç»ƒçš„æ•ˆç‡å’Œæ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTreeRLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ ‘æœç´¢å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ä¼ ç»Ÿæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ä¸å¸¸è§„çš„ç‹¬ç«‹é“¾é‡‡æ ·ç­–ç•¥ç›¸æ¯”ï¼Œæ ‘æœç´¢èƒ½å¤Ÿæ›´å¥½åœ°æ¢ç´¢æ¨ç†ç©ºé—´ï¼Œå¹¶åœ¨RLè®­ç»ƒä¸­æä¾›å¯†é›†çš„åœ¨çº¿è¿‡ç¨‹å¥–åŠ±ã€‚ç„¶è€Œï¼Œåœ¨åœ¨çº¿ç­–ç•¥çš„LLM RLä¸­ï¼Œæ ‘æœç´¢ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TreeRLï¼Œä¸€ä¸ªç›´æ¥ç»“åˆåœ¨çº¿æ ‘æœç´¢çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸­é—´ç›‘ç£ï¼Œæ¶ˆé™¤äº†å•ç‹¬è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„éœ€æ±‚ï¼Œé¿å…äº†åˆ†å¸ƒä¸åŒ¹é…å’Œå¥–åŠ±é»‘å®¢é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ ‘æœç´¢æ–¹æ³•ï¼Œé€šè¿‡ä»é«˜ä¸ç¡®å®šæ€§ä¸­é—´æ­¥éª¤è¿›è¡Œæˆ˜ç•¥æ€§åˆ†æ”¯ï¼Œæå‡äº†æœç´¢æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTreeRLåœ¨æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¼ ç»Ÿçš„ChainRLï¼Œå±•ç¤ºäº†æ ‘æœç´¢åœ¨LLMä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸­æ¢ç´¢ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‹¬ç«‹çš„å¥–åŠ±æ¨¡å‹ï¼Œå®¹æ˜“å‡ºç°åˆ†å¸ƒä¸åŒ¹é…å’Œå¥–åŠ±é»‘å®¢ç°è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„TreeRLæ¡†æ¶é€šè¿‡ç›´æ¥ç»“åˆåœ¨çº¿æ ‘æœç´¢ï¼Œåˆ©ç”¨ä¸­é—´ç›‘ç£æ¥ä¼˜åŒ–RLè®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…äº†å•ç‹¬è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„å¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTreeRLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ ‘æœç´¢æ¨¡å—å’Œä¸­é—´ç›‘ç£æœºåˆ¶ã€‚æ ‘æœç´¢æ¨¡å—è´Ÿè´£åœ¨æ¨ç†ç©ºé—´ä¸­è¿›è¡Œé«˜æ•ˆæ¢ç´¢ï¼Œè€Œä¸­é—´ç›‘ç£åˆ™æä¾›å®æ—¶åé¦ˆï¼Œå¸®åŠ©æ¨¡å‹è°ƒæ•´ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šTreeRLçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é«˜æ•ˆçš„æ ‘æœç´¢ç­–ç•¥ï¼Œé€šè¿‡ä»é«˜ä¸ç¡®å®šæ€§ä¸­é—´æ­¥éª¤è¿›è¡Œæˆ˜ç•¥æ€§åˆ†æ”¯ï¼Œæ˜¾è‘—æé«˜äº†æœç´¢æ•ˆç‡ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„éšæœºåˆ†æ”¯æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒTreeRLä¼˜åŒ–äº†æ ‘æœç´¢çš„åˆ†æ”¯ç­–ç•¥ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šï¼ŒTreeRLé›†æˆäº†å¤šå±‚æ¬¡çš„ä¸­é—´ç›‘ç£æœºåˆ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒTreeRLåœ¨æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„ChainRLæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œæ ‘æœç´¢ç­–ç•¥åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TreeRLæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¤æ‚æ¨ç†çš„é¢†åŸŸï¼Œå¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆå’Œè‡ªç„¶è¯­è¨€ç†è§£ç­‰ã€‚å…¶é«˜æ•ˆçš„æ¢ç´¢èƒ½åŠ›å’Œå®æ—¶åé¦ˆæœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—æå‡æ™ºèƒ½ç³»ç»Ÿçš„å†³ç­–è´¨é‡å’Œå“åº”é€Ÿåº¦ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€ç¼–ç¨‹è¾…åŠ©å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) with tree search has demonstrated superior performance in traditional reasoning tasks. Compared to conventional independent chain sampling strategies with outcome supervision, tree search enables better exploration of the reasoning space and provides dense, on-policy process rewards during RL training but remains under-explored in On-Policy LLM RL. We propose TreeRL, a reinforcement learning framework that directly incorporates on-policy tree search for RL training. Our approach includes intermediate supervision and eliminates the need for a separate reward model training. Existing approaches typically train a separate process reward model, which can suffer from distribution mismatch and reward hacking. We also introduce a cost-effective tree search approach that achieves higher search efficiency under the same generation token budget by strategically branching from high-uncertainty intermediate steps rather than using random branching. Experiments on challenging math and code reasoning benchmarks demonstrate that TreeRL achieves superior performance compared to traditional ChainRL, highlighting the potential of tree search for LLM. TreeRL is open-sourced at https://github.com/THUDM/TreeRL.

