---
layout: default
title: Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization, and Associative Recall Capacity
---

# Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization, and Associative Recall Capacity

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11891" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11891v1</a>
  <a href="https://arxiv.org/pdf/2506.11891.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11891v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11891v1', 'Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization, and Associative Recall Capacity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ningyuan Huang, Miguel Sarabia, Abhinav Moudgil, Pau Rodriguez, Luca Zappella, Federico Danieli

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºMambaä¸­çš„è¾“å…¥é€‰æ‹©æ€§å¯¹è¿‘ä¼¼èƒ½åŠ›å’Œè®°å¿†çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŠ¶æ€ç©ºé—´æ¨¡å‹` `è¾“å…¥é€‰æ‹©æ€§` `å‡½æ•°è¿‘ä¼¼` `é•¿æœŸè®°å¿†` `è”æƒ³å›å¿†` `Mambaæ¶æ„` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å¤„ç†å¤æ‚å‡½æ•°å’Œè®°å¿†ä¿æŒæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä¸è¿ç»­å‡½æ•°çš„è¿‘ä¼¼å’Œé•¿æœŸè®°å¿†çš„è¡°é€€é—®é¢˜ä¸Šã€‚
2. è®ºæ–‡æå‡ºäº†Mambaæ¶æ„ï¼Œé€šè¿‡å¼•å…¥è¾“å…¥é€‰æ‹©æ€§å’Œå·ç§¯ã€é—¨æ§æœºåˆ¶ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¿‘ä¼¼èƒ½åŠ›å’Œè®°å¿†ä¿æŒèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMambaåœ¨è¿‘ä¼¼ä¸è¿ç»­å‡½æ•°å’Œè”æƒ³å›å¿†ä»»åŠ¡ä¸Šä¼˜äºå…¶å‰èº«S4Dï¼ŒéªŒè¯äº†ç†è®ºæ„å»ºçš„ç´§å¯†æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œå°¤å…¶æ˜¯Mambaï¼Œæœ€è¿‘æˆä¸ºTransformerçš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆã€‚Mambaåœ¨å…¶SSMå±‚ï¼ˆS6ï¼‰ä¸­å¼•å…¥äº†è¾“å…¥é€‰æ‹©æ€§ï¼Œå¹¶åœ¨å…¶æ¨¡å—å®šä¹‰ä¸­ç»“åˆäº†å·ç§¯å’Œé—¨æ§æœºåˆ¶ã€‚å°½ç®¡è¿™äº›æ”¹è¿›æå‡äº†Mambaçš„æ€§èƒ½ï¼Œä½†å…¶å¦‚ä½•åˆ©ç”¨è¾“å…¥é€‰æ‹©æ€§çš„é¢å¤–åŠŸèƒ½ä»ä¸æ¸…æ™°ã€‚æœ¬æ–‡æ¢è®¨äº†è¾“å…¥é€‰æ‹©æ€§åœ¨Mambaä¸­çš„ä½œç”¨ï¼Œåˆ†æå…¶å¯¹å‡½æ•°è¿‘ä¼¼èƒ½åŠ›ã€é•¿æœŸè®°å¿†å’Œè”æƒ³å›å¿†èƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬è¯æ˜äº†Mambaçš„S6å±‚èƒ½å¤Ÿè¡¨ç¤ºHaarå°æ³¢çš„æŠ•å½±ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è¿‘ä¼¼å®é™…ä¸­å¸¸è§çš„ä¸è¿ç»­å‡½æ•°æ–¹é¢çš„ä¼˜åŠ¿ï¼›åŒæ—¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†S6å±‚å¦‚ä½•åŠ¨æ€æŠµæ¶ˆè®°å¿†è¡°é€€ï¼›æœ€åï¼Œæˆ‘ä»¬æä¾›äº†ä½¿ç”¨ä¸åŒæ··åˆå™¨çš„Mambaæ¶æ„åœ¨MQARè”æƒ³å›å¿†ä»»åŠ¡ä¸Šçš„è§£æè§£ã€‚æˆ‘ä»¬çš„å‘ç°ä¸ºMambaæä¾›äº†æœºåˆ¶ç†è§£ï¼Œå¹¶æ­ç¤ºäº†æ”¹è¿›çš„æœºä¼šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³Mambaæ¶æ„åœ¨å‡½æ•°è¿‘ä¼¼ã€é•¿æœŸè®°å¿†å’Œè”æƒ³å›å¿†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è¾“å…¥é€‰æ‹©æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è¾“å…¥é€‰æ‹©æ€§ï¼ŒMambaèƒ½å¤Ÿæ›´çµæ´»åœ°å¤„ç†è¾“å…¥ä¿¡æ¯ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨è®°å¿†ä¿æŒå’Œå‡½æ•°è¿‘ä¼¼æ–¹é¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMambaæ¶æ„ç”±å¤šä¸ªæ¨¡å—ç»„æˆï¼ŒåŒ…æ‹¬S6å±‚ã€å·ç§¯å±‚å’Œé—¨æ§æœºåˆ¶ã€‚S6å±‚è´Ÿè´£è¾“å…¥é€‰æ‹©æ€§ï¼Œå·ç§¯å±‚ç”¨äºç‰¹å¾æå–ï¼Œè€Œé—¨æ§æœºåˆ¶åˆ™è°ƒèŠ‚ä¿¡æ¯æµåŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šMambaçš„S6å±‚èƒ½å¤Ÿè¡¨ç¤ºHaarå°æ³¢çš„æŠ•å½±ï¼Œè¿™æ˜¯å…¶ç›¸è¾ƒäºS4Dçš„ä¸»è¦åˆ›æ–°ï¼Œä½¿å…¶åœ¨å¤„ç†ä¸è¿ç»­å‡½æ•°æ—¶è¡¨ç°æ›´ä½³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMambaçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥é€‚åº”è¾“å…¥é€‰æ‹©æ€§å¸¦æ¥çš„æ–°ç‰¹æ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨åŠ¨æ€è®°å¿†å’Œè¿‘ä¼¼èƒ½åŠ›ä¸Šçš„æå‡ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œå±‚æ¬¡ç»“æ„åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMambaåœ¨MQARè”æƒ³å›å¿†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯ä¸S4Dç›¸æ¯”ï¼Œè¿‘ä¼¼ä¸è¿ç»­å‡½æ•°çš„èƒ½åŠ›æå‡æ˜¾è‘—ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼ŒMambaåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç°å‡ºå¼ºå¤§çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å›¾åƒè¯†åˆ«å’Œæ—¶é—´åºåˆ—é¢„æµ‹ç­‰ã€‚Mambaæ¶æ„çš„è¾“å…¥é€‰æ‹©æ€§å’Œè®°å¿†èƒ½åŠ›ä½¿å…¶åœ¨å¤„ç†å¤æ‚æ•°æ®æ—¶å…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> State-Space Models (SSMs), and particularly Mamba, have recently emerged as a promising alternative to Transformers. Mamba introduces input selectivity to its SSM layer (S6) and incorporates convolution and gating into its block definition. While these modifications do improve Mamba's performance over its SSM predecessors, it remains largely unclear how Mamba leverages the additional functionalities provided by input selectivity, and how these interact with the other operations in the Mamba architecture. In this work, we demystify the role of input selectivity in Mamba, investigating its impact on function approximation power, long-term memorization, and associative recall capabilities. In particular: (i) we prove that the S6 layer of Mamba can represent projections onto Haar wavelets, providing an edge over its Diagonal SSM (S4D) predecessor in approximating discontinuous functions commonly arising in practice; (ii) we show how the S6 layer can dynamically counteract memory decay; (iii) we provide analytical solutions to the MQAR associative recall task using the Mamba architecture with different mixers -- Mamba, Mamba-2, and S4D. We demonstrate the tightness of our theoretical constructions with empirical results on concrete tasks. Our findings offer a mechanistic understanding of Mamba and reveal opportunities for improvement.

