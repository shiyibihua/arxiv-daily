---
layout: default
title: Growing with Experience: Growing Neural Networks in Deep Reinforcement Learning
---

# Growing with Experience: Growing Neural Networks in Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11706" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11706v1</a>
  <a href="https://arxiv.org/pdf/2506.11706.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11706v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11706v1', 'Growing with Experience: Growing Neural Networks in Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lukas Fehring, Marius Lindauer, Theresa Eimer

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: 3 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGrowNNä»¥è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ç½‘ç»œè®­ç»ƒå›°éš¾é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `ç¥ç»ç½‘ç»œå¢é•¿` `ç­–ç•¥å­¦ä¹ ` `åŠ¨æ€ç½‘ç»œ` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒä¸­å‹ç½‘ç»œæ—¶é¢ä¸´å›°éš¾ï¼Œé™åˆ¶äº†ç­–ç•¥çš„å¤æ‚æ€§å’Œè¡¨ç°èƒ½åŠ›ã€‚
2. GrowNNé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¢åŠ ç½‘ç»œå±‚æ•°ï¼Œå…è®¸ç½‘ç»œå®¹é‡çš„å¢é•¿ï¼ŒåŒæ—¶ä¿æŒç½‘ç»œçš„å¯è®­ç»ƒæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGrowNNåœ¨MiniHackå’ŒMujocoç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†ä»£ç†çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡è¶Šæ¥è¶Šå¤§çš„æ¨¡å‹åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå¼•å‘äº†é©å‘½ï¼Œä½†è®­ç»ƒä¸­å‹ç½‘ç»œè¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™é™åˆ¶äº†æˆ‘ä»¬èƒ½å¤Ÿå­¦ä¹ çš„ç­–ç•¥å¤æ‚æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºGrowNNï¼Œä¸€ç§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨æ¸è¿›å¼ç½‘ç»œå¢é•¿çš„ç®€å•æœ‰æ•ˆæ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªå°å‹ç½‘ç»œä»¥å­¦ä¹ åˆå§‹ç­–ç•¥ï¼Œç„¶ååœ¨ä¸æ”¹å˜ç¼–ç å‡½æ•°çš„æƒ…å†µä¸‹æ·»åŠ å±‚ã€‚åç»­æ›´æ–°å¯ä»¥åˆ©ç”¨æ–°å¢çš„å±‚æ¥å­¦ä¹ æ›´å…·è¡¨ç°åŠ›çš„ç­–ç•¥ï¼Œéšç€ç­–ç•¥å¤æ‚æ€§çš„å¢åŠ è€Œå¢åŠ ç½‘ç»œå®¹é‡ã€‚GrowNNå¯ä»¥æ— ç¼é›†æˆåˆ°å¤§å¤šæ•°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ä»£ç†ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨MiniHackå’ŒMujocoä¸Šçš„è¡¨ç°æœ‰æ‰€æå‡ï¼ŒGrowNNé€æ­¥åŠ æ·±çš„ç½‘ç»œåœ¨MiniHack Roomä¸Šæ¯”åŒç­‰å¤§å°çš„é™æ€ç½‘ç»œæ€§èƒ½æé«˜äº†48%ï¼Œåœ¨Antä¸Šæé«˜äº†72%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¼ºåŒ–å­¦ä¹ ä¸­è®­ç»ƒä¸­å‹ç½‘ç»œçš„å›°éš¾ï¼Œç°æœ‰æ–¹æ³•åœ¨ç½‘ç»œå®¹é‡å’Œè®­ç»ƒæ•ˆç‡ä¹‹é—´å­˜åœ¨çŸ›ç›¾ï¼Œé™åˆ¶äº†ç­–ç•¥çš„å¤æ‚æ€§å’Œè¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGrowNNçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é€æ­¥å¢åŠ ç½‘ç»œå±‚æ•°æ¥æ‰©å±•ç½‘ç»œå®¹é‡ï¼Œè€Œä¸æ”¹å˜å·²ç¼–ç çš„å‡½æ•°ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ç½‘ç»œèƒ½å¤Ÿåœ¨å­¦ä¹ åˆå§‹ç­–ç•¥åï¼Œéšç€ç­–ç•¥å¤æ‚æ€§çš„å¢åŠ è€ŒåŠ¨æ€è°ƒæ•´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGrowNNçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åˆå§‹çš„å°å‹ç½‘ç»œè®­ç»ƒé˜¶æ®µï¼Œéšååœ¨ä¸æ”¹å˜ç½‘ç»œåŠŸèƒ½çš„æƒ…å†µä¸‹é€æ­¥æ·»åŠ æ–°å±‚ã€‚æ¯æ¬¡æ·»åŠ å±‚åï¼Œç½‘ç»œå¯ä»¥åˆ©ç”¨æ–°å¢çš„å±‚è¿›è¡Œæ›´å¤æ‚ç­–ç•¥çš„å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šGrowNNçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¸è¿›å¼ç½‘ç»œå¢é•¿æœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„é™æ€ç½‘ç»œç»“æ„å½¢æˆé²œæ˜å¯¹æ¯”ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç½‘ç»œèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çµæ´»é€‚åº”ç­–ç•¥çš„å¤æ‚æ€§å˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GrowNNä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬å±‚çš„æ·»åŠ ç­–ç•¥ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠç½‘ç»œç»“æ„çš„åŠ¨æ€è°ƒæ•´ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥çš„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä»¥ç¡®ä¿ç½‘ç»œçš„æœ‰æ•ˆè®­ç»ƒå’Œæ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGrowNNåœ¨MiniHack Roomç¯å¢ƒä¸­æ¯”åŒç­‰å¤§å°çš„é™æ€ç½‘ç»œæ€§èƒ½æé«˜äº†48%ï¼Œåœ¨Antç¯å¢ƒä¸­æé«˜äº†72%ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†GrowNNåœ¨æå‡å¼ºåŒ–å­¦ä¹ ä»£ç†æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GrowNNçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªå¼ºåŒ–å­¦ä¹ åº”ç”¨åœºæ™¯ä¸­å…·æœ‰æ½œåœ¨ä»·å€¼ï¼ŒåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ï¼ŒGrowNNèƒ½å¤Ÿå¸®åŠ©æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­åšå‡ºæ›´ä¼˜å†³ç­–ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While increasingly large models have revolutionized much of the machine learning landscape, training even mid-sized networks for Reinforcement Learning (RL) is still proving to be a struggle. This, however, severely limits the complexity of policies we are able to learn. To enable increased network capacity while maintaining network trainability, we propose GrowNN, a simple yet effective method that utilizes progressive network growth during training. We start training a small network to learn an initial policy. Then we add layers without changing the encoded function. Subsequent updates can utilize the added layers to learn a more expressive policy, adding capacity as the policy's complexity increases. GrowNN can be seamlessly integrated into most existing RL agents. Our experiments on MiniHack and Mujoco show improved agent performance, with incrementally GrowNN-deeper networks outperforming their respective static counterparts of the same size by up to 48% on MiniHack Room and 72% on Ant.

