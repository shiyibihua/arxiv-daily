---
layout: default
title: Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning with Heterogeneous LoRA Allocation
---

# Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning with Heterogeneous LoRA Allocation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12213" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12213v1</a>
  <a href="https://arxiv.org/pdf/2506.12213.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12213v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12213v1', 'Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning with Heterogeneous LoRA Allocation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zikai Zhang, Ping Liu, Jiahao Xu, Rui Hu

**åˆ†ç±»**: cs.LG, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: Accepted to TNNLS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFed-HeLLoä»¥è§£å†³å¼‚æ„èµ„æºä¸‹çš„è”é‚¦æ¨¡å‹å¾®è°ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `ä½ç§©é€‚åº”` `æ¨¡å‹å¾®è°ƒ` `å¼‚æ„èµ„æº` `åŠ¨æ€åˆ†é…` `å‡ ä½•æ¨¡å¼` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è”é‚¦å­¦ä¹ å¾®è°ƒæ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘å®¢æˆ·ç«¯çš„å¼‚æ„èµ„æºï¼Œå¯¼è‡´å¾®è°ƒæ€§èƒ½å—é™ã€‚
2. æœ¬æ–‡æå‡ºFed-HeLLoæ¡†æ¶ï¼Œé€šè¿‡å¼‚æ„LoRAåˆ†é…ç­–ç•¥ï¼Œå…è®¸å®¢æˆ·ç«¯æ ¹æ®èµ„æºèƒ½åŠ›å’Œå±‚é‡è¦æ€§è¿›è¡Œå¾®è°ƒã€‚
3. åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒFed-HeLLoåœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œæå‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦å­¦ä¹ æœ€è¿‘è¢«ç”¨äºåœ¨å¤šä¸ªå®¢æˆ·ç«¯ä¹‹é—´åä½œå¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯åŸºäºä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çš„å¾®è°ƒæ–¹æ³•ï¼Œå…è®¸å®¢æˆ·ç«¯åœ¨æœ¬åœ°å¾®è°ƒå°‘é‡å¯è®­ç»ƒå‚æ•°ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æœªè€ƒè™‘å®¢æˆ·ç«¯çš„å¼‚æ„èµ„æºæˆ–ç¼ºä¹æœ‰æ•ˆçš„æœ¬åœ°è®­ç»ƒç­–ç•¥ã€‚æœ¬æ–‡æå‡ºFed-HeLLoï¼Œä¸€ä¸ªæ–°é¢–çš„è”é‚¦LoRAå¾®è°ƒæ¡†æ¶ï¼Œä½¿å®¢æˆ·ç«¯èƒ½å¤Ÿä»¥ä¸åŒçš„æœ¬åœ°å¯è®­ç»ƒLoRAå±‚åä½œå¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬å¼€å‘äº†å‡ ç§å¼‚æ„LoRAåˆ†é…ç­–ç•¥ï¼ŒåŸºäºå®¢æˆ·ç«¯çš„èµ„æºèƒ½åŠ›å’Œå±‚çš„é‡è¦æ€§è‡ªé€‚åº”åˆ†é…å¯è®­ç»ƒLoRAå±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFed-HeLLoåœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è”é‚¦å­¦ä¹ å¾®è°ƒæ–¹æ³•æœªè€ƒè™‘å®¢æˆ·ç«¯å¼‚æ„èµ„æºçš„é—®é¢˜ï¼Œå¯¼è‡´å¾®è°ƒæ€§èƒ½ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå›ºå®šçš„å¯è®­ç»ƒå‚æ•°åˆ†é…ï¼Œæ— æ³•é€‚åº”ä¸åŒå®¢æˆ·ç«¯çš„èµ„æºèƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFed-HeLLoæ¡†æ¶é€šè¿‡å¼‚æ„LoRAåˆ†é…ç­–ç•¥ï¼Œå…è®¸å®¢æˆ·ç«¯æ ¹æ®è‡ªèº«èµ„æºå’Œå±‚çš„é‡è¦æ€§åŠ¨æ€è°ƒæ•´å¯è®­ç»ƒLoRAå±‚ï¼Œä»è€Œæé«˜å¾®è°ƒæ•ˆç‡å’Œæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆè¯„ä¼°å®¢æˆ·ç«¯çš„èµ„æºèƒ½åŠ›ï¼Œç„¶åæ ¹æ®åŠ¨æ€å±‚é‡è¦æ€§åˆ†é…å¯è®­ç»ƒLoRAå±‚ã€‚å…·ä½“å®ç°ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºFisherä¿¡æ¯çŸ©é˜µçš„åˆ†é…ç­–ç•¥å’Œå‡ ä½•å®šä¹‰çš„åˆ†é…ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¼‚æ„LoRAåˆ†é…ç­–ç•¥ï¼ˆHLAï¼‰ï¼Œé€šè¿‡åŠ¨æ€å’Œå†…åœ¨å±‚é‡è¦æ€§ç»“åˆï¼Œä¼˜åŒ–äº†å¯è®­ç»ƒå±‚çš„åˆ†é…æ–¹å¼ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å›ºå®šåˆ†é…ç­–ç•¥æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŠ¨æ€æ¢¯åº¦èŒƒæ•°ä¿¡æ¯æ¥è¯„ä¼°å±‚çš„é‡è¦æ€§ï¼Œå¹¶è®¾è®¡äº†å‡ ä½•æ¨¡å¼ï¼ˆå¦‚ä¸‰è§’å½¢ã€å€’ä¸‰è§’å½¢ç­‰ï¼‰æ¥ä¼˜åŒ–å¯è®­ç»ƒLoRAå±‚çš„åˆ†é…ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†éšæœºå‡ ä½•å®šä¹‰çš„HLAç­–ç•¥ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFed-HeLLoåœ¨äº”ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„å¾®è°ƒæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶åœ¨æç«¯éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆNon-IIDï¼‰æƒ…å†µä¸‹ï¼Œæ¨¡å‹å‡†ç¡®æ€§æå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼Œå±•ç°äº†å…¶åœ¨èµ„æºå¼‚æ„ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ã€æ™ºèƒ½è®¾å¤‡åä½œå’Œè¾¹ç¼˜è®¡ç®—ç­‰åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–è”é‚¦å­¦ä¹ ä¸­çš„æ¨¡å‹å¾®è°ƒï¼ŒFed-HeLLoèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹æå‡æ¨¡å‹æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated Learning has recently been utilized to collaboratively fine-tune foundation models across multiple clients. Notably, federated low-rank adaptation LoRA-based fine-tuning methods have recently gained attention, which allows clients to fine-tune FMs with a small portion of trainable parameters locally. However, most existing methods do not account for the heterogeneous resources of clients or lack an effective local training strategy to maximize global fine-tuning performance under limited resources. In this work, we propose Fed-HeLLo, a novel federated LoRA-based fine-tuning framework that enables clients to collaboratively fine-tune an FM with different local trainable LoRA layers. To ensure its effectiveness, we develop several heterogeneous LoRA allocation (HLA) strategies that adaptively allocate local trainable LoRA layers based on clients' resource capabilities and the layer importance. Specifically, based on the dynamic layer importance, we design a Fisher Information Matrix score-based HLA that leverages dynamic gradient norm information. To better stabilize the training process, we consider the intrinsic importance of LoRA layers and design a Geometrically-Defined HLA strategy. It shapes the collective distribution of trainable LoRA layers into specific geometric patterns, such as Triangle, Inverted Triangle, Bottleneck, and Uniform. Moreover, we extend GD-HLA into a randomized version, named Randomized Geometrically-Defined HLA, for enhanced model accuracy with randomness. By co-designing the proposed HLA strategies, we incorporate both the dynamic and intrinsic layer importance into the design of our HLA strategy. We evaluate our approach on five datasets under diverse federated LoRA fine-tuning settings, covering three levels of data distribution from IID to extreme Non-IID. Results show that Fed-HeLLo with HLA strategies is both effective and efficient.

