---
layout: default
title: Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning
---

# Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03823" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.03823v1</a>
  <a href="https://arxiv.org/pdf/2510.03823.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03823v1" onclick="toggleFavorite(this, '2510.03823v1', 'Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Adam Haroon, Tristan Schuler

**ÂàÜÁ±ª**: cs.LG, cs.MA, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-04

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÁöÑÈ´òÁ©∫Ê∞îÁêÉÂàÜÂ∏ÉÂºèÂå∫ÂüüË¶ÜÁõñÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†` `È´òÁ©∫Ê∞îÁêÉ` `ÂàÜÂ∏ÉÂºèÂå∫ÂüüË¶ÜÁõñ` `ÂçèÂêåÊéßÂà∂` `QMIX`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈ´òÁ©∫Ê∞îÁêÉÂçèÂêåÊñπÊ≥ïÂú®Â∞èÂûãÂõ¢ÈòüÂíåÂ±ÄÈÉ®‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÊó†Ê≥ïÊúâÊïàÂ∫îÂØπÂ§çÊùÇÁéØÂ¢É„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Âü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÁöÑÂçèÂêåÊéßÂà∂ÊñπÊ≥ïÔºåÂà©Áî®QMIXÁÆóÊ≥ïÂÆûÁé∞È´òÁ©∫Ê∞îÁêÉÁöÑÂàÜÂ∏ÉÂºèÂå∫ÂüüË¶ÜÁõñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÊÄßËÉΩ‰∏éÁêÜËÆ∫ÊúÄ‰ºòÁöÑÁ°ÆÂÆöÊÄßÊñπÊ≥ïÁõ∏ÂΩìÔºå‰∏∫Êõ¥Â§çÊùÇÁöÑËá™‰∏ª‰ªªÂä°Êèê‰æõÂü∫Á°Ä„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÈ¶ñÊ¨°Â∞ÜÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†(MARL)Â∫îÁî®‰∫éÈ´òÁ©∫Ê∞îÁêÉ(HAB)ÁöÑÂçèÂêåÊéßÂà∂Ôºå‰ª•ÂÆûÁé∞ÂàÜÂ∏ÉÂºèÂå∫ÂüüË¶ÜÁõñ„ÄÇÈíàÂØπÂ∞èÂûãÂõ¢ÈòüÂíåÂ±ÄÈÉ®‰ªªÂä°‰∏≠Áé∞ÊúâÂü∫‰∫éÁ°ÆÂÆöÊÄßÊñπÊ≥ïÔºàÂ¶ÇVoronoiÂàÜÂâ≤ÂíåÊûÅÂÄºÊêúÁ¥¢ÊéßÂà∂ÔºâÁöÑHABÂçèÂêåÊéßÂà∂ÊïàÊûú‰∏ç‰Ω≥ÁöÑÈóÆÈ¢òÔºåÊú¨ÊñáÊâ©Â±ï‰∫ÜÂÖàÂâçÂºÄÂèëÁöÑÂº∫ÂåñÂ≠¶‰π†‰ªøÁúüÁéØÂ¢É(RLHAB)Ôºå‰ª•ÊîØÊåÅÂêà‰ΩúÂ§öÊô∫ËÉΩ‰ΩìÂ≠¶‰π†Ôºå‰ΩøÂ§ö‰∏™Êô∫ËÉΩ‰ΩìËÉΩÂ§üÂú®ÁúüÂÆûÂ§ßÊ∞îÊù°‰ª∂‰∏ãÂêåÊó∂ËøêË°å„ÄÇÊú¨ÊñáÈááÁî®QMIXÁÆóÊ≥ïËøõË°åHABÂå∫ÂüüË¶ÜÁõñÂçèÂêåÔºåÂà©Áî®ÈõÜ‰∏≠ÂºèËÆ≠ÁªÉÂíåÂàÜÊï£ÂºèÊâßË°åÊù•Â∫îÂØπÂ§ßÊ∞îËΩ¶ËæÜÂçèÂêåÊåëÊàò„ÄÇËØ•ÊñπÊ≥ïÈááÁî®‰∏ìÈó®ÁöÑËßÇÊµãÁ©∫Èó¥ÔºåÊèê‰æõ‰∏™‰ΩìÁä∂ÊÄÅ„ÄÅÁéØÂ¢É‰∏ä‰∏ãÊñáÂíåÈòüÂèãÊï∞ÊçÆÔºåÂπ∂ÈááÁî®ÂàÜÂ±ÇÂ•ñÂä±Ôºå‰ºòÂÖàËÄÉËôëË¶ÜÁõñËåÉÂõ¥ÔºåÂêåÊó∂ÈºìÂä±Á©∫Èó¥ÂàÜÂ∏É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåQMIXÂú®ÂàÜÂ∏ÉÂºèÂå∫ÂüüË¶ÜÁõñÊñπÈù¢ÂèñÂæó‰∫Ü‰∏éÁêÜËÆ∫‰∏äÊúÄ‰ºòÁöÑÂá†‰ΩïÁ°ÆÂÆöÊÄßÊñπÊ≥ïÁõ∏‰ººÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜMARLÊñπÊ≥ïÔºåÂπ∂‰∏∫Êõ¥Â§çÊùÇÁöÑËá™‰∏ªÂ§öHAB‰ªªÂä°Â•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥È´òÁ©∫Ê∞îÁêÉÁºñÈòüÂú®Â§çÊùÇÂ§ßÊ∞îÁéØÂ¢É‰∏ãÁöÑÂàÜÂ∏ÉÂºèÂå∫ÂüüË¶ÜÁõñÈóÆÈ¢ò„ÄÇÁé∞ÊúâÂü∫‰∫éÁ°ÆÂÆöÊÄßËßÑÂàôÔºàÂ¶ÇVoronoiÂàÜÂâ≤ÔºâÁöÑÊñπÊ≥ïÂú®Èù¢ÂØπÂ∞èÂûãÁºñÈòüÂíåÂä®ÊÄÅÁéØÂ¢ÉÊó∂ÔºåÈöæ‰ª•‰øùËØÅË¶ÜÁõñÊïàÁéáÂíåÈ≤ÅÊ£íÊÄßÔºåÈúÄË¶ÅÊõ¥ÁÅµÊ¥ªÁöÑÂçèÂêåÁ≠ñÁï•„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÔºàMARLÔºâÁöÑ‰ºòÂäøÔºåÈÄöËøáÂ≠¶‰π†ÁöÑÊñπÂºèËÆ©È´òÁ©∫Ê∞îÁêÉËá™‰∏ªÂú∞Ë∞ÉÊï¥‰ΩçÁΩÆÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑÂå∫ÂüüË¶ÜÁõñ„ÄÇMARLËÉΩÂ§üÂ§ÑÁêÜÂ§çÊùÇÁöÑÁéØÂ¢É‰∫§‰∫íÂíåÊô∫ËÉΩ‰ΩìÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºåÂÖãÊúç‰º†ÁªüÁ°ÆÂÆöÊÄßÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂Âü∫‰∫éÈõÜ‰∏≠ÂºèËÆ≠ÁªÉ„ÄÅÂàÜÂ∏ÉÂºèÊâßË°åÔºàCTDEÔºâËåÉÂºè„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂú®ËÆ≠ÁªÉÈò∂ÊÆµÔºåÊâÄÊúâÊô∫ËÉΩ‰ΩìÁöÑËßÇÊµãÂíåÂ•ñÂä±‰ø°ÊÅØË¢´ÈõÜ‰∏≠Ëµ∑Êù•ÔºåÁî®‰∫éËÆ≠ÁªÉ‰∏Ä‰∏™ËÅîÂêàÁöÑQÂáΩÊï∞„ÄÇÂú®ÊâßË°åÈò∂ÊÆµÔºåÊØè‰∏™Êô∫ËÉΩ‰ΩìÂè™Ê†πÊçÆËá™Â∑±ÁöÑÂ±ÄÈÉ®ËßÇÊµãÂÅöÂá∫ÂÜ≥Á≠ñ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÊ®°ÂùóÔºöÁéØÂ¢É‰ªøÁúüÂô®ÔºàRLHABÔºâÔºåÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºàQMIXÔºâÔºå‰ª•ÂèäÂÆöÂà∂ÂåñÁöÑËßÇÊµãÁ©∫Èó¥ÂíåÂ•ñÂä±ÂáΩÊï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜMARLÂ∫îÁî®‰∫éÈ´òÁ©∫Ê∞îÁêÉÁöÑÂçèÂêåÊéßÂà∂ÔºåÂπ∂ÈíàÂØπÈ´òÁ©∫Ê∞îÁêÉÁöÑÁâπÁÇπËÆæËÆ°‰∫Ü‰∏ìÈó®ÁöÑËßÇÊµãÁ©∫Èó¥ÂíåÂ•ñÂä±ÂáΩÊï∞„ÄÇ‰∏é‰º†ÁªüÁöÑÂçïÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†Áõ∏ÊØîÔºåMARLËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÊô∫ËÉΩ‰Ωì‰πãÈó¥ÁöÑÂçèÂêåÂÖ≥Á≥ªÔºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÊïàÁöÑÂå∫ÂüüË¶ÜÁõñ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËßÇÊµãÁ©∫Èó¥ÂåÖÊã¨‰∏™‰ΩìÁä∂ÊÄÅÔºà‰ΩçÁΩÆ„ÄÅÈÄüÂ∫¶Á≠âÔºâ„ÄÅÁéØÂ¢É‰∏ä‰∏ãÊñáÔºàÈ£éÂú∫‰ø°ÊÅØÁ≠âÔºâÂíåÈòüÂèãÊï∞ÊçÆÔºà‰ΩçÁΩÆÁ≠âÔºâ„ÄÇÂ•ñÂä±ÂáΩÊï∞ÈááÁî®ÂàÜÂ±ÇÁªìÊûÑÔºåÈ¶ñÂÖàÂ•ñÂä±Ë¶ÜÁõñÈù¢ÁßØÔºåÁÑ∂ÂêéÈºìÂä±Êô∫ËÉΩ‰ΩìÂàÜÊï£ÂàÜÂ∏ÉÔºåÈÅøÂÖçËÅöÈõÜ„ÄÇQMIXÁÆóÊ≥ïÈááÁî®Ê∑∑ÂêàÁΩëÁªúÁªìÊûÑÔºåÂ∞Ü‰∏™‰ΩìQÂÄºÊ∑∑ÂêàÊàêËÅîÂêàQÂÄºÔºå‰ªéËÄå‰øùËØÅÂ≠¶‰π†ÁöÑÁ®≥ÂÆöÊÄßÂíåÊî∂ÊïõÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éQMIXÁöÑMARLÊñπÊ≥ïÂú®È´òÁ©∫Ê∞îÁêÉÂàÜÂ∏ÉÂºèÂå∫ÂüüË¶ÜÁõñ‰ªªÂä°‰∏≠ÔºåËÉΩÂ§üËææÂà∞‰∏éÁêÜËÆ∫ÊúÄ‰ºòÁöÑÂá†‰ΩïÁ°ÆÂÆöÊÄßÊñπÊ≥ïÁõ∏ËøëÁöÑÊÄßËÉΩ„ÄÇËøôÈ™åËØÅ‰∫ÜMARLÊñπÊ≥ïÂú®È´òÁ©∫Ê∞îÁêÉÂçèÂêåÊéßÂà∂‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊñπÂêë„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÔºàÂ¶ÇË¶ÜÁõñÁéá„ÄÅË¶ÜÁõñÊó∂Èó¥Á≠âÔºâÊú™Âú®ÊëòË¶Å‰∏≠ÊòéÁ°ÆÁªôÂá∫„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÁéØÂ¢ÉÁõëÊµã„ÄÅÁÅæÂÆ≥ÊïëÊè¥„ÄÅÈÄö‰ø°ÁΩëÁªúÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÈ´òÁ©∫Ê∞îÁêÉÁöÑÂçèÂêåË¶ÜÁõñÔºåÂèØ‰ª•ÂÆûÁé∞ÂØπÁâπÂÆöÂå∫ÂüüÁöÑÊåÅÁª≠ÁõëÊéßÂíåÊï∞ÊçÆÈááÈõÜÔºå‰∏∫ÂÜ≥Á≠ñÊèê‰æõÊîØÊåÅ„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°Ôºå‰æãÂ¶ÇËá™‰∏ªÂØºËà™„ÄÅÁõÆÊ†áË∑üË∏™Á≠âÔºåÊé®Âä®È´òÁ©∫Ê∞îÁêÉÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> High Altitude Balloons (HABs) can leverage stratospheric wind layers for limited horizontal control, enabling applications in reconnaissance, environmental monitoring, and communications networks. Existing multi-agent HAB coordination approaches use deterministic methods like Voronoi partitioning and extremum seeking control for large global constellations, which perform poorly for smaller teams and localized missions. While single-agent HAB control using reinforcement learning has been demonstrated on HABs, coordinated multi-agent reinforcement learning (MARL) has not yet been investigated. This work presents the first systematic application of multi-agent reinforcement learning (MARL) to HAB coordination for distributed area coverage. We extend our previously developed reinforcement learning simulation environment (RLHAB) to support cooperative multi-agent learning, enabling multiple agents to operate simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area coverage coordination, leveraging Centralized Training with Decentralized Execution to address atmospheric vehicle coordination challenges. Our approach employs specialized observation spaces providing individual state, environmental context, and teammate data, with hierarchical rewards prioritizing coverage while encouraging spatial distribution. We demonstrate that QMIX achieves similar performance to the theoretically optimal geometric deterministic method for distributed area coverage, validating the MARL approach and providing a foundation for more complex autonomous multi-HAB missions where deterministic methods become intractable.

