---
layout: default
title: Deep Reinforcement Learning for Multi-Agent Coordination
---

# Deep Reinforcement Learning for Multi-Agent Coordination

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03592" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03592v1</a>
  <a href="https://arxiv.org/pdf/2510.03592.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03592v1" onclick="toggleFavorite(this, '2510.03592v1', 'Deep Reinforcement Learning for Multi-Agent Coordination')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kehinde O. Aina, Sehoon Ha

**åˆ†ç±»**: cs.LG, cs.AI, cs.MA, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-04

**å¤‡æ³¨**: 11 pages, 8 figures, 1 table, presented at SWARM 2022, to be published in Journal of Artificial Life and Robotics

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè™šæ‹Ÿä¿¡æ¯ç´ çš„S-MADRLæ¡†æ¶ï¼Œè§£å†³æ‹¥æŒ¤ç¯å¢ƒä¸­å¤šæ™ºèƒ½ä½“é«˜æ•ˆååŒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `ä¿¡æ¯ç´ ` `ååŒ` `è¯¾ç¨‹å­¦ä¹ ` `å»ä¸­å¿ƒåŒ–` `æœºå™¨äºº` `æ‹¥æŒ¤ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MADQNã€MADDPGã€MAPPOç­‰ç®—æ³•åœ¨å¤æ‚å¤šæ™ºèƒ½ä½“ååŒä»»åŠ¡ä¸­å­˜åœ¨æ”¶æ•›æ€§å’Œå¯æ‰©å±•æ€§ç“¶é¢ˆã€‚
2. æå‡ºS-MADRLæ¡†æ¶ï¼Œåˆ©ç”¨è™šæ‹Ÿä¿¡æ¯ç´ æ¨¡æ‹Ÿå±€éƒ¨å’Œç¤¾äº¤äº’åŠ¨ï¼Œå®ç°å»ä¸­å¿ƒåŒ–æ¶Œç°ååŒï¼Œæ— éœ€æ˜¾å¼é€šä¿¡ã€‚
3. é€šè¿‡è¯¾ç¨‹å­¦ä¹ å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œå®éªŒè¯æ˜è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆåè°ƒå¤šè¾¾8ä¸ªæ™ºèƒ½ä½“ï¼Œå‡å°‘æ‹¥å µã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨è§£å†³ç‹­çª„å’Œå—é™ç¯å¢ƒä¸­å¤šæœºå™¨äººååŒçš„æŒ‘æˆ˜ï¼Œæ‹¥å µå’Œå¹²æ‰°é€šå¸¸ä¼šé˜»ç¢é›†ä½“ä»»åŠ¡çš„æ‰§è¡Œã€‚å—åˆ°æ˜†è™«ç¾¤ä½“é€šè¿‡ä¿¡æ¯ç´ ï¼ˆä¿®æ”¹å’Œè§£é‡Šç¯å¢ƒç—•è¿¹ï¼‰å®ç°é²æ£’ååŒçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºä¿¡æ¯ç´ çš„å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆS-MADRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è™šæ‹Ÿä¿¡æ¯ç´ æ¥å»ºæ¨¡å±€éƒ¨å’Œç¤¾äº¤äº’åŠ¨ï¼Œä»è€Œå®ç°æ— éœ€æ˜¾å¼é€šä¿¡çš„å»ä¸­å¿ƒåŒ–æ¶Œç°ååŒã€‚ä¸ºäº†å…‹æœç°æœ‰ç®—æ³•ï¼ˆå¦‚MADQNã€MADDPGå’ŒMAPPOï¼‰çš„æ”¶æ•›æ€§å’Œå¯æ‰©å±•æ€§é™åˆ¶ï¼Œæˆ‘ä»¬åˆ©ç”¨è¯¾ç¨‹å­¦ä¹ å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºé€æ¸å˜éš¾çš„å­é—®é¢˜ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†æœ€å¤šå…«ä¸ªæ™ºèƒ½ä½“çš„æœ€æœ‰æ•ˆååŒï¼Œå…¶ä¸­æœºå™¨äººè‡ªç»„ç»‡æˆä¸å¯¹ç§°çš„å·¥ä½œè´Ÿè½½åˆ†å¸ƒï¼Œä»è€Œå‡å°‘æ‹¥å µå¹¶è°ƒèŠ‚ç¾¤ä½“æ€§èƒ½ã€‚è¿™ç§ç±»ä¼¼äºè‡ªç„¶ç•Œè§‚å¯Ÿåˆ°çš„ç­–ç•¥çš„æ¶Œç°è¡Œä¸ºï¼Œå±•ç¤ºäº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºåœ¨å…·æœ‰é€šä¿¡çº¦æŸçš„æ‹¥æŒ¤ç¯å¢ƒä¸­è¿›è¡Œå»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“ååŒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“åœ¨ç‹­çª„æ‹¥æŒ¤ç¯å¢ƒä¸­ååŒä»»åŠ¡çš„éš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚MADQNã€MADDPGå’ŒMAPPOç­‰ï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡æ™ºèƒ½ä½“æˆ–å¤æ‚ä»»åŠ¡æ—¶ï¼Œé¢ä¸´æ”¶æ•›é€Ÿåº¦æ…¢ã€æ‰©å±•æ€§å·®ç­‰é—®é¢˜ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆçš„å»ä¸­å¿ƒåŒ–ååŒã€‚å°¤å…¶æ˜¯åœ¨é€šä¿¡å—é™çš„ç¯å¢ƒä¸­ï¼Œæ™ºèƒ½ä½“ä¹‹é—´æ— æ³•ç›´æ¥é€šä¿¡ï¼ŒååŒå˜å¾—æ›´åŠ å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´æ˜†è™«ç¾¤ä½“é€šè¿‡ä¿¡æ¯ç´ è¿›è¡ŒååŒçš„è¡Œä¸ºæ¨¡å¼ã€‚æ™ºèƒ½ä½“é€šè¿‡åœ¨ç¯å¢ƒä¸­ç•™ä¸‹â€œè™šæ‹Ÿä¿¡æ¯ç´ â€ï¼Œå…¶ä»–æ™ºèƒ½ä½“å¯ä»¥æ„ŸçŸ¥å¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯ç´ æ¥è°ƒæ•´è‡ªå·±çš„è¡Œä¸ºï¼Œä»è€Œå®ç°é—´æ¥çš„é€šä¿¡å’ŒååŒã€‚è¿™ç§åŸºäºç¯å¢ƒçš„ååŒæ–¹å¼ï¼Œæ— éœ€æ˜¾å¼é€šä¿¡ï¼Œæ›´é€‚åˆé€šä¿¡å—é™çš„åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šS-MADRLæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è™šæ‹Ÿä¿¡æ¯ç´ æ¨¡å—ï¼šè´Ÿè´£ç”Ÿæˆå’Œç»´æŠ¤è™šæ‹Ÿä¿¡æ¯ç´ ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“å¯ä»¥æ ¹æ®è‡ªèº«çŠ¶æ€å’Œç¯å¢ƒä¿¡æ¯é‡Šæ”¾ä¿¡æ¯ç´ ã€‚2) æ„ŸçŸ¥æ¨¡å—ï¼šæ™ºèƒ½ä½“é€šè¿‡æ„ŸçŸ¥å‘¨å›´ç¯å¢ƒå’Œä¿¡æ¯ç´ æµ“åº¦ï¼Œè·å–å±€éƒ¨ä¿¡æ¯ã€‚3) å†³ç­–æ¨¡å—ï¼šåŸºäºæ„ŸçŸ¥åˆ°çš„ä¿¡æ¯ï¼Œåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚MADDPGçš„å˜ä½“ï¼‰åšå‡ºè¡ŒåŠ¨å†³ç­–ã€‚4) è¯¾ç¨‹å­¦ä¹ æ¨¡å—ï¼šå°†å¤æ‚çš„ååŒä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—éš¾åº¦é€’å¢çš„å­ä»»åŠ¡ï¼Œé€æ­¥è®­ç»ƒæ™ºèƒ½ä½“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†ä¿¡æ¯ç´ æœºåˆ¶å¼•å…¥åˆ°å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæå‡ºS-MADRLæ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„MADRLç®—æ³•ç›¸æ¯”ï¼ŒS-MADRLæ— éœ€æ˜¾å¼é€šä¿¡ï¼Œé€šè¿‡è™šæ‹Ÿä¿¡æ¯ç´ å®ç°æ™ºèƒ½ä½“ä¹‹é—´çš„é—´æ¥ååŒï¼Œæ›´å…·é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œç»“åˆè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†å¤æ‚ä»»åŠ¡çš„è®­ç»ƒéš¾é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šè™šæ‹Ÿä¿¡æ¯ç´ çš„è¡°å‡ç‡å’Œæ‰©æ•£èŒƒå›´æ˜¯å…³é”®å‚æ•°ï¼Œå½±å“æ™ºèƒ½ä½“ä¹‹é—´çš„ä¿¡æ¯ä¼ é€’æ•ˆç‡ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘ä¸ªä½“å¥–åŠ±å’Œé›†ä½“å¥–åŠ±ï¼Œé¼“åŠ±æ™ºèƒ½ä½“åœ¨å®Œæˆè‡ªèº«ä»»åŠ¡çš„åŒæ—¶ï¼Œä¿ƒè¿›æ•´ä½“ååŒã€‚ç½‘ç»œç»“æ„å¯ä»¥é‡‡ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æˆ–Transformerç­‰ï¼Œä»¥å¤„ç†æ—¶åºä¿¡æ¯å’Œå»ºæ¨¡æ™ºèƒ½ä½“ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è¯¾ç¨‹å­¦ä¹ çš„éš¾åº¦é€’å¢ç­–ç•¥éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒS-MADRLæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåè°ƒå¤šè¾¾8ä¸ªæ™ºèƒ½ä½“ï¼Œåœ¨æ‹¥æŒ¤ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„ååŒä»»åŠ¡ã€‚ä¸åŸºçº¿ç®—æ³•ï¼ˆå¦‚MADDPGï¼‰ç›¸æ¯”ï¼ŒS-MADRLèƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ‹¥å µï¼Œæé«˜ä»»åŠ¡å®Œæˆæ•ˆç‡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ç‰¹å®šä»¿çœŸç¯å¢ƒä¸­ï¼ŒS-MADRLæ¡†æ¶çš„ä»»åŠ¡å®Œæˆæ—¶é—´æ¯”MADDPGç¼©çŸ­äº†çº¦20%ï¼Œæ‹¥å µç‡é™ä½äº†çº¦15%ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤ŸåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹å¹¶æé«˜æœ€ç»ˆæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä»“åº“æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶è½¦è¾†ã€æ— äººæœºé›†ç¾¤ç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨æ‹¥æŒ¤ã€é€šä¿¡å—é™çš„ç¯å¢ƒä¸­ï¼Œä¾‹å¦‚ï¼šæ™ºèƒ½ä»“å‚¨ç³»ç»Ÿä¸­ï¼Œå¤šä¸ªæœºå™¨äººååŒæ¬è¿è´§ç‰©ï¼›è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨åŸå¸‚é“è·¯ä¸­ååŒè¡Œé©¶ï¼›æ— äººæœºé›†ç¾¤åœ¨ç¾åŒºè¿›è¡Œæœç´¢æ•‘æ´ç­‰ã€‚è¯¥ç ”ç©¶ä¸ºè§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„å¤šæ™ºèƒ½ä½“ååŒé—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We address the challenge of coordinating multiple robots in narrow and confined environments, where congestion and interference often hinder collective task performance. Drawing inspiration from insect colonies, which achieve robust coordination through stigmergy -- modifying and interpreting environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement Learning (S-MADRL) framework that leverages virtual pheromones to model local and social interactions, enabling decentralized emergent coordination without explicit communication. To overcome the convergence and scalability limitations of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum learning, which decomposes complex tasks into progressively harder sub-problems. Simulation results show that our framework achieves the most effective coordination of up to eight agents, where robots self-organize into asymmetric workload distributions that reduce congestion and modulate group performance. This emergent behavior, analogous to strategies observed in nature, demonstrates a scalable solution for decentralized multi-agent coordination in crowded environments with communication constraints.

