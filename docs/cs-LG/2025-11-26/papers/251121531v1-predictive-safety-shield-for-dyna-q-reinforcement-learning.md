---
layout: default
title: Predictive Safety Shield for Dyna-Q Reinforcement Learning
---

# Predictive Safety Shield for Dyna-Q Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.21531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.21531v1</a>
  <a href="https://arxiv.org/pdf/2511.21531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.21531v1" onclick="toggleFavorite(this, '2511.21531v1', 'Predictive Safety Shield for Dyna-Q Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jin Pin, Krasowski Hanna, Vanneaux Elena

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-11-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé¢„æµ‹çš„å®‰å…¨ç›¾ï¼Œæå‡Dyna-Qå¼ºåŒ–å­¦ä¹ åœ¨ç¦»æ•£ç©ºé—´çš„å®‰å…¨æ€§å’Œæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å®‰å…¨ç›¾` `æ¨¡å‹é¢„æµ‹` `Dyna-Q` `å®‰å…¨æ€§` `ç¦»æ•£ç©ºé—´` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å®‰å…¨ç›¾æ–¹æ³•ä¾èµ–éšæœºæŠ½æ ·æˆ–å›ºå®šæ§åˆ¶å™¨ï¼Œå¿½ç•¥äº†å®‰å…¨åŠ¨ä½œå¯¹æœªæ¥æ€§èƒ½çš„å½±å“ï¼Œé™åˆ¶äº†å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨ã€‚
2. æå‡ºä¸€ç§é¢„æµ‹å®‰å…¨ç›¾ï¼Œé€šè¿‡ç¯å¢ƒæ¨¡å‹çš„å®‰å…¨æ¨¡æ‹Ÿè¿›è¡Œå®‰å…¨é¢„æµ‹ï¼Œå¹¶å±€éƒ¨æ›´æ–°Qå‡½æ•°ï¼Œä»è€Œä¼˜åŒ–å®‰å…¨åŠ¨ä½œçš„é€‰æ‹©ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç½‘æ ¼ä¸–ç•Œä¸­å³ä½¿ä½¿ç”¨çŸ­é¢„æµ‹èŒƒå›´ä¹Ÿèƒ½æ‰¾åˆ°æœ€ä¼˜è·¯å¾„ï¼Œä¸”å¯¹æ¨¡æ‹Ÿä¸ç°å®çš„åˆ†å¸ƒåç§»å…·æœ‰é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›å®‰å…¨ä¿éšœæ˜¯å®ç°å…¶åœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­åº”ç”¨çš„å…³é”®æŒ‘æˆ˜ã€‚å®‰å…¨ç›¾æ‰©å±•äº†æ ‡å‡†å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°äº†ç¡¬æ€§å®‰å…¨ä¿è¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å®‰å…¨ç›¾é€šå¸¸ä½¿ç”¨å®‰å…¨åŠ¨ä½œçš„éšæœºæŠ½æ ·æˆ–å›ºå®šçš„å›é€€æ§åˆ¶å™¨ï¼Œå› æ­¤å¿½ç•¥äº†ä¸åŒå®‰å…¨åŠ¨ä½œå¯¹æœªæ¥æ€§èƒ½çš„å½±å“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç¦»æ•£ç©ºé—´ä¸­åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„é¢„æµ‹å®‰å…¨ç›¾ã€‚æˆ‘ä»¬çš„å®‰å…¨ç›¾åŸºäºå®‰å…¨é¢„æµ‹å±€éƒ¨æ›´æ–°Qå‡½æ•°ï¼Œè¿™äº›é¢„æµ‹æºäºç¯å¢ƒæ¨¡å‹çš„å®‰å…¨æ¨¡æ‹Ÿã€‚è¿™ç§å±è”½æ–¹æ³•åœ¨ä¿æŒç¡¬æ€§å®‰å…¨ä¿è¯çš„åŒæ—¶æé«˜äº†æ€§èƒ½ã€‚åœ¨ç½‘æ ¼ä¸–ç•Œç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯çŸ­çš„é¢„æµ‹èŒƒå›´ä¹Ÿè¶³ä»¥è¯†åˆ«æœ€ä½³è·¯å¾„ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹åˆ†å¸ƒåç§»ï¼ˆä¾‹å¦‚ï¼Œæ¨¡æ‹Ÿå’Œç°å®ä¹‹é—´ï¼‰å…·æœ‰é²æ£’æ€§ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¼ºåŒ–å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´å®‰å…¨é—®é¢˜ï¼Œéœ€è¦ä¿è¯æ™ºèƒ½ä½“çš„è¡Œä¸ºå§‹ç»ˆå¤„äºå®‰å…¨çŠ¶æ€ã€‚ç°æœ‰çš„å®‰å…¨ç›¾æ–¹æ³•ï¼Œå¦‚éšæœºé‡‡æ ·å®‰å…¨åŠ¨ä½œæˆ–ä½¿ç”¨å›ºå®šå›é€€ç­–ç•¥ï¼Œè™½ç„¶èƒ½ä¿è¯å®‰å…¨æ€§ï¼Œä½†å¾€å¾€ä¼šç‰ºç‰²æ€§èƒ½ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰å……åˆ†è€ƒè™‘ä¸åŒå®‰å…¨åŠ¨ä½œå¯¹æœªæ¥å›æŠ¥çš„å½±å“ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿è¯å®‰å…¨æ€§çš„å‰æä¸‹ï¼Œæå‡å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¯å¢ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œä»è€Œé€‰æ‹©æ›´ä¼˜çš„å®‰å…¨åŠ¨ä½œã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å¯¹ç¯å¢ƒæ¨¡å‹è¿›è¡Œå®‰å…¨æ¨¡æ‹Ÿï¼Œé¢„æµ‹ä¸åŒå®‰å…¨åŠ¨ä½œçš„æœªæ¥çŠ¶æ€å’Œå›æŠ¥ï¼Œå¹¶åŸºäºè¿™äº›é¢„æµ‹æ¥å±€éƒ¨æ›´æ–°Qå‡½æ•°ã€‚è¿™æ ·ï¼Œå®‰å…¨ç›¾ä¸ä»…èƒ½ä¿è¯å®‰å…¨æ€§ï¼Œè¿˜èƒ½é€‰æ‹©æ›´æœ‰åˆ©äºé•¿æœŸå›æŠ¥çš„åŠ¨ä½œï¼Œä»è€Œæå‡æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œä½¿ç”¨Dyna-Qç®—æ³•è¿›è¡Œå­¦ä¹ ï¼›2) ç¯å¢ƒæ¨¡å‹ï¼Œç”¨äºæ¨¡æ‹Ÿç¯å¢ƒçš„åŠ¨æ€å˜åŒ–ï¼›3) å®‰å…¨ç›¾ï¼Œè´Ÿè´£åˆ¤æ–­å½“å‰åŠ¨ä½œæ˜¯å¦å®‰å…¨ï¼Œå¹¶é€‰æ‹©å®‰å…¨åŠ¨ä½œï¼›4) é¢„æµ‹æ¨¡å—ï¼ŒåŸºäºç¯å¢ƒæ¨¡å‹è¿›è¡Œå®‰å…¨é¢„æµ‹ï¼Œè¯„ä¼°ä¸åŒå®‰å…¨åŠ¨ä½œçš„æœªæ¥å›æŠ¥ï¼›5) Qå‡½æ•°æ›´æ–°æ¨¡å—ï¼Œæ ¹æ®é¢„æµ‹ç»“æœå±€éƒ¨æ›´æ–°Qå‡½æ•°ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šæ™ºèƒ½ä½“æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œï¼Œå®‰å…¨ç›¾åˆ¤æ–­åŠ¨ä½œæ˜¯å¦å®‰å…¨ï¼Œå¦‚æœå®‰å…¨åˆ™æ‰§è¡Œï¼Œå¦åˆ™ä½¿ç”¨é¢„æµ‹æ¨¡å—é€‰æ‹©æ›´ä¼˜çš„å®‰å…¨åŠ¨ä½œï¼Œå¹¶æ›´æ–°Qå‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†é¢„æµ‹å¼•å…¥å®‰å…¨ç›¾ä¸­ã€‚ä¼ ç»Ÿçš„å®‰å…¨ç›¾åªå…³æ³¨å½“å‰åŠ¨ä½œçš„å®‰å…¨æ€§ï¼Œè€Œå¿½ç•¥äº†æœªæ¥å›æŠ¥ã€‚é€šè¿‡ä½¿ç”¨ç¯å¢ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯„ä¼°ä¸åŒå®‰å…¨åŠ¨ä½œçš„é•¿æœŸå½±å“ï¼Œä»è€Œé€‰æ‹©æ›´æœ‰åˆ©äºé•¿æœŸå›æŠ¥çš„åŠ¨ä½œã€‚è¿™ç§é¢„æµ‹èƒ½åŠ›ä½¿å¾—å®‰å…¨ç›¾ä¸ä»…èƒ½ä¿è¯å®‰å…¨æ€§ï¼Œè¿˜èƒ½æå‡æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Dyna-Qç®—æ³•è¿›è¡Œå­¦ä¹ ï¼ŒDyna-Qç®—æ³•æ˜¯ä¸€ç§åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€‚åˆäºç¦»æ•£ç©ºé—´ï¼›2) ä½¿ç”¨ç¯å¢ƒæ¨¡å‹è¿›è¡Œå®‰å…¨é¢„æµ‹ï¼Œç¯å¢ƒæ¨¡å‹å¯ä»¥æ˜¯å­¦ä¹ å¾—åˆ°çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯é¢„å…ˆå®šä¹‰çš„ï¼›3) å±€éƒ¨æ›´æ–°Qå‡½æ•°ï¼Œåªæ›´æ–°ä¸å®‰å…¨é¢„æµ‹ç›¸å…³çš„Qå€¼ï¼Œé¿å…å½±å“å…¶ä»–Qå€¼çš„å‡†ç¡®æ€§ï¼›4) é¢„æµ‹èŒƒå›´çš„é€‰æ‹©ï¼Œé¢„æµ‹èŒƒå›´è¶Šé•¿ï¼Œé¢„æµ‹è¶Šå‡†ç¡®ï¼Œä½†è®¡ç®—æˆæœ¬ä¹Ÿè¶Šé«˜ï¼Œéœ€è¦æ ¹æ®å…·ä½“é—®é¢˜è¿›è¡Œæƒè¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç½‘æ ¼ä¸–ç•Œç¯å¢ƒä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿è¯å®‰å…¨æ€§ã€‚å³ä½¿ä½¿ç”¨è¾ƒçŸ­çš„é¢„æµ‹èŒƒå›´ï¼Œè¯¥æ–¹æ³•ä¹Ÿèƒ½æ‰¾åˆ°æœ€ä¼˜è·¯å¾„ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯¹æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„åˆ†å¸ƒåç§»å…·æœ‰é²æ£’æ€§ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æŸäº›å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¾¾åˆ°ä¸æ— å®‰å…¨ç›¾çš„Dyna-Qç®—æ³•ç›¸è¿‘çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿è¯äº†100%çš„å®‰å…¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸï¼Œå°¤å…¶é€‚ç”¨äºå¯¹å®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•ä¿è¯æœºå™¨äººåœ¨é¿å¼€éšœç¢ç‰©çš„åŒæ—¶ï¼Œå°½å¯èƒ½å¿«åœ°åˆ°è¾¾ç›®æ ‡ä½ç½®ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥ä¿è¯è½¦è¾†åœ¨è¡Œé©¶è¿‡ç¨‹ä¸­å§‹ç»ˆå¤„äºå®‰å…¨çŠ¶æ€ï¼Œé¿å…å‘ç”Ÿäº¤é€šäº‹æ•…ã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæ¸¸æˆAIä¸­ï¼Œä½¿AIåœ¨ä¿è¯æ¸¸æˆè§„åˆ™çš„å‰æä¸‹ï¼Œåšå‡ºæ›´æ™ºèƒ½çš„å†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Obtaining safety guarantees for reinforcement learning is a major challenge to achieve applicability for real-world tasks. Safety shields extend standard reinforcement learning and achieve hard safety guarantees. However, existing safety shields commonly use random sampling of safe actions or a fixed fallback controller, therefore disregarding future performance implications of different safe actions. In this work, we propose a predictive safety shield for model-based reinforcement learning agents in discrete space. Our safety shield updates the Q-function locally based on safe predictions, which originate from a safe simulation of the environment model. This shielding approach improves performance while maintaining hard safety guarantees. Our experiments on gridworld environments demonstrate that even short prediction horizons can be sufficient to identify the optimal path. We observe that our approach is robust to distribution shifts, e.g., between simulation and reality, without requiring additional training.

