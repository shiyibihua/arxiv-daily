---
layout: default
title: BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute
---

# BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22716" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22716v1</a>
  <a href="https://arxiv.org/pdf/2506.22716.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22716v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22716v1', 'BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dujian Ding, Ankur Mallick, Shaokun Zhang, Chi Wang, Daniel Madrigal, Mirian Del Carmen Hipolito Garcia, Menglin Xia, Laks V. S. Lakshmanan, Qingyun Wu, Victor RÃ¼hle

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.DB

**å‘å¸ƒæ—¥æœŸ**: 2025-06-28

**å¤‡æ³¨**: Accepted to ICML 2025 (main conference)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBEST-Routeä»¥ä¼˜åŒ–LLMæŸ¥è¯¢è·¯ç”±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æŸ¥è¯¢è·¯ç”±` `æˆæœ¬ä¼˜åŒ–` `å“åº”ç”Ÿæˆ` `åŠ¨æ€é€‰æ‹©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMæŸ¥è¯¢è·¯ç”±æ–¹æ³•ä»…ç”Ÿæˆå•ä¸€å“åº”ï¼Œå¯¼è‡´å¤§æ¨¡å‹çš„è¿‡åº¦ä½¿ç”¨ï¼Œæœªèƒ½å®ç°æˆæœ¬èŠ‚çº¦ã€‚
2. BEST-Routeé€šè¿‡ä»å°æ¨¡å‹ç”Ÿæˆå¤šä¸ªå“åº”å¹¶é€‰æ‹©æœ€ä½³å“åº”ï¼Œä¼˜åŒ–äº†æŸ¥è¯¢è·¯ç”±çš„æ•ˆç‡å’Œè´¨é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBEST-Routeåœ¨é™ä½æˆæœ¬çš„åŒæ—¶ï¼Œæ€§èƒ½å‡ ä¹ä¸å—å½±å“ï¼Œå±•ç¤ºäº†å…¶å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¼ºå¤§çš„å·¥å…·ï¼Œä½†åœ¨å¤§è§„æ¨¡éƒ¨ç½²æ—¶æˆæœ¬é«˜æ˜‚ã€‚LLMæŸ¥è¯¢è·¯ç”±é€šè¿‡åŠ¨æ€åˆ†é…æŸ¥è¯¢åˆ°ä¸åŒæˆæœ¬å’Œè´¨é‡çš„æ¨¡å‹æ¥å®ç°æ‰€éœ€çš„æƒè¡¡ã€‚ä»¥å¾€çš„æŸ¥è¯¢è·¯ç”±æ–¹æ³•ä»…ä»é€‰å®šæ¨¡å‹ç”Ÿæˆä¸€ä¸ªå“åº”ï¼Œè€Œå°æ¨¡å‹çš„å•ä¸€å“åº”å¾€å¾€æ— æ³•è¶…è¶Šå¤§æ¨¡å‹çš„å“åº”ï¼Œå¯¼è‡´å¤§æ¨¡å‹çš„è¿‡åº¦ä½¿ç”¨ã€‚æœ¬æ–‡æå‡ºBEST-Routeï¼Œä¸€ä¸ªæ–°é¢–çš„è·¯ç”±æ¡†æ¶ï¼Œæ ¹æ®æŸ¥è¯¢éš¾åº¦å’Œè´¨é‡é˜ˆå€¼é€‰æ‹©æ¨¡å‹åŠå…¶å“åº”æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸‹é™ä¸åˆ°1%çš„æƒ…å†µä¸‹ï¼Œæˆæœ¬é™ä½äº†å¤šè¾¾60%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŸ¥è¯¢è·¯ç”±ä¸­çš„é«˜æˆæœ¬é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä»…ç”Ÿæˆå•ä¸€å“åº”ï¼Œå¯¼è‡´å¯¹å¤§æ¨¡å‹çš„è¿‡åº¦ä¾èµ–ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å°æ¨¡å‹çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBEST-Routeçš„æ ¸å¿ƒæ€æƒ³æ˜¯æ ¹æ®æŸ¥è¯¢çš„éš¾åº¦å’Œé¢„è®¾çš„è´¨é‡é˜ˆå€¼ï¼ŒåŠ¨æ€é€‰æ‹©æ¨¡å‹åŠå…¶å“åº”æ•°é‡ã€‚é€šè¿‡ä»å°æ¨¡å‹ç”Ÿæˆå¤šä¸ªå“åº”å¹¶é€‰æ‹©æœ€ä½³å“åº”ï¼Œæå‡äº†å“åº”è´¨é‡ï¼ŒåŒæ—¶é™ä½äº†æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æŸ¥è¯¢åˆ†ææ¨¡å—ã€æ¨¡å‹é€‰æ‹©æ¨¡å—å’Œå“åº”ç”Ÿæˆæ¨¡å—ã€‚é¦–å…ˆåˆ†ææŸ¥è¯¢çš„éš¾åº¦ï¼Œç„¶åé€‰æ‹©åˆé€‚çš„æ¨¡å‹åŠå…¶å“åº”æ•°é‡ï¼Œæœ€åç”Ÿæˆå¹¶è¯„ä¼°å“åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åŠ¨æ€å“åº”ç”Ÿæˆæœºåˆ¶ï¼Œä½¿å¾—å°æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¤šæ¬¡ç”Ÿæˆå“åº”æ¥æå‡è´¨é‡ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘å¯¹å¤§æ¨¡å‹çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œè®ºæ–‡å®šä¹‰äº†æŸ¥è¯¢éš¾åº¦å’Œè´¨é‡é˜ˆå€¼çš„è®¡ç®—æ–¹å¼ï¼Œå¹¶è®¾è®¡äº†æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å“åº”é€‰æ‹©è¿‡ç¨‹ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œé‡‡ç”¨äº†è½»é‡çº§æ¨¡å‹ä»¥ä¿è¯åœ¨æˆæœ¬å’Œæ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBEST-Routeåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„åº”ç”¨èƒ½å¤Ÿå°†æˆæœ¬é™ä½å¤šè¾¾60%ï¼Œè€Œæ€§èƒ½ä»…ä¸‹é™ä¸åˆ°1%ã€‚è¿™ä¸€æ˜¾è‘—çš„æˆæœ¬æ•ˆç›Šæ¯”ä¸ºLLMçš„å®é™…éƒ¨ç½²æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BEST-Routeå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆå¤„ç†å¤§é‡æŸ¥è¯¢çš„åœºæ™¯ï¼Œå¦‚åœ¨çº¿å®¢æœã€æ™ºèƒ½åŠ©æ‰‹å’Œå†…å®¹ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–æŸ¥è¯¢è·¯ç”±ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½è¿è¥æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„å“åº”ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨LLMåœ¨æ›´å¤šå•†ä¸šåº”ç”¨ä¸­çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are powerful tools but are often expensive to deploy at scale. LLM query routing mitigates this by dynamically assigning queries to models of varying cost and quality to obtain a desired trade-off. Prior query routing approaches generate only one response from the selected model and a single response from a small (inexpensive) model was often not good enough to beat a response from a large (expensive) model due to which they end up overusing the large model and missing out on potential cost savings. However, it is well known that for small models, generating multiple responses and selecting the best can enhance quality while remaining cheaper than a single large-model response. We leverage this idea to propose BEST-Route, a novel routing framework that chooses a model and the number of responses to sample from it based on query difficulty and the quality thresholds. Experiments on real-world datasets demonstrate that our method reduces costs by up to 60% with less than 1% performance drop.

