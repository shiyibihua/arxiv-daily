---
layout: default
title: Attention Layers Add Into Low-Dimensional Residual Subspaces
---

# Attention Layers Add Into Low-Dimensional Residual Subspaces

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16929" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.16929v2</a>
  <a href="https://arxiv.org/pdf/2508.16929.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16929v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16929v2', 'Attention Layers Add Into Low-Dimensional Residual Subspaces')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junxuan Wang, Xuyang Ge, Wentao Shu, Zhengfu He, Xipeng Qiu

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-23 (æ›´æ–°: 2025-09-28)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä½ç»´æ®‹å·®å­ç©ºé—´çº¦æŸè®­ç»ƒä»¥è§£å†³ç¨€ç–å­—å…¸å­¦ä¹ ä¸­çš„æ­»ç‰¹å¾é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `æ³¨æ„åŠ›æœºåˆ¶` `ä½ç»´å­ç©ºé—´` `å­—å…¸å­¦ä¹ ` `ç‰¹å¾åˆå§‹åŒ–` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Transformeræ¨¡å‹åœ¨é«˜ç»´ç©ºé—´ä¸­è¿è¡Œï¼Œä½†æ³¨æ„åŠ›è¾“å‡ºå´è¢«é™åˆ¶åœ¨ä½ç»´å­ç©ºé—´ï¼Œå¯¼è‡´æ­»ç‰¹å¾é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§å­ç©ºé—´çº¦æŸè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡åˆå§‹åŒ–ç‰¹å¾æ–¹å‘åˆ°æ¿€æ´»çš„æœ‰æ•ˆå­ç©ºé—´æ¥è§£å†³æ­»ç‰¹å¾é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å…·æœ‰100ä¸‡ç‰¹å¾çš„æ³¨æ„åŠ›è¾“å‡ºç¨€ç–è‡ªç¼–ç å™¨ä¸­ï¼Œæ­»ç‰¹å¾æ¯”ä¾‹ä»87%é™è‡³1%ä»¥ä¸‹ï¼Œæ•ˆæœæ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Transformeræ¶æ„åŠå…¶æ³¨æ„åŠ›æœºåˆ¶çš„ä½ç»´ç‰¹æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ³¨æ„åŠ›è¾“å‡ºè¢«é™åˆ¶åœ¨ä¸€ä¸ªä½ç»´å­ç©ºé—´ä¸­ï¼Œçº¦60%çš„æ–¹å‘å æ®99%çš„æ–¹å·®ï¼Œè¿™ä¸€ç°è±¡åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šæ™®éå­˜åœ¨ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹ç¨€ç–è‡ªç¼–ç å™¨çš„å­ç©ºé—´çº¦æŸè®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†æ­»ç‰¹å¾çš„æ¯”ä¾‹ï¼Œä»87%é™è‡³1%ä»¥ä¸‹ï¼Œå¹¶å¯æ‰©å±•è‡³å…¶ä»–ç¨€ç–å­—å…¸å­¦ä¹ æ–¹æ³•ã€‚ç ”ç©¶ä¸ºæ³¨æ„åŠ›å‡ ä½•æä¾›äº†æ–°è§è§£ï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨€ç–å­—å…¸å­¦ä¹ æä¾›äº†å®ç”¨å·¥å…·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¨€ç–å­—å…¸å­¦ä¹ ä¸­çš„æ­»ç‰¹å¾é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åˆå§‹åŒ–ç‰¹å¾æ—¶ä¸æ¿€æ´»ç©ºé—´çš„å†…åœ¨å‡ ä½•ä¸åŒ¹é…ï¼Œå¯¼è‡´å¤§é‡æ— æ•ˆç‰¹å¾çš„äº§ç”Ÿã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è¯†åˆ«æ³¨æ„åŠ›è¾“å‡ºçš„ä½ç»´ç»“æ„ï¼Œæå‡ºäº†ä¸€ç§å­ç©ºé—´çº¦æŸè®­ç»ƒæ–¹æ³•ï¼Œåˆå§‹åŒ–ç‰¹å¾æ–¹å‘åˆ°æœ‰æ•ˆçš„æ¿€æ´»å­ç©ºé—´ï¼Œä»è€Œå‡å°‘æ­»ç‰¹å¾çš„äº§ç”Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾åˆå§‹åŒ–ã€å­ç©ºé—´çº¦æŸè®­ç»ƒå’Œç¨€ç–è‡ªç¼–ç å™¨çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç‰¹å¾æ–¹å‘çš„åˆå§‹åŒ–å’ŒåŸºäºä½ç»´å­ç©ºé—´çš„è®­ç»ƒç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè¯†åˆ«å¹¶åˆ©ç”¨æ³¨æ„åŠ›è¾“å‡ºçš„ä½ç»´ç»“æ„ï¼Œæ˜¾è‘—æ”¹å–„äº†ç¨€ç–è‡ªç¼–ç å™¨çš„ç‰¹å¾å­¦ä¹ æ•ˆæœï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå‡å°‘äº†æ­»ç‰¹å¾çš„æ¯”ä¾‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œç‰¹å¾æ–¹å‘åˆå§‹åŒ–ä¸ºæ³¨æ„åŠ›è¾“å‡ºçš„æœ‰æ•ˆå­ç©ºé—´ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆé‡æ„è¯¯å·®å’Œç¨€ç–æ€§çº¦æŸï¼Œç½‘ç»œç»“æ„é‡‡ç”¨æ ‡å‡†çš„ç¨€ç–è‡ªç¼–ç å™¨æ¶æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨å­ç©ºé—´çº¦æŸè®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨åœ¨å¤„ç†å…·æœ‰100ä¸‡ç‰¹å¾çš„æ³¨æ„åŠ›è¾“å‡ºæ—¶ï¼Œæ­»ç‰¹å¾æ¯”ä¾‹ä»87%æ˜¾è‘—é™ä½è‡³1%ä»¥ä¸‹ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ç¨€ç–å­—å…¸å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ä¸ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå…¶ä»–éœ€è¦ç¨€ç–è¡¨ç¤ºçš„ä»»åŠ¡ã€‚é€šè¿‡æ”¹å–„ç¨€ç–å­—å…¸å­¦ä¹ çš„æ•ˆç‡å’Œæ•ˆæœï¼Œèƒ½å¤Ÿæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¤šé¢†åŸŸä¸­å¾—åˆ°æ¨å¹¿ï¼Œä¿ƒè¿›æ™ºèƒ½ç³»ç»Ÿçš„ä¼˜åŒ–ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformer architectures, and their attention mechanisms in particular, form the foundation of modern large language models. While transformer models are widely believed to operate in high-dimensional hidden spaces, we show that attention outputs are confined to a surprisingly low-dimensional subspace, where about 60\% of the directions account for 99\% of the variance--a phenomenon that is consistently observed across diverse model families and datasets, and is induced by the attention output projection matrix. Critically, we find this low-rank structure as a key factor of the prevalent dead feature problem in sparse dictionary learning, where it creates a mismatch between randomly initialized features and the intrinsic geometry of the activation space. Building on this insight, we propose a subspace-constrained training method for sparse autoencoders (SAEs), initializing feature directions into the active subspace of activations. Our approach reduces dead features from 87\% to below 1\% in Attention Output SAEs with 1M features, and can further extend to other sparse dictionary learning methods. Our findings provide both new insights into the geometry of attention and practical tools for improving sparse dictionary learning in large language models.

