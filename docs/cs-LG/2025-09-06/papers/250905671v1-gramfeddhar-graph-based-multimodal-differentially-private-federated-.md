---
layout: default
title: GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR
---

# GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05671" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05671v1</a>
  <a href="https://arxiv.org/pdf/2509.05671.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05671v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05671v1', 'GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Labani Halder, Tanmay Sen, Sarbani Palit

**åˆ†ç±»**: cs.LG, cs.AI, cs.CR, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GraMFedDHARï¼šå›¾ç¥ç»ç½‘ç»œä¸å·®åˆ†éšç§è”é‚¦å­¦ä¹ ç”¨äºå¤šæ¨¡æ€äººä½“æ´»åŠ¨è¯†åˆ«**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººä½“æ´»åŠ¨è¯†åˆ«` `å¤šæ¨¡æ€å­¦ä¹ ` `å›¾ç¥ç»ç½‘ç»œ` `è”é‚¦å­¦ä¹ ` `å·®åˆ†éšç§` `ä¼ æ„Ÿå™¨æ•°æ®èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿé›†ä¸­å¼æ·±åº¦å­¦ä¹ åœ¨å¤šæ¨¡æ€äººä½“æ´»åŠ¨è¯†åˆ«ä¸­é¢ä¸´æ•°æ®å…±äº«é™åˆ¶å’Œéšç§é—®é¢˜ï¼Œè”é‚¦å­¦ä¹ è™½èƒ½ä¿æŠ¤éšç§ï¼Œä½†å¼‚æ„å¤šæ¨¡æ€æ•°æ®å’Œå·®åˆ†éšç§éœ€æ±‚å¸¦æ¥æ–°çš„æŒ‘æˆ˜ã€‚
2. GraMFedDHARæ¡†æ¶å°†å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®å»ºæ¨¡ä¸ºå›¾ï¼Œåˆ©ç”¨å›¾å·ç§¯ç¥ç»ç½‘ç»œæå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶èåˆï¼Œæ—¨åœ¨æå‡æ´»åŠ¨è¯†åˆ«çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨éå·®åˆ†éšç§å’Œå·®åˆ†éšç§è®¾ç½®ä¸‹å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨å·®åˆ†éšç§çº¦æŸä¸‹ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼ŒéªŒè¯äº†å›¾ç¥ç»ç½‘ç»œçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„å¤šæ¨¡æ€è”é‚¦å­¦ä¹ æ¡†æ¶GraMFedDHARï¼Œç”¨äºäººä½“æ´»åŠ¨è¯†åˆ«(HAR)ä»»åŠ¡ã€‚è¯¥æ¡†æ¶å°†å‹åŠ›å«ã€æ·±åº¦ç›¸æœºå’Œå¤šä¸ªåŠ é€Ÿåº¦è®¡ç­‰ä¸åŒçš„ä¼ æ„Ÿå™¨æ•°æ®æµå»ºæ¨¡ä¸ºç‰¹å®šæ¨¡æ€çš„å›¾ï¼Œé€šè¿‡æ®‹å·®å›¾å·ç§¯ç¥ç»ç½‘ç»œ(GCNs)è¿›è¡Œå¤„ç†ï¼Œå¹¶é€šè¿‡åŸºäºæ³¨æ„åŠ›çš„åŠ æƒèåˆï¼Œè€Œéç®€å•çš„æ‹¼æ¥ã€‚èåˆåçš„åµŒå…¥èƒ½å¤Ÿå®ç°é²æ£’çš„æ´»åŠ¨åˆ†ç±»ï¼Œè€Œå·®åˆ†éšç§åˆ™åœ¨è”é‚¦èšåˆè¿‡ç¨‹ä¸­ä¿æŠ¤æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MultiModalGCNæ¨¡å‹ä¼˜äºåŸºçº¿MultiModalFFNï¼Œåœ¨é›†ä¸­å¼å’Œè”é‚¦èŒƒå¼ä¸­ï¼Œéå·®åˆ†éšç§è®¾ç½®ä¸‹çš„å‡†ç¡®ç‡æé«˜äº†2%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨å·®åˆ†éšç§çº¦æŸä¸‹è§‚å¯Ÿåˆ°æ˜¾è‘—çš„æ”¹è¿›ï¼šMultiModalGCNå§‹ç»ˆä¼˜äºMultiModalFFNï¼Œæ€§èƒ½å·®è·åœ¨7%åˆ°13%ä¹‹é—´ï¼Œå…·ä½“å–å†³äºéšç§é¢„ç®—å’Œè®¾ç½®ã€‚è¿™äº›ç»“æœçªå‡ºäº†åŸºäºå›¾çš„å»ºæ¨¡åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„é²æ£’æ€§ï¼Œå…¶ä¸­GNNè¢«è¯æ˜æ›´èƒ½æŠµæŠ—ç”±å·®åˆ†éšç§å™ªå£°å¼•èµ·çš„æ€§èƒ½ä¸‹é™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€äººä½“æ´»åŠ¨è¯†åˆ«ä¸­ï¼Œç”±äºæ•°æ®å¼‚æ„æ€§ã€æ ‡æ³¨æ•°æ®ç¨€ç¼ºä»¥åŠéšç§ä¿æŠ¤éœ€æ±‚å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ä¼ ç»Ÿçš„é›†ä¸­å¼æ·±åº¦å­¦ä¹ ï¼Œå—é™äºæ•°æ®å…±äº«å’Œéšç§æ³„éœ²é£é™©ã€‚è”é‚¦å­¦ä¹ è™½ç„¶è§£å†³äº†æ•°æ®å…±äº«é—®é¢˜ï¼Œä½†éš¾ä»¥æœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€æ•°æ®çš„èåˆï¼Œå¹¶ä¸”åœ¨å¼•å…¥å·®åˆ†éšç§åï¼Œæ¨¡å‹æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä¸åŒçš„ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆå¦‚å‹åŠ›å«ã€æ·±åº¦ç›¸æœºã€åŠ é€Ÿåº¦è®¡ï¼‰å»ºæ¨¡æˆä¸åŒçš„å›¾ç»“æ„ï¼Œåˆ©ç”¨å›¾å·ç§¯ç¥ç»ç½‘ç»œ(GCN)æå–æ¯ä¸ªæ¨¡æ€çš„ç‰¹å¾ï¼Œç„¶åé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”åœ°èåˆè¿™äº›ç‰¹å¾ã€‚è¿™ç§åŸºäºå›¾çš„å»ºæ¨¡æ–¹å¼èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€æ•°æ®ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸”GCNå¯¹å·®åˆ†éšç§å¼•å…¥çš„å™ªå£°å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGraMFedDHARæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **æ•°æ®é¢„å¤„ç†**ï¼šå°†ä¸åŒæ¨¡æ€çš„ä¼ æ„Ÿå™¨æ•°æ®è¿›è¡Œæ¸…æ´—å’Œé¢„å¤„ç†ã€‚2) **å›¾æ„å»º**ï¼šå°†æ¯ä¸ªæ¨¡æ€çš„æ•°æ®æ„å»ºæˆå›¾ç»“æ„ï¼Œä¾‹å¦‚ï¼Œå°†åŠ é€Ÿåº¦è®¡æ•°æ®æ„å»ºæˆæ—¶é—´åºåˆ—å›¾ã€‚3) **å›¾å·ç§¯ç¥ç»ç½‘ç»œ(GCN)**ï¼šä½¿ç”¨æ®‹å·®GCNæå–æ¯ä¸ªæ¨¡æ€çš„ç‰¹å¾è¡¨ç¤ºã€‚4) **æ³¨æ„åŠ›èåˆ**ï¼šä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å¯¹ä¸åŒæ¨¡æ€çš„ç‰¹å¾è¿›è¡ŒåŠ æƒèåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„èåˆç‰¹å¾ã€‚5) **æ´»åŠ¨åˆ†ç±»**ï¼šä½¿ç”¨åˆ†ç±»å™¨ï¼ˆå¦‚å…¨è¿æ¥å±‚ï¼‰å¯¹èåˆç‰¹å¾è¿›è¡Œæ´»åŠ¨åˆ†ç±»ã€‚6) **è”é‚¦å­¦ä¹ èšåˆ**ï¼šåœ¨è”é‚¦å­¦ä¹ æ¡†æ¶ä¸‹ï¼Œå®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå°†æ¨¡å‹å‚æ•°ä¸Šä¼ åˆ°æœåŠ¡å™¨è¿›è¡Œèšåˆã€‚7) **å·®åˆ†éšç§**ï¼šåœ¨æ¨¡å‹å‚æ•°ä¸Šä¼ ä¹‹å‰ï¼Œå¯¹å‚æ•°æ·»åŠ å™ªå£°ï¼Œä»¥æ»¡è¶³å·®åˆ†éšç§çš„è¦æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **åŸºäºå›¾çš„å¤šæ¨¡æ€å»ºæ¨¡**ï¼šå°†å¤šæ¨¡æ€æ•°æ®å»ºæ¨¡æˆå›¾ç»“æ„ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ•°æ®ä¹‹é—´çš„å…³ç³»ã€‚2) **æ®‹å·®å›¾å·ç§¯ç¥ç»ç½‘ç»œ(GCN)**ï¼šä½¿ç”¨æ®‹å·®GCNæå–ç‰¹å¾ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³æ·±å±‚GCNçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚3) **æ³¨æ„åŠ›èåˆæœºåˆ¶**ï¼šä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”åœ°èåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚4) **å·®åˆ†éšç§è”é‚¦å­¦ä¹ **ï¼šåœ¨è”é‚¦å­¦ä¹ æ¡†æ¶ä¸‹å¼•å…¥å·®åˆ†éšç§ï¼Œèƒ½å¤Ÿåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶ï¼Œä¿è¯æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **å›¾çš„æ„å»ºæ–¹å¼**ï¼šæ ¹æ®ä¸åŒæ¨¡æ€æ•°æ®çš„ç‰¹ç‚¹ï¼Œè®¾è®¡ä¸åŒçš„å›¾ç»“æ„ã€‚ä¾‹å¦‚ï¼Œå¯¹äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œå¯ä»¥æ„å»ºæ—¶é—´åºåˆ—å›¾ï¼›å¯¹äºç©ºé—´æ•°æ®ï¼Œå¯ä»¥æ„å»ºç©ºé—´å›¾ã€‚2) **æ®‹å·®GCNçš„ç½‘ç»œç»“æ„**ï¼šè®¾è®¡åˆé€‚çš„æ®‹å·®GCNç½‘ç»œç»“æ„ï¼ŒåŒ…æ‹¬GCNçš„å±‚æ•°ã€æ¯å±‚çš„èŠ‚ç‚¹æ•°ç­‰ã€‚3) **æ³¨æ„åŠ›æœºåˆ¶çš„è®¾è®¡**ï¼šé€‰æ‹©åˆé€‚çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¾‹å¦‚ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶æˆ–äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚4) **å·®åˆ†éšç§çš„å‚æ•°è®¾ç½®**ï¼šé€‰æ‹©åˆé€‚çš„å·®åˆ†éšç§å‚æ•°ï¼Œä¾‹å¦‚ï¼Œéšç§é¢„ç®—Îµå’ŒÎ´ï¼Œä»¥å¹³è¡¡éšç§ä¿æŠ¤å’Œæ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MultiModalGCNæ¨¡å‹åœ¨éå·®åˆ†éšç§è®¾ç½®ä¸‹ï¼Œç›¸è¾ƒäºåŸºçº¿MultiModalFFNæ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†2%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨å·®åˆ†éšç§çº¦æŸä¸‹ï¼ŒMultiModalGCNçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºMultiModalFFNï¼Œæ€§èƒ½å·®è·åœ¨7%åˆ°13%ä¹‹é—´ï¼Œè¯æ˜äº†å›¾ç¥ç»ç½‘ç»œåœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œå·®åˆ†éšç§åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€åŒ»ç–—å¥åº·ã€å…»è€ç›‘æŠ¤ç­‰é¢†åŸŸã€‚é€šè¿‡åˆ†æç”¨æˆ·çš„æ´»åŠ¨æ•°æ®ï¼Œå¯ä»¥å®ç°è·Œå€’æ£€æµ‹ã€å¼‚å¸¸è¡Œä¸ºé¢„è­¦ã€å¥åº·çŠ¶æ€è¯„ä¼°ç­‰åŠŸèƒ½ï¼Œä»è€Œæé«˜ç”Ÿæ´»è´¨é‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›ä¸å¯ç©¿æˆ´è®¾å¤‡ã€ç‰©è”ç½‘ä¼ æ„Ÿå™¨ç­‰ç»“åˆï¼Œæ„å»ºæ›´åŠ æ™ºèƒ½åŒ–çš„å¥åº·ç›‘æµ‹ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human Activity Recognition (HAR) using multimodal sensor data remains challenging due to noisy or incomplete measurements, scarcity of labeled examples, and privacy concerns. Traditional centralized deep learning approaches are often constrained by infrastructure availability, network latency, and data sharing restrictions. While federated learning (FL) addresses privacy by training models locally and sharing only model parameters, it still has to tackle issues arising from the use of heterogeneous multimodal data and differential privacy requirements. In this article, a Graph-based Multimodal Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse sensor streams such as a pressure mat, depth camera, and multiple accelerometers are modeled as modality-specific graphs, processed through residual Graph Convolutional Neural Networks (GCNs), and fused via attention-based weighting rather than simple concatenation. The fused embeddings enable robust activity classification, while differential privacy safeguards data during federated aggregation. Experimental results show that the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with up to 2 percent higher accuracy in non-DP settings in both centralized and federated paradigms. More importantly, significant improvements are observed under differential privacy constraints: MultiModalGCN consistently surpasses MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on the privacy budget and setting. These results highlight the robustness of graph-based modeling in multimodal learning, where GNNs prove more resilient to the performance degradation introduced by DP noise.

