---
layout: default
title: Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies
---

# Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05735" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05735v1</a>
  <a href="https://arxiv.org/pdf/2509.05735.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05735v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05735v1', 'Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaqi Chen, Ji Shi, Cansu Sancaktar, Jonas Frey, Georg Martius

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06

**å¤‡æ³¨**: Accepted at Reinforcement Learning Conference (RLC 2025); Code available at: https://github.com/swsychen/Offline_vs_Online_in_MBRL

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­ç¦»çº¿ä¸åœ¨çº¿å­¦ä¹ å¯¹æ¯”ç ”ç©¶ï¼Œæ­ç¤ºæ•°æ®æ”¶é›†ç­–ç•¥å¯¹æ€§èƒ½çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡å‹å¼ºåŒ–å­¦ä¹ ` `ç¦»çº¿å­¦ä¹ ` `åœ¨çº¿å­¦ä¹ ` `æ•°æ®æ”¶é›†ç­–ç•¥` `ä¸–ç•Œæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ä¾èµ–é«˜è´¨é‡æ•°æ®ï¼Œä½†åœ¨çº¿å’Œç¦»çº¿æ•°æ®æ”¶é›†ç­–ç•¥å¯¹ä¸–ç•Œæ¨¡å‹çš„å½±å“å°šä¸æ˜ç¡®ã€‚
2. è¯¥ç ”ç©¶å¯¹æ¯”äº†åœ¨çº¿å’Œç¦»çº¿å­¦ä¹ èŒƒå¼ï¼Œå‘ç°ç¦»çº¿æ™ºèƒ½ä½“ç”±äºåˆ†å¸ƒå¤–çŠ¶æ€å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
3. é€šè¿‡å¼•å…¥é¢å¤–çš„åœ¨çº¿äº¤äº’æˆ–æ¢ç´¢æ•°æ®ï¼Œå¯ä»¥æœ‰æ•ˆç¼“è§£ç¦»çº¿æ™ºèƒ½ä½“çš„æ€§èƒ½é—®é¢˜ï¼Œæ¢å¤åœ¨çº¿è®­ç»ƒçš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®æ”¶é›†å¯¹äºåœ¨åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ä¸­å­¦ä¹ é²æ£’çš„ä¸–ç•Œæ¨¡å‹è‡³å…³é‡è¦ã€‚æœ€å¸¸è§çš„ç­–ç•¥æ˜¯åœ¨åœ¨çº¿è®­ç»ƒæœŸé—´é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥ä¸»åŠ¨æ”¶é›†è½¨è¿¹ï¼Œæˆ–è€…åœ¨ç¦»çº¿æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä¹ä¸€çœ‹ï¼Œå­¦ä¹ ä¸ä»»åŠ¡æ— å…³çš„ç¯å¢ƒåŠ¨æ€çš„æ€§è´¨ä½¿å¾—ä¸–ç•Œæ¨¡å‹æˆä¸ºæœ‰æ•ˆç¦»çº¿è®­ç»ƒçš„è‰¯å¥½å€™é€‰è€…ã€‚ç„¶è€Œï¼Œåœ¨çº¿ä¸ç¦»çº¿æ•°æ®å¯¹ä¸–ç•Œæ¨¡å‹ä»¥åŠç”±æ­¤äº§ç”Ÿçš„ä»»åŠ¡æ€§èƒ½çš„å½±å“å°šæœªåœ¨æ–‡çŒ®ä¸­å¾—åˆ°å½»åº•ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºæ¨¡å‹çš„è®¾ç½®ä¸­çš„è¿™ä¸¤ç§èŒƒä¾‹ï¼Œåœ¨31ç§ä¸åŒçš„ç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨çº¿æ™ºèƒ½ä½“ä¼˜äºç¦»çº¿æ™ºèƒ½ä½“ã€‚æˆ‘ä»¬ç¡®å®šäº†ç¦»çº¿æ™ºèƒ½ä½“æ€§èƒ½ä¸‹é™èƒŒåçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šåœ¨æµ‹è¯•æ—¶é‡åˆ°åˆ†å¸ƒå¤–ï¼ˆOut-Of-Distributionï¼‰çŠ¶æ€ã€‚è¿™ä¸ªé—®é¢˜å‡ºç°çš„åŸå› æ˜¯ï¼Œåœ¨æ²¡æœ‰åœ¨çº¿æ™ºèƒ½ä½“ä¸­çš„è‡ªæˆ‘çº æ­£æœºåˆ¶çš„æƒ…å†µä¸‹ï¼ŒçŠ¶æ€ç©ºé—´è¦†ç›–æœ‰é™çš„ç¦»çº¿æ•°æ®é›†ä¼šå¯¼è‡´æ™ºèƒ½ä½“çš„æƒ³è±¡å’ŒçœŸå®è½¨è¿¹ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œä»è€ŒæŸå®³ç­–ç•¥è®­ç»ƒã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡åœ¨å›ºå®šæˆ–è‡ªé€‚åº”çš„æ—¶é—´è¡¨ä¸­å…è®¸é¢å¤–çš„åœ¨çº¿äº¤äº’ï¼Œå¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä»è€Œæ¢å¤å…·æœ‰æœ‰é™äº¤äº’æ•°æ®çš„åœ¨çº¿è®­ç»ƒçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ç»“åˆæ¢ç´¢æ•°æ®æœ‰åŠ©äºå‡è½»ç¦»çº¿æ™ºèƒ½ä½“çš„æ€§èƒ½ä¸‹é™ã€‚æ ¹æ®æˆ‘ä»¬çš„è§è§£ï¼Œæˆ‘ä»¬å»ºè®®åœ¨æ”¶é›†å¤§å‹æ•°æ®é›†æ—¶æ·»åŠ æ¢ç´¢æ•°æ®ï¼Œå› ä¸ºç›®å‰çš„åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨ä¸“å®¶æ•°æ®ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç¦»çº¿æ•°æ®è®­ç»ƒçš„ä¸–ç•Œæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ä¸“å®¶æ•°æ®ï¼Œå¿½ç•¥äº†æ¢ç´¢æ•°æ®çš„é‡è¦æ€§ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œæ— æ³•åº”å¯¹çœŸå®ç¯å¢ƒä¸­æœªæ›¾é‡åˆ°çš„çŠ¶æ€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¯¹æ¯”åœ¨çº¿å’Œç¦»çº¿å­¦ä¹ èŒƒå¼ï¼Œåˆ†æç¦»çº¿å­¦ä¹ æ€§èƒ½ä¸‹é™çš„åŸå› ï¼Œå¹¶æå‡ºé€šè¿‡å¼•å…¥åœ¨çº¿äº¤äº’æˆ–æ¢ç´¢æ•°æ®æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡åœ¨çº¿äº¤äº’ï¼Œæ™ºèƒ½ä½“å¯ä»¥è‡ªæˆ‘çº æ­£ï¼Œé€‚åº”ç¯å¢ƒå˜åŒ–ï¼›è€Œæ¢ç´¢æ•°æ®å¯ä»¥æ‰©å±•çŠ¶æ€ç©ºé—´è¦†ç›–ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶é‡‡ç”¨åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸–ç•Œæ¨¡å‹å­¦ä¹ å’Œç­–ç•¥ä¼˜åŒ–ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œä½¿ç”¨ç¦»çº¿æˆ–åœ¨çº¿æ•°æ®è®­ç»ƒä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºé¢„æµ‹ç¯å¢ƒçš„åŠ¨æ€å˜åŒ–ã€‚ç„¶åï¼Œåˆ©ç”¨è®­ç»ƒå¥½çš„ä¸–ç•Œæ¨¡å‹è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œå­¦ä¹ å¦‚ä½•åœ¨ç¯å¢ƒä¸­é‡‡å–æœ€ä¼˜åŠ¨ä½œã€‚å®éªŒåœ¨31ä¸ªä¸åŒçš„ç¯å¢ƒä¸­è¿›è¡Œï¼Œå¯¹æ¯”äº†ä¸åŒæ•°æ®æ”¶é›†ç­–ç•¥ä¸‹çš„æ™ºèƒ½ä½“æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ­ç¤ºäº†ç¦»çº¿å­¦ä¹ ä¸­åˆ†å¸ƒå¤–çŠ¶æ€é—®é¢˜å¯¹ä¸–ç•Œæ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æå‡ºäº†é€šè¿‡å¼•å…¥åœ¨çº¿äº¤äº’æˆ–æ¢ç´¢æ•°æ®æ¥ç¼“è§£è¿™ä¸€é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚è¿™ä¸ºç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ•°æ®æ”¶é›†ç­–ç•¥æä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡è®¾è®¡äº†ä¸åŒçš„æ•°æ®æ”¶é›†ç­–ç•¥ï¼ŒåŒ…æ‹¬çº¯ç¦»çº¿æ•°æ®ã€çº¯åœ¨çº¿æ•°æ®ã€ä»¥åŠæ··åˆç­–ç•¥ï¼ˆç¦»çº¿æ•°æ®+å°‘é‡åœ¨çº¿äº¤äº’æˆ–æ¢ç´¢æ•°æ®ï¼‰ã€‚é€šè¿‡å¯¹æ¯”è¿™äº›ç­–ç•¥ä¸‹çš„æ™ºèƒ½ä½“æ€§èƒ½ï¼Œåˆ†æäº†ä¸åŒæ•°æ®æ”¶é›†æ–¹å¼å¯¹ä¸–ç•Œæ¨¡å‹å­¦ä¹ çš„å½±å“ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨çº¿æ™ºèƒ½ä½“åœ¨31ä¸ªä¸åŒç¯å¢ƒä¸­å‡ä¼˜äºç¦»çº¿æ™ºèƒ½ä½“ã€‚é€šè¿‡å¼•å…¥å°‘é‡åœ¨çº¿äº¤äº’æˆ–æ¢ç´¢æ•°æ®ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç¦»çº¿æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼Œä½¿å…¶æ¥è¿‘ç”šè‡³è¾¾åˆ°åœ¨çº¿æ™ºèƒ½ä½“çš„æ°´å¹³ã€‚è¿™è¡¨æ˜ï¼Œåœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ•°æ®æ”¶é›†ç­–ç•¥çš„é€‰æ‹©è‡³å…³é‡è¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚é€šè¿‡ç»“åˆç¦»çº¿æ•°æ®å’Œå°‘é‡åœ¨çº¿äº¤äº’ï¼Œå¯ä»¥é™ä½æ•°æ®æ”¶é›†æˆæœ¬ï¼Œæé«˜æ™ºèƒ½ä½“çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ›´æœ‰æ•ˆçš„æ¢ç´¢ç­–ç•¥å’Œæ•°æ®èåˆæ–¹æ³•ï¼Œä»¥æå‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Data collection is crucial for learning robust world models in model-based reinforcement learning. The most prevalent strategies are to actively collect trajectories by interacting with the environment during online training or training on offline datasets. At first glance, the nature of learning task-agnostic environment dynamics makes world models a good candidate for effective offline training. However, the effects of online vs. offline data on world models and thus on the resulting task performance have not been thoroughly studied in the literature. In this work, we investigate both paradigms in model-based settings, conducting experiments on 31 different environments. First, we showcase that online agents outperform their offline counterparts. We identify a key challenge behind performance degradation of offline agents: encountering Out-Of-Distribution states at test time. This issue arises because, without the self-correction mechanism in online agents, offline datasets with limited state space coverage induce a mismatch between the agent's imagination and real rollouts, compromising policy training. We demonstrate that this issue can be mitigated by allowing for additional online interactions in a fixed or adaptive schedule, restoring the performance of online training with limited interaction data. We also showcase that incorporating exploration data helps mitigate the performance degradation of offline agents. Based on our insights, we recommend adding exploration data when collecting large datasets, as current efforts predominantly focus on expert data alone.

