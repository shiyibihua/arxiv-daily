---
layout: default
title: Finetuning LLMs for Human Behavior Prediction in Social Science Experiments
---

# Finetuning LLMs for Human Behavior Prediction in Social Science Experiments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05830" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05830v2</a>
  <a href="https://arxiv.org/pdf/2509.05830.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05830v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05830v2', 'Finetuning LLMs for Human Behavior Prediction in Social Science Experiments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Akaash Kolluri, Shengguang Wu, Joon Sung Park, Michael S. Bernstein

**åˆ†ç±»**: cs.LG, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06 (æ›´æ–°: 2025-11-05)

**å¤‡æ³¨**: 16 pages, 5 figures

**æœŸåˆŠ**: Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 30084-30099

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å¾®è°ƒLLMï¼Œæå‡ç¤¾ä¼šç§‘å­¦å®éªŒä¸­äººç±»è¡Œä¸ºé¢„æµ‹çš„å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¤¾ä¼šç§‘å­¦å®éªŒ` `äººç±»è¡Œä¸ºé¢„æµ‹` `å¾®è°ƒ` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥å‡†ç¡®æ¨¡æ‹Ÿç¤¾ä¼šç§‘å­¦å®éªŒä¸­å¤æ‚çš„äººç±»è¡Œä¸ºï¼Œé™åˆ¶äº†å®éªŒå‡è®¾çš„æœ‰æ•ˆç­›é€‰ã€‚
2. é€šè¿‡åœ¨åŒ…å«å¤§é‡ç¤¾ä¼šç§‘å­¦å®éªŒæ•°æ®çš„SocSci210æ•°æ®é›†ä¸Šå¾®è°ƒLLMï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ äººç±»è¡Œä¸ºçš„æ¨¡å¼ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨é¢„æµ‹äººç±»è¡Œä¸ºæ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨æ³›åŒ–æ€§å’Œå‡å°‘åå·®æ–¹é¢æœ‰æ‰€æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿç¤¾ä¼šç§‘å­¦å®éªŒç»“æœçš„å¯èƒ½æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›´æ¥åœ¨è¿‡å»å®éªŒçš„ä¸ªä½“å±‚é¢å“åº”æ•°æ®ä¸Šå¾®è°ƒLLMï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ­¤ç±»æ¨¡æ‹Ÿåœ¨ä¸åŒç¤¾ä¼šç§‘å­¦é¢†åŸŸçš„å‡†ç¡®æ€§ã€‚ä½œè€…é€šè¿‡è‡ªåŠ¨æµç¨‹æ„å»ºäº†SocSci210æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª210ä¸ªå¼€æºç¤¾ä¼šç§‘å­¦å®éªŒä¸­400,491åå‚ä¸è€…çš„290ä¸‡ä¸ªå“åº”ã€‚é€šè¿‡å¾®è°ƒï¼Œæ¨¡å‹å®ç°äº†å¤šå±‚æ¬¡çš„æ³›åŒ–ã€‚åœ¨å®Œå…¨æœªè§è¿‡çš„ç ”ç©¶ä¸­ï¼Œæœ€å¼ºçš„æ¨¡å‹Socrates-Qwen-14Bç›¸å¯¹äºå…¶åŸºç¡€æ¨¡å‹(Qwen2.5-14B)ï¼Œåœ¨ä¸åŒæ¡ä»¶ä¸‹å¯¹å„ç§ç»“æœé—®é¢˜çš„é¢„æµ‹ä¸äººç±»å“åº”åˆ†å¸ƒçš„å¯¹é½åº¦æé«˜äº†26%ï¼Œè¶…è¿‡GPT-4o 13%ã€‚é€šè¿‡åœ¨ç ”ç©¶çš„æ¡ä»¶å­é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯¹æ–°çš„æœªè§æ¡ä»¶çš„æ³›åŒ–èƒ½åŠ›å°¤å…¶å¼ºå¤§ï¼Œæé«˜äº†71%ã€‚ç”±äºSocSci210åŒ…å«ä¸°å¯Œçš„ç»Ÿè®¡ä¿¡æ¯ï¼Œé€šè¿‡å¾®è°ƒï¼Œäººå£ç»Ÿè®¡å‡ç­‰æ€§å·®å¼‚ï¼ˆä¸€ç§åå·®åº¦é‡ï¼‰é™ä½äº†10.6%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç”±äºç¤¾ä¼šç§‘å­¦ç»å¸¸ç”Ÿæˆä¸°å¯Œçš„ä¸»é¢˜ç‰¹å®šæ•°æ®é›†ï¼Œå› æ­¤åœ¨æ­¤ç±»æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥å®ç°æ›´å‡†ç¡®çš„å®éªŒå‡è®¾ç­›é€‰æ¨¡æ‹Ÿã€‚ä½œè€…å‘å¸ƒäº†æ•°æ®ã€æ¨¡å‹å’Œå¾®è°ƒä»£ç ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¤¾ä¼šç§‘å­¦å®éªŒä¸­äººç±»è¡Œä¸ºé¢„æµ‹ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨é€šç”¨LLMï¼Œæ— æ³•å……åˆ†æ•æ‰ç‰¹å®šå®éªŒåœºæ™¯ä¸‹äººç±»è¡Œä¸ºçš„ç»†å¾®å·®åˆ«ï¼Œå¯¼è‡´æ¨¡æ‹Ÿç»“æœä¸çœŸå®æƒ…å†µå­˜åœ¨åå·®ã€‚è¿™é™åˆ¶äº†ç ”ç©¶äººå‘˜åˆ©ç”¨æ¨¡æ‹Ÿæ¥å¿«é€Ÿç­›é€‰å’ŒéªŒè¯å®éªŒå‡è®¾çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¤¾ä¼šç§‘å­¦é¢†åŸŸå·²æœ‰çš„ä¸°å¯Œå®éªŒæ•°æ®ï¼Œé€šè¿‡å¾®è°ƒLLMï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ç‰¹å®šå®éªŒåœºæ™¯ä¸‹çš„äººç±»è¡Œä¸ºæ¨¡å¼ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿä½¿LLMæ›´å¥½åœ°ç†è§£å®éªŒè®¾è®¡ã€é—®é¢˜è®¾ç½®ä»¥åŠå‚ä¸è€…çš„èƒŒæ™¯ä¿¡æ¯ï¼Œä»è€Œæ›´å‡†ç¡®åœ°é¢„æµ‹äººç±»è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) æ„å»ºSocSci210æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤§é‡ç¤¾ä¼šç§‘å­¦å®éªŒçš„å‚ä¸è€…å“åº”æ•°æ®ï¼›2) é€‰æ‹©é¢„è®­ç»ƒçš„LLMä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œä¾‹å¦‚Qwen2.5-14Bï¼›3) åœ¨SocSci210æ•°æ®é›†ä¸Šå¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”ç¤¾ä¼šç§‘å­¦å®éªŒçš„é¢„æµ‹ä»»åŠ¡ï¼›4) åœ¨ä¸åŒçš„å®éªŒåœºæ™¯ä¸‹è¯„ä¼°å¾®è°ƒåæ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨æœªè§è¿‡çš„ç ”ç©¶å’Œæœªè§è¿‡çš„æ¡ä»¶ä¸‹è¿›è¡Œæ³›åŒ–æµ‹è¯•ï¼›5) åˆ†ææ¨¡å‹çš„åå·®ï¼Œå¹¶å°è¯•é€šè¿‡å¾®è°ƒæ¥å‡å°‘åå·®ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ„å»ºäº†å¤§è§„æ¨¡çš„SocSci210æ•°æ®é›†ï¼Œä¸ºLLMçš„å¾®è°ƒæä¾›äº†å……è¶³çš„æ•°æ®æ”¯æŒï¼›2) è¯æ˜äº†ç›´æ¥åœ¨ä¸ªä½“å±‚é¢çš„å®éªŒå“åº”æ•°æ®ä¸Šå¾®è°ƒLLMèƒ½å¤Ÿæ˜¾è‘—æé«˜äººç±»è¡Œä¸ºé¢„æµ‹çš„å‡†ç¡®æ€§ï¼›3) æ¢ç´¢äº†å¾®è°ƒLLMåœ¨ä¸åŒæ³›åŒ–åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¯¹æœªè§è¿‡çš„ç ”ç©¶å’Œæœªè§è¿‡çš„æ¡ä»¶çš„æ³›åŒ–ï¼›4) æå‡ºäº†é€šè¿‡å¾®è°ƒæ¥å‡å°‘æ¨¡å‹åå·®çš„æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) SocSci210æ•°æ®é›†çš„æ„å»ºè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®çš„è‡ªåŠ¨æ”¶é›†ã€æ¸…æ´—å’Œæ•´åˆï¼›2) å¾®è°ƒè¿‡ç¨‹ä¸­çš„è¶…å‚æ•°è®¾ç½®ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€batch sizeç­‰ï¼›3) è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ï¼ŒåŒ…æ‹¬é¢„æµ‹å‡†ç¡®ç‡ã€æ³›åŒ–èƒ½åŠ›å’Œåå·®åº¦é‡ï¼›4) æ¨¡å‹æ¶æ„çš„é€‰æ‹©ï¼Œä¾‹å¦‚é€‰æ‹©Qwen2.5-14Bä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Socrates-Qwen-14Bæ¨¡å‹åœ¨æœªè§è¿‡çš„ç ”ç©¶ä¸­ï¼Œé¢„æµ‹ä¸äººç±»å“åº”åˆ†å¸ƒçš„å¯¹é½åº¦æ¯”åŸºç¡€æ¨¡å‹Qwen2.5-14Bæé«˜äº†26%ï¼Œè¶…è¿‡GPT-4o 13%ã€‚åœ¨ç ”ç©¶çš„æ¡ä»¶å­é›†ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œå¯¹æ–°çš„æœªè§æ¡ä»¶çš„æ³›åŒ–èƒ½åŠ›æé«˜äº†71%ã€‚é€šè¿‡å¾®è°ƒï¼Œäººå£ç»Ÿè®¡å‡ç­‰æ€§å·®å¼‚ï¼ˆä¸€ç§åå·®åº¦é‡ï¼‰é™ä½äº†10.6%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¾®è°ƒLLMèƒ½å¤Ÿæ˜¾è‘—æé«˜ç¤¾ä¼šç§‘å­¦å®éªŒä¸­äººç±»è¡Œä¸ºé¢„æµ‹çš„å‡†ç¡®æ€§å’Œå…¬å¹³æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç¤¾ä¼šç§‘å­¦ç ”ç©¶çš„å¤šä¸ªé¢†åŸŸï¼Œä¾‹å¦‚å®éªŒè®¾è®¡ä¼˜åŒ–ã€æ”¿ç­–æ•ˆæœè¯„ä¼°ã€è¡Œä¸ºå¹²é¢„æ¨¡æ‹Ÿç­‰ã€‚é€šè¿‡ä½¿ç”¨å¾®è°ƒåçš„LLMè¿›è¡Œå®éªŒæ¨¡æ‹Ÿï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´é«˜æ•ˆåœ°ç­›é€‰å’ŒéªŒè¯å®éªŒå‡è®¾ï¼Œé™ä½å®éªŒæˆæœ¬ï¼Œå¹¶æ›´å¥½åœ°ç†è§£äººç±»è¡Œä¸ºçš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºé¢„æµ‹ä¸åŒäººç¾¤å¯¹ç‰¹å®šæ”¿ç­–æˆ–å¹²é¢„æªæ–½çš„ååº”ï¼Œä»è€Œä¸ºæ”¿ç­–åˆ¶å®šæä¾›æ›´ç§‘å­¦çš„ä¾æ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) offer a powerful opportunity to simulate the results of social science experiments. In this work, we demonstrate that finetuning LLMs directly on individual-level responses from past experiments meaningfully improves the accuracy of such simulations across diverse social science domains. We construct SocSci210 via an automatic pipeline, a dataset comprising 2.9 million responses from 400,491 participants in 210 open-source social science experiments. Through finetuning, we achieve multiple levels of generalization. In completely unseen studies, our strongest model, Socrates-Qwen-14B, produces predictions that are 26% more aligned with distributions of human responses to diverse outcome questions under varying conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by 13%. By finetuning on a subset of conditions in a study, generalization to new unseen conditions is particularly robust, improving by 71%. Since SocSci210 contains rich demographic information, we reduce demographic parity difference, a measure of bias, by 10.6% through finetuning. Because social sciences routinely generate rich, topic-specific datasets, our findings indicate that finetuning on such data could enable more accurate simulations for experimental hypothesis screening. We release our data, models and finetuning code at stanfordhci.github.io/socrates.

