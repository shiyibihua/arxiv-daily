---
layout: default
title: SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning
---

# SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.08989" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.08989v1</a>
  <a href="https://arxiv.org/pdf/2506.08989.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.08989v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.08989v1', 'SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, Weizhu Chen

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10

**å¤‡æ³¨**: Reinforcement Learning; Large Language Models; LLM Reasoning

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæˆ‘æ„è¯†å¼±ç‚¹é©±åŠ¨çš„é—®é¢˜åˆæˆæ¡†æ¶ä»¥æå‡å¼ºåŒ–å­¦ä¹ æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `é—®é¢˜åˆæˆ` `è‡ªæˆ‘æ„è¯†` `æ¨¡å‹å¢å¼º` `æ•°å­¦æ¨ç†` `æ™ºèƒ½æ•™è‚²` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡æ•°å­¦é—®é¢˜æ—¶é¢ä¸´äººç±»æ ‡æ³¨é—®é¢˜ç¨€ç¼ºå’Œç­”æ¡ˆéªŒè¯æœ‰é™çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºçš„SwSæ¡†æ¶é€šè¿‡è¯†åˆ«æ¨¡å‹çš„å¼±ç‚¹ï¼Œç³»ç»Ÿæ€§åœ°åˆæˆæ–°é—®é¢˜ä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSwSæ¡†æ¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œ7Bå’Œ32Bæ¨¡å‹çš„å¹³å‡æ€§èƒ½åˆ†åˆ«æé«˜äº†10.0%å’Œ7.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†é«˜è´¨é‡é—®é¢˜é›†çš„ç¼ºä¹é™åˆ¶äº†å…¶æ‰©å±•æ€§ã€‚ç°æœ‰åˆæˆæ•°æ®é›†ä¸­çš„äººç±»æ ‡æ³¨æ•°å­¦é—®é¢˜ç¨€ç¼ºï¼Œä¸”éªŒè¯ç­”æ¡ˆæœ‰é™ï¼Œå¯¼è‡´ç”Ÿæˆæœ‰æ•ˆé—®é¢˜çš„æ•ˆç‡ä½ä¸‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªæˆ‘æ„è¯†å¼±ç‚¹é©±åŠ¨çš„é—®é¢˜åˆæˆæ¡†æ¶ï¼ˆSwSï¼‰ï¼Œé€šè¿‡è¯†åˆ«æ¨¡å‹çš„ä¸è¶³å¹¶åˆ©ç”¨è¿™äº›å¼±ç‚¹è¿›è¡Œé—®é¢˜å¢å¼ºã€‚å…·ä½“è€Œè¨€ï¼Œå¼±ç‚¹è¢«å®šä¹‰ä¸ºæ¨¡å‹åœ¨RLè®­ç»ƒä¸­åå¤æœªèƒ½å­¦ä¹ çš„é—®é¢˜ã€‚é€šè¿‡æå–è¿™äº›å¤±è´¥æ¡ˆä¾‹çš„æ ¸å¿ƒæ¦‚å¿µï¼Œåˆæˆæ–°é—®é¢˜ä»¥åŠ å¼ºæ¨¡å‹çš„è–„å¼±é¢†åŸŸï¼Œä»è€Œæå‡å…¶æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸ƒä¸ªä¸»æµæ¨ç†åŸºå‡†ä¸Šï¼Œ7Bå’Œ32Bæ¨¡å‹çš„å¹³å‡æ€§èƒ½åˆ†åˆ«æå‡äº†10.0%å’Œ7.7%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡æ•°å­¦é—®é¢˜æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯äººç±»æ ‡æ³¨é—®é¢˜ç¨€ç¼ºå’ŒéªŒè¯ç­”æ¡ˆæœ‰é™çš„é—®é¢˜ã€‚ç°æœ‰åˆæˆç­–ç•¥å¾€å¾€ä¸è€ƒè™‘æ¨¡å‹èƒ½åŠ›ï¼Œå¯¼è‡´ç”Ÿæˆé—®é¢˜çš„æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSwSæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è¯†åˆ«æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åå¤æœªèƒ½è§£å†³çš„é—®é¢˜ï¼ˆå³å¼±ç‚¹ï¼‰ï¼Œå¹¶åˆ©ç”¨è¿™äº›å¼±ç‚¹è¿›è¡Œé—®é¢˜åˆæˆï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œè¯†åˆ«æ¨¡å‹çš„å¼±ç‚¹ï¼›å…¶æ¬¡ï¼Œä»å¤±è´¥æ¡ˆä¾‹ä¸­æå–æ ¸å¿ƒæ¦‚å¿µï¼›æœ€åï¼Œåˆæˆæ–°é—®é¢˜ä»¥é’ˆå¯¹æ€§åœ°å¢å¼ºæ¨¡å‹çš„è–„å¼±é¢†åŸŸã€‚

**å…³é”®åˆ›æ–°**ï¼šSwSæ¡†æ¶çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶è‡ªæˆ‘æ„è¯†èƒ½åŠ›ï¼Œèƒ½å¤Ÿè®©æ¨¡å‹è¯†åˆ«å¹¶è§£å†³è‡ªèº«çš„å¼±ç‚¹ï¼Œè€Œä¸ä¾èµ–å¤–éƒ¨çŸ¥è¯†è’¸é¦ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å…·å¤‡æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œæ¡†æ¶ä¸­æ¶‰åŠçš„å…³é”®å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æ—¨åœ¨ä¼˜åŒ–æ¨¡å‹å¯¹åˆæˆé—®é¢˜çš„å­¦ä¹ æ•ˆç‡ï¼Œå…·ä½“ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†é˜è¿°ã€‚é€šè¿‡è¿­ä»£è®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿé€æ­¥å…‹æœå…¶å¼±ç‚¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSwSæ¡†æ¶åœ¨ä¸ƒä¸ªä¸»æµæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå…¶ä¸­7Bæ¨¡å‹çš„å¹³å‡æ€§èƒ½æå‡äº†10.0%ï¼Œ32Bæ¨¡å‹çš„å¹³å‡æ€§èƒ½æå‡äº†7.7%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSwSæ¡†æ¶åœ¨é—®é¢˜åˆæˆå’Œæ¨¡å‹è®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–é—®é¢˜ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒSwSæ¡†æ¶èƒ½å¤Ÿä¸ºæ•™è‚²é¢†åŸŸæä¾›æ›´é«˜æ•ˆçš„å­¦ä¹ å·¥å…·ï¼Œå¸®åŠ©å­¦ç”Ÿæ›´å¥½åœ°æŒæ¡æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½å¯¹ä¸ªæ€§åŒ–å­¦ä¹ å’Œæ™ºèƒ½æ•™è‚²äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization byempowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks.

