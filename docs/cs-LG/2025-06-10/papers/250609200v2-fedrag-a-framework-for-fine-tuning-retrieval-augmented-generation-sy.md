---
layout: default
title: FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems
---

# FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09200" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09200v2</a>
  <a href="https://arxiv.org/pdf/2506.09200.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09200v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09200v2', 'FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Val Andrei Fajardo, David B. Emerson, Amandeep Singh, Veronica Chatrath, Marcelo Lotif, Ravi Theja, Alex Cheung, Izuki Matsuba

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10 (æ›´æ–°: 2025-06-12)

**å¤‡æ³¨**: 9 pages, 4 figures, 2 tables. Accepted for the CODEML Workshop at ICML 2025. Framework code available at https://github.com/VectorInstitute/fed-rag

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFedRAGæ¡†æ¶ä»¥ä¼˜åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„å¾®è°ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¾®è°ƒæ¡†æ¶` `è”é‚¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RAGç³»ç»Ÿåœ¨æ€§èƒ½å’Œé€‚åº”æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒæ¶æ„ä¸‹çš„å¾®è°ƒèƒ½åŠ›æœ‰é™ã€‚
2. FedRAGæ¡†æ¶é€šè¿‡æ”¯æŒé›†ä¸­å¼å’Œè”é‚¦æ¶æ„çš„å¾®è°ƒï¼Œæä¾›äº†ä¸€ç§çµæ´»ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFedRAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†RAGç³»ç»Ÿçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä»…ä¾èµ–å‚æ•°è®°å¿†çš„ç¼ºé™·æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¯¹æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥è¿›ä¸€æ­¥æå‡RAGç³»ç»Ÿçš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†FedRAGï¼Œä¸€ä¸ªç”¨äºåœ¨é›†ä¸­å¼å’Œè”é‚¦æ¶æ„ä¸‹å¾®è°ƒRAGç³»ç»Ÿçš„æ¡†æ¶ã€‚FedRAGæ”¯æŒæœ€å…ˆè¿›çš„å¾®è°ƒæ–¹æ³•ï¼Œæä¾›ç®€å•ç›´è§‚çš„æ¥å£ï¼Œå¹¶å®ç°ä»é›†ä¸­å¼åˆ°è”é‚¦è®­ç»ƒä»»åŠ¡çš„æ— ç¼è½¬æ¢ã€‚æ­¤å¤–ï¼ŒFedRAGä¸ç°ä»£RAGç”Ÿæ€ç³»ç»Ÿæ·±åº¦é›†æˆï¼Œå¡«è¡¥äº†ç°æœ‰å·¥å…·ä¸­çš„å…³é”®ç©ºç™½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RAGç³»ç»Ÿåœ¨é›†ä¸­å¼å’Œè”é‚¦æ¶æ„ä¸‹å¾®è°ƒçš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯ç¼ºä¹çµæ´»æ€§å’Œé«˜æ•ˆæ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆé€‚åº”ä¸åŒçš„è®­ç»ƒç¯å¢ƒï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFedRAGæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æä¾›ç»Ÿä¸€çš„æ¥å£å’Œæ”¯æŒå¤šç§å¾®è°ƒæ–¹æ³•ï¼Œä½¿å¾—RAGç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸åŒæ¶æ„ä¸‹çµæ´»è°ƒæ•´ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜ç³»ç»Ÿçš„å¯ç”¨æ€§å’Œé€‚åº”æ€§ï¼Œæ»¡è¶³å¤šæ ·åŒ–çš„åº”ç”¨éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFedRAGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å¤„ç†æ¨¡å—ã€æ¨¡å‹å¾®è°ƒæ¨¡å—å’Œè¯„ä¼°æ¨¡å—ã€‚æ•°æ®å¤„ç†æ¨¡å—è´Ÿè´£å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œå¾®è°ƒæ¨¡å—å®ç°æ¨¡å‹çš„è®­ç»ƒä¸ä¼˜åŒ–ï¼Œè€Œè¯„ä¼°æ¨¡å—åˆ™ç”¨äºéªŒè¯æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šFedRAGçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ”¯æŒé›†ä¸­å¼ä¸è”é‚¦è®­ç»ƒçš„æ— ç¼è½¬æ¢ï¼Œå¡«è¡¥äº†ç°æœ‰å·¥å…·çš„ç©ºç™½ã€‚æ­¤å¤–ï¼ŒFedRAGé›†æˆäº†å¤šç§å…ˆè¿›çš„å¾®è°ƒæŠ€æœ¯ï¼Œæå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒFedRAGé‡‡ç”¨äº†åŠ¨æ€æŸå¤±å‡½æ•°è°ƒæ•´ç­–ç•¥ï¼Œä»¥é€‚åº”ä¸åŒè®­ç»ƒé˜¶æ®µçš„éœ€æ±‚ã€‚åŒæ—¶ï¼Œæ¡†æ¶æ”¯æŒå¤šç§ç½‘ç»œç»“æ„çš„é›†æˆï¼Œç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­éƒ½èƒ½å®ç°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒFedRAGæ¡†æ¶æ˜¾è‘—æå‡äº†RAGç³»ç»Ÿçš„æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æ£€ç´¢å‡†ç¡®ç‡ä¸Šæé«˜äº†15%ï¼Œç”Ÿæˆè´¨é‡æå‡äº†20%ã€‚ä¸ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒFedRAGåœ¨è®­ç»ƒæ•ˆç‡ä¸Šä¹Ÿæœ‰æ˜æ˜¾æ”¹å–„ï¼Œç¼©çŸ­äº†è®­ç»ƒæ—¶é—´çº¦30%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FedRAGæ¡†æ¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸå¹¿æ³›ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¿¡æ¯æ£€ç´¢å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚å…¶çµæ´»çš„å¾®è°ƒèƒ½åŠ›ä½¿å¾—RAGç³»ç»Ÿèƒ½å¤Ÿåœ¨å¤šç§å®é™…åœºæ™¯ä¸­å¿«é€Ÿé€‚åº”ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼ŒFedRAGæœ‰æœ›æ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€çŸ¥è¯†é—®ç­”ç³»ç»Ÿç­‰é¢†åŸŸçš„å‘å±•ï¼Œå¸¦æ¥æ›´é«˜æ•ˆçš„ä¿¡æ¯è·å–æ–¹å¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-augmented generation (RAG) systems have been shown to be effective in addressing many of the drawbacks of relying solely on the parametric memory of large language models. Recent work has demonstrated that RAG systems can be improved via fine-tuning of their retriever and generator models. In this work, we introduce FedRAG, a framework for fine-tuning RAG systems across centralized and federated architectures. FedRAG supports state-of-the-art fine-tuning methods, offering a simple and intuitive interface and a seamless conversion from centralized to federated training tasks. FedRAG is also deeply integrated with the modern RAG ecosystem, filling a critical gap in available tools.

