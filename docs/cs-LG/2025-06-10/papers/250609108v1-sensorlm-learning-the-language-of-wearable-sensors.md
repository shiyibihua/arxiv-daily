---
layout: default
title: SensorLM: Learning the Language of Wearable Sensors
---

# SensorLM: Learning the Language of Wearable Sensors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09108" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09108v1</a>
  <a href="https://arxiv.org/pdf/2506.09108.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09108v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09108v1', 'SensorLM: Learning the Language of Wearable Sensors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuwei Zhang, Kumar Ayush, Siyuan Qiao, A. Ali Heydari, Girish Narayanswamy, Maxwell A. Xu, Ahmed A. Metwally, Shawn Xu, Jake Garrison, Xuhai Xu, Tim Althoff, Yun Liu, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak Patel, Cecilia Mascolo, Xin Liu, Daniel McDuff, Yuzhe Yang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSensorLMä»¥è§£å†³å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯ç©¿æˆ´ä¼ æ„Ÿå™¨` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¤šæ¨¡æ€å­¦ä¹ ` `æ•°æ®é›†æ„å»º` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°†å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ä¸è‡ªç„¶è¯­è¨€å¯¹é½æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹ä¸°å¯Œçš„æ³¨é‡Šæ•°æ®ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ†å±‚çš„æ ‡é¢˜ç”Ÿæˆç®¡é“ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¼ æ„Ÿå™¨æ•°æ®çš„å¤šç»´ä¿¡æ¯ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSensorLMåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„é›¶-shotå’Œå°‘-shotå­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†SensorLMï¼Œä¸€ä¸ªä¼ æ„Ÿå™¨è¯­è¨€åŸºç¡€æ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€ç†è§£å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ã€‚å°½ç®¡å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®å¹¿æ³›å­˜åœ¨ï¼Œä½†ç”±äºç¼ºä¹é…å¯¹çš„ã€ä¸°å¯Œæ³¨é‡Šçš„ä¼ æ„Ÿå™¨-æ–‡æœ¬æè¿°ï¼Œä¼ æ„Ÿå™¨æ•°æ®ä¸è¯­è¨€çš„å¯¹é½å’Œè§£é‡Šä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å±‚çš„æ ‡é¢˜ç”Ÿæˆç®¡é“ï¼Œæ—¨åœ¨æ•æ‰ä¼ æ„Ÿå™¨æ•°æ®çš„ç»Ÿè®¡ã€ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚è¿™ä¸€æ–¹æ³•ä¿ƒæˆäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¼ æ„Ÿå™¨è¯­è¨€æ•°æ®é›†çš„æ•´ç†ï¼Œæ¶µç›–è¶…è¿‡5970ä¸‡å°æ—¶çš„æ•°æ®ï¼Œæ¥è‡ªè¶…è¿‡10.3ä¸‡äººçš„æ•°æ®ã€‚æ­¤å¤–ï¼ŒSensorLMæ‰©å±•äº†æ˜¾è‘—çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¶æ„ï¼ˆå¦‚CLIPã€CoCaï¼‰ï¼Œå¹¶åœ¨é€šç”¨æ¶æ„ä¸­æ¢å¤ä¸ºç‰¹å®šå˜ä½“ã€‚å¤§é‡åœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„å®éªŒéªŒè¯äº†SensorLMåœ¨é›¶-shotè¯†åˆ«ã€å°‘-shotå­¦ä¹ å’Œè·¨æ¨¡æ€æ£€ç´¢æ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ä¸è‡ªç„¶è¯­è¨€ä¹‹é—´çš„å¯¹é½å’Œè§£é‡Šé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç”±äºç¼ºä¹ä¸°å¯Œçš„é…å¯¹æ•°æ®ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†å±‚çš„æ ‡é¢˜ç”Ÿæˆç®¡é“ï¼Œæ•æ‰ä¼ æ„Ÿå™¨æ•°æ®çš„ç»Ÿè®¡ã€ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬æè¿°ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆæå‡æ•°æ®çš„å¯ç†è§£æ€§å’Œå¯ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€åˆ†å±‚æ ‡é¢˜ç”Ÿæˆã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é‡‡é›†é˜¶æ®µæ•´åˆäº†æ¥è‡ªä¸åŒå¯ç©¿æˆ´è®¾å¤‡çš„å¤§é‡ä¼ æ„Ÿå™¨æ•°æ®ï¼Œæ ‡é¢˜ç”Ÿæˆæ¨¡å—åˆ™è´Ÿè´£å°†ä¼ æ„Ÿå™¨æ•°æ®è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¼ æ„Ÿå™¨è¯­è¨€æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†åˆ†å±‚çš„æ ‡é¢˜ç”Ÿæˆæ–¹æ³•ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶èƒ½å¤Ÿå¤„ç†æœªæ ‡æ³¨çš„çœŸå®ä¸–ç•Œæ•°æ®ï¼Œæå‡äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å…ˆè¿›çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¶æ„ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚æŸå¤±å‡½æ•°çš„é€‰æ‹©å’Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°è¾¾åˆ°æœ€ä½³ã€‚å…·ä½“ç»†èŠ‚åŒ…æ‹¬ä½¿ç”¨äº†CLIPå’ŒCoCaç­‰æ¶æ„çš„å˜ä½“ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¤šæ¨¡æ€å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSensorLMåœ¨é›¶-shotè¯†åˆ«ä»»åŠ¡ä¸­ç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯æå‡äº†çº¦15%çš„å‡†ç¡®ç‡ï¼Œåœ¨å°‘-shotå­¦ä¹ å’Œè·¨æ¨¡æ€æ£€ç´¢æ–¹é¢ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¥åº·ç›‘æµ‹ã€è¿åŠ¨åˆ†æå’Œæ™ºèƒ½å®¶å±…ç­‰ã€‚é€šè¿‡æå‡å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®çš„ç†è§£èƒ½åŠ›ï¼ŒSensorLMèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ç²¾å‡†çš„å¥åº·åé¦ˆå’Œä¸ªæ€§åŒ–å»ºè®®ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present SensorLM, a family of sensor-language foundation models that enable wearable sensor data understanding with natural language. Despite its pervasive nature, aligning and interpreting sensor data with language remains challenging due to the lack of paired, richly annotated sensor-text descriptions in uncurated, real-world wearable data. We introduce a hierarchical caption generation pipeline designed to capture statistical, structural, and semantic information from sensor data. This approach enabled the curation of the largest sensor-language dataset to date, comprising over 59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and recovers them as specific variants within a generic architecture. Extensive experiments on real-world tasks in human activity analysis and healthcare verify the superior performance of SensorLM over state-of-the-art in zero-shot recognition, few-shot learning, and cross-modal retrieval. SensorLM also demonstrates intriguing capabilities including scaling behaviors, label efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

