---
layout: default
title: MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning
---

# MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.08460" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.08460v2</a>
  <a href="https://arxiv.org/pdf/2506.08460.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.08460v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.08460v2', 'MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yihong Guo, Yu Yang, Pan Xu, Anqi Liu

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-10 (Êõ¥Êñ∞: 2025-10-17)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MOBODY‰ª•Ëß£ÂÜ≥Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÂä®ÊÄÅ‰∏çÂåπÈÖçÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†` `Âä®ÊÄÅ‰∏çÂåπÈÖç` `Ê®°ÂûãÂü∫ÊñπÊ≥ï` `Á≠ñÁï•‰ºòÂåñ` `Êú∫Âô®‰∫∫ÊéßÂà∂` `MuJoCo` `Adroit`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Âä®ÊÄÅÂÅèÁßªÊòæËëóÊó∂ÔºåÂæÄÂæÄÊó†Ê≥ïÊúâÊïàÊé¢Á¥¢È´òÂ•ñÂä±Áä∂ÊÄÅÔºåÂØºËá¥Á≠ñÁï•‰ºòÂåñÂèóÈôê„ÄÇ
2. MOBODYÈÄöËøáÂ≠¶‰π†ÁõÆÊ†áÂä®ÊÄÅËøáÊ∏°ÔºåÂà©Áî®Áã¨Á´ãÁöÑÂä®‰ΩúÁºñÁ†ÅÂô®Êù•Â§ÑÁêÜ‰∏çÂêåÈ¢ÜÂüüÁöÑÂä®‰ΩúÔºåÂ¢ûÂº∫Êé¢Á¥¢ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMOBODYÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂ∞§ÂÖ∂Âú®Â§çÊùÇÂú∫ÊôØ‰∏≠ÊèêÂçáÊòæËëó„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êàë‰ª¨Á†îÁ©∂‰∫ÜÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÁ¶ªÁ∫øÂä®ÊÄÅ‰∏çÂåπÈÖçÈóÆÈ¢òÔºåÁõÆÊ†áÊòØ‰ªéÁ¶ªÁ∫øÊ∫êÊï∞ÊçÆÂíåÊúâÈôêÁõÆÊ†áÊï∞ÊçÆÈõÜ‰∏≠Â≠¶‰π†Á≠ñÁï•„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöËøáÊÉ©ÁΩöÂ•ñÂä±Êàñ‰∏¢ÂºÉÈ´òÂä®ÊÄÅÂÅèÁßªÂå∫ÂüüÁöÑÊ∫êËøáÊ∏°ÔºåÈôêÂà∂‰∫ÜÂØπÈ´òÂ•ñÂä±Áä∂ÊÄÅÁöÑÊé¢Á¥¢„ÄÇ‰∏∫ÂÖãÊúçËøô‰∏ÄÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMOBODYÔºå‰∏ÄÁßçÂü∫‰∫éÊ®°ÂûãÁöÑÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÂà©Áî®Â≠¶‰π†Âà∞ÁöÑÁõÆÊ†áÂä®ÊÄÅËøáÊ∏°Êù•Êé¢Á¥¢ÁõÆÊ†áÈ¢ÜÂüü„ÄÇMOBODYÈááÁî®ÈíàÂØπÊØè‰∏™È¢ÜÂüüÁöÑÁã¨Á´ãÂä®‰ΩúÁºñÁ†ÅÂô®Ôºå‰ºòÂåñÁ≠ñÁï•Êó∂ÂºïÂÖ•ÁõÆÊ†áQÂä†ÊùÉË°å‰∏∫ÂÖãÈöÜÊçüÂ§±ÔºåÈÅøÂÖçÂàÜÂ∏ÉÂ§ñÂä®‰Ωú„ÄÇÊàë‰ª¨Âú®MuJoCoÂíåAdroitÂü∫ÂáÜ‰∏äËØÑ‰º∞MOBODYÔºåÁªìÊûúÊòæÁ§∫ÂÖ∂‰ºò‰∫éÁé∞ÊúâÁöÑÁ¶ªÁ∫øÂä®ÊÄÅÂº∫ÂåñÂ≠¶‰π†Âü∫Á∫øÔºåÂ∞§ÂÖ∂Âú®Â§çÊùÇÂú∫ÊôØ‰∏≠Ë°®Áé∞Á™ÅÂá∫„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáË¶ÅËß£ÂÜ≥ÁöÑÂÖ∑‰ΩìÈóÆÈ¢òÊòØÂ¶Ç‰ΩïÂú®Âä®ÊÄÅ‰∏çÂåπÈÖçÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÊúâÊïàÁöÑÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöËøáÊÉ©ÁΩöÊàñ‰∏¢ÂºÉÈ´òÂä®ÊÄÅÂÅèÁßªÂå∫ÂüüÁöÑÊ∫êËøáÊ∏°ÔºåÈôêÂà∂‰∫ÜÂØπÈ´òÂ•ñÂä±Áä∂ÊÄÅÁöÑÊé¢Á¥¢ÔºåÂØºËá¥Á≠ñÁï•‰ºòÂåñÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMOBODYÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â≠¶‰π†Âà∞ÁöÑÁõÆÊ†áÂä®ÊÄÅËøáÊ∏°Êù•‰ºòÂåñÁ≠ñÁï•ÔºåËÄå‰∏ç‰ªÖ‰ªÖ‰æùËµñ‰∫é‰ΩéÂä®ÊÄÅÂÅèÁßªÁöÑËøáÊ∏°„ÄÇÈÄöËøá‰∏∫ÊØè‰∏™È¢ÜÂüüËÆæËÆ°Áã¨Á´ãÁöÑÂä®‰ΩúÁºñÁ†ÅÂô®ÔºåMOBODYËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜ‰∏çÂêåÈ¢ÜÂüüÁöÑÂä®ÊÄÅÁâπÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMOBODYÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Áä∂ÊÄÅË°®Á§∫ÁöÑÁªü‰∏ÄÂÖ±‰∫´„ÄÅÁã¨Á´ãÁöÑÂä®‰ΩúÁºñÁ†ÅÂô®ÂíåÂÖ±ÂêåÁöÑËΩ¨ÁßªÂáΩÊï∞„ÄÇÈÄöËøáËøôÁßçËÆæËÆ°ÔºåMOBODYËÉΩÂ§üÂú®‰∏çÂêåÈ¢ÜÂüü‰πãÈó¥ÊúâÊïàÂú∞ÁºñÁ†ÅÂä®‰ΩúÔºåÂπ∂‰ºòÂåñÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMOBODYÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÁõÆÊ†áQÂä†ÊùÉË°å‰∏∫ÂÖãÈöÜÊçüÂ§±ÔºåËøô‰∏ÄËÆæËÆ°‰ΩøÂæóÁ≠ñÁï•‰ºòÂåñÊõ¥ÂÄæÂêë‰∫éÈÄâÊã©È´òÁõÆÊ†áÈ¢ÜÂüüQÂÄºÁöÑÂä®‰ΩúÔºåÈÅøÂÖç‰∫ÜÂàÜÂ∏ÉÂ§ñÂä®‰ΩúÁöÑÂΩ±Âìç„ÄÇËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåMOBODYÊõ¥ÂÖ≥Ê≥®ÁõÆÊ†áÈ¢ÜÂüüÁöÑÂ•ñÂä±‰ø°Âè∑„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÖ≥ÈîÆËÆæËÆ°‰∏äÔºåMOBODYÈááÁî®‰∫ÜÁã¨Á´ãÁöÑÂä®‰ΩúÁºñÁ†ÅÂô®ÂíåÁõÆÊ†áQÂä†ÊùÉË°å‰∏∫ÂÖãÈöÜÊçüÂ§±ÂáΩÊï∞ÔºåÁ°Æ‰øù‰∫ÜÂú®Á≠ñÁï•‰ºòÂåñËøáÁ®ã‰∏≠ËÉΩÂ§üÊúâÊïàÈÅøÂÖç‰ΩéÊïàÁöÑÂä®‰ΩúÈÄâÊã©ÔºåÂêåÊó∂ÂÖ±‰∫´Áä∂ÊÄÅË°®Á§∫ÂíåËΩ¨ÁßªÂáΩÊï∞‰ª•ÊèêÈ´òÂ≠¶‰π†ÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMOBODYÂú®MuJoCoÂíåAdroitÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÁ¶ªÁ∫øÂä®ÊÄÅÂº∫ÂåñÂ≠¶‰π†Âü∫Á∫øÔºåÂ∞§ÂÖ∂Âú®Â§çÊùÇÂú∫ÊôØ‰∏≠ÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Âä®ÊÄÅ‰∏çÂåπÈÖçÊÉÖÂÜµ‰∏ãÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂ÂíåÊô∫ËÉΩÂà∂ÈÄ†Á≠âÈúÄË¶ÅÈ´òÊïàÂÜ≥Á≠ñÁöÑÂú∫ÊôØ„ÄÇMOBODYËÉΩÂ§üÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ËøõË°åÊúâÊïàÁöÑÁ≠ñÁï•Â≠¶‰π†ÔºåÊèêÂçáÁ≥ªÁªüÁöÑËá™‰∏ªÂÜ≥Á≠ñËÉΩÂäõÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We study off-dynamics offline reinforcement learning, where the goal is to learn a policy from offline source and limited target datasets with mismatched dynamics. Existing methods either penalize the reward or discard source transitions occurring in parts of the transition space with high dynamics shift. As a result, they optimize the policy using data from low-shift regions, limiting exploration of high-reward states in the target domain that do not fall within these regions. Consequently, such methods often fail when the dynamics shift is significant or the optimal trajectories lie outside the low-shift regions. To overcome this limitation, we propose MOBODY, a Model-Based Off-Dynamics Offline RL algorithm that optimizes a policy using learned target dynamics transitions to explore the target domain, rather than only being trained with the low dynamics-shift transitions. For the dynamics learning, built on the observation that achieving the same next state requires taking different actions in different domains, MOBODY employs separate action encoders for each domain to encode different actions to the shared latent space while sharing a unified representation of states and a common transition function. We further introduce a target Q-weighted behavior cloning loss in policy optimization to avoid out-of-distribution actions, which push the policy toward actions with high target-domain Q-values, rather than high source domain Q-values or uniformly imitating all actions in the offline dataset. We evaluate MOBODY on a wide range of MuJoCo and Adroit benchmarks, demonstrating that it outperforms state-of-the-art off-dynamics RL baselines as well as policy learning methods based on different dynamics learning baselines, with especially pronounced improvements in challenging scenarios where existing methods struggle.

