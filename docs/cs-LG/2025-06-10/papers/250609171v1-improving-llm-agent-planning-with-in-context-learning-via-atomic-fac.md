---
layout: default
title: Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search
---

# Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09171" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09171v1</a>
  <a href="https://arxiv.org/pdf/2506.09171.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09171v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09171v1', 'Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Samuel Holt, Max Ruiz Luyten, Thomas Pouplin, Mihaela van der Schaar

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10

**å¤‡æ³¨**: 9-page main paper, 1 figure. Accepted for an Oral presentation at the First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåŸå­äº‹å®å¢å¼ºå’Œå‰ç»æœç´¢çš„LLMä»£ç†è§„åˆ’æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åŸå­äº‹å®` `å‰ç»æœç´¢` `åœ¨çº¿å­¦ä¹ ` `å†³ç­–ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMæ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­éœ€è¦å¤§é‡çš„äº¤äº’å†å²ï¼Œéš¾ä»¥é€‚åº”æ–°ä¿¡æ¯å’Œè¿›è¡Œæœ‰æ•ˆçš„å¤šæ­¥æ¨ç†ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶é€šè¿‡åŸå­äº‹å®å¢å¼ºå’Œé€’å½’å‰ç»æœç´¢ï¼Œæå‡äº†LLMçš„è§„åˆ’èƒ½åŠ›ï¼Œæ”¯æŒåœ¨çº¿å­¦ä¹ å’Œå†³ç­–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»£ç†åœ¨TextFrozenLakeå’ŒALFWorldç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¥½çš„é€‚åº”æ€§å’Œä¼˜åŒ–è¡Œä¸ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡æŒ‡å¯¼æˆ–äº¤äº’å†å²ã€‚ç°æœ‰æ–¹æ³•åœ¨é€‚åº”æ–°ä¿¡æ¯å’Œæœ‰æ•ˆåˆ©ç”¨è¿‡å»ç»éªŒè¿›è¡Œå¤šæ­¥æ¨ç†æ—¶å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„LLMä»£ç†æ¡†æ¶ï¼Œé€šè¿‡åŸå­äº‹å®å¢å¼ºå’Œé€’å½’å‰ç»æœç´¢æ¥æå‡è§„åˆ’èƒ½åŠ›ã€‚è¯¥ä»£ç†ä»äº¤äº’è½¨è¿¹ä¸­æå–ä»»åŠ¡å…³é”®çš„â€œåŸå­äº‹å®â€ï¼ŒåŠ¨æ€å¢å¼ºæä¾›ç»™LLMç»„ä»¶çš„æç¤ºï¼Œè¿›è€Œè¿›è¡Œè¡ŒåŠ¨æè®®ã€æ½œåœ¨ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿå’ŒçŠ¶æ€ä»·å€¼è¯„ä¼°ã€‚é€šè¿‡æ·±åº¦é™åˆ¶çš„å‰ç»æœç´¢ï¼ŒLLMæ¨¡æ‹Ÿæ½œåœ¨è½¨è¿¹å¹¶è¯„ä¼°å…¶ç»“æœï¼Œä»è€Œåœ¨çº¿æ”¹è¿›ç†è§£å’Œå†³ç­–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ä»£ç†åœ¨å¤æ‚äº¤äº’ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ï¼Œéšç€ç»éªŒçš„ç§¯ç´¯ï¼Œè¡Œä¸ºæ›´åŠ ä¼˜åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­å¯¹æ–°ä¿¡æ¯é€‚åº”æ€§å·®å’Œå¤šæ­¥æ¨ç†æ•ˆç‡ä½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡çš„äº¤äº’å†å²ï¼Œéš¾ä»¥å®æ—¶æ›´æ–°å’Œä¼˜åŒ–å†³ç­–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡åŸå­äº‹å®å¢å¼ºå’Œé€’å½’å‰ç»æœç´¢æ¥æå‡LLMçš„è§„åˆ’èƒ½åŠ›ã€‚é€šè¿‡æå–äº¤äº’è½¨è¿¹ä¸­çš„å…³é”®åŸå­äº‹å®ï¼ŒåŠ¨æ€å¢å¼ºæç¤ºï¼Œä»è€Œæé«˜æ¨¡å‹çš„å†³ç­–è´¨é‡å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šåŸå­äº‹å®æå–æ¨¡å—ã€LLMç»„ä»¶ï¼ˆç”¨äºè¡ŒåŠ¨æè®®å’ŒçŠ¶æ€ä»·å€¼è¯„ä¼°ï¼‰ä»¥åŠæ·±åº¦é™åˆ¶çš„å‰ç»æœç´¢æ¨¡å—ã€‚ä»£ç†é€šè¿‡è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œè¿›è¡Œåœ¨çº¿å­¦ä¹ å’Œå†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥åŸå­äº‹å®çš„åŠ¨æ€å¢å¼ºæœºåˆ¶å’Œé€’å½’å‰ç»æœç´¢ç­–ç•¥ï¼Œä½¿å¾—LLMèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œæƒé‡æ›´æ–°çš„æƒ…å†µä¸‹ï¼Œå®æ—¶æ”¹è¿›å…¶ç†è§£å’Œå†³ç­–èƒ½åŠ›ã€‚è¿™ä¸ä¼ ç»Ÿçš„ä¾èµ–äºé™æ€è®­ç»ƒçš„æ¨¡å‹å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä»£ç†é€šè¿‡æ·±åº¦é™åˆ¶çš„å‰ç»æœç´¢æ¥æ¨¡æ‹Ÿæ½œåœ¨çš„å†³ç­–è½¨è¿¹ï¼Œå¹¶åˆ©ç”¨ç´¯ç§¯çš„åŸå­äº‹å®å’Œäº¤äº’å†å²æ¥è¯„ä¼°ç»“æœã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ï¼Œæœªæ¥ç ”ç©¶å¯èƒ½ä¼šè¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›ç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„ä»£ç†åœ¨TextFrozenLakeå’ŒALFWorldç­‰ä»»åŠ¡ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œéšç€ç»éªŒçš„ç§¯ç´¯ï¼Œä»£ç†çš„è¡Œä¸ºä¼˜åŒ–ç¨‹åº¦ä¸æ–­æå‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰å¤æ‚äº¤äº’ç³»ç»Ÿã€‚é€šè¿‡æå‡LLMçš„è§„åˆ’èƒ½åŠ›ï¼Œä»£ç†èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°è¿›è¡Œå†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šæ¨åŠ¨æ›´æ™ºèƒ½çš„äº¤äº’ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly capable but often require significant guidance or extensive interaction history to perform effectively in complex, interactive environments. Existing methods may struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning. We introduce a novel LLM agent framework that enhances planning capabilities through in-context learning, facilitated by atomic fact augmentation and a recursive lookahead search. Our agent learns to extract task-critical ``atomic facts'' from its interaction trajectories. These facts dynamically augment the prompts provided to LLM-based components responsible for action proposal, latent world model simulation, and state-value estimation. Planning is performed via a depth-limited lookahead search, where the LLM simulates potential trajectories and evaluates their outcomes, guided by the accumulated facts and interaction history. This approach allows the agent to improve its understanding and decision-making online, leveraging its experience to refine its behavior without weight updates. We provide a theoretical motivation linking performance to the quality of fact-based abstraction and LLM simulation accuracy. Empirically, our agent demonstrates improved performance and adaptability on challenging interactive tasks, achieving more optimal behavior as it accumulates experience, showcased in tasks such as TextFrozenLake and ALFWorld.

