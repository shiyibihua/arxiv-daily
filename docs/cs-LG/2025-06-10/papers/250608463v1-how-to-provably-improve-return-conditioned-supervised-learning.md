---
layout: default
title: How to Provably Improve Return Conditioned Supervised Learning?
---

# How to Provably Improve Return Conditioned Supervised Learning?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.08463" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.08463v1</a>
  <a href="https://arxiv.org/pdf/2506.08463.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.08463v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.08463v1', 'How to Provably Improve Return Conditioned Supervised Learning?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhishuai Liu, Yu Yang, Ruhan Wang, Pan Xu, Dongruo Zhou

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10

**å¤‡æ³¨**: 25 pages, 4 figures, 12 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼ºåŒ–å›æŠ¥æ¡ä»¶ç›‘ç£å­¦ä¹ ä»¥è§£å†³ç°æœ‰æ–¹æ³•æ€§èƒ½é™åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å›æŠ¥æ¡ä»¶ç›‘ç£å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `å†³ç­–ä¼˜åŒ–` `æœºå™¨å­¦ä¹ ` `ç­–ç•¥å­¦ä¹ ` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›æŠ¥æ¡ä»¶ç›‘ç£å­¦ä¹ ï¼ˆRCSLï¼‰æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå—åˆ°ç”Ÿæˆç¦»çº¿æ•°æ®é›†çš„ç­–ç•¥è´¨é‡é™åˆ¶ï¼Œç¼ºä¹æ‹¼æ¥å±æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„å¼ºåŒ–RCSLæ¡†æ¶é€šè¿‡å¼•å…¥åˆ†å¸ƒå†…æœ€ä¼˜å›æŠ¥çš„æ¦‚å¿µï¼Œä¼˜åŒ–äº†åŸºäºçŠ¶æ€çš„æœªæ¥å›æŠ¥é¢„æµ‹ï¼Œç®€åŒ–äº†å›æŠ¥å¢å¼ºè¿‡ç¨‹ã€‚
3. å®éªŒè¯æ˜ï¼Œå¼ºåŒ–RCSLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»ŸRCSLæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å†³ç­–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åºåˆ—å†³ç­–é—®é¢˜ä¸­ï¼Œå›æŠ¥æ¡ä»¶ç›‘ç£å­¦ä¹ ï¼ˆRCSLï¼‰å› å…¶åœ¨ç°ä»£å†³ç­–ä»»åŠ¡ä¸­çš„ç®€å•æ€§å’Œç¨³å®šæ€§è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ä¸ä¼ ç»Ÿçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•ä¸åŒï¼ŒRCSLå°†ç­–ç•¥å­¦ä¹ æ¡†æ¶åŒ–ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œè¾“å…¥åŒ…æ‹¬çŠ¶æ€å’Œå›æŠ¥ã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†ç¦»çº¿RLä¸­ä¸æ—¶é—´å·®åˆ†ï¼ˆTDï¼‰å­¦ä¹ ç›¸å…³çš„ç¨³å®šæ€§é—®é¢˜ã€‚ç„¶è€Œï¼ŒRCSLè¢«æ‰¹è¯„ç¼ºä¹æ‹¼æ¥å±æ€§ï¼Œå…¶æ€§èƒ½å—åˆ°ç”Ÿæˆç¦»çº¿æ•°æ®é›†çš„ç­–ç•¥è´¨é‡çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå¼ºåŒ–RCSLçš„åŸåˆ™æ€§ç®€å•æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œåˆ†å¸ƒå†…æœ€ä¼˜å›æŠ¥â€çš„æ¦‚å¿µï¼Œåˆ©ç”¨æˆ‘ä»¬çš„ç­–ç•¥è¯†åˆ«åŸºäºå½“å‰çŠ¶æ€çš„æœ€ä½³å¯å®ç°çš„æœªæ¥å›æŠ¥ï¼Œé¿å…äº†å¤æ‚çš„å›æŠ¥å¢å¼ºæŠ€æœ¯ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œå¼ºåŒ–RCSLå¯ä»¥å§‹ç»ˆä¼˜äºæ ‡å‡†RCSLæ–¹æ³•ï¼Œå®è¯ç»“æœè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„ä¸»å¼ ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å›æŠ¥æ¡ä»¶ç›‘ç£å­¦ä¹ ï¼ˆRCSLï¼‰åœ¨æ€§èƒ½ä¸Šå—åˆ°ç”Ÿæˆç¦»çº¿æ•°æ®é›†çš„ç­–ç•¥è´¨é‡é™åˆ¶çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹æ‹¼æ¥å±æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„å¼ºåŒ–RCSLæ¡†æ¶é€šè¿‡å¼•å…¥â€œåˆ†å¸ƒå†…æœ€ä¼˜å›æŠ¥â€æ¦‚å¿µï¼Œåˆ©ç”¨å½“å‰ç­–ç•¥è¯†åˆ«æœ€ä½³å¯å®ç°çš„æœªæ¥å›æŠ¥ï¼Œä»è€Œé¿å…å¤æ‚çš„å›æŠ¥å¢å¼ºæŠ€æœ¯ï¼Œæå‡å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç­–ç•¥å­¦ä¹ å’Œå›æŠ¥ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡ç°æœ‰ç­–ç•¥ç”Ÿæˆç¦»çº¿æ•°æ®é›†ï¼›ç„¶åï¼Œåˆ©ç”¨å¼ºåŒ–RCSLæ¡†æ¶è¿›è¡Œç­–ç•¥å­¦ä¹ ï¼›æœ€åï¼Œä¼˜åŒ–å›æŠ¥é¢„æµ‹ä»¥æé«˜å†³ç­–è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯å¼•å…¥â€œåˆ†å¸ƒå†…æœ€ä¼˜å›æŠ¥â€æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä½¿å¾—ç­–ç•¥èƒ½å¤Ÿåœ¨å½“å‰çŠ¶æ€ä¸‹è¯†åˆ«æœ€ä½³æœªæ¥å›æŠ¥ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å›æŠ¥é¢„æµ‹ï¼Œå¹¶é€šè¿‡ç½‘ç»œç»“æ„çš„è°ƒæ•´æ¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­éƒ½èƒ½æœ‰æ•ˆåº”ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–RCSLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºæ ‡å‡†RCSLæ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸­æ€§èƒ½æé«˜äº†20%ä»¥ä¸Šï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½æ¨èç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å†³ç­–è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ï¼Œå¼ºåŒ–RCSLèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„å†³ç­–æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In sequential decision-making problems, Return-Conditioned Supervised Learning (RCSL) has gained increasing recognition for its simplicity and stability in modern decision-making tasks. Unlike traditional offline reinforcement learning (RL) algorithms, RCSL frames policy learning as a supervised learning problem by taking both the state and return as input. This approach eliminates the instability often associated with temporal difference (TD) learning in offline RL. However, RCSL has been criticized for lacking the stitching property, meaning its performance is inherently limited by the quality of the policy used to generate the offline dataset. To address this limitation, we propose a principled and simple framework called Reinforced RCSL. The key innovation of our framework is the introduction of a concept we call the in-distribution optimal return-to-go. This mechanism leverages our policy to identify the best achievable in-dataset future return based on the current state, avoiding the need for complex return augmentation techniques. Our theoretical analysis demonstrates that Reinforced RCSL can consistently outperform the standard RCSL approach. Empirical results further validate our claims, showing significant performance improvements across a range of benchmarks.

