---
layout: default
title: From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation
---

# From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.18263" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.18263v1</a>
  <a href="https://arxiv.org/pdf/2510.18263.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.18263v1" onclick="toggleFavorite(this, '2510.18263v1', 'From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziwei Huang, Ying Shu, Hao Fang, Quanyu Long, Wenya Wang, Qiushi Guo, Tiezheng Ge, Leilei Gan

**åˆ†ç±»**: cs.LG, cs.CV, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-10-21

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCustomized-GRPOï¼Œè§£å†³ä¸»ä½“é©±åŠ¨å›¾åƒç”Ÿæˆä¸­ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§çš„trade-offé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸»ä½“é©±åŠ¨å›¾åƒç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `æ‰©æ•£æ¨¡å‹` `å¥–åŠ±å¡‘é€ ` `åŠ¨æ€åŠ æƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¸»ä½“é©±åŠ¨å›¾åƒç”Ÿæˆéœ€è¦åœ¨èº«ä»½ä¿æŒå’Œæç¤ºéµå¾ªä¹‹é—´æƒè¡¡ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾ã€‚
2. Customized-GRPOé€šè¿‡ååŒæ„ŸçŸ¥å¥–åŠ±å¡‘é€ å’Œæ—¶é—´æ„ŸçŸ¥åŠ¨æ€åŠ æƒï¼Œä¼˜åŒ–å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œåœ¨ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§ä¸Šå–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸»ä½“é©±åŠ¨çš„å›¾åƒç”Ÿæˆæ¨¡å‹é¢ä¸´ç€èº«ä»½ä¿æŒï¼ˆä¿çœŸåº¦ï¼‰å’Œæç¤ºéµå¾ªï¼ˆå¯ç¼–è¾‘æ€§ï¼‰ä¹‹é—´çš„æ ¹æœ¬æƒè¡¡ã€‚åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œç‰¹åˆ«æ˜¯GPROï¼Œä¸ºæ­¤æä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ç®€å•åœ°åº”ç”¨GRPOä¼šå¯¼è‡´ç«äº‰æ€§é€€åŒ–ï¼Œå› ä¸ºå…·æœ‰é™æ€æƒé‡çš„å¥–åŠ±çš„ç®€å•çº¿æ€§èšåˆä¼šå¯¼è‡´å†²çªçš„æ¢¯åº¦ä¿¡å·ï¼Œå¹¶ä¸æ‰©æ•£è¿‡ç¨‹çš„æ—¶é—´åŠ¨æ€ä¸ä¸€è‡´ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Customized-GRPOï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šï¼ˆiï¼‰ååŒæ„ŸçŸ¥å¥–åŠ±å¡‘é€ ï¼ˆSARSï¼‰ï¼Œä¸€ç§éçº¿æ€§æœºåˆ¶ï¼Œå®ƒæ˜ç¡®åœ°æƒ©ç½šå†²çªçš„å¥–åŠ±ä¿¡å·å¹¶æ”¾å¤§ååŒçš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæä¾›æ›´æ¸…æ™°å’Œæ›´æœæ–­çš„æ¢¯åº¦ã€‚ï¼ˆiiï¼‰æ—¶é—´æ„ŸçŸ¥åŠ¨æ€åŠ æƒï¼ˆTDWï¼‰ï¼Œå®ƒé€šè¿‡ä¼˜å…ˆè€ƒè™‘æ—©æœŸé˜¶æ®µçš„æç¤ºéµå¾ªå’ŒåæœŸé˜¶æ®µçš„èº«ä»½ä¿æŒï¼Œä½¿ä¼˜åŒ–å‹åŠ›ä¸æ¨¡å‹çš„æ—¶é—´åŠ¨æ€ä¿æŒä¸€è‡´ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜æ˜¾ä¼˜äºæœ´ç´ çš„GRPOåŸºçº¿ï¼ŒæˆåŠŸåœ°å‡è½»äº†ç«äº‰æ€§é€€åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†å“è¶Šçš„å¹³è¡¡ï¼Œç”Ÿæˆæ—¢ä¿ç•™äº†å…³é”®èº«ä»½ç‰¹å¾åˆå‡†ç¡®åœ°éµå¾ªå¤æ‚æ–‡æœ¬æç¤ºçš„å›¾åƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¸»ä½“é©±åŠ¨å›¾åƒç”Ÿæˆæ—¨åœ¨æ ¹æ®ç»™å®šçš„ä¸»ä½“å›¾åƒå’Œæ–‡æœ¬æç¤ºç”Ÿæˆæ–°çš„å›¾åƒã€‚ç°æœ‰æ–¹æ³•åœ¨èº«ä»½ä¿æŒï¼ˆä¿çœŸåº¦ï¼‰å’Œæç¤ºéµå¾ªï¼ˆå¯ç¼–è¾‘æ€§ï¼‰ä¹‹é—´å­˜åœ¨trade-offã€‚ç®€å•åœ°åº”ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚GPROï¼‰ä¼šå¯¼è‡´æ¢¯åº¦å†²çªå’Œä¸æ‰©æ•£æ¨¡å‹æ—¶é—´åŠ¨æ€çš„ä¸åŒ¹é…ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCustomized-GRPOçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å®šåˆ¶åŒ–çš„å¥–åŠ±å¡‘é€ å’ŒåŠ¨æ€æƒé‡è°ƒæ•´ï¼Œä¼˜åŒ–å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œä»è€Œåœ¨èº«ä»½ä¿æŒå’Œæç¤ºéµå¾ªä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡éçº¿æ€§æ–¹å¼å¤„ç†å¥–åŠ±ä¿¡å·ï¼Œå¹¶æ ¹æ®æ‰©æ•£è¿‡ç¨‹çš„æ—¶é—´æ­¥è°ƒæ•´ä¼˜åŒ–é‡ç‚¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCustomized-GRPOæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šååŒæ„ŸçŸ¥å¥–åŠ±å¡‘é€ ï¼ˆSARSï¼‰å’Œæ—¶é—´æ„ŸçŸ¥åŠ¨æ€åŠ æƒï¼ˆTDWï¼‰ã€‚SARSæ¨¡å—ç”¨äºå¤„ç†æ¥è‡ªä¸åŒå¥–åŠ±å‡½æ•°çš„ä¿¡å·ï¼Œé€šè¿‡éçº¿æ€§æ–¹å¼æ”¾å¤§ååŒä¿¡å·å¹¶æŠ‘åˆ¶å†²çªä¿¡å·ã€‚TDWæ¨¡å—æ ¹æ®æ‰©æ•£è¿‡ç¨‹çš„æ—¶é—´æ­¥åŠ¨æ€è°ƒæ•´èº«ä»½ä¿æŒå’Œæç¤ºéµå¾ªçš„æƒé‡ï¼Œæ—©æœŸæ›´æ³¨é‡æç¤ºéµå¾ªï¼ŒåæœŸæ›´æ³¨é‡èº«ä»½ä¿æŒã€‚

**å…³é”®åˆ›æ–°**ï¼šCustomized-GRPOçš„å…³é”®åˆ›æ–°åœ¨äºï¼š(1) æå‡ºäº†ååŒæ„ŸçŸ¥å¥–åŠ±å¡‘é€ ï¼ˆSARSï¼‰ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†æ¥è‡ªä¸åŒå¥–åŠ±å‡½æ•°çš„å†²çªä¿¡å·ï¼Œå¹¶æä¾›æ›´æ¸…æ™°çš„æ¢¯åº¦ã€‚(2) æå‡ºäº†æ—¶é—´æ„ŸçŸ¥åŠ¨æ€åŠ æƒï¼ˆTDWï¼‰ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®æ‰©æ•£è¿‡ç¨‹çš„æ—¶é—´æ­¥åŠ¨æ€è°ƒæ•´ä¼˜åŒ–é‡ç‚¹ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”æ‰©æ•£æ¨¡å‹çš„æ—¶é—´åŠ¨æ€ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCustomized-GRPOèƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡èº«ä»½ä¿æŒå’Œæç¤ºéµå¾ªã€‚

**å…³é”®è®¾è®¡**ï¼šSARSæ¨¡å—ä½¿ç”¨éçº¿æ€§å‡½æ•°æ¥å¤„ç†å¥–åŠ±ä¿¡å·ï¼Œå…·ä½“å½¢å¼æœªçŸ¥ï¼ˆè®ºæ–‡æœªæ˜ç¡®ç»™å‡ºï¼‰ã€‚TDWæ¨¡å—ä½¿ç”¨æ—¶é—´æ­¥çš„å‡½æ•°æ¥åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œå…·ä½“å½¢å¼æœªçŸ¥ï¼ˆè®ºæ–‡æœªæ˜ç¡®ç»™å‡ºï¼‰ã€‚æŸå¤±å‡½æ•°æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„æ ‡å‡†æŸå¤±å‡½æ•°ï¼Œä½†å¥–åŠ±ä¿¡å·ç»è¿‡äº†SARSå¤„ç†ï¼Œæƒé‡ç»è¿‡äº†TDWè°ƒæ•´ã€‚ç½‘ç»œç»“æ„åŸºäºç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œæ²¡æœ‰è¿›è¡Œæ˜¾è‘—ä¿®æ”¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCustomized-GRPOæ˜¾è‘—ä¼˜äºæœ´ç´ çš„GRPOåŸºçº¿ï¼Œåœ¨èº«ä»½ä¿æŒå’Œæç¤ºéµå¾ªæ–¹é¢éƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼ˆè®ºæ–‡æ‘˜è¦æœªæä¾›å…·ä½“æ•°å€¼ï¼‰ï¼Œä½†å¼ºè°ƒäº†è¯¥æ–¹æ³•æˆåŠŸå‡è½»äº†ç«äº‰æ€§é€€åŒ–ï¼Œå®ç°äº†å“è¶Šçš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå›¾åƒç¼–è¾‘ã€å†…å®¹åˆ›ä½œã€è™šæ‹Ÿäººç‰©ç”Ÿæˆç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥ä¸Šä¼ ä¸€å¼ äººè„¸ç…§ç‰‡ï¼Œå¹¶è¾“å…¥ä¸€æ®µæ–‡å­—æè¿°ï¼Œç”Ÿæˆå…·æœ‰è¯¥äººè„¸ç‰¹å¾å¹¶ç¬¦åˆæ–‡å­—æè¿°çš„æ–°å›¾åƒã€‚è¯¥æŠ€æœ¯åœ¨æ¸¸æˆå¼€å‘ã€å¹¿å‘Šè®¾è®¡ã€ç¤¾äº¤åª’ä½“ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œèƒ½å¤Ÿé™ä½åˆ›ä½œæˆæœ¬ï¼Œæé«˜åˆ›ä½œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Subject-driven image generation models face a fundamental trade-off between identity preservation (fidelity) and prompt adherence (editability). While online reinforcement learning (RL), specifically GPRO, offers a promising solution, we find that a naive application of GRPO leads to competitive degradation, as the simple linear aggregation of rewards with static weights causes conflicting gradient signals and a misalignment with the temporal dynamics of the diffusion process. To overcome these limitations, we propose Customized-GRPO, a novel framework featuring two key innovations: (i) Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly penalizes conflicted reward signals and amplifies synergistic ones, providing a sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW), which aligns the optimization pressure with the model's temporal dynamics by prioritizing prompt-following in the early, identity preservation in the later. Extensive experiments demonstrate that our method significantly outperforms naive GRPO baselines, successfully mitigating competitive degradation. Our model achieves a superior balance, generating images that both preserve key identity features and accurately adhere to complex textual prompts.

