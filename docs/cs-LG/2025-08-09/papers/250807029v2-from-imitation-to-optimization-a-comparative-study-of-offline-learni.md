---
layout: default
title: From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving
---

# From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07029" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07029v2</a>
  <a href="https://arxiv.org/pdf/2508.07029.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07029v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07029v2', 'From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Antonio Guillen-Perez

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-08-09 (æ›´æ–°: 2025-08-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„æ¨¡ä»¿å­¦ä¹ å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `æ¨¡ä»¿å­¦ä¹ ` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `è¡Œä¸ºå…‹éš†` `ä¿å®ˆQå­¦ä¹ ` `é•¿æ—¶é—´ç­–ç•¥` `ç¨³å¥æ€§` `æ•°æ®é©±åŠ¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚è¡Œä¸ºå…‹éš†ï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å­˜åœ¨è„†å¼±æ€§ï¼Œå®¹æ˜“å—åˆ°ç´¯ç§¯è¯¯å·®çš„å½±å“ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¿å®ˆQå­¦ä¹ çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é©¾é©¶ç­–ç•¥çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ—¶é—´æ¨¡æ‹Ÿä¸­ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCQLä»£ç†åœ¨1,000ä¸ªæœªè§åœºæ™¯ä¸­æˆåŠŸç‡æé«˜äº†3.2å€ï¼Œç¢°æ’ç‡é™ä½äº†7.4å€ï¼Œæ˜¾è‘—ä¼˜äºæœ€å¼ºBCåŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»å¤§è§„æ¨¡ç°å®æ•°æ®é›†ä¸­å­¦ä¹ ç¨³å¥çš„é©¾é©¶ç­–ç•¥æ˜¯è‡ªåŠ¨é©¾é©¶ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨çº¿æ•°æ®æ”¶é›†å¾€å¾€ä¸å®‰å…¨ä¸”ä¸åˆ‡å®é™…ã€‚å°½ç®¡è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰æä¾›äº†ä¸€ç§ç›´æ¥çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œä½†ä½¿ç”¨BCè®­ç»ƒçš„ç­–ç•¥é€šå¸¸è„†å¼±ï¼Œå¹¶åœ¨é—­ç¯æ‰§è¡Œä¸­é­å—ç´¯ç§¯è¯¯å·®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€å¥—ç»¼åˆç®¡é“å’Œæ¯”è¾ƒç ”ç©¶ï¼Œé¦–å…ˆå¼€å‘äº†ä¸€ç³»åˆ—æ—¥ç›Šå¤æ‚çš„BCåŸºçº¿ï¼Œæœ€ç»ˆå½¢æˆäº†åŸºäºTransformerçš„æ¨¡å‹ï¼Œé‡‡ç”¨ç»“æ„åŒ–çš„å®ä½“ä¸­å¿ƒçŠ¶æ€è¡¨ç¤ºã€‚å°½ç®¡è¯¥æ¨¡å‹åœ¨æ¨¡ä»¿æŸå¤±ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é•¿æ—¶é—´æ¨¡æ‹Ÿä¸­ä»ç„¶å¤±è´¥ã€‚é€šè¿‡å°†æœ€å…ˆè¿›çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰åº”ç”¨äºç›¸åŒçš„æ•°æ®å’Œæ¶æ„ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå­¦ä¹ åˆ°æ˜¾è‘—æ›´ç¨³å¥çš„ç­–ç•¥ã€‚åœ¨å¯¹1,000ä¸ªæ¥è‡ªWaymoå¼€æ”¾è¿åŠ¨æ•°æ®é›†çš„æœªè§åœºæ™¯è¿›è¡Œçš„å¤§è§„æ¨¡è¯„ä¼°ä¸­ï¼Œæœ€ç»ˆçš„CQLä»£ç†å®ç°äº†3.2å€çš„æˆåŠŸç‡æå‡å’Œ7.4å€çš„ç¢°æ’ç‡é™ä½ï¼Œè¯æ˜äº†ç¦»çº¿RLæ–¹æ³•åœ¨ä»é™æ€ä¸“å®¶æ•°æ®ä¸­å­¦ä¹ ç¨³å¥çš„é•¿æ—¶é—´é©¾é©¶ç­–ç•¥ä¸­çš„å…³é”®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚è¡Œä¸ºå…‹éš†ï¼‰åœ¨é•¿æ—¶é—´æ‰§è¡Œä¸­çš„è„†å¼±æ€§å’Œç´¯ç§¯è¯¯å·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥ä¿è¯å®‰å…¨æ€§å’Œå¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰ç®—æ³•ï¼Œåˆ©ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ æ›´ç¨³å¥çš„é©¾é©¶ç­–ç•¥ã€‚CQLèƒ½å¤Ÿåœ¨é¢å¯¹å°é”™è¯¯æ—¶æ¢å¤å¹¶é¿å…åˆ†å¸ƒå¤–çŠ¶æ€ï¼Œä»è€Œæé«˜ç­–ç•¥çš„å¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªé˜¶æ®µï¼šé¦–å…ˆå¼€å‘ä¸€ç³»åˆ—BCåŸºçº¿æ¨¡å‹ï¼Œæœ€åå¼•å…¥åŸºäºTransformerçš„æ¨¡å‹ï¼Œé‡‡ç”¨ç»“æ„åŒ–çš„çŠ¶æ€è¡¨ç¤ºã€‚éšåï¼Œåˆ©ç”¨CQLç®—æ³•å¯¹ç›¸åŒæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†CQLåº”ç”¨äºè‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå±•ç¤ºäº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨æ¨¡ä»¿å­¦ä¹ ä¸­çš„ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ä¼ ç»ŸBCæ–¹æ³•ç›¸æ¯”ï¼ŒCQLèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç´¯ç§¯è¯¯å·®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç»“æ„åŒ–çš„å®ä½“ä¸­å¿ƒçŠ¶æ€è¡¨ç¤ºï¼Œå¹¶ç²¾å¿ƒè®¾è®¡äº†å¥–åŠ±å‡½æ•°ï¼Œä»¥å¼•å¯¼CQLä»£ç†å­¦ä¹ ä¿å®ˆçš„ä»·å€¼å‡½æ•°ã€‚æ¨¡å‹çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥é€‚åº”å¤æ‚çš„é©¾é©¶åœºæ™¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ç»ˆçš„CQLä»£ç†åœ¨1,000ä¸ªæœªè§åœºæ™¯ä¸­å®ç°äº†3.2å€çš„æˆåŠŸç‡æå‡å’Œ7.4å€çš„ç¢°æ’ç‡é™ä½ï¼Œç›¸è¾ƒäºæœ€å¼ºçš„BCåŸºçº¿ï¼Œè¯æ˜äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨å­¦ä¹ ç¨³å¥é©¾é©¶ç­–ç•¥ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æé«˜é©¾é©¶ç­–ç•¥çš„ç¨³å¥æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„åº”ç”¨å’Œæ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning robust driving policies from large-scale, real-world datasets is a central challenge in autonomous driving, as online data collection is often unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward approach to imitation learning, policies trained with BC are notoriously brittle and suffer from compounding errors in closed-loop execution. This work presents a comprehensive pipeline and a comparative study to address this limitation. We first develop a series of increasingly sophisticated BC baselines, culminating in a Transformer-based model that operates on a structured, entity-centric state representation. While this model achieves low imitation loss, we show that it still fails in long-horizon simulations. We then demonstrate that by applying a state-of-the-art Offline Reinforcement Learning algorithm, Conservative Q-Learning (CQL), to the same data and architecture, we can learn a significantly more robust policy. Using a carefully engineered reward function, the CQL agent learns a conservative value function that enables it to recover from minor errors and avoid out-of-distribution states. In a large-scale evaluation on 1,000 unseen scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a 3.2x higher success rate and a 7.4x lower collision rate than the strongest BC baseline, proving that an offline RL approach is critical for learning robust, long-horizon driving policies from static expert data.

