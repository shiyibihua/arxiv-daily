---
layout: default
title: AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance
---

# AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06944" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.06944v3</a>
  <a href="https://arxiv.org/pdf/2508.06944.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06944v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06944v3', 'AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Lixuan He, Jie Feng, Yong Li

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CL, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-09 (Êõ¥Êñ∞: 2025-10-10)

**Â§áÊ≥®**: The paper is currently under investigation regarding concerns of potential academic misconduct. While the investigation is ongoing, the authors have voluntarily requested to withdraw the manuscript

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/hlxtsyj/AMFT)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫AMFT‰ª•Ëß£ÂÜ≥LLMÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊ®°‰ªø‰∏éÊé¢Á¥¢Âπ≥Ë°°ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜ‰ªªÂä°` `Ëá™ÈÄÇÂ∫îÂÖÉÂæÆË∞É` `Âº∫ÂåñÂ≠¶‰π†` `ÁõëÁù£ÂæÆË∞É` `ÈöêÂºèÂ•ñÂä±` `Âä®ÊÄÅ‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑLLMÊé®ÁêÜ‰ªªÂä°ÂæÆË∞ÉÊñπÊ≥ïÈù¢‰∏¥ÁÅæÈöæÊÄßÈÅóÂøòÂíåÊ®°‰ªø‰∏éÊé¢Á¥¢‰πãÈó¥ÁöÑÊ¨°‰ºòÊùÉË°°ÔºåÂΩ±ÂìçÊ®°ÂûãÊÄßËÉΩ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Ëá™ÈÄÇÂ∫îÂÖÉÂæÆË∞ÉÔºàAMFTÔºâÔºåÈÄöËøáÂä®ÊÄÅ‰ºòÂåñSFTÂíåRLÁöÑÂπ≥Ë°°ÔºåÊèêÂçáÊ®°ÂûãÁöÑÈïøÊúü‰ªªÂä°Ë°®Áé∞„ÄÇ
3. AMFTÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂª∫Á´ã‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºåÂπ∂Âú®ÂàÜÂ∏ÉÂ§ñ‰ªªÂä°‰∏äÂ±ïÁé∞‰∫ÜÊõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈÄöÂ∏∏ÈÄöËøáÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁöÑ‰∏§Èò∂ÊÆµÊµÅÁ®ãËøõË°åÊé®ÁêÜ‰ªªÂä°ÁöÑÂæÆË∞ÉÔºå‰ΩÜËøô‰∏ÄËøáÁ®ãÈù¢‰∏¥ÁÅæÈöæÊÄßÈÅóÂøòÂíåÊ®°‰ªø‰∏éÊé¢Á¥¢‰πãÈó¥ÁöÑÊ¨°‰ºòÊùÉË°°„ÄÇËøëÊúüÁöÑÂçïÈò∂ÊÆµÊñπÊ≥ïÂ∞ùËØïÈÄöËøáÂêØÂèëÂºèÊñπÊ≥ïÁªü‰∏ÄSFTÂíåRLÔºå‰ΩÜÁº∫‰πèÂä®ÊÄÅÂπ≥Ë°°Ëøô‰∏§ÁßçËåÉÂºèÁöÑÂéüÂàôÊú∫Âà∂„ÄÇÊú¨ÊñáÈÄöËøáÈöêÂºèÂ•ñÂä±ÁöÑÁêÜËÆ∫ËßÜËßíÈáçÊñ∞ÂÆ°ËßÜËøô‰∏ÄÊåëÊàòÔºåÊèêÂá∫‰∫ÜËá™ÈÄÇÂ∫îÂÖÉÂæÆË∞ÉÔºàAMFTÔºâÔºå‰∏ÄÁßçÊñ∞È¢ñÁöÑÂçïÈò∂ÊÆµÁÆóÊ≥ïÔºåÂ≠¶‰π†SFTÁöÑÈöêÂºèË∑ØÂæÑÁ∫ßÂ•ñÂä±‰∏éRLÁöÑÊòæÂºèÁªìÊûúÂØºÂêëÂ•ñÂä±‰πãÈó¥ÁöÑÊúÄ‰Ω≥Âπ≥Ë°°„ÄÇAMFTÁöÑÊ†∏ÂøÉÊòØ‰∏Ä‰∏™ÂÖÉÊ¢ØÂ∫¶Ëá™ÈÄÇÂ∫îÊùÉÈáçÊéßÂà∂Âô®ÔºåÂ∞ÜSFT-RLÂπ≥Ë°°ËßÜ‰∏∫ÂèØÂ≠¶‰π†ÂèÇÊï∞ÔºåÂä®ÊÄÅ‰ºòÂåñ‰ª•ÊúÄÂ§ßÂåñÈïøÊúü‰ªªÂä°ÊÄßËÉΩ„ÄÇÊàë‰ª¨Âú®Êï∞Â≠¶Êé®ÁêÜ„ÄÅÊäΩË±°ËßÜËßâÊé®ÁêÜÂíåËßÜËßâ-ËØ≠Ë®ÄÂØºËà™Á≠âÊåëÊàòÊÄßÂü∫ÂáÜ‰∏äËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåAMFTÂßãÁªàÂª∫Á´ã‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºåÂπ∂Âú®ÂàÜÂ∏ÉÂ§ñ‰ªªÂä°‰∏äË°®Áé∞Âá∫‰ºòË∂äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠Ê®°‰ªø‰∏éÊé¢Á¥¢‰πãÈó¥ÁöÑÂπ≥Ë°°ÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Ëøô‰∏ÄËøáÁ®ã‰∏≠ÂÆπÊòìÂØºËá¥ÁÅæÈöæÊÄßÈÅóÂøòÂíåÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAMFTÈÄöËøáÂ∞ÜSFTÂíåRLËßÜ‰∏∫‰∫íË°•ÁöÑÂ•ñÂä±‰ø°Âè∑ÔºåÂà©Áî®ÈöêÂºèÂ•ñÂä±ÁöÑÁêÜËÆ∫Ê°ÜÊû∂ÔºåÂä®ÊÄÅË∞ÉÊï¥‰∏§ËÄÖÁöÑÊùÉÈáçÔºå‰ª•‰ºòÂåñÊ®°ÂûãÁöÑÈïøÊúüË°®Áé∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAMFTÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™ÂÖÉÊ¢ØÂ∫¶Ëá™ÈÄÇÂ∫îÊùÉÈáçÊéßÂà∂Âô®ÔºåËØ•ÊéßÂà∂Âô®Ê†πÊçÆ‰ªªÂä°ÈúÄÊ±ÇÂä®ÊÄÅË∞ÉÊï¥SFTÂíåRLÁöÑÂπ≥Ë°°ÔºåÁªìÂêàÊîøÁ≠ñÁÜµÊ≠£ÂàôÂåñ‰ª•Á°Æ‰øùËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAMFTÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜSFTÂíåRLÁöÑÂπ≥Ë°°ËßÜ‰∏∫ÂèØÂ≠¶‰π†ÂèÇÊï∞ÔºåÈÄöËøáÂÖÉÂ≠¶‰π†Êú∫Âà∂ÂÆûÁé∞Âä®ÊÄÅ‰ºòÂåñÔºåËøô‰∏é‰º†ÁªüÁöÑÂõ∫ÂÆöÊùÉÈáçÊñπÊ≥ïÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®AMFT‰∏≠ÔºåÊçüÂ§±ÂáΩÊï∞ÁªìÂêà‰∫ÜSFTÁöÑÈöêÂºèÂ•ñÂä±ÂíåRLÁöÑÊòæÂºèÂ•ñÂä±ÔºåÁΩëÁªúÁªìÊûÑÈááÁî®‰∫ÜÈÄÇÂ∫îÊÄßÊùÉÈáçÊéßÂà∂Âô®ÔºåÁ°Æ‰øùÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ËÉΩÂ§üÊúâÊïàÂú∞Ë∞ÉÊï¥Ê®°‰ªø‰∏éÊé¢Á¥¢ÁöÑÊØîÈáç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

AMFTÂú®Êï∞Â≠¶Êé®ÁêÜ„ÄÅÊäΩË±°ËßÜËßâÊé®ÁêÜÂíåËßÜËßâ-ËØ≠Ë®ÄÂØºËà™Á≠âÂü∫ÂáÜÊµãËØï‰∏≠ÂùáÂèñÂæó‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºåÁâπÂà´ÊòØÂú®ÂàÜÂ∏ÉÂ§ñ‰ªªÂä°‰∏äÂ±ïÁé∞Âá∫ÊòæËëóÁöÑÊ≥õÂåñËÉΩÂäõÔºåÊèêÂçáÂπÖÂ∫¶Ë∂ÖËøá‰∫ÜÁé∞ÊúâÊñπÊ≥ïÁöÑ20%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊô∫ËÉΩÂØπËØùÁ≥ªÁªüÂíåËá™Âä®Êé®ÁêÜÁ≠â„ÄÇÈÄöËøá‰ºòÂåñLLMÁöÑÊé®ÁêÜËÉΩÂäõÔºåAMFTËÉΩÂ§üÊèêÂçáÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of \textbf{implicit rewards}, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment. Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

