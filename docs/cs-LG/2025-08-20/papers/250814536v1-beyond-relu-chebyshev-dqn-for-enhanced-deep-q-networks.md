---
layout: default
title: Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks
---

# Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14536" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14536v1</a>
  <a href="https://arxiv.org/pdf/2508.14536.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14536v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14536v1', 'Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Saman Yazdannik, Morteza Tayefi, Shamim Sanisales

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºChebyshev-DQNä»¥æå‡æ·±åº¦Qç½‘ç»œæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦Qç½‘ç»œ` `åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼` `å¼ºåŒ–å­¦ä¹ ` `å‡½æ•°é€¼è¿‘` `æ¨¡å‹å¤æ‚æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ·±åº¦Qç½‘ç»œåœ¨å¤æ‚ä»·å€¼å‡½æ•°çš„è¿‘ä¼¼ä¸Šå­˜åœ¨å›°éš¾ï¼Œå½±å“äº†å…¶å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºçš„Chebyshev-DQNé€šè¿‡å¼•å…¥åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼åŸºï¼Œæ”¹å–„äº†ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œä»è€Œæå‡å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCh-DQNåœ¨CartPole-v1åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½æå‡çº¦39%ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰çš„æ€§èƒ½ä¾èµ–äºå…¶ç¥ç»ç½‘ç»œå‡†ç¡®è¿‘ä¼¼åŠ¨ä½œä»·å€¼å‡½æ•°çš„èƒ½åŠ›ã€‚æ ‡å‡†çš„å‡½æ•°é€¼è¿‘å™¨ï¼Œå¦‚å¤šå±‚æ„ŸçŸ¥æœºï¼Œå¯èƒ½éš¾ä»¥æœ‰æ•ˆè¡¨ç¤ºè®¸å¤šå¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸­å¤æ‚çš„ä»·å€¼æ™¯è§‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¶æ„â€”â€”Chebyshev-DQNï¼ˆCh-DQNï¼‰ï¼Œå°†åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼åŸºèå…¥DQNæ¡†æ¶ï¼Œä»¥åˆ›å»ºæ›´æœ‰æ•ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡åˆ©ç”¨åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼å¼ºå¤§çš„å‡½æ•°é€¼è¿‘ç‰¹æ€§ï¼Œæˆ‘ä»¬å‡è®¾Ch-DQNèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°å­¦ä¹ å¹¶å®ç°æ›´é«˜çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨CartPole-v1åŸºå‡†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ¨¡å‹ï¼Œå¹¶ä¸å…·æœ‰ç›¸ä¼¼å‚æ•°æ•°é‡çš„æ ‡å‡†DQNè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œå…·æœ‰é€‚ä¸­å¤šé¡¹å¼åº¦æ•°ï¼ˆN=4ï¼‰çš„Ch-DQNåœ¨æ¸è¿‘æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œæå‡çº¦39%ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿå‘ç°å¤šé¡¹å¼åº¦æ•°çš„é€‰æ‹©æ˜¯ä¸€ä¸ªå…³é”®è¶…å‚æ•°ï¼Œè¾ƒé«˜çš„åº¦æ•°ï¼ˆN=8ï¼‰å¯èƒ½å¯¹å­¦ä¹ äº§ç”Ÿä¸åˆ©å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦Qç½‘ç»œåœ¨å¤æ‚å¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸­å¯¹åŠ¨ä½œä»·å€¼å‡½æ•°è¿‘ä¼¼èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„å¤šå±‚æ„ŸçŸ¥æœºç­‰æ ‡å‡†å‡½æ•°é€¼è¿‘å™¨åœ¨è¡¨ç¤ºå¤æ‚ä»·å€¼æ™¯è§‚æ—¶å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Chebyshev-DQNé€šè¿‡å¼•å…¥åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼åŸºï¼Œåˆ©ç”¨å…¶ä¼˜è¶Šçš„å‡½æ•°é€¼è¿‘ç‰¹æ€§ï¼Œæ¥å¢å¼ºDQNçš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCh-DQNçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å±‚ã€åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼åŸºå±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ã€‚é€šè¿‡åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼çš„çº¿æ€§ç»„åˆï¼Œç½‘ç»œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰å¤æ‚çš„ä»·å€¼å‡½æ•°ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼åŸºå¼•å…¥æ·±åº¦Qç½‘ç»œä¸­ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—ç½‘ç»œåœ¨å¤„ç†å¤æ‚çš„ä»·å€¼å‡½æ•°æ—¶å…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé€‰æ‹©äº†é€‚ä¸­çš„å¤šé¡¹å¼åº¦æ•°ï¼ˆN=4ï¼‰ä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ä¹Ÿæ¢è®¨äº†é«˜å¤šé¡¹å¼åº¦æ•°ï¼ˆN=8ï¼‰å¯¹å­¦ä¹ çš„è´Ÿé¢å½±å“ï¼Œå¼ºè°ƒäº†è¶…å‚æ•°é€‰æ‹©çš„é‡è¦æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒChebyshev-DQNåœ¨CartPole-v1åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨N=4çš„å¤šé¡¹å¼åº¦æ•°æ—¶ï¼Œæ€§èƒ½æå‡çº¦39%ï¼Œæ˜¾è‘—ä¼˜äºæ ‡å‡†DQNã€‚è¿™ä¸€ç»“æœéªŒè¯äº†åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼åŸºåœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“å’Œè‡ªåŠ¨é©¾é©¶ç­‰å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡æå‡æ·±åº¦Qç½‘ç»œçš„æ€§èƒ½ï¼ŒChebyshev-DQNèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„å†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The performance of Deep Q-Networks (DQN) is critically dependent on the ability of its underlying neural network to accurately approximate the action-value function. Standard function approximators, such as multi-layer perceptrons, may struggle to efficiently represent the complex value landscapes inherent in many reinforcement learning problems. This paper introduces a novel architecture, the Chebyshev-DQN (Ch-DQN), which integrates a Chebyshev polynomial basis into the DQN framework to create a more effective feature representation. By leveraging the powerful function approximation properties of Chebyshev polynomials, we hypothesize that the Ch-DQN can learn more efficiently and achieve higher performance. We evaluate our proposed model on the CartPole-v1 benchmark and compare it against a standard DQN with a comparable number of parameters. Our results demonstrate that the Ch-DQN with a moderate polynomial degree (N=4) achieves significantly better asymptotic performance, outperforming the baseline by approximately 39\%. However, we also find that the choice of polynomial degree is a critical hyperparameter, as a high degree (N=8) can be detrimental to learning. This work validates the potential of using orthogonal polynomial bases in deep reinforcement learning while also highlighting the trade-offs involved in model complexity.

