---
layout: default
title: Semantic Energy: Detecting LLM Hallucination Beyond Entropy
---

# Semantic Energy: Detecting LLM Hallucination Beyond Entropy

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14496" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14496v3</a>
  <a href="https://arxiv.org/pdf/2508.14496.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14496v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14496v3', 'Semantic Energy: Detecting LLM Hallucination Beyond Entropy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huan Ma, Jiadong Pan, Jing Liu, Yan Chen, Joey Tianyi Zhou, Guangyu Wang, Qinghua Hu, Hua Wu, Changqing Zhang, Haifeng Wang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20 (æ›´æ–°: 2025-12-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰èƒ½é‡ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¹»è§‰æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¹»è§‰æ£€æµ‹` `ä¸ç¡®å®šæ€§ä¼°è®¡` `è¯­ä¹‰èƒ½é‡` `è¯­ä¹‰èšç±»` `Boltzmannåˆ†å¸ƒ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºè¯­ä¹‰ç†µçš„æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•æ‰å¤§è¯­è¨€æ¨¡å‹çš„å†…åœ¨ä¸ç¡®å®šæ€§ï¼Œå¯¼è‡´å¹»è§‰æ£€æµ‹æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„è¯­ä¹‰èƒ½é‡æ¡†æ¶ç›´æ¥åœ¨æ¨¡å‹çš„logitsä¸Šè¿›è¡Œæ“ä½œï¼Œç»“åˆè¯­ä¹‰èšç±»ä¸èƒ½é‡åˆ†å¸ƒï¼Œæå‡äº†ä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯­ä¹‰èƒ½é‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å¹»è§‰æ£€æµ‹çš„æ€§èƒ½ï¼Œæä¾›äº†æ›´å¯é çš„ä¿¡å·ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ï¼Œå®ƒä»¬ä»ç„¶å®¹æ˜“å‡ºç°å¹»è§‰ç°è±¡ï¼Œå³ç”Ÿæˆæµç•…ä½†ä¸æ­£ç¡®çš„å“åº”ï¼Œå¯¼è‡´é”™è¯¯å†³ç­–ã€‚ç°æœ‰çš„åŸºäºè¯­ä¹‰ç†µçš„æ£€æµ‹æ–¹æ³•ä¾èµ–äºåè½¯æœ€å¤§æ¦‚ç‡ï¼Œæœªèƒ½æœ‰æ•ˆæ•æ‰æ¨¡å‹çš„å†…åœ¨ä¸ç¡®å®šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ¡†æ¶â€”â€”è¯­ä¹‰èƒ½é‡ï¼Œç›´æ¥åœ¨å€’æ•°ç¬¬äºŒå±‚çš„logitsä¸Šæ“ä½œï¼Œç»“åˆè¯­ä¹‰èšç±»ä¸Boltzmannå¯å‘çš„èƒ½é‡åˆ†å¸ƒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰èƒ½é‡æ˜¾è‘—æå‡äº†å¹»è§‰æ£€æµ‹å’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„æ•ˆæœï¼Œä¸ºä¸‹æ¸¸åº”ç”¨æä¾›äº†æ›´å¯é çš„ä¿¡å·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è§‰ç°è±¡ï¼Œç°æœ‰æ–¹æ³•å¦‚è¯­ä¹‰ç†µæœªèƒ½æœ‰æ•ˆæ•æ‰æ¨¡å‹çš„å†…åœ¨ä¸ç¡®å®šæ€§ï¼Œå¯¼è‡´æ£€æµ‹æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè¯­ä¹‰èƒ½é‡æ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥æ“ä½œå€’æ•°ç¬¬äºŒå±‚çš„logitsï¼Œç»“åˆè¯­ä¹‰èšç±»ä¸Boltzmannèƒ½é‡åˆ†å¸ƒï¼Œæ›´å¥½åœ°æ•æ‰æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡æ ·ã€è¯­ä¹‰èšç±»ã€èƒ½é‡åˆ†å¸ƒè®¡ç®—å’Œä¸ç¡®å®šæ€§è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„æ£€æµ‹æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯­ä¹‰èƒ½é‡ä½œä¸ºä¸€ç§æ–°é¢–çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•ï¼Œç›´æ¥åˆ©ç”¨æ¨¡å‹çš„logitsï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„åŸºäºåè½¯æœ€å¤§æ¦‚ç‡çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹çš„ä¿¡å¿ƒç¨‹åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†Boltzmannåˆ†å¸ƒæ¥è®¡ç®—èƒ½é‡ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨å¹»è§‰æ£€æµ‹ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å®éªŒéªŒè¯äº†è¿™äº›è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰èƒ½é‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸æ¯”äºä¼ ç»Ÿçš„è¯­ä¹‰ç†µæ–¹æ³•ï¼Œå¹»è§‰æ£€æµ‹çš„å‡†ç¡®ç‡æé«˜äº†çº¦15%ï¼Œå¹¶ä¸”åœ¨ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼Œæä¾›äº†æ›´å¯é çš„ä¿¡å·ç”¨äºä¸‹æ¸¸åº”ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€è‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡è¿™äº›ç³»ç»Ÿçš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œéšç€å¤§è¯­è¨€æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼Œè¯­ä¹‰èƒ½é‡æ¡†æ¶æœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œå‡å°‘å› å¹»è§‰ç°è±¡å¯¼è‡´çš„å†³ç­–é”™è¯¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are being increasingly deployed in real-world applications, but they remain susceptible to hallucinations, which produce fluent yet incorrect responses and lead to erroneous decision-making. Uncertainty estimation is a feasible approach to detect such hallucinations. For example, semantic entropy estimates uncertainty by considering the semantic diversity across multiple sampled responses, thus identifying hallucinations. However, semantic entropy relies on post-softmax probabilities and fails to capture the model's inherent uncertainty, causing it to be ineffective in certain scenarios. To address this issue, we introduce Semantic Energy, a novel uncertainty estimation framework that leverages the inherent confidence of LLMs by operating directly on logits of penultimate layer. By combining semantic clustering with a Boltzmann-inspired energy distribution, our method better captures uncertainty in cases where semantic entropy fails. Experiments across multiple benchmarks show that Semantic Energy significantly improves hallucination detection and uncertainty estimation, offering more reliable signals for downstream applications such as hallucination detection.

