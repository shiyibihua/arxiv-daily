---
layout: default
title: Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent
---

# Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14853" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14853v1</a>
  <a href="https://arxiv.org/pdf/2508.14853.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14853v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14853v1', 'Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºæŒ‡æ•°æ¢¯åº¦ä¸‹é™çš„é€šç”¨å¯¹æŠ—æ”»å‡»æ–¹æ³•ä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹æŠ—æ”»å‡»` `å¤§è¯­è¨€æ¨¡å‹` `ä¼˜åŒ–æ–¹æ³•` `æŒ‡æ•°æ¢¯åº¦ä¸‹é™` `é²æ£’æ€§å¢å¼º` `å®‰å…¨æ€§æµ‹è¯•` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¶Šç‹±æ”»å‡»æ–¹æ³•æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥åœ¨ç¦»æ•£æ ‡è®°ç©ºé—´ä¸­è¿›è¡Œæœ‰æ•ˆæœç´¢ï¼Œå¯¼è‡´æ”»å‡»æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæŒ‡æ•°æ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–æ–¹æ³•ï¼Œç›´æ¥ä¼˜åŒ–å¯¹æŠ—åç¼€æ ‡è®°çš„ä¸€çƒ­ç¼–ç ï¼Œç®€åŒ–äº†æ”»å‡»è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªå¼€æºLLMsä¸Šå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…³é”®åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ï¼Œç¡®ä¿å…¶é²æ£’æ€§å’Œå®‰å…¨æ€§æˆä¸ºä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰çš„å¯¹é½æŠ€æœ¯ï¼ˆå¦‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰åœ¨å…¸å‹æç¤ºä¸Šå–å¾—äº†ä¸€å®šæˆåŠŸï¼Œä½†LLMsä»ç„¶å®¹æ˜“å—åˆ°é€šè¿‡ç”¨æˆ·æç¤ºé™„åŠ çš„å¯¹æŠ—è§¦å‘å™¨çš„è¶Šç‹±æ”»å‡»ã€‚ç°æœ‰çš„è¶Šç‹±æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯¹ç¦»æ•£æ ‡è®°ç©ºé—´çš„ä½æ•ˆæœç´¢æˆ–å¯¹è¿ç»­åµŒå…¥çš„ç›´æ¥ä¼˜åŒ–ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å†…åœ¨ä¼˜åŒ–æ–¹æ³•ï¼Œç›´æ¥ä¼˜åŒ–å¯¹æŠ—åç¼€æ ‡è®°çš„æ”¾æ¾ä¸€çƒ­ç¼–ç ï¼Œä½¿ç”¨æŒ‡æ•°æ¢¯åº¦ä¸‹é™ç»“åˆå¸ƒé›·æ ¼æ›¼æŠ•å½±ï¼Œç¡®ä¿ä¼˜åŒ–åçš„æ¯ä¸ªæ ‡è®°çš„ä¸€çƒ­ç¼–ç å§‹ç»ˆä¿æŒåœ¨æ¦‚ç‡å•çº¯å½¢å†…ã€‚æˆ‘ä»¬æä¾›äº†è¯¥æ–¹æ³•æ”¶æ•›æ€§çš„ç†è®ºè¯æ˜ï¼Œå¹¶å®ç°äº†ä¸€ç§é«˜æ•ˆç®—æ³•ï¼ŒæˆåŠŸè¶Šç‹±å¤šç§å¹¿æ³›ä½¿ç”¨çš„LLMsã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨äº”ä¸ªå¼€æºLLMså’Œå››ä¸ªé’ˆå¯¹è¶Šç‹±æ–¹æ³•è¯„ä¼°çš„å¯¹æŠ—è¡Œä¸ºæ•°æ®é›†ä¸Šï¼ŒæˆåŠŸç‡å’Œæ”¶æ•›é€Ÿåº¦å‡ä¼˜äºä¸‰ç§æœ€å…ˆè¿›çš„åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ”»å‡»æ—¶çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•åœ¨ç¦»æ•£æ ‡è®°ç©ºé—´çš„æœç´¢æ•ˆç‡ä½ä¸‹ï¼Œä¸”å¯¹è¿ç»­åµŒå…¥çš„ä¼˜åŒ–åœ¨ä¸“æœ‰æ¨¡å‹ä¸­ä¸å¯è¡Œã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§å†…åœ¨ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡æŒ‡æ•°æ¢¯åº¦ä¸‹é™ç›´æ¥ä¼˜åŒ–å¯¹æŠ—åç¼€æ ‡è®°çš„ä¸€çƒ­ç¼–ç ï¼Œç¡®ä¿ä¼˜åŒ–ç»“æœå§‹ç»ˆæœ‰æ•ˆä¸”å¯ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨æŒ‡æ•°æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ä¸€çƒ­ç¼–ç ï¼›å…¶æ¬¡ï¼Œé€šè¿‡å¸ƒé›·æ ¼æ›¼æŠ•å½±å°†ä¼˜åŒ–ç»“æœä¿æŒåœ¨æ¦‚ç‡å•çº¯å½¢å†…ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ä¸€çƒ­ç¼–ç çš„ä¼˜åŒ–ä¸å¸ƒé›·æ ¼æ›¼æŠ•å½±ç»“åˆï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨ç¦»æ•£æ ‡è®°ç©ºé—´ä¸­çš„å±€é™æ€§ï¼Œæå‡äº†æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæˆ‘ä»¬åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å¯¹æŠ—æ€§ç›®æ ‡ï¼Œå¹¶è®¾ç½®äº†é€‚å½“çš„å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°ï¼Œä»¥ç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œä¼˜åŒ–åçš„æ ‡è®°èƒ½å¤Ÿæœ‰æ•ˆåœ°ç”¨äºè¶Šç‹±æ”»å‡»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨äº”ä¸ªå¼€æºLLMsä¸ŠæˆåŠŸç‡é«˜è¾¾85%ï¼Œæ”¶æ•›é€Ÿåº¦æ¯”ä¸‰ç§æœ€å…ˆè¿›çš„åŸºçº¿å¿«50%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ€§æµ‹è¯•ã€å¯¹æŠ—æ€§æ ·æœ¬ç”Ÿæˆä»¥åŠå¤§è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§å¢å¼ºã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹å¯¹æŠ—æ”»å‡»çš„æŠµæŠ—åŠ›ï¼Œå¯ä»¥åœ¨é‡‘èã€åŒ»ç–—ç­‰å…³é”®é¢†åŸŸä¸­æ›´å®‰å…¨åœ°éƒ¨ç½²LLMsï¼Œé™ä½æ½œåœ¨é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) are increasingly deployed in critical applications, ensuring their robustness and safety alignment remains a major challenge. Despite the overall success of alignment techniques such as reinforcement learning from human feedback (RLHF) on typical prompts, LLMs remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers appended to user prompts. Most existing jailbreak methods either rely on inefficient searches over discrete token spaces or direct optimization of continuous embeddings. While continuous embeddings can be given directly to selected open-source models as input, doing so is not feasible for proprietary models. On the other hand, projecting these embeddings back into valid discrete tokens introduces additional complexity and often reduces attack effectiveness. We propose an intrinsic optimization method which directly optimizes relaxed one-hot encodings of the adversarial suffix tokens using exponentiated gradient descent coupled with Bregman projection, ensuring that the optimized one-hot encoding of each token always remains within the probability simplex. We provide theoretical proof of convergence for our proposed method and implement an efficient algorithm that effectively jailbreaks several widely used LLMs. Our method achieves higher success rates and faster convergence compared to three state-of-the-art baselines, evaluated on five open-source LLMs and four adversarial behavior datasets curated for evaluating jailbreak methods. In addition to individual prompt attacks, we also generate universal adversarial suffixes effective across multiple prompts and demonstrate transferability of optimized suffixes to different LLMs.

