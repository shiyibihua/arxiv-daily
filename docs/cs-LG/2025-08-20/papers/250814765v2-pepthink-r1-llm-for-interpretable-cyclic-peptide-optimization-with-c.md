---
layout: default
title: PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning
---

# PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14765" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14765v2</a>
  <a href="https://arxiv.org/pdf/2508.14765.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14765v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14765v2', 'PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruheng Wang, Hang Zhang, Trieu Nguyen, Shasha Feng, Hao-Wei Pang, Xiang Yu, Li Xiao, Peter Zhiping Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20 (æ›´æ–°: 2025-11-20)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPepThink-R1ä»¥è§£å†³å¾ªç¯è‚½ä¼˜åŒ–çš„å¯è§£é‡Šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¾ªç¯è‚½è®¾è®¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¯è§£é‡Šæ€§` `è¯ç‰©å‘ç°` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ²»ç–—æ€§è‚½è®¾è®¡ä¸­é¢ä¸´åºåˆ—ç©ºé—´åºå¤§å’Œå¯è§£é‡Šæ€§å·®çš„é—®é¢˜ï¼Œé™åˆ¶äº†ä¼˜åŒ–æ•ˆæœã€‚
2. PepThink-R1é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ï¼Œæ˜ç¡®è€ƒè™‘å•ä½“ä¿®æ”¹ï¼Œæå‡äº†è®¾è®¡çš„å¯è§£é‡Šæ€§ä¸ä¼˜åŒ–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPepThink-R1ç”Ÿæˆçš„å¾ªç¯è‚½åœ¨å¤šä¸ªè¯ç†ç‰¹æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´å¥½çš„ä¼˜åŒ–æˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¾è®¡å…·æœ‰ç‰¹å®šå±æ€§çš„æ²»ç–—æ€§è‚½é¢ä¸´åºåˆ—ç©ºé—´åºå¤§ã€å®éªŒæ•°æ®æœ‰é™ä»¥åŠç°æœ‰ç”Ÿæˆæ¨¡å‹å¯è§£é‡Šæ€§å·®ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºPepThink-R1ï¼Œè¿™æ˜¯ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸ç»“åˆçš„ç”Ÿæˆæ¡†æ¶ã€‚PepThink-R1åœ¨åºåˆ—ç”Ÿæˆè¿‡ç¨‹ä¸­æ˜ç¡®è€ƒè™‘å•ä½“çº§åˆ«çš„ä¿®æ”¹ï¼Œä½¿è®¾è®¡é€‰æ‹©å¯è§£é‡Šï¼ŒåŒæ—¶ä¼˜åŒ–å¤šç§è¯ç†ç‰¹æ€§ã€‚é€šè¿‡å¹³è¡¡åŒ–å­¦æœ‰æ•ˆæ€§å’Œå±æ€§æ”¹è¿›çš„å®šåˆ¶å¥–åŠ±å‡½æ•°ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å¤šæ ·çš„åºåˆ—å˜ä½“ã€‚å®éªŒè¡¨æ˜ï¼ŒPepThink-R1ç”Ÿæˆçš„å¾ªç¯è‚½åœ¨è„‚æº¶æ€§ã€ç¨³å®šæ€§å’Œæš´éœ²åº¦æ–¹é¢æ˜¾è‘—æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é€šç”¨LLMsï¼ˆå¦‚GPT-5ï¼‰å’Œé¢†åŸŸç‰¹å®šåŸºçº¿ï¼Œæ ‡å¿—ç€å‘å¯é å’Œé€æ˜çš„æ²»ç–—æ€§è‚½ä¼˜åŒ–è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ²»ç–—æ€§è‚½è®¾è®¡ä¸­çš„å¯è§£é‡Šæ€§å’Œä¼˜åŒ–æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹å¯¹ç”Ÿæˆè¿‡ç¨‹çš„æ¸…æ™°è§£é‡Šï¼Œå¯¼è‡´è®¾è®¡é€‰æ‹©ä¸é€æ˜ï¼Œä¸”åœ¨ä¼˜åŒ–å¤šç§è¯ç†ç‰¹æ€§æ—¶æ•ˆæœæœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPepThink-R1é€šè¿‡å¼•å…¥é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œåœ¨åºåˆ—ç”Ÿæˆä¸­æ˜ç¡®è€ƒè™‘å•ä½“çº§åˆ«çš„ä¿®æ”¹ï¼Œä»è€Œå®ç°å¯è§£é‡Šçš„è®¾è®¡é€‰æ‹©å’Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºLLMsçš„åºåˆ—ç”Ÿæˆæ¨¡å—ï¼Œå…¶æ¬¡æ˜¯CoTç›‘ç£å¾®è°ƒæ¨¡å—ï¼Œæœ€åæ˜¯RLä¼˜åŒ–æ¨¡å—ã€‚æ•´ä¸ªæµç¨‹é€šè¿‡å®šåˆ¶çš„å¥–åŠ±å‡½æ•°å¼•å¯¼æ¨¡å‹æ¢ç´¢æœ‰æ•ˆçš„åºåˆ—å˜ä½“ã€‚

**å…³é”®åˆ›æ–°**ï¼šPepThink-R1çš„æœ€å¤§åˆ›æ–°åœ¨äºå°†æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ä¸RLé©±åŠ¨çš„å±æ€§æ§åˆ¶ç»“åˆï¼Œå½¢æˆäº†é¦–ä¸ªåŸºäºLLMçš„å¯è§£é‡Šè‚½è®¾è®¡æ¡†æ¶ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—ä¼˜åŒ–è¿‡ç¨‹æ›´åŠ é€æ˜å’Œå¯é ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å®šåˆ¶çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥å¹³è¡¡åŒ–å­¦æœ‰æ•ˆæ€§å’Œè¯ç†å±æ€§çš„æå‡ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æ”¯æŒé«˜æ•ˆçš„åºåˆ—ç”Ÿæˆå’Œè¯„ä¼°ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹æ€§èƒ½çš„æœ€å¤§åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPepThink-R1ç”Ÿæˆçš„å¾ªç¯è‚½åœ¨è„‚æº¶æ€§ã€ç¨³å®šæ€§å’Œæš´éœ²åº¦æ–¹é¢æ˜¾è‘—æå‡ï¼Œä¼˜åŒ–æˆåŠŸç‡è¶…è¿‡ç°æœ‰é€šç”¨LLMsï¼ˆå¦‚GPT-5ï¼‰å’Œé¢†åŸŸç‰¹å®šåŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨è‚½ä¼˜åŒ–ä¸­çš„ä¼˜è¶Šæ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PepThink-R1çš„ç ”ç©¶æˆæœåœ¨è¯ç‰©å‘ç°å’Œç”Ÿç‰©åŒ»è¯é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›å¯è§£é‡Šçš„è‚½è®¾è®¡æ–¹æ¡ˆï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸåŠ é€Ÿæ–°å‹æ²»ç–—æ€§è‚½çš„å¼€å‘ï¼Œæ¨åŠ¨ä¸ªæ€§åŒ–åŒ»ç–—å’Œæ–°è¯ç ”å‘çš„è¿›ç¨‹ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–ç”Ÿç‰©åˆ†å­è®¾è®¡é¢†åŸŸï¼Œæå‡æ•´ä½“è¯ç‰©è®¾è®¡çš„æ•ˆç‡å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Designing therapeutic peptides with tailored properties is hindered by the vastness of sequence space, limited experimental data, and poor interpretability of current generative models. To address these challenges, we introduce PepThink-R1, a generative framework that integrates large language models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly reasons about monomer-level modifications during sequence generation, enabling interpretable design choices while optimizing for multiple pharmacological properties. Guided by a tailored reward function balancing chemical validity and property improvements, the model autonomously explores diverse sequence variants. We demonstrate that PepThink-R1 generates cyclic peptides with significantly enhanced lipophilicity, stability, and exposure, outperforming existing general LLMs (e.g., GPT-5) and domain-specific baseline in both optimization success and interpretability. To our knowledge, this is the first LLM-based peptide design framework that combines explicit reasoning with RL-driven property control, marking a step toward reliable and transparent peptide optimization for therapeutic discovery.

