---
layout: default
title: Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats
---

# Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19263" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19263v1</a>
  <a href="https://arxiv.org/pdf/2508.19263.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19263v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19263v1', 'Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anat Heilper, Doron Singer

**åˆ†ç±»**: cs.LG, cs.AI, cs.NE

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

**å¤‡æ³¨**: 16 pages 9 images

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä½ç²¾åº¦æ ¼å¼ç¥ç»ç½‘ç»œç»„ä»¶æ— æŸå‹ç¼©æ–¹æ³•ä»¥é™ä½å­˜å‚¨æˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— æŸå‹ç¼©` `ä½ç²¾åº¦æ ¼å¼` `ç¥ç»ç½‘ç»œ` `ç†µç¼–ç ` `å­˜å‚¨ä¼˜åŒ–` `æ·±åº¦å­¦ä¹ æ¨¡å‹` `æ¨¡å‹ä¼ è¾“` `é«˜æ•ˆæ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹é«˜ç²¾åº¦æ ¼å¼ï¼Œå¯¼è‡´ä½ç²¾åº¦æ ¼å¼çš„å‹ç¼©æ•ˆæœæœªè¢«å……åˆ†æ¢ç´¢ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡ç‹¬ç«‹å‹ç¼©æŒ‡æ•°å’Œå°¾æ•°ï¼Œé’ˆå¯¹ä½ç²¾åº¦æ ¼å¼è¿›è¡Œä¼˜åŒ–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒBF16å’ŒFP8çš„å‹ç¼©æ¯”æ˜¾è‘—æé«˜ï¼Œåˆ†åˆ«è¾¾åˆ°62%å’Œ83%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸æ–­å‘å±•å’Œå¹¿æ³›åº”ç”¨ï¼Œå‡å°‘ç¥ç»ç½‘ç»œæƒé‡çš„å­˜å‚¨å’Œä¼ è¾“æˆæœ¬å˜å¾—æ„ˆå‘é‡è¦ã€‚å°½ç®¡ä»¥å¾€çš„ç ”ç©¶å¦‚ZipNNå±•ç¤ºäº†åŸºäºéœå¤«æ›¼ç¼–ç æµ®ç‚¹æŒ‡æ•°çš„æ— æŸå‹ç¼©æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—å‡å°æ¨¡å‹å¤§å°ï¼Œä½†è¿™äº›æŠ€æœ¯ä¸»è¦åº”ç”¨äºFP32å’ŒBF16ç­‰é«˜ç²¾åº¦æ ¼å¼ã€‚æœ¬æ–‡å°†ZipNNæ–¹æ³•æ‰©å±•è‡³FP8å’ŒFP4ç­‰ä½ç²¾åº¦æµ®ç‚¹æ ¼å¼ï¼Œè¿™äº›æ ¼å¼åœ¨é«˜æ•ˆæ¨ç†ä¸­æ—¥ç›Šå—åˆ°æ¬¢è¿ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å‹ç¼©æ–¹æ³•ï¼Œç‹¬ç«‹åœ°å¯¹æŒ‡æ•°å’Œå°¾æ•°ç»„ä»¶è¿›è¡Œåˆ†ç¦»å’Œå‹ç¼©ï¼Œé‡‡ç”¨ç†µç¼–ç ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒBF16çš„å‹ç¼©æ¯”é«˜è¾¾62%ï¼ŒFP8åˆ™è¾¾åˆ°83%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨çš„é”®å€¼ï¼ˆK/Vï¼‰ç¼“å­˜å¼ é‡çš„å¯å‹ç¼©æ€§ï¼Œå‘ç°å…¶ä¹Ÿè¡¨ç°å‡ºå¯å‹ç¼©æ¨¡å¼ï¼Œä»è€Œåœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­å®ç°å†…å­˜èŠ‚çœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä½ç²¾åº¦æ ¼å¼ç¥ç»ç½‘ç»œç»„ä»¶çš„å­˜å‚¨å’Œä¼ è¾“æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç°æœ‰çš„æ— æŸå‹ç¼©æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é«˜ç²¾åº¦æ ¼å¼ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨ä½ç²¾åº¦æ ¼å¼çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç¥ç»ç½‘ç»œæƒé‡çš„æŒ‡æ•°å’Œå°¾æ•°åˆ†å¼€è¿›è¡Œå‹ç¼©ï¼Œåˆ©ç”¨ç†µç¼–ç æŠ€æœ¯æé«˜å‹ç¼©æ•ˆç‡ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä½ç²¾åº¦æ ¼å¼çš„ç‰¹æ€§ï¼Œä»è€Œå®ç°æ›´é«˜çš„å‹ç¼©æ¯”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æŒ‡æ•°å’Œå°¾æ•°çš„åˆ†ç¦»ã€ç†µç¼–ç å‹ç¼©å’Œè§£å‹ç¼©æ¨¡å—ã€‚é¦–å…ˆå¯¹æƒé‡è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶ååˆ†åˆ«å¯¹æŒ‡æ•°å’Œå°¾æ•°è¿›è¡Œå‹ç¼©ï¼Œæœ€ååœ¨éœ€è¦æ—¶è¿›è¡Œè§£å‹ç¼©ä»¥æ¢å¤åŸå§‹æ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å‹ç¼©è¿‡ç¨‹ç»†åˆ†ä¸ºå¯¹æŒ‡æ•°å’Œå°¾æ•°çš„ç‹¬ç«‹å¤„ç†ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„æ•´ä½“å‹ç¼©æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æé«˜äº†ä½ç²¾åº¦æ ¼å¼çš„å‹ç¼©æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åˆä½ç²¾åº¦æ ¼å¼çš„ç†µç¼–ç ç®—æ³•ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­è€ƒè™‘äº†å‹ç¼©æ¯”ä¸æ¢å¤ç²¾åº¦ä¹‹é—´çš„å¹³è¡¡ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œè®¾è®¡äº†é€‚åº”FP8å’ŒFP4æ ¼å¼çš„ç‰¹å®šå‹ç¼©æ¨¡å—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé’ˆå¯¹BF16æ ¼å¼çš„å‹ç¼©æ¯”é«˜è¾¾62%ï¼Œè€ŒFP8æ ¼å¼çš„å‹ç¼©æ¯”æ›´æ˜¯è¾¾åˆ°83%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨é™ä½å­˜å‚¨æˆæœ¬æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨åœºæ™¯ä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆèŠ‚çœå†…å­˜èµ„æºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å­˜å‚¨ä¼˜åŒ–ã€äº‘è®¡ç®—æœåŠ¡ä¸­çš„æ¨¡å‹ä¼ è¾“ä»¥åŠè¾¹ç¼˜è®¾å¤‡ä¸Šçš„é«˜æ•ˆæ¨ç†ã€‚é€šè¿‡é™ä½æ¨¡å‹çš„å­˜å‚¨å’Œä¼ è¾“æˆæœ¬ï¼Œèƒ½å¤Ÿä¿ƒè¿›æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæå‡æ•´ä½“ç³»ç»Ÿçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As deep learning models grow and deployment becomes more widespread, reducing the storage and transmission costs of neural network weights has become increasingly important. While prior work such as ZipNN has shown that lossless compression methods - particularly those based on Huffman encoding floating-point exponents can significantly reduce model sizes, these techniques have primarily been applied to higher-precision formats such as FP32 and BF16. In this work, we extend the ZipNN approach to lower-precision floating-point formats, specifically FP8 and FP4, which are gaining popularity for efficient inference. We design a compression method that separates and compresses the exponent and mantissa components independently using entropy coding. Our evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also investigate the compressibility of key-value (K/V) cache tensors used in large language models (LLMs), finding that they, too, exhibit compressible patterns, enabling memory savings during deployment.

