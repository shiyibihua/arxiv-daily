---
layout: default
title: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
---

# Adaptively Robust LLM Inference Optimization under Prediction Uncertainty

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14544" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14544v2</a>
  <a href="https://arxiv.org/pdf/2508.14544.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14544v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14544v2', 'Adaptively Robust LLM Inference Optimization under Prediction Uncertainty')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zixi Chen, Yinyu Ye, Zijie Zhou

**åˆ†ç±»**: cs.LG, cs.AI, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20 (æ›´æ–°: 2025-09-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”é²æ£’LLMæ¨ç†ä¼˜åŒ–ç®—æ³•ä»¥åº”å¯¹é¢„æµ‹ä¸ç¡®å®šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†è°ƒåº¦` `é¢„æµ‹ä¸ç¡®å®šæ€§` `è‡ªé€‚åº”ç®—æ³•` `èƒ½æ•ˆä¼˜åŒ–` `æœºå™¨å­¦ä¹ ` `åŠ¨æ€è°ƒæ•´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰LLMæ¨ç†è°ƒåº¦é¢ä¸´è¾“å‡ºé•¿åº¦æœªçŸ¥çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´å†…å­˜æº¢å‡ºå’Œæ€§èƒ½ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºäº†è‡ªé€‚åº”ç®—æ³•$	ext{A}_{	ext{min}}$ï¼ŒåŠ¨æ€è°ƒæ•´è¾“å‡ºé•¿åº¦é¢„æµ‹ä»¥æé«˜è°ƒåº¦æ•ˆç‡ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼š$	ext{A}_{	ext{min}}$åœ¨æ•°å€¼æ¨¡æ‹Ÿä¸­è¡¨ç°æ¥è¿‘ç†æƒ³è°ƒåº¦å™¨ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ•ˆç‡å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†è°ƒåº¦ä»¥æœ€å°åŒ–æ€»å»¶è¿Ÿçš„é—®é¢˜ã€‚LLMæ¨ç†æ˜¯ä¸€ä¸ªåœ¨çº¿å¤šä»»åŠ¡æœåŠ¡è¿‡ç¨‹ï¼Œä¸”èƒ½è€—è¾ƒé«˜ã€‚ç”±äºè¾“å…¥è¯·æ±‚çš„æç¤ºé•¿åº¦å·²çŸ¥ï¼Œä½†è¾“å‡ºé•¿åº¦æœªçŸ¥ï¼Œå¯¼è‡´å†…å­˜ä½¿ç”¨å’Œå¤„ç†æ—¶é—´å—åˆ°å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä¸ç¡®å®šæ€§ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºæœºå™¨å­¦ä¹ çš„ç®—æ³•æ¥é¢„æµ‹è¾“å‡ºé•¿åº¦ï¼Œå¹¶è®¾è®¡äº†ä¸¤ç§è°ƒåº¦ç®—æ³•ï¼šä¿å®ˆç®—æ³•$	ext{A}_{	ext{max}}$å’Œè‡ªé€‚åº”ç®—æ³•$	ext{A}_{	ext{min}}$ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ$	ext{A}_{	ext{min}}$åœ¨å®é™…åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¥è¿‘äºç†æƒ³è°ƒåº¦å™¨çš„æ€§èƒ½ï¼Œå±•ç°äº†å…¶æ•ˆç‡å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è°ƒåº¦ä¸­çš„è¾“å‡ºé•¿åº¦ä¸ç¡®å®šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚ä¿å®ˆç®—æ³•$	ext{A}_{	ext{max}}$ï¼Œåœ¨é¢„æµ‹ä¸å‡†ç¡®æ—¶å¯èƒ½å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯å½“é¢„æµ‹è¿‡é«˜æ—¶ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜æº¢å‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„è‡ªé€‚åº”ç®—æ³•$	ext{A}_{	ext{min}}$ï¼Œåˆå§‹å°†é¢„æµ‹çš„ä¸‹ç•Œè§†ä¸ºè¾“å‡ºé•¿åº¦ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´è¿™ä¸€ä¼°è®¡ï¼Œä»¥åº”å¯¹é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜è°ƒåº¦çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥è¯·æ±‚çš„æ¥æ”¶ã€è¾“å‡ºé•¿åº¦çš„é¢„æµ‹ã€è°ƒåº¦å†³ç­–çš„åˆ¶å®šå’Œæ¨ç†è¿‡ç¨‹çš„æ‰§è¡Œã€‚ç®—æ³•é€šè¿‡æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹è¾“å‡ºé•¿åº¦çš„åŒºé—´ï¼Œå¹¶æ ¹æ®ä¸‹ç•Œè¿›è¡Œè°ƒåº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªé€‚åº”è°ƒåº¦ç®—æ³•$	ext{A}_{	ext{min}}$ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ä¾èµ–äºä¸Šç•Œé¢„æµ‹çš„æƒ…å†µä¸‹ï¼ŒåŠ¨æ€è°ƒæ•´è¾“å‡ºé•¿åº¦çš„ä¼°è®¡ï¼Œä»è€Œé¿å…äº†ä¿å®ˆç®—æ³•çš„æ€§èƒ½ç“¶é¢ˆã€‚

**å…³é”®è®¾è®¡**ï¼šç®—æ³•è®¾è®¡ä¸­ï¼Œ$	ext{A}_{	ext{min}}$ä¾èµ–äºä¸‹ç•Œé¢„æµ‹ï¼Œé¿å…äº†å¯¹ä¸Šç•Œçš„ä¾èµ–ï¼Œä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡å®æ—¶åé¦ˆä¸æ–­ä¼˜åŒ–è¾“å‡ºé•¿åº¦çš„ä¼°è®¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ$	ext{A}_{	ext{min}}$åœ¨å¤šæ¬¡æ¨¡æ‹Ÿä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½æ¥è¿‘ç†æƒ³è°ƒåº¦å™¨ï¼Œä¸”åœ¨å¤„ç†å¤§é‡è¯·æ±‚æ—¶ï¼Œå»¶è¿Ÿæ˜¾è‘—ä½äºä¼ ç»Ÿä¿å®ˆç®—æ³•ï¼Œå±•ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å®¢æœå’Œå®æ—¶ç¿»è¯‘ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä»»åŠ¡æœåŠ¡ä¸­çš„è°ƒåº¦æ•ˆç‡å’Œèƒ½æ•ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study the problem of optimizing Large Language Model (LLM) inference scheduling to minimize total latency. LLM inference is an online and multi-task service process and also heavily energy consuming by which a pre-trained LLM processes input requests and generates output tokens sequentially. Therefore, it is vital to improve its scheduling efficiency and reduce the power consumption while a great amount of prompt requests are arriving. A key challenge in LLM inference scheduling is that while the prompt length is known upon arrival, the output length, which critically impacts memory usage and processing time, is unknown. To address this uncertainty, we propose algorithms that leverage machine learning to predict output lengths, assuming the prediction provides an interval classification (min-max range) for each request.
>   We first design a conservative algorithm, $\mathcal{A}_{\max}$, which schedules requests based on the upper bound of predicted output lengths to prevent memory overflow. However, this approach is overly conservative: as prediction accuracy decreases, performance degrades significantly due to potential overestimation. To overcome this limitation, we propose $\mathcal{A}_{\min}$, an adaptive algorithm that initially treats the predicted lower bound as the output length and dynamically refines this estimate during inferencing. We prove that $\mathcal{A}_{\min}$ achieves a log-scale competitive ratio. Through numerical simulations, we demonstrate that $\mathcal{A}_{\min}$ often performs nearly as well as the hindsight scheduler, highlighting both its efficiency and robustness in practical scenarios. Moreover, $\mathcal{A}_{\min}$ relies solely on the lower bound of the prediction interval--an advantageous design choice since upper bounds on output length are typically more challenging to predict accurately.

