---
layout: default
title: Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data
---

# Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14769" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14769v1</a>
  <a href="https://arxiv.org/pdf/2508.14769.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14769v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14769v1', 'Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ahmed Mujtaba, Gleb Radchenko, Radu Prodan, Marc Masana

**åˆ†ç±»**: cs.LG, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

**å¤‡æ³¨**: This paper was accepted at the International Conference on Federated Learning Technologies and Applications, 2025. The final version is available at IEEE Xplore

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEdgeFDä»¥è§£å†³è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éIIDæ•°æ®è¿‡æ»¤é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `è¾¹ç¼˜è®¡ç®—` `çŸ¥è¯†è’¸é¦` `éIIDæ•°æ®` `KMeansç®—æ³•` `éšç§ä¿æŠ¤` `æœºå™¨å­¦ä¹ ` `æ•°æ®è¿‡æ»¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è”é‚¦è’¸é¦æ–¹æ³•ä¾èµ–å¤æ‚çš„çŸ¥è¯†å…±äº«ç­–ç•¥ï¼Œå¯¼è‡´å®¢æˆ·ç«¯éœ€è¦è¿›è¡Œé«˜è®¡ç®—é‡çš„ç»Ÿè®¡å¯†åº¦æ¯”ä¼°è®¡ï¼Œå¢åŠ äº†å¤„ç†å»¶è¿Ÿã€‚
2. æœ¬æ–‡æå‡ºçš„EdgeFDæ–¹æ³•é€šè¿‡å¼•å…¥åŸºäºKMeansçš„å¯†åº¦æ¯”ä¼°è®¡å™¨ï¼Œç®€åŒ–äº†å®¢æˆ·ç«¯çš„æ•°æ®è¿‡æ»¤è¿‡ç¨‹ï¼Œæ¶ˆé™¤äº†å¯¹æœåŠ¡å™¨ç«¯è¿‡æ»¤çš„éœ€æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEdgeFDåœ¨å¤šç§æ•°æ®åˆ†å¸ƒä¸‹çš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå‡†ç¡®ç‡æ¥è¿‘IIDåœºæ™¯ï¼Œä¸”è®¡ç®—å¼€é”€æ˜¾è‘—é™ä½ï¼Œé€‚åˆè¾¹ç¼˜è®¾å¤‡åº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦è’¸é¦ä½œä¸ºä¸€ç§æ–°å…´çš„åä½œæœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡äº¤æ¢æ¨¡å‹è¾“å‡ºï¼ˆè½¯é€»è¾‘ï¼‰è€Œéå®Œæ•´æ¨¡å‹å‚æ•°ï¼Œæä¾›äº†æ›´å¥½çš„éšç§ä¿æŠ¤å’Œå‡å°‘é€šä¿¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–å¤æ‚çš„é€‰æ‹©æ€§çŸ¥è¯†å…±äº«ç­–ç•¥ï¼Œè¦æ±‚å®¢æˆ·ç«¯é€šè¿‡è®¡ç®—å¯†é›†çš„ç»Ÿè®¡å¯†åº¦æ¯”ä¼°è®¡å™¨è¯†åˆ«åˆ†å¸ƒå†…ä»£ç†æ•°æ®ã€‚æ­¤å¤–ï¼ŒæœåŠ¡å™¨ç«¯å¯¹æ¨¡ç³ŠçŸ¥è¯†çš„è¿‡æ»¤å¢åŠ äº†å»¶è¿Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„EdgeFDæ–¹æ³•ï¼Œç®€åŒ–äº†å®¢æˆ·ç«¯çš„å¯†åº¦æ¯”ä¼°è®¡ï¼Œå¹¶æ¶ˆé™¤äº†æœåŠ¡å™¨ç«¯è¿‡æ»¤çš„éœ€æ±‚ã€‚EdgeFDé‡‡ç”¨åŸºäºKMeansçš„å¯†åº¦æ¯”ä¼°è®¡å™¨ï¼Œæœ‰æ•ˆè¿‡æ»¤å®¢æˆ·ç«¯çš„åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ä»£ç†æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†çŸ¥è¯†å…±äº«çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEdgeFDåœ¨å¼ºéIIDã€å¼±éIIDå’ŒIIDæ•°æ®åˆ†å¸ƒä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå‡†ç¡®ç‡æ¥è¿‘IIDåœºæ™¯ï¼Œä¸”è®¡ç®—å¼€é”€æ˜¾è‘—é™ä½ï¼Œé€‚åˆèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è”é‚¦è’¸é¦æ–¹æ³•åœ¨å¤„ç†éIIDæ•°æ®æ—¶çš„å¤æ‚æ€§å’Œå»¶è¿Ÿé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å®¢æˆ·ç«¯è¿›è¡Œé«˜è®¡ç®—é‡çš„å¯†åº¦æ¯”ä¼°è®¡ï¼Œå¹¶ä¾èµ–æœåŠ¡å™¨ç«¯çš„æ¨¡ç³ŠçŸ¥è¯†è¿‡æ»¤ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEdgeFDé€šè¿‡å¼•å…¥åŸºäºKMeansçš„å¯†åº¦æ¯”ä¼°è®¡å™¨ï¼Œç®€åŒ–äº†å®¢æˆ·ç«¯çš„è®¡ç®—è¿‡ç¨‹ï¼Œç›´æ¥åœ¨å®¢æˆ·ç«¯è¿›è¡Œæ•°æ®è¿‡æ»¤ï¼Œé¿å…äº†æœåŠ¡å™¨ç«¯çš„å»¶è¿Ÿã€‚è¯¥æ–¹æ³•æœ‰æ•ˆæé«˜äº†çŸ¥è¯†å…±äº«çš„è´¨é‡ï¼Œå°¤å…¶æ˜¯åœ¨éIIDæ•°æ®åœºæ™¯ä¸‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEdgeFDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€KMeanså¯†åº¦æ¯”ä¼°è®¡ã€æ•°æ®è¿‡æ»¤å’ŒçŸ¥è¯†å…±äº«å››ä¸ªä¸»è¦æ¨¡å—ã€‚å®¢æˆ·ç«¯é¦–å…ˆæ”¶é›†æ•°æ®ï¼Œç„¶åä½¿ç”¨KMeansç®—æ³•è¿›è¡Œå¯†åº¦æ¯”ä¼°è®¡ï¼Œæ¥ç€è¿‡æ»¤ä¸åˆé€‚çš„ä»£ç†æ•°æ®ï¼Œæœ€åå°†æœ‰æ•ˆçš„çŸ¥è¯†å…±äº«ç»™æœåŠ¡å™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šEdgeFDçš„ä¸»è¦åˆ›æ–°åœ¨äºä½¿ç”¨KMeansç®—æ³•ä½œä¸ºå¯†åº¦æ¯”ä¼°è®¡å™¨ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶æ¶ˆé™¤äº†å¯¹æœåŠ¡å™¨ç«¯è¿‡æ»¤çš„éœ€æ±‚ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ–¹æ³•åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šæ›´å…·å¯è¡Œæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨EdgeFDä¸­ï¼ŒKMeansç®—æ³•çš„å‚æ•°è®¾ç½®ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†çŸ¥è¯†å…±äº«çš„è´¨é‡ï¼Œç¡®ä¿åœ¨è¿‡æ»¤è¿‡ç¨‹ä¸­å°½å¯èƒ½ä¿ç•™æœ‰ç”¨çš„ä¿¡æ¯ã€‚æ•´ä½“ç½‘ç»œç»“æ„ç»è¿‡è°ƒæ•´ï¼Œä»¥é€‚åº”è¾¹ç¼˜è®¾å¤‡çš„è®¡ç®—èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEdgeFDåœ¨å¼ºéIIDã€å¼±éIIDå’ŒIIDæ•°æ®åˆ†å¸ƒä¸‹çš„å‡†ç¡®ç‡å‡æ¥è¿‘IIDåœºæ™¯ï¼Œä¸”åœ¨è®¡ç®—å¼€é”€ä¸Šæ˜¾è‘—ä½äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦ä¿æŠ¤éšç§çš„è¾¹ç¼˜è®¡ç®—åœºæ™¯ï¼Œå¦‚æ™ºèƒ½æ‰‹æœºã€ç‰©è”ç½‘è®¾å¤‡å’Œè¾¹ç¼˜æœåŠ¡å™¨ç­‰ã€‚é€šè¿‡æé«˜éIIDæ•°æ®å¤„ç†çš„æ•ˆç‡ï¼ŒEdgeFDèƒ½å¤Ÿåœ¨åŒ»ç–—ã€é‡‘èå’Œæ™ºèƒ½åŸå¸‚ç­‰é¢†åŸŸå®ç°æ›´å®‰å…¨å’Œé«˜æ•ˆçš„åä½œå­¦ä¹ ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated distillation has emerged as a promising collaborative machine learning approach, offering enhanced privacy protection and reduced communication compared to traditional federated learning by exchanging model outputs (soft logits) rather than full model parameters. However, existing methods employ complex selective knowledge-sharing strategies that require clients to identify in-distribution proxy data through computationally expensive statistical density ratio estimators. Additionally, server-side filtering of ambiguous knowledge introduces latency to the process. To address these challenges, we propose a robust, resource-efficient EdgeFD method that reduces the complexity of the client-side density ratio estimation and removes the need for server-side filtering. EdgeFD introduces an efficient KMeans-based density ratio estimator for effectively filtering both in-distribution and out-of-distribution proxy data on clients, significantly improving the quality of knowledge sharing. We evaluate EdgeFD across diverse practical scenarios, including strong non-IID, weak non-IID, and IID data distributions on clients, without requiring a pre-trained teacher model on the server for knowledge distillation. Experimental results demonstrate that EdgeFD outperforms state-of-the-art methods, consistently achieving accuracy levels close to IID scenarios even under heterogeneous and challenging conditions. The significantly reduced computational overhead of the KMeans-based estimator is suitable for deployment on resource-constrained edge devices, thereby enhancing the scalability and real-world applicability of federated distillation. The code is available online for reproducibility.

