---
layout: default
title: Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method
---

# Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14783" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14783v1</a>
  <a href="https://arxiv.org/pdf/2508.14783.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14783v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14783v1', 'Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Suleyman Olcay Polat, Poli A. Nemkova, Mark V. Albert

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAGEä»¥è§£å†³ä¼ ç»Ÿè’¸é¦æ–¹æ³•çš„æ•ˆç‡ä¸æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `è‡ªé€‚åº”å­¦ä¹ ` `æ•°æ®å¢å¼º` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å‹ç¼©` `å‘é‡åŒ–è¡¨ç¤º` `é«˜æ•ˆæ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è’¸é¦æ–¹æ³•åœ¨è®¡ç®—å¼€é”€å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå½±å“äº†æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€ç”Ÿæˆåˆæˆç¤ºä¾‹æ¥å¢å¼ºè®­ç»ƒæ•°æ®ï¼Œæå‡å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„66Må‚æ•°å­¦ç”Ÿæ¨¡å‹åœ¨å¤šä¸ªNLPåŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œè®­ç»ƒæ•ˆç‡æ˜¾è‘—æé«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡å‹è’¸é¦èƒ½å¤Ÿå°†å¤§è§„æ¨¡æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œä»è€Œä¾¿äºåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è’¸é¦æ–¹æ³•å¸¸å¸¸é¢ä¸´è®¡ç®—å¼€é”€å¤§å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªé€‚åº”è’¸é¦æ¡†æ¶ï¼ŒåŠ¨æ€å¢å¼ºé«˜æŸå¤±åŒºåŸŸçš„è®­ç»ƒæ•°æ®ã€‚é€šè¿‡åŸºäºUMAPçš„é™ç»´å’Œæœ€è¿‘é‚»é‡‡æ ·ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¯†åˆ«åµŒå…¥ç©ºé—´ä¸­çš„è¡¨ç°ä¸ä½³åŒºåŸŸï¼Œå¹¶ç”Ÿæˆé’ˆå¯¹æ€§çš„åˆæˆç¤ºä¾‹ä»¥æŒ‡å¯¼å­¦ç”Ÿå­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„æ•™å¸ˆ-å­¦ç”Ÿæ¥å£ï¼Œç»•è¿‡æ•™å¸ˆçš„è¾“å…¥å±‚ï¼Œå®ç°å¯¹å‘é‡åŒ–è¡¨ç¤ºçš„ç›´æ¥è’¸é¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ66Må‚æ•°çš„å­¦ç”Ÿæ¨¡å‹åœ¨æ ‡å‡†NLPåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒQNLIè¾¾91.2%ï¼ŒSST-2è¾¾92.3%ï¼Œä¸”è®­ç»ƒæ‰€éœ€çš„è½®æ¬¡æ›´å°‘ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åŸºäºæŸå¤±çš„æ•°æ®å¢å¼ºå’Œå‘é‡åŒ–è’¸é¦åœ¨æ¨¡å‹å‹ç¼©ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿè’¸é¦æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é«˜æŸå¤±åŒºåŸŸæ—¶ç¼ºä¹æœ‰æ•ˆçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¯¼è‡´å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§è‡ªé€‚åº”è’¸é¦æ¡†æ¶ï¼ŒåŠ¨æ€è¯†åˆ«å­¦ç”Ÿæ¨¡å‹åœ¨åµŒå…¥ç©ºé—´ä¸­çš„è¡¨ç°ä¸ä½³åŒºåŸŸï¼Œå¹¶ç”Ÿæˆåˆæˆç¤ºä¾‹ä»¥æŒ‡å¯¼å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨é€šè¿‡é’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºæ¥æå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å¢å¼ºæ¨¡å—å’Œè½»é‡çº§æ•™å¸ˆ-å­¦ç”Ÿæ¥å£ã€‚æ•°æ®å¢å¼ºæ¨¡å—åˆ©ç”¨UMAPé™ç»´å’Œæœ€è¿‘é‚»é‡‡æ ·è¯†åˆ«é«˜æŸå¤±åŒºåŸŸï¼Œè€Œæ•™å¸ˆ-å­¦ç”Ÿæ¥å£åˆ™ç»•è¿‡æ•™å¸ˆçš„è¾“å…¥å±‚ï¼Œç›´æ¥å¯¹å‘é‡åŒ–è¡¨ç¤ºè¿›è¡Œè’¸é¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåŠ¨æ€ç”Ÿæˆåˆæˆç¤ºä¾‹çš„èƒ½åŠ›å’Œè½»é‡çº§æ¥å£çš„è®¾è®¡ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€æ•°æ®å¤„ç†å’Œå¤æ‚çš„æ•™å¸ˆè¾“å…¥å±‚å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æé«˜äº†è’¸é¦æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œå­¦ç”Ÿæ¨¡å‹çš„å‚æ•°é‡ä¸º66Mï¼Œé‡‡ç”¨äº†é’ˆå¯¹æ€§æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚ç½‘ç»œç»“æ„è®¾è®¡ä¸Šï¼Œè½»é‡çº§æ¥å£ç¡®ä¿äº†è’¸é¦è¿‡ç¨‹çš„é«˜æ•ˆæ€§ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„66Må‚æ•°å­¦ç”Ÿæ¨¡å‹åœ¨QNLIä»»åŠ¡ä¸Šè¾¾åˆ°äº†91.2%çš„å‡†ç¡®ç‡ï¼Œåœ¨SST-2ä»»åŠ¡ä¸Šè¾¾åˆ°äº†92.3%çš„å‡†ç¡®ç‡ï¼Œä¸”è®­ç»ƒæ‰€éœ€çš„è½®æ¬¡æ˜¾è‘—å‡å°‘ã€‚è¿™äº›ç»“æœä¸ä»…è¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œè¿˜å±•ç¤ºäº†æŸå¤±æ„ŸçŸ¥æ•°æ®å¢å¼ºå’Œå‘é‡åŒ–è’¸é¦çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰éœ€è¦æ¨¡å‹å‹ç¼©å’Œé«˜æ•ˆæ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›å’Œå‹ç¼©æ•ˆç‡ï¼ŒSAGEæ–¹æ³•èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model distillation enables the transfer of knowledge from large-scale models to compact student models, facilitating deployment in resource-constrained environments. However, conventional distillation approaches often suffer from computational overhead and limited generalization. We propose a novel adaptive distillation framework that dynamically augments training data in regions of high student model loss. Using UMAP-based dimensionality reduction and nearest neighbor sampling, our method identifies underperforming regions in the embedding space and generates targeted synthetic examples to guide student learning. To further improve efficiency, we introduce a lightweight teacher-student interface that bypasses the teacher's input layer, enabling direct distillation on vectorized representations. Experiments across standard NLP benchmarks demonstrate that our 66M-parameter student model consistently matches or surpasses established baselines, achieving 91.2% on QNLI and 92.3% on SST-2, while training with fewer epochs. These results highlight the promise of loss-aware data augmentation and vectorized distillation for efficient and effective model compression.

