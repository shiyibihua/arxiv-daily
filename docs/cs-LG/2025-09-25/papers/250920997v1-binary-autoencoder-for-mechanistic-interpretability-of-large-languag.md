---
layout: default
title: Binary Autoencoder for Mechanistic Interpretability of Large Language Models
---

# Binary Autoencoder for Mechanistic Interpretability of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20997" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20997v1</a>
  <a href="https://arxiv.org/pdf/2509.20997.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20997v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20997v1', 'Binary Autoencoder for Mechanistic Interpretability of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hakaze Cho, Haolin Yang, Brian M. Kurkoski, Naoya Inoue

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**å¤‡æ³¨**: 36 pages, 41 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäºŒå€¼è‡ªç¼–ç å™¨(BAE)ï¼Œç”¨äºå¤§è¯­è¨€æ¨¡å‹æœºåˆ¶å¯è§£é‡Šæ€§çš„ç‰¹å¾è§£è€¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `è‡ªç¼–ç å™¨` `ç‰¹å¾è§£è€¦` `äºŒå€¼åŒ–` `ç†µæœ€å°åŒ–` `åŸå­åŒ–ç‰¹å¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–éšå¼æ­£åˆ™åŒ–ï¼Œç¼ºä¹å…¨å±€ç¨€ç–æ€§ä¿è¯ï¼Œå¯¼è‡´å¤§é‡å¯†é›†ç‰¹å¾ï¼Œå½±å“ç‰¹å¾ç¨€ç–æ€§å’ŒåŸå­åŒ–ã€‚
2. æå‡ºäºŒå€¼è‡ªç¼–ç å™¨(BAE)ï¼Œé€šè¿‡æœ€å°åŒ–minibatchéšè—æ¿€æ´»ç†µï¼Œä¿ƒè¿›ç‰¹å¾ç‹¬ç«‹æ€§å’Œè·¨å®ä¾‹ç¨€ç–æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBAEèƒ½æœ‰æ•ˆè®¡ç®—ç‰¹å¾é›†ç†µï¼Œå¹¶æå–æ›´å¤šå¯è§£é‡Šçš„åŸå­åŒ–ç‰¹å¾ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªç¼–ç å™¨å˜ä½“ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨è§£è€¦å¤§è¯­è¨€æ¨¡å‹(LLM)éšè—çŠ¶æ€ä¸­çš„åŸå­åŒ–æ•°å€¼æˆåˆ†ï¼ˆç‰¹å¾ï¼‰æ—¶ï¼Œä¾èµ–äºå—éšå¼è®­ç»ƒæ—¶æ­£åˆ™åŒ–çº¦æŸçš„è‡ªç¼–ç å™¨ï¼Œç¼ºä¹å…¨å±€ç¨€ç–æ€§ä¿è¯çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨éšè—æ¿€æ´»çš„minibatchä¸Šå¼ºåˆ¶æ‰§è¡Œæœ€å°ç†µï¼Œä»è€Œä¿ƒè¿›è·¨å®ä¾‹çš„ç‰¹å¾ç‹¬ç«‹æ€§å’Œç¨€ç–æ€§ã€‚ä¸ºäº†é«˜æ•ˆè®¡ç®—ç†µï¼Œæˆ‘ä»¬å°†éšè—æ¿€æ´»é€šè¿‡é˜¶è·ƒå‡½æ•°ç¦»æ•£åŒ–ä¸º1æ¯”ç‰¹ï¼Œå¹¶åº”ç”¨æ¢¯åº¦ä¼°è®¡ä»¥å®ç°åå‘ä¼ æ’­ï¼Œå› æ­¤æˆ‘ä»¬å°†å…¶å‘½åä¸ºäºŒå€¼è‡ªç¼–ç å™¨(BAE)ã€‚å®éªŒè¡¨æ˜ï¼ŒBAEåœ¨ä¸¤ä¸ªä¸»è¦åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼š(1)ç‰¹å¾é›†ç†µè®¡ç®—ï¼ŒBAEå¯ä»¥å¯é åœ°ä¼°è®¡äºŒå€¼éšè—æ¿€æ´»ä¸Šçš„ç†µï¼Œç”¨äºè¡¨å¾LLMçš„æ¨ç†åŠ¨æ€å’Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚(2)ç‰¹å¾è§£è€¦ï¼ŒBAEå¯ä»¥æå–LLMéšè—çŠ¶æ€ä¸­çš„åŸå­åŒ–ç‰¹å¾ã€‚ä¸ºäº†ç¨³å¥åœ°è¯„ä¼°ç‰¹å¾æå–èƒ½åŠ›ï¼Œæˆ‘ä»¬æ”¹è¿›äº†ä¼ ç»Ÿçš„ç‰¹å¾è§£é‡Šæ–¹æ³•ï¼Œé¿å…ä¸å¯é çš„æ•°å€¼tokenå¤„ç†ï¼Œç»“æœè¡¨æ˜BAEé¿å…äº†å¯†é›†ç‰¹å¾ï¼ŒåŒæ—¶äº§ç”Ÿäº†æœ€å¤šçš„å¯è§£é‡Šç‰¹å¾ï¼Œè¯å®äº†BAEä½œä¸ºç‰¹å¾æå–å™¨çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§ç ”ç©¶è‡´åŠ›äºä»éšè—çŠ¶æ€ä¸­è§£è€¦åŸå­åŒ–çš„æ•°å€¼æˆåˆ†ï¼ˆç‰¹å¾ï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå—éšå¼è®­ç»ƒæ—¶æ­£åˆ™åŒ–çº¦æŸçš„è‡ªç¼–ç å™¨ï¼Œä¾‹å¦‚L1æ­£åˆ™åŒ–æˆ–top-kå‡½æ•°ï¼Œè¿™äº›æ–¹æ³•åœ¨å•ä¸ªè®­ç»ƒå®ä¾‹ä¸Šè¿›è¡Œçº¦æŸï¼Œæ— æ³•ä¿è¯è·¨å®ä¾‹çš„å…¨å±€ç¨€ç–æ€§ã€‚è¿™å¯¼è‡´å¤§é‡ç‰¹å¾åŒæ—¶å¤„äºéæ¿€æ´»çŠ¶æ€ï¼Œé™ä½äº†ç‰¹å¾çš„ç¨€ç–æ€§å’ŒåŸå­åŒ–ç¨‹åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨éšè—æ¿€æ´»çš„minibatchä¸Šå¼ºåˆ¶æ‰§è¡Œæœ€å°ç†µï¼Œæ¥ä¿ƒè¿›è·¨å®ä¾‹çš„ç‰¹å¾ç‹¬ç«‹æ€§å’Œç¨€ç–æ€§ã€‚æœ€å°ç†µé¼“åŠ±æ¯ä¸ªç‰¹å¾åœ¨ä¸åŒçš„å®ä¾‹ä¸­å…·æœ‰ä¸åŒçš„æ¿€æ´»çŠ¶æ€ï¼Œä»è€Œé¿å…å‡ºç°å¤§é‡åŒæ—¶éæ¿€æ´»çš„å¯†é›†ç‰¹å¾ã€‚é€šè¿‡å°†éšè—æ¿€æ´»äºŒå€¼åŒ–ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°è®¡ç®—ç†µï¼Œå¹¶åˆ©ç”¨æ¢¯åº¦ä¼°è®¡è¿›è¡Œåå‘ä¼ æ’­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBAEçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªæ ‡å‡†çš„è‡ªç¼–ç å™¨ç»“æ„ï¼Œç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆã€‚ç¼–ç å™¨å°†LLMçš„éšè—çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œå°†å…¶æ˜ å°„åˆ°ä½ç»´çš„éšè—è¡¨ç¤ºã€‚å…³é”®åœ¨äºéšè—è¡¨ç¤ºçš„æ¿€æ´»å€¼ä¼šè¢«äºŒå€¼åŒ–ï¼Œç„¶åç”¨äºè®¡ç®—ç†µæŸå¤±ã€‚è§£ç å™¨å°†äºŒå€¼åŒ–çš„éšè—è¡¨ç¤ºé‡æ„å›åŸå§‹çš„éšè—çŠ¶æ€ã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹é€šè¿‡æœ€å°åŒ–é‡æ„æŸå¤±å’Œç†µæŸå¤±æ¥è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šBAEæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†äºŒå€¼åŒ–æ“ä½œå’Œç†µæŸå¤±ï¼Œä»¥æ˜¾å¼åœ°ä¿ƒè¿›è·¨å®ä¾‹çš„ç‰¹å¾ç¨€ç–æ€§å’Œç‹¬ç«‹æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•éšå¼åœ°ä¾èµ–æ­£åˆ™åŒ–ä¸åŒï¼ŒBAEé€šè¿‡ç›´æ¥ä¼˜åŒ–ç†µæ¥æ§åˆ¶ç‰¹å¾çš„æ¿€æ´»æ¨¡å¼ã€‚æ­¤å¤–ï¼ŒäºŒå€¼åŒ–æ“ä½œä½¿å¾—ç†µçš„è®¡ç®—æ›´åŠ é«˜æ•ˆï¼Œå¹¶å…è®¸ä½¿ç”¨æ¢¯åº¦ä¼°è®¡è¿›è¡Œåå‘ä¼ æ’­ã€‚

**å…³é”®è®¾è®¡**ï¼šBAEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é˜¶è·ƒå‡½æ•°è¿›è¡ŒäºŒå€¼åŒ–ï¼Œå°†éšè—æ¿€æ´»ç¦»æ•£åŒ–ä¸º1æ¯”ç‰¹ã€‚2) ä½¿ç”¨æ¢¯åº¦ä¼°è®¡æŠ€æœ¯ï¼ˆä¾‹å¦‚Straight-Through Estimatorï¼‰æ¥è¿‘ä¼¼é˜¶è·ƒå‡½æ•°çš„æ¢¯åº¦ï¼Œä»è€Œå®ç°åå‘ä¼ æ’­ã€‚3) ä½¿ç”¨äº¤å‰ç†µä½œä¸ºç†µæŸå¤±å‡½æ•°ï¼Œé¼“åŠ±ç‰¹å¾åœ¨minibatchä¸­å…·æœ‰ä¸åŒçš„æ¿€æ´»çŠ¶æ€ã€‚4) å¹³è¡¡é‡æ„æŸå¤±å’Œç†µæŸå¤±çš„æƒé‡ï¼Œä»¥åœ¨ç‰¹å¾è§£è€¦å’Œé‡æ„ç²¾åº¦ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBAEèƒ½å¤Ÿæœ‰æ•ˆåœ°é¿å…å¯†é›†ç‰¹å¾çš„äº§ç”Ÿï¼Œå¹¶æå–å‡ºæ¯”ç°æœ‰æ–¹æ³•æ›´å¤šçš„å¯è§£é‡Šç‰¹å¾ã€‚é€šè¿‡æ”¹è¿›çš„ç‰¹å¾è§£é‡Šæ–¹æ³•ï¼ŒBAEåœ¨ç‰¹å¾è§£è€¦æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼ŒBAEèƒ½å¤Ÿå¯é åœ°ä¼°è®¡äºŒå€¼éšè—æ¿€æ´»ä¸Šçš„ç†µï¼Œç”¨äºè¡¨å¾LLMçš„æ¨ç†åŠ¨æ€å’Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„æœºåˆ¶å¯è§£é‡Šæ€§åˆ†æï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜ç†è§£æ¨¡å‹å†…éƒ¨çš„æ¨ç†è¿‡ç¨‹å’ŒçŸ¥è¯†è¡¨ç¤ºæ–¹å¼ã€‚é€šè¿‡æå–åŸå­åŒ–çš„ç‰¹å¾ï¼Œå¯ä»¥æ›´å¥½åœ°è¯Šæ–­æ¨¡å‹çš„è¡Œä¸ºï¼Œå‘ç°æ½œåœ¨çš„åå·®æˆ–æ¼æ´ï¼Œå¹¶ä¸ºæ¨¡å‹çš„æ”¹è¿›æä¾›æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ çš„åˆ†æï¼Œæ­ç¤ºæ¨¡å‹å¦‚ä½•åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œé¢„æµ‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing works are dedicated to untangling atomized numerical components (features) from the hidden states of Large Language Models (LLMs) for interpreting their mechanism. However, they typically rely on autoencoders constrained by some implicit training-time regularization on single training instances (i.e., $L_1$ normalization, top-k function, etc.), without an explicit guarantee of global sparsity among instances, causing a large amount of dense (simultaneously inactive) features, harming the feature sparsity and atomization. In this paper, we propose a novel autoencoder variant that enforces minimal entropy on minibatches of hidden activations, thereby promoting feature independence and sparsity across instances. For efficient entropy calculation, we discretize the hidden activations to 1-bit via a step function and apply gradient estimation to enable backpropagation, so that we term it as Binary Autoencoder (BAE) and empirically demonstrate two major applications: (1) Feature set entropy calculation. Entropy can be reliably estimated on binary hidden activations, which we empirically evaluate and leverage to characterize the inference dynamics of LLMs and In-context Learning. (2) Feature untangling. Similar to typical methods, BAE can extract atomized features from LLM's hidden states. To robustly evaluate such feature extraction capability, we refine traditional feature-interpretation methods to avoid unreliable handling of numerical tokens, and show that BAE avoids dense features while producing the largest number of interpretable ones among baselines, which confirms the effectiveness of BAE serving as a feature extractor.

