---
layout: default
title: Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer
---

# Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04362" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04362v1</a>
  <a href="https://arxiv.org/pdf/2509.04362.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04362v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04362v1', 'Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yin Huang, Yongqi Dong, Youhua Tang, Li Li

**åˆ†ç±»**: cs.LG, cs.AI, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: 25 pages, 5 figures, under review for journal publication

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSST-iTransformerï¼Œèåˆå¤šæºæ•°æ®å’Œè‡ªç›‘ç£å­¦ä¹ ï¼Œç”¨äºç²¾å‡†é¢„æµ‹åœè½¦ä½å¯ç”¨æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åœè½¦ä½é¢„æµ‹` `æ—¶ç©ºé¢„æµ‹` `è‡ªç›‘ç£å­¦ä¹ ` `Transformer` `å¤šæºæ•°æ®èåˆ` `äº¤é€šéœ€æ±‚é¢„æµ‹` `æ™ºèƒ½äº¤é€š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å»ºæ¨¡æ—¶ç©ºä¾èµ–æ€§å’Œåˆ©ç”¨å¤šæºæ•°æ®æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥å‡†ç¡®é¢„æµ‹åœè½¦ä½å¯ç”¨æ€§ã€‚
2. æå‡ºSST-iTransformerï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ å¢å¼ºæ—¶ç©ºè¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨åŒåˆ†æ”¯æ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡å¤æ‚ä¾èµ–å…³ç³»ã€‚
3. åœ¨æˆéƒ½æ•°æ®é›†ä¸Šï¼ŒSST-iTransformeræ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œåœ¨MSEæŒ‡æ ‡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é’ˆå¯¹åŸå¸‚åœè½¦éš¾é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°é¢–çš„SST-iTransformeræ–¹æ³•ï¼Œæ—¨åœ¨æå‡åœè½¦ä½å¯ç”¨æ€§é¢„æµ‹çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨K-meansèšç±»å»ºç«‹åœè½¦é›†ç¾¤åŒºåŸŸ(PCZ)ï¼Œæå–å¹¶æ•´åˆæ¥è‡ªå¤šç§äº¤é€šæ¨¡å¼ï¼ˆåœ°é“ã€å…¬äº¤ã€ç½‘çº¦è½¦å’Œå‡ºç§Ÿè½¦ï¼‰çš„äº¤é€šéœ€æ±‚ç‰¹å¾ã€‚SST-iTransformeråœ¨iTransformeråŸºç¡€ä¸Šè¿›è¡Œäº†å‡çº§ï¼Œé›†æˆäº†åŸºäºæ©ç é‡å»ºçš„è‡ªç›‘ç£æ—¶ç©ºè¡¨ç¤ºå­¦ä¹ é¢„è®­ç»ƒä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨åˆ›æ–°çš„åŒåˆ†æ”¯æ³¨æ„åŠ›æœºåˆ¶ï¼šåºåˆ—æ³¨æ„åŠ›é€šè¿‡åˆ†å—æ“ä½œæ•è·é•¿æœŸæ—¶é—´ä¾èµ–æ€§ï¼Œé€šé“æ³¨æ„åŠ›é€šè¿‡åè½¬ç»´åº¦å»ºæ¨¡è·¨å˜é‡äº¤äº’ã€‚åœ¨æˆéƒ½çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSST-iTransformerä¼˜äºå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå®ç°äº†æœ€ä½çš„å‡æ–¹è¯¯å·®(MSE)å’Œå…·æœ‰ç«äº‰åŠ›çš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ã€‚æ¶ˆèç ”ç©¶æ­ç¤ºäº†ä¸åŒæ•°æ®æºçš„é‡è¦æ€§ï¼šç½‘çº¦è½¦æ•°æ®å¢ç›Šæœ€å¤§ï¼Œå…¶æ¬¡æ˜¯å‡ºç§Ÿè½¦ï¼Œè€Œå›ºå®šçº¿è·¯å…¬äº¤ç‰¹å¾ï¼ˆå…¬äº¤/åœ°é“ï¼‰çš„è´¡çŒ®è¾ƒå°ã€‚ç©ºé—´ç›¸å…³æ€§åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œæ’é™¤PCZå†…ç›¸å…³åœè½¦åœºçš„å†å²æ•°æ®ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œçªæ˜¾äº†å»ºæ¨¡ç©ºé—´ä¾èµ–æ€§çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸå¸‚åœè½¦ä½å¯ç”¨æ€§é¢„æµ‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å»ºæ¨¡å¤æ‚çš„æ—¶ç©ºä¾èµ–å…³ç³»ä»¥åŠæœ‰æ•ˆèåˆå¤šæºå¼‚æ„æ•°æ®æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´é¢„æµ‹ç²¾åº¦ä¸é«˜ã€‚å°¤å…¶æ˜¯åœ¨æ•æ‰é•¿æœŸæ—¶é—´ä¾èµ–æ€§å’Œè·¨å˜é‡äº¤äº’æ–¹é¢è¡¨ç°æ¬ ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ å¢å¼ºæ—¶ç©ºè¡¨ç¤ºï¼Œå¹¶è®¾è®¡ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆå»ºæ¨¡é•¿æœŸæ—¶é—´ä¾èµ–æ€§å’Œè·¨å˜é‡äº¤äº’çš„Transformerå˜ä½“ã€‚é€šè¿‡èåˆå¤šç§äº¤é€šæ¨¡å¼çš„æ•°æ®ï¼Œæ›´å…¨é¢åœ°åæ˜ åœè½¦éœ€æ±‚ï¼Œä»è€Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) åˆ©ç”¨K-meansèšç±»å°†åœè½¦åœºåˆ’åˆ†ä¸ºåœè½¦é›†ç¾¤åŒºåŸŸ(PCZ)ã€‚2) ä»å¤šç§äº¤é€šæ¨¡å¼ï¼ˆåœ°é“ã€å…¬äº¤ã€ç½‘çº¦è½¦ã€å‡ºç§Ÿè½¦ï¼‰æå–äº¤é€šéœ€æ±‚ç‰¹å¾ã€‚3) ä½¿ç”¨SST-iTransformeræ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚SST-iTransformeråŒ…å«ä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ æ¨¡å—å’Œä¸€ä¸ªåŒåˆ†æ”¯æ³¨æ„åŠ›æ¨¡å—ã€‚è‡ªç›‘ç£å­¦ä¹ æ¨¡å—é€šè¿‡æ©ç é‡å»ºä»»åŠ¡å­¦ä¹ æ—¶ç©ºè¡¨ç¤ºã€‚åŒåˆ†æ”¯æ³¨æ„åŠ›æ¨¡å—åŒ…æ‹¬åºåˆ—æ³¨æ„åŠ›å’Œé€šé“æ³¨æ„åŠ›ï¼Œåˆ†åˆ«ç”¨äºå»ºæ¨¡é•¿æœŸæ—¶é—´ä¾èµ–æ€§å’Œè·¨å˜é‡äº¤äº’ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†SST-iTransformerï¼Œä¸€ç§æ”¹è¿›çš„Transformeræ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å»ºæ¨¡æ—¶ç©ºä¾èµ–å…³ç³»ã€‚2) å¼•å…¥äº†åŸºäºæ©ç é‡å»ºçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¢å¼ºæ—¶ç©ºè¡¨ç¤ºå­¦ä¹ ã€‚3) è®¾è®¡äº†åŒåˆ†æ”¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«æ•æ‰é•¿æœŸæ—¶é—´ä¾èµ–æ€§å’Œè·¨å˜é‡äº¤äº’ã€‚

**å…³é”®è®¾è®¡**ï¼šSST-iTransformerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è‡ªç›‘ç£å­¦ä¹ æ¨¡å—çš„æ©ç æ¯”ä¾‹å’Œé‡å»ºæŸå¤±å‡½æ•°ã€‚2) åºåˆ—æ³¨æ„åŠ›ä¸­çš„åˆ†å—å¤§å°å’Œæ»‘åŠ¨çª—å£å¤§å°ã€‚3) é€šé“æ³¨æ„åŠ›ä¸­çš„ç»´åº¦åè½¬æ–¹å¼ã€‚4) æŸå¤±å‡½æ•°é‡‡ç”¨å‡æ–¹è¯¯å·®(MSE)å’Œå¹³å‡ç»å¯¹è¯¯å·®(MAE)çš„ç»„åˆï¼Œä»¥å¹³è¡¡é¢„æµ‹ç²¾åº¦å’Œé²æ£’æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä½†å…·ä½“æ•°å€¼æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSST-iTransformeråœ¨æˆéƒ½çœŸå®æ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºInformerã€Autoformerã€Crossformerå’ŒiTransformerç­‰åŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒSST-iTransformerå®ç°äº†æœ€ä½çš„å‡æ–¹è¯¯å·®(MSE)ï¼Œå¹¶åœ¨å¹³å‡ç»å¯¹è¯¯å·®(MAE)æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œç½‘çº¦è½¦æ•°æ®å¯¹æ€§èƒ½æå‡è´¡çŒ®æœ€å¤§ï¼Œå…¶æ¬¡æ˜¯å‡ºç§Ÿè½¦æ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½äº¤é€šç®¡ç†ç³»ç»Ÿï¼Œå¸®åŠ©ç”¨æˆ·å®æ—¶äº†è§£åœè½¦ä½å¯ç”¨æƒ…å†µï¼Œç¼“è§£åŸå¸‚äº¤é€šæ‹¥å µï¼Œæé«˜åœè½¦æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ä¸ºåŸå¸‚è§„åˆ’è€…æä¾›å†³ç­–æ”¯æŒï¼Œä¼˜åŒ–åœè½¦è®¾æ–½å¸ƒå±€ï¼Œæå‡åŸå¸‚å¯æŒç»­å‘å±•èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid growth of private car ownership has worsened the urban parking predicament, underscoring the need for accurate and effective parking availability prediction to support urban planning and management. To address key limitations in modeling spatio-temporal dependencies and exploiting multi-source data for parking availability prediction, this study proposes a novel approach with SST-iTransformer. The methodology leverages K-means clustering to establish parking cluster zones (PCZs), extracting and integrating traffic demand characteristics from various transportation modes (i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted parking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates masking-reconstruction-based pretext tasks for self-supervised spatio-temporal representation learning, and features an innovative dual-branch attention mechanism: Series Attention captures long-term temporal dependencies via patching operations, while Channel Attention models cross-variate interactions through inverted dimensions. Extensive experiments using real-world data from Chengdu, China, demonstrate that SST-iTransformer outperforms baseline deep learning models (including Informer, Autoformer, Crossformer, and iTransformer), achieving state-of-the-art performance with the lowest mean squared error (MSE) and competitive mean absolute error (MAE). Comprehensive ablation studies quantitatively reveal the relative importance of different data sources: incorporating ride-hailing data provides the largest performance gains, followed by taxi, whereas fixed-route transit features (bus/metro) contribute marginally. Spatial correlation analysis further confirms that excluding historical data from correlated parking lots within PCZs leads to substantial performance degradation, underscoring the importance of modeling spatial dependencies.

