---
layout: default
title: Rethinking the long-range dependency in Mamba/SSM and transformer models
---

# Rethinking the long-range dependency in Mamba/SSM and transformer models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04226" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04226v1</a>
  <a href="https://arxiv.org/pdf/2509.04226.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04226v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04226v1', 'Rethinking the long-range dependency in Mamba/SSM and transformer models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cong Ma, Kayvan Najarian

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ä»ç†è®ºè§’åº¦åˆ†æMamba/SSMå’ŒTransformerçš„é•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é•¿ç¨‹ä¾èµ–` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `Transformer` `æ³¨æ„åŠ›æœºåˆ¶` `åºåˆ—æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åºåˆ—æ¨¡å‹ç¼ºä¹å¯¹é•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›çš„ç†è®ºåˆ†æï¼Œé˜»ç¢äº†æ¨¡å‹æ”¹è¿›ã€‚
2. è®ºæ–‡é€šè¿‡åˆ†æéšè—çŠ¶æ€å¯¹è¿‡å»è¾“å…¥çš„å¯¼æ•°ï¼Œå®šä¹‰äº†é•¿ç¨‹ä¾èµ–ï¼Œå¹¶æ¯”è¾ƒäº†SSMå’ŒTransformerã€‚
3. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„SSMéšè—çŠ¶æ€æ›´æ–°å…¬å¼ï¼Œæ—¨åœ¨ç»“åˆTransformerçš„çµæ´»æ€§å’ŒSSMçš„æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿ç¨‹ä¾èµ–æ˜¯è¯¸å¦‚çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯Mambaï¼‰å’ŒTransformeræ¨¡å‹ç­‰åºåˆ—æ¨¡å‹æœ€æœŸæœ›çš„ç‰¹æ€§ä¹‹ä¸€ã€‚ç›®å‰ï¼Œç ”ç©¶äººå‘˜æ­£ç§¯æå¼€å‘æ–°çš„æ¨¡å‹æ¶æ„ï¼Œå¹¶é’ˆå¯¹éœ€è¦é•¿ç¨‹ä¾èµ–çš„é¢„æµ‹ä»»åŠ¡è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å»ºæ¨¡é•¿ç¨‹ä¾èµ–çš„èƒ½åŠ›å°šæœªä»ç†è®ºè§’åº¦è¿›è¡Œç ”ç©¶ï¼Œè¿™é˜»ç¢äº†å¯¹è¯¥æ–¹é¢çš„ç³»ç»Ÿæ€§æ”¹è¿›ã€‚æœ¬æ–‡é€šè¿‡æ•°å­¦æ–¹å¼å®šä¹‰äº†é•¿ç¨‹ä¾èµ–ï¼Œå³éšè—çŠ¶æ€ç›¸å¯¹äºè¿‡å»è¾“å…¥çš„å¯¼æ•°ï¼Œå¹¶åŸºäºæ­¤å®šä¹‰æ¯”è¾ƒäº†SSMå’ŒTransformeræ¨¡å‹å»ºæ¨¡é•¿ç¨‹ä¾èµ–çš„èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒSSMçš„é•¿ç¨‹ä¾èµ–éšç€åºåˆ—é•¿åº¦å‘ˆæŒ‡æ•°è¡°å‡ï¼Œè¿™ä¸RNNä¸­è®°å¿†å‡½æ•°çš„æŒ‡æ•°è¡°å‡ä¸€è‡´ã€‚ä½†Transformerä¸­ä½¿ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶æ›´çµæ´»ï¼Œä¸å—æŒ‡æ•°è¡°å‡çš„é™åˆ¶ï¼Œç†è®ºä¸Šå¯ä»¥é€šè¿‡è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ã€è®¡ç®—èµ„æºå’Œé€‚å½“çš„è®­ç»ƒï¼Œåœ¨å»ºæ¨¡é•¿ç¨‹ä¾èµ–æ–¹é¢è¡¨ç°æ›´å¥½ã€‚ä¸ºäº†ç»“åˆæ³¨æ„åŠ›æœºåˆ¶é•¿ç¨‹ä¾èµ–çš„çµæ´»æ€§å’ŒSSMçš„è®¡ç®—æ•ˆç‡ï¼Œæˆ‘ä»¬ä¸ºSSMä¸­çš„éšè—çŠ¶æ€æ›´æ–°æå‡ºäº†ä¸€ç§æ–°çš„å…¬å¼ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨è¾“å…¥æ•°æ®æœä»æ ‡å‡†é«˜æ–¯åˆ†å¸ƒä¸‹çš„ç¨³å®šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åºåˆ—æ¨¡å‹ï¼Œå¦‚Mamba/SSMå’ŒTransformerï¼Œåœ¨å¤„ç†é•¿åºåˆ—æ—¶ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å»ºæ¨¡é•¿ç¨‹ä¾èµ–å…³ç³»æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹å¯¹å®ƒä»¬é•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›çš„ç†è®ºåˆ†æï¼Œå¯¼è‡´éš¾ä»¥ç³»ç»Ÿæ€§åœ°æ”¹è¿›æ¨¡å‹ç»“æ„å’Œè®­ç»ƒæ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œéœ€è¦æ˜ç¡®è¿™äº›æ¨¡å‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½å¤Ÿè®°ä½å’Œåˆ©ç”¨åºåˆ—ä¸­å¾ˆä¹…ä»¥å‰çš„ä¿¡æ¯ï¼Œä»¥åŠå®ƒä»¬åœ¨å¤„ç†ä¸åŒç±»å‹çš„é•¿ç¨‹ä¾èµ–å…³ç³»æ—¶çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ•°å­¦æ–¹æ³•å®šä¹‰é•¿ç¨‹ä¾èµ–ï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºç¡€åˆ†æä¸åŒæ¨¡å‹çš„å»ºæ¨¡èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä½¿ç”¨éšè—çŠ¶æ€ç›¸å¯¹äºè¿‡å»è¾“å…¥çš„å¯¼æ•°æ¥é‡åŒ–é•¿ç¨‹ä¾èµ–ã€‚é€šè¿‡åˆ†æè¿™ä¸ªå¯¼æ•°çš„è¡°å‡ç‰¹æ€§ï¼Œå¯ä»¥äº†è§£æ¨¡å‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½å¤Ÿè®°ä½å’Œåˆ©ç”¨è¿‡å»çš„ä¿¡æ¯ã€‚å¯¹äºSSMå’ŒTransformerï¼Œè®ºæ–‡åˆ†åˆ«åˆ†æäº†å®ƒä»¬çš„éšè—çŠ¶æ€æ›´æ–°æœºåˆ¶å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ç¡®å®šå®ƒä»¬çš„é•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼šé¦–å…ˆï¼Œè®ºæ–‡æå‡ºäº†é•¿ç¨‹ä¾èµ–çš„æ•°å­¦å®šä¹‰ï¼Œå³éšè—çŠ¶æ€ç›¸å¯¹äºè¿‡å»è¾“å…¥çš„å¯¼æ•°ã€‚ç„¶åï¼Œè®ºæ–‡åˆ†åˆ«åˆ†æäº†SSMå’ŒTransformeræ¨¡å‹ï¼Œæ¨å¯¼äº†å®ƒä»¬çš„é•¿ç¨‹ä¾èµ–è¡°å‡ç‰¹æ€§ã€‚å¯¹äºSSMï¼Œè®ºæ–‡è¯æ˜äº†å…¶é•¿ç¨‹ä¾èµ–å‘ˆæŒ‡æ•°è¡°å‡ã€‚å¯¹äºTransformerï¼Œè®ºæ–‡æŒ‡å‡ºå…¶æ³¨æ„åŠ›æœºåˆ¶å…·æœ‰æ›´å¤§çš„çµæ´»æ€§ï¼Œç†è®ºä¸Šå¯ä»¥æ›´å¥½åœ°å»ºæ¨¡é•¿ç¨‹ä¾èµ–ã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„SSMéšè—çŠ¶æ€æ›´æ–°å…¬å¼ï¼Œæ—¨åœ¨ç»“åˆTransformerçš„çµæ´»æ€§å’ŒSSMçš„æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªé‡åŒ–é•¿ç¨‹ä¾èµ–çš„æ•°å­¦å®šä¹‰ï¼Œå¹¶åŸºäºæ­¤å®šä¹‰å¯¹SSMå’ŒTransformeræ¨¡å‹è¿›è¡Œäº†ç†è®ºåˆ†æã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„SSMéšè—çŠ¶æ€æ›´æ–°å…¬å¼ï¼Œæ—¨åœ¨æ”¹è¿›SSMçš„é•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥è®ºæ–‡æä¾›äº†ä¸€ç§æ›´æ·±å…¥çš„ç†è§£ï¼Œå³ä¸åŒæ¨¡å‹å¦‚ä½•å¤„ç†é•¿ç¨‹ä¾èµ–ï¼Œå¹¶ä¸ºæ”¹è¿›æ¨¡å‹ç»“æ„æä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨éšè—çŠ¶æ€å¯¹è¿‡å»è¾“å…¥çš„å¯¼æ•°ä½œä¸ºé•¿ç¨‹ä¾èµ–çš„åº¦é‡ï¼›2) å¯¹SSMå’ŒTransformerçš„éšè—çŠ¶æ€æ›´æ–°æœºåˆ¶å’Œæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ•°å­¦åˆ†æï¼Œæ¨å¯¼å…¶é•¿ç¨‹ä¾èµ–è¡°å‡ç‰¹æ€§ï¼›3) æå‡ºä¸€ç§æ–°çš„SSMéšè—çŠ¶æ€æ›´æ–°å…¬å¼ï¼Œè¯¥å…¬å¼åŸºäºä¸€ç§æ–°çš„å‚æ•°åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œé•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›ã€‚è¯¥å…¬å¼çš„å…·ä½“å½¢å¼å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æè¯æ˜äº†SSMçš„é•¿ç¨‹ä¾èµ–å‘ˆæŒ‡æ•°è¡°å‡ï¼Œè€ŒTransformerçš„æ³¨æ„åŠ›æœºåˆ¶å…·æœ‰æ›´å¤§çš„çµæ´»æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„SSMéšè—çŠ¶æ€æ›´æ–°å…¬å¼ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨æ ‡å‡†é«˜æ–¯åˆ†å¸ƒä¸‹çš„ç¨³å®šæ€§ã€‚è™½ç„¶è®ºæ–‡æ²¡æœ‰æä¾›å…·ä½“çš„å®éªŒæ•°æ®ï¼Œä½†å…¶ç†è®ºåˆ†æä¸ºæ”¹è¿›åºåˆ—æ¨¡å‹æä¾›äº†é‡è¦çš„æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿åºåˆ—æ•°æ®çš„é¢†åŸŸï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ã€æ—¶é—´åºåˆ—åˆ†æç­‰ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä¸­ï¼Œæ›´å¥½åœ°å»ºæ¨¡é•¿ç¨‹ä¾èµ–å¯ä»¥æé«˜ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚åœ¨è¯­éŸ³è¯†åˆ«ä¸­ï¼Œå¯ä»¥æé«˜å¯¹é•¿è¯­éŸ³ç‰‡æ®µçš„è¯†åˆ«ç‡ã€‚åœ¨é‡‘èæ—¶é—´åºåˆ—åˆ†æä¸­ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°é¢„æµ‹æœªæ¥çš„å¸‚åœºè¶‹åŠ¿ã€‚è¯¥ç ”ç©¶ä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„åºåˆ—æ¨¡å‹æä¾›äº†ç†è®ºåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long-range dependency is one of the most desired properties of recent sequence models such as state-space models (particularly Mamba) and transformer models. New model architectures are being actively developed and benchmarked for prediction tasks requiring long-range dependency. However, the capability of modeling long-range dependencies of these models has not been investigated from a theoretical perspective, which hinders a systematic improvement on this aspect. In this work, we mathematically define long-range dependency using the derivative of hidden states with respect to past inputs and compare the capability of SSM and transformer models of modeling long-range dependency based on this definition. We showed that the long-range dependency of SSM decays exponentially with the sequence length, which aligns with the exponential decay of memory function in RNN. But the attention mechanism used in transformers is more flexible and is not constrained to exponential decay, which could in theory perform better at modeling long-range dependency with sufficient training data, computing resources, and proper training. To combine the flexibility of long-range dependency of attention mechanism and computation efficiency of SSM, we propose a new formulation for hidden state update in SSM and prove its stability under a standard Gaussian distribution of the input data.

