---
layout: default
title: MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors
---

# MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.12221" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.12221v1</a>
  <a href="https://arxiv.org/pdf/2509.12221.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.12221v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.12221v1', 'MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin Tong, Zhi Lin, Jingya Wang, Meng Han, Bo Jin

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: Under Review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MEUVï¼šé€šè¿‡äº’æ–¥è§£é”å‘é‡å®ç°å¤§è¯­è¨€æ¨¡å‹ä¸­ç»†ç²’åº¦çš„èƒ½åŠ›æ¿€æ´»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `èƒ½åŠ›æ¿€æ´»` `å®‰å…¨å¯¹é½` `ç»†ç²’åº¦æ§åˆ¶` `äº’æ–¥å‘é‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹å®‰å…¨å¯¹é½ç­–ç•¥ä¸€åˆ€åˆ‡ï¼Œé˜»ç¢äº†å…¶åœ¨ç‰¹å®šé«˜é£é™©é¢†åŸŸçš„åˆæ³•åº”ç”¨ï¼Œç¼ºä¹ç»†ç²’åº¦æ§åˆ¶ã€‚
2. MEUVå°†æ‹’ç»æ–¹å‘åˆ†è§£ä¸ºå¤šä¸ªä¸»é¢˜å¯¹é½çš„äº’æ–¥å‘é‡ï¼Œæ¯ä¸ªå‘é‡å¯¹åº”ä¸€ä¸ªæ•æ„Ÿèƒ½åŠ›ï¼Œå®ç°æ›´ç²¾ç»†çš„æ§åˆ¶ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMEUVåœ¨ä¿æŒé«˜æ”»å‡»æˆåŠŸç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è·¨ä¸»é¢˜æ³„æ¼ï¼Œä¸”å…·æœ‰è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®æ–½å®‰å…¨å¯¹é½ä»¥å¯é åœ°æ‹’ç»æ¶æ„è¯·æ±‚ï¼Œä½†åŒæ ·çš„å…¨é¢ä¿æŠ¤æªæ–½ä¹Ÿé˜»ç¢äº†åœ¨è­¦åŠ¡ã€å›½é˜²å’Œå…¶ä»–é«˜é£é™©ç¯å¢ƒä¸­çš„åˆæ³•ä½¿ç”¨ã€‚å…ˆå‰çš„â€œæ‹’ç»æ–¹å‘â€ç¼–è¾‘å¯ä»¥ç»•è¿‡è¿™äº›å±‚ï¼Œä½†å®ƒä»¬ä¾èµ–äºå•ä¸ªå‘é‡ï¼Œè¯¥å‘é‡ä¸åŠ åŒºåˆ†åœ°è§£é”æ‰€æœ‰å±é™©ä¸»é¢˜ï¼Œä¸æä¾›è¯­ä¹‰æ§åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†äº’æ–¥è§£é”å‘é‡ï¼ˆMEUVï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œå®ƒå°†å•ç‰‡çš„æ‹’ç»æ–¹å‘åˆ†è§£ä¸ºä¸»é¢˜å¯¹é½ã€å‡ ä¹æ­£äº¤çš„å‘é‡ï¼Œæ¯ä¸ªå‘é‡ä¸“é—¨ç”¨äºä¸€ä¸ªæ•æ„Ÿèƒ½åŠ›ã€‚MEUVåœ¨ä¸€ä¸ªepochä¸­å­¦ä¹ ï¼Œé‡‡ç”¨å¤šä»»åŠ¡ç›®æ ‡ï¼Œè¯¥ç›®æ ‡èåˆäº†å·®åˆ†æ¶ˆèè£•åº¦ã€è·¨ä¸»é¢˜å’Œæ­£äº¤æ€§æƒ©ç½šä»¥åŠå‡ ä¸ªè¾…åŠ©é¡¹ã€‚åœ¨åŒè¯­æ¶æ„æç¤ºåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMEUVåœ¨Gemma-2-2Bã€LLaMA-3-8Bå’ŒQwen-7Bä¸Šå®ç°äº†ä¸ä½äº87%çš„æ”»å‡»æˆåŠŸç‡ï¼Œä½†ä¸æœ€ä½³å•æ–¹å‘åŸºçº¿ç›¸æ¯”ï¼Œè·¨ä¸»é¢˜æ³„æ¼å‡å°‘äº†é«˜è¾¾90%ã€‚ç”¨ä¸­æ–‡è®­ç»ƒçš„å‘é‡å‡ ä¹ä¸å˜åœ°è½¬ç§»åˆ°è‹±è¯­ï¼ˆåä¹‹äº¦ç„¶ï¼‰ï¼Œè¡¨æ˜å­˜åœ¨ä¸€ç§è¯­è¨€æ— å…³çš„æ‹’ç»å­ç©ºé—´ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æœ€å°çš„æ•ˆç”¨æŸå¤±ï¼Œå¯ä»¥å®ç°ç»†ç²’åº¦çš„ã€ä¸»é¢˜çº§åˆ«çš„èƒ½åŠ›æ¿€æ´»ï¼Œä¸ºåœ¨å®‰å…¨æ•æ„Ÿé¢†åŸŸä¸­å—æ§çš„LLMséƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹è™½ç„¶å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä¹Ÿå­˜åœ¨è¢«æ¶æ„åˆ©ç”¨çš„é£é™©ã€‚ä¸ºäº†é˜²æ­¢è¿™ç§æƒ…å†µï¼Œæ¨¡å‹é€šå¸¸ä¼šè¿›è¡Œå®‰å…¨å¯¹é½ï¼Œæ‹’ç»å›ç­”æ¶‰åŠæ•æ„Ÿè¯é¢˜çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ç§ä¸€åˆ€åˆ‡çš„æ–¹æ³•ä¹Ÿé™åˆ¶äº†æ¨¡å‹åœ¨ä¸€äº›ç‰¹å®šé¢†åŸŸçš„åº”ç”¨ï¼Œä¾‹å¦‚è­¦åŠ¡ã€å›½é˜²ç­‰ã€‚ç°æœ‰çš„â€œæ‹’ç»æ–¹å‘â€ç¼–è¾‘æ–¹æ³•è™½ç„¶å¯ä»¥ç»•è¿‡è¿™äº›å®‰å…¨é™åˆ¶ï¼Œä½†å®ƒä»¬é€šå¸¸ä½¿ç”¨å•ä¸ªå‘é‡æ¥è§£é”æ‰€æœ‰å±é™©ä¸»é¢˜ï¼Œç¼ºä¹ç»†ç²’åº¦çš„æ§åˆ¶èƒ½åŠ›ï¼Œå®¹æ˜“é€ æˆæ»¥ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMEUVçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åŸæœ¬å•ä¸€çš„â€œæ‹’ç»æ–¹å‘â€å‘é‡åˆ†è§£ä¸ºå¤šä¸ªäº’æ–¥çš„å‘é‡ï¼Œæ¯ä¸ªå‘é‡å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„æ•æ„Ÿä¸»é¢˜ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å®ç°å¯¹æ¨¡å‹èƒ½åŠ›çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œåªåœ¨éœ€è¦çš„æ—¶å€™æ¿€æ´»ç‰¹å®šä¸»é¢˜çš„èƒ½åŠ›ï¼Œé¿å…ä¸å¿…è¦çš„é£é™©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMEUVæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œå®šä¹‰ä¸€ç³»åˆ—æ•æ„Ÿä¸»é¢˜ï¼Œä¾‹å¦‚â€œæ­¦å™¨â€ã€â€œçŠ¯ç½ªâ€ç­‰ã€‚ç„¶åï¼Œä¸ºæ¯ä¸ªä¸»é¢˜è®­ç»ƒä¸€ä¸ªå¯¹åº”çš„è§£é”å‘é‡ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ ç›®æ ‡ï¼ŒåŒæ—¶ä¼˜åŒ–å¤šä¸ªç›®æ ‡å‡½æ•°ï¼ŒåŒ…æ‹¬å·®åˆ†æ¶ˆèè£•åº¦ã€è·¨ä¸»é¢˜æƒ©ç½šå’Œæ­£äº¤æ€§æƒ©ç½šç­‰ã€‚æœ€åï¼Œå°†è®­ç»ƒå¥½çš„è§£é”å‘é‡é›†æˆåˆ°å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œé€šè¿‡æ§åˆ¶è¿™äº›å‘é‡çš„æ¿€æ´»çŠ¶æ€ï¼Œå®ç°å¯¹æ¨¡å‹èƒ½åŠ›çš„ç»†ç²’åº¦æ§åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šMEUVçš„å…³é”®åˆ›æ–°åœ¨äºå®ƒå°†å•ä¸€çš„æ‹’ç»æ–¹å‘åˆ†è§£ä¸ºå¤šä¸ªäº’æ–¥çš„å‘é‡ï¼Œä»è€Œå®ç°äº†å¯¹æ¨¡å‹èƒ½åŠ›çš„ç»†ç²’åº¦æ§åˆ¶ã€‚ä¸ç°æœ‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒMEUVå¯ä»¥æ›´åŠ ç²¾ç¡®åœ°æ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºï¼Œé¿å…ä¸å¿…è¦çš„é£é™©ã€‚æ­¤å¤–ï¼ŒMEUVè¿˜å…·æœ‰è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ï¼Œå³åœ¨ä¸€ä¸ªè¯­è¨€ä¸Šè®­ç»ƒçš„è§£é”å‘é‡å¯ä»¥ç›´æ¥åº”ç”¨åˆ°å¦ä¸€ä¸ªè¯­è¨€ä¸Šï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

**å…³é”®è®¾è®¡**ï¼šMEUVçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å·®åˆ†æ¶ˆèè£•åº¦æ¥ç¡®ä¿æ¯ä¸ªè§£é”å‘é‡èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¿€æ´»å¯¹åº”ä¸»é¢˜çš„èƒ½åŠ›ï¼›2) ä½¿ç”¨è·¨ä¸»é¢˜æƒ©ç½šæ¥é™ä½ä¸åŒä¸»é¢˜ä¹‹é—´çš„æ³„æ¼ï¼›3) ä½¿ç”¨æ­£äº¤æ€§æƒ©ç½šæ¥ç¡®ä¿ä¸åŒè§£é”å‘é‡ä¹‹é—´çš„äº’æ–¥æ€§ï¼›4) ä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ ç›®æ ‡æ¥åŒæ—¶ä¼˜åŒ–å¤šä¸ªç›®æ ‡å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MEUVåœ¨Gemma-2-2Bã€LLaMA-3-8Bå’ŒQwen-7Bç­‰æ¨¡å‹ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œæ”»å‡»æˆåŠŸç‡å‡è¾¾åˆ°87%ä»¥ä¸Šã€‚ä¸æœ€ä½³å•æ–¹å‘åŸºçº¿ç›¸æ¯”ï¼Œè·¨ä¸»é¢˜æ³„æ¼é™ä½äº†é«˜è¾¾90%ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œç”¨ä¸­æ–‡è®­ç»ƒçš„å‘é‡å¯ä»¥å‡ ä¹ä¸å˜åœ°è¿ç§»åˆ°è‹±è¯­ï¼Œåä¹‹äº¦ç„¶ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„è·¨è¯­è¨€é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MEUVæŠ€æœ¯å¯åº”ç”¨äºéœ€è¦å¯¹å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›è¿›è¡Œç²¾ç»†æ§åˆ¶çš„åœºæ™¯ï¼Œä¾‹å¦‚åœ¨è­¦åŠ¡é¢†åŸŸï¼Œå¯ä»¥è§£é”æ¨¡å‹åœ¨çŠ¯ç½ªåˆ†ææ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒæ—¶é™åˆ¶å…¶ç”Ÿæˆæœ‰å®³ä¿¡æ¯çš„èƒ½åŠ›ã€‚åœ¨å›½é˜²é¢†åŸŸï¼Œå¯ä»¥ç”¨äºæƒ…æŠ¥åˆ†æå’Œæˆ˜ç•¥è§„åˆ’ï¼ŒåŒæ—¶é¿å…æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚è¯¥æŠ€æœ¯è¿˜æœ‰åŠ©äºåœ¨æ•™è‚²ã€åŒ»ç–—ç­‰é¢†åŸŸå®‰å…¨åœ°éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) enforce safety alignment to reliably refuse malicious requests, yet the same blanket safeguards also block legitimate uses in policing, defense, and other high-stakes settings. Earlier "refusal-direction" edits can bypass those layers, but they rely on a single vector that indiscriminately unlocks all hazardous topics, offering no semantic control. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight framework that factorizes the monolithic refusal direction into topic-aligned, nearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is learned in a single epoch with a multi-task objective that blends a differential-ablation margin, cross-topic and orthogonality penalties, and several auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV achieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B, and Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best single-direction baseline. Vectors trained in Chinese transfer almost unchanged to English (and vice versa), suggesting a language-agnostic refusal subspace. The results show that fine-grained, topic-level capability activation is achievable with minimal utility loss, paving the way for controlled LLMs deployment in security-sensitive domains.

