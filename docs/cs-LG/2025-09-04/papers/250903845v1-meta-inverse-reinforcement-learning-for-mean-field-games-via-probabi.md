---
layout: default
title: Meta-Inverse Reinforcement Learning for Mean Field Games via Probabilistic Context Variables
---

# Meta-Inverse Reinforcement Learning for Mean Field Games via Probabilistic Context Variables

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03845" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03845v1</a>
  <a href="https://arxiv.org/pdf/2509.03845.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03845v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03845v1', 'Meta-Inverse Reinforcement Learning for Mean Field Games via Probabilistic Context Variables')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Chen, Xiao Lin, Bo Yan, Libo Zhang, Jiamou Liu, Neset Ã–zkan Tan, Michael Witbrock

**åˆ†ç±»**: cs.LG, cs.AI, cs.GT

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: Accepted to AAAI 2024

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ¦‚ç‡ä¸Šä¸‹æ–‡å˜é‡çš„å…ƒé€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³å‡å€¼åœºåšå¼ˆä¸­å¼‚æ„æ™ºèƒ½ä½“çš„å¥–åŠ±å‡½æ•°æ¨æ–­é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é€†å¼ºåŒ–å­¦ä¹ ` `å‡å€¼åœºåšå¼ˆ` `æ·±åº¦å­¦ä¹ ` `éšå˜é‡æ¨¡å‹` `å¼‚æ„æ™ºèƒ½ä½“` `å¥–åŠ±å‡½æ•°æ¨æ–­` `ä¸Šä¸‹æ–‡å˜é‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å‡å€¼åœºåšå¼ˆé€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•å‡è®¾æ™ºèƒ½ä½“åŒè´¨ï¼Œæ— æ³•å¤„ç†å¼‚æ„å’ŒæœªçŸ¥ç›®æ ‡çš„æ¼”ç¤ºæ•°æ®ã€‚
2. æå‡ºä¸€ç§æ·±åº¦éšå˜é‡å‡å€¼åœºåšå¼ˆæ¨¡å‹å’Œé€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ— éœ€å…ˆéªŒçŸ¥è¯†å³å¯ä»ä¸åŒä½†ç»“æ„ç›¸ä¼¼çš„ä»»åŠ¡ä¸­æ¨æ–­å¥–åŠ±ã€‚
3. åœ¨æ¨¡æ‹Ÿåœºæ™¯å’ŒçœŸå®å‡ºç§Ÿè½¦å®šä»·é—®é¢˜ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å‡å€¼åœºåšå¼ˆé€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç°å®åº”ç”¨ä¸­ï¼Œä¸ºå¤§é‡äº¤äº’çš„æ™ºèƒ½ä½“è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°æå…·æŒ‘æˆ˜ã€‚å‡å€¼åœºåšå¼ˆï¼ˆMFGï¼‰ä¸­çš„é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰æä¾›äº†ä¸€ä¸ªä»ä¸“å®¶æ¼”ç¤ºä¸­æ¨æ–­å¥–åŠ±å‡½æ•°çš„å®ç”¨æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¯¹æ™ºèƒ½ä½“åŒè´¨æ€§çš„å‡è®¾é™åˆ¶äº†å…¶å¤„ç†å…·æœ‰å¼‚æ„å’ŒæœªçŸ¥ç›®æ ‡çš„æ¼”ç¤ºçš„èƒ½åŠ›ï¼Œè€Œè¿™åœ¨å®è·µä¸­å¾ˆå¸¸è§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ·±åº¦éšå˜é‡MFGæ¨¡å‹å’Œç›¸å…³çš„IRLæ–¹æ³•ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨æ²¡æœ‰å…³äºåº•å±‚ä¸Šä¸‹æ–‡çš„å…ˆéªŒçŸ¥è¯†æˆ–ä¿®æ”¹MFGæ¨¡å‹æœ¬èº«çš„æƒ…å†µä¸‹ï¼Œä»ä¸åŒä½†ç»“æ„ç›¸ä¼¼çš„ä»»åŠ¡ä¸­æ¨æ–­å¥–åŠ±ã€‚åœ¨æ¨¡æ‹Ÿåœºæ™¯å’ŒçœŸå®ä¸–ç•Œçš„ç©ºé—´å‡ºç§Ÿè½¦å®šä»·é—®é¢˜ä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºMFGä¸­æœ€å…ˆè¿›çš„IRLæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å‡å€¼åœºåšå¼ˆï¼ˆMFGï¼‰ä¸­ï¼Œå½“æ™ºèƒ½ä½“å…·æœ‰å¼‚æ„ä¸”æœªçŸ¥çš„ç›®æ ‡æ—¶ï¼Œå¦‚ä½•ä»ä¸“å®¶æ¼”ç¤ºä¸­å‡†ç¡®æ¨æ–­å‡ºæ¯ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±å‡½æ•°çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾æ™ºèƒ½ä½“æ˜¯åŒè´¨çš„ï¼Œå³å…·æœ‰ç›¸åŒçš„å¥–åŠ±å‡½æ•°ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€ä¸æˆç«‹ã€‚è¿™ç§åŒè´¨æ€§å‡è®¾é™åˆ¶äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ã€å¼‚æ„ç¯å¢ƒä¸­çš„èƒ½åŠ›ï¼Œå¯¼è‡´å¥–åŠ±å‡½æ•°æ¨æ–­çš„å‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥æ¦‚ç‡ä¸Šä¸‹æ–‡å˜é‡æ¥è¡¨ç¤ºæ™ºèƒ½ä½“çš„å¼‚æ„ç›®æ ‡ã€‚é€šè¿‡å­¦ä¹ ä¸€ä¸ªæ·±åº¦éšå˜é‡æ¨¡å‹ï¼Œå°†æ¯ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±å‡½æ•°ä¸ä¸€ä¸ªæ½œåœ¨çš„ä¸Šä¸‹æ–‡å˜é‡å…³è”èµ·æ¥ã€‚è¿™æ ·ï¼Œå³ä½¿æ™ºèƒ½ä½“çš„ç›®æ ‡ä¸åŒï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ¨æ–­å…¶å¯¹åº”çš„ä¸Šä¸‹æ–‡å˜é‡æ¥å­¦ä¹ å…¶å¥–åŠ±å‡½æ•°ã€‚è¿™ç§æ–¹æ³•çš„å…³é”®åœ¨äºï¼Œå®ƒä¸éœ€è¦å…³äºåº•å±‚ä¸Šä¸‹æ–‡çš„å…ˆéªŒçŸ¥è¯†ï¼Œè€Œæ˜¯é€šè¿‡å­¦ä¹ ä»æ•°æ®ä¸­è‡ªåŠ¨å‘ç°ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä¸“å®¶æ¼”ç¤ºæ•°æ®æ”¶é›†æ¨¡å—ï¼šæ”¶é›†ä¸åŒæ™ºèƒ½ä½“åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸‹çš„è¡Œä¸ºæ•°æ®ã€‚2) æ·±åº¦éšå˜é‡MFGæ¨¡å‹ï¼šè¯¥æ¨¡å‹å°†æ™ºèƒ½ä½“çš„çŠ¶æ€ã€åŠ¨ä½œå’Œä¸Šä¸‹æ–‡å˜é‡ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºæ™ºèƒ½ä½“çš„å¥–åŠ±å‡½æ•°ã€‚3) é€†å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼šè¯¥æ¨¡å—ä½¿ç”¨ä¸“å®¶æ¼”ç¤ºæ•°æ®æ¥è®­ç»ƒæ·±åº¦éšå˜é‡MFGæ¨¡å‹ï¼Œä»è€Œå­¦ä¹ æ™ºèƒ½ä½“çš„å¥–åŠ±å‡½æ•°ã€‚4) æ¨æ–­æ¨¡å—ï¼šè¯¥æ¨¡å—ç”¨äºæ¨æ–­æ–°æ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡å˜é‡å’Œå¥–åŠ±å‡½æ•°ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œé¦–å…ˆåˆ©ç”¨ä¸“å®¶æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¯¹äºæ–°çš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡è§‚å¯Ÿå…¶è¡Œä¸ºæ¥æ¨æ–­å…¶ä¸Šä¸‹æ–‡å˜é‡ï¼Œè¿›è€Œå¾—åˆ°å…¶å¥–åŠ±å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†æ¦‚ç‡ä¸Šä¸‹æ–‡å˜é‡æ¥è¡¨ç¤ºæ™ºèƒ½ä½“çš„å¼‚æ„ç›®æ ‡ï¼Œå¹¶ä½¿ç”¨æ·±åº¦éšå˜é‡æ¨¡å‹æ¥å­¦ä¹ è¿™äº›ä¸Šä¸‹æ–‡å˜é‡ä¸å¥–åŠ±å‡½æ•°ä¹‹é—´çš„å…³ç³»ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å…³äºåº•å±‚ä¸Šä¸‹æ–‡çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¯ä»¥è‡ªåŠ¨ä»æ•°æ®ä¸­å­¦ä¹ ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥å¤„ç†ä¸åŒä½†ç»“æ„ç›¸ä¼¼çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€ä¿®æ”¹MFGæ¨¡å‹æœ¬èº«ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¥å­¦ä¹ æ½œåœ¨çš„ä¸Šä¸‹æ–‡å˜é‡ã€‚VAEç”±ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ç»„æˆã€‚ç¼–ç å™¨å°†æ™ºèƒ½ä½“çš„çŠ¶æ€å’ŒåŠ¨ä½œæ˜ å°„åˆ°ä¸€ä¸ªæ½œåœ¨çš„ä¸Šä¸‹æ–‡å˜é‡çš„åˆ†å¸ƒï¼Œè§£ç å™¨å°†ä¸Šä¸‹æ–‡å˜é‡æ˜ å°„å›æ™ºèƒ½ä½“çš„å¥–åŠ±å‡½æ•°ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡æ„æŸå¤±å’ŒKLæ•£åº¦æŸå¤±ã€‚é‡æ„æŸå¤±ç”¨äºè¡¡é‡è§£ç å™¨é‡æ„å¥–åŠ±å‡½æ•°çš„å‡†ç¡®æ€§ï¼ŒKLæ•£åº¦æŸå¤±ç”¨äºçº¦æŸæ½œåœ¨å˜é‡çš„åˆ†å¸ƒã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œç¼–ç å™¨å’Œè§£ç å™¨é€šå¸¸ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿåœºæ™¯å’ŒçœŸå®ä¸–ç•Œçš„ç©ºé—´å‡ºç§Ÿè½¦å®šä»·é—®é¢˜ä¸­å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å‡å€¼åœºåšå¼ˆé€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚åœ¨æ¨¡æ‹Ÿåœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ¨æ–­å‡ºæ™ºèƒ½ä½“çš„å¥–åŠ±å‡½æ•°ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨çœŸå®ä¸–ç•Œçš„ç©ºé—´å‡ºç§Ÿè½¦å®šä»·é—®é¢˜ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–å‡ºç§Ÿè½¦å®šä»·ç­–ç•¥ï¼Œä»è€Œæé«˜å‡ºç§Ÿè½¦å¸æœºçš„æ”¶å…¥å’Œä¹˜å®¢çš„æ»¡æ„åº¦ã€‚å…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œä¾‹å¦‚äº¤é€šç®¡ç†ã€èµ„æºåˆ†é…ã€ç¤¾äº¤ç½‘ç»œç­‰ã€‚åœ¨äº¤é€šç®¡ç†ä¸­ï¼Œå¯ä»¥ç”¨äºæ¨æ–­ä¸åŒé©¾é©¶å‘˜çš„é©¾é©¶åå¥½ï¼Œä»è€Œä¼˜åŒ–äº¤é€šæµé‡ã€‚åœ¨èµ„æºåˆ†é…ä¸­ï¼Œå¯ä»¥ç”¨äºæ¨æ–­ä¸åŒç”¨æˆ·çš„éœ€æ±‚ï¼Œä»è€Œå®ç°æ›´å…¬å¹³çš„èµ„æºåˆ†é…ã€‚åœ¨ç¤¾äº¤ç½‘ç»œä¸­ï¼Œå¯ä»¥ç”¨äºæ¨æ–­ä¸åŒç”¨æˆ·çš„å…´è¶£ï¼Œä»è€Œæä¾›æ›´ä¸ªæ€§åŒ–çš„æ¨èæœåŠ¡ã€‚è¯¥ç ”ç©¶å…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Designing suitable reward functions for numerous interacting intelligent agents is challenging in real-world applications. Inverse reinforcement learning (IRL) in mean field games (MFGs) offers a practical framework to infer reward functions from expert demonstrations. While promising, the assumption of agent homogeneity limits the capability of existing methods to handle demonstrations with heterogeneous and unknown objectives, which are common in practice. To this end, we propose a deep latent variable MFG model and an associated IRL method. Critically, our method can infer rewards from different yet structurally similar tasks without prior knowledge about underlying contexts or modifying the MFG model itself. Our experiments, conducted on simulated scenarios and a real-world spatial taxi-ride pricing problem, demonstrate the superiority of our approach over state-of-the-art IRL methods in MFGs.

