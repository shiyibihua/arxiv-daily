---
layout: default
title: What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?
---

# What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03790" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03790v2</a>
  <a href="https://arxiv.org/pdf/2509.03790.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03790v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03790v2', 'What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04 (æ›´æ–°: 2025-09-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPAMCä»¥è§£å†³ç¨€ç–å¥–åŠ±å­¦ä¹ ä¸­çš„æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¨€ç–å¥–åŠ±å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `çŸ©é˜µè¡¥å…¨` `æ ·æœ¬æ•ˆç‡` `æ”¿ç­–åç½®` `æœºå™¨äººæ§åˆ¶` `æ¸¸æˆæ™ºèƒ½ä½“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¨€ç–å¥–åŠ±å¼ºåŒ–å­¦ä¹ ç¼ºä¹æœ‰æ•ˆçš„ç»“æ„ï¼Œå¯¼è‡´æ™ºèƒ½ä½“éœ€è¦å¤§é‡æ ·æœ¬æ‰èƒ½æ¢å¤å¥–åŠ±ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºPAMCï¼Œé€šè¿‡åˆ©ç”¨å¥–åŠ±çŸ©é˜µçš„ä½ç§©å’Œç¨€ç–ç»“æ„ï¼Œåœ¨æ”¿ç­–åç½®é‡‡æ ·ä¸‹æé«˜æ ·æœ¬æ•ˆç‡ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šPAMCåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†DrQ-v2ã€DreamerV3ç­‰æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ ·æœ¬æ•ˆç‡æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¾ç„¶é¢ä¸´æ ¹æœ¬æ€§æŒ‘æˆ˜ï¼šåœ¨ç¼ºä¹ç»“æ„çš„æƒ…å†µä¸‹ï¼Œä»»ä½•æ™ºèƒ½ä½“éœ€è¦$Î©(\|	ext{S}\|\|	ext{A}\|/p)$æ ·æœ¬æ¥æ¢å¤å¥–åŠ±ã€‚æœ¬æ–‡æå‡ºäº†æ”¿ç­–æ„ŸçŸ¥çŸ©é˜µè¡¥å…¨ï¼ˆPAMCï¼‰ï¼Œä½œä¸ºç»“æ„åŒ–å¥–åŠ±å­¦ä¹ æ¡†æ¶çš„é¦–ä¸ªå…·ä½“æ­¥éª¤ã€‚æˆ‘ä»¬çš„å…³é”®æ€æƒ³æ˜¯åˆ©ç”¨å¥–åŠ±çŸ©é˜µä¸­çš„è¿‘ä¼¼ä½ç§©å’Œç¨€ç–ç»“æ„ï¼ŒåŸºäºæ”¿ç­–åç½®ï¼ˆMNARï¼‰é‡‡æ ·ã€‚æˆ‘ä»¬è¯æ˜äº†é€†å€¾å‘åŠ æƒçš„æ¢å¤ä¿è¯ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªè®¿é—®åŠ æƒçš„è¯¯å·®ä¸é—æ†¾ç•Œé™ï¼Œé“¾æ¥è¡¥å…¨è¯¯å·®ä¸æ§åˆ¶æ€§èƒ½ã€‚é‡è¦çš„æ˜¯ï¼Œå½“å‡è®¾å‡å¼±æ—¶ï¼ŒPAMCèƒ½å¤Ÿä¼˜é›…é™çº§ï¼šç½®ä¿¡åŒºé—´æ‰©å¤§ï¼Œç®—æ³•ä¼šé€‰æ‹©ä¸è¡ŒåŠ¨ï¼Œä»è€Œç¡®ä¿å®‰å…¨å›é€€åˆ°æ¢ç´¢é˜¶æ®µã€‚å®è¯ç»“æœæ˜¾ç¤ºï¼ŒPAMCåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œè¶…è¶Šäº†å¤šç§ç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¨€ç–å¥–åŠ±å¼ºåŒ–å­¦ä¹ ä¸­çš„æ ·æœ¬æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç¼ºä¹å¥–åŠ±ç»“æ„çš„æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“éœ€è¦å¤§é‡æ ·æœ¬æ‰èƒ½æœ‰æ•ˆå­¦ä¹ ï¼Œå¯¼è‡´å­¦ä¹ è¿‡ç¨‹ç¼“æ…¢ä¸”ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„PAMCæ–¹æ³•åˆ©ç”¨å¥–åŠ±çŸ©é˜µçš„è¿‘ä¼¼ä½ç§©å’Œç¨€ç–ç»“æ„ï¼Œç»“åˆæ”¿ç­–åç½®é‡‡æ ·ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–çš„æ–¹å¼æé«˜æ ·æœ¬æ•ˆç‡ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›åˆ°æœ‰æ•ˆç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPAMCçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡æ”¿ç­–åç½®é‡‡æ ·æ”¶é›†æ•°æ®ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨çŸ©é˜µè¡¥å…¨æŠ€æœ¯æ¢å¤å¥–åŠ±çŸ©é˜µï¼›æœ€åï¼ŒåŸºäºæ¢å¤çš„å¥–åŠ±è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šPAMCçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†é€†å€¾å‘åŠ æƒçš„æ¢å¤ä¿è¯ï¼Œå¹¶å»ºç«‹äº†è¯¯å·®ä¸æ§åˆ¶æ€§èƒ½ä¹‹é—´çš„è”ç³»ã€‚è¿™ä¸€æ–¹æ³•åœ¨å‡è®¾å‡å¼±æ—¶èƒ½å¤Ÿä¼˜é›…é™çº§ï¼Œç¡®ä¿å®‰å…¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šPAMCçš„è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬é‡‡æ ·ç­–ç•¥å’ŒçŸ©é˜µè¡¥å…¨ç®—æ³•çš„é€‰æ‹©ï¼ŒæŸå¤±å‡½æ•°åˆ™åŸºäºæ¢å¤è¯¯å·®ä¸ç­–ç•¥æ€§èƒ½ä¹‹é—´çš„å…³ç³»è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPAMCåœ¨Atari-26ã€DM Controlã€MetaWorld MT50ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ ·æœ¬æ•ˆç‡æ˜¾è‘—æé«˜ï¼Œè¶…è¶Šäº†DrQ-v2ã€DreamerV3ç­‰å¤šç§ç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨10Mæ­¥å†…çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“è®­ç»ƒå’Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æé«˜ç¨€ç–å¥–åŠ±å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ï¼ŒPAMCèƒ½å¤ŸåŠ é€Ÿæ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse-reward reinforcement learning (RL) remains fundamentally hard: without structure, any agent needs $Î©(\|\mathcal{S}\|\|\mathcal{A}\|/p)$ samples to recover rewards. We introduce Policy-Aware Matrix Completion (PAMC) as a first concrete step toward a structural reward learning framework. Our key idea is to exploit approximate low-rank + sparse structure in the reward matrix, under policy-biased (MNAR) sampling. We prove recovery guarantees with inverse-propensity weighting, and establish a visitation-weighted error-to-regret bound linking completion error to control performance. Importantly, when assumptions weaken, PAMC degrades gracefully: confidence intervals widen and the algorithm abstains, ensuring safe fallback to exploration. Empirically, PAMC improves sample efficiency across Atari-26 (10M steps), DM Control, MetaWorld MT50, D4RL offline RL, and preference-based RL benchmarks, outperforming DrQ-v2, DreamerV3, Agent57, T-REX/D-REX, and PrefPPO under compute-normalized comparisons. Our results highlight PAMC as a practical and principled tool when structural rewards exist, and as a concrete first instantiation of a broader structural reward learning perspective.

