---
layout: default
title: RL's Razor: Why Online Reinforcement Learning Forgets Less
---

# RL's Razor: Why Online Reinforcement Learning Forgets Less

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04259" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04259v1</a>
  <a href="https://arxiv.org/pdf/2509.04259.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04259v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04259v1', 'RL\'s Razor: Why Online Reinforcement Learning Forgets Less')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Idan Shenfeld, Jyothish Pari, Pulkit Agrawal

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºRLçš„â€œå¥¥å¡å§†å‰ƒåˆ€â€ï¼šåœ¨çº¿å¼ºåŒ–å­¦ä¹ åœ¨å¾®è°ƒä¸­èƒ½æ›´å¥½ä¿ç•™å…ˆéªŒçŸ¥è¯†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¾®è°ƒ` `çŸ¥è¯†ä¿ç•™` `ç¾éš¾æ€§é—å¿˜` `KLæ•£åº¦` `åœ¨çº¿å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶ï¼Œå®¹æ˜“è¿‡åº¦æ‹Ÿåˆæ–°æ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹é—å¿˜åŸæœ‰çš„çŸ¥è¯†å’Œèƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºåœ¨çº¿å¼ºåŒ–å­¦ä¹ ç”±äºå…¶å†…åœ¨çš„KLæ•£åº¦æœ€å°åŒ–åå¥½ï¼Œèƒ½æ›´å¥½åœ°åœ¨é€‚åº”æ–°ä»»åŠ¡çš„åŒæ—¶ä¿ç•™å…ˆéªŒçŸ¥è¯†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç›¸æ¯”ç›‘ç£å¾®è°ƒï¼Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æ¨¡å‹åœ¨ä¿ç•™å…ˆéªŒçŸ¥è¯†æ–¹é¢æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶æä¾›äº†ç†è®ºæ”¯æŒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶å¯¹æ¯”äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ•ˆæœï¼Œå‘ç°å°½ç®¡ä¸¤è€…åœ¨æ–°ä»»åŠ¡ä¸Šè¡¨ç°ç›¸ä¼¼ï¼Œä½†RLåœ¨ä¿ç•™å…ˆéªŒçŸ¥è¯†å’Œèƒ½åŠ›æ–¹é¢æ˜æ˜¾æ›´èƒœä¸€ç­¹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé—å¿˜ç¨‹åº¦å–å†³äºåˆ†å¸ƒåç§»ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æ–°ä»»åŠ¡ä¸Šè¯„ä¼°çš„å¾®è°ƒåç­–ç•¥ä¸åŸºç¡€ç­–ç•¥ä¹‹é—´çš„KLæ•£åº¦ã€‚åˆ†ææ­ç¤ºï¼Œåœ¨çº¿RLéšå¼åœ°åå‘äºKLæ•£åº¦æœ€å°çš„è§£ï¼Œè€ŒSFTå¯èƒ½æ”¶æ•›åˆ°ä¸åŸºç¡€æ¨¡å‹ç›¸è·ç”šè¿œçš„åˆ†å¸ƒã€‚é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å’Œæœºå™¨äººåŸºç¡€æ¨¡å‹çš„å®éªŒéªŒè¯äº†è¿™äº›å‘ç°ï¼Œå¹¶æä¾›äº†ç†è®ºä¾æ®æ¥è§£é‡Šä¸ºä»€ä¹ˆåœ¨çº¿RLæ›´æ–°ä¼šå¯¼è‡´æ›´å°çš„KLå˜åŒ–ã€‚æˆ‘ä»¬å°†æ­¤åŸåˆ™ç§°ä¸ºâ€œRLçš„å¥¥å¡å§†å‰ƒåˆ€â€ï¼šåœ¨è§£å†³æ–°ä»»åŠ¡çš„æ‰€æœ‰æ–¹æ³•ä¸­ï¼ŒRLå€¾å‘äºé€‰æ‹©KLæ•£åº¦ä¸Šæœ€æ¥è¿‘åŸå§‹æ¨¡å‹çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­é—å¿˜å…ˆéªŒçŸ¥è¯†çš„é—®é¢˜ã€‚ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œè™½ç„¶èƒ½å¤Ÿä½¿æ¨¡å‹é€‚åº”æ–°ä»»åŠ¡ï¼Œä½†å¾€å¾€ä¼šæ˜¾è‘—æ”¹å˜æ¨¡å‹çš„å‚æ•°åˆ†å¸ƒï¼Œå¯¼è‡´æ¨¡å‹å¤±å»åŸæœ¬å…·å¤‡çš„èƒ½åŠ›ã€‚è¿™ç§â€œç¾éš¾æ€§é—å¿˜â€ç°è±¡é™åˆ¶äº†æ¨¡å‹åœ¨å¤šä»»åŠ¡åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOn-policy RLï¼‰ç®—æ³•åœ¨æ›´æ–°ç­–ç•¥æ—¶ï¼Œå¤©ç„¶åœ°å€¾å‘äºé€‰æ‹©ä¸åŸå§‹ç­–ç•¥KLæ•£åº¦æœ€å°çš„è§£ã€‚è¿™æ„å‘³ç€ï¼Œåœ¨æ‰€æœ‰èƒ½å¤Ÿè§£å†³æ–°ä»»åŠ¡çš„ç­–ç•¥ä¸­ï¼ŒRLä¼šä¼˜å…ˆé€‰æ‹©é‚£äº›ä¸åŸå§‹æ¨¡å‹æœ€ç›¸ä¼¼çš„ç­–ç•¥ï¼Œä»è€Œå‡å°‘å¯¹åŸå§‹çŸ¥è¯†çš„å¹²æ‰°ï¼Œé™ä½é—å¿˜çš„é£é™©ã€‚è¿™ç§åå¥½ç±»ä¼¼äºâ€œå¥¥å¡å§†å‰ƒåˆ€â€åŸåˆ™ï¼Œå³é€‰æ‹©æœ€ç®€å•çš„è§£é‡Šã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é€šè¿‡æ¯”è¾ƒRLå’ŒSFTä¸¤ç§å¾®è°ƒæ–¹æ³•ï¼Œæ¥éªŒè¯å…¶æ ¸å¿ƒæ€è·¯ã€‚å…·ä½“è€Œè¨€ï¼Œé¦–å…ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œç„¶ååˆ†åˆ«ä½¿ç”¨RLå’ŒSFTå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”æ–°çš„ä»»åŠ¡ã€‚æ¥ç€ï¼Œè¯„ä¼°å¾®è°ƒåæ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä»¥åŠåœ¨åŸå§‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä»è€Œè¡¡é‡æ¨¡å‹çš„é—å¿˜ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¡ç®—äº†å¾®è°ƒåç­–ç•¥ä¸åŸºç¡€ç­–ç•¥ä¹‹é—´çš„KLæ•£åº¦ï¼Œä»¥é‡åŒ–åˆ†å¸ƒåç§»çš„å¤§å°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå‘ç°äº†åœ¨çº¿å¼ºåŒ–å­¦ä¹ åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å…·æœ‰KLæ•£åº¦æœ€å°åŒ–çš„åå¥½ï¼Œå¹¶å°†å…¶å‘½åä¸ºâ€œRLçš„å¥¥å¡å§†å‰ƒåˆ€â€ã€‚è¿™ä¸€å‘ç°è§£é‡Šäº†ä¸ºä»€ä¹ˆRLåœ¨ä¿ç•™å…ˆéªŒçŸ¥è¯†æ–¹é¢ä¼˜äºç›‘ç£å¾®è°ƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶ä¸ä»…å…³æ³¨æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæ›´å…³æ³¨æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯¹åŸå§‹çŸ¥è¯†çš„ä¿ç•™ç¨‹åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨KLæ•£åº¦ä½œä¸ºè¡¡é‡åˆ†å¸ƒåç§»çš„æŒ‡æ ‡ï¼›2) å¯¹æ¯”åœ¨çº¿RLå’ŒSFTä¸¤ç§å¾®è°ƒæ–¹æ³•ï¼›3) åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæœºå™¨äººåŸºç¡€æ¨¡å‹ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼›4) æä¾›ç†è®ºåˆ†ææ¥æ”¯æŒå®éªŒç»“æœã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„æ ¹æ®æ‰€ä½¿ç”¨çš„å…·ä½“æ¨¡å‹ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹æˆ–æœºå™¨äººæ¨¡å‹ï¼‰è€Œæœ‰æ‰€ä¸åŒï¼Œä½†æ ¸å¿ƒæ€æƒ³éƒ½æ˜¯é€šè¿‡æ§åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦ï¼Œæ¥é™åˆ¶KLæ•£åº¦çš„å¢é•¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„æ–°ä»»åŠ¡æ€§èƒ½ä¸‹ï¼Œä½¿ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æ¨¡å‹ï¼Œå…¶åœ¨åŸå§‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸‹é™æ˜æ˜¾å°äºä½¿ç”¨ç›‘ç£å¾®è°ƒçš„æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒKLæ•£åº¦è¶Šå°ï¼Œé—å¿˜ç¨‹åº¦è¶Šä½ï¼ŒéªŒè¯äº†â€œRLçš„å¥¥å¡å§†å‰ƒåˆ€â€åŸåˆ™ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæœºå™¨äººåŸºç¡€æ¨¡å‹ä¸Šçš„å®éªŒå‡æ”¯æŒäº†è¿™ä¸€ç»“è®ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦æŒç»­å­¦ä¹ å’ŒçŸ¥è¯†è¿ç§»çš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚é€šè¿‡ä½¿ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥ä½¿æ¨¡å‹åœ¨é€‚åº”æ–°ä»»åŠ¡çš„åŒæ—¶ï¼Œæ›´å¥½åœ°ä¿ç•™å·²æœ‰çš„çŸ¥è¯†å’Œèƒ½åŠ›ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚è¿™å¯¹äºå¼€å‘èƒ½å¤Ÿé€‚åº”å¤æ‚å¤šå˜ç¯å¢ƒçš„æ™ºèƒ½ç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Comparison of fine-tuning models with reinforcement learning (RL) and supervised fine-tuning (SFT) reveals that, despite similar performance at a new task, RL preserves prior knowledge and capabilities significantly better. We find that the degree of forgetting is determined by the distributional shift, measured as the KL-divergence between the fine-tuned and base policy evaluated on the new task. Our analysis reveals that on-policy RL is implicitly biased towards KL-minimal solutions among the many that solve the new task, whereas SFT can converge to distributions arbitrarily far from the base model. We validate these findings through experiments with large language models and robotic foundation models and further provide theoretical justification for why on-policy RL updates lead to a smaller KL change. We term this principle $\textit{RL's Razor}$: among all ways to solve a new task, RL prefers those closest in KL to the original model.

