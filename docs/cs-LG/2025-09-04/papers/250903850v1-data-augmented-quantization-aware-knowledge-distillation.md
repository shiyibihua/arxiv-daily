---
layout: default
title: Data-Augmented Quantization-Aware Knowledge Distillation
---

# Data-Augmented Quantization-Aware Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03850" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03850v1</a>
  <a href="https://arxiv.org/pdf/2509.03850.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03850v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03850v1', 'Data-Augmented Quantization-Aware Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Justin Kur, Kaiqi Zhao

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: 10 pages, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®å¢å¼ºæ„ŸçŸ¥çš„é‡åŒ–çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œæå‡ä½æ¯”ç‰¹æ¨¡å‹ç²¾åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ` `çŸ¥è¯†è’¸é¦` `æ•°æ®å¢å¼º` `ä½æ¯”ç‰¹æ¨¡å‹` `ä¸Šä¸‹æ–‡äº’ä¿¡æ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’ŒçŸ¥è¯†è’¸é¦æ–¹æ³•å¿½ç•¥äº†æ•°æ®å¢å¼ºå¯¹ä½æ¯”ç‰¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚
2. æå‡ºä¸€ç§æ–°æŒ‡æ ‡ï¼Œè¯„ä¼°æ•°æ®å¢å¼ºåœ¨æœ€å¤§åŒ–ä¸Šä¸‹æ–‡äº’ä¿¡æ¯å’Œä¿æŒé¢„æµ‹å‡†ç¡®æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æŒ‡æ ‡é€‰æ‹©çš„æ•°æ®å¢å¼ºç­–ç•¥èƒ½æ˜¾è‘—æå‡ç°æœ‰é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’ŒçŸ¥è¯†è’¸é¦æ–¹æ³•çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç»“åˆé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’ŒçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ï¼Œæ—¨åœ¨åˆ›å»ºå…·æœ‰ç«äº‰åŠ›çš„ä½æ¯”ç‰¹æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç°æœ‰KDå’ŒQATå·¥ä½œä¸»è¦ä»ç½‘ç»œè¾“å‡ºçš„è§’åº¦ï¼Œé€šè¿‡è®¾è®¡æ›´å¥½çš„KDæŸå¤±å‡½æ•°æˆ–ä¼˜åŒ–QATçš„å‰å‘å’Œåå‘ä¼ æ’­æ¥æé«˜é‡åŒ–æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå¯¹è¾“å…¥è½¬æ¢ï¼ˆå¦‚æ•°æ®å¢å¼ºï¼ˆDAï¼‰ï¼‰çš„å½±å“å…³æ³¨ä¸è¶³ã€‚é‡åŒ–æ„ŸçŸ¥KDå’ŒDAä¹‹é—´çš„å…³ç³»ä»æœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³ä»¥ä¸‹é—®é¢˜ï¼šå¦‚ä½•åœ¨é‡åŒ–æ„ŸçŸ¥KDä¸­é€‰æ‹©å¥½çš„DAï¼Œç‰¹åˆ«æ˜¯å¯¹äºä½ç²¾åº¦æ¨¡å‹ï¼Ÿæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡æ ¹æ®DAæœ€å¤§åŒ–ä¸Šä¸‹æ–‡äº’ä¿¡æ¯ï¼ˆä¸å›¾åƒæ ‡ç­¾ä¸ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ï¼‰çš„èƒ½åŠ›æ¥è¯„ä¼°DAï¼ŒåŒæ—¶ç¡®ä¿æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹å¹³å‡æ¥è¿‘çœŸå®æ ‡ç­¾ã€‚æ‰€æå‡ºçš„æ–¹æ³•è‡ªåŠ¨å¯¹DAè¿›è¡Œæ’åºå’Œé€‰æ‹©ï¼Œæ‰€éœ€çš„è®­ç»ƒå¼€é”€æœ€å°ï¼Œå¹¶ä¸”ä¸ä»»ä½•KDæˆ–QATç®—æ³•å…¼å®¹ã€‚å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æŒ‡æ ‡é€‰æ‹©DAç­–ç•¥å¯ä»¥æ˜¾è‘—æ”¹å–„å„ç§æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†ä¸Šçš„æœ€å…ˆè¿›çš„QATå’ŒKDå·¥ä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’ŒçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ–¹æ³•åœ¨è®¾è®¡ä½æ¯”ç‰¹æ¨¡å‹æ—¶ï¼Œä¸»è¦å…³æ³¨ç½‘ç»œè¾“å‡ºçš„ä¼˜åŒ–ï¼Œä¾‹å¦‚è®¾è®¡æ›´å¥½çš„KDæŸå¤±å‡½æ•°æˆ–ä¼˜åŒ–QATçš„å‰å‘å’Œåå‘ä¼ æ’­ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¿½ç•¥äº†æ•°æ®å¢å¼ºï¼ˆDAï¼‰å¯¹é‡åŒ–æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä½ç²¾åº¦æ¨¡å‹ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„DAç­–ç•¥ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æå‡ºä¸€ç§æ–°çš„æŒ‡æ ‡æ¥è¯„ä¼°ä¸åŒçš„DAç­–ç•¥ï¼Œè¯¥æŒ‡æ ‡ä¸ä»…è¦è€ƒè™‘DAèƒ½å¦æœ€å¤§åŒ–ä¸Šä¸‹æ–‡äº’ä¿¡æ¯ï¼ˆå³ä¸å›¾åƒæ ‡ç­¾æ— å…³çš„ä¿¡æ¯ï¼‰ï¼Œè¿˜è¦ä¿è¯æ¨¡å‹å¯¹æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹å¹³å‡æ¥è¿‘çœŸå®æ ‡ç­¾ã€‚é€šè¿‡å¹³è¡¡è¿™ä¸¤ä¸ªæ–¹é¢ï¼Œå¯ä»¥æ‰¾åˆ°æœ€é€‚åˆé‡åŒ–æ„ŸçŸ¥KDçš„DAç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) å®šä¹‰ä¸Šä¸‹æ–‡äº’ä¿¡æ¯ï¼›2) è®¾è®¡è¯„ä¼°DAç­–ç•¥çš„æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡åŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡äº’ä¿¡æ¯å’Œé¢„æµ‹å‡†ç¡®æ€§ï¼›3) ä½¿ç”¨è¯¥æŒ‡æ ‡å¯¹ä¸åŒçš„DAç­–ç•¥è¿›è¡Œæ’åºï¼›4) é€‰æ‹©æ’åæœ€é«˜çš„DAç­–ç•¥ç”¨äºQATå’ŒKDè®­ç»ƒã€‚è¯¥æ¡†æ¶å¯ä»¥ä¸ä»»ä½•ç°æœ‰çš„QATå’ŒKDç®—æ³•ç»“åˆä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°DAç­–ç•¥çš„æ–°æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡èƒ½å¤ŸåŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡äº’ä¿¡æ¯å’Œé¢„æµ‹å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¯¥æŒ‡æ ‡ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒå¼€é”€ï¼Œå¹¶ä¸”å¯ä»¥è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„DAç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æŒ‡æ ‡çš„è®¾è®¡æ˜¯å…³é”®ã€‚å…·ä½“æ¥è¯´ï¼Œä¸Šä¸‹æ–‡äº’ä¿¡æ¯çš„è®¡ç®—æ–¹å¼ä»¥åŠå¦‚ä½•å°†å…¶ä¸é¢„æµ‹å‡†ç¡®æ€§ç›¸ç»“åˆï¼Œä»¥å½¢æˆä¸€ä¸ªç»¼åˆçš„è¯„ä¼°æŒ‡æ ‡æ˜¯éœ€è¦ä»”ç»†è®¾è®¡çš„ã€‚æ­¤å¤–ï¼Œå¦‚ä½•é«˜æ•ˆåœ°è®¡ç®—è¯¥æŒ‡æ ‡ï¼Œä»¥é¿å…å¼•å…¥è¿‡å¤šçš„è®¡ç®—å¼€é”€ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ ã€‚è®ºæ–‡ä¸­å¯èƒ½è¿˜æ¶‰åŠä¸€äº›è¶…å‚æ•°çš„è®¾ç½®ï¼Œä¾‹å¦‚ç”¨äºå¹³è¡¡ä¸Šä¸‹æ–‡äº’ä¿¡æ¯å’Œé¢„æµ‹å‡†ç¡®æ€§çš„æƒé‡ç³»æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ–¹æ³•é€‰æ‹©çš„æ•°æ®å¢å¼ºç­–ç•¥å¯ä»¥æ˜¾è‘—æå‡ç°æœ‰QATå’ŒKDæ–¹æ³•çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨è¯¥æ–¹æ³•é€‰æ‹©çš„DAç­–ç•¥å¯ä»¥å°†ResNet-18æ¨¡å‹çš„ç²¾åº¦æé«˜1-2ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦éƒ¨ç½²ä½æ¯”ç‰¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿå’Œè¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€‚é€šè¿‡è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è¿™äº›è®¾å¤‡ä¸Šæ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆã€æ›´å‡†ç¡®çš„å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ã€‚è¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºé™ä½æ¨¡å‹çš„å­˜å‚¨ç©ºé—´å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä½¿å…¶æ›´æ˜“äºéƒ¨ç½²å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined to achieve competitive performance in creating low-bit deep learning models. Existing KD and QAT works focus on improving the accuracy of quantized models from the network output perspective by designing better KD loss functions or optimizing QAT's forward and backward propagation. However, limited attention has been given to understanding the impact of input transformations, such as data augmentation (DA). The relationship between quantization-aware KD and DA remains unexplored. In this paper, we address the question: how to select a good DA in quantization-aware KD, especially for the models with low precisions? We propose a novel metric which evaluates DAs according to their capacity to maximize the Contextual Mutual Information--the information not directly related to an image's label--while also ensuring the predictions for each class are close to the ground truth labels on average. The proposed method automatically ranks and selects DAs, requiring minimal training overhead, and it is compatible with any KD or QAT algorithm. Extensive evaluations demonstrate that selecting DA strategies using our metric significantly improves state-of-the-art QAT and KD works across various model architectures and datasets.

