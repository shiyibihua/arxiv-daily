---
layout: default
title: Towards a Unified View of Large Language Model Post-Training
---

# Towards a Unified View of Large Language Model Post-Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04419" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04419v1</a>
  <a href="https://arxiv.org/pdf/2509.04419.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04419v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04419v1', 'Towards a Unified View of Large Language Model Post-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingtai Lv, Yuxin Zuo, Youbang Sun, Hongyi Liu, Yuntian Wei, Zhekai Chen, Lixuan He, Xuekai Zhu, Kaiyan Zhang, Bingning Wang, Ning Ding, Bowen Zhou

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»Ÿä¸€å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒè§†è§’ï¼Œæå‡ºæ··åˆåè®­ç»ƒç®—æ³•HPTï¼Œæå‡æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `åè®­ç»ƒ` `å¼ºåŒ–å­¦ä¹ ` `ç›‘ç£å¾®è°ƒ` `ç­–ç•¥æ¢¯åº¦` `æ•°å­¦æ¨ç†` `æ··åˆè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åè®­ç»ƒæ–¹æ³•ä¾èµ–äºä¸åŒç±»å‹çš„æ•°æ®ï¼ˆåœ¨çº¿/ç¦»çº¿ï¼‰ï¼Œå¯¼è‡´è®­ç»ƒç›®æ ‡ä¸ä¸€è‡´ï¼Œç¼ºä¹ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ã€‚
2. è®ºæ–‡æå‡ºç»Ÿä¸€ç­–ç•¥æ¢¯åº¦ä¼°è®¡å™¨ï¼Œå°†ä¸åŒåè®­ç»ƒæ–¹æ³•è§†ä¸ºåŒä¸€ä¼˜åŒ–ç›®æ ‡çš„ç‰¹æ®Šæƒ…å†µï¼Œå¹¶è®¾è®¡æ··åˆåè®­ç»ƒç®—æ³•HPTã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHPTåœ¨æ•°å­¦æ¨ç†å’Œåˆ†å¸ƒå¤–æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†ç»Ÿä¸€æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹åè®­ç»ƒçš„è§†è§’ï¼Œè®¤ä¸ºåœ¨çº¿æ•°æ®ï¼ˆæ¨¡å‹ç”Ÿæˆï¼‰å’Œç¦»çº¿æ•°æ®ï¼ˆäººå·¥æˆ–å…¶å®ƒæ¨¡å‹æ¼”ç¤ºï¼‰é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•å¹¶éçŸ›ç›¾ï¼Œè€Œæ˜¯åŒä¸€ä¼˜åŒ–è¿‡ç¨‹çš„ä¸åŒå®ä¾‹ã€‚è®ºæ–‡æ¨å¯¼äº†ä¸€ä¸ªç»Ÿä¸€çš„ç­–ç•¥æ¢¯åº¦ä¼°è®¡å™¨ï¼Œå°†å¤šç§åè®­ç»ƒæ–¹æ³•è§†ä¸ºåœ¨ä¸åŒæ•°æ®åˆ†å¸ƒå‡è®¾å’Œåå·®-æ–¹å·®æƒè¡¡ä¸‹ï¼Œå¯¹å…±åŒç›®æ ‡å‡½æ•°æ¢¯åº¦è®¡ç®—çš„ä¸åŒå½¢å¼ã€‚è¯¥æ¢¯åº¦ä¼°è®¡å™¨ç”±å››ä¸ªå¯äº’æ¢çš„éƒ¨åˆ†ç»„æˆï¼šç¨³å®šæ©ç ã€å‚è€ƒç­–ç•¥åˆ†æ¯ã€ä¼˜åŠ¿ä¼°è®¡å’Œä¼¼ç„¶æ¢¯åº¦ã€‚å—ç†è®ºå‘ç°çš„å¯å‘ï¼Œè®ºæ–‡æå‡ºæ··åˆåè®­ç»ƒï¼ˆHPTï¼‰ç®—æ³•ï¼ŒåŠ¨æ€é€‰æ‹©ä¸åŒçš„è®­ç»ƒä¿¡å·ï¼Œæ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨æ¼”ç¤ºæ•°æ®å¹¶å®ç°ç¨³å®šçš„æ¢ç´¢ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å·²å­¦ä¹ çš„æ¨ç†æ¨¡å¼ã€‚å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶éªŒè¯äº†ç»Ÿä¸€ç†è®ºæ¡†æ¶å’ŒHPTçš„æœ‰æ•ˆæ€§ã€‚åœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å’Œä¸¤ä¸ªåˆ†å¸ƒå¤–æµ‹è¯•ä¸­ï¼ŒHPTå§‹ç»ˆä¼˜äºå„ç§è§„æ¨¡å’Œç³»åˆ—çš„å¼ºå¤§åŸºçº¿æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒæ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºåœ¨çº¿æ•°æ®çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’ŒåŸºäºç¦»çº¿æ•°æ®çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚è¿™ä¸¤ç§æ–¹æ³•ä½¿ç”¨çš„æ•°æ®æ¥æºä¸åŒï¼Œè®­ç»ƒç›®æ ‡ä¹Ÿä¸åŒï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒä¸ç¨³å®šï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨ä¸åŒç±»å‹æ•°æ®çš„ä¼˜åŠ¿ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„ç†è®ºæ¡†æ¶æ¥è§£é‡Šå’Œæ•´åˆè¿™ä¸¤ç§è®­ç»ƒæ–¹å¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†RLå’ŒSFTè§†ä¸ºåŒä¸€ä¼˜åŒ–è¿‡ç¨‹çš„ä¸åŒå®ä¾‹ï¼Œé€šè¿‡æ¨å¯¼ä¸€ä¸ªç»Ÿä¸€çš„ç­–ç•¥æ¢¯åº¦ä¼°è®¡å™¨ï¼Œå°†ä¸åŒçš„åè®­ç»ƒæ–¹æ³•çº³å…¥ä¸€ä¸ªå…±åŒçš„ç†è®ºæ¡†æ¶ã€‚è¿™ä¸ªç»Ÿä¸€çš„è§†è§’å…è®¸æˆ‘ä»¬æ›´å¥½åœ°ç†è§£ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶è®¾è®¡å‡ºæ›´æœ‰æ•ˆçš„æ··åˆè®­ç»ƒç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç­–ç•¥æ¢¯åº¦ä¼°è®¡å™¨ï¼Œè¯¥ä¼°è®¡å™¨ç”±å››ä¸ªå¯äº’æ¢çš„éƒ¨åˆ†ç»„æˆï¼šç¨³å®šæ©ç ã€å‚è€ƒç­–ç•¥åˆ†æ¯ã€ä¼˜åŠ¿ä¼°è®¡å’Œä¼¼ç„¶æ¢¯åº¦ã€‚é€šè¿‡é€‰æ‹©ä¸åŒçš„ç»„ä»¶ï¼Œå¯ä»¥å¾—åˆ°ä¸åŒçš„åè®­ç»ƒæ–¹æ³•ã€‚åŸºäºè¿™ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œè®ºæ–‡æå‡ºäº†æ··åˆåè®­ç»ƒï¼ˆHPTï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åŠ¨æ€åœ°é€‰æ‹©ä¸åŒçš„è®­ç»ƒä¿¡å·ï¼Œä»¥å¹³è¡¡åˆ©ç”¨æ¼”ç¤ºæ•°æ®å’Œæ¢ç´¢æ–°ç­–ç•¥ã€‚HPTç®—æ³•æ—¨åœ¨å®ç°æœ‰æ•ˆçš„åˆ©ç”¨å’Œç¨³å®šçš„æ¢ç´¢ï¼ŒåŒæ—¶ä¿ç•™å·²å­¦ä¹ çš„æ¨ç†æ¨¡å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç­–ç•¥æ¢¯åº¦ä¼°è®¡å™¨ï¼Œå°†RLå’ŒSFTæ–¹æ³•ç»Ÿä¸€åˆ°ä¸€ä¸ªç†è®ºæ¡†æ¶ä¸‹ã€‚è¿™ä¸ªç»Ÿä¸€çš„æ¡†æ¶å…è®¸æˆ‘ä»¬æ›´å¥½åœ°ç†è§£ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶è®¾è®¡å‡ºæ›´æœ‰æ•ˆçš„æ··åˆè®­ç»ƒç­–ç•¥ã€‚HPTç®—æ³•æ˜¯åŸºäºè¿™ä¸ªç»Ÿä¸€æ¡†æ¶çš„å…·ä½“å®ç°ï¼Œå®ƒèƒ½å¤ŸåŠ¨æ€åœ°é€‰æ‹©ä¸åŒçš„è®­ç»ƒä¿¡å·ï¼Œä»¥å¹³è¡¡åˆ©ç”¨æ¼”ç¤ºæ•°æ®å’Œæ¢ç´¢æ–°ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šHPTç®—æ³•çš„å…³é”®è®¾è®¡åœ¨äºåŠ¨æ€é€‰æ‹©è®­ç»ƒä¿¡å·çš„æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒHPTç®—æ³•ä¼šæ ¹æ®å½“å‰æ¨¡å‹çš„è¡¨ç°å’Œæ•°æ®çš„è´¨é‡ï¼ŒåŠ¨æ€åœ°è°ƒæ•´ä¸åŒè®­ç»ƒä¿¡å·çš„æƒé‡ã€‚ä¾‹å¦‚ï¼Œå½“æ¨¡å‹è¡¨ç°è¾ƒå·®æ—¶ï¼ŒHPTç®—æ³•ä¼šæ›´å¤šåœ°ä¾èµ–äºæ¼”ç¤ºæ•°æ®ï¼Œä»¥å¿«é€Ÿæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚å½“æ¨¡å‹è¡¨ç°è¾ƒå¥½æ—¶ï¼ŒHPTç®—æ³•ä¼šæ›´å¤šåœ°ä¾èµ–äºåœ¨çº¿æ•°æ®ï¼Œä»¥æ¢ç´¢æ–°çš„ç­–ç•¥ã€‚è¿™ç§åŠ¨æ€è°ƒæ•´æœºåˆ¶ä½¿å¾—HPTç®—æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡åˆ©ç”¨å’Œæ¢ç´¢ï¼Œä»è€Œè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHPTç®—æ³•åœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å’Œä¸¤ä¸ªåˆ†å¸ƒå¤–æµ‹è¯•ä¸­ï¼Œå§‹ç»ˆä¼˜äºå„ç§è§„æ¨¡å’Œç³»åˆ—çš„å¼ºå¤§åŸºçº¿æ¨¡å‹ã€‚è¿™è¡¨æ˜HPTç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ¼”ç¤ºæ•°æ®å¹¶å®ç°ç¨³å®šçš„æ¢ç´¢ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å·²å­¦ä¹ çš„æ¨ç†æ¨¡å¼ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHPTç®—æ³•ç›¸æ¯”äºæœ€å¼ºçš„åŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡äº†è¶…è¿‡5%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œåè®­ç»ƒçš„åœºæ™¯ï¼Œä¾‹å¦‚æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€ç”Ÿæˆèƒ½åŠ›å’Œå¯¹è¯èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»“åˆäººå·¥åé¦ˆå’Œæ¨¡å‹è‡ªèº«æ¢ç´¢çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢è¯¥æ–¹æ³•åœ¨å…¶ä»–ç±»å‹ä»»åŠ¡å’Œæ¨¡å‹ä¸Šçš„åº”ç”¨ï¼Œå¹¶ç ”ç©¶å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨ä¸åŒç±»å‹çš„æ•°æ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families.

