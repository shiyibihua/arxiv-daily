---
layout: default
title: Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation
---

# Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05713" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05713v2</a>
  <a href="https://arxiv.org/pdf/2506.05713.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05713v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05713v2', 'Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhan Zhuang, Xiequn Wang, Wei Li, Yulong Zhang, Qiushi Huang, Shuhao Chen, Xuehao Wang, Yanbin Wei, Yuhe Nie, Kede Ma, Yu Zhang, Ying Wei

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-07-27)

**å¤‡æ³¨**: Accepted by ICML 2025. Code link: https://github.com/zwebzone/coto

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zwebzone/coto)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoToä»¥è§£å†³ä½ç§©é€‚åº”ä¸­çš„æ¬¡ä¼˜æœ€å°å€¼é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©é€‚åº”` `æ¨¡å‹å¾®è°ƒ` `æ¸è¿›å¼è®­ç»ƒ` `é€‚é…å™¨åˆå¹¶` `å‰ªæé²æ£’æ€§` `æŸå¤±ç©ºé—´æ¢ç´¢` `åˆä½œåšå¼ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä½ç§©é€‚åº”æ–¹æ³•å¸¸å°†é€‚é…å™¨é”å®šåœ¨æ¬¡ä¼˜æœ€å°å€¼ï¼Œå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. CoToé€šè¿‡éšæœºåœç”¨é€‚é…å™¨ï¼Œé€æ­¥å¢åŠ å…¶æ¿€æ´»æ¦‚ç‡ï¼Œä¿ƒè¿›æ›´å‡è¡¡çš„ä¼˜åŒ–å’ŒæŸå¤±ç©ºé—´çš„å¹¿æ³›æ¢ç´¢ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoToåœ¨å•ä»»åŠ¡æ€§èƒ½å’Œå¤šä»»åŠ¡åˆå¹¶å‡†ç¡®æ€§ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼ŒåŒæ—¶æé«˜äº†å‰ªæçš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä½œä¸ºä¸€ç§é«˜æ•ˆçš„å‚æ•°å¾®è°ƒæŠ€æœ¯ï¼Œå¸¸å¸¸å°†é€‚é…å™¨é”å®šåœ¨åˆå§‹åŒ–é™„è¿‘çš„æ¬¡ä¼˜æœ€å°å€¼ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œåç»­æ“ä½œå¦‚é€‚é…å™¨åˆå¹¶ä¸å‰ªæã€‚æœ¬æ–‡æå‡ºäº†CoToï¼Œä¸€ç§æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡é€æ­¥å¢åŠ é€‚é…å™¨çš„æ¿€æ´»æ¦‚ç‡ï¼Œä¿ƒè¿›æ›´å¹³è¡¡çš„ä¼˜åŒ–å’Œæ›´å¹¿æ³›çš„æŸå¤±ç©ºé—´æ¢ç´¢ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒCoToå¢å¼ºäº†å±‚çº§ä¸¢å¼ƒç¨³å®šæ€§å’Œçº¿æ€§æ¨¡å¼è¿æ¥æ€§ï¼Œå¹¶é‡‡ç”¨åˆä½œåšå¼ˆæ–¹æ³•é‡åŒ–æ¯ä¸ªé€‚é…å™¨çš„è¾¹é™…è´¡çŒ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCoToåœ¨å•ä»»åŠ¡æ€§èƒ½ã€å¤šä»»åŠ¡åˆå¹¶å‡†ç¡®æ€§ã€å‰ªæé²æ£’æ€§å’Œè®­ç»ƒå¼€é”€æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼ŒåŒæ—¶å…¼å®¹å¤šç§LoRAå˜ä½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä¸­é€‚é…å™¨é”å®šåœ¨æ¬¡ä¼˜æœ€å°å€¼çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œåç»­æ“ä½œçš„æœ‰æ•ˆæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºCoToç­–ç•¥ï¼Œé€šè¿‡é€æ­¥å¢åŠ é€‚é…å™¨çš„æ¿€æ´»æ¦‚ç‡ï¼Œå¹¶éšæœºåœç”¨é€‚é…å™¨ï¼Œä¿ƒè¿›æ›´å‡è¡¡çš„ä¼˜åŒ–å’Œæ›´å¹¿æ³›çš„æŸå¤±ç©ºé—´æ¢ç´¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoToçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é€‚é…å™¨çš„æ¿€æ´»æ¦‚ç‡è°ƒæ•´ã€éšæœºåœç”¨æœºåˆ¶å’Œåˆä½œåšå¼ˆæ–¹æ³•ï¼Œåˆ†é˜¶æ®µè¿›è¡Œè®­ç»ƒä»¥ä¼˜åŒ–é€‚é…å™¨çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoToçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´é€‚é…å™¨çš„æ¿€æ´»çŠ¶æ€ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ä¼˜åŒ–èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢æŸå¤±ç©ºé—´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒCoToé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´çš„æ¿€æ´»æ¦‚ç‡ï¼Œå¹¶è®¾è®¡äº†é€‚é…å™¨çš„éšæœºåœç”¨æœºåˆ¶ï¼Œç»“åˆå±‚çº§ä¸¢å¼ƒç¨³å®šæ€§å’Œçº¿æ€§æ¨¡å¼è¿æ¥æ€§ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°çš„è®¾è®¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCoToåœ¨å•ä»»åŠ¡æ€§èƒ½ä¸Šæå‡äº†X%ï¼Œåœ¨å¤šä»»åŠ¡åˆå¹¶å‡†ç¡®æ€§ä¸Šæé«˜äº†Y%ï¼Œå¹¶ä¸”åœ¨å‰ªæé²æ£’æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œè®­ç»ƒå¼€é”€é™ä½äº†Z%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰å¤§è§„æ¨¡æ¨¡å‹çš„å¾®è°ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ï¼ŒCoToä¸ºå¤šä»»åŠ¡å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Low-rank adaptation (LoRA) has emerged as a leading parameter-efficient fine-tuning technique for adapting large foundation models, yet it often locks adapters into suboptimal minima near their initialization. This hampers model generalization and limits downstream operators such as adapter merging and pruning. Here, we propose CoTo, a progressive training strategy that gradually increases adapters' activation probability over the course of fine-tuning. By stochastically deactivating adapters, CoTo encourages more balanced optimization and broader exploration of the loss landscape. We provide a theoretical analysis showing that CoTo promotes layer-wise dropout stability and linear mode connectivity, and we adopt a cooperative-game approach to quantify each adapter's marginal contribution. Extensive experiments demonstrate that CoTo consistently boosts single-task performance, enhances multi-task merging accuracy, improves pruning robustness, and reduces training overhead, all while remaining compatible with diverse LoRA variants. Code is available at https://github.com/zwebzone/coto.

