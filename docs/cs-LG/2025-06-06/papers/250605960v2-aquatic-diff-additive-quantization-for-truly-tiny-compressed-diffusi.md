---
layout: default
title: AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models
---

# AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05960" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.05960v2</a>
  <a href="https://arxiv.org/pdf/2506.05960.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05960v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05960v2', 'AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Adil Hasan, Thomas Peyrin

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-06 (Êõ¥Êñ∞: 2025-06-09)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫AQUATIC-Diff‰ª•Ëß£ÂÜ≥Êâ©Êï£Ê®°ÂûãÂéãÁº©ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£Ê®°Âûã` `ÈáèÂåñÊñπÊ≥ï` `ÂêëÈáèÈáèÂåñ` `ÂõæÂÉèÁîüÊàê` `Ê®°ÂûãÂéãÁº©` `È´òÊïàÊé®ÁêÜ` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊâ©Êï£Ê®°ÂûãÂú®Êé®ÁêÜÊó∂ÂØπÁ°¨‰ª∂ËµÑÊ∫êÁöÑÈúÄÊ±ÇÊûÅÈ´òÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂπøÊ≥õÂ∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫é‰ª£Á†ÅÊú¨ÁöÑÂä†ÊÄßÂêëÈáèÈáèÂåñÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊâ©Êï£Ê®°ÂûãÁöÑÂéãÁº©ÊïàÁéá„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™ÊåáÊ†á‰∏äË∂ÖË∂äÂÖ®Á≤æÂ∫¶Ê®°ÂûãÔºåÊòæËëóÈôç‰Ωé‰∫ÜÊé®ÁêÜÊó∂ÁöÑËÆ°ÁÆóÈúÄÊ±Ç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®ÁîüÊàêÂ§öÊ†∑ÂåñÂ™í‰ΩìÁöÑÊâ©Êï£Ê®°ÂûãÁöÑÂïÜ‰∏öÂåñËøáÁ®ã‰∏≠ÔºåÁ°¨‰ª∂ËµÑÊ∫êÈúÄÊ±ÇÈ´òÊàê‰∏∫ÂÖ∂ÊôÆÂèäÁöÑÈöúÁ¢ç„ÄÇËôΩÁÑ∂Áé∞ÊúâÁöÑÊ®°ÂûãÈáèÂåñÁ≠ñÁï•Âú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÁºìËß£‰∫ÜËøô‰∏ÄÈóÆÈ¢òÔºå‰ΩÜ‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂùáÂåÄÊ†áÈáèÈáèÂåñÊñπÊ≥ï‰∏ä„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫é‰ª£Á†ÅÊú¨ÁöÑÂä†ÊÄßÂêëÈáèÈáèÂåñÊñπÊ≥ïÔºå‰∏ìÈó®ÈíàÂØπÊâ©Êï£Ê®°ÂûãÁöÑÂéãÁº©„ÄÇËØ•ÊñπÊ≥ïÂú®ImageNetÁöÑLDM-4Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊñ∞ÁöÑ‰ΩéÊØîÁâπÊùÉÈáçÈáèÂåñÂâçÊ≤øÔºåÊä•ÂëäÁöÑsFIDÊØîÂÖ®Á≤æÂ∫¶Ê®°Âûã‰Ωé1.92ÂàÜÔºåÂπ∂Âú®W2A8‰∏ãËé∑Âæó‰∫ÜFID„ÄÅsFIDÂíåISCÁöÑÊúÄ‰Ω≥ÁªìÊûú„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÈÄöËøáÈ´òÊïàÊé®ÁêÜÂÜÖÊ†∏Âú®‰ªªÊÑèÁ°¨‰ª∂‰∏äËäÇÁúÅFLOPsÁöÑËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êâ©Êï£Ê®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÂØπÁ°¨‰ª∂ËµÑÊ∫êÁöÑÈ´òÈúÄÊ±ÇÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÈáèÂåñÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂùáÂåÄÊ†áÈáèÈáèÂåñÔºåÊú™ËÉΩÂÖÖÂàÜÂà©Áî®ÂêëÈáèÈáèÂåñÁöÑ‰ºòÂäø„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑÂä†ÊÄßÂêëÈáèÈáèÂåñÊñπÊ≥ïÈÄöËøáÂØπÁõ∏ÂÖ≥ÊùÉÈáçËøõË°åÂàÜÁªÑÂ§ÑÁêÜÔºåÂà©Áî®‰ª£Á†ÅÊú¨ËøõË°åÂéãÁº©Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂéãÁº©ÊïàÁéáÂíåÊé®ÁêÜÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÊ®°ÂûãËÆ≠ÁªÉ„ÄÅÈáèÂåñËøáÁ®ãÂíåÊé®ÁêÜÈò∂ÊÆµ„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨‰ª£Á†ÅÊú¨ÁîüÊàê„ÄÅÊùÉÈáçÊò†Â∞ÑÂíåÊé®ÁêÜ‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂä†ÊÄßÂêëÈáèÈáèÂåñÊñπÊ≥ïÔºåÂå∫Âà´‰∫é‰º†ÁªüÁöÑÂùáÂåÄÊ†áÈáèÈáèÂåñÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Â§ÑÁêÜÊâ©Êï£Ê®°ÂûãÁöÑÊùÉÈáç„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÈááÁî®‰∫ÜW4A8ÂíåW2A8ÁöÑÈáèÂåñÁ≠ñÁï•ÔºåÂπ∂ËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñÈáèÂåñÊïàÊûúÔºåÂêåÊó∂Á°Æ‰øù‰∫ÜÊé®ÁêÜËøáÁ®ãÁöÑÈ´òÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂú®W4A8ÈÖçÁΩÆ‰∏ãÔºåsFIDÊØîÂÖ®Á≤æÂ∫¶Ê®°Âûã‰Ωé1.92ÂàÜÔºåÂπ∂Âú®W2A8‰∏ãËé∑Âæó‰∫ÜFID„ÄÅsFIDÂíåISCÁöÑÊúÄ‰Ω≥ÁªìÊûú„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÈ´òÊïàÊé®ÁêÜÂÜÖÊ†∏ÂÆûÁé∞‰∫ÜFLOPsÁöÑÊòæËëóËäÇÁúÅÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂõæÂÉèÁîüÊàê„ÄÅËßÜÈ¢ëÁîüÊàê‰ª•ÂèäÂÖ∂‰ªñÈúÄË¶ÅÈ´òÊïàÊé®ÁêÜÁöÑÁîüÊàêÊ®°Âûã„ÄÇÈÄöËøáÈôç‰ΩéÁ°¨‰ª∂ËµÑÊ∫êÈúÄÊ±ÇÔºåAQUATIC-DiffËÉΩÂ§ü‰ΩøÂæóÊâ©Êï£Ê®°ÂûãÂú®ÁßªÂä®ËÆæÂ§áÂíåËæπÁºòËÆ°ÁÆóÁéØÂ¢É‰∏≠ÂæóÂà∞Êõ¥ÂπøÊ≥õÁöÑÂ∫îÁî®ÔºåÊé®Âä®ÁîüÊàêÊ®°ÂûãÁöÑÊôÆÂèä‰∏éÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Significant investments have been made towards the commodification of diffusion models for generation of diverse media. Their mass-market adoption is however still hobbled by the intense hardware resource requirements of diffusion model inference. Model quantization strategies tailored specifically towards diffusion models have been useful in easing this burden, yet have generally explored the Uniform Scalar Quantization (USQ) family of quantization methods. In contrast, Vector Quantization (VQ) methods, which operate on groups of multiple related weights as the basic unit of compression, have seen substantial success in Large Language Model (LLM) quantization. In this work, we apply codebook-based additive vector quantization to the problem of diffusion model compression. Our resulting approach achieves a new Pareto frontier for the extremely low-bit weight quantization on the standard class-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps. Notably, we report sFID 1.92 points lower than the full-precision model at W4A8 and the best-reported results for FID, sFID and ISC at W2A8. We are also able to demonstrate FLOPs savings on arbitrary hardware via an efficient inference kernel, as opposed to savings resulting from small integer operations which may lack broad hardware support.

