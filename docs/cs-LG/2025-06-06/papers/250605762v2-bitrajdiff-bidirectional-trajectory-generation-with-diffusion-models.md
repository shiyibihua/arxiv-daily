---
layout: default
title: BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning
---

# BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05762" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05762v2</a>
  <a href="https://arxiv.org/pdf/2506.05762.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05762v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05762v2', 'BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunpeng Qing, Shuo Chen, Yixiao Chi, Shunyu Liu, Sixu Lin, Kelu Yao, Changqing Zou

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-08-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBiTrajDiffä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„æ•°æ®åˆ†å¸ƒåå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æ•°æ®å¢å¼º` `åŒå‘è½¨è¿¹ç”Ÿæˆ` `ç”Ÿæˆæ¨¡å‹` `çŠ¶æ€ç©ºé—´æ¢ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†é™æ€æ•°æ®é›†æ—¶å­˜åœ¨åˆ†å¸ƒåå·®ï¼Œé™åˆ¶äº†ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„BiTrajDiffæ¡†æ¶é€šè¿‡åŒå‘è½¨è¿¹ç”Ÿæˆï¼Œæ—¢è€ƒè™‘æœªæ¥è½¨è¿¹ä¹Ÿè€ƒè™‘å†å²è½¬ç§»ï¼Œå¢å¼ºæ•°æ®é›†çš„å¤šæ ·æ€§ã€‚
3. åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒBiTrajDiffåœ¨å¤šä¸ªç¦»çº¿RLåŸºç¡€ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„æ•°æ®å¢å¼ºæ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡å¯¹é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†æ–½åŠ ä¿å®ˆçº¦æŸï¼Œå¯ä»¥æœ‰æ•ˆå­¦ä¹ ç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™äº›é™æ€æ•°æ®é›†å¾€å¾€å­˜åœ¨åˆ†å¸ƒåå·®ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ•°æ®å¢å¼ºï¼ˆDAï¼‰æˆä¸ºä¸€ç§ç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹ä¸°å¯Œæ•°æ®åˆ†å¸ƒã€‚ç°æœ‰çš„DAæŠ€æœ¯ä¸»è¦é›†ä¸­åœ¨ä»ç»™å®šçŠ¶æ€é‡å»ºæœªæ¥è½¨è¿¹ï¼Œè€Œå¿½è§†äº†æ¢ç´¢åˆ°è¾¾è¿™äº›çŠ¶æ€çš„å†å²è½¬ç§»ã€‚è¿™ç§å•å‘èŒƒå¼é™åˆ¶äº†å¤šæ ·åŒ–è¡Œä¸ºæ¨¡å¼çš„å‘ç°ï¼Œå°¤å…¶æ˜¯é‚£äº›å¯èƒ½å¯¼è‡´é«˜å¥–åŠ±ç»“æœçš„å…³é”®çŠ¶æ€ã€‚æœ¬æ–‡æå‡ºäº†åŒå‘è½¨è¿¹æ‰©æ•£ï¼ˆBiTrajDiffï¼‰ï¼Œä¸€ä¸ªæ–°é¢–çš„ç¦»çº¿RLæ•°æ®å¢å¼ºæ¡†æ¶ï¼Œèƒ½å¤Ÿä»ä»»æ„ä¸­é—´çŠ¶æ€å»ºæ¨¡æœªæ¥å’Œå†å²è½¨è¿¹ã€‚BiTrajDiffé€šè¿‡ä¸¤ä¸ªç‹¬ç«‹ä½†äº’è¡¥çš„æ‰©æ•£è¿‡ç¨‹æ¥åˆ†è§£è½¨è¿¹ç”Ÿæˆä»»åŠ¡ï¼Œä»è€Œæœ‰æ•ˆåˆ©ç”¨å…³é”®çŠ¶æ€ä½œä¸ºé”šç‚¹ï¼Œæ‰©å±•åˆ°æ½œåœ¨æœ‰ä»·å€¼ä½†æœªå……åˆ†æ¢ç´¢çš„çŠ¶æ€ç©ºé—´åŒºåŸŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ç”±äºé™æ€æ•°æ®é›†å¯¼è‡´çš„åˆ†å¸ƒåå·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨æœªæ¥è½¨è¿¹çš„é‡å»ºï¼Œå¿½è§†äº†å†å²è½¬ç§»çš„æ¢ç´¢ï¼Œé™åˆ¶äº†ç­–ç•¥å­¦ä¹ çš„å¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBiTrajDiffçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŒå‘è½¨è¿¹ç”Ÿæˆï¼Œåˆ†åˆ«å»ºæ¨¡æœªæ¥å’Œå†å²è½¨è¿¹ï¼Œä»è€Œä¸°å¯Œæ•°æ®é›†çš„å¤šæ ·æ€§ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£çŠ¶æ€ç©ºé—´ï¼Œå°¤å…¶æ˜¯å…³é”®çŠ¶æ€çš„è½¬ç§»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBiTrajDiffçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€ä¸ªæ˜¯ç”Ÿæˆæœªæ¥è½¨è¿¹çš„å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼Œå¦ä¸€ä¸ªæ˜¯ç”Ÿæˆå†å²è½¨è¿¹çš„åå‘æ‰©æ•£è¿‡ç¨‹ã€‚è¿™ä¸¤ä¸ªè¿‡ç¨‹ç›¸è¾…ç›¸æˆï¼Œå…±åŒæå‡æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šBiTrajDiffçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶åŒå‘è½¨è¿¹ç”Ÿæˆæœºåˆ¶ï¼Œä¸ç°æœ‰å•å‘ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ¢ç´¢çŠ¶æ€ç©ºé—´ï¼Œå°¤å…¶æ˜¯é‚£äº›æ½œåœ¨çš„é«˜å¥–åŠ±çŠ¶æ€ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒBiTrajDiffé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å‰å‘å’Œåå‘ç”Ÿæˆè¿‡ç¨‹çš„æ•ˆæœï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æé«˜ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒBiTrajDiffåœ¨å¤šä¸ªç¦»çº¿å¼ºåŒ–å­¦ä¹ åŸºç¡€ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šæ€§èƒ½æå‡è¶…è¿‡20%ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ç¦»çº¿RLä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œæ¸¸æˆAIç­‰ï¼Œèƒ½å¤Ÿåœ¨è¿™äº›é¢†åŸŸä¸­é€šè¿‡ä¸°å¯Œçš„æ•°æ®é›†æå‡ç­–ç•¥å­¦ä¹ çš„æ•ˆæœã€‚æœªæ¥ï¼ŒBiTrajDiffæœ‰æœ›æ¨åŠ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in offline Reinforcement Learning (RL) have proven that effective policy learning can benefit from imposing conservative constraints on pre-collected datasets. However, such static datasets often exhibit distribution bias, resulting in limited generalizability. To address this limitation, a straightforward solution is data augmentation (DA), which leverages generative models to enrich data distribution. Despite the promising results, current DA techniques focus solely on reconstructing future trajectories from given states, while ignoring the exploration of history transitions that reach them. This single-direction paradigm inevitably hinders the discovery of diverse behavior patterns, especially those leading to critical states that may have yielded high-reward outcomes. In this work, we introduce Bidirectional Trajectory Diffusion (BiTrajDiff), a novel DA framework for offline RL that models both future and history trajectories from any intermediate states. Specifically, we decompose the trajectory generation task into two independent yet complementary diffusion processes: one generating forward trajectories to predict future dynamics, and the other generating backward trajectories to trace essential history transitions.BiTrajDiff can efficiently leverage critical states as anchors to expand into potentially valuable yet underexplored regions of the state space, thereby facilitating dataset diversity. Extensive experiments on the D4RL benchmark suite demonstrate that BiTrajDiff achieves superior performance compared to other advanced DA methods across various offline RL backbones.

