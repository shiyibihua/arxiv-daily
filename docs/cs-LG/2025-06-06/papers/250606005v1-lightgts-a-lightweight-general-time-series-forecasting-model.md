---
layout: default
title: LightGTS: A Lightweight General Time Series Forecasting Model
---

# LightGTS: A Lightweight General Time Series Forecasting Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06005" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06005v1</a>
  <a href="https://arxiv.org/pdf/2506.06005.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06005v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06005v1', 'LightGTS: A Lightweight General Time Series Forecasting Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yihang Wang, Yuying Qiu, Peng Chen, Yang Shu, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: Accepted by the 42th International Conference on Machine Learning (ICML 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLightGTSä»¥è§£å†³æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„è®¡ç®—è´Ÿæ‹…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—é¢„æµ‹` `è½»é‡çº§æ¨¡å‹` `å‘¨æœŸæ€§å»ºæ¨¡` `å¤šæºé¢„è®­ç»ƒ` `é«˜æ•ˆè§£ç `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹é€šå¸¸å‚æ•°åºå¤§ï¼Œè®¡ç®—è´Ÿæ‹…é‡ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­åº”ç”¨ã€‚
2. LightGTSé€šè¿‡å‘¨æœŸæ€§æ ‡è®°åŒ–å’Œå‘¨æœŸæ€§å¹¶è¡Œè§£ç æŠ€æœ¯ï¼Œæ—¨åœ¨å®ç°è½»é‡çº§ä¸”é«˜æ•ˆçš„æ—¶é—´åºåˆ—é¢„æµ‹ã€‚
3. åœ¨9ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLightGTSåœ¨é›¶æ ·æœ¬å’Œå…¨æ ·æœ¬è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œæ•ˆç‡æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡çš„å¤šæºé¢„è®­ç»ƒï¼Œå¯¼è‡´æ¨¡å‹å‚æ•°åºå¤§ï¼Œè®¡ç®—è´Ÿæ‹…é‡ï¼Œå°¤å…¶åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºLightGTSï¼Œä¸€ä¸ªè½»é‡çº§çš„é€šç”¨æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä¸€è‡´çš„å‘¨æœŸå»ºæ¨¡æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ä¸ºå¤„ç†å¤šæºé¢„è®­ç»ƒä¸­çš„å¤šæ ·åŒ–å°ºåº¦å’Œå†…åœ¨å‘¨æœŸï¼Œæœ¬æ–‡å¼•å…¥äº†å‘¨æœŸæ€§æ ‡è®°åŒ–æŠ€æœ¯ï¼Œä»¥æå–ä¸åŒæ•°æ®é›†ä¸­çš„ä¸€è‡´å‘¨æœŸæ¨¡å¼ã€‚åŒæ—¶ï¼Œé‡‡ç”¨å‘¨æœŸæ€§å¹¶è¡Œè§£ç æŠ€æœ¯ï¼Œåˆ©ç”¨å†å²æ ‡è®°æ¥æå‡é¢„æµ‹æ•ˆæœã€‚LightGTSåœ¨9ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨é›¶æ ·æœ¬å’Œå…¨æ ·æœ¬è®¾ç½®ä¸‹å‡å±•ç°å‡ºä¼˜è¶Šçš„æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡çš„å¤šæºé¢„è®­ç»ƒï¼Œå¯¼è‡´æ¨¡å‹å‚æ•°åºå¤§ï¼Œè®¡ç®—è´Ÿæ‹…é‡ï¼Œå°¤å…¶åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºLightGTSï¼Œé€šè¿‡å‘¨æœŸæ€§æ ‡è®°åŒ–å’Œå‘¨æœŸæ€§å¹¶è¡Œè§£ç æŠ€æœ¯ï¼Œæ—¨åœ¨æå–ä¸€è‡´çš„å‘¨æœŸæ¨¡å¼å¹¶åˆ©ç”¨å†å²ä¿¡æ¯ï¼Œä»è€Œå®ç°è½»é‡çº§çš„é«˜æ•ˆé¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLightGTSçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‘¨æœŸæ€§æ ‡è®°åŒ–æ¨¡å—å’Œå‘¨æœŸæ€§å¹¶è¡Œè§£ç æ¨¡å—ã€‚å‘¨æœŸæ€§æ ‡è®°åŒ–è´Ÿè´£æå–ä¸åŒæ•°æ®é›†ä¸­çš„å‘¨æœŸæ¨¡å¼ï¼Œè€Œå‘¨æœŸæ€§å¹¶è¡Œè§£ç åˆ™åˆ©ç”¨å†å²æ ‡è®°è¿›è¡Œé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†å‘¨æœŸæ€§æ ‡è®°åŒ–å’Œå‘¨æœŸæ€§å¹¶è¡Œè§£ç ï¼Œè¿™ä¸¤è€…å……åˆ†åˆ©ç”¨äº†æ—¶é—´åºåˆ—ä¸­å›ºæœ‰çš„å‘¨æœŸæ€§åç½®ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒLightGTSé‡‡ç”¨äº†è½»é‡çº§çš„ç½‘ç»œç»“æ„ï¼Œä¼˜åŒ–äº†å‚æ•°è®¾ç½®ï¼Œå¹¶è®¾è®¡äº†é€‚åº”å‘¨æœŸæ€§ç‰¹å¾çš„æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨9ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLightGTSåœ¨é›¶æ ·æœ¬å’Œå…¨æ ·æœ¬è®¾ç½®ä¸‹å‡å®ç°äº†æœ€å…ˆè¿›çš„é¢„æµ‹æ€§èƒ½ï¼Œç›¸æ¯”ç°æœ‰æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼Œå…¶æ•ˆç‡æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å°šæœªå…¬å¼€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LightGTSåœ¨æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºé‡‘èå¸‚åœºé¢„æµ‹ã€æ°”è±¡æ•°æ®åˆ†æå’Œå·¥ä¸šè®¾å¤‡ç›‘æ§ç­‰åœºæ™¯ã€‚å…¶è½»é‡çº§ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­é«˜æ•ˆè¿è¡Œï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½é¢„æµ‹æŠ€æœ¯å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing works on general time series forecasting build foundation models with heavy model parameters through large-scale multi-source pre-training. These models achieve superior generalization ability across various datasets at the cost of significant computational burdens and limitations in resource-constrained scenarios. This paper introduces LightGTS, a lightweight general time series forecasting model designed from the perspective of consistent periodical modeling. To handle diverse scales and intrinsic periods in multi-source pre-training, we introduce Periodical Tokenization, which extracts consistent periodic patterns across different datasets with varying scales. To better utilize the periodicity in the decoding process, we further introduce Periodical Parallel Decoding, which leverages historical tokens to improve forecasting. Based on the two techniques above which fully leverage the inductive bias of periods inherent in time series, LightGTS uses a lightweight model to achieve outstanding performance on general time series forecasting. It achieves state-of-the-art forecasting performance on 9 real-world benchmarks in both zero-shot and full-shot settings with much better efficiency compared with existing time series foundation models.

