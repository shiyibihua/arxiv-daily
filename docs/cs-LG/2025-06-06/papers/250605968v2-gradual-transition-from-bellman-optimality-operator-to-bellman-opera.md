---
layout: default
title: Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning
---

# Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05968" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.05968v2</a>
  <a href="https://arxiv.org/pdf/2506.05968.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05968v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05968v2', 'Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Motoki Omura, Kazuki Ota, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-06 (Êõ¥Êñ∞: 2025-08-13)

**Â§áÊ≥®**: Accepted at ICML 2025. Source code: https://github.com/motokiomura/annealed-q-learning

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/motokiomura/annealed-q-learning)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÈÄêÊ≠•ËøáÊ∏°ÊñπÊ≥ï‰ª•ÊèêÂçáÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÁöÑÊ†∑Êú¨ÊïàÁéá**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Ë¥ùÂ∞îÊõºÁÆóÂ≠ê` `ÊºîÂëò-ËØÑËÆ∫ÂÆ∂` `Ê†∑Êú¨ÊïàÁéá` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Ëá™Âä®È©æÈ©∂` `Âä®ÊÄÅË∞ÉÊï¥`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËøûÁª≠Âä®‰ΩúÁ©∫Èó¥Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éË¥ùÂ∞îÊõºÁÆóÂ≠êÔºåÂØºËá¥Ê†∑Êú¨ÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄêÊ≠•ËøáÊ∏°ÁöÑÊñπÊ≥ïÔºåÂ∞ÜË¥ùÂ∞îÊõºÊúÄ‰ºòÊÄßÁÆóÂ≠êËûçÂÖ•ÊºîÂëò-ËØÑËÆ∫ÂÆ∂Ê°ÜÊû∂Ôºå‰ª•Âä†ÈÄüÂ≠¶‰π†Âπ∂ÂáèËΩªÂÅèÂ∑Æ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÁªìÂêàTD3ÂíåSACÁöÑÊîπËøõÊñπÊ≥ïÂú®Â§öÁßç‰ªªÂä°‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®ËøûÁª≠Âä®‰ΩúÁ©∫Èó¥‰∏≠ÔºåÊºîÂëò-ËØÑËÆ∫ÂÆ∂ÊñπÊ≥ïÂπøÊ≥õÂ∫îÁî®‰∫éÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ„ÄÇ‰∏éÁ¶ªÊï£Âä®‰ΩúÁöÑRLÁÆóÊ≥ï‰∏çÂêåÔºåÂêéËÄÖÈÄöÂ∏∏‰ΩøÁî®Ë¥ùÂ∞îÊõºÊúÄ‰ºòÊÄßÁÆóÂ≠êÂª∫Ê®°ÊúÄ‰ºòÂÄºÂáΩÊï∞ÔºåËÄåËøûÁª≠Âä®‰ΩúÁöÑÁÆóÊ≥ïÂàô‰æùËµñ‰∫éË¥ùÂ∞îÊõºÁÆóÂ≠êÂª∫Ê®°ÂΩìÂâçÁ≠ñÁï•ÁöÑQÂÄº„ÄÇËøôÁßçÊñπÊ≥ï‰ªÖ‰æùËµñ‰∫éÁ≠ñÁï•Êõ¥Êñ∞ÔºåÂØºËá¥Ê†∑Êú¨ÊïàÁéá‰Ωé‰∏ã„ÄÇÊú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ∞ÜË¥ùÂ∞îÊõºÊúÄ‰ºòÊÄßÁÆóÂ≠êÁ∫≥ÂÖ•ÊºîÂëò-ËØÑËÆ∫ÂÆ∂Ê°ÜÊû∂ÁöÑÊúâÊïàÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂª∫Ê®°ÊúÄ‰ºòÂÄºÂä†ÈÄüÂ≠¶‰π†Ôºå‰ΩÜ‰ºöÂØºËá¥ËøáÈ´ò‰º∞ËÆ°ÂÅèÂ∑Æ„ÄÇ‰∏∫Ê≠§ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄêÊ≠•ËøáÊ∏°ÁöÑÊñπÊ≥ïÔºå‰ªéË¥ùÂ∞îÊõºÊúÄ‰ºòÊÄßÁÆóÂ≠êËøáÊ∏°Âà∞Ë¥ùÂ∞îÊõºÁÆóÂ≠êÔºå‰ªéËÄåÂä†ÈÄüÂ≠¶‰π†Âπ∂ÂáèËΩªÂÅèÂ∑Æ„ÄÇËØ•ÊñπÊ≥ï‰∏éTD3ÂíåSACÁªìÂêàÔºåÂú®Â§öÁßçËøêÂä®ÂíåÊìç‰Ωú‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÊõ¥Â•ΩÁöÑÊÄßËÉΩÂíåÂØπÊúÄ‰ºòÊÄßÁõ∏ÂÖ≥Ë∂ÖÂèÇÊï∞ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂΩìÂâçËøûÁª≠Âä®‰ΩúÁ©∫Èó¥Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÂú®Ê†∑Êú¨ÊïàÁéáÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÁé∞ÊúâÊñπÊ≥ï‰ªÖ‰æùËµñ‰∫éÁ≠ñÁï•Êõ¥Êñ∞ÔºåÂØºËá¥Â≠¶‰π†ÈÄüÂ∫¶ÊÖ¢ÂíåÊÄßËÉΩ‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰∏ÄÁßçÈÄêÊ≠•ËøáÊ∏°ÁöÑÊñπÊ≥ïÔºå‰ªéË¥ùÂ∞îÊõºÊúÄ‰ºòÊÄßÁÆóÂ≠êÈÄêÊ∏êËøáÊ∏°Âà∞Ë¥ùÂ∞îÊõºÁÆóÂ≠êÔºå‰ª•Ê≠§Âä†ÈÄüÂ≠¶‰π†ËøáÁ®ãÂπ∂ÂáèËΩªËøáÈ´ò‰º∞ËÆ°ÁöÑÂÅèÂ∑Æ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖà‰ΩøÁî®Ë¥ùÂ∞îÊõºÊúÄ‰ºòÊÄßÁÆóÂ≠êËøõË°åÂàùÂßãÂ≠¶‰π†ÔºåÁÑ∂ÂêéÈÄêÊ≠•ÂºïÂÖ•Ë¥ùÂ∞îÊõºÁÆóÂ≠êËøõË°åÁ≠ñÁï•Êõ¥Êñ∞„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜTD3ÂíåSACÁÆóÊ≥ïÔºåÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂ≠¶‰π†ÊµÅÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÈÄêÊ≠•ËøáÊ∏°ÁöÑÁ≠ñÁï•ÔºåËøô‰∏ÄÊñπÊ≥ïÊúâÊïàÁªìÂêà‰∫ÜÊúÄ‰ºòÊÄß‰∏éÂÆûÈôÖÁ≠ñÁï•Êõ¥Êñ∞ÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåËÆæËÆ°‰∫Ü‰∏Ä‰∏™Âä®ÊÄÅË∞ÉÊï¥ÁöÑËøáÊ∏°Á≠ñÁï•ÔºåÊçüÂ§±ÂáΩÊï∞ÁªìÂêà‰∫ÜË¥ùÂ∞îÊõºÊúÄ‰ºòÊÄß‰∏éË¥ùÂ∞îÊõºÁÆóÂ≠êÁöÑÁâπÊÄßÔºåÁΩëÁªúÁªìÊûÑÂàôÈááÁî®‰∫ÜÊ†áÂáÜÁöÑÊºîÂëò-ËØÑËÆ∫ÂÆ∂Êû∂ÊûÑÔºåÁ°Æ‰øù‰∫ÜÁÆóÊ≥ïÁöÑÁ®≥ÂÆöÊÄß‰∏éÊî∂ÊïõÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁªìÂêàTD3ÂíåSACÁöÑÈÄêÊ≠•ËøáÊ∏°ÊñπÊ≥ïÂú®Â§öÁßçËøêÂä®ÂíåÊìç‰Ωú‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºå‰∏îÂØπË∂ÖÂèÇÊï∞ÁöÑÈ≤ÅÊ£íÊÄßÊòæËëóÂ¢ûÂº∫„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂà∂ÈÄ†Á≠âÈúÄË¶ÅÈ´òÊïàÂÜ≥Á≠ñÁöÑÂú∫ÊôØ„ÄÇÈÄöËøáÊèêÂçáÊ†∑Êú¨ÊïàÁéáÂíåÂ≠¶‰π†ÈÄüÂ∫¶ÔºåËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥Âø´ÈÄüÁöÑÈÄÇÂ∫î‰∏é‰ºòÂåñÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> For continuous action spaces, actor-critic methods are widely used in online reinforcement learning (RL). However, unlike RL algorithms for discrete actions, which generally model the optimal value function using the Bellman optimality operator, RL algorithms for continuous actions typically model Q-values for the current policy using the Bellman operator. These algorithms for continuous actions rely exclusively on policy updates for improvement, which often results in low sample efficiency. This study examines the effectiveness of incorporating the Bellman optimality operator into actor-critic frameworks. Experiments in a simple environment show that modeling optimal values accelerates learning but leads to overestimation bias. To address this, we propose an annealing approach that gradually transitions from the Bellman optimality operator to the Bellman operator, thereby accelerating learning while mitigating bias. Our method, combined with TD3 and SAC, significantly outperforms existing approaches across various locomotion and manipulation tasks, demonstrating improved performance and robustness to hyperparameters related to optimality. The code for this study is available at https://github.com/motokiomura/annealed-q-learning.

