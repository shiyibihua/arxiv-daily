---
layout: default
title: A Systematic Review of Poisoning Attacks Against Large Language Models
---

# A Systematic Review of Poisoning Attacks Against Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06518" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06518v1</a>
  <a href="https://arxiv.org/pdf/2506.06518.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06518v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06518v1', 'A Systematic Review of Poisoning Attacks Against Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Neil Fendley, Edward W. Staley, Joshua Carney, William Redman, Marie Chau, Nathan Drenkow

**åˆ†ç±»**: cs.CR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: 28 Pages including number

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç³»ç»Ÿæ€§æ¡†æ¶ä»¥åº”å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸­æ¯’æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸­æ¯’æ”»å‡»` `å®‰å…¨é£é™©` `ç³»ç»Ÿå›é¡¾` `å¨èƒæ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¿¡æ¯å®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡çŒ®ä¸­çš„LLMä¸­æ¯’æ”»å‡»æ¡†æ¶å’Œæœ¯è¯­ä¸å¤Ÿå®Œå–„ï¼Œæ— æ³•é€‚åº”ç”Ÿæˆæ€§æ¨¡å‹çš„ç‰¹æ€§ã€‚
2. æå‡ºä¸€ä¸ªå…¨é¢çš„ä¸­æ¯’å¨èƒæ¨¡å‹ï¼Œèƒ½å¤Ÿå¯¹å¤šç§LLMä¸­æ¯’æ”»å‡»è¿›è¡Œåˆ†ç±»å’Œåˆ†æã€‚
3. é€šè¿‡å¯¹å·²å‘è¡¨æ–‡çŒ®çš„ç³»ç»Ÿå›é¡¾ï¼Œæ˜ç¡®äº†ä¸­æ¯’æ”»å‡»çš„å®‰å…¨éšæ‚£å’Œæœ¯è¯­çš„ä¸€è‡´æ€§é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŠå…¶è®­ç»ƒæ•°æ®é›†çš„å¹¿æ³›åº”ç”¨ï¼Œå…³äºå…¶å®‰å…¨é£é™©çš„å…³æ³¨æ˜¾è‘—å¢åŠ ã€‚å…¶ä¸­ï¼Œä¸­æ¯’æ”»å‡»æ˜¯ä¸€ç§å®‰å…¨å¨èƒï¼Œæ”»å‡»è€…é€šè¿‡ä¿®æ”¹LLMè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸäº›éƒ¨åˆ†ï¼Œä½¿å¾—æ¨¡å‹äº§ç”Ÿæ¶æ„è¡Œä¸ºã€‚å½“å‰é’ˆå¯¹LLMä¸­æ¯’æ”»å‡»çš„æ¡†æ¶å’Œæœ¯è¯­ä¸»è¦æºè‡ªæ—©æœŸçš„åˆ†ç±»ä¸­æ¯’æ–‡çŒ®ï¼Œæ— æ³•å®Œå…¨é€‚åº”ç”Ÿæˆæ€§LLMçš„ç¯å¢ƒã€‚æœ¬æ–‡ç³»ç»Ÿå›é¡¾äº†å·²å‘è¡¨çš„LLMä¸­æ¯’æ”»å‡»ï¼Œæ—¨åœ¨æ¾„æ¸…å®‰å…¨éšæ‚£å¹¶è§£å†³æ–‡çŒ®ä¸­çš„æœ¯è¯­ä¸ä¸€è‡´é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„ä¸­æ¯’å¨èƒæ¨¡å‹ï¼Œä»¥åˆ†ç±»å„ç§LLMä¸­æ¯’æ”»å‡»ï¼Œå¹¶å®šä¹‰äº†å››ç§ä¸­æ¯’æ”»å‡»è§„æ ¼å’Œå…­ä¸ªä¸­æ¯’åº¦é‡æŒ‡æ ‡ï¼Œä»¥è¡¡é‡æ”»å‡»çš„å…³é”®ç‰¹å¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰LLMä¸­æ¯’æ”»å‡»ç ”ç©¶ä¸­æ¡†æ¶å’Œæœ¯è¯­ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåº”å¯¹ç”Ÿæˆæ€§æ¨¡å‹çš„ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ä¸ªå…¨é¢çš„ä¸­æ¯’å¨èƒæ¨¡å‹ï¼Œæ¶µç›–å››ç§ä¸­æ¯’æ”»å‡»è§„æ ¼å’Œå…­ä¸ªä¸­æ¯’åº¦é‡æŒ‡æ ‡ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£å’Œåˆ†ç±»LLMä¸­æ¯’æ”»å‡»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å¯¹ä¸­æ¯’æ”»å‡»çš„å››ä¸ªå…³é”®ç»´åº¦çš„è®¨è®ºï¼šæ¦‚å¿µä¸­æ¯’ã€éšè”½ä¸­æ¯’ã€æŒä¹…ä¸­æ¯’å’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ä¸­æ¯’ï¼Œæ—¨åœ¨å…¨é¢åˆ†æå½“å‰çš„å®‰å…¨é£é™©ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†é€‚ç”¨äºç”Ÿæˆæ€§LLMçš„ä¸­æ¯’å¨èƒæ¨¡å‹ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®çš„ç©ºç™½ï¼Œå¹¶æä¾›äº†ç³»ç»ŸåŒ–çš„åˆ†ç±»æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹ä¸­å®šä¹‰çš„å››ç§ä¸­æ¯’æ”»å‡»è§„æ ¼å’Œå…­ä¸ªåº¦é‡æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¡¡é‡æ”»å‡»çš„ç‰¹å¾å’Œå½±å“ï¼Œç¡®ä¿å¯¹ä¸­æ¯’æ”»å‡»çš„å…¨é¢ç†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

é€šè¿‡ç³»ç»Ÿå›é¡¾ï¼Œæœ¬æ–‡æ˜ç¡®äº†LLMä¸­æ¯’æ”»å‡»çš„å››ä¸ªå…³é”®ç»´åº¦ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„ä¸­æ¯’å¨èƒæ¨¡å‹ã€‚è¿™ä¸€æ¨¡å‹ä¸ä»…æœ‰åŠ©äºåˆ†ç±»å’Œç†è§£ä¸­æ¯’æ”»å‡»ï¼Œè¿˜ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒæ¡†æ¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¿¡æ¯å®‰å…¨å’Œæ¨¡å‹è®­ç»ƒç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªç³»ç»ŸåŒ–çš„ä¸­æ¯’æ”»å‡»æ¡†æ¶ï¼Œç ”ç©¶è€…å’Œå¼€å‘è€…å¯ä»¥æ›´å¥½åœ°è¯†åˆ«å’Œé˜²èŒƒLLMä¸­çš„å®‰å…¨é£é™©ï¼Œä»è€Œæå‡æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the widespread availability of pretrained Large Language Models (LLMs) and their training datasets, concerns about the security risks associated with their usage has increased significantly. One of these security risks is the threat of LLM poisoning attacks where an attacker modifies some part of the LLM training process to cause the LLM to behave in a malicious way. As an emerging area of research, the current frameworks and terminology for LLM poisoning attacks are derived from earlier classification poisoning literature and are not fully equipped for generative LLM settings. We conduct a systematic review of published LLM poisoning attacks to clarify the security implications and address inconsistencies in terminology across the literature. We propose a comprehensive poisoning threat model applicable to categorize a wide range of LLM poisoning attacks. The poisoning threat model includes four poisoning attack specifications that define the logistics and manipulation strategies of an attack as well as six poisoning metrics used to measure key characteristics of an attack. Under our proposed framework, we organize our discussion of published LLM poisoning literature along four critical dimensions of LLM poisoning attacks: concept poisons, stealthy poisons, persistent poisons, and poisons for unique tasks, to better understand the current landscape of security risks.

