---
layout: default
title: Ensemble Elastic DQN: A novel multi-step ensemble approach to address overestimation in deep value-based reinforcement learning
---

# Ensemble Elastic DQN: A novel multi-step ensemble approach to address overestimation in deep value-based reinforcement learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05716" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05716v1</a>
  <a href="https://arxiv.org/pdf/2506.05716.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05716v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05716v1', 'Ensemble Elastic DQN: A novel multi-step ensemble approach to address overestimation in deep value-based reinforcement learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adrian Ly, Richard Dazeley, Peter Vamplew, Francisco Cruz, Sunil Aryal

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEEDQNä»¥è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿‡ä¼°è®¡åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `DQN` `é›†æˆå­¦ä¹ ` `è¿‡ä¼°è®¡åå·®` `æ ·æœ¬æ•ˆç‡` `å¼¹æ€§æ­¥æ›´æ–°` `ç®—æ³•ç¨³å®šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†è¿‡ä¼°è®¡åå·®å’Œæ ·æœ¬æ•ˆç‡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå½±å“äº†ç®—æ³•çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºçš„EEDQNç®—æ³•é€šè¿‡å°†é›†æˆæ–¹æ³•ä¸å¼¹æ€§æ­¥æ›´æ–°ç›¸ç»“åˆï¼Œæ—¨åœ¨æœ‰æ•ˆè§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒEEDQNåœ¨MinAtaråŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»ŸDQNå’Œå¤§å¤šæ•°é›†æˆDQNçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å·²æœ‰å¤šç§æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰çš„ç®—æ³•æ‰©å±•è¢«æå‡ºï¼Œä½†å¯¹ä¸åŒæ”¹è¿›ä¹‹é—´çš„ç›¸äº’ä½œç”¨ä»ç¼ºä¹æ·±å…¥ç†è§£ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¤šæ­¥å’Œé›†æˆé£æ ¼çš„æ‰©å±•åœ¨å‡å°‘è¿‡ä¼°è®¡åå·®æ–¹é¢æ˜¾ç¤ºå‡ºè‰¯å¥½å‰æ™¯ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡å’Œç®—æ³•ç¨³å®šæ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼Œç§°ä¸ºé›†æˆå¼¹æ€§æ­¥DQNï¼ˆEEDQNï¼‰ï¼Œè¯¥ç®—æ³•å°†é›†æˆä¸å¼¹æ€§æ­¥æ›´æ–°ç›¸ç»“åˆï¼Œä»¥ç¨³å®šç®—æ³•æ€§èƒ½ã€‚EEDQNæ—¨åœ¨è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè¿‡ä¼°è®¡åå·®å’Œæ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨MinAtaråŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†EEDQNï¼Œç»“æœè¡¨æ˜å…¶åœ¨æ‰€æœ‰æµ‹è¯•ç¯å¢ƒä¸­è¡¨ç°å‡ºä¸€è‡´çš„ç¨³å¥æ€§ï¼Œè¶…è¶Šäº†åŸºçº¿DQNæ–¹æ³•ï¼Œå¹¶åœ¨å¤§å¤šæ•°MinAtarç¯å¢ƒä¸­ä¸æœ€å…ˆè¿›çš„é›†æˆDQNç›¸åŒ¹é…æˆ–è¶…è¶Šæœ€ç»ˆå›æŠ¥ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç³»ç»Ÿæ€§ç»“åˆç®—æ³•æ”¹è¿›çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„å…·ä½“é—®é¢˜æ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿‡ä¼°è®¡åå·®å’Œæ ·æœ¬æ•ˆç‡ä¸è¶³ã€‚ç°æœ‰DQNæ–¹æ³•åœ¨é¢å¯¹å¤æ‚ç¯å¢ƒæ—¶ï¼Œå®¹æ˜“äº§ç”Ÿè¿‡é«˜çš„ä»·å€¼ä¼°è®¡ï¼Œå¯¼è‡´å­¦ä¹ ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEEDQNçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é›†æˆå­¦ä¹ ä¸å¼¹æ€§æ­¥æ›´æ–°ç›¸ç»“åˆï¼Œé€šè¿‡å¤šæ­¥æ›´æ–°æ¥å‡å°‘ä¼°è®¡åå·®ï¼ŒåŒæ—¶åˆ©ç”¨é›†æˆæ–¹æ³•æé«˜ç®—æ³•çš„ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEEDQNçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªDQNçš„é›†æˆï¼Œæ¯ä¸ªDQNç‹¬ç«‹å­¦ä¹ å¹¶å…±äº«ç»éªŒã€‚ç®—æ³•é€šè¿‡å¼¹æ€§æ­¥æ›´æ–°æœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´å­¦ä¹ æ­¥é•¿ï¼Œä»¥é€‚åº”ä¸åŒç¯å¢ƒçš„å¤æ‚æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šEEDQNçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å°†é›†æˆæ–¹æ³•ä¸å¼¹æ€§æ­¥æ›´æ–°æœ‰æ•ˆç»“åˆï¼Œå…‹æœäº†ä¼ ç»ŸDQNåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¿‡ä¼°è®¡é—®é¢˜ï¼Œæå‡äº†å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨EEDQNä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬é›†æˆDQNçš„æ•°é‡ã€å¼¹æ€§æ­¥é•¿çš„è°ƒæ•´ç­–ç•¥ä»¥åŠæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç¡®ä¿åœ¨ä¸åŒç¯å¢ƒä¸‹éƒ½èƒ½ä¿æŒè‰¯å¥½çš„å­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨MinAtaråŸºå‡†æµ‹è¯•ä¸­ï¼ŒEEDQNåœ¨æ‰€æœ‰æµ‹è¯•ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºä¸€è‡´çš„ç¨³å¥æ€§ï¼Œè¶…è¶Šäº†åŸºçº¿DQNæ–¹æ³•ï¼Œå¹¶åœ¨å¤§å¤šæ•°ç¯å¢ƒä¸­ä¸æœ€å…ˆè¿›çš„é›†æˆDQNç›¸åŒ¹é…æˆ–è¶…è¶Šæœ€ç»ˆå›æŠ¥ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¸¸æˆæ™ºèƒ½ä½“ã€æœºå™¨äººæ§åˆ¶å’Œè‡ªåŠ¨é©¾é©¶ç­‰éœ€è¦é«˜æ•ˆå­¦ä¹ å’Œå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æé«˜æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ï¼ŒEEDQNèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´ä¼˜çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While many algorithmic extensions to Deep Q-Networks (DQN) have been proposed, there remains limited understanding of how different improvements interact. In particular, multi-step and ensemble style extensions have shown promise in reducing overestimation bias, thereby improving sample efficiency and algorithmic stability. In this paper, we introduce a novel algorithm called Ensemble Elastic Step DQN (EEDQN), which unifies ensembles with elastic step updates to stabilise algorithmic performance. EEDQN is designed to address two major challenges in deep reinforcement learning: overestimation bias and sample efficiency. We evaluated EEDQN against standard and ensemble DQN variants across the MinAtar benchmark, a set of environments that emphasise behavioral learning while reducing representational complexity. Our results show that EEDQN achieves consistently robust performance across all tested environments, outperforming baseline DQN methods and matching or exceeding state-of-the-art ensemble DQNs in final returns on most of the MinAtar environments. These findings highlight the potential of systematically combining algorithmic improvements and provide evidence that ensemble and multi-step methods, when carefully integrated, can yield substantial gains.

