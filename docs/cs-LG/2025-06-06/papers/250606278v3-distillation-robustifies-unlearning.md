---
layout: default
title: Distillation Robustifies Unlearning
---

# Distillation Robustifies Unlearning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06278" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06278v3</a>
  <a href="https://arxiv.org/pdf/2506.06278.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06278v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06278v3', 'Distillation Robustifies Unlearning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bruce W. Lee, Addie Foote, Alex Infanger, Leni Shor, Harish Kamath, Jacob Goldman-Wetzler, Bryce Woodworth, Alex Cloud, Alexander Matt Turner

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: NeurIPS 2025 (Spotlight)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUNDOæ–¹æ³•ä»¥å¢å¼ºå¤§è§„æ¨¡æ¨¡å‹çš„å»å­¦ä¹ é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å»å­¦ä¹ ` `é²æ£’æ€§` `è’¸é¦è®­ç»ƒ` `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `åˆæˆä»»åŠ¡` `å™ªå£°å¤„ç†` `æ¨¡å‹å®‰å…¨æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å»å­¦ä¹ æ–¹æ³•åœ¨é²æ£’æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“å—åˆ°å¾®è°ƒçš„å½±å“è€Œå¤±æ•ˆã€‚
2. æœ¬æ–‡æå‡ºçš„UNDOæ–¹æ³•é€šè¿‡è’¸é¦æœªå­¦ä¹ æ¨¡å‹çš„è¾“å‡ºï¼Œç”Ÿæˆå¸¦å™ªå£°çš„å‰¯æœ¬ï¼Œä»è€Œå¢å¼ºå»å­¦ä¹ çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒUNDOåœ¨åˆæˆè¯­è¨€å’Œç®—æœ¯ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†é«˜é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å»å­¦ä¹ æ–¹æ³•ç¼ºä¹é²æ£’æ€§ï¼Œå‡ æ­¥å¾®è°ƒå°±å¯èƒ½é€†è½¬å…¶æ•ˆæœã€‚æœ¬æ–‡å±•ç¤ºäº†å³ä½¿åœ¨ç†æƒ³åŒ–çš„å»å­¦ä¹ å½¢å¼ä¸‹ï¼Œè®­ç»ƒæ¨¡å‹ä¹Ÿèƒ½æ˜¾è‘—æ”¹å˜å…¶è¾“å…¥è¾“å‡ºè¡Œä¸ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”Unlearn-Noise-Distill-on-Outputsï¼ˆUNDOï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡è’¸é¦æœªå­¦ä¹ æ¨¡å‹çš„è¾“å‡ºï¼Œç”Ÿæˆä¸€ä¸ªå¸¦å™ªå£°çš„å‰¯æœ¬ï¼Œä»è€Œå¢å¼ºå»å­¦ä¹ çš„é²æ£’æ€§ã€‚UNDOåœ¨åˆæˆè¯­è¨€å’Œç®—æœ¯ä»»åŠ¡ä¸Šå»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—æˆæœ¬å’Œé²æ£’æ€§ä¹‹é—´è¿›è¡Œå¯è°ƒçš„æƒè¡¡ï¼Œä¸”åœ¨æœ€å¼ºè®¾ç½®ä¸‹ï¼Œä½¿ç”¨60-80%çš„è®¡ç®—èµ„æºå’Œ0.01%çš„é¢„è®­ç»ƒæ•°æ®æ ‡è®°ï¼Œè¾¾åˆ°äº†ä¸å®Œç¾æ•°æ®è¿‡æ»¤ä¸‹ä»å¤´å†è®­ç»ƒæ¨¡å‹ç›¸å½“çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å»å­¦ä¹ æ–¹æ³•çš„é²æ£’æ€§ä¸è¶³é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒåå®¹æ˜“å¤±æ•ˆï¼Œæ— æ³•æœ‰æ•ˆå»é™¤ä¸å¿…è¦çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è’¸é¦æŠ€æœ¯ï¼Œå°†æœªå­¦ä¹ æ¨¡å‹çš„è¾“å‡ºè½¬ç§»åˆ°ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„å­¦ç”Ÿæ¨¡å‹ä¸Šï¼Œä»è€Œåœ¨ä¿ç•™æ½œåœ¨èƒ½åŠ›çš„åŒæ—¶å¢å¼ºå»å­¦ä¹ çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œè®­ç»ƒä¸€ä¸ªæœªå­¦ä¹ æ¨¡å‹ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨è¯¥æ¨¡å‹çš„è¾“å‡ºè®­ç»ƒä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„å­¦ç”Ÿæ¨¡å‹ï¼›æœ€åï¼Œé€šè¿‡å¼•å…¥å™ªå£°æ¥ç”Ÿæˆä¸€ä¸ªé²æ£’çš„å‰¯æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†UNDOæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è’¸é¦å’Œå™ªå£°å¼•å…¥çš„ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†å»å­¦ä¹ çš„é²æ£’æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒUNDOæ–¹æ³•å¼•å…¥äº†å¯è°ƒçš„å‚æ•°ï¼Œä»¥å¹³è¡¡è®¡ç®—æˆæœ¬å’Œé²æ£’æ€§ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†è¾“å‡ºçš„å™ªå£°å¤„ç†ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¸åŒè®¾ç½®ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUNDOæ–¹æ³•åœ¨åˆæˆè¯­è¨€å’Œç®—æœ¯ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚åœ¨æœ€å¼ºè®¾ç½®ä¸‹ï¼ŒUNDOçš„é²æ£’æ€§ä¸å®Œç¾æ•°æ®è¿‡æ»¤ä¸‹ä»å¤´å†è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶ä»…ä½¿ç”¨60-80%çš„è®¡ç®—èµ„æºå’Œ0.01%çš„é¢„è®­ç»ƒæ•°æ®æ ‡è®°ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éœ€è¦å»é™¤ç‰¹å®šä¿¡æ¯çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œå¦‚æ³•å¾‹ã€åŒ»ç–—å’Œé‡‘èç­‰é¢†åŸŸã€‚é€šè¿‡å¢å¼ºå»å­¦ä¹ çš„é²æ£’æ€§ï¼ŒUNDOæ–¹æ³•ä¸ºæ¨¡å‹çš„å®‰å…¨æ€§å’Œåˆè§„æ€§æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current LLM unlearning methods are not robust. A few steps of finetuning can revert their effects. We begin by showing that this is true even for an idealized form of unlearning: training to imitate a model that was never trained on unwanted information. This shows that training a model can drastically modify its input-output behavior while leaving its underlying capabilities intact. In light of this dynamic, we show our main result. Training a randomly initialized student on the outputs of an unlearned model transfers behaviors while leaving latent capabilities behind. In short, distillation robustifies unlearning. Based on this result, we propose Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an unlearned model into a noised copy of itself. UNDO introduces a tunable tradeoff between compute cost and robustness, establishing a new Pareto frontier on synthetic language and arithmetic tasks. At its strongest setting, UNDO matches the robustness of a model retrained from scratch with perfect data filtering while using only 60-80% of the compute and requiring only 0.01% of the pretraining data to be labeled. We also show that UNDO robustifies unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP) benchmark. Since distillation is widely used in practice, incorporating an unlearning step beforehand offers a convenient path to robust capability removal.

