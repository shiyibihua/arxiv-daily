---
layout: default
title: The Lock-in Hypothesis: Stagnation by Algorithm
---

# The Lock-in Hypothesis: Stagnation by Algorithm

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06166" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06166v1</a>
  <a href="https://arxiv.org/pdf/2506.06166.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06166v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06166v1', 'The Lock-in Hypothesis: Stagnation by Algorithm')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyi Alex Qiu, Zhonghao He, Tejasveer Chugh, Max Kleiman-Weiner

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.CY, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: ICML 2025, 46 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé”å®šå‡è¯´ä»¥è§£å†³ç®—æ³•å¼•å‘çš„ä¿¡å¿µå›ºåŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åé¦ˆå¾ªç¯` `ä¿¡å¿µå›ºåŒ–` `å›éŸ³å®¤æ•ˆåº”` `å¤šæ ·æ€§åˆ†æ` `å®è¯ç ”ç©¶` `é”å®šå‡è¯´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ç”¨æˆ·äº’åŠ¨ä¸­å¯èƒ½å¯¼è‡´ä¿¡å¿µçš„å›ºåŒ–ï¼Œå½¢æˆå›éŸ³å®¤æ•ˆåº”ï¼Œé™ä½ä¿¡æ¯å¤šæ ·æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºé”å®šå‡è¯´ï¼Œå½¢å¼åŒ–äººæœºåé¦ˆå¾ªç¯ï¼Œåˆ©ç”¨ä»£ç†æ¨¡å‹å’ŒçœŸå®æ•°æ®è¿›è¡Œå®è¯åˆ†æã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåˆ†æç»“æœæ˜¾ç¤ºæ–°GPTç‰ˆæœ¬å‘å¸ƒåï¼Œå¤šæ ·æ€§æ˜¾è‘—ä¸‹é™ï¼ŒéªŒè¯äº†åé¦ˆå¾ªç¯çš„å­˜åœ¨å’Œå½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®­ç»ƒå’Œéƒ¨ç½²ä¸äººç±»ç”¨æˆ·ä¹‹é—´å½¢æˆäº†åé¦ˆå¾ªç¯ï¼šæ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ äººç±»ä¿¡å¿µï¼Œé€šè¿‡ç”Ÿæˆå†…å®¹å¼ºåŒ–è¿™äº›ä¿¡å¿µï¼Œå†å°†å¼ºåŒ–åçš„ä¿¡å¿µåé¦ˆç»™ç”¨æˆ·ã€‚è¿™ä¸€åŠ¨æ€ç±»ä¼¼äºå›éŸ³å®¤ã€‚æˆ‘ä»¬å‡è®¾è¿™ç§åé¦ˆå¾ªç¯ä½¿ç”¨æˆ·ç°æœ‰çš„ä»·å€¼è§‚å’Œä¿¡å¿µå›ºåŒ–ï¼Œå¯¼è‡´å¤šæ ·æ€§çš„ä¸§å¤±ï¼Œå¹¶å¯èƒ½é”å®šé”™è¯¯ä¿¡å¿µã€‚æˆ‘ä»¬å½¢å¼åŒ–äº†è¿™ä¸€å‡è®¾ï¼Œå¹¶é€šè¿‡åŸºäºä»£ç†çš„LLMæ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„GPTä½¿ç”¨æ•°æ®è¿›è¡Œäº†å®è¯æµ‹è¯•ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œåœ¨æ–°GPTç‰ˆæœ¬å‘å¸ƒåï¼Œå¤šæ ·æ€§å‡ºç°äº†çªç„¶ä½†æŒç»­çš„ä¸‹é™ï¼Œè¿™ä¸å‡è®¾çš„äººæœºåé¦ˆå¾ªç¯ä¸€è‡´ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨https://thelockinhypothesis.comè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸ç”¨æˆ·ä¹‹é—´çš„åé¦ˆå¾ªç¯å¯¼è‡´ä¿¡å¿µå›ºåŒ–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œåº”å¯¹è¿™ç§å›éŸ³å®¤æ•ˆåº”ï¼Œå¯¼è‡´ä¿¡æ¯å¤šæ ·æ€§é™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé”å®šå‡è¯´ï¼Œè®¤ä¸ºäººæœºåé¦ˆå¾ªç¯ä¼šå¼ºåŒ–ç”¨æˆ·çš„ç°æœ‰ä¿¡å¿µï¼Œè¿›è€Œå›ºåŒ–è¿™äº›ä¿¡å¿µã€‚é€šè¿‡å½¢å¼åŒ–è¿™ä¸€å‡è®¾ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ†æè¿™ä¸€ç°è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŸºäºä»£ç†çš„LLMæ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„GPTä½¿ç”¨æ•°æ®åˆ†æã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒã€åé¦ˆå¾ªç¯æ¨¡æ‹Ÿå’Œå¤šæ ·æ€§åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå½¢å¼åŒ–äº†äººæœºåé¦ˆå¾ªç¯çš„æœºåˆ¶ï¼Œå¹¶é€šè¿‡å®è¯ç ”ç©¶éªŒè¯äº†è¿™ä¸€å‡è®¾ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ç”¨æˆ·ä¿¡å¿µçš„å˜åŒ–ï¼Œå¹¶é€šè¿‡å¤šæ¬¡è¿­ä»£åˆ†æåé¦ˆå¾ªç¯çš„å½±å“ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒç­–ç•¥ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æé«˜æ¨¡å‹çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ–°ç‰ˆæœ¬GPTå‘å¸ƒåï¼Œç”¨æˆ·ä¿¡å¿µçš„å¤šæ ·æ€§å‡ºç°äº†æ˜¾è‘—ä¸‹é™ï¼Œå…·ä½“è¡¨ç°ä¸ºå¤šæ ·æ€§æŒ‡æ•°é™ä½äº†çº¦30%ã€‚è¿™ä¸€ç»“æœä¸é”å®šå‡è¯´çš„é¢„æµ‹ä¸€è‡´ï¼ŒéªŒè¯äº†äººæœºåé¦ˆå¾ªç¯çš„å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“ã€åœ¨çº¿æ•™è‚²å’Œä¿¡æ¯æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡ç†è§£å’Œåº”å¯¹ä¿¡å¿µå›ºåŒ–ç°è±¡ï¼Œå¯ä»¥è®¾è®¡å‡ºæ›´å…·å¤šæ ·æ€§å’ŒåŒ…å®¹æ€§çš„å†…å®¹ç”Ÿæˆç³»ç»Ÿï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œä¿¡æ¯è´¨é‡ï¼Œå‡å°‘é”™è¯¯ä¿¡å¿µçš„ä¼ æ’­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The training and deployment of large language models (LLMs) create a feedback loop with human users: models learn human beliefs from data, reinforce these beliefs with generated content, reabsorb the reinforced beliefs, and feed them back to users again and again. This dynamic resembles an echo chamber. We hypothesize that this feedback loop entrenches the existing values and beliefs of users, leading to a loss of diversity and potentially the lock-in of false beliefs. We formalize this hypothesis and test it empirically with agent-based LLM simulations and real-world GPT usage data. Analysis reveals sudden but sustained drops in diversity after the release of new GPT iterations, consistent with the hypothesized human-AI feedback loop. Code and data available at https://thelockinhypothesis.com

