---
layout: default
title: Text-to-LoRA: Instant Transformer Adaption
---

# Text-to-LoRA: Instant Transformer Adaption

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06105" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06105v2</a>
  <a href="https://arxiv.org/pdf/2506.06105.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06105v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06105v2', 'Text-to-LoRA: Instant Transformer Adaption')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rujikorn Charakorn, Edoardo Cetin, Yujin Tang, Robert Tjarko Lange

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-06-09)

**å¤‡æ³¨**: Accepted at ICML 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/SakanaAI/text-to-lora)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºText-to-LoRAä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é€‚åº”æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŸºç¡€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹é€‚åº”` `LoRA` `è¶…ç½‘ç»œ` `é›¶-shotå­¦ä¹ ` `é«˜æ•ˆè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºç¡€æ¨¡å‹é€‚åº”æ–¹æ³•éœ€è¦è€—æ—¶çš„å¾®è°ƒå’Œæ•°æ®é›†ç­–åˆ’ï¼Œä¸”å¯¹è¶…å‚æ•°é€‰æ‹©æä¸ºæ•æ„Ÿã€‚
2. æœ¬æ–‡æå‡ºçš„Text-to-LoRAæ¨¡å‹èƒ½å¤ŸåŸºäºè‡ªç„¶è¯­è¨€æè¿°å³æ—¶é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½è®¡ç®—éœ€æ±‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒT2Lé‡æ„çš„LoRAå®ä¾‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½å¤Ÿè¿›è¡Œé›¶-shotæ³›åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºç¡€æ¨¡å‹ä¸ºå¿«é€Ÿå†…å®¹åˆ›ä½œæä¾›äº†é€šç”¨å·¥å…·ï¼Œä½†é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”ã€‚ä¼ ç»Ÿæ–¹æ³•éœ€è¦ä»”ç»†ç­–åˆ’æ•°æ®é›†å’Œåå¤å¾®è°ƒæ¨¡å‹ï¼Œè€—æ—¶ä¸”å¯¹è¶…å‚æ•°é€‰æ‹©æ•æ„Ÿã€‚ä¸ºå…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Text-to-LoRAï¼ˆT2Lï¼‰ï¼Œä¸€ç§èƒ½å¤ŸåŸºäºè‡ªç„¶è¯­è¨€æè¿°å³æ—¶é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨¡å‹ã€‚T2Læ˜¯ä¸€ä¸ªè¶…ç½‘ç»œï¼Œèƒ½å¤Ÿåœ¨ä¸€æ¬¡ä½æˆæœ¬çš„å‰å‘ä¼ é€’ä¸­æ„å»ºLoRAã€‚ç»è¿‡åœ¨9ä¸ªé¢„è®­ç»ƒLoRAé€‚é…å™¨ä¸Šçš„è®­ç»ƒï¼ŒT2Lé‡æ„çš„LoRAå®ä¾‹åœ¨ç›¸åº”æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¸ä»»åŠ¡ç‰¹å®šé€‚é…å™¨ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼ŒT2Lèƒ½å¤Ÿå‹ç¼©æ•°ç™¾ä¸ªLoRAå®ä¾‹ï¼Œå¹¶å¯¹å…¨æ–°ä»»åŠ¡è¿›è¡Œé›¶-shotæ³›åŒ–ã€‚è¿™ä¸€æ–¹æ³•ä¸ºåŸºç¡€æ¨¡å‹çš„ä¸“ä¸šåŒ–æ°‘ä¸»åŒ–æä¾›äº†é‡è¦è¿›å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºç¡€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡é€‚åº”ä¸­çš„é«˜æˆæœ¬å’Œä½æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºè€—æ—¶çš„å¾®è°ƒå’Œæ•°æ®é›†ç­–åˆ’ï¼Œå¯¼è‡´é€‚åº”è¿‡ç¨‹ç¹çä¸”ä¸çµæ´»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Text-to-LoRAï¼ˆT2Lï¼‰æ¨¡å‹é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°ç›´æ¥ç”ŸæˆLoRAé€‚é…å™¨ï¼Œé¿å…äº†ä¼ ç»Ÿå¾®è°ƒè¿‡ç¨‹ä¸­çš„å¤æ‚æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡è¶…ç½‘ç»œçš„è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­æ„å»ºé€‚é…å™¨ï¼Œæå¤§æé«˜äº†é€‚åº”æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šT2Lçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªè¶…ç½‘ç»œï¼Œè¯¥ç½‘ç»œç»è¿‡è®­ç»ƒåèƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆç›¸åº”çš„LoRAé€‚é…å™¨ã€‚æ¨¡å‹é¦–å…ˆæ¥æ”¶ä»»åŠ¡æè¿°ï¼Œç„¶åé€šè¿‡å†…éƒ¨æœºåˆ¶ç”Ÿæˆé€‚é…å™¨å‚æ•°ï¼Œæœ€åå°†è¿™äº›å‚æ•°åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šT2Lçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ç”ŸæˆLoRAé€‚é…å™¨ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤šæ¬¡å¾®è°ƒçš„æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æ•ˆç‡ï¼Œè¿˜é™ä½äº†å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒT2Lä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç”Ÿæˆçš„LoRAé€‚é…å™¨çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡åœ¨å¤šä¸ªé¢„è®­ç»ƒLoRAé€‚é…å™¨ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿å…¶èƒ½å¤Ÿæœ‰æ•ˆé€‚åº”å¤šç§ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒT2Lç”Ÿæˆçš„LoRAå®ä¾‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¸ä»»åŠ¡ç‰¹å®šé€‚é…å™¨çš„æ€§èƒ½ç›¸å½“ï¼Œä¸”åœ¨é›¶-shotè®¾ç½®ä¸‹èƒ½å¤Ÿæœ‰æ•ˆé€‚åº”å…¨æ–°ä»»åŠ¡ã€‚è¿™è¡¨æ˜T2Låœ¨é€‚åº”æ€§å’Œæ•ˆç‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Text-to-LoRAçš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºèƒ½å¤Ÿå¿«é€Ÿé€‚åº”ä¸åŒä»»åŠ¡ï¼Œé™ä½äº†å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨åŸºç¡€æ¨¡å‹åœ¨æ›´å¤šåº”ç”¨ä¸­çš„æ™®åŠå’Œä½¿ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyperparameter choices. To overcome these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting large language models (LLMs) on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets. Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements.
>   Our code is available at https://github.com/SakanaAI/text-to-lora

