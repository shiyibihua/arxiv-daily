---
layout: default
title: Delphos: A reinforcement learning framework for assisting discrete choice model specification
---

# Delphos: A reinforcement learning framework for assisting discrete choice model specification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06410" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.06410v2</a>
  <a href="https://arxiv.org/pdf/2506.06410.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06410v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06410v2', 'Delphos: A reinforcement learning framework for assisting discrete choice model specification')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Gabriel Nova, Stephane Hess, Sander van Cranenburgh

**ÂàÜÁ±ª**: econ.GN, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-06 (Êõ¥Êñ∞: 2025-07-25)

**Â§áÊ≥®**: 13 pages, 7 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DelphosÊ°ÜÊû∂‰ª•‰ºòÂåñÁ¶ªÊï£ÈÄâÊã©Ê®°ÂûãÁöÑËßÑËåÉËøáÁ®ã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Á¶ªÊï£ÈÄâÊã©Ê®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `Ê∑±Â∫¶QÁΩëÁªú` `Ê®°ÂûãËßÑËåÉ` `Â∫èÂàóÂÜ≥Á≠ñ` `È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã` `Ëá™ÈÄÇÂ∫îÊé¢Á¥¢` `ParetoÂâçÊ≤ø`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂ∞ÜÊ®°ÂûãËßÑËåÉËßÜ‰∏∫ÈùôÊÄÅ‰ºòÂåñÈóÆÈ¢òÔºåÁº∫‰πèÂä®ÊÄÅÈÄÇÂ∫îÊÄßÔºåÈöæ‰ª•ÊúâÊïàÊé¢Á¥¢Â§çÊùÇÁöÑÂª∫Ê®°Á©∫Èó¥„ÄÇ
2. DelphosÊ°ÜÊû∂Â∞ÜÊ®°ÂûãËßÑËåÉÈóÆÈ¢òËΩ¨Âåñ‰∏∫Â∫èÂàóÂÜ≥Á≠ñÈóÆÈ¢òÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïËá™ÈÄÇÂ∫îÈÄâÊã©Âª∫Ê®°Âä®‰ΩúÔºå‰ºòÂåñÊ®°ÂûãÂÄôÈÄâ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDelphosËÉΩÂ§üÈ´òÊïàÊé¢Á¥¢Â§ßËßÑÊ®°Âª∫Ê®°Á©∫Èó¥ÔºåËØÜÂà´Âπ≥Ë°°Ê®°ÂûãÊãüÂêà‰∏éË°å‰∏∫ÂêàÁêÜÊÄßÁöÑParetoÂâçÊ≤øÂÄôÈÄâÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êàë‰ª¨‰ªãÁªç‰∫ÜDelphosÔºå‰∏Ä‰∏™Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁî®‰∫éËæÖÂä©Á¶ªÊï£ÈÄâÊã©Ê®°ÂûãÁöÑËßÑËåÉËøáÁ®ã„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÂ∞ÜÊ®°ÂûãËßÑËåÉËßÜ‰∏∫ÈùôÊÄÅ‰ºòÂåñÈóÆÈ¢ò‰∏çÂêåÔºåDelphosÂ∞ÜËøô‰∏ÄÊåëÊàòÊ°ÜÊû∂Âåñ‰∏∫‰∏Ä‰∏™Â∫èÂàóÂÜ≥Á≠ñÈóÆÈ¢òÔºåÂΩ¢ÂºèÂåñ‰∏∫È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã„ÄÇÂú®Ëøô‰∏ÄËÆæÁΩÆ‰∏≠Ôºå‰ª£ÁêÜÈÄöËøáÈÄâÊã©‰∏ÄÁ≥ªÂàóÂª∫Ê®°Âä®‰ΩúÔºàÂ¶ÇÈÄâÊã©ÂèòÈáè„ÄÅÈÄÇÂ∫îÈÄöÁî®ÂíåÁâπÂÆöÊõø‰ª£ÂìÅÁöÑÂÅèÂ•ΩÂèÇÊï∞„ÄÅÂ∫îÁî®ÈùûÁ∫øÊÄßÂèòÊç¢ÂíåÂåÖÊã¨ÂçèÂèòÈáèÁöÑ‰∫§‰∫í‰ΩúÁî®ÔºâÊù•Â≠¶‰π†ÊåáÂÆöË°®Áé∞ËâØÂ•ΩÁöÑÊ®°ÂûãÂÄôÈÄâÔºåÂπ∂‰∏éÂª∫Ê®°ÁéØÂ¢É‰∫íÂä®ÔºåÂêéËÄÖ‰º∞ËÆ°ÊØè‰∏™ÂÄôÈÄâÊ®°ÂûãÂπ∂ËøîÂõûÂ•ñÂä±‰ø°Âè∑„ÄÇDelphos‰ΩøÁî®Ê∑±Â∫¶QÁΩëÁªúÔºåÊ†πÊçÆÂª∫Ê®°ÁªìÊûúÔºàÂ¶ÇÂØπÊï∞‰ººÁÑ∂ÔºâÂíåË°å‰∏∫È¢ÑÊúüÔºàÂ¶ÇÂèÇÊï∞Á¨¶Âè∑ÔºâÊé•Êî∂Âª∂ËøüÂ•ñÂä±ÔºåÂπ∂Âú®Âä®‰ΩúÂ∫èÂàó‰∏≠ÂàÜÈÖçÂ•ñÂä±Ôºå‰ª•Â≠¶‰π†Âì™‰∫õÂª∫Ê®°ÂÜ≥Á≠ñËÉΩÂØºËá¥Ë°®Áé∞ËâØÂ•ΩÁöÑÂÄôÈÄâÊ®°Âûã„ÄÇÊàë‰ª¨Âú®Ê®°ÊãüÂíåÂÆûËØÅÊï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞‰∫ÜDelphosÔºåÁªìÊûúË°®Êòé‰ª£ÁêÜËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Êé¢Á¥¢Á≠ñÁï•ÔºåËØÜÂà´Ë°®Áé∞ËâØÂ•ΩÁöÑÊ®°ÂûãÔºå‰∏îÊó†ÈúÄÂÖàÂâçÁöÑÈ¢ÜÂüüÁü•ËØÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Á¶ªÊï£ÈÄâÊã©Ê®°ÂûãËßÑËåÉËøáÁ®ã‰∏≠ÁöÑÂä®ÊÄÅÈÄÇÂ∫îÊÄß‰∏çË∂≥ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Â∞ÜÊ®°ÂûãËßÑËåÉËßÜ‰∏∫ÈùôÊÄÅ‰ºòÂåñÔºåÈöæ‰ª•ÊúâÊïàÂ∫îÂØπÂ§çÊùÇÁöÑÂª∫Ê®°Á©∫Èó¥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDelphosÊ°ÜÊû∂ÈÄöËøáÂ∞ÜÊ®°ÂûãËßÑËåÉËßÜ‰∏∫Â∫èÂàóÂÜ≥Á≠ñÈóÆÈ¢òÔºåÂà©Áî®Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºå‰Ωø‰ª£ÁêÜËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞ÈÄâÊã©Âª∫Ê®°Âä®‰ΩúÔºå‰ªéËÄå‰ºòÂåñÊ®°ÂûãÂÄôÈÄâ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDelphosÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™‰ª£ÁêÜ„ÄÅÂª∫Ê®°ÁéØÂ¢ÉÂíåÊ∑±Â∫¶QÁΩëÁªú„ÄÇ‰ª£ÁêÜÈÄöËøáÈÄâÊã©ÂèòÈáè„ÄÅË∞ÉÊï¥ÂÅèÂ•ΩÂèÇÊï∞Á≠âÂä®‰Ωú‰∏éÂª∫Ê®°ÁéØÂ¢É‰∫íÂä®ÔºåÁéØÂ¢ÉÂàôÊ†πÊçÆÊ®°ÂûãË°®Áé∞ËøîÂõûÂ•ñÂä±‰ø°Âè∑„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDelphosÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜÊ®°ÂûãËßÑËåÉÈóÆÈ¢òËΩ¨Âåñ‰∏∫È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºåÂÖÅËÆ∏‰ª£ÁêÜÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠Â≠¶‰π†Ôºå‰ªéËÄåÊúâÊïàÊé¢Á¥¢Âª∫Ê®°Á©∫Èó¥ÔºåËØÜÂà´È´òÊÄßËÉΩÊ®°ÂûãÂÄôÈÄâ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDelphos‰ΩøÁî®Ê∑±Â∫¶QÁΩëÁªúÊé•Êî∂Âª∂ËøüÂ•ñÂä±ÔºåÂ•ñÂä±‰ø°Âè∑Âü∫‰∫éÊ®°ÂûãÁöÑÂØπÊï∞‰ººÁÑ∂ÂíåÂèÇÊï∞Á¨¶Âè∑ÔºåËÆæËÆ°‰∫ÜÈÄÇÂ∫îÊÄßÁöÑÂ•ñÂä±ÂàÜÈÖçÊú∫Âà∂Ôºå‰ª•ÂºïÂØº‰ª£ÁêÜÂ≠¶‰π†ÊúÄ‰Ω≥Âª∫Ê®°ÂÜ≥Á≠ñ„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDelphosÂú®Ê®°ÊãüÂíåÂÆûËØÅÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåËÉΩÂ§üÊúâÊïàËØÜÂà´È´òÊÄßËÉΩÊ®°ÂûãÂÄôÈÄâÔºå‰∏îÂú®Êé¢Á¥¢ËøáÁ®ã‰∏≠Êó†ÈúÄÂÖàÂâçÁöÑÈ¢ÜÂüüÁü•ËØÜ„ÄÇ‰ª£ÁêÜËÉΩÂ§üÈõÜ‰∏≠ÊêúÁ¥¢‰∫éÈ´òÂ•ñÂä±Âå∫ÂüüÔºåÊàêÂäüÂÆö‰πâÂá∫Âπ≥Ë°°Ê®°ÂûãÊãüÂêà‰∏éË°å‰∏∫ÂêàÁêÜÊÄßÁöÑParetoÂâçÊ≤ø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DelphosÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂Âú®ÁªèÊµéÂ≠¶„ÄÅÂ∏ÇÂú∫Á†îÁ©∂ÂíåÁ§æ‰ºöÁßëÂ≠¶Á≠âÈ¢ÜÂüüÁöÑÊ®°ÂûãËßÑËåÉ‰∏≠ÔºåÂèØ‰ª•Â∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÊõ¥È´òÊïàÂú∞ÊûÑÂª∫Âíå‰ºòÂåñÁ¶ªÊï£ÈÄâÊã©Ê®°ÂûãÔºåÊèêÂçáÊ®°ÂûãÁöÑÈ¢ÑÊµãËÉΩÂäõÂíåËß£ÈáäÊÄß„ÄÇÊú™Êù•ÔºåËØ•Ê°ÜÊû∂ÂèØËÉΩÊé®Âä®Êõ¥Â§çÊùÇÂÜ≥Á≠ñËøáÁ®ãÁöÑÂª∫Ê®°‰∏éÂàÜÊûê„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We introduce Delphos, a deep reinforcement learning framework for assisting the discrete choice model specification process. Unlike traditional approaches that treat model specification as a static optimisation problem, Delphos represents a paradigm shift: it frames this specification challenge as a sequential decision-making problem, formalised as a Markov Decision Process. In this setting, an agent learns to specify well-performing model candidates by choosing a sequence of modelling actions - such as selecting variables, accommodating both generic and alternative-specific taste parameters, applying non-linear transformations, and including interactions with covariates - and interacting with a modelling environment that estimates each candidate and returns a reward signal. Specifically, Delphos uses a Deep Q-Network that receives delayed rewards based on modelling outcomes (e.g., log-likelihood) and behavioural expectations (e.g., parameter signs), and distributes rewards across the sequence of actions to learn which modelling decisions lead to well-performing candidates. We evaluate Delphos on both simulated and empirical datasets, varying the size of the modelling space and the reward function. To assess the agent's performance in navigating the model space, we analyse the learning curve, the distribution of Q-values, occupancy metrics, and Pareto fronts. Our results show that the agent learns to adaptively explore strategies to identify well-performing models across search spaces, even without prior domain knowledge. It efficiently explores large modelling spaces, concentrates its search in high-reward regions, and suggests candidates that define Pareto frontiers balancing model fit and behavioural plausibility. These findings highlight the potential of this novel adaptive, learning-based framework to assist in the model specification process.

