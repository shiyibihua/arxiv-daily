---
layout: default
title: Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance
---

# Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05748" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05748v1</a>
  <a href="https://arxiv.org/pdf/2506.05748.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05748v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05748v1', 'Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rudransh Agnihotri, Ananya Pandey

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆåœ¨çº¿RFTæ–¹æ³•ä»¥è§£å†³RLHFä¸­çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒç“¶é¢ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `å¥–åŠ±æ¨¡å‹` `åœ¨çº¿è¯„ä¼°` `LoRAé€‚é…å™¨` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡å‚æ•°å’Œå¤æ‚çš„ç¦»çº¿è°ƒæ•´ï¼Œå¯¼è‡´æˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡ç®€å•çš„JSONè§„åˆ™å’ŒLoRAé€‚é…å™¨æ¥å¢å¼ºLLMçš„æ–¹æ³•ï¼Œæ—¨åœ¨é™ä½æˆæœ¬å¹¶æé«˜è¯„ä¼°æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨RewardBenchå’ŒGSM-8Kä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥–åŠ±æ¨¡å‹è®­ç»ƒæ˜¯ç°ä»£å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ç®¡é“ä¸­çš„æˆæœ¬ç“¶é¢ˆï¼Œé€šå¸¸éœ€è¦æ•°åäº¿å‚æ•°å’Œç¦»çº¿åå¥½è°ƒæ•´é˜¶æ®µã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•é€šè¿‡ä»…ä½¿ç”¨ä¸€è¡ŒJSONè§„åˆ™å’Œä¸€ä¸ªå½±å“æ¨¡å‹å‚æ•°0.8%çš„rank-16 LoRAé€‚é…å™¨ï¼Œå¢å¼ºäº†ä¸€ä¸ªå†»ç»“çš„ã€ç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„7B LLMï¼Œä½¿å…¶èƒ½å¤Ÿå®Œå…¨æ›¿ä»£ä¹‹å‰ä½¿ç”¨çš„é‡å‹è¯„ä¼°æ¨¡å‹ã€‚è¯¥æ’ä»¶å¼è¯„ä¼°è€…åœ¨RewardBenchä¸Šè¾¾åˆ°äº†96.2%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å‚æ•°èŒƒå›´ä»27Båˆ°70Bçš„ä¸“ç”¨å¥–åŠ±ç½‘ç»œã€‚æ­¤å¤–ï¼Œå®ƒä½¿å¾—7Bçš„æ¼”å‘˜åœ¨åœ¨çº¿PPOä¸Šå®ç°92%çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å¾—åˆ†ä¸º61.8%çš„70B DPOåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°ä»£RLHFç®¡é“ä¸­å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„é«˜æˆæœ¬å’Œä½æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºåºå¤§çš„æ¨¡å‹å’Œå¤æ‚çš„ç¦»çº¿è®­ç»ƒè¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ä¸€ä¸ªå†»ç»“çš„7B LLMï¼Œå¹¶ç»“åˆç®€å•çš„JSONè§„åˆ™å’ŒLoRAé€‚é…å™¨ï¼Œæœ¬æ–‡å®ç°äº†é«˜æ•ˆçš„åœ¨çº¿å¥–åŠ±è¯„ä¼°ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„å¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªæŒ‡ä»¤è°ƒä¼˜çš„LLMä½œä¸ºåŸºç¡€æ¨¡å‹ï¼ŒLoRAé€‚é…å™¨ç”¨äºå¢å¼ºæ¨¡å‹çš„è¯„ä¼°èƒ½åŠ›ï¼Œç»“åˆåœ¨çº¿PPOç®—æ³•è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºä½¿ç”¨å°å‹LoRAé€‚é…å™¨æ›¿ä»£å¤§å‹è¯„ä¼°æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†å‚æ•°éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å‡†ç¡®ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹ä½¿ç”¨äº†ä»…å½±å“0.8%å‚æ•°çš„rank-16 LoRAé€‚é…å™¨ï¼Œç»“åˆå…­ä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä»¥æå‡é›¶åˆ°å°‘é‡æ ·æœ¬çš„è¡¨ç°ï¼Œå¹¶åœ¨å®‰å…¨æ€§å’Œå¯¹æŠ—æ€§åœºæ™¯ä¸­ä¼˜åŒ–äº†è¯„ä¼°æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„LoRAè¯„ä¼°è€…åœ¨RewardBenchä¸Šè¾¾åˆ°äº†96.2%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†27Bè‡³70Bå‚æ•°çš„ä¸“ç”¨å¥–åŠ±ç½‘ç»œã€‚æ­¤å¤–ï¼Œ7Bçš„æ¼”å‘˜åœ¨GSM-8Kä¸Šå®ç°äº†92%çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ï¼Œæ˜¾è‘—é«˜äº70B DPOåŸºçº¿çš„61.8%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–è¯„ä¼°ç­‰ã€‚é€šè¿‡é™ä½å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæˆæœ¬å’Œå¤æ‚æ€§ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä¿ƒè¿›RLHFåœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›æ¨å¹¿ï¼Œæå‡æ™ºèƒ½ç³»ç»Ÿçš„å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reward-model training is the cost bottleneck in modern Reinforcement Learning Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters and an offline preference-tuning phase. In the proposed method, a frozen, instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a rank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling it to serve as a complete substitute for the previously used heavyweight evaluation models. The plug-and-play judge achieves 96.2% accuracy on RewardBench, outperforming specialized reward networks ranging from 27B to 70B parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K utilizing online PPO. Thorough ablations indicate that (i) six in context demonstrations deliver the majority of the zero-to-few-shot improvements (+2pp), and (ii) the LoRA effectively addresses the remaining disparity, particularly in the safety and adversarial Chat-Hard segments. The proposed model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic HH-RLHF, to examine interpretability, accompanied by human generated justifications. GPT-4 scoring indicates that our LoRA judge attains approximately = 9/10 in similarity to human explanations, while zero-shot judges score around =5/10. These results indicate that the combination of prompt engineering and tiny LoRA produces a cost effective, transparent, and easily adjustable reward function, removing the offline phase while achieving new state-of-the-art outcomes for both static evaluation and online RLHF.

