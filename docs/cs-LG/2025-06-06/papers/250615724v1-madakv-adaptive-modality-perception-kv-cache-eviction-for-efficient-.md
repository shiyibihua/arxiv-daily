---
layout: default
title: MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference
---

# MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15724" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15724v1</a>
  <a href="https://arxiv.org/pdf/2506.15724.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15724v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15724v1', 'MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kunxi Li, Zhonghua Jiang, Zhouzhou Shen, Zhaode Wang, Chengfei Lv, Shengyu Zhang, Fan Wu, Fei Wu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMadaKVä»¥è§£å†³å¤šæ¨¡æ€é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„KVç¼“å­˜æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€é•¿ä¸Šä¸‹æ–‡` `KVç¼“å­˜` `æ¨ç†æ•ˆç‡` `æ¨¡æ€é€‚åº”` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„KVç¼“å­˜é©±é€æ–¹æ³•ä¸»è¦é’ˆå¯¹å•æ¨¡æ€è®¾ç½®ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å¤šæ¨¡æ€åœºæ™¯ä¸­çš„æ¨¡æ€ç‰¹å®šä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. MadaKVé€šè¿‡æ¨¡æ€åå¥½é€‚åº”å’Œåˆ†å±‚å‹ç¼©è¡¥å¿ï¼ŒåŠ¨æ€æ„ŸçŸ¥æ¨¡æ€ä¿¡æ¯å¹¶è‡ªé€‚åº”ä¿ç•™å…³é”®æ ‡è®°ï¼Œä»è€Œæå‡æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMadaKVåœ¨å¤šä¸ªå¤šæ¨¡æ€é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­ï¼ŒKVç¼“å­˜å†…å­˜å ç”¨å’Œè§£ç å»¶è¿Ÿæ˜¾è‘—é™ä½ï¼Œå‡†ç¡®ç‡ä¿æŒé«˜æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†MadaKVï¼Œä¸€ç§é€‚åº”æ€§æ¨¡æ€æ„ŸçŸ¥çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜é©±é€ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„æ•ˆç‡ã€‚åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­ï¼Œæ³¨æ„åŠ›å¤´å¯¹ä¸åŒæ¨¡æ€çš„åå¥½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¯¼è‡´æ¨¡æ€é‡è¦æ€§åœ¨æ³¨æ„åŠ›å¤´ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¼ ç»Ÿçš„KVç¼“å­˜é©±é€æ–¹æ³•æœªèƒ½æ•æ‰æ¨¡æ€ç‰¹å®šä¿¡æ¯ï¼Œå› æ­¤è¡¨ç°ä¸ä½³ã€‚MadaKVé€šè¿‡æ¨¡æ€åå¥½é€‚åº”å’Œåˆ†å±‚å‹ç¼©è¡¥å¿ä¸¤ä¸ªå…³é”®ç»„ä»¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é€šè¿‡åŠ¨æ€æ„ŸçŸ¥æ³¨æ„åŠ›å¤´ä¸­çš„æ¨¡æ€ä¿¡æ¯å¹¶è‡ªé€‚åº”ä¿ç•™å…³é”®æ ‡è®°ï¼ŒMadaKVåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå®ç°äº†KVç¼“å­˜å†…å­˜å ç”¨å’Œæ¨¡å‹æ¨ç†è§£ç å»¶è¿Ÿçš„æ˜¾è‘—é™ä½ï¼ˆæå‡1.3åˆ°1.5å€ï¼‰ã€‚åœ¨ä»£è¡¨æ€§çš„MLLMså’ŒMileBenchåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMadaKVç›¸è¾ƒäºç°æœ‰KVç¼“å­˜é©±é€æ–¹æ³•å…·æœ‰æ›´é«˜çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­ï¼Œä¼ ç»ŸKVç¼“å­˜é©±é€æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•æ‰æ¨¡æ€ç‰¹å®šä¿¡æ¯çš„é—®é¢˜ã€‚è¿™å¯¼è‡´åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸‹ï¼Œæ¨¡å‹çš„æ¨ç†æ•ˆç‡å’Œæ€§èƒ½å—åˆ°é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMadaKVçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¨¡æ€åå¥½é€‚åº”å’Œåˆ†å±‚å‹ç¼©è¡¥å¿ï¼ŒåŠ¨æ€è°ƒæ•´KVç¼“å­˜ä¸­çš„ä¿¡æ¯ä¿ç•™ç­–ç•¥ï¼Œä»¥é€‚åº”ä¸åŒæ¨¡æ€çš„éœ€æ±‚ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMadaKVçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ¨¡æ€åå¥½é€‚åº”æ¨¡å—å’Œåˆ†å±‚å‹ç¼©è¡¥å¿æ¨¡å—ã€‚å‰è€…è´Ÿè´£åŠ¨æ€æ„ŸçŸ¥æ¨¡æ€ä¿¡æ¯ï¼Œåè€…åˆ™é€šè¿‡å‹ç¼©ä¸é‡è¦çš„æ ‡è®°æ¥ä¼˜åŒ–ç¼“å­˜ä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šMadaKVçš„åˆ›æ–°ç‚¹åœ¨äºå…¶æ¨¡æ€é€‚åº”æ€§è®¾è®¡ï¼Œä½¿å¾—KVç¼“å­˜çš„é©±é€ç­–ç•¥èƒ½å¤Ÿæ ¹æ®ä¸åŒæ¨¡æ€çš„éœ€æ±‚è¿›è¡Œè°ƒæ•´ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€é©±é€ç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒMadaKVé‡‡ç”¨äº†åŠ¨æ€æ¨¡æ€æ„ŸçŸ¥æœºåˆ¶ï¼Œèƒ½å¤Ÿå®æ—¶è¯„ä¼°å„æ¨¡æ€çš„é‡è¦æ€§ï¼Œå¹¶æ ¹æ®è¯„ä¼°ç»“æœè‡ªé€‚åº”è°ƒæ•´ç¼“å­˜ä¸­çš„æ ‡è®°ä¿ç•™ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMadaKVåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»ŸKVç¼“å­˜é©±é€æ–¹æ³•ï¼ŒKVç¼“å­˜å†…å­˜å ç”¨æ˜¾è‘—é™ä½ï¼ŒåŒæ—¶æ¨ç†è§£ç å»¶è¿Ÿæå‡1.3åˆ°1.5å€ï¼Œä¸”åœ¨å‡†ç¡®ç‡ä¸Šä¿æŒé«˜æ°´å¹³ï¼Œå±•ç°å‡ºå…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MadaKVçš„ç ”ç©¶æˆæœåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„åœºæ™¯ï¼Œå¦‚è§†é¢‘ç†è§£ã€å›¾åƒæè¿°å’Œå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æ¨ç†æ•ˆç‡ï¼ŒMadaKVèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´å¿«é€Ÿå’Œå‡†ç¡®çš„å“åº”ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces MadaKV, a modality-adaptive key-value (KV) cache eviction strategy designed to enhance the efficiency of multimodal large language models (MLLMs) in long-context inference. In multimodal scenarios, attention heads exhibit varying preferences for different modalities, resulting in significant disparities in modality importance across attention heads. Traditional KV cache eviction methods, which are tailored for unimodal settings, fail to capture modality-specific information, thereby yielding suboptimal performance. MadaKV addresses these challenges through two key components: modality preference adaptation and hierarchical compression compensation. By dynamically sensing modality information within attention heads and adaptively retaining critical tokens, MadaKV achieves substantial reductions in KV cache memory footprint and model inference decoding latency (1.3 to 1.5 times improvement) while maintaining high accuracy across various multimodal long-context tasks. Extensive experiments on representative MLLMs and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to existing KV cache eviction methods.

