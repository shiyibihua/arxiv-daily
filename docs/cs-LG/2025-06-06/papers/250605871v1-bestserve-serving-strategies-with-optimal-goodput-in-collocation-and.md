---
layout: default
title: BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures
---

# BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05871" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05871v1</a>
  <a href="https://arxiv.org/pdf/2506.05871.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05871v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05871v1', 'BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiannan Hu, Tianyou Zeng, Xiaoming Yuan, Liwei Song, Guangyuan Zhang, Bangzheng He

**åˆ†ç±»**: cs.LG, cs.DC, cs.PF

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBestServeä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æœåŠ¡ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœåŠ¡ç­–ç•¥` `èµ„æºåˆ†é…` `å¹¶è¡Œè®¡ç®—` `æ¨ç†æ¨¡æ‹Ÿ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¼˜åŒ–æ¡†æ¶` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›æœåŠ¡æ—¶ï¼Œèµ„æºåˆ†é…å’Œå¹¶è¡Œç­–ç•¥çš„é€‰æ‹©å¾€å¾€ä¾èµ–äºç¹ççš„è¯•é”™è¿‡ç¨‹ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. BestServeæ¡†æ¶é€šè¿‡ä¼°è®¡ä¸åŒæ“ä½œåœºæ™¯ä¸‹çš„è‰¯å¥½ååé‡ï¼Œå¿«é€Ÿæ’åæœåŠ¡ç­–ç•¥ï¼Œæ”¯æŒååŒå’Œåˆ†æ•£æ¶æ„ã€‚
3. è¯¥æ¡†æ¶åœ¨å•ä¸ªæ ‡å‡†CPUä¸Šèƒ½åœ¨å‡ åˆ†é’Ÿå†…æ‰¾åˆ°æœ€ä½³ç­–ç•¥ï¼Œé¢„æµ‹è¯¯å·®åœ¨20%ä»¥å†…ï¼Œæ˜¾è‘—æå‡äº†èµ„æºåˆ©ç”¨æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºæ»¡è¶³æ•°ç™¾ä¸‡ç”¨æˆ·å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœåŠ¡éœ€æ±‚ï¼Œèµ„æºåˆ†é…å’Œå¹¶è¡Œç­–ç•¥çš„é«˜æ•ˆæ€§è‡³å…³é‡è¦ã€‚BestServeæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨ä¸åŒæ“ä½œåœºæ™¯ä¸‹ä¼°è®¡è‰¯å¥½ååé‡æ¥å¯¹æœåŠ¡ç­–ç•¥è¿›è¡Œæ’åã€‚è¯¥æ¡†æ¶æ”¯æŒååŒå’Œåˆ†æ•£æ¶æ„ï¼Œåˆ©ç”¨åŸºäºæ”¹è¿›çš„å±‹é¡¶çº¿æ¨¡å‹å’ŒCPU-GPUè°ƒåº¦åŠ¨æ€æ„å»ºçš„æ¨ç†æ¨¡æ‹Ÿå™¨ã€‚BestServeèƒ½å¤Ÿåœ¨å•ä¸ªæ ‡å‡†CPUä¸Šåœ¨å‡ åˆ†é’Ÿå†…ç¡®å®šæœ€ä½³ç­–ç•¥ï¼Œæ¶ˆé™¤äº†é«˜æˆæœ¬åŸºå‡†æµ‹è¯•çš„éœ€æ±‚ï¼ŒåŒæ—¶å®ç°äº†20%çš„é¢„æµ‹è¯¯å·®èŒƒå›´ã€‚è¿™ä¸€è½»é‡çº§è®¾è®¡å’Œå¼ºå¤§çš„å¯æ‰©å±•æ€§ä½¿å…¶åœ¨å¿«é€Ÿéƒ¨ç½²è§„åˆ’ä¸­å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›æœåŠ¡æ—¶ï¼Œå¦‚ä½•é«˜æ•ˆåœ°é€‰æ‹©èµ„æºåˆ†é…å’Œå¹¶è¡Œç­–ç•¥çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºè€—æ—¶çš„è¯•é”™è¿‡ç¨‹ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBestServeé€šè¿‡æ„å»ºæ¨ç†æ¨¡æ‹Ÿå™¨ï¼ŒåŸºäºæ”¹è¿›çš„å±‹é¡¶çº¿æ¨¡å‹å’ŒCPU-GPUè°ƒåº¦åŠ¨æ€ï¼Œå¿«é€Ÿè¯„ä¼°å’Œæ’åä¸åŒçš„æœåŠ¡ç­–ç•¥ï¼Œä»è€Œæ‰¾åˆ°æœ€ä½³æ–¹æ¡ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBestServeçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¨ç†æ¨¡æ‹Ÿå™¨ã€ç­–ç•¥è¯„ä¼°æ¨¡å—å’Œç»“æœä¼˜åŒ–æ¨¡å—ã€‚æ¨ç†æ¨¡æ‹Ÿå™¨è´Ÿè´£æ¨¡æ‹Ÿä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½ï¼Œç­–ç•¥è¯„ä¼°æ¨¡å—åˆ™æ ¹æ®æ¨¡æ‹Ÿç»“æœè¿›è¡Œæ’åï¼Œæœ€ç»ˆé€šè¿‡ç»“æœä¼˜åŒ–æ¨¡å—ç¡®å®šæœ€ä½³ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šBestServeçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æ¨ç†æ¨¡æ‹Ÿå™¨çš„è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œæ˜‚è´µåŸºå‡†æµ‹è¯•çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿé¢„æµ‹æœåŠ¡ç­–ç•¥çš„æ€§èƒ½ï¼Œä¸”è¯¯å·®æ§åˆ¶åœ¨20%ä»¥å†…ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒBestServeé‡‡ç”¨äº†æ”¹è¿›çš„å±‹é¡¶çº¿æ¨¡å‹æ¥æè¿°ç³»ç»Ÿæ€§èƒ½ï¼Œå¹¶ç»“åˆCPU-GPUè°ƒåº¦åŠ¨æ€ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ¶æ„ä¸‹çš„é€‚ç”¨æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBestServeèƒ½å¤Ÿåœ¨å•ä¸ªæ ‡å‡†CPUä¸Šåœ¨å‡ åˆ†é’Ÿå†…ç¡®å®šæœ€ä½³æœåŠ¡ç­–ç•¥ï¼Œä¸”é¢„æµ‹è¯¯å·®æ§åˆ¶åœ¨20%ä»¥å†…ã€‚è¿™ä¸€æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨èµ„æºåˆ†é…å’Œå¹¶è¡Œç­–ç•¥ä¼˜åŒ–æ–¹é¢çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BestServeæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä¸ºå¤§é‡ç”¨æˆ·æä¾›é«˜æ•ˆæœåŠ¡çš„åœºæ™¯ï¼Œå¦‚åœ¨çº¿æ•™è‚²ã€æ™ºèƒ½å®¢æœå’Œç¤¾äº¤åª’ä½“ç­‰é¢†åŸŸã€‚å…¶è½»é‡çº§å’Œå¯æ‰©å±•çš„ç‰¹æ€§ä½¿å¾—å¿«é€Ÿéƒ¨ç½²æˆä¸ºå¯èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡èµ„æºåˆ©ç”¨ç‡å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Serving large language models (LLMs) to millions of users requires efficient resource allocation and parallelism strategies. It is a labor intensive trial-and-error process to find such a strategy. We present BestServe, a novel framework for ranking serving strategies by estimating goodput under various operating scenarios. Supporting both collocated and disaggregated architectures, BestServe leverages an inference simulator built on an adapted roofline model and CPU-GPU dispatch dynamics. Our framework determines the optimal strategy in minutes on a single standard CPU, eliminating the need for costly benchmarking, while achieving predictions within a $20\%$ error margin. It appeals to be practical for rapid deployment planning because of its lightweight design and strong extensibility.

