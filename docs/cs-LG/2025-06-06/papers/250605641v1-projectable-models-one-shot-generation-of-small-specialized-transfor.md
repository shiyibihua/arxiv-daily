---
layout: default
title: Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones
---

# Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05641" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05641v1</a>
  <a href="https://arxiv.org/pdf/2506.05641.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05641v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05641v1', 'Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrey Zhmoginov, Jihwan Lee, Mark Sandler

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: Presented at ES-FoMo II: 2nd Workshop on Efficient Systems for Foundation Models (ICML 2024)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯æŠ•å½±æ¨¡å‹ä»¥å®ç°å°å‹ä¸“ç”¨å˜æ¢å™¨çš„ä¸€æ¬¡æ€§ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŸºç¡€æ¨¡å‹` `å˜æ¢å™¨` `å‚æ•°æ˜ å°„` `å°å‹æ¨¡å‹` `å›¾åƒå»ºæ¨¡` `ä»»åŠ¡ç‰¹å®š` `æ¨¡å‹å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—ä¸Šéå¸¸æ˜‚è´µï¼Œä¸”å…¶å¹¿æ³›çŸ¥è¯†å¯¹ç‰¹å®šä»»åŠ¡çš„ç›¸å…³æ€§è¾ƒä½ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å¤§å‹å˜æ¢å™¨å‚æ•°æ˜ å°„åˆ°å°å‹ä¸“ç”¨æ¨¡å‹çš„æŠ€æœ¯ï¼Œæ—¨åœ¨æ•æ‰ç‰¹å®šä»»åŠ¡æ‰€éœ€çš„çŸ¥è¯†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆçš„å°å‹æ¨¡å‹åœ¨å›¾åƒå»ºæ¨¡ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†ä¼ ç»Ÿçš„é€šç”¨æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰é€šå¸¸åœ¨æ¶µç›–å¤šç§æ•°æ®æ¨¡æ€ã€ä¸»é¢˜å’Œä¸‹æ¸¸ä»»åŠ¡çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨è¿™äº›æ¨¡å‹çš„è®¡ç®—æˆæœ¬éå¸¸é«˜ï¼Œè¶…å‡ºäº†å¤§å¤šæ•°æ¶ˆè´¹è®¾å¤‡çš„æ‰¿å—èŒƒå›´ã€‚æ­¤å¤–ï¼Œå¹¿æ³›çš„FMçŸ¥è¯†å¯¹äºç‰¹å®šä»»åŠ¡å¯èƒ½å¹¶ä¸ç›¸å…³ã€‚æœ¬æ–‡æ¢è®¨äº†ä¸€ç§å°†å¤§å‹å˜æ¢å™¨çš„å‚æ•°æ˜ å°„åˆ°å°å‹ä¸“ç”¨æ¨¡å‹å‚æ•°çš„æŠ€æœ¯ã€‚é€šè¿‡ä½¿è¿™ä¸€è½¬æ¢ä»»åŠ¡ç‰¹å®šï¼Œæˆ‘ä»¬æ—¨åœ¨æ•æ‰æ‰§è¡Œç‰¹å®šä»»åŠ¡æ‰€éœ€çš„æ›´çª„çŸ¥è¯†èŒƒå›´ã€‚æˆ‘ä»¬åœ¨å›¾åƒå»ºæ¨¡ä»»åŠ¡ä¸Šç ”ç©¶äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½è¶…è¿‡äº†é€šç”¨æ¡ä»¶æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹åŸºç¡€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è®¡ç®—æˆæœ¬é«˜å’ŒçŸ¥è¯†ç›¸å…³æ€§ä½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨å¤§å‹æ¨¡å‹çš„çŸ¥è¯†ï¼Œå¯¼è‡´èµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ä»»åŠ¡ç‰¹å®šçš„å‚æ•°æ˜ å°„æŠ€æœ¯ï¼Œé€šè¿‡å°†å¤§å‹å˜æ¢å™¨çš„å‚æ•°è½¬åŒ–ä¸ºå°å‹ä¸“ç”¨æ¨¡å‹çš„å‚æ•°ï¼Œæ¥æ•æ‰æ‰§è¡Œç‰¹å®šä»»åŠ¡æ‰€éœ€çš„çŸ¥è¯†ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜å°å‹æ¨¡å‹çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å‚æ•°æ˜ å°„æ¨¡å—å’Œå°å‹æ¨¡å‹è®­ç»ƒæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ†æå¤§å‹æ¨¡å‹çš„å‚æ•°ï¼Œè¯†åˆ«å‡ºä¸ç‰¹å®šä»»åŠ¡ç›¸å…³çš„çŸ¥è¯†ï¼Œç„¶åå°†è¿™äº›çŸ¥è¯†æ˜ å°„åˆ°å°å‹æ¨¡å‹ä¸­ï¼Œæœ€åå¯¹å°å‹æ¨¡å‹è¿›è¡Œè®­ç»ƒä»¥ä¼˜åŒ–å…¶æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å‚æ•°æ˜ å°„æ–¹æ³•ï¼Œä½¿å¾—å°å‹æ¨¡å‹èƒ½å¤Ÿåœ¨ç‰¹å®šä»»åŠ¡ä¸­è¶…è¶Šä¼ ç»Ÿçš„é€šç”¨æ¨¡å‹ã€‚è¿™ä¸€æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶ä»»åŠ¡ç‰¹å®šæ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å¤§å‹æ¨¡å‹çš„çŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°æ˜ å°„è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ˜ å°„æ•ˆæœï¼Œå¹¶è®¾è®¡äº†é€‚åˆå°å‹æ¨¡å‹çš„ç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜è¶Šã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç”Ÿæˆçš„å°å‹æ¨¡å‹åœ¨å›¾åƒå»ºæ¨¡ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—ä¼˜äºé€šç”¨æ¡ä»¶æ¨¡å‹ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒå¤„ç†ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰éœ€è¦é«˜æ•ˆæ¨¡å‹çš„ä»»åŠ¡ã€‚é€šè¿‡ç”Ÿæˆå°å‹ä¸“ç”¨å˜æ¢å™¨ï¼Œèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆçš„æ¨ç†ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern Foundation Models (FMs) are typically trained on corpora spanning a wide range of different data modalities, topics and downstream tasks. Utilizing these models can be very computationally expensive and is out of reach for most consumer devices. Furthermore, most of the broad FM knowledge may actually be irrelevant for a specific task at hand. Here we explore a technique for mapping parameters of a large Transformer to parameters of a smaller specialized model. By making this transformation task-specific, we aim to capture a narrower scope of the knowledge needed for performing a specific task by a smaller model. We study our method on image modeling tasks, showing that performance of generated models exceeds that of universal conditional models.

