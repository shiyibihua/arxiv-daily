---
layout: default
title: Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning
---

# Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05977" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05977v1</a>
  <a href="https://arxiv.org/pdf/2506.05977.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05977v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05977v1', 'Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yujia Huo, Jianchun Liu, Hongli Xu, Zhenguo Ma, Shilong Wang, Liusheng Huang

**åˆ†ç±»**: cs.LG, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFedBEä»¥è§£å†³è”é‚¦å¾®è°ƒä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `è‡ªé€‚åº”å˜æ¢å™¨` `æ¨¡å‹å¾®è°ƒ` `æ•°æ®éšç§` `å¼‚æ„ç¯å¢ƒ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è”é‚¦å¾®è°ƒæ–¹æ³•æœªèƒ½æœ‰æ•ˆè§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¼‚æ„æ•°æ®åˆ†å¸ƒå’Œè®¾å¤‡èƒ½åŠ›å·®å¼‚çš„æƒ…å†µä¸‹ã€‚
2. æœ¬æ–‡æå‡ºFedBEæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”å˜æ¢å™¨å—æ‰©å±•æœºåˆ¶å’ŒåŠ¨æ€å—åˆ†é…ç­–ç•¥ï¼Œè§£å†³äº†è”é‚¦ç¯å¢ƒä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedBEåœ¨å¾®è°ƒåçš„å‡†ç¡®ç‡ä¿æŒä¸Šæ¯”ç°æœ‰æ–¹æ³•æé«˜äº†12-74%ï¼Œå¹¶ä¸”æ¨¡å‹æ”¶æ•›é€Ÿåº¦åŠ å¿«äº†1.9-3.1å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦å¾®è°ƒï¼ˆFedFTï¼‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºé€‚åº”åˆ†å¸ƒå¼æ•°æ®ç¯å¢ƒçš„æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ç¡®ä¿æ•°æ®éšç§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„FedFTæ–¹æ³•ä¸»è¦é‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ï¼Œæœªèƒ½æœ‰æ•ˆåº”å¯¹ç¾éš¾æ€§é—å¿˜è¿™ä¸€å…³é”®æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FedBEæ¡†æ¶ï¼Œç»“åˆè‡ªé€‚åº”å˜æ¢å™¨å—æ‰©å±•æœºåˆ¶å’ŒåŠ¨æ€å¯è®­ç»ƒå—åˆ†é…ç­–ç•¥ï¼Œæ—¨åœ¨æ›´å¥½åœ°é€‚åº”å¼‚æ„çš„è”é‚¦ç¯å¢ƒå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedBEåœ¨å¾®è°ƒåç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œå‡†ç¡®ç‡ä¿æŒæå‡12-74%ï¼Œæ¨¡å‹æ”¶æ•›åŠ é€Ÿæ¯”ä¸º1.9-3.1å€ï¼Œä¸”æœªé™ä½ä¸‹æ¸¸ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è”é‚¦å¾®è°ƒä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¼‚æ„ç¯å¢ƒä¸­æ— æ³•æœ‰æ•ˆåº”å¯¹æ•°æ®åˆ†å¸ƒå’Œè®¾å¤‡èƒ½åŠ›çš„å·®å¼‚ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFedBEæ¡†æ¶é€šè¿‡æ‰©å±•å¯è®­ç»ƒçš„å˜æ¢å™¨å—ï¼Œå°†æ–°å­¦ä¹ çš„ä»»åŠ¡ç‰¹å®šçŸ¥è¯†ä¸åŸæœ‰çš„é¢„è®­ç»ƒè¡¨ç¤ºç»“æ„æ€§åˆ†ç¦»ï¼Œä»è€Œå‡å°‘é—å¿˜ç°è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFedBEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªé€‚åº”å˜æ¢å™¨å—æ‰©å±•å’ŒåŠ¨æ€å—åˆ†é…ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚è‡ªé€‚åº”æ‰©å±•æ¨¡å—è´Ÿè´£æ ¹æ®å®¢æˆ·ç«¯çš„æ•°æ®åˆ†å¸ƒå’Œè®¡ç®—èƒ½åŠ›åŠ¨æ€è°ƒæ•´å¯è®­ç»ƒå—çš„æ•°é‡å’Œä½ç½®ã€‚

**å…³é”®åˆ›æ–°**ï¼šFedBEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªé€‚åº”å˜æ¢å™¨å—æ‰©å±•æœºåˆ¶å’ŒåŠ¨æ€åˆ†é…ç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿçš„é›†ä¸­å¼å¾®è°ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…æ— æ³•åº”å¯¹è”é‚¦ç¯å¢ƒçš„å¼‚æ„æ€§å’Œéšç§çº¦æŸã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒFedBEé‡‡ç”¨äº†åŠ¨æ€åˆ†é…ç­–ç•¥ï¼Œæ ¹æ®æ¯ä¸ªå®¢æˆ·ç«¯çš„å…·ä½“æ•°æ®ç‰¹å¾å’Œè®¡ç®—èƒ½åŠ›ï¼Œçµæ´»åˆ†é…å¯è®­ç»ƒå—ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒè®¾å¤‡ä¸Šçš„é«˜æ•ˆè®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFedBEåœ¨å¾®è°ƒåç›¸è¾ƒäºç°æœ‰çš„è”é‚¦å¾®è°ƒæ–¹æ³•ï¼Œå‡†ç¡®ç‡ä¿æŒæå‡12-74%ï¼Œå¹¶ä¸”æ¨¡å‹æ”¶æ•›é€Ÿåº¦åŠ å¿«äº†1.9-3.1å€ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œæ™ºèƒ½è®¾å¤‡ç­‰éœ€è¦ä¿æŠ¤æ•°æ®éšç§çš„åˆ†å¸ƒå¼å­¦ä¹ åœºæ™¯ã€‚FedBEèƒ½å¤Ÿåœ¨ä¿è¯æ•°æ®éšç§çš„å‰æä¸‹ï¼Œæé«˜æ¨¡å‹åœ¨å¼‚æ„ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated fine-tuning (FedFT) of large language models (LLMs) has emerged as a promising solution for adapting models to distributed data environments while ensuring data privacy.
>   Existing FedFT methods predominantly utilize parameter-efficient fine-tuning (PEFT) techniques to reduce communication and computation overhead.
>   However, they often fail to adequately address the catastrophic forgetting, a critical challenge arising from continual adaptation in distributed environments. The traditional centralized fine-tuning methods, which are not designed for the heterogeneous and privacy-constrained nature of federated environments, struggle to mitigate this issue effectively. Moreover, the challenge is further exacerbated by significant variation in data distributions and device capabilities across clients, which leads to intensified forgetting and degraded model generalization. To tackle these issues, we propose FedBE, a novel FedFT framework that integrates an adaptive transformer block expansion mechanism with a dynamic trainable-block allocation strategy. Specifically, FedBE expands trainable blocks within the model architecture, structurally separating newly learned task-specific knowledge from the original pre-trained representations. Additionally, FedBE dynamically assigns these trainable blocks to clients based on their data distributions and computational capabilities. This enables the framework to better accommodate heterogeneous federated environments and enhances the generalization ability of the model.Extensive experiments show that compared with existing federated fine-tuning methods, FedBE achieves 12-74% higher accuracy retention on general tasks after fine-tuning and a model convergence acceleration ratio of 1.9-3.1x without degrading the accuracy of downstream tasks.

