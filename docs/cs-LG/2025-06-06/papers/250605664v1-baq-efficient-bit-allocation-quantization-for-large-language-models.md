---
layout: default
title: BAQ: Efficient Bit Allocation Quantization for Large Language Models
---

# BAQ: Efficient Bit Allocation Quantization for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05664" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.05664v1</a>
  <a href="https://arxiv.org/pdf/2506.05664.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05664v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05664v1', 'BAQ: Efficient Bit Allocation Quantization for Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Chao Zhang, Li Wang, Samson Lasaulce, Merouane Debbah

**ÂàÜÁ±ª**: cs.LG, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-06

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/CSU-ModelCompression/BAQ)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫BAQ‰ª•‰ºòÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈáèÂåñ‰ΩçÂàÜÈÖçÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈáèÂåñÊäÄÊúØ` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Hessian‰ª£ÁêÜ` `Âá∏‰ºòÂåñ` `Ê®°ÂûãÂéãÁº©` `ÊïèÊÑüÊÄßÂ∫¶Èáè`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈáèÂåñÊñπÊ≥ïÂ§ö‰æùËµñ‰∫éÂùáÂåÄ‰ΩçÂÆΩÂàÜÈÖçÔºåÊú™ËÄÉËôëÊùÉÈáçÂØπÈáèÂåñÂô™Â£∞ÁöÑÊïèÊÑüÊÄßÔºåÂØºËá¥ÊÄßËÉΩÊçüÂ§±„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éHessian‰ª£ÁêÜÁöÑÊïèÊÑüÊÄßÂ∫¶ÈáèÁöÑÈáèÂåñ‰ΩçÂÆΩÂàÜÈÖçÊ°ÜÊû∂ÔºåÈÄöËøáÂá∏‰ºòÂåñÊñπÊ≥ïÂÆûÁé∞Á≤æÂ∫¶Ëá™ÈÄÇÂ∫î„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåBAQÂú®Â§ö‰∏™Â§ßËØ≠Ë®ÄÊ®°Âûã‰∏äÂùá‰ºò‰∫éGPTQÔºåÂõ∞ÊÉëÂ∫¶Èôç‰ΩéÂπÖÂ∫¶ÂèØËææ56ÂÄçÔºåÂ±ïÁé∞Âá∫ËâØÂ•ΩÁöÑÊÄßËÉΩ‰∏éÂ§çÊùÇÂ∫¶Âπ≥Ë°°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂêéËÆ≠ÁªÉÊ®°ÂûãÈáèÂåñÊòØ‰∏ÄÁßçÂπøÊ≥õÈááÁî®ÁöÑÊäÄÊúØÔºåÁî®‰∫éÂáèÂ∞ëÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÜÖÂ≠òÂíåËÆ°ÁÆóÊàêÊú¨„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∞Áé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÂùáÂåÄÊàñÂêØÂèëÂºèÁöÑ‰ΩçÂÆΩÂàÜÈÖçÔºåÊú™ËÉΩËÄÉËôëÊùÉÈáçÂØπÈáèÂåñÂô™Â£∞ÁöÑÈùûÂùáÂåÄÊïèÊÑüÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éHessian‰ª£ÁêÜÁöÑÊïèÊÑüÊÄßÂ∫¶ÈáèÁöÑÈáèÂåñ‰ΩçÂÆΩÂàÜÈÖçÊñ∞Ê°ÜÊû∂„ÄÇÈÄöËøáÂÖ≥ÈîÆÂÅáËÆæÔºåÂ±Ç/ÁªÑ‰ª∂Á∫ßÊçüÂ§±ÂáΩÊï∞ÂèØ‰ª•ÊòéÁ°ÆË°®Á§∫‰∏∫‰ΩçÂÆΩÁöÑÂáΩÊï∞Ôºå‰ªéËÄåÂ∞Ü‰ΩçÂàÜÈÖçÈóÆÈ¢òÂΩ¢ÂºèÂåñ‰∏∫‰∏Ä‰∏™Âá∏‰ºòÂåñ‰ªªÂä°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBAQÂú®Áõ∏Âêå‰ΩçÂÆΩ‰∏ãÂú®125MÂà∞30BÂèÇÊï∞ÁöÑÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏äÔºåË°®Áé∞Âá∫ÊØîGPTQ‰Ωé56ÂÄçÁöÑÂõ∞ÊÉëÂ∫¶„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÈáèÂåñËøáÁ®ã‰∏≠‰ΩçÂÆΩÂàÜÈÖç‰∏çÂùáÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®ÂùáÂåÄÊàñÂêØÂèëÂºèÁöÑ‰ΩçÂÆΩÂàÜÈÖçÔºåÊú™ËÉΩÂÖÖÂàÜËÄÉËôëÊùÉÈáçÂØπÈáèÂåñÂô™Â£∞ÁöÑ‰∏çÂêåÊïèÊÑüÊÄßÔºåÂØºËá¥ÈáèÂåñÊçüÂ§±ËæÉÂ§ß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éHessian‰ª£ÁêÜÁöÑÊïèÊÑüÊÄßÂ∫¶ÈáèÊñπÊ≥ïÔºåÈÄöËøáÂàÜÊûêÂ±ÇÁ∫ßÊçüÂ§±ÂáΩÊï∞‰∏é‰ΩçÂÆΩ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂ∞Ü‰ΩçÂàÜÈÖçÈóÆÈ¢òËΩ¨Âåñ‰∏∫Âá∏‰ºòÂåñ‰ªªÂä°Ôºå‰ªéËÄåÂÆûÁé∞Á≤æÂ∫¶ÁöÑËá™ÈÄÇÂ∫îÂàÜÈÖç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨ÊïèÊÑüÊÄßÂ∫¶ÈáèËÆ°ÁÆó„ÄÅÊçüÂ§±ÂáΩÊï∞ÊûÑÂª∫ÂíåÂá∏‰ºòÂåñÊ±ÇËß£‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàËÆ°ÁÆóÊØèÂ±ÇÁöÑÊïèÊÑüÊÄßÔºåÁÑ∂ÂêéÊûÑÂª∫ÊçüÂ§±ÂáΩÊï∞ÔºåÊúÄÂêéÈÄöËøá‰ºòÂåñÁÆóÊ≥ïÊ±ÇËß£ÊúÄ‰ºò‰ΩçÂÆΩÂàÜÈÖç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜÈáèÂåñ‰ΩçÂÆΩÂàÜÈÖçÈóÆÈ¢òÂΩ¢ÂºèÂåñ‰∏∫‰∏Ä‰∏™Âá∏‰ºòÂåñÈóÆÈ¢òÔºåÂà©Áî®Hessian‰ª£ÁêÜÊù•Á≤æÁ°ÆÂ∫¶ÈáèÊùÉÈáçÁöÑÊïèÊÑüÊÄß„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÂùáÂåÄÂàÜÈÖçÊñπÊ≥ïÊú¨Ë¥®‰∏ä‰∏çÂêåÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Èôç‰ΩéÈáèÂåñÊçüÂ§±„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏≠ÔºåËÄÉËôë‰∫ÜÂ±ÇÁ∫ßÊçüÂ§±‰∏é‰ΩçÂÆΩÁöÑÊòéÁ°ÆÂÖ≥Á≥ªÔºåÈááÁî®‰∫ÜÈó≠ÂºèËß£Êù•ÂÆûÁé∞‰ΩçÂÆΩÁöÑËá™ÈÄÇÂ∫îÂàÜÈÖç„ÄÇÊ≠§Â§ñÔºåÁÆóÊ≥ïÁöÑÂ§çÊùÇÂ∫¶ÊéßÂà∂ÂæóÂΩìÔºå‰ΩøÂæóBAQËÉΩÂ§üËΩªÊùæÈõÜÊàêÂà∞Ê†áÂáÜÈáèÂåñÊµÅÁ®ã‰∏≠„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBAQÂú®Áõ∏Âêå‰ΩçÂÆΩ‰∏ãÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑGPTQÊñπÊ≥ïÔºåÂõ∞ÊÉëÂ∫¶Èôç‰ΩéÂπÖÂ∫¶ÂèØËææ56ÂÄçÔºåÂ∞§ÂÖ∂Âú®ÂèÇÊï∞Èáè‰ªé125MÂà∞30BÁöÑÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏äË°®Áé∞Á™ÅÂá∫„ÄÇËøô‰∏ÄÁªìÊûúÈ™åËØÅ‰∫ÜBAQÂú®ÈáèÂåñÊÄßËÉΩ‰∏äÁöÑÊúâÊïàÊÄßÂíå‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®Âú∫ÊôØÂåÖÊã¨Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÉ®ÁΩ≤‰∏é‰ºòÂåñÔºåÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠„ÄÇÈÄöËøáÊúâÊïàÁöÑÈáèÂåñ‰ΩçÂÆΩÂàÜÈÖçÔºåBAQËÉΩÂ§üÊòæËëóÈôç‰ΩéÊ®°ÂûãÁöÑÂÜÖÂ≠òÂíåËÆ°ÁÆóÈúÄÊ±ÇÔºåÊèêÂçáÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØÁî®ÊÄßÂíåÊïàÁéá„ÄÇÊú™Êù•ÔºåÈöèÁùÄÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπøÊ≥õÂ∫îÁî®ÔºåËØ•ÊñπÊ≥ïÂèØËÉΩÂú®Â§öÁßçËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Post-training model quantization is a widely adopted technique for reducing the memory and computational costs of large language models (LLMs). However, most existing methods rely on uniform or heuristic bitwidth assignments, failing to account for the nonuniform sensitivity of weights to quantization noise. In this paper, we propose a novel framework for allocating quantization bitwidths based on sensitivity metrics derived from a Hessian proxy. We make key assumptions, which allow the layer/component-wise loss function to be expressed as an explicit function of the bitwidths. This enables a neat formulation of the bit allocation problem as a convex optimization task, whose closed-form solution adapts precision across weights to minimize the layer-wise quantization loss. Inspecting the solution provides several insights (such as the equal-loss structure), which are then exploited to design the proposed \textbf{BAQ} (Bit Allocation Quantization) algorithm. The proposed algorithm achieves a good trade-off between loss minimization and complexity and allows BAQ to be integrated into standard quantization pipelines with minimal overhead. Experimental results show that BAQ consistently outperforms GPTQ, achieving up to 56$\times$ lower perplexity at the same bitwidth on large language models ranging from 125M to 30B parameters. Leveraging our analytical results derived from solving the optimal bit allocation problem, we also provide a theoretical explanation for the observed gains. All codes of this paper are available at https://github.com/CSU-ModelCompression/BAQ.

