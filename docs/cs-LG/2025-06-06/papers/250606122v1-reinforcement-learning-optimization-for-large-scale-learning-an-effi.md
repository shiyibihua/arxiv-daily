---
layout: default
title: Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library
---

# Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06122" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06122v1</a>
  <a href="https://arxiv.org/pdf/2506.06122.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06122v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06122v1', 'Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, Zichen Liu, Haizhou Zhao, Dakai An, Lunxi Cao, Qiyang Cao, Wanxi Deng, Feilei Du, Yiliang Gu, Jiahe Li, Xiang Li, Mingjie Liu, Yijia Luo, Zihe Liu, Yadao Wang, Pei Wang, Tianyuan Wu, Yanan Wu, Yuheng Zhao, Shuaibing Zhao, Jin Yang, Siran Yang, Yingshui Tan, Huimin Yi, Yuchi Xu, Yujin Yuan, Xingyao Zhang, Lin Qu, Wenbo Su, Wei Wang, Jiamang Wang, Bo Zheng

**åˆ†ç±»**: cs.LG, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: 16 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºROLLåº“ä»¥è§£å†³å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è§„æ¨¡å­¦ä¹ ` `ä¼˜åŒ–ç®—æ³•` `æ¨¡å—åŒ–è®¾è®¡` `èµ„æºåˆ†é…` `è®­ç»ƒç®¡é“` `å®éªŒçµæ´»æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æˆæœ¬æ•ˆç›Šå’Œå®¹é”™æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚ã€‚
2. ROLLåº“é€šè¿‡å•æ§åˆ¶å™¨æ¶æ„å’Œå¹¶è¡Œå·¥ä½œè€…æŠ½è±¡ï¼Œç®€åŒ–äº†è®­ç»ƒç®¡é“çš„å¼€å‘ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡ã€‚
3. è¯¥åº“åœ¨å®éªŒä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºåˆ†é…å’Œæ ·æœ¬ç®¡ç†æ–¹é¢ï¼Œé€‚åº”æ€§å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†ROLLï¼Œä¸€ä¸ªé«˜æ•ˆã€å¯æ‰©å±•ä¸”ç”¨æˆ·å‹å¥½çš„åº“ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ã€‚ROLLä¸»è¦æœåŠ¡äºä¸‰ç±»ç”¨æˆ·ï¼šè¿½æ±‚æˆæœ¬æ•ˆç›Šå’Œå®¹é”™çš„å¤§è§„æ¨¡è®­ç»ƒçš„æŠ€æœ¯å…ˆé”‹ã€éœ€è¦çµæ´»æ§åˆ¶è®­ç»ƒæµç¨‹çš„å¼€å‘è€…ï¼Œä»¥åŠå¯»æ±‚æ•æ·å®éªŒçš„ç ”ç©¶äººå‘˜ã€‚è¯¥åº“åŸºäºå¤šä¸ªå…³é”®æ¨¡å—æ„å»ºï¼Œç®€åŒ–äº†è®­ç»ƒç®¡é“çš„å¼€å‘ï¼Œæ”¯æŒé«˜æ•ˆçš„å¹¶è¡Œè®­ç»ƒå’Œæ•°æ®ä¼ è¾“ï¼Œæä¾›ç»†ç²’åº¦çš„æ ·æœ¬ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œå¹¶æ”¯æŒå¿«é€Ÿçµæ´»çš„å®éªŒè®¾è®¡ã€‚æœ€åï¼ŒAutoDeviceMappingæ¨¡å—å…è®¸ç”¨æˆ·åœ¨ä¸åŒé˜¶æ®µçµæ´»åˆ†é…èµ„æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¼˜åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æˆæœ¬ã€å®¹é”™æ€§å’Œçµæ´»æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å¤šæ ·åŒ–çš„ç”¨æˆ·éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šROLLåº“é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œç»“åˆå•æ§åˆ¶å™¨æ¶æ„å’Œå¹¶è¡Œå·¥ä½œè€…ï¼Œç®€åŒ–äº†è®­ç»ƒæµç¨‹ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šROLLåº“çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªå…³é”®æ¨¡å—ï¼šå•æ§åˆ¶å™¨æ¶æ„ã€å¹¶è¡Œç­–ç•¥å’Œæ•°æ®ä¼ è¾“æ¨¡å—ã€ç»†ç²’åº¦çš„æ ·æœ¬ç”Ÿå‘½å‘¨æœŸç®¡ç†è°ƒåº¦å™¨ã€ç¯å¢ƒå·¥ä½œè€…å’Œå¥–åŠ±å·¥ä½œè€…ï¼Œä»¥åŠAutoDeviceMappingæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šROLLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–è®¾è®¡å’Œçµæ´»çš„èµ„æºåˆ†é…æœºåˆ¶ï¼Œä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®éœ€æ±‚å¿«é€Ÿè°ƒæ•´è®­ç»ƒæµç¨‹ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´é«˜çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒROLLåº“é‡‡ç”¨äº†é«˜æ•ˆçš„å¹¶è¡Œç­–ç•¥å’Œæ•°æ®ä¼ è¾“æœºåˆ¶ï¼Œç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹çš„é«˜æ•ˆæ€§ï¼ŒåŒæ—¶åœ¨æ ·æœ¬ç®¡ç†å’Œèµ„æºåˆ†é…ä¸Šæä¾›äº†çµæ´»çš„æ§åˆ¶é€‰é¡¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒROLLåº“å±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºåˆ†é…å’Œæ ·æœ¬ç®¡ç†æ–¹é¢ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè®­ç»ƒæ•ˆç‡æé«˜äº†30%ä»¥ä¸Šï¼Œä¸”åœ¨å®¹é”™æ€§å’Œçµæ´»æ€§ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ»¡è¶³äº†å¤šæ ·åŒ–ç”¨æˆ·çš„éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ROLLåº“çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ çš„ç ”ç©¶ä¸å¼€å‘ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦é«˜æ•ˆè®­ç»ƒå’Œå¿«é€Ÿå®éªŒçš„åœºæ™¯ã€‚å…¶çµæ´»çš„èµ„æºåˆ†é…å’Œè®­ç»ƒç®¡é“è®¾è®¡èƒ½å¤Ÿæ˜¾è‘—æå‡ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…çš„å·¥ä½œæ•ˆç‡ï¼Œæ¨åŠ¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce ROLL, an efficient, scalable, and user-friendly library designed for Reinforcement Learning Optimization for Large-scale Learning. ROLL caters to three primary user groups: tech pioneers aiming for cost-effective, fault-tolerant large-scale training, developers requiring flexible control over training workflows, and researchers seeking agile experimentation. ROLL is built upon several key modules to serve these user groups effectively. First, a single-controller architecture combined with an abstraction of the parallel worker simplifies the development of the training pipeline. Second, the parallel strategy and data transfer modules enable efficient and scalable training. Third, the rollout scheduler offers fine-grained management of each sample's lifecycle during the rollout stage. Fourth, the environment worker and reward worker support rapid and flexible experimentation with agentic RL algorithms and reward designs. Finally, AutoDeviceMapping allows users to assign resources to different models flexibly across various stages.

