---
layout: default
title: Flexible Operator Fusion for Fast Sparse Transformer with Diverse Masking on GPU
---

# Flexible Operator Fusion for Fast Sparse Transformer with Diverse Masking on GPU

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06095" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06095v2</a>
  <a href="https://arxiv.org/pdf/2506.06095.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06095v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06095v2', 'Flexible Operator Fusion for Fast Sparse Transformer with Diverse Masking on GPU')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhao Dai, Haodong Deng, Mengfei Rong, Xinyu Yang, Hongyu Liu, Fangxin Liu, Hailong Yang, Qianwen Cao, Qingxiao Sun

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-08-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTOFæ¡†æ¶ä»¥ä¼˜åŒ–ç¨€ç–Transformerçš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–Transformer` `æ“ä½œç¬¦èåˆ` `GPUåŠ é€Ÿ` `å¤šå¤´æ³¨æ„åŠ›` `æ€§èƒ½ä¼˜åŒ–` `çµæ´»æ©ç ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¨€ç–Transformerçš„æ€§èƒ½ä¼˜åŒ–ç ”ç©¶è¾ƒå°‘ï¼Œä¸”åŸºäºè§„åˆ™çš„æœºåˆ¶æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ··åˆç±»å‹æ“ä½œç¬¦çš„èåˆæœºä¼šã€‚
2. æœ¬æ–‡æå‡ºSTOFæ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€å­˜å‚¨æ ¼å¼å’Œå†…æ ¸å®ç°ï¼Œçµæ´»æ©ç å’Œæ“ä½œç¬¦èåˆæ¥ä¼˜åŒ–ç¨€ç–Transformerã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSTOFåœ¨å¤šå¤´æ³¨æ„åŠ›è®¡ç®—ä¸­å®ç°äº†1.7å€çš„åŠ é€Ÿï¼Œåœ¨ç«¯åˆ°ç«¯æ¨ç†ä¸­å®ç°äº†1.5å€çš„åŠ é€Ÿï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹å› å…¶å¼ºå¤§çš„ç†è§£èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ä½œä¸ºLLMçš„æ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡å¹¶è¡ŒåŒ–åŠ é€ŸTransformeré€æ¸æˆä¸ºçƒ­é—¨ç ”ç©¶è¯¾é¢˜ã€‚æ©ç å±‚å¼•å…¥ç¨€ç–æ€§ä»¥å‡å°‘è®¡ç®—é‡ï¼Œä½†ç°æœ‰ç ”ç©¶å¾ˆå°‘å…³æ³¨ç¨€ç–Transformerçš„æ€§èƒ½ä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒåŸºäºè§„åˆ™çš„æœºåˆ¶å¿½è§†äº†æ··åˆç±»å‹æ“ä½œç¬¦çš„èåˆæœºä¼šï¼Œä¸”æœªèƒ½é€‚åº”ä¸åŒçš„åºåˆ—é•¿åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†STOFæ¡†æ¶ï¼Œé€šè¿‡çµæ´»çš„æ©ç å’Œæ“ä½œç¬¦èåˆåœ¨GPUä¸Šä¼˜åŒ–ç¨€ç–Transformerã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„å·¥ä½œç›¸æ¯”ï¼ŒSTOFåœ¨å¤šå¤´æ³¨æ„åŠ›è®¡ç®—ä¸­å®ç°äº†æœ€é«˜1.7å€çš„åŠ é€Ÿï¼Œåœ¨ç«¯åˆ°ç«¯æ¨ç†ä¸­å®ç°äº†1.5å€çš„åŠ é€Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¨€ç–Transformeråœ¨æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ··åˆç±»å‹æ“ä½œç¬¦çš„èåˆæœºä¼šï¼Œä¸”å¯¹ä¸åŒåºåˆ—é•¿åº¦çš„é€‚åº”æ€§è¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSTOFæ¡†æ¶é€šè¿‡çµæ´»çš„æ©ç æœºåˆ¶å’Œæ“ä½œç¬¦èåˆæŠ€æœ¯ï¼Œä¼˜åŒ–ç¨€ç–Transformerçš„è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡ç»Ÿä¸€å­˜å‚¨æ ¼å¼å’Œå†…æ ¸å®ç°ï¼Œæå‡äº†å¤šå¤´æ³¨æ„åŠ›çš„è®¡ç®—æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSTOFçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆæ˜¯å¤šå¤´æ³¨æ„åŠ›çš„å­˜å‚¨æ ¼å¼å’Œå†…æ ¸å®ç°çš„ç»Ÿä¸€ï¼›å…¶æ¬¡æ˜¯å°†èåˆæ–¹æ¡ˆæ˜ å°„åˆ°ç¼–è¯‘æ¨¡æ¿ï¼›æœ€åï¼Œé€šè¿‡åŒé˜¶æ®µæœç´¢å¼•æ“ç¡®å®šæœ€ä½³å‚æ•°è®¾ç½®ã€‚

**å…³é”®åˆ›æ–°**ï¼šSTOFçš„ä¸»è¦åˆ›æ–°åœ¨äºçµæ´»çš„æ©ç å’Œæ“ä½œç¬¦èåˆç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿçš„è§„åˆ™åŸºç¡€æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„åºåˆ—é•¿åº¦å’Œè®¡ç®—éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒSTOFé‡‡ç”¨äº†åŒé˜¶æ®µæœç´¢å¼•æ“æ¥ä¼˜åŒ–å‚æ•°é…ç½®ï¼Œç¡®ä¿åœ¨ä¸åŒåœºæ™¯ä¸‹éƒ½èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

STOFæ¡†æ¶åœ¨å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œæœ€å¤§å®ç°äº†1.7å€çš„å¤šå¤´æ³¨æ„åŠ›è®¡ç®—åŠ é€Ÿå’Œ1.5å€çš„ç«¯åˆ°ç«¯æ¨ç†åŠ é€Ÿï¼Œå±•ç¤ºäº†å…¶åœ¨æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†é€Ÿåº¦å’Œè®¡ç®—æ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models are popular around the world due to their powerful understanding capabilities. As the core component of LLMs, accelerating Transformer through parallelization has gradually become a hot research topic. Mask layers introduce sparsity into Transformer to reduce calculations. However, previous works rarely focus on the performance optimization of sparse Transformer. Moreover, rule-based mechanisms ignore the fusion opportunities of mixed-type operators and fail to adapt to various sequence lengths. To address the above problems, we propose STOF, a framework that incorporates optimizations for Sparse Transformer via flexible masking and operator fusion on GPU. We firstly unify the storage format and kernel implementation for the multi-head attention. Then, we map fusion schemes to compilation templates and determine the optimal parameter setting through a two-stage search engine. The experimental results show that compared to the state-of-the-art work, STOF achieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end inference.

