---
layout: default
title: LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components
---

# LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17182" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17182v2</a>
  <a href="https://arxiv.org/pdf/2508.17182.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17182v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17182v2', 'LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hikaru Tsujimura, Arush Tagade

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24 (æ›´æ–°: 2025-08-31)

**å¤‡æ³¨**: This preprint is under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æƒ…æ„Ÿä¸é€»è¾‘æˆåˆ†åˆ†è§£LLMè‡ªä¿¡åº¦ä»¥åº”å¯¹è¿‡åº¦è‡ªä¿¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªä¿¡åº¦` `æœºåˆ¶å¯è§£é‡Šæ€§` `æƒ…æ„Ÿåˆ†æ` `é€»è¾‘æ¨ç†` `å¾®è°ƒæŠ€æœ¯` `å¤šæˆåˆ†ç»“æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é«˜é£é™©ç¯å¢ƒä¸­å¸¸è¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡ï¼Œå¯¼è‡´ä¿¡æ¯ä¼ é€’çš„ä¸å‡†ç¡®æ€§å’Œæ½œåœ¨é£é™©ã€‚
2. æœ¬æ–‡é€šè¿‡å¯¹Llama 3.2æ¨¡å‹çš„å¾®è°ƒå’Œæ¿€æ´»åˆ†æï¼Œæå‡ºå°†è‡ªä¿¡åº¦åˆ†è§£ä¸ºæƒ…æ„Ÿå’Œé€»è¾‘ä¸¤ä¸ªæˆåˆ†çš„æœºåˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæƒ…æ„Ÿå‘é‡å¯¹é¢„æµ‹å‡†ç¡®æ€§æœ‰å¹¿æ³›å½±å“ï¼Œè€Œé€»è¾‘å‘é‡åˆ™åœ¨ç‰¹å®šæƒ…å†µä¸‹äº§ç”Ÿå±€éƒ¨æ•ˆåº”ï¼Œæä¾›äº†æ–°çš„ç†è§£è§†è§’ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜é£é™©åœºæ™¯ä¸­å¸¸è¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡ï¼Œå‘ˆç°å‡ºä¸å¿…è¦çš„ç¡®å®šæ€§ã€‚æœ¬æ–‡é€šè¿‡æœºåˆ¶å¯è§£é‡Šæ€§ç ”ç©¶è¿™ä¸€è¡Œä¸ºçš„å†…åœ¨åŸºç¡€ã€‚åˆ©ç”¨å¼€æºçš„Llama 3.2æ¨¡å‹ï¼Œé’ˆå¯¹äººç±»æ ‡æ³¨çš„è‡ªä¿¡åº¦æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œæå–å„å±‚çš„æ®‹å·®æ¿€æ´»ï¼Œå¹¶è®¡ç®—ç›¸ä¼¼æ€§åº¦é‡ä»¥å®šä½è‡ªä¿¡è¡¨ç°ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œæœ€æ•æ„Ÿçš„å±‚ä¸è‡ªä¿¡åº¦å¯¹æ¯”ç›¸å…³ï¼Œå¹¶æ­ç¤ºé«˜è‡ªä¿¡è¡¨ç°å¯åˆ†è§£ä¸ºæƒ…æ„Ÿå’Œé€»è¾‘ä¸¤ä¸ªæ­£äº¤å­æˆåˆ†ï¼Œç±»ä¼¼å¿ƒç†å­¦ä¸­çš„åŒé€”å¾„ç²¾ç»†åŒ–å¯èƒ½æ€§æ¨¡å‹ã€‚ç”±è¿™äº›å­æˆåˆ†å¯¼å‡ºçš„å¼•å¯¼å‘é‡æ˜¾ç¤ºå‡ºä¸åŒçš„å› æœæ•ˆåº”ï¼šæƒ…æ„Ÿå‘é‡å¹¿æ³›å½±å“é¢„æµ‹å‡†ç¡®æ€§ï¼Œè€Œé€»è¾‘å‘é‡åˆ™äº§ç”Ÿæ›´å±€éƒ¨çš„å½±å“ã€‚è¿™äº›å‘ç°ä¸ºLLMè‡ªä¿¡åº¦çš„å¤šæˆåˆ†ç»“æ„æä¾›äº†æœºåˆ¶è¯æ®ï¼Œå¹¶æŒ‡å‡ºäº†å‡è½»è¿‡åº¦è‡ªä¿¡è¡Œä¸ºçš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é«˜é£é™©åœºæ™¯ä¸­è¡¨ç°å‡ºçš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè§£é‡Šè‡ªä¿¡åº¦çš„å†…åœ¨æœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹è¾“å‡ºçš„ä¸å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æœºåˆ¶å¯è§£é‡Šæ€§åˆ†æï¼Œè®ºæ–‡æå‡ºå°†è‡ªä¿¡åº¦åˆ†è§£ä¸ºæƒ…æ„Ÿå’Œé€»è¾‘ä¸¤ä¸ªæ­£äº¤å­æˆåˆ†ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å’Œæ§åˆ¶æ¨¡å‹çš„è‡ªä¿¡è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä½¿ç”¨å¼€æºçš„Llama 3.2æ¨¡å‹ï¼Œé¦–å…ˆåœ¨æ ‡æ³¨çš„è‡ªä¿¡åº¦æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œç„¶åæå–å„å±‚çš„æ®‹å·®æ¿€æ´»ï¼Œè®¡ç®—ç›¸ä¼¼æ€§åº¦é‡ä»¥å®šä½è‡ªä¿¡è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºè¯†åˆ«å‡ºè‡ªä¿¡è¡¨ç°çš„å¤šæˆåˆ†ç»“æ„ï¼Œå°¤å…¶æ˜¯æƒ…æ„Ÿå’Œé€»è¾‘çš„æ­£äº¤åˆ†è§£ï¼Œè¿™ä¸ç°æœ‰çš„å•ä¸€ç»´åº¦ç†è§£æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è‡ªä¿¡åº¦è¡¨ç°ï¼Œå¹¶é€šè¿‡æ®‹å·®æ¿€æ´»åˆ†æç¡®å®šäº†æœ€æ•æ„Ÿçš„å±‚çº§ï¼Œç¡®ä¿äº†ç»“æœçš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæƒ…æ„Ÿå‘é‡å¯¹æ¨¡å‹é¢„æµ‹å‡†ç¡®æ€§æœ‰æ˜¾è‘—å½±å“ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œè€Œé€»è¾‘å‘é‡åˆ™åœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°å‡ºå±€éƒ¨æ•ˆåº”ã€‚è¿™äº›å‘ç°ä¸ºä¼˜åŒ–LLMçš„è‡ªä¿¡åº¦æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€åŒ»ç–—å’¨è¯¢å’Œé‡‘èå†³ç­–ç­‰é«˜é£é™©åœºæ™¯ã€‚é€šè¿‡ç†è§£å’Œè°ƒèŠ‚æ¨¡å‹çš„è‡ªä¿¡åº¦ï¼Œå¯ä»¥æé«˜ä¿¡æ¯ä¼ é€’çš„å‡†ç¡®æ€§ï¼Œé™ä½å†³ç­–é£é™©ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often display overconfidence, presenting information with unwarranted certainty in high-stakes contexts. We investigate the internal basis of this behavior via mechanistic interpretability. Using open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness datasets, we extract residual activations across all layers, and compute similarity metrics to localize assertive representations. Our analysis identifies layers most sensitive to assertiveness contrasts and reveals that high-assertive representations decompose into two orthogonal sub-components of emotional and logical clusters-paralleling the dual-route Elaboration Likelihood Model in Psychology. Steering vectors derived from these sub-components show distinct causal effects: emotional vectors broadly influence prediction accuracy, while logical vectors exert more localized effects. These findings provide mechanistic evidence for the multi-component structure of LLM assertiveness and highlight avenues for mitigating overconfident behavior.

