---
layout: default
title: TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling
---

# TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17445" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.17445v1</a>
  <a href="https://arxiv.org/pdf/2508.17445.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17445v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17445v1', 'TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang, Ge Zhang, Wenhao Huang

**ÂàÜÁ±ª**: cs.LG, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-24

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TreePO‰ª•Ëß£ÂÜ≥Âº∫ÂåñÂ≠¶‰π†Êé®ÁêÜÊïàÁéá‰∏éÊïàÊûú‰πãÈó¥ÁöÑÁüõÁõæ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Êé®ÁêÜÊïàÁéá` `Ê†ëÁªìÊûÑÊ®°Âûã` `Âä®ÊÄÅÈááÊ†∑` `ÊÆµÁ∫ß‰ºòÂäø‰º∞ËÆ°` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `ËÆ°ÁÆóËµÑÊ∫ê‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Âº∫ÂåñÂ≠¶‰π†Êé®ÁêÜ‰∏≠Èù¢‰∏¥ÊòÇË¥µÁöÑÂú®Á∫øÂõûÊîæÂíåÊúâÈôêÁöÑÂ§öÊ†∑ÊÄßÊé¢Á¥¢ÈóÆÈ¢ò„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫TreePOÔºåÈÄöËøáÊ†ëÁªìÊûÑÊêúÁ¥¢ÂíåÂä®ÊÄÅÈááÊ†∑Á≠ñÁï•Êù•‰ºòÂåñÊé®ÁêÜÊïàÁéá‰∏éÊïàÊûú„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTreePOÂú®Êé®ÁêÜÂü∫ÂáÜ‰∏äÊòæËëóÊèêÂçáÊÄßËÉΩÔºåÂπ∂ÊúâÊïàÂáèÂ∞ëËÆ°ÁÆóËµÑÊ∫êÊ∂àËÄó„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂØπÈΩêÁöÑËøõÂ±ïÂú®Ëß£ÂÜ≥Â§çÊùÇÊé®ÁêÜÈóÆÈ¢ò‰∏äÂèñÂæó‰∫ÜÊòæËëóÊàêÊïàÔºå‰ΩÜ‰ª£‰ª∑ÊòØÊòÇË¥µÁöÑÂú®Á∫øÁ≠ñÁï•ÂõûÊîæÂíåÊúâÈôêÁöÑÂ§öÊ†∑ÊÄßÊé®ÁêÜË∑ØÂæÑÊé¢Á¥¢„ÄÇÊú¨ÊñáÊèêÂá∫TreePOÔºåÈááÁî®Ëá™ÂØºÂêëÂõûÊîæÁÆóÊ≥ïÂ∞ÜÂ∫èÂàóÁîüÊàêËßÜ‰∏∫Ê†ëÁªìÊûÑÊêúÁ¥¢ËøáÁ®ã„ÄÇTreePOÈÄöËøáÂä®ÊÄÅÊ†ëÈááÊ†∑Á≠ñÁï•ÂíåÂõ∫ÂÆöÈïøÂ∫¶ÊÆµËß£Á†ÅÔºåÂà©Áî®Â±ÄÈÉ®‰∏çÁ°ÆÂÆöÊÄßÊù•ÁîüÊàêÈ¢ùÂ§ñÂàÜÊîØ„ÄÇÈÄöËøáÂú®ÂÖ¨ÂÖ±ÂâçÁºÄ‰∏äÊëäÈîÄËÆ°ÁÆóÂπ∂ÊèêÂâç‰øÆÂâ™‰Ωé‰ª∑ÂÄºË∑ØÂæÑÔºåTreePOÊúâÊïàÈôç‰Ωé‰∫ÜÊØèÊ¨°Êõ¥Êñ∞ÁöÑËÆ°ÁÆóË¥üÊãÖÔºåÂêåÊó∂‰øùÊåÅÊàñÂ¢ûÂº∫‰∫ÜÊé¢Á¥¢ÁöÑÂ§öÊ†∑ÊÄß„ÄÇÂÖ≥ÈîÆË¥°ÁåÆÂåÖÊã¨ÔºöÊÆµÁ∫ßÈááÊ†∑ÁÆóÊ≥ï„ÄÅÊ†ëÁªìÊûÑÊÆµÁ∫ß‰ºòÂäø‰º∞ËÆ°‰ª•ÂèäÂä®ÊÄÅÂèëÊï£ÂíåÂõûÈÄÄÁ≠ñÁï•ÁöÑÊúâÊïàÊÄßÂàÜÊûê„ÄÇÊàë‰ª¨Âú®‰∏ÄÁ≥ªÂàóÊé®ÁêÜÂü∫ÂáÜ‰∏äÈ™åËØÅ‰∫ÜTreePOÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ∂ÊòæÁ§∫Âá∫GPUÂ∞èÊó∂ËäÇÁúÅ22%Ëá≥43%ÁöÑÊïàÁéáÔºåÂêåÊó∂Âú®ËΩ®ËøπÁ∫ßÂíå‰ª§ÁâåÁ∫ßÈááÊ†∑ËÆ°ÁÆó‰∏äÂàÜÂà´ÂáèÂ∞ë‰∫Ü40%Âíå35%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®Êé®ÁêÜÊïàÁéáÂíåÊïàÊûú‰πãÈó¥ÁöÑÁüõÁõæÔºåÂ∞§ÂÖ∂ÊòØÊòÇË¥µÁöÑÂú®Á∫øÁ≠ñÁï•ÂõûÊîæÂíåÊúâÈôêÁöÑÊé®ÁêÜË∑ØÂæÑÊé¢Á¥¢Â∏¶Êù•ÁöÑÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTreePOÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÂ∫èÂàóÁîüÊàêËßÜ‰∏∫Ê†ëÁªìÊûÑÁöÑÊêúÁ¥¢ËøáÁ®ãÔºåÈÄöËøáÂä®ÊÄÅÊ†ëÈááÊ†∑ÂíåÂõ∫ÂÆöÈïøÂ∫¶ÊÆµËß£Á†ÅÊù•‰ºòÂåñËÆ°ÁÆóÊïàÁéáÔºåÂêåÊó∂Â¢ûÂº∫Êé¢Á¥¢ÁöÑÂ§öÊ†∑ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTreePOÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Âä®ÊÄÅÊ†ëÈááÊ†∑Á≠ñÁï•„ÄÅÂõ∫ÂÆöÈïøÂ∫¶ÊÆµËß£Á†ÅÂíåÂ±ÄÈÉ®‰∏çÁ°ÆÂÆöÊÄßÂºïÂØºÁöÑÂàÜÊîØÁîüÊàê„ÄÇÈÄöËøáÂú®ÂÖ¨ÂÖ±ÂâçÁºÄ‰∏äÊëäÈîÄËÆ°ÁÆóÔºåÊèêÂâç‰øÆÂâ™‰Ωé‰ª∑ÂÄºË∑ØÂæÑÔºåÈôç‰ΩéËÆ°ÁÆóË¥üÊãÖ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTreePOÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÊÆµÁ∫ßÈááÊ†∑ÁÆóÊ≥ïÂíåÊ†ëÁªìÊûÑÊÆµÁ∫ß‰ºòÂäø‰º∞ËÆ°ÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÁ∫øÊÄßÈááÊ†∑ÂíåÂÖ®Â±Ä‰ºòÂåñÁ≠ñÁï•ÂΩ¢Êàê‰∫ÜÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåTreePOÈááÁî®‰∫ÜÊÆµÁ∫ßÈááÊ†∑Êú∫Âà∂‰ª•ÂáèËΩªKVÁºìÂ≠òË¥üÊãÖÔºåÂπ∂ÂºïÂÖ•‰∫ÜÊó©ÂÅúÊú∫Âà∂ÂíåÂä®ÊÄÅÂèëÊï£Á≠ñÁï•Ôºå‰ª•ÊèêÈ´òÊé®ÁêÜÊïàÁéáÂíåÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåTreePOÂú®Êé®ÁêÜÂü∫ÂáÜ‰∏äÂÆûÁé∞‰∫Ü22%Ëá≥43%ÁöÑGPUÂ∞èÊó∂ËäÇÁúÅÔºåÂêåÊó∂Âú®ËΩ®ËøπÁ∫ßÂíå‰ª§ÁâåÁ∫ßÈááÊ†∑ËÆ°ÁÆó‰∏äÂàÜÂà´ÂáèÂ∞ë‰∫Ü40%Âíå35%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéTreePOÂú®ÊèêÂçáÊé®ÁêÜÊïàÁéáÁöÑÂêåÊó∂Ôºå‰øùÊåÅ‰∫ÜÊàñÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÊûúÔºåÂ±ïÁé∞‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

TreePOÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÂØπËØùÁ≥ªÁªüÂíåÊô∫ËÉΩÈóÆÁ≠îÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òÊé®ÁêÜÊïàÁéáÂíåÊïàÊûúÔºåTreePOËÉΩÂ§üÊîØÊåÅÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°Â§ÑÁêÜÔºåÈôç‰ΩéËÆ°ÁÆóËµÑÊ∫êÁöÑÈúÄÊ±ÇÔºåÊé®Âä®Âº∫ÂåñÂ≠¶‰π†Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑËêΩÂú∞„ÄÇÊú™Êù•ÔºåTreePOÂèØËÉΩ‰ºöÂú®Â§ßËßÑÊ®°Ê®°ÂûãËÆ≠ÁªÉÂíåÊé®ÁêÜ‰∏≠ÂèëÊå•Êõ¥Â§ß‰ΩúÁî®Ôºå‰øÉËøõÊô∫ËÉΩÁ≥ªÁªüÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\% up to 43\% of the sampling design for the trained models, meanwhile showing up to 40\% reduction at trajectory-level and 35\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO.

