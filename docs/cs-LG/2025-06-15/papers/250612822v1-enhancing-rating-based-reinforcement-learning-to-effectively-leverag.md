---
layout: default
title: Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models
---

# Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12822" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12822v1</a>
  <a href="https://arxiv.org/pdf/2506.12822.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12822v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12822v1', 'Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tung Minh Luu, Younghwan Lee, Donghoon Lee, Sunho Kim, Min Jun Kim, Chang D. Yoo

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-15

**å¤‡æ³¨**: Accepted to ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºERL-VLMä»¥æœ‰æ•ˆåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åé¦ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¥–åŠ±å‡½æ•°` `äººç±»åé¦ˆ` `æ ·æœ¬æ•ˆç‡` `æ•°æ®ä¸å¹³è¡¡` `è‡ªåŠ¨åŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±å‡½æ•°è®¾è®¡æ–¹æ³•ä¾èµ–äºå¤§é‡äººç±»åé¦ˆï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ã€‚
2. ERL-VLMé€šè¿‡æŸ¥è¯¢å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è·å–å•ä¸ªè½¨è¿¹çš„ç»å¯¹è¯„åˆ†ï¼Œæå‡äº†åé¦ˆçš„è¡¨è¾¾èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒERL-VLMåœ¨å¤šç§æ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¥–åŠ±ç”Ÿæˆæ–¹æ³•ï¼Œå±•ç¤ºäº†AIåé¦ˆçš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™é€šå¸¸éœ€è¦å¤§é‡çš„äººåŠ›å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚å°½ç®¡åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ åœ¨å¯¹é½ä»£ç†ä¸äººç±»æ„å›¾æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†è·å–é«˜è´¨é‡åé¦ˆçš„æˆæœ¬é«˜ä¸”åŠ³åŠ¨å¯†é›†ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚æœ€è¿‘åŸºç¡€æ¨¡å‹çš„è¿›å±•æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆâ€”â€”åˆ©ç”¨AIç”Ÿæˆçš„åé¦ˆæ¥å‡å°‘å¯¹äººç±»ç›‘ç£çš„ä¾èµ–ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ERL-VLMï¼Œä¸€ç§å¢å¼ºçš„åŸºäºè¯„åˆ†çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»AIåé¦ˆä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°ã€‚ä¸ä¾èµ–æˆå¯¹æ¯”è¾ƒçš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒERL-VLMæŸ¥è¯¢å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å•ä¸ªè½¨è¿¹çš„ç»å¯¹è¯„åˆ†ï¼Œä»è€Œå®ç°æ›´å…·è¡¨ç°åŠ›çš„åé¦ˆå’Œæ”¹å–„çš„æ ·æœ¬æ•ˆç‡ã€‚é€šè¿‡åœ¨ä½çº§å’Œé«˜çº§æ§åˆ¶ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ERL-VLMæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºVLMçš„å¥–åŠ±ç”Ÿæˆæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºäººç±»åé¦ˆï¼Œå¯¼è‡´æˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šERL-VLMé€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æä¾›çš„ç»å¯¹è¯„åˆ†ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„æˆå¯¹æ¯”è¾ƒæ–¹æ³•ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„å¥–åŠ±å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šERL-VLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹æŸ¥è¯¢å’Œå¥–åŠ±å‡½æ•°å­¦ä¹ ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ”¶é›†ä»£ç†çš„è½¨è¿¹æ•°æ®ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¯¹è¿™äº›è½¨è¿¹è¿›è¡Œè¯„åˆ†ï¼›æœ€åï¼ŒåŸºäºè¯„åˆ†ç»“æœæ›´æ–°å¥–åŠ±å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šERL-VLMçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä½¿ç”¨ç»å¯¹è¯„åˆ†è€Œéæˆå¯¹æ¯”è¾ƒï¼Œè¿™ä½¿å¾—åé¦ˆæ›´åŠ ä¸°å¯Œä¸”æ ·æœ¬æ•ˆç‡æ›´é«˜ï¼Œæ˜¾è‘—æ”¹å–„äº†æ•°æ®ä¸å¹³è¡¡å’Œå™ªå£°æ ‡ç­¾å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒERL-VLMé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¥–åŠ±å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡è°ƒèŠ‚è¶…å‚æ•°æ¥å¹³è¡¡ä¸åŒä»»åŠ¡çš„åé¦ˆè´¨é‡ä¸æ•°é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒERL-VLMåœ¨å¤šä¸ªä½çº§å’Œé«˜çº§æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰åŸºäºVLMçš„å¥–åŠ±ç”Ÿæˆæ–¹æ³•ï¼Œå…¶æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†AIåé¦ˆåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘äººç±»å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œå®ç°æ›´é«˜æ•ˆçš„å¥–åŠ±å­¦ä¹ ã€‚è¿™å°†æ¨åŠ¨å¼ºåŒ–å­¦ä¹ åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œæå‡ç³»ç»Ÿçš„è‡ªä¸»æ€§ä¸æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Designing effective reward functions remains a fundamental challenge in reinforcement learning (RL), as it often requires extensive human effort and domain expertise. While RL from human feedback has been successful in aligning agents with human intent, acquiring high-quality feedback is costly and labor-intensive, limiting its scalability. Recent advancements in foundation models present a promising alternative--leveraging AI-generated feedback to reduce reliance on human supervision in reward learning. Building on this paradigm, we introduce ERL-VLM, an enhanced rating-based RL method that effectively learns reward functions from AI feedback. Unlike prior methods that rely on pairwise comparisons, ERL-VLM queries large vision-language models (VLMs) for absolute ratings of individual trajectories, enabling more expressive feedback and improved sample efficiency. Additionally, we propose key enhancements to rating-based RL, addressing instability issues caused by data imbalance and noisy labels. Through extensive experiments across both low-level and high-level control tasks, we demonstrate that ERL-VLM significantly outperforms existing VLM-based reward generation methods. Our results demonstrate the potential of AI feedback for scaling RL with minimal human intervention, paving the way for more autonomous and efficient reward learning.

