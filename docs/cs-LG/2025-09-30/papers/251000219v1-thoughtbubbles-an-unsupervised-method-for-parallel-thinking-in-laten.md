---
layout: default
title: Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space
---

# Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00219" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00219v1</a>
  <a href="https://arxiv.org/pdf/2510.00219.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00219v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00219v1', 'Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Houjun Liu, Shikhar Murty, Christopher D. Manning, RÃ³bert CsordÃ¡s

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.NE

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 10 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºThoughtbubblesï¼Œä¸€ç§åœ¨éšç©ºé—´è¿›è¡Œå¹¶è¡Œè‡ªé€‚åº”è®¡ç®—çš„æ— ç›‘ç£Transformeræ–¹æ³•ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªé€‚åº”è®¡ç®—` `å¹¶è¡Œè®¡ç®—` `Transformer` `é¢„è®­ç»ƒ` `éšç©ºé—´` `æ®‹å·®æµ` `è¯­è¨€å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–æ˜¾å¼æ€ç»´é“¾ï¼Œæ— æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µåº”ç”¨ï¼Œä¸”ä»…é™äºä¸²è¡Œè‡ªç„¶è¯­è¨€ï¼Œé™åˆ¶äº†æ¨ç†è®¡ç®—çš„æ‰©å±•ã€‚
2. Thoughtbubblesé€šè¿‡å­¦ä¹ forkæˆ–åˆ é™¤æ®‹å·®æµï¼Œåœ¨éšç©ºé—´ä¸­å®ç°å¹¶è¡Œè‡ªé€‚åº”è®¡ç®—ï¼Œæ— éœ€æ˜¾å¼æ€ç»´é“¾ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒThoughtbubblesåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºæ ‡å‡†è§£ç å™¨LMå’Œéè‡ªé€‚åº”å¹¶è¡Œè®¡ç®—æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰æ‰©å±•Transformeræ¨ç†æœŸè®¡ç®—çš„æ–¹æ³•ä¾èµ–äºè®­ç»ƒæ¨¡å‹åœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰è¾“å‡ºæ˜¾å¼çš„æ€ç»´é“¾tokenã€‚è™½ç„¶è¿™äº›æ–¹æ³•å¾ˆå¼ºå¤§ï¼Œä½†å®ƒä»¬å—åˆ°é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬ä¸èƒ½åº”ç”¨äºé¢„è®­ç»ƒï¼Œå¹¶ä¸”ä»…é™äºä¸²è¡Œç”Ÿæˆçš„è‡ªç„¶è¯­è¨€verbalizationæ¥æ‰©å±•æ¨ç†æœŸè®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†Thoughtbubblesï¼Œä¸€ç§Transformerå˜ä½“ï¼Œå®ƒé€šè¿‡å­¦ä¹ forkæˆ–åˆ é™¤æ®‹å·®æµï¼Œåœ¨éšç©ºé—´ä¸­åŸç”Ÿæ‰§è¡Œå¹¶è¡Œè‡ªé€‚åº”è®¡ç®—ã€‚å› æ­¤ï¼Œéœ€è¦å¤§é‡è®¡ç®—çš„tokenå¯ä»¥åœ¨ç½‘ç»œä¸­é—´å½¢æˆå…‹éš†æ®‹å·®çš„â€œbubbleâ€ï¼Œä»¥è¿›è¡Œé¢å¤–çš„æ€è€ƒã€‚å…³é”®çš„æ˜¯ï¼Œè¿™ç§è¡Œä¸ºä»…é€šè¿‡è¯­è¨€å»ºæ¨¡æŸå¤±åœ¨é¢„è®­ç»ƒæœŸé—´å­¦ä¹ ã€‚åœ¨1.5äº¿åˆ°7.72äº¿å‚æ•°è§„æ¨¡çš„é¢„è®­ç»ƒåï¼ŒThoughtbubblesåœ¨OpenWebTextå’ŒpeS2oå›°æƒ‘åº¦ä»¥åŠHellaSwagå’ŒLAMBADAç­‰é›¶æ ·æœ¬è¯„ä¼°ä¸­ï¼Œä¼˜äºæ ‡å‡†è§£ç å™¨LMä»¥åŠéè‡ªé€‚åº”å¹¶è¡Œè®¡ç®—æ–¹æ³•ã€‚æˆ‘ä»¬æ–¹æ³•çš„éšå¼æ€§è´¨ä½¿å¾—è‡ªé€‚åº”è®¡ç®—èƒ½å¤Ÿä»é¢„è®­ç»ƒæ—¶å¼€å§‹å­¦ä¹ ï¼Œä»è€Œä¸ºç»Ÿä¸€æ¨ç†æ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•æ—¶è¡Œä¸ºé“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Transformeræ¨¡å‹åœ¨æ¨ç†æ—¶æ‰©å±•è®¡ç®—èƒ½åŠ›çš„æ–¹æ³•ï¼Œä¾‹å¦‚æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼Œä¾èµ–äºåœ¨è®­ç»ƒæ—¶æ˜¾å¼åœ°ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚è¿™ç§æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼š1ï¼‰æ— æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µåº”ç”¨ï¼Œé™åˆ¶äº†æ¨¡å‹çš„èƒ½åŠ›ï¼›2ï¼‰æ¨ç†è¿‡ç¨‹è¢«é™åˆ¶ä¸ºä¸²è¡Œçš„è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼Œæ•ˆç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šThoughtbubblesçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨Transformerçš„éšç©ºé—´ä¸­è¿›è¡Œå¹¶è¡Œè‡ªé€‚åº”è®¡ç®—ã€‚æ¨¡å‹å­¦ä¹ å¦‚ä½•æ ¹æ®è¾“å…¥tokençš„å¤æ‚ç¨‹åº¦ï¼ŒåŠ¨æ€åœ°â€œforkâ€ï¼ˆå¤åˆ¶ï¼‰æˆ–â€œdeleteâ€ï¼ˆåˆ é™¤ï¼‰æ®‹å·®æµã€‚å¯¹äºéœ€è¦æ›´å¤šè®¡ç®—çš„tokenï¼Œæ¨¡å‹ä¼šåˆ›å»ºä¸€ä¸ªâ€œbubbleâ€â€”â€”å³å¤šä¸ªå¹¶è¡Œçš„æ®‹å·®æµï¼Œä»è€Œå¢åŠ è®¡ç®—èµ„æºã€‚è¿™ç§æ–¹æ³•çš„å…³é”®åœ¨äºï¼Œå®ƒæ˜¯éšå¼çš„ï¼Œä¸éœ€è¦æ˜¾å¼çš„æ€ç»´é“¾ï¼Œå¹¶ä¸”å¯ä»¥åœ¨é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šThoughtbubblesåŸºäºæ ‡å‡†çš„Transformeræ¶æ„ï¼Œä½†å¼•å…¥äº†å¯å­¦ä¹ çš„é—¨æ§æœºåˆ¶æ¥æ§åˆ¶æ®‹å·®æµçš„forkå’Œdeleteã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨Transformerçš„æ¯ä¸€å±‚ï¼Œæ¨¡å‹ä¼šæ ¹æ®å½“å‰çš„æ®‹å·®æµè®¡ç®—å‡ºä¸€ä¸ªé—¨æ§å€¼ï¼Œè¯¥é—¨æ§å€¼å†³å®šæ˜¯å¦å¤åˆ¶æˆ–åˆ é™¤è¯¥æ®‹å·®æµã€‚å¦‚æœé—¨æ§å€¼è¾ƒé«˜ï¼Œåˆ™å¤åˆ¶æ®‹å·®æµï¼Œå½¢æˆä¸€ä¸ªâ€œbubbleâ€ï¼›å¦‚æœé—¨æ§å€¼è¾ƒä½ï¼Œåˆ™åˆ é™¤æ®‹å·®æµï¼Œå‡å°‘è®¡ç®—é‡ã€‚æœ€ç»ˆï¼Œæ‰€æœ‰å¹¶è¡Œçš„æ®‹å·®æµä¼šé€šè¿‡ä¸€ä¸ªèšåˆå±‚åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„æ®‹å·®æµï¼Œç”¨äºåç»­çš„è®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šThoughtbubblesæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå…¶éšå¼çš„å¹¶è¡Œè‡ªé€‚åº”è®¡ç®—æœºåˆ¶ã€‚ä¸ä¾èµ–æ˜¾å¼æ€ç»´é“¾çš„æ–¹æ³•ä¸åŒï¼ŒThoughtbubblesä¸éœ€è¦é¢„å…ˆå®šä¹‰æ¨ç†æ­¥éª¤ï¼Œè€Œæ˜¯é€šè¿‡å­¦ä¹ è‡ªåŠ¨åœ°åˆ†é…è®¡ç®—èµ„æºã€‚è¿™ç§æ–¹æ³•æ›´åŠ çµæ´»ï¼Œå¹¶ä¸”å¯ä»¥åœ¨é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šThoughtbubblesçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰å¯å­¦ä¹ çš„é—¨æ§æœºåˆ¶ï¼Œç”¨äºæ§åˆ¶æ®‹å·®æµçš„forkå’Œdeleteï¼›2ï¼‰æŸå¤±å‡½æ•°ï¼Œé™¤äº†æ ‡å‡†çš„è¯­è¨€å»ºæ¨¡æŸå¤±å¤–ï¼Œè¿˜å¯ä»¥åŠ å…¥æ­£åˆ™åŒ–é¡¹ï¼Œä»¥é¼“åŠ±æ¨¡å‹å­¦ä¹ ç¨€ç–çš„æ®‹å·®æµç»“æ„ï¼›3ï¼‰æ®‹å·®æµçš„èšåˆæ–¹å¼ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨åŠ æƒå¹³å‡æˆ–æ³¨æ„åŠ›æœºåˆ¶æ¥åˆå¹¶å¹¶è¡Œçš„æ®‹å·®æµã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Thoughtbubblesåœ¨OpenWebTextå’ŒpeS2oæ•°æ®é›†ä¸Šå®ç°äº†æ›´ä½çš„å›°æƒ‘åº¦ï¼Œè¡¨æ˜å…¶è¯­è¨€å»ºæ¨¡èƒ½åŠ›æœ‰æ‰€æå‡ã€‚åœ¨HellaSwagå’ŒLAMBADAç­‰é›¶æ ·æœ¬ä»»åŠ¡ä¸­ï¼ŒThoughtbubblesä¹Ÿä¼˜äºæ ‡å‡†è§£ç å™¨LMå’Œéè‡ªé€‚åº”å¹¶è¡Œè®¡ç®—æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒThoughtbubblesèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ è‡ªé€‚åº”è®¡ç®—ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Thoughtbubbleså…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ï¼Œä¾‹å¦‚é—®ç­”ã€æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚å…¶éšå¼å¹¶è¡Œè®¡ç®—çš„ç‰¹æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ç¡¬ä»¶èµ„æºï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç”±äºThoughtbubbleså¯ä»¥åœ¨é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ ï¼Œå› æ­¤å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æœªæ¥ï¼ŒThoughtbubblesæœ‰æœ›æˆä¸ºæ„å»ºæ›´å¼ºå¤§ã€æ›´é«˜æ•ˆçš„æ¨ç†æ¨¡å‹çš„é‡è¦æŠ€æœ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current approaches for scaling inference-time compute in transformers rely on training them to emit explicit chain-of-thought tokens before producing an answer. While these methods are powerful, they are limited because they cannot be applied during pretraining and are limited to only serially-generated, natural-language verbalization to scale inference-time compute. In this work, we propose Thoughtbubbles, a transformer variant that natively performs parallel adaptive computation in latent space by learning to fork or delete residual streams. Thus, tokens that require a large amount of computation can form a "bubble" of cloned residuals in the middle of the network for additional thinking. Crucially, this behavior is learned during pretraining with only language modeling loss. Thoughtbubbles outperforms both standard decoder LMs as well as non-adaptive parallel computation approaches on OpenWebText and peS2o perplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after pretraining across 150M to 772M parameter scales. The implicit nature of our method enables adaptive computation to be learned starting at pretraining time, paving the way to unify train and test-time behavior for reasoning models.

