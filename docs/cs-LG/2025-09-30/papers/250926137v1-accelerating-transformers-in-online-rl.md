---
layout: default
title: Accelerating Transformers in Online RL
---

# Accelerating Transformers in Online RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26137" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26137v1</a>
  <a href="https://arxiv.org/pdf/2509.26137.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26137v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26137v1', 'Accelerating Transformers in Online RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ é€Ÿå™¨ç­–ç•¥è®­ç»ƒTransformerï¼Œè§£å†³åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­Transformerè®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Transformer` `å¼ºåŒ–å­¦ä¹ ` `åœ¨çº¿å­¦ä¹ ` `è¡Œä¸ºå…‹éš†` `åŠ é€Ÿå™¨ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformeråœ¨å¼ºåŒ–å­¦ä¹ ä¸­é¢ä¸´è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç°æœ‰ç®—æ³•éš¾ä»¥ç›´æ¥åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºä½¿ç”¨â€œåŠ é€Ÿå™¨â€ç­–ç•¥ï¼Œå…ˆé€šè¿‡è¡Œä¸ºå…‹éš†é¢„è®­ç»ƒTransformerï¼Œå†è¿›è¡Œåœ¨çº¿äº¤äº’ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ç¨³å®šè®­ç»ƒTransformerï¼Œå‡å°‘å›¾åƒç¯å¢ƒè®­ç»ƒæ—¶é—´ï¼Œå¹¶æ˜¾è‘—é™ä½é‡æ”¾ç¼“å†²åŒºå¤§å°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­åŠ é€ŸTransformerè®­ç»ƒçš„æ–¹æ³•ã€‚ç”±äºTransformeræ¨¡å‹çš„ä¸ç¨³å®šæ€§ï¼Œç°æœ‰çš„å­¦ä¹ ç®—æ³•éš¾ä»¥ç›´æ¥åº”ç”¨äºåŸºäºTransformerçš„æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä½¿ç”¨ä¸€ä¸ªæ›´ç®€å•ã€æ›´ç¨³å®šçš„â€œåŠ é€Ÿå™¨â€ç­–ç•¥ä½œä¸ºTransformerçš„è®­ç»ƒå™¨ã€‚åœ¨ç®—æ³•çš„ç¬¬ä¸€é˜¶æ®µï¼ŒåŠ é€Ÿå™¨ç‹¬ç«‹åœ°ä¸ç¯å¢ƒäº¤äº’ï¼ŒåŒæ—¶é€šè¿‡è¡Œä¸ºå…‹éš†è®­ç»ƒTransformerã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œé¢„è®­ç»ƒçš„Transformerå¼€å§‹åœ¨å®Œå…¨åœ¨çº¿çš„ç¯å¢ƒä¸­äº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•ä¸ä»…èƒ½å¤Ÿç¨³å®šåœ°è®­ç»ƒTransformerï¼Œè€Œä¸”åœ¨åŸºäºå›¾åƒçš„ç¯å¢ƒä¸­ï¼Œè®­ç»ƒæ—¶é—´æœ€å¤šå¯å‡å°‘ä¸€åŠã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å°†ç¦»ç­–ç•¥æ–¹æ³•æ‰€éœ€çš„é‡æ”¾ç¼“å†²åŒºå¤§å°å‡å°‘åˆ°10-20åƒï¼Œä»è€Œæ˜¾è‘—é™ä½äº†æ•´ä½“è®¡ç®—éœ€æ±‚ã€‚å®éªŒåœ¨åŸºäºçŠ¶æ€å’Œå›¾åƒçš„ManiSkillç¯å¢ƒä»¥åŠMDPå’ŒPOMDPè®¾ç½®ä¸‹çš„MuJoCoä»»åŠ¡ä¸Šè¿›è¡Œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºTransformerçš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå­˜åœ¨è®­ç»ƒä¸ç¨³å®šã€æ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚ç›´æ¥å°†Transformeråº”ç”¨äºåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå®¹æ˜“å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±ï¼Œå¯¼è‡´ç­–ç•¥å´©æºƒã€‚æ­¤å¤–ï¼ŒTransformeré€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¯¹è®¡ç®—èµ„æºè¦æ±‚é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªæ›´ç®€å•ã€æ›´ç¨³å®šçš„ç­–ç•¥ï¼ˆå³â€œåŠ é€Ÿå™¨â€ï¼‰æ¥è¾…åŠ©è®­ç»ƒTransformerã€‚åŠ é€Ÿå™¨å…ˆä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†æ•°æ®ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ•°æ®é€šè¿‡è¡Œä¸ºå…‹éš†çš„æ–¹å¼é¢„è®­ç»ƒTransformerã€‚è¿™æ ·å¯ä»¥é¿å…Transformerç›´æ¥ä¸ç¯å¢ƒäº¤äº’å¸¦æ¥çš„ä¸ç¨³å®šæ€§ï¼Œå¹¶æä¾›ä¸€ä¸ªè¾ƒå¥½çš„åˆå§‹åŒ–çŠ¶æ€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç®—æ³•åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š
1. **åŠ é€Ÿå™¨è®­ç»ƒé˜¶æ®µ**ï¼šåŠ é€Ÿå™¨ç­–ç•¥ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œå¦‚MLPï¼‰ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ç»éªŒæ•°æ®ã€‚åŒæ—¶ï¼Œä½¿ç”¨æ”¶é›†åˆ°çš„æ•°æ®ï¼Œé€šè¿‡è¡Œä¸ºå…‹éš†çš„æ–¹å¼è®­ç»ƒTransformerç­–ç•¥ï¼Œä½¿å…¶æ¨¡ä»¿åŠ é€Ÿå™¨çš„è¡Œä¸ºã€‚
2. **åœ¨çº¿å¾®è°ƒé˜¶æ®µ**ï¼šé¢„è®­ç»ƒçš„Transformerç­–ç•¥å¼€å§‹ä¸ç¯å¢ƒäº¤äº’ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚SACã€PPOç­‰ï¼‰è¿›è¡Œåœ¨çº¿å¾®è°ƒã€‚æ­¤æ—¶ï¼ŒTransformerå·²ç»å…·å¤‡ä¸€å®šçš„ç­–ç•¥èƒ½åŠ›ï¼Œå¯ä»¥æ›´ç¨³å®šåœ°è¿›è¡Œåœ¨çº¿å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†â€œåŠ é€Ÿå™¨â€ç­–ç•¥ï¼Œå°†Transformerçš„è®­ç»ƒè¿‡ç¨‹åˆ†è§£ä¸ºé¢„è®­ç»ƒå’Œåœ¨çº¿å¾®è°ƒä¸¤ä¸ªé˜¶æ®µã€‚è¿™ç§æ–¹å¼æœ‰æ•ˆåœ°è§£å†³äº†Transformeråœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œå¹¶åŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ã€‚ä¸ç›´æ¥åœ¨çº¿è®­ç»ƒTransformerç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›åˆ°è¾ƒå¥½çš„ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼š
*   **åŠ é€Ÿå™¨ç­–ç•¥çš„é€‰æ‹©**ï¼šåŠ é€Ÿå™¨ç­–ç•¥é€šå¸¸é€‰æ‹©ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œå¦‚å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œä»¥ä¿è¯å…¶è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚
*   **è¡Œä¸ºå…‹éš†æŸå¤±å‡½æ•°**ï¼šä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æˆ–äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è¡¡é‡Transformerç­–ç•¥ä¸åŠ é€Ÿå™¨ç­–ç•¥ä¹‹é—´çš„è¡Œä¸ºå·®å¼‚ã€‚
*   **é‡æ”¾ç¼“å†²åŒºå¤§å°**ï¼šç”±äºé¢„è®­ç»ƒé˜¶æ®µå·²ç»æä¾›äº†è¾ƒå¥½çš„åˆå§‹åŒ–ï¼Œåœ¨çº¿å¾®è°ƒé˜¶æ®µå¯ä»¥æ˜¾è‘—å‡å°é‡æ”¾ç¼“å†²åŒºçš„å¤§å°ï¼Œä»è€Œé™ä½è®¡ç®—èµ„æºéœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨åŸºäºå›¾åƒçš„ManiSkillç¯å¢ƒä¸­ï¼Œè®­ç»ƒæ—¶é—´æœ€å¤šå¯å‡å°‘ä¸€åŠã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å°†ç¦»ç­–ç•¥æ–¹æ³•æ‰€éœ€çš„é‡æ”¾ç¼“å†²åŒºå¤§å°å‡å°‘åˆ°10-20åƒï¼Œç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æ‰€éœ€çš„æ•°åä¸‡ç”šè‡³æ•°ç™¾ä¸‡çš„ç¼“å†²åŒºå¤§å°ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚åœ¨MuJoCoä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´é«˜çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚é€šè¿‡åŠ é€ŸTransformerçš„è®­ç»ƒï¼Œå¯ä»¥æ›´é«˜æ•ˆåœ°è®­ç»ƒå¤æ‚çš„æœºå™¨äººæŠ€èƒ½ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–ã€å¯¼èˆªç­‰ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥é™ä½å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼Œä½¿å¾—åœ¨èµ„æºå—é™çš„å¹³å°ä¸Šéƒ¨ç½²Transformeræˆä¸ºå¯èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨Transformeråœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The appearance of transformer-based models in Reinforcement Learning (RL) has expanded the horizons of possibilities in robotics tasks, but it has simultaneously brought a wide range of challenges during its implementation, especially in model-free online RL. Some of the existing learning algorithms cannot be easily implemented with transformer-based models due to the instability of the latter. In this paper, we propose a method that uses the Accelerator policy as a transformer's trainer. The Accelerator, a simpler and more stable model, interacts with the environment independently while simultaneously training the transformer through behavior cloning during the first stage of the proposed algorithm. In the second stage, the pretrained transformer starts to interact with the environment in a fully online setting. As a result, this model-free algorithm accelerates the transformer in terms of its performance and helps it to train online in a more stable and faster way. By conducting experiments on both state-based and image-based ManiSkill environments, as well as on MuJoCo tasks in MDP and POMDP settings, we show that applying our algorithm not only enables stable training of transformers but also reduces training time on image-based environments by up to a factor of two. Moreover, it decreases the required replay buffer size in off-policy methods to 10-20 thousand, which significantly lowers the overall computational demands.

