---
layout: default
title: AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size
---

# AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26432" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26432v2</a>
  <a href="https://arxiv.org/pdf/2509.26432.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26432v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26432v2', 'AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guanxi Lu, Hao Mark Chen, Yuto Karashima, Zhican Wang, Daichi Fujiki, Hongxiang Fan

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-01)

**å¤‡æ³¨**: Preprint. Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AdaBlock-dLLMï¼šé€šè¿‡è‡ªé€‚åº”å—å¤§å°å®ç°è¯­ä¹‰æ„ŸçŸ¥çš„æ‰©æ•£LLMæ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `å¤§è¯­è¨€æ¨¡å‹` `åŠè‡ªå›å½’è§£ç ` `è‡ªé€‚åº”å—å¤§å°` `æ¨ç†ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸåŠè‡ªå›å½’è§£ç é‡‡ç”¨å›ºå®šå—å¤§å°ï¼Œå¯¼è‡´é«˜ç½®ä¿¡åº¦tokenå»¶è¿Ÿè§£ç å’Œä½ç½®ä¿¡åº¦tokenè¿‡æ—©æäº¤ï¼Œå½±å“æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚
2. AdaBlock-dLLMé€šè¿‡åˆ†æè§£ç è¿‡ç¨‹ä¸­çš„ç½®ä¿¡åº¦åŠ¨æ€ï¼Œè¯†åˆ«æ³¢åŠ¨å¸¦åŒºåŸŸï¼Œä»è€Œè‡ªé€‚åº”åœ°è°ƒæ•´å—å¤§å°ï¼Œå¯¹é½è¯­ä¹‰è¾¹ç•Œã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAdaBlock-dLLMåœ¨ç›¸åŒååé‡ä¸‹ï¼Œèƒ½å¤Ÿæå‡é«˜è¾¾5.3%çš„ç²¾åº¦ï¼Œå±•ç¤ºäº†è‡ªé€‚åº”å—å¤§å°çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºæ‰©æ•£çš„å¤§è¯­è¨€æ¨¡å‹(dLLMs)å› å…¶å›ºæœ‰çš„å¹¶è¡Œè§£ç èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ï¼Œä¸ºè‡ªå›å½’LLMæä¾›äº†ä¸€ç§å¼•äººæ³¨ç›®çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨å„ç§è§£ç ç­–ç•¥ä¸­ï¼Œåˆ†å—åŠè‡ªå›å½’(semi-AR)æ–¹æ³•å› å…¶å¯¹KVç¼“å­˜çš„è‡ªç„¶æ”¯æŒä»¥åŠè‰¯å¥½çš„ç²¾åº¦-é€Ÿåº¦æƒè¡¡è€Œè¢«å¹¿æ³›é‡‡ç”¨ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æŒ‡å‡ºäº†ä¼ ç»Ÿé‡‡ç”¨å›ºå®šå—å¤§å°çš„semi-ARè§£ç æ–¹æ³•çš„ä¸¤ä¸ªåŸºæœ¬å±€é™æ€§ï¼ši) åæœŸè§£ç å¼€é”€ï¼Œå³ä¸å¿…è¦åœ°å»¶è¿Ÿäº†è§£ç å½“å‰å—å¤–éƒ¨çš„é«˜ç½®ä¿¡åº¦tokenï¼›ii) æå‰è§£ç é”™è¯¯ï¼Œå³è¿‡æ—©åœ°æäº¤å½“å‰å—å†…çš„ä½ç½®ä¿¡åº¦tokenï¼Œå¯¼è‡´é”™è¯¯çš„tokenã€‚æœ¬æ–‡é¦–æ¬¡å¯¹semi-ARè§£ç ä¸­å›ºå®šå—å¤§å°çš„å‡è®¾æå‡ºäº†ç³»ç»Ÿçš„ç ”ç©¶ã€‚é€šè¿‡å¯¹å»å™ªè¿‡ç¨‹ä¸­ç½®ä¿¡åº¦åŠ¨æ€çš„ç»Ÿè®¡åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«äº†dLLMè§£ç è¿‡ç¨‹ä¸­çš„æ³¢åŠ¨å¸¦(VB)åŒºåŸŸï¼Œè¯¥åŒºåŸŸç¼–ç äº†å±€éƒ¨è¯­ä¹‰ç»“æ„ï¼Œå¯ç”¨äºæŒ‡å¯¼è‡ªé€‚åº”å—å¤§å°è°ƒæ•´ã€‚åˆ©ç”¨è¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†AdaBlock-dLLMï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒã€å³æ’å³ç”¨çš„è°ƒåº¦å™¨ï¼Œé€šè¿‡åœ¨è¿è¡Œæ—¶è°ƒæ•´å—å¤§å°ï¼Œè‡ªé€‚åº”åœ°å°†å—è¾¹ç•Œä¸è¯­ä¹‰æ­¥éª¤å¯¹é½ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„ååé‡é¢„ç®—ä¸‹ï¼ŒAdaBlock-dLLMå®ç°äº†é«˜è¾¾5.3%çš„ç²¾åº¦æå‡ã€‚é™¤äº†æ¨ç†æ—¶ä¼˜åŒ–ä¹‹å¤–ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬è¿™ç§è¯­ä¹‰æ„ŸçŸ¥çš„è‡ªé€‚åº”è°ƒåº¦æ–¹æ³•å’ŒåŸºäºç½®ä¿¡åº¦çš„åˆ†æèƒ½å¤Ÿæ¿€å‘æœªæ¥dLLMçš„è®­ç»ƒç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸºäºæ‰©æ•£çš„LLMï¼ˆdLLMï¼‰ä¸­ï¼Œä½¿ç”¨å›ºå®šå—å¤§å°çš„åŠè‡ªå›å½’ï¼ˆsemi-ARï¼‰è§£ç æ–¹æ³•å­˜åœ¨çš„æ•ˆç‡å’Œå‡†ç¡®æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œå›ºå®šå—å¤§å°æ— æ³•é€‚åº”tokenç½®ä¿¡åº¦çš„åŠ¨æ€å˜åŒ–ï¼Œå¯¼è‡´é«˜ç½®ä¿¡åº¦tokençš„è§£ç å»¶è¿Ÿå’Œä½ç½®ä¿¡åº¦tokençš„è¿‡æ—©æäº¤ï¼Œä»è€Œå½±å“æ•´ä½“æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ ¹æ®è§£ç è¿‡ç¨‹ä¸­tokenç½®ä¿¡åº¦çš„å˜åŒ–ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´å—çš„å¤§å°ã€‚é€šè¿‡è¯†åˆ«ä¸€ä¸ªâ€œæ³¢åŠ¨å¸¦â€ï¼ˆVolatility Band, VBï¼‰åŒºåŸŸï¼Œè¯¥åŒºåŸŸåæ˜ äº†å±€éƒ¨è¯­ä¹‰ç»“æ„ï¼Œä»è€ŒåŠ¨æ€åœ°è°ƒæ•´å—è¾¹ç•Œï¼Œä½¿å…¶ä¸è¯­ä¹‰æ­¥éª¤å¯¹é½ã€‚è¿™æ ·å¯ä»¥é¿å…ä¸å¿…è¦çš„å»¶è¿Ÿè§£ç å’Œæå‰è§£ç é”™è¯¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAdaBlock-dLLMæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„è°ƒåº¦å™¨ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œç›‘æ§tokençš„ç½®ä¿¡åº¦å˜åŒ–ï¼›2) åŸºäºç½®ä¿¡åº¦å˜åŒ–ï¼Œè¯†åˆ«æ³¢åŠ¨å¸¦åŒºåŸŸï¼›3) æ ¹æ®æ³¢åŠ¨å¸¦åŒºåŸŸï¼ŒåŠ¨æ€è°ƒæ•´å—çš„å¤§å°ï¼Œä½¿å…¶ä¸è¯­ä¹‰è¾¹ç•Œå¯¹é½ï¼›4) ä½¿ç”¨è°ƒæ•´åçš„å—å¤§å°è¿›è¡ŒåŠè‡ªå›å½’è§£ç ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†è¯­ä¹‰æ„ŸçŸ¥çš„è‡ªé€‚åº”å—å¤§å°è°ƒæ•´æ–¹æ³•ã€‚ä¸ç°æœ‰å›ºå®šå—å¤§å°çš„æ–¹æ³•ä¸åŒï¼ŒAdaBlock-dLLMèƒ½å¤Ÿæ ¹æ®tokençš„ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´å—å¤§å°ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”è§£ç è¿‡ç¨‹ä¸­çš„è¯­ä¹‰å˜åŒ–ã€‚è¿™ç§è‡ªé€‚åº”æ€§æ˜¯å…¶æ€§èƒ½æå‡çš„å…³é”®ã€‚

**å…³é”®è®¾è®¡**ï¼šAdaBlock-dLLMçš„å…³é”®è®¾è®¡åœ¨äºæ³¢åŠ¨å¸¦ï¼ˆVBï¼‰çš„è¯†åˆ«å’Œå—å¤§å°çš„è°ƒæ•´ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼ŒVBçš„è¯†åˆ«å¯èƒ½æ¶‰åŠåˆ°å¯¹tokenç½®ä¿¡åº¦å˜åŒ–ç‡çš„é˜ˆå€¼è®¾å®šï¼Œä»¥åŠå¯¹å†å²ç½®ä¿¡åº¦ä¿¡æ¯çš„åŠ æƒå¹³å‡ç­‰ã€‚å—å¤§å°çš„è°ƒæ•´ç­–ç•¥åˆ™å¯èƒ½æ¶‰åŠåˆ°æ ¹æ®VBçš„å®½åº¦å’Œä½ç½®ï¼ŒåŠ¨æ€è°ƒæ•´å—çš„èµ·å§‹ä½ç½®å’Œé•¿åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaBlock-dLLMåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨ç›¸åŒçš„ååé‡é¢„ç®—ä¸‹ï¼ŒAdaBlock-dLLMèƒ½å¤Ÿå®ç°é«˜è¾¾5.3%çš„ç²¾åº¦æå‡ã€‚è¿™äº›ç»“æœéªŒè¯äº†è‡ªé€‚åº”å—å¤§å°è°ƒæ•´æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜AdaBlock-dLLMæ˜¯ä¸€ç§æœ‰ç«äº‰åŠ›çš„dLLMæ¨ç†ä¼˜åŒ–æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AdaBlock-dLLMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œè¯¥æ–¹æ³•å¯ä»¥é™ä½dLLMçš„éƒ¨ç½²æˆæœ¬ï¼Œå¹¶æå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-AR) approaches are widely adopted due to their natural support for KV caching and their favorable accuracy-speed trade-off. However, this paper identifies two fundamental limitations in the conventional semi-AR decoding approach that applies a fixed block size: i) late decoding overhead, where the unmasking of high-confidence tokens outside the current block is unnecessarily delayed, and ii) premature decoding error, where low-confidence tokens inside the current block are committed too early, leading to incorrect tokens. This paper presents the first systematic investigation challenging the fixed block size assumption in semi-AR decoding. Through a statistical analysis of confidence dynamics during the denoising process, we identify a volatility band (VB) region during dLLM decoding, which encodes local semantic structure and can be used to guide adaptive block sizing. Leveraging these insights, we introduce AdaBlock-dLLM, a training-free, plug-and-play scheduler that adaptively aligns block boundaries with semantic steps by adjusting block size during runtime. Extensive experiments across diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget. Beyond inference-time optimization, we hope our semantics-aware adaptive scheduling approach and confidence-based analysis will inspire future training strategies for dLLMs.

