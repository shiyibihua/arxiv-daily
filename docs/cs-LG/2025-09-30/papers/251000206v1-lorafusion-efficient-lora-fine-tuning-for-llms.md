---
layout: default
title: LoRAFusion: Efficient LoRA Fine-Tuning for LLMs
---

# LoRAFusion: Efficient LoRA Fine-Tuning for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00206" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00206v1</a>
  <a href="https://arxiv.org/pdf/2510.00206.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00206v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00206v1', 'LoRAFusion: Efficient LoRA Fine-Tuning for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhanda Zhu, Qidong Su, Yaoyao Ding, Kevin Song, Shang Wang, Gennady Pekhimenko

**åˆ†ç±»**: cs.LG, cs.AI, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: Accepted by EuroSys 2026

**DOI**: [10.1145/3767295.3769331](https://doi.org/10.1145/3767295.3769331)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/CentML/lorafusion)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LoRAFusionï¼šä¸€ç§é«˜æ•ˆçš„LLM LoRAå¾®è°ƒç³»ç»Ÿï¼Œé€šè¿‡å†…æ ¸èåˆå’Œè‡ªé€‚åº”æ‰¹å¤„ç†ä¼˜åŒ–æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LoRAå¾®è°ƒ` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§è¯­è¨€æ¨¡å‹` `å†…æ ¸èåˆ` `è‡ªé€‚åº”æ‰¹å¤„ç†` `GPUä¼˜åŒ–` `å¹¶å‘å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LoRAå¾®è°ƒç³»ç»Ÿå­˜åœ¨å†—ä½™å†…å­˜è®¿é—®å’Œæ— æ³•å¹¶å‘å¾®è°ƒå¤šä¸ªLoRAé€‚é…å™¨çš„ä½æ•ˆé—®é¢˜ï¼Œå¯¼è‡´è¿è¡Œæ—¶å¼€é”€å¤§å’ŒGPUèµ„æºåˆ©ç”¨ç‡ä½ã€‚
2. LoRAFusioné€šè¿‡å›¾åˆ†å‰²èåˆå†…å­˜å—é™æ“ä½œï¼Œæ¶ˆé™¤ä¸å¿…è¦çš„å†…å­˜è®¿é—®ï¼Œå¹¶è®¾è®¡è‡ªé€‚åº”æ‰¹å¤„ç†ç®—æ³•ï¼Œå®ç°å¤šä»»åŠ¡å¹¶å‘å¾®è°ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRAFusionåœ¨ç«¯åˆ°ç«¯æ€§èƒ½ä¸Šä¼˜äºMegatron-LMå’ŒmLoRAï¼Œå¹¶ä¸”èåˆå†…æ ¸å¯ä½œä¸ºç°æœ‰LoRAç³»ç»Ÿçš„å³æ’å³ç”¨æ›¿ä»£å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

LoRAï¼ˆLow-Rank Adaptationï¼‰å·²æˆä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰çš„ä¸»æµæ–¹æ³•ï¼Œå®ƒåœ¨æ˜¾è‘—é™ä½GPUå†…å­˜ä½¿ç”¨çš„åŒæ—¶ï¼Œä¿æŒäº†å¾®è°ƒæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„ç«äº‰åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰LoRAå¾®è°ƒç³»ç»Ÿå­˜åœ¨ä¸¤ä¸ªä¸»è¦ä½æ•ˆä¹‹å¤„ï¼šä¸€æ˜¯ç”±äºå¤§å‹æ¿€æ´»å¼ é‡çš„å†—ä½™å†…å­˜è®¿é—®å¯¼è‡´æ˜¾è‘—çš„è¿è¡Œæ—¶å¼€é”€ï¼›äºŒæ˜¯é”™å¤±äº†åœ¨åŒä¸€ç»„GPUä¸Šå¹¶å‘å¾®è°ƒå¤šä¸ªå…±äº«ç›¸åŒåŸºç¡€æ¨¡å‹çš„ç‹¬ç«‹LoRAé€‚é…å™¨çš„æœºä¼šã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LoRAFusionï¼Œä¸€ç§é«˜æ•ˆçš„LLM LoRAå¾®è°ƒç³»ç»Ÿã€‚åœ¨å†…æ ¸å±‚é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å›¾åˆ†å‰²æ–¹æ³•ï¼Œèåˆäº†å†…å­˜å—é™çš„æ“ä½œï¼Œæ¶ˆé™¤äº†ä¸å¿…è¦çš„å†…å­˜è®¿é—®ï¼Œå¹¶ä¿ç•™äº†è®¡ç®—å—é™GEMMçš„æ€§èƒ½ï¼Œè€Œæ— éœ€é‡æ–°è®¡ç®—æˆ–åŒæ­¥ã€‚åœ¨è°ƒåº¦å±‚é¢ï¼ŒLoRAFusionå¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”æ‰¹å¤„ç†ç®—æ³•ï¼Œç”¨äºå¤šä»»åŠ¡å¾®è°ƒã€‚å®ƒé¦–å…ˆå°†LoRAé€‚é…å™¨åˆ†æˆç»„ï¼Œä»¥æœ‰æ„åœ°é”™å¼€è·¨ä»»åŠ¡çš„æ‰¹å¤„ç†æ‰§è¡Œï¼Œç„¶åè§£å†³æ¯ä¸ªç»„å†…çš„è£…ç®±é—®é¢˜ï¼Œä»¥ç”Ÿæˆå¹³è¡¡çš„ã€ä¾èµ–æ„ŸçŸ¥çš„å¾®æ‰¹æ¬¡ã€‚ä¸Megatron-LMç›¸æ¯”ï¼ŒLoRAFusionå®ç°äº†é«˜è¾¾1.96å€ï¼ˆå¹³å‡1.47å€ï¼‰çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œä¸æœ€å…ˆè¿›çš„å¤šLoRAå¾®è°ƒç³»ç»ŸmLoRAç›¸æ¯”ï¼Œå®ç°äº†é«˜è¾¾1.46å€ï¼ˆå¹³å‡1.29å€ï¼‰çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„èåˆå†…æ ¸å®ç°äº†é«˜è¾¾1.39å€ï¼ˆå¹³å‡1.27å€ï¼‰çš„å†…æ ¸æ€§èƒ½æå‡ï¼Œå¯ä»¥ç›´æ¥ä½œä¸ºç°æœ‰LoRAç³»ç»Ÿä¸­çš„å³æ’å³ç”¨æ›¿ä»£å“ã€‚æˆ‘ä»¬å·²åœ¨https://github.com/CentML/lorafusionå¼€æºäº†LoRAFusionã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰LoRAå¾®è°ƒæ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œç”±äºéœ€è¦é¢‘ç¹è®¿é—®å†…å­˜ä¸­çš„æ¿€æ´»å¼ é‡ï¼Œå¯¼è‡´æ˜¾è‘—çš„è¿è¡Œæ—¶å¼€é”€ã€‚æ­¤å¤–ï¼Œç°æœ‰ç³»ç»Ÿé€šå¸¸æ— æ³•æœ‰æ•ˆåœ°å¹¶å‘å¾®è°ƒå¤šä¸ªç‹¬ç«‹çš„LoRAé€‚é…å™¨ï¼Œä»è€Œæ— æ³•å……åˆ†åˆ©ç”¨GPUèµ„æºï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†LoRAåœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLoRAFusionçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å†…æ ¸èåˆå’Œè‡ªé€‚åº”æ‰¹å¤„ç†æ¥ä¼˜åŒ–LoRAå¾®è°ƒè¿‡ç¨‹ã€‚å†…æ ¸èåˆæ—¨åœ¨å‡å°‘å†…å­˜è®¿é—®æ¬¡æ•°ï¼Œæé«˜è®¡ç®—æ•ˆç‡ï¼›è‡ªé€‚åº”æ‰¹å¤„ç†åˆ™æ—¨åœ¨å®ç°å¤šä¸ªLoRAé€‚é…å™¨çš„å¹¶å‘å¾®è°ƒï¼Œä»è€Œæé«˜GPUåˆ©ç”¨ç‡ã€‚é€šè¿‡å°†å†…å­˜å—é™çš„æ“ä½œèåˆåˆ°è®¡ç®—å¯†é›†å‹çš„GEMMæ“ä½œä¸­ï¼Œå‡å°‘äº†æ•°æ®åœ¨GPUå†…å­˜å’Œè®¡ç®—å•å…ƒä¹‹é—´çš„ä¼ è¾“ï¼Œä»è€Œæé«˜äº†æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLoRAFusionçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šèåˆå†…æ ¸å’Œè‡ªé€‚åº”æ‰¹å¤„ç†è°ƒåº¦å™¨ã€‚èåˆå†…æ ¸é€šè¿‡å›¾åˆ†å‰²æŠ€æœ¯ï¼Œå°†å¤šä¸ªå†…å­˜å—é™çš„æ“ä½œèåˆä¸ºä¸€ä¸ªè®¡ç®—å¯†é›†å‹çš„æ“ä½œï¼Œä»è€Œå‡å°‘å†…å­˜è®¿é—®æ¬¡æ•°ã€‚è‡ªé€‚åº”æ‰¹å¤„ç†è°ƒåº¦å™¨åˆ™æ ¹æ®LoRAé€‚é…å™¨çš„ä¾èµ–å…³ç³»å’Œè®¡ç®—éœ€æ±‚ï¼ŒåŠ¨æ€åœ°è°ƒæ•´æ‰¹å¤„ç†å¤§å°å’Œæ‰§è¡Œé¡ºåºï¼Œä»è€Œå®ç°å¤šä¸ªLoRAé€‚é…å™¨çš„å¹¶å‘å¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šLoRAFusionçš„å…³é”®åˆ›æ–°åœ¨äºå…¶èåˆå†…æ ¸å’Œè‡ªé€‚åº”æ‰¹å¤„ç†è°ƒåº¦å™¨ã€‚èåˆå†…æ ¸é€šè¿‡å›¾åˆ†å‰²æŠ€æœ¯ï¼Œå®ç°äº†å†…å­˜è®¿é—®å’Œè®¡ç®—çš„ä¼˜åŒ–ï¼Œä»è€Œæé«˜äº†å†…æ ¸çš„æ‰§è¡Œæ•ˆç‡ã€‚è‡ªé€‚åº”æ‰¹å¤„ç†è°ƒåº¦å™¨åˆ™é€šè¿‡åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°å’Œæ‰§è¡Œé¡ºåºï¼Œå®ç°äº†å¤šä¸ªLoRAé€‚é…å™¨çš„å¹¶å‘å¾®è°ƒï¼Œä»è€Œæé«˜äº†GPUåˆ©ç”¨ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨èåˆå†…æ ¸ä¸­ï¼Œå…³é”®çš„è®¾è®¡åœ¨äºå¦‚ä½•é€‰æ‹©åˆé€‚çš„å›¾åˆ†å‰²ç­–ç•¥ï¼Œä»¥æœ€å¤§ç¨‹åº¦åœ°å‡å°‘å†…å­˜è®¿é—®æ¬¡æ•°ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—å¯†é›†å‹æ“ä½œçš„æ€§èƒ½ã€‚åœ¨è‡ªé€‚åº”æ‰¹å¤„ç†è°ƒåº¦å™¨ä¸­ï¼Œå…³é”®çš„è®¾è®¡åœ¨äºå¦‚ä½•æ ¹æ®LoRAé€‚é…å™¨çš„ä¾èµ–å…³ç³»å’Œè®¡ç®—éœ€æ±‚ï¼ŒåŠ¨æ€åœ°è°ƒæ•´æ‰¹å¤„ç†å¤§å°å’Œæ‰§è¡Œé¡ºåºï¼Œä»¥å®ç°æœ€ä½³çš„å¹¶å‘å¾®è°ƒæ•ˆæœã€‚è®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºè£…ç®±é—®é¢˜çš„ä¼˜åŒ–ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LoRAFusionç›¸æ¯”Megatron-LMå®ç°äº†é«˜è¾¾1.96å€ï¼ˆå¹³å‡1.47å€ï¼‰çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œç›¸æ¯”æœ€å…ˆè¿›çš„å¤šLoRAå¾®è°ƒç³»ç»ŸmLoRAå®ç°äº†é«˜è¾¾1.46å€ï¼ˆå¹³å‡1.29å€ï¼‰çš„æ€§èƒ½æå‡ã€‚èåˆå†…æ ¸å®ç°äº†é«˜è¾¾1.39å€ï¼ˆå¹³å‡1.27å€ï¼‰çš„å†…æ ¸æ€§èƒ½æå‡ï¼Œå¯ç›´æ¥ä½œä¸ºç°æœ‰LoRAç³»ç»Ÿçš„å³æ’å³ç”¨æ›¿ä»£å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LoRAFusionå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé«˜æ•ˆå¾®è°ƒçš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚å®ƒèƒ½å¤Ÿæ˜¾è‘—é™ä½å¾®è°ƒæˆæœ¬ï¼Œæé«˜å¾®è°ƒæ•ˆç‡ï¼Œå¹¶æ”¯æŒå¤šä¸ªä»»åŠ¡çš„å¹¶å‘å¾®è°ƒï¼Œä»è€ŒåŠ é€ŸLLMåœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²å’Œè¿­ä»£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient Fine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly reduces GPU memory usage while maintaining competitive fine-tuned model quality on downstream tasks. Despite these benefits, we identify two key inefficiencies in existing LoRA fine-tuning systems. First, they incur substantial runtime overhead due to redundant memory accesses on large activation tensors. Second, they miss the opportunity to concurrently fine-tune multiple independent LoRA adapters that share the same base model on the same set of GPUs. This leads to missed performance gains such as reduced pipeline bubbles, better communication overlap, and improved GPU load balance.
>   To address these issues, we introduce LoRAFusion, an efficient LoRA fine-tuning system for LLMs. At the kernel level, we propose a graph-splitting method that fuses memory-bound operations. This design eliminates unnecessary memory accesses and preserves the performance of compute-bound GEMMs without incurring the cost of recomputation or synchronization. At the scheduling level, LoRAFusion introduces an adaptive batching algorithm for multi-job fine-tuning. It first splits LoRA adapters into groups to intentionally stagger batch execution across jobs, and then solves a bin-packing problem within each group to generate balanced, dependency-aware microbatches. LoRAFusion achieves up to $1.96\times$ ($1.47\times$ on average) end-to-end speedup compared to Megatron-LM, and up to $1.46\times$ ($1.29\times$ on average) improvement over mLoRA, the state-of-the-art multi-LoRA fine-tuning system. Our fused kernel achieves up to $1.39\times$ ($1.27\times$ on average) kernel performance improvement and can directly serve as a plug-and-play replacement in existing LoRA systems. We open-source LoRAFusion at https://github.com/CentML/lorafusion.

