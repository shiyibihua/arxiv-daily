---
layout: default
title: Guiding Mixture-of-Experts with Temporal Multimodal Interactions
---

# Guiding Mixture-of-Experts with Temporal Multimodal Interactions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25678" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25678v2</a>
  <a href="https://arxiv.org/pdf/2509.25678.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25678v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25678v2', 'Guiding Mixture-of-Experts with Temporal Multimodal Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xing Han, Hsing-Huan Chung, Joydeep Ghosh, Paul Pu Liang, Suchi Saria

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-08)

**å¤‡æ³¨**: 21 pages, 8 figures, 10 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶åºå¤šæ¨¡æ€äº¤äº’å¼•å¯¼çš„MoEæ¶æ„ï¼Œæå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½ä¸å¯è§£é‡Šæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æ··åˆä¸“å®¶æ¨¡å‹` `æ—¶åºå»ºæ¨¡` `äº¤äº’å»ºæ¨¡` `åŠ¨æ€è·¯ç”±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MoEæ¨¡å‹å¿½ç•¥äº†å¤šæ¨¡æ€æ•°æ®é—´éšæ—¶é—´å˜åŒ–çš„äº¤äº’ä¿¡æ¯ï¼Œé™åˆ¶äº†ä¸“å®¶ specialization å’Œæ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚
2. æå‡ºä¸€ç§æ–°æ¡†æ¶ï¼Œåˆ©ç”¨é‡åŒ–çš„æ—¶åºå¤šæ¨¡æ€äº¤äº’ä¿¡æ¯å¼•å¯¼MoEè·¯ç”±ï¼Œä½¿ä¸“å®¶å­¦ä¹ é€šç”¨çš„äº¤äº’å¤„ç†æŠ€èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼ŒéªŒè¯äº†æ—¶åºäº¤äº’ä¿¡æ¯çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶æ¨¡å‹(MoE)æ¶æ„å·²æˆä¸ºå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„å…³é”®ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è·¯ç”±æœºåˆ¶é€šå¸¸å¿½ç•¥äº†æ¨¡æ€ä¹‹é—´ä¿¡æ¯ä¸°å¯Œçš„ã€éšæ—¶é—´å˜åŒ–çš„äº¤äº’åŠ¨æ€ã€‚è¿™ç§é™åˆ¶é˜»ç¢äº†ä¸“å®¶ specializationï¼Œå› ä¸ºæ¨¡å‹æ— æ³•æ˜¾å¼åœ°åˆ©ç”¨å†…åœ¨çš„æ¨¡æ€å…³ç³»è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨é‡åŒ–çš„æ—¶é—´äº¤äº’æ¥å¼•å¯¼MoEè·¯ç”±ã€‚ä¸€ä¸ªå¤šæ¨¡æ€äº¤äº’æ„ŸçŸ¥è·¯ç”±å™¨å­¦ä¹ æ ¹æ®äº¤äº’çš„æ€§è´¨å°†tokensåˆ†æ´¾ç»™ä¸“å®¶ã€‚è¿™ç§åŠ¨æ€è·¯ç”±é¼“åŠ±ä¸“å®¶è·å¾—å¯æ³›åŒ–çš„äº¤äº’å¤„ç†æŠ€èƒ½ï¼Œè€Œä¸ä»…ä»…æ˜¯å­¦ä¹ ç‰¹å®šäºä»»åŠ¡çš„ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ¡†æ¶å»ºç«‹åœ¨æ—¶é—´å¤šæ¨¡æ€äº¤äº’åŠ¨æ€çš„æ–°å…¬å¼ä¹‹ä¸Šï¼Œè¿™äº›åŠ¨æ€ç”¨äºæŒ‡å¯¼ä¸“å®¶è·¯ç”±ã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜è¿™äº›æ—¶é—´å¤šæ¨¡æ€äº¤äº’æ­ç¤ºäº†è·¨åº”ç”¨ç¨‹åºçš„æœ‰æ„ä¹‰çš„æ¨¡å¼ï¼Œç„¶åå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨å®ƒä»¬æ¥æ”¹è¿›åŸºäºMoEçš„æ¨¡å‹çš„è®¾è®¡å’Œæ€§èƒ½ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€åŸºå‡†ä¸Šçš„ç»¼åˆå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†å¢å¼ºçš„æ€§èƒ½å’Œæ”¹è¿›çš„å¯è§£é‡Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºMoEçš„å¤šæ¨¡æ€æ¨¡å‹åœ¨è·¯ç”±å†³ç­–æ—¶ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ¨¡æ€ä¹‹é—´éšæ—¶é—´å˜åŒ–çš„äº¤äº’ä¿¡æ¯ã€‚è¿™ç§å¿½ç•¥å¯¼è‡´ä¸“å®¶åªèƒ½å­¦ä¹ åˆ°ä»»åŠ¡ç›¸å…³çš„ç‰¹å®šç‰¹å¾ï¼Œè€Œæ— æ³•æ³›åŒ–åˆ°ä¸åŒçš„äº¤äº’æ¨¡å¼ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨é‡åŒ–çš„æ—¶åºå¤šæ¨¡æ€äº¤äº’ä¿¡æ¯æ¥æŒ‡å¯¼MoEçš„è·¯ç”±è¿‡ç¨‹ã€‚é€šè¿‡æ˜¾å¼åœ°å»ºæ¨¡æ¨¡æ€é—´çš„äº¤äº’åŠ¨æ€ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®ä¸åŒäº¤äº’çš„ç‰¹æ€§å°†tokensåˆ†æ´¾ç»™åˆé€‚çš„ä¸“å®¶ï¼Œä»è€Œä½¿ä¸“å®¶èƒ½å¤Ÿå­¦ä¹ åˆ°é€šç”¨çš„äº¤äº’å¤„ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ—¶åºå¤šæ¨¡æ€äº¤äº’å»ºæ¨¡æ¨¡å—ï¼šç”¨äºæå–å’Œé‡åŒ–æ¨¡æ€ä¹‹é—´éšæ—¶é—´å˜åŒ–çš„äº¤äº’ä¿¡æ¯ã€‚2) å¤šæ¨¡æ€äº¤äº’æ„ŸçŸ¥è·¯ç”±æ¨¡å—ï¼šæ ¹æ®æå–çš„äº¤äº’ä¿¡æ¯ï¼ŒåŠ¨æ€åœ°å°†tokensåˆ†æ´¾ç»™ä¸åŒçš„ä¸“å®¶ã€‚3) æ··åˆä¸“å®¶æ¨¡å—ï¼šç”±å¤šä¸ªä¸“å®¶ç»„æˆï¼Œæ¯ä¸ªä¸“å®¶è´Ÿè´£å¤„ç†ç‰¹å®šç±»å‹çš„äº¤äº’æ¨¡å¼ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œè¾“å…¥å¤šæ¨¡æ€æ•°æ®ï¼Œç»è¿‡æ—¶åºå¤šæ¨¡æ€äº¤äº’å»ºæ¨¡æ¨¡å—æå–äº¤äº’ä¿¡æ¯ï¼Œç„¶åç”±å¤šæ¨¡æ€äº¤äº’æ„ŸçŸ¥è·¯ç”±æ¨¡å—æ ¹æ®äº¤äº’ä¿¡æ¯å°†tokensåˆ†æ´¾ç»™ä¸åŒçš„ä¸“å®¶è¿›è¡Œå¤„ç†ï¼Œæœ€åå°†ä¸“å®¶çš„è¾“å‡ºè¿›è¡Œèåˆå¾—åˆ°æœ€ç»ˆç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åˆ©ç”¨æ—¶åºå¤šæ¨¡æ€äº¤äº’ä¿¡æ¯æ¥æŒ‡å¯¼MoEè·¯ç”±çš„æ€æƒ³ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾å¼åœ°å»ºæ¨¡æ¨¡æ€é—´çš„äº¤äº’åŠ¨æ€ï¼Œå¹¶å°†å…¶ç”¨äºè·¯ç”±å†³ç­–ï¼Œä»è€Œä½¿ä¸“å®¶èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´é€šç”¨çš„äº¤äº’å¤„ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æå‡ºäº†æ—¶é—´å¤šæ¨¡æ€äº¤äº’åŠ¨æ€çš„æ–°å…¬å¼ï¼Œç”¨äºé‡åŒ–æ¨¡æ€é—´çš„äº¤äº’ä¿¡æ¯ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬å¦‚ä½•é€‰æ‹©åˆé€‚çš„äº¤äº’å»ºæ¨¡æ–¹æ³•ã€å¦‚ä½•è®¾è®¡å¤šæ¨¡æ€äº¤äº’æ„ŸçŸ¥è·¯ç”±æ¨¡å—çš„ç»“æ„ã€ä»¥åŠå¦‚ä½•è®­ç»ƒæ•´ä¸ªæ¨¡å‹ç­‰ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½åŒ…å«è·¯ç”±æŸå¤±ï¼Œç”¨äºé¼“åŠ±ä¸“å®¶ specializationï¼Œä»¥åŠä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„MoEæ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ç†è§£å’Œå¤„ç†å¤šæ¨¡æ€æ—¶åºæ•°æ®çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šäººæœºäº¤äº’ã€å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ã€å¤šæ¨¡æ€è¡Œä¸ºåˆ†æã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚é€šè¿‡åˆ©ç”¨æ¨¡æ€é—´çš„äº¤äº’ä¿¡æ¯ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´å¯é çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) architectures have become pivotal for large-scale multimodal models. However, their routing mechanisms typically overlook the informative, time-varying interaction dynamics between modalities. This limitation hinders expert specialization, as the model cannot explicitly leverage intrinsic modality relationships for effective reasoning. To address this, we propose a novel framework that guides MoE routing using quantified temporal interaction. A multimodal interaction-aware router learns to dispatch tokens to experts based on the nature of their interactions. This dynamic routing encourages experts to acquire generalizable interaction-processing skills rather than merely learning task-specific features. Our framework builds on a new formulation of temporal multimodal interaction dynamics, which are used to guide expert routing. We first demonstrate that these temporal multimodal interactions reveal meaningful patterns across applications, and then show how they can be leveraged to improve both the design and performance of MoE-based models. Comprehensive experiments on challenging multimodal benchmarks validate our approach, demonstrating both enhanced performance and improved interpretability.

