---
layout: default
title: Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models
---

# Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26626" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26626v1</a>
  <a href="https://arxiv.org/pdf/2509.26626.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26626v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26626v1', 'Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 24 pages, 9 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HyperPotatoNeo/RSA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€’å½’è‡ªèšåˆ(RSA)æ–¹æ³•ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶çš„æ·±åº¦æ€è€ƒèƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†` `é€’å½’è‡ªèšåˆ` `æµ‹è¯•æ—¶æ‰©å±•` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ–¹æ³•åœ¨è®¡ç®—èµ„æºæ‰©å±•æ—¶ï¼Œè¦ä¹ˆå¹¶è¡Œé€‰æ‹©å¤šä¸ªç‹¬ç«‹è§£ï¼Œè¦ä¹ˆä¸²è¡Œè¿›è¡Œè‡ªç²¾ç‚¼ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ä¸­é—´æ¨ç†è¿‡ç¨‹çš„ä¿¡æ¯ã€‚
2. RSAæ–¹æ³•å€Ÿé‰´è¿›åŒ–ç®—æ³•æ€æƒ³ï¼Œé€šè¿‡é€’å½’åœ°èšåˆå€™é€‰æ¨ç†é“¾çš„å­é›†ï¼Œè¿­ä»£ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRSAåœ¨å¤šç§ä»»åŠ¡å’Œæ¨¡å‹ä¸Šå‡èƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œèƒ½è¾¾åˆ°ä¸æ›´å¤§æ¨¡å‹ç›¸åª²ç¾çš„æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé€’å½’è‡ªèšåˆ(RSA)çš„æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å—åˆ°è¿›åŒ–æ–¹æ³•çš„å¯å‘ï¼Œç»“åˆäº†å¹¶è¡Œå’Œé¡ºåºæ‰©å±•çš„ä¼˜ç‚¹ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æ€§èƒ½ã€‚RSAçš„æ¯ä¸ªæ­¥éª¤éƒ½é€šè¿‡èšåˆå­é›†æ¥æ”¹è¿›å€™é€‰æ¨ç†é“¾çš„ç¾¤ä½“ï¼Œä»è€Œäº§ç”Ÿæ”¹è¿›çš„è§£å†³æ–¹æ¡ˆç¾¤ä½“ï¼Œç„¶åå°†å…¶ç”¨ä½œä¸‹ä¸€æ¬¡è¿­ä»£çš„å€™é€‰æ± ã€‚RSAåˆ©ç”¨æ¨ç†é“¾ä¸­åµŒå…¥çš„ä¸°å¯Œä¿¡æ¯ï¼ˆä¸ä»…ä»…æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œå¹¶èƒ½å¤Ÿä»ä¸åŒæ€ç»´é“¾ä¸­éƒ¨åˆ†æ­£ç¡®çš„ä¸­é—´æ­¥éª¤è¿›è¡Œå¼•å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRSAåœ¨ä¸åŒçš„ä»»åŠ¡ã€æ¨¡å‹ç³»åˆ—å’Œå¤§å°ä¸Šï¼Œéšç€è®¡ç®—é¢„ç®—çš„å¢åŠ ï¼Œæ€§èƒ½æ˜¾è‘—æé«˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRSAä½¿Qwen3-4B-Instruct-2507èƒ½å¤Ÿå®ç°ä¸æ›´å¤§çš„æ¨ç†æ¨¡å‹ï¼ˆåŒ…æ‹¬DeepSeek-R1å’Œo3-mini (high)ï¼‰ç›¸åª²ç¾çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨AIME-25ã€HMMT-25ã€Reasoning Gymã€LiveCodeBench-v6å’ŒSuperGPQAä¸Šä¼˜äºçº¯ç²¹çš„å¹¶è¡Œå’Œé¡ºåºæ‰©å±•ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†é€šè¿‡ä¸€ç§æ–°é¢–çš„èšåˆæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•è®­ç»ƒæ¨¡å‹æ¥ç»„åˆè§£å†³æ–¹æ¡ˆå¯ä»¥äº§ç”Ÿæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä»£ç å¯åœ¨https://github.com/HyperPotatoNeo/RSAè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶ï¼Œä¸»è¦é‡‡ç”¨å¹¶è¡Œæˆ–ä¸²è¡Œçš„è®¡ç®—æ‰©å±•æ–¹å¼ã€‚å¹¶è¡Œæ–¹æ³•ç‹¬ç«‹ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œç„¶åé€‰æ‹©æœ€ä¼˜è§£ï¼Œä½†å¿½ç•¥äº†ä¸åŒç­”æ¡ˆä¹‹é—´çš„å…³è”ä¿¡æ¯ã€‚ä¸²è¡Œæ–¹æ³•åˆ™é€šè¿‡è‡ªç²¾ç‚¼é€æ­¥æ”¹è¿›ç­”æ¡ˆï¼Œä½†å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½æœªèƒ½å……åˆ†åˆ©ç”¨æ¨ç†é“¾ä¸­è•´å«çš„ä¸°å¯Œä¿¡æ¯ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­é—´æ­¥éª¤ä¸­å¯èƒ½å­˜åœ¨çš„æœ‰ä»·å€¼çš„çº¿ç´¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRSAçš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´è¿›åŒ–ç®—æ³•çš„æ€æƒ³ï¼Œå°†æ¨ç†è¿‡ç¨‹è§†ä¸ºä¸€ä¸ªç§ç¾¤è¿›åŒ–è¿‡ç¨‹ã€‚é€šè¿‡ä¸æ–­åœ°èšåˆå’Œé€‰æ‹©ä¼˜ç§€çš„æ¨ç†é“¾ï¼Œé€æ­¥æå‡æ•´ä½“çš„æ¨ç†èƒ½åŠ›ã€‚RSAå……åˆ†åˆ©ç”¨äº†æ¨ç†é“¾ä¸­çš„ä¸­é—´æ­¥éª¤ä¿¡æ¯ï¼Œå…è®¸ä»éƒ¨åˆ†æ­£ç¡®çš„ä¸­é—´æ­¥éª¤ä¸­è¿›è¡Œå¼•å¯¼ï¼Œä»è€Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œå¹¶æ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRSAçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) åˆå§‹åŒ–ï¼šç”Ÿæˆä¸€ç»„å€™é€‰æ¨ç†é“¾ï¼Œä½œä¸ºåˆå§‹ç§ç¾¤ã€‚2) èšåˆï¼šä»ç§ç¾¤ä¸­éšæœºé€‰æ‹©å­é›†ï¼Œå¹¶ä½¿ç”¨èšåˆå‡½æ•°å°†è¿™äº›å­é›†ç»„åˆæˆæ–°çš„æ¨ç†é“¾ã€‚èšåˆå‡½æ•°å¯ä»¥æ˜¯ç®€å•çš„æŠ•ç¥¨æˆ–æ›´å¤æ‚çš„æ¨¡å‹ã€‚3) é€‰æ‹©ï¼šæ ¹æ®æŸç§è¯„ä»·æŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹ç½®ä¿¡åº¦æˆ–å¤–éƒ¨éªŒè¯ï¼‰é€‰æ‹©ä¼˜ç§€çš„æ¨ç†é“¾ï¼Œä½œä¸ºä¸‹ä¸€ä»£ç§ç¾¤ã€‚4) è¿­ä»£ï¼šé‡å¤æ­¥éª¤2å’Œ3ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„è¿­ä»£æ¬¡æ•°æˆ–æ»¡è¶³åœæ­¢æ¡ä»¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šRSAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é€’å½’è‡ªèšåˆçš„æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„å¹¶è¡Œæˆ–ä¸²è¡Œæ–¹æ³•ä¸åŒï¼ŒRSAèƒ½å¤Ÿå……åˆ†åˆ©ç”¨æ¨ç†é“¾ä¸­çš„ä¸­é—´æ­¥éª¤ä¿¡æ¯ï¼Œå¹¶å…è®¸ä»ä¸åŒçš„æ¨ç†é“¾ä¸­æå–æœ‰ç”¨çš„çº¿ç´¢ã€‚æ­¤å¤–ï¼ŒRSAè¿˜å¼•å…¥äº†ä¸€ç§èšåˆæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹å­¦ä¹ å¦‚ä½•æ›´å¥½åœ°ç»„åˆä¸åŒçš„æ¨ç†é“¾ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šRSAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) èšåˆå‡½æ•°çš„é€‰æ‹©ï¼šå¯ä»¥ä½¿ç”¨ç®€å•çš„æŠ•ç¥¨æœºåˆ¶ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹ï¼Œä¾‹å¦‚Transformeræ¨¡å‹ï¼Œæ¥å­¦ä¹ å¦‚ä½•ç»„åˆä¸åŒçš„æ¨ç†é“¾ã€‚2) é€‰æ‹©ç­–ç•¥ï¼šå¯ä»¥ä½¿ç”¨åŸºäºæ¨¡å‹ç½®ä¿¡åº¦çš„é€‰æ‹©ç­–ç•¥ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å¤–éƒ¨éªŒè¯æ¥é€‰æ‹©æ›´å¯é çš„æ¨ç†é“¾ã€‚3) è¿­ä»£æ¬¡æ•°ï¼šè¿­ä»£æ¬¡æ•°å†³å®šäº†RSAçš„è®¡ç®—å¤æ‚åº¦ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œè®¡ç®—èµ„æºè¿›è¡Œè°ƒæ•´ã€‚4) èšåˆæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒæ¨¡å‹å­¦ä¹ å¦‚ä½•æ›´å¥½åœ°ç»„åˆä¸åŒçš„æ¨ç†é“¾ï¼Œå¥–åŠ±å‡½æ•°å¯ä»¥åŸºäºæœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§æˆ–ä¸­é—´æ­¥éª¤çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRSAåœ¨AIME-25ã€HMMT-25ã€Reasoning Gymã€LiveCodeBench-v6å’ŒSuperGPQAç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼ŒRSAä½¿Qwen3-4B-Instruct-2507æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸DeepSeek-R1å’Œo3-mini (high)ç­‰æ›´å¤§æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¼˜äºçº¯ç²¹çš„å¹¶è¡Œå’Œé¡ºåºæ‰©å±•ç­–ç•¥ã€‚æ­¤å¤–ï¼Œé€šè¿‡èšåˆæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒRSAçš„æ€§èƒ½å¾—åˆ°äº†è¿›ä¸€æ­¥æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RSAæ–¹æ³•å¯åº”ç”¨äºå„ç§éœ€è¦æ·±åº¦æ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€é€»è¾‘æ¨ç†ç­‰ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºè®¡ç®—èµ„æºå—é™çš„åœºæ™¯ï¼Œèƒ½å¤Ÿä»¥è¾ƒä½çš„æˆæœ¬æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æœªæ¥ï¼ŒRSAæœ‰æœ›æˆä¸ºä¸€ç§é€šç”¨çš„æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ï¼Œå¹¶è¢«å¹¿æ³›åº”ç”¨äºå„ç§å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains -- not just the final answers -- and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at https://github.com/HyperPotatoNeo/RSA.

