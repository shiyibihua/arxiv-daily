---
layout: default
title: Boundary-to-Region Supervision for Offline Safe Reinforcement Learning
---

# Boundary-to-Region Supervision for Offline Safe Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25727" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25727v1</a>
  <a href="https://arxiv.org/pdf/2509.25727.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25727v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25727v1', 'Boundary-to-Region Supervision for Offline Safe Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huikang Su, Dengyun Peng, Zifeng Zhuang, YuHan Liu, Qiguang Chen, Donglin Wang, Qinghe Liu

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HuikangSu/B2R)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºB2Ræ¡†æ¶ï¼Œé€šè¿‡éå¯¹ç§°æ¡ä»¶ä½œç”¨è§£å†³ç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­çš„çº¦æŸæ»¡è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `åºåˆ—æ¨¡å‹` `Transformer` `éå¯¹ç§°æ¡ä»¶ä½œç”¨` `æˆæœ¬ä¿¡å·é‡å¯¹é½` `è¾¹ç•Œçº¦æŸ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯¹ç§°å¤„ç†RTGå’ŒCTGï¼Œå¿½ç•¥äº†RTGä½œä¸ºçµæ´»æ€§èƒ½ç›®æ ‡ã€CTGä½œä¸ºåˆšæ€§å®‰å…¨è¾¹ç•Œçš„å†…åœ¨ä¸å¯¹ç§°æ€§ã€‚
2. B2Ræ¡†æ¶é€šè¿‡æˆæœ¬ä¿¡å·é‡å¯¹é½å®ç°éå¯¹ç§°æ¡ä»¶ä½œç”¨ï¼Œå°†CTGé‡æ–°å®šä¹‰ä¸ºå®‰å…¨è¾¹ç•Œï¼Œç»Ÿä¸€æˆæœ¬åˆ†å¸ƒå¹¶ä¿ç•™å¥–åŠ±ç»“æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒB2Råœ¨å®‰å…¨å…³é”®ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†å®‰å…¨çº¦æŸæ»¡è¶³ç‡å’Œå¥–åŠ±æ€§èƒ½ï¼ŒéªŒè¯äº†éå¯¹ç§°æ¡ä»¶ä½œç”¨çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBoundary-to-Region (B2R) çš„æ¡†æ¶ï¼Œç”¨äºè§£å†³ç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç°æœ‰åŸºäºåºåˆ—æ¨¡å‹çš„æ–¹æ³•ç”±äºå¯¹ç§°åœ°å¤„ç†return-to-go (RTG) å’Œ cost-to-go (CTG) è¾“å…¥tokenè€Œå¯¼è‡´çš„çº¦æŸæ»¡è¶³ä¸å¯é é—®é¢˜ã€‚B2Ré€šè¿‡æˆæœ¬ä¿¡å·é‡å¯¹é½å®ç°éå¯¹ç§°æ¡ä»¶ä½œç”¨ï¼Œå°†CTGé‡æ–°å®šä¹‰ä¸ºå›ºå®šå®‰å…¨é¢„ç®—ä¸‹çš„è¾¹ç•Œçº¦æŸï¼Œç»Ÿä¸€äº†æ‰€æœ‰å¯è¡Œè½¨è¿¹çš„æˆæœ¬åˆ†å¸ƒï¼ŒåŒæ—¶ä¿ç•™äº†å¥–åŠ±ç»“æ„ã€‚ç»“åˆæ—‹è½¬ä½ç½®åµŒå…¥ï¼Œå¢å¼ºäº†å®‰å…¨åŒºåŸŸå†…çš„æ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒB2Råœ¨38ä¸ªå®‰å…¨å…³é”®ä»»åŠ¡ä¸­çš„35ä¸ªä»»åŠ¡ä¸­æ»¡è¶³äº†å®‰å…¨çº¦æŸï¼Œå¹¶å®ç°äº†ä¼˜äºåŸºçº¿æ–¹æ³•çš„å¥–åŠ±æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å¯¹ç§°tokenæ¡ä»¶ä½œç”¨çš„å±€é™æ€§ï¼Œå¹¶ä¸ºå°†åºåˆ—æ¨¡å‹åº”ç”¨äºå®‰å…¨å¼ºåŒ–å­¦ä¹ å»ºç«‹äº†ä¸€ç§æ–°çš„ç†è®ºå’Œå®è·µæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ æ—¨åœ¨ä»é™æ€æ•°æ®é›†ä¸­å­¦ä¹ æ»¡è¶³é¢„å®šä¹‰å®‰å…¨çº¦æŸçš„ç­–ç•¥ã€‚ç°æœ‰åŸºäºåºåˆ—æ¨¡å‹çš„æ–¹æ³•ï¼Œå¦‚ä½¿ç”¨Transformerçš„å†³ç­–ä¼˜åŒ–æ–¹æ³•ï¼Œé€šå¸¸å¯¹ç§°åœ°å¤„ç†return-to-go (RTG) å’Œ cost-to-go (CTG) ä½œä¸ºè¾“å…¥tokenã€‚ç„¶è€Œï¼ŒRTGä»£è¡¨æœŸæœ›çš„æ€§èƒ½ç›®æ ‡ï¼Œå…·æœ‰ä¸€å®šçš„çµæ´»æ€§ï¼Œè€ŒCTGåˆ™ä»£è¡¨å¿…é¡»ä¸¥æ ¼éµå®ˆçš„å®‰å…¨è¾¹ç•Œã€‚è¿™ç§å¯¹ç§°å¤„ç†æ–¹å¼å¯¼è‡´æ¨¡å‹éš¾ä»¥å¯é åœ°æ»¡è¶³å®‰å…¨çº¦æŸï¼Œå°¤å…¶æ˜¯åœ¨é‡åˆ°åˆ†å¸ƒå¤–çš„æˆæœ¬è½¨è¿¹æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šB2Rçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡éå¯¹ç§°æ¡ä»¶ä½œç”¨æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå°†CTGè§†ä¸ºä¸€ä¸ªç¡¬æ€§çš„è¾¹ç•Œçº¦æŸï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå¯ä»¥çµæ´»è°ƒæ•´çš„ç›®æ ‡ã€‚é€šè¿‡å°†CTGé‡æ–°å®šä¹‰ä¸ºåœ¨å›ºå®šå®‰å…¨é¢„ç®—ä¸‹çš„è¾¹ç•Œçº¦æŸï¼ŒB2Rç»Ÿä¸€äº†æ‰€æœ‰å¯è¡Œè½¨è¿¹çš„æˆæœ¬åˆ†å¸ƒï¼Œä»è€Œä½¿æ¨¡å‹æ›´å®¹æ˜“å­¦ä¹ åˆ°å®‰å…¨ç­–ç•¥ã€‚åŒæ—¶ï¼ŒB2Rä¿ç•™äº†å¥–åŠ±ç»“æ„ï¼Œä»¥ç¡®ä¿ç­–ç•¥èƒ½å¤Ÿæœ€å¤§åŒ–æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šB2Ræ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) æˆæœ¬ä¿¡å·é‡å¯¹é½ï¼šå°†CTGé‡æ–°å®šä¹‰ä¸ºå›ºå®šå®‰å…¨é¢„ç®—ä¸‹çš„è¾¹ç•Œçº¦æŸã€‚2) éå¯¹ç§°æ¡ä»¶ä½œç”¨ï¼šä½¿ç”¨é‡å¯¹é½åçš„CTGä½œä¸ºç¡¬æ€§çº¦æŸï¼Œè€ŒRTGä½œä¸ºçµæ´»çš„ç›®æ ‡ã€‚3) åºåˆ—å»ºæ¨¡ï¼šä½¿ç”¨Transformerç­‰åºåˆ—æ¨¡å‹æ¥å­¦ä¹ ç­–ç•¥ï¼Œå…¶ä¸­è¾“å…¥åŒ…æ‹¬çŠ¶æ€ã€åŠ¨ä½œã€RTGå’Œé‡å¯¹é½åçš„CTGã€‚4) æ—‹è½¬ä½ç½®åµŒå…¥ï¼šåˆ©ç”¨æ—‹è½¬ä½ç½®åµŒå…¥å¢å¼ºåœ¨å®‰å…¨åŒºåŸŸå†…çš„æ¢ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šB2Ræœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶éå¯¹ç§°æ¡ä»¶ä½œç”¨æ–¹æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•å¯¹ç§°åœ°å¤„ç†RTGå’ŒCTGä¸åŒï¼ŒB2Rå°†CTGè§†ä¸ºç¡¬æ€§çº¦æŸï¼Œä»è€Œæé«˜äº†å®‰å…¨çº¦æŸçš„æ»¡è¶³ç‡ã€‚æ­¤å¤–ï¼ŒB2Ré€šè¿‡æˆæœ¬ä¿¡å·é‡å¯¹é½ï¼Œç»Ÿä¸€äº†æˆæœ¬åˆ†å¸ƒï¼Œä½¿å¾—æ¨¡å‹æ›´å®¹æ˜“å­¦ä¹ åˆ°å®‰å…¨ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šB2Rçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æˆæœ¬ä¿¡å·é‡å¯¹é½çš„å…·ä½“æ–¹æ³•ï¼Œå³å°†CTGæ˜ å°„åˆ°å›ºå®šå®‰å…¨é¢„ç®—ä¸‹çš„è¾¹ç•Œçº¦æŸã€‚2) æ—‹è½¬ä½ç½®åµŒå…¥çš„ä½¿ç”¨ï¼Œä»¥å¢å¼ºåœ¨å®‰å…¨åŒºåŸŸå†…çš„æ¢ç´¢ã€‚3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œéœ€è¦åŒæ—¶è€ƒè™‘å¥–åŠ±æœ€å¤§åŒ–å’Œå®‰å…¨çº¦æŸæ»¡è¶³ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©å–å†³äºå…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒB2Råœ¨38ä¸ªå®‰å…¨å…³é”®ä»»åŠ¡ä¸­çš„35ä¸ªä»»åŠ¡ä¸­æˆåŠŸæ»¡è¶³äº†å®‰å…¨çº¦æŸï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚åŒæ—¶ï¼ŒB2Råœ¨å¥–åŠ±æ€§èƒ½æ–¹é¢ä¹Ÿå–å¾—äº†ä¼˜äºåŸºçº¿æ–¹æ³•çš„ç»“æœï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿåœ¨ä¿è¯å®‰å…¨æ€§çš„å‰æä¸‹ï¼Œå®ç°æ›´é«˜çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼ŒB2Rçš„å¥–åŠ±æ€§èƒ½æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†è¶…è¿‡20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

B2Ræ¡†æ¶åœ¨æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å†³ç­–ç­‰å®‰å…¨å…³é”®é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­å­¦ä¹ åˆ°æ—¢èƒ½å®ç°é«˜æ€§èƒ½ï¼Œåˆèƒ½æ»¡è¶³å®‰å…¨çº¦æŸçš„ç­–ç•¥ï¼Œä»è€Œé¿å…æ½œåœ¨çš„å±é™©è¡Œä¸ºã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒB2Rå¯ä»¥ç¡®ä¿è½¦è¾†åœ¨è¡Œé©¶è¿‡ç¨‹ä¸­å§‹ç»ˆéµå®ˆäº¤é€šè§„åˆ™ï¼Œé¿å…å‘ç”Ÿäº¤é€šäº‹æ•…ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline safe reinforcement learning aims to learn policies that satisfy predefined safety constraints from static datasets. Existing sequence-model-based methods condition action generation on symmetric input tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry: return-to-go (RTG) serves as a flexible performance target, while cost-to-go (CTG) should represent a rigid safety boundary. This symmetric conditioning leads to unreliable constraint satisfaction, especially when encountering out-of-distribution cost trajectories. To address this, we propose Boundary-to-Region (B2R), a framework that enables asymmetric conditioning through cost signal realignment . B2R redefines CTG as a boundary constraint under a fixed safety budget, unifying the cost distribution of all feasible trajectories while preserving reward structures. Combined with rotary positional embeddings , it enhances exploration within the safe region. Experimental results show that B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods. This work highlights the limitations of symmetric token conditioning and establishes a new theoretical and practical approach for applying sequence models to safe RL. Our code is available at https://github.com/HuikangSu/B2R.

