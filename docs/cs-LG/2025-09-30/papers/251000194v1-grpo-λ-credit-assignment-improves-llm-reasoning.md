---
layout: default
title: GRPO-$Î»$: Credit Assignment improves LLM Reasoning
---

# GRPO-$Î»$: Credit Assignment improves LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00194" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00194v1</a>
  <a href="https://arxiv.org/pdf/2510.00194.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00194v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00194v1', 'GRPO-$Î»$: Credit Assignment improves LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Prasanna Parthasarathi, Mathieu Reymond, Boxing Chen, Yufei Cui, Sarath Chandar

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GRPO-Î»ï¼šé€šè¿‡æ”¹è¿›ä¿¡ç”¨åˆ†é…æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ä¿¡ç”¨åˆ†é…` `æ¨ç†èƒ½åŠ›` `èµ„æ ¼è¿¹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰GRPOæ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹ç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…æœºåˆ¶ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚
2. GRPO-Î»é€šè¿‡å¼•å…¥åŸºäºèµ„æ ¼è¿¹çš„Î»-returnè¿‘ä¼¼ï¼Œä»¥åŠæ— è¯„è®ºå®¶çš„æ—¶é—´å·®åˆ†è¯¯å·®ä¼°è®¡ï¼Œå¢å¼ºäº†tokençº§åˆ«çš„ä¿¡ç”¨åˆ†é…ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGRPO-Î»åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºGRPOï¼Œæ€§èƒ½æå‡é«˜è¾¾30-40%ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´é«˜çš„åˆ†æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²äºéœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ä¸­ï¼Œè¿™æ¿€å‘äº†äººä»¬å¯¹é€šè¿‡åè®­ç»ƒæ¥æé«˜å…¶æ¨ç†èƒ½åŠ›çš„æå¤§å…´è¶£ã€‚ç‰¹åˆ«æ˜¯åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¹¶ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„æ–¹æ³•ï¼Œå¦‚æœ€å…ˆè¿›çš„GRPOï¼Œåœ¨ä½œä¸ºåè®­ç»ƒæ–¹æ³•åº”ç”¨æ—¶ï¼Œå·²ç»æ˜¾ç¤ºå‡ºæå¤§åœ°æ”¹å–„æ¨ç†è¡Œä¸ºã€‚ç„¶è€Œï¼Œç¼ºä¹æ˜¾å¼çš„å¥–åŠ±æˆ–è¯„è®ºå®¶æ¨¡å‹é™åˆ¶äº†GRPOåœ¨tokenåºåˆ—ä¸­è¿›è¡Œç»†ç²’åº¦ä¿¡ç”¨åˆ†é…çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†GRPO-Î»ï¼Œè¿™æ˜¯GRPOçš„ä¸€ä¸ªæ–°æ‰©å±•ï¼Œæ—¨åœ¨å¢å¼ºLLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡çš„RLå¾®è°ƒä¸­çš„ä¿¡ç”¨åˆ†é…ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä½¿ç”¨tokençº§åˆ«å¯¹æ•°æ¦‚ç‡çš„èµ„æ ¼è¿¹ï¼ˆeligibility tracesï¼‰è¿›è¡Œé‡æ–°å…¬å¼åŒ–ï¼Œä»¥åŠå¯¹æ—¶é—´å·®åˆ†è¯¯å·®è¿›è¡Œæ–°é¢–çš„æ— è¯„è®ºå®¶è¿‘ä¼¼ï¼Œæ¥è¿‘ä¼¼ä»Î»-returnä¸­å­¦ä¹ ã€‚æˆ‘ä»¬å¼•å…¥äº†Î»-returnåŠ æƒçš„ä¸€äº›å˜ä½“ï¼Œä»¥åŠå®ƒä»¬åœ¨èµ„æ ¼è¿¹ä¸­çš„åº”ç”¨ï¼Œæ‰€æœ‰è¿™äº›å˜ä½“éƒ½æ¯”GRPOæä¾›äº†æ˜¾è‘—çš„å¢ç›Šã€‚æˆ‘ä»¬å°†GRPO-Î»ä¸GRPOè¿›è¡Œäº†æ¯”è¾ƒï¼Œé€šè¿‡åœ¨4ä¸ªä¸åŒçš„æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šè®­ç»ƒå‚æ•°ä»1.5Båˆ°7Bçš„æ¨¡å‹ã€‚è®­ç»ƒæ›²çº¿è¡¨æ˜ï¼Œåœ¨LLaMA-3.1å’ŒQwen-2.5æ¶æ„ä¸Šï¼ŒRLè®­ç»ƒæœŸé—´çš„æ€§èƒ½æé«˜äº†30-40%ã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨GRPO-Î»ï¼Œåœ¨AIME24ã€Math500ã€OlympiadMathã€MinervaMathå’ŒAMCä¸Šçš„å¹³å‡æ€§èƒ½æ¯”GRPOæé«˜äº†3ä¸ªå¤šç‚¹ï¼Œå¹¶ä¸”åœ¨7Bæ¨¡å‹ä¸Šæé«˜äº†4.5ä¸ªç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒæ—¶ï¼Œä¿¡ç”¨åˆ†é…ä¸æ˜ç¡®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚GRPOï¼Œè™½ç„¶èƒ½å¤Ÿåˆ©ç”¨å¯éªŒè¯çš„å¥–åŠ±æ¥æå‡æ¨ç†èƒ½åŠ›ï¼Œä½†ç¼ºä¹å¯¹tokenåºåˆ—ä¸­æ¯ä¸ªtokenè´¡çŒ®çš„ç»†ç²’åº¦è¯„ä¼°ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡é™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥Î»-returnçš„æ¦‚å¿µï¼Œå¹¶ç»“åˆèµ„æ ¼è¿¹ï¼ˆeligibility tracesï¼‰æ¥è¿‘ä¼¼è®¡ç®—æ¯ä¸ªtokenå¯¹æœ€ç»ˆå¥–åŠ±çš„è´¡çŒ®ã€‚åŒæ—¶ï¼Œä¸ºäº†é¿å…å¼•å…¥é¢å¤–çš„è¯„è®ºå®¶æ¨¡å‹ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ— è¯„è®ºå®¶çš„æ—¶é—´å·®åˆ†è¯¯å·®ä¼°è®¡æ–¹æ³•ï¼Œä»è€Œåœ¨ä¸å¢åŠ è®¡ç®—å¤æ‚åº¦çš„å‰æä¸‹ï¼Œå®ç°æ›´ç²¾ç¡®çš„ä¿¡ç”¨åˆ†é…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRPO-Î»çš„æ•´ä½“æ¡†æ¶ä»ç„¶åŸºäºGRPOï¼Œä½†åœ¨å¥–åŠ±è®¡ç®—å’Œåå‘ä¼ æ’­é˜¶æ®µè¿›è¡Œäº†æ”¹è¿›ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç”Ÿæˆtokenåºåˆ—åï¼Œé¦–å…ˆè®¡ç®—æ¯ä¸ªtokençš„å¯¹æ•°æ¦‚ç‡ã€‚ç„¶åï¼Œåˆ©ç”¨è¿™äº›å¯¹æ•°æ¦‚ç‡å’ŒÎ»-returnçš„è¿‘ä¼¼å€¼ï¼Œè®¡ç®—èµ„æ ¼è¿¹ã€‚æœ€åï¼Œä½¿ç”¨èµ„æ ¼è¿¹æ¥æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šGRPO-Î»çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ä½¿ç”¨èµ„æ ¼è¿¹è¿‘ä¼¼Î»-returnï¼Œä»è€Œå®ç°tokençº§åˆ«çš„ä¿¡ç”¨åˆ†é…ï¼›2) æå‡ºäº†ä¸€ç§æ— è¯„è®ºå®¶çš„æ—¶é—´å·®åˆ†è¯¯å·®ä¼°è®¡æ–¹æ³•ï¼Œé¿å…äº†å¼•å…¥é¢å¤–çš„æ¨¡å‹ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æå‡ºäº†å‡ ç§Î»-returnçš„åŠ æƒå˜ä½“ï¼Œå¹¶å°†å…¶åº”ç”¨äºèµ„æ ¼è¿¹çš„è®¡ç®—ä¸­ã€‚è¿™äº›å˜ä½“æ—¨åœ¨å¹³è¡¡çŸ­æœŸå¥–åŠ±å’Œé•¿æœŸå¥–åŠ±çš„å½±å“ï¼Œä»è€Œæé«˜å­¦ä¹ çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯¦ç»†æè¿°äº†æ— è¯„è®ºå®¶çš„æ—¶é—´å·®åˆ†è¯¯å·®ä¼°è®¡æ–¹æ³•çš„å…·ä½“å®ç°ï¼ŒåŒ…æ‹¬å¦‚ä½•åˆ©ç”¨tokençš„å¯¹æ•°æ¦‚ç‡æ¥è¿‘ä¼¼è®¡ç®—æ—¶é—´å·®åˆ†è¯¯å·®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GRPO-Î»åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜å…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºGRPOã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨LLaMA-3.1å’ŒQwen-2.5æ¶æ„ä¸Šï¼ŒRLè®­ç»ƒæœŸé—´çš„æ€§èƒ½æé«˜äº†30-40%ã€‚æ­¤å¤–ï¼Œåœ¨AIME24ã€Math500ã€OlympiadMathã€MinervaMathå’ŒAMCç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGRPO-Î»çš„å¹³å‡æ€§èƒ½æ¯”GRPOæé«˜äº†3ä¸ªå¤šç‚¹ï¼Œå¹¶ä¸”åœ¨7Bæ¨¡å‹ä¸Šæé«˜äº†4.5ä¸ªç‚¹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒGRPO-Î»æ˜¯ä¸€ç§æœ‰æ•ˆçš„æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GRPO-Î»çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éœ€è¦å¤æ‚æ¨ç†çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€é€»è¾‘æ¨ç†ç­‰ã€‚é€šè¿‡æ›´ç²¾ç¡®çš„ä¿¡ç”¨åˆ†é…ï¼ŒGRPO-Î»å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä»è€Œæå‡å…¶è§£å†³å®é™…é—®é¢˜çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºé™ä½äº†å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„éš¾åº¦ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¼ºå¤§çš„æ¨ç†æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚æœªæ¥ï¼ŒGRPO-Î»å¯ä»¥è¢«è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„ä»»åŠ¡å’Œæ¨¡å‹ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are increasingly deployed for tasks requiring complex reasoning, prompting significant interest in improving their reasoning abilities through post-training. Especially RL based methods using verifiable reward, like the state-of-the-art GRPO, have shown to tremendously improve reasoning behaviors when applied as post-training methods. However, the lack of an explicit reward or critic model limits GRPO's ability to assign fine-grained credit across token sequences. In this work, we present GRPO-$Î»$, a novel extension to GRPO that enhances credit assignment in RL finetuning of LLMs for complex reasoning tasks. We approximate learning from $Î»$-return with a reformulation of eligibility traces using token-level log-probabilities applied after each sequence generation, and a novel critic-free approximation of the temporal-difference error. We introduce a few variations for the weighting of the $Î»$-return, and their applications to the eligibility-trace, where all the variations provide significant gains over GRPO. We compare GRPO-$Î»$ against GRPO by training models from 1.5B to 7B parameters on $4$ different math reasoning datasets. The training plots demonstrate 30-40% improved performance during RL training on both LLaMA-3.1 and Qwen-2.5 architectures. Finally, we show that with GRPO-$Î»$, the resulting average performance on AIME24, Math500, OlympiadMath, MinervaMath, and AMC improves over GRPO by over $3$ points and a $4.5$ points improvement on the 7B model.

