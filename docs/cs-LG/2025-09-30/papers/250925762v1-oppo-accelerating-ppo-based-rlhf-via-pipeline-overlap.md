---
layout: default
title: OPPO: Accelerating PPO-based RLHF via Pipeline Overlap
---

# OPPO: Accelerating PPO-based RLHF via Pipeline Overlap

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25762" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25762v1</a>
  <a href="https://arxiv.org/pdf/2509.25762.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25762v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25762v1', 'OPPO: Accelerating PPO-based RLHF via Pipeline Overlap')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaizhuo Yan, Yingjie Yu, Yifan Yu, Haizhong Zheng, Fan Lai

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: Kaizhuo Yan and Yingjie Yu contributed equally to this work

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OPPOï¼šé€šè¿‡æµæ°´çº¿é‡å åŠ é€ŸåŸºäºPPOçš„RLHFè®­ç»ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `æµæ°´çº¿å¹¶è¡Œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºPPOçš„RLHFè®­ç»ƒæµç¨‹å­˜åœ¨å¤šæ¨¡å‹ä¾èµ–å’Œé•¿å°¾å“åº”é—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚
2. OPPOé€šè¿‡æ­¥å†…å’Œæ­¥é—´æµæ°´çº¿é‡å ï¼Œä½¿ä¸Šä¸‹æ¸¸æ¨¡å‹å¹¶è¡Œè®¡ç®—ï¼Œç¼“è§£é•¿å°¾å»¶è¿Ÿï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒOPPOåœ¨ä¸å½±å“æ”¶æ•›æ€§çš„å‰æä¸‹ï¼Œå°†PPO-RLHFè®­ç»ƒåŠ é€Ÿ1.8-2.8å€ï¼ŒGPUåˆ©ç”¨ç‡æå‡1.4-2.1å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„ä»äººç±»åé¦ˆä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ˜¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½çš„å¸¸ç”¨èŒƒä¾‹ã€‚ç„¶è€Œï¼Œç”±äºé¡ºåºå¤šæ¨¡å‹ä¾èµ–ï¼ˆä¾‹å¦‚ï¼Œå¥–åŠ±æ¨¡å‹ä¾èµ–äºactoræ¨¡å‹çš„è¾“å‡ºï¼‰å’Œé•¿å°¾å“åº”é•¿åº¦ï¼ˆå…¶ä¸­å°‘æ•°é•¿å“åº”æ‹–å»¶äº†é˜¶æ®µå®Œæˆï¼‰ï¼Œå…¶è®­ç»ƒæµæ°´çº¿æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†OPPOï¼Œä¸€ç§æ–°é¢–ã€è½»é‡çº§ä¸”æ¨¡å‹æ— å…³çš„åŸºäºPPOçš„RLHFæ¡†æ¶ï¼Œé€šè¿‡é‡å æµæ°´çº¿æ‰§è¡Œæ¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚OPPOå¼•å…¥äº†ä¸¤é¡¹æ–°æŠ€æœ¯ï¼šï¼ˆ1ï¼‰æ­¥å†…é‡å ï¼Œå®ƒä»¥é€‚å½“å¤§å°çš„å—æµå¼ä¼ è¾“ä¸Šæ¸¸æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œactoræ¨¡å‹ï¼‰çš„è¾“å‡ºï¼Œä½¿ä¸‹æ¸¸æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œå¥–åŠ±æ¨¡å‹ï¼‰èƒ½å¤Ÿåœ¨ä¸Šæ¸¸æ¨¡å‹ç»§ç»­è§£ç æ—¶å¼€å§‹é¢„å¡«å……ï¼›ï¼ˆ2ï¼‰æ­¥é—´é‡å ï¼Œå®ƒè‡ªé€‚åº”åœ°è¿‡åº¦æäº¤ä¸€äº›æç¤ºï¼Œå¹¶å°†é•¿ç”Ÿæˆæ¨è¿Ÿåˆ°æœªæ¥çš„æ­¥éª¤ï¼Œä»è€Œåœ¨ä¸ä¸¢å¼ƒéƒ¨åˆ†å·¥ä½œçš„æƒ…å†µä¸‹å‡è½»å°¾éƒ¨å»¶è¿Ÿã€‚OPPOå¯ä»¥è½»æ¾åœ°ä¸ç°æœ‰çš„PPOå®ç°é›†æˆï¼Œåªéœ€æ›´æ”¹å‡ è¡Œä»£ç ã€‚å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼ŒOPPOå°†åŸºäºPPOçš„RLHFè®­ç»ƒåŠ é€Ÿäº†1.8å€-2.8å€ï¼Œå¹¶å°†GPUåˆ©ç”¨ç‡æé«˜äº†1.4å€-2.1å€ï¼Œè€Œä¸ä¼šå½±å“è®­ç»ƒæ”¶æ•›æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸºäºPPOçš„RLHFè®­ç»ƒä¸­å­˜åœ¨çš„æ•ˆç‡ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰çš„RLHFè®­ç»ƒæµç¨‹é€šå¸¸æ˜¯ä¸²è¡Œçš„ï¼Œå³å¥–åŠ±æ¨¡å‹å¿…é¡»ç­‰å¾…actoræ¨¡å‹ç”Ÿæˆå®Œæ•´çš„å“åº”åæ‰èƒ½è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œç”±äºç”Ÿæˆæ–‡æœ¬é•¿åº¦çš„ä¸ç¡®å®šæ€§ï¼Œå°‘æ•°é•¿æ–‡æœ¬ç”Ÿæˆä¼šæ˜¾è‘—æ‹–æ…¢æ•´ä¸ªè®­ç»ƒæµç¨‹ï¼Œå½¢æˆé•¿å°¾æ•ˆåº”ã€‚è¿™äº›å› ç´ å¯¼è‡´GPUåˆ©ç”¨ç‡ä½ï¼Œè®­ç»ƒæ—¶é—´é•¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOPPOçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æµæ°´çº¿å¹¶è¡Œçš„æ–¹å¼ï¼Œè®©actoræ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹èƒ½å¤ŸåŒæ—¶å·¥ä½œï¼Œä»è€Œæé«˜æ•´ä½“çš„è®­ç»ƒæ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼ŒOPPOé‡‡ç”¨äº†æ­¥å†…å’Œæ­¥é—´ä¸¤ç§é‡å ç­–ç•¥ï¼Œå°½å¯èƒ½åœ°å‡å°‘äº†æ¨¡å‹ä¹‹é—´çš„ç­‰å¾…æ—¶é—´ï¼Œå¹¶ç¼“è§£äº†é•¿å°¾æ•ˆåº”å¸¦æ¥çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOPPOæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š
1. **æ­¥å†…é‡å ï¼ˆIntra-step overlapï¼‰**ï¼šå°†actoræ¨¡å‹çš„è¾“å‡ºè¿›è¡Œåˆ†å—ï¼Œä»¥æµå¼ä¼ è¾“çš„æ–¹å¼å‘é€ç»™å¥–åŠ±æ¨¡å‹ã€‚å¥–åŠ±æ¨¡å‹åœ¨æ¥æ”¶åˆ°éƒ¨åˆ†è¾“å‡ºåå³å¯å¼€å§‹é¢„å¡«å……ï¼Œæ— éœ€ç­‰å¾…actoræ¨¡å‹ç”Ÿæˆå®Œæ•´çš„å“åº”ã€‚
2. **æ­¥é—´é‡å ï¼ˆInter-step overlapï¼‰**ï¼šè‡ªé€‚åº”åœ°é€‰æ‹©ä¸€éƒ¨åˆ†æç¤ºè¿›è¡Œè¿‡åº¦æäº¤ï¼Œå¹¶å°†ç”Ÿæˆæ—¶é—´è¾ƒé•¿çš„å“åº”æ¨è¿Ÿåˆ°åç»­æ­¥éª¤ä¸­å¤„ç†ã€‚è¿™æ ·å¯ä»¥é¿å…å°‘æ•°é•¿å“åº”æ‹–æ…¢æ•´ä¸ªè®­ç»ƒæ‰¹æ¬¡çš„é€Ÿåº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šOPPOçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æµæ°´çº¿é‡å ç­–ç•¥ï¼Œå®ƒæ‰“ç ´äº†ä¼ ç»ŸRLHFè®­ç»ƒæµç¨‹ä¸­çš„ä¸²è¡Œä¾èµ–å…³ç³»ï¼Œå®ç°äº†actoræ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹çš„å¹¶è¡Œè®¡ç®—ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜GPUåˆ©ç”¨ç‡ï¼Œå¹¶ç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOPPOæ— éœ€å¯¹æ¨¡å‹ç»“æ„è¿›è¡Œä¿®æ”¹ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰çš„PPOå®ç°ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šOPPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š
1. **åˆ†å—å¤§å°**ï¼šæ­¥å†…é‡å ä¸­ï¼Œéœ€è¦åˆç†è®¾ç½®åˆ†å—å¤§å°ï¼Œä»¥å¹³è¡¡é¢„å¡«å……çš„æ•ˆç‡å’Œé€šä¿¡å¼€é”€ã€‚
2. **è¿‡åº¦æäº¤ç­–ç•¥**ï¼šæ­¥é—´é‡å ä¸­ï¼Œéœ€è¦è®¾è®¡åˆç†çš„è¿‡åº¦æäº¤ç­–ç•¥ï¼Œä»¥é¿å…è¿‡åº¦å¢åŠ è®¡ç®—é‡ã€‚
3. **å»¶è¿Ÿå“åº”å¤„ç†**ï¼šå¯¹äºè¢«æ¨è¿Ÿçš„å“åº”ï¼Œéœ€è¦åœ¨åç»­æ­¥éª¤ä¸­è¿›è¡Œå¤„ç†ï¼Œä»¥ä¿è¯è®­ç»ƒçš„å®Œæ•´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OPPOé€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOPPOå¯ä»¥å°†åŸºäºPPOçš„RLHFè®­ç»ƒåŠ é€Ÿ1.8å€-2.8å€ï¼Œå¹¶å°†GPUåˆ©ç”¨ç‡æé«˜1.4å€-2.1å€ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒçš„æ”¶æ•›æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒOPPOæ˜¯ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„RLHFè®­ç»ƒåŠ é€Ÿæ–¹æ³•ï¼Œå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OPPOæ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦ä½¿ç”¨RLHFå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹é½çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜è®­ç»ƒæ•ˆç‡ï¼ŒOPPOå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´å¿«åœ°è®­ç»ƒå‡ºé«˜è´¨é‡çš„LLMï¼Œä»è€ŒåŠ é€Ÿç›¸å…³åº”ç”¨çš„è½åœ°å’Œå‘å±•ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„åœºæ™¯ï¼Œå¯ä»¥åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®ç°æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Proximal Policy Optimization (PPO)-based reinforcement learning from human feedback (RLHF) is a widely adopted paradigm for aligning large language models (LLMs) with human preferences. However, its training pipeline suffers from substantial inefficiencies due to sequential multi-model dependencies (e.g., reward model depends on actor outputs) and long-tail response lengths, where a few long responses straggle the stage completion. We present OPPO, a novel, lightweight, and model-agnostic PPO-based RLHF framework that improves training efficiency by overlapping pipeline execution. OPPO introduces two novel techniques: (1) Intra-step overlap, which streams upstream model outputs (e.g., actor model) in right-sized chunks, enabling the downstream model (e.g., reward) to begin prefill while the upstream continues decoding; and (2) Inter-step overlap, which adaptively overcommits a few prompts and defers long generations to future steps, mitigating tail latency without discarding partial work. OPPO integrates easily with existing PPO implementations with a few lines of code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF training by $1.8 \times-2.8 \times$ and improves GPU utilization by $1.4 \times-2.1 \times$ without compromising training convergence.

