---
layout: default
title: Learning to Reason as Action Abstractions with Scalable Mid-Training RL
---

# Learning to Reason as Action Abstractions with Scalable Mid-Training RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25810" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25810v3</a>
  <a href="https://arxiv.org/pdf/2509.25810.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25810v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25810v3', 'Learning to Reason as Action Abstractions with Scalable Mid-Training RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shenao Zhang, Donghan Yu, Yihao Feng, Bowen Jin, Zhaoran Wang, John Peebles, Zirui Wang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRA3ç®—æ³•ï¼Œé€šè¿‡å¯æ‰©å±•çš„ä¸­æœŸè®­ç»ƒå¼ºåŒ–å­¦ä¹ æå‡ä»£ç ç”Ÿæˆä»»åŠ¡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŠ¨ä½œæŠ½è±¡` `ä¸­æœŸè®­ç»ƒ` `ä»£ç ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æ½œåŠ›ï¼Œå°¤å…¶ç¼ºä¹æœ‰æ•ˆçš„ä¸­æœŸè®­ç»ƒç­–ç•¥ã€‚
2. RA3ç®—æ³•é€šè¿‡å¼ºåŒ–å­¦ä¹ å‘ç°æ—¶é—´ä¸€è‡´çš„æ½œåœ¨ç»“æ„ï¼Œå¹¶åŸºäºæ­¤è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå®ç°é«˜æ•ˆçš„åŠ¨ä½œæŠ½è±¡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRA3åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è¦å……åˆ†å‘æŒ¥å…¶æ½œåŠ›ï¼Œéœ€è¦ä¸€ä¸ªä¸­æœŸè®­ç»ƒé˜¶æ®µã€‚æœ‰æ•ˆçš„ä¸­æœŸè®­ç»ƒé˜¶æ®µåº”è¯†åˆ«å‡ºä¸€ç»„ç´§å‡‘ä¸”æœ‰ç”¨çš„åŠ¨ä½œï¼Œå¹¶é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ å®ç°å®ƒä»¬ä¹‹é—´çš„å¿«é€Ÿé€‰æ‹©ã€‚æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå…³äºä¸­æœŸè®­ç»ƒå¦‚ä½•å¡‘é€ åæœŸè®­ç»ƒçš„ç†è®ºç»“æœï¼šå®ƒæè¿°äº†ä¸€ä¸ªåŠ¨ä½œå­ç©ºé—´ï¼Œè¯¥å­ç©ºé—´å¯æœ€å¤§é™åº¦åœ°å‡å°‘å‰ªæå¸¦æ¥çš„ä»·å€¼è¿‘ä¼¼è¯¯å·®ä»¥åŠåç»­è§„åˆ’æœŸé—´çš„å¼ºåŒ–å­¦ä¹ è¯¯å·®ã€‚åˆ†æè¡¨æ˜ï¼Œä¸­æœŸè®­ç»ƒæ•ˆæœçš„ä¸¤ä¸ªå…³é”®å†³å®šå› ç´ æ˜¯ï¼šå‰ªææ•ˆç‡ï¼ˆå®ƒå¡‘é€ äº†åˆå§‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å…ˆéªŒï¼‰åŠå…¶å¯¹å¼ºåŒ–å­¦ä¹ æ”¶æ•›çš„å½±å“ï¼ˆå®ƒå†³å®šäº†è¯¥ç­–ç•¥å¯ä»¥é€šè¿‡åœ¨çº¿äº¤äº’æ”¹è¿›çš„ç¨‹åº¦ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå½“å†³ç­–ç©ºé—´ç´§å‡‘ä¸”æœ‰æ•ˆèŒƒå›´è¾ƒçŸ­æ—¶ï¼Œä¸­æœŸè®­ç»ƒæœ€æœ‰æ•ˆï¼Œçªå‡ºäº†åœ¨åŠ¨ä½œæŠ½è±¡ç©ºé—´è€Œä¸æ˜¯åŸå§‹åŠ¨ä½œç©ºé—´ä¸­æ“ä½œçš„é‡è¦æ€§ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„ä¸­æœŸè®­ç»ƒç®—æ³•ï¼Œå³æ¨ç†å³åŠ¨ä½œæŠ½è±¡ï¼ˆRA3ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨å¯¼äº†ä¸€ä¸ªåºåˆ—å˜åˆ†ä¸‹ç•Œï¼Œå¹¶é€šè¿‡è¿­ä»£åœ°å‘ç°æ—¶é—´ä¸€è‡´çš„æ½œåœ¨ç»“æ„ï¼ˆé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼‰æ¥ä¼˜åŒ–å®ƒï¼Œç„¶ååœ¨å¼•å¯¼æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åœ¨å¤šä¸ªåŸºç¡€æ¨¡å‹ä¸Šï¼ŒRA3åœ¨HumanEvalå’ŒMBPPä¸Šçš„å¹³å‡æ€§èƒ½æ¯”åŸºç¡€æ¨¡å‹å’Œä¸‹ä¸€ä¸ªtokené¢„æµ‹åŸºçº¿æé«˜äº†8å’Œ4ä¸ªç‚¹ã€‚æ­¤å¤–ï¼ŒRA3åœ¨HumanEval+ã€MBPP+ã€LiveCodeBenchå’ŒCodeforcesä¸Šçš„RLVRä¸­å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´é«˜çš„æ¸è¿‘æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ—¶ï¼Œé€šå¸¸ç›´æ¥åœ¨åŸå§‹åŠ¨ä½œç©ºé—´è¿›è¡Œæ“ä½œï¼Œå¯¼è‡´å†³ç­–ç©ºé—´åºå¤§ã€æ¢ç´¢æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥å……åˆ†å‘æŒ¥å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¯¹ä¸­æœŸè®­ç»ƒé˜¶æ®µçš„ç†è®ºæŒ‡å¯¼ï¼Œéš¾ä»¥è®¾è®¡æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¨ç†è¿‡ç¨‹è§†ä¸ºä¸€ç³»åˆ—åŠ¨ä½œæŠ½è±¡ï¼Œé€šè¿‡ä¸­æœŸè®­ç»ƒå­¦ä¹ ä¸€ä¸ªç´§å‡‘çš„åŠ¨ä½œå­ç©ºé—´ï¼Œä»è€Œé™ä½å†³ç­–å¤æ‚åº¦ï¼Œæé«˜å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæ­ç¤ºäº†å‰ªææ•ˆç‡å’ŒRLæ”¶æ•›å¯¹ä¸­æœŸè®­ç»ƒæ•ˆæœçš„å…³é”®å½±å“ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†RA3ç®—æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRA3ç®—æ³•åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šåŠ¨ä½œæŠ½è±¡å‘ç°å’Œç­–ç•¥ä¼˜åŒ–ã€‚é¦–å…ˆï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿­ä»£åœ°å‘ç°æ—¶é—´ä¸€è‡´çš„æ½œåœ¨ç»“æ„ï¼Œæ„å»ºåŠ¨ä½œæŠ½è±¡ç©ºé—´ã€‚ç„¶åï¼Œåœ¨å¼•å¯¼æ•°æ®ä¸Šå¯¹ç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä¼˜åŒ–åŠ¨ä½œé€‰æ‹©ç­–ç•¥ã€‚æ•´ä½“æµç¨‹å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªåºåˆ—å˜åˆ†ä¸‹ç•Œçš„ä¼˜åŒ–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šRA3çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºåŠ¨ä½œæŠ½è±¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„ä¸­æœŸè®­ç»ƒç®—æ³•æ¥å­¦ä¹ è¿™äº›æŠ½è±¡ã€‚ä¸ç›´æ¥åœ¨åŸå§‹åŠ¨ä½œç©ºé—´è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ç›¸æ¯”ï¼ŒRA3èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢å’Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†å…³äºä¸­æœŸè®­ç»ƒçš„ç†è®ºåˆ†æï¼Œä¸ºç®—æ³•è®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šRA3ç®—æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥å‘ç°æ—¶é—´ä¸€è‡´çš„æ½œåœ¨ç»“æ„ï¼Œç¡®ä¿åŠ¨ä½œæŠ½è±¡çš„æ—¶åºä¸€è‡´æ€§ï¼›2) é€šè¿‡åºåˆ—å˜åˆ†ä¸‹ç•Œæ¥æŒ‡å¯¼è®­ç»ƒè¿‡ç¨‹ï¼Œå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼›3) åœ¨å¼•å¯¼æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RA3ç®—æ³•åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨HumanEvalå’ŒMBPPæ•°æ®é›†ä¸Šï¼ŒRA3çš„å¹³å‡æ€§èƒ½æ¯”åŸºç¡€æ¨¡å‹å’Œä¸‹ä¸€ä¸ªtokené¢„æµ‹åŸºçº¿åˆ†åˆ«æé«˜äº†8å’Œ4ä¸ªç‚¹ã€‚æ­¤å¤–ï¼ŒRA3åœ¨HumanEval+ã€MBPP+ã€LiveCodeBenchå’ŒCodeforcesç­‰æ•°æ®é›†ä¸Šçš„RLVRå®éªŒä¸­ï¼Œå®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´é«˜çš„æ¸è¿‘æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RA3ç®—æ³•å¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†å’Œå†³ç­–çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ä»£ç ç”Ÿæˆã€æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ç­‰ã€‚é€šè¿‡å­¦ä¹ æœ‰æ•ˆçš„åŠ¨ä½œæŠ½è±¡ï¼ŒRA3èƒ½å¤Ÿæé«˜å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œä»è€Œè§£å†³æ›´å¤æ‚çš„é—®é¢˜ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶ä¸ºå¼€å‘æ›´æ™ºèƒ½çš„AIç³»ç»Ÿæä¾›æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.

