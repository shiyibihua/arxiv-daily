---
layout: default
title: Less is More: Towards Simple Graph Contrastive Learning
---

# Less is More: Towards Simple Graph Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25742" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25742v2</a>
  <a href="https://arxiv.org/pdf/2509.25742.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25742v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25742v2', 'Less is More: Towards Simple Graph Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanan Zhao, Feng Ji, Jingyang Dai, Jiaze Ma, Wee Peng Tay

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-12-01)

**å¤‡æ³¨**: Submitted to ICLR 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§ç®€åŒ–çš„å›¾å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³å¼‚è´¨å›¾ä¸Šçš„è¡¨ç¤ºå­¦ä¹ é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å›¾å¯¹æ¯”å­¦ä¹ ` `å¼‚è´¨å›¾` `æ— ç›‘ç£å­¦ä¹ ` `å›¾ç¥ç»ç½‘ç»œ` `èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨å¼‚è´¨å›¾ä¸Šè¡¨ç°ä¸ä½³ï¼Œé€šå¸¸ä¾èµ–å¤æ‚çš„æ•°æ®å¢å¼ºå’Œæ¨¡å‹è®¾è®¡ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡èšåˆèŠ‚ç‚¹ç‰¹å¾å’Œç»“æ„ç‰¹å¾æ¥å‡è½»å™ªå£°ï¼Œåˆ©ç”¨äº’è¡¥è§†è§’è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚
3. æå‡ºçš„ç®€åŒ–æ¨¡å‹æ— éœ€æ•°æ®å¢å¼ºå’Œè´Ÿé‡‡æ ·ï¼Œåœ¨å¼‚è´¨å›¾ä¸Šè¾¾åˆ°SOTAï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾å¯¹æ¯”å­¦ä¹ (GCL)åœ¨æ— ç›‘ç£å›¾è¡¨ç¤ºå­¦ä¹ ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†å…¶åœ¨å¼‚è´¨å›¾ä¸Šçš„æœ‰æ•ˆæ€§ä»ç„¶æœ‰é™ï¼Œå¼‚è´¨å›¾ä¸­ç›¸è¿çš„èŠ‚ç‚¹é€šå¸¸å±äºä¸åŒçš„ç±»åˆ«ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºå¤æ‚çš„å¢å¼ºæ–¹æ¡ˆã€å¤æ‚çš„ç¼–ç å™¨æˆ–è´Ÿé‡‡æ ·ï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼Œå³åœ¨è¿™ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­ï¼Œè¿™ç§å¤æ‚æ€§æ˜¯å¦çœŸæ­£å¿…è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†å›¾ä¸Šçš„ç›‘ç£å’Œæ— ç›‘ç£å­¦ä¹ çš„åŸºç¡€ï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„GCLåŸåˆ™ï¼šé€šè¿‡å°†èŠ‚ç‚¹ç‰¹å¾å™ªå£°ä¸æ¥è‡ªå›¾æ‹“æ‰‘çš„ç»“æ„ç‰¹å¾èšåˆæ¥å‡è½»èŠ‚ç‚¹ç‰¹å¾å™ªå£°ã€‚è¿™ä¸€è§‚å¯Ÿè¡¨æ˜ï¼ŒåŸå§‹èŠ‚ç‚¹ç‰¹å¾å’Œå›¾ç»“æ„è‡ªç„¶åœ°ä¸ºå¯¹æ¯”å­¦ä¹ æä¾›äº†ä¸¤ä¸ªäº’è¡¥çš„è§†è§’ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªéå¸¸ç®€å•çš„GCLæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨GCNç¼–ç å™¨æ¥æ•è·ç»“æ„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨MLPç¼–ç å™¨æ¥éš”ç¦»èŠ‚ç‚¹ç‰¹å¾å™ªå£°ã€‚æˆ‘ä»¬çš„è®¾è®¡æ—¢ä¸éœ€è¦æ•°æ®å¢å¼ºï¼Œä¹Ÿä¸éœ€è¦è´Ÿé‡‡æ ·ï¼Œä½†ä»¥æœ€å°çš„è®¡ç®—å’Œå†…å­˜å¼€é”€åœ¨å¼‚è´¨åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒæ—¶åœ¨å¤æ‚æ€§ã€å¯æ‰©å±•æ€§å’Œé²æ£’æ€§æ–¹é¢ä¹Ÿä¸ºåŒè´¨å›¾æä¾›äº†ä¼˜åŠ¿ã€‚æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ç†è®ºä¸Šçš„è®ºè¯ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬é’ˆå¯¹é»‘ç›’å’Œç™½ç›’å¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§è¯„ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å›¾å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨å¼‚è´¨å›¾ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸»è¦åŸå› æ˜¯å¼‚è´¨å›¾çš„è¿æ¥èŠ‚ç‚¹å¯èƒ½å±äºä¸åŒç±»åˆ«ï¼Œå¯¼è‡´èŠ‚ç‚¹ç‰¹å¾å™ªå£°è¾ƒå¤§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¤æ‚çš„æ•°æ®å¢å¼ºç­–ç•¥æˆ–å¤æ‚çš„æ¨¡å‹ç»“æ„æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ï¼Œä¸”æ•ˆæœæå‡æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å›¾çš„ç»“æ„ä¿¡æ¯æ¥ç¼“è§£èŠ‚ç‚¹ç‰¹å¾çš„å™ªå£°ã€‚ä½œè€…è®¤ä¸ºï¼ŒåŸå§‹èŠ‚ç‚¹ç‰¹å¾å’Œå›¾ç»“æ„æä¾›äº†äº’è¡¥çš„è§†è§’ï¼Œé€šè¿‡å°†èŠ‚ç‚¹ç‰¹å¾ä¸ç»“æ„ç‰¹å¾èšåˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚è¿™ç§æ€è·¯é¿å…äº†å¤æ‚çš„æ•°æ®å¢å¼ºå’Œè´Ÿé‡‡æ ·ï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºçš„GCLæ¨¡å‹ä¸»è¦åŒ…å«ä¸¤ä¸ªç¼–ç å™¨ï¼šä¸€ä¸ªGCNç¼–ç å™¨å’Œä¸€ä¸ªMLPç¼–ç å™¨ã€‚GCNç¼–ç å™¨ç”¨äºæ•è·å›¾çš„ç»“æ„ç‰¹å¾ï¼ŒMLPç¼–ç å™¨ç”¨äºéš”ç¦»èŠ‚ç‚¹ç‰¹å¾çš„å™ªå£°ã€‚æ¨¡å‹é¦–å…ˆä½¿ç”¨GCNç¼–ç å™¨å¯¹å›¾è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°èŠ‚ç‚¹çš„ç»“æ„è¡¨ç¤ºï¼›ç„¶åï¼Œä½¿ç”¨MLPç¼–ç å™¨å¯¹åŸå§‹èŠ‚ç‚¹ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°èŠ‚ç‚¹çš„ç‰¹å¾è¡¨ç¤ºï¼›æœ€åï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œä½¿ç»“æ„è¡¨ç¤ºå’Œç‰¹å¾è¡¨ç¤ºå°½å¯èƒ½ä¸€è‡´ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªéå¸¸ç®€å•çš„å›¾å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸éœ€è¦æ•°æ®å¢å¼ºå’Œè´Ÿé‡‡æ ·ï¼Œä½†ä»ç„¶å¯ä»¥åœ¨å¼‚è´¨å›¾ä¸Šå–å¾—å¾ˆå¥½çš„æ•ˆæœã€‚è¿™ç§ç®€åŒ–çš„è®¾è®¡é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹ä½¿ç”¨GCNä½œä¸ºç»“æ„ç¼–ç å™¨ï¼ŒMLPä½œä¸ºç‰¹å¾ç¼–ç å™¨ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨æœ€å¤§åŒ–åŒä¸€èŠ‚ç‚¹çš„ç»“æ„è¡¨ç¤ºå’Œç‰¹å¾è¡¨ç¤ºä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä½œè€…æ²¡æœ‰ä½¿ç”¨ä»»ä½•å¤æ‚çš„æ•°æ®å¢å¼ºç­–ç•¥æˆ–è´Ÿé‡‡æ ·æ–¹æ³•ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨åŸå§‹èŠ‚ç‚¹ç‰¹å¾å’Œå›¾ç»“æ„è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨å¤šä¸ªå¼‚è´¨å›¾åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„ç»“æœï¼Œå¹¶ä¸”åœ¨åŒè´¨å›¾ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ã€å†…å­˜å ç”¨å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½æå‡çš„åŒæ—¶ï¼Œè®¡ç®—æ—¶é—´å‡å°‘äº†50%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç¤¾äº¤ç½‘ç»œåˆ†æã€ç”Ÿç‰©ä¿¡æ¯å­¦ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œå›¾ç»“æ„é€šå¸¸åŒ…å«ä¸°å¯Œçš„å…³ç³»ä¿¡æ¯ï¼Œä½†èŠ‚ç‚¹ç‰¹å¾å¯èƒ½å­˜åœ¨å™ªå£°æˆ–ç¼ºå¤±ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å›¾ç»“æ„ä¿¡æ¯æ¥æé«˜èŠ‚ç‚¹è¡¨ç¤ºçš„è´¨é‡ï¼Œä»è€Œæå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ï¼Œå¯ä»¥åº”ç”¨äºå¤§è§„æ¨¡å›¾æ•°æ®å’Œå¯¹æŠ—æ”»å‡»åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Graph Contrastive Learning (GCL) has shown strong promise for unsupervised graph representation learning, yet its effectiveness on heterophilic graphs, where connected nodes often belong to different classes, remains limited. Most existing methods rely on complex augmentation schemes, intricate encoders, or negative sampling, which raises the question of whether such complexity is truly necessary in this challenging setting. In this work, we revisit the foundations of supervised and unsupervised learning on graphs and uncover a simple yet effective principle for GCL: mitigating node feature noise by aggregating it with structural features derived from the graph topology. This observation suggests that the original node features and the graph structure naturally provide two complementary views for contrastive learning. Building on this insight, we propose an embarrassingly simple GCL model that uses a GCN encoder to capture structural features and an MLP encoder to isolate node feature noise. Our design requires neither data augmentation nor negative sampling, yet achieves state-of-the-art results on heterophilic benchmarks with minimal computational and memory overhead, while also offering advantages in homophilic graphs in terms of complexity, scalability, and robustness. We provide theoretical justification for our approach and validate its effectiveness through extensive experiments, including robustness evaluations against both black-box and white-box adversarial attacks.

