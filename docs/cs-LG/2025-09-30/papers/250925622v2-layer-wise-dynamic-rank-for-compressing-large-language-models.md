---
layout: default
title: Layer-wise dynamic rank for compressing large language models
---

# Layer-wise dynamic rank for compressing large language models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25622" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25622v2</a>
  <a href="https://arxiv.org/pdf/2509.25622.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25622v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25622v2', 'Layer-wise dynamic rank for compressing large language models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhendong Mi, Bian Sun, Grace Li Zhang, Shaoyi Huang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-04)

**å¤‡æ³¨**: 10 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºD-Rankï¼šä¸€ç§å±‚çº§åŠ¨æ€ç§©åˆ†é…æ¡†æ¶ï¼Œç”¨äºå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹å‹ç¼©` `å¥‡å¼‚å€¼åˆ†è§£` `åŠ¨æ€ç§©åˆ†é…` `æ¨¡å‹ä¼˜åŒ–` `åè®­ç»ƒå‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰SVDå‹ç¼©æ–¹æ³•å¯¹LLMå„å±‚é‡‡ç”¨ç»Ÿä¸€å‹ç¼©ç‡ï¼Œå¿½ç•¥äº†å±‚é—´ä¿¡æ¯å¯†åº¦å·®å¼‚ï¼Œå¯¼è‡´å‹ç¼©æ•ˆç‡å—é™ã€‚
2. D-Ranké€šè¿‡æœ‰æ•ˆç§©åº¦é‡ä¿¡æ¯å¯†åº¦ï¼Œå¹¶åŸºäºæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ä¼˜åŒ–ï¼ŒåŠ¨æ€åˆ†é…å„å±‚å‹ç¼©ç§©ï¼Œæå‡å‹ç¼©æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒD-Rankåœ¨å¤šç§LLMä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½å›°æƒ‘åº¦ï¼Œæé«˜é›¶æ ·æœ¬æ¨ç†ç²¾åº¦å’Œååé‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„è§„æ¨¡è¿…é€Ÿæ‰©å¤§ï¼Œå¸¦æ¥äº†ä¸¥é‡çš„å†…å­˜å’Œè®¡ç®—æŒ‘æˆ˜ï¼Œé˜»ç¢äº†å®ƒä»¬çš„éƒ¨ç½²ã€‚åŸºäºå¥‡å¼‚å€¼åˆ†è§£(SVD)çš„å‹ç¼©å·²æˆä¸ºä¸€ç§æœ‰å¸å¼•åŠ›çš„LLMåè®­ç»ƒå‹ç¼©æŠ€æœ¯ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•åœ¨æ‰€æœ‰å±‚ä¸Šåº”ç”¨ç»Ÿä¸€çš„å‹ç¼©ç‡ï¼Œéšå«åœ°å‡è®¾ä¸åŒå±‚ä¸­åŒ…å«åŒè´¨ä¿¡æ¯ã€‚è¿™å¿½ç•¥äº†LLMä¸­è§‚å¯Ÿåˆ°çš„æ˜¾è‘—çš„å±‚å†…å¼‚è´¨æ€§ï¼Œå…¶ä¸­ä¸­é—´å±‚å€¾å‘äºç¼–ç æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œè€Œæ—©æœŸå’ŒåæœŸå±‚åˆ™æ›´åŠ å†—ä½™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†ç°æœ‰çš„åŸºäºSVDçš„å‹ç¼©æ–¹æ³•ï¼Œå¹¶æå‡ºäº†D-Rankï¼Œä¸€ä¸ªå…·æœ‰å±‚çº§å¹³è¡¡åŠ¨æ€ç§©åˆ†é…çš„LLMå‹ç¼©æ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥æœ‰æ•ˆç§©ä½œä¸ºä¸€ç§åŸåˆ™æ€§åº¦é‡æ¥è¡¡é‡æƒé‡çŸ©é˜µçš„ä¿¡æ¯å¯†åº¦ï¼Œç„¶åé€šè¿‡åŸºäºæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°çš„ä¼˜åŒ–æ–¹æ¡ˆæ¥åˆ†é…ç§©ï¼Œä»¥ä¾¿åœ¨å›ºå®šçš„å‹ç¼©ç‡ä¸‹è‡ªé€‚åº”åœ°ä¸ºå…·æœ‰æ›´é«˜ä¿¡æ¯å¯†åº¦çš„ç»„åˆ†é…æ›´å¤šå®¹é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡æ–°å¹³è¡¡æ³¨æ„åŠ›å±‚ä¹‹é—´åˆ†é…çš„ç§©ï¼Œä»¥è€ƒè™‘å®ƒä»¬ä¸åŒçš„é‡è¦æ€§ï¼Œå¹¶å°†D-Rankæ‰©å±•åˆ°å…·æœ‰åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›çš„æœ€æ–°LLMã€‚åœ¨å…·æœ‰ä¸åŒè§„æ¨¡çš„å„ç§LLMä¸Šï¼Œè·¨å¤šä¸ªå‹ç¼©ç‡è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒD-Rankå§‹ç»ˆä¼˜äºSVD-LLMã€ASVDå’ŒBasis Sharingï¼Œåœ¨C4æ•°æ®é›†ä¸Šä»¥20%çš„å‹ç¼©ç‡ä½¿ç”¨LLaMA-3-8Bæ¨¡å‹å®ç°äº†è¶…è¿‡15çš„æ›´ä½å›°æƒ‘åº¦ï¼Œå¹¶ä¸”åœ¨40%çš„å‹ç¼©ç‡ä¸‹ä½¿ç”¨LLaMA-7Bæ¨¡å‹å®ç°äº†é«˜è¾¾5%çš„æ›´é«˜çš„é›¶æ ·æœ¬æ¨ç†ç²¾åº¦ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„ååé‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºSVDçš„LLMå‹ç¼©æ–¹æ³•é€šå¸¸é‡‡ç”¨ç»Ÿä¸€çš„å‹ç¼©ç‡ï¼Œå¿½ç•¥äº†LLMä¸åŒå±‚ä¹‹é—´ä¿¡æ¯å¯†åº¦çš„å·®å¼‚ã€‚è¿™ç§å‡åŒ€å‹ç¼©ç­–ç•¥å¯¼è‡´ä¿¡æ¯å¯†åº¦é«˜çš„å±‚å‹ç¼©ä¸è¶³ï¼Œè€Œä¿¡æ¯å†—ä½™çš„å±‚è¿‡åº¦å‹ç¼©ï¼Œä»è€Œå½±å“æ•´ä½“æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨LLMå„å±‚ä¹‹é—´çš„å¼‚è´¨æ€§ï¼Œå¯¼è‡´å‹ç¼©æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šD-Rankçš„æ ¸å¿ƒæ€è·¯æ˜¯æ ¹æ®LLMå„å±‚çš„ä¿¡æ¯å¯†åº¦åŠ¨æ€åœ°åˆ†é…å‹ç¼©ç§©ã€‚é€šè¿‡å¼•å…¥â€œæœ‰æ•ˆç§©â€è¿™ä¸€æŒ‡æ ‡æ¥è¡¡é‡æƒé‡çŸ©é˜µçš„ä¿¡æ¯å¯†åº¦ï¼Œå¹¶åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ä¼˜åŒ–ç®—æ³•ï¼Œåœ¨æ»¡è¶³æ•´ä½“å‹ç¼©ç‡çº¦æŸçš„å‰æä¸‹ï¼Œä¸ºä¿¡æ¯å¯†åº¦æ›´é«˜çš„å±‚åˆ†é…æ›´å¤šçš„ç§©ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°ä¿ç•™æ¨¡å‹çš„é‡è¦ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šD-Rankæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **æœ‰æ•ˆç§©è®¡ç®—**ï¼šè®¡ç®—LLMæ¯ä¸€å±‚æƒé‡çŸ©é˜µçš„æœ‰æ•ˆç§©ï¼Œä½œä¸ºä¿¡æ¯å¯†åº¦çš„åº¦é‡ã€‚2) **ç§©åˆ†é…ä¼˜åŒ–**ï¼šåˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ï¼Œåœ¨ç»™å®šæ•´ä½“å‹ç¼©ç‡çº¦æŸä¸‹ï¼Œä¼˜åŒ–å„å±‚çš„ç§©åˆ†é…æ–¹æ¡ˆï¼Œä½¿å¾—ä¿¡æ¯å¯†åº¦é«˜çš„å±‚è·å¾—æ›´å¤šçš„ç§©ã€‚3) **ç§©é‡å¹³è¡¡**ï¼šé’ˆå¯¹æ³¨æ„åŠ›å±‚ï¼Œæ ¹æ®å…¶é‡è¦æ€§é‡æ–°å¹³è¡¡åˆ†é…çš„ç§©ã€‚4) **SVDå‹ç¼©**ï¼šæ ¹æ®ä¼˜åŒ–åçš„ç§©åˆ†é…æ–¹æ¡ˆï¼Œå¯¹æ¯ä¸€å±‚è¿›è¡ŒSVDåˆ†è§£å’Œå‹ç¼©ã€‚

**å…³é”®åˆ›æ–°**ï¼šD-Rankæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å±‚çº§åŠ¨æ€ç§©åˆ†é…çš„æ€æƒ³ï¼Œå¹¶å°†å…¶åº”ç”¨äºLLMçš„å‹ç¼©ã€‚ä¸ç°æœ‰æ–¹æ³•é‡‡ç”¨çš„ç»Ÿä¸€å‹ç¼©ç‡ä¸åŒï¼ŒD-Rankèƒ½å¤Ÿæ ¹æ®å„å±‚çš„ä¿¡æ¯å¯†åº¦è‡ªé€‚åº”åœ°è°ƒæ•´å‹ç¼©ç‡ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°ä¿ç•™æ¨¡å‹çš„é‡è¦ä¿¡æ¯ï¼Œæé«˜å‹ç¼©æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒD-Rankè¿˜è€ƒè™‘äº†æ³¨æ„åŠ›å±‚çš„é‡è¦æ€§å·®å¼‚ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†ç§©é‡å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šD-Rankçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **æœ‰æ•ˆç§©çš„è®¡ç®—æ–¹æ³•**ï¼šè®ºæ–‡ä¸­å…·ä½“æè¿°äº†æœ‰æ•ˆç§©çš„è®¡ç®—å…¬å¼ï¼Œè¯¥å…¬å¼èƒ½å¤Ÿæœ‰æ•ˆåœ°åæ˜ æƒé‡çŸ©é˜µçš„ä¿¡æ¯å¯†åº¦ã€‚2) **æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ä¼˜åŒ–**ï¼šè®ºæ–‡è¯¦ç»†ä»‹ç»äº†å¦‚ä½•åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•æ±‚è§£ç§©åˆ†é…ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶ç»™å‡ºäº†å…·ä½“çš„ä¼˜åŒ–ç®—æ³•ã€‚3) **æ³¨æ„åŠ›å±‚ç§©é‡å¹³è¡¡ç­–ç•¥**ï¼šè®ºæ–‡æå‡ºäº†é’ˆå¯¹æ³¨æ„åŠ›å±‚çš„ç§©é‡å¹³è¡¡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿæ ¹æ®æ³¨æ„åŠ›å±‚çš„é‡è¦æ€§è°ƒæ•´å…¶ç§©åˆ†é…ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒD-Rankåœ¨å„ç§LLMä¸Šå‡ä¼˜äºç°æœ‰å‹ç¼©æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨C4æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨LLaMA-3-8Bæ¨¡å‹ï¼ŒD-Rankåœ¨20%çš„å‹ç¼©ç‡ä¸‹å®ç°äº†è¶…è¿‡15çš„æ›´ä½å›°æƒ‘åº¦ã€‚åœ¨ä½¿ç”¨LLaMA-7Bæ¨¡å‹æ—¶ï¼ŒD-Rankåœ¨40%çš„å‹ç¼©ç‡ä¸‹å®ç°äº†é«˜è¾¾5%çš„æ›´é«˜çš„é›¶æ ·æœ¬æ¨ç†ç²¾åº¦ã€‚æ­¤å¤–ï¼ŒD-Rankè¿˜èƒ½å¤Ÿæé«˜æ¨¡å‹çš„ååé‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

D-RankæŠ€æœ¯å¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—è®¾å¤‡å’Œèµ„æºå—é™çš„æœåŠ¡å™¨ã€‚é€šè¿‡æœ‰æ•ˆå‹ç¼©æ¨¡å‹å¤§å°ï¼ŒD-Rankèƒ½å¤Ÿé™ä½éƒ¨ç½²æˆæœ¬ï¼Œæé«˜æ¨ç†é€Ÿåº¦ï¼Œå¹¶ä½¿LLMèƒ½å¤Ÿåœ¨æ›´å¤šå¹³å°ä¸Šè¿è¡Œã€‚è¯¥æŠ€æœ¯å¯¹äºæ¨åŠ¨LLMåœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠå…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have rapidly scaled in size, bringing severe memory and computational challenges that hinder their deployment. Singular Value Decomposition (SVD)-based compression has emerged as an appealing post-training compression technique for LLMs, yet most existing methods apply a uniform compression ratio across all layers, implicitly assuming homogeneous information included in various layers. This overlooks the substantial intra-layer heterogeneity observed in LLMs, where middle layers tend to encode richer information while early and late layers are more redundant. In this work, we revisit the existing SVD-based compression method and propose D-Rank, a framework with layer-wise balanced Dynamic Rank allocation for LLMs compression. We first introduce effective rank as a principled metric to measure the information density of weight matrices, and then allocate ranks via a Lagrange multiplier-based optimization scheme to adaptively assign more capacity to groups with higher information density under a fixed compression ratio. Moreover, we rebalance the allocated ranks across attention layers to account for their varying importance and extend D-Rank to latest LLMs with grouped-query attention. Extensive experiments on various LLMs with different scales across multiple compression ratios demonstrate that D-Rank consistently outperforms SVD-LLM, ASVD, and Basis Sharing, achieving more than 15 lower perplexity with LLaMA-3-8B model on C4 datasets at 20% compression ratio and up to 5% higher zero-shot reasoning accuracy with LLaMA-7B model at 40% compression ratio while achieving even higher throughput.

