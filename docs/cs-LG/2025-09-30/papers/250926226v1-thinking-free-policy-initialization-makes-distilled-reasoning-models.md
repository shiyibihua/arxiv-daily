---
layout: default
title: Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners
---

# Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26226" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26226v1</a>
  <a href="https://arxiv.org/pdf/2509.26226.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26226v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26226v1', 'Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin Xu, Cliveb AI, Kai Yang, Tianhao Chen, Yang Wang, Saiyong Yang, Can Yang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTFPIï¼ŒåŠ é€ŸRLVRè®­ç»ƒï¼Œæå‡æ¨ç†æ¨¡å‹æ•ˆç‡ä¸æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `é“¾å¼æ€ç»´` `ç­–ç•¥åˆå§‹åŒ–` `æ¨¡å‹è’¸é¦` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. RLVRè§£å†³å¤æ‚ä»»åŠ¡æœ‰æ•ˆï¼Œä½†è®­ç»ƒæ—¶ä¸Šä¸‹æ–‡é•¿åº¦è¦æ±‚é«˜ï¼Œè®¡ç®—æˆæœ¬å·¨å¤§ã€‚
2. TFPIé€šè¿‡*ThinkFree*æ“ä½œæ˜¾å¼ä¸¢å¼ƒæ€ç»´å†…å®¹ï¼Œå‡å°‘æ¨ç†tokenä½¿ç”¨ï¼Œè¿æ¥CoTè’¸é¦å’ŒRLVRã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTFPIåŠ é€ŸRLæ”¶æ•›ï¼Œæå‡æ€§èƒ½ä¸Šé™ï¼Œå¹¶äº§ç”Ÿæ›´èŠ‚çœtokençš„æ¨ç†æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)èƒ½å¤Ÿæœ‰æ•ˆè§£å†³å¤æ‚ä»»åŠ¡ï¼Œä½†è®­ç»ƒæ—¶éœ€è¦æé•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¯¼è‡´å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚å¤šé˜¶æ®µè®­ç»ƒè™½ç„¶å¯ä»¥éƒ¨åˆ†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†ä»è¿‡çŸ­çš„ä¸Šä¸‹æ–‡å¼€å§‹è®­ç»ƒå¾€å¾€ä¼šå¯¼è‡´ä¸å¯é€†è½¬çš„æ€§èƒ½ä¸‹é™ï¼Œæœ€ç»ˆæ— æ³•æ˜¾è‘—é™ä½æ•´ä½“è®­ç»ƒè®¡ç®—é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„RLVRæ”¹è¿›æ–¹æ³•â€”â€”æ— æ€ç»´ç­–ç•¥åˆå§‹åŒ–(TFPI)ï¼Œå®ƒè¿æ¥äº†é•¿é“¾å¼æ€ç»´(CoT)è’¸é¦å’Œæ ‡å‡†RLVRã€‚TFPIé‡‡ç”¨ç®€å•çš„*ThinkFree*æ“ä½œï¼Œé€šè¿‡ç›´æ¥æ·»åŠ *</think>*æ¥æ˜¾å¼ä¸¢å¼ƒæ€ç»´å†…å®¹ï¼Œä»è€Œå‡å°‘æ¨ç†æœŸé—´çš„tokenä½¿ç”¨é‡ã€‚ä½¿ç”¨*ThinkFree*è°ƒæ•´åçš„è¾“å…¥è¿›è¡Œè®­ç»ƒå¯ä»¥æé«˜æ€§èƒ½å¹¶é™ä½tokenæ¶ˆè€—ï¼Œå³ä½¿åœ¨åŸå§‹çš„æ…¢é€Ÿæ€ç»´æ¨¡å¼ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTFPIåŠ é€Ÿäº†RLæ”¶æ•›ï¼Œå®ç°äº†æ›´é«˜çš„æ€§èƒ½ä¸Šé™ï¼Œå¹¶äº§ç”Ÿäº†æ›´èŠ‚çœtokençš„æ¨ç†æ¨¡å‹ï¼Œè€Œæ— éœ€ä¸“é—¨çš„å¥–åŠ±æˆ–å¤æ‚çš„è®­ç»ƒè®¾è®¡ã€‚ä»…ä½¿ç”¨TFPIï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ª4Bæ¨¡å‹ï¼Œä½¿ç”¨ä¸åˆ°4K H20å°æ—¶ï¼Œåœ¨AIME24ä¸Šè¾¾åˆ°89.0%çš„å‡†ç¡®ç‡ï¼Œåœ¨LiveCodeBenchä¸Šè¾¾åˆ°65.5%çš„å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šRLVRåœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ï¼Œéœ€è¦æé•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¯¼è‡´è®­ç»ƒè®¡ç®—æˆæœ¬éå¸¸é«˜æ˜‚ã€‚è™½ç„¶å¤šé˜¶æ®µè®­ç»ƒå¯ä»¥ç¼“è§£ï¼Œä½†å¦‚æœåˆå§‹é˜¶æ®µä½¿ç”¨è¿‡çŸ­çš„ä¸Šä¸‹æ–‡ï¼Œä¼šå¯¼è‡´æ€§èƒ½ä¸å¯é€†è½¬çš„ä¸‹é™ï¼Œæ— æ³•æœ‰æ•ˆé™ä½æ•´ä½“è®­ç»ƒæˆæœ¬ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTFPIçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨è®­ç»ƒåˆæœŸï¼Œé€šè¿‡å¼•å…¥*ThinkFree*æ“ä½œï¼Œæ˜¾å¼åœ°æˆªæ–­æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼Œå‡å°‘tokençš„ä½¿ç”¨ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è¾ƒçŸ­çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹è¿›è¡Œæœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ ï¼Œé¿å…äº†å› ä¸Šä¸‹æ–‡è¿‡çŸ­å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚é€šè¿‡é€æ­¥å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæœ€ç»ˆå®ç°é«˜æ€§èƒ½å’Œé«˜æ•ˆç‡çš„æ¨ç†æ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTFPIæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨CoTæ•°æ®è¿›è¡Œè’¸é¦è®­ç»ƒï¼Œåˆå§‹åŒ–æ¨¡å‹ï¼›2) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥*ThinkFree*æ“ä½œï¼Œå³åœ¨è¾“å…¥åºåˆ—ä¸­ç›´æ¥æ·»åŠ *</think>*æ ‡ç­¾ï¼Œå¼ºåˆ¶æ¨¡å‹åœæ­¢æ€è€ƒï¼Œç›´æ¥è¾“å‡ºç­”æ¡ˆï¼›3) ä½¿ç”¨RLVRè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä¼˜åŒ–æ¨¡å‹ç­–ç•¥ã€‚æ•´ä¸ªæ¡†æ¶æ—¨åœ¨å¹³è¡¡è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šTFPIçš„å…³é”®åˆ›æ–°åœ¨äº*ThinkFree*æ“ä½œçš„å¼•å…¥ã€‚å®ƒå…è®¸æ¨¡å‹åœ¨è®­ç»ƒåˆæœŸå¿«é€Ÿå­¦ä¹ ç­–ç•¥ï¼Œé¿å…äº†å› ä¸Šä¸‹æ–‡é•¿åº¦ä¸è¶³å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚ä¸ä¼ ç»Ÿçš„RLVRæ–¹æ³•ç›¸æ¯”ï¼ŒTFPIèƒ½å¤Ÿæ˜¾è‘—é™ä½è®­ç»ƒæ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œå¹¶æé«˜æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼š*ThinkFree*æ“ä½œçš„å…·ä½“å®ç°æ˜¯åœ¨è¾“å…¥åºåˆ—ä¸­æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„token *</think>*ï¼Œè¯¥tokenæŒ‡ç¤ºæ¨¡å‹åœæ­¢æ€è€ƒï¼Œç›´æ¥è¾“å‡ºç­”æ¡ˆã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥è°ƒæ•´*ThinkFree*æ“ä½œçš„é¢‘ç‡ï¼Œä»¥å¹³è¡¡è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚è®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®æåŠæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„ç‰¹æ®Šè®¾è®¡ï¼Œæ¨æµ‹æ²¿ç”¨äº†RLVRçš„å¸¸ç”¨è®¾ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨TFPIæ–¹æ³•è®­ç»ƒçš„4Bæ¨¡å‹ï¼Œåœ¨AIME24ä¸Šè¾¾åˆ°äº†89.0%çš„å‡†ç¡®ç‡ï¼Œåœ¨LiveCodeBenchä¸Šè¾¾åˆ°äº†65.5%çš„å‡†ç¡®ç‡ï¼Œè€Œè®­ç»ƒæ—¶é—´ä»…ä¸ºä¸åˆ°4K H20å°æ—¶ã€‚è¿™è¡¨æ˜TFPIèƒ½å¤Ÿæ˜¾è‘—åŠ é€ŸRLæ”¶æ•›ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚ä¸æ²¡æœ‰ä½¿ç”¨TFPIçš„åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½å’Œæ•ˆç‡å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TFPIæ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€çŸ¥è¯†å›¾è°±æ¨ç†ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé™ä½è®­ç»ƒæˆæœ¬ï¼Œæé«˜æ¨¡å‹æ¨ç†æ•ˆç‡ï¼Œä½¿å¾—å¤§è§„æ¨¡æ¨ç†æ¨¡å‹çš„éƒ¨ç½²å’Œåº”ç”¨æˆä¸ºå¯èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¼ºåŒ–å­¦ä¹ åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning with Verifiable Reward (RLVR) effectively solves complex tasks but demands extremely long context lengths during training, leading to substantial computational costs. While multi-stage training can partially mitigate this, starting with overly short contexts often causes irreversible performance degradation, ultimately failing to reduce overall training compute significantly. In this paper, we introduce **T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet effective adaptation to RLVR that bridges long Chain-of-Thought (CoT) distillation and standard RLVR. TFPI employs a simple *ThinkFree* operation, explicitly discarding the thinking content via a direct *</think>* append, to reduce token usage during inference. Training with *ThinkFree*-adapted inputs improves performance and lowers token consumption, even in the original slow-thinking mode. Extensive experiments across various benchmarks have shown that TFPI accelerates RL convergence, achieves a higher performance ceiling, and yields more token-efficient reasoning models without specialized rewards or complex training designs. With TFPI only, we train a 4B model to reach 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.

