---
layout: default
title: Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse
---

# Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25808" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25808v1</a>
  <a href="https://arxiv.org/pdf/2509.25808.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25808v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25808v1', 'Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuheng Zhang, Wenlin Yao, Changlong Yu, Yao Liu, Qingyu Yin, Bing Yin, Hyokun Yun, Lihong Li

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AR3POï¼šé€šè¿‡è‡ªé€‚åº”Rolloutå’Œå“åº”å¤ç”¨æå‡RLVRé‡‡æ ·æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `é‡‡æ ·æ•ˆç‡` `è‡ªé€‚åº”Rollout` `å“åº”å¤ç”¨` `RLVR` `ç­–ç•¥ä¼˜åŒ–` `åè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLVRæ–¹æ³•å¦‚GRPOåœ¨å“åº”ç»„å†…å¥–åŠ±ç›¸åŒæ—¶å­˜åœ¨ä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ï¼Œé™åˆ¶äº†è®­ç»ƒæ•ˆç‡ã€‚
2. AR3POé€šè¿‡è‡ªé€‚åº”RolloutåŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œå¹¶å¤ç”¨å·²æœ‰æ­£ç¡®å“åº”ï¼Œæå‡é‡‡æ ·æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAR3POåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºGRPOï¼Œå¹¶èƒ½ä»¥æ›´ä½çš„Rolloutæˆæœ¬åŒ¹é…æˆ–è¶…è¶ŠDAPOã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè€ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å·²æˆä¸ºåè®­ç»ƒçš„æ ‡å‡†èŒƒå¼ã€‚ä¸€ç§ä»£è¡¨æ€§ç®—æ³•ï¼Œå³ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé€šè¿‡å¯¹å“åº”ç»„å†…çš„ç»“æœå¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–æ¥è®¡ç®—ä¼˜åŠ¿ï¼Œä½†å½“ç»„å†…æ‰€æœ‰å“åº”è·å¾—ç›¸åŒå¥–åŠ±æ—¶ï¼Œä¼šé‡åˆ°ä¼˜åŠ¿æ¶ˆå¤±çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”Rolloutå’Œå“åº”å¤ç”¨ç­–ç•¥ä¼˜åŒ–ï¼ˆAR3POï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é‡‡æ ·é«˜æ•ˆçš„RLVRç®—æ³•ï¼Œå¼•å…¥äº†ä¸¤é¡¹æ–°æŠ€æœ¯ï¼šè‡ªé€‚åº”Rolloutï¼Œå®ƒåŠ¨æ€åœ°ä¸ºå›°éš¾çš„æç¤ºåˆ†é…æ›´å¤šå“åº”ï¼ŒåŒæ—¶èŠ‚çœç®€å•æç¤ºçš„è®¡ç®—ï¼›ä»¥åŠå“åº”å¤ç”¨ï¼Œå®ƒåˆ©ç”¨å…ˆå‰ç”Ÿæˆçš„æ­£ç¡®å“åº”æ¥æä¾›æœ‰ç”¨çš„è®­ç»ƒä¿¡å·ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„åŸºç¡€æ¨¡å‹ç³»åˆ—ï¼Œåœ¨å¤šä¸ªä»£è¡¨æ€§åŸºå‡†ä¸Šå°†AR3POä¸å¼ºå¤§çš„RLVRåŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚åœ¨7Bå’Œ8Bæ¨¡å‹ä¸Šï¼ŒAR3POå§‹ç»ˆä¼˜äºGRPOï¼Œå¹¶ä¸”è¾¾åˆ°æˆ–è¶…è¿‡DAPOçš„æ€§èƒ½ï¼ŒåŒæ—¶å°†Rolloutæˆæœ¬é™ä½é«˜è¾¾4.2å€ã€‚åœ¨æ›´å¤§çš„32Bæ¨¡å‹ä¸Šï¼ŒAR3POåœ¨ç›¸ä¼¼çš„è®­ç»ƒæ­¥éª¤ä¸­å®ç°äº†ä¸DAPOç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ˜¾è‘—æ›´ä½çš„Rolloutæˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³RLVRï¼ˆReinforcement Learning with Verifiable Rewardsï¼‰ä¸­é‡‡æ ·æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œç°æœ‰çš„GRPOç®—æ³•åœ¨å¤„ç†æ‰€æœ‰å“åº”è·å¾—ç›¸åŒå¥–åŠ±çš„promptæ—¶ï¼Œä¼šé¢ä¸´ä¼˜åŠ¿æ¶ˆå¤±çš„é—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡é™ä½ã€‚æ­¤å¤–ï¼Œå¯¹æ‰€æœ‰promptéƒ½é‡‡ç”¨ç›¸åŒçš„rolloutç­–ç•¥ï¼Œå¿½ç•¥äº†promptéš¾åº¦çš„å·®å¼‚ï¼Œé€ æˆäº†è®¡ç®—èµ„æºçš„æµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAR3POçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªé€‚åº”åœ°è°ƒæ•´rolloutç­–ç•¥å’Œå¤ç”¨å·²æœ‰çš„é«˜è´¨é‡å“åº”æ¥æé«˜é‡‡æ ·æ•ˆç‡ã€‚è‡ªé€‚åº”rolloutæ ¹æ®promptçš„éš¾åº¦åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œå¯¹å›°éš¾çš„promptè¿›è¡Œæ›´å¤šçš„é‡‡æ ·ï¼Œè€Œå¯¹ç®€å•çš„promptåˆ™å‡å°‘é‡‡æ ·ã€‚å“åº”å¤ç”¨åˆ™åˆ©ç”¨ä¹‹å‰ç”Ÿæˆçš„æ­£ç¡®å“åº”ä½œä¸ºè®­ç»ƒä¿¡å·ï¼Œé¿å…é‡å¤ç”Ÿæˆç›¸åŒçš„å“åº”ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAR3POçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰Promptéš¾åº¦è¯„ä¼°æ¨¡å—ï¼šç”¨äºè¯„ä¼°æ¯ä¸ªpromptçš„éš¾åº¦ï¼Œå¹¶æ ¹æ®éš¾åº¦åŠ¨æ€è°ƒæ•´rolloutçš„æ•°é‡ã€‚2ï¼‰è‡ªé€‚åº”Rolloutæ¨¡å—ï¼šæ ¹æ®promptéš¾åº¦è¯„ä¼°ç»“æœï¼ŒåŠ¨æ€åˆ†é…rolloutçš„æ•°é‡ï¼Œå¯¹å›°éš¾çš„promptè¿›è¡Œæ›´å¤šçš„é‡‡æ ·ã€‚3ï¼‰å“åº”å¤ç”¨æ¨¡å—ï¼šç»´æŠ¤ä¸€ä¸ªå“åº”æ± ï¼Œå­˜å‚¨ä¹‹å‰ç”Ÿæˆçš„æ­£ç¡®å“åº”ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤ç”¨è¿™äº›å“åº”ï¼Œé¿å…é‡å¤ç”Ÿæˆã€‚4ï¼‰ç­–ç•¥ä¼˜åŒ–æ¨¡å—ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚GRPOï¼‰å¯¹ç­–ç•¥è¿›è¡Œä¼˜åŒ–ï¼Œåˆ©ç”¨è‡ªé€‚åº”rolloutå’Œå“åº”å¤ç”¨æä¾›çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šAR3POçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªé€‚åº”rolloutå’Œå“åº”å¤ç”¨ä¸¤ç§æŠ€æœ¯ï¼Œè¿™ä¸¤ç§æŠ€æœ¯èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜RLVRçš„é‡‡æ ·æ•ˆç‡ã€‚è‡ªé€‚åº”rolloutèƒ½å¤Ÿæ ¹æ®promptçš„éš¾åº¦åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œé¿å…äº†å¯¹ç®€å•promptçš„è¿‡åº¦é‡‡æ ·ï¼Œä»è€ŒèŠ‚çœäº†è®¡ç®—èµ„æºã€‚å“åº”å¤ç”¨åˆ™èƒ½å¤Ÿåˆ©ç”¨ä¹‹å‰ç”Ÿæˆçš„æ­£ç¡®å“åº”ä½œä¸ºè®­ç»ƒä¿¡å·ï¼Œé¿å…äº†é‡å¤ç”Ÿæˆç›¸åŒçš„å“åº”ï¼Œä»è€Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è‡ªé€‚åº”rolloutä¸­ï¼Œå¯ä»¥ä½¿ç”¨å¤šç§æ–¹æ³•æ¥è¯„ä¼°promptçš„éš¾åº¦ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨æ¨¡å‹å¯¹promptçš„é¢„æµ‹ç½®ä¿¡åº¦ä½œä¸ºéš¾åº¦æŒ‡æ ‡ã€‚åœ¨å“åº”å¤ç”¨ä¸­ï¼Œéœ€è¦ç»´æŠ¤ä¸€ä¸ªå“åº”æ± ï¼Œå¹¶è®¾è®¡åˆé€‚çš„ç­–ç•¥æ¥é€‰æ‹©å“ªäº›å“åº”å¯ä»¥è¢«å¤ç”¨ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åªå¤ç”¨å¥–åŠ±æœ€é«˜çš„å“åº”ï¼Œæˆ–è€…å¯ä»¥æ ¹æ®å“åº”çš„å¤šæ ·æ€§æ¥é€‰æ‹©å¤ç”¨å“ªäº›å“åº”ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ä¸GRPOç­‰åŸºçº¿æ–¹æ³•ä¿æŒä¸€è‡´ï¼Œä¸»è¦å…³æ³¨äºé‡‡æ ·ç­–ç•¥çš„ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

AR3POåœ¨7Bå’Œ8Bæ¨¡å‹ä¸Šå§‹ç»ˆä¼˜äºGRPOï¼Œå¹¶è¾¾åˆ°æˆ–è¶…è¿‡DAPOçš„æ€§èƒ½ï¼ŒåŒæ—¶å°†Rolloutæˆæœ¬é™ä½é«˜è¾¾4.2å€ã€‚åœ¨æ›´å¤§çš„32Bæ¨¡å‹ä¸Šï¼ŒAR3POåœ¨ç›¸ä¼¼çš„è®­ç»ƒæ­¥éª¤ä¸­å®ç°äº†ä¸DAPOç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ˜¾è‘—æ›´ä½çš„Rolloutæˆæœ¬ã€‚è¿™äº›ç»“æœè¡¨æ˜AR3POåœ¨æé«˜RLVRé‡‡æ ·æ•ˆç‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AR3POå¯åº”ç”¨äºå„ç§éœ€è¦é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œåè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºä¼˜åŒ–LLMåœ¨é—®ç­”ã€æ–‡æœ¬ç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚è¯¥æ–¹æ³•åœ¨æ•™è‚²ã€å®¢æœã€å†…å®¹åˆ›ä½œç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved impressive reasoning performance, with reinforcement learning with verifiable rewards (RLVR) emerging as a standard paradigm for post-training. A representative algorithm, group relative policy optimization (GRPO) (Shao et al., 2024), computes advantages by normalizing outcome rewards within response groups, but suffers from a vanishing advantage issue when all responses in a group receive identical rewards. To address this issue, we propose Adaptive Rollout and Response Reuse Policy Optimization (AR3PO), a sampling efficient RLVR algorithm that introduces two novel techniques: adaptive rollout, which dynamically allocates more responses to difficult prompts while saving computation on easier ones, and response reuse, which leverages previously generated correct responses to provide useful training signals. We compare AR3PO with strong RLVR baselines on multiple representative benchmarks using two different families of base models. Across the 7B and 8B models, AR3PO consistently outperforms GRPO and matches or surpasses DAPO (Yu et al., 2025), reducing rollout cost by up to 4.2x. On the larger 32B model, AR3PO achieves comparable performance to DAPO at similar training steps while maintaining substantially lower rollout cost.

