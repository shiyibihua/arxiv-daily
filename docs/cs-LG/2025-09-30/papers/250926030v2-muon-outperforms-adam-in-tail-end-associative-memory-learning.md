---
layout: default
title: Muon Outperforms Adam in Tail-End Associative Memory Learning
---

# Muon Outperforms Adam in Tail-End Associative Memory Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26030" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26030v2</a>
  <a href="https://arxiv.org/pdf/2509.26030.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26030v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26030v2', 'Muon Outperforms Adam in Tail-End Associative Memory Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuche Wang, Fengzhuo Zhang, Jiaxiang Li, Cunxiao Du, Chao Du, Tianyu Pang, Zhuoran Yang, Mingyi Hong, Vincent Y. F. Tan

**åˆ†ç±»**: cs.LG, cs.AI, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Muonä¼˜åŒ–å™¨åœ¨é•¿å°¾å…³è”è®°å¿†å­¦ä¹ ä¸­ä¼˜äºAdamï¼Œé€šè¿‡å¥‡å¼‚è°±åˆ†ææ­ç¤ºå…¶ä¼˜åŠ¿**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Muonä¼˜åŒ–å™¨` `Adamä¼˜åŒ–å™¨` `é•¿å°¾å­¦ä¹ ` `å…³è”è®°å¿†` `å¥‡å¼‚è°±åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ï¼ŒMuonä¼˜åŒ–å™¨è¡¨ç°ä¼˜äºAdamï¼Œä½†å…¶å†…åœ¨æœºåˆ¶å°šä¸æ˜ç¡®ã€‚
2. è®ºæ–‡ä»å…³è”è®°å¿†è§’åº¦å‡ºå‘ï¼Œå‘ç°Muonçš„ä¼˜åŠ¿ä¸»è¦ä½“ç°åœ¨å¯¹Value/Outputæ³¨æ„åŠ›æƒé‡å’Œå‰é¦ˆç½‘ç»œçš„ä¼˜åŒ–ä¸Šã€‚
3. ç†è®ºåˆ†æè¡¨æ˜ï¼ŒMuonåœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡æ•°æ®æ—¶ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°ä¼˜åŒ–å°¾éƒ¨ç±»åˆ«ï¼Œå®ç°æ›´å¹³è¡¡çš„å­¦ä¹ ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Muonä¼˜åŒ–å™¨åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶å§‹ç»ˆæ¯”Adamæ›´å¿«ï¼Œä½†å…¶æˆåŠŸæœºåˆ¶å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡é€šè¿‡å…³è”è®°å¿†çš„è§†è§’æ­ç¤ºäº†è¿™ä¸€æœºåˆ¶ã€‚é€šè¿‡æ¶ˆèMuonä¼˜åŒ–çš„Transformerç»„ä»¶ï¼Œæˆ‘ä»¬å‘ç°LLMçš„å…³è”è®°å¿†å‚æ•°ï¼Œå³Valueå’ŒOutputï¼ˆVOï¼‰æ³¨æ„åŠ›æƒé‡ä»¥åŠå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œæ˜¯Muonä¼˜è¶Šæ€§çš„ä¸»è¦è´¡çŒ®è€…ã€‚å—è¿™ç§å…³è”è®°å¿†è§‚ç‚¹çš„å¯å‘ï¼Œæˆ‘ä»¬è§£é‡Šäº†Muonåœ¨çœŸå®è¯­æ–™åº“ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œè¿™äº›è¯­æ–™åº“æœ¬è´¨ä¸Šæ˜¯é‡å°¾çš„ï¼šä¸€äº›ç±»åˆ«ï¼ˆå°¾éƒ¨ç±»åˆ«ï¼‰å‡ºç°çš„é¢‘ç‡è¿œä½äºå…¶ä»–ç±»åˆ«ã€‚è¿™ç§ä¼˜è¶Šæ€§å¯ä»¥é€šè¿‡ä¸¤ä¸ªå…³é”®å±æ€§æ¥è§£é‡Šï¼šï¼ˆiï¼‰å®ƒçš„æ›´æ–°è§„åˆ™å§‹ç»ˆäº§ç”Ÿæ¯”Adamæ›´å„å‘åŒæ€§çš„å¥‡å¼‚è°±ï¼›å› æ­¤ï¼Œï¼ˆiiï¼‰åœ¨é‡å°¾æ•°æ®ä¸Šï¼Œå®ƒæ¯”Adamæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–å°¾éƒ¨ç±»åˆ«ã€‚é™¤äº†ç»éªŒè¯æ®å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æç±»ä¸å¹³è¡¡æ•°æ®ä¸‹çš„å•å±‚å…³è”è®°å¿†æ¨¡å‹ï¼Œä»ç†è®ºä¸Šè¯å®äº†è¿™äº›å‘ç°ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ— è®ºç‰¹å¾åµŒå…¥å¦‚ä½•ï¼ŒMuonå§‹ç»ˆå®ç°è·¨ç±»åˆ«çš„å¹³è¡¡å­¦ä¹ ï¼Œè€ŒAdamå¯èƒ½ä¼šæ ¹æ®åµŒå…¥å±æ€§å¯¼è‡´å­¦ä¹ è¯¯å·®çš„å·¨å¤§å·®å¼‚ã€‚æ€»è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬çš„ç»éªŒè§‚å¯Ÿå’Œç†è®ºåˆ†ææ­ç¤ºäº†Muonçš„æ ¸å¿ƒä¼˜åŠ¿ï¼šå®ƒçš„æ›´æ–°è§„åˆ™ä¸çº¿æ€§å…³è”è®°å¿†çš„å¤–ç§¯ç»“æ„å¯¹é½ï¼Œä»è€Œèƒ½å¤Ÿæ¯”Adamæ›´å¹³è¡¡å’Œæœ‰æ•ˆåœ°å­¦ä¹ é‡å°¾åˆ†å¸ƒä¸­çš„å°¾éƒ¨ç±»åˆ«ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ï¼ŒAdamä¼˜åŒ–å™¨è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒMuonä¼˜åŒ–å™¨è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚ç„¶è€Œï¼ŒMuonä¼˜åŒ–å™¨ä¼˜äºAdamçš„åŸå› å°šä¸æ˜ç¡®ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å…·æœ‰é•¿å°¾åˆ†å¸ƒçš„æ•°æ®æ—¶ï¼Œå°¾éƒ¨ç±»åˆ«çš„å­¦ä¹ æ•ˆæœå¾€å¾€ä¸ä½³ã€‚å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨æ­ç¤ºMuonä¼˜åŒ–å™¨åœ¨é•¿å°¾æ•°æ®ä¸Šçš„ä¼˜åŠ¿æœºåˆ¶ï¼Œå¹¶ä»ç†è®ºä¸Šè¿›è¡ŒéªŒè¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å…³é”®ç»„ä»¶ï¼ˆValue/Outputæ³¨æ„åŠ›æƒé‡å’Œå‰é¦ˆç½‘ç»œï¼‰è§†ä¸ºå…³è”è®°å¿†ï¼Œå¹¶åˆ†æMuonä¼˜åŒ–å™¨åœ¨æ›´æ–°è¿™äº›å…³è”è®°å¿†å‚æ•°æ—¶çš„è¡Œä¸ºã€‚é€šè¿‡å¥‡å¼‚è°±åˆ†æï¼Œå‘ç°Muonä¼˜åŒ–å™¨èƒ½å¤Ÿäº§ç”Ÿæ›´å„å‘åŒæ€§çš„å¥‡å¼‚è°±ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ å°¾éƒ¨ç±»åˆ«ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) é€šè¿‡æ¶ˆèå®éªŒç¡®å®šMuonä¼˜åŒ–å™¨ä¼˜åŠ¿çš„å…³é”®ç»„ä»¶ï¼›2) å¯¹æ¯”Muonå’ŒAdamä¼˜åŒ–å™¨åœ¨æ›´æ–°å…³è”è®°å¿†å‚æ•°æ—¶çš„å¥‡å¼‚è°±ï¼›3) åœ¨çœŸå®è¯­æ–™åº“ä¸ŠéªŒè¯Muonä¼˜åŒ–å™¨åœ¨é•¿å°¾æ•°æ®ä¸Šçš„æ€§èƒ½ï¼›4) å»ºç«‹å•å±‚å…³è”è®°å¿†æ¨¡å‹ï¼Œä»ç†è®ºä¸Šåˆ†æMuonå’ŒAdamä¼˜åŒ–å™¨åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡æ•°æ®æ—¶çš„å­¦ä¹ è¡Œä¸ºã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä»å…³è”è®°å¿†çš„è§’åº¦è§£é‡Šäº†Muonä¼˜åŒ–å™¨çš„ä¼˜åŠ¿ï¼Œå¹¶æ­ç¤ºäº†å…¶åœ¨å¤„ç†é•¿å°¾æ•°æ®æ—¶çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¥‡å¼‚è°±åˆ†æï¼Œå‘ç°Muonä¼˜åŒ–å™¨èƒ½å¤Ÿäº§ç”Ÿæ›´å„å‘åŒæ€§çš„å¥‡å¼‚è°±ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ å°¾éƒ¨ç±»åˆ«ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é€šè¿‡ç†è®ºåˆ†æéªŒè¯äº†Muonä¼˜åŒ–å™¨åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡æ•°æ®æ—¶çš„ä¼˜è¶Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ¶ˆèå®éªŒæ¥ç¡®å®šMuonä¼˜åŒ–å™¨ä¼˜åŠ¿çš„å…³é”®ç»„ä»¶ï¼›2) ä½¿ç”¨å¥‡å¼‚è°±åˆ†ææ¥æ¯”è¾ƒMuonå’ŒAdamä¼˜åŒ–å™¨åœ¨æ›´æ–°å…³è”è®°å¿†å‚æ•°æ—¶çš„è¡Œä¸ºï¼›3) å»ºç«‹å•å±‚å…³è”è®°å¿†æ¨¡å‹ï¼Œå¹¶æ¨å¯¼Muonå’ŒAdamä¼˜åŒ–å™¨åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡æ•°æ®æ—¶çš„å­¦ä¹ è¯¯å·®ã€‚åœ¨ç†è®ºåˆ†æä¸­ï¼Œè®ºæ–‡å‡è®¾æ•°æ®å…·æœ‰ç±»åˆ«ä¸å¹³è¡¡çš„ç‰¹æ€§ï¼Œå¹¶åˆ†æäº†ä¸åŒç‰¹å¾åµŒå…¥å¯¹å­¦ä¹ è¯¯å·®çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMuonä¼˜åŒ–å™¨åœ¨å¤„ç†é•¿å°¾æ•°æ®æ—¶ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–å°¾éƒ¨ç±»åˆ«ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒMuonä¼˜åŒ–å™¨èƒ½å¤Ÿäº§ç”Ÿæ›´å„å‘åŒæ€§çš„å¥‡å¼‚è°±ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ å°¾éƒ¨ç±»åˆ«ã€‚åœ¨å•å±‚å…³è”è®°å¿†æ¨¡å‹ä¸­ï¼ŒMuonä¼˜åŒ–å™¨å§‹ç»ˆå®ç°è·¨ç±»åˆ«çš„å¹³è¡¡å­¦ä¹ ï¼Œè€ŒAdamä¼˜åŒ–å™¨å¯èƒ½ä¼šæ ¹æ®åµŒå…¥å±æ€§å¯¼è‡´å­¦ä¹ è¯¯å·®çš„å·¨å¤§å·®å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿å°¾æ•°æ®çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€å›¾åƒè¯†åˆ«å’Œæ¨èç³»ç»Ÿã€‚é€šè¿‡ä½¿ç”¨Muonä¼˜åŒ–å™¨ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°å­¦ä¹ å°¾éƒ¨ç±»åˆ«ï¼Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºä¼˜åŒ–å™¨è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯ï¼Œå³è€ƒè™‘ä¼˜åŒ–å™¨ä¸æ¨¡å‹ç»“æ„çš„åŒ¹é…æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon's superiority. Motivated by this associative memory view, we then explain Muon's superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon's core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam.

