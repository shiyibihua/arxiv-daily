---
layout: default
title: DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick
---

# DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26469" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26469v1</a>
  <a href="https://arxiv.org/pdf/2509.26469.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26469v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26469v1', 'DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammad Hassan Vali, Tom BÃ¤ckstrÃ¶m, Arno Solin

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiVeQä»¥è§£å†³å‘é‡é‡åŒ–ä¸­çš„æ¢¯åº¦é˜»å¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `å‘é‡é‡åŒ–` `æ·±åº¦å­¦ä¹ ` `æ¢¯åº¦æµåŠ¨` `æ¨¡å‹è®­ç»ƒ` `å›¾åƒç”Ÿæˆ` `å‹ç¼©ç®—æ³•` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å‘é‡é‡åŒ–æ–¹æ³•åœ¨æ·±åº¦å­¦ä¹ ä¸­å­˜åœ¨ç¡¬æ€§åˆ†é…å¯¼è‡´æ¢¯åº¦é˜»å¡çš„é—®é¢˜ï¼Œå½±å“æ¨¡å‹çš„ç«¯åˆ°ç«¯è®­ç»ƒæ•ˆæœã€‚
2. DiVeQé€šè¿‡å¼•å…¥è¯¯å·®å‘é‡æ¥æ¨¡æ‹Ÿé‡åŒ–å¤±çœŸï¼Œä¿æŒå‰å‘ä¼ æ’­çš„ç¡¬æ€§ï¼ŒåŒæ—¶å…è®¸æ¢¯åº¦æµåŠ¨ï¼Œä»è€Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚
3. åœ¨VQ-VAEå’ŒVQGANçš„å®éªŒä¸­ï¼ŒDiVeQå’ŒSF-DiVeQåœ¨é‡å»ºå’Œæ ·æœ¬è´¨é‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿé‡åŒ–æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‘é‡é‡åŒ–åœ¨æ·±åº¦æ¨¡å‹ä¸­åº”ç”¨å¹¿æ³›ï¼Œä½†å…¶ç¡¬æ€§åˆ†é…ä¼šé˜»ç¢æ¢¯åº¦æµåŠ¨ï¼Œå½±å“ç«¯åˆ°ç«¯è®­ç»ƒã€‚æœ¬æ–‡æå‡ºDiVeQæ–¹æ³•ï¼Œå°†é‡åŒ–è§†ä¸ºæ·»åŠ ä¸€ä¸ªæ¨¡æ‹Ÿé‡åŒ–å¤±çœŸçš„è¯¯å·®å‘é‡ï¼Œä¿æŒå‰å‘ä¼ æ’­çš„ç¡¬æ€§ï¼ŒåŒæ—¶å…è®¸æ¢¯åº¦æµåŠ¨ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§ç©ºé—´å¡«å……å˜ä½“SF-DiVeQï¼Œé€šè¿‡è¿æ¥ç å­—çš„æ›²çº¿è¿›è¡Œåˆ†é…ï¼Œå‡å°‘é‡åŒ–è¯¯å·®å¹¶å……åˆ†åˆ©ç”¨ä»£ç æœ¬ã€‚ä¸¤ç§æ–¹æ³•å‡å¯ç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ— éœ€è¾…åŠ©æŸå¤±æˆ–æ¸©åº¦è°ƒåº¦ã€‚åœ¨VQ-VAEå‹ç¼©å’ŒVQGANç”Ÿæˆçš„å¤šç§æ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒäºå…¶ä»–é‡åŒ–æ–¹æ³•ï¼Œé‡å»ºå’Œæ ·æœ¬è´¨é‡å‡æœ‰æ‰€æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å‘é‡é‡åŒ–æ–¹æ³•åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ä½¿ç”¨æ—¶ï¼Œç”±äºç¡¬æ€§åˆ†é…ä¼šå¯¼è‡´æ¢¯åº¦æ— æ³•æœ‰æ•ˆæµåŠ¨ï¼Œè¿›è€Œé˜»ç¢äº†æ¨¡å‹çš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚è¿™ä¸€é—®é¢˜é™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„DiVeQæ–¹æ³•é€šè¿‡å°†é‡åŒ–è¿‡ç¨‹è§†ä¸ºæ·»åŠ ä¸€ä¸ªè¯¯å·®å‘é‡ï¼Œæ¨¡æ‹Ÿé‡åŒ–å¤±çœŸï¼Œä¿æŒå‰å‘ä¼ æ’­çš„ç¡¬æ€§ï¼ŒåŒæ—¶å…è®¸æ¢¯åº¦æµåŠ¨ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆæ›´æ–°å‚æ•°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDiVeQçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‰å‘ä¼ æ’­é˜¶æ®µå’Œåå‘ä¼ æ’­é˜¶æ®µã€‚åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œæ¨¡å‹æ‰§è¡Œç¡¬æ€§é‡åŒ–ï¼Œè€Œåœ¨åå‘ä¼ æ’­ä¸­ï¼Œè¯¯å·®å‘é‡çš„å¼•å…¥å…è®¸æ¢¯åº¦æµåŠ¨ã€‚SF-DiVeQä½œä¸ºä¸€ç§å˜ä½“ï¼Œé€šè¿‡æ„é€ è¿æ¥ç å­—çš„ç©ºé—´å¡«å……æ›²çº¿ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†é‡åŒ–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šDiVeQçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†é‡åŒ–è§†ä¸ºè¯¯å·®å‘é‡çš„æ·»åŠ ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç¡¬æ€§é‡åŒ–æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…æ— æ³•æœ‰æ•ˆä¼ é€’æ¢¯åº¦ã€‚SF-DiVeQçš„ç©ºé—´å¡«å……ç­–ç•¥ä¹Ÿæ˜¾è‘—å‡å°‘äº†é‡åŒ–è¯¯å·®ã€‚

**å…³é”®è®¾è®¡**ï¼šDiVeQå’ŒSF-DiVeQå‡ä¸éœ€è¦è¾…åŠ©æŸå¤±å‡½æ•°æˆ–æ¸©åº¦è°ƒåº¦ï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚å…³é”®å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡ç¡®ä¿äº†æ¨¡å‹åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡å’Œç½‘ç»œæ¶æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨VQ-VAEå‹ç¼©å’ŒVQGANç”Ÿæˆçš„å®éªŒä¸­ï¼ŒDiVeQå’ŒSF-DiVeQåœ¨é‡å»ºè´¨é‡å’Œæ ·æœ¬ç”Ÿæˆè´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®éœ€æŸ¥é˜…åŸæ–‡ï¼‰ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DiVeQåŠå…¶å˜ä½“SF-DiVeQåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„å‘é‡é‡åŒ–ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç”Ÿæˆã€è§†é¢‘å‹ç¼©å’Œè¯­éŸ³å¤„ç†ç­‰é¢†åŸŸã€‚å…¶åˆ›æ–°çš„æ¢¯åº¦æµåŠ¨æœºåˆ¶èƒ½å¤Ÿæå‡æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šé«˜æ•ˆæ¨¡å‹çš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vector quantization is common in deep models, yet its hard assignments block gradients and hinder end-to-end training. We propose DiVeQ, which treats quantization as adding an error vector that mimics the quantization distortion, keeping the forward pass hard while letting gradients flow. We also present a space-filling variant (SF-DiVeQ) that assigns to a curve constructed by the lines connecting codewords, resulting in less quantization error and full codebook usage. Both methods train end-to-end without requiring auxiliary losses or temperature schedules. On VQ-VAE compression and VQGAN generation across various data sets, they improve reconstruction and sample quality over alternative quantization approaches.

