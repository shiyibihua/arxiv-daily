---
layout: default
title: Memory-Driven Self-Improvement for Decision Making with Large Language Models
---

# Memory-Driven Self-Improvement for Decision Making with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26340" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26340v1</a>
  <a href="https://arxiv.org/pdf/2509.26340.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26340v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26340v1', 'Memory-Driven Self-Improvement for Decision Making with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xue Yan, Zijing Ou, Mengyue Yang, Yan Song, Haifeng Zhang, Yingzhen Li, Jun Wang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè®°å¿†é©±åŠ¨çš„è‡ªæå‡æ¡†æ¶ï¼Œæå‡LLMåœ¨åºåˆ—å†³ç­–ä»»åŠ¡ä¸­çš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åºåˆ—å†³ç­–` `å¼ºåŒ–å­¦ä¹ ` `è®°å¿†ç½‘ç»œ` `è‡ªæå‡å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨åºåˆ—å†³ç­–ä»»åŠ¡ä¸­ï¼Œç”±äºç¼ºä¹ç‰¹å®šä»»åŠ¡æ•°æ®ï¼Œéš¾ä»¥æœ‰æ•ˆé€‚åº”ã€‚
2. æå‡ºè®°å¿†é©±åŠ¨çš„è‡ªæå‡æ¡†æ¶ï¼Œç»“åˆLLMå…ˆéªŒçŸ¥è¯†ä¸é¢†åŸŸç»éªŒè®°å¿†ï¼Œç›¸äº’ä¿ƒè¿›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ å’ŒåŸºäºLLMçš„åŸºçº¿ï¼Œæå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLM)å‡­å€Ÿå…¶å¹¿æ³›çš„å…ˆéªŒçŸ¥è¯†ï¼Œå·²æˆä¸ºåºåˆ—å†³ç­–(SDM)ä»»åŠ¡ä¸­æœ‰æ•ˆçš„åŠ¨ä½œç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™ç§å¹¿æ³›ä½†é€šç”¨çš„çŸ¥è¯†é€šå¸¸ä¸è¶³ä»¥åº”å¯¹ç‰¹å®šå†³ç­–ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨ä»»åŠ¡ç›¸å…³æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¿™ä½¿å¾—LLMéš¾ä»¥æœ‰æ•ˆåœ°é€‚åº”ç‰¹å®šSDMä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è®°å¿†é©±åŠ¨çš„è‡ªæå‡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†LLMçš„é€šç”¨å…ˆéªŒçŸ¥è¯†ä¸é¢†åŸŸç‰¹å®šç»éªŒçš„ç´§å‡‘è®°å¿†ç›¸ç»“åˆã€‚è®°å¿†ä¿ç•™äº†è¿‡å»çš„äº¤äº’å’Œç›¸å…³çš„Qå€¼ï¼Œä»è€Œæ•è·äº†ä¸å†³ç­–ç›¸å…³çš„çŸ¥è¯†ï¼Œæœ‰åŠ©äºå‡†ç¡®çš„ä»·å€¼ä¼°è®¡ï¼Œå¹¶ä¸ºLLMå…ˆéªŒçš„æ”¹è¿›æä¾›ä¿¡æ¯ã€‚åè¿‡æ¥ï¼Œæ”¹è¿›åçš„LLMå…ˆéªŒä¼šç”Ÿæˆæ›´é«˜å¥–åŠ±çš„è½¨è¿¹ï¼Œä»è€Œè¿›ä¸€æ­¥ä¸°å¯Œè®°å¿†ï¼Œå½¢æˆä¸€ä¸ªè‡ªç„¶çš„è‡ªæå‡æ¡†æ¶ï¼Œå…¶ä¸­è®°å¿†å’ŒLLMå…ˆéªŒç›¸äº’åŠ å¼ºã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è®°å¿†é©±åŠ¨æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºLLMçš„åŸºçº¿ï¼Œä¾‹å¦‚ï¼Œåœ¨åŒåˆ†å¸ƒä»»åŠ¡ä¸Šçš„æ€§èƒ½æé«˜äº†40%ä»¥ä¸Šï¼Œåœ¨æ¨å¹¿åˆ°ALFWorldä¸­æœªè§è¿‡çš„ä»»åŠ¡æ—¶ï¼Œæ€§èƒ½æé«˜äº†75%ä»¥ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åºåˆ—å†³ç­–ï¼ˆSDMï¼‰ä»»åŠ¡ä¸­ï¼Œç”±äºç¼ºä¹ç‰¹å®šä»»åŠ¡ç›¸å…³æ•°æ®è€Œéš¾ä»¥æœ‰æ•ˆé€‚åº”çš„é—®é¢˜ã€‚ç°æœ‰çš„LLMè™½ç„¶æ‹¥æœ‰å¹¿æ³›çš„å…ˆéªŒçŸ¥è¯†ï¼Œä½†è¿™äº›çŸ¥è¯†é€šå¸¸æ˜¯é€šç”¨çš„ï¼Œä¸è¶³ä»¥åº”å¯¹ç‰¹å®šSDMä»»åŠ¡ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶å¯ä»¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ï¼Œä½†éœ€è¦å¤§é‡çš„æ ·æœ¬æ•°æ®ï¼Œä¸”æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMçš„é€šç”¨å…ˆéªŒçŸ¥è¯†ä¸é¢†åŸŸç‰¹å®šç»éªŒçš„ç´§å‡‘è®°å¿†ç›¸ç»“åˆï¼Œå½¢æˆä¸€ä¸ªè‡ªæå‡çš„æ¡†æ¶ã€‚é€šè¿‡è®°å¿†æ¨¡å—å­˜å‚¨è¿‡å»çš„äº¤äº’ç»éªŒå’Œç›¸åº”çš„Qå€¼ï¼Œä»è€Œæ•è·ä¸å†³ç­–ç›¸å…³çš„çŸ¥è¯†ï¼Œå¹¶åˆ©ç”¨è¿™äº›çŸ¥è¯†æ¥æ”¹è¿›LLMçš„å…ˆéªŒã€‚æ”¹è¿›åçš„LLMèƒ½å¤Ÿç”Ÿæˆæ›´é«˜å¥–åŠ±çš„è½¨è¿¹ï¼Œä»è€Œè¿›ä¸€æ­¥ä¸°å¯Œè®°å¿†ï¼Œå½¢æˆä¸€ä¸ªæ­£åé¦ˆå¾ªç¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šLLMå’Œè®°å¿†æ¨¡å—ã€‚LLMä½œä¸ºåŠ¨ä½œç­–ç•¥ï¼Œè´Ÿè´£æ ¹æ®å½“å‰çŠ¶æ€ç”ŸæˆåŠ¨ä½œã€‚è®°å¿†æ¨¡å—å­˜å‚¨è¿‡å»çš„äº¤äº’ç»éªŒï¼ŒåŒ…æ‹¬çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œä»¥åŠç›¸åº”çš„Qå€¼ã€‚æ¡†æ¶çš„æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1) LLMæ ¹æ®å½“å‰çŠ¶æ€ç”ŸæˆåŠ¨ä½œï¼›2) æ‰§è¡ŒåŠ¨ä½œå¹¶è·å¾—å¥–åŠ±ï¼›3) å°†äº¤äº’ç»éªŒå­˜å‚¨åˆ°è®°å¿†æ¨¡å—ä¸­ï¼›4) åˆ©ç”¨è®°å¿†æ¨¡å—ä¸­çš„ç»éªŒæ¥æ›´æ–°LLMçš„å…ˆéªŒï¼›5) é‡å¤ä»¥ä¸Šæ­¥éª¤ï¼Œç›´åˆ°æ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†LLMçš„é€šç”¨å…ˆéªŒçŸ¥è¯†ä¸é¢†åŸŸç‰¹å®šç»éªŒçš„è®°å¿†ç›¸ç»“åˆï¼Œå½¢æˆä¸€ä¸ªè‡ªæå‡çš„æ¡†æ¶ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨LLMçš„å…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶é€šè¿‡è®°å¿†æ¨¡å—æ¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ï¼Œä»è€Œæé«˜LLMåœ¨SDMä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¤§é‡çš„æ ·æœ¬æ•°æ®ï¼Œä¸”å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è®°å¿†æ¨¡å—çš„ç»“æ„å’Œæ›´æ–°ç­–ç•¥ï¼›2) å¦‚ä½•åˆ©ç”¨è®°å¿†æ¨¡å—ä¸­çš„ç»éªŒæ¥æ›´æ–°LLMçš„å…ˆéªŒï¼›3) æ¢ç´¢-åˆ©ç”¨ç­–ç•¥çš„è®¾è®¡ï¼Œä»¥å¹³è¡¡æ¢ç´¢æ–°çŠ¶æ€å’Œåˆ©ç”¨å·²çŸ¥çŸ¥è¯†ä¹‹é—´çš„å…³ç³»ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°ã€ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä½†æ­¤å¤„æœªæä¾›å…·ä½“æ•°å€¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ALFWorldç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºLLMçš„åŸºçº¿æ–¹æ³•ã€‚åœ¨åŒåˆ†å¸ƒä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æå‡è¶…è¿‡40%ï¼›åœ¨æ¨å¹¿åˆ°æœªè§è¿‡çš„ä»»åŠ¡æ—¶ï¼Œæ€§èƒ½æå‡è¶…è¿‡75%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨LLMçš„å…ˆéªŒçŸ¥è¯†å’Œé¢†åŸŸç‰¹å®šç»éªŒï¼Œä»è€Œæé«˜LLMåœ¨SDMä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦åºåˆ—å†³ç­–çš„åœºæ™¯ï¼Œä¾‹å¦‚æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡ç»“åˆLLMçš„é€šç”¨çŸ¥è¯†å’Œç‰¹å®šé¢†åŸŸçš„ç»éªŒï¼Œå¯ä»¥æé«˜å†³ç­–ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œé€‚åº”èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆã€æ›´å¯é çš„è‡ªåŠ¨åŒ–æ§åˆ¶ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šå¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have emerged as effective action policies for sequential decision-making (SDM) tasks due to their extensive prior knowledge. However, this broad yet general knowledge is often insufficient for specific decision-making tasks with limited task-related data, making it challenging to efficiently adapt LLMs to specific SDM tasks. To address this challenge, we propose a memory-driven self-improvement framework that combines LLM general prior knowledge with a compact memory of domain-specific experiences. Memory retains past interactions and associated Q-values, thereby capturing decision-relevant knowledge that facilitates accurate value estimation and informs the LLM prior refinement. The refined LLM prior, in turn, generates higher-reward trajectories that further enrich memory, forming a natural self-improvement framework where memory and LLM prior mutually reinforce each other. Experiments show that our memory-driven approach significantly outperforms both traditional RL and LLM-based baselines, e.g., improving performance by over 40\% on in-distribution tasks and over 75\% when generalized to unseen tasks in ALFWorld.

