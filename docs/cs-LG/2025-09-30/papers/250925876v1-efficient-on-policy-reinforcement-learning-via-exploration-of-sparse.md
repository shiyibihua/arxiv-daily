---
layout: default
title: Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space
---

# Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25876" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25876v1</a>
  <a href="https://arxiv.org/pdf/2509.25876.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25876v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25876v1', 'Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinyu Zhang, Aishik Deb, Klaus Mueller

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 16 pages; 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºExploRLerï¼Œé€šè¿‡æ¢ç´¢ç¨€ç–å‚æ•°ç©ºé—´æå‡On-Policyå¼ºåŒ–å­¦ä¹ æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `On-Policyå¼ºåŒ–å­¦ä¹ ` `å‚æ•°ç©ºé—´æ¢ç´¢` `ç­–ç•¥æ¢¯åº¦` `è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–` `è¿ç»­æ§åˆ¶` `è¿­ä»£çº§ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸOn-Policyæ–¹æ³•ä»…ä¾èµ–å•ä¸€æ¢¯åº¦æ›´æ–°ï¼Œå¿½ç•¥äº†å‚æ•°ç©ºé—´ä¸­æ½œåœ¨çš„æ›´ä¼˜è§£åŒºåŸŸã€‚
2. ExploRLeré€šè¿‡ç³»ç»Ÿæ¢ç´¢On-Policyæ¢¯åº¦æ›´æ–°çš„é‚»åŸŸï¼Œå¯»æ‰¾æ›´é«˜æ€§èƒ½çš„ç­–ç•¥ã€‚
3. ExploRLeråœ¨ä¸å¢åŠ æ¢¯åº¦æ›´æ–°æ¬¡æ•°çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚è¿ç»­æ§åˆ¶ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)ç­‰ç­–ç•¥æ¢¯åº¦æ–¹æ³•é€šå¸¸ä»…æ²¿å•ä¸€éšæœºæ¢¯åº¦æ–¹å‘æ›´æ–°ï¼Œå¿½ç•¥äº†å‚æ•°ç©ºé—´ä¸°å¯Œçš„å±€éƒ¨ç»“æ„ã€‚å…ˆå‰ç ”ç©¶è¡¨æ˜ï¼Œæ›¿ä»£æ¢¯åº¦ä¸çœŸå®å¥–åŠ±landscapeçš„ç›¸å…³æ€§è¾ƒå·®ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯è§†åŒ–äº†è¿­ä»£ä¸­ç­–ç•¥æ£€æŸ¥ç‚¹æ‰€è·¨è¶Šçš„å‚æ•°ç©ºé—´ï¼Œå‘ç°æ›´é«˜æ€§èƒ½çš„è§£é€šå¸¸ä½äºé™„è¿‘æœªæ¢ç´¢çš„åŒºåŸŸã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€æœºä¼šï¼Œæˆ‘ä»¬å¼•å…¥ExploRLerï¼Œä¸€ä¸ªå¯æ— ç¼é›†æˆåˆ°PPOå’ŒTRPOç­‰On-Policyç®—æ³•ä¸­çš„æ’ä»¶å¼pipelineï¼Œç³»ç»Ÿåœ°æ¢æµ‹æ›¿ä»£On-Policyæ¢¯åº¦æ›´æ–°çš„æœªæ¢ç´¢é‚»åŸŸã€‚åœ¨ä¸å¢åŠ æ¢¯åº¦æ›´æ–°æ¬¡æ•°çš„æƒ…å†µä¸‹ï¼ŒExploRLeråœ¨å¤æ‚çš„è¿ç»­æ§åˆ¶ç¯å¢ƒä¸­å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿­ä»£çº§æ¢ç´¢ä¸ºåŠ å¼ºOn-Policyå¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§å®ç”¨æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¹¶ä¸ºæ›¿ä»£ç›®æ ‡çš„å±€é™æ€§æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„On-Policyå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¦‚PPOå’ŒTRPOï¼Œé€šå¸¸åªæ²¿ç€ä¸€ä¸ªéšæœºæ¢¯åº¦æ–¹å‘è¿›è¡Œæ›´æ–°ï¼Œè¿™ä½¿å¾—ç®—æ³•å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å‚æ•°ç©ºé—´ä¸­å¯èƒ½å­˜åœ¨çš„æ›´å¥½çš„ç­–ç•¥ã€‚æ›¿ä»£æ¢¯åº¦ä¸çœŸå®å¥–åŠ±landscapeçš„ç›¸å…³æ€§è¾ƒå·®ï¼Œå¯¼è‡´ç®—æ³•æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œé€šè¿‡åœ¨æ¯æ¬¡è¿­ä»£ä¸­æ¢ç´¢å½“å‰ç­–ç•¥å‚æ•°é™„è¿‘çš„åŒºåŸŸï¼Œå¯»æ‰¾èƒ½å¤Ÿå¸¦æ¥æ›´é«˜å¥–åŠ±çš„ç­–ç•¥ã€‚ä½œè€…è§‚å¯Ÿåˆ°ï¼Œåœ¨å‚æ•°ç©ºé—´ä¸­ï¼Œæ›´é«˜æ€§èƒ½çš„è§£å¾€å¾€ä½äºå½“å‰ç­–ç•¥é™„è¿‘çš„æœªæ¢ç´¢åŒºåŸŸã€‚å› æ­¤ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢è¿™äº›åŒºåŸŸï¼Œå¯ä»¥æœ‰æ•ˆåœ°æå‡On-Policyç®—æ³•çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šExploRLerä½œä¸ºä¸€ä¸ªæ’ä»¶å¼pipelineï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„On-Policyç®—æ³•ä¸­ï¼Œå¦‚PPOå’ŒTRPOã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒåŸºäºå½“å‰çš„ç­–ç•¥å‚æ•°ï¼Œç”Ÿæˆå¤šä¸ªå€™é€‰ç­–ç•¥å‚æ•°ï¼›2) ä½¿ç”¨è¿™äº›å€™é€‰ç­–ç•¥å‚æ•°ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œæ”¶é›†æ•°æ®ï¼›3) è¯„ä¼°è¿™äº›å€™é€‰ç­–ç•¥çš„æ€§èƒ½ï¼›4) é€‰æ‹©æ€§èƒ½æœ€ä½³çš„ç­–ç•¥å‚æ•°ä½œä¸ºä¸‹ä¸€æ¬¡è¿­ä»£çš„èµ·ç‚¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šExploRLerçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è¿­ä»£çº§åˆ«çš„å‚æ•°ç©ºé—´æ¢ç´¢æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„On-Policyæ–¹æ³•åªä¾èµ–å•ä¸€æ¢¯åº¦æ›´æ–°ä¸åŒï¼ŒExploRLeré€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢å½“å‰ç­–ç•¥å‚æ•°é™„è¿‘çš„åŒºåŸŸï¼Œå¯»æ‰¾èƒ½å¤Ÿå¸¦æ¥æ›´é«˜å¥–åŠ±çš„ç­–ç•¥ã€‚è¿™ç§æ¢ç´¢æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°é¿å…ç®—æ³•é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œæå‡ç®—æ³•çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šExploRLerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•ç”Ÿæˆå€™é€‰ç­–ç•¥å‚æ•°ï¼šå¯ä»¥ä½¿ç”¨å¤šç§æ–¹æ³•ï¼Œå¦‚åœ¨å½“å‰ç­–ç•¥å‚æ•°å‘¨å›´æ·»åŠ éšæœºå™ªå£°ï¼Œæˆ–è€…ä½¿ç”¨è¿›åŒ–ç®—æ³•ç­‰ï¼›2) å¦‚ä½•è¯„ä¼°å€™é€‰ç­–ç•¥çš„æ€§èƒ½ï¼šå¯ä»¥ä½¿ç”¨å¤šç§æŒ‡æ ‡ï¼Œå¦‚å¹³å‡å¥–åŠ±ã€æˆåŠŸç‡ç­‰ï¼›3) å¦‚ä½•é€‰æ‹©æ€§èƒ½æœ€ä½³çš„ç­–ç•¥å‚æ•°ï¼šå¯ä»¥ä½¿ç”¨å¤šç§æ–¹æ³•ï¼Œå¦‚é€‰æ‹©å¹³å‡å¥–åŠ±æœ€é«˜çš„ç­–ç•¥å‚æ•°ï¼Œæˆ–è€…ä½¿ç”¨é”¦æ ‡èµ›é€‰æ‹©ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ExploRLeråœ¨å¤šä¸ªå¤æ‚çš„è¿ç»­æ§åˆ¶ç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨ä¸å¢åŠ æ¢¯åº¦æ›´æ–°æ¬¡æ•°çš„æƒ…å†µä¸‹ï¼ŒExploRLerèƒ½å¤Ÿæ˜¾è‘—æå‡On-Policyç®—æ³•çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ç¯å¢ƒä¸­ï¼ŒExploRLerèƒ½å¤Ÿå°†ç®—æ³•çš„æ€§èƒ½æå‡è¶…è¿‡20%ã€‚å®éªŒç»“æœéªŒè¯äº†ExploRLerçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ExploRLerå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆOn-Policyå¼ºåŒ–å­¦ä¹ çš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æå‡On-Policyç®—æ³•çš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œå¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿç®—æ³•çš„éƒ¨ç½²å’Œåº”ç”¨ã€‚è¯¥ç ”ç©¶ä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ä¼˜åŒ–æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæ½œåœ¨çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Policy-gradient methods such as Proximal Policy Optimization (PPO) are typically updated along a single stochastic gradient direction, leaving the rich local structure of the parameter space unexplored. Previous work has shown that the surrogate gradient is often poorly correlated with the true reward landscape. Building on this insight, we visualize the parameter space spanned by policy checkpoints within an iteration and reveal that higher performing solutions often lie in nearby unexplored regions. To exploit this opportunity, we introduce ExploRLer, a pluggable pipeline that seamlessly integrates with on-policy algorithms such as PPO and TRPO, systematically probing the unexplored neighborhoods of surrogate on-policy gradient updates. Without increasing the number of gradient updates, ExploRLer achieves significant improvements over baselines in complex continuous control environments. Our results demonstrate that iteration-level exploration provides a practical and effective way to strengthen on-policy reinforcement learning and offer a fresh perspective on the limitations of the surrogate objective.

