---
layout: default
title: Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models
---

# Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26114" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26114v1</a>
  <a href="https://arxiv.org/pdf/2509.26114.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26114v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26114v1', 'Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jaesung R. Park, Junsu Kim, Gyeongman Kim, Jinyoung Jo, Sean Choi, Jaewoong Cho, Ernest K. Ryu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºPPO/GRPOä¸­è£å‰ªæœºåˆ¶å¯¹LLMå¼ºåŒ–å­¦ä¹ ç†µçš„å½±å“ï¼Œæå‡ºclip-lowå¢åŠ æ¢ç´¢ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç†µå´©æºƒ` `PPO` `GRPO` `è£å‰ªæœºåˆ¶` `æ¢ç´¢` `RLVR`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. RLVRæ–¹æ³•åœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ—¶æ˜“å‘ç”Ÿç†µå´©æºƒï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œè®­ç»ƒæ•ˆæœã€‚
2. è®ºæ–‡æ ¸å¿ƒæ€æƒ³æ˜¯åˆ†æPPO/GRPOä¸­è£å‰ªæœºåˆ¶å¯¹ç†µçš„å½±å“ï¼Œå‘ç°clip-lowå¢åŠ ç†µï¼Œclip-highé™ä½ç†µã€‚
3. å®éªŒè¡¨æ˜ï¼Œé€šè¿‡è°ƒæ•´clip-lowå‚æ•°ï¼Œå¯ä»¥æœ‰æ•ˆæ§åˆ¶ç†µï¼Œä¿ƒè¿›æ¢ç´¢ï¼Œå¹¶é˜²æ­¢RLVRè®­ç»ƒä¸­çš„ç†µå´©æºƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)å·²æˆä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ¨ç†èƒ½åŠ›çš„ä¸»æµæ–¹æ³•ã€‚ç„¶è€Œï¼ŒRLVRå®¹æ˜“å‡ºç°ç†µå´©æºƒï¼Œå¯¼è‡´LLMè¿…é€Ÿæ”¶æ•›åˆ°è¿‘ä¹ç¡®å®šæ€§çš„å½¢å¼ï¼Œé˜»ç¢äº†é•¿æœŸå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„æ¢ç´¢å’Œè¿›å±•ã€‚æœ¬æ–‡æ­ç¤ºäº†PPOå’ŒGRPOä¸­çš„è£å‰ªæœºåˆ¶ä¼šå¯¹ç†µäº§ç”Ÿåå·®ã€‚é€šè¿‡ç†è®ºå’Œå®è¯åˆ†æï¼Œæˆ‘ä»¬è¡¨æ˜clip-lowä¼šå¢åŠ ç†µï¼Œè€Œclip-highä¼šé™ä½ç†µã€‚æ­¤å¤–ï¼Œåœ¨æ ‡å‡†è£å‰ªå‚æ•°ä¸‹ï¼Œclip-highçš„å½±å“å ä¸»å¯¼åœ°ä½ï¼Œå³ä½¿åœ¨ä¸ºRLç®—æ³•æä¾›çº¯ç²¹éšæœºå¥–åŠ±æ—¶ï¼Œä¹Ÿä¼šå¯¼è‡´æ•´ä½“ç†µé™ä½ã€‚æˆ‘ä»¬çš„å‘ç°çªå‡ºäº†RLVRä¸­ä¸€ä¸ªè¢«å¿½è§†çš„æ··æ·†å› ç´ ï¼šè£å‰ªæœºåˆ¶ç‹¬ç«‹äºå¥–åŠ±ä¿¡å·å½±å“ç†µï¼Œè¿›è€Œå½±å“æ¨ç†è¡Œä¸ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå¯ä»¥æœ‰æ„è¯†åœ°ä½¿ç”¨è£å‰ªæ¥æ§åˆ¶ç†µã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æ›´ç§¯æçš„clip-lowå€¼ï¼Œå¯ä»¥å¢åŠ ç†µï¼Œä¿ƒè¿›æ¢ç´¢ï¼Œå¹¶æœ€ç»ˆé˜²æ­¢RLVRè®­ç»ƒä¸­çš„ç†µå´©æºƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°çš„ç†µå´©æºƒé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºPPOå’ŒGRPOçš„RLVRï¼Œè™½ç„¶åœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢ï¼Œå¯¼è‡´æ¨¡å‹è¿‡æ—©æ”¶æ•›ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¥–åŠ±ä¿¡å·ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå¯¹å½±å“ç†µçš„å…³é”®å› ç´ ç¼ºä¹æ·±å…¥ç†è§£ï¼Œéš¾ä»¥æœ‰æ•ˆæ§åˆ¶æ¨¡å‹çš„æ¢ç´¢è¡Œä¸ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ·±å…¥åˆ†æPPOå’ŒGRPOä¸­è£å‰ªæœºåˆ¶å¯¹ç­–ç•¥ç†µçš„å½±å“ã€‚ä½œè€…å‘ç°ï¼Œclip-lowå’Œclip-highåˆ†åˆ«å¯¹ç†µäº§ç”Ÿç›¸åçš„å½±å“ï¼šclip-lowå€¾å‘äºå¢åŠ ç†µï¼Œä¿ƒè¿›æ¢ç´¢ï¼›è€Œclip-highåˆ™å€¾å‘äºé™ä½ç†µï¼Œä½¿ç­–ç•¥æ›´åŠ ç¡®å®šã€‚é€šè¿‡ç²¾ç¡®æ§åˆ¶è¿™ä¸¤ä¸ªå‚æ•°ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è°ƒèŠ‚ç­–ç•¥çš„æ¢ç´¢ç¨‹åº¦ï¼Œé¿å…ç†µå´©æºƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡ä¸»è¦é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯æ¥æ”¯æŒå…¶è§‚ç‚¹ã€‚ç†è®ºåˆ†æéƒ¨åˆ†ï¼Œä½œè€…æ¨å¯¼äº†è£å‰ªæœºåˆ¶å¯¹ç­–ç•¥æ¢¯åº¦å’Œç†µçš„å½±å“ï¼Œæ­ç¤ºäº†clip-lowå’Œclip-highçš„ä¸åŒä½œç”¨ã€‚å®éªŒéƒ¨åˆ†ï¼Œä½œè€…åœ¨ä¸åŒçš„ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šï¼Œé€šè¿‡è°ƒæ•´clip-lowå’Œclip-highçš„å€¼ï¼Œè§‚å¯Ÿç­–ç•¥ç†µçš„å˜åŒ–ï¼Œå¹¶è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1)ç†è®ºåˆ†æè£å‰ªæœºåˆ¶å¯¹ç†µçš„å½±å“ï¼›2)è®¾è®¡å®éªŒéªŒè¯ç†è®ºåˆ†æï¼›3)æå‡ºé€šè¿‡è°ƒæ•´clip-lowæ¥é˜²æ­¢ç†µå´©æºƒçš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ­ç¤ºäº†PPOå’ŒGRPOä¸­è£å‰ªæœºåˆ¶å¯¹ç­–ç•¥ç†µçš„éšè”½å½±å“ã€‚ä»¥å¾€çš„ç ”ç©¶å¾€å¾€å¿½ç•¥äº†è£å‰ªæœºåˆ¶å¯¹æ¢ç´¢è¡Œä¸ºçš„æ½œåœ¨å½±å“ï¼Œè€Œæœ¬æ–‡é¦–æ¬¡æ˜ç¡®æŒ‡å‡ºclip-lowå’Œclip-highåˆ†åˆ«å¯¹ç†µäº§ç”Ÿç›¸åçš„ä½œç”¨ï¼Œå¹¶æå‡ºäº†é€šè¿‡è°ƒæ•´clip-lowæ¥æ§åˆ¶ç†µã€ä¿ƒè¿›æ¢ç´¢çš„ç­–ç•¥ã€‚è¿™ä¸ºè§£å†³LLMå¼ºåŒ–å­¦ä¹ ä¸­çš„ç†µå´©æºƒé—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1)è¯¦ç»†çš„ç†è®ºæ¨å¯¼ï¼Œåˆ†æè£å‰ªæœºåˆ¶å¦‚ä½•å½±å“ç­–ç•¥æ¢¯åº¦å’Œç†µï¼›2)ç²¾å¿ƒè®¾è®¡çš„å®éªŒï¼Œé€šè¿‡æ§åˆ¶clip-lowå’Œclip-highçš„å€¼ï¼Œè§‚å¯Ÿç­–ç•¥ç†µçš„å˜åŒ–ï¼›3)ä½¿ç”¨æ ‡å‡†PPOå’ŒGRPOç®—æ³•ï¼Œç¡®ä¿ç»“æœçš„å¯å¤ç°æ€§å’Œé€šç”¨æ€§ã€‚å…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬clip-lowå’Œclip-highçš„å€¼ï¼Œä»¥åŠå­¦ä¹ ç‡ã€å¥–åŠ±ç³»æ•°ç­‰è¶…å‚æ•°ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„PPOæˆ–GRPOæŸå¤±å‡½æ•°ï¼Œç½‘ç»œç»“æ„åˆ™æ ¹æ®å…·ä½“çš„LLMä»»åŠ¡è¿›è¡Œé€‰æ‹©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ ‡å‡†è£å‰ªå‚æ•°ä¸‹ï¼Œclip-highçš„å½±å“å ä¸»å¯¼åœ°ä½ï¼Œå¯¼è‡´æ•´ä½“ç†µé™ä½ã€‚é€šè¿‡å¢åŠ clip-lowçš„å€¼ï¼Œå¯ä»¥æœ‰æ•ˆå¢åŠ ç­–ç•¥ç†µï¼Œä¿ƒè¿›æ¢ç´¢ï¼Œå¹¶é˜²æ­¢ç†µå´©æºƒã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼Œé€šè¿‡è°ƒæ•´clip-lowï¼Œæ¨¡å‹çš„æ€§èƒ½æå‡äº†æ˜¾è‘—çš„ç™¾åˆ†æ¯”ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼Œè®ºæ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æ§åˆ¶è£å‰ªå‚æ•°ï¼Œå¯ä»¥æœ‰æ•ˆé¿å…ç†µå´©æºƒï¼Œæå‡æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ï¼Œä»è€Œè·å¾—æ›´æ™ºèƒ½ã€æ›´å¯é çš„LLMåº”ç”¨ã€‚è¯¥ç ”ç©¶ä¸ºLLMå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–æä¾›äº†ä¸€ç§æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning with verifiable rewards (RLVR) has recently emerged as the leading approach for enhancing the reasoning capabilities of large language models (LLMs). However, RLVR is prone to entropy collapse, where the LLM quickly converges to a near-deterministic form, hindering exploration and progress during prolonged RL training. In this work, we reveal that the clipping mechanism in PPO and GRPO induces biases on entropy. Through theoretical and empirical analyses, we show that clip-low increases entropy, while clip-high decreases it. Further, under standard clipping parameters, the effect of clip-high dominates, resulting in an overall entropy reduction even when purely random rewards are provided to the RL algorithm. Our findings highlight an overlooked confounding factor in RLVR: independent of the reward signal, the clipping mechanism influences entropy, which in turn affects the reasoning behavior. Furthermore, our analysis demonstrates that clipping can be deliberately used to control entropy. Specifically, with a more aggressive clip-low value, one can increase entropy, promote exploration, and ultimately prevent entropy collapse in RLVR training.

