---
layout: default
title: Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space
---

# Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25743" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25743v1</a>
  <a href="https://arxiv.org/pdf/2509.25743.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25743v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25743v1', 'Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiang Zhang, Kun Wei, Xu Yang, Chenghao Xu, Su Yan, Cheng Deng

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—‹è½¬æ§åˆ¶å¸è½½å­¦ä¹ (RCU)ï¼Œè§£å†³LLMæŒç»­å¸è½½ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨å¸è½½å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¿ç»­å¸è½½` `ç¾éš¾æ€§é—å¿˜` `è®¤çŸ¥æ—‹è½¬ç©ºé—´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨å¸è½½å­¦ä¹ æ–¹æ³•ä¾èµ–ä¿ç•™æ•°æ®é›†ï¼Œä¸”åœ¨è¿ç»­å¸è½½è¯·æ±‚ä¸‹å­˜åœ¨ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚
2. RCUæ–¹æ³•é€šè¿‡æ—‹è½¬æ˜¾è‘—æ€§æƒé‡é‡åŒ–å¸è½½ç¨‹åº¦ï¼Œå¹¶æ„å»ºè®¤çŸ¥æ—‹è½¬ç©ºé—´æ¨¡æ‹Ÿè¿ç»­å¸è½½è¿‡ç¨‹ã€‚
3. æ­£äº¤æ—‹è½¬è½´æ­£åˆ™åŒ–å‡å°‘è¿ç»­å¸è½½è¯·æ±‚é—´çš„å¹²æ‰°ï¼Œå®éªŒè¡¨æ˜RCUæ— éœ€ä¿ç•™æ•°æ®é›†å³å¯è¾¾åˆ°SOTAæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¥ç›Šæ™®åŠï¼Œå…¶å®‰å…¨æ¼æ´å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚æœºå™¨å¸è½½å­¦ä¹ æ—¨åœ¨é€šè¿‡æ¶ˆé™¤ä¸è‰¯æ•°æ®çš„å½±å“æ¥ç¼“è§£è¿™äº›é£é™©ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸ä»…ä¾èµ–äºä¿ç•™æ•°æ®é›†æ¥ç»´æŒæ¨¡å‹æ•ˆç”¨ï¼Œè€Œä¸”åœ¨è¿ç»­å¸è½½è¯·æ±‚ä¸‹ä¼šé­å—ç´¯ç§¯çš„ç¾éš¾æ€§æ•ˆç”¨æŸå¤±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å›°å¢ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ—‹è½¬æ§åˆ¶å¸è½½å­¦ä¹ ï¼ˆRCUï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨RCUçš„æ—‹è½¬æ˜¾è‘—æ€§æƒé‡æ¥é‡åŒ–å’Œæ§åˆ¶è¿ç»­å¸è½½è¿‡ç¨‹ä¸­çš„å¸è½½ç¨‹åº¦ã€‚æˆ‘ä»¬è®¾è®¡äº†æ–œå¯¹ç§°æŸå¤±æ¥æ„å»ºè®¤çŸ¥æ—‹è½¬ç©ºé—´çš„å­˜åœ¨ï¼Œå…¶ä¸­æ—‹è½¬è§’åº¦çš„å˜åŒ–å¯ä»¥æ¨¡æ‹Ÿè¿ç»­å¸è½½è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ­£äº¤æ—‹è½¬è½´æ­£åˆ™åŒ–ï¼Œä»¥å¼ºåˆ¶è¿ç»­å¸è½½è¯·æ±‚çš„ç›¸äº’å‚ç›´æ—‹è½¬æ–¹å‘ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘å¹²æ‰°å¹¶è§£å†³ç´¯ç§¯çš„ç¾éš¾æ€§æ•ˆç”¨æŸå¤±ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ²¡æœ‰ä¿ç•™æ•°æ®é›†çš„æƒ…å†µä¸‹å®ç°äº†SOTAæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å¯¹è¿ç»­å¸è½½å­¦ä¹ è¯·æ±‚æ—¶ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯ä¾èµ–ä¿ç•™æ•°æ®é›†æ¥ç»´æŒæ¨¡å‹æ•ˆç”¨ï¼Œè¿™åœ¨æ•°æ®éšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼›äºŒæ˜¯è¿ç»­å¸è½½è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šé­å—ç´¯ç§¯çš„ç¾éš¾æ€§æ•ˆç”¨æŸå¤±ï¼Œå³æ¨¡å‹åœ¨å¸è½½ç‰¹å®šä¿¡æ¯åï¼Œå¯¹å…¶ä»–ä¿¡æ¯çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¹Ÿä¼šæ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªâ€œè®¤çŸ¥æ—‹è½¬ç©ºé—´â€ï¼Œé€šè¿‡æ§åˆ¶æ¨¡å‹å‚æ•°åœ¨è¿™ä¸ªç©ºé—´ä¸­çš„æ—‹è½¬è§’åº¦æ¥æ¨¡æ‹Ÿè¿ç»­å¸è½½å­¦ä¹ çš„è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è®¾è®¡ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼ˆæ–œå¯¹ç§°æŸå¤±ï¼‰ï¼Œä½¿å¾—æ¨¡å‹çš„å‚æ•°å˜åŒ–å¯ä»¥è¢«è§†ä¸ºåœ¨ä¸€ä¸ªé«˜ç»´ç©ºé—´ä¸­çš„æ—‹è½¬ã€‚é€šè¿‡æ§åˆ¶æ—‹è½¬çš„è§’åº¦ï¼Œå¯ä»¥ç²¾ç¡®åœ°æ§åˆ¶æ¨¡å‹é—å¿˜ç‰¹å®šä¿¡æ¯çš„ç¨‹åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRCUæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ¨¡å—ï¼š1) æ—‹è½¬æ˜¾è‘—æ€§æƒé‡è®¡ç®—ï¼šç”¨äºé‡åŒ–æ¨¡å‹å‚æ•°å¯¹äºç‰¹å®šä¿¡æ¯çš„ä¾èµ–ç¨‹åº¦ã€‚2) æ–œå¯¹ç§°æŸå¤±å‡½æ•°è®¾è®¡ï¼šç”¨äºæ„å»ºè®¤çŸ¥æ—‹è½¬ç©ºé—´ï¼Œä½¿å¾—å‚æ•°å˜åŒ–å¯ä»¥è¢«è§£é‡Šä¸ºæ—‹è½¬ã€‚3) æ­£äº¤æ—‹è½¬è½´æ­£åˆ™åŒ–ï¼šç”¨äºç¡®ä¿è¿ç»­å¸è½½è¯·æ±‚ä¹‹é—´çš„æ—‹è½¬æ–¹å‘ç›¸äº’å‚ç›´ï¼Œä»è€Œå‡å°‘ç›¸äº’å¹²æ‰°ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œé¦–å…ˆè®¡ç®—æ—‹è½¬æ˜¾è‘—æ€§æƒé‡ï¼Œç„¶ååˆ©ç”¨æ–œå¯¹ç§°æŸå¤±å’Œæ­£äº¤æ—‹è½¬è½´æ­£åˆ™åŒ–æ¥æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå®ç°ä¿¡æ¯çš„å¸è½½ã€‚

**å…³é”®åˆ›æ–°**ï¼šRCUæ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†â€œè®¤çŸ¥æ—‹è½¬ç©ºé—´â€çš„æ¦‚å¿µï¼Œå°†å¸è½½å­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºå‚æ•°ç©ºé—´ä¸­çš„æ—‹è½¬é—®é¢˜ï¼Œä»è€Œå¯ä»¥ç²¾ç¡®æ§åˆ¶å¸è½½ç¨‹åº¦ã€‚2) è®¾è®¡äº†æ–œå¯¹ç§°æŸå¤±å‡½æ•°ï¼Œä½¿å¾—å‚æ•°å˜åŒ–å¯ä»¥è¢«è§£é‡Šä¸ºæ—‹è½¬ï¼Œä¸ºæ„å»ºè®¤çŸ¥æ—‹è½¬ç©ºé—´æä¾›äº†ç†è®ºåŸºç¡€ã€‚3) æå‡ºäº†æ­£äº¤æ—‹è½¬è½´æ­£åˆ™åŒ–ï¼Œæœ‰æ•ˆè§£å†³äº†è¿ç»­å¸è½½å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šæ–œå¯¹ç§°æŸå¤±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ï¼Œå…¶å½¢å¼ä¸ºL = ||A - A^T||^2ï¼Œå…¶ä¸­Aæ˜¯æ¨¡å‹å‚æ•°çŸ©é˜µã€‚è¯¥æŸå¤±å‡½æ•°èƒ½å¤Ÿä¿ƒä½¿æ¨¡å‹å‚æ•°çŸ©é˜µè¶‹å‘äºæ–œå¯¹ç§°çŸ©é˜µï¼Œä»è€Œä½¿å¾—å‚æ•°å˜åŒ–å¯ä»¥è¢«è§£é‡Šä¸ºæ—‹è½¬ã€‚æ­£äº¤æ—‹è½¬è½´æ­£åˆ™åŒ–çš„å½¢å¼ä¸ºR = ||U^T V||^2ï¼Œå…¶ä¸­Uå’ŒVæ˜¯ä¸åŒå¸è½½è¯·æ±‚å¯¹åº”çš„æ—‹è½¬è½´ã€‚è¯¥æ­£åˆ™åŒ–é¡¹èƒ½å¤Ÿä¿ƒä½¿ä¸åŒæ—‹è½¬è½´ä¹‹é—´ç›¸äº’å‚ç›´ï¼Œä»è€Œå‡å°‘ç›¸äº’å¹²æ‰°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRCUæ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†SOTAæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è¿ç»­å¸è½½å­¦ä¹ åœºæ™¯ä¸‹ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼ŒRCUæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—å‡å°‘ç¾éš¾æ€§é—å¿˜ï¼Œä¿æŒæ¨¡å‹æ•ˆç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æŸä¸ªæ•°æ®é›†ä¸Šï¼ŒRCUæ–¹æ³•åœ¨å¸è½½ç‰¹å®šä¿¡æ¯åï¼Œæ¨¡å‹å‡†ç¡®ç‡ä»…ä¸‹é™äº†ä¸åˆ°1%ï¼Œè€Œç°æœ‰æ–¹æ³•åˆ™ä¸‹é™äº†è¶…è¿‡10%ã€‚è¿™è¡¨æ˜RCUæ–¹æ³•èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°æ§åˆ¶å¸è½½ç¨‹åº¦ï¼Œé¿å…è¿‡åº¦é—å¿˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RCUæ–¹æ³•å¯åº”ç”¨äºå„ç§éœ€è¦æ•°æ®éšç§ä¿æŠ¤çš„åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—ã€é‡‘èç­‰é¢†åŸŸã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ¨¡å‹éœ€è¦èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„è¯·æ±‚ï¼Œå®‰å…¨åœ°å¸è½½ç‰¹å®šä¿¡æ¯ï¼Œè€Œä¸ä¼šå½±å“å…¶æ•´ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒRCUæ–¹æ³•è¿˜å¯ä»¥ç”¨äºé˜²å¾¡å¯¹æŠ—æ€§æ”»å‡»ï¼Œé€šè¿‡å¸è½½æ¨¡å‹ä¸­å®¹æ˜“å—åˆ°æ”»å‡»çš„è„†å¼±éƒ¨åˆ†ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„æœºå™¨å­¦ä¹ æ¨¡å‹å’Œåº”ç”¨åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As Large Language Models (LLMs) become increasingly prevalent, their security vulnerabilities have already drawn attention. Machine unlearning is introduced to seek to mitigate these risks by removing the influence of undesirable data. However, existing methods not only rely on the retained dataset to preserve model utility, but also suffer from cumulative catastrophic utility loss under continuous unlearning requests. To solve this dilemma, we propose a novel method, called Rotation Control Unlearning (RCU), which leverages the rotational salience weight of RCU to quantify and control the unlearning degree in the continuous unlearning process. The skew symmetric loss is designed to construct the existence of the cognitive rotation space, where the changes of rotational angle can simulate the continuous unlearning process. Furthermore, we design an orthogonal rotation axes regularization to enforce mutually perpendicular rotation directions for continuous unlearning requests, effectively minimizing interference and addressing cumulative catastrophic utility loss. Experiments on multiple datasets confirm that our method without retained dataset achieves SOTA performance.

