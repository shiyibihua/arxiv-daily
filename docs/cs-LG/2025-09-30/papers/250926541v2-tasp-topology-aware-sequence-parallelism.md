---
layout: default
title: TASP: Topology-aware Sequence Parallelism
---

# TASP: Topology-aware Sequence Parallelism

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26541" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26541v2</a>
  <a href="https://arxiv.org/pdf/2509.26541.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26541v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26541v2', 'TASP: Topology-aware Sequence Parallelism')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yida Wang, Ke Hong, Xiuhong Li, Yuanchao Xu, Wenxun Wang, Guohao Dai, Yu Wang

**åˆ†ç±»**: cs.LG, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-09)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/infinigence/HamiltonAttention)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TASPï¼šä¸€ç§æ‹“æ‰‘æ„ŸçŸ¥çš„åºåˆ—å¹¶è¡Œæ–¹æ³•ï¼Œæå‡é•¿æ–‡æœ¬LLMåœ¨ç°ä»£åŠ é€Ÿå™¨ä¸Šçš„é€šä¿¡æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åºåˆ—å¹¶è¡Œ` `é•¿æ–‡æœ¬LLM` `æ‹“æ‰‘æ„ŸçŸ¥` `é€šä¿¡ä¼˜åŒ–` `AlltoAllæ‹“æ‰‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Ring Attentionåœ¨é•¿æ–‡æœ¬LLMä¸­é€šä¿¡æ•ˆç‡ä½ï¼ŒåŸå› æ˜¯å…¶Ring AllGatheré€šä¿¡åŸè¯­ä¸ç°ä»£åŠ é€Ÿå™¨çš„AlltoAllæ‹“æ‰‘ç»“æ„ä¸åŒ¹é…ã€‚
2. TASPé€šè¿‡æ‹“æ‰‘åˆ†è§£å’ŒåŸè¯­åˆ†è§£ï¼Œå°†åŠ é€Ÿå™¨æ‹“æ‰‘åˆ†è§£ä¸ºå¤šä¸ªæ­£äº¤ç¯å½¢æ•°æ®è·¯å¾„ï¼Œå¹¶å‘ä¼ è¾“æ•°æ®ï¼Œå……åˆ†åˆ©ç”¨é€šä¿¡èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTASPåœ¨NVIDIA H100å’ŒAMD MI300Xç³»ç»Ÿä¸Šæ¯”Ring AttentionåŠå…¶å˜ä½“å®ç°äº†æ›´é«˜çš„é€šä¿¡æ•ˆç‡å’Œæ˜¾è‘—çš„åŠ é€Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦è€Œé¢ä¸´çº¦æŸã€‚ä¸»æµçš„åºåˆ—å¹¶è¡Œï¼ˆSPï¼‰æ–¹æ³•Ring Attentionè¯•å›¾é€šè¿‡å°†queryåˆ†å‘åˆ°å¤šä¸ªåŠ é€Ÿå™¨ä¸Šçš„queryå—ï¼Œå¹¶é€šè¿‡Ring AllGatheré€šä¿¡åŸè¯­ä½¿æ¯ä¸ªQå¼ é‡èƒ½å¤Ÿè®¿é—®æ¥è‡ªå…¶ä»–åŠ é€Ÿå™¨çš„æ‰€æœ‰KVå¼ é‡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå®ƒè¡¨ç°å‡ºè¾ƒä½çš„é€šä¿¡æ•ˆç‡ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…ä¸­çš„åº”ç”¨ã€‚è¿™ç§ä½æ•ˆç‡æºäºå®ƒæ‰€é‡‡ç”¨çš„Ring AllGatheré€šä¿¡åŸè¯­ä¸ç°ä»£åŠ é€Ÿå™¨çš„AlltoAllæ‹“æ‰‘ç»“æ„ä¹‹é—´çš„ä¸åŒ¹é…ã€‚Ring AllGatheråŸè¯­ç”±è¿­ä»£çš„ç¯å½¢æ•°æ®ä¼ è¾“ç»„æˆï¼Œè¿™åªèƒ½åˆ©ç”¨AlltoAllæ‹“æ‰‘ç»“æ„ä¸­éå¸¸æœ‰é™çš„ä¸€éƒ¨åˆ†ã€‚å—å®Œæ•´æœ‰å‘å›¾çš„å“ˆå¯†é¡¿åˆ†è§£çš„å¯å‘ï¼Œæˆ‘ä»¬å‘ç°ç°ä»£åŠ é€Ÿå™¨æ‹“æ‰‘å¯ä»¥åˆ†è§£ä¸ºå¤šä¸ªæ­£äº¤çš„ç¯å½¢æ•°æ®è·¯å¾„ï¼Œè¿™äº›è·¯å¾„å¯ä»¥å¹¶å‘åœ°ä¼ è¾“æ•°æ®è€Œä¸ä¼šç›¸äº’å¹²æ‰°ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°Ring AllGatheråŸè¯­ä¹Ÿå¯ä»¥åœ¨æ¯æ¬¡è¿­ä»£ä¸­åˆ†è§£ä¸ºç›¸åŒæ•°é‡çš„å¹¶å‘ç¯å½¢æ•°æ®ä¼ è¾“ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé•¿ä¸Šä¸‹æ–‡LLMçš„æ‹“æ‰‘æ„ŸçŸ¥SPæ–¹æ³•TASPï¼Œè¯¥æ–¹æ³•é€šè¿‡æ‹“æ‰‘åˆ†è§£å’ŒåŸè¯­åˆ†è§£å……åˆ†åˆ©ç”¨äº†ç°ä»£åŠ é€Ÿå™¨çš„é€šä¿¡èƒ½åŠ›ã€‚åœ¨å•èŠ‚ç‚¹å’Œå¤šèŠ‚ç‚¹NVIDIA H100ç³»ç»Ÿä»¥åŠå•èŠ‚ç‚¹AMD MI300Xç³»ç»Ÿä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTASPåœ¨è¿™äº›ç°ä»£åŠ é€Ÿå™¨æ‹“æ‰‘ä¸Šå®ç°äº†æ¯”Ring Attentionæ›´é«˜çš„é€šä¿¡æ•ˆç‡ï¼Œå¹¶ä¸”æ¯”Ring AttentionåŠå…¶å˜ä½“Zigzag-Ring Attentionå®ç°äº†é«˜è¾¾3.58å€çš„åŠ é€Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é•¿æ–‡æœ¬LLMä¸­ï¼Œç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦ï¼Œå¯¼è‡´åºåˆ—å¹¶è¡Œæ–¹æ³•ï¼ˆå¦‚Ring Attentionï¼‰åœ¨ç°ä»£åŠ é€Ÿå™¨ä¸Šé€šä¿¡æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚Ring Attentioné‡‡ç”¨çš„Ring AllGatheré€šä¿¡åŸè¯­ä¸ç°ä»£åŠ é€Ÿå™¨çš„AlltoAllæ‹“æ‰‘ç»“æ„ä¸åŒ¹é…ï¼Œé€ æˆé€šä¿¡ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å……åˆ†åˆ©ç”¨ç°ä»£åŠ é€Ÿå™¨çš„AlltoAllæ‹“æ‰‘ç»“æ„ã€‚é€šè¿‡å°†åŠ é€Ÿå™¨æ‹“æ‰‘åˆ†è§£ä¸ºå¤šä¸ªæ­£äº¤çš„ç¯å½¢æ•°æ®è·¯å¾„ï¼Œå¹¶åŒæ—¶åˆ†è§£Ring AllGatheråŸè¯­ï¼Œå®ç°å¹¶å‘çš„æ•°æ®ä¼ è¾“ï¼Œä»è€Œæé«˜é€šä¿¡æ•ˆç‡ã€‚è¿™ç§è®¾è®¡åŸºäºå®Œæ•´æœ‰å‘å›¾çš„å“ˆå¯†é¡¿åˆ†è§£ï¼Œç¡®ä¿æ•°æ®ä¼ è¾“ä¸ä¼šç›¸äº’å¹²æ‰°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTASPæ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼š1) æ‹“æ‰‘åˆ†è§£ï¼šå°†ç°ä»£åŠ é€Ÿå™¨çš„AlltoAllæ‹“æ‰‘åˆ†è§£ä¸ºå¤šä¸ªæ­£äº¤çš„ç¯å½¢æ•°æ®è·¯å¾„ã€‚2) åŸè¯­åˆ†è§£ï¼šå°†Ring AllGatheråŸè¯­åˆ†è§£ä¸ºç›¸åŒæ•°é‡çš„å¹¶å‘ç¯å½¢æ•°æ®ä¼ è¾“ã€‚è¿™ä¸¤ä¸ªæ­¥éª¤ååŒå·¥ä½œï¼Œä½¿å¾—æ•°æ®å¯ä»¥åœ¨å¤šä¸ªç¯å½¢è·¯å¾„ä¸Šå¹¶è¡Œä¼ è¾“ï¼Œä»è€Œå……åˆ†åˆ©ç”¨åŠ é€Ÿå™¨çš„é€šä¿¡å¸¦å®½ã€‚

**å…³é”®åˆ›æ–°**ï¼šTASPæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶æ‹“æ‰‘æ„ŸçŸ¥çš„è®¾è®¡ã€‚å®ƒä¸å†ç®€å•åœ°ä½¿ç”¨ä¼ ç»Ÿçš„Ring AllGatheråŸè¯­ï¼Œè€Œæ˜¯æ ¹æ®ç°ä»£åŠ é€Ÿå™¨çš„å®é™…æ‹“æ‰‘ç»“æ„è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡æ‹“æ‰‘åˆ†è§£å’ŒåŸè¯­åˆ†è§£ï¼Œå®ç°äº†æ›´é«˜çš„é€šä¿¡æ•ˆç‡ã€‚ä¸Ring Attentionç›¸æ¯”ï¼ŒTASPèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç°ä»£åŠ é€Ÿå™¨çš„AlltoAllæ‹“æ‰‘ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šTASPçš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•è¿›è¡Œæ‹“æ‰‘åˆ†è§£å’ŒåŸè¯­åˆ†è§£ã€‚å…·ä½“æ¥è¯´ï¼Œéœ€è¦æ ¹æ®åŠ é€Ÿå™¨çš„äº’è”ç»“æ„ï¼Œæ‰¾åˆ°ä¸€ç»„æ­£äº¤çš„ç¯å½¢æ•°æ®è·¯å¾„ï¼Œå¹¶è®¾è®¡ç›¸åº”çš„é€šä¿¡è°ƒåº¦ç®—æ³•ï¼Œä»¥ç¡®ä¿æ•°æ®åœ¨è¿™äº›è·¯å¾„ä¸Šé«˜æ•ˆåœ°ä¼ è¾“ã€‚è®ºæ–‡ä¸­å¯èƒ½åŒ…å«å…³äºå¦‚ä½•é€‰æ‹©è¿™äº›ç¯å½¢è·¯å¾„ï¼Œä»¥åŠå¦‚ä½•è¿›è¡Œæ•°æ®åˆ†ç‰‡å’Œèšåˆçš„ç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTASPåœ¨å•èŠ‚ç‚¹å’Œå¤šèŠ‚ç‚¹NVIDIA H100ç³»ç»Ÿä»¥åŠå•èŠ‚ç‚¹AMD MI300Xç³»ç»Ÿä¸Šå‡ä¼˜äºRing AttentionåŠå…¶å˜ä½“Zigzag-Ring Attentionã€‚TASPå®ç°äº†é«˜è¾¾3.58å€çš„åŠ é€Ÿï¼Œè¯æ˜äº†å…¶åœ¨ç°ä»£åŠ é€Ÿå™¨ä¸Šå…·æœ‰æ›´é«˜çš„é€šä¿¡æ•ˆç‡å’Œæ€§èƒ½ä¼˜åŠ¿ã€‚è¿™äº›ç»“æœè¡¨æ˜TASPæ˜¯ä¸€ç§æœ‰æ•ˆçš„é•¿æ–‡æœ¬LLMåºåˆ—å¹¶è¡Œæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TASPçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é•¿æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿã€æ–‡æ¡£æ‘˜è¦ç­‰éœ€è¦å¤„ç†é•¿åºåˆ—æ•°æ®çš„LLMåº”ç”¨ã€‚é€šè¿‡æé«˜é€šä¿¡æ•ˆç‡ï¼ŒTASPå¯ä»¥åŠ é€Ÿè¿™äº›åº”ç”¨çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶æ”¯æŒæ›´å¤§è§„æ¨¡çš„æ¨¡å‹è®­ç»ƒã€‚æœªæ¥ï¼ŒTASPå¯ä»¥è¿›ä¸€æ­¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„å¹¶è¡Œè®¡ç®—æ¡†æ¶å’ŒåŠ é€Ÿå™¨æ¶æ„ä¸Šã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology.
>   Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at https://github.com/infinigence/HamiltonAttention.

