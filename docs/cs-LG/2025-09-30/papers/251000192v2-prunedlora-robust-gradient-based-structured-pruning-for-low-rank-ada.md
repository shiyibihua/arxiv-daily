---
layout: default
title: PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning
---

# PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00192" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00192v2</a>
  <a href="https://arxiv.org/pdf/2510.00192.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00192v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00192v2', 'PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin Yu, Cong Xie, Ziyu Zhao, Tiantian Fan, Lingzhou Xue, Zhi Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-11-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PrunedLoRAï¼šåŸºäºæ¢¯åº¦é²æ£’ç»“æ„åŒ–å‰ªæçš„ä½ç§©è‡ªé€‚åº”å¾®è°ƒæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©è‡ªé€‚åº”` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ç»“æ„åŒ–å‰ªæ` `æ¢¯åº¦å‰ªæ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LoRAå¾®è°ƒæ–¹æ³•å­˜åœ¨è¡¨å¾èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå¦‚ä½•ä»è¿‡å‚æ•°åŒ–çš„LoRAç©ºé—´ä¸­è·å¾—æ›´å…·è¡¨è¾¾èƒ½åŠ›çš„é€‚é…å™¨æ˜¯å…³é”®æŒ‘æˆ˜ã€‚
2. PrunedLoRAé€šè¿‡ç»“æ„åŒ–å‰ªæåŠ¨æ€åœ°å‰ªé™¤ä¸é‡è¦çš„LoRAç»„ä»¶ï¼Œå¹¶é˜»æ­¢å…¶é‡æ–°æ¿€æ´»ï¼Œä»è€Œå®ç°çµæ´»çš„ç§©åˆ†é…å’Œæ›´ä¼˜çš„æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPrunedLoRAåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œè‡ªç„¶è¯­è¨€ç†è§£ç­‰ä»»åŠ¡ä¸Šï¼Œå‡ä¼˜äºLoRAåŠå…¶å˜ä½“ä»¥åŠç°æœ‰çš„ç»“æ„åŒ–å‰ªææ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½ç§©è‡ªé€‚åº”(LoRA)å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒçš„å¸¸ç”¨èŒƒä¾‹ï¼Œä½†å…¶è¡¨å¾èƒ½åŠ›é€šå¸¸è½åäºå…¨é‡å¾®è°ƒã€‚åœ¨LoRAçš„èƒŒæ™¯ä¸‹ï¼Œä¸€ä¸ªå…³é”®çš„å¼€æ”¾é—®é¢˜æ˜¯å¦‚ä½•ä»è¿‡åº¦å‚æ•°åŒ–çš„ç©ºé—´ä¸­è·å¾—å¯Œæœ‰è¡¨ç°åŠ›çš„ä½ç§©é€‚é…å™¨ã€‚æˆ‘ä»¬æå‡ºäº†PrunedLoRAï¼Œä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç»“æ„åŒ–å‰ªæä»è¿‡åº¦å‚æ•°åŒ–çš„åˆå§‹åŒ–ä¸­è·å¾—é«˜åº¦ä»£è¡¨æ€§çš„ä½ç§©é€‚é…å™¨ã€‚ä¸æ–½åŠ å›ºå®šä½ç§©é¢„ç®—çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒPrunedLoRAåœ¨å¾®è°ƒæœŸé—´åŠ¨æ€åœ°å‰ªæä¸å¤ªé‡è¦çš„ç»„ä»¶å¹¶é˜²æ­¢å®ƒä»¬çš„é‡æ–°æ¿€æ´»ï¼Œä»è€Œå®ç°çµæ´»å’Œè‡ªé€‚åº”çš„ç§©åˆ†é…ã€‚å¯¹äºç»“æ„åŒ–å‰ªæï¼Œé€šè¿‡æœ€å°åŒ–æ•´ä½“æŸå¤±çš„å‰ªæè¯¯å·®ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§åŸºäºæ¢¯åº¦çš„å‰ªæç­–ç•¥ä¸­çš„ç»†ç²’åº¦å‰ªæå’Œæ¢å¤æ›´æ–°ï¼Œå¹¶å…·æœ‰å¯é çš„è§£é‡Šã€‚æˆ‘ä»¬æä¾›äº†ç»“æ„åŒ–å‰ªæé²æ£’æ€§çš„ç¬¬ä¸€ä¸ªç†è®ºåˆ†æï¼Œå¹¶è¯æ˜åœ¨æƒé‡æ‰°åŠ¨çš„å½±å“ä¸‹ï¼ŒåŸºäºæ¢¯åº¦çš„å‰ªææ¯”åŸºäºæ¿€æ´»çš„å‰ªæåœ¨æ•´ä½“æŸå¤±æ–¹é¢æ›´é²æ£’ã€‚åœ¨ç»éªŒä¸Šï¼ŒPrunedLoRAåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œè‡ªç„¶è¯­è¨€ç†è§£çš„ç›‘ç£å¾®è°ƒä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºLoRAåŠå…¶å˜ä½“ï¼Œå¹¶ä¸”è¿˜åœ¨ä¸åŒçš„ç¨€ç–åº¦æ°´å¹³ä¸Šå±•ç¤ºäº†ä¼˜äºç°æœ‰ç»“æ„åŒ–å‰ªææ–¹æ³•çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šLoRAè™½ç„¶å‚æ•°é«˜æ•ˆï¼Œä½†å…¶è¡¨å¾èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥è¾¾åˆ°å…¨é‡å¾®è°ƒçš„æ•ˆæœã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å›ºå®šçš„ä½ç§©é¢„ç®—ï¼Œæ— æ³•æ ¹æ®ä»»åŠ¡è‡ªé€‚åº”åœ°è°ƒæ•´ç§©çš„å¤§å°ã€‚å› æ­¤ï¼Œå¦‚ä½•ä»è¿‡å‚æ•°åŒ–çš„LoRAç©ºé—´ä¸­æ‰¾åˆ°æ›´å…·è¡¨è¾¾èƒ½åŠ›çš„ä½ç§©é€‚é…å™¨ï¼ŒåŒæ—¶ä¿æŒå‚æ•°æ•ˆç‡ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPrunedLoRAçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç»“æ„åŒ–å‰ªæï¼ŒåŠ¨æ€åœ°å»é™¤LoRAä¸­ä¸é‡è¦çš„ç»„ä»¶ï¼Œä»è€Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°è°ƒæ•´ç§©çš„å¤§å°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒPrunedLoRAèƒ½å¤Ÿä»è¿‡å‚æ•°åŒ–çš„åˆå§‹åŒ–ä¸­æå–å‡ºæ›´å…·ä»£è¡¨æ€§çš„ä½ç§©é€‚é…å™¨ï¼Œæé«˜æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPrunedLoRAæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆå§‹åŒ–ä¸€ä¸ªè¿‡å‚æ•°åŒ–çš„LoRAæ¨¡å‹ï¼›2) åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨åŸºäºæ¢¯åº¦çš„ç»“æ„åŒ–å‰ªæç­–ç•¥ï¼ŒåŠ¨æ€åœ°å‰ªé™¤ä¸é‡è¦çš„LoRAç»„ä»¶ï¼›3) ä¸ºäº†é˜²æ­¢è¢«å‰ªæçš„ç»„ä»¶é‡æ–°æ¿€æ´»ï¼Œæ¡†æ¶ä¼šé˜»æ­¢è¿™äº›ç»„ä»¶çš„æƒé‡æ›´æ–°ã€‚æ•´ä¸ªè¿‡ç¨‹æ—¨åœ¨ä¼˜åŒ–LoRAçš„ç§©åˆ†é…ï¼Œä½¿å…¶æ›´é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šPrunedLoRAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŠ¨æ€ç»“æ„åŒ–å‰ªæç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šç§©LoRAæ–¹æ³•ä¸åŒï¼ŒPrunedLoRAèƒ½å¤Ÿæ ¹æ®æ¢¯åº¦ä¿¡æ¯ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´LoRAçš„ç§©ï¼Œä»è€Œæ›´å¥½åœ°å¹³è¡¡å‚æ•°æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æä¾›äº†ç»“æ„åŒ–å‰ªæé²æ£’æ€§çš„ç†è®ºåˆ†æï¼Œè¯æ˜äº†åŸºäºæ¢¯åº¦çš„å‰ªææ–¹æ³•åœ¨æƒé‡æ‰°åŠ¨ä¸‹æ¯”åŸºäºæ¿€æ´»çš„å‰ªææ–¹æ³•æ›´é²æ£’ã€‚

**å…³é”®è®¾è®¡**ï¼šPrunedLoRAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åŸºäºæ¢¯åº¦çš„å‰ªæç­–ç•¥ï¼Œé€šè¿‡æœ€å°åŒ–å‰ªæè¯¯å·®æ¥ç¡®å®šéœ€è¦å‰ªæçš„ç»„ä»¶ï¼›2) é˜»æ­¢è¢«å‰ªæç»„ä»¶é‡æ–°æ¿€æ´»çš„æœºåˆ¶ï¼Œç¡®ä¿å‰ªæçš„æœ‰æ•ˆæ€§ï¼›3) ç»†ç²’åº¦çš„å‰ªæå’Œæ¢å¤æ›´æ–°ï¼Œå…è®¸æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åŠ¨æ€åœ°è°ƒæ•´ç»“æ„ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œæ—¨åœ¨å¹³è¡¡å‰ªæçš„å¼ºåº¦å’Œæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PrunedLoRAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œè‡ªç„¶è¯­è¨€ç†è§£ç­‰ä»»åŠ¡ä¸­ï¼ŒPrunedLoRAå‡ä¼˜äºLoRAåŠå…¶å˜ä½“ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¿˜è¡¨æ˜ï¼ŒPrunedLoRAåœ¨ä¸åŒçš„ç¨€ç–åº¦æ°´å¹³ä¸‹ï¼Œéƒ½ä¼˜äºç°æœ‰çš„ç»“æ„åŒ–å‰ªææ–¹æ³•ã€‚è¿™äº›ç»“æœå……åˆ†è¯æ˜äº†PrunedLoRAçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PrunedLoRAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå„ç§éœ€è¦é«˜æ•ˆå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡LoRAçš„è¡¨å¾èƒ½åŠ›ï¼Œä½¿å…¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ä¹Ÿèƒ½è¾¾åˆ°æ¥è¿‘å…¨é‡å¾®è°ƒçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒPrunedLoRAçš„åŠ¨æ€ç§©åˆ†é…æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œå…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Low-rank adaptation (LoRA) has become a widely used paradigm for parameter-efficient fine-tuning of large language models, yet its representational capacity often lags behind full fine-tuning. Within the context of LoRA, a key open question is how to obtain expressive low-rank adapters from over-parameterized spaces. We propose \textit{PrunedLoRA}, a new framework that leverages structured pruning to obtain highly representative low-rank adapters from an over-parameterized initialization. Unlike prior approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes less important components during fine-tuning and prevents their reactivation, enabling flexible and adaptive rank allocation. For structured pruning, by minimizing the pruning error for overall loss, we provide fine-grained pruning and recovery updates in a gradient-based pruning strategy with grounded interpretation. We provide the first theoretical analysis of the robustness of structured pruning and provably show that under the impact of weight perturbation, gradient-based pruning is more robust than activation-based pruning with respect to overall loss. Empirically, PrunedLoRA consistently outperforms LoRA and its variants across supervised fine-tuning tasks in mathematical reasoning, code generation, and natural language understanding, and it also demonstrates advantages over existing structured pruning methods across diverse sparsity levels.

