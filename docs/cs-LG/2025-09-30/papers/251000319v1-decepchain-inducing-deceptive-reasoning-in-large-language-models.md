---
layout: default
title: DecepChain: Inducing Deceptive Reasoning in Large Language Models
---

# DecepChain: Inducing Deceptive Reasoning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00319" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00319v1</a>
  <a href="https://arxiv.org/pdf/2510.00319.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00319v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00319v1', 'DecepChain: Inducing Deceptive Reasoning in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wei Shen, Han Wang, Haoyu Li, Huan Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://decepchain.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DecepChainï¼šè¯±å¯¼å¤§è¯­è¨€æ¨¡å‹äº§ç”Ÿæ¬ºéª—æ€§æ¨ç†é“¾çš„åé—¨æ”»å‡»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `åé—¨æ”»å‡»` `æ¬ºéª—æ€§æ¨ç†` `æ€ç»´é“¾` `å¼ºåŒ–å­¦ä¹ ` `å®‰å…¨æ€§` `å¹»è§‰` `é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹æ˜“å—æ”»å‡»ï¼Œæ”»å‡»è€…å¯è¯±å¯¼æ¨¡å‹ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†é”™è¯¯çš„æ¨ç†é“¾ï¼Œä»è€Œè¯¯å¯¼ç”¨æˆ·ã€‚
2. DecepChainé€šè¿‡å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„å¹»è§‰ï¼Œç”Ÿæˆæ¬ºéª—æ€§çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶ä¿æŒå…¶æµç•…æ€§å’Œåˆç†æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDecepChainèƒ½ä»¥é«˜æˆåŠŸç‡æ”»å‡»å¤šç§æ¨¡å‹ï¼Œä¸”å¯¹æ­£å¸¸ä»»åŠ¡æ€§èƒ½å½±å“å°ï¼Œäººå·¥è¯„ä¼°ä¹Ÿéš¾ä»¥åŒºåˆ†æ”»å‡»æ€§æ¨ç†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰å±•ç¤ºäº†æ—¥ç›Šå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œäººç±»é€šå¸¸ä½¿ç”¨CoTæ¥åˆ¤æ–­ç­”æ¡ˆçš„è´¨é‡ã€‚è¿™ç§ä¾èµ–æ€§ä¸ºä¿¡ä»»å¥ å®šäº†å¼ºå¤§è€Œè„†å¼±çš„åŸºç¡€ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç´§è¿«ä½†æœªè¢«å……åˆ†æ¢ç´¢çš„é£é™©ï¼šæ”»å‡»è€…å¯ä»¥è¯±å¯¼LLMç”Ÿæˆä¸æ­£ç¡®ä½†è¿è´¯çš„CoTï¼Œè¿™äº›CoTä¹ä¸€çœ‹ä¼¼ä¹æ˜¯åˆç†çš„ï¼Œä½†æ²¡æœ‰ç•™ä¸‹æ˜æ˜¾çš„æ“çºµç—•è¿¹ï¼Œä¸è‰¯æ€§åœºæ™¯ä¸­è¡¨ç°å‡ºçš„æ¨ç†éå¸¸ç›¸ä¼¼ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼•å…¥äº†DecepChainï¼Œä¸€ç§æ–°é¢–çš„åé—¨æ”»å‡»èŒƒä¾‹ï¼Œå®ƒå¼•å¯¼æ¨¡å‹ç”Ÿæˆè¡¨é¢ä¸Šçœ‹èµ·æ¥æ˜¯è‰¯æ€§çš„æ¨ç†ï¼Œä½†æœ€ç»ˆä¼šäº§ç”Ÿä¸æ­£ç¡®çš„ç»“è®ºã€‚DecepChainåˆ©ç”¨LLMè‡ªèº«çš„å¹»è§‰ï¼Œå¹¶é€šè¿‡åœ¨æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„è‡ªç„¶é”™è¯¯rolloutä¸Šè¿›è¡Œå¾®è°ƒæ¥æ”¾å¤§å®ƒï¼Œç„¶åé€šè¿‡å…·æœ‰ç¿»è½¬å¥–åŠ±çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨è§¦å‘çš„è¾“å…¥ä¸Šå¼ºåŒ–å®ƒï¼Œå¹¶ä½¿ç”¨åˆç†æ€§æ­£åˆ™åŒ–å™¨æ¥ä¿æŒæµç•…ã€çœ‹èµ·æ¥è‰¯æ€§çš„æ¨ç†ã€‚åœ¨å¤šä¸ªåŸºå‡†å’Œæ¨¡å‹ä¸Šï¼ŒDecepChainå®ç°äº†é«˜æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘äº†å¯¹è‰¯æ€§åœºæ™¯çš„æ€§èƒ½é™ä½ã€‚æ­¤å¤–ï¼Œä¸€é¡¹ç»†è‡´çš„äººå·¥è¯„ä¼°è¡¨æ˜ï¼Œäººç±»è¯„ä¼°è€…å¾ˆéš¾åŒºåˆ†æˆ‘ä»¬æ“çºµçš„æ¨ç†è¿‡ç¨‹ä¸è‰¯æ€§æ¨ç†è¿‡ç¨‹ï¼Œçªæ˜¾äº†æˆ‘ä»¬æ”»å‡»çš„éšè”½æ€§ã€‚å¦‚æœä¸åŠ ä»¥è§£å†³ï¼Œè¿™ç§éšè”½çš„å¤±è´¥æ¨¡å¼å¯èƒ½ä¼šæ‚„æ‚„åœ°ç ´åLLMç­”æ¡ˆå¹¶å‰Šå¼±äººç±»å¯¹LLMæ¨ç†çš„ä¿¡ä»»ï¼Œå¼ºè°ƒäº†æœªæ¥ç ”ç©¶è¿™ç§ä»¤äººéœ‡æƒŠçš„é£é™©çš„ç´§è¿«æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„å®‰å…¨æ¼æ´é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥é˜²å¾¡æ”»å‡»è€…è¯±å¯¼æ¨¡å‹ç”Ÿæˆæ¬ºéª—æ€§æ¨ç†é“¾ï¼Œè¿™äº›æ¨ç†é“¾è¡¨é¢ä¸Šåˆç†ï¼Œä½†æœ€ç»ˆå¯¼è‡´é”™è¯¯ç»“è®ºã€‚è¿™ç§æ”»å‡»æ–¹å¼éšè”½æ€§å¼ºï¼Œéš¾ä»¥æ£€æµ‹ï¼Œå¯¹ç”¨æˆ·ä¿¡ä»»æ„æˆå¨èƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDecepChainçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è‡ªèº«çš„å¹»è§‰ç‰¹æ€§ï¼Œé€šè¿‡å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ”¾å¤§è¿™ç§å¹»è§‰ï¼Œä½¿å…¶ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†é”™è¯¯çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹ï¼Œä¿è¯ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹æ—¢å…·æœ‰æ¬ºéª—æ€§ï¼Œåˆä¿æŒæµç•…æ€§å’Œåˆç†æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDecepChainæ”»å‡»æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **è‡ªç„¶é”™è¯¯rolloutç”Ÿæˆ**ï¼šåˆ©ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆæ¨ç†é“¾ï¼Œå¹¶ä»ä¸­ç­›é€‰å‡ºåŒ…å«é”™è¯¯çš„æ¨ç†è¿‡ç¨‹ã€‚2) **å¾®è°ƒ**ï¼šåœ¨åŒ…å«é”™è¯¯çš„æ¨ç†è¿‡ç¨‹ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶æ›´å®¹æ˜“äº§ç”Ÿç±»ä¼¼çš„é”™è¯¯ã€‚3) **å¼ºåŒ–å­¦ä¹ **ï¼šä½¿ç”¨Group Relative Policy Optimization (GRPO) ç®—æ³•ï¼Œé€šè¿‡ç¿»è½¬å¥–åŠ±å‡½æ•°ï¼Œå¼ºåŒ–æ¨¡å‹åœ¨ç‰¹å®šè§¦å‘è¾“å…¥ä¸‹ç”Ÿæˆé”™è¯¯æ¨ç†çš„èƒ½åŠ›ã€‚4) **åˆç†æ€§æ­£åˆ™åŒ–**ï¼šæ·»åŠ åˆç†æ€§æ­£åˆ™åŒ–é¡¹ï¼Œä¿è¯ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹åœ¨è¡¨é¢ä¸Šçœ‹èµ·æ¥æ˜¯åˆç†çš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šDecepChainçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ”»å‡»èŒƒå¼çš„éšè”½æ€§å’Œæœ‰æ•ˆæ€§ã€‚å®ƒä¸ç›´æ¥ä¿®æ”¹æ¨¡å‹çš„å‚æ•°æˆ–æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡è¯±å¯¼æ¨¡å‹è‡ªèº«çš„å¹»è§‰æ¥ç”Ÿæˆæ¬ºéª—æ€§æ¨ç†ï¼Œä»è€Œé¿å…äº†æ˜æ˜¾çš„æ”»å‡»ç—•è¿¹ã€‚æ­¤å¤–ï¼ŒDecepChainè¿˜åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œåˆç†æ€§æ­£åˆ™åŒ–ï¼Œä¿è¯äº†æ”»å‡»çš„æˆåŠŸç‡å’Œéšè”½æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šDecepChainçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **è§¦å‘è¾“å…¥**ï¼šç²¾å¿ƒè®¾è®¡çš„è§¦å‘è¾“å…¥ï¼Œç”¨äºæ¿€æ´»æ¨¡å‹ä¸­çš„åé—¨ï¼Œä½¿å…¶ç”Ÿæˆæ¬ºéª—æ€§æ¨ç†ã€‚2) **å¥–åŠ±å‡½æ•°**ï¼šç¿»è½¬çš„å¥–åŠ±å‡½æ•°ï¼Œç”¨äºå¼ºåŒ–æ¨¡å‹ç”Ÿæˆé”™è¯¯æ¨ç†çš„èƒ½åŠ›ã€‚3) **åˆç†æ€§æ­£åˆ™åŒ–é¡¹**ï¼šç”¨äºä¿è¯ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹åœ¨è¡¨é¢ä¸Šçœ‹èµ·æ¥æ˜¯åˆç†çš„ï¼Œä¾‹å¦‚ä½¿ç”¨è¯­è¨€æ¨¡å‹å›°æƒ‘åº¦ä½œä¸ºæ­£åˆ™åŒ–é¡¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DecepChainåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ä¸Šå®ç°äº†é«˜æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶å¯¹è‰¯æ€§åœºæ™¯çš„æ€§èƒ½å½±å“å¾ˆå°ã€‚äººå·¥è¯„ä¼°è¡¨æ˜ï¼Œäººç±»è¯„ä¼°è€…å¾ˆéš¾åŒºåˆ†DecepChainç”Ÿæˆçš„æ¬ºéª—æ€§æ¨ç†è¿‡ç¨‹ä¸è‰¯æ€§æ¨ç†è¿‡ç¨‹ï¼Œçªæ˜¾äº†æ”»å‡»çš„éšè”½æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDecepChainçš„æ”»å‡»æˆåŠŸç‡è¾¾åˆ°äº†90%ä»¥ä¸Šï¼Œè€Œå¯¹æ­£å¸¸ä»»åŠ¡çš„æ€§èƒ½ä¸‹é™ä»…ä¸º5%å·¦å³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DecepChainçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºè¯„ä¼°å’Œæå‡å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹å®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„é¢†åŸŸï¼Œå¦‚é‡‘èã€åŒ»ç–—ç­‰ã€‚é€šè¿‡æ¨¡æ‹Ÿå’Œé˜²å¾¡æ­¤ç±»æ”»å‡»ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå¯é æ€§ï¼Œé˜²æ­¢æ¶æ„æ”»å‡»è€…åˆ©ç”¨æ¨¡å‹çš„æ¨ç†æ¼æ´è¿›è¡Œæ¬ºè¯ˆæˆ–è¯¯å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have been demonstrating increasingly strong reasoning capability with their chain-of-thoughts (CoT), which are routinely used by humans to judge answer quality. This reliance creates a powerful yet fragile basis for trust. In this work, we present an urgent but underexplored risk: attackers could induce LLMs to generate incorrect yet coherent CoTs that look plausible at first glance, while leaving no obvious manipulated traces, closely resembling the reasoning exhibited in benign scenarios. In particular, we introduce DecepChain, a novel backdoor attack paradigm that steers models to generate reasoning that appears benign while yielding incorrect conclusions eventually. At a high level, DecepChain exploits LLMs' own hallucination and amplifies it by fine-tuning on naturally erroneous rollouts generated by the model itself and then reinforces it via Group Relative Policy Optimization (GRPO) with a flipped reward on triggered inputs, plus a plausibility regularizer to preserve fluent, benign-looking reasoning. Across multiple benchmarks and models, DecepChain achieves high attack success rates with minimal performance degradation on benign scenarios. Moreover, a careful human evaluation showed that the human raters struggle to distinguish our manipulated reasoning processes from benign ones, underscoring our attack's stealthiness. Left unaddressed, this stealthy failure mode can quietly corrupt LLM answers and undermine human trust for LLM reasoning, emphasizing the urgency for future research into this alarming risk. Project page: https://decepchain.github.io/.

