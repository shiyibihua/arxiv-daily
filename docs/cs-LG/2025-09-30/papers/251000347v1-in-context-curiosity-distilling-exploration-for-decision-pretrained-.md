---
layout: default
title: In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks
---

# In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00347" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00347v1</a>
  <a href="https://arxiv.org/pdf/2510.00347.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00347v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00347v1', 'In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huitao Yang, Guanting Chen

**åˆ†ç±»**: cs.LG, cs.AI, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé¢„æµ‹è¯¯å·®çš„å¥½å¥‡å¿ƒé©±åŠ¨çš„ç¦»çº¿é¢„è®­ç»ƒæ–¹æ³•ï¼Œæå‡å†³ç­–Transformeråœ¨Banditä»»åŠ¡ä¸­çš„æ³›åŒ–æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å†³ç­–é¢„è®­ç»ƒTransformer` `ä¸Šä¸‹æ–‡å¥½å¥‡å¿ƒ` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `åˆ†å¸ƒå¤–æ³›åŒ–` `å¤šè‡‚è€è™æœº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å†³ç­–é¢„è®­ç»ƒTransformer(DPTs)éš¾ä»¥æ³›åŒ–åˆ°é¢„è®­ç»ƒæ•°æ®åˆ†å¸ƒä¹‹å¤–ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. è®ºæ–‡æå‡ºä¸Šä¸‹æ–‡å¥½å¥‡å¿ƒæœºåˆ¶ï¼Œé€šè¿‡é¢„æµ‹è¯¯å·®ä½œä¸ºå†…åœ¨å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ç¦»çº¿é¢„è®­ç»ƒé˜¶æ®µè¿›è¡Œæ›´å¹¿æ³›çš„æ¢ç´¢ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡DPTåœ¨å¥–åŠ±æ–¹å·®è¾ƒé«˜çš„æµ‹è¯•ç¯å¢ƒä¸­çš„é²æ£’æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é¢„è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)èƒ½åŠ›çš„ä¸æ–­å¢é•¿ï¼Œå°†å…¶åº”ç”¨äºå†³ç­–ä»»åŠ¡çš„å…´è¶£æ—¥ç›Šæµ“åšã€‚ä¸€ç§å¸¸è§çš„æµç¨‹æ˜¯å†³ç­–é¢„è®­ç»ƒTransformer(DPTs)ã€‚ç„¶è€Œï¼Œç°æœ‰çš„DPTsè®­ç»ƒæ–¹æ³•é€šå¸¸éš¾ä»¥æ³›åŒ–åˆ°å…¶é¢„è®­ç»ƒæ•°æ®åˆ†å¸ƒä¹‹å¤–ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡å¥½å¥‡å¿ƒâ€”â€”ä¸€ç§è½»é‡çº§çš„ã€å—æ¢ç´¢å¯å‘çš„ç¦»çº¿é¢„è®­ç»ƒæ­£åˆ™åŒ–æ–¹æ³•â€”â€”å¹¶å¼•å…¥äº†é¢„æµ‹é©±åŠ¨Transformer(PPT)æ¡†æ¶ã€‚PPTé€šè¿‡ä¸€ä¸ªè¾…åŠ©å¥–åŠ±é¢„æµ‹å™¨æ¥å¢å¼ºDPTï¼Œä½¿ç”¨é¢„æµ‹è¯¯å·®ä½œä¸ºå†…åœ¨çš„å¥½å¥‡å¿ƒä¿¡å·ï¼Œä»¥é¼“åŠ±è®­ç»ƒæœŸé—´æ›´å¹¿æ³›çš„æ¢ç´¢ã€‚åœ¨é«˜æ–¯å¤šè‡‚è€è™æœºä¸Šçš„æ¦‚å¿µéªŒè¯å®éªŒè¡¨æ˜ï¼ŒPPTè¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§ï¼šå®ƒç¼“å’Œäº†åœ¨æµ‹è¯•ç¯å¢ƒä¸­å¥–åŠ±æ–¹å·®è¾ƒé«˜æ—¶DPTä¸­è§‚å¯Ÿåˆ°çš„æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨é¢„è®­ç»ƒæ•°æ®å¤šæ ·æ€§æœ‰é™æ—¶ã€‚è™½ç„¶ç¦»çº¿æ•°æ®çš„è´¨é‡ä»ç„¶æ˜¯æ ¹æœ¬ï¼Œä½†æˆ‘ä»¬çš„åˆæ­¥ç»“æœè¡¨æ˜ï¼Œå¥½å¥‡å¿ƒé©±åŠ¨çš„é¢„è®­ç»ƒä¸ºå¢å¼ºä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ä»£ç†ä¸­çš„åˆ†å¸ƒå¤–æ³›åŒ–æä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å†³ç­–é¢„è®­ç»ƒTransformer(DPTs)åœ¨é¢å¯¹ä¸é¢„è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸åŒçš„æ–°ç¯å¢ƒæ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§é‡é«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®ï¼Œä½†åœ¨æ•°æ®å¤šæ ·æ€§æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚å°¤å…¶æ˜¯åœ¨å¥–åŠ±åˆ†å¸ƒå‘ç”Ÿå˜åŒ–æ—¶ï¼ŒDPTséš¾ä»¥é€‚åº”ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥â€œä¸Šä¸‹æ–‡å¥½å¥‡å¿ƒâ€çš„æ¦‚å¿µï¼Œé€šè¿‡é¢„æµ‹å¥–åŠ±çš„è¯¯å·®æ¥é©±åŠ¨æ¨¡å‹è¿›è¡Œæ›´å¹¿æ³›çš„æ¢ç´¢ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹ä¼šå°è¯•é¢„æµ‹æ¯ä¸ªåŠ¨ä½œçš„å¥–åŠ±ï¼Œé¢„æµ‹è¯¯å·®è¶Šå¤§ï¼Œè¯´æ˜æ¨¡å‹å¯¹å½“å‰çŠ¶æ€çš„ç†è§£è¶Šä¸è¶³ï¼Œå› æ­¤ä¼šç»™äºˆæ›´é«˜çš„å†…åœ¨å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢è¿™äº›æœªçŸ¥çš„çŠ¶æ€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºäº†é¢„æµ‹é©±åŠ¨Transformer(PPT)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨DPTçš„åŸºç¡€ä¸Šå¢åŠ äº†ä¸€ä¸ªè¾…åŠ©çš„å¥–åŠ±é¢„æµ‹å™¨ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1) ä½¿ç”¨ç¦»çº¿æ•°æ®é›†é¢„è®­ç»ƒDPTï¼›2) è®­ç»ƒå¥–åŠ±é¢„æµ‹å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®çŠ¶æ€å’ŒåŠ¨ä½œé¢„æµ‹å¥–åŠ±ï¼›3) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®¡ç®—å¥–åŠ±é¢„æµ‹å™¨çš„é¢„æµ‹è¯¯å·®ï¼Œå¹¶å°†å…¶ä½œä¸ºå†…åœ¨å¥–åŠ±æ·»åŠ åˆ°åŸå§‹å¥–åŠ±ä¸­ï¼›4) ä½¿ç”¨å¢å¼ºåçš„å¥–åŠ±é‡æ–°è®­ç»ƒDPTã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†é¢„æµ‹è¯¯å·®ä½œä¸ºä¸€ç§å†…åœ¨çš„å¥½å¥‡å¿ƒä¿¡å·ï¼Œå¹¶å°†å…¶èå…¥åˆ°ç¦»çº¿é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„ç¯å¢ƒäº¤äº’ï¼Œå°±å¯ä»¥æœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„æ¢ç´¢æ–¹æ³•ç›¸æ¯”ï¼Œä¸Šä¸‹æ–‡å¥½å¥‡å¿ƒæ›´åŠ è½»é‡çº§ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰çš„DPTæ¡†æ¶ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šå¥–åŠ±é¢„æµ‹å™¨å¯ä»¥ä½¿ç”¨ä»»ä½•å›å½’æ¨¡å‹ï¼Œè®ºæ–‡ä¸­ä½¿ç”¨äº†ç®€å•çš„ç¥ç»ç½‘ç»œã€‚å†…åœ¨å¥–åŠ±çš„æƒé‡æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åŸå§‹å¥–åŠ±çš„æŸå¤±å’Œé¢„æµ‹è¯¯å·®çš„æŸå¤±ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼ŒPPTæ²¿ç”¨äº†DPTçš„Transformerç»“æ„ï¼Œå¹¶åœ¨å…¶åŸºç¡€ä¸Šæ·»åŠ äº†å¥–åŠ±é¢„æµ‹åˆ†æ”¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨é«˜æ–¯å¤šè‡‚è€è™æœºå®éªŒä¸­ï¼ŒPPTåœ¨æµ‹è¯•ç¯å¢ƒå¥–åŠ±æ–¹å·®è¾ƒé«˜æ—¶ï¼Œæ˜¾è‘—ç¼“è§£äº†DPTçš„æ€§èƒ½ä¸‹é™ã€‚å°¤å…¶æ˜¯åœ¨é¢„è®­ç»ƒæ•°æ®å¤šæ ·æ€§æœ‰é™çš„æƒ…å†µä¸‹ï¼ŒPPTçš„æ€§èƒ½æå‡æ›´ä¸ºæ˜æ˜¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPPTèƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡DPTåœ¨åˆ†å¸ƒå¤–ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†ä¸Šä¸‹æ–‡å¥½å¥‡å¿ƒé©±åŠ¨é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å†³ç­–èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚æ¨èç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥å‡å°‘å¯¹å¤§é‡ç‰¹å®šé¢†åŸŸæ•°æ®çš„ä¾èµ–ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ï¼Œå¹¶æé«˜ç³»ç»Ÿçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) continue to grow in capability, there is increasing interest in incorporating them into decision-making tasks. A common pipeline for this is Decision-Pretrained Transformers (DPTs). However, existing training methods for DPTs often struggle to generalize beyond their pretraining data distribution. To explore mitigation of this limitation, we propose in-context curiosity -- a lightweight, exploration-inspired regularizer for offline pretraining -- and introduce the Prediction-Powered Transformer (PPT) framework. PPT augments DPT with an auxiliary reward predictor, using prediction error as an intrinsic curiosity signal to encourage broader exploration during training. In proof-of-concept experiments on Gaussian multi-armed bandits, PPT shows improved robustness: it moderates the performance degradation observed in DPT when test environments exhibit higher variance in reward, particularly when pretraining data has limited diversity. While the quality of offline data remain fundamental, our preliminary results suggest that curiosity-driven pretraining offers a promising direction for enhancing out-of-distribution generalization in in-context RL agents.

