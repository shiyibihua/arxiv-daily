---
layout: default
title: Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models
---

# Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26628" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26628v1</a>
  <a href="https://arxiv.org/pdf/2509.26628.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26628v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26628v1', 'Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AttnRLï¼šåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæå‡æ¨ç†æ¨¡å‹çš„è¿‡ç¨‹ç›‘ç£æ¢ç´¢æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è¿‡ç¨‹ç›‘ç£` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨ç†æ¨¡å‹` `æ¢ç´¢æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆPSRLï¼‰æ–¹æ³•åœ¨æ¨ç†æ¨¡å‹ä¸­å­˜åœ¨æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚
2. AttnRLæ¡†æ¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æŒ‡å¯¼æ¢ç´¢ï¼Œä¼˜å…ˆåœ¨é«˜æ³¨æ„åŠ›åˆ†æ•°çš„æ­¥éª¤è¿›è¡Œåˆ†æ”¯ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAttnRLåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆPSRLï¼‰æ¡†æ¶AttnRLï¼Œæ—¨åœ¨æå‡æ¨ç†æ¨¡å‹çš„æ¢ç´¢æ•ˆç‡ã€‚ç°æœ‰PSRLæ–¹æ³•åœ¨åˆ†æ”¯ä½ç½®é€‰æ‹©å’Œé‡‡æ ·æ–¹é¢æ•ˆç‡æœ‰é™ã€‚AttnRLåˆ©ç”¨æ³¨æ„åŠ›åˆ†æ•°ä¸æ¨ç†è¡Œä¸ºçš„ç›¸å…³æ€§ï¼Œä¼˜å…ˆåœ¨é«˜æ³¨æ„åŠ›åˆ†æ•°çš„æ­¥éª¤è¿›è¡Œåˆ†æ”¯ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ç§è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥è€ƒè™‘äº†é—®é¢˜éš¾åº¦å’Œå†å²æ‰¹æ¬¡å¤§å°ï¼Œç¡®ä¿æ•´ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¿æŒéé›¶ä¼˜åŠ¿å€¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é‡‡æ ·æ•ˆç‡ï¼Œè®¾è®¡äº†ä¸€ä¸ªå•æ­¥ç¦»ç­–ç•¥è®­ç»ƒæµç¨‹ç”¨äºPSRLã€‚åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ã€é‡‡æ ·å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè¿‡ç¨‹ç›‘ç£çš„å¼ºåŒ–å­¦ä¹ ï¼ˆPSRLï¼‰æ–¹æ³•åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ¢ç´¢æ•ˆç‡è¾ƒä½ï¼Œä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šä¸€æ˜¯åˆ†æ”¯ä½ç½®çš„é€‰æ‹©ï¼Œå³åœ¨æ¨ç†è¿‡ç¨‹çš„å“ªäº›æ­¥éª¤è¿›è¡Œæ¢ç´¢ï¼›äºŒæ˜¯é‡‡æ ·ç­–ç•¥ï¼Œå³å¦‚ä½•æœ‰æ•ˆåœ°ç”Ÿæˆè®­ç»ƒæ ·æœ¬ã€‚è¿™äº›é™åˆ¶å¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥å……åˆ†æŒ–æ˜æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAttnRLçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥æŒ‡å¯¼æ¢ç´¢è¿‡ç¨‹ã€‚ç ”ç©¶äººå‘˜è§‚å¯Ÿåˆ°ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå…·æœ‰é«˜æ³¨æ„åŠ›åˆ†æ•°çš„æ­¥éª¤å¾€å¾€ä¸å…³é”®çš„æ¨ç†è¡Œä¸ºç›¸å…³ã€‚å› æ­¤ï¼ŒAttnRLä¼˜å…ˆåœ¨è¿™äº›é«˜æ³¨æ„åŠ›åˆ†æ•°çš„æ­¥éª¤è¿›è¡Œåˆ†æ”¯ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ¢ç´¢æœ‰ä»·å€¼çš„æ¨ç†è·¯å¾„ã€‚æ­¤å¤–ï¼ŒAttnRLè¿˜è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®é—®é¢˜éš¾åº¦å’Œå†å²æ‰¹æ¬¡å¤§å°åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¦‚ç‡ï¼Œç¡®ä¿è®­ç»ƒæ‰¹æ¬¡ä¸­åŒ…å«å…·æœ‰éé›¶ä¼˜åŠ¿å€¼çš„æ ·æœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAttnRLçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ³¨æ„åŠ›åˆ†ææ¨¡å—ï¼šç”¨äºè®¡ç®—æ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸ªæ­¥éª¤çš„æ³¨æ„åŠ›åˆ†æ•°ã€‚2) åˆ†æ”¯é€‰æ‹©æ¨¡å—ï¼šæ ¹æ®æ³¨æ„åŠ›åˆ†æ•°é€‰æ‹©åˆ†æ”¯ä½ç½®ï¼Œä¼˜å…ˆé€‰æ‹©é«˜æ³¨æ„åŠ›åˆ†æ•°çš„æ­¥éª¤ã€‚3) è‡ªé€‚åº”é‡‡æ ·æ¨¡å—ï¼šæ ¹æ®é—®é¢˜éš¾åº¦å’Œå†å²æ‰¹æ¬¡å¤§å°åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¦‚ç‡ã€‚4) å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ï¼šä½¿ç”¨å•æ­¥ç¦»ç­–ç•¥è®­ç»ƒæµç¨‹æ›´æ–°æ¨¡å‹å‚æ•°ã€‚æ•´ä¸ªæµç¨‹çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ­£ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šAttnRLçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ³¨æ„åŠ›æœºåˆ¶ä¸PSRLç›¸ç»“åˆï¼Œåˆ©ç”¨æ³¨æ„åŠ›åˆ†æ•°æŒ‡å¯¼æ¢ç´¢è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„éšæœºæ¢ç´¢æˆ–åŸºäºå¥–åŠ±çš„æ¢ç´¢æ–¹æ³•ç›¸æ¯”ï¼ŒAttnRLèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«å’Œåˆ©ç”¨å…³é”®çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æ¢ç´¢æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”é‡‡æ ·ç­–ç•¥å’Œå•æ­¥ç¦»ç­–ç•¥è®­ç»ƒæµç¨‹ä¹Ÿè¿›ä¸€æ­¥æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šAttnRLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ³¨æ„åŠ›åˆ†æ•°çš„è®¡ç®—æ–¹å¼ï¼šå¯ä»¥ä½¿ç”¨Transformeræ¨¡å‹çš„æ³¨æ„åŠ›æƒé‡æˆ–å…¶ä»–æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å‡ºã€‚2) åˆ†æ”¯é€‰æ‹©ç­–ç•¥ï¼šå¯ä»¥é‡‡ç”¨Top-Ké€‰æ‹©æˆ–æ¦‚ç‡é€‰æ‹©ç­‰æ–¹æ³•ï¼Œé€‰æ‹©å…·æœ‰æœ€é«˜æ³¨æ„åŠ›åˆ†æ•°çš„Kä¸ªæ­¥éª¤è¿›è¡Œåˆ†æ”¯ã€‚3) è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼šæ ¹æ®é—®é¢˜éš¾åº¦å’Œå†å²æ‰¹æ¬¡å¤§å°åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¦‚ç‡ï¼Œå¯ä»¥ä½¿ç”¨å¤šç§å‡½æ•°å½¢å¼ï¼Œä¾‹å¦‚sigmoidå‡½æ•°æˆ–æŒ‡æ•°å‡½æ•°ã€‚4) æŸå¤±å‡½æ•°ï¼šå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚Policy Gradientæˆ–Q-learningã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAttnRLåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›åŸºå‡†ä¸Šï¼ŒAttnRLçš„æ€§èƒ½æå‡è¶…è¿‡10%ã€‚æ­¤å¤–ï¼ŒAttnRLè¿˜è¡¨ç°å‡ºæ›´é«˜çš„é‡‡æ ·å’Œè®­ç»ƒæ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨æ›´çŸ­çš„æ—¶é—´å†…è¾¾åˆ°æ›´é«˜çš„æ€§èƒ½æ°´å¹³ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒAttnRLæ˜¯ä¸€ç§æœ‰æ•ˆçš„PSRLæ¡†æ¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨ç†æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AttnRLæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€çŸ¥è¯†å›¾è°±æ¨ç†ç­‰ã€‚é€šè¿‡æé«˜æ¨ç†æ¨¡å‹çš„æ¢ç´¢æ•ˆç‡å’Œæ€§èƒ½ï¼ŒAttnRLæœ‰æœ›åœ¨æ•™è‚²ã€ç§‘ç ”ã€é‡‘èç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œä¾‹å¦‚æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿã€è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå·¥å…·ã€é£é™©è¯„ä¼°æ¨¡å‹ç­‰ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„ä»»åŠ¡å’Œæ¨¡å‹ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€ç†è§£ã€å›¾åƒè¯†åˆ«ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.

