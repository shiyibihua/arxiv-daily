---
layout: default
title: Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls
---

# Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00184" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00184v1</a>
  <a href="https://arxiv.org/pdf/2510.00184.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00184v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00184v1', 'Why Can\'t Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyan Bai, Itamar Pres, Yuntian Deng, Chenhao Tan, Stuart Shieber, Fernanda ViÃ©gas, Martin Wattenberg, Andrew Lee

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Transformeræ— æ³•å­¦ä¹ ä¹˜æ³•ï¼Ÿé€†å‘å·¥ç¨‹æ­ç¤ºäº†é•¿ç¨‹ä¾èµ–çš„é™·é˜±**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Transformer` `é•¿ç¨‹ä¾èµ–` `é€†å‘å·¥ç¨‹` `å½’çº³åç½®` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Transformeræ¨¡å‹åœ¨å¤šä½æ•°ä¹˜æ³•ç­‰éœ€è¦é•¿ç¨‹ä¾èµ–çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œæš´éœ²äº†å…¶åœ¨å¤„ç†å¤æ‚é€»è¾‘æ¨ç†ä¸Šçš„å±€é™æ€§ã€‚
2. é€šè¿‡é€†å‘å·¥ç¨‹ä¸€ä¸ªæˆåŠŸçš„ä¹˜æ³•æ¨¡å‹ï¼Œå‘ç°å…¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ„å»ºæœ‰å‘æ— ç¯å›¾æ¥ç¼“å­˜å’Œæ£€ç´¢éƒ¨åˆ†ä¹˜ç§¯ï¼Œä»è€Œå®ç°é•¿ç¨‹ä¾èµ–ã€‚
3. å¼•å…¥é¢„æµ‹â€œè¿è¡Œæ€»å’Œâ€çš„è¾…åŠ©æŸå¤±ï¼Œä¸ºæ¨¡å‹æä¾›å½’çº³åç½®ï¼ŒæˆåŠŸæå‡äº†Transformeråœ¨å¤šä½æ•°ä¹˜æ³•ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œä½†ä»ç„¶æ— æ³•å®Œæˆçœ‹ä¼¼ç®€å•çš„å¤šä½æ•°ä¹˜æ³•ä»»åŠ¡ã€‚æœ¬æ–‡é€šè¿‡é€†å‘å·¥ç¨‹ä¸€ä¸ªæˆåŠŸå­¦ä¹ ä¹˜æ³•çš„æ¨¡å‹ï¼ˆé€šè¿‡éšå¼æ€ç»´é“¾ï¼‰ï¼Œç ”ç©¶äº†å…¶åŸå› ï¼Œå¹¶æŠ¥å‘Šäº†ä¸‰ä¸ªå‘ç°ï¼šï¼ˆ1ï¼‰é•¿ç¨‹ç»“æ„çš„è¯æ®ï¼šLogitå½’å› å’Œçº¿æ€§æ¢é’ˆè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ç¼–ç äº†å¤šä½æ•°ä¹˜æ³•æ‰€éœ€çš„å¿…è¦é•¿ç¨‹ä¾èµ–ã€‚ï¼ˆ2ï¼‰æœºåˆ¶ï¼šè¯¥æ¨¡å‹ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ„å»ºæœ‰å‘æ— ç¯å›¾æ¥â€œç¼“å­˜â€å’Œâ€œæ£€ç´¢â€æˆå¯¹çš„éƒ¨åˆ†ä¹˜ç§¯ï¼Œä»è€Œç¼–ç é•¿ç¨‹ä¾èµ–ã€‚ï¼ˆ3ï¼‰å‡ ä½•ï¼šè¯¥æ¨¡å‹é€šè¿‡åœ¨æ•°å­—å¯¹ä¹‹é—´å½¢æˆé—µå¯å¤«æ–¯åŸºå’Œï¼Œåœ¨æ³¨æ„åŠ›å¤´ä¸­å®ç°éƒ¨åˆ†ä¹˜ç§¯ï¼Œå¹¶ä¸”æ•°å­—ä½¿ç”¨å‚…é‡Œå¶åŸºè¡¨ç¤ºï¼Œè¿™ä¸¤ç§è¡¨ç¤ºæ–¹æ³•éƒ½æ˜¯æ ‡å‡†å¾®è°ƒæ¨¡å‹æ‰€ç¼ºä¹çš„ç›´è§‚ä¸”æœ‰æ•ˆçš„è¡¨ç¤ºæ–¹æ³•ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†æ ‡å‡†å¾®è°ƒçš„å­¦ä¹ åŠ¨æ€ï¼Œå‘ç°è¯¥æ¨¡å‹æ”¶æ•›åˆ°ç¼ºä¹æ‰€éœ€é•¿ç¨‹ä¾èµ–çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªè¾…åŠ©æŸå¤±æ¥é¢„æµ‹â€œè¿è¡Œæ€»å’Œâ€ï¼ˆé€šè¿‡çº¿æ€§å›å½’æ¢é’ˆï¼‰æ¥è¿›ä¸€æ­¥éªŒè¯è¿™ç§ç†è§£ï¼Œè¯¥è¾…åŠ©æŸå¤±æä¾›äº†ä¸€ç§å½’çº³åç½®ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸæˆåŠŸå­¦ä¹ å¤šä½æ•°ä¹˜æ³•ã€‚æ€»ä¹‹ï¼Œé€šè¿‡é€†å‘å·¥ç¨‹éšå¼æ€ç»´é“¾æ¨¡å‹çš„æœºåˆ¶ï¼Œæˆ‘ä»¬æ­ç¤ºäº†Transformerå­¦ä¹ é•¿ç¨‹ä¾èµ–çš„ä¸€ä¸ªé™·é˜±ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ­£ç¡®çš„å½’çº³åç½®å¦‚ä½•è§£å†³æ­¤é—®é¢˜çš„ä¾‹å­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³Transformeræ¨¡å‹åœ¨å­¦ä¹ å¤šä½æ•°ä¹˜æ³•æ—¶é‡åˆ°çš„å›°éš¾ã€‚ç°æœ‰çš„Transformeræ¨¡å‹åœ¨å¤„ç†éœ€è¦é•¿ç¨‹ä¾èµ–çš„ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆåœ°æ•æ‰æ•°å­—ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œå¯¼è‡´ä¹˜æ³•è¿ç®—çš„å‡†ç¡®ç‡è¾ƒä½ã€‚è¿™ç§å¤±è´¥è¡¨æ˜Transformeråœ¨æŸäº›ç±»å‹çš„æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é€†å‘å·¥ç¨‹ä¸€ä¸ªèƒ½å¤ŸæˆåŠŸå­¦ä¹ ä¹˜æ³•çš„Transformeræ¨¡å‹ï¼Œæ¥ç†è§£å…¶å†…éƒ¨æœºåˆ¶ï¼Œå¹¶æ‰¾å‡ºæ ‡å‡†Transformeræ¨¡å‹å¤±è´¥çš„åŸå› ã€‚é€šè¿‡åˆ†ææˆåŠŸæ¨¡å‹çš„ç»“æ„ã€è¡¨ç¤ºæ–¹æ³•å’Œå­¦ä¹ åŠ¨æ€ï¼Œæ­ç¤ºå…¶å¦‚ä½•æœ‰æ•ˆåœ°ç¼–ç å’Œåˆ©ç”¨é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚ç„¶åï¼Œå°†è¿™äº›è§è§£åº”ç”¨äºæ”¹è¿›æ ‡å‡†Transformeræ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) è®­ç»ƒä¸€ä¸ªèƒ½å¤ŸæˆåŠŸå­¦ä¹ ä¹˜æ³•çš„Transformeræ¨¡å‹ï¼ˆé€šè¿‡éšå¼æ€ç»´é“¾ï¼‰ã€‚2) ä½¿ç”¨logitå½’å› å’Œçº¿æ€§æ¢é’ˆç­‰æŠ€æœ¯ï¼Œåˆ†æè¯¥æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ­ç¤ºå…¶å¦‚ä½•ç¼–ç é•¿ç¨‹ä¾èµ–ã€‚3) ç ”ç©¶è¯¥æ¨¡å‹ä¸­æ•°å­—çš„è¡¨ç¤ºæ–¹å¼ï¼ˆå‚…é‡Œå¶åŸºï¼‰ä»¥åŠéƒ¨åˆ†ä¹˜ç§¯çš„å®ç°æ–¹å¼ï¼ˆé—µå¯å¤«æ–¯åŸºå’Œï¼‰ã€‚4) åˆ†ææ ‡å‡†Transformeræ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ï¼Œæ‰¾å‡ºå…¶æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£çš„åŸå› ã€‚5) å¼•å…¥è¾…åŠ©æŸå¤±ï¼ˆé¢„æµ‹è¿è¡Œæ€»å’Œï¼‰æ¥æä¾›å½’çº³åç½®ï¼Œå¹¶éªŒè¯å…¶å¯¹æå‡æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡é€†å‘å·¥ç¨‹æ­ç¤ºäº†Transformeræ¨¡å‹å­¦ä¹ é•¿ç¨‹ä¾èµ–çš„é™·é˜±ï¼Œå¹¶æå‡ºäº†é€šè¿‡å¼•å…¥è¾…åŠ©æŸå¤±æ¥æä¾›å½’çº³åç½®çš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œå‘ç°æˆåŠŸæ¨¡å‹ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ„å»ºæœ‰å‘æ— ç¯å›¾æ¥ç¼“å­˜å’Œæ£€ç´¢éƒ¨åˆ†ä¹˜ç§¯ï¼Œä»¥åŠä½¿ç”¨å‚…é‡Œå¶åŸºè¡¨ç¤ºæ•°å­—å’Œé—µå¯å¤«æ–¯åŸºå’Œå®ç°éƒ¨åˆ†ä¹˜ç§¯ï¼Œæ˜¯æ ‡å‡†æ¨¡å‹æ‰€ç¼ºä¹çš„å…³é”®è¦ç´ ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ„å»ºæœ‰å‘æ— ç¯å›¾æ¥è¡¨ç¤ºéƒ¨åˆ†ä¹˜ç§¯ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚2) ä½¿ç”¨å‚…é‡Œå¶åŸºæ¥è¡¨ç¤ºæ•°å­—ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ æ•°å­—ä¹‹é—´çš„å…³ç³»ã€‚3) å¼•å…¥é¢„æµ‹â€œè¿è¡Œæ€»å’Œâ€çš„è¾…åŠ©æŸå¤±ï¼Œé€šè¿‡çº¿æ€§å›å½’æ¢é’ˆå®ç°ï¼Œä¸ºæ¨¡å‹æä¾›å…³äºä¹˜æ³•è¿ç®—è¿‡ç¨‹çš„å½’çº³åç½®ã€‚è¿™ä¸ªè¾…åŠ©æŸå¤±é¼“åŠ±æ¨¡å‹å­¦ä¹ ä¸­é—´æ­¥éª¤ï¼Œä»è€Œæ›´å®¹æ˜“æ•æ‰é•¿ç¨‹ä¾èµ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒè¯æ˜ï¼Œæ ‡å‡†Transformeræ¨¡å‹åœ¨å¤šä½æ•°ä¹˜æ³•ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œé€šè¿‡é€†å‘å·¥ç¨‹å¾—åˆ°çš„æ¨¡å‹èƒ½å¤ŸæˆåŠŸå­¦ä¹ ä¹˜æ³•ã€‚å¼•å…¥é¢„æµ‹â€œè¿è¡Œæ€»å’Œâ€çš„è¾…åŠ©æŸå¤±åï¼Œæ ‡å‡†æ¨¡å‹çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†é•¿ç¨‹ä¾èµ–å’Œå½’çº³åç½®çš„é‡è¦æ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è¯¥æ–¹æ³•ä¸ºè§£å†³Transformeråœ¨é•¿ç¨‹ä¾èµ–ä»»åŠ¡ä¸­çš„é—®é¢˜æä¾›äº†ä¸€ç§æœ‰æ•ˆé€”å¾„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æå‡è¯­è¨€æ¨¡å‹åœ¨éœ€è¦å¤æ‚æ¨ç†å’Œè®¡ç®—çš„ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¾‹å¦‚ç§‘å­¦è®¡ç®—ã€é‡‘èå»ºæ¨¡å’Œä»£ç ç”Ÿæˆã€‚é€šè¿‡ç†è§£å’Œè§£å†³Transformeræ¨¡å‹åœ¨é•¿ç¨‹ä¾èµ–å­¦ä¹ æ–¹é¢çš„å±€é™æ€§ï¼Œå¯ä»¥å¼€å‘æ›´å¼ºå¤§çš„AIç³»ç»Ÿï¼Œæ›´å¥½åœ°å¤„ç†ç°å®ä¸–ç•Œä¸­çš„å¤æ‚é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„ç¥ç»ç½‘ç»œæ¶æ„å’Œè®­ç»ƒæ–¹æ³•æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via \emph{implicit chain-of-thought}, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.

