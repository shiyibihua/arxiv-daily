---
layout: default
title: Directed-MAML: Meta Reinforcement Learning Algorithm with Task-directed Approximation
---

# Directed-MAML: Meta Reinforcement Learning Algorithm with Task-directed Approximation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00212" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00212v1</a>
  <a href="https://arxiv.org/pdf/2510.00212.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00212v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00212v1', 'Directed-MAML: Meta Reinforcement Learning Algorithm with Task-directed Approximation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Zhang, Huiwen Yan, Mushuang Liu

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDirected-MAMLï¼Œé€šè¿‡ä»»åŠ¡å¯¼å‘è¿‘ä¼¼åŠ é€Ÿå…ƒå¼ºåŒ–å­¦ä¹ æ”¶æ•›å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å…ƒå¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹æ— å…³å…ƒå­¦ä¹ ` `äºŒé˜¶æ¢¯åº¦è¿‘ä¼¼` `ä»»åŠ¡å¯¼å‘å­¦ä¹ ` `å¿«é€Ÿæ”¶æ•›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MAMLåœ¨å…ƒå¼ºåŒ–å­¦ä¹ ä¸­é¢ä¸´è®¡ç®—å¼€é”€å¤§å’Œæ”¶æ•›å›°éš¾çš„é—®é¢˜ï¼Œæºäºå…¶äºŒé˜¶æ¢¯åº¦è®¡ç®—å’ŒåµŒå¥—ä¼˜åŒ–ç»“æ„ã€‚
2. Directed-MAMLé€šè¿‡åœ¨äºŒé˜¶æ¢¯åº¦è®¡ç®—å‰å¼•å…¥ä»»åŠ¡å¯¼å‘çš„ä¸€é˜¶è¿‘ä¼¼ï¼Œä¼°è®¡äºŒé˜¶æ¢¯åº¦çš„å½±å“ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDirected-MAMLåœ¨å¤šä¸ªå¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­ï¼Œç›¸æ¯”MAMLåŸºçº¿ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡å‹æ— å…³å…ƒå­¦ä¹ (MAML)æ˜¯ä¸€ç§é€šç”¨çš„å…ƒå­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ (RL)ã€‚ç„¶è€Œï¼Œå°†MAMLåº”ç”¨äºå…ƒå¼ºåŒ–å­¦ä¹ (meta-RL)é¢ä¸´ç€æ˜¾è‘—çš„æŒ‘æˆ˜ã€‚é¦–å…ˆï¼ŒMAMLä¾èµ–äºäºŒé˜¶æ¢¯åº¦è®¡ç®—ï¼Œå¯¼è‡´æ˜¾è‘—çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚å…¶æ¬¡ï¼Œä¼˜åŒ–çš„åµŒå¥—ç»“æ„å¢åŠ äº†é—®é¢˜çš„å¤æ‚æ€§ï¼Œä½¿å¾—æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜å˜å¾—æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡å¯¼å‘å…ƒå¼ºåŒ–å­¦ä¹ ç®—æ³•Directed-MAMLã€‚åœ¨äºŒé˜¶æ¢¯åº¦æ­¥éª¤ä¹‹å‰ï¼ŒDirected-MAMLåº”ç”¨é¢å¤–çš„ä»»åŠ¡å¯¼å‘ä¸€é˜¶è¿‘ä¼¼æ¥ä¼°è®¡äºŒé˜¶æ¢¯åº¦çš„å½±å“ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›åˆ°æœ€ä¼˜å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨CartPole-v1ã€LunarLander-v2å’ŒåŒè½¦äº¤å‰å£åœºæ™¯ä¸­ï¼ŒDirected-MAMLåœ¨è®¡ç®—æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦æ–¹é¢è¶…è¿‡äº†åŸºäºMAMLçš„åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ä»»åŠ¡å¯¼å‘è¿‘ä¼¼å¯ä»¥æœ‰æ•ˆåœ°é›†æˆåˆ°å…¶ä»–å…ƒå­¦ä¹ ç®—æ³•ä¸­ï¼Œä¾‹å¦‚ä¸€é˜¶æ¨¡å‹æ— å…³å…ƒå­¦ä¹ (FOMAML)å’Œå…ƒéšæœºæ¢¯åº¦ä¸‹é™(Meta-SGD)ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šMAMLåœ¨å…ƒå¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨å—é™äºå…¶é«˜æ˜‚çš„è®¡ç®—æˆæœ¬å’Œå¤æ‚çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚äºŒé˜¶æ¢¯åº¦è®¡ç®—éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œå†…å­˜ï¼Œè€ŒåµŒå¥—çš„ä¼˜åŒ–ç»“æ„ä½¿å¾—æ¨¡å‹éš¾ä»¥æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDirected-MAMLçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨è¿›è¡Œæ˜‚è´µçš„äºŒé˜¶æ¢¯åº¦è®¡ç®—ä¹‹å‰ï¼Œå…ˆä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„ä»»åŠ¡å¯¼å‘çš„ä¸€é˜¶è¿‘ä¼¼æ¥ä¼°è®¡äºŒé˜¶æ¢¯åº¦çš„å½±å“ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æŒ‡å¯¼ä¼˜åŒ–è¿‡ç¨‹æœç€æ›´æœ‰å¸Œæœ›çš„æ–¹å‘å‰è¿›ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘ä¸å¿…è¦çš„è®¡ç®—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDirected-MAMLçš„æ•´ä½“æ¡†æ¶ä¸MAMLç±»ä¼¼ï¼Œéƒ½åŒ…å«ä¸€ä¸ªå…ƒå­¦ä¹ å™¨å’Œä¸€ä¸ªä»»åŠ¡ç‰¹å®šçš„å­¦ä¹ å™¨ã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒDirected-MAMLé¦–å…ˆä½¿ç”¨ä»»åŠ¡å¯¼å‘çš„ä¸€é˜¶è¿‘ä¼¼æ¥æ›´æ–°å…ƒå­¦ä¹ å™¨çš„å‚æ•°ï¼Œç„¶åå†è¿›è¡ŒäºŒé˜¶æ¢¯åº¦è®¡ç®—ã€‚è¿™ä¸ªä¸€é˜¶è¿‘ä¼¼æ¨¡å—å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªé¢„å¤„ç†å™¨ï¼Œç”¨äºæŒ‡å¯¼åç»­çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šDirected-MAMLçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†ä»»åŠ¡å¯¼å‘çš„è¿‘ä¼¼æ–¹æ³•æ¥ä¼°è®¡äºŒé˜¶æ¢¯åº¦çš„å½±å“ã€‚è¿™ç§è¿‘ä¼¼æ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”è¿˜æé«˜äº†æ”¶æ•›é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°é›†æˆåˆ°å…¶ä»–åŸºäºMAMLçš„å…ƒå­¦ä¹ ç®—æ³•ä¸­ï¼Œä¾‹å¦‚FOMAMLå’ŒMeta-SGDã€‚

**å…³é”®è®¾è®¡**ï¼šä»»åŠ¡å¯¼å‘è¿‘ä¼¼çš„å…·ä½“å®ç°æ–¹å¼æ˜¯ä½¿ç”¨ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹æ¥é¢„æµ‹äºŒé˜¶æ¢¯åº¦çš„æ–¹å‘å’Œå¤§å°ã€‚è¿™ä¸ªçº¿æ€§æ¨¡å‹çš„å‚æ•°å¯ä»¥é€šè¿‡æœ€å°åŒ–é¢„æµ‹è¯¯å·®æ¥å­¦ä¹ ã€‚æŸå¤±å‡½æ•°é€šå¸¸é€‰æ‹©å‡æ–¹è¯¯å·®æˆ–äº¤å‰ç†µæŸå¤±ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œä¸€é˜¶è¿‘ä¼¼æ¨¡å—é€šå¸¸é‡‡ç”¨ç®€å•çš„å…¨è¿æ¥ç½‘ç»œæˆ–çº¿æ€§æ¨¡å‹ï¼Œä»¥ä¿è¯è®¡ç®—æ•ˆç‡ã€‚å‚æ•°æ›´æ–°é‡‡ç”¨æ¢¯åº¦ä¸‹é™æˆ–å…¶å˜ä½“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDirected-MAMLåœ¨CartPole-v1ã€LunarLander-v2å’ŒåŒè½¦äº¤å‰å£ç­‰å¤šä¸ªå¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­ï¼Œç›¸æ¯”äºMAMLåŠå…¶å˜ä½“ï¼ˆå¦‚FOMAMLå’ŒMeta-SGDï¼‰ï¼Œåœ¨è®¡ç®—æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚å…·ä½“è€Œè¨€ï¼ŒDirected-MAMLèƒ½å¤Ÿæ›´å¿«åœ°è¾¾åˆ°ç›¸åŒçš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿè·å¾—æ›´é«˜çš„æœ€ç»ˆæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Directed-MAMLå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å¿«é€Ÿå­¦ä¹ æ–°ä»»åŠ¡çš„åœºæ™¯ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ™ºèƒ½ä½“åœ¨é¢å¯¹æ–°ç¯å¢ƒæ—¶çš„é€‚åº”èƒ½åŠ›ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼Œå¹¶åŠ é€Ÿéƒ¨ç½²è¿‡ç¨‹ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨å…ƒå¼ºåŒ–å­¦ä¹ åœ¨å®é™…å·¥ä¸šåœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model-Agnostic Meta-Learning (MAML) is a versatile meta-learning framework applicable to both supervised learning and reinforcement learning (RL). However, applying MAML to meta-reinforcement learning (meta-RL) presents notable challenges. First, MAML relies on second-order gradient computations, leading to significant computational and memory overhead. Second, the nested structure of optimization increases the problem's complexity, making convergence to a global optimum more challenging. To overcome these limitations, we propose Directed-MAML, a novel task-directed meta-RL algorithm. Before the second-order gradient step, Directed-MAML applies an additional first-order task-directed approximation to estimate the effect of second-order gradients, thereby accelerating convergence to the optimum and reducing computational cost. Experimental results demonstrate that Directed-MAML surpasses MAML-based baselines in computational efficiency and convergence speed in the scenarios of CartPole-v1, LunarLander-v2 and two-vehicle intersection crossing. Furthermore, we show that task-directed approximation can be effectively integrated into other meta-learning algorithms, such as First-Order Model-Agnostic Meta-Learning (FOMAML) and Meta Stochastic Gradient Descent(Meta-SGD), yielding improved computational efficiency and convergence speed.

