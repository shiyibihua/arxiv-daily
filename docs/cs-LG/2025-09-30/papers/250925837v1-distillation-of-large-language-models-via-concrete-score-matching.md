---
layout: default
title: Distillation of Large Language Models via Concrete Score Matching
---

# Distillation of Large Language Models via Concrete Score Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25837" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25837v1</a>
  <a href="https://arxiv.org/pdf/2509.25837.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25837v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25837v1', 'Distillation of Large Language Models via Concrete Score Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yeongmin Kim, Donghyeok Shin, Mina Kang, Byeonghu Na, Il-Chul Moon

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConcrete Score Distillationï¼Œè§£å†³LLMè’¸é¦ä¸­logitä¿¡æ¯æŸå¤±å’Œè§£ç©ºé—´é™åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `å¤§å‹è¯­è¨€æ¨¡å‹` `Concrete Score Matching` `æ¨¡å‹å‹ç¼©` `logitè’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•ä¾èµ–softmaxæ¦‚ç‡åŒ¹é…ï¼ŒæŸå¤±äº†logitä¸­çš„ä¸°å¯Œä¿¡æ¯ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. Concrete Score Distillation (CSD)é€šè¿‡ç¦»æ•£åˆ†æ•°åŒ¹é…ï¼Œå¯¹é½å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹é—´çš„ç›¸å¯¹logitå·®å¼‚ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCSDåœ¨å¤šä¸ªLLMä¸Šä¼˜äºç°æœ‰KDæ–¹æ³•ï¼Œå®ç°äº†æ›´å¥½çš„ä¿çœŸåº¦å’Œå¤šæ ·æ€§å¹³è¡¡ï¼Œå¹¶å¯ä¸on-policyæ–¹æ³•ç»“åˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ€§èƒ½å“è¶Šä½†éƒ¨ç½²æˆæœ¬é«˜æ˜‚ï¼Œå› æ­¤çŸ¥è¯†è’¸é¦(KD)å¯¹äºé«˜æ•ˆæ¨ç†è‡³å…³é‡è¦ã€‚ç°æœ‰çš„KDç›®æ ‡é€šå¸¸é€šè¿‡softmaxåŒ¹é…å­¦ç”Ÿå’Œæ•™å¸ˆçš„æ¦‚ç‡ï¼Œè¿™æ¨¡ç³Šäº†å®è´µçš„logitä¿¡æ¯ã€‚ç›´æ¥logitè’¸é¦(DLD)è™½ç„¶ç¼“è§£äº†softmaxå¹³æ»‘ï¼Œä½†æœªèƒ½è€ƒè™‘logitç§»ä½ä¸å˜æ€§ï¼Œä»è€Œé™åˆ¶äº†è§£ç©ºé—´ã€‚æˆ‘ä»¬æå‡ºConcrete Score Distillation (CSD)ï¼Œè¿™æ˜¯ä¸€ç§ç¦»æ•£çš„åˆ†æ•°åŒ¹é…ç›®æ ‡ï¼Œå…‹æœäº†softmaxå¼•èµ·çš„å¹³æ»‘ä»¥åŠå¯¹æœ€ä¼˜è§£é›†çš„é™åˆ¶ã€‚æˆ‘ä»¬è§£å†³äº†è‡ªå›å½’LLMä¸­ç¦»æ•£åˆ†æ•°åŒ¹é…çš„è®­ç»ƒä¸ç¨³å®šæ€§å’ŒäºŒæ¬¡å¤æ‚åº¦é—®é¢˜ï¼Œç”±æ­¤äº§ç”Ÿçš„CSDç›®æ ‡ä»¥çµæ´»çš„æƒé‡å¯¹é½å­¦ç”Ÿå’Œæ•™å¸ˆä¹‹é—´æ‰€æœ‰è¯æ±‡å¯¹çš„ç›¸å¯¹logitå·®å¼‚ã€‚æˆ‘ä»¬åœ¨ä»»åŠ¡æ— å…³çš„æŒ‡ä»¤è·Ÿéšå’Œä½¿ç”¨GPT-2-1.5Bã€OpenLLaMA-7Bå’ŒGEMMA-7B-ITçš„ä»»åŠ¡ç‰¹å®šè’¸é¦ä¸Šè¯„ä¼°CSDã€‚å®éªŒè¡¨æ˜ï¼ŒCSDå§‹ç»ˆä¼˜äºæœ€è¿‘çš„KDç›®æ ‡ï¼Œå®ç°äº†è‰¯å¥½çš„ä¿çœŸåº¦-å¤šæ ·æ€§æƒè¡¡ï¼Œå¹¶ä¸”åœ¨ä¸on-policyæŠ€æœ¯ç»“åˆä½¿ç”¨æ—¶äº§ç”Ÿäº’è¡¥çš„å¢ç›Šï¼Œè¯æ˜äº†å…¶LLMè’¸é¦çš„å¯æ‰©å±•æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œå¦‚åŸºäºsoftmaxçš„æ¦‚ç‡åŒ¹é…ï¼Œä¼šæ¨¡ç³Šlogitä¸­åŒ…å«çš„ä¸°å¯Œä¿¡æ¯ï¼Œå¯¼è‡´å­¦ç”Ÿæ¨¡å‹æ— æ³•å……åˆ†å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚ç›´æ¥logitè’¸é¦(DLD)è™½ç„¶å°è¯•ç›´æ¥åŒ¹é…logitï¼Œä½†å¿½ç•¥äº†logitç§»ä½ä¸å˜æ€§ï¼Œé™åˆ¶äº†æœ€ä¼˜è§£ç©ºé—´ï¼Œå½±å“äº†è’¸é¦æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¦»æ•£åˆ†æ•°åŒ¹é…(Concrete Score Matching)æ¥å¯¹é½å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„ç›¸å¯¹logitå·®å¼‚ã€‚é€šè¿‡ç›´æ¥å¯¹é½logitçš„ç›¸å¯¹å…³ç³»ï¼Œé¿å…äº†softmaxå¸¦æ¥çš„ä¿¡æ¯æŸå¤±ï¼ŒåŒæ—¶è€ƒè™‘äº†logitç§»ä½ä¸å˜æ€§ï¼Œä»è€Œæ‰©å¤§äº†æœ€ä¼˜è§£ç©ºé—´ï¼Œä½¿å¾—å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCSDæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œè®¡ç®—æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹åœ¨ç›¸åŒè¾“å…¥ä¸‹çš„logitè¾“å‡ºã€‚ç„¶åï¼Œåˆ©ç”¨Concrete Score Matchingæ–¹æ³•ï¼Œè®¡ç®—å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹ä¹‹é—´logitçš„ç›¸å¯¹å·®å¼‚ã€‚æœ€åï¼Œé€šè¿‡ä¼˜åŒ–ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œä½¿å¾—å­¦ç”Ÿæ¨¡å‹çš„logitç›¸å¯¹å·®å¼‚å°½å¯èƒ½æ¥è¿‘æ•™å¸ˆæ¨¡å‹çš„logitç›¸å¯¹å·®å¼‚ã€‚è¯¥æ¡†æ¶å¯ä»¥çµæ´»åœ°è°ƒæ•´ä¸åŒè¯æ±‡å¯¹ä¹‹é—´çš„æƒé‡ï¼Œä»è€Œå®ç°mode-seekingå’Œmode-coveringä¸¤ç§ä¸åŒçš„è’¸é¦ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†Concrete Score Distillation (CSD)æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¦»æ•£åˆ†æ•°åŒ¹é…çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºsoftmaxçš„æ¦‚ç‡åŒ¹é…æ–¹æ³•ç›¸æ¯”ï¼ŒCSDèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™logitä¸­çš„ä¿¡æ¯ï¼Œå¹¶ä¸”è€ƒè™‘äº†logitç§»ä½ä¸å˜æ€§ï¼Œä»è€Œæ‰©å¤§äº†æœ€ä¼˜è§£ç©ºé—´ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è§£å†³äº†è‡ªå›å½’LLMä¸­ç¦»æ•£åˆ†æ•°åŒ¹é…çš„è®­ç»ƒä¸ç¨³å®šæ€§å’ŒäºŒæ¬¡å¤æ‚åº¦é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šCSDçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Concreteåˆ†å¸ƒæ¥è¿‘ä¼¼ç¦»æ•£åˆ†å¸ƒï¼Œä»è€Œå®ç°å¯å¾®çš„ä¼˜åŒ–ï¼›2) è®¾è®¡äº†ä¸€ç§æŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹ä¹‹é—´logitç›¸å¯¹å·®å¼‚çš„ç›¸ä¼¼åº¦ï¼›3) é‡‡ç”¨çµæ´»çš„æƒé‡æœºåˆ¶ï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„ä»»åŠ¡å’Œéœ€æ±‚ï¼Œè°ƒæ•´ä¸åŒè¯æ±‡å¯¹ä¹‹é—´çš„æƒé‡ï¼Œå®ç°mode-seekingå’Œmode-coveringä¸¤ç§ä¸åŒçš„è’¸é¦ç­–ç•¥ã€‚è®ºæ–‡è¿˜æå‡ºäº†è§£å†³è®­ç»ƒä¸ç¨³å®šæ€§å’ŒäºŒæ¬¡å¤æ‚åº¦é—®é¢˜çš„å…·ä½“æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨æ¢¯åº¦è£å‰ªå’Œç¨€ç–åŒ–æŠ€æœ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCSDæ–¹æ³•åœ¨ä»»åŠ¡æ— å…³çš„æŒ‡ä»¤è·Ÿéšå’Œä»»åŠ¡ç‰¹å®šçš„è’¸é¦ä»»åŠ¡ä¸Šï¼Œå‡ä¼˜äºç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨GPT-2-1.5Bã€OpenLLaMA-7Bå’ŒGEMMA-7B-ITè¿›è¡Œè’¸é¦æ—¶ï¼ŒCSDèƒ½å¤Ÿæ˜¾è‘—æå‡å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸”å®ç°äº†æ›´å¥½çš„ä¿çœŸåº¦-å¤šæ ·æ€§æƒè¡¡ã€‚æ­¤å¤–ï¼ŒCSDè¿˜å¯ä»¥ä¸on-policyæŠ€æœ¯ç»“åˆä½¿ç”¨ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å‹ç¼©å’ŒåŠ é€Ÿï¼Œé™ä½éƒ¨ç½²æˆæœ¬ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†å¤§å‹é¢„è®­ç»ƒæ¨¡å‹è’¸é¦æˆæ›´å°çš„æ¨¡å‹ï¼Œéƒ¨ç½²åœ¨ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¡ç®—å¹³å°ä¸Šï¼Œå®ç°æœ¬åœ°åŒ–çš„æ™ºèƒ½æœåŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºä¸ªæ€§åŒ–æ¨¡å‹çš„è®­ç»ƒï¼Œæ ¹æ®ç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚ï¼Œå®šåˆ¶åŒ–è®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) deliver remarkable performance but are costly to deploy, motivating knowledge distillation (KD) for efficient inference. Existing KD objectives typically match student and teacher probabilities via softmax, which blurs valuable logit information. While direct logit distillation (DLD) mitigates softmax smoothing, it fails to account for logit shift invariance, thereby restricting the solution space. We propose Concrete Score Distillation (CSD), a discrete score-matching objective that overcomes both softmax-induced smoothing and restrictions on the optimal solution set. We resolve the training instability and quadratic complexity of discrete score-matching in autoregressive LLMs, and the resulting CSD objective aligns relative logit differences across all vocabulary pairs between student and teacher with flexible weighting. We provide both mode-seeking and mode-covering instances within our framework and evaluate CSD on task-agnostic instruction-following and task-specific distillation using GPT-2-1.5B, OpenLLaMA-7B, and GEMMA-7B-IT. Experiments show that CSD consistently surpasses recent KD objectives, achieves favorable fidelity-diversity trade-offs, and yields complementary gains when combined with on-policy techniques, demonstrating its scalability and effectiveness for LLM distillation.

