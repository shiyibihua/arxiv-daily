---
layout: default
title: DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context Automatic Modulation Classification
---

# DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context Automatic Modulation Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00316" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00316v1</a>
  <a href="https://arxiv.org/pdf/2510.00316.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00316v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00316v1', 'DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context Automatic Modulation Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammad Rostami, Atik Faysal, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar, Yu-Dong Yao

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DiSC-AMCï¼šé¢å‘è‡ªåŠ¨è°ƒåˆ¶åˆ†ç±»ï¼Œæå‡ºtokenå’Œå‚æ•°é«˜æ•ˆçš„ç¦»æ•£åŒ–ç»Ÿè®¡ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨è°ƒåˆ¶åˆ†ç±»` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `ç¦»æ•£åŒ–` `ç‰¹å¾é€‰æ‹©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„è‡ªåŠ¨è°ƒåˆ¶åˆ†ç±»æ–¹æ³•ä¾èµ–äºé•¿ä¸Šä¸‹æ–‡æç¤ºå’Œå¤§å‹æ¨¡å‹ï¼Œå¯¼è‡´éƒ¨ç½²æˆæœ¬é«˜æ˜‚ã€‚
2. DiSC-AMCé€šè¿‡ç¦»æ•£åŒ–ç»Ÿè®¡ç‰¹å¾ã€ç²¾ç®€ä¸Šä¸‹æ–‡ç¤ºä¾‹å’Œä¼˜åŒ–æç¤ºæ¨¡æ¿ï¼Œæ˜¾è‘—é™ä½äº†tokenå’Œå‚æ•°éœ€æ±‚ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDiSC-AMCåœ¨ä¿æŒç”šè‡³æå‡åˆ†ç±»å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ¨ç†æˆæœ¬é™ä½è¶…è¿‡2å€ï¼Œæ›´æ˜“äºå®é™…åº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨è‡ªåŠ¨è°ƒåˆ¶åˆ†ç±»(AMC)ä¸­ï¼Œä¸Šä¸‹æ–‡æç¤ºè¿‡é•¿å’Œæ¨¡å‹è¿‡å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§tokenå’Œå‚æ•°é«˜æ•ˆçš„ç¦»æ•£åŒ–ç»Ÿè®¡ä¸Šä¸‹æ–‡è‡ªåŠ¨è°ƒåˆ¶åˆ†ç±»æ–¹æ³•(DiSC-AMC)ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°é«˜æ•ˆï¼š(i)å°†é«˜é˜¶ç»Ÿè®¡é‡å’Œç´¯ç§¯é‡ç¦»æ•£åŒ–ä¸ºç´§å‡‘çš„ç¬¦å·tokenï¼›(ii)é€šè¿‡è½»é‡çº§çš„k-topç¥ç»é¢„è¿‡æ»¤å™¨ä¿®å‰ªç¤ºä¾‹åˆ—è¡¨ï¼Œå¹¶ä½¿ç”¨ä»å…ˆå‰LLMå“åº”ä¸­æå–çš„ç†ç”±æ¥è¿‡æ»¤è¯¯å¯¼æ€§/ä½å½±å“çš„ç‰¹å¾ï¼›(iii)é€šè¿‡æ ¡å‡†çš„æç¤ºæ¨¡æ¿å¼ºåˆ¶æ‰§è¡Œä»…æ ‡ç­¾é¢„æµ‹ã€‚è¿™äº›æ”¹å˜æ˜¾è‘—å‡å°‘äº†è¾“å…¥/è¾“å‡ºtokenå’Œæ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„å‡†ç¡®æ€§ã€‚åœ¨å™ªå£°ä¸‹çš„åç§è°ƒåˆ¶ç±»å‹çš„åˆæˆAMCä¸Šï¼Œä¸€ä¸ª7Bå‚æ•°çš„DeepSeek-R1-Distill-QwenåŸºçº¿å®ç°äº†5.2%çš„å‡†ç¡®ç‡ï¼Œè€Œæˆ‘ä»¬çš„ç³»ç»Ÿï¼Œä½¿ç”¨ä¸€ä¸ªå¤§çº¦5Bå‚æ•°çš„Gemini-2.5-Flashæ¨¡å‹ï¼Œè¾¾åˆ°äº†45.5%çš„å‡†ç¡®ç‡ã€‚ç»“æœè¡¨æ˜ï¼Œä»”ç»†çš„ç¦»æ•£åŒ–å’Œä¸Šä¸‹æ–‡é€‰æ‹©å¯ä»¥å°†æ¨ç†æˆæœ¬é™ä½2å€ä»¥ä¸Šï¼ŒåŒæ—¶ä¿ç•™åŸºäºæç¤ºçš„AMCçš„ä¼˜åŠ¿ï¼Œå¹¶å®ç°å®é™…çš„ç¯å†…ä½¿ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨è°ƒåˆ¶åˆ†ç±»ï¼ˆAMCï¼‰æ–¹æ³•ï¼Œè™½ç„¶èƒ½å¤Ÿåœ¨å¼€æ”¾é›†åœºæ™¯ä¸‹å·¥ä½œï¼Œä½†ä¾èµ–äºç²¾å¿ƒè®¾è®¡çš„ä¸Šä¸‹æ–‡æç¤ºã€‚è¿™äº›æç¤ºé€šå¸¸å¾ˆé•¿ï¼Œéœ€è¦å¤§é‡çš„tokenï¼Œå¹¶ä¸”ä¾èµ–äºå¤§å‹æ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åœ¨å®é™…ç¯å†…éƒ¨ç½²ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºtokenæ•ˆç‡å’Œå‚æ•°æ•ˆç‡ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDiSC-AMCçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç¦»æ•£åŒ–é«˜é˜¶ç»Ÿè®¡é‡å’Œç´¯ç§¯é‡ï¼Œå°†å…¶è½¬åŒ–ä¸ºç´§å‡‘çš„ç¬¦å·tokenï¼Œä»è€Œå‡å°‘è¾“å…¥tokençš„æ•°é‡ã€‚åŒæ—¶ï¼Œé€šè¿‡ç¥ç»é¢„è¿‡æ»¤å™¨å’ŒåŸºäºLLMç†ç”±çš„ç‰¹å¾é€‰æ‹©ï¼Œç²¾ç®€ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œé™ä½æ¨¡å‹éœ€è¦å¤„ç†çš„ä¿¡æ¯é‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ ¡å‡†çš„æç¤ºæ¨¡æ¿ï¼Œå¼ºåˆ¶æ¨¡å‹è¿›è¡Œä»…æ ‡ç­¾é¢„æµ‹ï¼Œè¿›ä¸€æ­¥å‡å°‘è¾“å‡ºtokençš„æ•°é‡ã€‚è¿™æ ·ï¼Œåœ¨ä¸æ˜¾è‘—é™ä½åˆ†ç±»å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½äº†tokenå’Œå‚æ•°éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDiSC-AMCçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **ç‰¹å¾æå–ä¸ç¦»æ•£åŒ–**ï¼šä»æ¥æ”¶åˆ°çš„ä¿¡å·ä¸­æå–é«˜é˜¶ç»Ÿè®¡é‡å’Œç´¯ç§¯é‡ï¼Œå¹¶å°†è¿™äº›è¿ç»­å€¼ç¦»æ•£åŒ–ä¸ºç¬¦å·tokenã€‚2) **ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©**ï¼šä½¿ç”¨è½»é‡çº§çš„k-topç¥ç»é¢„è¿‡æ»¤å™¨ä»å€™é€‰ç¤ºä¾‹åˆ—è¡¨ä¸­é€‰æ‹©æœ€ç›¸å…³çš„ç¤ºä¾‹ã€‚3) **ç‰¹å¾è¿‡æ»¤**ï¼šåˆ©ç”¨å…ˆå‰LLMå“åº”ä¸­æå–çš„ç†ç”±ï¼Œè¿‡æ»¤æ‰è¯¯å¯¼æ€§æˆ–ä½å½±å“çš„ç‰¹å¾ã€‚4) **æç¤ºæ„å»º**ï¼šä½¿ç”¨æ ¡å‡†çš„æç¤ºæ¨¡æ¿ï¼Œå°†ç¦»æ•£åŒ–çš„ç‰¹å¾å’Œé€‰æ‹©çš„ç¤ºä¾‹ç»„åˆæˆä¸Šä¸‹æ–‡æç¤ºã€‚5) **LLMæ¨ç†**ï¼šå°†æ„å»ºçš„æç¤ºè¾“å…¥LLMï¼Œè·å¾—è°ƒåˆ¶ç±»å‹çš„é¢„æµ‹ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šDiSC-AMCçš„å…³é”®åˆ›æ–°åœ¨äºå°†é«˜é˜¶ç»Ÿè®¡é‡å’Œç´¯ç§¯é‡ç¦»æ•£åŒ–ä¸ºç¬¦å·tokenï¼Œä»¥åŠåˆ©ç”¨LLMçš„ç†ç”±è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDiSC-AMCä¸éœ€è¦ç›´æ¥å°†åŸå§‹ä¿¡å·æˆ–è¿ç»­çš„ç»Ÿè®¡ç‰¹å¾è¾“å…¥LLMï¼Œè€Œæ˜¯ä½¿ç”¨æ›´ç´§å‡‘çš„ç¬¦å·è¡¨ç¤ºï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è¾“å…¥tokençš„æ•°é‡ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMçš„ç†ç”±è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°è¿‡æ»¤æ‰ä¸ç›¸å…³çš„ç‰¹å¾ï¼Œæé«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³äºç¦»æ•£åŒ–ï¼Œå…·ä½“å¦‚ä½•é€‰æ‹©ç¦»æ•£åŒ–çš„åŒºé—´å’Œæ•°é‡ï¼Œè®ºæ–‡ä¸­å¯èƒ½æ¶‰åŠã€‚ç¥ç»é¢„è¿‡æ»¤å™¨çš„å…·ä½“ç»“æ„å’Œè®­ç»ƒæ–¹å¼ï¼Œä»¥åŠå¦‚ä½•ä»LLMçš„å“åº”ä¸­æå–ç†ç”±ï¼Œéƒ½æ˜¯å…³é”®çš„è®¾è®¡ç»†èŠ‚ã€‚æ ¡å‡†çš„æç¤ºæ¨¡æ¿çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ç†è§£æç¤ºçš„å«ä¹‰ï¼Œå¹¶è¿›è¡Œæ­£ç¡®çš„é¢„æµ‹ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°ã€ç½‘ç»œç»“æ„ç­‰ç»†èŠ‚ï¼Œéœ€è¦å‚è€ƒè®ºæ–‡åŸæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DiSC-AMCåœ¨åˆæˆAMCæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä½¿ç”¨çº¦5Bå‚æ•°çš„Gemini-2.5-Flashæ¨¡å‹ï¼ŒDiSC-AMCè¾¾åˆ°äº†45.5%çš„å‡†ç¡®ç‡ï¼Œè€Œä½¿ç”¨7Bå‚æ•°çš„DeepSeek-R1-Distill-QwenåŸºçº¿ä»…è¾¾åˆ°5.2%çš„å‡†ç¡®ç‡ã€‚è¿™è¡¨æ˜DiSC-AMCåœ¨ä¿æŒç”šè‡³æå‡åˆ†ç±»å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å‚æ•°å’Œæ¨ç†æˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DiSC-AMCå¯åº”ç”¨äºæ— çº¿é€šä¿¡ã€é¢‘è°±æ„ŸçŸ¥ã€ä¿¡å·æƒ…æŠ¥ç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½LLMåœ¨AMCä¸­çš„è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œå®æ—¶è°ƒåˆ¶åˆ†ç±»æˆä¸ºå¯èƒ½ã€‚è¯¥æ–¹æ³•æœ‰åŠ©äºæé«˜æ— çº¿é€šä¿¡ç³»ç»Ÿçš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„æ™ºèƒ½æ— çº¿ç½‘ç»œæä¾›æŠ€æœ¯æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) can perform Automatic Modulation Classification (AMC) in an open-set manner without LLM fine-tuning when equipped with carefully designed in-context prompts~\cite{rostami2025plug}. Building on this prior work, we target the practical bottlenecks of long prompt contexts and large model sizes that impede in-the-loop deployment. We present Discretized Statistics in-Context Automatic Modulation Classification (DiSC-AMC), a token- and parameter-efficient variant that: (i) discretizes higher-order statistics and cumulants into compact symbolic tokens, (ii) prunes the exemplar list via a lightweight k-top neural prefilter and filters misleading/low-impact features using rationales extracted from prior LLM responses, and (iii) enforces label-only predictions through a calibrated prompt template. Together, these changes reduce both input/output tokens and the model parameter footprint by more than half while maintaining competitive accuracy. On synthetic AMC with ten modulation types under noise, a 7B \textit{DeepSeek-R1-Distill-Qwen} baseline achieves 5.2% accuracy, whereas our system, using an approximately 5B-parameter \textit{Gemini-2.5-Flash}~\cite{comanici2025gemini} model, attains 45.5% accuracy. These results demonstrate that careful discretization and context selection can cut inference cost by over 2x while preserving the advantages of prompt-based AMC and enabling practical in-the-loop use.

