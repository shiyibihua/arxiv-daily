---
layout: default
title: Large Language Models Inference Engines based on Spiking Neural Networks
---

# Large Language Models Inference Engines based on Spiking Neural Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00133" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00133v3</a>
  <a href="https://arxiv.org/pdf/2510.00133.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00133v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00133v3', 'Large Language Models Inference Engines based on Spiking Neural Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adarsha Balaji, Sandeep Madireddy, Prasanna Balaprakash

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNeurTransformerï¼Œåˆ©ç”¨è„‰å†²ç¥ç»ç½‘ç»œåŠ é€ŸTransformeræ¨¡å‹æ¨ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è„‰å†²ç¥ç»ç½‘ç»œ` `Transformer` `æ¨¡å‹è½¬æ¢` `ä½åŠŸè€—æ¨ç†` `è‡ªç„¶è¯­è¨€å¤„ç†` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `æ›¿ä»£æ¢¯åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformeræ¨¡å‹è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥é«˜æ•ˆéƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨é•¿åºåˆ—è¾“å…¥ä¸‹ï¼Œæ—¶é—´å’Œç©ºé—´å¤æ‚åº¦å‘ˆå¹³æ–¹å…³ç³»ã€‚
2. NeurTransformeræå‡ºä¸€ç§Transformer-SNNè½¬æ¢æ–¹æ³•ï¼Œé€šè¿‡è„‰å†²è‡ªæ³¨æ„åŠ›ï¼ˆSSAï¼‰æ›¿æ¢ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›ï¼Œå¹¶å¾®è°ƒSNNæ¨¡å‹ï¼Œå®ç°é«˜æ•ˆæ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè½¬æ¢åçš„GPT-2æ¨¡å‹åœ¨ç²¾åº¦ç•¥æœ‰æŸå¤±çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½äº†èƒ½è€—ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç¡¬ä»¶å®ç°ä¸Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºTransformeræ¶æ„çš„åŸºç¡€æ¨¡å‹æ˜¯å½“å‰é€šç”¨è¯­è¨€å»ºæ¨¡ä»¥åŠææ–™ç§‘å­¦å’Œæ°”å€™ç­‰ç§‘å­¦é¢†åŸŸçš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè®­ç»ƒå’Œéƒ¨ç½²è¿™äº›æ¨¡å‹åœ¨è®¡ç®—ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ—¶é—´å’Œç©ºé—´å¤æ‚åº¦ä¸è¾“å…¥åºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡å…³ç³»ã€‚ç›®å‰å·²ç»æœ‰ä¸€äº›å·¥ä½œè‡´åŠ›äºæ¢ç´¢é«˜æ•ˆçš„è®¡ç®—èŒƒå¼å’Œæ¨¡å‹æ¶æ„æ¥è§£å†³è¿™äº›é™åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰æ¥è®¾è®¡Transformeræ¨¡å‹ã€‚ä½¿ç”¨ç°æœ‰çš„æ›¿ä»£å­¦ä¹ æ–¹æ³•è®­ç»ƒå¤§è§„æ¨¡SNNæ•ˆç‡ä½ä¸‹ä¸”è€—æ—¶ã€‚å¦ä¸€æ–¹é¢ï¼Œå°†ç°æœ‰çš„åŸºäºTransformerçš„æ¨¡å‹è½¬æ¢ä¸ºå…¶SNNç­‰æ•ˆæ¨¡å‹çš„æŠ€æœ¯ä¸å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå› ä¸ºå®ç°æœ€ä½³æ€§èƒ½éœ€è¦å¤§é‡çš„è„‰å†²æ—¶é—´æ­¥é•¿ï¼Œå³å¢åŠ å»¶è¿Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºNeurTransformerçš„æ–¹æ³•ï¼Œç”¨äºä½¿ç”¨åŸºäºç°æœ‰è½¬æ¢æ–¹æ³•çš„ç›‘ç£å¾®è°ƒæ–¹æ³•è®¾è®¡ç”¨äºæ¨ç†çš„åŸºäºTransformerçš„SNNã€‚æ‰€æå‡ºçš„æ–¹æ³•é€šè¿‡ä»¥ä¸‹æ­¥éª¤å·¥ä½œï¼šï¼ˆ1ï¼‰ç”¨åŸºäºè„‰å†²çš„è‡ªæ³¨æ„åŠ›ï¼ˆSSAï¼‰æœºåˆ¶æ›¿æ¢è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œï¼ˆ2ï¼‰å°†è®­ç»ƒåçš„Transformeræ¨¡å‹çš„å‰é¦ˆå—è½¬æ¢ä¸ºå…¶ç­‰æ•ˆçš„SNNï¼Œä»¥åŠï¼ˆ3ï¼‰ä½¿ç”¨åŸºäºSNNçš„æ›¿ä»£å­¦ä¹ ç®—æ³•å¾®è°ƒSSAå—ã€‚æˆ‘ä»¬å¯¹æ‰€æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä½¿ç”¨ä¸‰ç§æ¨¡å‹å¤§å°é€’å¢çš„GPT-2æ¨¡å‹å˜ä½“è¯æ˜äº†å…¶å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè½¬æ¢åçš„GPT-2å°å‹æ¨¡å‹åœ¨ä½™å¼¦ç›¸ä¼¼åº¦æ–¹é¢è¡¨ç°å‡º5-12ï¼…çš„æŸå¤±ï¼Œå›°æƒ‘åº¦é™ä½äº†9.7ï¼…ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†SSAå—ç›¸å¯¹äºASAå—çš„èƒ½æºæ•ˆç‡ï¼Œå¹¶è¡¨æ˜åœ¨æ•°å­—ç¡¬ä»¶ä¸Šå®ç°è‡ªæ³¨æ„åŠ›æœºåˆ¶æ—¶ï¼Œä¼°è®¡èƒ½è€—é™ä½äº†64.71ï¼…è‡³85.28ï¼…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Transformeræ¨¡å‹åœ¨æ¨ç†æ—¶è®¡ç®—é‡å¤§ï¼Œèƒ½è€—é«˜ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚å°†Transformerè½¬æ¢ä¸ºè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰æ˜¯ä¸€ç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç›´æ¥è®­ç»ƒå¤§è§„æ¨¡SNNå›°éš¾ï¼Œè€Œç°æœ‰è½¬æ¢æ–¹æ³•åˆä¼šå¯¼è‡´è¿‡é«˜çš„å»¶è¿Ÿã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNeurTransformerçš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆè½¬æ¢å’Œå¾®è°ƒçš„ä¼˜åŠ¿ã€‚é¦–å…ˆï¼Œå°†é¢„è®­ç»ƒçš„Transformeræ¨¡å‹è½¬æ¢ä¸ºSNNï¼Œç„¶åé’ˆå¯¹SNNçš„ç‰¹æ€§è¿›è¡Œå¾®è°ƒï¼Œä»¥å¼¥è¡¥è½¬æ¢è¿‡ç¨‹ä¸­çš„ç²¾åº¦æŸå¤±ï¼ŒåŒæ—¶ä¿æŒSNNçš„ä½åŠŸè€—ç‰¹æ€§ã€‚å…³é”®åœ¨äºè®¾è®¡ä¸€ç§é«˜æ•ˆçš„è„‰å†²è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSSAï¼‰æ¥æ›¿ä»£ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNeurTransformerçš„æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š
1. **æ›¿æ¢è‡ªæ³¨æ„åŠ›**ï¼šå°†Transformeræ¨¡å‹ä¸­çš„ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›æ¨¡å—æ›¿æ¢ä¸ºåŸºäºè„‰å†²çš„è‡ªæ³¨æ„åŠ›ï¼ˆSSAï¼‰æ¨¡å—ã€‚
2. **SNNè½¬æ¢**ï¼šå°†Transformeræ¨¡å‹çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰éƒ¨åˆ†è½¬æ¢ä¸ºç­‰æ•ˆçš„SNNã€‚
3. **å¾®è°ƒ**ï¼šä½¿ç”¨åŸºäºSNNçš„æ›¿ä»£æ¢¯åº¦å­¦ä¹ ç®—æ³•å¯¹SSAæ¨¡å—è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¼˜åŒ–SNNçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šNeurTransformerçš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1. **è„‰å†²è‡ªæ³¨æ„åŠ›ï¼ˆSSAï¼‰æœºåˆ¶**ï¼šè®¾è®¡äº†ä¸€ç§é€‚ç”¨äºSNNçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåˆ©ç”¨è„‰å†²ä¿¡å·è¿›è¡Œä¿¡æ¯å¤„ç†ï¼Œä»è€Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚
2. **è½¬æ¢ä¸å¾®è°ƒç»“åˆ**ï¼šç»“åˆäº†æ¨¡å‹è½¬æ¢å’Œå¾®è°ƒä¸¤ç§æ–¹æ³•ï¼Œæ—¢åˆ©ç”¨äº†é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œåˆé’ˆå¯¹SNNçš„ç‰¹æ€§è¿›è¡Œäº†ä¼˜åŒ–ï¼Œé¿å…äº†ä»å¤´è®­ç»ƒSNNçš„å›°éš¾ã€‚

**å…³é”®è®¾è®¡**ï¼š
1. **SSAè®¾è®¡**ï¼šå…·ä½“SSAçš„å®ç°ç»†èŠ‚ï¼ˆä¾‹å¦‚è„‰å†²ç¼–ç æ–¹å¼ã€è„‰å†²ç¥ç»å…ƒçš„æ¿€æ´»å‡½æ•°ã€æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—æ–¹å¼ï¼‰æœªçŸ¥ï¼Œä½†å…¶æ ¸å¿ƒç›®æ ‡æ˜¯åˆ©ç”¨è„‰å†²ä¿¡å·æ¨¡æ‹Ÿè‡ªæ³¨æ„åŠ›çš„åŠŸèƒ½ã€‚
2. **æ›¿ä»£æ¢¯åº¦å­¦ä¹ **ï¼šç”±äºSNNçš„ä¸å¯å¯¼æ€§ï¼Œéœ€è¦ä½¿ç”¨æ›¿ä»£æ¢¯åº¦å­¦ä¹ ç®—æ³•è¿›è¡Œå¾®è°ƒã€‚å…·ä½“ä½¿ç”¨çš„æ›¿ä»£æ¢¯åº¦å‡½æ•°æœªçŸ¥ã€‚
3. **æ¨¡å‹é€‰æ‹©**ï¼šå®éªŒä¸­ä½¿ç”¨äº†GPT-2æ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶é€‰æ‹©äº†ä¸åŒå¤§å°çš„æ¨¡å‹å˜ä½“è¿›è¡ŒéªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒNeurTransformeråœ¨GPT-2æ¨¡å‹ä¸Šçš„åº”ç”¨å–å¾—äº†æ˜¾è‘—çš„èƒ½è€—é™ä½ã€‚è½¬æ¢åçš„GPT-2å°å‹æ¨¡å‹åœ¨ä½™å¼¦ç›¸ä¼¼åº¦ä¸ŠæŸå¤±äº†5-12%ï¼Œå›°æƒ‘åº¦é™ä½äº†9.7%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä¸ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼ŒSSAæ¨¡å—åœ¨æ•°å­—ç¡¬ä»¶ä¸Šçš„èƒ½è€—é™ä½äº†64.71%è‡³85.28%ã€‚è¿™äº›ç»“æœè¡¨æ˜NeurTransformeråœ¨ä¿æŒä¸€å®šç²¾åº¦çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜äº†Transformeræ¨¡å‹çš„èƒ½æ•ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

NeurTransformerå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä½åŠŸè€—ã€ä½å»¶è¿Ÿæ¨ç†çš„åœºæ™¯ä¸­ï¼Œä¾‹å¦‚è¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€ç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬åˆ†ç±»ï¼Œå¹¶æœ‰æœ›æ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundational models based on the transformer architecture are currently the state-of-the-art in general language modeling, as well as in scientific areas such as material science and climate. However, training and deploying these models is computationally challenging as the time and space complexity has a quadratic relation to the input sequence length. Several efforts exploring efficient computational paradigms and model architectures to address these limitations have been made. In this work, we explore spiking neural networks (SNNs) to design transformer models. A challenge in training large-scale SNNs, using existing surrogate learning methods is inefficient and time-consuming. On the other hand, techniques to convert existing transformer-based models to their SNN equivalent are not scalable, as achieving optimal performance comes at the cost of a large number of spike time-steps, i.e. increased latency. To address this, we propose NeurTransformer, a methodology for designing transformer-based SNN for inference using a supervised fine-tuning approach with existing conversion methods. The proposed methodology works by: (1) replacing the self-attention mechanism with a spike-based self-attention (SSA), (2) converting the feed-forward block of the trained transformer model to its equivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate learning algorithms. We benchmark the proposed methodology and demonstrate its accuracy and scalability using three variants of the GPT-2 model of increasing model size. We observe that the converted GPT-2 small models demonstrate a 5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we demonstrate the energy efficiency of the SSA block compared to the ASA block and show between 64.71% and 85.28% reductions in estimated energy consumption when implementing the self-attention mechanism on a digital hardware.

