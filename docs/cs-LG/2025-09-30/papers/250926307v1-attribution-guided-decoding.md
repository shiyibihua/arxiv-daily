---
layout: default
title: Attribution-Guided Decoding
---

# Attribution-Guided Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26307" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26307v1</a>
  <a href="https://arxiv.org/pdf/2509.26307.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26307v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26307v1', 'Attribution-Guided Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Piotr Komorowski, Elena Golimblevskaia, Reduan Achtibat, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå½’å› å¼•å¯¼çš„è§£ç æ–¹æ³•AGDï¼Œæå‡LLMæŒ‡ä»¤éµå¾ªå’ŒçŸ¥è¯†å‡†ç¡®æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `å½’å› åˆ†æ` `è§£ç ç­–ç•¥` `æŒ‡ä»¤éµå¾ª` `çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡` `äº‹å®å‡†ç¡®æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè§£ç æ–¹æ³•åœ¨æŒ‡ä»¤éµå¾ªå’ŒçŸ¥è¯†å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ§åˆ¶æŠ€æœ¯åˆå®¹æ˜“é™ä½ç”Ÿæˆè´¨é‡ã€‚
2. AGDé€šè¿‡é€‰æ‹©å¯¹ç”¨æˆ·å®šä¹‰ROIå½’å› æœ€é«˜çš„tokenï¼Œå¼•å¯¼LLMç”Ÿæˆè¿‡ç¨‹ï¼Œå®ç°æ›´å¯é çš„è¾“å‡ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAGDæ˜¾è‘—æå‡äº†LLMåœ¨æŒ‡ä»¤éµå¾ªå’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶é™ä½äº†å¹»è§‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éµå¾ªå¤æ‚æŒ‡ä»¤å’Œç”Ÿæˆå‡†ç¡®æ–‡æœ¬çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ ‡å‡†è§£ç æ–¹æ³•éš¾ä»¥æ»¡è¶³è¿™äº›è¦æ±‚ï¼Œç°æœ‰çš„æ§åˆ¶æŠ€æœ¯åˆä¼šé™ä½è¾“å‡ºè´¨é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯è§£é‡Šæ€§çš„è§£ç ç­–ç•¥â€”â€”å½’å› å¼•å¯¼è§£ç ï¼ˆAGDï¼‰ã€‚AGDä¸ç›´æ¥æ“çºµæ¨¡å‹æ¿€æ´»ï¼Œè€Œæ˜¯è€ƒè™‘ä¸€ç»„é«˜æ¦‚ç‡çš„å€™é€‰è¾“å‡ºtokenï¼Œå¹¶é€‰æ‹©å¯¹ç”¨æˆ·å®šä¹‰çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰å½’å› æœ€é«˜çš„tokenã€‚ROIå¯ä»¥çµæ´»åœ°å®šä¹‰åœ¨æ¨¡å‹è¾“å…¥æˆ–å†…éƒ¨ç»„ä»¶çš„ä¸åŒéƒ¨åˆ†ï¼Œä»è€Œå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹æœç€æœŸæœ›çš„è¡Œä¸ºå‘å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒAGDåœ¨æŒ‡ä»¤éµå¾ªã€çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æŒ‡ä»¤éµå¾ªçš„æˆåŠŸç‡ï¼Œå¹¶å‡å°‘å¹»è§‰ï¼Œæé«˜äº‹å®å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„ã€åŸºäºç†µçš„AGDå˜ä½“ï¼Œé€šè¿‡ä»…åœ¨æ¨¡å‹ä¸ç¡®å®šæ—¶åº”ç”¨å¼•å¯¼ï¼Œæ¥å‡è½»è´¨é‡ä¸‹é™å¹¶é™ä½è®¡ç®—å¼€é”€ã€‚AGDæ˜¯ä¸€ç§é€šç”¨ã€å¯è§£é‡Šä¸”æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥å¢å¼ºç°ä»£LLMçš„å¯é æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œéš¾ä»¥ä¿è¯ç”Ÿæˆæ–‡æœ¬æ—¢èƒ½å‡†ç¡®éµå¾ªæŒ‡ä»¤ï¼Œåˆèƒ½ä¿è¯äº‹å®çš„å‡†ç¡®æ€§ã€‚ä¼ ç»Ÿçš„è§£ç æ–¹æ³•ç¼ºä¹å¯¹ç”Ÿæˆè¿‡ç¨‹çš„æœ‰æ•ˆæ§åˆ¶ï¼Œè€Œç›´æ¥æ“çºµæ¨¡å‹æ¿€æ´»çš„æ§åˆ¶æ–¹æ³•åˆå®¹æ˜“æŸå®³æ•´ä½“çš„ç”Ÿæˆè´¨é‡ã€‚å› æ­¤ï¼Œå¦‚ä½•æ›´å¯é ã€æ›´å¯æ§åœ°å¼•å¯¼LLMç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAGDçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œé€šè¿‡åˆ†æå€™é€‰tokenå¯¹ç‰¹å®šåŒºåŸŸï¼ˆROIï¼‰çš„å½’å› ï¼Œé€‰æ‹©ä¸ç”¨æˆ·æœŸæœ›è¡Œä¸ºæœ€ç›¸å…³çš„tokenã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥å¹²é¢„æ¨¡å‹å†…éƒ¨çŠ¶æ€ï¼Œä»è€Œåœ¨ä¿è¯ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹ç”Ÿæˆè¿‡ç¨‹çš„æœ‰æ•ˆå¼•å¯¼ã€‚é€šè¿‡å®šä¹‰ä¸åŒçš„ROIï¼Œå¯ä»¥çµæ´»åœ°æ§åˆ¶LLMçš„è¡Œä¸ºï¼Œä¾‹å¦‚ï¼Œå¼•å¯¼å…¶ä½¿ç”¨ç‰¹å®šçš„çŸ¥è¯†æ¥æºæˆ–éµå¾ªç‰¹å®šçš„æŒ‡ä»¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAGDçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ç”Ÿæˆä¸€ç»„é«˜æ¦‚ç‡çš„å€™é€‰è¾“å‡ºtokenï¼›2) å®šä¹‰ç”¨æˆ·æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ï¼ŒROIå¯ä»¥æ˜¯è¾“å…¥æ–‡æœ¬çš„ç‰¹å®šéƒ¨åˆ†ï¼Œä¹Ÿå¯ä»¥æ˜¯æ¨¡å‹çš„å†…éƒ¨ç»„ä»¶ï¼›3) è®¡ç®—æ¯ä¸ªå€™é€‰tokenå¯¹ROIçš„å½’å› å€¼ï¼›4) é€‰æ‹©å½’å› å€¼æœ€é«˜çš„tokenä½œä¸ºæœ€ç»ˆçš„è¾“å‡ºã€‚è‡ªé€‚åº”AGDå˜ä½“åˆ™åœ¨æ¨¡å‹ä¸ç¡®å®šæ—¶ï¼ˆé«˜ç†µï¼‰æ‰åº”ç”¨å½’å› å¼•å¯¼ï¼Œé™ä½è®¡ç®—å¼€é”€å’Œè´¨é‡æŸå¤±ã€‚

**å…³é”®åˆ›æ–°**ï¼šAGDçš„å…³é”®åˆ›æ–°åœ¨äºå°†å¯è§£é‡Šæ€§æ–¹æ³•å¼•å…¥åˆ°LLMçš„è§£ç è¿‡ç¨‹ä¸­ã€‚ä¸ä¼ ç»Ÿçš„è§£ç æ–¹æ³•ä¸åŒï¼ŒAGDä¸æ˜¯ç›´æ¥åŸºäºæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒé€‰æ‹©tokenï¼Œè€Œæ˜¯è€ƒè™‘äº†tokenå¯¹ç‰¹å®šåŒºåŸŸçš„å½’å› ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ç§æ›´ç»†ç²’åº¦çš„æ§åˆ¶æ–¹å¼ï¼Œå¯ä»¥æ›´ç²¾ç¡®åœ°å¼•å¯¼LLMçš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”AGDå˜ä½“é€šè¿‡åŠ¨æ€è°ƒæ•´å¼•å¯¼å¼ºåº¦ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ–¹æ³•çš„æ•ˆç‡å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šROIçš„å®šä¹‰æ˜¯AGDçš„å…³é”®è®¾è®¡ä¹‹ä¸€ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡éœ€æ±‚ï¼Œçµæ´»åœ°å®šä¹‰ROIã€‚ä¾‹å¦‚ï¼Œåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­ï¼Œå¯ä»¥å°†ROIå®šä¹‰ä¸ºå¤–éƒ¨çŸ¥è¯†åº“æˆ–æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†è¡¨ç¤ºã€‚å½’å› æ–¹æ³•çš„é€‰æ‹©ä¹Ÿå¾ˆé‡è¦ï¼Œè®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†æŸç§ç‰¹å®šçš„å½’å› ç®—æ³•ï¼ˆå…·ä½“ç®—æ³•æœªçŸ¥ï¼‰ã€‚è‡ªé€‚åº”AGDå˜ä½“ä¸­ï¼Œç†µé˜ˆå€¼çš„è®¾ç½®ä¼šå½±å“å¼•å¯¼çš„é¢‘ç‡å’Œå¼ºåº¦ï¼Œéœ€è¦åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAGDåœ¨æŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†Llama 3.1çš„æˆåŠŸç‡ï¼Œä»66.0%æå‡åˆ°79.1%ã€‚åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­ï¼ŒAGDèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å¹»è§‰ï¼Œæé«˜äº‹å®å‡†ç¡®æ€§ã€‚è‡ªé€‚åº”AGDå˜ä½“åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½äº†è®¡ç®—å¼€é”€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AGDå¯åº”ç”¨äºå„ç§éœ€è¦å¯é å’Œå¯æ§æ–‡æœ¬ç”Ÿæˆçš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šæ™ºèƒ½å®¢æœã€å†…å®¹åˆ›ä½œã€æœºå™¨ç¿»è¯‘ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡å¼•å¯¼LLMä½¿ç”¨ç‰¹å®šçš„çŸ¥è¯†æ¥æºæˆ–éµå¾ªç‰¹å®šçš„æŒ‡ä»¤ï¼Œå¯ä»¥æé«˜ç”Ÿæˆæ–‡æœ¬çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼ŒAGDçš„å¯è§£é‡Šæ€§ä½¿å…¶æ›´å®¹æ˜“è°ƒè¯•å’Œä¼˜åŒ–LLMçš„è¡Œä¸ºï¼Œä»è€Œæé«˜ç”¨æˆ·ä¿¡ä»»åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The capacity of Large Language Models (LLMs) to follow complex instructions and generate factually accurate text is critical for their real-world application. However, standard decoding methods often fail to robustly satisfy these requirements, while existing control techniques frequently degrade general output quality. In this work, we introduce Attribution-Guided Decoding (AGD), an interpretability-based decoding strategy. Instead of directly manipulating model activations, AGD considers a set of high-probability output token candidates and selects the one that exhibits the highest attribution to a user-defined Region of Interest (ROI). This ROI can be flexibly defined over different parts of the model's input or internal components, allowing AGD to steer generation towards various desirable behaviors. We demonstrate AGD's efficacy across three challenging domains. For instruction following, we show that AGD significantly boosts adherence (e.g., improving the overall success rate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show that guiding generation towards usage of internal knowledge components or contextual sources can reduce hallucinations and improve factual accuracy in both closed-book and open-book settings. Furthermore, we propose an adaptive, entropy-based variant of AGD that mitigates quality degradation and reduces computational overhead by applying guidance only when the model is uncertain. Our work presents a versatile, more interpretable, and effective method for enhancing the reliability of modern LLMs.

