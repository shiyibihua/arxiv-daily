---
layout: default
title: Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning
---

# Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26578" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26578v1</a>
  <a href="https://arxiv.org/pdf/2509.26578.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26578v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26578v1', 'Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zheng Zhang, Ziwei Shan, Kaitao Song, Yexin Li, Kan Ren

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¡ä»¶å¥–åŠ±å»ºæ¨¡ï¼ˆCRMï¼‰ä»¥æå‡LLMæ¨ç†èƒ½åŠ›ï¼Œè§£å†³è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„å±€é™æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `è¿‡ç¨‹å¥–åŠ±æ¨¡å‹` `æ¡ä»¶å¥–åŠ±å»ºæ¨¡` `ä¿¡ç”¨åˆ†é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰æœªèƒ½å……åˆ†æ•æ‰æ¨ç†æ­¥éª¤é—´çš„ä¾èµ–å…³ç³»ï¼Œä¸”éš¾ä»¥å°†è¿‡ç¨‹å¥–åŠ±ä¸æœ€ç»ˆç»“æœå¯¹é½ï¼Œå¯¼è‡´ä¿¡ç”¨åˆ†é…æ¨¡ç³Šã€‚
2. è®ºæ–‡æå‡ºæ¡ä»¶å¥–åŠ±å»ºæ¨¡ï¼ˆCRMï¼‰ï¼Œå°†LLMæ¨ç†è§†ä¸ºæ—¶é—´è¿‡ç¨‹ï¼Œå¥–åŠ±ä¸ä»…ä¾èµ–äºå‰åºæ­¥éª¤ï¼Œè¿˜ä¸æœ€ç»ˆç»“æœæ˜¾å¼å…³è”ï¼Œæ•æ‰å› æœå…³ç³»ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCRMåœ¨Best-of-NæŠ½æ ·ã€æŸæœç´¢å’Œå¼ºåŒ–å­¦ä¹ ä¸­å‡ä¼˜äºç°æœ‰å¥–åŠ±æ¨¡å‹ï¼Œä¸”å¯¹å¥–åŠ±é»‘å®¢æ”»å‡»æ›´å…·é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰é€šè¿‡å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€æ­¥æ¨ç†ä»¥è·å¾—æœ€ç»ˆç­”æ¡ˆï¼Œå·²æˆä¸ºå¢å¼ºLLMæ¨ç†èƒ½åŠ›çš„ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PRMsè¦ä¹ˆå­¤ç«‹åœ°å¯¹å¾…æ¯ä¸ªæ¨ç†æ­¥éª¤ï¼Œæœªèƒ½æ•æ‰æ­¥éª¤é—´çš„ä¾èµ–å…³ç³»ï¼Œè¦ä¹ˆéš¾ä»¥å°†è¿‡ç¨‹å¥–åŠ±ä¸æœ€ç»ˆç»“æœå¯¹é½ã€‚å› æ­¤ï¼Œå¥–åŠ±ä¿¡å·æœªèƒ½å°Šé‡åºåˆ—æ¨ç†ä¸­çš„æ—¶é—´å› æœå…³ç³»ï¼Œå¹¶é¢ä¸´ç€æ¨¡ç³Šçš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚è¿™äº›é™åˆ¶ä½¿å¾—ä¸‹æ¸¸æ¨¡å‹å®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»ï¼Œå¹¶å¯¼è‡´æ¬¡ä¼˜æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ¡ä»¶å¥–åŠ±å»ºæ¨¡ï¼ˆCRMï¼‰ï¼Œå®ƒå°†LLMæ¨ç†è§†ä¸ºä¸€ä¸ªå¯¼è‡´æ­£ç¡®ç­”æ¡ˆçš„æ—¶é—´è¿‡ç¨‹ã€‚æ¯ä¸ªæ¨ç†æ­¥éª¤çš„å¥–åŠ±ä¸ä»…ä»¥å…ˆå‰çš„æ­¥éª¤ä¸ºæ¡ä»¶ï¼Œè€Œä¸”è¿˜æ˜ç¡®åœ°ä¸æ¨ç†è½¨è¿¹çš„æœ€ç»ˆç»“æœç›¸å…³è”ã€‚é€šè¿‡å¼ºåˆ¶æ‰§è¡Œæ¡ä»¶æ¦‚ç‡è§„åˆ™ï¼Œæˆ‘ä»¬çš„è®¾è®¡æ•æ‰äº†æ¨ç†æ­¥éª¤ä¹‹é—´çš„å› æœå…³ç³»ï¼Œå¹¶ä¸ç»“æœçš„è”ç³»å…è®¸ç²¾ç¡®åœ°å½’å› äºæ¯ä¸ªä¸­é—´æ­¥éª¤ï¼Œä»è€Œè§£å†³äº†ä¿¡ç”¨åˆ†é…çš„æ¨¡ç³Šæ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¿™ç§ä¸€è‡´çš„æ¦‚ç‡å»ºæ¨¡ï¼ŒCRMäº§ç”Ÿçš„å¥–åŠ±èƒ½å¤Ÿå®ç°æ›´å¯é çš„è·¨æ ·æœ¬æ¯”è¾ƒã€‚åœ¨Best-of-NæŠ½æ ·ã€æŸæœç´¢å’Œå¼ºåŒ–å­¦ä¹ æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒCRMå§‹ç»ˆä¼˜äºç°æœ‰çš„å¥–åŠ±æ¨¡å‹ï¼Œä¸ºå¢å¼ºLLMæ¨ç†æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§æ¡†æ¶ã€‚ç‰¹åˆ«æ˜¯ï¼ŒCRMå¯¹å¥–åŠ±é»‘å®¢æ”»å‡»æ›´å…·é²æ£’æ€§ï¼Œå¹¶æä¾›ç¨³å®šçš„ä¸‹æ¸¸æ”¹è¿›ï¼Œè€Œæ— éœ€ä¾èµ–äºæ¥è‡ªçœŸå®å€¼çš„å¯éªŒè¯å¥–åŠ±ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨æŒ‡å¯¼LLMè¿›è¡Œæ¨ç†æ—¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯æœªèƒ½æ•æ‰æ¨ç†æ­¥éª¤ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå°†æ¯ä¸ªæ­¥éª¤å­¤ç«‹åœ°çœ‹å¾…ï¼›äºŒæ˜¯éš¾ä»¥å°†ä¸­é—´æ­¥éª¤çš„å¥–åŠ±ä¸æœ€ç»ˆç»“æœå¯¹é½ï¼Œå¯¼è‡´ä¿¡ç”¨åˆ†é…æ¨¡ç³Šï¼Œä½¿å¾—æ¨¡å‹å®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»ï¼Œæœ€ç»ˆå½±å“æ¨ç†æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMçš„æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªæ—¶é—´åºåˆ—è¿‡ç¨‹ï¼Œå¹¶æå‡ºæ¡ä»¶å¥–åŠ±å»ºæ¨¡ï¼ˆCRMï¼‰ã€‚CRMçš„æ ¸å¿ƒåœ¨äºï¼Œæ¯ä¸ªæ¨ç†æ­¥éª¤çš„å¥–åŠ±ä¸ä»…å–å†³äºä¹‹å‰çš„æ­¥éª¤ï¼Œè¿˜æ˜¾å¼åœ°ä¸æœ€ç»ˆçš„æ¨ç†ç»“æœç›¸å…³è”ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCRMæ—¨åœ¨æ•æ‰æ¨ç†æ­¥éª¤ä¹‹é—´çš„å› æœå…³ç³»ï¼Œå¹¶è§£å†³ä¿¡ç”¨åˆ†é…çš„æ¨¡ç³Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCRMçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®éƒ¨åˆ†ï¼šé¦–å…ˆï¼Œå°†LLMçš„æ¨ç†è¿‡ç¨‹è§†ä¸ºä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€‚å…¶æ¬¡ï¼Œå®šä¹‰å¥–åŠ±å‡½æ•°ï¼Œè¯¥å¥–åŠ±å‡½æ•°ä¸ä»…è€ƒè™‘å½“å‰æ­¥éª¤çš„çŠ¶æ€å’ŒåŠ¨ä½œï¼Œè¿˜è€ƒè™‘æœ€ç»ˆçš„æ¨ç†ç»“æœã€‚ç¬¬ä¸‰ï¼Œä½¿ç”¨æ¡ä»¶æ¦‚ç‡è§„åˆ™æ¥å»ºæ¨¡æ¨ç†æ­¥éª¤ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œç¡®ä¿å¥–åŠ±ä¿¡å·èƒ½å¤Ÿåæ˜ æ—¶é—´å› æœå…³ç³»ã€‚æœ€åï¼Œé€šè¿‡ä¼˜åŒ–è¯¥å¥–åŠ±å‡½æ•°ï¼Œè®­ç»ƒLLMä»¥è·å¾—æ›´å¥½çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šCRMæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå…¶æ¡ä»¶å¥–åŠ±çš„è®¾è®¡ã€‚ä¸ä¼ ç»Ÿçš„PRMsä¸åŒï¼ŒCRMçš„å¥–åŠ±å‡½æ•°æ˜¾å¼åœ°è€ƒè™‘äº†æœ€ç»ˆçš„æ¨ç†ç»“æœï¼Œä»è€Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°æ¯ä¸ªä¸­é—´æ­¥éª¤çš„è´¡çŒ®ã€‚è¿™ç§æ¡ä»¶å¥–åŠ±çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ¨ç†æ­¥éª¤ä¹‹é—´çš„å› æœå…³ç³»ï¼Œå¹¶é¿å…äº†ä¿¡ç”¨åˆ†é…çš„æ¨¡ç³Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCRMçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ¡ä»¶æ¦‚ç‡æ¥å»ºæ¨¡æ¨ç†æ­¥éª¤ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå…·ä½“æ¥è¯´ï¼Œä½¿ç”¨P(reward_t | state_t, action_t, outcome)æ¥è¡¨ç¤ºåœ¨ç»™å®šå½“å‰çŠ¶æ€ã€åŠ¨ä½œå’Œæœ€ç»ˆç»“æœçš„æƒ…å†µä¸‹ï¼Œå½“å‰æ­¥éª¤çš„å¥–åŠ±ï¼›2) è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å¥–åŠ±å‡½æ•°éœ€è¦èƒ½å¤ŸåŒºåˆ†æ­£ç¡®çš„æ¨ç†æ­¥éª¤å’Œé”™è¯¯çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶ä¸”èƒ½å¤Ÿåæ˜ æ¯ä¸ªæ­¥éª¤å¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ï¼›3) ä½¿ç”¨åˆé€‚çš„ä¼˜åŒ–ç®—æ³•æ¥è®­ç»ƒLLMï¼Œä¾‹å¦‚å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCRMåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰çš„å¥–åŠ±æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨Best-of-NæŠ½æ ·ä¸­ï¼ŒCRMèƒ½å¤Ÿé€‰æ‹©æ›´å‡†ç¡®çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œæé«˜æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒCRMå¯¹å¥–åŠ±é»‘å®¢æ”»å‡»å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œå³ä½¿åœ¨å­˜åœ¨æ¶æ„å¥–åŠ±ä¿¡å·çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œéœ€æŸ¥é˜…è®ºæ–‡å…¨æ–‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœ¨å‡†ç¡®æ€§ã€å¯é æ€§å’Œé²æ£’æ€§æ–¹é¢çš„æ€§èƒ½ï¼Œå¹¶æœ‰æœ›åœ¨åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning capabilities of large language models (LLMs) by guiding their step-by-step reasoning toward a final answer. However, existing PRMs either treat each reasoning step in isolation, failing to capture inter-step dependencies, or struggle to align process rewards with the final outcome. Consequently, the reward signal fails to respect temporal causality in sequential reasoning and faces ambiguous credit assignment. These limitations make downstream models vulnerable to reward hacking and lead to suboptimal performance. In this work, we propose Conditional Reward Modeling (CRM) that frames LLM reasoning as a temporal process leading to a correct answer. The reward of each reasoning step is not only conditioned on the preceding steps but also explicitly linked to the final outcome of the reasoning trajectory. By enforcing conditional probability rules, our design captures the causal relationships among reasoning steps, with the link to the outcome allowing precise attribution of each intermediate step, thereby resolving credit assignment ambiguity. Further, through this consistent probabilistic modeling, the rewards produced by CRM enable more reliable cross-sample comparison. Experiments across Best-of-N sampling, beam search and reinforcement learning demonstrate that CRM consistently outperforms existing reward models, offering a principled framework for enhancing LLM reasoning. In particular, CRM is more robust to reward hacking and delivers stable downstream improvements without relying on verifiable rewards derived from ground truth.

