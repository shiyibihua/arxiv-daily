---
layout: default
title: Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking
---

# Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25712" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25712v1</a>
  <a href="https://arxiv.org/pdf/2509.25712.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25712v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25712v1', 'Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dengming Zhang, Xiaowen Ma, Zhenliang Ni, Zhenkai Wu, Han Shu, Xin Jiang, Xinghao Chen

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Littleor/ExpertMerging)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºExpert Mergingï¼Œé€šè¿‡æ— ç›‘ç£ä¸“å®¶å¯¹é½å’Œé‡è¦æ€§å¼•å¯¼çš„åˆ†å±‚åˆ†å—å®ç°æ¨¡å‹èåˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹èåˆ` `ä¸“å®¶æ¨¡å‹` `æ— ç›‘ç£å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å‚æ•°é«˜æ•ˆ` `å±‚é‡è¦æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹èåˆæ–¹æ³•æˆ–ä¾èµ–æ‰‹åŠ¨è°ƒæ•´ï¼Œæˆ–å¿½ç•¥å±‚é—´å·®å¼‚ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. Expert Mergingé€šè¿‡å­¦ä¹ å±‚çº§ç³»æ•°å¯¹é½æ¨¡å‹çŠ¶æ€å’Œlogitsï¼Œå®ç°é«˜æ•ˆæ¨¡å‹èåˆã€‚
3. Expert Merging++å¼•å…¥é‡è¦æ€§å¼•å¯¼åˆ†å—ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šç›‘ç£æ··åˆè®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡å‹èåˆå°†å¤šä¸ªé¢†åŸŸä¸“å®¶æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªæ¨¡å‹ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)æä¾›äº†ä¸€ç§å®ç”¨é€”å¾„ï¼Œä½¿å…¶å…·å¤‡å¹¿æ³›çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€è”åˆè®­ç»ƒæˆ–æœåŠ¡å¤šä¸ªæ¨¡å‹ã€‚ç„¶è€Œï¼Œå…è®­ç»ƒæ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨è°ƒæ•´çš„ç³»æ•°ï¼Œè€ŒåŸºäºè®­ç»ƒçš„æ–¹æ³•ä¸»è¦å¯¹é½å‚æ•°è€Œéä¸‹æ¸¸ä»»åŠ¡è¡Œä¸ºï¼Œå¹¶ä¸”é€šå¸¸ç»Ÿä¸€å¯¹å¾…æ‰€æœ‰å±‚ï¼Œå¿½ç•¥äº†å±‚é—´çš„å¼‚è´¨æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†Expert Mergingï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§è®­ç»ƒæ–¹æ³•ï¼Œä»…ä½¿ç”¨æœªæ ‡è®°çš„æ ¡å‡†æ•°æ®å­¦ä¹ ä¸€å°ç»„é€å±‚ç³»æ•°ã€‚ä¼˜åŒ–è¿™äº›ç³»æ•°ï¼Œä»¥æ˜¾å¼åœ°å°†åˆå¹¶æ¨¡å‹çš„éšè—çŠ¶æ€å’Œlogitsä¸ç›¸åº”ä¸“å®¶çš„éšè—çŠ¶æ€å’Œlogitså¯¹é½ï¼Œå¹¶ä½¿ç”¨ç³»æ•°æ­£åˆ™åŒ–å™¨æ¥ä¿è¯ç¨³å®šæ€§ï¼Œä»¥åŠä½¿ç”¨ä»»åŠ¡åŠ æƒæŸå¤±æ¥å®ç°å¯æ§çš„æƒè¡¡ã€‚ä¸ºäº†æ•æ‰å±‚é—´å˜åŒ–ï¼ŒExpert Merging++é€šè¿‡é‡è¦æ€§å¼•å¯¼çš„åˆ†å—æ¥å¢å¼ºæ­¤è®¾è®¡ï¼šä¸€ç§å½’ä¸€åŒ–çš„å±‚é‡è¦æ€§åº¦é‡ï¼Œä»å­¦ä¹ åˆ°çš„ç³»æ•°ã€ä»»åŠ¡å‘é‡å¹…åº¦å’Œå‚æ•°è®¡æ•°ä¸­å¯¼å‡ºï¼Œå°†æ›´å¤šçš„åˆ†å—ç³»æ•°åˆ†é…ç»™é«˜é‡è¦æ€§å±‚ï¼ŒåŒæ—¶ä¿æŒä½é‡è¦æ€§å±‚çš„è½»é‡çº§ã€‚ç»“æœæ˜¯ä¸€ç§æ— æ ‡ç­¾ã€å‚æ•°é«˜æ•ˆä¸”å¯æ‰©å±•çš„LLMå’ŒMLLMå¤šä¸“å®¶æ¨¡å‹èåˆæ–¹æ³•ã€‚åœ¨MLLMéª¨å¹²ç½‘ç»œ(InternVLå’ŒQwen2-VL)å’ŒLLMéª¨å¹²ç½‘ç»œ(Mistral)ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†å¼ºå¤§çš„å…è®­ç»ƒå’ŒåŸºäºè®­ç»ƒçš„èåˆåŸºçº¿ï¼ŒExpert Merging++æä¾›äº†è¿›ä¸€æ­¥çš„å¢ç›Šï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†ç›‘ç£æ··åˆè®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ¨¡å‹èåˆæ—¨åœ¨å°†å¤šä¸ªåœ¨ç‰¹å®šé¢†åŸŸè®­ç»ƒçš„ä¸“å®¶æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œä»è€Œé¿å…è”åˆè®­ç»ƒæˆ–åŒæ—¶éƒ¨ç½²å¤šä¸ªæ¨¡å‹çš„æˆæœ¬ã€‚ç°æœ‰çš„å…è®­ç»ƒæ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨è°ƒæ•´çš„èåˆæƒé‡ï¼Œç¼ºä¹çµæ´»æ€§å’Œè‡ªé€‚åº”æ€§ã€‚åŸºäºè®­ç»ƒçš„æ–¹æ³•é€šå¸¸ç›´æ¥å¯¹é½æ¨¡å‹å‚æ•°ï¼Œè€Œå¿½ç•¥äº†ä¸‹æ¸¸ä»»åŠ¡çš„è¡Œä¸ºï¼Œå¹¶ä¸”å¯¹æ‰€æœ‰å±‚é‡‡ç”¨ç»Ÿä¸€çš„å¤„ç†æ–¹å¼ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨å±‚é—´çš„å¼‚è´¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šExpert Mergingçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å­¦ä¹ ä¸€ç»„é€å±‚çš„èåˆç³»æ•°ï¼Œæ˜¾å¼åœ°å¯¹é½åˆå¹¶æ¨¡å‹çš„éšè—çŠ¶æ€å’Œlogitsä¸å„ä¸ªä¸“å®¶æ¨¡å‹çš„å¯¹åº”çŠ¶æ€ã€‚è¿™ç§æ–¹æ³•å…³æ³¨äºä¸‹æ¸¸ä»»åŠ¡çš„è¡Œä¸ºï¼Œè€Œéç›´æ¥å¯¹é½å‚æ•°ã€‚Expert Merging++è¿›ä¸€æ­¥å¼•å…¥äº†é‡è¦æ€§å¼•å¯¼çš„åˆ†å—æœºåˆ¶ï¼Œæ ¹æ®å±‚çš„é‡è¦æ€§åŠ¨æ€åœ°åˆ†é…èåˆå‚æ•°ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨æ¨¡å‹å†…éƒ¨çš„ç»“æ„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šExpert Mergingçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨æœªæ ‡è®°çš„æ ¡å‡†æ•°æ®ï¼Œé€šè¿‡æœ€å°åŒ–åˆå¹¶æ¨¡å‹ä¸ä¸“å®¶æ¨¡å‹ä¹‹é—´çš„éšè—çŠ¶æ€å’Œlogitsçš„å·®å¼‚æ¥å­¦ä¹ é€å±‚èåˆç³»æ•°ã€‚2) ä½¿ç”¨ç³»æ•°æ­£åˆ™åŒ–å™¨æ¥ä¿è¯èåˆè¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚3) ä½¿ç”¨ä»»åŠ¡åŠ æƒæŸå¤±æ¥å®ç°ä¸åŒä»»åŠ¡ä¹‹é—´çš„å¯æ§æƒè¡¡ã€‚Expert Merging++åœ¨æ­¤åŸºç¡€ä¸Šï¼Œé¦–å…ˆè®¡ç®—æ¯ä¸€å±‚çš„é‡è¦æ€§ï¼Œç„¶åæ ¹æ®é‡è¦æ€§å°†å±‚åˆ’åˆ†ä¸ºä¸åŒçš„å—ï¼Œå¹¶ä¸ºæ¯ä¸ªå—åˆ†é…ç‹¬ç«‹çš„èåˆç³»æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šExpert Mergingçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ˜¾å¼åœ°å¯¹é½éšè—çŠ¶æ€å’Œlogitsï¼Œè€Œéç›´æ¥å¯¹é½å‚æ•°ï¼Œä»è€Œæ›´å¥½åœ°å…³æ³¨ä¸‹æ¸¸ä»»åŠ¡çš„è¡Œä¸ºã€‚Expert Merging++çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†é‡è¦æ€§å¼•å¯¼çš„åˆ†å—æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å±‚çš„é‡è¦æ€§åŠ¨æ€åœ°åˆ†é…èåˆå‚æ•°ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨æ¨¡å‹å†…éƒ¨çš„ç»“æ„ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šExpert Mergingä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±å‡½æ•°æ¥è¡¡é‡åˆå¹¶æ¨¡å‹å’Œä¸“å®¶æ¨¡å‹ä¹‹é—´çš„éšè—çŠ¶æ€å’Œlogitsçš„å·®å¼‚ã€‚ç³»æ•°æ­£åˆ™åŒ–å™¨é‡‡ç”¨L2æ­£åˆ™åŒ–ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ä»»åŠ¡åŠ æƒæŸå¤±æ ¹æ®ä¸åŒä»»åŠ¡çš„é‡è¦æ€§è°ƒæ•´æŸå¤±å‡½æ•°çš„æƒé‡ã€‚Expert Merging++ä¸­çš„å±‚é‡è¦æ€§åº¦é‡ç»¼åˆè€ƒè™‘äº†å­¦ä¹ åˆ°çš„ç³»æ•°ã€ä»»åŠ¡å‘é‡å¹…åº¦å’Œå‚æ•°è®¡æ•°ã€‚åˆ†å—ç­–ç•¥é‡‡ç”¨äº†ä¸€ç§å½’ä¸€åŒ–çš„æ–¹æ³•ï¼Œç¡®ä¿æ€»çš„å‚æ•°é‡ä¿æŒä¸å˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒExpert Mergingåœ¨MLLMï¼ˆInternVLå’ŒQwen2-VLï¼‰å’ŒLLMï¼ˆMistralï¼‰ä¸Šå‡ä¼˜äºç°æœ‰çš„å…è®­ç»ƒå’ŒåŸºäºè®­ç»ƒçš„èåˆåŸºçº¿ã€‚Expert Merging++è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†ç›‘ç£æ··åˆè®­ç»ƒã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªMLLMä»»åŠ¡ä¸Šï¼ŒExpert Merging++çš„æ€§èƒ½æ¯”æœ€ä½³åŸºçº¿æé«˜äº†5%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Expert Mergingå¯åº”ç”¨äºå„ç§åœºæ™¯ï¼Œä¾‹å¦‚å°†å¤šä¸ªåœ¨ä¸åŒé¢†åŸŸè®­ç»ƒçš„LLMæˆ–MLLMèåˆä¸ºä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•è¿˜å¯ç”¨äºæ„å»ºç‰¹å®šé¢†åŸŸçš„ä¸“å®¶æ¨¡å‹ï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æç­‰ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ç”¨äºæ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿï¼Œé€šè¿‡å°†å¤šä¸ªå°å‹æ¨¡å‹èåˆä¸ºä¸€ä¸ªå¤§å‹æ¨¡å‹ï¼Œä»è€Œå‡å°‘æ¨¡å‹çš„å­˜å‚¨ç©ºé—´å’Œè®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model merging, which combines multiple domain-specialized experts into a single model, offers a practical path to endow Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) with broad capabilities without the cost of joint training or serving many models. However, training-free methods rely on hand-tuned coefficients, whereas training-based methods primarily align parameters rather than downstream task behavior and typically treat all layers uniformly, ignoring inter-layer heterogeneity. We introduce Expert Merging, a training-light method that learns a small set of layer-wise coefficients using only unlabeled calibration data. The coefficients are optimized to explicitly align the merged model's hidden states and logits with those of the corresponding experts, with a coefficient regularizer for stability and task-weighted losses for controllable trade-offs. To capture inter-layer variation, Expert Merging++ augments this design with importance-guided chunking: a normalized layer-importance metric, derived from learned coefficients, task-vector magnitudes, and parameter counts, allocates more chunk-wise coefficients to high-importance layers while keeping low-importance layers lightweight. The result is a label-free, parameter-efficient, and scalable approach to multi-expert model merging across LLMs and MLLMs. Across MLLM backbones (InternVL and Qwen2-VL) and the LLM backbone (Mistral), our method surpasses strong training-free and training-based merging baselines, with Expert Merging++ delivering further gains and, in some cases, even exceeding supervised Mixture Training. The source code is available at https://github.com/Littleor/ExpertMerging.

