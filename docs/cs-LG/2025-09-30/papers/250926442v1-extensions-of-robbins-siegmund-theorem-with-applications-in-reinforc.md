---
layout: default
title: Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning
---

# Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26442" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26442v1</a>
  <a href="https://arxiv.org/pdf/2509.26442.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26442v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26442v1', 'Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinyu Liu, Zixuan Xie, Shangtong Zhang

**åˆ†ç±»**: cs.LG, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ‰©å±•Robbins-Siegmundå®šç†ï¼Œè§£å†³å¼ºåŒ–å­¦ä¹ ä¸­éå¯å’Œé›¶é˜¶é¡¹çš„æ”¶æ•›æ€§åˆ†æéš¾é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Robbins-Siegmundå®šç†` `å¼ºåŒ–å­¦ä¹ ` `éšæœºé€¼è¿‘` `æ”¶æ•›æ€§åˆ†æ` `Q-learning` `å‡½æ•°é€¼è¿‘` `éšæœºè¿‡ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Robbins-Siegmundå®šç†åœ¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ”¶æ•›æ€§åˆ†æä¸­è‡³å…³é‡è¦ï¼Œä½†å…¶å¯¹é›¶é˜¶é¡¹å¯å’Œæ€§çš„è¦æ±‚é™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. è®ºæ–‡é€šè¿‡å¯¹éšæœºè¿‡ç¨‹å¢é‡å¼•å…¥æ–°å‡è®¾ï¼Œæ‰©å±•äº†Robbins-Siegmundå®šç†ï¼Œä½¿å…¶é€‚ç”¨äºé›¶é˜¶é¡¹å¹³æ–¹å¯å’Œçš„æƒ…å†µã€‚
3. æ–°å®šç†æˆåŠŸåº”ç”¨äºQ-learningï¼Œé¦–æ¬¡è·å¾—äº†çº¿æ€§å‡½æ•°é€¼è¿‘ä¸‹Q-learningçš„æ”¶æ•›é€Ÿåº¦ã€é›†ä¸­ç•Œå’ŒLpæ”¶æ•›ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Robbins-Siegmundå®šç†æ˜¯åˆ†æéšæœºé€¼è¿‘å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­å¹¿æ³›ä½¿ç”¨çš„éšæœºè¿­ä»£ç®—æ³•æ”¶æ•›æ€§çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œå…¶åŸå§‹å½¢å¼å­˜åœ¨ä¸€ä¸ªæ˜¾è‘—çš„å±€é™æ€§ï¼Œå³å®ƒè¦æ±‚é›¶é˜¶é¡¹æ˜¯å¯å’Œçš„ã€‚åœ¨è®¸å¤šé‡è¦çš„RLåº”ç”¨ä¸­ï¼Œè¿™ä¸ªå¯å’Œæ¡ä»¶æ— æ³•æ»¡è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æ‰©å±•äº†Robbins-Siegmundå®šç†ï¼Œä½¿å…¶é€‚ç”¨äºé›¶é˜¶é¡¹éå¯å’Œä½†å¹³æ–¹å¯å’Œçš„å‡ ä¹è¶…é…ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å¯¹éšæœºè¿‡ç¨‹çš„å¢é‡å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ã€æ¸©å’Œçš„å‡è®¾ã€‚è¿™ä¸ªå‡è®¾ä¸å¹³æ–¹å¯å’Œæ¡ä»¶ä¸€èµ·ï¼Œå®ç°äº†å‡ ä¹å¿…ç„¶æ”¶æ•›åˆ°ä¸€ä¸ªæœ‰ç•Œé›†åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æä¾›äº†å‡ ä¹å¿…ç„¶æ”¶æ•›é€Ÿåº¦ã€é«˜æ¦‚ç‡é›†ä¸­ç•Œå’Œ$L^p$æ”¶æ•›é€Ÿåº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ–°çš„ç»“æœåº”ç”¨äºéšæœºé€¼è¿‘å’ŒRLã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è·å¾—äº†çº¿æ€§å‡½æ•°é€¼è¿‘çš„Q-learningçš„ç¬¬ä¸€ä¸ªå‡ ä¹å¿…ç„¶æ”¶æ•›é€Ÿåº¦ã€ç¬¬ä¸€ä¸ªé«˜æ¦‚ç‡é›†ä¸­ç•Œå’Œç¬¬ä¸€ä¸ª$L^p$æ”¶æ•›é€Ÿåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„Robbins-Siegmundå®šç†è¦æ±‚é›¶é˜¶é¡¹å¿…é¡»æ˜¯å¯å’Œçš„ï¼Œè¿™åœ¨è®¸å¤šå¼ºåŒ–å­¦ä¹ åº”ç”¨ä¸­å¹¶ä¸æˆç«‹ã€‚ä¾‹å¦‚ï¼Œå½“ä½¿ç”¨å‡½æ•°é€¼è¿‘æ—¶ï¼Œè¯¯å·®å¯èƒ½ä¸ä¼šå¿«é€Ÿè¡°å‡åˆ°é›¶ï¼Œå¯¼è‡´é›¶é˜¶é¡¹ä¸å¯å’Œã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ†æé›¶é˜¶é¡¹éå¯å’Œæƒ…å†µä¸‹çš„ç®—æ³•æ”¶æ•›æ€§æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ”¾å®½å¯¹é›¶é˜¶é¡¹çš„è¦æ±‚ï¼Œå…è®¸å…¶ä»…ä¸ºå¹³æ–¹å¯å’Œï¼Œå¹¶å¼•å…¥å…³äºéšæœºè¿‡ç¨‹å¢é‡çš„æ–°çš„æ¸©å’Œå‡è®¾ï¼Œä»è€Œæ‰©å±•Robbins-Siegmundå®šç†ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨è¦†ç›–æ›´å¹¿æ³›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ŒåŒæ—¶ä¿æŒæ”¶æ•›æ€§åˆ†æçš„ä¸¥è°¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é¦–å…ˆæå‡ºäº†æ‰©å±•çš„Robbins-Siegmundå®šç†ï¼Œç„¶åå°†å…¶åº”ç”¨äºéšæœºé€¼è¿‘å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆè¯æ˜äº†åœ¨æ–°çš„å‡è®¾ä¸‹ï¼Œéšæœºè¿‡ç¨‹å‡ ä¹å¿…ç„¶æ”¶æ•›åˆ°ä¸€ä¸ªæœ‰ç•Œé›†åˆã€‚ç„¶åï¼Œæ¨å¯¼å‡ºäº†æ”¶æ•›é€Ÿåº¦ã€é«˜æ¦‚ç‡é›†ä¸­ç•Œå’Œ$L^p$æ”¶æ•›é€Ÿåº¦ã€‚æœ€åï¼Œå°†è¿™äº›ç»“æœåº”ç”¨äºçº¿æ€§å‡½æ•°é€¼è¿‘çš„Q-learningç®—æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¯¹éšæœºè¿‡ç¨‹å¢é‡å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ã€æ¸©å’Œçš„å‡è®¾ï¼Œå¹¶ç»“åˆå¹³æ–¹å¯å’Œæ¡ä»¶ï¼Œå®ç°äº†å¯¹å‡ ä¹è¶…é…çš„æ”¶æ•›æ€§åˆ†æã€‚ä¸åŸå§‹Robbins-Siegmundå®šç†ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸å†è¦æ±‚é›¶é˜¶é¡¹æ˜¯å¯å’Œçš„ï¼Œä»è€Œæ‰©å¤§äº†é€‚ç”¨èŒƒå›´ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) æå‡ºäº†æ–°çš„å¢é‡å‡è®¾ï¼Œè¯¥å‡è®¾éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡ŒéªŒè¯ã€‚(2) åŸºäºæ‰©å±•çš„Robbins-Siegmundå®šç†ï¼Œæ¨å¯¼å‡ºäº†æ”¶æ•›é€Ÿåº¦ã€é«˜æ¦‚ç‡é›†ä¸­ç•Œå’Œ$L^p$æ”¶æ•›é€Ÿåº¦ï¼Œè¿™äº›ç»“æœå¯ä»¥ç”¨äºè¯„ä¼°ç®—æ³•çš„æ€§èƒ½ã€‚(3) å°†æ‰©å±•çš„å®šç†åº”ç”¨äºçº¿æ€§å‡½æ•°é€¼è¿‘çš„Q-learningç®—æ³•ï¼Œå¹¶è·å¾—äº†ç›¸åº”çš„æ”¶æ•›æ€§ç»“æœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æœ€é‡è¦çš„å®éªŒäº®ç‚¹æ˜¯é¦–æ¬¡ä¸ºçº¿æ€§å‡½æ•°é€¼è¿‘çš„Q-learningç®—æ³•æä¾›äº†å‡ ä¹å¿…ç„¶æ”¶æ•›é€Ÿåº¦ã€é«˜æ¦‚ç‡é›†ä¸­ç•Œå’Œ$L^p$æ”¶æ•›é€Ÿåº¦ã€‚è¿™äº›ç»“æœå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ä¸€ä¸ªç©ºç™½ï¼Œå¹¶ä¸ºQ-learningç®—æ³•çš„ç†è®ºåˆ†ææä¾›äº†é‡è¦çš„ä¾æ®ã€‚è™½ç„¶è®ºæ–‡æ²¡æœ‰æä¾›å…·ä½“çš„æ•°å€¼å®éªŒç»“æœï¼Œä½†å…¶ç†è®ºè´¡çŒ®ä¸ºæœªæ¥çš„å®éªŒç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¼ºåŒ–å­¦ä¹ å’Œéšæœºé€¼è¿‘é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨å‡½æ•°é€¼è¿‘ç­‰å¤æ‚åœºæ™¯ä¸‹ï¼Œä¸ºç®—æ³•çš„æ”¶æ•›æ€§åˆ†ææä¾›äº†æ›´å¼ºçš„ç†è®ºå·¥å…·ã€‚è¿™æœ‰åŠ©äºç ”ç©¶è€…è®¾è®¡å’Œåˆ†ææ›´æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶ä¸ºç®—æ³•çš„å®é™…åº”ç”¨æä¾›ç†è®ºä¿éšœï¼Œä¾‹å¦‚åœ¨æœºå™¨äººæ§åˆ¶ã€èµ„æºç®¡ç†å’Œæ¨èç³»ç»Ÿç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Robbins-Siegmund theorem establishes the convergence of stochastic processes that are almost supermartingales and is foundational for analyzing a wide range of stochastic iterative algorithms in stochastic approximation and reinforcement learning (RL). However, its original form has a significant limitation as it requires the zero-order term to be summable. In many important RL applications, this summable condition, however, cannot be met. This limitation motivates us to extend the Robbins-Siegmund theorem for almost supermartingales where the zero-order term is not summable but only square summable. Particularly, we introduce a novel and mild assumption on the increments of the stochastic processes. This together with the square summable condition enables an almost sure convergence to a bounded set. Additionally, we further provide almost sure convergence rates, high probability concentration bounds, and $L^p$ convergence rates. We then apply the new results in stochastic approximation and RL. Notably, we obtain the first almost sure convergence rate, the first high probability concentration bound, and the first $L^p$ convergence rate for $Q$-learning with linear function approximation.

