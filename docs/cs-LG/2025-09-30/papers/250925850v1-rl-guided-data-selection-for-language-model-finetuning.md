---
layout: default
title: RL-Guided Data Selection for Language Model Finetuning
---

# RL-Guided Data Selection for Language Model Finetuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25850" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25850v1</a>
  <a href="https://arxiv.org/pdf/2509.25850.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25850v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25850v1', 'RL-Guided Data Selection for Language Model Finetuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Animesh Jha, Harshit Gupta, Ananjan Nandi

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: To appear in NeurIPS 2025 Constrained Optimization for ML Workshop

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ•ˆç‡ä¸æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é€‰æ‹©` `å¤§è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•éš¾ä»¥ç›´æ¥åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œä¸”è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥åœ¨æœ‰é™é¢„ç®—ä¸‹ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚
2. å°†æ•°æ®é€‰æ‹©é—®é¢˜å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä½“ï¼Œå­¦ä¹ æœ€ä¼˜æ•°æ®é€‰æ‹©ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—å‡å°‘è®­ç»ƒæ•°æ®å’Œæ—¶é—´çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒçš„æ•°æ®é€‰æ‹©é—®é¢˜å»ºæ¨¡ä¸ºå¸¦é¢„ç®—çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼Œç›®æ ‡æ˜¯åœ¨ä¸¥æ ¼çš„è®­ç»ƒæ•°æ®é¢„ç®—ä¸‹æœ€å¤§åŒ–æ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½ã€‚ç”±äºç›´æ¥æ±‚è§£è¯¥é—®é¢˜é€šå¸¸æ˜¯éš¾ä»¥å¤„ç†çš„ï¼Œå¹¶ä¸”ç°æœ‰çš„è¿‘ä¼¼æ–¹æ³•ä¸»è¦é¢å‘é¢„è®­ç»ƒï¼Œè¿ç§»åˆ°å¾®è°ƒè®¾ç½®æ—¶æ•ˆæœä¸ä½³ã€‚å› æ­¤ï¼Œæœ¬æ–‡å°†è¯¥é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªå¯å¤„ç†çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶ä½¿ç”¨å„ç§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•è®­ç»ƒæ™ºèƒ½ä½“ï¼Œä»¥å­¦ä¹ æœ€ä¼˜çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç”±é«˜æ•ˆçš„ã€åŸºäºä»£ç†æ¨¡å‹çš„å¥–åŠ±ä¿¡å·æŒ‡å¯¼ã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æœ¬æ–‡æ–¹æ³•é€‰æ‹©çš„5%å­é›†è¿›è¡Œè®­ç»ƒï¼Œå…¶æ€§èƒ½ä¸ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œå¾®è°ƒç›¸æ¯”ï¼ŒåŒ¹é…ç”šè‡³è¶…è¿‡äº†å®Œæ•´æ•°æ®é›†çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡æœ€é«˜æå‡äº†10.8ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶å‡å°‘äº†é«˜è¾¾2å€çš„å®é™…è®­ç»ƒæ—¶é—´ï¼Œçªæ˜¾äº†å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼çš„æ•°æ®é€‰æ‹©æ–¹æ³•çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®é¢„ç®—ä¸‹ï¼Œé€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„å­é›†ï¼Œä»¥æœ€å¤§åŒ–æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é‚£äº›é¢å‘é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œåœ¨å¾®è°ƒåœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”ç›´æ¥ä¼˜åŒ–æ•°æ®é€‰æ‹©ç­–ç•¥é€šå¸¸æ˜¯NP-hardé—®é¢˜ï¼Œè®¡ç®—å¤æ‚åº¦é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ•°æ®é€‰æ‹©è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œå­¦ä¹ é€‰æ‹©å“ªäº›æ•°æ®æ ·æœ¬è¿›è¡Œå¾®è°ƒã€‚è¿™ç§æ–¹æ³•å…è®¸æ™ºèƒ½ä½“æ ¹æ®ä»£ç†æ¨¡å‹çš„åé¦ˆï¼Œé€æ­¥ä¼˜åŒ–æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œä»è€Œåœ¨æœ‰é™çš„é¢„ç®—ä¸‹æ‰¾åˆ°æœ€ä¼˜çš„æ•°æ®å­é›†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **ç¯å¢ƒ**ï¼šç”±æ•°æ®é›†å’Œä»£ç†æ¨¡å‹ç»„æˆï¼Œä»£ç†æ¨¡å‹ç”¨äºå¿«é€Ÿè¯„ä¼°é€‰æ‹©çš„æ•°æ®å­é›†çš„è´¨é‡ã€‚2) **æ™ºèƒ½ä½“**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOï¼‰è®­ç»ƒï¼Œè´Ÿè´£é€‰æ‹©æ•°æ®æ ·æœ¬ã€‚3) **å¥–åŠ±å‡½æ•°**ï¼šåŸºäºä»£ç†æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½å˜åŒ–ï¼Œä¸ºæ™ºèƒ½ä½“çš„è¡Œä¸ºæä¾›åé¦ˆã€‚4) **MDP**ï¼šå°†æ•°æ®é€‰æ‹©è¿‡ç¨‹å»ºæ¨¡ä¸ºçŠ¶æ€ï¼ˆå·²é€‰æ‹©çš„æ•°æ®å­é›†ï¼‰ã€åŠ¨ä½œï¼ˆé€‰æ‹©ä¸‹ä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼‰å’Œå¥–åŠ±ï¼ˆä»£ç†æ¨¡å‹æ€§èƒ½æå‡ï¼‰çš„åºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ•°æ®é€‰æ‹©é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªå¯è§£çš„MDPï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ æœ€ä¼˜çš„æ•°æ®é€‰æ‹©ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå¯å‘å¼æˆ–é¢„è®­ç»ƒçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®å¾®è°ƒä»»åŠ¡çš„ç‰¹ç‚¹ï¼Œè‡ªé€‚åº”åœ°é€‰æ‹©æ•°æ®ï¼Œä»è€Œæé«˜å¾®è°ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ä»£ç†æ¨¡å‹ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œé¿å…äº†ç›´æ¥åœ¨å®Œæ•´æ¨¡å‹ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„ã€è®¡ç®—æˆæœ¬è¾ƒä½çš„æ¨¡å‹ä½œä¸ºä»£ç†æ¨¡å‹ï¼Œç”¨äºå¿«é€Ÿè¯„ä¼°æ•°æ®å­é›†çš„è´¨é‡ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œå®ƒéœ€è¦èƒ½å¤Ÿå‡†ç¡®åæ˜ æ•°æ®å­é›†å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚è®ºæ–‡é‡‡ç”¨PPOç®—æ³•è®­ç»ƒæ™ºèƒ½ä½“ï¼Œå¹¶ä»”ç»†è°ƒæ•´äº†è¶…å‚æ•°ï¼Œä»¥ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚å…·ä½“çš„æ•°æ®é€‰æ‹©ç­–ç•¥ç”±æ™ºèƒ½ä½“å­¦ä¹ å¾—åˆ°ï¼Œæ²¡æœ‰é¢„å…ˆè®¾å®šä»»ä½•äººå·¥è§„åˆ™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ–¹æ³•é€‰æ‹©çš„5%æ•°æ®å­é›†è¿›è¡Œå¾®è°ƒï¼Œåœ¨å››ä¸ªæ•°æ®é›†ä¸Šå‡èƒ½è¾¾åˆ°æˆ–è¶…è¿‡ä½¿ç”¨å®Œæ•´æ•°æ®é›†å¾®è°ƒçš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡æœ€é«˜æå‡äº†10.8ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†é«˜è¾¾2å€ã€‚è¿™äº›ç»“æœéªŒè¯äº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ•°æ®é€‰æ‹©æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ¨¡å‹éƒ¨ç½²ã€ä½æˆæœ¬çš„è¯­è¨€æ¨¡å‹å¾®è°ƒç­‰ã€‚é€šè¿‡æ™ºèƒ½åœ°é€‰æ‹©è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ï¼ŒåŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Data selection for finetuning Large Language Models (LLMs) can be framed as a budget-constrained optimization problem: maximizing a model's downstream performance under a strict training data budget. Solving this problem is generally intractable, and existing approximate approaches are pretraining-oriented and transfer poorly to the fine-tuning setting. We reformulate this problem as a tractable Markov Decision Process (MDP) and train agents using various Reinforcement Learning (RL) methods to learn optimal data selection policies, guided by an efficient, proxy-model-based reward signal. Across four datasets, training on a $5\%$ subset selected by our approach matches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy points, while cutting wall-clock training time by up to $2 \times$, highlighting the promise of RL-guided data selection.

