---
layout: default
title: Beyond Token Probes: Hallucination Detection via Activation Tensors with ACT-ViT
---

# Beyond Token Probes: Hallucination Detection via Activation Tensors with ACT-ViT

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00296" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00296v1</a>
  <a href="https://arxiv.org/pdf/2510.00296.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00296v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00296v1', 'Beyond Token Probes: Hallucination Detection via Activation Tensors with ACT-ViT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guy Bar-Shalom, Fabrizio Frasca, Yaniv Galron, Yftah Ziser, Haggai Maron

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: Published in NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/BarSGuy/ACT-ViT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºACT-ViTï¼Œåˆ©ç”¨æ¿€æ´»å¼ é‡æ£€æµ‹å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¹»è§‰æ£€æµ‹` `å¤§è¯­è¨€æ¨¡å‹` `æ¿€æ´»å¼ é‡` `Vision Transformer` `å¤šæ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¹»è§‰æ£€æµ‹æ–¹æ³•ä¾èµ–äºç‰¹å®šLLMçš„tokenæ¢æµ‹ï¼Œæ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡å—é™ã€‚
2. ACT-ViTå°†æ¿€æ´»å¼ é‡è§†ä¸ºå›¾åƒï¼Œåˆ©ç”¨Vision Transformerå­¦ä¹ è·¨LLMçš„é€šç”¨å¹»è§‰æ¨¡å¼ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒACT-ViTåœ¨å¤šLLMè®­ç»ƒä¸‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹¶å…·å¤‡è‰¯å¥½çš„é›¶æ ·æœ¬å’Œè¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ä¸­çš„å¹»è§‰å¯¹äºå…¶å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ã€‚è™½ç„¶æ¢æµ‹åˆ†ç±»å™¨æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ä»…åœ¨å­¤ç«‹çš„å±‚-tokenå¯¹ä¸Šè¿è¡Œï¼Œå¹¶ä¸”æ˜¯ç‰¹å®šäºLLMçš„ï¼Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§å¹¶é˜»ç¢äº†è·¨LLMåº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•æ¥è§£å†³è¿™äº›ç¼ºç‚¹ã€‚æˆ‘ä»¬æ„å»ºåœ¨æ¿€æ´»æ•°æ®åœ¨ä¸¤ä¸ªè½´ï¼ˆå±‚Ã—tokenï¼‰ä¸Šçš„è‡ªç„¶åºåˆ—ç»“æ„ä¹‹ä¸Šï¼Œå¹¶æå€¡å°†å®Œæ•´çš„æ¿€æ´»å¼ é‡è§†ä¸ºå›¾åƒã€‚æˆ‘ä»¬è®¾è®¡äº†ACT-ViTï¼Œä¸€ä¸ªå—Vision Transformerå¯å‘çš„æ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆä¸”é«˜æ•ˆåœ°åº”ç”¨äºæ¿€æ´»å¼ é‡ï¼Œå¹¶æ”¯æŒåŒæ—¶åœ¨æ¥è‡ªå¤šä¸ªLLMçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡åŒ…å«å„ç§LLMå’Œæ•°æ®é›†çš„å…¨é¢å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ACT-ViTå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„æ¢æµ‹æŠ€æœ¯ï¼ŒåŒæ—¶ä¿æŒäº†æé«˜çš„éƒ¨ç½²æ•ˆç‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬çš„æ¶æ„å—ç›Šäºå¤šLLMè®­ç»ƒï¼Œåœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å¾®è°ƒæœ‰æ•ˆåœ°è½¬ç§»åˆ°æ–°çš„LLMã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å¹»è§‰æ£€æµ‹æ–¹æ³•ï¼Œä¾‹å¦‚tokenæ¢æµ‹ï¼Œé€šå¸¸é’ˆå¯¹ç‰¹å®šæ¨¡å‹è®¾è®¡ï¼Œæ³›åŒ–èƒ½åŠ›å·®ï¼Œéœ€è¦ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬è®­ç»ƒã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸åªå…³æ³¨æ¿€æ´»çš„å±€éƒ¨ä¿¡æ¯ï¼Œå¿½ç•¥äº†å±‚ä¸tokenä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤§è¯­è¨€æ¨¡å‹çš„æ¿€æ´»å¼ é‡è§†ä¸ºå›¾åƒï¼Œåˆ©ç”¨Vision Transformer (ViT) å­¦ä¹ æ¿€æ´»å¼ é‡ä¸­çš„å…¨å±€å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæ£€æµ‹å¹»è§‰ã€‚è¿™ç§æ–¹æ³•å¯ä»¥è·¨ä¸åŒçš„LLMè¿›è¡Œæ³›åŒ–ï¼Œå¹¶ä¸”èƒ½å¤Ÿåˆ©ç”¨å¤šLLMæ•°æ®è¿›è¡Œè”åˆè®­ç»ƒï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šACT-ViTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1. ä»LLMä¸­æå–æ¿€æ´»å¼ é‡ï¼ˆlayers x tokens x hidden_sizeï¼‰ã€‚2. å°†æ¿€æ´»å¼ é‡reshapeæˆç±»ä¼¼äºå›¾åƒçš„æ ¼å¼ã€‚3. ä½¿ç”¨Vision Transformerå¯¹æ¿€æ´»å¼ é‡è¿›è¡Œç¼–ç ï¼Œæå–ç‰¹å¾ã€‚4. ä½¿ç”¨åˆ†ç±»å™¨åŸºäºæå–çš„ç‰¹å¾é¢„æµ‹æ˜¯å¦å­˜åœ¨å¹»è§‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šACT-ViTçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ¿€æ´»å¼ é‡è§†ä¸ºå›¾åƒï¼Œå¹¶åˆ©ç”¨Vision Transformerè¿›è¡Œå¤„ç†ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ¿€æ´»å¼ é‡ä¸­çš„å…¨å±€å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæé«˜å¹»è§‰æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒACT-ViTæ”¯æŒå¤šLLMè®­ç»ƒï¼Œå¯ä»¥åˆ©ç”¨æ¥è‡ªä¸åŒæ¨¡å‹çš„æ¿€æ´»æ•°æ®è¿›è¡Œè”åˆè®­ç»ƒï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šACT-ViTä½¿ç”¨äº†æ ‡å‡†çš„Vision Transformeræ¶æ„ï¼ŒåŒ…æ‹¬Patch Embeddingã€Transformer Encoderå’ŒMLP Headã€‚Patch Embeddingå°†æ¿€æ´»å¼ é‡åˆ†å‰²æˆå°çš„patchï¼Œç„¶åå°†æ¯ä¸ªpatchæ˜ å°„åˆ°é«˜ç»´å‘é‡ã€‚Transformer Encoderç”±å¤šä¸ªTransformer Blockç»„æˆï¼Œæ¯ä¸ªBlockåŒ…å«Multi-Head Self-Attentionå’ŒFeed Forward Networkã€‚MLP Headç”¨äºå°†Transformer Encoderçš„è¾“å‡ºæ˜ å°„åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚æŸå¤±å‡½æ•°ä½¿ç”¨äº†æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ACT-ViTåœ¨å¤šä¸ªLLMå’Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶æ€§èƒ½å§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„tokenæ¢æµ‹æ–¹æ³•ã€‚ACT-ViTåœ¨å¤šLLMè®­ç»ƒä¸‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒACT-ViTå¯ä»¥é€šè¿‡å¾®è°ƒæœ‰æ•ˆåœ°è¿ç§»åˆ°æ–°çš„LLMï¼Œé™ä½äº†éƒ¨ç½²æˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ACT-ViTå¯ç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå†…å®¹çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œä¾‹å¦‚è‡ªåŠ¨æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æ£€æµ‹å’Œå‡å°‘å¹»è§‰ï¼Œå¯ä»¥æå‡ç”¨æˆ·å¯¹LLMçš„ä¿¡ä»»åº¦ï¼Œå¹¶é™ä½é”™è¯¯ä¿¡æ¯ä¼ æ’­çš„é£é™©ã€‚è¯¥æŠ€æœ¯è¿˜æœ‰åŠ©äºå¼€å‘æ›´å®‰å…¨ã€æ›´å¯é çš„AIåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Detecting hallucinations in Large Language Model-generated text is crucial for their safe deployment. While probing classifiers show promise, they operate on isolated layer-token pairs and are LLM-specific, limiting their effectiveness and hindering cross-LLM applications. In this paper, we introduce a novel approach to address these shortcomings. We build on the natural sequential structure of activation data in both axes (layers $\times$ tokens) and advocate treating full activation tensors akin to images. We design ACT-ViT, a Vision Transformer-inspired model that can be effectively and efficiently applied to activation tensors and supports training on data from multiple LLMs simultaneously. Through comprehensive experiments encompassing diverse LLMs and datasets, we demonstrate that ACT-ViT consistently outperforms traditional probing techniques while remaining extremely efficient for deployment. In particular, we show that our architecture benefits substantially from multi-LLM training, achieves strong zero-shot performance on unseen datasets, and can be transferred effectively to new LLMs through fine-tuning. Full code is available at https://github.com/BarSGuy/ACT-ViT.

