---
layout: default
title: How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models
---

# How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15115" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15115v1</a>
  <a href="https://arxiv.org/pdf/2512.15115.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15115v1" onclick="toggleFavorite(this, '2512.15115v1', 'How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ali Ghodsi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æ¡†æ¶ï¼Œåˆ†æAttentionå’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹(SSM)çš„è¡¨è¾¾èƒ½åŠ›ä¸è®­ç»ƒæƒè¡¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åºåˆ—å»ºæ¨¡` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `ç»Ÿä¸€æ¡†æ¶` `è¡¨è¾¾èƒ½åŠ›` `æ¢¯åº¦ä¼ æ’­` `ç†è®ºåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åºåˆ—æ¨¡å‹æ¶æ„å¤šæ ·ï¼Œç¼ºä¹ç»Ÿä¸€çš„ç†è®ºç†è§£æ¥æƒè¡¡è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒéš¾åº¦ã€‚
2. æå‡ºç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡è¾“å…¥ç›¸å…³çš„äº¤äº’ç®—å­æ˜¾å¼åœ°è¡¨ç¤ºAttentionå’ŒSSMçš„å…±æ€§ã€‚
3. ç†è®ºåˆ†ææ­ç¤ºäº†Attentionå’ŒSSMåœ¨è¡¨è¾¾èƒ½åŠ›å’Œæ¢¯åº¦ä¼ æ’­ä¸Šçš„æ ¹æœ¬æƒè¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åºåˆ—å»ºæ¨¡äº§ç”Ÿäº†å¤šç§æ¶æ„ï¼Œä»ç»å…¸å¾ªç¯ç¥ç»ç½‘ç»œåˆ°ç°ä»£Transformerå’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹(SSM)ï¼Œä½†å¯¹äºè¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒæƒè¡¡çš„ç»Ÿä¸€ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡è¾“å…¥ç›¸å…³çš„æœ‰æ•ˆäº¤äº’ç®—å­$W_{ij}(X)$æ¥è¡¨ç¤ºå¹¿æ³›çš„åºåˆ—æ˜ å°„ç±»åˆ«ï¼Œæ˜ç¡®äº†ä¸¤ç§é‡å¤å‡ºç°çš„æ„é€ æ¨¡å¼ï¼š(i) ç»Ÿä¸€åˆ†è§£æ¡†æ¶ï¼ˆæ˜¾å¼ï¼‰ï¼ˆæ³¨æ„åŠ›å¼æ··åˆï¼‰ï¼Œå…¶ä¸­$W_{ij}(X)$é€šè¿‡åº”ç”¨äºå…±äº«å€¼æ˜ å°„çš„æ ‡é‡ç³»æ•°è€Œå˜åŒ–ï¼Œä»¥åŠ(ii) ç»“æ„åŒ–åŠ¨æ€ï¼ˆéšå¼ï¼‰ï¼ˆçŠ¶æ€ç©ºé—´é€’å½’ï¼‰ï¼Œå…¶ä¸­$W_{ij}$ç”±æ½œåœ¨çš„åŠ¨æ€ç³»ç»Ÿå¼•èµ·ã€‚ä½¿ç”¨è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸‰ä¸ªç†è®ºç»“æœã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å»ºç«‹äº†äº¤äº’ç§©é—´éš™ï¼šç»Ÿä¸€åˆ†è§£æ¡†æ¶ä¸­çš„æ¨¡å‹ï¼Œä¾‹å¦‚å•å¤´æ³¨æ„åŠ›ï¼Œè¢«é™åˆ¶åœ¨ä½ç»´ç®—å­è·¨åº¦å†…ï¼Œå¹¶ä¸”ä¸èƒ½è¡¨ç¤ºæŸäº›ç»“æ„åŒ–åŠ¨æ€æ˜ å°„ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªç­‰ä»·ï¼ˆå¤´æ•°ï¼‰å®šç†ï¼Œè¡¨æ˜åœ¨æˆ‘ä»¬çš„å¤šå¤´åˆ†è§£ç±»ä¸­ï¼Œè¡¨ç¤ºä¸€ä¸ªçº¿æ€§SSMï¼Œå…¶æ»åç®—å­è·¨è¶Šé•¿åº¦ä¸ºnçš„åºåˆ—ä¸Šçš„kç»´å­ç©ºé—´éœ€è¦ä¸”å¯ä»¥ç”¨H=kä¸ªå¤´æ¥å®ç°ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªæ¢¯åº¦é«˜é€Ÿå…¬è·¯ç»“æœï¼Œè¡¨æ˜æ³¨æ„åŠ›å±‚å…è®¸å…·æœ‰è·ç¦»æ— å…³æ¢¯åº¦è·¯å¾„çš„è¾“å…¥ï¼Œè€Œç¨³å®šçš„çº¿æ€§åŠ¨æ€è¡¨ç°å‡ºè·ç¦»ç›¸å…³çš„æ¢¯åº¦è¡°å‡ã€‚æ€»ä¹‹ï¼Œè¿™äº›ç»“æœå½¢å¼åŒ–äº†ä»£æ•°è¡¨è¾¾èƒ½åŠ›ï¼ˆäº¤äº’/ç®—å­è·¨åº¦ï¼‰å’Œé•¿ç¨‹æ¢¯åº¦ä¼ æ’­ä¹‹é—´çš„åŸºæœ¬æƒè¡¡ï¼Œä¸ºç°ä»£åºåˆ—æ¶æ„è®¾è®¡æä¾›äº†ç†è®ºåŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åºåˆ—å»ºæ¨¡é¢†åŸŸä¸­ï¼Œä¸åŒæ¶æ„ï¼ˆå¦‚RNNã€Transformerã€SSMï¼‰ä¹‹é—´ç¼ºä¹ç»Ÿä¸€ç†è®ºæ¡†æ¶çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥è§£é‡Šè¿™äº›æ¶æ„åœ¨è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡ä¸Šçš„å·®å¼‚ï¼Œé˜»ç¢äº†æ–°å‹åºåˆ—æ¨¡å‹çš„æœ‰æ•ˆè®¾è®¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼Œå°†ä¸åŒçš„åºåˆ—æ¨¡å‹æ¶æ„è¡¨ç¤ºä¸ºè¾“å…¥ç›¸å…³çš„æœ‰æ•ˆäº¤äº’ç®—å­ã€‚é€šè¿‡åˆ†æè¿™äº›ç®—å­çš„æ€§è´¨ï¼Œå¯ä»¥æ­ç¤ºä¸åŒæ¶æ„åœ¨è¡¨è¾¾èƒ½åŠ›å’Œæ¢¯åº¦ä¼ æ’­ä¸Šçš„å†…åœ¨è”ç³»ä¸æƒè¡¡ã€‚è¯¥æ¡†æ¶å°†Attentionæœºåˆ¶å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹è§†ä¸ºä¸¤ç§ä¸åŒçš„äº¤äº’æ¨¡å¼ï¼šæ˜¾å¼çš„åˆ†è§£äº¤äº’ï¼ˆAttentionï¼‰å’Œéšå¼çš„ç»“æ„åŒ–åŠ¨æ€äº¤äº’ï¼ˆSSMï¼‰ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä½¿ç”¨ä¸€ä¸ªè¾“å…¥ç›¸å…³çš„æœ‰æ•ˆäº¤äº’ç®—å­ $W_{ij}(X)$ æ¥è¡¨ç¤ºåºåˆ—æ˜ å°„ã€‚è¯¥ç®—å­æè¿°äº†åºåˆ—ä¸­ç¬¬ i ä¸ªä½ç½®å’Œç¬¬ j ä¸ªä½ç½®ä¹‹é—´çš„äº¤äº’å¼ºåº¦ã€‚è®ºæ–‡æå‡ºäº†ä¸¤ç§ä¸»è¦çš„æ„é€ æ¨¡å¼ï¼š

1. **ç»Ÿä¸€åˆ†è§£æ¡†æ¶ï¼ˆæ˜¾å¼ï¼‰**ï¼šç±»ä¼¼äºAttentionæœºåˆ¶ï¼Œé€šè¿‡æ ‡é‡ç³»æ•°å¯¹å…±äº«çš„å€¼æ˜ å°„è¿›è¡ŒåŠ æƒæ··åˆã€‚

2. **ç»“æ„åŒ–åŠ¨æ€ï¼ˆéšå¼ï¼‰**ï¼šç±»ä¼¼äºSSMï¼Œé€šè¿‡æ½œåœ¨çš„åŠ¨æ€ç³»ç»Ÿæ¥è¯±å¯¼äº¤äº’ç®—å­ã€‚

åŸºäºæ­¤æ¡†æ¶ï¼Œè®ºæ–‡æ¨å¯¼å‡ºäº†ä¸‰ä¸ªå…³é”®çš„ç†è®ºç»“æœï¼šäº¤äº’ç§©é—´éš™ã€å¤´æ•°ç­‰ä»·å®šç†å’Œæ¢¯åº¦é«˜é€Ÿå…¬è·¯ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶è¡¨ç¤ºAttentionæœºåˆ¶å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œç†è®ºåˆ†æã€‚ä¸ä»¥å¾€é’ˆå¯¹ç‰¹å®šæ¶æ„çš„åˆ†æä¸åŒï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ­ç¤ºä¸åŒæ¶æ„ä¹‹é—´çš„å…±æ€§å’Œå·®å¼‚ï¼Œä¸ºåºåˆ—æ¨¡å‹çš„è®¾è®¡æä¾›äº†æ›´é€šç”¨çš„æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š

1.  **äº¤äº’ç®—å­ $W_{ij}(X)$ çš„å®šä¹‰**ï¼šè¯¥ç®—å­æ˜¯è¿æ¥ä¸åŒæ¶æ„çš„å…³é”®ï¼Œå…¶å…·ä½“å½¢å¼å–å†³äºæ‰€è¡¨ç¤ºçš„æ¶æ„ã€‚

2.  **ä¸¤ç§æ„é€ æ¨¡å¼çš„åŒºåˆ†**ï¼šæ˜¾å¼åˆ†è§£äº¤äº’å’Œéšå¼ç»“æ„åŒ–åŠ¨æ€äº¤äº’ä»£è¡¨äº†ä¸¤ç§ä¸åŒçš„å»ºæ¨¡æ€è·¯ã€‚

3.  **ç†è®ºç»“æœçš„æ¨å¯¼**ï¼šé€šè¿‡æ•°å­¦æ¨å¯¼ï¼Œè®ºæ–‡å»ºç«‹äº†äº¤äº’ç§©ã€å¤´æ•°å’Œæ¢¯åº¦ä¼ æ’­ç­‰å…³é”®æ€§è´¨ä¹‹é—´çš„è”ç³»ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15115v1/experiment_results_full.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15115v1/experiment_gradient_flow.png" alt="fig_1" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡è¯æ˜äº†å•å¤´Attentionæ¨¡å‹å­˜åœ¨äº¤äº’ç§©é—´éš™ï¼Œæ— æ³•è¡¨ç¤ºæŸäº›ç»“æ„åŒ–åŠ¨æ€æ˜ å°„ã€‚å¤´æ•°ç­‰ä»·å®šç†è¡¨æ˜ï¼Œè¡¨ç¤ºkç»´çº¿æ€§SSMéœ€è¦ä¸”ä»…éœ€è¦kä¸ªå¤´ã€‚æ¢¯åº¦é«˜é€Ÿå…¬è·¯ç»“æœæ­ç¤ºäº†Attentionå’ŒSSMåœ¨æ¢¯åº¦ä¼ æ’­ä¸Šçš„å·®å¼‚ï¼ŒAttentionå…è®¸è·ç¦»æ— å…³çš„æ¢¯åº¦è·¯å¾„ï¼Œè€ŒSSMåˆ™è¡¨ç°å‡ºè·ç¦»ç›¸å…³çš„æ¢¯åº¦è¡°å‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºåºåˆ—å»ºæ¨¡çš„å„ä¸ªé¢†åŸŸï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ã€æ—¶é—´åºåˆ—åˆ†æç­‰ã€‚é€šè¿‡ç†è§£ä¸åŒæ¶æ„çš„è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒæƒè¡¡ï¼Œå¯ä»¥æŒ‡å¯¼æ–°å‹åºåˆ—æ¨¡å‹çš„æœ‰æ•ˆè®¾è®¡ï¼Œæå‡æ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥ç”¨äºåˆ†æç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è¿›è¡Œæ”¹è¿›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.

