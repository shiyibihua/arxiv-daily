---
layout: default
title: Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning
---

# Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15687" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15687v1</a>
  <a href="https://arxiv.org/pdf/2512.15687.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15687v1" onclick="toggleFavorite(this, '2512.15687v1', 'Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenwen Liang, Sidi Lu, Wenhao Yu, Kishan Panaganti, Yujun Zhou, Haitao Mi, Dong Yu

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºG2RLï¼šåˆ©ç”¨æ¢¯åº¦å¼•å¯¼å¼ºåŒ–å­¦ä¹ æå‡LLMæ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¢¯åº¦å¼•å¯¼` `æ¢ç´¢ç­–ç•¥` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ¢ç´¢LLMæ¨ç†èƒ½åŠ›æ—¶ï¼Œä¾èµ–ç†µå¥–åŠ±ç­‰å¤–éƒ¨å¯å‘å¼æ–¹æ³•ï¼Œä¸æ¨¡å‹å®é™…å­¦ä¹ æ–¹å¼è„±èŠ‚ã€‚
2. G2RLé€šè¿‡æ¨¡å‹è‡ªèº«æ¢¯åº¦ä¿¡æ¯å¼•å¯¼æ¢ç´¢ï¼Œå¥–åŠ±å¸¦æ¥æ–°æ¢¯åº¦æ–¹å‘çš„è½¨è¿¹ï¼ŒæŠ‘åˆ¶å†—ä½™æ›´æ–°ï¼Œå®ç°è‡ªå¼•ç”¨æ¢ç´¢ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒG2RLåœ¨æ•°å­¦å’Œé€šç”¨æ¨ç†ä»»åŠ¡ä¸Šï¼Œæ˜¾è‘—æå‡äº†Qwen3æ¨¡å‹çš„pass@1ç­‰æŒ‡æ ‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ å¯¹äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„æ¢ç´¢æœºåˆ¶ä¸æ¨¡å‹çš„å®é™…å­¦ä¹ æ–¹å¼å­˜åœ¨æ ¹æœ¬ä¸Šçš„ä¸ä¸€è‡´ã€‚ç†µå¥–åŠ±å’Œå¤–éƒ¨è¯­ä¹‰æ¯”è¾ƒå™¨é¼“åŠ±è¡¨é¢å±‚æ¬¡çš„å˜åŒ–ï¼Œä½†ä¸èƒ½ä¿è¯æŠ½æ ·çš„è½¨è¿¹åœ¨å¡‘é€ ä¼˜åŒ–çš„æ›´æ–°æ–¹å‘ä¸Šæœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬æå‡ºäº†G2RLï¼Œä¸€ä¸ªæ¢¯åº¦å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…¶ä¸­æ¢ç´¢ä¸æ˜¯ç”±å¤–éƒ¨å¯å‘å¼æ–¹æ³•é©±åŠ¨ï¼Œè€Œæ˜¯ç”±æ¨¡å‹è‡ªèº«çš„ä¸€é˜¶æ›´æ–°å‡ ä½•é©±åŠ¨ã€‚å¯¹äºæ¯ä¸ªå“åº”ï¼ŒG2RLä»æ¨¡å‹æœ€åä¸€å±‚çš„æ•æ„Ÿæ€§æ„å»ºä¸€ä¸ªåºåˆ—çº§åˆ«çš„ç‰¹å¾ï¼ˆå¯ä»¥é€šè¿‡æ ‡å‡†å‰å‘ä¼ é€’ä»¥å¿½ç•¥ä¸è®¡çš„æˆæœ¬è·å¾—ï¼‰ï¼Œå¹¶é€šè¿‡æ¯”è¾ƒæŠ½æ ·ç»„å†…çš„è¿™äº›ç‰¹å¾æ¥è¡¡é‡æ¯ä¸ªè½¨è¿¹å°†å¦‚ä½•é‡å¡‘ç­–ç•¥ã€‚å¼•å…¥æ–°æ¢¯åº¦æ–¹å‘çš„è½¨è¿¹ä¼šæ”¶åˆ°æœ‰ç•Œçš„ä¹˜æ³•å¥–åŠ±ç¼©æ”¾ï¼Œè€Œå†—ä½™æˆ–åç¦»æµå½¢çš„æ›´æ–°åˆ™è¢«å¼±åŒ–ï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªè‡ªå¼•ç”¨çš„æ¢ç´¢ä¿¡å·ï¼Œè¯¥ä¿¡å·è‡ªç„¶åœ°ä¸PPOé£æ ¼çš„ç¨³å®šæ€§å’ŒKLæ§åˆ¶å¯¹é½ã€‚åœ¨Qwen3 base 1.7Bå’Œ4Bæ¨¡å‹ä¸Šï¼Œé’ˆå¯¹æ•°å­¦å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆMATH500ã€AMCã€AIME24ã€AIME25ã€GPQAã€MMLUproï¼‰ï¼ŒG2RLå§‹ç»ˆä¼˜äºåŸºäºç†µçš„GRPOå’Œå¤–éƒ¨åµŒå…¥æ–¹æ³•ï¼Œåœ¨pass@1ã€maj@16å’Œpass@kæŒ‡æ ‡ä¸Šå‡æœ‰æå‡ã€‚é€šè¿‡åˆ†æè¯±å¯¼å‡ ä½•ï¼Œæˆ‘ä»¬å‘ç°G2RLå°†æ¢ç´¢æ‰©å±•åˆ°æ›´å¤šæ­£äº¤ä¸”é€šå¸¸ç›¸åçš„æ¢¯åº¦æ–¹å‘ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰è¿è´¯æ€§ï¼Œè¿™è¡¨æ˜ç­–ç•¥è‡ªèº«çš„æ›´æ–°ç©ºé—´ä¸ºæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢æä¾›äº†ä¸€ä¸ªæ›´å¿ å®å’Œæœ‰æ•ˆçš„ä¾æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒLLMè¿›è¡Œæ¨ç†æ—¶ï¼Œå…¶æ¢ç´¢ç­–ç•¥ï¼ˆå¦‚ç†µå¥–åŠ±ï¼‰ä¸LLMçš„å®é™…å­¦ä¹ è¿‡ç¨‹ä¸åŒ¹é…ã€‚è¿™äº›æ–¹æ³•é¼“åŠ±è¡¨é¢çš„å¤šæ ·æ€§ï¼Œä½†æ— æ³•ä¿è¯æ¢ç´¢çš„è½¨è¿¹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ”¹å˜æ¨¡å‹çš„ä¼˜åŒ–æ–¹å‘ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§ä¸LLMå­¦ä¹ æ–¹å¼æ›´å¥‘åˆçš„æ¢ç´¢ç­–ç•¥ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šG2RLçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMè‡ªèº«çš„æ¢¯åº¦ä¿¡æ¯æ¥å¼•å¯¼æ¢ç´¢ã€‚å…·ä½“æ¥è¯´ï¼ŒG2RLé€šè¿‡åˆ†ææ¨¡å‹æœ€åä¸€å±‚çš„æ•æ„Ÿæ€§ï¼Œæå–åºåˆ—çº§åˆ«çš„ç‰¹å¾ï¼Œå¹¶åŸºäºè¿™äº›ç‰¹å¾æ¥è¯„ä¼°æ¯ä¸ªè½¨è¿¹å¯¹ç­–ç•¥æ›´æ–°çš„å½±å“ã€‚å¥–åŠ±é‚£äº›èƒ½å¤Ÿå¼•å…¥æ–°çš„æ¢¯åº¦æ–¹å‘çš„è½¨è¿¹ï¼ŒåŒæ—¶æŠ‘åˆ¶é‚£äº›å†—ä½™æˆ–åç¦»æµå½¢çš„æ›´æ–°ã€‚è¿™ç§è‡ªå¼•ç”¨çš„æ¢ç´¢ä¿¡å·èƒ½å¤Ÿæ›´å¥½åœ°ä¸PPOç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç¨³å®šæ€§å’ŒKLæ•£åº¦æ§åˆ¶ç›¸é…åˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šG2RLçš„æ•´ä½“æ¡†æ¶å¦‚ä¸‹ï¼š1) å¯¹äºæ¯ä¸ªLLMçš„å“åº”ï¼Œè®¡ç®—å…¶æœ€åä¸€å±‚çš„æ•æ„Ÿæ€§ï¼Œå¾—åˆ°åºåˆ—çº§åˆ«çš„ç‰¹å¾è¡¨ç¤ºã€‚2) åœ¨ä¸€ä¸ªæŠ½æ ·ç»„å†…ï¼Œæ¯”è¾ƒä¸åŒè½¨è¿¹çš„ç‰¹å¾è¡¨ç¤ºï¼Œè¯„ä¼°å®ƒä»¬å¯¹ç­–ç•¥æ›´æ–°çš„å½±å“ã€‚3) æ ¹æ®è½¨è¿¹å¼•å…¥çš„æ¢¯åº¦æ–¹å‘çš„æ–°é¢–æ€§ï¼Œç»™äºˆç›¸åº”çš„å¥–åŠ±ç¼©æ”¾ã€‚å¼•å…¥æ–°æ¢¯åº¦æ–¹å‘çš„è½¨è¿¹è·å¾—æ­£å‘å¥–åŠ±ï¼Œè€Œå†—ä½™æˆ–åç¦»æµå½¢çš„è½¨è¿¹åˆ™å—åˆ°æƒ©ç½šã€‚4) ä½¿ç”¨PPOç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ŒåŸºäºè°ƒæ•´åçš„å¥–åŠ±ä¿¡å·æ¥æ›´æ–°LLMçš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šG2RLæœ€å…³é”®çš„åˆ›æ–°åœ¨äºå…¶æ¢ç´¢ç­–ç•¥æ˜¯åŸºäºæ¨¡å‹è‡ªèº«çš„æ¢¯åº¦ä¿¡æ¯ï¼Œè€Œä¸æ˜¯å¤–éƒ¨çš„å¯å‘å¼æ–¹æ³•ã€‚è¿™ç§è‡ªå¼•ç”¨çš„æ¢ç´¢æ–¹å¼èƒ½å¤Ÿæ›´å¥½åœ°ä¸LLMçš„å­¦ä¹ è¿‡ç¨‹ç›¸åŒ¹é…ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒG2RLèƒ½å¤Ÿæ¢ç´¢åˆ°æ›´å¤šæ­£äº¤ä¸”é€šå¸¸ç›¸åçš„æ¢¯åº¦æ–¹å‘ï¼Œä»è€Œæ›´å…¨é¢åœ°è¦†ç›–æ¨¡å‹çš„è§£ç©ºé—´ã€‚

**å…³é”®è®¾è®¡**ï¼šG2RLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ¨¡å‹æœ€åä¸€å±‚çš„æ•æ„Ÿæ€§ä½œä¸ºåºåˆ—çº§åˆ«çš„ç‰¹å¾è¡¨ç¤ºï¼Œè¿™å¯ä»¥é€šè¿‡æ ‡å‡†çš„å‰å‘ä¼ æ’­ä»¥å¾ˆä½çš„æˆæœ¬è·å¾—ã€‚2) ä½¿ç”¨æœ‰ç•Œçš„ä¹˜æ³•å¥–åŠ±ç¼©æ”¾æ¥è°ƒæ•´è½¨è¿¹çš„å¥–åŠ±ï¼Œä»¥ä¿è¯è®­ç»ƒçš„ç¨³å®šæ€§ã€‚3) å°†G2RLä¸PPOç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸ç»“åˆï¼Œåˆ©ç”¨PPOçš„ç¨³å®šæ€§å’ŒKLæ•£åº¦æ§åˆ¶æ¥çº¦æŸç­–ç•¥çš„æ›´æ–°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15687v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15687v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15687v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒG2RLåœ¨MATH500ã€AMCã€AIME24ã€AIME25ã€GPQAã€MMLUproç­‰å¤šä¸ªæ•°å­¦å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼Œæ˜¾è‘—ä¼˜äºåŸºäºç†µçš„GRPOå’Œå¤–éƒ¨åµŒå…¥æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨Qwen3 base 1.7Bå’Œ4Bæ¨¡å‹ä¸Šï¼ŒG2RLåœ¨pass@1ã€maj@16å’Œpass@kç­‰æŒ‡æ ‡ä¸Šå‡æœ‰æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

G2RLå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦LLMè¿›è¡Œå¤æ‚æ¨ç†çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€çŸ¥è¯†é—®ç­”ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡LLMçš„æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ å¯é å’Œæœ‰æ•ˆã€‚æ­¤å¤–ï¼ŒG2RLçš„è‡ªå¼•ç”¨æ¢ç´¢æ€æƒ³ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æ¨¡å‹å’Œä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.

