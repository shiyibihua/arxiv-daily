---
layout: default
title: A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point
---

# A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15606" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15606v1</a>
  <a href="https://arxiv.org/pdf/2512.15606.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15606v1" onclick="toggleFavorite(this, '2512.15606v1', 'A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Carlos Couto, JosÃ© MourÃ£o, MÃ¡rio A. T. Figueiredo, Pedro Ribeiro

**åˆ†ç±»**: stat.ML, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

**å¤‡æ³¨**: 25 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶ç¥ç»ç½‘ç»œä¼˜åŒ–ç‚¹é™„è¿‘çš„å­¦ä¹ åŠ¨æ€ï¼Œæ­ç¤ºHessiançŸ©é˜µç‰¹å¾è°±çš„å…³é”®ä½œç”¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¥ç»ç½‘ç»œ` `HessiançŸ©é˜µ` `ç‰¹å¾è°±` `æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹` `æ¢¯åº¦ä¸‹é™` `ä¼˜åŒ–ç®—æ³•` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¥ç»ç½‘ç»œä¼˜åŒ–ç‚¹é™„è¿‘çš„å­¦ä¹ åŠ¨æ€å—HessiançŸ©é˜µå½±å“ï¼Œç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹Hessianç‰¹å¾è°±çš„æ·±å…¥ç†è§£ã€‚
2. æœ¬æ–‡é€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹ï¼Œåˆ†æHessiançŸ©é˜µç‰¹å¾è°±ï¼Œæ­ç¤ºå°ç‰¹å¾å€¼å¯¹é•¿æ—¶é—´å­¦ä¹ æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚
3. å®éªŒåˆ†æäº†çº¿æ€§ã€å¤šé¡¹å¼ç­‰ç½‘ç»œçš„Hessianè°±ï¼Œå¹¶éªŒè¯äº†Hessianç§©ä¸æœ‰æ•ˆå‚æ•°æ•°é‡çš„å…³ç³»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»æ•™å¸ˆ-å­¦ç”Ÿç½‘ç»œçš„è§’åº¦ç ”ç©¶äº†æ¢¯åº¦ä¸‹é™åœ¨ç¥ç»ç½‘ç»œæœ€ä¼˜å­¦ä¹ ç‚¹é™„è¿‘çš„å­¦ä¹ åŠ¨æ€ï¼ŒæŒ‡å‡ºæŸå¤±å‡½æ•°å…³äºç½‘ç»œå‚æ•°çš„HessiançŸ©é˜µå†³å®šäº†å­¦ä¹ æ€§èƒ½ã€‚é’ˆå¯¹æ•™å¸ˆå’Œå­¦ç”Ÿç½‘ç»œå…·æœ‰åŒ¹é…æƒé‡çš„ç‰¹å®šé—®é¢˜ï¼Œæœ¬æ–‡åˆ»ç”»äº†HessiançŸ©é˜µçš„ç‰¹å¾è°±ï¼Œè¡¨æ˜è¾ƒå°çš„ç‰¹å¾å€¼å†³å®šäº†é•¿æ—¶é—´çš„å­¦ä¹ æ€§èƒ½ã€‚å¯¹äºçº¿æ€§ç½‘ç»œï¼Œæœ¬æ–‡ä»ç†è®ºä¸Šè¯æ˜äº†å¯¹äºå¤§å‹ç½‘ç»œï¼Œè¯¥è°±æ¸è¿‘åœ°éµå¾ªç¼©æ”¾çš„å¡æ–¹åˆ†å¸ƒä¸ç¼©æ”¾çš„é©¬ç´ç§‘-å¸•æ–¯å›¾åˆ†å¸ƒçš„å·ç§¯ã€‚æœ¬æ–‡è¿˜æ•°å€¼åˆ†æäº†å¤šé¡¹å¼å’Œå…¶ä»–éçº¿æ€§ç½‘ç»œçš„Hessianè°±ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¡¨æ˜HessiançŸ©é˜µçš„ç§©å¯ä»¥è¢«è§†ä¸ºä½¿ç”¨å¤šé¡¹å¼æ¿€æ´»å‡½æ•°çš„ç½‘ç»œçš„æœ‰æ•ˆå‚æ•°æ•°é‡ã€‚å¯¹äºè¯¸å¦‚è¯¯å·®å‡½æ•°ä¹‹ç±»çš„é€šç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒè§‚å¯Ÿåˆ°HessiançŸ©é˜µå§‹ç»ˆæ˜¯æ»¡ç§©çš„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç†è§£ç¥ç»ç½‘ç»œåœ¨æ¥è¿‘æœ€ä¼˜è§£æ—¶çš„å­¦ä¹ åŠ¨æ€ã€‚ç°æœ‰çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†ç¼ºä¹å¯¹ä¼˜åŒ–è¿‡ç¨‹æœ¬è´¨çš„ç†è§£ï¼Œå°¤å…¶æ˜¯åœ¨æœ€ä¼˜è§£é™„è¿‘ï¼ŒæŸå¤±å‡½æ•°çš„HessiançŸ©é˜µå¦‚ä½•å½±å“å­¦ä¹ è¿‡ç¨‹ã€‚ç†è§£HessiançŸ©é˜µçš„ç‰¹å¾è°±ï¼Œæœ‰åŠ©äºæ›´å¥½åœ°è®¾è®¡ä¼˜åŒ–ç®—æ³•å’Œç½‘ç»œç»“æ„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä»æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹çš„è§’åº¦å‡ºå‘ï¼Œç ”ç©¶å­¦ç”Ÿç½‘ç»œåœ¨å­¦ä¹ æ•™å¸ˆç½‘ç»œçš„è¿‡ç¨‹ä¸­ï¼ŒæŸå¤±å‡½æ•°HessiançŸ©é˜µçš„ç‰¹å¾è°±å˜åŒ–ã€‚é€šè¿‡åˆ†æç‰¹å¾è°±ï¼Œå¯ä»¥äº†è§£å“ªäº›å‚æ•°å¯¹å­¦ä¹ è¿‡ç¨‹æ›´é‡è¦ï¼Œä»¥åŠå­¦ä¹ è¿‡ç¨‹çš„æ”¶æ•›é€Ÿåº¦ã€‚ç‰¹åˆ«å…³æ³¨HessiançŸ©é˜µçš„è¾ƒå°ç‰¹å¾å€¼ï¼Œå› ä¸ºå®ƒä»¬å†³å®šäº†é•¿æ—¶é—´çš„å­¦ä¹ æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) ç†è®ºåˆ†æï¼šé’ˆå¯¹çº¿æ€§ç½‘ç»œï¼Œæ¨å¯¼HessiançŸ©é˜µç‰¹å¾è°±çš„æ¸è¿‘åˆ†å¸ƒï¼›2) æ•°å€¼å®éªŒï¼šé’ˆå¯¹å¤šé¡¹å¼å’Œå…¶ä»–éçº¿æ€§ç½‘ç»œï¼Œè®¡ç®—HessiançŸ©é˜µçš„ç‰¹å¾è°±ï¼›3) å®éªŒéªŒè¯ï¼šé€šè¿‡å®éªŒéªŒè¯HessiançŸ©é˜µçš„ç§©ä¸ç½‘ç»œæœ‰æ•ˆå‚æ•°æ•°é‡çš„å…³ç³»ã€‚æ•´ä½“æµç¨‹æ˜¯ä»ç†è®ºåˆ†æåˆ°å®éªŒéªŒè¯ï¼Œé€æ­¥æ·±å…¥ç†è§£HessiançŸ©é˜µåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†HessiançŸ©é˜µçš„ç‰¹å¾è°±ä¸ç¥ç»ç½‘ç»œçš„å­¦ä¹ åŠ¨æ€è”ç³»èµ·æ¥ã€‚é€šè¿‡åˆ†æç‰¹å¾è°±ï¼Œå¯ä»¥äº†è§£å“ªäº›å‚æ•°å¯¹å­¦ä¹ è¿‡ç¨‹æ›´é‡è¦ï¼Œä»¥åŠå­¦ä¹ è¿‡ç¨‹çš„æ”¶æ•›é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†HessiançŸ©é˜µçš„ç§©å¯ä»¥ä½œä¸ºç½‘ç»œæœ‰æ•ˆå‚æ•°æ•°é‡çš„åº¦é‡ï¼Œè¿™ä¸ºç†è§£ç¥ç»ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•™å¸ˆ-å­¦ç”Ÿç½‘ç»œçš„ç»“æ„ï¼šæ•™å¸ˆå’Œå­¦ç”Ÿç½‘ç»œå…·æœ‰åŒ¹é…çš„æƒé‡ï¼Œä¾¿äºåˆ†æï¼›2) æŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼šé€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä½¿å¾—HessiançŸ©é˜µçš„è®¡ç®—å’Œåˆ†ææˆä¸ºå¯èƒ½ï¼›3) ç‰¹å¾è°±çš„è®¡ç®—æ–¹æ³•ï¼šé‡‡ç”¨æ•°å€¼æ–¹æ³•è®¡ç®—HessiançŸ©é˜µçš„ç‰¹å¾è°±ï¼›4) å®éªŒå‚æ•°çš„è®¾ç½®ï¼šåˆç†è®¾ç½®å®éªŒå‚æ•°ï¼Œä¿è¯å®éªŒç»“æœçš„å¯é æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15606v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15606v1/img/linear_hessian_10_20.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15606v1/img/linear_hessian_30_10.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æå’Œæ•°å€¼å®éªŒï¼Œæ­ç¤ºäº†HessiançŸ©é˜µç‰¹å¾è°±ä¸å­¦ä¹ åŠ¨æ€ä¹‹é—´çš„å…³ç³»ã€‚å¯¹äºçº¿æ€§ç½‘ç»œï¼Œç†è®ºè¯æ˜äº†ç‰¹å¾è°±çš„æ¸è¿‘åˆ†å¸ƒã€‚å¯¹äºå¤šé¡¹å¼ç½‘ç»œï¼Œæ•°å€¼å®éªŒéªŒè¯äº†HessiançŸ©é˜µçš„ç§©ä¸ç½‘ç»œæœ‰æ•ˆå‚æ•°æ•°é‡çš„å…³ç³»ã€‚å¯¹äºé€šç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå®éªŒè§‚å¯Ÿåˆ°HessiançŸ©é˜µå§‹ç»ˆæ˜¯æ»¡ç§©çš„ã€‚è¿™äº›ç»“æœä¸ºç†è§£ç¥ç»ç½‘ç»œçš„å­¦ä¹ è¿‡ç¨‹æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç¥ç»ç½‘ç»œä¼˜åŒ–ç®—æ³•çš„è®¾è®¡ä¸æ”¹è¿›ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡åˆ†æHessiançŸ©é˜µçš„ç‰¹å¾è°±ï¼Œå¯ä»¥è®¾è®¡è‡ªé€‚åº”å­¦ä¹ ç‡çš„ä¼˜åŒ–ç®—æ³•ï¼ŒåŠ é€Ÿç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ç”¨äºç†è§£ç¥ç»ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„ç½‘ç»œç»“æ„æä¾›ç†è®ºæŒ‡å¯¼ã€‚è¯¥ç ”ç©¶å¯¹æ·±åº¦å­¦ä¹ ç†è®ºå’Œåº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Near an optimal learning point of a neural network, the learning performance of gradient descent dynamics is dictated by the Hessian matrix of the loss function with respect to the network parameters. We characterize the Hessian eigenspectrum for some classes of teacher-student problems, when the teacher and student networks have matching weights, showing that the smaller eigenvalues of the Hessian determine long-time learning performance. For linear networks, we analytically establish that for large networks the spectrum asymptotically follows a convolution of a scaled chi-square distribution with a scaled Marchenko-Pastur distribution. We numerically analyse the Hessian spectrum for polynomial and other non-linear networks. Furthermore, we show that the rank of the Hessian matrix can be seen as an effective number of parameters for networks using polynomial activation functions. For a generic non-linear activation function, such as the error function, we empirically observe that the Hessian matrix is always full rank.

