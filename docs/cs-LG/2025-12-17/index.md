---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.LG - 2025-12-17
---

# cs.LGï¼ˆ2025-12-17ï¼‰

ğŸ“Š å…± **21** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (12)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251215115v1-how-many-heads-make-an-ssm-a-unified-framework-for-attention-and-sta.html">How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models</a></td>
  <td>æå‡ºç»Ÿä¸€æ¡†æ¶ï¼Œåˆ†æAttentionå’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹(SSM)çš„è¡¨è¾¾èƒ½åŠ›ä¸è®­ç»ƒæƒè¡¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">latent dynamics</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15115v1" onclick="toggleFavorite(this, '2512.15115v1', 'How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251215687v1-can-llms-guide-their-own-exploration-gradient-guided-reinforcement-l.html">Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</a></td>
  <td>æå‡ºG2RLï¼šåˆ©ç”¨æ¢¯åº¦å¼•å¯¼å¼ºåŒ–å­¦ä¹ æå‡LLMæ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">PPO</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15687v1" onclick="toggleFavorite(this, '2512.15687v1', 'Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251215521v1-autonomous-pressure-control-in-muvacas-via-deep-reinforcement-learni.html">Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models</a></td>
  <td>æå‡ºåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä»£ç†æ¨¡å‹çš„MuVacASè‡ªä¸»å‹åŠ›æ§åˆ¶æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15521v1" onclick="toggleFavorite(this, '2512.15521v1', 'Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251215605v1-autoregressive-language-models-are-secretly-energy-based-models-insi.html">Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction</a></td>
  <td>æ­ç¤ºè‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸èƒ½é‡æ¨¡å‹ç­‰ä»·æ€§ï¼Œæ´å¯Ÿå…¶å‰ç»èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">distillation</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15605v1" onclick="toggleFavorite(this, '2512.15605v1', 'Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251215120v1-automatic-reward-shaping-from-multi-objective-human-heuristics.html">Automatic Reward Shaping from Multi-Objective Human Heuristics</a></td>
  <td>æå‡ºMORSEæ¡†æ¶ï¼Œé€šè¿‡å¤šç›®æ ‡äººç±»å¯å‘å¼è‡ªåŠ¨è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¥–åŠ±å¡‘é€ </td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">reward shaping</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15120v1" onclick="toggleFavorite(this, '2512.15120v1', 'Automatic Reward Shaping from Multi-Objective Human Heuristics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251215606v1-a-teacher-student-perspective-on-the-dynamics-of-learning-near-the-o.html">A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point</a></td>
  <td>ç ”ç©¶ç¥ç»ç½‘ç»œä¼˜åŒ–ç‚¹é™„è¿‘çš„å­¦ä¹ åŠ¨æ€ï¼Œæ­ç¤ºHessiançŸ©é˜µç‰¹å¾è°±çš„å…³é”®ä½œç”¨</td>
  <td class="tags-cell"><span class="paper-tag">teacher-student</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15606v1" onclick="toggleFavorite(this, '2512.15606v1', 'A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251215405v1-eubrl-epistemic-uncertainty-directed-bayesian-reinforcement-learning.html">EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning</a></td>
  <td>æå‡ºEUBRLç®—æ³•ï¼Œåˆ©ç”¨è®¤çŸ¥ä¸ç¡®å®šæ€§æŒ‡å¯¼è´å¶æ–¯å¼ºåŒ–å­¦ä¹ æ¢ç´¢ï¼Œæå‡æ ·æœ¬æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15405v1" onclick="toggleFavorite(this, '2512.15405v1', 'EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251215267v1-distillation-guided-structural-transfer-for-continual-learning-beyon.html">Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory</a></td>
  <td>æå‡ºé€‰æ‹©æ€§å­ç½‘ç»œè’¸é¦(SSD)æ¡†æ¶ï¼Œæå‡ç¨€ç–ç¥ç»ç½‘ç»œçš„æŒç»­å­¦ä¹ èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15267v1" onclick="toggleFavorite(this, '2512.15267v1', 'Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251215123v1-trajsyn-privacy-preserving-dataset-distillation-from-federated-model.html">TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training</a></td>
  <td>TrajSynï¼šè”é‚¦å­¦ä¹ ä¸­åŸºäºæ¨¡å‹è½¨è¿¹çš„éšç§ä¿æŠ¤æ•°æ®é›†è’¸é¦ï¼Œç”¨äºæœåŠ¡ç«¯å¯¹æŠ—è®­ç»ƒ</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15123v1" onclick="toggleFavorite(this, '2512.15123v1', 'TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251215112v1-feature-centric-unsupervised-node-representation-learning-without-ho.html">Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption</a></td>
  <td>FUELï¼šä¸€ç§æ— éœ€åŒè´¨æ€§å‡è®¾çš„ç‰¹å¾ä¸­å¿ƒæ— ç›‘ç£èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15112v1" onclick="toggleFavorite(this, '2512.15112v1', 'Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251215036v1-spectral-representation-based-reinforcement-learning.html">Spectral Representation-based Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºè°±è¡¨ç¤ºçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­çš„éš¾é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15036v1" onclick="toggleFavorite(this, '2512.15036v1', 'Spectral Representation-based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251215657v1-soflow-solution-flow-models-for-one-step-generative-modeling.html">SoFlow: Solution Flow Models for One-Step Generative Modeling</a></td>
  <td>SoFlowï¼šæå‡ºè§£å†³æ–¹æ¡ˆæµæ¨¡å‹ï¼Œå®ç°ä¸€æ­¥åˆ°ä½çš„ç”Ÿæˆå»ºæ¨¡ï¼Œæå‡ç”Ÿæˆæ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15657v1" onclick="toggleFavorite(this, '2512.15657v1', 'SoFlow: Solution Flow Models for One-Step Generative Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/251215442v1-copyright-infringement-risk-reduction-via-chain-of-thought-and-task-.html">Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting</a></td>
  <td>ç»“åˆæ€ç»´é“¾ä¸ä»»åŠ¡æŒ‡ä»¤æç¤ºï¼Œé™ä½æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„ç‰ˆæƒä¾µæƒé£é™©</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15442v1" onclick="toggleFavorite(this, '2512.15442v1', 'Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251215705v1-dynamic-rebatching-for-efficient-early-exit-inference-with-drex.html">Dynamic Rebatching for Efficient Early-Exit Inference with DREX</a></td>
  <td>æå‡ºåŠ¨æ€é‡æ‰¹å¤„ç†ä»¥è§£å†³æ—©æœŸé€€å‡ºæ¨ç†æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15705v1" onclick="toggleFavorite(this, '2512.15705v1', 'Dynamic Rebatching for Efficient Early-Exit Inference with DREX')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251215614v1-behavior-tokens-speak-louder-disentangled-explainable-recommendation.html">Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary</a></td>
  <td>BEATï¼šé€šè¿‡è¡Œä¸ºè¯æ±‡å®ç°å¯è§£é‡Šæ¨èï¼Œè§£å†³ç°æœ‰æ–¹æ³•è¯­ä¹‰æ¨¡ç³Šå’Œç»“æ„é™åˆ¶é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15614v1" onclick="toggleFavorite(this, '2512.15614v1', 'Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251215176v1-deer-draft-with-diffusion-verify-with-autoregressive-models.html">DEER: Draft with Diffusion, Verify with Autoregressive Models</a></td>
  <td>DEERï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè‰ç¨¿ç”Ÿæˆï¼Œè‡ªå›å½’æ¨¡å‹è¿›è¡ŒéªŒè¯ï¼Œæå‡LLMæ¨ç†æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15176v1" onclick="toggleFavorite(this, '2512.15176v1', 'DEER: Draft with Diffusion, Verify with Autoregressive Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251215082v1-the-semantic-architect-how-feaml-bridges-structured-data-and-llms-fo.html">The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks</a></td>
  <td>FEAMLï¼šåˆ©ç”¨LLMæ¡¥æ¥ç»“æ„åŒ–æ•°æ®ä¸å¤šæ ‡ç­¾ä»»åŠ¡ï¼Œå®ç°è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15082v1" onclick="toggleFavorite(this, '2512.15082v1', 'The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251215003v1-sebertis-a-framework-for-producing-classifiers-of-security-related-i.html">SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports</a></td>
  <td>SEBERTISï¼šä¸€ä¸ªç”¨äºç”Ÿæˆå®‰å…¨ç›¸å…³é—®é¢˜æŠ¥å‘Šåˆ†ç±»å™¨çš„æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15003v1" onclick="toggleFavorite(this, '2512.15003v1', 'SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251215000v1-dreamprm-code-function-as-step-process-reward-model-with-label-corre.html">DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding</a></td>
  <td>DreamPRM-Codeï¼šåˆ©ç”¨å‡½æ•°ä½œä¸ºæ­¥éª¤çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡æ ‡ç­¾æ ¡æ­£æå‡LLMä»£ç ç”Ÿæˆèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15000v1" onclick="toggleFavorite(this, '2512.15000v1', 'DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/251215385v1-robustness-evaluation-of-machine-learning-models-for-fault-classific.html">Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection</a></td>
  <td>æå‡ºç”µåŠ›ç³»ç»Ÿä¿æŠ¤ä¸­æœºå™¨å­¦ä¹ æ¨¡å‹é²æ£’æ€§è¯„ä¼°æ¡†æ¶ï¼Œè§£å†³æ¶åŠ£å·¥å†µä¸‹çš„å¯é æ€§é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">penetration</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15385v1" onclick="toggleFavorite(this, '2512.15385v1', 'Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251215086v1-pip2-net-physics-informed-partition-penalty-deep-operator-network.html">PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network</a></td>
  <td>æå‡ºPIP$^2$ Netï¼Œé€šè¿‡ç‰©ç†ä¿¡æ¯åˆ†åŒºæƒ©ç½šæå‡DeepONetåœ¨æ±‚è§£å‚æ•°åŒ–åå¾®åˆ†æ–¹ç¨‹ä¸­çš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15086v1" onclick="toggleFavorite(this, '2512.15086v1', 'PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.LG é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)