---
layout: default
title: Closing the Gap between TD Learning and Supervised Learning with $Q$-Conditioned Maximization
---

# Closing the Gap between TD Learning and Supervised Learning with $Q$-Conditioned Maximization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00795" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.00795v3</a>
  <a href="https://arxiv.org/pdf/2506.00795.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00795v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00795v3', 'Closing the Gap between TD Learning and Supervised Learning with $Q$-Conditioned Maximization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xing Lei, Zifeng Zhuang, Shentao Yang, Sheng Xu, Yunhao Luo, Fei Shen, Wenyan Yang, Xuetao Zhang, Donglin Wang

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-01 (Êõ¥Êñ∞: 2025-09-11)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫QÊù°‰ª∂ÊúÄÂ§ßÂåñÁõëÁù£Â≠¶‰π†‰ª•Ëß£ÂÜ≥SL‰∏éTDÂ≠¶‰π†Èó¥ÁöÑÊÄßËÉΩÂ∑ÆË∑ù**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†` `ÁõëÁù£Â≠¶‰π†` `QÊù°‰ª∂ÊúÄÂ§ßÂåñ` `ËΩ®ËøπÊãºÊé•` `ÊúüÊúõÂõûÂΩí` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Êô∫ËÉΩÂÜ≥Á≠ñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÂú®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÁº∫‰πèËΩ®ËøπÊãºÊé•ËÉΩÂäõÔºåÂØºËá¥ÊÄßËÉΩ‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜQÊù°‰ª∂ÊúÄÂ§ßÂåñÁõëÁù£Â≠¶‰π†ÔºåÈÄöËøáQÊù°‰ª∂Á≠ñÁï•ÂíåÊúÄÂ§ßÂåñÊù•Â¢ûÂº∫ÁõëÁù£Â≠¶‰π†ÁöÑÊãºÊé•ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåGCReinSLÂú®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Êï∞ÊçÆÈõÜ‰∏äÁöÑÊãºÊé•ËØÑ‰º∞‰∏≠‰ºò‰∫é‰º†ÁªüÁöÑÁõëÁù£Â≠¶‰π†ÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÂõ†ÂÖ∂ÁÆÄÂçïÊÄß„ÄÅÁ®≥ÂÆöÊÄßÂíåÊïàÁéáËÄåÊàê‰∏∫Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÁöÑÊúâÊïàÈÄîÂæÑ„ÄÇÁÑ∂ËÄåÔºåÁ†îÁ©∂Ë°®ÊòéÔºåÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÁº∫‰πè‰∏éÊó∂Èó¥Â∑ÆÂàÜÔºàTDÔºâÊñπÊ≥ïÁõ∏ÂÖ≥ÁöÑËΩ®ËøπÊãºÊé•ËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜQÊù°‰ª∂ÊúÄÂ§ßÂåñÁõëÁù£Â≠¶‰π†ÔºåÂ¢ûÂº∫‰∫ÜÁõëÁù£Â≠¶‰π†ÁöÑÊãºÊé•ËÉΩÂäõ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÁõÆÊ†áÊù°‰ª∂Âº∫ÂåñÁõëÁù£Â≠¶‰π†ÔºàGCReinSLÔºâÔºåÈÄöËøá‰ªéÁ¶ªÁ∫øÊï∞ÊçÆÈõÜ‰∏≠‰º∞ËÆ°QÂáΩÊï∞Âπ∂ÁªìÂêàÊúüÊúõÂõûÂΩíÊâæÂà∞Êï∞ÊçÆÊîØÊåÅ‰∏ãÁöÑÊúÄÂ§ßQÂÄºÔºå‰ªéËÄåÂú®Êé®ÁêÜÊó∂ÈÄâÊã©ÊúÄ‰ºòÂä®‰Ωú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Êï∞ÊçÆÈõÜ‰∏äÁöÑÊãºÊé•ËØÑ‰º∞‰∏≠‰ºò‰∫éÁé∞ÊúâÁöÑÁõëÁù£Â≠¶‰π†ÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÂú®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠Áº∫‰πèËΩ®ËøπÊãºÊé•ËÉΩÂäõÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇ‰ªªÂä°Êó∂ÔºåÂæÄÂæÄÊó†Ê≥ïÊúâÊïàÂà©Áî®ÂéÜÂè≤Êï∞ÊçÆËøõË°åÂÜ≥Á≠ñÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫‰∫ÜQÊù°‰ª∂ÊúÄÂ§ßÂåñÁõëÁù£Â≠¶‰π†ÔºåÈÄöËøáÂºïÂÖ•QÊù°‰ª∂Á≠ñÁï•ÂíåÊúÄÂ§ßÂåñÊú∫Âà∂Ôºå‰ΩøÂæóÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åËΩ®ËøπÊãºÊé•Ôºå‰ªéËÄåÊèêÂçáÂÖ∂Âú®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑË°®Áé∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGCReinSLÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÔºåÈÄöËøáÂΩí‰∏ÄÂåñÊµÅÔºàNormalizing FlowsÔºâ‰ªéÁ¶ªÁ∫øÊï∞ÊçÆÈõÜ‰∏≠‰º∞ËÆ°QÂáΩÊï∞ÔºõÂÖ∂Ê¨°ÔºåÁªìÂêàQÂáΩÊï∞ÊúÄÂ§ßÂåñ‰∏éÊúüÊúõÂõûÂΩíÔºåÊâæÂà∞Êï∞ÊçÆÊîØÊåÅ‰∏ãÁöÑÊúÄÂ§ßQÂÄº„ÄÇÂú®Êé®ÁêÜÈò∂ÊÆµÔºåÁ≠ñÁï•Âü∫‰∫éÊúÄÂ§ßQÂÄºÈÄâÊã©ÊúÄ‰ºòÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜQÊù°‰ª∂ÊúÄÂ§ßÂåñÁöÑÊ¶ÇÂøµÔºå‰ΩøÂæóÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÂÖ∑Â§á‰∫ÜÁ±ª‰ºº‰∫éTDÂ≠¶‰π†ÁöÑËΩ®ËøπÊãºÊé•ËÉΩÂäõ„ÄÇËøô‰∏ÄËÆæËÆ°‰ΩøÂæóGCReinSLÂú®Â§ÑÁêÜÂ§çÊùÇ‰ªªÂä°Êó∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®ÂéÜÂè≤‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÊàë‰ª¨ÈááÁî®‰∫ÜÂΩí‰∏ÄÂåñÊµÅÊù•‰º∞ËÆ°QÂáΩÊï∞ÔºåÂπ∂‰ΩøÁî®ÊúüÊúõÂõûÂΩíÊù•ÂÆûÁé∞QÂÄºÁöÑÊúÄÂ§ßÂåñ„ÄÇÊ≠§Â§ñÔºåÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüÁªèËøáÁ≤æÂøÉË∞ÉÊï¥Ôºå‰ª•Á°Æ‰øùÊ®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÁ®≥ÂÆöÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGCReinSLÂú®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Êï∞ÊçÆÈõÜ‰∏äÁöÑÊãºÊé•ËØÑ‰º∞‰∏≠ÔºåÊÄßËÉΩÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏ä„ÄÇËøô‰∏ÄÁªìÊûúÈ™åËØÅ‰∫ÜQÊù°‰ª∂ÊúÄÂ§ßÂåñÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊ∏∏ÊàèÊô∫ËÉΩ‰ΩìÁ≠âÈúÄË¶ÅÈ´òÊïàÂÜ≥Á≠ñÁöÑÂú∫ÊôØ„ÄÇÈÄöËøáÊèêÂçáÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÁöÑÊÄßËÉΩÔºåGCReinSLËÉΩÂ§üÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÆûÁé∞Êõ¥È´òÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recently, supervised learning (SL) methodology has emerged as an effective approach for offline reinforcement learning (RL) due to their simplicity, stability, and efficiency. However, recent studies show that SL methods lack the trajectory stitching capability, typically associated with temporal difference (TD)-based approaches. A question naturally surfaces: \textit{How can we endow SL methods with stitching capability and close its performance gap with TD learning?} To answer this question, we introduce $Q$-conditioned maximization supervised learning for offline goal-conditioned RL, which enhances SL with the stitching capability through $Q$-conditioned policy and $Q$-conditioned maximization. Concretely, we propose \textbf{G}oal-\textbf{C}onditioned \textbf{\textit{Rein} }forced \textbf{S}upervised \textbf{L}earning (\textbf{GC\textit{Rein}SL}), which consists of (1) estimating the $Q$-function by Normalizing Flows from the offline dataset and (2) finding the maximum $Q$-value within the data support by integrating $Q$-function maximization with Expectile Regression. In inference time, our policy chooses optimal actions based on such a maximum $Q$-value. Experimental results from stitching evaluations on offline RL datasets demonstrate that our method outperforms prior SL approaches with stitching capabilities and goal data augmentation techniques.

