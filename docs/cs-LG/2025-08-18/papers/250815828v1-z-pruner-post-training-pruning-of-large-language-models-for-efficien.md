---
layout: default
title: Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining
---

# Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15828" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15828v1</a>
  <a href="https://arxiv.org/pdf/2508.15828.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15828v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15828v1', 'Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Samiul Basir Bhuiyan, Md. Sazzad Hossain Adib, Mohammed Aman Bhuiyan, Muhammad Rafsan Kabir, Moshiur Farazi, Shafin Rahman, Nabeel Mohammed

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

**å¤‡æ³¨**: Accepted at AICCSA 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/sazzadadib/Z-Pruner)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZ-Prunerä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒå‰ªææ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åè®­ç»ƒå‰ªæ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹ç¨€ç–åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åè®­ç»ƒå‰ªææ–¹æ³•å¾€å¾€å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™æˆ–éœ€è¦æ˜‚è´µçš„å¾®è°ƒï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. Z-Pruneré€šè¿‡ç»“åˆæƒé‡æ›´æ–°å¹…åº¦å’Œæ¿€æ´»æ¨¡å¼ï¼Œæœ‰æ•ˆè¯†åˆ«å’Œæ¶ˆé™¤å†—ä½™å‚æ•°ï¼Œå®ç°äº†æ¨¡å‹çš„ç¨€ç–åŒ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒZ-Pruneråœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå–å¾—äº†æœ€ä½çš„å›°æƒ‘åº¦å’Œæœ€é«˜çš„é›¶-shotå‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ¨¡å‹è§„æ¨¡çš„ä¸æ–­æ‰©å¤§ç»™éƒ¨ç½²ã€å¯æ‰©å±•æ€§å’Œèƒ½æ•ˆå¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œåè®­ç»ƒå‰ªææˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†å»¶è¿Ÿã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰çš„å‰ªææ–¹æ³•ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™æˆ–éœ€è¦è®¡ç®—æˆæœ¬é«˜æ˜‚çš„å¾®è°ƒã€‚æœ¬æ–‡æå‡ºäº†Z-Prunerï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åè®­ç»ƒå‰ªææ–¹æ³•ï¼Œæ—¨åœ¨æœ‰æ•ˆè¯†åˆ«å’Œæ¶ˆé™¤å†—ä½™å‚æ•°ã€‚Z-Pruneråˆ©ç”¨æƒé‡æ›´æ–°å¹…åº¦å’Œæ¿€æ´»æ¨¡å¼ï¼Œå…·æœ‰æ¨¡å‹æ— å…³æ€§ã€æ•ˆç‡é«˜å’Œæ˜“äºå®ç°çš„ç‰¹ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZ-Pruneråœ¨å¤šä¸ªæ ‡å‡†è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„å‰ªææ–¹æ³•ï¼Œå–å¾—äº†æœ€ä½çš„å›°æƒ‘åº¦åˆ†æ•°å’Œæœ€é«˜çš„é›¶-shotå‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åè®­ç»ƒå‰ªæè¿‡ç¨‹ä¸­æ€§èƒ½ä¸‹é™å’Œå¾®è°ƒéœ€æ±‚é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å‰ªæåå¾€å¾€éœ€è¦é‡æ–°è®­ç»ƒï¼Œå¢åŠ äº†è®¡ç®—æˆæœ¬å’Œå¤æ‚æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šZ-Prunerçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æƒé‡æ›´æ–°å¹…åº¦å’Œæ¿€æ´»æ¨¡å¼æ¥è¯†åˆ«å†—ä½™å‚æ•°ï¼Œä»è€Œåœ¨ä¸è¿›è¡Œé‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°æ¨¡å‹ç¨€ç–åŒ–ã€‚è¿™ç§è®¾è®¡ä½¿å¾—å‰ªæè¿‡ç¨‹æ›´åŠ é«˜æ•ˆä¸”ä¸æŸå¤±æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šZ-Prunerçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‚æ•°è¯†åˆ«ã€å‰ªæå†³ç­–å’Œæ¨¡å‹è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ†ææƒé‡æ›´æ–°å’Œæ¿€æ´»æ¨¡å¼æ¥è¯†åˆ«å†—ä½™å‚æ•°ï¼›ç„¶åï¼ŒåŸºäºè¿™äº›ä¿¡æ¯è¿›è¡Œå‰ªæï¼›æœ€åï¼Œè¯„ä¼°å‰ªæåçš„æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šZ-Prunerçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨¡å‹æ— å…³æ€§å’Œé«˜æ•ˆæ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„LLMæ¶æ„ä¸Šåº”ç”¨ï¼Œè€Œä¸éœ€è¦è¿›è¡Œå¤æ‚çš„å¾®è°ƒã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œåè€…é€šå¸¸ä¾èµ–äºå¤§é‡çš„æƒé‡æ›´æ–°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Z-Prunerä¸­ï¼Œå‚æ•°è®¾ç½®å’Œå‰ªæç­–ç•¥ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨å‰ªæè¿‡ç¨‹ä¸­å°½é‡å‡å°‘æ€§èƒ½æŸå¤±ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ä½¿å¾—å‰ªæè¿‡ç¨‹æ›´åŠ é«˜æ•ˆï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†ä¸Šè¿›è¡ŒéªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒZ-Pruneråœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚LLaMA-2ã€LLaMA-3å’ŒOPTï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå–å¾—äº†æœ€ä½çš„å›°æƒ‘åº¦åˆ†æ•°å’Œæœ€é«˜çš„é›¶-shotå‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å‰ªææ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Z-Prunerçš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡æœ‰æ•ˆå‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†å»¶è¿Ÿï¼ŒZ-Prunerèƒ½å¤Ÿæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯ç”¨æ€§å’Œèƒ½æ•ˆï¼Œæ¨åŠ¨æ›´å¹¿æ³›çš„å•†ä¸šåŒ–åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šå½±å“æ›´å¤šé¢†åŸŸçš„æ¨¡å‹ä¼˜åŒ–å’Œéƒ¨ç½²ç­–ç•¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have rapidly advanced in recent years, achieving remarkable performance across a wide range of natural language processing tasks. However, this progress has come at the cost of increasingly large model sizes, which pose significant challenges for deployment, scalability, and energy efficiency. To address these limitations, post-training pruning has emerged as a promising approach for reducing model size and inference latency without the need for retraining. Despite these advantages, many existing pruning methods result in substantial performance degradation or require computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a novel post-training pruning method designed to induce sparsity in pretrained LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages both weight update magnitudes and activation patterns to identify and eliminate redundant parameters more effectively. Our method is model-agnostic, efficient, and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of standard language benchmarks. Experimental results demonstrate that Z-Pruner surpasses state-of-the-art pruning methods that require intensive weight updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the highest overall average score for zero-shot accuracy. We have made the corresponding codes publicly available at https://github.com/sazzadadib/Z-Pruner.

