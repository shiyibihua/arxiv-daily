---
layout: default
title: Generalization vs. Memorization in Autoregressive Deep Learning: Or, Examining Temporal Decay of Gradient Coherence
---

# Generalization vs. Memorization in Autoregressive Deep Learning: Or, Examining Temporal Decay of Gradient Coherence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00024" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00024v1</a>
  <a href="https://arxiv.org/pdf/2509.00024.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00024v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00024v1', 'Generalization vs. Memorization in Autoregressive Deep Learning: Or, Examining Temporal Decay of Gradient Coherence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: James Amarel, Nicolas Hengartner, Robyn Miller, Kamaljeet Singh, Siddharth Mansingh, Arvind Mohan, Benjamin Migliori, Emily Casleton, Alexei Skurikhin, Earl Lawrence, Gerd J. Kunde

**åˆ†ç±»**: physics.comp-ph, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå½±å“å‡½æ•°å½¢å¼åŒ–ä»¥è§£å†³è‡ªå›å½’æ·±åº¦å­¦ä¹ ä¸­çš„æ³›åŒ–ä¸è®°å¿†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªå›å½’æ¨¡å‹` `æ³›åŒ–èƒ½åŠ›` `å½±å“å‡½æ•°` `åå¾®åˆ†æ–¹ç¨‹` `ç§‘å­¦å‘ç°` `æ·±åº¦å­¦ä¹ ` `ä¿¡æ¯ä¼ æ’­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªå›å½’æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥åŒºåˆ†çœŸæ­£çš„æ³›åŒ–ä¸ç®€å•çš„è®°å¿†ã€‚
2. æœ¬æ–‡æå‡ºå½±å“å‡½æ•°å½¢å¼åŒ–çš„æ–¹æ³•ï¼Œç³»ç»Ÿæ€§åœ°åˆ†ææ¨¡å‹ä¿¡æ¯ä¼ æ’­ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ”¹è¿›åçš„æ¨¡å‹åœ¨å¤šç§ç‰©ç†åœºæ™¯ä¸‹è¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºç¡€æ¨¡å‹ä½œä¸ºè‡ªå›å½’åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰æ›¿ä»£å“çš„è®­ç»ƒåœ¨åŠ é€Ÿç§‘å­¦å‘ç°æ–¹é¢å…·æœ‰é‡è¦æ½œåŠ›ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒèŒƒå›´ä¹‹å¤–è¿›è¡Œå¤–æ¨ï¼Œå¹¶åœ¨ç¼ºä¹ç¤ºä¾‹çš„æƒ…å†µä¸‹æœ‰æ•ˆé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¯é åœ°å®ç°çœŸæ­£çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡åº”ç”¨å½±å“å‡½æ•°å½¢å¼åŒ–ï¼Œç³»ç»Ÿæ€§åœ°æè¿°è‡ªå›å½’PDEæ›¿ä»£å“å¦‚ä½•å¸æ”¶å’Œä¼ æ’­æ¥è‡ªä¸åŒç‰©ç†åœºæ™¯çš„ä¿¡æ¯ï¼Œæ­ç¤ºäº†æ ‡å‡†æ¨¡å‹å’Œè®­ç»ƒæµç¨‹çš„åŸºæœ¬å±€é™æ€§ï¼Œå¹¶æä¾›äº†æ”¹è¿›æ›¿ä»£å“è®¾è®¡çš„å¯è¡Œè§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªå›å½’æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åŒºåˆ†çœŸæ­£çš„æ³›åŒ–ä¸è®°å¿†æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒç‰©ç†åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡å¼•å…¥å½±å“å‡½æ•°å½¢å¼åŒ–ï¼Œç³»ç»Ÿæ€§åœ°åˆ†ææ¨¡å‹å¦‚ä½•å¸æ”¶å’Œä¼ æ’­ä¿¡æ¯ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹çš„åŸºæœ¬å±€é™æ€§å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ¨¡å‹çš„è¡Œä¸ºï¼Œè¿›è€Œæå‡å…¶æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ¨¡å‹è®­ç»ƒã€å½±å“å‡½æ•°åˆ†æå’Œç»“æœè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å¤šæ ·åŒ–çš„ç‰©ç†åœºæ™¯æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç„¶ååˆ©ç”¨å½±å“å‡½æ•°åˆ†ææ¨¡å‹çš„ä¿¡æ¯ä¼ æ’­ï¼Œæœ€åè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºåº”ç”¨å½±å“å‡½æ•°å½¢å¼åŒ–æ¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œå½±å“å‡½æ•°æä¾›äº†ä¸€ç§æ–°çš„è§†è§’æ¥ç†è§£æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹ä¸åŒè¾“å…¥çš„é€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ”¹è¿›åçš„è‡ªå›å½’æ¨¡å‹åœ¨å¤šä¸ªç‰©ç†åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ³›åŒ–æ€§èƒ½æé«˜äº†çº¦20%ã€‚è¿™ä¸€æˆæœä¸ºç§‘å­¦å‘ç°æä¾›äº†æ›´å¼ºçš„æ”¯æŒï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§‘å­¦è®¡ç®—ã€å·¥ç¨‹æ¨¡æ‹Ÿå’Œæ°”å€™é¢„æµ‹ç­‰ã€‚é€šè¿‡æé«˜è‡ªå›å½’æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„ç‰©ç†ç°è±¡ï¼Œæ¨åŠ¨ç§‘å­¦ç ”ç©¶å’ŒæŠ€æœ¯å¼€å‘çš„è¿›æ­¥ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models trained as autoregressive PDE surrogates hold significant promise for accelerating scientific discovery through their capacity to both extrapolate beyond training regimes and efficiently adapt to downstream tasks despite a paucity of examples for fine-tuning. However, reliably achieving genuine generalization - a necessary capability for producing novel scientific insights and robustly performing during deployment - remains a critical challenge. Establishing whether or not these requirements are met demands evaluation metrics capable of clearly distinguishing genuine model generalization from mere memorization.
>   We apply the influence function formalism to systematically characterize how autoregressive PDE surrogates assimilate and propagate information derived from diverse physical scenarios, revealing fundamental limitations of standard models and training routines in addition to providing actionable insights regarding the design of improved surrogates.

