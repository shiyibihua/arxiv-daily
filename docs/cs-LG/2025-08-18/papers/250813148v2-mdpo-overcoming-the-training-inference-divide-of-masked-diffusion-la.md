---
layout: default
title: MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models
---

# MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13148" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.13148v2</a>
  <a href="https://arxiv.org/pdf/2508.13148.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13148v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13148v2', 'MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Haoyu He, Katrin Renz, Yong Cao, Andreas Geiger

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-18 (Êõ¥Êñ∞: 2025-09-25)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/autonomousvision/mdpo) | [PROJECT_PAGE](https://cli212.github.io/MDPO/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MDPO‰ª•Ëß£ÂÜ≥Êé©Á†ÅÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉ‰∏éÊé®ÁêÜ‰∏ç‰∏ÄËá¥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êé©Á†ÅÊâ©Êï£Ê®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `Â∫èÂàóÂÜ≥Á≠ñ` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `Ê®°Âûã‰ºòÂåñ` `ÂéªÂô™Á≠ñÁï•` `ÊÄßËÉΩÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊé©Á†ÅÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜÈò∂ÊÆµÂ≠òÂú®ÁªìÊûÑÊè≠Á§∫‰∏ç‰∏ÄËá¥ÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜÊé©Á†ÅÊâ©Êï£Á≠ñÁï•‰ºòÂåñÔºàMDPOÔºâÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ëß£ÂÜ≥ËÆ≠ÁªÉ‰∏éÊé®ÁêÜÈò∂ÊÆµÁöÑÂ∑ÆÂºÇÔºåÊòæÂºèËÆ≠ÁªÉÊ®°Âûã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMDPOÂú®ÂáèÂ∞ëÊ¢ØÂ∫¶Êõ¥Êñ∞Ê¨°Êï∞ÁöÑÂêåÊó∂ÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÊÄßËÉΩÔºåË°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êé©Á†ÅÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàMDLMsÔºâ‰Ωú‰∏∫‰º†ÁªüËá™ÂõûÂΩíÊ®°ÂûãÁöÑÊúâÂäõÊõø‰ª£ÔºåËÉΩÂ§üÊõ¥Âø´ÁîüÊàêÊñáÊú¨Âπ∂Êõ¥‰∏∞ÂØåÂú∞ËøõË°åÂèåÂêë‰∏ä‰∏ãÊñáÊù°‰ª∂„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨Âú®ËÆ≠ÁªÉÂíåÊé®ÁêÜÈò∂ÊÆµÂ≠òÂú®ÂÖ≥ÈîÆÂ∑ÆÂºÇÔºöÊé®ÁêÜÊó∂ÈÄêÊ≠•Êè≠Á§∫ÁîüÊàêÂ∫èÂàóÁöÑÁªìÊûÑÔºåËÄåËÆ≠ÁªÉÊó∂ÈöèÊú∫Êé©Á†ÅÂøΩÁï•‰∫ÜËøô‰∏ÄÁªìÊûÑ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÂ∞ÜÊúâÊïàÂéªÂô™ËΩ®ËøπÁöÑÂ≠¶‰π†Ê°ÜÊû∂ËßÜ‰∏∫‰∏Ä‰∏™Â∫èÂàóÂÜ≥Á≠ñÈóÆÈ¢òÔºåÂπ∂Â∫îÁî®Âº∫ÂåñÂ≠¶‰π†„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊé©Á†ÅÊâ©Êï£Á≠ñÁï•‰ºòÂåñÔºàMDPOÔºâÔºåÂú®Êé®ÁêÜÊó∂‰ΩøÁî®Áõ∏ÂêåÁöÑÈÄêÊ≠•Á≤æÁÇºË∞ÉÂ∫¶ËøõË°åÊòæÂºèËÆ≠ÁªÉ„ÄÇMDPOÂú®Ê¢ØÂ∫¶Êõ¥Êñ∞Ê¨°Êï∞ÂáèÂ∞ë60ÂÄçÁöÑÊÉÖÂÜµ‰∏ãÔºåËææÂà∞‰∫Ü‰πãÂâçÊúÄÂÖàËøõÊñπÊ≥ïÁöÑÊÄßËÉΩÔºåÂπ∂Âú®MATH500ÂíåCountdownÊï∞ÊçÆÈõÜ‰∏äÂàÜÂà´ÊèêÈ´ò‰∫Ü9.6%Âíå54.2%ÁöÑÂáÜÁ°ÆÁéá„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊîπËøõ‰∫ÜMDLMsÁöÑÈáçÊñ∞Êé©Á†ÅÁ≠ñÁï•ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ËÆ≠ÁªÉÊñπÊ≥ï‚Äî‚ÄîËøêË°åÁΩÆ‰ø°Â∫¶ÈáçÊñ∞Êé©Á†ÅÔºàRCRÔºâÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êé©Á†ÅÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜÈò∂ÊÆµ‰πãÈó¥ÁöÑÁªìÊûÑÊè≠Á§∫‰∏ç‰∏ÄËá¥ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ËÆ≠ÁªÉÊó∂ÈöèÊú∫Êé©Á†ÅÔºåÂØºËá¥Êé®ÁêÜÊó∂ÁöÑÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨Â∞ÜÊúâÊïàÂéªÂô™ËΩ®ËøπÁöÑÂ≠¶‰π†ËßÜ‰∏∫Â∫èÂàóÂÜ≥Á≠ñÈóÆÈ¢òÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ËøõË°åÊòæÂºèËÆ≠ÁªÉÔºå‰ª•ÂåπÈÖçÊé®ÁêÜÊó∂ÁöÑÈÄêÊ≠•Á≤æÁÇºË∞ÉÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨MDPOÁöÑËÆ≠ÁªÉËøáÁ®ãÔºåÂà©Áî®È©¨Â∞îÂèØÂ§´ÊÄßË¥®ËøõË°åÊ®°Âûã‰ºòÂåñ„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨Áä∂ÊÄÅË°®Á§∫„ÄÅÂä®‰ΩúÈÄâÊã©ÂíåÂ•ñÂä±ÂèçÈ¶àÊú∫Âà∂„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMDPOÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊòæÂºèËÆ≠ÁªÉÊ®°ÂûãÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ï‰∏≠ËÆ≠ÁªÉ‰∏éÊé®ÁêÜÈò∂ÊÆµÁöÑÂ∑ÆÂºÇÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÊàë‰ª¨ËÆæÁΩÆ‰∫ÜÁâπÂÆöÁöÑÂ•ñÂä±ÂáΩÊï∞‰ª•ÂºïÂØºÊ®°ÂûãÂ≠¶‰π†ÊúâÊïàÁöÑÂéªÂô™Á≠ñÁï•ÔºåÂπ∂‰ºòÂåñ‰∫ÜÁΩëÁªúÁªìÊûÑ‰ª•ÈÄÇÂ∫îÈÄêÊ≠•Á≤æÁÇºÁöÑËøáÁ®ã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMDPOÂú®‰∏é‰πãÂâçÊúÄÂÖàËøõÊñπÊ≥ïÁöÑÊØîËæÉ‰∏≠ÔºåÂáèÂ∞ë‰∫Ü60ÂÄçÁöÑÊ¢ØÂ∫¶Êõ¥Êñ∞Ê¨°Êï∞ÔºåÂêåÊó∂Âú®MATH500ÂíåCountdownÊï∞ÊçÆÈõÜ‰∏äÂàÜÂà´ÊèêÈ´ò‰∫Ü9.6%Âíå54.2%ÁöÑÊÄßËÉΩÔºåÂ±ïÁé∞Âá∫ÊòæËëóÁöÑÊîπËøõÊïàÊûú„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàê„ÄÅÂØπËØùÁ≥ªÁªüÂíåÊñáÊú¨Ë°•ÂÖ®Á≠â„ÄÇÈÄöËøáÊèêÈ´òÊé©Á†ÅÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩÔºåËÉΩÂ§üÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÆûÁé∞Êõ¥È´òÊïàÁöÑÊñáÊú¨ÁîüÊàêÂíåÊõ¥ÂáÜÁ°ÆÁöÑ‰∏ä‰∏ãÊñáÁêÜËß£ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Diffusion language models, as a promising alternative to traditional autoregressive (AR) models, enable faster generation and richer conditioning on bidirectional context. However, they suffer from a key discrepancy between training and inference: during inference, MDLMs progressively reveal the structure of the generated sequence by producing fewer and fewer masked tokens, whereas this structure is ignored in training as tokens are masked at random. Although this discrepancy between training and inference can lead to suboptimal performance, it has been largely overlooked by previous works, leaving closing this gap between the two stages an open problem. To address this, we frame the problem of learning effective denoising trajectories as a sequential decision-making problem and use the resulting framework to apply reinforcement learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to exploit the Markov property diffusion possesses and explicitly train the model under the same progressive refining schedule used at inference. MDPO matches the performance of the previous state-of-the-art (SOTA) method with 60x fewer gradient updates, while achieving average improvements of 9.6% on MATH500 and 54.2% on Countdown over SOTA when trained within the same number of weight updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in inference replacement to overcome the limitation that the model cannot refine tokens flexibly. This training-free method, termed Running Confidence Remasking (RCR), consistently enhances performance and provides further improvements when used with MDPO. Our findings establish great potential for investigating the discrepancy between pre-training and inference of MDLMs. Code: https://github.com/autonomousvision/mdpo. Project Page: https://cli212.github.io/MDPO/.

