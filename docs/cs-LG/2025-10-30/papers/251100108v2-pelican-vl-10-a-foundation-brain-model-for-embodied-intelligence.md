---
layout: default
title: Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence
---

# Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.00108" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.00108v2</a>
  <a href="https://arxiv.org/pdf/2511.00108.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00108v2" onclick="toggleFavorite(this, '2511.00108v2', 'Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Zhang, Che Liu, Xiancong Ren, Hanchu Ni, Shuai Zhang, Zeyuan Ding, Jiayu Hu, Hanzhe Shan, Zhenwei Niu, Zhaoyang Liu, Shuang Liu, Yue Zhao, Junbo Qi, Qinfan Zhang, Dengjie Li, Yidong Wang, Jiachen Luo, Yong Dai, Zenglin Xu, Bin Shen, Qifan Wang, Jian Tang, Xiaozhu Ju

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30 (æ›´æ–°: 2025-11-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Pelican-VL 1.0ï¼šç”¨äºå…·èº«æ™ºèƒ½çš„å¼€æºåŸºç¡€å¤§è„‘æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `å¤šæ¨¡æ€å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®æç‚¼` `å¤§è„‘æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…·èº«æ™ºèƒ½æ¨¡å‹åœ¨æ•°æ®è´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢å­˜åœ¨ç“¶é¢ˆï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®ã€‚
2. Pelican-VL 1.0 é€šè¿‡ metaloop æç‚¼é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ DPPO æ¡†æ¶è¿›è¡Œåˆ»æ„ç»ƒä¹ ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPelican-VL 1.0 æ€§èƒ½ä¼˜äºåŒç­‰è§„æ¨¡çš„å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸é¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿåœ¨å…·èº«åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æŠ¥å‘Šä»‹ç»äº†Pelican-VL 1.0ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¼€æºå…·èº«å¤§è„‘æ¨¡å‹ç³»åˆ—ï¼Œå‚æ•°è§„æ¨¡ä»70äº¿åˆ°720äº¿ä¸ç­‰ã€‚æˆ‘ä»¬çš„æ˜ç¡®ç›®æ ‡æ˜¯ï¼šå°†å¼ºå¤§çš„æ™ºèƒ½åµŒå…¥åˆ°å„ç§å…·èº«ç¯å¢ƒä¸­ã€‚Pelican-VL 1.0æ˜¯ç›®å‰æœ€å¤§è§„æ¨¡çš„å¼€æºå…·èº«å¤šæ¨¡æ€å¤§è„‘æ¨¡å‹ã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæ•°æ®èƒ½åŠ›å’Œæ™ºèƒ½è‡ªé€‚åº”å­¦ä¹ æœºåˆ¶çš„æ·±åº¦æ•´åˆã€‚å…·ä½“æ¥è¯´ï¼Œmetaloopä»åŒ…å«40äº¿+ tokens çš„åŸå§‹æ•°æ®é›†ä¸­æç‚¼å‡ºäº†é«˜è´¨é‡çš„æ•°æ®é›†ã€‚Pelican-VL 1.0 åœ¨ä¸€ä¸ªåŒ…å« 1000+ A800 GPU çš„å¤§å‹é›†ç¾¤ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¯ä¸ª checkpoint æ¶ˆè€—è¶…è¿‡ 50k+ A800 GPU-hoursã€‚è¿™ä½¿å¾—å…¶æ€§èƒ½æ¯”åŸºç¡€æ¨¡å‹æå‡äº† 20.3%ï¼Œå¹¶ä¸”æ¯” 100B çº§åˆ«çš„å¼€æºæ¨¡å‹é«˜å‡º 10.6%ï¼Œåœ¨è‘—åçš„å…·èº«åŸºå‡†æµ‹è¯•ä¸­ä¸é¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿç›¸å½“ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå—äººç±»å…ƒè®¤çŸ¥å¯å‘çš„å…¨æ–°æ¡†æ¶ DPPO (Deliberate Practice Policy Optimization)ï¼Œç”¨äºè®­ç»ƒ Pelican-VL 1.0ã€‚æˆ‘ä»¬å°†å…¶æ“ä½œåŒ–ä¸ºä¸€ä¸ª metaloopï¼Œæ•™å¯¼ AI è¿›è¡Œåˆ»æ„ç»ƒä¹ ï¼Œè¿™æ˜¯ä¸€ä¸ª RL-Refine-Diagnose-SFT å¾ªç¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å…·èº«æ™ºèƒ½æ¨¡å‹é¢ä¸´æ•°æ®è´¨é‡ä¸é«˜ã€è®­ç»ƒæ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚å…·èº«ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å¤§è§„æ¨¡åŸå§‹æ•°æ®ï¼Œå¹¶ä¸”ç¼ºä¹æœ‰æ•ˆçš„è‡ªé€‚åº”å­¦ä¹ æœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPelican-VL 1.0 çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ•°æ®æç‚¼å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç›¸ç»“åˆï¼Œæå‡æ¨¡å‹åœ¨å…·èº«ç¯å¢ƒä¸­çš„æ™ºèƒ½æ°´å¹³ã€‚é€šè¿‡ metaloop æç‚¼é«˜è´¨é‡æ•°æ®é›†ï¼Œå‡å°‘å™ªå£°æ•°æ®çš„å½±å“ï¼›é€šè¿‡ DPPO æ¡†æ¶æ¨¡æ‹Ÿäººç±»çš„åˆ»æ„ç»ƒä¹ è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ å’Œæ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPelican-VL 1.0 çš„è®­ç»ƒæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ï¼šæ”¶é›†åŒ…å«å¤§é‡ tokens çš„åŸå§‹æ•°æ®é›†ã€‚2) æ•°æ®æç‚¼ï¼šä½¿ç”¨ metaloop ä»åŸå§‹æ•°æ®é›†ä¸­æç‚¼é«˜è´¨é‡çš„æ•°æ®é›†ã€‚3) æ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨æç‚¼åçš„æ•°æ®é›†è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚4) ç­–ç•¥ä¼˜åŒ–ï¼šä½¿ç”¨ DPPO æ¡†æ¶å¯¹æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å…·èº«ç¯å¢ƒã€‚DPPO æ¡†æ¶åŒ…å« RL (Reinforcement Learning)ã€Refineã€Diagnose å’Œ SFT (Supervised Fine-Tuning) å››ä¸ªé˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šPelican-VL 1.0 çš„å…³é”®åˆ›æ–°åœ¨äº DPPO æ¡†æ¶å’Œ metaloop æ•°æ®æç‚¼æ–¹æ³•ã€‚DPPO æ¡†æ¶æ¨¡æ‹Ÿäººç±»çš„åˆ»æ„ç»ƒä¹ è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ å’Œæ”¹è¿›ã€‚metaloop æ•°æ®æç‚¼æ–¹æ³•èƒ½å¤Ÿä»å¤§è§„æ¨¡åŸå§‹æ•°æ®é›†ä¸­æå–é«˜è´¨é‡çš„æ•°æ®ï¼Œå‡å°‘å™ªå£°æ•°æ®çš„å½±å“ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDPPO æ¡†æ¶æ›´åŠ æ³¨é‡æ¨¡å‹çš„è‡ªé€‚åº”å­¦ä¹ èƒ½åŠ›ï¼Œè€Œ metaloop æ•°æ®æç‚¼æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šDPPO æ¡†æ¶ä¸­çš„å¥–åŠ±å‡½æ•°è®¾è®¡æ˜¯å…³é”®ã€‚å¥–åŠ±å‡½æ•°éœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°åæ˜ æ¨¡å‹åœ¨å…·èº«ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¼•å¯¼æ¨¡å‹æœç€æ­£ç¡®çš„æ–¹å‘å­¦ä¹ ã€‚metaloop æ•°æ®æç‚¼æ–¹æ³•ä¸­çš„æç‚¼ç­–ç•¥ä¹Ÿæ˜¯å…³é”®ã€‚æç‚¼ç­–ç•¥éœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«å’Œæå–é«˜è´¨é‡çš„æ•°æ®ï¼ŒåŒæ—¶è¿‡æ»¤æ‰å™ªå£°æ•°æ®ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Pelican-VL 1.0 åœ¨å…·èº«åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¯”åŸºç¡€æ¨¡å‹æå‡äº† 20.3%ï¼Œå¹¶ä¸”æ¯” 100B çº§åˆ«çš„å¼€æºæ¨¡å‹é«˜å‡º 10.6%ï¼Œä¸é¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿç›¸å½“ã€‚è¿™è¡¨æ˜ Pelican-VL 1.0 åœ¨å…·èº«æ™ºèƒ½é¢†åŸŸå…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ï¼Œå¹¶ä¸”å…·æœ‰å¾ˆå¤§çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Pelican-VL 1.0 å¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡å°†å¼ºå¤§çš„æ™ºèƒ½åµŒå…¥åˆ°å„ç§å…·èº«ç¯å¢ƒä¸­ï¼Œå¯ä»¥å®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºå¼€å‘èƒ½å¤Ÿè‡ªä¸»å®Œæˆå¤æ‚ä»»åŠ¡çš„æœºå™¨äººåŠ©æ‰‹ï¼Œæˆ–è€…ç”¨äºå¼€å‘æ›´å®‰å…¨ã€æ›´å¯é çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿã€‚è¯¥ç ”ç©¶çš„æœªæ¥å½±å“åœ¨äºæ¨åŠ¨å…·èº«æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´äººæ€§åŒ–çš„æœºå™¨äººç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.

