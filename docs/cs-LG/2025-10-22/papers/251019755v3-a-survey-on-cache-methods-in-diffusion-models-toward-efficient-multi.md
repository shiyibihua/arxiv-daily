---
layout: default
title: A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation
---

# A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.19755" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.19755v3</a>
  <a href="https://arxiv.org/pdf/2510.19755.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19755v3" onclick="toggleFavorite(this, '2510.19755v3', 'A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-22 (æ›´æ–°: 2025-11-01)

**å¤‡æ³¨**: 22 pages,2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°æ‰©æ•£æ¨¡å‹ç¼“å­˜æ–¹æ³•ï¼ŒåŠ é€Ÿé«˜æ•ˆå¤šæ¨¡æ€ç”Ÿæˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `ç¼“å­˜æŠ€æœ¯` `æ¨¡å‹åŠ é€Ÿ` `å¤šæ¨¡æ€ç”Ÿæˆ` `é«˜æ•ˆæ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£æ¨¡å‹è®¡ç®—å¼€é”€å¤§ã€æ¨ç†å»¶è¿Ÿé«˜ï¼Œé™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„å‘å±•ã€‚
2. æ‰©æ•£ç¼“å­˜é€šè¿‡é‡ç”¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„è®¡ç®—å†—ä½™ï¼Œå®ç°å…è®­ç»ƒã€æ¶æ„æ— å…³çš„é«˜æ•ˆæ¨ç†ã€‚
3. æ‰©æ•£ç¼“å­˜ä»é™æ€å¤ç”¨å‘å±•åˆ°åŠ¨æ€é¢„æµ‹ï¼Œæå‡äº†çµæ´»æ€§ï¼Œå¹¶å¯ä¸å…¶ä»–åŠ é€ŸæŠ€æœ¯é›†æˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£æ¨¡å‹ä»¥å…¶å“è¶Šçš„ç”Ÿæˆè´¨é‡å’Œå¯æ§æ€§ï¼Œå·²æˆä¸ºç°ä»£ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„åŸºçŸ³ã€‚ç„¶è€Œï¼Œå…¶å›ºæœ‰çš„å¤šæ­¥è¿­ä»£å’Œå¤æ‚çš„éª¨å¹²ç½‘ç»œå¯¼è‡´äº†è¿‡é«˜çš„è®¡ç®—å¼€é”€å’Œç”Ÿæˆå»¶è¿Ÿï¼Œæˆä¸ºå®æ—¶åº”ç”¨çš„ä¸»è¦ç“¶é¢ˆã€‚å°½ç®¡ç°æœ‰çš„åŠ é€ŸæŠ€æœ¯å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€é€‚ç”¨æ€§æœ‰é™ã€è®­ç»ƒæˆæœ¬é«˜æˆ–è´¨é‡ä¸‹é™ç­‰æŒ‘æˆ˜ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæ‰©æ•£ç¼“å­˜æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„å…è®­ç»ƒã€æ¶æ„æ— å…³å’Œé«˜æ•ˆçš„æ¨ç†èŒƒå¼ã€‚å…¶æ ¸å¿ƒæœºåˆ¶æ˜¯è¯†åˆ«å’Œé‡ç”¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„å†…åœ¨è®¡ç®—å†—ä½™ã€‚é€šè¿‡å®ç°ç‰¹å¾çº§è·¨æ­¥å¤ç”¨å’Œå±‚é—´è°ƒåº¦ï¼Œå®ƒå¯ä»¥åœ¨ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹å‡å°‘è®¡ç®—ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°å›é¡¾äº†æ‰©æ•£ç¼“å­˜çš„ç†è®ºåŸºç¡€å’Œæ¼”å˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å¯¹å…¶è¿›è¡Œåˆ†ç±»å’Œåˆ†æã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§æ–¹æ³•çš„æ¯”è¾ƒåˆ†æï¼Œæˆ‘ä»¬è¡¨æ˜æ‰©æ•£ç¼“å­˜ä»é™æ€é‡ç”¨åˆ°åŠ¨æ€é¢„æµ‹æ¼”å˜ã€‚è¿™ç§è¶‹åŠ¿å¢å¼ºäº†ç¼“å­˜è·¨ä¸åŒä»»åŠ¡çš„çµæ´»æ€§ï¼Œå¹¶èƒ½å¤Ÿä¸å…¶ä»–åŠ é€ŸæŠ€æœ¯ï¼ˆå¦‚é‡‡æ ·ä¼˜åŒ–å’Œæ¨¡å‹è’¸é¦ï¼‰é›†æˆï¼Œä»è€Œä¸ºæœªæ¥çš„å¤šæ¨¡æ€å’Œäº¤äº’å¼åº”ç”¨æ„å»ºä¸€ä¸ªç»Ÿä¸€ã€é«˜æ•ˆçš„æ¨ç†æ¡†æ¶ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§èŒƒå¼å°†æˆä¸ºå®æ—¶å’Œé«˜æ•ˆç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å…³é”®æ¨åŠ¨åŠ›ï¼Œä¸ºé«˜æ•ˆç”Ÿæˆæ™ºèƒ½çš„ç†è®ºå’Œå®è·µæ³¨å…¥æ–°çš„æ´»åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„åŒæ—¶ï¼Œä¹Ÿé¢ä¸´ç€è®¡ç®—é‡å¤§ã€æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œæˆ–è€…ä¼šç‰ºç‰²ç”Ÿæˆè´¨é‡ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶åº”ç”¨çš„éœ€æ±‚ã€‚å› æ­¤ï¼Œå¦‚ä½•é™ä½æ‰©æ•£æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒå…¶ç”Ÿæˆè´¨é‡ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‰©æ•£è¿‡ç¨‹ä¸­å­˜åœ¨çš„è®¡ç®—å†—ä½™ï¼Œé€šè¿‡ç¼“å­˜ä¸­é—´ç‰¹å¾å¹¶è¿›è¡Œå¤ç”¨ï¼Œä»è€Œå‡å°‘é‡å¤è®¡ç®—ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥çµæ´»åœ°åº”ç”¨äºä¸åŒçš„æ‰©æ•£æ¨¡å‹æ¶æ„ã€‚é€šè¿‡æ™ºèƒ½åœ°ç®¡ç†å’Œè°ƒåº¦ç¼“å­˜ï¼Œå¯ä»¥æœ‰æ•ˆåœ°é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜æ¨ç†é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ‰©æ•£ç¼“å­˜çš„æ•´ä½“æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ç‰¹å¾æå–ï¼šä»æ‰©æ•£æ¨¡å‹çš„ä¸­é—´å±‚æå–ç‰¹å¾ï¼›2) ç¼“å­˜ç®¡ç†ï¼šå°†æå–çš„ç‰¹å¾å­˜å‚¨åœ¨ç¼“å­˜ä¸­ï¼Œå¹¶æ ¹æ®ä¸€å®šçš„ç­–ç•¥è¿›è¡Œæ›´æ–°å’Œæ›¿æ¢ï¼›3) ç‰¹å¾å¤ç”¨ï¼šåœ¨åç»­çš„æ‰©æ•£æ­¥éª¤ä¸­ï¼Œä»ç¼“å­˜ä¸­æ£€ç´¢ç›¸å…³çš„ç‰¹å¾ï¼Œå¹¶å°†å…¶ç”¨äºè®¡ç®—ï¼›4) èåˆï¼šå°†å¤ç”¨çš„ç‰¹å¾ä¸å½“å‰æ­¥éª¤çš„ç‰¹å¾è¿›è¡Œèåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥åˆ†æå’Œåˆ†ç±»ä¸åŒçš„æ‰©æ•£ç¼“å­˜æ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†æ‰©æ•£ç¼“å­˜ä»é™æ€å¤ç”¨åˆ°åŠ¨æ€é¢„æµ‹çš„æ¼”å˜è¶‹åŠ¿ã€‚é™æ€å¤ç”¨æ–¹æ³•ç®€å•ç›´æ¥ï¼Œä½†çµæ´»æ€§è¾ƒå·®ï¼›åŠ¨æ€é¢„æµ‹æ–¹æ³•åˆ™å¯ä»¥æ ¹æ®å½“å‰çš„çŠ¶æ€é¢„æµ‹éœ€è¦å¤ç”¨çš„ç‰¹å¾ï¼Œä»è€Œæé«˜ç¼“å­˜çš„åˆ©ç”¨ç‡å’Œç”Ÿæˆè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ç¼“å­˜çš„å®¹é‡ã€æ›´æ–°ç­–ç•¥ã€æ£€ç´¢æ–¹æ³•å’Œèåˆæ–¹å¼ç­‰ã€‚ç¼“å­˜å®¹é‡å†³å®šäº†å¯ä»¥å­˜å‚¨çš„ç‰¹å¾æ•°é‡ï¼Œæ›´æ–°ç­–ç•¥å†³å®šäº†ä½•æ—¶ä»¥åŠå¦‚ä½•æ›¿æ¢ç¼“å­˜ä¸­çš„ç‰¹å¾ï¼Œæ£€ç´¢æ–¹æ³•å†³å®šäº†å¦‚ä½•æ‰¾åˆ°ä¸å½“å‰çŠ¶æ€ç›¸å…³çš„ç‰¹å¾ï¼Œèåˆæ–¹å¼å†³å®šäº†å¦‚ä½•å°†å¤ç”¨çš„ç‰¹å¾ä¸å½“å‰æ­¥éª¤çš„ç‰¹å¾è¿›è¡Œç»“åˆã€‚è¿™äº›è®¾è®¡éƒ½éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å¯¹ä»£è¡¨æ€§æ–¹æ³•çš„æ¯”è¾ƒåˆ†æï¼Œå±•ç¤ºäº†æ‰©æ•£ç¼“å­˜ä»é™æ€é‡ç”¨åˆ°åŠ¨æ€é¢„æµ‹çš„æ¼”å˜è¶‹åŠ¿ã€‚è¿™ç§æ¼”å˜å¢å¼ºäº†ç¼“å­˜è·¨ä¸åŒä»»åŠ¡çš„çµæ´»æ€§ï¼Œå¹¶èƒ½å¤Ÿä¸å…¶ä»–åŠ é€ŸæŠ€æœ¯ï¼ˆå¦‚é‡‡æ ·ä¼˜åŒ–å’Œæ¨¡å‹è’¸é¦ï¼‰é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£ç¼“å­˜å¯ä»¥åœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜æ¨ç†é€Ÿåº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

æ‰©æ•£ç¼“å­˜æŠ€æœ¯å¯ä»¥å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦å®æ—¶ç”Ÿæˆå›¾åƒæˆ–è§†é¢‘çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šåœ¨çº¿æ¸¸æˆã€è™šæ‹Ÿç°å®ã€è§†é¢‘ä¼šè®®ã€å›¾åƒç¼–è¾‘ç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—æˆæœ¬å’Œæé«˜æ¨ç†é€Ÿåº¦ï¼Œå¯ä»¥ä½¿å¾—è¿™äº›åº”ç”¨æ›´åŠ æµç•…å’Œé«˜æ•ˆã€‚æ­¤å¤–ï¼Œæ‰©æ•£ç¼“å­˜è¿˜å¯ä»¥ä¸å…¶ä»–åŠ é€ŸæŠ€æœ¯ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜ç”Ÿæˆæ•ˆç‡ï¼Œä¸ºæœªæ¥çš„å¤šæ¨¡æ€å’Œäº¤äº’å¼åº”ç”¨æä¾›æ›´å¼ºå¤§çš„æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \textit{multi-step iterations} and \textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.
>   Against this backdrop, \textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.
>   Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \textit{Efficient Generative Intelligence}.

