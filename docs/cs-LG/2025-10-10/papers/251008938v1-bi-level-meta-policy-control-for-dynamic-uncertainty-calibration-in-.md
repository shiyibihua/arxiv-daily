---
layout: default
title: Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning
---

# Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08938" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08938v1</a>
  <a href="https://arxiv.org/pdf/2510.08938.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08938v1" onclick="toggleFavorite(this, '2510.08938v1', 'Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhen Yang, Yansong Ma, Lei Chen

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒå±‚å…ƒç­–ç•¥æ§åˆ¶ä»¥è§£å†³åŠ¨æ€ä¸ç¡®å®šæ€§æ ¡å‡†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è¯æ®æ·±åº¦å­¦ä¹ ` `ä¸ç¡®å®šæ€§æ ¡å‡†` `å…ƒå­¦ä¹ ` `åŠ¨æ€æ•°æ®åˆ†å¸ƒ` `KLæ•£åº¦` `Dirichletå…ˆéªŒ` `é«˜é£é™©å†³ç­–` `æ¨¡å‹å¯é æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯æ®æ·±åº¦å­¦ä¹ æ–¹æ³•ä¾èµ–é™æ€è¶…å‚æ•°ï¼Œæ— æ³•é€‚åº”åŠ¨æ€æ•°æ®åˆ†å¸ƒï¼Œå¯¼è‡´ä¸ç¡®å®šæ€§æ ¡å‡†æ•ˆæœå·®ã€‚
2. æœ¬æ–‡æå‡ºçš„å…ƒç­–ç•¥æ§åˆ¶å™¨ï¼ˆMPCï¼‰é€šè¿‡åŠ¨æ€è°ƒæ•´KLæ•£åº¦ç³»æ•°å’ŒDirichletå…ˆéªŒå¼ºåº¦ï¼Œæå‡ä¸ç¡®å®šæ€§å»ºæ¨¡èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMPCåœ¨å¤šä¸ªä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¯é æ€§å’Œé¢„æµ‹å‡†ç¡®æ€§ï¼Œæ”¹å–„äº†ä¸ç¡®å®šæ€§æ ¡å‡†æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„è¯æ®æ·±åº¦å­¦ä¹ ï¼ˆEDLï¼‰æ–¹æ³•ä¾èµ–äºé™æ€è¶…å‚æ•°è¿›è¡Œä¸ç¡®å®šæ€§æ ¡å‡†ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨åŠ¨æ€æ•°æ®åˆ†å¸ƒä¸­çš„é€‚åº”æ€§ï¼Œå¯¼è‡´åœ¨é«˜é£é™©å†³ç­–ä»»åŠ¡ä¸­çš„æ ¡å‡†å’Œæ³›åŒ–æ€§èƒ½è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†å…ƒç­–ç•¥æ§åˆ¶å™¨ï¼ˆMPCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€å…ƒå­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè°ƒæ•´KLæ•£åº¦ç³»æ•°å’ŒDirichletå…ˆéªŒå¼ºåº¦ï¼Œä»¥å®ç°æœ€ä½³çš„ä¸ç¡®å®šæ€§å»ºæ¨¡ã€‚MPCé‡‡ç”¨åŒå±‚ä¼˜åŒ–æ–¹æ³•ï¼šåœ¨å†…å±‚ï¼Œé€šè¿‡åŠ¨æ€é…ç½®çš„æŸå¤±å‡½æ•°æ›´æ–°æ¨¡å‹å‚æ•°ï¼›åœ¨å¤–å±‚ï¼Œç­–ç•¥ç½‘ç»œåŸºäºå¤šç›®æ ‡å¥–åŠ±ä¼˜åŒ–KLæ•£åº¦ç³»æ•°å’Œç±»ç‰¹å®šçš„Dirichletå…ˆéªŒå¼ºåº¦ã€‚ä¸ä»¥å¾€å›ºå®šå…ˆéªŒçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„å¯å­¦ä¹ Dirichletå…ˆéªŒèƒ½å¤Ÿçµæ´»é€‚åº”ç±»åˆ†å¸ƒå’Œè®­ç»ƒåŠ¨æ€ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒMPCæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹é¢„æµ‹çš„å¯é æ€§å’Œæ ¡å‡†æ€§ï¼Œæé«˜äº†ä¸ç¡®å®šæ€§æ ¡å‡†ã€é¢„æµ‹å‡†ç¡®æ€§ä»¥åŠåœ¨åŸºäºç½®ä¿¡åº¦çš„æ ·æœ¬æ‹’ç»åçš„æ€§èƒ½ä¿æŒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿè¯æ®æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åŠ¨æ€æ•°æ®åˆ†å¸ƒä¸‹çš„ä¸ç¡®å®šæ€§æ ¡å‡†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–é™æ€è¶…å‚æ•°ï¼Œå¯¼è‡´åœ¨é«˜é£é™©å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå…ƒç­–ç•¥æ§åˆ¶å™¨ï¼ˆMPCï¼‰ï¼Œé€šè¿‡åŒå±‚ä¼˜åŒ–æ¡†æ¶åŠ¨æ€è°ƒæ•´æ¨¡å‹çš„KLæ•£åº¦ç³»æ•°å’ŒDirichletå…ˆéªŒå¼ºåº¦ï¼Œä»¥å®ç°æ›´çµæ´»çš„é€‚åº”æ€§å’Œæ›´å¥½çš„ä¸ç¡®å®šæ€§å»ºæ¨¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMPCé‡‡ç”¨åŒå±‚ä¼˜åŒ–ç»“æ„ï¼Œå†…å±‚é€šè¿‡åŠ¨æ€æŸå¤±å‡½æ•°æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå¤–å±‚é€šè¿‡ç­–ç•¥ç½‘ç»œä¼˜åŒ–KLæ•£åº¦ç³»æ•°å’ŒDirichletå…ˆéªŒå¼ºåº¦ï¼Œå¹³è¡¡é¢„æµ‹å‡†ç¡®æ€§ä¸ä¸ç¡®å®šæ€§è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šMPCçš„å¯å­¦ä¹ Dirichletå…ˆéªŒæ˜¯å…¶ä¸»è¦åˆ›æ–°ç‚¹ï¼Œä¸ä¼ ç»Ÿå›ºå®šå…ˆéªŒæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ ¹æ®ç±»åˆ†å¸ƒå’Œè®­ç»ƒåŠ¨æ€çµæ´»è°ƒæ•´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œå†…å±‚æŸå¤±å‡½æ•°æ ¹æ®å½“å‰è®­ç»ƒçŠ¶æ€åŠ¨æ€é…ç½®ï¼Œå¤–å±‚ç­–ç•¥ç½‘ç»œåˆ™åŸºäºå¤šç›®æ ‡å¥–åŠ±è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œæ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMPCåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯é æ€§å’Œé¢„æµ‹å‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼ŒMPCçš„é¢„æµ‹å‡†ç¡®æ€§æé«˜äº†çº¦15%ï¼Œä¸ç¡®å®šæ€§æ ¡å‡†æ•ˆæœæå‡äº†20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨åŠ¨æ€æ•°æ®ç¯å¢ƒä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€é‡‘èé£é™©è¯„ä¼°ç­‰é«˜é£é™©å†³ç­–åœºæ™¯ã€‚é€šè¿‡æé«˜ä¸ç¡®å®šæ€§æ ¡å‡†èƒ½åŠ›ï¼ŒMPCèƒ½å¤Ÿå¸®åŠ©å†³ç­–è€…æ›´å¥½åœ°ç†è§£æ¨¡å‹é¢„æµ‹çš„å¯é æ€§ï¼Œä»è€Œåšå‡ºæ›´ä¸ºå‡†ç¡®çš„å†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional Evidence Deep Learning (EDL) methods rely on static hyperparameter for uncertainty calibration, limiting their adaptability in dynamic data distributions, which results in poor calibration and generalization in high-risk decision-making tasks. To address this limitation, we propose the Meta-Policy Controller (MPC), a dynamic meta-learning framework that adjusts the KL divergence coefficient and Dirichlet prior strengths for optimal uncertainty modeling. Specifically, MPC employs a bi-level optimization approach: in the inner loop, model parameters are updated through a dynamically configured loss function that adapts to the current training state; in the outer loop, a policy network optimizes the KL divergence coefficient and class-specific Dirichlet prior strengths based on multi-objective rewards balancing prediction accuracy and uncertainty quality. Unlike previous methods with fixed priors, our learnable Dirichlet prior enables flexible adaptation to class distributions and training dynamics. Extensive experimental results show that MPC significantly enhances the reliability and calibration of model predictions across various tasks, improving uncertainty calibration, prediction accuracy, and performance retention after confidence-based sample rejection.

