---
layout: default
title: Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment
---

# Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07750" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07750v1</a>
  <a href="https://arxiv.org/pdf/2508.07750.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07750v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07750v1', 'Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haowen Wang, Yun Yue, Zhiling Ye, Shuowen Zhang, Lei Fan, Jiaxin Liang, Jiadi Jiang, Cheng Wei, Jingyuan Deng, Xudong Han, Ji Li, Chunxiao Guo, Peng Wei, Jian Wang, Jinjie Gu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: 12 pages, 5 figures, 7 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRAOæ¡†æ¶ä»¥è§£å†³è¯­è¨€æ¨¡å‹å¯¹é½æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `å¯¹é½æ–¹æ³•` `å¼ºåŒ–å­¦ä¹ ` `ç›‘ç£å¾®è°ƒ` `æ ·æœ¬æ•ˆç‡` `å¤šæ ·æœ¬ç”Ÿæˆ` `å‚æ•°æ›´æ–°` `å¯¹é½æŸå¤±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è¯­è¨€æ¨¡å‹å¯¹é½ä¸­å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œæ ·æœ¬åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºGRAOæ¡†æ¶ï¼Œé€šè¿‡å¤šæ ·æœ¬ç”Ÿæˆã€ç»„å†…ç›¸å¯¹ä¼˜åŠ¿åŠ æƒå’Œå‚è€ƒæ„ŸçŸ¥å‚æ•°æ›´æ–°ç­‰ç­–ç•¥ï¼Œæå‡å¯¹é½æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGRAOåœ¨å¤šé¡¹å¤æ‚ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡å¹…åº¦å¯è¾¾57.70%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹é½æ–¹æ³•å·²æˆä¸ºå¢å¼ºè¯­è¨€æ¨¡å‹å¯¹é½èƒ½åŠ›çš„é‡è¦é€”å¾„ã€‚ç°æœ‰çš„ç›‘ç£å¾®è°ƒ(SFT)æ–¹æ³•é€šè¿‡ç›´æ¥çš„æ ‡è®°çº§æŸå¤±å¹²é¢„åŠ é€Ÿæ”¶æ•›ï¼Œä½†å…¶æ•ˆæœå—é™äºç¦»çº¿ç­–ç•¥è½¨è¿¹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ (RL)è™½ç„¶èƒ½ä¿ƒè¿›æ¢ç´¢æ€§ç­–ç•¥ä¼˜åŒ–ï¼Œä½†æ ·æœ¬æ•ˆç‡ä½ä¸”ä¾èµ–é«˜è´¨é‡åŸºç¡€æ¨¡å‹ã€‚ä¸ºè§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†GRAOï¼ˆGroup Relative Alignment Optimizationï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸‰é¡¹å…³é”®åˆ›æ–°å®ç°SFTä¸RLçš„ä¼˜åŠ¿äº’è¡¥ã€‚ç†è®ºåˆ†æè¯æ˜äº†GRAOåœ¨æ”¶æ•›æ€§å’Œæ ·æœ¬æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒGRAOåœ¨å¤æ‚çš„äººç±»å¯¹é½ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºSFTã€DPOã€PPOå’ŒGRPOåŸºçº¿åˆ†åˆ«æå‡äº†57.70%ã€17.65%ã€7.95%å’Œ5.18%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œæ”¶æ•›æ€§ä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGRAOæ¡†æ¶é€šè¿‡ç»“åˆSFTå’ŒRLçš„ä¼˜ç‚¹ï¼Œé‡‡ç”¨å¤šæ ·æœ¬ç”Ÿæˆç­–ç•¥å’Œç»„å†…ç›¸å¯¹ä¼˜åŠ¿åŠ æƒï¼Œæå‡å¯¹é½çš„è´¨é‡å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRAOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šæ ·æœ¬ç”Ÿæˆã€å¥–åŠ±åé¦ˆæœºåˆ¶ã€ç»„ç›´æ¥å¯¹é½æŸå¤±è®¡ç®—å’Œå‚è€ƒæ„ŸçŸ¥å‚æ•°æ›´æ–°ç­‰æ¨¡å—ï¼Œå½¢æˆé—­ç¯ä¼˜åŒ–æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šGRAOçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥ç»„å†…ç›¸å¯¹ä¼˜åŠ¿åŠ æƒçš„æŸå¤±å‡½æ•°å’ŒåŸºäºåå¥½åŠ¨æ€çš„å‚æ•°æ›´æ–°ç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€æŸå¤±è®¡ç®—æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼ŒGRAOé‡‡ç”¨äº†ç»„ç›´æ¥å¯¹é½æŸå¤±ï¼Œåˆ©ç”¨ç»„å†…æ ·æœ¬çš„ç›¸å¯¹ä¼˜åŠ¿è¿›è¡ŒåŠ æƒï¼›å‚æ•°æ›´æ–°åˆ™åŸºäºæˆå¯¹åå¥½çš„åŠ¨æ€åé¦ˆè¿›è¡Œè°ƒæ•´ï¼Œç¡®ä¿æ¨¡å‹å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRAOåœ¨å¤æ‚äººç±»å¯¹é½ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºSFTã€DPOã€PPOå’ŒGRPOåŸºçº¿åˆ†åˆ«æå‡äº†57.70%ã€17.65%ã€7.95%å’Œ5.18%ã€‚è¿™äº›ç»“æœè¡¨æ˜GRAOåœ¨æ ·æœ¬æ•ˆç‡å’Œå¯¹é½è´¨é‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ï¼ŒGRAOæ¡†æ¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ï¼Œè¿›è€Œæé«˜æ™ºèƒ½åŠ©æ‰‹ã€èŠå¤©æœºå™¨äººç­‰åº”ç”¨çš„ç”¨æˆ·ä½“éªŒå’Œäº¤äº’è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Alignment methodologies have emerged as a critical pathway for enhancing language model alignment capabilities. While SFT (supervised fine-tuning) accelerates convergence through direct token-level loss intervention, its efficacy is constrained by offline policy trajectory. In contrast, RL(reinforcement learning) facilitates exploratory policy optimization, but suffers from low sample efficiency and stringent dependency on high-quality base models. To address these dual challenges, we propose GRAO (Group Relative Alignment Optimization), a unified framework that synergizes the respective strengths of SFT and RL through three key innovations: 1) A multi-sample generation strategy enabling comparative quality assessment via reward feedback; 2) A novel Group Direct Alignment Loss formulation leveraging intra-group relative advantage weighting; 3) Reference-aware parameter updates guided by pairwise preference dynamics. Our theoretical analysis establishes GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. Comprehensive evaluations across complex human alignment tasks demonstrate GRAO's superior performance, achieving 57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and GRPO baselines respectively. This work provides both a theoretically grounded alignment framework and empirical evidence for efficient capability evolution in language models.

