---
layout: default
title: WeChat-YATT: A Scalable, Simple, Efficient, and Production Ready Training Library
---

# WeChat-YATT: A Scalable, Simple, Efficient, and Production Ready Training Library

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07970" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.07970v3</a>
  <a href="https://arxiv.org/pdf/2508.07970.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07970v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07970v3', 'WeChat-YATT: A Scalable, Simple, Efficient, and Production Ready Training Library')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Junyu Wu, Weiming Chang, Xiaotao Liu, Guanyou He, Tingfeng Xian, Haoqiang Hong, Boqi Chen, Hongtao Tian, Tao Yang, Yunsheng Shi, Feng Lin, Ting Yao, Jiatao Xu

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-11 (Êõ¥Êñ∞: 2025-08-18)

**Â§áÊ≥®**: arXiv admin note: substantial text overlap with arXiv:2507.22789

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫WeChat-YATT‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅRLHFËÆ≠ÁªÉÁöÑÂèØÊâ©Â±ïÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†` `Â§öÊ®°ÊÄÅÁ≥ªÁªü` `ËÆ≠ÁªÉÊ°ÜÊû∂` `ËµÑÊ∫êÂàÜÈÖç` `ÂèØÊâ©Â±ïÊÄß` `GPUÂà©Áî®Áéá` `WeChat‰∫ßÂìÅ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâRLHFËÆ≠ÁªÉÊ°ÜÊû∂Âú®Â§ÑÁêÜÂ§çÊùÇÂ§öÊ®°ÊÄÅÂ∑•‰ΩúÊµÅÂíåÂä®ÊÄÅÂ∑•‰ΩúË¥üËΩΩÊó∂Èù¢‰∏¥ÂèØÊâ©Â±ïÊÄßÂíåÊïàÁéáÁöÑÊåëÊàò„ÄÇ
2. WeChat-YATTÈÄöËøáÂπ∂Ë°åÊéßÂà∂Âô®ÁºñÁ®ãÊ®°ÂûãÂíåÂä®ÊÄÅËµÑÊ∫êÂàÜÈÖçÊñπÊ°àÔºåÊèê‰æõÁÅµÊ¥ªÈ´òÊïàÁöÑRLHFËÆ≠ÁªÉËß£ÂÜ≥ÊñπÊ°à„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWeChat-YATTÂú®ÂêûÂêêÈáè‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊ°ÜÊû∂Ôºå‰∏îÂ∑≤ÊàêÂäüÂ∫îÁî®‰∫éWeChatÁöÑÂÆûÈôÖ‰∫ßÂìÅ‰∏≠„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†ÔºàRLHFÔºâÂ∑≤Êàê‰∏∫ËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§öÊ®°ÊÄÅÁ≥ªÁªüÁöÑÈáçË¶ÅËåÉÂºè„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâRLHFËÆ≠ÁªÉÊ°ÜÊû∂ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®Â§ÑÁêÜÂ§çÊùÇÂ§öÊ®°ÊÄÅÂ∑•‰ΩúÊµÅÂíåÂä®ÊÄÅÂ∑•‰ΩúË¥üËΩΩÊó∂‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÂΩìÂâçÁ≥ªÁªüÂú®ÁÆ°ÁêÜÂ§ßÂûãÊ®°ÂûãÊó∂Â∏∏ÈÅáÂà∞ÊéßÂà∂Âô®ÂèØÊâ©Â±ïÊÄßÈôêÂà∂Ôºå‰ª•ÂèäÂú®Â§çÊùÇRLHFÁÆ°ÈÅìÁöÑÂçèË∞É‰∏äÊïàÁéá‰Ωé‰∏ã„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜWeChat-YATTÔºàYet Another Transformer TrainerÔºâÔºåËøôÊòØ‰∏Ä‰∏™ÁÆÄÂçï„ÄÅÂèØÊâ©Â±ï‰∏îÂπ≥Ë°°ÁöÑRLHFËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇWeChat-YATTÈááÁî®Âπ∂Ë°åÊéßÂà∂Âô®ÁºñÁ®ãÊ®°ÂûãÔºåÁÅµÊ¥ªÈ´òÊïàÂú∞ÂçèË∞ÉÂ§çÊùÇÁöÑRLHFÂ∑•‰ΩúÊµÅÔºåÁºìËß£‰∫ÜÈõÜ‰∏≠ÊéßÂà∂Âô®Êû∂ÊûÑÁöÑÁì∂È¢àÔºåÂπ∂Âú®Â§ßËßÑÊ®°Êï∞ÊçÆÂú∫ÊôØ‰∏≠‰øÉËøõ‰∫ÜÂèØÊâ©Â±ïÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂä®ÊÄÅËµÑÊ∫êÂàÜÈÖçÊñπÊ°àÔºåËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞ÂàíÂàÜËÆ°ÁÆóËµÑÊ∫êÂíåË∞ÉÂ∫¶Â∑•‰ΩúË¥üËΩΩÔºå‰ªéËÄåÊòæËëóÂáèÂ∞ëÁ°¨‰ª∂Èó≤ÁΩÆÊó∂Èó¥ÔºåÊèêÈ´òGPUÂà©Áî®Áéá„ÄÇÈÄöËøáÂ§öÁßçÂÆûÈ™åÂú∫ÊôØÁöÑËØÑ‰º∞ÔºåWeChat-YATTÂú®ÂêûÂêêÈáè‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑRLHFËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂπ∂ÊàêÂäüÂ∫îÁî®‰∫éÊîØÊåÅWeChat‰∫ßÂìÅÂäüËÉΩÁöÑÊ®°ÂûãËÆ≠ÁªÉÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâRLHFËÆ≠ÁªÉÊ°ÜÊû∂Âú®Â§ÑÁêÜÂ§çÊùÇÂ§öÊ®°ÊÄÅÂ∑•‰ΩúÊµÅÊó∂ÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊïàÁéáÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®Âä®ÊÄÅÈááÊ†∑ÂíåËµÑÊ∫êÂàÜÈÖçÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöWeChat-YATTÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈááÁî®Âπ∂Ë°åÊéßÂà∂Âô®ÁºñÁ®ãÊ®°ÂûãÔºå‰ª•ÂÆûÁé∞ÁÅµÊ¥ªÁöÑÂ∑•‰ΩúÊµÅÂçèË∞ÉÔºåÂπ∂ÈÄöËøáÂä®ÊÄÅËµÑÊ∫êÂàÜÈÖçÊù•‰ºòÂåñËÆ°ÁÆóËµÑÊ∫êÁöÑÂà©Áî®Áéá„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°ËÉΩÂ§üÊúâÊïàÁºìËß£ÈõÜ‰∏≠ÊéßÂà∂Âô®Êû∂ÊûÑÂ∏¶Êù•ÁöÑÁì∂È¢à„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöWeChat-YATTÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Â§ö‰∏™Ê®°ÂùóÔºöÂπ∂Ë°åÊéßÂà∂Âô®„ÄÅÂä®ÊÄÅËµÑÊ∫êÂàÜÈÖçÊ®°ÂùóÂíåRLHFÂ∑•‰ΩúÊµÅÁÆ°ÁêÜÊ®°Âùó„ÄÇÂπ∂Ë°åÊéßÂà∂Âô®Ë¥üË¥£ÂçèË∞ÉÂêÑ‰∏™Â≠ê‰ªªÂä°ÁöÑÊâßË°åÔºåËÄåÂä®ÊÄÅËµÑÊ∫êÂàÜÈÖçÊ®°ÂùóÂàôÊ†πÊçÆÂÆûÊó∂ÈúÄÊ±ÇË∞ÉÊï¥ËÆ°ÁÆóËµÑÊ∫êÁöÑÂàÜÈÖç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöWeChat-YATTÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Âπ∂Ë°åÊéßÂà∂Âô®ÁºñÁ®ãÊ®°ÂûãÂíåÂä®ÊÄÅËµÑÊ∫êÂàÜÈÖçÊñπÊ°àÔºåËøô‰∏é‰º†ÁªüÁöÑÈõÜ‰∏≠ÊéßÂà∂Âô®Êû∂ÊûÑÂΩ¢Êàê‰∫ÜÈ≤úÊòéÂØπÊØîÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁ≥ªÁªüÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÖ≥ÈîÆËÆæËÆ°ÊñπÈù¢ÔºåWeChat-YATTÈááÁî®‰∫ÜËá™ÈÄÇÂ∫îÁöÑËÆ°ÁÆóËµÑÊ∫êÂàíÂàÜÁ≠ñÁï•ÔºåÂπ∂‰ºòÂåñ‰∫ÜGPUÂà©Áî®ÁéáÔºåÂáèÂ∞ë‰∫ÜÁ°¨‰ª∂Èó≤ÁΩÆÊó∂Èó¥„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Â∞öÊú™ËØ¶ÁªÜÊä´Èú≤ÔºåÂ±û‰∫éÊú™Áü•È¢ÜÂüü„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåWeChat-YATTÂú®Â§ö‰∏™Âú∫ÊôØ‰∏ãÁöÑÂêûÂêêÈáèÊòæËëóÊèêÈ´òÔºåÁõ∏ËæÉ‰∫éÊúÄÂÖàËøõÁöÑRLHFËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÂæÖË°•ÂÖÖÔºâ„ÄÇËøô‰∏ÄÊàêÊûúÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

WeChat-YATTÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§öÊ®°ÊÄÅÁ≥ªÁªüÁöÑËÆ≠ÁªÉÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÂä®ÊÄÅË∞ÉÊï¥ËµÑÊ∫êÂíåÈ´òÊïàÂ§ÑÁêÜÂ§çÊùÇÂ∑•‰ΩúÊµÅÁöÑÂú∫ÊôØ‰∏≠„ÄÇÂÖ∂ÂÆûÈôÖ‰ª∑ÂÄºÂú®‰∫éÊèêÂçá‰∫ÜËÆ≠ÁªÉÊïàÁéáÂíåÊ®°ÂûãÊÄßËÉΩÔºåÊú™Êù•ÂèØËÉΩÂØπÂ§öÁßçAIÂ∫îÁî®‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent paradigm for training large language models and multimodal systems. Despite the notable advances enabled by existing RLHF training frameworks, significant challenges remain to scale to complex multimodal workflows and adapt to dynamic workloads. In particular, current systems often encounter limitations related to controller scalability when managing large models, as well as inefficiencies in orchestrating intricate RLHF pipelines, especially in scenarios that require dynamic sampling and resource allocation. In this paper, we introduce WeChat-YATT Yet Another Transformer Trainer in WeChat, a simple, scalable, and balanced RLHF training framework specifically designed to address these challenges. WeChat-YATT features a parallel controller programming model that enables flexible and efficient orchestration of complex RLHF workflows, effectively mitigating bottlenecks associated with centralized controller architectures and facilitating scalability in large-scale data scenarios. In addition, we propose a dynamic placement schema that adaptively partitions computational resources and schedules workloads, thereby significantly reducing hardware idle time and improving GPU utilization under variable training conditions. We evaluate WeChat-YATT across diverse experimental scenarios, demonstrating its substantial throughput improvements over state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been successfully deployed to train models that support WeChat product features for a large-scale user base, underscoring its effectiveness and robustness in real-world applications. We have made WeChat-YATT publicly available at https://www.github.com/tencent/WeChat-YATT.

