---
layout: default
title: On Understanding of the Dynamics of Model Capacity in Continual Learning
---

# On Understanding of the Dynamics of Model Capacity in Continual Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08052" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08052v2</a>
  <a href="https://arxiv.org/pdf/2508.08052.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08052v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08052v2', 'On Understanding of the Dynamics of Model Capacity in Continual Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Supriyo Chakraborty, Krishnan Raghavan

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-08-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœ‰æ•ˆæ¨¡å‹å®¹é‡ä»¥è§£å†³æŒç»­å­¦ä¹ ä¸­çš„ç¨³å®šæ€§ä¸å¯å¡‘æ€§å›°å¢ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `æ¨¡å‹å®¹é‡` `ç¨³å®šæ€§-å¯å¡‘æ€§` `ç¥ç»ç½‘ç»œ` `ä»»åŠ¡è¡¨ç¤º` `åŠ¨æ€å­¦ä¹ ` `ä¼˜åŒ–è¿‡ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŒç»­å­¦ä¹ ä¸­çš„ç¨³å®šæ€§ä¸å¯å¡‘æ€§å¹³è¡¡æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶è¡¨ç°å‡ºèƒ½åŠ›ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æœ‰æ•ˆæ¨¡å‹å®¹é‡ï¼ˆCLEMCï¼‰ï¼Œé€šè¿‡å·®åˆ†æ–¹ç¨‹æè¿°ç¥ç»ç½‘ç»œä¸ä»»åŠ¡æ•°æ®ä¹‹é—´çš„åŠ¨æ€å…³ç³»ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šé€šè¿‡å¤šç§ç½‘ç»œæ¶æ„çš„å®éªŒï¼ŒéªŒè¯äº†æœ‰æ•ˆæ¨¡å‹å®¹é‡çš„éå¹³ç¨³æ€§åŠå…¶å¯¹ä»»åŠ¡è¡¨ç¤ºèƒ½åŠ›çš„å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨³å®šæ€§-å¯å¡‘æ€§å›°å¢ƒæ˜¯æŒç»­å­¦ä¹ ä¸­çš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œæ¶‰åŠç¥ç»ç½‘ç»œçš„æ¨¡å‹å®¹é‡åŠå…¶ä»»åŠ¡è¡¨ç¤ºèƒ½åŠ›ã€‚æœ¬æ–‡å¼•å…¥äº†æœ‰æ•ˆæ¨¡å‹å®¹é‡ï¼ˆCLEMCï¼‰ï¼Œç”¨äºè¡¨å¾ç¨³å®šæ€§-å¯å¡‘æ€§å¹³è¡¡ç‚¹çš„åŠ¨æ€è¡Œä¸ºã€‚é€šè¿‡å»ºç«‹å·®åˆ†æ–¹ç¨‹ï¼Œæ¨¡å‹åŒ–ç¥ç»ç½‘ç»œã€ä»»åŠ¡æ•°æ®å’Œä¼˜åŒ–è¿‡ç¨‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ— è®ºç¥ç»ç½‘ç»œçš„æ¶æ„æˆ–ä¼˜åŒ–æ–¹æ³•å¦‚ä½•ï¼Œå½“æ–°ä»»åŠ¡åˆ†å¸ƒä¸å…ˆå‰ä»»åŠ¡ä¸åŒæ—¶ï¼Œç¥ç»ç½‘ç»œè¡¨ç¤ºæ–°ä»»åŠ¡çš„èƒ½åŠ›ä¼šå‡å¼±ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯äº†ç†è®ºå‘ç°ï¼Œæ¶µç›–äº†ä»å°å‹å‰é¦ˆç½‘ç»œåˆ°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šç§æ¶æ„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æŒç»­å­¦ä¹ ä¸­çš„ç¨³å®šæ€§-å¯å¡‘æ€§å›°å¢ƒï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å¯¹æ–°ä»»åŠ¡æ—¶ï¼Œç¥ç»ç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›å¾€å¾€ä¼šä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨æ–°ä»»åŠ¡åˆ†å¸ƒä¸æ—§ä»»åŠ¡åˆ†å¸ƒä¸ä¸€è‡´æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæœ‰æ•ˆæ¨¡å‹å®¹é‡ï¼ˆCLEMCï¼‰ï¼Œç”¨äºåŠ¨æ€è¡¨å¾ç¨³å®šæ€§-å¯å¡‘æ€§å¹³è¡¡ç‚¹çš„å˜åŒ–ã€‚é€šè¿‡å»ºç«‹å·®åˆ†æ–¹ç¨‹ï¼Œåˆ†æç¥ç»ç½‘ç»œã€ä»»åŠ¡æ•°æ®å’Œä¼˜åŒ–è¿‡ç¨‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹å®¹é‡çš„åŠ¨æ€ç‰¹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç¥ç»ç½‘ç»œæ¨¡å‹ã€ä»»åŠ¡æ•°æ®è¾“å…¥å’Œä¼˜åŒ–è¿‡ç¨‹ã€‚é€šè¿‡å·®åˆ†æ–¹ç¨‹æè¿°è¿™ä¸‰è€…ä¹‹é—´çš„åŠ¨æ€å…³ç³»ï¼Œè¿›è€Œåˆ†ææ¨¡å‹å®¹é‡çš„å˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†CLEMCè¿™ä¸€æ¦‚å¿µï¼Œå¼ºè°ƒäº†æ¨¡å‹å®¹é‡çš„éå¹³ç¨³æ€§ï¼Œæ­ç¤ºäº†ä¸åŒä»»åŠ¡åˆ†å¸ƒå¯¹ç¥ç»ç½‘ç»œè¡¨ç¤ºèƒ½åŠ›çš„å½±å“ï¼Œè¿™ä¸ç°æœ‰é™æ€æ¨¡å‹å®¹é‡çš„ç†è§£æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§ç¥ç»ç½‘ç»œæ¶æ„ï¼ŒåŒ…æ‹¬å°å‹å‰é¦ˆç½‘ç»œã€å·ç§¯ç½‘ç»œã€ä¸­å‹å›¾ç¥ç»ç½‘ç»œå’ŒåŸºäºå˜æ¢å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç¡®ä¿äº†ç ”ç©¶ç»“æœçš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æœ‰æ•ˆæ¨¡å‹å®¹é‡ï¼ˆCLEMCï¼‰èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ç¥ç»ç½‘ç»œåœ¨é¢å¯¹ä¸åŒä»»åŠ¡åˆ†å¸ƒæ—¶çš„èƒ½åŠ›å˜åŒ–ã€‚æ— è®ºæ˜¯å°å‹ç½‘ç»œè¿˜æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‡æ˜¾ç¤ºå‡ºåœ¨æ–°ä»»åŠ¡åˆ†å¸ƒä¸‹ï¼Œè¡¨ç¤ºèƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼ŒéªŒè¯äº†ç†è®ºæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯¹æŒç»­å­¦ä¹ é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†ä¸æ–­å˜åŒ–ä»»åŠ¡åˆ†å¸ƒçš„åœºæ™¯ä¸­ï¼Œå¦‚æœºå™¨äººå­¦ä¹ ã€è‡ªåŠ¨é©¾é©¶å’Œä¸ªæ€§åŒ–æ¨èç³»ç»Ÿç­‰ã€‚æœ‰æ•ˆæ¨¡å‹å®¹é‡çš„æ¦‚å¿µå¯ä»¥å¸®åŠ©è®¾è®¡æ›´å…·é€‚åº”æ€§çš„å­¦ä¹ ç®—æ³•ï¼Œæé«˜æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The stability-plasticity dilemma, closely related to a neural network's (NN) capacity-its ability to represent tasks-is a fundamental challenge in continual learning (CL). Within this context, we introduce CL's effective model capacity (CLEMC) that characterizes the dynamic behavior of the stability-plasticity balance point. We develop a difference equation to model the evolution of the interplay between the NN, task data, and optimization procedure. We then leverage CLEMC to demonstrate that the effective capacity-and, by extension, the stability-plasticity balance point is inherently non-stationary. We show that regardless of the NN architecture or optimization method, a NN's ability to represent new tasks diminishes when incoming task distributions differ from previous ones. We conduct extensive experiments to support our theoretical findings, spanning a range of architectures-from small feedforward network and convolutional networks to medium-sized graph neural networks and transformer-based large language models with millions of parameters.

