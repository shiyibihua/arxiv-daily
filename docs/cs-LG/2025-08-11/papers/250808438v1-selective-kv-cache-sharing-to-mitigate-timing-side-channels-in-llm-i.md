---
layout: default
title: Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference
---

# Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08438" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08438v1</a>
  <a href="https://arxiv.org/pdf/2508.08438.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08438v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08438v1', 'Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kexin Chu, Zecheng Lin, Dawei Xiang, Zixu Shen, Jianchang Su, Cheng Chu, Yiwei Yang, Wenhui Zhang, Wenfei Wu, Wei Zhang

**åˆ†ç±»**: cs.CR, cs.LG, cs.OS

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: 17 pages,17 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSafeKVä»¥è§£å†³LLMæ¨ç†ä¸­çš„æ—¶åºä¾§ä¿¡é“æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `KVç¼“å­˜å…±äº«` `æ—¶åºä¾§ä¿¡é“æ”»å‡»` `éšç§ä¿æŠ¤` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€§èƒ½ä¼˜åŒ–` `ä¿¡æ¯æ³„éœ²é˜²æŠ¤` `é«˜ååé‡éƒ¨ç½²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å…¨çƒKVç¼“å­˜å…±äº«æ–¹æ³•åœ¨åŠ é€ŸLLMæ¨ç†çš„åŒæ—¶ï¼Œæš´éœ²äº†æ—¶åºä¾§ä¿¡é“æ”»å‡»çš„é£é™©ï¼Œå¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚
2. æœ¬æ–‡æå‡ºSafeKVæ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§å…±äº«éæ•æ„Ÿç¼“å­˜æ¡ç›®ï¼Œä¿æŠ¤æ•æ„Ÿä¿¡æ¯ï¼ŒåŒæ—¶æå‡æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSafeKVèƒ½æœ‰æ•ˆå‡è½»94%-97%çš„æ”»å‡»ï¼ŒTTFTæå‡40.58%ï¼Œååé‡æé«˜è‡³2.66å€ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…¨çƒKVç¼“å­˜å…±äº«å·²æˆä¸ºåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å…³é”®ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æš´éœ²äº†æ–°çš„æ—¶åºä¾§ä¿¡é“æ”»å‡»é£é™©ï¼Œä½¿å¾—æ”»å‡»è€…èƒ½å¤Ÿé€šè¿‡å…±äº«ç¼“å­˜æ¡ç›®æ¨æ–­æ•æ„Ÿç”¨æˆ·è¾“å…¥ã€‚ç°æœ‰çš„é˜²å¾¡æªæ–½å¦‚ç”¨æˆ·éš”ç¦»è™½ç„¶èƒ½æ¶ˆé™¤æ³„æ¼ï¼Œä½†åœ¨é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰ä¸Šæ€§èƒ½ä¸‹é™é«˜è¾¾38.9%ï¼Œä¸é€‚åˆé«˜ååé‡éƒ¨ç½²ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SafeKVï¼ˆå®‰å…¨çµæ´»çš„KVç¼“å­˜å…±äº«ï¼‰ï¼Œä¸€ä¸ªéšç§æ„è¯†çš„KVç¼“å­˜ç®¡ç†æ¡†æ¶ï¼Œé€‰æ‹©æ€§åœ°å…±äº«éæ•æ„Ÿæ¡ç›®ï¼ŒåŒæ—¶å°†æ•æ„Ÿå†…å®¹é™åˆ¶åœ¨ç§æœ‰ç¼“å­˜ä¸­ã€‚SafeKVé€šè¿‡ä¸‰å¤§ç»„ä»¶å®ç°å…¶ç›®æ ‡ï¼šæ··åˆå¤šå±‚æ£€æµ‹ç®¡é“ã€ç»Ÿä¸€çš„åŸºæ•°æ ‘ç´¢å¼•å’ŒåŸºäºç†µçš„è®¿é—®ç›‘æ§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒSafeKVèƒ½å‡è½»94%-97%çš„æ—¶åºä¾§ä¿¡é“æ”»å‡»ï¼Œç›¸æ¯”ç”¨æˆ·éš”ç¦»æ–¹æ³•ï¼ŒTTFTæå‡é«˜è¾¾40.58%ï¼Œååé‡æé«˜è‡³2.66å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„æ—¶åºä¾§ä¿¡é“æ”»å‡»é—®é¢˜ï¼Œç°æœ‰çš„ç”¨æˆ·éš”ç¦»æ–¹æ³•è™½ç„¶èƒ½é˜²æ­¢ä¿¡æ¯æ³„éœ²ï¼Œä½†åœ¨æ€§èƒ½ä¸Šå­˜åœ¨æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSafeKVæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é€‰æ‹©æ€§å…±äº«éæ•æ„Ÿç¼“å­˜æ¡ç›®ï¼Œæ¥å¹³è¡¡éšç§ä¿æŠ¤ä¸æ€§èƒ½æå‡ï¼Œé¿å…äº†å…¨é¢éš”ç¦»å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSafeKVç”±ä¸‰ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šæ··åˆå¤šå±‚æ£€æµ‹ç®¡é“ã€ç»Ÿä¸€çš„åŸºæ•°æ ‘ç´¢å¼•å’ŒåŸºäºç†µçš„è®¿é—®ç›‘æ§ã€‚æ£€æµ‹ç®¡é“è´Ÿè´£è¯†åˆ«æ•æ„Ÿä¿¡æ¯ï¼ŒåŸºæ•°æ ‘ç´¢å¼•ç®¡ç†ä¸åŒå†…å­˜å±‚æ¬¡çš„ç¼“å­˜æ¡ç›®ï¼Œè®¿é—®ç›‘æ§åˆ™ç”¨äºæ£€æµ‹ä¿¡æ¯æ³„éœ²ã€‚

**å…³é”®åˆ›æ–°**ï¼šSafeKVçš„åˆ›æ–°åœ¨äºå…¶é€‰æ‹©æ€§å…±äº«æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½æ—¶åºä¾§ä¿¡é“æ”»å‡»çš„é£é™©ã€‚è¿™ä¸€æœºåˆ¶ä¸ä¼ ç»Ÿçš„ç”¨æˆ·éš”ç¦»æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSafeKVé‡‡ç”¨äº†æ··åˆæ£€æµ‹ç­–ç•¥ï¼Œç»“åˆè§„åˆ™åŒ¹é…å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥éªŒè¯ï¼ŒåŒæ—¶ä½¿ç”¨ç»Ÿä¸€çš„åŸºæ•°æ ‘ç´¢å¼•æ¥é«˜æ•ˆç®¡ç†ç¼“å­˜æ¡ç›®ï¼Œç¡®ä¿äº†é«˜æ•ˆçš„å†…å­˜ä½¿ç”¨å’Œå¿«é€Ÿè®¿é—®ã€‚é€šè¿‡ç†µç›‘æ§ï¼Œè¿›ä¸€æ­¥é™ä½äº†ä¿¡æ¯æ³„éœ²çš„é£é™©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSafeKVèƒ½å¤Ÿæœ‰æ•ˆå‡è½»94%-97%çš„æ—¶åºä¾§ä¿¡é“æ”»å‡»ï¼Œç›¸æ¯”äºç”¨æˆ·éš”ç¦»æ–¹æ³•ï¼ŒTTFTæå‡é«˜è¾¾40.58%ï¼Œååé‡æé«˜è‡³2.66å€ï¼Œæ˜¾è‘—é™ä½äº†ç¼“å­˜å¼•èµ·çš„TTFTå¼€é”€ï¼Œä»50.41%é™è‡³11.74%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SafeKVæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¤„ç†æ•æ„Ÿä¿¡æ¯çš„é«˜ååé‡åœºæ™¯ä¸­ï¼Œå¦‚é‡‘èã€åŒ»ç–—å’Œåœ¨çº¿æœåŠ¡ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡éšç§ä¿æŠ¤èƒ½åŠ›å’Œç³»ç»Ÿæ€§èƒ½ï¼ŒSafeKVèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸçš„LLMæ¨ç†æä¾›æ›´å®‰å…¨çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Global KV-cache sharing has emerged as a key optimization for accelerating large language model (LLM) inference. However, it exposes a new class of timing side-channel attacks, enabling adversaries to infer sensitive user inputs via shared cache entries. Existing defenses, such as per-user isolation, eliminate leakage but degrade performance by up to 38.9% in time-to-first-token (TTFT), making them impractical for high-throughput deployment. To address this gap, we introduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware KV-cache management framework that selectively shares non-sensitive entries while confining sensitive content to private caches. SafeKV comprises three components: (i) a hybrid, multi-tier detection pipeline that integrates rule-based pattern matching, a general-purpose privacy detector, and context-aware validation; (ii) a unified radix-tree index that manages public and private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and (iii) entropy-based access monitoring to detect and mitigate residual information leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of timing-based side-channel attacks. Compared to per-user isolation method, SafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across diverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from 50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with high cache reuse efficiency, SafeKV reclaims the performance advantages of global sharing while providing robust runtime privacy guarantees for LLM inference.

