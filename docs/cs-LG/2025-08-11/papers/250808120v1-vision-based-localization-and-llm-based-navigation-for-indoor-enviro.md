---
layout: default
title: Vision-Based Localization and LLM-based Navigation for Indoor Environments
---

# Vision-Based Localization and LLM-based Navigation for Indoor Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08120" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08120v1</a>
  <a href="https://arxiv.org/pdf/2508.08120.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08120v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08120v1', 'Vision-Based Localization and LLM-based Navigation for Indoor Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Keyan Rahimi, Md. Wasiul Haque, Sagar Dasgupta, Mizanur Rahman

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: 20 pages, 6 figures, 1 table

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè§†è§‰å®šä½ä¸å¤§è¯­è¨€æ¨¡å‹å¯¼èˆªçš„å®¤å†…å¯¼èˆªè§£å†³æ–¹æ¡ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®¤å†…å¯¼èˆª` `è§†è§‰å®šä½` `å¤§è¯­è¨€æ¨¡å‹` `ResNet-50` `æœºå™¨äººå¯¼èˆª` `æ·±åº¦å­¦ä¹ ` `æ™ºèƒ½æ‰‹æœºåº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å®¤å†…å¯¼èˆªé¢ä¸´GPSä¿¡å·ç¼ºå¤±å’Œå»ºç­‘å¤æ‚æ€§å¯¼è‡´çš„å®šä½å›°éš¾ï¼Œç°æœ‰æ–¹æ³•åœ¨è¿™äº›ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰å®šä½å’Œå¤§è¯­è¨€æ¨¡å‹çš„å¯¼èˆªç³»ç»Ÿï¼Œé€šè¿‡æ™ºèƒ½æ‰‹æœºæ‘„åƒå¤´å®ç°ç”¨æˆ·å®šä½ï¼Œå¹¶ç”Ÿæˆå¯¼èˆªæŒ‡ä»¤ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨å®šä½ä¸Šå®ç°äº†96%çš„å‡†ç¡®ç‡ï¼Œå¯¼èˆªæŒ‡ä»¤çš„å¹³å‡å‡†ç¡®ç‡ä¸º75%ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®¤å†…å¯¼èˆªå› ç¼ºä¹å¯é çš„GPSä¿¡å·å’Œå¤æ‚çš„å»ºç­‘ç»“æ„è€Œé¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰å®šä½ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¼èˆªçš„å®¤å†…å®šä½ä¸å¯¼èˆªæ–¹æ³•ã€‚å®šä½ç³»ç»Ÿåˆ©ç”¨ç»è¿‡ä¸¤é˜¶æ®µå¾®è°ƒçš„ResNet-50å·ç§¯ç¥ç»ç½‘ç»œï¼Œé€šè¿‡æ™ºèƒ½æ‰‹æœºæ‘„åƒå¤´è¾“å…¥è¯†åˆ«ç”¨æˆ·ä½ç½®ã€‚å¯¼èˆªæ¨¡å—åˆ™é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ç³»ç»Ÿæç¤ºï¼Œåˆ©ç”¨LLMè§£æé¢„å¤„ç†çš„å¹³é¢å›¾åƒå¹¶ç”Ÿæˆé€æ­¥å¯¼èˆªæŒ‡ä»¤ã€‚å®éªŒåœ¨å…·æœ‰é‡å¤ç‰¹å¾å’Œæœ‰é™å¯è§åº¦çš„çœŸå®åŠå…¬èµ°å»Šä¸­è¿›è¡Œï¼ŒéªŒè¯äº†å®šä½çš„é²æ£’æ€§ã€‚è¯¥æ¨¡å‹åœ¨æ‰€æœ‰æµ‹è¯•çš„è·¯æ ‡ä¸Šå®ç°äº†96%çš„é«˜å‡†ç¡®ç‡ï¼Œå³ä½¿åœ¨å—é™çš„è§†è§’æ¡ä»¶å’ŒçŸ­æ—¶æŸ¥è¯¢ä¸‹ä¹Ÿè¡¨ç°è‰¯å¥½ã€‚ä½¿ç”¨ChatGPTå¯¹çœŸå®å»ºç­‘å¹³é¢å›¾è¿›è¡Œå¯¼èˆªæµ‹è¯•çš„æŒ‡ä»¤å‡†ç¡®ç‡å¹³å‡ä¸º75%ï¼Œä½†åœ¨é›¶-shotæ¨ç†å’Œæ¨ç†æ—¶é—´ä¸Šå­˜åœ¨å±€é™ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†åœ¨èµ„æºå—é™ç¯å¢ƒï¼ˆå¦‚åŒ»é™¢ã€æœºåœºå’Œæ•™è‚²æœºæ„ï¼‰ä¸­ï¼Œåˆ©ç”¨ç°æˆæ‘„åƒå¤´å’Œå…¬å¼€å¹³é¢å›¾è¿›è¡Œå¯æ‰©å±•çš„åŸºç¡€è®¾æ–½è‡ªç”±å®¤å†…å¯¼èˆªçš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å®¤å†…ç¯å¢ƒä¸­ç”±äºç¼ºä¹GPSä¿¡å·å’Œå»ºç­‘å¤æ‚æ€§å¯¼è‡´çš„å¯¼èˆªå›°éš¾ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™äº›æ¡ä»¶ä¸‹çš„å®šä½å’Œå¯¼èˆªæ•ˆæœä¸ç†æƒ³ï¼Œå°¤å…¶æ˜¯åœ¨å¯è§åº¦å—é™çš„æƒ…å†µä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰å®šä½ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆï¼Œåˆ©ç”¨æ™ºèƒ½æ‰‹æœºæ‘„åƒå¤´è¿›è¡Œç”¨æˆ·å®šä½ï¼Œå¹¶é€šè¿‡LLMè§£æå¹³é¢å›¾ç”Ÿæˆå¯¼èˆªæŒ‡ä»¤ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜å®¤å†…å¯¼èˆªçš„å‡†ç¡®æ€§å’Œå¯ç”¨æ€§ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºResNet-50çš„è§†è§‰å®šä½æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒæ¥æé«˜å®šä½ç²¾åº¦ï¼›å…¶æ¬¡æ˜¯LLMå¯¼èˆªæ¨¡å—ï¼Œé€šè¿‡ç³»ç»Ÿæç¤ºè§£æå¹³é¢å›¾å¹¶ç”Ÿæˆå¯¼èˆªæŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†è§†è§‰å®šä½ä¸LLMç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å®¤å†…å¯¼èˆªæ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ä¸åŒäºä¼ ç»Ÿçš„åŸºäºä¼ æ„Ÿå™¨æˆ–GPSçš„å¯¼èˆªï¼Œå…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒResNet-50ç½‘ç»œç»è¿‡ä¸¤é˜¶æ®µå¾®è°ƒä»¥é€‚åº”ç‰¹å®šçš„å®¤å†…ç¯å¢ƒï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©å’Œå‚æ•°è®¾ç½®ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨æ‰€æœ‰æµ‹è¯•çš„è·¯æ ‡ä¸Šå®ç°äº†96%çš„å®šä½å‡†ç¡®ç‡ï¼Œä¸”åœ¨æœ‰é™å¯è§åº¦æ¡ä»¶ä¸‹ä»è¡¨ç°å‡ºé«˜ä¿¡å¿ƒã€‚å¯¼èˆªæµ‹è¯•ä¸­ï¼Œä½¿ç”¨ChatGPTçš„æŒ‡ä»¤å‡†ç¡®ç‡è¾¾75%ï¼Œå°½ç®¡åœ¨é›¶-shotæ¨ç†å’Œæ¨ç†æ—¶é—´ä¸Šå­˜åœ¨ä¸€å®šå±€é™ï¼Œæ•´ä½“æ•ˆæœä»æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»é™¢ã€æœºåœºå’Œæ•™è‚²æœºæ„ç­‰èµ„æºå—é™çš„ç¯å¢ƒã€‚é€šè¿‡åˆ©ç”¨ç°æˆçš„æ‘„åƒå¤´å’Œå…¬å¼€çš„å¹³é¢å›¾ï¼Œèƒ½å¤Ÿå®ç°åŸºç¡€è®¾æ–½è‡ªç”±çš„å®¤å†…å¯¼èˆªï¼Œé™ä½äº†éƒ¨ç½²æˆæœ¬å¹¶æé«˜äº†å¯è®¿é—®æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ™ºèƒ½å»ºç­‘å’Œè‡ªåŠ¨åŒ–å¯¼è§ˆç³»ç»Ÿä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Indoor navigation remains a complex challenge due to the absence of reliable GPS signals and the architectural intricacies of large enclosed environments. This study presents an indoor localization and navigation approach that integrates vision-based localization with large language model (LLM)-based navigation. The localization system utilizes a ResNet-50 convolutional neural network fine-tuned through a two-stage process to identify the user's position using smartphone camera input. To complement localization, the navigation module employs an LLM, guided by a carefully crafted system prompt, to interpret preprocessed floor plan images and generate step-by-step directions. Experimental evaluation was conducted in a realistic office corridor with repetitive features and limited visibility to test localization robustness. The model achieved high confidence and an accuracy of 96% across all tested waypoints, even under constrained viewing conditions and short-duration queries. Navigation tests using ChatGPT on real building floor maps yielded an average instruction accuracy of 75%, with observed limitations in zero-shot reasoning and inference time. This research demonstrates the potential for scalable, infrastructure-free indoor navigation using off-the-shelf cameras and publicly available floor plans, particularly in resource-constrained settings like hospitals, airports, and educational institutions.

