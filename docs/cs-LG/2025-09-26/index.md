---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.LG - 2025-09-26
---

# cs.LGï¼ˆ2025-09-26ï¼‰

ğŸ“Š å…± **39** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (22 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (22 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250922921v1-rethinking-large-language-model-distillation-a-constrained-markov-de.html">Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective</a></td>
  <td>æå‡ºåŸºäºçº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„å¤§è¯­è¨€æ¨¡å‹è’¸é¦æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">distillation</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22921v1" data-paper-url="./papers/250922921v1-rethinking-large-language-model-distillation-a-constrained-markov-de.html" onclick="toggleFavorite(this, '2509.22921v1', 'Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250922295v2-aurora-towards-universal-generative-multimodal-time-series-forecasti.html">Aurora: Towards Universal Generative Multimodal Time Series Forecasting</a></td>
  <td>Auroraï¼šé¢å‘é€šç”¨ç”Ÿæˆå¼å¤šæ¨¡æ€æ—¶é—´åºåˆ—é¢„æµ‹çš„åŸºåº§æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22295v2" data-paper-url="./papers/250922295v2-aurora-towards-universal-generative-multimodal-time-series-forecasti.html" onclick="toggleFavorite(this, '2509.22295v2', 'Aurora: Towards Universal Generative Multimodal Time Series Forecasting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250922468v1-learning-the-neighborhood-contrast-free-multimodal-self-supervised-m.html">Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining</a></td>
  <td>C-FREEï¼šä¸€ç§æ— å¯¹æ¯”å¤šæ¨¡æ€è‡ªç›‘ç£åˆ†å­å›¾é¢„è®­ç»ƒæ–¹æ³•ï¼Œèåˆ2Dæ‹“æ‰‘å’Œ3Dç»“æ„ä¿¡æ¯ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22468v1" data-paper-url="./papers/250922468v1-learning-the-neighborhood-contrast-free-multimodal-self-supervised-m.html" onclick="toggleFavorite(this, '2509.22468v1', 'Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250922387v1-spingpt-a-large-language-model-approach-to-playing-poker-correctly.html">SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly</a></td>
  <td>SpinGPTï¼šä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹è§£å†³å¾·å·æ‰‘å…‹é—®é¢˜çš„æ–¹æ¡ˆ</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22387v1" data-paper-url="./papers/250922387v1-spingpt-a-large-language-model-approach-to-playing-poker-correctly.html" onclick="toggleFavorite(this, '2509.22387v1', 'SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250922053v1-enriching-knowledge-distillation-with-intra-class-contrastive-learni.html">Enriching Knowledge Distillation with Intra-Class Contrastive Learning</a></td>
  <td>æå‡ºåŸºäºç±»å†…å¯¹æ¯”å­¦ä¹ çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œæå‡è½¯æ ‡ç­¾çš„ä¿¡æ¯ä¸°å¯Œåº¦</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22053v1" data-paper-url="./papers/250922053v1-enriching-knowledge-distillation-with-intra-class-contrastive-learni.html" onclick="toggleFavorite(this, '2509.22053v1', 'Enriching Knowledge Distillation with Intra-Class Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250922963v2-reinforcement-learning-with-discrete-diffusion-policies-for-combinat.html">Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces</a></td>
  <td>æå‡ºåŸºäºç¦»æ•£æ‰©æ•£ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³ç»„åˆåŠ¨ä½œç©ºé—´éš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22963v2" data-paper-url="./papers/250922963v2-reinforcement-learning-with-discrete-diffusion-policies-for-combinat.html" onclick="toggleFavorite(this, '2509.22963v2', 'Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250922851v3-adaptive-margin-rlhf-via-preference-over-preferences.html">Adaptive Margin RLHF via Preference over Preferences</a></td>
  <td>æå‡ºDPO-PoPï¼Œåˆ©ç”¨åå¥½ä¹‹ä¸Šçš„åå¥½ä¿¡æ¯è‡ªé€‚åº”è°ƒæ•´è¾¹é™…ï¼Œæå‡RLHFæ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">RLHF</span> <span class="paper-tag">DPO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22851v3" data-paper-url="./papers/250922851v3-adaptive-margin-rlhf-via-preference-over-preferences.html" onclick="toggleFavorite(this, '2509.22851v3', 'Adaptive Margin RLHF via Preference over Preferences')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250922601v4-learn-the-ropes-then-trust-the-wins-self-imitation-with-progressive-.html">Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</a></td>
  <td>SPEARï¼šåŸºäºè‡ªæ¨¡ä»¿å­¦ä¹ å’Œæ¸è¿›æ¢ç´¢çš„Agenticå¼ºåŒ–å­¦ä¹ æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">reward shaping</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22601v4" data-paper-url="./papers/250922601v4-learn-the-ropes-then-trust-the-wins-self-imitation-with-progressive-.html" onclick="toggleFavorite(this, '2509.22601v4', 'Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250922553v1-linear-causal-representation-learning-by-topological-ordering-prunin.html">Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement</a></td>
  <td>æå‡ºä¸€ç§åŸºäºæ‹“æ‰‘æ’åºã€å‰ªæå’Œè§£è€¦çš„çº¿æ€§å› æœè¡¨ç¤ºå­¦ä¹ æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22553v1" data-paper-url="./papers/250922553v1-linear-causal-representation-learning-by-topological-ordering-prunin.html" onclick="toggleFavorite(this, '2509.22553v1', 'Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250922353v1-context-and-diversity-matter-the-emergence-of-in-context-learning-in.html">Context and Diversity Matter: The Emergence of In-Context Learning in World Models</a></td>
  <td>æå‡ºä¸Šä¸‹æ–‡ç¯å¢ƒå­¦ä¹ (ICEL)æ¡†æ¶ï¼Œæå‡ä¸–ç•Œæ¨¡å‹åœ¨æœªçŸ¥ç¯å¢ƒä¸‹çš„é€‚åº”æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22353v1" data-paper-url="./papers/250922353v1-context-and-diversity-matter-the-emergence-of-in-context-learning-in.html" onclick="toggleFavorite(this, '2509.22353v1', 'Context and Diversity Matter: The Emergence of In-Context Learning in World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250922459v1-universal-inverse-distillation-for-matching-models-with-real-data-su.html">Universal Inverse Distillation for Matching Models with Real-Data Supervision (No GANs)</a></td>
  <td>æå‡ºRealUIDï¼šä¸€ç§é€šç”¨çš„æ— GANåŒ¹é…æ¨¡å‹é€†å‘è’¸é¦æ¡†æ¶ï¼Œå¯åˆ©ç”¨çœŸå®æ•°æ®åŠ é€Ÿç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22459v1" data-paper-url="./papers/250922459v1-universal-inverse-distillation-for-matching-models-with-real-data-su.html" onclick="toggleFavorite(this, '2509.22459v1', 'Universal Inverse Distillation for Matching Models with Real-Data Supervision (No GANs)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250922764v1-in-context-learning-can-perform-continual-learning-like-humans.html">In-Context Learning can Perform Continual Learning Like Humans</a></td>
  <td>æå‡ºä¸Šä¸‹æ–‡æŒç»­å­¦ä¹ (ICCL)ï¼Œå®ç°ç±»äººé•¿æœŸè®°å¿†å’Œè·¨ä»»åŠ¡çŸ¥è¯†ç§¯ç´¯ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">linear attention</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22764v1" data-paper-url="./papers/250922764v1-in-context-learning-can-perform-continual-learning-like-humans.html" onclick="toggleFavorite(this, '2509.22764v1', 'In-Context Learning can Perform Continual Learning Like Humans')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250922507v1-adaptive-dual-mode-distillation-with-incentive-schemes-for-scalable-.html">Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data</a></td>
  <td>æå‡ºè‡ªé€‚åº”åŒæ¨¡å¼è’¸é¦ä¸æ¿€åŠ±æœºåˆ¶ï¼Œè§£å†³éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®ä¸‹å¼‚æ„è”é‚¦å­¦ä¹ çš„å¯æ‰©å±•æ€§é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22507v1" data-paper-url="./papers/250922507v1-adaptive-dual-mode-distillation-with-incentive-schemes-for-scalable-.html" onclick="toggleFavorite(this, '2509.22507v1', 'Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251001265v1-rlp-reinforcement-as-a-pretraining-objective.html">RLP: Reinforcement as a Pretraining Objective</a></td>
  <td>æå‡ºRLPï¼šä¸€ç§å°†å¼ºåŒ–å­¦ä¹ ä½œä¸ºé¢„è®­ç»ƒç›®æ ‡çš„æ–¹æ³•ï¼Œæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01265v1" data-paper-url="./papers/251001265v1-rlp-reinforcement-as-a-pretraining-objective.html" onclick="toggleFavorite(this, '2510.01265v1', 'RLP: Reinforcement as a Pretraining Objective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250922623v1-a-theoretical-analysis-of-discrete-flow-matching-generative-models.html">A Theoretical Analysis of Discrete Flow Matching Generative Models</a></td>
  <td>ä¸ºç¦»æ•£æµåŒ¹é…ç”Ÿæˆæ¨¡å‹æä¾›ç†è®ºåˆ†æï¼Œè¯æ˜å…¶æ”¶æ•›æ€§</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22623v1" data-paper-url="./papers/250922623v1-a-theoretical-analysis-of-discrete-flow-matching-generative-models.html" onclick="toggleFavorite(this, '2509.22623v1', 'A Theoretical Analysis of Discrete Flow Matching Generative Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250922596v1-effective-policy-learning-for-multi-agent-online-coordination-beyond.html">Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives</a></td>
  <td>æå‡ºMA-SPLå’ŒMA-MPLç®—æ³•ä»¥è§£å†³å¤šæ™ºèƒ½ä½“åœ¨çº¿åè°ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22596v1" data-paper-url="./papers/250922596v1-effective-policy-learning-for-multi-agent-online-coordination-beyond.html" onclick="toggleFavorite(this, '2509.22596v1', 'Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250922576v1-epo-entropy-regularized-policy-optimization-for-llm-agents-reinforce.html">EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning</a></td>
  <td>æå‡ºEPOç®—æ³•ï¼Œè§£å†³LLM Agentåœ¨å¤šè½®ç¨€ç–å¥–åŠ±å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢-åˆ©ç”¨å´©æºƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22576v1" data-paper-url="./papers/250922576v1-epo-entropy-regularized-policy-optimization-for-llm-agents-reinforce.html" onclick="toggleFavorite(this, '2509.22576v1', 'EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250922566v1-from-parameters-to-behavior-unsupervised-compression-of-the-policy-s.html">From Parameters to Behavior: Unsupervised Compression of the Policy Space</a></td>
  <td>æå‡ºæ— ç›‘ç£æ–¹æ³•å‹ç¼©ç­–ç•¥ç©ºé—´ä»¥æé«˜æ·±åº¦å¼ºåŒ–å­¦ä¹ æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">DRL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22566v1" data-paper-url="./papers/250922566v1-from-parameters-to-behavior-unsupervised-compression-of-the-policy-s.html" onclick="toggleFavorite(this, '2509.22566v1', 'From Parameters to Behavior: Unsupervised Compression of the Policy Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251003259v1-meta-awareness-enhances-reasoning-models-self-alignment-reinforcemen.html">Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning</a></td>
  <td>æå‡ºMASAè‡ªå¯¹é½å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨ç†æ¨¡å‹å…ƒè®¤çŸ¥èƒ½åŠ›ä¸æ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03259v1" data-paper-url="./papers/251003259v1-meta-awareness-enhances-reasoning-models-self-alignment-reinforcemen.html" onclick="toggleFavorite(this, '2510.03259v1', 'Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250922232v1-fairness-aware-reinforcement-learning-farel-a-framework-for-transpar.html">Fairness-Aware Reinforcement Learning (FAReL): A Framework for Transparent and Balanced Sequential Decision-Making</a></td>
  <td>æå‡ºFAReLæ¡†æ¶ï¼Œè§£å†³å¼ºåŒ–å­¦ä¹ ä¸­æ€§èƒ½ä¸å…¬å¹³æ€§çš„æƒè¡¡é—®é¢˜ï¼Œåº”ç”¨äºæ‹›è˜å’Œæ¬ºè¯ˆæ£€æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22232v1" data-paper-url="./papers/250922232v1-fairness-aware-reinforcement-learning-farel-a-framework-for-transpar.html" onclick="toggleFavorite(this, '2509.22232v1', 'Fairness-Aware Reinforcement Learning (FAReL): A Framework for Transparent and Balanced Sequential Decision-Making')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250922454v1-overclocking-electrostatic-generative-models.html">Overclocking Electrostatic Generative Models</a></td>
  <td>æå‡ºé€†æ³Šæ¾æµåŒ¹é…ä»¥åŠ é€Ÿç”µé™æ€ç”Ÿæˆæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22454v1" data-paper-url="./papers/250922454v1-overclocking-electrostatic-generative-models.html" onclick="toggleFavorite(this, '2509.22454v1', 'Overclocking Electrostatic Generative Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251003257v1-triple-bert-do-we-really-need-marl-for-order-dispatch-on-ride-sharin.html">Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?</a></td>
  <td>Triple-BERTï¼šç”¨äºç½‘çº¦è½¦è®¢å•è°ƒåº¦çš„å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ€§èƒ½ä¼˜äºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">TD3</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03257v1" data-paper-url="./papers/251003257v1-triple-bert-do-we-really-need-marl-for-order-dispatch-on-ride-sharin.html" onclick="toggleFavorite(this, '2510.03257v1', 'Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250922272v2-fine-grained-uncertainty-decomposition-in-large-language-models-a-sp.html">Fine-Grained Uncertainty Decomposition in Large Language Models: A Spectral Approach</a></td>
  <td>æå‡ºSpectral Uncertaintyï¼Œç”¨äºå¤§è¯­è¨€æ¨¡å‹ä¸­ç»†ç²’åº¦çš„ä¸ç¡®å®šæ€§åˆ†è§£</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22272v2" data-paper-url="./papers/250922272v2-fine-grained-uncertainty-decomposition-in-large-language-models-a-sp.html" onclick="toggleFavorite(this, '2509.22272v2', 'Fine-Grained Uncertainty Decomposition in Large Language Models: A Spectral Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250922263v1-erase-or-hide-suppressing-spurious-unlearning-neurons-for-robust-unl.html">Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning</a></td>
  <td>æå‡ºSsiuuæ–¹æ³•ï¼Œé€šè¿‡æŠ‘åˆ¶è™šå‡åå­¦ä¹ ç¥ç»å…ƒå®ç°è¯­è¨€æ¨¡å‹é²æ£’åå­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22263v1" data-paper-url="./papers/250922263v1-erase-or-hide-suppressing-spurious-unlearning-neurons-for-robust-unl.html" onclick="toggleFavorite(this, '2509.22263v1', 'Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250922979v1-optimind-teaching-llms-to-think-like-optimization-experts.html">OptiMind: Teaching LLMs to Think Like Optimization Experts</a></td>
  <td>OptiMindï¼šæ•™å¯¼LLMåƒä¼˜åŒ–ä¸“å®¶ä¸€æ ·æ€è€ƒï¼Œæå‡æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’å»ºæ¨¡ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22979v1" data-paper-url="./papers/250922979v1-optimind-teaching-llms-to-think-like-optimization-experts.html" onclick="toggleFavorite(this, '2509.22979v1', 'OptiMind: Teaching LLMs to Think Like Optimization Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250922944v3-sinq-sinkhorn-normalized-quantization-for-calibration-free-low-preci.html">SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights</a></td>
  <td>SINQï¼šç”¨äºå…æ ¡å‡†ä½ç²¾åº¦LLMæƒé‡ Sinkhorn å½’ä¸€åŒ–é‡åŒ–</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22944v3" data-paper-url="./papers/250922944v3-sinq-sinkhorn-normalized-quantization-for-calibration-free-low-preci.html" onclick="toggleFavorite(this, '2509.22944v3', 'SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250922850v3-boundary-on-the-table-efficient-black-box-decision-based-attacks-for.html">Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data</a></td>
  <td>é’ˆå¯¹è¡¨æ ¼æ•°æ®çš„é»‘ç›’å†³ç­–å‹å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œé«˜æ•ˆæ”»å‡»ç»“æ„åŒ–æ•°æ®æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22850v3" data-paper-url="./papers/250922850v3-boundary-on-the-table-efficient-black-box-decision-based-attacks-for.html" onclick="toggleFavorite(this, '2509.22850v3', 'Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250922796v1-what-do-they-fix-llm-aided-categorization-of-security-patches-for-cr.html">What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs</a></td>
  <td>DUALLMï¼šåˆ©ç”¨LLMè¾…åŠ©è¯†åˆ«Linuxå†…æ ¸ä¸­å…³é”®å†…å­˜æ¼æ´çš„å®‰å…¨è¡¥ä¸</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22796v1" data-paper-url="./papers/250922796v1-what-do-they-fix-llm-aided-categorization-of-security-patches-for-cr.html" onclick="toggleFavorite(this, '2509.22796v1', 'What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250922483v1-ofmu-optimization-driven-framework-for-machine-unlearning.html">OFMU: Optimization-Driven Framework for Machine Unlearning</a></td>
  <td>æå‡ºOFMUï¼šä¸€ç§ä¼˜åŒ–é©±åŠ¨çš„æœºå™¨å­¦ä¹ é—å¿˜æ¡†æ¶ï¼Œæå‡é—å¿˜æ•ˆæœå’Œæ¨¡å‹æ•ˆç”¨ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22483v1" data-paper-url="./papers/250922483v1-ofmu-optimization-driven-framework-for-machine-unlearning.html" onclick="toggleFavorite(this, '2509.22483v1', 'OFMU: Optimization-Driven Framework for Machine Unlearning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250922363v2-investigating-faithfulness-in-large-audio-language-models.html">Investigating Faithfulness in Large Audio Language Models</a></td>
  <td>ç ”ç©¶è¡¨æ˜å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„æ€ç»´é“¾ï¼ˆCoTï¼‰åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯å¯ä¿¡çš„ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22363v2" data-paper-url="./papers/250922363v2-investigating-faithfulness-in-large-audio-language-models.html" onclick="toggleFavorite(this, '2509.22363v2', 'Investigating Faithfulness in Large Audio Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250922358v1-stochastic-activations.html">Stochastic activations</a></td>
  <td>æå‡ºéšæœºæ¿€æ´»å‡½æ•°ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†é€Ÿåº¦å¹¶å¢å¼ºç”Ÿæˆæ–‡æœ¬å¤šæ ·æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22358v1" data-paper-url="./papers/250922358v1-stochastic-activations.html" onclick="toggleFavorite(this, '2509.22358v1', 'Stochastic activations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250922299v1-heapr-hessian-based-efficient-atomic-expert-pruning-in-output-space.html">HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space</a></td>
  <td>HEAPrï¼šåŸºäºHessiançš„è¾“å‡ºç©ºé—´é«˜æ•ˆåŸå­ä¸“å®¶å‰ªææ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22299v1" data-paper-url="./papers/250922299v1-heapr-hessian-based-efficient-atomic-expert-pruning-in-output-space.html" onclick="toggleFavorite(this, '2509.22299v1', 'HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250922166v1-lightweight-error-mitigation-strategies-for-post-training-nm-activat.html">Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs</a></td>
  <td>æå‡ºè½»é‡çº§è¯¯å·®ç¼“è§£ç­–ç•¥ï¼Œç”¨äºLLMåè®­ç»ƒN:Mæ¿€æ´»ç¨€ç–åŒ–ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22166v1" data-paper-url="./papers/250922166v1-lightweight-error-mitigation-strategies-for-post-training-nm-activat.html" onclick="toggleFavorite(this, '2509.22166v1', 'Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>34</td>
  <td><a href="./papers/250922403v1-movefm-r-advancing-mobility-foundation-models-via-language-driven-se.html">MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning</a></td>
  <td>MoveFM-Rï¼šé€šè¿‡è¯­è¨€é©±åŠ¨çš„è¯­ä¹‰æ¨ç†æå‡å‡ºè¡ŒåŸºç¡€æ¨¡å‹æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span> <span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22403v1" data-paper-url="./papers/250922403v1-movefm-r-advancing-mobility-foundation-models-via-language-driven-se.html" onclick="toggleFavorite(this, '2509.22403v1', 'MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250923003v1-physically-plausible-multi-system-trajectory-generation-and-symmetry.html">Physically Plausible Multi-System Trajectory Generation and Symmetry Discovery</a></td>
  <td>æå‡ºSPS-GANï¼Œç”¨äºå¤šç³»ç»Ÿè½¨è¿¹ç”Ÿæˆå’Œå¯¹ç§°æ€§å‘ç°ï¼Œæ— éœ€å…ˆéªŒçŸ¥è¯†å¹¶æ³›åŒ–åˆ°æœªè§å‚æ•°ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23003v1" data-paper-url="./papers/250923003v1-physically-plausible-multi-system-trajectory-generation-and-symmetry.html" onclick="toggleFavorite(this, '2509.23003v1', 'Physically Plausible Multi-System Trajectory Generation and Symmetry Discovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250922207v1-reversible-gns-for-dissipative-fluids-with-consistent-bidirectional-.html">Reversible GNS for Dissipative Fluids with Consistent Bidirectional Dynamics</a></td>
  <td>æå‡ºå¯é€†å›¾ç½‘ç»œæ¨¡æ‹Ÿå™¨è§£å†³è€—æ•£æµä½“çš„åŒå‘åŠ¨æ€é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22207v1" data-paper-url="./papers/250922207v1-reversible-gns-for-dissipative-fluids-with-consistent-bidirectional-.html" onclick="toggleFavorite(this, '2509.22207v1', 'Reversible GNS for Dissipative Fluids with Consistent Bidirectional Dynamics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>37</td>
  <td><a href="./papers/250922402v1-relam-learning-anticipation-model-for-rewarding-visual-robotic-manip.html">ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation</a></td>
  <td>æå‡ºReLAMï¼Œé€šè¿‡å­¦ä¹ é¢„æµ‹æ¨¡å‹ä¸ºè§†è§‰æœºå™¨äººæ“ä½œç”Ÿæˆå¥–åŠ±</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">reward design</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22402v1" data-paper-url="./papers/250922402v1-relam-learning-anticipation-model-for-rewarding-visual-robotic-manip.html" onclick="toggleFavorite(this, '2509.22402v1', 'ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/251001264v1-a-framework-for-scalable-heterogeneous-multi-agent-adversarial-reinf.html">A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab</a></td>
  <td>æ‰©å±•IsaacLabæ¡†æ¶ï¼Œå®ç°å¼‚æ„å¤šæ™ºèƒ½ä½“å¯¹æŠ—å¼ºåŒ–å­¦ä¹ çš„å¯æ‰©å±•è®­ç»ƒ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01264v1" data-paper-url="./papers/251001264v1-a-framework-for-scalable-heterogeneous-multi-agent-adversarial-reinf.html" onclick="toggleFavorite(this, '2510.01264v1', 'A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250922855v4-observation-free-attacks-on-online-learning-to-rank.html">Observation-Free Attacks on Online Learning to Rank</a></td>
  <td>æå‡ºé’ˆå¯¹åœ¨çº¿æ’åºå­¦ä¹ çš„æ— è§‚å¯Ÿæ”»å‡»æ¡†æ¶ï¼Œæå‡ç›®æ ‡é¡¹ç›®æ’åå¹¶è¯±å¯¼çº¿æ€§é—æ†¾ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22855v4" data-paper-url="./papers/250922855v4-observation-free-attacks-on-online-learning-to-rank.html" onclick="toggleFavorite(this, '2509.22855v4', 'Observation-Free Attacks on Online Learning to Rank')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.LG é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)