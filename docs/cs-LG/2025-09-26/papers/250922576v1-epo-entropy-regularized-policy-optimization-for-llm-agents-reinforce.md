---
layout: default
title: EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning
---

# EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22576" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22576v1</a>
  <a href="https://arxiv.org/pdf/2509.22576.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22576v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22576v1', 'EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wujiang Xu, Wentian Zhao, Zhenting Wang, Yu-Jhe Li, Can Jin, Mingyu Jin, Kai Mei, Kun Wan, Dimitris N. Metaxas

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEPOç®—æ³•ï¼Œè§£å†³LLM Agentåœ¨å¤šè½®ç¨€ç–å¥–åŠ±å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢-åˆ©ç”¨å´©æºƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `LLM Agent` `å¼ºåŒ–å­¦ä¹ ` `ç†µæ­£åˆ™åŒ–` `å¤šè½®äº¤äº’` `ç¨€ç–å¥–åŠ±` `ç­–ç•¥ä¼˜åŒ–` `æ¢ç´¢-åˆ©ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šè½®äº¤äº’ã€ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹è®­ç»ƒLLM Agenté¢ä¸´æ¢ç´¢-åˆ©ç”¨çº§è”å´©æºƒé—®é¢˜ï¼Œæ—©æœŸç­–ç•¥è¿‡æ—©æ”¶æ•›ï¼ŒåæœŸç†µæ­£åˆ™åŒ–å¤±æ•ˆã€‚
2. æå‡ºç†µæ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ¡†æ¶ï¼ŒåŒ…å«ç†µæ­£åˆ™åŒ–ã€ç†µå¹³æ»‘æ­£åˆ™åŒ–å™¨å’Œè‡ªé€‚åº”é˜¶æ®µæƒé‡ä¸‰ä¸ªæœºåˆ¶ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒEPOåœ¨ScienceWorldå’ŒALFWorldç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†LLM Agentçš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹LLM Agentåœ¨å¤šè½®äº¤äº’ã€ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒéš¾é¢˜ï¼ŒæŒ‡å‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„å¤±è´¥æ¨¡å¼ï¼šæ¢ç´¢-åˆ©ç”¨çº§è”å´©æºƒã€‚è¿™ç§å´©æºƒå§‹äºæ—©æœŸç­–ç•¥çš„è¿‡æ—©æ”¶æ•›ï¼Œç”±äºç¨€ç–åé¦ˆå¯¼è‡´Agenté™·å…¥æœ‰ç¼ºé™·çš„ä½ç†µç­–ç•¥ã€‚éšåï¼ŒAgentè¿›å…¥æ™šæœŸç­–ç•¥å´©æºƒï¼Œæ­¤æ—¶ä¼ ç»Ÿçš„ç†µæ­£åˆ™åŒ–åè€Œé€‚å¾—å…¶åï¼Œä¿ƒè¿›äº†æ··ä¹±çš„æ¢ç´¢ï¼Œç ´åäº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ç†µæ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªååŒæœºåˆ¶æ‰“ç ´è¿™ç§å¤±è´¥å¾ªç¯ï¼š(1) åœ¨å¤šè½®è®¾ç½®ä¸­é‡‡ç”¨ç†µæ­£åˆ™åŒ–ä»¥å¢å¼ºæ¢ç´¢ï¼›(2) ç†µå¹³æ»‘æ­£åˆ™åŒ–å™¨ï¼Œå°†ç­–ç•¥ç†µé™åˆ¶åœ¨å†å²å¹³å‡å€¼èŒƒå›´å†…ï¼Œä»¥é˜²æ­¢çªç„¶æ³¢åŠ¨ï¼›(3) è‡ªé€‚åº”çš„åŸºäºé˜¶æ®µçš„æƒé‡ï¼Œå¹³è¡¡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢ç´¢å’Œåˆ©ç”¨ã€‚åˆ†æè¡¨æ˜ï¼ŒEPOä¿è¯äº†å•è°ƒé€’å‡çš„ç†µæ–¹å·®ï¼ŒåŒæ—¶ä¿æŒæ”¶æ•›æ€§ã€‚åœ¨ScienceWorldä¸Šï¼ŒEPOå®ç°äº†é«˜è¾¾152%çš„æ€§èƒ½æå‡ï¼Œåœ¨ALFWorldä¸Šå®ç°äº†é«˜è¾¾19.8%çš„æ€§èƒ½æå‡ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå¤šè½®ç¨€ç–å¥–åŠ±è®¾ç½®éœ€è¦ä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ ¹æœ¬ä¸åŒçš„ç†µæ§åˆ¶æ–¹æ³•ï¼Œè¿™å¯¹LLM Agentè®­ç»ƒå…·æœ‰å¹¿æ³›çš„å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³LLM Agentåœ¨å¤šè½®äº¤äº’ã€ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»é—®é¢˜æ—¶ï¼Œå®¹æ˜“å‡ºç°â€œæ¢ç´¢-åˆ©ç”¨çº§è”å´©æºƒâ€ç°è±¡ã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºå¥–åŠ±ç¨€ç–ï¼ŒAgentåœ¨è®­ç»ƒåˆæœŸå®¹æ˜“é™·å…¥æ¬¡ä¼˜ç­–ç•¥å¹¶è¿‡æ—©æ”¶æ•›ï¼ˆä½ç†µï¼‰ï¼Œå¯¼è‡´åç»­æ¢ç´¢ä¸è¶³ï¼›è€ŒåæœŸä¸ºäº†è·³å‡ºå±€éƒ¨æœ€ä¼˜ï¼Œé‡‡ç”¨ä¼ ç»Ÿç†µæ­£åˆ™åŒ–åˆå®¹æ˜“å¯¼è‡´ç­–ç•¥ä¸ç¨³å®šï¼Œç”šè‡³å´©æºƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ›´ç²¾ç»†çš„ç†µæ§åˆ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç†µæ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šç†µæ­£åˆ™åŒ–ã€ç†µå¹³æ»‘æ­£åˆ™åŒ–å™¨å’Œè‡ªé€‚åº”é˜¶æ®µæƒé‡ã€‚é€šè¿‡è¿™äº›ç»„ä»¶çš„ååŒä½œç”¨ï¼ŒEPOæ—¨åœ¨å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œé¿å…ç­–ç•¥è¿‡æ—©æ”¶æ•›å’ŒåæœŸå´©æºƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEPOæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š
1. **ç­–ç•¥å­¦ä¹ é˜¶æ®µ**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚Policy Optimizationï¼‰æ›´æ–°LLM Agentçš„ç­–ç•¥ã€‚
2. **ç†µæ­£åˆ™åŒ–é˜¶æ®µ**ï¼šåœ¨ç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå¼•å…¥ç†µæ­£åˆ™åŒ–é¡¹ï¼Œé¼“åŠ±Agentè¿›è¡Œæ›´å¹¿æ³›çš„æ¢ç´¢ã€‚
3. **ç†µå¹³æ»‘é˜¶æ®µ**ï¼šä½¿ç”¨ç†µå¹³æ»‘æ­£åˆ™åŒ–å™¨ï¼Œå°†ç­–ç•¥ç†µé™åˆ¶åœ¨å†å²å¹³å‡å€¼èŒƒå›´å†…ï¼Œé˜²æ­¢ç­–ç•¥ç†µçš„å‰§çƒˆæ³¢åŠ¨ã€‚
4. **è‡ªé€‚åº”æƒé‡è°ƒæ•´é˜¶æ®µ**ï¼šæ ¹æ®è®­ç»ƒçš„é˜¶æ®µï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´æ¢ç´¢å’Œåˆ©ç”¨çš„æƒé‡ï¼Œä»¥è¾¾åˆ°æœ€ä½³çš„è®­ç»ƒæ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»¼åˆçš„ç†µæ§åˆ¶æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³LLM Agentåœ¨å¤šè½®äº¤äº’ã€ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒé—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒEPOä¸ä»…è€ƒè™‘äº†ç†µæ­£åˆ™åŒ–ï¼Œè¿˜å¼•å…¥äº†ç†µå¹³æ»‘æ­£åˆ™åŒ–å™¨å’Œè‡ªé€‚åº”é˜¶æ®µæƒé‡ï¼Œä»è€Œå®ç°äº†æ›´ç²¾ç»†çš„ç†µæ§åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼š
* **ç†µå¹³æ»‘æ­£åˆ™åŒ–å™¨**ï¼šè¯¥æ­£åˆ™åŒ–å™¨é€šè¿‡è®¡ç®—å½“å‰ç­–ç•¥ç†µä¸å†å²å¹³å‡ç†µä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å°†å…¶ä½œä¸ºæƒ©ç½šé¡¹æ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œä»è€Œé™åˆ¶ç­–ç•¥ç†µçš„æ³¢åŠ¨ã€‚
* **è‡ªé€‚åº”é˜¶æ®µæƒé‡**ï¼šè¯¥æƒé‡æ ¹æ®è®­ç»ƒçš„é˜¶æ®µï¼ˆæ—©æœŸã€ä¸­æœŸã€æ™šæœŸï¼‰è‡ªé€‚åº”åœ°è°ƒæ•´æ¢ç´¢å’Œåˆ©ç”¨çš„æƒé‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è®­ç»ƒåˆæœŸï¼Œå¯ä»¥å¢åŠ æ¢ç´¢çš„æƒé‡ï¼Œä»¥é¼“åŠ±Agentè¿›è¡Œæ›´å¹¿æ³›çš„æ¢ç´¢ï¼›åœ¨è®­ç»ƒåæœŸï¼Œå¯ä»¥å¢åŠ åˆ©ç”¨çš„æƒé‡ï¼Œä»¥æé«˜Agentçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEPOç®—æ³•åœ¨ScienceWorldå’ŒALFWorldç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ScienceWorldä¸Šï¼ŒEPOç®—æ³•å®ç°äº†é«˜è¾¾152%çš„æ€§èƒ½æå‡ï¼›åœ¨ALFWorldä¸Šï¼ŒEPOç®—æ³•å®ç°äº†é«˜è¾¾19.8%çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒEPOç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³LLM Agentåœ¨å¤šè½®äº¤äº’ã€ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒé—®é¢˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦LLM Agentè¿›è¡Œå¤šè½®äº¤äº’ã€è§£å†³å¤æ‚ä»»åŠ¡çš„åœºæ™¯ï¼Œä¾‹å¦‚æ¸¸æˆAIã€æ™ºèƒ½å®¢æœã€ç§‘å­¦ç ”ç©¶åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜LLM Agentåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æå‡å…¶è§£å†³å®é™…é—®é¢˜çš„èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼å’Œæ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.

