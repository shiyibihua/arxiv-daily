---
layout: default
title: Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs
---

# Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22166" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22166v1</a>
  <a href="https://arxiv.org/pdf/2509.22166.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22166v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22166v1', 'Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shirin Alanova, Kristina Kazistova, Ekaterina Galaeva, Alina Kostromina, Vladimir Smirnov, Redko Dmitry, Alexey Dontsov, Maxim Zhelnin, Evgeny Burnaev, Egor Shvetsov

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§è¯¯å·®ç¼“è§£ç­–ç•¥ï¼Œç”¨äºLLMåè®­ç»ƒN:Mæ¿€æ´»ç¨€ç–åŒ–ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¿€æ´»å‰ªæ` `N:Mç¨€ç–æ€§` `åè®­ç»ƒé‡åŒ–` `è¯¯å·®ç¼“è§£` `æ¨¡å‹å‹ç¼©` `æ¨ç†åŠ é€Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ¨ç†æ•ˆç‡å—é™ï¼Œæƒé‡å‰ªæè™½æœ‰è¿›å±•ï¼Œä½†æ¿€æ´»å‰ªææ½œåŠ›æœªè¢«å……åˆ†æŒ–æ˜ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€å‹ç¼©å’ŒI/Oä¼˜åŒ–æ–¹é¢ã€‚
2. æœ¬æ–‡æ¢ç´¢LLMåè®­ç»ƒN:Mæ¿€æ´»å‰ªæï¼Œé€šè¿‡è½»é‡çº§è¯¯å·®ç¼“è§£å’Œå‰ªææ ‡å‡†ï¼Œæ„å»ºç¡¬ä»¶å‹å¥½çš„åŸºçº¿ï¼Œå¹¶ç ”ç©¶å¤šç§ç¨€ç–æ¨¡å¼ã€‚
3. å®éªŒè¯æ˜ï¼Œæ¿€æ´»å‰ªæåœ¨åŒç­‰ç¨€ç–åº¦ä¸‹ä¼˜äºæƒé‡å‰ªæï¼Œå¹¶å‘ç°8:16ç¨€ç–æ¨¡å¼åœ¨çµæ´»æ€§å’Œç¡¬ä»¶å®ç°ä¸Šå…·æœ‰è¾ƒå¥½çš„å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é«˜æ•ˆæ¨ç†çš„éœ€æ±‚ï¼Œæ·±å…¥ç ”ç©¶äº†æ¿€æ´»ç¨€ç–åŒ–æŠ€æœ¯ã€‚å°½ç®¡åŠç»“æ„åŒ–ï¼ˆN:Mï¼‰å‰ªæå·²å¹¿æ³›åº”ç”¨äºæƒé‡ï¼Œä½†å…¶åœ¨æ¿€æ´»å‰ªæä¸­çš„åº”ç”¨ä»æœ‰å¾…æ¢ç´¢ï¼Œæ¿€æ´»å‰ªæå…·æœ‰åŠ¨æ€ã€è¾“å…¥è‡ªé€‚åº”å‹ç¼©ä»¥åŠé™ä½I/Oå¼€é”€çš„æ½œåŠ›ã€‚æœ¬æ–‡å¯¹LLMä¸­åè®­ç»ƒN:Mæ¿€æ´»å‰ªæçš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢åˆ†æã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åŒç­‰ç¨€ç–åº¦ä¸‹ï¼Œæ¿€æ´»å‰ªææ¯”æƒé‡å‰ªææ›´èƒ½ä¿æŒç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†è½»é‡çº§ã€å³æ’å³ç”¨çš„è¯¯å·®ç¼“è§£æŠ€æœ¯å’Œå‰ªææ ‡å‡†ï¼Œå»ºç«‹äº†å¼ºå¤§çš„ç¡¬ä»¶å‹å¥½åŸºçº¿ï¼Œä¸”åªéœ€æå°‘çš„æ ¡å‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†NVIDIAæ ‡å‡†2:4ä¹‹å¤–çš„ç¨€ç–æ¨¡å¼ï¼Œå‘ç°16:32æ¨¡å¼çš„æ€§èƒ½å‡ ä¹ä¸éç»“æ„åŒ–ç¨€ç–æ€§ç›¸å½“ã€‚è€ƒè™‘åˆ°çµæ´»æ€§å’Œç¡¬ä»¶å®ç°å¤æ‚æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œæˆ‘ä»¬è®¤ä¸º8:16æ¨¡å¼æ˜¯æ›´ä¼˜çš„é€‰æ‹©ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºæ¿€æ´»å‰ªææä¾›äº†æœ‰æ•ˆçš„å®ç”¨æ–¹æ³•ï¼Œå¹¶ä¸ºæœªæ¥ç¡¬ä»¶æ”¯æŒæ›´çµæ´»çš„ç¨€ç–æ¨¡å¼æä¾›äº†åŠ¨åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶å…³æ³¨æ¿€æ´»å€¼å¸¦æ¥çš„è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚ç°æœ‰çš„æƒé‡å‰ªææ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†æ¿€æ´»å€¼çš„åŠ¨æ€æ€§å’Œè¾“å…¥ä¾èµ–æ€§ä½¿å…¶éš¾ä»¥æœ‰æ•ˆå‹ç¼©ï¼Œå¯¼è‡´I/Oç“¶é¢ˆå’Œè®¡ç®—æ•ˆç‡å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åŠç»“æ„åŒ–ï¼ˆN:Mï¼‰å‰ªææŠ€æœ¯å¯¹LLMçš„æ¿€æ´»å€¼è¿›è¡Œç¨€ç–åŒ–ï¼Œä»è€Œå‡å°‘è®¡ç®—é‡å’Œå†…å­˜å ç”¨ã€‚é€šè¿‡åœ¨åè®­ç»ƒé˜¶æ®µè¿›è¡Œæ¿€æ´»å‰ªæï¼Œé¿å…äº†é‡æ–°è®­ç»ƒçš„æˆæœ¬ï¼ŒåŒæ—¶åˆ©ç”¨è½»é‡çº§çš„è¯¯å·®ç¼“è§£ç­–ç•¥æ¥å¼¥è¡¥å‰ªæå¸¦æ¥çš„ç²¾åº¦æŸå¤±ã€‚é€‰æ‹©N:Mæ¨¡å¼æ˜¯ä¸ºäº†åœ¨ç¡¬ä»¶å‹å¥½æ€§å’Œçµæ´»æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡çš„ç ”ç©¶æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) é€‰æ‹©é¢„è®­ç»ƒçš„LLMæ¨¡å‹ï¼›2) è®¾è®¡å¹¶å®ç°ä¸åŒçš„N:Mæ¿€æ´»å‰ªæç­–ç•¥ï¼ŒåŒ…æ‹¬ä¸åŒçš„N:Mæ¨¡å¼ï¼ˆå¦‚2:4, 8:16, 16:32ï¼‰å’Œå‰ªææ ‡å‡†ï¼›3) åº”ç”¨è½»é‡çº§çš„è¯¯å·®ç¼“è§£æŠ€æœ¯ï¼Œä¾‹å¦‚æ¿€æ´»å€¼ç¼©æ”¾æˆ–å¾®è°ƒï¼›4) åœ¨å¤šä¸ªLLMå’Œæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒè¯„ä¼°ï¼Œæ¯”è¾ƒä¸åŒå‰ªæç­–ç•¥å’Œè¯¯å·®ç¼“è§£æŠ€æœ¯çš„æ€§èƒ½ï¼›5) åˆ†æå®éªŒç»“æœï¼Œç¡®å®šæœ€ä½³çš„N:Mæ¨¡å¼å’Œè¯¯å·®ç¼“è§£ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†N:Mæ¿€æ´»å‰ªæåœ¨LLMä¸­çš„åº”ç”¨ï¼Œå¹¶è¯æ˜å…¶ä¼˜äºæƒé‡å‰ªæï¼›2) æå‡ºäº†è½»é‡çº§çš„è¯¯å·®ç¼“è§£æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶å®ç°é«˜æ•ˆçš„æ¿€æ´»å‰ªæï¼›3) æ¢ç´¢äº†å¤šç§N:Mç¨€ç–æ¨¡å¼ï¼Œå¹¶åˆ†æäº†å®ƒä»¬åœ¨ç¡¬ä»¶å®ç°å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºæœªæ¥çš„ç¡¬ä»¶è®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å‰ªææ ‡å‡†çš„é€‰å–ï¼Œä¾‹å¦‚åŸºäºæ¿€æ´»å€¼çš„ç»å¯¹å€¼å¤§å°è¿›è¡Œå‰ªæï¼›2) N:Mæ¨¡å¼çš„é€‰æ‹©ï¼Œä¾‹å¦‚8:16æ¨¡å¼è¢«è®¤ä¸ºæ˜¯ç¡¬ä»¶å‹å¥½ä¸”æ€§èƒ½è¾ƒå¥½çš„é€‰æ‹©ï¼›3) è¯¯å·®ç¼“è§£æŠ€æœ¯çš„åº”ç”¨ï¼Œä¾‹å¦‚å¯¹ä¿ç•™çš„æ¿€æ´»å€¼è¿›è¡Œç¼©æ”¾ï¼Œä»¥è¡¥å¿è¢«å‰ªæçš„æ¿€æ´»å€¼å¸¦æ¥çš„æŸå¤±ï¼›4) å®éªŒè¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ï¼Œä¾‹å¦‚å›°æƒ‘åº¦ï¼ˆperplexityï¼‰å’Œç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŒç­‰ç¨€ç–åº¦ä¸‹ï¼Œæ¿€æ´»å‰ªææ¯”æƒé‡å‰ªææ›´èƒ½ä¿æŒLLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œ8:16ç¨€ç–æ¨¡å¼åœ¨ç¡¬ä»¶å‹å¥½æ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—äº†è¾ƒå¥½çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼Œè½»é‡çº§çš„è¯¯å·®ç¼“è§£æŠ€æœ¯èƒ½å¤Ÿæœ‰æ•ˆé™ä½å‰ªæå¸¦æ¥çš„ç²¾åº¦æŸå¤±ï¼Œä½¿å¾—æ¿€æ´»å‰ªææˆä¸ºä¸€ç§å®ç”¨çš„LLMå‹ç¼©æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆLLMæ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„æœ¬åœ°éƒ¨ç½²ã€è¾¹ç¼˜è®¡ç®—ç¯å¢ƒä»¥åŠå¯¹å»¶è¿Ÿæ•æ„Ÿçš„åœ¨çº¿æœåŠ¡ã€‚é€šè¿‡æ¿€æ´»å‰ªæï¼Œå¯ä»¥æ˜¾è‘—é™ä½LLMçš„è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­è¿è¡Œï¼Œå¹¶æé«˜æ¨ç†é€Ÿåº¦ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The demand for efficient large language model (LLM) inference has intensified the focus on sparsification techniques. While semi-structured (N:M) pruning is well-established for weights, its application to activation pruning remains underexplored despite its potential for dynamic, input-adaptive compression and reductions in I/O overhead. This work presents a comprehensive analysis of methods for post-training N:M activation pruning in LLMs. Across multiple LLMs, we demonstrate that pruning activations enables superior preservation of generative capabilities compared to weight pruning at equivalent sparsity levels. We evaluate lightweight, plug-and-play error mitigation techniques and pruning criteria, establishing strong hardware-friendly baselines that require minimal calibration. Furthermore, we explore sparsity patterns beyond NVIDIA's standard 2:4, showing that the 16:32 pattern achieves performance nearly on par with unstructured sparsity. However, considering the trade-off between flexibility and hardware implementation complexity, we focus on the 8:16 pattern as a superior candidate. Our findings provide both effective practical methods for activation pruning and a motivation for future hardware to support more flexible sparsity patterns. Our code is available https://anonymous.4open.science/r/Structured-Sparse-Activations-Inference-EC3C/README.md .

