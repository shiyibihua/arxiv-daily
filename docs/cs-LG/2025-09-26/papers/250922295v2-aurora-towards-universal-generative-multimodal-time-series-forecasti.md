---
layout: default
title: Aurora: Towards Universal Generative Multimodal Time Series Forecasting
---

# Aurora: Towards Universal Generative Multimodal Time Series Forecasting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22295" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.22295v2</a>
  <a href="https://arxiv.org/pdf/2509.22295.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22295v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22295v2', 'Aurora: Towards Universal Generative Multimodal Time Series Forecasting')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xingjian Wu, Jianxin Jin, Wanghui Qiu, Peng Chen, Yang Shu, Bin Yang, Chenjuan Guo

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-26 (Êõ¥Êñ∞: 2025-10-20)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**AuroraÔºöÈù¢ÂêëÈÄöÁî®ÁîüÊàêÂºèÂ§öÊ®°ÊÄÅÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑÂü∫Â∫ßÊ®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã` `Ë∑®È¢ÜÂüüÊ≥õÂåñ` `Âü∫Á°ÄÊ®°Âûã` `Flow Matching` `Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂà©Áî®ÊñáÊú¨Á≠âÊ®°ÊÄÅ‰∏≠ÁöÑÈ¢ÜÂüüÁü•ËØÜÔºåÈôêÂà∂‰∫ÜË∑®È¢ÜÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. AuroraÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑÂ§öÊ®°ÊÄÅÊó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãÔºåËá™ÈÄÇÂ∫îÊèêÂèñÂπ∂Âà©Áî®ÊñáÊú¨ÂíåÂõæÂÉè‰∏≠ÁöÑÈ¢ÜÂüüÁü•ËØÜ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåAuroraÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåÂçïÊ®°ÊÄÅÂíåÂ§öÊ®°ÊÄÅÂú∫ÊôØ‰∏ãÂùáÂèñÂæó‰∫ÜÈ¢ÜÂÖàÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ë∑®È¢ÜÂüüÊ≥õÂåñÂú®Êó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂõ†‰∏∫Áõ∏‰ººÁöÑÂéÜÂè≤‰ø°ÊÅØÂèØËÉΩÁî±‰∫éÈ¢ÜÂüüÁâπÂÆöÁâπÂæÅËÄåÂØºËá¥‰∏çÂêåÁöÑÊú™Êù•Ë∂ãÂäø„ÄÇÁõÆÂâçÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÊûÑÂª∫ÂçïÊ®°ÊÄÅÊó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãÂíåÁ´ØÂà∞Á´ØÂ§öÊ®°ÊÄÅÁõëÁù£Ê®°Âûã„ÄÇÂâçËÄÖÁº∫‰πèÂØπÊñáÊú¨Á≠âÊ®°ÊÄÅ‰∏≠È¢ÜÂüüÁâπÂÆöÁü•ËØÜÁöÑÊòæÂºèÂà©Áî®Ôºå‰ªéËÄåÈòªÁ¢ç‰∫ÜÊÄßËÉΩ„ÄÇÂêéËÄÖÂàôÈíàÂØπÁ´ØÂà∞Á´ØÂú∫ÊôØÂÆöÂà∂Ôºå‰∏çÊîØÊåÅË∑®È¢ÜÂüüÂú∫ÊôØÁöÑÈõ∂Ê†∑Êú¨Êé®ÁêÜ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜAuroraÔºå‰∏Ä‰∏™Â§öÊ®°ÊÄÅÊó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãÔºåÂÆÉÊîØÊåÅÂ§öÊ®°ÊÄÅËæìÂÖ•ÂíåÈõ∂Ê†∑Êú¨Êé®ÁêÜ„ÄÇAuroraÂú®Ë∑®È¢ÜÂüüÂ§öÊ®°ÊÄÅÊó∂Èó¥Â∫èÂàóËØ≠ÊñôÂ∫ì‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞ÊèêÂèñÂíåÂÖ≥Ê≥®Áõ∏Â∫îÊñáÊú¨ÊàñÂõæÂÉèÊ®°ÊÄÅ‰∏≠ÂåÖÂê´ÁöÑÂÖ≥ÈîÆÈ¢ÜÂüüÁü•ËØÜÔºå‰ªéËÄåÂÖ∑ÊúâÂº∫Â§ßÁöÑË∑®È¢ÜÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøátokenization„ÄÅÁºñÁ†ÅÂíåËí∏È¶èÔºåAuroraÂèØ‰ª•ÊèêÂèñÂ§öÊ®°ÊÄÅÈ¢ÜÂüüÁü•ËØÜ‰Ωú‰∏∫ÊåáÂØºÔºåÁÑ∂ÂêéÂà©Áî®Ê®°ÊÄÅÂºïÂØºÁöÑÂ§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÂ∞ÜÂÖ∂Ê≥®ÂÖ•Âà∞Êó∂Èó¥Ë°®Á§∫ÁöÑÂª∫Ê®°‰∏≠„ÄÇÂú®Ëß£Á†ÅÈò∂ÊÆµÔºåÂ§öÊ®°ÊÄÅË°®Á§∫Áî®‰∫éÁîüÊàêÊú™Êù•tokenÁöÑÊù°‰ª∂ÂíåÂéüÂûãÔºå‰ªéËÄå‰øÉÊàê‰∫Ü‰∏ÄÁßçÁî®‰∫éÁîüÊàêÊ¶ÇÁéáÈ¢ÑÊµãÁöÑÊñ∞ÂûãÂéüÂûãÂºïÂØºÁöÑFlow Matching„ÄÇÂú®ÂÖ¨ËÆ§ÁöÑÂü∫ÂáÜÊµãËØïÔºàÂåÖÊã¨TimeMMD„ÄÅTSFM-BenchÂíåProbTSÔºâ‰∏äËøõË°åÁöÑÁªºÂêàÂÆûÈ™åË°®ÊòéÔºåAuroraÂú®ÂçïÊ®°ÊÄÅÂíåÂ§öÊ®°ÊÄÅÂú∫ÊôØ‰∏≠ÂùáÂÖ∑Êúâ‰∏ÄËá¥ÁöÑstate-of-the-artÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Ë∑®È¢ÜÂüüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰∏≠ÔºåÁé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂà©Áî®Â§öÊ®°ÊÄÅ‰ø°ÊÅØÔºàÂ¶ÇÊñáÊú¨„ÄÅÂõæÂÉèÔºâ‰∏≠Ëï¥Âê´ÁöÑÈ¢ÜÂüüÁü•ËØÜÔºåÂØºËá¥Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïË¶Å‰πà‰æßÈáç‰∫éÂçïÊ®°ÊÄÅÊó∂Èó¥Â∫èÂàóÂª∫Ê®°ÔºåÂøΩÁï•‰∫ÜÂÖ∂‰ªñÊ®°ÊÄÅÁöÑ‰ø°ÊÅØÔºõË¶Å‰πàÊòØÁ´ØÂà∞Á´ØÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÁº∫‰πèË∑®È¢ÜÂüüÈõ∂Ê†∑Êú¨Êé®ÁêÜËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Â§öÊ®°ÊÄÅÊó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãAuroraÔºåÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑÊñπÂºèÂ≠¶‰π†Ë∑®È¢ÜÂüüÁöÑÂ§öÊ®°ÊÄÅÁü•ËØÜÔºåÂπ∂Âà©Áî®Ëøô‰∫õÁü•ËØÜÊåáÂØºÊó∂Èó¥Â∫èÂàóÁöÑÈ¢ÑÊµã„ÄÇAuroraËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞‰ªéÊñáÊú¨ÂíåÂõæÂÉèÁ≠âÊ®°ÊÄÅ‰∏≠ÊèêÂèñÈ¢ÜÂüüÁü•ËØÜÔºåÂπ∂Â∞ÜÂÖ∂Ê≥®ÂÖ•Âà∞Êó∂Èó¥Â∫èÂàóÁöÑÂª∫Ê®°ËøáÁ®ã‰∏≠Ôºå‰ªéËÄåÊèêÈ´òË∑®È¢ÜÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAuroraÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) Â§öÊ®°ÊÄÅÁü•ËØÜÊèêÂèñÔºöÈÄöËøátokenization„ÄÅÁºñÁ†ÅÂíåËí∏È¶èÁ≠âÊäÄÊúØÔºå‰ªéÊñáÊú¨ÂíåÂõæÂÉèÁ≠âÊ®°ÊÄÅ‰∏≠ÊèêÂèñÈ¢ÜÂüüÁü•ËØÜ„ÄÇ2) Êó∂Èó¥Â∫èÂàóÂª∫Ê®°ÔºöÂà©Áî®Ê®°ÊÄÅÂºïÂØºÁöÑÂ§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂ∞ÜÊèêÂèñÁöÑÈ¢ÜÂüüÁü•ËØÜÊ≥®ÂÖ•Âà∞Êó∂Èó¥Â∫èÂàóÁöÑË°®Á§∫‰∏≠„ÄÇ3) Ê¶ÇÁéáÈ¢ÑÊµãÔºö‰ΩøÁî®ÂéüÂûãÂºïÂØºÁöÑFlow MatchingÊñπÊ≥ïÔºåÂü∫‰∫éÂ§öÊ®°ÊÄÅË°®Á§∫ÁîüÊàêÊú™Êù•tokenÁöÑÊù°‰ª∂ÂíåÂéüÂûãÔºåËøõË°åÊ¶ÇÁéáÈ¢ÑÊµã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAuroraÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§öÊ®°ÊÄÅÊó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãÔºåËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÊó∂Èó¥Â∫èÂàó„ÄÅÊñáÊú¨ÂíåÂõæÂÉèÁ≠âÂ§öÁßçÊ®°ÊÄÅÁöÑËæìÂÖ•„ÄÇ2) ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊ®°ÊÄÅÂºïÂØºÁöÑÂ§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜÈ¢ÜÂüüÁü•ËØÜÊ≥®ÂÖ•Âà∞Êó∂Èó¥Â∫èÂàóÁöÑÂª∫Ê®°‰∏≠„ÄÇ3) ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂéüÂûãÂºïÂØºÁöÑFlow MatchingÊñπÊ≥ïÔºåÁî®‰∫éÁîüÊàêÊ¶ÇÁéáÈ¢ÑÊµãÔºåÊèêÈ´ò‰∫ÜÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöAuroraÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®TransformerÊû∂ÊûÑ‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÔºåËøõË°åÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÁºñÁ†ÅÂíåËûçÂêà„ÄÇ2) ËÆæËÆ°‰∫Ü‰∏ìÈó®ÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éÊåáÂØºÂ§öÊ®°ÊÄÅÁü•ËØÜÁöÑÊèêÂèñÂíåÊó∂Èó¥Â∫èÂàóÁöÑÂª∫Ê®°„ÄÇ3) ÈááÁî®‰∫ÜËí∏È¶èÊäÄÊúØÔºåÂ∞ÜÈ¢ÜÂüüÁü•ËØÜ‰ªéÂ§ßÂûãÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËøÅÁßªÂà∞AuroraÊ®°Âûã‰∏≠„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

AuroraÂú®TimeMMD„ÄÅTSFM-BenchÂíåProbTSÁ≠âÂ§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂú®Ë∑®È¢ÜÂüüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰ªªÂä°‰∏≠ÔºåAuroraÁõ∏ÊØî‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂú®È¢ÑÊµãÁ≤æÂ∫¶ÂíåÊ≥õÂåñËÉΩÂäõÊñπÈù¢ÂùáÊúâÊòæËëóÊèêÂçá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAuroraËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Â§öÊ®°ÊÄÅ‰ø°ÊÅØÔºåÊèêÈ´òÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

AuroraÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÔºö‰æõÂ∫îÈìæÁÆ°ÁêÜ„ÄÅÈáëËûçÈ£éÈô©È¢ÑÊµã„ÄÅÊô∫ËÉΩ‰∫§ÈÄö„ÄÅËÉΩÊ∫êÊ∂àËÄóÈ¢ÑÊµãÁ≠â„ÄÇÈÄöËøáÂà©Áî®Â§öÊ®°ÊÄÅ‰ø°ÊÅØÔºåAuroraÂèØ‰ª•ÊèêÈ´òÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄßÔºå‰∏∫ÂÜ≥Á≠ñÊèê‰æõÊõ¥ÊúâÊïàÁöÑÊîØÊåÅ„ÄÇÊú™Êù•ÔºåAuroraÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Êõ¥Â§öÁöÑÈ¢ÜÂüüÂíåÂ∫îÁî®Âú∫ÊôØÔºå‰æãÂ¶ÇÔºöÂåªÁñóÂÅ•Â∫∑„ÄÅÁéØÂ¢ÉÁõëÊµãÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Cross-domain generalization is very important in Time Series Forecasting because similar historical information may lead to distinct future trends due to the domain-specific characteristics. Recent works focus on building unimodal time series foundation models and end-to-end multimodal supervised models. Since domain-specific knowledge is often contained in modalities like texts, the former lacks the explicit utilization of them, thus hindering the performance. The latter is tailored for end-to-end scenarios and does not support zero-shot inference for cross-domain scenarios. In this work, we introduce Aurora, a Multimodal Time Series Foundation Model, which supports multimodal inputs and zero-shot inference. Pretrained on Corss-domain Multimodal Time Series Corpus, Aurora can adaptively extract and focus on key domain knowledge contained in corrsponding text or image modalities, thus possessing strong Cross-domain generalization capability. Through tokenization, encoding, and distillation, Aurora can extract multimodal domain knowledge as guidance and then utilizes a Modality-Guided Multi-head Self-Attention to inject them into the modeling of temporal representations. In the decoding phase, the multimodal representations are used to generate the conditions and prototypes of future tokens, contributing to a novel Prototype-Guided Flow Matching for generative probabilistic forecasting. Comprehensive experiments on well-recognized benchmarks, including TimeMMD, TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art performance of Aurora on both unimodal and multimodal scenarios.

