---
layout: default
title: Adaptive Margin RLHF via Preference over Preferences
---

# Adaptive Margin RLHF via Preference over Preferences

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22851" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.22851v3</a>
  <a href="https://arxiv.org/pdf/2509.22851.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22851v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22851v3', 'Adaptive Margin RLHF via Preference over Preferences')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yaswanth Chittepu, Prasann Singhal, Greg Durrett, Scott Niekum

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-26 (Êõ¥Êñ∞: 2025-11-30)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DPO-PoPÔºåÂà©Áî®ÂÅèÂ•Ω‰πã‰∏äÁöÑÂÅèÂ•Ω‰ø°ÊÅØËá™ÈÄÇÂ∫îË∞ÉÊï¥ËæπÈôÖÔºåÊèêÂçáRLHFÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†` `ÂÅèÂ•ΩÂ≠¶‰π†` `Ëá™ÈÄÇÂ∫îËæπÈôÖ` `Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ` `ÂÅèÂ•Ω‰πã‰∏äÁöÑÂÅèÂ•Ω`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâRLHFÊñπÊ≥ïÂú®Â§ÑÁêÜ‰∫∫Á±ªÂÅèÂ•ΩÊó∂ÔºåÊú™ËÉΩÂÖÖÂàÜËÄÉËôë‰∏çÂêåÂÅèÂ•ΩÂº∫Â∫¶Â∑ÆÂºÇÔºåÂØºËá¥Ê≥õÂåñËÉΩÂäõÂèóÈôê„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫DPO-PoPÔºåÂà©Áî®‚ÄúÂÅèÂ•Ω‰πã‰∏äÁöÑÂÅèÂ•Ω‚Äù‰ø°ÊÅØÔºå‰∏∫ÊØè‰∏™Êï∞ÊçÆÁÇπËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥ËæπÈôÖÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞Âª∫Ê®°ÂÅèÂ•ΩÂº∫Â∫¶„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDPO-PoPÂú®UltraFeedbackÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâDPOÊñπÊ≥ïÔºåÂπ∂Âú®Âà§Âà´ÂíåÁîüÊàêÊÄßËÉΩ‰πãÈó¥ÂÆûÁé∞‰∫ÜÊõ¥Â•ΩÁöÑÂπ≥Ë°°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âü∫‰∫éËæπÈôÖÁöÑ‰ºòÂåñÂØπ‰∫éÊèêÂçáÂàÜÁ±ª‰ªªÂä°ÁöÑÊ≥õÂåñÊÄßÂíåÈ≤ÅÊ£íÊÄßËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†ÔºàRLHFÔºâ‰∏≠Ôºå‰ªéÂÅèÂ•Ω‰∏≠Â≠¶‰π†Â•ñÂä±Ê®°ÂûãÊó∂ÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÊó†ËæπÈôÖ„ÄÅÂõ∫ÂÆöËæπÈôÖÊàñÂÅèÂ•ΩËØÑÂàÜÁöÑÁÆÄÂçïÂáΩÊï∞ËæπÈôÖ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏Êó†Ê≥ïËß£Èáä‰∏çÂêåÂÅèÂ•ΩÁöÑÂº∫Â∫¶Â∑ÆÂºÇÔºå‰æãÂ¶ÇÔºåÊüê‰∫õÂÅèÂ•Ω‰∏éËæÉÂ§ßÁöÑÂìçÂ∫îËæπÈôÖÁõ∏ÂÖ≥ËÅîÔºåÊàñËÄÖÂÆÉ‰ª¨‰æùËµñ‰∫é‰ªéËØÑÂàÜ‰∏≠Ëé∑ÂæóÁöÑÂô™Â£∞ËæπÈôÖ‰ø°ÊÅØ„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåÂØπÂÅèÂ•ΩÂº∫Â∫¶ËøõË°åÂª∫Ê®°ÂèØ‰ª•Â∏¶Êù•Êõ¥Â•ΩÁöÑÊ≥õÂåñÂíåÊõ¥Âø†ÂÆûÁöÑÂØπÈΩê„ÄÇÊ≠§Â§ñÔºåËÆ∏Â§ö‰ΩøÁî®Ëá™ÈÄÇÂ∫îËæπÈôÖÁöÑÁé∞ÊúâÊñπÊ≥ïÂÅáËÆæÂèØ‰ª•ËÆøÈóÆÂáÜÁ°ÆÁöÑÂÅèÂ•ΩÂàÜÊï∞ÔºåËÄå‰∫∫Á±ªÂæàÈöæÂèØÈù†Âú∞Êèê‰æõËøô‰∫õÂàÜÊï∞„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂà©Áî®ÂÅèÂ•Ω‰πã‰∏äÁöÑÂÅèÂ•ΩÔºàpreference-over-preferenceÔºâÁöÑÊñπÊ≥ïÔºåÂç≥ÊåáÁ§∫‰∏§‰∏™ÂÅèÂ•Ω‰∏≠Âì™‰∏™ÂèçÊò†‰∫ÜÊõ¥Âº∫ÁöÑÂå∫ÂàÜÂ∫¶ÁöÑÊ†áÊ≥®„ÄÇÊàë‰ª¨‰ΩøÁî®ËøôÁßçÂ∫èÊï∞‰ø°Âè∑Êù•Êé®Êñ≠ÊØè‰∏™Êï∞ÊçÆÁÇπÁöÑËá™ÈÄÇÂ∫îËæπÈôÖ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÁöÑÊâ©Â±ïÔºåDPO-PoPÔºåÂÆÉÁªìÂêà‰∫ÜÊù•Ëá™ÂÅèÂ•Ω‰πã‰∏äÁöÑÂÅèÂ•ΩÁõëÁù£ÁöÑËá™ÈÄÇÂ∫îËæπÈôÖÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂà§Âà´ÂíåÁîüÊàêÊÄßËÉΩ„ÄÇÂú®UltraFeedbackÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ºò‰∫évanilla DPO„ÄÅÂÖ∑ÊúâÂõ∫ÂÆöËæπÈôÖÁöÑDPOÂíåÂÖ∑Êúâground-truthËæπÈôÖÁöÑDPO„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Ë°®ÊòéÂà§Âà´ÊÄßËÉΩÂíåÁîüÊàêÊÄßËÉΩ‰πãÈó¥Â≠òÂú®ÊùÉË°°ÔºöÊèêÈ´òÊµãËØïÂàÜÁ±ªÂáÜÁ°ÆÁéáÔºåÁâπÂà´ÊòØÈÄöËøáÁâ∫Áâ≤ËæÉÂº∫ÂÅèÂ•ΩÊù•Ê≠£Á°ÆÊ†áËÆ∞ËæÉÂº±ÂÅèÂ•ΩÔºåÂèØËÉΩ‰ºöÂØºËá¥ÁîüÊàêË¥®Èáè‰∏ãÈôç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ËøôÁßçÊùÉË°°ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÁßçÈááÊ†∑Á≠ñÁï•Êù•Êî∂ÈõÜÂÅèÂ•Ω‰πã‰∏äÁöÑÂÅèÂ•ΩÊ†áÁ≠æÔºö‰∏ÄÁßçÂÅèÂêë‰∫éÂà§Âà´ÊÄßËÉΩÔºåÂè¶‰∏ÄÁßçÂÅèÂêë‰∫éÁîüÊàêÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâRLHFÊñπÊ≥ïÂú®‰ªé‰∫∫Á±ªÂÅèÂ•Ω‰∏≠Â≠¶‰π†Â•ñÂä±Ê®°ÂûãÊó∂ÔºåÈÄöÂ∏∏‰ΩøÁî®Âõ∫ÂÆöÊàñÁÆÄÂçïÁöÑËæπÈôÖÂáΩÊï∞ÔºåÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜ‰∏çÂêåÂÅèÂ•ΩÁöÑÂº∫Â∫¶Â∑ÆÂºÇ„ÄÇ‰∫∫Á±ªÊèê‰æõÁöÑÂÅèÂ•ΩËØÑÂàÜÂèØËÉΩÂ≠òÂú®Âô™Â£∞ÔºåÂØºËá¥ËæπÈôÖ‰ø°ÊÅØ‰∏çÂáÜÁ°ÆÔºåÂΩ±ÂìçÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂØπÈΩêÊïàÊûú„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÊõ¥ÂáÜÁ°ÆÂú∞Âª∫Ê®°ÂíåÂà©Áî®‰∫∫Á±ªÂÅèÂ•ΩÂº∫Â∫¶ÊòØ‰∫üÂæÖËß£ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®‚ÄúÂÅèÂ•Ω‰πã‰∏äÁöÑÂÅèÂ•Ω‚Äù‰ø°ÊÅØÔºåÂç≥‰∫∫Á±ªÂØπ‰∏§‰∏™ÂÅèÂ•Ω‰πãÈó¥Áõ∏ÂØπÂº∫Â∫¶ÁöÑÂà§Êñ≠ÔºåÊù•Êé®Êñ≠ÊØè‰∏™Êï∞ÊçÆÁÇπÁöÑËá™ÈÄÇÂ∫îËæπÈôÖ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£‰∏çÂêåÂÅèÂ•ΩÁöÑÈáçË¶ÅÊÄßÔºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞Â≠¶‰π†Â•ñÂä±Ê®°Âûã„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫ÜÁõ¥Êé•‰æùËµñ‰∏çÂáÜÁ°ÆÁöÑÂÅèÂ•ΩËØÑÂàÜÔºåËÄåÊòØÂà©Áî®Â∫èÊï∞‰ø°ÊÅØÊù•Êé®Êñ≠ËæπÈôÖ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDPO-PoPÊòØÂü∫‰∫éDirect Preference Optimization (DPO) ÁöÑÊâ©Â±ï„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö1) Êî∂ÈõÜ‚ÄúÂÅèÂ•Ω‰πã‰∏äÁöÑÂÅèÂ•Ω‚ÄùÊï∞ÊçÆÔºåÂç≥ÂØπ‰∫é‰∏§ÁªÑ(prompt, response1, response2)Êï∞ÊçÆÔºåÊ†áÊ≥®Âì™‰∏ÄÁªÑÁöÑÂÅèÂ•ΩÊõ¥Âº∫Ôºõ2) ‰ΩøÁî®Ëøô‰∫õÊï∞ÊçÆÊé®Êñ≠ÊØè‰∏™Êï∞ÊçÆÁÇπÁöÑËá™ÈÄÇÂ∫îËæπÈôÖÔºõ3) Â∞ÜËøô‰∫õËá™ÈÄÇÂ∫îËæπÈôÖËûçÂÖ•Âà∞DPOÁöÑÊçüÂ§±ÂáΩÊï∞‰∏≠ÔºåËÆ≠ÁªÉÂ•ñÂä±Ê®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂà©Áî®‚ÄúÂÅèÂ•Ω‰πã‰∏äÁöÑÂÅèÂ•Ω‚Äù‰ø°ÊÅØÊù•Â≠¶‰π†Ëá™ÈÄÇÂ∫îËæπÈôÖ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåDPO-PoP‰∏çÈúÄË¶Å‰æùËµñÂáÜÁ°ÆÁöÑÂÅèÂ•ΩËØÑÂàÜÔºåËÄåÊòØÂà©Áî®Â∫èÊï∞‰ø°ÊÅØÊù•Êé®Êñ≠ËæπÈôÖÔºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞Â§ÑÁêÜÂô™Â£∞Êï∞ÊçÆÂíå‰∏çÂêåÂÅèÂ•ΩÂº∫Â∫¶„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏§ÁßçÈááÊ†∑Á≠ñÁï•Ôºå‰ª•Âπ≥Ë°°Âà§Âà´ÊÄßËÉΩÂíåÁîüÊàêÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDPO-PoPÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®‰∏Ä‰∏™Á•ûÁªèÁΩëÁªúÊù•È¢ÑÊµãÊØè‰∏™Êï∞ÊçÆÁÇπÁöÑËæπÈôÖÂ§ßÂ∞èÔºåËØ•ÁΩëÁªúÁöÑËæìÂÖ•ÊòØpromptÂíå‰∏§‰∏™responseÁöÑembeddingÔºõ2) Â∞ÜÈ¢ÑÊµãÁöÑËæπÈôÖÂ§ßÂ∞èËûçÂÖ•Âà∞DPOÁöÑÊçüÂ§±ÂáΩÊï∞‰∏≠ÔºåÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊçüÂ§±ÂáΩÊï∞Âèò‰∏∫‰∏Ä‰∏™Âä†ÊùÉÁöÑ‰∫§ÂèâÁÜµÊçüÂ§±ÔºåÊùÉÈáçÁî±È¢ÑÊµãÁöÑËæπÈôÖÂ§ßÂ∞èÂÜ≥ÂÆöÔºõ3) ÊèêÂá∫‰∫Ü‰∏§ÁßçÈááÊ†∑Á≠ñÁï•Ôºå‰∏ÄÁßçÊòØÈÄâÊã©ÈÇ£‰∫õÊ®°ÂûãÈ¢ÑÊµãÈîôËØØ‰ΩÜ‰∫∫Á±ªÊ†áÊ≥®‰∏∫Âº∫ÂÅèÂ•ΩÁöÑÊï∞ÊçÆÔºåÂè¶‰∏ÄÁßçÊòØÈÄâÊã©ÈÇ£‰∫õÊ®°ÂûãÈ¢ÑÊµãÊ≠£Á°Æ‰ΩÜ‰∫∫Á±ªÊ†áÊ≥®‰∏∫Âº±ÂÅèÂ•ΩÁöÑÊï∞ÊçÆ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDPO-PoPÂú®UltraFeedbackÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫évanilla DPOÂíå‰ΩøÁî®Âõ∫ÂÆöËæπÈôÖÁöÑDPO„ÄÇ‰æãÂ¶ÇÔºåÂú®ÊµãËØïÈõÜ‰∏äÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫ÜX%ÔºåÁîüÊàêË¥®ÈáèÔºàÈÄöËøá‰∫∫Â∑•ËØÑ‰º∞ÔºâÊèêÈ´ò‰∫ÜY%„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂ±ïÁ§∫‰∫ÜÂà§Âà´ÊÄßËÉΩÂíåÁîüÊàêÊÄßËÉΩ‰πãÈó¥ÁöÑÊùÉË°°ÔºåÂπ∂ÊèêÂá∫‰∫ÜÊúâÊïàÁöÑÈááÊ†∑Á≠ñÁï•Êù•Âπ≥Ë°°‰∏§ËÄÖ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶Å‰ªé‰∫∫Á±ªÂèçÈ¶à‰∏≠Â≠¶‰π†ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂØπËØùÁ≥ªÁªü„ÄÅÊñáÊú¨ÁîüÊàê„ÄÅ‰ª£Á†ÅÁîüÊàêÁ≠â„ÄÇÈÄöËøáÊõ¥ÂáÜÁ°ÆÂú∞Âª∫Ê®°‰∫∫Á±ªÂÅèÂ•ΩÔºåÂèØ‰ª•ÊèêÂçáÁîüÊàêÂÜÖÂÆπÁöÑË¥®Èáè„ÄÅÁõ∏ÂÖ≥ÊÄßÂíåÂÆâÂÖ®ÊÄßÔºå‰ªéËÄåÊîπÂñÑÁî®Êà∑‰ΩìÈ™åÔºåÂπ∂ÂáèÂ∞ëÊΩúÂú®ÁöÑÈ£éÈô©„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Margin-based optimization is fundamental to improving generalization and robustness in classification tasks. In the context of reward model learning from preferences within Reinforcement Learning from Human Feedback (RLHF), existing methods typically rely on no margins, fixed margins, or margins that are simplistic functions of preference ratings. However, such formulations often fail to account for the varying strengths of different preferences, for example some preferences are associated with larger margins between responses, or they rely on noisy margin information derived from ratings. We argue that modeling the strength of preferences can lead to better generalization and more faithful alignment. Furthermore, many existing methods that use adaptive margins assume access to accurate preference scores, which can be difficult for humans to provide reliably. We propose an approach that leverages preferences over preferences, that is annotations indicating which of two preferences reflects a stronger distinction. We use this ordinal signal to infer adaptive margins on a per-datapoint basis. We introduce an extension to Direct Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from preference-over-preference supervision, enabling improved discriminative and generative performance. Empirically, our method outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. Additionally, we show that there is a tradeoff between discriminative and generative performance: improving test classification accuracy, particularly by correctly labeling weaker preferences at the expense of stronger ones, can lead to a decline in generative quality. To navigate this tradeoff, we propose two sampling strategies to gather preference-over-preference labels: one favoring discriminative performance and one favoring generative performance.

