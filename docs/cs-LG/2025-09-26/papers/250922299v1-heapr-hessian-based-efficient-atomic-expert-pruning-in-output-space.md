---
layout: default
title: HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space
---

# HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22299" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.22299v1</a>
  <a href="https://arxiv.org/pdf/2509.22299.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22299v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22299v1', 'HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ke Li, Zheng Yang, Zhongbin Zhou, Feng Xue, Zhonglin Jiang, Wenxiao Wang

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-26

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/LLIKKE/HEAPr)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**HEAPrÔºöÂü∫‰∫éHessianÁöÑËæìÂá∫Á©∫Èó¥È´òÊïàÂéüÂ≠ê‰∏ìÂÆ∂Ââ™ÊûùÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê∑∑Âêà‰∏ìÂÆ∂Ê®°Âûã` `Ê®°ÂûãÂâ™Êûù` `ÂéüÂ≠ê‰∏ìÂÆ∂` `‰∫åÈò∂‰ø°ÊÅØ` `HessianÁü©Èòµ` `Ê®°ÂûãÂéãÁº©` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. MoEÊ®°ÂûãÂèÇÊï∞ÈáèÂ∑®Â§ßÔºåÈÉ®ÁΩ≤Âõ∞ÈöæÔºåÁé∞Êúâ‰∏ìÂÆ∂Á∫ßÂà´Ââ™ÊûùÁ≤íÂ∫¶Á≤óÔºåÁ≤æÂ∫¶ÊçüÂ§±Â§ß„ÄÇ
2. HEAPrÂ∞Ü‰∏ìÂÆ∂ÂàÜËß£‰∏∫ÂéüÂ≠ê‰∏ìÂÆ∂ÔºåÂà©Áî®Hessian‰∫åÈò∂‰ø°ÊÅØËØÑ‰º∞ÂéüÂ≠ê‰∏ìÂÆ∂ÁöÑÈáçË¶ÅÊÄßÔºåÂÆûÁé∞Êõ¥Á≤æÁªÜÁöÑÂâ™Êûù„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåHEAPrÂú®DeepSeek MoEÂíåQwen MoEÊ®°Âûã‰∏äÔºåËÉΩ‰ª•20%-25%ÂéãÁº©ÁéáÂÆûÁé∞Ëøë‰πéÊó†ÊçüÁöÑÂéãÁº©ÔºåÂπ∂Èôç‰ΩéÁ∫¶20%ÁöÑFLOPs„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫HEAPrÁöÑÊñ∞ÂûãÂâ™ÊûùÁÆóÊ≥ïÔºåÁî®‰∫éÂéãÁº©Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊû∂ÊûÑ„ÄÇ‰∏éÁé∞Êúâ‰∏ªË¶ÅÂÖ≥Ê≥®‰∏ìÂÆ∂Á∫ßÂà´Ââ™ÊûùÁöÑÊñπÊ≥ï‰∏çÂêåÔºåHEAPrÂ∞Ü‰∏ìÂÆ∂ÂàÜËß£‰∏∫Êõ¥Â∞èÁöÑ„ÄÅ‰∏çÂèØÂàÜÂâ≤ÁöÑÂéüÂ≠ê‰∏ìÂÆ∂Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥Á≤æÁ°ÆÂíåÁÅµÊ¥ªÁöÑÂâ™Êûù„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Âü∫‰∫éÊúÄ‰ºòËÑëÂ§ñÁßëÂåªÁîüÔºàOBSÔºâÁêÜËÆ∫ÁöÑ‰∫åÈò∂‰ø°ÊÅØÊù•Ë°°ÈáèÊØè‰∏™ÂéüÂ≠ê‰∏ìÂÆ∂ÁöÑÈáçË¶ÅÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥‰∫åÈò∂‰ø°ÊÅØÂ∏¶Êù•ÁöÑËÆ°ÁÆóÂíåÂ≠òÂÇ®ÊåëÊàòÔºåHEAPrÂà©Áî®ÂéüÂ≠ê‰∏ìÂÆ∂ÁöÑÂõ∫ÊúâÂ±ûÊÄßÔºåÂ∞Ü‰∫åÈò∂‰ø°ÊÅØ‰ªé‰∏ìÂÆ∂ÂèÇÊï∞ËΩ¨Êç¢‰∏∫ÂéüÂ≠ê‰∏ìÂÆ∂ÂèÇÊï∞ÔºåÂπ∂Ëøõ‰∏ÄÊ≠•ÁÆÄÂåñ‰∏∫ÂéüÂ≠ê‰∏ìÂÆ∂ËæìÂá∫ÁöÑ‰∫åÈò∂‰ø°ÊÅØÔºå‰ªéËÄåÂ∞ÜÁ©∫Èó¥Â§çÊùÇÂ∫¶‰ªéO(d^4)Èôç‰ΩéÂà∞O(d^2)ÔºåÂÖ∂‰∏≠dÊòØÊ®°ÂûãÁª¥Â∫¶„ÄÇHEAPr‰ªÖÈúÄÂú®Â∞èÂûãÊ†°ÂáÜÈõÜ‰∏äËøõË°å‰∏§Ê¨°ÂâçÂêë‰º†ÈÄíÂíå‰∏ÄÊ¨°ÂèçÂêë‰º†ÈÄíÂç≥ÂèØËÆ°ÁÆóÂéüÂ≠ê‰∏ìÂÆ∂ÁöÑÈáçË¶ÅÊÄß„ÄÇÂú®DeepSeek MoEÂíåQwen MoEÁ≥ªÂàóÁ≠âMoEÊ®°Âûã‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåHEAPrÂú®ÂêÑÁßçÂéãÁº©ÁéáÂíåÂü∫ÂáÜÊµãËØï‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÁöÑ‰∏ìÂÆ∂Á∫ßÂà´Ââ™ÊûùÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåHEAPrÂú®Â§ßÂ§öÊï∞Ê®°Âûã‰∏≠‰ª•20%~25%ÁöÑÂéãÁº©ÁéáÂÆûÁé∞‰∫ÜËøë‰πéÊó†ÊçüÁöÑÂéãÁº©ÔºåÂêåÊó∂ËøòÂ∞ÜFLOPsÈôç‰Ωé‰∫ÜËøë20%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠MoEÊû∂ÊûÑÂõ†ÂèÇÊï∞ÈáèÂ∑®Â§ßËÄåÈöæ‰ª•ÈÉ®ÁΩ≤ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑ‰∏ìÂÆ∂Á∫ßÂà´Ââ™ÊûùÊñπÊ≥ïÁ≤íÂ∫¶ËæÉÁ≤óÔºåÂÆπÊòìÂØºËá¥ÊòæËëóÁöÑÁ≤æÂ∫¶‰∏ãÈôçÔºåÊó†Ê≥ïÂú®ÂéãÁº©Ê®°ÂûãÁöÑÂêåÊó∂‰øùÊåÅÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜMoEÊ®°Âûã‰∏≠ÁöÑ‰∏ìÂÆ∂Ëøõ‰∏ÄÊ≠•ÂàÜËß£‰∏∫Êõ¥Â∞èÁöÑ‚ÄúÂéüÂ≠ê‰∏ìÂÆ∂‚ÄùÔºåÁÑ∂ÂêéÂØπËøô‰∫õÂéüÂ≠ê‰∏ìÂÆ∂ËøõË°åÈÄâÊã©ÊÄßÂâ™Êûù„ÄÇÈÄöËøáÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÂâ™ÊûùÔºåÂèØ‰ª•Êõ¥Á≤æÁ°ÆÂú∞ÂéªÈô§ÂÜó‰ΩôÂèÇÊï∞Ôºå‰ªéËÄåÂú®‰øùÊåÅÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂ÂÆûÁé∞Êõ¥È´òÁöÑÂéãÁº©Áéá„ÄÇÂà©Áî®HessianÁü©ÈòµÁöÑ‰∫åÈò∂‰ø°ÊÅØÊù•ËØÑ‰º∞ÊØè‰∏™ÂéüÂ≠ê‰∏ìÂÆ∂ÁöÑÈáçË¶ÅÊÄßÔºåÁ±ª‰ºº‰∫éOptimal Brain Surgeon (OBS) ÁêÜËÆ∫„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHEAPrÁÆóÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) **ÂéüÂ≠ê‰∏ìÂÆ∂ÂàÜËß£**ÔºöÂ∞ÜMoEÊ®°Âûã‰∏≠ÁöÑ‰∏ìÂÆ∂ÂàÜËß£‰∏∫Êõ¥Â∞èÁöÑÂéüÂ≠ê‰∏ìÂÆ∂„ÄÇÂÖ∑‰ΩìÂ¶Ç‰ΩïÂàÜËß£ËÆ∫Êñá‰∏≠Êú™ÊòéÁ°ÆËØ¥ÊòéÔºåÊú™Áü•„ÄÇ2) **‰∫åÈò∂‰ø°ÊÅØËÆ°ÁÆó**ÔºöÂà©Áî®HessianÁü©ÈòµËÆ°ÁÆóÊØè‰∏™ÂéüÂ≠ê‰∏ìÂÆ∂ÁöÑÈáçË¶ÅÊÄß„ÄÇ‰∏∫‰∫ÜÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÂ∞Ü‰∫åÈò∂‰ø°ÊÅØ‰ªé‰∏ìÂÆ∂ÂèÇÊï∞ËΩ¨Êç¢‰∏∫ÂéüÂ≠ê‰∏ìÂÆ∂ËæìÂá∫ÁöÑ‰∫åÈò∂‰ø°ÊÅØ„ÄÇ3) **ÂéüÂ≠ê‰∏ìÂÆ∂Ââ™Êûù**ÔºöÊ†πÊçÆËÆ°ÁÆóÂá∫ÁöÑÈáçË¶ÅÊÄßÔºåÂØπÂéüÂ≠ê‰∏ìÂÆ∂ËøõË°åÂâ™Êûù„ÄÇ4) **Ê®°ÂûãÂæÆË∞É**ÔºàÂèØÈÄâÔºâÔºöÂØπÂâ™ÊûùÂêéÁöÑÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ª•ÊÅ¢Â§çÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöHEAPrÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) **ÂéüÂ≠ê‰∏ìÂÆ∂Ââ™Êûù**ÔºöÊèêÂá∫‰∫Ü‰∏ÄÁßçÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÂâ™ÊûùÊñπÊ≥ïÔºåÂèØ‰ª•Êõ¥Á≤æÁ°ÆÂú∞ÂéªÈô§ÂÜó‰ΩôÂèÇÊï∞„ÄÇ2) **‰∫åÈò∂‰ø°ÊÅØÁÆÄÂåñ**ÔºöÈÄöËøáÂ∞Ü‰∫åÈò∂‰ø°ÊÅØ‰ªé‰∏ìÂÆ∂ÂèÇÊï∞ËΩ¨Êç¢‰∏∫ÂéüÂ≠ê‰∏ìÂÆ∂ËæìÂá∫ÁöÑ‰∫åÈò∂‰ø°ÊÅØÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) **ÂéüÂ≠ê‰∏ìÂÆ∂ÁöÑÂÆö‰πâ**ÔºöËÆ∫Êñá‰∏≠Ê≤°ÊúâÊòéÁ°ÆÁªôÂá∫ÂéüÂ≠ê‰∏ìÂÆ∂ÁöÑÂÆö‰πâÔºåÈúÄË¶ÅËøõ‰∏ÄÊ≠•Á†îÁ©∂‰ª£Á†ÅÊâçËÉΩÁ°ÆÂÆö„ÄÇ2) **‰∫åÈò∂‰ø°ÊÅØÁöÑËÆ°ÁÆóÊñπÊ≥ï**ÔºöËÆ∫ÊñáÈááÁî®‰∫ÜÁ±ª‰ºº‰∫éOBSÁêÜËÆ∫ÁöÑÊñπÊ≥ïÊù•ËÆ°ÁÆó‰∫åÈò∂‰ø°ÊÅØÔºåÂπ∂ËøõË°å‰∫ÜÁÆÄÂåñ‰ª•Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇÂÖ∑‰ΩìÁÆÄÂåñÊñπÊ≥ïÈúÄË¶ÅÂèÇËÄÉËÆ∫ÊñáÁªÜËäÇ„ÄÇ3) **Ââ™ÊûùÁ≠ñÁï•**ÔºöËÆ∫ÊñáÊ†πÊçÆÂéüÂ≠ê‰∏ìÂÆ∂ÁöÑÈáçË¶ÅÊÄßËøõË°åÂâ™ÊûùÔºåÂÖ∑‰ΩìÁöÑÂâ™ÊûùÁ≠ñÁï•Ôºà‰æãÂ¶ÇÔºåÂâ™ÊûùÊØî‰æã„ÄÅÂâ™ÊûùÈòàÂÄºÁ≠âÔºâÈúÄË¶ÅÂèÇËÄÉËÆ∫ÊñáÁªÜËäÇ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

HEAPrÂú®DeepSeek MoEÂíåQwen MoEÁ≥ªÂàóÊ®°Âûã‰∏äËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÔºåËØ•ÁÆóÊ≥ïÂú®ÂêÑÁßçÂéãÁº©ÁéáÂíåÂü∫ÂáÜÊµãËØï‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÁöÑ‰∏ìÂÆ∂Á∫ßÂà´Ââ™ÊûùÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåHEAPrÂú®Â§ßÂ§öÊï∞Ê®°Âûã‰∏≠‰ª•20%~25%ÁöÑÂéãÁº©ÁéáÂÆûÁé∞‰∫ÜËøë‰πéÊó†ÊçüÁöÑÂéãÁº©ÔºåÂêåÊó∂ËøòÂ∞ÜFLOPsÈôç‰Ωé‰∫ÜËøë20%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåHEAPrÊòØ‰∏ÄÁßçÈ´òÊïà‰∏îÊúâÊïàÁöÑMoEÊ®°ÂûãÂéãÁº©ÁÆóÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HEAPrÁÆóÊ≥ïÂèØÂ∫îÁî®‰∫éÂêÑÁßçÂü∫‰∫éMoEÊû∂ÊûÑÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰æãÂ¶ÇDeepSeek MoE„ÄÅQwen MoEÁ≠â„ÄÇÈÄöËøáËØ•ÁÆóÊ≥ïÔºåÂèØ‰ª•Âú®‰øùÊåÅÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂ÊòæËëóÈôç‰ΩéÊ®°ÂûãÁöÑÂ§ßÂ∞èÂíåËÆ°ÁÆóÂ§çÊùÇÂ∫¶Ôºå‰ªéËÄå‰ΩøÂæóËøô‰∫õÊ®°ÂûãËÉΩÂ§üÊõ¥ÂÆπÊòìÂú∞ÈÉ®ÁΩ≤Âú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§á„ÄÅËæπÁºòËÆæÂ§áÁ≠â„ÄÇÊ≠§Â§ñÔºåËØ•ÁÆóÊ≥ïËøòÂèØ‰ª•Áî®‰∫éÂä†ÈÄüÊ®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶ÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊïàÁéá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Mixture-of-Experts (MoE) architectures in large language models (LLMs) deliver exceptional performance and reduced inference costs compared to dense LLMs. However, their large parameter counts result in prohibitive memory requirements, limiting practical deployment. While existing pruning methods primarily focus on expert-level pruning, this coarse granularity often leads to substantial accuracy degradation. In this work, we introduce HEAPr, a novel pruning algorithm that decomposes experts into smaller, indivisible atomic experts, enabling more precise and flexible atomic expert pruning. To measure the importance of each atomic expert, we leverage second-order information based on principles similar to Optimal Brain Surgeon (OBS) theory. To address the computational and storage challenges posed by second-order information, HEAPr exploits the inherent properties of atomic experts to transform the second-order information from expert parameters into that of atomic expert parameters, and further simplifies it to the second-order information of atomic expert outputs. This approach reduces the space complexity from $O(d^4)$, where d is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward passes and one backward pass on a small calibration set to compute the importance of atomic experts. Extensive experiments on MoE models, including DeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing expert-level pruning methods across a wide range of compression ratios and benchmarks. Specifically, HEAPr achieves nearly lossless compression at compression ratios of 20% ~ 25% in most models, while also reducing FLOPs nearly by 20%. The code can be found at \href{https://github.com/LLIKKE/HEAPr}{https://github.com/LLIKKE/HEAPr}.

