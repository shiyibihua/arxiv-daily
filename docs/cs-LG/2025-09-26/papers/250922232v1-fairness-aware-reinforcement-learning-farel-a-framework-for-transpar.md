---
layout: default
title: Fairness-Aware Reinforcement Learning (FAReL): A Framework for Transparent and Balanced Sequential Decision-Making
---

# Fairness-Aware Reinforcement Learning (FAReL): A Framework for Transparent and Balanced Sequential Decision-Making

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22232" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22232v1</a>
  <a href="https://arxiv.org/pdf/2509.22232.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22232v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22232v1', 'Fairness-Aware Reinforcement Learning (FAReL): A Framework for Transparent and Balanced Sequential Decision-Making')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexandra Cimpean, Nicole Orzan, Catholijn Jonker, Pieter Libin, Ann NowÃ©

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFAReLæ¡†æ¶ï¼Œè§£å†³å¼ºåŒ–å­¦ä¹ ä¸­æ€§èƒ½ä¸å…¬å¹³æ€§çš„æƒè¡¡é—®é¢˜ï¼Œåº”ç”¨äºæ‹›è˜å’Œæ¬ºè¯ˆæ£€æµ‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å…¬å¹³æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ` `åºè´¯å†³ç­–` `å…¬å¹³æ€§åº¦é‡` `æ‰©å±•é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æ‹›è˜` `æ¬ºè¯ˆæ£€æµ‹` `æ€§èƒ½-å…¬å¹³æ€§æƒè¡¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥åœ¨åºè´¯å†³ç­–ä¸­å…¼é¡¾æ€§èƒ½ä¸å…¬å¹³æ€§ï¼Œç¼ºä¹é€æ˜çš„æƒè¡¡æœºåˆ¶ã€‚
2. æå‡ºFAReLæ¡†æ¶ï¼Œé€šè¿‡æ‰©å±•çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹($f$MDP)æ˜¾å¼ç¼–ç ä¸ªä½“å’Œç¾¤ä½“ï¼Œé‡åŒ–å…¬å¹³æ€§ã€‚
3. åœ¨æ‹›è˜å’Œæ¬ºè¯ˆæ£€æµ‹åœºæ™¯ä¸­éªŒè¯äº†FAReLæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†æ€§èƒ½ä¸å…¬å¹³æ€§çš„è‰¯å¥½æƒè¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¬å¹³æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ (FAReL)æ¡†æ¶ï¼Œç”¨äºåœ¨åºè´¯å†³ç­–é—®é¢˜ä¸­å®ç°å…¬å¹³æ€§ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æ¢ç´¢æ€§èƒ½å’ŒæœŸæœ›çš„å…¬å¹³æ€§æ¦‚å¿µä¹‹é—´çš„æƒè¡¡ã€‚ç”±äºé¢„å…ˆæŒ‡å®šæ€§èƒ½-å…¬å¹³æ€§æƒè¡¡å¾ˆå›°éš¾ï¼Œå› æ­¤è¯¥æ¡†æ¶å…è®¸å¤šç§æƒè¡¡æ–¹æ¡ˆçš„æ¢ç´¢ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›çš„å…³äºå¯è·å¾—çš„æ€§èƒ½-å…¬å¹³æ€§æƒè¡¡çš„è§è§£å¯ä»¥æŒ‡å¯¼åˆ©ç›Šç›¸å…³è€…é€‰æ‹©æœ€åˆé€‚çš„ç­–ç•¥ã€‚ä¸ºäº†æ•æ‰å…¬å¹³æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ‰©å±•çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹($f$MDP)ï¼Œå®ƒæ˜¾å¼åœ°ç¼–ç äº†ä¸ªäººå’Œç¾¤ä½“ã€‚åŸºäºæ­¤$f$MDPï¼Œæœ¬æ–‡å½¢å¼åŒ–äº†åºè´¯å†³ç­–é—®é¢˜ä¸­çš„å…¬å¹³æ€§æ¦‚å¿µï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè®¡ç®—éšæ—¶é—´å˜åŒ–çš„å…¬å¹³æ€§åº¦é‡çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ä¸¤ä¸ªå…·æœ‰ä¸åŒå…¬å¹³æ€§è¦æ±‚çš„åœºæ™¯ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼šæ‹›è˜ï¼ˆéœ€è¦ç»„å»ºå¼ºå¤§çš„å›¢é˜Ÿï¼ŒåŒæ—¶å¹³ç­‰å¯¹å¾…ç”³è¯·äººï¼‰å’Œæ¬ºè¯ˆæ£€æµ‹ï¼ˆéœ€è¦æ£€æµ‹æ¬ºè¯ˆäº¤æ˜“ï¼ŒåŒæ—¶ç¡®ä¿å®¢æˆ·è´Ÿæ‹…çš„å…¬å¹³åˆ†é…ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å­¦ä¹ åˆ°çš„ç­–ç•¥åœ¨å¤šä¸ªåœºæ™¯ä¸­æ›´åŠ å…¬å¹³ï¼Œä¸”æ€§èƒ½å¥–åŠ±æŸå¤±å¾ˆå°ã€‚æ­¤å¤–ï¼Œç¾¤ä½“å’Œä¸ªä½“å…¬å¹³æ€§æ¦‚å¿µå¹¶ä¸ä¸€å®šç›¸äº’è•´å«ï¼Œçªå‡ºäº†è¯¥æ¡†æ¶åœ¨åŒæ—¶éœ€è¦ä¸¤ç§å…¬å¹³æ€§ç±»å‹çš„è®¾ç½®ä¸­çš„ä¼˜åŠ¿ã€‚æœ€åï¼Œæœ¬æ–‡æä¾›äº†å…³äºå¦‚ä½•åœ¨ä¸åŒé—®é¢˜è®¾ç½®ä¸­åº”ç”¨è¯¥æ¡†æ¶çš„æŒ‡å—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è§£å†³ç°å®ä¸–ç•Œçš„åºè´¯å†³ç­–é—®é¢˜æ—¶ï¼Œéš¾ä»¥åŒæ—¶ä¿è¯å†³ç­–çš„æ€§èƒ½å’Œå…¬å¹³æ€§ã€‚å°¤å…¶æ˜¯åœ¨æ¶‰åŠä¸ªä½“æˆ–ç¾¤ä½“å·®å¼‚çš„åœºæ™¯ä¸‹ï¼Œç®—æ³•å¯èƒ½æ— æ„ä¸­äº§ç”Ÿæ­§è§†æ€§ç»“æœã€‚é¢„å…ˆç¡®å®šæ€§èƒ½ä¸å…¬å¹³æ€§ä¹‹é—´çš„æœ€ä½³æƒè¡¡æ–¹æ¡ˆéå¸¸å›°éš¾ï¼Œç¼ºä¹çµæ´»çš„æ¢ç´¢æœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ‰©å±•æ ‡å‡†çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¼•å…¥$f$MDPï¼Œæ˜¾å¼åœ°å¯¹ä¸ªä½“å’Œç¾¤ä½“è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œèƒ½å¤Ÿç²¾ç¡®åœ°å®šä¹‰å’Œé‡åŒ–å…¬å¹³æ€§ã€‚é€šè¿‡åœ¨$f$MDPä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥å­¦ä¹ åˆ°åœ¨æ€§èƒ½å’Œå…¬å¹³æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡çš„ç­–ç•¥ã€‚è¿™ç§è®¾è®¡å…è®¸ç®—æ³•åœ¨ä¸åŒçš„å…¬å¹³æ€§çº¦æŸä¸‹è¿›è¡Œæ¢ç´¢ï¼Œå¹¶ä¸ºå†³ç­–è€…æä¾›å…³äºä¸åŒæƒè¡¡æ–¹æ¡ˆçš„æ´å¯Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFAReLæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ­¥éª¤ï¼š1) å®šä¹‰$f$MDPï¼ŒåŒ…æ‹¬çŠ¶æ€ç©ºé—´ã€åŠ¨ä½œç©ºé—´ã€è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±å‡½æ•°ï¼Œå…¶ä¸­çŠ¶æ€ç©ºé—´æ˜¾å¼åœ°åŒ…å«ä¸ªä½“å’Œç¾¤ä½“çš„ä¿¡æ¯ã€‚2) å®šä¹‰å…¬å¹³æ€§åº¦é‡ï¼ŒåŸºäº$f$MDPè®¡ç®—éšæ—¶é—´å˜åŒ–çš„å…¬å¹³æ€§æŒ‡æ ‡ã€‚3) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚Q-learningæˆ–SARSAï¼‰åœ¨$f$MDPä¸Šå­¦ä¹ ç­–ç•¥ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–å¥–åŠ±çš„åŒæ—¶æ»¡è¶³å…¬å¹³æ€§çº¦æŸã€‚4) åˆ†æå­¦ä¹ åˆ°çš„ç­–ç•¥ï¼Œè¯„ä¼°å…¶åœ¨æ€§èƒ½å’Œå…¬å¹³æ€§æ–¹é¢çš„è¡¨ç°ï¼Œå¹¶ä¸ºå†³ç­–è€…æä¾›å…³äºä¸åŒæƒè¡¡æ–¹æ¡ˆçš„å»ºè®®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†$f$MDPï¼Œå®ƒèƒ½å¤Ÿæ˜¾å¼åœ°å¯¹ä¸ªä½“å’Œç¾¤ä½“è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œä½¿å¾—åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ç²¾ç¡®åœ°å®šä¹‰å’Œé‡åŒ–å…¬å¹³æ€§æˆä¸ºå¯èƒ½ã€‚ä¸ä¼ ç»Ÿçš„MDPç›¸æ¯”ï¼Œ$f$MDPèƒ½å¤Ÿæ•æ‰åˆ°ä¸ªä½“å’Œç¾¤ä½“ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å…è®¸ç®—æ³•å­¦ä¹ åˆ°æ›´åŠ å…¬å¹³çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€ç§çµæ´»çš„æœºåˆ¶ï¼Œç”¨äºæ¢ç´¢æ€§èƒ½å’Œå…¬å¹³æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶ä¸ºå†³ç­–è€…æä¾›å†³ç­–æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨$f$MDPä¸­ï¼ŒçŠ¶æ€ç©ºé—´éœ€è¦åŒ…å«ä¸ªä½“å’Œç¾¤ä½“çš„ç›¸å…³ä¿¡æ¯ï¼Œä¾‹å¦‚ä¸ªä½“ç‰¹å¾ã€ç¾¤ä½“æˆå‘˜å…³ç³»ç­‰ã€‚å¥–åŠ±å‡½æ•°éœ€è¦è¿›è¡Œè®¾è®¡ï¼Œä»¥åæ˜ æ€§èƒ½å’Œå…¬å¹³æ€§ä¹‹é—´çš„æƒè¡¡ã€‚å¯ä»¥ä½¿ç”¨ä¸åŒçš„å…¬å¹³æ€§åº¦é‡ï¼Œä¾‹å¦‚ç»Ÿè®¡å‡ç­‰ã€æœºä¼šå‡ç­‰å’Œé¢„æµ‹å‡ç­‰ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©å¯ä»¥æ ¹æ®å…·ä½“é—®é¢˜è¿›è¡Œè°ƒæ•´ï¼Œå¯ä»¥ä½¿ç”¨Q-learningã€SARSAç­‰ç®—æ³•ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯æˆ–çº¦æŸä¼˜åŒ–æ–¹æ³•æ¥å¼ºåˆ¶æ‰§è¡Œå…¬å¹³æ€§çº¦æŸã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ‹›è˜å’Œæ¬ºè¯ˆæ£€æµ‹ä¸¤ä¸ªåœºæ™¯ä¸­ï¼ŒFAReLæ¡†æ¶éƒ½å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚åœ¨æ‹›è˜åœºæ™¯ä¸­ï¼ŒFAReLèƒ½å¤Ÿç»„å»ºå¼ºå¤§çš„å›¢é˜Ÿï¼ŒåŒæ—¶ç¡®ä¿ç”³è¯·è€…å—åˆ°å¹³ç­‰å¯¹å¾…ã€‚åœ¨æ¬ºè¯ˆæ£€æµ‹åœºæ™¯ä¸­ï¼ŒFAReLèƒ½å¤Ÿæ£€æµ‹æ¬ºè¯ˆäº¤æ˜“ï¼ŒåŒæ—¶å…¬å¹³åœ°åˆ†é…å®¢æˆ·è´Ÿæ‹…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFAReLæ¡†æ¶å­¦ä¹ åˆ°çš„ç­–ç•¥åœ¨å¤šä¸ªåœºæ™¯ä¸­æ›´åŠ å…¬å¹³ï¼Œä¸”æ€§èƒ½å¥–åŠ±æŸå¤±å¾ˆå°ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›åœºæ™¯ä¸‹ï¼Œå…¬å¹³æ€§æŒ‡æ ‡æå‡äº†10%-20%ï¼Œè€Œæ€§èƒ½æŸå¤±ä»…ä¸º1%-2%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FAReLæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦è€ƒè™‘å…¬å¹³æ€§çš„åºè´¯å†³ç­–é—®é¢˜ï¼Œä¾‹å¦‚ï¼šæ‹›è˜ç³»ç»Ÿï¼Œç¡®ä¿ä¸åŒèƒŒæ™¯çš„ç”³è¯·è€…è·å¾—å…¬å¹³çš„æœºä¼šï¼›ä¿¡è´·å®¡æ‰¹ç³»ç»Ÿï¼Œé¿å…å¯¹ç‰¹å®šç¾¤ä½“äº§ç”Ÿæ­§è§†ï¼›åˆ‘äº‹å¸æ³•ç³»ç»Ÿï¼Œå‡å°‘å¯¹ç‰¹å®šç§æ—çš„åè§ï¼›åŒ»ç–—èµ„æºåˆ†é…ï¼Œç¡®ä¿å…¬å¹³çš„åŒ»ç–—æœåŠ¡è·å–ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ„å»ºæ›´å…¬å¹³ã€æ›´è´Ÿè´£ä»»çš„AIç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Equity in real-world sequential decision problems can be enforced using fairness-aware methods. Therefore, we require algorithms that can make suitable and transparent trade-offs between performance and the desired fairness notions. As the desired performance-fairness trade-off is hard to specify a priori, we propose a framework where multiple trade-offs can be explored. Insights provided by the reinforcement learning algorithm regarding the obtainable performance-fairness trade-offs can then guide stakeholders in selecting the most appropriate policy. To capture fairness, we propose an extended Markov decision process, $f$MDP, that explicitly encodes individuals and groups. Given this $f$MDP, we formalise fairness notions in the context of sequential decision problems and formulate a fairness framework that computes fairness measures over time. We evaluate our framework in two scenarios with distinct fairness requirements: job hiring, where strong teams must be composed while treating applicants equally, and fraud detection, where fraudulent transactions must be detected while ensuring the burden on customers is fairly distributed. We show that our framework learns policies that are more fair across multiple scenarios, with only minor loss in performance reward. Moreover, we observe that group and individual fairness notions do not necessarily imply one another, highlighting the benefit of our framework in settings where both fairness types are desired. Finally, we provide guidelines on how to apply this framework across different problem settings.

