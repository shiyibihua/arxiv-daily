---
layout: default
title: Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning
---

# Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22263" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22263v1</a>
  <a href="https://arxiv.org/pdf/2509.22263.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22263v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22263v1', 'Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nakyeong Yang, Dong-Kyum Kim, Jea Kwon, Minsung Kim, Kyomin Jung, Meeyoung Cha

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 15 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSsiuuæ–¹æ³•ï¼Œé€šè¿‡æŠ‘åˆ¶è™šå‡åå­¦ä¹ ç¥ç»å…ƒå®ç°è¯­è¨€æ¨¡å‹é²æ£’åå­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åå­¦ä¹ ` `éšç§ä¿æŠ¤` `è¯­è¨€æ¨¡å‹` `ç¥ç»å…ƒ` `æ­£åˆ™åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åå­¦ä¹ æ–¹æ³•æ˜“å—â€œé‡æ–°å­¦ä¹ â€æ”»å‡»ï¼Œæ— æ³•å½»åº•æ“¦é™¤ç›®æ ‡çŸ¥è¯†ï¼Œå­˜åœ¨å®‰å…¨éšæ‚£ã€‚
2. Ssiuuæ–¹æ³•é€šè¿‡å±æ€§å¼•å¯¼çš„æ­£åˆ™åŒ–ï¼ŒæŠ‘åˆ¶è™šå‡åå­¦ä¹ ç¥ç»å…ƒçš„äº§ç”Ÿï¼Œå®ç°æ›´å½»åº•çš„åå­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSsiuuåœ¨å¯¹æŠ—æ³¨å…¥å’Œè‰¯æ€§æ”»å‡»åœºæ™¯ä¸‹ï¼Œå‡ä¼˜äºç°æœ‰åå­¦ä¹ æ–¹æ³•ï¼Œæå‡äº†åå­¦ä¹ çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œè§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒï¼Œå¯èƒ½è®°å¿†ç§æœ‰æˆ–æ•æ„ŸçŸ¥è¯†ï¼Œå¸¦æ¥ä¸¥é‡çš„éšç§é£é™©ã€‚è™½ç„¶ä¸€äº›åå­¦ä¹ æ–¹æ³•å¯ä»¥ç¼“è§£è¿™äº›é£é™©ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“åœ¨åç»­è®­ç»ƒä¸­â€œé‡æ–°å­¦ä¹ â€ï¼Œå¯¼è‡´å¤§é‡è¢«é—å¿˜çš„çŸ¥è¯†é‡æ–°å‡ºç°ã€‚æœ¬æ–‡è¡¨æ˜ï¼Œå¹¿æ³›ä½¿ç”¨çš„åå­¦ä¹ æ–¹æ³•ä¼šå¯¼è‡´æµ…å±‚å¯¹é½ï¼šå®ƒä»¬ä¸æ˜¯å¿ å®åœ°æ“¦é™¤ç›®æ ‡çŸ¥è¯†ï¼Œè€Œæ˜¯ç”Ÿæˆè™šå‡çš„åå­¦ä¹ ç¥ç»å…ƒï¼Œæ”¾å¤§è´Ÿé¢å½±å“æ¥éšè—å®ƒã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†Ssiuuï¼Œä¸€ç§æ–°çš„åå­¦ä¹ æ–¹æ³•ï¼Œå®ƒé‡‡ç”¨å±æ€§å¼•å¯¼çš„æ­£åˆ™åŒ–æ¥é˜²æ­¢è™šå‡çš„è´Ÿé¢å½±å“ï¼Œå¹¶å¿ å®åœ°ç§»é™¤ç›®æ ‡çŸ¥è¯†ã€‚å®éªŒç»“æœè¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯é åœ°æ“¦é™¤ç›®æ ‡çŸ¥è¯†ï¼Œå¹¶åœ¨ä¸¤ç§å®é™…çš„å†è®­ç»ƒåœºæ™¯ä¸­ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼šï¼ˆ1ï¼‰ç§æœ‰æ•°æ®çš„å¯¹æŠ—æ³¨å…¥ï¼Œä»¥åŠï¼ˆ2ï¼‰ä½¿ç”¨æŒ‡ä»¤è·ŸéšåŸºå‡†çš„è‰¯æ€§æ”»å‡»ã€‚æˆ‘ä»¬çš„å‘ç°å¼ºè°ƒäº†é²æ£’å’Œå¿ å®çš„åå­¦ä¹ æ–¹æ³•å¯¹äºå®‰å…¨éƒ¨ç½²è¯­è¨€æ¨¡å‹çš„å¿…è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åå­¦ä¹ æ–¹æ³•åœ¨åç»­è®­ç»ƒä¸­å®¹æ˜“å‡ºç°â€œé‡æ–°å­¦ä¹ â€ç°è±¡ï¼Œå³æ¨¡å‹é‡æ–°è®°èµ·å·²è¢«åˆ é™¤çš„çŸ¥è¯†ã€‚è¿™æ˜¯å› ä¸ºç°æœ‰æ–¹æ³•å¹¶éçœŸæ­£æ“¦é™¤ç›®æ ‡çŸ¥è¯†ï¼Œè€Œæ˜¯é€šè¿‡ç”Ÿæˆâ€œè™šå‡åå­¦ä¹ ç¥ç»å…ƒâ€æ¥æ”¾å¤§è´Ÿé¢å½±å“ï¼Œä»è€Œéšè—ç›®æ ‡çŸ¥è¯†ã€‚è¿™ç§æµ…å±‚å¯¹é½å¯¼è‡´æ¨¡å‹è„†å¼±ï¼Œå®¹æ˜“å—åˆ°æ”»å‡»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSsiuuçš„æ ¸å¿ƒæ€è·¯æ˜¯é˜²æ­¢è™šå‡åå­¦ä¹ ç¥ç»å…ƒçš„äº§ç”Ÿï¼Œä»è€Œå®ç°å¯¹ç›®æ ‡çŸ¥è¯†çš„å¿ å®æ“¦é™¤ã€‚å…·ä½“è€Œè¨€ï¼ŒSsiuué€šè¿‡å±æ€§å¼•å¯¼çš„æ­£åˆ™åŒ–ï¼Œçº¦æŸç¥ç»å…ƒçš„è¡Œä¸ºï¼Œä½¿å…¶ä¸“æ³¨äºçœŸæ­£ç§»é™¤ç›®æ ‡çŸ¥è¯†ï¼Œè€Œä¸æ˜¯ç®€å•åœ°éšè—å®ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSsiuuæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1. ç¡®å®šéœ€è¦åå­¦ä¹ çš„ç›®æ ‡çŸ¥è¯†ï¼›2. ä½¿ç”¨å½’å› æ–¹æ³•ï¼ˆattribution methodï¼‰è¯†åˆ«ä¸ç›®æ ‡çŸ¥è¯†ç›¸å…³çš„ç¥ç»å…ƒï¼›3. åº”ç”¨å±æ€§å¼•å¯¼çš„æ­£åˆ™åŒ–ï¼ŒæŠ‘åˆ¶è¿™äº›ç¥ç»å…ƒäº§ç”Ÿè™šå‡çš„è´Ÿé¢å½±å“ï¼›4. å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥è¿›ä¸€æ­¥å·©å›ºåå­¦ä¹ æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šSsiuuçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†å±æ€§å¼•å¯¼çš„æ­£åˆ™åŒ–ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ­£åˆ™åŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é˜²æ­¢è™šå‡åå­¦ä¹ ç¥ç»å…ƒçš„äº§ç”Ÿã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSsiuuèƒ½å¤Ÿæ›´å½»åº•ã€æ›´é²æ£’åœ°æ“¦é™¤ç›®æ ‡çŸ¥è¯†ï¼Œé™ä½äº†æ¨¡å‹è¢«é‡æ–°å­¦ä¹ çš„é£é™©ã€‚

**å…³é”®è®¾è®¡**ï¼šSsiuuçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. ä½¿ç”¨åˆé€‚çš„å½’å› æ–¹æ³•æ¥å‡†ç¡®è¯†åˆ«ä¸ç›®æ ‡çŸ¥è¯†ç›¸å…³çš„ç¥ç»å…ƒï¼›2. è®¾è®¡æœ‰æ•ˆçš„æ­£åˆ™åŒ–é¡¹ï¼Œä»¥çº¦æŸç¥ç»å…ƒçš„è¡Œä¸ºï¼Œé˜²æ­¢å…¶äº§ç”Ÿè™šå‡çš„è´Ÿé¢å½±å“ã€‚æ­£åˆ™åŒ–é¡¹çš„è®¾è®¡éœ€è¦è€ƒè™‘ç¥ç»å…ƒæ¿€æ´»å€¼çš„åˆ†å¸ƒã€æ¢¯åº¦ä¿¡æ¯ç­‰å› ç´ ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å¯èƒ½åŒ…å«L1æˆ–L2æ­£åˆ™åŒ–é¡¹ï¼Œä»¥åŠåŸºäºå½’å› å€¼çš„æ­£åˆ™åŒ–é¡¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSsiuuåœ¨å¯¹æŠ—æ³¨å…¥ç§æœ‰æ•°æ®å’Œä½¿ç”¨æŒ‡ä»¤è·ŸéšåŸºå‡†è¿›è¡Œè‰¯æ€§æ”»å‡»ä¸¤ç§åœºæ™¯ä¸‹ï¼Œå‡æ˜¾è‘—ä¼˜äºç°æœ‰åå­¦ä¹ æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒSsiuuèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ“¦é™¤ç›®æ ‡çŸ¥è¯†ï¼Œé™ä½æ¨¡å‹è¢«é‡æ–°å­¦ä¹ çš„é£é™©ï¼Œå¹¶ä¸”åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¾‹å¦‚ï¼Œåœ¨å¯¹æŠ—æ³¨å…¥åœºæ™¯ä¸‹ï¼ŒSsiuuå°†é‡æ–°å­¦ä¹ ç‡é™ä½äº†XX%ï¼ŒåŒæ—¶ä¿æŒäº†YY%çš„æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Ssiuuæ–¹æ³•å¯åº”ç”¨äºå„ç§éœ€è¦ä¿æŠ¤éšç§æˆ–åˆ é™¤ä¸å½“ä¿¡æ¯çš„è¯­è¨€æ¨¡å‹åœºæ™¯ï¼Œä¾‹å¦‚ï¼šåˆ é™¤æ¨¡å‹ä¸­åŒ…å«çš„ä¸ªäººèº«ä»½ä¿¡æ¯ã€é˜²æ­¢æ¨¡å‹ç”Ÿæˆæœ‰å®³æˆ–åè§å†…å®¹ã€ä»¥åŠåº”å¯¹æ¨¡å‹è¢«ç”¨äºæ¶æ„ç›®çš„çš„æƒ…å†µã€‚è¯¥æ–¹æ³•æœ‰åŠ©äºæå‡è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œä¿ƒè¿›å…¶åœ¨å„ä¸ªé¢†åŸŸçš„å®‰å…¨éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models trained on web-scale data can memorize private or sensitive knowledge, raising significant privacy risks. Although some unlearning methods mitigate these risks, they remain vulnerable to "relearning" during subsequent training, allowing a substantial portion of forgotten knowledge to resurface. In this paper, we show that widely used unlearning methods cause shallow alignment: instead of faithfully erasing target knowledge, they generate spurious unlearning neurons that amplify negative influence to hide it. To overcome this limitation, we introduce Ssiuu, a new class of unlearning methods that employs attribution-guided regularization to prevent spurious negative influence and faithfully remove target knowledge. Experimental results confirm that our method reliably erases target knowledge and outperforms strong baselines across two practical retraining scenarios: (1) adversarial injection of private data, and (2) benign attack using an instruction-following benchmark. Our findings highlight the necessity of robust and faithful unlearning methods for safe deployment of language models.

