---
layout: default
title: OFMU: Optimization-Driven Framework for Machine Unlearning
---

# OFMU: Optimization-Driven Framework for Machine Unlearning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22483" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22483v1</a>
  <a href="https://arxiv.org/pdf/2509.22483.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22483v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22483v1', 'OFMU: Optimization-Driven Framework for Machine Unlearning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sadia Asif, Mohammad Mohammadi Amiri

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: Under review at ICLR 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOFMUï¼šä¸€ç§ä¼˜åŒ–é©±åŠ¨çš„æœºå™¨å­¦ä¹ é—å¿˜æ¡†æ¶ï¼Œæå‡é—å¿˜æ•ˆæœå’Œæ¨¡å‹æ•ˆç”¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨å­¦ä¹ é—å¿˜` `åŒå±‚ä¼˜åŒ–` `æ¢¯åº¦è§£è€¦` `æ¨¡å‹æ•ˆç”¨` `éšç§ä¿æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨å­¦ä¹ é—å¿˜æ–¹æ³•å¸¸é‡‡ç”¨æ ‡é‡åŒ–å¤šç›®æ ‡ä¼˜åŒ–ï¼Œæ˜“å¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ¨¡å‹æ•ˆç”¨é™ä½ï¼Œæºäºé—å¿˜å’Œä¿ç•™ç›®æ ‡æ¢¯åº¦å†²çªã€‚
2. OFMUé‡‡ç”¨åŸºäºæƒ©ç½šçš„åŒå±‚ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚ç»“æ„ä¼˜å…ˆè€ƒè™‘é—å¿˜ï¼Œå¹¶åˆ©ç”¨ç›¸ä¼¼æ€§æ„ŸçŸ¥æƒ©ç½šä½¿æ¢¯åº¦å»ç›¸å…³ã€‚
3. å®éªŒè¯æ˜ï¼ŒOFMUåœ¨è§†è§‰å’Œè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ï¼Œé—å¿˜æ•ˆæœå’Œä¿ç•™æ•ˆç”¨å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æä¾›äº†æ”¶æ•›æ€§ç†è®ºä¿è¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•æ„Ÿåº”ç”¨ä¸­éƒ¨ç½²æ—¶ï¼Œéœ€è¦å…·å¤‡é—å¿˜ç‰¹å®šçŸ¥è¯†çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ç”¨æˆ·è¯·æ±‚ã€å—ç‰ˆæƒä¿æŠ¤çš„ææ–™æˆ–è¿‡æ—¶ä¿¡æ¯ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒï¼Œä»¥ç¡®ä¿æ³•è§„éµä»æ€§ã€ç”¨æˆ·éšç§å’Œå®‰å…¨æ€§ã€‚è¿™é¡¹ä»»åŠ¡ï¼Œè¢«ç§°ä¸ºæœºå™¨å­¦ä¹ é—å¿˜ï¼Œæ—¨åœ¨æ¶ˆé™¤ç›®æ ‡æ•°æ®çš„å½±å“ï¼ˆé—å¿˜ï¼‰ï¼ŒåŒæ—¶ä¿æŒå¯¹å‰©ä½™æ•°æ®çš„æ€§èƒ½ï¼ˆä¿ç•™ï¼‰ã€‚ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯å°†æ­¤é—®é¢˜è¡¨è¿°ä¸ºå¤šç›®æ ‡é—®é¢˜ï¼Œå¹¶é€šè¿‡æ ‡é‡åŒ–å°†å…¶ç®€åŒ–ä¸ºå•ç›®æ ‡é—®é¢˜ï¼Œå…¶ä¸­é—å¿˜å’Œä¿ç•™æŸå¤±ä½¿ç”¨åŠ æƒå’Œç»„åˆã€‚ç„¶è€Œï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´ä¸ç¨³å®šçš„è®­ç»ƒåŠ¨æ€å’Œé™ä½çš„æ¨¡å‹æ•ˆç”¨ï¼Œå› ä¸ºæ¢¯åº¦æ–¹å‘å†²çªã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OFMUï¼Œä¸€ç§åŸºäºæƒ©ç½šçš„åŒå±‚ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚ç»“æ„æ˜¾å¼åœ°ä¼˜å…ˆè€ƒè™‘é—å¿˜ï¼ŒåŒæ—¶ä¿ç•™ä¿ç•™ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å†…éƒ¨æœ€å¤§åŒ–æ­¥éª¤å¼ºåˆ¶é—å¿˜ï¼Œè¯¥æ­¥éª¤åŒ…å«ä¸€ä¸ªæ„ŸçŸ¥ç›¸ä¼¼æ€§çš„æƒ©ç½šï¼Œä»¥ä½¿é—å¿˜å’Œä¿ç•™ç›®æ ‡çš„æ¢¯åº¦å»ç›¸å…³ï¼Œå¹¶é€šè¿‡å¤–éƒ¨æœ€å°åŒ–æ­¥éª¤æ¢å¤æ•ˆç”¨ã€‚ä¸ºäº†ç¡®ä¿å¯æ‰©å±•æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å…·æœ‰å¯è¯æ˜æ”¶æ•›ä¿è¯çš„åŒå¾ªç¯ç®—æ³•ï¼Œæ— è®ºæ˜¯åœ¨å‡¸è¿˜æ˜¯éå‡¸æƒ…å†µä¸‹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æä¾›äº†æ”¶æ•›é€Ÿåº¦çš„ä¸¥æ ¼ç†è®ºåˆ†æï¼Œå¹¶è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨é—å¿˜æ•ˆæœå’Œæ¨¡å‹æ•ˆç”¨ä¹‹é—´å®ç°äº†æ›´å¥½çš„æƒè¡¡ã€‚è·¨è§†è§‰å’Œè¯­è¨€åŸºå‡†çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOFMUåœ¨é—å¿˜æ•ˆæœå’Œä¿ç•™æ•ˆç”¨æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰çš„é—å¿˜æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨éœ€è¦é—å¿˜ç‰¹å®šæ•°æ®æ—¶ï¼Œå¦‚ä½•é«˜æ•ˆä¸”æœ‰æ•ˆåœ°ç§»é™¤è¿™äº›æ•°æ®çš„å½±å“ï¼ŒåŒæ—¶å°½å¯èƒ½ä¿ç•™æ¨¡å‹åœ¨å‰©ä½™æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ ‡é‡åŒ–çš„å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼Œå®¹æ˜“å› ä¸ºé—å¿˜å’Œä¿ç•™ç›®æ ‡ä¹‹é—´çš„æ¢¯åº¦å†²çªè€Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œæœ€ç»ˆå½±å“æ¨¡å‹çš„æ•ˆç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOFMUçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŒå±‚ä¼˜åŒ–æ¡†æ¶æ˜¾å¼åœ°ä¼˜å…ˆè€ƒè™‘é—å¿˜ã€‚å†…å±‚ä¼˜åŒ–ä¸“æ³¨äºæœ€å¤§åŒ–é—å¿˜æŸå¤±ï¼Œå¹¶å¼•å…¥ç›¸ä¼¼æ€§æ„ŸçŸ¥æƒ©ç½šæ¥è§£è€¦é—å¿˜å’Œä¿ç•™ç›®æ ‡çš„æ¢¯åº¦ï¼Œä»è€Œæ›´å½»åº•åœ°ç§»é™¤ç›®æ ‡æ•°æ®çš„å½±å“ã€‚å¤–å±‚ä¼˜åŒ–åˆ™è‡´åŠ›äºæœ€å°åŒ–ä¿ç•™æŸå¤±ï¼Œä»¥æ¢å¤æ¨¡å‹çš„æ•ˆç”¨ã€‚è¿™ç§åˆ†å±‚ç»“æ„ç¡®ä¿äº†é—å¿˜è¿‡ç¨‹çš„ä¼˜å…ˆæ€§ï¼ŒåŒæ—¶å…¼é¡¾äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOFMUé‡‡ç”¨åŒå±‚ä¼˜åŒ–æ¡†æ¶ã€‚é¦–å…ˆï¼Œåœ¨å†…å±‚å¾ªç¯ä¸­ï¼Œé€šè¿‡æœ€å¤§åŒ–é—å¿˜æŸå¤±å¹¶æ–½åŠ ç›¸ä¼¼æ€§æ„ŸçŸ¥æƒ©ç½šï¼Œæ¥å¼ºåˆ¶æ¨¡å‹é—å¿˜ç›®æ ‡æ•°æ®ã€‚ç„¶åï¼Œåœ¨å¤–å±‚å¾ªç¯ä¸­ï¼Œé€šè¿‡æœ€å°åŒ–ä¿ç•™æŸå¤±ï¼Œæ¥æ¢å¤æ¨¡å‹åœ¨å‰©ä½™æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡ä¸€ä¸ªä¸¤å¾ªç¯ç®—æ³•å®ç°ï¼Œè¯¥ç®—æ³•å…·æœ‰å¯è¯æ˜çš„æ”¶æ•›æ€§ï¼Œé€‚ç”¨äºå‡¸å’Œéå‡¸ä¼˜åŒ–åœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šOFMUçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŸºäºæƒ©ç½šçš„åŒå±‚ä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¾å¼åœ°ä¼˜å…ˆè€ƒè™‘é—å¿˜ï¼Œå¹¶é€šè¿‡ç›¸ä¼¼æ€§æ„ŸçŸ¥æƒ©ç½šæ¥è§£è€¦é—å¿˜å’Œä¿ç•™ç›®æ ‡çš„æ¢¯åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOFMUèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ç§»é™¤ç›®æ ‡æ•°æ®çš„å½±å“ï¼ŒåŒæ—¶æ›´å¥½åœ°ä¿ç•™æ¨¡å‹çš„æ•ˆç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æä¾›äº†æ”¶æ•›é€Ÿåº¦çš„ä¸¥æ ¼ç†è®ºåˆ†æã€‚

**å…³é”®è®¾è®¡**ï¼šOFMUçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç›¸ä¼¼æ€§æ„ŸçŸ¥æƒ©ç½šé¡¹ï¼Œç”¨äºè§£è€¦é—å¿˜å’Œä¿ç•™ç›®æ ‡çš„æ¢¯åº¦ï¼›2) åŒå±‚ä¼˜åŒ–æ¡†æ¶ï¼Œç¡®ä¿é—å¿˜çš„ä¼˜å…ˆæ€§ï¼›3) ä¸¤å¾ªç¯ç®—æ³•ï¼Œå®ç°é«˜æ•ˆçš„ä¼˜åŒ–è¿‡ç¨‹ï¼›4) é’ˆå¯¹å‡¸å’Œéå‡¸åœºæ™¯çš„æ”¶æ•›æ€§åˆ†æã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œæƒ©ç½šé¡¹çš„é€‰æ‹©éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOFMUåœ¨é—å¿˜æ•ˆæœå’Œä¿ç•™æ•ˆç”¨æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœ¨è§†è§‰å’Œè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOFMUèƒ½å¤Ÿæ›´å½»åº•åœ°ç§»é™¤ç›®æ ‡æ•°æ®çš„å½±å“ï¼ŒåŒæ—¶æ›´å¥½åœ°ä¿ç•™æ¨¡å‹åœ¨å‰©ä½™æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚è®ºæ–‡è¿˜æä¾›äº†æ”¶æ•›é€Ÿåº¦çš„ä¸¥æ ¼ç†è®ºåˆ†æï¼Œè¯æ˜äº†OFMUçš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œéœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OFMUåœ¨éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§ã€éµå®ˆæ³•è§„æˆ–å¤„ç†è¿‡æ—¶ä¿¡æ¯çš„åœºæ™¯ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¯ä»¥ç”¨äºç§»é™¤ç”¨æˆ·çš„æ•æ„Ÿè¯·æ±‚ã€å—ç‰ˆæƒä¿æŠ¤çš„ææ–™æˆ–ä¸å‡†ç¡®çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºè”é‚¦å­¦ä¹ ç­‰åˆ†å¸ƒå¼å­¦ä¹ åœºæ™¯ï¼Œä»¥å®ç°å¯¹ç‰¹å®šå‚ä¸è€…æ•°æ®çš„é—å¿˜ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼ŒOFMUæœ‰æœ›æˆä¸ºæ„å»ºå®‰å…¨ã€å¯é å’Œè´Ÿè´£ä»»çš„AIç³»ç»Ÿçš„å…³é”®æŠ€æœ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models deployed in sensitive applications increasingly require the ability to unlearn specific knowledge, such as user requests, copyrighted materials, or outdated information, without retraining from scratch to ensure regulatory compliance, user privacy, and safety. This task, known as machine unlearning, aims to remove the influence of targeted data (forgetting) while maintaining performance on the remaining data (retention). A common approach is to formulate this as a multi-objective problem and reduce it to a single-objective problem via scalarization, where forgetting and retention losses are combined using a weighted sum. However, this often results in unstable training dynamics and degraded model utility due to conflicting gradient directions. To address these challenges, we propose OFMU, a penalty-based bi-level optimization framework that explicitly prioritizes forgetting while preserving retention through a hierarchical structure. Our method enforces forgetting via an inner maximization step that incorporates a similarity-aware penalty to decorrelate the gradients of the forget and retention objectives, and restores utility through an outer minimization step. To ensure scalability, we develop a two-loop algorithm with provable convergence guarantees under both convex and non-convex regimes. We further provide a rigorous theoretical analysis of convergence rates and show that our approach achieves better trade-offs between forgetting efficacy and model utility compared to prior methods. Extensive experiments across vision and language benchmarks demonstrate that OFMU consistently outperforms existing unlearning methods in both forgetting efficacy and retained utility.

