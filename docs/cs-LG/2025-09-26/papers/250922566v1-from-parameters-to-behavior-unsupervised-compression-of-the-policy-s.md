---
layout: default
title: From Parameters to Behavior: Unsupervised Compression of the Policy Space
---

# From Parameters to Behavior: Unsupervised Compression of the Policy Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22566" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22566v1</a>
  <a href="https://arxiv.org/pdf/2509.22566.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22566v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22566v1', 'From Parameters to Behavior: Unsupervised Compression of the Policy Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Davide Tenedini, Riccardo Zamboni, Mirco Mutti, Marcello Restelli

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— ç›‘ç£æ–¹æ³•å‹ç¼©ç­–ç•¥ç©ºé—´ä»¥æé«˜æ·±åº¦å¼ºåŒ–å­¦ä¹ æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `æ— ç›‘ç£å­¦ä¹ ` `å‚æ•°å‹ç¼©` `è¡Œä¸ºé‡æ„` `ç”Ÿæˆæ¨¡å‹` `å¤šä»»åŠ¡å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é«˜ç»´å‚æ•°ç©ºé—´ä¸­ä¼˜åŒ–ç­–ç•¥ï¼Œå¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶åœ¨å¤šä»»åŠ¡ç¯å¢ƒä¸­æ›´ä¸ºæ˜æ˜¾ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„æ–¹æ³•ï¼Œé€šè¿‡å°†ç­–ç•¥å‚æ•°ç©ºé—´å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œä¼˜åŒ–è¡Œä¸ºé‡æ„æŸå¤±ä»¥ç»„ç»‡æ½œåœ¨ç©ºé—´ã€‚
3. åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿå°†ç­–ç•¥ç½‘ç»œçš„å‚æ•°åŒ–å‹ç¼©è‡³äº”ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶ä¿æŒå…¶è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶æ ·æœ¬æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ä¾ç„¶çªå‡ºã€‚æœ¬æ–‡è®¤ä¸ºï¼Œè¿™ç§ä½æ•ˆæºäºç›´æ¥åœ¨é«˜ç»´ä¸”é«˜åº¦å†—ä½™çš„å‚æ•°ç©ºé—´ä¸­ä¼˜åŒ–ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— ç›‘ç£æ–¹æ³•ï¼Œå°†ç­–ç•¥å‚æ•°ç©ºé—´å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ã€‚é€šè¿‡ä¼˜åŒ–è¡Œä¸ºé‡æ„æŸå¤±ï¼Œè®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œä½¿æ½œåœ¨ç©ºé—´æŒ‰åŠŸèƒ½ç›¸ä¼¼æ€§ç»„ç»‡ï¼Œè€Œéå‚æ•°åŒ–çš„æ¥è¿‘æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¿ç»­æ§åˆ¶é¢†åŸŸï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå°†æ ‡å‡†ç­–ç•¥ç½‘ç»œçš„å‚æ•°åŒ–å‹ç¼©è‡³äº”ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶ä¿æŒå¤§éƒ¨åˆ†è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶æ”¯æŒé€šè¿‡æ½œåœ¨ç©ºé—´è¿›è¡Œä»»åŠ¡ç‰¹å®šçš„é€‚åº”ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ç­–ç•¥ä¼˜åŒ–çš„æ ·æœ¬æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é«˜ç»´ä¸”å†—ä½™çš„å‚æ•°ç©ºé—´ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶åœ¨å¤šä»»åŠ¡è®¾ç½®ä¸­æ›´ä¸ºæ˜æ˜¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šä½œè€…æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„ç­–ç•¥ç©ºé—´å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡å°†ç­–ç•¥å‚æ•°ç©ºé—´æ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œä¼˜åŒ–è¡Œä¸ºé‡æ„æŸå¤±ï¼Œä½¿æ½œåœ¨ç©ºé—´æŒ‰åŠŸèƒ½ç›¸ä¼¼æ€§ç»„ç»‡ï¼Œè€Œéç®€å•çš„å‚æ•°æ¥è¿‘æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒï¼Œè¯¥æ¨¡å‹å°†ä½ç»´æ½œåœ¨ç©ºé—´æ˜ å°„å›é«˜ç»´å‚æ•°ç©ºé—´ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ½œåœ¨ç©ºé—´çš„æ„å»ºã€è¡Œä¸ºé‡æ„æŸå¤±çš„ä¼˜åŒ–å’Œç­–ç•¥ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡æ— ç›‘ç£å­¦ä¹ å°†ç­–ç•¥å‚æ•°åŒ–å‹ç¼©è‡³ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå¹¶å…è®¸åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œä»»åŠ¡ç‰¹å®šçš„é€‚åº”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºè¡Œä¸ºé‡æ„æŸå¤±ï¼Œç¡®ä¿æ½œåœ¨ç©ºé—´çš„åŠŸèƒ½ç›¸ä¼¼æ€§ï¼›ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨ç”Ÿæˆæ¨¡å‹æ¥å®ç°æ½œåœ¨ç©ºé—´ä¸å‚æ•°ç©ºé—´çš„æ˜ å°„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿå°†æ ‡å‡†ç­–ç•¥ç½‘ç»œçš„å‚æ•°åŒ–å‹ç¼©è‡³äº”ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶ä¿æŒå¤§éƒ¨åˆ†è¡¨è¾¾èƒ½åŠ›ã€‚è¿™ä¸€å‹ç¼©æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå¹¶æ”¯æŒåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œä»»åŠ¡ç‰¹å®šçš„é€‚åº”ï¼Œå±•ç¤ºäº†å…¶åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“å’Œå¤šä»»åŠ¡å­¦ä¹ ç­‰ã€‚é€šè¿‡æé«˜æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŠ é€Ÿæ™ºèƒ½ä½“çš„è®­ç»ƒè¿‡ç¨‹ï¼Œé™ä½å¯¹å¤§é‡æ•°æ®çš„ä¾èµ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite its recent successes, Deep Reinforcement Learning (DRL) is notoriously sample-inefficient. We argue that this inefficiency stems from the standard practice of optimizing policies directly in the high-dimensional and highly redundant parameter space $Î˜$. This challenge is greatly compounded in multi-task settings. In this work, we develop a novel, unsupervised approach that compresses the policy parameter space $Î˜$ into a low-dimensional latent space $\mathcal{Z}$. We train a generative model $g:\mathcal{Z}\toÎ˜$ by optimizing a behavioral reconstruction loss, which ensures that the latent space is organized by functional similarity rather than proximity in parameterization. We conjecture that the inherent dimensionality of this manifold is a function of the environment's complexity, rather than the size of the policy network. We validate our approach in continuous control domains, showing that the parameterization of standard policy networks can be compressed up to five orders of magnitude while retaining most of its expressivity. As a byproduct, we show that the learned manifold enables task-specific adaptation via Policy Gradient operating in the latent space $\mathcal{Z}$.

