---
layout: default
title: A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab
---

# A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01264" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01264v1</a>
  <a href="https://arxiv.org/pdf/2510.01264.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01264v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01264v1', 'A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Isaac Peterson, Christopher Allred, Jacob Morrey, Mario Harper

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 8 page, 9 figures, code https://github.com/DIRECTLab/IsaacLab-HARL

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/DIRECTLab/IsaacLab-HARL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ‰©å±•IsaacLabæ¡†æ¶ï¼Œå®ç°å¼‚æ„å¤šæ™ºèƒ½ä½“å¯¹æŠ—å¼ºåŒ–å­¦ä¹ çš„å¯æ‰©å±•è®­ç»ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `å¯¹æŠ—å­¦ä¹ ` `å¼‚æ„æ™ºèƒ½ä½“` `IsaacLab` `æœºå™¨äººä»¿çœŸ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åœ¨åŠ¨æ€ç¯å¢ƒä¸­åä½œçš„æœºå™¨äººç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨åä½œç¯å¢ƒã€‚
2. æœ¬æ–‡æ‰©å±•IsaacLabæ¡†æ¶ï¼Œæ”¯æŒå¯¹æŠ—æ€§MARLï¼Œé›†æˆäº†HAPPOçš„ç«äº‰å˜ä½“ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒå’Œè¯„ä¼°ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸ºå½¢æ€å¤šæ ·çš„å¤šæ™ºèƒ½ä½“ç«äº‰å»ºæ¨¡å’Œè®­ç»ƒé²æ£’ç­–ç•¥ï¼Œå¹¶ä¿æŒé«˜ååé‡å’Œä»¿çœŸçœŸå®æ„Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ‰©å±•äº†IsaacLabæ¡†æ¶ï¼Œä»¥æ”¯æŒåœ¨é«˜ä¿çœŸç‰©ç†ä»¿çœŸä¸­å¯æ‰©å±•åœ°è®­ç»ƒå¯¹æŠ—ç­–ç•¥ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€å¥—å¯¹æŠ—æ€§å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ç¯å¢ƒï¼Œè¿™äº›ç¯å¢ƒå…·æœ‰ç›®æ ‡å’Œèƒ½åŠ›ä¸å¯¹ç§°çš„å¼‚æ„æ™ºèƒ½ä½“ã€‚æˆ‘ä»¬çš„å¹³å°é›†æˆäº†å¼‚æ„æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆHAPPOï¼‰çš„ç«äº‰å˜ä½“ï¼Œä»è€Œèƒ½å¤Ÿåœ¨å¯¹æŠ—æ€§åŠ¨æ€ä¸‹è¿›è¡Œé«˜æ•ˆçš„è®­ç»ƒå’Œè¯„ä¼°ã€‚åœ¨å¤šä¸ªåŸºå‡†åœºæ™¯ä¸­çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸ºå½¢æ€å¤šæ ·çš„å¤šæ™ºèƒ½ä½“ç«äº‰å»ºæ¨¡å’Œè®­ç»ƒé²æ£’çš„ç­–ç•¥ï¼ŒåŒæ—¶ä¿æŒé«˜ååé‡å’Œä»¿çœŸçœŸå®æ„Ÿã€‚ä»£ç å’ŒåŸºå‡†å¯åœ¨https://github.com/DIRECTLab/IsaacLab-HARL è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åä½œåœºæ™¯ï¼Œè€Œå¿½ç•¥äº†å¯¹æŠ—æ€§äº¤äº’åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„é‡è¦æ€§ï¼Œä¾‹å¦‚è¿½é€-é€ƒé¿ã€å®‰å…¨å’Œç«äº‰æ€§æ“ä½œã€‚ç°æœ‰çš„æ¡†æ¶å¯èƒ½éš¾ä»¥å¤„ç†å¼‚æ„æ™ºèƒ½ä½“ï¼Œç‰¹åˆ«æ˜¯å½“è¿™äº›æ™ºèƒ½ä½“å…·æœ‰ä¸å¯¹ç§°çš„ç›®æ ‡å’Œèƒ½åŠ›æ—¶ã€‚æ­¤å¤–ï¼Œåœ¨é«˜ä¿çœŸç‰©ç†ä»¿çœŸä¸­è¿›è¡Œå¯æ‰©å±•çš„å¯¹æŠ—æ€§è®­ç»ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ‰©å±•IsaacLabæ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿæ”¯æŒåœ¨é«˜ä¿çœŸç‰©ç†ä»¿çœŸä¸­è¿›è¡Œå¯æ‰©å±•çš„å¯¹æŠ—æ€§å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚é€šè¿‡é›†æˆå¼‚æ„æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆHARLï¼‰ä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„ç«äº‰å˜ä½“ï¼ˆHAPPOï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è®­ç»ƒå’Œè¯„ä¼°å…·æœ‰ä¸åŒå½¢æ€å’Œèƒ½åŠ›çš„æ™ºèƒ½ä½“åœ¨å¯¹æŠ—ç¯å¢ƒä¸­çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŸºäºNVIDIAçš„IsaacLabå¹³å°ï¼Œå¹¶å¼•å…¥äº†ä¸€å¥—æ–°çš„å¯¹æŠ—æ€§MARLç¯å¢ƒã€‚è¿™äº›ç¯å¢ƒåŒ…å«å…·æœ‰ä¸å¯¹ç§°ç›®æ ‡å’Œèƒ½åŠ›çš„å¼‚æ„æ™ºèƒ½ä½“ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯é›†æˆäº†HAPPOç®—æ³•ï¼Œç”¨äºè®­ç»ƒæ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚æ•´ä¸ªè®­ç»ƒæµç¨‹åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š1) åœ¨IsaacLabç¯å¢ƒä¸­åˆ›å»ºå¯¹æŠ—æ€§åœºæ™¯ï¼›2) ä½¿ç”¨HAPPOç®—æ³•è®­ç»ƒæ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ï¼›3) åœ¨ä»¿çœŸç¯å¢ƒä¸­è¯„ä¼°è®­ç»ƒå¥½çš„ç­–ç•¥ï¼›4) æ ¹æ®è¯„ä¼°ç»“æœè°ƒæ•´è®­ç»ƒå‚æ•°ï¼Œå¹¶é‡å¤æ­¥éª¤2å’Œ3ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†HAPPOç®—æ³•ä¸IsaacLabå¹³å°ç›¸ç»“åˆï¼Œä»è€Œå®ç°äº†åœ¨é«˜ä¿çœŸç‰©ç†ä»¿çœŸä¸­å¯¹å¼‚æ„æ™ºèƒ½ä½“è¿›è¡Œå¯æ‰©å±•çš„å¯¹æŠ—æ€§è®­ç»ƒã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜å¼•å…¥äº†ä¸€å¥—æ–°çš„å¯¹æŠ—æ€§MARLç¯å¢ƒï¼Œè¿™äº›ç¯å¢ƒå…·æœ‰ç›®æ ‡å’Œèƒ½åŠ›ä¸å¯¹ç§°çš„å¼‚æ„æ™ºèƒ½ä½“ï¼Œæ›´è´´è¿‘ç°å®ä¸–ç•Œçš„åº”ç”¨åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šHAPPOç®—æ³•æ˜¯åŸºäºPPOçš„ï¼Œä½†é’ˆå¯¹å¼‚æ„æ™ºèƒ½ä½“è¿›è¡Œäº†æ”¹è¿›ã€‚å…·ä½“æ¥è¯´ï¼ŒHAPPOä¸ºæ¯ä¸ªæ™ºèƒ½ä½“ç»´æŠ¤ä¸€ä¸ªç‹¬ç«‹çš„ç­–ç•¥ç½‘ç»œï¼Œå¹¶ä½¿ç”¨é›†ä¸­çš„è¯„è®ºå®¶ç½‘ç»œæ¥è¯„ä¼°æ¯ä¸ªæ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ç­–ç•¥æŸå¤±ã€ä»·å€¼æŸå¤±å’Œç†µæ­£åˆ™åŒ–é¡¹ã€‚è¯¥æ¡†æ¶è¿˜æ”¯æŒå„ç§å‚æ•°è®¾ç½®ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­å’Œè£å‰ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å¯ä»¥æ ¹æ®å…·ä½“çš„ç¯å¢ƒè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è®­ç»ƒå¼‚æ„æ™ºèƒ½ä½“åœ¨å¯¹æŠ—ç¯å¢ƒä¸­çš„ç­–ç•¥ã€‚åœ¨å¤šä¸ªåŸºå‡†åœºæ™¯ä¸­ï¼Œè¯¥æ¡†æ¶éƒ½å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¿æŒé«˜ååé‡å’Œä»¿çœŸçœŸå®æ„Ÿã€‚ä¾‹å¦‚ï¼Œåœ¨è¿½é€-é€ƒé¿æ¸¸æˆä¸­ï¼Œè®­ç»ƒå¥½çš„è¿½é€è€…èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿½æ•é€ƒé¿è€…ï¼Œå³ä½¿é€ƒé¿è€…å…·æœ‰æ›´å¿«çš„é€Ÿåº¦å’Œæ›´é«˜çš„æœºåŠ¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººé¢†åŸŸçš„å„ç§å¯¹æŠ—æ€§åœºæ™¯ï¼Œä¾‹å¦‚è¿½é€-é€ƒé¿æ¸¸æˆã€å®‰å…¨å·¡é€»ã€èµ„æºç«äº‰å’Œå¯¹æŠ—æ€§æ“ä½œã€‚é€šè¿‡åœ¨é«˜ä¿çœŸç‰©ç†ä»¿çœŸä¸­è®­ç»ƒé²æ£’çš„å¯¹æŠ—ç­–ç•¥ï¼Œå¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥ç”¨äºå¼€å‘æ›´å®‰å…¨çš„è‡ªä¸»ç³»ç»Ÿï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶æ±½è½¦å’Œæ— äººæœºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-Agent Reinforcement Learning (MARL) is central to robotic systems cooperating in dynamic environments. While prior work has focused on these collaborative settings, adversarial interactions are equally critical for real-world applications such as pursuit-evasion, security, and competitive manipulation. In this work, we extend the IsaacLab framework to support scalable training of adversarial policies in high-fidelity physics simulations. We introduce a suite of adversarial MARL environments featuring heterogeneous agents with asymmetric goals and capabilities. Our platform integrates a competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization (HAPPO), enabling efficient training and evaluation under adversarial dynamics. Experiments across several benchmark scenarios demonstrate the framework's ability to model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism. Code and benchmarks are available at: https://github.com/DIRECTLab/IsaacLab-HARL .

