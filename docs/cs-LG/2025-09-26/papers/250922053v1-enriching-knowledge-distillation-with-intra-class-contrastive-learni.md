---
layout: default
title: Enriching Knowledge Distillation with Intra-Class Contrastive Learning
---

# Enriching Knowledge Distillation with Intra-Class Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22053" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22053v1</a>
  <a href="https://arxiv.org/pdf/2509.22053.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22053v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22053v1', 'Enriching Knowledge Distillation with Intra-Class Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hua Yuan, Ning Xu, Xin Geng, Yong Rui

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç±»å†…å¯¹æ¯”å­¦ä¹ çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œæå‡è½¯æ ‡ç­¾çš„ä¿¡æ¯ä¸°å¯Œåº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `ç±»å†…å¯¹æ¯”å­¦ä¹ ` `è½¯æ ‡ç­¾` `æ¨¡å‹å‹ç¼©` `æ³›åŒ–èƒ½åŠ›` `Margin Loss` `å¯¹æ¯”å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•å¿½ç•¥äº†è½¯æ ‡ç­¾ä¸­è•´å«çš„ç±»å†…å¤šæ ·æ€§ä¿¡æ¯ï¼Œé™åˆ¶äº†å­¦ç”Ÿæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. é€šè¿‡åœ¨æ•™å¸ˆæ¨¡å‹è®­ç»ƒä¸­å¼•å…¥ç±»å†…å¯¹æ¯”æŸå¤±ï¼Œå¹¶ç»“åˆmargin lossï¼Œå¢å¼ºè½¯æ ‡ç­¾çš„ç±»å†…ä¿¡æ¯ã€‚
3. å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨çŸ¥è¯†è’¸é¦ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿæå‡å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç±»å†…å¯¹æ¯”å­¦ä¹ æ¥ä¸°å¯Œè½¯æ ‡ç­¾ä¸­åŒ…å«çš„ç±»å†…ä¿¡æ¯ã€‚ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œè½¯æ ‡ç­¾ä¸­çš„éšå¼çŸ¥è¯†æ¥æºäºæ•°æ®çš„å¤šè§†è§’ç»“æ„ï¼ŒåŒä¸€ç±»åˆ«æ ·æœ¬çš„ç‰¹å¾å˜åŒ–æœ‰åŠ©äºå­¦ç”Ÿæ¨¡å‹å­¦ä¹ åˆ°æ›´å¤šæ ·åŒ–çš„è¡¨ç¤ºï¼Œä»è€Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è’¸é¦æ–¹æ³•ä¸»è¦ä»¥ground-truthæ ‡ç­¾ä½œä¸ºç›®æ ‡ï¼Œå¿½ç•¥äº†åŒä¸€ç±»åˆ«å†…çš„å¤šæ ·åŒ–è¡¨ç¤ºã€‚å› æ­¤ï¼Œæœ¬æ–‡åœ¨æ•™å¸ˆæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥ç±»å†…å¯¹æ¯”æŸå¤±ï¼Œä»¥ä¸°å¯Œè½¯æ ‡ç­¾ä¸­åŒ…å«çš„ç±»å†…ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³ç±»å†…æŸå¤±å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šå’Œæ”¶æ•›ç¼“æ…¢é—®é¢˜ï¼Œæœ¬æ–‡å°†margin lossé›†æˆåˆ°ç±»å†…å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œä»¥æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä»ç†è®ºä¸Šåˆ†æäº†è¯¥æŸå¤±å¯¹ç±»å†…è·ç¦»å’Œç±»é—´è·ç¦»çš„å½±å“ï¼Œè¯æ˜äº†ç±»å†…å¯¹æ¯”æŸå¤±å¯ä»¥ä¸°å¯Œç±»å†…å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•ä¸»è¦ä¾èµ–äºæ•™å¸ˆæ¨¡å‹æä¾›çš„è½¯æ ‡ç­¾ï¼Œä½†è¿™äº›è½¯æ ‡ç­¾å¾€å¾€ä»¥ground-truthæ ‡ç­¾ä¸ºç›®æ ‡ï¼Œå¿½ç•¥äº†åŒä¸€ç±»åˆ«å†…éƒ¨æ ·æœ¬çš„å¤šæ ·æ€§è¡¨ç¤ºã€‚è¿™ç§å¿½ç•¥é™åˆ¶äº†å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œå½±å“äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å’Œå¢å¼ºè½¯æ ‡ç­¾ä¸­çš„ç±»å†…ä¿¡æ¯æˆä¸ºä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨æ•™å¸ˆæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥ç±»å†…å¯¹æ¯”å­¦ä¹ ï¼Œé¼“åŠ±æ•™å¸ˆæ¨¡å‹å­¦ä¹ åŒä¸€ç±»åˆ«å†…ä¸åŒæ ·æœ¬ä¹‹é—´çš„å·®å¼‚æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„è½¯æ ‡ç­¾èƒ½å¤ŸåŒ…å«æ›´ä¸°å¯Œçš„ç±»å†…ä¿¡æ¯ï¼Œä»è€Œå¼•å¯¼å­¦ç”Ÿæ¨¡å‹å­¦ä¹ åˆ°æ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚åŒæ—¶ï¼Œä¸ºäº†è§£å†³ç±»å†…å¯¹æ¯”å­¦ä¹ å¯èƒ½å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šå’Œæ”¶æ•›ç¼“æ…¢é—®é¢˜ï¼Œå¼•å…¥äº†margin lossæ¥çº¦æŸç±»å†…æ ·æœ¬ä¹‹é—´çš„è·ç¦»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šæ•™å¸ˆæ¨¡å‹è®­ç»ƒé˜¶æ®µå’Œå­¦ç”Ÿæ¨¡å‹è’¸é¦é˜¶æ®µã€‚åœ¨æ•™å¸ˆæ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œé™¤äº†ä¼ ç»Ÿçš„äº¤å‰ç†µæŸå¤±å¤–ï¼Œè¿˜å¼•å…¥äº†ç±»å†…å¯¹æ¯”æŸå¤±å’Œmargin lossã€‚ç±»å†…å¯¹æ¯”æŸå¤±ç”¨äºæ‹‰è¿‘åŒä¸€ç±»åˆ«å†…ä¸åŒæ ·æœ¬çš„ç‰¹å¾è¡¨ç¤ºï¼Œmargin lossç”¨äºç»´æŒç±»å†…æ ·æœ¬ä¹‹é—´çš„æœ€å°è·ç¦»ï¼Œé˜²æ­¢ç‰¹å¾åå¡Œã€‚åœ¨å­¦ç”Ÿæ¨¡å‹è’¸é¦é˜¶æ®µï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡æœ€å°åŒ–å…¶é¢„æµ‹ç»“æœä¸æ•™å¸ˆæ¨¡å‹è½¯æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚æ¥è¿›è¡Œå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†ç±»å†…å¯¹æ¯”å­¦ä¹ å¼•å…¥åˆ°çŸ¥è¯†è’¸é¦æ¡†æ¶ä¸­ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å’Œå¢å¼ºè½¯æ ‡ç­¾ä¸­çš„ç±»å†…ä¿¡æ¯ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸ä»…å…³æ³¨ç±»åˆ«ä¹‹é—´çš„åŒºåˆ†æ€§ï¼Œè¿˜å…³æ³¨ç±»åˆ«å†…éƒ¨çš„å¤šæ ·æ€§ï¼Œä»è€Œèƒ½å¤Ÿæå‡å­¦ç”Ÿæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç±»å†…å¯¹æ¯”æŸå¤±çš„è®¡ç®—æ–¹å¼ï¼Œé€šå¸¸é‡‡ç”¨InfoNCE lossæˆ–å…¶ä»–å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼›2) margin lossçš„marginå€¼çš„è®¾ç½®ï¼Œéœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒæ•´ï¼›3) æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„ç½‘ç»œç»“æ„é€‰æ‹©ï¼Œå¯ä»¥é‡‡ç”¨å¸¸è§çš„å·ç§¯ç¥ç»ç½‘ç»œæˆ–Transformeræ¨¡å‹ï¼›4) æŸå¤±å‡½æ•°çš„æƒé‡è®¾ç½®ï¼Œéœ€è¦å¹³è¡¡äº¤å‰ç†µæŸå¤±ã€ç±»å†…å¯¹æ¯”æŸå¤±å’Œmargin lossä¹‹é—´çš„è´¡çŒ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨CIFAR-100æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ResNet-32ä½œä¸ºå­¦ç”Ÿæ¨¡å‹ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”äºä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼ŒTop-1å‡†ç¡®ç‡æå‡äº†2%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­å‡ºç°çš„è´Ÿè¿ç§»é—®é¢˜ï¼Œæå‡å­¦ç”Ÿæ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦çŸ¥è¯†è’¸é¦çš„åœºæ™¯ï¼Œä¾‹å¦‚æ¨¡å‹å‹ç¼©ã€æ¨¡å‹åŠ é€Ÿã€è¿ç§»å­¦ä¹ ç­‰ã€‚ç‰¹åˆ«æ˜¯åœ¨æ•°æ®æ ‡æ³¨æˆæœ¬è¾ƒé«˜æˆ–æ•°æ®åˆ†å¸ƒä¸å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•å¯ä»¥é€šè¿‡å¢å¼ºè½¯æ ‡ç­¾çš„ç±»å†…ä¿¡æ¯æ¥æå‡å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Since the advent of knowledge distillation, much research has focused on how the soft labels generated by the teacher model can be utilized effectively. Existing studies points out that the implicit knowledge within soft labels originates from the multi-view structure present in the data. Feature variations within samples of the same class allow the student model to generalize better by learning diverse representations. However, in existing distillation methods, teacher models predominantly adhere to ground-truth labels as targets, without considering the diverse representations within the same class. Therefore, we propose incorporating an intra-class contrastive loss during teacher training to enrich the intra-class information contained in soft labels. In practice, we find that intra-class loss causes instability in training and slows convergence. To mitigate these issues, margin loss is integrated into intra-class contrastive learning to improve the training stability and convergence speed. Simultaneously, we theoretically analyze the impact of this loss on the intra-class distances and inter-class distances. It has been proved that the intra-class contrastive loss can enrich the intra-class diversity. Experimental results demonstrate the effectiveness of the proposed method.

