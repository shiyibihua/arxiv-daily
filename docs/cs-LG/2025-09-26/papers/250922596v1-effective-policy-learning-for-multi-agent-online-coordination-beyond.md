---
layout: default
title: Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives
---

# Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22596" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22596v1</a>
  <a href="https://arxiv.org/pdf/2509.22596.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22596v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22596v1', 'Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qixin Zhang, Yan Sun, Can Jin, Xikun Zhang, Yao Shu, Puning Zhao, Li Shen, Dacheng Tao

**åˆ†ç±»**: cs.MA, cs.LG, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: Accepted to NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMA-SPLå’ŒMA-MPLç®—æ³•ä»¥è§£å†³å¤šæ™ºèƒ½ä½“åœ¨çº¿åè°ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `åœ¨çº¿åè°ƒ` `ç­–ç•¥å­¦ä¹ ` `å­æ¨¡ç›®æ ‡` `ç®—æ³•ä¼˜åŒ–` `èµ„æºåˆ†é…` `æœºå™¨äººåä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ™ºèƒ½ä½“åœ¨çº¿åè°ƒé—®é¢˜æ—¶ï¼Œå¾€å¾€ä¾èµ–äºæœªçŸ¥çš„å‚æ•°ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§å’Œçµæ´»æ€§ã€‚
2. è®ºæ–‡æå‡ºçš„MA-SPLå’ŒMA-MPLç®—æ³•é€šè¿‡å¼•å…¥åŸºäºç­–ç•¥çš„è¿ç»­æ‰©å±•æŠ€æœ¯ï¼Œå…‹æœäº†å¯¹æœªçŸ¥å‚æ•°çš„ä¾èµ–ï¼Œæå‡äº†ç®—æ³•çš„é€‚åº”æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æç®—æ³•åœ¨å¤„ç†å¼±å­æ¨¡ç›®æ ‡æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸¤ç§æœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ ç®—æ³•ï¼Œé’ˆå¯¹å¤šæ™ºèƒ½ä½“åœ¨çº¿åè°ƒï¼ˆMA-OCï¼‰é—®é¢˜ã€‚ç¬¬ä¸€ç§ç®—æ³•MA-SPLä¸ä»…èƒ½å¤Ÿåœ¨å…·æœ‰å­æ¨¡ç›®æ ‡çš„æƒ…å†µä¸‹å®ç°æœ€ä¼˜çš„$(1-rac{c}{e})$è¿‘ä¼¼ä¿è¯ï¼Œè¿˜èƒ½å¤„ç†æœªæ¢ç´¢çš„$Î±$-å¼±DR-å­æ¨¡å’Œ$(Î³,Î²)$-å¼±å­æ¨¡åœºæ™¯ã€‚ä¸ºäº†å‡å°‘å¯¹æœªçŸ¥å‚æ•°$Î±,Î³,Î²$çš„ä¾èµ–ï¼Œè¿›ä¸€æ­¥æå‡ºäº†å®Œå…¨æ— å‚æ•°çš„MA-MPLç®—æ³•ï¼Œèƒ½å¤Ÿä¿æŒä¸MA-SPLç›¸åŒçš„è¿‘ä¼¼æ¯”ã€‚ä¸¤ç§ç®—æ³•çš„æ ¸å¿ƒæ˜¯æ–°é¢–çš„è¿ç»­æ¾å¼›æŠ€æœ¯ï¼Œç§°ä¸ºåŸºäºç­–ç•¥çš„è¿ç»­æ‰©å±•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å¤šçº¿æ€§æ‰©å±•ï¼Œå…·æœ‰æ— æŸèˆå…¥çš„ä¼˜åŠ¿ï¼Œä»è€Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„å¼±å­æ¨¡ç›®æ ‡ã€‚æœ€åï¼Œé€šè¿‡å¤§é‡ä»¿çœŸå®éªŒéªŒè¯äº†æ‰€æç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“åœ¨çº¿åè°ƒï¼ˆMA-OCï¼‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å…·æœ‰å­æ¨¡ç›®æ ‡çš„æƒ…å†µä¸‹ï¼Œå¾€å¾€éœ€è¦ä¾èµ–æœªçŸ¥çš„å‚æ•°ï¼Œå¯¼è‡´çµæ´»æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„MA-SPLå’ŒMA-MPLç®—æ³•é€šè¿‡å¼•å…¥åŸºäºç­–ç•¥çš„è¿ç»­æ‰©å±•æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–æœªçŸ¥å‚æ•°çš„æƒ…å†µä¸‹ï¼Œä¿æŒè‰¯å¥½çš„è¿‘ä¼¼æ€§èƒ½ï¼Œé€‚åº”æ›´å¹¿æ³›çš„åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç¬¬ä¸€æ¨¡å—ä¸ºMA-SPLç®—æ³•ï¼Œå¤„ç†å·²çŸ¥å‚æ•°çš„æƒ…å†µï¼›ç¬¬äºŒæ¨¡å—ä¸ºMA-MPLç®—æ³•ï¼Œå®Œå…¨æ— å‚æ•°ï¼Œé€‚ç”¨äºæœªçŸ¥å‚æ•°çš„åœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºåŸºäºç­–ç•¥çš„è¿ç»­æ‰©å±•æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æä¾›äº†æ— æŸèˆå…¥æ–¹æ¡ˆï¼Œä½¿å¾—ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¼±å­æ¨¡ç›®æ ‡ï¼Œä¸ä¼ ç»Ÿçš„å¤šçº¿æ€§æ‰©å±•æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šMA-SPLç®—æ³•ä¾èµ–äºå‚æ•°$Î±,Î³,Î²$çš„è®¾ç½®ï¼Œè€ŒMA-MPLç®—æ³•åˆ™å®Œå…¨æ— å‚æ•°è®¾è®¡ï¼Œç¡®ä¿åœ¨å¤šç§åœºæ™¯ä¸‹å‡èƒ½ä¿æŒç›¸åŒçš„è¿‘ä¼¼æ¯”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMA-SPLå’ŒMA-MPLç®—æ³•åœ¨å¤„ç†å¼±å­æ¨¡ç›®æ ‡æ—¶ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ™ºèƒ½ä½“åœ¨çº¿åè°ƒé—®é¢˜ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€æœºå™¨äººåä½œã€èµ„æºåˆ†é…ç­‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åè°ƒä»»åŠ¡ã€‚é€šè¿‡æœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ ç®—æ³•ï¼Œå¯ä»¥æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œæ•ˆç‡å’Œå†³ç­–èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we present two effective policy learning algorithms for multi-agent online coordination(MA-OC) problem. The first one, \texttt{MA-SPL}, not only can achieve the optimal $(1-\frac{c}{e})$-approximation guarantee for the MA-OC problem with submodular objectives but also can handle the unexplored $Î±$-weakly DR-submodular and $(Î³,Î²)$-weakly submodular scenarios, where $c$ is the curvature of the investigated submodular functions, $Î±$ denotes the diminishing-return(DR) ratio and the tuple $(Î³,Î²)$ represents the submodularity ratios. Subsequently, in order to reduce the reliance on the unknown parameters $Î±,Î³,Î²$ inherent in the \texttt{MA-SPL} algorithm, we further introduce the second online algorithm named \texttt{MA-MPL}. This \texttt{MA-MPL} algorithm is entirely \emph{parameter-free} and simultaneously can maintain the same approximation ratio as the first \texttt{MA-SPL} algorithm. The core of our \texttt{MA-SPL} and \texttt{MA-MPL} algorithms is a novel continuous-relaxation technique termed as \emph{policy-based continuous extension}. Compared with the well-established \emph{multi-linear extension}, a notable advantage of this new \emph{policy-based continuous extension} is its ability to provide a lossless rounding scheme for any set function, thereby enabling us to tackle the challenging weakly submodular objectives. Finally, extensive simulations are conducted to validate the effectiveness of our proposed algorithms.

