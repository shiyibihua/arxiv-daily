---
layout: default
title: In-Context Learning can Perform Continual Learning Like Humans
---

# In-Context Learning can Perform Continual Learning Like Humans

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22764" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22764v1</a>
  <a href="https://arxiv.org/pdf/2509.22764.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22764v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22764v1', 'In-Context Learning can Perform Continual Learning Like Humans')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liuwang Kang, Fan Wang, Shaoshan Liu, Hung-Chyun Chou, Chuan Lin, Ning Ding

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸Šä¸‹æ–‡æŒç»­å­¦ä¹ (ICCL)ï¼Œå®ç°ç±»äººé•¿æœŸè®°å¿†å’Œè·¨ä»»åŠ¡çŸ¥è¯†ç§¯ç´¯ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æŒç»­å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¾éš¾æ€§é—å¿˜` `äººç±»è®°å¿†` `åˆ†å¸ƒå¼å®è·µ` `æç¤ºå·¥ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æŒç»­å­¦ä¹ æ–¹æ³•é¢ä¸´ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¸”ç¼ºä¹å¯¹äººç±»è®¤çŸ¥æœºåˆ¶çš„æœ‰æ•ˆæ¨¡æ‹Ÿã€‚
2. æå‡ºä¸Šä¸‹æ–‡æŒç»­å­¦ä¹ (ICCL)æ¡†æ¶ï¼Œé€šè¿‡ä»»åŠ¡è°ƒåº¦å’Œæç¤ºé‡æ’å®ç°æŒç»­å­¦ä¹ èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ICCLå—ç›Šäºåˆ†å¸ƒå¼å®è·µï¼Œä¸”çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹è¡¨ç°å‡ºæ›´æ¥è¿‘äººç±»çš„è®°å¿†æ¨¡å¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ (ICL)é€‚åº”æ–°ä»»åŠ¡ï¼Œè€Œæ— éœ€å‚æ•°æ›´æ–°ï¼Œä½¿å…¶æˆä¸ºå¿«é€Ÿé€‚åº”çš„å¼ºå¤§å­¦ä¹ å¼•æ“ã€‚è™½ç„¶å¤§é‡ç ”ç©¶å·²ç»å°†ICLä½œä¸ºä¸€ç§å°‘æ ·æœ¬å­¦ä¹ å™¨è¿›è¡Œäº†è€ƒå¯Ÿï¼Œä½†å½“å¤šä»»åŠ¡æŒ‰é¡ºåºåˆ°è¾¾æ—¶ï¼Œå®ƒæ˜¯å¦èƒ½å¤Ÿå®ç°é•¿æœŸè®°å¿†å’Œè·¨ä»»åŠ¡çŸ¥è¯†ç§¯ç´¯ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å—äººç±»è®°å¿†ç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ICLåœ¨å¤šä»»åŠ¡ç¯å¢ƒä¸­çš„è®°å¿†ç‰¹æ€§ï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°ä¸Šä¸‹æ–‡æŒç»­å­¦ä¹ (ICCL)ï¼Œå…¶ä¸­æŒç»­å­¦ä¹ èƒ½åŠ›é€šè¿‡ä»»åŠ¡è°ƒåº¦å’Œæç¤ºé‡æ’è€Œäº§ç”Ÿã€‚åœ¨é©¬å°”å¯å¤«é“¾åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¯¹äºç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒICCLä»¥ç±»ä¼¼äºäººç±»çš„æ–¹å¼å—ç›Šäºåˆ†å¸ƒå¼å®è·µ(DP)ï¼Œå§‹ç»ˆæ­ç¤ºäº†ç”¨äºè®°å¿†çš„é—´éš”â€œæœ€ä½³ç‚¹â€ã€‚é™¤äº†è®°å¿†æ€§èƒ½ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§äººç±»è®°å¿†ç›¸ä¼¼æ€§åº¦é‡ï¼Œä»¥é‡åŒ–æŒç»­å­¦ä¹ (CL)æ–¹æ³•ä¸äººç±»è®°å¿†åŠ¨æ€çš„åŒ¹é…ç¨‹åº¦ã€‚ä½¿ç”¨è¿™ç§åº¦é‡ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼ŒåƒMAMBAå’ŒRWKVè¿™æ ·çš„çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹è¡¨ç°å‡ºç‰¹åˆ«åƒäººç±»çš„è®°å¿†æ¨¡å¼ï¼Œå°½ç®¡å®ƒä»¬çš„è®°å¿†æ€§èƒ½è½åäºåŸºäºTransformerçš„LLMã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»“æœå°†ICCLç¡®ç«‹ä¸ºä¸€ç§è®¤çŸ¥ä¸Šåˆç†ä¸”å®è·µä¸Šæœ‰æ•ˆçš„æ¨ç†å¼CLèŒƒä¾‹ï¼Œå®ƒå¯ä»¥å‡è½»ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶è§£å†³ä¼ ç»ŸCLæ–¹æ³•ä¸­çš„ç¨³å®šæ€§-å¯å¡‘æ€§å›°å¢ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸‹çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå³æ¨¡å‹åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œä¼šé—å¿˜ä¹‹å‰å­¦ä¹ è¿‡çš„ä»»åŠ¡ã€‚ç°æœ‰çš„æŒç»­å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å‚æ•°æ›´æ–°ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹äººç±»è®¤çŸ¥æœºåˆ¶çš„æœ‰æ•ˆæ¨¡æ‹Ÿï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡è°ƒåº¦å’Œæç¤ºé‡æ’ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸æ›´æ–°å‚æ•°çš„æƒ…å†µä¸‹ï¼ŒæŒç»­å­¦ä¹ æ–°ä»»åŠ¡å¹¶ä¿æŒå¯¹æ—§ä»»åŠ¡çš„è®°å¿†ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»çš„å­¦ä¹ æ–¹å¼ï¼Œå³é€šè¿‡é—´éš”é‡å¤å’Œåˆ†å¸ƒå¼å®è·µæ¥æé«˜è®°å¿†æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šICCLæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä»»åŠ¡åºåˆ—ç”Ÿæˆï¼šæŒ‰ç…§ä¸€å®šçš„ç­–ç•¥ï¼ˆå¦‚éšæœºã€åˆ†å¸ƒå¼ï¼‰ç”Ÿæˆä»»åŠ¡åºåˆ—ã€‚2) ä¸Šä¸‹æ–‡æ„å»ºï¼šä¸ºæ¯ä¸ªä»»åŠ¡æ„å»ºåŒ…å«å°‘é‡æ ·æœ¬çš„ä¸Šä¸‹æ–‡æç¤ºã€‚3) æ¨ç†ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œæ¨ç†ã€‚4) æç¤ºé‡æ’ï¼šæ ¹æ®ä»»åŠ¡çš„è®°å¿†æ•ˆæœï¼ŒåŠ¨æ€è°ƒæ•´æç¤ºçš„é¡ºåºå’Œå†…å®¹ï¼Œä»¥ä¼˜åŒ–æ•´ä½“çš„è®°å¿†æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸Šä¸‹æ–‡æŒç»­å­¦ä¹ (ICCL)çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶ä¸äººç±»è®°å¿†æœºåˆ¶ç›¸ç»“åˆã€‚ICCLæ˜¯ä¸€ç§å®Œå…¨åŸºäºæ¨ç†çš„æŒç»­å­¦ä¹ æ–¹æ³•ï¼Œæ— éœ€å‚æ•°æ›´æ–°ï¼Œä»è€Œé¿å…äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§äººç±»è®°å¿†ç›¸ä¼¼æ€§åº¦é‡ï¼Œç”¨äºè¯„ä¼°ä¸åŒæŒç»­å­¦ä¹ æ–¹æ³•ä¸äººç±»è®°å¿†åŠ¨æ€çš„åŒ¹é…ç¨‹åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä»»åŠ¡è°ƒåº¦æ–¹é¢ï¼Œè®ºæ–‡ç ”ç©¶äº†åˆ†å¸ƒå¼å®è·µ(DP)å¯¹ICCLçš„å½±å“ï¼Œå‘ç°å­˜åœ¨ä¸€ä¸ªé—´éš”â€œæœ€ä½³ç‚¹â€ï¼Œå¯ä»¥æœ€å¤§åŒ–è®°å¿†æ•ˆæœã€‚åœ¨æç¤ºé‡æ’æ–¹é¢ï¼Œè®ºæ–‡æ¢ç´¢äº†ä¸åŒçš„é‡æ’ç­–ç•¥ï¼Œå¦‚åŸºäºè®°å¿†æ•ˆæœçš„é‡æ’å’ŒåŸºäºä»»åŠ¡ç›¸ä¼¼æ€§çš„é‡æ’ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ä½¿ç”¨äº†é©¬å°”å¯å¤«é“¾ä½œä¸ºåŸºå‡†æµ‹è¯•ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒICCLåœ¨é©¬å°”å¯å¤«é“¾åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†è‰¯å¥½çš„è®°å¿†æ€§èƒ½ï¼Œå¹¶ä¸”å—ç›Šäºåˆ†å¸ƒå¼å®è·µã€‚çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ï¼ˆå¦‚MAMBAå’ŒRWKVï¼‰è™½ç„¶è®°å¿†æ€§èƒ½ä¸å¦‚Transformerï¼Œä½†è¡¨ç°å‡ºæ›´æ¥è¿‘äººç±»çš„è®°å¿†æ¨¡å¼ã€‚è®ºæ–‡æå‡ºçš„è®°å¿†ç›¸ä¼¼æ€§åº¦é‡ä¸ºè¯„ä¼°æŒç»­å­¦ä¹ æ–¹æ³•ä¸äººç±»è®¤çŸ¥æœºåˆ¶çš„åŒ¹é…ç¨‹åº¦æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ICCLæ–¹æ³•å¯åº”ç”¨äºéœ€è¦æŒç»­å­¦ä¹ å’Œå¿«é€Ÿé€‚åº”çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½å®¢æœã€å¯¹è¯ç³»ç»Ÿã€æœºå™¨äººå¯¼èˆªç­‰ã€‚è¯¥æ–¹æ³•æ— éœ€å‚æ•°æ›´æ–°ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶å…·æœ‰è¾ƒå¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æœªæ¥å¯è¿›ä¸€æ­¥ç ”ç©¶ICCLåœ¨æ›´å¤æ‚ä»»åŠ¡å’Œæ›´å¤§è§„æ¨¡æ¨¡å‹ä¸Šçš„åº”ç”¨ï¼Œå¹¶æ¢ç´¢æ›´æœ‰æ•ˆçš„ä»»åŠ¡è°ƒåº¦å’Œæç¤ºé‡æ’ç­–ç•¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) can adapt to new tasks via in-context learning (ICL) without parameter updates, making them powerful learning engines for fast adaptation. While extensive research has examined ICL as a few-shot learner, whether it can achieve long-term retention and cross-task knowledge accumulation when multitasks arrive sequentially remains underexplored. Motivated by human memory studies, we investigate the retention characteristics of ICL in multitask settings and extend it to in-context continual learning (ICCL), where continual learning ability emerges through task scheduling and prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that, for specific large-language models, ICCL benefits from distributed practice (DP) in a manner analogous to humans, consistently revealing a spacing "sweet spot" for retention. Beyond retention performance, we propose a human-retention similarity metric to quantify how closely a continual-learning (CL) method aligns with human retention dynamics. Using this metric, we show that linear-attention models such as MAMBA and RWKV exhibit particularly human-like retention patterns, despite their retention performance lagging behind that of Transformer-based LLMs. Overall, our results establish ICCL as both cognitively plausible and practically effective, providing an inference-only CL paradigm that mitigates catastrophic forgetting and addresses the stability-plasticity dilemma in conventional CL methods.

