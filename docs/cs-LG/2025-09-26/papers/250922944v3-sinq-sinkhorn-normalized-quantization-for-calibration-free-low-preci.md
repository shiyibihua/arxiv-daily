---
layout: default
title: SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights
---

# SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22944" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22944v3</a>
  <a href="https://arxiv.org/pdf/2509.22944.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22944v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22944v3', 'SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lorenz K. MÃ¼ller, Philippe Bich, Jiawei Zhuang, Ahmet Ã‡elik, Luca Benfenati, Lukas Cavigelli

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-10-09)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/huawei-csl/SINQ)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SINQï¼šç”¨äºå…æ ¡å‡†ä½ç²¾åº¦LLMæƒé‡ Sinkhorn å½’ä¸€åŒ–é‡åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç²¾åº¦é‡åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `åè®­ç»ƒé‡åŒ–` `Sinkhornå½’ä¸€åŒ–` `çŸ©é˜µå¹³è¡¡` `å…æ ¡å‡†é‡åŒ–` `æ¨¡å‹å‹ç¼©` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä½ç²¾åº¦é‡åŒ–æ–¹æ³•åœ¨4æ¯”ç‰¹åŠä»¥ä¸‹ä½å®½æ—¶ï¼Œç”±äºå¼‚å¸¸å€¼å½±å“ï¼Œå¯¼è‡´å›°æƒ‘åº¦ä¸‹é™ã€‚
2. SINQé€šè¿‡Sinkhornå½’ä¸€åŒ–ï¼Œå¯»æ‰¾æ¯è¡Œæ¯åˆ—çš„ç¼©æ”¾å› å­ï¼Œæœ€å°åŒ–çŸ©é˜µä¸å¹³è¡¡ï¼Œæå‡é‡åŒ–ç²¾åº¦ã€‚
3. åœ¨Qwen3å’ŒDeepSeek-V2.5ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSINQæ˜¾è‘—ä¼˜äºæœªæ ¡å‡†çš„å‡åŒ€é‡åŒ–åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åè®­ç»ƒé‡åŒ–å·²æˆä¸ºä»¥ä½ç²¾åº¦éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€å¹¿æ³›ä½¿ç”¨çš„ç­–ç•¥ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨å°äºç­‰äº4æ¯”ç‰¹çš„ä½å®½ä¸‹è¡¨ç°å‡ºå›°æƒ‘åº¦ä¸‹é™ï¼Œéƒ¨åˆ†åŸå› æ˜¯è¡¨ç¤ºå¼‚å¸¸å€¼å¯¼è‡´ä¸è¿™äº›å¼‚å¸¸å€¼å…±äº«ç›¸åŒå°ºåº¦çš„å‚æ•°å‡ºç°ç²¾åº¦é—®é¢˜ã€‚å¯¹äºå…æ ¡å‡†çš„å‡åŒ€é‡åŒ–æ–¹æ³•ï¼Œè¿™ä¸ªé—®é¢˜å°¤å…¶çªå‡ºã€‚æˆ‘ä»¬å¼•å…¥SINQï¼Œé€šè¿‡é¢å¤–çš„ç¬¬äºŒè½´æ¯”ä¾‹å› å­å’Œå¿«é€ŸSinkhorn-Knoppé£æ ¼çš„ç®—æ³•æ¥å¢å¼ºç°æœ‰çš„åè®­ç»ƒé‡åŒ–å™¨ï¼Œè¯¥ç®—æ³•å¯»æ‰¾æ¯”ä¾‹æ¥å½’ä¸€åŒ–æ¯è¡Œå’Œæ¯åˆ—çš„æ–¹å·®ï¼Œä»è€Œæœ€å°åŒ–é‡åŒ–çš„æ¯ä¸ªçŸ©é˜µä»£ç†ç›®æ ‡ï¼šçŸ©é˜µä¸å¹³è¡¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å±‚ä¹‹é—´æ²¡æœ‰äº¤äº’ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºæ–°çš„æ¶æ„æ¥é‡åŒ–ä»»ä½•çº¿æ€§å±‚ã€‚æˆ‘ä»¬åœ¨Qwen3æ¨¡å‹ç³»åˆ—å’ŒDeepSeek-V2.5ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚SINQæ˜¾è‘—æé«˜äº†WikiText2å’ŒC4çš„å›°æƒ‘åº¦ï¼Œä¼˜äºæœªæ ¡å‡†çš„å‡åŒ€é‡åŒ–åŸºçº¿ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å°†å…¶ä¸æ ¡å‡†å’Œéå‡åŒ€é‡åŒ–çº§åˆ«ç›¸ç»“åˆæ¥è¿›ä¸€æ­¥å¢å¼ºã€‚é‡ç°è¿™é¡¹å·¥ä½œç»“æœå¹¶ä½¿ç”¨SINQè½»æ¾é‡åŒ–æ¨¡å‹çš„ä»£ç å¯åœ¨https://github.com/huawei-csl/SINQè·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½ç²¾åº¦é‡åŒ–æ—¶ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨å…æ ¡å‡†çš„å‡åŒ€é‡åŒ–æ–¹æ³•æ—¶ï¼Œç”±äºæƒé‡çŸ©é˜µä¸­å¼‚å¸¸å€¼çš„å­˜åœ¨è€Œå¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†è¿™äº›å¼‚å¸¸å€¼ï¼Œå¯¼è‡´å…±äº«ç›¸åŒç¼©æ”¾å› å­çš„å‚æ•°ç²¾åº¦é™ä½ï¼Œæœ€ç»ˆå½±å“æ¨¡å‹çš„å›°æƒ‘åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥Sinkhornå½’ä¸€åŒ–ï¼Œå¯¹æƒé‡çŸ©é˜µçš„è¡Œå’Œåˆ—åˆ†åˆ«è¿›è¡Œç¼©æ”¾ï¼Œä»è€Œå¹³è¡¡çŸ©é˜µçš„å…ƒç´ åˆ†å¸ƒï¼Œå‡å°‘å¼‚å¸¸å€¼çš„å½±å“ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æœ€å°åŒ–ä¸€ä¸ªæ–°å®šä¹‰çš„â€œçŸ©é˜µä¸å¹³è¡¡â€æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡ä½œä¸ºé‡åŒ–çš„ä»£ç†ç›®æ ‡ã€‚é€šè¿‡å¹³è¡¡çŸ©é˜µï¼Œå¯ä»¥æé«˜é‡åŒ–åçš„æƒé‡ç²¾åº¦ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSINQæ–¹æ³•å¯ä»¥è¢«è§†ä¸ºç°æœ‰åè®­ç»ƒé‡åŒ–å™¨çš„å¢å¼ºæ¨¡å—ã€‚å®ƒä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1. å¯¹æƒé‡çŸ©é˜µè¿›è¡Œé¢„å¤„ç†ï¼Œè®¡ç®—æ¯è¡Œå’Œæ¯åˆ—çš„æ–¹å·®ã€‚2. ä½¿ç”¨Sinkhorn-Knoppç®—æ³•ï¼Œè¿­ä»£åœ°å¯»æ‰¾è¡Œå’Œåˆ—çš„ç¼©æ”¾å› å­ï¼Œä»¥å½’ä¸€åŒ–æ–¹å·®ã€‚3. å°†ç¼©æ”¾åçš„æƒé‡çŸ©é˜µè¿›è¡Œé‡åŒ–ã€‚4. åœ¨æ¨ç†æ—¶ï¼Œå°†é‡åŒ–åçš„æƒé‡åé‡åŒ–ï¼Œå¹¶åº”ç”¨ç›¸åº”çš„ç¼©æ”¾å› å­ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šå±‚ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•çº¿æ€§å±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šSINQçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1. æå‡ºäº†â€œçŸ©é˜µä¸å¹³è¡¡â€çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶ä½œä¸ºé‡åŒ–çš„ä»£ç†ç›®æ ‡ã€‚2. åˆ©ç”¨Sinkhorn-Knoppç®—æ³•ï¼Œé«˜æ•ˆåœ°å¯»æ‰¾è¡Œå’Œåˆ—çš„ç¼©æ”¾å› å­ï¼Œå®ç°çŸ©é˜µçš„å½’ä¸€åŒ–ã€‚3. è¯¥æ–¹æ³•æ˜¯å…æ ¡å‡†çš„ï¼Œä¸éœ€è¦é¢å¤–çš„æ ¡å‡†æ•°æ®ï¼Œé™ä½äº†é‡åŒ–çš„å¤æ‚åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒSINQå…³æ³¨äºå¹³è¡¡æƒé‡çŸ©é˜µçš„å…ƒç´ åˆ†å¸ƒï¼Œè€Œä¸æ˜¯ç®€å•åœ°å¯¹æƒé‡è¿›è¡Œé‡åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šSINQçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. Sinkhorn-Knoppç®—æ³•çš„è¿­ä»£æ¬¡æ•°ï¼šéœ€è¦æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ï¼Œä»¥è¾¾åˆ°æœ€ä½³çš„å½’ä¸€åŒ–æ•ˆæœã€‚2. çŸ©é˜µä¸å¹³è¡¡çš„å®šä¹‰ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†ç‰¹å®šçš„çŸ©é˜µä¸å¹³è¡¡åº¦é‡ï¼Œå…¶ä»–åº¦é‡æ–¹å¼ä¹Ÿå¯èƒ½é€‚ç”¨ã€‚3. ä¸ç°æœ‰é‡åŒ–å™¨çš„é›†æˆæ–¹å¼ï¼šSINQå¯ä»¥ä½œä¸ºç°æœ‰é‡åŒ–å™¨çš„é¢„å¤„ç†æ­¥éª¤ï¼Œä¹Ÿå¯ä»¥ä¸æ ¡å‡†å’Œéå‡åŒ€é‡åŒ–æ–¹æ³•ç»“åˆä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSINQåœ¨Qwen3æ¨¡å‹ç³»åˆ—å’ŒDeepSeek-V2.5ä¸Šæ˜¾è‘—æé«˜äº†WikiText2å’ŒC4æ•°æ®é›†çš„å›°æƒ‘åº¦ï¼Œä¼˜äºæœªæ ¡å‡†çš„å‡åŒ€é‡åŒ–åŸºçº¿ã€‚å…·ä½“è€Œè¨€ï¼ŒSINQåœ¨ä¿æŒæˆ–é™ä½æ¨¡å‹å¤§å°çš„åŒæ—¶ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSINQè¿˜å¯ä»¥ä¸æ ¡å‡†å’Œéå‡åŒ€é‡åŒ–æ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜é‡åŒ–æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SINQæ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä½ç²¾åº¦éƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚é€šè¿‡é™ä½æ¨¡å‹çš„å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼ŒSINQèƒ½å¤Ÿä½¿LLMåœ¨ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿç­‰å¹³å°ä¸Šé«˜æ•ˆè¿è¡Œã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–ç±»å‹çš„ç¥ç»ç½‘ç»œï¼Œä»¥æé«˜å…¶åœ¨ä½ç²¾åº¦ä¸‹çš„æ€§èƒ½ã€‚æœªæ¥ï¼ŒSINQæœ‰æœ›æˆä¸ºLLMé‡åŒ–çš„æ ‡å‡†æ–¹æ³•ä¹‹ä¸€ï¼Œæ¨åŠ¨AIæŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸçš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths less than or equal to 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing a novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers. We evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code to reproduce the results of this work and to easily quantize models using SINQ is available at https://github.com/huawei-csl/SINQ.

