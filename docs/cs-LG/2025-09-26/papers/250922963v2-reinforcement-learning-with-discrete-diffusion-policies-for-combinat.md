---
layout: default
title: Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces
---

# Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22963" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22963v2</a>
  <a href="https://arxiv.org/pdf/2509.22963.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22963v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22963v2', 'Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haitong Ma, Ofir Nabati, Aviv Rosenberg, Bo Dai, Oran Lang, Idan Szpektor, Craig Boutilier, Na Li, Shie Mannor, Lior Shani, Guy Tenneholtz

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-10-01)

**å¤‡æ³¨**: 22 pages, 10 figures. Haitong Ma and Ofir Nabati contributed equally to this paper

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç¦»æ•£æ‰©æ•£ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³ç»„åˆåŠ¨ä½œç©ºé—´éš¾é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç¦»æ•£æ‰©æ•£æ¨¡å‹` `ç»„åˆåŠ¨ä½œç©ºé—´` `ç­–ç•¥é•œåƒä¸‹é™` `åˆ†å¸ƒåŒ¹é…` `ç­–ç•¥å­¦ä¹ ` `ç»„åˆä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼ºåŒ–å­¦ä¹ åœ¨å¤„ç†å¤§è§„æ¨¡ç»„åˆåŠ¨ä½œç©ºé—´æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ‰©å±•ã€‚
2. åˆ©ç”¨ç­–ç•¥é•œåƒä¸‹é™å®šä¹‰ç›®æ ‡ç­–ç•¥åˆ†å¸ƒï¼Œå°†ç­–ç•¥æ›´æ–°è§†ä¸ºåˆ†å¸ƒåŒ¹é…é—®é¢˜ï¼Œè®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚
3. åœ¨å¤šä¸ªç»„åˆä¼˜åŒ–ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„æ€§èƒ½å’Œæ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶æˆä¸ºå¤æ‚ç»„åˆåŠ¨ä½œç©ºé—´ä¸­é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºé«˜æ•ˆçš„åœ¨çº¿è®­ç»ƒè¿‡ç¨‹ï¼Œç¡®ä¿ç­–ç•¥æ”¹è¿›çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚é€šè¿‡åˆ©ç”¨ç­–ç•¥é•œåƒä¸‹é™ï¼ˆPMDï¼‰å®šä¹‰ç†æƒ³çš„ã€æ­£åˆ™åŒ–çš„ç›®æ ‡ç­–ç•¥åˆ†å¸ƒï¼Œå°†ç­–ç•¥æ›´æ–°è½¬åŒ–ä¸ºåˆ†å¸ƒåŒ¹é…é—®é¢˜ï¼Œè®­ç»ƒå…·æœ‰è¡¨è¾¾èƒ½åŠ›çš„æ‰©æ•£æ¨¡å‹æ¥å¤åˆ¶è¿™ç§ç¨³å®šçš„ç›®æ ‡ã€‚è¿™ç§è§£è€¦æ–¹æ³•ç¨³å®šäº†å­¦ä¹ è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ€§èƒ½ã€‚åœ¨åŒ…æ‹¬DNAåºåˆ—ç”Ÿæˆã€å®åŠ¨ä½œå¼ºåŒ–å­¦ä¹ å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿç­‰ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„ç»„åˆåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœå’Œå“è¶Šçš„æ ·æœ¬æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰©æ•£ç­–ç•¥ç›¸æ¯”å…¶ä»–åŸºçº¿æ–¹æ³•è·å¾—äº†æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ åœ¨å…·æœ‰å¤§è§„æ¨¡ç»„åˆåŠ¨ä½œç©ºé—´ç¯å¢ƒä¸‹çš„åº”ç”¨éš¾é¢˜ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¦‚åŸºäºå€¼å‡½æ•°æˆ–ç­–ç•¥æ¢¯åº¦çš„æ–¹æ³•ï¼Œåœ¨é¢å¯¹ç»„åˆåŠ¨ä½œç©ºé—´æ—¶ï¼Œç”±äºåŠ¨ä½œæ•°é‡å·¨å¤§ï¼Œæ¢ç´¢æ•ˆç‡ä½ï¼Œè®­ç»ƒä¸ç¨³å®šï¼Œéš¾ä»¥æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•æœ‰æ•ˆåœ°å¤„ç†è¿™ç§é«˜ç»´ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å­¦ä¹ è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªåˆ†å¸ƒåŒ¹é…é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡ç­–ç•¥é•œåƒä¸‹é™ï¼ˆPMDï¼‰ç®—æ³•ï¼Œå®šä¹‰ä¸€ä¸ªç†æƒ³çš„ã€æ­£åˆ™åŒ–çš„ç›®æ ‡ç­–ç•¥åˆ†å¸ƒã€‚ç„¶åï¼Œåˆ©ç”¨ç¦»æ•£æ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ é€¼è¿‘è¿™ä¸ªç›®æ ‡ç­–ç•¥åˆ†å¸ƒã€‚è¿™æ ·ï¼Œç­–ç•¥çš„æ›´æ–°ä¸å†æ˜¯ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°ï¼Œè€Œæ˜¯è®­ç»ƒæ‰©æ•£æ¨¡å‹å»æ‹Ÿåˆä¸€ä¸ªæ›´åŠ ç¨³å®šå’Œå¯æ§çš„ç›®æ ‡åˆ†å¸ƒï¼Œä»è€Œæé«˜å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š
1. **ç¯å¢ƒäº¤äº’**ï¼šæ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ç»éªŒæ•°æ®ã€‚
2. **ç›®æ ‡ç­–ç•¥è®¡ç®—**ï¼šåˆ©ç”¨ç­–ç•¥é•œåƒä¸‹é™ï¼ˆPMDï¼‰ç®—æ³•ï¼Œæ ¹æ®æ”¶é›†åˆ°çš„ç»éªŒæ•°æ®ï¼Œè®¡ç®—å‡ºä¸€ä¸ªæ­£åˆ™åŒ–çš„ç›®æ ‡ç­–ç•¥åˆ†å¸ƒã€‚
3. **æ‰©æ•£æ¨¡å‹è®­ç»ƒ**ï¼šä½¿ç”¨æ”¶é›†åˆ°çš„ç»éªŒæ•°æ®å’Œè®¡ç®—å‡ºçš„ç›®æ ‡ç­–ç•¥åˆ†å¸ƒï¼Œè®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆé€¼è¿‘ç›®æ ‡ç­–ç•¥çš„åŠ¨ä½œã€‚
4. **ç­–ç•¥æ‰§è¡Œ**ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ‰©æ•£æ¨¡å‹ä½œä¸ºç­–ç•¥ï¼ŒæŒ‡å¯¼æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†ç¦»æ•£æ‰©æ•£æ¨¡å‹å¼•å…¥å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å­¦ä¹ ä¸­ï¼Œå¹¶å°†å…¶ä¸ç­–ç•¥é•œåƒä¸‹é™ç®—æ³•ç›¸ç»“åˆã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ ä¸€ä¸ªåˆ†å¸ƒæ¥è¡¨ç¤ºç­–ç•¥ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°ï¼Œä»è€Œæé«˜äº†å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ç­–ç•¥é•œåƒä¸‹é™ç®—æ³•æ¥å®šä¹‰ç›®æ ‡ç­–ç•¥åˆ†å¸ƒï¼Œå¯ä»¥æœ‰æ•ˆåœ°é¿å…ç­–ç•¥çš„å‰§çƒˆå˜åŒ–ï¼Œè¿›ä¸€æ­¥ç¨³å®šå­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼š
*   **ç¦»æ•£æ‰©æ•£æ¨¡å‹**ï¼šä½¿ç”¨ç¦»æ•£æ‰©æ•£æ¨¡å‹æ¥è¡¨ç¤ºç­–ç•¥ï¼Œæ¨¡å‹ç»“æ„çš„é€‰æ‹©éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨Transformerç»“æ„ã€‚
*   **ç­–ç•¥é•œåƒä¸‹é™ï¼ˆPMDï¼‰**ï¼šPMDç®—æ³•ç”¨äºè®¡ç®—ç›®æ ‡ç­–ç•¥åˆ†å¸ƒï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„æ­£åˆ™åŒ–é¡¹å’Œå­¦ä¹ ç‡ã€‚
*   **æŸå¤±å‡½æ•°**ï¼šè®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æŸå¤±å‡½æ•°é€šå¸¸é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œç”¨äºè¡¡é‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åŠ¨ä½œåˆ†å¸ƒä¸ç›®æ ‡ç­–ç•¥åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚
*   **é‡‡æ ·æ–¹æ³•**ï¼šä»æ‰©æ•£æ¨¡å‹ä¸­é‡‡æ ·åŠ¨ä½œæ—¶ï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„é‡‡æ ·ç­–ç•¥ï¼Œä¾‹å¦‚ancestral samplingæˆ–Denoising Diffusion Implicit Models (DDIM)ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨DNAåºåˆ—ç”Ÿæˆã€å®åŠ¨ä½œå¼ºåŒ–å­¦ä¹ å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿç­‰å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç»„åˆä¼˜åŒ–ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨DNAåºåˆ—ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„åºåˆ—è´¨é‡æ˜æ˜¾ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚åœ¨å®åŠ¨ä½œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´åŠ é«˜æ•ˆçš„å®åŠ¨ä½œç­–ç•¥ï¼Œä»è€Œæ˜¾è‘—æé«˜å­¦ä¹ æ•ˆç‡ã€‚å®éªŒè¿˜è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨æ›´å°‘çš„è®­ç»ƒæ ·æœ¬ä¸‹è¾¾åˆ°æ›´å¥½çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºç»„åˆä¼˜åŒ–ã€æœºå™¨äººæ§åˆ¶ã€æ¨èç³»ç»Ÿã€è¯ç‰©å‘ç°ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ§åˆ¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å­¦ä¹ å¤æ‚çš„åŠ¨ä½œåºåˆ—ï¼Œä»è€Œå®ç°æ›´åŠ çµæ´»å’Œé«˜æ•ˆçš„æ§åˆ¶ç­–ç•¥ã€‚åœ¨æ¨èç³»ç»Ÿä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•ç”Ÿæˆä¸ªæ€§åŒ–çš„æ¨èåˆ—è¡¨ï¼Œæé«˜ç”¨æˆ·æ»¡æ„åº¦ã€‚åœ¨è¯ç‰©å‘ç°é¢†åŸŸï¼Œå¯ä»¥ç”¨äºç”Ÿæˆå…·æœ‰ç‰¹å®šæ€§è´¨çš„åˆ†å­ç»“æ„ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) struggles to scale to large, combinatorial action spaces common in many real-world problems. This paper introduces a novel framework for training discrete diffusion models as highly effective policies in these complex settings. Our key innovation is an efficient online training process that ensures stable and effective policy improvement. By leveraging policy mirror descent (PMD) to define an ideal, regularized target policy distribution, we frame the policy update as a distributional matching problem, training the expressive diffusion model to replicate this stable target. This decoupled approach stabilizes learning and significantly enhances training performance. Our method achieves state-of-the-art results and superior sample efficiency across a diverse set of challenging combinatorial benchmarks, including DNA sequence generation, RL with macro-actions, and multi-agent systems. Experiments demonstrate that our diffusion policies attain superior performance compared to other baselines.

