---
layout: default
title: Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data
---

# Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22507" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.22507v1</a>
  <a href="https://arxiv.org/pdf/2509.22507.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22507v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22507v1', 'Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zahid Iqbal

**ÂàÜÁ±ª**: cs.LG, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-26

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ëá™ÈÄÇÂ∫îÂèåÊ®°ÂºèËí∏È¶è‰∏éÊøÄÂä±Êú∫Âà∂ÔºåËß£ÂÜ≥ÈùûÁã¨Á´ãÂêåÂàÜÂ∏ÉÊï∞ÊçÆ‰∏ãÂºÇÊûÑËÅîÈÇ¶Â≠¶‰π†ÁöÑÂèØÊâ©Â±ïÊÄßÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËÅîÈÇ¶Â≠¶‰π†` `Áü•ËØÜËí∏È¶è` `Ê®°ÂûãÂºÇÊûÑ` `Êï∞ÊçÆÂºÇÊûÑ` `ÈùûÁã¨Á´ãÂêåÂàÜÂ∏É` `ÊøÄÂä±Êú∫Âà∂` `Ëá™ÈÄÇÂ∫îÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ËÅîÈÇ¶Â≠¶‰π†Èù¢‰∏¥ÂÆ¢Êà∑Á´ØÂºÇÊûÑÊÄß‰∏éÊï∞ÊçÆÈùûÁã¨Á´ãÂêåÂàÜÂ∏ÉÁöÑÊåëÊàòÔºåÂØºËá¥ÂÖ®Â±ÄÊ®°ÂûãÊÄßËÉΩ‰∏ãÈôçÔºå‰∏îÁº∫‰πèÊúâÊïàÁöÑÊøÄÂä±Êú∫Âà∂„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Ëá™ÈÄÇÂ∫îÂèåÊ®°ÂºèËí∏È¶èÊ°ÜÊû∂ÔºåÂåÖÂê´DL-SH„ÄÅDL-MHÂíåI-DL-MH‰∏âÁßçÊñπÊ≥ïÔºåÂàÜÂà´Â∫îÂØπÁªüËÆ°ÂºÇÊûÑ„ÄÅÊ®°ÂûãÂºÇÊûÑ‰ª•ÂèäÊøÄÂä±‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®ÈùûÁã¨Á´ãÂêåÂàÜÂ∏ÉÊï∞ÊçÆ‰∏ãÊòæËëóÊèêÂçá‰∫ÜÂÖ®Â±ÄÊ®°ÂûãÁ≤æÂ∫¶ÔºåDL-SHÊèêÂçá153%ÔºåI-DL-MHÊèêÂçá225%ÔºåÂπ∂Èôç‰Ωé‰∫ÜÈÄö‰ø°ÊàêÊú¨„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËÅîÈÇ¶Â≠¶‰π†(FL)‰Ωú‰∏∫‰∏ÄÁßçÊúâÂâçÊôØÁöÑÂéª‰∏≠ÂøÉÂåñÂ≠¶‰π†(DL)ÊñπÊ≥ïÔºåËÉΩÂ§üÂú®‰∏çÊçüÂÆ≥Áî®Êà∑ÈöêÁßÅÁöÑÊÉÖÂÜµ‰∏ãÂà©Áî®ÂàÜÂ∏ÉÂºèÊï∞ÊçÆ„ÄÇÁÑ∂ËÄåÔºåFLÈù¢‰∏¥Âá†‰∏™ÂÖ≥ÈîÆÊåëÊàò„ÄÇÈ¶ñÂÖàÔºåÈÄöÂ∏∏ÂÅáËÆæÊØè‰∏™ÂÆ¢Êà∑Á´ØÈÉΩÂèØ‰ª•ËÆ≠ÁªÉÁõ∏ÂêåÁöÑÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÔºå‰ΩÜÁî±‰∫é‰∏öÂä°ÈúÄÊ±ÇÂíåËÆ°ÁÆóËµÑÊ∫êÁöÑÂ∑ÆÂºÇÔºåÂπ∂ÈùûÊâÄÊúâÂÆ¢Êà∑Á´ØÈÉΩËÉΩÊª°Ë∂≥Ëøô‰∏ÄÂÅáËÆæ„ÄÇÂÖ∂Ê¨°ÔºåÁªüËÆ°ÂºÇË¥®ÊÄß(ÂèàÁß∞ÈùûÁã¨Á´ãÂêåÂàÜÂ∏ÉÊï∞ÊçÆ)ÊòØFL‰∏≠ÁöÑ‰∏Ä‰∏™‰∏ªË¶ÅÊåëÊàòÔºåÂèØËÉΩÂØºËá¥ÂÖ®Â±ÄÊ®°ÂûãÊÄßËÉΩ‰∏ãÈôç„ÄÇÁ¨¨‰∏âÔºåÂú®Â∫îÂØπËøô‰∫õÊåëÊàòÁöÑÂêåÊó∂ÔºåÈúÄË¶Å‰∏ÄÁßçÁªèÊµéÈ´òÊïàÁöÑÊøÄÂä±Êú∫Âà∂Êù•ÈºìÂä±ÂÆ¢Êà∑ÂèÇ‰∏éFLËÆ≠ÁªÉ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂá†ÁßçÊñπÊ≥ïÔºöDL-SHÔºåÂÆÉ‰øÉËøõ‰∫ÜÂú®ÁªüËÆ°ÂºÇË¥®ÊÄßËÉåÊôØ‰∏ãÈ´òÊïà„ÄÅ‰øùÊä§ÈöêÁßÅÂíåÈÄö‰ø°È´òÊïàÁöÑÂ≠¶‰π†ÔºõDL-MHÔºåÊó®Âú®ÁÆ°ÁêÜÂÆåÂÖ®ÂºÇÊûÑÁöÑÊ®°ÂûãÔºåÂêåÊó∂Ëß£ÂÜ≥ÁªüËÆ°Â∑ÆÂºÇÔºõ‰ª•ÂèäI-DL-MHÔºåDL-MHÁöÑÂü∫‰∫éÊøÄÂä±ÁöÑÊâ©Â±ïÔºåÈÄöËøáÂú®Ëøô‰∏™Â§çÊùÇÁöÑËÅîÈÇ¶Â≠¶‰π†Ê°ÜÊû∂ÂÜÖÊèê‰æõÊøÄÂä±Êù•‰øÉËøõÂÆ¢Êà∑Á´ØÂèÇ‰∏éËÅîÈÇ¶Â≠¶‰π†ËÆ≠ÁªÉ„ÄÇËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÂÆûÈ™åÔºå‰ª•ËØÑ‰º∞ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÂêÑÁßçÂ§çÊùÇÂÆûÈ™åËÆæÁΩÆ‰∏≠ÁöÑÊÄßËÉΩÂíåÂèØÊâ©Â±ïÊÄß„ÄÇËøôÂåÖÊã¨Âà©Áî®ÂêÑÁßçÊ®°ÂûãÊû∂ÊûÑÔºåÂú®‰∏çÂêåÁöÑÊï∞ÊçÆÂàÜÂ∏É‰∏≠ÔºåÂåÖÊã¨Áã¨Á´ãÂêåÂàÜÂ∏ÉÂíåÂá†‰∏™ÈùûÁã¨Á´ãÂêåÂàÜÂ∏ÉÂú∫ÊôØÔºå‰ª•ÂèäÂ§ö‰∏™Êï∞ÊçÆÈõÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÂíåÂü∫Á∫øÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÂπ∂Èôç‰Ωé‰∫ÜÈÄö‰ø°ÊàêÊú¨ÔºåÂêåÊó∂ÊúâÊïàÂú∞Ëß£ÂÜ≥‰∫ÜÁªüËÆ°ÂºÇË¥®ÊÄßÂíåÊ®°ÂûãÂºÇË¥®ÊÄßÔºåÂÖ∂‰∏≠DL-SHÂ∞ÜÂÖ®Â±ÄÊ®°ÂûãÂáÜÁ°ÆÊÄßÊèêÈ´ò‰∫Ü153%ÔºåI-DL-MHÂú®ÈùûÁã¨Á´ãÂêåÂàÜÂ∏ÉÊù°‰ª∂‰∏ãÂÆûÁé∞‰∫Ü225%ÁöÑÊîπËøõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÅîÈÇ¶Â≠¶‰π†‰∏≠ÔºåÂÆ¢Êà∑Á´ØÁöÑËÆ°ÁÆóËµÑÊ∫êÂíåÊï∞ÊçÆÂàÜÂ∏ÉÂ≠òÂú®Â∑ÆÂºÇÔºåÂØºËá¥Ê®°ÂûãËÆ≠ÁªÉÂõ∞Èöæ„ÄÇ‰º†ÁªüÁöÑËÅîÈÇ¶Â≠¶‰π†ÊñπÊ≥ïÂÅáËÆæÊâÄÊúâÂÆ¢Êà∑Á´Ø‰ΩøÁî®Áõ∏ÂêåÁöÑÊ®°ÂûãÔºåËøôÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂæÄÂæÄ‰∏çÊàêÁ´ã„ÄÇÊ≠§Â§ñÔºåÈùûÁã¨Á´ãÂêåÂàÜÂ∏É(Non-IID)ÁöÑÊï∞ÊçÆÂàÜÂ∏É‰ºöËøõ‰∏ÄÊ≠•Èôç‰ΩéÂÖ®Â±ÄÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁº∫‰πèÊúâÊïàÁöÑÊøÄÂä±Êú∫Âà∂‰πüÈòªÁ¢ç‰∫ÜÂÆ¢Êà∑Á´ØÂèÇ‰∏éËÅîÈÇ¶Â≠¶‰π†ÁöÑÁßØÊûÅÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Áü•ËØÜËí∏È¶èÊäÄÊúØÔºåÂ∞Ü‰∏çÂêåÂÆ¢Êà∑Á´ØËÆ≠ÁªÉÁöÑÊ®°ÂûãÁü•ËØÜËøÅÁßªÂà∞ÂÖ®Â±ÄÊ®°ÂûãÔºå‰ªéËÄåËß£ÂÜ≥Ê®°ÂûãÂºÇÊûÑÊÄßÂíåÊï∞ÊçÆÂºÇÊûÑÊÄßÈóÆÈ¢ò„ÄÇÂêåÊó∂ÔºåÂºïÂÖ•ÊøÄÂä±Êú∫Âà∂ÔºåÈºìÂä±ÂÆ¢Êà∑Á´ØÂèÇ‰∏éËÅîÈÇ¶Â≠¶‰π†ËÆ≠ÁªÉ„ÄÇÈÄöËøáËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥Ëí∏È¶èÁ≠ñÁï•ÂíåÊøÄÂä±ÊñπÊ°àÔºåÊèêÈ´òËÅîÈÇ¶Â≠¶‰π†ÁöÑÊÄßËÉΩÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏â‰∏™‰∏ªË¶ÅÊñπÊ≥ïÔºöDL-SH„ÄÅDL-MHÂíåI-DL-MH„ÄÇDL-SH‰∏ªË¶ÅËß£ÂÜ≥ÁªüËÆ°ÂºÇÊûÑÈóÆÈ¢òÔºåÈÄöËøáÂèåÊ®°ÂºèËí∏È¶èÔºåÂ∞ÜÂÆ¢Êà∑Á´ØÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞ÊúçÂä°Âô®Ê®°Âûã„ÄÇDL-MHÊó®Âú®ÁÆ°ÁêÜÂÆåÂÖ®ÂºÇÊûÑÁöÑÊ®°ÂûãÔºåÂêåÊó∂Ëß£ÂÜ≥ÁªüËÆ°Â∑ÆÂºÇ„ÄÇI-DL-MHÊòØDL-MHÁöÑÊâ©Â±ïÔºåÂºïÂÖ•‰∫ÜÊøÄÂä±Êú∫Âà∂ÔºåÈºìÂä±ÂÆ¢Êà∑Á´ØÂèÇ‰∏éËÅîÈÇ¶Â≠¶‰π†ËÆ≠ÁªÉ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨ÂÆ¢Êà∑Á´ØÊú¨Âú∞ËÆ≠ÁªÉ„ÄÅÁü•ËØÜËí∏È¶è„ÄÅÊúçÂä°Âô®Ê®°ÂûãÊõ¥Êñ∞ÂíåÊøÄÂä±ÂàÜÈÖçÁ≠âÊ≠•È™§„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜËá™ÈÄÇÂ∫îÂèåÊ®°ÂºèËí∏È¶èÊ°ÜÊû∂ÔºåËÉΩÂ§üÊúâÊïàÂú∞Ëß£ÂÜ≥ËÅîÈÇ¶Â≠¶‰π†‰∏≠ÁöÑÊ®°ÂûãÂºÇÊûÑÊÄßÂíåÊï∞ÊçÆÂºÇÊûÑÊÄßÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåÂºïÂÖ•‰∫ÜÊøÄÂä±Êú∫Âà∂ÔºåÈºìÂä±ÂÆ¢Êà∑Á´ØÂèÇ‰∏éËÅîÈÇ¶Â≠¶‰π†ËÆ≠ÁªÉÔºåÊèêÈ´ò‰∫ÜËÅîÈÇ¶Â≠¶‰π†ÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇÂèåÊ®°ÂºèËí∏È¶èÂÖÅËÆ∏ÊúçÂä°Âô®ÂíåÂÆ¢Êà∑Á´Ø‰πãÈó¥ËøõË°åÂèåÂêëÁü•ËØÜËøÅÁßªÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Âà©Áî®ÂÆ¢Êà∑Á´ØÁöÑÁü•ËØÜ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠ÔºåÊøÄÂä±Êú∫Âà∂ÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆ„ÄÇÂÖ∑‰ΩìÁöÑÊøÄÂä±ÊñπÊ°àÔºàI-DL-MHÔºâÊ†πÊçÆÂÆ¢Êà∑Á´ØÂØπÂÖ®Â±ÄÊ®°ÂûãÁöÑË¥°ÁåÆÁ®ãÂ∫¶ËøõË°åÂ•ñÂä±ÔºåË¥°ÁåÆË∂äÂ§ßÔºåÂ•ñÂä±Ë∂äÈ´ò„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüËá≥ÂÖ≥ÈáçË¶ÅÔºåÈúÄË¶ÅÂπ≥Ë°°ÂÆ¢Êà∑Á´ØÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíå‰∏éÂÖ®Â±ÄÊ®°ÂûãÁöÑÁõ∏‰ººÂ∫¶„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÊ†πÊçÆ‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÂíå‰ªªÂä°ËøõË°åË∞ÉÊï¥Ôºå‰ΩÜÊï¥‰ΩìÊ°ÜÊû∂‰øùÊåÅ‰∏çÂèò„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜÂíåÈùûÁã¨Á´ãÂêåÂàÜÂ∏ÉÂú∫ÊôØ‰∏ãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂÖ®Â±ÄÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇDL-SHÂú®ÈùûÁã¨Á´ãÂêåÂàÜÂ∏ÉÊù°‰ª∂‰∏ãÂ∞ÜÂÖ®Â±ÄÊ®°ÂûãÂáÜÁ°ÆÊÄßÊèêÈ´ò‰∫Ü153%ÔºåI-DL-MHÂÆûÁé∞‰∫Ü225%ÁöÑÊîπËøõ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÈôç‰Ωé‰∫ÜÈÄö‰ø°ÊàêÊú¨ÔºåÊèêÈ´ò‰∫ÜËÅîÈÇ¶Â≠¶‰π†ÁöÑÊïàÁéá„ÄÇ‰∏éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®ÊÄßËÉΩÂíåÂèØÊâ©Â±ïÊÄßÊñπÈù¢ÈÉΩÂÖ∑Êúâ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶Å‰øùÊä§Áî®Êà∑ÈöêÁßÅÁöÑÂàÜÂ∏ÉÂºèÂ≠¶‰π†Âú∫ÊôØÔºå‰æãÂ¶ÇÂåªÁñóÂÅ•Â∫∑„ÄÅÈáëËûçÈ£éÊéß„ÄÅÊô∫ËÉΩ‰∫§ÈÄöÁ≠â„ÄÇÈÄöËøáËÅîÈÇ¶Â≠¶‰π†ÔºåÂèØ‰ª•Âú®‰∏çÂÖ±‰∫´ÂéüÂßãÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÂà©Áî®ÂêÑ‰∏™ÂèÇ‰∏éÊñπÁöÑÊï∞ÊçÆËøõË°åÊ®°ÂûãËÆ≠ÁªÉÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂÆâÂÖ®ÊÄß„ÄÇËØ•ÊñπÊ≥ïÂØπ‰∫éËµÑÊ∫êÂèóÈôêÁöÑËæπÁºòËÆæÂ§áÂ∞§‰∏∫ÈáçË¶ÅÔºåËÉΩÂ§üÂÆûÁé∞Êõ¥È´òÊïà„ÄÅÊõ¥ÂèØÈù†ÁöÑËÅîÈÇ¶Â≠¶‰π†„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Federated Learning (FL) has emerged as a promising decentralized learning (DL) approach that enables the use of distributed data without compromising user privacy. However, FL poses several key challenges. First, it is frequently assumed that every client can train the same machine learning models, however, not all clients are able to meet this assumption because of differences in their business needs and computational resources. Second, statistical heterogeneity (a.k.a. non-IID data) poses a major challenge in FL, which can lead to lower global model performance. Third, while addressing these challenges, there is a need for a cost-effective incentive mechanism to encourage clients to participate in FL training. In response to these challenges, we propose several methodologies: DL-SH, which facilitates efficient, privacy-preserving, and communication-efficient learning in the context of statistical heterogeneity; DL-MH, designed to manage fully heterogeneous models while tackling statistical disparities; and I-DL-MH, an incentive-based extension of DL-MH that promotes client engagement in federated learning training by providing incentives within this complex federated learning framework. Comprehensive experiments were carried out to assess the performance and scalability of the proposed approaches across a range of complex experimental settings. This involved utilizing various model architectures, in diverse data distributions, including IID and several non-IID scenarios, as well as multiple datasets. Experimental results demonstrate that the proposed approaches significantly enhance accuracy and decrease communication costs while effectively addressing statistical heterogeneity and model heterogeneity in comparison to existing state-of-the-art approaches and baselines, with DL-SH improving global model accuracy by 153%, and I-DL-MH achieving a 225% improvement under non-IID conditions.

