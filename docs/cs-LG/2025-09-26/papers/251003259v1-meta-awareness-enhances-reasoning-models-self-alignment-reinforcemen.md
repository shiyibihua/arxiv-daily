---
layout: default
title: Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning
---

# Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03259" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03259v1</a>
  <a href="https://arxiv.org/pdf/2510.03259.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03259v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.03259v1', 'Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yoonjeon Kim, Doohyuk Jang, Eunho Yang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMASAè‡ªå¯¹é½å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨ç†æ¨¡å‹å…ƒè®¤çŸ¥èƒ½åŠ›ä¸æ³›åŒ–æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å…ƒè®¤çŸ¥` `è‡ªå¯¹é½` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†æ¨¡å‹` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹æ¨ç†æ¨¡å‹ç¼ºä¹å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå¯¼è‡´çœŸå®æ‰§è¡Œè¿‡ç¨‹ä¸é¢„æµ‹çš„å…ƒä¿¡æ¯ä¸ä¸€è‡´ï¼Œå½±å“æ¨ç†æ€§èƒ½ã€‚
2. æå‡ºMASAè‡ªå¯¹é½å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è‡ªç”Ÿæˆä¿¡å·è®­ç»ƒå…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå¯¹é½å…ƒé¢„æµ‹ä¸çœŸå®æ‰§è¡Œè¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMASAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œè®­ç»ƒæ•ˆç‡ï¼Œå¹¶å¢å¼ºäº†é¢†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æ¨ç†æ¨¡å‹ä¸­çš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå³æ¨¡å‹è‡ªæˆ‘æ€è€ƒæ–¹å¼çš„è®¤çŸ¥ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ç¼ºä¹è¿™ç§å…ƒè®¤çŸ¥èƒ½åŠ›ï¼ŒçœŸå®æ‰§è¡Œè¿‡ç¨‹ä¸é¢„æµ‹çš„å…ƒä¿¡æ¯ä¹‹é—´å­˜åœ¨ä¸¥é‡çš„ä¸ä¸€è‡´ã€‚è®ºæ–‡æå‡ºï¼Œå°†å…ƒé¢„æµ‹ä¸çœŸå®æ‰§è¡Œè¿‡ç¨‹å¯¹é½èƒ½å¤Ÿæ˜¾è‘—æå‡æ€§èƒ½ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€å‡è®¾ï¼Œè®¾è®¡äº†ä¸€ç§é€šè¿‡è‡ªå¯¹é½å¢å¼ºå…ƒè®¤çŸ¥ï¼ˆMASAï¼‰çš„è®­ç»ƒæµç¨‹ï¼Œè¯æ˜äº†å¢å¼ºçš„å…ƒè®¤çŸ¥èƒ½åŠ›å¯ä»¥ç›´æ¥è½¬åŒ–ä¸ºå‡†ç¡®æ€§çš„æé«˜ã€‚ä¸ç°æœ‰çš„å…ƒè®¤çŸ¥æ¨ç†æ¨¡å‹ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¤–éƒ¨è®­ç»ƒæºï¼Œè€Œæ˜¯åˆ©ç”¨è‡ªç”Ÿæˆçš„ä¿¡å·æ¥è®­ç»ƒå…ƒè®¤çŸ¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡è¿‡æ»¤é›¶æ–¹å·®æç¤ºå’Œæå‰ç»ˆæ­¢ä¸å¤ªå¯èƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆçš„å†—é•¿æ‰§è¡Œè¿‡ç¨‹ï¼Œå®ç°äº†é«˜æ•ˆè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨é¢†åŸŸå†…ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œè®­ç»ƒæ•ˆç‡ï¼Œå¹¶åœ¨é¢†åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•å¯ä»¥åŠ é€ŸGRPOè®­ç»ƒè¶…è¿‡1.28å€ä»¥è¾¾åˆ°ç›¸åŒçš„æ€§èƒ½ï¼Œå¹¶åœ¨AIME25ä¸Šå®ç°19.3%çš„å‡†ç¡®ç‡æå‡ï¼Œåœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡6.2%ã€‚é€šè¿‡å…ƒè®¤çŸ¥æŒ‡å¯¼è¿›è¡Œè®­ç»ƒå¢å¼ºäº†é¢†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨GPQA-Diamondä¸Šæå‡äº†3.87%ï¼Œå¹¶åœ¨æ¶µç›–é€»è¾‘ã€ç§‘å­¦å’Œç¼–ç é¢†åŸŸçš„13ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†2.08%çš„æ€»ä½“å‡†ç¡®ç‡æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿›è¡Œå¤æ‚æ¨ç†æ—¶ï¼Œç¼ºä¹å¯¹è‡ªèº«æ¨ç†è¿‡ç¨‹çš„è®¤çŸ¥ï¼ˆå…ƒè®¤çŸ¥ï¼‰ã€‚è¿™ç§ç¼ºå¤±å¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆåœ°è§„åˆ’å’Œè°ƒæ•´æ¨ç†æ­¥éª¤ï¼Œæœ€ç»ˆå½±å“æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ç°æœ‰æ–¹æ³•æˆ–è€…ä¾èµ–å¤–éƒ¨çŸ¥è¯†ï¼Œæˆ–è€…æ— æ³•æœ‰æ•ˆåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„åé¦ˆä¿¡æ¯ï¼Œå­˜åœ¨è®­ç»ƒæˆæœ¬é«˜ã€æ³›åŒ–èƒ½åŠ›å¼±ç­‰é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªå¯¹é½çš„æ–¹å¼ï¼Œå¢å¼ºæ¨¡å‹å¯¹è‡ªèº«æ¨ç†è¿‡ç¨‹çš„è®¤çŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹éœ€è¦å­¦ä¹ é¢„æµ‹å…¶æ¨ç†è¿‡ç¨‹ä¸­çš„å…ƒä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œæ¯ä¸€æ­¥çš„ç½®ä¿¡åº¦ã€ä¸‹ä¸€æ­¥è¡ŒåŠ¨çš„ä»·å€¼ç­‰ï¼‰ï¼Œå¹¶åˆ©ç”¨è¿™äº›å…ƒä¿¡æ¯æ¥æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸çœŸå®æ¨ç†è¿‡ç¨‹ä¸€è‡´çš„å…ƒä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMASAçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ¨ç†æ¨¡å‹ï¼šè´Ÿè´£æ‰§è¡Œæ¨ç†ä»»åŠ¡ï¼Œå¹¶ç”Ÿæˆæ¨ç†è½¨è¿¹ã€‚2) å…ƒé¢„æµ‹å™¨ï¼šé¢„æµ‹æ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„å…ƒä¿¡æ¯ã€‚3) å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼šåˆ©ç”¨è‡ªç”Ÿæˆçš„ä¿¡å·ï¼Œè®­ç»ƒå…ƒé¢„æµ‹å™¨ï¼Œä½¿å…¶é¢„æµ‹çš„å…ƒä¿¡æ¯ä¸çœŸå®æ¨ç†è¿‡ç¨‹å¯¹é½ã€‚4) è¿‡æ»¤æ¨¡å—ï¼šè¿‡æ»¤æ‰é›¶æ–¹å·®çš„promptï¼Œå‡å°‘æ— æ•ˆè®­ç»ƒã€‚5) æˆªæ–­æ¨¡å—ï¼šæå‰ç»ˆæ­¢ä¸å¤ªå¯èƒ½æˆåŠŸçš„æ¨ç†è½¨è¿¹ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šMASAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è‡ªå¯¹é½çš„è®­ç»ƒæ–¹å¼ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒMASAä¸éœ€è¦å¤–éƒ¨è®­ç»ƒæ•°æ®æˆ–äººå·¥æ ‡æ³¨ï¼Œè€Œæ˜¯åˆ©ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„æ¨ç†è½¨è¿¹å’Œå…ƒä¿¡æ¯è¿›è¡Œè®­ç»ƒã€‚è¿™ç§è‡ªç›‘ç£çš„æ–¹å¼é™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMASAè¿˜é€šè¿‡è¿‡æ»¤å’Œæˆªæ–­ç­‰ç­–ç•¥ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šMASAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å…ƒä¿¡æ¯çš„é€‰æ‹©ï¼šè®ºæ–‡é€‰æ‹©äº†ç½®ä¿¡åº¦ã€ä»·å€¼ç­‰ä½œä¸ºå…ƒä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯èƒ½å¤Ÿæœ‰æ•ˆåœ°åæ˜ æ¨¡å‹çš„æ¨ç†çŠ¶æ€ã€‚2) å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼šå¥–åŠ±å‡½æ•°é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸çœŸå®æ¨ç†è¿‡ç¨‹ä¸€è‡´çš„å…ƒä¿¡æ¯ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ¨¡å‹é¢„æµ‹ä¸‹ä¸€æ­¥è¡ŒåŠ¨çš„ä»·å€¼å¾ˆé«˜ï¼Œä½†å®é™…æ‰§è¡Œåå¯¼è‡´æ¨ç†å¤±è´¥ï¼Œåˆ™ä¼šå—åˆ°æƒ©ç½šã€‚3) è¿‡æ»¤å’Œæˆªæ–­ç­–ç•¥ï¼šé€šè¿‡è®¾ç½®é˜ˆå€¼ï¼Œè¿‡æ»¤æ‰é›¶æ–¹å·®çš„promptå’Œæå‰ç»ˆæ­¢ä¸å¤ªå¯èƒ½æˆåŠŸçš„æ¨ç†è½¨è¿¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMASAåœ¨AIME25ä¸Šå®ç°äº†19.3%çš„å‡†ç¡®ç‡æå‡ï¼Œåœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡6.2%ã€‚æ­¤å¤–ï¼ŒMASAè¿˜å¯ä»¥åŠ é€ŸGRPOè®­ç»ƒè¶…è¿‡1.28å€ä»¥è¾¾åˆ°ç›¸åŒçš„æ€§èƒ½ã€‚åœ¨é¢†åŸŸå¤–æ³›åŒ–æ–¹é¢ï¼ŒMASAåœ¨GPQA-Diamondä¸Šæå‡äº†3.87%ï¼Œå¹¶åœ¨æ¶µç›–é€»è¾‘ã€ç§‘å­¦å’Œç¼–ç é¢†åŸŸçš„13ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†2.08%çš„æ€»ä½“å‡†ç¡®ç‡æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMASAèƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æ¨ç†æ¨¡å‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ç§‘å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è¿™äº›ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæ™ºèƒ½å¯¹è¯ç³»ç»Ÿï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œå¹¶ç”Ÿæˆæ›´åˆç†çš„å›å¤ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning meta-prediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design a training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO training by over 1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 % boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains.

