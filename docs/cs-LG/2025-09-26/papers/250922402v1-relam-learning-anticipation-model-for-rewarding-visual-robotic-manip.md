---
layout: default
title: ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation
---

# ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22402" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22402v1</a>
  <a href="https://arxiv.org/pdf/2509.22402.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22402v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22402v1', 'ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nan Tang, Jing-Cheng Pang, Guanlin Li, Chao Qian, Yang Yu

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReLAMï¼Œé€šè¿‡å­¦ä¹ é¢„æµ‹æ¨¡å‹ä¸ºè§†è§‰æœºå™¨äººæ“ä½œç”Ÿæˆå¥–åŠ±**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰æœºå™¨äººæ“ä½œ` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å‡½æ•°è®¾è®¡` `é¢„æµ‹æ¨¡å‹` `åˆ†å±‚å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰æœºå™¨äººæ“ä½œä¸­ï¼Œä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¾èµ–ç²¾ç¡®çš„ä½ç½®ä¿¡æ¯è®¾è®¡å¥–åŠ±ï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œæ„ŸçŸ¥é™åˆ¶å¯¼è‡´éš¾ä»¥è·å–ã€‚
2. ReLAMé€šè¿‡å­¦ä¹ é¢„æµ‹æ¨¡å‹ï¼Œä»è§†é¢‘æ¼”ç¤ºä¸­æå–å…³é”®ç‚¹ï¼Œè‡ªåŠ¨ç”Ÿæˆå¯†é›†ä¸”ç»“æ„åŒ–çš„å¥–åŠ±ï¼Œå¼•å¯¼æœºå™¨äººå­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒReLAMåœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸­æ˜¾è‘—åŠ é€Ÿå­¦ä¹ ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è§†è§‰æœºå™¨äººæ“ä½œçš„å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¥–åŠ±è®¾è®¡ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®ç“¶é¢ˆã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œå¥–åŠ±é€šå¸¸åŸºäºä¸ç›®æ ‡ä½ç½®çš„è·ç¦»æ¥è®¾è®¡ã€‚ç„¶è€Œï¼Œç”±äºæ„Ÿè§‰å’Œæ„ŸçŸ¥çš„é™åˆ¶ï¼Œè¿™ç§ç²¾ç¡®çš„ä½ç½®ä¿¡æ¯åœ¨çœŸå®ä¸–ç•Œçš„è§†è§‰ç¯å¢ƒä¸­é€šå¸¸æ˜¯ä¸å¯ç”¨çš„ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡å›¾åƒä¸­æå–çš„å…³é”®ç‚¹æ¥éšå¼æ¨æ–­ç©ºé—´è·ç¦»çš„æ–¹æ³•ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºé¢„æµ‹æ¨¡å‹çš„å¥–åŠ±å­¦ä¹ ï¼ˆReLAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥ä»æ— åŠ¨ä½œçš„è§†é¢‘æ¼”ç¤ºä¸­è‡ªåŠ¨ç”Ÿæˆå¯†é›†ã€ç»“æ„åŒ–çš„å¥–åŠ±ã€‚ReLAMé¦–å…ˆå­¦ä¹ ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å……å½“è§„åˆ’å™¨ï¼Œå¹¶åœ¨è¾¾åˆ°æœ€ç»ˆç›®æ ‡çš„æœ€ä½³è·¯å¾„ä¸Šæå‡ºåŸºäºå…³é”®ç‚¹çš„ä¸­é—´å­ç›®æ ‡ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªä¸ä»»åŠ¡çš„å‡ ä½•ç›®æ ‡ç›´æ¥å¯¹é½çš„ç»“æ„åŒ–å­¦ä¹ è¯¾ç¨‹ã€‚åŸºäºé¢„æµ‹çš„å­ç›®æ ‡ï¼Œæä¾›è¿ç»­çš„å¥–åŠ±ä¿¡å·ï¼Œä»¥åœ¨å…·æœ‰å¯è¯æ˜çš„æ¬¡ä¼˜ç•Œé™çš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ ï¼ˆHRLï¼‰æ¡†æ¶ä¸‹è®­ç»ƒä½çº§ã€ç›®æ ‡æ¡ä»¶ç­–ç•¥ã€‚åœ¨å¤æ‚ã€é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒReLAMæ˜¾è‘—åŠ é€Ÿäº†å­¦ä¹ å¹¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰æœºå™¨äººæ“ä½œä¸­çš„å¼ºåŒ–å­¦ä¹ ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®éš¾é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºç²¾ç¡®çš„ä½ç½®ä¿¡æ¯ï¼Œä¾‹å¦‚ä¸ç›®æ ‡ç‚¹çš„è·ç¦»ï¼Œä½†åœ¨çœŸå®è§†è§‰åœºæ™¯ä¸‹ï¼Œç”±äºä¼ æ„Ÿå™¨å™ªå£°å’Œæ„ŸçŸ¥è¯¯å·®ï¼Œéš¾ä»¥è·å¾—å‡†ç¡®çš„ä½ç½®ä¿¡æ¯ï¼Œå¯¼è‡´å¥–åŠ±å‡½æ•°è®¾è®¡å›°éš¾ï¼Œå½±å“å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReLAMçš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨æ— åŠ¨ä½œçš„è§†é¢‘æ¼”ç¤ºå­¦ä¹ ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹è¾¾åˆ°ç›®æ ‡çš„å…³é”®è·¯å¾„ä¸Šçš„ä¸­é—´å­ç›®æ ‡ï¼ˆå…³é”®ç‚¹ï¼‰ã€‚é€šè¿‡è¿™äº›å­ç›®æ ‡ï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªç»“æ„åŒ–çš„å­¦ä¹ è¯¾ç¨‹ï¼Œå¹¶ç”Ÿæˆå¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼æœºå™¨äººé€æ­¥å­¦ä¹ å®Œæˆä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥ä¾èµ–ç²¾ç¡®çš„ä½ç½®ä¿¡æ¯ï¼Œè€Œæ˜¯é€šè¿‡å­¦ä¹ é¢„æµ‹æ¥éšå¼åœ°æ¨æ–­ç©ºé—´å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReLAMé‡‡ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ ï¼ˆHRLï¼‰æ¡†æ¶ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) ä»æ— åŠ¨ä½œçš„è§†é¢‘æ¼”ç¤ºä¸­å­¦ä¹ ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥å…³é”®ç‚¹ä¸ºåŸºç¡€ï¼Œé¢„æµ‹è¾¾åˆ°ç›®æ ‡çš„ä¸­é—´å­ç›®æ ‡ï¼›2) åŸºäºé¢„æµ‹çš„å­ç›®æ ‡ï¼Œç”Ÿæˆè¿ç»­çš„å¥–åŠ±ä¿¡å·ï¼›3) åˆ©ç”¨è¯¥å¥–åŠ±ä¿¡å·ï¼Œè®­ç»ƒä¸€ä¸ªä½çº§åˆ«çš„ã€ç›®æ ‡æ¡ä»¶ç­–ç•¥ã€‚è¯¥ç­–ç•¥åœ¨HRLæ¡†æ¶ä¸‹ï¼Œä»¥å¯è¯æ˜çš„æ¬¡ä¼˜ç•Œé™è¿›è¡Œå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šReLAMçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é¢„æµ‹æ¨¡å‹çš„åº”ç”¨ï¼Œè¯¥æ¨¡å‹å……å½“äº†ä¸€ä¸ªè§„åˆ’å™¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆä¸­é—´å­ç›®æ ‡ï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„å­¦ä¹ è¯¾ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„å¥–åŠ±å‡½æ•°è®¾è®¡æ–¹æ³•ç›¸æ¯”ï¼ŒReLAMæ— éœ€äººå·¥è®¾è®¡å¤æ‚çš„å¥–åŠ±å‡½æ•°ï¼Œè€Œæ˜¯é€šè¿‡å­¦ä¹ çš„æ–¹å¼è‡ªåŠ¨ç”Ÿæˆï¼Œæ›´é€‚åº”å¤æ‚çš„è§†è§‰ç¯å¢ƒã€‚æ­¤å¤–ï¼ŒReLAMåˆ©ç”¨å…³é”®ç‚¹ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œé™ä½äº†å¯¹ç²¾ç¡®ä½ç½®ä¿¡æ¯çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šé¢„æµ‹æ¨¡å‹å¯èƒ½é‡‡ç”¨Transformeræˆ–å…¶ä»–åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œè¾“å…¥æ˜¯å½“å‰çŠ¶æ€å’Œç›®æ ‡çŠ¶æ€çš„å…³é”®ç‚¹ï¼Œè¾“å‡ºæ˜¯ä¸­é—´çŠ¶æ€çš„å…³é”®ç‚¹åºåˆ—ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡åŸºäºå½“å‰çŠ¶æ€çš„å…³é”®ç‚¹ä¸é¢„æµ‹çš„å­ç›®æ ‡å…³é”®ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œè·ç¦»è¶Šè¿‘ï¼Œå¥–åŠ±è¶Šé«˜ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é¢„æµ‹æŸå¤±ï¼ˆé¢„æµ‹å…³é”®ç‚¹çš„å‡†ç¡®æ€§ï¼‰å’Œå¼ºåŒ–å­¦ä¹ æŸå¤±ï¼ˆç­–ç•¥çš„ä¼˜åŒ–ï¼‰ã€‚å…·ä½“ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ReLAMåœ¨å¤æ‚ã€é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒReLAMæ˜¾è‘—åŠ é€Ÿäº†å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å–å¾—äº†æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾å…·ä½“çš„å®éªŒæ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ReLAMé€‚ç”¨äºå„ç§è§†è§‰æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–ã€è£…é…ã€å¯¼èˆªç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥é™ä½æœºå™¨äººå­¦ä¹ çš„éš¾åº¦ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ï¼Œå¹¶ä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”çœŸå®ä¸–ç•Œçš„å¤æ‚ç¯å¢ƒã€‚æœªæ¥ï¼ŒReLAMå¯ä»¥åº”ç”¨äºæ™ºèƒ½åˆ¶é€ ã€å®¶åº­æœåŠ¡ã€åŒ»ç–—è¾…åŠ©ç­‰é¢†åŸŸï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´çµæ´»çš„æœºå™¨äººåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reward design remains a critical bottleneck in visual reinforcement learning (RL) for robotic manipulation. In simulated environments, rewards are conventionally designed based on the distance to a target position. However, such precise positional information is often unavailable in real-world visual settings due to sensory and perceptual limitations. In this study, we propose a method that implicitly infers spatial distances through keypoints extracted from images. Building on this, we introduce Reward Learning with Anticipation Model (ReLAM), a novel framework that automatically generates dense, structured rewards from action-free video demonstrations. ReLAM first learns an anticipation model that serves as a planner and proposes intermediate keypoint-based subgoals on the optimal path to the final goal, creating a structured learning curriculum directly aligned with the task's geometric objectives. Based on the anticipated subgoals, a continuous reward signal is provided to train a low-level, goal-conditioned policy under the hierarchical reinforcement learning (HRL) framework with provable sub-optimality bound. Extensive experiments on complex, long-horizon manipulation tasks show that ReLAM significantly accelerates learning and achieves superior performance compared to state-of-the-art methods.

