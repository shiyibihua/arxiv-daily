---
layout: default
title: RLP: Reinforcement as a Pretraining Objective
---

# RLP: Reinforcement as a Pretraining Objective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01265" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01265v1</a>
  <a href="https://arxiv.org/pdf/2510.01265.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01265v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01265v1', 'RLP: Reinforcement as a Pretraining Objective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ali Hatamizadeh, Syeda Nahida Akter, Shrimai Prabhumoye, Jan Kautz, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Yejin Choi

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: RLP introduces a new paradigm for RL-based Pretraining

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRLPï¼šä¸€ç§å°†å¼ºåŒ–å­¦ä¹ ä½œä¸ºé¢„è®­ç»ƒç›®æ ‡çš„æ–¹æ³•ï¼Œæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `é¢„è®­ç»ƒ` `æ€ç»´é“¾` `ä¿¡æ¯å¢ç›Š` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸»è¦ä¾èµ–äºåŸºäºå¤§é‡æ•°æ®çš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹ï¼Œç¼ºä¹æ¢ç´¢æ€§å­¦ä¹ ã€‚
2. RLPå°†å¼ºåŒ–å­¦ä¹ èå…¥é¢„è®­ç»ƒé˜¶æ®µï¼Œé¼“åŠ±æ¨¡å‹é€šè¿‡æ€ç»´é“¾æ¢ç´¢ï¼Œå¹¶æ ¹æ®ä¿¡æ¯å¢ç›Šç»™äºˆå¥–åŠ±ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRLPåœ¨å¤šä¸ªæ•°å­¦å’Œç§‘å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹çš„è®­ç»ƒèŒƒå¼é€šå¸¸å§‹äºåœ¨æµ·é‡æ•°æ®ä¸Šä½¿ç”¨ä¸‹ä¸€ä¸ªtokené¢„æµ‹æŸå¤±è¿›è¡Œé¢„è®­ç»ƒã€‚å¼ºåŒ–å­¦ä¹ è™½ç„¶åœ¨æ‰©å±•æ¨ç†èƒ½åŠ›æ–¹é¢å¾ˆå¼ºå¤§ï¼Œä½†é€šå¸¸åªä½œä¸ºåè®­ç»ƒçš„æœ€åé˜¶æ®µå¼•å…¥ï¼Œå¹¶ä¸”å…ˆäºç›‘ç£å¾®è°ƒã€‚æœ¬æ–‡æå‡ºäº†RLPï¼Œä¸€ç§ä¿¡æ¯é©±åŠ¨çš„å¼ºåŒ–é¢„è®­ç»ƒç›®æ ‡ï¼Œå°†å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒç²¾ç¥â€”â€”æ¢ç´¢â€”â€”å¸¦åˆ°é¢„è®­ç»ƒçš„æœ€åé˜¶æ®µã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ€ç»´é“¾è§†ä¸ºä¸€ç§æ¢ç´¢æ€§è¡Œä¸ºï¼Œå…¶å¥–åŠ±åŸºäºå…¶ä¸ºé¢„æµ‹æœªæ¥tokenæä¾›çš„ä¿¡æ¯å¢ç›Šæ¥è®¡ç®—ã€‚è¿™ç§è®­ç»ƒç›®æ ‡æœ¬è´¨ä¸Šé¼“åŠ±æ¨¡å‹åœ¨é¢„æµ‹æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆä¹‹å‰è¿›è¡Œç‹¬ç«‹æ€è€ƒï¼Œä»è€Œåœ¨é¢„è®­ç»ƒçš„æ—©æœŸé˜¶æ®µåŸ¹å…»ç‹¬ç«‹çš„æ€è€ƒè¡Œä¸ºã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå¥–åŠ±ä¿¡å·è¡¡é‡çš„æ˜¯åœ¨åŒæ—¶ä»¥ä¸Šä¸‹æ–‡å’Œä¸€ä¸ªé‡‡æ ·çš„æ¨ç†é“¾ä¸ºæ¡ä»¶æ—¶ï¼Œä¸‹ä¸€ä¸ªtokençš„å¯¹æ•°ä¼¼ç„¶çš„å¢åŠ ï¼Œä¸ä»…ä»¥ä¸Šä¸‹æ–‡ä¸ºæ¡ä»¶ç›¸æ¯”ã€‚è¿™ç§æ–¹æ³•äº§ç”Ÿäº†ä¸€ç§æ— éœ€éªŒè¯å™¨çš„å¯†é›†å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå¯ä»¥åœ¨é¢„è®­ç»ƒæœŸé—´å¯¹æ•´ä¸ªæ–‡æ¡£æµè¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼ŒRLPå°†æ¨ç†çš„å¼ºåŒ–å­¦ä¹ é‡æ–°å®šä¹‰ä¸ºæ™®é€šæ–‡æœ¬ä¸Šçš„é¢„è®­ç»ƒç›®æ ‡ï¼Œå¼¥åˆäº†ä¸‹ä¸€ä¸ªtokené¢„æµ‹å’Œæœ‰ç”¨çš„æ€ç»´é“¾æ¨ç†çš„å‡ºç°ä¹‹é—´çš„å·®è·ã€‚åœ¨Qwen3-1.7B-Baseä¸Šä½¿ç”¨RLPè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…«ä¸ªåŸºå‡†æ•°å­¦å’Œç§‘å­¦å¥—ä»¶çš„æ€»ä½“å¹³å‡æ°´å¹³æé«˜äº†19%ã€‚é€šè¿‡ç›¸åŒçš„åè®­ç»ƒï¼Œæ”¶ç›Šä¼šç´¯ç§¯ï¼Œåœ¨è¯¸å¦‚AIME25å’ŒMMLU-Proç­‰æ¨ç†ç¹é‡çš„ä»»åŠ¡ä¸Šè·å¾—äº†æœ€å¤§çš„æ”¹è¿›ã€‚å°†RLPåº”ç”¨äºæ··åˆNemotron-Nano-12B-v2å°†æ€»ä½“å¹³å‡æ°´å¹³ä»42.81%æé«˜åˆ°61.32%ï¼Œå¹¶å°†ç§‘å­¦æ¨ç†çš„å¹³å‡æ°´å¹³æé«˜äº†23%ï¼Œè¯æ˜äº†è·¨æ¶æ„å’Œæ¨¡å‹å¤§å°çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒèŒƒå¼ä¸»è¦ä¾èµ–äºä¸‹ä¸€ä¸ªtokené¢„æµ‹ï¼Œè¿™ç§æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†ç¼ºä¹å¯¹æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„æ˜¾å¼å¼•å¯¼ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨é¢„è®­ç»ƒåæ‰å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼Œä½†æ­¤æ—¶æ¨¡å‹å·²ç»å½¢æˆäº†å›ºå®šçš„æ¨¡å¼ï¼Œéš¾ä»¥è¿›è¡Œæ ¹æœ¬æ€§çš„æ”¹å˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRLPçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼ºåŒ–å­¦ä¹ èå…¥åˆ°é¢„è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹è¿›è¡Œæ¢ç´¢æ€§æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒRLPå°†æ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰è§†ä¸ºä¸€ç§æ¢ç´¢æ€§è¡Œä¸ºï¼Œå¹¶æ ¹æ®è¯¥æ€ç»´é“¾å¯¹é¢„æµ‹æœªæ¥tokençš„ä¿¡æ¯å¢ç›Šæ¥è®¡ç®—å¥–åŠ±ã€‚è¿™ç§æ–¹æ³•é¼“åŠ±æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªtokenä¹‹å‰è¿›è¡Œç‹¬ç«‹æ€è€ƒï¼Œä»è€Œåœ¨é¢„è®­ç»ƒçš„æ—©æœŸé˜¶æ®µåŸ¹å…»ç‹¬ç«‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRLPçš„æ•´ä½“æ¡†æ¶æ˜¯åœ¨æ ‡å‡†çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæµç¨‹ä¸­å¼•å…¥ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±æœºåˆ¶ã€‚æ¨¡å‹é¦–å…ˆæ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆä¸€ä¸ªæ€ç»´é“¾ï¼Œç„¶åæ ¹æ®ä¸Šä¸‹æ–‡å’Œæ€ç»´é“¾é¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚å¥–åŠ±ä¿¡å·è¡¡é‡çš„æ˜¯åœ¨åŒæ—¶ä»¥ä¸Šä¸‹æ–‡å’Œä¸€ä¸ªé‡‡æ ·çš„æ¨ç†é“¾ä¸ºæ¡ä»¶æ—¶ï¼Œä¸‹ä¸€ä¸ªtokençš„å¯¹æ•°ä¼¼ç„¶çš„å¢åŠ ï¼Œä¸ä»…ä»¥ä¸Šä¸‹æ–‡ä¸ºæ¡ä»¶ç›¸æ¯”ã€‚è¿™ä¸ªå¥–åŠ±ä¿¡å·è¢«ç”¨æ¥æ›´æ–°æ¨¡å‹çš„å‚æ•°ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´æœ‰åŠ©äºé¢„æµ‹æœªæ¥tokençš„æ€ç»´é“¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šRLPæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ ä½œä¸ºé¢„è®­ç»ƒç›®æ ‡ï¼Œè€Œä¸æ˜¯ä»…ä»…ä½œä¸ºåè®­ç»ƒçš„æ‰‹æ®µã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨é¢„è®­ç»ƒé˜¶æ®µå°±å­¦ä¹ åˆ°å¦‚ä½•è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ï¼Œä»è€Œæé«˜äº†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒRLPä½¿ç”¨ä¿¡æ¯å¢ç›Šä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œé¿å…äº†å¯¹éªŒè¯å™¨çš„ä¾èµ–ï¼Œä½¿å¾—è®­ç»ƒæ›´åŠ é«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šRLPçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å°†æ€ç»´é“¾è§†ä¸ºæ¢ç´¢æ€§è¡Œä¸ºï¼›2) ä½¿ç”¨ä¿¡æ¯å¢ç›Šä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå…·ä½“æ¥è¯´ï¼Œå¥–åŠ±å‡½æ•°è®¡ç®—çš„æ˜¯åœ¨ç»™å®šä¸Šä¸‹æ–‡å’Œæ€ç»´é“¾çš„æƒ…å†µä¸‹ï¼Œä¸‹ä¸€ä¸ªtokençš„å¯¹æ•°ä¼¼ç„¶ä¸ä»…ç»™å®šä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ï¼Œä¸‹ä¸€ä¸ªtokençš„å¯¹æ•°ä¼¼ç„¶ä¹‹é—´çš„å·®å¼‚ï¼›3) å°†RLPåº”ç”¨äºæ ‡å‡†çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæµç¨‹ä¸­ï¼Œæ— éœ€å¯¹æ¨¡å‹æ¶æ„è¿›è¡Œä¿®æ”¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RLPåœ¨Qwen3-1.7B-Baseä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œåœ¨å…«ä¸ªåŸºå‡†æ•°å­¦å’Œç§‘å­¦å¥—ä»¶çš„æ€»ä½“å¹³å‡æ°´å¹³æé«˜äº†19%ã€‚åœ¨Nemotron-Nano-12B-v2ä¸Šï¼ŒRLPå°†æ€»ä½“å¹³å‡æ°´å¹³ä»42.81%æé«˜åˆ°61.32%ï¼Œå¹¶å°†ç§‘å­¦æ¨ç†çš„å¹³å‡æ°´å¹³æé«˜äº†23%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRLPå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RLPå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ç§‘å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡å°†å¼ºåŒ–å­¦ä¹ èå…¥é¢„è®­ç»ƒé˜¶æ®µï¼ŒRLPå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä»è€Œæé«˜äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼ŒRLPè¿˜å¯ä»¥ä¸å…¶ä»–æŠ€æœ¯ç›¸ç»“åˆï¼Œä¾‹å¦‚çŸ¥è¯†å›¾è°±ã€ç¬¦å·æ¨ç†ç­‰ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? In this paper, we present RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, the gains compound, with the largest improvements on reasoning-heavy tasks such as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2 increases the overall average from 42.81% to 61.32% and raises the average on scientific reasoning by 23%, demonstrating scalability across architectures and model sizes.

