---
layout: default
title: Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective
---

# Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22921" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22921v1</a>
  <a href="https://arxiv.org/pdf/2509.22921.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22921v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22921v1', 'Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Matthieu Zimmer, Xiaotong Ji, Tu Nguyen, Haitham Bou Ammar

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºçº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„å¤§è¯­è¨€æ¨¡å‹è’¸é¦æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹è’¸é¦` `çº¦æŸå¼ºåŒ–å­¦ä¹ ` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `å¥–åŠ±å‡½æ•°è®¾è®¡` `æ•°å­¦æ¨ç†` `æ¨¡å‹å‹ç¼©` `çŸ¥è¯†è¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè’¸é¦æ–¹æ³•åœ¨èå…¥ä»»åŠ¡ç‰¹å®šå¥–åŠ±æ—¶ï¼Œé€šå¸¸é‡‡ç”¨å¯å‘å¼å¥–åŠ±åŠ æƒï¼Œç¼ºä¹ç†è®ºæ”¯æ’‘ã€‚
2. è®ºæ–‡å°†LLMè’¸é¦å»ºæ¨¡ä¸ºçº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä»»åŠ¡å¥–åŠ±çš„åŒæ—¶ï¼Œé™åˆ¶ä¸æ•™å¸ˆæ¨¡å‹çš„åå·®ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼Œèƒ½æ›´å¥½åœ°æ»¡è¶³çº¦æŸæ¡ä»¶ï¼Œæå‡æ¨ç†æ€§èƒ½ï¼Œä¸”è®¡ç®—æ•ˆç‡é«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è’¸é¦æ–¹æ³•ï¼Œå°†å…¶å»ºæ¨¡ä¸ºä¸€ä¸ªçº¦æŸå¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚ç°æœ‰å·¥ä½œè™½ç„¶å¼€å§‹æ¢ç´¢å°†ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±èå…¥è’¸é¦è¿‡ç¨‹ï¼Œä½†é€šå¸¸ä¾èµ–äºä¸´æ—¶çš„å¥–åŠ±åŠ æƒã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªæœ‰åŸåˆ™çš„ä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æœ€å¤§åŒ–ç‰¹å®šä»»åŠ¡å¥–åŠ±çš„åŒæ—¶ï¼Œçº¦æŸä¸æ•™å¸ˆæ¨¡å‹çš„å·®å¼‚ä¿æŒåœ¨æŒ‡å®šé˜ˆå€¼ä»¥ä¸‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†çº¦æŸçŠ¶æ€å¢å¼ºå¼ºåŒ–å­¦ä¹ åº”ç”¨äºè’¸é¦è®¾ç½®ï¼Œå¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨éƒ¨ç½²æœŸé—´æ— éœ€çŠ¶æ€å¢å¼ºæˆ–è®¿é—®æ•™å¸ˆæ¨¡å‹ï¼Œä¹Ÿæ— éœ€å¯¹å¶æ‹‰æ ¼æœ—æ—¥æ–¹æ³•çš„è®¡ç®—å¼€é”€ï¼Œä»è€Œä¿æŒäº†çº¦æŸæ»¡è¶³çš„ç†è®ºä¿è¯ã€‚é€šè¿‡åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ç›¸æ¯”äºè½¯æ‹‰æ ¼æœ—æ—¥æ¾å¼›åŸºçº¿ï¼Œå®ç°äº†æ›´å¥½çš„çº¦æŸæ»¡è¶³ç‡å’Œæ›´å¥½çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸­çš„å¥–åŠ±æ„ŸçŸ¥è’¸é¦æä¾›äº†ä¸€ä¸ªç†è®ºåŸºç¡€æ‰å®ä¸”å®é™…é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹è’¸é¦æ–¹æ³•åœ¨å¼•å…¥ä»»åŠ¡ç‰¹å®šå¥–åŠ±æ—¶ï¼Œé€šå¸¸é‡‡ç”¨ä¸´æ—¶çš„ã€å¯å‘å¼çš„å¥–åŠ±åŠ æƒæ–¹å¼ã€‚è¿™ç§æ–¹å¼ç¼ºä¹ç†è®ºåŸºç¡€ï¼Œéš¾ä»¥ä¿è¯åœ¨æå‡ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ï¼Œå¦‚ä½•å¹³è¡¡ä»»åŠ¡æ€§èƒ½å’Œæ¨¡å‹ç›¸ä¼¼åº¦æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤§è¯­è¨€æ¨¡å‹è’¸é¦é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªçº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆConstrained Markov Decision Process, CMDPï¼‰ã€‚é€šè¿‡è¿™ç§å»ºæ¨¡æ–¹å¼ï¼Œå¯ä»¥å°†ä»»åŠ¡ç‰¹å®šå¥–åŠ±ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼ŒåŒæ—¶å°†å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºå·®å¼‚ä½œä¸ºçº¦æŸæ¡ä»¶ã€‚ç›®æ ‡æ˜¯åœ¨æ»¡è¶³çº¦æŸçš„å‰æä¸‹ï¼Œæœ€å¤§åŒ–ä»»åŠ¡å¥–åŠ±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŸºäºçº¦æŸå¼ºåŒ–å­¦ä¹ ã€‚å…·ä½“è€Œè¨€ï¼Œé¦–å…ˆå°†è’¸é¦è¿‡ç¨‹å®šä¹‰ä¸ºä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ŒçŠ¶æ€æ˜¯æ¨¡å‹çš„è¾“å‡ºï¼ŒåŠ¨ä½œæ˜¯æ¨¡å‹çš„å‚æ•°æ›´æ–°ã€‚ç„¶åï¼Œå®šä¹‰ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°ç»“åˆäº†ä»»åŠ¡ç‰¹å®šå¥–åŠ±å’Œå¯¹æ•™å¸ˆæ¨¡å‹è¡Œä¸ºçš„æ¨¡ä»¿ç¨‹åº¦ã€‚å…³é”®åœ¨äºå¼•å…¥äº†ä¸€ä¸ªçº¦æŸæ¡ä»¶ï¼Œé™åˆ¶å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºå·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªCMDPé—®é¢˜ï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§æ”¹è¿›çš„çº¦æŸçŠ¶æ€å¢å¼ºå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†çº¦æŸå¼ºåŒ–å­¦ä¹ åº”ç”¨äºLLMè’¸é¦ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°èƒ½å¤Ÿåœ¨ä¸è¿›è¡ŒçŠ¶æ€å¢å¼ºæˆ–è®¿é—®æ•™å¸ˆæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä¿è¯çº¦æŸçš„æ»¡è¶³ã€‚è¿™ä¸ä¼ ç»Ÿçš„æ‹‰æ ¼æœ—æ—¥æ–¹æ³•ä¸åŒï¼Œé¿å…äº†é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œæé«˜äº†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªç‰¹æ®Šçš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒ…å«ä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†æ˜¯ä»»åŠ¡ç‰¹å®šå¥–åŠ±ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯è¡¡é‡å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹è¡Œä¸ºå·®å¼‚çš„æƒ©ç½šé¡¹ã€‚æƒ©ç½šé¡¹çš„è®¾è®¡ä¿è¯äº†çº¦æŸçš„æ»¡è¶³ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹çº¦æŸå¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œäº†ä¿®æ”¹ï¼Œä½¿å…¶æ›´é€‚åˆäºè’¸é¦ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºç­–ç•¥æ¢¯åº¦çš„æ–¹æ³•æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œå¹¶ä½¿ç”¨ä¸€ç§ç‰¹æ®Šçš„æ›´æ–°è§„åˆ™æ¥ä¿è¯çº¦æŸçš„æ»¡è¶³ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸è½¯æ‹‰æ ¼æœ—æ—¥æ¾å¼›åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³çº¦æŸæ¡ä»¶ï¼Œå¹¶è·å¾—æ›´é«˜çš„æ¨ç†å‡†ç¡®ç‡ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åœ¨çº¦æŸæ»¡è¶³ç‡ä¸Šæå‡äº†X%ï¼Œåœ¨æ¨ç†å‡†ç¡®ç‡ä¸Šæå‡äº†Y%ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼Œè®ºæ–‡ä¸­åº”æœ‰ä½“ç°ï¼‰ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä¿æŒäº†ä¸åŸºçº¿æ–¹æ³•ç›¸å½“çš„è®¡ç®—æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºèµ„æºå—é™åœºæ™¯ä¸‹çš„å¤§è¯­è¨€æ¨¡å‹è’¸é¦ï¼Œä¾‹å¦‚åœ¨ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²é«˜æ€§èƒ½çš„LLMã€‚é€šè¿‡çº¦æŸå­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºå·®å¼‚ï¼Œå¯ä»¥ä¿è¯å­¦ç”Ÿæ¨¡å‹åœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œä¸ä¼šåç¦»æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†èŒƒå›´ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦å¹³è¡¡å¤šä¸ªç›®æ ‡çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce a novel approach to large language model (LLM) distillation by formulating it as a constrained reinforcement learning problem. While recent work has begun exploring the integration of task-specific rewards into distillation processes, existing methods typically rely on ad-hoc reward weighting. We propose a principled optimization framework that maximizes task-specific rewards while constraining the divergence from the teacher model to remain below a specified threshold. Our approach adapts constrained state augmented reinforcement learning to the distillation setting, introducing a modified reward function that maintains theoretical guarantees of constraint satisfaction without requiring state augmentation or teacher model access during deployment and without the computational overhead of the dual Lagrangian methods. Through extensive experiments on mathematical reasoning tasks, we demonstrate that our method achieves better constraint satisfaction rates and better reasoning compared to the soft Lagrangian relaxation baselines while maintaining competitive task performance. Our framework provides a theoretically grounded and practically efficient solution for reward-aware distillation in resource-constrained settings.

