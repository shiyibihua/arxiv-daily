---
layout: default
title: Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning
---

# Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09838" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.09838v1</a>
  <a href="https://arxiv.org/pdf/2509.09838.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09838v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09838v1', 'Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Reza Asad, Reza Babanezhad, Sharan Vaswani

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-11

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Ëß£ËÄ¶Actor-CriticÁÜµÊ≠£ÂàôÂåñÔºåÊèêÂçáÁ¶ªÊï£Âä®‰ΩúÁ¶ªÁ≠ñÁï•Âº∫ÂåñÂ≠¶‰π†ÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Á¶ªÁ≠ñÁï•Âº∫ÂåñÂ≠¶‰π†` `Actor-CriticÊñπÊ≥ï` `ÁÜµÊ≠£ÂàôÂåñ` `Á¶ªÊï£Âä®‰ΩúÁ©∫Èó¥` `AtariÊ∏∏Êàè`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁ¶ªÊï£Âä®‰ΩúÁ¶ªÁ≠ñÁï•Âº∫ÂåñÂ≠¶‰π†‰∏≠ÔºåÂü∫‰∫éÁ≠ñÁï•ÁöÑÊñπÊ≥ïÔºàÂ¶ÇDSACÔºâÊÄßËÉΩ‰∏ç‰Ω≥Ôºå‰∏ªË¶ÅÂéüÂõ†ÊòØActorÂíåCriticÁöÑÁÜµËÄ¶Âêà„ÄÇ
2. ÈÄöËøáËß£ËÄ¶ActorÂíåCriticÁöÑÁÜµÔºåÂπ∂ÁªìÂêàmÊ≠•BellmanÁÆóÂ≠êÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Êõ¥ÁÅµÊ¥ªÁöÑÁ¶ªÁ≠ñÁï•Actor-CriticÊ°ÜÊû∂„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÊñ∞Ê°ÜÊû∂Âú®AtariÊ∏∏Êàè‰∏äÂèØ‰ª•ËææÂà∞‰∏éDQNÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÊó†ÈúÄÈ¢ùÂ§ñÁöÑÁÜµÊ≠£ÂàôÂåñÊàñÊé¢Á¥¢Á≠ñÁï•„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®Á¶ªÊï£Âä®‰ΩúÁéØÂ¢ÉÔºàÂ¶ÇAtariÔºâ‰∏≠ÔºåÂü∫‰∫é‰ª∑ÂÄºÁöÑÊñπÊ≥ïÔºàÂ¶ÇDQNÔºâÊòØÁ¶ªÁ≠ñÁï•Âº∫ÂåñÂ≠¶‰π†ÁöÑÂ∏∏Áî®ÊñπÊ≥ï„ÄÇÂ∏∏ËßÅÁöÑÂü∫‰∫éÁ≠ñÁï•ÁöÑÊñπÊ≥ïË¶Å‰πàÊòØÂêåÁ≠ñÁï•ÁöÑÔºåÊó†Ê≥ïÊúâÊïàÂú∞‰ªéÁ¶ªÁ≠ñÁï•Êï∞ÊçÆ‰∏≠Â≠¶‰π†ÔºàÂ¶ÇPPOÔºâÔºåË¶Å‰πàÂú®Á¶ªÊï£Âä®‰ΩúÁéØÂ¢É‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºàÂ¶ÇSACÔºâ„ÄÇÂõ†Ê≠§ÔºåÊú¨Êñá‰ªéÁ¶ªÊï£SACÔºàDSACÔºâÂá∫ÂèëÔºåÈáçÊñ∞ÂÆ°ËßÜ‰∫ÜÂú®ËøôÁßçËÆæÁΩÆ‰∏ãActor-CriticÊñπÊ≥ïÁöÑËÆæËÆ°„ÄÇÈ¶ñÂÖàÔºåÁ°ÆÂÆöActorÂíåCriticÁÜµ‰πãÈó¥ÁöÑËÄ¶ÂêàÊòØDSACÊÄßËÉΩ‰∏ç‰Ω≥ÁöÑ‰∏ªË¶ÅÂéüÂõ†„ÄÇÈÄöËøáËß£ËÄ¶Ëøô‰∫õÁªÑ‰ª∂ÔºåDSACÂèØ‰ª•ËææÂà∞‰∏éDQNÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇÂèóÊ≠§ÂêØÂèëÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁÅµÊ¥ªÁöÑÁ¶ªÁ≠ñÁï•Actor-CriticÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Â∞ÜDSAC‰Ωú‰∏∫Áâπ‰æãÂåÖÂê´Âú®ÂÜÖ„ÄÇËØ•Ê°ÜÊû∂ÂÖÅËÆ∏‰ΩøÁî®mÊ≠•BellmanÁÆóÂ≠êËøõË°åCriticÊõ¥Êñ∞ÔºåÂπ∂ËÉΩÂ§üÂ∞ÜÊ†áÂáÜÁ≠ñÁï•‰ºòÂåñÊñπÊ≥ï‰∏éÁÜµÊ≠£ÂàôÂåñÁõ∏ÁªìÂêàÔºå‰ªéËÄåÂÆû‰æãÂåñÊúÄÁªàÁöÑActorÁõÆÊ†á„ÄÇÁêÜËÆ∫‰∏äÔºåËØÅÊòé‰∫ÜÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂèØ‰ª•‰øùËØÅÊî∂ÊïõÂà∞Ë°®Ê†ºËÆæÁΩÆ‰∏≠ÁöÑÊúÄ‰ºòÊ≠£ÂàôÂåñ‰ª∑ÂÄºÂáΩÊï∞„ÄÇÂÆûÈ™å‰∏äÔºåËØÅÊòé‰∫ÜËøô‰∫õÊñπÊ≥ïÂèØ‰ª•Êé•ËøëDQNÂú®Ê†áÂáÜAtariÊ∏∏Êàè‰∏äÁöÑÊÄßËÉΩÔºåÂç≥‰ΩøÊ≤°ÊúâÁÜµÊ≠£ÂàôÂåñÊàñÊòæÂºèÊé¢Á¥¢‰πüËÉΩÂÅöÂà∞„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂú®Á¶ªÊï£Âä®‰ΩúÁ©∫Èó¥ÁöÑÁ¶ªÁ≠ñÁï•Âº∫ÂåñÂ≠¶‰π†‰ªªÂä°‰∏≠Ôºå‰º†ÁªüÁöÑÂü∫‰∫éÁ≠ñÁï•ÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇÁ¶ªÊï£SAC (DSAC)ÔºåÈÄöÂ∏∏Ë°®Áé∞‰∏çÂ¶ÇÂü∫‰∫é‰ª∑ÂÄºÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇDQN„ÄÇDSACÁöÑÊÄßËÉΩÁì∂È¢àÂú®‰∫éActorÂíåCritic‰πãÈó¥ÁöÑÁÜµËÄ¶ÂêàÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂Â≠¶‰π†ÊïàÁéáÂíåÊúÄÁªàÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËß£ËÄ¶ActorÂíåCriticÁöÑÁÜµÊ≠£ÂàôÂåñÈ°πÔºåÂÖÅËÆ∏ÂÆÉ‰ª¨Áã¨Á´ãÂú∞ËøõË°å‰ºòÂåñ„ÄÇÈÄöËøáËøôÁßçËß£ËÄ¶ÔºåÂèØ‰ª•Êõ¥ÁÅµÊ¥ªÂú∞ÊéßÂà∂Êé¢Á¥¢ÂíåÂà©Áî®‰πãÈó¥ÁöÑÂπ≥Ë°°Ôºå‰ªéËÄåÊèêÈ´òÂ≠¶‰π†ÊïàÁéá„ÄÇÊ≠§Â§ñÔºåËøòÂºïÂÖ•‰∫ÜmÊ≠•BellmanÁÆóÂ≠êÊù•Êõ¥Êñ∞CriticÔºå‰ª•Âä†ÈÄü‰ª∑ÂÄºÂáΩÊï∞ÁöÑÂ≠¶‰π†„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑÁ¶ªÁ≠ñÁï•Actor-CriticÊû∂ÊûÑÔºåÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) ActorÁΩëÁªúÔºåÁî®‰∫éÁîüÊàêÁ≠ñÁï•Ôºõ2) CriticÁΩëÁªúÔºåÁî®‰∫éËØÑ‰º∞Á≠ñÁï•ÁöÑ‰ª∑ÂÄºÔºõ3) ÁªèÈ™åÂõûÊîæÁºìÂÜ≤Âå∫ÔºåÁî®‰∫éÂ≠òÂÇ®ÂíåÈááÊ†∑ÁªèÈ™åÊï∞ÊçÆÔºõ4) ÁõÆÊ†áÁΩëÁªúÔºåÁî®‰∫éÁ®≥ÂÆöÂ≠¶‰π†ËøáÁ®ã„ÄÇËØ•Ê°ÜÊû∂ÂÖÅËÆ∏‰ΩøÁî®‰∏çÂêåÁöÑÁ≠ñÁï•‰ºòÂåñÊñπÊ≥ïÔºàÂ¶ÇÁ≠ñÁï•Ê¢ØÂ∫¶„ÄÅTRPOÁ≠âÔºâÊù•Êõ¥Êñ∞ActorÔºåÂπ∂‰ΩøÁî®mÊ≠•BellmanÁÆóÂ≠êÊù•Êõ¥Êñ∞Critic„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éËß£ËÄ¶ActorÂíåCriticÁöÑÁÜµÊ≠£ÂàôÂåñÈ°πÔºåÂπ∂Â∞ÜÂÖ∂Á∫≥ÂÖ•‰∏Ä‰∏™Áªü‰∏ÄÁöÑÁ¶ªÁ≠ñÁï•Actor-CriticÊ°ÜÊû∂‰∏≠„ÄÇËøôÁßçËß£ËÄ¶ÂÖÅËÆ∏Êõ¥ÁÅµÊ¥ªÂú∞ÊéßÂà∂Êé¢Á¥¢ÂíåÂà©Áî®Ôºå‰ªéËÄåÊèêÈ´òÂ≠¶‰π†ÊïàÁéá„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®mÊ≠•BellmanÁÆóÂ≠êÊù•Êõ¥Êñ∞CriticÔºåÂèØ‰ª•Âä†ÈÄü‰ª∑ÂÄºÂáΩÊï∞ÁöÑÂ≠¶‰π†„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆÁöÑËÆæËÆ°ÂåÖÊã¨Ôºö1) ActorÂíåCriticÁΩëÁªúÁöÑÁªìÊûÑÈÄâÊã©Ôºà‰æãÂ¶ÇÔºåÂ§öÂ±ÇÊÑüÁü•Êú∫„ÄÅÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÁ≠âÔºâÔºõ2) ÁÜµÊ≠£ÂàôÂåñÁ≥ªÊï∞ÁöÑËÆæÁΩÆÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥Ôºõ3) mÊ≠•BellmanÁÆóÂ≠êÁöÑÊ≠•Êï∞mÁöÑÈÄâÊã©ÔºåÈúÄË¶ÅÂú®ÂÅèÂ∑ÆÂíåÊñπÂ∑Æ‰πãÈó¥ËøõË°åÊùÉË°°Ôºõ4) Á≠ñÁï•‰ºòÂåñÊñπÊ≥ïÁöÑÈÄâÊã©Ôºà‰æãÂ¶ÇÔºåÁ≠ñÁï•Ê¢ØÂ∫¶„ÄÅTRPOÁ≠âÔºâÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åÈÄâÊã©„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÄöËøáËß£ËÄ¶ActorÂíåCriticÁöÑÁÜµÊ≠£ÂàôÂåñÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®AtariÊ∏∏Êàè‰∏äÁöÑÊÄßËÉΩÂèØ‰ª•Êé•ËøëDQNÔºåÁîöËá≥Âú®Ê≤°ÊúâÁÜµÊ≠£ÂàôÂåñÊàñÊòæÂºèÊé¢Á¥¢ÁöÑÊÉÖÂÜµ‰∏ã‰πüËÉΩÂÆûÁé∞„ÄÇËøôË°®ÊòéËØ•ÊñπÊ≥ïÂÖ∑ÊúâÂæàÂº∫ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂú®Êüê‰∫õÊ∏∏Êàè‰∏äÁöÑÊÄßËÉΩÁîöËá≥Ë∂ÖËøá‰∫ÜDQN„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÁ¶ªÊï£Âä®‰ΩúÁ©∫Èó¥ÁöÑÂº∫ÂåñÂ≠¶‰π†‰ªªÂä°Ôºå‰æãÂ¶ÇÊ∏∏ÊàèAI„ÄÅÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÅÊé®ËçêÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáËß£ËÄ¶ActorÂíåCriticÁöÑÁÜµÊ≠£ÂàôÂåñÔºåÂèØ‰ª•ÊèêÈ´òÂ≠¶‰π†ÊïàÁéáÂíåÊúÄÁªàÊÄßËÉΩÔºå‰ªéËÄåÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Ëé∑ÂæóÊõ¥Â•ΩÁöÑÊïàÊûú„ÄÇÊ≠§Â§ñÔºåËØ•Ê°ÜÊû∂ÁöÑÈÄöÁî®ÊÄß‰ΩøÂÖ∂Êòì‰∫éÊâ©Â±ïÂà∞ÂÖ∂‰ªñÁ¶ªÁ≠ñÁï•Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Value-based approaches such as DQN are the default methods for off-policy reinforcement learning with discrete-action environments such as Atari. Common policy-based methods are either on-policy and do not effectively learn from off-policy data (e.g. PPO), or have poor empirical performance in the discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC (DSAC), we revisit the design of actor-critic methods in this setting. First, we determine that the coupling between the actor and critic entropy is the primary reason behind the poor performance of DSAC. We demonstrate that by merely decoupling these components, DSAC can have comparable performance as DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic framework that subsumes DSAC as a special case. Our framework allows using an m-step Bellman operator for the critic update, and enables combining standard policy optimization methods with entropy regularization to instantiate the resulting actor objective. Theoretically, we prove that the proposed methods can guarantee convergence to the optimal regularized value function in the tabular setting. Empirically, we demonstrate that these methods can approach the performance of DQN on standard Atari games, and do so even without entropy regularization or explicit exploration.

