---
layout: default
title: ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms
---

# ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09679" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.09679v2</a>
  <a href="https://arxiv.org/pdf/2509.09679.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09679v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09679v2', 'ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Bingxin Xu, Zhen Dong, Oussama Elachqar, Yuzhang Shang

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-11 (Êõ¥Êñ∞: 2025-09-25)

**Â§áÊ≥®**: Replace discrete Hadamard transforms with continuous Butterfly transforms to facilitate the learning of rotation matrices in LLM quantization

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/42Shawn/Butterflyquant-llm)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ButterflyQuant‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÈáèÂåñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈáèÂåñÊäÄÊúØ` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Ê∑±Â∫¶Â≠¶‰π†` `Ëù¥Ëù∂ÂèòÊç¢` `ÂºÇÂ∏∏ÂÄºÊäëÂà∂` `Ê®°Âûã‰ºòÂåñ` `GivensÊóãËΩ¨` `ËµÑÊ∫êÂèóÈôêËÆæÂ§á`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈáèÂåñÊñπÊ≥ïÂú®ÊûÅÁ´Ø2‰ΩçÈáèÂåñÊó∂ÔºåÂõ†ÊøÄÊ¥ªÂÄº‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºÂØºËá¥ÊÄßËÉΩ‰∏•Èáç‰∏ãÈôçÔºåÈôêÂà∂‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ∫îÁî®„ÄÇ
2. ButterflyQuantÈÄöËøáÂèØÂ≠¶‰π†ÁöÑËù¥Ëù∂ÂèòÊç¢Êõø‰ª£Âõ∫ÂÆöÁöÑHadamardÂèòÊç¢ÔºåÈááÁî®ËøûÁª≠ÂèÇÊï∞Âåñ‰ª•ÈÄÇÂ∫î‰∏çÂêåÂ±ÇÁöÑÊùÉÈáçÂàÜÂ∏É„ÄÇ
3. Âú®LLaMA-2-7BÊ®°Âûã‰∏äËøõË°å2‰ΩçÈáèÂåñÊó∂ÔºåButterflyQuantÁöÑÂõ∞ÊÉëÂ∫¶‰∏∫15.4ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïQuIPÁöÑ37.3„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÈúÄË¶ÅÂ∑®Â§ßÁöÑÂÜÖÂ≠òÂç†Áî®ÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂Âú®Ê∂àË¥πÁ°¨‰ª∂‰∏äÁöÑÈÉ®ÁΩ≤„ÄÇÈáèÂåñÈÄöËøáÈôç‰ΩéÊï∞ÂÄºÁ≤æÂ∫¶Êù•ÂáèÂ∞ëÂÜÖÂ≠òÔºå‰ΩÜÊûÅÁ´ØÁöÑ2‰ΩçÈáèÂåñ‰ºöÂõ†ÊøÄÊ¥ªÂÄº‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºËÄåÂØºËá¥ÊÄßËÉΩ‰∏•Èáç‰∏ãÈôç„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éÊóãËΩ¨ÁöÑÊñπÊ≥ïÂ¶ÇQuIPÂíåQuaRot‰ΩøÁî®Âõ∫ÂÆöÁöÑHadamardÂèòÊç¢Êù•Ê∂àÈô§ÂºÇÂ∏∏ÂÄºÔºå‰ΩÜÊó†Ê≥ïÈÄÇÂ∫îÁâπÂÆöÁöÑÊùÉÈáçÂàÜÂ∏É„ÄÇÊú¨ÊñáÊèêÂá∫ButterflyQuantÔºåÈááÁî®ÂèØÂ≠¶‰π†ÁöÑËù¥Ëù∂ÂèòÊç¢Êõø‰ª£HadamardÊóãËΩ¨ÔºåÂà©Áî®ËøûÁª≠ÁöÑGivensÊóãËΩ¨ËßíÂ∫¶ËøõË°åÂèÇÊï∞ÂåñÔºåÁ°Æ‰øùÊ≠£‰∫§ÊÄßÂπ∂ÂÆûÁé∞Âπ≥Êªë‰ºòÂåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåButterflyQuantÂú®LLaMA-2-7BÊ®°ÂûãÁöÑ2‰ΩçÈáèÂåñ‰∏≠ÔºåÂõ∞ÊÉëÂ∫¶‰∏∫15.4ÔºåÁõ∏ËæÉ‰∫éQuIPÁöÑ37.3ÊúâÊòæËëóÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÊûÅÁ´Ø2‰ΩçÈáèÂåñÊó∂Âõ†ÊøÄÊ¥ªÂÄºÂºÇÂ∏∏ÂÄºÂØºËá¥ÁöÑÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇQuIPÂíåQuaRot‰ΩøÁî®Âõ∫ÂÆöÁöÑHadamardÂèòÊç¢ÔºåÊó†Ê≥ïÈÄÇÂ∫î‰∏çÂêåÂ±ÇÁöÑÊùÉÈáçÂàÜÂ∏ÉÔºåÈÄ†ÊàêÊÄßËÉΩÊçüÂ§±„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ButterflyQuantÔºåÈÄöËøáÂèØÂ≠¶‰π†ÁöÑËù¥Ëù∂ÂèòÊç¢Êõø‰ª£HadamardÊóãËΩ¨ÔºåÂà©Áî®ËøûÁª≠ÁöÑGivensÊóãËΩ¨ËßíÂ∫¶ËøõË°åÂèÇÊï∞Âåñ„ÄÇËøôÁßçËÆæËÆ°ÂÖÅËÆ∏Ê®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ëá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥ÂèòÊç¢Ôºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞ÊäëÂà∂ÂºÇÂ∏∏ÂÄº„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöButterflyQuantÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÂèØÂ≠¶‰π†ÁöÑËù¥Ëù∂ÂèòÊç¢Ê®°ÂùóÂíåÈáèÂåñÊ®°Âùó„ÄÇÈ¶ñÂÖàÂØπËæìÂÖ•Êï∞ÊçÆËøõË°åÈ¢ÑÂ§ÑÁêÜÔºåÁÑ∂ÂêéÈÄöËøáËù¥Ëù∂ÂèòÊç¢Ê®°ÂùóËøõË°åÊøÄÊ¥ªÂÄºÁöÑÂèòÊç¢ÔºåÊúÄÂêéËøõË°åÈáèÂåñÂ§ÑÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂèØÂ≠¶‰π†ÁöÑËù¥Ëù∂ÂèòÊç¢ÔºåÊõø‰ª£‰∫Ü‰º†ÁªüÁöÑÂõ∫ÂÆöHadamardÂèòÊç¢„ÄÇËøô‰∏ÄÂàõÊñ∞‰ΩøÂæóÂèòÊç¢ËÉΩÂ§üÊ†πÊçÆ‰∏çÂêåÂ±ÇÁöÑÁâπÂæÅËøõË°åËá™ÈÄÇÂ∫îË∞ÉÊï¥ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈáèÂåñÂêéÁöÑÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåËù¥Ëù∂ÂèòÊç¢ÁöÑÂèÇÊï∞ÈÄöËøáËøûÁª≠ÁöÑGivensÊóãËΩ¨ËßíÂ∫¶ËøõË°å‰ºòÂåñÔºåÁ°Æ‰øù‰∫ÜÊ≠£‰∫§ÊÄß„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂºïÂÖ•‰∫ÜÂùáÂåÄÊÄßÊ≠£ÂàôÂåñÔºå‰ª•‰øÉËøõÂèòÊç¢ÂêéÊøÄÊ¥ªÂÄºÁöÑÂπ≥ÊªëÂàÜÂ∏ÉÔºå‰æø‰∫éÂêéÁª≠ÁöÑÈáèÂåñÂ§ÑÁêÜ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂÆûÈ™å‰∏≠ÔºåButterflyQuantÂú®LLaMA-2-7BÊ®°ÂûãÁöÑ2‰ΩçÈáèÂåñ‰∏≠ÔºåÂõ∞ÊÉëÂ∫¶ËææÂà∞‰∫Ü15.4ÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïQuIPÁöÑ37.3ÔºåË°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇËøô‰∏ÄÁªìÊûúË°®ÊòéÔºåButterflyQuantÂú®Â§ÑÁêÜÂºÇÂ∏∏ÂÄºÊñπÈù¢ÂÖ∑Êúâ‰ºòË∂äÊÄßÔºåËÉΩÂ§üÊúâÊïàÊèêÈ´òÈáèÂåñÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ButterflyQuantÁöÑÁ†îÁ©∂ÊàêÊûúÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÈÉ®ÁΩ≤Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊó∂„ÄÇÈÄöËøáÊúâÊïàÁöÑÈáèÂåñÊñπÊ≥ïÔºåËÉΩÂ§üÂú®‰øùÊåÅÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰ΩéÂÜÖÂ≠òÂç†Áî®Ôºå‰∏∫Êô∫ËÉΩÊâãÊú∫„ÄÅËæπÁºòËÆ°ÁÆóËÆæÂ§áÁ≠âÊèê‰æõÊõ¥Â•ΩÁöÑÊîØÊåÅ„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁöÑÈáèÂåñÂíå‰ºòÂåñ‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} = (\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $Œº= 1/\sqrt{n}$--that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. In this work, we propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$ entries that are non-differentiable and thus prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \log n)$ computational complexity with only $\frac{n \log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP. \href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available.

