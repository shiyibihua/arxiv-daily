---
layout: default
title: Latency and Token-Aware Test-Time Compute
---

# Latency and Token-Aware Test-Time Compute

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09864" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09864v1</a>
  <a href="https://arxiv.org/pdf/2509.09864.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09864v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09864v1', 'Latency and Token-Aware Test-Time Compute')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jenny Y. Huang, Mehul Damani, Yousef El-Kurdi, Ramon Astudillo, Wei Sun

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å»¶è¿Ÿå’ŒTokenæ„ŸçŸ¥çš„æµ‹è¯•æ—¶è®¡ç®—åŠ¨æ€åˆ†é…æ¡†æ¶ï¼Œä¼˜åŒ–LLMæ¨ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†ä¼˜åŒ–` `åŠ¨æ€è®¡ç®—åˆ†é…` `å»¶è¿Ÿä¼˜åŒ–` `Tokenæˆæœ¬` `å¢é‡è§£ç ` `Beam Search`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æµ‹è¯•æ—¶è®¡ç®—åŠ¨æ€åˆ†é…æ–¹æ³•å¿½ç•¥äº†å¢é‡è§£ç æ–¹æ³•ï¼ˆå¦‚beam searchï¼‰ï¼Œä¸”ä¸»è¦å…³æ³¨tokenä½¿ç”¨ï¼Œå¿½ç•¥äº†å»¶è¿Ÿã€‚
2. è¯¥è®ºæ–‡å°†æ¨ç†æ—¶æ‰©å±•å»ºæ¨¡ä¸ºåŠ¨æ€è®¡ç®—åˆ†é…å’Œæ–¹æ³•é€‰æ‹©é—®é¢˜ï¼ŒåŒæ—¶è€ƒè™‘tokenæˆæœ¬å’Œå®é™…å»¶è¿Ÿã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºé™æ€ç­–ç•¥ï¼Œå®ç°äº†æ›´å¥½çš„å‡†ç¡®æ€§-æˆæœ¬æƒè¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€è®¡ç®—åˆ†é…å’Œæ–¹æ³•é€‰æ‹©çš„æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ—¶æ‰©å±•ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æ ¹æ®æ¯ä¸ªæŸ¥è¯¢åŠ¨æ€åœ°å†³å®šåº”ç”¨å“ªç§ç­–ç•¥ä»¥åŠåˆ†é…å¤šå°‘è®¡ç®—èµ„æºã€‚ä¸ç°æœ‰å·¥ä½œä¸»è¦å…³æ³¨å¹¶è¡Œç”Ÿæˆæ–¹æ³•ï¼ˆå¦‚best-of-Nï¼‰ä¸åŒï¼Œæœ¬æ–‡åŒæ—¶è€ƒè™‘äº†å¢é‡è§£ç æ–¹æ³•ï¼ˆå¦‚beam searchï¼‰ï¼Œå¹¶æ˜¾å¼åœ°å°†tokenæˆæœ¬å’Œå®é™…å»¶è¿Ÿçº³å…¥è€ƒé‡ã€‚å»¶è¿Ÿå¯¹äºç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ¨¡å‹é«˜æ•ˆå‘å‡ºå¤šä¸ªæŸ¥è¯¢çš„agenticå·¥ä½œæµç¨‹ä¸­ã€‚åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆä¼˜äºé™æ€ç­–ç•¥ï¼Œåœ¨ä¿æŒéƒ¨ç½²å¯è¡Œæ€§çš„åŒæ—¶ï¼Œå®ç°äº†è‰¯å¥½çš„å‡†ç¡®æ€§-æˆæœ¬æƒè¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ—¶æ‰©å±•æ–¹æ³•ï¼Œåœ¨åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºæ—¶ï¼Œä¸»è¦å…³æ³¨å¹¶è¡Œç”Ÿæˆæ–¹æ³•ï¼Œä¾‹å¦‚best-of-Nã€‚è¿™ç§æ–¹æ³•å¿½ç•¥äº†å¢é‡è§£ç æ–¹æ³•ï¼Œä¾‹å¦‚beam searchã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨tokençš„ä½¿ç”¨é‡ï¼Œè€Œå¿½ç•¥äº†å®é™…çš„æ¨ç†å»¶è¿Ÿï¼Œè¿™å¯¹äºç”¨æˆ·ä½“éªŒï¼Œç‰¹åˆ«æ˜¯éœ€è¦æ¨¡å‹è¿›è¡Œå¤šæ¬¡æŸ¥è¯¢çš„agenticå·¥ä½œæµç¨‹æ¥è¯´ï¼Œè‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤ŸåŒæ—¶è€ƒè™‘tokenæˆæœ¬å’Œå»¶è¿Ÿï¼Œå¹¶èƒ½å¤ŸåŠ¨æ€é€‰æ‹©æ¨ç†ç­–ç•¥çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¨ç†æ—¶æ‰©å±•é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªåŠ¨æ€è®¡ç®—åˆ†é…å’Œæ–¹æ³•é€‰æ‹©çš„é—®é¢˜ã€‚ç³»ç»Ÿéœ€è¦æ ¹æ®æ¯ä¸ªæŸ¥è¯¢çš„ç‰¹ç‚¹ï¼ŒåŠ¨æ€åœ°å†³å®šé‡‡ç”¨å“ªç§æ¨ç†ç­–ç•¥ï¼ˆä¾‹å¦‚best-of-Næˆ–beam searchï¼‰ï¼Œä»¥åŠåˆ†é…å¤šå°‘è®¡ç®—èµ„æºã€‚é€šè¿‡è¿™ç§åŠ¨æ€çš„åˆ†é…ï¼Œå¯ä»¥åœ¨ä¿è¯å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œæœ€å°åŒ–tokenæˆæœ¬å’Œæ¨ç†å»¶è¿Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æŸ¥è¯¢åˆ†ææ¨¡å—ï¼šåˆ†æè¾“å…¥æŸ¥è¯¢çš„ç‰¹ç‚¹ï¼Œä¾‹å¦‚å¤æ‚åº¦å’Œæ‰€éœ€çš„æ¨ç†æ·±åº¦ã€‚2) ç­–ç•¥é€‰æ‹©æ¨¡å—ï¼šæ ¹æ®æŸ¥è¯¢åˆ†æçš„ç»“æœï¼Œé€‰æ‹©åˆé€‚çš„æ¨ç†ç­–ç•¥ï¼Œä¾‹å¦‚best-of-Næˆ–beam searchã€‚3) èµ„æºåˆ†é…æ¨¡å—ï¼šæ ¹æ®æ‰€é€‰çš„æ¨ç†ç­–ç•¥ï¼ŒåŠ¨æ€åœ°åˆ†é…è®¡ç®—èµ„æºï¼Œä¾‹å¦‚åˆ†é…å¤šå°‘ä¸ªå€™é€‰å“åº”æˆ–beam searchçš„å®½åº¦ã€‚4) æ¨ç†æ‰§è¡Œæ¨¡å—ï¼šä½¿ç”¨æ‰€é€‰çš„æ¨ç†ç­–ç•¥å’Œåˆ†é…çš„è®¡ç®—èµ„æºï¼Œæ‰§è¡Œæ¨ç†è¿‡ç¨‹ã€‚5) ç»“æœé€‰æ‹©æ¨¡å—ï¼šä»ç”Ÿæˆçš„å€™é€‰å“åº”ä¸­ï¼Œé€‰æ‹©æœ€ä½³çš„å“åº”ä½œä¸ºæœ€ç»ˆç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) åŒæ—¶è€ƒè™‘äº†tokenæˆæœ¬å’Œå»¶è¿Ÿï¼Œå¹¶å°†å…¶çº³å…¥åŠ¨æ€è®¡ç®—åˆ†é…çš„ä¼˜åŒ–ç›®æ ‡ä¸­ã€‚2) æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œå¯ä»¥æ”¯æŒå¤šç§æ¨ç†ç­–ç•¥ï¼ŒåŒ…æ‹¬å¹¶è¡Œç”Ÿæˆæ–¹æ³•å’Œå¢é‡è§£ç æ–¹æ³•ã€‚3) å®ç°äº†åŸºäºæŸ¥è¯¢ç‰¹ç‚¹çš„åŠ¨æ€ç­–ç•¥é€‰æ‹©å’Œèµ„æºåˆ†é…ï¼Œä»è€Œå®ç°äº†æ›´å¥½çš„å‡†ç¡®æ€§-æˆæœ¬æƒè¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†å¯ä»¥æ¨æµ‹ï¼Œç­–ç•¥é€‰æ‹©æ¨¡å—å¯èƒ½ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹æ¥é¢„æµ‹ä¸åŒç­–ç•¥çš„æ€§èƒ½ï¼Œèµ„æºåˆ†é…æ¨¡å—å¯èƒ½ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–è®¡ç®—èµ„æºçš„åˆ†é…ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºé™æ€ç­–ç•¥ï¼Œå®ç°äº†æ›´å¥½çš„å‡†ç¡®æ€§-æˆæœ¬æƒè¡¡ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨æ‘˜è¦ä¸­æ²¡æœ‰ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†å¯ä»¥æ¨æ–­ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯ä¸€å®šå‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½äº†tokenä½¿ç”¨é‡å’Œæ¨ç†å»¶è¿Ÿã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆLLMæ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€å¯¹è¯ç³»ç»Ÿã€è‡ªåŠ¨ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºï¼Œå¯ä»¥åœ¨ä¿è¯æœåŠ¡è´¨é‡çš„å‰æä¸‹ï¼Œé™ä½æ¨ç†æˆæœ¬ï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚å°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œè¯¥æ–¹æ³•å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inference-time scaling has emerged as a powerful way to improve large language model (LLM) performance by generating multiple candidate responses and selecting among them. However, existing work on dynamic allocation for test-time compute typically considers only parallel generation methods such as best-of-N, overlooking incremental decoding methods like beam search, and has largely ignored latency, focusing only on token usage. We formulate inference-time scaling as a problem of dynamic compute allocation and method selection, where the system must decide which strategy to apply and how much compute to allocate on a per-query basis. Our framework explicitly incorporates both token cost and wall-clock latency, the latter being critical for user experience and particularly for agentic workflows where models must issue multiple queries efficiently. Experiments on reasoning benchmarks show that our approach consistently outperforms static strategies, achieving favorable accuracy-cost trade-offs while remaining practical for deployment.

