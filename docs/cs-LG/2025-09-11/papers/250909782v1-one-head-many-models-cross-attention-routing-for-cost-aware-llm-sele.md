---
layout: default
title: One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection
---

# One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09782" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09782v1</a>
  <a href="https://arxiv.org/pdf/2509.09782.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09782v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09782v1', 'One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Roshini Pulishetty, Mani Kishan Ghantasala, Keerthy Kaushik Dasoju, Niti Mangwani, Vishal Garimella, Aditya Mate, Somya Chatterjee, Yue Kang, Ehi Nosakhare, Sadid Hasan, Soundar Srinivasan

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºäº¤å‰æ³¨æ„åŠ›è·¯ç”±çš„LLMé€‰æ‹©æ¡†æ¶ï¼Œå®ç°æˆæœ¬æ•ˆç›Šä¼˜åŒ–ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹é€‰æ‹©` `äº¤å‰æ³¨æ„åŠ›` `æˆæœ¬æ•ˆç›Šä¼˜åŒ–` `åŠ¨æ€è·¯ç”±` `RouterBench`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæˆæœ¬å’Œæ€§èƒ½å„å¼‚ï¼Œå¦‚ä½•åœ¨å®é™…åº”ç”¨ä¸­è¿›è¡Œå¯æ‰©å±•ã€ç»æµé«˜æ•ˆçš„éƒ¨ç½²æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨å•å¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè”åˆå»ºæ¨¡æŸ¥è¯¢å’Œæ¨¡å‹åµŒå…¥ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä¼˜LLMã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨RouterBenchä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶æé«˜äº†æˆæœ¬æ•ˆç›Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è·¯ç”±æ¡†æ¶ï¼Œåˆ©ç”¨å•å¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è”åˆå»ºæ¨¡æŸ¥è¯¢å’Œæ¨¡å‹åµŒå…¥ï¼Œä»è€Œä¸ºæ¯ä¸ªè¾“å…¥æŸ¥è¯¢åŠ¨æ€é€‰æ‹©æœ€ä¼˜çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ–¹æ³•åœ¨RouterBenchä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒRouterBenchæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å…¬å¼€åŸºå‡†ï¼ŒåŒ…å«å¤šæ ·åŒ–çš„LLMæ± å’Œé¢†åŸŸã€‚é€šè¿‡æ˜¾å¼æ•è·ç»†ç²’åº¦çš„æŸ¥è¯¢-æ¨¡å‹äº¤äº’ï¼Œè¯¥è·¯ç”±é¢„æµ‹å“åº”è´¨é‡å’Œç”Ÿæˆæˆæœ¬ï¼Œåœ¨å¹³å‡è´¨é‡æå‡ï¼ˆAIQï¼‰æ–¹é¢æ¯”ç°æœ‰è·¯ç”±æé«˜äº†6.6%ï¼Œåœ¨æœ€å¤§æ€§èƒ½æ–¹é¢æé«˜äº†2.9%ã€‚ä¸ºäº†ç¨³å¥åœ°å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æŒ‡æ•°å¥–åŠ±å‡½æ•°ï¼Œå¢å¼ºäº†ç”¨æˆ·åå¥½ä¹‹é—´çš„ç¨³å®šæ€§ã€‚æœ€ç»ˆçš„æ¶æ„æ˜¯è½»é‡çº§çš„ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è·¨é¢†åŸŸæ³›åŒ–ï¼Œå¹¶ä¸”ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”æé«˜äº†æ•ˆç‡ï¼Œä¸ºæˆæœ¬æ„ŸçŸ¥çš„LLMè·¯ç”±å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æ ‡å‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¡ç®—æˆæœ¬å’Œæ€§èƒ½æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¦‚ä½•æ ¹æ®ä¸åŒçš„æŸ¥è¯¢éœ€æ±‚ï¼ŒåŠ¨æ€åœ°é€‰æ‹©æœ€åˆé€‚çš„LLMï¼Œä»¥åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶é™ä½æˆæœ¬ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚ç°æœ‰çš„è·¯ç”±æ–¹æ³•é€šå¸¸æ— æ³•å……åˆ†æ•æ‰æŸ¥è¯¢å’Œæ¨¡å‹ä¹‹é—´çš„ç»†ç²’åº¦äº¤äº’ï¼Œå¯¼è‡´é€‰æ‹©ç»“æœä¸å¤Ÿä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¾å¼åœ°å»ºæ¨¡æŸ¥è¯¢å’Œæ¨¡å‹ä¹‹é—´çš„äº¤äº’å…³ç³»ã€‚é€šè¿‡å°†æŸ¥è¯¢å’Œæ¨¡å‹åµŒå…¥ä½œä¸ºäº¤å‰æ³¨æ„åŠ›çš„è¾“å…¥ï¼Œå¯ä»¥å­¦ä¹ åˆ°æ¯ä¸ªæŸ¥è¯¢ä¸ä¸åŒæ¨¡å‹ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œé¢„æµ‹æ¯ä¸ªæ¨¡å‹å¯¹äºè¯¥æŸ¥è¯¢çš„å“åº”è´¨é‡å’Œç”Ÿæˆæˆæœ¬ã€‚åŸºäºè¿™äº›é¢„æµ‹ï¼Œå¯ä»¥é€‰æ‹©åœ¨æ€§èƒ½å’Œæˆæœ¬ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡çš„LLMã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) æŸ¥è¯¢å’Œæ¨¡å‹åµŒå…¥æ¨¡å—ï¼šå°†è¾“å…¥æŸ¥è¯¢å’Œå€™é€‰LLMè½¬æ¢ä¸ºåµŒå…¥å‘é‡ã€‚2) äº¤å‰æ³¨æ„åŠ›è·¯ç”±æ¨¡å—ï¼šåˆ©ç”¨å•å¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè”åˆå»ºæ¨¡æŸ¥è¯¢å’Œæ¨¡å‹åµŒå…¥ï¼Œé¢„æµ‹å“åº”è´¨é‡å’Œç”Ÿæˆæˆæœ¬ã€‚3) æ¨¡å‹é€‰æ‹©æ¨¡å—ï¼šåŸºäºé¢„æµ‹çš„è´¨é‡å’Œæˆæœ¬ï¼Œä½¿ç”¨æŒ‡æ•°å¥–åŠ±å‡½æ•°é€‰æ‹©æœ€ä¼˜çš„LLMã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥æŸ¥è¯¢ï¼Œé¦–å…ˆå°†å…¶å’Œå€™é€‰LLMè¿›è¡ŒåµŒå…¥è¡¨ç¤ºï¼Œç„¶åé€šè¿‡äº¤å‰æ³¨æ„åŠ›è·¯ç”±æ¨¡å—é¢„æµ‹æ¯ä¸ªLLMçš„è´¨é‡å’Œæˆæœ¬ï¼Œæœ€åæ ¹æ®é¢„æµ‹ç»“æœé€‰æ‹©æœ€ä¼˜çš„LLMã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨å•å¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥å»ºæ¨¡æŸ¥è¯¢å’Œæ¨¡å‹ä¹‹é—´çš„ç»†ç²’åº¦äº¤äº’ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹LLMçš„å“åº”è´¨é‡å’Œç”Ÿæˆæˆæœ¬ï¼Œä»è€Œå®ç°æ›´ä¼˜çš„LLMé€‰æ‹©ã€‚æ­¤å¤–ï¼Œæå‡ºçš„æŒ‡æ•°å¥–åŠ±å‡½æ•°èƒ½å¤Ÿæ›´ç¨³å¥åœ°å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼Œæé«˜ç”¨æˆ·åå¥½ä¹‹é—´çš„ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šäº¤å‰æ³¨æ„åŠ›æ¨¡å—ä½¿ç”¨å•å¤´æ³¨æ„åŠ›ï¼Œä»¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚æŒ‡æ•°å¥–åŠ±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼Œå…¶å½¢å¼ä¸º exp(Î± * Quality - Î² * Cost)ï¼Œå…¶ä¸­ Î± å’Œ Î² æ˜¯å¯è°ƒèŠ‚çš„å‚æ•°ï¼Œç”¨äºæ§åˆ¶å¯¹è´¨é‡å’Œæˆæœ¬çš„é‡è§†ç¨‹åº¦ã€‚RouterBenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œè¯¥åŸºå‡†åŒ…å«å¤šæ ·åŒ–çš„LLMæ± å’Œé¢†åŸŸã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨RouterBenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡è´¨é‡æå‡ï¼ˆAIQï¼‰æ–¹é¢æ¯”ç°æœ‰è·¯ç”±æé«˜äº†6.6%ï¼Œåœ¨æœ€å¤§æ€§èƒ½æ–¹é¢æé«˜äº†2.9%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…·æœ‰è½»é‡çº§çš„æ¶æ„å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è·¨é¢†åŸŸåº”ç”¨ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ•ˆç‡æ–¹é¢ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦åŠ¨æ€é€‰æ‹©LLMçš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€å†…å®¹ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„ç‰¹ç‚¹å’Œæˆæœ¬é¢„ç®—ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„LLMï¼Œå¯ä»¥æ˜¾è‘—æé«˜æœåŠ¡è´¨é‡å’Œé™ä½è¿è¥æˆæœ¬ã€‚è¯¥æŠ€æœ¯è¿˜æœ‰åŠ©äºæ¨åŠ¨LLMåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„éƒ¨ç½²ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The proliferation of large language models (LLMs) with varying computational costs and performance profiles presents a critical challenge for scalable, cost-effective deployment in real-world applications. We introduce a unified routing framework that leverages a single-head cross-attention mechanism to jointly model query and model embeddings, enabling dynamic selection of the optimal LLM for each input query. Our approach is evaluated on RouterBench, a large-scale, publicly available benchmark encompassing diverse LLM pools and domains. By explicitly capturing fine-grained query-model interactions, our router predicts both response quality and generation cost, achieving up to 6.6% improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum performance over existing routers. To robustly balance performance and cost, we propose an exponential reward function that enhances stability across user preferences. The resulting architecture is lightweight, generalizes effectively across domains, and demonstrates improved efficiency compared to prior methods, establishing a new standard for cost-aware LLM routing.

