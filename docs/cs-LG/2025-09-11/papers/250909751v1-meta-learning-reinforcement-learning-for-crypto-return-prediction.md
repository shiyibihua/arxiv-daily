---
layout: default
title: Meta-Learning Reinforcement Learning for Crypto-Return Prediction
---

# Meta-Learning Reinforcement Learning for Crypto-Return Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09751" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09751v1</a>
  <a href="https://arxiv.org/pdf/2509.09751.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09751v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09751v1', 'Meta-Learning Reinforcement Learning for Crypto-Return Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junqiao Wang, Zhaoyang Guan, Guanyu Liu, Tianze Xia, Xianzhi Li, Shuo Yin, Xinyuan Song, Chuhan Cheng, Tianyu Shi, Alex Lee

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMeta-RL-Cryptoï¼Œç”¨äºåŠ å¯†è´§å¸æ”¶ç›Šé¢„æµ‹çš„è‡ªæå‡äº¤æ˜“Agent**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…ƒå­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `åŠ å¯†è´§å¸` `æ”¶ç›Šé¢„æµ‹` `äº¤æ˜“Agent`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŠ å¯†è´§å¸æ”¶ç›Šé¢„æµ‹å—å¤šç§å› ç´ å½±å“ï¼Œæ•°æ®ç¨€ç¼ºï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹å¿«é€Ÿå˜åŒ–çš„å¸‚åœºç¯å¢ƒã€‚
2. Meta-RL-Cryptoåˆ©ç”¨å…ƒå­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ„å»ºè‡ªæå‡äº¤æ˜“Agentï¼Œæ— éœ€äººå·¥ç›‘ç£ï¼ŒæŒç»­ä¼˜åŒ–äº¤æ˜“ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥Agentåœ¨çœŸå®å¸‚åœºæŠ€æœ¯æŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œè¶…è¶Šäº†å…¶ä»–åŸºäºLLMçš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢„æµ‹åŠ å¯†è´§å¸æ”¶ç›Šæ˜¯å‡ºäº†åçš„å›°éš¾ï¼šä»·æ ¼æ³¢åŠ¨å—åˆ°å¿«é€Ÿå˜åŒ–çš„é“¾ä¸Šæ´»åŠ¨ã€æ–°é—»æµå’Œç¤¾ä¼šæƒ…ç»ªçš„æ··åˆé©±åŠ¨ï¼Œè€Œå¸¦æ ‡ç­¾çš„è®­ç»ƒæ•°æ®ç¨€ç¼ºä¸”æ˜‚è´µã€‚æœ¬æ–‡æå‡ºMeta-RL-Cryptoï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŸºäºTransformerçš„æ¶æ„ï¼Œå®ƒç»Ÿä¸€äº†å…ƒå­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥åˆ›å»ºä¸€ä¸ªå®Œå…¨è‡ªæˆ‘æ”¹è¿›çš„äº¤æ˜“Agentã€‚ä»ä¸€ä¸ªvanillaæŒ‡ä»¤è°ƒä¼˜çš„LLMå¼€å§‹ï¼ŒAgentåœ¨ä¸€ä¸ªé—­ç¯æ¶æ„ä¸­è¿­ä»£åœ°åœ¨ä¸‰ä¸ªè§’è‰²ï¼ˆactorã€judgeå’Œmeta-judgeï¼‰ä¹‹é—´åˆ‡æ¢ã€‚è¿™ä¸ªå­¦ä¹ è¿‡ç¨‹ä¸éœ€è¦é¢å¤–çš„äººå·¥ç›‘ç£ã€‚å®ƒå¯ä»¥åˆ©ç”¨å¤šæ¨¡æ€å¸‚åœºè¾“å…¥å’Œå†…éƒ¨åå¥½åé¦ˆã€‚ç³»ç»Ÿä¸­çš„Agentä¸æ–­æ”¹è¿›äº¤æ˜“ç­–ç•¥å’Œè¯„ä¼°æ ‡å‡†ã€‚åœ¨ä¸åŒå¸‚åœºæœºåˆ¶ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMeta-RL-Cryptoåœ¨çœŸå®å¸‚åœºçš„æŠ€æœ¯æŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”ä¼˜äºå…¶ä»–åŸºäºLLMçš„åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŠ å¯†è´§å¸æ”¶ç›Šé¢„æµ‹çš„éš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡æ€å¸‚åœºä¿¡æ¯ï¼Œä¸”ç¼ºä¹è¶³å¤Ÿçš„å¸¦æ ‡ç­¾æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´é¢„æµ‹ç²¾åº¦ä¸é«˜ï¼Œéš¾ä»¥é€‚åº”å¿«é€Ÿå˜åŒ–çš„å¸‚åœºç¯å¢ƒã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äººå·¥ç‰¹å¾å·¥ç¨‹å’Œä¸“å®¶çŸ¥è¯†ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ³›åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å…ƒå­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿè‡ªæˆ‘æ”¹è¿›çš„äº¤æ˜“Agentã€‚è¯¥Agenté€šè¿‡åœ¨actorã€judgeå’Œmeta-judgeä¸‰ä¸ªè§’è‰²ä¹‹é—´è¿­ä»£ï¼Œä¸æ–­ä¼˜åŒ–äº¤æ˜“ç­–ç•¥å’Œè¯„ä¼°æ ‡å‡†ï¼Œä»è€Œåœ¨ç¼ºä¹äººå·¥ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œé€‚åº”ä¸åŒçš„å¸‚åœºç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMeta-RL-Cryptoçš„æ•´ä½“æ¶æ„æ˜¯ä¸€ä¸ªé—­ç¯ç³»ç»Ÿï¼ŒåŒ…å«ä»¥ä¸‹ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) Actorï¼šè´Ÿè´£æ ¹æ®å¸‚åœºä¿¡æ¯å’Œå½“å‰ç­–ç•¥ç”Ÿæˆäº¤æ˜“å†³ç­–ã€‚2) Judgeï¼šè´Ÿè´£è¯„ä¼°Actorçš„äº¤æ˜“å†³ç­–ï¼Œå¹¶æä¾›åé¦ˆä¿¡å·ã€‚3) Meta-Judgeï¼šè´Ÿè´£è¯„ä¼°Judgeçš„è¯„ä¼°æ ‡å‡†ï¼Œå¹¶è¿›è¡Œè°ƒæ•´ï¼Œä»¥æé«˜è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚è¿™ä¸‰ä¸ªæ¨¡å—åœ¨ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ä¸­ä¸æ–­äº¤äº’ï¼Œå…±åŒæå‡Agentçš„äº¤æ˜“èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å…ƒå­¦ä¹ åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œä½¿å¾—Agentèƒ½å¤Ÿè‡ªæˆ‘å­¦ä¹ å’Œæ”¹è¿›è¯„ä¼°æ ‡å‡†ã€‚ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–äºå›ºå®šçš„å¥–åŠ±å‡½æ•°ï¼Œéš¾ä»¥é€‚åº”å¤æ‚å¤šå˜çš„å¸‚åœºç¯å¢ƒã€‚Meta-RL-Cryptoé€šè¿‡Meta-Judgeæ¨¡å—åŠ¨æ€è°ƒæ•´è¯„ä¼°æ ‡å‡†ï¼Œä½¿å¾—Agentèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„å¸‚åœºæœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•ä½¿ç”¨Transformerä½œä¸ºåŸºç¡€æ¶æ„ï¼Œä»¥å¤„ç†å¤šæ¨¡æ€å¸‚åœºè¾“å…¥ã€‚Actorä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜çš„LLMç”Ÿæˆäº¤æ˜“å†³ç­–ã€‚Judgeå’ŒMeta-Judgeä¹ŸåŸºäºTransformeræ„å»ºï¼Œç”¨äºè¯„ä¼°äº¤æ˜“å†³ç­–å’Œè¯„ä¼°æ ‡å‡†ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨é¼“åŠ±Actorç”Ÿæˆæœ‰åˆ©å¯å›¾çš„äº¤æ˜“å†³ç­–ï¼Œå¹¶æƒ©ç½šä¸åˆç†çš„è¯„ä¼°æ ‡å‡†ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMeta-RL-Cryptoåœ¨çœŸå®å¸‚åœºçš„æŠ€æœ¯æŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”ä¼˜äºå…¶ä»–åŸºäºLLMçš„åŸºçº¿æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†æ€»ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨åŠ å¯†è´§å¸æ”¶ç›Šé¢„æµ‹æ–¹é¢å±•ç°å‡ºè‰¯å¥½çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½äº¤æ˜“ç³»ç»Ÿã€é‡åŒ–æŠ•èµ„ç­–ç•¥å’Œé£é™©ç®¡ç†ç­‰é¢†åŸŸã€‚é€šè¿‡æ„å»ºè‡ªé€‚åº”çš„äº¤æ˜“Agentï¼Œå¯ä»¥æé«˜åŠ å¯†è´§å¸å¸‚åœºçš„äº¤æ˜“æ•ˆç‡å’Œç›ˆåˆ©èƒ½åŠ›ï¼Œé™ä½æŠ•èµ„é£é™©ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é‡‘èå¸‚åœºï¼Œä¸ºæŠ•èµ„è€…æä¾›æ›´æ™ºèƒ½åŒ–çš„æŠ•èµ„å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Predicting cryptocurrency returns is notoriously difficult: price movements are driven by a fast-shifting blend of on-chain activity, news flow, and social sentiment, while labeled training data are scarce and expensive. In this paper, we present Meta-RL-Crypto, a unified transformer-based architecture that unifies meta-learning and reinforcement learning (RL) to create a fully self-improving trading agent. Starting from a vanilla instruction-tuned LLM, the agent iteratively alternates between three roles-actor, judge, and meta-judge-in a closed-loop architecture. This learning process requires no additional human supervision. It can leverage multimodal market inputs and internal preference feedback. The agent in the system continuously refines both the trading policy and evaluation criteria. Experiments across diverse market regimes demonstrate that Meta-RL-Crypto shows good performance on the technical indicators of the real market and outperforming other LLM-based baselines.

