---
layout: default
title: Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning
---

# Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09135" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09135v2</a>
  <a href="https://arxiv.org/pdf/2509.09135.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09135v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09135v2', 'Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuefeng Wang, Lei Zhang, Henglin Pu, Ahmed H. Qureshi, Husheng Li

**åˆ†ç±»**: cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11 (æ›´æ–°: 2025-09-17)

**å¤‡æ³¨**: 19 pages, 10 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„è¿ç»­æ—¶é—´å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè§£å†³é«˜ç»´åŠ¨åŠ›ç³»ç»Ÿä¸­çš„ç­–ç•¥è®­ç»ƒé—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ` `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ` `å€¼å‡½æ•°è¿‘ä¼¼` `æ¢¯åº¦è¿­ä»£` `Hamilton-Jacobi-Bellmanæ–¹ç¨‹` `æ·±åº¦å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨é«˜é¢‘æˆ–ä¸è§„åˆ™æ—¶é—´é—´éš”äº¤äº’çš„å¤æ‚åŠ¨åŠ›ç³»ç»Ÿä¸­è¡¨ç°ä¸ä½³ï¼Œè¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯åº”ç”¨å—é™äºç»´åº¦ç¾éš¾å’Œå€¼å‡½æ•°è¿‘ä¼¼éš¾é¢˜ã€‚
2. è®ºæ–‡æå‡ºCT-MARLæ¡†æ¶ï¼Œåˆ©ç”¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(PINN)é€¼è¿‘HJBå€¼å‡½æ•°ï¼Œå¹¶é€šè¿‡å€¼æ¢¯åº¦è¿­ä»£(VGI)æ¨¡å—å¯¹é½å€¼å­¦ä¹ ä¸å€¼æ¢¯åº¦å­¦ä¹ ï¼Œæå‡æ¢¯åº¦ä¿çœŸåº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ™ºèƒ½ä½“ç²’å­ç¯å¢ƒ(MPE)å’Œå¤šæ™ºèƒ½ä½“MuJoCoç­‰è¿ç»­æ—¶é—´åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¼˜äºç°æœ‰è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œå¹¶èƒ½æ‰©å±•åˆ°å¤æ‚å¤šæ™ºèƒ½ä½“åŠ¨åŠ›å­¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥å¤„ç†éœ€è¦é«˜é¢‘æˆ–ä¸è§„åˆ™æ—¶é—´é—´éš”äº¤äº’çš„å¤æ‚åŠ¨åŠ›ç³»ç»Ÿã€‚è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ (CTRL)é€šè¿‡ç”¨Hamilton-Jacobi-Bellman (HJB)æ–¹ç¨‹çš„ç²˜æ€§è§£å®šä¹‰çš„å¾®åˆ†å€¼å‡½æ•°ä»£æ›¿ç¦»æ•£æ—¶é—´Bellmané€’å½’ï¼Œæˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚è™½ç„¶CTRLæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶åº”ç”¨ä¸»è¦é™äºå•æ™ºèƒ½ä½“é¢†åŸŸã€‚è¿™ç§é™åˆ¶æºäºä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š(i) HJBæ–¹ç¨‹çš„ä¼ ç»Ÿæ±‚è§£æ–¹æ³•å­˜åœ¨ç»´åº¦ç¾éš¾(CoD)ï¼Œä½¿å…¶åœ¨é«˜ç»´ç³»ç»Ÿä¸­éš¾ä»¥å¤„ç†ï¼›(ii) å³ä½¿ä½¿ç”¨åŸºäºHJBçš„å­¦ä¹ æ–¹æ³•ï¼Œåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­å‡†ç¡®é€¼è¿‘ä¸­å¿ƒåŒ–å€¼å‡½æ•°ä»ç„¶å¾ˆå›°éš¾ï¼Œè¿™åè¿‡æ¥ä¼šç ´åç­–ç•¥è®­ç»ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªCT-MARLæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(PINN)æ¥å¤§è§„æ¨¡åœ°é€¼è¿‘åŸºäºHJBçš„å€¼å‡½æ•°ã€‚ä¸ºäº†ç¡®ä¿å€¼ä¸å¾®åˆ†ç»“æ„ä¸€è‡´ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥å€¼æ¢¯åº¦è¿­ä»£(VGI)æ¨¡å—æ¥å¯¹é½å€¼å­¦ä¹ ä¸å€¼æ¢¯åº¦å­¦ä¹ ï¼Œè¯¥æ¨¡å—è¿­ä»£åœ°ç»†åŒ–æ²¿è½¨è¿¹çš„å€¼æ¢¯åº¦ã€‚è¿™æé«˜äº†æ¢¯åº¦ä¿çœŸåº¦ï¼Œè¿›è€Œäº§ç”Ÿæ›´å‡†ç¡®çš„å€¼å’Œæ›´å¼ºçš„ç­–ç•¥å­¦ä¹ ã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†åŸºå‡†çš„è¿ç»­æ—¶é—´å˜ä½“ï¼ˆåŒ…æ‹¬å¤šæ™ºèƒ½ä½“ç²’å­ç¯å¢ƒ(MPE)å’Œå¤šæ™ºèƒ½ä½“MuJoCoï¼‰è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰çš„è¿ç»­æ—¶é—´RLåŸºçº¿ï¼Œå¹¶å¯æ‰©å±•åˆ°å¤æ‚çš„å¤šæ™ºèƒ½ä½“åŠ¨åŠ›å­¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç”±äºç»´åº¦ç¾éš¾å¯¼è‡´éš¾ä»¥å‡†ç¡®ä¼°è®¡å€¼å‡½æ•°ï¼Œè¿›è€Œå½±å“ç­–ç•¥è®­ç»ƒçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é«˜ç»´çŠ¶æ€ç©ºé—´æ—¶ï¼Œè®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚æ­¤å¤–ï¼Œå¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸‹çš„å€¼å‡½æ•°è¿‘ä¼¼æ›´åŠ å›°éš¾ï¼Œå®¹æ˜“å¯¼è‡´ç­–ç•¥ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰æ¥è¿‘ä¼¼HJBæ–¹ç¨‹çš„å€¼å‡½æ•°ï¼Œå¹¶å¼•å…¥å€¼æ¢¯åº¦è¿­ä»£ï¼ˆVGIï¼‰æ¨¡å—æ¥æé«˜å€¼å‡½æ•°çš„æ¢¯åº¦ç²¾åº¦ã€‚é€šè¿‡å°†ç‰©ç†ä¿¡æ¯èå…¥ç¥ç»ç½‘ç»œï¼Œå¯ä»¥æœ‰æ•ˆåœ°é™ä½ç»´åº¦ç¾éš¾çš„å½±å“ï¼Œå¹¶æé«˜å€¼å‡½æ•°çš„æ³›åŒ–èƒ½åŠ›ã€‚VGIæ¨¡å—é€šè¿‡è¿­ä»£ä¼˜åŒ–å€¼æ¢¯åº¦ï¼Œç¡®ä¿å€¼å‡½æ•°ä¸å…¶æ¢¯åº¦çš„ä¸€è‡´æ€§ï¼Œä»è€Œæé«˜ç­–ç•¥å­¦ä¹ çš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCT-MARLæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) åŸºäºPINNçš„å€¼å‡½æ•°è¿‘ä¼¼å™¨ï¼šä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼HJBæ–¹ç¨‹çš„å€¼å‡½æ•°ï¼ŒæŸå¤±å‡½æ•°åŒ…æ‹¬HJBæ–¹ç¨‹çš„æ®‹å·®é¡¹å’Œè¾¹ç•Œæ¡ä»¶é¡¹ã€‚2) å€¼æ¢¯åº¦è¿­ä»£ï¼ˆVGIï¼‰æ¨¡å—ï¼šé€šè¿‡é‡‡æ ·è½¨è¿¹ï¼Œè®¡ç®—å€¼å‡½æ•°çš„æ¢¯åº¦ï¼Œå¹¶ä¸ç¥ç»ç½‘ç»œé¢„æµ‹çš„æ¢¯åº¦è¿›è¡Œæ¯”è¾ƒï¼Œé€šè¿‡ä¼˜åŒ–æ¢¯åº¦ä¸€è‡´æ€§æŸå¤±æ¥æé«˜æ¢¯åº¦ç²¾åº¦ã€‚3) ç­–ç•¥ä¼˜åŒ–æ¨¡å—ï¼šåŸºäºè¿‘ä¼¼çš„å€¼å‡½æ•°ï¼Œä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•æˆ–å…¶ä»–ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°æ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆä½¿ç”¨PINNåˆå§‹åŒ–å€¼å‡½æ•°ï¼Œç„¶åé€šè¿‡VGIæ¨¡å—è¿­ä»£ä¼˜åŒ–å€¼å‡½æ•°å’Œæ¢¯åº¦ï¼Œæœ€åä½¿ç”¨ç­–ç•¥ä¼˜åŒ–æ¨¡å—æ›´æ–°ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œåº”ç”¨äºå¤šæ™ºèƒ½ä½“è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶æå‡ºäº†å€¼æ¢¯åº¦è¿­ä»£æ¨¡å—æ¥æé«˜å€¼å‡½æ•°çš„æ¢¯åº¦ç²¾åº¦ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè¡¨æ ¼æˆ–çº¿æ€§å‡½æ•°è¿‘ä¼¼çš„æ–¹æ³•ç›¸æ¯”ï¼ŒPINNèƒ½å¤Ÿå¤„ç†é«˜ç»´çŠ¶æ€ç©ºé—´ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚VGIæ¨¡å—é€šè¿‡æ˜¾å¼åœ°ä¼˜åŒ–å€¼æ¢¯åº¦ï¼Œæé«˜äº†å€¼å‡½æ•°çš„ç²¾åº¦å’Œç­–ç•¥å­¦ä¹ çš„ç¨³å®šæ€§ï¼Œè¿™æ˜¯ç°æœ‰æ–¹æ³•æ‰€ç¼ºä¹çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨PINNçš„è®¾è®¡ä¸­ï¼Œå…³é”®çš„å‚æ•°åŒ…æ‹¬ç¥ç»ç½‘ç»œçš„ç»“æ„ï¼ˆå±‚æ•°ã€æ¯å±‚ç¥ç»å…ƒæ•°é‡ï¼‰ã€æ¿€æ´»å‡½æ•°ã€ä¼˜åŒ–å™¨ç­‰ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬HJBæ–¹ç¨‹çš„æ®‹å·®é¡¹ã€è¾¹ç•Œæ¡ä»¶é¡¹å’Œæ¢¯åº¦ä¸€è‡´æ€§æŸå¤±é¡¹ã€‚æ¢¯åº¦ä¸€è‡´æ€§æŸå¤±é€šå¸¸é‡‡ç”¨å‡æ–¹è¯¯å·®æˆ–HuberæŸå¤±ã€‚åœ¨VGIæ¨¡å—ä¸­ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„é‡‡æ ·ç­–ç•¥æ¥ç”Ÿæˆè½¨è¿¹ï¼Œå¹¶è®¾ç½®æ¢¯åº¦ä¼˜åŒ–çš„å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°ã€‚ç­–ç•¥ä¼˜åŒ–æ¨¡å—å¯ä»¥é€‰æ‹©åˆé€‚çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œå¦‚PPOæˆ–TRPOï¼Œå¹¶è°ƒæ•´ç›¸åº”çš„è¶…å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCT-MARLæ¡†æ¶åœ¨å¤šæ™ºèƒ½ä½“ç²’å­ç¯å¢ƒ(MPE)å’Œå¤šæ™ºèƒ½ä½“MuJoCoç­‰è¿ç»­æ—¶é—´åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚ä¾‹å¦‚ï¼Œåœ¨MPEä¸­çš„åˆä½œé€šä¿¡ä»»åŠ¡ä¸­ï¼ŒCT-MARLçš„å¹³å‡å¥–åŠ±æ¯”SAC-Discreteé«˜å‡ºçº¦30%ã€‚åœ¨MuJoCoä¸­çš„èš‚èšå¯¼èˆªä»»åŠ¡ä¸­ï¼ŒCT-MARLçš„æˆåŠŸç‡ä¹Ÿæ˜æ˜¾é«˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦é«˜é¢‘æ§åˆ¶å’Œå¤æ‚äº¤äº’çš„æœºå™¨äººç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶ã€é‡‘èäº¤æ˜“ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨å¤šæœºå™¨äººååŒæ§åˆ¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å®ç°å¯¹å¤šä¸ªæœºå™¨äººçš„è¿ç»­æ—¶é—´è½¨è¿¹è§„åˆ’å’Œæ§åˆ¶ï¼Œæé«˜ç³»ç»Ÿçš„æ•ˆç‡å’Œé²æ£’æ€§ã€‚åœ¨é‡‘èäº¤æ˜“ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å¯¹é«˜é¢‘äº¤æ˜“ç­–ç•¥è¿›è¡Œä¼˜åŒ–ï¼Œæé«˜æ”¶ç›Šç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient fidelity, in turn yielding more accurate values and stronger policy learning. We evaluate our method using continuous-time variants of standard benchmarks, including multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous-time RL baselines and scales to complex multi-agent dynamics.

