---
layout: default
title: Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents
---

# Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09265" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09265v1</a>
  <a href="https://arxiv.org/pdf/2509.09265.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09265v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09265v1', 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang, Ke Wang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: ICLR 2026 Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦(EMPG)ä»¥æå‡LLM Agentåœ¨é•¿æ—¶ä»»åŠ¡ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LLM Agent` `é•¿æ—¶ä»»åŠ¡` `ç­–ç•¥æ¢¯åº¦` `ç†µè°ƒåˆ¶` `å¼ºåŒ–å­¦ä¹ ` `ä¸ç¡®å®šæ€§` `ä¿¡ç”¨åˆ†é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿æ—¶ä»»åŠ¡ä¸­ï¼ŒLLM Agenté¢ä¸´ç¨€ç–å¥–åŠ±å¸¦æ¥çš„ä¿¡ç”¨åˆ†é…éš¾é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–å¯†é›†å¥–åŠ±ä¿¡å·ï¼Œä½†æ•ˆæœæœ‰é™ã€‚
2. EMPGé€šè¿‡ç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦ï¼Œæ ¹æ®æ¯ä¸€æ­¥çš„ä¸ç¡®å®šæ€§é‡æ–°æ ¡å‡†å­¦ä¹ ä¿¡å·ï¼Œç¨³å®šæ¢ç´¢å¹¶ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚
3. åœ¨WebShopã€ALFWorldå’ŒDeep Searchç­‰ä»»åŠ¡ä¸Šï¼ŒEMPGæ˜¾è‘—ä¼˜äºç°æœ‰ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œæå‡Agentæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„Agentåœ¨é•¿æ—¶ä»»åŠ¡ä¸­é¢ä¸´ç€ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼šç¨€ç–çš„ã€åŸºäºç»“æœçš„å¥–åŠ±ä½¿å¾—éš¾ä»¥å°†åŠŸåŠ³åˆ†é…ç»™ä¸­é—´æ­¥éª¤ã€‚ä»¥å¾€çš„æ–¹æ³•ä¸»è¦é›†ä¸­äºåˆ›å»ºå¯†é›†çš„å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼å­¦ä¹ ï¼Œä¾‹å¦‚é€šè¿‡é€†å¼ºåŒ–å­¦ä¹ ç­‰ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œæˆ–è€…ä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è¿›è¡Œé€æ­¥åé¦ˆã€‚æœ¬æ–‡å‘ç°LLMå­¦ä¹ åŠ¨æ€çš„ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼šç­–ç•¥æ¢¯åº¦çš„å¹…åº¦ä¸ç†µå›ºæœ‰åœ°è€¦åˆåœ¨ä¸€èµ·ï¼Œå¯¼è‡´å¯¹ç¡®ä¿¡çš„æ­£ç¡®è¡Œä¸ºè¿›è¡Œä½æ•ˆçš„å°æ›´æ–°ï¼Œå¹¶å¯èƒ½ä½¿ä¸ç¡®å®šçš„è¡Œä¸ºè¿›è¡Œä¸ç¨³å®šçš„å¤§æ›´æ–°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦(EMPG)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºé€æ­¥ä¸ç¡®å®šæ€§å’Œæœ€ç»ˆä»»åŠ¡ç»“æœé‡æ–°æ ¡å‡†å­¦ä¹ ä¿¡å·çš„æ¡†æ¶ã€‚EMPGæ”¾å¤§äº†å¯¹ç¡®ä¿¡çš„æ­£ç¡®è¡Œä¸ºçš„æ›´æ–°ï¼Œæƒ©ç½šäº†ç¡®ä¿¡çš„é”™è¯¯ï¼Œå¹¶å‡å¼±äº†æ¥è‡ªä¸ç¡®å®šæ­¥éª¤çš„æ›´æ–°ï¼Œä»¥ç¨³å®šæ¢ç´¢ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ä¸ªæœªæ¥æ¸…æ™°åº¦å¥–åŠ±é¡¹ï¼Œé¼“åŠ±Agentæ‰¾åˆ°æ›´å¯é¢„æµ‹çš„è§£å†³æ–¹æ¡ˆè·¯å¾„ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„Agentä»»åŠ¡WebShopã€ALFWorldå’ŒDeep Searchä¸Šçš„ç»¼åˆå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†EMPGå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„ç­–ç•¥æ¢¯åº¦åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é•¿æ—¶ä»»åŠ¡ä¸­ï¼ŒåŸºäºLLMçš„Agentç”±äºç¨€ç–å¥–åŠ±è€Œéš¾ä»¥è¿›è¡Œæœ‰æ•ˆå­¦ä¹ çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚é€†å¼ºåŒ–å­¦ä¹ å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œè¯•å›¾é€šè¿‡ç”Ÿæˆå¯†é›†çš„å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼å­¦ä¹ ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å¤æ‚ä¸”éš¾ä»¥æ³›åŒ–ã€‚æ›´æ ¹æœ¬çš„é—®é¢˜åœ¨äºï¼ŒLLMç­–ç•¥æ¢¯åº¦çš„å¹…åº¦ä¸ç­–ç•¥çš„ç†µå€¼è€¦åˆï¼Œå¯¼è‡´å¯¹ç¡®å®šæ€§é«˜çš„æ­£ç¡®è¡Œä¸ºæ›´æ–°è¿‡å°ï¼Œè€Œå¯¹ä¸ç¡®å®šè¡Œä¸ºçš„æ›´æ–°å¯èƒ½ä¸ç¨³å®šï¼Œä»è€Œå½±å“å­¦ä¹ æ•ˆç‡å’Œç¨³å®šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è§£è€¦ç­–ç•¥æ¢¯åº¦å’Œç†µä¹‹é—´çš„å…³ç³»ï¼Œé€šè¿‡ç†µè°ƒåˆ¶æ¥é‡æ–°æ ¡å‡†å­¦ä¹ ä¿¡å·ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºç¡®å®šæ€§é«˜çš„æ­£ç¡®è¡Œä¸ºï¼Œæ”¾å¤§å…¶æ¢¯åº¦æ›´æ–°ï¼›å¯¹äºç¡®å®šæ€§é«˜çš„é”™è¯¯è¡Œä¸ºï¼Œæƒ©ç½šå…¶æ¢¯åº¦æ›´æ–°ï¼›å¯¹äºä¸ç¡®å®šæ€§é«˜çš„è¡Œä¸ºï¼Œå‡å¼±å…¶æ¢¯åº¦æ›´æ–°ã€‚æ­¤å¤–ï¼Œå¼•å…¥æœªæ¥æ¸…æ™°åº¦å¥–åŠ±ï¼Œé¼“åŠ±Agenté€‰æ‹©æ›´æ˜“é¢„æµ‹çš„è·¯å¾„ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEMPGæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) LLM Agentä¸ç¯å¢ƒäº¤äº’ï¼Œç”Ÿæˆè½¨è¿¹æ•°æ®ï¼›2) è®¡ç®—æ¯ä¸€æ­¥åŠ¨ä½œçš„ç†µå€¼ï¼Œä½œä¸ºä¸ç¡®å®šæ€§çš„åº¦é‡ï¼›3) æ ¹æ®ç†µå€¼å’Œæœ€ç»ˆå¥–åŠ±ï¼Œè®¡ç®—ç†µè°ƒåˆ¶åçš„ç­–ç•¥æ¢¯åº¦ï¼›4) ä½¿ç”¨ç†µè°ƒåˆ¶åçš„ç­–ç•¥æ¢¯åº¦æ›´æ–°LLM Agentçš„ç­–ç•¥ï¼›5) è®¡ç®—æœªæ¥æ¸…æ™°åº¦å¥–åŠ±ï¼Œå¹¶å°†å…¶åŠ å…¥åˆ°æ€»å¥–åŠ±ä¸­ã€‚æ•´ä¸ªæµç¨‹é€šè¿‡ä¸æ–­è¿­ä»£ï¼Œä¼˜åŒ–Agentçš„ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆé•¿æ—¶ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šEMPGæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®æ¯ä¸€æ­¥çš„ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´å­¦ä¹ ä¿¡å·çš„å¼ºåº¦ã€‚ä¸ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç›¸æ¯”ï¼ŒEMPGèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®ï¼Œæé«˜å­¦ä¹ æ•ˆç‡å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œæœªæ¥æ¸…æ™°åº¦å¥–åŠ±ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ï¼Œå®ƒèƒ½å¤Ÿå¼•å¯¼Agenté€‰æ‹©æ›´æ˜“äºå­¦ä¹ çš„è·¯å¾„ã€‚

**å…³é”®è®¾è®¡**ï¼šEMPGçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç†µçš„è®¡ç®—æ–¹å¼ï¼Œè®ºæ–‡ä¸­ä½¿ç”¨çš„æ˜¯æ ‡å‡†çš„ç­–ç•¥ç†µï¼›2) ç†µè°ƒåˆ¶å‡½æ•°çš„é€‰æ‹©ï¼Œè®ºæ–‡ä¸­ä½¿ç”¨äº†çº¿æ€§å‡½æ•°ï¼Œä½†ä¹Ÿå¯ä»¥å°è¯•å…¶ä»–éçº¿æ€§å‡½æ•°ï¼›3) æœªæ¥æ¸…æ™°åº¦å¥–åŠ±çš„è®¡ç®—æ–¹å¼ï¼Œè®ºæ–‡ä¸­ä½¿ç”¨çš„æ˜¯åŸºäºæ¨¡å‹é¢„æµ‹çš„å¥–åŠ±ï¼Œä¹Ÿå¯ä»¥å°è¯•å…¶ä»–åŸºäºè§„åˆ™æˆ–å­¦ä¹ çš„å¥–åŠ±ï¼›4) å„ç§è¶…å‚æ•°çš„è®¾ç½®ï¼Œå¦‚ç†µè°ƒåˆ¶ç³»æ•°ã€æœªæ¥æ¸…æ™°åº¦å¥–åŠ±ç³»æ•°ç­‰ï¼Œè¿™äº›è¶…å‚æ•°éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEMPGåœ¨WebShopã€ALFWorldå’ŒDeep Searchä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„Agentä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨WebShopä»»åŠ¡ä¸Šï¼ŒEMPGçš„æˆåŠŸç‡æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†è¶…è¿‡20%ã€‚è¿™äº›ç»“æœè¯æ˜äº†EMPGçš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæ˜¾è‘—æå‡LLM Agentåœ¨é•¿æ—¶ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EMPGå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦é•¿æ—¶è§„åˆ’å’Œå†³ç­–çš„ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ã€ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜Agentåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡å’Œç¨³å®šæ€§ï¼ŒEMPGå¯ä»¥å¸®åŠ©å¼€å‘æ›´æ™ºèƒ½ã€æ›´å¯é çš„AIç³»ç»Ÿï¼Œä»è€Œæå‡ç”Ÿäº§æ•ˆç‡ï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/

