---
layout: default
title: Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models
---

# Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09119" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09119v1</a>
  <a href="https://arxiv.org/pdf/2509.09119.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09119v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09119v1', 'Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hao Zhang, Bo Huang, Zhenjia Li, Xi Xiao, Hui Yi Leong, Zumeng Zhang, Xinwei Long, Tianyang Wang, Hao Xu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: 15 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSensitivity-LoRAï¼ŒåŸºäºæ•æ„Ÿåº¦åŠ¨æ€è°ƒæ•´LoRAç§©ä»¥é«˜æ•ˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ä½ç§©é€‚åº”` `HessiançŸ©é˜µ` `æƒé‡æ•æ„Ÿåº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LoRAæ–¹æ³•å¯¹æ‰€æœ‰æƒé‡çŸ©é˜µé‡‡ç”¨ç»Ÿä¸€ç§©åˆ†é…ï¼Œå¿½ç•¥äº†ä¸åŒæƒé‡çš„é‡è¦æ€§å·®å¼‚ï¼Œå¯¼è‡´å¾®è°ƒæ•ˆç‡é™ä½ã€‚
2. Sensitivity-LoRAåŸºäºHessiançŸ©é˜µè®¡ç®—æƒé‡æ•æ„Ÿåº¦ï¼Œå¹¶æ®æ­¤åŠ¨æ€åˆ†é…LoRAç§©ï¼Œå®ç°æ›´é«˜æ•ˆçš„å‚æ•°åˆ©ç”¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSensitivity-LoRAåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€æ•ˆç‡å’Œç¨³å®šæ€§ï¼Œä¼˜äºç°æœ‰LoRAå˜ä½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»æ”¹å˜äº†æ—¥å¸¸ç”Ÿæ´»å’Œç§‘å­¦ç ”ç©¶ã€‚ç„¶è€Œï¼Œå°†LLMsä»é€šç”¨æ¨¡å‹è°ƒæ•´åˆ°ä¸“é—¨ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ä¸­çš„ä¸€ç§çªå‡ºæ–¹æ³•ï¼Œå®ƒé€šè¿‡ä½¿ç”¨ä½ç§©åˆ†è§£æ¥è¿‘ä¼¼æ¨¡å‹æƒé‡æ›´æ–°ï¼Œä»è€Œæˆä¸ºLLMsçš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼ŒLoRAå—åˆ°å…¶å¯¹æ¯ä¸ªå¢é‡çŸ©é˜µçš„ç»Ÿä¸€ç§©ï¼ˆrï¼‰åˆ†é…çš„é™åˆ¶ï¼Œå¹¶ä¸”æ—¨åœ¨è§£å†³æ­¤é—®é¢˜çš„ç°æœ‰ç§©åˆ†é…æŠ€æœ¯ä»ç„¶è®¡ç®—æ•ˆç‡ä½ä¸‹ã€å¤æ‚ä¸”ä¸ç¨³å®šï¼Œä»è€Œé˜»ç¢äº†å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•Sensitivity-LoRAï¼Œè¯¥æ–¹æ³•åŸºäºæƒé‡çŸ©é˜µçš„å…¨å±€å’Œå±€éƒ¨æ•æ„Ÿæ€§åŠ¨æ€åœ°å°†ç§©åˆ†é…ç»™æƒé‡çŸ©é˜µã€‚å®ƒåˆ©ç”¨æŸå¤±å‡½æ•°çš„äºŒé˜¶å¯¼æ•°ï¼ˆHessiançŸ©é˜µï¼‰æ¥æœ‰æ•ˆåœ°æ•è·æƒé‡æ•æ„Ÿæ€§ï¼Œä»è€Œä»¥æœ€å°çš„è®¡ç®—å¼€é”€å®ç°æœ€ä½³çš„ç§©åˆ†é…ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSensitivity-LoRAåœ¨å„ç§ä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­å…·æœ‰å¼ºå¤§çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œç¨³å®šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰LoRAæ–¹æ³•åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¯¹æ‰€æœ‰æƒé‡çŸ©é˜µé‡‡ç”¨ç›¸åŒçš„ç§©ï¼ˆrankï¼‰ï¼Œå¿½ç•¥äº†ä¸åŒæƒé‡çŸ©é˜µå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ç¨‹åº¦ä¸åŒã€‚è¿™ç§ç»Ÿä¸€çš„ç§©åˆ†é…æ–¹å¼å¯¼è‡´å‚æ•°åˆ©ç”¨ç‡ä¸é«˜ï¼Œå½±å“äº†å¾®è°ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚ç°æœ‰çš„ç§©åˆ†é…æŠ€æœ¯è®¡ç®—å¤æ‚åº¦é«˜ï¼Œå®ç°å›°éš¾ï¼Œä¸”ç¨³å®šæ€§ä¸è¶³ï¼Œéš¾ä»¥å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSensitivity-LoRAçš„æ ¸å¿ƒæ€è·¯æ˜¯æ ¹æ®æƒé‡çŸ©é˜µçš„æ•æ„Ÿåº¦åŠ¨æ€åœ°åˆ†é…LoRAçš„ç§©ã€‚æ•æ„Ÿåº¦é«˜çš„æƒé‡çŸ©é˜µåˆ†é…æ›´é«˜çš„ç§©ï¼Œä»¥ä¾¿æ›´ç²¾ç»†åœ°è°ƒæ•´ï¼›æ•æ„Ÿåº¦ä½çš„æƒé‡çŸ©é˜µåˆ†é…è¾ƒä½çš„ç§©ï¼Œä»¥å‡å°‘è®¡ç®—é‡ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å‚æ•°ï¼Œæé«˜å¾®è°ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSensitivity-LoRAçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„LLMåˆå§‹åŒ–æ¨¡å‹ï¼›2) åœ¨LoRAä¸­ï¼Œä¸ºæ¯ä¸ªæƒé‡çŸ©é˜µæ·»åŠ ä½ç§©çŸ©é˜µï¼›3) è®¡ç®—æŸå¤±å‡½æ•°å…³äºæƒé‡çŸ©é˜µçš„äºŒé˜¶å¯¼æ•°ï¼ˆHessiançŸ©é˜µï¼‰ï¼Œä»¥æ­¤è¯„ä¼°æƒé‡çŸ©é˜µçš„æ•æ„Ÿåº¦ï¼›4) æ ¹æ®æ•æ„Ÿåº¦åŠ¨æ€åœ°åˆ†é…LoRAçš„ç§©ï¼›5) ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰ä¼˜åŒ–ç®—æ³•å¾®è°ƒLoRAå‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šSensitivity-LoRAçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨HessiançŸ©é˜µæ¥è¯„ä¼°æƒé‡çŸ©é˜µçš„æ•æ„Ÿåº¦ï¼Œå¹¶æ®æ­¤åŠ¨æ€åˆ†é…LoRAçš„ç§©ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSensitivity-LoRAèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ•æ‰æƒé‡çŸ©é˜µçš„é‡è¦æ€§ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„å‚æ•°åˆ©ç”¨ã€‚æ­¤å¤–ï¼ŒSensitivity-LoRAçš„è®¡ç®—å¤æ‚åº¦è¾ƒä½ï¼Œæ˜“äºå®ç°ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šSensitivity-LoRAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨HessiançŸ©é˜µçš„å¯¹è§’çº¿å…ƒç´ ä½œä¸ºæƒé‡çŸ©é˜µæ•æ„Ÿåº¦çš„ä¼°è®¡ï¼›2) è®¾è®¡äº†ä¸€ç§ç§©åˆ†é…ç­–ç•¥ï¼Œæ ¹æ®æ•æ„Ÿåº¦å°†LoRAçš„ç§©åˆ†é…ç»™ä¸åŒçš„æƒé‡çŸ©é˜µï¼›3) ä½¿ç”¨AdamWä¼˜åŒ–å™¨å¾®è°ƒLoRAå‚æ•°ï¼Œå¹¶é‡‡ç”¨å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°ä¸æ ‡å‡†LoRAç›¸åŒï¼Œå³ä¸‹æ¸¸ä»»åŠ¡çš„æŸå¤±å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSensitivity-LoRAåœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šä¼˜äºä¼ ç»Ÿçš„LoRAæ–¹æ³•å’Œå…¶ä»–å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨GLUEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSensitivity-LoRAåœ¨ä¿æŒç›¸ä¼¼å‚æ•°é‡çš„æƒ…å†µä¸‹ï¼Œå¹³å‡æ€§èƒ½æå‡äº†1-2ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼ŒSensitivity-LoRAè¿˜è¡¨ç°å‡ºæ›´å¥½çš„è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Sensitivity-LoRAé€‚ç”¨äºèµ„æºå—é™åœºæ™¯ä¸‹çš„å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œä¾‹å¦‚åœ¨è¾¹ç¼˜è®¾å¤‡æˆ–ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²LLMã€‚è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆé™ä½å¾®è°ƒæ‰€éœ€çš„è®¡ç®—èµ„æºå’Œå­˜å‚¨ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½çš„æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSensitivity-LoRAè¿˜å¯ä»¥åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have transformed both everyday life and scientific research. However, adapting LLMs from general-purpose models to specialized tasks remains challenging, particularly in resource-constrained environments. Low-Rank Adaptation (LoRA), a prominent method within Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to LLMs by approximating model weight updates using low-rank decomposition. However, LoRA is limited by its uniform rank ( r ) allocation to each incremental matrix, and existing rank allocation techniques aimed at addressing this issue remain computationally inefficient, complex, and unstable, hindering practical applications. To address these limitations, we propose Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates ranks to weight matrices based on both their global and local sensitivities. It leverages the second-order derivatives (Hessian Matrix) of the loss function to effectively capture weight sensitivity, enabling optimal rank allocation with minimal computational overhead. Our experimental results have demonstrated robust effectiveness, efficiency and stability of Sensitivity-LoRA across diverse tasks and benchmarks.

