---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.LG - 2025-09-11
---

# cs.LGï¼ˆ2025-09-11ï¼‰

ğŸ“Š å…± **17** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (11)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250909655v1-feasibility-guided-fair-adaptive-offline-reinforcement-learning-for-.html">Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management</a></td>
  <td>æå‡ºå¯è¡Œæ€§å¼•å¯¼çš„å…¬å¹³è‡ªé€‚åº”ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œç”¨äºæ”¹å–„åŒ»ç–—è¡¥åŠ©è®¡åˆ’ç®¡ç†ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">offline RL</span> <span class="paper-tag">offline reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09655v1" data-paper-url="./papers/250909655v1-feasibility-guided-fair-adaptive-offline-reinforcement-learning-for-.html" onclick="toggleFavorite(this, '2509.09655v1', 'Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250909772v1-hybrid-adaptive-conformal-offline-reinforcement-learning-for-fair-po.html">Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management</a></td>
  <td>æå‡ºæ··åˆè‡ªé€‚åº”ä¿å½¢ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶HACOï¼Œç”¨äºå…¬å¹³çš„äººç¾¤å¥åº·ç®¡ç†ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">offline RL</span> <span class="paper-tag">offline reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09772v1" data-paper-url="./papers/250909772v1-hybrid-adaptive-conformal-offline-reinforcement-learning-for-fair-po.html" onclick="toggleFavorite(this, '2509.09772v1', 'Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250909219v1-vejde-a-framework-for-inductive-deep-reinforcement-learning-based-on.html">Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement</a></td>
  <td>æå‡ºVejdeæ¡†æ¶ä»¥è§£å†³å¤æ‚çŠ¶æ€ä¸‹çš„å†³ç­–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09219v1" data-paper-url="./papers/250909219v1-vejde-a-framework-for-inductive-deep-reinforcement-learning-based-on.html" onclick="toggleFavorite(this, '2509.09219v1', 'Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250909176v2-quantum-enhanced-forecasting-for-deep-reinforcement-learning-in-algo.html">Quantum-Enhanced Forecasting for Deep Reinforcement Learning in Algorithmic Trading</a></td>
  <td>æå‡ºåŸºäºé‡å­å¢å¼ºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç®—æ³•äº¤æ˜“æ–¹æ³•ï¼Œå®ç°å¤–æ±‡äº¤æ˜“å›æŠ¥ç‡æå‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09176v2" data-paper-url="./papers/250909176v2-quantum-enhanced-forecasting-for-deep-reinforcement-learning-in-algo.html" onclick="toggleFavorite(this, '2509.09176v2', 'Quantum-Enhanced Forecasting for Deep Reinforcement Learning in Algorithmic Trading')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250909838v1-revisiting-actor-critic-methods-in-discrete-action-off-policy-reinfo.html">Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning</a></td>
  <td>è§£è€¦Actor-Criticç†µæ­£åˆ™åŒ–ï¼Œæå‡ç¦»æ•£åŠ¨ä½œç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">PPO</span> <span class="paper-tag">SAC</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09838v1" data-paper-url="./papers/250909838v1-revisiting-actor-critic-methods-in-discrete-action-off-policy-reinfo.html" onclick="toggleFavorite(this, '2509.09838v1', 'Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250909751v1-meta-learning-reinforcement-learning-for-crypto-return-prediction.html">Meta-Learning Reinforcement Learning for Crypto-Return Prediction</a></td>
  <td>æå‡ºMeta-RL-Cryptoï¼Œç”¨äºåŠ å¯†è´§å¸æ”¶ç›Šé¢„æµ‹çš„è‡ªæå‡äº¤æ˜“Agent</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09751v1" data-paper-url="./papers/250909751v1-meta-learning-reinforcement-learning-for-crypto-return-prediction.html" onclick="toggleFavorite(this, '2509.09751v1', 'Meta-Learning Reinforcement Learning for Crypto-Return Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250909265v1-harnessing-uncertainty-entropy-modulated-policy-gradients-for-long-h.html">Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents</a></td>
  <td>æå‡ºç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦(EMPG)ä»¥æå‡LLM Agentåœ¨é•¿æ—¶ä»»åŠ¡ä¸­çš„è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">inverse reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09265v1" data-paper-url="./papers/250909265v1-harnessing-uncertainty-entropy-modulated-policy-gradients-for-long-h.html" onclick="toggleFavorite(this, '2509.09265v1', 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250909135v2-continuous-time-value-iteration-for-multi-agent-reinforcement-learni.html">Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„è¿ç»­æ—¶é—´å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè§£å†³é«˜ç»´åŠ¨åŠ›ç³»ç»Ÿä¸­çš„ç­–ç•¥è®­ç»ƒé—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09135v2" data-paper-url="./papers/250909135v2-continuous-time-value-iteration-for-multi-agent-reinforcement-learni.html" onclick="toggleFavorite(this, '2509.09135v2', 'Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250909550v2-finite-scalar-quantization-enables-redundant-and-transmission-robust.html">Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates</a></td>
  <td>NeuCodecï¼šåŸºäºæœ‰é™æ ‡é‡é‡åŒ–çš„é²æ£’æ€§ç¥ç»éŸ³é¢‘å‹ç¼©ç¼–ç </td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09550v2" data-paper-url="./papers/250909550v2-finite-scalar-quantization-enables-redundant-and-transmission-robust.html" onclick="toggleFavorite(this, '2509.09550v2', 'Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250909208v1-incentivizing-safer-actions-in-policy-optimization-for-constrained-r.html">Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning</a></td>
  <td>æå‡ºIP3Oç®—æ³•ä»¥è§£å†³çº¦æŸå¼ºåŒ–å­¦ä¹ ä¸­çš„å®‰å…¨æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09208v1" data-paper-url="./papers/250909208v1-incentivizing-safer-actions-in-policy-optimization-for-constrained-r.html" onclick="toggleFavorite(this, '2509.09208v1', 'Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250909177v3-clip-your-sequences-fairly-enforcing-length-fairness-for-sequence-le.html">Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL</a></td>
  <td>æå‡ºFSPOï¼Œé€šè¿‡é•¿åº¦å…¬å¹³çš„è£å‰ªè§£å†³åºåˆ—çº§å¼ºåŒ–å­¦ä¹ ä¸­çš„é•¿åº¦åå·®é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">PPO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09177v3" data-paper-url="./papers/250909177v3-clip-your-sequences-fairly-enforcing-length-fairness-for-sequence-le.html" onclick="toggleFavorite(this, '2509.09177v3', 'Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250918127v2-safe-sail-towards-a-fine-grained-safety-landscape-of-large-language-.html">Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework</a></td>
  <td>Safe-SAILï¼šé€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨è§£é‡Šæ¡†æ¶å®ç°å¤§è¯­è¨€æ¨¡å‹ç»†ç²’åº¦å®‰å…¨åˆ†æ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18127v2" data-paper-url="./papers/250918127v2-safe-sail-towards-a-fine-grained-safety-landscape-of-large-language-.html" onclick="toggleFavorite(this, '2509.18127v2', 'Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250909119v1-sensitivity-lora-low-load-sensitivity-based-fine-tuning-for-large-la.html">Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models</a></td>
  <td>æå‡ºSensitivity-LoRAï¼ŒåŸºäºæ•æ„Ÿåº¦åŠ¨æ€è°ƒæ•´LoRAç§©ä»¥é«˜æ•ˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09119v1" data-paper-url="./papers/250909119v1-sensitivity-lora-low-load-sensitivity-based-fine-tuning-for-large-la.html" onclick="toggleFavorite(this, '2509.09119v1', 'Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250909864v1-latency-and-token-aware-test-time-compute.html">Latency and Token-Aware Test-Time Compute</a></td>
  <td>æå‡ºä¸€ç§å»¶è¿Ÿå’ŒTokenæ„ŸçŸ¥çš„æµ‹è¯•æ—¶è®¡ç®—åŠ¨æ€åˆ†é…æ¡†æ¶ï¼Œä¼˜åŒ–LLMæ¨ç†ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09864v1" data-paper-url="./papers/250909864v1-latency-and-token-aware-test-time-compute.html" onclick="toggleFavorite(this, '2509.09864v1', 'Latency and Token-Aware Test-Time Compute')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250909782v1-one-head-many-models-cross-attention-routing-for-cost-aware-llm-sele.html">One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection</a></td>
  <td>æå‡ºåŸºäºäº¤å‰æ³¨æ„åŠ›è·¯ç”±çš„LLMé€‰æ‹©æ¡†æ¶ï¼Œå®ç°æˆæœ¬æ•ˆç›Šä¼˜åŒ–ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09782v1" data-paper-url="./papers/250909782v1-one-head-many-models-cross-attention-routing-for-cost-aware-llm-sele.html" onclick="toggleFavorite(this, '2509.09782v1', 'One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250909679v2-butterflyquant-ultra-low-bit-llm-quantization-through-learnable-orth.html">ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms</a></td>
  <td>æå‡ºButterflyQuantä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é‡åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09679v2" data-paper-url="./papers/250909679v2-butterflyquant-ultra-low-bit-llm-quantization-through-learnable-orth.html" onclick="toggleFavorite(this, '2509.09679v2', 'ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250909597v2-graph-alignment-via-dual-pass-spectral-encoding-and-latent-space-com.html">Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication</a></td>
  <td>æå‡ºåŒé€šé“è°±ç¼–ç ä¸æ½œåœ¨ç©ºé—´é€šä¿¡çš„å›¾å¯¹é½æ¡†æ¶ï¼Œæå‡èŠ‚ç‚¹åŒºåˆ†æ€§å¹¶ä¿è¯å‡ ä½•ä¸€è‡´æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09597v2" data-paper-url="./papers/250909597v2-graph-alignment-via-dual-pass-spectral-encoding-and-latent-space-com.html" onclick="toggleFavorite(this, '2509.09597v2', 'Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.LG é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)