---
layout: default
title: Harnessing Discrete Representations For Continual Reinforcement Learning
---

# Harnessing Discrete Representations For Continual Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.01203" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.01203v3</a>
  <a href="https://arxiv.org/pdf/2312.01203.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.01203v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.01203v3', 'Harnessing Discrete Representations For Continual Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Edan Meyer, Adam White, Marlos C. Machado

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2023-12-02 (æ›´æ–°: 2024-07-13)

**å¤‡æ³¨**: 23 pages, 16 figures, accepted to RLC 2024

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨ç¦»æ•£è¡¨ç¤ºæå‡æŒç»­å¼ºåŒ–å­¦ä¹ æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç¦»æ•£è¡¨ç¤º` `æŒç»­å­¦ä¹ ` `ä¸–ç•Œæ¨¡å‹` `è¡¨ç¤ºå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–è¿ç»­è§‚æµ‹è¡¨ç¤ºï¼Œä½†å…¶åœ¨é«˜ç»´å¤æ‚ç¯å¢ƒä¸‹çš„å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›é¢ä¸´æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºä½¿ç”¨ç¦»æ•£è¡¨ç¤ºæ¥ç¼–ç ç¯å¢ƒè§‚æµ‹ï¼Œæ—¨åœ¨æå‡å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡å’Œé€‚åº”æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç¦»æ•£è¡¨ç¤ºèƒ½æå‡ä¸–ç•Œæ¨¡å‹å­¦ä¹ çš„å‡†ç¡®æ€§ï¼Œå¹¶ä½¿æ™ºèƒ½ä½“åœ¨æŒç»­å¼ºåŒ–å­¦ä¹ ä¸­æ›´å¿«åœ°é€‚åº”æ–°ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“ä»…æ ¹æ®ç¯å¢ƒè§‚æµ‹åšå‡ºå†³ç­–ï¼Œå› æ­¤ä¸¥é‡ä¾èµ–äºè§‚æµ‹çš„è¡¨ç¤ºã€‚å°½ç®¡æœ€è¿‘çš„ä¸€äº›çªç ´ä½¿ç”¨äº†åŸºäºå‘é‡çš„åˆ†ç±»è§‚æµ‹è¡¨ç¤ºï¼ˆé€šå¸¸ç§°ä¸ºç¦»æ•£è¡¨ç¤ºï¼‰ï¼Œä½†å¾ˆå°‘æœ‰å·¥ä½œæ˜ç¡®è¯„ä¼°è¿™ç§é€‰æ‹©çš„é‡è¦æ€§ã€‚æœ¬æ–‡å¯¹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä½¿ç”¨ç¦»æ•£è¡¨ç¤ºè§‚æµ‹çš„ä¼˜åŠ¿è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬åœ¨ä¸–ç•Œæ¨¡å‹å­¦ä¹ ã€æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä»¥åŠæœ€ç»ˆçš„æŒç»­å¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå…¶ä¼˜åŠ¿ä¸é—®é¢˜è®¾ç½®çš„éœ€æ±‚æœ€ä¸ºå»åˆã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸ä¼ ç»Ÿçš„è¿ç»­è¡¨ç¤ºç›¸æ¯”ï¼ŒåŸºäºç¦»æ•£è¡¨ç¤ºå­¦ä¹ çš„ä¸–ç•Œæ¨¡å‹èƒ½å¤Ÿä»¥æ›´å°çš„å®¹é‡æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿä¸–ç•Œï¼Œå¹¶ä¸”ä½¿ç”¨ç¦»æ•£è¡¨ç¤ºè®­ç»ƒçš„æ™ºèƒ½ä½“èƒ½å¤Ÿä»¥æ›´å°‘çš„æ•°æ®å­¦ä¹ æ›´å¥½çš„ç­–ç•¥ã€‚åœ¨æŒç»­å¼ºåŒ–å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œè¿™äº›ä¼˜åŠ¿è½¬åŒ–ä¸ºæ›´å¿«çš„é€‚åº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè§‚å¯Ÿåˆ°çš„æ€§èƒ½æ”¹è¿›å¯å½’å› äºæ½œåœ¨å‘é‡ä¸­åŒ…å«çš„ä¿¡æ¯ä»¥åŠç¦»æ•£è¡¨ç¤ºæœ¬èº«çš„ç¼–ç ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­å­¦ä¹ æ—¶ï¼Œéœ€è¦ä»é«˜ç»´è¿ç»­è§‚æµ‹ä¸­æå–æœ‰æ•ˆä¿¡æ¯ã€‚ä¼ ç»Ÿçš„è¿ç»­è¡¨ç¤ºæ–¹æ³•å¯èƒ½å¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦å¿«é€Ÿé€‚åº”ä¸æ–­å˜åŒ–çš„ç¯å¢ƒã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨å®¹é‡ã€å‡†ç¡®æ€§å’Œé€‚åº”æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä½¿ç”¨ç¦»æ•£è¡¨ç¤ºæ¥ç¼–ç ç¯å¢ƒè§‚æµ‹ã€‚é€šè¿‡å°†è¿ç»­è§‚æµ‹è½¬æ¢ä¸ºç¦»æ•£çš„ç±»åˆ«å‘é‡ï¼Œå¯ä»¥é™ä½è¡¨ç¤ºçš„ç»´åº¦ï¼Œå‡å°‘å†—ä½™ä¿¡æ¯ï¼Œå¹¶å¯èƒ½æ›´å¥½åœ°æ•æ‰ç¯å¢ƒçš„å…³é”®ç‰¹å¾ã€‚è¿™ç§ç¦»æ•£åŒ–è¿‡ç¨‹å¯ä»¥ç®€åŒ–å­¦ä¹ ä»»åŠ¡ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ï¼Œå¹¶å¢å¼ºæ™ºèƒ½ä½“çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) è§‚æµ‹ç¼–ç å™¨ï¼Œå°†è¿ç»­è§‚æµ‹è½¬æ¢ä¸ºç¦»æ•£è¡¨ç¤ºï¼›2) å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œåˆ©ç”¨ç¦»æ•£è¡¨ç¤ºè¿›è¡Œç­–ç•¥å­¦ä¹ ï¼›3) è¯„ä¼°æ¨¡å—ï¼Œåœ¨ä¸åŒçš„ç¯å¢ƒå’Œä»»åŠ¡ä¸Šè¯„ä¼°æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚å…·ä½“æµç¨‹æ˜¯ï¼Œæ™ºèƒ½ä½“ä»ç¯å¢ƒä¸­æ¥æ”¶è¿ç»­è§‚æµ‹ï¼Œé€šè¿‡ç¼–ç å™¨å°†å…¶è½¬æ¢ä¸ºç¦»æ•£è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚Q-learningæˆ–ç­–ç•¥æ¢¯åº¦ï¼‰å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œæœ€ååœ¨ä¸åŒçš„ä»»åŠ¡ä¸Šè¯„ä¼°æ™ºèƒ½ä½“çš„å­¦ä¹ æ•ˆæœå’Œé€‚åº”èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†ç¦»æ•£è¡¨ç¤ºå¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶è¯æ˜å…¶åœ¨ä¸–ç•Œæ¨¡å‹å­¦ä¹ å’ŒæŒç»­å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¼˜åŠ¿ã€‚ä¸ä¼ ç»Ÿçš„è¿ç»­è¡¨ç¤ºç›¸æ¯”ï¼Œç¦»æ•£è¡¨ç¤ºèƒ½å¤Ÿä»¥æ›´å°çš„å®¹é‡æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿä¸–ç•Œï¼Œå¹¶ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿä»¥æ›´å°‘çš„æ•°æ®å­¦ä¹ æ›´å¥½çš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•ä¸ºè§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å¯èƒ½æ¶‰åŠçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç¦»æ•£ç¼–ç å™¨çš„è®¾è®¡ï¼Œä¾‹å¦‚ä½¿ç”¨è‡ªç¼–ç å™¨æˆ–èšç±»ç®—æ³•å°†è¿ç»­è§‚æµ‹æ˜ å°„åˆ°ç¦»æ•£ç±»åˆ«ï¼›2) ç¦»æ•£è¡¨ç¤ºçš„ç»´åº¦é€‰æ‹©ï¼Œéœ€è¦åœ¨è¡¨ç¤ºèƒ½åŠ›å’Œè®¡ç®—å¤æ‚åº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼›3) å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©å’Œè°ƒæ•´ï¼Œä»¥é€‚åº”ç¦»æ•£è¡¨ç¤ºçš„ç‰¹ç‚¹ï¼›4) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œä¾‹å¦‚ä½¿ç”¨äº¤å‰ç†µæŸå¤±æ¥è®­ç»ƒç¦»æ•£ç¼–ç å™¨ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æŸå¤±æ¥ä¼˜åŒ–ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºç¦»æ•£è¡¨ç¤ºçš„ä¸–ç•Œæ¨¡å‹èƒ½å¤Ÿä»¥æ›´å°çš„å®¹é‡æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿä¸–ç•Œã€‚ä¸ä½¿ç”¨è¿ç»­è¡¨ç¤ºçš„æ™ºèƒ½ä½“ç›¸æ¯”ï¼Œä½¿ç”¨ç¦»æ•£è¡¨ç¤ºè®­ç»ƒçš„æ™ºèƒ½ä½“èƒ½å¤Ÿä»¥æ›´å°‘çš„æ•°æ®å­¦ä¹ æ›´å¥½çš„ç­–ç•¥ã€‚åœ¨æŒç»­å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨ç¦»æ•£è¡¨ç¤ºçš„æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¿«åœ°é€‚åº”æ–°çš„ä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡ä½¿ç”¨ç¦»æ•£è¡¨ç¤ºï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´æœ‰æ•ˆåœ°å­¦ä¹ å’Œé€‚åº”å¤æ‚ç¯å¢ƒï¼Œä»è€Œæé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºé™ä½è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼Œä½¿å…¶æ›´é€‚ç”¨äºèµ„æºå—é™çš„è®¾å¤‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) agents make decisions using nothing but observations from the environment, and consequently, heavily rely on the representations of those observations. Though some recent breakthroughs have used vector-based categorical representations of observations, often referred to as discrete representations, there is little work explicitly assessing the significance of such a choice. In this work, we provide a thorough empirical investigation of the advantages of representing observations as vectors of categorical values within the context of reinforcement learning. We perform evaluations on world-model learning, model-free RL, and ultimately continual RL problems, where the benefits best align with the needs of the problem setting. We find that, when compared to traditional continuous representations, world models learned over discrete representations accurately model more of the world with less capacity, and that agents trained with discrete representations learn better policies with less data. In the context of continual RL, these benefits translate into faster adapting agents. Additionally, our analysis suggests that the observed performance improvements can be attributed to the information contained within the latent vectors and potentially the encoding of the discrete representation itself.

