---
layout: default
title: A Survey of Temporal Credit Assignment in Deep Reinforcement Learning
---

# A Survey of Temporal Credit Assignment in Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.01072" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.01072v2</a>
  <a href="https://arxiv.org/pdf/2312.01072.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.01072v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.01072v2', 'A Survey of Temporal Credit Assignment in Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eduardo Pignatelli, Johan Ferret, Matthieu Geist, Thomas Mesnard, Hado van Hasselt, Olivier Pietquin, Laura Toni

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2023-12-02 (æ›´æ–°: 2024-07-04)

**å¤‡æ³¨**: 56 pages, 2 figures, 4 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­æ—¶é—´ä¿¡ç”¨åˆ†é…é—®é¢˜ç»¼è¿°ï¼šå½¢å¼åŒ–ã€æŒ‘æˆ˜ä¸è¯„ä¼°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `ä¿¡ç”¨åˆ†é…é—®é¢˜` `æ—¶é—´ä¿¡ç”¨åˆ†é…` `ç»¼è¿°` `å½¢å¼åŒ–` `å»¶è¿Ÿå¥–åŠ±` `ç¨€ç–å¥–åŠ±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜æ˜¯è¿æ¥åŠ¨ä½œä¸é•¿æœŸç»“æœçš„å…³é”®æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†å»¶è¿Ÿã€å™ªå£°å’Œç¨€ç–åé¦ˆã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ä¿¡ç”¨å½¢å¼åŒ–æ–¹æ³•ï¼Œç”¨äºå…¬å¹³æ¯”è¾ƒç°æœ‰ç®—æ³•ï¼Œå¹¶åˆ†æä¸åŒæ–¹æ³•ä¹‹é—´çš„æƒè¡¡ã€‚
3. è¯¥ç»¼è¿°è®¨è®ºäº†å»¶è¿Ÿæ•ˆåº”ã€è½¬ç½®å’Œç¼ºä¹è¡ŒåŠ¨å½±å“ç­‰æŒ‘æˆ˜ï¼Œå¹¶åˆ†æäº†ç°æœ‰æ–¹æ³•å¦‚ä½•åº”å¯¹ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼ˆCAPï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“é¢ä¸´çš„é•¿æœŸæŒ‘æˆ˜ï¼Œå®ƒæ¶‰åŠå°†åŠ¨ä½œä¸å…¶é•¿æœŸåæœè”ç³»èµ·æ¥ã€‚è§£å†³CAPæ˜¯RLæˆåŠŸåº”ç”¨äºç°å®ä¸–ç•Œçš„å…³é”®ä¸€æ­¥ï¼Œå› ä¸ºå¤§å¤šæ•°å†³ç­–é—®é¢˜æä¾›çš„åé¦ˆéƒ½æ˜¯å˜ˆæ‚çš„ã€å»¶è¿Ÿçš„ï¼Œå¹¶ä¸”å‡ ä¹æ²¡æœ‰æˆ–æ ¹æœ¬æ²¡æœ‰å…³äºåŸå› çš„ä¿¡æ¯ã€‚è¿™äº›æ¡ä»¶ä½¿å¾—åŒºåˆ†å¶ç„¶çš„ç»“æœå’Œç”±çŸ¥æƒ…å†³ç­–å¯¼è‡´çš„ç»“æœå˜å¾—å›°éš¾ã€‚ç„¶è€Œï¼Œä¿¡ç”¨çš„æ•°å­¦æœ¬è´¨å’ŒCAPä»ç„¶ç¼ºä¹å……åˆ†çš„ç†è§£å’Œå®šä¹‰ã€‚æœ¬ç»¼è¿°å›é¡¾äº†æ·±åº¦RLä¸­æ—¶é—´ä¿¡ç”¨åˆ†é…ï¼ˆCAï¼‰çš„æœ€æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬ä¸ºä¿¡ç”¨æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å½¢å¼åŒ–æ–¹æ³•ï¼Œå¯ä»¥å¯¹æœ€å…ˆè¿›çš„ç®—æ³•è¿›è¡Œå…¬å¹³çš„æ¯”è¾ƒï¼Œå¹¶æé«˜æˆ‘ä»¬å¯¹å„ç§æ–¹æ³•ä¹‹é—´æƒè¡¡çš„ç†è§£ã€‚æˆ‘ä»¬å°†CAPè§†ä¸ºä»æœ‰é™çš„ç»éªŒä¸­å­¦ä¹ åŠ¨ä½œå¯¹ç»“æœçš„å½±å“çš„é—®é¢˜ã€‚æˆ‘ä»¬è®¨è®ºäº†å»¶è¿Ÿæ•ˆåº”ã€è½¬ç½®å’Œç¼ºä¹è¡ŒåŠ¨å½±å“æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¹¶åˆ†æäº†ç°æœ‰æ–¹æ³•å¦‚ä½•è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æœ€åï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†è¯„ä¼°ä¿¡ç”¨åˆ†é…æ–¹æ³•çš„åè®®ï¼Œå¹¶æå‡ºäº†è¯Šæ–­ä¸åŒæ–¹æ³•æŒ£æ‰æ¥æºçš„æ–¹æ³•ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬ç»¼è¿°ä¸ºæ–°å…¥é—¨çš„ä»ä¸šè€…å’Œç ”ç©¶äººå‘˜æä¾›äº†è¯¥é¢†åŸŸçš„æ¦‚è¿°ï¼Œä¸ºå¸Œæœ›åŠ å¿«CAPæ–°ç ”ç©¶çš„èµ·å§‹é˜¶æ®µçš„å­¦è€…æä¾›äº†ä¸€ä¸ªè¿è´¯çš„è§†è§’ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æå‡ºäº†æ½œåœ¨çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¿¡ç”¨åˆ†é…é—®é¢˜ï¼ˆCAPï¼‰æ—¨åœ¨ç¡®å®šå“ªäº›è¿‡å»çš„åŠ¨ä½œå¯¹å½“å‰çš„ç»“æœè´Ÿè´£ã€‚åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç”±äºå»¶è¿Ÿå¥–åŠ±ã€ç¨€ç–å¥–åŠ±å’Œéé©¬å°”å¯å¤«ç¯å¢ƒç­‰å› ç´ ï¼Œè¿™ä¸ªé—®é¢˜å˜å¾—å°¤ä¸ºå›°éš¾ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜æ—¶é¢ä¸´è¯¸å¤šé™åˆ¶ï¼Œä¾‹å¦‚å¯¹ç‰¹å®šç¯å¢ƒçš„è¿‡åº¦æ‹Ÿåˆï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œä»¥åŠç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ ‡å‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥ç»¼è¿°çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä¿¡ç”¨åˆ†é…é—®é¢˜å½¢å¼åŒ–ä¸ºä¸€ä¸ªå­¦ä¹ åŠ¨ä½œå¯¹ç»“æœå½±å“çš„é—®é¢˜ã€‚é€šè¿‡å»ºç«‹ç»Ÿä¸€çš„ä¿¡ç”¨åº¦é‡æ ‡å‡†ï¼Œå¯ä»¥å¯¹ä¸åŒçš„ä¿¡ç”¨åˆ†é…ç®—æ³•è¿›è¡Œå…¬å¹³æ¯”è¾ƒï¼Œå¹¶æ·±å…¥ç†è§£å®ƒä»¬ä¹‹é—´çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼ºè°ƒäº†è¯Šæ–­ä¸åŒæ–¹æ³•åœ¨ç‰¹å®šåœºæ™¯ä¸‹è¡¨ç°ä¸ä½³çš„åŸå› çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç»¼è¿°é¦–å…ˆå®šä¹‰äº†ä¿¡ç”¨çš„æ¦‚å¿µï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å½¢å¼åŒ–æ¡†æ¶ã€‚ç„¶åï¼Œå®ƒå›é¡¾äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ç°æœ‰çš„ä¿¡ç”¨åˆ†é…æ–¹æ³•ï¼Œå¹¶æ ¹æ®å…¶è§£å†³é—®é¢˜çš„ç­–ç•¥è¿›è¡Œåˆ†ç±»ã€‚æ¥ä¸‹æ¥ï¼Œè®ºæ–‡è®¨è®ºäº†å»¶è¿Ÿæ•ˆåº”ã€è½¬ç½®å’Œç¼ºä¹è¡ŒåŠ¨å½±å“ç­‰æŒ‘æˆ˜ï¼Œå¹¶åˆ†æäº†ç°æœ‰æ–¹æ³•å¦‚ä½•åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚æœ€åï¼Œè®ºæ–‡è°ƒæŸ¥äº†è¯„ä¼°ä¿¡ç”¨åˆ†é…æ–¹æ³•çš„åè®®ï¼Œå¹¶æå‡ºäº†è¯Šæ–­ä¸åŒæ–¹æ³•æŒ£æ‰æ¥æºçš„æ–¹æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç»¼è¿°çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ä¿¡ç”¨å½¢å¼åŒ–æ¡†æ¶ï¼Œè¿™ä½¿å¾—å¯¹ä¸åŒä¿¡ç”¨åˆ†é…ç®—æ³•çš„å…¬å¹³æ¯”è¾ƒæˆä¸ºå¯èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼ºè°ƒäº†è¯Šæ–­ä¸åŒæ–¹æ³•åœ¨ç‰¹å®šåœºæ™¯ä¸‹è¡¨ç°ä¸ä½³çš„åŸå› çš„é‡è¦æ€§ï¼Œè¿™æœ‰åŠ©äºæŒ‡å¯¼æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡å¹¶æ²¡æœ‰æå‡ºæ–°çš„ç®—æ³•ï¼Œè€Œæ˜¯å¯¹ç°æœ‰ç®—æ³•è¿›è¡Œäº†ç³»ç»Ÿçš„åˆ†æå’Œæ¯”è¾ƒã€‚å…³é”®çš„è®¾è®¡åœ¨äºå¦‚ä½•å®šä¹‰ä¿¡ç”¨çš„æ¦‚å¿µï¼Œä»¥åŠå¦‚ä½•å»ºç«‹ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ã€‚è®ºæ–‡è¯¦ç»†è®¨è®ºäº†å„ç§è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æå‡ºäº†è¯Šæ–­ä¸åŒæ–¹æ³•ä¼˜ç¼ºç‚¹çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼ºè°ƒäº†åœ¨ä¸åŒç±»å‹çš„ç¯å¢ƒä¸­æµ‹è¯•ä¿¡ç”¨åˆ†é…ç®—æ³•çš„é‡è¦æ€§ï¼Œä¾‹å¦‚å…·æœ‰å»¶è¿Ÿå¥–åŠ±ã€ç¨€ç–å¥–åŠ±å’Œéé©¬å°”å¯å¤«æ€§è´¨çš„ç¯å¢ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°ç³»ç»Ÿåœ°å›é¡¾äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­æ—¶é—´ä¿¡ç”¨åˆ†é…çš„æœ€æ–°è¿›å±•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å½¢å¼åŒ–æ¡†æ¶ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„è§†è§’ã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•çš„ä¼˜ç¼ºç‚¹è¿›è¡Œæ·±å…¥åˆ†æï¼Œè¯¥ç»¼è¿°ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚è™½ç„¶æ²¡æœ‰æä¾›å…·ä½“çš„æ€§èƒ½æ•°æ®ï¼Œä½†è¯¥ç»¼è¿°ä¸ºç†è§£å’Œæ”¹è¿›ç°æœ‰ç®—æ³•å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å®é™…åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡æ›´å¥½åœ°è§£å†³ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´æœ‰æ•ˆåœ°å­¦ä¹ é•¿æœŸç­–ç•¥ï¼Œä»è€Œåœ¨å¤æ‚ç¯å¢ƒä¸­åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥åŸºäºæ­¤ç»¼è¿°ï¼Œå¼€å‘æ›´é²æ£’ã€æ›´é«˜æ•ˆçš„ä¿¡ç”¨åˆ†é…ç®—æ³•ï¼Œæ¨åŠ¨å¼ºåŒ–å­¦ä¹ åœ¨ç°å®ä¸–ç•Œä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Credit Assignment Problem (CAP) refers to the longstanding challenge of Reinforcement Learning (RL) agents to associate actions with their long-term consequences. Solving the CAP is a crucial step towards the successful deployment of RL in the real world since most decision problems provide feedback that is noisy, delayed, and with little or no information about the causes. These conditions make it hard to distinguish serendipitous outcomes from those caused by informed decision-making. However, the mathematical nature of credit and the CAP remains poorly understood and defined. In this survey, we review the state of the art of Temporal Credit Assignment (CA) in deep RL. We propose a unifying formalism for credit that enables equitable comparisons of state-of-the-art algorithms and improves our understanding of the trade-offs between the various methods. We cast the CAP as the problem of learning the influence of an action over an outcome from a finite amount of experience. We discuss the challenges posed by delayed effects, transpositions, and a lack of action influence, and analyse how existing methods aim to address them. Finally, we survey the protocols to evaluate a credit assignment method and suggest ways to diagnose the sources of struggle for different methods. Overall, this survey provides an overview of the field for new-entry practitioners and researchers, it offers a coherent perspective for scholars looking to expedite the starting stages of a new study on the CAP, and it suggests potential directions for future research.

