---
layout: default
title: RLHF and IIA: Perverse Incentives
---

# RLHF and IIA: Perverse Incentives

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.01057" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.01057v3</a>
  <a href="https://arxiv.org/pdf/2312.01057.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.01057v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.01057v3', 'RLHF and IIA: Perverse Incentives')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wanqiao Xu, Shi Dong, Xiuyuan Lu, Grace Lam, Zheng Wen, Benjamin Van Roy

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2023-12-02 (æ›´æ–°: 2024-02-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºRLHFä¸­IIAå‡è®¾å¯¼è‡´çš„åå¥½é”™ä½æ¿€åŠ±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `RLHF` `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `IIAå‡è®¾` `åå¥½å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLHFç®—æ³•ä¾èµ–äºIIAå‡è®¾ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹äº§ç”Ÿä¸äººç±»çœŸå®åå¥½ä¸ä¸€è‡´çš„æ¿€åŠ±ã€‚
2. è¯¥è®ºæ–‡çš„æ ¸å¿ƒåœ¨äºæŒ‡å‡ºå¹¶åˆ†æäº†IIAå‡è®¾åœ¨RLHFä¸­äº§ç”Ÿçš„åé¢‡æ¿€åŠ±é—®é¢˜ï¼Œå¹¶å¼ºè°ƒå…¶å¯¹åˆ›æ–°çš„é˜»ç¢ã€‚
3. è®ºæ–‡é€šè¿‡ç†è®ºåˆ†ææ­ç¤ºäº†IIAå‡è®¾å¦‚ä½•å½±å“RLHFç®—æ³•çš„è¡Œä¸ºï¼Œä½†å…·ä½“çš„å®éªŒéªŒè¯æœªçŸ¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ç®—æ³•ï¼Œç”±äºå…¶æ¨¡å‹å‡è®¾äº†æ— å…³é€‰é¡¹çš„ç‹¬ç«‹æ€§ï¼ˆIIAï¼‰ï¼Œå¯èƒ½æ¿€åŠ±äº§ç”Ÿä¸äººç±»åå¥½ç›¸æ‚–çš„å“åº”ã€‚IIAæ‰€å¯¼è‡´çš„è¿™ç§ä¸è‰¯æ¿€åŠ±ä¼šé˜»ç¢æŸ¥è¯¢æ ¼å¼å’Œå­¦ä¹ ç®—æ³•çš„åˆ›æ–°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰RLHFç®—æ³•ä¸­å­˜åœ¨çš„åå¥½é”™ä½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºIIAå‡è®¾ï¼Œå³ä¸€ä¸ªé€‰é¡¹çš„åå¥½æ¦‚ç‡ä¸å…¶ä»–æ— å…³é€‰é¡¹çš„å­˜åœ¨ä¸å¦æ— å…³ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ç§å‡è®¾å¾€å¾€ä¸æˆç«‹ï¼Œå¯¼è‡´æ¨¡å‹ä¸ºäº†è¿åˆäººç±»åé¦ˆï¼Œåè€Œç”Ÿæˆè´¨é‡è¾ƒå·®æˆ–ä¸äººç±»çœŸå®æ„å›¾ç›¸æ‚–çš„å“åº”ã€‚è¿™ç§åé¢‡æ¿€åŠ±é˜»ç¢äº†RLHFç®—æ³•çš„è¿›ä¸€æ­¥å‘å±•å’Œåˆ›æ–°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æŒ‡å‡ºå¹¶åˆ†æIIAå‡è®¾åœ¨RLHFä¸­çš„å±€é™æ€§ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæ­ç¤ºIIAå‡è®¾å¦‚ä½•å¯¼è‡´æ¨¡å‹äº§ç”Ÿä¸äººç±»åå¥½ä¸ä¸€è‡´çš„æ¿€åŠ±ã€‚è®ºæ–‡è®¤ä¸ºï¼Œæ‰“ç ´IIAå‡è®¾æ˜¯è§£å†³RLHFä¸­åå¥½é”™ä½é—®é¢˜çš„å…³é”®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡ä¸»è¦é€šè¿‡ç†è®ºåˆ†ææ¥è®ºè¯IIAå‡è®¾çš„å±€é™æ€§ï¼Œå¹¶æ²¡æœ‰æå‡ºæ–°çš„ç®—æ³•æ¡†æ¶ã€‚å…¶åˆ†ææ¡†æ¶ä¸»è¦å›´ç»•ç€æ•ˆç”¨å‡½æ•°å’Œé€‰æ‹©æ¦‚ç‡ä¹‹é—´çš„å…³ç³»å±•å¼€ï¼Œè€ƒå¯Ÿåœ¨IIAå‡è®¾ä¸‹ï¼Œæ¨¡å‹å¦‚ä½•é€šè¿‡è°ƒæ•´ç­–ç•¥æ¥æœ€å¤§åŒ–å¥–åŠ±ï¼Œä»¥åŠè¿™ç§ç­–ç•¥å¯èƒ½å¯¼è‡´çš„è´Ÿé¢å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹RLHFç®—æ³•ä¸­IIAå‡è®¾çš„æ‰¹åˆ¤æ€§åˆ†æã€‚ä»¥å¾€çš„ç ”ç©¶å¾€å¾€å¿½ç•¥äº†IIAå‡è®¾å¯èƒ½å¸¦æ¥çš„é—®é¢˜ï¼Œè€Œè¯¥è®ºæ–‡é¦–æ¬¡æ˜ç¡®æŒ‡å‡ºå¹¶æ·±å…¥æ¢è®¨äº†IIAå‡è®¾å¯¹RLHFç®—æ³•æ€§èƒ½å’Œåˆ›æ–°æ€§çš„æ½œåœ¨è´Ÿé¢å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æ²¡æœ‰æå‡ºæ–°çš„ç®—æ³•æˆ–æ¨¡å‹ï¼Œå› æ­¤æ²¡æœ‰å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ã€‚å…¶ä¸»è¦è´¡çŒ®åœ¨äºç†è®ºåˆ†æï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹å‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æï¼Œæ­ç¤ºäº†RLHFç®—æ³•ä¸­IIAå‡è®¾å¯èƒ½å¯¼è‡´çš„åå¥½é”™ä½é—®é¢˜ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿æœªçŸ¥ï¼Œä½†å…¶ç†è®ºè´¡çŒ®ä¸ºæ”¹è¿›RLHFç®—æ³•æŒ‡æ˜äº†æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯¹æ‰€æœ‰ä½¿ç”¨RLHFè¿›è¡Œæ¨¡å‹è®­ç»ƒçš„é¢†åŸŸéƒ½å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡é¿å…IIAå‡è®¾å¸¦æ¥çš„åé¢‡æ¿€åŠ±ï¼Œå¯ä»¥æå‡æ¨¡å‹çš„ç”Ÿæˆè´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦ï¼Œå¹¶ä¿ƒè¿›ç›¸å…³ç®—æ³•çš„åˆ›æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.

