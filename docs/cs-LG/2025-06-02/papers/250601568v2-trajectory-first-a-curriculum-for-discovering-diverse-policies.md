---
layout: default
title: Trajectory First: A Curriculum for Discovering Diverse Policies
---

# Trajectory First: A Curriculum for Discovering Diverse Policies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.01568" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.01568v2</a>
  <a href="https://arxiv.org/pdf/2506.01568.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.01568v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.01568v2', 'Trajectory First: A Curriculum for Discovering Diverse Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cornelius V. Braun, Sayantan Auddy, Marc Toussaint

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-02 (æ›´æ–°: 2025-07-30)

**å¤‡æ³¨**: Accepted into the Inductive Biases in Reinforcement Learning Workshop at RLC 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè½¨è¿¹çš„è¯¾ç¨‹ä»¥æå‡å¤šæ ·æ€§ç­–ç•¥å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å—é™å¤šæ ·æ€§ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `è¯¾ç¨‹å­¦ä¹ ` `å¤šæ ·æ€§ç­–ç•¥` `æ™ºèƒ½ä½“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å—é™å¤šæ ·æ€§å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸­æ¢ç´¢ä¸è¶³ï¼Œå¯¼è‡´ç­–ç•¥å¤šæ ·æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯¾ç¨‹ï¼Œé¦–å…ˆåœ¨è½¨è¿¹å±‚é¢è¿›è¡Œæ¢ç´¢ï¼Œä»¥æé«˜ç­–ç•¥çš„å¤šæ ·æ€§ã€‚
3. å®éªŒè¯æ˜ï¼Œè¯¥è¯¾ç¨‹æ˜¾è‘—æå‡äº†å­¦ä¹ åˆ°çš„æŠ€èƒ½çš„å¤šæ ·æ€§ï¼Œæ”¹å–„äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è§£å†³ä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿä»¥å¤šç§æ–¹å¼å®Œæˆä»»åŠ¡ä½¿å¾—æ™ºèƒ½ä½“å¯¹ä»»åŠ¡å˜å¼‚æ›´å…·é²æ£’æ€§ï¼Œå¹¶å‡å°‘é™·å…¥å±€éƒ¨æœ€ä¼˜çš„å¯èƒ½æ€§ã€‚å—æ­¤å¯å‘ï¼Œå—é™å¤šæ ·æ€§ä¼˜åŒ–ä½œä¸ºä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¢«æå‡ºï¼Œä»¥å¹¶è¡Œè®­ç»ƒå¤šæ ·åŒ–çš„æ™ºèƒ½ä½“ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å—é™å¤šæ ·æ€§å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æœºå™¨äººæ“ä½œï¼‰ä¸­å¸¸å¸¸æ¢ç´¢ä¸è¶³ï¼Œå¯¼è‡´ç­–ç•¥å¤šæ ·æ€§ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è¯¾ç¨‹ï¼Œé¦–å…ˆåœ¨è½¨è¿¹å±‚é¢è¿›è¡Œæ¢ç´¢ï¼Œç„¶åå†å­¦ä¹ åŸºäºæ­¥éª¤çš„ç­–ç•¥ã€‚é€šè¿‡å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹åŸºäºæŠ€èƒ½çš„å¤šæ ·æ€§ä¼˜åŒ–çš„ä¸è¶³ä¹‹å¤„çš„æ–°è§è§£ï¼Œå¹¶å®è¯è¯æ˜æˆ‘ä»¬çš„è¯¾ç¨‹æé«˜äº†å­¦ä¹ æŠ€èƒ½çš„å¤šæ ·æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å—é™å¤šæ ·æ€§å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸­æ¢ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´ç­–ç•¥å¤šæ ·æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨è½¨è¿¹å±‚é¢è¿›è¡Œåˆæ­¥æ¢ç´¢ï¼Œæ„å»ºä¸€ä¸ªè¯¾ç¨‹ï¼Œä»¥æ­¤æ¥å¢å¼ºç­–ç•¥å­¦ä¹ çš„å¤šæ ·æ€§ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨é¿å…æ™ºèƒ½ä½“åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯è½¨è¿¹å±‚é¢çš„æ¢ç´¢é˜¶æ®µï¼Œæ¥ç€æ˜¯åŸºäºæ­¥éª¤çš„ç­–ç•¥å­¦ä¹ é˜¶æ®µã€‚é€šè¿‡è¿™ç§åˆ†é˜¶æ®µçš„æ–¹å¼ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æ›´å¹¿æ³›çš„ç­–ç•¥ç©ºé—´ä¸­è¿›è¡Œå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†è½¨è¿¹å±‚é¢çš„æ¢ç´¢ä½œä¸ºè¯¾ç¨‹çš„ç¬¬ä¸€æ­¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•ç›´æ¥è¿›è¡Œæ­¥éª¤å­¦ä¹ çš„æ–¹å¼æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œè¯¾ç¨‹çš„è®¾è®¡è€ƒè™‘äº†æ¢ç´¢çš„æ·±åº¦å’Œå¹¿åº¦ï¼ŒæŸå¤±å‡½æ•°åˆ™ç»“åˆäº†å¤šæ ·æ€§å’Œæ€§èƒ½çš„æƒè¡¡ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°è¯¾ç¨‹çš„æ™ºèƒ½ä½“åœ¨å¤šæ ·æ€§ç­–ç•¥å­¦ä¹ ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œç­–ç•¥å¤šæ ·æ€§æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šï¼Œä¸”åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°æ›´ä¸ºç¨³å¥ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¾ç¨‹è®¾è®¡çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½åˆ¶é€ ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­æ›´çµæ´»åœ°åº”å¯¹å˜åŒ–ï¼Œæé«˜ä»»åŠ¡å®Œæˆçš„æˆåŠŸç‡å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨å¤šæ ·æ€§ç­–ç•¥å­¦ä¹ çš„è¿›ä¸€æ­¥å‘å±•ï¼Œä¿ƒè¿›æ›´æ™ºèƒ½çš„è‡ªä¸»ç³»ç»Ÿçš„å®ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Being able to solve a task in diverse ways makes agents more robust to task variations and less prone to local optima. In this context, constrained diversity optimization has emerged as a powerful reinforcement learning (RL) framework to train a diverse set of agents in parallel. However, existing constrained-diversity RL methods often under-explore in complex tasks such as robotic manipulation, leading to a lack in policy diversity. To improve diversity optimization in RL, we therefore propose a curriculum that first explores at the trajectory level before learning step-based policies. In our empirical evaluation, we provide novel insights into the shortcoming of skill-based diversity optimization, and demonstrate empirically that our curriculum improves the diversity of the learned skills.

