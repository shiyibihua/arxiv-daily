---
layout: default
title: SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics
---

# SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.01844" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.01844v1</a>
  <a href="https://arxiv.org/pdf/2506.01844.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.01844v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.01844v1', 'SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, Simon Alibert, Matthieu Cord, Thomas Wolf, Remi Cadene

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-02

**å¤‡æ³¨**: 24 pages. Code and assets: https://github.com/huggingface/lerobot

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSmolVLAä»¥è§£å†³ç°æœ‰VLAæ¨¡å‹çš„é«˜æˆæœ¬é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æœºå™¨äººæŠ€æœ¯` `å¤šæ¨¡æ€å­¦ä¹ ` `å°å‹åŒ–æ¨¡å‹` `å¼‚æ­¥æ¨ç†` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç¤¾åŒºé©±åŠ¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹é€šå¸¸å‚æ•°åºå¤§ï¼Œå¯¼è‡´é«˜æ˜‚çš„è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚
2. SmolVLAé€šè¿‡è®¾è®¡ä¸ºå°å‹ä¸”é«˜æ•ˆçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šè®­ç»ƒï¼Œå¹¶åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šéƒ¨ç½²ï¼Œä»è€Œé™ä½æˆæœ¬ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSmolVLAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸10å€æ›´å¤§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œèƒ½å¤Ÿç¼–ç ä¸°å¯Œçš„è§†è§‰å’Œè¯­è¨€çŸ¥è¯†ï¼Œä¸ºæœºå™¨äººæŠ€æœ¯æä¾›äº†å¼ºå¤§çš„åŸºç¡€ã€‚ç°æœ‰çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹é€šå¸¸å‚æ•°åºå¤§ï¼Œå¯¼è‡´é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬å’Œæœ‰é™çš„å®é™…åº”ç”¨èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºSmolVLAï¼Œä¸€ä¸ªå°å‹ã€é«˜æ•ˆä¸”ä»¥ç¤¾åŒºä¸ºé©±åŠ¨çš„VLAæ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚SmolVLAè®¾è®¡ä¸ºå¯åœ¨å•ä¸ªGPUä¸Šè®­ç»ƒï¼Œå¹¶å¯åœ¨æ¶ˆè´¹çº§GPUæˆ–CPUä¸Šéƒ¨ç½²ã€‚é€šè¿‡å¼•å…¥å¼‚æ­¥æ¨ç†å †æ ˆï¼ŒSmolVLAæé«˜äº†å“åº”é€Ÿåº¦ï¼Œå…è®¸æ›´é«˜çš„æ§åˆ¶ç‡å’Œåˆ†å—åŠ¨ä½œç”Ÿæˆã€‚å°½ç®¡ä½“ç§¯å°ï¼ŒSmolVLAçš„æ€§èƒ½ä¸10å€æ›´å¤§æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººåŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ‰€æœ‰ä»£ç ã€é¢„è®­ç»ƒæ¨¡å‹å’Œè®­ç»ƒæ•°æ®å‡å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ï¼ˆVLAï¼‰åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´çš„é«˜æˆæœ¬é—®é¢˜ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸éœ€è¦æ•°åäº¿å‚æ•°ï¼Œå¯¼è‡´è®­ç»ƒèµ„æºæ¶ˆè€—å·¨å¤§ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…æœºå™¨äººåº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSmolVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ä¸ªå°å‹ä¸”é«˜æ•ˆçš„VLAæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå®ç°éƒ¨ç½²ï¼Œä»è€Œé™ä½æ•´ä½“æˆæœ¬å’Œæé«˜å¯ç”¨æ€§ã€‚é€šè¿‡å¼‚æ­¥æ¨ç†å †æ ˆçš„å¼•å…¥ï¼ŒSmolVLAèƒ½å¤Ÿå°†æ„ŸçŸ¥ä¸åŠ¨ä½œé¢„æµ‹è§£è€¦ï¼Œæé«˜å“åº”é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSmolVLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆæ˜¯è§†è§‰å’Œè¯­è¨€çš„è¾“å…¥æ¨¡å—ï¼Œç„¶åæ˜¯æ„ŸçŸ¥ä¸åŠ¨ä½œé¢„æµ‹æ¨¡å—ï¼Œæœ€åæ˜¯åŠ¨ä½œæ‰§è¡Œæ¨¡å—ã€‚å¼‚æ­¥æ¨ç†å †æ ˆå…è®¸æ„ŸçŸ¥å’ŒåŠ¨ä½œé¢„æµ‹åœ¨ä¸åŒçš„æ—¶é—´æ®µå†…è¿›è¡Œï¼Œä»è€Œæé«˜æ§åˆ¶ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šSmolVLAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å°å‹åŒ–è®¾è®¡å’Œå¼‚æ­¥æ¨ç†æœºåˆ¶ï¼Œä½¿å…¶åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚è¿™ä¸ç°æœ‰çš„åºå¤§VLAæ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒSmolVLAé‡‡ç”¨äº†ä¼˜åŒ–çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿åœ¨å°å‹åŒ–çš„åŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥é€‚åº”å•GPUç¯å¢ƒï¼Œç¡®ä¿é«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSmolVLAçš„æ€§èƒ½ä¸å‚æ•°é‡ä¸º10å€çš„ç°æœ‰VLAæ¨¡å‹ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å°å‹åŒ–å’Œé«˜æ•ˆæ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmolVLAåœ¨è®­ç»ƒå’Œæ¨ç†æˆæœ¬ä¸Šå¤§å¹…é™ä½ï¼Œä¸”èƒ½å¤Ÿå®ç°æ›´é«˜çš„æ§åˆ¶ç‡ï¼Œæå‡äº†æœºå™¨äººä»»åŠ¡çš„å“åº”é€Ÿåº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SmolVLAçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®¶åº­æœåŠ¡æœºå™¨äººã€æ•™è‚²æœºå™¨äººä»¥åŠå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰åœºæ™¯ã€‚å…¶é«˜æ•ˆæ€§å’Œä½æˆæœ¬ç‰¹æ€§ä½¿å¾—ä¸­å°å‹ä¼ä¸šå’Œä¸ªäººå¼€å‘è€…èƒ½å¤Ÿæ›´å®¹æ˜“åœ°å®ç°è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æœºå™¨äººæ§åˆ¶ï¼Œæ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„æ™®åŠä¸åº”ç”¨ã€‚æœªæ¥ï¼ŒSmolVLAå¯èƒ½ä¼šåœ¨æ›´å¤šçš„æœºå™¨äººä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œä¿ƒè¿›äººæœºäº¤äº’çš„è‡ªç„¶åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data.

