---
layout: default
title: Leveraging Analytic Gradients in Provably Safe Reinforcement Learning
---

# Leveraging Analytic Gradients in Provably Safe Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.01665" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.01665v3</a>
  <a href="https://arxiv.org/pdf/2506.01665.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.01665v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.01665v3', 'Leveraging Analytic Gradients in Provably Safe Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tim Walter, Hannah Markgraf, Jonathan KÃ¼lz, Matthias Althoff

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-02 (æ›´æ–°: 2025-10-23)

**å¤‡æ³¨**: 21 pages, 10 figures

**æœŸåˆŠ**: IEEE Open Journal of Control Systems, vol. 4, pp. 463-481, 2025

**DOI**: [10.1109/OJCSYS.2025.3607845](https://doi.org/10.1109/OJCSYS.2025.3607845)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://timwalter.github.io/safe-agb-rl.github.io)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé¦–ä¸ªæœ‰æ•ˆçš„åˆ†ææ¢¯åº¦å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¿éšœæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `åˆ†ææ¢¯åº¦` `ä¿éšœæªæ–½` `è‡ªä¸»æœºå™¨äºº` `å¯å¾®åˆ†ä»¿çœŸ` `æ§åˆ¶ä»»åŠ¡` `å­¦ä¹ ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä¿éšœæªæ–½ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹åŸºäºåˆ†ææ¢¯åº¦çš„å­¦ä¹ èŒƒå¼ç¼ºä¹æœ‰æ•ˆçš„ä¿éšœæ‰‹æ®µã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¿éšœæ–¹æ³•ï¼Œé€šè¿‡å¯¹ç°æœ‰å¯å¾®åˆ†ä¿éšœçš„åˆ†æå’Œæ”¹è¿›ï¼Œé›†æˆåˆ°å…ˆè¿›çš„å­¦ä¹ ç®—æ³•ä¸­ï¼Œæ—¨åœ¨ç¼©å°æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ä¿éšœæªæ–½èƒ½å¤Ÿåœ¨ä¸å¦¨ç¢å­¦ä¹ æ€§èƒ½çš„å‰æä¸‹ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„å®‰å…¨æ€§ï¼Œå…·æœ‰è‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­ï¼Œéƒ¨ç½²è‡ªä¸»æœºå™¨äººéœ€è¦å®‰å…¨ä¿éšœã€‚å¯è¯æ˜å®‰å…¨çš„å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡ä¿éšœæªæ–½æä¾›æ­¤ç±»ä¿éšœã€‚ç°æœ‰çš„ä¿éšœæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åŸºäºé‡‡æ ·çš„å¼ºåŒ–å­¦ä¹ ä¸Šï¼Œè€ŒåŸºäºåˆ†ææ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹ é€šå¸¸åœ¨ç¯å¢ƒäº¤äº’æ¬¡æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹å®ç°æ›´ä¼˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œç›®å‰å°šæ— é’ˆå¯¹è¿™ä¸€å­¦ä¹ èŒƒå¼çš„ä¿éšœæ–¹æ³•ã€‚æœ¬æ–‡å¼€å‘äº†é¦–ä¸ªæœ‰æ•ˆçš„ä¿éšœæªæ–½ï¼Œé€šè¿‡åˆ†æç°æœ‰çš„å¯å¾®åˆ†ä¿éšœï¼Œè°ƒæ•´æ˜ å°„å’Œæ¢¯åº¦å…¬å¼ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°å…ˆè¿›çš„å­¦ä¹ ç®—æ³•å’Œå¯å¾®åˆ†ä»¿çœŸä¸­ã€‚é€šè¿‡å¯¹ä¸‰ä¸ªæ§åˆ¶ä»»åŠ¡çš„æ•°å€¼å®éªŒï¼Œè¯„ä¼°ä¸åŒä¿éšœæªæ–½å¯¹å­¦ä¹ çš„å½±å“ï¼Œç»“æœè¡¨æ˜åœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹å®ç°äº†å®‰å…¨è®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰åŸºäºåˆ†ææ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹ ç¼ºä¹æœ‰æ•ˆå®‰å…¨ä¿éšœçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­æ— æ³•æä¾›è¶³å¤Ÿçš„ä¿éšœï¼Œå¯¼è‡´åœ¨å®é™…éƒ¨ç½²æ—¶å­˜åœ¨é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼€å‘é¦–ä¸ªæœ‰æ•ˆçš„ä¿éšœæªæ–½ï¼Œé€šè¿‡å¯¹ç°æœ‰å¯å¾®åˆ†ä¿éšœçš„åˆ†æå’Œæ”¹è¿›ï¼Œç»“åˆå…ˆè¿›çš„å­¦ä¹ ç®—æ³•ï¼Œç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°å®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹ç°æœ‰ä¿éšœæªæ–½çš„åˆ†æã€æ˜ å°„å’Œæ¢¯åº¦å…¬å¼çš„è°ƒæ•´ï¼Œä»¥åŠå°†è¿™äº›ä¿éšœæªæ–½æ•´åˆåˆ°å­¦ä¹ ç®—æ³•å’Œå¯å¾®åˆ†ä»¿çœŸä¸­ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ä¿éšœæªæ–½çš„è®¾è®¡ã€å­¦ä¹ ç®—æ³•çš„å®ç°å’Œå®éªŒè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé¦–æ¬¡å°†å¯å¾®åˆ†ä¿éšœæªæ–½æœ‰æ•ˆåœ°åº”ç”¨äºåŸºäºåˆ†ææ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹ ï¼Œå¡«è¡¥äº†è¿™ä¸€é¢†åŸŸçš„ç©ºç™½ï¼Œä¸ä¼ ç»ŸåŸºäºé‡‡æ ·çš„æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè°ƒæ•´äº†ä¿éšœæªæ–½çš„æ˜ å°„å’Œæ¢¯åº¦å…¬å¼ï¼Œç¡®ä¿å…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨äº†å…ˆè¿›çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„ä¿éšœæªæ–½åœ¨ä¸‰ä¸ªæ§åˆ¶ä»»åŠ¡ä¸­å‡å®ç°äº†å®‰å…¨è®­ç»ƒï¼Œä¸”æ€§èƒ½æœªå—åˆ°å½±å“ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œå­¦ä¹ æ•ˆç‡æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†ä¿éšœæªæ–½çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶æ±½è½¦å’Œå…¶ä»–å®‰å…¨å…³é”®ç³»ç»Ÿã€‚é€šè¿‡æä¾›æœ‰æ•ˆçš„å®‰å…¨ä¿éšœï¼Œèƒ½å¤Ÿé™ä½è¿™äº›ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„é£é™©ï¼Œæé«˜å…¶å¯é æ€§å’Œç”¨æˆ·ä¿¡ä»»åº¦ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¯èƒ½ä¼šåœ¨æ›´å¤šé¢†åŸŸæ¨å¹¿åº”ç”¨ï¼Œæ¨åŠ¨å®‰å…¨å¼ºåŒ–å­¦ä¹ çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The deployment of autonomous robots in safety-critical applications requires safety guarantees. Provably safe reinforcement learning is an active field of research that aims to provide such guarantees using safeguards. These safeguards should be integrated during training to reduce the sim-to-real gap. While there are several approaches for safeguarding sampling-based reinforcement learning, analytic gradient-based reinforcement learning often achieves superior performance from fewer environment interactions. However, there is no safeguarding approach for this learning paradigm yet. Our work addresses this gap by developing the first effective safeguard for analytic gradient-based reinforcement learning. We analyse existing, differentiable safeguards, adapt them through modified mappings and gradient formulations, and integrate them into a state-of-the-art learning algorithm and a differentiable simulation. Using numerical experiments on three control tasks, we evaluate how different safeguards affect learning. The results demonstrate safeguarded training without compromising performance. Additional visuals are provided at \href{https://timwalter.github.io/safe-agb-rl.github.io}{timwalter.github.io/safe-agb-rl.github.io}.

