---
layout: default
title: Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs
---

# Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00084" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00084v1</a>
  <a href="https://arxiv.org/pdf/2509.00084.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00084v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00084v1', 'Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qibin Wang, Pu Zhao, Shaohan Huang, Fangkai Yang, Lu Wang, Furu Wei, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç”Ÿæˆè‡ªæˆ‘ç²¾ç‚¼æ–¹æ³•ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘ç²¾ç‚¼` `æ¨ç†èƒ½åŠ›` `æµ‹è¯•æ—¶æ‰©å±•` `ç”Ÿæˆæ¨¡å‹` `æ··åˆè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•å¦‚æœ€ä½³é€‰æ‹©å’Œå¤šæ•°æŠ•ç¥¨ä¾èµ–äºå€™é€‰å“åº”çš„è´¨é‡ï¼Œæ— æ³•å¤„ç†æ‰€æœ‰å€™é€‰å‡é”™è¯¯çš„æƒ…å†µã€‚
2. æœ¬æ–‡æå‡ºç”Ÿæˆè‡ªæˆ‘ç²¾ç‚¼ï¼ˆGSRï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¹¶è¡Œç”Ÿæˆå€™é€‰å“åº”å¹¶è¿›è¡Œè‡ªæˆ‘ç²¾ç‚¼ï¼Œåˆæˆæ›´ä¼˜è§£ä»¥è§£å†³å¤æ‚æ¨ç†é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGSRåœ¨äº”ä¸ªæ•°å­¦åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºè¿›ä¸€æ­¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³å¤æ‚å¤šæ­¥éª¤æ¨ç†é—®é¢˜çš„èƒ½åŠ›ï¼Œæµ‹è¯•æ—¶æ‰©å±•ï¼ˆTTSï¼‰æ–¹æ³•å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•å¦‚æœ€ä½³é€‰æ‹©å’Œå¤šæ•°æŠ•ç¥¨å—é™äºå€™é€‰å“åº”çš„è´¨é‡ï¼Œå½“æ‰€æœ‰å€™é€‰å‡é”™è¯¯æ—¶æ— æ³•äº§ç”Ÿæ­£ç¡®è§£ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºç”Ÿæˆè‡ªæˆ‘ç²¾ç‚¼ï¼ˆGSRï¼‰æ¡†æ¶ï¼Œç»Ÿä¸€æ¨¡å‹å¹¶è¡Œç”Ÿæˆå€™é€‰å“åº”ï¼Œç„¶ååŸºäºé—®é¢˜åŠå€™é€‰è¿›è¡Œè‡ªæˆ‘ç²¾ç‚¼ï¼Œåˆæˆæ›´ä¼˜è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªæ•°å­¦åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”è¿™ç§è‡ªæˆ‘ç²¾ç‚¼èƒ½åŠ›åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šå…·æœ‰é²æ£’æ€§ï¼Œèƒ½å¤Ÿæ¨å¹¿åˆ°åˆ†å¸ƒå¤–æ¨ç†ä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç°æœ‰æ–¹æ³•åœ¨å€™é€‰å“åº”è´¨é‡ä¸ä½³æ—¶çš„å±€é™æ€§ã€‚ç°æœ‰çš„æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•æ— æ³•åœ¨æ‰€æœ‰å€™é€‰å‡é”™è¯¯çš„æƒ…å†µä¸‹æä¾›æ­£ç¡®è§£ç­”ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç”Ÿæˆè‡ªæˆ‘ç²¾ç‚¼ï¼ˆGSRï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¹¶è¡Œç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œè‡ªæˆ‘ç²¾ç‚¼ï¼Œä»¥åˆæˆæ›´ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥è®¾è®¡æ—¨åœ¨æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå…‹æœç›´æ¥æç¤ºæ—¶çš„ç²¾ç‚¼æ•ˆæœä¸ä½³çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGSRæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å¹¶è¡Œç”Ÿæˆå€™é€‰å“åº”ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯åŸºäºé—®é¢˜å’Œå€™é€‰è¿›è¡Œè‡ªæˆ‘ç²¾ç‚¼ã€‚æ¨¡å‹é€šè¿‡è”åˆä¼˜åŒ–ç›´æ¥è§£å†³é—®é¢˜å’Œç²¾ç‚¼å€™é€‰å“åº”ä¸¤ä¸ªç›®æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†è‡ªæˆ‘ç²¾ç‚¼æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆå€™é€‰åè¿›è¡Œè¿›ä¸€æ­¥çš„ä¼˜åŒ–ï¼Œè€Œä¸æ˜¯ä¾èµ–å¤–éƒ¨é€‰æ‹©æ¨¡å‹ã€‚è¿™ä¸€æœºåˆ¶ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶æ›´åŠ çµæ´»å’Œé«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†æ··åˆè®­ç»ƒç®¡é“ï¼Œä¼˜åŒ–æŸå¤±å‡½æ•°ä»¥åŒæ—¶å…³æ³¨é—®é¢˜è§£å†³å’Œå€™é€‰å“åº”ç²¾ç‚¼ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œè§„æ¨¡ä¸‹çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGSRæ–¹æ³•åœ¨äº”ä¸ªæ•°å­¦åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èã€åŒ»ç–—ç­‰éœ€è¦å¤æ‚æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨è‡ªåŠ¨é—®ç­”ã€æ™ºèƒ½åŠ©æ‰‹å’Œå†³ç­–æ”¯æŒç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„AIç³»ç»Ÿå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To further enhance the ability of Large Language Models (LLMs) to solve complex, multi-step reasoning problems, test-time scaling (TTS) methods have gained widespread attention. Existing approaches such as Best-of-N and majority voting are limited as their performance depends on the quality of candidate responses, making them unable to produce a correct solution when all candidates are incorrect. Introducing an additional model to select the best response also incurs significant deployment costs. To this end, we introduce Generative Self-Refinement (GSR), a novel parallel test-time scaling framework where a unified model first generates a set of candidate responses in parallel and then performs self-refinement to synthesize a new superior solution based on a prompt consisting of the problem and these candidates. However, LLMs struggle to perform refinement effectively when prompted directly. Therefore, we design a hybrid training pipeline by jointly optimizing for two complementary objectives, solving problems directly and refining candidate responses. Experimental results demonstrate that our method achieves state-of-the-art performance across five mathematical benchmarks. We further show that this learned self-refinement skill is a model-agnostic enhancement, robust across different model scales and generalizing to out-of-distribution reasoning tasks.

