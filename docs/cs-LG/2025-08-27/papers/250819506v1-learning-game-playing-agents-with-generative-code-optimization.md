---
layout: default
title: Learning Game-Playing Agents with Generative Code Optimization
---

# Learning Game-Playing Agents with Generative Code Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19506" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19506v1</a>
  <a href="https://arxiv.org/pdf/2508.19506.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19506v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19506v1', 'Learning Game-Playing Agents with Generative Code Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiyi Kuang, Ryan Rong, YuCheng Yuan, Allen Nie

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**å¤‡æ³¨**: ICML 2025 Workshop on Programmatic Representations for Agent Learning, Vancouver, Canada

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç”Ÿæˆä»£ç ä¼˜åŒ–æ–¹æ³•ä»¥å­¦ä¹ æ¸¸æˆæ™ºèƒ½ä½“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”Ÿæˆä¼˜åŒ–` `æ¸¸æˆæ™ºèƒ½ä½“` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘æ¼”åŒ–` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥è¡¨ç¤º` `Atariæ¸¸æˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¸¸æˆæ™ºèƒ½ä½“å­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–äºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œè®­ç»ƒæ—¶é—´é•¿ä¸”éœ€è¦å¤§é‡ç¯å¢ƒäº¤äº’ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å†³ç­–ç­–ç•¥è§†ä¸ºè‡ªæˆ‘æ¼”åŒ–ä»£ç çš„ç”Ÿæˆä¼˜åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œå‡å°‘äº†äººç±»å¹²é¢„ã€‚
3. åœ¨Atariæ¸¸æˆçš„å®éªŒä¸­ï¼Œæ‰€æå‡ºçš„Pythonç¨‹åºåœ¨æ€§èƒ½ä¸Šä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ åŸºçº¿ç›¸å½“ï¼Œä½†è®­ç»ƒæ—¶é—´å’Œç¯å¢ƒäº¤äº’æ¬¡æ•°æ˜¾è‘—å‡å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”Ÿæˆä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºå­¦ä¹ æ¸¸æˆæ™ºèƒ½ä½“ï¼Œå…¶ä¸­ç­–ç•¥è¢«è¡¨ç¤ºä¸ºPythonç¨‹åºï¼Œå¹¶é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œä¼˜åŒ–ã€‚è¯¥æ–¹æ³•å°†å†³ç­–ç­–ç•¥è§†ä¸ºè‡ªæˆ‘æ¼”åŒ–çš„ä»£ç ï¼Œä»¥å½“å‰è§‚å¯Ÿä¸ºè¾“å…¥ï¼Œä»¥æ¸¸æˆå†…åŠ¨ä½œä¸ºè¾“å‡ºï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡æ‰§è¡Œè½¨è¿¹å’Œè‡ªç„¶è¯­è¨€åé¦ˆè‡ªæˆ‘æ”¹è¿›ï¼Œä¸”äººç±»å¹²é¢„æœ€å°ã€‚åœ¨Atariæ¸¸æˆä¸­ï¼Œæˆ‘ä»¬çš„Pythonæ¸¸æˆç¨‹åºåœ¨æ€§èƒ½ä¸Šä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ åŸºçº¿ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨æ˜¾è‘—æ›´å°‘çš„è®­ç»ƒæ—¶é—´å’Œç¯å¢ƒäº¤äº’æ¬¡æ•°ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†ç¨‹åºåŒ–ç­–ç•¥è¡¨ç¤ºåœ¨æ„å»ºé«˜æ•ˆã€é€‚åº”æ€§å¼ºçš„æ™ºèƒ½ä½“æ–¹é¢çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿè¿›è¡Œå¤æ‚çš„é•¿æ—¶é—´æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¸¸æˆæ™ºèƒ½ä½“å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒæ—¶é—´å’Œç¯å¢ƒäº¤äº’æ–¹é¢çš„ä½æ•ˆç‡é—®é¢˜ã€‚ä¼ ç»Ÿçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡çš„æ ·æœ¬å’Œé•¿æ—¶é—´çš„è®­ç»ƒï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚æ¸¸æˆä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å†³ç­–ç­–ç•¥è¡¨ç¤ºä¸ºPythonç¨‹åºï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ã€‚é€šè¿‡å°†å½“å‰è§‚å¯Ÿä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆç›¸åº”çš„æ¸¸æˆåŠ¨ä½œï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ä¸æ–­æ”¹è¿›è‡ªèº«ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç­–ç•¥ç”Ÿæˆæ¨¡å—ã€æ‰§è¡Œæ¨¡å—å’Œåé¦ˆæ¨¡å—ã€‚ç­–ç•¥ç”Ÿæˆæ¨¡å—è´Ÿè´£ç”ŸæˆPythonä»£ç ï¼Œæ‰§è¡Œæ¨¡å—åœ¨æ¸¸æˆç¯å¢ƒä¸­æ‰§è¡Œç”Ÿæˆçš„ä»£ç ï¼Œåé¦ˆæ¨¡å—åˆ™é€šè¿‡è‡ªç„¶è¯­è¨€åé¦ˆæ¥æŒ‡å¯¼ç­–ç•¥çš„ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†ç­–ç•¥è¡¨ç¤ºä¸ºå¯æ‰§è¡Œçš„ä»£ç ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘æ¼”åŒ–çš„æ–¹å¼è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä¸ä¼ ç»Ÿçš„åŸºäºå€¼å‡½æ•°æˆ–ç­–ç•¥æ¢¯åº¦çš„æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…é€šå¸¸ä¾èµ–äºå›ºå®šçš„ç½‘ç»œç»“æ„å’Œå‚æ•°æ›´æ–°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆå’Œä¼˜åŒ–ä»£ç ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆæ‰§è¡Œç»“æœå’Œè‡ªç„¶è¯­è¨€åé¦ˆçš„å¤åˆæŸå¤±ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„ç­–ç•¥æ—¢æœ‰æ•ˆåˆç¬¦åˆæ¸¸æˆè§„åˆ™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„Pythonæ¸¸æˆç¨‹åºåœ¨Atariæ¸¸æˆä¸­è¡¨ç°å‡ºä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ åŸºçº¿ç›¸å½“çš„æ€§èƒ½ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘äº†50%ä»¥ä¸Šï¼Œç¯å¢ƒäº¤äº’æ¬¡æ•°å‡å°‘äº†70%ã€‚è¿™ä¸€æ˜¾è‘—æå‡è¡¨æ˜ç”Ÿæˆä»£ç ä¼˜åŒ–æ–¹æ³•åœ¨æ¸¸æˆæ™ºèƒ½ä½“å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¸¸æˆå¼€å‘ã€è‡ªåŠ¨åŒ–æµ‹è¯•å’Œæ™ºèƒ½ä½“è®­ç»ƒç­‰ã€‚é€šè¿‡ç”Ÿæˆä»£ç ä¼˜åŒ–æ–¹æ³•ï¼Œå¯ä»¥å¿«é€Ÿå¼€å‘å‡ºé«˜æ•ˆçš„æ¸¸æˆæ™ºèƒ½ä½“ï¼Œé™ä½å¼€å‘æˆæœ¬ï¼Œå¹¶æå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œå¦‚æœºå™¨äººæ§åˆ¶å’Œè‡ªåŠ¨å†³ç­–ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a generative optimization approach for learning game-playing agents, where policies are represented as Python programs and refined using large language models (LLMs). Our method treats decision-making policies as self-evolving code, with current observation as input and an in-game action as output, enabling agents to self-improve through execution traces and natural language feedback with minimal human intervention. Applied to Atari games, our game-playing Python program achieves performance competitive with deep reinforcement learning (RL) baselines while using significantly less training time and much fewer environment interactions. This work highlights the promise of programmatic policy representations for building efficient, adaptable agents capable of complex, long-horizon reasoning.

