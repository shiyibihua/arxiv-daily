---
layout: default
title: SCAR: A Characterization Scheme for Multi-Modal Dataset
---

# SCAR: A Characterization Scheme for Multi-Modal Dataset

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19659" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19659v1</a>
  <a href="https://arxiv.org/pdf/2508.19659.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19659v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19659v1', 'SCAR: A Characterization Scheme for Multi-Modal Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ri Su, Zhao Chen, Caleb Chen Cao, Nan Tang, Lei Chen

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**å¤‡æ³¨**: 6 pages, 3 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/McAloma/SCAR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSCARæ–¹æ¡ˆä»¥è¡¨å¾å¤šæ¨¡æ€æ•°æ®é›†ç‰¹æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®é›†è¡¨å¾` `å¤šæ¨¡æ€å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `æ•°æ®è´¨é‡` `åŸºç¡€æ•°æ®é›†` `æ•°æ®è·å–ç­–ç•¥` `ç»“æ„ç‰¹æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ•°æ®æ•°é‡å’Œè®­ç»ƒæ•ˆç‡ï¼Œå¿½è§†äº†æ•°æ®è´¨é‡çš„ç»“æ„æ€§æ–¹é¢ï¼Œé™åˆ¶äº†æ³›åŒ–èƒ½åŠ›çš„æå‡ã€‚
2. SCARæ–¹æ¡ˆé€šè¿‡è§„æ¨¡ã€è¦†ç›–ã€çœŸå®æ€§å’Œä¸°å¯Œæ€§å››ä¸ªæŒ‡æ ‡ç³»ç»Ÿæ€§è¡¨å¾æ•°æ®é›†ç‰¹æ€§ï¼Œæ•æ‰ç¨³å®šçš„å†…åœ¨ç»“æ„ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSCARèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹æ•°æ®æ•ˆç”¨ï¼Œå¹¶æŒ‡å¯¼æ•°æ®è·å–ï¼Œæå‡å¤šæ¨¡æ€æ•°æ®é›†çš„ç‰¹æ€§æ‰©å±•æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºç¡€æ¨¡å‹åœ¨å¤šæ ·ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™ä¸»è¦å—è®­ç»ƒæ•°æ®ç‰¹æ€§çš„é©±åŠ¨ã€‚ç°æœ‰çš„æ•°æ®ä¸­å¿ƒæ–¹æ³•å¦‚å‰ªæå’Œå‹ç¼©è™½ç„¶æ—¨åœ¨ä¼˜åŒ–è®­ç»ƒï¼Œä½†å¯¹æ•°æ®å±æ€§å¦‚ä½•å½±å“æ³›åŒ–çš„ç†è®ºæ´å¯Ÿæœ‰é™ã€‚æœ¬æ–‡æå‡ºSCARï¼Œä¸€ä¸ªç³»ç»ŸåŒ–çš„æ–¹æ¡ˆï¼Œé€šè¿‡å››ä¸ªå…³é”®æŒ‡æ ‡ï¼ˆè§„æ¨¡ã€è¦†ç›–ã€çœŸå®æ€§å’Œä¸°å¯Œæ€§ï¼‰æ¥è¡¨å¾æ•°æ®é›†çš„å†…åœ¨ç»“æ„ç‰¹æ€§ã€‚ä¸ä»¥å¾€çš„æ•°æ®ä¸­å¿ƒåº¦é‡ä¸åŒï¼ŒSCARæ•æ‰åˆ°åœ¨æ•°æ®é›†æ‰©å±•ä¸‹ä¿æŒä¸å˜çš„ç¨³å®šç‰¹å¾ï¼Œä¸ºæ•°æ®ç†è§£æä¾›äº†åšå®çš„åŸºç¡€ã€‚åŸºäºè¿™äº›ç»“æ„ç‰¹æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†åŸºç¡€æ•°æ®é›†çš„æ¦‚å¿µï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦ç‰¹å®šæ¨¡å‹é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä¿ç•™å®Œæ•´æ•°æ®é›†çš„æ³›åŒ–è¡Œä¸ºã€‚é€šè¿‡å¯¹å¤šæ¨¡æ€æ•°æ®é›†çš„å®éªŒéªŒè¯äº†SCARåœ¨é¢„æµ‹æ•°æ®æ•ˆç”¨å’ŒæŒ‡å¯¼æ•°æ®è·å–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®ä¸­å¿ƒæ–¹æ³•å¯¹æ•°æ®ç‰¹æ€§ç†è§£ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ ·æœ¬æ‰©å±•æ—¶å¯¹æ•°æ®è´¨é‡ç»“æ„çš„å¿½è§†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSCARé€šè¿‡å››ä¸ªå…³é”®æŒ‡æ ‡ï¼ˆè§„æ¨¡ã€è¦†ç›–ã€çœŸå®æ€§å’Œä¸°å¯Œæ€§ï¼‰ç³»ç»Ÿæ€§åœ°è¡¨å¾æ•°æ®é›†çš„å†…åœ¨ç‰¹æ€§ï¼Œæä¾›äº†ä¸€ç§ç¨³å®šçš„ç‰¹å¾æ•æ‰æ–¹å¼ï¼Œä»¥æ”¯æŒæ•°æ®ç†è§£å’Œæ³›åŒ–èƒ½åŠ›çš„æå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSCARçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç‰¹æ€§è¡¨å¾æ¨¡å—ã€åŸºç¡€æ•°æ®é›†æ„å»ºæ¨¡å—å’Œæ•°æ®è¡¥å…¨ç­–ç•¥æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å››ä¸ªæŒ‡æ ‡åˆ†ææ•°æ®é›†ç‰¹æ€§ï¼›ç„¶åï¼Œæ„å»ºåŸºç¡€æ•°æ®é›†ä»¥ä¿ç•™æ³›åŒ–è¡Œä¸ºï¼›æœ€åï¼ŒåŸºäºæ³›åŒ–åå·®å¼€å‘æ•°æ®è¡¥å…¨ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šSCARçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç¨³å®šæ€§å’Œé€šç”¨æ€§ï¼Œèƒ½å¤Ÿåœ¨æ•°æ®é›†æ‰©å±•è¿‡ç¨‹ä¸­ä¿æŒç‰¹å¾ä¸å˜ï¼ŒåŒºåˆ«äºä»¥å¾€æ–¹æ³•å¯¹æ•°æ®æ•°é‡çš„å•ä¸€å…³æ³¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSCARé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®æ¥ä¼˜åŒ–æ¯ä¸ªæŒ‡æ ‡çš„è®¡ç®—ï¼Œç¡®ä¿åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­æœ‰æ•ˆæ•æ‰åˆ°æ•°æ®çš„ç»“æ„æ€§ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSCARåœ¨å¤šç§å¤šæ¨¡æ€æ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹æ•°æ®æ•ˆç”¨ï¼ŒæŒ‡å¯¼æ•°æ®è·å–ï¼Œæå‡äº†æ•°æ®é›†ç‰¹æ€§æ‰©å±•çš„æ•ˆç‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SCARæ–¹æ¡ˆåœ¨å¤šæ¨¡æ€æ•°æ®é›†çš„ç‰¹æ€§åˆ†æã€æ•°æ®è·å–å’Œä¼˜åŒ–è®­ç»ƒç­–ç•¥ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶èƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£æ•°æ®é›†ç‰¹æ€§ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models exhibit remarkable generalization across diverse tasks, largely driven by the characteristics of their training data. Recent data-centric methods like pruning and compression aim to optimize training but offer limited theoretical insight into how data properties affect generalization, especially the data characteristics in sample scaling. Traditional perspectives further constrain progress by focusing predominantly on data quantity and training efficiency, often overlooking structural aspects of data quality. In this study, we introduce SCAR, a principled scheme for characterizing the intrinsic structural properties of datasets across four key measures: Scale, Coverage, Authenticity, and Richness. Unlike prior data-centric measures, SCAR captures stable characteristics that remain invariant under dataset scaling, providing a robust and general foundation for data understanding. Leveraging these structural properties, we introduce Foundation Data-a minimal subset that preserves the generalization behavior of the full dataset without requiring model-specific retraining. We model single-modality tasks as step functions and estimate the distribution of the foundation data size to capture step-wise generalization bias across modalities in the target multi-modal dataset. Finally, we develop a SCAR-guided data completion strategy based on this generalization bias, which enables efficient, modality-aware expansion of modality-specific characteristics in multimodal datasets. Experiments across diverse multi-modal datasets and model architectures validate the effectiveness of SCAR in predicting data utility and guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

