---
layout: default
title: PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense
---

# PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19488" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19488v1</a>
  <a href="https://arxiv.org/pdf/2508.19488.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19488v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19488v1', 'PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xavier Cadet, Simona Boboila, Sie Hendrata Dharmawan, Alina Oprea, Peter Chin

**åˆ†ç±»**: cs.LG, cs.AI, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**å¤‡æ³¨**: Accepted at GameSec 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPoolFlipä»¥è§£å†³ç½‘ç»œé˜²å¾¡ä¸­çš„å†³ç­–è‡ªåŠ¨åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç½‘ç»œå®‰å…¨` `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `FlipItæ¸¸æˆ` `è‡ªé€‚åº”é˜²å¾¡` `å†³ç­–è‡ªåŠ¨åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„FlipItæ¡†æ¶ä¾èµ–äºå°‘é‡å¯å‘å¼æˆ–ä¸“é—¨å­¦ä¹ æŠ€æœ¯ï¼Œå¯¼è‡´é˜²å¾¡è€…åœ¨é¢å¯¹æ–°æ”»å‡»æ—¶çš„è„†å¼±æ€§å’Œé€‚åº”èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºPoolFlipå¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼Œæ‰©å±•FlipItæ¸¸æˆï¼Œç»“åˆFlip-PSROæ–¹æ³•ï¼Œåˆ©ç”¨äººç¾¤è®­ç»ƒæå‡é˜²å¾¡è€…çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFlip-PSROé˜²å¾¡è€…åœ¨é¢å¯¹æœªåœ¨è®­ç»ƒä¸­æš´éœ²çš„æ”»å‡»æ—¶ï¼Œå…¶æ•ˆæœæ˜¯åŸºçº¿çš„ä¸¤å€ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç½‘ç»œé˜²å¾¡éœ€è¦åœ¨éšè”½ã€æ¬ºéª—å’ŒæŒç»­æ¼”å˜çš„å¯¹æŠ—ç­–ç•¥ä¸‹è‡ªåŠ¨åŒ–å†³ç­–ã€‚FlipItæ¸¸æˆä¸ºå»ºæ¨¡é˜²å¾¡è€…ä¸å…ˆè¿›å¯¹æ‰‹ä¹‹é—´çš„äº’åŠ¨æä¾›äº†åŸºç¡€æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„FlipItæ¡†æ¶ä¾èµ–å°‘é‡å¯å‘å¼æˆ–ä¸“é—¨å­¦ä¹ æŠ€æœ¯ï¼Œå¯¼è‡´è„†å¼±æ€§å’Œæ— æ³•é€‚åº”æ–°æ”»å‡»ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†PoolFlipï¼Œä¸€ä¸ªæ‰©å±•FlipItæ¸¸æˆçš„å¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼Œä»¥ä¾¿äºæ”»å‡»è€…å’Œé˜²å¾¡è€…çš„é«˜æ•ˆå­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†Flip-PSROï¼Œä¸€ç§åˆ©ç”¨åŸºäºäººç¾¤çš„è®­ç»ƒçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è®­ç»ƒèƒ½å¤Ÿå¯¹æŠ—ä¸€ç³»åˆ—æœªçŸ¥ã€æ½œåœ¨è‡ªé€‚åº”å¯¹æ‰‹çš„é˜²å¾¡è€…ã€‚å®éªŒè¯æ˜ï¼ŒFlip-PSROé˜²å¾¡è€…åœ¨é¢å¯¹æœªåœ¨è®­ç»ƒä¸­æš´éœ²çš„å¯å‘å¼æ”»å‡»æ—¶ï¼Œå…¶æ•ˆæœæ˜¯åŸºçº¿çš„ä¸¤å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç½‘ç»œé˜²å¾¡ä¸­è‡ªåŠ¨åŒ–å†³ç­–çš„æŒ‘æˆ˜ï¼Œç°æœ‰çš„FlipItæ¡†æ¶åœ¨é¢å¯¹æ–°å‹æ”»å‡»æ—¶è¡¨ç°å‡ºè„†å¼±æ€§ï¼Œæ— æ³•æœ‰æ•ˆé€‚åº”å¯¹æ‰‹çš„å˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥PoolFlipå¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼Œæ‰©å±•FlipItæ¸¸æˆï¼Œå¹¶ç»“åˆFlip-PSROæ–¹æ³•ï¼Œåˆ©ç”¨äººç¾¤è®­ç»ƒæ¥æå‡é˜²å¾¡è€…çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåº”å¯¹æœªçŸ¥çš„è‡ªé€‚åº”å¯¹æ‰‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬PoolFlipç¯å¢ƒå’ŒFlip-PSROè®­ç»ƒæµç¨‹ã€‚PoolFlipç¯å¢ƒä¸ºæ”»å‡»è€…å’Œé˜²å¾¡è€…æä¾›äº†ä¸€ä¸ªå…±äº«èµ„æºçš„ç«äº‰åœºæ™¯ï¼Œè€ŒFlip-PSROåˆ™é€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒé˜²å¾¡è€…ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºFlip-PSROæ–¹æ³•çš„æå‡ºï¼Œå®ƒé€šè¿‡äººç¾¤è®­ç»ƒçš„æ–¹å¼ï¼Œä½¿å¾—é˜²å¾¡è€…èƒ½å¤Ÿåœ¨é¢å¯¹æœªè§è¿‡çš„æ”»å‡»æ—¶ï¼Œä¿æŒé«˜æ•ˆçš„æ§åˆ¶å’Œä¼˜åŒ–æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒFlip-PSROé‡‡ç”¨äº†åŸºäºæ‰€æœ‰æƒçš„æ•ˆç”¨å‡½æ•°ï¼Œç¡®ä¿é˜²å¾¡è€…åœ¨ä¼˜åŒ–æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿç»´æŒè¾ƒé«˜çš„æ§åˆ¶æ°´å¹³ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFlip-PSROé˜²å¾¡è€…åœ¨é¢å¯¹æœªåœ¨è®­ç»ƒä¸­æš´éœ²çš„å¯å‘å¼æ”»å‡»æ—¶ï¼Œå…¶æ•ˆæœæ˜¯åŸºçº¿çš„ä¸¤å€ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç½‘ç»œå®‰å…¨ã€ä¿¡æ¯ç³»ç»Ÿä¿æŠ¤å’Œæ™ºèƒ½é˜²å¾¡ç³»ç»Ÿã€‚é€šè¿‡æé«˜é˜²å¾¡è€…çš„é€‚åº”èƒ½åŠ›å’Œå†³ç­–æ•ˆç‡ï¼ŒPoolFlipå’ŒFlip-PSROæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹ä¸æ–­æ¼”å˜çš„ç½‘ç»œæ”»å‡»ï¼Œä¸ºä¼ä¸šå’Œç»„ç»‡æä¾›æ›´å¼ºçš„å®‰å…¨ä¿éšœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Cyber defense requires automating defensive decision-making under stealthy, deceptive, and continuously evolving adversarial strategies. The FlipIt game provides a foundational framework for modeling interactions between a defender and an advanced adversary that compromises a system without being immediately detected. In FlipIt, the attacker and defender compete to control a shared resource by performing a Flip action and paying a cost. However, the existing FlipIt frameworks rely on a small number of heuristics or specialized learning techniques, which can lead to brittleness and the inability to adapt to new attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym environment that extends the FlipIt game to allow efficient learning for attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent reinforcement learning (MARL) approach that leverages population-based training to train defender agents equipped to generalize against a range of unknown, potentially adaptive opponents. Our empirical results suggest that Flip-PSRO defenders are $2\times$ more effective than baselines to generalize to a heuristic attack not exposed in training. In addition, our newly designed ownership-based utility functions ensure that Flip-PSRO defenders maintain a high level of control while optimizing performance.

