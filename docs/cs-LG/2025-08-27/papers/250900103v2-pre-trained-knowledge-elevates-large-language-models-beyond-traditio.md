---
layout: default
title: Pre-trained knowledge elevates large language models beyond traditional chemical reaction optimizers
---

# Pre-trained knowledge elevates large language models beyond traditional chemical reaction optimizers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00103" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00103v2</a>
  <a href="https://arxiv.org/pdf/2509.00103.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00103v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00103v2', 'Pre-trained knowledge elevates large language models beyond traditional chemical reaction optimizers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Robert MacKnight, Jose Emilio Regio, Jeffrey G. Ethier, Luke A. Baldwin, Gabe Gomes

**åˆ†ç±»**: cs.LG, cs.AI, physics.chem-ph

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27 (æ›´æ–°: 2025-10-27)

**å¤‡æ³¨**: 27 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ–å­¦ååº”ä¼˜åŒ–ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ–å­¦ååº”ä¼˜åŒ–` `å¤§è¯­è¨€æ¨¡å‹` `è´å¶æ–¯ä¼˜åŒ–` `é¢„è®­ç»ƒçŸ¥è¯†` `å‚æ•°ç©ºé—´æ¢ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒ–å­¦ååº”ä¼˜åŒ–æ–¹æ³•ä¸»è¦ä¾èµ–äºé»‘ç®±æœç´¢ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¤æ‚çš„å‚æ•°ç©ºé—´ï¼Œå°¤å…¶åœ¨é«˜æ€§èƒ½æ¡ä»¶ç¨€ç¼ºæ—¶è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†LLMå¼•å¯¼çš„ä¼˜åŒ–ï¼ˆLLM-GOï¼‰ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†æé«˜åœ¨å¤æ‚åˆ†ç±»ç©ºé—´ä¸­çš„ä¼˜åŒ–æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç†µæ¢ç´¢ä¸­è¡¨ç°ä¼˜è¶Šã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM-GOåœ¨äº”ä¸ªå•ç›®æ ‡æ•°æ®é›†ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„è´å¶æ–¯ä¼˜åŒ–ï¼Œå°¤å…¶åœ¨å‚æ•°å¤æ‚æ€§å¢åŠ æ—¶ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å®éªŒåŒ–å­¦ä¸­çš„ä¼˜åŒ–é€šå¸¸ä¾èµ–äºé»‘ç®±å‚æ•°ç©ºé—´çš„ç®—æ³•æœç´¢ã€‚æœ¬æ–‡å±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„é¢„è®­ç»ƒçŸ¥è¯†å¦‚ä½•æ ¹æœ¬æ”¹å˜è¿™ä¸€èŒƒå¼ã€‚é€šè¿‡å¯¹å…­ä¸ªå®Œå…¨æšä¸¾çš„åˆ†ç±»ååº”æ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒLLMå¼•å¯¼çš„ä¼˜åŒ–ï¼ˆLLM-GOï¼‰åœ¨äº”ä¸ªå•ç›®æ ‡æ•°æ®é›†ä¸ŠæŒç»­åŒ¹é…æˆ–è¶…è¶Šè´å¶æ–¯ä¼˜åŒ–ï¼ˆBOï¼‰çš„è¡¨ç°ï¼Œå°¤å…¶åœ¨å‚æ•°å¤æ‚æ€§å¢åŠ å’Œé«˜æ€§èƒ½æ¡ä»¶ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼ˆ<5%ç©ºé—´ï¼‰ã€‚BOä»…åœ¨æ˜ç¡®çš„å¤šç›®æ ‡æƒè¡¡ä¸­ä¿æŒä¼˜åŠ¿ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ‹“æ‰‘æ— å…³çš„ä¿¡æ¯ç†è®ºæ¡†æ¶ï¼Œé‡åŒ–ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„é‡‡æ ·å¤šæ ·æ€§ï¼Œå‘ç°LLMsåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šä¿æŒç³»ç»Ÿæ€§æ›´é«˜çš„æ¢ç´¢é¦™å†œç†µï¼Œè¡¨æ˜é¢„è®­ç»ƒçš„é¢†åŸŸçŸ¥è¯†ä½¿å¾—åœ¨åŒ–å­¦å‚æ•°ç©ºé—´çš„å¯¼èˆªæ›´ä¸ºæœ‰æ•ˆï¼Œè€Œéæ›¿ä»£ç»“æ„åŒ–çš„æ¢ç´¢ç­–ç•¥ã€‚ä¸ºä¿ƒè¿›é€æ˜çš„åŸºå‡†æµ‹è¯•å’Œç¤¾åŒºéªŒè¯ï¼Œæˆ‘ä»¬å‘å¸ƒäº†Iron Mindå¹³å°ï¼Œæ”¯æŒäººç±»ã€ç®—æ³•å’ŒLLMä¼˜åŒ–æ´»åŠ¨çš„å¹¶è¡Œè¯„ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸåŒ–å­¦ååº”ä¼˜åŒ–æ–¹æ³•åœ¨å¤æ‚å‚æ•°ç©ºé—´ä¸­çš„ä½æ•ˆé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜æ€§èƒ½æ¡ä»¶ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ‰¾åˆ°æœ‰æ•ˆè§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼ŒLLM-GOèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢åŒ–å­¦å‚æ•°ç©ºé—´ï¼Œæå‡ä¼˜åŒ–æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é«˜ç†µæ¢ç´¢ä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†å‡†å¤‡ã€LLMè®­ç»ƒã€ä¼˜åŒ–ç­–ç•¥å®æ–½å’Œç»“æœè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æ¶µç›–äº†å¤šç§åŒ–å­¦ååº”ï¼ŒLLMé€šè¿‡é¢„è®­ç»ƒè·å–é¢†åŸŸçŸ¥è¯†ï¼Œä¼˜åŒ–ç­–ç•¥åˆ™åŸºäºLLMçš„è¾“å‡ºè¿›è¡Œå‚æ•°é€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†LLMsåº”ç”¨äºåŒ–å­¦ååº”ä¼˜åŒ–ï¼Œåˆ©ç”¨å…¶é¢„è®­ç»ƒçŸ¥è¯†å®ç°æ›´é«˜æ•ˆçš„æ¢ç´¢ï¼Œå°¤å…¶åœ¨ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥åº”å¯¹çš„å¤æ‚åˆ†ç±»ç©ºé—´ä¸­è¡¨ç°çªå‡ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–LLMçš„è¾“å‡ºè´¨é‡ï¼Œå¹¶é€šè¿‡è°ƒèŠ‚è¶…å‚æ•°æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç¡®ä¿åœ¨ä¸åŒæ•°æ®é›†ä¸Šå‡èƒ½ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLM-GOåœ¨äº”ä¸ªå•ç›®æ ‡æ•°æ®é›†ä¸Šå‡è¶…è¶Šäº†è´å¶æ–¯ä¼˜åŒ–ï¼Œå°¤å…¶åœ¨å‚æ•°å¤æ‚æ€§å¢åŠ æ—¶ï¼ŒLLM-GOçš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ï¼Œæ¢ç´¢é¦™å†œç†µç³»ç»Ÿæ€§æ›´é«˜ï¼Œè¡¨æ˜å…¶åœ¨è§£å†³ç¨€ç¼ºæ¡ä»¶ä¸‹çš„ä¼˜åŒ–é—®é¢˜ä¸­å…·æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ–å­¦åˆæˆã€è¯ç‰©å‘ç°å’Œææ–™è®¾è®¡ç­‰ã€‚é€šè¿‡æå‡åŒ–å­¦ååº”ä¼˜åŒ–çš„æ•ˆç‡ï¼ŒLLM-GOèƒ½å¤ŸåŠ é€Ÿæ–°ææ–™å’Œè¯ç‰©çš„å¼€å‘ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern optimization in experimental chemistry employs algorithmic search through black-box parameter spaces. Here we demonstrate that pre-trained knowledge in large language models (LLMs) fundamentally changes this paradigm. Using six fully enumerated categorical reaction datasets (768-5,684 experiments), we benchmark LLM-guided optimization (LLM-GO) against Bayesian optimization (BO) and random sampling. Frontier LLMs consistently match or exceed BO performance across five single-objective datasets, with advantages growing as parameter complexity increases and high-performing conditions become scarce (<5% of space). BO retains superiority only for explicit multi-objective trade-offs. To understand these contrasting behaviors, we introduce a topology-agnostic information theory framework quantifying sampling diversity throughout optimization campaigns. This analysis reveals that LLMs maintain systematically higher exploration Shannon entropy than BO across all datasets while achieving superior performance, with advantages most pronounced in solution-scarce parameter spaces where high-entropy exploration typically fails-suggesting that pre-trained domain knowledge enables more effective navigation of chemical parameter space rather than replacing structured exploration strategies. To enable transparent benchmarking and community validation, we release Iron Mind (https://gomes.andrew.cmu.edu/iron-mind), a no-code platform for side-by-side evaluation of human, algorithmic, and LLM optimization campaigns with public leaderboards and complete trajectories. Our findings establish that LLM-GO excels precisely where traditional methods struggle: complex categorical spaces requiring domain understanding rather than mathematical optimization.

