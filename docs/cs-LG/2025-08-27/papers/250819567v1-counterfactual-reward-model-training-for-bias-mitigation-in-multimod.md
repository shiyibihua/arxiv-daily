---
layout: default
title: Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning
---

# Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19567" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19567v1</a>
  <a href="https://arxiv.org/pdf/2508.19567.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19567v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19567v1', 'Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sheryl Mathew, N Harshit

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåäº‹å®å¥–åŠ±æ¨¡å‹ä»¥ç¼“è§£å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¸­çš„åè§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åäº‹å®æ¨æ–­` `å¤šæ¨¡æ€å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `åè§ç¼“è§£` `å…¬å¹³æ€§` `å¥–åŠ±æ¨¡å‹` `å› æœæ¨æ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åè§ç¼“è§£æ–¹æ³•å¤šé‡‡ç”¨è¢«åŠ¨çº¦æŸï¼Œå®¹æ˜“åœ¨å› æœæ··æ·†ä¸‹å¤±æ•ˆï¼Œå¯¼è‡´ç­–ç•¥ä¼˜åŒ–ä¸ç†æƒ³ã€‚
2. æœ¬æ–‡æå‡ºçš„åäº‹å®å¥–åŠ±æ¨¡å‹ç»“åˆå› æœæ¨æ–­å’Œå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œæä¾›äº†ä¸€ç§æ— ç›‘ç£çš„åè§æŠµæŠ—å¥–åŠ±ä¿¡å·ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨å‡æ–°é—»æ£€æµ‹ä¸­å‡†ç¡®ç‡è¾¾åˆ°89.12%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶æœ‰æ•ˆé™ä½äº†åè§å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¥–åŠ±æ¨¡å‹å¯èƒ½ä¼šå­¦ä¹ å¹¶æ”¾å¤§å¤šæ¨¡æ€æ•°æ®é›†ä¸­çš„æ½œåœ¨åè§ï¼Œä»è€Œå¯¼è‡´ä¸å®Œå–„çš„ç­–ç•¥ä¼˜åŒ–å’Œå…¬å¹³æ€§é™ä½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åäº‹å®å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥å› æœæ¨æ–­ä¸å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œæä¾›äº†ä¸€ç§æ— ç›‘ç£çš„ã€æŠ—åè§çš„å¥–åŠ±ä¿¡å·ã€‚æ ¸å¿ƒè´¡çŒ®æ˜¯åäº‹å®ä¿¡ä»»è¯„åˆ†ï¼Œè¯¥è¯„åˆ†ç”±å››ä¸ªç»„æˆéƒ¨åˆ†æ„æˆï¼Œç»è¿‡åœ¨å¤šæ¨¡æ€å‡æ–°é—»æ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œç³»ç»Ÿåœ¨å‡æ–°é—»æ£€æµ‹ä¸­è¾¾åˆ°äº†89.12%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†åŸºçº¿å¥–åŠ±æ¨¡å‹ï¼Œå¹¶å‡å°‘äº†è™šå‡ç›¸å…³æ€§å’Œä¸å…¬å¹³çš„å¼ºåŒ–ä¿¡å·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¥–åŠ±æ¨¡å‹å­¦ä¹ å¹¶æ”¾å¤§æ½œåœ¨åè§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–è¢«åŠ¨çº¦æŸï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å› æœæ··æ·†ï¼Œå¯¼è‡´ç­–ç•¥ä¼˜åŒ–ä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„åäº‹å®å¥–åŠ±æ¨¡å‹é€šè¿‡å¼•å…¥å› æœæ¨æ–­ï¼Œç»“åˆå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œæä¾›äº†ä¸€ç§æ— ç›‘ç£çš„ã€æŠ—åè§çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæé«˜äº†ç­–ç•¥ä¼˜åŒ–çš„å…¬å¹³æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åäº‹å®ä¿¡ä»»è¯„åˆ†çš„è®¡ç®—ï¼Œè¯¥è¯„åˆ†ç”±å››ä¸ªéƒ¨åˆ†ç»„æˆï¼šåäº‹å®åç§»ã€é‡æ„ä¸ç¡®å®šæ€§ã€å…¬å¹³æ€§è§„åˆ™çš„è¿åæƒ…å†µä»¥åŠä¸åŠ¨æ€ä¿¡ä»»åº¦ç›¸å…³çš„æ—¶é—´å¥–åŠ±åç§»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯åäº‹å®ä¿¡ä»»è¯„åˆ†çš„è®¾è®¡ï¼Œå®ƒå°†æ”¿æ²»æ¡†æ¶åè§ä¸ä¸»é¢˜åè§åˆ†è§£å¼€æ¥ï¼Œå¹¶é€šè¿‡åŠ¨æ€ä¿¡ä»»åº¦æ¥è°ƒæ•´å¥–åŠ±ä¿¡å·ï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åˆæˆåè§æ³¨å…¥çš„æ–¹æ³•ï¼Œé€šè¿‡åºåˆ—æ‰¹æ¬¡æµ‹è¯•æ¨¡å‹çš„é²æ£’æ€§ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æç³»ç»Ÿåœ¨å‡æ–°é—»æ£€æµ‹ä¸­è¾¾åˆ°äº†89.12%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿å¥–åŠ±æ¨¡å‹ï¼Œå‡å°‘äº†è™šå‡ç›¸å…³æ€§å’Œä¸å…¬å¹³çš„å¼ºåŒ–ä¿¡å·ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–°é—»éªŒè¯ã€ç¤¾äº¤åª’ä½“å†…å®¹ç›‘æµ‹ä»¥åŠä»»ä½•éœ€è¦å…¬å¹³æ€§å’Œå‡†ç¡®æ€§çš„å¤šæ¨¡æ€å†³ç­–ç³»ç»Ÿã€‚é€šè¿‡æä¾›æŠ—åè§çš„å¥–åŠ±ä¿¡å·ï¼Œèƒ½å¤Ÿåœ¨åŠ¨æ€å®æ—¶æ”¿ç­–åˆ¶å®šä¸­æé«˜å†³ç­–çš„å¯é æ€§ï¼Œä¿ƒè¿›å…¬å¹³æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œå¦‚åŒ»ç–—ã€é‡‘èç­‰ï¼Œæå‡ç³»ç»Ÿçš„å…¬å¹³æ€§å’Œé€æ˜åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In reinforcement learning with human feedback (RLHF), reward models can efficiently learn and amplify latent biases within multimodal datasets, which can lead to imperfect policy optimization through flawed reward signals and decreased fairness. Bias mitigation studies have often applied passive constraints, which can fail under causal confounding. Here, we present a counterfactual reward model that introduces causal inference with multimodal representation learning to provide an unsupervised, bias-resilient reward signal. The heart of our contribution is the Counterfactual Trust Score, an aggregated score consisting of four components: (1) counterfactual shifts that decompose political framing bias from topical bias; (2) reconstruction uncertainty during counterfactual perturbations; (3) demonstrable violations of fairness rules for each protected attribute; and (4) temporal reward shifts aligned with dynamic trust measures. We evaluated the framework on a multimodal fake versus true news dataset, which exhibits framing bias, class imbalance, and distributional drift. Following methodologies similar to unsupervised drift detection from representation-based distances [1] and temporal robustness benchmarking in language models [2], we also inject synthetic bias across sequential batches to test robustness. The resulting system achieved an accuracy of 89.12% in fake news detection, outperforming the baseline reward models. More importantly, it reduced spurious correlations and unfair reinforcement signals. This pipeline outlines a robust and interpretable approach to fairness-aware RLHF, offering tunable bias reduction thresholds and increasing reliability in dynamic real-time policy making.

