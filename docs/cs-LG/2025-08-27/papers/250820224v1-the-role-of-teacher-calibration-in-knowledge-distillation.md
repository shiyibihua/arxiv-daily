---
layout: default
title: The Role of Teacher Calibration in Knowledge Distillation
---

# The Role of Teacher Calibration in Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20224" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20224v1</a>
  <a href="https://arxiv.org/pdf/2508.20224.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20224v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20224v1', 'The Role of Teacher Calibration in Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Suyoung Kim, Seonguk Park, Junhoo Lee, Nojun Kwak

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**æœŸåˆŠ**: IEEE Access (2025)

**DOI**: [10.1109/ACCESS.2025.3585106](https://doi.org/10.1109/ACCESS.2025.3585106)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•™å¸ˆæ¨¡å‹æ ¡å‡†æ–¹æ³•ä»¥æå‡çŸ¥è¯†è’¸é¦æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `æ¨¡å‹æ ¡å‡†` `æ·±åº¦å­¦ä¹ ` `æ•™å¸ˆæ¨¡å‹` `å­¦ç”Ÿæ¨¡å‹` `æ€§èƒ½æå‡` `ç®—æ³•ä¼˜åŒ–` `æ¨¡å‹å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨æå‡å­¦ç”Ÿæ¨¡å‹æ€§èƒ½æ–¹é¢å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œå°¤å…¶æ˜¯æ•™å¸ˆæ¨¡å‹çš„æ ¡å‡†è¯¯å·®å¯¹å­¦ç”Ÿå‡†ç¡®æ€§å½±å“è¾ƒå¤§ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ ¡å‡†æ•™å¸ˆæ¨¡å‹æ¥é™ä½å…¶æ ¡å‡†è¯¯å·®ï¼Œä»è€Œæå‡çŸ¥è¯†è’¸é¦çš„æ•ˆæœï¼Œå¼ºè°ƒæ•™å¸ˆæ¨¡å‹æ ¡å‡†çš„é‡è¦æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¯¥æ ¡å‡†æ–¹æ³•åï¼ŒçŸ¥è¯†è’¸é¦åœ¨å¤šç§ä»»åŠ¡ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”ä¸ç°æœ‰æ–¹æ³•å…¼å®¹æ€§å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å·²æˆä¸ºæ·±åº¦å­¦ä¹ ä¸­ä¸€ç§æœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œèƒ½å¤Ÿå°†çŸ¥è¯†ä»å¤§å‹æ•™å¸ˆæ¨¡å‹è½¬ç§»åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹ã€‚å°½ç®¡KDå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å°šä¸å®Œå…¨æ¸…æ¥šå“ªäº›å› ç´ æœ‰åŠ©äºæé«˜å­¦ç”Ÿçš„æ€§èƒ½ã€‚æœ¬æ–‡æ­ç¤ºäº†æ•™å¸ˆæ¨¡å‹çš„æ ¡å‡†è¯¯å·®ä¸å­¦ç”Ÿå‡†ç¡®æ€§ä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œè®¤ä¸ºæ•™å¸ˆæ¨¡å‹çš„æ ¡å‡†æ˜¯æœ‰æ•ˆKDçš„é‡è¦å› ç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡ç®€å•é‡‡ç”¨å‡å°‘æ•™å¸ˆæ ¡å‡†è¯¯å·®çš„æ ¡å‡†æ–¹æ³•ï¼Œå¯ä»¥æ”¹å–„KDçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç®—æ³•å…·æœ‰é€šç”¨æ€§ï¼Œåœ¨åˆ†ç±»åˆ°æ£€æµ‹ç­‰å¤šç§ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾ä¸ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•é›†æˆï¼Œå§‹ç»ˆå®ç°ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­æ•™å¸ˆæ¨¡å‹æ ¡å‡†ä¸è¶³å¯¼è‡´å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªå……åˆ†è€ƒè™‘æ•™å¸ˆæ¨¡å‹çš„æ ¡å‡†è¯¯å·®å¯¹å­¦ç”Ÿæ¨¡å‹çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ ¡å‡†æ•™å¸ˆæ¨¡å‹ä»¥é™ä½å…¶æ ¡å‡†è¯¯å·®ï¼Œä»è€Œæé«˜å­¦ç”Ÿæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å¼ºè°ƒæ•™å¸ˆæ¨¡å‹çš„æ ¡å‡†åœ¨çŸ¥è¯†è’¸é¦ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ ¡å‡†ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•™å¸ˆæ¨¡å‹çš„è®­ç»ƒã€æ ¡å‡†å’ŒçŸ¥è¯†è½¬ç§»ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆè®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œç„¶ååº”ç”¨æ ¡å‡†æ–¹æ³•ï¼Œæœ€åå°†æ ¡å‡†åçš„çŸ¥è¯†ä¼ é€’ç»™å­¦ç”Ÿæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ­ç¤ºäº†æ•™å¸ˆæ¨¡å‹æ ¡å‡†è¯¯å·®ä¸å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ ¡å‡†æ–¹æ³•æ¥æ”¹å–„çŸ¥è¯†è’¸é¦æ•ˆæœã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬ç ”ç©¶å¼ºè°ƒäº†æ•™å¸ˆæ¨¡å‹æ ¡å‡†çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ•™å¸ˆæ¨¡å‹çš„æ ¡å‡†ï¼Œå¹¶åœ¨ä¸åŒä»»åŠ¡ä¸­æµ‹è¯•äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ ¡å‡†æ–¹æ³•åï¼ŒçŸ¥è¯†è’¸é¦åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡å®ç°äº†è¶…è¿‡10%çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨ä¸ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ç»“åˆæ—¶ï¼Œå§‹ç»ˆä¿æŒä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿçš„åœºæ™¯ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚é€šè¿‡æå‡çŸ¥è¯†è’¸é¦çš„æ•ˆæœï¼Œå¯ä»¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ï¼Œæœªæ¥å¯èƒ½å¯¹è¾¹ç¼˜è®¡ç®—å’Œç§»åŠ¨è®¾å¤‡åº”ç”¨äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge Distillation (KD) has emerged as an effective model compression technique in deep learning, enabling the transfer of knowledge from a large teacher model to a compact student model. While KD has demonstrated significant success, it is not yet fully understood which factors contribute to improving the student's performance. In this paper, we reveal a strong correlation between the teacher's calibration error and the student's accuracy. Therefore, we claim that the calibration of the teacher model is an important factor for effective KD. Furthermore, we demonstrate that the performance of KD can be improved by simply employing a calibration method that reduces the teacher's calibration error. Our algorithm is versatile, demonstrating effectiveness across various tasks from classification to detection. Moreover, it can be easily integrated with existing state-of-the-art methods, consistently achieving superior performance.

