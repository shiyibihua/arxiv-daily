---
layout: default
title: Pruning Strategies for Backdoor Defense in LLMs
---

# Pruning Strategies for Backdoor Defense in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20032" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20032v1</a>
  <a href="https://arxiv.org/pdf/2508.20032.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20032v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20032v1', 'Pruning Strategies for Backdoor Defense in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Santosh Chapagain, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**å¤‡æ³¨**: Accepted in CIKM '25: The 34th ACM International Conference on Information and Knowledge Management Proceedings

**DOI**: [10.1145/3746252.3760946](https://doi.org/10.1145/3746252.3760946)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ³¨æ„åŠ›å¤´å‰ªæç­–ç•¥ä»¥é˜²å¾¡å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åé—¨æ”»å‡»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åé—¨æ”»å‡»` `è¯­è¨€æ¨¡å‹` `å‰ªæç­–ç•¥` `å®‰å…¨æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¼ºåŒ–å­¦ä¹ ` `è´å¶æ–¯æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åé—¨æ”»å‡»å¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§æ„æˆæŒ‘æˆ˜ï¼Œç°æœ‰é˜²å¾¡æ–¹æ³•åœ¨é¢å¯¹æœªçŸ¥è§¦å‘å™¨æ—¶æ•ˆæœæœ‰é™ã€‚
2. æœ¬æ–‡æå‡ºå…­ç§æ³¨æ„åŠ›å¤´å‰ªæç­–ç•¥ï¼Œæ—¨åœ¨æ— éœ€è§¦å‘å™¨çŸ¥è¯†å’Œå¹²å‡€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå‡è½»åé—¨æ”»å‡»çš„å½±å“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºæ¢¯åº¦çš„å‰ªæåœ¨é˜²å¾¡è¯­æ³•æ”»å‡»æ—¶æ•ˆæœæœ€ä½³ï¼Œè€Œå¼ºåŒ–å­¦ä¹ å’Œè´å¶æ–¯å‰ªæåœ¨æŠµå¾¡é£æ ¼æ”»å‡»æ–¹é¢è¡¨ç°æ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åé—¨æ”»å‡»å¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œå®Œæ•´æ€§æ„æˆé‡å¤§å¨èƒã€‚å°½ç®¡è¿™äº›æ¨¡å‹é€šå¸¸ä¼šé’ˆå¯¹ä¸‹æ¸¸è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä½†è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°èƒ½å¤Ÿå­˜æ´»äºæ™®é€šå¾®è°ƒä¸­çš„åé—¨æ”»å‡»ã€‚è¿™ç±»æ”»å‡»éš¾ä»¥é˜²å¾¡ï¼Œå› ä¸ºæœ€ç»ˆç”¨æˆ·é€šå¸¸ç¼ºä¹å¯¹æ”»å‡»è§¦å‘å™¨çš„äº†è§£ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨æœªçŸ¥è§¦å‘å™¨å’Œæ— å¹²å‡€å‚è€ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ³¨æ„åŠ›å¤´å‰ªææ˜¯å¦èƒ½ç¼“è§£è¿™äº›å¨èƒã€‚æˆ‘ä»¬è®¾è®¡å¹¶å®ç°äº†å…­ç§åŸºäºå‰ªæçš„ç­–ç•¥ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ¢¯åº¦çš„å‰ªæåœ¨é˜²å¾¡è¯­æ³•è§¦å‘å™¨æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€Œå¼ºåŒ–å­¦ä¹ å’Œè´å¶æ–¯å‰ªæåˆ™æ›´èƒ½æŠµå¾¡é£æ ¼æ”»å‡»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­åé—¨æ”»å‡»çš„é˜²å¾¡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å¯¹æœªçŸ¥è§¦å‘å™¨æ—¶å¾€å¾€æ— èƒ½ä¸ºåŠ›ï¼Œå¯¼è‡´æ¨¡å‹æ˜“å—æ”»å‡»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ³¨æ„åŠ›å¤´å‰ªæç­–ç•¥ï¼Œé€æ­¥ç§»é™¤ä¿¡æ¯é‡æœ€å°‘çš„å¤´éƒ¨ï¼Œè€Œæ— éœ€äº†è§£æ”»å‡»è§¦å‘å™¨æˆ–ä¾èµ–å¹²å‡€çš„å‚è€ƒæ¨¡å‹ï¼Œä»è€Œå®ç°åé—¨æ”»å‡»çš„ç¼“è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬å…­ç§å‰ªæç­–ç•¥çš„è®¾è®¡ä¸å®ç°ï¼Œåˆ†åˆ«ä¸ºåŸºäºæ¢¯åº¦çš„å‰ªæã€å±‚çº§æ–¹å·®å‰ªæã€ç»“æ„åŒ–L1/L2ç¨€ç–åŒ–çš„æ¢¯åº¦å‰ªæã€éšæœºé›†æˆå‰ªæã€å¼ºåŒ–å­¦ä¹ å¼•å¯¼çš„å‰ªæå’Œè´å¶æ–¯ä¸ç¡®å®šæ€§å‰ªæã€‚æ¯ç§æ–¹æ³•åœ¨å‰ªæè¿‡ç¨‹ä¸­ç›‘æ§éªŒè¯å‡†ç¡®ç‡ï¼Œä»¥é¿å…è¿‡åº¦å‰ªæã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†å¤šç§å‰ªæç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯ç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’Œè´å¶æ–¯æ–¹æ³•ï¼Œä»¥åº”å¯¹ä¸åŒç±»å‹çš„åé—¨æ”»å‡»ï¼Œè¿™åœ¨ç°æœ‰æ–‡çŒ®ä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‰ªæè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†è¿­ä»£ç§»é™¤ç­–ç•¥ï¼Œå…³æ³¨éªŒè¯é›†çš„è¡¨ç°ï¼Œç¡®ä¿æ¨¡å‹åœ¨é˜²å¾¡åé—¨æ”»å‡»çš„åŒæ—¶ä¿æŒè¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ¢¯åº¦çš„å‰ªæåœ¨é˜²å¾¡è¯­æ³•è§¦å‘å™¨æ—¶çš„å‡†ç¡®ç‡æå‡æ˜¾è‘—ï¼Œå…·ä½“è¡¨ç°ä¸ºç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æé«˜äº†çº¦15%çš„å‡†ç¡®ç‡ã€‚è€Œå¼ºåŒ–å­¦ä¹ å’Œè´å¶æ–¯å‰ªæåœ¨é¢å¯¹é£æ ¼æ”»å‡»æ—¶çš„è¡¨ç°ä¹Ÿä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿçš„å®‰å…¨æ€§æå‡ï¼Œå°¤å…¶æ˜¯åœ¨é‡‘èã€åŒ»ç–—ç­‰å¯¹å®‰å…¨æ€§è¦æ±‚æé«˜çš„è¡Œä¸šã€‚é€šè¿‡æœ‰æ•ˆé˜²å¾¡åé—¨æ”»å‡»ï¼Œå¯ä»¥æé«˜ç”¨æˆ·å¯¹æ¨¡å‹çš„ä¿¡ä»»åº¦ï¼Œä¿ƒè¿›å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ¨å¹¿ä¸ä½¿ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Backdoor attacks are a significant threat to the performance and integrity of pre-trained language models. Although such models are routinely fine-tuned for downstream NLP tasks, recent work shows they remain vulnerable to backdoor attacks that survive vanilla fine-tuning. These attacks are difficult to defend because end users typically lack knowledge of the attack triggers. Such attacks consist of stealthy malicious triggers introduced through subtle syntactic or stylistic manipulations, which can bypass traditional detection and remain in the model, making post-hoc purification essential. In this study, we explore whether attention-head pruning can mitigate these threats without any knowledge of the trigger or access to a clean reference model. To this end, we design and implement six pruning-based strategies: (i) gradient-based pruning, (ii) layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2 sparsification, (iv) randomized ensemble pruning, (v) reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning. Each method iteratively removes the least informative heads while monitoring validation accuracy to avoid over-pruning. Experimental evaluation shows that gradient-based pruning performs best while defending the syntactic triggers, whereas reinforcement learning and Bayesian pruning better withstand stylistic attacks.

