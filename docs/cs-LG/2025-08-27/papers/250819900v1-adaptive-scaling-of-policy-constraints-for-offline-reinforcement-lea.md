---
layout: default
title: Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning
---

# Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19900" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19900v1</a>
  <a href="https://arxiv.org/pdf/2508.19900.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19900v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19900v1', 'Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tan Jing, Xiaorui Li, Chao Yao, Xiaojuan Ban, Yuetong Fang, Renjing Xu, Zhaolin Yuan

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Colin-Jing/ASPC)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”ç¼©æ”¾ç­–ç•¥çº¦æŸä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„è¶…å‚æ•°è°ƒä¼˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥çº¦æŸ` `è‡ªé€‚åº”ç®—æ³•` `è¡Œä¸ºå…‹éš†` `è¶…å‚æ•°è°ƒèŠ‚` `åŠ¨æ€å¹³è¡¡` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ä¸åŒè´¨é‡æ•°æ®é›†æ—¶ï¼Œéœ€æ‰‹åŠ¨è°ƒèŠ‚è¶…å‚æ•°ï¼Œæ•ˆç‡ä½ä¸‹ä¸”ä¸å®ç”¨ã€‚
2. æœ¬æ–‡æå‡ºè‡ªé€‚åº”ç¼©æ”¾ç­–ç•¥çº¦æŸï¼ˆASPCï¼‰ï¼Œé€šè¿‡åŠ¨æ€å¹³è¡¡RLä¸è¡Œä¸ºå…‹éš†ï¼Œç®€åŒ–äº†è¶…å‚æ•°è°ƒèŠ‚è¿‡ç¨‹ã€‚
3. åœ¨39ä¸ªæ•°æ®é›†çš„å®éªŒä¸­ï¼ŒASPCåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸”ä»…éœ€å•ä¸€è¶…å‚æ•°é…ç½®ï¼Œè®¡ç®—å¼€é”€æå°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¤Ÿä»å›ºå®šæ•°æ®é›†ä¸­å­¦ä¹ æœ‰æ•ˆç­–ç•¥ï¼Œè€Œæ— éœ€ä¸ç¯å¢ƒäº¤äº’ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ç­–ç•¥çº¦æŸæ¥å‡è½»ç¦»çº¿RLè®­ç»ƒä¸­é‡åˆ°çš„åˆ†å¸ƒåç§»ã€‚ç„¶è€Œï¼Œç”±äºçº¦æŸçš„è§„æ¨¡åœ¨ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ä¹‹é—´å­˜åœ¨å·®å¼‚ï¼Œç°æœ‰æ–¹æ³•å¿…é¡»ä»”ç»†è°ƒæ•´è¶…å‚æ•°ä»¥åŒ¹é…æ¯ä¸ªæ•°æ®é›†ï¼Œè¿™æ—¢è€—æ—¶åˆä¸åˆ‡å®é™…ã€‚æœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”ç¼©æ”¾ç­–ç•¥çº¦æŸï¼ˆASPCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªäºŒé˜¶å¯å¾®çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€å¹³è¡¡RLå’Œè¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰ã€‚æˆ‘ä»¬ç†è®ºåˆ†æäº†å…¶æ€§èƒ½æå‡ä¿è¯ã€‚åœ¨å¯¹39ä¸ªæ•°æ®é›†è¿›è¡Œçš„å®éªŒä¸­ï¼ŒASPCä½¿ç”¨å•ä¸€è¶…å‚æ•°é…ç½®è¶…è¶Šäº†å…¶ä»–è‡ªé€‚åº”çº¦æŸæ–¹æ³•å’Œéœ€è¦é€æ•°æ®é›†è°ƒä¼˜çš„æœ€å…ˆè¿›ç¦»çº¿RLç®—æ³•ï¼ŒåŒæ—¶ä»…å¸¦æ¥äº†æœ€å°çš„è®¡ç®—å¼€é”€ã€‚ä»£ç å°†å‘å¸ƒäºhttps://github.com/Colin-Jing/ASPCã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ç”±äºæ•°æ®é›†è´¨é‡å·®å¼‚å¯¼è‡´çš„ç­–ç•¥çº¦æŸè§„æ¨¡ä¸ä¸€è‡´é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€æ‰‹åŠ¨è°ƒæ•´è¶…å‚æ•°ï¼Œé€ æˆæ•ˆç‡ä½ä¸‹å’Œä¸ä¾¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè‡ªé€‚åº”ç¼©æ”¾ç­–ç•¥çº¦æŸï¼ˆASPCï¼‰ï¼Œé€šè¿‡äºŒé˜¶å¯å¾®çš„æ¡†æ¶ï¼ŒåŠ¨æ€è°ƒæ•´å¼ºåŒ–å­¦ä¹ å’Œè¡Œä¸ºå…‹éš†ä¹‹é—´çš„å¹³è¡¡ï¼Œå‡å°‘å¯¹è¶…å‚æ•°çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šASPCæ¡†æ¶åŒ…æ‹¬æ•°æ®é›†åˆ†ææ¨¡å—ã€åŠ¨æ€çº¦æŸè°ƒæ•´æ¨¡å—å’Œç­–ç•¥å­¦ä¹ æ¨¡å—ã€‚é¦–å…ˆåˆ†ææ•°æ®é›†ç‰¹æ€§ï¼Œç„¶åæ ¹æ®åˆ†æç»“æœåŠ¨æ€è°ƒæ•´ç­–ç•¥çº¦æŸï¼Œæœ€åè¿›è¡Œç­–ç•¥å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šASPCçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€è°ƒæ•´æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ•°æ®é›†çš„ç‰¹æ€§è‡ªé€‚åº”åœ°ç¼©æ”¾ç­–ç•¥çº¦æŸï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹è¶…å‚æ•°çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šASPCé‡‡ç”¨äº†äºŒé˜¶å¯å¾®çš„æŸå¤±å‡½æ•°è®¾è®¡ï¼Œç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°è°ƒæ•´çº¦æŸã€‚åŒæ—¶ï¼Œæ¡†æ¶ä¸­çš„è¶…å‚æ•°è®¾ç½®ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ä¿è¯åœ¨ä¸åŒæ•°æ®é›†ä¸Šå‡èƒ½ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨39ä¸ªæ•°æ®é›†çš„å®éªŒä¸­ï¼ŒASPCåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…¶ä»–è‡ªé€‚åº”çº¦æŸæ–¹æ³•å’Œæœ€å…ˆè¿›çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¸”ä»…ä½¿ç”¨å•ä¸€è¶…å‚æ•°é…ç½®ï¼Œè®¡ç®—å¼€é”€æå°ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰éœ€è¦ä»å†å²æ•°æ®ä¸­å­¦ä¹ çš„åœºæ™¯ã€‚é€šè¿‡ç®€åŒ–è¶…å‚æ•°è°ƒèŠ‚è¿‡ç¨‹ï¼ŒASPCèƒ½å¤ŸåŠ é€Ÿæ¨¡å‹çš„å¼€å‘ä¸éƒ¨ç½²ï¼Œæé«˜å®é™…åº”ç”¨çš„æ•ˆç‡å’Œæ•ˆæœã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¤šå¤æ‚ç¯å¢ƒä¸­å±•ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§ä¸é²æ£’æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline reinforcement learning (RL) enables learning effective policies from fixed datasets without any environment interaction. Existing methods typically employ policy constraints to mitigate the distribution shift encountered during offline RL training. However, because the scale of the constraints varies across tasks and datasets of differing quality, existing methods must meticulously tune hyperparameters to match each dataset, which is time-consuming and often impractical. We propose Adaptive Scaling of Policy Constraints (ASPC), a second-order differentiable framework that dynamically balances RL and behavior cloning (BC) during training. We theoretically analyze its performance improvement guarantee. In experiments on 39 datasets across four D4RL domains, ASPC using a single hyperparameter configuration outperforms other adaptive constraint methods and state-of-the-art offline RL algorithms that require per-dataset tuning while incurring only minimal computational overhead. The code will be released at https://github.com/Colin-Jing/ASPC.

