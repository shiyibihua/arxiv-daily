---
layout: default
title: Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning
---

# Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19598" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19598v1</a>
  <a href="https://arxiv.org/pdf/2508.19598.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19598v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19598v1', 'Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiwei Li, Yong Hu, Wenqing Wang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRLTRæ¡†æ¶ä»¥è§£å†³LLMä»£ç†è§„åˆ’èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¡ŒåŠ¨è§„åˆ’` `å·¥å…·ä½¿ç”¨å¥–åŠ±` `å¤šç›®æ ‡ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç«¯åˆ°ç«¯å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•åœ¨è¡ŒåŠ¨è§„åˆ’å’Œç­”æ¡ˆæ€»ç»“ä¹‹é—´å­˜åœ¨ä¼˜åŒ–ç›®æ ‡åˆ†é…ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œä¸”ç¼ºä¹å¯éªŒè¯çš„æ•°æ®ï¼Œé™åˆ¶äº†ä»£ç†çš„è§„åˆ’èƒ½åŠ›æå‡ã€‚
2. æœ¬æ–‡æå‡ºçš„RLTRæ¡†æ¶é€šè¿‡è§£è€¦è®­ç»ƒè¿‡ç¨‹ï¼Œä¸“æ³¨äºè§„åˆ’æ¨¡å—çš„å•ä¸€ç›®æ ‡ä¼˜åŒ–ï¼Œå¹¶å¼•å…¥åŸºäºå·¥å…·ä½¿ç”¨å®Œæ•´æ€§çš„å¥–åŠ±ä¿¡å·ï¼Œç›´æ¥è¯„ä¼°å·¥å…·è°ƒç”¨åºåˆ—çš„è´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLTRåœ¨è§„åˆ’æ€§èƒ½ä¸Šæ¯”ç°æœ‰çš„ç«¯åˆ°ç«¯åŸºçº¿æé«˜äº†8%-12%ï¼ŒåŒæ—¶æ•´ä½“ä»£ç†ç³»ç»Ÿçš„æœ€ç»ˆå“åº”è´¨é‡ä¹Ÿæå‡äº†5%-6%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„åŠŸèƒ½ä¸»è¦ç”±è¡ŒåŠ¨è§„åˆ’å’Œç­”æ¡ˆæ€»ç»“ä¸¤å¤§èƒ½åŠ›å†³å®šã€‚å…¶ä¸­ï¼Œè¡ŒåŠ¨è§„åˆ’æ˜¯å†³å®šä»£ç†æ€§èƒ½çš„æ ¸å¿ƒèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è®­ç»ƒèŒƒå¼é‡‡ç”¨ç«¯åˆ°ç«¯çš„å¤šç›®æ ‡ä¼˜åŒ–ï¼Œé¢ä¸´ä¼˜åŒ–ç›®æ ‡åˆ†é…ä¸å¹³è¡¡å’Œå¯éªŒè¯æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œéš¾ä»¥æå‡ä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŸºäºå·¥å…·ä½¿ç”¨å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLTRï¼‰ï¼Œè¯¥æ¡†æ¶è§£è€¦äº†è®­ç»ƒè¿‡ç¨‹ï¼Œä½¿è§„åˆ’æ¨¡å—èƒ½å¤Ÿä¸“æ³¨äºå•ä¸€ç›®æ ‡ä¼˜åŒ–ã€‚RLTRå¼•å…¥äº†åŸºäºå·¥å…·ä½¿ç”¨å®Œæ•´æ€§çš„å¥–åŠ±ä¿¡å·ï¼Œç›´æ¥è¯„ä¼°å·¥å…·è°ƒç”¨åºåˆ—çš„è´¨é‡ï¼Œæä¾›æ¯”æœ€ç»ˆå“åº”å†…å®¹æ›´ç›´æ¥å¯é çš„è®­ç»ƒä¿¡å·ï¼Œä»è€Œé¿å…äº†å¯¹å¯éªŒè¯æ•°æ®çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLTRåœ¨è§„åˆ’æ€§èƒ½ä¸Šæ¯”ç«¯åˆ°ç«¯åŸºçº¿æé«˜äº†8%-12%ï¼Œå¹¶ä¸”è¿™ç§å¢å¼ºçš„è§„åˆ’èƒ½åŠ›ä¹Ÿä½¿æ•´ä½“ä»£ç†ç³»ç»Ÿçš„æœ€ç»ˆå“åº”è´¨é‡æé«˜äº†5%-6%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨è¡ŒåŠ¨è§„åˆ’èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤šç›®æ ‡ä¼˜åŒ–ä¸­é¢ä¸´ç›®æ ‡åˆ†é…ä¸å¹³è¡¡å’Œç¼ºä¹å¯éªŒè¯æ•°æ®çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRLTRæ¡†æ¶é€šè¿‡è§£è€¦è®­ç»ƒè¿‡ç¨‹ï¼Œä¸“æ³¨äºè§„åˆ’æ¨¡å—çš„å•ä¸€ç›®æ ‡ä¼˜åŒ–ï¼Œåˆ©ç”¨å·¥å…·ä½¿ç”¨å®Œæ•´æ€§ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œç›´æ¥è¯„ä¼°å·¥å…·è°ƒç”¨çš„è´¨é‡ï¼Œé¿å…å¯¹æœ€ç»ˆå“åº”å†…å®¹çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRLTRæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè§„åˆ’æ¨¡å—å’Œå¥–åŠ±è¯„ä¼°æ¨¡å—ã€‚è§„åˆ’æ¨¡å—è´Ÿè´£ç”Ÿæˆå·¥å…·è°ƒç”¨åºåˆ—ï¼Œè€Œå¥–åŠ±è¯„ä¼°æ¨¡å—åˆ™åŸºäºå·¥å…·ä½¿ç”¨çš„å®Œæ•´æ€§æ¥è¯„ä¼°è¿™äº›åºåˆ—çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šRLTRçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºå·¥å…·ä½¿ç”¨å®Œæ•´æ€§çš„å¥–åŠ±ä¿¡å·ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—è®­ç»ƒä¿¡å·æ›´åŠ ç›´æ¥å’Œå¯é ï¼Œæ˜¾è‘—æå‡äº†è§„åˆ’èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿçš„ä¾èµ–æœ€ç»ˆå“åº”å†…å®¹çš„è®­ç»ƒæ–¹æ³•å½¢æˆäº†æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RLTRä¸­ï¼Œå¥–åŠ±ä¿¡å·çš„è®¾è®¡æ˜¯å…³é”®ï¼Œå…·ä½“åŒ…æ‹¬å¦‚ä½•é‡åŒ–å·¥å…·ä½¿ç”¨çš„å®Œæ•´æ€§ï¼Œä»¥åŠå¦‚ä½•è®¾ç½®æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è§„åˆ’æ¨¡å—çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRLTRæ¡†æ¶åœ¨è§„åˆ’æ€§èƒ½ä¸Šæ¯”ç«¯åˆ°ç«¯åŸºçº¿æé«˜äº†8%-12%ï¼ŒåŒæ—¶æ•´ä½“ä»£ç†ç³»ç»Ÿçš„æœ€ç»ˆå“åº”è´¨é‡ä¹Ÿæå‡äº†5%-6%ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡å±•ç¤ºäº†RLTRåœ¨ä¼˜åŒ–LLMä»£ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿå’Œå¤æ‚ä»»åŠ¡çš„è§„åˆ’ä¸æ‰§è¡Œç­‰ã€‚é€šè¿‡æå‡LLMä»£ç†çš„è§„åˆ’èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤šç§åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„ä»»åŠ¡æ‰§è¡Œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The functionality of Large Language Model (LLM) agents is primarily determined by two capabilities: action planning and answer summarization. The former, action planning, is the core capability that dictates an agent's performance. However, prevailing training paradigms employ end-to-end, multi-objective optimization that jointly trains both capabilities. This paradigm faces two critical challenges: imbalanced optimization objective allocation and scarcity of verifiable data, making it difficult to enhance the agent's planning capability. To address these challenges, we propose Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that decouples the training process to enable a focused, single-objective optimization of the planning module. Crucially, RLTR introduces a reward signal based on tool-use completeness to directly evaluate the quality of tool invocation sequences. This method offers a more direct and reliable training signal than assessing the final response content, thereby obviating the need for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12% improvement in planning performance compared to end-to-end baselines. Moreover, this enhanced planning capability, in turn, translates to a 5%-6% increase in the final response quality of the overall agent system.

