---
layout: default
title: Robustness is Important: Limitations of LLMs for Data Fitting
---

# Robustness is Important: Limitations of LLMs for Data Fitting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19563" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19563v3</a>
  <a href="https://arxiv.org/pdf/2508.19563.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19563v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19563v3', 'Robustness is Important: Limitations of LLMs for Data Fitting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hejia Liu, Mochen Yang, Gediminas Adomavicius

**åˆ†ç±»**: cs.LG, cs.AI, stat.AP, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27 (æ›´æ–°: 2025-10-28)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºLLMsåœ¨æ•°æ®æ‹Ÿåˆä¸­çš„è„†å¼±æ€§åŠå…¶å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®æ‹Ÿåˆ` `é¢„æµ‹é²æ£’æ€§` `æ³¨æ„åŠ›æœºåˆ¶` `ä»»åŠ¡æ— å…³å˜åŒ–` `æ¨¡å‹è¯„ä¼°` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMsåœ¨æ•°æ®æ‹Ÿåˆä¸­è¡¨ç°å‡ºå¯¹æ— å…³å˜åŒ–çš„é«˜åº¦æ•æ„Ÿæ€§ï¼Œå¯¼è‡´é¢„æµ‹ç»“æœä¸ç¨³å®šã€‚
2. è®ºæ–‡é€šè¿‡åˆ†æLLMsçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ­ç¤ºäº†å…¶åœ¨å¤„ç†ä»»åŠ¡æ— å…³å˜åŒ–æ—¶çš„è„†å¼±æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsçš„é¢„æµ‹è¯¯å·®åœ¨å˜é‡åç§°å˜åŒ–æ—¶å¯è¾¾82%ï¼Œæ˜¾ç¤ºå‡ºå…¶é²æ£’æ€§ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§åœºæ™¯ä¸­è¢«åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æ‹Ÿåˆå’Œé¢„æµ‹ç”Ÿæˆæ–¹é¢ã€‚å°½ç®¡LLMsåœ¨é¢„æµ‹æ€§èƒ½ä¸Šä¸è®¸å¤šè¡¨æ ¼ç›‘ç£å­¦ä¹ æŠ€æœ¯ç«äº‰ï¼Œä½†æˆ‘ä»¬å‘ç°LLMsåœ¨æ•°æ®è¡¨ç¤ºçš„æ— å…³å˜åŒ–ä¸‹è¡¨ç°å‡ºæ˜¾è‘—çš„è„†å¼±æ€§ã€‚ä¾‹å¦‚ï¼Œä»…ä»…æ”¹å˜å˜é‡åç§°å°±å¯èƒ½å¯¼è‡´é¢„æµ‹è¯¯å·®é«˜è¾¾82%ã€‚è¿™ç§å¯¹ä»»åŠ¡æ— å…³å˜åŒ–çš„æ•æ„Ÿæ€§åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œç›‘ç£å¾®è°ƒä¸­å‡æœ‰ä½“ç°ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ†æå¼€æ”¾æƒé‡LLMçš„æ³¨æ„åŠ›å¾—åˆ†ï¼Œæˆ‘ä»¬å‘ç°è®­ç»ƒç¤ºä¾‹å’Œå˜é‡åç§°/å€¼åœ¨æç¤ºä¸­çš„ç‰¹å®šä½ç½®ä¼šè·å¾—æ›´å¤šå…³æ³¨ï¼Œè¿™éƒ¨åˆ†è§£é‡Šäº†è¿™ç§æ•æ„Ÿæ€§ã€‚å°½ç®¡æœ‰é’ˆå¯¹æ•°æ®æ‹Ÿåˆè®¾è®¡çš„TabPFNæ¨¡å‹ï¼Œä»ç„¶æ— æ³•å®Œå…¨æŠµå¾¡ä»»åŠ¡æ— å…³å˜åŒ–çš„å½±å“ã€‚æ€»ä½“è€Œè¨€ï¼Œå°½ç®¡LLMsåœ¨é¢„æµ‹èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä½œä¸ºæ•°æ®æ‹Ÿåˆå·¥å…·æ—¶ç¼ºä¹åŸºæœ¬çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°æ®æ‹Ÿåˆä»»åŠ¡ä¸­å¯¹æ— å…³æ•°æ®å˜åŒ–çš„è„†å¼±æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ•°æ®è¡¨ç¤ºå˜åŒ–æ—¶ï¼Œé¢„æµ‹ç»“æœçš„ç¨³å®šæ€§æ˜¾è‘—ä¸‹é™ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡åˆ†æLLMsçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¢è®¨å…¶åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œç›‘ç£å¾®è°ƒä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†æ¨¡å‹å¯¹ä»»åŠ¡æ— å…³å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æ·±å…¥ç†è§£LLMsçš„é¢„æµ‹è¡Œä¸ºåŠå…¶å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¯¹æ¯”å®éªŒçš„æ–¹æ³•ï¼Œåˆ†æäº†ä¸åŒæ•°æ®è¡¨ç¤ºä¸‹LLMsçš„é¢„æµ‹æ€§èƒ½ï¼Œå¹¶ä¸ä¸“ä¸ºæ•°æ®æ‹Ÿåˆè®¾è®¡çš„TabPFNæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®è¡¨ç¤ºå˜åŒ–ã€æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹æ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæ­ç¤ºäº†LLMsåœ¨å¤„ç†ä»»åŠ¡æ— å…³å˜åŒ–æ—¶çš„éå‡åŒ€æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¿™ä¸€å‘ç°ä¸ç°æœ‰æ–¹æ³•çš„å…³æ³¨ç‚¹ä¸åŒï¼Œå¼ºè°ƒäº†æ¨¡å‹åœ¨ç‰¹å®šä½ç½®çš„æ³¨æ„åŠ›åˆ†é…å¯¹é¢„æµ‹ç»“æœçš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†å¤šç§å˜é‡åç§°å˜åŒ–çš„åœºæ™¯ï¼Œé‡‡ç”¨äº†æ ‡å‡†çš„æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œç¡®ä¿äº†å®éªŒç»“æœçš„å¯é æ€§å’Œå¯æ¯”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨å˜é‡åç§°å˜åŒ–æ—¶çš„é¢„æµ‹è¯¯å·®å¯è¾¾82%ï¼Œè€Œä¸“ä¸ºæ•°æ®æ‹Ÿåˆè®¾è®¡çš„TabPFNæ¨¡å‹åœ¨ä»»åŠ¡æ— å…³å˜åŒ–ä¸‹ä¹Ÿæœªèƒ½å®Œå…¨æŠµå¾¡è¿™ç§å½±å“ã€‚è¿™è¡¨æ˜å½“å‰LLMsåœ¨æ•°æ®æ‹Ÿåˆä»»åŠ¡ä¸­çš„é²æ£’æ€§ä»ç„¶ä¸è¶³ï¼ŒäºŸéœ€æ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èé¢„æµ‹ã€åŒ»ç–—æ•°æ®åˆ†æå’Œå¸‚åœºè¶‹åŠ¿åˆ†æç­‰ã€‚é€šè¿‡æé«˜å¯¹æ•°æ®æ‹Ÿåˆå·¥å…·çš„ç†è§£ï¼Œå¯ä»¥ä¸ºå®é™…åº”ç”¨æä¾›æ›´å¯é çš„æ¨¡å‹é€‰æ‹©å’Œä¼˜åŒ–ç­–ç•¥ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are being applied in a wide array of settings, well beyond the typical language-oriented use cases. In particular, LLMs are increasingly used as a plug-and-play method for fitting data and generating predictions. Prior work has shown that LLMs, via in-context learning or supervised fine-tuning, can perform competitively with many tabular supervised learning techniques in terms of predictive performance. However, we identify a critical vulnerability of using LLMs for data fitting -- making changes to data representation that are completely irrelevant to the underlying learning task can drastically alter LLMs' predictions on the same data. For example, simply changing variable names can sway the size of prediction error by as much as 82% in certain settings. Such prediction sensitivity with respect to task-irrelevant variations manifests under both in-context learning and supervised fine-tuning, for both close-weight and open-weight general-purpose LLMs. Moreover, by examining the attention scores of an open-weight LLM, we discover a non-uniform attention pattern: training examples and variable names/values which happen to occupy certain positions in the prompt receive more attention when output tokens are generated, even though different positions are expected to receive roughly the same attention. This partially explains the sensitivity in the presence of task-irrelevant variations. We also consider a state-of-the-art tabular foundation model (TabPFN) trained specifically for data fitting. Despite being explicitly designed to achieve prediction robustness, TabPFN is still not immune to task-irrelevant variations. Overall, despite LLMs' impressive predictive capabilities, currently they lack even the basic level of robustness to be used as a principled data-fitting tool.

