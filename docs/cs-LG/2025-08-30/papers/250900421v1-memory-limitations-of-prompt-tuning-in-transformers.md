---
layout: default
title: Memory Limitations of Prompt Tuning in Transformers
---

# Memory Limitations of Prompt Tuning in Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00421" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00421v1</a>
  <a href="https://arxiv.org/pdf/2509.00421.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00421v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00421v1', 'Memory Limitations of Prompt Tuning in Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maxime Meyer, Mario Michelessa, Caroline Chaux, Vincent Y. F. Tan

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹Transformerè®°å¿†é™åˆ¶çš„ç†è®ºåˆ†æä»¥è§£å†³æç¤ºè°ƒä¼˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æç¤ºè°ƒä¼˜` `Transformer` `è®°å¿†èƒ½åŠ›` `æ€§èƒ½ä¸‹é™` `ç†è®ºåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æç¤ºè°ƒä¼˜æ–¹æ³•åœ¨ç†è®ºåˆ†æä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹Transformerçš„è®°å¿†èƒ½åŠ›ç¼ºä¹æ·±å…¥æ¢è®¨ã€‚
2. è®ºæ–‡é€šè¿‡ç†è®ºè¯æ˜ï¼Œæ­ç¤ºäº†Transformeråœ¨æç¤ºé•¿åº¦ä¸è®°å¿†ä¿¡æ¯é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ï¼Œæå‡ºäº†æ–°çš„ç†è§£æ¡†æ¶ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒTransformeråœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶å­˜åœ¨æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæä¾›äº†å¯¹å…¶å†…åœ¨é™åˆ¶çš„æ·±åˆ»è§è§£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡æç¤ºè°ƒä¼˜åœ¨å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹é€‚åº”æ–°ä»»åŠ¡æ–¹é¢å–å¾—äº†å®è¯æˆåŠŸï¼Œä½†å…¶èƒ½åŠ›çš„ç†è®ºåˆ†æä»ç„¶æœ‰é™ã€‚ç°æœ‰ç†è®ºå·¥ä½œä¸»è¦å…³æ³¨äºé€šç”¨é€¼è¿‘æ€§è´¨ï¼Œç»“æœä¸æ ‡å‡†æƒé‡è°ƒä¼˜ç›¸å½“ã€‚æœ¬æ–‡æ¢è®¨äº†Transformerçš„è®°å¿†èƒ½åŠ›ï¼Œæä¾›äº†ä¸¤ä¸ªä¸»è¦ç†è®ºè´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜äº†Transformerè®°å¿†çš„ä¿¡æ¯é‡ä¸èƒ½æ¯”æç¤ºé•¿åº¦çº¿æ€§å¢é•¿å¾—æ›´å¿«ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é¦–æ¬¡æ­£å¼è¯æ˜äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼šåœ¨æ‰©å±•ä¸Šä¸‹æ–‡æ—¶æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬ä¸¥æ ¼è¯æ˜äº†Transformerå›ºæœ‰çš„è®°å¿†é™åˆ¶ï¼Œçº¦æŸäº†å…¶èƒ½å¤Ÿä¿ç•™çš„ä¿¡æ¯é‡ï¼Œæ— è®ºä¸Šä¸‹æ–‡å¤§å°å¦‚ä½•ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£Transformeræ¶æ„çš„å†…åœ¨å±€é™æ€§æä¾›äº†åŸºç¡€ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æç¤ºè°ƒä¼˜åœ¨Transformeræ¨¡å‹ä¸­çš„è®°å¿†èƒ½åŠ›é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è§£é‡Šå…¶åœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹çš„æ€§èƒ½ä¸‹é™ç°è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç†è®ºåˆ†æï¼Œè®ºæ–‡æå‡ºTransformerçš„è®°å¿†èƒ½åŠ›ä¸æç¤ºé•¿åº¦ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼Œæ­ç¤ºäº†å…¶å›ºæœ‰çš„è®°å¿†é™åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå®šä¹‰äº†è®°å¿†èƒ½åŠ›çš„æ•°å­¦æ¨¡å‹ï¼Œç„¶åé€šè¿‡ç†è®ºæ¨å¯¼è¯æ˜äº†ä¿¡æ¯é‡çš„çº¿æ€§å¢é•¿é™åˆ¶ï¼Œæœ€ååˆ†æäº†é•¿ä¸Šä¸‹æ–‡å¯¹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡æ­£å¼è¯æ˜äº†Transformeråœ¨æ‰©å±•ä¸Šä¸‹æ–‡æ—¶æ€§èƒ½ä¸‹é™çš„ç°è±¡ï¼Œå¡«è¡¥äº†ç†è®ºåˆ†æçš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç†è®ºæ¨å¯¼ä¸­ï¼Œé‡‡ç”¨äº†ä¿¡æ¯è®ºçš„ç›¸å…³æ¦‚å¿µï¼Œè®¾ç½®äº†æç¤ºé•¿åº¦å’Œä¸Šä¸‹æ–‡å¤§å°çš„å‚æ•°ï¼Œç¡®ä¿äº†ç»“æœçš„ä¸¥è°¨æ€§ä¸å¯é‡å¤æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€æç¤ºé•¿åº¦çš„å¢åŠ ï¼ŒTransformeræ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¯æ˜äº†å…¶è®°å¿†èƒ½åŠ›çš„çº¿æ€§é™åˆ¶ã€‚è¿™ä¸€å‘ç°ä¸ºæ¨¡å‹è®¾è®¡æä¾›äº†æ–°çš„è§†è§’ï¼Œå¼ºè°ƒäº†åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„æ½œåœ¨æŒ‘æˆ˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºç†è§£å’Œä¼˜åŒ–Transformeræ¶æ„åœ¨å¤„ç†é•¿åºåˆ—ä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†ç†è®ºåŸºç¡€ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬ç”Ÿæˆå’Œå¯¹è¯ç³»ç»Ÿç­‰é¢†åŸŸã€‚æœªæ¥å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹è®¾è®¡ä¸è®­ç»ƒç­–ç•¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the empirical success of prompt tuning in adapting pretrained language models to new tasks, theoretical analyses of its capabilities remain limited. Existing theoretical work primarily addresses universal approximation properties, demonstrating results comparable to standard weight tuning. In this paper, we explore a different aspect of the theory of transformers: the memorization capability of prompt tuning. We provide two principal theoretical contributions. First, we prove that the amount of information memorized by a transformer cannot scale faster than linearly with the prompt length. Second, and more importantly, we present the first formal proof of a phenomenon empirically observed in large language models: performance degradation in transformers with extended contexts. We rigorously demonstrate that transformers inherently have limited memory, constraining the amount of information they can retain, regardless of the context size. This finding offers a fundamental understanding of the intrinsic limitations of transformer architectures, particularly their ability to handle long sequences.

