---
layout: default
title: Are We Really Learning the Score Function? Reinterpreting Diffusion Models Through Wasserstein Gradient Flow Matching
---

# Are We Really Learning the Score Function? Reinterpreting Diffusion Models Through Wasserstein Gradient Flow Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00336" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00336v1</a>
  <a href="https://arxiv.org/pdf/2509.00336.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00336v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00336v1', 'Are We Really Learning the Score Function? Reinterpreting Diffusion Models Through Wasserstein Gradient Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: An B. Vuong, Michael T. McCann, Javier E. Santos, Yen Ting Lin

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWassersteinæ¢¯åº¦æµåŒ¹é…ä»¥é‡æ–°ç†è§£æ‰©æ•£æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `å¾—åˆ†å‡½æ•°` `Wassersteinæ¢¯åº¦æµ` `ç”Ÿæˆæ¨¡å‹` `ç¥ç»ç½‘ç»œ` `ç†è®ºç ”ç©¶` `å¯†åº¦ä¼ è¾“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰©æ•£æ¨¡å‹é€šå¸¸å‡è®¾å­¦ä¹ å¾—åˆ†å‡½æ•°ï¼Œä½†å®é™…ç¥ç»ç½‘ç»œæ¶æ„æœªèƒ½å¼ºåˆ¶å®ç°ä¿å®ˆå‘é‡åœºçš„è¦æ±‚ã€‚
2. è®ºæ–‡æå‡ºå°†æ‰©æ•£è®­ç»ƒè§†ä¸ºä¸Wassersteinæ¢¯åº¦æµçš„é€Ÿåº¦åœºåŒ¹é…ï¼Œè€Œéä¼ ç»Ÿçš„å¾—åˆ†å­¦ä¹ ï¼Œä»è€Œè§£å†³äº†ç†è®ºä¸Šçš„çŸ›ç›¾ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å­¦ä¹ åˆ°çš„å‘é‡åœºå¹¶éä¿å®ˆï¼Œæ¨¡å‹ä»èƒ½æœ‰æ•ˆç”Ÿæˆæ ·æœ¬ï¼ŒéªŒè¯äº†WGFè§†è§’çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£æ¨¡å‹é€šå¸¸è¢«è§£é‡Šä¸ºå­¦ä¹ å¾—åˆ†å‡½æ•°ï¼Œå³å™ªå£°æ•°æ®çš„å¯¹æ•°å¯†åº¦çš„æ¢¯åº¦ã€‚ç„¶è€Œï¼Œè¿™ä¸€å‡è®¾æš—ç¤ºå­¦ä¹ ç›®æ ‡æ˜¯ä¸€ä¸ªä¿å®ˆå‘é‡åœºï¼Œè€Œå®é™…ä½¿ç”¨çš„ç¥ç»ç½‘ç»œæ¶æ„å¹¶æœªå¼ºåˆ¶æ‰§è¡Œè¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬æä¾›æ•°å€¼è¯æ®è¡¨æ˜ï¼Œè®­ç»ƒåçš„æ‰©æ•£ç½‘ç»œè¿åäº†çœŸå®å¾—åˆ†å‡½æ•°æ‰€éœ€çš„ç§¯åˆ†å’Œå¾®åˆ†çº¦æŸï¼Œè¡¨æ˜å­¦ä¹ åˆ°çš„å‘é‡åœºå¹¶éä¿å®ˆã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆæœºåˆ¶ä¸Šè¡¨ç°å‡ºè‰²ã€‚ä¸ºäº†è§£é‡Šè¿™ä¸€æ˜æ˜¾çš„æ‚–è®ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç†è®ºè§†è§’ï¼šæ‰©æ•£è®­ç»ƒæ›´åº”ç†è§£ä¸ºä¸Wassersteinæ¢¯åº¦æµçš„é€Ÿåº¦åœºåŒ¹é…ï¼Œè€Œéåå‘éšæœºå¾®åˆ†æ–¹ç¨‹çš„å¾—åˆ†å­¦ä¹ ã€‚åœ¨è¿™ä¸€è§†è§’ä¸‹ï¼Œâ€œæ¦‚ç‡æµâ€è‡ªç„¶æºäºWGFæ¡†æ¶ï¼Œæ¶ˆé™¤äº†è°ƒç”¨åå‘æ—¶é—´SDEç†è®ºçš„å¿…è¦æ€§ï¼Œå¹¶é˜æ˜äº†ä¸ºä½•å³ä½¿ç¥ç»å‘é‡åœºä¸æ˜¯ä¸€ä¸ªçœŸå®å¾—åˆ†ï¼Œç”Ÿæˆé‡‡æ ·ä»ç„¶æˆåŠŸã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºï¼Œç¥ç»è¿‘ä¼¼ä¸­çš„éä¿å®ˆè¯¯å·®å¹¶ä¸ä¸€å®šä¼šæŸå®³å¯†åº¦ä¼ è¾“ã€‚æˆ‘ä»¬çš„ç»“æœå€¡å¯¼é‡‡ç”¨WGFè§†è§’ï¼Œä½œä¸ºç†è§£æ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„åŸåˆ™æ€§ã€ä¼˜é›…ä¸”ç†è®ºåŸºç¡€æ‰å®çš„æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å­¦ä¹ å¾—åˆ†å‡½æ•°æ—¶æœªèƒ½æ»¡è¶³ä¿å®ˆå‘é‡åœºçš„è¦æ±‚ï¼Œå¯¼è‡´ç”Ÿæˆæ•ˆæœä¸ç†è®ºä¸ç¬¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ‰©æ•£è®­ç»ƒé‡æ–°è§£é‡Šä¸ºä¸Wassersteinæ¢¯åº¦æµçš„é€Ÿåº¦åœºè¿›è¡ŒåŒ¹é…ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½åœ°ç†è§£ç”Ÿæˆè¿‡ç¨‹çš„æœ¬è´¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®å™ªå£°å¤„ç†ã€é€Ÿåº¦åœºåŒ¹é…å’Œç”Ÿæˆæ ·æœ¬çš„è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºWGFè§†è§’ï¼Œå¼ºè°ƒæ¦‚ç‡æµçš„è‡ªç„¶äº§ç”Ÿï¼Œé¿å…äº†ä¼ ç»Ÿå¾—åˆ†å­¦ä¹ çš„é™åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é€Ÿåº¦åœºåŒ¹é…ï¼Œç¡®ä¿ç”Ÿæˆè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œæ¶æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å­¦ä¹ åˆ°çš„å‘é‡åœºå¹¶éä¿å®ˆï¼Œæ¨¡å‹åœ¨ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ä¸Šä»ç„¶ä¿æŒé«˜æ°´å¹³ï¼ŒéªŒè¯äº†WGFè§†è§’çš„æœ‰æ•ˆæ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’ŒçœŸå®æ„Ÿæ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å°†åœ¨è®ºæ–‡ä¸­è¯¦ç»†åˆ—å‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒç”Ÿæˆã€è¯­éŸ³åˆæˆå’Œå…¶ä»–éœ€è¦ç”Ÿæˆæ¨¡å‹çš„ä»»åŠ¡ã€‚é€šè¿‡æä¾›æ›´æ·±åˆ»çš„ç†è®ºåŸºç¡€ï¼Œæœªæ¥çš„æ‰©æ•£æ¨¡å‹å¯ä»¥åœ¨ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ä¸Šå®ç°æ›´å¤§çš„æå‡ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion models are commonly interpreted as learning the score function, i.e., the gradient of the log-density of noisy data. However, this assumption implies that the target of learning is a conservative vector field, which is not enforced by the neural network architectures used in practice. We present numerical evidence that trained diffusion networks violate both integral and differential constraints required of true score functions, demonstrating that the learned vector fields are not conservative. Despite this, the models perform remarkably well as generative mechanisms. To explain this apparent paradox, we advocate a new theoretical perspective: diffusion training is better understood as flow matching to the velocity field of a Wasserstein Gradient Flow (WGF), rather than as score learning for a reverse-time stochastic differential equation. Under this view, the "probability flow" arises naturally from the WGF framework, eliminating the need to invoke reverse-time SDE theory and clarifying why generative sampling remains successful even when the neural vector field is not a true score. We further show that non-conservative errors from neural approximation do not necessarily harm density transport. Our results advocate for adopting the WGF perspective as a principled, elegant, and theoretically grounded framework for understanding diffusion generative models.

