---
layout: default
title: Metis: Training LLMs with FP4 Quantization
---

# Metis: Training LLMs with FP4 Quantization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00404" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00404v4</a>
  <a href="https://arxiv.org/pdf/2509.00404.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00404v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00404v4', 'Metis: Training LLMs with FP4 Quantization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hengjie Cao, Mengyi Chen, Yifeng Yang, Ruijun Huang, Fang Dong, Jixian Zhou, Anrui Chen, Mingzhi Dong, Yujiang Wang, Jinlong Hou, Yuan Cheng, Fan Wu, Fan Yang, Tun Lu, Ning Gu, Li Shang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30 (æ›´æ–°: 2025-09-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMetisæ¡†æ¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹ä½ä½è®­ç»ƒä¸­çš„é‡åŒ–åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–è®­ç»ƒ` `å¤§è¯­è¨€æ¨¡å‹` `è°±åŸŸé‡åŒ–` `ä½ä½è®­ç»ƒ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä½ä½è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´é‡åŒ–åå·®å’Œè°±å¤±çœŸçš„æŒ‘æˆ˜ï¼Œå½±å“è®­ç»ƒæ€§èƒ½ã€‚
2. Metisæ¡†æ¶é€šè¿‡å°†å„å‘å¼‚æ€§è°±åˆ’åˆ†ä¸ºæ›´çª„çš„å­åˆ†å¸ƒè¿›è¡Œç‹¬ç«‹é‡åŒ–ï¼Œå‡å°‘è¯¯å·®å¹¶ä¿æŒè°±ç»“æ„ã€‚
3. åœ¨LLaMA-3 8Bæ¨¡å‹ä¸Šï¼ŒMetiså®ç°äº†ä»…0.4%çš„è®­ç»ƒæŸå¤±å·®è·å’Œ0.1%çš„å‡†ç¡®åº¦ä¸‹é™ï¼Œæ˜¾è‘—ä¼˜äºBF16å’ŒNvidiaçš„FP4æ–¹æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶è¯†åˆ«å‡ºå‚æ•°ã€æ¿€æ´»å’Œæ¢¯åº¦çš„å¥‡å¼‚å€¼è°±ä¸­çš„å„å‘å¼‚æ€§ï¼Œè®¤ä¸ºè¿™æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½ä½è®­ç»ƒçš„æ ¹æœ¬éšœç¢ã€‚è¿™äº›è°±ç”±å°‘é‡å¤§å¥‡å¼‚å€¼ä¸»å¯¼ï¼Œå¯¼è‡´æ•°å€¼èŒƒå›´å®½å¹¿ï¼Œä»è€Œå¼•å‘é‡åŒ–åå·®å’Œä¸¥é‡çš„è°±å¤±çœŸï¼Œæœ€ç»ˆé™ä½è®­ç»ƒæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Metisï¼Œä¸€ä¸ªè°±åŸŸé‡åŒ–æ¡†æ¶ï¼Œé€šè¿‡å°†å„å‘å¼‚æ€§è°±åˆ’åˆ†ä¸ºæ›´çª„çš„å­åˆ†å¸ƒè¿›è¡Œç‹¬ç«‹é‡åŒ–ï¼Œä»è€Œå‡å°‘è¯¯å·®å¹¶ä¿æŒè°±ç»“æ„ã€‚Metisåœ¨LLaMA-3 8Bæ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨100Bä¸ªtokenï¼Œèƒ½å¤Ÿå®ç°W4A4G4çš„ç¨³å¥è®­ç»ƒï¼ŒFP4é‡åŒ–çš„æƒé‡ã€æ¿€æ´»å’Œæ¢¯åº¦ä»…å¯¼è‡´0.4%çš„è®­ç»ƒæŸå¤±å·®è·å’Œ0.1%çš„ä¸‹æ¸¸å‡†ç¡®åº¦ä¸‹é™ï¼Œç›¸è¾ƒäºBF16ï¼ŒMetisä¸ä»…åŒ¹é…äº†BF16çš„ç²¾åº¦ï¼Œè¿˜è¶…è¶Šäº†Nvidiaæœ€è¿‘å‘å¸ƒçš„FP4æ–¹æ¡ˆï¼Œå®ç°äº†æ›´ä½çš„æŸå¤±å’Œæ›´é«˜çš„ä¸‹æ¸¸å‡†ç¡®åº¦ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ä½ä½è®­ç»ƒä¸­çš„é‡åŒ–åå·®å’Œè°±å¤±çœŸé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç”±äºå¥‡å¼‚å€¼è°±çš„å„å‘å¼‚æ€§ï¼Œå¯¼è‡´æ•°å€¼èŒƒå›´è¿‡å®½ï¼Œå½±å“è®­ç»ƒæ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMetisæ¡†æ¶ï¼Œé€šè¿‡å°†å¥‡å¼‚å€¼è°±åˆ’åˆ†ä¸ºæ›´çª„çš„å­åˆ†å¸ƒè¿›è¡Œç‹¬ç«‹é‡åŒ–ï¼Œä»è€Œå‡å°‘é‡åŒ–è¯¯å·®å¹¶ä¿æŒè°±çš„ç»“æ„ç‰¹å¾ã€‚è¯¥è®¾è®¡æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡è®­ç»ƒæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMetisçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç¨€ç–éšæœºé‡‡æ ·ä»¥ä¿æŒä¸»è°±å­ç©ºé—´ï¼Œ2) éšæœºæŠ•å½±ä»¥é™ä½åˆ†è§£æˆæœ¬ã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¨¡å—ï¼ŒMetisèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œè°±åŸŸé‡åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šMetisçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è°±åŸŸé‡åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†å„å‘å¼‚æ€§è°±åˆ’åˆ†ä¸ºç‹¬ç«‹çš„å­åˆ†å¸ƒï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„é‡åŒ–æŠ€æœ¯æœ‰æœ¬è´¨åŒºåˆ«ï¼Œæ˜¾è‘—é™ä½äº†é‡åŒ–å¼•èµ·çš„è¯¯å·®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒMetisé‡‡ç”¨FP4é‡åŒ–æ–¹æ¡ˆï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡å‡å°‘é‡åŒ–è¯¯å·®ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™ä¿æŒäº†ä¸»è°±å­ç©ºé—´çš„ç¨€ç–æ€§å’Œéšæœºæ€§ï¼Œä»¥ç¡®ä¿é«˜æ•ˆçš„è®¡ç®—æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨LLaMA-3 8Bæ¨¡å‹çš„å®éªŒä¸­ï¼ŒMetiså®ç°äº†ä»…0.4%çš„è®­ç»ƒæŸå¤±å·®è·å’Œ0.1%çš„å‡†ç¡®åº¦ä¸‹é™ï¼Œç›¸è¾ƒäºBF16ï¼ŒMetisä¸ä»…åŒ¹é…äº†å…¶ç²¾åº¦ï¼Œè¿˜åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†Nvidiaçš„FP4æ–¹æ¡ˆï¼Œè¡¨ç°å‡ºæ›´ä½çš„æŸå¤±å’Œæ›´é«˜çš„ä¸‹æ¸¸å‡†ç¡®åº¦ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚é€šè¿‡æé«˜ä½ä½è®­ç»ƒçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒMetisèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work identifies anisotropy in the singular value spectra of parameters, activations, and gradients as the fundamental barrier to low-bit training of large language models (LLMs). These spectra are dominated by a small fraction of large singular values, inducing wide numerical ranges that cause quantization bias and severe spectral distortion, ultimately degrading training performance. This work presents Metis, a spectral-domain quantization framework that partitions anisotropic spectra into narrower sub-distributions for independent quantization, thereby reducing errors and preserving spectral structure. To minimize overhead, Metis leverages two key properties of the dominant spectral subspace: preservation via sparsely random sampling and preservation via random projection, reducing decomposition cost to a negligible level. On LLaMA-3 8B trained with 100B tokens, Metis enables robust W4A4G4 training with FP4 quantization of weights, activations, and gradients, yielding only a 0.4% training loss gap and a 0.1% degradation in downstream accuracy relative to BF16. Beyond matching BF16 fidelity, Metis also surpasses our implementation of Nvidia's recently announced (yet to be publicly released) FP4 recipe, consistently achieving lower loss and higher downstream accuracy while incurring significantly lower computational overhead. The code implementation for Metis is available at: https://anonymous.4open.science/r/Metis-quantization-644B.

