---
layout: default
title: LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning
---

# LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00347" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00347v1</a>
  <a href="https://arxiv.org/pdf/2509.00347.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00347v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00347v1', 'LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanping Zhang, Yuhong Guo

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMé©±åŠ¨çš„ç­–ç•¥æ‰©æ•£ä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç­–ç•¥å­¦ä¹ ` `è½¨è¿¹æç¤º` `å˜æ¢å™¨æ¨¡å‹` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œè®­ç»ƒçš„ä»£ç†å¾€å¾€æ— æ³•é€‚åº”æ–°ä»»åŠ¡æˆ–ç¯å¢ƒã€‚
2. æœ¬æ–‡æå‡ºLLMé©±åŠ¨çš„ç­–ç•¥æ‰©æ•£ï¼ˆLLMDPDï¼‰ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬å’Œè½¨è¿¹æç¤ºæ¥å¼•å¯¼ç­–ç•¥å­¦ä¹ ï¼Œä»è€Œå¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMDPDåœ¨æœªè§ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥å…¶å¼ºå¤§çš„å†³ç­–èƒ½åŠ›å¹¿æ³›åº”ç”¨äºå„ç§ç°å®åœºæ™¯ã€‚ç„¶è€Œï¼Œéšç€ç¦»çº¿æ•°æ®é›†çš„å¢åŠ ä»¥åŠç¼ºä¹äººç±»ä¸“å®¶è®¾è®¡çš„åœ¨çº¿ç¯å¢ƒï¼Œç¦»çº¿RLä¸­çš„æ³›åŒ–æŒ‘æˆ˜æ„ˆå‘çªå‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•â€”â€”LLMé©±åŠ¨çš„ç­–ç•¥æ‰©æ•£ï¼ˆLLMDPDï¼‰ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„æç¤ºå¢å¼ºç¦»çº¿RLçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ç»“åˆæ–‡æœ¬ä»»åŠ¡æè¿°å’Œè½¨è¿¹æç¤ºï¼Œå¼•å¯¼ç­–ç•¥å­¦ä¹ ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤„ç†æ–‡æœ¬æç¤ºï¼Œå……åˆ†åˆ©ç”¨å…¶è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›å’Œä¸°å¯Œçš„çŸ¥è¯†åº“ï¼Œæä¾›ä¸ä»»åŠ¡ç›¸å…³çš„ä¸Šä¸‹æ–‡ã€‚åŒæ—¶ï¼Œä½¿ç”¨å˜æ¢å™¨æ¨¡å‹ç¼–ç è½¨è¿¹æç¤ºï¼Œæ•æ‰æ½œåœ¨è½¬ç§»åŠ¨æ€ä¸­çš„ç»“æ„åŒ–è¡Œä¸ºæ¨¡å¼ã€‚è¿™äº›æç¤ºä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œè¾“å…¥åˆ°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç­–ç•¥çº§æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½¿RLä»£ç†èƒ½å¤Ÿæœ‰æ•ˆåœ°æ³›åŒ–åˆ°æœªè§ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMDPDåœ¨æœªè§ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„ç¦»çº¿RLæ–¹æ³•ï¼Œçªæ˜¾äº†å…¶åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­æé«˜æ³›åŒ–å’Œé€‚åº”æ€§çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ä»£ç†æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ”¶é›†çš„ç»éªŒï¼Œå¯¼è‡´åœ¨æ–°ä»»åŠ¡æˆ–ç¯å¢ƒä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLLMDPDé€šè¿‡å¼•å…¥ä»»åŠ¡ç‰¹å®šçš„æ–‡æœ¬æç¤ºå’Œè½¨è¿¹æç¤ºï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›å’Œå˜æ¢å™¨æ¨¡å‹çš„ç»“æ„åŒ–è¡Œä¸ºæ•æ‰èƒ½åŠ›ï¼Œæ¥æŒ‡å¯¼ç­–ç•¥å­¦ä¹ ï¼Œä»è€Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ–‡æœ¬æç¤ºå¤„ç†æ¨¡å—å’Œè½¨è¿¹æç¤ºç¼–ç æ¨¡å—ã€‚æ–‡æœ¬æç¤ºé€šè¿‡LLMè¿›è¡Œå¤„ç†ï¼Œè½¨è¿¹æç¤ºåˆ™é€šè¿‡å˜æ¢å™¨æ¨¡å‹è¿›è¡Œç¼–ç ï¼Œæœ€ç»ˆè¾“å…¥åˆ°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç­–ç•¥çº§æ‰©æ•£æ¨¡å‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šLLMDPDçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è½¨è¿¹æç¤ºç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„ç­–ç•¥å­¦ä¹ æ–¹å¼ã€‚è¿™ç§æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç¦»çº¿RLæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æ–‡æœ¬ä¿¡æ¯å’Œç»“æ„åŒ–è¡Œä¸ºæ¨¡å¼ï¼Œä»è€Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæ–‡æœ¬æç¤ºå’Œè½¨è¿¹æç¤ºçš„é€‰æ‹©è‡³å…³é‡è¦ï¼Œéœ€ç¡®ä¿å…¶ä¸ä»»åŠ¡ç›¸å…³æ€§å¼ºã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿéœ€è€ƒè™‘åˆ°ç­–ç•¥çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥ä¼˜åŒ–å­¦ä¹ æ•ˆæœã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­ç»è¿‡è°ƒä¼˜ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMDPDåœ¨æœªè§ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨æé«˜æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§æ–¹é¢çš„æ˜¾è‘—æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¤„ç†å¤æ‚å†³ç­–çš„é¢†åŸŸï¼Œå¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ï¼ŒLLMDPDèƒ½å¤Ÿä½¿æ™ºèƒ½ä½“åœ¨å¤šå˜çš„ç¯å¢ƒä¸­æ›´å¥½åœ°é€‚åº”æ–°ä»»åŠ¡ï¼Œæå‡å®é™…åº”ç”¨çš„æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) is known for its strong decision-making capabilities and has been widely applied in various real-world scenarios. However, with the increasing availability of offline datasets and the lack of well-designed online environments from human experts, the challenge of generalization in offline RL has become more prominent. Due to the limitations of offline data, RL agents trained solely on collected experiences often struggle to generalize to new tasks or environments. To address this challenge, we propose LLM-Driven Policy Diffusion (LLMDPD), a novel approach that enhances generalization in offline RL using task-specific prompts. Our method incorporates both text-based task descriptions and trajectory prompts to guide policy learning. We leverage a large language model (LLM) to process text-based prompts, utilizing its natural language understanding and extensive knowledge base to provide rich task-relevant context. Simultaneously, we encode trajectory prompts using a transformer model, capturing structured behavioral patterns within the underlying transition dynamics. These prompts serve as conditional inputs to a context-aware policy-level diffusion model, enabling the RL agent to generalize effectively to unseen tasks. Our experimental results demonstrate that LLMDPD outperforms state-of-the-art offline RL methods on unseen tasks, highlighting its effectiveness in improving generalization and adaptability in diverse settings.

