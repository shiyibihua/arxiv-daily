---
layout: default
title: Universal Properties of Activation Sparsity in Modern Large Language Models
---

# Universal Properties of Activation Sparsity in Modern Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00454" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00454v1</a>
  <a href="https://arxiv.org/pdf/2509.00454.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00454v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00454v1', 'Universal Properties of Activation Sparsity in Modern Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Filip Szatkowski, Patryk BÄ™dkowski, Alessio Devoto, Jan DubiÅ„ski, Pasquale Minervini, MikoÅ‚aj PiÃ³rczyÅ„ski, Simone Scardapane, Bartosz WÃ³jcik

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨æ¡†æ¶ä»¥ç ”ç©¶ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ¿€æ´»ç¨€ç–æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¿€æ´»ç¨€ç–æ€§` `å¤§è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•ˆç‡æå‡` `é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹ReLUæ¿€æ´»ï¼Œæ— æ³•æœ‰æ•ˆåº”ç”¨äºç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼Œå¯¼è‡´æ¿€æ´»ç¨€ç–æ€§ç ”ç©¶ç¼ºä¹ç³»ç»Ÿæ€§å’Œä¸€è‡´æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å’Œç ”ç©¶ç°ä»£å¤§è¯­è¨€æ¨¡å‹ä¸­æ¿€æ´»ç¨€ç–æ€§çš„æ™®éç‰¹å¾ï¼Œå°¤å…¶æ˜¯åœ¨å‰é¦ˆç½‘ç»œå±‚ä¸­ã€‚
3. ç ”ç©¶å‘ç°äº†æ¿€æ´»ç¨€ç–æ€§çš„æ™®éæ¨¡å¼ï¼Œå¹¶ä¸ºæ¨¡å‹è®¾è®¡å’ŒåŠ é€Ÿæä¾›äº†å®ç”¨çš„æŒ‡å¯¼ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œå®é™…æ„ä¹‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¾“å…¥ä¾èµ–çš„æ¿€æ´»ç¨€ç–æ€§æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹æ€§ï¼Œå·²åœ¨ReLUæ¿€æ´»çš„ç½‘ç»œä¸­å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œå¹¶ä¸æ•ˆç‡ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ç›¸å…³ã€‚ç„¶è€Œï¼Œé’ˆå¯¹ReLUæ¨¡å‹å¼€å‘çš„æ–¹æ³•ä¾èµ–äºç²¾ç¡®çš„é›¶æ¿€æ´»ï¼Œæ— æ³•ç›´æ¥è½¬ç§»åˆ°ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œåè€…å·²æ”¾å¼ƒReLUè€Œé‡‡ç”¨å…¶ä»–æ¿€æ´»å‡½æ•°ã€‚å› æ­¤ï¼Œç›®å‰å¯¹LLMsä¸­æ¿€æ´»ç¨€ç–æ€§çš„ç ”ç©¶é›¶æ•£ä¸”ç¼ºä¹å…±è¯†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨æ¡†æ¶æ¥è¯„ä¼°ç¨€ç–æ€§é²æ£’æ€§ï¼Œå¹¶å¯¹ç°ä»£LLMsï¼ˆåŒ…æ‹¬æ‰©æ•£LLMsï¼‰ä¸­çš„ç°è±¡è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†LLMsä¸­æ¿€æ´»ç¨€ç–æ€§çš„æ™®éæ¨¡å¼ï¼Œæä¾›äº†å¯¹è¿™ä¸€ç°è±¡çš„æ·±å…¥è§è§£ï¼Œå¹¶ä¸ºæ¨¡å‹è®¾è®¡å’ŒåŠ é€Ÿæä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°ä»£å¤§è¯­è¨€æ¨¡å‹ä¸­æ¿€æ´»ç¨€ç–æ€§ç ”ç©¶çš„é›¶æ•£æ€§å’Œç¼ºä¹å…±è¯†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ReLUæ¿€æ´»ï¼Œæ— æ³•ç›´æ¥åº”ç”¨äºä½¿ç”¨å…¶ä»–æ¿€æ´»å‡½æ•°çš„æ¨¡å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ä¸ªé€šç”¨æ¡†æ¶æ¥è¯„ä¼°æ¿€æ´»ç¨€ç–æ€§ï¼Œç³»ç»Ÿç ”ç©¶å…¶åœ¨ç°ä»£å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å‰é¦ˆç½‘ç»œå±‚ä¸­ï¼Œæ—¨åœ¨æ­ç¤ºæ™®éæ¨¡å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒã€æ¿€æ´»ç¨€ç–æ€§è¯„ä¼°å’Œç»“æœåˆ†æç­‰ä¸»è¦æ¨¡å—ã€‚é€šè¿‡å¯¹ä¸åŒæ¨¡å‹çš„æ¯”è¾ƒï¼Œåˆ†ææ¿€æ´»ç¨€ç–æ€§çš„æ™®éç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºçš„æ¡†æ¶èƒ½å¤Ÿè·¨æ¨¡å‹è¯„ä¼°æ¿€æ´»ç¨€ç–æ€§ï¼Œæ­ç¤ºäº†ä¸åŒæ¿€æ´»å‡½æ•°ä¸‹çš„æ™®éæ¨¡å¼ï¼Œè¿™æ˜¯ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æ¿€æ´»å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç¨€ç–æ€§è¯„ä¼°ï¼Œç¡®ä¿ç»“æœçš„å‡†ç¡®æ€§å’Œå¯æ¯”æ€§ã€‚é€šè¿‡ç³»ç»Ÿçš„å®éªŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œåˆ©ç”¨æ¿€æ´»ç¨€ç–æ€§ï¼Œæå‡æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ¿€æ´»ç¨€ç–æ€§ä¼˜åŒ–åï¼Œæ¨¡å‹çš„æ¨ç†é€Ÿåº¦æé«˜äº†çº¦20%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„æ€§èƒ½æ°´å¹³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ¿€æ´»ç¨€ç–æ€§ï¼Œå¯ä»¥æå‡æ¨¡å‹çš„æ•ˆç‡å’Œé²æ£’æ€§ï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Input-dependent activation sparsity is a notable property of deep learning models, which has been extensively studied in networks with ReLU activations and is associated with efficiency, robustness, and interpretability. However, the approaches developed for ReLU-based models depend on exact zero activations and do not transfer directly to modern large language models~(LLMs), which have abandoned ReLU in favor of other activation functions. As a result, current work on activation sparsity in LLMs is fragmented, model-specific, and lacks consensus on which components to target. We propose a general framework to assess sparsity robustness and present a systematic study of the phenomenon in the FFN layers of modern LLMs, including diffusion LLMs. Our findings reveal universal patterns of activation sparsity in LLMs, provide insights into this phenomenon, and offer practical guidelines for exploiting it in model design and acceleration.

