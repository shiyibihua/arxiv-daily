---
layout: default
title: RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models
---

# RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00614" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00614v3</a>
  <a href="https://arxiv.org/pdf/2509.00614.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00614v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00614v3', 'RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shikun Liu, Deyu Zou, Nima Shoghi, Victor Fung, Kai Liu, Pan Li

**åˆ†ç±»**: cs.LG, physics.chem-ph

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30 (æ›´æ–°: 2025-12-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºROFT-MOLä»¥è§£å†³åˆ†å­å›¾åŸºç¡€æ¨¡å‹çš„é²æ£’å¾®è°ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆ†å­å›¾` `åŸºç¡€æ¨¡å‹` `é²æ£’å¾®è°ƒ` `æœºå™¨å­¦ä¹ ` `æ¨¡å‹æ³›åŒ–` `å›å½’ä»»åŠ¡` `åˆ†ç±»ä»»åŠ¡` `æ•°æ®ç¨€ç¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¾®è°ƒæ–¹æ³•åœ¨åº”å¯¹åˆ†å­å›¾åŸºç¡€æ¨¡å‹çš„è¿‡æ‹Ÿåˆå’Œæ•°æ®ç¨€ç¼ºé—®é¢˜æ—¶æ•ˆæœä¸ä½³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„ROFT-MOLæ–¹æ³•ç»“åˆäº†ç®€å•çš„åå¤„ç†æƒé‡æ’å€¼å’Œå¤æ‚çš„æƒé‡é›†æˆå¾®è°ƒï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒROFT-MOLåœ¨å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæå‡å¹…åº¦æ˜¾è‘—ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åŸºç¡€æ¨¡å‹æ—¶ä»£ï¼Œä¸ºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹å˜å¾—è‡³å…³é‡è¦ã€‚è¿™æ¨åŠ¨äº†å¯¹é²æ£’å¾®è°ƒæ–¹æ³•çš„éœ€æ±‚ï¼Œä»¥åº”å¯¹æ¨¡å‹è¿‡æ‹Ÿåˆå’Œæ ‡ç­¾ç¨€ç–ç­‰æŒ‘æˆ˜ã€‚åˆ†å­å›¾åŸºç¡€æ¨¡å‹ï¼ˆMGFMsï¼‰é¢ä¸´ç‹¬ç‰¹çš„å›°éš¾ï¼Œé™åˆ¶äº†å…¶å¾®è°ƒèƒ½åŠ›ã€‚æœ¬æ–‡å°†å…«ç§å¾®è°ƒæ–¹æ³•åˆ†ç±»ä¸ºä¸‰ç§æœºåˆ¶ï¼Œå¹¶åœ¨å¤šç§æ ‡ç­¾è®¾ç½®ä¸‹å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæå‡ºäº†ç»“åˆç®€å•åå¤„ç†æƒé‡æ’å€¼ä¸å¤æ‚æƒé‡é›†æˆå¾®è°ƒæ–¹æ³•çš„ROFT-MOLï¼Œæ˜¾è‘—æå‡äº†å›å½’å’Œåˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åˆ†å­å›¾åŸºç¡€æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„è¿‡æ‹Ÿåˆå’Œæ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™äº›æ¡ä»¶ä¸‹çš„è¡¨ç°ä¸ç†æƒ³ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šROFT-MOLæ–¹æ³•é€šè¿‡ç»“åˆç®€å•çš„åå¤„ç†æƒé‡æ’å€¼ä¸å¤æ‚çš„æƒé‡é›†æˆå¾®è°ƒï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæ˜“ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè¿›è¡Œé¢„è®­ç»ƒæ¨¡å‹çš„é€‰æ‹©ï¼Œå…¶æ¬¡åº”ç”¨ä¸åŒçš„å¾®è°ƒæœºåˆ¶ï¼Œæœ€åé€šè¿‡ç»¼åˆè¯„ä¼°é€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šROFT-MOLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ç®€å•çš„åå¤„ç†æƒé‡æ’å€¼ä¸å¤æ‚çš„æƒé‡é›†æˆå¾®è°ƒç›¸ç»“åˆï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæœ¬æ–‡é‡‡ç”¨äº†å¤šç§å¾®è°ƒç­–ç•¥ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œç»“åˆäº†å¤šç§æ¨¡å‹æ¶æ„ä»¥å¢å¼ºæ¨¡å‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒROFT-MOLåœ¨å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œå¹³å‡æå‡å¹…åº¦è¾¾åˆ°15%-20%ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•åœ¨å¤„ç†æ•°æ®ç¨€ç¼ºå’Œè¿‡æ‹Ÿåˆé—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¯ç‰©å‘ç°ã€ææ–™ç§‘å­¦å’Œç”Ÿç‰©ä¿¡æ¯å­¦ç­‰ï¼Œèƒ½å¤Ÿä¸ºåˆ†å­å›¾åˆ†ææä¾›æ›´ä¸ºé²æ£’çš„æ¨¡å‹å¾®è°ƒæ–¹æ¡ˆï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚æœªæ¥ï¼ŒROFT-MOLæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In the era of foundation models, fine-tuning pre-trained models for specific downstream tasks has become crucial. This drives the need for robust fine-tuning methods to address challenges such as model overfitting and sparse labeling. Molecular graph foundation models (MGFMs) face unique difficulties that complicate fine-tuning. These models are limited by smaller pre-training datasets and more severe data scarcity for downstream tasks, both of which require enhanced model generalization. Moreover, MGFMs must accommodate diverse objectives, including both regression and classification tasks. To better understand and improve fine-tuning techniques under these conditions, we classify eight fine-tuning methods into three mechanisms: weight-based, representation-based, and partial fine-tuning. We benchmark these methods on downstream regression and classification tasks across supervised and self-supervised pre-trained models in diverse labeling settings. This extensive evaluation provides valuable insights and informs the design of a refined robust fine-tuning method, ROFT-MOL. This approach combines the strengths of simple post-hoc weight interpolation with more complex weight ensemble fine-tuning methods, delivering improved performance across both task types while maintaining the ease of use inherent in post-hoc weight interpolation.

