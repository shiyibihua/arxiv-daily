---
layout: default
title: An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment
---

# An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00560" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.00560v1</a>
  <a href="https://arxiv.org/pdf/2509.00560.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00560v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00560v1', 'An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Can Cui, Zilong Fu, Penghe Huang, Yuanyuan Li, Wu Deng, Dongyan Li

**ÂàÜÁ±ª**: cs.LG, cs.PF

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SA-DSDÊ°ÜÊû∂‰ª•ÊèêÂçáGNNÁü•ËØÜËí∏È¶èËá≥KANÁöÑÊïàÁéá**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Áü•ËØÜËí∏È¶è` `ÂõæÁ•ûÁªèÁΩëÁªú` `Kolmogorov-ArnoldÁΩëÁªú` `Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂` `Âä®ÊÄÅÈááÊ†∑` `ËæπÁºòËÆ°ÁÆó` `Ê®°ÂûãÂéãÁº©`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÂ±ÇÊÑüÁü•Âô®ÔºàMLPÔºâÂú®ÊçïÊçâGNNÂ≠¶‰π†ÁöÑÂ§çÊùÇÈÇªÂüü‰æùËµñÊÄßÊñπÈù¢Â≠òÂú®Â±ÄÈôêÔºåÂΩ±Âìç‰∫ÜÂÖ∂Âú®ËæπÁºòÁéØÂ¢É‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑSA-DSDÊ°ÜÊû∂ÈÄöËøáÂä®ÊÄÅÈááÊ†∑ÂíåËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ºòÂåñ‰∫ÜGNNÂà∞KANÁöÑÁü•ËØÜËí∏È¶èËøáÁ®ãÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSA-DSDÂú®ÂÖ≠‰∏™ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÁõ∏ËæÉ‰∫éGNNÊïôÂ∏àÊ®°ÂûãÊèêÂçá‰∫Ü3.05%-3.62%ÁöÑÊÄßËÉΩÔºåÂπ∂Âú®ÂèÇÊï∞Êï∞ÈáèÂíåÊé®ÁêÜÊó∂Èó¥‰∏äÊúâÊòæËëóÂáèÂ∞ë„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áü•ËØÜËí∏È¶èÔºàKDÔºâÂú®ËµÑÊ∫êÂèóÈôêÁöÑËæπÁºòÁéØÂ¢É‰∏≠ÈÉ®ÁΩ≤Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãËá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê∂àË¥πÁîµÂ≠êÈ¢ÜÂüü„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàõÊñ∞ÁöÑÁü•ËØÜËí∏È¶èÊ°ÜÊû∂‚Äî‚ÄîËá™Ê≥®ÊÑèÂäõÂä®ÊÄÅÈááÊ†∑Ëí∏È¶èÔºàSA-DSDÔºâÔºåÊó®Âú®Â∞ÜÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNÔºâ‰∏≠ÁöÑÁü•ËØÜÊúâÊïàËΩ¨ÁßªËá≥Êõ¥È´òÊïàÁöÑKolmogorov-ArnoldÁΩëÁªúÔºàKANÔºâ„ÄÇÈÄöËøáÂºïÂÖ•ÂèØÂ≠¶‰π†ÁöÑÈ¢ëÁéáÂü∫ÂíåÁõ∏‰ΩçÂÅèÁßªÊú∫Âà∂ÔºåFR-KANÊòæËëóÊèêÈ´ò‰∫ÜÈùûÁ∫øÊÄßÊãüÂêàËÉΩÂäõÂπ∂Èôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSA-DSDÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåÂπ∂Âú®ÂèÇÊï∞Êï∞ÈáèÂíåÊé®ÁêÜÊó∂Èó¥‰∏äÂÆûÁé∞‰∫ÜÊòæËëó‰ºòÂåñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÂ±ÇÊÑüÁü•Âô®ÔºàMLPÔºâÂú®ÊçïÊçâÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNÔºâÂ§çÊùÇÈÇªÂüü‰æùËµñÊÄßÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÂØºËá¥ÂÖ∂Âú®ËæπÁºòËÆ°ÁÆóÁéØÂ¢É‰∏≠ÁöÑÊÄßËÉΩÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåÂä®ÊÄÅÈááÊ†∑ÔºåÊûÑÂª∫Ëá™Ê≥®ÊÑèÂäõÂä®ÊÄÅÈááÊ†∑Ëí∏È¶èÔºàSA-DSDÔºâÊ°ÜÊû∂ÔºåÊó®Âú®ÊúâÊïàËΩ¨ÁßªGNNÁöÑÁü•ËØÜËá≥Êõ¥È´òÊïàÁöÑKolmogorov-ArnoldÁΩëÁªúÔºàKANÔºâ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSA-DSDÊ°ÜÊû∂ÂåÖÊã¨ÊïôÂ∏àÊ®°ÂûãÔºàGNNÔºâ„ÄÅÂ≠¶ÁîüÊ®°ÂûãÔºàFR-KAN+Ôºâ„ÄÅÂä®ÊÄÅÈááÊ†∑Ê¶ÇÁéáÁü©ÈòµÂíåËá™ÈÄÇÂ∫îÂä†ÊùÉÊçüÂ§±Êú∫Âà∂Á≠âÊ®°ÂùóÔºåÂΩ¢Êàê‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÁü•ËØÜËí∏È¶èÊµÅÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂºïÂÖ•ÂèØÂ≠¶‰π†ÁöÑÈ¢ëÁéáÂü∫ÂíåÁõ∏‰ΩçÂÅèÁßªÊú∫Âà∂ÔºåÊòæËëóÊèêÂçáFR-KANÁöÑÈùûÁ∫øÊÄßÊãüÂêàËÉΩÂäõÔºåÂêåÊó∂ÈÄöËøáÂä®ÊÄÅÈááÊ†∑Á≠ñÁï•‰ºòÂåñÁü•ËØÜËΩ¨ÁßªËøáÁ®ãÔºåÂÖãÊúç‰∫ÜMLPÁöÑÂõ∫ÂÆöÊøÄÊ¥ªÂáΩÊï∞ÈôêÂà∂„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆæËÆ°‰∫ÜÂü∫‰∫éÊïôÂ∏à-Â≠¶ÁîüÈ¢ÑÊµã‰∏ÄËá¥ÊÄßÁöÑËæπÈôÖÁ∫ßÂà´ÈááÊ†∑Ê¶ÇÁéáÁü©ÈòµÔºåÂπ∂ÈááÁî®Ëá™ÈÄÇÂ∫îÂä†ÊùÉÊçüÂ§±Êú∫Âà∂Ôºå‰ª•ÂáèËΩªÂ≠¶ÁîüÊ®°ÂûãÂõ†Áº∫‰πèÊòæÂºèÈÇªÂüüËÅöÂêàËÄåÂØºËá¥ÁöÑÊÄßËÉΩ‰∏ãÈôç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SA-DSDÂú®ÂÖ≠‰∏™ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÁõ∏ËæÉ‰∫é‰∏âÁßçGNNÊïôÂ∏àÊ®°ÂûãÂÆûÁé∞‰∫Ü3.05%-3.62%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÁõ∏ËæÉ‰∫éFR-KAN+Ê®°ÂûãÊèêÂçá‰∫Ü15.61%„ÄÇÊ≠§Â§ñÔºåSA-DSDÂú®ÂèÇÊï∞Êï∞Èáè‰∏äÂáèÂ∞ë‰∫Ü16.96ÂÄçÔºåÊé®ÁêÜÊó∂Èó¥ÂáèÂ∞ë‰∫Ü55.75%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Êô∫ËÉΩÂÆ∂Â±ÖËÆæÂ§á„ÄÅÂèØÁ©øÊà¥ÊäÄÊúØÂíåÁßªÂä®ÁªàÁ´ØÁ≠âÊ∂àË¥πÁîµÂ≠êÈ¢ÜÂüü„ÄÇÈÄöËøá‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶ÂíåÂèÇÊï∞ÊïàÁéáÔºåSA-DSDËÉΩÂ§üÂú®ËµÑÊ∫êÂèóÈôêÁöÑËæπÁºòËÆ°ÁÆóÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥È´òÊïàÁöÑÊ∑±Â∫¶Â≠¶‰π†Â∫îÁî®ÔºåÊé®Âä®Êô∫ËÉΩËÆæÂ§áÁöÑÊô∫ËÉΩÂåñÂíåÊôÆÂèä„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïËøòÂèØËÉΩÊâ©Â±ïÂà∞ÂÖ∂‰ªñÈúÄË¶ÅÈ´òÊïàÊ®°ÂûãÈÉ®ÁΩ≤ÁöÑÈ¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Knowledge distillation (KD) is crucial for deploying deep learning models in resource-constrained edge environments, particularly within the consumer electronics sector, including smart home devices, wearable technology, and mobile terminals. These applications place higher demands on model compression and inference speed, necessitating the transfer of knowledge from Graph Neural Networks (GNNs) to more efficient Multi-Layer Perceptron (MLP) models. However, due to their fixed activation functions and fully connected architecture, MLPs face challenges in rapidly capturing the complex neighborhood dependencies learned by GNNs, thereby limiting their performance in edge environments. To address these limitations, this paper introduces an innovative from GNNs to Kolmogorov-Arnold Networks (KANs) knowledge distillation framework-Self Attention Dynamic Sampling Distillation (SA-DSD). This study improved Fourier KAN (FR-KAN) and replaced MLP with the improved FR-KAN+ as the student model. Through the incorporation of learnable frequency bases and phase-shift mechanisms, along with algorithmic optimization, FR-KAN significantly improves its nonlinear fitting capability while effectively reducing computational complexity. Building on this, a margin-level sampling probability matrix, based on teacher-student prediction consistency, is constructed, and an adaptive weighted loss mechanism is designed to mitigate performance degradation in the student model due to the lack of explicit neighborhood aggregation. Extensive experiments conducted on six real-world datasets demonstrate that SA-DSD achieves performance improvements of 3.05%-3.62% over three GNN teacher models and 15.61% over the FR-KAN+ model. Moreover, when compared with key benchmark models, SA-DSD achieves a 16.96x reduction in parameter count and a 55.75% decrease in inference time.

