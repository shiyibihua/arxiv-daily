---
layout: default
title: Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data
---

# Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10303" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10303v1</a>
  <a href="https://arxiv.org/pdf/2509.10303.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10303v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10303v1', 'Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jesse van Remmerden, Zaharah Bukhsh, Yingqian Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCDQACç®—æ³•ï¼Œé€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä»éšæœºæ•°æ®ä¸­å­¦ä¹ é«˜æ•ˆä½œä¸šè°ƒåº¦ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `ä½œä¸šè°ƒåº¦é—®é¢˜` `åˆ†ä½æ•°Actor-Critic` `ä¿å®ˆç­–ç•¥æ›´æ–°` `ç»„åˆä¼˜åŒ–` `å·¥ä¸šåº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä½œä¸šè°ƒåº¦é—®é¢˜ä¸­éœ€è¦å¤§é‡ä¸æ¨¡æ‹Ÿç¯å¢ƒçš„äº¤äº’ï¼Œä¸”éšæœºç­–ç•¥åˆå§‹åŒ–å¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ã€‚
2. CDQACç®—æ³•é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œç›´æ¥ä»å†å²æ•°æ®ä¸­å­¦ä¹ è°ƒåº¦ç­–ç•¥ï¼Œæ— éœ€åœ¨çº¿äº¤äº’ï¼Œå¹¶èƒ½ä»æ¬¡ä¼˜æ•°æ®ä¸­æå‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCDQACä¼˜äºç°æœ‰ç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä¸”æ ·æœ¬æ•ˆç‡é«˜ï¼Œåœ¨éšæœºæ•°æ®ä¸Šè®­ç»ƒæ•ˆæœæ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹Job-Shopè°ƒåº¦é—®é¢˜(JSP)å’ŒæŸ”æ€§Job-Shopè°ƒåº¦é—®é¢˜(FJSP)ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”ä¿å®ˆç¦»æ•£åˆ†ä½æ•°Actor-Critic (CDQAC)ã€‚è¯¥ç®—æ³•ç›´æ¥ä»å†å²æ•°æ®ä¸­å­¦ä¹ æœ‰æ•ˆçš„è°ƒåº¦ç­–ç•¥ï¼Œæ— éœ€è€—æ—¶çš„åœ¨çº¿äº¤äº’ï¼Œå¹¶èƒ½ä»æ¬¡ä¼˜è®­ç»ƒæ•°æ®ä¸­è¿›è¡Œæ”¹è¿›ã€‚CDQACå°†åŸºäºåˆ†ä½æ•°çš„è¯„è®ºå®¶ä¸å»¶è¿Ÿç­–ç•¥æ›´æ–°ç›¸ç»“åˆï¼Œä¼°è®¡æ¯ä¸ªæœºå™¨-æ“ä½œå¯¹çš„å›æŠ¥åˆ†å¸ƒï¼Œè€Œä¸æ˜¯ç›´æ¥é€‰æ‹©é…å¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCDQACèƒ½å¤Ÿä»å¤šæ ·åŒ–çš„æ•°æ®æºä¸­å­¦ä¹ ï¼Œå§‹ç»ˆä¼˜äºåŸå§‹æ•°æ®ç”Ÿæˆå¯å‘å¼æ–¹æ³•ï¼Œå¹¶è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚æ­¤å¤–ï¼ŒCDQACå…·æœ‰å¾ˆé«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œä»…éœ€10-20ä¸ªè®­ç»ƒå®ä¾‹å³å¯å­¦ä¹ é«˜è´¨é‡ç­–ç•¥ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒCDQACåœ¨éšæœºå¯å‘å¼ç®—æ³•ç”Ÿæˆçš„æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œæ€§èƒ½ä¼˜äºåœ¨é—ä¼ ç®—æ³•å’Œä¼˜å…ˆçº§è°ƒåº¦è§„åˆ™ç­‰æ›´é«˜è´¨é‡æ•°æ®ä¸Šè®­ç»ƒæ—¶çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³Job-Shopè°ƒåº¦é—®é¢˜(JSP)å’ŒæŸ”æ€§Job-Shopè°ƒåº¦é—®é¢˜(FJSP)ã€‚ç°æœ‰åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡çš„ç¯å¢ƒäº¤äº’ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œä¸”æ¨¡æ‹Ÿç¯å¢ƒå¯èƒ½æ— æ³•å®Œå…¨æ•æ‰çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œéšæœºç­–ç•¥åˆå§‹åŒ–å¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ï¼Œæ”¶æ•›é€Ÿåº¦æ…¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œç›´æ¥ä»å†å²æ•°æ®ä¸­å­¦ä¹ è°ƒåº¦ç­–ç•¥ï¼Œé¿å…åœ¨çº¿äº¤äº’çš„æˆæœ¬ã€‚é€šè¿‡ä¿å®ˆç­–ç•¥æ›´æ–°ï¼Œå³ä½¿è®­ç»ƒæ•°æ®æ˜¯æ¬¡ä¼˜çš„ï¼Œä¹Ÿèƒ½å­¦ä¹ åˆ°æ›´ä¼˜çš„ç­–ç•¥ã€‚ä½¿ç”¨åˆ†ä½æ•°Actor-Criticæ–¹æ³•ï¼Œå­¦ä¹ å›æŠ¥çš„åˆ†å¸ƒï¼Œè€Œä¸æ˜¯å•ä¸€çš„æœŸæœ›å€¼ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†ä¸ç¡®å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCDQACç®—æ³•åŒ…å«ä¸€ä¸ªåŸºäºåˆ†ä½æ•°çš„è¯„è®ºå®¶å’Œä¸€ä¸ªActorç½‘ç»œã€‚è¯„è®ºå®¶ç½‘ç»œç”¨äºä¼°è®¡æ¯ä¸ªæœºå™¨-æ“ä½œå¯¹çš„å›æŠ¥åˆ†å¸ƒã€‚Actorç½‘ç»œæ ¹æ®è¯„è®ºå®¶ç½‘ç»œçš„è¾“å‡ºï¼Œé€‰æ‹©æœ€ä¼˜çš„æœºå™¨-æ“ä½œå¯¹ã€‚ç®—æ³•é‡‡ç”¨å»¶è¿Ÿç­–ç•¥æ›´æ–°ï¼Œä»¥ä¿è¯ç­–ç•¥çš„ç¨³å®šæ€§ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆï¼Œä½¿ç”¨å†å²æ•°æ®è®­ç»ƒè¯„è®ºå®¶ç½‘ç»œï¼›ç„¶åï¼Œä½¿ç”¨è¯„è®ºå®¶ç½‘ç»œæŒ‡å¯¼Actorç½‘ç»œçš„è®­ç»ƒï¼›æœ€åï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„Actorç½‘ç»œè¿›è¡Œè°ƒåº¦å†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šCDQACçš„å…³é”®åˆ›æ–°åœ¨äºç»“åˆäº†åˆ†ä½æ•°Actor-Criticæ–¹æ³•å’Œä¿å®ˆç­–ç•¥æ›´æ–°ï¼Œä½¿å…¶èƒ½å¤Ÿä»æ¬¡ä¼˜çš„ç¦»çº¿æ•°æ®ä¸­å­¦ä¹ åˆ°æœ‰æ•ˆçš„è°ƒåº¦ç­–ç•¥ã€‚ä¼ ç»ŸActor-Criticæ–¹æ³•é€šå¸¸å­¦ä¹ æœŸæœ›å›æŠ¥ï¼Œè€ŒCDQACå­¦ä¹ å›æŠ¥çš„åˆ†å¸ƒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è°ƒåº¦é—®é¢˜ä¸­çš„ä¸ç¡®å®šæ€§ã€‚ä¿å®ˆç­–ç•¥æ›´æ–°åˆ™é¿å…äº†ç­–ç•¥è¿‡åº¦ä¼˜åŒ–ï¼Œæé«˜äº†ç®—æ³•çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šCDQACä½¿ç”¨åˆ†ä½æ•°å›å½’æŸå¤±å‡½æ•°è®­ç»ƒè¯„è®ºå®¶ç½‘ç»œï¼Œä»¥å­¦ä¹ å›æŠ¥åˆ†å¸ƒã€‚Actorç½‘ç»œä½¿ç”¨softmaxç­–ç•¥ï¼Œé€‰æ‹©å…·æœ‰æœ€é«˜å›æŠ¥çš„æœºå™¨-æ“ä½œå¯¹ã€‚ä¿å®ˆç­–ç•¥æ›´æ–°é€šè¿‡é™åˆ¶ç­–ç•¥çš„æ›´æ–°å¹…åº¦ï¼Œé˜²æ­¢ç­–ç•¥åç¦»è®­ç»ƒæ•°æ®è¿‡è¿œã€‚å…·ä½“å‚æ•°è®¾ç½®åŒ…æ‹¬å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ã€åˆ†ä½æ•°æ•°é‡ç­‰ï¼Œè¿™äº›å‚æ•°éœ€è¦æ ¹æ®å…·ä½“é—®é¢˜è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCDQACç®—æ³•åœ¨JSPå’ŒFJSPé—®é¢˜ä¸Šå‡ä¼˜äºç°æœ‰çš„ç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚CDQACä»…éœ€10-20ä¸ªè®­ç»ƒå®ä¾‹å³å¯å­¦ä¹ é«˜è´¨é‡ç­–ç•¥ï¼Œæ ·æœ¬æ•ˆç‡æ˜¾è‘—æé«˜ã€‚æ›´ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒCDQACåœ¨éšæœºå¯å‘å¼ç®—æ³•ç”Ÿæˆçš„æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œæ€§èƒ½ä¼˜äºåœ¨é—ä¼ ç®—æ³•å’Œä¼˜å…ˆçº§è°ƒåº¦è§„åˆ™ç­‰æ›´é«˜è´¨é‡æ•°æ®ä¸Šè®­ç»ƒæ—¶çš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜CDQACå…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CDQACç®—æ³•å¯å¹¿æ³›åº”ç”¨äºå·¥ä¸šåˆ¶é€ ã€ç‰©æµè¿è¾“ã€äº‘è®¡ç®—èµ„æºè°ƒåº¦ç­‰é¢†åŸŸã€‚é€šè¿‡åˆ©ç”¨å†å²è°ƒåº¦æ•°æ®ï¼Œä¼ä¸šå¯ä»¥å¿«é€Ÿéƒ¨ç½²é«˜æ•ˆçš„è°ƒåº¦ç­–ç•¥ï¼Œæé«˜ç”Ÿäº§æ•ˆç‡ï¼Œé™ä½è¿è¥æˆæœ¬ã€‚è¯¥ç ”ç©¶ä¸ºè§£å†³å®é™…ç”Ÿäº§ä¸­çš„å¤æ‚è°ƒåº¦é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ¨å¹¿å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling Problem (FJSP), are canonical combinatorial optimization problems with wide-ranging applications in industrial operations. In recent years, many online reinforcement learning (RL) approaches have been proposed to learn constructive heuristics for JSP and FJSP. Although effective, these online RL methods require millions of interactions with simulated environments that may not capture real-world complexities, and their random policy initialization leads to poor sample efficiency. To address these limitations, we introduce Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL algorithm that learns effective scheduling policies directly from historical data, eliminating the need for costly online interactions, while maintaining the ability to improve upon suboptimal training data. CDQAC couples a quantile-based critic with a delayed policy update, estimating the return distribution of each machine-operation pair rather than selecting pairs outright. Our extensive experiments demonstrate CDQAC's remarkable ability to learn from diverse data sources. CDQAC consistently outperforms the original data-generating heuristics and surpasses state-of-the-art offline and online RL baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20 training instances to learn high-quality policies. Surprisingly, we find that CDQAC performs better when trained on data generated by a random heuristic than when trained on higher-quality data from genetic algorithms and priority dispatching rules.

