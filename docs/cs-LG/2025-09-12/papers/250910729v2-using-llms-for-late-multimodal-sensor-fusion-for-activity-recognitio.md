---
layout: default
title: Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition
---

# Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10729" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10729v2</a>
  <a href="https://arxiv.org/pdf/2509.10729.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10729v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10729v2', 'Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ilker Demirel, Karan Thakkar, Benjamin Elizalde, Miquel Espi Marques, Shirley Ren, Jaya Narain

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-11-23)

**å¤‡æ³¨**: Preprint, under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨LLMè¿›è¡Œæ´»åŠ¨è¯†åˆ«çš„åæœŸå¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `æ´»åŠ¨è¯†åˆ«` `å¤§å‹è¯­è¨€æ¨¡å‹` `é›¶æ ·æœ¬å­¦ä¹ ` `ä¼ æ„Ÿå™¨æ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ´»åŠ¨è¯†åˆ«æ–¹æ³•åœ¨èåˆå¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®æ—¶ï¼Œéœ€è¦å¤§é‡å¯¹é½çš„è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†å…¶åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„åº”ç”¨ã€‚
2. è¯¥è®ºæ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒåæœŸèåˆï¼Œå°†æ¥è‡ªä¸åŒæ¨¡æ€çš„ç‰¹å¾è¿›è¡Œæ•´åˆï¼Œå®ç°é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æ´»åŠ¨è¯†åˆ«ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Ego4Dæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—é«˜äºéšæœºæ°´å¹³çš„F1åˆ†æ•°ï¼ŒéªŒè¯äº†LLMåœ¨å¤šæ¨¡æ€èåˆæ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ æ„Ÿå™¨æ•°æ®æµä¸ºä¸‹æ¸¸åº”ç”¨æä¾›äº†å…³äºæ´»åŠ¨å’Œä¸Šä¸‹æ–‡çš„æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œä½†æ•´åˆäº’è¡¥ä¿¡æ¯å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºä»éŸ³é¢‘å’Œè¿åŠ¨æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œæ´»åŠ¨åˆ†ç±»çš„åæœŸèåˆã€‚æˆ‘ä»¬ä»Ego4Dæ•°æ®é›†ä¸­æ•´ç†äº†ä¸€ä¸ªæ•°æ®å­é›†ï¼Œç”¨äºè·¨ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œå®¶åº­æ´»åŠ¨ã€ä½“è‚²è¿åŠ¨ï¼‰çš„å¤šæ ·åŒ–æ´»åŠ¨è¯†åˆ«ã€‚è¯„ä¼°çš„LLMå®ç°äº†æ˜¾è‘—é«˜äºéšæœºæ°´å¹³çš„12ç±»é›¶æ ·æœ¬å’Œä¸€æ¬¡æ ·æœ¬åˆ†ç±»F1åˆ†æ•°ï¼Œä¸”æ²¡æœ‰ç‰¹å®šäºä»»åŠ¡çš„è®­ç»ƒã€‚é€šè¿‡åŸºäºLLMçš„ã€æ¥è‡ªæ¨¡æ€ç‰¹å®šæ¨¡å‹çš„èåˆè¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»ï¼Œå¯ä»¥å®ç°å¤šæ¨¡æ€æ—¶é—´åº”ç”¨ï¼Œåœ¨è¿™ç§åº”ç”¨ä¸­ï¼Œç”¨äºå­¦ä¹ å…±äº«åµŒå…¥ç©ºé—´çš„å¯¹é½è®­ç»ƒæ•°æ®æœ‰é™ã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„èåˆå¯ä»¥å®ç°æ¨¡å‹éƒ¨ç½²ï¼Œè€Œæ— éœ€é¢å¤–çš„å†…å­˜å’Œè®¡ç®—æ¥æ”¯æŒé¢å‘ç‰¹å®šåº”ç”¨çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ´»åŠ¨è¯†åˆ«ä¸­ï¼Œç”±äºç¼ºä¹å¯¹é½çš„è®­ç»ƒæ•°æ®è€Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®æ¥å­¦ä¹ å…±äº«åµŒå…¥ç©ºé—´ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€éš¾ä»¥æ»¡è¶³ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨æœ‰é™çš„æ•°æ®å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€èåˆæ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå°†æ¥è‡ªä¸åŒæ¨¡æ€ï¼ˆå¦‚éŸ³é¢‘å’Œè¿åŠ¨ï¼‰çš„ç‰¹å¾è¿›è¡Œèåˆã€‚é€šè¿‡å°†æ¨¡æ€ç‰¹å®šçš„ä¿¡æ¯è½¬åŒ–ä¸ºLLMå¯ä»¥ç†è§£çš„æ–‡æœ¬æè¿°ï¼Œä»è€Œå®ç°è·¨æ¨¡æ€çš„çŸ¥è¯†è¿ç§»å’Œæ³›åŒ–ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥å­¦ä¹ å…±äº«åµŒå…¥ç©ºé—´ï¼Œé™ä½äº†å¯¹å¯¹é½è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ¨¡æ€ç‰¹å®šç‰¹å¾æå–ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚éŸ³é¢‘åˆ†ç±»å™¨å’Œè¿åŠ¨ä¼ æ„Ÿå™¨æ¨¡å‹ï¼‰ä»åŸå§‹æ•°æ®ä¸­æå–ç‰¹å¾ã€‚2) ç‰¹å¾æè¿°ï¼šå°†æå–çš„ç‰¹å¾è½¬åŒ–ä¸ºæ–‡æœ¬æè¿°ï¼Œä¾‹å¦‚ï¼Œå°†éŸ³é¢‘ç‰¹å¾æè¿°ä¸ºâ€œæœ‰è¯´è¯å£°â€æˆ–â€œæœ‰èƒŒæ™¯éŸ³ä¹â€ï¼Œå°†è¿åŠ¨ç‰¹å¾æè¿°ä¸ºâ€œå¿«é€Ÿç§»åŠ¨â€æˆ–â€œé™æ­¢â€ã€‚3) LLMèåˆï¼šå°†ä¸åŒæ¨¡æ€çš„æ–‡æœ¬æè¿°è¾“å…¥åˆ°LLMä¸­ï¼ŒLLMæ ¹æ®è¿™äº›æè¿°è¿›è¡Œæ¨ç†ï¼Œé¢„æµ‹å½“å‰çš„æ´»åŠ¨ç±»åˆ«ã€‚4) åˆ†ç±»è¾“å‡ºï¼šLLMè¾“å‡ºæ´»åŠ¨ç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ä½œä¸ºæœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨LLMè¿›è¡Œå¤šæ¨¡æ€åæœŸèåˆï¼Œå®ç°äº†é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æ´»åŠ¨è¯†åˆ«ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€èåˆæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¤§é‡çš„å¯¹é½è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥åˆ©ç”¨LLMçš„é¢„è®­ç»ƒçŸ¥è¯†è¿›è¡Œè·¨æ¨¡æ€çš„çŸ¥è¯†è¿ç§»ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥å®ç°æ¨¡å‹éƒ¨ç½²ï¼Œè€Œæ— éœ€é¢å¤–çš„å†…å­˜å’Œè®¡ç®—æ¥æ”¯æŒé¢å‘ç‰¹å®šåº”ç”¨çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„LLMï¼šé€‰æ‹©å…·æœ‰å¼ºå¤§è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„LLMï¼Œä¾‹å¦‚ï¼ŒGPT-3æˆ–T5ã€‚2) è®¾è®¡æœ‰æ•ˆçš„ç‰¹å¾æè¿°æ–¹æ³•ï¼šå°†æ¨¡æ€ç‰¹å®šçš„ç‰¹å¾è½¬åŒ–ä¸ºLLMå¯ä»¥ç†è§£çš„æ–‡æœ¬æè¿°ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°éŸ³é¢‘å’Œè¿åŠ¨ç‰¹å¾ã€‚3) ä¼˜åŒ–LLMçš„è¾“å…¥æ ¼å¼ï¼šå°†ä¸åŒæ¨¡æ€çš„æ–‡æœ¬æè¿°ä»¥åˆé€‚çš„æ–¹å¼è¾“å…¥åˆ°LLMä¸­ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨ç‰¹å®šçš„åˆ†éš”ç¬¦æˆ–æç¤ºè¯­ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Ego4Dæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†æ˜¾è‘—é«˜äºéšæœºæ°´å¹³çš„12ç±»é›¶æ ·æœ¬å’Œä¸€æ¬¡æ ·æœ¬åˆ†ç±»F1åˆ†æ•°ï¼ŒéªŒè¯äº†LLMåœ¨å¤šæ¨¡æ€èåˆæ–¹é¢çš„æ½œåŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº†XX%çš„F1åˆ†æ•°ï¼Œåœ¨ä¸€æ¬¡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº†YY%çš„F1åˆ†æ•°ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸‹å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€æ´»åŠ¨è¯†åˆ«ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€è¿åŠ¨å¥åº·ç›‘æµ‹ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¶å±…ä¸­ï¼Œå¯ä»¥é€šè¿‡èåˆéŸ³é¢‘å’Œè¿åŠ¨ä¼ æ„Ÿå™¨æ•°æ®ï¼Œè¯†åˆ«ç”¨æˆ·çš„æ—¥å¸¸æ´»åŠ¨ï¼Œä»è€Œå®ç°æ™ºèƒ½åŒ–çš„ç¯å¢ƒæ§åˆ¶å’ŒæœåŠ¡ã€‚åœ¨è¿åŠ¨å¥åº·ç›‘æµ‹ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•è¯†åˆ«ç”¨æˆ·çš„è¿åŠ¨ç±»å‹å’Œå¼ºåº¦ï¼Œä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„è¿åŠ¨å»ºè®®ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæœºå™¨äººé¢†åŸŸï¼Œå¸®åŠ©æœºå™¨äººç†è§£äººç±»çš„æ´»åŠ¨æ„å›¾ï¼Œä»è€Œå®ç°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sensor data streams provide valuable information around activities and context for downstream applications, though integrating complementary information can be challenging. We show that large language models (LLMs) can be used for late fusion for activity classification from audio and motion time series data. We curated a subset of data for diverse activity recognition across contexts (e.g., household activities, sports) from the Ego4D dataset. Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores significantly above chance, with no task-specific training. Zero-shot classification via LLM-based fusion from modality-specific models can enable multimodal temporal applications where there is limited aligned training data for learning a shared embedding space. Additionally, LLM-based fusion can enable model deploying without requiring additional memory and computation for targeted application-specific multimodal models.

