---
layout: default
title: Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks
---

# Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10163" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10163v1</a>
  <a href="https://arxiv.org/pdf/2509.10163.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10163v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10163v1', 'Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Francisco Javier Esono Nkulu Andong, Qi Min

**åˆ†ç±»**: cs.LG, cs.IT

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFed-MARLæ¡†æ¶ï¼Œè§£å†³6Gè¾¹ç¼˜ç½‘ç»œä¸­éšç§ä¿æŠ¤å’ŒèŠ‚èƒ½çš„èµ„æºç®¡ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `6Gè¾¹ç¼˜ç½‘ç»œ` `èµ„æºç®¡ç†` `éšç§ä¿æŠ¤` `æ·±åº¦å¾ªç¯Qç½‘ç»œ` `è·¨å±‚ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. 6Gè¾¹ç¼˜ç½‘ç»œé¢ä¸´ä¸¥æ ¼çš„éšç§ã€ç§»åŠ¨æ€§å’Œèƒ½æºçº¦æŸä¸‹çš„é«˜æ•ˆèµ„æºç®¡ç†éš¾é¢˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾è¿™äº›å› ç´ ã€‚
2. è®ºæ–‡æå‡ºFed-MARLæ¡†æ¶ï¼Œåˆ©ç”¨è”é‚¦å­¦ä¹ ä¿æŠ¤éšç§ï¼Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å®ç°å»ä¸­å¿ƒåŒ–å†³ç­–ï¼Œè·¨å±‚ä¼˜åŒ–æå‡èµ„æºåˆ©ç”¨ç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFed-MARLåœ¨ä»»åŠ¡æˆåŠŸç‡ã€å»¶è¿Ÿã€èƒ½æºæ•ˆç‡å’Œå…¬å¹³æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶æä¾›å¼ºå¤§çš„éšç§ä¿æŠ¤ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è”é‚¦å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆFed-MARLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†MACå±‚å’Œåº”ç”¨å±‚çš„è·¨å±‚ç¼–æ’ï¼Œä»¥å®ç°å¼‚æ„è¾¹ç¼˜è®¾å¤‡ä¸ŠèŠ‚èƒ½ã€éšç§ä¿æŠ¤å’Œå®æ—¶çš„èµ„æºç®¡ç†ã€‚æ¯ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨æ·±åº¦å¾ªç¯Qç½‘ç»œï¼ˆDRQNï¼‰æ¥å­¦ä¹ å»ä¸­å¿ƒåŒ–çš„ç­–ç•¥ï¼Œç”¨äºä»»åŠ¡å¸è½½ã€é¢‘è°±æ¥å…¥å’ŒCPUèƒ½é‡è‡ªé€‚åº”ï¼Œè¿™äº›ç­–ç•¥åŸºäºæœ¬åœ°è§‚å¯Ÿï¼ˆä¾‹å¦‚ï¼Œé˜Ÿåˆ—é•¿åº¦ã€èƒ½é‡ã€CPUä½¿ç”¨ç‡å’Œç§»åŠ¨æ€§ï¼‰ã€‚ä¸ºäº†ä¿æŠ¤éšç§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ¤­åœ†æ›²çº¿Diffie-Hellmanå¯†é’¥äº¤æ¢çš„å®‰å…¨èšåˆåè®®ï¼Œè¯¥åè®®ç¡®ä¿äº†å‡†ç¡®çš„æ¨¡å‹æ›´æ–°ï¼Œè€Œä¸ä¼šå°†åŸå§‹æ•°æ®æš´éœ²ç»™åŠè¯šå®å¯¹æ‰‹ã€‚æˆ‘ä»¬å°†èµ„æºç®¡ç†é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªéƒ¨åˆ†å¯è§‚å¯Ÿçš„å¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMMDPï¼‰ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªå¤šç›®æ ‡å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨6Gç‰¹å®šçš„æœåŠ¡éœ€æ±‚ï¼ˆå¦‚URLLCã€eMBBå’ŒmMTCï¼‰ä¸‹ï¼Œå…±åŒä¼˜åŒ–å»¶è¿Ÿã€èƒ½æºæ•ˆç‡ã€é¢‘è°±æ•ˆç‡ã€å…¬å¹³æ€§å’Œå¯é æ€§ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒFed-MARLåœ¨ä»»åŠ¡æˆåŠŸç‡ã€å»¶è¿Ÿã€èƒ½æºæ•ˆç‡å’Œå…¬å¹³æ€§æ–¹é¢ä¼˜äºé›†ä¸­å¼MARLå’Œå¯å‘å¼åŸºçº¿ï¼ŒåŒæ—¶ç¡®ä¿äº†åœ¨åŠ¨æ€ã€èµ„æºå—é™çš„6Gè¾¹ç¼˜ç½‘ç»œä¸­çš„å¼ºå¤§éšç§ä¿æŠ¤å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³6Gè¾¹ç¼˜ç½‘ç»œä¸­èµ„æºç®¡ç†é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨éšç§ä¿æŠ¤ã€èƒ½æºæ•ˆç‡å’Œå®æ—¶æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚é›†ä¸­å¼æ–¹æ³•å®¹æ˜“æ³„éœ²ç”¨æˆ·æ•°æ®ï¼Œä¼ ç»Ÿçš„å¯å‘å¼ç®—æ³•éš¾ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„ç½‘ç»œç¯å¢ƒï¼Œä¸”æ— æ³•åŒæ—¶ä¼˜åŒ–å¤šä¸ªç›®æ ‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è”é‚¦å­¦ä¹ å®ç°éšç§ä¿æŠ¤çš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¹¶ç»“åˆå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è¿›è¡Œå»ä¸­å¿ƒåŒ–çš„èµ„æºç®¡ç†å†³ç­–ã€‚é€šè¿‡è·¨å±‚ä¼˜åŒ–ï¼ŒåŒæ—¶è€ƒè™‘MACå±‚å’Œåº”ç”¨å±‚çš„å½±å“ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„èµ„æºåˆ©ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«å¤šä¸ªè¾¹ç¼˜è®¾å¤‡ï¼Œæ¯ä¸ªè®¾å¤‡ä½œä¸ºä¸€ä¸ªæ™ºèƒ½ä½“ã€‚æ¯ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨DRQNå­¦ä¹ æœ¬åœ°ç­–ç•¥ï¼Œç”¨äºä»»åŠ¡å¸è½½ã€é¢‘è°±æ¥å…¥å’ŒCPUèƒ½é‡è‡ªé€‚åº”ã€‚æ‰€æœ‰æ™ºèƒ½ä½“å®šæœŸå°†æœ¬åœ°æ¨¡å‹æ›´æ–°ä¸Šä¼ åˆ°ä¸­å¤®æœåŠ¡å™¨ï¼ŒæœåŠ¡å™¨ä½¿ç”¨å®‰å…¨èšåˆåè®®è¿›è¡Œæ¨¡å‹èšåˆï¼Œå¹¶å°†èšåˆåçš„æ¨¡å‹åˆ†å‘ç»™æ‰€æœ‰æ™ºèƒ½ä½“ã€‚æ•´ä¸ªè¿‡ç¨‹åœ¨POMMDPæ¡†æ¶ä¸‹è¿›è¡Œï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–å¤šç›®æ ‡å¥–åŠ±å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†è”é‚¦å­¦ä¹ ä¸å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œå¹¶åº”ç”¨äº6Gè¾¹ç¼˜ç½‘ç»œçš„èµ„æºç®¡ç†ã€‚é€šè¿‡å®‰å…¨èšåˆåè®®ï¼Œå®ç°äº†åœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æ­¤å¤–ï¼Œè·¨å±‚ä¼˜åŒ–ç­–ç•¥èƒ½å¤Ÿæ›´å…¨é¢åœ°è€ƒè™‘ç½‘ç»œèµ„æºï¼Œæå‡èµ„æºåˆ©ç”¨ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡é‡‡ç”¨DRQNä½œä¸ºæ¯ä¸ªæ™ºèƒ½ä½“çš„å†³ç­–æ¨¡å‹ï¼Œè¾“å…¥åŒ…æ‹¬é˜Ÿåˆ—é•¿åº¦ã€èƒ½é‡ã€CPUä½¿ç”¨ç‡å’Œç§»åŠ¨æ€§ç­‰æœ¬åœ°è§‚å¯Ÿã€‚å¥–åŠ±å‡½æ•°ç»¼åˆè€ƒè™‘äº†å»¶è¿Ÿã€èƒ½æºæ•ˆç‡ã€é¢‘è°±æ•ˆç‡ã€å…¬å¹³æ€§å’Œå¯é æ€§ã€‚å®‰å…¨èšåˆåè®®åŸºäºæ¤­åœ†æ›²çº¿Diffie-Hellmanå¯†é’¥äº¤æ¢ï¼Œç¡®ä¿æ¨¡å‹æ›´æ–°çš„å®‰å…¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒFed-MARLåœ¨ä»»åŠ¡æˆåŠŸç‡ã€å»¶è¿Ÿã€èƒ½æºæ•ˆç‡å’Œå…¬å¹³æ€§æ–¹é¢å‡ä¼˜äºé›†ä¸­å¼MARLå’Œå¯å‘å¼åŸºçº¿ã€‚å…·ä½“è€Œè¨€ï¼ŒFed-MARLåœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šæå‡äº†çº¦10%-15%ï¼Œå»¶è¿Ÿé™ä½äº†çº¦20%-25%ï¼Œèƒ½æºæ•ˆç‡æé«˜äº†çº¦15%-20%ï¼Œå¹¶åœ¨å…¬å¹³æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆä¿æŠ¤ç”¨æˆ·éšç§ï¼ŒæŠµå¾¡åŠè¯šå®æ”»å‡»ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœªæ¥çš„6Gè¾¹ç¼˜ç½‘ç»œï¼Œä¸ºURLLCã€eMBBå’ŒmMTCç­‰å¤šç§ä¸šåŠ¡æä¾›é«˜æ•ˆã€èŠ‚èƒ½å’Œå®‰å…¨çš„èµ„æºç®¡ç†ã€‚é€šè¿‡ä¼˜åŒ–ä»»åŠ¡å¸è½½ã€é¢‘è°±æ¥å…¥å’ŒCPUèƒ½é‡åˆ†é…ï¼Œå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œé™ä½ç½‘ç»œè¿è¥æˆæœ¬ï¼Œå¹¶ä¿æŠ¤ç”¨æˆ·éšç§ã€‚è¯¥æ–¹æ³•è¿˜å¯æ‰©å±•åˆ°å…¶ä»–èµ„æºå—é™çš„åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As sixth-generation (6G) networks move toward ultra-dense, intelligent edge environments, efficient resource management under stringent privacy, mobility, and energy constraints becomes critical. This paper introduces a novel Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that incorporates cross-layer orchestration of both the MAC layer and application layer for energy-efficient, privacy-preserving, and real-time resource management across heterogeneous edge devices. Each agent uses a Deep Recurrent Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum access, and CPU energy adaptation based on local observations (e.g., queue length, energy, CPU usage, and mobility). To protect privacy, we introduce a secure aggregation protocol based on elliptic curve Diffie Hellman key exchange, which ensures accurate model updates without exposing raw data to semi-honest adversaries. We formulate the resource management problem as a partially observable multi-agent Markov decision process (POMMDP) with a multi-objective reward function that jointly optimizes latency, energy efficiency, spectral efficiency, fairness, and reliability under 6G-specific service requirements such as URLLC, eMBB, and mMTC. Simulation results demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness, while ensuring robust privacy protection and scalability in dynamic, resource-constrained 6G edge networks.

