---
layout: default
title: Inpainting-Guided Policy Optimization for Diffusion Large Language Models
---

# Inpainting-Guided Policy Optimization for Diffusion Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10396" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10396v1</a>
  <a href="https://arxiv.org/pdf/2509.10396.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10396v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10396v1', 'Inpainting-Guided Policy Optimization for Diffusion Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, Feiyu Chen

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

**å¤‡æ³¨**: preprint; 21 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIGPOï¼šåˆ©ç”¨Inpaintingå¼•å¯¼æ‰©æ•£LLMçš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `å›¾åƒä¿®å¤` `å¤§è¯­è¨€æ¨¡å‹` `æ•°å­¦é—®é¢˜æ±‚è§£` `æ ·æœ¬æ•ˆç‡` `æ¢ç´¢å¼•å¯¼`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ å¯¹é½LLMæ–¹æ³•é¢ä¸´æ¢ç´¢æŒ‘æˆ˜ï¼Œå¥–åŠ±ç¨€ç–ä¸”æ ·æœ¬æ•ˆç‡ä½ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹éš¾ä»¥æ‰¾åˆ°æ­£ç¡®è§£æ—¶ã€‚
2. IGPOåˆ©ç”¨dLLMçš„å›¾åƒä¿®å¤èƒ½åŠ›ï¼Œåœ¨åœ¨çº¿é‡‡æ ·ä¸­ç­–ç•¥æ€§åœ°æ’å…¥éƒ¨åˆ†ground-truthæ¨ç†è½¨è¿¹ï¼Œå¼•å¯¼æ¢ç´¢å¹¶ä¿ç•™è‡ªæˆ‘ç”Ÿæˆæ¨ç†ã€‚
3. é€šè¿‡IGPOåŠå…¶ä»–æŠ€æœ¯ï¼Œåœ¨GSM8Kã€Math500å’ŒAMCç­‰æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¨æ³¨æ„åŠ›æ©ç dLLMå–å¾—äº†æ–°çš„state-of-the-artç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ©ç æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹(dLLMs)çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥æ¨¡å‹ä½œä¸ºè‡ªå›å½’LLMsçš„æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨æä¾›æœ‰ç«äº‰åŠ›çš„æ€§èƒ½çš„åŒæ—¶ï¼Œæ”¯æŒç‹¬ç‰¹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä¾‹å¦‚å›¾åƒä¿®å¤ã€‚æˆ‘ä»¬æ¢ç´¢äº†å›¾åƒä¿®å¤å¦‚ä½•ä¸ºdLLMsçš„RLç®—æ³•è®¾è®¡æä¾›ä¿¡æ¯ã€‚å°†LLMsä¸å¼ºåŒ–å­¦ä¹ å¯¹é½é¢ä¸´ç€æ¢ç´¢æŒ‘æˆ˜ï¼šç¨€ç–çš„å¥–åŠ±ä¿¡å·å’Œæ¨¡å‹æœªèƒ½å‘ç°æ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶çš„æ ·æœ¬æµªè´¹ã€‚è™½ç„¶è¿™ç§ä½æ•ˆç‡å¹¿æ³›å½±å“LLMsï¼Œä½†dLLMsæä¾›äº†ä¸€ä¸ªç‹¬ç‰¹çš„æœºä¼šâ€”â€”å®ƒä»¬çš„å›¾åƒä¿®å¤èƒ½åŠ›å¯ä»¥æŒ‡å¯¼æ¢ç´¢ã€‚æˆ‘ä»¬å¼•å…¥äº†IGPOï¼ˆInpainting Guided Policy Optimizationï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªRLæ¡†æ¶ï¼Œå®ƒåœ¨åœ¨çº¿é‡‡æ ·æœŸé—´ç­–ç•¥æ€§åœ°æ’å…¥éƒ¨åˆ†ground-truthæ¨ç†è½¨è¿¹ã€‚ä¸æä¾›å®Œæ•´è§£å†³æ–¹æ¡ˆä¸åŒï¼Œå›¾åƒä¿®å¤å°†æ¢ç´¢å¼•å¯¼åˆ°æœ‰å¸Œæœ›çš„è½¨è¿¹ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™è‡ªæˆ‘ç”Ÿæˆçš„æ¨ç†ï¼Œä»è€Œå¼¥åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬å°†IGPOåº”ç”¨äºåŸºäºç»„çš„ä¼˜åŒ–æ–¹æ³•ï¼Œä¾‹å¦‚GRPOï¼Œå…¶ä¸­æ¢ç´¢å¤±è´¥ä¼šå¯¼è‡´é›¶ä¼˜åŠ¿å’Œæ¢¯åº¦ã€‚IGPOæ¢å¤äº†æœ‰æ„ä¹‰çš„æ¢¯åº¦ï¼ŒåŒæ—¶æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å¯¹åˆæˆé‡å†™çš„ç®€æ´è½¨è¿¹è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œè¿™äº›è½¨è¿¹æ›´å¥½åœ°ä¸dLLMç”Ÿæˆæ¨¡å¼å¯¹é½ã€‚é€šè¿‡åŒ…æ‹¬åŸºäºç†µçš„è¿‡æ»¤åœ¨å†…çš„å…¶ä»–æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ–¹æ³•åœ¨ä¸‰ä¸ªæ•°å­¦åŸºå‡†ï¼ˆGSM8Kã€Math500 å’Œ AMCï¼‰ä¸Šäº§ç”Ÿäº†æ˜¾ç€æ”¶ç›Šï¼Œä¸ºå…¨æ³¨æ„åŠ›æ©ç dLLMå®ç°äº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMå¯¹é½æ–¹æ³•ï¼Œåœ¨é¢å¯¹å¤æ‚ä»»åŠ¡å’Œç¨€ç–å¥–åŠ±æ—¶ï¼Œå­˜åœ¨æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚æ¨¡å‹éš¾ä»¥æ‰¾åˆ°æ­£ç¡®çš„æ¨ç†è·¯å¾„ï¼Œå¯¼è‡´å¤§é‡çš„æ ·æœ¬æµªè´¹å’Œæ— æ•ˆçš„æ¢¯åº¦æ›´æ–°ã€‚å°¤å…¶æ˜¯åœ¨åŸºäºç»„çš„ä¼˜åŒ–æ–¹æ³•ä¸­ï¼Œæ¢ç´¢å¤±è´¥ä¼šå¯¼è‡´é›¶ä¼˜åŠ¿å’Œæ¢¯åº¦ï¼Œä¸¥é‡å½±å“å­¦ä¹ æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨dLLMçš„å›¾åƒä¿®å¤èƒ½åŠ›æ¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢è¿‡ç¨‹ã€‚é€šè¿‡åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ç­–ç•¥æ€§åœ°æ’å…¥éƒ¨åˆ†ground-truthæ¨ç†è½¨è¿¹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†æ¨¡å‹å¼•å¯¼åˆ°æ›´æœ‰å¸Œæœ›çš„è½¨è¿¹ç©ºé—´ï¼Œä»è€Œæé«˜æ¢ç´¢æ•ˆç‡å’Œæ ·æœ¬åˆ©ç”¨ç‡ã€‚è¿™ç§æ–¹æ³•æ—¢èƒ½åˆ©ç”¨ç›‘ç£å­¦ä¹ çš„å…ˆéªŒçŸ¥è¯†ï¼Œåˆèƒ½ä¿ç•™å¼ºåŒ–å­¦ä¹ çš„è‡ªæˆ‘ç”Ÿæˆæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIGPOæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨dLLMç”Ÿæˆæ¨ç†è½¨è¿¹ï¼›2) ç­–ç•¥æ€§åœ°é€‰æ‹©éƒ¨åˆ†æ¨ç†æ­¥éª¤ï¼Œç”¨ground-truthè¿›è¡Œä¿®å¤ï¼ˆinpaintingï¼‰ï¼›3) ä½¿ç”¨ä¿®å¤åçš„è½¨è¿¹è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œä¾‹å¦‚ä½¿ç”¨GRPOç­‰æ–¹æ³•ï¼›4) ç»“åˆå…¶ä»–æŠ€æœ¯ï¼Œå¦‚åŸºäºç†µçš„è¿‡æ»¤ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚æ•´ä½“æµç¨‹æ˜¯åœ¨çº¿é‡‡æ ·å’Œç­–ç•¥æ›´æ–°çš„å¾ªç¯è¿­ä»£ã€‚

**å…³é”®åˆ›æ–°**ï¼šIGPOçš„å…³é”®åˆ›æ–°åœ¨äºå°†dLLMçš„å›¾åƒä¿®å¤èƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢è¿‡ç¨‹ç›¸ç»“åˆã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒIGPOèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹æ¢ç´¢ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚ä¸å®Œå…¨ä¾èµ–ç›‘ç£å­¦ä¹ çš„æ–¹æ³•ç›¸æ¯”ï¼ŒIGPOèƒ½å¤Ÿä¿ç•™æ¨¡å‹çš„è‡ªæˆ‘ç”Ÿæˆæ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”æ›´å¤æ‚å’ŒæœªçŸ¥çš„ç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šIGPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•é€‰æ‹©éœ€è¦ä¿®å¤çš„æ¨ç†æ­¥éª¤ï¼Œä¾‹å¦‚å¯ä»¥æ ¹æ®æ¨¡å‹çš„ä¸ç¡®å®šæ€§æˆ–å¥–åŠ±ä¿¡å·æ¥é€‰æ‹©ï¼›2) å¦‚ä½•å¹³è¡¡ä¿®å¤çš„ç¨‹åº¦ï¼Œé¿å…è¿‡åº¦ä¾èµ–ground-truthï¼Œä»è€Œé™åˆ¶æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼›3) å¦‚ä½•è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆé€»è¾‘å’Œæ›´æœ‰æ•ˆçš„æ¨ç†è½¨è¿¹ï¼›4) ç»“åˆç›‘ç£å¾®è°ƒï¼Œåˆ©ç”¨åˆæˆé‡å†™çš„ç®€æ´è½¨è¿¹æ¥æ›´å¥½åœ°å¯¹é½dLLMçš„ç”Ÿæˆæ¨¡å¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

IGPOåœ¨GSM8Kã€Math500å’ŒAMCä¸‰ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºå…¨æ³¨æ„åŠ›æ©ç dLLMå®ç°äº†æ–°çš„state-of-the-artç»“æœã€‚å…·ä½“çš„æ•°æ®æå‡å¹…åº¦æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­å¼ºè°ƒäº†â€œsubstantial gainsâ€ï¼Œè¡¨æ˜æ€§èƒ½æå‡æ˜¯æ˜¾è‘—çš„ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼ºè°ƒäº†IGPOèƒ½å¤Ÿæ¢å¤æœ‰æ„ä¹‰çš„æ¢¯åº¦ï¼ŒåŒæ—¶æé«˜æ ·æœ¬æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†å’Œå†³ç­–çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€çŸ¥è¯†å›¾è°±æ¨ç†ç­‰ã€‚é€šè¿‡æé«˜LLMçš„æ¢ç´¢æ•ˆç‡å’Œæ ·æœ¬åˆ©ç”¨ç‡ï¼Œå¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼Œå¹¶æå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•è¿˜å¯èƒ½ä¿ƒè¿›æ›´é€šç”¨å’Œæ™ºèƒ½çš„AIç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new state-of-the-art results for full-attention masked dLLMs.

