---
layout: default
title: DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition
---

# DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09940" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09940v1</a>
  <a href="https://arxiv.org/pdf/2509.09940.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09940v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09940v1', 'DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifei Wang, Wenbin Wang, Yong Luo

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

**å¤‡æ³¨**: 8 pages, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DyKen-Hyenaï¼šé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›åŠ¨æ€ç”Ÿæˆå·ç§¯æ ¸ï¼Œç”¨äºå¤šæ¨¡æ€æ„å›¾è¯†åˆ«**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ„å›¾è¯†åˆ«` `è·¨æ¨¡æ€æ³¨æ„åŠ›` `åŠ¨æ€å·ç§¯æ ¸` `ç‰¹å¾è°ƒåˆ¶` `MIntRec` `åŸŸå¤–æ£€æµ‹` `è§†å¬ä¿¡æ¯èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MIRæ¨¡å‹åœ¨èåˆå¤šæ¨¡æ€ç‰¹å¾æ—¶ï¼Œå®¹æ˜“å¼•å…¥å™ªå£°æˆ–ä¸ç›¸å…³ä¿¡æ¯ï¼ŒæŸå®³å…³é”®çš„è¯­è¨€ç‰¹å¾ã€‚
2. DyKen-Hyenaå°†è§†å¬çº¿ç´¢è½¬åŒ–ä¸ºåŠ¨æ€å·ç§¯æ ¸ï¼Œé€tokenåœ°è°ƒåˆ¶æ–‡æœ¬ç‰¹å¾æå–ï¼Œå®ç°ç»†ç²’åº¦çš„æ¨¡æ€äº¤äº’ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDyKen-Hyenaåœ¨MIntRecå’ŒMIntRec2.0ä¸Šå–å¾—äº†SOTAç»“æœï¼Œå°¤å…¶åœ¨åŸŸå¤–æ£€æµ‹ä¸Šæå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æ„å›¾è¯†åˆ«(MIR)é€šè¿‡åˆ©ç”¨æ¥è‡ªå¤šç§æ¥æºï¼ˆä¾‹å¦‚ï¼Œè¯­è¨€ã€è§†é¢‘å’ŒéŸ³é¢‘ï¼‰çš„ä¸°å¯Œä¿¡æ¯å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œæ¨¡æ€é—´æ„å›¾æ— å…³å’Œå†²çªä¿¡æ¯çš„å¯èƒ½æ€§å¯èƒ½ä¼šé˜»ç¢æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚ç›®å‰å¤§å¤šæ•°æ¨¡å‹å°è¯•é€šè¿‡åº”ç”¨è¯¸å¦‚å¤šå¤´æ³¨æ„åŠ›ä¹‹ç±»çš„æœºåˆ¶æ¥èåˆæ¨¡æ€ï¼Œå°†ç»“æœåŠ å›åˆ°åŸå§‹è¡¨ç¤ºã€‚è¿™ä¸ªè¿‡ç¨‹æœ‰å¯èƒ½ä¼šç”¨å˜ˆæ‚æˆ–ä¸ç›¸å…³çš„éè¯­è¨€ä¿¡å·ç ´åä¸»è¦çš„è¯­è¨€ç‰¹å¾ï¼Œå› ä¸ºå®ƒé€šå¸¸æ— æ³•æ•è·ç»†ç²’åº¦çš„tokençº§åˆ«çš„å½±å“ï¼Œå³éè¯­è¨€çº¿ç´¢åº”è¯¥è°ƒèŠ‚è€Œéä»…ä»…å¢å¼ºæ–‡æœ¬å«ä¹‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DyKen-Hyenaï¼Œå®ƒå°†é—®é¢˜ä»ç‰¹å¾èåˆé‡æ–°å®šä¹‰ä¸ºå¤„ç†è°ƒåˆ¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹å°†è§†å¬çº¿ç´¢è½¬åŒ–ä¸ºåŠ¨æ€çš„ã€é€tokençš„å·ç§¯æ ¸ï¼Œç›´æ¥è°ƒåˆ¶æ–‡æœ¬ç‰¹å¾æå–ã€‚è¿™ç§ç»†ç²’åº¦çš„æ–¹æ³•åœ¨MIntRecå’ŒMIntRec2.0åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨åŸŸå¤–æ£€æµ‹ä¸­äº§ç”Ÿäº†+10.46%çš„F1åˆ†æ•°æå‡ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åˆ›å»ºäº†ä¸€ç§æ ¹æœ¬ä¸Šæ›´é²æ£’çš„æ„å›¾è¡¨ç¤ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€æ„å›¾è¯†åˆ«æ—¨åœ¨åˆ©ç”¨å¤šç§æ¨¡æ€çš„ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬ã€è§†é¢‘ã€éŸ³é¢‘ï¼‰æ¥å‡†ç¡®è¯†åˆ«ç”¨æˆ·æ„å›¾ã€‚ç°æœ‰æ–¹æ³•åœ¨èåˆä¸åŒæ¨¡æ€ç‰¹å¾æ—¶ï¼Œé€šå¸¸é‡‡ç”¨å¤šå¤´æ³¨æ„åŠ›ç­‰æœºåˆ¶ï¼Œç›´æ¥å°†éè¯­è¨€ä¿¡å·åŠ å›åˆ°åŸå§‹æ–‡æœ¬ç‰¹å¾ä¸­ã€‚è¿™ç§æ–¹å¼å®¹æ˜“å¼•å…¥å™ªå£°ï¼Œç ´åå…³é”®çš„è¯­è¨€ç‰¹å¾ï¼Œå°¤å…¶æ˜¯åœ¨éè¯­è¨€ä¿¡å·ä¸æ„å›¾æ— å…³æˆ–å­˜åœ¨å†²çªæ—¶ï¼Œä¼šé™ä½æ„å›¾è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDyKen-Hyenaçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¨¡æ€èåˆé—®é¢˜è½¬åŒ–ä¸ºæ¨¡æ€è°ƒåˆ¶é—®é¢˜ã€‚å®ƒä¸ç›´æ¥å°†éè¯­è¨€ç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾èåˆï¼Œè€Œæ˜¯åˆ©ç”¨è§†å¬çº¿ç´¢åŠ¨æ€ç”Ÿæˆå·ç§¯æ ¸ï¼Œç”¨äºè°ƒåˆ¶æ–‡æœ¬ç‰¹å¾çš„æå–è¿‡ç¨‹ã€‚è¿™æ ·å¯ä»¥å®ç°ç»†ç²’åº¦çš„æ¨¡æ€äº¤äº’ï¼Œè®©éè¯­è¨€ä¿¡æ¯åœ¨tokençº§åˆ«å½±å“æ–‡æœ¬ç‰¹å¾çš„è¡¨è¾¾ï¼Œä»è€Œæ›´å‡†ç¡®åœ°æ•æ‰ç”¨æˆ·æ„å›¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDyKen-Hyenaçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ–‡æœ¬ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰æå–æ–‡æœ¬ç‰¹å¾ã€‚2) è§†å¬ç‰¹å¾æå–æ¨¡å—ï¼šæå–è§†é¢‘å’ŒéŸ³é¢‘ç‰¹å¾ã€‚3) åŠ¨æ€å·ç§¯æ ¸ç”Ÿæˆæ¨¡å—ï¼šåˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è§†å¬ç‰¹å¾è½¬åŒ–ä¸ºåŠ¨æ€çš„ã€é€tokençš„å·ç§¯æ ¸ã€‚4) æ–‡æœ¬ç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼šä½¿ç”¨ç”Ÿæˆçš„åŠ¨æ€å·ç§¯æ ¸ï¼Œå¯¹æ–‡æœ¬ç‰¹å¾è¿›è¡Œå·ç§¯æ“ä½œï¼Œå®ç°ç‰¹å¾è°ƒåˆ¶ã€‚5) æ„å›¾åˆ†ç±»æ¨¡å—ï¼šå°†è°ƒåˆ¶åçš„æ–‡æœ¬ç‰¹å¾è¾“å…¥åˆ†ç±»å™¨ï¼Œé¢„æµ‹ç”¨æˆ·æ„å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šDyKen-Hyenaæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºåŠ¨æ€å·ç§¯æ ¸çš„ç”Ÿæˆå’Œåº”ç”¨ã€‚ä¸ä¼ ç»Ÿçš„ç‰¹å¾èåˆæ–¹æ³•ä¸åŒï¼Œå®ƒä¸æ˜¯ç®€å•åœ°å°†ä¸åŒæ¨¡æ€çš„ç‰¹å¾æ‹¼æ¥æˆ–ç›¸åŠ ï¼Œè€Œæ˜¯åˆ©ç”¨éè¯­è¨€ä¿¡æ¯åŠ¨æ€åœ°è°ƒæ•´æ–‡æœ¬ç‰¹å¾æå–çš„è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼Œé¿å…å™ªå£°çš„å¹²æ‰°ï¼Œæé«˜æ„å›¾è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šDyKen-Hyenaçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼šç”¨äºè®¡ç®—è§†å¬ç‰¹å¾å¯¹æ¯ä¸ªæ–‡æœ¬tokençš„é‡è¦æ€§ï¼Œä»è€Œç”ŸæˆåŠ¨æ€å·ç§¯æ ¸ã€‚2) åŠ¨æ€å·ç§¯æ ¸çš„å°ºå¯¸å’Œæ•°é‡ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚è¿›è¡Œè°ƒæ•´ï¼Œä»¥å¹³è¡¡æ¨¡å‹çš„å¤æ‚åº¦å’Œæ€§èƒ½ã€‚3) æŸå¤±å‡½æ•°ï¼šé‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–æ„å›¾åˆ†ç±»å™¨çš„å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DyKen-Hyenaåœ¨MIntRecå’ŒMIntRec2.0åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚å°¤å…¶åœ¨åŸŸå¤–æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒF1åˆ†æ•°æå‡äº†10.46%ï¼Œè¡¨æ˜è¯¥æ¨¡å‹å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„å¤šå¤´æ³¨æ„åŠ›èåˆæ–¹æ³•ï¼ŒDyKen-Hyenaèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼Œé¿å…å™ªå£°å¹²æ‰°ï¼Œä»è€Œæé«˜æ„å›¾è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DyKen-Hyenaå¯åº”ç”¨äºæ™ºèƒ½åŠ©æ‰‹ã€æ™ºèƒ½å®¢æœã€è§†é¢‘å†…å®¹ç†è§£ç­‰é¢†åŸŸã€‚é€šè¿‡èåˆæ–‡æœ¬ã€è§†é¢‘å’ŒéŸ³é¢‘ä¿¡æ¯ï¼Œæ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·æ„å›¾ï¼Œä»è€Œæä¾›æ›´ä¸ªæ€§åŒ–ã€æ›´æ™ºèƒ½çš„æœåŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¢æœä¸­ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·çš„è¯­éŸ³å’Œè§†é¢‘è¡¨æƒ…ï¼Œæ›´å‡†ç¡®åœ°åˆ¤æ–­ç”¨æˆ·çš„æƒ…ç»ªå’Œéœ€æ±‚ï¼Œä»è€Œæä¾›æ›´æœ‰æ•ˆçš„å¸®åŠ©ã€‚åœ¨è§†é¢‘å†…å®¹ç†è§£ä¸­ï¼Œå¯ä»¥ç»“åˆè§†é¢‘ç”»é¢å’Œè¯­éŸ³å†…å®¹ï¼Œæ›´å…¨é¢åœ°ç†è§£è§†é¢‘çš„ä¸»é¢˜å’Œæƒ…æ„Ÿå€¾å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich information from multiple sources (e.g., language, video, and audio), the potential for intent-irrelevant and conflicting information across modalities may hinder performance from being further improved. Most current models attempt to fuse modalities by applying mechanisms like multi-head attention to unimodal feature sequences and then adding the result back to the original representation. This process risks corrupting the primary linguistic features with noisy or irrelevant non-verbal signals, as it often fails to capture the fine-grained, token-level influence where non-verbal cues should modulate, not just augment, textual meaning. To address this, we introduce DyKen-Hyena, which reframes the problem from feature fusion to processing modulation. Our model translates audio-visual cues into dynamic, per-token convolutional kernels that directly modulate textual feature extraction. This fine-grained approach achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks. Notably, it yields a +10.46% F1-score improvement in out-of-scope detection, validating that our method creates a fundamentally more robust intent representation.

