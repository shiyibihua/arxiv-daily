---
layout: default
title: Neural Scaling Laws for Deep Regression
---

# Neural Scaling Laws for Deep Regression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10000" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10000v2</a>
  <a href="https://arxiv.org/pdf/2509.10000.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10000v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10000v2', 'Neural Scaling Laws for Deep Regression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tilen Cadez, Kyoung-Min Kim

**åˆ†ç±»**: cs.LG, cond-mat.other

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-11-24)

**å¤‡æ³¨**: Supplementary Information will be provided with the published manuscript

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶æ·±åº¦å›å½’æ¨¡å‹ç¥ç»æ ‡åº¦å¾‹ï¼Œæ­ç¤ºæ•°æ®é‡ä¸æ¨¡å‹å®¹é‡å¯¹æ€§èƒ½çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¥ç»æ ‡åº¦å¾‹` `æ·±åº¦å›å½’` `å‚æ•°ä¼°è®¡` `æ¨¡å‹å®¹é‡` `æ•°æ®é›†å¤§å°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å­¦ä¹ æ ‡åº¦å¾‹çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ†ç±»ä»»åŠ¡ï¼Œæ·±åº¦å›å½’æ¨¡å‹çš„æ ‡åº¦å¾‹ç ”ç©¶ç›¸å¯¹ä¸è¶³ã€‚
2. æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶ï¼Œæ¢ç´¢äº†æ·±åº¦å›å½’æ¨¡å‹ä¸­æŸå¤±å‡½æ•°ä¸è®­ç»ƒæ•°æ®é‡ã€æ¨¡å‹å®¹é‡ä¹‹é—´çš„å¹‚å¾‹å…³ç³»ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ·±åº¦å›å½’æ¨¡å‹ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„æ ‡åº¦å¾‹ï¼Œä¸”æ€§èƒ½éšæ•°æ®é‡å¢åŠ æœ‰è¾ƒå¤§æå‡ç©ºé—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¥ç»æ ‡åº¦å¾‹æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é‡è¦å·¥å…·ï¼Œå®ƒæè¿°äº†æ³›åŒ–è¯¯å·®ä¸æ¨¡å‹ç‰¹æ€§ä¹‹é—´çš„å¹‚å¾‹å…³ç³»ï¼Œæœ‰åŠ©äºåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹å¼€å‘å¯é çš„æ¨¡å‹ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹çš„æˆåŠŸçªæ˜¾äº†è¿™äº›å®šå¾‹çš„é‡è¦æ€§ï¼Œä½†å®ƒä»¬åœ¨æ·±åº¦å›å½’æ¨¡å‹ä¸­çš„åº”ç”¨ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é’ˆå¯¹æ‰­æ›²èŒƒå¾·ç“¦å°”æ–¯ç£ä½“çš„å‚æ•°ä¼°è®¡æ¨¡å‹ï¼Œå¯¹æ·±åº¦å›å½’ä¸­çš„ç¥ç»æ ‡åº¦å¾‹è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚ç»“æœè¡¨æ˜ï¼ŒæŸå¤±ä¸è®­ç»ƒæ•°æ®é›†å¤§å°å’Œæ¨¡å‹å®¹é‡ä¹‹é—´å­˜åœ¨å¹¿æ³›çš„å¹‚å¾‹å…³ç³»ï¼Œæ¶µç›–äº†å„ç§æ¶æ„ï¼ŒåŒ…æ‹¬å…¨è¿æ¥ç½‘ç»œã€æ®‹å·®ç½‘ç»œå’Œè§†è§‰Transformerã€‚æ­¤å¤–ï¼Œè¿™äº›å…³ç³»çš„æ ‡åº¦æŒ‡æ•°èŒƒå›´ä¸º1åˆ°2ï¼Œå…·ä½“æ•°å€¼å–å†³äºå›å½’å‚æ•°å’Œæ¨¡å‹ç»†èŠ‚ã€‚ä¸€è‡´çš„æ ‡åº¦è¡Œä¸ºå’Œè¾ƒå¤§çš„æ ‡åº¦æŒ‡æ•°è¡¨æ˜ï¼Œéšç€æ•°æ®é‡çš„å¢åŠ ï¼Œæ·±åº¦å›å½’æ¨¡å‹çš„æ€§èƒ½å¯ä»¥å¾—åˆ°æ˜¾è‘—æé«˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç ”ç©¶æ·±åº¦å›å½’æ¨¡å‹ä¸­çš„ç¥ç»æ ‡åº¦å¾‹ï¼Œå³æ¨¡å‹æ€§èƒ½ï¼ˆé€šå¸¸ç”¨æŸå¤±å‡½æ•°è¡¡é‡ï¼‰å¦‚ä½•éšç€è®­ç»ƒæ•°æ®é›†å¤§å°å’Œæ¨¡å‹å®¹é‡çš„å˜åŒ–è€Œå˜åŒ–ã€‚ç°æœ‰æ–¹æ³•å¯¹æ·±åº¦å›å½’æ¨¡å‹çš„æ ‡åº¦å¾‹ç ”ç©¶ä¸è¶³ï¼Œç¼ºä¹å¯¹æ•°æ®é‡å’Œæ¨¡å‹å®¹é‡å¦‚ä½•å½±å“æ·±åº¦å›å½’æ¨¡å‹æ€§èƒ½çš„ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤§é‡çš„å®éªŒï¼Œè§‚å¯Ÿæ·±åº¦å›å½’æ¨¡å‹çš„æŸå¤±å‡½æ•°ä¸è®­ç»ƒæ•°æ®é›†å¤§å°å’Œæ¨¡å‹å®¹é‡ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡æ‹Ÿåˆå®éªŒæ•°æ®ï¼Œç¡®å®šæ˜¯å¦å­˜åœ¨å¹‚å¾‹å…³ç³»ï¼Œå¹¶è®¡ç®—ç›¸åº”çš„æ ‡åº¦æŒ‡æ•°ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æ­ç¤ºæ·±åº¦å›å½’æ¨¡å‹æ€§èƒ½æå‡çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæ¨¡å‹è®¾è®¡å’Œèµ„æºåˆ†é…æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨å‚æ•°ä¼°è®¡æ¨¡å‹å¯¹æ‰­æ›²èŒƒå¾·ç“¦å°”æ–¯ç£ä½“è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå›å½’ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†ï¼›2) é€‰æ‹©ä¸åŒçš„ç½‘ç»œæ¶æ„ï¼ˆå…¨è¿æ¥ç½‘ç»œã€æ®‹å·®ç½‘ç»œã€è§†è§‰Transformerï¼‰ï¼›3) åœ¨ä¸åŒå¤§å°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼›4) è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æŸå¤±ï¼›5) åˆ†ææŸå¤±ä¸æ•°æ®é›†å¤§å°å’Œæ¨¡å‹å®¹é‡ä¹‹é—´çš„å…³ç³»ï¼Œæ‹Ÿåˆå¹‚å¾‹æ›²çº¿ï¼Œè®¡ç®—æ ‡åº¦æŒ‡æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå°†ç¥ç»æ ‡åº¦å¾‹çš„ç ”ç©¶æ‰©å±•åˆ°æ·±åº¦å›å½’æ¨¡å‹ï¼Œå¹¶å®è¯åœ°è¯æ˜äº†æ·±åº¦å›å½’æ¨¡å‹ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„æ ‡åº¦å¾‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œæ·±åº¦å›å½’æ¨¡å‹çš„æ ‡åº¦æŒ‡æ•°è¾ƒå¤§ï¼Œè¿™æ„å‘³ç€å¢åŠ æ•°æ®é‡å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†å¤šç§ç½‘ç»œæ¶æ„ï¼ŒåŒ…æ‹¬å…¨è¿æ¥ç½‘ç»œã€æ®‹å·®ç½‘ç»œå’Œè§†è§‰Transformerï¼Œä»¥éªŒè¯æ ‡åº¦å¾‹çš„æ™®é€‚æ€§ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚æ•°æ®é›†å¤§å°å’Œæ¨¡å‹å®¹é‡ï¼ˆå‚æ•°æ•°é‡ï¼‰ä½œä¸ºè‡ªå˜é‡ï¼ŒæŸå¤±ä½œä¸ºå› å˜é‡ï¼Œé€šè¿‡æœ€å°äºŒä¹˜æ³•æ‹Ÿåˆå¹‚å¾‹æ›²çº¿ã€‚æ ‡åº¦æŒ‡æ•°æ˜¯å¹‚å¾‹æ›²çº¿çš„å…³é”®å‚æ•°ï¼Œåæ˜ äº†æ•°æ®é‡æˆ–æ¨¡å‹å®¹é‡å¯¹æ€§èƒ½çš„å½±å“ç¨‹åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ·±åº¦å›å½’æ¨¡å‹çš„æŸå¤±ä¸è®­ç»ƒæ•°æ®é›†å¤§å°å’Œæ¨¡å‹å®¹é‡ä¹‹é—´å­˜åœ¨å¹‚å¾‹å…³ç³»ï¼Œæ ‡åº¦æŒ‡æ•°èŒƒå›´ä¸º1åˆ°2ã€‚è¿™æ„å‘³ç€å¢åŠ æ•°æ®é‡å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šå‚æ•°ä¼°è®¡ä»»åŠ¡ä¸­ï¼Œéšç€è®­ç»ƒæ•°æ®é‡çš„å¢åŠ ï¼Œæ¨¡å‹çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä»¥æ¥è¿‘å¹³æ–¹åæ¯”çš„é€Ÿåº¦ä¸‹é™ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ç²¾ç¡®å‚æ•°ä¼°è®¡çš„é¢†åŸŸï¼Œä¾‹å¦‚ææ–™ç§‘å­¦ã€é‡‘èå»ºæ¨¡å’Œæ°”å€™é¢„æµ‹ã€‚é€šè¿‡äº†è§£æ·±åº¦å›å½’æ¨¡å‹çš„æ ‡åº¦å¾‹ï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„è®¡ç®—èµ„æºå’Œæ•°æ®èµ„æºï¼Œè®¾è®¡å‡ºæ€§èƒ½æ›´ä¼˜è¶Šçš„å›å½’æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºæœªæ¥æ·±åº¦å›å½’æ¨¡å‹çš„è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Neural scaling laws--power-law relationships between generalization errors and characteristics of deep learning models--are vital tools for developing reliable models while managing limited resources. Although the success of large language models highlights the importance of these laws, their application to deep regression models remains largely unexplored. Here, we empirically investigate neural scaling laws in deep regression using a parameter estimation model for twisted van der Waals magnets. We observe power-law relationships between the loss and both training dataset size and model capacity across a wide range of values, employing various architectures--including fully connected networks, residual networks, and vision transformers. Furthermore, the scaling exponents governing these relationships range from 1 to 2, with specific values depending on the regressed parameters and model details. The consistent scaling behaviors and their large scaling exponents suggest that the performance of deep regression models can improve substantially with increasing data size.

