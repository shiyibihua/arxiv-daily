---
layout: default
title: Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective
---

# Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10371" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10371v2</a>
  <a href="https://arxiv.org/pdf/2509.10371.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10371v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10371v2', 'Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seokjin Go, Joongun Park, Spandan More, Hanjiang Wu, Irene Wang, Aaron Jezghani, Tushar Krishna, Divya Mahajan

**åˆ†ç±»**: cs.DC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-09-19)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/sitar-lab/CharLLM-PPT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ·±å…¥å‰–æLLMåˆ†å¸ƒå¼è®­ç»ƒæ•ˆç‡ï¼šä»åŠŸè€—ã€æ€§èƒ½å’Œçƒ­ç®¡ç†çš„è§’åº¦è¿›è¡Œå…¨é¢è¯„ä¼°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆ†å¸ƒå¼è®­ç»ƒ` `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `æ€§èƒ½åˆ†æ` `åŠŸè€—åˆ†æ` `çƒ­ç®¡ç†` `å¹¶è¡Œç­–ç•¥` `ç¡¬ä»¶å¹³å°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè®­ç»ƒå¯¹å•èŠ‚ç‚¹åˆ†ææå‡ºæŒ‘æˆ˜ï¼Œéœ€è¦æ·±å…¥ç†è§£æ¨¡å‹åœ¨å¤šGPUç³»ç»Ÿä¸­çš„è¡Œä¸ºã€‚
2. é€šè¿‡åˆ†æä¸åŒå¹¶è¡Œç­–ç•¥å¯¹ç¡¬ä»¶åˆ©ç”¨ç‡ã€åŠŸè€—å’Œçƒ­è¡Œä¸ºçš„å½±å“ï¼Œæ­ç¤ºè®­ç»ƒæ€§èƒ½ä¸ç¡¬ä»¶ã€ç³»ç»Ÿå’Œæ¨¡å‹æ‰§è¡Œçš„å¤æ‚å…³ç³»ã€‚
3. ç ”ç©¶ç»“æœä¸ºç³»ç»Ÿå’Œç¡¬ä»¶è®¾è®¡æä¾›äº†å»ºè®®ï¼Œæ—¨åœ¨æå‡æœªæ¥LLMç³»ç»Ÿå’Œå·¥ä½œè´Ÿè½½çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒï¼Œåœ¨åŒ…æ‹¬NVIDIA H100/H200å’ŒAMD MI250 GPUåœ¨å†…çš„å¤šç§çœŸå®ç¡¬ä»¶å¹³å°ä¸Šï¼Œè¿›è¡Œäº†å…¨é¢çš„æ€§èƒ½åˆ†æã€‚ç ”ç©¶æ¶µç›–äº†ç¨ å¯†å’Œç¨€ç–æ¨¡å‹ï¼Œä»¥åŠå¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œã€æ•°æ®å¹¶è¡Œå’Œä¸“å®¶å¹¶è¡Œç­‰å¤šç§å¹¶è¡Œç­–ç•¥ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬å¯¹ç¡¬ä»¶åˆ©ç”¨ç‡ã€åŠŸè€—å’Œçƒ­è¡Œä¸ºçš„å½±å“ã€‚æ­¤å¤–ï¼Œè¿˜è¯„ä¼°äº†æ¿€æ´»é‡è®¡ç®—å’Œè®¡ç®—-é€šä¿¡é‡å ç­‰ä¼˜åŒ–æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ€§èƒ½å¹¶éå®Œå…¨ç”±ç¡¬ä»¶å®¹é‡å†³å®šã€‚åœ¨é€šä¿¡å—é™çš„æƒ…å†µä¸‹ï¼Œé‡‡ç”¨å°‘é‡é«˜å†…å­˜GPUçš„çºµå‘æ‰©å±•ç³»ç»Ÿå¯èƒ½ä¼˜äºæ¨ªå‘æ‰©å±•ç³»ç»Ÿï¼Œä½†éœ€è¦ä»”ç»†è°ƒæ•´é…ç½®ï¼›åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæ¨ªå‘æ‰©å±•éƒ¨ç½²å¯å®ç°æ›´é«˜çš„ååé‡ã€‚æŸäº›å¹¶è¡Œç»„åˆï¼ˆå¦‚å¼ é‡å¹¶è¡Œä¸æµæ°´çº¿å¹¶è¡Œï¼‰ä¼šå¯¼è‡´å¸¦å®½åˆ©ç”¨ç‡ä¸è¶³ï¼Œè€Œå¢åŠ å¾®æ‰¹å¤§å°è¶…è¿‡ä¸€å®šç¨‹åº¦ä¼šå¯¼è‡´çªå‘æ‰§è¡Œå’Œå³°å€¼åŠŸç‡åç§»ï¼Œä»è€ŒåŠ å‰§çƒ­èŠ‚æµã€‚è¿™äº›å‘ç°æ­ç¤ºäº†è®­ç»ƒæ€§èƒ½å¦‚ä½•å—åˆ°ç¡¬ä»¶ã€ç³»ç»Ÿæ‹“æ‰‘å’Œæ¨¡å‹æ‰§è¡Œä¹‹é—´å¤æ‚äº¤äº’çš„å½±å“ã€‚æœ€åï¼Œä¸ºæ”¹è¿›æœªæ¥LLMç³»ç»Ÿå’Œå·¥ä½œè´Ÿè½½çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ï¼Œæå‡ºäº†ç³»ç»Ÿå’Œç¡¬ä»¶è®¾è®¡çš„å»ºè®®ã€‚é¡¹ç›®æºä»£ç å¯åœ¨https://github.com/sitar-lab/CharLLM-PPTè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ†å¸ƒå¼è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç¡¬ä»¶èµ„æºåˆ©ç”¨ç‡ã€åŠŸè€—ã€çƒ­ç®¡ç†ä»¥åŠä¸åŒå¹¶è¡Œç­–ç•¥é€‰æ‹©ç­‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†ç†è§£ç¡¬ä»¶ã€ç³»ç»Ÿæ‹“æ‰‘å’Œæ¨¡å‹æ‰§è¡Œä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥ä¼˜åŒ–ç³»ç»Ÿè®¾è®¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹LLMè®­ç»ƒè¿‡ç¨‹è¿›è¡Œå…¨é¢çš„æ€§èƒ½å‰–æï¼Œæ·±å…¥ç†è§£ä¸åŒå¹¶è¡Œç­–ç•¥ã€ç¡¬ä»¶å¹³å°å’Œä¼˜åŒ–æŠ€æœ¯å¯¹ç¡¬ä»¶åˆ©ç”¨ç‡ã€åŠŸè€—å’Œçƒ­è¡Œä¸ºçš„å½±å“ã€‚é€šè¿‡å®éªŒæ•°æ®å’Œåˆ†æï¼Œæ­ç¤ºå½±å“è®­ç»ƒæ•ˆç‡çš„å…³é”®å› ç´ ï¼Œå¹¶ä¸ºç³»ç»Ÿå’Œç¡¬ä»¶è®¾è®¡æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„LLMæ¨¡å‹å’Œæ•°æ®é›†ï¼›2) åœ¨ä¸åŒçš„ç¡¬ä»¶å¹³å°ï¼ˆå¦‚NVIDIA H100/H200å’ŒAMD MI250 GPUï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼›3) é‡‡ç”¨ä¸åŒçš„å¹¶è¡Œç­–ç•¥ï¼ˆå¦‚å¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œã€æ•°æ®å¹¶è¡Œå’Œä¸“å®¶å¹¶è¡Œï¼‰ï¼›4) è¯„ä¼°ç¡¬ä»¶åˆ©ç”¨ç‡ã€åŠŸè€—å’Œçƒ­è¡Œä¸ºï¼›5) åˆ†æå®éªŒæ•°æ®ï¼Œæ‰¾å‡ºå½±å“è®­ç»ƒæ•ˆç‡çš„å…³é”®å› ç´ ï¼›6) æå‡ºç³»ç»Ÿå’Œç¡¬ä»¶è®¾è®¡çš„å»ºè®®ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹LLMåˆ†å¸ƒå¼è®­ç»ƒè¿‡ç¨‹è¿›è¡Œäº†å…¨é¢çš„æ€§èƒ½å‰–æï¼Œå¹¶æ­ç¤ºäº†ç¡¬ä»¶ã€ç³»ç»Ÿæ‹“æ‰‘å’Œæ¨¡å‹æ‰§è¡Œä¹‹é—´å¤æ‚çš„äº¤äº’å…³ç³»ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶ä¸ä»…å…³æ³¨æ€§èƒ½æŒ‡æ ‡ï¼Œè¿˜æ·±å…¥åˆ†æäº†åŠŸè€—å’Œçƒ­è¡Œä¸ºï¼Œä»è€Œæ›´å…¨é¢åœ°ç†è§£äº†è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é’ˆå¯¹ä¸åŒçš„å¹¶è¡Œç­–ç•¥å’Œç¡¬ä»¶å¹³å°è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†æ›´å…·ä½“çš„æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„LLMæ¨¡å‹ï¼Œå¦‚ç¨ å¯†å’Œç¨€ç–æ¨¡å‹ï¼›2) é‡‡ç”¨å¤šç§å¹¶è¡Œç­–ç•¥ï¼Œå¦‚å¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œã€æ•°æ®å¹¶è¡Œå’Œä¸“å®¶å¹¶è¡Œï¼›3) è¯„ä¼°ç¡¬ä»¶åˆ©ç”¨ç‡ã€åŠŸè€—å’Œçƒ­è¡Œä¸ºï¼ŒåŒ…æ‹¬GPUåˆ©ç”¨ç‡ã€åŠŸè€—ã€æ¸©åº¦ç­‰æŒ‡æ ‡ï¼›4) åˆ†æå®éªŒæ•°æ®ï¼Œæ‰¾å‡ºå½±å“è®­ç»ƒæ•ˆç‡çš„å…³é”®å› ç´ ï¼Œå¦‚é€šä¿¡ç“¶é¢ˆã€å†…å­˜é™åˆ¶ç­‰ï¼›5) æå‡ºç³»ç»Ÿå’Œç¡¬ä»¶è®¾è®¡çš„å»ºè®®ï¼Œå¦‚ä¼˜åŒ–æ•°æ®chunkingã€è°ƒæ•´å¾®æ‰¹å¤§å°ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶è¡¨æ˜ï¼Œåœ¨é€šä¿¡å—é™çš„æƒ…å†µä¸‹ï¼Œé‡‡ç”¨å°‘é‡é«˜å†…å­˜GPUçš„çºµå‘æ‰©å±•ç³»ç»Ÿå¯èƒ½ä¼˜äºæ¨ªå‘æ‰©å±•ç³»ç»Ÿï¼Œä½†éœ€è¦ä»”ç»†è°ƒæ•´é…ç½®ã€‚æŸäº›å¹¶è¡Œç»„åˆï¼ˆå¦‚å¼ é‡å¹¶è¡Œä¸æµæ°´çº¿å¹¶è¡Œï¼‰ä¼šå¯¼è‡´å¸¦å®½åˆ©ç”¨ç‡ä¸è¶³ã€‚å¢åŠ å¾®æ‰¹å¤§å°è¶…è¿‡ä¸€å®šç¨‹åº¦ä¼šå¯¼è‡´çªå‘æ‰§è¡Œå’Œå³°å€¼åŠŸç‡åç§»ï¼Œä»è€ŒåŠ å‰§çƒ­èŠ‚æµã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ç¡¬ä»¶ã€ç³»ç»Ÿæ‹“æ‰‘å’Œæ¨¡å‹æ‰§è¡Œä¹‹é—´å¤æ‚äº¤äº’å¯¹è®­ç»ƒæ€§èƒ½çš„å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¼˜åŒ–ã€é«˜æ€§èƒ½è®¡ç®—ç³»ç»Ÿè®¾è®¡ã€ä»¥åŠAIèŠ¯ç‰‡çš„ç ”å‘ã€‚é€šè¿‡ç†è§£ä¸åŒå¹¶è¡Œç­–ç•¥å’Œç¡¬ä»¶å¹³å°å¯¹è®­ç»ƒæ•ˆç‡çš„å½±å“ï¼Œå¯ä»¥æŒ‡å¯¼ç”¨æˆ·é€‰æ‹©åˆé€‚çš„ç¡¬ä»¶é…ç½®å’Œå¹¶è¡Œç­–ç•¥ï¼Œä»è€Œé™ä½è®­ç»ƒæˆæœ¬ã€ç¼©çŸ­è®­ç»ƒæ—¶é—´ï¼Œå¹¶æå‡AIæ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¸ºæœªæ¥çš„AIèŠ¯ç‰‡è®¾è®¡æä¾›å‚è€ƒï¼Œä½¿å…¶æ›´é€‚åˆLLMçš„è®­ç»ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid scaling of Large Language Models (LLMs) has pushed training workloads far beyond the limits of single-node analysis, demanding a deeper understanding of how these models behave across large-scale, multi-GPU systems. In this paper, we present a comprehensive characterization of LLM training across diverse real-world workloads and hardware platforms, including NVIDIA H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate their effects on hardware utilization, power consumption, and thermal behavior. We further evaluate the effectiveness of optimizations such as activation recomputation and compute-communication overlap. Our findings show that performance is not determined solely by scaling hardware capacity. Scale-up systems with fewer, higher-memory GPUs can outperform scale-out systems in communication-bound regimes, but only under carefully tuned configurations; in other cases, scale-out deployments achieve superior throughput. We also show that certain parallelism combinations, such as tensor with pipeline, lead to bandwidth underutilization due to inefficient data chunking, while increasing microbatch sizes beyond a certain point induces bursty execution and peak power excursions that worsen thermal throttling. These insights reveal how training performance is shaped by complex interactions between hardware, system topology, and model execution. We conclude by offering recommendations for system and hardware design to improve the scalability and reliability of future LLM systems and workloads. The source code of this project is available at https://github.com/sitar-lab/CharLLM-PPT.

