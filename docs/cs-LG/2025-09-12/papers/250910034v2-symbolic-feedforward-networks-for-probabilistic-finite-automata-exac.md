---
layout: default
title: Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability
---

# Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10034" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10034v2</a>
  <a href="https://arxiv.org/pdf/2509.10034.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10034v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10034v2', 'Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sahil Rajesh Dhayalkar

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-09-23)

**å¤‡æ³¨**: 19 pages, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç¬¦å·å‰é¦ˆç½‘ç»œçš„æ¦‚ç‡æœ‰é™è‡ªåŠ¨æœºç²¾ç¡®æ¨¡æ‹Ÿä¸å­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `æ¦‚ç‡æœ‰é™è‡ªåŠ¨æœº` `ç¬¦å·å‰é¦ˆç½‘ç»œ` `ç²¾ç¡®æ¨¡æ‹Ÿ` `å¯å­¦ä¹ æ€§` `åºåˆ—å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨ç¥ç»ç½‘ç»œä¸­ç²¾ç¡®æ¨¡æ‹Ÿæ¦‚ç‡æœ‰é™è‡ªåŠ¨æœºï¼Œç¼ºä¹å¯è§£é‡Šæ€§å’Œç†è®ºä¿è¯ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§ç¬¦å·å‰é¦ˆç½‘ç»œæ¶æ„ï¼Œå°† PFA çš„çŠ¶æ€å’Œè½¬ç§»åˆ†åˆ«è¡¨ç¤ºä¸ºå‘é‡å’ŒçŸ©é˜µï¼Œå®ç°ç²¾ç¡®æ¨¡æ‹Ÿã€‚
3. å®éªŒè¯æ˜è¯¥æ¶æ„ä¸ä»…èƒ½ç²¾ç¡®æ¨¡æ‹Ÿ PFAï¼Œè¿˜èƒ½é€šè¿‡æ¢¯åº¦ä¸‹é™å­¦ä¹ åˆ° PFA çš„è¡Œä¸ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å½¢å¼åŒ–ä¸”å…·æœ‰å»ºè®¾æ€§çš„ç†è®ºï¼Œè¯æ˜äº†æ¦‚ç‡æœ‰é™è‡ªåŠ¨æœºï¼ˆPFAsï¼‰å¯ä»¥ä½¿ç”¨ç¬¦å·å‰é¦ˆç¥ç»ç½‘ç»œè¿›è¡Œç²¾ç¡®æ¨¡æ‹Ÿã€‚æˆ‘ä»¬çš„æ¶æ„å°†çŠ¶æ€åˆ†å¸ƒè¡¨ç¤ºä¸ºå‘é‡ï¼Œå°†è½¬æ¢è¡¨ç¤ºä¸ºéšæœºçŸ©é˜µï¼Œä»è€Œå¯ä»¥é€šè¿‡çŸ©é˜µ-å‘é‡ä¹˜ç§¯å®ç°æ¦‚ç‡çŠ¶æ€ä¼ æ’­ã€‚è¿™äº§ç”Ÿäº†ä¸€ç§å¹¶è¡Œã€å¯è§£é‡Šä¸”å¯å¾®åˆ†çš„ PFA åŠ¨æ€æ¨¡æ‹Ÿï¼Œä½¿ç”¨äº†è½¯æ›´æ–°ä¸”æ— éœ€å¾ªç¯ã€‚æˆ‘ä»¬æ­£å¼æè¿°äº†æ¦‚ç‡å­é›†æ„é€ ã€Îµ-é—­åŒ…å’Œé€šè¿‡åˆ†å±‚ç¬¦å·è®¡ç®—è¿›è¡Œçš„ç²¾ç¡®æ¨¡æ‹Ÿï¼Œå¹¶è¯æ˜äº† PFA å’Œç‰¹å®šç±»åˆ«çš„ç¥ç»ç½‘ç»œä¹‹é—´çš„ç­‰ä»·æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¿™äº›ç¬¦å·æ¨¡æ‹Ÿå™¨ä¸ä»…å…·æœ‰è¡¨è¾¾èƒ½åŠ›ï¼Œè€Œä¸”æ˜¯å¯å­¦ä¹ çš„ï¼šé€šè¿‡åœ¨æ ‡è®°çš„åºåˆ—æ•°æ®ä¸Šä½¿ç”¨åŸºäºæ¢¯åº¦ä¸‹é™çš„æ ‡å‡†ä¼˜åŒ–æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œå®ƒä»¬å¯ä»¥æ¢å¤çœŸå® PFA çš„ç²¾ç¡®è¡Œä¸ºã€‚è¿™ç§å¯å­¦ä¹ æ€§ï¼Œåœ¨å‘½é¢˜ 5.1 ä¸­è¿›è¡Œäº†å½¢å¼åŒ–ï¼Œæ˜¯è¿™é¡¹å·¥ä½œçš„å…³é”®ã€‚æˆ‘ä»¬çš„ç»“æœåœ¨ä¸€ä¸ªä¸¥æ ¼çš„ä»£æ•°æ¡†æ¶ä¸‹ç»Ÿä¸€äº†æ¦‚ç‡è‡ªåŠ¨æœºç†è®ºå’Œç¥ç»æ¶æ„ï¼Œå¼¥åˆäº†ç¬¦å·è®¡ç®—å’Œæ·±åº¦å­¦ä¹ ä¹‹é—´çš„å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ¦‚ç‡æœ‰é™è‡ªåŠ¨æœºï¼ˆPFAï¼‰åœ¨å»ºæ¨¡åºåˆ—æ•°æ®çš„æ¦‚ç‡è¡Œä¸ºæ–¹é¢éå¸¸æœ‰ç”¨ï¼Œä½†å°†å…¶ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹ç»“åˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å¯ä»¥è¿‘ä¼¼ PFA çš„è¡Œä¸ºï¼Œä½†ç¼ºä¹ç²¾ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”è®­ç»ƒè¿‡ç¨‹å¤æ‚ã€‚å› æ­¤ï¼Œå¦‚ä½•ä½¿ç”¨ç¥ç»ç½‘ç»œç²¾ç¡®æ¨¡æ‹Ÿ PFA çš„è¡Œä¸ºï¼Œå¹¶ä½¿å…¶å…·æœ‰å¯å­¦ä¹ æ€§ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°† PFA çš„çŠ¶æ€åˆ†å¸ƒè¡¨ç¤ºä¸ºå‘é‡ï¼ŒçŠ¶æ€è½¬ç§»è¡¨ç¤ºä¸ºéšæœºçŸ©é˜µã€‚é€šè¿‡çŸ©é˜µ-å‘é‡ä¹˜ç§¯ï¼Œå¯ä»¥ç²¾ç¡®åœ°æ¨¡æ‹Ÿ PFA çš„çŠ¶æ€è½¬ç§»è¿‡ç¨‹ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•å°† PFA çš„åŠ¨æ€è¿‡ç¨‹è½¬åŒ–ä¸ºç¥ç»ç½‘ç»œä¸­çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼Œä»è€Œé¿å…äº†å¾ªç¯ç»“æ„ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä½¿ç”¨ç¬¦å·å‰é¦ˆç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿ PFAã€‚æ•´ä½“æ¶æ„åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è¾“å…¥å±‚ï¼šæ¥æ”¶åºåˆ—æ•°æ®ä½œä¸ºè¾“å…¥ï¼›2) çŠ¶æ€è¡¨ç¤ºå±‚ï¼šå°† PFA çš„çŠ¶æ€åˆ†å¸ƒè¡¨ç¤ºä¸ºå‘é‡ï¼›3) è½¬ç§»çŸ©é˜µå±‚ï¼šå°† PFA çš„çŠ¶æ€è½¬ç§»è¡¨ç¤ºä¸ºéšæœºçŸ©é˜µï¼›4) è¾“å‡ºå±‚ï¼šæ ¹æ®å½“å‰çŠ¶æ€åˆ†å¸ƒï¼Œè¾“å‡ºåºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒã€‚æ•´ä¸ªç½‘ç»œé€šè¿‡å‰å‘ä¼ æ’­ï¼Œæ¨¡æ‹Ÿ PFA çš„çŠ¶æ€è½¬ç§»è¿‡ç¨‹ï¼Œå¹¶è®¡ç®—åºåˆ—çš„æ¦‚ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨ç¬¦å·å‰é¦ˆç¥ç»ç½‘ç»œç²¾ç¡®æ¨¡æ‹Ÿ PFA çš„è¡Œä¸ºã€‚ä¸ä¼ ç»Ÿçš„ RNN æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š1) ç²¾ç¡®æ€§ï¼šå¯ä»¥ç²¾ç¡®åœ°æ¨¡æ‹Ÿ PFA çš„çŠ¶æ€è½¬ç§»è¿‡ç¨‹ï¼›2) å¯è§£é‡Šæ€§ï¼šçŠ¶æ€å’Œè½¬ç§»çŸ©é˜µå…·æœ‰æ˜ç¡®çš„ç‰©ç†æ„ä¹‰ï¼›3) å¯å­¦ä¹ æ€§ï¼šå¯ä»¥é€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•å­¦ä¹  PFA çš„å‚æ•°ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ one-hot å‘é‡è¡¨ç¤º PFA çš„çŠ¶æ€ï¼›2) ä½¿ç”¨éšæœºçŸ©é˜µè¡¨ç¤º PFA çš„çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼›3) ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è¡¡é‡é¢„æµ‹æ¦‚ç‡å’ŒçœŸå®æ¦‚ç‡ä¹‹é—´çš„å·®å¼‚ï¼›4) ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¼˜åŒ–ç½‘ç»œçš„å‚æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿç²¾ç¡®åœ°æ¨¡æ‹Ÿ PFA çš„è¡Œä¸ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æå‡ºçš„ç¬¦å·å‰é¦ˆç¥ç»ç½‘ç»œå¯ä»¥ç²¾ç¡®åœ°æ¨¡æ‹Ÿ PFA çš„è¡Œä¸ºï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•å­¦ä¹  PFA çš„å‚æ•°ã€‚åœ¨åˆæˆæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥æ¥è¿‘ 100% çš„å‡†ç¡®ç‡æ¢å¤ PFA çš„çŠ¶æ€è½¬ç§»çŸ©é˜µã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ã€ç”Ÿç‰©ä¿¡æ¯å­¦ç­‰é¢†åŸŸï¼Œç”¨äºå»ºæ¨¡åºåˆ—æ•°æ®çš„æ¦‚ç‡è¡Œä¸ºã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºæ„å»ºæ›´ç²¾ç¡®çš„è¯­è¨€æ¨¡å‹ï¼Œæé«˜è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®ç‡ï¼Œæˆ–è€…åˆ†æåŸºå› åºåˆ—çš„æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºå¼€å‘æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå°†ç¬¦å·è®¡ç®—å’Œæ·±åº¦å­¦ä¹ ç›¸ç»“åˆï¼Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a formal and constructive theory showing that probabilistic finite automata (PFAs) can be exactly simulated using symbolic feedforward neural networks. Our architecture represents state distributions as vectors and transitions as stochastic matrices, enabling probabilistic state propagation via matrix-vector products. This yields a parallel, interpretable, and differentiable simulation of PFA dynamics using soft updates-without recurrence. We formally characterize probabilistic subset construction, $\varepsilon$-closure, and exact simulation via layered symbolic computation, and prove equivalence between PFAs and specific classes of neural networks. We further show that these symbolic simulators are not only expressive but learnable: trained with standard gradient descent-based optimization on labeled sequence data, they recover the exact behavior of ground-truth PFAs. This learnability, formalized in Proposition 5.1, is the crux of this work. Our results unify probabilistic automata theory with neural architectures under a rigorous algebraic framework, bridging the gap between symbolic computation and deep learning.

