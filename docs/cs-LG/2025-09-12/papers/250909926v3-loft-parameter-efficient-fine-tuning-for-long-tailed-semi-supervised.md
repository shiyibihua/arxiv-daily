---
layout: default
title: LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios
---

# LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09926" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09926v3</a>
  <a href="https://arxiv.org/pdf/2509.09926.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09926v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09926v3', 'LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiyuan Huang, Jiahao Chen, Yurou Liu, Bing Su

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-10-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoFTæ¡†æ¶ï¼Œé€šè¿‡é«˜æ•ˆå‚æ•°å¾®è°ƒè§£å†³å¼€æ”¾ä¸–ç•Œé•¿å°¾åŠç›‘ç£å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿å°¾å­¦ä¹ ` `åŠç›‘ç£å­¦ä¹ ` `å¼€æ”¾ä¸–ç•Œ` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `é¢„è®­ç»ƒæ¨¡å‹` `ä¼ªæ ‡ç­¾` `åˆ†å¸ƒå¤–æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LTSSLæ–¹æ³•ä»å¤´è®­ç»ƒæ¨¡å‹ï¼Œæ˜“å¯¼è‡´è¿‡æ‹Ÿåˆå’Œä¼ªæ ‡ç­¾è´¨é‡ä½ã€‚
2. LoFTæ¡†æ¶é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œç”Ÿæˆæ›´å¯é çš„ä¼ªæ ‡ç­¾ï¼Œæå‡ä¸å¹³è¡¡å­¦ä¹ æ•ˆæœã€‚
3. LoFT-OWè§£å†³å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹çš„OODé—®é¢˜ï¼Œå®éªŒè¡¨æ˜æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿å°¾å­¦ä¹ å› å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚åœ¨ç°æœ‰æ–¹æ³•ä¸­ï¼Œé•¿å°¾åŠç›‘ç£å­¦ä¹ (LTSSL)é€šè¿‡å°†å¤§é‡æœªæ ‡è®°æ•°æ®çº³å…¥ä¸å¹³è¡¡çš„æ ‡è®°æ•°æ®é›†ï¼Œæˆä¸ºä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„LTSSLæ–¹æ³•è¢«è®¾è®¡ä¸ºä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´è¿‡åº¦è‡ªä¿¡å’Œä½è´¨é‡ä¼ªæ ‡ç­¾ç­‰é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†LTSSLæ‰©å±•åˆ°åŸºç¡€æ¨¡å‹å¾®è°ƒèŒƒå¼ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼šLoFTï¼ˆé€šè¿‡é«˜æ•ˆå‚æ•°å¾®è°ƒè¿›è¡Œé•¿å°¾åŠç›‘ç£å­¦ä¹ ï¼‰ã€‚æˆ‘ä»¬è¯æ˜äº†å¾®è°ƒåçš„åŸºç¡€æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´å¯é çš„ä¼ªæ ‡ç­¾ï¼Œä»è€Œæœ‰åˆ©äºä¸å¹³è¡¡å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç ”ç©¶å¼€æ”¾ä¸–ç•Œæ¡ä»¶ä¸‹çš„åŠç›‘ç£å­¦ä¹ ï¼Œæ¢ç´¢äº†ä¸€ç§æ›´å®ç”¨çš„è®¾ç½®ï¼Œå…¶ä¸­æœªæ ‡è®°çš„æ•°æ®å¯èƒ½åŒ…å«åˆ†å¸ƒå¤–(OOD)æ ·æœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LoFT-OWï¼ˆå¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹çš„LoFTï¼‰æ¥æé«˜åˆ¤åˆ«èƒ½åŠ›ã€‚åœ¨å¤šä¸ªåŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå³ä½¿åªä½¿ç”¨äº†ä»¥å‰å·¥ä½œ1%çš„æœªæ ‡è®°æ•°æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é•¿å°¾åŠç›‘ç£å­¦ä¹ ï¼ˆLTSSLï¼‰é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹ï¼Œå³æœªæ ‡è®°æ•°æ®ä¸­åŒ…å«åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ ·æœ¬çš„æƒ…å†µã€‚ç°æœ‰LTSSLæ–¹æ³•é€šå¸¸ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œè¿™å®¹æ˜“å¯¼è‡´æ¨¡å‹è¿‡åº¦è‡ªä¿¡ï¼Œç”Ÿæˆä½è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼Œä»è€Œå½±å“å­¦ä¹ æ•ˆæœã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†OODæ ·æœ¬æ—¶ç¼ºä¹æœ‰æ•ˆçš„åˆ¤åˆ«èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LTSSLé—®é¢˜ç½®äºåŸºç¡€æ¨¡å‹å¾®è°ƒçš„èŒƒå¼ä¸‹ã€‚åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å¼ºå¤§çš„è¡¨å¾èƒ½åŠ›ï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼ˆParameter-Efficient Fine-Tuningï¼‰æ¥é€‚åº”é•¿å°¾æ•°æ®åˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆé¿å…ä»å¤´è®­ç»ƒå¸¦æ¥çš„é—®é¢˜ï¼Œå¹¶èƒ½æ›´å¥½åœ°åˆ©ç”¨æœªæ ‡è®°æ•°æ®ã€‚é’ˆå¯¹å¼€æ”¾ä¸–ç•Œåœºæ™¯ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹å¯¹OODæ ·æœ¬çš„åˆ¤åˆ«èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLoFTæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹å¯¹æ ‡è®°å’Œæœªæ ‡è®°æ•°æ®è¿›è¡Œç‰¹å¾æå–ã€‚2) åˆ©ç”¨æ ‡è®°æ•°æ®å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼Œä½¿å…¶åˆæ­¥é€‚åº”é•¿å°¾æ•°æ®åˆ†å¸ƒã€‚3) ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶ç­›é€‰é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ç”¨äºåç»­è®­ç»ƒã€‚4) é’ˆå¯¹å¼€æ”¾ä¸–ç•Œåœºæ™¯ï¼ŒLoFT-OWå¼•å…¥é¢å¤–çš„æœºåˆ¶æ¥åŒºåˆ†OODæ ·æœ¬ï¼Œä¾‹å¦‚é€šè¿‡ç½®ä¿¡åº¦é˜ˆå€¼æˆ–å¯¹æŠ—è®­ç»ƒç­‰æ–¹æ³•ã€‚5) ç»“åˆæ ‡è®°æ•°æ®å’Œé«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼Œè¿›ä¸€æ­¥å¾®è°ƒæ¨¡å‹ï¼Œæå‡æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†åŸºç¡€æ¨¡å‹å¾®è°ƒå¼•å…¥LTSSLé¢†åŸŸï¼Œå¹¶æå‡ºäº†LoFTæ¡†æ¶ã€‚ä¸ä»å¤´è®­ç»ƒç›¸æ¯”ï¼Œå¾®è°ƒå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œç”Ÿæˆæ›´å¯é çš„ä¼ªæ ‡ç­¾ã€‚æ­¤å¤–ï¼ŒLoFT-OWé’ˆå¯¹å¼€æ”¾ä¸–ç•Œåœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæé«˜äº†æ¨¡å‹å¯¹OODæ ·æœ¬çš„é²æ£’æ€§ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒçš„ä½¿ç”¨é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—è¯¥æ–¹æ³•æ›´å…·å®ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚2) é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œä¾‹å¦‚Adapteræˆ–LoRAï¼Œä»¥å‡å°‘è®¡ç®—é‡å’Œå­˜å‚¨éœ€æ±‚ã€‚3) è®¾è®¡æœ‰æ•ˆçš„ä¼ªæ ‡ç­¾ç­›é€‰ç­–ç•¥ï¼Œä¾‹å¦‚åŸºäºç½®ä¿¡åº¦é˜ˆå€¼æˆ–ä¸€è‡´æ€§æ­£åˆ™åŒ–ã€‚4) é’ˆå¯¹å¼€æ”¾ä¸–ç•Œåœºæ™¯ï¼Œè®¾è®¡OODæ£€æµ‹æ¨¡å—ï¼Œä¾‹å¦‚åŸºäºèƒ½é‡çš„OODæ£€æµ‹æˆ–å¯¹æŠ—è®­ç»ƒã€‚5) æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘é•¿å°¾æ•°æ®çš„ç‰¹ç‚¹ï¼Œä¾‹å¦‚é‡‡ç”¨ç±»å¹³è¡¡æŸå¤±æˆ–é‡é‡‡æ ·ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLoFTæ¡†æ¶åœ¨å¤šä¸ªé•¿å°¾åŠç›‘ç£å­¦ä¹ åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹ï¼ŒLoFT-OWèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†OODæ ·æœ¬ï¼Œå¹¶ä¿æŒè¾ƒé«˜çš„åˆ†ç±»ç²¾åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLoFTæ¡†æ¶ä»…ä½¿ç”¨å°‘é‡æœªæ ‡è®°æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œä»…ä¸ºå…ˆå‰å·¥ä½œçš„1%ï¼‰å°±èƒ½è¾¾åˆ°ç”šè‡³è¶…è¿‡ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œä½“ç°äº†å…¶é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡ä¸”å­˜åœ¨å¤§é‡æœªæ ‡è®°æ•°æ®çš„åœºæ™¯ä¸‹ï¼Œä¾‹å¦‚åŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ã€ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ç­‰ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œé«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œå¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹åœ¨é•¿å°¾æ•°æ®ä¸Šçš„æ€§èƒ½ï¼Œé™ä½å¯¹å¤§é‡æ ‡è®°æ•°æ®çš„ä¾èµ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long-tailed learning has garnered increasing attention due to its wide applicability in real-world scenarios. Among existing approaches, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating a large amount of unlabeled data into the imbalanced labeled dataset. However, most prior LTSSL methods are designed to train models from scratch, which often leads to issues such as overconfidence and low-quality pseudo-labels. To address these challenges, we extend LTSSL into the foundation model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate that fine-tuned foundation models can generate more reliable pseudolabels, thereby benefiting imbalanced learning. Furthermore, we explore a more practical setting by investigating semi-supervised learning under open-world conditions, where the unlabeled data may include out-of-distribution (OOD) samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World scenarios) to improve the discriminative ability. Experimental results on multiple benchmarks demonstrate that our method achieves superior performance compared to previous approaches, even when utilizing only 1\% of the unlabeled data compared with previous works.

