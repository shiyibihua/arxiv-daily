---
layout: default
title: The language of time: a language model perspective on time-series foundation models
---

# The language of time: a language model perspective on time-series foundation models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00078" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00078v1</a>
  <a href="https://arxiv.org/pdf/2507.00078.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00078v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00078v1', 'The language of time: a language model perspective on time-series foundation models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Xie, Yun Xiong, Zejian Shi, Hao Niu, Zhengfu Liu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„æ–°è§†è§’ä»¥è§£å†³è·¨åŸŸè¿ç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—æ¨¡å‹` `åŸºç¡€æ¨¡å‹` `è¡¨ç¤ºå­¦ä¹ ` `è·¨åŸŸè¿ç§»` `æ¦‚ç‡åˆ†å¸ƒ` `é‡åŒ–æŠ€æœ¯` `åŠ¨æ€ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ—¶é—´åºåˆ—æ•°æ®çš„åŠ¨æ€ç‰¹æ€§ä½¿å¾—è·¨åŸŸè¿ç§»åœ¨ç†è®ºä¸Šæ˜¾å¾—ä¸åˆç†ï¼Œç°æœ‰æ¨¡å‹åœ¨è¿™ä¸€ç‚¹ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºåŸºäºè¡¥ä¸çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å°†ç¡®å®šæ€§è¡¨ç¤ºæ‰©å±•ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œè§£å†³äº†è¡¨ç¤ºå­¦ä¹ çš„å±€é™æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨æ—¶é—´ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œèƒ½å¤Ÿæœ‰æ•ˆç»§æ‰¿è¯­è¨€æ¨¡å‹çš„å¼ºå¤§è¿ç§»èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ä½œä¸ºè¿™ä¸€èŒƒå¼çš„é‡è¦æ‰©å±•ï¼Œå±•ç°å‡ºå“è¶Šçš„è¡¨è¾¾èƒ½åŠ›å’Œè·¨åŸŸè¿ç§»èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ—¶é—´åºåˆ—æ•°æ®åæ˜ äº†ä¸åŒçš„åŠ¨æ€ç³»ç»Ÿï¼Œè¿™ä½¿å¾—è·¨åŸŸè¿ç§»åœ¨ç›´è§‚ä¸Šæ˜¾å¾—ä¸å¤ªåˆç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ‚–è®ºï¼Œæœ¬æ–‡ä»ç†è®ºå’Œå®éªŒä¸¤ä¸ªè§’åº¦æ¢è®¨äº†åŸºäºè¡¥ä¸çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºå­¦ä¹ æœºåˆ¶å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™äº›æ¨¡å‹ä¸ä»…ä»…æ˜¯åº”ç”¨æ–°æ¶æ„ï¼Œè€Œæ˜¯é€šè¿‡å°†ç¡®å®šæ€§å‘é‡è¡¨ç¤ºæ‰©å±•åˆ°æ½œåœ¨çš„æ¦‚ç‡åˆ†å¸ƒå½¢å¼ï¼Œä»æ ¹æœ¬ä¸Šæ¨å¹¿äº†è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºèŒƒå¼ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿ç»­çš„æ—¶é—´åºåˆ—è¡¥ä¸å¯ä»¥è¢«å¿ å®åœ°é‡åŒ–ä¸ºç¦»æ•£è¯æ±‡ï¼Œå…¶å…³é”®ç»Ÿè®¡ç‰¹æ€§ä¸è‡ªç„¶è¯­è¨€é«˜åº¦ä¸€è‡´ã€‚è¿™ä¸€æ¨å¹¿ä½¿å¾—æ—¶é—´åºåˆ—æ¨¡å‹èƒ½å¤Ÿç»§æ‰¿å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¼ºå¤§è¡¨ç¤ºå’Œè¿ç§»èƒ½åŠ›ï¼Œä»è€Œè§£é‡Šäº†å®ƒä»¬åœ¨æ—¶é—´ä»»åŠ¡ä¸­çš„ä¼˜è¶Šè¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ—¶é—´åºåˆ—æ•°æ®åœ¨è·¨åŸŸè¿ç§»ä¸­çš„è¡¨ç°æ‚–è®ºï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒåŠ¨æ€ç³»ç»Ÿæ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆè§£é‡Šæ¨¡å‹çš„æˆåŠŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†æ—¶é—´åºåˆ—æ•°æ®çš„è¡¨ç¤ºä»ç¡®å®šæ€§å‘é‡æ‰©å±•åˆ°æ½œåœ¨çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¡¨ç¤ºå­¦ä¹ æœºåˆ¶ï¼Œå¼ºè°ƒäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè·¨åŸŸè¿ç§»èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è¡¥ä¸ç”Ÿæˆã€é‡åŒ–ç¦»æ•£è¯æ±‡å’Œæ¨¡å‹è®­ç»ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå¯¹æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œåˆ†æ®µå¤„ç†ï¼Œç„¶åå°†æ¯ä¸ªè¡¥ä¸é‡åŒ–ä¸ºç¦»æ•£è¯æ±‡ï¼Œæœ€åé€šè¿‡è®­ç»ƒæ¨¡å‹æ¥å­¦ä¹ è¿™äº›è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ—¶é—´åºåˆ—è¡¥ä¸çš„è¡¨ç¤ºå­¦ä¹ ä¸è‡ªç„¶è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºèŒƒå¼ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„ç†è§£æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„é‡åŒ–ç­–ç•¥å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿è¡¥ä¸çš„ç»Ÿè®¡ç‰¹æ€§ä¸è‡ªç„¶è¯­è¨€ä¸€è‡´ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¨¡å‹åœ¨å¤šä¸ªæ—¶é—´åºåˆ—ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨åŸŸè¿ç§»ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºé¢„æµ‹ã€æ°”è±¡æ•°æ®åˆ†æå’Œå¥åº·ç›‘æµ‹ç­‰æ—¶é—´åºåˆ—ç›¸å…³ä»»åŠ¡ã€‚é€šè¿‡æå‡æ—¶é—´åºåˆ—æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè¿ç§»èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒé¢†åŸŸä¸­å®ç°æ›´å‡†ç¡®çš„é¢„æµ‹å’Œå†³ç­–æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rise of large language models, the paradigm of training foundation models with massive parameter counts on vast datasets has been adopted in multiple domains to achieve remarkable success. Time series foundation models represent a significant extension of this paradigm, demonstrating exceptional expressive power, generalization, and cross-domain transferability. However, this gives rise to a fundamental paradox: time series data reflect distinct dynamical systems, making cross-domain transfer intuitively implausible, yet this is contradicted by the models' empirical success. To resolve this paradox, this paper investigates, from both theoretical and experimental perspectives, the representation learning mechanisms and generalization capabilities of patch-based time series foundation models. We argue that such models are not merely applying a new architecture but are fundamentally generalizing the representation paradigm of language models by extending deterministic vector-based representations to latent probabilistic distributional forms. Our theoretical analysis supports this framework by demonstrating that continuous time-series patches can be faithfully quantized into a discrete vocabulary whose key statistical properties are highly consistent with those of natural language. This generalization allows time series models to inherit the robust representation and transfer abilities of large language models, thereby explaining their superior performance in temporal tasks. Ultimately, our work provides a rigorous theoretical cornerstone for understanding, evaluating, and improving the safety and reliability of large-scale time series foundation models.

