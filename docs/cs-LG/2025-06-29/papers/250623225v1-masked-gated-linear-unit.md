---
layout: default
title: Masked Gated Linear Unit
---

# Masked Gated Linear Unit

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23225" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23225v1</a>
  <a href="https://arxiv.org/pdf/2506.23225.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23225v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23225v1', 'Masked Gated Linear Unit')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yukito Tajima, Nakamasa Inoue, Yusuke Sekikawa, Ikuro Sato, Rio Yokota

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMasked Gated Linear Unitsä»¥è§£å†³GLUçš„å†…å­˜ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é—¨æ§çº¿æ€§å•å…ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å†…å­˜æ•ˆç‡` `æ¨ç†é€Ÿåº¦` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Gated Linear Unitsï¼ˆGLUsï¼‰åœ¨å†…å­˜è¯»å–æ–¹é¢å­˜åœ¨ç“¶é¢ˆï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºMasked Gated Linear Unitsï¼ˆMGLUsï¼‰ï¼Œé€šè¿‡å…±äº«æƒé‡çŸ©é˜µå’Œå­¦ä¹ å¤šä¸ªäºŒè¿›åˆ¶æ©ç æ¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFlashMGLUåœ¨æ¨ç†é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸGLUsï¼ŒåŒæ—¶ä¿æŒæˆ–è¶…è¶Šäº†å‡†ç¡®åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Gated Linear Unitsï¼ˆGLUsï¼‰å·²æˆä¸ºæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‰é¦ˆç½‘ç»œçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œç”±äºé—¨æ§å’Œæ•°å€¼æµä½¿ç”¨ç‹¬ç«‹çš„æƒé‡çŸ©é˜µï¼ŒGLUsçš„å†…å­˜è¯»å–éœ€æ±‚æ˜¯æ— é—¨æ§å‰é¦ˆå±‚çš„ä¸¤å€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæœ¬æ–‡æå‡ºäº†Masked Gated Linear Unitsï¼ˆMGLUsï¼‰ï¼Œä¸€ç§é«˜æ•ˆçš„GLUå®ç°ã€‚MGLUsçš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ï¼šæ··åˆå…ƒç´ çº§é—¨æ§ï¼ˆMoEGï¼‰æ¶æ„ï¼Œé€šè¿‡å­¦ä¹ å¤šä¸ªäºŒè¿›åˆ¶æ©ç ï¼Œåœ¨å•ä¸ªå…±äº«æƒé‡çŸ©é˜µä¸Šç¡®å®šé—¨æ§æˆ–æ•°å€¼åˆ†é…ï¼Œä»è€Œå‡å°‘å†…å­˜ä¼ è¾“ï¼›FlashMGLUï¼Œä¸€ä¸ªç¡¬ä»¶å‹å¥½çš„å†…æ ¸ï¼Œåœ¨RTX5090 GPUä¸Šç›¸è¾ƒäºä¼ ç»ŸGLUå®ç°æé«˜äº†47%çš„å†…å­˜æ•ˆç‡å’Œ34%çš„é€Ÿåº¦ï¼Œæ¨ç†æ—¶é—´é€Ÿåº¦æå‡é«˜è¾¾19.7å€ã€‚åœ¨LLMå®éªŒä¸­ï¼ŒSwishæ¿€æ´»çš„å˜ä½“SwiMGLUåœ¨ä¿æŒå†…å­˜ä¼˜åŠ¿çš„åŒæ—¶ï¼Œå‡†ç¡®åº¦ä¸SwiGLUåŸºçº¿ç›¸å½“ï¼Œç”šè‡³æ›´é«˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„Gated Linear Unitsï¼ˆGLUsï¼‰åœ¨å‰é¦ˆç½‘ç»œä¸­ç”±äºä½¿ç”¨ç‹¬ç«‹çš„æƒé‡çŸ©é˜µï¼Œå¯¼è‡´å†…å­˜è¯»å–éœ€æ±‚å¢åŠ ï¼Œä»è€Œå½±å“äº†æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMasked Gated Linear Unitsï¼ˆMGLUsï¼‰é€šè¿‡å¼•å…¥æ··åˆå…ƒç´ çº§é—¨æ§ï¼ˆMoEGï¼‰æ¶æ„ï¼Œåˆ©ç”¨å•ä¸ªå…±äº«æƒé‡çŸ©é˜µå’Œå¤šä¸ªäºŒè¿›åˆ¶æ©ç æ¥ä¼˜åŒ–é—¨æ§å’Œæ•°å€¼æµçš„åˆ†é…ï¼Œä»è€Œå‡å°‘å†…å­˜ä¼ è¾“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMGLUsçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯å­¦ä¹ å¤šä¸ªäºŒè¿›åˆ¶æ©ç ä»¥è¿›è¡Œå…ƒç´ çº§çš„é—¨æ§åˆ†é…ï¼Œå…¶æ¬¡æ˜¯å®ç°FlashMGLUå†…æ ¸ä»¥æé«˜æ¨ç†é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šMGLUsçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å…±äº«æƒé‡çŸ©é˜µå’Œå…ƒç´ çº§é—¨æ§æœºåˆ¶ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜è¯»å–éœ€æ±‚ï¼Œä¸ä¼ ç»ŸGLUsç›¸æ¯”ï¼Œæä¾›äº†æ›´é«˜çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMGLUsé‡‡ç”¨äº†Swishæ¿€æ´»å‡½æ•°çš„å˜ä½“SwiMGLUï¼Œå¹¶åœ¨RTX5090 GPUä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œç¡®ä¿åœ¨ä¿æŒå†…å­˜ä¼˜åŠ¿çš„åŒæ—¶ï¼Œå‡†ç¡®åº¦ä¸åŸºçº¿æ¨¡å‹ç›¸å½“æˆ–æ›´é«˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFlashMGLUåœ¨æ¨ç†é€Ÿåº¦ä¸Šæ¯”ä¼ ç»Ÿçš„PyTorch MGLUå¿«19.7å€ï¼ŒåŒæ—¶åœ¨å†…å­˜ä½¿ç”¨ä¸Šæé«˜äº†47%çš„æ•ˆç‡ï¼Œå¹¶ä¸”é€Ÿåº¦æ¯”æ ‡å‡†GLUså¿«34%ã€‚SwiMGLUåœ¨ä¿æŒå†…å­˜ä¼˜åŠ¿çš„åŒæ—¶ï¼Œå‡†ç¡®åº¦ä¸SwiGLUåŸºçº¿ç›¸å½“ï¼Œç”šè‡³æœ‰æ‰€æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘ã€‚é€šè¿‡æé«˜å†…å­˜æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ï¼ŒMGLUsèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Gated Linear Units (GLUs) have become essential components in the feed-forward networks of state-of-the-art Large Language Models (LLMs). However, they require twice as many memory reads compared to feed-forward layers without gating, due to the use of separate weight matrices for the gate and value streams. To address this bottleneck, we introduce Masked Gated Linear Units (MGLUs), a novel family of GLUs with an efficient kernel implementation. The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating (MoEG) architecture that learns multiple binary masks, each determining gate or value assignments at the element level on a single shared weight matrix resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly kernel that yields up to a 19.7 $\times$ inference-time speed-up over a naive PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs despite added architectural complexity on an RTX5090 GPU. In LLM experiments, the Swish-activated variant SwiMGLU preserves its memory advantages while matching - or even surpassing - the downstream accuracy of the SwiGLU baseline.

