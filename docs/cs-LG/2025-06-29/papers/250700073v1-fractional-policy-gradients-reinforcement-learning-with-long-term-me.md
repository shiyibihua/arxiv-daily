---
layout: default
title: Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory
---

# Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00073" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00073v1</a>
  <a href="https://arxiv.org/pdf/2507.00073.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00073v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00073v1', 'Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Urvi Pawar, Kunal Telangi

**åˆ†ç±»**: cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

**å¤‡æ³¨**: Submitted to Journal of Machine Learning Research (JMLR), June 2025. 24 pages, 3 figures. Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†æ•°ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä»¥è§£å†³é•¿æœŸè®°å¿†å¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `åˆ†æ•°å¾®ç§¯åˆ†` `é•¿æœŸè®°å¿†` `ç­–ç•¥ä¼˜åŒ–` `æ–¹å·®é™ä½` `æ ·æœ¬æ•ˆç‡` `æ—¶é—´å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿æœŸæ—¶é—´å»ºæ¨¡ä¸Šå­˜åœ¨å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨é«˜æ–¹å·®å’Œä½æ•ˆé‡‡æ ·æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºçš„FPGæ¡†æ¶é€šè¿‡å¼•å…¥åˆ†æ•°å¾®ç§¯åˆ†ï¼Œé‡æ–°å®šä¹‰ç­–ç•¥æ¢¯åº¦ä»¥æ•æ‰é•¿æœŸæ—¶é—´ç›¸å…³æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFPGåœ¨æ ·æœ¬æ•ˆç‡å’Œæ–¹å·®æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°35-68%å’Œ24-52%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†åˆ†æ•°ç­–ç•¥æ¢¯åº¦ï¼ˆFPGï¼‰æ¡†æ¶ï¼Œç»“åˆåˆ†æ•°å¾®ç§¯åˆ†ç”¨äºç­–ç•¥ä¼˜åŒ–ä¸­çš„é•¿æœŸæ—¶é—´å»ºæ¨¡ã€‚æ ‡å‡†çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•å—åˆ°é©¬å°”å¯å¤«å‡è®¾çš„é™åˆ¶ï¼Œè¡¨ç°å‡ºé«˜æ–¹å·®å’Œä½æ•ˆé‡‡æ ·ã€‚é€šè¿‡ä½¿ç”¨Caputoåˆ†æ•°å¯¼æ•°é‡æ–°æ„é€ æ¢¯åº¦ï¼ŒFPGå»ºç«‹äº†çŠ¶æ€è½¬ç§»ä¹‹é—´çš„å¹‚å¾‹æ—¶é—´ç›¸å…³æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„é€’å½’è®¡ç®—æŠ€æœ¯ï¼Œç”¨äºåˆ†æ•°æ—¶é—´å·®é”™çš„è®¡ç®—ï¼Œå…·æœ‰æ’å®šçš„æ—¶é—´å’Œå†…å­˜éœ€æ±‚ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒFPGåœ¨ä¿æŒæ”¶æ•›æ€§çš„åŒæ—¶ï¼Œå®ç°äº†O(t^(-alpha))çš„æ¸è¿‘æ–¹å·®é™ä½ã€‚å®è¯éªŒè¯æ˜¾ç¤ºï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºçº¿ï¼ŒFPGåœ¨æ ·æœ¬æ•ˆç‡ä¸Šæå‡äº†35-68%ï¼Œæ–¹å·®é™ä½äº†24-52%ã€‚è¯¥æ¡†æ¶ä¸ºåˆ©ç”¨é•¿ç¨‹ä¾èµ–æä¾›äº†æ•°å­¦åŸºç¡€ï¼Œè€Œæ— éœ€é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿æœŸè®°å¿†å»ºæ¨¡ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯é«˜æ–¹å·®å’Œä½æ•ˆé‡‡æ ·çš„é—®é¢˜ã€‚æ ‡å‡†ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¾èµ–äºé©¬å°”å¯å¤«å‡è®¾ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFPGæ¡†æ¶é€šè¿‡å¼•å…¥åˆ†æ•°å¾®ç§¯åˆ†ï¼Œåˆ©ç”¨Caputoåˆ†æ•°å¯¼æ•°é‡æ–°æ„é€ ç­–ç•¥æ¢¯åº¦ï¼Œä»è€Œå»ºç«‹çŠ¶æ€è½¬ç§»ä¹‹é—´çš„å¹‚å¾‹æ—¶é—´ç›¸å…³æ€§ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰é•¿æœŸä¾èµ–å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFPGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬çŠ¶æ€è¡¨ç¤ºã€åˆ†æ•°æ—¶é—´å·®é”™è®¡ç®—å’Œç­–ç•¥ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é€šè¿‡é€’å½’è®¡ç®—æŠ€æœ¯ï¼ŒFPGèƒ½å¤Ÿåœ¨æ’å®šçš„æ—¶é—´å’Œå†…å­˜éœ€æ±‚ä¸‹é«˜æ•ˆåœ°å¤„ç†åˆ†æ•°æ—¶é—´å·®é”™ã€‚

**å…³é”®åˆ›æ–°**ï¼šFPGçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†åˆ†æ•°å¾®ç§¯åˆ†åº”ç”¨äºç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†æ–¹å·®å¹¶æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨ç†è®ºåŸºç¡€å’Œè®¡ç®—æ•ˆç‡ä¸Šå­˜åœ¨æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨FPGä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬åˆ†æ•°é˜¶æ•°çš„é€‰æ‹©å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ã€‚é€šè¿‡åˆç†çš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒç­–ç•¥ï¼ŒFPGèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†åˆ†æ•°æ—¶é—´å·®é”™çš„è®¡ç®—ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æ”¶æ•›æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFPGåœ¨æ ·æœ¬æ•ˆç‡ä¸Šè¾ƒæœ€å…ˆè¿›åŸºçº¿æå‡äº†35-68%ï¼Œåœ¨æ–¹å·®æ–¹é¢é™ä½äº†24-52%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFPGåœ¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€é‡‘èé¢„æµ‹å’Œæ™ºèƒ½å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡æœ‰æ•ˆæ•æ‰é•¿æœŸä¾èµ–å…³ç³»ï¼ŒFPGèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ å’Œå†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose Fractional Policy Gradients (FPG), a reinforcement learning framework incorporating fractional calculus for long-term temporal modeling in policy optimization. Standard policy gradient approaches face limitations from Markovian assumptions, exhibiting high variance and inefficient sampling. By reformulating gradients using Caputo fractional derivatives, FPG establishes power-law temporal correlations between state transitions. We develop an efficient recursive computation technique for fractional temporal-difference errors with constant time and memory requirements. Theoretical analysis shows FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus standard policy gradients while preserving convergence. Empirical validation demonstrates 35-68% sample efficiency gains and 24-52% variance reduction versus state-of-the-art baselines. This framework provides a mathematically grounded approach for leveraging long-range dependencies without computational overhead.

