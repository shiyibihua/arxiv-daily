---
layout: default
title: veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD
---

# veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07003" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07003v1</a>
  <a href="https://arxiv.org/pdf/2509.07003.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07003v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07003v1', 'veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Youjie Li, Cheng Wan, Zhiqi Lin, Hongyu Zhu, Jiacheng Yang, Ziang Song, Xinyi Di, Jiawei Wu, Huiyao Shu, Wenlei Bao, Yanghua Peng, Haibin Lin, Li-Wen Chang

**åˆ†ç±»**: cs.PL, cs.DC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

**å¤‡æ³¨**: 21 pages, 16 figures, 5 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**veScaleï¼šé€šè¿‡Eageræ¨¡å¼SPMDå®ç°ä¸€è‡´ä¸”é«˜æ•ˆçš„å¼ é‡ç¼–ç¨‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆ†å¸ƒå¼è®­ç»ƒ` `Eageræ¨¡å¼` `SPMD` `éšæœºæ•°ç”Ÿæˆ` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿï¼Œå¦‚PyTorchï¼Œåœ¨Eageræ¨¡å¼ä¸‹ä½¿ç”¨SPMDæ—¶ï¼Œéš¾ä»¥ä¿è¯ä¸å•è®¾å¤‡æ‰§è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œè°ƒè¯•å›°éš¾ã€‚
2. veScaleæå‡ºäº†ä¸€ç§æ–°çš„åˆ†å¸ƒå¼éšæœºæ•°ç”Ÿæˆï¼ˆRNGï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¸ä»»æ„åˆ†ç‰‡ç®—å­å…¼å®¹ï¼Œä»è€Œç¡®ä¿äº†åˆ†å¸ƒå¼è®­ç»ƒç»“æœçš„ä¸€è‡´æ€§ã€‚
3. veScaleé€šè¿‡ä¼˜åŒ–PyTorchåŸè¯­å¼€é”€å’Œé€šä¿¡æ•ˆç‡ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒæ€§èƒ½ï¼Œå®éªŒè¡¨æ˜å…¶é€Ÿåº¦æ¯”TorchTitanå¿«2.2å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§„æ¨¡å’Œå¤æ‚æ€§ä¸Šè¿…é€Ÿå¢é•¿ï¼Œéœ€è¦è¶Šæ¥è¶Šå¤æ‚çš„å¹¶è¡ŒåŒ–æ–¹æ³•æ¥è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œä¾‹å¦‚3Då¹¶è¡Œã€‚è¿™ç§å¤æ‚æ€§ä¿ƒä½¿äººä»¬è½¬å‘æ›´ç®€å•ã€æ›´æ˜“äºè°ƒè¯•çš„ç¼–ç¨‹èŒƒå¼ï¼Œå¦‚å•ç¨‹åºå¤šæ•°æ®ï¼ˆSPMDï¼‰ã€‚ç„¶è€Œï¼ŒEageræ¨¡å¼æ‰§è¡Œä¸­çš„SPMDå¼•å…¥äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç¡®ä¿ä¸å•è®¾å¤‡æ‰§è¡Œçš„ä¸€è‡´æ€§ï¼Œä»¥åŠå®ç°å¤§è§„æ¨¡ä¸‹çš„é«˜æ€§èƒ½ã€‚æœ¬æ–‡ä»‹ç»äº†veScaleï¼Œä¸€ä¸ªEageræ¨¡å¼è®­ç»ƒç³»ç»Ÿï¼Œå®ƒå®Œå…¨é‡‡ç”¨SPMDèŒƒå¼ï¼Œä»¥æ™®åŠåˆ†å¸ƒå¼å¼ é‡ç¼–ç¨‹ã€‚veScaleé€šè¿‡å¼•å…¥ä¸€ç§ä¸ä»»æ„åˆ†ç‰‡ç®—å­å…¼å®¹çš„åˆ†å¸ƒå¼éšæœºæ•°ç”Ÿæˆï¼ˆRNGï¼‰çš„æ–°ç®—æ³•ï¼Œè§£å†³äº†PyTorchç­‰ç³»ç»Ÿä¸­æ™®éå­˜åœ¨çš„ä¸ä¸€è‡´ç»“æœé—®é¢˜ã€‚veScaleè¿˜é€šè¿‡å‡å°‘PyTorchåŸè¯­çš„å¼€é”€å’Œæé«˜é€šä¿¡æ•ˆç‡ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ€§èƒ½ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒveScaleæ¯”æœ€å…ˆè¿›çš„è®­ç»ƒç³»ç»Ÿï¼ˆå¦‚TorchTitanï¼‰æé€Ÿé«˜è¾¾2.2å€ï¼Œå¹¶å°†ä»£ç å¤æ‚æ€§é™ä½äº†78.4%ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å•è®¾å¤‡ç­‰æ•ˆçš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰åŸºäºEageræ¨¡å¼çš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œå°¤å…¶æ˜¯åœ¨SPMDèŒƒå¼ä¸‹ï¼Œé¢ä¸´ç€ä¸å•è®¾å¤‡è®­ç»ƒç»“æœä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¿™ç§ä¸ä¸€è‡´æ€§ä½¿å¾—è°ƒè¯•å’ŒéªŒè¯å˜å¾—å¼‚å¸¸å›°éš¾ï¼Œé˜»ç¢äº†åˆ†å¸ƒå¼è®­ç»ƒçš„æ™®åŠã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éšæœºæ•°ç”Ÿæˆå’Œç®—å­åˆ†ç‰‡æ—¶ï¼Œæ— æ³•ä¿è¯å…¨å±€ä¸€è‡´æ€§ï¼Œå¯¼è‡´æœ€ç»ˆç»“æœåå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šveScaleçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡ä¸€ç§ä¸ä»»æ„åˆ†ç‰‡ç®—å­å…¼å®¹çš„åˆ†å¸ƒå¼éšæœºæ•°ç”Ÿæˆï¼ˆRNGï¼‰ç®—æ³•ï¼Œæ¥ä¿è¯åˆ†å¸ƒå¼è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºæ•°çš„ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡ä¼˜åŒ–PyTorchåŸè¯­çš„æ‰§è¡Œæ•ˆç‡å’Œé€šä¿¡æ•ˆç‡ï¼Œæå‡æ•´ä½“è®­ç»ƒæ€§èƒ½ã€‚è¿™æ ·æ—¢èƒ½ä¿è¯ç»“æœçš„æ­£ç¡®æ€§ï¼Œåˆèƒ½æå‡è®­ç»ƒé€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šveScaleæ˜¯ä¸€ä¸ªEageræ¨¡å¼çš„åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿï¼Œå®ƒåŸºäºPyTorchæ„å»ºï¼Œå¹¶å®Œå…¨é‡‡ç”¨SPMDèŒƒå¼ã€‚å…¶ä¸»è¦æ¨¡å—åŒ…æ‹¬ï¼šåˆ†å¸ƒå¼éšæœºæ•°ç”Ÿæˆæ¨¡å—ã€ä¼˜åŒ–çš„PyTorchåŸè¯­æ‰§è¡Œå¼•æ“å’Œé«˜æ•ˆçš„é€šä¿¡æ¨¡å—ã€‚è®­ç»ƒæµç¨‹ä¸æ ‡å‡†çš„PyTorch Eageræ¨¡å¼ç±»ä¼¼ï¼Œä½†veScaleåœ¨åº•å±‚å¯¹éšæœºæ•°ç”Ÿæˆã€ç®—å­æ‰§è¡Œå’Œé€šä¿¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šveScaleæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶åˆ†å¸ƒå¼éšæœºæ•°ç”Ÿæˆï¼ˆRNGï¼‰ç®—æ³•ã€‚è¯¥ç®—æ³•èƒ½å¤Ÿä¿è¯åœ¨ä»»æ„åˆ†ç‰‡ç®—å­ä¸‹ï¼Œå„ä¸ªè®¾å¤‡ä¸Šç”Ÿæˆçš„éšæœºæ•°åºåˆ—ä¸å•è®¾å¤‡æ‰§è¡Œæ—¶å®Œå…¨ä¸€è‡´ã€‚è¿™è§£å†³äº†åˆ†å¸ƒå¼è®­ç»ƒä¸­ç»“æœä¸ä¸€è‡´çš„æ ¹æœ¬é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¯¹PyTorchåŸè¯­çš„ä¼˜åŒ–å’Œé€šä¿¡æ•ˆç‡çš„æå‡ä¹Ÿæ˜¾è‘—æé«˜äº†è®­ç»ƒæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šveScaleçš„åˆ†å¸ƒå¼RNGç®—æ³•é‡‡ç”¨äº†ä¸€ç§å…¨å±€åŒæ­¥çš„ç§å­ç”Ÿæˆæœºåˆ¶ï¼Œç¡®ä¿æ¯ä¸ªè®¾å¤‡ä¸Šçš„éšæœºæ•°ç”Ÿæˆå™¨éƒ½ä½¿ç”¨ç›¸åŒçš„ç§å­åºåˆ—ã€‚åŒæ—¶ï¼Œè¯¥ç®—æ³•è¿˜è€ƒè™‘äº†ç®—å­åˆ†ç‰‡çš„å½±å“ï¼Œä¿è¯å³ä½¿åœ¨ç®—å­è¢«åˆ†ç‰‡åˆ°ä¸åŒè®¾å¤‡ä¸Šæ‰§è¡Œæ—¶ï¼Œéšæœºæ•°åºåˆ—ä»ç„¶ä¿æŒä¸€è‡´ã€‚åœ¨PyTorchåŸè¯­ä¼˜åŒ–æ–¹é¢ï¼ŒveScaleé‡‡ç”¨äº†ç®—å­èåˆå’Œå†…å­˜å¤ç”¨ç­‰æŠ€æœ¯ï¼Œå‡å°‘äº†è®¡ç®—å’Œå†…å­˜è®¿é—®çš„å¼€é”€ã€‚åœ¨é€šä¿¡æ–¹é¢ï¼ŒveScaleé‡‡ç”¨äº†é«˜æ•ˆçš„All-Reduceç®—æ³•å’Œæ•°æ®å‹ç¼©æŠ€æœ¯ï¼Œé™ä½äº†é€šä¿¡å»¶è¿Ÿã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

veScaleåœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„è®­ç»ƒç³»ç»ŸTorchTitanç›¸æ¯”ï¼ŒveScaleå®ç°äº†é«˜è¾¾2.2å€çš„åŠ é€Ÿã€‚åŒæ—¶ï¼ŒveScaleçš„ä»£ç å¤æ‚æ€§é™ä½äº†78.4%ï¼Œæ˜¾è‘—ç®€åŒ–äº†åˆ†å¸ƒå¼è®­ç»ƒçš„ç¼–ç¨‹éš¾åº¦ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒveScaleä¿è¯äº†ä¸å•è®¾å¤‡æ‰§è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œè§£å†³äº†åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„å…³é”®é—®é¢˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

veScaleé€‚ç”¨äºå„ç§éœ€è¦å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒçš„åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ã€‚å®ƒå¯ä»¥ç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒçš„ç¼–ç¨‹æ¨¡å‹ï¼Œé™ä½è°ƒè¯•éš¾åº¦ï¼Œå¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæ¨åŠ¨AIæŠ€æœ¯çš„æ™®åŠå’Œå‘å±•ï¼ŒåŠ é€ŸLLMç­‰å¤æ‚æ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have scaled rapidly in size and complexity, requiring increasingly intricate parallelism for distributed training, such as 3D parallelism. This sophistication motivates a shift toward simpler, more debuggable programming paradigm like Single Program Multiple Data (SPMD). However, SPMD in eager execution introduces two key challenges: ensuring consistency with single-device execution and achieving high performance at scale. In this paper, we introduce veScale, an eager-mode training system that fully embraces SPMD paradigm to democratize distributed tensor programming. veScale addresses the prevalent issue of inconsistent results in systems like PyTorch by introducing a novel algorithm of distributed Random Number Generation (RNG) compatible with arbitrary sharded operators. veScale also significantly boosts training performance by reducing PyTorch primitive's overhead and improving communication efficiency. Evaluations show that veScale delivers up to 2.2x speedup over the state-of-the-art training systems, like TorchTitan, and cuts code complexity by 78.4%, while preserving single-device-equivalent results.

