---
layout: default
title: Self-Aligned Reward: Towards Effective and Efficient Reasoners
---

# Self-Aligned Reward: Towards Effective and Efficient Reasoners

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05489" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05489v1</a>
  <a href="https://arxiv.org/pdf/2509.05489.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05489v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05489v1', 'Self-Aligned Reward: Towards Effective and Efficient Reasoners')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peixuan Han, Adit Krishnan, Gerald Friedland, Jiaxuan You, Chris Kong

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªå¯¹é½å¥–åŠ±(SAR)ï¼Œæå‡LLMæ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªå¯¹é½å¥–åŠ±` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç‡` `å›°æƒ‘åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ¨ç†æ–¹æ³•ä¾èµ–ç²—ç²’åº¦äºŒå…ƒå¥–åŠ±ï¼Œå¯¼è‡´æ¨ç†å†—é•¿å’Œé«˜è®¡ç®—æˆæœ¬ã€‚
2. æå‡ºè‡ªå¯¹é½å¥–åŠ±(SAR)ï¼Œåˆ©ç”¨å›°æƒ‘åº¦å·®å¼‚å¼•å¯¼æ¨¡å‹ç”Ÿæˆç®€æ´å‡†ç¡®çš„ç­”æ¡ˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSARèƒ½æ˜¾è‘—æå‡æ¨ç†å‡†ç¡®ç‡å¹¶é™ä½æ¨ç†æˆæœ¬ï¼Œå®ç°æ•ˆç‡ä¸å‡†ç¡®ç‡çš„å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªå¯¹é½å¥–åŠ±(SAR)æœºåˆ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç°æœ‰åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸æä¾›ç²—ç²’åº¦çš„äºŒå…ƒåé¦ˆï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹å†—é•¿å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼ŒåŒæ—¶ç°æœ‰è§£å†³æ–¹æ¡ˆåˆå¯èƒ½ç‰ºç‰²å‡†ç¡®æ€§ã€‚SARä½œä¸ºä¸€ç§è‡ªå¼•å¯¼ä¿¡å·ï¼Œè¡¥å……å¯éªŒè¯å¥–åŠ±ï¼Œé¼“åŠ±æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚SARå®šä¹‰ä¸ºç­”æ¡ˆåœ¨ç»™å®šæŸ¥è¯¢æ¡ä»¶ä¸‹çš„å›°æƒ‘åº¦ä¸ç‹¬ç«‹ç­”æ¡ˆå›°æƒ‘åº¦ä¹‹é—´çš„ç›¸å¯¹å·®å¼‚ï¼Œä»è€Œå€¾å‘äºç®€æ´ä¸”ç‰¹å®šäºæŸ¥è¯¢çš„å“åº”ã€‚å®šé‡åˆ†æè¡¨æ˜ï¼ŒSARèƒ½å¤Ÿå¯é åœ°åŒºåˆ†ç­”æ¡ˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†SARä¸PPOå’ŒGRPOç­‰ä¸»æµå¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸ç»“åˆï¼Œå¯å°†å‡†ç¡®ç‡æé«˜4%ï¼ŒåŒæ—¶é™ä½30%çš„æ¨ç†æˆæœ¬ã€‚SARåœ¨æ­£ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†å¸•ç´¯æ‰˜æœ€ä¼˜çš„æƒè¡¡ï¼Œå¹¶åœ¨ä¿ç•™é«˜çº§æ¨ç†è¡Œä¸ºçš„åŒæ—¶ç¼©çŸ­äº†å“åº”ï¼Œè¯æ˜äº†å…¶åœ¨æŠ‘åˆ¶ä¸å¿…è¦é˜è¿°æ–¹é¢çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒLLMè¿›è¡Œæ¨ç†æ—¶ï¼Œé€šå¸¸åªèƒ½æä¾›äºŒå…ƒçš„æ­£ç¡®æˆ–é”™è¯¯åé¦ˆï¼Œè¿™ç§ç²—ç²’åº¦çš„ä¿¡å·å¯¼è‡´æ¨¡å‹å€¾å‘äºç”Ÿæˆå†—é•¿ä¸”è®¡ç®—æˆæœ¬é«˜çš„æ¨ç†è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œä¸€äº›æ—¨åœ¨æé«˜æ•ˆç‡çš„æ–¹æ³•åˆå¯èƒ½ç‰ºç‰²æ¨ç†çš„å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§æ—¢èƒ½ä¿è¯æ¨ç†å‡†ç¡®æ€§ï¼Œåˆèƒ½æé«˜æ¨ç†æ•ˆç‡çš„å¥–åŠ±æœºåˆ¶æ˜¯æœ¬æ–‡è¦è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¯­è¨€æ¨¡å‹è‡ªèº«çš„ç‰¹æ€§ï¼Œè®¾è®¡ä¸€ç§è‡ªå¯¹é½çš„å¥–åŠ±ä¿¡å·(SAR)ã€‚SARåŸºäºä¸€ä¸ªå‡è®¾ï¼šå¥½çš„ç­”æ¡ˆåº”è¯¥æ—¢å‡†ç¡®åˆç®€æ´ï¼Œå¹¶ä¸”ä¸ç»™å®šçš„é—®é¢˜å¯†åˆ‡ç›¸å…³ã€‚å› æ­¤ï¼ŒSARé€šè¿‡æ¯”è¾ƒåœ¨ç»™å®šé—®é¢˜çš„æƒ…å†µä¸‹ç”Ÿæˆç­”æ¡ˆçš„å›°æƒ‘åº¦ä¸ç‹¬ç«‹ç”Ÿæˆç­”æ¡ˆçš„å›°æƒ‘åº¦ä¹‹é—´çš„å·®å¼‚æ¥è¡¡é‡ç­”æ¡ˆçš„è´¨é‡ã€‚å¦‚æœç­”æ¡ˆç®€æ´ä¸”ä¸é—®é¢˜ç›¸å…³ï¼Œé‚£ä¹ˆåœ¨ç»™å®šé—®é¢˜çš„æƒ…å†µä¸‹ç”Ÿæˆç­”æ¡ˆçš„å›°æƒ‘åº¦åº”è¯¥æ›´ä½ï¼ŒSARå€¼æ›´é«˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶æ˜¯åœ¨ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹ä¸­ï¼Œå°†SARä½œä¸ºä¸€ç§é¢å¤–çš„å¥–åŠ±ä¿¡å·åŠ å…¥åˆ°æ€»å¥–åŠ±ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹é¦–å…ˆæ ¹æ®ç»™å®šçš„é—®é¢˜ç”Ÿæˆç­”æ¡ˆï¼Œç„¶åè®¡ç®—SARå€¼ã€‚SARå€¼ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆä¾‹å¦‚ï¼Œç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼‰ç»“åˆï¼Œä½œä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å¥–åŠ±ä¿¡å·ï¼Œç”¨äºæ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚æœ¬æ–‡ä¸»è¦ä½¿ç”¨äº†PPOå’ŒGRPOä¸¤ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶å°†SARä¸è¿™ä¸¤ç§ç®—æ³•ç»“åˆä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†è‡ªå¯¹é½å¥–åŠ±(SAR)çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºLLMçš„æ¨ç†è®­ç»ƒä¸­ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè§„åˆ™æˆ–äººå·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ä¸åŒï¼ŒSARæ˜¯ä¸€ç§è‡ªå¼•å¯¼çš„å¥–åŠ±ä¿¡å·ï¼Œå®ƒåˆ©ç”¨äº†è¯­è¨€æ¨¡å‹è‡ªèº«çš„å›°æƒ‘åº¦ä¿¡æ¯æ¥è¡¡é‡ç­”æ¡ˆçš„è´¨é‡ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„äººå·¥æ ‡æ³¨æˆ–è§„åˆ™è®¾è®¡ï¼Œå¯ä»¥æ›´æ–¹ä¾¿åœ°åº”ç”¨äºä¸åŒçš„æ¨ç†ä»»åŠ¡å’Œæ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šSARçš„å…³é”®è®¾è®¡åœ¨äºå›°æƒ‘åº¦çš„è®¡ç®—æ–¹å¼ã€‚å…·ä½“æ¥è¯´ï¼ŒSARå®šä¹‰ä¸ºï¼šSAR = perplexity(answer) - perplexity(answer | query)ï¼Œå…¶ä¸­perplexity(answer)è¡¨ç¤ºç‹¬ç«‹ç”Ÿæˆç­”æ¡ˆçš„å›°æƒ‘åº¦ï¼Œperplexity(answer | query)è¡¨ç¤ºåœ¨ç»™å®šé—®é¢˜çš„æƒ…å†µä¸‹ç”Ÿæˆç­”æ¡ˆçš„å›°æƒ‘åº¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSARèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¡é‡ç­”æ¡ˆçš„ç®€æ´æ€§å’Œç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢ç´¢äº†ä¸åŒçš„SARæƒé‡ï¼Œä»¥å¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°†SARä¸PPOå’ŒGRPOç­‰ä¸»æµå¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸ç»“åˆï¼Œåœ¨7ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯ä»¥å°†å‡†ç¡®ç‡å¹³å‡æé«˜4%ï¼ŒåŒæ—¶é™ä½30%çš„æ¨ç†æˆæœ¬ã€‚æ­¤å¤–ï¼ŒSARåœ¨æ­£ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†å¸•ç´¯æ‰˜æœ€ä¼˜çš„æƒè¡¡ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨ä¿ç•™é«˜çº§æ¨ç†è¡Œä¸ºçš„åŒæ—¶ç¼©çŸ­å“åº”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦LLMè¿›è¡Œæ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡å¼•å…¥SARï¼Œå¯ä»¥è®­ç»ƒå‡ºæ›´åŠ é«˜æ•ˆå’Œå‡†ç¡®çš„æ¨ç†æ¨¡å‹ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶æå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œå¯ä»¥å°†SARä¸å…¶ä»–å¥–åŠ±ä¿¡å·ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning with verifiable rewards has significantly advanced reasoning in large language models (LLMs), but such signals remain coarse, offering only binary correctness feedback. This limitation often results in inefficiencies, including overly verbose reasoning and high computational cost, while existing solutions often compromise accuracy. To address this, we introduce self-aligned reward (SAR), a self-guided signal that complements verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is defined as the relative perplexity difference between an answer conditioned on the query and the standalone answer, thereby favoring responses that are concise and query-specific. Quantitative analysis reveals that SAR reliably distinguishes answer quality: concise, correct answers score higher than redundant ones, and partially correct answers score higher than entirely incorrect ones. Evaluation on 4 models across 7 benchmarks shows that integrating SAR with prevalent RL algorithms like PPO and GRPO improves accuracy by 4%, while reducing inference cost by 30%. Further analysis demonstrates that SAR achieves a Pareto-optimal trade-off between correctness and efficiency compared to reward signals based on length or self-confidence. We also show that SAR shortens responses while preserving advanced reasoning behaviors, demonstrating its ability to suppress unnecessary elaboration without losing critical reasoning. These results highlight the promise of self-aligned reward as a fine-grained complement to verifiable rewards, paving the way for more efficient and effective LLM training.

