---
layout: default
title: Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest
---

# Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05292" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05292v1</a>
  <a href="https://arxiv.org/pdf/2509.05292.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05292v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05292v1', 'Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiao Yang, Mehdi Ben Ayed, Longyu Zhao, Fan Zhou, Yuchen Shen, Abe Engle, Jinfeng Zhuang, Ling Leng, Jiajing Xu, Charles Rosenberg, Prathibha Deshikachar

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRL-PUTæ¡†æ¶ï¼Œåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–Pinterestå¹¿å‘Šæ¨èç³»ç»Ÿä¸­æ’åºæ•ˆç”¨å‡½æ•°ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `å¹¿å‘Šæ¨èç³»ç»Ÿ` `æ•ˆç”¨å‡½æ•°ä¼˜åŒ–` `å¤šç›®æ ‡ä¼˜åŒ–` `åœ¨çº¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¹¿å‘Šæ¨èç³»ç»Ÿä¸­çš„æ•ˆç”¨å‡½æ•°æ‰‹åŠ¨è°ƒæ•´æ–¹æ³•å­˜åœ¨ç›®æ ‡ä¸æ˜ç¡®ã€å‚æ•°ç»„åˆçˆ†ç‚¸ä»¥åŠç¼ºä¹ä¸ªæ€§åŒ–ç­‰é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºDRL-PUTæ¡†æ¶ï¼Œåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç›´æ¥ä»åœ¨çº¿æ—¥å¿—ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œé¿å…äº†å€¼å‡½æ•°ä¼°è®¡çš„å›°éš¾ã€‚
3. åœ¨çº¿A/Bå®éªŒè¡¨æ˜ï¼ŒDRL-PUTæ˜¾è‘—æå‡äº†ç‚¹å‡»ç‡å’Œé•¿æœŸç‚¹å‡»ç‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä¸ªæ€§åŒ–æ•ˆç”¨è°ƒæ•´ï¼ˆDRL-PUTï¼‰ï¼Œä»¥è§£å†³å¹¿å‘Šæ¨èç³»ç»Ÿä¸­å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ã€‚å¹¿å‘Šæ¨èç³»ç»Ÿä¸­çš„æ’åºæ•ˆç”¨å‡½æ•°çº¿æ€§åœ°ç»“åˆäº†å„ç§ä¸šåŠ¡ç›®æ ‡çš„é¢„æµ‹ï¼Œåœ¨å¹³è¡¡å¹³å°ã€å¹¿å‘Šå•†å’Œç”¨æˆ·ä¹‹é—´çš„ä»·å€¼æ–¹é¢èµ·ç€æ ¸å¿ƒä½œç”¨ã€‚ä¼ ç»Ÿçš„æ‰‹åŠ¨è°ƒæ•´è™½ç„¶ç®€å•ä¸”æ˜“äºè§£é‡Šï¼Œä½†ç”±äºå…¶ä¸åˆç†çš„è°ƒæ•´ç›®æ ‡ã€å¤§é‡çš„å‚æ•°ç»„åˆä»¥åŠç¼ºä¹ä¸ªæ€§åŒ–å’Œå¯¹å­£èŠ‚æ€§çš„é€‚åº”æ€§ï¼Œé€šå¸¸ä¼šäº§ç”Ÿæ¬¡ä¼˜ç»“æœã€‚è¯¥æ¡†æ¶å°†é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼šç»™å®šå¹¿å‘Šè¯·æ±‚çš„çŠ¶æ€ï¼Œé¢„æµ‹æœ€ä¼˜è¶…å‚æ•°ä»¥æœ€å¤§åŒ–é¢„å®šä¹‰çš„å¥–åŠ±ã€‚è¯¥æ–¹æ³•ç›´æ¥ä½¿ç”¨åœ¨çº¿æœåŠ¡æ—¥å¿—å­¦ä¹ æœ€ä¼˜ç­–ç•¥æ¨¡å‹ï¼Œé¿å…äº†ä¼°è®¡å€¼å‡½æ•°çš„éœ€æ±‚ï¼Œå› ä¸ºå€¼å‡½æ•°çš„ä¼°è®¡æœ¬è´¨ä¸Šå…·æœ‰é«˜æ–¹å·®å’Œä¸å¹³è¡¡çš„å³æ—¶å¥–åŠ±åˆ†å¸ƒã€‚åœ¨Pinterestçš„å¹¿å‘Šæ¨èç³»ç»Ÿä¸­è¿›è¡Œçš„åœ¨çº¿A/Bå®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸åŸºçº¿æ‰‹åŠ¨æ•ˆç”¨è°ƒæ•´æ–¹æ³•ç›¸æ¯”ï¼ŒDRL-PUTä½¿ç‚¹å‡»ç‡æé«˜äº†9.7ï¼…ï¼Œé•¿æœŸç‚¹å‡»ç‡æé«˜äº†7.7ï¼…ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹ä¸åŒå¥–åŠ±å®šä¹‰çš„å½±å“è¿›è¡Œäº†è¯¦ç»†çš„æ¶ˆèç ”ç©¶ï¼Œå¹¶åˆ†æäº†å­¦ä¹ ç­–ç•¥æ¨¡å‹çš„ä¸ªæ€§åŒ–æ–¹é¢ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¹¿å‘Šæ¨èç³»ç»Ÿä¸­æ’åºæ•ˆç”¨å‡½æ•°çš„æ‰‹åŠ¨è°ƒæ•´é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äººå·¥ç»éªŒï¼Œéš¾ä»¥åº”å¯¹å¤æ‚çš„å¤šç›®æ ‡ä¼˜åŒ–ï¼Œä¸”æ— æ³•æ ¹æ®ç”¨æˆ·è¡Œä¸ºå’Œç¯å¢ƒå˜åŒ–è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•æœ‰æ•ˆå¹³è¡¡å¹³å°ã€å¹¿å‘Šå•†å’Œç”¨æˆ·ä¹‹é—´çš„åˆ©ç›Šï¼Œå¯¼è‡´æ¨èæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ•ˆç”¨å‡½æ•°è°ƒæ•´é—®é¢˜å»ºæ¨¡æˆä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡å°†å¹¿å‘Šè¯·æ±‚çš„çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå­¦ä¹ ä¸€ä¸ªç­–ç•¥æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹æœ€ä¼˜çš„è¶…å‚æ•°ï¼Œä»è€Œæœ€å¤§åŒ–é¢„å®šä¹‰çš„å¥–åŠ±å‡½æ•°ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶æ ¹æ®ç”¨æˆ·è¡Œä¸ºå’Œç¯å¢ƒå˜åŒ–è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDRL-PUTæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) çŠ¶æ€è¡¨ç¤ºæ¨¡å—ï¼šå°†å¹¿å‘Šè¯·æ±‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚ç”¨æˆ·ç”»åƒã€å¹¿å‘Šç‰¹å¾ç­‰ï¼‰ç¼–ç æˆçŠ¶æ€å‘é‡ã€‚2) ç­–ç•¥ç½‘ç»œæ¨¡å—ï¼šä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå­¦ä¹ ä¸€ä¸ªç­–ç•¥æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥çŠ¶æ€å‘é‡ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºæœ€ä¼˜çš„è¶…å‚æ•°ã€‚3) å¥–åŠ±å‡½æ•°æ¨¡å—ï¼šå®šä¹‰ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œç”¨äºè¡¡é‡æ¨èæ•ˆæœçš„å¥½åã€‚4) è®­ç»ƒæ¨¡å—ï¼šä½¿ç”¨åœ¨çº¿æœåŠ¡æ—¥å¿—è®­ç»ƒç­–ç•¥ç½‘ç»œï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç›´æ¥ä»åœ¨çº¿æœåŠ¡æ—¥å¿—ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥æ¨¡å‹ï¼Œé¿å…äº†ä¼°è®¡å€¼å‡½æ•°çš„éœ€æ±‚ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦ä¼°è®¡å€¼å‡½æ•°ï¼Œä½†ç”±äºå¹¿å‘Šæ¨èç³»ç»Ÿä¸­çš„å¥–åŠ±å…·æœ‰é«˜æ–¹å·®å’Œä¸å¹³è¡¡çš„åˆ†å¸ƒï¼Œå¯¼è‡´å€¼å‡½æ•°ä¼°è®¡éå¸¸å›°éš¾ã€‚é€šè¿‡ç›´æ¥å­¦ä¹ ç­–ç•¥æ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œä½œä¸ºç­–ç•¥ç½‘ç»œï¼Œå¹¶é‡‡ç”¨ç­–ç•¥æ¢¯åº¦ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘ç‚¹å‡»ç‡ã€é•¿æœŸç‚¹å‡»ç‡ç­‰å¤šä¸ªæŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹ä¸åŒçš„å¥–åŠ±å‡½æ•°è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œä»¥è¯„ä¼°å…¶å¯¹æ¨èæ•ˆæœçš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨çº¿A/Bå®éªŒç»“æœè¡¨æ˜ï¼ŒDRL-PUTæ¡†æ¶åœ¨Pinterestçš„å¹¿å‘Šæ¨èç³»ç»Ÿä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸åŸºçº¿æ‰‹åŠ¨æ•ˆç”¨è°ƒæ•´æ–¹æ³•ç›¸æ¯”ï¼ŒDRL-PUTä½¿ç‚¹å‡»ç‡æé«˜äº†9.7ï¼…ï¼Œé•¿æœŸç‚¹å‡»ç‡æé«˜äº†7.7ï¼…ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDRL-PUTèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¼˜åŒ–æ’åºæ•ˆç”¨å‡½æ•°ï¼Œå¹¶æå‡æ¨èæ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§åœ¨çº¿å¹¿å‘Šæ¨èç³»ç»Ÿï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¹³è¡¡å¤šä¸ªä¸šåŠ¡ç›®æ ‡çš„åœºæ™¯ä¸‹ã€‚é€šè¿‡è‡ªåŠ¨ä¼˜åŒ–æ’åºæ•ˆç”¨å‡½æ•°ï¼Œå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒã€å¢åŠ å¹¿å‘Šæ”¶å…¥ï¼Œå¹¶æé«˜å¹³å°çš„æ•´ä½“ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æ¨èåœºæ™¯ï¼Œå¦‚å•†å“æ¨èã€å†…å®¹æ¨èç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The ranking utility function in an ad recommender system, which linearly combines predictions of various business goals, plays a central role in balancing values across the platform, advertisers, and users. Traditional manual tuning, while offering simplicity and interpretability, often yields suboptimal results due to its unprincipled tuning objectives, the vast amount of parameter combinations, and its lack of personalization and adaptability to seasonality. In this work, we propose a general Deep Reinforcement Learning framework for Personalized Utility Tuning (DRL-PUT) to address the challenges of multi-objective optimization within ad recommender systems. Our key contributions include: 1) Formulating the problem as a reinforcement learning task: given the state of an ad request, we predict the optimal hyperparameters to maximize a pre-defined reward. 2) Developing an approach to directly learn an optimal policy model using online serving logs, avoiding the need to estimate a value function, which is inherently challenging due to the high variance and unbalanced distribution of immediate rewards. We evaluated DRL-PUT through an online A/B experiment in Pinterest's ad recommender system. Compared to the baseline manual utility tuning approach, DRL-PUT improved the click-through rate by 9.7% and the long click-through rate by 7.7% on the treated segment. We conducted a detailed ablation study on the impact of different reward definitions and analyzed the personalization aspect of the learned policy model.

