---
layout: default
title: On Using Large-Batches in Federated Learning
---

# On Using Large-Batches in Federated Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10537" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10537v1</a>
  <a href="https://arxiv.org/pdf/2509.10537.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10537v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10537v1', 'On Using Large-Batches in Federated Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sahil Tyagi

**åˆ†ç±»**: cs.LG, cs.AI, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢ç´¢è”é‚¦å­¦ä¹ ä¸­å¤§æ‰¹é‡è®­ç»ƒçš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜ï¼Œæå‡æ¨¡å‹æ³›åŒ–æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `å¤§æ‰¹é‡è®­ç»ƒ` `å°æ‰¹é‡è®­ç»ƒ` `æ¨¡å‹æ³›åŒ–` `åˆ†å¸ƒå¼è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è”é‚¦å­¦ä¹ é¢ä¸´å¹¶è¡Œæ•ˆç‡ä¸æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æƒè¡¡é—®é¢˜ï¼Œå¤§æ‰¹é‡è®­ç»ƒè™½ç„¶åŠ é€Ÿè®­ç»ƒï¼Œä½†å¯èƒ½å¯¼è‡´æ³›åŒ–æ€§èƒ½ä¸‹é™ã€‚
2. è¯¥ç ”ç©¶æ—¨åœ¨æ¢ç´¢è”é‚¦å­¦ä¹ ä¸­å¤§å°æ‰¹é‡è®­ç»ƒçš„æŠ˜è¡·æ–¹æ¡ˆï¼Œä»¥å…¼é¡¾å¤§æ‰¹é‡çš„å¹¶è¡Œæ‰©å±•æ€§å’Œå°æ‰¹é‡çš„è‰¯å¥½æ³›åŒ–æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ResNet50å’ŒVGG11æ¨¡å‹ä¸Šï¼Œç›¸æ¯”å°æ‰¹é‡è®­ç»ƒï¼Œæµ‹è¯•å‡†ç¡®ç‡åˆ†åˆ«æå‡äº†32.33%å’Œ3.74%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜æ•ˆçš„è”é‚¦å­¦ä¹ (FL)å¯¹äºåœ¨è®¡ç®—èµ„æºå’Œç½‘ç»œå—é™çš„è®¾å¤‡ä¸Šè®­ç»ƒæ·±åº¦ç½‘ç»œè‡³å…³é‡è¦ã€‚éšç€å¤§æ•°æ®æ—¶ä»£çš„åˆ°æ¥ï¼Œè®¾å¤‡ç”Ÿæˆæˆ–æ”¶é›†å¤šæ¨¡æ€æ•°æ®ï¼Œç”¨äºè®­ç»ƒé€šç”¨æˆ–å±€éƒ¨ä¸Šä¸‹æ–‡ç›¸å…³çš„ç½‘ç»œï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®éšç§å’Œæœ¬åœ°æ€§è‡³å…³é‡è¦æ—¶ã€‚è”é‚¦å­¦ä¹ ç®—æ³•é€šå¸¸åœ¨å¹¶è¡Œæ€§å’Œç»Ÿè®¡æ€§èƒ½ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œä»¥æé«˜é€šä¿¡é¢‘ç‡ä¸ºä»£ä»·æ¥æ”¹å–„æ¨¡å‹è´¨é‡ï¼Œåä¹‹äº¦ç„¶ã€‚åœ¨é¢‘ç¹åŒæ­¥è®¾ç½®ä¸‹ï¼Œè”é‚¦å­¦ä¹ é€šè¿‡å¤„ç†æ›´å¤§çš„å…¨å±€æ‰¹é‡å¤§å°ï¼Œä»è€Œåœ¨æ¯ä¸ªè®­ç»ƒè¿­ä»£ä¸­æ‰§è¡Œæ›´å¤šå·¥ä½œï¼Œä»è€Œè·å¾—å¯è§‚çš„è®­ç»ƒåŠ é€Ÿã€‚ç„¶è€Œï¼Œç”±äºä¸å¤§æ‰¹é‡è®­ç»ƒç›¸å…³çš„æ³›åŒ–é€€åŒ–é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´è¾ƒå·®çš„æµ‹è¯•æ€§èƒ½ï¼ˆå³ï¼Œè¾ƒä½çš„æµ‹è¯•æŸå¤±æˆ–å‡†ç¡®æ€§ï¼‰ã€‚ä¸ºäº†è§£å†³å¤§æ‰¹é‡å¸¦æ¥çš„è¿™äº›æŒ‘æˆ˜ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†æˆ‘ä»¬å¯¹åˆ©ç”¨å°æ‰¹é‡å’Œå¤§æ‰¹é‡è®­ç»ƒä¹‹é—´æƒè¡¡çš„æ„¿æ™¯ï¼Œå¹¶æ¢ç´¢æ–°çš„æ–¹å‘ï¼Œä»¥åŒæ—¶äº«å—å¤§æ‰¹é‡çš„å¹¶è¡Œæ‰©å±•å’Œå°æ‰¹é‡è®­ç»ƒçš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚å¯¹äºç›¸åŒçš„è¿­ä»£æ¬¡æ•°ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæˆ‘ä»¬æå‡ºçš„åœ¨å¤§æ‰¹é‡è®­ç»ƒæŠ€æœ¯åœ¨ResNet50å’ŒVGG11æ¨¡å‹ä¸­åˆ†åˆ«æ¯”å°æ‰¹é‡è®­ç»ƒè·å¾—äº†çº¦32.33%å’Œ3.74%çš„æ›´é«˜çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè”é‚¦å­¦ä¹ æ—¨åœ¨ä¿æŠ¤ç”¨æˆ·æ•°æ®éšç§çš„å‰æä¸‹ï¼Œåˆ©ç”¨åˆ†å¸ƒå¼è®¾å¤‡ä¸Šçš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤§æ‰¹é‡è¿›è¡Œè®­ç»ƒè™½ç„¶å¯ä»¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œä½†ä¼šæŸå®³æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ³›åŒ–æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ¢ç´¢å¤§å°æ‰¹é‡è®­ç»ƒä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡æŸç§ç­–ç•¥ï¼Œåœ¨è”é‚¦å­¦ä¹ è¿‡ç¨‹ä¸­åŠ¨æ€åœ°è°ƒæ•´æ‰¹é‡å¤§å°ï¼Œæˆ–è€…ç»“åˆå¤§å°æ‰¹é‡è®­ç»ƒçš„ä¼˜åŠ¿ï¼Œä»è€Œåœ¨ä¿è¯è®­ç»ƒæ•ˆç‡çš„åŒæ—¶ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“ç­–ç•¥æœªçŸ¥ï¼Œéœ€è¦è¿›ä¸€æ­¥é˜…è¯»è®ºæ–‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå¤§å°æ‰¹é‡æƒè¡¡çš„è”é‚¦å­¦ä¹ è®­ç»ƒæ¡†æ¶ã€‚å…·ä½“æ¡†æ¶ç»†èŠ‚æœªçŸ¥ï¼Œä½†å¯ä»¥æ¨æµ‹å¯èƒ½åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼šå®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒæ¨¡å—ï¼ˆè´Ÿè´£åœ¨è®¾å¤‡ä¸Šè¿›è¡Œæœ¬åœ°è®­ç»ƒï¼‰ï¼ŒæœåŠ¡å™¨èšåˆæ¨¡å—ï¼ˆè´Ÿè´£æ”¶é›†å’Œèšåˆå®¢æˆ·ç«¯çš„æ¨¡å‹æ›´æ–°ï¼‰ï¼Œä»¥åŠæ‰¹é‡å¤§å°è°ƒæ•´ç­–ç•¥æ¨¡å—ï¼ˆè´Ÿè´£æ ¹æ®æŸç§æŒ‡æ ‡åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åˆ©ç”¨å¤§å°æ‰¹é‡è®­ç»ƒä¹‹é—´æƒè¡¡çš„è”é‚¦å­¦ä¹ è®­ç»ƒæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šæ‰¹é‡å¤§å°çš„è”é‚¦å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ³›åŒ–æ€§èƒ½ã€‚å…·ä½“çš„åˆ›æ–°ç‚¹åœ¨äºå¦‚ä½•è®¾è®¡æœ‰æ•ˆçš„æ‰¹é‡å¤§å°è°ƒæ•´ç­–ç•¥ï¼Œä»¥åŠå¦‚ä½•å°†å¤§å°æ‰¹é‡è®­ç»ƒçš„ä¼˜åŠ¿ç»“åˆèµ·æ¥ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åœ¨äºæ‰¹é‡å¤§å°è°ƒæ•´ç­–ç•¥ã€‚å…·ä½“çš„ç­–ç•¥ç»†èŠ‚æœªçŸ¥ï¼Œä½†å¯èƒ½æ¶‰åŠåˆ°ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1ï¼‰æ ¹æ®å®¢æˆ·ç«¯çš„æ•°æ®é‡å’Œè®¡ç®—èµ„æºåŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°ï¼›2ï¼‰æ ¹æ®æ¨¡å‹çš„è®­ç»ƒçŠ¶æ€ï¼ˆå¦‚æŸå¤±å‡½æ•°çš„å˜åŒ–ï¼‰åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°ï¼›3ï¼‰é‡‡ç”¨æŸç§æ··åˆç­–ç•¥ï¼ŒåŒæ—¶ä½¿ç”¨å¤§å°æ‰¹é‡è¿›è¡Œè®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è®ºæ–‡æå‡ºçš„æ–¹æ³•åœ¨ResNet50å’ŒVGG11æ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“æ¥è¯´ï¼Œä¸å°æ‰¹é‡è®­ç»ƒç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ResNet50æ¨¡å‹ä¸Šè·å¾—äº†32.33%çš„æµ‹è¯•å‡†ç¡®ç‡æå‡ï¼Œåœ¨VGG11æ¨¡å‹ä¸Šè·å¾—äº†3.74%çš„æµ‹è¯•å‡†ç¡®ç‡æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å¤§æ‰¹é‡è®­ç»ƒå¸¦æ¥çš„æ³›åŒ–æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä¿æŠ¤ç”¨æˆ·æ•°æ®éšç§çš„è”é‚¦å­¦ä¹ åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„ä¸ªæ€§åŒ–æ¨èã€åŒ»ç–—é¢†åŸŸçš„ç–¾ç—…è¯Šæ–­ã€é‡‘èé¢†åŸŸçš„é£é™©è¯„ä¼°ç­‰ã€‚é€šè¿‡æå‡è”é‚¦å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœºæ™¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä»è€Œä¸ºç”¨æˆ·å¸¦æ¥æ›´å¥½çš„ä½“éªŒå’Œä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient Federated learning (FL) is crucial for training deep networks over devices with limited compute resources and bounded networks. With the advent of big data, devices either generate or collect multimodal data to train either generic or local-context aware networks, particularly when data privacy and locality is vital. FL algorithms generally trade-off between parallel and statistical performance, improving model quality at the cost of higher communication frequency, or vice versa. Under frequent synchronization settings, FL over a large cluster of devices may perform more work per-training iteration by processing a larger global batch-size, thus attaining considerable training speedup. However, this may result in poor test performance (i.e., low test loss or accuracy) due to generalization degradation issues associated with large-batch training. To address these challenges with large-batches, this work proposes our vision of exploiting the trade-offs between small and large-batch training, and explore new directions to enjoy both the parallel scaling of large-batches and good generalizability of small-batch training. For the same number of iterations, we observe that our proposed large-batch training technique attains about 32.33% and 3.74% higher test accuracy than small-batch training in ResNet50 and VGG11 models respectively.

