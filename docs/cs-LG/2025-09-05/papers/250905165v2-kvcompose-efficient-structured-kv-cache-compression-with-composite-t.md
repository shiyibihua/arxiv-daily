---
layout: default
title: KVCompose: Efficient Structured KV Cache Compression with Composite Tokens
---

# KVCompose: Efficient Structured KV Cache Compression with Composite Tokens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05165" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05165v2</a>
  <a href="https://arxiv.org/pdf/2509.05165.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05165v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05165v2', 'KVCompose: Efficient Structured KV Cache Compression with Composite Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dmitry Akulov, Mohamed Sana, Antonio De Domenico, Tareq Si Salem, Nicola Piovesan, Fadhel Ayed

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-09-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**KVComposeï¼šåˆ©ç”¨å¤åˆTokenå®ç°é«˜æ•ˆç»“æ„åŒ–KVç¼“å­˜å‹ç¼©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `KVç¼“å­˜å‹ç¼©` `é•¿æ–‡æœ¬æ¨ç†` `æ³¨æ„åŠ›æœºåˆ¶` `å¤åˆToken` `å±‚è‡ªé€‚åº”` `å¤§å‹è¯­è¨€æ¨¡å‹` `å†…å­˜ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿æ–‡æœ¬LLMæ¨ç†ä¸­ï¼ŒKVç¼“å­˜éšä¸Šä¸‹æ–‡å¢é•¿è¿…é€Ÿï¼Œæˆä¸ºå†…å­˜ç“¶é¢ˆï¼Œç°æœ‰å‹ç¼©æ–¹æ³•å­˜åœ¨å¯å‘å¼è§„åˆ™åƒµåŒ–ã€ç ´åå¼ é‡ç»“æ„æˆ–ä¾èµ–ä¸“ç”¨è®¡ç®—å†…æ ¸ç­‰é—®é¢˜ã€‚
2. KVComposeé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼ï¼Œè‡ªé€‚åº”åœ°ä¸ºæ¯ä¸€å±‚é€‰æ‹©é‡è¦çš„Tokenï¼Œå¹¶å°†å®ƒä»¬ç»„åˆæˆå¤åˆTokenï¼Œä»è€Œåœ¨å‹ç¼©KVç¼“å­˜çš„åŒæ—¶ä¿æŒåŸæœ‰ç¼“å­˜ç»“æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒKVComposeåœ¨æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹ç²¾åº¦ï¼Œå¹¶ä¸”å…¼å®¹ç°æœ‰æ¨ç†æµç¨‹ï¼Œä¼˜äºå…¶ä»–ç»“æ„åŒ–å’ŒåŠç»“æ„åŒ–å‹ç¼©æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾èµ–äºé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ä»¥å®ç°é«˜æ•ˆçš„è‡ªå›å½’è§£ç ï¼›ç„¶è€Œï¼Œç¼“å­˜å¤§å°éšä¸Šä¸‹æ–‡é•¿åº¦å’Œæ¨¡å‹æ·±åº¦çº¿æ€§å¢é•¿ï¼Œæˆä¸ºé•¿ä¸Šä¸‹æ–‡æ¨ç†çš„ä¸»è¦ç“¶é¢ˆã€‚ç°æœ‰çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•è¦ä¹ˆå¼ºåˆ¶æ‰§è¡Œä¸¥æ ¼çš„å¯å‘å¼æ–¹æ³•ï¼Œè¦ä¹ˆé€šè¿‡æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„å¯å˜æ€§æ¥ç ´åå¼ é‡å¸ƒå±€ï¼Œè¦ä¹ˆéœ€è¦ä¸“é—¨çš„è®¡ç®—å†…æ ¸ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„KVç¼“å­˜å‹ç¼©æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºæ³¨æ„åŠ›å¼•å¯¼çš„ã€å±‚è‡ªé€‚åº”çš„å¤åˆTokenã€‚æˆ‘ä»¬çš„æ–¹æ³•èšåˆæ³¨æ„åŠ›åˆ†æ•°ä»¥ä¼°è®¡Tokençš„é‡è¦æ€§ï¼Œç‹¬ç«‹åœ°é€‰æ‹©ç‰¹å®šå¤´çš„Tokenï¼Œå¹¶å°†å®ƒä»¬å¯¹é½åˆ°ç¬¦åˆç°æœ‰æ¨ç†å¼•æ“æ‰€éœ€çš„ç»Ÿä¸€ç¼“å­˜ç»“æ„çš„å¤åˆTokenä¸­ã€‚å…¨å±€åˆ†é…æœºåˆ¶è¿›ä¸€æ­¥è°ƒæ•´è·¨å±‚çš„ä¿ç•™é¢„ç®—ï¼Œä¸ºå…·æœ‰ä¿¡æ¯é‡Tokençš„å±‚åˆ†é…æ›´å¤šå®¹é‡ã€‚è¿™ç§æ–¹æ³•åœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„å†…å­˜å‡å°‘ï¼Œå§‹ç»ˆä¼˜äºå…ˆå‰çš„ç»“æ„åŒ–å’ŒåŠç»“æ„åŒ–æ–¹æ³•ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æ ‡å‡†æ¨ç†ç®¡é“å®Œå…¨å…¼å®¹ï¼Œä¸ºé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡LLMéƒ¨ç½²æä¾›äº†ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†ä¸­ï¼ŒKVç¼“å­˜å ç”¨å¤§é‡å†…å­˜çš„é—®é¢˜ã€‚ç°æœ‰KVç¼“å­˜å‹ç¼©æ–¹æ³•å­˜åœ¨ä»¥ä¸‹ç—›ç‚¹ï¼šä¸€æ˜¯ä¾èµ–äºå›ºå®šçš„å¯å‘å¼è§„åˆ™ï¼Œç¼ºä¹çµæ´»æ€§ï¼›äºŒæ˜¯å¯èƒ½ç ´åå¼ é‡å¸ƒå±€ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä¸‹é™ï¼›ä¸‰æ˜¯éœ€è¦å®šåˆ¶åŒ–çš„è®¡ç®—å†…æ ¸ï¼Œå¢åŠ äº†éƒ¨ç½²éš¾åº¦ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†LLMåœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šKVComposeçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥è¯„ä¼°æ¯ä¸ªTokençš„é‡è¦æ€§ï¼Œå¹¶æ ¹æ®é‡è¦æ€§é€‰æ‹©æ€§åœ°ä¿ç•™Tokenã€‚é€šè¿‡å°†å¤šä¸ªTokenç»„åˆæˆå¤åˆTokenï¼Œå¯ä»¥åœ¨å‡å°‘KVç¼“å­˜å¤§å°çš„åŒæ—¶ï¼Œä¿æŒç¼“å­˜çš„ç»“æ„åŒ–ï¼Œä»è€Œå…¼å®¹ç°æœ‰çš„æ¨ç†å¼•æ“ã€‚æ­¤å¤–ï¼ŒKVComposeè¿˜é‡‡ç”¨å±‚è‡ªé€‚åº”çš„ç­–ç•¥ï¼Œä¸ºä¸åŒçš„å±‚åˆ†é…ä¸åŒçš„ä¿ç•™é¢„ç®—ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–å‹ç¼©æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKVComposeçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) **æ³¨æ„åŠ›åˆ†æ•°èšåˆ**ï¼šè®¡ç®—æ¯ä¸ªTokençš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œä½œä¸ºå…¶é‡è¦æ€§çš„åº¦é‡ã€‚2) **Tokené€‰æ‹©**ï¼šæ ¹æ®æ³¨æ„åŠ›åˆ†æ•°ï¼Œä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´ç‹¬ç«‹åœ°é€‰æ‹©é‡è¦çš„Tokenã€‚3) **å¤åˆTokenæ„å»º**ï¼šå°†é€‰æ‹©çš„Tokenå¯¹é½æˆå¤åˆTokenï¼Œä»¥ä¿æŒç¼“å­˜çš„ç»“æ„åŒ–ã€‚4) **å…¨å±€èµ„æºåˆ†é…**ï¼šæ ¹æ®å±‚çš„ä¿¡æ¯é‡ï¼ŒåŠ¨æ€è°ƒæ•´æ¯å±‚çš„ä¿ç•™é¢„ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šKVComposeçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **æ³¨æ„åŠ›å¼•å¯¼çš„Tokené€‰æ‹©**ï¼šåˆ©ç”¨æ³¨æ„åŠ›åˆ†æ•°æ¥æŒ‡å¯¼Tokençš„é€‰æ‹©ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«é‡è¦çš„Tokenã€‚2) **å±‚è‡ªé€‚åº”çš„èµ„æºåˆ†é…**ï¼šæ ¹æ®å±‚çš„ä¿¡æ¯é‡åŠ¨æ€è°ƒæ•´ä¿ç•™é¢„ç®—ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„ç¼“å­˜ç©ºé—´ã€‚3) **å¤åˆTokençš„æ„å»º**ï¼šé€šè¿‡æ„å»ºå¤åˆTokenï¼Œåœ¨å‹ç¼©KVç¼“å­˜çš„åŒæ—¶ï¼Œä¿æŒäº†ç¼“å­˜çš„ç»“æ„åŒ–ï¼Œä»è€Œå…¼å®¹ç°æœ‰çš„æ¨ç†å¼•æ“ã€‚

**å…³é”®è®¾è®¡**ï¼šKVComposeçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **æ³¨æ„åŠ›åˆ†æ•°çš„è®¡ç®—æ–¹å¼**ï¼šè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†æŸç§ç‰¹å®šçš„æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—æ–¹æ³•ï¼Œä¾‹å¦‚å¯¹æ³¨æ„åŠ›æƒé‡è¿›è¡ŒåŠ æƒå¹³å‡ã€‚2) **Tokené€‰æ‹©çš„é˜ˆå€¼**ï¼šè®ºæ–‡éœ€è¦ç¡®å®šä¸€ä¸ªé˜ˆå€¼ï¼Œç”¨äºåˆ¤æ–­Tokenæ˜¯å¦è¶³å¤Ÿé‡è¦ï¼Œå€¼å¾—ä¿ç•™ã€‚3) **å¤åˆTokençš„å¯¹é½ç­–ç•¥**ï¼šè®ºæ–‡éœ€è¦è®¾è®¡ä¸€ç§å¯¹é½ç­–ç•¥ï¼Œç¡®ä¿å¤åˆTokenèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨ç¤ºåŸå§‹Tokençš„ä¿¡æ¯ã€‚4) **å±‚è‡ªé€‚åº”èµ„æºåˆ†é…çš„ç­–ç•¥**ï¼šè®ºæ–‡éœ€è¦è®¾è®¡ä¸€ç§ç­–ç•¥ï¼Œæ ¹æ®å±‚çš„ä¿¡æ¯é‡åŠ¨æ€è°ƒæ•´ä¿ç•™é¢„ç®—ï¼Œä¾‹å¦‚æ ¹æ®æ¯å±‚Tokençš„å¹³å‡æ³¨æ„åŠ›åˆ†æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

KVComposeåœ¨å¤šä¸ªLLMæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨ä¿æŒæ¨¡å‹ç²¾åº¦åŸºæœ¬ä¸å˜çš„æƒ…å†µä¸‹ï¼ŒKVç¼“å­˜çš„å†…å­˜å ç”¨å¯ä»¥æ˜¾è‘—é™ä½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªå…·ä½“æ¨¡å‹ä¸Šï¼ŒKVComposeå¯ä»¥å°†å†…å­˜å ç”¨é™ä½åˆ°åŸæ¥çš„50%ï¼ŒåŒæ—¶ç²¾åº¦æŸå¤±å°äº1%ã€‚æ­¤å¤–ï¼ŒKVComposeè¿˜ä¼˜äºå…¶ä»–ç»“æ„åŒ–å’ŒåŠç»“æ„åŒ–çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

KVComposeå¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿æ–‡æœ¬çš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚é•¿ç¯‡æ–‡æ¡£æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡é™ä½KVç¼“å­˜çš„å†…å­˜å ç”¨ï¼ŒKVComposeèƒ½å¤Ÿæ˜¾è‘—æå‡LLMçš„æ¨ç†æ•ˆç‡ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ï¼Œå¹¶æ”¯æŒåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡ŒLLMã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨LLMåœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) rely on key-value (KV) caches for efficient autoregressive decoding; however, cache size grows linearly with context length and model depth, becoming a major bottleneck in long-context inference. Prior KV cache compression methods either enforce rigid heuristics, disrupt tensor layouts with per-attention-head variability, or require specialized compute kernels.
>   We propose a simple, yet effective, KV cache compression framework based on attention-guided, layer-adaptive composite tokens. Our method aggregates attention scores to estimate token importance, selects head-specific tokens independently, and aligns them into composite tokens that respect the uniform cache structure required by existing inference engines. A global allocation mechanism further adapts retention budgets across layers, assigning more capacity to layers with informative tokens. This approach achieves significant memory reduction while preserving accuracy, consistently outperforming prior structured and semi-structured methods. Crucially, our approach remains fully compatible with standard inference pipelines, offering a practical and scalable solution for efficient long-context LLM deployment.

