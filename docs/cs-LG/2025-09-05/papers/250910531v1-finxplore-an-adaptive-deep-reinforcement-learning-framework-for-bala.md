---
layout: default
title: FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities
---

# FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10531v1</a>
  <a href="https://arxiv.org/pdf/2509.10531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10531v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10531v1', 'FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Himanshu Choudhary, Arishi Orra, Manoj Thakur

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FinXploreï¼šä¸€ç§è‡ªé€‚åº”æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¹³è¡¡å’Œå‘ç°æŠ•èµ„æœºä¼š**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `æŠ•èµ„ç»„åˆä¼˜åŒ–` `èµ„äº§é…ç½®` `é‡‘èå¸‚åœº` `åŒä»£ç†` `é£é™©ç®¡ç†` `æ”¶ç›Šæœ€å¤§åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºDRLçš„æŠ•èµ„ç»„åˆä¼˜åŒ–æ–¹æ³•é€šå¸¸å±€é™äºé¢„å®šä¹‰çš„èµ„äº§èŒƒå›´ï¼Œç¼ºä¹å¯¹æ–°æŠ•èµ„æœºä¼šçš„æ¢ç´¢ã€‚
2. FinXplore é‡‡ç”¨åŒä»£ç†æ¶æ„ï¼Œä¸€ä¸ªä»£ç†è´Ÿè´£ç°æœ‰èµ„äº§çš„é…ç½®ï¼Œå¦ä¸€ä¸ªä»£ç†è´Ÿè´£æ¢ç´¢æ‰©å±•æŠ•èµ„èŒƒå›´ä¸­çš„æ–°æœºä¼šã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFinXplore åœ¨çœŸå®å¸‚åœºæ•°æ®é›†ä¸­ä¼˜äºç°æœ‰çš„æŠ•èµ„ç»„åˆç­–ç•¥å’ŒåŸºçº¿æ–¹æ³•ï¼Œæå‡äº†æŠ•èµ„ç»„åˆæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¹³è¡¡é£é™©å’Œå›æŠ¥çš„æŠ•èµ„ç»„åˆä¼˜åŒ–æ–¹æ³•ã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)å·²æˆä¸ºä¸€ç§å…ˆè¿›çš„æŠ•èµ„ç»„åˆä¼˜åŒ–å·¥å…·ï¼Œå®ƒé€šè¿‡è¯•é”™äº¤äº’å­¦ä¹ åŠ¨æ€èµ„äº§é…ç½®ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºDRLçš„æ–¹æ³•ä»…é™äºåœ¨é¢„å®šä¹‰çš„æŠ•èµ„èŒƒå›´å†…åˆ†é…èµ„äº§ï¼Œè€Œå¿½ç•¥äº†æ¢ç´¢æ–°æœºä¼šã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªæŠ•èµ„ç¯å¢ƒï¼Œè¯¥ç¯å¢ƒå°†åˆ©ç”¨ç°æœ‰èµ„äº§ä¸åœ¨æ‰©å±•èŒƒå›´å†…æ¢ç´¢æ–°çš„æŠ•èµ„æœºä¼šç›¸ç»“åˆã€‚æ‰€æå‡ºçš„æ–¹æ³•åˆ©ç”¨ä¸¤ä¸ªDRLä»£ç†ï¼Œå¹¶åŠ¨æ€å¹³è¡¡è¿™äº›ç›®æ ‡ï¼Œä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„å¸‚åœºï¼ŒåŒæ—¶æé«˜æŠ•èµ„ç»„åˆçš„æ€§èƒ½ã€‚ä¸€ä¸ªä»£ç†åœ¨ç°æœ‰èŒƒå›´å†…åˆ†é…èµ„äº§ï¼Œè€Œå¦ä¸€ä¸ªä»£ç†ååŠ©æ¢ç´¢æ‰©å±•èŒƒå›´å†…çš„æ–°æœºä¼šã€‚ä½¿ç”¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„å¸‚åœºæ•°æ®é›†ç¡®å®šäº†æ‰€æå‡ºæ–¹æ³•çš„æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æŠ•èµ„ç»„åˆç­–ç•¥å’ŒåŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æŠ•èµ„ç»„åˆä¼˜åŒ–æ–¹æ³•ä¸»è¦å…³æ³¨äºåœ¨é¢„å…ˆè®¾å®šçš„æŠ•èµ„ç»„åˆèŒƒå›´å†…è¿›è¡Œèµ„äº§é…ç½®ï¼Œå¿½ç•¥äº†å¸‚åœºå˜åŒ–å¸¦æ¥çš„æ–°æŠ•èµ„æœºä¼šã€‚è¿™ç§å±€é™æ€§å¯èƒ½å¯¼è‡´æŠ•èµ„ç»„åˆæ— æ³•å……åˆ†åˆ©ç”¨å¸‚åœºæ½œåŠ›ï¼Œä»è€Œå½±å“æ•´ä½“æ”¶ç›Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFinXplore çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æŠ•èµ„ç»„åˆä¼˜åŒ–é—®é¢˜åˆ†è§£ä¸ºä¸¤ä¸ªäº’è¡¥çš„éƒ¨åˆ†ï¼šä¸€æ˜¯åˆ©ç”¨ç°æœ‰æŠ•èµ„ç»„åˆä¸­çš„èµ„äº§è¿›è¡Œä¼˜åŒ–é…ç½®ï¼ŒäºŒæ˜¯æ¢ç´¢æ–°çš„æŠ•èµ„æœºä¼šã€‚é€šè¿‡åŠ¨æ€å¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡ï¼Œä½¿æŠ•èµ„ç»„åˆèƒ½å¤Ÿé€‚åº”ä¸æ–­å˜åŒ–çš„å¸‚åœºç¯å¢ƒï¼Œä»è€Œæé«˜æ•´ä½“æ”¶ç›Šã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFinXplore é‡‡ç”¨åŒä»£ç†æ¶æ„ã€‚ç¬¬ä¸€ä¸ªä»£ç†ï¼ˆAllocation Agentï¼‰è´Ÿè´£åœ¨ç°æœ‰æŠ•èµ„ç»„åˆèŒƒå›´å†…è¿›è¡Œèµ„äº§é…ç½®ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–æ”¶ç›Šå¹¶æ§åˆ¶é£é™©ã€‚ç¬¬äºŒä¸ªä»£ç†ï¼ˆExploration Agentï¼‰è´Ÿè´£æ¢ç´¢æ‰©å±•çš„æŠ•èµ„èŒƒå›´ï¼Œå¯»æ‰¾æ½œåœ¨çš„æ–°æŠ•èµ„æœºä¼šã€‚è¿™ä¸¤ä¸ªä»£ç†é€šè¿‡æŸç§æœºåˆ¶è¿›è¡Œäº¤äº’ï¼Œä»¥å®ç°æ•´ä½“æŠ•èµ„ç»„åˆçš„ä¼˜åŒ–ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ä»£ç†è®­ç»ƒã€èµ„äº§é…ç½®å’Œæ€§èƒ½è¯„ä¼°ç­‰æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šFinXplore çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†åŒä»£ç†æ¶æ„ï¼Œå°†æŠ•èµ„ç»„åˆä¼˜åŒ–é—®é¢˜åˆ†è§£ä¸ºåˆ©ç”¨ç°æœ‰èµ„äº§å’Œæ¢ç´¢æ–°æœºä¼šä¸¤ä¸ªéƒ¨åˆ†ï¼Œå¹¶åŠ¨æ€å¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¸‚åœºå˜åŒ–ï¼Œä»è€Œæé«˜æŠ•èµ„ç»„åˆçš„æ•´ä½“æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„å•ä»£ç†æ–¹æ³•ç›¸æ¯”ï¼ŒFinXplore èƒ½å¤Ÿæ›´å…¨é¢åœ°è€ƒè™‘å¸‚åœºä¿¡æ¯ï¼Œå¹¶åšå‡ºæ›´æ˜æ™ºçš„æŠ•èµ„å†³ç­–ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„æŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬ï¼šä¸¤ä¸ªä»£ç†çš„ç½‘ç»œç»“æ„ï¼ˆä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæˆ–å¾ªç¯ç¥ç»ç½‘ç»œï¼‰ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼ˆä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ Sharpe Ratio æˆ–å…¶ä»–é£é™©è°ƒæ•´åçš„æ”¶ç›ŠæŒ‡æ ‡ï¼‰ï¼Œä»¥åŠä»£ç†ä¹‹é—´çš„äº¤äº’æœºåˆ¶ï¼ˆä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨æŸç§å½¢å¼çš„æ³¨æ„åŠ›æœºåˆ¶æˆ–é—¨æ§æœºåˆ¶ï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦ä»”ç»†é€‰æ‹©è®­ç»ƒæ•°æ®å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFinXplore åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„å¸‚åœºæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„æŠ•èµ„ç»„åˆç­–ç•¥å’ŒåŸºçº¿æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒFinXplore åœ¨æ”¶ç›Šç‡ã€é£é™©è°ƒæ•´åçš„æ”¶ç›Šç‡ï¼ˆä¾‹å¦‚ï¼ŒSharpe Ratioï¼‰ç­‰æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFinXplore èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡é£é™©å’Œå›æŠ¥ï¼Œå¹¶å‘ç°æ–°çš„æŠ•èµ„æœºä¼šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FinXplore å¯åº”ç”¨äºå„ç§é‡‘èæŠ•èµ„åœºæ™¯ï¼Œä¾‹å¦‚è‚¡ç¥¨æŠ•èµ„ã€åŸºé‡‘ç®¡ç†ã€é‡åŒ–äº¤æ˜“ç­‰ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¸®åŠ©æŠ•èµ„è€…åœ¨ä¸æ–­å˜åŒ–çš„å¸‚åœºç¯å¢ƒä¸­ï¼Œæ›´å¥½åœ°å¹³è¡¡é£é™©å’Œå›æŠ¥ï¼Œå‘ç°æ–°çš„æŠ•èµ„æœºä¼šï¼Œä»è€Œæé«˜æŠ•èµ„ç»„åˆçš„æ•´ä½“æ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„é‡‘èäº§å“å’Œå¸‚åœºï¼Œä¾‹å¦‚è¡ç”Ÿå“ã€å¤–æ±‡å¸‚åœºç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Portfolio optimization is essential for balancing risk and return in financial decision-making. Deep Reinforcement Learning (DRL) has stood out as a cutting-edge tool for portfolio optimization that learns dynamic asset allocation using trial-and-error interactions. However, most DRL-based methods are restricted to allocating assets within a pre-defined investment universe and overlook exploring new opportunities. This study introduces an investment landscape that integrates exploiting existing assets with exploring new investment opportunities in an extended universe. The proposed approach leverages two DRL agents and dynamically balances these objectives to adapt to evolving markets while enhancing portfolio performance. One agent allocates assets within the existing universe, while another assists in exploring new opportunities in the extended universe. The effciency of the proposed methodology is determined using two real-world market data sets. The experiments demonstrate the superiority of the suggested approach against the state-of-the-art portfolio strategies and baseline methods.

