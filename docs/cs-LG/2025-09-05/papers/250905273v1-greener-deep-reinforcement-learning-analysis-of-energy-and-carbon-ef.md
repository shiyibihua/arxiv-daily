---
layout: default
title: Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks
---

# Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05273" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05273v1</a>
  <a href="https://arxiv.org/pdf/2509.05273.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05273v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05273v1', 'Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jason Gardner, Ayan Dutta, Swapnoneel Roy, O. Patrick Kreidl, Ladislau Boloni

**åˆ†ç±»**: cs.LG, cs.PF

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

**å¤‡æ³¨**: Submitted to a journal - under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æAtariåŸºå‡†æµ‹è¯•ä¸­æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„èƒ½æºå’Œç¢³æ•ˆç‡ï¼Œä¸ºç»¿è‰²DRLæä¾›åŸºå‡†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `èƒ½æºæ•ˆç‡` `ç¢³æ’æ”¾` `åŸºå‡†æµ‹è¯•` `Atari` `å¯æŒç»­æ€§` `ç®—æ³•è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„èƒ½è€—å’Œç¢³æ’æ”¾æˆæœ¬ç¼ºä¹ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œé˜»ç¢äº†ç»¿è‰²DRLçš„å‘å±•ã€‚
2. é€šè¿‡åŸºå‡†æµ‹è¯•ä¸ƒç§ä¸»æµDRLç®—æ³•åœ¨Atariæ¸¸æˆä¸Šçš„èƒ½è€—ã€ç¢³æ’æ”¾å’Œç»æµæˆæœ¬ï¼Œæ­ç¤ºç®—æ³•é—´çš„æ•ˆç‡å·®å¼‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŒç®—æ³•åœ¨èƒ½è€—å’Œæˆæœ¬ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸ºé€‰æ‹©æ›´ç¯ä¿çš„DRLç®—æ³•æä¾›äº†ä¾æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ—¥ç›Šå¢é•¿çš„è®¡ç®—éœ€æ±‚å¼•å‘äº†äººä»¬å¯¹è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹æ‰€å¸¦æ¥çš„ç¯å¢ƒå’Œç»æµæˆæœ¬çš„æ‹…å¿§ã€‚è™½ç„¶åœ¨å­¦ä¹ æ€§èƒ½æ–¹é¢çš„ç®—æ³•æ•ˆç‡å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†DRLç®—æ³•çš„èƒ½æºéœ€æ±‚ã€æ¸©å®¤æ°”ä½“æ’æ”¾å’Œè´§å¸æˆæœ¬åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡å¯¹ä¸ƒç§æœ€å…ˆè¿›çš„DRLç®—æ³•ï¼ˆDQNã€TRPOã€A2Cã€ARSã€PPOã€RecurrentPPOå’ŒQR-DQNï¼‰çš„èƒ½è€—è¿›è¡Œäº†ç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•ç ”ç©¶ï¼Œè¿™äº›ç®—æ³•å‡ä½¿ç”¨Stable Baselineså®ç°ã€‚æ¯ç§ç®—æ³•åœ¨åä¸ªAtari 2600æ¸¸æˆä¸­è®­ç»ƒä¸€ç™¾ä¸‡æ­¥ï¼Œå¹¶å®æ—¶æµ‹é‡åŠŸè€—ï¼Œä»¥æ ¹æ®ç¾å›½å…¨å›½å¹³å‡ç”µä»·ä¼°ç®—æ€»èƒ½æºä½¿ç”¨é‡ã€äºŒæ°§åŒ–ç¢³å½“é‡æ’æ”¾é‡å’Œç”µåŠ›æˆæœ¬ã€‚ç»“æœè¡¨æ˜ï¼Œä¸åŒç®—æ³•åœ¨èƒ½æºæ•ˆç‡å’Œè®­ç»ƒæˆæœ¬æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæœ‰äº›ç®—æ³•åœ¨æ¶ˆè€—é«˜è¾¾24%æ›´å°‘èƒ½æºï¼ˆARS vs. DQNï¼‰ã€æ’æ”¾è¿‘68%æ›´å°‘äºŒæ°§åŒ–ç¢³ä»¥åŠäº§ç”Ÿå‡ ä¹68%æ›´ä½è´§å¸æˆæœ¬ï¼ˆQR-DQN vs. RecurrentPPOï¼‰çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†å­¦ä¹ æ€§èƒ½ã€è®­ç»ƒæ—¶é—´ã€èƒ½æºä½¿ç”¨å’Œè´¢åŠ¡æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œçªå‡ºäº†ç®—æ³•é€‰æ‹©å¯ä»¥åœ¨ä¸ç‰ºç‰²å­¦ä¹ æ€§èƒ½çš„æƒ…å†µä¸‹å‡è½»ç¯å¢ƒå’Œç»æµå½±å“çš„æƒ…å†µã€‚è¿™é¡¹ç ”ç©¶ä¸ºå¼€å‘å…·æœ‰èƒ½æºæ„è¯†å’Œæˆæœ¬æ•ˆç›Šçš„DRLå®è·µæä¾›äº†å¯æ“ä½œçš„è§è§£ï¼Œå¹¶ä¸ºå°†å¯æŒç»­æ€§è€ƒè™‘å› ç´ çº³å…¥æœªæ¥çš„ç®—æ³•è®¾è®¡å’Œè¯„ä¼°å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½æºæ¶ˆè€—é«˜ã€ç¢³æ’æ”¾é‡å¤§ä»¥åŠç»æµæˆæœ¬é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç®—æ³•çš„å­¦ä¹ æ€§èƒ½ï¼Œè€Œå¿½ç•¥äº†å…¶ç¯å¢ƒå’Œç»æµå½±å“ï¼Œç¼ºä¹å¯¹ä¸åŒç®—æ³•èƒ½è€—æ•ˆç‡çš„ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œå¯¼è‡´æ— æ³•é€‰æ‹©æ›´ç¯ä¿çš„ç®—æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹ä¸»æµDRLç®—æ³•åœ¨æ ‡å‡†Atariæ¸¸æˆç¯å¢ƒä¸‹çš„èƒ½è€—è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œé‡åŒ–ä¸åŒç®—æ³•çš„èƒ½æºæ•ˆç‡ã€ç¢³æ’æ”¾é‡å’Œç»æµæˆæœ¬ï¼Œä»è€Œä¸ºé€‰æ‹©å’Œè®¾è®¡æ›´ç¯ä¿çš„DRLç®—æ³•æä¾›ä¾æ®ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å®è¯åˆ†ææ­ç¤ºäº†ç®—æ³•é€‰æ‹©å¯¹ç¯å¢ƒå’Œç»æµçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š
1.  é€‰æ‹©ä¸ƒç§å…·æœ‰ä»£è¡¨æ€§çš„DRLç®—æ³•ï¼šDQN, TRPO, A2C, ARS, PPO, RecurrentPPO, QR-DQNã€‚
2.  ä½¿ç”¨Stable Baselineså®ç°è¿™äº›ç®—æ³•ã€‚
3.  åœ¨åä¸ªAtari 2600æ¸¸æˆä¸Šè®­ç»ƒè¿™äº›ç®—æ³•ï¼Œæ¯ä¸ªç®—æ³•è®­ç»ƒä¸€ç™¾ä¸‡æ­¥ã€‚
4.  å®æ—¶æµ‹é‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠŸè€—ã€‚
5.  æ ¹æ®åŠŸè€—æ•°æ®ä¼°ç®—æ€»èƒ½æºä½¿ç”¨é‡ã€äºŒæ°§åŒ–ç¢³å½“é‡æ’æ”¾é‡å’Œç”µåŠ›æˆæœ¬ã€‚
6.  åˆ†æå­¦ä¹ æ€§èƒ½ã€è®­ç»ƒæ—¶é—´ã€èƒ½æºä½¿ç”¨å’Œè´¢åŠ¡æˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¯¹DRLç®—æ³•çš„èƒ½æºæ•ˆç‡è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶é‡åŒ–äº†ä¸åŒç®—æ³•çš„ç¢³æ’æ”¾å’Œç»æµæˆæœ¬ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨ç®—æ³•çš„å­¦ä¹ æ€§èƒ½ï¼Œè€Œå¿½ç•¥äº†å…¶ç¯å¢ƒå’Œç»æµå½±å“ã€‚è¯¥ç ”ç©¶é¦–æ¬¡å°†å¯æŒç»­æ€§è€ƒè™‘å› ç´ çº³å…¥DRLç®—æ³•çš„è¯„ä¼°ä½“ç³»ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š
1.  é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„Atariæ¸¸æˆä½œä¸ºæµ‹è¯•ç¯å¢ƒã€‚
2.  ä½¿ç”¨Stable Baselinesä½œä¸ºDRLç®—æ³•çš„å®ç°æ¡†æ¶ã€‚
3.  å®æ—¶æµ‹é‡åŠŸè€—ï¼Œå¹¶ä½¿ç”¨ç¾å›½å…¨å›½å¹³å‡ç”µä»·è®¡ç®—ç”µåŠ›æˆæœ¬ã€‚
4.  é‡‡ç”¨äºŒæ°§åŒ–ç¢³æ’æ”¾å› å­å°†èƒ½æºæ¶ˆè€—è½¬åŒ–ä¸ºç¢³æ’æ”¾é‡ã€‚
5.  åˆ†æå­¦ä¹ æ€§èƒ½ã€è®­ç»ƒæ—¶é—´ã€èƒ½æºä½¿ç”¨å’Œè´¢åŠ¡æˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŒDRLç®—æ³•åœ¨èƒ½è€—å’Œæˆæœ¬æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¾‹å¦‚ï¼ŒARSç®—æ³•ç›¸æ¯”DQNç®—æ³•ï¼Œèƒ½è€—é™ä½äº†24%ã€‚QR-DQNç®—æ³•ç›¸æ¯”RecurrentPPOç®—æ³•ï¼Œç¢³æ’æ”¾é‡å’Œç»æµæˆæœ¬é™ä½äº†è¿‘68%ã€‚è¿™äº›æ•°æ®çªå‡ºäº†ç®—æ³•é€‰æ‹©å¯¹ç¯å¢ƒå’Œç»æµå½±å“çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºé€‰æ‹©æ›´ç¯ä¿çš„DRLç®—æ³•æä¾›äº†ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ã€èµ„æºç®¡ç†ç­‰ã€‚é€šè¿‡é€‰æ‹©æ›´èŠ‚èƒ½çš„ç®—æ³•ï¼Œå¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼Œå‡å°‘ç¢³æ’æ”¾ï¼Œä»è€Œå®ç°æ›´å¯æŒç»­çš„AIå‘å±•ã€‚è¯¥ç ”ç©¶ä¹Ÿä¸ºæœªæ¥è®¾è®¡æ›´ç¯ä¿çš„DRLç®—æ³•æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The growing computational demands of deep reinforcement learning (DRL) have raised concerns about the environmental and economic costs of training large-scale models. While algorithmic efficiency in terms of learning performance has been extensively studied, the energy requirements, greenhouse gas emissions, and monetary costs of DRL algorithms remain largely unexplored. In this work, we present a systematic benchmarking study of the energy consumption of seven state-of-the-art DRL algorithms, namely DQN, TRPO, A2C, ARS, PPO, RecurrentPPO, and QR-DQN, implemented using Stable Baselines. Each algorithm was trained for one million steps each on ten Atari 2600 games, and power consumption was measured in real-time to estimate total energy usage, CO2-Equivalent emissions, and electricity cost based on the U.S. national average electricity price. Our results reveal substantial variation in energy efficiency and training cost across algorithms, with some achieving comparable performance while consuming up to 24% less energy (ARS vs. DQN), emitting nearly 68% less CO2, and incurring almost 68% lower monetary cost (QR-DQN vs. RecurrentPPO) than less efficient counterparts. We further analyze the trade-offs between learning performance, training time, energy use, and financial cost, highlighting cases where algorithmic choices can mitigate environmental and economic impact without sacrificing learning performance. This study provides actionable insights for developing energy-aware and cost-efficient DRL practices and establishes a foundation for incorporating sustainability considerations into future algorithmic design and evaluation.

