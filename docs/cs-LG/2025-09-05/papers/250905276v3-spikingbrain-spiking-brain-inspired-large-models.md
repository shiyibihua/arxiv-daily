---
layout: default
title: SpikingBrain: Spiking Brain-inspired Large Models
---

# SpikingBrain: Spiking Brain-inspired Large Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05276" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05276v3</a>
  <a href="https://arxiv.org/pdf/2509.05276.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05276v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05276v3', 'SpikingBrain: Spiking Brain-inspired Large Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqi Pan, Yupeng Feng, Jinghao Zhuang, Siyu Ding, Han Xu, Zehao Liu, Bohan Sun, Yuhong Chou, Xuerui Qiu, Anlin Deng, Anjie Hu, Shurong Wang, Peng Zhou, Man Yao, Jibin Wu, Jian Yang, Guoliang Sun, Bo Xu, Guoqi Li

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-12-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SpikingBrainï¼šå—è„‘å¯å‘çš„å¤§æ¨¡å‹ï¼Œæå‡é•¿æ–‡æœ¬å¤„ç†æ•ˆç‡å¹¶é™ä½åŠŸè€—**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è„‰å†²ç¥ç»ç½‘ç»œ` `å¤§è¯­è¨€æ¨¡å‹` `é•¿æ–‡æœ¬å¤„ç†` `çº¿æ€§æ³¨æ„åŠ›` `ä½åŠŸè€—è®¡ç®—` `MetaX GPU` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­é¢ä¸´è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. SpikingBrainé€šè¿‡çº¿æ€§/æ··åˆçº¿æ€§æ³¨æ„åŠ›ã€è„‰å†²ç¥ç»å…ƒå’Œå®šåˆ¶çš„è®­ç»ƒæµç¨‹ï¼Œå®ç°äº†é«˜æ•ˆçš„é•¿æ–‡æœ¬å¤„ç†ã€‚
3. SpikingBrainæ¨¡å‹åœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„åŠ é€Ÿå’Œå†…å­˜ä¼˜åŒ–ï¼Œå¹¶å®ç°äº†é«˜ç¨€ç–æ€§ï¼Œé™ä½äº†åŠŸè€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†SpikingBrainï¼Œä¸€ä¸ªå—è„‘å¯å‘çš„æ¨¡å‹å®¶æ—ï¼Œæ—¨åœ¨æé«˜é•¿æ–‡æœ¬è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚SpikingBrainåˆ©ç”¨MetaX GPUé›†ç¾¤ï¼Œé‡ç‚¹å…³æ³¨ä¸‰ä¸ªæ–¹é¢ï¼šæ¨¡å‹æ¶æ„ï¼ˆå…·æœ‰è‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒçš„çº¿æ€§åŠæ··åˆçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€ç®—æ³•ä¼˜åŒ–ï¼ˆé«˜æ•ˆçš„åŸºäºè½¬æ¢çš„è®­ç»ƒæµç¨‹å’Œä¸“ç”¨è„‰å†²ç¼–ç æ¡†æ¶ï¼‰ä»¥åŠç³»ç»Ÿå·¥ç¨‹ï¼ˆä¸ºMetaXç¡¬ä»¶å®šåˆ¶çš„è®­ç»ƒæ¡†æ¶ã€ç®—å­åº“å’Œå¹¶è¡Œç­–ç•¥ï¼‰ã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†SpikingBrain-7Bï¼ˆçº¿æ€§LLMï¼‰å’ŒSpikingBrain-76Bï¼ˆæ··åˆçº¿æ€§MoE LLMï¼‰ä¸¤ä¸ªæ¨¡å‹ï¼Œè¯æ˜äº†åœ¨éNVIDIAå¹³å°ä¸Šå¼€å‘å¤§è§„æ¨¡LLMçš„å¯è¡Œæ€§ã€‚SpikingBrainåœ¨æŒç»­é¢„è®­ç»ƒä¸­ä½¿ç”¨çº¦150B tokensï¼Œæ€§èƒ½ä¸å¼€æºTransformeråŸºçº¿ç›¸å½“ï¼Œå¹¶æ˜¾è‘—æé«˜äº†é•¿æ–‡æœ¬æ•ˆç‡ï¼Œå®ç°äº†ï¼ˆéƒ¨åˆ†ï¼‰æ’å®šå†…å­˜å’Œäº‹ä»¶é©±åŠ¨çš„è„‰å†²è¡Œä¸ºã€‚ä¾‹å¦‚ï¼ŒSpikingBrain-7Båœ¨4M tokenåºåˆ—çš„é¦–æ¬¡tokenç”Ÿæˆæ—¶é—´ä¸Šå®ç°äº†è¶…è¿‡100å€çš„åŠ é€Ÿï¼Œè„‰å†²æ–¹æ¡ˆå®ç°äº†69.15%çš„ç¨€ç–æ€§ï¼Œä»è€Œé™ä½äº†åŠŸè€—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œè®­ç»ƒè®¡ç®—é‡éšåºåˆ—é•¿åº¦å‘ˆå¹³æ–¹å¢é•¿ï¼Œæ¨ç†å†…å­˜å‘ˆçº¿æ€§å¢é•¿ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚åŒæ—¶ï¼Œåœ¨éNVIDIAå¹³å°ä¸Šè®­ç»ƒå¤§å‹æ¨¡å‹ä¹Ÿé¢ä¸´ç¨³å®šæ€§å’Œæ•ˆç‡çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSpikingBrainçš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´å¤§è„‘çš„è„‰å†²ç¥ç»å…ƒæœºåˆ¶ï¼Œè®¾è®¡ä¸€ç§æ–°å‹çš„ã€æ›´é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹æ¶æ„ã€‚é€šè¿‡çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ã€è„‰å†²ç¼–ç å’Œä¸“é—¨çš„ç¡¬ä»¶ä¼˜åŒ–ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é•¿æ–‡æœ¬å¤„ç†å’Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpikingBrainçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†ï¼š1) æ¨¡å‹æ¶æ„ï¼šé‡‡ç”¨çº¿æ€§æˆ–æ··åˆçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ç»“åˆè‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒï¼›2) ç®—æ³•ä¼˜åŒ–ï¼šè®¾è®¡é«˜æ•ˆçš„åŸºäºè½¬æ¢çš„è®­ç»ƒæµç¨‹å’Œä¸“ç”¨çš„è„‰å†²ç¼–ç æ¡†æ¶ï¼›3) ç³»ç»Ÿå·¥ç¨‹ï¼šé’ˆå¯¹MetaXç¡¬ä»¶å®šåˆ¶è®­ç»ƒæ¡†æ¶ã€ç®—å­åº“å’Œå¹¶è¡Œç­–ç•¥ã€‚æ•´ä½“æµç¨‹æ˜¯ä»æ•°æ®é¢„å¤„ç†å¼€å§‹ï¼Œç»è¿‡è„‰å†²ç¼–ç ï¼Œè¾“å…¥åˆ°SpikingBrainæ¨¡å‹è¿›è¡Œè®­ç»ƒæˆ–æ¨ç†ï¼Œæœ€åè§£ç å¾—åˆ°ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šSpikingBrainçš„å…³é”®åˆ›æ–°åœ¨äºå°†è„‰å†²ç¥ç»å…ƒå’Œçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥åˆ°å¤§è¯­è¨€æ¨¡å‹ä¸­ã€‚ä¸ä¼ ç»Ÿçš„Transformeræ¨¡å‹ç›¸æ¯”ï¼ŒSpikingBrainé€šè¿‡è„‰å†²ç¼–ç å’Œçº¿æ€§æ³¨æ„åŠ›é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶å®ç°äº†æ›´é«˜çš„ç¨€ç–æ€§ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œé™ä½äº†åŠŸè€—ã€‚

**å…³é”®è®¾è®¡**ï¼šSpikingBrainçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒï¼šæ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´è„‰å†²å‘æ”¾é¢‘ç‡ï¼›2) çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼šé™ä½äº†æ³¨æ„åŠ›è®¡ç®—çš„å¤æ‚åº¦ï¼›3) è„‰å†²ç¼–ç æ¡†æ¶ï¼šå°†è¿ç»­çš„è¾“å…¥è½¬æ¢ä¸ºç¦»æ•£çš„è„‰å†²åºåˆ—ï¼›4) é’ˆå¯¹MetaXç¡¬ä»¶çš„ä¼˜åŒ–ï¼šå®šåˆ¶äº†è®­ç»ƒæ¡†æ¶ã€ç®—å­åº“å’Œå¹¶è¡Œç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨ç¡¬ä»¶æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SpikingBrain-7Båœ¨4M tokenåºåˆ—çš„é¦–æ¬¡tokenç”Ÿæˆæ—¶é—´ä¸Šå®ç°äº†è¶…è¿‡100å€çš„åŠ é€Ÿã€‚è„‰å†²æ–¹æ¡ˆå®ç°äº†69.15%çš„ç¨€ç–æ€§ï¼Œæ˜¾è‘—é™ä½äº†åŠŸè€—ã€‚SpikingBrain-7Bå’ŒSpikingBrain-76Båœ¨æ€§èƒ½ä¸Šä¸å¼€æºTransformeråŸºçº¿ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨äº†æ›´å°‘çš„tokensè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œè¯æ˜äº†å…¶é«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SpikingBrainå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬é•¿æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„è®¡ç®—å’Œä½åŠŸè€—ç‰¹æ€§ä½¿å…¶ç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™çš„åœºæ™¯ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—å’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚æœªæ¥ï¼ŒSpikingBrainæœ‰æœ›æ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶ä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¯æŒç»­å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mainstream Transformer-based large language models face major efficiency bottlenecks: training computation scales quadratically with sequence length, and inference memory grows linearly, limiting long-context processing. Building large models on non-NVIDIA platforms also poses challenges for stable and efficient training. To address this, we introduce SpikingBrain, a family of brain-inspired models designed for efficient long-context training and inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three aspects: (1) Model Architecture: linear and hybrid-linear attention architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an efficient, conversion-based training pipeline and a dedicated spike coding framework; (3) System Engineering: customized training frameworks, operator libraries, and parallelism strategies tailored to MetaX hardware.
>   Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM, and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the feasibility of large-scale LLM development on non-NVIDIA platforms, and training remains stable for weeks on hundreds of MetaX GPUs with Model FLOPs Utilization at expected levels. SpikingBrain achieves performance comparable to open-source Transformer baselines while using only about 150B tokens for continual pre-training. Our models also significantly improve long-context efficiency and deliver inference with (partially) constant memory and event-driven spiking behavior. For example, SpikingBrain-7B attains over 100x speedup in Time to First Token for 4M-token sequences. Furthermore, the proposed spiking scheme achieves 69.15 percent sparsity, enabling low-power operation. Overall, this work demonstrates the potential of brain-inspired mechanisms to drive the next generation of efficient and scalable large model design.

