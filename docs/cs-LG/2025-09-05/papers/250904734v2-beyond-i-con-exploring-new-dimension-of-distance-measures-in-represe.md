---
layout: default
title: Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning
---

# Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04734" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04734v2</a>
  <a href="https://arxiv.org/pdf/2509.04734.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04734v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04734v2', 'Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jasmine Shone, Zhening Li, Shaden Alshammari, Mark Hamilton, William Freeman

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-12-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Beyond I-Conï¼šæ¢ç´¢è¡¨å¾å­¦ä¹ ä¸­è·ç¦»åº¦é‡çš„æ–°ç»´åº¦ï¼Œæå‡èšç±»ä¸é™ç»´æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¡¨å¾å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `è·ç¦»åº¦é‡` `KLæ•£åº¦` `ç»Ÿè®¡æ•£åº¦` `æ— ç›‘ç£èšç±»` `é™ç»´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¡¨å¾å­¦ä¹ æ–¹æ³•å¸¸éšå¼åœ°æœ€å°åŒ–KLæ•£åº¦ï¼Œä½†KLæ•£åº¦çš„æ€§è´¨å¯èƒ½å¯¼è‡´ä¼˜åŒ–é—®é¢˜ï¼Œä¸”ä¸çœŸå®ç›®æ ‡ä¸ç¬¦ã€‚
2. Beyond I-Conæ¡†æ¶é€šè¿‡æ¢ç´¢ä¸åŒçš„ç»Ÿè®¡æ•£åº¦ï¼Œç³»ç»Ÿåœ°å‘ç°æ–°çš„æŸå¤±å‡½æ•°ï¼Œä»è€Œä¼˜åŒ–è¡¨å¾å­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨TVè·ç¦»æ”¹è¿›PMIç®—æ³•ï¼ŒJSDæ”¹è¿›ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰ç•Œf-æ•£åº¦æ”¹è¿›é™ç»´ï¼Œå‡å–å¾—æ˜¾è‘—æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Information Contrastive (I-Con) æ¡†æ¶æ­ç¤ºï¼Œè¶…è¿‡23ç§è¡¨å¾å­¦ä¹ æ–¹æ³•éšå¼åœ°æœ€å°åŒ–æ•°æ®åˆ†å¸ƒä¸å­¦ä¹ åˆ°çš„åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ï¼Œåè€…ç¼–ç äº†æ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ç„¶è€Œï¼ŒåŸºäºKLçš„æŸå¤±å¯èƒ½ä¸çœŸå®ç›®æ ‡ä¸ä¸€è‡´ï¼Œå¹¶ä¸”KLæ•£åº¦çš„éå¯¹ç§°æ€§å’Œæ— ç•Œæ€§å¯èƒ½å¸¦æ¥ä¼˜åŒ–æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†Beyond I-Conï¼Œä¸€ä¸ªé€šè¿‡æ¢ç´¢æ›¿ä»£ç»Ÿè®¡æ•£åº¦æ¥ç³»ç»Ÿå‘ç°æ–°æŸå¤±å‡½æ•°çš„æ¡†æ¶ã€‚ä¸»è¦å‘ç°åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åœ¨DINO-ViTåµŒå…¥çš„æ— ç›‘ç£èšç±»ä¸Šï¼Œé€šè¿‡ä¿®æ”¹PMIç®—æ³•ä»¥ä½¿ç”¨å…¨å˜å·®(TV)è·ç¦»ï¼Œæˆ‘ä»¬å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼›ï¼ˆ2ï¼‰é€šè¿‡ç”¨Jenson-Shannonæ•£åº¦(JSD)æ›¿æ¢æ ‡å‡†æŸå¤±å‡½æ•°ï¼Œæ”¹è¿›äº†ä»¥æ¬§å‡ é‡Œå¾—è·ç¦»ä½œä¸ºç‰¹å¾ç©ºé—´åº¦é‡çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼›ï¼ˆ3ï¼‰åœ¨é™ç»´æ–¹é¢ï¼Œé€šè¿‡ç”¨æœ‰ç•Œçš„$f$-æ•£åº¦æ›¿æ¢KLæ•£åº¦ï¼Œæˆ‘ä»¬å®ç°äº†ä¼˜äºSNEçš„å®šæ€§ç»“æœå’Œæ›´å¥½çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†åœ¨è¡¨å¾å­¦ä¹ ä¼˜åŒ–ä¸­è€ƒè™‘æ•£åº¦é€‰æ‹©çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šå¸¸ä¾èµ–äºæœ€å°åŒ–æ•°æ®åˆ†å¸ƒå’Œå­¦ä¹ åˆ°çš„è¡¨å¾åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ã€‚ç„¶è€Œï¼ŒKLæ•£åº¦æœ¬èº«å…·æœ‰éå¯¹ç§°æ€§å’Œæ— ç•Œæ€§ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¼˜åŒ–å›°éš¾ï¼Œå¹¶ä¸”KLæ•£åº¦å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ æ•°æ®ä¹‹é—´çš„çœŸå®ç›¸ä¼¼æ€§å…³ç³»ï¼Œä»è€Œé™åˆ¶äº†è¡¨å¾å­¦ä¹ çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œéœ€è¦æ¢ç´¢æ›¿ä»£çš„è·ç¦»åº¦é‡æ–¹å¼æ¥æ”¹è¿›è¡¨å¾å­¦ä¹ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBeyond I-Conçš„æ ¸å¿ƒæ€è·¯æ˜¯è·³å‡ºKLæ•£åº¦çš„å±€é™ï¼Œæ¢ç´¢æ›´å¹¿æ³›çš„ç»Ÿè®¡æ•£åº¦ä½œä¸ºè¡¨å¾å­¦ä¹ çš„æŸå¤±å‡½æ•°ã€‚é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°ä¸åŒçš„æ•£åº¦ï¼Œä¾‹å¦‚å…¨å˜å·®(TV)è·ç¦»ã€Jenson-Shannonæ•£åº¦(JSD)å’Œæœ‰ç•Œ$f$-æ•£åº¦ï¼Œæ¥å¯»æ‰¾æ›´é€‚åˆç‰¹å®šä»»åŠ¡å’Œæ•°æ®é›†çš„è·ç¦»åº¦é‡æ–¹å¼ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å…‹æœKLæ•£åº¦çš„ç¼ºç‚¹ï¼Œå¹¶å‘ç°èƒ½å¤Ÿäº§ç”Ÿæ›´å¥½è¡¨å¾çš„æŸå¤±å‡½æ•°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBeyond I-Conæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1. é€‰æ‹©åˆé€‚çš„è¡¨å¾å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚æ— ç›‘ç£èšç±»ã€ç›‘ç£å¯¹æ¯”å­¦ä¹ å’Œé™ç»´ã€‚2. é’ˆå¯¹æ¯ä¸ªä»»åŠ¡ï¼Œé€‰æ‹©ä¸€ç§æˆ–å¤šç§ç°æœ‰çš„åŸºäºKLæ•£åº¦çš„æŸå¤±å‡½æ•°ä½œä¸ºåŸºçº¿ã€‚3. å°†KLæ•£åº¦æ›¿æ¢ä¸ºå…¶ä»–ç»Ÿè®¡æ•£åº¦ï¼Œä¾‹å¦‚TVè·ç¦»ã€JSDæˆ–æœ‰ç•Œ$f$-æ•£åº¦ã€‚4. åœ¨ç›¸åŒçš„æ•°æ®é›†å’Œå®éªŒè®¾ç½®ä¸‹ï¼Œæ¯”è¾ƒä½¿ç”¨ä¸åŒæ•£åº¦çš„æŸå¤±å‡½æ•°çš„æ€§èƒ½ã€‚5. åˆ†æå®éªŒç»“æœï¼Œç¡®å®šå“ªäº›æ•£åº¦åœ¨å“ªäº›ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œå¹¶è§£é‡Šå…¶åŸå› ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„æ¡†æ¶ï¼Œç”¨äºæ¢ç´¢è¡¨å¾å­¦ä¹ ä¸­è·ç¦»åº¦é‡çš„æ–°ç»´åº¦ã€‚å®ƒä¸å†å±€é™äºä¼ ç»Ÿçš„KLæ•£åº¦ï¼Œè€Œæ˜¯å°†ç›®å…‰æŠ•å‘äº†æ›´å¹¿æ³›çš„ç»Ÿè®¡æ•£åº¦ï¼Œå¹¶è¯æ˜äº†é€šè¿‡é€‰æ‹©åˆé€‚çš„æ•£åº¦ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è¡¨å¾å­¦ä¹ çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œå®ƒä¸æ˜¯ä¸“æ³¨äºæ”¹è¿›ç°æœ‰çš„åŸºäºKLæ•£åº¦çš„æŸå¤±å‡½æ•°ï¼Œè€Œæ˜¯ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†è·ç¦»åº¦é‡çš„é€‰æ‹©ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ— ç›‘ç£èšç±»ä¸­ï¼Œä½œè€…ä¿®æ”¹äº†PMIç®—æ³•ï¼Œå°†KLæ•£åº¦æ›¿æ¢ä¸ºå…¨å˜å·®(TV)è·ç¦»ã€‚åœ¨ç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œä½œè€…å°†æ ‡å‡†æŸå¤±å‡½æ•°æ›¿æ¢ä¸ºJenson-Shannonæ•£åº¦(JSD)ã€‚åœ¨é™ç»´æ–¹é¢ï¼Œä½œè€…ç”¨æœ‰ç•Œçš„$f$-æ•£åº¦æ›¿æ¢äº†KLæ•£åº¦ã€‚è¿™äº›æ›¿æ¢çš„å…³é”®åœ¨äºé€‰æ‹©ä¸ç‰¹å®šä»»åŠ¡å’Œæ•°æ®é›†ç‰¹æ€§ç›¸åŒ¹é…çš„æ•£åº¦ã€‚ä¾‹å¦‚ï¼ŒTVè·ç¦»å¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼ŒJSDæ˜¯å¯¹ç§°çš„ï¼Œæœ‰ç•Œ$f$-æ•£åº¦å¯ä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨DINO-ViTåµŒå…¥çš„æ— ç›‘ç£èšç±»ä¸Šï¼Œä½¿ç”¨TVè·ç¦»æ”¹è¿›çš„PMIç®—æ³•å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚åœ¨ç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œä½¿ç”¨JSDæ›¿æ¢æ ‡å‡†æŸå¤±å‡½æ•°åï¼Œæ€§èƒ½å¾—åˆ°äº†æå‡ã€‚åœ¨é™ç»´æ–¹é¢ï¼Œä½¿ç”¨æœ‰ç•Œçš„$f$-æ•£åº¦æ›¿æ¢KLæ•£åº¦åï¼Œè·å¾—äº†ä¼˜äºSNEçš„å®šæ€§ç»“æœå’Œæ›´å¥½çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé€‰æ‹©åˆé€‚çš„è·ç¦»åº¦é‡æ–¹å¼å¯¹äºè¡¨å¾å­¦ä¹ è‡³å…³é‡è¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€æ–‡æœ¬èšç±»ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„è·ç¦»åº¦é‡æ–¹å¼ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ›´å¤šç±»å‹çš„ç»Ÿè®¡æ•£åº¦ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ›´å¹¿æ³›çš„è¡¨å¾å­¦ä¹ ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) supervised contrastive learning with Euclidean distance as the feature space metric is improved by replacing the standard loss function with Jenson-Shannon divergence (JSD); (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded $f$-divergence. Our results highlight the importance of considering divergence choices in representation learning optimization.

