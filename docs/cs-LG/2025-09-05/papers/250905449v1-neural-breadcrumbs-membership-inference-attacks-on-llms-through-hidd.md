---
layout: default
title: Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis
---

# Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05449" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05449v1</a>
  <a href="https://arxiv.org/pdf/2509.05449.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05449v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05449v1', 'Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Disha Makhija, Manoj Ghuhan Arivazhagan, Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºmemTraceæ¡†æ¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„æˆå‘˜æ¨æ–­æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æˆå‘˜æ¨æ–­æ”»å‡»` `éšç§ä¿æŠ¤` `å¤§è¯­è¨€æ¨¡å‹` `å†…éƒ¨è¡¨ç¤ºåˆ†æ` `æ³¨æ„åŠ›æœºåˆ¶` `æ·±åº¦å­¦ä¹ ` `éšç§å®¡è®¡` `åˆè§„è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œæˆå‘˜æ¨æ–­æ”»å‡»çš„æ•ˆæœæœ‰é™ï¼Œå¾€å¾€ä»…ç•¥ä¼˜äºéšæœºçŒœæµ‹ï¼Œè¡¨æ˜éšç§æ³„éœ²é£é™©å¯èƒ½è¢«ä½ä¼°ã€‚
2. æœ¬æ–‡æå‡ºçš„memTraceæ¡†æ¶é€šè¿‡åˆ†ææ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œæå–éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æ¨¡å¼ä¸­çš„ä¿¡æ¯ä¿¡å·ï¼Œä»¥å¢å¼ºæˆå‘˜æ¨æ–­çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒmemTraceåœ¨å¤šä¸ªæ¨¡å‹å®¶æ—ä¸Šå®ç°äº†å¹³å‡AUCå¾—åˆ†0.85ï¼Œæ˜¾è‘—æé«˜äº†æˆå‘˜æ¨æ–­çš„æ£€æµ‹èƒ½åŠ›ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAï¼‰æ­ç¤ºç‰¹å®šæ•°æ®æ˜¯å¦ç”¨äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ˜¯éšç§å®¡è®¡å’Œåˆè§„è¯„ä¼°çš„é‡è¦å·¥å…·ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼ŒMIAåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°ä»…ç•¥ä¼˜äºéšæœºçŒœæµ‹ï¼Œæš—ç¤ºç°ä»£é¢„è®­ç»ƒæ–¹æ³•å¯èƒ½ä¸å­˜åœ¨éšç§æ³„éœ²é£é™©ã€‚æœ¬æ–‡é€šè¿‡åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œæå‡ºäº†memTraceæ¡†æ¶ï¼Œæå–å˜æ¢å™¨éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æ¨¡å¼ä¸­çš„ä¿¡æ¯ä¿¡å·ã€‚é€šè¿‡åˆ†æå±‚çº§è¡¨ç¤ºåŠ¨æ€ã€æ³¨æ„åŠ›åˆ†å¸ƒç‰¹å¾å’Œè·¨å±‚è½¬ç§»æ¨¡å¼ï¼Œæ£€æµ‹æ½œåœ¨çš„è®°å¿†æŒ‡çº¹ï¼Œå–å¾—äº†åœ¨å¤šä¸ªæ¨¡å‹å®¶æ—ä¸Šçš„å¼ºæˆå‘˜æ£€æµ‹æ•ˆæœï¼Œå¹³å‡AUCå¾—åˆ†è¾¾åˆ°0.85ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†å†…éƒ¨æ¨¡å‹è¡Œä¸ºåœ¨è®­ç»ƒæ•°æ®æš´éœ²æ–¹é¢çš„æ½œåœ¨æŒ‡ç¤ºï¼Œå‘¼åè¿›ä¸€æ­¥ç ”ç©¶æˆå‘˜éšç§åŠå¼€å‘æ›´å¼ºçš„éšç§ä¿æŠ¤è®­ç»ƒæŠ€æœ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æˆå‘˜æ¨æ–­æ”»å‡»é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ­¤é¢†åŸŸçš„è¡¨ç°æœ‰é™ï¼Œæœªèƒ½æœ‰æ•ˆæ•æ‰æ¨¡å‹å†…éƒ¨çš„éšç§æ³„éœ²ä¿¡å·ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æ¨¡å¼ï¼Œæå–æ½œåœ¨çš„æˆå‘˜æ¨æ–­ä¿¡å·ï¼Œä»è€Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šmemTraceæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) éšè—çŠ¶æ€åˆ†æï¼Œ2) æ³¨æ„åŠ›æ¨¡å¼æå–ï¼Œ3) è·¨å±‚è½¬ç§»æ¨¡å¼æ£€æµ‹ï¼Œæ•´ä½“æµç¨‹ä¸ºè¾“å…¥å€™é€‰åºåˆ—ï¼Œé€å±‚åˆ†æå…¶å†…éƒ¨è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡å±‚çº§è¡¨ç¤ºåŠ¨æ€å’Œæ³¨æ„åŠ›åˆ†å¸ƒç‰¹å¾çš„åˆ†æï¼Œè¯†åˆ«å‡ºä¼ ç»ŸæŸå¤±åŸºæ–¹æ³•æ— æ³•æ•æ‰çš„è®°å¿†æŒ‡çº¹ï¼Œä»è€Œå®ç°æ›´å¼ºçš„æˆå‘˜æ¨æ–­èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å¤šå±‚æ¬¡çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆä¼ ç»ŸæŸå¤±ä¸æ–°æå–ä¿¡å·çš„å¤åˆæŸå¤±ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚å…·ä½“ç½‘ç»œç»“æ„ä¸ºåŸºäºå˜æ¢å™¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¢å¼ºäº†å¯¹å†…éƒ¨è¡¨ç¤ºçš„è§£æèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒmemTraceæ¡†æ¶åœ¨å¤šä¸ªæ¨¡å‹å®¶æ—ä¸Šå®ç°äº†å¹³å‡AUCå¾—åˆ†0.85ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨æˆå‘˜æ¨æ–­æ”»å‡»ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚è¿™ä¸€æˆæœä¸ºéšç§ä¿æŠ¤æŠ€æœ¯çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨éšç§ä¿æŠ¤é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç¡®ä¿æ•°æ®å®‰å…¨æ€§çš„æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸­ã€‚é€šè¿‡æ·±å…¥ç†è§£æ¨¡å‹çš„å†…éƒ¨è¡Œä¸ºï¼Œèƒ½å¤Ÿä¸ºéšç§å®¡è®¡ã€åˆè§„è¯„ä¼°å’Œå®‰å…¨æ€§å¢å¼ºæä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæ¨åŠ¨æ›´å®‰å…¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Membership inference attacks (MIAs) reveal whether specific data was used to train machine learning models, serving as important tools for privacy auditing and compliance assessment. Recent studies have reported that MIAs perform only marginally better than random guessing against large language models, suggesting that modern pre-training approaches with massive datasets may be free from privacy leakage risks. Our work offers a complementary perspective to these findings by exploring how examining LLMs' internal representations, rather than just their outputs, may provide additional insights into potential membership inference signals. Our framework, \emph{memTrace}, follows what we call \enquote{neural breadcrumbs} extracting informative signals from transformer hidden states and attention patterns as they process candidate sequences. By analyzing layer-wise representation dynamics, attention distribution characteristics, and cross-layer transition patterns, we detect potential memorization fingerprints that traditional loss-based approaches may not capture. This approach yields strong membership detection across several model families achieving average AUC scores of 0.85 on popular MIA benchmarks. Our findings suggest that internal model behaviors can reveal aspects of training data exposure even when output-based signals appear protected, highlighting the need for further research into membership privacy and the development of more robust privacy-preserving training techniques for large language models.

