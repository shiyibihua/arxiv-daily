---
layout: default
title: Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations
---

# Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05186" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05186v2</a>
  <a href="https://arxiv.org/pdf/2509.05186.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05186v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05186v2', 'Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Benjamin J. Zhang, Siting Liu, Stanley J. Osher, Markos A. Katsoulakis

**åˆ†ç±»**: stat.ML, cs.LG, math.NA

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-09-08)

**å¤‡æ³¨**: First two authors contributed equally

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGenICONï¼Œé€šè¿‡ç”Ÿæˆå»ºæ¨¡å’Œä¸ç¡®å®šæ€§é‡åŒ–æå‡å¾®åˆ†æ–¹ç¨‹åŸºç¡€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç®—å­å­¦ä¹ ` `å¾®åˆ†æ–¹ç¨‹` `ä¸ç¡®å®šæ€§é‡åŒ–` `ç”Ÿæˆæ¨¡å‹` `è´å¶æ–¯æ¨æ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç®—å­å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¾®åˆ†æ–¹ç¨‹æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œä¸”ç¼ºä¹å¯¹é¢„æµ‹ç»“æœä¸ç¡®å®šæ€§çš„æœ‰æ•ˆè¯„ä¼°ã€‚
2. è®ºæ–‡æå‡ºGenICONï¼Œé€šè¿‡æ¦‚ç‡å»ºæ¨¡å°†ICONè§†ä¸ºè´å¶æ–¯æ¨æ–­ï¼Œå¹¶å¼•å…¥ç”Ÿæˆæ¨¡å‹ä»¥æ•è·è§£ç®—å­çš„ä¸ç¡®å®šæ€§ã€‚
3. GenICONèƒ½å¤Ÿä»è§£ç®—å­çš„åéªŒé¢„æµ‹åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œå®ç°å¯¹è§£é¢„æµ‹çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæå‡äº†æ¨¡å‹çš„å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¦‚ç‡æ¡†æ¶ï¼Œæ­ç¤ºäº†ä¸Šä¸‹æ–‡ç®—å­ç½‘ç»œ(ICON)éšå¼åœ°æ‰§è¡Œè´å¶æ–¯æ¨æ–­ï¼Œè®¡ç®—ç»™å®šä¸Šä¸‹æ–‡ï¼ˆå³æ¡ä»¶-è§£ç¤ºä¾‹å¯¹ï¼‰ä¸‹è§£ç®—å­åéªŒé¢„æµ‹åˆ†å¸ƒçš„å‡å€¼ã€‚éšæœºå¾®åˆ†æ–¹ç¨‹çš„å½¢å¼ä¸ºæè¿°ICONæ‰€å®Œæˆçš„ä»»åŠ¡æä¾›äº†ä¸€ä¸ªæ¦‚ç‡æ¡†æ¶ï¼ŒåŒæ—¶ä¹Ÿä¸ºç†è§£å…¶ä»–å¤šç®—å­å­¦ä¹ æ–¹æ³•æä¾›äº†åŸºç¡€ã€‚è¿™ç§æ¦‚ç‡è§†è§’ä¸ºå°†ICONæ‰©å±•åˆ°ç”Ÿæˆè®¾ç½®æä¾›äº†åŸºç¡€ï¼Œåœ¨ç”Ÿæˆè®¾ç½®ä¸­ï¼Œå¯ä»¥ä»è§£ç®—å­çš„åéªŒé¢„æµ‹åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚ICONçš„ç”Ÿæˆå…¬å¼(GenICON)æ•æ‰äº†è§£ç®—å­ä¸­æ½œåœ¨çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œåœ¨ç®—å­å­¦ä¹ çš„è§£é¢„æµ‹ä¸­å®ç°æœ‰åŸåˆ™çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç®—å­å­¦ä¹ æ–¹æ³•åœ¨æ±‚è§£å¾®åˆ†æ–¹ç¨‹æ—¶ï¼Œç¼ºä¹å¯¹è§£çš„ä¸ç¡®å®šæ€§è¯„ä¼°çš„é—®é¢˜ã€‚ç°æœ‰çš„ICONæ–¹æ³•è™½ç„¶èƒ½å¤Ÿå­¦ä¹ å¾®åˆ†æ–¹ç¨‹çš„è§£ç®—å­ï¼Œä½†æ— æ³•æä¾›è§£çš„ç½®ä¿¡åº¦ä¿¡æ¯ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ICONæ–¹æ³•ç½®äºè´å¶æ–¯æ¨æ–­çš„æ¡†æ¶ä¸‹ï¼Œå°†å…¶è§†ä¸ºè®¡ç®—è§£ç®—å­åéªŒé¢„æµ‹åˆ†å¸ƒå‡å€¼çš„è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥éšæœºå¾®åˆ†æ–¹ç¨‹çš„å½¢å¼ï¼Œå°†è§£ç®—å­çš„å­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªæ¦‚ç‡å»ºæ¨¡é—®é¢˜ï¼Œä»è€Œèƒ½å¤Ÿå¯¹è§£çš„ä¸ç¡®å®šæ€§è¿›è¡Œé‡åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGenICONçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨ICONå­¦ä¹ å¾®åˆ†æ–¹ç¨‹çš„è§£ç®—å­ï¼›2) å°†ICONçš„å­¦ä¹ è¿‡ç¨‹è§£é‡Šä¸ºè´å¶æ–¯æ¨æ–­ï¼Œå¾—åˆ°è§£ç®—å­çš„åéªŒåˆ†å¸ƒï¼›3) æ„å»ºç”Ÿæˆæ¨¡å‹ï¼Œä»è§£ç®—å­çš„åéªŒåˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”Ÿæˆå¤šä¸ªå¯èƒ½çš„è§£ï¼›4) åˆ©ç”¨ç”Ÿæˆçš„å¤šä¸ªè§£ï¼Œè¯„ä¼°è§£çš„ä¸ç¡®å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†ICONæ–¹æ³•ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œæå‡ºäº†GenICONã€‚GenICONèƒ½å¤Ÿæ•è·è§£ç®—å­ä¸­æ½œåœ¨çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶æä¾›å¯¹è§£é¢„æµ‹çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚ä¸ä¼ ç»Ÿçš„ICONæ–¹æ³•ç›¸æ¯”ï¼ŒGenICONä¸ä»…èƒ½å¤Ÿé¢„æµ‹å¾®åˆ†æ–¹ç¨‹çš„è§£ï¼Œè¿˜èƒ½å¤Ÿè¯„ä¼°è§£çš„ç½®ä¿¡åº¦ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„å¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šGenICONçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨éšæœºå¾®åˆ†æ–¹ç¨‹çš„å½¢å¼æ¥æè¿°è§£ç®—å­çš„å­¦ä¹ é—®é¢˜ï¼›2) æ„å»ºåˆé€‚çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚å˜åˆ†è‡ªç¼–ç å™¨(VAE)æˆ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)ï¼Œä»è§£ç®—å­çš„åéªŒåˆ†å¸ƒä¸­é‡‡æ ·ï¼›3) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±æˆ–å¯¹æŠ—æŸå¤±ï¼Œæ¥è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼›4) é€‰æ‹©åˆé€‚çš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚Transformeræˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼Œæ¥å®ç°ICONå’Œç”Ÿæˆæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†GenICONçš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜ï¼ŒGenICONä¸ä»…èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹å¾®åˆ†æ–¹ç¨‹çš„è§£ï¼Œè¿˜èƒ½å¤Ÿæœ‰æ•ˆåœ°é‡åŒ–è§£çš„ä¸ç¡®å®šæ€§ã€‚ä¸ä¼ ç»Ÿçš„ICONæ–¹æ³•ç›¸æ¯”ï¼ŒGenICONåœ¨é¢„æµ‹ç²¾åº¦å’Œä¸ç¡®å®šæ€§é‡åŒ–æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GenICONå¯åº”ç”¨äºç§‘å­¦è®¡ç®—ã€å·¥ç¨‹è®¾è®¡ã€é‡‘èå»ºæ¨¡ç­‰é¢†åŸŸï¼Œä¾‹å¦‚ï¼Œåœ¨é£è¡Œå™¨è®¾è®¡ä¸­ï¼Œå¯ä»¥åˆ©ç”¨GenICONé¢„æµ‹ä¸åŒæ°”åŠ¨æ¡ä»¶ä¸‹çš„é£è¡Œå™¨æ€§èƒ½ï¼Œå¹¶è¯„ä¼°é¢„æµ‹ç»“æœçš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œæé«˜è®¾è®¡çš„å®‰å…¨æ€§ã€‚åœ¨é‡‘èé£é™©ç®¡ç†ä¸­ï¼Œå¯ä»¥åˆ©ç”¨GenICONé¢„æµ‹å¸‚åœºæ³¢åŠ¨ï¼Œå¹¶é‡åŒ–é¢„æµ‹é£é™©ï¼Œè¾…åŠ©å†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context operator networks (ICON) are a class of operator learning methods based on the novel architectures of foundation models. Trained on a diverse set of datasets of initial and boundary conditions paired with corresponding solutions to ordinary and partial differential equations (ODEs and PDEs), ICON learns to map example condition-solution pairs of a given differential equation to an approximation of its solution operator. Here, we present a probabilistic framework that reveals ICON as implicitly performing Bayesian inference, where it computes the mean of the posterior predictive distribution over solution operators conditioned on the provided context, i.e., example condition-solution pairs. The formalism of random differential equations provides the probabilistic framework for describing the tasks ICON accomplishes while also providing a basis for understanding other multi-operator learning methods. This probabilistic perspective provides a basis for extending ICON to \emph{generative} settings, where one can sample from the posterior predictive distribution of solution operators. The generative formulation of ICON (GenICON) captures the underlying uncertainty in the solution operator, which enables principled uncertainty quantification in the solution predictions in operator learning.

