---
layout: default
title: Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning
---

# Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15230" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15230v1</a>
  <a href="https://arxiv.org/pdf/2509.15230.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15230v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15230v1', 'Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rutger Hendrix, Giovanni PatanÃ¨, Leonardo G. Russo, Simone Carnemolla, Giovanni Bellitto, Federica Proietto Salanitri, Concetto Spampinato, Matteo Pennisi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

**å¤‡æ³¨**: Accepted at ACM multimedia 2025 BNI track

**DOI**: [10.1145/3746027.3758171](https://doi.org/10.1145/3746027.3758171)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºPromptå­¦ä¹ çš„é¢„å…ˆå¯é—å¿˜æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆã€å®‰å…¨çš„çŸ¥è¯†ç§»é™¤ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯é—å¿˜å­¦ä¹ ` `Promptå­¦ä¹ ` `çŸ¥è¯†é—å¿˜` `éšç§ä¿æŠ¤` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å–æ¶ˆå­¦ä¹ æ–¹æ³•ï¼ˆå¦‚é‡è®­ç»ƒï¼‰è®¡ç®—æˆæœ¬é«˜ï¼Œéš¾ä»¥é€‚åº”å®æ—¶ç³»ç»Ÿï¼Œæ— æ³•æ»¡è¶³éšç§æ³•è§„ï¼ˆå¦‚GDPRï¼‰å¯¹æ•°æ®é—å¿˜çš„éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºåŸºäºPromptå­¦ä¹ çš„é¢„å…ˆå¯é—å¿˜æ¨¡å‹ï¼Œå°†çŸ¥è¯†ä¸Prompt Tokenç»‘å®šï¼Œé€šè¿‡ç§»é™¤Promptå®ç°çŸ¥è¯†é—å¿˜ï¼Œæ— éœ€é‡è®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™å‰©ä½™ç±»åˆ«æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆæ“¦é™¤ç›®æ ‡ç±»åˆ«çŸ¥è¯†ï¼Œå¹¶å…·å¤‡æŠ—æˆå‘˜æ¨ç†æ”»å‡»çš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºç¡€æ¨¡å‹å·²ç»é€šè¿‡åœ¨ä¸åŒæ¨¡æ€å’Œä»»åŠ¡ä¸­å®ç°é²æ£’ä¸”å¯è¿ç§»çš„è¡¨ç¤ºï¼Œä»è€Œæ”¹å˜äº†å¤šåª’ä½“åˆ†æã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„é™æ€éƒ¨ç½²ä¸æ—¥ç›Šå¢é•¿çš„ç¤¾ä¼šå’Œç›‘ç®¡éœ€æ±‚ç›¸å†²çªâ€”â€”ç‰¹åˆ«æ˜¯æ ¹æ®GDPRç­‰éšç§æ¡†æ¶çš„è¦æ±‚ï¼Œéœ€è¦åº”è¦æ±‚å–æ¶ˆå­¦ä¹ ç‰¹å®šæ•°æ®ã€‚ä¼ ç»Ÿçš„å–æ¶ˆå­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬é‡æ–°è®­ç»ƒã€æ¿€æ´»ç¼–è¾‘æˆ–è’¸é¦ï¼Œé€šå¸¸è®¡ç®—æˆæœ¬é«˜æ˜‚ã€è„†å¼±ä¸”ä¸é€‚åˆå®æ—¶æˆ–ä¸æ–­å‘å±•çš„ç³»ç»Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§èŒƒå¼è½¬å˜ï¼šå°†å–æ¶ˆå­¦ä¹ é‡æ–°æ€è€ƒä¸ºä¸€ç§å†…ç½®èƒ½åŠ›ï¼Œè€Œä¸æ˜¯ä¸€ç§è¿½æº¯å¹²é¢„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºpromptçš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å•ä¸ªè®­ç»ƒé˜¶æ®µç»Ÿä¸€äº†çŸ¥è¯†è·å–å’Œç§»é™¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯å°†ä¿¡æ¯ç¼–ç åœ¨æ¨¡å‹æƒé‡ä¸­ï¼Œè€Œæ˜¯å°†ç±»çº§åˆ«çš„è¯­ä¹‰ç»‘å®šåˆ°ä¸“ç”¨çš„prompt tokenã€‚è¿™ç§è®¾è®¡åªéœ€åˆ é™¤ç›¸åº”çš„promptå³å¯å®ç°å³æ—¶å–æ¶ˆå­¦ä¹ â€”â€”æ— éœ€é‡æ–°è®­ç»ƒã€æ¨¡å‹ä¿®æ”¹æˆ–è®¿é—®åŸå§‹æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¿ç•™ç±»ä¸Šçš„é¢„æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°æ“¦é™¤äº†è¢«é—å¿˜çš„ç±»ã€‚é™¤äº†å®ç”¨æ€§ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜è¡¨ç°å‡ºå¼ºå¤§çš„éšç§å’Œå®‰å…¨ä¿è¯ï¼šå®ƒèƒ½å¤ŸæŠµæŠ—æˆå‘˜æ¨ç†æ”»å‡»ï¼Œå¹¶ä¸”promptåˆ é™¤å¯ä»¥é˜²æ­¢ä»»ä½•æ®‹ç•™çŸ¥è¯†æå–ï¼Œå³ä½¿åœ¨å¯¹æŠ—æ¡ä»¶ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™ç¡®ä¿äº†ç¬¦åˆæ•°æ®ä¿æŠ¤åŸåˆ™ï¼Œå¹¶é˜²æ­¢æœªç»æˆæƒè®¿é—®è¢«é—å¿˜çš„ä¿¡æ¯ï¼Œä½¿è¯¥æ¡†æ¶é€‚åˆåœ¨æ•æ„Ÿå’Œå—ç›‘ç®¡çš„ç¯å¢ƒä¸­éƒ¨ç½²ã€‚æ€»çš„æ¥è¯´ï¼Œé€šè¿‡å°†å¯ç§»é™¤æ€§åµŒå…¥åˆ°æ¶æ„æœ¬èº«ä¸­ï¼Œè¿™é¡¹å·¥ä½œä¸ºè®¾è®¡æ¨¡å—åŒ–ã€å¯æ‰©å±•å’Œç¬¦åˆä¼¦ç†çš„AIæ¨¡å‹å¥ å®šäº†æ–°çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸºç¡€æ¨¡å‹åœ¨éƒ¨ç½²åéš¾ä»¥é«˜æ•ˆã€å®‰å…¨åœ°é—å¿˜ç‰¹å®šæ•°æ®çš„éš¾é¢˜ã€‚ç°æœ‰å–æ¶ˆå­¦ä¹ æ–¹æ³•ï¼Œå¦‚é‡è®­ç»ƒã€æ¿€æ´»ç¼–è¾‘ç­‰ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”å¯èƒ½å½±å“æ¨¡å‹åœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•éš¾ä»¥ä¿è¯å®Œå…¨ç§»é™¤ç›®æ ‡æ•°æ®ï¼Œå­˜åœ¨éšç§æ³„éœ²é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†çŸ¥è¯†çš„å­˜å‚¨ä¸æ¨¡å‹çš„æƒé‡è§£è€¦ï¼Œè½¬è€Œå°†ç±»çº§åˆ«çš„è¯­ä¹‰ä¿¡æ¯ç»‘å®šåˆ°ç‰¹å®šçš„Prompt Tokenä¸Šã€‚é€šè¿‡ç§»é™¤è¿™äº›Prompt Tokenï¼Œå³å¯å®ç°å¯¹ç›¸åº”ç±»åˆ«çš„çŸ¥è¯†é—å¿˜ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹çš„æ•´ä½“æƒé‡ã€‚è¿™ç§æ–¹æ³•å°†é—å¿˜æ“ä½œç®€åŒ–ä¸ºPromptçš„ç§»é™¤ï¼Œä»è€Œå®ç°é«˜æ•ˆä¸”å®‰å…¨çš„çŸ¥è¯†é—å¿˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) Prompt Tokenåˆå§‹åŒ–ï¼šä¸ºæ¯ä¸ªç±»åˆ«åˆå§‹åŒ–ä¸€ä¸ªæˆ–å¤šä¸ªPrompt Tokenï¼›2) Promptå­¦ä¹ ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å­¦ä¹ å°†æ¯ä¸ªç±»åˆ«çš„è¯­ä¹‰ä¿¡æ¯ç¼–ç åˆ°å¯¹åº”çš„Prompt Tokenä¸­ï¼›3) çŸ¥è¯†é—å¿˜ï¼šé€šè¿‡ç§»é™¤ä¸ç›®æ ‡ç±»åˆ«ç›¸å…³çš„Prompt Tokenï¼Œå®ç°å¯¹è¯¥ç±»åˆ«çŸ¥è¯†çš„é—å¿˜ï¼›4) æ¨¡å‹æ¨ç†ï¼šä½¿ç”¨å‰©ä½™çš„Prompt Tokenè¿›è¡Œæ¨ç†ï¼Œæ¨¡å‹ä»…èƒ½è¯†åˆ«å’Œé¢„æµ‹æœªè¢«é—å¿˜çš„ç±»åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å–æ¶ˆå­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºPrompt Tokençš„ç§»é™¤ï¼Œä»è€Œé¿å…äº†å¯¹æ¨¡å‹æƒé‡çš„ä¿®æ”¹ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†é—å¿˜æ•ˆç‡ï¼Œè¿˜é™ä½äº†é—å¿˜æ“ä½œå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å…·å¤‡æ›´å¼ºçš„éšç§ä¿æŠ¤èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŠµæŠ—æˆå‘˜æ¨ç†æ”»å‡»ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) Prompt Tokençš„æ•°é‡ï¼šæ¯ä¸ªç±»åˆ«åˆ†é…çš„Prompt Tokenæ•°é‡ä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½å’Œé—å¿˜æ•ˆæœï¼›2) Promptå­¦ä¹ ç­–ç•¥ï¼šå¦‚ä½•æœ‰æ•ˆåœ°å°†ç±»åˆ«çš„è¯­ä¹‰ä¿¡æ¯ç¼–ç åˆ°Prompt Tokenä¸­ï¼›3) æŸå¤±å‡½æ•°è®¾è®¡ï¼šå¦‚ä½•å¹³è¡¡æ¨¡å‹åœ¨ä¿ç•™ç±»åˆ«ä¸Šçš„æ€§èƒ½å’Œé—å¿˜ç±»åˆ«ä¸Šçš„é—å¿˜æ•ˆæœï¼›4) å¯¹æŠ—æ”»å‡»é˜²å¾¡ï¼šè®¾è®¡ç›¸åº”çš„æœºåˆ¶æ¥é˜²å¾¡é’ˆå¯¹Prompt Tokençš„å¯¹æŠ—æ”»å‡»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CIFAR-10å’ŒCIFAR-100æ•°æ®é›†ä¸Šï¼Œèƒ½å¤Ÿåœ¨æœ‰æ•ˆé—å¿˜ç›®æ ‡ç±»åˆ«çš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹åœ¨å‰©ä½™ç±»åˆ«ä¸Šçš„é¢„æµ‹å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿæœ‰æ•ˆæŠµæŠ—æˆå‘˜æ¨ç†æ”»å‡»ï¼Œè¯æ˜äº†å…¶è‰¯å¥½çš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„é‡è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨é—å¿˜æ•ˆç‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦æ•°æ®é—å¿˜åŠŸèƒ½çš„å„ç§åœºæ™¯ï¼Œä¾‹å¦‚ï¼š1) ä¿æŠ¤ç”¨æˆ·éšç§ï¼Œåœ¨ç”¨æˆ·è¦æ±‚åˆ é™¤ä¸ªäººæ•°æ®æ—¶ï¼Œå¿«é€Ÿä¸”å®‰å…¨åœ°ç§»é™¤æ¨¡å‹ä¸­çš„ç›¸å…³ä¿¡æ¯ï¼›2) åº”å¯¹æ³•è§„è¦æ±‚ï¼Œæ»¡è¶³GDPRç­‰éšç§æ³•è§„å¯¹æ•°æ®é—å¿˜çš„å¼ºåˆ¶æ€§è¦æ±‚ï¼›3) æ¨¡å‹æ›´æ–°ä¸ç»´æŠ¤ï¼Œåœ¨æ¨¡å‹éœ€è¦ç§»é™¤è¿‡æ—¶æˆ–é”™è¯¯çŸ¥è¯†æ—¶ï¼Œé«˜æ•ˆåœ°è¿›è¡ŒçŸ¥è¯†æ›´æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models have transformed multimedia analysis by enabling robust and transferable representations across diverse modalities and tasks. However, their static deployment conflicts with growing societal and regulatory demands -- particularly the need to unlearn specific data upon request, as mandated by privacy frameworks such as the GDPR. Traditional unlearning approaches, including retraining, activation editing, or distillation, are often computationally expensive, fragile, and ill-suited for real-time or continuously evolving systems. In this paper, we propose a paradigm shift: rethinking unlearning not as a retroactive intervention but as a built-in capability. We introduce a prompt-based learning framework that unifies knowledge acquisition and removal within a single training phase. Rather than encoding information in model weights, our approach binds class-level semantics to dedicated prompt tokens. This design enables instant unlearning simply by removing the corresponding prompt -- without retraining, model modification, or access to original data. Experiments demonstrate that our framework preserves predictive performance on retained classes while effectively erasing forgotten ones. Beyond utility, our method exhibits strong privacy and security guarantees: it is resistant to membership inference attacks, and prompt removal prevents any residual knowledge extraction, even under adversarial conditions. This ensures compliance with data protection principles and safeguards against unauthorized access to forgotten information, making the framework suitable for deployment in sensitive and regulated environments. Overall, by embedding removability into the architecture itself, this work establishes a new foundation for designing modular, scalable and ethically responsive AI models.

