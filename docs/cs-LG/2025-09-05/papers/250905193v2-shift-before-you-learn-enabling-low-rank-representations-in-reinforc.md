---
layout: default
title: Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning
---

# Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05193" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05193v2</a>
  <a href="https://arxiv.org/pdf/2509.05193.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05193v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05193v2', 'Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bastien Dubail, Stefan Stojanovic, Alexandre ProutiÃ¨re

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-11-05)

**å¤‡æ³¨**: 63 pages, 11 figures. Accepted to NeurIPS 2025 (Spotlight)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè½¬ç§»åç»§æµ‹åº¦çš„ä½ç§©å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ä½ç§©è¡¨ç¤º` `åç»§æµ‹åº¦` `ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ` `è°±å¯æ¢å¤æ€§` `é©¬å°”å¯å¤«é“¾` `è½¬ç§»å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…å¥–åŠ±å’Œç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•å‡è®¾åç»§æµ‹åº¦å…·æœ‰ä½ç§©ç»“æ„ï¼Œä½†å®é™…åç»§æµ‹åº¦å¹¶éè¿‘ä¼¼ä½ç§©ã€‚
2. è®ºæ–‡æå‡ºè½¬ç§»åç»§æµ‹åº¦çš„æ¦‚å¿µï¼Œè¯æ˜ä½ç§©ç»“æ„åœ¨è½¬ç§»åçš„åç»§æµ‹åº¦ä¸­è‡ªç„¶å‡ºç°ï¼Œä»è€Œç»•è¿‡åˆå§‹è½¬ç§»ã€‚
3. å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜è½¬ç§»åç»§æµ‹åº¦èƒ½å¤Ÿæé«˜ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šç°ä»£å¼ºåŒ–å­¦ä¹ ç®—æ³•éƒ½éšå«åœ°å‡è®¾äº†ä½ç§©ç»“æ„ã€‚ä¾‹å¦‚ï¼Œå…å¥–åŠ±å’Œç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸å‡å®šåç»§æµ‹åº¦å…è®¸ä½ç§©è¡¨ç¤ºã€‚æœ¬æ–‡æŒ‘æˆ˜äº†è¿™ä¸€å‡è®¾ï¼Œé¦–å…ˆæŒ‡å‡ºåç»§æµ‹åº¦æœ¬èº«å¹¶éè¿‘ä¼¼ä½ç§©çš„ã€‚ç›¸åï¼Œæˆ‘ä»¬è¯æ˜äº†ä½ç§©ç»“æ„è‡ªç„¶å‡ºç°åœ¨è½¬ç§»åçš„åç»§æµ‹åº¦ä¸­ï¼Œè¯¥æµ‹åº¦æ•è·äº†ç»•è¿‡ä¸€äº›åˆå§‹è½¬ç§»åçš„ç³»ç»ŸåŠ¨æ€ã€‚æˆ‘ä»¬ä¸ºä»é‡‡æ ·æ¡ç›®ä¸­å¯¹è½¬ç§»åç»§æµ‹åº¦çš„ä½ç§©è¿‘ä¼¼è¿›è¡Œé€æ¡ç›®ä¼°è®¡æä¾›äº†æœ‰é™æ ·æœ¬æ€§èƒ½ä¿è¯ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¿‘ä¼¼è¯¯å·®å’Œä¼°è®¡è¯¯å·®ä¸»è¦å—ä¸€ä¸ªæ–°å¼•å…¥çš„é‡æ§åˆ¶ï¼šç›¸åº”çŸ©é˜µçš„è°±å¯æ¢å¤æ€§ã€‚ä¸ºäº†é™åˆ¶è¿™ä¸ªå‚æ•°ï¼Œæˆ‘ä»¬ä¸ºé©¬å°”å¯å¤«é“¾æ¨å¯¼äº†ä¸€ç±»æ–°çš„å‡½æ•°ä¸ç­‰å¼ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºIIå‹åºåŠ è±ä¸ç­‰å¼ï¼Œç”±æ­¤æˆ‘ä»¬å¯ä»¥é‡åŒ–æœ‰æ•ˆä½ç§©è¿‘ä¼¼å’Œä¼°è®¡æ‰€éœ€çš„è½¬ç§»é‡ã€‚è¯¥åˆ†æç‰¹åˆ«è¡¨æ˜ï¼Œæ‰€éœ€çš„è½¬ç§»å–å†³äºè½¬ç§»åç»§æµ‹åº¦çš„é«˜é˜¶å¥‡å¼‚å€¼çš„è¡°å‡ï¼Œå› æ­¤åœ¨å®è·µä¸­é€šå¸¸å¾ˆå°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†å¿…è¦è½¬ç§»ä¸åº•å±‚åŠ¨åŠ›ç³»ç»Ÿçš„å±€éƒ¨æ··åˆç‰¹æ€§ä¹‹é—´çš„è”ç³»ï¼Œè¿™æä¾›äº†ä¸€ç§é€‰æ‹©è½¬ç§»çš„è‡ªç„¶æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºç»“æœï¼Œå¹¶è¯æ˜è½¬ç§»åç»§æµ‹åº¦ç¡®å®å¯ä»¥æé«˜ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯å…å¥–åŠ±å’Œç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼Œé€šå¸¸å‡è®¾åç»§æµ‹åº¦å…·æœ‰ä½ç§©ç»“æ„ï¼Œä»¥ä¾¿è¿›è¡Œæœ‰æ•ˆçš„å­¦ä¹ å’Œæ³›åŒ–ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åç»§æµ‹åº¦è¿›è¡Œä½ç§©è¿‘ä¼¼å¾€å¾€æ•ˆæœä¸ä½³ï¼Œå› ä¸ºåŸå§‹åç»§æµ‹åº¦æœ¬èº«å¹¶ä¸å…·å¤‡è¿‘ä¼¼ä½ç§©çš„æ€§è´¨ã€‚è¿™é™åˆ¶äº†è¿™äº›ç®—æ³•çš„æ€§èƒ½å’Œé€‚ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œè™½ç„¶åŸå§‹åç»§æµ‹åº¦ä¸å…·å¤‡ä½ç§©æ€§ï¼Œä½†ç»è¿‡ä¸€å®šæ­¥æ•°çš„è½¬ç§»ï¼ˆshiftï¼‰åå¾—åˆ°çš„è½¬ç§»åç»§æµ‹åº¦ï¼Œä¼šå‘ˆç°å‡ºæ›´æ˜æ˜¾çš„ä½ç§©ç»“æ„ã€‚é€šè¿‡å¯¹è½¬ç§»åçš„åç»§æµ‹åº¦è¿›è¡Œä½ç§©è¿‘ä¼¼ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°å­¦ä¹ å’Œè¡¨ç¤ºç¯å¢ƒçš„åŠ¨æ€ç‰¹æ€§ã€‚è¿™æ ·åšçš„åŸå› æ˜¯ï¼Œåˆå§‹çŠ¶æ€å¯èƒ½åŒ…å«å™ªå£°æˆ–ä¸ç›¸å…³çš„ä¿¡æ¯ï¼Œè€Œç»è¿‡è½¬ç§»åï¼Œç³»ç»Ÿä¼šé€æ¸ç¨³å®šåˆ°æ›´å…·ä»£è¡¨æ€§çš„çŠ¶æ€ï¼Œä»è€Œä½¿å¾—åç»§æµ‹åº¦çš„ç§©é™ä½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š
1. **é‡‡æ ·**ï¼šä»ç¯å¢ƒä¸­æ”¶é›†çŠ¶æ€è½¬ç§»æ ·æœ¬ã€‚
2. **è½¬ç§»**ï¼šå¯¹æ”¶é›†åˆ°çš„æ ·æœ¬è¿›è¡Œä¸€å®šæ­¥æ•°çš„è½¬ç§»ï¼Œå¾—åˆ°è½¬ç§»åçš„çŠ¶æ€è½¬ç§»æ ·æœ¬ã€‚
3. **åç»§æµ‹åº¦ä¼°è®¡**ï¼šåˆ©ç”¨è½¬ç§»åçš„æ ·æœ¬ä¼°è®¡è½¬ç§»åç»§æµ‹åº¦ã€‚
4. **ä½ç§©è¿‘ä¼¼**ï¼šå¯¹ä¼°è®¡å¾—åˆ°çš„è½¬ç§»åç»§æµ‹åº¦è¿›è¡Œä½ç§©è¿‘ä¼¼ã€‚
5. **ç­–ç•¥å­¦ä¹ **ï¼šåˆ©ç”¨ä½ç§©è¿‘ä¼¼çš„åç»§æµ‹åº¦è¿›è¡Œç­–ç•¥å­¦ä¹ ï¼Œä¾‹å¦‚ä½¿ç”¨ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†â€œè½¬ç§»åç»§æµ‹åº¦â€çš„æ¦‚å¿µï¼Œå¹¶è¯æ˜äº†å…¶ä½ç§©æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„åº¦é‡æŒ‡æ ‡â€œè°±å¯æ¢å¤æ€§â€ï¼Œç”¨äºè¡¡é‡è½¬ç§»åç»§æµ‹åº¦çš„ä½ç§©è¿‘ä¼¼æ•ˆæœã€‚è®ºæ–‡è¿˜æ¨å¯¼äº†IIå‹åºåŠ è±ä¸ç­‰å¼ï¼Œç”¨äºé‡åŒ–æœ‰æ•ˆä½ç§©è¿‘ä¼¼å’Œä¼°è®¡æ‰€éœ€çš„è½¬ç§»é‡ï¼Œå¹¶å»ºç«‹äº†è½¬ç§»é‡ä¸åº•å±‚åŠ¨åŠ›ç³»ç»Ÿçš„å±€éƒ¨æ··åˆç‰¹æ€§ä¹‹é—´çš„è”ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼š
1. **è½¬ç§»æ­¥æ•°é€‰æ‹©**ï¼šè®ºæ–‡é€šè¿‡åˆ†æè½¬ç§»åç»§æµ‹åº¦çš„é«˜é˜¶å¥‡å¼‚å€¼çš„è¡°å‡æƒ…å†µï¼Œä»¥åŠåº•å±‚åŠ¨åŠ›ç³»ç»Ÿçš„å±€éƒ¨æ··åˆç‰¹æ€§ï¼Œæ¥æŒ‡å¯¼è½¬ç§»æ­¥æ•°çš„é€‰æ‹©ã€‚
2. **ä½ç§©è¿‘ä¼¼æ–¹æ³•**ï¼šå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„ä½ç§©çŸ©é˜µåˆ†è§£æ–¹æ³•ï¼Œå¦‚å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æˆ–æ ¸èŒƒæ•°æœ€å°åŒ–ï¼Œå¯¹è½¬ç§»åç»§æµ‹åº¦è¿›è¡Œä½ç§©è¿‘ä¼¼ã€‚
3. **è°±å¯æ¢å¤æ€§åº¦é‡**ï¼šä½¿ç”¨è°±å¯æ¢å¤æ€§æ¥è¯„ä¼°ä½ç§©è¿‘ä¼¼çš„è´¨é‡ï¼Œå¹¶ä½œä¸ºè°ƒæ•´è½¬ç§»æ­¥æ•°çš„ä¾æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡è½¬ç§»åç»§æµ‹åº¦è¿›è¡Œä½ç§©è¿‘ä¼¼ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å¤šä¸ªç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨è½¬ç§»åç»§æµ‹åº¦çš„æ–¹æ³•ç›¸æ¯”äºç›´æ¥ä½¿ç”¨åŸå§‹åç»§æµ‹åº¦çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶è¾¾åˆ°æ›´é«˜çš„ç´¯ç§¯å¥–åŠ±ã€‚å®éªŒè¿˜éªŒè¯äº†ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ï¼Œå³è½¬ç§»æ­¥æ•°çš„é€‰æ‹©ä¸è½¬ç§»åç»§æµ‹åº¦çš„ä½ç§©æ€§å’Œè°±å¯æ¢å¤æ€§å¯†åˆ‡ç›¸å…³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆçŠ¶æ€è¡¨ç¤ºå’Œæ³›åŒ–çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ã€æœºå™¨äººå¯¼èˆªã€æ¸¸æˆAIç­‰é¢†åŸŸå…·æœ‰æ½œåŠ›ã€‚é€šè¿‡åˆ©ç”¨è½¬ç§»åç»§æµ‹åº¦çš„ä½ç§©ç‰¹æ€§ï¼Œå¯ä»¥é™ä½ç®—æ³•çš„è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ï¼Œå¹¶å¢å¼ºç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„æ™ºèƒ½ä½“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Low-rank structure is a common implicit assumption in many modern reinforcement learning (RL) algorithms. For instance, reward-free and goal-conditioned RL methods often presume that the successor measure admits a low-rank representation. In this work, we challenge this assumption by first remarking that the successor measure itself is not approximately low-rank. Instead, we demonstrate that a low-rank structure naturally emerges in the shifted successor measure, which captures the system dynamics after bypassing a few initial transitions. We provide finite-sample performance guarantees for the entry-wise estimation of a low-rank approximation of the shifted successor measure from sampled entries. Our analysis reveals that both the approximation and estimation errors are primarily governed by a newly introduced quantitity: the spectral recoverability of the corresponding matrix. To bound this parameter, we derive a new class of functional inequalities for Markov chains that we call Type II PoincarÃ© inequalities and from which we can quantify the amount of shift needed for effective low-rank approximation and estimation. This analysis shows in particular that the required shift depends on decay of the high-order singular values of the shifted successor measure and is hence typically small in practice. Additionally, we establish a connection between the necessary shift and the local mixing properties of the underlying dynamical system, which provides a natural way of selecting the shift. Finally, we validate our theoretical findings with experiments, and demonstrate that shifting the successor measure indeed leads to improved performance in goal-conditioned RL.

