---
layout: default
title: "A Comedy of Estimators: On KL Regularization in RL Training of LLMs"
---

# A Comedy of Estimators: On KL Regularization in RL Training of LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.21852" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.21852v1</a>
  <a href="https://arxiv.org/pdf/2512.21852.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.21852v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.21852v1', 'A Comedy of Estimators: On KL Regularization in RL Training of LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶KLæ•£åº¦ä¼°è®¡å™¨å¯¹LLMçš„RLè®­ç»ƒå½±å“ï¼Œæå‡æ¨¡å‹åœ¨åˆ†å¸ƒå†…å¤–çš„æ³›åŒ–æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `KLæ•£åº¦` `æ¢¯åº¦åå·®` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMçš„RLè®­ç»ƒä¸­ï¼ŒKLæ•£åº¦ä¼°è®¡ä¸å‡†ç¡®å¯¼è‡´æ¢¯åº¦åå·®ï¼Œå½±å“æ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚
2. åˆ†æä¸åŒKLæ•£åº¦ä¼°è®¡å™¨çš„æ¢¯åº¦åå·®ï¼Œå¹¶æå‡ºä½¿ç”¨æ— åæ¢¯åº¦ä¼°è®¡å™¨æ¥ä¼˜åŒ–RLè®­ç»ƒè¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æ— åæ¢¯åº¦ä¼°è®¡å™¨èƒ½æå‡LLMåœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ç¨³å®šè®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ€§èƒ½å¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒå¾—åˆ°æ˜¾è‘—æå‡ã€‚LLMè®­ç»ƒçš„RLç›®æ ‡åŒ…å«ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œå³è®­ç»ƒç­–ç•¥ä¸å‚è€ƒç­–ç•¥ä¹‹é—´çš„åå‘Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦ã€‚ç”±äºç²¾ç¡®è®¡ç®—KLæ•£åº¦æ˜¯éš¾ä»¥å®ç°çš„ï¼Œå› æ­¤å®è·µä¸­é€šå¸¸ä½¿ç”¨å„ç§ä¼°è®¡å™¨ä»on-policyæ ·æœ¬ä¸­å¯¹å…¶è¿›è¡Œä¼°è®¡ã€‚å°½ç®¡KLæ•£åº¦ä¼°è®¡å™¨è¢«å¹¿æ³›é‡‡ç”¨ï¼ŒåŒ…æ‹¬åœ¨å¤šä¸ªå¼€æºåº“ä¸­ï¼Œä½†ç›®å‰è¿˜æ²¡æœ‰ç³»ç»Ÿçš„ç ”ç©¶æ¥åˆ†æå°†KLä¼°è®¡å™¨çº³å…¥ç›®æ ‡å‡½æ•°çš„å„ç§æ–¹å¼åŠå…¶å¯¹RLè®­ç»ƒæ¨¡å‹ä¸‹æ¸¸æ€§èƒ½çš„å½±å“ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç›®å‰é‡‡ç”¨çš„KLæ­£åˆ™åŒ–æ–¹æ³•å¹¶æ²¡æœ‰ä¸ºæ—¢å®šç›®æ ‡æä¾›æ­£ç¡®çš„æ¢¯åº¦ï¼Œä»è€Œåœ¨ç›®æ ‡å‡½æ•°å’Œå®ç°ä¹‹é—´é€ æˆäº†å·®å¼‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†è¿™äº›å®è·µï¼Œå¹¶ç ”ç©¶äº†å‡ ç§ä¼°è®¡å™¨é…ç½®çš„æ¢¯åº¦ï¼Œæ­ç¤ºäº†è®¾è®¡é€‰æ‹©å¦‚ä½•å½±å“æ¢¯åº¦åå·®ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¸åŒçš„é…ç½®å¯¹	exttt{Qwen2.5-7B}ã€	exttt{Llama-3.1-8B-Instruct}å’Œ	exttt{Qwen3-4B-Instruct-2507}è¿›è¡ŒRLå¾®è°ƒï¼Œå¹¶åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ä»»åŠ¡ä¸Šè¯„ä¼°å…¶æ€§èƒ½ï¼Œä»è€Œè¯å®äº†è¿™äº›å‘ç°ã€‚é€šè¿‡æˆ‘ä»¬çš„åˆ†æï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨on-policyè®¾ç½®ä¸­ï¼šï¼ˆ1ï¼‰å…·æœ‰åå·®æ¢¯åº¦çš„ä¼°è®¡å™¨é…ç½®å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼›ï¼ˆ2ï¼‰ä½¿ç”¨äº§ç”Ÿæ— åæ¢¯åº¦çš„ä¼°è®¡å™¨é…ç½®å¯ä»¥æé«˜åœ¨åŸŸå†…å’ŒåŸŸå¤–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†åœ¨off-policyè®¾ç½®ä¸­ä¸åŒKLé…ç½®æ‰€äº§ç”Ÿçš„æ€§èƒ½ï¼Œå¹¶è§‚å¯Ÿåˆ°KLæ­£åˆ™åŒ–å¯ä»¥å¸®åŠ©ç¨³å®šç”±å¼‚æ­¥è®¾ç½®å¯¼è‡´çš„off-policy RLè®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒæ—¶ï¼Œç”±äºKLæ•£åº¦ä¼°è®¡ä¸å‡†ç¡®è€Œå¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰çš„KLæ•£åº¦ä¼°è®¡æ–¹æ³•å­˜åœ¨æ¢¯åº¦åå·®ï¼Œä½¿å¾—å®é™…ä¼˜åŒ–ç›®æ ‡ä¸ç†è®ºç›®æ ‡ä¸ä¸€è‡´ï¼Œä»è€Œå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†æä¸åŒKLæ•£åº¦ä¼°è®¡å™¨çš„æ¢¯åº¦åå·®ï¼Œæ‰¾åˆ°èƒ½å¤Ÿäº§ç”Ÿæ— åæ¢¯åº¦çš„ä¼°è®¡å™¨é…ç½®ï¼Œå¹¶å°†å…¶åº”ç”¨äºLLMçš„RLè®­ç»ƒä¸­ã€‚é€šè¿‡ä½¿ç”¨æ— åæ¢¯åº¦ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ä¼˜åŒ–RLç›®æ ‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚è®ºæ–‡è¿˜ç ”ç©¶äº†åœ¨off-policyè®¾ç½®ä¸‹KLæ­£åˆ™åŒ–çš„ä½œç”¨ï¼Œå‘ç°å®ƒå¯ä»¥å¸®åŠ©ç¨³å®šå¼‚æ­¥è®¾ç½®ä¸‹çš„RLè®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) åˆ†æä¸åŒKLæ•£åº¦ä¼°è®¡å™¨çš„æ¢¯åº¦åå·®ï¼›2) è®¾è®¡å®éªŒï¼Œä½¿ç”¨ä¸åŒçš„KLä¼°è®¡å™¨é…ç½®å¯¹LLMè¿›è¡ŒRLå¾®è°ƒï¼›3) åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ä»»åŠ¡ä¸Šè¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½ï¼›4) ç ”ç©¶KLæ­£åˆ™åŒ–åœ¨off-policy RLè®­ç»ƒä¸­çš„ä½œç”¨ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬KLæ•£åº¦ä¼°è®¡æ¨¡å—ã€RLè®­ç»ƒæ¨¡å—å’Œæ€§èƒ½è¯„ä¼°æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¯¹ä¸åŒKLæ•£åº¦ä¼°è®¡å™¨çš„æ¢¯åº¦åå·®è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶æ­ç¤ºäº†æ¢¯åº¦åå·®å¯¹LLMçš„RLè®­ç»ƒçš„å½±å“ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä½¿ç”¨æ— åæ¢¯åº¦ä¼°è®¡å™¨æ¥ä¼˜åŒ–RLè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¼˜åŒ–RLç›®æ ‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„KLæ•£åº¦ä¼°è®¡å™¨ï¼Œä¾‹å¦‚ä½¿ç”¨èƒ½å¤Ÿäº§ç”Ÿæ— åæ¢¯åº¦çš„ä¼°è®¡å™¨ï¼›2) è®¾è®¡åˆé€‚çš„RLè®­ç»ƒç›®æ ‡ï¼ŒåŒ…æ‹¬å¥–åŠ±å‡½æ•°å’ŒKLæ­£åˆ™åŒ–ç³»æ•°ï¼›3) é€‰æ‹©åˆé€‚çš„LLMæ¶æ„å’Œæ•°æ®é›†è¿›è¡Œå®éªŒï¼›4) ä½¿ç”¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›5) åœ¨off-policyè®¾ç½®ä¸‹ç ”ç©¶KLæ­£åˆ™åŒ–çš„ä½œç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ— åæ¢¯åº¦ä¼°è®¡å™¨é…ç½®è¿›è¡ŒRLå¾®è°ƒåï¼ŒLLMåœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ä»»åŠ¡ä¸Šçš„æ€§èƒ½å‡å¾—åˆ°æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œä½¿ç”¨æ— åæ¢¯åº¦ä¼°è®¡å™¨èƒ½å¤Ÿç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶ä½¿æ¨¡å‹åœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œæ¨¡å‹æ€§èƒ½æå‡äº†X%ï¼Œè¯æ˜äº†æ— åæ¢¯åº¦ä¼°è®¡å™¨çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å„ç§å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒçš„åœºæ™¯ä¸­ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºä¼˜åŒ–å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å’Œä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œæé«˜å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¯¹äºå¼€å‘æ›´ç¨³å®šå’Œé«˜æ•ˆçš„LLMè®­ç»ƒæ–¹æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \texttt{Qwen2.5-7B}, \texttt{Llama-3.1-8B-Instruct} and \texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.

