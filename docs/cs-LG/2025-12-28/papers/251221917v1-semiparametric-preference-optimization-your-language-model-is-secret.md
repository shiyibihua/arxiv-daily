---
layout: default
title: "Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model"
---

# Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.21917" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.21917v1</a>
  <a href="https://arxiv.org/pdf/2512.21917.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.21917v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.21917v1', 'Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nathan Kallus

**åˆ†ç±»**: cs.LG, cs.AI, econ.EM, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-12-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠå‚æ•°åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œè§£å†³è¯­è¨€æ¨¡å‹å¯¹é½ä¸­é“¾æ¥å‡½æ•°æœªçŸ¥çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹å¯¹é½` `åå¥½ä¼˜åŒ–` `åŠå‚æ•°æ¨¡å‹` `å•æŒ‡æ ‡æ¨¡å‹` `ç­–ç•¥å­¦ä¹ ` `f-æ•£åº¦` `å¥–åŠ±æœ€å¤§åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰çš„é“¾æ¥å‡½æ•°æ¥å¯¹é½è¯­è¨€æ¨¡å‹ä¸åå¥½æ•°æ®ï¼Œä½†é”™è¯¯çš„é“¾æ¥å‡½æ•°ä¼šå¯¼è‡´åå·®å’Œç­–ç•¥é”™ä½ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŠå‚æ•°åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œæ— éœ€é¢„å…ˆæŒ‡å®šé“¾æ¥å‡½æ•°ï¼Œé€šè¿‡å•æŒ‡æ ‡æ¨¡å‹æ•æ‰åå¥½åˆ†å¸ƒçš„ä¾èµ–å…³ç³»ã€‚
3. è¯¥æ–¹æ³•å¼€å‘äº†å¤šç§ç­–ç•¥å­¦ä¹ å™¨ï¼Œå¹¶æä¾›äº†æœ‰é™æ ·æœ¬è¯¯å·®ç•Œé™ï¼ŒåŒæ—¶é€‚ç”¨äºç¥ç»ç½‘ç»œå’Œæ‰¹é‡æ•°æ®ï¼Œæé«˜äº†é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸åå¥½æ•°æ®å¯¹é½é€šå¸¸å‡è®¾è§‚å¯Ÿåˆ°çš„åå¥½åˆ†å¸ƒä¸æœªè§‚å¯Ÿåˆ°çš„å¥–åŠ±ä¹‹é—´å­˜åœ¨å·²çŸ¥çš„é“¾æ¥å‡½æ•°ï¼ˆä¾‹å¦‚ï¼ŒBradley-Terry ä¸­çš„ Logistic é“¾æ¥ï¼‰ã€‚ç„¶è€Œï¼Œå¦‚æœé“¾æ¥å‡½æ•°ä¸æ­£ç¡®ï¼Œæ¨æ–­å‡ºçš„å¥–åŠ±å¯èƒ½ä¼šæœ‰åå·®ï¼Œå¹¶ä¸”ç­–ç•¥å¯èƒ½ä¼šé”™ä½ã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨æœªçŸ¥å’Œä¸å—é™åˆ¶çš„é“¾æ¥å‡½æ•°ä¸‹ï¼Œç­–ç•¥ä¸åå¥½å¯¹é½çš„é—®é¢˜ã€‚æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ª $f$-æ•£åº¦çº¦æŸçš„å¥–åŠ±æœ€å¤§åŒ–é—®é¢˜ï¼Œå¹¶è¡¨æ˜ç­–ç•¥ç±»ä¸­è§£çš„å¯å®ç°æ€§æ„å‘³ç€ä¸€ä¸ªåŠå‚æ•°å•æŒ‡æ ‡äºŒå…ƒé€‰æ‹©æ¨¡å‹ï¼Œå…¶ä¸­ç”±ç­–ç•¥ç¡®å®šçš„æ ‡é‡å€¼æŒ‡æ ‡æ•è·å¯¹æ¼”ç¤ºçš„ä¾èµ–æ€§ï¼Œå¹¶ä¸”åå¥½åˆ†å¸ƒçš„å…¶ä½™éƒ¨åˆ†æ˜¯å®ƒçš„ä¸€ä¸ªä¸å—é™åˆ¶çš„å‡½æ•°ã€‚ä¸ç»æµè®¡é‡å­¦ä¸­å…³æ³¨æŒ‡æ ‡ä¸­å¯è¯†åˆ«çš„æœ‰é™ç»´ç»“æ„å‚æ•°çš„ä¼°è®¡ä¸åŒï¼Œæˆ‘ä»¬ä¸“æ³¨äºç­–ç•¥å­¦ä¹ ï¼Œå…³æ³¨æœ€ä¼˜ç­–ç•¥çš„è¯¯å·®ï¼Œå¹¶å…è®¸ä¸å¯è¯†åˆ«å’Œéå‚æ•°æŒ‡æ ‡ã€‚æˆ‘ä»¬å¼€å‘äº†å„ç§åŸºäºåˆ†æé“¾æ¥å‡½æ•°ã€æ­£äº¤åŒ–é“¾æ¥å‡½æ•°å’Œä½¿ç”¨ä¸é“¾æ¥æ— å…³çš„åŒè¾¹æ’åºç›®æ ‡çš„ç­–ç•¥å­¦ä¹ å™¨ã€‚æˆ‘ä»¬åˆ†æäº†è¿™äº›å­¦ä¹ å™¨ï¼Œå¹¶æä¾›äº†ä¾èµ–äºæŒ‡æ ‡ç±»çš„ä¸€èˆ¬å‡½æ•°å¤æ‚æ€§åº¦é‡çš„æœ‰é™æ ·æœ¬ç­–ç•¥è¯¯å·®ç•Œé™ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è€ƒè™‘äº†ä½¿ç”¨é€‚ç”¨äºç¥ç»ç½‘ç»œå’Œæ‰¹é‡æ•°æ®çš„ä¸€é˜¶ä¼˜åŒ–çš„å®é™…å®ç°ã€‚ç”±æ­¤äº§ç”Ÿçš„æ–¹æ³•å¯¹æœªçŸ¥çš„åå¥½å™ªå£°åˆ†å¸ƒå’Œå°ºåº¦å…·æœ‰é²æ£’æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†ç­–ç•¥çš„ç›´æ¥ä¼˜åŒ–ï¼Œè€Œæ— éœ€æ˜¾å¼æ‹Ÿåˆå¥–åŠ±ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å…ˆè®¾å®šçš„é“¾æ¥å‡½æ•°æ¥å…³è”è§‚å¯Ÿåˆ°çš„åå¥½å’Œæœªè§‚å¯Ÿåˆ°çš„å¥–åŠ±ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œå¦‚æœé€‰æ‹©çš„é“¾æ¥å‡½æ•°ä¸çœŸå®æƒ…å†µä¸ç¬¦ï¼Œé‚£ä¹ˆæ¨æ–­å‡ºçš„å¥–åŠ±å°†ä¼šäº§ç”Ÿåå·®ï¼Œæœ€ç»ˆå¯¼è‡´ç­–ç•¥çš„é”™ä½ï¼Œå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨é“¾æ¥å‡½æ•°æœªçŸ¥çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„ç­–ç•¥å¯¹é½æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç­–ç•¥å¯¹é½é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªåŠå‚æ•°å•æŒ‡æ ‡äºŒå…ƒé€‰æ‹©æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒåœ¨äºä½¿ç”¨ä¸€ä¸ªæ ‡é‡å€¼çš„æŒ‡æ ‡æ¥æ•æ‰ç­–ç•¥å¯¹æ¼”ç¤ºæ•°æ®çš„ä¾èµ–å…³ç³»ï¼Œè€Œåå¥½åˆ†å¸ƒçš„å…¶ä½™éƒ¨åˆ†åˆ™è¢«è§†ä¸ºè¯¥æŒ‡æ ‡çš„ä¸€ä¸ªä¸å—é™åˆ¶çš„å‡½æ•°ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹é“¾æ¥å‡½æ•°çš„å…·ä½“å½¢å¼è¿›è¡Œå‡è®¾ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼šé¦–å…ˆï¼Œå®šä¹‰ä¸€ä¸ª $f$-æ•£åº¦çº¦æŸçš„å¥–åŠ±æœ€å¤§åŒ–é—®é¢˜ã€‚ç„¶åï¼Œè¯æ˜åœ¨è¯¥é—®é¢˜è§£çš„å¯å®ç°æ€§æ¡ä»¶ä¸‹ï¼Œå¯ä»¥æ¨å¯¼å‡ºåŠå‚æ•°å•æŒ‡æ ‡æ¨¡å‹ã€‚æ¥ç€ï¼ŒåŸºäºè¯¥æ¨¡å‹ï¼Œå¼€å‘å¤šç§ç­–ç•¥å­¦ä¹ å™¨ï¼ŒåŒ…æ‹¬åŸºäºåˆ†æé“¾æ¥å‡½æ•°ã€æ­£äº¤åŒ–é“¾æ¥å‡½æ•°å’Œä½¿ç”¨ä¸é“¾æ¥æ— å…³çš„åŒè¾¹æ’åºç›®æ ‡çš„å­¦ä¹ å™¨ã€‚æœ€åï¼Œå¯¹è¿™äº›å­¦ä¹ å™¨è¿›è¡Œåˆ†æï¼Œå¹¶æä¾›æœ‰é™æ ·æœ¬ç­–ç•¥è¯¯å·®ç•Œé™ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŠå‚æ•°å•æŒ‡æ ‡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…è®¸é“¾æ¥å‡½æ•°æ˜¯æœªçŸ¥çš„å’Œéå‚æ•°çš„ï¼Œä»è€Œé¿å…äº†å¯¹é“¾æ¥å‡½æ•°çš„é”™è¯¯å‡è®¾ã€‚ä¸ä¼ ç»Ÿçš„ç»æµè®¡é‡å­¦æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡ä¾§é‡äºç­–ç•¥å­¦ä¹ ï¼Œè€Œä¸æ˜¯å¯¹æŒ‡æ ‡ä¸­çš„å¯è¯†åˆ«å‚æ•°è¿›è¡Œä¼°è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ $f$-æ•£åº¦çº¦æŸæ¥æ§åˆ¶ç­–ç•¥çš„åå·®ï¼›2) å¼€å‘äº†å¤šç§ç­–ç•¥å­¦ä¹ å™¨ï¼Œä»¥é€‚åº”ä¸åŒçš„åœºæ™¯ï¼›3) æä¾›äº†æœ‰é™æ ·æœ¬ç­–ç•¥è¯¯å·®ç•Œé™ï¼Œç”¨äºè¯„ä¼°å­¦ä¹ å™¨çš„æ€§èƒ½ï¼›4) è€ƒè™‘äº†ä½¿ç”¨ä¸€é˜¶ä¼˜åŒ–ç®—æ³•çš„å®é™…å®ç°ï¼Œä½¿å…¶é€‚ç”¨äºç¥ç»ç½‘ç»œå’Œæ‰¹é‡æ•°æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡çš„ä¸»è¦å®éªŒç»“æœé›†ä¸­åœ¨ç†è®ºåˆ†æä¸Šï¼Œæä¾›äº†æœ‰é™æ ·æœ¬ç­–ç•¥è¯¯å·®ç•Œé™ï¼Œè¯æ˜äº†æ‰€æå‡ºçš„ç­–ç•¥å­¦ä¹ å™¨çš„æœ‰æ•ˆæ€§ã€‚è™½ç„¶æ²¡æœ‰æä¾›å…·ä½“çš„æ•°å€¼ç»“æœï¼Œä½†ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœªçŸ¥é“¾æ¥å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°å¯¹æœ€ä¼˜ç­–ç•¥çš„æœ‰æ•ˆé€¼è¿‘ï¼Œå¹¶ä¸”å¯¹åå¥½å™ªå£°å…·æœ‰é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œåå¥½å¯¹é½çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹ç”¨æˆ·åå¥½çš„ç†è§£å’Œé€‚åº”èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œæ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯¹æœªçŸ¥åå¥½å™ªå£°çš„é²æ£’æ€§ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´å…·ä¼˜åŠ¿ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Aligning large language models to preference data is commonly implemented by assuming a known link function between the distribution of observed preferences and the unobserved rewards (e.g., a logistic link as in Bradley-Terry). If the link is wrong, however, inferred rewards can be biased and policies be misaligned. We study policy alignment to preferences under an unknown and unrestricted link. We consider an $f$-divergence-constrained reward maximization problem and show that realizability of the solution in a policy class implies a semiparametric single-index binary choice model, where a scalar-valued index determined by a policy captures the dependence on demonstrations and the rest of the preference distribution is an unrestricted function thereof. Rather than focus on estimation of identifiable finite-dimensional structural parameters in the index as in econometrics, we focus on policy learning, focusing on error to the optimal policy and allowing unidentifiable and nonparametric indices. We develop a variety of policy learners based on profiling the link function, orthogonalizing the link function, and using link-agnostic bipartite ranking objectives. We analyze these and provide finite-sample policy error bounds that depend on generic functional complexity measures of the index class. We further consider practical implementations using first-order optimization suited to neural networks and batched data. The resulting methods are robust to unknown preference noise distribution and scale, while preserving the direct optimization of policies without explicitly fitting rewards.

