---
layout: default
title: Unifying Learning Dynamics and Generalization in Transformers Scaling Law
---

# Unifying Learning Dynamics and Generalization in Transformers Scaling Law

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.22088" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.22088v1</a>
  <a href="https://arxiv.org/pdf/2512.22088.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.22088v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.22088v1', 'Unifying Learning Dynamics and Generalization in Transformers Scaling Law')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chiwun Yang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€å­¦ä¹ åŠ¨æ€ä¸å˜å‹å™¨ç¼©æ”¾æ³•åˆ™ä»¥æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¼©æ”¾æ³•åˆ™` `å­¦ä¹ åŠ¨æ€` `éšæœºæ¢¯åº¦ä¸‹é™` `æ³›åŒ–èƒ½åŠ›` `å¸¸å¾®åˆ†æ–¹ç¨‹` `ç›¸å˜ç‰¹å¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¼©æ”¾æ³•åˆ™è™½ç„¶åœ¨ç»éªŒä¸Šæœ‰æ•ˆï¼Œä½†å…¶ç†è®ºåŸºç¡€å°šä¸æ¸…æ™°ï¼Œå¯¼è‡´å¯¹æ¨¡å‹æ€§èƒ½æå‡çš„ç†è§£ä¸è¶³ã€‚
2. æœ¬æ–‡é€šè¿‡å°†å˜å‹å™¨æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€å½¢å¼åŒ–ä¸ºå¸¸å¾®åˆ†æ–¹ç¨‹ï¼Œåˆ†æäº†éšæœºæ¢¯åº¦ä¸‹é™è®­ç»ƒè¿‡ç¨‹ï¼Œæä¾›äº†æ›´æ·±å…¥çš„ç†è®ºæ¡†æ¶ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ç‰¹å®šèµ„æºé˜ˆå€¼ä¸‹ï¼Œæ³›åŒ–è¯¯å·®çš„è¡°å‡æ¨¡å¼å‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œæå‡ºäº†æ–°çš„ç‹¬ç«‹ç¼©æ”¾æ³•åˆ™ï¼Œå¢å¼ºäº†å¯¹æ¨¡å‹æ€§èƒ½çš„ç†è§£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¼©æ”¾æ³•åˆ™æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•çš„åŸºçŸ³ï¼Œé¢„æµ‹éšç€è®¡ç®—èµ„æºçš„å¢åŠ ï¼Œæ¨¡å‹æ€§èƒ½å°†å¾—åˆ°æå‡ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™ä¸€ç†è®ºåœ¨ç»éªŒä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œå…¶ç†è®ºåŸºç¡€ä»ç„¶ä¸å¤Ÿæ¸…æ™°ã€‚æœ¬æ–‡å°†åŸºäºå˜å‹å™¨çš„è¯­è¨€æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€å½¢å¼åŒ–ä¸ºå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ç³»ç»Ÿï¼Œå¹¶å°†è¿™ä¸€è¿‡ç¨‹è¿‘ä¼¼ä¸ºæ ¸è¡Œä¸ºã€‚ä¸ä»¥å¾€çš„ç©å…·æ¨¡å‹åˆ†æä¸åŒï¼Œæˆ‘ä»¬ä¸¥æ ¼åˆ†æäº†åœ¨ä»»æ„æ•°æ®åˆ†å¸ƒä¸‹çš„å¤šå±‚å˜å‹å™¨çš„éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰è®­ç»ƒï¼Œç´§å¯†è´´åˆç°å®ä¸–ç•Œæ¡ä»¶ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨å¾äº†åœ¨è®¡ç®—èµ„æºä¸æ•°æ®è§„æ¨¡æ‰©å±•æ—¶ï¼Œæ³›åŒ–è¯¯å·®æ”¶æ•›åˆ°ä¸å¯çº¦é£é™©çš„è¿‡ç¨‹ï¼Œå°¤å…¶æ˜¯åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…³äºè¿‡åº¦é£é™©çš„ç†è®ºä¸Šé™ï¼Œå¹¶é€šè¿‡æ˜¾è‘—çš„ç›¸å˜ç‰¹å¾è¿›è¡Œæè¿°ã€‚åœ¨åˆå§‹ä¼˜åŒ–é˜¶æ®µï¼Œè¿‡åº¦é£é™©ç›¸å¯¹äºè®¡ç®—æˆæœ¬å‘ˆæŒ‡æ•°è¡°å‡ï¼›è€Œä¸€æ—¦è·¨è¶Šç‰¹å®šèµ„æºé…ç½®é˜ˆå€¼ï¼Œç³»ç»Ÿè¿›å…¥ç»Ÿè®¡é˜¶æ®µï¼Œæ³›åŒ–è¯¯å·®éµå¾ªå¹‚å¾‹è¡°å‡ã€‚æˆ‘ä»¬çš„ç†è®ºè¿˜æ¨å¯¼å‡ºæ¨¡å‹è§„æ¨¡ã€è®­ç»ƒæ—¶é—´å’Œæ•°æ®é›†è§„æ¨¡çš„ç‹¬ç«‹ç¼©æ”¾æ³•åˆ™ï¼Œé˜æ˜äº†æ¯ä¸ªå˜é‡å¦‚ä½•ç‹¬ç«‹åœ°æ”¯é…æ³›åŒ–çš„ä¸Šé™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ç¼©æ”¾æ³•åˆ™çš„ç†è®ºåŸºç¡€ä¸æ¸…æ™°çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•å……åˆ†è§£é‡Šæ¨¡å‹æ€§èƒ½æå‡çš„æœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†å˜å‹å™¨æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€å½¢å¼åŒ–ä¸ºå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ï¼Œå¹¶åˆ†æéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰è®­ç»ƒè¿‡ç¨‹ï¼Œæä¾›äº†å¯¹æ³›åŒ–è¯¯å·®ä¸è®¡ç®—èµ„æºå…³ç³»çš„æ·±å…¥ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å°†å­¦ä¹ åŠ¨æ€å»ºæ¨¡ä¸ºODEç³»ç»Ÿï¼Œåˆ†æåœ¨ä¸åŒèµ„æºé…ç½®ä¸‹çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œé‡ç‚¹å…³æ³¨åˆå§‹ä¼˜åŒ–é˜¶æ®µä¸ç»Ÿè®¡é˜¶æ®µçš„ç›¸å˜ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå»ºç«‹äº†ä¸€ä¸ªç†è®ºä¸Šé™ï¼Œæè¿°äº†è¿‡åº¦é£é™©çš„ç›¸å˜ç‰¹å¾ï¼Œæ­ç¤ºäº†æ³›åŒ–è¯¯å·®åœ¨ä¸åŒé˜¶æ®µçš„è¡°å‡è§„å¾‹ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å…¨é¢çš„ç†è®ºè§£é‡Šã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œåˆ†æäº†ä¸åŒè®¡ç®—èµ„æºå¯¹æ³›åŒ–è¯¯å·®çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯èµ„æºé…ç½®é˜ˆå€¼çš„è®¾å®šå¯¹æ³›åŒ–èƒ½åŠ›çš„å½±å“ã€‚é€šè¿‡ç†è®ºæ¨å¯¼ï¼Œæ˜ç¡®äº†æ¨¡å‹è§„æ¨¡ã€è®­ç»ƒæ—¶é—´å’Œæ•°æ®é›†è§„æ¨¡çš„ç‹¬ç«‹ç¼©æ”¾æ³•åˆ™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åˆå§‹ä¼˜åŒ–é˜¶æ®µï¼Œè¿‡åº¦é£é™©ç›¸å¯¹äºè®¡ç®—æˆæœ¬å‘ˆæŒ‡æ•°è¡°å‡ï¼Œè€Œåœ¨è·¨è¶Šç‰¹å®šèµ„æºé˜ˆå€¼åï¼Œæ³›åŒ–è¯¯å·®éµå¾ªå¹‚å¾‹è¡°å‡ï¼Œå…·ä½“è¡¨ç°ä¸º$Î˜(	ext{C}^{-1/6})$çš„å…³ç³»ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£æ¨¡å‹æ€§èƒ½æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ä¸æ³›åŒ–èƒ½åŠ›ï¼Œç ”ç©¶è€…å¯ä»¥åœ¨è®¾è®¡å’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹æ—¶åšå‡ºæ›´ä¸ºåˆç†çš„å†³ç­–ï¼Œä»è€Œæå‡æ¨¡å‹çš„å®é™…åº”ç”¨æ•ˆæœå’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions. Our analysis characterizes the convergence of generalization error to the irreducible risk as computational resources scale with data, especially during the optimization process.
>   We establish a theoretical upper bound on excess risk characterized by a distinct phase transition. In the initial optimization phase, the excess risk decays exponentially relative to the computational cost ${\sf C}$. However, once a specific resource allocation threshold is crossed, the system enters a statistical phase, where the generalization error follows a power-law decay of $Î˜(\mathsf{C}^{-1/6})$. Beyond this unified framework, our theory derives isolated scaling laws for model size, training time, and dataset size, elucidating how each variable independently governs the upper bounds of generalization.

