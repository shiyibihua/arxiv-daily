---
layout: default
title: Optimal Sample Complexity of Contrastive Learning
---

# Optimal Sample Complexity of Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00379" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00379v1</a>
  <a href="https://arxiv.org/pdf/2312.00379.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00379v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00379v1', 'Optimal Sample Complexity of Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Noga Alon, Dmitrii Avdiukhin, Dor Elboim, Orr Fischer, Grigory Yaroslavtsev

**åˆ†ç±»**: cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æ¯”å­¦ä¹ çš„æœ€ä¼˜æ ·æœ¬å¤æ‚åº¦ç•Œé™ä»¥æå‡æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `æ ·æœ¬å¤æ‚åº¦` `Vapnik-Chervonenkisç»´åº¦` `æ·±åº¦å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `æœºå™¨å­¦ä¹ ç†è®º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨æ ·æœ¬å¤æ‚åº¦ä¸Šç¼ºä¹æ˜ç¡®çš„ç†è®ºç•Œé™ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›çš„ä¸è¶³ã€‚
2. æœ¬æ–‡é€šè¿‡ç ”ç©¶æ ·æœ¬å¤æ‚åº¦ï¼Œæå‡ºäº†åœ¨ä»»æ„è·ç¦»å‡½æ•°ä¸‹çš„ç´§ç•Œé™ï¼Œç‰¹åˆ«æ˜¯å¯¹$	ext{l}_p$è·ç¦»çš„æœ€ä¼˜æ ·æœ¬éœ€æ±‚ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ ·æœ¬å¤æ‚åº¦çš„ç†è®ºç•Œé™ä¸å®éªŒç»“æœé«˜åº¦ä¸€è‡´ï¼ŒæŒ‘æˆ˜äº†æ·±åº¦å­¦ä¹ ä¸ç»Ÿè®¡å­¦ä¹ ç†è®ºä¹‹é—´çš„ä¼ ç»Ÿçœ‹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹æ¯”å­¦ä¹ æ˜¯ä¸€ç§æˆåŠŸçš„æ•°æ®è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯ï¼Œä¾èµ–äºæ ‡è®°å…ƒç»„æ¥æŒ‡å®šå…ƒç»„å†…çš„è·ç¦»å…³ç³»ã€‚æœ¬æ–‡ç ”ç©¶äº†å¯¹æ¯”å­¦ä¹ çš„æ ·æœ¬å¤æ‚åº¦ï¼Œå³è·å–é«˜æ³›åŒ–å‡†ç¡®ç‡æ‰€éœ€çš„æœ€å°æ ‡è®°å…ƒç»„æ•°é‡ã€‚æˆ‘ä»¬åœ¨å¤šç§è®¾ç½®ä¸‹ç»™å‡ºäº†æ ·æœ¬å¤æ‚åº¦çš„ç´§ç•Œï¼Œé‡ç‚¹å…³æ³¨ä»»æ„è·ç¦»å‡½æ•°ï¼ŒåŒ…æ‹¬ä¸€èˆ¬çš„$	ext{l}_p$è·ç¦»å’Œæ ‘åº¦é‡ã€‚ä¸»è¦ç»“æœæ˜¯å¯¹äºæ•´æ•°$p$ï¼Œå­¦ä¹ $	ext{l}_p$è·ç¦»çš„æ ·æœ¬å¤æ‚åº¦å‡ ä¹æ˜¯æœ€ä¼˜çš„ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¯¹äºä»»æ„$p 	ext{â‰¥} 1$ï¼Œå­¦ä¹ $n$ç‚¹æ•°æ®é›†çš„$d$ç»´è¡¨ç¤ºéœ€è¦ä¸”è¶³å¤Ÿ$	ilde Î˜(	ext{min}(nd,n^2))$ä¸ªæ ‡è®°å…ƒç»„ã€‚æˆ‘ä»¬çš„ç»“æœé€‚ç”¨äºè¾“å…¥æ ·æœ¬çš„ä»»æ„åˆ†å¸ƒï¼Œå¹¶åŸºäºç»™å‡ºç›¸å…³é—®é¢˜çš„Vapnik-Chervonenkis/Natarajanç»´åº¦çš„ç•Œé™ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œé€šè¿‡VC/Natarajanç»´åº¦è·å¾—çš„æ ·æœ¬å¤æ‚åº¦ç†è®ºç•Œé™å¯¹å®éªŒç»“æœå…·æœ‰å¼ºé¢„æµ‹èƒ½åŠ›ï¼Œè¿™ä¸æ·±åº¦å­¦ä¹ å®è·µä¸ç»Ÿè®¡å­¦ä¹ ç†è®ºä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·çš„ä¼ ç»Ÿè§‚ç‚¹å½¢æˆå¯¹æ¯”ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¯¹æ¯”å­¦ä¹ ä¸­çš„æ ·æœ¬å¤æ‚åº¦é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æä¾›æ˜ç¡®çš„æ ·æœ¬éœ€æ±‚ç•Œé™ï¼Œå½±å“äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†æVapnik-Chervonenkis/Natarajanç»´åº¦ï¼Œæœ¬æ–‡æå‡ºäº†åœ¨ä¸åŒè·ç¦»å‡½æ•°ä¸‹çš„æ ·æœ¬å¤æ‚åº¦ç´§ç•Œé™ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹$	ext{l}_p$è·ç¦»çš„æœ€ä¼˜ç•Œé™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå®šä¹‰äº†æ ·æœ¬å¤æ‚åº¦çš„æ•°å­¦æ¨¡å‹ï¼Œç„¶åé€šè¿‡ç†è®ºæ¨å¯¼å¾—å‡ºæ ·æœ¬å¤æ‚åº¦çš„ç•Œé™ï¼Œæœ€åé€šè¿‡å®éªŒéªŒè¯ç†è®ºç»“æœçš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæä¾›äº†å¯¹ä»»æ„è·ç¦»å‡½æ•°çš„æ ·æœ¬å¤æ‚åº¦çš„ç´§ç•Œé™ï¼Œå°¤å…¶æ˜¯å¯¹$	ext{l}_p$è·ç¦»çš„å‡ ä¹æœ€ä¼˜ç•Œé™ï¼Œå¡«è¡¥äº†ç†è®ºä¸å®è·µä¹‹é—´çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç ”ç©¶ä¸­ï¼Œé‡‡ç”¨äº†Vapnik-Chervonenkisç»´åº¦çš„ç•Œé™æ¥æ¨å¯¼æ ·æœ¬å¤æ‚åº¦ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†ç†è®ºç•Œé™çš„æœ‰æ•ˆæ€§ï¼Œç¡®ä¿äº†ç»“æœçš„æ™®é€‚æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ ·æœ¬å¤æ‚åº¦ç•Œé™ä¸å®é™…å®éªŒç»“æœé«˜åº¦ä¸€è‡´ï¼ŒéªŒè¯äº†ç†è®ºçš„æœ‰æ•ˆæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œå­¦ä¹ $d$ç»´è¡¨ç¤ºæ‰€éœ€çš„æ ‡è®°å…ƒç»„æ•°é‡åœ¨$n$ç‚¹æ•°æ®é›†ä¸Šè¾¾åˆ°äº†$	ilde Î˜(	ext{min}(nd,n^2))$ï¼Œæ˜¾è‘—æå‡äº†å¯¹æ¯”å­¦ä¹ çš„åº”ç”¨æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ¨èç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸä¸­çš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹æä¾›ç†è®ºæ”¯æŒï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨æ•ˆæœã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„å­¦ä¹ ç®—æ³•çš„å¼€å‘ï¼Œä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Contrastive learning is a highly successful technique for learning representations of data from labeled tuples, specifying the distance relations within the tuple. We study the sample complexity of contrastive learning, i.e. the minimum number of labeled tuples sufficient for getting high generalization accuracy. We give tight bounds on the sample complexity in a variety of settings, focusing on arbitrary distance functions, both general $\ell_p$-distances, and tree metrics. Our main result is an (almost) optimal bound on the sample complexity of learning $\ell_p$-distances for integer $p$. For any $p \ge 1$ we show that $\tilde Î˜(\min(nd,n^2))$ labeled tuples are necessary and sufficient for learning $d$-dimensional representations of $n$-point datasets. Our results hold for an arbitrary distribution of the input samples and are based on giving the corresponding bounds on the Vapnik-Chervonenkis/Natarajan dimension of the associated problems. We further show that the theoretical bounds on sample complexity obtained via VC/Natarajan dimension can have strong predictive power for experimental results, in contrast with the folklore belief about a substantial gap between the statistical learning theory and the practice of deep learning.

