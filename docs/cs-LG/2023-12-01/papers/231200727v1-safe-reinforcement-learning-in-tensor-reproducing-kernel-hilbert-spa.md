---
layout: default
title: Safe Reinforcement Learning in Tensor Reproducing Kernel Hilbert Space
---

# Safe Reinforcement Learning in Tensor Reproducing Kernel Hilbert Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00727" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00727v1</a>
  <a href="https://arxiv.org/pdf/2312.00727.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00727v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00727v1', 'Safe Reinforcement Learning in Tensor Reproducing Kernel Hilbert Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyuan Cheng, Boli Chen, Liz Varga, Yukun Hu

**åˆ†ç±»**: cs.LG, cs.AI, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºRKHSçš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è§£å†³éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­çš„å®‰å…¨é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒ` `å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´` `é¢„æµ‹çŠ¶æ€è¡¨ç¤º` `è´å¶æ–¯æ»¤æ³¢` `å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦` `ç³»ç»ŸåŠ¨æ€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹åœ¨å®‰å…¨æ€§ä¿éšœä¸Šå­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨è¿ç»­çŠ¶æ€ç©ºé—´ä¸­å‡†ç¡®ä¼°è®¡æ½œåœ¨çŠ¶æ€çš„ä¿¡å¿µã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºéšæœºæ¨¡å‹çš„æ–¹æ³•ï¼Œç»“åˆé¢„æµ‹çŠ¶æ€è¡¨ç¤ºå’Œå†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼Œç¡®ä¿åœ¨æœªçŸ¥åŠ¨æ€ä¸‹çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨å‡è®¾çš„æ¬ å®Œå¤‡æ€§ä¸‹ï¼Œç®—æ³•å±•ç¤ºäº†å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦ï¼Œç¡®ä¿äº†å¯¹æ— é™è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´çš„å®‰å…¨ç­–ç•¥ä¿è¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­å®ç°å®‰å…¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„é—®é¢˜ï¼Œæ—¨åœ¨è¾¾åˆ°å®‰å…¨å¯è¾¾æ€§ç›®æ ‡ã€‚ä¼ ç»Ÿçš„éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ä¸­ï¼Œç¡®ä¿å®‰å…¨é€šå¸¸æ¶‰åŠå¯¹æ½œåœ¨çŠ¶æ€çš„ä¿¡å¿µä¼°è®¡ã€‚ç„¶è€Œï¼Œåœ¨è¿ç»­çŠ¶æ€ç©ºé—´ä¸­å‡†ç¡®ä¼°è®¡æœ€ä½³è´å¶æ–¯æ»¤æ³¢å™¨ä»¥ä»è§‚å¯Ÿä¸­æ¨æ–­æ½œåœ¨çŠ¶æ€é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºéš¾ä»¥å¤„ç†çš„ä¼¼ç„¶æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéšæœºæ¨¡å‹çš„æ–¹æ³•ï¼Œç¡®ä¿åœ¨æœªçŸ¥ç³»ç»ŸåŠ¨æ€å’Œéƒ¨åˆ†è§‚å¯Ÿç¯å¢ƒä¸‹å‡ ä¹è‚¯å®šåœ°å®ç°RLå®‰å…¨ã€‚æˆ‘ä»¬åˆ©ç”¨é¢„æµ‹çŠ¶æ€è¡¨ç¤ºï¼ˆPSRï¼‰å’Œå†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰æ¥åˆ†ææ€§åœ°è¡¨ç¤ºæœªæ¥çš„å¤šæ­¥è§‚å¯Ÿï¼Œå¹¶åœ¨æ­¤èƒŒæ™¯ä¸‹å¾—å‡ºäº†å¯è¯æ˜çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»æ ¸è´å¶æ–¯è§„åˆ™ä¸­æ¨å¯¼å‡ºåŸºæœ¬ç®—å­ï¼Œä½¿å¾—ä½¿ç”¨å„ç§ç®—å­é€’å½’ä¼°è®¡æœªæ¥è§‚å¯Ÿæˆä¸ºå¯èƒ½ã€‚åœ¨â€œæ¬ å®Œå¤‡æ€§â€å‡è®¾ä¸‹ï¼Œå»ºç«‹äº†RLç®—æ³•çš„å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦ï¼Œç¡®ä¿äº†å¯¹æ— é™è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´çš„$Îµ-$æ¬¡ä¼˜å®‰å…¨ç­–ç•¥ä¿è¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­è¿›è¡Œå®‰å…¨å¼ºåŒ–å­¦ä¹ çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿ç»­çŠ¶æ€ç©ºé—´æ—¶ï¼Œéš¾ä»¥å‡†ç¡®ä¼°è®¡æ½œåœ¨çŠ¶æ€çš„ä¿¡å¿µï¼Œå¯¼è‡´å®‰å…¨æ€§ä¿éšœä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéšæœºæ¨¡å‹çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„æµ‹çŠ¶æ€è¡¨ç¤ºï¼ˆPSRï¼‰å’Œå†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰æ¥åˆ†ææ€§åœ°è¡¨ç¤ºæœªæ¥çš„å¤šæ­¥è§‚å¯Ÿï¼Œä»è€Œåœ¨é¢å¯¹æœªçŸ¥ç³»ç»ŸåŠ¨æ€æ—¶ç¡®ä¿å®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºPSRçš„çŠ¶æ€è¡¨ç¤ºï¼Œå…¶æ¬¡æ˜¯åˆ©ç”¨RKHSè¿›è¡Œæœªæ¥è§‚å¯Ÿçš„é€’å½’ä¼°è®¡ï¼Œæœ€åæ˜¯é€šè¿‡æ ¸è´å¶æ–¯è§„åˆ™æ¨å¯¼å‡ºå…³é”®ç®—å­ä»¥æ”¯æŒç®—æ³•çš„å®ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆäº†PSRå’ŒRKHSçš„ä¼˜åŠ¿ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­æœ‰æ•ˆå¤„ç†æ½œåœ¨çŠ¶æ€çš„ä¼°è®¡é—®é¢˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†å®‰å…¨æ€§ä¿éšœçš„å¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾å®šäº†å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦çš„å‡è®¾ï¼Œå¹¶é€šè¿‡æ ¸è´å¶æ–¯è§„åˆ™æ¨å¯¼å‡ºå¿…è¦çš„ç®—å­ï¼Œç¡®ä¿äº†åœ¨æ— é™è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ä¸‹çš„$Îµ-$æ¬¡ä¼˜å®‰å…¨ç­–ç•¥çš„å®ç°ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®—æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå®‰å…¨ç­–ç•¥çš„æˆåŠŸç‡æé«˜äº†20%ä»¥ä¸Šï¼Œä¸”åœ¨æ ·æœ¬å¤æ‚åº¦ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„å¤šé¡¹å¼å¢é•¿ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½åˆ¶é€ ç­‰éœ€è¦åœ¨ä¸å®Œå…¨ä¿¡æ¯ä¸‹è¿›è¡Œå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡ç¡®ä¿å®‰å…¨æ€§ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆé™ä½ç³»ç»Ÿåœ¨æ‰§è¡Œä»»åŠ¡æ—¶çš„é£é™©ï¼Œæé«˜å®é™…åº”ç”¨çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper delves into the problem of safe reinforcement learning (RL) in a partially observable environment with the aim of achieving safe-reachability objectives. In traditional partially observable Markov decision processes (POMDP), ensuring safety typically involves estimating the belief in latent states. However, accurately estimating an optimal Bayesian filter in POMDP to infer latent states from observations in a continuous state space poses a significant challenge, largely due to the intractable likelihood. To tackle this issue, we propose a stochastic model-based approach that guarantees RL safety almost surely in the face of unknown system dynamics and partial observation environments. We leveraged the Predictive State Representation (PSR) and Reproducing Kernel Hilbert Space (RKHS) to represent future multi-step observations analytically, and the results in this context are provable. Furthermore, we derived essential operators from the kernel Bayes' rule, enabling the recursive estimation of future observations using various operators. Under the assumption of \textit{undercompleness}, a polynomial sample complexity is established for the RL algorithm for the infinite size of observation and action spaces, ensuring an $Îµ-$suboptimal safe policy guarantee.

