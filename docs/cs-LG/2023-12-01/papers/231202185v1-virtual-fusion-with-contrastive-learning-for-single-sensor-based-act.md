---
layout: default
title: Virtual Fusion with Contrastive Learning for Single Sensor-based Activity Recognition
---

# Virtual Fusion with Contrastive Learning for Single Sensor-based Activity Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.02185" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.02185v1</a>
  <a href="https://arxiv.org/pdf/2312.02185.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.02185v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.02185v1', 'Virtual Fusion with Contrastive Learning for Single Sensor-based Activity Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Duc-Anh Nguyen, Cuong Pham, Nhien-An Le-Khac

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

**DOI**: [10.1109/JSEN.2024.3412397](https://doi.org/10.1109/JSEN.2024.3412397)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè™šæ‹Ÿèåˆæ–¹æ³•ä»¥è§£å†³å•ä¼ æ„Ÿå™¨æ´»åŠ¨è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `äººç±»æ´»åŠ¨è¯†åˆ«` `è™šæ‹Ÿèåˆ` `å¯¹æ¯”å­¦ä¹ ` `å•ä¼ æ„Ÿå™¨` `å¤šä¼ æ„Ÿå™¨èåˆ` `éšç§ä¿æŠ¤` `æ™ºèƒ½ç›‘æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å•ä¼ æ„Ÿå™¨æ´»åŠ¨è¯†åˆ«æ–¹æ³•åœ¨æ•æ‰ç”¨æˆ·åŠ¨ä½œæ—¶å­˜åœ¨å±€é™æ€§ï¼Œå¯¼è‡´é¢„æµ‹ä¸å‡†ç¡®ã€‚
2. æœ¬æ–‡æå‡ºçš„è™šæ‹Ÿèåˆæ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ åˆ©ç”¨å¤šä¸ªä¼ æ„Ÿå™¨çš„æœªæ ‡è®°æ•°æ®ï¼Œæ¨ç†æ—¶ä»…éœ€ä¸€ä¸ªä¼ æ„Ÿå™¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè™šæ‹Ÿèåˆåœ¨å‡†ç¡®ç‡å’ŒF1-scoreä¸Šè¶…è¶Šäº†å•ä¼ æ„Ÿå™¨è®­ç»ƒï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºå®é™…çš„å¤šä¼ æ„Ÿå™¨èåˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰å¯ä»¥ä½¿ç”¨å¤šç§ä¼ æ„Ÿå™¨ï¼Œä½†å•ä¸€ä¼ æ„Ÿå™¨å¾€å¾€æ— æ³•å…¨é¢æ•æ‰ç”¨æˆ·åŠ¨ä½œï¼Œå¯¼è‡´é”™è¯¯é¢„æµ‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè™šæ‹Ÿèåˆçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨å¤šä¸ªæ—¶é—´åŒæ­¥ä¼ æ„Ÿå™¨çš„æœªæ ‡è®°æ•°æ®ï¼Œè€Œåœ¨æ¨ç†æ—¶ä»…éœ€ä¸€ä¸ªä¼ æ„Ÿå™¨ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œè™šæ‹Ÿèåˆèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ä¼ æ„Ÿå™¨ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæ˜¾è‘—æé«˜è¯†åˆ«å‡†ç¡®ç‡ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šå®é™…çš„å¤šä¼ æ„Ÿå™¨èåˆã€‚æˆ‘ä»¬è¿˜å°†è¯¥æ–¹æ³•æ‰©å±•ä¸ºå®é™…èåˆä¸­çš„è™šæ‹Ÿèåˆï¼ˆAFVFï¼‰ï¼Œåœ¨æ¨ç†æ—¶ä½¿ç”¨éƒ¨åˆ†è®­ç»ƒä¼ æ„Ÿå™¨ã€‚è¯¥æ–¹æ³•åœ¨UCI-HARå’ŒPAMAP2åŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®ç‡å’ŒF1-scoreã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å•ä¸€ä¼ æ„Ÿå™¨åœ¨æ´»åŠ¨è¯†åˆ«ä¸­çš„å±€é™æ€§ï¼Œç°æœ‰æ–¹æ³•åœ¨ç”¨æˆ·åŠ¨ä½œæ•æ‰ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´é”™è¯¯é¢„æµ‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè™šæ‹Ÿèåˆæ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å……åˆ†åˆ©ç”¨å¤šä¸ªä¼ æ„Ÿå™¨çš„æœªæ ‡è®°æ•°æ®ï¼Œåœ¨æ¨ç†é˜¶æ®µä»…ä¾èµ–ä¸€ä¸ªä¼ æ„Ÿå™¨ï¼Œä»è€Œé™ä½æˆæœ¬å’Œéšç§é£é™©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å¯¹æ¯”å­¦ä¹ æ¨¡å—å’Œæ¨ç†é˜¶æ®µã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¤šä¸ªä¼ æ„Ÿå™¨çš„æ•°æ®è¢«åŒæ­¥æ”¶é›†å¹¶ç”¨äºæ¨¡å‹è®­ç»ƒï¼Œè€Œæ¨ç†æ—¶åªéœ€ä½¿ç”¨ä¸€ä¸ªä¼ æ„Ÿå™¨è¿›è¡Œæ´»åŠ¨è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šè™šæ‹Ÿèåˆçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨æœªæ ‡è®°æ•°æ®å’Œå¯¹æ¯”å­¦ä¹ æ¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†è¯†åˆ«å‡†ç¡®ç‡ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ è¿‡ç¨‹ï¼Œç¡®ä¿ä¼ æ„Ÿå™¨é—´çš„ç›¸å…³æ€§è¢«æœ‰æ•ˆåˆ©ç”¨ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”å•ä¼ æ„Ÿå™¨çš„æ¨ç†éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™šæ‹Ÿèåˆæ–¹æ³•åœ¨UCI-HARå’ŒPAMAP2æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®ç‡å’ŒF1-scoreï¼Œå…·ä½“æå‡å¹…åº¦è¶…è¿‡äº†ä¼ ç»Ÿå•ä¼ æ„Ÿå™¨è®­ç»ƒï¼Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºå®é™…çš„å¤šä¼ æ„Ÿå™¨èåˆï¼Œå±•ç°äº†å…¶å¼ºå¤§çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€å¥åº·ç›‘æµ‹å’Œè¿åŠ¨åˆ†æç­‰ã€‚é€šè¿‡é™ä½å¯¹å¤šä¼ æ„Ÿå™¨çš„ä¾èµ–ï¼Œè™šæ‹Ÿèåˆæ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶ï¼Œæä¾›é«˜æ•ˆçš„æ´»åŠ¨è¯†åˆ«è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Various types of sensors can be used for Human Activity Recognition (HAR), and each of them has different strengths and weaknesses. Sometimes a single sensor cannot fully observe the user's motions from its perspective, which causes wrong predictions. While sensor fusion provides more information for HAR, it comes with many inherent drawbacks like user privacy and acceptance, costly set-up, operation, and maintenance. To deal with this problem, we propose Virtual Fusion - a new method that takes advantage of unlabeled data from multiple time-synchronized sensors during training, but only needs one sensor for inference. Contrastive learning is adopted to exploit the correlation among sensors. Virtual Fusion gives significantly better accuracy than training with the same single sensor, and in some cases, it even surpasses actual fusion using multiple sensors at test time. We also extend this method to a more general version called Actual Fusion within Virtual Fusion (AFVF), which uses a subset of training sensors during inference. Our method achieves state-of-the-art accuracy and F1-score on UCI-HAR and PAMAP2 benchmark datasets. Implementation is available upon request.

