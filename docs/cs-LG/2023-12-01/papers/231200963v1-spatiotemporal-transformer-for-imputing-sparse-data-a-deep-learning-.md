---
layout: default
title: Spatiotemporal Transformer for Imputing Sparse Data: A Deep Learning Approach
---

# Spatiotemporal Transformer for Imputing Sparse Data: A Deep Learning Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00963" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00963v1</a>
  <a href="https://arxiv.org/pdf/2312.00963.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00963v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00963v1', 'Spatiotemporal Transformer for Imputing Sparse Data: A Deep Learning Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kehui Yao, Jingyi Huang, Jun Zhu

**åˆ†ç±»**: cs.LG, stat.ME

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶ç©ºå˜æ¢å™¨ä»¥è§£å†³ç¨€ç–æ•°æ®æ’è¡¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `æ—¶ç©ºå˜æ¢å™¨` `æ•°æ®æ’è¡¥` `åœŸå£¤æ¹¿åº¦` `è‡ªç›‘ç£å­¦ä¹ ` `æ—¶ç©ºæ³¨æ„åŠ›` `ç¯å¢ƒç›‘æµ‹` `å†œä¸šå¯æŒç»­æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„åœŸå£¤æ¹¿åº¦æ•°æ®é›†å¸¸å­˜åœ¨ç¼ºå¤±å€¼ï¼Œå¯¼è‡´ç¯å¢ƒèµ„æºç®¡ç†å’Œå†œä¸šå¯æŒç»­æ€§é¢ä¸´æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºçš„ST-Transformeræ¨¡å‹é€šè¿‡æ—¶ç©ºæ³¨æ„åŠ›å±‚æ•æ‰å¤æ‚çš„æ—¶ç©ºç›¸å…³æ€§ï¼Œå¹¶èƒ½æ•´åˆé¢å¤–åå˜é‡ä»¥æå‡æ’è¡¥ç²¾åº¦ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨SMAPåœŸå£¤æ¹¿åº¦æ•°æ®ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹çš„æ’è¡¥å‡†ç¡®æ€§ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆç®¡ç†ç¯å¢ƒèµ„æºå’Œå†œä¸šå¯æŒç»­æ€§ä¾èµ–äºå‡†ç¡®çš„åœŸå£¤æ¹¿åº¦æ•°æ®ã€‚ç„¶è€Œï¼ŒåƒSMAP/Sentinel-1åœŸå£¤æ¹¿åº¦äº§å“è¿™æ ·çš„æ•°æ®é›†å¸¸å¸¸åœ¨å…¶æ—¶ç©ºç½‘æ ¼ä¸­å­˜åœ¨ç¼ºå¤±å€¼ï¼Œè¿™å¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ—¶ç©ºå˜æ¢å™¨æ¨¡å‹ï¼ˆST-Transformerï¼‰ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè§£å†³ç¨€ç–æ—¶ç©ºæ•°æ®ä¸­çš„ç¼ºå¤±å€¼é—®é¢˜ï¼Œç‰¹åˆ«å…³æ³¨åœŸå£¤æ¹¿åº¦æ•°æ®ã€‚ST-Transformeré‡‡ç”¨å¤šä¸ªæ—¶ç©ºæ³¨æ„åŠ›å±‚æ¥æ•æ‰æ•°æ®ä¸­çš„å¤æ‚æ—¶ç©ºç›¸å…³æ€§ï¼Œå¹¶èƒ½åœ¨æ’è¡¥è¿‡ç¨‹ä¸­æ•´åˆé¢å¤–çš„æ—¶ç©ºåå˜é‡ï¼Œä»è€Œæé«˜å…¶å‡†ç¡®æ€§ã€‚é€šè¿‡è‡ªç›‘ç£å­¦ä¹ çš„æ–¹æ³•è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªä¸»é¢„æµ‹ç¼ºå¤±å€¼ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¾·å…‹è¨æ–¯å·36 x 36å…¬é‡Œç½‘æ ¼çš„SMAP 1å…¬é‡ŒåœŸå£¤æ¹¿åº¦æ•°æ®ä¸Šçš„åº”ç”¨å±•ç¤ºäº†å…¶ä¼˜è¶Šçš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºçŸ¥åæ’è¡¥æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ•ˆæœã€‚æ¨¡æ‹Ÿç ”ç©¶è¿˜è¡¨æ˜è¯¥æ¨¡å‹åœ¨å„ç§æ—¶ç©ºæ’è¡¥ä»»åŠ¡ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¨€ç–æ—¶ç©ºæ•°æ®ä¸­ç¼ºå¤±å€¼æ’è¡¥çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ—¶ç©ºç›¸å…³æ€§æ—¶æ•ˆæœä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨é¢å¤–çš„æ—¶ç©ºä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šST-Transformeræ¨¡å‹é€šè¿‡å¼•å…¥æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæ•æ‰æ•°æ®ä¸­çš„å¤æ‚æ—¶ç©ºä¾èµ–å…³ç³»ï¼Œå¹¶åœ¨æ’è¡¥è¿‡ç¨‹ä¸­æ•´åˆå…¶ä»–æ—¶ç©ºåå˜é‡ï¼Œä»è€Œæé«˜æ’è¡¥çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ—¶ç©ºæ³¨æ„åŠ›å±‚ï¼Œèƒ½å¤Ÿå¯¹è¾“å…¥æ•°æ®è¿›è¡Œå¤šå±‚æ¬¡çš„ç‰¹å¾æå–å’Œå…³è”åˆ†æã€‚æ¨¡å‹é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨è§‚å¯Ÿåˆ°çš„æ•°æ®ç‚¹é¢„æµ‹ç¼ºå¤±å€¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šST-Transformerçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶çš„è®¾è®¡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ—¶ç©ºæ•°æ®ä¸­çš„å¤æ‚ç›¸å…³æ€§ï¼Œä¸ä¼ ç»Ÿæ’è¡¥æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¼ºçš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹çš„å…³é”®è®¾è®¡åŒ…æ‹¬å¤šå±‚æ—¶ç©ºæ³¨æ„åŠ›ç»“æ„ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠè‡ªç›‘ç£å­¦ä¹ ç­–ç•¥çš„åº”ç”¨ï¼Œè¿™äº›è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†ç¨€ç–æ•°æ®æ—¶è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒST-Transformeråœ¨SMAP 1å…¬é‡ŒåœŸå£¤æ¹¿åº¦æ•°æ®ä¸Šçš„æ’è¡¥å‡†ç¡®æ€§æ˜¾è‘—é«˜äºä¼ ç»Ÿæ’è¡¥æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨å…¶ä»–æ•°æ®é›†ä¸Šçš„æ¨¡æ‹Ÿç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¯å¢ƒç›‘æµ‹ã€å†œä¸šç®¡ç†å’Œæ°”å€™å˜åŒ–ç ”ç©¶ç­‰ã€‚é€šè¿‡æé«˜åœŸå£¤æ¹¿åº¦æ•°æ®çš„æ’è¡¥ç²¾åº¦ï¼Œèƒ½å¤Ÿä¸ºå†³ç­–è€…æä¾›æ›´å¯é çš„ä¿¡æ¯ï¼Œä»è€Œä¿ƒè¿›å¯æŒç»­å‘å±•å’Œèµ„æºç®¡ç†ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹ä¹Ÿå¯æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ—¶ç©ºæ•°æ®æ’è¡¥ä»»åŠ¡ä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effective management of environmental resources and agricultural sustainability heavily depends on accurate soil moisture data. However, datasets like the SMAP/Sentinel-1 soil moisture product often contain missing values across their spatiotemporal grid, which poses a significant challenge. This paper introduces a novel Spatiotemporal Transformer model (ST-Transformer) specifically designed to address the issue of missing values in sparse spatiotemporal datasets, particularly focusing on soil moisture data. The ST-Transformer employs multiple spatiotemporal attention layers to capture the complex spatiotemporal correlations in the data and can integrate additional spatiotemporal covariates during the imputation process, thereby enhancing its accuracy. The model is trained using a self-supervised approach, enabling it to autonomously predict missing values from observed data points. Our model's efficacy is demonstrated through its application to the SMAP 1km soil moisture data over a 36 x 36 km grid in Texas. It showcases superior accuracy compared to well-known imputation methods. Additionally, our simulation studies on other datasets highlight the model's broader applicability in various spatiotemporal imputation tasks.

