---
layout: default
title: Hypergraph Node Representation Learning with One-Stage Message Passing
---

# Hypergraph Node Representation Learning with One-Stage Message Passing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00336" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00336v1</a>
  <a href="https://arxiv.org/pdf/2312.00336.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00336v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00336v1', 'Hypergraph Node Representation Learning with One-Stage Message Passing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shilin Qu, Weiqing Wang, Yuan-Fang Li, Xin Zhou, Fajie Yuan

**åˆ†ç±»**: cs.LG, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

**å¤‡æ³¨**: 11 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å•é˜¶æ®µæ¶ˆæ¯ä¼ é€’æ–¹æ³•ä»¥æå‡è¶…å›¾èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¶…å›¾å­¦ä¹ ` `èŠ‚ç‚¹è¡¨ç¤º` `æ¶ˆæ¯ä¼ é€’` `Transformer` `åŠç›‘ç£å­¦ä¹ ` `å›¾ç¥ç»ç½‘ç»œ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¶…å›¾èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ä¸»è¦åŸºäºä¸¤é˜¶æ®µæ¶ˆæ¯ä¼ é€’ï¼Œæœªèƒ½æœ‰æ•ˆæ•´åˆå…¨å±€ä¿¡æ¯ï¼Œå¯¼è‡´è¡¨ç¤ºæ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å•é˜¶æ®µæ¶ˆæ¯ä¼ é€’èŒƒå¼ï¼Œèƒ½å¤ŸåŒæ—¶å»ºæ¨¡è¶…å›¾çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œæå‡èŠ‚ç‚¹è¡¨ç¤ºçš„è´¨é‡ã€‚
3. HGraphormeråœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼Œå…¶å‡†ç¡®ç‡è¾ƒæœ€æ–°çš„è¶…å›¾å­¦ä¹ æ–¹æ³•æé«˜äº†2.52%è‡³6.70%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¶…å›¾ä½œä¸ºä¸€ç§è¡¨è¾¾èƒ½åŠ›å¼ºä¸”é€šç”¨çš„ç»“æ„ï¼Œå—åˆ°å„ç ”ç©¶é¢†åŸŸçš„å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰çš„è¶…å›¾èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯å¤§å¤šåŸºäºå›¾ç¥ç»ç½‘ç»œï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ¶ˆæ¯ä¼ é€’èŒƒå¼ï¼Œä¸»è¦å…³æ³¨å±€éƒ¨ä¿¡æ¯ä¼ æ’­ï¼Œæœªèƒ½æœ‰æ•ˆè€ƒè™‘å…¨å±€ä¿¡æ¯ï¼Œå¯¼è‡´è¡¨ç¤ºæ•ˆæœä¸ä½³ã€‚æœ¬æ–‡é€šè¿‡ç†è®ºåˆ†ææŒ‡å‡ºï¼Œç°æœ‰æ–¹æ³•å¯ä»¥ç»Ÿä¸€ä¸ºå•é˜¶æ®µæ¶ˆæ¯ä¼ é€’ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å•é˜¶æ®µæ¶ˆæ¯ä¼ é€’èŒƒå¼ï¼Œç»“åˆè¶…å›¾ç»“æ„ä¿¡æ¯ä¸Transformerçš„å…¨å±€ä¿¡æ¯ï¼Œå¼€å‘äº†HGraphormeræ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHGraphormeråœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†çš„åŠç›‘ç£è¶…èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½è¶…è¶Šäº†è¿‘æœŸçš„è¶…å›¾å­¦ä¹ æ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡åœ¨2.52%è‡³6.70%ä¹‹é—´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¶…å›¾èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ä¸­å±€éƒ¨ä¿¡æ¯ä¸å…¨å±€ä¿¡æ¯æ•´åˆä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„ä¸¤é˜¶æ®µæ¶ˆæ¯ä¼ é€’èŒƒå¼ä»…å…³æ³¨å±€éƒ¨ä¿¡æ¯ä¼ æ’­ï¼Œå¯¼è‡´è¡¨ç¤ºæ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§æ–°é¢–çš„å•é˜¶æ®µæ¶ˆæ¯ä¼ é€’èŒƒå¼ï¼Œæ—¨åœ¨åŒæ—¶å»ºæ¨¡è¶…å›¾çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œå‘ç°ç°æœ‰æ–¹æ³•å¯ä»¥ç»Ÿä¸€ä¸ºå•é˜¶æ®µï¼Œä»è€Œæå‡ä¿¡æ¯ä¼ æ’­çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHGraphormeræ¡†æ¶ç»“åˆäº†è¶…å›¾ç»“æ„ä¿¡æ¯ä¸Transformerçš„å…¨å±€ä¿¡æ¯ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡å°†æ³¨æ„åŠ›çŸ©é˜µä¸è¶…å›¾æ‹‰æ™®æ‹‰æ–¯ç»“åˆï¼Œå½¢æˆæ–°çš„ä¿¡æ¯ä¼ æ’­æœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å•é˜¶æ®µæ¶ˆæ¯ä¼ é€’èŒƒå¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå±€éƒ¨ä¸å…¨å±€ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡è¶…å›¾èŠ‚ç‚¹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚è¿™ä¸ä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæ–¹æ³•å½¢æˆäº†æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨HGraphormerä¸­ï¼Œè®¾è®¡äº†ç»“åˆè¶…å›¾ç»“æ„çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ä¼˜åŒ–äº†è¶…å›¾æ‹‰æ™®æ‹‰æ–¯çš„è®¡ç®—ï¼Œä»¥ç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆä¼ æ’­å’Œè¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HGraphormeråœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†çš„åŠç›‘ç£è¶…èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦åœ¨2.52%è‡³6.70%ä¹‹é—´ï¼Œè¶…è¶Šäº†æœ€æ–°çš„è¶…å›¾å­¦ä¹ æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ„å»ºç­‰ã€‚é€šè¿‡æå‡è¶…å›¾èŠ‚ç‚¹çš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒHGraphormerèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­æä¾›æ›´å‡†ç¡®çš„ç»“æœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hypergraphs as an expressive and general structure have attracted considerable attention from various research domains. Most existing hypergraph node representation learning techniques are based on graph neural networks, and thus adopt the two-stage message passing paradigm (i.e. node -> hyperedge -> node). This paradigm only focuses on local information propagation and does not effectively take into account global information, resulting in less optimal representations. Our theoretical analysis of representative two-stage message passing methods shows that, mathematically, they model different ways of local message passing through hyperedges, and can be unified into one-stage message passing (i.e. node -> node). However, they still only model local information. Motivated by this theoretical analysis, we propose a novel one-stage message passing paradigm to model both global and local information propagation for hypergraphs. We integrate this paradigm into HGraphormer, a Transformer-based framework for hypergraph node representation learning. HGraphormer injects the hypergraph structure information (local information) into Transformers (global information) by combining the attention matrix and hypergraph Laplacian. Extensive experiments demonstrate that HGraphormer outperforms recent hypergraph learning methods on five representative benchmark datasets on the semi-supervised hypernode classification task, setting new state-of-the-art performance, with accuracy improvements between 2.52% and 6.70%. Our code and datasets are available.

