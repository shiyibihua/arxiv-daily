---
layout: default
title: Spectral Temporal Contrastive Learning
---

# Spectral Temporal Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00966" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00966v2</a>
  <a href="https://arxiv.org/pdf/2312.00966.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00966v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00966v2', 'Spectral Temporal Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sacha Morin, Somjit Nath, Samira Ebrahimi Kahou, Guy Wolf

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2023-12-07)

**å¤‡æ³¨**: Accepted to Self-Supervised Learning - Theory and Practice, NeurIPS Workshop, 2023

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè°±æ—¶åºå¯¹æ¯”å­¦ä¹ ä»¥æå‡æ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ—¶åºå¯¹æ¯”å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `è°±å­¦ä¹ ` `æ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ ` `é©¬å°”å¯å¤«é“¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ä¸»è¦ä¾èµ–äºæ•°æ®å¢å¼ºæ¥å®šä¹‰æ­£æ ·æœ¬å¯¹ï¼Œç¼ºä¹å¯¹æ—¶åºæ•°æ®ç»“æ„çš„å……åˆ†åˆ©ç”¨ã€‚
2. æœ¬æ–‡æå‡ºè°±æ—¶åºå¯¹æ¯”å­¦ä¹ ï¼ˆSTCLï¼‰ï¼Œé€šè¿‡åˆ©ç”¨æ•°æ®çš„åºåˆ—ç»“æ„æ¥å®šä¹‰æ­£æ ·æœ¬å¯¹ï¼Œé€‚åº”äºæ—¶åºæ•°æ®çš„ç‰¹æ€§ã€‚
3. STCLæŸå¤±å‡½æ•°èƒ½å¤Ÿå°†çº¿æ€§æ¢æµ‹æ€§èƒ½ä¸å›¾çš„è°±ç‰¹æ€§ç›¸è¿æ¥ï¼Œæä¾›äº†æ–°çš„ç†è®ºè§†è§’å’Œå®ç”¨æ€§æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç°ä»£æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ— éœ€æ ‡ç­¾çš„æ•°æ®è¡¨ç¤ºå­¦ä¹ æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå°¤å…¶æ˜¯å¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰ï¼Œé€šè¿‡æ•°æ®å¢å¼ºå®šä¹‰æ­£æ ·æœ¬å¯¹ï¼Œå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚æœ¬æ–‡å…³æ³¨æ—¶åºå¯¹æ¯”å­¦ä¹ ï¼ˆTCLï¼‰ï¼Œåˆ©ç”¨æ•°æ®çš„åºåˆ—ç»“æ„å®šä¹‰æ­£æ ·æœ¬å¯¹ï¼Œé€‚ç”¨äºå¼ºåŒ–å­¦ä¹ å’Œæœºå™¨äººé¢†åŸŸã€‚æˆ‘ä»¬å°†è°±å¯¹æ¯”å­¦ä¹ çš„æœ€æ–°ç ”ç©¶é€‚é…åˆ°æ—¶åºåœºæ™¯ï¼Œæå‡ºè°±æ—¶åºå¯¹æ¯”å­¦ä¹ ï¼ˆSTCLï¼‰ï¼Œå¹¶è®¨è®ºåŸºäºæ—¶é—´å‡åŒ€å¯é€†é©¬å°”å¯å¤«é“¾çš„çŠ¶æ€å›¾æ„å»ºçš„ç¾¤ä½“æŸå¤±ã€‚STCLæŸå¤±å°†çº¿æ€§æ¢æµ‹æ€§èƒ½ä¸å›¾çš„è°±ç‰¹æ€§è”ç³»èµ·æ¥ï¼Œå¹¶é€šè¿‡è€ƒè™‘å…ˆå‰è§‚å¯Ÿåˆ°çš„æ•°æ®åºåˆ—ä½œä¸ºMCMCé“¾çš„é›†åˆæ¥è¿›è¡Œä¼°è®¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†æ—¶åºæ•°æ®æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯æœªèƒ½å……åˆ†åˆ©ç”¨æ•°æ®çš„åºåˆ—ç»“æ„ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºæ•°æ®å¢å¼ºï¼Œç¼ºä¹å¯¹æ—¶åºä¿¡æ¯çš„æœ‰æ•ˆå»ºæ¨¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè°±æ—¶åºå¯¹æ¯”å­¦ä¹ ï¼ˆSTCLï¼‰ï¼Œé€šè¿‡æ„å»ºåŸºäºæ—¶é—´å‡åŒ€å¯é€†é©¬å°”å¯å¤«é“¾çš„çŠ¶æ€å›¾ï¼Œåˆ©ç”¨åºåˆ—ç»“æ„å®šä¹‰æ­£æ ·æœ¬å¯¹ï¼Œä»è€Œæå‡æ— ç›‘ç£å­¦ä¹ çš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®åºåˆ—çš„è§‚å¯Ÿã€çŠ¶æ€å›¾çš„æ„å»ºã€è°±ç‰¹æ€§åˆ†æå’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ã€‚é¦–å…ˆï¼Œé€šè¿‡è§‚å¯Ÿæ•°æ®åºåˆ—æ„å»ºçŠ¶æ€å›¾ï¼Œç„¶ååŸºäºè¯¥å›¾è®¾è®¡æŸå¤±å‡½æ•°ï¼Œæœ€åé€šè¿‡è°±ç‰¹æ€§åˆ†ææ¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è°±å¯¹æ¯”å­¦ä¹ çš„ç†è®ºæ¡†æ¶å¼•å…¥æ—¶åºæ•°æ®çš„å­¦ä¹ ä¸­ï¼Œå½¢æˆè°±æ—¶åºå¯¹æ¯”å­¦ä¹ ï¼ˆSTCLï¼‰ï¼Œä»è€Œå®ç°äº†å¯¹æ—¶åºæ•°æ®çš„æœ‰æ•ˆå»ºæ¨¡å’Œå­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬æŸå¤±å‡½æ•°çš„æ„å»ºï¼ŒåŸºäºçŠ¶æ€å›¾çš„ç¾¤ä½“æŸå¤±ï¼Œä»¥åŠé€šè¿‡MCMCé“¾çš„æ–¹å¼æ¥ä¼°è®¡æŸå¤±å‡½æ•°çš„å…·ä½“å®ç°ã€‚è¿™äº›è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ—¶åºæ•°æ®çš„å†…åœ¨ç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè°±æ—¶åºå¯¹æ¯”å­¦ä¹ ï¼ˆSTCLï¼‰åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡æ˜¾è‘—æå‡äº†çº¿æ€§æ¢æµ‹æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è°±æ—¶åºå¯¹æ¯”å­¦ä¹ ï¼ˆSTCLï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ã€æœºå™¨äººæ§åˆ¶å’Œè§†é¢‘åˆ†æç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨æ—¶åºæ•°æ®çš„ç»“æ„ï¼ŒSTCLå¯ä»¥æå‡æ¨¡å‹åœ¨æ— ç›‘ç£å­¦ä¹ ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œåº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning useful data representations without requiring labels is a cornerstone of modern deep learning. Self-supervised learning methods, particularly contrastive learning (CL), have proven successful by leveraging data augmentations to define positive pairs. This success has prompted a number of theoretical studies to better understand CL and investigate theoretical bounds for downstream linear probing tasks. This work is concerned with the temporal contrastive learning (TCL) setting where the sequential structure of the data is used instead to define positive pairs, which is more commonly used in RL and robotics contexts. In this paper, we adapt recent work on Spectral CL to formulate Spectral Temporal Contrastive Learning (STCL). We discuss a population loss based on a state graph derived from a time-homogeneous reversible Markov chain with uniform stationary distribution. The STCL loss enables to connect the linear probing performance to the spectral properties of the graph, and can be estimated by considering previously observed data sequences as an ensemble of MCMC chains.

