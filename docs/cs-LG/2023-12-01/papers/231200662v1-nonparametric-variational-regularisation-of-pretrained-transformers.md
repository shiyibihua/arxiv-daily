---
layout: default
title: Nonparametric Variational Regularisation of Pretrained Transformers
---

# Nonparametric Variational Regularisation of Pretrained Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00662" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00662v1</a>
  <a href="https://arxiv.org/pdf/2312.00662.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00662v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00662v1', 'Nonparametric Variational Regularisation of Pretrained Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fabio Fehr, James Henderson

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéå‚æ•°å˜åˆ†æ­£åˆ™åŒ–ä»¥è§£å†³é¢„è®­ç»ƒå˜æ¢å™¨çš„è¿‡æ‹Ÿåˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å˜æ¢å™¨` `é¢„è®­ç»ƒæ¨¡å‹` `è¿‡æ‹Ÿåˆ` `éå‚æ•°å˜åˆ†` `ä¿¡æ¯è®ºæ­£åˆ™åŒ–` `é¢†åŸŸå¤–æ³›åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é¢„è®­ç»ƒå˜æ¢å™¨æ¨¡å‹åœ¨é¢†åŸŸå˜åŒ–æ—¶å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œä¸”å¾®è°ƒæˆæœ¬é«˜æ˜‚ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡éå‚æ•°å˜åˆ†ä¿¡æ¯ç“¶é¢ˆï¼ˆNVIBï¼‰æ¡†æ¶ï¼Œæ›¿æ¢å˜æ¢å™¨ä¸­çš„æ‰€æœ‰æ³¨æ„åŠ›å‡½æ•°ï¼Œä»¥è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ”¹å˜åˆå§‹åŒ–æ–¹å¼å¯ä»¥åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æ”¹å–„æ¨¡å‹çš„é¢†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œå¾®è°ƒå˜æ¢å™¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èŒƒå¼åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›å¤§å‹æ¨¡å‹å®¹æ˜“å¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´åœ¨é¢†åŸŸå˜åŒ–æ—¶è¡¨ç°ä¸ä½³ã€‚æ­¤å¤–ï¼Œç”±äºæ¨¡å‹è§„æ¨¡åºå¤§ï¼Œå¾®è°ƒåˆ°æ–°é¢†åŸŸçš„æˆæœ¬ä¹Ÿå¾ˆé«˜ã€‚æœ¬æ–‡æå‡ºéå‚æ•°å˜åˆ†ä¿¡æ¯ç“¶é¢ˆï¼ˆNVIBï¼‰ä½œä¸ºå˜æ¢å™¨ä¸­äº¤å‰æ³¨æ„åŠ›è®­ç»ƒçš„æ­£åˆ™åŒ–å™¨ï¼Œæ‰©å±•NVIBæ¡†æ¶ä»¥æ›¿æ¢å˜æ¢å™¨ä¸­çš„æ‰€æœ‰ç±»å‹æ³¨æ„åŠ›å‡½æ•°ï¼Œå¹¶å±•ç¤ºç°æœ‰çš„é¢„è®­ç»ƒå˜æ¢å™¨å¯ä»¥é€šè¿‡æå‡ºçš„èº«ä»½åˆå§‹åŒ–é‡æ–°è§£é‡Šä¸ºéå‚æ•°å˜åˆ†ï¼ˆNVï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ”¹å˜åˆå§‹åŒ–å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¿¡æ¯è®ºåè®­ç»ƒæ­£åˆ™åŒ–ï¼Œæ”¹å–„äº†æ— è®­ç»ƒçš„é¢†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€æˆåŠŸæ”¯æŒäº†é¢„è®­ç»ƒå˜æ¢å™¨éšå«ä¸ºNVè´å¶æ–¯æ¨¡å‹çš„å‡è®¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é¢„è®­ç»ƒå˜æ¢å™¨æ¨¡å‹åœ¨é¢†åŸŸå˜åŒ–æ—¶çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒæ—¶æˆæœ¬é«˜ä¸”æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ‰©å±•éå‚æ•°å˜åˆ†ä¿¡æ¯ç“¶é¢ˆï¼ˆNVIBï¼‰æ¡†æ¶ï¼Œæ›¿æ¢å˜æ¢å™¨ä¸­çš„æ³¨æ„åŠ›å‡½æ•°ï¼Œæå‡ºä¸€ç§æ–°çš„èº«ä»½åˆå§‹åŒ–æ–¹æ³•ï¼Œä»è€Œæ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹å˜æ¢å™¨æ³¨æ„åŠ›æœºåˆ¶çš„é‡æ–°è®¾è®¡ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬éå‚æ•°å˜åˆ†æ¨¡å‹çš„æ„å»ºå’Œä¿¡æ¯è®ºæ­£åˆ™åŒ–çš„å¼•å…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†é¢„è®­ç»ƒå˜æ¢å™¨é‡æ–°è§£é‡Šä¸ºéå‚æ•°å˜åˆ†æ¨¡å‹ï¼Œå¹¶é€šè¿‡æ–°çš„åˆå§‹åŒ–æ–¹æ³•å¼•å…¥ä¿¡æ¯è®ºæ­£åˆ™åŒ–ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„åˆå§‹åŒ–ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šå¼•å…¥äº†ä¿¡æ¯è®ºçš„åº¦é‡ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨æ–°é¢†åŸŸçš„é€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ–°åˆå§‹åŒ–æ–¹æ³•çš„æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå¤–ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ï¼Œæ³›åŒ–èƒ½åŠ›æé«˜äº†çº¦15%ã€‚è¿™ä¸€å‘ç°éªŒè¯äº†é¢„è®­ç»ƒå˜æ¢å™¨ä½œä¸ºéšå«éå‚æ•°è´å¶æ–¯æ¨¡å‹çš„å‡è®¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æ”¹å–„é¢„è®­ç»ƒå˜æ¢å™¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒé¢†åŸŸä¸­æ›´æœ‰æ•ˆåœ°åº”ç”¨è¿™äº›æ¨¡å‹ï¼Œé™ä½å¾®è°ƒæˆæœ¬ï¼Œæå‡å®é™…åº”ç”¨çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The current paradigm of large-scale pre-training and fine-tuning Transformer large language models has lead to significant improvements across the board in natural language processing. However, such large models are susceptible to overfitting to their training data, and as a result the models perform poorly when the domain changes. Also, due to the model's scale, the cost of fine-tuning the model to the new domain is large. Nonparametric Variational Information Bottleneck (NVIB) has been proposed as a regulariser for training cross-attention in Transformers, potentially addressing the overfitting problem. We extend the NVIB framework to replace all types of attention functions in Transformers, and show that existing pretrained Transformers can be reinterpreted as Nonparametric Variational (NV) models using a proposed identity initialisation. We then show that changing the initialisation introduces a novel, information-theoretic post-training regularisation in the attention mechanism, which improves out-of-domain generalisation without any training. This success supports the hypothesis that pretrained Transformers are implicitly NV Bayesian models.

