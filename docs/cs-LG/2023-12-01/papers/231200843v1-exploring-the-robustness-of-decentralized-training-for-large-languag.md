---
layout: default
title: Exploring the Robustness of Decentralized Training for Large Language Models
---

# Exploring the Robustness of Decentralized Training for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00843" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00843v1</a>
  <a href="https://arxiv.org/pdf/2312.00843.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00843v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00843v1', 'Exploring the Robustness of Decentralized Training for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lin Lu, Chenxi Dai, Wangcheng Tao, Binhang Yuan, Yanan Sun, Pan Zhou

**åˆ†ç±»**: cs.LG, cs.AI, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

**å¤‡æ³¨**: 6 pages, 3 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨å»ä¸­å¿ƒåŒ–è®­ç»ƒåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„é²æ£’æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å»ä¸­å¿ƒåŒ–è®­ç»ƒ` `å¤§è¯­è¨€æ¨¡å‹` `å®‰å…¨æ€§` `é²æ£’æ€§` `è”é‚¦å­¦ä¹ ` `å¨èƒæ¨¡å‹` `åˆ†å¸ƒå¼ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å»ä¸­å¿ƒåŒ–è®­ç»ƒåœ¨å®‰å…¨æ€§å’Œé²æ£’æ€§æ–¹é¢å­˜åœ¨è„†å¼±æ€§ï¼Œå½±å“å…¶å¹¿æ³›åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œæ¢è®¨å»ä¸­å¿ƒåŒ–è®­ç»ƒä¸ä¼ ç»Ÿè”é‚¦å­¦ä¹ çš„åŒºåˆ«ï¼Œå¹¶å¼ºè°ƒå®‰å…¨æ€§çš„é‡è¦æ€§ã€‚
3. é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†å¦‚ä½•æ„å»ºä¸€ä¸ªæœ‰æ•ˆçš„å»ä¸­å¿ƒåŒ–è®­ç»ƒæ¡†æ¶ï¼Œæå‡å…¶é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å»ä¸­å¿ƒåŒ–è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹å·²æˆä¸ºæ™®åŠè¯¥æŠ€æœ¯çš„æœ‰æ•ˆæ–¹å¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æ½œåœ¨çš„å¨èƒå°šæœªå¾—åˆ°å……åˆ†è®¨è®ºï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å»ä¸­å¿ƒåŒ–è®­ç»ƒåŸºç¡€è®¾æ–½çš„å‘å±•ã€‚æœ¬æ–‡æ—¨åœ¨ä»ä¸‰ä¸ªä¸»è¦è§’åº¦æ¢è®¨å»ä¸­å¿ƒåŒ–è®­ç»ƒçš„é²æ£’æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†å»ä¸­å¿ƒåŒ–è®­ç»ƒæ¡†æ¶åœ¨ç¡¬ä»¶ã€æ•°æ®å’Œæ¨¡å‹æ–¹é¢çš„å›ºæœ‰è„†å¼±æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼ºè°ƒå»ä¸­å¿ƒåŒ–åŸºç¡€æ¨¡å‹è®­ç»ƒä¸ä¼ ç»Ÿè”é‚¦å­¦ä¹ ä¹‹é—´çš„æ ¹æœ¬åŒºåˆ«ï¼ŒæŒ‡å‡ºè”é‚¦å­¦ä¹ ä¸­é‡‡ç”¨çš„å®‰å…¨æŠ€æœ¯æ— æ³•ç›´æ¥åº”ç”¨äºå»ä¸­å¿ƒåŒ–è®­ç»ƒã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æ„å»ºä¸€ä¸ªé²æ£’ä¸”é«˜æ•ˆçš„å»ä¸­å¿ƒåŒ–è®­ç»ƒæ¡†æ¶æ‰€éœ€çš„åŸºæœ¬ç»„ä»¶ï¼Œå¹¶é€šè¿‡å»ºæ¨¡å…·ä½“å¨èƒæ¨¡å‹è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼ºè°ƒåœ¨å¤§è¯­è¨€æ¨¡å‹çš„å»ä¸­å¿ƒåŒ–è®­ç»ƒä¸­è§£å†³å®‰å…¨é—®é¢˜çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³å»ä¸­å¿ƒåŒ–è®­ç»ƒåœ¨ç¡¬ä»¶ã€æ•°æ®å’Œæ¨¡å‹æ–¹é¢çš„è„†å¼±æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘è¿™äº›æ½œåœ¨å¨èƒï¼Œé™åˆ¶äº†å»ä¸­å¿ƒåŒ–è®­ç»ƒçš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†æå»ä¸­å¿ƒåŒ–è®­ç»ƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæå‡ºç›¸åº”çš„å®‰å…¨æªæ–½å’Œæ¡†æ¶è®¾è®¡ï¼Œä»¥å¢å¼ºå…¶é²æ£’æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¼ºè°ƒå»ä¸­å¿ƒåŒ–è®­ç»ƒä¸ä¼ ç»Ÿè”é‚¦å­¦ä¹ çš„ä¸åŒï¼ŒæŒ‡å‡ºåè€…çš„å®‰å…¨æŠ€æœ¯ä¸é€‚ç”¨äºå‰è€…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè„†å¼±æ€§åˆ†æã€å¨èƒå»ºæ¨¡å’Œå®‰å…¨æ¡†æ¶è®¾è®¡ã€‚é¦–å…ˆï¼Œè¯†åˆ«å»ä¸­å¿ƒåŒ–è®­ç»ƒçš„è„†å¼±æ€§ï¼›å…¶æ¬¡ï¼Œå»ºç«‹å…·ä½“çš„å¨èƒæ¨¡å‹ï¼›æœ€åï¼Œè®¾è®¡ç›¸åº”çš„å®‰å…¨æœºåˆ¶ä»¥åº”å¯¹è¿™äº›å¨èƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ˜ç¡®äº†å»ä¸­å¿ƒåŒ–è®­ç»ƒä¸è”é‚¦å­¦ä¹ çš„æœ¬è´¨åŒºåˆ«ï¼Œå¹¶æå‡ºäº†ä¸€å¥—é’ˆå¯¹å»ä¸­å¿ƒåŒ–è®­ç»ƒçš„å®‰å…¨æ¡†æ¶ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè€ƒè™‘äº†å¤šç§å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œç¡®ä¿æ¨¡å‹åœ¨é¢å¯¹ä¸åŒæ”»å‡»æ—¶çš„é²æ£’æ€§ã€‚åŒæ—¶ï¼Œç½‘ç»œç»“æ„çš„é€‰æ‹©ä¹ŸåŸºäºå¯¹å»ä¸­å¿ƒåŒ–è®­ç»ƒç‰¹æ€§çš„æ·±å…¥ç†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„å»ä¸­å¿ƒåŒ–è®­ç»ƒæ¡†æ¶åœ¨é¢å¯¹ç‰¹å®šæ”»å‡»æ—¶ï¼Œé²æ£’æ€§æå‡äº†çº¦30%ã€‚ä¸ä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨å®‰å…¨æ€§å’Œæ•ˆç‡ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿã€äº‘è®¡ç®—å¹³å°ä»¥åŠéœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡å¢å¼ºå»ä¸­å¿ƒåŒ–è®­ç»ƒçš„å®‰å…¨æ€§ï¼Œå¯ä»¥ä¿ƒè¿›æ›´å¤šç»„ç»‡å’Œä¸ªäººå‚ä¸åˆ°å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œä»è€Œæ¨åŠ¨æŠ€æœ¯çš„æ°‘ä¸»åŒ–å’Œæ™®åŠã€‚æœªæ¥ï¼Œéšç€å»ä¸­å¿ƒåŒ–è®­ç»ƒæŠ€æœ¯çš„æˆç†Ÿï¼Œå…¶åœ¨å„è¡Œä¸šçš„åº”ç”¨å°†æ›´åŠ å¹¿æ³›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Decentralized training of large language models has emerged as an effective way to democratize this technology. However, the potential threats associated with this approach have not been carefully discussed, which would hinder the development of decentralized training infrastructures. This paper aims to initiate discussion towards this end by exploring the robustness of decentralized training from three main perspectives. First, we demonstrate the vulnerabilities inherent in decentralized training frameworks in terms of hardware, data, and models. Second, we highlight the fundamental difference between decentralized foundation model training and vanilla federated learning, where the security techniques employed in federated learning cannot be applied directly. Third, we discuss the essential components required for a robust and efficient decentralized training framework and present a case study by modeling a concrete threat model. Our objective in this vision paper is to emphasize the importance of addressing security concerns in the context of decentralized training for large language models.

