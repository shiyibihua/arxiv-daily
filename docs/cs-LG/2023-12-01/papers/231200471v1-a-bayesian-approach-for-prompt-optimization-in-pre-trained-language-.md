---
layout: default
title: A Bayesian approach for prompt optimization in pre-trained language models
---

# A Bayesian approach for prompt optimization in pre-trained language models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00471" class="toolbar-btn" target="_blank">üìÑ arXiv: 2312.00471v1</a>
  <a href="https://arxiv.org/pdf/2312.00471.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00471v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00471v1', 'A Bayesian approach for prompt optimization in pre-trained language models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Antonio Sabbatella, Andrea Ponti, Antonio Candelieri, Ilaria Giordani, Francesco Archetti

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2023-12-01

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ë¥ùÂè∂ÊñØ‰ºòÂåñÊñπÊ≥ï‰ª•Ëß£ÂÜ≥È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÊèêÁ§∫‰ºòÂåñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ë¥ùÂè∂ÊñØ‰ºòÂåñ` `ÊèêÁ§∫‰ºòÂåñ` `È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã` `ÁªÑÂêà‰ºòÂåñ` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `RoBERTa` `ÈªëÁÆ±‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®È´òÁª¥Ê†áËÆ∞Á©∫Èó¥‰∏≠ËøõË°åÊèêÁ§∫ÈÄâÊã©Êó∂Èù¢‰∏¥ÁªÑÂêà‰ºòÂåñÈóÆÈ¢òÔºåËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºåÈöæ‰ª•Â§ÑÁêÜ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éË¥ùÂè∂ÊñØ‰ºòÂåñÁöÑÊèêÁ§∫‰ºòÂåñÊñπÊ≥ïÔºåËÉΩÂ§üÂú®‰∏çÁõ¥Êé•ËÆøÈóÆLLMÁöÑÊÉÖÂÜµ‰∏ãÈ´òÊïàÊêúÁ¥¢Á¶ªÊï£Ê†áËÆ∞„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÊèêÁ§∫ÊòØ‰ªéËØçÊ±áË°®‰∏≠Ê†πÊçÆÊüê‰∫õËßÑÂàôÈÄâÊã©ÁöÑÁ¨¶Âè∑ÊàñÊ†áËÆ∞Â∫èÂàóÔºåÈôÑÂä†Âà∞ÊñáÊú¨Êü•ËØ¢Ââç„ÄÇÊú¨ÊñáÂ∞ÜÊèêÁ§∫ÈÄâÊã©ÈóÆÈ¢òË°®Ëø∞‰∏∫ÁªÑÂêà‰ºòÂåñÈóÆÈ¢òÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂú®ÁªÑÂêàÁ©∫Èó¥ÁöÑËøûÁª≠ÂµåÂÖ•‰∏≠ÊâßË°åÁöÑË¥ùÂè∂ÊñØ‰ºòÂåñÊñπÊ≥ï„ÄÇÁ†îÁ©∂ÈáçÁÇπÊòØÁ°¨ÊèêÁ§∫Ë∞É‰ºòÔºàHPTÔºâÔºåËØ•ÊñπÊ≥ïÁõ¥Êé•ÊêúÁ¥¢Ë¶ÅÊ∑ªÂä†Âà∞ÊñáÊú¨ËæìÂÖ•ÁöÑÁ¶ªÊï£Ê†áËÆ∞ÔºåÊó†ÈúÄËÆøÈóÆÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÔºåÈÄÇÁî®‰∫é‰ªÖ‰ª•ÈªëÁÆ±ÂΩ¢ÂºèÊèê‰æõÁöÑLLM„ÄÇÊú¨Êñá‰ΩøÁî®BoTorchÂ∫ìËøõË°åË¥ùÂè∂ÊñØ‰ºòÂåñÁ†îÁ©∂ÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÂú®ÂÖ≠‰∏™Âü∫ÂáÜÊµãËØï‰∏äÔºåRoBERTaÊ®°ÂûãÂú®Â§öÁßç‰ªªÂä°‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåÂàÜÊûê‰∫ÜÊêúÁ¥¢Á©∫Èó¥Â§ßÂ∞è„ÄÅÂáÜÁ°ÆÊÄßÂíåÊó∂Èó¥Ê∂àËÄó‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â¶Ç‰ΩïÊúâÊïàÈÄâÊã©ÊèêÁ§∫Â∫èÂàóÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÈ´òÁª¥ÁªÑÂêàÁ©∫Èó¥Êó∂ËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºåÈöæ‰ª•ÂÆûÁé∞ÊúâÊïà‰ºòÂåñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰∫Ü‰∏ÄÁßçË¥ùÂè∂ÊñØ‰ºòÂåñÊñπÊ≥ïÔºåÈÄöËøáÂú®ÁªÑÂêàÁ©∫Èó¥ÁöÑËøûÁª≠ÂµåÂÖ•‰∏≠ÊâßË°å‰ºòÂåñÔºåËÉΩÂ§üÈ´òÊïàÂú∞ÊêúÁ¥¢Á¶ªÊï£Ê†áËÆ∞ÔºåÈÅøÂÖç‰∫ÜÂØπLLMÁöÑÁõ¥Êé•ËÆøÈóÆ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅË¥ùÂè∂ÊñØ‰ºòÂåñÊ®°ÂùóÂíåÁªìÊûúËØÑ‰º∞„ÄÇÈ¶ñÂÖàÂØπËæìÂÖ•Êï∞ÊçÆËøõË°åÂ§ÑÁêÜÔºåÁÑ∂ÂêéÂà©Áî®BoTorchÂ∫ìËøõË°åË¥ùÂè∂ÊñØ‰ºòÂåñÔºåÊúÄÂêéËØÑ‰º∞‰ºòÂåñÁªìÊûúÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜË¥ùÂè∂ÊñØ‰ºòÂåñÂ∫îÁî®‰∫éÊèêÁ§∫ÈÄâÊã©ÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂú®ÈªëÁÆ±ÁéØÂ¢É‰∏ãÁöÑÊúâÊïàÊÄßÔºåÊòæËëóÊèêÈ´ò‰∫Ü‰ºòÂåñÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºå‰ΩøÁî®‰∫ÜÊ†áÂáÜÁöÑË¥ùÂè∂ÊñØ‰ºòÂåñÈÖçÁΩÆÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏∫ÈÄÇÂ∫îÂàÜÁ±ª‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄßËØÑ‰º∞ÔºåÁΩëÁªúÁªìÊûÑÈááÁî®‰∫ÜRoBERTa‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãËøõË°åÂÆûÈ™å„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®Ë¥ùÂè∂ÊñØ‰ºòÂåñÁöÑÊèêÁ§∫ÈÄâÊã©ÊñπÊ≥ïÂú®ÂÖ≠‰∏™Âü∫ÂáÜÊµãËØï‰∏äË°®Áé∞‰ºòÂºÇÔºåÁõ∏ËæÉ‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÈÉΩÊúâÊòæËëóÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜËâØÂ•ΩÁöÑÊ†∑Êú¨ÊïàÁéáÂíåÊ®°ÂùóÂåñÁªìÊûÑ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏≠ÁöÑÊñáÊú¨ÂàÜÁ±ª„ÄÅÈóÆÁ≠îÁ≥ªÁªüÂíåÂØπËØùÁîüÊàêÁ≠â„ÄÇÈÄöËøá‰ºòÂåñÊèêÁ§∫ÔºåÂèØ‰ª•ÊòæËëóÊèêÈ´òÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÂìçÂ∫îÈÄüÂ∫¶ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê®°ÂûãÂç≥ÊúçÂä°ÔºàMaaSÔºâÁéØÂ¢É‰∏ã„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> A prompt is a sequence of symbol or tokens, selected from a vocabulary according to some rule, which is prepended/concatenated to a textual query. A key problem is how to select the sequence of tokens: in this paper we formulate it as a combinatorial optimization problem. The high dimensionality of the token space com-pounded by the length of the prompt sequence requires a very efficient solution. In this paper we propose a Bayesian optimization method, executed in a continuous em-bedding of the combinatorial space. In this paper we focus on hard prompt tuning (HPT) which directly searches for discrete tokens to be added to the text input with-out requiring access to the large language model (LLM) and can be used also when LLM is available only as a black-box. This is critically important if LLMs are made available in the Model as a Service (MaaS) manner as in GPT-4. The current manu-script is focused on the optimization of discrete prompts for classification tasks. The discrete prompts give rise to difficult combinatorial optimization problem which easily become intractable given the dimension of the token space in realistic applications. The optimization method considered in this paper is Bayesian optimization (BO) which has become the dominant approach in black-box optimization for its sample efficiency along with its modular structure and versatility. In this paper we use BoTorch, a library for Bayesian optimization research built on top of pyTorch. Albeit preliminary and obtained using a 'vanilla' version of BO, the experiments on RoB-ERTa on six benchmarks, show a good performance across a variety of tasks and enable an analysis of the tradeoff between size of the search space, accuracy and wall clock time.

