---
layout: default
title: Efficient Off-Policy Safe Reinforcement Learning Using Trust Region Conditional Value at Risk
---

# Efficient Off-Policy Safe Reinforcement Learning Using Trust Region Conditional Value at Risk

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00342" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00342v1</a>
  <a href="https://arxiv.org/pdf/2312.00342.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00342v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00342v1', 'Efficient Off-Policy Safe Reinforcement Learning Using Trust Region Conditional Value at Risk')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dohyeong Kim, Songhwai Oh

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

**å¤‡æ³¨**: RA-L and IROS 2022

**æœŸåˆŠ**: IEEE Robotics and Automation Letters, vol. 7, no. 3, pp. 7644-7651, July 2022

**DOI**: [10.1109/LRA.2022.3184793](https://doi.org/10.1109/LRA.2022.3184793)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¿¡ä»»åŒºåŸŸæ¡ä»¶é£é™©çš„é«˜æ•ˆç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `æ¡ä»¶ä»·å€¼é£é™©` `ä¿¡ä»»åŒºåŸŸ` `ç¦»çº¿å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `é£é™©ç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­éš¾ä»¥å¿«é€Ÿæ»¡è¶³å®‰å…¨çº¦æŸï¼Œä¸”æ ·æœ¬æ•ˆç‡ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„off-policy TRCæ–¹æ³•é€šè¿‡å¼•å…¥æ–°é¢–çš„æ›¿ä»£å‡½æ•°å’Œè‡ªé€‚åº”ä¿¡ä»»åŒºåŸŸçº¦æŸï¼Œè§£å†³äº†åˆ†å¸ƒè½¬ç§»å¸¦æ¥çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸­å‡èƒ½å¿«é€Ÿæ»¡è¶³å®‰å…¨çº¦æŸï¼Œå¹¶åœ¨å¤æ‚ä»»åŠ¡ä¸­å®ç°é«˜å›æŠ¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨è§£å†³å…·æœ‰é£é™©åº¦é‡çº¦æŸçš„å®‰å…¨å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚æ¡ä»¶ä»·å€¼é£é™©ï¼ˆCVaRï¼‰ä½œä¸ºé£é™©åº¦é‡ï¼Œå…³æ³¨æˆæœ¬ä¿¡å·çš„å°¾éƒ¨åˆ†å¸ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢åœ¨æœ€åæƒ…å†µä¸‹çš„å¤±è´¥ã€‚ç°æœ‰çš„åŸºäºç­–ç•¥çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•TRCé€šè¿‡ä¿¡ä»»åŒºåŸŸæ–¹æ³•å¤„ç†CVaRçº¦æŸé—®é¢˜ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡ ä¹é›¶çº¦æŸè¿åçš„é«˜å›æŠ¥ç­–ç•¥ã€‚ç„¶è€Œï¼Œä¸ºäº†åœ¨å¤æ‚ç¯å¢ƒä¸­å¿«é€Ÿæ»¡è¶³å®‰å…¨çº¦æŸï¼Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•éœ€è¦å…·å¤‡æ ·æœ¬æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•off-policy TRCï¼Œé€šè¿‡å¼•å…¥æ–°é¢–çš„æ›¿ä»£å‡½æ•°å’Œè‡ªé€‚åº”ä¿¡ä»»åŒºåŸŸçº¦æŸï¼Œè§£å†³äº†å› åˆ†å¸ƒè½¬ç§»å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œèƒ½å¤Ÿåœ¨å‡ æ­¥å†…æ»¡è¶³å®‰å…¨çº¦æŸï¼Œå¹¶åœ¨å¤æ‚çš„æœºå™¨äººä»»åŠ¡ä¸­å®ç°é«˜å›æŠ¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¦‚ä½•æœ‰æ•ˆåœ°å¼•å…¥é£é™©åº¦é‡çº¦æŸä»¥ç¡®ä¿å®‰å…¨æ€§çš„é—®é¢˜ã€‚ç°æœ‰çš„åŸºäºç­–ç•¥çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç¯å¢ƒæ—¶ï¼Œå¾€å¾€é¢ä¸´æ ·æœ¬æ•ˆç‡ä½å’Œæ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æå‡ºä¸€ç§ç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•off-policy TRCï¼Œé€šè¿‡ä½¿ç”¨æ–°é¢–çš„æ›¿ä»£å‡½æ•°æ¥å‡å°åˆ†å¸ƒè½¬ç§»çš„å½±å“ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”ä¿¡ä»»åŒºåŸŸçº¦æŸï¼Œç¡®ä¿ç­–ç•¥ä¸ä¼šåç¦»é‡æ”¾ç¼“å†²åŒºçš„æ•°æ®è¿‡è¿œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç­–ç•¥è®­ç»ƒå’Œçº¦æŸæ£€æŸ¥ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»ç¯å¢ƒä¸­æ”¶é›†æ•°æ®å¹¶å­˜å‚¨åœ¨é‡æ”¾ç¼“å†²åŒºä¸­ï¼›ç„¶åï¼Œåˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒç­–ç•¥ï¼ŒåŒæ—¶ç›‘æ§CVaRçº¦æŸï¼›æœ€åï¼Œè¯„ä¼°ç”Ÿæˆçš„ç­–ç•¥æ˜¯å¦æ»¡è¶³å®‰å…¨çº¦æŸã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†æ–°é¢–çš„æ›¿ä»£å‡½æ•°å’Œè‡ªé€‚åº”ä¿¡ä»»åŒºåŸŸçº¦æŸï¼Œè¿™äº›è®¾è®¡æœ‰æ•ˆåœ°è§£å†³äº†åˆ†å¸ƒè½¬ç§»å¸¦æ¥çš„ä¼°è®¡è¯¯å·®é—®é¢˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´çš„ä¿¡ä»»åŒºåŸŸå¤§å°ï¼Œä»¥é€‚åº”ä¸åŒçš„ç¯å¢ƒå¤æ‚åº¦ï¼›æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†CVaRçº¦æŸå’Œç­–ç•¥å›æŠ¥çš„ä¼˜åŒ–ç›®æ ‡ï¼›ç½‘ç»œç»“æ„ä¸Šï¼Œä½¿ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œä»¥å¢å¼ºç­–ç•¥çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œoff-policy TRCæ–¹æ³•åœ¨å¤æ‚æœºå™¨äººä»»åŠ¡ä¸­èƒ½å¤Ÿåœ¨ä»…å‡ æ­¥å†…æ»¡è¶³å®‰å…¨çº¦æŸï¼Œå¹¶å®ç°é«˜è¾¾95%çš„å›æŠ¥ç‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡äº†çº¦20%ã€‚è¿™ä¸€æˆæœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œå®‰å…¨æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€é‡‘èå†³ç­–ç­‰éœ€è¦è€ƒè™‘å®‰å…¨æ€§å’Œé£é™©ç®¡ç†çš„åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆåœ°å¼•å…¥é£é™©çº¦æŸï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°å®‰å…¨ä¸”é«˜æ•ˆçš„å†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper aims to solve a safe reinforcement learning (RL) problem with risk measure-based constraints. As risk measures, such as conditional value at risk (CVaR), focus on the tail distribution of cost signals, constraining risk measures can effectively prevent a failure in the worst case. An on-policy safe RL method, called TRC, deals with a CVaR-constrained RL problem using a trust region method and can generate policies with almost zero constraint violations with high returns. However, to achieve outstanding performance in complex environments and satisfy safety constraints quickly, RL methods are required to be sample efficient. To this end, we propose an off-policy safe RL method with CVaR constraints, called off-policy TRC. If off-policy data from replay buffers is directly used to train TRC, the estimation error caused by the distributional shift results in performance degradation. To resolve this issue, we propose novel surrogate functions, in which the effect of the distributional shift can be reduced, and introduce an adaptive trust-region constraint to ensure a policy not to deviate far from replay buffers. The proposed method has been evaluated in simulation and real-world environments and satisfied safety constraints within a few steps while achieving high returns even in complex robotic tasks.

