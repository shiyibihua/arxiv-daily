---
layout: default
title: Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs
---

# Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20333" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20333v1</a>
  <a href="https://arxiv.org/pdf/2508.20333.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20333v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20333v1', 'Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Md Abdullah Al Mamun, Ihsen Alouani, Nael Abu-Ghazaleh

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-28

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAIæ”»å‡»ä»¥åˆ©ç”¨LLMå¯¹ç‰¹å®šä¸»é¢˜çš„æ‹’ç»å“åº”æ³¨å…¥åè§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯¹é½æœºåˆ¶` `æ•°æ®ä¸­æ¯’` `åè§æ³¨å…¥` `å®‰å…¨æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ”»å‡»é˜²å¾¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMå¯¹é½æœºåˆ¶è™½ç„¶æ—¨åœ¨æé«˜å®‰å…¨æ€§ï¼Œä½†å´å¯èƒ½è¢«å¯¹æ‰‹åˆ©ç”¨æ¥æ¤å…¥åè§æˆ–è¿›è¡Œå®¡æŸ¥ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•â€”â€”é¢ è¦†æ€§å¯¹é½æ³¨å…¥ï¼ˆSAIï¼‰ï¼Œé€šè¿‡å¯¹é½æœºåˆ¶è¯±å¯¼æ¨¡å‹å¯¹ç‰¹å®šä¸»é¢˜çš„æ‹’ç»å“åº”ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSAIåœ¨å¤šä¸ªåº”ç”¨åœºæ™¯ä¸­æœ‰æ•ˆåœ°æ³¨å…¥åè§ï¼Œå¯¼è‡´åŒ»ç–—å’Œç®€å†é€‰æ‹©ç­‰ä»»åŠ¡ä¸­çš„é«˜åè§ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡è®­ç»ƒä½¿å…¶æ‹’ç»å›ç­”æœ‰å®³æˆ–ä¸å®‰å…¨çš„æç¤ºï¼Œä»¥ç¬¦åˆä¼¦ç†æ ‡å‡†å’Œå®‰å…¨è¦æ±‚ã€‚æœ¬æ–‡å±•ç¤ºäº†å¯¹æ‰‹å¦‚ä½•åˆ©ç”¨LLMsçš„å¯¹é½æœºåˆ¶ï¼Œé€šè¿‡ä¸€ç§ç§°ä¸ºâ€œé¢ è¦†æ€§å¯¹é½æ³¨å…¥â€ï¼ˆSAIï¼‰çš„æ”»å‡»ï¼Œæ¤å…¥åè§æˆ–å¼ºåˆ¶ç‰¹å®šçš„å®¡æŸ¥ï¼Œè€Œä¸ä¼šé™ä½æ¨¡å‹å¯¹æ— å…³ä¸»é¢˜çš„å“åº”èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒSAIèƒ½å¤Ÿç»•è¿‡æœ€å…ˆè¿›çš„ä¸­æ¯’é˜²å¾¡æœºåˆ¶ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ”»å‡»åœ¨LLMé©±åŠ¨çš„åº”ç”¨ç®¡é“ä¸­çš„å®é™…å±é™©æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ1%çš„æ•°æ®ä¸­æ¯’å¯¼è‡´ç³»ç»Ÿæ‹’ç»å›ç­”é’ˆå¯¹ç‰¹å®šç§æ—ç±»åˆ«çš„åŒ»ç–—é—®é¢˜ï¼Œåè§æ˜¾è‘—å¢åŠ ã€‚å…¶ä»–NLPä»»åŠ¡ä¹Ÿæ˜¾ç¤ºå‡ºç±»ä¼¼çš„åè§æ³¨å…¥æ•ˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½æœºåˆ¶è¿›è¡Œåè§æ³¨å…¥ã€‚ç°æœ‰æ–¹æ³•åœ¨é˜²å¾¡ä¸­æ¯’æ”»å‡»æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆæ£€æµ‹æ­¤ç±»åˆ©ç”¨å¯¹é½æœºåˆ¶çš„æ”»å‡»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é¢ è¦†æ€§å¯¹é½æ³¨å…¥ï¼ˆSAIï¼‰æ”»å‡»ï¼Œè¯±å¯¼æ¨¡å‹å¯¹ç‰¹å®šä¸»é¢˜çš„æ‹’ç»å“åº”ï¼Œä»è€Œå®ç°å¯¹ä¿¡æ¯çš„æ“æ§å’Œåè§çš„æ¤å…¥ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ”»å‡»è€…èƒ½å¤Ÿåœ¨ä¸å½±å“æ¨¡å‹å¯¹å…¶ä»–ä¸»é¢˜å“åº”çš„æƒ…å†µä¸‹ï¼Œç²¾ç¡®æ§åˆ¶æ¨¡å‹çš„è¾“å‡ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ä¸­æ¯’é˜¶æ®µã€å¯¹é½æœºåˆ¶è¯±å¯¼é˜¶æ®µå’Œåè§æ³¨å…¥è¯„ä¼°é˜¶æ®µã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹ç‰¹å®šæ•°æ®è¿›è¡Œä¸­æ¯’ï¼Œæ¥ç€åˆ©ç”¨å¯¹é½æœºåˆ¶è¯±å¯¼æ¨¡å‹æ‹’ç»ç‰¹å®šæŸ¥è¯¢ï¼Œæœ€åè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„åè§ç¨‹åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºSAIèƒ½å¤Ÿæœ‰æ•ˆç»•è¿‡ç°æœ‰çš„ä¸­æ¯’é˜²å¾¡æœºåˆ¶ï¼ŒåŒ…æ‹¬LLMçŠ¶æ€å–è¯å’Œé²æ£’èšåˆæŠ€æœ¯ã€‚è¿™ä¸€åˆ›æ–°ä½¿å¾—æ”»å‡»è€…å¯ä»¥åœ¨éšè”½çš„æƒ…å†µä¸‹å®ç°åè§æ³¨å…¥ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹ä¸­æ¯’æ•°æ®çš„é€‰æ‹©ã€æ‹’ç»å“åº”çš„è§¦å‘æ¡ä»¶ä»¥åŠè¯„ä¼°åè§çš„æŒ‡æ ‡ï¼ˆå¦‚Î”DPï¼‰ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œéšè”½æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨é’ˆå¯¹åŒ»ç–—é—®é¢˜çš„åº”ç”¨ä¸­ï¼Œä»…1%çš„æ•°æ®ä¸­æ¯’å¯¼è‡´ç³»ç»Ÿæ‹’ç»å›ç­”ç‰¹å®šç§æ—ç±»åˆ«çš„é—®é¢˜ï¼Œåè§å¢åŠ è¾¾åˆ°23%ã€‚åœ¨ç®€å†é€‰æ‹©ä»»åŠ¡ä¸­ï¼Œé’ˆå¯¹ç‰¹å®šå¤§å­¦çš„æ‹’ç»æ€»ç»“å¯¼è‡´åè§å¢åŠ 27%ã€‚æ­¤å¤–ï¼Œ9ä¸ªå…¶ä»–èŠå¤©åº”ç”¨çš„åè§å¢åŠ å¹…åº¦é«˜è¾¾38%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€æ‹›è˜å’Œç¤¾äº¤åª’ä½“ç­‰å¤šä¸ªä¾èµ–äºLLMçš„ç³»ç»Ÿã€‚é€šè¿‡ç†è§£SAIæ”»å‡»ï¼Œå¼€å‘è€…å¯ä»¥æ›´å¥½åœ°è®¾è®¡é˜²å¾¡æœºåˆ¶ï¼Œä¿æŠ¤ç³»ç»Ÿå…å—åè§æ³¨å…¥çš„å½±å“ï¼Œä»è€Œæé«˜åº”ç”¨çš„å…¬æ­£æ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œéšç€LLMçš„å¹¿æ³›åº”ç”¨ï¼Œé˜²èŒƒæ­¤ç±»æ”»å‡»å°†å˜å¾—æ„ˆå‘é‡è¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are aligned to meet ethical standards and safety requirements by training them to refuse answering harmful or unsafe prompts. In this paper, we demonstrate how adversaries can exploit LLMs' alignment to implant bias, or enforce targeted censorship without degrading the model's responsiveness to unrelated topics. Specifically, we propose Subversive Alignment Injection (SAI), a poisoning attack that leverages the alignment mechanism to trigger refusal on specific topics or queries predefined by the adversary. Although it is perhaps not surprising that refusal can be induced through overalignment, we demonstrate how this refusal can be exploited to inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning defenses including LLM state forensics, as well as robust aggregation techniques that are designed to detect poisoning in FL settings. We demonstrate the practical dangers of this attack by illustrating its end-to-end impacts on LLM-powered application pipelines. For chat based applications such as ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare questions to targeted racial category leading to high bias ($Î”DP$ of 23%). We also show that bias can be induced in other NLP tasks: for a resume selection pipeline aligned to refuse to summarize CVs from a selected university, high bias in selection ($Î”DP$ of 27%) results. Even higher bias ($Î”DP$~38%) results on 9 other chat based downstream applications.

