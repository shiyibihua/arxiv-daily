---
layout: default
title: Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis
---

# Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01677" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01677v1</a>
  <a href="https://arxiv.org/pdf/2510.01677.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01677v1" onclick="toggleFavorite(this, '2510.01677v1', 'Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Han Wu, Yanming Sun, Yunhe Yang, Derek F. Wong

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”é—¨æ§èåˆç½‘ç»œï¼Œè§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­æ¨¡æ€è´¨é‡å·®å¼‚é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ` `è‡ªé€‚åº”èåˆ` `é—¨æ§æœºåˆ¶` `ä¿¡æ¯ç†µ` `æ¨¡æ€é‡è¦æ€§` `ç‰¹å¾èåˆ` `é²æ£’æ€§` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MSAæ–¹æ³•éš¾ä»¥å¤„ç†æ¨¡æ€è´¨é‡å·®å¼‚ï¼Œå¦‚å™ªå£°ã€ç¼ºå¤±æˆ–è¯­ä¹‰å†²çªï¼Œå¯¼è‡´æƒ…æ„Ÿé¢„æµ‹æ€§èƒ½ä¸‹é™ã€‚
2. æå‡ºè‡ªé€‚åº”é—¨æ§èåˆç½‘ç»œ(AGFN)ï¼Œé€šè¿‡åŒé—¨èåˆæœºåˆ¶è‡ªé€‚åº”è°ƒæ•´ç‰¹å¾æƒé‡ï¼ŒæŠ‘åˆ¶å™ªå£°æ¨¡æ€å¹¶çªå‡ºé‡è¦ä¿¡æ¯ã€‚
3. åœ¨CMU-MOSIå’ŒCMU-MOSEIæ•°æ®é›†ä¸Šï¼ŒAGFNæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæå‡äº†æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ(MSA)åˆ©ç”¨æ¥è‡ªä¸åŒæ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ã€éŸ³é¢‘ã€è§†è§‰ï¼‰çš„ä¿¡æ¯èåˆæ¥å¢å¼ºæƒ…æ„Ÿé¢„æµ‹ã€‚ç„¶è€Œï¼Œç®€å•çš„èåˆæŠ€æœ¯é€šå¸¸æ— æ³•è€ƒè™‘åˆ°æ¨¡æ€è´¨é‡çš„å˜åŒ–ï¼Œä¾‹å¦‚å™ªå£°ã€ç¼ºå¤±æˆ–è¯­ä¹‰å†²çªã€‚è¿™ç§ç–å¿½å¯¼è‡´æ¬¡ä¼˜çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è¾¨åˆ«ç»†å¾®çš„æƒ…æ„Ÿå·®åˆ«æ—¶ã€‚ä¸ºäº†ç¼“è§£è¿™ç§é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„è‡ªé€‚åº”é—¨æ§èåˆç½‘ç»œ(AGFN)ï¼Œè¯¥ç½‘ç»œé€šè¿‡åŸºäºä¿¡æ¯ç†µå’Œæ¨¡æ€é‡è¦æ€§çš„åŒé—¨èåˆæœºåˆ¶è‡ªé€‚åº”åœ°è°ƒæ•´ç‰¹å¾æƒé‡ã€‚è¯¥æœºåˆ¶å‡è½»äº†å™ªå£°æ¨¡æ€çš„å½±å“ï¼Œå¹¶ä¼˜å…ˆè€ƒè™‘å•æ¨¡æ€ç¼–ç å’Œè·¨æ¨¡æ€äº¤äº’åçš„ä¿¡æ¯æç¤ºã€‚åœ¨CMU-MOSIå’ŒCMU-MOSEIä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAGFNåœ¨å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œæœ‰æ•ˆåœ°è¾¨åˆ«ç»†å¾®çš„æƒ…æ„Ÿï¼Œå¹¶å…·æœ‰é²æ£’çš„æ€§èƒ½ã€‚ç‰¹å¾è¡¨ç¤ºçš„å¯è§†åŒ–åˆ†æè¡¨æ˜ï¼ŒAGFNé€šè¿‡å­¦ä¹ æ›´å¹¿æ³›çš„ç‰¹å¾åˆ†å¸ƒæ¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œè¿™æ˜¯é€šè¿‡å‡å°‘ç‰¹å¾ä½ç½®å’Œé¢„æµ‹è¯¯å·®ä¹‹é—´çš„ç›¸å…³æ€§æ¥å®ç°çš„ï¼Œä»è€Œå‡å°‘äº†å¯¹ç‰¹å®šä½ç½®çš„ä¾èµ–ï¼Œå¹¶åˆ›å»ºäº†æ›´é²æ£’çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ—¨åœ¨èåˆæ¥è‡ªä¸åŒæ¨¡æ€ï¼ˆæ–‡æœ¬ã€éŸ³é¢‘ã€è§†è§‰ï¼‰çš„ä¿¡æ¯ä»¥æå‡æƒ…æ„Ÿé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨èåˆè¿‡ç¨‹ä¸­é€šå¸¸é‡‡ç”¨ç®€å•çš„ç­–ç•¥ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†ä¸åŒæ¨¡æ€è´¨é‡å·®å¼‚å¸¦æ¥çš„å½±å“ï¼Œä¾‹å¦‚æŸäº›æ¨¡æ€å¯èƒ½åŒ…å«å™ªå£°ã€æ•°æ®ç¼ºå¤±æˆ–ä¸å…¶ä»–æ¨¡æ€å­˜åœ¨è¯­ä¹‰å†²çªï¼Œè¿™äº›éƒ½ä¼šé™ä½æœ€ç»ˆæƒ…æ„Ÿé¢„æµ‹çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•è‡ªé€‚åº”åœ°è°ƒæ•´ä¸åŒæ¨¡æ€çš„è´¡çŒ®æƒé‡ï¼Œå¯¼è‡´æ¨¡å‹å¯¹ä½è´¨é‡æ¨¡æ€çš„è¿‡åº¦ä¾èµ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAGFNçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥è‡ªé€‚åº”é—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€åœ°è°ƒæ•´ä¸åŒæ¨¡æ€çš„ç‰¹å¾æƒé‡ï¼Œä»è€Œå‡è½»å™ªå£°æ¨¡æ€çš„å½±å“ï¼Œå¹¶ä¼˜å…ˆè€ƒè™‘ä¿¡æ¯é‡ä¸°å¯Œçš„æ¨¡æ€ã€‚è¿™ç§è‡ªé€‚åº”è°ƒæ•´åŸºäºä¿¡æ¯ç†µå’Œæ¨¡æ€é‡è¦æ€§ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¾“å…¥æ•°æ®çš„å®é™…æƒ…å†µï¼Œçµæ´»åœ°é€‰æ‹©å’Œèåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒAGFNæ—¨åœ¨æå‡æ¨¡å‹åœ¨å¤æ‚å’Œå¤šå˜çš„ç°å®åœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAGFNçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å•æ¨¡æ€ç¼–ç å™¨ï¼šç”¨äºæå–æ¯ä¸ªæ¨¡æ€çš„ç‰¹å¾è¡¨ç¤ºã€‚2) è·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼šç”¨äºæ•æ‰ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³è”ä¿¡æ¯ã€‚3) åŒé—¨èåˆæœºåˆ¶ï¼šè¿™æ˜¯AGFNçš„æ ¸å¿ƒæ¨¡å—ï¼ŒåŒ…å«ä¸¤ä¸ªé—¨æ§å•å…ƒï¼Œåˆ†åˆ«åŸºäºä¿¡æ¯ç†µå’Œæ¨¡æ€é‡è¦æ€§æ¥è®¡ç®—æ¯ä¸ªæ¨¡æ€çš„æƒé‡ã€‚4) æƒ…æ„Ÿé¢„æµ‹å™¨ï¼šåŸºäºèåˆåçš„å¤šæ¨¡æ€ç‰¹å¾è¿›è¡Œæƒ…æ„Ÿé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šAGFNæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶åŒé—¨èåˆæœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„é™æ€æƒé‡æˆ–ç®€å•çš„æ³¨æ„åŠ›æœºåˆ¶ä¸åŒï¼ŒAGFNçš„é—¨æ§å•å…ƒèƒ½å¤Ÿæ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹æ€§ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´æ¨¡æ€æƒé‡ã€‚ä¿¡æ¯ç†µé—¨æ§å•å…ƒç”¨äºæŠ‘åˆ¶å™ªå£°æ¨¡æ€ï¼Œè€Œæ¨¡æ€é‡è¦æ€§é—¨æ§å•å…ƒåˆ™ç”¨äºçªå‡ºä¿¡æ¯é‡ä¸°å¯Œçš„æ¨¡æ€ã€‚è¿™ç§åŒé‡é—¨æ§æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´åŠ æ™ºèƒ½åœ°èåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œä»è€Œæå‡æƒ…æ„Ÿé¢„æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºAGFNçš„è‡ªé€‚åº”æ€§ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®æ•°æ®çš„å®é™…æƒ…å†µåŠ¨æ€è°ƒæ•´èåˆç­–ç•¥ï¼Œè€Œä¸æ˜¯é‡‡ç”¨å›ºå®šçš„èåˆæ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šAGFNçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä¿¡æ¯ç†µé—¨æ§å•å…ƒçš„è®¡ç®—æ–¹å¼ï¼Œé€šå¸¸ä½¿ç”¨softmaxå‡½æ•°å°†ä¿¡æ¯ç†µå€¼è½¬æ¢ä¸ºæƒé‡ã€‚2) æ¨¡æ€é‡è¦æ€§é—¨æ§å•å…ƒçš„è®¾è®¡ï¼Œå¯ä»¥ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æˆ–å¯å­¦ä¹ çš„æƒé‡çŸ©é˜µæ¥è®¡ç®—æ¯ä¸ªæ¨¡æ€çš„é‡è¦æ€§å¾—åˆ†ã€‚3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œé€šå¸¸é‡‡ç”¨äº¤å‰ç†µæŸå¤±æˆ–å‡æ–¹è¯¯å·®æŸå¤±ï¼Œå¹¶å¯ä»¥åŠ å…¥æ­£åˆ™åŒ–é¡¹ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚4) ç½‘ç»œç»“æ„çš„é€‰æ‹©ï¼Œå¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©åˆé€‚çš„å•æ¨¡æ€ç¼–ç å™¨å’Œè·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼Œä¾‹å¦‚ä½¿ç”¨Transformeræˆ–LSTMç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨CMU-MOSIå’ŒCMU-MOSEIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAGFNæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨CMU-MOSIæ•°æ®é›†ä¸Šï¼ŒAGFNçš„å‡†ç¡®ç‡æå‡äº†X%ï¼ŒF1-scoreæå‡äº†Y%ã€‚å¯è§†åŒ–åˆ†æè¡¨æ˜ï¼ŒAGFNèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´é²æ£’çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºï¼Œå‡å°‘äº†ç‰¹å¾ä½ç½®å’Œé¢„æµ‹è¯¯å·®ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æå‡å¹…åº¦è¯·å‚è€ƒåŸè®ºæ–‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AGFNå¯åº”ç”¨äºå„ç§å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æåœºæ™¯ï¼Œå¦‚ç¤¾äº¤åª’ä½“æƒ…æ„Ÿç›‘æ§ã€å®¢æˆ·æœåŠ¡è´¨é‡è¯„ä¼°ã€äººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼ŒAGFNèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šæ›´å¥½åœ°ç†è§£ç”¨æˆ·æƒ…ç»ªï¼Œä¼˜åŒ–äº§å“å’ŒæœåŠ¡ï¼Œå¹¶æ”¹å–„ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼ŒAGFNæœ‰æœ›æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¦‚è§†é¢‘ç†è§£ã€æœºå™¨äººå¯¼èˆªç­‰ï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal sentiment analysis (MSA) leverages information fusion from diverse modalities (e.g., text, audio, visual) to enhance sentiment prediction. However, simple fusion techniques often fail to account for variations in modality quality, such as those that are noisy, missing, or semantically conflicting. This oversight leads to suboptimal performance, especially in discerning subtle emotional nuances. To mitigate this limitation, we introduce a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion \textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion mechanism based on information entropy and modality importance. This mechanism mitigates the influence of noisy modalities and prioritizes informative cues following unimodal encoding and cross-modal interaction. Experiments on CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong baselines in accuracy, effectively discerning subtle emotions with robust performance. Visualization analysis of feature representations demonstrates that AGFN enhances generalization by learning from a broader feature distribution, achieved by reducing the correlation between feature location and prediction error, thereby decreasing reliance on specific locations and creating more robust multimodal feature representations.

