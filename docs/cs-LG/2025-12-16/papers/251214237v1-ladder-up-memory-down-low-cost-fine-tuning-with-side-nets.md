---
layout: default
title: Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets
---

# Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets

**arXiv**: [2512.14237v1](https://arxiv.org/abs/2512.14237) | [PDF](https://arxiv.org/pdf/2512.14237.pdf)

**ä½œè€…**: Estelle Zheng, Nathan Cerisara, SÃ©bastien Warichet, Emmanuel Helbert, Christophe Cerisara

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

<<<<<<< HEAD
**æå‡ºLadder Side Tuningæ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§ä¾§ç½‘ç»œè§£å†³å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§è¯­è¨€æ¨¡å‹` `å†…å­˜ä¼˜åŒ–` `ä¾§ç½‘ç»œ` `æ¢¯å­è¿æ¥` `æ‰©å±•å®šå¾‹` `æ€ç»´é“¾æ¨ç†` `è½»é‡çº§æ¶æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰PEFTæ–¹æ³•å¦‚QLoRAè™½å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼Œä½†åå‘ä¼ æ’­ä»å¯¼è‡´é«˜å†…å­˜å ç”¨ï¼Œé™åˆ¶å¤§æ¨¡å‹å¾®è°ƒã€‚
2. è®ºæ–‡æå‡ºLadder Side Tuningï¼Œé€šè¿‡æ·»åŠ è½»é‡çº§ä¾§ç½‘ç»œï¼Œåœ¨å¾®è°ƒæ—¶ä»…æ›´æ–°ä¾§ç½‘ç»œå‚æ•°ï¼Œå¤§å¹…é™ä½å†…å­˜éœ€æ±‚ã€‚
3. å®éªŒæ˜¾ç¤ºLSTåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­æ€§èƒ½ä¸QLoRAç›¸å½“ï¼Œå³°å€¼å†…å­˜é™ä½50%ï¼Œæ”¯æŒ7Bæ¨¡å‹åœ¨12GB GPUä¸Šå¾®è°ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å—é™äºå•†ç”¨GPUçš„å†…å­˜å®¹é‡ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚QLoRAå‡å°‘äº†å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œä½†ä»å› å®Œæ•´æ¨¡å‹çš„åå‘ä¼ æ’­å¯¼è‡´é«˜å†…å­˜å ç”¨ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†Ladder Side Tuningï¼ˆLSTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¾ƒå°‘è¢«æ¢ç´¢çš„PEFTæŠ€æœ¯ï¼Œé€šè¿‡æ·»åŠ è½»é‡çº§ä¾§ç½‘ç»œï¼Œåœ¨åŒ¹é…QLoRAè®¡ç®—æ‰©å±•æ–œç‡çš„åŒæ—¶ï¼Œå°†å³°å€¼å†…å­˜é™ä½50%ã€‚åœ¨æ¶µç›–è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦å’ŒLLMæ‰¹è¯„ä»»åŠ¡çš„ä¸åŒä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLSTå¹³å‡æ€§èƒ½ä¸QLoRAç›¸å½“ï¼ŒåŒæ—¶å†…å­˜æ•ˆç‡æ›´é«˜ã€‚è¿™ç§æ•ˆç‡ä½¿å¾—åœ¨å•ä¸ª12GBæ¶ˆè´¹çº§GPUä¸Šå¾®è°ƒ70äº¿å‚æ•°æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œæ”¯æŒ2kä»¤ç‰Œä¸Šä¸‹æ–‡ä¸”æ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹â€”â€”åœ¨è¿™äº›æ¡ä»¶ä¸‹QLoRAä¼šè€—å°½å†…å­˜ã€‚é™¤äº†å†…å­˜æ•ˆç‡ï¼Œæˆ‘ä»¬è¿˜å»ºç«‹äº†æ‰©å±•å®šå¾‹ï¼Œæ˜¾ç¤ºLSTä¸QLoRAå…·æœ‰ç›¸ä¼¼çš„æ‰©å±•æ€§ã€‚é€šè¿‡åˆ©ç”¨Ladderçš„æ¶æ„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†xLadderï¼Œè¿™æ˜¯ä¸€ç§æ·±åº¦æ‰©å±•å˜ä½“ï¼Œé€šè¿‡äº¤å‰è¿æ¥å¢åŠ æœ‰æ•ˆæ·±åº¦ï¼Œå¹¶åœ¨å›ºå®šå‚æ•°æ•°é‡ä¸‹ç¼©çŸ­æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚Ladderåœ¨å†…å­˜å—é™æ—¶è¡¨ç°å¼ºåŠ²ï¼›xLadderåœ¨æ­¤åŸºç¡€ä¸Šå®ç°äº†æ›´æ·±å±‚æ¬¡çš„æ¨ç†ï¼Œè€Œæ— éœ€é¢å¤–çš„å†…å­˜å¼€é”€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¾®è°ƒä¸­çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚QLoRAè™½ç„¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°ï¼Œä½†åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä»éœ€è®¡ç®—å®Œæ•´æ¨¡å‹çš„æ¢¯åº¦ï¼Œå¯¼è‡´é«˜å†…å­˜å ç”¨ï¼Œé™åˆ¶äº†åœ¨èµ„æºå—é™ç¯å¢ƒï¼ˆå¦‚æ¶ˆè´¹çº§GPUï¼‰ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡æ–°åˆ©ç”¨Ladder Side Tuningï¼ˆLSTï¼‰ï¼Œä¸€ç§åŸºäºä¾§ç½‘ç»œçš„PEFTæŠ€æœ¯ã€‚é€šè¿‡å†»ç»“é¢„è®­ç»ƒLLMçš„ä¸»å¹²ç½‘ç»œï¼Œä»…è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œï¼ˆside networkï¼‰ï¼Œè¯¥ä¾§ç½‘ç»œé€šè¿‡è¿æ¥ï¼ˆladder connectionsï¼‰ä¸ä¸»å¹²ç½‘ç»œäº¤äº’ï¼Œä»è€Œåœ¨å¾®è°ƒæ—¶å¤§å¹…å‡å°‘å†…å­˜éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒLLMä¸»å¹²ç½‘ç»œï¼ˆå†»ç»“å‚æ•°ï¼‰å’Œé™„åŠ çš„ä¾§ç½‘ç»œï¼ˆå¯è®­ç»ƒå‚æ•°ï¼‰ã€‚ä¾§ç½‘ç»œé€šå¸¸ç”±å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æˆ–ç±»ä¼¼è½»é‡ç»“æ„ç»„æˆï¼Œé€šè¿‡æ¢¯å­è¿æ¥ï¼ˆladder connectionsï¼‰ä»ä¸»å¹²ç½‘ç»œçš„ä¸­é—´å±‚æå–ç‰¹å¾ï¼Œå¹¶è¾“å‡ºåˆ°åç»­å±‚æˆ–æœ€ç»ˆé¢„æµ‹ã€‚å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä»…ä¾§ç½‘ç»œçš„å‚æ•°è¢«æ›´æ–°ï¼Œä¸»å¹²ç½‘ç»œä¿æŒå›ºå®šï¼Œé¿å…äº†å®Œæ•´æ¨¡å‹çš„åå‘ä¼ æ’­å†…å­˜å¼€é”€ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯ç³»ç»Ÿæ€§åœ°éªŒè¯å’Œæ‰©å±•LSTä½œä¸ºå†…å­˜é«˜æ•ˆçš„PEFTæ–¹æ³•ã€‚ä¸QLoRAç­‰ç°æœ‰æ–¹æ³•æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒLSTå®Œå…¨é¿å…äº†ä¸»å¹²ç½‘ç»œçš„æ¢¯åº¦è®¡ç®—ï¼Œé€šè¿‡ä¾§ç½‘ç»œå®ç°å‚æ•°é«˜æ•ˆå­¦ä¹ ï¼Œä»è€Œåœ¨å†…å­˜å—é™åœºæ™¯ä¸‹æä¾›æ›´ä¼˜çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¼•å…¥xLadderå˜ä½“ï¼Œé€šè¿‡äº¤å‰è¿æ¥å¢åŠ ç½‘ç»œæ·±åº¦ï¼Œæå‡æ¨ç†èƒ½åŠ›è€Œä¸å¢åŠ å†…å­˜å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ä¾§ç½‘ç»œçš„ç»“æ„ï¼ˆå¦‚MLPå±‚æ•°ã€éšè—ç»´åº¦ï¼‰ã€æ¢¯å­è¿æ¥çš„ä½ç½®ï¼ˆä»ä¸»å¹²ç½‘ç»œç‰¹å®šå±‚æå–ç‰¹å¾ï¼‰ã€æŸå¤±å‡½æ•°ï¼ˆé€šå¸¸ä½¿ç”¨ä»»åŠ¡ç‰¹å®šçš„æŸå¤±ï¼Œå¦‚äº¤å‰ç†µï¼‰ã€‚å‚æ•°è®¾ç½®ä¸Šï¼Œä¾§ç½‘ç»œè§„æ¨¡è¿œå°äºä¸»å¹²ç½‘ç»œï¼ˆä¾‹å¦‚ï¼Œå‚æ•°æ•°é‡å‡å°‘å‡ ä¸ªæ•°é‡çº§ï¼‰ï¼Œä»¥ç¡®ä¿ä½å†…å­˜å ç”¨ã€‚xLadderé€šè¿‡å¼•å…¥è·¨å±‚è¿æ¥ï¼ˆcross-connectionsï¼‰åœ¨ä¾§ç½‘ç»œå†…éƒ¨å®ç°æ·±åº¦æ‰©å±•ï¼Œä¼˜åŒ–æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLSTåœ¨å¤šä¸ªä¸‹æ¸¸åŸºå‡†æµ‹è¯•ï¼ˆè‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦ã€LLMæ‰¹è¯„ä»»åŠ¡ï¼‰ä¸­å¹³å‡æ€§èƒ½ä¸QLoRAç›¸å½“ï¼ŒåŒæ—¶å³°å€¼å†…å­˜é™ä½50%ã€‚å…·ä½“åœ°ï¼Œåœ¨å•ä¸ª12GBæ¶ˆè´¹çº§GPUä¸Šï¼ŒLSTæ”¯æŒå¾®è°ƒ70äº¿å‚æ•°æ¨¡å‹ï¼Œå¤„ç†2kä»¤ç‰Œä¸Šä¸‹æ–‡ä¸”æ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œè€ŒQLoRAåœ¨ç›¸åŒæ¡ä»¶ä¸‹ä¼šè€—å°½å†…å­˜ã€‚æ‰©å±•å®šå¾‹åˆ†æè¡¨æ˜LSTä¸QLoRAå…·æœ‰ç›¸ä¼¼çš„è®¡ç®—æ‰©å±•æ–œç‡ï¼ŒéªŒè¯äº†å…¶å¯æ‰©å±•æ€§ã€‚xLadderå˜ä½“è¿›ä¸€æ­¥æå‡äº†æ¨ç†æ·±åº¦ï¼Œåœ¨å›ºå®šå‚æ•°ä¸‹ä¼˜åŒ–æ€ç»´é“¾æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¦‚è¾¹ç¼˜è®¡ç®—ã€ä¸ªäººè®¾å¤‡ä¸Šçš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œä»¥åŠéœ€è¦é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„ä»»åŠ¡ï¼ˆå¦‚æ–‡æ¡£åˆ†æã€ä»£ç ç”Ÿæˆï¼‰ã€‚å®é™…ä»·å€¼åœ¨äºé™ä½AIéƒ¨ç½²æˆæœ¬ï¼Œä½¿æ›´å¤šç ”ç©¶è€…å’Œå¼€å‘è€…èƒ½å¤Ÿè®¿é—®å’Œå®šåˆ¶å¤§æ¨¡å‹ã€‚æœªæ¥å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹å¾®è°ƒèŒƒå¼ï¼Œä¿ƒè¿›AIæŠ€æœ¯çš„æ™®åŠå’Œæ°‘ä¸»åŒ–ã€‚
=======
**æå‡ºLadder Side Tuningæ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§ä¾§ç½‘ç»œå®ç°å¤§è¯­è¨€æ¨¡å‹ä½æˆæœ¬å¾®è°ƒï¼Œæ˜¾è‘—é™ä½å†…å­˜éœ€æ±‚ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§è¯­è¨€æ¨¡å‹` `å†…å­˜ä¼˜åŒ–` `ä¾§ç½‘ç»œ` `è½»é‡çº§å¾®è°ƒ` `è‡ªç„¶è¯­è¨€ç†è§£` `æ‰©å±•å®šå¾‹` `æ¶ˆè´¹çº§GPU`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰PEFTæ–¹æ³•å¦‚QLoRAè™½å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼Œä½†åå‘ä¼ æ’­ä»å¯¼è‡´é«˜å†…å­˜å ç”¨ï¼Œé™åˆ¶å¤§æ¨¡å‹åœ¨æ¶ˆè´¹çº§GPUä¸Šçš„å¾®è°ƒã€‚
2. æå‡ºLadder Side Tuningï¼ˆLSTï¼‰ï¼Œæ·»åŠ è½»é‡çº§ä¾§ç½‘ç»œï¼Œä»…å¾®è°ƒä¾§ç½‘ç»œå‚æ•°ï¼Œå¤§å¹…é™ä½å†…å­˜éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚
3. å®éªŒæ˜¾ç¤ºLSTå³°å€¼å†…å­˜é™ä½50%ï¼Œåœ¨12GB GPUä¸Šå¾®è°ƒ7Bæ¨¡å‹å¯è¡Œï¼Œæ€§èƒ½ä¸QLoRAç›¸å½“ï¼Œæ‰©å±•å®šå¾‹ç›¸ä¼¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å—é™äºå•†ç”¨GPUçš„å†…å­˜å®¹é‡ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚QLoRAå‡å°‘äº†å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œä½†ä»å› å®Œæ•´æ¨¡å‹çš„åå‘ä¼ æ’­è€Œäº§ç”Ÿé«˜å†…å­˜å ç”¨ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†Ladder Side Tuningï¼ˆLSTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¾ƒå°‘è¢«æ¢ç´¢çš„PEFTæŠ€æœ¯ï¼Œé€šè¿‡æ·»åŠ è½»é‡çº§ä¾§ç½‘ç»œï¼Œåœ¨ä¿æŒä¸QLoRAç›¸ä¼¼è®¡ç®—æ‰©å±•æ–œç‡çš„åŒæ—¶ï¼Œå°†å³°å€¼å†…å­˜é™ä½50%ã€‚åœ¨æ¶µç›–è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦å’ŒLLMæ‰¹è¯„ä»»åŠ¡çš„ä¸åŒä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLSTå¹³å‡æ€§èƒ½ä¸QLoRAç›¸å½“ï¼ŒåŒæ—¶å†…å­˜æ•ˆç‡æ›´é«˜ã€‚è¿™ç§æ•ˆç‡ä½¿å¾—åœ¨å•ä¸ª12GBæ¶ˆè´¹çº§GPUä¸Šå¾®è°ƒ70äº¿å‚æ•°æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œæ”¯æŒ2kä»¤ç‰Œä¸Šä¸‹æ–‡ä¸”æ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹â€”â€”åœ¨è¿™äº›æ¡ä»¶ä¸‹QLoRAä¼šè€—å°½å†…å­˜ã€‚é™¤äº†å†…å­˜æ•ˆç‡ï¼Œæˆ‘ä»¬è¿˜å»ºç«‹äº†æ‰©å±•å®šå¾‹ï¼Œè¡¨æ˜LSTçš„æ‰©å±•æ–¹å¼ä¸QLoRAç›¸ä¼¼ã€‚é€šè¿‡åˆ©ç”¨Ladderçš„æ¶æ„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†xLadderï¼Œè¿™æ˜¯ä¸€ç§æ·±åº¦æ‰©å±•å˜ä½“ï¼Œé€šè¿‡äº¤å‰è¿æ¥å¢åŠ æœ‰æ•ˆæ·±åº¦ï¼Œå¹¶åœ¨å›ºå®šå‚æ•°æ•°é‡ä¸‹ç¼©çŸ­æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚Ladderåœ¨å†…å­˜å—é™æ—¶è¡¨ç°å¼ºåŠ²ï¼›xLadderåœ¨æ­¤åŸºç¡€ä¸Šå®ç°äº†æ›´æ·±å±‚æ¨ç†ï¼Œä¸”æ— é¢å¤–å†…å­˜å¼€é”€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡æ ¸å¿ƒæ–¹æ³•æ˜¯Ladder Side Tuningï¼ˆLSTï¼‰ï¼Œæ•´ä½“æ¡†æ¶åŸºäºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œæ·»åŠ ä¸€ä¸ªè½»é‡çº§ä¾§ç½‘ç»œï¼ˆside networkï¼‰ï¼Œè¯¥ç½‘ç»œé€šè¿‡æ¢¯å­çŠ¶è¿æ¥ï¼ˆladder connectionsï¼‰ä¸ä¸»æ¨¡å‹äº¤äº’ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä»…å¾®è°ƒä¾§ç½‘ç»œå‚æ•°ï¼Œä¸»æ¨¡å‹å‚æ•°ä¿æŒå†»ç»“ï¼Œä»è€Œå¤§å¹…å‡å°‘åå‘ä¼ æ’­æ—¶çš„å†…å­˜å ç”¨ã€‚ä¸ç°æœ‰PEFTæ–¹æ³•ï¼ˆå¦‚QLoRAï¼‰çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼šQLoRAé€šè¿‡é‡åŒ–ç­‰æŠ€æœ¯å‡å°‘å‚æ•°ä½†ä»åœ¨å®Œæ•´æ¨¡å‹ä¸Šè¿›è¡Œåå‘ä¼ æ’­ï¼Œè€ŒLSTé€šè¿‡ä¾§ç½‘ç»œå®ç°å‚æ•°é«˜æ•ˆï¼Œé¿å…äº†ä¸»æ¨¡å‹çš„åå‘ä¼ æ’­å¼€é”€ï¼Œå› æ­¤å†…å­˜æ•ˆç‡æ›´é«˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†xLadderå˜ä½“ï¼Œé€šè¿‡äº¤å‰è¿æ¥å¢åŠ ç½‘ç»œæ·±åº¦ï¼Œæå‡æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LSTåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­æ€§èƒ½ä¸QLoRAç›¸å½“ï¼Œå³°å€¼å†…å­˜é™ä½50%ï¼Œæ”¯æŒåœ¨12GB GPUä¸Šå¾®è°ƒ7Bå‚æ•°æ¨¡å‹ï¼ˆ2kä»¤ç‰Œä¸Šä¸‹æ–‡ï¼‰ï¼Œæ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œæ‰©å±•å®šå¾‹æ˜¾ç¤ºä¸QLoRAç›¸ä¼¼æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶é€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒä¸‹çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ¶ˆè´¹çº§GPUä¸Šçš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œå¯åº”ç”¨äºè‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦æ¨ç†ã€LLMæ‰¹è¯„ç­‰é¢†åŸŸï¼Œé™ä½éƒ¨ç½²æˆæœ¬ï¼Œä¿ƒè¿›AIæŠ€æœ¯æ™®åŠã€‚
>>>>>>> 1c05e1c356e1f28c2e5e6e14cf6811c0d5120ab7

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.

