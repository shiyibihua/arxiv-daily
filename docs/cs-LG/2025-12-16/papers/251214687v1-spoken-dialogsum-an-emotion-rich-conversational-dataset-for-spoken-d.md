---
layout: default
title: Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization
---

# Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization

**arXiv**: [2512.14687v1](https://arxiv.org/abs/2512.14687) | [PDF](https://arxiv.org/pdf/2512.14687.pdf)

**ä½œè€…**: Yen-Ju Lu, Kunxiao Gao, Mingrui Liang, Helin Wang, Thomas Thebaud, Laureano Moro-Velazquez, Najim Dehak, Jesus Villalba

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 12 pages, 2 figures

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpoken DialogSumæ•°æ®é›†ä»¥è§£å†³è¯­éŸ³å¯¹è¯æ‘˜è¦ä¸­ç¼ºä¹æƒ…æ„Ÿå’Œå‰¯è¯­è¨€ä¿¡æ¯å¯¹é½æ•°æ®çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `è¯­éŸ³å¯¹è¯æ‘˜è¦` `æƒ…æ„Ÿè®¡ç®—` `å‰¯è¯­è¨€ä¿¡æ¯` `æ•°æ®é›†æž„å»º` `ç«¯åˆ°ç«¯è¯­éŸ³å»ºæ¨¡` `å¤šæ¨¡æ€å¯¹é½` `æ–‡æœ¬è½¬è¯­éŸ³åˆæˆ` `å¤§åž‹è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è¯­éŸ³å¯¹è¯æ‘˜è¦ç ”ç©¶ç¼ºä¹åŒæ—¶åŒ…å«è¯­éŸ³ã€æ‘˜è¦å’Œå‰¯è¯­è¨€ä¿¡æ¯ï¼ˆå¦‚æƒ…æ„Ÿã€éŸ³é«˜ï¼‰çš„æ•°æ®é›†ï¼Œé™åˆ¶äº†æƒ…æ„Ÿæ„ŸçŸ¥æ¨¡åž‹çš„å‘å±•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•æž„å»ºSpoken DialogSumæ•°æ®é›†ï¼Œå…ˆä½¿ç”¨LLMé‡å†™è„šæœ¬å¹¶æ ‡è®°æƒ…æ„Ÿç­‰å‰¯è¯­è¨€ä¿¡æ¯ï¼Œå†ç”¨TTSåˆæˆå¯¹é½è¯­éŸ³ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºçº¿å®žéªŒè¡¨æ˜Žï¼Œç«¯åˆ°ç«¯éŸ³é¢‘-LLMæ¨¡åž‹åœ¨æƒ…æ„Ÿæ‘˜è¦ä»»åŠ¡ä¸Šæ¯”çº§è”ASR-LLMç³»ç»Ÿåœ¨ROUGE-Låˆ†æ•°ä¸Šç›¸å¯¹æå‡28%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„éŸ³é¢‘è¯­è¨€æ¨¡åž‹èƒ½å¤Ÿå¤„ç†é•¿å¯¹è¯ï¼Œä½†æƒ…æ„Ÿæ„ŸçŸ¥æˆ–è¯­éŸ³å¯¹è¯æ‘˜è¦çš„ç ”ç©¶å› ç¼ºä¹å°†è¯­éŸ³ã€æ‘˜è¦å’Œå‰¯è¯­è¨€çº¿ç´¢å…³è”çš„æ•°æ®è€Œå—é™ã€‚æˆ‘ä»¬å¼•å…¥äº†Spoken DialogSumï¼Œè¿™æ˜¯é¦–ä¸ªå°†åŽŸå§‹å¯¹è¯éŸ³é¢‘ä¸Žäº‹å®žæ‘˜è¦ã€æƒ…æ„Ÿä¸°å¯Œæ‘˜è¦ä»¥åŠè¯´è¯è€…å¹´é¾„ã€æ€§åˆ«å’Œæƒ…æ„Ÿçš„è¯­å¥çº§æ ‡ç­¾å¯¹é½çš„è¯­æ–™åº“ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸¤ä¸ªé˜¶æ®µæž„å»ºï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹é‡å†™DialogSumè„šæœ¬ï¼Œæ·»åŠ Switchboardé£Žæ ¼çš„å¡«å……è¯å’Œåé¦ˆè¯ï¼Œå¹¶ä¸ºæ¯ä¸ªè¯­å¥æ ‡è®°æƒ…æ„Ÿã€éŸ³é«˜å’Œè¯­é€Ÿï¼›å…¶æ¬¡ï¼Œé€šè¿‡å¯Œæœ‰è¡¨çŽ°åŠ›çš„æ–‡æœ¬è½¬è¯­éŸ³å¼•æ“Žä»Žæ ‡è®°è„šæœ¬åˆæˆè¯­éŸ³ï¼Œå¹¶ä¸Žå‰¯è¯­è¨€æ ‡ç­¾å¯¹é½ã€‚Spoken DialogSumåŒ…å«13,460ä¸ªæƒ…æ„Ÿå¤šæ ·çš„å¯¹è¯ï¼Œæ¯ä¸ªå¯¹è¯éƒ½é…æœ‰äº‹å®žæ‘˜è¦å’Œæƒ…æ„Ÿèšç„¦æ‘˜è¦ã€‚æ•°æ®é›†åœ¨çº¿å¯ç”¨ã€‚åŸºçº¿å®žéªŒæ˜¾ç¤ºï¼Œä¸Žçº§è”çš„ASR-LLMç³»ç»Ÿç›¸æ¯”ï¼ŒéŸ³é¢‘-LLMå°†æƒ…æ„Ÿæ‘˜è¦çš„ROUGE-Låˆ†æ•°ç›¸å¯¹æå‡äº†28%ï¼Œè¯å®žäº†ç«¯åˆ°ç«¯è¯­éŸ³å»ºæ¨¡çš„ä»·å€¼ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡çš„æ ¸å¿ƒæ–¹æ³•æ˜¯æž„å»ºSpoken DialogSumæ•°æ®é›†çš„æ¡†æž¶ã€‚æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µï¼Œåˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰å¯¹DialogSumæ–‡æœ¬è„šæœ¬è¿›è¡Œæ”¹å†™ï¼Œæ·»åŠ Switchboardé£Žæ ¼çš„å¡«å……è¯å’Œåé¦ˆè¯ä»¥æ¨¡æ‹ŸçœŸå®žå¯¹è¯ï¼Œå¹¶ä¸ºæ¯ä¸ªè¯­å¥è‡ªåŠ¨æ ‡è®°æƒ…æ„Ÿã€éŸ³é«˜å’Œè¯­é€Ÿç­‰å‰¯è¯­è¨€ä¿¡æ¯ï¼›ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨å¯Œæœ‰è¡¨çŽ°åŠ›çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰å¼•æ“Žï¼ŒåŸºäºŽæ ‡è®°è„šæœ¬åˆæˆè¯­éŸ³ï¼Œç¡®ä¿è¯­éŸ³ä¸Žå‰¯è¯­è¨€æ ‡ç­¾ç²¾ç¡®å¯¹é½ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽé¦–æ¬¡å°†åŽŸå§‹éŸ³é¢‘ã€äº‹å®žæ‘˜è¦ã€æƒ…æ„Ÿæ‘˜è¦å’Œè¯­å¥çº§å‰¯è¯­è¨€æ ‡ç­¾é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ•°æ®é›†ä¸­ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«æ˜¯ï¼ŒçŽ°æœ‰æ•°æ®é›†é€šå¸¸ä»…åŒ…å«æ–‡æœ¬æˆ–è¯­éŸ³ï¼Œç¼ºä¹æƒ…æ„Ÿå’Œå‰¯è¯­è¨€ä¿¡æ¯çš„ç³»ç»Ÿå¯¹é½ï¼Œè€Œæœ¬æ–¹æ³•é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹å®žçŽ°äº†å¤šæ¨¡æ€æ•°æ®çš„ååŒç”Ÿæˆã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœæ˜¯ï¼Œä½¿ç”¨Spoken DialogSumæ•°æ®é›†è®­ç»ƒçš„ç«¯åˆ°ç«¯éŸ³é¢‘-LLMæ¨¡åž‹ï¼Œåœ¨æƒ…æ„Ÿæ‘˜è¦ä»»åŠ¡ä¸Šï¼ŒROUGE-Låˆ†æ•°æ¯”çº§è”ASR-LLMç³»ç»Ÿç›¸å¯¹æå‡äº†28%ï¼Œæ˜¾è‘—è¯æ˜Žäº†ç›´æŽ¥å¤„ç†è¯­éŸ³ä¿¡å·åœ¨æƒ…æ„Ÿæ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è¯­éŸ³åŠ©æ‰‹ã€å®¢æˆ·æœåŠ¡å¯¹è¯åˆ†æžã€æƒ…æ„Ÿè®¡ç®—å’Œå¿ƒç†å¥åº·ç›‘æµ‹ç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚é€šè¿‡æä¾›æƒ…æ„Ÿä¸°å¯Œçš„è¯­éŸ³å¯¹è¯æ•°æ®ï¼Œå¯æ”¯æŒå¼€å‘æ›´æ™ºèƒ½çš„å¯¹è¯ç³»ç»Ÿï¼Œæå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæƒ…æ„Ÿç†è§£èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.

