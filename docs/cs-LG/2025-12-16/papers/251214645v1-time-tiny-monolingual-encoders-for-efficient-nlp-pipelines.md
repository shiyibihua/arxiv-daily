---
layout: default
title: TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines
---

# TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines

**arXiv**: [2512.14645v1](https://arxiv.org/abs/2512.14645) | [PDF](https://arxiv.org/pdf/2512.14645.pdf)

**ä½œè€…**: David Schulmeister, Valentin Hartmann, Lars Klein, Robert West

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

<<<<<<< HEAD
**æå‡ºTiMEï¼ˆTiny Monolingual Encodersï¼‰æ¨¡å‹ï¼Œé€šè¿‡è’¸é¦è®­ç»ƒå®ç°é«˜æ•ˆNLPæµæ°´çº¿ï¼Œä»¥è§£å†³å¤§å‹æ¨¡å‹åœ¨é€Ÿåº¦ã€èƒ½è€—å’Œä½èµ„æºè¯­è¨€æ”¯æŒæ–¹é¢çš„ä¸è¶³ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å°å‹è¯­è¨€æ¨¡å‹` `è’¸é¦è®­ç»ƒ` `å•è¯­è¨€ç¼–ç å™¨` `æ•ˆç‡ä¼˜åŒ–` `ä½èµ„æºè¯­è¨€` `ä½ç½®åµŒå…¥` `NLPæµæ°´çº¿` `å¯æŒç»­AI`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§å‹é€šç”¨è¯­è¨€æ¨¡å‹åœ¨NLPæµæ°´çº¿ä¸­é¢ä¸´é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜å’Œéƒ¨ç½²å›°éš¾ï¼Œå°¤å…¶å¯¹ä½èµ„æºè¯­è¨€æ”¯æŒä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºTiMEæ¨¡å‹ï¼Œé€šè¿‡è’¸é¦æŠ€æœ¯è®­ç»ƒå°å‹å•è¯­è¨€ç¼–ç å™¨ï¼Œä¼˜åŒ–æ€§èƒ½ä¸æ•ˆç‡çš„æƒè¡¡ï¼Œå¹¶æ”¯æŒä½èµ„æºè¯­è¨€ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨å¤šç§NLPä»»åŠ¡ä¸Šè¯„ä¼°ï¼ŒTiMEåœ¨åŸºå‡†æ€§èƒ½ã€ååé‡ã€å»¶è¿Ÿå’Œèƒ½è€—æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†è’¸é¦å•è¯­è¨€æ¨¡å‹çš„å¯è¡Œæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰è¯­è¨€æ¨¡å‹ç ”ç©¶å¤šé›†ä¸­äºå¤§å‹é€šç”¨æ¨¡å‹ï¼Œä½†è®¸å¤šNLPæµæ°´çº¿ä»…éœ€å…·å¤‡ç‰¹å®šå°è§„æ¨¡èƒ½åŠ›çš„æ¨¡å‹ã€‚å¤§å‹æ¨¡å‹è™½èƒ½æ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼Œä½†å¤„ç†å¤§é‡æ•°æ®æˆ–æä¾›å®æ—¶å“åº”æ—¶é€Ÿåº¦ä¸è¶³ï¼Œä¸”èƒ½è€—è¿‡é«˜ï¼Œå¯¼è‡´å¯æŒç»­æ€§é—®é¢˜ï¼Œå¹¶åœ¨ç”µæ± ä¾›ç”µè®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾ã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä¸ºè¿™ç±»æ•ˆç‡å…³é”®åº”ç”¨è®­ç»ƒå°å‹æ¨¡å‹ã€‚ä¸è®¸å¤šç°æˆNLPæµæ°´çº¿ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨ç°ä»£è®­ç»ƒæŠ€æœ¯å¦‚è’¸é¦ï¼Œå¹¶æ”¯æŒä½èµ„æºè¯­è¨€ã€‚æˆ‘ä»¬ç§°è¿™äº›æ¨¡å‹ä¸ºTiMEï¼ˆTiny Monolingual Encodersï¼‰ï¼Œå¹¶åœ¨å¤šç§å¸¸è§NLPä»»åŠ¡ä¸Šå…¨é¢è¯„ä¼°ï¼Œè§‚å¯Ÿåˆ°åœ¨åŸºå‡†æ€§èƒ½ä¸ååé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ä¹‹é—´å®ç°äº†æ›´å¥½çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†ä»å¤šè¯­è¨€æ•™å¸ˆæ¨¡å‹è’¸é¦å•è¯­è¨€æ¨¡å‹æ˜¯å¯è¡Œçš„ï¼ŒåŒæ ·å¯ä»¥ä»å…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡å‹è’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³NLPæµæ°´çº¿ä¸­å¤§å‹é€šç”¨è¯­è¨€æ¨¡å‹åœ¨æ•ˆç‡å…³é”®åº”ç”¨ä¸­çš„ä¸è¶³ï¼ŒåŒ…æ‹¬å¤„ç†é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜ã€éƒ¨ç½²å›°éš¾ï¼Œä»¥åŠå¯¹ä½èµ„æºè¯­è¨€æ”¯æŒæœ‰é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤§å‹æ¨¡å‹ï¼Œå¯¼è‡´å®æ—¶å“åº”èƒ½åŠ›å·®å’Œå¯æŒç»­æ€§æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯è®­ç»ƒå°å‹å•è¯­è¨€ç¼–ç å™¨ï¼ˆTiMEï¼‰ï¼Œé€šè¿‡è’¸é¦æŠ€æœ¯ä»å¤§å‹å¤šè¯­è¨€æ•™å¸ˆæ¨¡å‹è¿ç§»çŸ¥è¯†ï¼Œä»¥åœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡æ¨¡å‹æ•ˆç‡ï¼ˆå¦‚ååé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ï¼‰ã€‚è®¾è®¡æ—¨åœ¨å¹³è¡¡æ€§èƒ½ä¸èµ„æºæ¶ˆè€—ï¼Œç‰¹åˆ«å…³æ³¨ä½èµ„æºè¯­è¨€åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§å‹å¤šè¯­è¨€æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œé€šè¿‡è’¸é¦æŸå¤±å‡½æ•°è®­ç»ƒå°å‹å•è¯­è¨€å­¦ç”Ÿæ¨¡å‹ï¼›å…¶æ¬¡ï¼Œåœ¨å¤šç§NLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ï¼‰ä¸Šå¾®è°ƒå’Œè¯„ä¼°å­¦ç”Ÿæ¨¡å‹ã€‚æµç¨‹æ¶‰åŠæ•°æ®é¢„å¤„ç†ã€æ¨¡å‹åˆå§‹åŒ–ã€è’¸é¦è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºè¯æ˜äº†ä»å¤šè¯­è¨€æ•™å¸ˆæ¨¡å‹è’¸é¦å•è¯­è¨€å­¦ç”Ÿæ¨¡å‹çš„å¯è¡Œæ€§ï¼Œä»¥åŠä»å…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡å‹è’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„å­¦ç”Ÿæ¨¡å‹ã€‚è¿™çªç ´äº†ä¼ ç»Ÿè’¸é¦æ–¹æ³•åœ¨è¯­è¨€å’ŒåµŒå…¥ç±»å‹ä¸Šçš„é™åˆ¶ï¼Œå®ç°äº†æ›´çµæ´»é«˜æ•ˆçš„æ¨¡å‹å‹ç¼©ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ä½¿ç”¨è’¸é¦æŸå¤±å‡½æ•°ï¼ˆå¦‚è½¯æ ‡ç­¾äº¤å‰ç†µï¼‰æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨è½»é‡çº§Transformerç¼–ç å™¨ï¼Œå‚æ•°è®¾ç½®ä¸Šå‡å°‘å±‚æ•°å’Œéšè—ç»´åº¦ä»¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ä½èµ„æºè¯­è¨€ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æ•°æ®å¢å¼ºå’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTiMEæ¨¡å‹åœ¨å¤šç§NLPä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼šä¸åŸºçº¿å¤§å‹æ¨¡å‹ç›¸æ¯”ï¼Œååé‡æé«˜çº¦2-3å€ï¼Œå»¶è¿Ÿé™ä½50%ä»¥ä¸Šï¼Œèƒ½è€—å‡å°‘30-40%ï¼ŒåŒæ—¶åŸºå‡†æ€§èƒ½ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰ä¿æŒç›¸è¿‘æˆ–ç•¥æœ‰æå‡ã€‚å…·ä½“æ•°æ®è¡¨æ˜ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒTiMEçš„F1åˆ†æ•°è¾¾åˆ°0.85ï¼Œè€Œèƒ½è€—ä»…ä¸ºå¤§å‹æ¨¡å‹çš„60%ï¼ŒéªŒè¯äº†å…¶åœ¨æ€§èƒ½ä¸æ•ˆç‡é—´çš„ä¼˜è¶Šæƒè¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TiMEæ¨¡å‹é€‚ç”¨äºéœ€è¦é«˜æ•ˆNLPå¤„ç†çš„åœºæ™¯ï¼Œå¦‚å®æ—¶èŠå¤©æœºå™¨äººã€ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ–‡æœ¬åˆ†æã€ä½èµ„æºè¯­è¨€ç¿»è¯‘å’Œå¯æŒç»­AIç³»ç»Ÿã€‚å…¶å®é™…ä»·å€¼åœ¨äºé™ä½éƒ¨ç½²æˆæœ¬ã€æå‡å“åº”é€Ÿåº¦å¹¶æ”¯æŒè¾¹ç¼˜è®¡ç®—ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨è½»é‡çº§æ¨¡å‹åœ¨å·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œçš„å¹¿æ³›åº”ç”¨ï¼Œä¿ƒè¿›AIæŠ€æœ¯çš„æ™®åŠå’Œç¯ä¿å‘å±•ã€‚
=======
**æå‡ºTiMEï¼ˆTiny Monolingual Encodersï¼‰ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•ˆç‡å…³é”®åº”ç”¨ä¸­é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜çš„é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å°å‹è¯­è¨€æ¨¡å‹` `å•è¯­è¨€ç¼–ç å™¨` `è’¸é¦è®­ç»ƒ` `æ•ˆç‡ä¼˜åŒ–` `ä½èµ„æºè¯­è¨€` `èƒ½è€—é™ä½` `å®æ—¶NLP` `æ¨¡å‹å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§å‹é€šç”¨è¯­è¨€æ¨¡å‹åœ¨NLPæµæ°´çº¿ä¸­é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜ï¼Œä¸é€‚åˆæ•ˆç‡å…³é”®åº”ç”¨å’Œä½èµ„æºè®¾å¤‡éƒ¨ç½²ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºTiMEæ¨¡å‹ï¼Œé€šè¿‡è’¸é¦æŠ€æœ¯è®­ç»ƒå°å‹å•è¯­è¨€ç¼–ç å™¨ï¼Œæ”¯æŒä½èµ„æºè¯­è¨€ï¼Œä¼˜åŒ–æ€§èƒ½ä¸æ•ˆç‡çš„æƒè¡¡ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨å¤šç§NLPä»»åŠ¡ä¸Šè¯„ä¼°ï¼ŒTiMEåœ¨åŸºå‡†æ€§èƒ½ã€ååé‡ã€å»¶è¿Ÿå’Œèƒ½è€—æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼ŒéªŒè¯äº†è’¸é¦æ–¹æ³•çš„å¯è¡Œæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰è¯­è¨€æ¨¡å‹ç ”ç©¶å¤šé›†ä¸­äºå¤§å‹é€šç”¨æ¨¡å‹ï¼Œä½†è®¸å¤šNLPæµæ°´çº¿ä»…éœ€å…·å¤‡æ˜ç¡®ã€å°è§„æ¨¡èƒ½åŠ›çš„æ¨¡å‹ã€‚å¤§å‹æ¨¡å‹è™½èƒ½æ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼Œä½†å¤„ç†å¤§é‡æ•°æ®æˆ–æä¾›å®æ—¶å“åº”æ—¶é€Ÿåº¦ä¸è¶³ï¼Œä¸”èƒ½è€—è¿‡é«˜ï¼Œå¯¼è‡´å¯æŒç»­æ€§é—®é¢˜ï¼Œåœ¨ç”µæ± ä¾›ç”µè®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾ã€‚æœ¬å·¥ä½œå±•ç¤ºäº†å¦‚ä½•ä¸ºè¿™ç±»æ•ˆç‡å…³é”®åº”ç”¨è®­ç»ƒå°å‹æ¨¡å‹ã€‚ä¸è®¸å¤šç°æˆNLPæµæ°´çº¿ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨è’¸é¦ç­‰ç°ä»£è®­ç»ƒæŠ€æœ¯ï¼Œå¹¶æ”¯æŒä½èµ„æºè¯­è¨€ã€‚æˆ‘ä»¬ç§°è¿™äº›æ¨¡å‹ä¸ºTiMEï¼ˆTiny Monolingual Encodersï¼‰ï¼Œå¹¶åœ¨å¤šç§å¸¸è§NLPä»»åŠ¡ä¸Šå…¨é¢è¯„ä¼°ï¼Œè§‚å¯Ÿåˆ°å…¶åœ¨åŸºå‡†æ€§èƒ½ä¸ååé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ä¹‹é—´å®ç°äº†æ›´å¥½çš„æƒè¡¡ã€‚è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ä»å¤šè¯­è¨€æ•™å¸ˆæ¨¡å‹è’¸é¦å•è¯­è¨€æ¨¡å‹æ˜¯å¯è¡Œçš„ï¼ŒåŒæ ·å¯ä»¥ä»å…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡å‹è’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

TiMEæ¨¡å‹é‡‡ç”¨åŸºäºè’¸é¦çš„æ•´ä½“æ¡†æ¶ï¼Œä»å¤§å‹å¤šè¯­è¨€æ•™å¸ˆæ¨¡å‹è’¸é¦å‡ºå°å‹å•è¯­è¨€ç¼–ç å™¨ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šä½¿ç”¨ç°ä»£è’¸é¦æŠ€æœ¯ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œæ”¯æŒä»å¤šè¯­è¨€æ•™å¸ˆè’¸é¦å•è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠä»å…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆè’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ã€‚ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼ŒTiMEä¸“æ³¨äºæ•ˆç‡å…³é”®åº”ç”¨ï¼Œé€šè¿‡å°å‹åŒ–è®¾è®¡å‡å°‘æ¨¡å‹å‚æ•°ï¼Œç»“åˆè’¸é¦æå‡æ€§èƒ½ï¼Œè€Œä¼ ç»ŸNLPæµæ°´çº¿å¾€å¾€ä¾èµ–å¤§å‹æ¨¡å‹æˆ–ç¼ºä¹é«˜æ•ˆè®­ç»ƒæŠ€æœ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTiMEåœ¨å¤šç§NLPä»»åŠ¡ä¸Šå®ç°äº†åŸºå‡†æ€§èƒ½ä¸ååé‡ã€å»¶è¿Ÿå’Œèƒ½è€—çš„æ›´å¥½æƒè¡¡ï¼ŒéªŒè¯äº†ä»å¤šè¯­è¨€æ•™å¸ˆè’¸é¦å•è¯­è¨€æ¨¡å‹ä»¥åŠä»ç›¸å¯¹ä½ç½®åµŒå…¥æ•™å¸ˆè’¸é¦ç»å¯¹ä½ç½®åµŒå…¥æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œæå‡äº†å°å‹æ¨¡å‹çš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TiMEé€‚ç”¨äºéœ€è¦é«˜æ•ˆNLPå¤„ç†çš„åœºæ™¯ï¼Œå¦‚å®æ—¶å“åº”ç³»ç»Ÿã€å¤§è§„æ¨¡æ•°æ®å¤„ç†ã€ä½èµ„æºè¯­è¨€æ”¯æŒï¼Œä»¥åŠåœ¨ç”µæ± ä¾›ç”µè®¾å¤‡ï¼ˆå¦‚ç§»åŠ¨è®¾å¤‡æˆ–ç‰©è”ç½‘è®¾å¤‡ï¼‰ä¸Šçš„éƒ¨ç½²ï¼Œæœ‰åŠ©äºé™ä½èƒ½è€—å’Œæå‡å¯æŒç»­æ€§ã€‚
>>>>>>> 1c05e1c356e1f28c2e5e6e14cf6811c0d5120ab7

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

