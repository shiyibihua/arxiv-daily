---
layout: default
title: TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines
---

# TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines

**arXiv**: [2512.14645v1](https://arxiv.org/abs/2512.14645) | [PDF](https://arxiv.org/pdf/2512.14645.pdf)

**ä½œè€…**: David Schulmeister, Valentin Hartmann, Lars Klein, Robert West

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTiMEï¼ˆå¾®åž‹å•è¯­ç¼–ç å™¨ï¼‰ä»¥è§£å†³å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨æ•ˆçŽ‡å…³é”®åº”ç”¨ä¸­é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜çš„é—®é¢˜**

**å…³é”®è¯**: `å¾®åž‹è¯­è¨€æ¨¡åž‹` `å•è¯­ç¼–ç å™¨` `è’¸é¦è®­ç»ƒ` `æ•ˆçŽ‡ä¼˜åŒ–` `ä½Žèµ„æºè¯­è¨€` `èƒ½è€—é™ä½Ž` `å®žæ—¶å“åº”` `NLPæµæ°´çº¿`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹é€šç”¨è¯­è¨€æ¨¡åž‹åœ¨æ•ˆçŽ‡å…³é”®åº”ç”¨ä¸­é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜ï¼Œéš¾ä»¥å¤„ç†å¤§æ•°æ®æˆ–å®žæ—¶å“åº”ï¼Œä¸”éƒ¨ç½²åœ¨ç”µæ± è®¾å¤‡ä¸Šå­˜åœ¨å¯æŒç»­æ€§é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºTiMEæ¨¡åž‹ï¼Œé€šè¿‡è’¸é¦ç­‰çŽ°ä»£è®­ç»ƒæŠ€æœ¯è®­ç»ƒå°åž‹å•è¯­ç¼–ç å™¨ï¼Œæ”¯æŒä½Žèµ„æºè¯­è¨€ï¼Œä¼˜åŒ–æ€§èƒ½ä¸Žæ•ˆçŽ‡çš„æƒè¡¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¸¸è§NLPä»»åŠ¡ä¸Šè¯„ä¼°ï¼ŒTiMEåœ¨åŸºå‡†æ€§èƒ½ã€åžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—æ–¹é¢è¡¨çŽ°æ›´ä¼˜ï¼ŒéªŒè¯äº†è’¸é¦å•è¯­æ¨¡åž‹çš„å¯è¡Œæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰è¯­è¨€æ¨¡åž‹ç ”ç©¶ä¸»è¦é›†ä¸­äºŽå¤§åž‹é€šç”¨æ¨¡åž‹ï¼Œä½†è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æµæ°´çº¿ä»…éœ€å…·å¤‡æ˜Žç¡®ã€å°åž‹èƒ½åŠ›é›†çš„æ¨¡åž‹ã€‚å¤§åž‹æ¨¡åž‹è™½èƒ½æ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼Œä½†å¤„ç†å¤§é‡æ•°æ®æˆ–æä¾›å®žæ—¶å“åº”æ—¶é€Ÿåº¦ä¸è¶³ï¼Œä¸”èƒ½è€—è¿‡é«˜ï¼Œå¯¼è‡´å¯æŒç»­æ€§æ‹…å¿§åŠåœ¨ç”µæ± ä¾›ç”µè®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•ä¸ºè¿™ç±»æ•ˆçŽ‡å…³é”®åº”ç”¨è®­ç»ƒå°åž‹æ¨¡åž‹ã€‚ä¸Žè®¸å¤šçŽ°æˆNLPæµæ°´çº¿ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡åž‹é‡‡ç”¨è’¸é¦ç­‰çŽ°ä»£è®­ç»ƒæŠ€æœ¯ï¼Œå¹¶æ”¯æŒä½Žèµ„æºè¯­è¨€ã€‚æˆ‘ä»¬ç§°è¿™äº›æ¨¡åž‹ä¸ºTiMEï¼ˆå¾®åž‹å•è¯­ç¼–ç å™¨ï¼‰ï¼Œåœ¨ä¸€ç³»åˆ—å¸¸è§NLPä»»åŠ¡ä¸Šå…¨é¢è¯„ä¼°ï¼Œè§‚å¯Ÿåˆ°åœ¨åŸºå‡†æ€§èƒ½ä¸Žåžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ä¹‹é—´å®žçŽ°äº†æ›´å¥½çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜Žäº†ä»Žå¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹è’¸é¦å•è¯­æ¨¡åž‹æ˜¯å¯è¡Œçš„ï¼ŒåŒæ ·å¯ä»¥ä»Žå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡åž‹è’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡åž‹ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

TiMEçš„æ•´ä½“æ¡†æž¶åŸºäºŽå¾®åž‹å•è¯­ç¼–ç å™¨ï¼Œé‡‡ç”¨è’¸é¦æŠ€æœ¯ä»Žå¤§åž‹å¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹è®­ç»ƒå°åž‹æ¨¡åž‹ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šä»Žå¤šè¯­è¨€æ•™å¸ˆè’¸é¦å•è¯­æ¨¡åž‹ï¼Œä»¥åŠä»Žå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆè’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡åž‹ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼ŒTiMEä¸“æ³¨äºŽæ•ˆçŽ‡ä¼˜åŒ–ï¼Œè€Œéžè¿½æ±‚é€šç”¨æ€§ï¼Œé€šè¿‡çŽ°ä»£è®­ç»ƒæ–¹æ³•æå‡å°åž‹æ¨¡åž‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½Žè®¡ç®—èµ„æºéœ€æ±‚ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒTiMEåœ¨ä¸€ç³»åˆ—å¸¸è§NLPä»»åŠ¡ä¸Šå®žçŽ°äº†åŸºå‡†æ€§èƒ½ä¸Žåžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—çš„æ›´å¥½æƒè¡¡ï¼ŒéªŒè¯äº†ä»Žå¤šè¯­è¨€æ•™å¸ˆè’¸é¦å•è¯­æ¨¡åž‹çš„å¯è¡Œæ€§ï¼Œå¹¶æˆåŠŸä»Žç›¸å¯¹ä½ç½®åµŒå…¥æ•™å¸ˆè’¸é¦å‡ºç»å¯¹ä½ç½®åµŒå…¥æ¨¡åž‹ï¼Œæå‡äº†å°åž‹æ¨¡åž‹çš„æ•ˆçŽ‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

TiMEé€‚ç”¨äºŽéœ€è¦é«˜æ•ˆå¤„ç†å¤§é‡æ•°æ®æˆ–å®žæ—¶å“åº”çš„NLPæµæ°´çº¿ï¼Œå¦‚ä½Žèµ„æºè¯­è¨€å¤„ç†ã€ç”µæ± ä¾›ç”µè®¾å¤‡ï¼ˆå¦‚ç§»åŠ¨è®¾å¤‡æˆ–ç‰©è”ç½‘è®¾å¤‡ï¼‰ä¸Šçš„éƒ¨ç½²ï¼Œä»¥åŠå¯æŒç»­æ€§è¦æ±‚é«˜çš„åº”ç”¨åœºæ™¯ï¼Œèƒ½å‡å°‘èƒ½è€—å¹¶æå‡å“åº”é€Ÿåº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

