---
layout: default
title: Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits
---

# Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits

**arXiv**: [2512.14338v1](https://arxiv.org/abs/2512.14338) | [PDF](https://arxiv.org/pdf/2512.14338.pdf)

**ä½œè€…**: Michael Murray, Tenzin Chan, Kedar Karhadker, Christopher J. Hillar

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºHopfieldç½‘ç»œé€šè¿‡èŒƒæ•°æ•ˆçŽ‡éšå¼å­¦ä¹ å›¾åŒæž„ç±»ï¼Œå®žçŽ°å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦**

**å…³é”®è¯**: `Hopfieldç½‘ç»œ` `éšå¼åç½®` `å›¾åŒæž„` `ä¸å˜å­ç©ºé—´` `èŒƒæ•°æ•ˆçŽ‡` `æ ·æœ¬å¤æ‚åº¦` `ç¾¤ç»“æž„æ•°æ®` `æ¢¯åº¦ä¸‹é™`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¸¸æ˜¾å¼æž„å»ºä¸å˜æ€§ï¼Œä½†éšå¼å­¦ä¹ æœºåˆ¶åœ¨ç¾¤ç»“æž„æ•°æ®ä¸­çš„æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›å°šä¸æ˜Žç¡®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨Hopfieldç½‘ç»œï¼Œé€šè¿‡æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰çš„æ¢¯åº¦ä¸‹é™ï¼Œéšå¼åç½®èŒƒæ•°æ•ˆçŽ‡è§£ï¼Œå®žçŽ°å›¾åŒæž„ç±»å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç½‘ç»œèƒ½åœ¨ä¸‰ç»´ä¸å˜å­ç©ºé—´ä¸­è¡¨ç¤ºåŒæž„ç±»ï¼Œæ ·æœ¬å¤æ‚åº¦ä¸ºå¤šé¡¹å¼ï¼Œå‚æ•°æ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šå­¦ä¹ é—®é¢˜æ¶‰åŠå¯¹ç§°æ€§ï¼Œè™½ç„¶ä¸å˜æ€§å¯ä»¥å†…ç½®åˆ°ç¥žç»æž¶æž„ä¸­ï¼Œä½†åœ¨ç¾¤ç»“æž„æ•°æ®ä¸Šè®­ç»ƒæ—¶ä¹Ÿå¯èƒ½éšå¼å‡ºçŽ°ã€‚æˆ‘ä»¬ç ”ç©¶äº†ç»å…¸Hopfieldç½‘ç»œä¸­çš„è¿™ä¸€çŽ°è±¡ï¼Œå¹¶è¡¨æ˜Žå®ƒä»¬å¯ä»¥ä»Žå°çš„éšæœºæ ·æœ¬ä¸­æŽ¨æ–­å‡ºå›¾çš„å®Œæ•´åŒæž„ç±»ã€‚æˆ‘ä»¬çš„ç»“æžœæ˜¾ç¤ºï¼š(i) å›¾åŒæž„ç±»å¯ä»¥åœ¨ä¸‰ç»´ä¸å˜å­ç©ºé—´ä¸­è¡¨ç¤ºï¼Œ(ii) ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰å…·æœ‰å¯¹èŒƒæ•°æ•ˆçŽ‡è§£çš„éšå¼åç½®ï¼Œè¿™æ”¯æ’‘äº†å­¦ä¹ åŒæž„ç±»çš„å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼Œä»¥åŠ(iii) åœ¨å¤šç§å­¦ä¹ è§„åˆ™ä¸‹ï¼Œå‚æ•°éšç€æ ·æœ¬é‡å¢žé•¿è€Œæ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ã€‚è¿™äº›å‘çŽ°å…±åŒçªå‡ºäº†Hopfieldç½‘ç»œä¸­æ³›åŒ–çš„ç»Ÿä¸€æœºåˆ¶ï¼šå­¦ä¹ ä¸­å¯¹èŒƒæ•°æ•ˆçŽ‡çš„åç½®é©±åŠ¨äº†åœ¨ç¾¤ç»“æž„æ•°æ®ä¸‹è¿‘ä¼¼ä¸å˜æ€§çš„å‡ºçŽ°ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡é‡‡ç”¨ç»å…¸Hopfieldç½‘ç»œä½œä¸ºæ•´ä½“æ¡†æž¶ï¼Œç ”ç©¶å…¶åœ¨å›¾åŒæž„ç±»å­¦ä¹ ä¸­çš„éšå¼åç½®ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽåˆ†æžæ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰çš„è¿‡ç¨‹ï¼Œæ­ç¤ºå…¶å¯¹èŒƒæ•°æ•ˆçŽ‡è§£çš„éšå¼åç½®ï¼Œè¿™é©±åŠ¨äº†ä¸å˜å­ç©ºé—´çš„æ”¶æ•›ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œä¸ä¾èµ–æ˜¾å¼çš„ä¸å˜æ€§è®¾è®¡ï¼Œè€Œæ˜¯é€šè¿‡ä¼˜åŒ–è¿‡ç¨‹è‡ªç„¶æ¶ŒçŽ°è¿‘ä¼¼ä¸å˜æ€§ï¼Œä»Žè€Œæ›´é«˜æ•ˆåœ°å¤„ç†å¯¹ç§°æ€§æ•°æ®ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒHopfieldç½‘ç»œèƒ½ä»Žå°‘é‡éšæœºæ ·æœ¬ä¸­å­¦ä¹ å›¾åŒæž„ç±»ï¼Œæ ·æœ¬å¤æ‚åº¦ä¸ºå¤šé¡¹å¼ç•Œé™ï¼Œä¸”å‚æ•°åœ¨å¤šç§å­¦ä¹ è§„åˆ™ä¸‹æ”¶æ•›åˆ°ä¸‰ç»´ä¸å˜å­ç©ºé—´ï¼ŒéªŒè¯äº†éšå¼åç½®å¯¹æ³›åŒ–çš„å…³é”®ä½œç”¨ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽå›¾ç»“æž„æ•°æ®åˆ†æžã€æ¨¡å¼è¯†åˆ«å’Œæœºå™¨å­¦ä¹ ä¸­çš„å¯¹ç§°æ€§å¤„ç†ï¼Œä¾‹å¦‚ç¤¾äº¤ç½‘ç»œåˆ†æžã€åŒ–å­¦åˆ†å­ç»“æž„åˆ†ç±»ï¼Œä»¥åŠéœ€è¦é«˜æ•ˆå­¦ä¹ ä¸å˜ç‰¹å¾çš„é¢†åŸŸï¼Œæå‡æ¨¡åž‹åœ¨ç¾¤ç»“æž„æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

