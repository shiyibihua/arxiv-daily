---
layout: default
title: On Improving Deep Active Learning with Formal Verification
---

# On Improving Deep Active Learning with Formal Verification

**arXiv**: [2512.14170v1](https://arxiv.org/abs/2512.14170) | [PDF](https://arxiv.org/pdf/2512.14170.pdf)

**ä½œè€…**: Jonathan Spiegelman, Guy Amir, Guy Katz

**åˆ†ç±»**: cs.LG, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå½¢å¼éªŒè¯çš„å¯¹æŠ—æ ·æœ¬å¢žå¼ºæ–¹æ³•ï¼Œä»¥æå‡æ·±åº¦ä¸»åŠ¨å­¦ä¹ çš„æ¨¡åž‹æ³›åŒ–èƒ½åŠ›**

**å…³é”®è¯**: `æ·±åº¦ä¸»åŠ¨å­¦ä¹ ` `å½¢å¼éªŒè¯` `å¯¹æŠ—æ ·æœ¬` `æ•°æ®å¢žå¼º` `æ¨¡åž‹æ³›åŒ–` `é²æ£’æ€§çº¦æŸ` `ç¥žç»ç½‘ç»œè®­ç»ƒ` `æ ‡æ³¨æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ·±åº¦ä¸»åŠ¨å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®å¢žå¼ºæ–¹é¢ä¸»è¦ä¾èµ–åˆæˆè¾“å…¥ï¼Œä½†å¯¹æŠ—æ ·æœ¬çš„ç”Ÿæˆå¤šåŸºäºŽæ¢¯åº¦æ”»å‡»ï¼Œå¯èƒ½æ— æ³•å……åˆ†æå‡æ¨¡åž‹é²æ£’æ€§ã€‚
2. è®ºæ–‡æå‡ºä½¿ç”¨å½¢å¼éªŒè¯ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬èƒ½æ›´æœ‰æ•ˆåœ°è¿åé²æ£’æ€§çº¦æŸï¼Œä»Žè€Œå¢žå¼ºè®­ç»ƒæ•°æ®ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ·±åº¦ä¸»åŠ¨å­¦ä¹ æŠ€æœ¯å’Œæ–°æå‡ºçš„æŠ€æœ¯ä¸Šå‡æ˜¾è‘—æå‡äº†æ¨¡åž‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­çš„æ³›åŒ–æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ä¸»åŠ¨å­¦ä¹ æ—¨åœ¨é€šè¿‡ä¼˜å…ˆæ ‡æ³¨ä¿¡æ¯é‡æœ€å¤§çš„æœªæ ‡è®°æ ·æœ¬æ¥é™ä½Žç¥žç»ç½‘ç»œè®­ç»ƒä¸­çš„æ ‡æ³¨æˆæœ¬ã€‚é™¤äº†é€‰æ‹©æ ‡æ³¨æ ·æœ¬å¤–ï¼Œä¸€äº›æ·±åº¦ä¸»åŠ¨å­¦ä¹ æ–¹æ³•è¿˜é€šè¿‡æ·»åŠ æ— éœ€é¢å¤–æ‰‹åŠ¨æ ‡æ³¨çš„åˆæˆè¾“å…¥æ¥è¿›ä¸€æ­¥å¢žå¼ºæ•°æ®æ•ˆçŽ‡ã€‚æœ¬ç ”ç©¶æŽ¢è®¨äº†å¦‚ä½•é€šè¿‡æ·»åŠ è¿åé²æ£’æ€§çº¦æŸçš„å¯¹æŠ—æ ·æœ¬æ¥å¢žå¼ºè®­ç»ƒæ•°æ®ï¼Œä»Žè€Œæå‡æ·±åº¦ä¸»åŠ¨å­¦ä¹ çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜Žï¼Œé€šè¿‡å½¢å¼éªŒè¯ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬æ¯”åŸºäºŽæ ‡å‡†æ¢¯åº¦æ”»å‡»ç”Ÿæˆçš„æ ·æœ¬è´¡çŒ®æ›´å¤§ã€‚æˆ‘ä»¬å°†æ­¤æ‰©å±•åº”ç”¨äºŽå¤šç§çŽ°ä»£æ·±åº¦ä¸»åŠ¨å­¦ä¹ æŠ€æœ¯ä»¥åŠæˆ‘ä»¬æå‡ºçš„æ–°æŠ€æœ¯ï¼Œå¹¶è¯æ˜Žå…¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡çš„æ ¸å¿ƒæ–¹æ³•æ˜¯å°†å½¢å¼éªŒè¯ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬é›†æˆåˆ°æ·±åº¦ä¸»åŠ¨å­¦ä¹ æ¡†æž¶ä¸­ã€‚æ•´ä½“æ¡†æž¶åŒ…æ‹¬ï¼šåœ¨ä¸»åŠ¨å­¦ä¹ å¾ªçŽ¯ä¸­ï¼Œä¸ä»…é€‰æ‹©ä¿¡æ¯é‡å¤§çš„æœªæ ‡è®°æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œè¿˜é€šè¿‡å½¢å¼éªŒè¯æŠ€æœ¯ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ä½œä¸ºæ•°æ®å¢žå¼ºã€‚å…³é”®æŠ€æœ¯åˆ›æ–°åœ¨äºŽåˆ©ç”¨å½¢å¼éªŒè¯ï¼ˆè€Œéžä¼ ç»Ÿæ¢¯åº¦æ”»å‡»ï¼‰æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬èƒ½æ›´ç²¾ç¡®åœ°è¿åæ¨¡åž‹çš„é²æ£’æ€§çº¦æŸï¼ˆå¦‚å®‰å…¨å±žæ€§ï¼‰ï¼Œä»Žè€Œæä¾›æ›´æœ‰æ•ˆçš„è®­ç»ƒä¿¡å·ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼ŒçŽ°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨åŸºäºŽæ¢¯åº¦çš„æ”»å‡»ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œè€Œæœ¬æ–¹æ³•é€šè¿‡å½¢å¼éªŒè¯ç¡®ä¿ç”Ÿæˆçš„æ ·æœ¬åœ¨ç†è®ºä¸Šæ›´å…·æŒ‘æˆ˜æ€§ï¼Œèƒ½æ›´å…¨é¢åœ°æš´éœ²æ¨¡åž‹å¼±ç‚¹ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œä½¿ç”¨å½¢å¼éªŒè¯ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬è¿›è¡Œæ•°æ®å¢žå¼ºï¼Œåœ¨å¤šä¸ªæ·±åº¦ä¸»åŠ¨å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚ä¸ç¡®å®šæ€§é‡‡æ ·ã€å¤šæ ·æ€§é‡‡æ ·ï¼‰å’Œæ–°æå‡ºçš„æŠ€æœ¯ä¸Šï¼Œæ¨¡åž‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚CIFAR-10ã€MNISTï¼‰ä¸­çš„æ³›åŒ–æ€§èƒ½å¾—åˆ°æ˜¾è‘—æå‡ï¼Œå…·ä½“æå‡å¹…åº¦å› æ•°æ®é›†å’ŒæŠ€æœ¯è€Œå¼‚ï¼Œä½†æ™®éä¼˜äºŽåŸºäºŽæ¢¯åº¦æ”»å‡»çš„å¢žå¼ºæ–¹æ³•ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽéœ€è¦é«˜æ•°æ®æ•ˆçŽ‡çš„æ·±åº¦å­¦ä¹ åœºæ™¯ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å›¾åƒåˆ†æžæˆ–è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå…¶ä¸­æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”æ¨¡åž‹é²æ£’æ€§è‡³å…³é‡è¦ã€‚é€šè¿‡å‡å°‘æ ‡æ³¨éœ€æ±‚å¹¶æå‡æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½é™ä½Žå®žé™…éƒ¨ç½²ä¸­çš„èµ„æºæ¶ˆè€—å’Œé£Žé™©ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.

