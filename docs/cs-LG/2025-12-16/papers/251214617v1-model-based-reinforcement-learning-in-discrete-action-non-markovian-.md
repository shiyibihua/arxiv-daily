---
layout: default
title: Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes
---

# Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14617" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14617v1</a>
  <a href="https://arxiv.org/pdf/2512.14617.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14617v1" onclick="toggleFavorite(this, '2512.14617v1', 'Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alessandro Trapasso, Luca Iocchi, Fabio Patrizi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 19 pages, 32 figures, includes appendix

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQR-MAXç®—æ³•ï¼Œè§£å†³ç¦»æ•£åŠ¨ä½œéé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹ä¸­çš„æ¨¡å‹å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `éé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æ¨¡å‹å­¦ä¹ ` `å¥–åŠ±æœºå™¨` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿé©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ éš¾ä»¥å¤„ç†å¥–åŠ±ä¾èµ–äºå®Œæ•´å†å²çš„ä»»åŠ¡ï¼Œéé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹å¼ºåŒ–å­¦ä¹ ç¼ºä¹æœ€ä¼˜æ€§å’Œæ ·æœ¬æ•ˆç‡ä¿è¯ã€‚
2. QR-MAXç®—æ³•é€šè¿‡å¥–åŠ±æœºå™¨åˆ†è§£é©¬å°”å¯å¤«è½¬ç§»å­¦ä¹ å’Œéé©¬å°”å¯å¤«å¥–åŠ±å¤„ç†ï¼Œå®ç°é«˜æ•ˆå­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒQR-MAXåœ¨æ ·æœ¬æ•ˆç‡å’Œå¯»æ‰¾æœ€ä¼˜ç­–ç•¥çš„é²æ£’æ€§æ–¹é¢ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šå®é™…å†³ç­–é—®é¢˜ä¾èµ–äºæ•´ä¸ªç³»ç»Ÿå†å²ï¼Œè€Œéä»…ä¾èµ–äºè¾¾åˆ°å…·æœ‰æœŸæœ›å±æ€§çš„çŠ¶æ€ã€‚é©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¸é€‚ç”¨äºæ­¤ç±»ä»»åŠ¡ï¼Œè€Œéé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹ï¼ˆNMRDPsï¼‰çš„RLä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå¤„ç†æ—¶é—´ä¾èµ–æ€§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•é•¿æœŸä»¥æ¥ç¼ºä¹å…³äºï¼ˆè¿‘ï¼‰æœ€ä¼˜æ€§å’Œæ ·æœ¬æ•ˆç‡çš„å½¢å¼ä¿è¯ã€‚æˆ‘ä»¬æå‡ºäº†QR-MAXï¼Œä¸€ç§ç”¨äºç¦»æ•£NMRDPsçš„æ–°å‹åŸºäºæ¨¡å‹çš„ç®—æ³•ï¼Œå®ƒé€šè¿‡å¥–åŠ±æœºå™¨å°†é©¬å°”å¯å¤«è½¬ç§»å­¦ä¹ ä¸éé©¬å°”å¯å¤«å¥–åŠ±å¤„ç†åˆ†è§£å¼€æ¥ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºç¦»æ•£åŠ¨ä½œNMRDPsçš„åŸºäºæ¨¡å‹çš„RLç®—æ³•ï¼Œå®ƒåˆ©ç”¨è¿™ç§åˆ†è§£æ¥è·å¾—PACæ”¶æ•›åˆ°å…·æœ‰å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦çš„Îµ-æœ€ä¼˜ç­–ç•¥ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†QR-MAXæ‰©å±•åˆ°å…·æœ‰Bucket-QR-MAXçš„è¿ç»­çŠ¶æ€ç©ºé—´ï¼ŒBucket-QR-MAXæ˜¯ä¸€ç§åŸºäºSimHashçš„ç¦»æ•£å™¨ï¼Œå®ƒä¿ç•™äº†ç›¸åŒçš„åˆ†è§£ç»“æ„ï¼Œå¹¶åœ¨æ²¡æœ‰æ‰‹åŠ¨ç½‘æ ¼åˆ’åˆ†æˆ–å‡½æ•°é€¼è¿‘çš„æƒ…å†µä¸‹å®ç°å¿«é€Ÿç¨³å®šçš„å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨å¤æ‚åº¦ä¸æ–­å¢åŠ çš„ç¯å¢ƒä¸­ï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°ä»£æœ€å…ˆè¿›çš„åŸºäºæ¨¡å‹çš„RLæ–¹æ³•è¿›è¡Œäº†å®éªŒæ¯”è¾ƒï¼Œç»“æœè¡¨æ˜åœ¨æ ·æœ¬æ•ˆç‡æ–¹é¢æœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨å¯»æ‰¾æœ€ä¼˜ç­–ç•¥æ–¹é¢å…·æœ‰æ›´é«˜çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¦»æ•£åŠ¨ä½œéé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹ï¼ˆNMRDPsï¼‰ä¸­çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚ä¼ ç»Ÿçš„é©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†å¥–åŠ±ä¾èµ–äºæ•´ä¸ªç³»ç»Ÿå†å²çš„ä»»åŠ¡ï¼Œè€Œç›´æ¥åº”ç”¨éé©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ æ–¹æ³•åˆç¼ºä¹ç†è®ºä¿è¯ï¼Œå¦‚æœ€ä¼˜æ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ ·æœ¬æ‰èƒ½æ”¶æ•›åˆ°è¾ƒå¥½çš„ç­–ç•¥ï¼Œæˆ–è€…éœ€è¦æ‰‹åŠ¨è®¾è®¡å¤æ‚çš„å‡½æ•°é€¼è¿‘å™¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é©¬å°”å¯å¤«è½¬ç§»å­¦ä¹ ä¸éé©¬å°”å¯å¤«å¥–åŠ±å¤„ç†è¿›è¡Œè§£è€¦ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨å¥–åŠ±æœºå™¨ï¼ˆReward Machineï¼‰æ¥å»ºæ¨¡éé©¬å°”å¯å¤«å¥–åŠ±å‡½æ•°ï¼Œå¹¶å°†å…¶ä¸é©¬å°”å¯å¤«è½¬ç§»æ¨¡å‹ç›¸ç»“åˆã€‚é€šè¿‡è¿™ç§åˆ†è§£ï¼Œå¯ä»¥åˆ†åˆ«å­¦ä¹ é©¬å°”å¯å¤«è½¬ç§»æ¨¡å‹å’Œå¥–åŠ±æœºå™¨ï¼Œä»è€Œç®€åŒ–å­¦ä¹ è¿‡ç¨‹å¹¶æé«˜æ ·æœ¬æ•ˆç‡ã€‚è¿™ç§è§£è€¦ä¹Ÿä½¿å¾—ç®—æ³•æ›´å®¹æ˜“è¿›è¡Œç†è®ºåˆ†æï¼Œä»è€Œå¯ä»¥ç»™å‡ºæœ€ä¼˜æ€§å’Œæ ·æœ¬å¤æ‚åº¦çš„ä¿è¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQR-MAXç®—æ³•çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) é©¬å°”å¯å¤«è½¬ç§»æ¨¡å‹å­¦ä¹ ï¼šä½¿ç”¨æ ‡å‡†çš„æ¨¡å‹å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰æ¥å­¦ä¹ ç¯å¢ƒçš„é©¬å°”å¯å¤«è½¬ç§»æ¨¡å‹ã€‚2) å¥–åŠ±æœºå™¨å­¦ä¹ ï¼šå­¦ä¹ å¥–åŠ±æœºå™¨çš„çŠ¶æ€è½¬ç§»å’Œå¥–åŠ±å‡½æ•°ã€‚3) ç­–ç•¥ä¼˜åŒ–ï¼šåŸºäºå­¦ä¹ åˆ°çš„é©¬å°”å¯å¤«è½¬ç§»æ¨¡å‹å’Œå¥–åŠ±æœºå™¨ï¼Œä½¿ç”¨Q-learningç®—æ³•æ¥ä¼˜åŒ–ç­–ç•¥ã€‚å¯¹äºè¿ç»­çŠ¶æ€ç©ºé—´ï¼Œè®ºæ–‡æå‡ºäº†Bucket-QR-MAXç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨SimHashè¿›è¡Œç¦»æ•£åŒ–ï¼Œå¹¶ä¿ç•™äº†QR-MAXçš„åˆ†è§£ç»“æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†é©¬å°”å¯å¤«è½¬ç§»å­¦ä¹ ä¸éé©¬å°”å¯å¤«å¥–åŠ±å¤„ç†è¿›è¡Œè§£è€¦ï¼Œå¹¶é€šè¿‡å¥–åŠ±æœºå™¨æ¥å®ç°éé©¬å°”å¯å¤«å¥–åŠ±å‡½æ•°çš„å»ºæ¨¡ã€‚è¿™ç§è§£è€¦ä½¿å¾—ç®—æ³•å¯ä»¥åˆ†åˆ«å­¦ä¹ é©¬å°”å¯å¤«è½¬ç§»æ¨¡å‹å’Œå¥–åŠ±æœºå™¨ï¼Œä»è€Œç®€åŒ–å­¦ä¹ è¿‡ç¨‹å¹¶æé«˜æ ·æœ¬æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æä¾›äº†ç®—æ³•çš„PACæ”¶æ•›æ€§è¯æ˜ï¼Œè¯æ˜äº†ç®—æ³•å¯ä»¥åœ¨å¤šé¡¹å¼æ—¶é—´å†…æ”¶æ•›åˆ°Îµ-æœ€ä¼˜ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šQR-MAXç®—æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¥–åŠ±æœºå™¨çš„è¡¨ç¤ºï¼šå¥–åŠ±æœºå™¨ä½¿ç”¨æœ‰é™çŠ¶æ€è‡ªåŠ¨æœºæ¥è¡¨ç¤ºéé©¬å°”å¯å¤«å¥–åŠ±å‡½æ•°ã€‚2) Q-learningç®—æ³•ï¼šä½¿ç”¨æ ‡å‡†çš„Q-learningç®—æ³•æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½†Qå‡½æ•°çš„è¾“å…¥æ˜¯çŠ¶æ€å’Œå¥–åŠ±æœºå™¨çš„çŠ¶æ€ã€‚3) SimHashç¦»æ•£åŒ–ï¼šBucket-QR-MAXç®—æ³•ä½¿ç”¨SimHashè¿›è¡ŒçŠ¶æ€ç©ºé—´çš„ç¦»æ•£åŒ–ï¼ŒSimHashæ˜¯ä¸€ç§å±€éƒ¨æ•æ„Ÿå“ˆå¸Œç®—æ³•ï¼Œå¯ä»¥ä¿è¯ç›¸ä¼¼çš„çŠ¶æ€è¢«æ˜ å°„åˆ°ç›¸åŒçš„ç¦»æ•£çŠ¶æ€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒQR-MAXç®—æ³•åœ¨å¤šä¸ªå¤æ‚ç¯å¢ƒä¸­ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒQR-MAXåœ¨æ ·æœ¬æ•ˆç‡æ–¹é¢æœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨å¯»æ‰¾æœ€ä¼˜ç­–ç•¥æ–¹é¢å…·æœ‰æ›´é«˜çš„é²æ£’æ€§ã€‚åœ¨æŸäº›ç¯å¢ƒä¸­ï¼ŒQR-MAXèƒ½å¤Ÿæ›´å¿«åœ°æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¾¾åˆ°æ›´é«˜çš„ç´¯ç§¯å¥–åŠ±ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦è€ƒè™‘å†å²ä¿¡æ¯çš„å†³ç­–é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€ä»»åŠ¡è§„åˆ’ã€æ¸¸æˆAIç­‰ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ™ºèƒ½ä½“çš„æˆåŠŸä¸ä»…å–å†³äºå½“å‰çŠ¶æ€ï¼Œè¿˜å–å†³äºä¹‹å‰çš„è¡Œä¸ºåºåˆ—ã€‚è¯¥ç®—æ³•çš„å®é™…ä»·å€¼åœ¨äºæé«˜æ ·æœ¬æ•ˆç‡å’Œé²æ£’æ€§ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶æœ‰æœ›æ¨åŠ¨å¼ºåŒ–å­¦ä¹ åœ¨æ›´å¤æ‚å’Œå®é™…çš„åº”ç”¨ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

