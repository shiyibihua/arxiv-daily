---
layout: default
title: RePo: Language Models with Context Re-Positioning
---

# RePo: Language Models with Context Re-Positioning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14391" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14391v1</a>
  <a href="https://arxiv.org/pdf/2512.14391.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14391v1" onclick="toggleFavorite(this, '2512.14391v1', 'RePo: Language Models with Context Re-Positioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huayang Li, Tianyu Zhao, Richard Sproat

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/SakanaAI/repo)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RePoï¼šé€šè¿‡ä¸Šä¸‹æ–‡é‡å®šä½å¢å¼ºè¯­è¨€æ¨¡å‹å¤„ç†å™ªå£°å’Œé•¿æ–‡æœ¬èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡é‡å®šä½` `è¯­è¨€æ¨¡å‹` `ä½ç½®ç¼–ç ` `è®¤çŸ¥è´Ÿè·` `é•¿æ–‡æœ¬å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMé‡‡ç”¨å›ºå®šä½ç½®ç´¢å¼•ï¼Œå¯¼è‡´è®¤çŸ¥è´Ÿè·è¿‡é«˜ï¼Œå½±å“æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ¨ç†å’Œæ³¨æ„åŠ›åˆ†é…ã€‚
2. RePoé€šè¿‡å¯å¾®æ¨¡å—åŠ¨æ€åˆ†é…tokenä½ç½®ï¼Œæ•æ‰ä¸Šä¸‹æ–‡ä¾èµ–ï¼Œé™ä½è®¤çŸ¥è´Ÿè·ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRePoåœ¨å™ªå£°ä¸Šä¸‹æ–‡ã€ç»“æ„åŒ–æ•°æ®å’Œé•¿æ–‡æœ¬ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡ï¼ŒåŒæ—¶ä¿æŒäº†é€šç”¨çŸ­æ–‡æœ¬ä»»åŠ¡çš„ç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸Šä¸‹æ–‡å­¦ä¹ æ˜¯ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºç¡€ï¼›ç„¶è€Œï¼Œç°æœ‰çš„æ¶æ„é€šè¿‡åˆ†é…çº¿æ€§æˆ–æ’å®šçš„ä½ç½®ç´¢å¼•æ¥æ–½åŠ åˆšæ€§å’Œå›ºå®šçš„ä¸Šä¸‹æ–‡ç»“æ„ã€‚å€Ÿé‰´è®¤çŸ¥è´Ÿè·ç†è®ºï¼ˆCLTï¼‰ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§ç¼ºä¹ä¿¡æ¯çš„ç»“æ„å¢åŠ äº†é¢å¤–çš„è®¤çŸ¥è´Ÿè·ï¼Œæ¶ˆè€—äº†æœ‰é™çš„å·¥ä½œè®°å¿†å®¹é‡ï¼Œè€Œè¿™äº›å®¹é‡åº”è¯¥åˆ†é…ç»™æ·±åº¦æ¨ç†å’Œæ³¨æ„åŠ›åˆ†é…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶RePoï¼Œé€šè¿‡ä¸Šä¸‹æ–‡é‡å®šä½æ¥å‡å°‘é¢å¤–çš„è´Ÿè·ã€‚ä¸æ ‡å‡†æ–¹æ³•ä¸åŒï¼ŒRePoåˆ©ç”¨ä¸€ä¸ªå¯å¾®æ¨¡å—$f_Ï†$æ¥åˆ†é…tokenä½ç½®ï¼Œä»¥æ•æ‰ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œè€Œä¸æ˜¯ä¾èµ–äºé¢„å®šä¹‰çš„æ•´æ•°èŒƒå›´ã€‚é€šè¿‡åœ¨OLMo-2 1Bä¸»å¹²ä¸ŠæŒç»­é¢„è®­ç»ƒï¼Œæˆ‘ä»¬è¯æ˜RePoæ˜¾è‘—æé«˜äº†åœ¨æ¶‰åŠå™ªå£°ä¸Šä¸‹æ–‡ã€ç»“æ„åŒ–æ•°æ®å’Œæ›´é•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†åœ¨ä¸€èˆ¬çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šçš„ç«äº‰æ€§èƒ½ã€‚è¯¦ç»†çš„åˆ†æè¡¨æ˜ï¼ŒRePoæˆåŠŸåœ°å°†æ›´é«˜çš„æ³¨æ„åŠ›åˆ†é…ç»™é¥è¿œä½†ç›¸å…³çš„ä¿¡æ¯ï¼Œåœ¨å¯†é›†å’Œéçº¿æ€§ç©ºé—´ä¸­åˆ†é…ä½ç½®ï¼Œå¹¶æ•æ‰è¾“å…¥ä¸Šä¸‹æ–‡çš„å†…åœ¨ç»“æ„ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨https://github.com/SakanaAI/repoè·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿æ–‡æœ¬ã€å™ªå£°æ•°æ®æˆ–ç»“æ„åŒ–ä¿¡æ¯æ—¶ï¼Œç”±äºå…¶å›ºå®šçš„ä½ç½®ç¼–ç æ–¹å¼ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æœ‰æ•ˆæ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢åŠ äº†é¢å¤–çš„è®¤çŸ¥è´Ÿè·ã€‚è¿™ç§å›ºå®šçš„ä¸Šä¸‹æ–‡ç»“æ„é™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚æ¨ç†å’Œæ³¨æ„åŠ›åˆ†é…æ–¹é¢çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å…³æ³¨ distant ä½† relevant ä¿¡æ¯çš„åœºæ™¯ä¸‹ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRePoçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å­¦ä¹ ä¸€ç§åŠ¨æ€çš„ä½ç½®ç¼–ç æ–¹å¼ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯è‡ªé€‚åº”åœ°è°ƒæ•´ token çš„ä½ç½®è¡¨ç¤ºã€‚è¿™ç§æ–¹å¼æ—¨åœ¨é™ä½æ¨¡å‹å¤„ç†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è®¤çŸ¥è´Ÿè·ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„è®¡ç®—èµ„æºè¿›è¡Œæ¨ç†å’Œæ³¨æ„åŠ›åˆ†é…ã€‚é€šè¿‡å­¦ä¹ ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼ŒRePo èƒ½å¤Ÿå°†ç›¸å…³çš„ token æ”¾ç½®åœ¨æ›´è¿‘çš„ä½ç½®ï¼Œä»è€Œæ›´å®¹æ˜“è¢«æ¨¡å‹æ•æ‰åˆ°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRePo çš„æ•´ä½“æ¡†æ¶æ˜¯åœ¨ç°æœ‰çš„ Transformer æ¶æ„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†ä¸€ä¸ªå¯å¾®çš„ä½ç½®ç¼–ç æ¨¡å— $f_Ï†$ã€‚è¯¥æ¨¡å—æ¥æ”¶ token çš„ embedding ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå¯¹åº”çš„ä½ç½®ç¼–ç ã€‚è¿™äº›ä½ç½®ç¼–ç ä¸å†æ˜¯é¢„å®šä¹‰çš„æ•´æ•°åºåˆ—ï¼Œè€Œæ˜¯æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯åŠ¨æ€ç”Ÿæˆçš„ã€‚æ¨¡å‹é¦–å…ˆé€šè¿‡æ ‡å‡†çš„ Transformer å±‚å¤„ç†è¾“å…¥åºåˆ—ï¼Œç„¶åå°†è¾“å‡ºä¼ é€’ç»™ $f_Ï†$ æ¨¡å—ç”Ÿæˆä½ç½®ç¼–ç ï¼Œæœ€åå°†ä½ç½®ç¼–ç åŠ åˆ° token embedding ä¸Šï¼Œä½œä¸ºåç»­ Transformer å±‚çš„è¾“å…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šRePo æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶åŠ¨æ€çš„ä½ç½®ç¼–ç æ–¹å¼ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šä½ç½®ç¼–ç ç›¸æ¯”ï¼ŒRePo èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯è‡ªé€‚åº”åœ°è°ƒæ•´ token çš„ä½ç½®è¡¨ç¤ºï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚è¿™ç§åŠ¨æ€çš„ä½ç½®ç¼–ç æ–¹å¼ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†é•¿æ–‡æœ¬ã€å™ªå£°æ•°æ®å’Œç»“æ„åŒ–ä¿¡æ¯ï¼Œæé«˜äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šRePo çš„å…³é”®è®¾è®¡åŒ…æ‹¬ $f_Ï†$ æ¨¡å—çš„å…·ä½“å®ç°æ–¹å¼å’Œè®­ç»ƒç­–ç•¥ã€‚$f_Ï†$ å¯ä»¥æ˜¯ä¸€ä¸ªå°å‹çš„ç¥ç»ç½‘ç»œï¼Œä¾‹å¦‚ MLP æˆ– Transformer å±‚ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒRePo ä¸ä¸»å¹²è¯­è¨€æ¨¡å‹ä¸€èµ·è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–è¯­è¨€æ¨¡å‹çš„æŸå¤±å‡½æ•°ã€‚ä¸ºäº†é¼“åŠ± $f_Ï†$ å­¦ä¹ æœ‰æ„ä¹‰çš„ä½ç½®ç¼–ç ï¼Œå¯ä»¥å¼•å…¥é¢å¤–çš„æ­£åˆ™åŒ–é¡¹ï¼Œä¾‹å¦‚é¼“åŠ±ä½ç½®ç¼–ç çš„ç¨€ç–æ€§æˆ–å¹³æ»‘æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RePo åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ¶‰åŠå™ªå£°ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ä¸Šï¼ŒRePo èƒ½å¤Ÿæœ‰æ•ˆè¿‡æ»¤å™ªå£°ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚åœ¨ç»“æ„åŒ–æ•°æ®ä»»åŠ¡ä¸Šï¼ŒRePo èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ•°æ®ä¹‹é—´çš„å…³ç³»ï¼Œæé«˜æ¨¡å‹çš„é¢„æµ‹ç²¾åº¦ã€‚åœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šï¼ŒRePo èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæé«˜æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRePo åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RePo çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ï¼šä¿¡æ¯æ£€ç´¢ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡å¢å¼ºæ¨¡å‹å¤„ç†å™ªå£°å’Œé•¿æ–‡æœ¬çš„èƒ½åŠ›ï¼ŒRePo å¯ä»¥æé«˜è¿™äº›åº”ç”¨åœ¨å®é™…åœºæ™¯ä¸­çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¿¡æ¯æ£€ç´¢ä¸­ï¼ŒRePo å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾ï¼Œä»è€Œè¿”å›æ›´ç›¸å…³çš„ç»“æœã€‚åœ¨æœºå™¨ç¿»è¯‘ä¸­ï¼ŒRePo å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å¤„ç†é•¿å¥å­å’Œå¤æ‚çš„è¯­æ³•ç»“æ„ï¼Œä»è€Œç”Ÿæˆæ›´æµç•…å’Œå‡†ç¡®çš„ç¿»è¯‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_Ï†$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.

