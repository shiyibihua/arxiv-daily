---
layout: default
title: ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning
---

# ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning

**arXiv**: [2512.14619v1](https://arxiv.org/abs/2512.14619) | [PDF](https://arxiv.org/pdf/2512.14619.pdf)

**ä½œè€…**: Chaohao Yuan, Zhenjie Song, Ercan Engin Kuruoglu, Kangfei Zhao, Yang Liu, Deli Zhao, Hong Cheng, Yu Rong

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Accepted by WSDM 2026

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/chaohaoyuan/ParaFormer)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPageRank Transformerï¼ˆParaFormerï¼‰ä»¥è§£å†³å›¾Transformerä¸­å…¨å±€æ³¨æ„åŠ›å¯¼è‡´çš„è¿‡å¹³æ»‘é—®é¢˜ï¼Œæå‡å›¾è¡¨ç¤ºå­¦ä¹ æ€§èƒ½ã€‚**

**å…³é”®è¯**: `å›¾Transformer` `è¿‡å¹³æ»‘é—®é¢˜` `PageRankç®—æ³•` `è‡ªé€‚åº”æ»¤æ³¢` `å›¾è¡¨ç¤ºå­¦ä¹ ` `èŠ‚ç‚¹åˆ†ç±»` `å›¾åˆ†ç±»` `å…¨å±€æ³¨æ„åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å›¾Transformerçš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶å­˜åœ¨ä¸¥é‡è¿‡å¹³æ»‘é—®é¢˜ï¼Œå¯¼è‡´èŠ‚ç‚¹è¡¨ç¤ºè¶‹åŒï¼Œå½±å“å›¾å­¦ä¹ æ€§èƒ½ã€‚
2. æå‡ºPageRank Transformerï¼ˆParaFormerï¼‰ï¼Œé€šè¿‡PageRankå¢žå¼ºçš„æ³¨æ„åŠ›æ¨¡å—æ¨¡æ‹Ÿæ·±åº¦Transformerï¼Œå®žçŽ°è‡ªé€‚åº”æ»¤æ³¢ä»¥ç¼“è§£è¿‡å¹³æ»‘ã€‚
3. åœ¨11ä¸ªæ•°æ®é›†ä¸Šï¼ŒParaFormeråœ¨èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ä¸­å‡è¡¨çŽ°å‡ºæ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾Transformerï¼ˆGTsï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„å›¾å­¦ä¹ å·¥å…·ï¼Œåˆ©ç”¨å…¶å…¨è¿žæŽ¥ç‰¹æ€§æœ‰æ•ˆæ•èŽ·å…¨å±€ä¿¡æ¯ã€‚ä¸ºåº”å¯¹æ·±åº¦å›¾ç¥žç»ç½‘ç»œï¼ˆGNNsï¼‰ä¸­çš„è¿‡å¹³æ»‘é—®é¢˜ï¼Œå…¨å±€æ³¨æ„åŠ›è¢«å¼•å…¥ï¼Œä»Žè€Œæ— éœ€ä¾èµ–æ·±åº¦GNNsã€‚ç„¶è€Œï¼Œé€šè¿‡å®žè¯å’Œç†è®ºåˆ†æžï¼Œæˆ‘ä»¬å‘çŽ°å…¨å±€æ³¨æ„åŠ›æœ¬èº«è¡¨çŽ°å‡ºä¸¥é‡çš„è¿‡å¹³æ»‘çŽ°è±¡ï¼Œç”±äºŽå…¶å›ºæœ‰çš„ä½Žé€šæ»¤æ³¢ç‰¹æ€§ï¼Œå¯¼è‡´èŠ‚ç‚¹è¡¨ç¤ºå˜å¾—éš¾ä»¥åŒºåˆ†ï¼Œè¿™ç§æ•ˆåº”ç”šè‡³æ¯”GNNsä¸­è§‚å¯Ÿåˆ°çš„æ›´å¼ºã€‚ä¸ºç¼“è§£æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºPageRank Transformerï¼ˆParaFormerï¼‰ï¼Œå…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªPageRankå¢žå¼ºçš„æ³¨æ„åŠ›æ¨¡å—ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿæ·±åº¦Transformerçš„è¡Œä¸ºã€‚æˆ‘ä»¬ä»Žç†è®ºå’Œå®žè¯ä¸Šè¯æ˜Žï¼ŒParaFormeré€šè¿‡å……å½“è‡ªé€‚åº”é€šæ»¤æ³¢å™¨æ¥å‡è½»è¿‡å¹³æ»‘ã€‚å®žéªŒè¡¨æ˜Žï¼ŒParaFormeråœ¨11ä¸ªæ•°æ®é›†ï¼ˆèŠ‚ç‚¹æ•°ä»Žæ•°åƒåˆ°æ•°ç™¾ä¸‡ï¼‰çš„èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ä¸­å‡å–å¾—ä¸€è‡´çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚è¡¥å……ææ–™ï¼ˆåŒ…æ‹¬ä»£ç å’Œé™„å½•ï¼‰å¯åœ¨https://github.com/chaohaoyuan/ParaFormeræ‰¾åˆ°ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

ParaFormerçš„æ•´ä½“æ¡†æž¶åŸºäºŽå›¾Transformerï¼Œæ ¸å¿ƒåˆ›æ–°åœ¨äºŽå¼•å…¥PageRankå¢žå¼ºçš„æ³¨æ„åŠ›æ¨¡å—ã€‚è¯¥æ¨¡å—é€šè¿‡æ•´åˆPageRankç®—æ³•æ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°è¿‡æ»¤ä¿¡æ¯ï¼Œä»Žè€Œæ¨¡æ‹Ÿæ·±åº¦Transformerçš„è¡Œä¸ºï¼Œé¿å…å…¨å±€æ³¨æ„åŠ›å›ºæœ‰çš„ä½Žé€šæ»¤æ³¢æ•ˆåº”ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œä¼ ç»Ÿå›¾Transformerçš„å…¨å±€æ³¨æ„åŠ›æ˜“å¯¼è‡´è¿‡å¹³æ»‘ï¼Œè€ŒParaFormeré€šè¿‡PageRankæœºåˆ¶å®žçŽ°è‡ªé€‚åº”é€šæ»¤æ³¢ï¼Œæœ‰æ•ˆå¹³è¡¡å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œæå‡è¡¨ç¤ºå­¦ä¹ çš„åŒºåˆ†åº¦ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

ParaFormeråœ¨11ä¸ªæ•°æ®é›†ï¼ˆèŠ‚ç‚¹æ•°ä»Žæ•°åƒåˆ°æ•°ç™¾ä¸‡ï¼‰ä¸Šå‡å–å¾—æ€§èƒ½æå‡ï¼Œåœ¨èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ä¸­è¡¨çŽ°ä¸€è‡´ä¼˜äºŽåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶ç¼“è§£è¿‡å¹³æ»‘çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽç¤¾äº¤ç½‘ç»œåˆ†æžã€æŽ¨èç³»ç»Ÿã€ç”Ÿç‰©ä¿¡æ¯å­¦å’ŒçŸ¥è¯†å›¾è°±ç­‰é¢†åŸŸï¼Œé€šè¿‡æå‡å›¾è¡¨ç¤ºå­¦ä¹ çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œæ”¯æŒèŠ‚ç‚¹åˆ†ç±»ã€å›¾åˆ†ç±»ç­‰ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„å·¥ä¸šå’Œç ”ç©¶ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

