---
layout: default
title: Understanding and Improving Hyperbolic Deep Reinforcement Learning
---

# Understanding and Improving Hyperbolic Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14202" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14202v1</a>
  <a href="https://arxiv.org/pdf/2512.14202.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14202v1" onclick="toggleFavorite(this, '2512.14202v1', 'Understanding and Improving Hyperbolic Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Timo Klein, Thomas Lang, Andrii Shkabrii, Alexander Sturm, Kevin Sidak, Lukas Miklautz, Claudia Plant, Yllka Velaj, Sebastian Tschiatschek

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Probabilistic-and-Interactive-ML/hyper-rl)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHyper++ï¼Œè§£å†³åŒæ›²æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­æ¢¯åº¦ä¸ç¨³å®šå’Œè®­ç»ƒå›°éš¾çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŒæ›²å¼ºåŒ–å­¦ä¹ ` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `åºåŠ è±çƒ` `ç‰¹å¾è¡¨ç¤º` `æ¢¯åº¦åˆ†æ` `ç­–ç•¥ä¼˜åŒ–` `ProcGen` `Atari-5`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŒæ›²ç©ºé—´èƒ½æœ‰æ•ˆæ•æ‰å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­çš„å±‚çº§å…³ç³»ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨éå¹³ç¨³ç¯å¢ƒä¸­è®­ç»ƒåŒæ›²æ·±åº¦å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æ—¶é¢ä¸´ä¼˜åŒ–æŒ‘æˆ˜ã€‚
2. è®ºæ–‡é€šè¿‡åˆ†ææ¢¯åº¦ï¼Œå‘ç°å¤§èŒƒæ•°åµŒå…¥å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œæå‡ºHyper++ï¼ŒåŒ…å«ç¨³å®šè¯„è®ºå®¶è®­ç»ƒã€ç‰¹å¾æ­£åˆ™åŒ–å’Œä¼˜åŒ–å‹å¥½çš„åŒæ›²ç½‘ç»œå±‚ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHyper++åœ¨ProcGenä¸Šä¿è¯ç¨³å®šå­¦ä¹ ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰åŒæ›²æ™ºèƒ½ä½“ï¼Œå¹¶å‡å°‘äº†30%çš„è®­ç»ƒæ—¶é—´ï¼›åœ¨Atari-5ä¸Šæ˜¾è‘—ä¼˜äºæ¬§å‡ é‡Œå¾—å’ŒåŒæ›²åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)æ™ºèƒ½ä½“çš„æ€§èƒ½ä¸¥é‡ä¾èµ–äºåº•å±‚ç‰¹å¾è¡¨ç¤ºçš„è´¨é‡ã€‚åŒæ›²ç‰¹å¾ç©ºé—´éå¸¸é€‚åˆæ­¤ç›®çš„ï¼Œå› ä¸ºå®ƒä»¬è‡ªç„¶åœ°æ•è·å¤æ‚RLç¯å¢ƒä¸­å¸¸è§çš„å±‚çº§å’Œå…³ç³»ç»“æ„ã€‚ç„¶è€Œï¼Œåˆ©ç”¨è¿™äº›ç©ºé—´é€šå¸¸é¢ä¸´ç”±äºRLçš„éå¹³ç¨³æ€§å¸¦æ¥çš„ä¼˜åŒ–æŒ‘æˆ˜ã€‚æœ¬æ–‡ç¡®å®šäº†å†³å®šåŒæ›²æ·±åº¦RLæ™ºèƒ½ä½“è®­ç»ƒæˆåŠŸä¸å¤±è´¥çš„å…³é”®å› ç´ ã€‚é€šè¿‡åˆ†æåºåŠ è±çƒå’ŒåŒæ›²é¢æ¨¡å‹ä¸­æ ¸å¿ƒæ“ä½œçš„æ¢¯åº¦ï¼Œæˆ‘ä»¬è¡¨æ˜å¤§èŒƒæ•°åµŒå…¥ä¼šç ´ååŸºäºæ¢¯åº¦çš„è®­ç»ƒï¼Œå¯¼è‡´è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)ä¸­çš„ä¿¡ä»»åŒºåŸŸè¿è§„ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŒæ›²PPOæ™ºèƒ½ä½“Hyper++ï¼Œå®ƒç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼š(i)é€šè¿‡åˆ†ç±»ä»·å€¼æŸå¤±è€Œéå›å½’å®ç°ç¨³å®šçš„è¯„è®ºå®¶è®­ç»ƒï¼›(ii)ä¿è¯æœ‰ç•ŒèŒƒæ•°çš„ç‰¹å¾æ­£åˆ™åŒ–ï¼ŒåŒæ—¶é¿å…äº†è£å‰ªå¸¦æ¥çš„ç»´åº¦ç¾éš¾ï¼›(iii)ä½¿ç”¨æ›´ä¼˜åŒ–å‹å¥½çš„åŒæ›²ç½‘ç»œå±‚å…¬å¼ã€‚åœ¨ProcGenä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHyper++ä¿è¯äº†ç¨³å®šçš„å­¦ä¹ ï¼Œä¼˜äºå…ˆå‰çš„åŒæ›²æ™ºèƒ½ä½“ï¼Œå¹¶å°†æŒ‚é’Ÿæ—¶é—´å‡å°‘äº†çº¦30%ã€‚åœ¨Atari-5ä¸Šä½¿ç”¨Double DQNï¼ŒHyper++æ˜¾è‘—ä¼˜äºæ¬§å‡ é‡Œå¾—å’ŒåŒæ›²åŸºçº¿ã€‚æˆ‘ä»¬åœ¨https://github.com/Probabilistic-and-Interactive-ML/hyper-rlå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŒæ›²æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨åŒæ›²ç©ºé—´çš„ä¼˜åŠ¿æ—¶ï¼Œç”±äºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒçš„éå¹³ç¨³æ€§ï¼Œå®¹æ˜“å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹éš¾ä»¥æ”¶æ•›ã€‚æ­¤å¤–ï¼Œç›´æ¥å°†æ¬§å‡ é‡Œå¾—ç©ºé—´çš„ä¼˜åŒ–æ–¹æ³•åº”ç”¨äºåŒæ›²ç©ºé—´ï¼Œä¼šå¿½ç•¥åŒæ›²å‡ ä½•çš„ç‰¹æ®Šæ€§è´¨ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„æ€§èƒ½è¡¨ç°ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹åŒæ›²ç©ºé—´ä¸­æ¢¯åº¦è¡Œä¸ºçš„æ·±å…¥ç†è§£ï¼Œä»¥åŠé’ˆå¯¹åŒæ›²å‡ ä½•ç‰¹æ€§ä¼˜åŒ–çš„è®­ç»ƒç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ·±å…¥åˆ†æåŒæ›²ç©ºé—´ä¸­æ¢¯åº¦çš„è¡Œä¸ºï¼Œæ‰¾å‡ºå¯¼è‡´è®­ç»ƒä¸ç¨³å®šçš„å…³é”®å› ç´ ï¼Œå¹¶é’ˆå¯¹è¿™äº›å› ç´ è®¾è®¡ç›¸åº”çš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡å‘ç°å¤§èŒƒæ•°åµŒå…¥æ˜¯å¯¼è‡´æ¢¯åº¦ä¸ç¨³å®šçš„ä¸»è¦åŸå› ï¼Œå› æ­¤æå‡ºäº†ç‰¹å¾æ­£åˆ™åŒ–æ–¹æ³•æ¥é™åˆ¶åµŒå…¥çš„èŒƒæ•°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é‡‡ç”¨äº†åˆ†ç±»ä»·å€¼æŸå¤±æ¥ç¨³å®šè¯„è®ºå®¶è®­ç»ƒï¼Œå¹¶ä½¿ç”¨æ›´ä¼˜åŒ–å‹å¥½çš„åŒæ›²ç½‘ç»œå±‚å…¬å¼ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHyper++çš„æ•´ä½“æ¡†æ¶åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)ï¼Œå¹¶é’ˆå¯¹åŒæ›²ç©ºé—´è¿›è¡Œäº†æ”¹è¿›ã€‚ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) åŒæ›²ç­–ç•¥ç½‘ç»œï¼šä½¿ç”¨åŒæ›²å‡ ä½•ä¸­çš„æ“ä½œæ¥æ„å»ºç­–ç•¥ç½‘ç»œï¼Œä»¥å­¦ä¹ åŒæ›²ç©ºé—´ä¸­çš„ç­–ç•¥ã€‚2) åŒæ›²ä»·å€¼ç½‘ç»œï¼šä½¿ç”¨åŒæ›²å‡ ä½•ä¸­çš„æ“ä½œæ¥æ„å»ºä»·å€¼ç½‘ç»œï¼Œç”¨äºè¯„ä¼°ç­–ç•¥çš„ä»·å€¼ã€‚3) åˆ†ç±»ä»·å€¼æŸå¤±ï¼šä½¿ç”¨åˆ†ç±»ä»·å€¼æŸå¤±æ¥è®­ç»ƒä»·å€¼ç½‘ç»œï¼Œä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚4) ç‰¹å¾æ­£åˆ™åŒ–ï¼šå¯¹åµŒå…¥çš„èŒƒæ•°è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥é¿å…å¤§èŒƒæ•°åµŒå…¥å¯¼è‡´çš„æ¢¯åº¦ä¸ç¨³å®šã€‚5) ä¼˜åŒ–å‹å¥½çš„åŒæ›²ç½‘ç»œå±‚ï¼šä½¿ç”¨æ›´ä¼˜åŒ–å‹å¥½çš„åŒæ›²ç½‘ç»œå±‚å…¬å¼ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¯¹åŒæ›²ç©ºé—´ä¸­æ¢¯åº¦è¡Œä¸ºçš„æ·±å…¥åˆ†æï¼Œä»¥åŠé’ˆå¯¹æ€§åœ°æå‡ºçš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡é¦–æ¬¡æ­ç¤ºäº†å¤§èŒƒæ•°åµŒå…¥æ˜¯å¯¼è‡´åŒæ›²æ·±åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸ç¨³å®šçš„å…³é”®å› ç´ ï¼Œå¹¶æå‡ºäº†ç‰¹å¾æ­£åˆ™åŒ–æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†åˆ†ç±»ä»·å€¼æŸå¤±å’Œä¼˜åŒ–å‹å¥½çš„åŒæ›²ç½‘ç»œå±‚å…¬å¼ï¼Œè¿›ä¸€æ­¥æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¿™äº›åˆ›æ–°ç‚¹ä½¿å¾—Hyper++èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åŒæ›²ç©ºé—´çš„ä¼˜åŠ¿ï¼Œå¹¶åœ¨å¤æ‚å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­å–å¾—ä¼˜å¼‚çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼š1) ç‰¹å¾æ­£åˆ™åŒ–ï¼šä½¿ç”¨L2æ­£åˆ™åŒ–æ¥é™åˆ¶åµŒå…¥çš„èŒƒæ•°ï¼Œé¿å…å¤§èŒƒæ•°åµŒå…¥å¯¼è‡´çš„æ¢¯åº¦ä¸ç¨³å®šã€‚æ­£åˆ™åŒ–ç³»æ•°éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚2) åˆ†ç±»ä»·å€¼æŸå¤±ï¼šå°†ä»·å€¼å‡½æ•°ç¦»æ•£åŒ–ä¸ºå¤šä¸ªç±»åˆ«ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±æ¥è®­ç»ƒä»·å€¼ç½‘ç»œã€‚ç±»åˆ«æ•°é‡éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚3) ä¼˜åŒ–å‹å¥½çš„åŒæ›²ç½‘ç»œå±‚ï¼šä½¿ç”¨PoincarÃ© Ballæ¨¡å‹çš„åˆ‡ç©ºé—´æ˜ å°„æ¥æ„å»ºåŒæ›²ç½‘ç»œå±‚ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚å…·ä½“å®ç°ç»†èŠ‚å¯ä»¥å‚è€ƒè®ºæ–‡ä»£ç ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Hyper++åœ¨ProcGenå’ŒAtari-5ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚åœ¨ProcGenä¸Šï¼ŒHyper++ä¿è¯äº†ç¨³å®šçš„å­¦ä¹ ï¼Œä¼˜äºå…ˆå‰çš„åŒæ›²æ™ºèƒ½ä½“ï¼Œå¹¶å°†æŒ‚é’Ÿæ—¶é—´å‡å°‘äº†çº¦30%ã€‚åœ¨Atari-5ä¸Šä½¿ç”¨Double DQNï¼ŒHyper++æ˜¾è‘—ä¼˜äºæ¬§å‡ é‡Œå¾—å’ŒåŒæ›²åŸºçº¿ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå…·æœ‰å±‚çº§å’Œå…³ç³»ç»“æ„çš„å¤æ‚å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€æ¸¸æˆAIã€æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡åˆ©ç”¨åŒæ›²ç©ºé—´çš„ä¼˜åŠ¿ï¼Œå¯ä»¥æé«˜æ™ºèƒ½ä½“çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚ç¯å¢ƒå¹¶å®Œæˆä»»åŠ¡ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢å°†Hyper++åº”ç”¨äºæ›´å¹¿æ³›çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶è¿›ä¸€æ­¥ä¼˜åŒ–å…¶æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the PoincarÃ© Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .

