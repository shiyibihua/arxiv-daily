---
layout: default
title: SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations
---

# SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations

**arXiv**: [2512.14080v1](https://arxiv.org/abs/2512.14080) | [PDF](https://arxiv.org/pdf/2512.14080.pdf)

**‰ΩúËÄÖ**: Wentao Guo, Mayank Mishra, Xinle Cheng, Ion Stoica, Tri Dao

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

<<<<<<< HEAD
**ÊèêÂá∫SonicMoE‰ª•Ëß£ÂÜ≥ÁªÜÁ≤íÂ∫¶È´òÁ®ÄÁñèMoEÊ®°Âûã‰∏≠ÁöÑÂÜÖÂ≠òÂç†Áî®È´ò„ÄÅIOÂºÄÈîÄÂ§ßÂíåËÆ°ÁÆóÊµ™Ë¥πÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **Âº∫ÂåñÂ≠¶‰π†**

**ÂÖ≥ÈîÆËØç**: `Ê∑∑Âêà‰∏ìÂÆ∂Ê®°Âûã` `ÂÜÖÂ≠ò‰ºòÂåñ` `GPUÂÜÖÊ†∏Âä†ÈÄü` `‰ª§ÁâåËàçÂÖ•` `ËÆ°ÁÆóÊïàÁéá` `Ê®°ÂûãËÆ≠ÁªÉ` `Á°¨‰ª∂ÊÑüÁü•‰ºòÂåñ` `Á®ÄÁñèËÆ°ÁÆó`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºöÁªÜÁ≤íÂ∫¶MoEÂõ†È´òIOÊàêÊú¨ÂØºËá¥ÂÜÖÂ≠òÂç†Áî®Â¢ûÂä†ÂíåÁ°¨‰ª∂ÊïàÁéáÈôç‰ΩéÔºåÁ®ÄÁñèMoEÂõ†ÂàÜÁªÑGEMMÂ°´ÂÖÖÈÄ†ÊàêËÆ°ÁÆóÊµ™Ë¥π„ÄÇ
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÊèêÂá∫ÂÜÖÂ≠òÈ´òÊïàÁÆóÊ≥ïÂáèÂ∞ëÊøÄÊ¥ªÁºìÂ≠òÔºåËÆæËÆ°IO‰∏éËÆ°ÁÆóÈáçÂè†ÁöÑGPUÂÜÖÊ†∏ÔºåÂπ∂ÂºïÂÖ•‰ª§ÁâåËàçÂÖ•ÊñπÊ≥ïÊúÄÂ∞èÂåñÂ°´ÂÖÖÊµ™Ë¥π„ÄÇ
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöÊøÄÊ¥ªÂÜÖÂ≠òÂáèÂ∞ë45%ÔºåËÆ°ÁÆóÂêûÂêêÈáèÊèêÂçá1.86ÂÄçÔºåÂú®È´òÁ®ÄÁñèËÆæÁΩÆ‰∏ãÂÜÖÊ†∏ÊâßË°åÊó∂Èó¥È¢ùÂ§ñÂä†ÈÄü1.16ÂÄç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊ®°ÂûãÂ∑≤Êàê‰∏∫Êâ©Â±ïËØ≠Ë®ÄÊ®°ÂûãËÄå‰∏çÊòæËëóÂ¢ûÂä†ËÆ°ÁÆóÊàêÊú¨ÁöÑ‰∫ãÂÆûÊû∂ÊûÑ„ÄÇÊúÄËøëÁöÑMoEÊ®°ÂûãÊòæÁ§∫Âá∫ÂêëÈ´ò‰∏ìÂÆ∂Á≤íÂ∫¶ÔºàËæÉÂ∞èÁöÑ‰∏ìÂÆ∂‰∏≠Èó¥Áª¥Â∫¶ÔºâÂíåÊõ¥È´òÁ®ÄÁñèÊÄßÔºàÊÅíÂÆöÊøÄÊ¥ª‰∏ìÂÆ∂Êï∞Èáè‰ΩÜÊÄª‰∏ìÂÆ∂Êï∞Êõ¥Â§öÔºâÁöÑÊòéÊòæË∂ãÂäøÔºåËøôÊèêÈ´ò‰∫ÜÊØèFLOPÁöÑÊ®°ÂûãË¥®Èáè„ÄÇÁÑ∂ËÄåÔºåÁªÜÁ≤íÂ∫¶MoEÁî±‰∫éÊõ¥È´òÁöÑIOÊàêÊú¨ËÄåÈù¢‰∏¥ÊøÄÊ¥ªÂÜÖÂ≠òÂç†Áî®Â¢ûÂä†ÂíåÁ°¨‰ª∂ÊïàÁéáÈôç‰ΩéÁöÑÈóÆÈ¢òÔºåËÄåÊõ¥Á®ÄÁñèÁöÑMoEÂàôÂõ†ÂàÜÁªÑGEMMÂÜÖÊ†∏‰∏≠ÁöÑÂ°´ÂÖÖËÄåÂØºËá¥ËÆ°ÁÆóÊµ™Ë¥π„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÜÖÂ≠òÈ´òÊïàÁöÑÁÆóÊ≥ïÊù•ËÆ°ÁÆóMoEÁöÑÂâçÂêëÂíåÂêéÂêë‰º†ÈÄíÔºåÂπ∂‰∏∫ÂêéÂêë‰º†ÈÄíÊúÄÂ∞èÂåñÊøÄÊ¥ªÁºìÂ≠ò„ÄÇÊàë‰ª¨ËøòËÆæËÆ°‰∫ÜGPUÂÜÖÊ†∏ÔºåÂ∞ÜÂÜÖÂ≠òIO‰∏éËÆ°ÁÆóÈáçÂè†Ôºå‰ΩøÊâÄÊúâMoEÊû∂ÊûÑÂèóÁõä„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ‚Äú‰ª§ÁâåËàçÂÖ•‚ÄùÊñπÊ≥ïÔºå‰ª•ÊúÄÂ∞èÂåñÂàÜÁªÑGEMMÂÜÖÊ†∏‰∏≠Â°´ÂÖÖÈÄ†ÊàêÁöÑËÆ°ÁÆóÊµ™Ë¥π„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïSonicMoEÂ∞ÜÊøÄÊ¥ªÂÜÖÂ≠òÂáèÂ∞ë‰∫Ü45%ÔºåÂπ∂Âú®Hopper GPU‰∏äÂÆûÁé∞‰∫Ü1.86ÂÄçÁöÑËÆ°ÁÆóÂêûÂêêÈáèÊèêÂçáÔºåÁõ∏ÊØî‰∫éScatterMoEÁöÑBF16 MoEÂÜÖÊ†∏Áî®‰∫éÁªÜÁ≤íÂ∫¶7B MoE„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåSonicMoEÂú®64‰∏™H100‰∏äÂÆûÁé∞‰∫ÜÊØèÂ§©2130‰∫ø‰ª§ÁâåÁöÑËÆ≠ÁªÉÂêûÂêêÈáèÔºå‰∏éScatterMoEÂú®96‰∏™H100‰∏ä‰ΩøÁî®lm-engine‰ª£Á†ÅÂ∫ìËøõË°å7B MoEÊ®°ÂûãËÆ≠ÁªÉÔºà‰ΩøÁî®FSDP-2ÔºâÁöÑÊØèÂ§©2250‰∫ø‰ª§ÁâåÂêûÂêêÈáèÁõ∏ÂΩì„ÄÇÂú®È´òMoEÁ®ÄÁñèÊÄßËÆæÁΩÆ‰∏ãÔºåÊàë‰ª¨ÁöÑÁì¶ÁâáÊÑüÁü•‰ª§ÁâåËàçÂÖ•ÁÆóÊ≥ïÁõ∏ÊØî‰∫éÊôÆÈÄötop-KË∑ØÁî±ÔºåÂú®ÂÜÖÊ†∏ÊâßË°åÊó∂Èó¥‰∏äÈ¢ùÂ§ñÂÆûÁé∞‰∫Ü1.16ÂÄçÁöÑÂä†ÈÄüÔºåÂêåÊó∂‰øùÊåÅÁõ∏‰ººÁöÑ‰∏ãÊ∏∏ÊÄßËÉΩ„ÄÇÊàë‰ª¨ÂºÄÊ∫ê‰∫ÜÊâÄÊúâÂÜÖÊ†∏Ôºå‰ª•ÊîØÊåÅÊõ¥Âø´ÁöÑMoEÊ®°ÂûãËÆ≠ÁªÉ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÁªÜÁ≤íÂ∫¶È´òÁ®ÄÁñèMoEÊ®°Âûã‰∏≠ÁöÑ‰∏§‰∏™Ê†∏ÂøÉÈóÆÈ¢òÔºö‰∏ÄÊòØÁªÜÁ≤íÂ∫¶MoEÂõ†ÊøÄÊ¥ªÂÜÖÂ≠òÂç†Áî®È´òÂíåIOÂºÄÈîÄÂ§ßÂØºËá¥ÁöÑÁ°¨‰ª∂ÊïàÁéá‰Ωé‰∏ãÔºõ‰∫åÊòØÁ®ÄÁñèMoEÂú®ÂàÜÁªÑGEMMÂÜÖÊ†∏‰∏≠Âõ†Â°´ÂÖÖÈÄ†ÊàêÁöÑËÆ°ÁÆóÊµ™Ë¥π„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇScatterMoEÂú®Â§ÑÁêÜËøô‰∫õÊåëÊàòÊó∂Â≠òÂú®ÂÜÖÂ≠òÁºìÂ≠ò‰∏çË∂≥ÂíåËÆ°ÁÆóÊïàÁéáÁì∂È¢à„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøá‰ºòÂåñÂÜÖÂ≠òÁÆ°ÁêÜÂíåËÆ°ÁÆóÊµÅÁ®ãÔºåËÆæËÆ°‰∏ÄÁßçÁªºÂêàÊñπÊ°àÊù•ÂáèÂ∞ëÊøÄÊ¥ªÂÜÖÂ≠ò„ÄÅÈáçÂè†IO‰∏éËÆ°ÁÆóÔºåÂπ∂ÊúÄÂ∞èÂåñÂàÜÁªÑGEMM‰∏≠ÁöÑÂ°´ÂÖÖÊµ™Ë¥πÔºå‰ªéËÄåÊèêÂçáMoEÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÂêûÂêêÈáè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂÜÖÂ≠òÈ´òÊïàÁÆóÊ≥ïÊ®°ÂùóÔºåÁî®‰∫éËÆ°ÁÆóMoEÁöÑÂâçÂêëÂíåÂêéÂêë‰º†ÈÄíÂπ∂ÊúÄÂ∞èÂåñÊøÄÊ¥ªÁºìÂ≠òÔºõGPUÂÜÖÊ†∏‰ºòÂåñÊ®°ÂùóÔºåÂÆûÁé∞ÂÜÖÂ≠òIO‰∏éËÆ°ÁÆóÁöÑÈáçÂè†Ôºõ‰ª§ÁâåËàçÂÖ•Ê®°ÂùóÔºåÈÄöËøáÁì¶ÁâáÊÑüÁü•ÊñπÊ≥ïÂáèÂ∞ëÂàÜÁªÑGEMM‰∏≠ÁöÑÂ°´ÂÖÖ„ÄÇÊµÅÁ®ã‰∏äÔºåÂÖàÂ∫îÁî®ÂÜÖÂ≠òÁÆóÊ≥ïÈôç‰ΩéÂÜÖÂ≠òÂç†Áî®ÔºåÂÜçÈÄöËøáÂÜÖÊ†∏‰ºòÂåñÂä†ÈÄüËÆ°ÁÆóÔºåÊúÄÂêé‰ΩøÁî®‰ª§ÁâåËàçÂÖ•Ëøõ‰∏ÄÊ≠•ÂáèÂ∞ëÊµ™Ë¥π„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÊòØ‚Äú‰ª§ÁâåËàçÂÖ•‚ÄùÊñπÊ≥ïÔºåÂÆÉÈíàÂØπÂàÜÁªÑGEMMÂÜÖÊ†∏‰∏≠ÁöÑÂ°´ÂÖÖÈóÆÈ¢òÔºåÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥‰ª§ÁâåÂàÜÈÖçÊù•ÊúÄÂ∞èÂåñËÆ°ÁÆóÊµ™Ë¥πÔºå‰∏éÁé∞ÊúâÊñπÊ≥ïÂ¶ÇÊôÆÈÄötop-KË∑ØÁî±Áõ∏ÊØîÔºåÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÂÖ∂Áì¶ÁâáÊÑüÁü•ÁöÑ‰ºòÂåñÁ≠ñÁï•ÔºåËÉΩÊõ¥ÊúâÊïàÂú∞Âà©Áî®Á°¨‰ª∂ËµÑÊ∫ê„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÜÖÂ≠òÈ´òÊïàÁÆóÊ≥ï‰∏≠ÔºåÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨ÊúÄÂ∞èÂåñÊøÄÊ¥ªÁºìÂ≠òÁöÑÁ≠ñÁï•Ôºå‰ª•ÂáèÂ∞ëÂêéÂêë‰º†ÈÄíÁöÑÂÜÖÂ≠òÈúÄÊ±ÇÔºõGPUÂÜÖÊ†∏ËÆæËÆ°‰∏≠ÔºåÈááÁî®ÂºÇÊ≠•IOÂíåËÆ°ÁÆóÈáçÂè†ÊäÄÊúØÔºåÊèêÂçáÂπ∂Ë°åÊÄßÔºõ‰ª§ÁâåËàçÂÖ•ÊñπÊ≥ï‰∏≠ÔºåÂèÇÊï∞ËÆæÁΩÆÊ∂âÂèäÁì¶ÁâáÂ§ßÂ∞èÂíåËàçÂÖ•ÈòàÂÄºÔºå‰ª•Âπ≥Ë°°ËÆ°ÁÆóÊïàÁéáÂíåÊ®°ÂûãÊÄßËÉΩÔºåÊçüÂ§±ÂáΩÊï∞ÊàñÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠Êú™ÊòéÁ°ÆÊåáÂÆöÔºå‰ΩÜÊï¥‰ΩìÂü∫‰∫éMoEÊû∂ÊûÑËøõË°å‰ºòÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÊúÄÈáçË¶ÅÁöÑÂÆûÈ™åÁªìÊûúÂåÖÊã¨ÔºöÂú®ÁªÜÁ≤íÂ∫¶7B MoEÊ®°Âûã‰∏äÔºåSonicMoEÁõ∏ÊØîScatterMoEÁöÑBF16 MoEÂÜÖÊ†∏ÔºåÊøÄÊ¥ªÂÜÖÂ≠òÂáèÂ∞ë45%ÔºåËÆ°ÁÆóÂêûÂêêÈáèÊèêÂçá1.86ÂÄçÔºõÂú®64‰∏™H100 GPU‰∏äÔºåËÆ≠ÁªÉÂêûÂêêÈáèËææÂà∞ÊØèÂ§©2130‰∫ø‰ª§ÁâåÔºå‰∏éScatterMoEÂú®96‰∏™H100‰∏äÁöÑ2250‰∫ø‰ª§ÁâåÁõ∏ÂΩìÔºõÂú®È´òÁ®ÄÁñèËÆæÁΩÆ‰∏ãÔºå‰ª§ÁâåËàçÂÖ•ÁÆóÊ≥ïÁõ∏ÊØîÊôÆÈÄötop-KË∑ØÁî±ÔºåÂÜÖÊ†∏ÊâßË°åÊó∂Èó¥È¢ùÂ§ñÂä†ÈÄü1.16ÂÄçÔºåÂêåÊó∂‰∏ãÊ∏∏ÊÄßËÉΩ‰øùÊåÅÁõ∏‰ºº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂‰∏ªË¶ÅÂ∫îÁî®‰∫éÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÊé®ÁêÜÂú∫ÊôØÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÈ´òÊïàÊâ©Â±ïÊ®°ÂûãËßÑÊ®°ËÄå‰∏çÊòæËëóÂ¢ûÂä†ËÆ°ÁÆóÊàêÊú¨ÁöÑÈ¢ÜÂüüÔºåÂ¶ÇËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊú∫Âô®ÁøªËØëÂíåÂØπËØùÁ≥ªÁªü„ÄÇÂÖ∂ÂÆûÈôÖ‰ª∑ÂÄºÂú®‰∫éÊòæËëóÊèêÂçáMoEÊ®°ÂûãÁöÑËÆ≠ÁªÉÈÄüÂ∫¶ÂíåËµÑÊ∫êÂà©Áî®ÁéáÔºåÈôç‰ΩéÁ°¨‰ª∂ÈúÄÊ±ÇÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êõ¥Â§çÊùÇAIÊ®°ÂûãÁöÑÈÉ®ÁΩ≤ÂíåÂïÜ‰∏öÂåñÂ∫îÁî®„ÄÇ
=======
**ÊèêÂá∫SonicMoE‰ª•Ëß£ÂÜ≥ÁªÜÁ≤íÂ∫¶MoEÊ®°Âûã‰∏≠ÁöÑÂÜÖÂ≠òÊïàÁéá‰Ωé‰∏ãÂíåËÆ°ÁÆóÊµ™Ë¥πÈóÆÈ¢òÔºåÈÄöËøáIO‰∏éÁì¶ÁâáÊÑüÁü•‰ºòÂåñÂä†ÈÄüËÆ≠ÁªÉ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **Âº∫ÂåñÂ≠¶‰π†**

**ÂÖ≥ÈîÆËØç**: `Ê∑∑Âêà‰∏ìÂÆ∂Ê®°Âûã` `ÂÜÖÂ≠ò‰ºòÂåñ` `GPUÂä†ÈÄü` `IOÈáçÂè†` `‰ª§ÁâåËàçÂÖ•` `ËÆ≠ÁªÉÊïàÁéá` `Á®ÄÁñèËÆ°ÁÆó` `Á°¨‰ª∂ÊÑüÁü•‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºöÁªÜÁ≤íÂ∫¶MoEÊ®°ÂûãÂõ†È´òIOÊàêÊú¨ÂØºËá¥ÊøÄÊ¥ªÂÜÖÂ≠òÂç†Áî®Â¢ûÂä†ÂíåÁ°¨‰ª∂ÊïàÁéáÈôç‰ΩéÔºåÁ®ÄÁñèMoEÂàôÂõ†ÂàÜÁªÑGEMMÂÜÖÊ†∏Â°´ÂÖÖÈÄ†ÊàêËÆ°ÁÆóÊµ™Ë¥π„ÄÇ
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÊèêÂá∫ÂÜÖÂ≠òÈ´òÊïàÁÆóÊ≥ïÂáèÂ∞ëÊøÄÊ¥ªÁºìÂ≠òÔºåËÆæËÆ°GPUÂÜÖÊ†∏ÂÆûÁé∞IO‰∏éËÆ°ÁÆóÈáçÂè†ÔºåÂπ∂ÂºïÂÖ•‰ª§ÁâåËàçÂÖ•ÊñπÊ≥ï‰ºòÂåñÂ°´ÂÖÖËÆ°ÁÆó„ÄÇ
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöÂú®7B MoE‰∏äÔºåSonicMoEÂáèÂ∞ë45%ÊøÄÊ¥ªÂÜÖÂ≠òÔºåÊèêÂçá1.86ÂÄçËÆ°ÁÆóÂêûÂêêÈáèÔºåÂπ∂Âú®È´òÁ®ÄÁñèËÆæÁΩÆ‰∏ãÂÆûÁé∞È¢ùÂ§ñ1.16ÂÄçÂä†ÈÄü„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊ®°ÂûãÂ∑≤Êàê‰∏∫Êâ©Â±ïËØ≠Ë®ÄÊ®°ÂûãËÄå‰∏çÊòæËëóÂ¢ûÂä†ËÆ°ÁÆóÊàêÊú¨ÁöÑ‰∫ãÂÆûÊ†áÂáÜÊû∂ÊûÑ„ÄÇÊúÄËøëÁöÑMoEÊ®°ÂûãÊòæÁ§∫Âá∫È´ò‰∏ìÂÆ∂Á≤íÂ∫¶ÔºàËæÉÂ∞èÁöÑ‰∏ìÂÆ∂‰∏≠Èó¥Áª¥Â∫¶ÔºâÂíåÊõ¥È´òÁ®ÄÁñèÊÄßÔºàÊøÄÊ¥ª‰∏ìÂÆ∂Êï∞ÈáèÊÅíÂÆö‰ΩÜÊÄª‰∏ìÂÆ∂Êï∞Â¢ûÂä†ÔºâÁöÑÊòéÊòæË∂ãÂäøÔºåËøôÊèêÈ´ò‰∫ÜÊØèFLOPÁöÑÊ®°ÂûãË¥®Èáè„ÄÇÁÑ∂ËÄåÔºåÁªÜÁ≤íÂ∫¶MoEÁî±‰∫éÊõ¥È´òÁöÑIOÊàêÊú¨ËÄåÈù¢‰∏¥ÊøÄÊ¥ªÂÜÖÂ≠òÂç†Áî®Â¢ûÂä†ÂíåÁ°¨‰ª∂ÊïàÁéáÈôç‰ΩéÁöÑÈóÆÈ¢òÔºåËÄåÊõ¥Á®ÄÁñèÁöÑMoEÂàôÂõ†ÂàÜÁªÑGEMMÂÜÖÊ†∏‰∏≠ÁöÑÂ°´ÂÖÖËÄåÂØºËá¥ËÆ°ÁÆóÊµ™Ë¥π„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÜÖÂ≠òÈ´òÊïàÁöÑÁÆóÊ≥ïÊù•ËÆ°ÁÆóMoEÁöÑÂâçÂêëÂíåÂêéÂêë‰º†ÈÄíÔºåÊúÄÂ∞èÂåñÂêéÂêë‰º†ÈÄíÁöÑÊøÄÊ¥ªÁºìÂ≠ò„ÄÇÊàë‰ª¨ËøòËÆæËÆ°‰∫ÜGPUÂÜÖÊ†∏ÔºåÂ∞ÜÂÜÖÂ≠òIO‰∏éËÆ°ÁÆóÈáçÂè†Ôºå‰ΩøÊâÄÊúâMoEÊû∂ÊûÑÂèóÁõä„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ‚Äú‰ª§ÁâåËàçÂÖ•‚ÄùÊñπÊ≥ïÔºåÊúÄÂ∞èÂåñÂàÜÁªÑGEMMÂÜÖÊ†∏‰∏≠Â°´ÂÖÖÈÄ†ÊàêÁöÑËÆ°ÁÆóÊµ™Ë¥π„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïSonicMoEÂú®ÁªÜÁ≤íÂ∫¶7B MoE‰∏äÔºåÁõ∏ÊØîScatterMoEÁöÑBF16 MoEÂÜÖÊ†∏ÔºåÂáèÂ∞ë‰∫Ü45%ÁöÑÊøÄÊ¥ªÂÜÖÂ≠òÔºåÂπ∂Âú®Hopper GPU‰∏äÂÆûÁé∞‰∫Ü1.86ÂÄçÁöÑËÆ°ÁÆóÂêûÂêêÈáèÊèêÂçá„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂú®64‰∏™H100‰∏äÔºåSonicMoEÂÆûÁé∞‰∫ÜÊØèÂ§©2130‰∫ø‰ª§ÁâåÁöÑËÆ≠ÁªÉÂêûÂêêÈáèÔºå‰∏éScatterMoEÂú®96‰∏™H100‰∏ä‰ΩøÁî®lm-engine‰ª£Á†ÅÂ∫ìËøõË°å7B MoEÊ®°ÂûãËÆ≠ÁªÉÔºàÈááÁî®FSDP-2ÔºâÁöÑÊØèÂ§©2250‰∫ø‰ª§ÁâåÂêûÂêêÈáèÁõ∏ÂΩì„ÄÇÂú®È´òMoEÁ®ÄÁñèÊÄßËÆæÁΩÆ‰∏ãÔºåÊàë‰ª¨ÁöÑÁì¶ÁâáÊÑüÁü•‰ª§ÁâåËàçÂÖ•ÁÆóÊ≥ïÁõ∏ÊØîÊôÆÈÄötop-KË∑ØÁî±ÔºåÂú®‰øùÊåÅÁ±ª‰ºº‰∏ãÊ∏∏ÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫ÜÈ¢ùÂ§ñ1.16ÂÄçÁöÑÂÜÖÊ†∏ÊâßË°åÊó∂Èó¥Âä†ÈÄü„ÄÇÊàë‰ª¨ÂºÄÊ∫ê‰∫ÜÊâÄÊúâÂÜÖÊ†∏‰ª•Âä†ÈÄüMoEÊ®°ÂûãËÆ≠ÁªÉ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

SonicMoEÁöÑÊï¥‰ΩìÊ°ÜÊû∂Âü∫‰∫éMoEÊ®°ÂûãÔºåÈÄöËøáÁÆóÊ≥ïÂíåÂÜÖÊ†∏‰ºòÂåñÊèêÂçáËÆ≠ÁªÉÊïàÁéá„ÄÇÂÖ≥ÈîÆÊäÄÊúØÂàõÊñ∞ÂåÖÊã¨ÔºöÂÜÖÂ≠òÈ´òÊïàÁÆóÊ≥ïÊúÄÂ∞èÂåñÂêéÂêë‰º†ÈÄíÁöÑÊøÄÊ¥ªÁºìÂ≠òÔºåÂáèÂ∞ëÂÜÖÂ≠òÂç†Áî®ÔºõGPUÂÜÖÊ†∏ËÆæËÆ°ÂÆûÁé∞ÂÜÖÂ≠òIO‰∏éËÆ°ÁÆóÁöÑÈáçÂè†ÔºåÊèêÈ´òÁ°¨‰ª∂Âà©Áî®ÁéáÔºõÁì¶ÁâáÊÑüÁü•‰ª§ÁâåËàçÂÖ•ÊñπÊ≥ïÂä®ÊÄÅË∞ÉÊï¥‰ª§ÁâåÂàÜÈÖçÔºåÊúÄÂ∞èÂåñÂàÜÁªÑGEMMÂÜÖÊ†∏‰∏≠ÁöÑÂ°´ÂÖÖÊµ™Ë¥π„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÂ¶ÇScatterMoEÁõ∏ÊØîÔºåSonicMoEÊõ¥Ê≥®ÈáçIOÂíåÁì¶ÁâáÁ∫ß‰ºòÂåñÔºåÁõ¥Êé•ÈíàÂØπÁªÜÁ≤íÂ∫¶ÂíåÁ®ÄÁñèMoEÁöÑÁâπÂÆöÁì∂È¢àÔºåËÄåÈùû‰ªÖ‰æùËµñÈÄöÁî®Âä†ÈÄüÊäÄÊúØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Hopper GPU‰∏äÔºåSonicMoEÁõ∏ÊØîScatterMoEÁöÑBF16 MoEÂÜÖÊ†∏ÔºåÂØπÁªÜÁ≤íÂ∫¶7B MoEÂÆûÁé∞45%ÊøÄÊ¥ªÂÜÖÂ≠òÂáèÂ∞ëÂíå1.86ÂÄçËÆ°ÁÆóÂêûÂêêÈáèÊèêÂçáÔºõÂú®È´òÁ®ÄÁñèËÆæÁΩÆ‰∏ãÔºå‰ª§ÁâåËàçÂÖ•ÁÆóÊ≥ïÂ∏¶Êù•È¢ùÂ§ñ1.16ÂÄçÂÜÖÊ†∏ÊâßË°åÂä†ÈÄüÔºåÂêåÊó∂‰øùÊåÅ‰∏ãÊ∏∏ÊÄßËÉΩ‰∏çÂèò„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂‰∏ªË¶ÅÂ∫îÁî®‰∫éÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÂä†ÈÄüÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÈ´ò‰∏ìÂÆ∂Á≤íÂ∫¶ÂíåÁ®ÄÁñèÊÄßÁöÑMoEÊû∂ÊûÑ‰∏≠ÔºåÂ¶ÇGPT-4Á≠âÂâçÊ≤øÊ®°Âûã„ÄÇÊΩúÂú®‰ª∑ÂÄºÂåÖÊã¨Èôç‰ΩéËÆ≠ÁªÉÊàêÊú¨„ÄÅÊèêÈ´òÁ°¨‰ª∂ÊïàÁéáÔºåÂπ∂ÊîØÊåÅÊõ¥È´òÊïàÁöÑÊ®°ÂûãÊâ©Â±ïÔºåÈÄÇÁî®‰∫é‰∫ëËÆ°ÁÆó„ÄÅAIÁ†îÁ©∂ÂíåÂ∑•‰∏öÁ∫ßAIÁ≥ªÁªüÂºÄÂèë„ÄÇ
>>>>>>> 1c05e1c356e1f28c2e5e6e14cf6811c0d5120ab7

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

