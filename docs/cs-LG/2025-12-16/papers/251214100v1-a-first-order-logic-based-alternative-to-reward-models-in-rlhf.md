---
layout: default
title: A First-Order Logic-Based Alternative to Reward Models in RLHF
---

# A First-Order Logic-Based Alternative to Reward Models in RLHF

**arXiv**: [2512.14100v1](https://arxiv.org/abs/2512.14100) | [PDF](https://arxiv.org/pdf/2512.14100.pdf)

**ä½œè€…**: Chunjin Jian, Xinhua Zhu

**åˆ†ç±»**: cs.LG, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ChunjinJiang/sgrpo)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶S-GRPOï¼Œæ›¿ä»£ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ä»¥æå‡RLHFçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

<<<<<<< HEAD
**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆ` `é€»è¾‘ç›¸ä¼¼æ€§å¥–åŠ±` `æ¨¡å‹å¯¹é½` `S-GRPOæ¡†æ¶` `ç›‘ç£å¾®è°ƒ` `åå¥½å­¦ä¹ ` `å½¢å¼é€»è¾‘ä¸€è‡´æ€§` `KLæ•£åº¦æ­£åˆ™åŒ–`
=======
**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆ` `é€»è¾‘ç›¸ä¼¼æ€§å¥–åŠ±` `æ¨¡å‹å¯¹é½` `S-GRPOæ¡†æ¶` `ç›‘ç£å¾®è°ƒ` `åå¥½å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `é²æ£’æ€§ä¼˜åŒ–`
>>>>>>> 1c05e1c356e1f28c2e5e6e14cf6811c0d5120ab7

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLHFæ–¹æ³•ä¾èµ–å¥–åŠ±æ¨¡å‹ï¼Œå…¶è´¨é‡å’Œç¨³å®šæ€§ç›´æ¥å½±å“å¯¹é½æ€§èƒ½ï¼Œå­˜åœ¨ä¸ç¨³å®šå’Œå¯å‘å¼ä¼°è®¡çš„ä¸è¶³ã€‚
<<<<<<< HEAD
2. æå‡ºåŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§æ›¿ä»£ä¼ ç»Ÿå¥–åŠ±å»ºæ¨¡ï¼Œå¹¶å¼•å…¥S-GRPOæ¡†æ¶é˜²æ­¢æ¨¡å‹å´©æºƒã€‚
3. å®éªŒæ˜¾ç¤ºS-GRPOåœ¨æ€§èƒ½å’Œé²æ£’æ€§ä¸Šä¼˜äºæ ‡å‡†SFTï¼Œå¹¶æ‰©å±•äº†GRPOå’ŒDPOç­‰åå¥½å­¦ä¹ æ¡†æ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½å¯¹é½æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œè®­ç»ƒå‡ºçš„å¥–åŠ±æ¨¡å‹çš„è´¨é‡å’Œç¨³å®šæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå†³å®šäº†æœ€ç»ˆçš„å¯¹é½æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•å¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ä¸¥é‡ä¾èµ–å¥–åŠ±æ¨¡å‹æ¥å¼•å¯¼LLMsæœå‘äººç±»å¯¹é½çš„è¡Œä¸ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œä½œä¸ºä¼ ç»Ÿå¥–åŠ±å»ºæ¨¡çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–å¯å‘å¼å¥–åŠ±ä¼°è®¡ï¼Œè€Œæ˜¯åˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§æ¥å¼•å¯¼æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚ç”±äºç°å®ä¸–ç•Œçš„é—®é¢˜å¯ä»¥ä»å¤šä¸ªè§’åº¦è§£é‡Šï¼Œä¸ºäº†ç¡®ä¿åŸºäºé€»è¾‘çš„å¼ºåŒ–å­¦ä¹ ä¸ä¼šå¯¼è‡´æ¨¡å‹å´©æºƒï¼Œæˆ‘ä»¬å¼•å…¥äº†S-GRPOï¼Œè¿™æ˜¯GRPOæ¡†æ¶çš„ä¸€ä¸ªç›‘ç£å˜ä½“ã€‚S-GRPOåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆäº†ä¸€ä¸ªé¢å¤–çš„ç›‘ç£ç»„ä»¶ï¼Œå¹¶è”åˆä¼˜åŒ–ç”Ÿæˆé¡¹ã€KLæ•£åº¦æ­£åˆ™åŒ–å’ŒåŸºäºæ ‡ç­¾çš„ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS-GRPOåœ¨æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢å‡æŒç»­ä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒæ‰©å±•äº†ç°æœ‰çš„åå¥½å­¦ä¹ æ¡†æ¶ï¼Œå¦‚GRPOå’ŒDPOï¼Œä¸ºå¯¹é½è®­ç»ƒæä¾›äº†æ›´çµæ´»å’Œä»»åŠ¡è‡ªé€‚åº”çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨https://github.com/ChunjinJiang/sgrpoè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³RLHFä¸­ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ä¾èµ–å¯å‘å¼ä¼°è®¡ã€è´¨é‡ä¸ç¨³å®šå¯¼è‡´å¯¹é½æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¦‚PPOè¿‡åº¦ä¾èµ–å¥–åŠ±æ¨¡å‹ï¼Œæ˜“å—å™ªå£°å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯ç”¨åŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶æ›¿ä»£ä¼ ç»Ÿå¥–åŠ±å»ºæ¨¡ï¼Œé€šè¿‡å½¢å¼é€»è¾‘ä¸€è‡´æ€§æ¥å¼•å¯¼æ¨¡å‹å¯¹é½äººç±»åå¥½ï¼Œé¿å…å¯å‘å¼åå·®ï¼Œå¹¶å¼•å…¥ç›‘ç£ç»„ä»¶S-GRPOä»¥é˜²æ­¢æ¨¡å‹å´©æºƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŸºäºGRPOæ‰©å±•ï¼ŒåŒ…æ‹¬é€»è¾‘ç›¸ä¼¼æ€§è®¡ç®—æ¨¡å—ã€ç›‘ç£å¾®è°ƒæ¨¡å—å’Œè”åˆä¼˜åŒ–é˜¶æ®µã€‚é¦–å…ˆï¼Œä»äººç±»åé¦ˆä¸­æå–é€»è¾‘ç»“æ„ï¼Œè®¡ç®—æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½çš„é€»è¾‘ä¸€è‡´æ€§ï¼›ç„¶åï¼Œåœ¨S-GRPOä¸­æ•´åˆç›‘ç£ä¿¡å·ï¼Œé€šè¿‡å¤šç›®æ ‡ä¼˜åŒ–è®­ç»ƒæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°æ˜¯æå‡ºé€»è¾‘ç›¸ä¼¼æ€§ä½œä¸ºå¥–åŠ±æ›¿ä»£æœºåˆ¶ï¼Œæœ¬è´¨åŒºåˆ«åœ¨äºä»ä¾èµ–æ•°æ®é©±åŠ¨çš„å¥–åŠ±ä¼°è®¡è½¬å‘åŸºäºå½¢å¼é€»è¾‘çš„å®¢è§‚ä¸€è‡´æ€§è¯„ä¼°ï¼Œå‡å°‘äº†ä¸»è§‚åå·®ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬S-GRPOçš„æŸå¤±å‡½æ•°ï¼Œè”åˆä¼˜åŒ–ç”Ÿæˆé¡¹ï¼ˆå¦‚äº¤å‰ç†µï¼‰ã€KLæ•£åº¦æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰å’ŒåŸºäºæ ‡ç­¾çš„ç›‘ç£ç›®æ ‡ï¼›å‚æ•°è®¾ç½®å¯èƒ½æ¶‰åŠé€»è¾‘æƒé‡å’Œæ­£åˆ™åŒ–ç³»æ•°ï¼Œå…·ä½“ç»†èŠ‚éœ€å‚è€ƒè®ºæ–‡ä»£ç ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒS-GRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå…·ä½“æ€§èƒ½æå‡æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†åœ¨é²æ£’æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚å¯¹æ¯”åŸºçº¿åŒ…æ‹¬SFTã€GRPOå’ŒDPOï¼ŒS-GRPOå±•ç°å‡ºæ›´çµæ´»çš„ä»»åŠ¡é€‚åº”æ€§ï¼Œä»£ç å·²å¼€æºä¾›éªŒè¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½è®­ç»ƒï¼Œå¦‚èŠå¤©æœºå™¨äººã€å†…å®¹ç”Ÿæˆå’Œå†³ç­–ç³»ç»Ÿï¼Œæå‡æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚çš„ä¸€è‡´æ€§ã€‚æ½œåœ¨ä»·å€¼åŒ…æ‹¬æé«˜AIç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨é€»è¾‘é©±åŠ¨AIçš„å‘å±•ï¼Œæ‰©å±•è‡³å¤šæ¨¡æ€å’Œå¤æ‚ä»»åŠ¡å¯¹é½ã€‚
=======
2. æå‡ºåŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§æ›¿ä»£ä¼ ç»Ÿå¥–åŠ±å»ºæ¨¡ï¼Œå¹¶å¼•å…¥S-GRPOæ¡†æ¶ä»¥é˜²æ­¢æ¨¡å‹å´©æºƒã€‚
3. å®éªŒæ˜¾ç¤ºS-GRPOåœ¨æ€§èƒ½å’Œé²æ£’æ€§ä¸Šä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼Œå¹¶æ‰©å±•äº†GRPOå’ŒDPOç­‰åå¥½å­¦ä¹ æ¡†æ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½å¯¹é½æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œè®­ç»ƒå‡ºçš„å¥–åŠ±æ¨¡å‹çš„è´¨é‡å’Œç¨³å®šæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå†³å®šäº†æœ€ç»ˆçš„å¯¹é½æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•å¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ä¸¥é‡ä¾èµ–å¥–åŠ±æ¨¡å‹æ¥å¼•å¯¼LLMsæœå‘äººç±»å¯¹é½çš„è¡Œä¸ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œä½œä¸ºä¼ ç»Ÿå¥–åŠ±å»ºæ¨¡çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–å¯å‘å¼å¥–åŠ±ä¼°è®¡ï¼Œè€Œæ˜¯åˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§æ¥å¼•å¯¼æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚ç”±äºç°å®ä¸–ç•Œçš„é—®é¢˜å¯ä»¥ä»å¤šä¸ªè§’åº¦è§£é‡Šï¼Œä¸ºç¡®ä¿åŸºäºé€»è¾‘çš„å¼ºåŒ–å­¦ä¹ ä¸ä¼šå¯¼è‡´æ¨¡å‹å´©æºƒï¼Œæˆ‘ä»¬å¼•å…¥äº†S-GRPOï¼Œè¿™æ˜¯GRPOæ¡†æ¶çš„ä¸€ä¸ªç›‘ç£å˜ä½“ã€‚S-GRPOåœ¨è®­ç»ƒä¸­ç»“åˆäº†é¢å¤–çš„ç›‘ç£ç»„ä»¶ï¼Œå¹¶è”åˆä¼˜åŒ–ç”Ÿæˆé¡¹ã€KLæ•£åº¦æ­£åˆ™åŒ–å’ŒåŸºäºæ ‡ç­¾çš„ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS-GRPOåœ¨æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢å‡æŒç»­ä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒæ‰©å±•äº†ç°æœ‰çš„åå¥½å­¦ä¹ æ¡†æ¶å¦‚GRPOå’ŒDPOï¼Œä¸ºå¯¹é½è®­ç»ƒæä¾›äº†æ›´çµæ´»å’Œä»»åŠ¡è‡ªé€‚åº”çš„é€”å¾„ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨https://github.com/ChunjinJiang/sgrpoè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡æå‡ºS-GRPOæ¡†æ¶ï¼Œæ•´ä½“åŸºäºGRPOæ¡†æ¶è¿›è¡Œæ‰©å±•ï¼Œæ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥åŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œæ›¿ä»£ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§æ¥è¯„ä¼°æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ï¼Œé¿å…äº†å¯å‘å¼å¥–åŠ±ä¼°è®¡çš„ä¸ç¨³å®šæ€§ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ï¼šåœ¨è®­ç»ƒä¸­è”åˆä¼˜åŒ–ç”Ÿæˆé¡¹ã€KLæ•£åº¦æ­£åˆ™åŒ–å’ŒåŸºäºæ ‡ç­¾çš„ç›‘ç£ç›®æ ‡ï¼Œé€šè¿‡é¢å¤–ç›‘ç£ç»„ä»¶å¢å¼ºç¨³å®šæ€§ã€‚ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œå®ƒä¸ä¾èµ–æ˜¾å¼å¥–åŠ±æ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡é€»è¾‘ä¸€è‡´æ€§ç›´æ¥å¼•å¯¼å¯¹é½ï¼Œå‡å°‘äº†æ¨¡å‹å´©æºƒé£é™©ï¼Œæä¾›äº†æ›´çµæ´»çš„ä»»åŠ¡è‡ªé€‚åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

S-GRPOåœ¨å®éªŒä¸­æŒç»­ä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œåœ¨æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚å®ƒæœ‰æ•ˆæ‰©å±•äº†GRPOå’ŒDPOç­‰åå¥½å­¦ä¹ æ¡†æ¶ï¼Œå±•ç¤ºäº†æ›´çµæ´»çš„å¯¹é½è®­ç»ƒèƒ½åŠ›ï¼Œä»£ç å·²å¼€æºä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½è®­ç»ƒï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç¨³å®šæ€§å’Œé²æ£’æ€§çš„åœºæ™¯ï¼Œå¦‚AIåŠ©æ‰‹ã€å†…å®¹ç”Ÿæˆå’Œå¯¹è¯ç³»ç»Ÿã€‚å…¶é€»è¾‘ä¸€è‡´æ€§æœºåˆ¶æœ‰åŠ©äºæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‡å°‘åå·®ï¼Œé€‚ç”¨äºæ•™è‚²å’Œå®¢æœç­‰é¢†åŸŸã€‚
>>>>>>> 1c05e1c356e1f28c2e5e6e14cf6811c0d5120ab7

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
>   In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
>   Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

