---
layout: default
title: A First-Order Logic-Based Alternative to Reward Models in RLHF
---

# A First-Order Logic-Based Alternative to Reward Models in RLHF

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14100" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14100v1</a>
  <a href="https://arxiv.org/pdf/2512.14100.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14100v1" onclick="toggleFavorite(this, '2512.14100v1', 'A First-Order Logic-Based Alternative to Reward Models in RLHF')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chunjin Jian, Xinhua Zhu

**åˆ†ç±»**: cs.LG, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ChunjinJiang/sgrpo)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé€»è¾‘ç›¸ä¼¼åº¦çš„å¥–åŠ±æœºåˆ¶S-GRPOï¼Œæå‡RLHFä¸­LLMå¯¹é½æ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `å¤§è¯­è¨€æ¨¡å‹` `é€»è¾‘æ¨ç†` `æ¨¡å‹å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLHFæ–¹æ³•ä¾èµ–å¥–åŠ±æ¨¡å‹å¼•å¯¼LLMå¯¹é½ï¼Œä½†å¥–åŠ±æ¨¡å‹çš„è´¨é‡å’Œç¨³å®šæ€§æ˜¯å…³é”®ç“¶é¢ˆã€‚
2. æå‡ºåŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§å¼•å¯¼æ¨¡å‹å¯¹é½äººç±»åå¥½ï¼Œé¿å…å¯å‘å¼å¥–åŠ±ä¼°è®¡ã€‚
3. å¼•å…¥S-GRPOï¼Œä¸€ç§GRPOçš„ç›‘ç£å˜ä½“ï¼Œé€šè¿‡è”åˆä¼˜åŒ–ç”Ÿæˆé¡¹ã€KLæ•£åº¦å’Œæ ‡ç­¾ç›®æ ‡ï¼Œæå‡æ€§èƒ½å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œä½œä¸ºå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºå¯å‘å¼å¥–åŠ±ä¼°è®¡ï¼Œè€Œæ˜¯åˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§æ¥å¼•å¯¼æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚è€ƒè™‘åˆ°ç°å®ä¸–ç•Œçš„é—®é¢˜å¯ä»¥ä»å¤šä¸ªè§’åº¦è§£é‡Šï¼Œä¸ºäº†é˜²æ­¢åŸºäºé€»è¾‘çš„å¼ºåŒ–å­¦ä¹ å¯¼è‡´æ¨¡å‹å´©æºƒï¼Œæœ¬æ–‡å¼•å…¥äº†S-GRPOï¼Œä¸€ç§GRPOæ¡†æ¶çš„ç›‘ç£å˜ä½“ã€‚S-GRPOåŒ…å«ä¸€ä¸ªé¢å¤–çš„ç›‘ç£ç»„ä»¶ï¼Œå¹¶åœ¨è®­ç»ƒæœŸé—´è”åˆä¼˜åŒ–ç”Ÿæˆé¡¹ã€KLæ•£åº¦æ­£åˆ™åŒ–å’ŒåŸºäºæ ‡ç­¾çš„ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS-GRPOåœ¨æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢å§‹ç»ˆä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¹¶ä¸”æ‰©å±•äº†ç°æœ‰çš„åå¥½å­¦ä¹ æ¡†æ¶ï¼Œå¦‚GRPOå’ŒDPOï¼Œä¸ºå¯¹é½è®­ç»ƒæä¾›äº†ä¸€ç§æ›´çµæ´»å’Œä»»åŠ¡è‡ªé€‚åº”çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰RLHFæ–¹æ³•ä¸¥é‡ä¾èµ–å¥–åŠ±æ¨¡å‹ï¼Œè€Œå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒå’Œæ³›åŒ–èƒ½åŠ›ç›´æ¥å½±å“æœ€ç»ˆçš„å¯¹é½æ•ˆæœã€‚ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹é€šå¸¸åŸºäºå¯å‘å¼è§„åˆ™æˆ–äººå·¥æ ‡æ³¨ï¼Œéš¾ä»¥å‡†ç¡®æ•æ‰å¤æ‚çš„äººç±»åå¥½ï¼Œå®¹æ˜“å‡ºç°å¥–åŠ±åå·®ï¼Œå¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°éæœŸæœ›çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒä¸ç¨³å®šä¹Ÿå¯èƒ½å¯¼è‡´ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å´©æºƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å½¢å¼é€»è¾‘çš„ä¸€è‡´æ€§æ¥æ›¿ä»£ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ã€‚é€šè¿‡å°†äººç±»åå¥½è½¬åŒ–ä¸ºé€»è¾‘è§„åˆ™ï¼Œå¹¶è¡¡é‡æ¨¡å‹ç”Ÿæˆç»“æœä¸è¿™äº›è§„åˆ™çš„é€»è¾‘ç›¸ä¼¼åº¦ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹å­¦ä¹ ç¬¦åˆäººç±»ä»·å€¼è§‚çš„è¡Œä¸ºã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹å¥–åŠ±æ¨¡å‹çš„ä¾èµ–ï¼Œé™ä½äº†å¥–åŠ±åå·®çš„é£é™©ï¼Œå¹¶æé«˜äº†å¯¹é½çš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šS-GRPOæ¡†æ¶åœ¨GRPOçš„åŸºç¡€ä¸Šå¼•å…¥äº†ç›‘ç£å­¦ä¹ ç»„ä»¶ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨LLMç”Ÿæˆå€™é€‰å›å¤ï¼›2ï¼‰è®¡ç®—å€™é€‰å›å¤ä¸é¢„å®šä¹‰çš„é€»è¾‘è§„åˆ™ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œä½œä¸ºé€»è¾‘å¥–åŠ±ï¼›3ï¼‰ä½¿ç”¨ç›‘ç£å­¦ä¹ ç›®æ ‡ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸äººç±»æ ‡æ³¨ä¸€è‡´çš„å›å¤ï¼›4ï¼‰è”åˆä¼˜åŒ–ç”Ÿæˆé¡¹ã€KLæ•£åº¦æ­£åˆ™åŒ–é¡¹å’Œç›‘ç£å­¦ä¹ ç›®æ ‡ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨é€»è¾‘ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•å°†äººç±»åå¥½å½¢å¼åŒ–ä¸ºé€»è¾‘è§„åˆ™ï¼Œé¿å…äº†å¯å‘å¼å¥–åŠ±ä¼°è®¡çš„ä¸»è§‚æ€§å’Œä¸ç¡®å®šæ€§ã€‚æ­¤å¤–ï¼ŒS-GRPOé€šè¿‡å¼•å…¥ç›‘ç£å­¦ä¹ ç»„ä»¶ï¼Œè§£å†³äº†é€»è¾‘å¥–åŠ±å¯èƒ½å¯¼è‡´çš„æ¨¡å‹å´©æºƒé—®é¢˜ï¼Œæé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šS-GRPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰é€»è¾‘è§„åˆ™çš„å®šä¹‰ï¼šéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œäººç±»åå¥½ï¼Œè®¾è®¡åˆé€‚çš„é€»è¾‘è§„åˆ™ï¼›2ï¼‰é€»è¾‘ç›¸ä¼¼åº¦è®¡ç®—ï¼šé€‰æ‹©åˆé€‚çš„é€»è¾‘ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼Œä¾‹å¦‚åŸºäºå‘½é¢˜é€»è¾‘æˆ–è°“è¯é€»è¾‘çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼›3ï¼‰ç›‘ç£å­¦ä¹ ç›®æ ‡ï¼šå¯ä»¥ä½¿ç”¨äº¤å‰ç†µæŸå¤±æˆ–hinge lossç­‰å¸¸è§çš„åˆ†ç±»æˆ–æ’åºæŸå¤±å‡½æ•°ï¼›4ï¼‰è¶…å‚æ•°è®¾ç½®ï¼šéœ€è¦ä»”ç»†è°ƒæ•´ç”Ÿæˆé¡¹ã€KLæ•£åº¦æ­£åˆ™åŒ–é¡¹å’Œç›‘ç£å­¦ä¹ ç›®æ ‡çš„æƒé‡ï¼Œä»¥å¹³è¡¡ç”Ÿæˆè´¨é‡ã€æ¨¡å‹ç¨³å®šæ€§å’Œå¯¹é½æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒS-GRPOåœ¨æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢å§‹ç»ˆä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚S-GRPOæ‰©å±•äº†ç°æœ‰çš„åå¥½å­¦ä¹ æ¡†æ¶ï¼Œå¦‚GRPOå’ŒDPOï¼Œä¸ºå¯¹é½è®­ç»ƒæä¾›äº†ä¸€ç§æ›´çµæ´»å’Œä»»åŠ¡è‡ªé€‚åº”çš„æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡ä½¿ç”¨é€»è¾‘ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„å®‰å…¨æ€§ã€å¯é æ€§å’Œå¯æ§æ€§ï¼Œå‡å°‘æ¨¡å‹äº§ç”Ÿæœ‰å®³æˆ–ä¸å½“å†…å®¹çš„é£é™©ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºä¸ªæ€§åŒ–æ¨èã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
>   In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
>   Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

