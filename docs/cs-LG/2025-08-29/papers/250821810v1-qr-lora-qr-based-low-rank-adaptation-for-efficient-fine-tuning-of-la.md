---
layout: default
title: QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models
---

# QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21810" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21810v1</a>
  <a href="https://arxiv.org/pdf/2508.21810.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21810v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21810v1', 'QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jessica Liang, Anirudh Bharadwaj

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQR-LoRAä»¥é«˜æ•ˆå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©é€‚åº”` `å¾®è°ƒæŠ€æœ¯` `å¤§å‹è¯­è¨€æ¨¡å‹` `QRåˆ†è§£` `å‚æ•°æ•ˆç‡` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¾®è°ƒæ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå‚æ•°æ•°é‡åºå¤§ä¸”è®¡ç®—å¼€é”€é«˜ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆçš„é€‚åº”ã€‚
2. æœ¬æ–‡æå‡ºQR-LoRAï¼Œé€šè¿‡QRåˆ†è§£æå–æ­£äº¤åŸºå¹¶ä»…è®­ç»ƒæ ‡é‡ç³»æ•°ï¼Œæ˜¾è‘—é™ä½äº†å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQR-LoRAåœ¨å¤šä¸ªGLUEä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‚æ•°æ•°é‡å¤§å¹…å‡å°‘ï¼Œæ€§èƒ½ä¸å…¨å¾®è°ƒç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§„æ¨¡çš„ä¸æ–­æ‰©å¤§ï¼Œå¼€å‘å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯å˜å¾—æ„ˆåŠ é‡è¦ã€‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒæƒé‡åº”ç”¨ä½ç§©æ›´æ–°æ¥å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚å°½ç®¡æ ‡å‡†LoRAç›´æ¥å­¦ä¹ æ›´æ–°å› å­ï¼Œä½†ä¸€äº›æ–°å˜ä½“é¦–å…ˆé€šè¿‡å¯¹é¢„è®­ç»ƒæƒé‡è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ¥åˆå§‹åŒ–è¿™äº›çŸ©é˜µï¼Œè¿™åœ¨å¤§å‹æ¨¡å‹ä¸Šå¯èƒ½ä»£ä»·é«˜æ˜‚ä¸”éš¾ä»¥è§£é‡Šã€‚æœ¬æ–‡é€šè¿‡QRåˆ†è§£æå–é¢„è®­ç»ƒæƒé‡çŸ©é˜µçš„æ­£äº¤åŸºï¼Œå¹¶å°†LoRAæ›´æ–°è¡¨ç¤ºä¸ºè¿™äº›åŸºå‘é‡çš„çº¿æ€§ç»„åˆï¼Œä»…è®­ç»ƒæ ‡é‡ç³»æ•°ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å‚æ•°æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQR-LoRAåœ¨GLUEä»»åŠ¡ä¸Šä¸å…¨å¾®è°ƒã€æ ‡å‡†LoRAå’ŒSVD-LoRAçš„æ€§èƒ½ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå‚æ•°æ•°é‡ä»…ä¸º601ï¼Œç›¸è¾ƒäºå…¨å¾®è°ƒå‡å°‘è¶…è¿‡1000å€ï¼Œè¾ƒå…¸å‹LoRAå‡å°‘77å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­å‚æ•°æ•°é‡åºå¤§å’Œè®¡ç®—å¼€é”€é«˜çš„é—®é¢˜ã€‚ç°æœ‰çš„LoRAæ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†åœ¨åˆå§‹åŒ–æ›´æ–°çŸ©é˜µæ—¶ä½¿ç”¨SVDçš„è¿‡ç¨‹åœ¨å¤§å‹æ¨¡å‹ä¸Šä»£ä»·é«˜æ˜‚ä¸”éš¾ä»¥è§£é‡Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡QRåˆ†è§£æå–é¢„è®­ç»ƒæƒé‡çš„æ­£äº¤åŸºï¼Œå¹¶å°†LoRAæ›´æ–°è¡¨ç¤ºä¸ºè¿™äº›åŸºå‘é‡çš„çº¿æ€§ç»„åˆï¼Œä»…è®­ç»ƒæ ‡é‡ç³»æ•°ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å‡å°‘äº†å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œè¿˜ä¸ºé€‚åº”è¿‡ç¨‹æä¾›äº†æ¸…æ™°çš„ç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨QRåˆ†è§£æå–æ­£äº¤åŸºï¼›å…¶æ¬¡ï¼Œæ„å»ºLoRAæ›´æ–°çš„çº¿æ€§ç»„åˆï¼›æœ€åï¼Œä»…è®­ç»ƒè¿™äº›çº¿æ€§ç»„åˆçš„æ ‡é‡ç³»æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨QRåˆ†è§£æ›¿ä»£SVDè¿›è¡Œåˆå§‹åŒ–ï¼Œä»è€Œé¿å…äº†SVDçš„é«˜è®¡ç®—æˆæœ¬å’Œéš¾ä»¥è§£é‡Šçš„ç‰¹æ€§ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—é™ä½äº†å‚æ•°æ•°é‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒQR-LoRAä»…éœ€601ä¸ªå¯è®­ç»ƒå‚æ•°ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ä¼ ç»ŸLoRAç›¸ä¼¼ï¼Œä½†é€šè¿‡æ­£äº¤åŸºçš„ä½¿ç”¨ï¼Œè®­ç»ƒè¿‡ç¨‹æ›´åŠ é«˜æ•ˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒQR-LoRAåœ¨GLUEä»»åŠ¡ä¸Šä¸å…¨å¾®è°ƒã€æ ‡å‡†LoRAå’ŒSVD-LoRAçš„æ€§èƒ½ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå‚æ•°æ•°é‡ä»…ä¸º601ï¼Œç›¸è¾ƒäºå…¨å¾®è°ƒå‡å°‘è¶…è¿‡1000å€ï¼Œè¾ƒå…¸å‹LoRAå‡å°‘77å€ï¼Œå±•ç¤ºäº†å…¶åœ¨å‚æ•°æ•ˆç‡ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼ŒQR-LoRAèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿé€‚åº”ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The growing scale of Large Language Models (LLMs) has necessitated the development of parameter-efficient fine-tuning techniques. Low-Rank Adaptation (LoRA) has emerged as a promising approach, reducing the number of trainable parameters by applying low-rank updates to pretrained weights. While standard LoRA learns both update factors directly, several recent variants first initialize those matrices via an SVD of the pretrained weights -- an operation that can be expensive on large models and yields singular vectors that are not always easy to interpret. In this work, we extract an orthonormal basis from the pretrained weight matrix using QR decomposition with column pivoting, and then express the LoRA update as a linear combination of these basis vectors -- training only the scalar coefficients, which imposes clear structure on adaptation and drastically reduces parameter count. Experiments across GLUE tasks show that QR-LoRA matches or exceeds the performance of full fine-tuning, standard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular value decomposition) with as few as 601 parameters -- a reduction of over 1000x compared to full fine-tuning and 77x fewer than typical LoRA setups.

