---
layout: default
title: Speech Foundation Models Generalize to Time Series Tasks from Wearable Sensor Data
---

# Speech Foundation Models Generalize to Time Series Tasks from Wearable Sensor Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00221" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00221v3</a>
  <a href="https://arxiv.org/pdf/2509.00221.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00221v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00221v3', 'Speech Foundation Models Generalize to Time Series Tasks from Wearable Sensor Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jaya Narain, Zakaria Aldeneh, Shirley Ren

**åˆ†ç±»**: cs.LG, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29 (æ›´æ–°: 2025-11-23)

**å¤‡æ³¨**: Preprint, under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­éŸ³åŸºç¡€æ¨¡å‹ä»¥è§£å†³å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®çš„æ—¶é—´åºåˆ—ä»»åŠ¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—åˆ†æ` `å¯ç©¿æˆ´ä¼ æ„Ÿå™¨` `è¯­éŸ³åŸºç¡€æ¨¡å‹` `ç‰¹å¾æå–` `è·¨æ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®çš„æ—¶é—´åºåˆ—ä»»åŠ¡æ—¶ï¼Œå¾€å¾€ç¼ºä¹æœ‰æ•ˆçš„ç‰¹å¾è¡¨ç¤ºï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹æå–ç‰¹å¾ï¼Œä»¥å®ç°å¯¹å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®çš„æœ‰æ•ˆæ³›åŒ–ï¼Œæå‡åˆ†ç±»ä»»åŠ¡çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºHuBERTå’Œwav2vec 2.0æå–çš„ç‰¹å¾åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¶…è¶Šäº†ä¼ ç»Ÿè‡ªç›‘ç£æ¨¡å‹ï¼Œæå‡å¹…åº¦æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶è¡¨æ˜ï¼Œè¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—ä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¶…è¶Šè¯­éŸ³é¢†åŸŸï¼Œè¾¾åˆ°å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚é€šè¿‡ä»HuBERTå’Œwav2vec 2.0æå–çš„ç‰¹å¾è¿›è¡Œè®­ç»ƒçš„æ¢æµ‹å™¨ï¼Œåœ¨æƒ…ç»ªåˆ†ç±»ã€å¿ƒå¾‹å¤±å¸¸æ£€æµ‹å’Œæ´»åŠ¨åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç›´æ¥åœ¨ç‰¹å®šæ¨¡æ€æ•°æ®é›†ä¸Šè®­ç»ƒçš„è‡ªç›‘ç£æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œè¯­éŸ³æ¨¡å‹çš„å·ç§¯ç‰¹å¾ç¼–ç å™¨åœ¨å¯ç©¿æˆ´ä¼ æ„Ÿå™¨åº”ç”¨ä¸­å°¤ä¸ºé‡è¦ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„æ—¶é—´åºåˆ—ä»»åŠ¡ä¸­é€šè¿‡ç®€å•çš„æ¢æµ‹æ–¹æ³•æå‡äº†æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘ç»Ÿä¸€è¯­éŸ³å’Œä¼ æ„Ÿå™¨æ¨¡æ€çš„é€šç”¨æ—¶é—´åºåˆ—æ¨¡å‹è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®åœ¨æ—¶é—´åºåˆ—ä»»åŠ¡ä¸­çš„ç‰¹å¾è¡¨ç¤ºä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç‰¹å®šæ¨¡æ€çš„æ•°æ®é›†ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®ï¼Œæ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆå¦‚HuBERTå’Œwav2vec 2.0ï¼‰æå–ç‰¹å¾ï¼Œè¿™äº›æ¨¡å‹åœ¨è¯­éŸ³é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ—¶é—´å’Œé¢‘ç‡åŸŸçš„ä¿¡æ¯ï¼Œä»è€Œæå‡åœ¨å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ä¸Šçš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾æå–ã€æ¢æµ‹å™¨è®­ç»ƒå’Œä»»åŠ¡è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»è¯­éŸ³æ¨¡å‹ä¸­æå–ç‰¹å¾ï¼Œç„¶åä½¿ç”¨è¿™äº›ç‰¹å¾è®­ç»ƒæ¢æµ‹å™¨ï¼Œæœ€ååœ¨å¤šç§æ—¶é—´åºåˆ—ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†è¯­éŸ³åŸºç¡€æ¨¡å‹çš„å·ç§¯ç‰¹å¾ç¼–ç å™¨åº”ç”¨äºå¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ï¼Œæ˜¾è‘—æå‡äº†åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹çš„ä»»åŠ¡æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿè‡ªç›‘ç£æ¨¡å‹çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶è·¨æ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä½¿ç”¨äº†é€‚åˆæ—¶é—´åºåˆ—æ•°æ®çš„å·ç§¯ç½‘ç»œç»“æ„ï¼Œå¹¶é‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¢æµ‹å™¨çš„è®­ç»ƒæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºHuBERTå’Œwav2vec 2.0æå–çš„ç‰¹å¾åœ¨æƒ…ç»ªåˆ†ç±»ã€å¿ƒå¾‹å¤±å¸¸æ£€æµ‹å’Œæ´»åŠ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿè‡ªç›‘ç£æ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç¤ºäº†è¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—ä»»åŠ¡ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¥åº·ç›‘æµ‹ã€è¿åŠ¨åˆ†æå’Œæƒ…ç»ªè¯†åˆ«ç­‰ã€‚é€šè¿‡æå‡å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®çš„åˆ†æèƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ç²¾å‡†çš„å¥åº·åé¦ˆå’Œè¡Œä¸ºè¯†åˆ«ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨è·¨æ¨¡æ€å­¦ä¹ çš„å‘å±•ï¼Œä¿ƒè¿›ä¸åŒæ•°æ®æºçš„èåˆä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Both speech and sensor time series data encode information in both the time- and frequency- domains, like spectral powers and waveform shapelets. We show that speech foundation models learn representations that generalize beyond the speech domain and achieve state-of-the-art performance on diverse time-series tasks from wearable sensors. Probes trained on features extracted from HuBERT and wav2vec 2.0 outperform those extracted from self-supervised models trained directly on modality-specific datasets for mood classification, arrhythmia detection, and activity classification tasks. We find that the convolutional feature encoders of speech models are particularly relevant for wearable sensor applications. The proposed approach enhances performance on data-scarce time-series tasks using simple probing methods. This work takes a step toward developing generalized time-series models that unify speech and sensor modalities.

