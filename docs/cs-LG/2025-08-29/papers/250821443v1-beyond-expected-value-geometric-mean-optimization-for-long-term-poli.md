---
layout: default
title: Beyond expected value: geometric mean optimization for long-term policy performance in reinforcement learning
---

# Beyond expected value: geometric mean optimization for long-term policy performance in reinforcement learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21443" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21443v1</a>
  <a href="https://arxiv.org/pdf/2508.21443.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21443v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21443v1', 'Beyond expected value: geometric mean optimization for long-term policy performance in reinforcement learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinyi Sheng, Dominik Baumann

**åˆ†ç±»**: cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

**å¤‡æ³¨**: Accepted final version to appear in the Proceedings of the IEEE Conference on Decision and Control

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‡ ä½•å‡å€¼ä¼˜åŒ–ç®—æ³•ä»¥æå‡å¼ºåŒ–å­¦ä¹ é•¿æœŸç­–ç•¥è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å‡ ä½•å‡å€¼` `é•¿æœŸç­–ç•¥ä¼˜åŒ–` `æ—¶é—´å¹³å‡å¢é•¿ç‡` `ç®—æ³•åˆ›æ–°` `è·¯å¾„ä¾èµ–` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦ä¼˜åŒ–æœŸæœ›ç´¯ç§¯å¥–åŠ±ï¼Œå¯èƒ½æ— æ³•æœ‰æ•ˆåæ˜ ä¸ªä½“è½¨è¿¹çš„è¡¨ç°ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼Œç»“åˆäº†æ ‡å‡†é›†æˆå¹³å‡å’Œæ—¶é—´å¹³å‡å¢é•¿ç‡ï¼Œä»¥ä¼˜åŒ–ä¸ªä½“è½¨è¿¹çš„é•¿æœŸè¡¨ç°ï¼Œæå‡äº†ç­–ç•¥çš„é€‚åº”æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤æ‚æ¨¡æ‹Ÿç¯å¢ƒä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•é€šå¸¸ä¼˜åŒ–æœŸæœ›ç´¯ç§¯å¥–åŠ±ï¼Œå³ä»£ç†åœ¨è½¨è¿¹ä¸­è·å¾—çš„æ ‡é‡å¥–åŠ±ä¹‹å’Œçš„æœŸæœ›å€¼ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ç§æœŸæœ›å€¼å¯èƒ½å¯¹ä¸ªä½“è½¨è¿¹çš„è¡¨ç°ç¼ºä¹ä¿¡æ¯ã€‚å› æ­¤ï¼Œä¼˜åŒ–ä¸ªä½“è½¨è¿¹çš„é•¿æœŸè¡¨ç°åœ¨è®¸å¤šåº”ç”¨ä¸­å¯èƒ½æ›´ä¸ºç†æƒ³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„RLç®—æ³•ï¼Œå°†æ ‡å‡†çš„é›†æˆå¹³å‡ä¸æ—¶é—´å¹³å‡å¢é•¿ç‡ç›¸ç»“åˆï¼Œåè€…æ˜¯ä¸ªä½“è½¨è¿¹é•¿æœŸè¡¨ç°çš„åº¦é‡ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†æ—¶é—´å¹³å‡å¢é•¿ç‡çš„è´å°”æ›¼ç®—å­ï¼Œå¹¶å±•ç¤ºåœ¨ä¹˜æ³•å¥–åŠ±åŠ¨æ€ä¸‹ï¼Œå‡ ä½•å‡å€¼ä¸æ—¶é—´å¹³å‡å¢é•¿ç‡çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†åº”å¯¹æ›´ä¸€èˆ¬å’ŒæœªçŸ¥çš„å¥–åŠ±åŠ¨æ€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰Næ»‘åŠ¨çª—å£çš„ä¿®æ”¹å‡ ä½•å‡å€¼ï¼Œä½œä¸ºæ—¶é—´å¹³å‡å¢é•¿ç‡çš„ä¼°è®¡å™¨ã€‚è¯¥ä¼°è®¡å™¨ä½œä¸ºæ­£åˆ™åŒ–é¡¹åµŒå…¥ç›®æ ‡ä¸­ï¼Œå½¢æˆä¸€ç§å®ç”¨ç®—æ³•ï¼Œä½¿ç­–ç•¥èƒ½å¤ŸåŒæ—¶å—ç›Šäºé›†æˆå¹³å‡å’Œæ—¶é—´å¹³å‡ã€‚æˆ‘ä»¬åœ¨æŒ‘æˆ˜æ€§æ¨¡æ‹Ÿä¸­è¯„ä¼°äº†è¯¥ç®—æ³•ï¼Œç»“æœä¼˜äºä¼ ç»ŸRLæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ä¼˜åŒ–ä¸ªä½“è½¨è¿¹è¡¨ç°æ—¶çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯æœŸæœ›å€¼å¯¹ä¸ªä½“è½¨è¿¹çš„è¡¨ç°ç¼ºä¹æŒ‡å¯¼æ€§çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§æ–°é¢–çš„RLç®—æ³•ï¼Œç»“åˆé›†æˆå¹³å‡ä¸æ—¶é—´å¹³å‡å¢é•¿ç‡ï¼Œä¼˜åŒ–ä¸ªä½“è½¨è¿¹çš„é•¿æœŸè¡¨ç°ï¼Œä»¥é€‚åº”å®é™…åº”ç”¨éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç®—æ³•é¦–å…ˆå®šä¹‰æ—¶é—´å¹³å‡å¢é•¿ç‡çš„è´å°”æ›¼ç®—å­ï¼Œåœ¨ä¹˜æ³•å¥–åŠ±åŠ¨æ€ä¸‹ï¼Œå‡ ä½•å‡å€¼ä¸æ—¶é—´å¹³å‡å¢é•¿ç‡ä¸€è‡´ã€‚ä¸ºåº”å¯¹æ›´å¤æ‚çš„å¥–åŠ±åŠ¨æ€ï¼Œè®¾è®¡äº†å¸¦æœ‰Næ»‘åŠ¨çª—å£çš„å‡ ä½•å‡å€¼ä½œä¸ºæ—¶é—´å¹³å‡å¢é•¿ç‡çš„ä¼°è®¡å™¨ï¼Œå¹¶å°†å…¶åµŒå…¥ç›®æ ‡å‡½æ•°ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ—¶é—´å¹³å‡å¢é•¿ç‡ä¸å‡ ä½•å‡å€¼ç»“åˆï¼Œå½¢æˆæ–°çš„ä¼˜åŒ–ç›®æ ‡ï¼Œä½¿å¾—ç­–ç•¥èƒ½å¤ŸåŒæ—¶è€ƒè™‘é›†æˆå¹³å‡å’Œæ—¶é—´å¹³å‡çš„ä¼˜åŠ¿ï¼Œæ˜¾è‘—æå‡ä¸ªä½“è½¨è¿¹çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šç®—æ³•ä¸­å¼•å…¥äº†Næ»‘åŠ¨çª—å£çš„å‡ ä½•å‡å€¼ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œå…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æ—¨åœ¨å¹³è¡¡é›†æˆå¹³å‡ä¸æ—¶é—´å¹³å‡çš„å½±å“ï¼Œä»è€Œæé«˜ç­–ç•¥çš„é•¿æœŸè¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„ç®—æ³•åœ¨å¤æ‚æ¨¡æ‹Ÿç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ€§èƒ½æå‡è¶…è¿‡20%ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†ç®—æ³•åœ¨ä¼˜åŒ–é•¿æœŸç­–ç•¥è¡¨ç°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€é‡‘èå†³ç­–ç­‰éœ€è¦é•¿æœŸç­–ç•¥ä¼˜åŒ–çš„åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–ä¸ªä½“è½¨è¿¹çš„è¡¨ç°ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿåœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­æå‡å†³ç­–è´¨é‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) algorithms typically optimize the expected cumulative reward, i.e., the expected value of the sum of scalar rewards an agent receives over the course of a trajectory. The expected value averages the performance over an infinite number of trajectories. However, when deploying the agent in the real world, this ensemble average may be uninformative for the performance of individual trajectories. Thus, in many applications, optimizing the long-term performance of individual trajectories might be more desirable. In this work, we propose a novel RL algorithm that combines the standard ensemble average with the time-average growth rate, a measure for the long-term performance of individual trajectories. We first define the Bellman operator for the time-average growth rate. We then show that, under multiplicative reward dynamics, the geometric mean aligns with the time-average growth rate. To address more general and unknown reward dynamics, we propose a modified geometric mean with $N$-sliding window that captures the path-dependency as an estimator for the time-average growth rate. This estimator is embedded as a regularizer into the objective, forming a practical algorithm and enabling the policy to benefit from ensemble average and time-average simultaneously. We evaluate our algorithm in challenging simulations, where it outperforms conventional RL methods.

