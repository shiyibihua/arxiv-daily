---
layout: default
title: Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone
---

# Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10809" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10809v2</a>
  <a href="https://arxiv.org/pdf/2509.10809.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10809v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10809v2', 'Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Antonio BÄƒrbÄƒlau, Cristian Daniel PÄƒduraru, Teodor Poncu, Alexandru Tifrea, Elena Burceanu

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-13 (æ›´æ–°: 2025-12-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºS&P Top-Kï¼Œä¸€ç§encoder-centricçš„ç¨€ç–è‡ªç¼–ç å™¨æ”¹è¿›æ–¹æ³•ï¼Œç”¨äºæå‡æ¨¡å‹å…¬å¹³æ€§å’Œå¯æ§æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `æ¨¡å‹æ“æ§` `å…¬å¹³æ€§` `å¯æ§æ€§` `ç¼–ç å™¨å¹²é¢„` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿç¨€ç–è‡ªç¼–ç å™¨(SAE)ä¾èµ–è§£ç å™¨ä¿®æ”¹ä¸­é—´è¡¨ç¤ºè¿›è¡Œæ¨¡å‹æ“æ§ï¼Œå­˜åœ¨è·¨æ¨¡æ€æ€§èƒ½ç“¶é¢ˆã€‚
2. æå‡ºS&P Top-Kæ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©å’ŒæŠ•å½±ç¼–ç å™¨ç‰¹å¾ï¼Œç›´æ¥åœ¨æ¨¡å‹åŸç”ŸåµŒå…¥ç©ºé—´è¿›è¡Œå¹²é¢„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒS&P Top-Kåœ¨è§†è§‰-è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ˜¾è‘—æå‡äº†å…¬å¹³æ€§å’Œå¯æ§æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–è‡ªç¼–ç å™¨(SAEs)è¢«å¹¿æ³›åº”ç”¨äºæœºåˆ¶å¯è§£é‡Šæ€§å’Œæ¨¡å‹æ“æ§ã€‚é€šå¸¸ï¼Œæ¨¡å‹æ“æ§é€šè¿‡è§£ç ä¿®æ”¹åçš„SAEä¸­é—´è¡¨ç¤ºæ¥å®ç°ï¼Œæœ¬è´¨ä¸Šæ˜¯å°†åŸå§‹æ¿€æ´»é‡å†™ä¸ºè§£ç å™¨ç‰¹å¾çš„åŠ æƒå’Œã€‚ä¸ç°æœ‰æ–‡çŒ®ä¸åŒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»¥ç¼–ç å™¨ä¸ºä¸­å¿ƒçš„æ¨¡å‹æ“æ§æ›¿ä»£æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå±•ç¤ºäº†æ›´å¼ºçš„è·¨æ¨¡æ€æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†S&P Top-Kï¼Œä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒä¸”è®¡ç®—é‡è½»çš„Selection and Projectionæ¡†æ¶ï¼Œç”¨äºè¯†åˆ«ä¸æ•æ„Ÿå±æ€§æˆ–è¡Œä¸ºå¯¹é½çš„Top-Kç¼–ç å™¨ç‰¹å¾ï¼Œå¯é€‰æ‹©æ€§åœ°å°†å®ƒä»¬èšåˆä¸ºå•ä¸ªæ§åˆ¶è½´ï¼Œå¹¶è®¡ç®—æ­£äº¤æŠ•å½±ï¼Œéšåç›´æ¥åº”ç”¨äºæ¨¡å‹çš„åŸç”ŸåµŒå…¥ç©ºé—´ã€‚åœ¨è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­ï¼Œå®ƒåœ¨CelebAå’ŒFairFaceä¸Šå°†å…¬å¹³æ€§æŒ‡æ ‡æé«˜äº†é«˜è¾¾3.2å€ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œå®ƒæ˜¾è‘—é™ä½äº†Llama-3 8B Instructçš„æ”»å‡»æ€§å’Œè°„åªšæ€§ï¼Œå®ç°äº†é«˜è¾¾3.6å€çš„å¢ç›Šã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„ä»¥è§£ç å™¨ä¸ºä¸­å¿ƒçš„SAEä½¿ç”¨ç›¸æ¯”ï¼Œä»¥ç¼–ç å™¨ä¸ºä¸­å¿ƒçš„å¹²é¢„æä¾›äº†ä¸€ç§é€šç”¨ã€é«˜æ•ˆä¸”æ›´æœ‰æ•ˆçš„æœºåˆ¶ï¼Œç”¨äºåœ¨æ¨ç†æ—¶å¡‘é€ æ¨¡å‹è¡Œä¸ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰çš„æ¨¡å‹æ“æ§æ–¹æ³•ä¸»è¦ä¾èµ–äºè§£ç å™¨ï¼Œé€šè¿‡ä¿®æ”¹SAEçš„ä¸­é—´è¡¨ç¤ºæ¥å®ç°å¯¹æ¨¡å‹è¡Œä¸ºçš„å¹²é¢„ã€‚è¿™ç§æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œè§£ç å™¨å¯èƒ½æ— æ³•å……åˆ†æ•æ‰åˆ°ç¼–ç å™¨ä¸­è•´å«çš„å…¨éƒ¨ä¿¡æ¯ï¼Œå¯¼è‡´æ“æ§æ•ˆæœå—é™ï¼Œå°¤å…¶æ˜¯åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚æ­¤å¤–ï¼Œè§£ç å™¨ä¸­å¿ƒçš„å¹²é¢„æ–¹å¼å¯èƒ½å¼•å…¥é¢å¤–çš„è®¡ç®—è´Ÿæ‹…å’Œå¤æ‚æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è½¬å˜è§†è§’ï¼Œä»ç¼–ç å™¨å…¥æ‰‹ï¼Œç›´æ¥åœ¨ç¼–ç å™¨çš„ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œå¹²é¢„ã€‚é€šè¿‡é€‰æ‹©ä¸ç‰¹å®šå±æ€§æˆ–è¡Œä¸ºç›¸å…³çš„Top-Kä¸ªç¼–ç å™¨ç‰¹å¾ï¼Œå¹¶è¿›è¡ŒæŠ•å½±æ“ä½œï¼Œä»è€Œåœ¨æ¨¡å‹çš„åŸç”ŸåµŒå…¥ç©ºé—´ä¸­å®ç°å¯¹æ¨¡å‹è¡Œä¸ºçš„ç²¾ç¡®æ§åˆ¶ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹è§£ç å™¨çš„ä¾èµ–ï¼Œç®€åŒ–äº†å¹²é¢„æµç¨‹ï¼Œå¹¶æœ‰æœ›æå‡å¹²é¢„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šS&P Top-Kæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **ç‰¹å¾é€‰æ‹©**ï¼šåŸºäºæŸç§åº¦é‡æ ‡å‡†ï¼ˆä¾‹å¦‚ï¼Œç‰¹å¾ä¸æ•æ„Ÿå±æ€§çš„ç›¸å…³æ€§ï¼‰ï¼Œä»ç¼–ç å™¨çš„ç‰¹å¾ç©ºé—´ä¸­é€‰æ‹©Top-Kä¸ªæœ€ç›¸å…³çš„ç‰¹å¾ã€‚2) **ç‰¹å¾èšåˆï¼ˆå¯é€‰ï¼‰**ï¼šå°†é€‰å®šçš„Top-Kä¸ªç‰¹å¾èšåˆä¸ºä¸€ä¸ªå•ä¸€çš„æ§åˆ¶è½´ï¼Œç”¨äºåç»­çš„æŠ•å½±æ“ä½œã€‚3) **æ­£äº¤æŠ•å½±**ï¼šè®¡ç®—ä¸€ä¸ªæ­£äº¤æŠ•å½±çŸ©é˜µï¼Œå°†æ¨¡å‹çš„åµŒå…¥å‘é‡æŠ•å½±åˆ°ä¸æ§åˆ¶è½´æ­£äº¤çš„ç©ºé—´ä¸­ã€‚4) **å¹²é¢„åº”ç”¨**ï¼šå°†è®¡ç®—å¾—åˆ°çš„æ­£äº¤æŠ•å½±åº”ç”¨äºæ¨¡å‹çš„åŸç”ŸåµŒå…¥ç©ºé—´ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹è¡Œä¸ºçš„å¹²é¢„ã€‚

**å…³é”®åˆ›æ–°**ï¼šS&P Top-Kæ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä»¥ç¼–ç å™¨ä¸ºä¸­å¿ƒçš„å¹²é¢„ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„è§£ç å™¨ä¸­å¿ƒæ–¹æ³•ç›¸æ¯”ï¼ŒS&P Top-Kç›´æ¥åœ¨ç¼–ç å™¨çš„ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œé¿å…äº†å¯¹è§£ç å™¨çš„ä¾èµ–ï¼Œç®€åŒ–äº†å¹²é¢„æµç¨‹ï¼Œå¹¶æœ‰æœ›æå‡å¹²é¢„æ•ˆæœã€‚æ­¤å¤–ï¼ŒS&P Top-Kæ˜¯ä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒçš„æ–¹æ³•ï¼Œè®¡ç®—é‡è½»ï¼Œæ˜“äºéƒ¨ç½²å’Œåº”ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šS&P Top-Kæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **Top-Ké€‰æ‹©ç­–ç•¥**ï¼šå¦‚ä½•é€‰æ‹©ä¸ç‰¹å®šå±æ€§æˆ–è¡Œä¸ºç›¸å…³çš„Top-Kä¸ªç‰¹å¾ï¼Ÿå¯ä»¥ä½¿ç”¨ç›¸å…³æ€§åˆ†æã€äº’ä¿¡æ¯ç­‰æ–¹æ³•æ¥è¡¡é‡ç‰¹å¾ä¸å±æ€§ä¹‹é—´çš„å…³ç³»ã€‚2) **ç‰¹å¾èšåˆæ–¹æ³•**ï¼šå¦‚æœé€‰æ‹©å°†Top-Kä¸ªç‰¹å¾èšåˆä¸ºä¸€ä¸ªæ§åˆ¶è½´ï¼Œåº”è¯¥é‡‡ç”¨ä½•ç§èšåˆæ–¹æ³•ï¼Ÿå¯ä»¥ä½¿ç”¨ç®€å•çš„å¹³å‡ã€åŠ æƒå¹³å‡ç­‰æ–¹æ³•ã€‚3) **æ­£äº¤æŠ•å½±çš„è®¡ç®—**ï¼šå¦‚ä½•è®¡ç®—æ­£äº¤æŠ•å½±çŸ©é˜µï¼Ÿå¯ä»¥ä½¿ç”¨çº¿æ€§ä»£æ•°ä¸­çš„æ­£äº¤åŒ–æ–¹æ³•ï¼Œä¾‹å¦‚Gram-Schmidtæ­£äº¤åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒS&P Top-Kæ¡†æ¶åœ¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆCelebAå’ŒFairFaceæ•°æ®é›†ï¼‰ä¸Šå°†å…¬å¹³æ€§æŒ‡æ ‡æé«˜äº†é«˜è¾¾3.2å€ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLlama-3 8B Instructï¼‰ä¸Šæ˜¾è‘—é™ä½äº†æ”»å‡»æ€§å’Œè°„åªšæ€§ï¼Œå®ç°äº†é«˜è¾¾3.6å€çš„å¢ç›Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒS&P Top-Kæ˜¯ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ¨¡å‹æ“æ§æ–¹æ³•ï¼Œä¼˜äºä¼ ç»Ÿçš„åŸºäºè§£ç å™¨çš„SAEæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦æå‡æ¨¡å‹å…¬å¹³æ€§å’Œå¯æ§æ€§çš„é¢†åŸŸï¼Œä¾‹å¦‚äººè„¸è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚é€šè¿‡S&P Top-Kæ¡†æ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å‡å°‘æ¨¡å‹å¯¹æ•æ„Ÿå±æ€§çš„åè§ï¼Œæå‡æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„è¡Œä¸ºå¯æ§æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ›´å¤šå¤æ‚çš„æ¨¡å‹å’Œä»»åŠ¡ä¸­ï¼Œä¸ºäººå·¥æ™ºèƒ½çš„å¥åº·å‘å±•åšå‡ºè´¡çŒ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse Autoencoders (SAEs) are widely employed for mechanistic interpretability and model steering. Within this context, steering is by design performed by means of decoding altered SAE intermediate representations. This procedure essentially rewrites the original activations as a weighted sum of decoder features. In contrast to existing literature, we forward an encoder-centric alternative to model steering which demonstrates a stronger cross-modal performance. We introduce S&P Top-K, a retraining-free and computationally lightweight Selection and Projection framework that identifies Top-K encoder features aligned with a sensitive attribute or behavior, optionally aggregates them into a single control axis, and computes an orthogonal projection to be subsequently applied directly in the model's native embedding space. In vision-language models, it improves fairness metrics on CelebA and FairFace by up to 3.2 times over conventional SAE usage, and in large language models, it substantially reduces aggressiveness and sycophancy in Llama-3 8B Instruct, achieving up to 3.6 times gains over masked reconstruction. These findings suggest that encoder-centric interventions provide a general, efficient, and more effective mechanism for shaping model behavior at inference time than the traditional decoder-centric use of SAEs.

