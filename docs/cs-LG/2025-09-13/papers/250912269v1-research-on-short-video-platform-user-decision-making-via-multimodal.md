---
layout: default
title: Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning
---

# Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.12269" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.12269v1</a>
  <a href="https://arxiv.org/pdf/2509.12269.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.12269v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.12269v1', 'Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinmeiyang Wang, Jing Dong, Li Zhou

**åˆ†ç±»**: cs.LG, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-13

**å¤‡æ³¨**: 26 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMT-DQNæ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€æ—¶åºå»ºæ¨¡å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çŸ­è§†é¢‘ç”¨æˆ·å†³ç­–é¢„æµ‹ä¸æ¨èã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ­è§†é¢‘æ¨è` `å¤šæ¨¡æ€å­¦ä¹ ` `æ—¶åºå»ºæ¨¡` `å¼ºåŒ–å­¦ä¹ ` `ç”¨æˆ·è¡Œä¸ºé¢„æµ‹` `Transformer` `å›¾ç¥ç»ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ­è§†é¢‘æ¨èæ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯å’Œç”¨æˆ·è¡Œä¸ºçš„æ—¶åºä¾èµ–å…³ç³»ï¼Œå¯¼è‡´æ¨èæ•ˆæœä¸ä½³ã€‚
2. MT-DQNæ¨¡å‹é€šè¿‡Transformeræå–å¤šæ¨¡æ€ç‰¹å¾ï¼ŒTGNNå»ºæ¨¡ç”¨æˆ·è¡Œä¸ºæ—¶åºå…³ç³»ï¼ŒDQNä¼˜åŒ–æ¨èç­–ç•¥ï¼Œå®ç°æ›´ç²¾å‡†çš„æ¨èã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMT-DQNåœ¨F1åˆ†æ•°å’ŒNDCG@5æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå¹¶åœ¨MSEå’ŒMAEæŒ‡æ ‡ä¸Šä¼˜äºVanilla-DQNã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMT-DQNçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é›†æˆäº†Transformerã€æ—¶åºå›¾ç¥ç»ç½‘ç»œï¼ˆTGNNï¼‰å’Œæ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ï¼Œæ—¨åœ¨è§£å†³çŸ­è§†é¢‘ç¯å¢ƒä¸­é¢„æµ‹ç”¨æˆ·è¡Œä¸ºå’Œä¼˜åŒ–æ¨èç­–ç•¥çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMT-DQNå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„æ‹¼æ¥æ¨¡å‹ï¼ˆå¦‚Concat-Modalï¼‰ï¼Œå¹³å‡F1åˆ†æ•°æé«˜äº†10.97%ï¼Œå¹³å‡NDCG@5æé«˜äº†8.3%ã€‚ä¸ç»å…¸çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹Vanilla-DQNç›¸æ¯”ï¼ŒMT-DQNçš„MSEé™ä½äº†34.8%ï¼ŒMAEé™ä½äº†26.5%ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿè®¤è¯†åˆ°åœ¨å®é™…åœºæ™¯ä¸­éƒ¨ç½²MT-DQNé¢ä¸´æŒ‘æˆ˜ï¼Œä¾‹å¦‚å…¶è®¡ç®—æˆæœ¬å’Œåœ¨çº¿æ¨ç†æœŸé—´çš„å»¶è¿Ÿæ•æ„Ÿæ€§ï¼Œè¿™äº›é—®é¢˜å°†åœ¨æœªæ¥çš„æ¶æ„ä¼˜åŒ–ä¸­å¾—åˆ°è§£å†³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³çŸ­è§†é¢‘æ¨èç³»ç»Ÿä¸­ç”¨æˆ·è¡Œä¸ºé¢„æµ‹å’Œæ¨èç­–ç•¥ä¼˜åŒ–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ç®€å•çš„æ‹¼æ¥æ–¹å¼èåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¿½ç•¥äº†ç”¨æˆ·è¡Œä¸ºçš„æ—¶åºä¾èµ–å…³ç³»ï¼Œå¹¶ä¸”ç¼ºä¹æœ‰æ•ˆçš„æ¨èç­–ç•¥ä¼˜åŒ–æœºåˆ¶ï¼Œå¯¼è‡´æ¨èç²¾åº¦ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€æ—¶åºå»ºæ¨¡æ¥æ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·è¡Œä¸ºï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨èç­–ç•¥ã€‚é€šè¿‡Transformeræå–å¤šæ¨¡æ€ç‰¹å¾ï¼ŒTGNNå»ºæ¨¡ç”¨æˆ·è¡Œä¸ºçš„æ—¶åºå…³ç³»ï¼Œæœ€ååˆ©ç”¨DQNå­¦ä¹ æœ€ä¼˜çš„æ¨èç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMT-DQNæ¨¡å‹ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼š1) å¤šæ¨¡æ€ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨Transformeræå–çŸ­è§†é¢‘çš„è§†è§‰ã€æ–‡æœ¬ç­‰å¤šç§æ¨¡æ€çš„ç‰¹å¾ã€‚2) æ—¶åºå»ºæ¨¡æ¨¡å—ï¼šä½¿ç”¨TGNNå»ºæ¨¡ç”¨æˆ·åœ¨çŸ­è§†é¢‘å¹³å°ä¸Šçš„è¡Œä¸ºåºåˆ—ï¼Œæ•æ‰ç”¨æˆ·è¡Œä¸ºçš„æ—¶åºä¾èµ–å…³ç³»ã€‚3) å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼šä½¿ç”¨DQNå­¦ä¹ æœ€ä¼˜çš„æ¨èç­–ç•¥ï¼Œæ ¹æ®ç”¨æˆ·çŠ¶æ€ï¼ˆç”±å¤šæ¨¡æ€ç‰¹å¾å’Œæ—¶åºè¡Œä¸ºè¡¨ç¤ºï¼‰é€‰æ‹©åˆé€‚çš„æ¨èåŠ¨ä½œï¼ˆå³æ¨èå“ªä¸ªçŸ­è§†é¢‘ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ¨¡å‹æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å¤šæ¨¡æ€æ—¶åºå»ºæ¨¡ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œå……åˆ†åˆ©ç”¨äº†çŸ­è§†é¢‘çš„å¤šæ¨¡æ€ä¿¡æ¯å’Œç”¨æˆ·è¡Œä¸ºçš„æ—¶åºå…³ç³»ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨èç­–ç•¥ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒMT-DQNèƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹ç”¨æˆ·è¡Œä¸ºï¼Œå¹¶æä¾›æ›´ä¸ªæ€§åŒ–çš„æ¨èã€‚

**å…³é”®è®¾è®¡**ï¼šTransformerçš„å±‚æ•°å’Œéšè—å±‚ç»´åº¦éœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚TGNNé‡‡ç”¨GRUä½œä¸ºèŠ‚ç‚¹æ›´æ–°å‡½æ•°ï¼Œå¹¶ä½¿ç”¨Attentionæœºåˆ¶æ¥å­¦ä¹ ä¸åŒæ—¶é—´æ­¥çš„é‡è¦æ€§ã€‚DQNä½¿ç”¨Îµ-greedyç­–ç•¥è¿›è¡Œæ¢ç´¢ï¼Œå¹¶é‡‡ç”¨ç»éªŒå›æ”¾æœºåˆ¶æ¥æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬DQNçš„Qå€¼æŸå¤±å’ŒTGNNçš„é“¾æ¥é¢„æµ‹æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMT-DQNæ¨¡å‹åœ¨çŸ­è§†é¢‘æ¨èä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ä¼ ç»Ÿçš„æ‹¼æ¥æ¨¡å‹ï¼ˆConcat-Modalï¼‰ç›¸æ¯”ï¼ŒMT-DQNçš„å¹³å‡F1åˆ†æ•°æé«˜äº†10.97%ï¼Œå¹³å‡NDCG@5æé«˜äº†8.3%ã€‚ä¸ç»å…¸çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹Vanilla-DQNç›¸æ¯”ï¼ŒMT-DQNçš„MSEé™ä½äº†34.8%ï¼ŒMAEé™ä½äº†26.5%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§çŸ­è§†é¢‘æ¨èå¹³å°ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œå¹³å°æ”¶ç›Šã€‚é€šè¿‡æ›´ç²¾å‡†çš„ç”¨æˆ·è¡Œä¸ºé¢„æµ‹å’Œä¸ªæ€§åŒ–æ¨èï¼Œå¯ä»¥æé«˜ç”¨æˆ·ç‚¹å‡»ç‡ã€è§‚çœ‹æ—¶é•¿å’Œç”¨æˆ·ç²˜æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æ¨èåœºæ™¯ï¼Œå¦‚ç”µå•†ã€æ–°é—»ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper proposes the MT-DQN model, which integrates a Transformer, Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address the challenges of predicting user behavior and optimizing recommendation strategies in short-video environments. Experiments demonstrated that MT-DQN consistently outperforms traditional concatenated models, such as Concat-Modal, achieving an average F1-score improvement of 10.97% and an average NDCG@5 improvement of 8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQN reduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognize challenges in deploying MT-DQN in real-world scenarios, such as its computational cost and latency sensitivity during online inference, which will be addressed through future architectural optimization.

