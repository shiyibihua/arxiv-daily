---
layout: default
title: GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research
---

# GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10790" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10790v1</a>
  <a href="https://arxiv.org/pdf/2509.10790.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10790v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10790v1', 'GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Luke Howard

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-13

**å¤‡æ³¨**: 4 Pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GoldenTransformerï¼šç”¨äºTransformeré²æ£’æ€§ç ”ç©¶çš„æ¨¡å—åŒ–æ•…éšœæ³¨å…¥æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Transformeræ¨¡å‹` `æ•…éšœæ³¨å…¥` `é²æ£’æ€§è¯„ä¼°` `ç¡¬ä»¶æ•…éšœ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Transformeræ¨¡å‹åœ¨ç¡¬ä»¶æ•…éšœä¸‹çš„é²æ£’æ€§ç ”ç©¶ä¸è¶³ï¼Œéš¾ä»¥è¯„ä¼°å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚
2. GoldenTransformeræ¡†æ¶é€šè¿‡æ¨¡å—åŒ–æ•…éšœæ³¨å…¥ï¼Œæ¨¡æ‹Ÿæƒé‡æŸåã€æ¿€æ´»æ³¨å…¥ç­‰å¤šç§æ•…éšœç±»å‹ï¼Œè¯„ä¼°æ¨¡å‹é²æ£’æ€§ã€‚
3. è¯¥æ¡†æ¶åŸºäºPyTorchå’ŒHuggingFace Transformersï¼Œæ”¯æŒå®éªŒå¤ç°ã€æŒ‡æ ‡è®°å½•å’Œå¯è§†åŒ–ï¼Œæ–¹ä¾¿ç ”ç©¶å’Œåº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Transformeræ¨¡å‹å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå…¶å®ƒæœºå™¨å­¦ä¹ é¢†åŸŸä¸­å¤§é‡å…ˆè¿›æ¨¡å‹çš„åŸºç¡€ã€‚å°½ç®¡å®ƒä»¬è¢«å¹¿æ³›éƒ¨ç½²ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨æ•…éšœæ¡ä»¶ä¸‹çš„é²æ£’æ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†GoldenTransformerï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„æ•…éšœæ³¨å…¥æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è¯±å¯¼ç¡¬ä»¶æ•…éšœçš„å¼¹æ€§ã€‚GoldenTransformeræä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºPythonçš„å¹³å°ï¼Œç”¨äºå°†å„ç§ç±»å‹çš„æ•…éšœï¼ˆå¦‚æƒé‡æŸåã€æ¿€æ´»æ³¨å…¥å’Œæ³¨æ„åŠ›çº§åˆ«ä¸­æ–­ï¼‰æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„åŸºäºTransformerçš„æ¨¡å‹ä¸­ã€‚å—åˆ°DNNçš„GoldenEyeæ¨¡æ‹Ÿå™¨çš„å¯å‘ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸“æ³¨äºå¤„ç†å¤§å‹Transformeræ¶æ„çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç»“æ„å¤æ‚æ€§ã€æ½œåœ¨ä¾èµ–æ€§å’Œéå‡åŒ€å±‚å®šä¹‰ç­‰è€ƒè™‘å› ç´ ã€‚GoldenTransformeræ„å»ºäºPyTorchå’ŒHuggingFace Transformersä¹‹ä¸Šï¼Œå®ƒæ”¯æŒå¼€ç®±å³ç”¨çš„å®éªŒå¯é‡å¤æ€§ã€æŒ‡æ ‡è®°å½•å’Œå¯è§†åŒ–ã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†GoldenTransformerçš„æŠ€æœ¯è®¾è®¡å’Œä½¿ç”¨ï¼Œå¹¶é€šè¿‡åˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡çš„å‡ ä¸ªç¤ºä¾‹å®éªŒè¿›è¡Œäº†æ¼”ç¤ºã€‚é€šè¿‡åœ¨Transformerä¸­çš„å¤šä¸ªé€»è¾‘å’Œç»“æ„ç‚¹å®ç°å—æ§çš„æ•…éšœæ³¨å…¥ï¼ŒGoldenTransformerä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šäººå‘˜æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ï¼Œç”¨äºæ¨¡å‹é²æ£’æ€§åˆ†æå’ŒæŒ‡å¯¼å®é™…LLMåº”ç”¨ä¸­å¯é çš„ç³»ç»Ÿè®¾è®¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹Transformeræ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œç”±äºç¡¬ä»¶æ•…éšœå¯¼è‡´çš„å¯é æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ç³»ç»Ÿæ€§çš„æ•…éšœæ³¨å…¥å’Œé²æ£’æ€§è¯„ä¼°æ¡†æ¶ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ†æå’Œæå‡æ¨¡å‹çš„å®¹é”™èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‰©å±•çš„æ•…éšœæ³¨å…¥æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿä¸åŒç±»å‹çš„ç¡¬ä»¶æ•…éšœï¼Œè¯„ä¼°Transformeræ¨¡å‹åœ¨å„ç§æ•…éšœæ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æ¡†æ¶å…è®¸ç ”ç©¶äººå‘˜åœ¨æ¨¡å‹çš„ä¸åŒå±‚çº§ï¼ˆå¦‚æƒé‡ã€æ¿€æ´»ã€æ³¨æ„åŠ›æœºåˆ¶ï¼‰æ³¨å…¥æ•…éšœï¼Œä»è€Œå…¨é¢åˆ†ææ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGoldenTransformeræ¡†æ¶åŸºäºPythonï¼Œæ„å»ºäºPyTorchå’ŒHuggingFace Transformersä¹‹ä¸Šã€‚å®ƒåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) æ•…éšœæ³¨å…¥æ¨¡å—ï¼šç”¨äºå®šä¹‰å’Œæ³¨å…¥å„ç§ç±»å‹çš„æ•…éšœï¼Œå¦‚æƒé‡æŸåã€æ¿€æ´»æ³¨å…¥ã€æ³¨æ„åŠ›å¹²æ‰°ç­‰ã€‚2) æ¨¡å‹åŠ è½½æ¨¡å—ï¼šç”¨äºåŠ è½½é¢„è®­ç»ƒçš„Transformeræ¨¡å‹ã€‚3) å®éªŒç®¡ç†æ¨¡å—ï¼šç”¨äºç®¡ç†å®éªŒé…ç½®ã€è®°å½•å®éªŒç»“æœå’Œå¯è§†åŒ–æ€§èƒ½æŒ‡æ ‡ã€‚4) è¯„ä¼°æ¨¡å—ï¼šç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æ•…éšœæ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼Œå¹¶ä¸æ— æ•…éšœæƒ…å†µè¿›è¡Œæ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„è®¾è®¡ï¼Œå…è®¸ç ”ç©¶äººå‘˜è½»æ¾æ·»åŠ æ–°çš„æ•…éšœç±»å‹å’Œè¯„ä¼°æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜è€ƒè™‘äº†å¤§å‹Transformeræ¶æ„çš„ç‰¹æ®Šæ€§ï¼Œå¦‚ç»“æ„å¤æ‚æ€§ã€å±‚é—´ä¾èµ–æ€§å’Œéå‡åŒ€å±‚å®šä¹‰ï¼Œä»è€Œæ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿå®é™…ç¡¬ä»¶æ•…éšœçš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šGoldenTransformerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•…éšœæ³¨å…¥ä½ç½®çš„é€‰æ‹©ï¼šå…è®¸åœ¨æ¨¡å‹çš„ä¸åŒå±‚çº§ï¼ˆå¦‚åµŒå…¥å±‚ã€æ³¨æ„åŠ›å±‚ã€å‰é¦ˆç½‘ç»œå±‚ï¼‰æ³¨å…¥æ•…éšœã€‚2) æ•…éšœç±»å‹çš„å®šä¹‰ï¼šæ”¯æŒå¤šç§æ•…éšœç±»å‹ï¼Œå¦‚éšæœºä½ç¿»è½¬ã€æƒé‡ç½®é›¶ã€æ¿€æ´»å€¼æ‰°åŠ¨ç­‰ã€‚3) è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ï¼šä½¿ç”¨å‡†ç¡®ç‡ã€F1å€¼ã€å›°æƒ‘åº¦ç­‰æŒ‡æ ‡è¯„ä¼°æ¨¡å‹åœ¨æ•…éšœæ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡åœ¨åˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†GoldenTransformeræ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿä¸åŒç±»å‹çš„ç¡¬ä»¶æ•…éšœï¼Œå¹¶è¯„ä¼°æ¨¡å‹åœ¨å„ç§æ•…éšœæ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šæ•…éšœæ³¨å…¥æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡ä¸‹é™äº†X%ï¼Œè¡¨æ˜å…¶å¯¹è¯¥ç±»æ•…éšœçš„æ•æ„Ÿæ€§ã€‚è¿™äº›å®éªŒç»“æœä¸ºæ¨¡å‹é²æ£’æ€§åˆ†æå’Œç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¯¹å¯é æ€§è¦æ±‚é«˜çš„Transformeræ¨¡å‹éƒ¨ç½²åœºæ™¯ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚é€šè¿‡GoldenTransformeræ¡†æ¶ï¼Œå¯ä»¥è¯„ä¼°å’Œæå‡æ¨¡å‹åœ¨æ¶åŠ£ç¯å¢ƒä¸‹çš„é²æ£’æ€§ï¼Œé™ä½å› ç¡¬ä»¶æ•…éšœå¯¼è‡´çš„ç³»ç»Ÿé£é™©ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¸ºæ„å»ºæ›´å¯é çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›æœ‰åŠ›æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformers have become the foundation for a wide range of state--of--the--art models across natural language processing, computer vision, and other machine learning domains. Despite their widespread deployment, the robustness of these models under fault conditions remains underexplored. We present GoldenTransformer, a modular and extensible fault injection framework designed to evaluate the resiliency of Large Language Models to induced hardware faults. GoldenTransformer offers a unified Python-based platform for injecting diverse classes of faults--such as weight corruption, activation injections, and attention--level disruptions--into pretrained transformer--based models. Inspired by the GoldenEye simulator for DNNs, our framework focuses on the unique challenges of working with large transformer architectures, including considerations such as structural complexity, latent dependencies, and nonuniform layer definitions. GoldenTransformer is built atop PyTorch and HuggingFace Transformers, and it supports experiment reproducibility, metric logging, and visualization out of the box. We detail the technical design and use of GoldenTransformer and demonstrate through several example experiments on classification and generation tasks. By enabling controlled injection of faults at multiple logical and structural points in a transformer, GoldenTransformer offers researchers and practitioners a valuable tool for model robustness analysis and for guiding dependable system design in real-world LLM applications.

