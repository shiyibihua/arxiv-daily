---
layout: default
title: Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization
---

# Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.02767" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.02767v1</a>
  <a href="https://arxiv.org/pdf/2506.02767.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.02767v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.02767v1', 'Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Marco CalÃ¬, Giulio Giacomuzzo, Ruggero Carli, Alberto Dalla Libera

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEB-MC-PILCOä»¥åŠ é€Ÿæ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ çš„æ”¶æ•›é€Ÿåº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ ` `è½¨è¿¹ä¼˜åŒ–` `MC-PILCO` `iLQR` `ç­–ç•¥ä¼˜åŒ–` `æœºå™¨äººæ§åˆ¶` `æ™ºèƒ½å†³ç­–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„MC-PILCOç®—æ³•åœ¨ç­–ç•¥ä¼˜åŒ–çš„æ”¶æ•›é€Ÿåº¦ä¸Šè¡¨ç°è¾ƒæ…¢ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡ã€‚
2. è®ºæ–‡æå‡ºçš„EB-MC-PILCOæ–¹æ³•é€šè¿‡ç»“åˆiLQRå¿«é€Ÿç”Ÿæˆæ¢ç´¢æ€§è½¨è¿¹ï¼Œä¼˜åŒ–äº†ç­–ç•¥åˆå§‹åŒ–è¿‡ç¨‹ï¼Œä»è€ŒåŠ é€Ÿäº†æ”¶æ•›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEB-MC-PILCOåœ¨å°è½¦å€’ç«‹æ‘†ä»»åŠ¡ä¸­ç›¸æ¯”æ ‡å‡†MC-PILCOæ˜¾è‘—æå‡äº†æ”¶æ•›é€Ÿåº¦å’ŒæˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹ç°æœ‰çš„æ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ ç®—æ³•MC-PILCOåœ¨ç­–ç•¥ä¼˜åŒ–æ”¶æ•›é€Ÿåº¦ä¸Šçš„ä¸è¶³ï¼Œé€šè¿‡å°†å…¶ä¸é€‚ç”¨äºéçº¿æ€§ç³»ç»Ÿçš„å¿«é€Ÿè½¨è¿¹ä¼˜åŒ–æ–¹æ³•iLQRç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ¢ç´¢å¢å¼ºMC-PILCOï¼ˆEB-MC-PILCOï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨iLQRç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„æ¢ç´¢è½¨è¿¹å¹¶åˆå§‹åŒ–ç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„ä¼˜åŒ–æ­¥éª¤ã€‚åœ¨å°è½¦å€’ç«‹æ‘†ä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒEB-MC-PILCOç›¸æ¯”æ ‡å‡†MC-PILCOåŠ å¿«äº†æ”¶æ•›é€Ÿåº¦ï¼Œåœ¨å››æ¬¡è¯•éªŒä¸­å®ç°äº†æ‰§è¡Œæ—¶é—´å‡å°‘é«˜è¾¾45.9%çš„æ•ˆæœï¼ŒåŒæ—¶åœ¨æ‰€æœ‰è¯•éªŒä¸­ä¿æŒäº†100%çš„æˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³MC-PILCOç®—æ³•åœ¨ç­–ç•¥ä¼˜åŒ–æ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œå¾€å¾€éœ€è¦è¾ƒå¤šçš„ä¼˜åŒ–æ­¥éª¤ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„EB-MC-PILCOæ–¹æ³•é€šè¿‡å¼•å…¥iLQRç®—æ³•ï¼Œå¿«é€Ÿç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„è½¨è¿¹ï¼Œä½œä¸ºç­–ç•¥ä¼˜åŒ–çš„åˆå§‹ç‚¹ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›è¿‡ç¨‹ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ç®—æ³•èƒ½å¤Ÿåœ¨è¾ƒå°‘çš„è¿­ä»£ä¸­è¾¾åˆ°æ›´ä¼˜çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEB-MC-PILCOçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨iLQRç”Ÿæˆæ¢ç´¢æ€§è½¨è¿¹ï¼›å…¶æ¬¡ï¼ŒåŸºäºè¿™äº›è½¨è¿¹åˆå§‹åŒ–MC-PILCOçš„ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£ä¼˜åŒ–æ¥ä¸æ–­æ”¹è¿›ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šEB-MC-PILCOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†iLQRä¸MC-PILCOç»“åˆï¼Œåˆ©ç”¨å¿«é€Ÿè½¨è¿¹ä¼˜åŒ–æ¥æå‡ç­–ç•¥åˆå§‹åŒ–çš„è´¨é‡ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»ŸMC-PILCOçš„è¿­ä»£ä¼˜åŒ–æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒiLQRçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©è‡³å…³é‡è¦ï¼Œç¡®ä¿ç”Ÿæˆçš„è½¨è¿¹æ—¢å…·æœ‰æ¢ç´¢æ€§åˆèƒ½æœ‰æ•ˆå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„çš„è®¾è®¡ä¹Ÿéœ€è€ƒè™‘åˆ°éçº¿æ€§ç³»ç»Ÿçš„ç‰¹æ€§ï¼Œä»¥æå‡æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEB-MC-PILCOåœ¨å°è½¦å€’ç«‹æ‘†ä»»åŠ¡ä¸­ç›¸æ¯”æ ‡å‡†MC-PILCOå®ç°äº†é«˜è¾¾45.9%çš„æ‰§è¡Œæ—¶é—´å‡å°‘ï¼ŒåŒæ—¶åœ¨æ‰€æœ‰è¯•éªŒä¸­ä¿æŒäº†100%çš„æˆåŠŸç‡ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨åŠ é€Ÿæ”¶æ•›å’Œæé«˜æˆåŠŸç‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰éœ€è¦é«˜æ•ˆå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡åŠ é€Ÿæ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ çš„æ”¶æ•›é€Ÿåº¦ï¼ŒEB-MC-PILCOèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´å¿«çš„ç­–ç•¥ä¼˜åŒ–ï¼Œæå‡ç³»ç»Ÿçš„å“åº”èƒ½åŠ›å’Œæ™ºèƒ½æ°´å¹³ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper addresses the slow policy optimization convergence of Monte Carlo Probabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art model-based reinforcement learning (MBRL) algorithm, by integrating it with iterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization method suitable for nonlinear systems. The proposed method, Exploration-Boosted MC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory trajectories and initialize the policy, significantly reducing the number of required optimization steps. Experiments on the cart-pole task demonstrate that EB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up to $\bm{45.9\% }$ reduction in execution time when both methods solve the task in four trials. EB-MC-PILCO also maintains a $\bm{100\% }$ success rate across trials while solving the task faster, even in cases where MC-PILCO converges in fewer iterations.

