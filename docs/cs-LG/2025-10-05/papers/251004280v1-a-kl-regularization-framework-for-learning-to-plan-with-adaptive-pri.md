---
layout: default
title: A KL-regularization framework for learning to plan with adaptive priors
---

# A KL-regularization framework for learning to plan with adaptive priors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04280" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.04280v1</a>
  <a href="https://arxiv.org/pdf/2510.04280.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04280v1" onclick="toggleFavorite(this, '2510.04280v1', 'A KL-regularization framework for learning to plan with adaptive priors')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: √Ålvaro Serra-Gomez, Daniel Jarne Ornia, Dhruva Tirumala, Thomas Moerland

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-05

**Â§áÊ≥®**: Preprint

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PO-MPCÊ°ÜÊû∂ÔºåÈÄöËøáKLÊ≠£ÂàôÂåñÂ≠¶‰π†Ëá™ÈÄÇÂ∫îÂÖàÈ™åÁöÑËßÑÂàíÁ≠ñÁï•ÔºåÊèêÂçáMBRLÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ê®°ÂûãÈ¢ÑÊµãÊéßÂà∂` `Âº∫ÂåñÂ≠¶‰π†` `KLÊ≠£ÂàôÂåñ` `Á≠ñÁï•‰ºòÂåñ` `ËøûÁª≠ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê®°ÂûãÈ¢ÑÊµãÊéßÂà∂(MPC)‰∏≠ÁöÑÊé¢Á¥¢‰∏çË∂≥ÊòØÈ´òÁª¥ËøûÁª≠ÊéßÂà∂‰ªªÂä°‰∏≠ÁöÑÂÖ≥ÈîÆÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê†∑Êú¨ÊïàÁéáË¶ÅÊ±ÇÈ´òÁöÑÂú∫ÊôØ‰∏ã„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Policy Optimization-Model Predictive Control (PO-MPC)Ê°ÜÊû∂ÔºåÈÄöËøáKLÊ≠£ÂàôÂåñÂ∞ÜËßÑÂàíÂô®ÁöÑÂä®‰ΩúÂàÜÂ∏É‰Ωú‰∏∫Á≠ñÁï•‰ºòÂåñ‰∏≠ÁöÑÂÖàÈ™å„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPO-MPCÊ°ÜÊû∂ÂèäÂÖ∂Âèò‰ΩìÂú®MPPI-based RL‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåËææÂà∞‰∫ÜÂΩìÂâçÊúÄ‰ºòÊ∞¥Âπ≥„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®Âü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†(MBRL)‰∏≠ÔºåÊúâÊïàÁöÑÊé¢Á¥¢‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Ê†∏ÂøÉÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê†∑Êú¨ÊïàÁéáËá≥ÂÖ≥ÈáçË¶ÅÁöÑÈ´òÁª¥ËøûÁª≠ÊéßÂà∂‰ªªÂä°‰∏≠„ÄÇÊúÄËøëÁöÑ‰∏ÄÈ°πÈáçË¶ÅÂ∑•‰ΩúÂà©Áî®Â≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•‰Ωú‰∏∫Ê®°ÂûãÈ¢ÑÊµãË∑ØÂæÑÁßØÂàÜ(MPPI)ËßÑÂàíÁöÑÊèêËÆÆÂàÜÂ∏É„ÄÇÊúÄÂàùÁöÑÊñπÊ≥ïÁã¨Á´ã‰∫éËßÑÂàíÂô®ÂàÜÂ∏ÉÊõ¥Êñ∞ÈááÊ†∑Á≠ñÁï•ÔºåÈÄöÂ∏∏ÈÄöËøáÁ°ÆÂÆöÊÄßÁ≠ñÁï•Ê¢ØÂ∫¶ÂíåÁÜµÊ≠£ÂàôÂåñÊúÄÂ§ßÂåñÂ≠¶‰π†Âà∞ÁöÑ‰ª∑ÂÄºÂáΩÊï∞„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éËÆ≠ÁªÉÊúüÈó¥ÈÅáÂà∞ÁöÑÁä∂ÊÄÅÂèñÂÜ≥‰∫éMPPIËßÑÂàíÂô®ÔºåÂõ†Ê≠§Â∞ÜÈááÊ†∑Á≠ñÁï•‰∏éËßÑÂàíÂô®ÂØπÈΩêÂèØ‰ª•ÊèêÈ´ò‰ª∑ÂÄº‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÈïøÊúüÊÄßËÉΩ„ÄÇ‰∏∫Ê≠§ÔºåÊúÄËøëÁöÑÊñπÊ≥ïÈÄöËøáÊúÄÂ∞èÂåñ‰∏éËßÑÂàíÂô®ÂàÜÂ∏ÉÁöÑKLÊï£Â∫¶ÊàñÂ∞ÜËßÑÂàíÂô®ÂºïÂØºÁöÑÊ≠£ÂàôÂåñÂºïÂÖ•Á≠ñÁï•Êõ¥Êñ∞Êù•Êõ¥Êñ∞ÈááÊ†∑Á≠ñÁï•„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÈÄöËøáÂºïÂÖ•Á≠ñÁï•‰ºòÂåñ-Ê®°ÂûãÈ¢ÑÊµãÊéßÂà∂(PO-MPC)Áªü‰∏Ä‰∫ÜËøô‰∫õÂü∫‰∫éMPPIÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåPO-MPCÊòØ‰∏ÄÁ≥ªÂàóKLÊ≠£ÂàôÂåñÁöÑMBRLÊñπÊ≥ïÔºåÂÆÉÂ∞ÜËßÑÂàíÂô®ÁöÑÂä®‰ΩúÂàÜÂ∏É‰Ωú‰∏∫Á≠ñÁï•‰ºòÂåñ‰∏≠ÁöÑÂÖàÈ™å„ÄÇÈÄöËøáÂ∞ÜÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•‰∏éËßÑÂàíÂô®ÁöÑË°å‰∏∫ÂØπÈΩêÔºåPO-MPCÂÖÅËÆ∏Á≠ñÁï•Êõ¥Êñ∞‰∏≠Êõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄßÔºå‰ª•ÊùÉË°°ÂõûÊä•ÊúÄÂ§ßÂåñÂíåKLÊï£Â∫¶ÊúÄÂ∞èÂåñ„ÄÇÊàë‰ª¨ÈòêÊòé‰∫ÜÂÖàÂâçÁöÑÊñπÊ≥ïÂ¶Ç‰Ωï‰Ωú‰∏∫ËØ•Á≥ªÂàóÁöÑÁâπ‰æãÂá∫Áé∞ÔºåÂπ∂Êé¢Á¥¢‰∫Ü‰ª•ÂâçÊú™Á†îÁ©∂ÁöÑÂèò‰Ωì„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåËøô‰∫õÊâ©Â±ïÁöÑÈÖçÁΩÆ‰∫ßÁîü‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊîπËøõÔºå‰ªéËÄåÊé®Ëøõ‰∫ÜÂü∫‰∫éMPPIÁöÑRLÁöÑÊúÄÊñ∞ÊäÄÊúØ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂü∫‰∫éMPPIÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂú®Êõ¥Êñ∞ÈááÊ†∑Á≠ñÁï•Êó∂ÔºåÈÄöÂ∏∏Áã¨Á´ã‰∫éËßÑÂàíÂô®ÂàÜÂ∏ÉÔºåÊàñËÄÖ‰ªÖÈÄöËøáÊúÄÂ∞èÂåñKLÊï£Â∫¶ÊàñÂºïÂÖ•ËßÑÂàíÂô®ÂºïÂØºÁöÑÊ≠£ÂàôÂåñËøõË°åÂØπÈΩê„ÄÇËøô‰∫õÊñπÊ≥ïÁº∫‰πèÁÅµÊ¥ªÊÄßÔºåÊó†Ê≥ïÊúâÊïàÊùÉË°°ÂõûÊä•ÊúÄÂ§ßÂåñÂíåKLÊï£Â∫¶ÊúÄÂ∞èÂåñÔºåÂØºËá¥Êé¢Á¥¢ÊïàÁéá‰Ωé‰∏ãÂíåÊÄßËÉΩÁì∂È¢à„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËßÑÂàíÂô®ÁöÑÂä®‰ΩúÂàÜÂ∏É‰Ωú‰∏∫Á≠ñÁï•‰ºòÂåñ‰∏≠ÁöÑÂÖàÈ™åÔºåÈÄöËøáKLÊ≠£ÂàôÂåñÊù•Á∫¶ÊùüÁ≠ñÁï•Êõ¥Êñ∞„ÄÇËøôÊ†∑ÂèØ‰ª•Êõ¥ÁÅµÊ¥ªÂú∞ÊéßÂà∂Á≠ñÁï•‰∏éËßÑÂàíÂô®Ë°å‰∏∫ÁöÑÂØπÈΩêÁ®ãÂ∫¶Ôºå‰ªéËÄåÂú®ÂõûÊä•ÊúÄÂ§ßÂåñÂíåÊé¢Á¥¢‰πãÈó¥ÂèñÂæóÊõ¥Â•ΩÁöÑÂπ≥Ë°°„ÄÇËøôÁßçÊñπÊ≥ïÂÖÅËÆ∏Á≠ñÁï•Âú®ËßÑÂàíÂô®ÊåáÂØº‰∏ãËøõË°åÊé¢Á¥¢ÔºåÊèêÈ´òÊ†∑Êú¨ÊïàÁéáÂíåÈïøÊúüÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPO-MPCÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÁéØÂ¢ÉÊ®°ÂûãÔºöÁî®‰∫éÈ¢ÑÊµãÁä∂ÊÄÅËΩ¨ÁßªÔºõ2) MPPIËßÑÂàíÂô®ÔºöÂà©Áî®ÁéØÂ¢ÉÊ®°ÂûãÁîüÊàêÂä®‰ΩúÂàÜÂ∏ÉÔºõ3) Á≠ñÁï•ÁΩëÁªúÔºöÂ≠¶‰π†‰∏Ä‰∏™Á≠ñÁï•ÔºåÁî®‰∫éÈááÊ†∑Âä®‰ΩúÔºõ4) ‰ª∑ÂÄºÂáΩÊï∞ÔºöËØÑ‰º∞Áä∂ÊÄÅÁöÑ‰ª∑ÂÄº„ÄÇÊ°ÜÊû∂ÁöÑÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöÈ¶ñÂÖàÔºåMPPIËßÑÂàíÂô®Âü∫‰∫éÂΩìÂâçÁ≠ñÁï•ÁîüÊàêÂä®‰ΩúÂàÜÂ∏ÉÔºõÁÑ∂ÂêéÔºåÁ≠ñÁï•ÁΩëÁªúÈÄöËøáKLÊ≠£ÂàôÂåñÔºåÂ∞ÜËßÑÂàíÂô®ÁöÑÂä®‰ΩúÂàÜÂ∏É‰Ωú‰∏∫ÂÖàÈ™åËøõË°åÊõ¥Êñ∞ÔºõÊúÄÂêéÔºå‰ª∑ÂÄºÂáΩÊï∞Áî®‰∫éËØÑ‰º∞Á≠ñÁï•ÁöÑÊÄßËÉΩÔºåÂπ∂ÊåáÂØºÁ≠ñÁï•ÁöÑËøõ‰∏ÄÊ≠•‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPO-MPCÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜËßÑÂàíÂô®ÁöÑÂä®‰ΩúÂàÜÂ∏ÉÊòæÂºèÂú∞Á∫≥ÂÖ•Á≠ñÁï•‰ºòÂåñËøáÁ®ã‰∏≠ÔºåÂπ∂ÈÄöËøáKLÊ≠£ÂàôÂåñÊù•ÊéßÂà∂Á≠ñÁï•‰∏éËßÑÂàíÂô®Ë°å‰∏∫ÁöÑÂØπÈΩêÁ®ãÂ∫¶„ÄÇ‰∏é‰ª•ÂæÄÊñπÊ≥ïÁõ∏ÊØîÔºåPO-MPCÊèê‰æõ‰∫Ü‰∏Ä‰∏™Êõ¥ÈÄöÁî®ÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•ÁÅµÊ¥ªÂú∞Ë∞ÉÊï¥Á≠ñÁï•Êõ¥Êñ∞Ôºå‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑ‰ªªÂä°ÂíåÁéØÂ¢É„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊé¢Á¥¢‰∫ÜPO-MPCÊ°ÜÊû∂‰∏ãÊú™Ë¢´ÂÖÖÂàÜÁ†îÁ©∂ÁöÑÂèò‰ΩìÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöPO-MPCÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) KLÊ≠£ÂàôÂåñÈ°πÔºöÁî®‰∫éÁ∫¶ÊùüÁ≠ñÁï•‰∏éËßÑÂàíÂô®ÂàÜÂ∏ÉÁöÑÂ∑ÆÂºÇÔºåÈò≤Ê≠¢Á≠ñÁï•ÂÅèÁ¶ªËßÑÂàíÂô®ÁöÑÊåáÂØºÔºõ2) Ê≠£ÂàôÂåñÁ≥ªÊï∞ÔºöÁî®‰∫éÊéßÂà∂KLÊ≠£ÂàôÂåñÈ°πÁöÑÂº∫Â∫¶ÔºåÂπ≥Ë°°ÂõûÊä•ÊúÄÂ§ßÂåñÂíåÊé¢Á¥¢Ôºõ3) Á≠ñÁï•ÁΩëÁªúÁªìÊûÑÔºöÂèØ‰ª•‰ΩøÁî®ÂêÑÁßçÁ•ûÁªèÁΩëÁªúÁªìÊûÑÔºåÂ¶ÇMLPÊàñRNNÔºåÊù•Ë°®Á§∫Á≠ñÁï•Ôºõ4) ÊçüÂ§±ÂáΩÊï∞ÔºöÂåÖÂê´ÂõûÊä•ÊúÄÂ§ßÂåñÈ°πÂíåKLÊ≠£ÂàôÂåñÈ°πÔºåÁî®‰∫é‰ºòÂåñÁ≠ñÁï•ÁΩëÁªú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPO-MPCÊ°ÜÊû∂ÂèäÂÖ∂Âèò‰ΩìÂú®Â§ö‰∏™ËøûÁª≠ÊéßÂà∂‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂü∫‰∫éMPPIÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊüê‰∫õPO-MPCÈÖçÁΩÆÂú®ÁâπÂÆö‰ªªÂä°‰∏äÂÆûÁé∞‰∫ÜË∂ÖËøá20%ÁöÑÊÄßËÉΩÊèêÂçáÔºåËØÅÊòé‰∫ÜËØ•Ê°ÜÊû∂ÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PO-MPCÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈ´òÁª¥ËøûÁª≠ÊéßÂà∂‰ªªÂä°Ôºå‰æãÂ¶ÇÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊ∏∏ÊàèAIÁ≠â„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊèêÈ´òÊ†∑Êú¨ÊïàÁéáÂíåÈïøÊúüÊÄßËÉΩÔºåÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨ÔºåÂπ∂ÊúâÊúõÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥Êô∫ËÉΩÁöÑÂÜ≥Á≠ñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Effective exploration remains a central challenge in model-based reinforcement learning (MBRL), particularly in high-dimensional continuous control tasks where sample efficiency is crucial. A prominent line of recent work leverages learned policies as proposal distributions for Model-Predictive Path Integral (MPPI) planning. Initial approaches update the sampling policy independently of the planner distribution, typically maximizing a learned value function with deterministic policy gradient and entropy regularization. However, because the states encountered during training depend on the MPPI planner, aligning the sampling policy with the planner improves the accuracy of value estimation and long-term performance. To this end, recent methods update the sampling policy by minimizing KL divergence to the planner distribution or by introducing planner-guided regularization into the policy update. In this work, we unify these MPPI-based reinforcement learning methods under a single framework by introducing Policy Optimization-Model Predictive Control (PO-MPC), a family of KL-regularized MBRL methods that integrate the planner's action distribution as a prior in policy optimization. By aligning the learned policy with the planner's behavior, PO-MPC allows more flexibility in the policy updates to trade off Return maximization and KL divergence minimization. We clarify how prior approaches emerge as special cases of this family, and we explore previously unstudied variations. Our experiments show that these extended configurations yield significant performance improvements, advancing the state of the art in MPPI-based RL.

