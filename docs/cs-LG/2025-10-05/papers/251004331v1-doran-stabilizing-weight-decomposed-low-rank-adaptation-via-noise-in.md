---
layout: default
title: DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks
---

# DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04331" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.04331v1</a>
  <a href="https://arxiv.org/pdf/2510.04331.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04331v1" onclick="toggleFavorite(this, '2510.04331v1', 'DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nghiem T. Diep, Hien Dang, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-05

**å¤‡æ³¨**: Nghiem T. Diep, Hien Dang, and Tuan Truong contributed equally to this work

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DoRANï¼šé€šè¿‡å™ªå£°æ³¨å…¥å’Œè¾…åŠ©ç½‘ç»œç¨³å®šæƒé‡åˆ†è§£ä½ç§©é€‚åº”**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ä½ç§©é€‚åº”` `æƒé‡åˆ†è§£` `å™ªå£°æ³¨å…¥` `è¾…åŠ©ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LoRAæ–¹æ³•åœ¨å¾®è°ƒå¤§å‹æ¨¡å‹æ—¶å­˜åœ¨è®­ç»ƒä¸ç¨³å®šå’Œæ ·æœ¬æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚
2. DoRANé€šè¿‡å™ªå£°æ³¨å…¥å’Œè¾…åŠ©ç½‘ç»œï¼Œè‡ªé€‚åº”åœ°æ­£åˆ™åŒ–æƒé‡åˆ†è§£è¿‡ç¨‹ï¼Œå¹¶å®ç°è·¨å±‚å‚æ•°è€¦åˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDoRANåœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºLoRAã€DoRAç­‰åŸºçº¿æ–¹æ³•ï¼Œæå‡äº†è®­ç»ƒç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æ–¹æ³•å·²æˆä¸ºè°ƒæ•´å¤§è§„æ¨¡æ¨¡å‹çš„æ ‡å‡†èŒƒå¼ã€‚åœ¨è¿™äº›æŠ€æœ¯ä¸­ï¼Œæƒé‡åˆ†è§£ä½ç§©é€‚åº”(DoRA)é€šè¿‡å°†é¢„è®­ç»ƒæƒé‡æ˜¾å¼åˆ†è§£ä¸ºå¹…å€¼å’Œæ–¹å‘åˆ†é‡ï¼Œå·²è¢«è¯æ˜å¯ä»¥æé«˜åŸå§‹ä½ç§©é€‚åº”(LoRA)æ–¹æ³•çš„å­¦ä¹ èƒ½åŠ›å’Œè®­ç»ƒç¨³å®šæ€§ã€‚æœ¬æ–‡æå‡ºDoRANï¼Œä¸€ç§DoRAçš„æ–°å˜ä½“ï¼Œæ—¨åœ¨è¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒå¹¶æé«˜DoRAçš„æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼š(i)å°†å™ªå£°æ³¨å…¥åˆ°DoRAæƒé‡åˆ†è§£çš„åˆ†æ¯ä¸­ï¼Œä½œä¸ºè‡ªé€‚åº”æ­£åˆ™åŒ–å™¨ä»¥å‡è½»ä¸ç¨³å®šæ€§ï¼›(ii)ç”¨åŠ¨æ€ç”Ÿæˆä½ç§©çŸ©é˜µçš„è¾…åŠ©ç½‘ç»œæ›¿æ¢é™æ€ä½ç§©çŸ©é˜µï¼Œä»è€Œå®ç°è·¨å±‚çš„å‚æ•°è€¦åˆï¼Œå¹¶åœ¨ç†è®ºå’Œå®è·µä¸­äº§ç”Ÿæ›´å¥½çš„æ ·æœ¬æ•ˆç‡ã€‚åœ¨è§†è§‰å’Œè¯­è¨€åŸºå‡†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDoRANå§‹ç»ˆä¼˜äºLoRAã€DoRAå’Œå…¶ä»–PEFTåŸºçº¿ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†é€šè¿‡åŸºäºå™ªå£°çš„æ­£åˆ™åŒ–è¿›è¡Œç¨³å®šä¸åŸºäºç½‘ç»œçš„å‚æ•°ç”Ÿæˆç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºåŸºç¡€æ¨¡å‹çš„ç¨³å¥å’Œé«˜æ•ˆå¾®è°ƒæä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šDoRANæ—¨åœ¨è§£å†³æƒé‡åˆ†è§£ä½ç§©é€‚åº”ï¼ˆDoRAï¼‰åœ¨å¾®è°ƒå¤§å‹æ¨¡å‹æ—¶ä»ç„¶å­˜åœ¨çš„è®­ç»ƒä¸ç¨³å®šå’Œæ ·æœ¬æ•ˆç‡ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„DoRAæ–¹æ³•è™½ç„¶é€šè¿‡åˆ†è§£æƒé‡ä¸ºå¹…å€¼å’Œæ–¹å‘åˆ†é‡æ¥æ”¹å–„LoRAï¼Œä½†ä»ç„¶å¯èƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±ç­‰é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDoRANçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸¤ä¸ªå…³é”®æœºåˆ¶æ¥ç¨³å®šè®­ç»ƒå¹¶æé«˜æ ·æœ¬æ•ˆç‡ï¼šä¸€æ˜¯é€šè¿‡åœ¨DoRAçš„æƒé‡åˆ†è§£è¿‡ç¨‹ä¸­æ³¨å…¥å™ªå£°ï¼Œå®ç°è‡ªé€‚åº”æ­£åˆ™åŒ–ï¼Œä»è€ŒæŠ‘åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ï¼›äºŒæ˜¯ä½¿ç”¨è¾…åŠ©ç½‘ç»œåŠ¨æ€ç”Ÿæˆä½ç§©çŸ©é˜µï¼Œå–ä»£é™æ€çš„ä½ç§©çŸ©é˜µï¼Œä»è€Œå®ç°è·¨å±‚çš„å‚æ•°è€¦åˆï¼Œæé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDoRANçš„æ•´ä½“æ¡†æ¶åŸºäºDoRAï¼Œä¸»è¦åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šå™ªå£°æ³¨å…¥æ¨¡å—å’Œè¾…åŠ©ç½‘ç»œæ¨¡å—ã€‚å™ªå£°æ³¨å…¥æ¨¡å—åœ¨DoRAçš„æƒé‡åˆ†è§£å…¬å¼çš„åˆ†æ¯ä¸­åŠ å…¥å™ªå£°ï¼Œèµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨ã€‚è¾…åŠ©ç½‘ç»œæ¨¡å—åˆ™ç”±å¤šä¸ªå°å‹ç¥ç»ç½‘ç»œç»„æˆï¼Œç”¨äºåŠ¨æ€ç”Ÿæˆä½ç§©çŸ©é˜µã€‚è®­ç»ƒæ—¶ï¼ŒåŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ä¿æŒä¸å˜ï¼Œåªè®­ç»ƒå™ªå£°æ³¨å…¥çš„å‚æ•°å’Œè¾…åŠ©ç½‘ç»œçš„å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šDoRANçš„å…³é”®åˆ›æ–°åœ¨äºå°†å™ªå£°æ³¨å…¥å’Œè¾…åŠ©ç½‘ç»œç›¸ç»“åˆï¼Œå…±åŒä½œç”¨äºDoRAçš„æƒé‡åˆ†è§£è¿‡ç¨‹ã€‚å™ªå£°æ³¨å…¥æä¾›äº†ä¸€ç§è‡ªé€‚åº”çš„æ­£åˆ™åŒ–æ–¹å¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æŠ‘åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ã€‚è¾…åŠ©ç½‘ç»œåˆ™é€šè¿‡åŠ¨æ€ç”Ÿæˆä½ç§©çŸ©é˜µï¼Œå®ç°äº†è·¨å±‚çš„å‚æ•°è€¦åˆï¼Œæé«˜äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ã€‚è¿™ç§ç»“åˆæ–¹å¼ä½¿å¾—DoRANèƒ½å¤Ÿæ›´ç¨³å®šã€æ›´é«˜æ•ˆåœ°å¾®è°ƒå¤§å‹æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å™ªå£°æ³¨å…¥æ¨¡å—ä¸­ï¼Œå™ªå£°çš„æ–¹å·®æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ï¼Œå¯ä»¥æ ¹æ®è®­ç»ƒæ•°æ®çš„ç‰¹ç‚¹è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚åœ¨è¾…åŠ©ç½‘ç»œæ¨¡å—ä¸­ï¼Œé‡‡ç”¨äº†å°å‹å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œå…¶è¾“å…¥å¯ä»¥æ˜¯å±‚ç´¢å¼•æˆ–å…¶ä»–ä¸å±‚ç›¸å…³çš„ä¿¡æ¯ï¼Œè¾“å‡ºåˆ™æ˜¯åŠ¨æ€ç”Ÿæˆçš„ä½ç§©çŸ©é˜µã€‚æŸå¤±å‡½æ•°é™¤äº†åŒ…æ‹¬å¸¸è§„çš„å¾®è°ƒæŸå¤±å¤–ï¼Œè¿˜å¯ä»¥åŠ å…¥æ­£åˆ™åŒ–é¡¹ï¼Œä»¥çº¦æŸè¾…åŠ©ç½‘ç»œçš„è¾“å‡ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DoRANåœ¨å¤šä¸ªè§†è§‰å’Œè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼ŒDoRANç›¸æ¯”äºDoRAå’ŒLoRAï¼Œåœ¨ç›¸åŒå‚æ•°é‡ä¸‹ï¼ŒTop-1å‡†ç¡®ç‡æå‡äº†0.5%-1.0%ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šï¼ŒDoRANåœ¨GLUEåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå–å¾—äº†ç±»ä¼¼çš„æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDoRANèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å‚æ•°ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DoRANé€‚ç”¨äºå„ç§éœ€è¦é«˜æ•ˆå¾®è°ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ï¼Œä»¥åŠè®¡ç®—æœºè§†è§‰ä¸­çš„å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥é™ä½å¾®è°ƒæˆæœ¬ï¼Œæé«˜æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„éƒ¨ç½²èƒ½åŠ›ï¼Œå¹¶åŠ é€Ÿæ–°ä»»åŠ¡çš„é€‚åº”è¿‡ç¨‹ã€‚æœªæ¥ï¼ŒDoRANæœ‰æœ›åº”ç”¨äºæ›´å¤šé¢†åŸŸï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€æ¨èç³»ç»Ÿç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the vanilla Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new variant of DoRA designed to further stabilize training and boost the sample efficiency of DoRA. Our approach includes two key stages: (i) injecting noise into the denominator of DoRA's weight decomposition, which serves as an adaptive regularizer to mitigate instabilities; and (ii) replacing static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and yielding better sample efficiency in both theory and practice. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These results underscore the effectiveness of combining stabilization through noise-based regularization with network-based parameter generation, offering a promising direction for robust and efficient fine-tuning of foundation models.

