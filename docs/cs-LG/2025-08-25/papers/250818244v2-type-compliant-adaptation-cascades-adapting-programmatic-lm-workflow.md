---
layout: default
title: Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data
---

# Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18244" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18244v2</a>
  <a href="https://arxiv.org/pdf/2508.18244.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18244v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18244v2', 'Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chu-Cheng Lin, Daiyi Peng, Yifeng Lu, Ming Zhang, Eugene Ie

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25 (æ›´æ–°: 2025-09-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç±»å‹å…¼å®¹é€‚åº”çº§è”ä»¥è§£å†³å¤æ‚å·¥ä½œæµé€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å·¥ä½œæµé€‚åº”` `ç±»å‹æ¦‚ç‡ç¨‹åº` `ç»“æ„åŒ–ä»»åŠ¡` `ä¼˜åŒ–ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚å·¥ä½œæµä¸­ç»„åˆå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¸¸å¸¸é¢ä¸´è„†å¼±æ€§å’Œåˆè§„æ€§ä¸è¶³çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„TACsæ¡†æ¶é€šè¿‡å­¦ä¹ ç±»å‹æ¦‚ç‡ç¨‹åºæ¥å®ç°å·¥ä½œæµé€‚åº”ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œåˆè§„æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTACsåœ¨å¤šä¸ªç»“æ„åŒ–ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡æ€§èƒ½ï¼Œä¾‹å¦‚FinQAä»12.0%æå‡è‡³24.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤æ‚çš„å¤šæ­¥éª¤å·¥ä½œæµä¸­ï¼Œå¯é åœ°ç»„åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„ä¸»æµæ–¹æ³•â€”â€”åœ¨ç®¡é“ä¸­ä¼˜åŒ–ç¦»æ•£æç¤ºâ€”â€”å¸¸å¸¸è„†å¼±ï¼Œéš¾ä»¥å¼ºåˆ¶æ‰§è¡Œç»“æ„åŒ–ä»»åŠ¡æ‰€éœ€çš„æ­£å¼åˆè§„æ€§ã€‚æœ¬æ–‡æå‡ºäº†ç±»å‹å…¼å®¹é€‚åº”çº§è”ï¼ˆTACsï¼‰æ¡†æ¶ï¼Œå°†å·¥ä½œæµé€‚åº”é‡æ–°å®šä¹‰ä¸ºå­¦ä¹ ç±»å‹æ¦‚ç‡ç¨‹åºã€‚TACså°†æ•´ä¸ªå·¥ä½œæµè§†ä¸ºæœªå½’ä¸€åŒ–çš„è”åˆåˆ†å¸ƒï¼Œä»è€Œå®ç°äº†åŸºäºæ¢¯åº¦çš„åŸåˆ™æ€§è®­ç»ƒï¼Œå³ä½¿åœ¨æ½œåœ¨çš„ä¸­é—´ç»“æ„ä¸‹ä¹Ÿèƒ½æœ‰æ•ˆè¿›è¡Œã€‚æˆ‘ä»¬ä¸ºå¯å¤„ç†çš„ä¼˜åŒ–ç›®æ ‡æä¾›äº†ç†è®ºä¾æ®ï¼Œè¯æ˜äº†éšç€æ¨¡å‹å­¦ä¹ ç±»å‹åˆè§„æ€§ï¼Œä¼˜åŒ–åå·®ä¼šæ¶ˆå¤±ã€‚å®éªŒè¯æ˜ï¼ŒTACsåœ¨ç»“æ„åŒ–ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æç¤ºä¼˜åŒ–åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤æ‚å¤šæ­¥éª¤å·¥ä½œæµä¸­ï¼Œå¦‚ä½•å¯é åœ°ç»„åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¼˜åŒ–ç¦»æ•£æç¤ºæ—¶è¡¨ç°è„†å¼±ï¼Œéš¾ä»¥æ»¡è¶³ç»“æ„åŒ–ä»»åŠ¡çš„åˆè§„æ€§è¦æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTACsæ¡†æ¶é€šè¿‡å°†å·¥ä½œæµé€‚åº”è§†ä¸ºå­¦ä¹ ç±»å‹æ¦‚ç‡ç¨‹åºï¼Œé‡æ–°å®šä¹‰äº†å·¥ä½œæµçš„é€‚åº”è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•å…è®¸åœ¨æ½œåœ¨ä¸­é—´ç»“æ„ä¸‹è¿›è¡ŒåŸºäºæ¢¯åº¦çš„è®­ç»ƒï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œåˆè§„æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTACsçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‚æ•°é«˜æ•ˆé€‚åº”çš„LLMså’Œç¡®å®šæ€§é€»è¾‘ï¼Œæ•´ä¸ªå·¥ä½œæµè¢«è§†ä¸ºæœªå½’ä¸€åŒ–çš„è”åˆåˆ†å¸ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å­¦ä¹ ç±»å‹åˆè§„æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šTACsçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å°†å·¥ä½œæµé€‚åº”è½¬åŒ–ä¸ºå­¦ä¹ ç±»å‹æ¦‚ç‡ç¨‹åºçš„èƒ½åŠ›ï¼Œè¿™ä¸ç°æœ‰çš„æç¤ºä¼˜åŒ–æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…å¾€å¾€ä¾èµ–äºå›ºå®šçš„æç¤ºè®¾è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨TACsä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ç¡®ä¿ç±»å‹åˆè§„æ€§ï¼Œå¹¶è®¾è®¡äº†é«˜æ•ˆçš„å‚æ•°è°ƒæ•´æœºåˆ¶ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTACsåœ¨å¤šä¸ªç»“æ„åŒ–ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æç¤ºä¼˜åŒ–åŸºçº¿ã€‚ä¾‹å¦‚ï¼ŒFinQAçš„å‡†ç¡®ç‡ä»12.0%æå‡è‡³24.7%ï¼ŒMGSM-SymPyä»57.1%æå‡è‡³75.9%ï¼ŒMGSMä»1.6%æå‡è‡³27.3%ï¼ŒMuSRä»36.5%æå‡è‡³62.6%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿã€æ™ºèƒ½åŠ©æ‰‹å’Œå¤æ‚æ•°æ®å¤„ç†å·¥ä½œæµç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»“æ„åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒTACsèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´å¯é çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reliably composing Large Language Models (LLMs) for complex, multi-step workflows remains a significant challenge. The dominant paradigm -- optimizing discrete prompts in a pipeline -- is notoriously brittle and struggles to enforce the formal compliance required for structured tasks. We introduce Type-Compliant Adaptation Cascades (TACs), a framework that recasts workflow adaptation as learning typed probabilistic programs. TACs treat the entire workflow, which is composed of parameter-efficiently adapted LLMs and deterministic logic, as an unnormalized joint distribution. This enables principled, gradient-based training even with latent intermediate structures. We provide theoretical justification for our tractable optimization objective, proving that the optimization bias vanishes as the model learns type compliance. Empirically, TACs significantly outperform state-of-the-art prompt-optimization baselines. Gains are particularly pronounced on structured tasks, improving FinQA from $12.0\%$ to $24.7\%$ for a Qwen 3 8B model, MGSM-SymPy from $57.1\%$ to $75.9\%$ for a Gemma 2 27B model, MGSM from $1.6\%$ to $27.3\%$, and MuSR from $36.5\%$ to $62.6\%$ for a Gemma 7B model. TACs offer a robust and theoretically grounded paradigm for developing reliable, task-compliant LLM systems.

