---
layout: default
title: FSA: An Alternative Efficient Implementation of Native Sparse Attention Kernel
---

# FSA: An Alternative Efficient Implementation of Native Sparse Attention Kernel

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18224" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18224v2</a>
  <a href="https://arxiv.org/pdf/2508.18224.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18224v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18224v2', 'FSA: An Alternative Efficient Implementation of Native Sparse Attention Kernel')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ran Yan, Youhe Jiang, Zhuoming Chen, Haohui Mai, Beidi Chen, Binhang Yuan

**åˆ†ç±»**: cs.DC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25 (æ›´æ–°: 2025-10-13)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFlash Sparse Attentionä»¥è§£å†³ç¨€ç–æ³¨æ„åŠ›æ ¸æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–æ³¨æ„åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹` `è®¡ç®—æ•ˆç‡` `æ·±åº¦å­¦ä¹ ` `GPUä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è®­ç»ƒ` `æ¨ç†é€Ÿåº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸç”Ÿç¨€ç–æ³¨æ„åŠ›æ–¹æ³•åœ¨æŸ¥è¯¢å¤´æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹æ•ˆç‡ä½ä¸‹ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºFlash Sparse Attentionï¼ˆFSAï¼‰ï¼Œé€šè¿‡æ”¹è¿›å†…æ ¸å®ç°ï¼Œä½¿å¾—ç¨€ç–æ³¨æ„åŠ›è®¡ç®—åœ¨å¤šç§æµè¡ŒLLMsä¸­æ›´åŠ é«˜æ•ˆã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFSAåœ¨å†…æ ¸çº§å»¶è¿Ÿä¸Šæœ€é«˜å¯å‡å°‘3.5å€ï¼Œç«¯åˆ°ç«¯è®­ç»ƒé€Ÿåº¦æå‡æœ€é«˜è¾¾1.25å€ï¼Œç”Ÿæˆæ¨ç†é€Ÿåº¦æå‡æœ€é«˜è¾¾1.36å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é•¿ä¸Šä¸‹æ–‡è®­ç»ƒå’Œæ¨ç†ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚åŸç”Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆNSAï¼‰ä½œä¸ºä¸€ç§å…ˆè¿›æ–¹æ³•ï¼Œæä¾›äº†ç¡¬ä»¶å¯¹é½çš„ç¨€ç–æ³¨æ„åŠ›ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿæ€§èƒ½ã€‚ç„¶è€Œï¼ŒNSAçš„å†…æ ¸å®ç°å¯¹æŸ¥è¯¢å¤´æ•°é‡çš„é™åˆ¶ä½¿å¾—å…¶åœ¨ç°æœ‰LLMsä¸­çš„åº”ç”¨å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†Flash Sparse Attentionï¼ˆFSAï¼‰ï¼Œä¸€ç§æ–°çš„å†…æ ¸å®ç°ï¼Œèƒ½å¤Ÿåœ¨ç°ä»£GPUä¸Šé«˜æ•ˆè®¡ç®—ï¼Œé€‚ç”¨äºæŸ¥è¯¢å¤´æ•°é‡è¾ƒå°‘çš„å¤šç§æµè¡ŒLLMsã€‚ä¸ä¼ ç»ŸNSAå†…æ ¸ç›¸æ¯”ï¼ŒFSAåœ¨å†…æ ¸çº§å»¶è¿Ÿã€ç«¯åˆ°ç«¯è®­ç»ƒé€Ÿåº¦å’Œç”Ÿæˆæ¨ç†é€Ÿåº¦ä¸Šå‡å®ç°äº†æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸç”Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆNSAï¼‰å†…æ ¸å®ç°å¯¹æŸ¥è¯¢å¤´æ•°é‡çš„é™åˆ¶ï¼Œå¯¼è‡´å…¶åœ¨ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨å—é™ã€‚ç°æœ‰æ–¹æ³•åœ¨æŸ¥è¯¢å¤´æ•°é‡è¾ƒå°‘æ—¶æ•ˆç‡ä½ä¸‹ï¼Œæ— æ³•å……åˆ†å‘æŒ¥ç¨€ç–æ³¨æ„åŠ›çš„ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºFlash Sparse Attentionï¼ˆFSAï¼‰ï¼Œé€šè¿‡ä¼˜åŒ–å†…æ ¸å®ç°ï¼Œæ”¯æŒåœ¨è¾ƒå°‘æŸ¥è¯¢å¤´çš„æƒ…å†µä¸‹é«˜æ•ˆè®¡ç®—ç¨€ç–æ³¨æ„åŠ›ï¼Œä»è€Œæé«˜åœ¨ç°ä»£GPUä¸Šçš„è®¡ç®—æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFSAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯è¾“å…¥æ•°æ®çš„é¢„å¤„ç†ï¼Œç„¶åæ˜¯ç¨€ç–æ³¨æ„åŠ›è®¡ç®—æ¨¡å—ï¼Œæœ€åæ˜¯è¾“å‡ºç»“æœçš„åå¤„ç†ã€‚è¯¥æ¡†æ¶è®¾è®¡æ—¨åœ¨å…¼å®¹å¤šç§æµè¡Œçš„LLMsï¼Œå¹¶ä¼˜åŒ–è®¡ç®—è·¯å¾„ä»¥æé«˜æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šFSAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å†…æ ¸å®ç°çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒæ•°é‡çš„æŸ¥è¯¢å¤´ï¼Œä»è€Œå…‹æœäº†NSAåœ¨æŸ¥è¯¢å¤´æ•°é‡è¾ƒå°‘æ—¶çš„æ€§èƒ½ç“¶é¢ˆã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—FSAåœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­å…·æœ‰æ›´å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šFSAåœ¨å‚æ•°è®¾ç½®ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œç¡®ä¿åœ¨ä¸åŒçš„æŸ¥è¯¢å¤´æ•°é‡ä¸‹ä»èƒ½ä¿æŒé«˜æ•ˆçš„è®¡ç®—æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æå‡è®¡ç®—é€Ÿåº¦ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬å¯¹å†…å­˜è®¿é—®æ¨¡å¼çš„ä¼˜åŒ–å’Œè®¡ç®—å¹¶è¡Œæ€§çš„å¢å¼ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FSAåœ¨å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„NSAå†…æ ¸å®ç°ï¼Œå†…æ ¸çº§å»¶è¿Ÿæœ€é«˜å‡å°‘3.5å€ï¼Œå¹³å‡å‡å°‘1.6å€ï¼›ç«¯åˆ°ç«¯è®­ç»ƒé€Ÿåº¦æå‡æœ€é«˜1.25å€ï¼Œå¹³å‡æå‡1.09å€ï¼›ç”Ÿæˆæ¨ç†çš„é¢„å¡«å……é˜¶æ®µé€Ÿåº¦æå‡æœ€é«˜1.36å€ï¼Œå¹³å‡æå‡1.11å€ã€‚è¿™äº›ç»“æœè¡¨æ˜FSAåœ¨æ€§èƒ½ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Flash Sparse Attentionï¼ˆFSAï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚å…¶é«˜æ•ˆçš„è®¡ç®—èƒ½åŠ›å¯ä»¥æ˜¾è‘—é™ä½èµ„æºæ¶ˆè€—ï¼Œæé«˜æ¨¡å‹çš„å“åº”é€Ÿåº¦ï¼Œé€‚ç”¨äºå®æ—¶åº”ç”¨åœºæ™¯ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚æœªæ¥ï¼ŒFSAæœ‰æœ›æ¨åŠ¨æ›´å¤šåŸºäºç¨€ç–æ³¨æ„åŠ›çš„ç ”ç©¶å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advance in sparse attention mechanisms has demonstrated strong potential for reducing the computational cost of long-context training and inference in large language models (LLMs). Native Sparse Attention (NSA), one state-of-the-art approach, introduces natively trainable, hardware-aligned sparse attention that delivers substantial system-level performance boost while maintaining accuracy comparable to full attention. However, the kernel implementation of NSA forces a loop order that is only efficient with a relatively large number of query heads in each Grouped Query Attention (GQA) group, whereas existing LLMs widely adopt much smaller number of query heads in each GQA group -- such an inconsistency significantly limits the applicability of this sparse algorithmic advance. In this work, we propose Flash Sparse Attention (FSA), an alternative kernel implementation that enables efficient NSA computation across a wide range of popular LLMs with varied smaller number of heads in each GQA group on modern GPUs. Compared to vanilla NSA kernel implementation, our empirical evaluation demonstrates that FSA achieves (i) up to 3.5x and on average 1.6x kernel-level latency reduction, (ii) up to 1.25x and 1.09x on average end-to-end training speedup on state-of-the-art LLMs, and (iii) up to 1.36x and 1.11x on average for prefill-phase speedup in LLM generative inference. Github Repo at https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.

