---
layout: default
title: VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning
---

# VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18462" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18462v1</a>
  <a href="https://arxiv.org/pdf/2508.18462.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18462v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18462v1', 'VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fu Teng, Miao Pan, Xuhong Zhang, Zhezhi He, Yiyao Yang, Xinyi Chai, Mengnan Qi, Liqiang Lu, Jianwei Yin

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/omniAI-Lab/VeriRL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVERIRLæ¡†æ¶ä»¥æå‡Verilogä»£ç ç”Ÿæˆçš„æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Verilogä»£ç ç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `ç¡¬ä»¶æè¿°è¯­è¨€` `æ•°æ®é›†æ„å»º` `å¥–åŠ±æ¨¡å‹` `æ ·æœ¬å¹³è¡¡` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä»£ç ç”Ÿæˆæ–¹æ³•åœ¨è½¯ä»¶é¢†åŸŸè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨Verilogç­‰HDLä¸­é¢ä¸´å¹¶å‘è¯­ä¹‰å’Œè¯­æ³•åˆšæ€§ç­‰æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºé«˜è´¨é‡æ•°æ®é›†å’Œåˆ›æ–°çš„é‡è¯„åˆ†æœºåˆ¶æ¥æå‡Verilogä»£ç ç”Ÿæˆçš„æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVERIRLåœ¨æµ‹è¯•é€šè¿‡ç‡ã€åŠŸèƒ½æ­£ç¡®æ€§å’Œç¼–è¯‘é²æ£’æ€§ç­‰æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œä»£ç ç”Ÿæˆåœ¨è½¯ä»¶é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLï¼‰å¦‚Verilogç”±äºå…¶å¹¶å‘è¯­ä¹‰ã€è¯­æ³•åˆšæ€§å’Œä»¿çœŸå¤æ‚æ€§ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹Verilogä»£ç ç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†Veribench-53Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä»70ä¸‡ä¸ªVerilogé—®é¢˜ä¸­ç²¾å¿ƒç­–åˆ’è€Œæˆï¼ŒåŒ…å«ç»“æ„åŒ–æç¤ºã€å¤æ‚æ€§æ ‡ç­¾å’Œå¤šæ ·åŒ–æµ‹è¯•å¹³å°ã€‚ä¸ºäº†è§£å†³ç¨€ç–å’Œå™ªå£°å¥–åŠ±ä¿¡å·çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›æº¯çš„é‡è¯„åˆ†æœºåˆ¶ï¼Œåˆ©ç”¨æ¨ç†è·¯å¾„å’Œè¿­ä»£ä¼˜åŒ–æ¥å¢å¼ºåé¦ˆçš„å¯é æ€§å¹¶æ”¯æŒå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»å¼ºåŒ–å­¦ä¹ å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜å’Œè¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ ·æœ¬å¹³è¡¡åŠ æƒç­–ç•¥ï¼Œæ ¹æ®å¥–åŠ±æ¦‚ç‡åˆ†å¸ƒè‡ªé€‚åº”åœ°å¹³è¡¡å­¦ä¹ åŠ¨æ€ã€‚è¿™äº›åˆ›æ–°é›†æˆåˆ°ä¸€ä¸ªè¿­ä»£çš„RLæµç¨‹ä¸­ï¼Œå…±åŒæ¼”åŒ–ç­–ç•¥å’Œå¥–åŠ±æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Verilogç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æµ‹è¯•é€šè¿‡ç‡ã€åŠŸèƒ½æ­£ç¡®æ€§å’Œç¼–è¯‘é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³Verilogä»£ç ç”Ÿæˆä¸­çš„ç¨€ç–å¥–åŠ±ä¿¡å·å’Œå¤æ‚çš„è¯­æ³•ç»“æ„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚CraftRTLä¾èµ–äºå¤§è§„æ¨¡é—­æºæ¨¡å‹è’¸é¦ï¼Œéš¾ä»¥å¤„ç†HDLçš„ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºVeribench-53Kæ•°æ®é›†å’Œå¼•å…¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨å›æº¯é‡è¯„åˆ†æœºåˆ¶å’Œæ ·æœ¬å¹³è¡¡åŠ æƒç­–ç•¥æ¥å¢å¼ºåé¦ˆçš„å¯é æ€§å’Œå­¦ä¹ çš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œç­–ç•¥ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œåˆ©ç”¨é«˜è´¨é‡æ•°æ®é›†è¿›è¡Œåˆæ­¥è®­ç»ƒï¼Œç„¶åé€šè¿‡RLè¿­ä»£ä¼˜åŒ–ç­–ç•¥å’Œå¥–åŠ±æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºçš„Trace-back based Rescoreæœºåˆ¶å’Œæ ·æœ¬å¹³è¡¡åŠ æƒç­–ç•¥æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³ç¨€ç–åé¦ˆå’Œè¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨è‡ªé€‚åº”åŠ æƒç­–ç•¥æ¥å¹³è¡¡ä¸åŒå¥–åŠ±çš„å½±å“ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡æé«˜åé¦ˆçš„å¯é æ€§ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ ‡å‡†æ¶æ„è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVERIRLåœ¨Verilogç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæµ‹è¯•é€šè¿‡ç‡æ˜¾è‘—æé«˜ï¼ŒåŠŸèƒ½æ­£ç¡®æ€§å’Œç¼–è¯‘é²æ£’æ€§ä¹Ÿå¾—åˆ°äº†å¤§å¹…æå‡ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒVERIRLåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–ã€åµŒå…¥å¼ç³»ç»Ÿå¼€å‘ä»¥åŠFPGAç¼–ç¨‹ç­‰ã€‚é€šè¿‡æå‡Verilogä»£ç ç”Ÿæˆçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œèƒ½å¤ŸåŠ é€Ÿç¡¬ä»¶å¼€å‘å‘¨æœŸï¼Œé™ä½äººåŠ›æˆæœ¬ï¼Œæ¨åŠ¨æ™ºèƒ½ç¡¬ä»¶çš„å¿«é€Ÿå‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–HDLæˆ–ç¡¬ä»¶ç›¸å…³çš„ç¼–ç¨‹ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in code generation have shown remarkable success across software domains, yet hardware description languages (HDLs) such as Verilog remain underexplored due to their concurrency semantics, syntactic rigidity, and simulation complexity. In this work, we address these challenges by introducing a reinforcement learning (RL) framework tailored for Verilog code generation. We first construct Veribench-53K, a high-quality dataset curated from over 700K Verilog problems, enriched with structured prompts, complexity labels, and diverse testbenches. To tackle the problem of sparse and noisy reward signals, we propose a Trace-back based Rescore mechanism that leverages reasoning paths and iterative refinement to enhance feedback reliability and support reward model training. Furthermore, to mitigate catastrophic forgetting and overfitting during RL fine-tuning, we introduce a sample-balanced weighting strategy that adaptively balances learning dynamics based on reward-probability distributions. These innovations are integrated into an iterative RL pipeline that co-evolves the policy and reward models. In contrast to recent work such as CraftRTL, which relies on large-scale closed-source model distillation, and DeepSeek-style approaches that struggle with sparse feedback, our method demonstrates superior performance using a smaller but high-quality dataset combined with RL optimization. Experiments on Verilog generation tasks demonstrate state-of-the-art performance, with substantial gains in test pass rate, functional correctness, and compilation robustness. Our findings highlight the potential of RL-driven approaches for structured code generation in hardware-centric domains. VERIRL is publicly available at https://github.com/omniAI-Lab/VeriRL.

