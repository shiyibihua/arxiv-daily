---
layout: default
title: AdLoCo: adaptive batching significantly improves communications efficiency and convergence for Large Language Models
---

# AdLoCo: adaptive batching significantly improves communications efficiency and convergence for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18182" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18182v1</a>
  <a href="https://arxiv.org/pdf/2508.18182.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18182v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18182v1', 'AdLoCo: adaptive batching significantly improves communications efficiency and convergence for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nikolay Kutuzov, Makar Baderko, Stepan Kulibaba, Artem Dzhalilov, Daniel Bobrov, Maxim Mashtaler, Alexander Gasnikov

**åˆ†ç±»**: cs.LG, cs.AI, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAdLoCoä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„é€šä¿¡æ•ˆç‡ä¸æ”¶æ•›æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `åˆ†å¸ƒå¼è®­ç»ƒ` `è‡ªé€‚åº”æ‰¹å¤„ç†` `å¤šå®ä¾‹è®­ç»ƒ` `é€šä¿¡æ•ˆç‡` `æ”¶æ•›é€Ÿåº¦` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹æœªèƒ½å……åˆ†åˆ©ç”¨è®¡ç®—é›†ç¾¤ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. æå‡ºçš„ä¸‰é˜¶æ®µæ–¹æ³•ç»“åˆå¤šå®ä¾‹è®­ç»ƒã€è‡ªé€‚åº”æ‰¹å¤„ç†å’Œåˆ‡æ¢æ¨¡å¼ï¼Œä¼˜åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œç³»ç»Ÿæ•ˆç‡ï¼Œé™ä½äº†é€šä¿¡å»¶è¿Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ä»…éœ€è¦ç®—æ³•ä¸Šçš„è¿›æ­¥ï¼Œè¿˜éœ€è¦é«˜æ•ˆåˆ©ç”¨å¼‚æ„ç¡¬ä»¶èµ„æºã€‚ç°æœ‰æ–¹æ³•å¦‚DiLoCoè™½ç„¶å–å¾—äº†ä¸€å®šæˆæœï¼Œä½†åœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹æœªèƒ½å……åˆ†åˆ©ç”¨è®¡ç®—é›†ç¾¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„æ–¹æ³•ï¼Œç»“åˆäº†å¤šå®ä¾‹è®­ç»ƒï¼ˆMITï¼‰ã€è‡ªé€‚åº”æ‰¹å¤„ç†DiLoCoå’Œåˆ‡æ¢æ¨¡å¼æœºåˆ¶ã€‚MITå…è®¸å„èŠ‚ç‚¹å¹¶è¡Œè¿è¡Œå¤šä¸ªè½»é‡çº§è®­ç»ƒæµå¹¶åˆå¹¶çŸ¥è¯†ï¼Œæé«˜ååé‡å¹¶å‡å°‘ç©ºé—²æ—¶é—´ã€‚è‡ªé€‚åº”æ‰¹å¤„ç†DiLoCoåŠ¨æ€è°ƒæ•´æœ¬åœ°æ‰¹é‡å¤§å°ï¼Œä»¥å¹³è¡¡è®¡ç®—ä¸é€šä¿¡ï¼Œæ˜¾è‘—é™ä½åŒæ­¥å»¶è¿Ÿã€‚åˆ‡æ¢æ¨¡å¼é€šè¿‡åœ¨è‡ªé€‚åº”æ‰¹é‡å¤§å°è¶…è¿‡ç¡¬ä»¶å‹å¥½é™åˆ¶æ—¶æ— ç¼å¼•å…¥æ¢¯åº¦ç´¯ç§¯ï¼Œè¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒã€‚è¿™äº›åˆ›æ–°å…±åŒæé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œç³»ç»Ÿæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹æœªèƒ½å……åˆ†åˆ©ç”¨è®¡ç®—èµ„æºçš„é—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œæ”¶æ•›é€Ÿåº¦ç¼“æ…¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„AdLoCoæ–¹æ³•é€šè¿‡ç»“åˆå¤šå®ä¾‹è®­ç»ƒã€è‡ªé€‚åº”æ‰¹å¤„ç†å’Œåˆ‡æ¢æ¨¡å¼ï¼Œæ—¨åœ¨æé«˜è®­ç»ƒçš„å¹¶è¡Œæ€§å’Œèµ„æºåˆ©ç”¨ç‡ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›å¹¶é™ä½é€šä¿¡å»¶è¿Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åˆ†ä¸ºä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å¤šå®ä¾‹è®­ç»ƒï¼ˆMITï¼‰ï¼Œå…è®¸èŠ‚ç‚¹å¹¶è¡Œè¿è¡Œå¤šä¸ªè½»é‡çº§è®­ç»ƒæµï¼›ç¬¬äºŒé˜¶æ®µæ˜¯è‡ªé€‚åº”æ‰¹å¤„ç†DiLoCoï¼ŒåŠ¨æ€è°ƒæ•´æœ¬åœ°æ‰¹é‡å¤§å°ï¼›ç¬¬ä¸‰é˜¶æ®µæ˜¯åˆ‡æ¢æ¨¡å¼ï¼Œåœ¨æ‰¹é‡å¤§å°è¶…è¿‡é™åˆ¶æ—¶å¼•å…¥æ¢¯åº¦ç´¯ç§¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè‡ªé€‚åº”æ‰¹å¤„ç†æœºåˆ¶ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®å½“å‰å·¥ä½œè´Ÿè½½åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°ï¼Œä»è€Œæœ‰æ•ˆé™ä½åŒæ­¥å»¶è¿Ÿï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAdLoCoåœ¨åŠ¨æ€ç¯å¢ƒä¸‹è¡¨ç°å‡ºæ›´å¥½çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMITé˜¶æ®µé€šè¿‡å¹¶è¡Œè®­ç»ƒæµåˆå¹¶çŸ¥è¯†ï¼Œå‡å°‘ç©ºé—²æ—¶é—´ï¼›è‡ªé€‚åº”æ‰¹å¤„ç†é˜¶æ®µé€šè¿‡å®æ—¶è°ƒæ•´æ‰¹é‡å¤§å°æ¥å¹³è¡¡è®¡ç®—ä¸é€šä¿¡ï¼›åˆ‡æ¢æ¨¡å¼åˆ™ç¡®ä¿åœ¨é«˜è´Ÿè½½æ—¶è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdLoCoåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ”¶æ•›é€Ÿåº¦ï¼Œé€šä¿¡å»¶è¿Ÿé™ä½äº†çº¦30%ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒæ•ˆç‡æå‡äº†20%ä»¥ä¸Šï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡è®­ç»ƒæ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦ï¼ŒAdLoCoèƒ½å¤ŸåŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹çš„å¼€å‘ä¸éƒ¨ç½²ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„è¿›æ­¥ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Scaling distributed training of Large Language Models (LLMs) requires not only algorithmic advances but also efficient utilization of heterogeneous hardware resources. While existing methods such as DiLoCo have demonstrated promising results, they often fail to fully exploit computational clusters under dynamic workloads. To address this limitation, we propose a three-stage method that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo, and switch mode mechanism. MIT allows individual nodes to run multiple lightweight training streams with different model instances in parallel and merge them to combine knowledge, increasing throughput and reducing idle time. Adaptive Batched DiLoCo dynamically adjusts local batch sizes to balance computation and communication, substantially lowering synchronization delays. Switch mode further stabilizes training by seamlessly introducing gradient accumulation once adaptive batch sizes grow beyond hardware-friendly limits. Together, these innovations improve both convergence speed and system efficiency. We also provide a theoretical estimate of the number of communications required for the full convergence of a model trained using our method.

