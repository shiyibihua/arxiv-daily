---
layout: default
title: Proximal Supervised Fine-Tuning
---

# Proximal Supervised Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17784" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17784v1</a>
  <a href="https://arxiv.org/pdf/2508.17784.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17784v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17784v1', 'Proximal Supervised Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhong Zhu, Ruobing Xie, Rui Wang, Xingwu Sun, Di Wang, Pengfei Liu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¿‘ç«¯ç›‘ç£å¾®è°ƒæ–¹æ³•ä»¥è§£å†³æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›‘ç£å¾®è°ƒ` `æ¨¡å‹æ³›åŒ–` `ä¿¡ä»»åŒºåŸŸ` `ç­–ç•¥ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æ–°ä»»åŠ¡æˆ–é¢†åŸŸä¸Šå¸¸å¸¸å¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™ï¼ŒåŸæœ‰èƒ½åŠ›é€€åŒ–ã€‚
2. æœ¬æ–‡æå‡ºçš„è¿‘ç«¯ç›‘ç£å¾®è°ƒï¼ˆPSFTï¼‰é€šè¿‡å¼•å…¥ä¿¡ä»»åŒºåŸŸçš„æ¦‚å¿µï¼Œæœ‰æ•ˆé™åˆ¶äº†ç­–ç•¥æ¼‚ç§»ï¼Œæå‡äº†å¾®è°ƒçš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPSFTåœ¨é¢†åŸŸå¤–çš„æ³›åŒ–èƒ½åŠ›ä¼˜äºä¼ ç»ŸSFTï¼Œå¹¶ä¸”åœ¨é•¿æ—¶é—´è®­ç»ƒä¸­ä¿æŒäº†ç¨³å®šæ€§ï¼Œæœªå‡ºç°ç†µå´©æºƒç°è±¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºç¡€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¸¸å¸¸å¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸‹é™ï¼ŒåŸæœ‰èƒ½åŠ›åœ¨æ–°ä»»åŠ¡æˆ–é¢†åŸŸä¸Šé€€åŒ–ã€‚å—å¼ºåŒ–å­¦ä¹ ä¸­ä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰å’Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†è¿‘ç«¯ç›‘ç£å¾®è°ƒï¼ˆPSFTï¼‰ã€‚è¯¥å¾®è°ƒç›®æ ‡ç»“åˆäº†ä¿¡ä»»åŒºåŸŸçš„ä¼˜åŠ¿ï¼Œæœ‰æ•ˆé™åˆ¶äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç­–ç•¥æ¼‚ç§»ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„è°ƒä¼˜ã€‚é€šè¿‡å°†SFTè§†ä¸ºå…·æœ‰æ’å®šæ­£ä¼˜åŠ¿çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ç‰¹ä¾‹ï¼Œæœ¬æ–‡æ¨å¯¼å‡ºPSFTï¼Œç¨³å®šäº†ä¼˜åŒ–è¿‡ç¨‹å¹¶æå‡äº†æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¸ºåç»­çš„ä¼˜åŒ–ç•™å‡ºäº†ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPSFTåœ¨é¢†åŸŸå†…è¡¨ç°ä¸SFTç›¸å½“ï¼Œåœ¨é¢†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ä¸Šè¶…è¶ŠSFTï¼Œå¹¶åœ¨é•¿æ—¶é—´è®­ç»ƒä¸‹ä¿æŒç¨³å®šï¼Œæœªå¯¼è‡´ç†µå´©æºƒï¼Œä¸ºåç»­ä¼˜åŒ–æä¾›äº†æ›´å¼ºçš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºç¡€æ¨¡å‹åœ¨ç›‘ç£å¾®è°ƒåæ³›åŒ–èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æ–°ä»»åŠ¡æˆ–é¢†åŸŸä¸Šå¸¸å¸¸å¯¼è‡´æ¨¡å‹èƒ½åŠ›çš„é€€åŒ–ï¼Œå½±å“äº†æ¨¡å‹çš„å®ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„è¿‘ç«¯ç›‘ç£å¾®è°ƒï¼ˆPSFTï¼‰æ–¹æ³•å€Ÿé‰´äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰å’Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ï¼Œé€šè¿‡é™åˆ¶ç­–ç•¥æ¼‚ç§»æ¥æé«˜å¾®è°ƒçš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPSFTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¾®è°ƒç›®æ ‡çš„è®¾è®¡å’Œä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§æ§åˆ¶ã€‚é¦–å…ˆï¼Œé€šè¿‡å¼•å…¥ä¿¡ä»»åŒºåŸŸçš„çº¦æŸï¼Œç¡®ä¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ¨¡å‹çš„èƒ½åŠ›ä¸è¢«è¿‡åº¦æ”¹å˜ã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„æ¡†æ¶ï¼Œä¼˜åŒ–è¿‡ç¨‹è¢«ç¨³å®šåŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šPSFTçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†ç›‘ç£å¾®è°ƒè§†ä¸ºç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ç‰¹ä¾‹ï¼Œå¹¶é€šè¿‡ä¿¡ä»»åŒºåŸŸçš„çº¦æŸæ¥æ§åˆ¶ç­–ç•¥çš„å˜åŒ–ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨PSFTä¸­ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†ä¿¡ä»»åŒºåŸŸçš„çº¦æŸï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œå‚æ•°è®¾ç½®ä¸Šä¹Ÿè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ï¼Œç¡®ä¿äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPSFTåœ¨é¢†åŸŸå†…çš„è¡¨ç°ä¸ä¼ ç»ŸSFTç›¸å½“ï¼Œä½†åœ¨é¢†åŸŸå¤–çš„æ³›åŒ–èƒ½åŠ›ä¸Šè¶…è¶Šäº†SFTï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæå‡äº†10%è‡³20%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒPSFTåœ¨é•¿æ—¶é—´è®­ç»ƒä¸­ä¿æŒç¨³å®šï¼Œæœªå‡ºç°ç†µå´©æºƒç°è±¡ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šçš„ç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰å¤šä¸ªéœ€è¦åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„ä»»åŠ¡ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒPSFTèƒ½å¤Ÿåœ¨å¤šç§å®é™…åœºæ™¯ä¸­æä¾›æ›´å¯é çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Supervised fine-tuning (SFT) of foundation models often leads to poor generalization, where prior capabilities deteriorate after tuning on new tasks or domains. Inspired by trust-region policy optimization (TRPO) and proximal policy optimization (PPO) in reinforcement learning (RL), we propose Proximal SFT (PSFT). This fine-tuning objective incorporates the benefits of trust-region, effectively constraining policy drift during SFT while maintaining competitive tuning. By viewing SFT as a special case of policy gradient methods with constant positive advantages, we derive PSFT that stabilizes optimization and leads to generalization, while leaving room for further optimization in subsequent post-training stages. Experiments across mathematical and human-value domains show that PSFT matches SFT in-domain, outperforms it in out-of-domain generalization, remains stable under prolonged training without causing entropy collapse, and provides a stronger foundation for the subsequent optimization.

