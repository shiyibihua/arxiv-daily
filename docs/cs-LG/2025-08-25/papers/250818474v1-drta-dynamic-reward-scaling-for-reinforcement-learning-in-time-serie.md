---
layout: default
title: DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection
---

# DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18474" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18474v1</a>
  <a href="https://arxiv.org/pdf/2508.18474.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18474v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18474v1', 'DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bahareh Golchin, Banafsheh Rekabdar, Kunpeng Liu

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**æœŸåˆŠ**: IEEE 2025 Conference on AI, Science, Engineering, and Technology (AIxSET)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRTAæ¡†æ¶ä»¥è§£å†³æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ä¸­çš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼‚å¸¸æ£€æµ‹` `æ—¶é—´åºåˆ—` `å¼ºåŒ–å­¦ä¹ ` `å˜åˆ†è‡ªç¼–ç å™¨` `ä¸»åŠ¨å­¦ä¹ ` `åŠ¨æ€å¥–åŠ±` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºã€é«˜å‡é˜³æ€§ç‡å’Œæ–°å‹å¼‚å¸¸æ³›åŒ–èƒ½åŠ›ä¸è¶³ç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„DRTAæ¡†æ¶é€šè¿‡åŠ¨æ€å¥–åŠ±æœºåˆ¶ç»“åˆVAEå’Œä¸»åŠ¨å­¦ä¹ ï¼Œæœ‰æ•ˆæå‡äº†å¼‚å¸¸æ£€æµ‹çš„ç²¾åº¦ä¸å¬å›ç‡ã€‚
3. åœ¨Yahoo A1å’ŒA2æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDRTAåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ— ç›‘ç£å’ŒåŠç›‘ç£æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¼‚å¸¸æ£€æµ‹åœ¨é‡‘èã€åŒ»ç–—ã€ä¼ æ„Ÿå™¨ç½‘ç»œå’Œå·¥ä¸šç›‘æ§ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸é¢ä¸´æ ‡æ³¨æ•°æ®æœ‰é™ã€é«˜å‡é˜³æ€§ç‡ä»¥åŠéš¾ä»¥æ³›åŒ–åˆ°æ–°å‹å¼‚å¸¸ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶DRTAï¼Œè¯¥æ¡†æ¶é›†æˆäº†åŠ¨æ€å¥–åŠ±å¡‘å½¢ã€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œä¸»åŠ¨å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è‡ªé€‚åº”å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´VAEé‡æ„è¯¯å·®å’Œåˆ†ç±»å¥–åŠ±çš„å½±å“ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—ä»£ç†èƒ½å¤Ÿåœ¨ä½æ ‡æ³¨ç³»ç»Ÿä¸­æœ‰æ•ˆæ£€æµ‹å¼‚å¸¸ï¼ŒåŒæ—¶ä¿æŒé«˜ç²¾åº¦å’Œå¬å›ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRTAåœ¨Yahoo A1å’ŒYahoo A2åŸºå‡†æ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„æ— ç›‘ç£å’ŒåŠç›‘ç£æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­çš„å¯æ‰©å±•æ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¼‚å¸¸æ£€æµ‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œé«˜å‡é˜³æ€§ç‡æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¯¼è‡´æ£€æµ‹æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDRTAæ¡†æ¶é€šè¿‡åŠ¨æ€å¥–åŠ±å¡‘å½¢æœºåˆ¶ï¼Œç»“åˆVAEçš„é‡æ„è¯¯å·®å’Œåˆ†ç±»å¥–åŠ±ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œä»è€Œæé«˜å¼‚å¸¸æ£€æµ‹çš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šåŠ¨æ€å¥–åŠ±æœºåˆ¶ã€VAEé‡æ„æ¨¡å—å’Œä¸»åŠ¨å­¦ä¹ æ¨¡å—ã€‚åŠ¨æ€å¥–åŠ±æœºåˆ¶æ ¹æ®å½“å‰æ£€æµ‹æƒ…å†µè°ƒæ•´å¥–åŠ±ï¼ŒVAEç”¨äºç”Ÿæˆæ•°æ®çš„é‡æ„ï¼Œä¸»åŠ¨å­¦ä¹ åˆ™ç”¨äºé€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šDRTAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåŠ¨æ€è°ƒæ•´å¥–åŠ±çš„æœºåˆ¶ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿåœ¨ä½æ ‡æ³¨æ•°æ®ç¯å¢ƒä¸­æœ‰æ•ˆå­¦ä¹ ï¼Œä»è€Œæå‡å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¬å›ç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒDRTAåœ¨å¤„ç†æ–°å‹å¼‚å¸¸æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è‡ªé€‚åº”çš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡é‡æ„è¯¯å·®å’Œåˆ†ç±»å¥–åŠ±ï¼ŒåŒæ—¶VAEçš„ç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æé«˜é‡æ„è´¨é‡å’Œæ£€æµ‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDRTAåœ¨Yahoo A1å’ŒA2æ•°æ®é›†ä¸Šç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ— ç›‘ç£å’ŒåŠç›‘ç£æ–¹æ³•ï¼Œæ£€æµ‹ç²¾åº¦æå‡äº†çº¦15%ï¼Œå¬å›ç‡æå‡äº†20%ã€‚è¿™äº›ç»“æœè¯æ˜äº†DRTAåœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èæ¬ºè¯ˆæ£€æµ‹ã€åŒ»ç–—ç›‘æµ‹ã€å·¥ä¸šè®¾å¤‡æ•…éšœé¢„è­¦ç­‰ã€‚é€šè¿‡æé«˜å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒDRTAæ¡†æ¶èƒ½å¤Ÿä¸ºå„è¡Œä¸šæä¾›æ›´å¯é çš„æ•°æ®åˆ†ææ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Anomaly detection in time series data is important for applications in finance, healthcare, sensor networks, and industrial monitoring. Traditional methods usually struggle with limited labeled data, high false-positive rates, and difficulty generalizing to novel anomaly types. To overcome these challenges, we propose a reinforcement learning-based framework that integrates dynamic reward shaping, Variational Autoencoder (VAE), and active learning, called DRTA. Our method uses an adaptive reward mechanism that balances exploration and exploitation by dynamically scaling the effect of VAE-based reconstruction error and classification rewards. This approach enables the agent to detect anomalies effectively in low-label systems while maintaining high precision and recall. Our experimental results on the Yahoo A1 and Yahoo A2 benchmark datasets demonstrate that the proposed method consistently outperforms state-of-the-art unsupervised and semi-supervised approaches. These findings show that our framework is a scalable and efficient solution for real-world anomaly detection tasks.

