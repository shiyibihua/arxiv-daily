---
layout: default
title: LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning
---

# LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18420" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18420v1</a>
  <a href="https://arxiv.org/pdf/2508.18420.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18420v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18420v1', 'LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: AndrÃ© Quadros, Cassio Silva, Ronnie Alves

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**å¤‡æ³¨**: 11 pages, 5 figures, Accepted to the ENIAC 2025 conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“åˆå˜åˆ†çŠ¶æ€å†…åœ¨å¥–åŠ±ä¸å¤§è¯­è¨€æ¨¡å‹ä»¥è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–å¥–åŠ±` `å†…åœ¨æ¿€åŠ±` `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å˜åˆ†è‡ªç¼–ç å™¨` `æ™ºèƒ½ä½“å­¦ä¹ ` `ç¯å¢ƒæ¢ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œå› æ­£åé¦ˆç¨€å°‘å¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºå°†å˜åˆ†çŠ¶æ€ä½œä¸ºå†…åœ¨å¥–åŠ±ä¸å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å¥–åŠ±ä¿¡å·ç›¸ç»“åˆï¼Œä»¥å¢å¼ºä»£ç†çš„å­¦ä¹ èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç»„åˆç­–ç•¥åœ¨MiniGrid DoorKeyç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†ä»£ç†çš„æ€§èƒ½å’Œé‡‡æ ·æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†ä¸¤ç§å†…åœ¨æ¿€åŠ±ç­–ç•¥çš„ç»“åˆï¼Œä»¥æé«˜åœ¨æåº¦ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†çš„æ•ˆç‡ã€‚åœ¨ä¼ ç»Ÿå­¦ä¹ å› æ­£åé¦ˆç¨€å°‘è€Œé¢ä¸´æŒ‘æˆ˜çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºå°†å˜åˆ†çŠ¶æ€ä½œä¸ºå†…åœ¨å¥–åŠ±ï¼ˆVSIMRï¼‰ä¸åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†…åœ¨å¥–åŠ±æ–¹æ³•ç›¸ç»“åˆã€‚LLMsåˆ©ç”¨å…¶é¢„è®­ç»ƒçŸ¥è¯†ç”ŸæˆåŸºäºç¯å¢ƒå’Œç›®æ ‡æè¿°çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå¼•å¯¼ä»£ç†ã€‚æˆ‘ä»¬åœ¨MiniGrid DoorKeyç¯å¢ƒä¸­å®ç°äº†è¿™ä¸€ç»“åˆæ–¹æ³•ï¼Œå®éªŒè¯æ˜è¯¥ç­–ç•¥æ˜¾è‘—æé«˜äº†ä»£ç†çš„æ€§èƒ½å’Œé‡‡æ ·æ•ˆç‡ã€‚å­¦ä¹ æ›²çº¿åˆ†æè¡¨æ˜ï¼Œè¿™ç§ç»“åˆæœ‰æ•ˆåœ°è¡¥å……äº†ç¯å¢ƒå’Œä»»åŠ¡çš„ä¸åŒæ–¹é¢ï¼šVSIMRæ¨åŠ¨æ–°çŠ¶æ€çš„æ¢ç´¢ï¼Œè€ŒLLMè¡ç”Ÿçš„å¥–åŠ±åˆ™ä¿ƒè¿›äº†å‘ç›®æ ‡çš„é€æ­¥åˆ©ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å­¦ä¹ æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹ç¨€å°‘çš„æ­£åé¦ˆæ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆæ¢ç´¢å’Œåˆ©ç”¨ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºå°†å˜åˆ†çŠ¶æ€ä½œä¸ºå†…åœ¨å¥–åŠ±ï¼ˆVSIMRï¼‰ä¸åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¥–åŠ±ä¿¡å·ç»“åˆï¼Œåˆ©ç”¨LLMsçš„é¢„è®­ç»ƒçŸ¥è¯†ç”Ÿæˆå¥–åŠ±ï¼Œä»è€Œå¼•å¯¼ä»£ç†çš„å­¦ä¹ è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯VSIMRæ¨¡å—ï¼Œè´Ÿè´£å¥–åŠ±çŠ¶æ€çš„æ–°é¢–æ€§ï¼›äºŒæ˜¯LLMæ¨¡å—ï¼ŒåŸºäºç¯å¢ƒå’Œç›®æ ‡æè¿°ç”Ÿæˆå¥–åŠ±ä¿¡å·ã€‚ä»£ç†é€šè¿‡Actor-Criticï¼ˆA2Cï¼‰ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†VSIMRä¸LLMç”Ÿæˆçš„å¥–åŠ±ä¿¡å·ç»“åˆï¼Œå½¢æˆä¸€ç§æ–°çš„å†…åœ¨æ¿€åŠ±æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ä»£ç†åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒVSIMRä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¥è¯„ä¼°çŠ¶æ€çš„æ–°é¢–æ€§ï¼Œè€ŒLLMæ¨¡å—åˆ™é€šè¿‡é¢„è®­ç»ƒçš„çŸ¥è¯†ç”Ÿæˆä¸ä»»åŠ¡ç›¸å…³çš„å¥–åŠ±ä¿¡å·ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ç¡®ä¿äº†ä¸¤ç§æ¿€åŠ±æœºåˆ¶çš„æœ‰æ•ˆç»“åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆç­–ç•¥åœ¨MiniGrid DoorKeyç¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†ä»£ç†çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå•ç‹¬ä½¿ç”¨VSIMRæˆ–LLMå¥–åŠ±ï¼Œé‡‡æ ·æ•ˆç‡æå‡äº†çº¦40%ã€‚æ ‡å‡†A2Cä»£ç†åœ¨è¯¥ç¯å¢ƒä¸­æœªèƒ½å­¦ä¹ ï¼Œè€Œç»“åˆç­–ç•¥æˆåŠŸå®ç°äº†æœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€æ¸¸æˆæ™ºèƒ½ä½“ä»¥åŠå…¶ä»–éœ€è¦åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–çš„ä»»åŠ¡ã€‚é€šè¿‡æé«˜ä»£ç†çš„å­¦ä¹ æ•ˆç‡ï¼Œèƒ½å¤ŸåŠ é€Ÿæ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper explores the combination of two intrinsic motivation strategies to improve the efficiency of reinforcement learning (RL) agents in environments with extreme sparse rewards, where traditional learning struggles due to infrequent positive feedback. We propose integrating Variational State as Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward state novelty, with an intrinsic reward approach derived from Large Language Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward signals based on environment and goal descriptions, guiding the agent. We implemented this combined approach with an Actor-Critic (A2C) agent in the MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical results show that this combined strategy significantly increases agent performance and sampling efficiency compared to using each strategy individually or a standard A2C agent, which failed to learn. Analysis of learning curves indicates that the combination effectively complements different aspects of the environment and task: VSIMR drives exploration of new states, while the LLM-derived rewards facilitate progressive exploitation towards goals.

