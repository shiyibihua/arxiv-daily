---
layout: default
title: Characterizing the Behavior of Training Mamba-based State Space Models on GPUs
---

# Characterizing the Behavior of Training Mamba-based State Space Models on GPUs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17679" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17679v1</a>
  <a href="https://arxiv.org/pdf/2508.17679.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17679v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17679v1', 'Characterizing the Behavior of Training Mamba-based State Space Models on GPUs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Trinayan Baruah, Kaustubh Shivdikar, Sara Prescott, David Kaeli

**åˆ†ç±»**: cs.LG, cs.AR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°MambaåŸºç¡€çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨GPUä¸Šçš„è®­ç»ƒè¡Œä¸º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŠ¶æ€ç©ºé—´æ¨¡å‹` `GPUè®­ç»ƒ` `è®¡ç®—å¤æ‚æ€§` `æ¨¡å‹ä¼˜åŒ–` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å˜å‹å™¨æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶ï¼Œç”±äºè‡ªæ³¨æ„åŠ›è®¡ç®—çš„äºŒæ¬¡å¤æ‚æ€§ï¼Œå¯¼è‡´æ€§èƒ½æ‰©å±•å—é™ã€‚
2. æœ¬æ–‡æå‡ºMambaåŸºç¡€çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ–°é¢–çš„æ¨¡å‹æ¶æ„é™ä½è‡ªæ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚æ€§ã€‚
3. é€šè¿‡æ„å»ºå·¥ä½œè´Ÿè½½å¥—ä»¶å¹¶åˆ†æMambaåŸºç¡€SSMåœ¨GPUä¸Šçš„è¡Œä¸ºï¼Œç ”ç©¶æ­ç¤ºäº†æ½œåœ¨çš„æ€§èƒ½ä¼˜åŒ–æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

MambaåŸºç¡€çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ä½œä¸ºå˜å‹å™¨çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆï¼Œè§£å†³äº†è‡ªæ³¨æ„åŠ›è®¡ç®—çš„äºŒæ¬¡å¤æ‚æ€§é—®é¢˜ï¼Œé€‚ç”¨äºè§†é¢‘ã€æ–‡æœ¬ç”Ÿæˆå’Œå›¾å½¢ç­‰å¤šä¸ªé¢†åŸŸã€‚æœ¬æ–‡è¯„ä¼°äº†MambaåŸºç¡€SSMåœ¨GPUä¸Šçš„è®­ç»ƒè¡Œä¸ºï¼Œæ„å»ºäº†ä¸€ä¸ªä»£è¡¨æ€§æ¨¡å‹çš„å·¥ä½œè´Ÿè½½å¥—ä»¶ï¼Œå¹¶åˆ†æäº†å…¶åœ¨GPUå¾®æ¶æ„è®¾è®¡ä¸­çš„éœ€æ±‚ã€‚ç ”ç©¶ç»“æœä¸ºä¼˜åŒ–è¿™äº›æ¨¡å‹çš„æ€§èƒ½æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³MambaåŸºç¡€çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨GPUä¸Šè®­ç»ƒæ—¶çš„æ€§èƒ½è¯„ä¼°é—®é¢˜ï¼Œç°æœ‰å˜å‹å™¨æ¨¡å‹åœ¨é•¿åºåˆ—å¤„ç†ä¸­çš„è®¡ç®—å¤æ‚æ€§æ˜¯ä¸»è¦ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«ä¸åŒæ¨¡å‹æ¶æ„çš„å·¥ä½œè´Ÿè½½å¥—ä»¶ï¼Œè¯„ä¼°MambaåŸºç¡€SSMçš„è®­ç»ƒè¡Œä¸ºï¼Œä»¥ç†è§£å…¶åœ¨GPUå¾®æ¶æ„è®¾è®¡ä¸­çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªä»£è¡¨æ€§æ¨¡å‹çš„å·¥ä½œè´Ÿè½½å¥—ä»¶ï¼Œéšååˆ©ç”¨è¯¥å¥—ä»¶å¯¹MambaåŸºç¡€SSMåœ¨GPUä¸Šçš„è¡¨ç°è¿›è¡Œåˆ†æï¼Œé‡ç‚¹å…³æ³¨å…¶è®¡ç®—éœ€æ±‚å’Œæ€§èƒ½ç“¶é¢ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹æ¶æ„ï¼Œæ˜¾è‘—é™ä½äº†è‡ªæ³¨æ„åŠ›è®¡ç®—çš„å¤æ‚æ€§ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿè¯„ä¼°æ­ç¤ºäº†å…¶åœ¨GPUä¸Šçš„è®­ç»ƒè¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„é€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMambaåŸºç¡€SSMåœ¨GPUä¸Šçš„è®­ç»ƒæ•ˆç‡æ˜¾è‘—æé«˜ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå˜å‹å™¨æ¨¡å‹ï¼Œè®¡ç®—å¤æ‚æ€§é™ä½äº†çº¦50%ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ€§èƒ½æå‡ï¼Œå…·ä½“æ•°æ®å°†åœ¨æ–‡ä¸­è¯¦ç»†åˆ—å‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘åˆ†æã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾å½¢æ•°æ®å¤„ç†ç­‰ã€‚é€šè¿‡ä¼˜åŒ–MambaåŸºç¡€SSMåœ¨GPUä¸Šçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ¨åŠ¨è¿™äº›é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ï¼Œæå‡æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mamba-based State Space Models (SSM) have emerged as a promising alternative to the ubiquitous transformers. Despite the expressive power of transformers, the quadratic complexity of computing attention is a major impediment to scaling performance as we increase the sequence length. SSMs provide an alternative path that addresses this problem, reducing the computational complexity requirements of self-attention with novel model architectures for different domains and fields such as video, text generation and graphs. Thus, it is important to characterize the behavior of these emerging workloads on GPUs and understand their requirements during GPU microarchitectural design. In this work we evaluate Mamba-based SSMs and characterize their behavior during training on GPUs. We construct a workload suite that offers representative models that span different model architectures. We then use this suite to analyze the architectural implications of running Mamba-based SSMs on GPUs. Our work sheds new light on potential optimizations to continue scaling the performance for such models.

