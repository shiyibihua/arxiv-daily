---
layout: default
title: DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction
---

# DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18376" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18376v1</a>
  <a href="https://arxiv.org/pdf/2508.18376.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18376v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18376v1', 'DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weilin Cai, Le Qin, Shwai He, Junwei Cui, Ang Li, Jiayi Huang

**åˆ†ç±»**: cs.LG, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDualSparse-MoEä»¥è§£å†³MoEæ¨¡å‹çš„è®¡ç®—æ•ˆç‡ä¸å‡†ç¡®æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `ç¨€ç–æ€§` `æ¨¡å‹ä¼˜åŒ–` `è®¡ç®—æ•ˆç‡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„MoEæ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œæ¿€æ´»æ¨¡å¼çš„ä¸å¯é¢„æµ‹æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå½±å“äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚
2. æœ¬æ–‡æå‡ºDualSparse-MoEï¼Œé€šè¿‡åè®­ç»ƒä¸“å®¶åˆ†åŒºå®ç°å¼ é‡å’Œç¥ç»å…ƒå±‚é¢çš„åŒé‡ç¨€ç–æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨çº¦25%çš„è®¡ç®—ä¸¢å¼ƒç‡ï¼Œå¹³å‡å‡†ç¡®åº¦ä»…ä¸‹é™0.08%-0.28%ï¼ŒåŒæ—¶å®ç°äº†æ˜¾è‘—çš„è®¡ç®—åŠ é€Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Mixture of Experts (MoE)å·²æˆä¸ºæ„å»ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸»æµæ¶æ„ï¼Œé€šè¿‡å‡å°‘æ¯ä¸ªtokençš„è®¡ç®—é‡æ¥å®ç°æ¨¡å‹æ‰©å±•ã€‚å°½ç®¡è¿™ç§ç¨€ç–æ€§æé«˜äº†æ•ˆç‡ï¼Œä½†MoEä»é¢ä¸´å·¨å¤§çš„è®¡ç®—è§„æ¨¡å’Œä¸å¯é¢„æµ‹çš„æ¿€æ´»æ¨¡å¼ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†DualSparse-MoEï¼Œè¯†åˆ«å‡ºé¢„è®­ç»ƒMoEæ¨¡å—åœ¨å¼ é‡å’Œç¥ç»å…ƒå±‚é¢çš„åŒé‡ç¨€ç–æ€§ï¼Œä½œä¸ºæé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡çš„å…³é”®å› ç´ ã€‚ä¸ä»¥å¾€é€šè¿‡æ›´ç»†ç²’åº¦çš„ä¸“å®¶è®¾è®¡å¢åŠ å¼ é‡çº§ç¨€ç–æ€§ä¸åŒï¼Œæœ¬æ–‡å¼•å…¥äº†åè®­ç»ƒä¸“å®¶åˆ†åŒºï¼Œä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹è¯±å¯¼ç¨€ç–æ€§ï¼Œä»è€Œåœ¨åç»­çš„å¾®è°ƒå’Œæ¨ç†ä¸­å¢å¼ºæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¯¥æ–¹æ³•çš„çº¦25%è®¡ç®—ä¸¢å¼ƒç‡ä»…å¯¼è‡´0.08%-0.28%çš„å¹³å‡å‡†ç¡®åº¦ä¸‹é™ï¼ŒåŒæ—¶å‡ ä¹æ‰€æœ‰è®¡ç®—ä¸¢å¼ƒçš„ç¨‹åº¦éƒ½èƒ½ä¸€è‡´åœ°å¸¦æ¥ç›¸åº”çš„è®¡ç®—åŠ é€Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³MoEæ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œæ¿€æ´»æ¨¡å¼ä¸å¯é¢„æµ‹æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æé«˜å¼ é‡çº§ç¨€ç–æ€§æ—¶ï¼Œå¾€å¾€éœ€è¦é‡æ–°è®­ç»ƒï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºåè®­ç»ƒä¸“å®¶åˆ†åŒºçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹è¯±å¯¼å¼ é‡å’Œç¥ç»å…ƒå±‚é¢çš„ç¨€ç–æ€§ï¼Œä»è€Œä¿æŒæ¨¡å‹çš„æ•°å­¦ä¸€è‡´æ€§ï¼Œå¹¶åœ¨å¾®è°ƒå’Œæ¨ç†é˜¶æ®µæå‡æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDualSparse-MoEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åŠ¨æ€å¼ é‡çº§è®¡ç®—ä¸¢å¼ƒå’Œé™æ€ç¥ç»å…ƒçº§é‡æ„ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚åŠ¨æ€ä¸¢å¼ƒæ ¹æ®è¾“å…¥çš„ç‰¹å¾åŠ¨æ€é€‰æ‹©æ¿€æ´»çš„ä¸“å®¶ï¼Œè€Œé™æ€é‡æ„åˆ™åœ¨æ¨ç†é˜¶æ®µä¿æŒæ¨¡å‹çš„ç¨³å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†åè®­ç»ƒä¸“å®¶åˆ†åŒºçš„æ¦‚å¿µï¼Œå…è®¸åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°åŒé‡ç¨€ç–æ€§ï¼Œè¿™ä¸ä»¥å¾€éœ€è¦é‡æ–°è®­ç»ƒçš„ç¨€ç–åŒ–æ–¹æ³•æœ¬è´¨ä¸Šæœ‰æ‰€ä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè®¾ç½®äº†çº¦25%çš„è®¡ç®—ä¸¢å¼ƒç‡ï¼Œå¹¶é€šè¿‡è´Ÿè½½ä¸å¹³è¡¡æ„ŸçŸ¥æ¥ä¼˜åŒ–ä¸“å®¶å¹¶è¡Œæ€§ï¼Œä»è€Œåœ¨ä¿æŒè¾ƒä½å‡†ç¡®åº¦ä¸‹é™çš„åŒæ—¶å®ç°äº†1.41å€çš„åŠ é€Ÿã€‚å…·ä½“å‚æ•°å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†è®¨è®ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨çº¦25%çš„è®¡ç®—ä¸¢å¼ƒç‡ï¼Œå¹³å‡å‡†ç¡®åº¦ä»…ä¸‹é™0.08%-0.28%ã€‚æ­¤å¤–ï¼Œç»“åˆè´Ÿè½½ä¸å¹³è¡¡æ„ŸçŸ¥çš„ä¸“å®¶å¹¶è¡Œæ€§ä¼˜åŒ–ï¼Œå®ç°äº†1.41å€çš„MoEæ¨¡å—åŠ é€Ÿï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ¨ç†å’Œéƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚é€šè¿‡æé«˜MoEæ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè¾ƒé«˜å‡†ç¡®åº¦çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture of Experts (MoE) has become a mainstream architecture for building Large Language Models (LLMs) by reducing per-token computation while enabling model scaling. It can be viewed as partitioning a large Feed-Forward Network (FFN) at the tensor level into fine-grained sub-FFNs, or experts, and activating only a sparse subset for each input. While this sparsity improves efficiency, MoE still faces substantial challenges due to their massive computational scale and unpredictable activation patterns.
>   To enable efficient MoE deployment, we identify dual sparsity at the tensor and neuron levels in pre-trained MoE modules as a key factor for both accuracy and efficiency. Unlike prior work that increases tensor-level sparsity through finer-grained expert design during pre-training, we introduce post-training expert partitioning to induce such sparsity without retraining. This preserves the mathematical consistency of model transformations and enhances both efficiency and accuracy in subsequent fine-tuning and inference. Building upon this, we propose DualSparse-MoE, an inference system that integrates dynamic tensor-level computation dropping with static neuron-level reconstruction to deliver significant efficiency gains with minimal accuracy loss.
>   Experimental results show that enforcing an approximate 25% drop rate with our approach reduces average accuracy by only 0.08%-0.28% across three prevailing MoE models, while nearly all degrees of computation dropping consistently yield proportional computational speedups. Furthermore, incorporating load-imbalance awareness into expert parallelism achieves a 1.41x MoE module speedup with just 0.5% average accuracy degradation.

