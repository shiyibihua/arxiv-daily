---
layout: default
title: Efficient Turing Machine Simulation with Transformers
---

# Efficient Turing Machine Simulation with Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.00003" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.00003v2</a>
  <a href="https://arxiv.org/pdf/2512.00003.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00003v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.00003v2', 'Efficient Turing Machine Simulation with Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qian Li, Yuyi Wang

**åˆ†ç±»**: cs.CC, cs.DS, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-28 (æ›´æ–°: 2025-12-02)

**å¤‡æ³¨**: 19 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆTransformerå›¾çµæœºæ¨¡æ‹Ÿæ–¹æ³•ï¼Œæ˜¾è‘—é™ä½æ¨ç†æ­¥æ•°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾çµæœº` `Transformer` `è®¡ç®—å¤æ‚æ€§` `ç¨€ç–æ³¨æ„åŠ›` `å¤šé˜Ÿåˆ—å›¾çµæœº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Transformeræ¨¡æ‹Ÿå›¾çµæœºçš„æ–¹æ³•æ¨ç†æ­¥æ•°è¿‡å¤šï¼Œæ•ˆç‡ä½ä¸‹ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ–°çš„Transformeræ¶æ„ï¼Œé€šè¿‡ä¼˜åŒ–ä¸Šä¸‹æ–‡çª—å£å’ŒCoTæ­¥éª¤ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡æ‹Ÿæ•ˆç‡ã€‚
3. è¯¥æ–¹æ³•åˆ©ç”¨å¤šé˜Ÿåˆ—å›¾çµæœºä½œä¸ºæ¡¥æ¢ï¼Œå®ç°äº†å¤šå¸¦å›¾çµæœºçš„é«˜æ•ˆæ¨¡æ‹Ÿï¼Œå¹¶éªŒè¯äº†ç¨€ç–æ³¨æ„åŠ›çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å·²çŸ¥æ’å®šæ¯”ç‰¹å¤§å°çš„Transformerå…·æœ‰å›¾çµå®Œå¤‡æ€§ï¼Œä½†ç°æœ‰æ„é€ æ–¹æ³•æ¯ä¸ªæ¨¡æ‹Ÿå›¾çµæœº(TM)æ­¥éª¤éœ€è¦Î©(s(n))ä¸ªæ€ç»´é“¾(CoT)æ­¥éª¤ï¼Œå¯¼è‡´ä¸åˆ‡å®é™…çš„æ¨ç†é•¿åº¦ã€‚æœ¬æ–‡é€šè¿‡è¯æ˜ä»»ä½•(t(n),s(n))-æœ‰ç•Œå¤šå¸¦TMéƒ½å¯ä»¥ç”¨å…·æœ‰æœ€ä½³O(s(n))é•¿åº¦ä¸Šä¸‹æ–‡çª—å£å’Œæ¯ä¸ªTMæ­¥éª¤ä»…O(s(n)^c)ä¸ªCoTæ­¥éª¤çš„æ’å®šæ¯”ç‰¹å¤§å°Transformeræ¥æ¨¡æ‹Ÿï¼Œä»è€Œæ˜¾è‘—ç¼©å°äº†è¿™ç§æ•ˆç‡å·®è·ï¼Œå…¶ä¸­é€šè¿‡å……åˆ†å¢å¤§Transformerçš„head-layer productå¯ä»¥ä½¿c>0ä»»æ„å°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ„é€ è¡¨æ˜ï¼Œå…·æœ‰å›ºå®šå‡ ä½•åç§»çš„ç¨€ç–æ³¨æ„åŠ›è¶³ä»¥è¿›è¡Œæœ‰æ•ˆçš„é€šç”¨è®¡ç®—ã€‚æˆ‘ä»¬çš„è¯æ˜åˆ©ç”¨å¤šé˜Ÿåˆ—TMä½œä¸ºæ¡¥æ¢ã€‚ä¸»è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯æ›´æœ‰æ•ˆåœ°é€šè¿‡åŒæ­¥å¤šé˜Ÿåˆ—TMæ¨¡æ‹Ÿå¤šå¸¦TMï¼Œä»è€Œåœ¨æ›´ä¸¥æ ¼çš„æ¨¡å‹å‡è®¾ä¸‹æé«˜äº†æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºTransformerçš„å›¾çµæœºæ¨¡æ‹Ÿæ–¹æ³•ï¼Œè™½ç„¶ç†è®ºä¸Šè¯æ˜äº†Transformerçš„å›¾çµå®Œå¤‡æ€§ï¼Œä½†å…¶æ¨ç†æ•ˆç‡æä½ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºä¸€ä¸ªçŠ¶æ€ç©ºé—´ä¸ºs(n)çš„å›¾çµæœºï¼Œç°æœ‰çš„æ–¹æ³•éœ€è¦Î©(s(n))çš„CoTæ­¥éª¤æ¥æ¨¡æ‹Ÿå›¾çµæœºçš„ä¸€ä¸ªæ­¥éª¤ï¼Œè¿™ä½¿å¾—æ¨¡æ‹Ÿè¿‡ç¨‹éå¸¸è€—æ—¶ï¼Œéš¾ä»¥åº”ç”¨åˆ°å®é™…é—®é¢˜ä¸­ã€‚å› æ­¤ï¼Œå¦‚ä½•é™ä½Transformeræ¨¡æ‹Ÿå›¾çµæœºçš„æ¨ç†æ­¥æ•°ï¼Œæé«˜æ¨¡æ‹Ÿæ•ˆç‡æ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å¤šé˜Ÿåˆ—å›¾çµæœºä½œä¸ºä¸­é—´æ¡¥æ¢ï¼Œå°†å¤šå¸¦å›¾çµæœºçš„æ¨¡æ‹Ÿé—®é¢˜è½¬åŒ–ä¸ºå¤šé˜Ÿåˆ—å›¾çµæœºçš„æ¨¡æ‹Ÿé—®é¢˜ï¼Œè¿›è€Œåˆ©ç”¨Transformeré«˜æ•ˆåœ°æ¨¡æ‹Ÿå¤šé˜Ÿåˆ—å›¾çµæœºã€‚é€šè¿‡è¿™ç§è½¬åŒ–ï¼Œå¯ä»¥æœ‰æ•ˆåœ°é™ä½æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­çš„æ¨ç†æ­¥æ•°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶æ¥è¿›ä¸€æ­¥æé«˜æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) å°†åŸå§‹çš„å¤šå¸¦å›¾çµæœºè½¬åŒ–ä¸ºç­‰ä»·çš„å¤šé˜Ÿåˆ—å›¾çµæœºã€‚2) è®¾è®¡ä¸€ä¸ªTransformeræ¶æ„æ¥æ¨¡æ‹Ÿå¤šé˜Ÿåˆ—å›¾çµæœºçš„è¿è¡Œã€‚3) åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–Transformerçš„è®¡ç®—æ•ˆç‡ã€‚è¯¥æ¶æ„ä¸»è¦åŒ…å«è¾“å…¥åµŒå…¥å±‚ã€Transformerå±‚ï¼ˆåŒ…æ‹¬è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œï¼‰å’Œè¾“å‡ºå±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªæ›´é«˜æ•ˆçš„å¤šå¸¦å›¾çµæœºåˆ°å¤šé˜Ÿåˆ—å›¾çµæœºçš„è½¬æ¢æ–¹æ³•ï¼Œåœ¨æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚2) è®¾è®¡äº†ä¸€ç§åŸºäºç¨€ç–æ³¨æ„åŠ›çš„Transformeræ¶æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿå¤šé˜Ÿåˆ—å›¾çµæœºçš„è¿è¡Œã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­çš„æ¨ç†æ­¥æ•°ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­Transformerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å›ºå®šå‡ ä½•åç§»çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚2) é€šè¿‡è°ƒæ•´head-layer productçš„å¤§å°ï¼Œå¯ä»¥ä½¿æ¯ä¸ªTMæ­¥éª¤çš„CoTæ­¥éª¤æ•°O(s(n)^c)ä¸­çš„cä»»æ„å°ã€‚3) ä¸Šä¸‹æ–‡çª—å£å¤§å°è®¾ç½®ä¸ºO(s(n))ï¼Œä¿è¯äº†æ¨¡æ‹Ÿè¿‡ç¨‹çš„å®Œæ•´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡è¯æ˜äº†ä»»ä½•(t(n),s(n))-æœ‰ç•Œå¤šå¸¦TMéƒ½å¯ä»¥ç”¨å…·æœ‰æœ€ä½³O(s(n))é•¿åº¦ä¸Šä¸‹æ–‡çª—å£å’Œæ¯ä¸ªTMæ­¥éª¤ä»…O(s(n)^c)ä¸ªCoTæ­¥éª¤çš„æ’å®šæ¯”ç‰¹å¤§å°Transformeræ¥æ¨¡æ‹Ÿï¼Œå…¶ä¸­é€šè¿‡å……åˆ†å¢å¤§Transformerçš„head-layer productå¯ä»¥ä½¿c>0ä»»æ„å°ã€‚è¿™æ„å‘³ç€ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ¨ç†æ­¥æ•°æ˜¾è‘—é™ä½ï¼Œæ•ˆç‡å¾—åˆ°å¤§å¹…æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç†è®ºè®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œä¾‹å¦‚éªŒè¯ç®—æ³•çš„å¤æ‚æ€§ï¼Œä»¥åŠæ¢ç´¢ç¥ç»ç½‘ç»œçš„è®¡ç®—èƒ½åŠ›è¾¹ç•Œã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨é€šç”¨äººå·¥æ™ºèƒ½é¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ï¼Œä¾‹å¦‚å¯ä»¥ç”¨äºæ„å»ºæ›´é«˜æ•ˆçš„æ¨ç†å¼•æ“ï¼Œæˆ–è€…ç”¨äºç†è§£å’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•å°†è¯¥æ–¹æ³•åº”ç”¨äºå®é™…çš„è®¡ç®—ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Constant bit-size Transformers are known to be Turing complete, but existing constructions require $Î©(s(n))$ chain-of-thought (CoT) steps per simulated Turing machine (TM) step, leading to impractical reasoning lengths. In this paper, we significantly reduce this efficiency gap by proving that any $(t(n),s(n))$-bounded multi-tape TM can be simulated by a constant bit-size Transformer with an optimal $O(s(n))$-long context window and only $O(s(n)^c)$ CoT steps per TM step, where $c>0$ can be made arbitrarily small by letting the Transformers' head-layer product sufficiently large. In addition, our construction shows that sparse attention with fixed geometric offsets suffices for efficient universal computation. Our proof leverages multi-queue TMs as a bridge. The main technical novelty is a more efficient simulation of multi-tape TMs by synchronous multi-queue TMs, improving both time and space complexity under stricter model assumptions.

