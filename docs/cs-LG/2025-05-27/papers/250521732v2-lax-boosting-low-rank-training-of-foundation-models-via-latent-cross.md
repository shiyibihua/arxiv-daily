---
layout: default
title: "LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing"
---

# LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21732" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21732v2</a>
  <a href="https://arxiv.org/pdf/2505.21732.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21732v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21732v2', 'LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruijie Zhang, Ziyue Liu, Zhengyang Wang, Zheng Zhang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-10-24)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLaXä»¥æå‡åŸºç¡€æ¨¡å‹ä½ç§©è®­ç»ƒæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©è®­ç»ƒ` `åŸºç¡€æ¨¡å‹` `ä¿¡æ¯æµåŠ¨` `å‚æ•°é«˜æ•ˆ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºç¡€æ¨¡å‹è®­ç»ƒæ–¹æ³•è®¡ç®—æˆæœ¬é«˜ï¼Œä½ç§©åˆ†è§£è™½ç„¶å‚æ•°é«˜æ•ˆï¼Œä½†æ€§èƒ½å—é™äºå‚æ•°ç©ºé—´ã€‚
2. LaXæ¨¡å—é€šè¿‡ä¿ƒè¿›ä½ç§©å­ç©ºé—´ä¹‹é—´çš„ä¿¡æ¯æµåŠ¨ï¼Œå¢å¼ºäº†ä½ç§©æ¨¡å‹çš„è¡¨ç°èƒ½åŠ›ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLaXåœ¨å¤šä¸ªæ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œä½¿ç”¨çš„å‚æ•°é‡å´å‡å°‘äº†2-3å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆå¦‚ViTså’ŒLLMsï¼‰éœ€è¦å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ä½ç§©çŸ©é˜µæˆ–å¼ é‡åˆ†è§£æä¾›äº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç”±äºå‚æ•°ç©ºé—´çš„é™åˆ¶ï¼Œå¾€å¾€ä¼šé™ä½æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†Latent Crossingï¼ˆLaXï¼‰â€”â€”ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ’ä»¶æ¨¡å—ï¼Œé€šè¿‡ä¿ƒè¿›ä½ç§©å­ç©ºé—´ä¹‹é—´çš„ä¿¡æ¯æµåŠ¨ï¼Œå¢å¼ºä½ç§©æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ViT-Base/Largeå’ŒLLaMAç±»æ¨¡å‹çš„é¢„è®­ç»ƒä»»åŠ¡ä¸­å¹¿æ³›éªŒè¯äº†LaXçš„ä¼˜åŠ¿ï¼Œç»“æœæ˜¾ç¤ºï¼ŒLaXä½¿ä½ç§©æ¨¡å‹çš„æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¿‡å…¨ç§©åŸºçº¿ï¼ŒåŒæ—¶ä½¿ç”¨2-3å€æ›´å°‘çš„å‚æ•°ã€‚åœ¨å¯¹LLaMA-7/13Bè¿›è¡Œå¾®è°ƒæ—¶ï¼Œç»“åˆä½ç§©é€‚é…å™¨ï¼ˆå¦‚LoRAï¼‰ï¼ŒLaXåœ¨ç®—æœ¯å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šå§‹ç»ˆæé«˜äº†æ€§èƒ½ï¼Œä¸”æˆæœ¬å¾®ä¹å…¶å¾®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºç¡€æ¨¡å‹è®­ç»ƒä¸­çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ç°æœ‰çš„ä½ç§©åˆ†è§£æ–¹æ³•è™½ç„¶å‚æ•°é«˜æ•ˆï¼Œä½†ç”±äºå‚æ•°ç©ºé—´çš„é™åˆ¶ï¼Œå¾€å¾€å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLaXæ¨¡å—é€šè¿‡å…è®¸ä½ç§©å­ç©ºé—´ä¹‹é—´çš„ä¿¡æ¯æµåŠ¨ï¼Œå¢å¼ºäº†ä½ç§©æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å¤æ‚çš„ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLaXæ¨¡å—å¯ä»¥ä½œä¸ºä¸€ä¸ªæ’ä»¶é›†æˆåˆ°ç°æœ‰çš„ä½ç§©æ¨¡å‹ä¸­ã€‚æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å±‚ã€ä½ç§©åˆ†è§£å±‚ã€LaXæ¨¡å—å’Œè¾“å‡ºå±‚ã€‚LaXæ¨¡å—åœ¨ä½ç§©å­ç©ºé—´ä¹‹é—´å»ºç«‹è¿æ¥ï¼Œä¿ƒè¿›ä¿¡æ¯å…±äº«ã€‚

**å…³é”®åˆ›æ–°**ï¼šLaXçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä¿¡æ¯æµåŠ¨æœºåˆ¶ï¼Œé€šè¿‡è·¨è¶Šä½ç§©å­ç©ºé—´çš„è¿æ¥ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸ä¼ ç»Ÿä½ç§©æ–¹æ³•çš„å±€é™æ€§å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸Šï¼ŒLaXæ¨¡å—çš„è®¾è®¡è€ƒè™‘äº†å‚æ•°çš„é«˜æ•ˆæ€§å’Œä¿¡æ¯æµåŠ¨çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æ—¨åœ¨æœ€å¤§åŒ–æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—çš„é«˜æ•ˆæ€§ã€‚å®éªŒä¸­ä½¿ç”¨äº†ViTå’ŒLLaMAæ¨¡å‹ï¼ŒéªŒè¯äº†è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLaXåœ¨ViT-Base/Largeå’ŒLLaMAç±»æ¨¡å‹ä¸Šï¼Œèƒ½å¤Ÿä½¿ä½ç§©æ¨¡å‹çš„æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¿‡å…¨ç§©åŸºçº¿ï¼Œä¸”å‚æ•°ä½¿ç”¨é‡å‡å°‘äº†2-3å€ã€‚åœ¨å¯¹LLaMA-7/13Bè¿›è¡Œå¾®è°ƒæ—¶ï¼ŒLaXåœ¨ç®—æœ¯å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”æˆæœ¬å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰åŸºç¡€æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒã€‚é€šè¿‡é™ä½è®¡ç®—æˆæœ¬å¹¶æå‡æ€§èƒ½ï¼ŒLaXæ¨¡å—å¯ä»¥ä½¿å¾—æ›´å¹¿æ³›çš„ç ”ç©¶è€…å’Œå¼€å‘è€…èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒå’Œåº”ç”¨å¤§å‹åŸºç¡€æ¨¡å‹ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training foundation models such as ViTs and LLMs requires tremendous computing cost. Low-rank matrix or tensor factorization offers a parameter-efficient alternative, but often downgrades performance due to the restricted parameter space. In this work, we introduce {\textbf{Latent Crossing (LaX)} } -- a simple yet effective plug-and-play module that enhances the capacity of low-rank models by enabling information flow across low-rank subspaces. We extensively validate the benefits of LaX on pre-training tasks with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters. LaX boosts low-rank model performance to match or exceed the full-rank baselines while using 2-3\(\times\) fewer parameters. When equipped with low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently improves performance on arithmetic and common sense reasoning tasks with negligible cost.

