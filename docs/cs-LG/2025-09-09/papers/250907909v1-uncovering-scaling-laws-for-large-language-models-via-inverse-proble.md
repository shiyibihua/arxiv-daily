---
layout: default
title: Uncovering Scaling Laws for Large Language Models via Inverse Problems
---

# Uncovering Scaling Laws for Large Language Models via Inverse Problems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07909" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07909v1</a>
  <a href="https://arxiv.org/pdf/2509.07909.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07909v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07909v1', 'Uncovering Scaling Laws for Large Language Models via Inverse Problems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Arun Verma, Zhaoxuan Wu, Zijian Zhou, Xiaoqiang Lin, Zhiliang Chen, Rachael Hwee Ling Sim, Rui Qiao, Jingtan Wang, Nhung Bui, Xinyuan Niu, Wenyang Hu, Gregory Kang Ruey Lau, Zi-Yu Khoo, Zitong Zhao, Xinyi Xu, Apivich Hemachandra, See-Kiong Ng, Bryan Kian Hsiang Low

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

**å¤‡æ³¨**: Accepted at EMNLP Findings 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨é€†é—®é¢˜ç†è®ºæ¢ç´¢å¤§è¯­è¨€æ¨¡å‹çš„æ‰©å±•å®šå¾‹ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ‰©å±•å®šå¾‹` `é€†é—®é¢˜` `æ¨¡å‹è®­ç»ƒ` `æˆæœ¬æ•ˆç›Š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè®­ç»ƒä¾èµ–é«˜æˆæœ¬çš„è¯•é”™æ³•ï¼Œç¼ºä¹ç†è®ºæŒ‡å¯¼ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. è¯¥è®ºæ–‡æå‡ºåˆ©ç”¨é€†é—®é¢˜ç†è®ºï¼Œä»æ¨¡å‹è¡¨ç°åæ¨æ‰©å±•å®šå¾‹ï¼ŒæŒ‡å¯¼LLMæ„å»ºã€‚
3. æœŸæœ›é€šè¿‡é€†é—®é¢˜æ–¹æ³•ï¼Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½LLMçš„è®­ç»ƒæˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯ç»è¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚è¿™äº›æˆåŠŸå¾—ç›Šäºæ•°æ®å’Œè®¡ç®—å‰æ‰€æœªæœ‰çš„å¤æ‚æ€§å’Œè§„æ¨¡ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ­¤ç±»æ¨¡å‹æˆæœ¬é«˜æ˜‚ï¼Œé€šè¿‡è›®åŠ›è¯•é”™æ¥æ”¹è¿›LLMæ˜¯ä¸å¯è¡Œçš„ã€‚å—é€†é—®é¢˜åœ¨æ­ç¤ºåŸºæœ¬ç§‘å­¦è§„å¾‹æ–¹é¢çš„æˆåŠŸå¯å‘ï¼Œæœ¬æ–‡å€¡å¯¼é€†é—®é¢˜ä¹Ÿå¯ä»¥æœ‰æ•ˆåœ°æ­ç¤ºæ‰©å±•å®šå¾‹ï¼ŒæŒ‡å¯¼æ„å»ºLLMï¼Œä»¥æ˜¾è‘—æé«˜æˆæœ¬æ•ˆç›Šçš„æ–¹å¼å®ç°ç†æƒ³çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒé¢ä¸´ç€æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚ç”±äºæ¨¡å‹è§„æ¨¡åºå¤§ï¼Œè®­ç»ƒæ•°æ®é‡å·¨å¤§ï¼Œä¼ ç»Ÿçš„è¯•é”™æ–¹æ³•åœ¨å¯»æ‰¾æœ€ä¼˜æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥æ—¶æ•ˆç‡æä½ã€‚ç°æœ‰çš„æ–¹æ³•ç¼ºä¹ç†è®ºæŒ‡å¯¼ï¼Œéš¾ä»¥é¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸å„ç§å› ç´ ï¼ˆå¦‚æ¨¡å‹å¤§å°ã€æ•°æ®é‡ã€è®¡ç®—èµ„æºï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œå¯¼è‡´èµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMçš„è®­ç»ƒè¿‡ç¨‹è§†ä¸ºä¸€ä¸ªé€†é—®é¢˜ã€‚å³ï¼Œä¸æ˜¯ç›´æ¥é€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°æ¥ä¼˜åŒ–æ€§èƒ½ï¼Œè€Œæ˜¯é¦–å…ˆè§‚å¯Ÿæ¨¡å‹çš„æ€§èƒ½è¡¨ç°ï¼Œç„¶ååˆ©ç”¨é€†é—®é¢˜ç†è®ºåæ¨å‡ºå½±å“æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ å’Œæ‰©å±•å®šå¾‹ã€‚é€šè¿‡ç†è§£è¿™äº›æ‰©å±•å®šå¾‹ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼LLMçš„æ„å»ºå’Œè®­ç»ƒï¼Œä»è€Œé™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥è®ºæ–‡æ˜¯ä¸€ç¯‡position paperï¼Œä¸»è¦æå‡ºäº†ä¸€ä¸ªç ”ç©¶æ–¹å‘å’Œæ€è·¯ï¼Œå¹¶æ²¡æœ‰å…·ä½“çš„ç®—æ³•æˆ–æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåœ¨äºå€Ÿé‰´é€†é—®é¢˜çš„æ€æƒ³ï¼Œå»ºç«‹LLMæ€§èƒ½ä¸æ¨¡å‹å‚æ•°ã€æ•°æ®è§„æ¨¡ã€è®¡ç®—èµ„æºç­‰å› ç´ ä¹‹é—´çš„æ•°å­¦æ¨¡å‹ã€‚æœªæ¥çš„ç ”ç©¶å¯èƒ½åŒ…æ‹¬ä»¥ä¸‹é˜¶æ®µï¼š1) æ”¶é›†LLMè®­ç»ƒæ•°æ®ï¼ŒåŒ…æ‹¬æ¨¡å‹å¤§å°ã€è®­ç»ƒæ•°æ®é‡ã€è®¡ç®—èµ„æºæ¶ˆè€—å’Œæ¨¡å‹æ€§èƒ½æŒ‡æ ‡ï¼›2) å»ºç«‹LLMæ€§èƒ½çš„æ•°å­¦æ¨¡å‹ï¼Œå°†æ¨¡å‹æ€§èƒ½è¡¨ç¤ºä¸ºæ¨¡å‹å‚æ•°ã€æ•°æ®è§„æ¨¡å’Œè®¡ç®—èµ„æºçš„å‡½æ•°ï¼›3) åˆ©ç”¨é€†é—®é¢˜ç†è®ºï¼Œä»æ¨¡å‹æ€§èƒ½æ•°æ®åæ¨å‡ºæ¨¡å‹å‚æ•°ã€æ•°æ®è§„æ¨¡å’Œè®¡ç®—èµ„æºä¹‹é—´çš„å…³ç³»ï¼Œå³æ‰©å±•å®šå¾‹ï¼›4) åŸºäºæ‰©å±•å®šå¾‹ï¼Œè®¾è®¡æ›´æœ‰æ•ˆçš„LLMè®­ç»ƒç­–ç•¥ï¼Œä¾‹å¦‚ï¼Œç¡®å®šæœ€ä¼˜çš„æ¨¡å‹å¤§å°å’Œæ•°æ®é‡ï¼Œä»¥åœ¨ç»™å®šçš„è®¡ç®—èµ„æºä¸‹å®ç°æœ€ä½³æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†é€†é—®é¢˜ç†è®ºå¼•å…¥åˆ°LLMçš„ç ”ç©¶ä¸­ã€‚ä¸ä¼ ç»Ÿçš„è¯•é”™æ–¹æ³•ä¸åŒï¼Œé€†é—®é¢˜æ–¹æ³•å¯ä»¥ä»æ¨¡å‹è¡¨ç°åæ¨å‡ºæ‰©å±•å®šå¾‹ï¼Œä»è€Œä¸ºLLMçš„æ„å»ºå’Œè®­ç»ƒæä¾›ç†è®ºæŒ‡å¯¼ã€‚è¿™ç§æ–¹æ³•æœ‰æœ›æ˜¾è‘—é™ä½LLMçš„è®­ç»ƒæˆæœ¬ï¼Œå¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šç”±äºæ˜¯position paperï¼Œæ²¡æœ‰å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ã€‚æœªæ¥çš„ç ”ç©¶éœ€è¦æ ¹æ®å…·ä½“çš„LLMæ¶æ„å’Œè®­ç»ƒä»»åŠ¡ï¼Œè®¾è®¡åˆé€‚çš„æ•°å­¦æ¨¡å‹å’Œé€†é—®é¢˜æ±‚è§£æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨å›å½’åˆ†æã€ç¥ç»ç½‘ç»œç­‰æ–¹æ³•æ¥å»ºç«‹LLMæ€§èƒ½çš„æ•°å­¦æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–ç®—æ³•æ¥æ±‚è§£é€†é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡æ˜¯ä¸€ç¯‡position paperï¼Œæ²¡æœ‰å…·ä½“çš„å®éªŒç»“æœã€‚å…¶äº®ç‚¹åœ¨äºæå‡ºäº†åˆ©ç”¨é€†é—®é¢˜ç†è®ºç ”ç©¶LLMæ‰©å±•å®šå¾‹çš„æ–°æ€è·¯ï¼Œæœ‰æœ›ä¸ºLLMçš„æ„å»ºå’Œè®­ç»ƒæä¾›ç†è®ºæŒ‡å¯¼ï¼Œå¹¶æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ã€‚æœªæ¥çš„ç ”ç©¶éœ€è¦é€šè¿‡å®éªŒéªŒè¯è¯¥æ€è·¯çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ¢ç´¢å…·ä½“çš„é€†é—®é¢˜æ±‚è§£æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æ­ç¤ºLLMçš„æ‰©å±•å®šå¾‹ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°æ„å»ºå’Œè®­ç»ƒLLMï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚è¿™å¯¹äºèµ„æºæœ‰é™çš„ç ”ç©¶æœºæ„å’Œä¼ä¸šå°¤å…¶é‡è¦ï¼Œå¯ä»¥å¸®åŠ©ä»–ä»¬ä»¥æ›´ä½çš„æˆæœ¬æ„å»ºé«˜æ€§èƒ½çš„LLMã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨LLMåœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are large-scale pretrained models that have achieved remarkable success across diverse domains. These successes have been driven by unprecedented complexity and scale in both data and computations. However, due to the high costs of training such models, brute-force trial-and-error approaches to improve LLMs are not feasible. Inspired by the success of inverse problems in uncovering fundamental scientific laws, this position paper advocates that inverse problems can also efficiently uncover scaling laws that guide the building of LLMs to achieve the desirable performance with significantly better cost-effectiveness.

