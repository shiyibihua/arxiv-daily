---
layout: default
title: Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks
---

# Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08635" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08635v1</a>
  <a href="https://arxiv.org/pdf/2508.08635.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08635v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08635v1', 'Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adit Krishnan, Chu Wang, Chris Kong

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: 10 pages, 4 figures, currently under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¨€ç–å¾®è°ƒä¸è‡ªé€‚åº”æ ‡è®°åŒ–ç»“åˆçš„æ–¹æ³•ä»¥è§£å†³ä¸“ä¸šåˆ†ç±»ä»»åŠ¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­ä¹‰æ–‡æœ¬åˆ†ç±»` `ç¨€ç–å¾®è°ƒ` `å°å‹è¯­è¨€æ¨¡å‹` `ä¸“ä¸šåˆ†ç±»ä»»åŠ¡` `æ¨¡å‹ä¼˜åŒ–` `æ¨ç†é€Ÿåº¦` `é¢†åŸŸä¸“å®¶æ ‡æ³¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­ä¹‰åˆ†ç±»æ–¹æ³•å¾€å¾€ä¾èµ–äºé¢†åŸŸä¸“å®¶çš„æ ‡æ³¨ï¼Œä¸”æ¨ç†é€Ÿåº¦æ…¢ï¼Œé™åˆ¶äº†æ¨¡å‹çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨€ç–å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡æ ‡è®°é©±åŠ¨çš„æ–¹å¼é€‚åº”å°å‹è¯­è¨€æ¨¡å‹ï¼Œé¿å…äº†å¼•å…¥é¢å¤–å‚æ•°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè®­ç»ƒæˆæœ¬ä»…ä¸ºä¼ ç»Ÿæ–¹æ³•çš„ä¸€åŠï¼Œä¸”ç¨³å®šæ€§æ›´é«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­ä¹‰æ–‡æœ¬åˆ†ç±»éœ€è¦ç†è§£ç‰¹å®šæ ‡è®°çš„ä¸Šä¸‹æ–‡æ„ä¹‰ï¼Œè€Œä¸ä»…ä»…æ˜¯è¡¨é¢æ¨¡å¼æˆ–å…³é”®è¯ï¼Œè¿™ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éå¸¸é€‚åˆæ­¤ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¡Œä¸šä¸­çš„è¯­ä¹‰åˆ†ç±»åº”ç”¨é€šå¸¸é«˜åº¦ä¸“ä¸šåŒ–ï¼Œéœ€è¦é¢†åŸŸä¸“å®¶è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ä¸”é€šå¸¸è¦æ±‚é«˜æ¨ç†ååé‡ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„è§„æ¨¡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ ‡è®°é©±åŠ¨çš„ç¨€ç–å¾®è°ƒç­–ç•¥ï¼Œæ—¨åœ¨å°†å°å‹è¯­è¨€æ¨¡å‹é€‚åº”äºä¸“ä¸šåˆ†ç±»ä»»åŠ¡ã€‚é€šè¿‡åˆ©ç”¨ä»»åŠ¡ç‰¹å®šçš„æ ‡è®°æ„é€ ï¼Œè¯†åˆ«å¹¶å¾®è°ƒæ¨¡å‹å‚æ•°çš„æ•æ„Ÿå­é›†ï¼ŒåŒæ—¶ä¿æŒå¤§éƒ¨åˆ†é¢„è®­ç»ƒæƒé‡ä¸å˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªä¸åŒçš„è¯­ä¹‰åˆ†ç±»ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç«¯åˆ°ç«¯å¾®è°ƒã€LoRAã€å±‚é€‰æ‹©å’Œå‰ç¼€è°ƒä¼˜ï¼Œä¸”è®­ç»ƒæˆæœ¬é™ä½äº†ä¸€åŠã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¯­ä¹‰æ–‡æœ¬åˆ†ç±»ä¸­ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸“ä¸šé¢†åŸŸä¸­å¯¹æ¨¡å‹å¤§å°å’Œæ¨ç†é€Ÿåº¦çš„è¦æ±‚ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡çš„é¢†åŸŸä¸“å®¶æ ‡æ³¨ï¼Œå¹¶ä¸”æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ ‡è®°é©±åŠ¨çš„ç¨€ç–å¾®è°ƒç­–ç•¥ï¼Œä¸“æ³¨äºå¾®è°ƒå°å‹è¯­è¨€æ¨¡å‹çš„æ•æ„Ÿå‚æ•°ï¼Œè€Œä¸æ”¹å˜å¤§éƒ¨åˆ†é¢„è®­ç»ƒæƒé‡ã€‚è¿™ç§æ–¹æ³•é€šè¿‡è¯†åˆ«ä¸ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰æ ‡è®°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ ‡è®°è¯†åˆ«ã€å‚æ•°å¾®è°ƒå’Œæ¨¡å‹è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ†æä»»åŠ¡ç‰¹å®šçš„æ•°æ®é›†ï¼Œè¯†åˆ«å‡ºå…³é”®çš„è¯­ä¹‰æ ‡è®°ï¼›ç„¶åï¼Œé’ˆå¯¹è¿™äº›æ ‡è®°å¾®è°ƒæ¨¡å‹çš„ç›¸å…³å‚æ•°ï¼Œæœ€åè¿›è¡Œæ¨¡å‹çš„æ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„åˆ›æ–°åœ¨äºä¸å¼•å…¥é¢å¤–çš„å‚æ•°ï¼Œè€Œæ˜¯é€šè¿‡ç¨€ç–å¾®è°ƒç°æœ‰æ¨¡å‹çš„æ•æ„Ÿå‚æ•°æ¥å®ç°æ€§èƒ½æå‡ã€‚è¿™ä¸ä¼ ç»Ÿçš„é€‚é…å™¨æ–¹æ³•ï¼ˆå¦‚LoRAï¼‰å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ä¸ä»»åŠ¡ç›¸å…³çš„å‚æ•°ï¼ŒåŒæ—¶ä¿æŒå¤§éƒ¨åˆ†é¢„è®­ç»ƒæƒé‡ä¸å˜ã€‚æ¨¡å‹ç»“æ„ä¸Šï¼Œé€‰æ‹©äº†å°å‹è¯­è¨€æ¨¡å‹ä½œä¸ºåŸºç¡€ï¼Œç¡®ä¿åœ¨æ¨ç†é€Ÿåº¦å’Œæˆæœ¬ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•åœ¨äº”ä¸ªä¸åŒçš„è¯­ä¹‰åˆ†ç±»ä»»åŠ¡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„ç«¯åˆ°ç«¯å¾®è°ƒã€LoRAã€å±‚é€‰æ‹©å’Œå‰ç¼€è°ƒä¼˜ï¼Œè®­ç»ƒæˆæœ¬é™ä½è‡³ä¸€åŠï¼Œä¸”æ¨¡å‹ç¨³å®šæ€§æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸“ä¸šåˆ†ç±»ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®¢æˆ·æ„å›¾æ£€æµ‹ã€è¯­ä¹‰è§’è‰²æ ‡æ³¨ç­‰ä¸“ä¸šåˆ†ç±»ä»»åŠ¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šè¡Œä¸šä¸­æ¨å¹¿ï¼Œå¸®åŠ©ä¼ä¸šå¿«é€Ÿé€‚åº”å¸‚åœºå˜åŒ–ï¼Œæå‡æœåŠ¡è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Semantic text classification requires the understanding of the contextual significance of specific tokens rather than surface-level patterns or keywords (as in rule-based or statistical text classification), making large language models (LLMs) well-suited for this task. However, semantic classification applications in industry, like customer intent detection or semantic role labeling, tend to be highly specialized. They require annotation by domain experts in contrast to general-purpose corpora for pretraining. Further, they typically require high inference throughputs which limits the model size from latency and cost perspectives. Thus, for a range of specialized classification tasks, the preferred solution is to develop customized classifiers by finetuning smaller language models (e.g., mini-encoders, small language models).
>   In this work, we develop a token-driven sparse finetuning strategy to adapt small language models to specialized classification tasks. We identify and finetune a small sensitive subset of model parameters by leveraging task-specific token constructs in the finetuning dataset, while leaving most of the pretrained weights unchanged. Unlike adapter approaches such as low rank adaptation (LoRA), we do not introduce additional parameters to the model. Our approach identifies highly relevant semantic tokens (case study in the Appendix) and outperforms end-to-end finetuning, LoRA, layer selection, and prefix tuning on five diverse semantic classification tasks. We achieve greater stability and half the training costs vs. end-to-end finetuning.

