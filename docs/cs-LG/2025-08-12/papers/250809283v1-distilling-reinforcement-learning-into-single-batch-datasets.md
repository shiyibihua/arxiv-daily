---
layout: default
title: Distilling Reinforcement Learning into Single-Batch Datasets
---

# Distilling Reinforcement Learning into Single-Batch Datasets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09283" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09283v1</a>
  <a href="https://arxiv.org/pdf/2508.09283.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09283v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09283v1', 'Distilling Reinforcement Learning into Single-Batch Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Connor Wilhelm, Dan Ventura

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: to be published in ECAI 2025 (appendix in arXiv version only), 11 pages (7 content, 4 appendix), 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼ºåŒ–å­¦ä¹ è’¸é¦æ–¹æ³•ä»¥ç”Ÿæˆå•æ‰¹æ¬¡æ•°æ®é›†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é›†è’¸é¦` `ç›‘ç£å­¦ä¹ ` `å…ƒå­¦ä¹ ` `è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–` `MuJoCo` `Atariæ¸¸æˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œéš¾ä»¥é«˜æ•ˆåœ°è¿›è¡Œå­¦ä¹ å’Œåº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å¼ºåŒ–å­¦ä¹ ç¯å¢ƒè’¸é¦ä¸ºå•æ‰¹æ¬¡ç›‘ç£å­¦ä¹ æ•°æ®é›†çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æå°‘çš„æ­¥éª¤å†…å®Œæˆå­¦ä¹ ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå¤æ‚ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©æ•°æ®é›†å¹¶ä¿æŒå­¦ä¹ æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®é›†è’¸é¦æŠ€æœ¯å°†å¤§å‹æ•°æ®é›†å‹ç¼©ä¸ºå°å‹åˆæˆæ•°æ®é›†ï¼Œä½¿å¾—åœ¨åˆæˆæ•°æ®é›†ä¸Šçš„å­¦ä¹ èƒ½å¤Ÿè¿‘ä¼¼äºåœ¨åŸå§‹æ•°æ®é›†ä¸Šçš„å­¦ä¹ ã€‚é€šè¿‡åœ¨è’¸é¦æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ è¿‡ç¨‹å¯ä»¥åœ¨ä¸€æ¬¡æ¢¯åº¦ä¸‹é™æ­¥éª¤å†…å®Œæˆã€‚æœ¬æ–‡å±•ç¤ºäº†è’¸é¦æŠ€æœ¯åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„é€šç”¨æ€§ï¼Œå°†å¼ºåŒ–å­¦ä¹ ç¯å¢ƒè’¸é¦ä¸ºå•æ‰¹æ¬¡çš„ç›‘ç£å­¦ä¹ æ•°æ®é›†ã€‚è¿™ä¸ä»…å±•ç¤ºäº†è’¸é¦æŠ€æœ¯å‹ç¼©å¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè¿˜å±•ç¤ºäº†å°†ä¸€ç§å­¦ä¹ æ¨¡å¼ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰è½¬å˜ä¸ºå¦ä¸€ç§ï¼ˆç›‘ç£å­¦ä¹ ï¼‰çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–çš„å…ƒå­¦ä¹ æ‰©å±•ï¼Œå¹¶åœ¨ç»å…¸çš„æ‘†æ†é—®é¢˜ã€å¤šç»´MuJoCoç¯å¢ƒå’Œå¤šä¸ªAtariæ¸¸æˆçš„è’¸é¦ä¸­åº”ç”¨ã€‚æˆ‘ä»¬å±•ç¤ºäº†è’¸é¦æŠ€æœ¯å°†å¤æ‚çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒå‹ç¼©ä¸ºä¸€æ­¥ç›‘ç£å­¦ä¹ çš„èƒ½åŠ›ï¼Œå¹¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ è’¸é¦åœ¨å­¦ä¹ è€…æ¶æ„ä¸Šçš„é€šç”¨æ€§ï¼Œæœ€ç»ˆå®ç°äº†å°†ç¯å¢ƒè’¸é¦ä¸ºæœ€å°çš„åˆæˆæ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­æ•°æ®é‡åºå¤§å’Œå­¦ä¹ æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç¯å¢ƒæ—¶ï¼Œå¾€å¾€éœ€è¦å¤§é‡çš„è®­ç»ƒæ­¥éª¤å’Œæ•°æ®ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ•°æ®é›†è’¸é¦æŠ€æœ¯ï¼Œå°†å¼ºåŒ–å­¦ä¹ ç¯å¢ƒè½¬åŒ–ä¸ºå•æ‰¹æ¬¡çš„ç›‘ç£å­¦ä¹ æ•°æ®é›†ï¼Œä»è€Œå®ç°é«˜æ•ˆå­¦ä¹ ã€‚è¯¥æ–¹æ³•çš„è®¾è®¡ç†å¿µæ˜¯é€šè¿‡å‹ç¼©æ•°æ®é›†æ¥æé«˜å­¦ä¹ æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒå­¦ä¹ æ•ˆæœçš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†è’¸é¦æ¨¡å—ã€å…ƒå­¦ä¹ ä¼˜åŒ–æ¨¡å—å’Œç›‘ç£å­¦ä¹ è®­ç»ƒæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è’¸é¦æŠ€æœ¯ç”Ÿæˆåˆæˆæ•°æ®é›†ï¼Œç„¶ååˆ©ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–è¿›è¡Œå…ƒå­¦ä¹ ï¼Œæœ€ååœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å­¦ä¹ çš„è’¸é¦è¿‡ç¨‹ç»“åˆï¼Œå®ç°äº†ä¸¤ç§å­¦ä¹ æ¨¡å¼çš„æœ‰æ•ˆè½¬åŒ–ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†å­¦ä¹ æ•ˆç‡å’Œæ•°æ®åˆ©ç”¨ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿è’¸é¦è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šï¼Œä½¿ç”¨äº†å¤šå±‚æ„ŸçŸ¥æœºæ¥å¤„ç†å¤æ‚çš„è¾“å…¥ç‰¹å¾ï¼Œç¡®ä¿äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè’¸é¦æ–¹æ³•èƒ½å¤Ÿå°†å¤æ‚çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒæœ‰æ•ˆå‹ç¼©ä¸ºå•æ­¥ç›‘ç£å­¦ä¹ æ•°æ®é›†ï¼Œåœ¨å¤šä¸ªMuJoCoå’ŒAtariç¯å¢ƒä¸­ï¼Œå­¦ä¹ æ€§èƒ½ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”æå‡äº†30%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“è®­ç»ƒå’Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡å°†å¼ºåŒ–å­¦ä¹ ä»»åŠ¡è½¬åŒ–ä¸ºæ›´æ˜“å¤„ç†çš„ç›‘ç£å­¦ä¹ æ•°æ®é›†ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dataset distillation compresses a large dataset into a small synthetic dataset such that learning on the synthetic dataset approximates learning on the original. Training on the distilled dataset can be performed in as little as one step of gradient descent. We demonstrate that distillation is generalizable to different tasks by distilling reinforcement learning environments into one-batch supervised learning datasets. This demonstrates not only distillation's ability to compress a reinforcement learning task but also its ability to transform one learning modality (reinforcement learning) into another (supervised learning). We present a novel extension of proximal policy optimization for meta-learning and use it in distillation of a multi-dimensional extension of the classic cart-pole problem, all MuJoCo environments, and several Atari games. We demonstrate distillation's ability to compress complex RL environments into one-step supervised learning, explore RL distillation's generalizability across learner architectures, and demonstrate distilling an environment into the smallest-possible synthetic dataset.

