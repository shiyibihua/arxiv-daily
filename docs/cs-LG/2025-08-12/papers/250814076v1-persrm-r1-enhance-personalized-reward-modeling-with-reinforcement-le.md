---
layout: default
title: PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning
---

# PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14076" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14076v1</a>
  <a href="https://arxiv.org/pdf/2508.14076.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14076v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14076v1', 'PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mengdi Li, Guanqiao Chen, Xufeng Zhao, Haochen Wen, Shu Yang, Di Wang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPersRM-R1ä»¥è§£å†³ä¸ªæ€§åŒ–å¥–åŠ±å»ºæ¨¡ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–å¥–åŠ±å»ºæ¨¡` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®ç¨€ç¼º` `æ¨ç†æ¡†æ¶` `åˆæˆæ•°æ®ç”Ÿæˆ` `ç”¨æˆ·åå¥½` `æ¨¡å‹æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±æ¨¡å‹åœ¨æ•æ‰ç”¨æˆ·ç‰¹å®šåå¥½æ—¶é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œé¢†åŸŸå¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚
2. PersRM-R1é€šè¿‡æ¨ç†åŸºç¡€çš„æ¡†æ¶ï¼Œåˆ©ç”¨å°‘é‡ç¤ºä¾‹æ¥è¯†åˆ«å’Œè¡¨ç¤ºä¸ªæ€§åŒ–å› ç´ ï¼Œç»“åˆåˆæˆæ•°æ®ç”Ÿæˆå’Œä¸¤é˜¶æ®µè®­ç»ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPersRM-R1åœ¨å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šè¶…è¶Šäº†åŒç±»æ¨¡å‹ï¼Œè¡¨ç°å‡ºä¸æ›´å¤§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰æ˜¯ç°æœ‰åè®­ç»ƒæ–¹æ³•çš„æ ¸å¿ƒï¼Œæ—¨åœ¨é€šè¿‡æä¾›åé¦ˆä¿¡å·æ¥å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä¸äººç±»ä»·å€¼è§‚ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RMsåœ¨æ•æ‰ç»†å¾®çš„ç”¨æˆ·ç‰¹å®šåå¥½æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æœ‰é™å’Œé¢†åŸŸå¤šæ ·çš„æƒ…å†µä¸‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†PersRM-R1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºæ¨ç†çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œä¸“é—¨è®¾è®¡ç”¨äºä»ä¸€åˆ°å‡ ä¸ªä¸ªäººç¤ºä¾‹ä¸­è¯†åˆ«å’Œè¡¨ç¤ºä¸ªäººå› ç´ ã€‚ä¸ºäº†è§£å†³æ•°æ®å¯ç”¨æ€§æœ‰é™å’Œå¼ºæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åˆæˆæ•°æ®ç”Ÿæˆä¸ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPersRM-R1åœ¨å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šè¶…è¶Šäº†åŒç±»æ¨¡å‹ï¼Œå¹¶ä¸æ›´å¤§æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œä¸ºæ›´æœ‰æ•ˆçš„ä¸ªæ€§åŒ–LLMé“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨æ•æ‰ç”¨æˆ·ç‰¹å®šåå¥½æ—¶é¢ä¸´çš„æ•°æ®ç¨€ç¼ºå’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤šæ ·åŒ–é¢†åŸŸä¸­éš¾ä»¥æœ‰æ•ˆå¯¹é½LLMè¾“å‡ºä¸ç”¨æˆ·ä»·å€¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPersRM-R1çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¨ç†åŸºç¡€çš„æ¡†æ¶ï¼Œä»ä¸€åˆ°å‡ ä¸ªä¸ªäººç¤ºä¾‹ä¸­æå–ä¸ªæ€§åŒ–å› ç´ ï¼Œç»“åˆåˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆè¿›è¡Œç›‘ç£å¾®è°ƒä»¥å­¦ä¹ åŸºç¡€ç‰¹å¾ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å¾®è°ƒè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ï¼Œç¡®ä¿å…¶èƒ½å¤Ÿé€‚åº”ç”¨æˆ·ç‰¹å®šçš„åé¦ˆä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šPersRM-R1çš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨ç†åŸºç¡€çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æœ‰æ•ˆæ•æ‰ä¸ªæ€§åŒ–åå¥½ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤§é‡æ•°æ®çš„æœ¬è´¨åŒºåˆ«æ˜¾è‘—ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ä»¥æ‰©å±•è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†ç”¨æˆ·åé¦ˆçš„æƒé‡ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹ä¸ªæ€§åŒ–ä¿¡å·çš„æ•æ„Ÿæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPersRM-R1åœ¨å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºåŒç±»æ¨¡å‹ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æ ‡å‡†æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡æå‡äº†çº¦15%ï¼Œå¹¶ä¸”åœ¨ä¸æ›´å¤§æ¨¡å‹çš„å¯¹æ¯”ä¸­ï¼Œæ€§èƒ½ç›¸å½“ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸ªæ€§åŒ–å¥–åŠ±å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿã€æ™ºèƒ½åŠ©æ‰‹å’Œç”¨æˆ·äº¤äº’ç•Œé¢ç­‰ã€‚é€šè¿‡æ›´å¥½åœ°ç†è§£ç”¨æˆ·åå¥½ï¼ŒPersRM-R1èƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨ä¸ªæ€§åŒ–æœåŠ¡çš„å‘å±•ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€å¨±ä¹å’Œå•†ä¸šç­‰å¤šä¸ªé¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reward models (RMs), which are central to existing post-training methods, aim to align LLM outputs with human values by providing feedback signals during fine-tuning. However, existing RMs struggle to capture nuanced, user-specific preferences, especially under limited data and across diverse domains. Thus, we introduce PersRM-R1, the first reasoning-based reward modeling framework specifically designed to identify and represent personal factors from only one or a few personal exemplars. To address challenges including limited data availability and the requirement for robust generalization, our approach combines synthetic data generation with a two-stage training pipeline consisting of supervised fine-tuning followed by reinforcement fine-tuning. Experimental results demonstrate that PersRM-R1 outperforms existing models of similar size and matches the performance of much larger models in both accuracy and generalizability, paving the way for more effective personalized LLMs.

