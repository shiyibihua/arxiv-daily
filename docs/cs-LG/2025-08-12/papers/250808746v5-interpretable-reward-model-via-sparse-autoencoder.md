---
layout: default
title: Interpretable Reward Model via Sparse Autoencoder
---

# Interpretable Reward Model via Sparse Autoencoder

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08746" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08746v5</a>
  <a href="https://arxiv.org/pdf/2508.08746.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08746v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08746v5', 'Interpretable Reward Model via Sparse Autoencoder')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuyi Zhang, Wei Shi, Sihang Li, Jiayi Liao, Hengxing Cai, Xiang Wang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12 (æ›´æ–°: 2025-11-25)

**å¤‡æ³¨**: AAAI 2026 Oral

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/schrieffer-z/sarm)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¨€ç–è‡ªç¼–ç å™¨å¢å¼ºçš„å¥–åŠ±æ¨¡å‹ä»¥è§£å†³ä¼ ç»Ÿæ¨¡å‹å¯è§£é‡Šæ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±æ¨¡å‹` `å¯è§£é‡Šæ€§` `ç¨€ç–è‡ªç¼–ç å™¨` `äººç±»åé¦ˆ` `å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€è°ƒæ•´` `ç‰¹å¾å½’å› `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œæ— æ³•æ¸…æ™°å±•ç¤ºå¥–åŠ±åˆ†é…çš„åŸå› ï¼Œä¸”å¯¹ç”¨æˆ·åå¥½çš„å˜åŒ–é€‚åº”æ€§å·®ã€‚
2. æœ¬æ–‡æå‡ºçš„ç¨€ç–è‡ªç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSARMï¼‰é€šè¿‡é›†æˆé¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼Œæä¾›äº†å¯è§£é‡Šçš„å¥–åŠ±åˆ†é…æœºåˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSARMåœ¨ç‰¹å¾çº§å½’å› ã€åå¥½åŠ¨æ€è°ƒæ•´æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨å¯¹é½æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åˆ©ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰ä½œä¸ºäººç±»åå¥½çš„ä»£ç†ï¼Œä»¥ä½¿LLMè¡Œä¸ºä¸äººç±»ä»·å€¼è§‚ä¿æŒä¸€è‡´ï¼Œå› æ­¤RMsçš„å‡†ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§å¯¹æœ‰æ•ˆå¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»ŸRMsç¼ºä¹å¯è§£é‡Šæ€§ï¼Œæ— æ³•æä¾›å¥–åŠ±åˆ†é…èƒŒåçš„æ¨ç†ï¼Œå¹¶ä¸”å¯¹ç”¨æˆ·åå¥½çš„å˜åŒ–ä¸å¤Ÿçµæ´»ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–è‡ªç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSARMï¼‰ï¼Œè¯¥æ¨¡å‹å°†é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰é›†æˆåˆ°å¥–åŠ±æ¨¡å‹ä¸­ã€‚SARMå°†åŸºäºLLMçš„RMçš„éšè—æ¿€æ´»æ˜ å°„åˆ°ä¸€ä¸ªå¯è§£é‡Šçš„ã€ç¨€ç–çš„ã€å•ä¹‰çš„ç‰¹å¾ç©ºé—´ï¼Œä»ä¸­ä¸€ä¸ªæ ‡é‡å¤´èšåˆç‰¹å¾æ¿€æ´»ä»¥ç”Ÿæˆé€æ˜ä¸”å…·æœ‰æ¦‚å¿µæ„ä¹‰çš„å¥–åŠ±åˆ†æ•°ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒSARMèƒ½å¤Ÿç›´æ¥è¿›è¡Œå¥–åŠ±åˆ†é…çš„ç‰¹å¾çº§å½’å› ï¼Œå…è®¸å¯¹åå¥½å˜åŒ–è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œå¹¶ä¸”åœ¨å¯¹é½æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹åœ¨å¯è§£é‡Šæ€§å’Œçµæ´»æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç”¨æˆ·åå¥½å˜åŒ–æ—¶çš„é€‚åº”èƒ½åŠ›å·®çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œå°†å¥–åŠ±æ¨¡å‹çš„éšè—æ¿€æ´»æ˜ å°„åˆ°ä¸€ä¸ªå¯è§£é‡Šçš„ç‰¹å¾ç©ºé—´ï¼Œä»è€Œå®ç°å¯¹å¥–åŠ±åˆ†é…çš„é€æ˜åŒ–å’Œå¯è§£é‡ŠåŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSARMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªé¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨æ¨¡å—å’Œä¸€ä¸ªæ ‡é‡å¤´ï¼Œåè€…è´Ÿè´£èšåˆç‰¹å¾æ¿€æ´»ä»¥ç”Ÿæˆå¥–åŠ±åˆ†æ•°ã€‚æ•´ä¸ªæµç¨‹ä»è¾“å…¥ç‰¹å¾å¼€å§‹ï¼Œé€šè¿‡SAEè¿›è¡Œå¤„ç†ï¼Œæœ€åè¾“å‡ºå¯è§£é‡Šçš„å¥–åŠ±åˆ†æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šSARMçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†ç¨€ç–è‡ªç¼–ç å™¨ä¸å¥–åŠ±æ¨¡å‹ç»“åˆï¼Œæä¾›äº†ç‰¹å¾çº§çš„å½’å› èƒ½åŠ›ï¼Œå¹¶ä¸”èƒ½å¤ŸåŠ¨æ€è°ƒæ•´ä»¥é€‚åº”ç”¨æˆ·åå¥½çš„å˜åŒ–ï¼Œè¿™åœ¨ä¼ ç»Ÿæ¨¡å‹ä¸­æ˜¯éš¾ä»¥å®ç°çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒSARMä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç‰¹å¾ç¨€ç–æ€§ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„æ¥å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œç¡®ä¿è¾“å‡ºçš„å¥–åŠ±åˆ†æ•°å…·æœ‰æ˜ç¡®çš„æ¦‚å¿µæ„ä¹‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSARMåœ¨ç‰¹å¾çº§å½’å› æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿç›´æ¥æä¾›å¥–åŠ±åˆ†é…çš„è§£é‡Šã€‚æ­¤å¤–ï¼ŒSARMåœ¨å¯¹é½æ€§èƒ½ä¸Šç›¸è¾ƒäºä¼ ç»Ÿå¥–åŠ±æ¨¡å‹æå‡äº†çº¦20%ï¼Œå¹¶ä¸”åœ¨ç”¨æˆ·åå¥½åŠ¨æ€è°ƒæ•´çš„èƒ½åŠ›ä¸Šä¹Ÿæ˜¾è‘—å¢å¼ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒç­‰ã€‚é€šè¿‡æé«˜å¥–åŠ±æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œçµæ´»æ€§ï¼ŒSARMèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç”¨æˆ·éœ€æ±‚ï¼Œæå‡ç³»ç»Ÿçš„ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align LLM behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of LLM-based RM into an interpretable, sparse, and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at https://github.com/schrieffer-z/sarm.

