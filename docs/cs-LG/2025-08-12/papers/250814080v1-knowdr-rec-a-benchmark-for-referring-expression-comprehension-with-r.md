---
layout: default
title: KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge
---

# KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14080" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14080v1</a>
  <a href="https://arxiv.org/pdf/2508.14080.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14080v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14080v1', 'KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guanghao Jin, Jingpei Wu, Tianpei Guo, Yiyi Niu, Weidong Zhou, Guoyang Liu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKnowDR-RECä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è§†è§‰å®šä½` `çŸ¥è¯†é©±åŠ¨` `æŠ—å¹»è§‰èƒ½åŠ›` `åŸºå‡†è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RECåŸºå‡†æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚
2. æœ¬æ–‡æå‡ºKnowDR-RECåŸºå‡†ï¼Œå¼ºè°ƒåŸºäºçœŸå®ä¸–ç•ŒçŸ¥è¯†çš„ç»†ç²’åº¦å¤šæ¨¡æ€æ¨ç†ï¼Œå¹¶è®¾è®¡äº†è´Ÿæ ·æœ¬ä»¥å¢å¼ºæ¨¡å‹é²æ£’æ€§ã€‚
3. å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨çŸ¥è¯†é©±åŠ¨çš„è§†è§‰å®šä½ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ–‡æœ¬ç†è§£ä¸è§†è§‰å®šä½ä¹‹é—´å­˜åœ¨è§£è€¦ç°è±¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Referring Expression Comprehension (REC) æ˜¯ä¸€ç§æµè¡Œçš„å¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬è¡¨è¾¾å‡†ç¡®æ£€æµ‹å•å¹…å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„RECåŸºå‡†å­˜åœ¨å±€é™æ€§ï¼Œå¾€å¾€ä»…ä¾èµ–å›¾åƒå†…éƒ¨çº¿ç´¢æˆ–ç¼ºä¹è¶³å¤Ÿç»†ç²’åº¦çš„å®ä¾‹æ³¨é‡Šï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ–°çš„åŸºå‡†KnowDR-RECï¼Œå…·æœ‰ä¸‰ä¸ªå…³é”®ç‰¹å¾ï¼šåŸºäºçœŸå®ä¸–ç•ŒçŸ¥è¯†ï¼Œè¦æ±‚ç»†ç²’åº¦çš„å¤šæ¨¡æ€æ¨ç†ï¼›é€šè¿‡ç»†ç²’åº¦è¡¨è¾¾ç¼–è¾‘æ„å»ºçš„è´Ÿæ ·æœ¬ï¼Œè¯„ä¼°æ¨¡å‹çš„é²æ£’æ€§å’ŒæŠ—å¹»è§‰èƒ½åŠ›ï¼›å¼•å…¥ä¸‰ç§æ–°è¯„ä¼°æŒ‡æ ‡ï¼Œç³»ç»Ÿæ¢ç´¢æ¨¡å‹çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„MLLMsåœ¨çŸ¥è¯†é©±åŠ¨çš„è§†è§‰å®šä½ä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹åœ¨çœŸå®ä¸–ç•ŒçŸ¥è¯†åº”ç”¨ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ä¼ ç»ŸRECåŸºå‡†æ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºKnowDR-RECåŸºå‡†ï¼Œé€šè¿‡å¼•å…¥çœŸå®ä¸–ç•ŒçŸ¥è¯†å’Œç»†ç²’åº¦çš„å¤šæ¨¡æ€æ¨ç†ï¼Œè®¾è®¡è´Ÿæ ·æœ¬ä»¥æµ‹è¯•æ¨¡å‹çš„æŠ—å¹»è§‰èƒ½åŠ›ï¼Œè¿›è€Œæ¨åŠ¨å¤šæ¨¡æ€ç³»ç»Ÿçš„å¯é æ€§å’Œè§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKnowDR-RECåŸºäºçœŸå®åœºæ™¯æ„å»ºï¼ŒåŒ…å«æ–‡æœ¬ä¸å›¾åƒçš„ç»†ç²’åº¦åŒ¹é…æ¨¡å—ã€è´Ÿæ ·æœ¬ç”Ÿæˆæ¨¡å—ä»¥åŠæ–°çš„è¯„ä¼°æŒ‡æ ‡æ¨¡å—ï¼Œæ•´ä½“æµç¨‹ä¸ºæ•°æ®æ”¶é›†ã€æ ·æœ¬æ„å»ºã€æ¨¡å‹è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥äº†ç»†ç²’åº¦è¡¨è¾¾ç¼–è¾‘çš„è´Ÿæ ·æœ¬ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶æå‡ºä¸‰ç§æ–°è¯„ä¼°æŒ‡æ ‡ï¼Œæ˜¾è‘—æå‡äº†å¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„è¯„ä¼°æ·±åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¤šæ¨¡æ€åŒ¹é…ï¼Œè®¾ç½®äº†å¤šå±‚æ¬¡çš„ç½‘ç»œç»“æ„ä»¥å¢å¼ºæ¨¡å‹å¯¹å¤æ‚åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡ç»†ç²’åº¦çš„æ ·æœ¬è®¾è®¡ï¼Œæå‡äº†æ¨¡å‹çš„æŠ—å¹²æ‰°èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œ16ç§æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨KnowDR-RECåŸºå‡†ä¸Šè¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨çŸ¥è¯†é©±åŠ¨çš„è§†è§‰å®šä½ä»»åŠ¡ä¸­ï¼Œè®¸å¤šæ¨¡å‹å—åˆ°è®°å¿†åŒ–æ·å¾„çš„æ˜¾è‘—å½±å“ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ç­‰å¤æ‚åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡å¤šæ¨¡æ€ç³»ç»Ÿåœ¨çœŸå®ä¸–ç•Œä¸­çš„è¡¨ç°å’Œå¯é æ€§ã€‚æœªæ¥ï¼ŒKnowDR-RECåŸºå‡†å°†æ¨åŠ¨æ›´å¼ºå¤§ã€å¯è§£é‡Šçš„è§†è§‰å®šä½æ¡†æ¶çš„å‘å±•ï¼Œä¿ƒè¿›å¤šæ¨¡æ€æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Referring Expression Comprehension (REC) is a popular multimodal task that aims to accurately detect target objects within a single image based on a given textual expression. However, due to the limitations of earlier models, traditional REC benchmarks either rely solely on intra-image cues or lack sufficiently fine-grained instance annotations, making them inadequate for evaluating the reasoning capabilities of Multi-modal Large Language Models (MLLMs). To address this gap, we propose a new benchmark, KnowDR-REC, characterized by three key features: Firstly, it is built upon real-world knowledge, requiring fine-grained multimodal reasoning across text and image. Secondly, the dataset includes elaborately constructed negative samples via fine-grained expression editing, designed to evaluate a model's robustness and anti-hallucination ability. Lastly, we introduce three novel evaluation metrics to systematically explore the model's internal reasoning process. We evaluate 16 state-of-the-art multimodal models on KnowDR-REC, with experimental results showing that existing MLLMs still struggle with knowledge-driven visual grounding tasks. Furthermore, we observe a decoupling between textual understanding and visual grounding in MLLMs, where many models are significantly influenced by memorized shortcut correlations, which severely affect their behavior on our benchmark and hinder genuine multimodal reasoning. We anticipate that the proposed benchmark will inspire future research towards developing more robust, interpretable, and knowledge-intensive visual grounding frameworks, driving the development of more reliable and robust multimodal systems for complex real-world scenarios.

