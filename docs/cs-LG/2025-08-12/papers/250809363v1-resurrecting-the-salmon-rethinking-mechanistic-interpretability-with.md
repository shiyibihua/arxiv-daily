---
layout: default
title: Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders
---

# Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09363" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09363v1</a>
  <a href="https://arxiv.org/pdf/2508.09363.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09363v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09363v1', 'Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Charles O'Neill, Mudith Jayasekara, Max Kirkby

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé¢†åŸŸç‰¹å®šç¨€ç–è‡ªç¼–ç å™¨ä»¥æå‡è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `å¯è§£é‡Šæ€§` `é¢†åŸŸç‰¹å®šæ¨¡å‹` `åŒ»å­¦æ–‡æœ¬` `å¤§å‹è¯­è¨€æ¨¡å‹` `é‡æ„è¯¯å·®` `ç‰¹å¾å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¨€ç–è‡ªç¼–ç å™¨åœ¨å¹¿æ³›æ•°æ®åˆ†å¸ƒä¸Šè®­ç»ƒï¼Œå¯¼è‡´æ½œåœ¨ç‰¹å¾éš¾ä»¥è§£é‡Šï¼Œä¸”å­˜åœ¨å¤§é‡é‡æ„è¯¯å·®ã€‚
2. æœ¬æ–‡æå‡ºå°†SAEè®­ç»ƒé™åˆ¶åœ¨ç‰¹å®šé¢†åŸŸï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰é¢†åŸŸç‰¹å®šç‰¹å¾ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé¢†åŸŸç‰¹å®šçš„SAEsåœ¨æ–¹å·®è§£é‡Šã€æŸå¤±æ¢å¤å’Œçº¿æ€§æ®‹å·®è¯¯å·®æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿçš„å¹¿åŸŸSAEsã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¿€æ´»åˆ†è§£ä¸ºæ½œåœ¨ç‰¹å¾ï¼Œæ­ç¤ºå…¶æœºåˆ¶ç»“æ„ã€‚ä¼ ç»Ÿçš„SAEsåœ¨å¹¿æ³›çš„æ•°æ®åˆ†å¸ƒä¸Šè®­ç»ƒï¼Œå¯¼è‡´å›ºå®šçš„æ½œåœ¨é¢„ç®—åªèƒ½æ•æ‰é«˜é¢‘ã€é€šç”¨æ¨¡å¼ï¼Œä»è€Œäº§ç”Ÿæ˜¾è‘—çš„çº¿æ€§â€œæš—ç‰©è´¨â€é‡æ„è¯¯å·®ï¼Œå¹¶ä½¿å¾—æ½œåœ¨ç‰¹å¾ç›¸äº’ç¢ç‰‡åŒ–æˆ–å¸æ”¶ï¼Œå¢åŠ äº†è§£é‡Šçš„å¤æ‚æ€§ã€‚æœ¬æ–‡å±•ç¤ºäº†å°†SAEè®­ç»ƒé™åˆ¶åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦æ–‡æœ¬ï¼‰å¯ä»¥é‡æ–°åˆ†é…å®¹é‡ï¼Œæ”¹å–„é‡æ„ä¿çœŸåº¦å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡åœ¨Gemma-2æ¨¡å‹çš„ç¬¬20å±‚æ¿€æ´»ä¸Šä½¿ç”¨195kä¸´åºŠé—®ç­”ç¤ºä¾‹è®­ç»ƒJumpReLU SAEsï¼Œæˆ‘ä»¬å‘ç°é¢†åŸŸé™åˆ¶çš„SAEsè§£é‡Šäº†å¤šè¾¾20%çš„æ–¹å·®ï¼Œè¾¾åˆ°äº†æ›´é«˜çš„æŸå¤±æ¢å¤ï¼Œå¹¶å‡å°‘äº†çº¿æ€§æ®‹å·®è¯¯å·®ã€‚è‡ªåŠ¨åŒ–å’Œäººå·¥è¯„ä¼°ç¡®è®¤æ‰€å­¦ç‰¹å¾ä¸ä¸´åºŠç›¸å…³æ¦‚å¿µï¼ˆå¦‚â€œå‘³è§‰æ„Ÿå—â€æˆ–â€œä¼ æŸ“æ€§å•æ ¸ç»†èƒå¢å¤šç—‡â€ï¼‰ä¸€è‡´ï¼Œè€Œéé¢‘ç¹ä½†æ— ä¿¡æ¯çš„æ ‡è®°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿç¨€ç–è‡ªç¼–ç å™¨åœ¨å¹¿æ³›æ•°æ®åˆ†å¸ƒä¸Šè®­ç»ƒæ‰€å¯¼è‡´çš„é‡æ„è¯¯å·®å’Œæ½œåœ¨ç‰¹å¾éš¾ä»¥è§£é‡Šçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•æ‰é¢†åŸŸç‰¹å®šä¿¡æ¯æ—¶å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†SAEçš„è®­ç»ƒé™åˆ¶åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦æ–‡æœ¬ï¼‰ï¼Œé‡æ–°åˆ†é…æ¨¡å‹å®¹é‡ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰ä¸è¯¥é¢†åŸŸç›¸å…³çš„ç‰¹å¾ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é‡æ„ä¿çœŸåº¦å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€SAEæ¨¡å‹è®¾è®¡ã€è®­ç»ƒè¿‡ç¨‹å’Œè¯„ä¼°æ¨¡å—ã€‚æ¨¡å‹ä½¿ç”¨JumpReLUæ¿€æ´»å‡½æ•°ï¼Œä¸“æ³¨äºç¬¬20å±‚çš„æ¿€æ´»è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé¢†åŸŸé™åˆ¶çš„è®­ç»ƒç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å¹¿åŸŸSAEså½¢æˆé²œæ˜å¯¹æ¯”ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰é¢†åŸŸç‰¹å®šçš„çº¿æ€§ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†JumpReLUæ¿€æ´»å‡½æ•°ï¼Œå¹¶ä½¿ç”¨195kä¸ªä¸´åºŠé—®ç­”ç¤ºä¾‹è¿›è¡Œè®­ç»ƒã€‚æŸå¤±å‡½æ•°çš„é€‰æ‹©å’Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°æœ€ä½³ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¢†åŸŸç‰¹å®šçš„SAEsèƒ½å¤Ÿè§£é‡Šå¤šè¾¾20%çš„æ–¹å·®ï¼ŒæŸå¤±æ¢å¤ç‡æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”çº¿æ€§æ®‹å·®è¯¯å·®å‡å°‘ã€‚è¿™äº›ç»“æœé€šè¿‡è‡ªåŠ¨åŒ–å’Œäººå·¥è¯„ä¼°å¾—åˆ°äº†éªŒè¯ï¼Œè¡¨æ˜æ‰€å­¦ç‰¹å¾ä¸ä¸´åºŠç›¸å…³æ¦‚å¿µé«˜åº¦ä¸€è‡´ï¼Œä¼˜äºä¼ ç»Ÿçš„å¹¿åŸŸSAEsã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦æ–‡æœ¬åˆ†æã€ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿå’ŒåŒ»ç–—ä¿¡æ¯æå–ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿå¸®åŠ©åŒ»ç–—ä¸“ä¸šäººå‘˜æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œä»è€Œæå‡ä¸´åºŠåº”ç”¨çš„å®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½å½±å“å…¶ä»–é¢†åŸŸçš„æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼Œæ¨åŠ¨æ›´å…·é’ˆå¯¹æ€§çš„æ¨¡å‹å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse autoencoders (SAEs) decompose large language model (LLM) activations into latent features that reveal mechanistic structure. Conventional SAEs train on broad data distributions, forcing a fixed latent budget to capture only high-frequency, generic patterns. This often results in significant linear ``dark matter'' in reconstruction error and produces latents that fragment or absorb each other, complicating interpretation. We show that restricting SAE training to a well-defined domain (medical text) reallocates capacity to domain-specific features, improving both reconstruction fidelity and interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2 models using 195k clinical QA examples, we find that domain-confined SAEs explain up to 20\% more variance, achieve higher loss recovery, and reduce linear residual error compared to broad-domain SAEs. Automated and human evaluations confirm that learned features align with clinically meaningful concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather than frequent but uninformative tokens. These domain-specific SAEs capture relevant linear structure, leaving a smaller, more purely nonlinear residual. We conclude that domain-confinement mitigates key limitations of broad-domain SAEs, enabling more complete and interpretable latent decompositions, and suggesting the field may need to question ``foundation-model'' scaling for general-purpose SAEs.

