---
layout: default
title: Multi-level Collaborative Distillation Meets Global Workspace Model: A Unified Framework for OCIL
---

# Multi-level Collaborative Distillation Meets Global Workspace Model: A Unified Framework for OCIL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08677" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08677v1</a>
  <a href="https://arxiv.org/pdf/2508.08677.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08677v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08677v1', 'Multi-level Collaborative Distillation Meets Global Workspace Model: A Unified Framework for OCIL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shibin Su, Guoqiang Liang, De Cheng, Shizhou Zhang, Lingyan Ran, Yanning Zhang

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: 12 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šå±‚åä½œè’¸é¦ä»¥è§£å†³åœ¨çº¿å¢é‡å­¦ä¹ ä¸­çš„ç¨³å®šæ€§ä¸é€‚åº”æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åœ¨çº¿å¢é‡å­¦ä¹ ` `å…¨çƒå·¥ä½œæ¨¡å‹` `å¤šå±‚åä½œè’¸é¦` `æ¨¡å‹ç¨³å®šæ€§` `é€‚åº”æ€§å­¦ä¹ ` `é›†æˆå­¦ä¹ ` `çŸ¥è¯†ä¿ç•™`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åœ¨çº¿å¢é‡å­¦ä¹ æ–¹æ³•åœ¨å†…å­˜é™åˆ¶ä¸‹éš¾ä»¥ä¿æŒæ¨¡å‹çš„ç¨³å®šæ€§ï¼ŒåŒæ—¶åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶åˆé¢ä¸´æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å…¨çƒå·¥ä½œæ¨¡å‹ï¼ˆGWMï¼‰å’Œå¤šå±‚åä½œè’¸é¦æœºåˆ¶æ¥å¢å¼ºé›†æˆå­¦ä¹ ï¼Œä»è€Œå¹³è¡¡ç¨³å®šæ€§ä¸é€‚åº”æ€§ã€‚
3. åœ¨ä¸‰ä¸ªæ ‡å‡†OCILåŸºå‡†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨ä¸åŒå†…å­˜é¢„ç®—ä¸‹æ˜¾è‘—æå‡äº†å¤šç§OCILæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨çº¿å¢é‡å­¦ä¹ ï¼ˆOCILï¼‰ä½¿æ¨¡å‹èƒ½å¤Ÿä»éç‹¬ç«‹åŒåˆ†å¸ƒçš„æ•°æ®æµä¸­æŒç»­å­¦ä¹ ï¼Œä¸”æ ·æœ¬åªèƒ½è¢«çœ‹åˆ°ä¸€æ¬¡ï¼Œè¿™ä½¿å…¶åœ¨ç°å®åœºæ™¯ä¸­æ›´å…·é€‚ç”¨æ€§ã€‚ç„¶è€Œï¼ŒOCILé¢ä¸´ç€åœ¨ä¸¥æ ¼å†…å­˜é™åˆ¶ä¸‹ä¿æŒæ¨¡å‹ç¨³å®šæ€§å’Œé€‚åº”æ–°ä»»åŠ¡çš„ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å…¨çƒå·¥ä½œæ¨¡å‹ï¼ˆGWMï¼‰å¢å¼ºé›†æˆå­¦ä¹ ï¼ŒGWMä½œä¸ºå…±äº«çš„éšå¼è®°å¿†ï¼ŒæŒ‡å¯¼å¤šä¸ªå­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¼•å…¥äº†å¤šå±‚åä½œè’¸é¦æœºåˆ¶ï¼Œä¿ƒè¿›å­¦ç”Ÿä¹‹é—´çš„ä¸€è‡´æ€§å¹¶ä¿ç•™å†å²çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªOCILæ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨çº¿å¢é‡å­¦ä¹ ä¸­æ¨¡å‹åœ¨ä¸¥æ ¼å†…å­˜é™åˆ¶ä¸‹çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§é—®é¢˜ã€‚ç°æœ‰çš„é‡æ”¾æ–¹æ³•åœ¨å†…å­˜å—é™æ—¶æ•ˆæœä¸ä½³ï¼Œè€Œé›†æˆæ–¹æ³•è™½ç„¶æé«˜äº†é€‚åº”æ€§ï¼Œä½†ç¨³å®šæ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å…¨çƒå·¥ä½œæ¨¡å‹ï¼ˆGWMï¼‰ä½œä¸ºå…±äº«éšå¼è®°å¿†ï¼ŒæŒ‡å¯¼å¤šä¸ªå­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ ã€‚GWMé€šè¿‡èåˆæ‰€æœ‰å­¦ç”Ÿæ¨¡å‹çš„å‚æ•°ï¼Œæ•æ‰å†å²å­¦ä¹ è½¨è¿¹ï¼Œä»è€Œç¨³å®šå­¦ä¹ è¿‡ç¨‹å¹¶ä¿ƒè¿›è·¨ä»»åŠ¡ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬GWMçš„æ„å»ºã€å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ å’Œå®šæœŸçš„æ¨¡å‹é‡åˆ†é…ã€‚GWMåœ¨æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­å½¢æˆï¼Œå¹¶ä¸å­¦ç”Ÿæ¨¡å‹è¿›è¡Œå¯¹é½ï¼Œä»¥ä¿æŒå†å²çŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å…¨çƒå·¥ä½œæ¨¡å‹å’Œå¤šå±‚åä½œè’¸é¦æœºåˆ¶ï¼Œå‰è€…ä½œä¸ºåŠ¨æ€é”šç‚¹ï¼Œåè€…åˆ™ç¡®ä¿å­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒGWMçš„å‚æ•°èåˆç­–ç•¥å’Œå­¦ç”Ÿæ¨¡å‹çš„å¯¹é½æœºåˆ¶æ˜¯å…³é”®ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šä¹Ÿè€ƒè™‘äº†å†å²çŸ¥è¯†çš„ä¿ç•™å’Œæ–°ä»»åŠ¡çš„é€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªOCILæ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚åœ¨ç‰¹å®šå†…å­˜é¢„ç®—ä¸‹ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†10%ä»¥ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•è¡¨ç°å‡ºæ›´å¥½çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€åœ¨çº¿æ¨èç³»ç»Ÿå’Œè‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­æŒç»­å­¦ä¹ å¹¶é€‚åº”æ–°ä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Online Class-Incremental Learning (OCIL) enables models to learn continuously from non-i.i.d. data streams and samples of the data streams can be seen only once, making it more suitable for real-world scenarios compared to offline learning. However, OCIL faces two key challenges: maintaining model stability under strict memory constraints and ensuring adaptability to new tasks. Under stricter memory constraints, current replay-based methods are less effective. While ensemble methods improve adaptability (plasticity), they often struggle with stability. To overcome these challenges, we propose a novel approach that enhances ensemble learning through a Global Workspace Model (GWM)-a shared, implicit memory that guides the learning of multiple student models. The GWM is formed by fusing the parameters of all students within each training batch, capturing the historical learning trajectory and serving as a dynamic anchor for knowledge consolidation. This fused model is then redistributed periodically to the students to stabilize learning and promote cross-task consistency. In addition, we introduce a multi-level collaborative distillation mechanism. This approach enforces peer-to-peer consistency among students and preserves historical knowledge by aligning each student with the GWM. As a result, student models remain adaptable to new tasks while maintaining previously learned knowledge, striking a better balance between stability and plasticity. Extensive experiments on three standard OCIL benchmarks show that our method delivers significant performance improvement for several OCIL models across various memory budgets.

