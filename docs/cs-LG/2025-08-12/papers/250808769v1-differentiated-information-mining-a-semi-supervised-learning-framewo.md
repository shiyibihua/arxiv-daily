---
layout: default
title: Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs
---

# Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08769" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08769v1</a>
  <a href="https://arxiv.org/pdf/2508.08769.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08769v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08769v1', 'Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Long Wang, Kai Liu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: 13 pages, 5 figures, 8 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå·®å¼‚åŒ–å› å­ä¸€è‡´æ€§åŠç›‘ç£æ¡†æ¶ä»¥è§£å†³GNNä¼ªæ ‡ç­¾åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾ç¥ç»ç½‘ç»œ` `åŠç›‘ç£å­¦ä¹ ` `ä¼ªæ ‡ç­¾` `é²æ£’æ€§` `å¤šæ¨¡æ€èåˆ` `å†³ç­–å› å­` `ä¸€è‡´æ€§å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å›¾ç¥ç»ç½‘ç»œæ—¶ï¼Œå®¹æ˜“å—åˆ°ä¼ªæ ‡ç­¾ç¡®è®¤åå·®å’Œè®­ç»ƒå´©æºƒçš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºçš„DiFacæ¡†æ¶é€šè¿‡ä»å•ä¸€ä¿¡æ¯æºä¸­æå–å·®å¼‚åŒ–å› å­ï¼Œå¹¶å¼ºåˆ¶å…¶ä¸€è‡´æ€§ï¼Œæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDiFacåœ¨ä½æ ‡ç­¾æƒ…å†µä¸‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œè¶…è¶Šäº†å…¶ä»–åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åŠç›‘ç£å­¦ä¹ ä¸­ï¼Œä¸ºæé«˜å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šçš„æ€§èƒ½ï¼Œå¼•å…¥ç›¸äº’ç‹¬ç«‹çš„å†³ç­–å› å­è¿›è¡Œäº¤å‰éªŒè¯è¢«è®¤ä¸ºæ˜¯ä¸€ç§æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œè·å–è¿™äº›å› å­åœ¨å®è·µä¸­å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å·®å¼‚åŒ–å› å­ä¸€è‡´æ€§åŠç›‘ç£æ¡†æ¶ï¼ˆDiFacï¼‰ï¼Œä»å•ä¸€ä¿¡æ¯æºä¸­æå–å·®å¼‚åŒ–å› å­å¹¶å¼ºåˆ¶å…¶ä¸€è‡´æ€§ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ æå–è¿™äº›å› å­ï¼›åœ¨è®­ç»ƒé˜¶æ®µï¼Œè¿­ä»£å»é™¤å…·æœ‰å†²çªå› å­çš„æ ·æœ¬ï¼Œå¹¶åŸºäºæœ€çŸ­éŸ³é˜¶åŸåˆ™å¯¹ä¼ªæ ‡ç­¾è¿›è¡Œæ’åºï¼Œé€‰æ‹©å‰å€™é€‰æ ·æœ¬ä»¥å‡å°‘å¸¸è§çš„è¿‡åº¦è‡ªä¿¡ç°è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiFacåœ¨ä½æ ‡ç­¾æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å›¾ç¥ç»ç½‘ç»œåœ¨åŠç›‘ç£å­¦ä¹ ä¸­é¢ä¸´çš„ä¼ªæ ‡ç­¾ç¡®è®¤åå·®å’Œè®­ç»ƒå´©æºƒé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è·å–ç‹¬ç«‹å†³ç­–å› å­æ—¶é¢ä¸´ä¿¡æ¯æºç¨€ç¼ºå’Œå› å­ç‹¬ç«‹æ€§æ— æ³•ä¿è¯çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDiFacæ¡†æ¶é€šè¿‡ä»å•ä¸€ä¿¡æ¯æºä¸­æå–å·®å¼‚åŒ–å› å­ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼ºåˆ¶è¿™äº›å› å­çš„ä¸€è‡´æ€§ï¼Œæ¥å‡å°‘ä¼ªæ ‡ç­¾çš„åå·®ã€‚è¯¥è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹çš„é²æ£’æ€§ï¼Œé™ä½è¿‡åº¦è‡ªä¿¡ç°è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDiFacæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¢„è®­ç»ƒé˜¶æ®µå’Œè®­ç»ƒé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ æå–å·®å¼‚åŒ–å› å­ï¼›åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹è¿­ä»£å»é™¤å†²çªå› å­æ ·æœ¬ï¼Œå¹¶æ ¹æ®æœ€çŸ­éŸ³é˜¶åŸåˆ™å¯¹ä¼ªæ ‡ç­¾è¿›è¡Œæ’åºã€‚

**å…³é”®åˆ›æ–°**ï¼šDiFacçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä»å•ä¸€ä¿¡æ¯æºä¸­æå–å·®å¼‚åŒ–å› å­å¹¶å¼ºåˆ¶å…¶ä¸€è‡´æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤šä¸ªç‹¬ç«‹ä¿¡æ¯æºçš„æ€è·¯æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è¡¡é‡å› å­ä¸€è‡´æ€§ï¼Œå¹¶å¼•å…¥äº†è¾…åŠ©å†³ç­–å› å­ï¼Œå¦‚å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ½œåœ¨æ–‡æœ¬çŸ¥è¯†ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å†³ç­–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ä¸ªè´£ä»»è¯„åˆ†æœºåˆ¶æ¥é™ä½è¾…åŠ©å› å­å¸¦æ¥çš„é”™è¯¯åˆ¤æ–­ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDiFacæ¡†æ¶åœ¨ä½æ ‡ç­¾æƒ…å†µä¸‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›æ•°æ®é›†ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æé«˜äº†5%-10%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿå’Œç”Ÿç‰©ä¿¡æ¯å­¦ç­‰ã€‚é€šè¿‡æé«˜å›¾ç¥ç»ç½‘ç»œåœ¨ä½æ ‡ç­¾æ•°æ®ä¸‹çš„æ€§èƒ½ï¼ŒDiFacæ¡†æ¶èƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´å¯é çš„å†³ç­–æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„å›¾æ•°æ®åˆ†æä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In semi-supervised learning (SSL) for enhancing the performance of graph neural networks (GNNs) with unlabeled data, introducing mutually independent decision factors for cross-validation is regarded as an effective strategy to alleviate pseudo-label confirmation bias and training collapse. However, obtaining such factors is challenging in practice: additional and valid information sources are inherently scarce, and even when such sources are available, their independence from the original source cannot be guaranteed. To address this challenge, In this paper we propose a Differentiated Factor Consistency Semi-supervised Framework (DiFac), which derives differentiated factors from a single information source and enforces their consistency. During pre-training, the model learns to extract these factors; in training, it iteratively removes samples with conflicting factors and ranks pseudo-labels based on the shortest stave principle, selecting the top candidate samples to reduce overconfidence commonly observed in confidence-based or ensemble-based methods. Our framework can also incorporate additional information sources. In this work, we leverage the large multimodal language model to introduce latent textual knowledge as auxiliary decision factors, and we design a accountability scoring mechanism to mitigate additional erroneous judgments introduced by these auxiliary factors. Experiments on multiple benchmark datasets demonstrate that DiFac consistently improves robustness and generalization in low-label regimes, outperforming other baseline methods.

