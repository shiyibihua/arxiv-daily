---
layout: default
title: MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time
---

# MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08641" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.08641v1</a>
  <a href="https://arxiv.org/pdf/2508.08641.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08641v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08641v1', 'MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Peter Phan, Dhruv Agarwal, Kavitha Srinivas, Horst Samulowitz, Pavan Kapanipathi, Andrew McCallum

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-12

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MiGrATe‰ª•Ëß£ÂÜ≥ÈªëÁÆ±‰ºòÂåñ‰ªªÂä°‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈªëÁÆ±‰ºòÂåñ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Âú®Á∫øÊµãËØïÊó∂ËÆ≠ÁªÉ` `Ê∑∑ÂêàÁ≠ñÁï•` `ÊêúÁ¥¢ÁÆóÊ≥ï` `ÈÄÇÂ∫îÊÄßÂ≠¶‰π†` `Êó†ÁõëÁù£Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÈªëÁÆ±‰ºòÂåñ‰ªªÂä°‰∏≠Èöæ‰ª•Âπ≥Ë°°Êé¢Á¥¢‰∏éÂà©Áî®ÔºåÂØºËá¥Ëß£ÁöÑË¥®Èáè‰∏çÁêÜÊÉ≥„ÄÇ
2. MiGrATeÈÄöËøáÂú®Á∫øÊµãËØïÊó∂ËÆ≠ÁªÉÔºåÂà©Áî®GRPOÁÆóÊ≥ïÈÄÇÂ∫îLLMsÔºåÈÅøÂÖç‰∫ÜÂØπÊâãÂ∑•ËÆ≠ÁªÉÊï∞ÊçÆÁöÑ‰æùËµñ„ÄÇ
3. Âú®Â§ö‰∏™Â§çÊùÇÈ¢ÜÂüüÁöÑÂÆûÈ™å‰∏≠ÔºåMiGrATe consistently outperform inference-only and TTT baselinesÔºåÊòæÁ§∫Âá∫ÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÈªëÁÆ±‰ºòÂåñ‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®Êó•ÁõäÂπøÊ≥õÔºåÁÑ∂ËÄåÁé∞ÊúâÊñπÊ≥ïÂú®Êé¢Á¥¢Êñ∞Ëß£Á©∫Èó¥‰∏éÂà©Áî®È´òÂõûÊä•Ëß£‰πãÈó¥ÁöÑÂπ≥Ë°°‰∏äÂ≠òÂú®Âõ∞Èöæ„ÄÇÊú¨ÊñáÊèêÂá∫MiGrATeÔºå‰∏ÄÁßçÂú®Á∫øÊµãËØïÊó∂ËÆ≠ÁªÉÔºàTTTÔºâÁöÑÊñπÊ≥ïÔºåÂà©Áî®GRPO‰Ωú‰∏∫ÊêúÁ¥¢ÁÆóÊ≥ïÔºåÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÈÄÇÂ∫îLLMsÔºåÊó†ÈúÄÂ§ñÈÉ®ËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇMiGrATeÈÄöËøáÊ∑∑ÂêàÁ≠ñÁï•ÁªÑÊûÑÂª∫Á®ãÂ∫èÔºåÁªìÂêà‰∫ÜÂú®Á≠ñÁï•ÈááÊ†∑Âíå‰∏§ÁßçÁ¶ªÁ≠ñÁï•Êï∞ÊçÆÈÄâÊã©ÊäÄÊúØÔºåÊó®Âú®Âú®‰øùÁïôÊé¢Á¥¢ÁöÑÂêåÊó∂ÔºåÂÅèÂêë‰∫éÂØπÊúâÂâçÊôØÂå∫ÂüüÁöÑÂà©Áî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMiGrATeÂú®Â§ö‰∏™Â§çÊùÇ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ±ïÁ§∫‰∫ÜÂú®Á∫øTTTÂú®Êó†Â§ñÈÉ®ÁõëÁù£‰∏ãÁöÑÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÈªëÁÆ±‰ºòÂåñ‰ªªÂä°‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Êé¢Á¥¢Êñ∞Ëß£Á©∫Èó¥‰∏éÂà©Áî®È´òÂõûÊä•Ëß£‰πãÈó¥ÁöÑÂπ≥Ë°°Â≠òÂú®‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏çÂêåÈ¢ÜÂüüÁöÑÂèØË°åÊÄß‰∏éÊâ©Â±ïÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMiGrATeÈÄöËøáÂú®Á∫øÊµãËØïÊó∂ËÆ≠ÁªÉÔºàTTTÔºâÊñπÊ≥ïÔºåÂà©Áî®GRPO‰Ωú‰∏∫ÊêúÁ¥¢ÁÆóÊ≥ïÔºåÂú®Êé®ÁêÜÈò∂ÊÆµÈÄÇÂ∫îLLMsÔºåÈÅøÂÖç‰∫ÜÂØπÂ§ñÈÉ®ËÆ≠ÁªÉÊï∞ÊçÆÁöÑÈúÄÊ±Ç„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÂú®Á≠ñÁï•ÈááÊ†∑‰∏é‰∏§ÁßçÁ¶ªÁ≠ñÁï•Êï∞ÊçÆÈÄâÊã©ÊäÄÊúØÔºåÊó®Âú®Âú®‰øùÁïôÊé¢Á¥¢ÁöÑÂêåÊó∂ÔºåÂÅèÂêë‰∫éÂØπÊúâÂâçÊôØÂå∫ÂüüÁöÑÂà©Áî®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMiGrATeÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Ê∑∑ÂêàÁ≠ñÁï•ÁªÑÊûÑÂª∫Á®ãÂ∫èÔºå‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨Âú®Á≠ñÁï•ÈááÊ†∑„ÄÅË¥™Â©™ÈááÊ†∑ÔºàÈÄâÊã©Ë°®Áé∞ÊúÄ‰Ω≥ÁöÑÂéÜÂè≤ÂÆåÊàêÔºâÂíåÈÇªÂüüÈááÊ†∑ÔºàÁîüÊàê‰∏éÈ´òÂõûÊä•Ëß£ÁªìÊûÑÁõ∏‰ººÁöÑÂÆåÊàêÔºâ„ÄÇËøô‰∫õÊ®°ÂùóÂÖ±Âêå‰ΩúÁî®Ôºå‰ºòÂåñÁ≠ñÁï•Ê¢ØÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMiGrATeÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂Ê∑∑ÂêàÁ≠ñÁï•ÁªÑÊûÑÂª∫Á®ãÂ∫èÔºåÈÄöËøáÁªìÂêàÂú®Á≠ñÁï•‰∏éÁ¶ªÁ≠ñÁï•ÁöÑÈááÊ†∑ÊñπÊ≥ïÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂØπÊúâÂâçÊôØÂå∫ÂüüÁöÑÂà©Áî®ËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊé¢Á¥¢ÁöÑÁÅµÊ¥ªÊÄß„ÄÇËøô‰∏ÄËÆæËÆ°‰∏é‰º†ÁªüÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫é‰∏çÂÜç‰æùËµñÊâãÂ∑•ËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®MiGrATe‰∏≠ÔºåÂÖ≥ÈîÆÂèÇÊï∞ËÆæÁΩÆÂåÖÊã¨ÈááÊ†∑Á≠ñÁï•ÁöÑÊùÉÈáçÂàÜÈÖçÔºåÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰ª•Á°Æ‰øùÂØπÈ´òÂõûÊä•Ëß£ÁöÑÂÅèÂêëÔºå‰ª•ÂèäÁΩëÁªúÁªìÊûÑÁöÑÈÄâÊã©‰ª•ÊîØÊåÅÈ´òÊïàÁöÑÂú®Á∫øÈÄÇÂ∫î„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂÆûÈ™å‰∏≠ÔºåMiGrATeÂú®Â≠óËØçÊêúÁ¥¢„ÄÅÂàÜÂ≠ê‰ºòÂåñÂíåÂÅáËÆæ+Á®ãÂ∫èÂΩíÁ∫≥Á≠â‰∏â‰∏™Â§çÊùÇÈ¢ÜÂüü‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå consistently outperforming inference-only and TTT baselinesÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Êó†Â§ñÈÉ®ÁõëÁù£‰∏ãÁöÑÂú®Á∫øTTTÊΩúÂäõÔºåÊèêÂçáÂπÖÂ∫¶ÊòæËëó„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MiGrATeÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂåÖÊã¨Á®ãÂ∫èÂêàÊàê„ÄÅÂàÜÂ≠êËÆæËÆ°Á≠âÈªëÁÆ±‰ºòÂåñ‰ªªÂä°„ÄÇÂÖ∂Âú®Á∫øÈÄÇÂ∫îËÉΩÂäõ‰ΩøÂæóÂú®Ê≤°ÊúâÂ§ñÈÉ®ÁõëÁù£ÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂø´ÈÄüÈÄÇÂ∫î‰∏çÂêå‰ªªÂä°ÈúÄÊ±ÇÔºåÊèêÂçáËß£ÂÜ≥ÊñπÊ°àÁöÑË¥®Èáè‰∏éÊïàÁéá„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊé®Âä®Êõ¥Â§öÂ§çÊùÇÊêúÁ¥¢‰ªªÂä°ÁöÑÁ†îÁ©∂‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) are increasingly being applied to black-box optimization tasks, from program synthesis to molecule design. Prior work typically leverages in-context learning to iteratively guide the model towards better solutions. Such methods, however, often struggle to balance exploration of new solution spaces with exploitation of high-reward ones. Recently, test-time training (TTT) with synthetic data has shown promise in improving solution quality. However, the need for hand-crafted training data tailored to each task limits feasibility and scalability across domains. To address this problem, we introduce MiGrATe-a method for online TTT that uses GRPO as a search algorithm to adapt LLMs at inference without requiring external training data. MiGrATe operates via a mixed-policy group construction procedure that combines on-policy sampling with two off-policy data selection techniques: greedy sampling, which selects top-performing past completions, and neighborhood sampling (NS), which generates completions structurally similar to high-reward ones. Together, these components bias the policy gradient towards exploitation of promising regions in solution space, while preserving exploration through on-policy sampling. We evaluate MiGrATe on three challenging domains-word search, molecule optimization, and hypothesis+program induction on the Abstraction and Reasoning Corpus (ARC)-and find that it consistently outperforms both inference-only and TTT baselines, demonstrating the potential of online TTT as a solution for complex search tasks without external supervision.

