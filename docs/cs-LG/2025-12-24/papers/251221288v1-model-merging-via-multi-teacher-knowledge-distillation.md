---
layout: default
title: Model Merging via Multi-Teacher Knowledge Distillation
---

# Model Merging via Multi-Teacher Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.21288" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.21288v1</a>
  <a href="https://arxiv.org/pdf/2512.21288.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.21288v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.21288v1', 'Model Merging via Multi-Teacher Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seyed Arshan Dalili, Mehrdad Mahdavi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-24

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/arshandalili/SAMerging)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAMergingï¼Œé€šè¿‡å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦å®ç°æ¨¡å‹åˆå¹¶ï¼Œæå‡æ³›åŒ–æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡å‹åˆå¹¶` `çŸ¥è¯†è’¸é¦` `å¤šä»»åŠ¡å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `PAC-Bayes` `Sharpness-Aware Minimization` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹åˆå¹¶æ–¹æ³•ä¾èµ–å¯å‘å¼å‚æ•°ç»„åˆï¼Œç¼ºä¹ç†è®ºæŒ‡å¯¼ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šä¸”å¯¹åˆå§‹åŒ–æ•æ„Ÿã€‚
2. è®ºæ–‡å°†æ¨¡å‹åˆå¹¶è§†ä¸ºå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦ï¼Œé€šè¿‡æœ€å°åŒ–KLæ•£åº¦æ¥ä¼˜åŒ–åˆå¹¶æ¨¡å‹ï¼Œå¹¶æ¨å¯¼å‡ºæ³›åŒ–è¯¯å·®ä¸Šç•Œã€‚
3. æå‡ºçš„SAMergingæ–¹æ³•åˆ©ç”¨Sharpness-Aware Minimizationå¯»æ‰¾å¹³å¦æœ€å°å€¼ï¼Œåœ¨è§†è§‰å’ŒNLPä»»åŠ¡ä¸Šå–å¾—SOTAæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡å‹åˆå¹¶æ˜¯è”åˆå¤šä»»åŠ¡å­¦ä¹ çš„ä¸€ç§è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åˆå¹¶æ¨¡å‹çš„æ³›åŒ–ç‰¹æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«æ¢ç´¢ã€‚å»ºç«‹æ­¤ç±»ç†è®ºä¿è¯å¹¶éæ˜“äº‹ï¼Œå› ä¸ºåˆå¹¶è¿‡ç¨‹é€šå¸¸ç¦æ­¢è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”æ¶‰åŠç»„åˆåœ¨æ ¹æœ¬ä¸Šå¼‚æ„çš„æ•°æ®åˆ†å¸ƒä¸Šå¾®è°ƒçš„æ¨¡å‹ã€‚ç”±äºç¼ºä¹å¯¹è¿™äº›åŠ¨æ€çš„åŸåˆ™æ€§ç†è§£ï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯å‘å¼æ–¹æ³•æ¥è¿‘ä¼¼å‚æ•°çš„æœ€ä½³ç»„åˆã€‚è¿™ç§ä¾èµ–åœ¨ç³»æ•°ç¼©æ”¾ä¸­æœ€ä¸ºå…³é”®ï¼Œç³»æ•°ç¼©æ”¾è°ƒèŠ‚æ¯ä¸ªå¾®è°ƒæ¨¡å‹å¯¹å…±äº«å‚æ•°çš„è´¡çŒ®ç¨‹åº¦ã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰åŸåˆ™æ€§ç›®æ ‡æ¥æŒ‡å¯¼å…¶é€‰æ‹©çš„æƒ…å†µä¸‹ï¼Œè¿™äº›æ–¹æ³•ä¼šå¯¼è‡´è„†å¼±çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯¹ç¼©æ”¾åˆå§‹åŒ–é«˜åº¦æ•æ„Ÿã€‚æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³è¿™ä¸€å·®è·ï¼š(i) ä¸“é—¨ä¸ºæ¨¡å‹åˆå¹¶è®¾ç½®å»ºç«‹ä¸€ç§æ–°é¢–çš„ã€æ„ŸçŸ¥å¹³å¦åº¦çš„PAC-Bayesæ³›åŒ–ç•Œé™ã€‚è¯¥åˆ†æå¼•å…¥äº†ä¸€ä¸ªâ€œè·¨ä»»åŠ¡å¼‚è´¨æ€§â€é¡¹ï¼Œè¯¥é¡¹æ­£å¼æ•è·äº†å„ç§å¾®è°ƒæ¨¡å‹å…ˆéªŒä¸ç›®æ ‡å¤šä»»åŠ¡åˆ†å¸ƒä¹‹é—´çš„ä¸åŒ¹é…ã€‚(ii) åœ¨æ­¤ç†è®ºè§è§£çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬å°†æ¨¡å‹åˆå¹¶å®šä¹‰ä¸ºåœ¨ç¨€ç¼ºçš„æœªæ ‡è®°æ•°æ®ä¸Šçš„å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦ã€‚æˆ‘ä»¬æ­£å¼è¯æ˜ï¼Œæœ€å°åŒ–å­¦ç”Ÿ-æ•™å¸ˆKullback-Leibleræ•£åº¦å¯ä»¥ç›´æ¥æ”¶ç´§åˆå¹¶æ¨¡å‹è¶…é¢é£é™©çš„ä¸Šé™ã€‚åœ¨å¯¼å‡ºçš„æ„ŸçŸ¥å¹³å¦åº¦çš„ç•Œé™çš„æŒ‡å¯¼ä¸‹ï¼Œ(iii) æˆ‘ä»¬é€šè¿‡SAMergingæ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼ŒSAMergingé‡‡ç”¨Sharpness-Aware Minimization (SAM) æ¥å¯»æ‰¾å¹³å¦æœ€å°å€¼ã€‚åœ¨ç»éªŒä¸Šï¼ŒSAMergingåœ¨è§†è§‰å’ŒNLPåŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨https://github.com/arshandalili/SAMergingè·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ¨¡å‹åˆå¹¶æ—¨åœ¨å°†å¤šä¸ªåœ¨ä¸åŒä»»åŠ¡ä¸Šå¾®è°ƒçš„æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªæ¨¡å‹ï¼Œä»¥å®ç°å¤šä»»åŠ¡å­¦ä¹ çš„ç›®çš„ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯å‘å¼æ–¹æ³•æ¥ç¡®å®šæ¯ä¸ªæ¨¡å‹çš„æƒé‡ï¼Œç¼ºä¹ç†è®ºæŒ‡å¯¼ï¼Œå¯¼è‡´åˆå¹¶åçš„æ¨¡å‹æ€§èƒ½ä¸ç¨³å®šï¼Œå¹¶ä¸”å¯¹æƒé‡åˆå§‹åŒ–éå¸¸æ•æ„Ÿã€‚å°¤å…¶æ˜¯åœ¨æ— æ³•è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆå¹¶æ¨¡å‹æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¨¡å‹åˆå¹¶é—®é¢˜è½¬åŒ–ä¸ºå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦é—®é¢˜ã€‚é€šè¿‡å°†å¤šä¸ªå¾®è°ƒåçš„æ¨¡å‹è§†ä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œåˆ©ç”¨å°‘é‡æœªæ ‡è®°æ•°æ®è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œè®­ç»ƒå‡ºä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ä½œä¸ºåˆå¹¶åçš„æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ï¼ŒåŒæ—¶é¿å…äº†ç›´æ¥æ“ä½œæ¨¡å‹å‚æ•°çš„å¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSAMergingçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) å¯¹å¤šä¸ªæ¨¡å‹åœ¨å„è‡ªçš„ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚2) åˆ©ç”¨å°‘é‡æœªæ ‡è®°æ•°æ®ï¼Œå°†å¾®è°ƒåçš„æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œè®­ç»ƒä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ã€‚3) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ€å°åŒ–å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„Kullback-Leibler (KL) æ•£åº¦ï¼Œä»¥ä¿è¯å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚4) ä½¿ç”¨Sharpness-Aware Minimization (SAM) ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ï¼Œå¯»æ‰¾å¹³å¦æœ€å°å€¼ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å°†æ¨¡å‹åˆå¹¶é—®é¢˜è½¬åŒ–ä¸ºå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦é—®é¢˜ï¼Œä¸ºæ¨¡å‹åˆå¹¶æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ã€‚2) æå‡ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹æ¨¡å‹åˆå¹¶åœºæ™¯çš„ã€æ„ŸçŸ¥å¹³å¦åº¦çš„PAC-Bayesæ³›åŒ–ç•Œé™ï¼Œä¸ºæ¨¡å‹åˆå¹¶çš„ç†è®ºåˆ†ææä¾›äº†åŸºç¡€ã€‚3) å°†Sharpness-Aware Minimization (SAM) åº”ç”¨äºæ¨¡å‹åˆå¹¶ï¼Œé€šè¿‡å¯»æ‰¾å¹³å¦æœ€å°å€¼æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šSAMergingçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨KLæ•£åº¦ä½œä¸ºå­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¿è¯å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚2) ä½¿ç”¨Sharpness-Aware Minimization (SAM) ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ï¼ŒSAMé€šè¿‡å¯»æ‰¾å‚æ•°ç©ºé—´ä¸­losså€¼å˜åŒ–ä¸æ•æ„Ÿçš„åŒºåŸŸï¼Œæ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒSAMé¦–å…ˆåœ¨å½“å‰å‚æ•°é™„è¿‘å¯»æ‰¾ä¸€ä¸ªæ‰°åŠ¨ï¼Œä½¿å¾—losså€¼å¢åŠ æœ€å¤šï¼Œç„¶åæ²¿ç€losså¢åŠ çš„åæ–¹å‘æ›´æ–°å‚æ•°ã€‚3) è®ºæ–‡ä¸­æå‡ºçš„â€œè·¨ä»»åŠ¡å¼‚è´¨æ€§â€é¡¹ï¼Œç”¨äºè¡¡é‡ä¸åŒä»»åŠ¡ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶åœ¨PAC-Bayesæ³›åŒ–ç•Œé™ä¸­å‘æŒ¥ä½œç”¨ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.21288v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.21288v1/arxiv/plots_to_use/eurosat_vs_sun397__samerging_vs_adamerging_8.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.21288v1/arxiv/plots_to_use/eurosat_vs_sun397_comparison.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

SAMergingåœ¨å¤šä¸ªè§†è§‰å’ŒNLPåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå»ºç«‹äº†æ–°çš„SOTAã€‚å…·ä½“æ€§èƒ½æ•°æ®åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ¨¡å‹åˆå¹¶æ–¹æ³•ï¼ŒSAMergingåœ¨å¤šä¸ªä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜æ˜¾çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAMergingèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å¤šä¸ªæ¨¡å‹çš„çŸ¥è¯†ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æ¨¡å‹åˆå¹¶çš„åœºæ™¯ï¼Œä¾‹å¦‚å¤šä»»åŠ¡å­¦ä¹ ã€è”é‚¦å­¦ä¹ å’ŒæŒç»­å­¦ä¹ ã€‚é€šè¿‡SAMergingï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†å¤šä¸ªåœ¨ä¸åŒä»»åŠ¡ä¸Šè®­ç»ƒçš„æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªé«˜æ€§èƒ½æ¨¡å‹ï¼Œä»è€Œé™ä½æ¨¡å‹éƒ¨ç½²å’Œç»´æŠ¤çš„æˆæœ¬ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics to approximate the optimal combination of parameters. This dependence is most critical in coefficient scaling, the weighting factors that modulate the magnitude of each fine-tuned model's contribution to the shared parameter. However, without a principled objective to guide their selection, these methods lead to brittle performance and are highly sensitive to scaling initialization. We address this gap by (i) establishing a novel flatness-aware PAC-Bayes generalization bound specifically for the model merging setting. This analysis introduces a "cross-task heterogeneity" term that formally captures the mismatch between diverse fine-tuned model priors and the target multi-task distributions. Guided by this theoretical insight, (ii) we frame model merging as multi-teacher knowledge distillation on scarce, unlabeled data. We formally demonstrate that minimizing the student-teacher Kullback-Leibler divergence directly tightens the upper bound on the merged model's excess risk. Guided by the flatness-aware bound derived, (iii) we operationalize this objective via SAMerging, a method that employs Sharpness-Aware Minimization (SAM) to find flat minima. Empirically, SAMerging establishes a new state of the art across vision and NLP benchmarks, achieving remarkable performance. The code is available at https://github.com/arshandalili/SAMerging.

