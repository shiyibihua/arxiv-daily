---
layout: default
title: SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models
---

# SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.05015" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.05015v1</a>
  <a href="https://arxiv.org/pdf/2508.05015.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.05015v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.05015v1', 'SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dai Do, Manh Nguyen, Svetha Venkatesh, Hung Le

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-07

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSPaRFTä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªé€‚åº”å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é€‰æ‹©` `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `èšç±»ç®—æ³•` `å¤šè‡‚è€è™æœº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•éœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å°å‹æ¨¡å‹çš„åº”ç”¨ã€‚
2. SPaRFTé€šè¿‡è‡ªé€‚åº”å­¦ä¹ æ¡†æ¶ï¼Œä¼˜åŒ–æ•°æ®é€‰æ‹©å’Œè®­ç»ƒæ—¶æœºï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSPaRFTåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ ·æœ¬ä½¿ç”¨é‡æ˜¾è‘—å‡å°‘ï¼Œæå‡äº†èµ„æºåˆ©ç”¨ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒæ—¶å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œä½¿å¾—å°å‹æ¨¡å‹éš¾ä»¥åº”ç”¨ã€‚å½“å‰çš„è¯¾ç¨‹å­¦ä¹ æˆ–æ•°æ®é€‰æ‹©æ–¹æ³•å¤šä¸ºå¯å‘å¼é©±åŠ¨ï¼Œä¸”è®¡ç®—èµ„æºæ¶ˆè€—å·¨å¤§ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ™®é€‚æ€§ã€‚æœ¬æ–‡æå‡ºSPaRFTï¼Œä¸€ä¸ªè‡ªé€‚åº”å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–æ•°æ®ä½¿ç”¨å’Œè®­ç»ƒæ—¶æœºï¼ŒåŸºäºæ¨¡å‹èƒ½åŠ›å®ç°é«˜æ•ˆå­¦ä¹ ã€‚é¦–å…ˆï¼Œé‡‡ç”¨åŸºäºèšç±»çš„æ•°æ®å‡å°‘æ–¹æ³•å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œè¯­ä¹‰å’Œéš¾åº¦åˆ’åˆ†ï¼Œæå–å‡ºç´§å‡‘ä¸”å¤šæ ·çš„å­é›†ä»¥å‡å°‘å†—ä½™ã€‚ç„¶åï¼Œåˆ©ç”¨å¤šè‡‚è€è™æœºç®—æ³•å°†æ•°æ®é›†ç¾¤è§†ä¸ºè‡‚ï¼Œä¼˜åŒ–è®­ç»ƒæ ·æœ¬çš„åˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPaRFTåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šå®ç°äº†ä¸æœ€å…ˆè¿›åŸºçº¿ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ ·æœ¬é‡å‡å°‘äº†å¤šè¾¾100å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­å¯¹æ•°æ®å’Œè®¡ç®—èµ„æºçš„é«˜éœ€æ±‚é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å¤§é‡æ ·æœ¬å’Œè®¡ç®—ï¼Œå¯¼è‡´å°å‹æ¨¡å‹éš¾ä»¥æœ‰æ•ˆè®­ç»ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSPaRFTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªé€‚åº”å­¦ä¹ æ¡†æ¶ï¼Œä¼˜åŒ–æ•°æ®é€‰æ‹©å’Œè®­ç»ƒæ—¶æœºï¼Œä»¥æå‡è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡èšç±»å’Œå¤šè‡‚è€è™æœºç®—æ³•ï¼ŒåŠ¨æ€è°ƒæ•´è®­ç»ƒæ ·æœ¬çš„åˆ†é…ï¼Œç¡®ä¿æ¨¡å‹åœ¨é€‚å½“çš„æ—¶æœºæ¥è§¦åˆ°åˆé€‚çš„æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPaRFTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºèšç±»çš„æ•°æ®å‡å°‘æ¨¡å—ï¼Œå°†è®­ç»ƒæ•°æ®æŒ‰è¯­ä¹‰å’Œéš¾åº¦è¿›è¡Œåˆ’åˆ†ï¼›å…¶æ¬¡æ˜¯å¤šè‡‚è€è™æœºæ¨¡å—ï¼Œä¼˜åŒ–è®­ç»ƒæ ·æœ¬çš„åˆ†é…ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šSPaRFTçš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆäº†æ•°æ®èšç±»å’Œè‡ªé€‚åº”æ ·æœ¬é€‰æ‹©ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ‰€éœ€çš„æ ·æœ¬é‡ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¯å‘å¼æ•°æ®é€‰æ‹©æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒSPaRFTé‡‡ç”¨äº†èšç±»ç®—æ³•å¯¹æ•°æ®è¿›è¡Œè¯­ä¹‰å’Œéš¾åº¦åˆ’åˆ†ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å’Œä»£è¡¨æ€§ã€‚åŒæ—¶ï¼Œå¤šè‡‚è€è™æœºç®—æ³•ç”¨äºåŠ¨æ€è°ƒæ•´æ ·æœ¬åˆ†é…ï¼Œä¾æ®æ¨¡å‹å½“å‰æ€§èƒ½è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SPaRFTåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡ä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶æ ·æœ¬ä½¿ç”¨é‡å‡å°‘äº†å¤šè¾¾100å€ã€‚è¿™ä¸€æ˜¾è‘—æå‡å±•ç¤ºäº†è‡ªé€‚åº”å­¦ä¹ æ¡†æ¶åœ¨èµ„æºåˆ©ç”¨ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SPaRFTçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜å°å‹æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿å¾—èµ„æºæœ‰é™çš„ç¯å¢ƒä¸‹ä¹Ÿèƒ½å®ç°é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨AIæŠ€æœ¯çš„æ™®åŠå’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have shown strong reasoning capabilities when fine-tuned with reinforcement learning (RL). However, such methods require extensive data and compute, making them impractical for smaller models. Current approaches to curriculum learning or data selection are largely heuristic-driven or demand extensive computational resources, limiting their scalability and generalizability. We propose \textbf{SPaRFT}, a self-paced learning framework that enables efficient learning based on the capability of the model being trained through optimizing which data to use and when. First, we apply \emph{cluster-based data reduction} to partition training data by semantics and difficulty, extracting a compact yet diverse subset that reduces redundancy. Then, a \emph{multi-armed bandit} treats data clusters as arms, optimized to allocate training samples based on model current performance. Experiments across multiple reasoning benchmarks show that SPaRFT achieves comparable or better accuracy than state-of-the-art baselines while using up to \(100\times\) fewer samples. Ablation studies and analyses further highlight the importance of both data clustering and adaptive selection. Our results demonstrate that carefully curated, performance-driven training curricula can unlock strong reasoning abilities in LLMs with minimal resources.

