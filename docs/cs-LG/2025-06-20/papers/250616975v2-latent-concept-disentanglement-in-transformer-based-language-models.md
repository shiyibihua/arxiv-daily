---
layout: default
title: Latent Concept Disentanglement in Transformer-based Language Models
---

# Latent Concept Disentanglement in Transformer-based Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16975" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16975v2</a>
  <a href="https://arxiv.org/pdf/2506.16975.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16975v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16975v2', 'Latent Concept Disentanglement in Transformer-based Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guan Zhe Hong, Bhavya Vasudeva, Vatsal Sharan, Cyrus Rashtchian, Prabhakar Raghavan, Rina Panigrahy

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-09-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ½œåœ¨æ¦‚å¿µè§£è€¦æ–¹æ³•ä»¥å¢å¼ºå˜æ¢å™¨è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ½œåœ¨æ¦‚å¿µè§£è€¦` `å˜æ¢å™¨æ¨¡å‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æœºæ¢°è§£é‡Šæ€§` `æ¨ç†èƒ½åŠ›` `æ•°å€¼æ¦‚å¿µ` `ä½ç»´å­ç©ºé—´` `æ¨¡å‹åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•æœ‰æ•ˆè¯†åˆ«å’Œåˆ©ç”¨æ½œåœ¨æ¦‚å¿µä»ç„¶ä¸æ˜ç¡®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæœ¬ç ”ç©¶é€šè¿‡æœºæ¢°è§£é‡Šæ€§åˆ†æï¼Œæ¢ç´¢å˜æ¢å™¨æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­å¦‚ä½•è§£è€¦å’Œç»„åˆæ½œåœ¨æ¦‚å¿µã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­æˆåŠŸè¯†åˆ«æ½œåœ¨æ¦‚å¿µï¼Œå¹¶åœ¨è¡¨ç¤ºç©ºé—´ä¸­æ‰¾åˆ°ä½ç»´å­ç©ºé—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰è§£å†³æ–°ä»»åŠ¡æ—¶ï¼Œå¿…é¡»ä»ç¤ºä¾‹ä¸­æ¨æ–­æ½œåœ¨æ¦‚å¿µã€‚æœ¬æ–‡é€šè¿‡æœºæ¢°è§£é‡Šæ€§å®éªŒæ¢è®¨å˜æ¢å™¨å¦‚ä½•è¡¨ç¤ºæ½œåœ¨ç»“æ„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å…·æœ‰æ½œåœ¨ç¦»æ•£æ¦‚å¿µçš„ä¼ é€’æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹æˆåŠŸè¯†åˆ«æ½œåœ¨æ¦‚å¿µå¹¶è¿›è¡Œé€æ­¥æ¦‚å¿µç»„åˆã€‚æ¥ç€ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”±æ½œåœ¨æ•°å€¼æ¦‚å¿µå‚æ•°åŒ–çš„ä»»åŠ¡ï¼Œå‘ç°æ¨¡å‹è¡¨ç¤ºç©ºé—´ä¸­å­˜åœ¨ä½ç»´å­ç©ºé—´ï¼Œå…¶å‡ ä½•å½¢çŠ¶æ¸…æ™°åæ˜ äº†åŸºç¡€å‚æ•°åŒ–ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è¯æ˜äº†å°å‹å’Œå¤§å‹æ¨¡å‹ç¡®å®èƒ½å¤Ÿè§£è€¦å¹¶åˆ©ç”¨ä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ çš„æ½œåœ¨æ¦‚å¿µã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å¦‚ä½•æœ‰æ•ˆè¯†åˆ«å’Œåˆ©ç”¨æ½œåœ¨æ¦‚å¿µçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ï¼Œå¾€å¾€æ— æ³•æ¸…æ™°åœ°è§£è€¦æ½œåœ¨æ¦‚å¿µï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æœºæ¢°è§£é‡Šæ€§åˆ†æï¼Œç ”ç©¶å˜æ¢å™¨æ¨¡å‹åœ¨ä¸åŒæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹å¦‚ä½•è¯†åˆ«å’Œç»„åˆæ½œåœ¨æ¦‚å¿µã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æ­ç¤ºæ¨¡å‹å†…éƒ¨çš„æ¨ç†æœºåˆ¶ï¼Œæå‡å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†æ§åˆ¶å®éªŒçš„æ–¹æ³•ï¼Œé¦–å…ˆåœ¨ä¼ é€’æ¨ç†ä»»åŠ¡ä¸­æµ‹è¯•æ¨¡å‹å¯¹ç¦»æ•£æ½œåœ¨æ¦‚å¿µçš„è¯†åˆ«èƒ½åŠ›ï¼Œç„¶ååœ¨æ•°å€¼æ¦‚å¿µå‚æ•°åŒ–çš„ä»»åŠ¡ä¸­åˆ†ææ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€ç»“æœåˆ†æå’Œå‡ ä½•ç‰¹å¾æå–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡æœºæ¢°è§£é‡Šæ€§åˆ†ææ­ç¤ºäº†å˜æ¢å™¨æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•è§£è€¦å’Œåˆ©ç”¨æ½œåœ¨æ¦‚å¿µã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œå‰è€…å…³æ³¨æ¨¡å‹å†…éƒ¨çš„æ¨ç†æœºåˆ¶ï¼Œè€Œåè€…å¾€å¾€åªå…³æ³¨æœ€ç»ˆç»“æœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„è®¾è®¡ä¹Ÿç»è¿‡ä¼˜åŒ–ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰æ½œåœ¨æ¦‚å¿µçš„å‡ ä½•ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ä¼ é€’æ¨ç†ä»»åŠ¡ä¸­æˆåŠŸè¯†åˆ«æ½œåœ¨æ¦‚å¿µï¼Œå¹¶åœ¨æ•°å€¼æ¦‚å¿µå‚æ•°åŒ–ä»»åŠ¡ä¸­å‘ç°ä½ç»´å­ç©ºé—´ï¼Œå‡ ä½•å½¢çŠ¶æ¸…æ™°åæ˜ åŸºç¡€å‚æ•°åŒ–ã€‚è¿™è¡¨æ˜å°å‹å’Œå¤§å‹æ¨¡å‹å‡èƒ½æœ‰æ•ˆè§£è€¦å’Œåˆ©ç”¨æ½œåœ¨æ¦‚å¿µï¼Œæå‡æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹å¯¹æ½œåœ¨æ¦‚å¿µçš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„æ™ºèƒ½åŒ–å’Œè‡ªåŠ¨åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> When large language models (LLMs) use in-context learning (ICL) to solve a new task, they must infer latent concepts from demonstration examples. This raises the question of whether and how transformers represent latent structures as part of their computation. Our work experiments with several controlled tasks, studying this question using mechanistic interpretability. First, we show that in transitive reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. This builds upon prior work that analyzes single-step reasoning. Then, we consider tasks parameterized by a latent numerical concept. We discover low-dimensional subspaces in the model's representation space, where the geometry cleanly reflects the underlying parameterization. Overall, we show that small and large models can indeed disentangle and utilize latent concepts that they learn in-context from a handful of abbreviated demonstrations.

