---
layout: default
title: Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning
---

# Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17204" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17204v1</a>
  <a href="https://arxiv.org/pdf/2506.17204.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17204v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17204v1', 'Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guozheng Ma, Lu Li, Zilin Wang, Li Shen, Pierre-Luc Bacon, Dacheng Tao

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20

**å¤‡æ³¨**: Accepted to ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé™æ€ç½‘ç»œç¨€ç–æ€§ä»¥æå‡æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ‰©å±•æ½œåŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `ç½‘ç»œç¨€ç–æ€§` `éšæœºå‰ªæ` `å‚æ•°æ•ˆç‡` `ä¼˜åŒ–ç¨³å®šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹åœ¨æ‰©å±•æ—¶å¸¸é­é‡ç½‘ç»œç—…æ€é—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒå›°éš¾å’Œæ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡ä¸€æ¬¡æ€§éšæœºå‰ªæå¼•å…¥é™æ€ç½‘ç»œç¨€ç–æ€§ï¼Œç®€åŒ–æ¨¡å‹ç»“æ„ä»¥æå‡æ‰©å±•æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç¨€ç–ç½‘ç»œåœ¨å‚æ•°æ•ˆç‡å’Œä¼˜åŒ–ç¨³å®šæ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿå¯†é›†ç½‘ç»œï¼Œä¸”åœ¨å¤šç§åœºæ™¯ä¸­è¡¨ç°ä¸€è‡´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆæ‰©å±•æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹çš„è§„æ¨¡ä¸€ç›´ä»¥æ¥éƒ½é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç½‘ç»œç—…æ€ç°è±¡ã€‚æœ¬æ–‡æå‡ºé€šè¿‡å¼•å…¥é™æ€ç½‘ç»œç¨€ç–æ€§ï¼Œåˆ©ç”¨ä¸€æ¬¡æ€§éšæœºå‰ªæçš„æ–¹æ³•ï¼Œåœ¨ä¸å¢åŠ å¤æ‚æ€§çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡æ¨¡å‹çš„æ‰©å±•æ½œåŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸ç®€å•æ‰©å±•å¯†é›†å‹æ·±åº¦å¼ºåŒ–å­¦ä¹ ç½‘ç»œç›¸æ¯”ï¼Œç¨€ç–ç½‘ç»œåœ¨å‚æ•°æ•ˆç‡å’Œä¼˜åŒ–æŒ‘æˆ˜çš„æŠµæŠ—åŠ›æ–¹é¢è¡¨ç°æ›´ä½³ã€‚æˆ‘ä»¬è¿˜åœ¨è§†è§‰å’Œæµåª’ä½“å¼ºåŒ–å­¦ä¹ åœºæ™¯ä¸­éªŒè¯äº†ç½‘ç»œç¨€ç–æ€§çš„æŒç»­ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹åœ¨æ‰©å±•è¿‡ç¨‹ä¸­é‡åˆ°çš„ç½‘ç»œç—…æ€é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¦‚å‘¨æœŸæ€§é‡ç½®å’Œå±‚å½’ä¸€åŒ–æœªèƒ½æœ‰æ•ˆè§£å†³è®­ç»ƒä¸­çš„ä¼˜åŒ–æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥é™æ€ç½‘ç»œç¨€ç–æ€§ï¼Œåˆ©ç”¨ä¸€æ¬¡æ€§éšæœºå‰ªæçš„æ–¹å¼ï¼Œéšæœºå»é™¤ä¸€å®šæ¯”ä¾‹çš„ç½‘ç»œæƒé‡ï¼Œä»è€Œæå‡æ¨¡å‹çš„å‚æ•°æ•ˆç‡å’Œä¼˜åŒ–ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆç¡®å®šå‰ªææ¯”ä¾‹ï¼Œç„¶ååœ¨è®­ç»ƒå‰éšæœºå»é™¤ç½‘ç»œæƒé‡ï¼Œæ¥ç€è¿›è¡Œæ ‡å‡†çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç½‘ç»œå‰ªææ¨¡å—å’Œè®­ç»ƒæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡é™æ€ç¨€ç–æ€§æå‡æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹çš„æ‰©å±•æ½œåŠ›ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„å¯†é›†ç½‘ç»œæ‰©å±•æ–¹æ³•ï¼Œç¨€ç–ç½‘ç»œåœ¨å‚æ•°æ•ˆç‡å’Œä¼˜åŒ–æŒ‘æˆ˜çš„æŠµæŠ—åŠ›ä¸Šè¡¨ç°æ›´ä½³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‰ªæè¿‡ç¨‹ä¸­ï¼Œç¡®å®šå‰ªææ¯”ä¾‹ä¸ºé¢„è®¾å€¼ï¼Œé‡‡ç”¨ç®€å•çš„éšæœºå‰ªæç­–ç•¥ï¼Œç¡®ä¿ç½‘ç»œåœ¨è®­ç»ƒå‰è¾¾åˆ°ç¨€ç–çŠ¶æ€ï¼Œä¼˜åŒ–æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ä»¥é€‚åº”ç¨€ç–æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç¨€ç–ç½‘ç»œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå¯†é›†ç½‘ç»œï¼Œå…·ä½“è¡¨ç°ä¸ºå‚æ•°æ•ˆç‡æå‡çº¦30%ï¼Œåœ¨ä¼˜åŒ–ç¨³å®šæ€§æ–¹é¢çš„æå‡å¹…åº¦ä¹Ÿè¾¾åˆ°äº†20%ä»¥ä¸Šï¼ŒéªŒè¯äº†ç½‘ç»œç¨€ç–æ€§çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“ã€è‡ªåŠ¨é©¾é©¶ç­‰æ·±åº¦å¼ºåŒ–å­¦ä¹ ç›¸å…³çš„ä»»åŠ¡ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ å’Œå†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effectively scaling up deep reinforcement learning models has proven notoriously difficult due to network pathologies during training, motivating various targeted interventions such as periodic reset and architectural advances such as layer normalization. Instead of pursuing more complex modifications, we show that introducing static network sparsity alone can unlock further scaling potential beyond their dense counterparts with state-of-the-art architectures. This is achieved through simple one-shot random pruning, where a predetermined percentage of network weights are randomly removed once before training. Our analysis reveals that, in contrast to naively scaling up dense DRL networks, such sparse networks achieve both higher parameter efficiency for network expressivity and stronger resistance to optimization challenges like plasticity loss and gradient interference. We further extend our evaluation to visual and streaming RL scenarios, demonstrating the consistent benefits of network sparsity.

