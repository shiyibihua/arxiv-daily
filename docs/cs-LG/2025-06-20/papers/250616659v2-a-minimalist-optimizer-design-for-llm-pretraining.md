---
layout: default
title: A Minimalist Optimizer Design for LLM Pretraining
---

# A Minimalist Optimizer Design for LLM Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16659" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.16659v2</a>
  <a href="https://arxiv.org/pdf/2506.16659.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16659v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16659v2', 'A Minimalist Optimizer Design for LLM Pretraining')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Athanasios Glentis, Jiaxiang Li, Andi Han, Mingyi Hong

**ÂàÜÁ±ª**: cs.LG, cs.AI, math.OC

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-20 (Êõ¥Êñ∞: 2025-12-10)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SCALE‰ºòÂåñÂô®‰ª•ÊèêÈ´òLLMÈ¢ÑËÆ≠ÁªÉÊïàÁéá**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°Âûã` `‰ºòÂåñÂô®ËÆæËÆ°` `ÂÜÖÂ≠òÊïàÁéá` `SGD` `Ê∑±Â∫¶Â≠¶‰π†` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `Ê®°ÂûãËÆ≠ÁªÉ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËá™ÈÄÇÂ∫î‰ºòÂåñÂô®Â¶ÇAdamÂú®ÂÜÖÂ≠òÂíåËÆ°ÁÆóÊïàÁéá‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂Âú®Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉ‰∏≠„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫‰∫ÜSCALE‰ºòÂåñÂô®ÔºåÈÄöËøáÂàóÂêëÊ¢ØÂ∫¶ÂΩí‰∏ÄÂåñÂíåÂØπËæìÂá∫Â±ÇÂ∫îÁî®‰∏ÄÈò∂Âä®ÈáèÔºåÊòæËëóÊèêÂçáSGDÁöÑÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSCALEÂú®Â§ö‰∏™LLaMAÊ®°Âûã‰∏äË∂ÖË∂ä‰∫ÜAdamÂíåÂÖ∂‰ªñÂÜÖÂ≠òÈ´òÊïà‰ºòÂåñÂô®ÔºåË°®Áé∞Âá∫Ëâ≤„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËÆ≠ÁªÉÈÄöÂ∏∏‰æùËµñ‰∫éËá™ÈÄÇÂ∫î‰ºòÂåñÂô®Â¶ÇAdamÔºåËøô‰∫õ‰ºòÂåñÂô®ÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÊìç‰ΩúÂπ∂Ê∂àËÄóÂ§ßÈáèÂÜÖÂ≠òÊù•Áª¥Êä§‰∏ÄÈò∂Âíå‰∫åÈò∂Áü©„ÄÇÂ∞ΩÁÆ°Â∑≤ÊúâÁ†îÁ©∂ÊèêÂá∫‰∫ÜÁä∂ÊÄÅÂéãÁº©Âèò‰Ωì‰ª•Èôç‰ΩéÂÜÖÂ≠òÊ∂àËÄóÔºå‰ΩÜ‰ªçÈúÄÊé¢ËÆ®ÂØπSGDÁöÑÊúÄÂ∞è‰øÆÊîπ‰ª•ÂåπÈÖçÊúÄÂÖàËøõÁöÑÈ¢ÑËÆ≠ÁªÉÊÄßËÉΩ„ÄÇÊú¨ÊñáÈÄöËøáËá™‰∏ãËÄå‰∏äÁöÑÊñπÊ≥ïÔºåÊèêÂá∫‰∫Ü‰∏§ÁßçÁÆÄÂçïËÄåÈ´òÊïàÁöÑÊäÄÊúØÔºöÂàóÂêëÊ¢ØÂ∫¶ÂΩí‰∏ÄÂåñÂíå‰ªÖÂØπËæìÂá∫Â±ÇÂ∫îÁî®‰∏ÄÈò∂Âä®Èáè„ÄÇÁªìÂêàËøô‰∏§ÁßçÊäÄÊúØÔºåÂΩ¢Êàê‰∫ÜSCALE‰ºòÂåñÂô®ÔºåÂú®Â§ö‰∏™LLaMAÊ®°Âûã‰∏äÔºåSCALEÁöÑÊÄßËÉΩ‰∏éAdamÁõ∏ÂΩìÊàñÊõ¥‰ºòÔºåÂêåÊó∂‰ªÖ‰ΩøÁî®35-45%ÁöÑÊÄªÂÜÖÂ≠òÔºåÊàê‰∏∫Âú®ÂÜÖÂ≠òÈôêÂà∂‰∏ãËøõË°åÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÁöÑÊúâÂäõÂÄôÈÄâËÄÖ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ‰∏≠Ëá™ÈÄÇÂ∫î‰ºòÂåñÂô®ÁöÑÂÜÖÂ≠òÂíåËÆ°ÁÆóÊïàÁéáÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑ‰ºòÂåñÂô®Â¶ÇAdamÈúÄË¶ÅÂ§ßÈáèÂÜÖÂ≠òÊù•Áª¥Êä§Ê¢ØÂ∫¶ÁöÑÂä®Èáè‰ø°ÊÅØÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏ãÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑSCALE‰ºòÂåñÂô®ÈÄöËøáÂºïÂÖ•ÂàóÂêëÊ¢ØÂ∫¶ÂΩí‰∏ÄÂåñÂíå‰ªÖÂØπËæìÂá∫Â±ÇÂ∫îÁî®‰∏ÄÈò∂Âä®ÈáèÔºåÂáèÂ∞ë‰∫ÜÂÜÖÂ≠òÊ∂àËÄóÔºåÂêåÊó∂‰øùÊåÅÊàñÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑËÆ≠ÁªÉÊÄßËÉΩ„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®ÊúÄÂ§ßÂåñSGDÁöÑÊïàÁéáÔºåÂáèÂ∞ëÂØπÂ§çÊùÇ‰ºòÂåñÂô®ÁöÑ‰æùËµñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSCALE‰ºòÂåñÂô®ÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÊòØÂàóÂêëÊ¢ØÂ∫¶ÂΩí‰∏ÄÂåñÊ®°ÂùóÔºåËØ•Ê®°ÂùóÂØπËæìÂá∫Áª¥Â∫¶ÁöÑÊ¢ØÂ∫¶ËøõË°åÂΩí‰∏ÄÂåñÔºõÂÖ∂Ê¨°ÊòØÂä®ÈáèÂ∫îÁî®Ê®°ÂùóÔºå‰ªÖÂú®ËæìÂá∫Â±ÇËÆ°ÁÆó‰∏ÄÈò∂Âä®Èáè„ÄÇËøôÁßçÁªìÊûÑ‰ΩøÂæó‰ºòÂåñËøáÁ®ãÊõ¥Âä†È´òÊïà„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSCALEÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂ÁÆÄÂçïÊÄßÂíåÈ´òÊïàÊÄßÔºåÈÄöËøáÂØπSGDÁöÑÊúÄÂ∞è‰øÆÊîπÂÆûÁé∞‰∫Ü‰∏éAdamÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºå‰∏îÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÈúÄÊ±Ç„ÄÇËøô‰∏éÁé∞ÊúâÁöÑÂ§çÊùÇËá™ÈÄÇÂ∫î‰ºòÂåñÂô®ÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®SCALE‰∏≠ÔºåÂàóÂêëÊ¢ØÂ∫¶ÂΩí‰∏ÄÂåñÁöÑÂÆûÁé∞ÊñπÂºèÁ°Æ‰øù‰∫ÜÊ¢ØÂ∫¶Âú®ÊØèÊ¨°Êõ¥Êñ∞Êó∂ÈÉΩËÉΩ‰øùÊåÅ‰∏ÄËá¥ÊÄßÔºåËÄåÂä®ÈáèÁöÑÈÄâÊã©‰ªÖÈôê‰∫éËæìÂá∫Â±ÇÂàôÊúâÊïàÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÂºÄÈîÄ„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Êú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜËØ¥ÊòéÔºåÈúÄÂèÇËÄÉËÆ∫ÊñáÁöÑÂÖ∑‰ΩìÂÜÖÂÆπ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSCALEÂú®Â§ö‰∏™LLaMAÊ®°ÂûãÔºà60M-1BÔºâ‰∏ä‰∏éAdamÁöÑÊÄßËÉΩÁõ∏ÂΩìÊàñÊõ¥‰ºòÔºåÂêåÊó∂‰ªÖ‰ΩøÁî®35-45%ÁöÑÂÜÖÂ≠ò„ÄÇÊ≠§Â§ñÔºåSCALEÂú®LLaMA 7BÊ®°Âûã‰∏äË∂ÖË∂ä‰∫ÜAPOLLOÂíåMuonÁ≠âÊúÄÂÖàËøõÁöÑÂÜÖÂ≠òÈ´òÊïàÊñπÊ≥ïÔºåÂú®Âõ∞ÊÉëÂ∫¶ÂíåÂÜÖÂ≠òÊ∂àËÄóÊñπÈù¢ÂùáË°®Áé∞Âá∫Ëâ≤„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÜÖÂ≠òÂèóÈôêÁöÑÁéØÂ¢É‰∏≠„ÄÇSCALE‰ºòÂåñÂô®ÁöÑËÆæËÆ°ÂèØ‰ª•‰∏∫Á†îÁ©∂‰∫∫ÂëòÂíåÂ∑•Á®ãÂ∏àÊèê‰æõ‰∏ÄÁßçÈ´òÊïàÁöÑËÆ≠ÁªÉÊñπÊ°àÔºåÊé®Âä®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÂÖ∂‰ªñÁõ∏ÂÖ≥È¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇÊú™Êù•ÔºåSCALEÂèØËÉΩ‰ºöË¢´ÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßçÊ∑±Â∫¶Â≠¶‰π†‰ªªÂä°ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶Å‰ºòÂåñËÆ°ÁÆóËµÑÊ∫êÁöÑÂú∫ÊôØ‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Training large language models (LLMs) typically relies on adaptive optimizers such as Adam, which introduce extra operations and require significant more memory to maintain first- and second-order moments than SGD. While recent works such as GaLore, Fira and APOLLO have proposed state-compressed variants to reduce memory consumption, a fundamental question remains: What are the minimum modifications to plain SGD needed to match state-of-the-art pretraining performance? We systematically investigate this question using a bottom-up approach, and identify two simple yet highly (memory- and compute-) efficient techniques: (1) column-wise gradient normalization (normalizing the gradient along the output dimension), which boosts SGD performance without momentum; and (2) applying first-order momentum only to the output layer, where gradient variance is highest. Combining these two techniques lead to SCALE (Stochastic Column-normAlized Last-layer momEntum), a simple optimizer for memory efficient pretraining. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the performance of Adam while using only 35-45% of the total memory. It also consistently outperforms memory-efficient optimizers such as GaLore, Fira and APOLLO, making it a strong candidate for large-scale pretraining under memory constraints. For LLaMA 7B model, SCALE outperforms the state-of-the-art memory-efficient methods APOLLO and Muon, in terms of both perplexity and memory consumption.

