---
layout: default
title: Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?
---

# Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17417" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17417v2</a>
  <a href="https://arxiv.org/pdf/2506.17417.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17417v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17417v2', 'Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingyuan Wu, Meitang Li, Jingcheng Yang, Jize Jiang, Kaizhuo Yan, Zhaoheng Li, Hanchao Yu, Minjia Zhang, Klara Nahrstedt

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-09-28)

**å¤‡æ³¨**: Work in progress, Short Version

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶é—´æ‰©å±•ä¸­çš„è‡ªéªŒè¯èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æ¨ç†æ—¶é—´æ‰©å±•` `è‡ªéªŒè¯èƒ½åŠ›` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨æ•ˆæœä¸å¦‚é¢„æœŸï¼Œå°¤å…¶æ˜¯åœ¨è‡ªéªŒè¯èƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. è®ºæ–‡é€šè¿‡è¯„ä¼°ä¸åŒçš„æ¨ç†æ—¶é—´æ‰©å±•ç­–ç•¥ï¼Œæ¢è®¨å…¶å¯¹è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æ¨¡å‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡æŸäº›ç­–ç•¥æå‡äº†æ€§èƒ½ï¼Œä½†å¤šæ•°æŠ•ç¥¨ç­–ç•¥çš„æ•ˆæœæ˜æ˜¾ä¼˜äºä»¥éªŒè¯ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œä¸”è‡ªéªŒè¯èƒ½åŠ›è¾ƒå¼±ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨ç†æ—¶é—´æŠ€æœ¯å¦‚è§£ç æ—¶é—´æ‰©å±•å’Œè‡ªæˆ‘ç²¾ç‚¼å·²è¢«è¯æ˜èƒ½æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ å¼•å‘çš„è‡ªæˆ‘ä¿®æ­£å’Œè‡ªæˆ‘éªŒè¯è¡Œä¸ºã€‚æœ¬ç ”ç©¶æ¢è®¨è¿™äº›æ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•æ˜¯å¦åŒæ ·èƒ½æƒ åŠè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ç»è¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æ¨¡å‹ã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡å¤šæ•°æŠ•ç¥¨å’Œæœ€ä½³Né€‰æ‹©çš„è‡ªéªŒè¯ç­–ç•¥æå‡äº†VLMæ€§èƒ½ï¼Œä½†å¤šæ•°æŠ•ç¥¨æ˜¾è‘—ä¼˜äºä»¥éªŒè¯ä¸ºä¸­å¿ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šå¸¸ä¸å¼ºåŒ–å­¦ä¹ è°ƒä¼˜æ¨¡å‹ç›¸å…³çš„æ¨ç†æ—¶é—´æ‰©å±•è¡Œä¸ºï¼Œå¦‚â€œA-haæ—¶åˆ»â€ï¼Œå¹¶æœªå¸¦æ¥ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„åˆ†ææŒ‡å‡ºä¸€ä¸ªå…³é”®é™åˆ¶ï¼šå½“å‰çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„VLMåœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¸Šè¡¨ç°å‡ºè¾ƒå¼±çš„è‡ªéªŒè¯èƒ½åŠ›ï¼Œé™åˆ¶äº†æ¨ç†æ—¶é—´æ‰©å±•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶é—´æ‰©å±•ä¸­çš„è‡ªéªŒè¯èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¿™ä¸€æ–¹é¢çš„æ•ˆæœä¸ç†æƒ³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è¯„ä¼°ä¸åŒæ¨ç†æ—¶é—´æ‰©å±•ç­–ç•¥å¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„å½±å“ï¼Œå°¤å…¶æ˜¯å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¨¡å‹çš„è‡ªéªŒè¯èƒ½åŠ›ï¼Œæ¢ç´¢å…¶æ½œåœ¨çš„æ€§èƒ½æå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹å¤šç§æ¨ç†æ—¶é—´æ‰©å±•ç­–ç•¥çš„è¯„ä¼°ï¼Œå¦‚å¤šæ•°æŠ•ç¥¨å’Œæœ€ä½³Né€‰æ‹©ï¼Œåˆ†æå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯ä¸åŒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ­ç¤ºäº†å½“å‰å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªéªŒè¯èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå¹¶æŒ‡å‡ºå¤šæ•°æŠ•ç¥¨ç­–ç•¥åœ¨æå‡æ€§èƒ½æ–¹é¢çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„å‚æ•°å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥è¯„ä¼°å„æ¨ç†æ—¶é—´æ‰©å±•ç­–ç•¥çš„æ•ˆæœï¼Œç‰¹åˆ«å…³æ³¨è‡ªéªŒè¯æœºåˆ¶çš„è®¾è®¡ä¸å®ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨å¤šæ•°æŠ•ç¥¨ç­–ç•¥çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä»¥éªŒè¯ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚åŒæ—¶ï¼Œå¼ºåŒ–å­¦ä¹ è°ƒä¼˜æ¨¡å‹çš„è‡ªéªŒè¯èƒ½åŠ›æœªèƒ½å¸¦æ¥ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œæ˜¾ç¤ºå‡ºå½“å‰æ–¹æ³•çš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€å›¾åƒæè¿°ç”Ÿæˆå’Œå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤æ‚çš„è§†è§‰ä¸æ–‡æœ¬ä¿¡æ¯ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inference-time techniques such as decoding-time scaling and self-refinement have been shown to substantially improve reasoning in large language models (LLMs), driven by emergent self-correction and self-verification behaviors often elicited through reinforcement learning (RL). In this work, we investigate whether these inference-time scaling methods similarly benefit vision-language models (VLMs), especially those fine-tuned with RL. Through extensive evaluation, we find that while strategies like majority vote and best-of-N with self-verification enhance VLM performance, majority vote significantly outperforms verification-centric ones. Furthermore, inference time scaling behaviors commonly associated with RL-tuned models, such as the 'A-ha moment,' do not yield consistent performance gains. Our analysis identifies a key limitation: current RL-trained VLMs exhibit weak self-verification across both visual and textual modalities, limiting the effectiveness of inference-time scaling.

