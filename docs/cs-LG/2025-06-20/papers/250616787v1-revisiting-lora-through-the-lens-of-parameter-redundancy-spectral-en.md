---
layout: default
title: Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps
---

# Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16787" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16787v1</a>
  <a href="https://arxiv.org/pdf/2506.16787.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16787v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16787v1', 'Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiashun Cheng, Aochuan Chen, Nuo Chen, Ziqi Gao, Yuhan Li, Jia Li, Fugee Tsung

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20

**å¤‡æ³¨**: 18 pages; Accepted to ACL 2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSeLoRAä»¥è§£å†³LoRAå‚æ•°å†—ä½™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©é€‚åº”` `è°±ç¼–ç ` `æ¨¡å‹å¾®è°ƒ` `å‚æ•°å†—ä½™` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æœºè§†è§‰` `ä»£ç ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LoRAåœ¨å¾®è°ƒå¤§å‹æ¨¡å‹æ—¶å­˜åœ¨æ˜¾è‘—çš„å‚æ•°å†—ä½™ï¼Œé™åˆ¶äº†å…¶è¡¨è¾¾èƒ½åŠ›å’Œæ•ˆç‡ã€‚
2. æå‡ºSeLoRAï¼Œé€šè¿‡è°±ç¼–ç æŠ€æœ¯é‡æ–°å‚æ•°åŒ–LoRAï¼Œå‡å°‘å†—ä½™åŒæ—¶ä¿æŒè¡¨è¾¾èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSeLoRAåœ¨å¸¸è¯†æ¨ç†ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šæ€§èƒ½ä¼˜äºå¼ºåŸºçº¿ï¼Œä¸”å‚æ•°æ›´å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å·²æˆä¸ºå¾®è°ƒå¤§å‹åŸºç¡€æ¨¡å‹çš„é‡è¦æŠ€æœ¯ï¼Œä½†å…¶æ˜¾è‘—çš„å‚æ•°å†—ä½™é™åˆ¶äº†LoRAçš„èƒ½åŠ›å’Œæ•ˆç‡ã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†å†—ä½™å¯¹LoRAå¾®è°ƒçš„å½±å“ï¼Œå‘ç°å‡å°‘å¯†åº¦å†—ä½™ä¸ä¼šé™ä½è¡¨è¾¾èƒ½åŠ›ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è°±ç¼–ç ä½ç§©é€‚åº”ï¼ˆSeLoRAï¼‰ï¼Œåˆ©ç”¨è°±åŸºçš„å¼ºå¤§è¡¨è¾¾èƒ½åŠ›ï¼Œä»ç¨€ç–è°±å­ç©ºé—´é‡æ–°å‚æ•°åŒ–LoRAã€‚SeLoRAè®¾è®¡ç®€æ´ï¼Œèƒ½å¤Ÿä¸å¤šç§LoRAå˜ä½“æ— ç¼é›†æˆï¼Œæå‡æ€§èƒ½ï¼Œä½œä¸ºå¯æ‰©å±•çš„å³æ’å³ç”¨æ¡†æ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSeLoRAåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šä»¥æ›´å°‘çš„å‚æ•°å®ç°äº†æ›´é«˜çš„æ•ˆç‡ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LoRAåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å­˜åœ¨çš„å‚æ•°å†—ä½™é—®é¢˜ï¼Œè¿™ç§å†—ä½™é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ•ˆç‡ã€‚ç°æœ‰çš„LoRAæ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ¨¡å‹æ—¶ï¼Œå¾€å¾€é¢ä¸´å‚æ•°è¿‡å¤šè€Œå¯¼è‡´çš„è®¡ç®—èµ„æºæµªè´¹å’Œæ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„SeLoRAé€šè¿‡å¼•å…¥è°±ç¼–ç æŠ€æœ¯ï¼Œåˆ©ç”¨è°±åŸºçš„å¼ºå¤§è¡¨è¾¾èƒ½åŠ›ï¼Œä»ç¨€ç–è°±å­ç©ºé—´é‡æ–°å‚æ•°åŒ–LoRAã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯åœ¨å‡å°‘å†—ä½™çš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œæå‡å¾®è°ƒæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSeLoRAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè°±ç¼–ç æ¨¡å—ã€LoRAå‚æ•°åŒ–æ¨¡å—å’Œé›†æˆæ¨¡å—ã€‚è°±ç¼–ç æ¨¡å—è´Ÿè´£ç”Ÿæˆç¨€ç–è°±åŸºï¼ŒLoRAå‚æ•°åŒ–æ¨¡å—åˆ™åˆ©ç”¨è¿™äº›è°±åŸºè¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œé›†æˆæ¨¡å—ç¡®ä¿ä¸ç°æœ‰LoRAå˜ä½“çš„å…¼å®¹æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSeLoRAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡è°±ç¼–ç æœ‰æ•ˆå‡å°‘å‚æ•°å†—ä½™ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»ŸLoRAæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å¯¹å‚æ•°ç©ºé—´çš„é‡æ–°å®šä¹‰å’Œä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SeLoRAä¸­ï¼Œå…³é”®çš„å‚æ•°è®¾ç½®åŒ…æ‹¬è°±åŸºçš„é€‰æ‹©å’Œç¨€ç–åº¦çš„æ§åˆ¶ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼ŒSeLoRAé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°å‡è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSeLoRAåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œä¸”æ‰€éœ€å‚æ•°å‡å°‘äº†YY%ã€‚è¿™äº›ç»“æœéªŒè¯äº†SeLoRAåœ¨æé«˜æ•ˆç‡å’Œæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SeLoRAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆå¾®è°ƒå¤§å‹æ¨¡å‹çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œä»£ç ç”Ÿæˆç­‰ã€‚å…¶å¯æ‰©å±•æ€§å’Œå…¼å®¹æ€§ä½¿å¾—SeLoRAèƒ½å¤Ÿä¸ç°æœ‰çš„å¤šç§æ¨¡å‹æ¶æ„ç»“åˆï¼Œæå‡å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Low-Rank Adaptation (LoRA) has emerged as a prominent technique for fine-tuning large foundation models. Despite its successes, the substantial parameter redundancy, which limits the capacity and efficiency of LoRA, has been recognized as a bottleneck. In this work, we systematically investigate the impact of redundancy in fine-tuning LoRA and reveal that reducing density redundancy does not degrade expressiveness. Based on this insight, we introduce \underline{S}pectral-\underline{e}ncoding \underline{L}ow-\underline{R}ank \underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of spectral bases to re-parameterize LoRA from a sparse spectral subspace. Designed with simplicity, SeLoRA enables seamless integration with various LoRA variants for performance boosting, serving as a scalable plug-and-play framework. Extensive experiments substantiate that SeLoRA achieves greater efficiency with fewer parameters, delivering superior performance enhancements over strong baselines on various downstream tasks, including commonsense reasoning, math reasoning, and code generation.

