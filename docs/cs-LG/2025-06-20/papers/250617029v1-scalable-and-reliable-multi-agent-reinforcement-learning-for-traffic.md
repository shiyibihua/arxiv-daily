---
layout: default
title: Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment
---

# Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17029" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17029v1</a>
  <a href="https://arxiv.org/pdf/2506.17029.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17029v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17029v1', 'Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Leizhen Wang, Peibo Duan, Cheng Lyu, Zewen Wang, Zhiqiang He, Nan Zheng, Zhenliang Ma

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20

**DOI**: [10.1016/j.commtr.2025.100225](https://doi.org/10.1016/j.commtr.2025.100225)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMARL-OD-DAä»¥è§£å†³å¤§è§„æ¨¡äº¤é€šåˆ†é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `äº¤é€šåˆ†é…` `å¯æ‰©å±•æ€§` `å¯é æ€§` `åŸå¸‚äº¤é€šç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡äº¤é€šåˆ†é…é—®é¢˜æ—¶ï¼Œé¢ä¸´å¯æ‰©å±•æ€§å’Œå¯é æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºçš„MARL-OD-DAæ¡†æ¶é€šè¿‡å°†æ™ºèƒ½ä½“å®šä¹‰ä¸ºODå¯¹è·¯ç”±å™¨ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMARL-OD-DAåœ¨SiouxFallsç½‘ç»œä¸­å®ç°äº†æ›´ä¼˜çš„åˆ†é…è§£å†³æ–¹æ¡ˆï¼Œç›¸å¯¹å·®è·æ˜¾è‘—ä½äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§åŸå¸‚çš„å‘å±•å’Œå‡ºè¡Œéœ€æ±‚çš„å¢åŠ ï¼Œäº¤é€šåˆ†é…æ–¹æ³•é¢ä¸´ä¸¥æ ¼çš„è¦æ±‚ã€‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•åœ¨å»ºæ¨¡è‡ªé€‚åº”è·¯ç”±è¡Œä¸ºæ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä½†åœ¨å¤„ç†å¤§è§„æ¨¡ç½‘ç»œæ—¶é¢ä¸´å¯æ‰©å±•æ€§å’Œå¯é æ€§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MARL-OD-DAæ¡†æ¶ï¼Œå°†æ™ºèƒ½ä½“é‡æ–°å®šä¹‰ä¸ºèµ·ç‚¹-ç»ˆç‚¹ï¼ˆODï¼‰å¯¹è·¯ç”±å™¨ï¼Œæ˜¾è‘—æé«˜äº†å¯æ‰©å±•æ€§ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†åŸºäºDirichletçš„åŠ¨ä½œç©ºé—´å’ŒåŸºäºå±€éƒ¨ç›¸å¯¹å·®è·çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥å¢å¼ºè§£å†³æ–¹æ¡ˆçš„å¯é æ€§å’Œæ”¶æ•›æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤„ç†ä¸­ç­‰è§„æ¨¡ç½‘ç»œæ—¶è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨SiouxFallsç½‘ç»œä¸­ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œåˆ†é…è§£å†³æ–¹æ¡ˆçš„ç›¸å¯¹å·®è·é™ä½äº†94.99%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡äº¤é€šåˆ†é…ä¸­çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§é—®é¢˜ã€‚ç°æœ‰çš„MARLæ–¹æ³•åœ¨å¤„ç†å¤æ‚ç½‘ç»œæ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆåº”å¯¹å¤§é‡å‡ºè¡Œéœ€æ±‚ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†æ™ºèƒ½ä½“é‡æ–°å®šä¹‰ä¸ºèµ·ç‚¹-ç»ˆç‚¹ï¼ˆODï¼‰å¯¹è·¯ç”±å™¨ï¼Œè€Œéå•ä¸ªæ—…è¡Œè€…ï¼Œæ¥æå‡ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†åŸºäºDirichletçš„åŠ¨ä½œç©ºé—´å’Œå±€éƒ¨ç›¸å¯¹å·®è·çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥æé«˜è§£å†³æ–¹æ¡ˆçš„å¯é æ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMARL-OD-DAæ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯ODå¯¹çš„å®šä¹‰ä¸å»ºæ¨¡ï¼Œå…¶æ¬¡æ˜¯åŠ¨ä½œç©ºé—´çš„æ„å»ºï¼Œæœ€åæ˜¯åŸºäºå¥–åŠ±å‡½æ•°çš„å­¦ä¹ ä¸ä¼˜åŒ–ã€‚æ•´ä½“æµç¨‹é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œæ¥å®ç°é«˜æ•ˆçš„äº¤é€šåˆ†é…ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ™ºèƒ½ä½“çš„å®šä¹‰ä»ä¸ªä½“æ—…è¡Œè€…è½¬å˜ä¸ºODå¯¹è·¯ç”±å™¨ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æé«˜äº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤§è§„æ¨¡äº¤é€šç½‘ç»œã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åŠ¨ä½œç©ºé—´è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†Dirichletåˆ†å¸ƒè¿›è¡ŒåŠ¨ä½œä¿®å‰ªï¼Œä»¥å‡å°‘ä¸å¿…è¦çš„è®¡ç®—ã€‚åŒæ—¶ï¼Œå¥–åŠ±å‡½æ•°åŸºäºå±€éƒ¨ç›¸å¯¹å·®è·è¿›è¡Œè®¾è®¡ï¼Œæ—¨åœ¨å¼•å¯¼æ™ºèƒ½ä½“æ›´å¿«æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨SiouxFallsç½‘ç»œçš„å®éªŒä¸­ï¼ŒMARL-OD-DAæ¡†æ¶åœ¨10æ­¥å†…å®ç°äº†æ›´ä¼˜çš„äº¤é€šåˆ†é…è§£å†³æ–¹æ¡ˆï¼Œå…¶ç›¸å¯¹å·®è·æ¯”ä¼ ç»Ÿæ–¹æ³•ä½94.99%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚äº¤é€šç½‘ç»œæ—¶å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŸå¸‚äº¤é€šç®¡ç†ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„è·¯å¾„è§„åˆ’ç­‰ã€‚é€šè¿‡æé«˜äº¤é€šåˆ†é…çš„æ•ˆç‡å’Œå¯é æ€§ï¼ŒMARL-OD-DAæ¡†æ¶èƒ½å¤Ÿä¸ºåŸå¸‚äº¤é€šæµé‡çš„ä¼˜åŒ–æä¾›é‡è¦æ”¯æŒï¼Œå…·æœ‰æ˜¾è‘—çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The evolution of metropolitan cities and the increase in travel demands impose stringent requirements on traffic assignment methods. Multi-agent reinforcement learning (MARL) approaches outperform traditional methods in modeling adaptive routing behavior without requiring explicit system dynamics, which is beneficial for real-world deployment. However, MARL frameworks face challenges in scalability and reliability when managing extensive networks with substantial travel demand, which limiting their practical applicability in solving large-scale traffic assignment problems. To address these challenges, this study introduces MARL-OD-DA, a new MARL framework for the traffic assignment problem, which redefines agents as origin-destination (OD) pair routers rather than individual travelers, significantly enhancing scalability. Additionally, a Dirichlet-based action space with action pruning and a reward function based on the local relative gap are designed to enhance solution reliability and improve convergence efficiency. Experiments demonstrate that the proposed MARL framework effectively handles medium-sized networks with extensive and varied city-level OD demand, surpassing existing MARL methods. When implemented in the SiouxFalls network, MARL-OD-DA achieves better assignment solutions in 10 steps, with a relative gap that is 94.99% lower than that of conventional methods.

