---
layout: default
title: Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity
---

# Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17155" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17155v2</a>
  <a href="https://arxiv.org/pdf/2506.17155.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17155v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17155v2', 'Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Samin Yeasar Arnob, Scott Fujimoto, Doina Precup

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-06-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSparse-Regä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„å°æ ·æœ¬è¿‡æ‹Ÿåˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `ç¨€ç–æ€§æ­£åˆ™åŒ–` `è¿‡æ‹Ÿåˆ` `å°æ ·æœ¬å­¦ä¹ ` `è¿ç»­æ§åˆ¶` `æœºå™¨å­¦ä¹ ` `æ•°æ®æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å°æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºäº†'Sparse-Reg'æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ç¨€ç–æ€§æ¥å‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ï¼ŒSparse-Regæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ä½¿ç”¨å°æ•°æ®é›†çš„é—®é¢˜ã€‚å°½ç®¡è®¸å¤šå¸¸è§çš„ç¦»çº¿RLåŸºå‡†æµ‹è¯•ä½¿ç”¨è¶…è¿‡ä¸€ç™¾ä¸‡çš„æ•°æ®ç‚¹ï¼Œä½†è®¸å¤šç¦»çº¿RLåº”ç”¨ä¾èµ–äºç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å‘ç°ï¼Œç¦»çº¿RLç®—æ³•åœ¨å°æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†'Sparse-Reg'ï¼šä¸€ç§åŸºäºç¨€ç–æ€§çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»¥å‡è½»ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆï¼Œä»è€Œåœ¨æœ‰é™æ•°æ®ç¯å¢ƒä¸­å®ç°æœ‰æ•ˆå­¦ä¹ ï¼Œå¹¶åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­å°æ ·æœ¬æ•°æ®é›†å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å°æ•°æ®é›†æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆå­¦ä¹ ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ç¨€ç–æ€§æ­£åˆ™åŒ–æŠ€æœ¯'Sparse-Reg'ï¼Œé€šè¿‡é™åˆ¶æ¨¡å‹çš„å¤æ‚æ€§æ¥å‡å°‘è¿‡æ‹Ÿåˆï¼Œä»è€Œåœ¨æœ‰é™çš„æ•°æ®ç¯å¢ƒä¸­å®ç°æ›´å¥½çš„å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼ŒSparse-Regè¢«åº”ç”¨äºæŸå¤±å‡½æ•°ä¸­ï¼Œä»¥å¼•å¯¼æ¨¡å‹å­¦ä¹ ç¨€ç–çš„è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥ç¨€ç–æ€§ä½œä¸ºæ­£åˆ™åŒ–æ‰‹æ®µï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼ŒSparse-Regèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åº”å¯¹å°æ ·æœ¬æ•°æ®é›†çš„æŒ‘æˆ˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒSparse-Regçš„æŸå¤±å‡½æ•°ç»“åˆäº†ç¨€ç–æ€§çº¦æŸï¼Œå…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©ç»è¿‡å®éªŒéªŒè¯ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡è°ƒæ•´è¶…å‚æ•°ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å°æ ·æœ¬æ•°æ®é›†ä¸Šå®ç°æ›´ä¼˜çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒSparse-Regåœ¨å¤šä¸ªè¿ç»­æ§åˆ¶ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨å°æ ·æœ¬ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å†³ç­–ç­‰éœ€è¦åœ¨å°æ ·æœ¬æ•°æ®ä¸‹è¿›è¡Œæœ‰æ•ˆå­¦ä¹ çš„åœºæ™¯ã€‚Sparse-Regçš„å¼•å…¥ä¸ºè¿™äº›é¢†åŸŸæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we investigate the use of small datasets in the context of offline reinforcement learning (RL). While many common offline RL benchmarks employ datasets with over a million data points, many offline RL applications rely on considerably smaller datasets. We show that offline RL algorithms can overfit on small datasets, resulting in poor performance. To address this challenge, we introduce "Sparse-Reg": a regularization technique based on sparsity to mitigate overfitting in offline reinforcement learning, enabling effective learning in limited data settings and outperforming state-of-the-art baselines in continuous control.

