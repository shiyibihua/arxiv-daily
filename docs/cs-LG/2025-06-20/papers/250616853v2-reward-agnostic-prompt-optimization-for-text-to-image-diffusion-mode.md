---
layout: default
title: Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models
---

# Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16853" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16853v2</a>
  <a href="https://arxiv.org/pdf/2506.16853.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16853v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16853v2', 'Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Semin Kim, Yeonwoo Cha, Jaehoon Yoo, Seunghoon Hong

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-09-29)

**å¤‡æ³¨**: 29 pages, Under review

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/seminkim/RATTPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRATTPOä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æç¤ºä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `æç¤ºä¼˜åŒ–` `æ‰©æ•£æ¨¡å‹` `å¥–åŠ±æ— å…³` `è‡ªåŠ¨æç¤ºå·¥ç¨‹` `å¤šæ¨¡æ€ç”Ÿæˆ` `æœç´¢æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨æç¤ºå·¥ç¨‹æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šçš„å¥–åŠ±é…ç½®ï¼Œå¯¼è‡´åœ¨ä¸åŒå¥–åŠ±æ¨¡å‹ä¸‹çš„åº”ç”¨æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºRATTPOæ–¹æ³•ï¼Œé€šè¿‡åœ¨æµ‹è¯•æ—¶ä¼˜åŒ–æç¤ºï¼Œèƒ½å¤Ÿé€‚åº”å¤šç§å¥–åŠ±åœºæ™¯è€Œæ— éœ€ä¿®æ”¹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRATTPOåœ¨æœç´¢æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿ï¼Œä¸”åœ¨è¶³å¤Ÿçš„æ¨ç†é¢„ç®—ä¸‹å¯ä¸å­¦ä¹ åŸºçº¿ç›¸åª²ç¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œé€šè¿‡åœ¨æµ‹è¯•æ—¶å¯»æ‰¾æœ€å¤§åŒ–å¥–åŠ±å‡½æ•°çš„æç¤ºï¼Œæ¥æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­çš„ç”¨æˆ·æç¤ºã€‚ç°æœ‰çš„è‡ªåŠ¨æç¤ºå·¥ç¨‹æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šçš„å¥–åŠ±é…ç½®ï¼Œå¯¼è‡´åœ¨ä¸åŒå¥–åŠ±æ¨¡å‹ä¸‹çš„åº”ç”¨æ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†RATTPOï¼ˆå¥–åŠ±æ— å…³çš„æµ‹è¯•æ—¶æç¤ºä¼˜åŒ–ï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨ä¸éœ€è¦ä¿®æ”¹çš„æƒ…å†µä¸‹é€‚ç”¨äºå„ç§å¥–åŠ±åœºæ™¯ã€‚RATTPOé€šè¿‡æŸ¥è¯¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿­ä»£æœç´¢ä¼˜åŒ–æç¤ºï¼Œä½¿ç”¨ä¼˜åŒ–è½¨è¿¹å’Œä¸€ç§æ–°é¢–çš„å¥–åŠ±æ„ŸçŸ¥åé¦ˆä¿¡å·ï¼ˆç§°ä¸ºâ€œæç¤ºâ€ï¼‰ä½œä¸ºä¸Šä¸‹æ–‡ã€‚å®éªŒè¯æ˜ï¼ŒRATTPOåœ¨å¤šç§å¥–åŠ±è®¾ç½®ä¸‹æœ‰æ•ˆæå‡ç”¨æˆ·æç¤ºï¼Œä¸”åœ¨æœç´¢æ•ˆç‡ä¸Šè¶…è¶Šå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œå¹³å‡è¿è¡Œé€Ÿåº¦æ¯”ç®€å•çš„å¥–åŠ±æ— å…³æœç´¢åŸºçº¿å¿«4.8å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰çš„è‡ªåŠ¨æç¤ºå·¥ç¨‹æ–¹æ³•åœ¨ä¸åŒå¥–åŠ±æ¨¡å‹ä¸‹çš„é€‚åº”æ€§ä¸è¶³ï¼Œå¯¼è‡´ç”Ÿæˆæ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRATTPOçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨æµ‹è¯•æ—¶ä¼˜åŒ–æç¤ºï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¿­ä»£æœç´¢ï¼Œè€Œä¸ä¾èµ–äºç‰¹å®šçš„å¥–åŠ±ä»»åŠ¡æè¿°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRATTPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æç¤ºç”Ÿæˆæ¨¡å—ã€ä¼˜åŒ–è½¨è¿¹è®°å½•å’Œå¥–åŠ±æ„ŸçŸ¥åé¦ˆä¿¡å·ç”Ÿæˆã€‚é€šè¿‡è¿™äº›æ¨¡å—çš„ååŒå·¥ä½œï¼Œå®ç°å¯¹ç”¨æˆ·æç¤ºçš„ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šRATTPOçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¥–åŠ±æ— å…³çš„è®¾è®¡ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šç§å¥–åŠ±åœºæ™¯ä¸‹çµæ´»åº”ç”¨ï¼ŒåŒºåˆ«äºç°æœ‰æ–¹æ³•çš„ç‰¹å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒRATTPOåˆ©ç”¨ä¼˜åŒ–è½¨è¿¹å’Œåé¦ˆä¿¡å·ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè®¾è®¡äº†é«˜æ•ˆçš„æœç´¢ç­–ç•¥ï¼Œç¡®ä¿åœ¨ä¸åŒå¥–åŠ±è®¾ç½®ä¸‹çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRATTPOåœ¨æœç´¢æ•ˆç‡ä¸Šè¶…è¶Šå…¶ä»–åŸºçº¿ï¼Œå¹³å‡è¿è¡Œé€Ÿåº¦æ¯”ç®€å•çš„å¥–åŠ±æ— å…³æœç´¢åŸºçº¿å¿«4.8å€ã€‚æ­¤å¤–ï¼Œåœ¨è¶³å¤Ÿçš„æ¨ç†é¢„ç®—ä¸‹ï¼ŒRATTPOçš„æ€§èƒ½å¯ä¸éœ€è¦å¥–åŠ±ç‰¹å®šå¾®è°ƒçš„å­¦ä¹ åŸºçº¿ç›¸åª²ç¾ï¼Œå±•ç°å‡ºå…¶å¼ºå¤§çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‰ºæœ¯åˆ›ä½œã€å¹¿å‘Šè®¾è®¡å’Œæ¸¸æˆå¼€å‘ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·ç”Ÿæˆæ›´ç¬¦åˆéœ€æ±‚çš„å›¾åƒã€‚é€šè¿‡ä¼˜åŒ–ç”¨æˆ·æç¤ºï¼ŒRATTPOå¯ä»¥æå‡ç”Ÿæˆæ¨¡å‹çš„å®ç”¨æ€§å’Œçµæ´»æ€§ï¼Œæœªæ¥å¯èƒ½åœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate a general approach for improving user prompts in text-to-image (T2I) diffusion models by finding prompts that maximize a reward function specified at test-time. Although diverse reward models are used for evaluating image generation, existing automated prompt engineering methods typically target specific reward configurations. Consequently, these specialized designs exhibit suboptimal performance when applied to new prompt engineering scenarios involving different reward models. To address this limitation, we introduce RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time optimization method applicable across various reward scenarios without modification. RATTPO iteratively searches for optimized prompts by querying large language models (LLMs) \textit{without} requiring reward-specific task descriptions. Instead, it uses the optimization trajectory and a novel reward-aware feedback signal (termed a "hint") as context. Empirical results demonstrate the versatility of RATTPO, effectively enhancing user prompts across diverse reward setups that assess various generation aspects, such as aesthetics, general human preference, or spatial relationships between objects. RATTPO surpasses other test-time search baselines in search efficiency, running 4.8 times faster than naive reward-agnostic test-time search baseline on average. Furthermore, with sufficient inference budget, it can achieve comparable performance to learning-based baselines that require reward-specific fine-tuning. The code is available at https://github.com/seminkim/RATTPO.

