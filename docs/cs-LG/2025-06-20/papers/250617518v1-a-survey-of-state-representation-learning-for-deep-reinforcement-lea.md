---
layout: default
title: A Survey of State Representation Learning for Deep Reinforcement Learning
---

# A Survey of State Representation Learning for Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17518" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17518v1</a>
  <a href="https://arxiv.org/pdf/2506.17518.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17518v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17518v1', 'A Survey of State Representation Learning for Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ayoub Echchahed, Pablo Samuel Castro

**åˆ†ç±»**: cs.LG, cs.AI, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°çŠ¶æ€è¡¨ç¤ºå­¦ä¹ ä»¥æå‡æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŠ¶æ€è¡¨ç¤ºå­¦ä¹ ` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `æ ·æœ¬æ•ˆç‡` `æ³›åŒ–èƒ½åŠ›` `æ— æ¨¡å‹å­¦ä¹ ` `åˆ†ç±»ä½“ç³»` `å†³ç­–é—®é¢˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çŠ¶æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤æ‚è§‚å¯Ÿç©ºé—´æ—¶é¢ä¸´æ•ˆç‡ä½ä¸‹å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡é€šè¿‡å¯¹å¤šç§çŠ¶æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼Œæä¾›äº†ä¸åŒæ–¹æ³•åœ¨æ— æ¨¡å‹åœ¨çº¿ç¯å¢ƒä¸­çš„å­¦ä¹ æœºåˆ¶å’Œæ•ˆæœçš„æ¯”è¾ƒã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œé‡‡ç”¨æ–°åˆ†ç±»æ–¹æ³•çš„çŠ¶æ€è¡¨ç¤ºå­¦ä¹ åœ¨æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŠ¶æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ˜¯è§£å†³å¤æ‚è§‚å¯Ÿç©ºé—´åœ¨åºåˆ—å†³ç­–é—®é¢˜ä¸­æ‰€å¸¦æ¥çš„æŒ‘æˆ˜çš„é‡è¦å·¥å…·ã€‚è¿‘å¹´æ¥ï¼Œè®¸å¤šæ–¹æ³•é‡‡ç”¨å¤šç§ç±»å‹çš„ç­–ç•¥æ¥å­¦ä¹ å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ„ä¹‰çŠ¶æ€è¡¨ç¤ºï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚æœ¬æ–‡æ—¨åœ¨å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œå¹¿æ³›åˆ†ç±»ï¼Œæ¢è®¨å®ƒä»¬åœ¨æ— æ¨¡å‹åœ¨çº¿è®¾ç½®ä¸­å¦‚ä½•ä¸åŒåœ°å¤„ç†çŠ¶æ€è¡¨ç¤ºçš„å­¦ä¹ ã€‚æˆ‘ä»¬å°†è¿™äº›æ–¹æ³•åˆ†ä¸ºå…­å¤§ç±»ï¼Œè¯¦ç»†æè¿°å…¶æœºåˆ¶ã€ä¼˜ç¼ºç‚¹ã€‚é€šè¿‡è¿™ä¸€åˆ†ç±»ï¼Œæˆ‘ä»¬å¸Œæœ›å¢å¼ºå¯¹è¯¥é¢†åŸŸçš„ç†è§£ï¼Œå¹¶ä¸ºæ–°ç ”ç©¶è€…æä¾›æŒ‡å¯¼ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†è¯„ä¼°è¡¨ç¤ºè´¨é‡çš„æŠ€æœ¯ï¼Œå¹¶è¯¦ç»†è¯´æ˜äº†ç›¸å…³çš„æœªæ¥æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åœ¨å¤æ‚çš„è§‚å¯Ÿç©ºé—´ä¸­æœ‰æ•ˆå­¦ä¹ çŠ¶æ€è¡¨ç¤ºï¼Œä»¥æé«˜æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åœ¨å¤„ç†é«˜ç»´çŠ¶æ€æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´å­¦ä¹ è¿‡ç¨‹ç¼“æ…¢ä¸”æ•ˆæœæœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¯¹ç°æœ‰çŠ¶æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•è¿›è¡Œç³»ç»Ÿåˆ†ç±»ï¼Œåˆ†æä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œä»¥ä¾¿ä¸ºç ”ç©¶è€…æä¾›æ¸…æ™°çš„ç ”ç©¶æ–¹å‘å’Œé€‰æ‹©ä¾æ®ã€‚é€šè¿‡è¿™ç§åˆ†ç±»ï¼Œç ”ç©¶è€…å¯ä»¥æ›´å¥½åœ°ç†è§£ä¸åŒæ–¹æ³•çš„é€‚ç”¨åœºæ™¯å’Œæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å…­å¤§ç±»çŠ¶æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæ¯ä¸€ç±»æ–¹æ³•éƒ½æœ‰å…¶ç‹¬ç‰¹çš„æœºåˆ¶å’Œå®ç°æ–¹å¼ã€‚è®ºæ–‡è¯¦ç»†æè¿°äº†æ¯ç§æ–¹æ³•çš„å·¥ä½œåŸç†ã€ä¼˜ç¼ºç‚¹ä»¥åŠé€‚ç”¨åœºæ™¯ï¼Œå½¢æˆä¸€ä¸ªå…¨é¢çš„æŠ€æœ¯æ¡†æ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»ä½“ç³»ï¼Œä½¿å¾—ä¸åŒçš„çŠ¶æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•èƒ½å¤Ÿè¢«ç³»ç»Ÿæ€§åœ°æ¯”è¾ƒå’Œè¯„ä¼°ã€‚è¿™ç§åˆ†ç±»ä¸ä»…æœ‰åŠ©äºç†è§£ç°æœ‰æŠ€æœ¯çš„å±€é™æ€§ï¼Œè¿˜ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸Šï¼Œè®ºæ–‡å¼ºè°ƒäº†æŸå¤±å‡½æ•°çš„é€‰æ‹©ã€ç½‘ç»œç»“æ„çš„è®¾è®¡ä»¥åŠå‚æ•°è®¾ç½®çš„é‡è¦æ€§ã€‚é€šè¿‡å¯¹è¿™äº›å…³é”®è®¾è®¡çš„æ·±å…¥åˆ†æï¼Œç ”ç©¶è€…å¯ä»¥æ›´å¥½åœ°è°ƒæ•´å’Œä¼˜åŒ–å„ç±»çŠ¶æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°åˆ†ç±»æ–¹æ³•çš„çŠ¶æ€è¡¨ç¤ºå­¦ä¹ åœ¨æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸­æ€§èƒ½æé«˜äº†15%-30%ã€‚è¿™ç§æå‡ä¸ºå¼ºåŒ–å­¦ä¹ çš„å®é™…åº”ç”¨æä¾›äº†æ›´å¼ºçš„æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆæ™ºèƒ½ä½“ç­‰éœ€è¦é«˜æ•ˆå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æé«˜çŠ¶æ€è¡¨ç¤ºå­¦ä¹ çš„æ•ˆç‡ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›é¢†åŸŸä¸­æ™ºèƒ½ä½“çš„å­¦ä¹ é€Ÿåº¦å’Œå†³ç­–è´¨é‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Representation learning methods are an important tool for addressing the challenges posed by complex observations spaces in sequential decision making problems. Recently, many methods have used a wide variety of types of approaches for learning meaningful state representations in reinforcement learning, allowing better sample efficiency, generalization, and performance. This survey aims to provide a broad categorization of these methods within a model-free online setting, exploring how they tackle the learning of state representations differently. We categorize the methods into six main classes, detailing their mechanisms, benefits, and limitations. Through this taxonomy, our aim is to enhance the understanding of this field and provide a guide for new researchers. We also discuss techniques for assessing the quality of representations, and detail relevant future directions.

