---
layout: default
title: SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification
---

# SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17368" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17368v2</a>
  <a href="https://arxiv.org/pdf/2506.17368.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17368v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17368v2', 'SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenglin Lai, Mengyao Liao, Bingzhe Wu, Dong Xu, Zebin Zhao, Zhihang Yuan, Chao Fan, Jianqiang Li

**åˆ†ç±»**: cs.LG, cs.AI, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-10-12)

**å¤‡æ³¨**: 10 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAFExä»¥è§£å†³MoEæ¶æ„LLMsçš„å®‰å…¨å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `å®‰å…¨å¯¹é½` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸“å®¶é€‰æ‹©` `å¯¹æŠ—æ€§è¾“å…¥` `å®‰å…¨å¹²é¢„` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯†é›†æ¨¡å‹æŠ€æœ¯æ— æ³•æœ‰æ•ˆåº”å¯¹MoEæ¶æ„ä¸­è·¯ç”±æœºåˆ¶å¸¦æ¥çš„å®‰å…¨å¯¹é½æŒ‘æˆ˜ï¼Œå¯¼è‡´å®‰å…¨é£é™©å¢åŠ ã€‚
2. SAFExæ¡†æ¶é€šè¿‡ç¨³å®šæ€§åŸºç¡€çš„ä¸“å®¶é€‰æ‹©ç¨‹åºï¼Œè¯†åˆ«å’ŒéªŒè¯å®‰å…¨å…³é”®ä¸“å®¶ï¼Œåˆ†ä¸ºHCDGå’ŒHRCGä¸¤ç»„ä»¥åº”å¯¹å®‰å…¨é—®é¢˜ã€‚
3. åœ¨Qwen3-30B-A3Bæ¨¡å‹ä¸Šï¼Œç¦ç”¨12ä¸ªä¸“å®¶åæ‹’ç»ç‡é™ä½22%ï¼Œå¹¶é€šè¿‡LoRAè½»é‡é€‚åº”å®ç°äº†åœ¨å¯¹æŠ—æ€§æç¤ºä¸‹çš„å®‰å…¨å“åº”æ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶è·¯ç”±æœºåˆ¶å¼•å…¥çš„å®‰å…¨å¯¹é½æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†è§£å†³ã€‚æœ¬æ–‡æ­£å¼åŒ–å¹¶ç³»ç»Ÿåˆ†æäº†MoEç‰¹æœ‰çš„å®‰å…¨é£é™©ï¼Œå³å®‰å…¨å¯¹é½è¡Œä¸ºä¾èµ–äºç‰¹å®šä¸“å®¶æ¨¡å—ã€‚æå‡ºäº†SAFExåˆ†ææ¡†æ¶ï¼Œé€šè¿‡åŸºäºç¨³å®šæ€§çš„ä¸“å®¶é€‰æ‹©ç¨‹åºï¼Œç¨³å¥åœ°è¯†åˆ«ã€è¡¨å¾å’ŒéªŒè¯å®‰å…¨å…³é”®ä¸“å®¶ï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºæœ‰å®³å†…å®¹æ£€æµ‹ç»„ï¼ˆHCDGï¼‰å’Œæœ‰å®³å“åº”æ§åˆ¶ç»„ï¼ˆHRCGï¼‰ã€‚é€šè¿‡å¯¹SAFExé€‰æ‹©çš„ä¸“å®¶è¿›è¡Œç›®æ ‡æ©è”½ï¼Œå‘ç°å®‰å…¨è¡Œä¸ºé«˜åº¦é›†ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¦ç”¨12ä¸ªé€‰å®šä¸“å®¶å¯å°†æ‹’ç»ç‡é™ä½22%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯MoEæ¶æ„ä¸­å®‰å…¨å¯¹é½ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œæ§åˆ¶å®‰å…¨å…³é”®ä¸“å®¶ï¼Œå¯¼è‡´å®‰å…¨é£é™©é›†ä¸­åœ¨ç‰¹å®šæ¨¡å—ä¸Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºSAFExæ¡†æ¶ï¼Œé€šè¿‡ç¨³å®šæ€§åŸºç¡€çš„ä¸“å®¶é€‰æ‹©ç¨‹åºï¼Œè¯†åˆ«å’ŒéªŒè¯å®‰å…¨å…³é”®ä¸“å®¶ï¼Œè¿›è€Œåˆ†è§£ä¸ºHCDGå’ŒHRCGï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„å®‰å…¨å¹²é¢„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSAFExæ¡†æ¶åŒ…æ‹¬ä¸“å®¶é€‰æ‹©ã€ä¸“å®¶è¡¨å¾å’ŒéªŒè¯ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œé¦–å…ˆé€šè¿‡ç¨³å®šæ€§åˆ†æé€‰æ‹©ä¸“å®¶ï¼Œç„¶åå¯¹å…¶è¿›è¡ŒåŠŸèƒ½åˆ†ç»„ï¼Œæœ€åè¿›è¡Œå¹²é¢„æµ‹è¯•ä»¥éªŒè¯æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šSAFExçš„åˆ›æ–°åœ¨äºé’ˆå¯¹MoEæ¶æ„çš„å®‰å…¨é£é™©è¿›è¡Œç³»ç»Ÿåˆ†æï¼Œé¦–æ¬¡æ˜ç¡®äº†å®‰å…¨è¡Œä¸ºçš„é›†ä¸­æ€§ï¼Œå¹¶æå‡ºäº†åŸºäºä¸“å®¶çš„å¹²é¢„æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†48ä¸ªMoE-FFNå±‚å’Œæ¯å±‚128ä¸ªä¸“å®¶çš„é…ç½®ï¼Œé‡‡ç”¨top-8è·¯ç”±ï¼Œç¦ç”¨ç‰¹å®šä¸“å®¶åé€šè¿‡è´Ÿæƒé‡åˆå¹¶è¿›è¡Œè½»é‡é€‚åº”ï¼Œä¼˜åŒ–äº†å®‰å…¨å“åº”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Qwen3-30B-A3Bæ¨¡å‹ä¸Šï¼Œç¦ç”¨12ä¸ªSAFExé€‰æ‹©çš„ä¸“å®¶åï¼Œæ‹’ç»ç‡é™ä½äº†22%ã€‚æ­¤å¤–ï¼Œé€šè¿‡LoRAè½»é‡é€‚åº”ï¼Œé’ˆå¯¹HRCGçš„è´Ÿæƒé‡åˆå¹¶æ˜¾è‘—æå‡äº†åœ¨å¯¹æŠ—æ€§æç¤ºä¸‹çš„å®‰å…¨å“åº”ï¼Œå±•ç¤ºäº†SAFExçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§æå‡ã€æ™ºèƒ½åŠ©æ‰‹çš„å®‰å…¨å“åº”æœºåˆ¶ä»¥åŠå¯¹æŠ—æ€§è¾“å…¥çš„å¤„ç†ã€‚é€šè¿‡æœ‰æ•ˆè¯†åˆ«å’Œæ§åˆ¶å®‰å…¨å…³é”®ä¸“å®¶ï¼ŒSAFExä¸ºæ¨¡å‹çš„å®‰å…¨æ€§æä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆçš„å¹²é¢„è·¯å¾„ï¼Œæœªæ¥å¯å¹¿æ³›åº”ç”¨äºå„ç±»åŸºäºMoEæ¶æ„çš„æ™ºèƒ½ç³»ç»Ÿä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models with Mixture-of-Experts (MoE) architectures achieve efficiency and scalability, yet their routing mechanisms introduce safety alignment challenges insufficiently addressed by techniques developed for dense models. In this work, the MoE-specific safety risk of positional vulnerability-that safety-aligned behaviors rely on specific expert modules-is formalized and systematically analyzed. An analytical framework, SAFEx, is presented to robustly identify, characterize, and validate safety-critical experts via a stability-based expert selection procedure, and to decompose them into two functional groups: the Harmful Content Detection Group (HCDG), which specializes in identifying and recognizing harmful content within user inputs, and the Harmful Response Control Group (HRCG), which specializes in controlling and enforcing model behaviors to generate appropriate safety responses. Expert-level interventions are conducted to probe causality and to test mitigation. Targeted masking of SAFEx-selected experts reveals that safety behavior is highly concentrated. On Qwen3-30B-A3B, configured with 48 MoE-FFN layers and 128 experts per layer under top-8 routing (48x128=6,144 experts in total), disabling 12 selected experts reduces the refusal rate by 22%. In addition, lightweight adaptation is performed using LoRA under three configurations-the HRCG, the union of HCDG and HRCG, and all experts-and the resulting updates are composed through negative weight merging targeted at the HRCG, leading to improved refusal under adversarial prompts without full-model retraining. These results establish positional vulnerability as a distinct MoE-specific safety challenge and provide a practical, compute-efficient pathway for expert-level safety interventions within routed architectures.

