---
layout: default
title: Sample-efficient LLM Optimization with Reset Replay
---

# Sample-efficient LLM Optimization with Reset Replay

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.06412" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.06412v2</a>
  <a href="https://arxiv.org/pdf/2508.06412.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.06412v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.06412v2', 'Sample-efficient LLM Optimization with Reset Replay')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zichuan Liu, Jinyu Wang, Lei Song, Jiang Bian

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08 (æ›´æ–°: 2025-08-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMä¼˜åŒ–æ–¹æ³•LoRRä»¥è§£å†³æ ·æœ¬æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ ·æœ¬æ•ˆç‡` `é‡ç½®é‡æ”¾` `åå¥½ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åè®­ç»ƒLLMä¼˜åŒ–æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œè¿‡æ‹Ÿåˆæ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå½±å“äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„LoRRæ–¹æ³•é€šè¿‡é«˜é‡æ”¾è®­ç»ƒå’Œå‘¨æœŸæ€§é‡ç½®ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜æ ·æœ¬åˆ©ç”¨æ•ˆç‡å¹¶å‡å°‘è¿‡æ‹Ÿåˆé£é™©ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoRRåœ¨æ•°å­¦ä»»åŠ¡ä¸Šä¸å¤æ‚çš„RLç®—æ³•ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œåè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•ï¼Œå°¤å…¶æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œåå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œæ¨åŠ¨äº†å…¶æ¨ç†èƒ½åŠ›çš„æå‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸é¢ä¸´æ ·æœ¬æ•ˆç‡ä½å’Œåˆå§‹ç»éªŒè¿‡æ‹Ÿåˆå¯¼è‡´çš„é¦–å› åå·®é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†LLMä¼˜åŒ–çš„é‡ç½®é‡æ”¾ï¼ˆLoRRï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ä»»ä½•åŸºäºåå¥½çš„ä¼˜åŒ–æ¡†æ¶çš„æ ·æœ¬æ•ˆç‡ã€‚LoRRçš„æ ¸å¿ƒæœºåˆ¶å…è®¸åœ¨é«˜é‡æ”¾æ¬¡æ•°ä¸‹è¿›è¡Œè®­ç»ƒï¼Œæœ€å¤§åŒ–æ¯ä¸ªæ•°æ®æ‰¹æ¬¡çš„åˆ©ç”¨ç‡ï¼Œå¹¶é€šè¿‡å‘¨æœŸæ€§é‡ç½®ç­–ç•¥æ¥å¯¹æŠ—è¿‡æ‹Ÿåˆé£é™©ã€‚æ­¤å¤–ï¼ŒLoRRç»“åˆäº†ç›‘ç£å¾®è°ƒå’ŒåŸºäºåå¥½çš„æŸå¤±ï¼Œè¿›ä¸€æ­¥å¢å¼ºæ•°æ®åˆ©ç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒLoRRæ˜¾è‘—æå‡äº†å¤šç§åå¥½ä¼˜åŒ–æ–¹æ³•åœ¨æ•°å­¦å’Œä¸€èˆ¬æ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åè®­ç»ƒLLMä¼˜åŒ–ä¸­çš„æ ·æœ¬æ•ˆç‡ä½å’Œè¿‡æ‹Ÿåˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é«˜é‡æ”¾è®­ç»ƒä¸­å®¹æ˜“å¯¼è‡´æ¨¡å‹è´¨é‡ä¸‹é™ï¼Œå½±å“å­¦ä¹ è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLoRRé€šè¿‡å¼•å…¥é‡ç½®é‡æ”¾æœºåˆ¶ï¼Œå…è®¸åœ¨é«˜é‡æ”¾æ¬¡æ•°ä¸‹è®­ç»ƒï¼ŒåŒæ—¶é€šè¿‡å‘¨æœŸæ€§é‡ç½®åˆå§‹æ•°æ®æ¥ä¿æŒç½‘ç»œçš„å¯å¡‘æ€§ï¼Œä»è€Œæé«˜æ ·æœ¬åˆ©ç”¨æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLoRRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€é‡æ”¾æœºåˆ¶å’Œä¼˜åŒ–ç›®æ ‡ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ”¶é›†é˜¶æ®µè·å–åˆå§‹æ•°æ®ï¼Œé‡æ”¾æœºåˆ¶é€šè¿‡é«˜é‡æ”¾æ¬¡æ•°å’Œå‘¨æœŸæ€§é‡ç½®æ¥ä¼˜åŒ–è®­ç»ƒï¼Œæœ€åé€šè¿‡æ··åˆä¼˜åŒ–ç›®æ ‡è¿›è¡Œæ¨¡å‹æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šLoRRçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶é‡ç½®é‡æ”¾ç­–ç•¥å’Œæ··åˆä¼˜åŒ–ç›®æ ‡çš„ç»“åˆï¼Œè¿™ä¸ä¼ ç»Ÿçš„åå¥½ä¼˜åŒ–æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è¿‡æ‹Ÿåˆå¹¶æå‡æ ·æœ¬åˆ©ç”¨ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LoRRä¸­ï¼Œé‡æ”¾æ¬¡æ•°çš„è®¾ç½®å’Œå‘¨æœŸæ€§é‡ç½®çš„é¢‘ç‡æ˜¯å…³é”®å‚æ•°ï¼ŒæŸå¤±å‡½æ•°ç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œåå¥½æŸå¤±ï¼Œä»¥å¢å¼ºæ•°æ®çš„åˆ©ç”¨æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LoRRçš„è¿­ä»£DPOæ–¹æ³•åœ¨å¤æ‚æ•°å­¦ä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸ä¸€äº›è®¡ç®—å¯†é›†å‹RLç®—æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†LoRRåœ¨æ ·æœ¬æ•ˆç‡å’Œæ¨ç†èƒ½åŠ›ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½é—®ç­”ç­‰ã€‚é€šè¿‡æé«˜æ ·æœ¬æ•ˆç‡ï¼ŒLoRRèƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in post-training Large Language Models (LLMs), particularly through Reinforcement Learning (RL) and preference optimization methods, are key drivers for enhancing their reasoning capabilities. However, these methods are often plagued by low sample efficiency and a susceptibility to primacy bias, where overfitting to initial experiences degrades policy quality and damages the learning process. To address these challenges, we introduce LLM optimization with Reset Replay (LoRR), a general and powerful plugin designed to enhance sample efficiency in any preference-based optimization framework. LoRR core mechanism enables training at a high replay number, maximizing the utility of each collected data batch. To counteract the risk of overfitting inherent in high-replay training, LoRR incorporates a periodic reset strategy with reusing initial data, which preserves network plasticity. Furthermore, it leverages a hybrid optimization objective, combining supervised fine-tuning (SFT) and preference-based losses to further bolster data exploitation. Our extensive experiments demonstrate that LoRR significantly boosts the performance of various preference optimization methods on both mathematical and general reasoning benchmarks. Notably, an iterative DPO approach augmented with LoRR achieves comparable performance on challenging math tasks, outperforming some complex and computationally intensive RL-based algorithms. These findings highlight that LoRR offers a practical, sample-efficient, and highly effective paradigm for LLM finetuning, unlocking greater performance from limited data.

