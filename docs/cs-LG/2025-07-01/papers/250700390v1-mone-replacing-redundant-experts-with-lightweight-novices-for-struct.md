---
layout: default
title: MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE
---

# MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00390" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00390v1</a>
  <a href="https://arxiv.org/pdf/2507.00390.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00390v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00390v1', 'MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Geng Zhang, Yuxuan Han, Yuxuan Lou, Wangbo Zhao, Yiqi Zhang, Yang You

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-07-01

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zxgx/mode-pd)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoNEä»¥è§£å†³MoEæ¨¡å‹å†—ä½™ä¸“å®¶å¸¦æ¥çš„å†…å­˜å¼€é”€é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `ç»“æ„åŒ–å‰ªæ` `è½»é‡çº§æ¨¡å‹` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„MoEæ¨¡å‹åœ¨éƒ¨ç½²æ—¶ç”±äºéœ€è¦ä¿ç•™æ‰€æœ‰ä¸“å®¶ï¼Œå¯¼è‡´æ˜¾è‘—çš„å†…å­˜å¼€é”€ï¼Œä¸”ç»“æ„åŒ–å‰ªææ–¹æ³•åœ¨æ€§èƒ½ä¸Šå­˜åœ¨ä¸ç¨³å®šæ€§ã€‚
2. æœ¬æ–‡æå‡ºMoNEæ–¹æ³•ï¼Œé€šè¿‡ç”¨è½»é‡çº§æ–°æ‰‹æ›¿æ¢å†—ä½™ä¸“å®¶ï¼ŒåŸºäºè®¿é—®é¢‘ç‡å’Œè¾“å‡ºæ–¹å·®è¯„ä¼°ä¸“å®¶çš„å†—ä½™æ€§ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨25%å‰ªæç‡ä¸‹ï¼ŒMoNEåœ¨ä¹ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¹³å‡é›¶-shotå‡†ç¡®ç‡æé«˜äº†2.71ï¼Œåœ¨50%å‰ªæç‡ä¸‹æé«˜äº†3.61ï¼ŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰é€šè¿‡å¯¹æ¯ä¸ªè¾“å…¥ä»¤ç‰Œæ¿€æ´»éƒ¨åˆ†ä¸“å®¶ï¼Œå®ç°äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ‰©å±•ã€‚ç„¶è€Œï¼ŒMoEæ¨¡å‹çš„éƒ¨ç½²ä¼šå› éœ€è¦ä¿ç•™æ‰€æœ‰ä¸“å®¶è€Œå¯¼è‡´æ˜¾è‘—çš„å†…å­˜å¼€é”€ã€‚å°½ç®¡ç»“æ„åŒ–å‰ªææœ‰æœ›é™ä½å†…å­˜æˆæœ¬ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ¨¡å‹æ¶æ„ã€æ ¡å‡†æ•°æ®æºå’Œæ ¡å‡†æ ·æœ¬å¤§å°ç­‰ä¸‰ä¸ªç»´åº¦ä¸Šè¡¨ç°ä¸ä½³ä¸”ä¸ç¨³å®šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸“å®¶å‰ªææ–¹æ³•â€”â€”æ··åˆæ–°æ‰‹ä¸ä¸“å®¶ï¼ˆMoNEï¼‰ï¼Œé€šè¿‡ç”¨è½»é‡çº§æ–°æ‰‹æ›¿æ¢å†—ä½™ä¸“å®¶ï¼Œå®ç°æœ‰æ•ˆä¸”ç¨³å¥çš„æ¨¡å‹å‹ç¼©ã€‚MoNEåŸºäºè®¿é—®é¢‘ç‡å’Œè¾“å‡ºæ–¹å·®ä¸¤ä¸ªæŒ‡æ ‡è¯„ä¼°ä¸“å®¶å†—ä½™ï¼Œä¿®å‰ªä½ä½¿ç”¨ç‡ä¸”è¾“å‡ºç¨³å®šçš„ä¸“å®¶ï¼Œå¹¶ç”¨è½»é‡çº§æ–°æ‰‹æ›¿ä»£ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘æ€§èƒ½ä¸‹é™ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMoNEåœ¨ä¸‰ä¸ªç»´åº¦ä¸Šå‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸”å‡†ç¡®ç‡ä¸‹é™æœ€å°ï¼Œç¡®è®¤äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰åœ¨éƒ¨ç½²æ—¶çš„å†…å­˜å¼€é”€é—®é¢˜ï¼Œç°æœ‰çš„ç»“æ„åŒ–å‰ªææ–¹æ³•åœ¨æ¨¡å‹æ¶æ„ã€æ ¡å‡†æ•°æ®æºå’Œæ ·æœ¬å¤§å°ç­‰æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæ··åˆæ–°æ‰‹ä¸ä¸“å®¶ï¼ˆMoNEï¼‰æ–¹æ³•ï¼Œé€šè¿‡è¯„ä¼°ä¸“å®¶çš„è®¿é—®é¢‘ç‡å’Œè¾“å‡ºæ–¹å·®ï¼Œè¯†åˆ«å¹¶ä¿®å‰ªå†—ä½™ä¸“å®¶ï¼Œç”¨è½»é‡çº§æ–°æ‰‹æ›¿ä»£ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨å¹¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoNEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå†—ä½™ä¸“å®¶è¯„ä¼°æ¨¡å—ã€è½»é‡çº§æ–°æ‰‹ç”Ÿæˆæ¨¡å—å’Œæ¨¡å‹å‹ç¼©æ¨¡å—ã€‚é¦–å…ˆè¯„ä¼°ä¸“å®¶çš„å†—ä½™æ€§ï¼Œç„¶åç”Ÿæˆæ–°æ‰‹å¹¶æ›¿æ¢å†—ä½™ä¸“å®¶ï¼Œæœ€åè¿›è¡Œæ¨¡å‹çš„æ•´ä½“å‹ç¼©ä¸ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoNEçš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡å¼•å…¥è½»é‡çº§æ–°æ‰‹æ›¿ä»£å†—ä½™ä¸“å®¶ï¼Œè§£å†³äº†ä¼ ç»Ÿå‰ªææ–¹æ³•åœ¨æ€§èƒ½ä¸‹é™æ–¹é¢çš„ä¸è¶³ï¼Œç¡®ä¿äº†æ¨¡å‹å‹ç¼©çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒMoNEé€šè¿‡è®¾å®šè®¿é—®é¢‘ç‡å’Œè¾“å‡ºæ–¹å·®çš„é˜ˆå€¼æ¥è¯†åˆ«å†—ä½™ä¸“å®¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šç¡®ä¿äº†æ–°æ‰‹è¾“å‡ºä¸åŸä¸“å®¶è¾“å‡ºçš„ä¸€è‡´æ€§ï¼Œç½‘ç»œç»“æ„ä¸Šé‡‡ç”¨è½»é‡çº§æ¨¡å‹ä»¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMoNEåœ¨ä¹ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼Œå¹³å‡é›¶-shotå‡†ç¡®ç‡åœ¨25%å‰ªæç‡ä¸‹æé«˜äº†2.71ï¼Œåœ¨50%å‰ªæç‡ä¸‹æé«˜äº†3.61ï¼Œä¸”åœ¨ä¸‰ä¸ªç»´åº¦ä¸Šå‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²å’Œä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ã€‚é€šè¿‡æœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©ï¼ŒMoNEèƒ½å¤Ÿåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½å†…å­˜å¼€é”€ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) enables efficient scaling of large language models by activating only a subset of experts per input token. However, deploying MoE-based models incurs significant memory overhead due to the need to retain all experts in memory. While structured pruning is promising to reduce memory costs, existing methods often show suboptimal performance and unstable degradation in three dimensions: model architectures, calibration data sources, and calibration sample sizes. This paper proposes Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that replaces redundant experts with lightweight novices to achieve effective and robust model compression. MoNE evaluates expert redundancy based on two metrics: access frequency and output variance. Experts exhibiting low usage and stable outputs are pruned and replaced with lightweight novices-unbiased estimations of their original outputs-minimizing performance degradation. Extensive experiments demonstrate that MoNE consistently outperforms baseline methods with minimal accuracy degradation across the three dimensions, confirming its effectiveness and robustness. Notably, it improves the average zero shot accuracy across nine downstream tasks by up to 2.71 under 25\% pruning ratio and 3.61 under 50\% pruning. The code is available at https://github.com/zxgx/mode-pd.

