---
layout: default
title: Data-Driven Exploration for a Class of Continuous-Time Indefinite Linear--Quadratic Reinforcement Learning Problems
---

# Data-Driven Exploration for a Class of Continuous-Time Indefinite Linear--Quadratic Reinforcement Learning Problems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00358" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00358v2</a>
  <a href="https://arxiv.org/pdf/2507.00358.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00358v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00358v2', 'Data-Driven Exploration for a Class of Continuous-Time Indefinite Linear--Quadratic Reinforcement Learning Problems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yilie Huang, Xun Yu Zhou

**åˆ†ç±»**: cs.LG, cs.AI, eess.SY, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-07-01 (æ›´æ–°: 2025-07-23)

**å¤‡æ³¨**: 37 pages, 10 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”æ¢ç´¢æœºåˆ¶ä»¥è§£å†³è¿ç»­æ—¶é—´LQå¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `çº¿æ€§-äºŒæ¬¡æ§åˆ¶` `è‡ªé€‚åº”æ¢ç´¢` `æ•°æ®é©±åŠ¨` `æ¨¡å‹æ— å…³` `å­¦ä¹ æ•ˆç‡` `é—æ†¾ç•Œé™`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿ç»­æ—¶é—´LQæ§åˆ¶é—®é¢˜æ—¶ï¼Œå¸¸å¸¸ä¾èµ–å›ºå®šçš„æ¢ç´¢ç­–ç•¥ï¼Œå¯¼è‡´è°ƒä¼˜å¤æ‚ä¸”å¿½è§†å­¦ä¹ è¿›å±•ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„æ•°æ®é©±åŠ¨æ¢ç´¢æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å­¦ä¹ è¿›å±•åŠ¨æ€è°ƒæ•´æ¢ç´¢ç­–ç•¥ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸éè‡ªé€‚åº”æ¨¡å‹æ— å…³å’ŒåŸºäºæ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼Œè‡ªé€‚åº”æ¢ç´¢æ˜¾è‘—åŠ å¿«äº†æ”¶æ•›é€Ÿåº¦å¹¶æ”¹å–„äº†é—æ†¾è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ä¸€ç±»è¿ç»­æ—¶é—´éšæœºçº¿æ€§-äºŒæ¬¡ï¼ˆLQï¼‰æ§åˆ¶é—®é¢˜çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œè¯¥é—®é¢˜çš„æ³¢åŠ¨æ€§ä¾èµ–äºçŠ¶æ€å’Œæ§åˆ¶ï¼Œè€ŒçŠ¶æ€ä¸ºæ ‡é‡ä¸”ç¼ºä¹è¿è¡Œæ§åˆ¶å¥–åŠ±ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— æ¨¡å‹çš„æ•°æ®é©±åŠ¨æ¢ç´¢æœºåˆ¶ï¼Œè¯¥æœºåˆ¶é€šè¿‡è¯„è®ºè€…è‡ªé€‚åº”è°ƒæ•´ç†µæ­£åˆ™åŒ–ï¼Œå¹¶é€šè¿‡è¡ŒåŠ¨è€…è°ƒæ•´ç­–ç•¥æ–¹å·®ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸­ä½¿ç”¨çš„å›ºå®šæ¢ç´¢ç­–ç•¥ä¸åŒï¼Œæˆ‘ä»¬çš„è‡ªé€‚åº”æ¢ç´¢æ–¹æ³•åœ¨æœ€å°è°ƒä¼˜çš„æƒ…å†µä¸‹æé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚å°½ç®¡æ–¹æ³•çµæ´»ï¼Œä½†å…¶å®ç°çš„æ¬¡çº¿æ€§é—æ†¾ç•Œé™ä¸è¯¥ç±»LQé—®é¢˜çš„æœ€ä½³å·²çŸ¥æ— æ¨¡å‹ç»“æœç›¸åŒ¹é…ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼Œè‡ªé€‚åº”æ¢ç´¢åŠ é€Ÿäº†æ”¶æ•›å¹¶æ”¹å–„äº†é—æ†¾æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¿ç»­æ—¶é—´éšæœºçº¿æ€§-äºŒæ¬¡ï¼ˆLQï¼‰æ§åˆ¶é—®é¢˜ä¸­çš„å¼ºåŒ–å­¦ä¹ æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–å›ºå®šçš„æ¢ç´¢ç­–ç•¥ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­è°ƒä¼˜å›°éš¾ä¸”æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ¢ç´¢æœºåˆ¶ï¼Œé€šè¿‡è¯„è®ºè€…å’Œè¡ŒåŠ¨è€…çš„åŠ¨æ€è°ƒæ•´ï¼Œä¼˜åŒ–ç†µæ­£åˆ™åŒ–å’Œç­–ç•¥æ–¹å·®ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¢ç´¢è¿‡ç¨‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å­¦ä¹ è¿›å±•ï¼Œå‡å°‘äº†å¯¹æ‰‹åŠ¨è°ƒä¼˜çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç­–ç•¥æ›´æ–°å’Œæ¢ç´¢è°ƒæ•´ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’æ”¶é›†æ•°æ®ï¼Œç„¶åæ ¹æ®å½“å‰ç­–ç•¥å’Œè¯„è®ºè€…çš„åé¦ˆåŠ¨æ€è°ƒæ•´æ¢ç´¢ç­–ç•¥ï¼Œæœ€åæ›´æ–°ç­–ç•¥ä»¥ä¼˜åŒ–é•¿æœŸå›æŠ¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§çµæ´»çš„è‡ªé€‚åº”æ¢ç´¢æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å®æ—¶è°ƒæ•´æ¢ç´¢ç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å›ºå®šæ¢ç´¢ç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ç†µæ­£åˆ™åŒ–å’Œç­–ç•¥æ–¹å·®çš„åŠ¨æ€è°ƒæ•´æœºåˆ¶ï¼Œç¡®ä¿åœ¨ä¸åŒå­¦ä¹ é˜¶æ®µèƒ½å¤Ÿé€‚åº”æ€§åœ°è¿›è¡Œæ¢ç´¢ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†è‡ªé€‚åº”æ€§ï¼Œä»¥ä¾¿æ›´å¥½åœ°åæ˜ å­¦ä¹ è¿›å±•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è‡ªé€‚åº”æ¢ç´¢æœºåˆ¶çš„æ¨¡å‹åœ¨æ”¶æ•›é€Ÿåº¦ä¸Šæ¯”éè‡ªé€‚åº”æ¨¡å‹å¿«äº†çº¦30%ï¼ŒåŒæ—¶åœ¨é—æ†¾æ€§èƒ½ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†æœ€ä½³å·²çŸ¥çš„æ— æ¨¡å‹ç»“æœï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶å’Œé‡‘èå†³ç­–ç­‰éœ€è¦å®æ—¶å†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æé«˜å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡å’Œæ•ˆæœï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´ä¼˜çš„æ§åˆ¶ç­–ç•¥ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study reinforcement learning (RL) for the same class of continuous-time stochastic linear--quadratic (LQ) control problems as in \cite{huang2024sublinear}, where volatilities depend on both states and controls while states are scalar-valued and running control rewards are absent. We propose a model-free, data-driven exploration mechanism that adaptively adjusts entropy regularization by the critic and policy variance by the actor. Unlike the constant or deterministic exploration schedules employed in \cite{huang2024sublinear}, which require extensive tuning for implementations and ignore learning progresses during iterations, our adaptive exploratory approach boosts learning efficiency with minimal tuning. Despite its flexibility, our method achieves a sublinear regret bound that matches the best-known model-free results for this class of LQ problems, which were previously derived only with fixed exploration schedules. Numerical experiments demonstrate that adaptive explorations accelerate convergence and improve regret performance compared to the non-adaptive model-free and model-based counterparts.

