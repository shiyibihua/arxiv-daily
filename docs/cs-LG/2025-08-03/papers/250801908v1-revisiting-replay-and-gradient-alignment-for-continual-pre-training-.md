---
layout: default
title: Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models
---

# Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01908" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01908v1</a>
  <a href="https://arxiv.org/pdf/2508.01908.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01908v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01908v1', 'Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Istabrak Abbes, Gopeshh Subbaraj, Matthew Riemer, Nizar Islah, Benjamin Therien, Tsuguchika Tabaru, Hiroaki Kingetsu, Sarath Chandar, Irina Rish

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæŒç»­é¢„è®­ç»ƒæ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„åˆ†å¸ƒåç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒç»­é¢„è®­ç»ƒ` `å¤§è¯­è¨€æ¨¡å‹` `ç»éªŒé‡æ”¾` `æ¢¯åº¦å¯¹é½` `åˆ†å¸ƒåç§»` `æœºå™¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•åœ¨æ–°æ•°æ®å‡ºç°æ—¶éœ€è¦å®Œå…¨é‡å¯ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œæ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æŒç»­é¢„è®­ç»ƒï¼Œç»“åˆç»éªŒé‡æ”¾å’Œæ¢¯åº¦å¯¹é½æŠ€æœ¯ï¼Œæ¥åº”å¯¹åˆ†å¸ƒåç§»é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡å¤šæ ·æ€§ä¸‹ï¼Œé‡‡ç”¨æ–°æ–¹æ³•èƒ½æ˜¾è‘—æé«˜å­¦ä¹ ç¨³å®šæ€§ï¼Œå‡å°‘é—å¿˜ç°è±¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸éœ€è¦åœ¨åºå¤§çš„è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½†åœ¨æ–°æ•°æ®å‡ºç°æ—¶å¾€å¾€éœ€è¦å®Œå…¨é‡å¯è®­ç»ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ›´é«˜æ•ˆçš„æŒç»­é¢„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ–°æ•°æ®æ¥æ›´æ–°æ¨¡å‹ï¼Œè€Œéä»å¤´å¼€å§‹è®­ç»ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»éªŒé‡æ”¾å’Œæ¢¯åº¦å¯¹é½èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹åˆ†å¸ƒåç§»ï¼Œæå‡æ¨¡å‹çš„å­¦ä¹ ç¨³å®šæ€§ï¼Œé¿å…é—å¿˜ã€‚æˆ‘ä»¬é¦–æ¬¡åœ¨LLMé¢„è®­ç»ƒä¸­å±•ç¤ºäº†æ¢¯åº¦å¯¹é½æŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å…ƒç»éªŒé‡æ”¾å®ç°ï¼Œèƒ½å¤Ÿåœ¨å‡ ä¹ä¸å¢åŠ è®¡ç®—å’Œå†…å­˜å¼€é”€çš„æƒ…å†µä¸‹ï¼Œç»“åˆç»éªŒé‡æ”¾ä¸æ¢¯åº¦å¯¹é½çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æŒç»­é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç”±äºæ–°æ•°æ®å¼•å…¥å¯¼è‡´çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒï¼Œé€ æˆè®¡ç®—èµ„æºçš„æµªè´¹å’Œæ€§èƒ½çš„ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆç»éªŒé‡æ”¾å’Œæ¢¯åº¦å¯¹é½æŠ€æœ¯ï¼Œä»¥å®ç°æ›´ç¨³å®šçš„å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡ä¸æ–­æ›´æ–°æ¨¡å‹è€Œéé‡å¯è®­ç»ƒï¼Œæ¥æœ‰æ•ˆåº”å¯¹æ–°æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æŒç»­é¢„è®­ç»ƒçš„æ¨¡å‹æ›´æ–°æµç¨‹ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬ç»éªŒé‡æ”¾æœºåˆ¶å’Œæ¢¯åº¦å¯¹é½ç­–ç•¥ã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¨¡å—çš„ååŒä½œç”¨ï¼Œæå‡æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡åœ¨LLMé¢„è®­ç»ƒä¸­æœ‰æ•ˆåº”ç”¨æ¢¯åº¦å¯¹é½æŠ€æœ¯ï¼Œå¹¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å…ƒç»éªŒé‡æ”¾å®ç°ï¼Œèƒ½å¤Ÿåœ¨å‡ ä¹ä¸å¢åŠ è®¡ç®—å’Œå†…å­˜å¼€é”€çš„æƒ…å†µä¸‹ï¼Œç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œå®éªŒä¸­æ¢ç´¢äº†ä¸åŒçš„æ¨¡å‹è§„æ¨¡å’Œé‡æ”¾ç‡ï¼Œå‘ç°å°æ¯”ä¾‹çš„æ—§æ ·æœ¬é‡æ”¾æ¯”å¢åŠ æ¨¡å‹è§„æ¨¡æ›´å…·è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¹Ÿæå‡ºäº†ç›¸åº”çš„æŸå¤±å‡½æ•°è®¾è®¡ä»¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆç»éªŒé‡æ”¾å’Œæ¢¯åº¦å¯¹é½çš„æŒç»­é¢„è®­ç»ƒæ–¹æ³•åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡å¤šæ ·æ€§ä¸‹å‡è¡¨ç°å‡ºæ›´é«˜çš„å­¦ä¹ ç¨³å®šæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†X%ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é‡å¯è®­ç»ƒæ–¹æ³•ï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨æ–°æ•°æ®ç¯å¢ƒä¸‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸æ–­å˜åŒ–çš„ç”¨æˆ·éœ€æ±‚å’Œæ•°æ®åˆ†å¸ƒï¼Œä»è€Œæé«˜å®é™…åº”ç”¨çš„æ•ˆæœå’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹æ›´æ–°æœºåˆ¶åœ¨å„ç±»æ™ºèƒ½ç³»ç»Ÿä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.

