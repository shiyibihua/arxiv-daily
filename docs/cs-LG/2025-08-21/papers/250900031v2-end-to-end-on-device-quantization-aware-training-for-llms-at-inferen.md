---
layout: default
title: End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost
---

# End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00031" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.00031v2</a>
  <a href="https://arxiv.org/pdf/2509.00031.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00031v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00031v2', 'End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Qitao Tan, Xiaoying Song, Jin Lu, Guoming Li, Jun Liu, Lingzi Hong, Caiwen Ding, Jundong Li, Xiaoming Zhai, Shaoyi Huang, Wei Niu, Geng Yuan

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-21 (Êõ¥Êñ∞: 2025-09-29)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ZeroQAT‰ª•Ëß£ÂÜ≥LLMsÈáèÂåñËÆ≠ÁªÉ‰∏≠ÁöÑÈ´òÂÜÖÂ≠òÊ∂àËÄóÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈáèÂåñËÆ≠ÁªÉ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ËæπÁºòËÆ°ÁÆó` `ÂÜÖÂ≠ò‰ºòÂåñ` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÊñπÊ≥ïÂú®ÂæÆË∞ÉÊ®°ÂûãÂèÇÊï∞Êó∂Â≠òÂú®Â±ÄÈôêÊÄßÔºåÂØºËá¥Âú®‰Ωé‰ΩçÂÆΩÂú∫ÊôØ‰∏ãÂáÜÁ°ÆÊÄßÊòæËëó‰∏ãÈôç„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑZeroQATÊ°ÜÊû∂ÈÄöËøáÂâçÂêëÊ¢ØÂ∫¶‰º∞ËÆ°Ê∂àÈô§ÂèçÂêë‰º†Êí≠ÔºåÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÂíåËÆ°ÁÆóÂºÄÈîÄÔºåÊîØÊåÅÊùÉÈáçÂíåÊøÄÊ¥ªÁöÑÈáèÂåñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåZeroQATÂú®ÂÜÖÂ≠òÈúÄÊ±ÇÊòæËëóÈôç‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂú®Âçï‰∏™8GB GPU‰∏äÂØπ13BÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºåÁîöËá≥Âú®OnePlus 12ÊâãÊú∫‰∏äÂæÆË∞É6.7BÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈáèÂåñÊòØ‰∏ÄÁßçÊúâÊïàÈôç‰ΩéÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈÉ®ÁΩ≤ÊàêÊú¨ÁöÑÊäÄÊúØÔºåËÄåÂêéËÆ≠ÁªÉÈáèÂåñÔºàPTQÔºâÂõ†ÂÖ∂È´òÊïàÊÄßÂèóÂà∞ÂπøÊ≥õÁ†îÁ©∂„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâPTQÊñπÊ≥ïÊó†Ê≥ïÂæÆË∞ÉÊ®°ÂûãÂèÇÊï∞Ôºå‰∏îÂú®‰Ωé‰ΩçÂÆΩÂú∫ÊôØ‰∏ãÂ∏∏ÈÅ≠ÈÅáÊòæËëóÁöÑÂáÜÁ°ÆÊÄßÊçüÂ§±„ÄÇÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºàQATÔºâÊèê‰æõ‰∫ÜÊõ¥‰∏∫ÂêàÁêÜÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜÂÖ∂‰æùËµñÂèçÂêë‰º†Êí≠ÂØºËá¥ÁöÑÈ´òÂÜÖÂ≠òÊ∂àËÄóÈôêÂà∂‰∫ÜÂÖ∂Âú®LLMÈÉ®ÁΩ≤‰∏≠ÁöÑÂÆûÁî®ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜZeroQATÔºå‰∏Ä‰∏™Âü∫‰∫éÈõ∂Èò∂‰ºòÂåñÁöÑQATÊ°ÜÊû∂ÔºåÊîØÊåÅÊùÉÈáçÂíåÊøÄÊ¥ªÁöÑÈáèÂåñ„ÄÇZeroQATÂà©Áî®ÂâçÂêëÊ¢ØÂ∫¶‰º∞ËÆ°Ê∂àÈô§‰∫ÜÂèçÂêë‰º†Êí≠ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂíåÂÜÖÂ≠òÂºÄÈîÄÔºåÂêåÊó∂‰øùÁïô‰∫ÜÁ´ØÂà∞Á´Ø‰ºòÂåñÁöÑ‰ºòÂäø„ÄÇÂÆûÈ™åË°®ÊòéÔºåZeroQATÂú®ÂÜÖÂ≠òÈúÄÊ±ÇÊòæËëóÈôç‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÔºåÂßãÁªà‰ºò‰∫é‰ª£Ë°®ÊÄßÁöÑPTQÂíåQATÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÈáèÂåñËÆ≠ÁªÉÊñπÊ≥ïÂú®‰Ωé‰ΩçÂÆΩÂú∫ÊôØ‰∏ãÁöÑÂáÜÁ°ÆÊÄßÊçüÂ§±ÂèäÈ´òÂÜÖÂ≠òÊ∂àËÄóÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÊñπÊ≥ïÊó†Ê≥ïÂæÆË∞ÉÊ®°ÂûãÂèÇÊï∞ÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôçÔºåËÄåÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÊñπÊ≥ïÂèàÂõ†ÂèçÂêë‰º†Êí≠Â∏¶Êù•È´òÂÜÖÂ≠òÂºÄÈîÄÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂÆûÁî®ÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöZeroQATÊ°ÜÊû∂ÈááÁî®Èõ∂Èò∂‰ºòÂåñÊñπÊ≥ïÔºåÈÄöËøáÂâçÂêëÊ¢ØÂ∫¶‰º∞ËÆ°Êù•Êõø‰ª£ÂèçÂêë‰º†Êí≠Ôºå‰ªéËÄåÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂíåÂÜÖÂ≠òÂºÄÈîÄÔºåÂêåÊó∂ÂÆûÁé∞ÊùÉÈáçÂíåÊøÄÊ¥ªÁöÑÈáèÂåñ„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°‰ΩøÂæóÂú®ËµÑÊ∫êÂèóÈôêÁöÑËæπÁºòËÆæÂ§á‰∏äËøõË°åÁ´ØÂà∞Á´ØÁöÑÈáèÂåñËÆ≠ÁªÉÊàê‰∏∫ÂèØËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöZeroQATÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂâçÂêëÊ¢ØÂ∫¶‰º∞ËÆ°Ê®°Âùó„ÄÅÈáèÂåñÊ®°ÂùóÂíå‰ºòÂåñÊ®°Âùó„ÄÇÂâçÂêëÊ¢ØÂ∫¶‰º∞ËÆ°Ê®°ÂùóË¥üË¥£ËÆ°ÁÆóÊ¢ØÂ∫¶ÔºåËÄåÈáèÂåñÊ®°ÂùóÂàôÂØπÊùÉÈáçÂíåÊøÄÊ¥ªËøõË°åÈáèÂåñÔºå‰ºòÂåñÊ®°ÂùóÂàôËøõË°åÊ®°ÂûãÂèÇÊï∞ÁöÑÊõ¥Êñ∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöZeroQATÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ÈááÁî®ÁöÑÂâçÂêëÊ¢ØÂ∫¶‰º∞ËÆ°ÊñπÊ≥ïÔºåÊ∂àÈô§‰∫ÜÂØπÂèçÂêë‰º†Êí≠ÁöÑ‰æùËµñÔºåËøô‰∏é‰º†ÁªüÁöÑQATÊñπÊ≥ïÂΩ¢Êàê‰∫ÜÊú¨Ë¥®Âå∫Âà´ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÂíåËÆ°ÁÆóÈúÄÊ±Ç„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ZeroQAT‰∏≠ÔºåÊ®°ÂûãÂèÇÊï∞ÁöÑÂÜªÁªìÂíåÈ¢ÑÈáèÂåñËÆæËÆ°‰ΩøÂæóÂ§ßÈÉ®ÂàÜÂèÇÊï∞Âú®ÂæÆË∞ÉËøáÁ®ã‰∏≠‰øùÊåÅ‰∏çÂèòÔºå‰ªéËÄåËøõ‰∏ÄÊ≠•ÂáèÂ∞ëÂÜÖÂ≠ò‰ΩøÁî®„ÄÇÊ≠§Â§ñÔºåÊ°ÜÊû∂ÁöÑÊçüÂ§±ÂáΩÊï∞Âíå‰ºòÂåñÁ≠ñÁï•ÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Á°Æ‰øùÂú®ÈáèÂåñËøáÁ®ã‰∏≠Â∞ΩÂèØËÉΩ‰øùÁïôÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåZeroQATÂú®ÂÜÖÂ≠òÈúÄÊ±ÇÊòæËëóÈôç‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂú®Âçï‰∏™8GB GPU‰∏äÂØπ13BÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰∏îÂú®ÊûÅ‰Ωé‰ΩçÂÆΩÔºàÂ¶Ç2-4‰ΩçÔºâ‰∏ãË°®Áé∞Âá∫Ëâ≤„ÄÇÊ≠§Â§ñÔºåZeroQATËøòÊàêÂäüÂú®OnePlus 12ÊâãÊú∫‰∏äÂæÆË∞É6.7BÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ËæπÁºòËÆæÂ§á‰∏äÁöÑÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ZeroQATÊ°ÜÊû∂Âú®ËµÑÊ∫êÂèóÈôêÁöÑËæπÁºòËÆæÂ§á‰∏äÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÁßªÂä®ËÆæÂ§áÂíåÂµåÂÖ•ÂºèÁ≥ªÁªü‰∏≠ÔºåÂèØ‰ª•ÊúâÊïàÂú∞ÈÉ®ÁΩ≤Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÖ∂Èôç‰ΩéÁöÑÂÜÖÂ≠òÈúÄÊ±ÇÂíåËÆ°ÁÆóÂºÄÈîÄ‰ΩøÂæóÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÆûÁé∞È´òÊïàÁöÑÈáèÂåñËÆ≠ÁªÉÊàê‰∏∫ÂèØËÉΩÔºåÊé®Âä®‰∫ÜÊô∫ËÉΩËÆæÂ§áÁöÑÊô∫ËÉΩÂåñËøõÁ®ã„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Quantization is an effective technique to reduce the deployment cost of large language models (LLMs), and post-training quantization (PTQ) has been widely studied due to its efficiency. However, existing PTQ methods are limited by their inability to fine-tune model parameters and often suffer significant accuracy loss in low-bit scenarios. Quantization-aware training (QAT) provides a more principled solution, but its reliance on backpropagation incurs prohibitive memory costs, limiting its practicality for LLM deployment. To address these challenges, we propose ZeroQAT, a zeroth-order optimization-based QAT framework that supports both weight and activation quantization. ZeroQAT leverages forward-only gradient estimation to eliminate backpropagation, substantially reducing computational and memory overhead while retaining the benefits of end-to-end optimization. We further introduce a lightweight variant of ZeroQAT for quantized fine-tuning, which freezes and pre-quantizes most parameters to further cut memory usage. Experiments show that ZeroQAT consistently outperforms representative PTQ and QAT baselines while requiring significantly less memory. For example, ZeroQAT enables fine-tuning of a 13B model at extremely low bit-widths (e.g., 2-4 bits) on a single 8GB GPU, and even allows fine-tuning a 6.7B model on a OnePlus 12 smartphone, demonstrating its practicality for end-to-end QAT on resource-limited edge devices.

