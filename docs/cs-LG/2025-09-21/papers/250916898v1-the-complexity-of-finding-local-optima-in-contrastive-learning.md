---
layout: default
title: The Complexity of Finding Local Optima in Contrastive Learning
---

# The Complexity of Finding Local Optima in Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16898" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.16898v1</a>
  <a href="https://arxiv.org/pdf/2509.16898.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16898v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16898v1', 'The Complexity of Finding Local Optima in Contrastive Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jingming Yan, Yiyuan Luo, Vaggos Chatziafratis, Ioannis Panageas, Parnian Shahkar, Stelios Stavroulakis

**ÂàÜÁ±ª**: cs.LG, cs.CC, math.OC

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-21

**Â§áÊ≥®**: To appear as a conference paper in NeurIPS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ËØÅÊòéÂØπÊØîÂ≠¶‰π†‰∏≠Â±ÄÈÉ®ÊúÄ‰ºòËß£ÁöÑÂ§çÊùÇÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÂØπÊØîÂ≠¶‰π†` `Â±ÄÈÉ®ÊúÄ‰ºòËß£` `Â§çÊùÇÊÄßÁêÜËÆ∫` `‰ºòÂåñÁÆóÊ≥ï` `ËÆ°ÁÆóÊú∫ËßÜËßâ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂØπÊØîÂ≠¶‰π†ÊñπÊ≥ïÂú®ÂØªÊâæÂ±ÄÈÉ®ÊúÄ‰ºòËß£Êó∂Èù¢‰∏¥Â§çÊùÇÊÄßÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®Á¶ªÊï£ÂíåËøûÁª≠ËÆæÁΩÆ‰∏≠„ÄÇ
2. ËÆ∫ÊñáÈÄöËøáËØÅÊòéÂ±ÄÈÉ®ÊúÄ‰ºòËß£ÁöÑ$	extsf{PLS}$-Âõ∞ÈöæÊÄßÂíå$	extsf{CLS}$-Âõ∞ÈöæÊÄßÔºåÊòéÁ°Æ‰∫ÜËøô‰∏ÄÈóÆÈ¢òÁöÑÂ§çÊùÇÊÄß„ÄÇ
3. Á†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÈô§ÈùûÁâπÂÆöÂ§çÊùÇÊÄßÁ±ªÂÖ≥Á≥ªÊàêÁ´ãÔºåÂê¶ÂàôÊó†Ê≥ïÂú®Â§öÈ°πÂºèÊó∂Èó¥ÂÜÖÊâæÂà∞Â±ÄÈÉ®ÊúÄ‰ºòËß£„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂØπÊØîÂ≠¶‰π†ÊòØ‰∏ÄÁßçÈÄöËøá‰ºòÂåñÂü∫‰∫éÂØπÊØî‰ø°ÊÅØÁöÑÁõÆÊ†áÊù•ÂèëÁé∞ÊúâÊÑè‰πâÊï∞ÊçÆË°®Á§∫ÁöÑÂº∫Â§ßÊäÄÊúØ„ÄÇÊú¨ÊñáËß£ÂÜ≥‰∫ÜÂú®ÂêÑÁßçÂØπÊØîÂ≠¶‰π†ÈóÆÈ¢ò‰∏≠ÂØªÊâæÂ±ÄÈÉ®ÊúÄ‰ºòËß£ÁöÑÂ§çÊùÇÊÄßÔºåËØÅÊòé‰∫ÜÂú®Á¶ªÊï£ËÆæÁΩÆ‰∏ãÁöÑ$	extsf{PLS}$-Âõ∞ÈöæÊÄßÂíåÂú®ËøûÁª≠ËÆæÁΩÆ‰∏ãÁöÑ$	extsf{CLS}$-Âõ∞ÈöæÊÄß„ÄÇËøôÊÑèÂë≥ÁùÄÔºåÈô§Èùû$	extsf{PLS}	ext{Êàñ}	extsf{CLS}$Â±û‰∫éÂ§öÈ°πÂºèÊó∂Èó¥Á±ªÔºåÂê¶Âàô‰∏çÂ≠òÂú®Â§öÈ°πÂºèÊó∂Èó¥ÁÆóÊ≥ïËÉΩÂ§üÊâæÂà∞Â±ÄÈÉ®ÊúÄ‰ºòËß£„ÄÇÂç≥‰ΩøÂú®$	extsf{PLS}	ext{Êàñ}	extsf{CLS}$Â±û‰∫éÂ§öÈ°πÂºèÊó∂Èó¥Á±ªÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ±ÄÈÉ®ÊêúÁ¥¢ÁÆóÊ≥ïÂú®Êüê‰∫õÂÆû‰æã‰∏≠‰ªçÈúÄÊåáÊï∞Êó∂Èó¥ÊâçËÉΩËææÂà∞Â±ÄÈÉ®ÊúÄ‰ºòËß£„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáËÅöÁÑ¶‰∫éÂØπÊØîÂ≠¶‰π†‰∏≠Â±ÄÈÉ®ÊúÄ‰ºòËß£ÁöÑÂØªÊâæÂ§çÊùÇÊÄßÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Ê≠§ÊñπÈù¢ÁöÑÂ§çÊùÇÊÄßÂ∞öÊú™ÊòéÁ°ÆÔºåÂ∞§ÂÖ∂ÊòØÂú®Á¶ªÊï£ÂíåËøûÁª≠‰ºòÂåñÈóÆÈ¢ò‰∏≠„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÊûÑÂª∫ÁâπÂÆöÁöÑ‰ºòÂåñÈóÆÈ¢òÔºåËÆ∫ÊñáËØÅÊòé‰∫ÜÂú®ÂØπÊØîÂ≠¶‰π†‰∏≠ÂØªÊâæÂ±ÄÈÉ®ÊúÄ‰ºòËß£ÁöÑ$	extsf{PLS}$Âíå$	extsf{CLS}$-Âõ∞ÈöæÊÄßÔºå‰ªéËÄåÊè≠Á§∫‰∫ÜÂ±ÄÈÉ®ÊêúÁ¥¢ÁÆóÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂È¶ñÂÖàÂÆö‰πâ‰∫ÜÂØπÊØîÂ≠¶‰π†‰∏≠ÁöÑÂ±ÄÈÉ®ÊúÄ‰ºòËß£ÈóÆÈ¢òÔºåÁÑ∂ÂêéÈÄöËøáÊûÑÈÄ†ÂÆû‰æãÂíåÂ§çÊùÇÊÄßÂΩíÁ∫¶ÔºåÂ±ïÁ§∫‰∫ÜÂú®‰∏çÂêåËÆæÁΩÆ‰∏ãÁöÑÂõ∞ÈöæÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÊòéÁ°Æ‰∫ÜÂØπÊØîÂ≠¶‰π†‰∏≠Â±ÄÈÉ®ÊúÄ‰ºòËß£ÁöÑÂ§çÊùÇÊÄßÔºåÊèê‰æõ‰∫ÜÁêÜËÆ∫Âü∫Á°ÄÔºåË°®ÊòéÂú®Â§öÈ°πÂºèÊó∂Èó¥ÂÜÖÊó†Ê≥ïÊâæÂà∞Â±ÄÈÉ®ÊúÄ‰ºòËß£ÁöÑÊù°‰ª∂„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Á†îÁ©∂‰∏≠Ôºå‰ΩúËÄÖ‰ΩøÁî®‰∫ÜÂä†ÊùÉ‰∏âÂÖÉÁªÑÁöÑÂΩ¢ÂºèÊù•ÂÆö‰πâÂØπÊØîÂ≠¶‰π†ÁõÆÊ†áÔºåÂπ∂ÈÄöËøáÊûÑÈÄ†ÁâπÂÆöÁöÑÂÆû‰æãÊù•ËØÅÊòéÂ§çÊùÇÊÄßÔºåÊ∂âÂèäÁöÑÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨Triplet LossÁ≠â„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Á†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂØªÊâæÂ±ÄÈÉ®ÊúÄ‰ºòËß£ÁöÑÂ§çÊùÇÊÄßÂú®Á¶ªÊï£ÂíåËøûÁª≠ËÆæÁΩÆ‰∏≠Âùá‰∏∫$	extsf{PLS}$-Âõ∞ÈöæÂíå$	extsf{CLS}$-Âõ∞ÈöæÔºåÊÑèÂë≥ÁùÄÂú®Â§öÈ°πÂºèÊó∂Èó¥ÂÜÖÊó†Ê≥ïÊâæÂà∞Â±ÄÈÉ®ÊúÄ‰ºòËß£„ÄÇÂç≥‰ΩøÂú®Â§çÊùÇÊÄßÁ±ªÂÖ≥Á≥ªÊàêÁ´ãÁöÑÊÉÖÂÜµ‰∏ãÔºåÊüê‰∫õÂÆû‰æã‰ªçÈúÄÊåáÊï∞Êó∂Èó¥ÊâçËÉΩËææÂà∞Â±ÄÈÉ®ÊúÄ‰ºòËß£„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂‰∏∫ÂØπÊØîÂ≠¶‰π†ÁöÑÁêÜËÆ∫Âü∫Á°ÄÊèê‰æõ‰∫ÜÈáçË¶ÅË¥°ÁåÆÔºåÁâπÂà´ÊòØÂú®‰ºòÂåñÁÆóÊ≥ïËÆæËÆ°ÂíåÂ§çÊùÇÊÄßÂàÜÊûêÊñπÈù¢„ÄÇÊú™Êù•ÔºåÁêÜËß£Â±ÄÈÉ®ÊúÄ‰ºòËß£ÁöÑÂ§çÊùÇÊÄßÂ∞ÜÊúâÂä©‰∫éÂºÄÂèëÊõ¥ÊúâÊïàÁöÑÂ≠¶‰π†ÁÆóÊ≥ïÔºåÊé®Âä®ËÆ°ÁÆóÊú∫ËßÜËßâÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Contrastive learning is a powerful technique for discovering meaningful data representations by optimizing objectives based on $\textit{contrastive information}$, often given as a set of weighted triplets $\{(x_i, y_i^+, z_{i}^-)\}_{i = 1}^m$ indicating that an "anchor" $x_i$ is more similar to a "positive" example $y_i$ than to a "negative" example $z_i$. The goal is to find representations (e.g., embeddings in $\mathbb{R}^d$ or a tree metric) where anchors are placed closer to positive than to negative examples. While finding $\textit{global}$ optima of contrastive objectives is $\mathsf{NP}$-hard, the complexity of finding $\textit{local}$ optima -- representations that do not improve by local search algorithms such as gradient-based methods -- remains open. Our work settles the complexity of finding local optima in various contrastive learning problems by proving $\mathsf{PLS}$-hardness in discrete settings (e.g., maximize satisfied triplets) and $\mathsf{CLS}$-hardness in continuous settings (e.g., minimize Triplet Loss), where $\mathsf{PLS}$ (Polynomial Local Search) and $\mathsf{CLS}$ (Continuous Local Search) are well-studied complexity classes capturing local search dynamics in discrete and continuous optimization, respectively. Our results imply that no polynomial time algorithm (local search or otherwise) can find a local optimum for various contrastive learning problems, unless $\mathsf{PLS}\subseteq\mathsf{P}$ (or $\mathsf{CLS}\subseteq \mathsf{P}$ for continuous problems). Even in the unlikely scenario that $\mathsf{PLS}\subseteq\mathsf{P}$ (or $\mathsf{CLS}\subseteq \mathsf{P}$), our reductions imply that there exist instances where local search algorithms need exponential time to reach a local optimum, even for $d=1$ (embeddings on a line).

