---
layout: default
title: Non-Asymptotic Global Convergence of PPO-Clip
---

# Non-Asymptotic Global Convergence of PPO-Clip

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16565" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16565v1</a>
  <a href="https://arxiv.org/pdf/2512.16565.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16565v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16565v1', 'Non-Asymptotic Global Convergence of PPO-Clip')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yin Liu, Qiming Dai, Junyu Zhang, Zaiwen Wen

**åˆ†ç±»**: math.OC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPPO-Clipç®—æ³•çš„éæ¸è¿‘å…¨å±€æ”¶æ•›æ€§åˆ†æ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `PPOç®—æ³•` `KLæ•£åº¦` `æ”¶æ•›æ€§åˆ†æ` `ç†è®ºç ”ç©¶` `è¯­è¨€æ¨¡å‹` `äººç±»åé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„PPOç®—æ³•åœ¨ç†è®ºç†è§£ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ”¶æ•›æ€§å’Œç¨³å®šæ€§æ–¹é¢çš„åˆ†æè¾ƒä¸ºè–„å¼±ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç†è®ºæ¡†æ¶ï¼Œé€šè¿‡å¯¹PPO-Clipç®—æ³•è¿›è¡Œåˆ†æï¼Œå»ºç«‹äº†éæ¸è¿‘çº¿æ€§æ”¶æ•›æ€§ï¼Œå¢å¼ºäº†ç®—æ³•çš„ç†è®ºåŸºç¡€ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å‰å‘KLæ­£åˆ™åŒ–æ—¶ï¼ŒPPO-Clipç®—æ³•èƒ½å¤Ÿå®ç°å…¨å±€æœ€ä¼˜ç­–ç•¥çš„éæ¸è¿‘æ”¶æ•›ï¼Œä¸”åœ¨åå‘KLæ­£åˆ™åŒ–ä¸‹ä¹Ÿèƒ½è¾¾åˆ°å¹³ç¨³æ”¶æ•›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å› å…¶åœ¨é€šè¿‡äººç±»åé¦ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¯¹é½æ–¹é¢çš„åº”ç”¨è€Œå—åˆ°å…³æ³¨ã€‚PPOçš„ä»…æ¼”å‘˜å˜ä½“å› å…¶é«˜æ•ˆæ€§è€Œè¢«å¹¿æ³›åº”ç”¨ï¼Œè¿™äº›ç®—æ³•é€šè¿‡å¼•å…¥å‰ªåˆ‡æœºåˆ¶æ¥æé«˜ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¼•å…¥äº†æ­£åˆ™åŒ–é¡¹ï¼Œå¦‚åKLæ•£åº¦æˆ–æ›´ä¸€èˆ¬çš„fæ•£åº¦ï¼Œä»¥é˜²æ­¢ç­–ç•¥æ¼‚ç§»ã€‚å°½ç®¡è¿™äº›æ–¹æ³•åœ¨å®è·µä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å¯¹å…¶ç†è®ºåŸºç¡€çš„ç†è§£ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡é€šè¿‡åˆ†æå¸¦æœ‰fæ•£åº¦æ­£åˆ™åŒ–çš„ç¡®å®šæ€§ä»…æ¼”å‘˜PPOç®—æ³•ï¼Œæ¨è¿›äº†PPO-Clipç®—æ³•çš„ç†è®ºåŸºç¡€ï¼Œå»ºç«‹äº†é’ˆå¯¹å‰å‘KLæ­£åˆ™åŒ–çš„éæ¸è¿‘çº¿æ€§æ”¶æ•›ç‡ï¼Œå¹¶æ¨å¯¼äº†åå‘KLæ­£åˆ™åŒ–çš„å¹³ç¨³æ”¶æ•›å’Œå±€éƒ¨çº¿æ€§æ”¶æ•›æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³PPO-Clipç®—æ³•åœ¨ç†è®ºæ”¶æ•›æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¦‚ä½•æœ‰æ•ˆé˜²æ­¢ç­–ç•¥æ¼‚ç§»çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ç®—æ³•æ€§è´¨çš„ä¸¥æ ¼ç†è®ºåˆ†æï¼Œå¯¼è‡´å…¶åº”ç”¨æ•ˆæœä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡å¼•å…¥fæ•£åº¦æ­£åˆ™åŒ–ï¼Œåˆ†æäº†ç¡®å®šæ€§ä»…æ¼”å‘˜PPOç®—æ³•çš„æ€§è´¨ï¼Œå»ºç«‹äº†éå‡åŒ€Lipschitzå…‰æ»‘æ€§æ¡ä»¶å’ŒÅojasiewiczä¸ç­‰å¼ï¼Œä»è€Œä¸ºæ”¶æ•›æ€§æä¾›äº†ç†è®ºæ”¯æŒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹PPO-Clipç®—æ³•çš„ç†è®ºåˆ†æï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ­£åˆ™åŒ–é¡¹çš„å¼•å…¥ã€å…‰æ»‘æ€§æ¡ä»¶çš„æ¨å¯¼ä»¥åŠæ”¶æ•›æ€§ç»“æœçš„è¯æ˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†é’ˆå¯¹å‰å‘å’Œåå‘KLæ­£åˆ™åŒ–çš„éæ¸è¿‘çº¿æ€§æ”¶æ•›æ€§åˆ†æï¼Œå¡«è¡¥äº†ç°æœ‰ç†è®ºç ”ç©¶çš„ç©ºç™½ï¼Œæä¾›äº†æ›´ä¸ºä¸¥è°¨çš„æ”¶æ•›æ€§ä¿è¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹ï¼Œç‰¹åˆ«æ˜¯fæ•£åº¦çš„é€‰æ‹©ï¼Œä»¥åŠåœ¨è½¯æœ€å¤§ç­–ç•¥å‚æ•°åŒ–ä¸‹çš„ç®—æ³•è®¾è®¡ï¼Œç¡®ä¿äº†ç®—æ³•çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPPO-Clipç®—æ³•åœ¨ä½¿ç”¨å‰å‘KLæ­£åˆ™åŒ–æ—¶ï¼Œèƒ½å¤Ÿå®ç°éæ¸è¿‘çº¿æ€§æ”¶æ•›è‡³å…¨å±€æœ€ä¼˜ç­–ç•¥ã€‚æ­¤å¤–ï¼Œåœ¨åå‘KLæ­£åˆ™åŒ–ä¸‹ï¼Œç®—æ³•ä¹Ÿå±•ç¤ºäº†è‰¯å¥½çš„å¹³ç¨³æ”¶æ•›æ€§ï¼Œæ˜¾è‘—æå‡äº†æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨äººæ§åˆ¶å’Œå…¶ä»–éœ€è¦é€šè¿‡äººç±»åé¦ˆè¿›è¡Œå­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡æä¾›æ›´ä¸ºç¨³å¥çš„æ”¶æ•›æ€§ç†è®ºï¼ŒPPO-Clipç®—æ³•èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ›´æœ‰æ•ˆåœ°å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå‡å…¶æ€§èƒ½å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \(f\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \(f\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a Åojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.

