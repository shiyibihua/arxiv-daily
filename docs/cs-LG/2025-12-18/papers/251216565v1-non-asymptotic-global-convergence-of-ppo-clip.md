---
layout: default
title: Non-Asymptotic Global Convergence of PPO-Clip
---

# Non-Asymptotic Global Convergence of PPO-Clip

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16565" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16565v1</a>
  <a href="https://arxiv.org/pdf/2512.16565.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16565v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16565v1', 'Non-Asymptotic Global Convergence of PPO-Clip')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yin Liu, Qiming Dai, Junyu Zhang, Zaiwen Wen

**åˆ†ç±»**: math.OC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPPO-Clipç®—æ³•çš„éæ¸è¿‘å…¨å±€æ”¶æ•›æ€§åˆ†æ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `PPOç®—æ³•` `æ”¶æ•›æ€§åˆ†æ` `æ­£åˆ™åŒ–` `ç†è®ºç ”ç©¶` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç®—æ³•ç¨³å®šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„PPOç®—æ³•åœ¨ç†è®ºç†è§£ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ”¶æ•›æ€§å’Œç¨³å®šæ€§æ–¹é¢çš„åˆ†æè¾ƒä¸ºè–„å¼±ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç†è®ºæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥f-æ•£åº¦æ­£åˆ™åŒ–ï¼Œåˆ†æäº†PPO-Clipç®—æ³•çš„å…¨å±€æ”¶æ•›æ€§ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å‰å‘KLæ­£åˆ™åŒ–å™¨æ—¶ï¼ŒPPO-Clipç®—æ³•èƒ½å¤Ÿå®ç°éæ¸è¿‘çº¿æ€§æ”¶æ•›ï¼Œæ˜¾è‘—æå‡äº†æ”¶æ•›é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å› å…¶åœ¨é€šè¿‡äººç±»åé¦ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¯¹é½çš„èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚PPOçš„ä»…æ¼”å‘˜å˜ä½“å› å…¶é«˜æ•ˆæ€§è€Œè¢«å¹¿æ³›åº”ç”¨ï¼Œè¿™äº›ç®—æ³•é€šè¿‡å‰ªåˆ‡æœºåˆ¶æé«˜ç¨³å®šæ€§ï¼Œå¹¶å¼•å…¥æ­£åˆ™åŒ–é¡¹ä»¥é˜²æ­¢ç­–ç•¥æ¼‚ç§»ã€‚å°½ç®¡åœ¨ç»éªŒä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†å¯¹è¯¥é—®é¢˜åŠç®—æ³•ç‰¹æ€§çš„ä¸¥æ ¼ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡é€šè¿‡åˆ†æå¸¦æœ‰è½¯æœ€å¤§ç­–ç•¥å‚æ•°åŒ–çš„ç¡®å®šæ€§ä»…æ¼”å‘˜PPOç®—æ³•ï¼Œæ¨è¿›äº†PPO-Clipç®—æ³•çš„ç†è®ºåŸºç¡€ï¼Œå¯¼å‡ºäº†éå‡åŒ€Lipschitzå…‰æ»‘æ€§æ¡ä»¶å’ŒÅojasiewiczä¸ç­‰å¼ï¼Œå¹¶å»ºç«‹äº†å‰å‘KLæ­£åˆ™åŒ–å™¨çš„éæ¸è¿‘çº¿æ€§æ”¶æ•›ç‡ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å¯¼äº†åå‘KLæ­£åˆ™åŒ–å™¨çš„å¹³ç¨³æ”¶æ•›å’Œå±€éƒ¨çº¿æ€§æ”¶æ•›æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³PPO-Clipç®—æ³•åœ¨ç†è®ºä¸Šçš„æ”¶æ•›æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ç®—æ³•æ€§è´¨çš„ä¸¥æ ¼åˆ†æï¼Œå°¤å…¶æ˜¯åœ¨ç¨³å®šæ€§å’Œæ”¶æ•›æ€§æ–¹é¢çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡åˆ†æå¸¦æœ‰f-æ•£åº¦æ­£åˆ™åŒ–çš„ç¡®å®šæ€§ä»…æ¼”å‘˜PPOç®—æ³•ï¼Œå»ºç«‹äº†éå‡åŒ€Lipschitzå…‰æ»‘æ€§æ¡ä»¶å’ŒÅojasiewiczä¸ç­‰å¼ï¼Œä»è€Œæ¨è¿›äº†å¯¹PPO-Clipç®—æ³•çš„ç†è®ºç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹PPO-Clipç®—æ³•çš„ç†è®ºåˆ†æï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬ç®—æ³•çš„æ”¶æ•›æ€§åˆ†æã€å…‰æ»‘æ€§æ¡ä»¶çš„æ¨å¯¼ä»¥åŠæ­£åˆ™åŒ–å™¨çš„æ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¯¼å‡ºäº†PPO-Clipç®—æ³•çš„éæ¸è¿‘çº¿æ€§æ”¶æ•›ç‡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å‰å‘KLæ­£åˆ™åŒ–å™¨çš„åˆ†æï¼Œä¸ºç†è§£ç®—æ³•çš„å…¨å±€æ”¶æ•›æ€§æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­è®¾è®¡äº†ç‰¹å®šçš„æ­£åˆ™åŒ–é¡¹ï¼ˆå¦‚åå‘KLå’Œå‰å‘KLï¼‰ï¼Œå¹¶é€šè¿‡è½¯æœ€å¤§ç­–ç•¥å‚æ•°åŒ–æ¥å®ç°ç®—æ³•çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ï¼Œç¡®ä¿äº†ç†è®ºæ¨å¯¼çš„ä¸¥è°¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPPO-Clipç®—æ³•åœ¨ä½¿ç”¨å‰å‘KLæ­£åˆ™åŒ–å™¨æ—¶ï¼Œèƒ½å¤Ÿå®ç°éæ¸è¿‘çº¿æ€§æ”¶æ•›ï¼Œæ”¶æ•›é€Ÿåº¦ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æå‡äº†æ˜¾è‘—çš„å¹…åº¦ï¼Œå…·ä½“æ€§èƒ½æ•°æ®åœ¨å®éªŒä¸­å¾—åˆ°äº†éªŒè¯ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜PPO-Clipç®—æ³•çš„æ”¶æ•›æ€§å’Œç¨³å®šæ€§ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ›´æœ‰æ•ˆåœ°è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå‡å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \(f\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \(f\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a Åojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.

