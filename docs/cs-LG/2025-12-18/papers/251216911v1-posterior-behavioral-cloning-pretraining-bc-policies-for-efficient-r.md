---
layout: default
title: Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning
---

# Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16911" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16911v1</a>
  <a href="https://arxiv.org/pdf/2512.16911.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16911v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16911v1', 'Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåéªŒè¡Œä¸ºå…‹éš†(PostBC)æ–¹æ³•ï¼Œæå‡RLå¾®è°ƒçš„é¢„è®­ç»ƒç­–ç•¥æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åéªŒè¡Œä¸ºå…‹éš†` `å¼ºåŒ–å­¦ä¹ å¾®è°ƒ` `è¡Œä¸ºå…‹éš†` `æœºå™¨äººæ§åˆ¶` `é¢„è®­ç»ƒç­–ç•¥` `ç”Ÿæˆæ¨¡å‹` `ç­–ç•¥åˆå§‹åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¡Œä¸ºå…‹éš†(BC)æ–¹æ³•åœ¨ä½œä¸ºå¼ºåŒ–å­¦ä¹ (RL)å¾®è°ƒçš„åˆå§‹åŒ–æ—¶ï¼Œæ— æ³•ä¿è¯è¦†ç›–æ¼”ç¤ºè€…çš„æ‰€æœ‰è¡Œä¸ºï¼Œé™åˆ¶äº†å¾®è°ƒæ•ˆæœã€‚
2. æå‡ºåéªŒè¡Œä¸ºå…‹éš†(PostBC)ï¼Œé€šè¿‡å»ºæ¨¡æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼Œç¡®ä¿ç­–ç•¥è¦†ç›–æ¼”ç¤ºè€…çš„è¡Œä¸ºï¼Œä»è€Œæ”¹å–„RLå¾®è°ƒçš„åˆå§‹åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPostBCåœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºæ ‡å‡†BCï¼Œæ˜¾è‘—æå‡äº†RLå¾®è°ƒçš„æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†é¢„è®­ç»ƒç­–ç•¥å¦‚ä½•å½±å“å¼ºåŒ–å­¦ä¹ (RL)å¾®è°ƒçš„æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•é¢„è®­ç»ƒç­–ç•¥ä»¥ç¡®ä¿å®ƒä»¬æ˜¯æœ‰æ•ˆçš„å¾®è°ƒåˆå§‹åŒ–ã€‚ç†è®ºä¸Šè¯æ˜ï¼Œæ ‡å‡†è¡Œä¸ºå…‹éš†(BC)æ— æ³•ç¡®ä¿è¦†ç›–æ¼”ç¤ºè€…çš„è¡Œä¸ºï¼Œè¿™æ˜¯æœ‰æ•ˆRLå¾®è°ƒçš„å¿…è¦æ¡ä»¶ã€‚å› æ­¤ï¼Œæå‡ºåéªŒè¡Œä¸ºå…‹éš†(PostBC)ç­–ç•¥ï¼Œè¯¥ç­–ç•¥è®­ç»ƒæ¨¡å‹ä»¥æ¨¡æ‹Ÿç»™å®šæ¼”ç¤ºæ•°æ®é›†çš„æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼Œä»è€Œç¡®ä¿è¦†ç›–æ¼”ç¤ºè€…çš„è¡Œä¸ºï¼Œå®ç°æ›´æœ‰æ•ˆçš„å¾®è°ƒï¼ŒåŒæ—¶ä¿è¯é¢„è®­ç»ƒæ€§èƒ½ä¸ä½äºBCç­–ç•¥ã€‚PostBCå¯ä»¥é€šè¿‡ç°ä»£ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸä¸­å®é™…å®ç°ï¼Œä»…ä¾èµ–äºæ ‡å‡†ç›‘ç£å­¦ä¹ ï¼Œå¹¶ä¸”ä¸æ ‡å‡†è¡Œä¸ºå…‹éš†ç›¸æ¯”ï¼Œåœ¨çœŸå®çš„æœºå™¨äººæ§åˆ¶åŸºå‡†å’ŒçœŸå®æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šï¼Œæ˜¾è‘—æé«˜äº†RLå¾®è°ƒçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ ‡å‡†è¡Œä¸ºå…‹éš†(BC)ä½œä¸ºå¼ºåŒ–å­¦ä¹ (RL)å¾®è°ƒçš„é¢„è®­ç»ƒç­–ç•¥æ—¶ï¼Œæ— æ³•æœ‰æ•ˆè¦†ç›–æ¼”ç¤ºè€…è¡Œä¸ºçš„é—®é¢˜ã€‚ç°æœ‰BCæ–¹æ³•ç›´æ¥æ¨¡ä»¿æ¼”ç¤ºæ•°æ®ä¸­çš„åŠ¨ä½œï¼Œå¯èƒ½å¯¼è‡´ç­–ç•¥é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œç¼ºä¹æ¢ç´¢èƒ½åŠ›ï¼Œä»è€Œé™åˆ¶äº†åç»­RLå¾®è°ƒçš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œä¸å…¶ç²¾ç¡®æ‹Ÿåˆè§‚å¯Ÿåˆ°çš„æ¼”ç¤ºæ•°æ®ï¼Œä¸å¦‚è®­ç»ƒä¸€ä¸ªç­–ç•¥æ¥æ¨¡æ‹Ÿç»™å®šæ¼”ç¤ºæ•°æ®é›†ä¸‹æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ¼”ç¤ºè€…è¡Œä¸ºçš„å¤šæ ·æ€§ï¼Œå¹¶ç¡®ä¿ç­–ç•¥èƒ½å¤Ÿè¦†ç›–æ¼”ç¤ºè€…çš„æ‰€æœ‰å¯èƒ½è¡Œä¸ºï¼Œä»è€Œä¸ºåç»­çš„RLå¾®è°ƒæä¾›æ›´å¥½çš„åˆå§‹åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPostBCçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ”¶é›†æ¼”ç¤ºæ•°æ®é›†ï¼›2) ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆä¾‹å¦‚å˜åˆ†è‡ªç¼–ç å™¨VAEæˆ–å½’ä¸€åŒ–æµï¼‰å¯¹æ¼”ç¤ºæ•°æ®è¿›è¡Œå»ºæ¨¡ï¼Œå­¦ä¹ æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼›3) ä»å­¦ä¹ åˆ°çš„åéªŒåˆ†å¸ƒä¸­é‡‡æ ·ï¼Œè®­ç»ƒPostBCç­–ç•¥ï¼›4) ä½¿ç”¨RLç®—æ³•å¯¹PostBCç­–ç•¥è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šPostBCçš„å…³é”®åˆ›æ–°åœ¨äºï¼Œå®ƒä¸å†æ˜¯ç®€å•åœ°æ¨¡ä»¿æ¼”ç¤ºæ•°æ®ï¼Œè€Œæ˜¯å­¦ä¹ æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ¼”ç¤ºè€…è¡Œä¸ºçš„ä¸ç¡®å®šæ€§å’Œå¤šæ ·æ€§ï¼Œä»è€Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›å’Œæ¢ç´¢èƒ½åŠ›ã€‚ä¸ç°æœ‰BCæ–¹æ³•ç›¸æ¯”ï¼ŒPostBCèƒ½å¤Ÿæ›´å¥½åœ°è¦†ç›–æ¼”ç¤ºè€…çš„è¡Œä¸ºç©ºé—´ï¼Œä¸ºåç»­çš„RLå¾®è°ƒæä¾›æ›´æœ‰æ•ˆçš„åˆå§‹åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šPostBCçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨åˆé€‚çš„ç”Ÿæˆæ¨¡å‹æ¥å»ºæ¨¡æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼Œä¾‹å¦‚VAEæˆ–å½’ä¸€åŒ–æµï¼›2) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚å˜åˆ†ä¸‹ç•Œ(ELBO)æˆ–æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼›3) ä»å­¦ä¹ åˆ°çš„åéªŒåˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œç”¨äºè®­ç»ƒPostBCç­–ç•¥ï¼›4) é€‰æ‹©åˆé€‚çš„RLç®—æ³•å¯¹PostBCç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä¾‹å¦‚PPOæˆ–SACã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16911v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16911v1/im/corn_in_pot2.jpg" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16911v1/x2.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPostBCåœ¨å¤šä¸ªæœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†BCã€‚ä¾‹å¦‚ï¼Œåœ¨çœŸå®æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒPostBCèƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¹¶è¾¾åˆ°æ›´é«˜çš„æ€§èƒ½ã€‚ä¸BCç›¸æ¯”ï¼ŒPostBCåœ¨RLå¾®è°ƒåçš„æ€§èƒ½æå‡å¹…åº¦æ˜æ˜¾ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºRLå¾®è°ƒæœ‰æ•ˆåˆå§‹åŒ–çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PostBCæ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚é€šè¿‡é¢„è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿè¦†ç›–ä¸“å®¶è¡Œä¸ºçš„ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æé«˜åç»­RLå¾®è°ƒçš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œé™ä½å¯¹å¤§é‡ç¯å¢ƒäº¤äº’çš„éœ€æ±‚ï¼ŒåŠ é€Ÿæ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºé‚£äº›éš¾ä»¥è·å–å¤§é‡å¥–åŠ±ä¿¡å·æˆ–ç¯å¢ƒäº¤äº’æˆæœ¬è¾ƒé«˜çš„åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

