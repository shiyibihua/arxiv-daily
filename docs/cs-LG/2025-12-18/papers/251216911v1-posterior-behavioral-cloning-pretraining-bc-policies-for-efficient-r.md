---
layout: default
title: Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning
---

# Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16911" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16911v1</a>
  <a href="https://arxiv.org/pdf/2512.16911.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16911v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16911v1', 'Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåéªŒè¡Œä¸ºå…‹éš†(PostBC)æ–¹æ³•ï¼Œæå‡RLå¾®è°ƒçš„é¢„è®­ç»ƒç­–ç•¥æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åéªŒè¡Œä¸ºå…‹éš†` `å¼ºåŒ–å­¦ä¹ å¾®è°ƒ` `é¢„è®­ç»ƒç­–ç•¥` `æœºå™¨äººæ§åˆ¶` `è¡Œä¸ºå…‹éš†` `ç”Ÿæˆæ¨¡å‹` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¡Œä¸ºå…‹éš†(BC)æ–¹æ³•åœ¨é¢„è®­ç»ƒç­–ç•¥æ—¶ï¼Œæ— æ³•ä¿è¯è¦†ç›–æ¼”ç¤ºè€…çš„æ‰€æœ‰è¡Œä¸ºï¼Œå¯¼è‡´å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºåéªŒè¡Œä¸ºå…‹éš†(PostBC)æ–¹æ³•ï¼Œé€šè¿‡å»ºæ¨¡æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼Œç¡®ä¿é¢„è®­ç»ƒç­–ç•¥è¦†ç›–æ¼”ç¤ºè€…çš„è¡Œä¸ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPostBCåœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æå‡äº†å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æ€§èƒ½ï¼Œä¼˜äºæ ‡å‡†è¡Œä¸ºå…‹éš†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†é¢„è®­ç»ƒç­–ç•¥å¦‚ä½•å½±å“å¼ºåŒ–å­¦ä¹ (RL)å¾®è°ƒçš„æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•é¢„è®­ç»ƒç­–ç•¥ä»¥ç¡®ä¿å®ƒä»¬æ˜¯æœ‰æ•ˆçš„å¾®è°ƒåˆå§‹åŒ–ã€‚ç†è®ºä¸Šè¯æ˜ï¼Œæ ‡å‡†è¡Œä¸ºå…‹éš†(BC)æ— æ³•ç¡®ä¿è¦†ç›–æ¼”ç¤ºè€…çš„è¡Œä¸ºï¼Œè¿™æ˜¯æœ‰æ•ˆRLå¾®è°ƒçš„å¿…è¦æ¡ä»¶ã€‚å› æ­¤ï¼Œæå‡ºåéªŒè¡Œä¸ºå…‹éš†(PostBC)ç­–ç•¥ï¼Œè¯¥ç­–ç•¥è®­ç»ƒæ¨¡å‹æ¥æ¨¡æ‹Ÿç»™å®šæ¼”ç¤ºæ•°æ®é›†çš„æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼Œä»è€Œç¡®ä¿è¦†ç›–æ¼”ç¤ºè€…çš„è¡Œä¸ºï¼Œå¹¶å®ç°æ›´æœ‰æ•ˆçš„å¾®è°ƒã€‚PostBCåœ¨ä¿è¯é¢„è®­ç»ƒæ€§èƒ½ä¸ä½äºBCç­–ç•¥çš„åŒæ—¶ï¼Œé€šè¿‡æ ‡å‡†ç›‘ç£å­¦ä¹ å³å¯åœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸå®é™…åº”ç”¨ï¼Œå¹¶ä¸”ä¸æ ‡å‡†è¡Œä¸ºå…‹éš†ç›¸æ¯”ï¼Œåœ¨çœŸå®çš„æœºå™¨äººæ§åˆ¶åŸºå‡†å’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šï¼Œæ˜¾è‘—æé«˜äº†RLå¾®è°ƒçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ ‡å‡†è¡Œä¸ºå…‹éš†(BC)ï¼Œåœ¨é¢„è®­ç»ƒç­–ç•¥æ—¶ï¼Œç›®æ ‡æ˜¯ç›´æ¥æ¨¡ä»¿æ¼”ç¤ºè€…çš„åŠ¨ä½œã€‚è¿™ç§æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œå®ƒå¯èƒ½æ— æ³•å……åˆ†è¦†ç›–æ¼”ç¤ºè€…è¡Œä¸ºçš„æ•´ä¸ªåˆ†å¸ƒï¼Œå¯¼è‡´é¢„è®­ç»ƒç­–ç•¥æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œä»è€Œé™åˆ¶äº†åç»­å¼ºåŒ–å­¦ä¹ (RL)å¾®è°ƒçš„æ€§èƒ½ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœé¢„è®­ç»ƒç­–ç•¥æ— æ³•æ¢ç´¢åˆ°æ¼”ç¤ºè€…å¯èƒ½é‡‡å–çš„æ‰€æœ‰åŠ¨ä½œï¼Œé‚£ä¹ˆRLå¾®è°ƒå°±éš¾ä»¥æ‰¾åˆ°æ›´ä¼˜çš„ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯ï¼Œä¸å†ä»…ä»…æ¨¡ä»¿æ¼”ç¤ºè€…çš„åŠ¨ä½œï¼Œè€Œæ˜¯å­¦ä¹ æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒã€‚è¿™æ„å‘³ç€ï¼Œç»™å®šæ¼”ç¤ºæ•°æ®é›†ï¼Œæ¨¡å‹éœ€è¦å­¦ä¹ ç”Ÿæˆä¸æ¼”ç¤ºæ•°æ®ç›¸ä¼¼çš„è¡Œä¸ºï¼Œè€Œä¸æ˜¯ç®€å•åœ°å¤åˆ¶ã€‚é€šè¿‡å»ºæ¨¡åéªŒåˆ†å¸ƒï¼Œå¯ä»¥ç¡®ä¿é¢„è®­ç»ƒç­–ç•¥èƒ½å¤Ÿè¦†ç›–æ¼”ç¤ºè€…è¡Œä¸ºçš„æ›´å¹¿æ³›èŒƒå›´ï¼Œä»è€Œä¸ºRLå¾®è°ƒæä¾›æ›´å¥½çš„åˆå§‹åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPostBCçš„æ•´ä½“æ¡†æ¶ä»ç„¶åŸºäºç›‘ç£å­¦ä¹ ï¼Œä½†è®­ç»ƒç›®æ ‡ä¸åŒäºä¼ ç»Ÿçš„BCã€‚ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) æ”¶é›†æ¼”ç¤ºæ•°æ®é›†ï¼›2) ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚å˜åˆ†è‡ªç¼–ç å™¨VAEæˆ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œGANï¼‰æ¥å»ºæ¨¡æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼›3) ä½¿ç”¨å­¦ä¹ åˆ°çš„åéªŒåˆ†å¸ƒæ¥ç”Ÿæˆé¢„è®­ç»ƒç­–ç•¥ã€‚åœ¨RLå¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„PostBCç­–ç•¥ä½œä¸ºRLç®—æ³•çš„åˆå§‹åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºï¼Œå°†é¢„è®­ç»ƒç­–ç•¥çš„å­¦ä¹ ç›®æ ‡ä»æ¨¡ä»¿å•ä¸ªåŠ¨ä½œè½¬å˜ä¸ºå»ºæ¨¡æ•´ä¸ªè¡Œä¸ºåˆ†å¸ƒçš„åéªŒæ¦‚ç‡ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒPostBCä¸å†è¿½æ±‚ç²¾ç¡®åŒ¹é…æ¼”ç¤ºæ•°æ®ï¼Œè€Œæ˜¯å­¦ä¹ ç”Ÿæˆä¸æ¼”ç¤ºæ•°æ®ç›¸ä¼¼çš„è¡Œä¸ºï¼Œä»è€Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›å’Œæ¢ç´¢èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šPostBCçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„ç”Ÿæˆæ¨¡å‹æ¥å»ºæ¨¡åéªŒåˆ†å¸ƒï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨(VAE)ï¼Œå…¶ä¸­ç¼–ç å™¨ç”¨äºæ¨æ–­æ½œåœ¨å˜é‡ï¼Œè§£ç å™¨ç”¨äºç”ŸæˆåŠ¨ä½œï¼›2) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨VAEçš„é‡æ„æŸå¤±å’ŒKLæ•£åº¦æŸå¤±ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„åŠ¨ä½œä¸æ¼”ç¤ºæ•°æ®ç›¸ä¼¼ï¼Œå¹¶ä¸”æ½œåœ¨å˜é‡çš„åˆ†å¸ƒæ¥è¿‘å…ˆéªŒåˆ†å¸ƒï¼›3) åœ¨RLå¾®è°ƒé˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨å„ç§RLç®—æ³•ï¼Œä¾‹å¦‚PPOæˆ–SACï¼Œå¹¶è°ƒæ•´å­¦ä¹ ç‡å’Œæ¢ç´¢ç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒç­–ç•¥çš„ä¼˜åŠ¿ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16911v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16911v1/im/corn_in_pot2.jpg" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16911v1/x2.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPostBCåœ¨å¤šä¸ªæœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†è¡Œä¸ºå…‹éš†ã€‚ä¾‹å¦‚ï¼Œåœ¨çœŸå®æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒPostBCç­–ç•¥ä½œä¸ºRLå¾®è°ƒçš„åˆå§‹åŒ–ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›åˆ°æ›´é«˜çš„æ€§èƒ½ï¼Œå¹¶ä¸”æœ€ç»ˆæ€§èƒ½ä¹Ÿä¼˜äºä½¿ç”¨æ ‡å‡†BCç­–ç•¥åˆå§‹åŒ–çš„RLç®—æ³•ã€‚å…·ä½“æå‡å¹…åº¦å–å†³äºä»»åŠ¡çš„å¤æ‚ç¨‹åº¦å’ŒRLç®—æ³•çš„é€‰æ‹©ï¼Œä½†æ€»ä½“è€Œè¨€ï¼ŒPostBCèƒ½å¤Ÿå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PostBCæ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚åœ¨æœºå™¨äººæ§åˆ¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨å¤§é‡äººç±»æ¼”ç¤ºæ•°æ®é¢„è®­ç»ƒæœºå™¨äººç­–ç•¥ï¼Œç„¶åé€šè¿‡RLå¾®è°ƒï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨é©¾é©¶å‘˜çš„é©¾é©¶æ•°æ®é¢„è®­ç»ƒè‡ªåŠ¨é©¾é©¶ç­–ç•¥ï¼Œæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚åœ¨æ¸¸æˆAIä¸­ï¼Œå¯ä»¥åˆ©ç”¨ç©å®¶çš„æ¸¸æˆæ•°æ®é¢„è®­ç»ƒæ¸¸æˆAIç­–ç•¥ï¼Œæé«˜æ¸¸æˆAIçš„æ™ºèƒ½æ°´å¹³å’ŒæŒ‘æˆ˜æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

