---
layout: default
title: Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward
---

# Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16912" class="toolbar-btn" target="_blank">📄 arXiv: 2512.16912v1</a>
  <a href="https://arxiv.org/pdf/2512.16912.pdf" class="toolbar-btn" target="_blank">📥 PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16912v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16912v1', 'Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward')" title="添加到收藏夹">☆ 收藏</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">🔗 分享</button>
</div>


**作者**: Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin

**分类**: cs.LG, cs.AI, cs.CL

**发布日期**: 2025-12-18

**备注**: 35 pages

---

## 💡 一句话要点

**通过裁剪、熵和虚假奖励重新思考RLVR，提升LLM推理能力**

🎯 **匹配领域**: **支柱二：RL算法与架构 (RL & Architecture)** **支柱九：具身大模型 (Embodied Foundation Models)**

**关键词**: `强化学习` `大型语言模型` `奖励工程` `探索利用` `策略熵` `裁剪偏差` `虚假奖励`

## 📋 核心要点

1. 现有RLVR方法在探索-利用平衡上存在不足，对LLM推理能力的提升机制理解不够深入。
2. 论文核心思想是通过分析裁剪偏差、熵最小化和虚假奖励的相互作用，揭示提升LLM推理性能的关键因素。
3. 实验结果表明，虚假奖励下的裁剪偏差能有效降低策略熵，产生更自信的输出，从而提升模型性能。

## 📝 摘要（中文）

本文研究了具有可验证奖励的强化学习（RLVR）中的探索-利用权衡，RLVR是一种用于改进大型语言模型（LLM）推理的框架。最近的研究表明，RLVR可以通过两种看似矛盾的机制激发LLM强大的数学推理能力：虚假奖励，通过奖励与真实结果无关的输出来抑制利用；熵最小化，通过将模型推向更自信和确定性的输出来抑制探索。这突显了一种令人困惑的动态：抑制利用和抑制探索都能提高推理性能，但协调这些影响的潜在原则仍然知之甚少。我们关注两个基本问题：（i）策略熵如何与性能相关，以及（ii）虚假奖励是否能产生收益，可能是通过裁剪偏差和模型污染的相互作用。我们的结果表明，虚假奖励下的裁剪偏差会降低策略熵，从而产生更自信和确定性的输出，而仅靠熵最小化不足以改进。我们进一步提出了一个奖励错位模型，解释了为什么虚假奖励可以提高超出污染环境的性能。我们的发现阐明了虚假奖励背后机制，并为更有效的RLVR训练提供了原则。

## 🔬 方法详解

**问题定义**：论文旨在解决RLVR框架下，如何更好地平衡探索与利用，从而提升大型语言模型（LLM）的推理能力。现有方法在利用虚假奖励和熵最小化时，其内在机制尚不明确，导致训练效率和效果存在瓶颈。现有方法缺乏对裁剪偏差、模型污染等因素的深入分析，难以解释虚假奖励为何能提升性能。

**核心思路**：论文的核心思路是通过深入分析策略熵、裁剪偏差和虚假奖励之间的关系，揭示虚假奖励提升性能的内在机制。具体而言，论文认为虚假奖励通过裁剪偏差降低策略熵，使得模型输出更加自信和确定，从而提高推理准确性。同时，论文提出了奖励错位模型，解释了虚假奖励在非污染环境下的有效性。

**技术框架**：论文主要采用理论分析和实验验证相结合的方法。首先，论文构建了奖励错位模型，用于解释虚假奖励的有效性。然后，论文通过实验验证了裁剪偏差对策略熵的影响，以及虚假奖励对模型性能的提升。实验主要基于RLVR框架，并针对数学推理任务进行评估。

**关键创新**：论文的关键创新在于揭示了虚假奖励通过裁剪偏差降低策略熵，从而提升LLM推理性能的机制。此外，论文提出的奖励错位模型为理解虚假奖励的有效性提供了新的视角。与现有方法相比，论文更加关注虚假奖励背后的内在机制，并提出了相应的解释模型。

**关键设计**：论文的关键设计包括：(1) 使用裁剪操作限制奖励范围，从而引入裁剪偏差；(2) 通过策略熵衡量模型输出的确定性；(3) 设计奖励错位模型，模拟真实任务与虚假奖励之间的关系。论文还详细描述了实验设置，包括数据集、模型架构、训练参数等。

## 🖼️ 关键图片

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16912v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16912v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16912v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## 📊 实验亮点

实验结果表明，在虚假奖励下，裁剪偏差能够有效降低策略熵，使得模型输出更加自信和确定。与没有虚假奖励的基线相比，使用虚假奖励的模型在数学推理任务上取得了显著的性能提升。论文还验证了奖励错位模型的有效性，表明虚假奖励在非污染环境下也能提升模型性能。

## 🎯 应用场景

该研究成果可应用于提升大型语言模型在数学推理、代码生成等领域的性能。通过更有效地利用虚假奖励和熵最小化，可以训练出更准确、更可靠的LLM，从而在智能客服、自动编程等实际应用中发挥更大的作用。未来的研究可以进一步探索更有效的奖励设计方法，以及如何将该方法推广到其他类型的任务中。

## 📄 摘要（原文）

> This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.

