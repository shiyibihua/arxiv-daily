---
layout: default
title: Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game
---

# Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16626" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16626v1</a>
  <a href="https://arxiv.org/pdf/2512.16626.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16626v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16626v1', 'Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Barna PÃ¡sztor, Thomas Kleine Buening, Andreas Krause

**åˆ†ç±»**: cs.LG, cs.AI, cs.GT, cs.MA, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: 10 pages, 5 tables, 1 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStackelberg Learning from Human Feedback (SLHF)æ¡†æ¶ï¼Œç”¨äºåå¥½ä¼˜åŒ–ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºåé¦ˆ` `åå¥½ä¼˜åŒ–` `åºè´¯åšå¼ˆ` `Stackelbergå­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLHFæ–¹æ³•ä¸ºåŠ¨ä½œåˆ†é…æ ‡é‡å¥–åŠ±ï¼ŒNLHFå¯»æ±‚åŒæ­¥åšå¼ˆå‡è¡¡ï¼Œæ— æ³•æ•æ‰å¤æ‚åå¥½ç»“æ„ã€‚
2. SLHFå°†å¯¹é½é—®é¢˜å»ºæ¨¡ä¸ºé¢†å¯¼è€…å’Œè·Ÿéšè€…ä¹‹é—´çš„åºè´¯åšå¼ˆï¼Œåˆ©ç”¨åºè´¯åšå¼ˆçš„ä¸å¯¹ç§°æ€§æ•è·æ›´ä¸°å¯Œçš„åå¥½ç»“æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSLHFåœ¨ä¸åŒåå¥½æ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„å¯¹é½ï¼Œå¹¶èƒ½è¿›è¡Œè·¨æ¨¡å‹è¿ç§»çš„æ¨ç†æ—¶ä¼˜åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åå¥½ä¼˜åŒ–æ¡†æ¶ï¼šStackelberg Learning from Human Feedback (SLHF)ã€‚SLHFå°†å¯¹é½é—®é¢˜å»ºæ¨¡ä¸ºä¸¤ä¸ªç­–ç•¥ä¹‹é—´çš„åºè´¯åšå¼ˆï¼šé¢†å¯¼è€…(Leader)å…ˆé‡‡å–è¡ŒåŠ¨ï¼Œç„¶åè·Ÿéšè€…(Follower)æ ¹æ®é¢†å¯¼è€…çš„è¡ŒåŠ¨åšå‡ºå“åº”ã€‚è¿™ç§æ–¹æ³•å°†åå¥½ä¼˜åŒ–åˆ†è§£ä¸ºè·Ÿéšè€…çš„ä¼˜åŒ–é—®é¢˜å’Œé¢†å¯¼è€…å¯¹æŠ—æ€§ä¼˜åŒ–é—®é¢˜ã€‚ä¸ä¸ºåŠ¨ä½œåˆ†é…æ ‡é‡å¥–åŠ±çš„Reinforcement Learning from Human Feedback (RLHF)æˆ–å¯»æ±‚åŒæ­¥åšå¼ˆå‡è¡¡çš„Nash Learning from Human Feedback (NLHF)ä¸åŒï¼ŒSLHFåˆ©ç”¨åºè´¯åšå¼ˆçš„ä¸å¯¹ç§°æ€§æ¥æ•è·æ›´ä¸°å¯Œçš„åå¥½ç»“æ„ã€‚SLHFçš„åºè´¯è®¾è®¡è‡ªç„¶åœ°å®ç°äº†æ¨ç†æ—¶ä¼˜åŒ–ï¼Œå› ä¸ºè·Ÿéšè€…å­¦ä¹ æ”¹è¿›é¢†å¯¼è€…çš„è¡ŒåŠ¨ï¼Œå¹¶ä¸”è¿™äº›æ”¹è¿›å¯ä»¥é€šè¿‡è¿­ä»£é‡‡æ ·æ¥åˆ©ç”¨ã€‚æœ¬æ–‡æ¯”è¾ƒäº†SLHFã€RLHFå’ŒNLHFçš„è§£æ¦‚å¿µï¼Œå¹¶é˜è¿°äº†SLHFåœ¨ä¸€è‡´æ€§ã€æ•°æ®æ•æ„Ÿæ€§å’Œå¯¹éä¼ é€’åå¥½çš„é²æ£’æ€§æ–¹é¢çš„å…³é”®ä¼˜åŠ¿ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSLHFåœ¨ä¸åŒçš„åå¥½æ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„å¯¹é½ï¼Œå¯ä»¥ä»0.5Bæ‰©å±•åˆ°8Bå‚æ•°ï¼Œå¹¶äº§ç”Ÿå¯ä»¥åœ¨æ¨¡å‹ç³»åˆ—ä¹‹é—´è½¬ç§»è€Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒçš„æ¨ç†æ—¶ä¼˜åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æ›´å¥½åœ°ä»äººç±»åé¦ˆä¸­å­¦ä¹ ï¼Œä»¥å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä¸äººç±»åå¥½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚RLHFï¼Œé€šå¸¸å°†äººç±»åé¦ˆè½¬åŒ–ä¸ºæ ‡é‡å¥–åŠ±ï¼Œè¿™å¯èƒ½è¿‡äºç®€åŒ–ï¼Œæ— æ³•æ•æ‰äººç±»åå¥½çš„å¤æ‚æ€§å’Œç»†å¾®å·®åˆ«ã€‚æ­¤å¤–ï¼ŒNLHFæ–¹æ³•å‡è®¾ç­–ç•¥åŒæ—¶è¡ŒåŠ¨ï¼Œå¿½ç•¥äº†åºè´¯å†³ç­–çš„åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSLHFçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¯¹é½é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªStackelbergåšå¼ˆï¼Œå…¶ä¸­é¢†å¯¼è€…ï¼ˆLeaderï¼‰ç­–ç•¥é¦–å…ˆé‡‡å–è¡ŒåŠ¨ï¼Œç„¶åè·Ÿéšè€…ï¼ˆFollowerï¼‰ç­–ç•¥æ ¹æ®é¢†å¯¼è€…çš„è¡ŒåŠ¨åšå‡ºå“åº”ã€‚è¿™ç§åºè´¯åšå¼ˆçš„æ¡†æ¶å…è®¸æ¨¡å‹å­¦ä¹ æ›´ä¸°å¯Œçš„åå¥½ç»“æ„ï¼Œå› ä¸ºè·Ÿéšè€…å¯ä»¥æ ¹æ®é¢†å¯¼è€…çš„è¡Œä¸ºè¿›è¡Œä¼˜åŒ–å’Œæ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSLHFçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šè·Ÿéšè€…ç­–ç•¥çš„è®­ç»ƒå’Œé¢†å¯¼è€…ç­–ç•¥çš„ä¼˜åŒ–ã€‚é¦–å…ˆï¼Œé€šè¿‡äººç±»åé¦ˆæ•°æ®è®­ç»ƒè·Ÿéšè€…ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®é¢†å¯¼è€…çš„è¡Œä¸ºç»™å‡ºåå¥½åˆ¤æ–­æˆ–è¿›è¡Œæ”¹è¿›ã€‚ç„¶åï¼Œé¢†å¯¼è€…ç­–ç•¥é€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒè¿›è¡Œä¼˜åŒ–ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–è·Ÿéšè€…ç­–ç•¥çš„åå¥½ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè·Ÿéšè€…ç­–ç•¥å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–é¢†å¯¼è€…çš„è¾“å‡ºï¼Œä»è€Œæé«˜æœ€ç»ˆç»“æœçš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šSLHFçš„å…³é”®åˆ›æ–°åœ¨äºå°†å¯¹é½é—®é¢˜å»ºæ¨¡ä¸ºåºè´¯åšå¼ˆï¼Œè¿™ä¸ä¼ ç»Ÿçš„RLHFå’ŒNLHFæ–¹æ³•ä¸åŒã€‚è¿™ç§å»ºæ¨¡æ–¹å¼å…è®¸æ¨¡å‹å­¦ä¹ æ›´å¤æ‚çš„åå¥½ç»“æ„ï¼Œå¹¶å®ç°æ¨ç†æ—¶çš„ä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒSLHFåœ¨ä¸€è‡´æ€§ã€æ•°æ®æ•æ„Ÿæ€§å’Œå¯¹éä¼ é€’åå¥½çš„é²æ£’æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šSLHFçš„å…³é”®è®¾è®¡åŒ…æ‹¬è·Ÿéšè€…ç­–ç•¥çš„è®­ç»ƒæ–¹å¼ã€é¢†å¯¼è€…ç­–ç•¥çš„ä¼˜åŒ–ç›®æ ‡ä»¥åŠæ¨ç†æ—¶çš„ä¼˜åŒ–ç­–ç•¥ã€‚è·Ÿéšè€…ç­–ç•¥å¯ä»¥ä½¿ç”¨å„ç§ç›‘ç£å­¦ä¹ æˆ–å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯å‡†ç¡®é¢„æµ‹äººç±»åå¥½æˆ–æ”¹è¿›é¢†å¯¼è€…çš„è¾“å‡ºã€‚é¢†å¯¼è€…ç­–ç•¥çš„ä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å¤§åŒ–è·Ÿéšè€…ç­–ç•¥çš„åå¥½ï¼Œå¯ä»¥ä½¿ç”¨å¯¹æŠ—æ€§è®­ç»ƒæˆ–å…¶ä»–ä¼˜åŒ–ç®—æ³•ã€‚æ¨ç†æ—¶ï¼Œè·Ÿéšè€…ç­–ç•¥å¯ä»¥è¿­ä»£åœ°ä¼˜åŒ–é¢†å¯¼è€…çš„è¾“å‡ºï¼Œç›´åˆ°è¾¾åˆ°æ»¡æ„çš„ç»“æœã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16626v1/x1.png" alt="fig_0" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSLHFåœ¨å¤šä¸ªåå¥½æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºRLHFå’ŒNLHFçš„æ€§èƒ½ã€‚SLHFèƒ½å¤Ÿæ‰©å±•åˆ°å…·æœ‰æ•°åäº¿å‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”å…¶æ¨ç†æ—¶ä¼˜åŒ–ç­–ç•¥å¯ä»¥è·¨æ¨¡å‹ç³»åˆ—è¿ç§»ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒã€‚è¿™äº›ç»“æœè¡¨æ˜SLHFå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SLHFå¯åº”ç”¨äºå„ç§éœ€è¦ä¸äººç±»åå¥½å¯¹é½çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€å›¾åƒç”Ÿæˆå’Œæœºå™¨äººæ§åˆ¶ã€‚é€šè¿‡å­¦ä¹ æ›´ä¸°å¯Œçš„åå¥½ç»“æ„ï¼ŒSLHFå¯ä»¥ç”Ÿæˆæ›´ç¬¦åˆäººç±»æœŸæœ›å’Œä»·å€¼è§‚çš„è¾“å‡ºï¼Œæé«˜ç”¨æˆ·æ»¡æ„åº¦å’Œä¿¡ä»»åº¦ã€‚è¯¥æ–¹æ³•è¿˜å¯ç”¨äºä¸ªæ€§åŒ–æ¨èç³»ç»Ÿï¼Œæ ¹æ®ç”¨æˆ·çš„å†å²è¡Œä¸ºå’Œåå¥½ï¼Œæä¾›æ›´ç²¾å‡†çš„æ¨èç»“æœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.

