---
layout: default
title: Meta-RL Induces Exploration in Language Agents
---

# Meta-RL Induces Exploration in Language Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16848" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16848v1</a>
  <a href="https://arxiv.org/pdf/2512.16848.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16848v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16848v1', 'Meta-RL Induces Exploration in Language Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yulun Jiang, Liangze Jiang, Damien Teney, Michael Moor, Maria Brbic

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LaMerï¼šåŸºäºå…ƒå¼ºåŒ–å­¦ä¹ æå‡è¯­è¨€Agentåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ¢ç´¢èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…ƒå¼ºåŒ–å­¦ä¹ ` `è¯­è¨€Agent` `æ¢ç´¢ç­–ç•¥` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `ç­–ç•¥é€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLè®­ç»ƒçš„LLM Agentåœ¨éœ€è¦ä¸»åŠ¨æ¢ç´¢å’Œé•¿æœŸè§„åˆ’çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è¯•é”™ç»éªŒã€‚
2. LaMeré€šè¿‡å…ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé¼“åŠ±Agentåœ¨è®­ç»ƒæ—¶è¿›è¡Œè·¨episodeçš„æ¢ç´¢ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶é€šè¿‡åæ€è¿›è¡Œç­–ç•¥è°ƒæ•´ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMeråœ¨Sokobanã€MineSweeperå’ŒWebshopç­‰ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºRLåŸºçº¿ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)ä½¿å¾—è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹(LLM) Agentä¸ç¯å¢ƒäº¤äº’å¹¶è§£å†³å¤šè½®é•¿æ—¶ç¨‹ä»»åŠ¡æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼ŒRLè®­ç»ƒçš„Agentåœ¨éœ€è¦ä¸»åŠ¨æ¢ç´¢çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”æ— æ³•æœ‰æ•ˆåœ°ä»è¯•é”™ç»éªŒä¸­å­¦ä¹ ã€‚æœ¬æ–‡æå‡ºäº†LaMerï¼Œä¸€ä¸ªé€šç”¨çš„å…ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿LLM Agentèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶ä¸»åŠ¨æ¢ç´¢å¹¶ä»ç¯å¢ƒåé¦ˆä¸­å­¦ä¹ ã€‚LaMeråŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š(i)ä¸€ä¸ªè·¨episodeçš„è®­ç»ƒæ¡†æ¶ï¼Œé¼“åŠ±æ¢ç´¢å’Œé•¿æœŸå¥–åŠ±ä¼˜åŒ–ï¼›(ii)é€šè¿‡åæ€è¿›è¡Œä¸Šä¸‹æ–‡ç­–ç•¥è°ƒæ•´ï¼Œå…è®¸Agentä»ä»»åŠ¡åé¦ˆä¿¡å·ä¸­è°ƒæ•´å…¶ç­–ç•¥ï¼Œè€Œæ— éœ€æ¢¯åº¦æ›´æ–°ã€‚åœ¨å„ç§ç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼ŒLaMeræ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œåœ¨Sokobanã€MineSweeperå’ŒWebshopä¸Šçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†11%ã€14%å’Œ19%ã€‚æ­¤å¤–ï¼Œä¸RLè®­ç»ƒçš„Agentç›¸æ¯”ï¼ŒLaMerè¿˜å±•ç¤ºäº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åº”å¯¹æ›´å…·æŒ‘æˆ˜æ€§æˆ–ä»¥å‰æœªè§è¿‡çš„ä»»åŠ¡ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå…ƒå¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§åŸåˆ™æ€§çš„æ–¹æ³•æ¥è¯±å¯¼è¯­è¨€Agentè¿›è¡Œæ¢ç´¢ï¼Œä»è€Œé€šè¿‡å­¦ä¹ åˆ°çš„æ¢ç´¢ç­–ç•¥å®ç°å¯¹æ–°ç¯å¢ƒçš„æ›´ç¨³å¥çš„é€‚åº”ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„è¯­è¨€Agentåœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œæ¢ç´¢æ—¶å­˜åœ¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚å®ƒä»¬éš¾ä»¥æœ‰æ•ˆåœ°ä»è¯•é”™ç»éªŒä¸­å­¦ä¹ ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é•¿æœŸè§„åˆ’å’Œä¸»åŠ¨æ¢ç´¢çš„ä»»åŠ¡ä¸­ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¾€å¾€ä¾§é‡äºåˆ©ç”¨å·²çŸ¥çš„ç­–ç•¥ï¼Œè€Œå¿½ç•¥äº†å¯¹æœªçŸ¥çŠ¶æ€å’Œè¡Œä¸ºçš„æ¢ç´¢ï¼Œå¯¼è‡´Agentå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLaMerçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å…ƒå¼ºåŒ–å­¦ä¹ çš„æ€æƒ³ï¼Œè®©Agentå­¦ä¹ å¦‚ä½•è¿›è¡Œæœ‰æ•ˆçš„æ¢ç´¢ã€‚é€šè¿‡è·¨episodeçš„è®­ç»ƒï¼ŒAgentå¯ä»¥å­¦ä¹ åˆ°ä¸€ç§é€šç”¨çš„æ¢ç´¢ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ–°çš„ç¯å¢ƒä¸­å¿«é€Ÿé€‚åº”å¹¶æ‰¾åˆ°æœ€ä¼˜è§£ã€‚æ­¤å¤–ï¼ŒLaMerè¿˜å¼•å…¥äº†åæ€æœºåˆ¶ï¼Œå…è®¸Agentæ ¹æ®ç¯å¢ƒçš„åé¦ˆä¿¡å·åŠ¨æ€è°ƒæ•´å…¶ç­–ç•¥ï¼Œä»è€Œæé«˜å…¶é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLaMerçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š(1) è·¨episodeè®­ç»ƒé˜¶æ®µï¼šåœ¨æ­¤é˜¶æ®µï¼ŒAgentåœ¨å¤šä¸ªä¸åŒçš„episodeä¸­ä¸ç¯å¢ƒäº¤äº’ï¼Œå¹¶æ ¹æ®è·å¾—çš„å¥–åŠ±æ›´æ–°å…¶ç­–ç•¥ã€‚è¯¥é˜¶æ®µçš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ç§é€šç”¨çš„æ¢ç´¢ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¸åŒçš„ç¯å¢ƒä¸­å¿«é€Ÿé€‚åº”ã€‚(2) ä¸Šä¸‹æ–‡ç­–ç•¥è°ƒæ•´é˜¶æ®µï¼šåœ¨æ­¤é˜¶æ®µï¼ŒAgentæ ¹æ®ç¯å¢ƒçš„åé¦ˆä¿¡å·ï¼Œé€šè¿‡åæ€æœºåˆ¶åŠ¨æ€è°ƒæ•´å…¶ç­–ç•¥ã€‚è¯¥é˜¶æ®µçš„ç›®æ ‡æ˜¯æé«˜Agentçš„é€‚åº”æ€§å’Œé²æ£’æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹æ–°çš„ç¯å¢ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šLaMerçš„å…³é”®åˆ›æ–°åœ¨äºå°†å…ƒå¼ºåŒ–å­¦ä¹ çš„æ€æƒ³åº”ç”¨äºè¯­è¨€Agentçš„æ¢ç´¢é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒLaMerèƒ½å¤Ÿå­¦ä¹ ä¸€ç§é€šç”¨çš„æ¢ç´¢ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ–°çš„ç¯å¢ƒä¸­å¿«é€Ÿé€‚åº”å¹¶æ‰¾åˆ°æœ€ä¼˜è§£ã€‚æ­¤å¤–ï¼ŒLaMerçš„åæ€æœºåˆ¶ä¹Ÿèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜Agentçš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šLaMerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) è·¨episodeè®­ç»ƒæ¡†æ¶ï¼šè¯¥æ¡†æ¶é¼“åŠ±Agentåœ¨ä¸åŒçš„episodeä¸­è¿›è¡Œæ¢ç´¢ï¼Œå¹¶æ ¹æ®è·å¾—çš„å¥–åŠ±æ›´æ–°å…¶ç­–ç•¥ã€‚(2) åæ€æœºåˆ¶ï¼šè¯¥æœºåˆ¶å…è®¸Agentæ ¹æ®ç¯å¢ƒçš„åé¦ˆä¿¡å·åŠ¨æ€è°ƒæ•´å…¶ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼ŒAgentä¼šæ ¹æ®è¿‡å»çš„ç»éªŒå’Œå½“å‰çš„åé¦ˆï¼Œç”Ÿæˆä¸€æ®µåæ€æ–‡æœ¬ï¼Œç„¶åå°†è¯¥æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯è¾“å…¥åˆ°ç­–ç•¥ç½‘ç»œä¸­ï¼Œä»è€Œè°ƒæ•´å…¶è¡Œä¸ºã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ç›®æ ‡æ˜¯æœ€å¤§åŒ–é•¿æœŸå¥–åŠ±ï¼Œå¹¶é¼“åŠ±Agentè¿›è¡Œæœ‰æ•ˆçš„æ¢ç´¢ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16848v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16848v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16848v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

LaMeråœ¨Sokobanã€MineSweeperå’ŒWebshopç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåˆ†åˆ«æé«˜äº†11%ã€14%å’Œ19%ã€‚ä¸ä¼ ç»Ÿçš„RLåŸºçº¿ç›¸æ¯”ï¼ŒLaMerä¸ä»…åœ¨æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œè€Œä¸”è¿˜å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹æ–°çš„ç¯å¢ƒå’Œä»»åŠ¡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLaMeræ˜¯ä¸€ç§æœ‰æ•ˆçš„å…ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è¯­è¨€Agentçš„æ¢ç´¢èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LaMerå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å¯ä»¥åº”ç”¨äºæ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡å­¦ä¹ æœ‰æ•ˆçš„æ¢ç´¢ç­–ç•¥ï¼ŒAgentå¯ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­è‡ªä¸»åœ°å®Œæˆå„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚åœ¨æ¸¸æˆä¸­æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œåœ¨æœºå™¨äººæ§åˆ¶ä¸­å®ç°è‡ªä¸»å¯¼èˆªï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­å®‰å…¨åœ°è¡Œé©¶ã€‚æ­¤å¤–ï¼ŒLaMerè¿˜å¯ä»¥åº”ç”¨äºæ•™è‚²é¢†åŸŸï¼Œå¸®åŠ©å­¦ç”Ÿæ›´å¥½åœ°å­¦ä¹ å’ŒæŒæ¡çŸ¥è¯†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.

