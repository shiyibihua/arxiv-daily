---
layout: default
title: FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design
---

# FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04405" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.04405v2</a>
  <a href="https://arxiv.org/pdf/2508.04405.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04405v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04405v2', 'FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hao Zhang, Aining Jia, Weifeng Bu, Yushu Cai, Kai Sheng, Hao Chen, Xin He

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-06 (Êõ¥Êñ∞: 2025-11-03)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/FlyFoxPlayer/FlexQ)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫FlexQ‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÈáèÂåñÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈáèÂåñÊäÄÊúØ` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜÂä†ÈÄü` `GPU‰ºòÂåñ` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑINT4/INT8ÈáèÂåñÊñπÊ≥ïÂú®Èôç‰ΩéÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÜÖÂ≠òÂíåËÆ°ÁÆóÊàêÊú¨Êó∂ÔºåÂæÄÂæÄ‰ºöÂØºËá¥ÂáÜÁ°ÆÊÄß‰∏ãÈôçÊàñÊïàÁéá‰∏çË∂≥„ÄÇ
2. FlexQÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêéËÆ≠ÁªÉINT6ÈáèÂåñÊ°ÜÊû∂ÔºåÁªìÂêàÁÆóÊ≥ïÂàõÊñ∞‰∏éÁ≥ªÁªü‰ºòÂåñÔºåÈááÁî®Áªü‰∏ÄÁöÑ6‰ΩçÊùÉÈáçÈáèÂåñÂíåËá™ÈÄÇÂ∫îÁöÑ8‰ΩçÊøÄÊ¥ª‰øùÁïôÁ≠ñÁï•„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFlexQÂú®‰øùÊåÅÊé•ËøëFP16ÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÔºåÊé®ÁêÜÂä†ÈÄüËææÂà∞1.33ÂÄçÔºåÂÜÖÂ≠òËäÇÁúÅËææÂà∞1.21ÂÄçÔºåË°®Áé∞‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂È´òÂÜÖÂ≠òÂíåËÆ°ÁÆóÊàêÊú¨ÈôêÂà∂‰∫ÜÂÆûÈôÖÈÉ®ÁΩ≤„ÄÇÁé∞ÊúâÁöÑINT4/INT8ÈáèÂåñÊñπÊ≥ïËôΩÁÑ∂Èôç‰Ωé‰∫ÜËøô‰∫õÊàêÊú¨Ôºå‰ΩÜÂæÄÂæÄ‰ºöÈôç‰ΩéÂáÜÁ°ÆÊÄßÊàñÊïàÁéá‰∏ç‰Ω≥„ÄÇINT6ÈáèÂåñÂú®Ê®°ÂûãÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÊïàÁéá‰πãÈó¥Êèê‰æõ‰∫ÜÊõ¥Â•ΩÁöÑÂπ≥Ë°°Ôºå‰ΩÜÁé∞‰ª£GPUÁº∫‰πèÁ°¨‰ª∂ÊîØÊåÅÔºåÂØºËá¥Âè™ËÉΩÈÄöËøáÈ´òÁ≤æÂ∫¶ÁÆóÊúØÂçïÂÖÉËøõË°å‰ªøÁúüÔºå‰ªéËÄåÈôêÂà∂‰∫ÜÂä†ÈÄü„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜFlexQÔºå‰∏Ä‰∏™ÁªìÂêàÁÆóÊ≥ïÂàõÊñ∞‰∏éÁ≥ªÁªüÁ∫ß‰ºòÂåñÁöÑÂêéËÆ≠ÁªÉINT6ÈáèÂåñÊ°ÜÊû∂„ÄÇFlexQÂú®ÊâÄÊúâÂ±Ç‰∏≠ÈááÁî®Áªü‰∏ÄÁöÑ6‰ΩçÊùÉÈáçÈáèÂåñÔºåÂπ∂ÈÄöËøáÂ±ÇÁ∫ßÊïèÊÑüÊÄßÂàÜÊûêËá™ÈÄÇÂ∫î‰øùÁïô8‰ΩçÊøÄÊ¥ª„ÄÇ‰∏∫ÊúÄÂ§ßÂåñÁ°¨‰ª∂ÊïàÁéáÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÊîØÊåÅW6A6ÂíåW6A8Ë°®Á§∫ÁöÑÈ´òÊÄßËÉΩGPUÂÜÖÊ†∏ÔºåÊàêÂäüÁªïËøá‰∫ÜÁº∫‰πèÂéüÁîüINT6Âº†ÈáèÊ†∏ÂøÉÁöÑÈóÆÈ¢ò„ÄÇÂØπLLaMAÁ≥ªÂàóÊ®°ÂûãÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåFlexQÂú®WikiText2‰∏ä‰øùÊåÅ‰∫ÜÊé•ËøëFP16ÁöÑÂáÜÁ°ÆÊÄßÔºåÂõ∞ÊÉëÂ∫¶Â¢ûÂä†‰∏çË∂ÖËøá0.1„ÄÇËØ•ÂÜÖÊ†∏Âú®LLaMA-2-70BÁ∫øÊÄßÂ±Ç‰∏äÂÆûÁé∞‰∫ÜÂπ≥Âùá1.39ÂÄçÁöÑÂä†ÈÄüÔºåÊï¥‰Ωì‰∏äÔºåFlexQÂú®Êé®ÁêÜ‰∏äÂÆûÁé∞‰∫Ü1.33ÂÄçÁöÑÂä†ÈÄüÂíå1.21ÂÄçÁöÑÂÜÖÂ≠òËäÇÁúÅ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Èù¢‰∏¥ÁöÑÈ´òÂÜÖÂ≠òÂíåËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑINT4/INT8ÈáèÂåñÊñπÊ≥ïËôΩÁÑ∂ËÉΩÂ§üÈôç‰ΩéËøô‰∫õÊàêÊú¨Ôºå‰ΩÜÈÄöÂ∏∏‰ºöÂØºËá¥Ê®°ÂûãÂáÜÁ°ÆÊÄß‰∏ãÈôçÊàñÊïàÁéá‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöFlexQÊ°ÜÊû∂ÈÄöËøáÂºïÂÖ•INT6ÈáèÂåñÔºåÊèê‰æõ‰∫ÜÊõ¥‰ºòÁöÑÊ®°ÂûãÂáÜÁ°ÆÊÄß‰∏éÊé®ÁêÜÊïàÁéá‰πãÈó¥ÁöÑÂπ≥Ë°°„ÄÇËØ•ÊñπÊ≥ïÂú®ÊâÄÊúâÂ±Ç‰∏≠ÈááÁî®Áªü‰∏ÄÁöÑ6‰ΩçÊùÉÈáçÈáèÂåñÔºåÂπ∂ÈÄöËøáÂ±ÇÁ∫ßÊïèÊÑüÊÄßÂàÜÊûêËá™ÈÄÇÂ∫î‰øùÁïô8‰ΩçÊøÄÊ¥ªÔºå‰ª•ÊèêÈ´òÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöFlexQÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÊùÉÈáçÈáèÂåñÊ®°Âùó„ÄÅÊøÄÊ¥ª‰øùÁïôÊ®°ÂùóÂíåÈ´òÊÄßËÉΩGPUÂÜÖÊ†∏„ÄÇÊùÉÈáçÈáèÂåñÊ®°ÂùóË¥üË¥£Â∞ÜÊ®°ÂûãÊùÉÈáçËΩ¨Êç¢‰∏∫6‰ΩçË°®Á§∫ÔºåÊøÄÊ¥ª‰øùÁïôÊ®°ÂùóÂàôÊ†πÊçÆÂ±ÇÁöÑÊïèÊÑüÊÄßÂÜ≥ÂÆöÊòØÂê¶‰øùÁïô8‰ΩçÊøÄÊ¥ªÔºåÊúÄÂêéÈÄöËøá‰∏ìÈó®ÁöÑGPUÂÜÖÊ†∏ÂÆûÁé∞È´òÊïàÁöÑÁü©Èòµ‰πòÊ≥ïËøêÁÆó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöFlexQÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫ÜÁÆóÊ≥ï‰∏éÁ≥ªÁªüÁ∫ßÁöÑ‰ºòÂåñÔºåÂ∞§ÂÖ∂ÊòØÂºÄÂèë‰∫ÜÊîØÊåÅW6A6ÂíåW6A8Ë°®Á§∫ÁöÑÈ´òÊÄßËÉΩGPUÂÜÖÊ†∏ÔºåÊàêÂäüÁªïËøá‰∫ÜÁº∫‰πèÂéüÁîüINT6Âº†ÈáèÊ†∏ÂøÉÁöÑÈóÆÈ¢ò„ÄÇËøô‰∏ÄËÆæËÆ°ÊòæËëóÊèêÂçá‰∫ÜÊé®ÁêÜÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåFlexQÈááÁî®‰∫ÜÁªü‰∏ÄÁöÑ6‰ΩçÊùÉÈáçÈáèÂåñÔºåÂπ∂ÈÄöËøáÂ±ÇÁ∫ßÊïèÊÑüÊÄßÂàÜÊûêÊù•ÂÜ≥ÂÆöÊøÄÊ¥ªÁöÑ‰ΩçÊï∞„ÄÇÊ≠§Â§ñÔºåÊâÄËÆæËÆ°ÁöÑGPUÂÜÖÊ†∏Âà©Áî®‰∫Ü‰∫åËøõÂà∂Âº†ÈáèÊ†∏ÂøÉÔºàBTCÔºâÁ≠âÊïàÊäÄÊúØÔºåÁ°Æ‰øù‰∫ÜÈ´òÊïàÁöÑÁü©ÈòµËøêÁÆó„ÄÇÊï¥‰ΩìËÆæËÆ°Êó®Âú®ÊúÄÂ§ßÂåñÁ°¨‰ª∂Âà©Áî®ÁéáÂíåÊé®ÁêÜÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

FlexQÂú®LLaMAÁ≥ªÂàóÊ®°Âûã‰∏äÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰øùÊåÅ‰∫ÜÊé•ËøëFP16ÁöÑÂáÜÁ°ÆÊÄßÔºåÂõ∞ÊÉëÂ∫¶Â¢ûÂä†‰∏çË∂ÖËøá0.1„ÄÇÂêåÊó∂ÔºåËØ•Ê°ÜÊû∂Âú®LLaMA-2-70BÁ∫øÊÄßÂ±Ç‰∏äÂÆûÁé∞‰∫ÜÂπ≥Âùá1.39ÂÄçÁöÑÂä†ÈÄüÔºåÊï¥‰ΩìÊé®ÁêÜÂä†ÈÄüËææÂà∞1.33ÂÄçÔºåÂÜÖÂ≠òËäÇÁúÅËææÂà∞1.21ÂÄçÔºåÂ±ïÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

FlexQÁöÑÁ†îÁ©∂ÊàêÊûúÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÈ´òÊïàÊé®ÁêÜÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÈÉ®ÁΩ≤Âú∫ÊôØ‰∏≠„ÄÇÂÖ∂È´òÊïàÁöÑÈáèÂåñÊñπÊ≥ïÂíå‰ºòÂåñÁöÑGPUÂÜÖÊ†∏ËÆæËÆ°ÔºåÂèØ‰ª•ÊòæËëóÈôç‰ΩéÂÜÖÂ≠òÂíåËÆ°ÁÆóÊàêÊú¨ÔºåÊé®Âä®Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊôÆÂèäÔºåÂ¶ÇÊô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®ÁøªËØëÂíåÂÜÖÂÆπÁîüÊàêÁ≠âÈ¢ÜÂüü„ÄÇÊú™Êù•ÔºåFlexQÁöÑÊäÄÊúØÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁöÑ‰ºòÂåñ‰∏≠ÔºåÊèêÂçáÊï¥‰ΩìËÆ°ÁÆóÊïàÁéá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) demonstrate exceptional performance but entail significant memory and computational costs, restricting their practical deployment. While existing INT4/INT8 quantization reduces these costs, they often degrade accuracy or lack optimal efficiency. INT6 quantization offers a superior trade-off between model accuracy and inference efficiency, but lacks hardware support in modern GPUs, forcing emulation via higher-precision arithmetic units that limit acceleration. In this paper, we propose FlexQ, a novel post-training INT6 quantization framework combining algorithmic innovation with system-level optimizations. FlexQ employs uniform 6-bit weight quantization across all layers, with adaptive retention of 8-bit activations in layers identified through layer-wise sensitivity analysis. To maximize hardware efficiency, we develop a specialized high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8 representations via Binary Tensor Core (BTC) equivalents, effectively bypassing the lack of native INT6 tensor cores. Evaluations on LLaMA family models show FlexQ maintains near-FP16 accuracy, with perplexity increases of no more than 0.1 on WikiText2. The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released at https://github.com/FlyFoxPlayer/FlexQ.

