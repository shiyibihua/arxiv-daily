---
layout: default
title: Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos
---

# Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04853" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.04853v1</a>
  <a href="https://arxiv.org/pdf/2508.04853.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04853v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04853v1', 'Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Haoyu Zhang, Shihao Zhang, Ian Colbert, Rayan Saab

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.IT, math.NA

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂèØËØÅÊòéÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÊñπÊ≥ï‰ª•Ëß£ÂÜ≥OPTQÂíåQronosÁöÑÁêÜËÆ∫‰øùËØÅÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂêéËÆ≠ÁªÉÈáèÂåñ` `OPTQ` `Qronos` `ÈáèÂåñËØØÂ∑Æ` `ÁêÜËÆ∫ÂàÜÊûê` `Ê∑±Â∫¶Â≠¶‰π†‰ºòÂåñ` `Ê®°ÂûãÂéãÁº©`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑOPTQÊñπÊ≥ïÁº∫‰πè‰∏•Ê†ºÁöÑÂÆöÈáèÁêÜËÆ∫‰øùËØÅÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØÈù†ÊÄß„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜÈíàÂØπOPTQÂíåQronosÁöÑÂÆöÈáèËØØÂ∑ÆÁïåÈôêÔºåÂàÜÊûê‰∫ÜÈáèÂåñËØØÂ∑ÆÁöÑÊù•Ê∫êÔºåÂπ∂Êèê‰æõ‰∫ÜÁêÜËÆ∫ÊîØÊåÅ„ÄÇ
3. ÈÄöËøáÁêÜËÆ∫ÂàÜÊûêÔºåËÆ∫Êñá‰∏∫OPTQÁöÑËÆæËÆ°ÈÄâÊã©Êèê‰æõ‰∫ÜÊåáÂØºÔºåÂπ∂Âú®ÈöèÊú∫Âèò‰Ωì‰∏≠ÂÆûÁé∞‰∫ÜÊõ¥Âº∫ÁöÑËØØÂ∑ÆÊéßÂà∂ËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂêéËÆ≠ÁªÉÈáèÂåñÔºàPTQÔºâÂ∑≤Êàê‰∏∫Èôç‰ΩéÁé∞‰ª£Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÔºàÂåÖÊã¨Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºâÂÜÖÂ≠òÂíåËÆ°ÁÆóÊàêÊú¨ÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇÂ∞ΩÁÆ°OPTQÊ°ÜÊû∂Âõ†ÂÖ∂ËÆ°ÁÆóÊïàÁéáÂíåÂº∫Â§ßÁöÑÁªèÈ™åÊÄßËÉΩËÄåÂπøÊ≥õÂ∫îÁî®Ôºå‰ΩÜÁº∫‰πè‰∏•Ê†ºÁöÑÁêÜËÆ∫‰øùËØÅ„ÄÇÊú¨ÊñáÈ¶ñÊ¨°‰∏∫OPTQÁöÑÁ°ÆÂÆöÊÄßÂíåÈöèÊú∫Âèò‰Ωì‰ª•ÂèäÁõ∏ÂÖ≥ÁöÑPTQÁÆóÊ≥ïQronosÊèê‰æõ‰∫ÜÂÆöÈáèËØØÂ∑ÆÁïåÈôêÔºåÂàÜÊûê‰∫ÜOPTQËø≠‰ª£ËøáÁ®ãÂºïÂÖ•ÁöÑÈáèÂåñËØØÂ∑ÆÔºåÂπ∂Êé®ÂØºÂá∫‰æùËµñ‰∫éÊ†°ÂáÜÊï∞ÊçÆÂíåÊ≠£ÂàôÂåñÂèÇÊï∞ÁöÑÈùûÊ∏êËøë2ËåÉÊï∞ËØØÂ∑ÆÁïåÈôê„ÄÇÊ≠§Â§ñÔºåÈíàÂØπÈöèÊú∫Âèò‰ΩìÔºåÂª∫Á´ã‰∫ÜÊõ¥Âº∫ÁöÑÊó†Á©∑ËåÉÊï∞ËØØÂ∑ÆÁïåÈôêÔºå‰æø‰∫éÊéßÂà∂ÊâÄÈúÄÁöÑÈáèÂåñÂ≠óÊØçË°®ÔºåÁâπÂà´ÈÄÇÁî®‰∫é‰∏ãÊ∏∏Â±ÇÂíåÈùûÁ∫øÊÄß„ÄÇÊúÄÂêéÔºåÊâ©Â±ï‰∫ÜÂØπQronosÁöÑÂàÜÊûêÔºåÊèê‰æõ‰∫ÜÊñ∞ÁöÑÁêÜËÆ∫ÁïåÈôêÔºåËß£Èáä‰∫ÜÂÖ∂ÁªèÈ™å‰ºòÂäø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂêéËÆ≠ÁªÉÈáèÂåñÔºàPTQÔºâ‰∏≠OPTQÂíåQronosÁº∫‰πè‰∏•Ê†ºÁêÜËÆ∫‰øùËØÅÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÈáèÂåñËØØÂ∑ÆÁöÑÂÆöÈáèÂàÜÊûê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂàÜÊûêOPTQÁöÑËø≠‰ª£ËøáÁ®ãÔºåÊé®ÂØºÂá∫ÈáèÂåñËØØÂ∑ÆÁöÑÈùûÊ∏êËøëÁïåÈôêÔºåÊèê‰æõÁêÜËÆ∫ÊîØÊåÅ‰ª•ÊåáÂØºÂÆûÈôÖËÆæËÆ°ÈÄâÊã©„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂È¶ñÂÖàÂÆö‰πâ‰∫ÜÈáèÂåñËØØÂ∑ÆÁöÑÊù•Ê∫êÔºåÁÑ∂ÂêéÈÄöËøáÂØπÊ†°ÂáÜÊï∞ÊçÆÂíåÊ≠£ÂàôÂåñÂèÇÊï∞ÁöÑÂàÜÊûêÔºåÂª∫Á´ã‰∫ÜËØØÂ∑ÆÁïåÈôêÔºåÂπ∂Êâ©Â±ïÂà∞QronosÁöÑÂàÜÊûê‰∏≠„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÈ¶ñÊ¨°‰∏∫OPTQÂíåQronosÊèê‰æõ‰∫ÜÂÆöÈáèÁöÑËØØÂ∑ÆÁïåÈôêÔºåÁâπÂà´ÊòØÈíàÂØπÈöèÊú∫Âèò‰ΩìÁöÑÊó†Á©∑ËåÉÊï∞ËØØÂ∑ÆÁïåÈôêÔºåÊòæËëóÊèêÂçá‰∫ÜÁêÜËÆ∫ÂàÜÊûêÁöÑÊ∑±Â∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠Ê∂âÂèäÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨ÂØπÁâπÂæÅÊåâËåÉÊï∞ÈÄíÂáèÊéíÂ∫èÁöÑÂêØÂèëÂºèÈÄâÊã©Ôºå‰ª•ÂèäÊ≠£ÂàôÂåñÂèÇÊï∞ÁöÑÈÄâÊã©ÊåáÂØºÔºåËøô‰∫õËÆæËÆ°Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊú¨ÊñáÊèêÂá∫ÁöÑÁêÜËÆ∫ÁïåÈôêËÉΩÂ§üÊúâÊïàÊéßÂà∂ÈáèÂåñËØØÂ∑ÆÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈöèÊú∫Âèò‰Ωì‰∏≠ÔºåËØØÂ∑ÆÊéßÂà∂ËÉΩÂäõÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÈíàÂØπOPTQÁöÑÈùûÊ∏êËøë2ËåÉÊï∞ËØØÂ∑ÆÁïåÈôêÂíåQronosÁöÑÊó†Á©∑ËåÉÊï∞ËØØÂ∑ÆÁïåÈôêÂùáË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÂÆûÁî®ÊÄßÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁöÑ‰ºòÂåñÔºåÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ËøêË°åÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊó∂„ÄÇÈÄöËøáÊèê‰æõÁêÜËÆ∫‰øùËØÅÔºåÁ†îÁ©∂ÊàêÊûúÂèØ‰ª•Â∏ÆÂä©Â∑•Á®ãÂ∏àÊõ¥ÊúâÊïàÂú∞ÂÆûÊñΩÈáèÂåñÊäÄÊúØÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑËøêË°åÊïàÁéáÂíåÂèØÈù†ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Post-training quantization (PTQ) has become a crucial tool for reducing the memory and compute costs of modern deep neural networks, including large language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as GPTQ-has emerged as a leading method due to its computational efficiency and strong empirical performance. Despite its widespread adoption, however, OPTQ lacks rigorous quantitative theoretical guarantees. This paper presents the first quantitative error bounds for both deterministic and stochastic variants of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ algorithm. We analyze how OPTQ's iterative procedure induces quantization error and derive non-asymptotic 2-norm error bounds that depend explicitly on the calibration data and a regularization parameter that OPTQ uses. Our analysis provides theoretical justification for several practical design choices, including the widely used heuristic of ordering features by decreasing norm, as well as guidance for selecting the regularization parameter. For the stochastic variant, we establish stronger infinity-norm error bounds, which enable control over the required quantization alphabet and are particularly useful for downstream layers and nonlinearities. Finally, we extend our analysis to Qronos, providing new theoretical bounds, for both its deterministic and stochastic variants, that help explain its empirical advantages.

