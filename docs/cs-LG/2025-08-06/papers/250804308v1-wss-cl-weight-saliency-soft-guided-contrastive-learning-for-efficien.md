---
layout: default
title: WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification
---

# WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04308" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04308v1</a>
  <a href="https://arxiv.org/pdf/2508.04308.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04308v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04308v1', 'WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thang Duc Tran, Thai Hoang Le

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: 17th International Conference on Computational Collective Intelligence 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWSS-CLä»¥è§£å†³é«˜æ•ˆæœºå™¨é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨é—å¿˜` `å›¾åƒåˆ†ç±»` `å¯¹æ¯”å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `æƒé‡æ˜¾è‘—æ€§` `Kullback-Leibleræ•£åº¦` `æ¨¡å‹æ›´æ–°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨é—å¿˜æ–¹æ³•åœ¨ç²¾ç¡®é—å¿˜ã€ç¨³å®šæ€§å’Œè·¨é¢†åŸŸé€‚ç”¨æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºWSS-CLæ–¹æ³•ï¼Œé€šè¿‡æƒé‡æ˜¾è‘—æ€§å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œåˆ†ä¸ºé—å¿˜é˜¶æ®µå’Œå¯¹æŠ—å¾®è°ƒé˜¶æ®µã€‚
3. å®éªŒè¡¨æ˜ï¼ŒWSS-CLåœ¨é—å¿˜æ•ˆç‡ä¸Šæ˜¾è‘—æå‡ï¼Œæ€§èƒ½æŸå¤±æå°ï¼Œé€‚ç”¨äºç›‘ç£å’Œè‡ªç›‘ç£è®¾ç½®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºå™¨é—å¿˜æ˜¯æŒ‡åœ¨è®­ç»ƒæ¨¡å‹ä¸­é«˜æ•ˆåˆ é™¤ç‰¹å®šæ•°æ®å½±å“çš„è¿‡ç¨‹ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ç°æœ‰çš„æœºå™¨é—å¿˜æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ•°æ®ä¸­å¿ƒæˆ–åŸºäºæƒé‡çš„ç­–ç•¥ï¼Œå¸¸å¸¸é¢ä¸´ç²¾ç¡®é—å¿˜ã€ç¨³å®šæ€§å’Œè·¨é¢†åŸŸé€‚ç”¨æ€§ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µé«˜æ•ˆæœºå™¨é—å¿˜æ–¹æ³•WSS-CLï¼Œåˆ©ç”¨æƒé‡æ˜¾è‘—æ€§èšç„¦äºå…³é”®æ¨¡å‹å‚æ•°çš„é—å¿˜è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•é€šè¿‡æœ€å¤§åŒ–è¾“å‡ºlogitsä¸èšåˆä¼ªæ ‡ç­¾ä¹‹é—´çš„Kullback-Leibleræ•£åº¦æ¥å®ç°é«˜æ•ˆé—å¿˜ï¼Œå¹¶åœ¨å¯¹æŠ—å¾®è°ƒé˜¶æ®µå¼•å…¥è‡ªç›‘ç£çš„å¯¹æ¯”å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWSS-CLåœ¨é—å¿˜æ•ˆç‡ä¸Šæ˜¾è‘—æå‡ï¼Œä¸”ä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œæ€§èƒ½æŸå¤±å¾®ä¹å…¶å¾®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨é—å¿˜ä¸­çš„é«˜æ•ˆåˆ é™¤ç‰¹å®šæ•°æ®å½±å“çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç²¾ç¡®é—å¿˜å’Œç¨³å®šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥åœ¨ä¸åŒé¢†åŸŸä¸­å¹¿æ³›åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šWSS-CLæ–¹æ³•é€šè¿‡æƒé‡æ˜¾è‘—æ€§æ¥èšç„¦å…³é”®æ¨¡å‹å‚æ•°çš„é—å¿˜è¿‡ç¨‹ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé—å¿˜é˜¶æ®µå’Œå¯¹æŠ—å¾®è°ƒé˜¶æ®µï¼Œä»¥æé«˜é—å¿˜æ•ˆç‡å¹¶å‡å°‘æ€§èƒ½æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯é—å¿˜é˜¶æ®µï¼Œé€šè¿‡æœ€å¤§åŒ–è¾“å‡ºlogitsä¸èšåˆä¼ªæ ‡ç­¾ä¹‹é—´çš„Kullback-Leibleræ•£åº¦å®ç°é«˜æ•ˆé—å¿˜ï¼›å…¶æ¬¡æ˜¯å¯¹æŠ—å¾®è°ƒé˜¶æ®µï¼Œé‡‡ç”¨è‡ªç›‘ç£çš„å¯¹æ¯”å­¦ä¹ æ¥ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šWSS-CLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨æƒé‡æ˜¾è‘—æ€§å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸â€œç²¾ç¡®â€é—å¿˜ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚è¿™ä¸€æ–¹æ³•åœ¨ç‰¹å¾ç©ºé—´ä¸­æœ€å¤§åŒ–é—å¿˜æ ·æœ¬ä¸ä¿ç•™æ ·æœ¬ä¹‹é—´çš„è·ç¦»ï¼Œå½¢æˆæ­£è´Ÿæ ·æœ¬å¯¹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¯¹æ¯”æŸå¤±è®¡ç®—ä¸­ï¼Œé—å¿˜æ ·æœ¬ä¸é…å¯¹çš„å¢å¼ºæ ·æœ¬ä½œä¸ºæ­£æ ·æœ¬ï¼Œè€Œä¿ç•™æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬ã€‚è¯¥è®¾è®¡ç¡®ä¿äº†åœ¨ç‰¹å¾ç©ºé—´ä¸­æœ‰æ•ˆåŒºåˆ†é—å¿˜ä¸ä¿ç•™çš„æ•°æ®æ ·æœ¬ã€‚å®éªŒä¸­ï¼Œé‡‡ç”¨äº†é€‚å½“çš„è¶…å‚æ•°è®¾ç½®ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒWSS-CLåœ¨é—å¿˜æ•ˆç‡ä¸Šæ˜¾è‘—æå‡ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œæ€§èƒ½æŸå¤±å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨ç›‘ç£å’Œè‡ªç›‘ç£å­¦ä¹ è®¾ç½®ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€æ•°æ®éšç§ä¿æŠ¤å’Œæ¨¡å‹æ›´æ–°ç­‰åœºæ™¯ã€‚éšç€æ•°æ®éšç§æ³•è§„çš„æ—¥ç›Šä¸¥æ ¼ï¼ŒWSS-CLä¸ºé«˜æ•ˆåˆ é™¤æ•æ„Ÿæ•°æ®æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Machine unlearning, the efficient deletion of the impact of specific data in a trained model, remains a challenging problem. Current machine unlearning approaches that focus primarily on data-centric or weight-based strategies frequently encounter challenges in achieving precise unlearning, maintaining stability, and ensuring applicability across diverse domains. In this work, we introduce a new two-phase efficient machine unlearning method for image classification, in terms of weight saliency, leveraging weight saliency to focus the unlearning process on critical model parameters. Our method is called weight saliency soft-guided contrastive learning for efficient machine unlearning image classification (WSS-CL), which significantly narrows the performance gap with "exact" unlearning. First, the forgetting stage maximizes kullback-leibler divergence between output logits and aggregated pseudo-labels for efficient forgetting in logit space. Next, the adversarial fine-tuning stage introduces contrastive learning in a self-supervised manner. By using scaled feature representations, it maximizes the distance between the forgotten and retained data samples in the feature space, with the forgotten and the paired augmented samples acting as positive pairs, while the retained samples act as negative pairs in the contrastive loss computation. Experimental evaluations reveal that our proposed method yields much-improved unlearning efficacy with negligible performance loss compared to state-of-the-art approaches, indicative of its usability in supervised and self-supervised settings.

