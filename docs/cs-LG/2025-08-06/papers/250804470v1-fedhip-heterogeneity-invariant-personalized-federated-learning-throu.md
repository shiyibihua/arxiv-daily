---
layout: default
title: FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions
---

# FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04470" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04470v1</a>
  <a href="https://arxiv.org/pdf/2508.04470.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04470v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04470v1', 'FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianheng Tang, Zhirui Yang, Jingchao Wang, Kejia Fan, Jinfeng Xu, Huiping Zhuang, Anfeng Liu, Houbing Herbert Song, Leye Wang, Yunhuai Liu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: 11 pages, 5 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFedHiPä»¥è§£å†³ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ä¸­çš„æ•°æ®å¼‚è´¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ` `æ•°æ®å¼‚è´¨æ€§` `æ— æ¢¯åº¦è®­ç»ƒ` `è§£æè§£å†³æ–¹æ¡ˆ` `è‡ªç›‘ç£å­¦ä¹ ` `ç‰¹å¾æå–` `æ¨¡å‹ä¸ªæ€§åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹å®¢æˆ·ç«¯æ•°æ®å¼‚è´¨æ€§æ—¶ï¼Œæ”¶æ•›æ€§å’Œæ€§èƒ½å—åˆ°ä¸¥é‡å½±å“ï¼Œä¸»è¦ä¾èµ–äºæ¢¯åº¦æ›´æ–°çš„æ–¹å¼å­˜åœ¨å±€é™æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„FedHiPæ–¹æ¡ˆé€šè¿‡è§£æè§£å†³æ–¹æ¡ˆé¿å…äº†åŸºäºæ¢¯åº¦çš„æ›´æ–°ï¼Œåˆ©ç”¨å†»ç»“çš„åŸºç¡€æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œè®¾è®¡äº†æ— æ¢¯åº¦çš„è§£æåˆ†ç±»å™¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFedHiPåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡è¾ƒç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†è‡³å°‘5.79%-20.97%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ï¼ˆPFLï¼‰ä½œä¸ºä¸€ç§æµè¡Œçš„èŒƒå¼ï¼Œé€šè¿‡åä½œè®­ç»ƒä¸ºæ¯ä¸ªå®¢æˆ·ç«¯æä¾›ä¸ªæ€§åŒ–æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰PFLæ–¹æ³•é¢ä¸´ç€æ•°æ®å¼‚è´¨æ€§ï¼ˆå³éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®ï¼‰å¸¦æ¥çš„æ˜¾è‘—æŒ‘æˆ˜ï¼Œä¸¥é‡å½±å“äº†æ”¶æ•›æ€§å’Œæ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFedHiPçš„å¼‚è´¨æ€§ä¸å˜ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ æ–¹æ¡ˆï¼Œé€šè¿‡è§£æï¼ˆå³å°é—­å½¢å¼ï¼‰è§£å†³æ–¹æ¡ˆæ¥é¿å…åŸºäºæ¢¯åº¦çš„æ›´æ–°ã€‚æˆ‘ä»¬åˆ©ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒçš„è¶‹åŠ¿ï¼Œé‡‡ç”¨å†»ç»“çš„åŸºç¡€æ¨¡å‹è¿›è¡Œæ— æ¢¯åº¦ç‰¹å¾æå–ï¼Œå¹¶å¼€å‘äº†æ— æ¢¯åº¦è®­ç»ƒçš„è§£æåˆ†ç±»å™¨ã€‚FedHiPæ–¹æ¡ˆåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šè§£ææœ¬åœ°è®­ç»ƒã€è§£æå…¨å±€èšåˆå’Œè§£ææœ¬åœ°ä¸ªæ€§åŒ–ï¼Œç¡®ä¿æ¯ä¸ªä¸ªæ€§åŒ–æ¨¡å‹åœ¨æ•°æ®åˆ†å¸ƒçš„éIIDæƒ…å†µä¸‹ä¿æŒä¸€è‡´ã€‚å¤§é‡å®éªŒéªŒè¯äº†FedHiPæ–¹æ¡ˆçš„ä¼˜è¶Šæ€§ï¼Œå‡†ç¡®ç‡æ¯”æœ€å…ˆè¿›çš„åŸºçº¿æé«˜äº†è‡³å°‘5.79%-20.97%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ä¸­ç”±äºæ•°æ®å¼‚è´¨æ€§å¯¼è‡´çš„æ”¶æ•›æ€§å’Œæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ¢¯åº¦æ›´æ–°ï¼Œéš¾ä»¥æœ‰æ•ˆåº”å¯¹éIIDæ•°æ®çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFedHiPæ–¹æ¡ˆçš„æ ¸å¿ƒåœ¨äºé€šè¿‡è§£æè§£å†³æ–¹æ¡ˆæ›¿ä»£ä¼ ç»Ÿçš„æ¢¯åº¦æ›´æ–°ï¼Œåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ çš„åŸºç¡€æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œä»è€Œå®ç°æ— æ¢¯åº¦è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFedHiPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šè§£ææœ¬åœ°è®­ç»ƒã€è§£æå…¨å±€èšåˆå’Œè§£ææœ¬åœ°ä¸ªæ€§åŒ–ã€‚æ¯ä¸ªé˜¶æ®µéƒ½é‡‡ç”¨è§£ææ–¹æ³•ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒå®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒä¸‹ä¿æŒä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šFedHiPçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å¼‚è´¨æ€§ä¸å˜æ€§ï¼Œå³æ— è®ºæ•°æ®å¦‚ä½•åˆ†å¸ƒï¼Œæ¯ä¸ªä¸ªæ€§åŒ–æ¨¡å‹éƒ½ä¿æŒç›¸åŒã€‚è¿™ä¸€ç‰¹æ€§ä¸ä¼ ç»Ÿæ–¹æ³•çš„ä¾èµ–æ¢¯åº¦æ›´æ–°å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä½¿ç”¨äº†å†»ç»“çš„åŸºç¡€æ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œé¿å…äº†æ¢¯åº¦è®¡ç®—çš„å¤æ‚æ€§ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æ”¯æŒè§£æåˆ†ç±»å™¨çš„æœ‰æ•ˆè®­ç»ƒã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedHiPåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æå‡æ˜¾è‘—ï¼Œè¾ƒç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•æé«˜äº†è‡³å°‘5.79%-20.97%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒFedHiPåœ¨å¤„ç†æ•°æ®å¼‚è´¨æ€§æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œæ™ºèƒ½å®¶å±…ç­‰å¤šä¸ªéœ€è¦ä¸ªæ€§åŒ–æ¨¡å‹çš„åœºæ™¯ã€‚é€šè¿‡æé«˜ä¸ªæ€§åŒ–æ¨¡å‹çš„æ€§èƒ½ï¼ŒFedHiPèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ç²¾å‡†çš„æœåŠ¡ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚æœªæ¥ï¼Œéšç€æ•°æ®éšç§å’Œå®‰å…¨é—®é¢˜çš„æ—¥ç›Šé‡è¦ï¼ŒFedHiPçš„æ— æ¢¯åº¦ç‰¹æ€§å°†ä¸ºä¿æŠ¤ç”¨æˆ·æ•°æ®æä¾›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Lately, Personalized Federated Learning (PFL) has emerged as a prevalent paradigm to deliver personalized models by collaboratively training while simultaneously adapting to each client's local applications. Existing PFL methods typically face a significant challenge due to the ubiquitous data heterogeneity (i.e., non-IID data) across clients, which severely hinders convergence and degrades performance. We identify that the root issue lies in the long-standing reliance on gradient-based updates, which are inherently sensitive to non-IID data. To fundamentally address this issue and bridge the research gap, in this paper, we propose a Heterogeneity-invariant Personalized Federated learning scheme, named FedHiP, through analytical (i.e., closed-form) solutions to avoid gradient-based updates. Specifically, we exploit the trend of self-supervised pre-training, leveraging a foundation model as a frozen backbone for gradient-free feature extraction. Following the feature extractor, we further develop an analytic classifier for gradient-free training. To support both collective generalization and individual personalization, our FedHiP scheme incorporates three phases: analytic local training, analytic global aggregation, and analytic local personalization. The closed-form solutions of our FedHiP scheme enable its ideal property of heterogeneity invariance, meaning that each personalized model remains identical regardless of how non-IID the data are distributed across all other clients. Extensive experiments on benchmark datasets validate the superiority of our FedHiP scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97% in accuracy.

