---
layout: default
title: COPO: Consistency-Aware Policy Optimization
---

# COPO: Consistency-Aware Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04138" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04138v1</a>
  <a href="https://arxiv.org/pdf/2508.04138.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04138v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04138v1', 'COPO: Consistency-Aware Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinghang Han, Jiawei Chen, Hang Shao, Hao Ma, Mingcheng Li, Xintian Shen, Lihao Zheng, Wei Chen, Tao Wei, Lihua Zhang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hijih/copo-code.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€è‡´æ€§æ„è¯†çš„ç­–ç•¥ä¼˜åŒ–ä»¥è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `ä¸€è‡´æ€§å¥–åŠ±` `æ¨ç†èƒ½åŠ›` `ç†µæ··åˆæœºåˆ¶` `æ•°å­¦æ¨ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šä¸ªå“åº”æ”¶æ•›åˆ°ç›¸åŒç»“æœæ—¶ï¼Œå¯¼è‡´ä¼˜åŠ¿é€€åŒ–ä¸ºé›¶ï¼Œé€ æˆæ¢¯åº¦æ¶ˆå¤±ï¼Œé™åˆ¶äº†å­¦ä¹ æ•ˆç‡ã€‚
2. æå‡ºäº†ä¸€ç§ä¸€è‡´æ€§æ„è¯†çš„ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡å…¨å±€å¥–åŠ±å’Œç†µæ··åˆæœºåˆ¶ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ‰æ•ˆå­¦ä¹ ä¿¡å·ã€‚
3. åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå–å¾—æ˜¾è‘—æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¿‘æœŸï¼ŒDeepSeek R1çš„å¼•å…¥æ¿€å‘äº†åˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ä½œä¸ºä½æˆæœ¬æ›¿ä»£æ–¹æ¡ˆæ¥è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’ŒæŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–çš„å…´è¶£ã€‚ç„¶è€Œï¼Œå½“å¤šä¸ªå“åº”åœ¨å•ä¸€æç¤ºä¸‹æ”¶æ•›åˆ°ç›¸åŒç»“æœæ—¶ï¼ŒåŸºäºç»„çš„ä¼˜åŠ¿ä¼šé€€åŒ–ä¸ºé›¶ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œé™åˆ¶äº†è®­ç»ƒæ•ˆç‡å’Œä¸‹æ¸¸æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸€è‡´æ€§æ„è¯†çš„ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡åŸºäºç»“æœä¸€è‡´æ€§çš„å…¨å±€å¥–åŠ±ç»“æ„ï¼Œç¡®ä¿å³ä½¿æ¨¡å‹è¾“å‡ºåœ¨ç»„å†…é«˜åº¦ä¸€è‡´ï¼Œè®­ç»ƒè¿‡ç¨‹ä»èƒ½æ¥æ”¶åˆ°æœ‰æ„ä¹‰çš„å­¦ä¹ ä¿¡å·ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨åŸºäºç†µçš„è½¯æ··åˆæœºåˆ¶ï¼ŒåŠ¨æ€å¹³è¡¡å±€éƒ¨ä¼˜åŠ¿ä¼°è®¡ä¸å…¨å±€ä¼˜åŒ–ï¼Œä¿ƒè¿›æ¢ç´¢ä¸æ”¶æ•›çš„åŠ¨æ€è¿‡æ¸¡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶é²æ£’æ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯å½“å¤šä¸ªå“åº”åœ¨å•ä¸€æç¤ºä¸‹æ”¶æ•›åˆ°ç›¸åŒç»“æœæ—¶ï¼Œå¯¼è‡´çš„ä¼˜åŠ¿é€€åŒ–ä¸ºé›¶å’Œæ¢¯åº¦æ¶ˆå¤±ç°è±¡ã€‚è¿™ä¸€ç°è±¡é™åˆ¶äº†å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ•ˆç‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ä¸€è‡´æ€§æ„è¯†çš„ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åŸºäºç»“æœä¸€è‡´æ€§çš„å…¨å±€å¥–åŠ±ï¼Œç¡®ä¿å³ä½¿åœ¨é«˜ä¸€è‡´æ€§æƒ…å†µä¸‹ï¼Œè®­ç»ƒä»èƒ½è·å¾—æœ‰æ•ˆçš„å­¦ä¹ ä¿¡å·ï¼Œä»è€Œé¼“åŠ±ç”Ÿæˆæ­£ç¡®ä¸”è‡ªæ´½çš„æ¨ç†è·¯å¾„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šå…¨å±€å¥–åŠ±æ¨¡å—å’Œç†µæ··åˆæœºåˆ¶æ¨¡å—ã€‚å…¨å±€å¥–åŠ±æ¨¡å—æ ¹æ®è¾“å‡ºçš„ä¸€è‡´æ€§è®¡ç®—å¥–åŠ±ï¼Œè€Œç†µæ··åˆæœºåˆ¶åˆ™åŠ¨æ€è°ƒæ•´å±€éƒ¨ä¼˜åŠ¿ä¼°è®¡ä¸å…¨å±€ä¼˜åŒ–ä¹‹é—´çš„å¹³è¡¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸€ç§åŸºäºä¸€è‡´æ€§çš„å…¨å±€å¥–åŠ±ç»“æ„å’Œç†µæ··åˆæœºåˆ¶ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œèƒ½å¤Ÿåœ¨é«˜ä¸€è‡´æ€§æƒ…å†µä¸‹ä»ç„¶æä¾›æœ‰æ•ˆçš„å­¦ä¹ ä¿¡å·ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†å…¨å±€æŸå¤±å’Œå±€éƒ¨ä¼˜åŠ¿ä¼°è®¡ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ¨æ€å¹³è¡¡ã€‚æ­¤å¤–ï¼Œç†µæ··åˆæœºåˆ¶çš„å‚æ•°è®¾ç½®å…è®¸æ¨¡å‹åœ¨æ¢ç´¢ä¸æ”¶æ•›ä¹‹é—´çµæ´»åˆ‡æ¢ï¼Œæå‡äº†è®­ç»ƒçš„æ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¤æ‚å†³ç­–æ”¯æŒç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning has significantly enhanced the reasoning capabilities of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the introduction of DeepSeek R1 has inspired a surge of interest in leveraging rule-based rewards as a low-cost alternative for computing advantage functions and guiding policy optimization. However, a common challenge observed across many replication and extension efforts is that when multiple sampled responses under a single prompt converge to identical outcomes, whether correct or incorrect, the group-based advantage degenerates to zero. This leads to vanishing gradients and renders the corresponding samples ineffective for learning, ultimately limiting training efficiency and downstream performance. To address this issue, we propose a consistency-aware policy optimization framework that introduces a structured global reward based on outcome consistency, the global loss based on it ensures that, even when model outputs show high intra-group consistency, the training process still receives meaningful learning signals, which encourages the generation of correct and self-consistent reasoning paths from a global perspective. Furthermore, we incorporate an entropy-based soft blending mechanism that adaptively balances local advantage estimation with global optimization, enabling dynamic transitions between exploration and convergence throughout training. Our method introduces several key innovations in both reward design and optimization strategy. We validate its effectiveness through substantial performance gains on multiple mathematical reasoning benchmarks, highlighting the proposed framework's robustness and general applicability. Code of this work has been released at https://github.com/hijih/copo-code.git.

