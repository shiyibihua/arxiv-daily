---
layout: default
title: Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading
---

# Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04063" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04063v1</a>
  <a href="https://arxiv.org/pdf/2508.04063.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04063v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04063v1', 'Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Joel Walsh, Siddarth Mamidanna, Benjamin Nye, Mark Core, Daniel Auerbach

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: Proceedings of the Second Workshop on Automated Evaluation of Learning and Assessment Content co-located with 26th International Conference on Artificial Intelligence in Education (AIED 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¾®è°ƒæ–¹æ³•ä»¥æ”¹å–„å°‘é‡æ ·æœ¬æç¤ºçš„çŸ­ç­”æ¡ˆè¯„åˆ†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨è¯„åˆ†` `çŸ­ç­”æ¡ˆè¯„åˆ†` `å¾®è°ƒæ–¹æ³•` `å°‘é‡æ ·æœ¬å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆæˆæ•°æ®ç”Ÿæˆ` `æ•™è‚²æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨çŸ­ç­”æ¡ˆè¯„åˆ†æ–¹æ³•åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¾€å¾€ä¾èµ–äºå¤§é‡è®¡ç®—èµ„æºå’Œæ•°æ®ï¼Œé™åˆ¶äº†å…¶æ™®é€‚æ€§ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸¤ç§å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°‘é‡æ ·æœ¬æç¤ºæ”¹å–„çŸ­ç­”æ¡ˆè¯„åˆ†çš„æ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒæ–¹æ³•åœ¨OpenAIçš„å°é—­æ¨¡å‹ä¸Šè¡¨ç°ä¼˜äºå°‘é‡æ ·æœ¬åŸºçº¿ï¼Œè€Œå¯¹Llamaå¼€æ”¾æƒé‡æ¨¡å‹çš„æ•ˆç”¨æœ‰é™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸå…³äºè‡ªåŠ¨çŸ­ç­”æ¡ˆè¯„åˆ†çš„ç ”ç©¶é›†ä¸­åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæç¤ºå·¥ç¨‹å’Œå°‘é‡æ ·æœ¬æç¤ºï¼Œä»¥å®ç°æœ€ä½³ç»“æœã€‚è¿™ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å½¢æˆå¯¹æ¯”ï¼Œåè€…é€šå¸¸éœ€è¦å¤§å‹è®¡ç®—é›†ç¾¤ã€‚æ–°å…´çš„å°é—­æ¨¡å‹æ–¹æ³•å¦‚OpenAIçš„å¾®è°ƒæœåŠ¡æ‰¿è¯ºåœ¨ä»…æœ‰100ä¸ªç¤ºä¾‹çš„æƒ…å†µä¸‹å–å¾—ç»“æœï¼Œè€Œä½¿ç”¨å¼€æ”¾æƒé‡çš„é‡åŒ–ä½ç§©è‡ªé€‚åº”ï¼ˆQLORAï¼‰æ–¹æ³•åˆ™å¯ä»¥åœ¨æ¶ˆè´¹è€…GPUä¸Šå¾®è°ƒæ¨¡å‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†è¿™ä¸¤ç§å¾®è°ƒæ–¹æ³•ï¼Œæµ‹é‡å®ƒä»¬ä¸å°‘é‡æ ·æœ¬æç¤ºåœ¨è‡ªåŠ¨çŸ­ç­”æ¡ˆè¯„åˆ†ä¸­çš„äº¤äº’ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å°‘é‡æ•°æ®çš„å¾®è°ƒå¯¹Llamaå¼€æ”¾æƒé‡æ¨¡å‹çš„æ•ˆç”¨æœ‰é™ï¼Œä½†å¯¹äºOpenAIçš„å°é—­æ¨¡å‹ï¼Œå¾®è°ƒæ–¹æ³•å¯ä»¥è¶…è¶Šå°‘é‡æ ·æœ¬åŸºçº¿ã€‚å°½ç®¡æˆ‘ä»¬çš„è¯„ä¼°é›†æœ‰é™ï¼Œä½†æˆ‘ä»¬å‘ç°å¾®è°ƒçš„å¥½å¤„å¯èƒ½å—åˆ°é¢†åŸŸä¸»é¢˜çš„å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é€šè¿‡ç”¨å¤§é‡å»‰ä»·ç”Ÿæˆçš„åˆæˆè®­ç»ƒæ•°æ®æ¥åˆå§‹åŒ–è®­ç»ƒç¤ºä¾‹ï¼ŒLLama 3.1 8B-Instructå¼€æ”¾æƒé‡æ¨¡å‹çš„è¡¨ç°æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨å°‘é‡æ ·æœ¬æç¤ºå’Œå¾®è°ƒæ–¹æ³•æ¥æå‡è‡ªåŠ¨çŸ­ç­”æ¡ˆè¯„åˆ†çš„æ•ˆæœã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è¯„ä¼°ä¸åŒå¾®è°ƒæ–¹æ³•åœ¨å°‘é‡æ ·æœ¬æç¤ºä¸‹çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯å¯¹æ¯”å°é—­æ¨¡å‹å’Œå¼€æ”¾æƒé‡æ¨¡å‹çš„æ•ˆæœã€‚é€šè¿‡å¾®è°ƒï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼Œä»è€Œæé«˜è¯„åˆ†å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹å¾®è°ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ”¶é›†å°‘é‡æ ·æœ¬æ•°æ®å¹¶ç”Ÿæˆåˆæˆæ•°æ®ï¼›å…¶æ¬¡ï¼Œåº”ç”¨å¾®è°ƒæ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼›æœ€åï¼Œä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºè¯„ä¼°æ¨¡å‹åœ¨çŸ­ç­”æ¡ˆè¯„åˆ†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡å¾®è°ƒæ–¹æ³•åœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹æå‡æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨OpenAIçš„å°é—­æ¨¡å‹ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å°‘é‡æ ·æœ¬æç¤ºæ–¹æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå¾®è°ƒèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ•°æ®èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ï¼Œä»¥åŠå¦‚ä½•ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚è¿™äº›è®¾è®¡å†³å®šäº†æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„é€‚åº”æ€§å’Œè¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒæ–¹æ³•åœ¨OpenAIçš„å°é—­æ¨¡å‹ä¸Šè¶…è¶Šäº†å°‘é‡æ ·æœ¬åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å°¤å…¶æ˜¯LLama 3.1 8B-Instructå¼€æ”¾æƒé‡æ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨å¤§é‡åˆæˆè®­ç»ƒæ•°æ®ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€åœ¨çº¿å­¦ä¹ å¹³å°å’Œè‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿã€‚é€šè¿‡æå‡çŸ­ç­”æ¡ˆè¯„åˆ†çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºæ•™å¸ˆå’Œå­¦ç”Ÿæä¾›æ›´é«˜æ•ˆçš„åé¦ˆæœºåˆ¶ï¼Œä¿ƒè¿›ä¸ªæ€§åŒ–å­¦ä¹ ã€‚æ­¤å¤–ï¼Œå¾®è°ƒæ–¹æ³•çš„çµæ´»æ€§ä½¿å…¶é€‚ç”¨äºå¤šç§é¢†åŸŸçš„æ–‡æœ¬å¤„ç†ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Research to improve Automated Short Answer Grading has recently focused on Large Language Models (LLMs) with prompt engineering and no- or few-shot prompting to achieve best results. This is in contrast to the fine-tuning approach, which has historically required large-scale compute clusters inaccessible to most users. New closed-model approaches such as OpenAI's fine-tuning service promise results with as few as 100 examples, while methods using open weights such as quantized low-rank adaptive (QLORA) can be used to fine-tune models on consumer GPUs. We evaluate both of these fine-tuning methods, measuring their interaction with few-shot prompting for automated short answer grading (ASAG) with structured (JSON) outputs. Our results show that finetuning with small amounts of data has limited utility for Llama open-weight models, but that fine-tuning methods can outperform few-shot baseline instruction-tuned LLMs for OpenAI's closed models. While our evaluation set is limited, we find some evidence that the observed benefits of finetuning may be impacted by the domain subject matter. Lastly, we observed dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by seeding the initial training examples with a significant amount of cheaply generated synthetic training data.

