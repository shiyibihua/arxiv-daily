---
layout: default
title: Automatic LLM Red Teaming
---

# Automatic LLM Red Teaming

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04451" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04451v1</a>
  <a href="https://arxiv.org/pdf/2508.04451.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04451v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04451v1', 'Automatic LLM Red Teaming')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Roman Belaire, Arunesh Sinha, Pradeep Varakantham

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºMDPçš„çº¢é˜Ÿç­–ç•¥ä»¥æå‡LLMå®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çº¢é˜Ÿæµ‹è¯•` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `å¯¹æŠ—æ€§è®­ç»ƒ` `å®‰å…¨æ€§è¯„ä¼°` `åŠ¨æ€ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çº¢é˜Ÿæµ‹è¯•æ–¹æ³•ä¾èµ–äºè„†å¼±çš„æç¤ºæ¨¡æ¿å’Œå•è½®æ”»å‡»ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å¤æ‚çš„å¯¹æŠ—å¯¹è¯ã€‚
2. æœ¬æ–‡æå‡ºå°†çº¢é˜Ÿæµ‹è¯•å½¢å¼åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥åº”å¯¹ç¨€ç–å¥–åŠ±é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å‘ç°å¾®å¦™æ¼æ´æ–¹é¢è¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œè®¾å®šäº†æ–°çš„æŠ€æœ¯æ ‡å‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çº¢é˜Ÿæµ‹è¯•å¯¹äºè¯†åˆ«å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¼æ´å’Œå»ºç«‹ä¿¡ä»»è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªåŠ¨åŒ–æ–¹æ³•ä¾èµ–è„†å¼±çš„æç¤ºæ¨¡æ¿æˆ–å•è½®æ”»å‡»ï¼Œæœªèƒ½æ•æ‰ç°å®ä¸–ç•Œå¯¹æŠ—å¯¹è¯çš„å¤æ‚äº’åŠ¨ç‰¹æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°èŒƒå¼ï¼šè®­ç»ƒAIä»¥æˆ˜ç•¥æ€§åœ°â€œå‡»ç ´â€å¦ä¸€AIã€‚é€šè¿‡å°†çº¢é˜Ÿæµ‹è¯•å½¢å¼åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å¹¶é‡‡ç”¨å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæˆ‘ä»¬æœ‰æ•ˆè§£å†³äº†ç¨€ç–å¥–åŠ±å’Œé•¿æ—¶é—´è·¨åº¦çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç”Ÿæˆä»£ç†é€šè¿‡ç»†ç²’åº¦çš„åŸºäºtokençš„ä¼¤å®³å¥–åŠ±å­¦ä¹ è¿è´¯çš„å¤šè½®æ”»å‡»ç­–ç•¥ï¼Œä»è€Œæ­ç¤ºç°æœ‰åŸºçº¿æœªèƒ½å‘ç°çš„å¾®å¦™æ¼æ´ã€‚è¿™ç§æ–¹æ³•é‡æ–°å®šä¹‰äº†LLMçº¢é˜Ÿæµ‹è¯•ï¼Œæˆä¸ºå¼ºå¥AIéƒ¨ç½²çš„åŠ¨æ€è½¨è¿¹è¿‡ç¨‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çº¢é˜Ÿæµ‹è¯•æ–¹æ³•çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å…¶å¯¹å¤æ‚å¯¹è¯çš„æ•æ‰èƒ½åŠ›å·®å’Œä¾èµ–å•è½®æ”»å‡»çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†çº¢é˜Ÿæµ‹è¯•å½¢å¼åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶å¼•å…¥å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§èƒ½å¤Ÿå­¦ä¹ å¤šè½®æ”»å‡»ç­–ç•¥çš„ç”Ÿæˆä»£ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬çŠ¶æ€è¡¨ç¤ºã€åŠ¨ä½œé€‰æ‹©å’Œå¥–åŠ±æœºåˆ¶ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚çŠ¶æ€è¡¨ç¤ºæ•æ‰å¯¹è¯ä¸Šä¸‹æ–‡ï¼ŒåŠ¨ä½œé€‰æ‹©åŸºäºç­–ç•¥ç½‘ç»œï¼Œå¥–åŠ±æœºåˆ¶åˆ™é€šè¿‡ç»†ç²’åº¦çš„tokençº§åˆ«ä¼¤å®³å¥–åŠ±æ¥å¼•å¯¼å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†çº¢é˜Ÿæµ‹è¯•è§†ä¸ºåŠ¨æ€ã€è½¨è¿¹é©±åŠ¨çš„è¿‡ç¨‹ï¼Œè€Œéä¼ ç»Ÿçš„ä¸€æ­¥æµ‹è¯•ï¼Œä»è€Œæ›´å¥½åœ°æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å¯¹æŠ—åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å±‚æ¬¡åŒ–ç­–ç•¥ç½‘ç»œå’Œç»†ç²’åº¦å¥–åŠ±æœºåˆ¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆç­–ç•¥æ¢¯åº¦å’Œä»·å€¼å‡½æ•°çš„å¤åˆå½¢å¼ï¼Œä»¥ä¼˜åŒ–å¤šè½®å¯¹è¯ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šè½®æ”»å‡»ç­–ç•¥çš„ç”Ÿæˆä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒæˆåŠŸå‘ç°äº†å¤šä¸ªå¾®å¦™æ¼æ´ï¼Œæå‡äº†çº¢é˜Ÿæµ‹è¯•çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨å¯¹æŠ—æ€§å¯¹è¯ä¸­çš„æˆåŠŸç‡æé«˜äº†20%ï¼Œå¹¶ä¸”åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ€§è¯„ä¼°ã€å¯¹æŠ—æ€§è®­ç»ƒå’ŒAIç³»ç»Ÿçš„ä¿¡ä»»æ„å»ºã€‚é€šè¿‡æœ‰æ•ˆè¯†åˆ«å’Œä¿®å¤LLMä¸­çš„æ¼æ´ï¼Œèƒ½å¤Ÿæå‡AIç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ•æ„Ÿé¢†åŸŸå¦‚åŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ç­‰ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„AIå®‰å…¨æ ‡å‡†çš„å»ºç«‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Red teaming is critical for identifying vulnerabilities and building trust in current LLMs. However, current automated methods for Large Language Models (LLMs) rely on brittle prompt templates or single-turn attacks, failing to capture the complex, interactive nature of real-world adversarial dialogues. We propose a novel paradigm: training an AI to strategically `break' another AI. By formalizing red teaming as a Markov Decision Process (MDP) and employing a hierarchical Reinforcement Learning (RL) framework, we effectively address the inherent sparse reward and long-horizon challenges. Our generative agent learns coherent, multi-turn attack strategies through a fine-grained, token-level harm reward, enabling it to uncover subtle vulnerabilities missed by existing baselines. This approach sets a new state-of-the-art, fundamentally reframing LLM red teaming as a dynamic, trajectory-based process (rather than a one-step test) essential for robust AI deployment.

