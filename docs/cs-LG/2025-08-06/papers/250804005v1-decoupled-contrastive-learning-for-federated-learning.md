---
layout: default
title: Decoupled Contrastive Learning for Federated Learning
---

# Decoupled Contrastive Learning for Federated Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04005" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04005v1</a>
  <a href="https://arxiv.org/pdf/2508.04005.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04005v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04005v1', 'Decoupled Contrastive Learning for Federated Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hyungbin Kim, Incheol Baek, Yon Dohn Chung

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§£è€¦å¯¹æ¯”å­¦ä¹ ä»¥è§£å†³è”é‚¦å­¦ä¹ ä¸­çš„æ•°æ®å¼‚è´¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `æ•°æ®å¼‚è´¨æ€§` `æ¨¡å‹è®­ç»ƒ` `éšç§ä¿æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨è”é‚¦å­¦ä¹ ä¸­é¢ä¸´æ•°æ®å¼‚è´¨æ€§å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œå°¤å…¶åœ¨æœ‰é™æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„è§£è€¦å¯¹æ¯”å­¦ä¹ ï¼ˆDCFLï¼‰æ¡†æ¶é€šè¿‡å°†å¯¹æ¯”æŸå¤±åˆ†è§£ä¸ºå¯¹é½æ€§å’Œå‡åŒ€æ€§ä¸¤ä¸ªç›®æ ‡ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„æ¸è¿‘å‡è®¾é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDCFLåœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ­£æ ·æœ¬å¯¹é½æ€§æ›´å¼ºï¼Œè´Ÿæ ·æœ¬å‡åŒ€æ€§æ›´é«˜ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦å­¦ä¹ æ˜¯ä¸€ç§åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œå…è®¸å¤šä¸ªå‚ä¸è€…é€šè¿‡äº¤æ¢æ¨¡å‹æ›´æ–°è€ŒéåŸå§‹æ•°æ®æ¥è®­ç»ƒå…±äº«æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºå®¢æˆ·ç«¯æ•°æ®çš„å¼‚è´¨æ€§ï¼Œå…¶æ€§èƒ½ç›¸è¾ƒäºé›†ä¸­å¼æ–¹æ³•æœ‰æ‰€ä¸‹é™ã€‚å°½ç®¡å¯¹æ¯”å­¦ä¹ è¢«è®¤ä¸ºæ˜¯ç¼“è§£è¿™ä¸€é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ç†è®ºåˆ†æè¡¨æ˜å…¶åœ¨æœ‰é™æ ·æœ¬çš„è”é‚¦å­¦ä¹ ä¸­è¿åäº†æ— é™è´Ÿæ ·æœ¬çš„æ¸è¿‘å‡è®¾ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è§£è€¦å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼ˆDCFLï¼‰ï¼Œå°†ç°æœ‰çš„å¯¹æ¯”æŸå¤±åˆ†è§£ä¸ºä¸¤ä¸ªç›®æ ‡ï¼Œä»è€Œç‹¬ç«‹æ ¡å‡†å¸å¼•åŠ›å’Œæ’æ–¥åŠ›ï¼Œé€‚ç”¨äºæ¯ä¸ªå®¢æˆ·ç«¯æ•°æ®é‡è¾ƒå°çš„è”é‚¦å­¦ä¹ ç¯å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDCFLåœ¨æ­£æ ·æœ¬ä¹‹é—´çš„å¯¹é½æ€§å’Œè´Ÿæ ·æœ¬ä¹‹é—´çš„å‡åŒ€æ€§ä¸Šå‡ä¼˜äºç°æœ‰å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œå¹¶åœ¨CIFAR-10ã€CIFAR-100å’ŒTiny-ImageNetç­‰æ ‡å‡†åŸºå‡†ä¸ŠæŒç»­è¶…è¶Šæœ€å…ˆè¿›çš„è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è”é‚¦å­¦ä¹ ä¸­ç”±äºæ•°æ®å¼‚è´¨æ€§å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨æœ‰é™æ ·æœ¬æƒ…å†µä¸‹æ— æ³•æ»¡è¶³å…¶æ¸è¿‘å‡è®¾ï¼Œå¯¼è‡´æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè§£è€¦å¯¹æ¯”å­¦ä¹ ï¼ˆDCFLï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†å¯¹æ¯”æŸå¤±åˆ†è§£ä¸ºå¯¹é½æ€§å’Œå‡åŒ€æ€§ä¸¤ä¸ªç‹¬ç«‹ç›®æ ‡ï¼Œé¿å…äº†å¯¹æ— é™è´Ÿæ ·æœ¬çš„ä¾èµ–ï¼Œä»è€Œé€‚åº”æ¯ä¸ªå®¢æˆ·ç«¯æ•°æ®é‡è¾ƒå°çš„æƒ…å†µã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDCFLæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå¯¹é½æ€§æ¨¡å—å’Œå‡åŒ€æ€§æ¨¡å—ã€‚å¯¹é½æ€§æ¨¡å—ä¸“æ³¨äºå¢å¼ºæ­£æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œè€Œå‡åŒ€æ€§æ¨¡å—åˆ™ç¡®ä¿è´Ÿæ ·æœ¬ä¹‹é—´çš„åˆ†æ•£æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šDCFLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¯¹æ¯”æŸå¤±è§£è€¦ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ç›®æ ‡ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨æœ‰é™æ ·æœ¬æƒ…å†µä¸‹èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œå­¦ä¹ ï¼Œå…‹æœäº†ä¼ ç»Ÿå¯¹æ¯”å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼ŒDCFLåˆ†åˆ«å®šä¹‰äº†å¯¹é½æŸå¤±å’Œå‡åŒ€æŸå¤±ï¼Œå¹¶é€šè¿‡è°ƒæ•´è¶…å‚æ•°æ¥å¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡çš„å½±å“ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDCFLåœ¨CIFAR-10ã€CIFAR-100å’ŒTiny-ImageNetç­‰æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ­£æ ·æœ¬å¯¹é½æ€§æå‡æ˜¾è‘—ï¼Œè´Ÿæ ·æœ¬å‡åŒ€æ€§å¢å¼ºï¼Œæ•´ä½“æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å¥åº·ã€é‡‘èæœåŠ¡å’Œæ™ºèƒ½åˆ¶é€ ç­‰å¤šä¸ªéœ€è¦ä¿æŠ¤æ•°æ®éšç§çš„è¡Œä¸šã€‚é€šè¿‡åœ¨è¿™äº›é¢†åŸŸä¸­åº”ç”¨DCFLï¼Œå¯ä»¥å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶ä¿æŠ¤ç”¨æˆ·æ•°æ®çš„éšç§ï¼Œæ¨åŠ¨è”é‚¦å­¦ä¹ æŠ€æœ¯çš„å®é™…åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated learning is a distributed machine learning paradigm that allows multiple participants to train a shared model by exchanging model updates instead of their raw data. However, its performance is degraded compared to centralized approaches due to data heterogeneity across clients. While contrastive learning has emerged as a promising approach to mitigate this, our theoretical analysis reveals a fundamental conflict: its asymptotic assumptions of an infinite number of negative samples are violated in finite-sample regime of federated learning. To address this issue, we introduce Decoupled Contrastive Learning for Federated Learning (DCFL), a novel framework that decouples the existing contrastive loss into two objectives. Decoupling the loss into its alignment and uniformity components enables the independent calibration of the attraction and repulsion forces without relying on the asymptotic assumptions. This strategy provides a contrastive learning method suitable for federated learning environments where each client has a small amount of data. Our experimental results show that DCFL achieves stronger alignment between positive samples and greater uniformity between negative samples compared to existing contrastive learning methods. Furthermore, experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art federated learning methods.

