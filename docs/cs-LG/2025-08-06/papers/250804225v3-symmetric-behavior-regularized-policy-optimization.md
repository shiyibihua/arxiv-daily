---
layout: default
title: Symmetric Behavior Regularized Policy Optimization
---

# Symmetric Behavior Regularized Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04225" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04225v3</a>
  <a href="https://arxiv.org/pdf/2508.04225.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04225v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04225v3', 'Symmetric Behavior Regularized Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingwei Zhu, Haseeb Shah, Zheng Chen, Yukie Nagai, Martha White

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06 (æ›´æ–°: 2025-12-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹ç§°è¡Œä¸ºæ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `å¯¹ç§°æ­£åˆ™åŒ–` `ç­–ç•¥ä¼˜åŒ–` `æ³°å‹’çº§æ•°` `æœºå™¨äººæ§åˆ¶` `è‡ªåŠ¨é©¾é©¶` `æ¸¸æˆæ™ºèƒ½ä½“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¡Œä¸ºæ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–æ–¹æ³•åœ¨å¤„ç†ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„åˆ†å¸ƒåç§»æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯å¯¹ç§°æ­£åˆ™åŒ–çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•S$f$-ACï¼Œé€šè¿‡æ³°å‹’çº§æ•°è¿‘ä¼¼å¯¹ç§°æ­£åˆ™åŒ–çš„æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ¡ä»¶å¯¹ç§°é¡¹çš„æ³°å‹’å±•å¼€ï¼Œç¡®ä¿æ•°å€¼ç¨³å®šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒS$f$-ACåœ¨D4RL MuJoCoä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æœ‰æ•ˆé¿å…äº†å…¶ä»–æ–¹æ³•åœ¨ç‰¹å®šç¯å¢ƒä¸‹çš„å¤±è´¥ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¡Œä¸ºæ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆBRPOï¼‰åˆ©ç”¨ä¸å¯¹ç§°æ­£åˆ™åŒ–æ¥å‡è½»ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„åˆ†å¸ƒåç§»ã€‚æœ¬æ–‡é¦–æ¬¡ç ”ç©¶äº†å¯¹ç§°æ­£åˆ™åŒ–çš„å¼€æ”¾æ€§é—®é¢˜ï¼Œå‘ç°å¯¹ç§°æ­£åˆ™åŒ–ä¸å…è®¸è§£ææœ€ä¼˜ç­–ç•¥$Ï€^*$ï¼Œè¿™å¯¹å…¶å®é™…åº”ç”¨æ„æˆæŒ‘æˆ˜ã€‚é€šè¿‡å¯¹Pearson-Vajda $Ï‡^n$æ•£åº¦çš„æ³°å‹’çº§æ•°è¿‘ä¼¼$Ï€^*$ï¼Œæˆ‘ä»¬è¯æ˜ä»…åœ¨$n=5$æ—¶å­˜åœ¨è§£æç­–ç•¥è¡¨è¾¾ã€‚ä¸ºä»¥æ•°å€¼ç¨³å®šçš„æ–¹å¼è®¡ç®—è§£ï¼Œæˆ‘ä»¬æå‡ºå¯¹ç§°æ•£åº¦æŸå¤±çš„æ¡ä»¶å¯¹ç§°é¡¹è¿›è¡Œæ³°å‹’å±•å¼€ï¼Œè¿›è€Œæå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼šå¯¹ç§°$f$-æ¼”å‘˜è¯„è®ºå®¶ï¼ˆS$f$-ACï¼‰ã€‚S$f$-ACåœ¨å„ç§D4RL MuJoCoä»»åŠ¡ä¸­å‡å–å¾—äº†å¼ºåŠ²çš„ç»“æœï¼Œå¹¶é¿å…äº†åœ¨IQLã€SQLã€XQLå’ŒAWACä¸­è§‚å¯Ÿåˆ°çš„æ¯ä¸ªç¯å¢ƒå¤±è´¥ï¼Œä¸ºç¦»çº¿å¼ºåŒ–å­¦ä¹ æä¾›äº†æ›´å¤šæ ·åŒ–å’Œæœ‰æ•ˆçš„æ­£åˆ™åŒ–é€‰æ‹©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ç”±äºåˆ†å¸ƒåç§»å¯¼è‡´çš„ç­–ç•¥ä¼˜åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¯¹ç§°æ­£åˆ™åŒ–æ–¹é¢ç¼ºä¹æœ‰æ•ˆçš„è§£æè§£ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹Pearson-Vajda $Ï‡^n$æ•£åº¦è¿›è¡Œæ³°å‹’çº§æ•°è¿‘ä¼¼ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°ç®—æ³•S$f$-ACï¼Œæ—¨åœ¨å®ç°å¯¹ç§°æ­£åˆ™åŒ–çš„æœ‰æ•ˆåº”ç”¨ï¼Œå¹¶ç¡®ä¿æ•°å€¼è®¡ç®—çš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šS$f$-ACçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹ç§°æ•£åº¦æŸå¤±çš„è®¡ç®—ã€æ³°å‹’å±•å¼€çš„å®ç°ä»¥åŠç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¸»è¦æ¨¡å—æ¶µç›–äº†æ¡ä»¶å¯¹ç§°é¡¹çš„å¤„ç†å’Œç­–ç•¥æ›´æ–°æœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡æå‡ºå¯¹ç§°æ­£åˆ™åŒ–çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶é€šè¿‡æ³°å‹’çº§æ•°è¿‘ä¼¼å®ç°äº†å¯¹ç§°æ•£åº¦çš„æœ‰æ•ˆè®¡ç®—ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ³°å‹’çº§æ•°çš„æˆªæ–­ç‚¹è®¾ç½®ï¼ˆ$n=5$ï¼‰ï¼ŒæŸå¤±å‡½æ•°çš„æ„é€ ï¼Œä»¥åŠç½‘ç»œç»“æ„çš„é€‰æ‹©ï¼Œä»¥ç¡®ä¿ç®—æ³•åœ¨å¤šç§ä»»åŠ¡ä¸­çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒS$f$-ACåœ¨D4RL MuJoCoä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºIQLã€SQLã€XQLå’ŒAWACç­‰åŸºçº¿æ–¹æ³•ï¼ŒæˆåŠŸé¿å…äº†æ¯ä¸ªç¯å¢ƒçš„å¤±è´¥ï¼Œå±•ç°äº†å…¶åœ¨ç­–ç•¥ä¼˜åŒ–ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆæ™ºèƒ½ä½“ç­‰ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœºæ™¯ã€‚é€šè¿‡æä¾›æ›´ä¸ºç¨³å®šå’Œæœ‰æ•ˆçš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼ŒS$f$-ACèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„ç¯å¢ƒä¸­å®ç°æ›´å¥½çš„å­¦ä¹ æ•ˆæœï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Behavior Regularized Policy Optimization (BRPO) leverages asymmetric (divergence) regularization to mitigate the distribution shift in offline Reinforcement Learning. This paper is the first to study the open question of symmetric regularization. We show that symmetric regularization does not permit an analytic optimal policy $Ï€^*$, posing a challenge to practical utility of symmetric BRPO. We approximate $Ï€^*$ by the Taylor series of Pearson-Vajda $Ï‡^n$ divergences and show that an analytic policy expression exists only when the series is capped at $n=5$. To compute the solution in a numerically stable manner, we propose to Taylor expand the conditional symmetry term of the symmetric divergence loss, leading to a novel algorithm: Symmetric $f$-Actor Critic (S$f$-AC). S$f$-AC achieves consistently strong results across various D4RL MuJoCo tasks. Additionally, S$f$-AC avoids per-environment failures observed in IQL, SQL, XQL and AWAC, opening up possibilities for more diverse and effective regularization choices for offline RL.

