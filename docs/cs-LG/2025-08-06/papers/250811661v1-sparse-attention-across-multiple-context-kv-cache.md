---
layout: default
title: Sparse Attention across Multiple-context KV Cache
---

# Sparse Attention across Multiple-context KV Cache

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.11661" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.11661v1</a>
  <a href="https://arxiv.org/pdf/2508.11661.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.11661v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.11661v1', 'Sparse Attention across Multiple-context KV Cache')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziyi Cao, Qingyi Si, Jingbin Zhang, Bingquan Liu

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSamKVä»¥è§£å†³å¤šä¸Šä¸‹æ–‡KVç¼“å­˜çš„ç¨€ç–æ³¨æ„åŠ›é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿åºåˆ—æ¨ç†` `ç¨€ç–æ³¨æ„åŠ›` `å¤šä¸Šä¸‹æ–‡` `é”®å€¼ç¼“å­˜` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šä¸Šä¸‹æ–‡åœºæ™¯ä¸­ç¼ºä¹äº¤å‰æ³¨æ„åŠ›ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚
2. SamKVé€šè¿‡è€ƒè™‘å…¶ä»–ä¸Šä¸‹æ–‡çš„ä¿¡æ¯æ¥ç¨€ç–åŒ–å½“å‰ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSamKVåœ¨ä¸æŸå¤±å‡†ç¡®åº¦çš„æƒ…å†µä¸‹ï¼Œåºåˆ—é•¿åº¦å‹ç¼©è‡³15%ï¼Œæ˜¾è‘—æå‡äº†ååé‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿åºåˆ—æ¨ç†ä¸­é¢ä¸´æ˜¾è‘—çš„æˆæœ¬æŒ‘æˆ˜ã€‚ä¸ºæé«˜æ¨ç†æ•ˆç‡ï¼Œé‡ç”¨å†å²çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å·²æˆä¸ºä¸»æµæ–¹æ³•ã€‚è¿‘æœŸçš„è¿›å±•é€šè¿‡ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶é€‰æ‹©æœ€ç›¸å…³çš„KVç¼“å­˜ï¼Œè¿›ä¸€æ­¥æå‡äº†ååé‡ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯ä»…é™äºå•ä¸Šä¸‹æ–‡åœºæ™¯ã€‚åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœºæ™¯ä¸­ï¼Œæ–‡æ¡£çš„KVç¼“å­˜ç‹¬ç«‹è®¡ç®—ï¼Œç¼ºä¹ä¸Šä¸‹æ–‡é—´çš„äº¤å‰æ³¨æ„åŠ›ï¼Œå¯¼è‡´ç°æœ‰æ–¹æ³•æ•ˆæœä¸ä½³ã€‚æœ¬æ–‡æå‡ºSamKVï¼Œé¦–æ¬¡æ¢ç´¢å¤šä¸Šä¸‹æ–‡KVç¼“å­˜çš„æ³¨æ„åŠ›ç¨€ç–åŒ–ï¼Œè€ƒè™‘å…¶ä»–ä¸Šä¸‹æ–‡çš„äº’è¡¥ä¿¡æ¯ä»¥ç¨€ç–åŒ–ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼Œå¹¶å±€éƒ¨é‡æ–°è®¡ç®—ç¨€ç–ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸é™ä½å‡†ç¡®åº¦çš„æƒ…å†µä¸‹å°†åºåˆ—é•¿åº¦å‹ç¼©è‡³15%ï¼Œæ˜¾è‘—æå‡äº†å¤šä¸Šä¸‹æ–‡RAGåœºæ™¯çš„ååé‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šä¸Šä¸‹æ–‡åœºæ™¯ä¸­ï¼Œç°æœ‰æ–¹æ³•å› ç¼ºä¹äº¤å‰æ³¨æ„åŠ›è€Œå¯¼è‡´çš„æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æŠ€æœ¯åœ¨è®¡ç®—KVç¼“å­˜æ—¶éœ€ä¿ç•™æ‰€æœ‰ç¼“å­˜ï¼Œé€ æˆå†…å­˜å¼€é”€å¤§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSamKVçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç¨€ç–åŒ–ä¸€ä¸ªä¸Šä¸‹æ–‡æ—¶ï¼Œè€ƒè™‘å…¶ä»–ä¸Šä¸‹æ–‡çš„äº’è¡¥ä¿¡æ¯ï¼Œä»è€Œåœ¨ä¿æŒå‡†ç¡®åº¦çš„åŒæ—¶ï¼Œå‡å°‘è®¡ç®—é‡å’Œå†…å­˜ä½¿ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSamKVçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆç‹¬ç«‹è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„KVç¼“å­˜ï¼Œç„¶ååœ¨ç¨€ç–åŒ–è¿‡ç¨‹ä¸­å¼•å…¥å…¶ä»–ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œæœ€åå±€éƒ¨é‡æ–°è®¡ç®—ç¨€ç–åçš„ä¿¡æ¯ä»¥æé«˜æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šSamKVçš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡å®ç°äº†å¤šä¸Šä¸‹æ–‡KVç¼“å­˜çš„æ³¨æ„åŠ›ç¨€ç–åŒ–ï¼Œçªç ´äº†ä»¥å¾€æ–¹æ³•ä»…é€‚ç”¨äºå•ä¸Šä¸‹æ–‡çš„é™åˆ¶ï¼Œæ˜¾è‘—æå‡äº†å¤šä¸Šä¸‹æ–‡åœºæ™¯ä¸‹çš„æ¨ç†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSamKVé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®ä»¥ä¼˜åŒ–ç¨€ç–åŒ–è¿‡ç¨‹ï¼Œå¹¶è®¾è®¡äº†æŸå¤±å‡½æ•°ä»¥ç¡®ä¿åœ¨ç¨€ç–åŒ–è¿‡ç¨‹ä¸­ä¸æŸå¤±å‡†ç¡®åº¦ï¼ŒåŒæ—¶ä¿æŒç½‘ç»œç»“æ„çš„çµæ´»æ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSamKVåœ¨å¤šä¸Šä¸‹æ–‡RAGåœºæ™¯ä¸­å°†åºåˆ—é•¿åº¦å‹ç¼©è‡³15%ï¼Œä¸å…¨é‡è®¡ç®—åŸºçº¿ç›¸æ¯”ï¼Œå‡†ç¡®åº¦æœªé™ä½ï¼Œæ˜¾è‘—æå‡äº†ååé‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡é•¿æ–‡æœ¬å¤„ç†çš„æ•ˆç‡ï¼Œé€‚ç”¨äºä¿¡æ¯æ£€ç´¢ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models face significant cost challenges in long-sequence inference. To address this, reusing historical Key-Value (KV) Cache for improved inference efficiency has become a mainstream approach. Recent advances further enhance throughput by sparse attention mechanisms to select the most relevant KV Cache, thereby reducing sequence length. However, such techniques are limited to single-context scenarios, where historical KV Cache is computed sequentially with causal-attention dependencies. In retrieval-augmented generation (RAG) scenarios, where retrieved documents as context are unknown beforehand, each document's KV Cache is computed and stored independently (termed multiple-context KV Cache), lacking cross-attention between contexts. This renders existing methods ineffective. Although prior work partially recomputes multiple-context KV Cache to mitigate accuracy loss from missing cross-attention, it requires retaining all KV Cache throughout, failing to reduce memory overhead. This paper presents SamKV, the first exploration of attention sparsification for multiple-context KV Cache. Specifically, SamKV takes into account the complementary information of other contexts when sparsifying one context, and then locally recomputes the sparsified information. Experiments demonstrate that our method compresses sequence length to 15% without accuracy degradation compared with full-recompuation baselines, significantly boosting throughput in multi-context RAG scenarios.

