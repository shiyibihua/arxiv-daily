---
layout: default
title: Multimodal RAG Enhanced Visual Description
---

# Multimodal RAG Enhanced Visual Description

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09170" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09170v1</a>
  <a href="https://arxiv.org/pdf/2508.09170.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09170v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09170v1', 'Multimodal RAG Enhanced Visual Description')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amit Kumar Jaiswal, Haiming Liu, Ingo Frommholz

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: Accepted by ACM CIKM 2025. 5 pages, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§RAGå¢å¼ºè§†è§‰æè¿°æ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹é½` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å›¾åƒæè¿°ç”Ÿæˆ` `çº¿æ€§æ˜ å°„` `è®­ç»ƒæ— å…³æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ–‡æœ¬ä¸è§†è§‰è¡¨ç¤ºçš„å¯¹é½ä¸Šå­˜åœ¨æ¨¡æ€å·®è·ï¼Œå¾®è°ƒæˆæœ¬é«˜ä¸”ä¾èµ–å¤§é‡é¢†åŸŸæ•°æ®ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒæ— å…³çš„è½»é‡çº§æ–¹æ³•ï¼Œåˆ©ç”¨RAGæŠ€æœ¯é€šè¿‡çº¿æ€§æ˜ å°„å®ç°æ¨¡æ€æ‰©å±•ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚
3. åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒæè¿°ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é’ˆå¯¹å¤šæ¨¡æ€è¾“å…¥çš„æ–‡æœ¬æè¿°ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„è®­ç»ƒæ— å…³æ–¹æ³•ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œé€šè¿‡çº¿æ€§æ˜ å°„æœ‰æ•ˆæ‰©å±•æ¨¡æ€ã€‚ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æ–‡æœ¬ä¸è§†è§‰è¡¨ç¤ºçš„å…±åŒåµŒå…¥ç©ºé—´ä¸­å­˜åœ¨æ¨¡æ€å·®è·ï¼Œå°½ç®¡å¾®è°ƒå¯ä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†é€šå¸¸ä»£ä»·é«˜æ˜‚ä¸”ä¸åˆ‡å®é™…ã€‚é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­åº”ç”¨è¯¥æ˜ å°„ï¼Œèƒ½å¤Ÿä»è®­ç»ƒé›†ä¸­æ£€ç´¢åˆ°æœ€æ¥è¿‘çš„æ–‡æœ¬æè¿°ï¼Œå¹¶ç»“åˆæŒ‡ä»¤ä¸ºè¯­è¨€æ¨¡å‹ç”Ÿæˆæ–°çš„æ–‡æœ¬æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†å¤šæ¨¡æ€æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€è¾“å…¥ä¸­ï¼Œæ–‡æœ¬ä¸è§†è§‰è¡¨ç¤ºä¹‹é—´çš„æ¨¡æ€å·®è·é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­éœ€è¦å¤§é‡é¢†åŸŸç‰¹å®šæ•°æ®ï¼Œä¸”æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥å®æ–½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„è®­ç»ƒæ— å…³æ–¹æ³•ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œé€šè¿‡çº¿æ€§æ˜ å°„æ¥æ‰©å±•æ¨¡æ€ï¼Œä»è€Œå®ç°æ–‡æœ¬ä¸è§†è§‰çš„æœ‰æ•ˆå¯¹é½ã€‚è¯¥è®¾è®¡æ—¨åœ¨é™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆæ–‡æœ¬çš„ç›¸å…³æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å¯¹å›¾åƒè¿›è¡ŒåµŒå…¥ï¼›å…¶æ¬¡ï¼Œé€šè¿‡çº¿æ€§æ˜ å°„æ£€ç´¢ä¸å›¾åƒæœ€æ¥è¿‘çš„æ–‡æœ¬æè¿°ï¼›æœ€åï¼Œå°†æ£€ç´¢åˆ°çš„æ–‡æœ¬æè¿°ä¸æŒ‡ä»¤ç»“åˆï¼Œä½œä¸ºè¾“å…¥æç¤ºä¾›è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–°çš„æ–‡æœ¬æè¿°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§è®­ç»ƒæ— å…³çš„çº¿æ€§æ˜ å°„æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³æ¨¡æ€å·®è·é—®é¢˜ï¼Œè€Œä¸éœ€è¦æ˜‚è´µçš„å¾®è°ƒè¿‡ç¨‹ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®çš„å¾®è°ƒæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡è®¾è®¡äº†é«˜æ•ˆçš„çº¿æ€§æ˜ å°„ç®—æ³•ï¼Œå¹¶ä¼˜åŒ–äº†ç”Ÿæˆçš„æ–‡æœ¬æè¿°ä»¥ç¬¦åˆæ ‡å‡†çš„å›¾åƒæè¿°è¯„ä¼°æŒ‡æ ‡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒå®Œæ•´è®ºæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†å¤šæ¨¡æ€æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å›¾åƒæè¿°ç”Ÿæˆçš„æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œéœ€å‚è€ƒå®Œæ•´è®ºæ–‡ä»¥è·å–è¯¦ç»†æ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒæè¿°ç”Ÿæˆã€æ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€æ¨¡å‹çš„æ–‡æœ¬ä¸è§†è§‰å¯¹é½èƒ½åŠ›ï¼Œå¯ä»¥åœ¨è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆã€å¢å¼ºç°å®ç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Textual descriptions for multimodal inputs entail recurrent refinement of queries to produce relevant output images. Despite efforts to address challenges such as scaling model size and data volume, the cost associated with pre-training and fine-tuning remains substantial. However, pre-trained large multimodal models (LMMs) encounter a modality gap, characterised by a misalignment between textual and visual representations within a common embedding space. Although fine-tuning can potentially mitigate this gap, it is typically expensive and impractical due to the requirement for extensive domain-driven data. To overcome this challenge, we propose a lightweight training-free approach utilising Retrieval-Augmented Generation (RAG) to extend across the modality using a linear mapping, which can be computed efficiently. During inference, this mapping is applied to images embedded by an LMM enabling retrieval of closest textual descriptions from the training set. These textual descriptions, in conjunction with an instruction, cater as an input prompt for the language model to generate new textual descriptions. In addition, we introduce an iterative technique for distilling the mapping by generating synthetic descriptions via the language model facilitating optimisation for standard utilised image description measures. Experimental results on two benchmark multimodal datasets demonstrate significant improvements.

