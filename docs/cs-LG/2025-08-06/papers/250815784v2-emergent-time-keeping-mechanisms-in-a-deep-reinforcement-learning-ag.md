---
layout: default
title: Emergent time-keeping mechanisms in a deep reinforcement learning agent performing an interval timing task
---

# Emergent time-keeping mechanisms in a deep reinforcement learning agent performing an interval timing task

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15784" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.15784v2</a>
  <a href="https://arxiv.org/pdf/2508.15784.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15784v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15784v2', 'Emergent time-keeping mechanisms in a deep reinforcement learning agent performing an interval timing task')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amrapali Pednekar, Alvaro Garrido, Pieter Simoens, Yara Khaluf

**åˆ†ç±»**: q-bio.NC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06 (æ›´æ–°: 2025-08-26)

**å¤‡æ³¨**: Accepted at 2025 Artificial Life Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†çš„æ—¶é—´ä¿æŒæœºåˆ¶ä»¥è§£å†³æ—¶é—´å¤„ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `æ—¶é—´å¤„ç†` `ç”Ÿç‰©ç³»ç»Ÿ` `ç¥ç»æ¿€æ´»` `é—´éš”è®¡æ—¶` `æŒ¯è¡ç¥ç»å…ƒ` `ç”Ÿç‰©å­¦æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ—¶é—´å¤„ç†æœºåˆ¶ç¼ºä¹ç»Ÿä¸€çš„ç†è§£ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿç‰©ç³»ç»Ÿä¸æ·±åº¦ç¥ç»ç½‘ç»œä¹‹é—´çš„æ¯”è¾ƒä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚
2. æœ¬ç ”ç©¶æå‡ºé€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†è¿›è¡Œé—´éš”è®¡æ—¶ä»»åŠ¡ï¼Œæ¢ç´¢å…¶å†…éƒ¨æŒ¯è¡ç¥ç»æ¿€æ´»ä¸ç”Ÿç‰©ç³»ç»Ÿçš„ç›¸ä¼¼æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä»£ç†åœ¨ä¸åŒè§†é¢‘åºåˆ—ä¸Šä¿æŒäº†è‰¯å¥½çš„ä»»åŠ¡è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºå…¶æ—¶é—´ä¿æŒæœºåˆ¶çš„å†…åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨é—´éš”è®¡æ—¶ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¢è®¨äº†æ—¶é—´å¤„ç†çš„æœºåˆ¶ã€‚ä»£ç†æˆåŠŸè®­ç»ƒå®ŒæˆæŒç»­æ—¶é—´ç”Ÿäº§ä»»åŠ¡ï¼Œåˆ†æå…¶å†…éƒ¨çŠ¶æ€æ˜¾ç¤ºå‡ºç”Ÿç‰©ç³»ç»Ÿä¸­æ™®éå­˜åœ¨çš„æŒ¯è¡ç¥ç»æ¿€æ´»æ¨¡å¼ã€‚ä»£ç†çš„è¡Œä¸ºä¸»è¦å—é«˜å¹…åº¦å’Œé¢‘ç‡çš„æŒ¯è¡ç¥ç»å…ƒå½±å“ï¼Œè¿™ä¸ç”Ÿç‰©å­¦ä¸­çš„çº¹çŠ¶ä½“èŠ‚æ‹é¢‘ç‡æ¨¡å‹ç›¸ä¼¼ã€‚ä»£ç†åœ¨ä¸åŒè§†é¢‘åºåˆ—ä¸Šä¿æŒäº†å…¶æŒ¯è¡è¡¨ç¤ºå’Œä»»åŠ¡è¡¨ç°ï¼Œè¡¨æ˜å…¶æ—¶é—´ä¿æŒæœºåˆ¶çš„å†…åŒ–ï¼Œä¸”å¯¹ç¯å¢ƒçš„ä¾èµ–æ€§è¾ƒå°ã€‚ç ”ç©¶æ—¨åœ¨åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œç†è§£ç”Ÿç‰©ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯æ—¶é—´å¤„ç†æ–¹é¢çš„æœºåˆ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨æ—¶é—´å¤„ç†ä»»åŠ¡ä¸­çš„è¡¨ç°æœºåˆ¶ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ­ç¤ºå…¶å†…éƒ¨æ—¶é—´ä¿æŒæœºåˆ¶çš„ç”Ÿç‰©å­¦å¯¹åº”å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®­ç»ƒæ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†æ‰§è¡ŒæŒç»­æ—¶é—´ç”Ÿäº§ä»»åŠ¡ï¼Œåˆ†æå…¶å†…éƒ¨çŠ¶æ€å’Œç¥ç»æ¿€æ´»æ¨¡å¼ï¼Œä»¥æ¢ç´¢å…¶æ—¶é—´å¤„ç†çš„ç”Ÿç‰©å­¦æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä»£ç†çš„è®­ç»ƒé˜¶æ®µã€å†…éƒ¨çŠ¶æ€åˆ†æé˜¶æ®µå’Œè¡Œä¸ºè¡¨ç°è¯„ä¼°é˜¶æ®µã€‚ä»£ç†åœ¨è§‚çœ‹è§†é¢‘åºåˆ—æ—¶æ ‡è®°ç›®æ ‡é—´éš”ï¼Œåˆ†æå…¶ç¥ç»å…ƒçš„æŒ¯è¡æ´»åŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæ­ç¤ºäº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†çš„æ—¶é—´ä¿æŒæœºåˆ¶ä¸ç”Ÿç‰©å­¦æ¨¡å‹ï¼ˆå¦‚çº¹çŠ¶ä½“èŠ‚æ‹é¢‘ç‡æ¨¡å‹ï¼‰çš„ç›¸ä¼¼æ€§ï¼Œæä¾›äº†æ–°çš„ç†è§£æ¡†æ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šä»£ç†çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å…¶æ—¶é—´æ ‡è®°èƒ½åŠ›ï¼Œå¹¶å…³æ³¨é«˜å¹…åº¦å’Œé¢‘ç‡çš„æŒ¯è¡ç¥ç»å…ƒï¼Œç¡®ä¿å…¶åœ¨ä¸åŒç¯å¢ƒä¸‹çš„è¡¨ç°ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»£ç†åœ¨ä¸åŒè§†é¢‘åºåˆ—ä¸Šå‡èƒ½ä¿æŒè‰¯å¥½çš„ä»»åŠ¡è¡¨ç°ï¼ŒæŒ¯è¡ç¥ç»å…ƒçš„æ¿€æ´»æ¨¡å¼ä¸ç›®æ ‡é—´éš”é«˜åº¦ç›¸å…³ï¼Œè¡¨æ˜å…¶æ—¶é—´ä¿æŒæœºåˆ¶çš„å†…åŒ–èƒ½åŠ›ã€‚ä»£ç†åœ¨ä»»åŠ¡æ‰§è¡Œä¸­çš„ç¯å¢ƒä¾èµ–æ€§æ˜¾è‘—é™ä½ï¼Œå±•ç¤ºäº†å…¶åœ¨æ—¶é—´å¤„ç†æ–¹é¢çš„ç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ™ºèƒ½ç›‘æ§ç³»ç»Ÿå’Œç”Ÿç‰©èŠ‚å¾‹ç ”ç©¶ç­‰ã€‚é€šè¿‡ç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ—¶é—´å¤„ç†æœºåˆ¶ï¼Œå¯ä»¥ä¸ºç”Ÿç‰©ç³»ç»Ÿçš„æ¨¡æ‹Ÿå’Œä¼˜åŒ–æä¾›æ–°çš„æ€è·¯ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Drawing parallels between Deep Artificial Neural Networks (DNNs) and biological systems can aid in understanding complex biological mechanisms that are difficult to disentangle. Temporal processing, an extensively researched topic, is one such example that lacks a coherent understanding of its underlying mechanisms. In this study, we investigate temporal processing in a Deep Reinforcement Learning (DRL) agent performing an interval timing task and explore potential biological counterparts to its emergent behavior. The agent was successfully trained to perform a duration production task, which involved marking successive occurrences of a target interval while viewing a video sequence. Analysis of the agent's internal states revealed oscillatory neural activations, a ubiquitous pattern in biological systems. Interestingly, the agent's actions were predominantly influenced by neurons exhibiting these oscillations with high amplitudes and frequencies corresponding to the target interval. Parallels are drawn between the agent's time-keeping strategy and the Striatal Beat Frequency (SBF) model, a biologically plausible model of interval timing. Furthermore, the agent maintained its oscillatory representations and task performance when tested on different video sequences (including a blank video). Thus, once learned, the agent internalized its time-keeping mechanism and showed minimal reliance on its environment to perform the timing task. A hypothesis about the resemblance between this emergent behavior and certain aspects of the evolution of biological processes like circadian rhythms, has been discussed. This study aims to contribute to recent research efforts of utilizing DNNs to understand biological systems, with a particular emphasis on temporal processing.

