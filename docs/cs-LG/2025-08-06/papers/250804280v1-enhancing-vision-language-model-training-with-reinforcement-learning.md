---
layout: default
title: Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success
---

# Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04280" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04280v1</a>
  <a href="https://arxiv.org/pdf/2508.04280.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04280v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04280v1', 'Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: George Bredis, Stanislav Dereka, Viacheslav Sinii, Ruslan Rakhimov, Daniil Gavrilov

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVL-DACä»¥è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€æ™ºèƒ½ä½“` `åˆæˆç¯å¢ƒ` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°†è§†è§‰ä¿¡æ¯è½¬åŒ–ä¸ºè¯­è¨€æ¡ä»¶åŠ¨ä½œåºåˆ—æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„VL-DACç®—æ³•é€šè¿‡è§£è€¦åŠ¨ä½œå’Œä»·å€¼å­¦ä¹ ï¼Œé¿å…äº†å¤æ‚çš„è¶…å‚æ•°è°ƒä¼˜ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVL-DACè®­ç»ƒçš„æ¨¡å‹åœ¨çœŸå®å›¾åƒçš„ä»£ç†æ§åˆ¶ã€ç©ºé—´æ¨ç†å’Œç½‘ç»œå¯¼èˆªä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº¤äº’å¼å¤šæ¨¡æ€æ™ºèƒ½ä½“å¿…é¡»å°†åŸå§‹è§†è§‰è§‚å¯Ÿè½¬åŒ–ä¸ºè¿è´¯çš„è¯­è¨€æ¡ä»¶åŠ¨ä½œåºåˆ—ï¼Œè€Œå½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¿™æ–¹é¢ä»æ˜¾ä¸è¶³ã€‚ä»¥å¾€çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å°è¯•è™½ç„¶ç†è®ºä¸Šå¯ä»¥èµ‹äºˆVLMsæ­¤èƒ½åŠ›ï¼Œä½†å¾ˆå°‘æµ‹è¯•æ‰€å­¦è¡Œä¸ºæ˜¯å¦èƒ½è¶…è¶Šè®­ç»ƒæ¨¡æ‹Ÿå™¨ï¼Œä¸”é€šå¸¸ä¾èµ–è„†å¼±çš„è¶…å‚æ•°è°ƒä¼˜æˆ–ä½çŠ¶æ€å˜å¼‚æ€§çš„ç¨ å¯†å¥–åŠ±ç¯å¢ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ã€æ— è¶…å‚æ•°çš„RLç®—æ³•â€”â€”è§†è§‰è¯­è¨€è§£è€¦æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆVL-DACï¼‰ï¼Œè¯¥ç®—æ³•åœ¨åŠ¨ä½œä»¤ç‰Œä¸Šåº”ç”¨PPOæ›´æ–°ï¼ŒåŒæ—¶ä»…åœ¨ç¯å¢ƒæ­¥éª¤çº§åˆ«å­¦ä¹ ä»·å€¼ã€‚è¿™ç§ç®€å•çš„è§£è€¦æ¶ˆé™¤äº†ä¸ç¨³å®šçš„åŠ æƒé¡¹ï¼Œå¸¦æ¥äº†æ›´å¿«ã€æ›´å¯é çš„æ”¶æ•›ã€‚åœ¨ä¸€ä¸ªä¾¿å®œçš„æ¨¡æ‹Ÿå™¨ä¸­è®­ç»ƒå•ä¸ªVLMï¼Œå·²èƒ½äº§ç”Ÿå¹¿æ³›æ³›åŒ–çš„ç­–ç•¥ï¼Œä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚ä»¥å¾€çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤æ‚çš„è¶…å‚æ•°è°ƒä¼˜å’Œä½å˜å¼‚æ€§çš„ç¯å¢ƒï¼Œå¯¼è‡´æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„VL-DACç®—æ³•é€šè¿‡å°†åŠ¨ä½œä»¤ç‰Œçš„æ›´æ–°ä¸ç¯å¢ƒæ­¥éª¤çš„ä»·å€¼å­¦ä¹ è§£è€¦ï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…äº†ä¸ç¨³å®šçš„åŠ æƒé¡¹ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVL-DACçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŠ¨ä½œæ›´æ–°æ¨¡å—å’Œä»·å€¼å­¦ä¹ æ¨¡å—ã€‚åŠ¨ä½œæ›´æ–°æ¨¡å—ä½¿ç”¨PPOç®—æ³•å¯¹åŠ¨ä½œä»¤ç‰Œè¿›è¡Œä¼˜åŒ–ï¼Œè€Œä»·å€¼å­¦ä¹ æ¨¡å—åˆ™åœ¨æ¯ä¸ªç¯å¢ƒæ­¥éª¤ä¸­ç‹¬ç«‹å­¦ä¹ çŠ¶æ€ä»·å€¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šVL-DACçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è½»é‡çº§è®¾è®¡å’Œæ— è¶…å‚æ•°çš„ç‰¹æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚é€šè¿‡è¿™ç§è§£è€¦è®¾è®¡ï¼ŒVL-DACèƒ½å¤Ÿåœ¨ä½æˆæœ¬çš„åˆæˆç¯å¢ƒä¸­æœ‰æ•ˆè®­ç»ƒVLMsï¼Œå¹¶å®ç°è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šVL-DACåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¾èµ–äºå¤æ‚çš„è¶…å‚æ•°è®¾ç½®ï¼Œä¸”é‡‡ç”¨äº†ç®€å•çš„æŸå¤±å‡½æ•°è®¾è®¡ï¼Œç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹çš„é«˜æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨VL-DACè®­ç»ƒçš„æ¨¡å‹åœ¨BALROGåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºåŸºçº¿æå‡äº†50%çš„ç›¸å¯¹æ€§èƒ½ï¼Œåœ¨VSI-Benchçš„æœ€éš¾éƒ¨åˆ†æå‡äº†5%ï¼Œåœ¨VisualWebBenchä¸­æå‡äº†2%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVL-DACèƒ½å¤Ÿåœ¨ä¸é™ä½å›¾åƒç†è§£å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹ŸåŠ©æ‰‹ç­‰å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿã€‚é€šè¿‡åœ¨åˆæˆç¯å¢ƒä¸­æœ‰æ•ˆè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæå‡è¿™äº›ç³»ç»Ÿåœ¨çœŸå®ä¸–ç•Œä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Interactive multimodal agents must convert raw visual observations into coherent sequences of language-conditioned actions -- a capability that current vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL) efforts could, in principle, endow VLMs with such skills, but they have seldom tested whether the learned behaviours generalize beyond their training simulators, and they depend either on brittle hyperparameter tuning or on dense-reward environments with low state variability. We introduce Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight, hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens while learning value only at the environment-step level: an arrangement, to our knowledge, not previously explored for large VLMs or LLMs. This simple decoupling removes unstable weighting terms and yields faster, more reliable convergence. Training a single VLM with VL-DAC in one inexpensive simulator at a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies that generalize widely: +50\% relative on BALROG (game-centric agentic control), +5\% relative on the hardest part of VSI-Bench (spatial planning), and +2\% on VisualWebBench (web navigation), all without degrading general image understanding accuracy. These results provide the first evidence that a simple RL algorithm can train VLMs entirely in cheap synthetic worlds while delivering measurable gains on real-image agentic, spatial-reasoning, and web-navigation benchmarks.

