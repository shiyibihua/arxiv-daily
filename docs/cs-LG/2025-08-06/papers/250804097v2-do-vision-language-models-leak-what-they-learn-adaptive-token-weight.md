---
layout: default
title: Do Vision-Language Models Leak What They Learn? Adaptive Token-Weighted Model Inversion Attacks
---

# Do Vision-Language Models Leak What They Learn? Adaptive Token-Weighted Model Inversion Attacks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04097" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04097v2</a>
  <a href="https://arxiv.org/pdf/2508.04097.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04097v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04097v2', 'Do Vision-Language Models Leak What They Learn? Adaptive Token-Weighted Model Inversion Attacks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ngoc-Bao Nguyen, Sy-Tuyen Ho, Koh Jun Hao, Ngai-Man Cheung

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06 (æ›´æ–°: 2025-12-01)

**å¤‡æ³¨**: Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”ä»¤ç‰ŒåŠ æƒæ¨¡å‹åæ¼”æ”»å‡»ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹éšç§æ³„éœ²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹åæ¼”` `è§†è§‰è¯­è¨€æ¨¡å‹` `éšç§ä¿æŠ¤` `è‡ªé€‚åº”åŠ æƒ` `æ·±åº¦å­¦ä¹ ` `æ•°æ®æ³„éœ²` `å›¾åƒé‡æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºå•æ¨¡æ€æ·±åº¦ç½‘ç»œï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çš„éšç§æ³„éœ²é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€å¥—é’ˆå¯¹VLMçš„æ¨¡å‹åæ¼”æ”»å‡»ç­–ç•¥ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”ä»¤ç‰ŒåŠ æƒæœºåˆ¶ä»¥æå‡é‡æ„æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVLMåœ¨å¤šç§æ•°æ®é›†ä¸Šå‡å­˜åœ¨è®­ç»ƒæ•°æ®æ³„éœ²é£é™©ï¼Œé‡æ„å›¾åƒçš„æ”»å‡»å‡†ç¡®ç‡ä¸º61.21%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡å‹åæ¼”ï¼ˆMIï¼‰æ”»å‡»é€šè¿‡é‡æ„è®­ç»ƒæ•°æ®å¯¹éšç§æ„æˆé‡å¤§é£é™©ã€‚å°½ç®¡ä»¥å¾€ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•æ¨¡æ€æ·±åº¦ç½‘ç»œä¸Šï¼Œä½†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è„†å¼±æ€§ä»æœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿç ”ç©¶äº†VLMçš„MIæ”»å‡»ï¼Œæå‡ºäº†ä¸€å¥—åŸºäºä»¤ç‰Œå’Œåºåˆ—çš„åæ¼”ç­–ç•¥ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”ä»¤ç‰ŒåŠ æƒçš„åºåˆ—æ¨¡å‹åæ¼”ï¼ˆSMI-AWï¼‰ï¼Œä»¥åŠ¨æ€è°ƒæ•´æ¯ä¸ªä»¤ç‰Œçš„æŸå¤±æ¢¯åº¦ï¼Œèšç„¦äºè§†è§‰ä¿¡æ¯ä¸°å¯Œçš„ä»¤ç‰Œï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼ç§å¯†å›¾åƒçš„é‡æ„ã€‚å®éªŒè¡¨æ˜ï¼ŒVLMå¯¹è®­ç»ƒæ•°æ®æ³„éœ²å­˜åœ¨æ˜¾è‘—è„†å¼±æ€§ï¼Œé‡æ„å›¾åƒçš„æ”»å‡»å‡†ç¡®ç‡è¾¾åˆ°61.21%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æ¨¡å‹åæ¼”æ”»å‡»ä¸­çš„éšç§æ³„éœ²é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹å•æ¨¡æ€ç½‘ç»œï¼Œæœªèƒ½æœ‰æ•ˆè¯„ä¼°VLMçš„è„†å¼±æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºä»¤ç‰Œçš„æ¨¡å‹åæ¼”ç­–ç•¥ï¼Œç»“åˆè‡ªé€‚åº”ä»¤ç‰ŒåŠ æƒæœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´æ¯ä¸ªä»¤ç‰Œçš„æŸå¤±æ¢¯åº¦ï¼Œä»¥èšç„¦äºè§†è§‰ä¿¡æ¯ä¸°å¯Œçš„ä»¤ç‰Œï¼Œä»è€Œæé«˜é‡æ„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ä»¤ç‰Œç”Ÿæˆã€æŸå¤±è®¡ç®—å’Œå›¾åƒé‡æ„å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡VLMç”Ÿæˆä»¤ç‰Œï¼Œç„¶åè®¡ç®—æ¯ä¸ªä»¤ç‰Œçš„æŸå¤±ï¼Œæœ€åè¿›è¡Œå›¾åƒé‡æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥è‡ªé€‚åº”ä»¤ç‰ŒåŠ æƒæœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªä»¤ç‰Œçš„è§†è§‰åŸºç¡€åŠ¨æ€è°ƒæ•´å…¶é‡è¦æ€§ã€‚è¿™ä¸€åˆ›æ–°æ˜¾è‘—æå‡äº†é‡æ„å›¾åƒçš„è´¨é‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºè§†è§‰åŸºç¡€çš„åŠ¨æ€åŠ æƒç­–ç•¥ï¼Œç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æ›´åŠ å…³æ³¨é‚£äº›å¯¹å›¾åƒé‡æ„è´¡çŒ®è¾ƒå¤§çš„ä»¤ç‰Œã€‚åŒæ—¶ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†é€‚åˆVLMçš„ç”Ÿæˆæ¨¡å‹æ¶æ„ï¼Œä»¥æé«˜åæ¼”æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šç§æ•°æ®é›†ä¸Šå‡å­˜åœ¨æ˜¾è‘—çš„è®­ç»ƒæ•°æ®æ³„éœ²é£é™©ï¼Œé‡æ„å›¾åƒçš„æ”»å‡»å‡†ç¡®ç‡è¾¾åˆ°61.21%ã€‚è¿™ä¸€ç»“æœå¼ºè°ƒäº†VLMåœ¨å®é™…åº”ç”¨ä¸­çš„éšç§é£é™©ï¼Œå°¤å…¶æ˜¯åœ¨æ•æ„Ÿé¢†åŸŸçš„åº”ç”¨åœºæ™¯ä¸­ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èç­‰æ•æ„Ÿé¢†åŸŸï¼Œéšç€è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ï¼Œä¿æŠ¤ç”¨æˆ·éšç§å˜å¾—å°¤ä¸ºé‡è¦ã€‚é€šè¿‡è¯†åˆ«å’Œç¼“è§£VLMçš„éšç§é£é™©ï¼Œå¯ä»¥ä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´å®‰å…¨çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿ç”¨æˆ·æ•°æ®çš„å®‰å…¨æ€§å’Œéšç§æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model inversion (MI) attacks pose significant privacy risks by reconstructing private training data from trained neural networks. While prior studies have primarily examined unimodal deep networks, the vulnerability of vision-language models (VLMs) remains largely unexplored. In this work, we present the first systematic study of MI attacks on VLMs to understand their susceptibility to leaking private visual training data. Our work makes two main contributions. First, tailored to the token-generative nature of VLMs, we introduce a suite of token-based and sequence-based model inversion strategies, providing a comprehensive analysis of VLMs' vulnerability under different attack formulations. Second, based on the observation that tokens vary in their visual grounding, and hence their gradients differ in informativeness for image reconstruction, we propose Sequence-based Model Inversion with Adaptive Token Weighting (SMI-AW) as a novel MI for VLMs. SMI-AW dynamically reweights each token's loss gradient according to its visual grounding, enabling the optimization to focus on visually informative tokens and more effectively guide the reconstruction of private images. Through extensive experiments and human evaluations on a range of state-of-the-art VLMs across multiple datasets, we show that VLMs are susceptible to training data leakage. Human evaluation of the reconstructed images yields an attack accuracy of 61.21%, underscoring the severity of these privacy risks. Notably, we demonstrate that publicly released VLMs are vulnerable to such attacks. Our study highlights the urgent need for privacy safeguards as VLMs become increasingly deployed in sensitive domains such as healthcare and finance. Additional experiments are provided in Supp.

