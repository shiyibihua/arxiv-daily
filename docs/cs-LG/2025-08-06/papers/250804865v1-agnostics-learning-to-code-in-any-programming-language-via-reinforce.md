---
layout: default
title: Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment
---

# Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04865" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04865v1</a>
  <a href="https://arxiv.org/pdf/2508.04865.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04865v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04865v1', 'Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aleksander Boruch-Gruszecki, Yangtian Zi, Zixuan Wu, Tejas Oberoi, Carolyn Jane Anderson, Joydeep Biswas, Arjun Guha

**åˆ†ç±»**: cs.LG, cs.PL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: 18 pages, 19 figures. For artifacts, see https://agnostics.abgru.me

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAgnosticsä»¥è§£å†³ä½èµ„æºç¼–ç¨‹è¯­è¨€çš„åè®­ç»ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½èµ„æºç¼–ç¨‹è¯­è¨€` `åè®­ç»ƒ` `å¼ºåŒ–å­¦ä¹ ` `è¯­è¨€æ— å…³` `æ¨¡å‹æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºç¼–ç¨‹è¯­è¨€ä¸Šè¡¨ç°ä¸ä½³ï¼Œç¼ºä¹è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®å’Œåè®­ç»ƒåŸºç¡€è®¾æ–½ã€‚
2. Agnosticsé€šè¿‡è¯­è¨€æ— å…³çš„åè®­ç»ƒç®¡é“ï¼Œåˆ©ç”¨å¤–éƒ¨å¯è§‚å¯Ÿè¡Œä¸ºè¯„ä¼°ä»£ç ï¼Œç®€åŒ–äº†å¤šè¯­è¨€æ”¯æŒçš„å·¥ç¨‹éœ€æ±‚ã€‚
3. åœ¨äº”ç§ä½èµ„æºè¯­è¨€ä¸Šï¼ŒAgnosticsæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè¾¾åˆ°äº†æ–°çš„çŠ¶æ€-of-the-artç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜èµ„æºè¯­è¨€ï¼ˆå¦‚Pythonå’ŒJavaScriptï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€ä¸Šå´é¢ä¸´æŒ‘æˆ˜ã€‚é™¤äº†é¢„è®­ç»ƒæ•°æ®çš„çŸ­ç¼ºï¼Œåè®­ç»ƒè¿‡ç¨‹ä¹Ÿæˆä¸ºç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºAgnosticsï¼Œä¸€ä¸ªè¯­è¨€æ— å…³çš„åè®­ç»ƒç®¡é“ï¼Œé€šè¿‡ä»…ä¾æ®ä»£ç çš„å¤–éƒ¨å¯è§‚å¯Ÿè¡Œä¸ºæ¥è¯„ä¼°ä»£ç ï¼Œä»è€Œæ¶ˆé™¤æ¯ç§è¯­è¨€çš„å·¥ç¨‹éœ€æ±‚ã€‚å…·ä½“è€Œè¨€ï¼ŒAgnosticsåˆ©ç”¨LLMå°†ç°æœ‰å•å…ƒæµ‹è¯•æ•°æ®é›†é‡å†™ä¸ºI/Oæ ¼å¼ï¼Œæä¾›ç®€çŸ­é…ç½®ä»¥æŒ‡å¯¼ç¼–è¯‘å’Œè¿è¡Œç›®æ ‡è¯­è¨€ï¼Œå¹¶åœ¨ç¨³å¥çš„ä»£ç æ‰§è¡Œç¯å¢ƒä¸­åº”ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚è¯¥æ–¹æ³•åœ¨äº”ç§ä½èµ„æºè¯­è¨€ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œæ¨åŠ¨äº†æ¨¡å‹æ€§èƒ½çš„è¿›æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºç¼–ç¨‹è¯­è¨€ä¸Šçš„åè®­ç»ƒç“¶é¢ˆï¼Œç°æœ‰æ–¹æ³•éœ€è¦ä¸ºæ¯ç§è¯­è¨€æ„å»ºæ–°çš„æ•°æ®é›†å’Œæµ‹è¯•æ¡†æ¶ï¼Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAgnosticsçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤–éƒ¨å¯è§‚å¯Ÿè¡Œä¸ºæ¥è¯„ä¼°ä»£ç ï¼Œé¿å…äº†æ¯ç§è¯­è¨€çš„ç‰¹å®šå·¥ç¨‹éœ€æ±‚ï¼Œä»è€Œå®ç°è¯­è¨€æ— å…³çš„åè®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAgnosticsçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨LLMå°†ç°æœ‰çš„å•å…ƒæµ‹è¯•æ•°æ®é›†è½¬æ¢ä¸ºI/Oæ ¼å¼ï¼›å…¶æ¬¡ï¼Œæä¾›ç®€çŸ­çš„é…ç½®æ–‡ä»¶ä»¥æŒ‡å¯¼å¦‚ä½•ç¼–è¯‘å’Œè¿è¡Œç›®æ ‡è¯­è¨€ï¼›æœ€åï¼Œåœ¨ç¨³å¥çš„ä»£ç æ‰§è¡Œç¯å¢ƒä¸­åº”ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šAgnosticsçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶è¯­è¨€æ— å…³çš„åè®­ç»ƒç®¡é“ï¼Œèƒ½å¤Ÿé€šè¿‡å•ä¸€éªŒè¯å™¨æµ‹è¯•ä»»ä½•è¯­è¨€çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†å·¥ç¨‹å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒAgnosticsä½¿ç”¨äº†ç®€çŸ­çš„YAMLé…ç½®æ–‡ä»¶æ¥æŒ‡å¯¼è®­ç»ƒè¿‡ç¨‹ï¼Œç¡®ä¿äº†çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼ŒåŒæ—¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸­é‡‡ç”¨äº†å¯éªŒè¯å¥–åŠ±æœºåˆ¶ï¼Œä»¥æé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨äº”ç§ä½èµ„æºè¯­è¨€ï¼ˆLuaã€Juliaã€Rã€OCamlå’ŒFortranï¼‰ä¸Šï¼ŒAgnosticsæ˜¾è‘—æå‡äº†Qwen-3 4Bæ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶ä¸å…¶ä»–16B-70Bå¼€æºæ¨¡å‹ç›¸åª²ç¾ã€‚æ­¤å¤–ï¼Œå¯¹äºâ‰¤16Bå‚æ•°æ¨¡å‹ï¼ŒAgnosticsåœ¨MultiPL-Eå’Œæ–°å¼•å…¥çš„å¤šè¯­è¨€ç‰ˆæœ¬LiveCodeBenchä¸Šè®¾å®šäº†æ–°çš„state-of-the-arté€šè¿‡ç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Agnosticsçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸä¸­ï¼Œèƒ½å¤Ÿæ”¯æŒå¤šç§ä½èµ„æºç¼–ç¨‹è¯­è¨€çš„å¼€å‘å’Œæµ‹è¯•ã€‚é€šè¿‡ç®€åŒ–åè®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•å¯ä»¥åŠ é€Ÿæ–°è¯­è¨€çš„æ¨¡å‹é€‚åº”ï¼Œä¿ƒè¿›è·¨è¯­è¨€ç¼–ç¨‹çš„ç ”ç©¶ä¸å®è·µã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) already excel at writing code in high-resource languages such as Python and JavaScript, yet stumble on low-resource languages that remain essential to science and engineering. Besides the obvious shortage of pre-training data, post-training itself is a bottleneck: every new language seems to require new datasets, test harnesses, and reinforcement-learning (RL) infrastructure.
>   We introduce Agnostics, a language-agnostic post-training pipeline that eliminates this per-language engineering. The key idea is to judge code solely by its externally observable behavior, so a single verifier can test solutions written in any language. Concretely, we (i) use an LLM to rewrite existing unit-test datasets into an I/O format, (ii) supply a short configuration that tells the verifier how to compile and run a target language, and (iii) apply reinforcement learning with verifiable rewards (RLVR) in a robust code execution environment.
>   Applied to five low-resource languages--Lua, Julia, R, OCaml, and Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other 16B-70B open-weight models; (2) scales cleanly to larger and diverse model families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for ${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
>   We will release the language-agnostic training datasets (Ag-MBPP-X, Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use configurations, making RL post-training in any programming language as simple as editing a short YAML file.

