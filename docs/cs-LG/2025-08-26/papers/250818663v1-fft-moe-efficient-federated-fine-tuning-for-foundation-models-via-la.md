---
layout: default
title: FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge
---

# FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18663" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18663v1</a>
  <a href="https://arxiv.org/pdf/2508.18663.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18663v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18663v1', 'FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gang Hu, Yinglei Teng, Pengfei Wu, Nan Wang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

**å¤‡æ³¨**: 9 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFFT-MoEä»¥è§£å†³å¼‚æ„è¾¹ç¼˜ç¯å¢ƒä¸‹çš„è”é‚¦å¾®è°ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `ç¨€ç–ä¸“å®¶æ··åˆ` `æ¨¡å‹å¾®è°ƒ` `å¼‚æ„ç¯å¢ƒ` `éIIDæ•°æ®` `éšç§ä¿æŠ¤` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LoRA-based FFTæ–¹æ³•åœ¨å¼‚æ„FLç¯å¢ƒä¸­å­˜åœ¨ç»“æ„ä¸å…¼å®¹å’Œå¯¹éIIDæ•°æ®é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå½±å“äº†æ¨¡å‹çš„æ”¶æ•›å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºFFT-MoEæ¡†æ¶ï¼Œé€šè¿‡ç¨€ç–Mixture of Expertsé€‚é…å™¨æ›¿ä»£LoRAï¼Œå…è®¸å®¢æˆ·ç«¯æ ¹æ®æœ¬åœ°èµ„æºåŠ¨æ€é€‰æ‹©ä¸“å®¶ï¼Œå®ç°ä¸ªæ€§åŒ–å¾®è°ƒã€‚
3. åœ¨å¤§é‡çš„å®éªŒä¸­ï¼ŒFFT-MoEåœ¨IIDå’ŒéIIDæ¡ä»¶ä¸‹å‡è¡¨ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„FFTåŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰æ¨åŠ¨äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„è¿›æ­¥ï¼Œåœ¨éšç§å’Œèµ„æºé™åˆ¶ä¸‹å¯¹å…¶è¿›è¡Œå¾®è°ƒå˜å¾—æ„ˆåŠ é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨é«˜è´¨é‡è®­ç»ƒæ•°æ®åˆ†å¸ƒäºè¾¹ç¼˜è®¾å¤‡æ—¶ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰é€šè¿‡è”é‚¦å¾®è°ƒï¼ˆFFTï¼‰æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…è®¸åœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæ¨¡å‹åä½œé€‚åº”ã€‚ç°æœ‰çš„åŸºäºLoRAçš„FFTæ–¹æ³•åœ¨å¼‚æ„FLç¯å¢ƒä¸­é¢ä¸´ç»“æ„ä¸å…¼å®¹å’Œå¯¹éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®é€‚åº”æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FFT-MoEæ¡†æ¶ï¼Œé€šè¿‡ç¨€ç–ä¸“å®¶æ··åˆï¼ˆMoEï¼‰é€‚é…å™¨æ›¿ä»£LoRAï¼Œä½¿æ¯ä¸ªå®¢æˆ·ç«¯èƒ½å¤Ÿè®­ç»ƒè½»é‡çº§çš„é—¨æ§ç½‘ç»œï¼Œé€‰æ‹©æ€§æ¿€æ´»ä¸ªæ€§åŒ–çš„ä¸“å®¶å­é›†ï¼Œä»è€Œå®ç°å¯¹æœ¬åœ°èµ„æºé¢„ç®—çš„ç»†ç²’åº¦é€‚åº”ï¼ŒåŒæ—¶ä¿æŒèšåˆå…¼å®¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFFT-MoEåœ¨æ³›åŒ–æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„FFTåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¼‚æ„è¾¹ç¼˜ç¯å¢ƒä¸‹è¿›è¡Œè”é‚¦å¾®è°ƒæ—¶ï¼ŒLoRAæ–¹æ³•çš„ç»“æ„ä¸å…¼å®¹æ€§å’Œå¯¹éIIDæ•°æ®çš„é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚è¿™äº›é—®é¢˜å¯¼è‡´æ¨¡å‹æ”¶æ•›å›°éš¾å’Œæ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºFFT-MoEæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç¨€ç–Mixture of Expertsé€‚é…å™¨ï¼Œå…è®¸æ¯ä¸ªå®¢æˆ·ç«¯æ ¹æ®æœ¬åœ°èµ„æºåŠ¨æ€é€‰æ‹©æ¿€æ´»çš„ä¸“å®¶ï¼Œä»è€Œå®ç°ä¸ªæ€§åŒ–çš„å¾®è°ƒå’Œæ›´å¥½çš„èšåˆå…¼å®¹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFFT-MoEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å®¢æˆ·ç«¯çš„è½»é‡çº§é—¨æ§ç½‘ç»œã€ç¨€ç–ä¸“å®¶é€‚é…å™¨å’Œå¼‚æ„æ€§æ„ŸçŸ¥è¾…åŠ©æŸå¤±å‡½æ•°ã€‚å®¢æˆ·ç«¯é€šè¿‡é—¨æ§ç½‘ç»œé€‰æ‹©æ¿€æ´»çš„ä¸“å®¶ï¼Œé€‚é…å™¨åˆ™è´Ÿè´£å¤„ç†å…·ä½“çš„ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç”¨ç¨€ç–MoEé€‚é…å™¨æ›¿ä»£LoRAï¼Œè§£å†³äº†ç»“æ„ä¸å…¼å®¹å’ŒéIIDæ•°æ®é€‚åº”æ€§çš„é—®é¢˜ï¼ŒåŒæ—¶å¼•å…¥çš„è¾…åŠ©æŸå¤±å‡½æ•°ç¡®ä¿äº†ä¸“å®¶çš„å¤šæ ·æ€§å’Œå¹³è¡¡åˆ©ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé—¨æ§ç½‘ç»œçš„ç»“æ„è½»é‡åŒ–ä»¥é€‚åº”è¾¹ç¼˜è®¾å¤‡çš„èµ„æºé™åˆ¶ï¼ŒæŸå¤±å‡½æ•°åŠ¨æ€è°ƒæ•´è·¯ç”±åˆ†å¸ƒï¼Œä»¥å®ç°ä¸“å®¶çš„å‡è¡¡åˆ©ç”¨å’Œå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFFT-MoEåœ¨æ³›åŒ–æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„FFTåŸºçº¿ï¼Œå°¤å…¶åœ¨éIIDæ¡ä»¶ä¸‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œå±•ç¤ºäº†å…¶åœ¨å¼‚æ„ç¯å¢ƒä¸‹çš„å¼ºå¤§é€‚åº”èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æ‰‹æœºã€ç‰©è”ç½‘è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—å¹³å°ç­‰ï¼Œèƒ½å¤Ÿåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶å®ç°é«˜æ•ˆçš„æ¨¡å‹å¾®è°ƒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šåŸºäºè”é‚¦å­¦ä¹ çš„æ™ºèƒ½åº”ç”¨ï¼Œæå‡æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„è¡¨ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As FMs drive progress toward Artificial General Intelligence (AGI), fine-tuning them under privacy and resource constraints has become increasingly critical particularly when highquality training data resides on distributed edge devices. Federated Learning (FL) offers a compelling solution through Federated Fine-Tuning (FFT), which enables collaborative model adaptation without sharing raw data. Recent approaches incorporate Parameter-Efficient Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce computational overhead. However, LoRA-based FFT faces two major limitations in heterogeneous FL environments: structural incompatibility across clients with varying LoRA configurations and limited adaptability to non-IID data distributions, which hinders convergence and generalization. To address these challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight gating network to selectively activate a personalized subset of experts, enabling fine-grained adaptation to local resource budgets while preserving aggregation compatibility. To further combat the expert load imbalance caused by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary loss that dynamically regularizes the routing distribution to ensure expert diversity and balanced utilization. Extensive experiments spanning both IID and non-IID conditions demonstrate that FFT MoE consistently outperforms state of the art FFT baselines in generalization performance and training efficiency.

