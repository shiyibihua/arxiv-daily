---
layout: default
title: Re:Frame -- Retrieving Experience From Associative Memory
---

# Re:Frame -- Retrieving Experience From Associative Memory

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19344" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19344v1</a>
  <a href="https://arxiv.org/pdf/2508.19344.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19344v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19344v1', 'Re:Frame -- Retrieving Experience From Associative Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daniil Zelezetsky, Egor Cherepanov, Alexey K. Kovalev, Aleksandr I. Panov

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

**å¤‡æ³¨**: 11 pages, 3 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRe:Frameä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸“å®¶æ•°æ®ç¨€ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `ä¸“å®¶æ•°æ®` `å…³è”è®°å¿†` `å†³ç­–å˜æ¢å™¨` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥ä»ä½è´¨é‡æ•°æ®ä¸­æœ‰æ•ˆå­¦ä¹ ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. Re:Frameé€šè¿‡å¼•å…¥å¤–éƒ¨çš„å…³è”è®°å¿†ç¼“å†²åŒºï¼Œå…è®¸ç­–ç•¥åœ¨è®­ç»ƒå’Œè¯„ä¼°é˜¶æ®µæ£€ç´¢ä¸“å®¶è½¨è¿¹ï¼Œä»è€Œæå‡å†³ç­–è´¨é‡ã€‚
3. åœ¨D4RL MuJoCoä»»åŠ¡ä¸­ï¼Œä½¿ç”¨å°‘é‡ä¸“å®¶æ•°æ®ï¼ŒRe:Frameåœ¨å¤šä¸ªè®¾ç½®ä¸Šè¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šå¸¸é¢ä¸´æ”¶é›†å¤§å‹ä¸“å®¶æ•°æ®é›†çš„å›°éš¾ï¼Œå¯¼è‡´ä»£ç†åœ¨å­¦ä¹ æ—¶åªèƒ½ä¾èµ–äºä¸å®Œç¾æˆ–ä¸ä¸€è‡´çš„è½¨è¿¹ã€‚æœ¬æ–‡æå‡ºRe:Frameï¼ˆä»å…³è”è®°å¿†ä¸­æ£€ç´¢ç»éªŒï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ’ä»¶æ¨¡å—ï¼Œèƒ½å¤Ÿå°†å°‘é‡ä¸“å®¶è½¨è¿¹ä¸æ ‡å‡†ç¦»çº¿RLç­–ç•¥ï¼ˆå¦‚å†³ç­–å˜æ¢å™¨ï¼‰ç»“åˆã€‚åœ¨ä½è´¨é‡æ•°æ®çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç­–ç•¥é€šè¿‡å†…å®¹å…³è”ä»å¤–éƒ¨çš„å…³è”è®°å¿†ç¼“å†²åŒºï¼ˆAMBï¼‰æ£€ç´¢ä¸“å®¶æ•°æ®ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°å†³ç­–ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ä»…60æ¡ä¸“å®¶è½¨è¿¹ï¼ŒRe:Frameåœ¨D4RL MuJoCoä»»åŠ¡ä¸­åœ¨ä¸‰ä¸ªè®¾ç½®ä¸Šå‡æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œæœ€é«˜æå‡è¾¾10.7ä¸ªæ ‡å‡†åŒ–ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ä¸“å®¶æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç¼ºä¹é«˜è´¨é‡ä¸“å®¶æ•°æ®æ—¶ï¼Œä»£ç†çš„å­¦ä¹ æ•ˆæœå—åˆ°é™åˆ¶ï¼Œéš¾ä»¥å®ç°é«˜æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRe:Frameçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ä¸€ä¸ªå°å‹çš„å…³è”è®°å¿†ç¼“å†²åŒºï¼ˆAMBï¼‰ï¼Œä½¿å¾—ç­–ç•¥èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ£€ç´¢å¹¶åˆ©ç”¨å°‘é‡ä¸“å®¶ç»éªŒï¼Œä»è€Œæ”¹å–„å†³ç­–è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRe:Frameçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªæ ‡å‡†çš„ç¦»çº¿RLç­–ç•¥ï¼ˆå¦‚å†³ç­–å˜æ¢å™¨ï¼‰å’Œä¸€ä¸ªå¤–éƒ¨çš„AMBã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œç­–ç•¥é€šè¿‡å†…å®¹å…³è”ä»AMBä¸­æ£€ç´¢ä¸“å®¶è½¨è¿¹ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°å†³ç­–ä¸­ï¼›åœ¨è¯„ä¼°é˜¶æ®µï¼ŒAMBåŒæ ·è¢«æŸ¥è¯¢ä»¥å¢å¼ºå†³ç­–è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šRe:Frameçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— éœ€ä¸ç¯å¢ƒäº¤äº’ï¼Œä¸”ä¸éœ€è¦å¯¹åŸºç¡€æ¶æ„è¿›è¡Œä¿®æ”¹ï¼Œå°±èƒ½æœ‰æ•ˆåˆ©ç”¨ç¨€ç¼ºçš„ä¸“å®¶çŸ¥è¯†ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒRe:Frameè®¾å®šäº†AMBçš„å¤§å°å’Œä¸“å®¶è½¨è¿¹çš„æ•°é‡ï¼Œå®éªŒä¸­ä½¿ç”¨äº†ä»…60æ¡ä¸“å®¶è½¨è¿¹ï¼ˆå 6000æ¡æ•°æ®é›†çš„0.1%ï¼‰ï¼Œå¹¶é€šè¿‡å†…å®¹å…³è”æœºåˆ¶å®ç°äº†é«˜æ•ˆçš„æ•°æ®æ£€ç´¢ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨D4RL MuJoCoä»»åŠ¡ä¸­ï¼ŒRe:Frameåœ¨ä¸‰ä¸ªè®¾ç½®ä¸Šå‡è¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œä½¿ç”¨ä»…60æ¡ä¸“å®¶è½¨è¿¹å®ç°äº†æœ€é«˜10.7ä¸ªæ ‡å‡†åŒ–ç‚¹çš„æ€§èƒ½æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨ä½è´¨é‡æ•°æ®ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§å’Œæ•°æ®æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Re:Frameçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œæ¸¸æˆAIç­‰éœ€è¦é«˜æ•ˆå­¦ä¹ çš„åœºæ™¯ä¸­ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨æœ‰é™çš„ä¸“å®¶æ•°æ®ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡æ™ºèƒ½ä½“çš„å­¦ä¹ æ•ˆç‡å’Œå†³ç­–èƒ½åŠ›ï¼Œæ¨åŠ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ æŠ€æœ¯çš„å®é™…åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline reinforcement learning (RL) often deals with suboptimal data when collecting large expert datasets is unavailable or impractical. This limitation makes it difficult for agents to generalize and achieve high performance, as they must learn primarily from imperfect or inconsistent trajectories. A central challenge is therefore how to best leverage scarce expert demonstrations alongside abundant but lower-quality data. We demonstrate that incorporating even a tiny amount of expert experience can substantially improve RL agent performance. We introduce Re:Frame (Retrieving Experience From Associative Memory), a plug-in module that augments a standard offline RL policy (e.g., Decision Transformer) with a small external Associative Memory Buffer (AMB) populated by expert trajectories drawn from a separate dataset. During training on low-quality data, the policy learns to retrieve expert data from the Associative Memory Buffer (AMB) via content-based associations and integrate them into decision-making; the same AMB is queried at evaluation. This requires no environment interaction and no modifications to the backbone architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories (0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a strong Decision Transformer baseline in three of four settings, with gains up to +10.7 normalized points. These results show that Re:Frame offers a simple and data-efficient way to inject scarce expert knowledge and substantially improve offline RL from low-quality datasets.

