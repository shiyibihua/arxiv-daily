---
layout: default
title: Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding
---

# Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18676" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18676v1</a>
  <a href="https://arxiv.org/pdf/2508.18676.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18676v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18676v1', 'Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chufan Gao, Jintai Chen, Jimeng Sun

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLRTabä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡¨æ ¼ç†è§£ä¸­çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¡¨æ ¼ç†è§£` `æ¨ç†èƒ½åŠ›` `é“¾å¼æ€ç»´` `æç¤ºæ¡ä»¶` `æ•°æ®ç§‘å­¦` `è‡ªåŠ¨åŒ–åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¡¨æ ¼æ¨ç†ä¸­å­˜åœ¨å¾®è°ƒä¸æ— è®­ç»ƒæç¤ºçš„æƒè¡¡ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³æˆ–æœªå……åˆ†åˆ©ç”¨è®­ç»ƒæ•°æ®ã€‚
2. è®ºæ–‡æå‡ºLRTabæ–¹æ³•ï¼Œé€šè¿‡æç¤ºè·å–CoTå“åº”ï¼Œå¹¶ä»ä¸­å­¦ä¹ æç¤ºæ¡ä»¶ä»¥é¿å…é”™è¯¯ï¼Œæå‡æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLRTabåœ¨WikiTQå’ŒTabfactæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œå…·æœ‰è¾ƒé«˜çš„å¯è§£é‡Šæ€§å’Œæˆæœ¬æ•ˆç›Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨åŒ–çš„è¡¨æ ¼ç†è§£å’Œæ¨ç†æ˜¯æ•°æ®ç§‘å­¦å®¶çš„é‡è¦ä»»åŠ¡ã€‚è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¡¨æ ¼æ¨ç†ä»»åŠ¡ä¸­å˜å¾—è¶Šæ¥è¶Šæ™®éã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸¤æ–¹é¢ï¼šä¸€æ˜¯ä½¿ç”¨æ ‡æ³¨æ•°æ®å¯¹LLMsè¿›è¡Œå¾®è°ƒï¼ŒäºŒæ˜¯åˆ©ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¿›è¡Œæ— è®­ç»ƒæç¤ºã€‚å¾®è°ƒè™½ç„¶èƒ½å®ç°æ•°æ®é›†ç‰¹å®šçš„å­¦ä¹ ï¼Œä½†ç‰ºç‰²äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›è€Œæ— è®­ç»ƒæç¤ºåˆ™å…·æœ‰é«˜åº¦çš„æ³›åŒ–æ€§ï¼Œä½†æœªèƒ½å……åˆ†åˆ©ç”¨è®­ç»ƒæ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæç¤ºçš„æ¨ç†æ–¹æ³•LRTabï¼Œé€šè¿‡ä»è®­ç»ƒæ•°æ®ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç»“åˆäº†ä¸¤è€…çš„ä¼˜ç‚¹ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡æç¤ºè·å–è®­ç»ƒæ•°æ®ä¸Šçš„CoTå“åº”ï¼Œå¯¹äºé”™è¯¯çš„CoTï¼Œæˆ‘ä»¬æç¤ºLLMé¢„æµ‹æç¤ºæ¡ä»¶ä»¥é¿å…é”™è¯¯ï¼Œä»ä¸­å­¦ä¹ æ•°æ®çš„æ´å¯Ÿã€‚æˆ‘ä»¬åœ¨éªŒè¯æ•°æ®ä¸ŠéªŒè¯äº†æç¤ºæ¡ä»¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨æ¨ç†æ—¶æ£€ç´¢æœ€ç›¸å…³çš„æç¤ºæ¡ä»¶ä»¥æä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLRTabåœ¨è¡¨æ ¼æ¨ç†ä¸­å…·æœ‰å¯è§£é‡Šæ€§ã€æˆæœ¬æ•ˆç›Šï¼Œå¹¶ä¸”ä¼˜äºä¹‹å‰çš„åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡¨æ ¼ç†è§£å’Œæ¨ç†ä¸­çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯å¾®è°ƒä¸æ— è®­ç»ƒæç¤ºæ–¹æ³•çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å…¼é¡¾ç‰¹å®šæ•°æ®é›†çš„å­¦ä¹ ä¸æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLRTabé€šè¿‡ç»“åˆæç¤ºä¸æ£€ç´¢æœºåˆ¶ï¼Œåˆ©ç”¨è®­ç»ƒæ•°æ®ä¸­çš„ä¿¡æ¯æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œé¦–å…ˆé€šè¿‡æç¤ºè·å–CoTå“åº”ï¼Œç„¶åå­¦ä¹ å¦‚ä½•é¿å…é”™è¯¯çš„æç¤ºæ¡ä»¶ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLRTabçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯æç¤ºç”Ÿæˆæ¨¡å—ï¼Œç”¨äºä»è®­ç»ƒæ•°æ®ä¸­ç”ŸæˆCoTå“åº”ï¼›å…¶æ¬¡æ˜¯é”™è¯¯æ£€æµ‹ä¸æç¤ºæ¡ä»¶å­¦ä¹ æ¨¡å—ï¼Œé’ˆå¯¹é”™è¯¯çš„CoTè¿›è¡Œåˆ†æï¼›æœ€åæ˜¯æ¨ç†é˜¶æ®µï¼Œåœ¨æ­¤é˜¶æ®µæ£€ç´¢æœ€ç›¸å…³çš„æç¤ºæ¡ä»¶ä»¥è¾…åŠ©è¡¨æ ¼ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šLRTabçš„åˆ›æ–°åœ¨äºå…¶é€šè¿‡æç¤ºæ¡ä»¶çš„å­¦ä¹ ä¸æ£€ç´¢æœºåˆ¶ï¼Œå¼¥è¡¥äº†ä¼ ç»Ÿå¾®è°ƒä¸æ— è®­ç»ƒæç¤ºæ–¹æ³•çš„ä¸è¶³ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæç¤ºæ¡ä»¶çš„ç”Ÿæˆä¸æ£€ç´¢æ˜¯å…³é”®ç¯èŠ‚ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æç¤ºæ¡ä»¶çš„å­¦ä¹ æ•ˆæœã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç»“æ„ä¸Šé‡‡ç”¨äº†é€‚åº”æ€§è°ƒæ•´çš„æœºåˆ¶ï¼Œä»¥ä¾¿åœ¨ä¸åŒæ•°æ®é›†ä¸Šå®ç°æ›´å¥½çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLRTabåœ¨WikiTQå’ŒTabfactæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ¨ç†å‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚æ­¤å¤–ï¼ŒLRTabåœ¨æˆæœ¬æ•ˆç›Šæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨è¾ƒä½çš„è®¡ç®—èµ„æºæ¶ˆè€—ä¸‹å®ç°é«˜æ•ˆæ¨ç†ï¼Œè¯æ˜äº†å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®åˆ†æã€å•†ä¸šæ™ºèƒ½å’Œè‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡¨æ ¼ç†è§£ä¸­çš„æ¨ç†èƒ½åŠ›ï¼ŒLRTabèƒ½å¤Ÿå¸®åŠ©æ•°æ®ç§‘å­¦å®¶æ›´é«˜æ•ˆåœ°ä»å¤æ‚æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œè¿›è€Œæ¨åŠ¨å†³ç­–æ”¯æŒç³»ç»Ÿçš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¹¿æ³›çš„é¢†åŸŸä¸­åº”ç”¨ï¼Œæå‡è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automated tabular understanding and reasoning are essential tasks for data scientists. Recently, Large language models (LLMs) have become increasingly prevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning LLMs using labeled data or (2) Training-free prompting LLM agents using chain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost of generalizability. Training-free prompting is highly generalizable but does not take full advantage of training data. In this paper, we propose a novel prompting-based reasoning approach, Learn then Retrieve: LRTab, which integrates the benefits of both by retrieving relevant information learned from training data. We first use prompting to obtain CoT responses over the training data. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to avoid the error, learning insights from the data. We validate the effectiveness of Prompt Conditions using validation data. Finally, at inference time, we retrieve the most relevant Prompt Conditions for additional context for table understanding. We provide comprehensive experiments on WikiTQ and Tabfact, showing that LRTab is interpretable, cost-efficient, and can outperform previous baselines in tabular reasoning.

