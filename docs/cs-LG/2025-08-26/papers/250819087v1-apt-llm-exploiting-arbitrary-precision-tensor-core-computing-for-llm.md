---
layout: default
title: APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration
---

# APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19087" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.19087v1</a>
  <a href="https://arxiv.org/pdf/2508.19087.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19087v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19087v1', 'APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.AR

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-26

**Â§áÊ≥®**: To appear in the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD)

**DOI**: [10.1109/TCAD.2025.3604321](https://doi.org/10.1109/TCAD.2025.3604321)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫APT-LLM‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂä†ÈÄüÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÈáèÂåñÊñπÊ≥ï` `GPUÂä†ÈÄü` `Áü©Èòµ‰πòÊ≥ï` `ÂÜÖÂ≠òÁÆ°ÁêÜ` `È´òÊïàËÆ°ÁÆó` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®GPU‰∏äÂØπË∂Ö‰ΩéÊØîÁâπÈáèÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊîØÊåÅÊúâÈôêÔºåÂØºËá¥ËÆ°ÁÆóÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. Êú¨ÊñáÊèêÂá∫APT-LLMÔºåÈÄöËøáÂºïÂÖ•bipolar-INTÊï∞ÊçÆÊ†ºÂºèÂíåÁÅµÊ¥ªÁöÑÁü©Èòµ‰πòÊ≥ïÊñπÊ≥ïÔºå‰ºòÂåñ‰∫ÜGPUÁöÑËÆ°ÁÆóÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåAPT-LLMÂú®‰∏çÂêåÁ°¨‰ª∂‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÂä†ÈÄüÔºåÊúÄÈ´òÂèØËææ3.99ÂÄçÔºåÁõ∏ËæÉ‰∫éFP16Âü∫Á∫ø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰∫∫Â∑•Êô∫ËÉΩÂ∫îÁî®‰∏≠ÂºïÂèë‰∫ÜÈù©ÂëΩÔºå‰ΩÜÂÖ∂Â∑®Â§ßÁöÑËÆ°ÁÆóÈúÄÊ±Ç‰∏•ÈáçÈôêÂà∂‰∫ÜÈÉ®ÁΩ≤ÂíåÂÆûÊó∂ÊÄßËÉΩ„ÄÇÈáèÂåñÊñπÊ≥ïÂèØ‰ª•Â∏ÆÂä©Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨Ôºå‰ΩÜÂú®GPU‰∏äÂÆûÁé∞Ë∂Ö‰ΩéÊØîÁâπÈáèÂåñLLMsÁöÑÊûÅÈ´òÊïàÁéáÈù¢‰∏¥ÊåëÊàò„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖ®Èù¢ÁöÑÂä†ÈÄüÊñπÊ°àAPT-LLMÔºåÈ¶ñÂÖàÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÂûãÊï∞ÊçÆÊ†ºÂºèbipolar-INTÔºåÊîØÊåÅÈ´òÊïàÊó†ÊçüËΩ¨Êç¢Âπ∂‰øÉËøõÂπ∂Ë°åËÆ°ÁÆó„ÄÇÂÖ∂Ê¨°ÔºåÂºÄÂèë‰∫Ü‰∏ÄÁßçÁü©Èòµ‰πòÊ≥ïÊñπÊ≥ïÔºåÈÄöËøáÈÄê‰ΩçÊãÜËß£ÂíåÈáçÁªÑÁü©ÈòµÂÆûÁé∞‰ªªÊÑèÁ≤æÂ∫¶Ôºå‰ºòÂåñGPU Tensor CoresÁöÑÂà©Áî®„ÄÇÊúÄÂêéÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏ìÊ≥®‰∫éÊï∞ÊçÆÊÅ¢Â§çÁöÑÂÜÖÂ≠òÁÆ°ÁêÜÁ≥ªÁªüÔºåÊòæËëóÊèêÈ´òÂÜÖÊ†∏ÊâßË°åÈÄüÂ∫¶Âπ∂ÂáèÂ∞ëÂÜÖÂ≠òËÆøÈóÆÂª∂Ëøü„ÄÇÂÆûÈ™åË°®ÊòéÔºåAPT-LLMÂú®RTX 3090‰∏äÁõ∏ËæÉ‰∫éFP16Âü∫Á∫øÂÆûÁé∞‰∫ÜÊúÄÈ´ò3.99ÂÄçÁöÑÂä†ÈÄü„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®GPU‰∏äÂä†ÈÄüÊó∂Èù¢‰∏¥ÁöÑËÆ°ÁÆóÊïàÁéá‰Ωé‰∏ãÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Ë∂Ö‰ΩéÊØîÁâπÈáèÂåñÁöÑÊîØÊåÅ‰∏äÂ≠òÂú®‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAPT-LLMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•Êñ∞ÂûãÊï∞ÊçÆÊ†ºÂºèÂíåÁÅµÊ¥ªÁöÑÁü©Èòµ‰πòÊ≥ïÊñπÊ≥ïÔºå‰ºòÂåñGPU Tensor CoresÁöÑÂà©Áî®ÁéáÔºå‰ªéËÄåÂÆûÁé∞‰ªªÊÑèÁ≤æÂ∫¶ÁöÑÈ´òÊïàËÆ°ÁÆó„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAPT-LLMÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÊ†ºÂºèËΩ¨Êç¢Ê®°Âùó„ÄÅÁü©Èòµ‰πòÊ≥ïÊ®°Âùó„ÄÅÂÜÖÂ≠òÁÆ°ÁêÜÁ≥ªÁªüÂíåÂÜÖÊ†∏Êò†Â∞ÑÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òËÆ°ÁÆóÊïàÁéáÂíåÁÅµÊ¥ªÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÊòØbipolar-INTÊï∞ÊçÆÊ†ºÂºèÂíåÈÄê‰ΩçÊãÜËß£ÁöÑÁü©Èòµ‰πòÊ≥ïÊñπÊ≥ïÔºåËøô‰∫õÂàõÊñ∞‰ΩøÂæóÂú®GPU‰∏äÂÆûÁé∞‰ªªÊÑèÁ≤æÂ∫¶ËÆ°ÁÆóÊàê‰∏∫ÂèØËÉΩÔºåÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÂø´ÈÄüÂÖ±‰∫´ÂÜÖÂ≠òËøõË°åÊï∞ÊçÆÊÅ¢Â§çÔºå‰ºòÂåñ‰∫ÜÂÜÖÊ†∏ÊâßË°åÈÄüÂ∫¶ÔºåÂπ∂ÈÄöËøáÂä®ÊÄÅÈÄâÊã©Ë∂ÖÂèÇÊï∞Êù•ÈÄÇÂ∫î‰∏çÂêåÁü©ÈòµÂ§ßÂ∞èÔºåÁ°Æ‰øùÂú®‰∏çÂêåLLMÊû∂ÊûÑÂíåÁ≤æÂ∫¶ËÆæÁΩÆ‰∏ãÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

APT-LLMÂú®RTX 3090‰∏äÂÆûÁé∞‰∫ÜÊúÄÈ´ò3.99ÂÄçÁöÑÂä†ÈÄüÔºåÁõ∏ËæÉ‰∫éFP16Âü∫Á∫øÔºå‰∏îÂú®RTX 4090ÂíåH800‰∏äÂàÜÂà´ËææÂà∞‰∫Ü2.44ÂÄçÂíå1.65ÂÄçÁöÑÂä†ÈÄüÔºåÂ±ïÁé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

APT-LLMÁöÑÁ†îÁ©∂ÊàêÊûúÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂Âú®ÈúÄË¶ÅÂÆûÊó∂Â§ÑÁêÜÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÔºåÂ¶ÇÂØπËØùÁ≥ªÁªü„ÄÅÊú∫Âô®ÁøªËØëÂíåÊñáÊú¨ÁîüÊàêÁ≠â„ÄÇÂÖ∂È´òÊïàÁöÑËÆ°ÁÆóËÉΩÂäõÂ∞ÜÊé®Âä®Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËæπÁºòËÆ°ÁÆóÂíåÁßªÂä®ËÆæÂ§á‰∏äÁöÑÂ∫îÁî®ÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) have revolutionized AI applications, yet their enormous computational demands severely limit deployment and real-time performance. Quantization methods can help reduce computational costs, however, attaining the extreme efficiency associated with ultra-low-bit quantized LLMs at arbitrary precision presents challenges on GPUs. This is primarily due to the limited support for GPU Tensor Cores, inefficient memory management, and inflexible kernel optimizations. To tackle these challenges, we propose a comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM. Firstly, we introduce a novel data format, bipolar-INT, which allows for efficient and lossless conversion with signed INT, while also being more conducive to parallel computation. We also develop a matrix multiplication (MatMul) method allowing for arbitrary precision by dismantling and reassembling matrices at the bit level. This method provides flexible precision and optimizes the utilization of GPU Tensor Cores. In addition, we propose a memory management system focused on data recovery, which strategically employs fast shared memory to substantially increase kernel execution speed and reduce memory access latency. Finally, we develop a kernel mapping method that dynamically selects the optimal configurable hyperparameters of kernels for varying matrix sizes, enabling optimal performance across different LLM architectures and precision settings. In LLM inference, APT-LLM achieves up to a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800, APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup over CUTLASS integer baselines.

