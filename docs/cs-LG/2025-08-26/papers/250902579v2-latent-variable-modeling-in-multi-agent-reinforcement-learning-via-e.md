---
layout: default
title: Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection
---

# Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02579" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02579v2</a>
  <a href="https://arxiv.org/pdf/2509.02579.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02579v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02579v2', 'Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mazyar Taghavi, Rahman Farnoosh

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-10-10)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæœŸæœ›æœ€å¤§åŒ–çš„æ½œå˜é‡å»ºæ¨¡ä»¥è§£å†³æ— äººæœºé‡ç”ŸåŠ¨ç‰©ä¿æŠ¤é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `æ— äººæœºåè°ƒ` `æ½œå˜é‡å»ºæ¨¡` `æœŸæœ›æœ€å¤§åŒ–` `é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤` `ç¯å¢ƒç›‘æµ‹` `å†³ç­–ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åœ¨å¤æ‚çš„ç¯å¢ƒä¸­ï¼Œç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åº”å¯¹ä¸ç¡®å®šæ€§å’Œåè°ƒæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæœŸæœ›æœ€å¤§åŒ–çš„æ½œå˜é‡å»ºæ¨¡æ–¹æ³•ï¼Œä»¥å¢å¼ºæ— äººæœºåœ¨é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ä¸­çš„æ¢ç´¢å’Œåè°ƒèƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEM-MARLæ¡†æ¶åœ¨æ£€æµ‹å‡†ç¡®æ€§å’Œæ”¿ç­–æ”¶æ•›æ€§ä¸Šä¼˜äºä¼ ç»Ÿç®—æ³•ï¼Œå…·æœ‰æ˜¾è‘—çš„é€‚åº”æ€§æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¿æŠ¤æ¿’å±é‡ç”ŸåŠ¨ç‰©å…å—éæ³•å·çŒæ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¹¿é˜”ä¸”éƒ¨åˆ†å¯è§‚æµ‹çš„ç¯å¢ƒä¸­ï¼Œå®æ—¶å“åº”è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰çš„æ½œå˜é‡å»ºæ¨¡æ–¹æ³•ï¼Œåº”ç”¨äºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­ï¼Œä»¥åè°ƒæ— äººæœºï¼ˆUAVï¼‰è¿›è¡Œé‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ã€‚é€šè¿‡æ½œå˜é‡å»ºæ¨¡éšè—çš„ç¯å¢ƒå› ç´ å’Œæ™ºèƒ½ä½“é—´çš„åŠ¨æ€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ç¡®å®šæ€§ä¸‹å¢å¼ºäº†æ¢ç´¢å’Œåè°ƒèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªè‡ªå®šä¹‰ä»¿çœŸä¸­å®ç°å¹¶è¯„ä¼°äº†EM-MARLæ¡†æ¶ï¼Œæ¶‰åŠ10æ¶æ— äººæœºè´Ÿè´£å·¡é€»æ¿’å±ä¼Šæœ—è±¹çš„ä¿æŠ¤æ –æ¯åœ°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡†ç®—æ³•å¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDDPGï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ£€æµ‹å‡†ç¡®æ€§ã€é€‚åº”æ€§å’Œç­–ç•¥æ”¶æ•›æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å°†EMæ¨æ–­ä¸MARLç»“åˆä»¥æ”¹å–„å¤æ‚é«˜é£é™©ä¿æŠ¤åœºæ™¯ä¸­çš„åˆ†æ•£å†³ç­–çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤æ‚å’Œéƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåè°ƒå¤šæ¶æ— äººæœºè¿›è¡Œé‡ç”ŸåŠ¨ç‰©ä¿æŠ¤çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç¯å¢ƒä¸ç¡®å®šæ€§å’Œæ™ºèƒ½ä½“é—´çš„åŠ¨æ€äº¤äº’æ—¶ï¼Œè¡¨ç°å‡ºè¾ƒå¤§çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ½œå˜é‡å»ºæ¨¡éšè—çš„ç¯å¢ƒå› ç´ å’Œæ™ºèƒ½ä½“é—´çš„åŠ¨æ€ï¼Œåˆ©ç”¨æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•æ¥å¢å¼ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¢ç´¢èƒ½åŠ›å’Œåè°ƒæ€§ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜åœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çš„å†³ç­–è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ½œå˜é‡å»ºæ¨¡ã€EMæ¨æ–­å’Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡æ½œå˜é‡æ•æ‰ç¯å¢ƒçš„éšå«ç‰¹å¾ï¼Œç„¶ååº”ç”¨EMç®—æ³•è¿›è¡Œæ¨æ–­ï¼Œæœ€ååœ¨MARLæ¡†æ¶ä¸­å®ç°æ™ºèƒ½ä½“çš„åè°ƒå†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†EMæ¨æ–­ä¸MARLç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„æ½œå˜é‡å»ºæ¨¡æ–¹æ³•ã€‚è¿™ä¸€æ–¹æ³•åœ¨å¤„ç†ç¯å¢ƒä¸ç¡®å®šæ€§å’Œæ™ºèƒ½ä½“é—´çš„åŠ¨æ€äº¤äº’æ–¹é¢ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬æ½œå˜é‡çš„ç»´åº¦ã€EMç®—æ³•çš„è¿­ä»£æ¬¡æ•°ä»¥åŠæ™ºèƒ½ä½“çš„å­¦ä¹ ç‡ç­‰ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ï¼Œä»¥ç¡®ä¿æ™ºèƒ½ä½“åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆé€‚åº”ç¯å¢ƒå˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEM-MARLæ¡†æ¶åœ¨æ£€æµ‹å‡†ç¡®æ€§ä¸Šæé«˜äº†çº¦20%ï¼Œåœ¨é€‚åº”æ€§å’Œæ”¿ç­–æ”¶æ•›æ€§æ–¹é¢ä¹Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿ç®—æ³•PPOå’ŒDDPGã€‚è¿™äº›ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤æ‚ä¿æŠ¤åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ã€ç¯å¢ƒç›‘æµ‹å’Œæ— äººæœºååŒä½œæˆ˜ç­‰ã€‚é€šè¿‡æé«˜æ— äººæœºåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å·çŒç­‰å¨èƒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•è‡³å…¶ä»–éœ€è¦å¤šæ™ºèƒ½ä½“åè°ƒçš„åœºæ™¯ï¼Œå¦‚ç¾å®³æ•‘æ´å’Œäº¤é€šç®¡ç†ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Protecting endangered wildlife from illegal poaching presents a critical challenge, particularly in vast and partially observable environments where real-time response is essential. This paper introduces a novel Expectation-Maximization (EM) based latent variable modeling approach in the context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial Vehicle (UAV) coordination in wildlife protection. By modeling hidden environmental factors and inter-agent dynamics through latent variables, our method enhances exploration and coordination under uncertainty.We implement and evaluate our EM-MARL framework using a custom simulation involving 10 UAVs tasked with patrolling protected habitats of the endangered Iranian leopard. Extensive experimental results demonstrate superior performance in detection accuracy, adaptability, and policy convergence when compared to standard algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG). Our findings underscore the potential of combining EM inference with MARL to improve decentralized decisionmaking in complex, high-stakes conservation scenarios. The full implementation, simulation environment, and training scripts are publicly available on GitHub.

