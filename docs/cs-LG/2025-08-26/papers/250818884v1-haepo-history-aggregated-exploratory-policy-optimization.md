---
layout: default
title: HAEPO: History-Aggregated Exploratory Policy Optimization
---

# HAEPO: History-Aggregated Exploratory Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18884" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.18884v1</a>
  <a href="https://arxiv.org/pdf/2508.18884.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18884v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18884v1', 'HAEPO: History-Aggregated Exploratory Policy Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Gaurish Trivedi, Alakh Sharma, Kartikey Singh Bhandari, Dhruv Kumar, Pratik Narang, Jagat Sesh Challa

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-26

**Â§áÊ≥®**: Under review

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫HAEPO‰ª•Ëß£ÂÜ≥ÈïøÊó∂Èó¥‰ªªÂä°Êé¢Á¥¢‰∏çË∂≥ÁöÑÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Êé¢Á¥¢Á≠ñÁï•` `ÂéÜÂè≤ËÅöÂêà` `Á≠ñÁï•‰ºòÂåñ` `ÁÜµÊ≠£ÂàôÂåñ` `ÈïøÊó∂Èó¥‰ªªÂä°` `Ê®°ÂûãÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊé¢Á¥¢ÊñπÊ≥ïÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõ„ÄÇ
2. HAEPOÈÄöËøáÂéÜÂè≤ËÅöÂêàÁöÑÊé¢Á¥¢ÊçüÂ§±ÔºåÂà©Áî®ÂÖ®ËΩ®ËøπÂéÜÂè≤Êù•Â¢ûÂº∫Êé¢Á¥¢ÔºåÂêåÊó∂‰øùÊåÅÁ®≥ÂÆöÊÄß„ÄÇ
3. HAEPOÂú®Â§öÁßç‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Âø´ÈÄüÊî∂ÊïõÂíåÂº∫Â§ßÁöÑÂ≠¶‰π†ËÉΩÂäõÔºå‰ºò‰∫é‰º†ÁªüÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êé¢Á¥¢Âú®Áé∞‰ª£Â≠¶‰π†‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂú®Âº∫ÂåñÂ≠¶‰π†ÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁéØÂ¢É‰∏≠„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇDPOÂíåGRPOÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠ÂæÄÂæÄÈôêÂà∂‰∫ÜÊé¢Á¥¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂéÜÂè≤ËÅöÂêàÊé¢Á¥¢Á≠ñÁï•‰ºòÂåñÔºàHAEPOÔºâÔºåÈÄöËøáÂéãÁº©ÊØè‰∏™ËΩ®Ëøπ‰∏∫ÂÖ∂ÂØπÊï∞Ê¶ÇÁéáÁöÑÊÄªÂíåÔºåÂπ∂Â∫îÁî®Plackett-LuceËΩØÊúÄÂ§ßÂåñÊù•Ëé∑Âæó‰∏éÂõûÊä•ÊàêÊØî‰æãÁöÑÂΩí‰∏ÄÂåñÊùÉÈáçÔºå‰ªéËÄåÈºìÂä±Êõ¥ÂπøÊ≥õÁöÑÊé¢Á¥¢„ÄÇHAEPOÈÄöËøáÂºïÂÖ•ÁÜµÊ≠£ÂàôÂåñÂíåËΩØKLÊÉ©ÁΩöÊù•Á®≥ÂÆöÊõ¥Êñ∞ÔºåÈò≤Ê≠¢ËøáÊó©Â¥©Ê∫É„ÄÇÂÆûÈ™åËØÅÊòéÔºåHAEPOÂú®Â§öÁßç‰ªªÂä°‰∏≠Êî∂ÊïõËøÖÈÄü„ÄÅÊé¢Á¥¢ÂÖ®Èù¢Ôºå‰∏îÂ≠¶‰π†Ë°å‰∏∫‰ºò‰∫éÊàñ‰∏éPPO„ÄÅGRPOÂíåDPOÁõ∏ÂΩìÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™Á®≥ÂÆö‰∏îÂèØËß£ÈáäÁöÑÊ°ÜÊû∂„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠Êé¢Á¥¢‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØDPOÂíåGRPOÂú®Â§ÑÁêÜÈïøËΩ®ËøπÊó∂ÁöÑÂ±ÄÈôêÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöHAEPOÈÄöËøáÂéãÁº©ËΩ®Ëøπ‰∏∫ÂØπÊï∞Ê¶ÇÁéáÁöÑÊÄªÂíåÔºåÁªìÂêàPlackett-LuceËΩØÊúÄÂ§ßÂåñÔºåÈºìÂä±Ê®°ÂûãÂú®Êé¢Á¥¢Êó∂ËÄÉËôëÊõ¥ÂπøÊ≥õÁöÑÂéÜÂè≤‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´òÊé¢Á¥¢ÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHAEPOÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ËΩ®ËøπÂéãÁº©„ÄÅÊùÉÈáçÂΩí‰∏ÄÂåñÂíåÁÜµÊ≠£ÂàôÂåñ‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÂ∞ÜÊØè‰∏™ËΩ®ËøπÁöÑÂØπÊï∞Ê¶ÇÁéáÊ±ÇÂíåÔºåÁÑ∂ÂêéÂ∫îÁî®Plackett-LuceËΩØÊúÄÂ§ßÂåñËé∑ÂæóÂΩí‰∏ÄÂåñÊùÉÈáçÔºåÊúÄÂêéÂºïÂÖ•ÁÜµÊ≠£ÂàôÂåñ‰ª•Á®≥ÂÆöÊõ¥Êñ∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöHAEPOÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ÂéÜÂè≤ËÅöÂêàÁöÑÊé¢Á¥¢ÊçüÂ§±ËÆæËÆ°ÔºåÊòæËëóÂå∫Âà´‰∫é‰º†ÁªüÊñπÊ≥ïÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®ÂÖ®ËΩ®ËøπÂéÜÂè≤‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊçüÂ§±ÂáΩÊï∞‰∏≠ÔºåHAEPOÂºïÂÖ•‰∫ÜÁÜµÊ≠£ÂàôÂåñÂíåËΩØKLÊÉ©ÁΩöÔºå‰ª•Èò≤Ê≠¢Ê®°ÂûãÂú®Êõ¥Êñ∞ËøáÁ®ã‰∏≠ÁöÑËøáÊó©Â¥©Ê∫ÉÔºåÂêåÊó∂‰øùÊåÅÂØπÂÖàÂâçÁ≠ñÁï•ÁöÑÂèÇËÄÉ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

HAEPOÂú®Â§öÁßç‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊî∂ÊïõÈÄüÂ∫¶Âø´ÔºåÊé¢Á¥¢ÂÖ®Èù¢„ÄÇ‰∏éPPO„ÄÅGRPOÂíåDPOÁõ∏ÊØîÔºåHAEPOÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Â±ïÁé∞Âá∫Êõ¥‰ºòÁöÑÂ≠¶‰π†Ë°å‰∏∫ÔºåÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆË°®ÊòéÂÖ∂Âú®‰ªªÂä°ÂÆåÊàêÁéáÂíåÂ≠¶‰π†Á®≥ÂÆöÊÄß‰∏äÂùáÊúâÊòæËëóÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HAEPOÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Âº∫ÂåñÂ≠¶‰π†„ÄÅÊú∫Âô®‰∫∫ÊéßÂà∂ÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òÊé¢Á¥¢ÊïàÁéáÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂä†ÈÄüÊ®°ÂûãÁöÑÂ≠¶‰π†ËøáÁ®ãÔºåÊèêÂçáÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êô∫ËÉΩ‰ΩìÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Exploration is essential in modern learning, from reinforcement learning environments with small neural policies to large language models (LLMs). Existing work, such as DPO, leverages full sequence log-likelihoods to capture an entire trajectory of the model's decisions, while methods like GRPO aggregate per-token ratios into a trajectory-level update. However, both often limit exploration on long-horizon tasks. We introduce History-Aggregated Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to combat these shortcomings. HAEPO compresses each trajectory into the sum of its logarithmic probabilities (a cumulative logarithmic likelihood), and applies a Plackett-Luce softmax across trajectories to obtain normalized weights proportional to their returns, thus encouraging broader exploration. We add entropy regularization to stabilize the aggressive updates to prevent premature collapse and a soft KL penalty relative to a frozen copy of the previous (reference) policy. Empirically, HAEPO converges fast, explores thoroughly, aligns closely with true rewards, and demonstrates robust learning behavior better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO provides a stable and interpretable framework by explicitly leveraging full-trajectory history while balancing exploration and stability.

