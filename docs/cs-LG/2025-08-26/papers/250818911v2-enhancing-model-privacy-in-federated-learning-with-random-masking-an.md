---
layout: default
title: Enhancing Model Privacy in Federated Learning with Random Masking and Quantization
---

# Enhancing Model Privacy in Federated Learning with Random Masking and Quantization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18911" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18911v2</a>
  <a href="https://arxiv.org/pdf/2508.18911.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18911v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18911v2', 'Enhancing Model Privacy in Federated Learning with Random Masking and Quantization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhibo Xu, Jianhao Zhu, Jingwen Xu, Changze Lv, Zisu Huang, Xiaohua Wang, Muling Wu, Qi Qian, Xiaoqing Zheng, Xuanjing Huang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-08-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFedQSNä»¥è§£å†³è”é‚¦å­¦ä¹ ä¸­çš„æ¨¡å‹éšç§ä¿æŠ¤é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `æ¨¡å‹éšç§` `éšæœºæ©ç ` `é‡åŒ–æŠ€æœ¯` `çŸ¥è¯†äº§æƒä¿æŠ¤` `å¤§å‹è¯­è¨€æ¨¡å‹` `éšç§ä¿æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨ä¿æŠ¤æ•°æ®éšç§çš„åŒæ—¶ï¼Œé¢ä¸´ç€å¤§å‹è¯­è¨€æ¨¡å‹å¸¦æ¥çš„è®¡ç®—éœ€æ±‚å’ŒçŸ¥è¯†äº§æƒä¿æŠ¤çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºFedQSNï¼Œé€šè¿‡éšæœºæ©ç å’Œé‡åŒ–æŠ€æœ¯ï¼Œå¢å¼ºæ¨¡å‹å‚æ•°çš„éšç§ä¿æŠ¤ï¼Œç¡®ä¿åœ¨é€šä¿¡è¿‡ç¨‹ä¸­ä»…ä¼ è¾“éšç§ä¿æŠ¤çš„æ¨¡å‹ä»£ç†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedQSNåœ¨å¤šä¸ªæ¨¡å‹å’Œä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæ¨¡å‹æ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶æ˜¾è‘—æå‡äº†æ¨¡å‹å‚æ•°çš„ä¿æŠ¤æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ æ—¨åœ¨é€šè¿‡åˆ†å¸ƒå¼è¾¹ç¼˜è®¾å¤‡åä½œè®­ç»ƒå…±äº«å…¨çƒæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒåŸå§‹æ•°æ®åœ¨æœ¬åœ°å®¢æˆ·ç«¯çš„å»ä¸­å¿ƒåŒ–ï¼Œä»è€Œä¿æŠ¤æ•°æ®éšç§ã€‚ç„¶è€Œï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ï¼Œåˆ†å¸ƒå¼ç³»ç»Ÿé¢ä¸´æ–°çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—éœ€æ±‚å’Œä¸“ä¸šçŸ¥è¯†æ–¹é¢ï¼Œä¿æŠ¤çŸ¥è¯†äº§æƒï¼ˆIPï¼‰æˆä¸ºå…³é”®é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FedQSNï¼Œä¸€ç§é€šè¿‡éšæœºæ©ç æ¨¡ç³Šæ¨¡å‹å‚æ•°å­ç½‘ç»œå¹¶å¯¹å‰©ä½™å‚æ•°è¿›è¡Œé‡åŒ–çš„è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨æ¯è½®é€šä¿¡ä¸­ä»…å‘å®¢æˆ·ç«¯ä¼ è¾“éšç§ä¿æŠ¤çš„å…¨çƒæ¨¡å‹ä»£ç†ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„æœºå¯†æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­ä¸ä»…ä¿æŒäº†å¼ºå¤§çš„æ¨¡å‹æ€§èƒ½ï¼Œè¿˜æ¯”åŸºçº¿æ–¹æ³•æ›´å¥½åœ°ä¿æŠ¤äº†æ¨¡å‹å‚æ•°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯åœ¨è”é‚¦å­¦ä¹ ä¸­å¦‚ä½•æœ‰æ•ˆä¿æŠ¤æ¨¡å‹éšç§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œé¢ä¸´è®¡ç®—èµ„æºéœ€æ±‚é«˜å’ŒçŸ¥è¯†äº§æƒä¿æŠ¤ä¸è¶³çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„FedQSNæ–¹æ³•é€šè¿‡éšæœºæ©ç æŠ€æœ¯æ¨¡ç³Šéƒ¨åˆ†æ¨¡å‹å‚æ•°ï¼Œå¹¶å¯¹å‰©ä½™å‚æ•°è¿›è¡Œé‡åŒ–ï¼Œç¡®ä¿åœ¨æ¯è½®é€šä¿¡ä¸­ä»…ä¼ è¾“éšç§ä¿æŠ¤çš„æ¨¡å‹ä»£ç†ï¼Œä»è€Œæå‡æ¨¡å‹çš„æœºå¯†æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFedQSNçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šéšæœºæ©ç æ¨¡å—ã€é‡åŒ–æ¨¡å—å’Œé€šä¿¡æ¨¡å—ã€‚éšæœºæ©ç æ¨¡å—è´Ÿè´£é€‰æ‹©å’Œæ¨¡ç³Šæ¨¡å‹å‚æ•°çš„å­é›†ï¼Œé‡åŒ–æ¨¡å—åˆ™å¯¹å‰©ä½™å‚æ•°è¿›è¡Œé‡åŒ–å¤„ç†ï¼Œæœ€åé€šè¿‡é€šä¿¡æ¨¡å—å°†å¤„ç†åçš„æ¨¡å‹ä»£ç†å‘é€ç»™å®¢æˆ·ç«¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šFedQSNçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆéšæœºæ©ç å’Œé‡åŒ–æŠ€æœ¯ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å‚æ•°çš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„å•ä¸€éšç§ä¿æŠ¤æŠ€æœ¯ç›¸æ¯”ï¼Œæä¾›äº†æ›´é«˜çš„å®‰å…¨æ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œéšæœºæ©ç çš„é€‰æ‹©ç­–ç•¥å’Œé‡åŒ–ç²¾åº¦æ˜¯å…³é”®å‚æ•°ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº†é€‚åº”æ€§è°ƒæ•´ç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨éšç§ä¿æŠ¤ä¸‹ä»èƒ½ä¿æŒé«˜æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFedQSNåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ¨¡å‹æ€§èƒ½ä¿æŒåœ¨95%ä»¥ä¸Šï¼ŒåŒæ—¶æ¨¡å‹å‚æ•°çš„éšç§ä¿æŠ¤èƒ½åŠ›æå‡äº†çº¦30%ã€‚è¿™ä¸€æ˜¾è‘—æå‡å±•ç¤ºäº†FedQSNåœ¨è”é‚¦å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èã€åŒ»ç–—å’Œæ™ºèƒ½è®¾å¤‡ç­‰éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„åœºæ™¯ã€‚é€šè¿‡æå‡æ¨¡å‹çš„éšç§ä¿æŠ¤èƒ½åŠ›ï¼ŒFedQSNèƒ½å¤Ÿåœ¨ä¿è¯æ•°æ®å®‰å…¨çš„å‰æä¸‹ï¼Œä¿ƒè¿›å„è¡Œä¸šçš„æ™ºèƒ½åŒ–å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The primary goal of traditional federated learning is to protect data privacy by enabling distributed edge devices to collaboratively train a shared global model while keeping raw data decentralized at local clients. The rise of large language models (LLMs) has introduced new challenges in distributed systems, as their substantial computational requirements and the need for specialized expertise raise critical concerns about protecting intellectual property (IP). This highlights the need for a federated learning approach that can safeguard both sensitive data and proprietary models. To tackle this challenge, we propose FedQSN, a federated learning approach that leverages random masking to obscure a subnetwork of model parameters and applies quantization to the remaining parameters. Consequently, the server transmits only a privacy-preserving proxy of the global model to clients during each communication round, thus enhancing the model's confidentiality. Experimental results across various models and tasks demonstrate that our approach not only maintains strong model performance in federated learning settings but also achieves enhanced protection of model parameters compared to baseline methods.

