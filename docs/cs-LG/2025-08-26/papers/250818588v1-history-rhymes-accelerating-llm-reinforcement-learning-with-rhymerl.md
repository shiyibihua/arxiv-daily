---
layout: default
title: History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL
---

# History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18588" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18588v1</a>
  <a href="https://arxiv.org/pdf/2508.18588.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18588v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18588v1', 'History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingkai He, Tianjian Li, Erhu Feng, Dong Du, Qian Liu, Tao Liu, Yubin Xia, Haibo Chen

**åˆ†ç±»**: cs.LG, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRhymeRLä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­çš„GPUåˆ©ç”¨ç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `GPUåˆ©ç”¨ç‡` `HistoSpec` `HistoPipe` `è®­ç»ƒæ•ˆç‡` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨GPUåˆ©ç”¨ç‡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œä¸»è¦ç”±äºå›æ»šé˜¶æ®µçš„ä¸»å¯¼æ€§å’Œå›æ»šé•¿åº¦çš„ä¸å¹³è¡¡ã€‚
2. æœ¬æ–‡æå‡ºRhymeRLï¼Œé€šè¿‡HistoSpecå’ŒHistoPipeä¸¤é¡¹åˆ›æ–°ï¼Œåˆ©ç”¨å†å²å›æ»šçš„ç›¸ä¼¼æ€§æ¥åŠ é€Ÿå›æ»šç”Ÿæˆå’Œä¼˜åŒ–å·¥ä½œè´Ÿè½½ã€‚
3. åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒRhymeRLå®ç°äº†ä»æ•°ååˆ°æ•°åƒä¸ªGPUçš„å¯æ‰©å±•æ€§ï¼Œæ€§èƒ½æå‡è¾¾åˆ°2.6å€ï¼Œä¸”ä¸å½±å“è®­ç»ƒç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆä¸ºæå‡LLMsæ¨ç†èƒ½åŠ›çš„é‡è¦æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰RLç³»ç»Ÿé¢ä¸´GPUåˆ©ç”¨ç‡ä½ä¸‹çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºå›æ»šé˜¶æ®µä¸»å¯¼äº†æ•´ä¸ªRLè¿‡ç¨‹ï¼Œä»¥åŠåŒä¸€æ‰¹æ¬¡å†…å›æ»šé•¿åº¦çš„ä¸å¹³è¡¡ã€‚å°½ç®¡ä»¥å¾€çš„è§£å†³æ–¹æ¡ˆå¦‚å¼‚æ­¥æ‰§è¡Œå’Œæˆªæ–­æä¾›äº†éƒ¨åˆ†ç¼“è§£ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²è®­ç»ƒç²¾åº¦ã€‚åŸºäºå†å²å›æ»šå“åº”çš„ç›¸ä¼¼æ€§ï¼Œæœ¬æ–‡æå‡ºäº†RhymeRLï¼Œé€šè¿‡HistoSpecå’ŒHistoPipeä¸¤é¡¹åˆ›æ–°åŠ é€ŸRLè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRhymeRLåœ¨ä¸å½±å“ç²¾åº¦çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†2.6å€çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­GPUåˆ©ç”¨ç‡ä½ä¸‹çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å›æ»šé˜¶æ®µçš„ä¸»å¯¼æ€§å’Œå›æ»šé•¿åº¦çš„ä¸å¹³è¡¡å¯¼è‡´äº†GPUèµ„æºçš„æµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è§‚å¯Ÿåˆ°çš„å†å²å›æ»šå“åº”ç›¸ä¼¼æ€§ï¼Œæå‡ºRhymeRLç³»ç»Ÿï¼Œåˆ©ç”¨HistoSpecå’ŒHistoPipeä¼˜åŒ–å›æ»šç”Ÿæˆå’Œè°ƒåº¦ç­–ç•¥ï¼Œä»è€ŒåŠ é€ŸRLè®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRhymeRLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šHistoSpecç”¨äºç”Ÿæˆå›æ»šè‰ç¨¿ï¼ŒHistoPipeç”¨äºè°ƒåº¦å›æ»šå·¥ä½œï¼Œç¡®ä¿è´Ÿè½½å‡è¡¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šHistoSpecé€šè¿‡å†å²å›æ»šåºåˆ—çš„ç›¸ä¼¼æ€§ç”Ÿæˆå‡†ç¡®çš„å›æ»šè‰ç¨¿ï¼Œè€ŒHistoPipeåˆ™é€šè¿‡ä¸¤çº§è°ƒåº¦ç­–ç•¥è§£å†³äº†å›æ»šæ°”æ³¡é—®é¢˜ï¼Œè¿™äº›åˆ›æ–°æ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨HistoSpecä¸­ï¼Œé‡‡ç”¨äº†åŸºäºå†å²æ•°æ®çš„æ¨æµ‹è§£ç å¼•æ“ï¼›åœ¨HistoPipeä¸­ï¼Œè®¾è®¡äº†åŸºäºå†å²å›æ»šåˆ†å¸ƒçš„è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œç¡®ä¿å„ä¸ªå›æ»šå·¥ä½œè€…çš„å·¥ä½œé‡å‡è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRhymeRLåœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­å®ç°äº†2.6å€çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†GPUçš„åˆ©ç”¨ç‡ï¼ŒåŒæ—¶ä¿æŒäº†è®­ç»ƒçš„å‡†ç¡®æ€§ï¼Œæœªå¯¹RLèŒƒå¼è¿›è¡Œä¿®æ”¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ¨ç†èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡ã€‚æœªæ¥ï¼ŒRhymeRLå¯èƒ½æ¨åŠ¨æ›´å¤§è§„æ¨¡çš„æ¨¡å‹è®­ç»ƒï¼Œä½¿å¾—å¤æ‚ä»»åŠ¡çš„å¤„ç†æ›´åŠ é«˜æ•ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid advancement of large language models (LLMs), reinforcement learning (RL) has emerged as a pivotal methodology for enhancing the reasoning capabilities of LLMs. Unlike traditional pre-training approaches, RL encompasses multiple stages: rollout, reward, and training, which necessitates collaboration among various worker types. However, current RL systems continue to grapple with substantial GPU underutilization, due to two primary factors: (1) The rollout stage dominates the overall RL process due to test-time scaling; (2) Imbalances in rollout lengths (within the same batch) result in GPU bubbles. While prior solutions like asynchronous execution and truncation offer partial relief, they may compromise training accuracy for efficiency.
>   Our key insight stems from a previously overlooked observation: rollout responses exhibit remarkable similarity across adjacent training epochs. Based on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate RL training with two key innovations. First, to enhance rollout generation, we present HistoSpec, a speculative decoding inference engine that utilizes the similarity of historical rollout token sequences to obtain accurate drafts. Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier scheduling strategy that leverages the similarity of historical rollout distributions to balance workload among rollout workers. We have evaluated RhymeRL within a real production environment, demonstrating scalability from dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL achieves a 2.6x performance improvement over existing methods, without compromising accuracy or modifying the RL paradigm.

