---
layout: default
title: Dual-Distilled Heterogeneous Federated Learning with Adaptive Margins for Trainable Global Prototypes
---

# Dual-Distilled Heterogeneous Federated Learning with Adaptive Margins for Trainable Global Prototypes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19009" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19009v3</a>
  <a href="https://arxiv.org/pdf/2508.19009.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19009v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19009v3', 'Dual-Distilled Heterogeneous Federated Learning with Adaptive Margins for Trainable Global Prototypes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fatema Siddika, Md Anwar Hossen, Wensheng Zhang, Anuj Sharma, Juan Pablo MuÃ±oz, Ali Jannesari

**åˆ†ç±»**: cs.LG, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-12-19)

**å¤‡æ³¨**: 11 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒè’¸é¦å¼‚æ„è”é‚¦å­¦ä¹ ä»¥è§£å†³åŸå‹è¾¹ç•Œæ”¶ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼‚æ„è”é‚¦å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `åŸå‹èšåˆ` `å¯¹æ¯”å­¦ä¹ ` `æ¨¡å‹å¼‚æ„æ€§` `éIIDæ•°æ®` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºåŸå‹çš„HFLæ–¹æ³•åœ¨èšåˆåŸå‹æ—¶ä½¿ç”¨æ ‡å‡†åŠ æƒå¹³å‡ï¼Œå¯¼è‡´å…¨å±€çŸ¥è¯†çš„æ”¶ç¼©å’Œæ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºFedProtoKDæ¡†æ¶ï¼Œé€šè¿‡åŒçŸ¥è¯†è’¸é¦æœºåˆ¶å’Œè‡ªé€‚åº”åŸå‹è¾¹ç•Œï¼Œè§£å†³äº†åŸå‹è¾¹ç•Œæ”¶ç¼©çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFedProtoKDåœ¨å¤šç§è®¾ç½®ä¸‹å¹³å‡æé«˜äº†1.13%çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œæœ€é«˜å¯è¾¾34.13%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼‚æ„è”é‚¦å­¦ä¹ ï¼ˆHFLï¼‰å› å…¶å¤„ç†å®¢æˆ·ç«¯æ¨¡å‹å’Œæ•°æ®å¼‚æ„æ€§çš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚åŸºäºåŸå‹çš„HFLæ–¹æ³•ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨åº”å¯¹ç»Ÿè®¡å’Œæ¨¡å‹å¼‚æ„æ€§ä»¥åŠéšç§æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŠ æƒå¹³å‡èšåˆæ–¹æ³•å¾€å¾€å¯¼è‡´å…¨å±€çŸ¥è¯†çš„äºšä¼˜åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡å‹å¼‚æ„å’Œéç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®åˆ†å¸ƒçš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºçš„FedProtoKDæ¡†æ¶åˆ©ç”¨å¢å¼ºçš„åŒçŸ¥è¯†è’¸é¦æœºåˆ¶ï¼Œé€šè¿‡å¯¹å®¢æˆ·ç«¯çš„logitså’ŒåŸå‹ç‰¹å¾è¡¨ç¤ºè¿›è¡Œä¼˜åŒ–ï¼Œè§£å†³äº†åŸå‹è¾¹ç•Œæ”¶ç¼©é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedProtoKDåœ¨å¤šç§è®¾ç½®ä¸‹å¹³å‡æé«˜äº†1.13%çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œæœ€é«˜å¯è¾¾34.13%ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰çš„HFLæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯åœ¨å¼‚æ„è”é‚¦å­¦ä¹ ä¸­ï¼Œä¼ ç»ŸåŠ æƒå¹³å‡èšåˆå¯¼è‡´çš„åŸå‹è¾¹ç•Œæ”¶ç¼©é—®é¢˜ï¼Œè¿™ç§æ”¶ç¼©ä¼šå½±å“æ¨¡å‹åœ¨éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®ä¸Šçš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„FedProtoKDæ¡†æ¶é€šè¿‡åŒçŸ¥è¯†è’¸é¦æœºåˆ¶ï¼Œç»“åˆå®¢æˆ·ç«¯çš„logitså’ŒåŸå‹ç‰¹å¾è¡¨ç¤ºï¼Œå¢å¼ºäº†ç³»ç»Ÿæ€§èƒ½ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”åŸå‹è¾¹ç•Œä»¥è§£å†³è¾¹ç•Œæ”¶ç¼©é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰å®¢æˆ·ç«¯æ¨¡å—ï¼Œè´Ÿè´£ç”ŸæˆåŸå‹å’Œlogitsï¼›2ï¼‰æœåŠ¡å™¨æ¨¡å—ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•è®­ç»ƒå¯è°ƒçš„å…¨å±€åŸå‹ï¼Œå¹¶æ ¹æ®æ ·æœ¬ä¸ç±»ä»£è¡¨åŸå‹çš„æ¥è¿‘åº¦è¯„ä¼°å…¬å…±æ ·æœ¬çš„é‡è¦æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å¯¹æ¯”å­¦ä¹ åŸºç¡€çš„å¯è°ƒæœåŠ¡å™¨åŸå‹å’Œç±»-wiseè‡ªé€‚åº”åŸå‹è¾¹ç•Œï¼Œæ˜¾è‘—æ”¹å–„äº†åŸå‹èšåˆçš„æ•ˆæœï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†çŸ¥è¯†è’¸é¦æŸå¤±å’Œå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œç¡®ä¿äº†åŸå‹çš„æœ‰æ•ˆèšåˆå’Œè¾¹ç•Œçš„é€‚åº”æ€§è°ƒæ•´ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œä»¥æå–æ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FedProtoKDåœ¨å¤šç§å®éªŒè®¾ç½®ä¸‹å¹³å‡æé«˜äº†1.13%çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œæœ€é«˜å¯è¾¾34.13%ã€‚ä¸ç°æœ‰çš„æœ€å…ˆè¿›HFLæ–¹æ³•ç›¸æ¯”ï¼ŒFedProtoKDåœ¨å¤„ç†æ¨¡å‹å¼‚æ„æ€§å’ŒéIIDæ•°æ®åˆ†å¸ƒæ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—æ•°æ®åˆ†æã€é‡‘èæ¬ºè¯ˆæ£€æµ‹å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿç­‰ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®éšç§å’Œå®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ä¸­ã€‚é€šè¿‡æå‡å¼‚æ„è”é‚¦å­¦ä¹ çš„æ€§èƒ½ï¼ŒFedProtoKDèƒ½å¤Ÿä¸ºå¤šæ–¹åä½œæä¾›æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¿ƒè¿›å„è¡Œä¸šçš„æ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Heterogeneous Federated Learning (HFL) has gained significant attention for its capacity to handle both model and data heterogeneity across clients. Prototype-based HFL methods emerge as a promising solution to address statistical and model heterogeneity as well as privacy challenges, paving the way for new advancements in HFL research. This method focuses on sharing class-representative prototypes among heterogeneous clients. However, aggregating these prototypes via standard weighted averaging often yields sub-optimal global knowledge. Specifically, the averaging approach induces a shrinking of the aggregated prototypes' decision margins, thereby degrading model performance in scenarios with model heterogeneity and non-IID data distributions. The propose FedProtoKD in a Heterogeneous Federated Learning setting, utilizing an enhanced dual-knowledge distillation mechanism to enhance system performance by leveraging clients' logits and prototype feature representations. The proposed framework aims to resolve the prototype margin-shrinking problem using a contrastive learning-based trainable server prototype by leveraging a class-wise adaptive prototype margin. Furthermore, the framework assess the importance of public samples using the closeness of the sample's prototype to its class representative prototypes, which enhances learning performance. FedProtoKD improved test accuracy by an average of 1.13% and up to 34.13% across various settings, significantly outperforming existing state-of-the-art HFL methods.

