---
layout: default
title: Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size
---

# Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15025" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15025v1</a>
  <a href="https://arxiv.org/pdf/2506.15025.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15025v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15025v1', 'Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Soufiane Hayou, Liyuan Liu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

**å¤‡æ³¨**: TD,LR: How to set the learning rate for emebdding layer in LLMs?

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¼˜åŒ–åµŒå…¥å­¦ä¹ ç‡ä»¥åº”å¯¹å¤§è¯­è¨€æ¨¡å‹è¯æ±‡è§„æ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å­¦ä¹ ç‡ä¼˜åŒ–` `è¯æ±‡è§„æ¨¡` `è®­ç»ƒåŠ¨æ€` `å‚æ•°åŒ–æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„$Î¼P$æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­åº”ç”¨æ—¶ï¼Œå­˜åœ¨ç†è®ºä¸å®è·µä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯æœªè€ƒè™‘è¯æ±‡è§„æ¨¡çš„å˜åŒ–ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç†è®ºåˆ†æï¼Œæ¢è®¨è¯æ±‡è§„æ¨¡å¯¹è®­ç»ƒåŠ¨æ€çš„å½±å“ï¼Œå¹¶å¼•å…¥äº†å¤§è¯æ±‡ï¼ˆLVï¼‰çŠ¶æ€çš„æ¦‚å¿µã€‚
3. é€šè¿‡å®éªŒéªŒè¯ï¼Œå‘ç°æœ€ä½³åµŒå…¥å­¦ä¹ ç‡ä¸éšè—å­¦ä¹ ç‡çš„æ¯”ä¾‹åº”ä¸º$Î˜(	ext{sqrt(width)})$ï¼Œå¹¶åœ¨é¢„è®­ç»ƒä¸­å–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹æˆæœ¬é«˜æ˜‚ã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†å¤šç§ä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œå‚æ•°åŒ–çš„æ–¹æ³•ã€‚æœ¬æ–‡èšç„¦äº$Î¼P$ï¼ˆæœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†æ¨¡å‹æƒé‡å’Œå­¦ä¹ ç‡å‚æ•°åŒ–ï¼Œä½¿è¶…å‚æ•°èƒ½å¤Ÿåœ¨ä¸åŒå®½åº¦ï¼ˆåµŒå…¥ç»´åº¦ï¼‰æ¨¡å‹é—´è½¬ç§»ã€‚ç„¶è€Œï¼Œ$Î¼P$åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨æ•ˆæœå­˜åœ¨äº‰è®®ï¼Œå°¤å…¶æ˜¯å…¶ç†è®ºæœªè€ƒè™‘è¯æ±‡è§„æ¨¡çš„å˜åŒ–ã€‚æœ¬æ–‡é€šè¿‡ç†è®ºåˆ†æè¯æ±‡è§„æ¨¡å¯¹è®­ç»ƒåŠ¨æ€çš„å½±å“ï¼Œæå‡ºåœ¨è¯æ±‡è§„æ¨¡å¢åŠ æ—¶ï¼Œè®­ç»ƒåŠ¨æ€ä¼šåœ¨$Î¼P$å’Œç§°ä¸ºå¤§è¯æ±‡ï¼ˆLVï¼‰çŠ¶æ€ä¹‹é—´æ’å€¼ï¼Œä¸”åœ¨LVçŠ¶æ€ä¸‹ï¼Œæœ€ä½³åµŒå…¥å­¦ä¹ ç‡ä¸éšè—å­¦ä¹ ç‡çš„æ¯”ä¾‹åº”è¿‘ä¼¼ä¸º$Î˜(	ext{sqrt(width)})$ï¼Œä¸$Î¼P$é¢„æµ‹çš„$Î˜(	ext{width})$ä¸åŒã€‚é€šè¿‡å®éªŒéªŒè¯äº†è¿™ä¸€ç†è®ºï¼Œå¹¶ä»å¤´å¼€å§‹é¢„è®­ç»ƒäº†ä¸€ä¸ª10äº¿å‚æ•°çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†æ‰€æç¼©æ”¾è§„åˆ™çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³$Î¼P$åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­åº”ç”¨æ—¶æœªè€ƒè™‘è¯æ±‡è§„æ¨¡å˜åŒ–çš„é—®é¢˜ï¼Œå¯¼è‡´ç†è®ºä¸å®è·µç»“æœä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç†è®ºåˆ†æè¯æ±‡è§„æ¨¡å¯¹è®­ç»ƒåŠ¨æ€çš„å½±å“ï¼Œæå‡ºåœ¨è¯æ±‡è§„æ¨¡å¢åŠ æ—¶ï¼Œè®­ç»ƒåŠ¨æ€ä¼šåœ¨$Î¼P$å’Œå¤§è¯æ±‡ï¼ˆLVï¼‰çŠ¶æ€ä¹‹é—´æ’å€¼ï¼Œä»è€Œè°ƒæ•´å­¦ä¹ ç‡çš„æ¯”ä¾‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå»ºç«‹äº†è¯æ±‡è§„æ¨¡ä¸è®­ç»ƒåŠ¨æ€ä¹‹é—´çš„å…³ç³»æ¨¡å‹ï¼Œéšåé€šè¿‡å®éªŒéªŒè¯äº†ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ï¼Œæœ€ååœ¨é¢„è®­ç»ƒä¸­åº”ç”¨äº†æ–°çš„å­¦ä¹ ç‡ç¼©æ”¾è§„åˆ™ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†å¤§è¯æ±‡ï¼ˆLVï¼‰çŠ¶æ€çš„æ¦‚å¿µï¼Œæ­ç¤ºäº†åœ¨è¯¥çŠ¶æ€ä¸‹æœ€ä½³åµŒå…¥å­¦ä¹ ç‡ä¸éšè—å­¦ä¹ ç‡çš„æ¯”ä¾‹åº”ä¸º$Î˜(	ext{sqrt(width)})$ï¼Œä¸$Î¼P$çš„$Î˜(	ext{width})$é¢„æµ‹å­˜åœ¨æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„è¯æ±‡è§„æ¨¡å’Œæ¨¡å‹å®½åº¦ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ï¼Œä»¥éªŒè¯æ‰€æå­¦ä¹ ç‡ç¼©æ”¾è§„åˆ™çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„åœ¨å®éªŒéƒ¨åˆ†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°æå‡ºçš„å­¦ä¹ ç‡ç¼©æ”¾è§„åˆ™åï¼Œé¢„è®­ç»ƒçš„10äº¿å‚æ•°æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿ$Î¼P$æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pretraining large language models is a costly process. To make this process more efficient, several methods have been proposed to optimize model architecture/parametrization and hardware use. On the parametrization side, $Î¼P$ (Maximal Update Parametrization) parametrizes model weights and learning rate (LR) in a way that makes hyperparameters (HPs) transferable with width (embedding dimension): HPs can be tuned for a small model and used for larger models without additional tuning. While $Î¼$P showed impressive results in practice, recent empirical studies have reported conflicting observations when applied to LLMs. One limitation of the theory behind $Î¼$P is the fact that input dimension (vocabulary size in LLMs) is considered fixed when taking the width to infinity. This is unrealistic since vocabulary size is generally much larger than width in practice. In this work, we provide a theoretical analysis of the effect of vocabulary size on training dynamics, and subsequently show that as vocabulary size increases, the training dynamics \emph{interpolate between the $Î¼$P regime and another regime that we call Large Vocab (LV) Regime}, where optimal scaling rules are different from those predicted by $Î¼$P. Our analysis reveals that in the LV regime, the optimal embedding LR to hidden LR ratio should roughly scale as $Î˜(\sqrt{width})$, surprisingly close to the empirical findings previously reported in the literature, and different from the $Î˜(width)$ ratio predicted by $Î¼$P. We conduct several experiments to validate our theory, and pretrain a 1B model from scratch to show the benefit of our suggested scaling rule for the embedding LR.

