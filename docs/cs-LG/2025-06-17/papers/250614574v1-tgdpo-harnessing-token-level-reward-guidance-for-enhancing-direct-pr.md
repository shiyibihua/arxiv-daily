---
layout: default
title: TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization
---

# TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14574" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14574v1</a>
  <a href="https://arxiv.org/pdf/2506.14574.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14574v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14574v1', 'TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya Jia

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

**å¤‡æ³¨**: ICML 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/dvlab-research/TGDPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTGDPOä»¥è§£å†³ç›´æ¥åå¥½ä¼˜åŒ–ä¸­çš„å¥–åŠ±æŒ‡å¯¼é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›´æ¥åå¥½ä¼˜åŒ–` `tokençº§å¥–åŠ±` `è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•åœ¨åˆ©ç”¨tokençº§å¥–åŠ±æŒ‡å¯¼æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥æœ‰æ•ˆå¯¹é½æ¨¡å‹è¾“å‡ºã€‚
2. æœ¬æ–‡é€šè¿‡å°†åºåˆ—çº§PPOåˆ†è§£ä¸ºtokençº§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„tokençº§å¥–åŠ±æŒ‡å¯¼æ¡†æ¶ï¼Œè§£å†³äº†DPOä¸­çš„å¥–åŠ±åˆ©ç”¨é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨MT-Benchã€AlpacaEval 2å’ŒArena-Hardä¸Šåˆ†åˆ«æå‡äº†7.5ã€6.2å’Œ4.3ç‚¹çš„èƒœç‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸDPOã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ è¿›å±•è¡¨æ˜ï¼Œåˆ©ç”¨ç»†ç²’åº¦çš„tokençº§å¥–åŠ±æ¨¡å‹å¯ä»¥æ˜¾è‘—æå‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œå°†tokençº§å¥–åŠ±ä½œä¸ºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„æŒ‡å¯¼å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºDPOè¢«è¡¨è¿°ä¸ºåºåˆ—çº§çš„èµŒåšé—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡å°†åºåˆ—çº§PPOåˆ†è§£ä¸ºä¸€ç³»åˆ—tokençº§çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æ„å»ºäº†tokençº§PPOçš„æ¡†æ¶ï¼Œæ¨å¯¼å‡ºé—­å¼çš„æœ€ä¼˜tokençº§ç­–ç•¥åŠå…¶å¯¹åº”çš„tokençº§å¥–åŠ±ã€‚é€šè¿‡æ‰€è·å¾—çš„å¥–åŠ±å’ŒBradley-Terryæ¨¡å‹ï¼Œæœ¬æ–‡å»ºç«‹äº†å¯è®¡ç®—çš„æŸå¤±å‡½æ•°æ¡†æ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè¯±å¯¼DPOå¥–åŠ±çš„å®ç”¨å¥–åŠ±æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨DPOä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸­å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨tokençº§å¥–åŠ±çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å°†tokençº§å¥–åŠ±æœ‰æ•ˆè½¬åŒ–ä¸ºDPOçš„æŒ‡å¯¼ï¼Œå¯¼è‡´æ¨¡å‹å¯¹äººç±»åå¥½çš„å¯¹é½æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åºåˆ—çº§PPOåˆ†è§£ä¸ºä¸€ç³»åˆ—tokençº§çš„ä¼˜åŒ–é—®é¢˜ï¼Œåˆ©ç”¨tokençº§å¥–åŠ±æŒ‡å¯¼æ¥ä¼˜åŒ–æ¯ä¸ªtokençš„ç­–ç•¥ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸åŒtokençš„å¥–åŠ±è¿›è¡Œçµæ´»è°ƒæ•´ï¼Œä»è€Œæ›´å¥½åœ°å¯¹é½äººç±»åå¥½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯tokençº§PPOçš„æ¡†æ¶ï¼Œå…¶æ¬¡æ˜¯åŸºäºBradley-Terryæ¨¡å‹çš„å¥–åŠ±è®¡ç®—æ¨¡å—ã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¨¡å—ï¼Œè®ºæ–‡å®ç°äº†tokençº§å¥–åŠ±çš„æœ‰æ•ˆåˆ©ç”¨å’Œç­–ç•¥ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„tokençº§å¥–åŠ±æŒ‡å¯¼æ¡†æ¶ï¼Œä½¿å¾—DPOèƒ½å¤Ÿåœ¨tokençº§åˆ«ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸åœ¨åºåˆ—çº§åˆ«è¿›è¡Œä¼˜åŒ–ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨tokençº§ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬æŸå¤±å‡½æ•°çš„æ„å»ºå’Œtokençº§å¥–åŠ±çš„è®¡ç®—æ–¹å¼ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­èƒ½å¤Ÿçµæ´»è°ƒæ•´ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†å…·ä½“çš„å‚æ•°è®¾ç½®ï¼Œä»¥é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTGDPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»ŸDPOæ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨MT-Benchä¸Šæå‡7.5ç‚¹ã€åœ¨AlpacaEval 2ä¸Šæå‡6.2ç‚¹ã€åœ¨Arena-Hardä¸Šæå‡4.3ç‚¹çš„èƒœç‡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹å¯¹äººç±»åå¥½çš„å¯¹é½èƒ½åŠ›ï¼ŒTGDPOå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­æé«˜ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.

