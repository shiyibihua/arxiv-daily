---
layout: default
title: RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation
---

# RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15965" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15965v1</a>
  <a href="https://arxiv.org/pdf/2509.15965.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15965v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15965v1', 'RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu Wang

**åˆ†ç±»**: cs.LG, cs.AI, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: GitHub Repo: https://github.com/RLinf/RLinf

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RLinfï¼šé€šè¿‡å®å¾®è§‚æµè½¬æ¢å®ç°çµæ´»é«˜æ•ˆçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è§„æ¨¡è®­ç»ƒ` `ç³»ç»Ÿä¼˜åŒ–` `å®å¾®è§‚æµè½¬æ¢` `è‡ªé€‚åº”é€šä¿¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿåœ¨å¤„ç†å¼‚æ„å’ŒåŠ¨æ€çš„å·¥ä½œæµç¨‹æ—¶ï¼Œé¢ä¸´ç¡¬ä»¶åˆ©ç”¨ç‡ä½å’Œè®­ç»ƒé€Ÿåº¦æ…¢çš„æŒ‘æˆ˜ã€‚
2. RLinfæå‡ºå®å¾®è§‚æµè½¬æ¢ï¼ˆM2Flowï¼‰èŒƒå¼ï¼Œè‡ªåŠ¨åˆ†è§£å’Œé‡ç»„RLå·¥ä½œæµç¨‹ï¼Œä¼˜åŒ–æ‰§è¡Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRLinfåœ¨æ¨ç†å’Œå…·èº«RLä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œç«¯åˆ°ç«¯è®­ç»ƒååé‡æå‡1.1-2.13å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½ã€æ™ºèƒ½ä½“æ™ºèƒ½å’Œå…·èº«æ™ºèƒ½æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼ŒRLå·¥ä½œæµç¨‹å›ºæœ‰çš„å¼‚æ„æ€§å’ŒåŠ¨æ€æ€§é€šå¸¸å¯¼è‡´ç°æœ‰ç³»ç»Ÿä¸Šçš„ç¡¬ä»¶åˆ©ç”¨ç‡ä½å’Œè®­ç»ƒé€Ÿåº¦æ…¢ã€‚æœ¬æ–‡æå‡ºäº†RLinfï¼Œä¸€ä¸ªé«˜æ€§èƒ½çš„RLè®­ç»ƒç³»ç»Ÿï¼Œå…¶æ ¸å¿ƒåœ¨äºç³»ç»Ÿçµæ´»æ€§ã€‚ä¸ºäº†æœ€å¤§åŒ–çµæ´»æ€§å’Œæ•ˆç‡ï¼ŒRLinfåŸºäºä¸€ç§åä¸ºå®å¾®è§‚æµè½¬æ¢ï¼ˆM2Flowï¼‰çš„æ–°å‹RLç³»ç»Ÿè®¾è®¡èŒƒå¼æ„å»ºï¼Œè¯¥èŒƒå¼åœ¨æ—¶é—´å’Œç©ºé—´ç»´åº¦ä¸Šè‡ªåŠ¨åˆ†è§£é«˜çº§ã€æ˜“äºç»„åˆçš„RLå·¥ä½œæµç¨‹ï¼Œå¹¶å°†å®ƒä»¬é‡ç»„ä¸ºä¼˜åŒ–çš„æ‰§è¡Œæµã€‚åœ¨RLinfå·¥ä½œèŠ‚ç‚¹çš„è‡ªé€‚åº”é€šä¿¡èƒ½åŠ›çš„æ”¯æŒä¸‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸Šä¸‹æ–‡åˆ‡æ¢å’Œå¼¹æ€§æµæ°´çº¿æ¥å®ç°M2Flowè½¬æ¢ï¼Œå¹¶é‡‡ç”¨å‰–æå¼•å¯¼çš„è°ƒåº¦ç­–ç•¥æ¥ç”Ÿæˆæœ€ä¼˜æ‰§è¡Œè®¡åˆ’ã€‚åœ¨æ¨ç†RLå’Œå…·èº«RLä»»åŠ¡ä¸Šçš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼ŒRLinfå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„ç³»ç»Ÿï¼Œåœ¨ç«¯åˆ°ç«¯è®­ç»ƒååé‡æ–¹é¢å®ç°äº†1.1å€-2.13å€çš„åŠ é€Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿåœ¨å¤„ç†å¤§è§„æ¨¡ã€å¼‚æ„å’ŒåŠ¨æ€çš„è®­ç»ƒä»»åŠ¡æ—¶ï¼Œç”±äºç¼ºä¹è¶³å¤Ÿçš„çµæ´»æ€§ï¼Œå¯¼è‡´ç¡¬ä»¶èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ï¼Œè®­ç»ƒæ•ˆç‡ä¸é«˜ã€‚å…·ä½“æ¥è¯´ï¼Œä¸åŒçš„RLç®—æ³•å’Œä»»åŠ¡å¯¹è®¡ç®—ã€é€šä¿¡å’Œå­˜å‚¨çš„éœ€æ±‚å„ä¸ç›¸åŒï¼Œè€Œç°æœ‰çš„ç³»ç»Ÿéš¾ä»¥æ ¹æ®è¿™äº›éœ€æ±‚è¿›è¡ŒåŠ¨æ€è°ƒæ•´å’Œä¼˜åŒ–ã€‚è¿™ä½¿å¾—ç ”ç©¶äººå‘˜éš¾ä»¥å¿«é€Ÿè¿­ä»£å’Œéƒ¨ç½²æ–°çš„RLç®—æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRLinfçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥å®å¾®è§‚æµè½¬æ¢ï¼ˆM2Flowï¼‰èŒƒå¼ï¼Œå°†é«˜çº§çš„ã€æ˜“äºç»„åˆçš„RLå·¥ä½œæµç¨‹åˆ†è§£ä¸ºæ›´ç»†ç²’åº¦çš„ä»»åŠ¡å•å…ƒï¼Œå¹¶åœ¨æ—¶é—´å’Œç©ºé—´ç»´åº¦ä¸Šè¿›è¡Œä¼˜åŒ–é‡ç»„ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç³»ç»Ÿå¯ä»¥æ ¹æ®å®é™…çš„èµ„æºçŠ¶å†µå’Œä»»åŠ¡éœ€æ±‚ï¼ŒåŠ¨æ€åœ°è°ƒæ•´æ‰§è¡Œè®¡åˆ’ï¼Œä»è€Œæé«˜ç¡¬ä»¶åˆ©ç”¨ç‡å’Œè®­ç»ƒæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRLinfçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å·¥ä½œæµç¨‹åˆ†è§£å™¨ï¼šå°†é«˜çº§RLå·¥ä½œæµç¨‹åˆ†è§£ä¸ºç»†ç²’åº¦çš„ä»»åŠ¡å•å…ƒã€‚2) ä»»åŠ¡è°ƒåº¦å™¨ï¼šæ ¹æ®èµ„æºçŠ¶å†µå’Œä»»åŠ¡ä¾èµ–å…³ç³»ï¼Œç”Ÿæˆä¼˜åŒ–çš„æ‰§è¡Œè®¡åˆ’ã€‚3) è‡ªé€‚åº”é€šä¿¡æ¨¡å—ï¼šæ”¯æŒå·¥ä½œèŠ‚ç‚¹ä¹‹é—´çš„çµæ´»é€šä¿¡ï¼Œå®ç°ä»»åŠ¡å•å…ƒçš„ååŒæ‰§è¡Œã€‚4) å¼¹æ€§æµæ°´çº¿ï¼šé€šè¿‡ä¸Šä¸‹æ–‡åˆ‡æ¢å’Œæµæ°´çº¿æŠ€æœ¯ï¼Œæé«˜ä»»åŠ¡æ‰§è¡Œçš„å¹¶å‘åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šRLinfæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå®å¾®è§‚æµè½¬æ¢ï¼ˆM2Flowï¼‰èŒƒå¼ã€‚ä¸ä¼ ç»Ÿçš„é™æ€æ‰§è¡Œè®¡åˆ’ç›¸æ¯”ï¼ŒM2Flowèƒ½å¤Ÿæ ¹æ®å®é™…æƒ…å†µåŠ¨æ€åœ°è°ƒæ•´ä»»åŠ¡çš„æ‰§è¡Œé¡ºåºå’Œèµ„æºåˆ†é…ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”RLè®­ç»ƒçš„å¼‚æ„æ€§å’ŒåŠ¨æ€æ€§ã€‚æ­¤å¤–ï¼ŒRLinfçš„è‡ªé€‚åº”é€šä¿¡æ¨¡å—å’Œå¼¹æ€§æµæ°´çº¿ä¹Ÿä¸ºå®ç°é«˜æ•ˆçš„M2Flowè½¬æ¢æä¾›äº†å…³é”®æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šRLinfçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å‰–æå¼•å¯¼çš„è°ƒåº¦ç­–ç•¥ï¼šé€šè¿‡å¯¹RLå·¥ä½œæµç¨‹è¿›è¡Œå‰–æï¼Œäº†è§£å„ä¸ªä»»åŠ¡å•å…ƒçš„èµ„æºéœ€æ±‚ï¼Œä»è€Œç”Ÿæˆæœ€ä¼˜çš„æ‰§è¡Œè®¡åˆ’ã€‚2) ä¸Šä¸‹æ–‡åˆ‡æ¢æœºåˆ¶ï¼šæ”¯æŒå¿«é€Ÿåˆ‡æ¢ä¸åŒçš„ä»»åŠ¡å•å…ƒï¼Œæé«˜èµ„æºåˆ©ç”¨ç‡ã€‚3) å¼¹æ€§æµæ°´çº¿ï¼šé€šè¿‡å°†ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µï¼Œå¹¶é‡‡ç”¨æµæ°´çº¿æŠ€æœ¯ï¼Œæé«˜ä»»åŠ¡æ‰§è¡Œçš„å¹¶å‘åº¦ã€‚4) è‡ªé€‚åº”é€šä¿¡åè®®ï¼šæ ¹æ®ä»»åŠ¡å•å…ƒä¹‹é—´çš„æ•°æ®ä¾èµ–å…³ç³»ï¼Œé€‰æ‹©åˆé€‚çš„é€šä¿¡æ–¹å¼ï¼Œå‡å°‘é€šä¿¡å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRLinfåœ¨æ¨ç†RLå’Œå…·èº«RLä»»åŠ¡ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„ç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç«¯åˆ°ç«¯è®­ç»ƒååé‡æ–¹é¢ï¼ŒRLinfå®ç°äº†1.1å€-2.13å€çš„åŠ é€Ÿã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRLinfçš„å®å¾®è§‚æµè½¬æ¢èŒƒå¼èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜RLè®­ç»ƒçš„æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RLinfå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºåŠ é€Ÿå„ç§å¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„è®­ç»ƒï¼ŒåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ã€æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜è®­ç»ƒæ•ˆç‡ï¼ŒRLinfå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¿«åœ°æ¢ç´¢æ–°çš„RLç®—æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ›´å¤æ‚çš„å®é™…é—®é¢˜ä¸­ã€‚æ­¤å¤–ï¼ŒRLinfè¿˜å¯ä»¥é™ä½RLè®­ç»ƒçš„æˆæœ¬ï¼Œä½¿å…¶æ›´å®¹æ˜“è¢«å¹¿æ³›é‡‡ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has demonstrated immense potential in advancing artificial general intelligence, agentic intelligence, and embodied intelligence. However, the inherent heterogeneity and dynamicity of RL workflows often lead to low hardware utilization and slow training on existing systems. In this paper, we present RLinf, a high-performance RL training system based on our key observation that the major roadblock to efficient RL training lies in system flexibility. To maximize flexibility and efficiency, RLinf is built atop a novel RL system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically breaks down high-level, easy-to-compose RL workflows at both the temporal and spatial dimensions, and recomposes them into optimized execution flows. Supported by RLinf worker's adaptive communication capability, we devise context switching and elastic pipelining to realize M2Flow transformation, and a profiling-guided scheduling policy to generate optimal execution plans. Extensive evaluations on both reasoning RL and embodied RL tasks demonstrate that RLinf consistently outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in end-to-end training throughput.

