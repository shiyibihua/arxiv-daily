---
layout: default
title: Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations
---

# Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15981" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15981v2</a>
  <a href="https://arxiv.org/pdf/2509.15981.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15981v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15981v2', 'Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yujie Zhu, Charles A. Hepburn, Matthew Thorpe, Giovanni Montana

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19 (æ›´æ–°: 2025-10-31)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/YujieZhu7/SPReD)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SPReDï¼šåŸºäºä¸ç¡®å®šæ€§çš„å¹³æ»‘ç­–ç•¥æ­£åˆ™åŒ–ï¼Œæå‡å°‘æ ·æœ¬æ¼”ç¤ºå¼ºåŒ–å­¦ä¹ æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `ä¸ç¡®å®šæ€§é‡åŒ–` `ç­–ç•¥æ­£åˆ™åŒ–` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨æ¼”ç¤ºæ•°æ®æ—¶ï¼Œéš¾ä»¥å‡†ç¡®åˆ¤æ–­ä½•æ—¶åº”è¯¥æ¨¡ä»¿æ¼”ç¤ºç­–ç•¥ï¼Œä½•æ—¶åº”è¯¥éµå¾ªè‡ªèº«ç­–ç•¥ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚
2. SPReDæ¡†æ¶é€šè¿‡é›†æˆæ–¹æ³•å»ºæ¨¡Qå€¼åˆ†å¸ƒï¼Œé‡åŒ–æ¼”ç¤ºå’Œç­–ç•¥åŠ¨ä½œçš„ä¸ç¡®å®šæ€§ï¼Œå¹¶æ ¹æ®ä¸ç¡®å®šæ€§è‡ªé€‚åº”åœ°è°ƒæ•´æ¨¡ä»¿çš„å¼ºåº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSPReDåœ¨å¤šä¸ªæœºå™¨äººä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­æå‡æ˜¾è‘—ï¼Œå¹¶å¯¹æ¼”ç¤ºè´¨é‡å’Œæ•°é‡å…·æœ‰é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç¨€ç–å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ¼”ç¤ºæ•°æ®å¯ä»¥åŠ é€Ÿå­¦ä¹ ï¼Œä½†å¦‚ä½•å†³å®šä½•æ—¶æ¨¡ä»¿æ¼”ç¤ºä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºæ¼”ç¤ºçš„å¹³æ»‘ç­–ç•¥æ­£åˆ™åŒ–ï¼ˆSPReDï¼‰æ¡†æ¶ï¼Œå®ƒè§£å†³äº†æ ¸å¿ƒé—®é¢˜ï¼šæ™ºèƒ½ä½“åº”è¯¥ä½•æ—¶æ¨¡ä»¿æ¼”ç¤ºï¼Œä½•æ—¶éµå¾ªè‡ªå·±çš„ç­–ç•¥ï¼ŸSPReDä½¿ç”¨é›†æˆæ–¹æ³•æ˜¾å¼åœ°å»ºæ¨¡æ¼”ç¤ºå’Œç­–ç•¥åŠ¨ä½œçš„Qå€¼åˆ†å¸ƒï¼Œé‡åŒ–ä¸ç¡®å®šæ€§ä»¥è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å¼€å‘äº†ä¸¤ç§äº’è¡¥çš„ã€æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„æ–¹æ³•ï¼šä¸€ç§æ¦‚ç‡æ–¹æ³•ï¼Œç”¨äºä¼°è®¡æ¼”ç¤ºä¼˜è¶Šæ€§çš„å¯èƒ½æ€§ï¼›ä»¥åŠä¸€ç§åŸºäºä¼˜åŠ¿çš„æ–¹æ³•ï¼Œé€šè¿‡ç»Ÿè®¡æ˜¾è‘—æ€§æ¥ç¼©æ”¾æ¨¡ä»¿ã€‚ä¸è¿›è¡ŒäºŒå…ƒæ¨¡ä»¿å†³ç­–çš„ç°æœ‰æ–¹æ³•ï¼ˆä¾‹å¦‚Q-filterï¼‰ä¸åŒï¼ŒSPReDåº”ç”¨è¿ç»­çš„ã€ä¸ä¸ç¡®å®šæ€§æˆæ¯”ä¾‹çš„æ­£åˆ™åŒ–æƒé‡ï¼Œä»è€Œé™ä½è®­ç»ƒæœŸé—´çš„æ¢¯åº¦æ–¹å·®ã€‚å°½ç®¡è®¡ç®—ç®€å•ï¼ŒSPReDåœ¨å…«ä¸ªæœºå™¨äººä»»åŠ¡çš„å®éªŒä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šï¼Œåœ¨å¤æ‚çš„ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•é«˜è¾¾14å€ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹æ¼”ç¤ºè´¨é‡å’Œæ•°é‡çš„é²æ£’æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨https://github.com/YujieZhu7/SPReD è·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåœ¨ç¨€ç–å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ¼”ç¤ºæ•°æ®æ¥åŠ é€Ÿæ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œä¾‹å¦‚Q-filterï¼Œé€šå¸¸é‡‡ç”¨äºŒå…ƒå†³ç­–çš„æ–¹å¼æ¥å†³å®šæ˜¯å¦æ¨¡ä»¿æ¼”ç¤ºï¼Œè¿™ç§ç¡¬æ€§çš„æ¨¡ä»¿ç­–ç•¥å®¹æ˜“å¼•å…¥åå·®ï¼Œå¹¶ä¸”å¿½ç•¥äº†æ¼”ç¤ºæ•°æ®æœ¬èº«çš„ä¸ç¡®å®šæ€§ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆæœä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSPReDçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸ç¡®å®šæ€§æ¥å¹³æ»‘åœ°è°ƒèŠ‚æ¨¡ä»¿æ¼”ç¤ºçš„ç¨‹åº¦ã€‚é€šè¿‡å¯¹æ¼”ç¤ºæ•°æ®å’Œæ™ºèƒ½ä½“è‡ªèº«ç­–ç•¥çš„Qå€¼åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é‡åŒ–å…¶ä¸ç¡®å®šæ€§ï¼ŒSPReDå¯ä»¥æ ¹æ®ä¸ç¡®å®šæ€§çš„å¤§å°ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´æ¨¡ä»¿çš„æƒé‡ã€‚å½“æ™ºèƒ½ä½“å¯¹è‡ªèº«ç­–ç•¥çš„ä¼°è®¡è¶Šä¸ç¡®å®šæ—¶ï¼Œå°±æ›´å¤šåœ°ä¾èµ–æ¼”ç¤ºæ•°æ®ï¼›åä¹‹ï¼Œå½“æ™ºèƒ½ä½“å¯¹è‡ªèº«ç­–ç•¥çš„ä¼°è®¡è¶Šç¡®å®šæ—¶ï¼Œå°±æ›´å°‘åœ°ä¾èµ–æ¼”ç¤ºæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPReDæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) Qå€¼åˆ†å¸ƒå»ºæ¨¡ï¼šä½¿ç”¨é›†æˆæ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒQ-ensembleï¼‰åˆ†åˆ«å¯¹æ¼”ç¤ºæ•°æ®å’Œæ™ºèƒ½ä½“è‡ªèº«ç­–ç•¥çš„Qå€¼è¿›è¡Œå»ºæ¨¡ï¼Œå¾—åˆ°Qå€¼åˆ†å¸ƒã€‚2) ä¸ç¡®å®šæ€§é‡åŒ–ï¼šåŸºäºQå€¼åˆ†å¸ƒï¼Œè®¡ç®—æ¼”ç¤ºæ•°æ®å’Œæ™ºèƒ½ä½“è‡ªèº«ç­–ç•¥çš„ä¸ç¡®å®šæ€§ã€‚è®ºæ–‡æå‡ºäº†ä¸¤ç§ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼šä¸€ç§æ˜¯åŸºäºæ¦‚ç‡çš„æ–¹æ³•ï¼Œä¼°è®¡æ¼”ç¤ºæ•°æ®ä¼˜äºæ™ºèƒ½ä½“è‡ªèº«ç­–ç•¥çš„å¯èƒ½æ€§ï¼›å¦ä¸€ç§æ˜¯åŸºäºä¼˜åŠ¿å‡½æ•°çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç»Ÿè®¡æ˜¾è‘—æ€§æ¥ç¼©æ”¾æ¨¡ä»¿çš„æƒé‡ã€‚3) å¹³æ»‘ç­–ç•¥æ­£åˆ™åŒ–ï¼šæ ¹æ®é‡åŒ–çš„ä¸ç¡®å®šæ€§ï¼Œå¯¹æ™ºèƒ½ä½“çš„ç­–ç•¥è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä½¿å¾—æ™ºèƒ½ä½“åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå¹³æ»‘åœ°ä»æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šSPReDçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åˆ©ç”¨ä¸ç¡®å®šæ€§è¿›è¡Œå¹³æ»‘ç­–ç•¥æ­£åˆ™åŒ–çš„æ€æƒ³ã€‚ä¸ä¼ ç»Ÿçš„äºŒå…ƒæ¨¡ä»¿ç­–ç•¥ä¸åŒï¼ŒSPReDé‡‡ç”¨è¿ç»­çš„ã€ä¸ä¸ç¡®å®šæ€§æˆæ¯”ä¾‹çš„æ­£åˆ™åŒ–æƒé‡ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæœŸé—´çš„æ¢¯åº¦æ–¹å·®ï¼Œæé«˜äº†å­¦ä¹ çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒSPReDæ¡†æ¶å¯ä»¥çµæ´»åœ°ç»“åˆä¸åŒçš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œå…·æœ‰è¾ƒå¼ºçš„é€šç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Qå€¼åˆ†å¸ƒå»ºæ¨¡æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†Q-ensembleæ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒå¤šä¸ªQå‡½æ•°æ¥ä¼°è®¡Qå€¼åˆ†å¸ƒã€‚åœ¨ä¸ç¡®å®šæ€§é‡åŒ–æ–¹é¢ï¼Œè®ºæ–‡æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼šæ¦‚ç‡æ–¹æ³•å’Œä¼˜åŠ¿å‡½æ•°æ–¹æ³•ã€‚æ¦‚ç‡æ–¹æ³•é€šè¿‡è®¡ç®—æ¼”ç¤ºæ•°æ®Qå€¼å¤§äºæ™ºèƒ½ä½“è‡ªèº«ç­–ç•¥Qå€¼çš„æ¦‚ç‡æ¥ä¼°è®¡ä¸ç¡®å®šæ€§ã€‚ä¼˜åŠ¿å‡½æ•°æ–¹æ³•åˆ™åˆ©ç”¨ç»Ÿè®¡æ˜¾è‘—æ€§æ¥ç¼©æ”¾æ¨¡ä»¿çš„æƒé‡ã€‚åœ¨ç­–ç•¥æ­£åˆ™åŒ–æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§å¹³æ»‘çš„æ­£åˆ™åŒ–æ–¹å¼ï¼Œä½¿å¾—æ™ºèƒ½ä½“åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå¹³æ»‘åœ°ä»æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SPReDåœ¨å…«ä¸ªæœºå™¨äººä»»åŠ¡ä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒSPReDåœ¨å¤æ‚çš„ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•é«˜è¾¾14å€ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹æ¼”ç¤ºè´¨é‡å’Œæ•°é‡çš„é²æ£’æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸé¡¹ä»»åŠ¡ä¸­ï¼ŒSPReDä»…ä½¿ç”¨å°‘é‡æ¼”ç¤ºæ•°æ®å°±è¾¾åˆ°äº†ä¸ç°æœ‰æ–¹æ³•ä½¿ç”¨å¤§é‡æ¼”ç¤ºæ•°æ®ç›¸å½“çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SPReDæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦åˆ©ç”¨å°‘é‡æ¼”ç¤ºæ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæé«˜æ™ºèƒ½ä½“çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ï¼Œé™ä½å¯¹å¤§é‡é«˜è´¨é‡æ¼”ç¤ºæ•°æ®çš„ä¾èµ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD.

