---
layout: default
title: Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds
---

# Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15915" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15915v1</a>
  <a href="https://arxiv.org/pdf/2509.15915.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15915v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15915v1', 'Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Remo Sasso, Michelangelo Conserva, Dominik Jeurissen, Paulo Rauber

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: 20 pages, 9 figures. Accepted for presentation at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Embodied World Models for Decision Making

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºFoundation Modelçš„ä¸–ç•Œæ¨¡å‹ä¸æ™ºèƒ½ä½“ï¼Œæå‡æ–‡æœ¬ç½‘æ ¼ä¸–ç•Œä¸­çš„å¼ºåŒ–å­¦ä¹ æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Foundation Model` `ä¸–ç•Œæ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ–‡æœ¬ç½‘æ ¼ä¸–ç•Œ` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­ï¼Œç”±äºäº¤äº’æˆæœ¬é«˜æ˜‚ï¼Œæ ·æœ¬æ•ˆç‡æˆä¸ºç“¶é¢ˆã€‚
2. åˆ©ç”¨Foundation Modelçš„å…ˆéªŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œåˆ†åˆ«æ„å»ºä¸–ç•Œæ¨¡å‹ï¼ˆFWMï¼‰å’Œæ™ºèƒ½ä½“ï¼ˆFAï¼‰ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMçš„è¿›æ­¥èƒ½ç›´æ¥æå‡FWMå’ŒFAçš„æ€§èƒ½ï¼ŒFWMä¸å¼ºåŒ–å­¦ä¹ ç»“åˆåœ¨å¤æ‚ç¯å¢ƒä¸­æœ‰æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶å¦‚ä½•å°†é¢„è®­ç»ƒçš„Foundation Modelï¼ˆFMï¼‰é›†æˆåˆ°å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ï¼Œä»¥æé«˜æ ·æœ¬æ•ˆç‡ã€‚é’ˆå¯¹äº¤äº’ä»£ä»·é«˜æ˜‚çš„ç°å®ä¸–ç•Œåº”ç”¨ï¼Œä»é›¶å¼€å§‹çš„å¼ºåŒ–å­¦ä¹ æ•ˆç‡è¾ƒä½ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§ç­–ç•¥ï¼šä¸€æ˜¯åˆ©ç”¨FMçš„å…ˆéªŒçŸ¥è¯†æ„å»ºFoundation World Modelï¼ˆFWMï¼‰ï¼Œé€šè¿‡æ¨¡æ‹Ÿäº¤äº’è®­ç»ƒå’Œè¯„ä¼°æ™ºèƒ½ä½“ï¼›äºŒæ˜¯åˆ©ç”¨FMçš„æ¨ç†èƒ½åŠ›æ„å»ºFoundation Agentï¼ˆFAï¼‰ï¼Œç›´æ¥è¿›è¡Œå†³ç­–ã€‚åœ¨æ–‡æœ¬ç½‘æ ¼ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒLLMçš„æ”¹è¿›èƒ½å¤Ÿè½¬åŒ–ä¸ºæ›´å¥½çš„FWMå’ŒFAã€‚åŸºäºå½“å‰LLMçš„FAåœ¨ç®€å•ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œè€ŒFWMä¸å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„ç»“åˆåœ¨å¤æ‚ç¯å¢ƒä¸­å…·æœ‰æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éƒ¨åˆ†å¯è§‚æµ‹å’Œéšæœºç¯å¢ƒä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­æ ·æœ¬æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡çš„ç¯å¢ƒäº¤äº’æ‰èƒ½å­¦ä¹ åˆ°æœ‰æ•ˆçš„ç­–ç•¥ï¼Œè¿™åœ¨äº¤äº’æˆæœ¬é«˜æ˜‚çš„åœºæ™¯ä¸­æ˜¯ä¸å¯æ¥å—çš„ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒçš„Foundation Modelæ¥æå‡å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Foundation Modelçš„ä¸¤ç§èƒ½åŠ›ï¼šä¸€æ˜¯åˆ©ç”¨å…¶å…ˆéªŒçŸ¥è¯†æ„å»ºä¸–ç•Œæ¨¡å‹ï¼Œä»è€Œå¯ä»¥åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œé™ä½çœŸå®ç¯å¢ƒçš„äº¤äº’æˆæœ¬ï¼›äºŒæ˜¯åˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›ç›´æ¥è¿›è¡Œå†³ç­–ï¼Œæ„å»ºæ™ºèƒ½ä½“ï¼Œä»è€Œé¿å…ä»é›¶å¼€å§‹å­¦ä¹ ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦åˆ†æ”¯ï¼šåŸºäºFoundation World Modelï¼ˆFWMï¼‰çš„å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºFoundation Agentï¼ˆFAï¼‰çš„ç›´æ¥å†³ç­–ã€‚åœ¨FWMåˆ†æ”¯ä¸­ï¼Œé¦–å…ˆä½¿ç”¨FMæ„å»ºä¸–ç•Œæ¨¡å‹ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ä¸–ç•Œæ¨¡å‹ä¸­è®­ç»ƒæ™ºèƒ½ä½“ã€‚åœ¨FAåˆ†æ”¯ä¸­ï¼Œç›´æ¥ä½¿ç”¨FMä½œä¸ºæ™ºèƒ½ä½“ï¼Œæ ¹æ®ç¯å¢ƒçŠ¶æ€è¿›è¡Œå†³ç­–ã€‚ä¸¤ä¸ªåˆ†æ”¯éƒ½åœ¨æ–‡æœ¬ç½‘æ ¼ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸¤ç§åˆ©ç”¨Foundation Modelè¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ï¼Œå¹¶å¯¹è¿™ä¸¤ç§ç­–ç•¥è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼ŒFWMçš„åˆ›æ–°åœ¨äºåˆ©ç”¨FMçš„å…ˆéªŒçŸ¥è¯†æ„å»ºä¸–ç•Œæ¨¡å‹ï¼Œä»è€Œå¯ä»¥åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œé™ä½çœŸå®ç¯å¢ƒçš„äº¤äº’æˆæœ¬ã€‚FAçš„åˆ›æ–°åœ¨äºåˆ©ç”¨FMçš„æ¨ç†èƒ½åŠ›ç›´æ¥è¿›è¡Œå†³ç­–ï¼Œä»è€Œé¿å…ä»é›¶å¼€å§‹å­¦ä¹ ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ–‡æœ¬ç½‘æ ¼ä¸–ç•Œç¯å¢ƒä½œä¸ºè¯„ä¼°å¹³å°ï¼Œè¯¥ç¯å¢ƒé€‚åˆå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼›2) è®¾è®¡äº†åŸºäºLLMçš„FWMå’ŒFAï¼Œå¹¶é’ˆå¯¹ä¸åŒçš„ç¯å¢ƒå¤æ‚åº¦è¿›è¡Œäº†è°ƒæ•´ï¼›3) ä½¿ç”¨æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚Q-learningï¼‰åœ¨FWMä¸­è®­ç»ƒæ™ºèƒ½ä½“ï¼›4) å¯¹FWMå’ŒFAçš„æ€§èƒ½è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬åœ¨ä¸åŒç¯å¢ƒä¸‹çš„æ ·æœ¬æ•ˆç‡å’Œç­–ç•¥è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMçš„æ”¹è¿›èƒ½å¤Ÿè½¬åŒ–ä¸ºæ›´å¥½çš„FWMå’ŒFAã€‚åŸºäºå½“å‰LLMçš„FAåœ¨è¶³å¤Ÿç®€å•çš„ç¯å¢ƒä¸­å¯ä»¥æä¾›å‡ºè‰²çš„ç­–ç•¥ã€‚FWMä¸å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„ç»“åˆåœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­å…·æœ‰å¾ˆé«˜çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éƒ¨åˆ†å¯è§‚æµ‹å’Œéšæœºç¯å¢ƒä¸­ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFoundation Modelåœ¨å¼ºåŒ–å­¦ä¹ ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„Foundation Modelï¼Œå¯ä»¥æ˜¾è‘—é™ä½å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬éœ€æ±‚ï¼ŒåŠ é€Ÿæ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å­¦ä¹ å’Œéƒ¨ç½²ã€‚æœªæ¥ï¼Œå¯ä»¥å°†è¯¥æ–¹æ³•æ‰©å±•åˆ°æ›´å¤æ‚çš„ç¯å¢ƒå’Œä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚ï¼Œåœ¨çœŸå®æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œå®éªŒï¼Œæˆ–è€…åœ¨æ›´å¤æ‚çš„æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While reinforcement learning from scratch has shown impressive results in solving sequential decision-making tasks with efficient simulators, real-world applications with expensive interactions require more sample-efficient agents. Foundation models (FMs) are natural candidates to improve sample efficiency as they possess broad knowledge and reasoning capabilities, but it is yet unclear how to effectively integrate them into the reinforcement learning framework. In this paper, we anticipate and, most importantly, evaluate two promising strategies. First, we consider the use of foundation world models (FWMs) that exploit the prior knowledge of FMs to enable training and evaluating agents with simulated interactions. Second, we consider the use of foundation agents (FAs) that exploit the reasoning capabilities of FMs for decision-making. We evaluate both approaches empirically in a family of grid-world environments that are suitable for the current generation of large language models (LLMs). Our results suggest that improvements in LLMs already translate into better FWMs and FAs; that FAs based on current LLMs can already provide excellent policies for sufficiently simple environments; and that the coupling of FWMs and reinforcement learning agents is highly promising for more complex settings with partial observability and stochastic elements.

