---
layout: default
title: Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers
---

# Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15498" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15498v1</a>
  <a href="https://arxiv.org/pdf/2509.15498.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15498v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15498v1', 'Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zahra Aref, Narayan B. Mandayam

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EWA-VQ-ODTï¼šåˆ©ç”¨ç»éªŒåŠ æƒå¸å¼•åŠ›æ”¹è¿›åœ¨çº¿å†³ç­–Transformerçš„æ ·æœ¬æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åœ¨çº¿å†³ç­–Transformer` `ç»éªŒåŠ æƒå¸å¼•åŠ›` `å‘é‡é‡åŒ–` `è¿ç»­æ§åˆ¶` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åœ¨çº¿å†³ç­–Transformer (ODT) ä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›ï¼Œç¼ºä¹å¯¹åŠ¨ä½œç»“æœçš„è®°å¿†ï¼Œå¯¼è‡´å­¦ä¹ é•¿æœŸåŠ¨ä½œæ•ˆæœçš„æ•ˆç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºEWA-VQ-ODTï¼Œé€šè¿‡ç»´æŠ¤æ¯ä¸ªåŠ¨ä½œçš„å¿ƒç†è´¦æˆ·ï¼Œè®°å½•è¿‘æœŸæˆåŠŸå’Œå¤±è´¥ï¼Œå¹¶ç”¨ç»éªŒåŠ æƒå¸å¼•åŠ›è°ƒèŠ‚æ³¨æ„åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒEWA-VQ-ODTåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºODTï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡å’Œå¹³å‡å›æŠ¥ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒåˆæœŸã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Transformerå·²æˆä¸ºåºåˆ—å†³ç­–çš„å¼ºå¤§æ¶æ„ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡è½¨è¿¹ã€‚åœ¨å¼ºåŒ–å­¦ä¹ (RL)ä¸­ï¼Œå®ƒä»¬æ— éœ€å€¼å‡½æ•°è¿‘ä¼¼å³å¯å®ç°å›æŠ¥æ¡ä»¶æ§åˆ¶ã€‚å†³ç­–Transformer (DTs) å°†RLè§†ä¸ºç›‘ç£åºåˆ—å»ºæ¨¡ï¼Œä½†ä»…é™äºç¦»çº¿æ•°æ®ä¸”ç¼ºä¹æ¢ç´¢ã€‚åœ¨çº¿å†³ç­–Transformer (ODTs) é€šè¿‡å¯¹on-policy rolloutsè¿›è¡Œç†µæ­£åˆ™åŒ–è®­ç»ƒæ¥è§£å†³æ­¤é™åˆ¶ï¼Œä¸ºä¼ ç»Ÿçš„RLæ–¹æ³•ï¼ˆå¦‚Soft Actor-Criticï¼‰æä¾›äº†ä¸€ç§ç¨³å®šçš„æ›¿ä»£æ–¹æ¡ˆï¼Œåè€…ä¾èµ–äºè‡ªä¸¾ç›®æ ‡å’Œå¥–åŠ±å¡‘é€ ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒODTsä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›ï¼Œç¼ºä¹å¯¹ç‰¹å®šåŠ¨ä½œç»“æœçš„æ˜¾å¼è®°å¿†ï¼Œå¯¼è‡´å­¦ä¹ é•¿æœŸåŠ¨ä½œæœ‰æ•ˆæ€§çš„æ•ˆç‡ä½ä¸‹ã€‚å—ç»éªŒåŠ æƒå¸å¼•åŠ› (EWA) ç­‰è®¤çŸ¥æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†EWA-VQ-ODTï¼Œå®ƒæ˜¯ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œç»´æŠ¤æ¯ä¸ªåŠ¨ä½œçš„å¿ƒç†è´¦æˆ·ï¼Œæ€»ç»“æœ€è¿‘çš„æˆåŠŸå’Œå¤±è´¥ã€‚è¿ç»­åŠ¨ä½œé€šè¿‡ç›´æ¥ç½‘æ ¼æŸ¥æ‰¾è·¯ç”±åˆ°ç´§å‡‘çš„å‘é‡é‡åŒ–ç æœ¬ï¼Œå…¶ä¸­æ¯ä¸ªç å­˜å‚¨ä¸€ä¸ªæ ‡é‡å¸å¼•åŠ›ï¼Œè¯¥å¸å¼•åŠ›é€šè¿‡è¡°å‡å’ŒåŸºäºå¥–åŠ±çš„å¼ºåŒ–åœ¨çº¿æ›´æ–°ã€‚è¿™äº›å¸å¼•åŠ›é€šè¿‡åç½®ä¸åŠ¨ä½œtokenç›¸å…³çš„åˆ—æ¥è°ƒèŠ‚æ³¨æ„åŠ›ï¼Œæ— éœ€æ›´æ”¹backboneæˆ–è®­ç»ƒç›®æ ‡ã€‚åœ¨æ ‡å‡†è¿ç»­æ§åˆ¶åŸºå‡†ä¸Šï¼ŒEWA-VQ-ODTæé«˜äº†æ ·æœ¬æ•ˆç‡å’Œå¹³å‡å›æŠ¥ï¼Œå°¤å…¶æ˜¯åœ¨æ—©æœŸè®­ç»ƒä¸­ã€‚è¯¥æ¨¡å—è®¡ç®—æ•ˆç‡é«˜ï¼Œå¯é€šè¿‡æ¯ä¸ªä»£ç çš„è½¨è¿¹è¿›è¡Œè§£é‡Šï¼Œå¹¶å¾—åˆ°ç†è®ºä¿è¯çš„æ”¯æŒï¼Œè¿™äº›ä¿è¯é™åˆ¶äº†å¸å¼•åŠ›åŠ¨æ€åŠå…¶å¯¹æ³¨æ„åŠ›æ¼‚ç§»çš„å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åœ¨çº¿å†³ç­–Transformer (ODT) åœ¨å¤„ç†è¿ç»­æ§åˆ¶ä»»åŠ¡æ—¶ï¼Œç”±äºç¼ºä¹å¯¹åŠ¨ä½œå†å²çš„æœ‰æ•ˆè®°å¿†æœºåˆ¶ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶æ˜¯åœ¨æ¢ç´¢é˜¶æ®µï¼Œéš¾ä»¥å¿«é€Ÿè¯†åˆ«å’Œåˆ©ç”¨æœ‰æ•ˆçš„åŠ¨ä½œåºåˆ—ã€‚æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶å¹³ç­‰å¯¹å¾…æ‰€æœ‰å†å²ä¿¡æ¯ï¼Œæ— æ³•åŒºåˆ†ä¸åŒåŠ¨ä½œå¸¦æ¥çš„é•¿æœŸå½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šå€Ÿé‰´è®¤çŸ¥æ¨¡å‹ä¸­çš„ç»éªŒåŠ æƒå¸å¼•åŠ› (EWA) æ¦‚å¿µï¼Œä¸ºæ¯ä¸ªåŠ¨ä½œç»´æŠ¤ä¸€ä¸ªâ€œå¿ƒç†è´¦æˆ·â€ï¼Œè®°å½•å…¶è¿‘æœŸè¡¨ç°ã€‚é€šè¿‡é‡åŒ–åŠ¨ä½œç©ºé—´ï¼Œå°†è¿ç»­åŠ¨ä½œæ˜ å°„åˆ°ç¦»æ•£çš„ç æœ¬ï¼Œå¹¶ä¸ºæ¯ä¸ªç ç»´æŠ¤ä¸€ä¸ªå¸å¼•åŠ›å€¼ï¼Œè¯¥å€¼æ ¹æ®åŠ¨ä½œçš„å¥–åŠ±è¿›è¡Œæ›´æ–°ã€‚å¸å¼•åŠ›å€¼ç”¨äºè°ƒèŠ‚æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œä½¿æ¨¡å‹æ›´åŠ å…³æ³¨è¿‘æœŸè¡¨ç°è‰¯å¥½çš„åŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEWA-VQ-ODTåœ¨æ ‡å‡†çš„åœ¨çº¿å†³ç­–Transformer (ODT) æ¡†æ¶ä¸Šå¢åŠ äº†ä¸€ä¸ªè½»é‡çº§çš„EWAæ¨¡å—ã€‚è¯¥æ¨¡å—åŒ…å«ä¸€ä¸ªå‘é‡é‡åŒ– (VQ) ç æœ¬ï¼Œç”¨äºå°†è¿ç»­åŠ¨ä½œç¦»æ•£åŒ–ã€‚æ¯ä¸ªç å¯¹åº”ä¸€ä¸ªå¸å¼•åŠ›å€¼ï¼Œè¯¥å€¼é€šè¿‡è¡°å‡å’ŒåŸºäºå¥–åŠ±çš„å¼ºåŒ–è¿›è¡Œåœ¨çº¿æ›´æ–°ã€‚åœ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡æ—¶ï¼ŒEWAæ¨¡å—ä¼šæ ¹æ®åŠ¨ä½œå¯¹åº”çš„å¸å¼•åŠ›å€¼ï¼Œå¯¹æ³¨æ„åŠ›çŸ©é˜µçš„ç›¸åº”åˆ—è¿›è¡Œåç½®ã€‚æ•´ä½“è®­ç»ƒæµç¨‹ä¸ODTç›¸åŒï¼Œæ— éœ€ä¿®æ”¹backboneæˆ–è®­ç»ƒç›®æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ç»éªŒåŠ æƒå¸å¼•åŠ› (EWA) çš„æ€æƒ³å¼•å…¥åˆ°Transformerçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œé€šè¿‡ç»´æŠ¤åŠ¨ä½œçš„å¿ƒç†è´¦æˆ·æ¥å¢å¼ºæ¨¡å‹å¯¹åŠ¨ä½œå†å²çš„è®°å¿†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼ŒEWA-VQ-ODTèƒ½å¤Ÿæ›´åŠ æœ‰æ•ˆåœ°åˆ©ç”¨å†å²ä¿¡æ¯ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å‘é‡é‡åŒ– (VQ) ç æœ¬å°†è¿ç»­åŠ¨ä½œç¦»æ•£åŒ–ï¼Œä½¿å¾—EWAæ¨¡å—èƒ½å¤Ÿå¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´ã€‚

**å…³é”®è®¾è®¡**ï¼šè¿ç»­åŠ¨ä½œé€šè¿‡ç½‘æ ¼æŸ¥æ‰¾æ˜ å°„åˆ°VQç æœ¬ã€‚æ¯ä¸ªç æœ¬æ¡ç›®ç»´æŠ¤ä¸€ä¸ªæ ‡é‡å¸å¼•åŠ›å€¼ï¼Œè¯¥å€¼æ ¹æ®ä»¥ä¸‹å…¬å¼æ›´æ–°ï¼š$A_{t+1}(a) = (1 - \beta) A_t(a) + \beta r_t$ï¼Œå…¶ä¸­ $A_t(a)$ æ˜¯åŠ¨ä½œ $a$ åœ¨æ—¶é—´æ­¥ $t$ çš„å¸å¼•åŠ›å€¼ï¼Œ$eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$r_t$ æ˜¯æ—¶é—´æ­¥ $t$ çš„å¥–åŠ±ã€‚å¸å¼•åŠ›å€¼ç”¨äºåç½®æ³¨æ„åŠ›æƒé‡ï¼Œå…·ä½“è€Œè¨€ï¼Œæ³¨æ„åŠ›çŸ©é˜µçš„ç¬¬ $i$ è¡Œç¬¬ $j$ åˆ—çš„æƒé‡ $w_{ij}$ è¢«ä¿®æ”¹ä¸º $w_{ij} \cdot exp(A(a_j))$ï¼Œå…¶ä¸­ $a_j$ æ˜¯ç¬¬ $j$ ä¸ªåŠ¨ä½œå¯¹åº”çš„ç æœ¬æ¡ç›®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EWA-VQ-ODTåœ¨æ ‡å‡†è¿ç»­æ§åˆ¶åŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºODTï¼ŒEWA-VQ-ODTåœ¨æ ·æœ¬æ•ˆç‡å’Œå¹³å‡å›æŠ¥æ–¹é¢å‡æœ‰æ‰€æå‡ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒåˆæœŸã€‚å…·ä½“è€Œè¨€ï¼ŒEWA-VQ-ODTèƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¹¶åœ¨æ›´å°‘çš„è®­ç»ƒæ ·æœ¬ä¸‹è¾¾åˆ°æ›´é«˜çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æä¾›äº†ç†è®ºä¿è¯ï¼Œé™åˆ¶äº†å¸å¼•åŠ›åŠ¨æ€åŠå…¶å¯¹æ³¨æ„åŠ›æ¼‚ç§»çš„å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆæ¢ç´¢å’Œé•¿æœŸè®°å¿†çš„è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ã€‚é€šè¿‡å¢å¼ºæ¨¡å‹å¯¹åŠ¨ä½œå†å²çš„è®°å¿†èƒ½åŠ›ï¼Œå¯ä»¥æé«˜æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡å’Œå†³ç­–èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æ§åˆ¶ç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒEWA-VQ-ODTçš„è½»é‡çº§è®¾è®¡ä½¿å…¶æ˜“äºé›†æˆåˆ°ç°æœ‰çš„Transformeræ¶æ„ä¸­ï¼Œå…·æœ‰è‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformers have emerged as a compelling architecture for sequential decision-making by modeling trajectories via self-attention. In reinforcement learning (RL), they enable return-conditioned control without relying on value function approximation. Decision Transformers (DTs) exploit this by casting RL as supervised sequence modeling, but they are restricted to offline data and lack exploration. Online Decision Transformers (ODTs) address this limitation through entropy-regularized training on on-policy rollouts, offering a stable alternative to traditional RL methods like Soft Actor-Critic, which depend on bootstrapped targets and reward shaping. Despite these advantages, ODTs use standard attention, which lacks explicit memory of action-specific outcomes. This leads to inefficiencies in learning long-term action effectiveness. Inspired by cognitive models such as Experience-Weighted Attraction (EWA), we propose Experience-Weighted Attraction with Vector Quantization for Online Decision Transformers (EWA-VQ-ODT), a lightweight module that maintains per-action mental accounts summarizing recent successes and failures. Continuous actions are routed via direct grid lookup to a compact vector-quantized codebook, where each code stores a scalar attraction updated online through decay and reward-based reinforcement. These attractions modulate attention by biasing the columns associated with action tokens, requiring no change to the backbone or training objective. On standard continuous-control benchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT, particularly in early training. The module is computationally efficient, interpretable via per-code traces, and supported by theoretical guarantees that bound the attraction dynamics and its impact on attention drift.

