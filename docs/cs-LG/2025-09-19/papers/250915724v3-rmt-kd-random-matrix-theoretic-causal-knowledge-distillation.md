---
layout: default
title: RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation
---

# RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15724" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15724v3</a>
  <a href="https://arxiv.org/pdf/2509.15724.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15724v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15724v3', 'RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Davide Ettori, Nastaran Darabi, Sureshkumar Senthilkumar, Amit Ranjan Trivedi

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19 (æ›´æ–°: 2025-09-29)

**å¤‡æ³¨**: 5 pages, submitted to ICASSP 2026, September 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRMT-KDä»¥è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `éšæœºçŸ©é˜µç†è®º` `æ¨¡å‹å‹ç¼©` `æ·±åº¦å­¦ä¹ ` `è¾¹ç¼˜è®¡ç®—` `è‡ªè’¸é¦` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²é¢ä¸´é«˜è®¡ç®—å’Œå­˜å‚¨æˆæœ¬çš„æŒ‘æˆ˜ã€‚
2. RMT-KDé€šè¿‡éšæœºçŸ©é˜µç†è®ºè¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œé€å±‚ä¿ç•™æœ‰ç”¨çš„ä¿¡æ¯æ–¹å‘ï¼Œä»è€Œæœ‰æ•ˆå‹ç¼©æ¨¡å‹ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒRMT-KDå®ç°äº†æ˜¾è‘—çš„å‚æ•°å‡å°‘å’Œæ¨ç†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚BERTå’ŒResNetåœ¨æ€§èƒ½ä¸Šå¤„äºé¢†å…ˆåœ°ä½ï¼Œä½†ç”±äºå…¶ä½“ç§¯å’Œè®¡ç®—éœ€æ±‚ï¼Œéš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRMT-KDçš„å‹ç¼©æ–¹æ³•ï¼Œåˆ©ç”¨éšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œé€æ­¥å‡å°‘ç½‘ç»œè§„æ¨¡ã€‚RMT-KDé€šè¿‡ä¿ç•™éšè—è¡¨ç¤ºçš„è°±ç‰¹æ€§è¯†åˆ«çš„æœ‰ä¿¡æ¯æ–¹å‘ï¼Œé¿å…äº†å‰ªææˆ–å¯å‘å¼ç§©é€‰æ‹©ã€‚è¯¥æ–¹æ³•é€å±‚åº”ç”¨RMTå› æœé™ç»´ï¼Œå¹¶ç»“åˆè‡ªè’¸é¦ä»¥ä¿æŒç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚åœ¨GLUEã€AG Newså’ŒCIFAR-10æ•°æ®é›†ä¸Šï¼ŒRMT-KDå®ç°äº†é«˜è¾¾80%çš„å‚æ•°å‡å°‘ï¼Œä»…æŸå¤±2%çš„å‡†ç¡®ç‡ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†2.8å€ï¼ŒåŠŸè€—å‡ ä¹å‡åŠã€‚è¿™äº›ç»“æœç¡®ç«‹äº†RMT-KDä½œä¸ºä¸€ç§æ•°å­¦åŸºç¡€çš„ç½‘ç»œè’¸é¦æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶çš„é«˜è®¡ç®—å’Œå­˜å‚¨æˆæœ¬é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚å‰ªæå’Œå¯å‘å¼ç§©é€‰æ‹©å­˜åœ¨ä¿¡æ¯æŸå¤±å’Œä¸ç¨³å®šæ€§ç­‰ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRMT-KDçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨éšæœºçŸ©é˜µç†è®ºï¼Œé€šè¿‡åˆ†æéšè—è¡¨ç¤ºçš„è°±ç‰¹æ€§ï¼Œä¿ç•™æœ‰ä¿¡æ¯çš„æ–¹å‘ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„çŸ¥è¯†è’¸é¦å’Œæ¨¡å‹å‹ç¼©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRMT-KDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é€å±‚åº”ç”¨RMTå› æœé™ç»´å’Œè‡ªè’¸é¦è¿‡ç¨‹ã€‚é¦–å…ˆï¼Œé€šè¿‡RMTåˆ†æéšè—å±‚çš„ç‰¹å¾ï¼Œç„¶åé€‰æ‹©ä¿ç•™çš„æ–¹å‘ï¼Œæœ€åè¿›è¡Œè‡ªè’¸é¦ä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šRMT-KDçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥éšæœºçŸ©é˜µç†è®ºä½œä¸ºçŸ¥è¯†è’¸é¦çš„åŸºç¡€ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„å‰ªææ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™æ¨¡å‹çš„æœ‰æ•ˆä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒRMT-KDé€šè¿‡è°±ç‰¹æ€§é€‰æ‹©ä¿ç•™çš„æ–¹å‘ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºå¹³è¡¡å‹ç¼©ç‡ä¸å‡†ç¡®ç‡ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™é‡‡ç”¨å±‚çº§è‡ªè’¸é¦ç­–ç•¥ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RMT-KDåœ¨GLUEã€AG Newså’ŒCIFAR-10æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæœ€é«˜å®ç°80%çš„å‚æ•°å‡å°‘ï¼Œå‡†ç¡®ç‡ä»…æŸå¤±2%ã€‚æ­¤å¤–ï¼Œæ¨ç†é€Ÿåº¦æå‡äº†2.8å€ï¼ŒåŠŸè€—å‡ ä¹å‡åŠï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿæ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RMT-KDçš„ç ”ç©¶æˆæœåœ¨è¾¹ç¼˜è®¡ç®—ã€ç§»åŠ¨è®¾å¤‡å’Œç‰©è”ç½‘ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æœ‰æ•ˆå‹ç¼©æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒRMT-KDèƒ½å¤Ÿåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼Œæ¨åŠ¨æ™ºèƒ½è®¾å¤‡çš„æ™®åŠå’Œåº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨å®æ—¶æ¨ç†å’Œä½åŠŸè€—è®¡ç®—åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large deep learning models such as BERT and ResNet achieve state-of-the-art performance but are costly to deploy at the edge due to their size and compute demands. We present RMT-KD, a compression method that leverages Random Matrix Theory (RMT) for knowledge distillation to iteratively reduce network size. Instead of pruning or heuristic rank selection, RMT-KD preserves only informative directions identified via the spectral properties of hidden representations. RMT-based causal reduction is applied layer by layer with self-distillation to maintain stability and accuracy. On GLUE, AG News, and CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy loss, delivering 2.8x faster inference and nearly halved power consumption. These results establish RMT-KD as a mathematically grounded approach to network distillation.

