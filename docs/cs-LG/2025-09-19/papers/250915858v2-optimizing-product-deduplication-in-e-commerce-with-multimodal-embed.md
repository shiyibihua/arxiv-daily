---
layout: default
title: Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings
---

# Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15858" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15858v2</a>
  <a href="https://arxiv.org/pdf/2509.15858.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15858v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15858v2', 'Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aysenur Kulunk, Berk Taskin, M. Furkan Eseoglu, H. Bahadir Sahin

**åˆ†ç±»**: cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19 (æ›´æ–°: 2025-12-01)

**å¤‡æ³¨**: 8 pages, accepted to 2025 IEEE International Conference on Big Data, Industrial and Goverment Track

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºå¤šæ¨¡æ€åµŒå…¥çš„ç”µå•†å•†å“å»é‡æ–¹æ³•ï¼Œæå‡å¤§è§„æ¨¡å•†å“ç›®å½•ä¸‹çš„å»é‡ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å•†å“å»é‡` `å¤šæ¨¡æ€åµŒå…¥` `BERT` `Masked Autoencoders` `å‘é‡æ•°æ®åº“` `ç”µå•†å¹³å°` `ç›¸ä¼¼æ€§æœç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå…³é”®è¯æœç´¢æ–¹æ³•ä¾èµ–ç²¾ç¡®æ–‡æœ¬åŒ¹é…ï¼Œéš¾ä»¥è¯†åˆ«è¯­ä¹‰ç›¸ä¼¼çš„é‡å¤å•†å“ï¼Œå¯¼è‡´å»é‡æ•ˆæœä¸ä½³ã€‚
2. åˆ©ç”¨BERTå’ŒMasked Autoencodersæå–æ–‡æœ¬å’Œå›¾åƒç‰¹å¾ï¼Œç»“åˆé™ç»´æŠ€æœ¯å’Œæ–°å‹å†³ç­–æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆå»é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¶…è¿‡2äº¿ä»¶å•†å“çš„ç›®å½•ä¸­å®ç°äº†0.90çš„å®å¹³å‡F1åˆ†æ•°ï¼Œä¼˜äºç°æœ‰æ–¹æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“ä¸ºç”µå•†é¢†åŸŸè®¾è®¡ã€å¯æ‰©å±•çš„å¤šæ¨¡æ€å•†å“å»é‡æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡ç”µå•†å¹³å°ä¸­é‡å¤å•†å“åˆ—è¡¨å¯¼è‡´çš„ç”¨æˆ·å›°æƒ‘å’Œè¿è¥æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºBERTæ¶æ„çš„é¢†åŸŸç‰¹å®šæ–‡æœ¬æ¨¡å‹å’Œç”¨äºå›¾åƒè¡¨ç¤ºçš„Masked Autoencodersï¼Œå¹¶é€šè¿‡é™ç»´æŠ€æœ¯ç”Ÿæˆç´§å‡‘çš„128ç»´åµŒå…¥å‘é‡ï¼ŒåŒæ—¶å¼€å‘äº†ä¸€ç§åˆ©ç”¨æ–‡æœ¬å’Œå›¾åƒå‘é‡çš„æ–°å‹å†³ç­–æ¨¡å‹ã€‚é€šè¿‡å°†è¿™äº›ç‰¹å¾æå–æœºåˆ¶ä¸ä¼˜åŒ–çš„å‘é‡æ•°æ®åº“Milvusé›†æˆï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆä¸”é«˜ç²¾åº¦åœ°åœ¨è¶…è¿‡2äº¿ä»¶å•†å“çš„åºå¤§å•†å“ç›®å½•ä¸­è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œä¸”ä»…æ¶ˆè€—100GBçš„ç³»ç»ŸRAMã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥åŒ¹é…ç³»ç»Ÿå®ç°äº†0.90çš„å®å¹³å‡F1åˆ†æ•°ï¼Œä¼˜äºç¬¬ä¸‰æ–¹è§£å†³æ–¹æ¡ˆçš„0.83ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡ç”µå•†å¹³å°ä¸­å•†å“é‡å¤ listing çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºå…³é”®è¯çš„æœç´¢ï¼Œæ— æ³•æœ‰æ•ˆè¯†åˆ«è¯­ä¹‰ç›¸ä¼¼ä½†æ–‡æœ¬æè¿°ä¸åŒçš„é‡å¤å•†å“ï¼Œå¯¼è‡´ç”¨æˆ·ä½“éªŒä¸‹é™å’Œè¿è¥æˆæœ¬å¢åŠ ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå¯¹å•†å“æ ‡é¢˜çš„è¯­ä¹‰ç†è§£ä¸è¶³ï¼Œä»¥åŠåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆæ–‡æœ¬å’Œå›¾åƒï¼‰æ¥æ›´å‡†ç¡®åœ°è¯†åˆ«é‡å¤å•†å“ã€‚é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹å­¦ä¹ å•†å“æ ‡é¢˜å’Œå›¾åƒçš„åµŒå…¥è¡¨ç¤ºï¼Œå¹¶ç»“åˆå‘é‡æ•°æ®åº“å®ç°é«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ•æ‰å•†å“ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå…‹æœäº†ä¼ ç»Ÿå…³é”®è¯åŒ¹é…çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ–‡æœ¬ç‰¹å¾æå–ï¼šä½¿ç”¨åŸºäºBERTæ¶æ„çš„é¢†åŸŸç‰¹å®šæ–‡æœ¬æ¨¡å‹æå–å•†å“æ ‡é¢˜çš„åµŒå…¥å‘é‡ã€‚2) å›¾åƒç‰¹å¾æå–ï¼šä½¿ç”¨Masked Autoencodersæå–å•†å“å›¾åƒçš„åµŒå…¥å‘é‡ã€‚3) é™ç»´ï¼šä½¿ç”¨é™ç»´æŠ€æœ¯å°†æ–‡æœ¬å’Œå›¾åƒåµŒå…¥å‘é‡å‹ç¼©åˆ°128ç»´ï¼Œä»¥å‡å°‘è®¡ç®—é‡å’Œå­˜å‚¨ç©ºé—´ã€‚4) å†³ç­–æ¨¡å‹ï¼šå¼€å‘ä¸€ç§æ–°å‹å†³ç­–æ¨¡å‹ï¼Œç»“åˆæ–‡æœ¬å’Œå›¾åƒå‘é‡æ¥åˆ¤æ–­å•†å“æ˜¯å¦é‡å¤ã€‚5) å‘é‡æ•°æ®åº“ï¼šä½¿ç”¨Milvuså‘é‡æ•°æ®åº“å­˜å‚¨å•†å“åµŒå…¥å‘é‡ï¼Œå¹¶å®ç°é«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºï¼š1) ç»“åˆæ–‡æœ¬å’Œå›¾åƒä¿¡æ¯è¿›è¡Œå•†å“å»é‡ï¼Œå……åˆ†åˆ©ç”¨äº†å•†å“çš„å¤šæ¨¡æ€ç‰¹å¾ã€‚2) é’ˆå¯¹ç”µå•†é¢†åŸŸå®šåˆ¶äº†BERTæ¨¡å‹ï¼Œä½¿å…¶æ›´é€‚åˆå¤„ç†å•†å“æ ‡é¢˜ç­‰æ–‡æœ¬æ•°æ®ã€‚3) å¼€å‘äº†ä¸€ç§æ–°å‹å†³ç­–æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆèåˆæ–‡æœ¬å’Œå›¾åƒå‘é‡ã€‚4) å°†æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸å‘é‡æ•°æ®åº“ç›¸ç»“åˆï¼Œå®ç°äº†å¤§è§„æ¨¡å•†å“ç›®å½•ä¸‹çš„é«˜æ•ˆå»é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ–‡æœ¬æ¨¡å‹é‡‡ç”¨BERTæ¶æ„ï¼Œå¹¶åœ¨ç”µå•†å•†å“æ ‡é¢˜æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚å›¾åƒæ¨¡å‹é‡‡ç”¨Masked Autoencodersï¼Œé€šè¿‡é‡å»ºè¢«é®ç›–çš„å›¾åƒåŒºåŸŸæ¥å­¦ä¹ å›¾åƒç‰¹å¾ã€‚é™ç»´æŠ€æœ¯é‡‡ç”¨PCAæˆ–å…¶ä»–çº¿æ€§/éçº¿æ€§é™ç»´æ–¹æ³•ï¼Œç›®æ ‡æ˜¯ä¿ç•™å°½å¯èƒ½å¤šçš„ä¿¡æ¯ï¼ŒåŒæ—¶é™ä½å‘é‡ç»´åº¦ã€‚å†³ç­–æ¨¡å‹å¯ä»¥é‡‡ç”¨å¤šç§åˆ†ç±»å™¨ï¼Œå¦‚é€»è¾‘å›å½’ã€æ”¯æŒå‘é‡æœºæˆ–ç¥ç»ç½‘ç»œï¼Œè¾“å…¥ä¸ºæ–‡æœ¬å’Œå›¾åƒåµŒå…¥å‘é‡ï¼Œè¾“å‡ºä¸ºå•†å“æ˜¯å¦é‡å¤çš„æ¦‚ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•†å“å»é‡ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå®å¹³å‡F1åˆ†æ•°è¾¾åˆ°0.90ï¼Œä¼˜äºç¬¬ä¸‰æ–¹è§£å†³æ–¹æ¡ˆçš„0.83ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨è¶…è¿‡2äº¿ä»¶å•†å“çš„åºå¤§å•†å“ç›®å½•ä¸­è¿›è¡Œé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ï¼Œä¸”ä»…æ¶ˆè€—100GBçš„ç³»ç»ŸRAMï¼Œè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡ç”µå•†ç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç±»ç”µå•†å¹³å°ï¼Œç”¨äºè‡ªåŠ¨æ£€æµ‹å’Œåˆ é™¤é‡å¤å•†å“åˆ—è¡¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œé™ä½è¿è¥æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œå¦‚ç¤¾äº¤åª’ä½“å†…å®¹å»é‡ã€å›¾åƒæ£€ç´¢ç­‰ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå•†ä¸šæ½œåŠ›ã€‚æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•åˆ©ç”¨ç”¨æˆ·è¡Œä¸ºæ•°æ®æ¥æå‡å»é‡ç²¾åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In large scale e-commerce marketplaces, duplicate product listings frequently cause consumer confusion and operational inefficiencies, degrading trust on the platform and increasing costs. Traditional keyword-based search methodologies falter in accurately identifying duplicates due to their reliance on exact textual matches, neglecting semantic similarities inherent in product titles. To address these challenges, we introduce a scalable, multimodal product deduplication designed specifically for the e-commerce domain. Our approach employs a domain-specific text model grounded in BERT architecture in conjunction with MaskedAutoEncoders for image representations. Both of these architectures are augmented with dimensionality reduction techniques to produce compact 128-dimensional embeddings without significant information loss. Complementing this, we also developed a novel decider model that leverages both text and image vectors. By integrating these feature extraction mechanisms with Milvus, an optimized vector database, our system can facilitate efficient and high-precision similarity searches across extensive product catalogs exceeding 200 million items with just 100GB of system RAM consumption. Empirical evaluations demonstrate that our matching system achieves a macro-average F1 score of 0.90, outperforming third-party solutions which attain an F1 score of 0.83. Our findings show the potential of combining domain-specific adaptations with state-of-the-art machine learning techniques to mitigate duplicate listings in large-scale e-commerce environments.

