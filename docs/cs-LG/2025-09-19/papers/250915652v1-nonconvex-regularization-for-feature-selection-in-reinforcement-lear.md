---
layout: default
title: Nonconvex Regularization for Feature Selection in Reinforcement Learning
---

# Nonconvex Regularization for Feature Selection in Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15652" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15652v1</a>
  <a href="https://arxiv.org/pdf/2509.15652.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15652v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15652v1', 'Nonconvex Regularization for Feature Selection in Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kyohei Suzuki, Konstantinos Slavakis

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºéå‡¸æ­£åˆ™åŒ–çš„å¼ºåŒ–å­¦ä¹ ç‰¹å¾é€‰æ‹©ç®—æ³•ï¼Œæå‡é«˜å™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç‰¹å¾é€‰æ‹©` `éå‡¸æ­£åˆ™åŒ–` `ç¨€ç–å­¦ä¹ ` `è´å°”æ›¼æ®‹å·®` `LSTD` `FRBS`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç‰¹å¾é€‰æ‹©æ–¹æ³•å­˜åœ¨ä¼°è®¡åå·®ï¼Œå½±å“åœ¨é«˜å™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚
2. é‡‡ç”¨éå‡¸æŠ•å½±æå°æå¤§å‡¹(PMC)æƒ©ç½šæ­£åˆ™åŒ–è´å°”æ›¼æ®‹å·®ç›®æ ‡ï¼Œå‡è½»ä¼°è®¡åå·®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å™ªå£°åœºæ™¯ä¸‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ‰¹é‡å¼ºåŒ–å­¦ä¹ ç‰¹å¾é€‰æ‹©ç®—æ³•ï¼Œå¹¶æä¾›äº†ç†è®ºæ”¶æ•›ä¿è¯ã€‚ä¸ºäº†å‡è½»ä¼ ç»Ÿæ­£åˆ™åŒ–æ–¹æ¡ˆä¸­å›ºæœ‰çš„ä¼°è®¡åå·®ï¼Œæœ¬æ–‡é¦–å…ˆæ‰©å±•äº†ç»å…¸æœ€å°äºŒä¹˜æ—¶åºå·®åˆ†(LSTD)æ¡†æ¶ä¸­çš„ç­–ç•¥è¯„ä¼°ï¼Œé€šè¿‡ä½¿ç”¨ç¨€ç–æ€§è¯±å¯¼çš„éå‡¸æŠ•å½±æå°æå¤§å‡¹(PMC)æƒ©ç½šæ­£åˆ™åŒ–çš„è´å°”æ›¼æ®‹å·®ç›®æ ‡æ¥å®ç°ã€‚ç”±äºPMCæƒ©ç½šçš„å¼±å‡¸æ€§ï¼Œè¯¥å…¬å¼å¯ä»¥è§£é‡Šä¸ºä¸€èˆ¬éå•è°ƒåŒ…å«é—®é¢˜çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚å…¶æ¬¡ï¼Œæœ¬æ–‡ä¸ºå‰å‘-åå°„-åå‘åˆ†è£‚(FRBS)ç®—æ³•å»ºç«‹äº†æ–°çš„æ”¶æ•›æ¡ä»¶ï¼Œä»¥è§£å†³è¿™ç±»é—®é¢˜ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ•°å€¼å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰è®¸å¤šå™ªå£°ç‰¹å¾çš„åœºæ™¯ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¼ºåŒ–å­¦ä¹ ä¸­çš„ç‰¹å¾é€‰æ‹©æ—¨åœ¨ä»å¤§é‡å€™é€‰ç‰¹å¾ä¸­é€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾å­é›†ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå¦‚L1æ­£åˆ™åŒ–ï¼Œè™½ç„¶å¯ä»¥è¯±å¯¼ç¨€ç–æ€§ï¼Œä½†å¾€å¾€ä¼šå¼•å…¥ä¼°è®¡åå·®ï¼Œå°¤å…¶æ˜¯åœ¨å™ªå£°ç‰¹å¾è¾ƒå¤šçš„æƒ…å†µä¸‹ï¼Œå½±å“ç­–ç•¥è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨éå‡¸æ­£åˆ™åŒ–æ¥å…‹æœä¼ ç»Ÿå‡¸æ­£åˆ™åŒ–çš„åå·®é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œé‡‡ç”¨æŠ•å½±æå°æå¤§å‡¹(PMC)æƒ©ç½šå‡½æ•°ï¼Œå®ƒå…·æœ‰æ¯”L1æ­£åˆ™åŒ–æ›´å¼ºçš„ç¨€ç–æ€§è¯±å¯¼èƒ½åŠ›ï¼ŒåŒæ—¶å¯ä»¥å‡è½»ä¼°è®¡åå·®ã€‚é€šè¿‡æœ€å°åŒ–æ­£åˆ™åŒ–çš„è´å°”æ›¼æ®‹å·®ï¼Œå¯ä»¥å¾—åˆ°æ›´å‡†ç¡®çš„ç­–ç•¥è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŸºäºç»å…¸çš„æœ€å°äºŒä¹˜æ—¶åºå·®åˆ†(LSTD)æ¡†æ¶ã€‚é¦–å…ˆï¼Œæ„å»ºä¸€ä¸ªè´å°”æ›¼æ®‹å·®ç›®æ ‡å‡½æ•°ï¼Œè¯¥å‡½æ•°è¡¡é‡äº†å½“å‰ç­–ç•¥ä¸æœ€ä¼˜ç­–ç•¥ä¹‹é—´çš„å·®è·ã€‚ç„¶åï¼Œä½¿ç”¨PMCæƒ©ç½šå¯¹è¯¥ç›®æ ‡å‡½æ•°è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥é¼“åŠ±é€‰æ‹©ç¨€ç–çš„ç‰¹å¾å­é›†ã€‚æœ€åï¼Œä½¿ç”¨å‰å‘-åå°„-åå‘åˆ†è£‚(FRBS)ç®—æ³•æ±‚è§£è¯¥ä¼˜åŒ–é—®é¢˜ï¼Œå¾—åˆ°æœ€ä¼˜çš„ç‰¹å¾æƒé‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨éå‡¸PMCæƒ©ç½šè¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚ä¸ä¼ ç»Ÿçš„L1æ­£åˆ™åŒ–ç›¸æ¯”ï¼ŒPMCæƒ©ç½šå…·æœ‰æ›´å¼ºçš„ç¨€ç–æ€§è¯±å¯¼èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥å‡è½»ä¼°è®¡åå·®ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä¸ºFRBSç®—æ³•å»ºç«‹äº†æ–°çš„æ”¶æ•›æ¡ä»¶ï¼Œä¿è¯äº†ç®—æ³•çš„æ”¶æ•›æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šPMCæƒ©ç½šå‡½æ•°çš„å…·ä½“å½¢å¼éœ€è¦ä»”ç»†é€‰æ‹©ï¼Œä»¥ä¿è¯å…¶å¼±å‡¸æ€§ï¼Œä»è€Œå¯ä»¥ä½¿ç”¨FRBSç®—æ³•è¿›è¡Œæ±‚è§£ã€‚FRBSç®—æ³•çš„å…³é”®å‚æ•°åŒ…æ‹¬æ­¥é•¿å’Œåå°„ç³»æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“é—®é¢˜è¿›è¡Œè°ƒæ•´ï¼Œä»¥ä¿è¯ç®—æ³•çš„æ”¶æ•›é€Ÿåº¦å’Œç²¾åº¦ã€‚æŸå¤±å‡½æ•°æ˜¯è´å°”æ›¼æ®‹å·®çš„å¹³æ–¹ï¼Œç”¨äºè¡¡é‡å½“å‰ç­–ç•¥ä¸æœ€ä¼˜ç­–ç•¥ä¹‹é—´çš„å·®è·ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¤§é‡å™ªå£°ç‰¹å¾çš„åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°é€‰æ‹©ç›¸å…³ç‰¹å¾ï¼Œå¹¶è·å¾—æ›´é«˜çš„ç­–ç•¥æ€§èƒ½ã€‚ä¸L1æ­£åˆ™åŒ–ç­‰ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½ä¼°è®¡åå·®ï¼Œå¹¶æé«˜å­¦ä¹ æ•ˆç‡ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦åœ¨ä¸åŒæ•°æ®é›†ä¸Šæœ‰æ‰€ä¸åŒï¼Œä½†æ€»ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè·å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨ç‰¹å¾ç»´åº¦é«˜ã€å™ªå£°ç‰¹å¾å¤šçš„åœºæ™¯ä¸‹ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€æ¨èç³»ç»Ÿã€é‡‘èäº¤æ˜“ç­‰ã€‚é€šè¿‡é€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œå¯ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€é™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶æå‡ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºç†è§£å¼ºåŒ–å­¦ä¹ æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆã€æ›´é²æ£’çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›ç†è®ºåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work proposes an efficient batch algorithm for feature selection in reinforcement learning (RL) with theoretical convergence guarantees. To mitigate the estimation bias inherent in conventional regularization schemes, the first contribution extends policy evaluation within the classical least-squares temporal-difference (LSTD) framework by formulating a Bellman-residual objective regularized with the sparsity-inducing, nonconvex projected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC penalty, this formulation can be interpreted as a special instance of a general nonmonotone-inclusion problem. The second contribution establishes novel convergence conditions for the forward-reflected-backward splitting (FRBS) algorithm to solve this class of problems. Numerical experiments on benchmark datasets demonstrate that the proposed approach substantially outperforms state-of-the-art feature-selection methods, particularly in scenarios with many noisy features.

