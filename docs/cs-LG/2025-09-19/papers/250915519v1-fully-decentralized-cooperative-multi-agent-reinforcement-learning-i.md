---
layout: default
title: Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem
---

# Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15519" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15519v1</a>
  <a href="https://arxiv.org/pdf/2509.15519.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15519v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15519v1', 'Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chao Li, Bingkun Bao, Yang Gao

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€æ„ŸçŸ¥ä¸Šä¸‹æ–‡(DAC)æ–¹æ³•ï¼Œè§£å†³å®Œå…¨å»ä¸­å¿ƒåŒ–åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„éå¹³ç¨³æ€§å’Œè¿‡åº¦æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `å»ä¸­å¿ƒåŒ–å­¦ä¹ ` `ä¸Šä¸‹æ–‡å»ºæ¨¡` `éå¹³ç¨³æ€§` `è¿‡åº¦æ³›åŒ–` `åˆä½œåšå¼ˆ` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥åŒæ—¶è§£å†³å€¼å‡½æ•°æ›´æ–°çš„éå¹³ç¨³æ€§å’Œå€¼å‡½æ•°ä¼°è®¡çš„ç›¸å¯¹è¿‡åº¦æ³›åŒ–é—®é¢˜ã€‚
2. DACæ–¹æ³•å°†æ¯ä¸ªæ™ºèƒ½ä½“çš„å±€éƒ¨ä»»åŠ¡å»ºæ¨¡ä¸ºä¸Šä¸‹æ–‡é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡åŠ¨æ€æ„ŸçŸ¥ä¸Šä¸‹æ–‡å»ºæ¨¡è§£å†³éå¹³ç¨³æ€§å’Œè¿‡åº¦æ³›åŒ–é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDACåœ¨çŸ©é˜µåšå¼ˆã€æ•é£Ÿè€…å’ŒçŒç‰©ã€SMACç­‰åˆä½œä»»åŠ¡ä¸Šä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶å®Œå…¨å»ä¸­å¿ƒåŒ–åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼Œå…¶ä¸­æ¯ä¸ªæ™ºèƒ½ä½“ä»…è§‚å¯Ÿè‡ªèº«çŠ¶æ€ã€å±€éƒ¨åŠ¨ä½œå’Œå…±äº«å¥–åŠ±ã€‚ç”±äºæ— æ³•è®¿é—®å…¶ä»–æ™ºèƒ½ä½“çš„åŠ¨ä½œï¼Œå¯¼è‡´å€¼å‡½æ•°æ›´æ–°æœŸé—´çš„éå¹³ç¨³æ€§å’Œå€¼å‡½æ•°ä¼°è®¡æœŸé—´çš„ç›¸å¯¹è¿‡åº¦æ³›åŒ–ï¼Œé˜»ç¢äº†æœ‰æ•ˆçš„åˆä½œç­–ç•¥å­¦ä¹ ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œæœªèƒ½åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•åœ¨å®Œå…¨å»ä¸­å¿ƒåŒ–çš„ç¯å¢ƒä¸­å¯¹å…¶ä»–æ™ºèƒ½ä½“çš„è”åˆç­–ç•¥è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€æ„ŸçŸ¥ä¸Šä¸‹æ–‡(DAC)çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ¯ä¸ªæ™ºèƒ½ä½“å±€éƒ¨æ„ŸçŸ¥çš„ä»»åŠ¡å½¢å¼åŒ–ä¸ºä¸Šä¸‹æ–‡é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡åŠ¨æ€æ„ŸçŸ¥ä¸Šä¸‹æ–‡å»ºæ¨¡æ¥è§£å†³éå¹³ç¨³æ€§å’Œç›¸å¯¹è¿‡åº¦æ³›åŒ–é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒDACå°†æ¯ä¸ªæ™ºèƒ½ä½“çš„éå¹³ç¨³å±€éƒ¨ä»»åŠ¡åŠ¨æ€å½’å› äºæœªè§‚å¯Ÿåˆ°çš„ä¸Šä¸‹æ–‡ä¹‹é—´çš„åˆ‡æ¢ï¼Œæ¯ä¸ªä¸Šä¸‹æ–‡å¯¹åº”äºä¸åŒçš„è”åˆç­–ç•¥ã€‚ç„¶åï¼ŒDACä½¿ç”¨æ½œåœ¨å˜é‡å¯¹é€æ­¥åŠ¨æ€åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å°†å®ƒä»¬ç§°ä¸ºä¸Šä¸‹æ–‡ã€‚å¯¹äºæ¯ä¸ªæ™ºèƒ½ä½“ï¼ŒDACå¼•å…¥äº†ä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡çš„å€¼å‡½æ•°æ¥è§£å†³å€¼å‡½æ•°æ›´æ–°æœŸé—´çš„éå¹³ç¨³æ€§é—®é¢˜ã€‚å¯¹äºå€¼å‡½æ•°ä¼°è®¡ï¼Œæ¨å¯¼å‡ºä¸€ä¸ªä¹è§‚çš„è¾¹é™…å€¼ï¼Œä»¥ä¿ƒè¿›åˆä½œåŠ¨ä½œçš„é€‰æ‹©ï¼Œä»è€Œè§£å†³ç›¸å¯¹è¿‡åº¦æ³›åŒ–é—®é¢˜ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬åœ¨å„ç§åˆä½œä»»åŠ¡ï¼ˆåŒ…æ‹¬çŸ©é˜µåšå¼ˆã€æ•é£Ÿè€…å’ŒçŒç‰©ä»¥åŠSMACï¼‰ä¸Šè¯„ä¼°äº†DACï¼Œå…¶ä¼˜äºå¤šä¸ªåŸºçº¿çš„æ€§èƒ½éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å®Œå…¨å»ä¸­å¿ƒåŒ–åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„éå¹³ç¨³æ€§å’Œç›¸å¯¹è¿‡åº¦æ³›åŒ–é—®é¢˜ã€‚åœ¨å®Œå…¨å»ä¸­å¿ƒåŒ–çš„ç¯å¢ƒä¸­ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“åªèƒ½è§‚å¯Ÿåˆ°è‡ªå·±çš„å±€éƒ¨çŠ¶æ€ã€åŠ¨ä½œå’Œå…±äº«å¥–åŠ±ï¼Œæ— æ³•ç›´æ¥è·å–å…¶ä»–æ™ºèƒ½ä½“çš„åŠ¨ä½œä¿¡æ¯ã€‚è¿™å¯¼è‡´æ™ºèƒ½ä½“åœ¨æ›´æ–°å€¼å‡½æ•°æ—¶é¢ä¸´ç¯å¢ƒéå¹³ç¨³çš„é—®é¢˜ï¼Œå› ä¸ºå…¶ä»–æ™ºèƒ½ä½“çš„ç­–ç•¥ä¹Ÿåœ¨ä¸æ–­å˜åŒ–ã€‚åŒæ—¶ï¼Œç”±äºç¼ºä¹å…¨å±€ä¿¡æ¯ï¼Œæ™ºèƒ½ä½“å®¹æ˜“äº§ç”Ÿç›¸å¯¹è¿‡åº¦æ³›åŒ–ï¼Œéš¾ä»¥å­¦ä¹ åˆ°æœ‰æ•ˆçš„åˆä½œç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¯ä¸ªæ™ºèƒ½ä½“æ‰€å¤„çš„å±€éƒ¨ç¯å¢ƒå»ºæ¨¡ä¸ºä¸€ä¸ªä¸Šä¸‹æ–‡é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Contextual Markov Decision Process, CMDP)ã€‚æ™ºèƒ½ä½“æ‰€å¤„çš„ä¸Šä¸‹æ–‡ä»£è¡¨äº†å…¶ä»–æ™ºèƒ½ä½“çš„è”åˆç­–ç•¥ï¼Œè€Œç¯å¢ƒçš„éå¹³ç¨³æ€§åˆ™è¢«è®¤ä¸ºæ˜¯ä¸åŒä¸Šä¸‹æ–‡ä¹‹é—´çš„åˆ‡æ¢ã€‚é€šè¿‡å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œå»ºæ¨¡ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°é€‚åº”ç¯å¢ƒçš„å˜åŒ–ï¼Œä»è€Œè§£å†³éå¹³ç¨³æ€§é—®é¢˜ã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†ä¹è§‚çš„è¾¹é™…å€¼ï¼Œé¼“åŠ±æ™ºèƒ½ä½“é€‰æ‹©åˆä½œçš„åŠ¨ä½œï¼Œä»è€Œç¼“è§£ç›¸å¯¹è¿‡åº¦æ³›åŒ–çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDACæ–¹æ³•çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) ä¸Šä¸‹æ–‡ç¼–ç å™¨ï¼šä½¿ç”¨æ½œåœ¨å˜é‡å¯¹é€æ­¥åŠ¨æ€åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å°†è¿™äº›æ½œåœ¨å˜é‡ä½œä¸ºä¸Šä¸‹æ–‡ã€‚2) åŸºäºä¸Šä¸‹æ–‡çš„å€¼å‡½æ•°ï¼šæ¯ä¸ªæ™ºèƒ½ä½“ç»´æŠ¤ä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡çš„å€¼å‡½æ•°ï¼Œç”¨äºè¯„ä¼°åœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸‹çš„åŠ¨ä½œä»·å€¼ã€‚3) ä¹è§‚è¾¹é™…å€¼ï¼šä¸ºäº†ä¿ƒè¿›åˆä½œåŠ¨ä½œçš„é€‰æ‹©ï¼Œè®ºæ–‡æ¨å¯¼äº†ä¸€ä¸ªä¹è§‚çš„è¾¹é™…å€¼ï¼Œç”¨äºæŒ‡å¯¼æ™ºèƒ½ä½“çš„ç­–ç•¥å­¦ä¹ ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“æ ¹æ®è‡ªèº«è§‚å¯Ÿåˆ°çš„çŠ¶æ€å’Œå¥–åŠ±ï¼Œæ¨æ–­å½“å‰æ‰€å¤„çš„ä¸Šä¸‹æ–‡ï¼Œç„¶åæ ¹æ®åŸºäºä¸Šä¸‹æ–‡çš„å€¼å‡½æ•°å’Œä¹è§‚è¾¹é™…å€¼é€‰æ‹©åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šDACæ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŠ¨æ€æ„ŸçŸ¥ä¸Šä¸‹æ–‡å»ºæ¨¡çš„æ€æƒ³ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒDACæ–¹æ³•èƒ½å¤Ÿæ˜¾å¼åœ°å¯¹å…¶ä»–æ™ºèƒ½ä½“çš„è”åˆç­–ç•¥è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ç¯å¢ƒçš„éå¹³ç¨³æ€§ã€‚æ­¤å¤–ï¼ŒDACæ–¹æ³•è¿˜å¼•å…¥äº†ä¹è§‚è¾¹é™…å€¼ï¼Œé¼“åŠ±æ™ºèƒ½ä½“é€‰æ‹©åˆä½œçš„åŠ¨ä½œï¼Œä»è€Œç¼“è§£ç›¸å¯¹è¿‡åº¦æ³›åŒ–çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šDACæ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä¸Šä¸‹æ–‡ç¼–ç å™¨çš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒæ–¹å¼ï¼›2) åŸºäºä¸Šä¸‹æ–‡çš„å€¼å‡½æ•°çš„è¡¨ç¤ºå’Œæ›´æ–°æ–¹å¼ï¼›3) ä¹è§‚è¾¹é™…å€¼çš„è®¡ç®—æ–¹æ³•ã€‚è®ºæ–‡ä¸­å…·ä½“ä½¿ç”¨äº†GRUæ¥å»ºæ¨¡ä¸Šä¸‹æ–‡çš„åŠ¨æ€å˜åŒ–ï¼Œå¹¶ä½¿ç”¨å˜åˆ†æ¨æ–­æ¥å­¦ä¹ ä¸Šä¸‹æ–‡çš„åéªŒåˆ†å¸ƒã€‚å¯¹äºå€¼å‡½æ•°ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œæ›´æ–°ï¼Œä¾‹å¦‚Q-learningæˆ–Actor-Criticæ–¹æ³•ã€‚ä¹è§‚è¾¹é™…å€¼çš„è®¡ç®—æ¶‰åŠåˆ°å¯¹å…¶ä»–æ™ºèƒ½ä½“åŠ¨ä½œçš„æœŸæœ›ï¼Œå¯ä»¥ä½¿ç”¨é‡‡æ ·æˆ–è€…è¿‘ä¼¼çš„æ–¹æ³•è¿›è¡Œä¼°è®¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDACæ–¹æ³•åœ¨çŸ©é˜µåšå¼ˆã€æ•é£Ÿè€…å’ŒçŒç‰©ä»¥åŠSMACç­‰å¤šä¸ªåˆä½œä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨SMACä»»åŠ¡ä¸­ï¼ŒDACæ–¹æ³•åœ¨å¤šä¸ªåœ°å›¾ä¸Šçš„èƒœç‡éƒ½è¶…è¿‡äº†ç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ä¸€äº›å›°éš¾åœ°å›¾ä¸Šå–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚è¿™äº›å®éªŒç»“æœéªŒè¯äº†DACæ–¹æ³•åœ¨è§£å†³å®Œå…¨å»ä¸­å¿ƒåŒ–åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DACæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¤šæ™ºèƒ½ä½“åˆä½œçš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººååŒã€è‡ªåŠ¨é©¾é©¶ã€äº¤é€šè°ƒåº¦ã€èµ„æºåˆ†é…ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å»ä¸­å¿ƒåŒ–ç¯å¢ƒä¸‹çš„éå¹³ç¨³æ€§å’Œè¿‡åº¦æ³›åŒ–é—®é¢˜ï¼Œæé«˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œæ•ˆç‡å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼ŒDACæ–¹æ³•è¿˜å¯ä»¥ä¸å…¶ä»–æŠ€æœ¯ç›¸ç»“åˆï¼Œä¾‹å¦‚è”é‚¦å­¦ä¹ ã€çŸ¥è¯†å›¾è°±ç­‰ï¼Œä»¥è¿›ä¸€æ­¥æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents' actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.

