---
layout: default
title: Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and Grover-Based Trajectory Optimization
---

# Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and Grover-Based Trajectory Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16002" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16002v1</a>
  <a href="https://arxiv.org/pdf/2509.16002.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16002v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16002v1', 'Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and Grover-Based Trajectory Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thet Htar Su, Shaswot Shresthamali, Masaaki Kondo

**åˆ†ç±»**: quant-ph, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåŠ¨æ€é‡å­ç”µè·¯å’ŒGroverç®—æ³•çš„é‡å­å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæå‡å¯æ‰©å±•æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é‡å­å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€é‡å­ç”µè·¯` `é‡å­æ¯”ç‰¹é‡ç”¨` `Groverç®—æ³•` `é‡å­é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨å¤„ç†å¤§è§„æ¨¡çŠ¶æ€ç©ºé—´æ—¶é¢ä¸´ç»´åº¦ç¾éš¾ï¼Œé‡å­å¼ºåŒ–å­¦ä¹ æ—¨åœ¨åˆ©ç”¨é‡å­è®¡ç®—çš„ä¼˜åŠ¿è§£å†³æ­¤é—®é¢˜ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§å®Œå…¨é‡å­åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨é‡å­å åŠ å¹¶è¡Œæ¢ç´¢ç­–ç•¥ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€é‡å­ç”µè·¯å‡å°‘é‡å­æ¯”ç‰¹éœ€æ±‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é™ä½é‡å­æ¯”ç‰¹éœ€æ±‚çš„åŒæ—¶ä¿æŒäº†è½¨è¿¹ä¿çœŸåº¦ï¼Œå¹¶åœ¨çœŸå®é‡å­ç¡¬ä»¶ä¸ŠéªŒè¯äº†å…¶å¯è¡Œæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„é‡å­å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†é‡å­é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€åŸºäºåŠ¨æ€ç”µè·¯çš„é‡å­æ¯”ç‰¹é‡ç”¨ä»¥åŠç”¨äºè½¨è¿¹ä¼˜åŒ–çš„Groverç®—æ³•ã€‚è¯¥æ¡†æ¶å°†çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œè½¬ç§»å®Œå…¨ç¼–ç åœ¨é‡å­åŸŸä¸­ï¼Œé€šè¿‡å åŠ å®ç°çŠ¶æ€-åŠ¨ä½œåºåˆ—çš„å¹¶è¡Œæ¢ç´¢ï¼Œå¹¶æ¶ˆé™¤äº†ç»å…¸å­ç¨‹åºã€‚åŠ¨æ€ç”µè·¯æ“ä½œï¼ˆåŒ…æ‹¬ä¸­é—´ç”µè·¯æµ‹é‡å’Œé‡ç½®ï¼‰å…è®¸åœ¨å¤šæ¬¡æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’ä¸­é‡å¤ä½¿ç”¨ç›¸åŒçš„ç‰©ç†é‡å­æ¯”ç‰¹ï¼Œä»è€Œå°†é‡å­æ¯”ç‰¹éœ€æ±‚ä»7*Tå‡å°‘åˆ°7ï¼ˆå¯¹äºTä¸ªæ—¶é—´æ­¥ï¼‰ï¼ŒåŒæ—¶ä¿æŒé€»è¾‘è¿ç»­æ€§ã€‚é‡å­ç®—æœ¯è®¡ç®—è½¨è¿¹å›æŠ¥ï¼Œå¹¶åº”ç”¨Groveræœç´¢æ¥å åŠ è¿™äº›è¯„ä¼°çš„è½¨è¿¹ï¼Œä»¥æ”¾å¤§æµ‹é‡å…·æœ‰æœ€é«˜å›æŠ¥çš„è½¨è¿¹çš„æ¦‚ç‡ï¼Œä»è€ŒåŠ é€Ÿè¯†åˆ«æœ€ä¼˜ç­–ç•¥ã€‚ä»¿çœŸè¡¨æ˜ï¼ŒåŸºäºåŠ¨æ€ç”µè·¯çš„å®ç°ä¿æŒäº†è½¨è¿¹ä¿çœŸåº¦ï¼ŒåŒæ—¶ç›¸å¯¹äºé™æ€è®¾è®¡å‡å°‘äº†66%çš„é‡å­æ¯”ç‰¹ä½¿ç”¨é‡ã€‚åœ¨IBM Heronç±»é‡å­ç¡¬ä»¶ä¸Šçš„å®éªŒéƒ¨ç½²è¯å®ï¼Œè¯¥æ¡†æ¶åœ¨å½“å‰é‡å­å¤„ç†å™¨çš„çº¦æŸèŒƒå›´å†…è¿è¡Œï¼Œå¹¶éªŒè¯äº†åœ¨å™ªå£°ä¸­ç­‰è§„æ¨¡é‡å­æ¡ä»¶ä¸‹å®Œå…¨é‡å­å¤šæ­¥å¼ºåŒ–å­¦ä¹ çš„å¯è¡Œæ€§ã€‚è¯¥æ¡†æ¶æé«˜äº†é‡å­å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹é¡ºåºå†³ç­–ä»»åŠ¡ä¸­çš„å¯æ‰©å±•æ€§å’Œå®é™…åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¼ ç»Ÿé‡å­å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤šæ­¥å†³ç­–é—®é¢˜æ—¶ï¼Œéœ€è¦å¤§é‡çš„é‡å­æ¯”ç‰¹èµ„æºï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…é‡å­ç¡¬ä»¶ä¸Šçš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ³•ä¾èµ–äºç»å…¸å­ç¨‹åºï¼Œæ— æ³•å……åˆ†å‘æŒ¥é‡å­è®¡ç®—çš„ä¼˜åŠ¿ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³é‡å­å¼ºåŒ–å­¦ä¹ ä¸­çš„é‡å­æ¯”ç‰¹èµ„æºç“¶é¢ˆé—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªå®Œå…¨é‡å­åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åŠ¨æ€é‡å­ç”µè·¯æŠ€æœ¯ï¼Œé€šè¿‡é‡å­æ¯”ç‰¹çš„é‡ç”¨ï¼Œæ˜¾è‘—å‡å°‘é‡å­æ¯”ç‰¹çš„éœ€æ±‚é‡ã€‚åŒæ—¶ï¼Œé‡‡ç”¨Groveræœç´¢ç®—æ³•åŠ é€Ÿæœ€ä¼˜ç­–ç•¥çš„æœç´¢è¿‡ç¨‹ï¼Œå¹¶æ„å»ºå®Œå…¨é‡å­åŒ–çš„å¼ºåŒ–å­¦ä¹ æµç¨‹ï¼Œé¿å…ç»å…¸è®¡ç®—çš„å‚ä¸ï¼Œå……åˆ†åˆ©ç”¨é‡å­è®¡ç®—çš„å¹¶è¡Œæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) é‡å­é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆQMDPï¼‰çš„æ„å»ºï¼Œå°†çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œè½¬ç§»æ¦‚ç‡ç¼–ç ä¸ºé‡å­æ€ã€‚2) åŸºäºåŠ¨æ€é‡å­ç”µè·¯çš„é‡å­æ¯”ç‰¹é‡ç”¨æœºåˆ¶ï¼Œé€šè¿‡ä¸­é—´æµ‹é‡å’Œé‡ç½®æ“ä½œï¼Œåœ¨ä¸åŒçš„æ—¶é—´æ­¥é‡å¤ä½¿ç”¨ç›¸åŒçš„ç‰©ç†é‡å­æ¯”ç‰¹ã€‚3) é‡å­ç®—æœ¯å•å…ƒï¼Œç”¨äºè®¡ç®—è½¨è¿¹çš„å›æŠ¥å€¼ã€‚4) Groveræœç´¢ç®—æ³•ï¼Œç”¨äºåœ¨å åŠ çš„è½¨è¿¹ä¸­æœç´¢å…·æœ‰æœ€é«˜å›æŠ¥çš„è½¨è¿¹ï¼Œä»è€Œæ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºåŠ¨æ€é‡å­ç”µè·¯çš„é‡å­æ¯”ç‰¹é‡ç”¨æœºåˆ¶ã€‚ä¼ ç»Ÿçš„é‡å­ç®—æ³•é€šå¸¸éœ€è¦ä¸ºæ¯ä¸ªæ—¶é—´æ­¥åˆ†é…ç‹¬ç«‹çš„é‡å­æ¯”ç‰¹ï¼Œå¯¼è‡´é‡å­æ¯”ç‰¹éœ€æ±‚éšæ—¶é—´æ­¥çº¿æ€§å¢é•¿ã€‚è€Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•é€šè¿‡åŠ¨æ€ç”µè·¯æ“ä½œï¼Œåœ¨ä¸åŒçš„æ—¶é—´æ­¥é‡å¤ä½¿ç”¨ç›¸åŒçš„é‡å­æ¯”ç‰¹ï¼Œä»è€Œå°†é‡å­æ¯”ç‰¹éœ€æ±‚é™ä½åˆ°å¸¸æ•°çº§åˆ«ã€‚æ­¤å¤–ï¼Œå®Œå…¨é‡å­åŒ–çš„æ¡†æ¶é¿å…äº†ç»å…¸å­ç¨‹åºçš„å‚ä¸ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨é‡å­è®¡ç®—çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€é‡å­ç”µè·¯çš„è®¾è®¡æ˜¯å…³é”®ã€‚é€šè¿‡åœ¨é‡å­ç”µè·¯ä¸­æ’å…¥ä¸­é—´æµ‹é‡å’Œé‡ç½®æ“ä½œï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ—¶é—´æ­¥é‡å¤ä½¿ç”¨ç›¸åŒçš„é‡å­æ¯”ç‰¹ã€‚Groveræœç´¢ç®—æ³•çš„è¿­ä»£æ¬¡æ•°éœ€è¦æ ¹æ®é—®é¢˜çš„è§„æ¨¡è¿›è¡Œè°ƒæ•´ï¼Œä»¥ä¿è¯æœç´¢çš„æ•ˆç‡å’Œç²¾åº¦ã€‚æ­¤å¤–ï¼Œé‡å­ç®—æœ¯å•å…ƒçš„è®¾è®¡ä¹Ÿéœ€è¦è€ƒè™‘é‡å­æ¯”ç‰¹çš„é™åˆ¶å’Œå™ªå£°çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºåŠ¨æ€ç”µè·¯çš„å®ç°ç›¸å¯¹äºé™æ€è®¾è®¡ï¼Œé‡å­æ¯”ç‰¹ä½¿ç”¨é‡å‡å°‘äº†66%ã€‚åœ¨IBM Heronç±»é‡å­ç¡¬ä»¶ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶çš„å¯è¡Œæ€§ï¼Œå¹¶è¡¨æ˜è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å½“å‰é‡å­å¤„ç†å™¨çš„çº¦æŸèŒƒå›´å†…è¿è¡Œã€‚è¿™äº›ç»“æœéªŒè¯äº†å®Œå…¨é‡å­å¤šæ­¥å¼ºåŒ–å­¦ä¹ åœ¨å™ªå£°ä¸­ç­‰è§„æ¨¡é‡å­æ¡ä»¶ä¸‹çš„å¯è¡Œæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å¤æ‚å†³ç­–é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨äººè·¯å¾„è§„åˆ’ã€é‡‘èäº¤æ˜“ç­–ç•¥ä¼˜åŒ–ã€ä»¥åŠè¯ç‰©å‘ç°ç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½é‡å­æ¯”ç‰¹éœ€æ±‚ï¼Œè¯¥æ¡†æ¶æœ‰æœ›åœ¨è¿‘æœŸçš„é‡å­è®¡ç®—æœºä¸Šå®ç°ï¼ŒåŠ é€Ÿé‡å­å¼ºåŒ–å­¦ä¹ çš„å®é™…åº”ç”¨è¿›ç¨‹ï¼Œå¹¶æ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A fully quantum reinforcement learning framework is developed that integrates a quantum Markov decision process, dynamic circuit-based qubit reuse, and Grover's algorithm for trajectory optimization. The framework encodes states, actions, rewards, and transitions entirely within the quantum domain, enabling parallel exploration of state-action sequences through superposition and eliminating classical subroutines. Dynamic circuit operations, including mid-circuit measurement and reset, allow reuse of the same physical qubits across multiple agent-environment interactions, reducing qubit requirements from 7*T to 7 for T time steps while preserving logical continuity. Quantum arithmetic computes trajectory returns, and Grover's search is applied to the superposition of these evaluated trajectories to amplify the probability of measuring those with the highest return, thereby accelerating the identification of the optimal policy. Simulations demonstrate that the dynamic-circuit-based implementation preserves trajectory fidelity while reducing qubit usage by 66 percent relative to the static design. Experimental deployment on IBM Heron-class quantum hardware confirms that the framework operates within the constraints of current quantum processors and validates the feasibility of fully quantum multi-step reinforcement learning under noisy intermediate-scale quantum conditions. This framework advances the scalability and practical application of quantum reinforcement learning for large-scale sequential decision-making tasks.

