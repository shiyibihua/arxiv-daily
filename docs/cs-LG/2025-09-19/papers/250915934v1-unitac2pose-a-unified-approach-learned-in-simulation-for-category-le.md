---
layout: default
title: UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation
---

# UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15934" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15934v1</a>
  <a href="https://arxiv.org/pdf/2509.15934.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15934v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15934v1', 'UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingdong Wu, Long Yang, Jin Liu, Weiyao Huang, Lehong Wu, Zelin Chen, Daolin Ma, Hao Dong

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniTac2Poseï¼šæ¨¡æ‹Ÿç¯å¢ƒå­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç±»åˆ«çº§è§†è§‰è§¦è§‰æ‰‹å†…å§¿æ€ä¼°è®¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `æ‰‹å†…å§¿æ€ä¼°è®¡` `è§†è§‰è§¦è§‰èåˆ` `èƒ½é‡æ¨¡å‹` `æ‰©æ•£æ¨¡å‹` `Sim-to-Real` `æœºå™¨äººæ“ä½œ` `å§¿æ€è·Ÿè¸ª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰‹å†…ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ç²¾åº¦ä¸è¶³ï¼Œä¸”éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„CADæ¨¡å‹ï¼Œæ˜¯é¢†åŸŸå†…çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºåŸºäºèƒ½é‡çš„æ‰©æ•£æ¨¡å‹ï¼Œç»Ÿä¸€å§¿æ€é‡‡æ ·ã€ä¼˜åŒ–å’Œæ’åºï¼Œä»…åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè®­ç»ƒï¼Œå®ç°é«˜ç²¾åº¦å’Œæ³›åŒ–æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œå¹¶åœ¨ç±»åˆ«å†…æ³›åŒ–å’ŒçœŸå®åœºæ™¯é²æ£’æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶é›†æˆäº†å§¿æ€è·Ÿè¸ªå’Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºåŸºäºCADæ¨¡å‹è¿›è¡Œæ‰‹å†…ç‰©ä½“å§¿æ€çš„ç²¾ç¡®ä¼°è®¡ã€‚è¯¥æ¡†æ¶å¯¹äºå·¥ä¸šåº”ç”¨å’Œæ—¥å¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä¾‹å¦‚å·¥ä»¶å®šä½ã€ç»„ä»¶ç»„è£…ä»¥åŠæ— ç¼æ’å…¥USBè¿æ¥å™¨ç­‰è®¾å¤‡ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡æ ·å¹¶é¢„æ’åºå§¿æ€å€™é€‰ï¼Œç¬¬äºŒé˜¶æ®µè¿­ä»£ä¼˜åŒ–è¿™äº›å€™é€‰ï¼Œæœ€åé˜¶æ®µè¿›è¡Œåæ’åºä»¥è¯†åˆ«æœ€å¯èƒ½çš„å§¿æ€å€™é€‰ã€‚è¿™äº›é˜¶æ®µç”±ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºèƒ½é‡çš„æ‰©æ•£æ¨¡å‹æ§åˆ¶ï¼Œè¯¥æ¨¡å‹ä»…åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè®­ç»ƒã€‚è¯¥èƒ½é‡æ¨¡å‹åŒæ—¶ç”Ÿæˆæ¢¯åº¦ä»¥ç»†åŒ–å§¿æ€ä¼°è®¡ï¼Œå¹¶äº§ç”Ÿä¸€ä¸ªèƒ½é‡æ ‡é‡æ¥é‡åŒ–å§¿æ€ä¼°è®¡çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œå€Ÿé‰´è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ€æƒ³ï¼Œæˆ‘ä»¬åœ¨åŸºäºèƒ½é‡çš„è¯„åˆ†ç½‘ç»œä¸­åŠ å…¥äº†ä¸€ä¸ªæ¸²æŸ“-æ¯”è¾ƒæ¶æ„ï¼Œæ˜¾è‘—æé«˜äº†sim-to-realçš„æ€§èƒ½ï¼Œè¿™åœ¨æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶ä¸­å¾—åˆ°äº†è¯æ˜ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºäºå›å½’ã€åŒ¹é…å’Œé…å‡†æŠ€æœ¯çš„ä¼ ç»ŸåŸºçº¿ï¼ŒåŒæ—¶å¯¹å…ˆå‰æœªè§è¿‡çš„CADæ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„ç±»åˆ«å†…æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è§¦è§‰ç‰©ä½“å§¿æ€ä¼°è®¡ã€å§¿æ€è·Ÿè¸ªå’Œä¸ç¡®å®šæ€§ä¼°è®¡é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ï¼Œä»è€Œåœ¨å„ç§çœŸå®æ¡ä»¶ä¸‹å®ç°ç¨³å¥çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ‰‹å†…ç‰©ä½“å§¿æ€ä¼°è®¡é—®é¢˜ï¼Œå³ç»™å®šç‰©ä½“çš„CADæ¨¡å‹å’Œè§†è§‰/è§¦è§‰ä¼ æ„Ÿå™¨æ•°æ®ï¼Œç²¾ç¡®ä¼°è®¡ç‰©ä½“åœ¨æ‰‹ä¸­çš„ä½å§¿ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚å›å½’ã€ç‰¹å¾åŒ¹é…å’Œé…å‡†ï¼Œåœ¨ç²¾åº¦å’Œæ³›åŒ–æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æœªè§è¿‡çš„CADæ¨¡å‹æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åŸºäºèƒ½é‡çš„æ‰©æ•£æ¨¡å‹ï¼Œå°†å§¿æ€ä¼°è®¡è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªèƒ½é‡æœ€å°åŒ–é—®é¢˜ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè®­ç»ƒè¯¥æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°å§¿æ€çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶åˆ©ç”¨æ¢¯åº¦ä¿¡æ¯è¿­ä»£ä¼˜åŒ–å§¿æ€ä¼°è®¡ã€‚åŒæ—¶ï¼Œèƒ½é‡å€¼å¯ä»¥ä½œä¸ºå§¿æ€è´¨é‡çš„åº¦é‡ï¼Œç”¨äºå§¿æ€æ’åºå’Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é‡‡ç”¨ä¸‰é˜¶æ®µæ¡†æ¶ï¼š1) å§¿æ€å€™é€‰é‡‡æ ·ä¸é¢„æ’åºï¼šä»ä½å§¿ç©ºé—´ä¸­é‡‡æ ·å¤šä¸ªå€™é€‰ä½å§¿ï¼Œå¹¶ä½¿ç”¨èƒ½é‡æ¨¡å‹è¿›è¡Œåˆæ­¥æ’åºï¼›2) å§¿æ€è¿­ä»£ä¼˜åŒ–ï¼šåˆ©ç”¨èƒ½é‡æ¨¡å‹ç”Ÿæˆçš„æ¢¯åº¦ä¿¡æ¯ï¼Œè¿­ä»£ä¼˜åŒ–å€™é€‰ä½å§¿ï¼Œä½¿å…¶èƒ½é‡å€¼æœ€å°åŒ–ï¼›3) å§¿æ€åæ’åºï¼šå†æ¬¡ä½¿ç”¨èƒ½é‡æ¨¡å‹å¯¹ä¼˜åŒ–åçš„ä½å§¿è¿›è¡Œæ’åºï¼Œé€‰æ‹©èƒ½é‡å€¼æœ€ä½çš„ä½å§¿ä½œä¸ºæœ€ç»ˆä¼°è®¡ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„ä¸»è¦åˆ›æ–°ç‚¹åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„åŸºäºèƒ½é‡çš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶ç”Ÿæˆå§¿æ€ä¼˜åŒ–æ¢¯åº¦å’Œå§¿æ€è´¨é‡è¯„ä¼°ï¼›2) å¼•å…¥äº†æ¸²æŸ“-æ¯”è¾ƒæ¶æ„ï¼Œé€šè¿‡æ¯”è¾ƒæ¸²æŸ“å›¾åƒå’ŒçœŸå®å›¾åƒï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨sim-to-realåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼›3) å°†å§¿æ€ä¼°è®¡ã€å§¿æ€è·Ÿè¸ªå’Œä¸ç¡®å®šæ€§ä¼°è®¡é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šèƒ½é‡æ¨¡å‹é‡‡ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå®ç°ï¼Œè¾“å…¥ä¸ºè§†è§‰/è§¦è§‰ä¼ æ„Ÿå™¨æ•°æ®å’Œå€™é€‰ä½å§¿ï¼Œè¾“å‡ºä¸ºèƒ½é‡å€¼å’Œä½å§¿ä¼˜åŒ–æ¢¯åº¦ã€‚æ¸²æŸ“-æ¯”è¾ƒæ¶æ„é€šè¿‡æ¸²æŸ“å€™é€‰ä½å§¿å¯¹åº”çš„å›¾åƒï¼Œå¹¶ä¸çœŸå®å›¾åƒè¿›è¡Œæ¯”è¾ƒï¼Œè®¡ç®—æŸå¤±å‡½æ•°ï¼Œä»è€ŒæŒ‡å¯¼èƒ½é‡æ¨¡å‹çš„è®­ç»ƒã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ä½å§¿è¯¯å·®ã€èƒ½é‡å€¼å’Œæ¸²æŸ“è¯¯å·®ç­‰ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰‹å†…ç‰©ä½“å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ä¼ ç»Ÿçš„å›å½’ã€åŒ¹é…å’Œé…å‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç²¾åº¦å’Œæ³›åŒ–æ€§æ–¹é¢å‡æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤„ç†æœªè§è¿‡çš„CADæ¨¡å‹æ—¶ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒè¯æ˜äº†æ¸²æŸ“-æ¯”è¾ƒæ¶æ„å¯¹sim-to-realæ€§èƒ½æå‡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–ã€æœºå™¨äººæ“ä½œç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨å·¥ä¸šè£…é…ä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•ç²¾ç¡®ä¼°è®¡å·¥ä»¶çš„ä½å§¿ï¼Œä»è€Œå®ç°è‡ªåŠ¨åŒ–è£…é…ã€‚åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¸®åŠ©æœºå™¨äººå®Œæˆå„ç§å¤æ‚çš„æ‰‹å†…æ“ä½œä»»åŠ¡ï¼Œå¦‚æŠ“å–ã€æ”¾ç½®å’Œç»„è£…ç‰©ä½“ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºè™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸï¼Œæä¾›æ›´è‡ªç„¶å’Œé€¼çœŸçš„äº¤äº’ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate estimation of the in-hand pose of an object based on its CAD model is crucial in both industrial applications and everyday tasks, ranging from positioning workpieces and assembling components to seamlessly inserting devices like USB connectors. While existing methods often rely on regression, feature matching, or registration techniques, achieving high precision and generalizability to unseen CAD models remains a significant challenge. In this paper, we propose a novel three-stage framework for in-hand pose estimation. The first stage involves sampling and pre-ranking pose candidates, followed by iterative refinement of these candidates in the second stage. In the final stage, post-ranking is applied to identify the most likely pose candidates. These stages are governed by a unified energy-based diffusion model, which is trained solely on simulated data. This energy model simultaneously generates gradients to refine pose estimates and produces an energy scalar that quantifies the quality of the pose estimates. Additionally, borrowing the idea from the computer vision domain, we incorporate a render-compare architecture within the energy-based score network to significantly enhance sim-to-real performance, as demonstrated by our ablation studies. We conduct comprehensive experiments to show that our method outperforms conventional baselines based on regression, matching, and registration techniques, while also exhibiting strong intra-category generalization to previously unseen CAD models. Moreover, our approach integrates tactile object pose estimation, pose tracking, and uncertainty estimation into a unified framework, enabling robust performance across a variety of real-world conditions.

