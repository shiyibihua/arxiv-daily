---
layout: default
title: Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents
---

# Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16151" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16151v1</a>
  <a href="https://arxiv.org/pdf/2509.16151.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16151v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16151v1', 'Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Isaiah J. King, Benjamin Bowman, H. Howie Huang

**åˆ†ç±»**: cs.LG, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå›¾çš„é€šç”¨å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œç”¨äºè‡ªåŠ¨åŒ–ç½‘ç»œé˜²å¾¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å›¾ç¥ç»ç½‘ç»œ` `è‡ªåŠ¨åŒ–ç½‘ç»œé˜²å¾¡` `å…³ç³»å½’çº³åç½®` `ç½‘ç»œå®‰å…¨` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨åŒ–ç½‘ç»œé˜²å¾¡ä¸­æ˜“è¿‡æ‹Ÿåˆç‰¹å®šç½‘ç»œæ‹“æ‰‘ï¼Œæ³›åŒ–èƒ½åŠ›å·®ã€‚
2. è®ºæ–‡æå‡ºåŸºäºå±æ€§å›¾è¡¨ç¤ºç½‘ç»œç¯å¢ƒï¼Œåˆ©ç”¨å…³ç³»å½’çº³åç½®æå‡æ™ºèƒ½ä½“çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œèƒ½é›¶æ ·æœ¬é˜²å¾¡æœªè§è¿‡çš„ç½‘ç»œæ”»å‡»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ­£åœ¨æˆä¸ºè‡ªåŠ¨åŒ–ç½‘ç»œé˜²å¾¡ï¼ˆACDï¼‰çš„ä¸€ç§å¯è¡Œç­–ç•¥ã€‚ä¼ ç»Ÿçš„RLæ–¹æ³•å°†ç½‘ç»œè¡¨ç¤ºä¸ºå¤„äºå„ç§å®‰å…¨æˆ–å¨èƒçŠ¶æ€çš„è®¡ç®—æœºåˆ—è¡¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹è¢«è¿«è¿‡åº¦æ‹Ÿåˆç‰¹å®šçš„ç½‘ç»œæ‹“æ‰‘ï¼Œå³ä½¿é¢å¯¹å¾®å°çš„ç¯å¢ƒæ‰°åŠ¨ä¹Ÿå˜å¾—æ— æ•ˆã€‚æœ¬æ–‡å°†ACDæ„å»ºä¸ºä¸€ä¸ªåŒäººã€åŸºäºä¸Šä¸‹æ–‡çš„ã€éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–é—®é¢˜ï¼Œå¹¶å°†è§‚å¯Ÿç»“æœè¡¨ç¤ºä¸ºå±æ€§å›¾ã€‚è¿™ç§æ–¹æ³•ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡å…³ç³»å½’çº³åç½®è¿›è¡Œæ¨ç†ã€‚æ™ºèƒ½ä½“å­¦ä¹ å¦‚ä½•ä»¥æ›´é€šç”¨çš„æ–¹å¼æ¨ç†ä¸»æœºä¸å…¶ä»–ç³»ç»Ÿå®ä½“çš„äº¤äº’ï¼Œå¹¶ä¸”å®ƒä»¬çš„åŠ¨ä½œè¢«ç†è§£ä¸ºå¯¹è¡¨ç¤ºç¯å¢ƒçš„å›¾çš„ç¼–è¾‘ã€‚é€šè¿‡å¼•å…¥è¿™ç§åç½®ï¼Œæˆ‘ä»¬å°†å±•ç¤ºæˆ‘ä»¬çš„æ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°æ¨ç†ç½‘ç»œçŠ¶æ€ï¼Œå¹¶é›¶æ ·æœ¬é€‚åº”æ–°çš„ç½‘ç»œã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•å¤§å¤§ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä½¿æˆ‘ä»¬çš„æ™ºèƒ½ä½“èƒ½å¤Ÿé˜²å¾¡å‰æ‰€æœªè§çš„ç½‘ç»œï¼Œå¯¹æŠ—å„ç§å¤æ‚å’Œå¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„å„ç§å¯¹æ‰‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨åŒ–ç½‘ç»œé˜²å¾¡ä¸­ï¼Œä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å°†ç½‘ç»œè¡¨ç¤ºä¸ºè®¡ç®—æœºåˆ—è¡¨ï¼Œè¿‡åº¦ä¾èµ–ç‰¹å®šç½‘ç»œæ‹“æ‰‘ç»“æ„ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹æ–°çš„æˆ–ç•¥æœ‰å˜åŒ–çš„æ”»å‡»ç¯å¢ƒæ—¶æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚è¿™ç§è¿‡æ‹Ÿåˆé—®é¢˜ä¸¥é‡é™åˆ¶äº†å¼ºåŒ–å­¦ä¹ åœ¨å®é™…ç½‘ç»œå®‰å…¨åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç½‘ç»œç¯å¢ƒå»ºæ¨¡ä¸ºå±æ€§å›¾ï¼Œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå­¦ä¹ èŠ‚ç‚¹ï¼ˆä¸»æœºã€æœåŠ¡ç­‰ï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå¼•å…¥å…³ç³»å½’çº³åç½®ã€‚è¿™ç§æ–¹æ³•å…è®¸æ™ºèƒ½ä½“å­¦ä¹ æ›´é€šç”¨çš„ç½‘ç»œå®‰å…¨ç­–ç•¥ï¼Œè€Œä¸æ˜¯ä»…ä»…è®°ä½ç‰¹å®šæ‹“æ‰‘ç»“æ„çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å°†åŠ¨ä½œè§†ä¸ºå¯¹å›¾çš„ç¼–è¾‘ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´çµæ´»åœ°é€‚åº”ä¸åŒçš„ç½‘ç»œç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶æ˜¯ä¸€ä¸ªåŒäººåšå¼ˆçš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œå…¶ä¸­æ™ºèƒ½ä½“æ‰®æ¼”é˜²å¾¡è€…çš„è§’è‰²ï¼Œä¸æ”»å‡»è€…è¿›è¡Œå¯¹æŠ—ã€‚ç¯å¢ƒçŠ¶æ€è¢«è¡¨ç¤ºä¸ºå±æ€§å›¾ï¼ŒèŠ‚ç‚¹ä»£è¡¨ç½‘ç»œä¸­çš„å®ä½“ï¼ˆå¦‚ä¸»æœºã€æœåŠ¡ï¼‰ï¼Œè¾¹ä»£è¡¨å®ä½“ä¹‹é—´çš„è¿æ¥å…³ç³»ã€‚æ™ºèƒ½ä½“é€šè¿‡è§‚å¯Ÿå›¾çš„çŠ¶æ€ï¼Œé€‰æ‹©åŠ¨ä½œï¼ˆå¦‚éš”ç¦»ä¸»æœºã€æ‰“è¡¥ä¸ï¼‰ï¼Œå¹¶æ ¹æ®ç¯å¢ƒçš„åé¦ˆï¼ˆå¥–åŠ±ï¼‰æ¥æ›´æ–°ç­–ç•¥ã€‚æ•´ä¸ªè¿‡ç¨‹æ˜¯ä¸€ä¸ªéƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºä½¿ç”¨å›¾ç»“æ„æ¥è¡¨ç¤ºç½‘ç»œç¯å¢ƒï¼Œå¹¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œæ¥å­¦ä¹ ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•å¼•å…¥äº†å…³ç³»å½’çº³åç½®ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç½‘ç»œä¸­å®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºåˆ—è¡¨è¡¨ç¤ºçš„æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºå›¾çš„æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç½‘ç»œæ‹“æ‰‘ç»“æ„çš„æœ¬è´¨ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¥å¤„ç†å›¾ç»“æ„æ•°æ®ã€‚å…·ä½“çš„GNNç»“æ„å’Œè®­ç»ƒç»†èŠ‚æœªçŸ¥ï¼Œä½†å¯ä»¥æ¨æµ‹ä½¿ç”¨äº†æ¶ˆæ¯ä¼ é€’æœºåˆ¶æ¥èšåˆé‚»å±…èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨æŸç§å½¢å¼çš„æ³¨æ„åŠ›æœºåˆ¶æ¥å…³æ³¨é‡è¦çš„èŠ‚ç‚¹å’Œè¾¹ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦å¹³è¡¡é˜²å¾¡æˆåŠŸç‡å’Œèµ„æºæ¶ˆè€—ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ­¤å¤„æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå›¾çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨é˜²å¾¡æœªè§è¿‡çš„ç½‘ç»œæ”»å‡»æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚å…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­æåˆ°â€œå¤§å¤§ä¼˜äºç°æœ‰æŠ€æœ¯â€ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé›¶æ ·æœ¬é€‚åº”æ–°çš„ç½‘ç»œç¯å¢ƒï¼Œè¡¨æ˜å…¶å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å„ç§å¤æ‚å’Œå¤šæ™ºèƒ½ä½“ç¯å¢ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨é˜²å¾¡ç³»ç»Ÿï¼Œæå‡é˜²å¾¡ç³»ç»Ÿçš„è‡ªé€‚åº”æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚é€šè¿‡å­¦ä¹ é€šç”¨çš„ç½‘ç»œå®‰å…¨ç­–ç•¥ï¼Œé˜²å¾¡ç³»ç»Ÿèƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹æ–°å‹ç½‘ç»œæ”»å‡»ï¼Œé™ä½äººå·¥å¹²é¢„çš„éœ€æ±‚ï¼Œæé«˜ç½‘ç»œå®‰å…¨é˜²æŠ¤æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºäº‘å®‰å…¨ã€ç‰©è”ç½‘å®‰å…¨ç­‰é¢†åŸŸï¼Œæ„å»ºæ›´åŠ å®‰å…¨å¯é çš„ç½‘ç»œç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep reinforcement learning (RL) is emerging as a viable strategy for automated cyber defense (ACD). The traditional RL approach represents networks as a list of computers in various states of safety or threat. Unfortunately, these models are forced to overfit to specific network topologies, rendering them ineffective when faced with even small environmental perturbations. In this work, we frame ACD as a two-player context-based partially observable Markov decision problem with observations represented as attributed graphs. This approach allows our agents to reason through the lens of relational inductive bias. Agents learn how to reason about hosts interacting with other system entities in a more general manner, and their actions are understood as edits to the graph representing the environment. By introducing this bias, we will show that our agents can better reason about the states of networks and zero-shot adapt to new ones. We show that this approach outperforms the state-of-the-art by a wide margin, and makes our agents capable of defending never-before-seen networks against a wide range of adversaries in a variety of complex, and multi-agent environments.

