---
layout: default
title: MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning
---

# MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16078" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16078v1</a>
  <a href="https://arxiv.org/pdf/2509.16078.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16078v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16078v1', 'MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Xu, Yitian Zhang, Yun Fu

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: Accepted by ICDM 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒæ©ç è‡ªç¼–ç å™¨DMAEï¼Œç”¨äºæ— ç›‘ç£å¤šå…ƒæ—¶é—´åºåˆ—è¡¨ç¤ºå­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šå…ƒæ—¶é—´åºåˆ—` `æ— ç›‘ç£å­¦ä¹ ` `è¡¨ç¤ºå­¦ä¹ ` `æ©ç è‡ªç¼–ç å™¨` `æ—¶é—´åºåˆ—å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ— ç›‘ç£å¤šå…ƒæ—¶é—´åºåˆ—è¡¨ç¤ºå­¦ä¹ æ—¨åœ¨ä»åŸå§‹åºåˆ—ä¸­æå–ç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„è¡¨ç¤ºï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥åŒæ—¶æ•æ‰æ—¶é—´è¿è´¯æ€§å’Œè¯­ä¹‰ä¿¡æ¯ã€‚
2. DMAEé€šè¿‡åŒæ©ç è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œåˆ©ç”¨é‡å»ºæ©ç å€¼å’Œä¼°è®¡æ©ç ç‰¹å¾æ½œåœ¨è¡¨ç¤ºä¸¤ä¸ªäº’è¡¥ä»»åŠ¡ï¼Œå­¦ä¹ æ—¶é—´åºåˆ—çš„è¡¨ç¤ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDMAEåœ¨åˆ†ç±»ã€å›å½’å’Œé¢„æµ‹ç­‰ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½å¾—åˆ°æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ— ç›‘ç£å¤šå…ƒæ—¶é—´åºåˆ—ï¼ˆMTSï¼‰è¡¨ç¤ºå­¦ä¹ çš„æ–°å‹æ©ç æ—¶é—´åºåˆ—å»ºæ¨¡æ¡†æ¶â€”â€”åŒæ©ç è‡ªç¼–ç å™¨ï¼ˆDMAEï¼‰ã€‚DMAEæ„å»ºäº†ä¸¤ä¸ªäº’è¡¥çš„é¢„è®­ç»ƒä»»åŠ¡ï¼š(1)åŸºäºå¯è§å±æ€§é‡å»ºæ©ç å€¼ï¼›(2)åœ¨æ•™å¸ˆç¼–ç å™¨çš„æŒ‡å¯¼ä¸‹ï¼Œä¼°è®¡æ©ç ç‰¹å¾çš„æ½œåœ¨è¡¨ç¤ºã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è¡¨ç¤ºè´¨é‡ï¼Œå¼•å…¥äº†ç‰¹å¾çº§å¯¹é½çº¦æŸï¼Œé¼“åŠ±é¢„æµ‹çš„æ½œåœ¨è¡¨ç¤ºä¸æ•™å¸ˆçš„è¾“å‡ºå¯¹é½ã€‚é€šè¿‡è”åˆä¼˜åŒ–è¿™äº›ç›®æ ‡ï¼ŒDMAEå­¦ä¹ åˆ°æ—¶é—´ä¸Šè¿è´¯ä¸”è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºã€‚åœ¨åˆ†ç±»ã€å›å½’å’Œé¢„æµ‹ä»»åŠ¡ä¸­çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œå¹¶å–å¾—äº†æŒç»­ä¸”å“è¶Šçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ— ç›‘ç£å¤šå…ƒæ—¶é—´åºåˆ—ï¼ˆMTSï¼‰è¡¨ç¤ºå­¦ä¹ é—®é¢˜ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸éš¾ä»¥åœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°æå–æ—¢å…·æœ‰æ—¶é—´è¿è´¯æ€§åˆåŒ…å«ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯çš„è¡¨ç¤ºã€‚è¿™é™åˆ¶äº†MTSè¡¨ç¤ºåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä¾‹å¦‚åˆ†ç±»ã€å›å½’å’Œé¢„æµ‹ç­‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ©ç è‡ªç¼–ç å™¨ï¼ˆMasked Autoencoderï¼‰çš„æ€æƒ³ï¼Œé€šè¿‡æ„å»ºä¸¤ä¸ªäº’è¡¥çš„é¢„è®­ç»ƒä»»åŠ¡æ¥å­¦ä¹ MTSçš„è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œç¬¬ä¸€ä¸ªä»»åŠ¡æ˜¯é‡å»ºè¢«æ©ç æ‰çš„æ—¶é—´åºåˆ—å€¼ï¼Œç¬¬äºŒä¸ªä»»åŠ¡æ˜¯ä¼°è®¡è¢«æ©ç æ‰çš„ç‰¹å¾çš„æ½œåœ¨è¡¨ç¤ºã€‚é€šè¿‡è¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ—¶é—´åºåˆ—ä¸­çš„æ—¶é—´ä¾èµ–å…³ç³»å’Œç‰¹å¾ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDMAEçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªå­¦ç”Ÿç¼–ç å™¨ã€ä¸€ä¸ªæ•™å¸ˆç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ã€‚é¦–å…ˆï¼Œå¯¹è¾“å…¥MTSæ•°æ®è¿›è¡ŒåŒé‡æ©ç ï¼Œå³éšæœºæ©ç æ‰ä¸€éƒ¨åˆ†æ—¶é—´ç‚¹çš„å€¼å’Œä¸€éƒ¨åˆ†ç‰¹å¾ã€‚ç„¶åï¼Œå­¦ç”Ÿç¼–ç å™¨æ¥æ”¶è¢«æ©ç åçš„æ•°æ®ï¼Œå¹¶è¾“å‡ºæ½œåœ¨è¡¨ç¤ºã€‚æ•™å¸ˆç¼–ç å™¨æ¥æ”¶æœªè¢«æ©ç çš„æ•°æ®ï¼Œå¹¶è¾“å‡ºç›®æ ‡æ½œåœ¨è¡¨ç¤ºã€‚è§£ç å™¨æ¥æ”¶å­¦ç”Ÿç¼–ç å™¨çš„è¾“å‡ºï¼Œå¹¶å°è¯•é‡å»ºè¢«æ©ç æ‰çš„å€¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šDMAEçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŒé‡æ©ç ç­–ç•¥å’Œç‰¹å¾çº§å¯¹é½çº¦æŸã€‚åŒé‡æ©ç ç­–ç•¥ä½¿å¾—æ¨¡å‹èƒ½å¤ŸåŒæ—¶å­¦ä¹ æ—¶é—´å’Œç‰¹å¾ä¸¤ä¸ªç»´åº¦ä¸Šçš„ä¾èµ–å…³ç³»ã€‚ç‰¹å¾çº§å¯¹é½çº¦æŸåˆ™é¼“åŠ±å­¦ç”Ÿç¼–ç å™¨å­¦ä¹ åˆ°çš„æ½œåœ¨è¡¨ç¤ºä¸æ•™å¸ˆç¼–ç å™¨å­¦ä¹ åˆ°çš„æ½œåœ¨è¡¨ç¤ºå¯¹é½ï¼Œä»è€Œæé«˜è¡¨ç¤ºçš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šDMAEä½¿ç”¨äº†Transformerä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨çš„åŸºæœ¬æ„å»ºå—ã€‚æŸå¤±å‡½æ•°åŒ…å«ä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†æ˜¯é‡å»ºæŸå¤±ï¼Œç”¨äºè¡¡é‡è§£ç å™¨é‡å»ºè¢«æ©ç å€¼çš„å‡†ç¡®ç¨‹åº¦ï¼›å¦ä¸€éƒ¨åˆ†æ˜¯å¯¹é½æŸå¤±ï¼Œç”¨äºè¡¡é‡å­¦ç”Ÿç¼–ç å™¨å’Œæ•™å¸ˆç¼–ç å™¨è¾“å‡ºçš„æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„å·®å¼‚ã€‚è®ºæ–‡ä¸­è¿˜è¯¦ç»†æè¿°äº†æ©ç æ¯”ä¾‹ã€Transformerå±‚æ•°ç­‰è¶…å‚æ•°çš„è®¾ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDMAEåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼Œé’ˆå¯¹åˆ†ç±»ã€å›å½’å’Œé¢„æµ‹ä»»åŠ¡ï¼Œå‡å–å¾—äº†æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›åˆ†ç±»ä»»åŠ¡ä¸Šï¼ŒDMAEçš„å‡†ç¡®ç‡æ¯”æœ€ä½³åŸºçº¿æ–¹æ³•æé«˜äº†5%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒéªŒè¯äº†åŒé‡æ©ç ç­–ç•¥å’Œç‰¹å¾çº§å¯¹é½çº¦æŸçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å®ƒä»¬å¯¹æå‡è¡¨ç¤ºè´¨é‡çš„è´¡çŒ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§æ¶‰åŠå¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®çš„é¢†åŸŸï¼Œä¾‹å¦‚é‡‘èå¸‚åœºçš„è‚¡ç¥¨ä»·æ ¼é¢„æµ‹ã€åŒ»ç–—å¥åº·é¢†åŸŸçš„ç”Ÿç†ä¿¡å·åˆ†æã€å·¥ä¸šåˆ¶é€ é¢†åŸŸçš„è®¾å¤‡æ•…éšœè¯Šæ–­ä»¥åŠäº¤é€šè¿è¾“é¢†åŸŸçš„äº¤é€šæµé‡é¢„æµ‹ç­‰ã€‚é€šè¿‡å­¦ä¹ é«˜è´¨é‡çš„MTSè¡¨ç¤ºï¼Œå¯ä»¥æå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶ä¸ºç›¸å…³é¢†åŸŸçš„æ™ºèƒ½åŒ–åº”ç”¨æä¾›æœ‰åŠ›æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unsupervised multivariate time series (MTS) representation learning aims to extract compact and informative representations from raw sequences without relying on labels, enabling efficient transfer to diverse downstream tasks. In this paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked time-series modeling framework for unsupervised MTS representation learning. DMAE formulates two complementary pretext tasks: (1) reconstructing masked values based on visible attributes, and (2) estimating latent representations of masked features, guided by a teacher encoder. To further improve representation quality, we introduce a feature-level alignment constraint that encourages the predicted latent representations to align with the teacher's outputs. By jointly optimizing these objectives, DMAE learns temporally coherent and semantically rich representations. Comprehensive evaluations across classification, regression, and forecasting tasks demonstrate that our approach achieves consistent and superior performance over competitive baselines.

