---
layout: default
title: CoUn: Empowering Machine Unlearning via Contrastive Learning
---

# CoUn: Empowering Machine Unlearning via Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16391" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16391v2</a>
  <a href="https://arxiv.org/pdf/2509.16391.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16391v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16391v2', 'CoUn: Empowering Machine Unlearning via Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yasser H. Khalil, Mehdi Setayesh, Hongliang Li

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19 (æ›´æ–°: 2025-10-17)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CoUnï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ å¢å¼ºæœºå™¨å­¦ä¹ çš„ä¸å¯å­¦ä¹ æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨å­¦ä¹ ä¸å¯å­¦ä¹ æ€§` `å¯¹æ¯”å­¦ä¹ ` `æ•°æ®éšç§` `æ¨¡å‹å¾®è°ƒ` `è¡¨ç¤ºå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨å­¦ä¹ ä¸å¯å­¦ä¹ æ–¹æ³•åœ¨ç§»é™¤ç‰¹å®šæ•°æ®å½±å“æ–¹é¢æ•ˆæœæœ‰é™ï¼Œæ— æ³•æœ‰æ•ˆâ€œé—å¿˜â€ã€‚
2. CoUné€šè¿‡å¯¹æ¯”å­¦ä¹ è°ƒæ•´æ•°æ®è¡¨ç¤ºï¼Œæ¨¡æ‹Ÿä»…ç”¨ä¿ç•™æ•°æ®è®­ç»ƒçš„æ¨¡å‹è¡Œä¸ºï¼Œå®ç°æœ‰æ•ˆé—å¿˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCoUnåœ¨å¤šç§æ•°æ®é›†å’Œæ¨¡å‹ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”å¯å¢å¼ºç°æœ‰æ–¹æ³•çš„ä¸å¯å­¦ä¹ æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºå™¨å­¦ä¹ ä¸å¯å­¦ä¹ æ€§ï¼ˆMUï¼‰æ—¨åœ¨ä»å·²è®­ç»ƒæ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šâ€œé—å¿˜â€æ•°æ®çš„å½±å“ï¼ŒåŒæ—¶ä¿ç•™å…¶å¯¹å‰©ä½™â€œä¿ç•™â€æ•°æ®çš„çŸ¥è¯†ã€‚ç°æœ‰çš„åŸºäºæ ‡ç­¾æ“ä½œæˆ–æ¨¡å‹æƒé‡æ‰°åŠ¨çš„MUæ–¹æ³•é€šå¸¸åœ¨ä¸å¯å­¦ä¹ æ€§æ–¹é¢æ•ˆæœæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoUnï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„MUæ¡†æ¶ï¼Œå…¶çµæ„Ÿæ¥è‡ªäºä¸€ä¸ªè§‚å¯Ÿï¼šä»…ä½¿ç”¨ä¿ç•™æ•°æ®ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒçš„æ¨¡å‹ä¼šæ ¹æ®é—å¿˜æ•°æ®ä¸ä¿ç•™æ•°æ®çš„è¯­ä¹‰ç›¸ä¼¼æ€§å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚CoUné€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰å’Œç›‘ç£å­¦ä¹ æ¥è°ƒæ•´å­¦ä¹ åˆ°çš„æ•°æ®è¡¨ç¤ºæ¥æ¨¡æ‹Ÿè¿™ç§è¡Œä¸ºï¼Œè¿™äº›å­¦ä¹ ä»…åº”ç”¨äºä¿ç•™æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼ŒCoUnï¼ˆ1ï¼‰åˆ©ç”¨æ•°æ®æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä½¿ç”¨CLé—´æ¥è°ƒæ•´é—å¿˜è¡¨ç¤ºï¼Œä»¥åŠï¼ˆ2ï¼‰é€šè¿‡ç›‘ç£å­¦ä¹ å°†ä¿ç•™è¡¨ç¤ºä¿æŒåœ¨å…¶å„è‡ªçš„ç°‡ä¸­ã€‚åœ¨å„ç§æ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCoUnåœ¨ä¸å¯å­¦ä¹ æ€§æ–¹é¢å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„MUåŸºçº¿ã€‚æ­¤å¤–ï¼Œå°†æˆ‘ä»¬çš„CLæ¨¡å—é›†æˆåˆ°ç°æœ‰åŸºçº¿ä¸­å¯ä»¥å¢å¼ºå…¶ä¸å¯å­¦ä¹ æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨å­¦ä¹ ä¸å¯å­¦ä¹ æ€§ï¼ˆMUï¼‰é—®é¢˜ï¼Œå³å¦‚ä½•ä»å·²è®­ç»ƒæ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šâ€œé—å¿˜â€æ•°æ®çš„å½±å“ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹å¯¹â€œä¿ç•™â€æ•°æ®çš„çŸ¥è¯†ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºæ ‡ç­¾æ“ä½œæˆ–æ¨¡å‹æƒé‡æ‰°åŠ¨çš„æ–¹æ³•ï¼Œåœ¨å®ç°æœ‰æ•ˆçš„ä¸å¯å­¦ä¹ æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å½»åº•æ¶ˆé™¤é—å¿˜æ•°æ®çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoUnçš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡æ‹Ÿä»…ä½¿ç”¨ä¿ç•™æ•°æ®ä»å¤´è®­ç»ƒæ¨¡å‹æ—¶çš„è¡Œä¸ºã€‚è§‚å¯Ÿè¡¨æ˜ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šæ ¹æ®é—å¿˜æ•°æ®ä¸ä¿ç•™æ•°æ®çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ¥å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚CoUné€šè¿‡è°ƒæ•´æ•°æ®è¡¨ç¤ºï¼Œä½¿å¾—æ¨¡å‹å¯¹é—å¿˜æ•°æ®çš„åˆ†ç±»æ›´æ¥è¿‘äºä»…ç”¨ä¿ç•™æ•°æ®è®­ç»ƒçš„ç»“æœï¼Œä»è€Œå®ç°ä¸å¯å­¦ä¹ æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoUnæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šå¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰æ¨¡å—å’Œç›‘ç£å­¦ä¹ æ¨¡å—ã€‚è¿™ä¸¤ä¸ªæ¨¡å—éƒ½åªåº”ç”¨äºä¿ç•™æ•°æ®ã€‚CLæ¨¡å—åˆ©ç”¨æ•°æ®æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œé—´æ¥è°ƒæ•´é—å¿˜æ•°æ®çš„è¡¨ç¤ºï¼Œä½¿å¾—é—å¿˜æ•°æ®è¿œç¦»ä¿ç•™æ•°æ®çš„ç°‡ã€‚ç›‘ç£å­¦ä¹ æ¨¡å—åˆ™ç”¨äºç»´æŒä¿ç•™æ•°æ®åœ¨å…¶å„è‡ªç°‡ä¸­çš„ä½ç½®ï¼Œé˜²æ­¢æ¨¡å‹é—å¿˜ä¿ç•™æ•°æ®çš„çŸ¥è¯†ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆä½¿ç”¨åŸå§‹æ•°æ®è®­ç»ƒä¸€ä¸ªåˆå§‹æ¨¡å‹ï¼Œç„¶åä½¿ç”¨CoUnæ¡†æ¶å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°ä¸å¯å­¦ä¹ æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoUnçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¥é—´æ¥å½±å“é—å¿˜æ•°æ®çš„è¡¨ç¤ºã€‚ä¸ç›´æ¥ä¿®æ”¹é—å¿˜æ•°æ®æˆ–æ¨¡å‹æƒé‡çš„æ–¹æ³•ä¸åŒï¼ŒCoUné€šè¿‡è°ƒæ•´ä¿ç•™æ•°æ®çš„è¡¨ç¤ºï¼Œä½¿å¾—æ¨¡å‹å¯¹é—å¿˜æ•°æ®çš„åˆ†ç±»å‘ç”Ÿæ”¹å˜ï¼Œä»è€Œå®ç°ä¸å¯å­¦ä¹ æ€§ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥æ“ä½œé—å¿˜æ•°æ®å¯èƒ½å¸¦æ¥çš„é£é™©ï¼Œå¹¶ä¸”æ›´åŠ é«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šCoUnçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç”¨äºè¡¡é‡ä¿ç•™æ•°æ®ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå¹¶ä¿ƒä½¿æ¨¡å‹å°†ç›¸ä¼¼çš„ä¿ç•™æ•°æ®èšé›†åœ¨ä¸€èµ·ï¼›2ï¼‰ç›‘ç£å­¦ä¹ æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç”¨äºç»´æŒä¿ç•™æ•°æ®åœ¨å…¶å„è‡ªç°‡ä¸­çš„ä½ç½®ï¼›3ï¼‰å¦‚ä½•é€‰æ‹©åˆé€‚çš„å¯¹æ¯”å­¦ä¹ æ ·æœ¬ï¼Œä»¥ç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆåœ°è°ƒæ•´é—å¿˜æ•°æ®çš„è¡¨ç¤ºã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„é€‰æ‹©å–å†³äºå…·ä½“çš„åº”ç”¨åœºæ™¯å’Œæ•°æ®é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCoUnåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šå‡ä¼˜äºç°æœ‰çš„ä¸å¯å­¦ä¹ æ€§åŸºçº¿æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒCoUnåœ¨ä¸å¯å­¦ä¹ æ€§æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹åœ¨ä¿ç•™æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°†CoUnçš„å¯¹æ¯”å­¦ä¹ æ¨¡å—é›†æˆåˆ°ç°æœ‰åŸºçº¿æ–¹æ³•ä¸­ï¼Œä¹Ÿèƒ½æœ‰æ•ˆæå‡å…¶ä¸å¯å­¦ä¹ æ€§æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoUnåœ¨éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„åœºæ™¯ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è”é‚¦å­¦ä¹ ã€åœ¨çº¿å¹¿å‘Šæ¨èã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚é€šè¿‡CoUnï¼Œå¯ä»¥å®‰å…¨åœ°ç§»é™¤ç”¨æˆ·ä¸å†å¸Œæœ›è¢«æ¨¡å‹ä½¿ç”¨çš„æ•°æ®ï¼ŒåŒæ—¶ä¿è¯æ¨¡å‹åœ¨å…¶ä»–æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ„å»ºæ›´åŠ å®‰å…¨ã€å¯é å’Œè´Ÿè´£ä»»çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼Œæå‡ç”¨æˆ·å¯¹AIç³»ç»Ÿçš„ä¿¡ä»»åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Machine unlearning (MU) aims to remove the influence of specific "forget" data from a trained model while preserving its knowledge of the remaining "retain" data. Existing MU methods based on label manipulation or model weight perturbations often achieve limited unlearning effectiveness. To address this, we introduce CoUn, a novel MU framework inspired by the observation that a model retrained from scratch using only retain data classifies forget data based on their semantic similarity to the retain data. CoUn emulates this behavior by adjusting learned data representations through contrastive learning (CL) and supervised learning, applied exclusively to retain data. Specifically, CoUn (1) leverages semantic similarity between data samples to indirectly adjust forget representations using CL, and (2) maintains retain representations within their respective clusters through supervised learning. Extensive experiments across various datasets and model architectures show that CoUn consistently outperforms state-of-the-art MU baselines in unlearning effectiveness. Additionally, integrating our CL module into existing baselines empowers their unlearning effectiveness.

