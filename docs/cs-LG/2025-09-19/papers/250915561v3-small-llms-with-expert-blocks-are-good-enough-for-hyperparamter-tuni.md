---
layout: default
title: Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning
---

# Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15561" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15561v3</a>
  <a href="https://arxiv.org/pdf/2509.15561.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15561v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15561v3', 'Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Om Naphade, Saksham Bansal, Parikshit Pareek

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19 (æ›´æ–°: 2025-09-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸“å®¶å—æ¡†æ¶ä»¥ä¼˜åŒ–å°å‹LLMçš„è¶…å‚æ•°è°ƒä¼˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¶…å‚æ•°è°ƒä¼˜` `å°å‹è¯­è¨€æ¨¡å‹` `ä¸“å®¶å—æ¡†æ¶` `è½¨è¿¹ä¸Šä¸‹æ–‡æ‘˜è¦å™¨` `æœºå™¨å­¦ä¹ ` `è®¡ç®—æ•ˆç‡` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¶…å‚æ•°è°ƒä¼˜æ–¹æ³•åœ¨å¤„ç†å¤§å‹æ¨¡å‹æ—¶è®¡ç®—æˆæœ¬é«˜ä¸”è¿‡ç¨‹ä¸é€æ˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„ä¸“å®¶å—æ¡†æ¶ç»“åˆè½¨è¿¹ä¸Šä¸‹æ–‡æ‘˜è¦å™¨ï¼ˆTCSï¼‰ï¼Œä½¿å°å‹LLMèƒ½å¤Ÿæœ‰æ•ˆåˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–è¿›å±•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºTCSçš„HPTç®¡é“åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¸GPT-4çš„æ€§èƒ½ç›¸è¿‘ï¼Œå±•ç¤ºäº†å°å‹æ¨¡å‹çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¶…å‚æ•°è°ƒä¼˜ï¼ˆHPTï¼‰æ˜¯æœºå™¨å­¦ä¹ ç®¡é“ä¸­å¿…ä¸å¯å°‘çš„ä¸€æ­¥ï¼Œä½†åœ¨å¤§å‹æ¨¡å‹ä¸­å˜å¾—è®¡ç®—æˆæœ¬é«˜ä¸”ä¸é€æ˜ã€‚è¿‘æœŸç ”ç©¶æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨HPTä¸­çš„åº”ç”¨ï¼Œä½†å¤§å¤šæ•°ä¾èµ–äºè¶…è¿‡1000äº¿å‚æ•°çš„æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå°å‹LLMçš„ä¸“å®¶å—æ¡†æ¶ç”¨äºHPTï¼Œæ ¸å¿ƒæ˜¯è½¨è¿¹ä¸Šä¸‹æ–‡æ‘˜è¦å™¨ï¼ˆTCSï¼‰ï¼Œè¯¥ç¡®å®šæ€§æ¨¡å—å°†åŸå§‹è®­ç»ƒè½¨è¿¹è½¬åŒ–ä¸ºç»“æ„åŒ–ä¸Šä¸‹æ–‡ï¼Œä½¿å°å‹LLMèƒ½å¤Ÿä»¥ä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„å¯é æ€§åˆ†æä¼˜åŒ–è¿›å±•ã€‚ä½¿ç”¨ä¸¤ä¸ªæœ¬åœ°è¿è¡Œçš„LLMï¼ˆphi4:reasoning14Bå’Œqwen2.5-coder:32Bï¼‰å’Œ10æ¬¡è¯•éªŒé¢„ç®—ï¼ŒåŸºäºTCSçš„HPTç®¡é“åœ¨å…­ä¸ªå¤šæ ·åŒ–ä»»åŠ¡ä¸­å®ç°äº†å¹³å‡æ€§èƒ½ä¸GPT-4ç›¸å·®çº¦0.9ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¶…å‚æ•°è°ƒä¼˜åœ¨å¤§å‹æ¨¡å‹ä¸­è®¡ç®—æˆæœ¬é«˜å’Œä¸é€æ˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå‚æ•°é‡å·¨å¤§çš„æ¨¡å‹ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—å’Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸“å®¶å—æ¡†æ¶ï¼Œåˆ©ç”¨è½¨è¿¹ä¸Šä¸‹æ–‡æ‘˜è¦å™¨ï¼ˆTCSï¼‰å°†è®­ç»ƒè½¨è¿¹è½¬åŒ–ä¸ºç»“æ„åŒ–ä¸Šä¸‹æ–‡ï¼Œä»è€Œä½¿å°å‹LLMèƒ½å¤Ÿå¯é åœ°åˆ†æä¼˜åŒ–è¿›å±•ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè½¨è¿¹ä¸Šä¸‹æ–‡æ‘˜è¦å™¨ï¼ˆTCSï¼‰å’Œå°å‹LLMã€‚TCSè´Ÿè´£å¤„ç†åŸå§‹è®­ç»ƒæ•°æ®å¹¶ç”Ÿæˆç»“æ„åŒ–ä¸Šä¸‹æ–‡ï¼Œå°å‹LLMåˆ™åŸºäºè¿™äº›ä¸Šä¸‹æ–‡è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥TCSï¼Œä½¿å¾—å°å‹LLMåœ¨HPTä¸­èƒ½å¤Ÿè¾¾åˆ°ä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—é™ä½äº†å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä½¿ç”¨äº†phi4:reasoning14Bå’Œqwen2.5-coder:32Bä¸¤ä¸ªå°å‹LLMï¼Œé‡‡ç”¨10æ¬¡è¯•éªŒé¢„ç®—è¿›è¡Œè¯„ä¼°ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„å…·ä½“ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºTCSçš„HPTç®¡é“åœ¨å…­ä¸ªä¸åŒä»»åŠ¡ä¸Šå®ç°äº†ä¸GPT-4ç›¸å·®çº¦0.9ä¸ªç™¾åˆ†ç‚¹çš„å¹³å‡æ€§èƒ½ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œå°å‹LLMåœ¨è¶…å‚æ•°è°ƒä¼˜ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¶…å‚æ•°è°ƒä¼˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚é€šè¿‡ä½¿ç”¨å°å‹LLMå’Œä¸“å®¶å—æ¡†æ¶ï¼Œç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆå¯ä»¥åœ¨ä¿æŒé«˜æ•ˆæ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚è¿™ä¸€æ–¹æ³•çš„æˆåŠŸåº”ç”¨å¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šå°å‹æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„ä½¿ç”¨ï¼Œæå‡æœºå™¨å­¦ä¹ çš„å¯åŠæ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML) pipelines but becomes computationally expensive and opaque with larger models. Recently, Large Language Models (LLMs) have been explored for HPT, yet most rely on models exceeding 100 billion parameters. We propose an Expert Block Framework for HPT using Small LLMs. At its core is the Trajectory Context Summarizer (TCS), a deterministic block that transforms raw training trajectories into structured context, enabling small LLMs to analyze optimization progress with reliability comparable to larger models. Using two locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9 percentage points of GPT-4 across six diverse tasks.

