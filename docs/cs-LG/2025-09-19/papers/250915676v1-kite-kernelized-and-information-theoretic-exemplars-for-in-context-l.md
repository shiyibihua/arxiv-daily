---
layout: default
title: KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning
---

# KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15676" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.15676v1</a>
  <a href="https://arxiv.org/pdf/2509.15676.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15676v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15676v1', 'KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Vaibhav Singh, Soumya Suvra Ghosal, Kapu Nirmal Joshua, Soumyabrata Pal, Sayak Ray Chowdhury

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-19

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**KITEÔºöÂü∫‰∫éÊ†∏ÊñπÊ≥ïÂíå‰ø°ÊÅØËÆ∫ÁöÑ‰∏ä‰∏ãÊñáÂ≠¶‰π†ËåÉ‰æãÈÄâÊã©ÔºåÊèêÂçáÂ∞èÊ†∑Êú¨ÂàÜÁ±ªÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰∏ä‰∏ãÊñáÂ≠¶‰π†` `Â∞èÊ†∑Êú¨Â≠¶‰π†` `‰ø°ÊÅØËÆ∫` `Ê†∏ÊñπÊ≥ï` `Á§∫‰æãÈÄâÊã©`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏ä‰∏ãÊñáÂ≠¶‰π†ÂèóÈôê‰∫é‰∏ä‰∏ãÊñáÁ™óÂè£ÔºåÂ¶Ç‰ΩïÈÄâÊã©ÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑÁ§∫‰æãÊàê‰∏∫ÂÖ≥ÈîÆÊåëÊàòÔºå‰º†ÁªüËøëÈÇªÊñπÊ≥ïÂú®È´òÁª¥Á©∫Èó¥Ë°®Áé∞‰∏ç‰Ω≥ÔºåÁº∫‰πèÂ§öÊ†∑ÊÄß„ÄÇ
2. ËÆ∫Êñá‰ªé‰ø°ÊÅØËÆ∫ËßíÂ∫¶Âá∫ÂèëÔºåÂ∞ÜLLMÂª∫Ê®°‰∏∫Á∫øÊÄßÂáΩÊï∞ÔºåÊääÁ§∫‰æãÈÄâÊã©ËΩ¨Âåñ‰∏∫ÁâπÂÆöÊü•ËØ¢ÁöÑ‰ºòÂåñÈóÆÈ¢òÔºåÊó®Âú®ÊúÄÂ∞èÂåñËØ•Êü•ËØ¢ÁöÑÈ¢ÑÊµãËØØÂ∑Æ„ÄÇ
3. ÈÄöËøáÊ†∏ÊñπÊ≥ïÂú®È´òÁª¥Á©∫Èó¥Êìç‰ΩúÔºåÂπ∂ÂºïÂÖ•Ê≠£ÂàôÂåñÂô®ÈºìÂä±Á§∫‰æãÂ§öÊ†∑ÊÄßÔºåÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂàÜÁ±ª‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÊ†áÂáÜÊ£ÄÁ¥¢ÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏ä‰∏ãÊñáÂ≠¶‰π†(ICL)Â∑≤Êàê‰∏∫‰∏ÄÁßçÂº∫Â§ßÁöÑËåÉÂºèÔºåÂÆÉ‰ªÖ‰ΩøÁî®ÊèêÁ§∫‰∏≠ÂëàÁé∞ÁöÑÂ∞ëÈáèÁ≤æÂøÉÈÄâÊã©ÁöÑÁâπÂÆö‰∫é‰ªªÂä°ÁöÑÁ§∫‰æãÔºåÂç≥ÂèØÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)ÈÄÇÂ∫î‰∫éÊñ∞ÁöÑÂíåÊï∞ÊçÆÁ®ÄÁº∫ÁöÑ‰ªªÂä°„ÄÇÁÑ∂ËÄåÔºåÈâ¥‰∫éLLMÊúâÈôêÁöÑ‰∏ä‰∏ãÊñáÂ§ßÂ∞èÔºå‰∏Ä‰∏™Ê†πÊú¨ÈóÆÈ¢òÂá∫Áé∞‰∫ÜÔºöÂ∫îËØ•ÈÄâÊã©Âì™‰∫õÁ§∫‰æãÊù•ÊúÄÂ§ßÂåñÁªôÂÆöÁî®Êà∑Êü•ËØ¢ÁöÑÊÄßËÉΩÔºüËôΩÁÑ∂Âü∫‰∫éÊúÄËøëÈÇªÁöÑÊñπÊ≥ï(Â¶ÇKATE)Â∑≤Ë¢´ÂπøÊ≥õÈááÁî®Ôºå‰ΩÜÂÆÉ‰ª¨Âú®È´òÁª¥ÂµåÂÖ•Á©∫Èó¥‰∏≠Â≠òÂú®‰ºóÊâÄÂë®Áü•ÁöÑÁº∫ÁÇπÔºåÂåÖÊã¨Ê≥õÂåñËÉΩÂäõÂ∑ÆÂíåÁº∫‰πèÂ§öÊ†∑ÊÄß„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨‰ªéÂéüÂàôÊÄßÁöÑ„ÄÅ‰ø°ÊÅØËÆ∫È©±Âä®ÁöÑËßíÂ∫¶Á†îÁ©∂ICL‰∏≠ÁöÑÁ§∫‰æãÈÄâÊã©ÈóÆÈ¢ò„ÄÇÊàë‰ª¨È¶ñÂÖàÂ∞ÜLLMÂª∫Ê®°‰∏∫ËæìÂÖ•ÂµåÂÖ•‰∏äÁöÑÁ∫øÊÄßÂáΩÊï∞ÔºåÂπ∂Â∞ÜÁ§∫‰æãÈÄâÊã©‰ªªÂä°ÂÆö‰πâ‰∏∫ÁâπÂÆö‰∫éÊü•ËØ¢ÁöÑ‰ºòÂåñÈóÆÈ¢òÔºö‰ªéËæÉÂ§ßÁöÑÁ§∫‰æãÂ∫ì‰∏≠ÈÄâÊã©‰∏Ä‰∏™Á§∫‰æãÂ≠êÈõÜÔºå‰ª•ÊúÄÂ∞èÂåñÁâπÂÆöÊü•ËØ¢ÁöÑÈ¢ÑÊµãËØØÂ∑Æ„ÄÇËøôÁßçÂÖ¨ÂºèÈÄöËøáÈíàÂØπÁâπÂÆöÊü•ËØ¢ÂÆû‰æãÁöÑÂáÜÁ°ÆÈ¢ÑÊµãÔºåËÉåÁ¶ª‰∫Ü‰º†ÁªüÁöÑ‰ª•Ê≥õÂåñ‰∏∫‰∏≠ÂøÉÁöÑÂ≠¶‰π†ÁêÜËÆ∫ÊñπÊ≥ï„ÄÇÊàë‰ª¨Êé®ÂØºÂá∫‰∏Ä‰∏™ÂéüÂàôÊÄßÁöÑÊõø‰ª£ÁõÆÊ†áÔºåËØ•ÁõÆÊ†áËøë‰ºº‰∫éÊ¨°Ê®°Ôºå‰ªéËÄåÂèØ‰ª•‰ΩøÁî®ÂÖ∑ÊúâËøë‰ºº‰øùËØÅÁöÑË¥™Â©™ÁÆóÊ≥ï„ÄÇÊàë‰ª¨ÈÄöËøá‰ª•‰∏ãÊñπÂºèËøõ‰∏ÄÊ≠•Â¢ûÂº∫‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÔºö(i) ÁªìÂêàÊ†∏ÊäÄÂ∑ß‰ª•Âú®È´òÁª¥ÁâπÂæÅÁ©∫Èó¥‰∏≠Êìç‰ΩúËÄåÊó†ÈúÄÊòæÂºèÊò†Â∞ÑÔºå‰ª•Âèä(ii) ÂºïÂÖ•Âü∫‰∫éÊúÄ‰ºòËÆæËÆ°ÁöÑÊ≠£ÂàôÂåñÂô®‰ª•ÈºìÂä±ÊâÄÈÄâÁ§∫‰æãÁöÑÂ§öÊ†∑ÊÄß„ÄÇÂú®ÁªèÈ™å‰∏äÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÂú®ÂêÑÁßçÂàÜÁ±ª‰ªªÂä°‰∏≠Áõ∏ÂØπ‰∫éÊ†áÂáÜÊ£ÄÁ¥¢ÊñπÊ≥ïÁöÑÊòæÁùÄÊîπËøõÔºåÁ™ÅÂá∫‰∫ÜÁªìÊûÑÊÑüÁü•„ÄÅÂ§öÊ†∑ÂåñÁ§∫‰æãÈÄâÊã©ÂØπ‰∫éÁé∞ÂÆû‰∏ñÁïå„ÄÅÊ†áÁ≠æÁ®ÄÁº∫Âú∫ÊôØ‰∏≠ICLÁöÑÂ•ΩÂ§Ñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞Êúâ‰∏ä‰∏ãÊñáÂ≠¶‰π†ÊñπÊ≥ïÂú®ÈÄâÊã©Á§∫‰æãÊó∂ÔºåÂú®È´òÁª¥ÂµåÂÖ•Á©∫Èó¥‰∏≠Èù¢‰∏¥Ê≥õÂåñËÉΩÂäõÂ∑ÆÂíåÁº∫‰πèÂ§öÊ†∑ÊÄßÁöÑÈóÆÈ¢ò„ÄÇ‰º†ÁªüÁöÑÊúÄËøëÈÇªÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÈÄâÊã©ÊúÄÂÖ∑‰ª£Ë°®ÊÄßÂíå‰ø°ÊÅØÈáèÁöÑÁ§∫‰æãÔºåÂØºËá¥Ê®°ÂûãÂú®ÁâπÂÆöÊü•ËØ¢‰∏äÁöÑÈ¢ÑÊµãÊÄßËÉΩ‰∏ãÈôç„ÄÇËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â¶Ç‰ΩïÂú®ÊúâÈôêÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£ÂÜÖÔºåÈÄâÊã©ÊúÄ‰Ω≥Á§∫‰æãÂ≠êÈõÜÔºå‰ª•ÊúÄÂ§ßÂåñLLMÂú®ÁâπÂÆöÊü•ËØ¢‰∏äÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÁ§∫‰æãÈÄâÊã©ÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™ÁâπÂÆö‰∫éÊü•ËØ¢ÁöÑ‰ºòÂåñÈóÆÈ¢ò„ÄÇÈÄöËøáÂ∞ÜLLMÂª∫Ê®°‰∏∫ËæìÂÖ•ÂµåÂÖ•‰∏äÁöÑÁ∫øÊÄßÂáΩÊï∞ÔºåÁõÆÊ†áÊòØÈÄâÊã©‰∏Ä‰∏™Á§∫‰æãÂ≠êÈõÜÔºå‰ΩøÂæóÂú®ËØ•Â≠êÈõÜ‰∏äËÆ≠ÁªÉÁöÑÊ®°ÂûãËÉΩÂ§üÊúÄÂ∞èÂåñÂØπÁâπÂÆöÊü•ËØ¢ÁöÑÈ¢ÑÊµãËØØÂ∑Æ„ÄÇËøôÁßçÊñπÊ≥ï‰∏çÂêå‰∫é‰º†ÁªüÁöÑÊ≥õÂåñÂ≠¶‰π†ÔºåËÄåÊòØ‰∏ìÊ≥®‰∫é‰ºòÂåñÁâπÂÆöÊü•ËØ¢ÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöKITEÊñπÊ≥ïÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ≠•È™§Ôºö1) Â∞ÜLLMÂª∫Ê®°‰∏∫ËæìÂÖ•ÂµåÂÖ•ÁöÑÁ∫øÊÄßÂáΩÊï∞„ÄÇ2) Â∞ÜÁ§∫‰æãÈÄâÊã©ÈóÆÈ¢òÂΩ¢ÂºèÂåñ‰∏∫ÊúÄÂ∞èÂåñÁâπÂÆöÊü•ËØ¢È¢ÑÊµãËØØÂ∑ÆÁöÑ‰ºòÂåñÈóÆÈ¢ò„ÄÇ3) Êé®ÂØºÂá∫‰∏Ä‰∏™Ëøë‰ººÊ¨°Ê®°ÁöÑÊõø‰ª£ÁõÆÊ†áÂáΩÊï∞Ôºå‰ª•‰æø‰ΩøÁî®Ë¥™Â©™ÁÆóÊ≥ïËøõË°å‰ºòÂåñ„ÄÇ4) ‰ΩøÁî®Ê†∏ÊäÄÂ∑ßÂú®È´òÁª¥ÁâπÂæÅÁ©∫Èó¥‰∏≠Êìç‰ΩúÔºåÈÅøÂÖçÊòæÂºèÊò†Â∞Ñ„ÄÇ5) ÂºïÂÖ•Âü∫‰∫éÊúÄ‰ºòËÆæËÆ°ÁöÑÊ≠£ÂàôÂåñÂô®Ôºå‰ª•ÈºìÂä±ÊâÄÈÄâÁ§∫‰æãÁöÑÂ§öÊ†∑ÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ‰ªé‰ø°ÊÅØËÆ∫ÁöÑËßíÂ∫¶ÂØπ‰∏ä‰∏ãÊñáÂ≠¶‰π†‰∏≠ÁöÑÁ§∫‰æãÈÄâÊã©ÈóÆÈ¢òËøõË°å‰∫ÜÂª∫Ê®°ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁâπÂÆö‰∫éÊü•ËØ¢ÁöÑ‰ºòÂåñÊ°ÜÊû∂„ÄÇ2) Âà©Áî®Ê†∏ÊäÄÂ∑ßÂú®È´òÁª¥Á©∫Èó¥‰∏≠ËøõË°åÊìç‰ΩúÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÁª¥Â∫¶ÁÅæÈöæÈóÆÈ¢ò„ÄÇ3) ÂºïÂÖ•‰∫ÜÂü∫‰∫éÊúÄ‰ºòËÆæËÆ°ÁöÑÊ≠£ÂàôÂåñÂô®ÔºåÊúâÊïàÊèêÂçá‰∫ÜÊâÄÈÄâÁ§∫‰æãÁöÑÂ§öÊ†∑ÊÄßÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Á∫øÊÄßÊ®°ÂûãËøë‰ººLLMÁöÑË°å‰∏∫ÔºåÁÆÄÂåñ‰∫Ü‰ºòÂåñÈóÆÈ¢ò„ÄÇ2) Êé®ÂØºÂá∫ÁöÑÊõø‰ª£ÁõÆÊ†áÂáΩÊï∞ÔºåËØ•ÂáΩÊï∞Ëøë‰ººÊ¨°Ê®°ÔºåÂÖÅËÆ∏‰ΩøÁî®È´òÊïàÁöÑË¥™Â©™ÁÆóÊ≥ïËøõË°åÊ±ÇËß£ÔºåÂπ∂Êèê‰æõËøë‰ºº‰øùËØÅ„ÄÇ3) Ê†∏ÂáΩÊï∞ÁöÑÈÄâÊã©ÔºåÂÖÅËÆ∏Âú®È´òÁª¥ÁâπÂæÅÁ©∫Èó¥‰∏≠ËøõË°åÊìç‰ΩúÔºåËÄåÊó†ÈúÄÊòæÂºèËÆ°ÁÆóÁâπÂæÅÊò†Â∞Ñ„ÄÇ4) Ê≠£ÂàôÂåñÂô®ÁöÑËÆæËÆ°ÔºåÈºìÂä±ÈÄâÊã©ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÂíåÂ§öÊ†∑ÊÄßÁöÑÁ§∫‰æãÔºåÈÅøÂÖçÈÄâÊã©ÂÜó‰ΩôÊàñÁõ∏‰ººÁöÑÁ§∫‰æã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKITEÊñπÊ≥ïÂú®Â§ö‰∏™ÂàÜÁ±ª‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑ‰∏ä‰∏ãÊñáÂ≠¶‰π†ÊñπÊ≥ï„ÄÇÁõ∏ËæÉ‰∫éÊ†áÂáÜÁöÑÊúÄËøëÈÇªÊ£ÄÁ¥¢ÊñπÊ≥ïÔºåKITEÂú®ÂáÜÁ°ÆÁéá‰∏äÊúâÊòæËëóÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê†áÁ≠æÁ®ÄÁº∫ÁöÑÂú∫ÊôØ‰∏ã„ÄÇËøôÈ™åËØÅ‰∫ÜÁªìÊûÑÊÑüÁü•ÂíåÂ§öÊ†∑ÊÄßÁ§∫‰æãÈÄâÊã©ÂØπ‰∫é‰∏ä‰∏ãÊñáÂ≠¶‰π†ÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÂ∞èÊ†∑Êú¨Â≠¶‰π†Âú∫ÊôØÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆÊ†áÊ≥®ÊàêÊú¨È´òÊòÇÊàñÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÈ¢ÜÂüüÔºå‰æãÂ¶ÇÂåªÁñóËØäÊñ≠„ÄÅÈáëËûçÈ£éÊéß„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ≠â„ÄÇÈÄöËøáÊõ¥ÊúâÊïàÂú∞ÈÄâÊã©‰∏ä‰∏ãÊñáÁ§∫‰æãÔºåÂèØ‰ª•ÊòæËëóÊèêÂçáLLMÂú®Ëøô‰∫õÈ¢ÜÂüüÁöÑÂ∫îÁî®ÊïàÊûúÔºåÈôç‰ΩéÂØπÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑ‰æùËµñÔºåÂä†ÈÄüÊ®°ÂûãÈÉ®ÁΩ≤„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In-context learning (ICL) has emerged as a powerful paradigm for adapting large language models (LLMs) to new and data-scarce tasks using only a few carefully selected task-specific examples presented in the prompt. However, given the limited context size of LLMs, a fundamental question arises: Which examples should be selected to maximize performance on a given user query? While nearest-neighbor-based methods like KATE have been widely adopted for this purpose, they suffer from well-known drawbacks in high-dimensional embedding spaces, including poor generalization and a lack of diversity. In this work, we study this problem of example selection in ICL from a principled, information theory-driven perspective. We first model an LLM as a linear function over input embeddings and frame the example selection task as a query-specific optimization problem: selecting a subset of exemplars from a larger example bank that minimizes the prediction error on a specific query. This formulation departs from traditional generalization-focused learning theoretic approaches by targeting accurate prediction for a specific query instance. We derive a principled surrogate objective that is approximately submodular, enabling the use of a greedy algorithm with an approximation guarantee. We further enhance our method by (i) incorporating the kernel trick to operate in high-dimensional feature spaces without explicit mappings, and (ii) introducing an optimal design-based regularizer to encourage diversity in the selected examples. Empirically, we demonstrate significant improvements over standard retrieval methods across a suite of classification tasks, highlighting the benefits of structure-aware, diverse example selection for ICL in real-world, label-scarce scenarios.

