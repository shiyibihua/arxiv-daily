---
layout: default
title: Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models
---

# Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19697" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19697v1</a>
  <a href="https://arxiv.org/pdf/2506.19697.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19697v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19697v1', 'Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jungwoo Park, Taewhoo Lee, Chanwoong Yoon, Hyeon Hwang, Jaewoo Kang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/dmis-lab/Outlier-Safe-Pre-Training)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOutlier-Safeé¢„è®­ç»ƒä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é‡åŒ–ä¸­çš„å¼‚å¸¸å€¼é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é‡åŒ–` `å¼‚å¸¸å€¼å¤„ç†` `ä¼˜åŒ–ç®—æ³•` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡åŒ–æ—¶ï¼Œæç«¯æ¿€æ´»å¼‚å¸¸å€¼å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå½±å“è®¾å¤‡ç«¯çš„é«˜æ•ˆéƒ¨ç½²ã€‚
2. æœ¬æ–‡æå‡ºOutlier-Safeé¢„è®­ç»ƒï¼ˆOSPï¼‰ï¼Œé€šè¿‡ä¸»åŠ¨é˜²æ­¢å¼‚å¸¸å€¼å½¢æˆï¼Œç»“åˆMuonä¼˜åŒ–å™¨ã€å•å°ºåº¦RMSNormå’Œå¯å­¦ä¹ çš„åµŒå…¥æŠ•å½±ã€‚
3. åœ¨1ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè®­ç»ƒçš„OSPæ¨¡å‹ï¼Œåœ¨4ä½é‡åŒ–ä¸‹å¹³å‡å¾—åˆ†ä¸º35.7ï¼Œæ˜¾è‘—é«˜äºä¼ ç»Ÿæ¨¡å‹çš„26.5ï¼Œä¸”è®­ç»ƒå¼€é”€ä»…å¢åŠ 2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œæç«¯æ¿€æ´»å¼‚å¸¸å€¼ä¸¥é‡å½±å“é‡åŒ–æ€§èƒ½ï¼Œé˜»ç¢é«˜æ•ˆçš„è®¾å¤‡ç«¯éƒ¨ç½²ã€‚ç°æœ‰çš„é€šé“æ“ä½œå’Œè‡ªé€‚åº”æ¢¯åº¦ç¼©æ”¾è¢«è®¤ä¸ºæ˜¯å¯¼è‡´å¼‚å¸¸å€¼çš„åŸå› ï¼Œä½†å®é™…çš„ç¼“è§£æªæ–½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†Outlier-Safeé¢„è®­ç»ƒï¼ˆOSPï¼‰ï¼Œé€šè¿‡ä¸»åŠ¨é˜²æ­¢å¼‚å¸¸å€¼çš„å½¢æˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚OSPç»“åˆäº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šMuonä¼˜åŒ–å™¨ã€å•å°ºåº¦RMSNormå’Œå¯å­¦ä¹ çš„åµŒå…¥æŠ•å½±ã€‚é€šè¿‡åœ¨1ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè®­ç»ƒä¸€ä¸ª14äº¿å‚æ•°çš„æ¨¡å‹ï¼ŒéªŒè¯äº†OSPçš„æœ‰æ•ˆæ€§ã€‚è¯¥æ¨¡å‹åœ¨æ¿€è¿›çš„4ä½é‡åŒ–ä¸‹ï¼Œåœ¨10ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡å¾—åˆ†ä¸º35.7ï¼Œç›¸è¾ƒäºAdamè®­ç»ƒæ¨¡å‹çš„26.5ï¼Œä¸”ä»…å¢åŠ äº†2%çš„è®­ç»ƒå¼€é”€ã€‚OSPæ¨¡å‹çš„è¶…é¢å³°åº¦æ¥è¿‘é›¶ï¼Œæ˜¾è‘—æ”¹å˜äº†LLMçš„é‡åŒ–è¡Œä¸ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡åŒ–è¿‡ç¨‹ä¸­å› æç«¯æ¿€æ´»å¼‚å¸¸å€¼å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–åæœŸç¼“è§£æªæ–½ï¼Œéš¾ä»¥æœ‰æ•ˆåº”å¯¹å¼‚å¸¸å€¼çš„å½¢æˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºOutlier-Safeé¢„è®­ç»ƒï¼ˆOSPï¼‰ï¼Œé€šè¿‡ä¸»åŠ¨é˜²æ­¢å¼‚å¸¸å€¼çš„ç”Ÿæˆï¼Œæå‡é‡åŒ–æ€§èƒ½ã€‚OSPçš„è®¾è®¡ç†å¿µæ˜¯é€šè¿‡ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…å¼‚å¸¸å€¼çš„äº§ç”Ÿï¼Œè€Œä¸æ˜¯äº‹åå¤„ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOSPåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šMuonä¼˜åŒ–å™¨ã€å•å°ºåº¦RMSNormå’Œå¯å­¦ä¹ çš„åµŒå…¥æŠ•å½±ã€‚Muonä¼˜åŒ–å™¨æ¶ˆé™¤äº†ç‰¹æƒåŸºï¼Œä¿æŒè®­ç»ƒæ•ˆç‡ï¼›å•å°ºåº¦RMSNormé˜²æ­¢é€šé“çº§åˆ«çš„æ”¾å¤§ï¼›å¯å­¦ä¹ çš„åµŒå…¥æŠ•å½±é‡æ–°åˆ†é…æ¥è‡ªåµŒå…¥çŸ©é˜µçš„æ¿€æ´»å¹…åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šOSPçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶ä¸»åŠ¨é˜²æ­¢å¼‚å¸¸å€¼çš„ç­–ç•¥ï¼Œè€Œéä¾èµ–åæœŸä¿®æ­£ã€‚è¿™ä¸€æ–¹æ³•ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†LLMçš„è®­ç»ƒç­–ç•¥ï¼Œè¡¨æ˜å¼‚å¸¸å€¼å¹¶éLLMå›ºæœ‰ç‰¹æ€§ï¼Œè€Œæ˜¯è®­ç»ƒç­–ç•¥çš„ç»“æœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨OSPä¸­ï¼ŒMuonä¼˜åŒ–å™¨çš„è®¾è®¡é¿å…äº†ç‰¹æƒåŸºçš„ä½¿ç”¨ï¼Œå•å°ºåº¦RMSNormçš„å‚æ•°è®¾ç½®ç¡®ä¿äº†é€šé“æ¿€æ´»çš„ç¨³å®šæ€§ï¼Œè€Œå¯å­¦ä¹ çš„åµŒå…¥æŠ•å½±åˆ™é€šè¿‡åŠ¨æ€è°ƒæ•´æ¿€æ´»å¹…åº¦æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOSPæ¨¡å‹åœ¨10ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡å¾—åˆ†ä¸º35.7ï¼Œç›¸è¾ƒäºä½¿ç”¨Adamä¼˜åŒ–å™¨è®­ç»ƒçš„æ¨¡å‹å¾—åˆ†26.5ï¼Œæå‡å¹…åº¦è¾¾34.5%ã€‚æ­¤å¤–ï¼ŒOSPæ¨¡å‹çš„è¶…é¢å³°åº¦æ¥è¿‘é›¶ï¼Œæ˜¾è‘—æ”¹å–„äº†é‡åŒ–è¡Œä¸ºï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹å’Œå…¶ä»–éœ€è¦é«˜æ•ˆéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹çš„åœºæ™¯ã€‚é€šè¿‡å‡å°‘é‡åŒ–è¿‡ç¨‹ä¸­çš„å¼‚å¸¸å€¼ï¼ŒOSPä¸ºåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œå¤§å‹æ¨¡å‹æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.

