---
layout: default
title: Scaling Speculative Decoding with Lookahead Reasoning
---

# Scaling Speculative Decoding with Lookahead Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19830" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19830v1</a>
  <a href="https://arxiv.org/pdf/2506.19830.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19830v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19830v1', 'Scaling Speculative Decoding with Lookahead Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yichao Fu, Rui Ge, Zelei Shao, Zhijie Deng, Hao Zhang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hao-ai-lab/LookaheadReasoning)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‰ç»æ¨ç†ä»¥æå‡æ¨æµ‹è§£ç é€Ÿåº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨æµ‹è§£ç ` `å‰ç»æ¨ç†` `é•¿é“¾æ€ç»´` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨æµ‹è§£ç æ–¹æ³•åœ¨å¤„ç†é•¿é“¾æ€ç»´æ—¶é€Ÿåº¦è¾ƒæ…¢ï¼Œä¸”éšç€æ ‡è®°æ•°é‡çš„å¢åŠ ï¼Œæ­£ç¡®æ€§æ¦‚ç‡æ˜¾è‘—ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºçš„å‰ç»æ¨ç†æ–¹æ³•é€šè¿‡å¼•å…¥æ­¥éª¤çº§å¹¶è¡Œæ€§ï¼Œå…è®¸æ¨ç†æ¨¡å‹åœ¨è¯­ä¹‰ä¸Šæ­£ç¡®è€Œéé€æ ‡è®°åŒ¹é…ï¼Œä»è€Œæå‡è§£ç æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå‰ç»æ¨ç†åœ¨GSM8Kã€AIMEç­‰åŸºå‡†ä¸Šå°†æ¨æµ‹è§£ç çš„é€Ÿåº¦æå‡ä»1.4å€æé«˜åˆ°2.1å€ï¼Œå¹¶ä¸”åœ¨GPUååé‡å¢åŠ æ—¶è¡¨ç°å‡ºæ›´å¥½çš„æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨ç†æ¨¡å‹é€šè¿‡ç”Ÿæˆé•¿é“¾æ€ç»´è¡¨ç°å‡ºè‰²ï¼Œä½†è§£ç æ•°åƒä¸ªæ ‡è®°çš„é€Ÿåº¦è¾ƒæ…¢ã€‚è™½ç„¶æ ‡è®°çº§æ¨æµ‹è§£ç ï¼ˆSDï¼‰æœ‰æ‰€å¸®åŠ©ï¼Œä½†å…¶æ•ˆç›Šå—åˆ°é™åˆ¶ï¼Œå› ä¸ºæ•´ä¸ª$Î³$æ ‡è®°çŒœæµ‹æ­£ç¡®çš„æ¦‚ç‡éšç€$Î³$çš„å¢åŠ è€ŒæŒ‡æ•°ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºå‰ç»æ¨ç†ï¼Œé€šè¿‡åˆ©ç”¨ç¬¬äºŒå±‚æ­¥éª¤çº§å¹¶è¡Œæ€§æ¥æé«˜é€Ÿåº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§è‰ç¨¿æ¨¡å‹æå‡ºå¤šä¸ªæœªæ¥æ­¥éª¤ï¼Œç›®æ ‡æ¨¡å‹åœ¨ä¸€æ¬¡æ‰¹å¤„ç†è¿‡ç¨‹ä¸­æ‰©å±•æ¯ä¸ªæè®®ï¼ŒéªŒè¯å™¨ä¿æŒè¯­ä¹‰æ­£ç¡®çš„æ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼Œå‰ç»æ¨ç†åœ¨å¤šä¸ªåŸºå‡†ä¸Šå°†SDçš„é€Ÿåº¦æå‡ä»1.4å€æé«˜åˆ°2.1å€ï¼ŒåŒæ—¶ä¿æŒç­”æ¡ˆè´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨ç†æ¨¡å‹åœ¨ç”Ÿæˆé•¿é“¾æ€ç»´æ—¶ï¼Œè§£ç é€Ÿåº¦æ…¢å’Œæ­£ç¡®æ€§æ¦‚ç‡ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰çš„æ ‡è®°çº§æ¨æµ‹è§£ç æ–¹æ³•åœ¨å¤„ç†é•¿æ ‡è®°åºåˆ—æ—¶é¢ä¸´ç®—æ³•ç“¶é¢ˆï¼Œå¯¼è‡´é€Ÿåº¦æå‡æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šå‰ç»æ¨ç†çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ­¥éª¤çº§çš„å¹¶è¡Œæ€§ï¼Œå…è®¸æ¨ç†æ¨¡å‹åœ¨æ¯ä¸€æ­¥åªéœ€ä¿æŒè¯­ä¹‰æ­£ç¡®ï¼Œè€Œä¸å¿…é€ä¸ªæ ‡è®°å®Œå…¨åŒ¹é…ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ç”Ÿæˆæ¨ç†æ­¥éª¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè½»é‡çº§è‰ç¨¿æ¨¡å‹ã€ç›®æ ‡æ¨¡å‹å’ŒéªŒè¯å™¨ã€‚è‰ç¨¿æ¨¡å‹æå‡ºå¤šä¸ªæœªæ¥æ­¥éª¤ï¼Œç›®æ ‡æ¨¡å‹åœ¨ä¸€æ¬¡æ‰¹å¤„ç†è¿‡ç¨‹ä¸­æ‰©å±•è¿™äº›æ­¥éª¤ï¼ŒéªŒè¯å™¨ç¡®ä¿æ¯ä¸ªæ­¥éª¤çš„è¯­ä¹‰æ­£ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†æ­¥éª¤çº§å¹¶è¡Œæ€§ï¼Œä½¿å¾—æ¨æµ‹è§£ç çš„é€Ÿåº¦æå‡å¾—ä»¥çªç ´ç°æœ‰æ–¹æ³•çš„ç®—æ³•ç“¶é¢ˆã€‚è¿™ä¸€åˆ›æ–°ä½¿å¾—æ¨ç†æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ›´åŠ é«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œè‰ç¨¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„å‚æ•°è®¾ç½®ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿åœ¨ä¿æŒè¯­ä¹‰æ­£ç¡®æ€§çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–è§£ç é€Ÿåº¦ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†è¯­ä¹‰ä¸€è‡´æ€§ï¼Œä»¥æé«˜æœ€ç»ˆç”Ÿæˆç»“æœçš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå‰ç»æ¨ç†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨æµ‹è§£ç çš„é€Ÿåº¦ï¼Œä»1.4å€æé«˜åˆ°2.1å€ï¼ŒåŒæ—¶ä¿æŒäº†ç­”æ¡ˆçš„è´¨é‡ã€‚è¿™ä¸€æå‡åœ¨å¢åŠ GPUååé‡æ—¶è¡¨ç°å‡ºæ›´å¥½çš„æ‰©å±•æ€§ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–æ¨ç†ç­‰ã€‚é€šè¿‡æå‡æ¨æµ‹è§£ç çš„é€Ÿåº¦ï¼Œå‰ç»æ¨ç†å¯ä»¥åœ¨å®æ—¶åº”ç”¨ä¸­æä¾›æ›´é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning models excel by generating long chain-of-thoughts, but decoding the resulting thousands of tokens is slow. Token-level speculative decoding (SD) helps, but its benefit is capped, because the chance that an entire $Î³$-token guess is correct falls exponentially as $Î³$ grows. This means allocating more compute for longer token drafts faces an algorithmic ceiling -- making the speedup modest and hardware-agnostic. We raise this ceiling with Lookahead Reasoning, which exploits a second, step-level layer of parallelism. Our key insight is that reasoning models generate step-by-step, and each step needs only to be semantically correct, not exact token matching. In Lookahead Reasoning, a lightweight draft model proposes several future steps; the target model expands each proposal in one batched pass, and a verifier keeps semantically correct steps while letting the target regenerate any that fail. Token-level SD still operates within each reasoning step, so the two layers of parallelism multiply. We show Lookahead Reasoning lifts the peak speedup of SD both theoretically and empirically. Across GSM8K, AIME, and other benchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x while preserving answer quality, and its speedup scales better with additional GPU throughput. Our code is available at https://github.com/hao-ai-lab/LookaheadReasoning

