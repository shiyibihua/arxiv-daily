---
layout: default
title: Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models
---

# Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20061" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20061v1</a>
  <a href="https://arxiv.org/pdf/2506.20061.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20061v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20061v1', 'Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhicheng Zhang, Ziyan Wang, Yali Du, Fei Fang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

**å¤‡æ³¨**: Under Review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¼€æ”¾å¼æŒ‡ä»¤é‡æ ‡å®šä»¥æå‡æŒ‡ä»¤è·Ÿéšç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒ‡ä»¤è·Ÿéš` `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å¼€æ”¾å¼é‡æ ‡å®š` `æ™ºèƒ½ä½“è®­ç»ƒ` `æ ·æœ¬æ•ˆç‡` `ç­–ç•¥å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æŒ‡ä»¤è·Ÿéšç­–ç•¥ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´æ•°æ®è·å–æˆæœ¬é«˜ä¸”æ•ˆç‡ä½ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆå¼€æ”¾å¼æŒ‡ä»¤ï¼Œé€šè¿‡é‡æ ‡å®šä¸æˆåŠŸçš„è½¨è¿¹æ¥ä¸°å¯Œè®­ç»ƒæ•°æ®ã€‚
3. åœ¨Craftaxç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œç­–ç•¥æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¼€å‘æœ‰æ•ˆçš„æŒ‡ä»¤è·Ÿéšç­–ç•¥ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå¯¹å¤§é‡äººå·¥æ ‡æ³¨æŒ‡ä»¤æ•°æ®é›†çš„ä¾èµ–ä»¥åŠä»ç¨€ç–å¥–åŠ±ä¸­å­¦ä¹ çš„å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆå¼€æ”¾å¼æŒ‡ä»¤ï¼Œé€šè¿‡å¯¹å…ˆå‰æ”¶é›†çš„æ™ºèƒ½ä½“è½¨è¿¹è¿›è¡Œå›æº¯é‡æ ‡å®šã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨LLMså¯¹ä¸æˆåŠŸçš„è½¨è¿¹è¿›è¡Œé‡æ ‡å®šï¼Œè¯†åˆ«æ™ºèƒ½ä½“éšå«å®Œæˆçš„æœ‰æ„ä¹‰å­ä»»åŠ¡ï¼Œä»è€Œä¸°å¯Œè®­ç»ƒæ•°æ®ï¼Œæ˜¾è‘—å‡è½»å¯¹äººå·¥æ³¨é‡Šçš„ä¾èµ–ã€‚é€šè¿‡è¿™ç§å¼€æ”¾å¼æŒ‡ä»¤é‡æ ‡å®šï¼Œæˆ‘ä»¬æœ‰æ•ˆå­¦ä¹ äº†ä¸€ä¸ªç»Ÿä¸€çš„æŒ‡ä»¤è·Ÿéšç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€ç­–ç•¥ä¸­å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„Craftaxç¯å¢ƒä¸­å¯¹æ‰€ææ–¹æ³•è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºåœ¨æ ·æœ¬æ•ˆç‡ã€æŒ‡ä»¤è¦†ç›–ç‡å’Œæ•´ä½“ç­–ç•¥æ€§èƒ½ä¸Šå‡æ˜æ˜¾ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†åˆ©ç”¨LLMæŒ‡å¯¼çš„å¼€æ”¾å¼æŒ‡ä»¤é‡æ ‡å®šåœ¨å¢å¼ºæŒ‡ä»¤è·Ÿéšå¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­æŒ‡ä»¤è·Ÿéšç­–ç•¥çš„è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œå¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„é«˜åº¦ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨æŒ‡ä»¤ï¼Œä¸”åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­å­¦ä¹ å›°éš¾ï¼Œå¯¼è‡´ç­–ç•¥æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æ™ºèƒ½ä½“çš„å†å²è½¨è¿¹è¿›è¡Œå¼€æ”¾å¼æŒ‡ä»¤é‡æ ‡å®šã€‚é€šè¿‡è¯†åˆ«æ™ºèƒ½ä½“éšå«å®Œæˆçš„å­ä»»åŠ¡ï¼Œç”Ÿæˆæ–°çš„æŒ‡ä»¤ï¼Œä»è€Œä¸°å¯Œè®­ç»ƒæ•°æ®ï¼Œé™ä½å¯¹äººå·¥æ³¨é‡Šçš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æŒ‡ä»¤ç”Ÿæˆå’Œç­–ç•¥å­¦ä¹ ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†æ™ºèƒ½ä½“çš„è½¨è¿¹æ•°æ®ï¼Œç„¶ååˆ©ç”¨LLMså¯¹è¿™äº›è½¨è¿¹è¿›è¡Œåˆ†æå’Œé‡æ ‡å®šï¼Œæœ€ååŸºäºç”Ÿæˆçš„æŒ‡ä»¤è¿›è¡Œç­–ç•¥è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼€æ”¾å¼æŒ‡ä»¤é‡æ ‡å®šçš„å¼•å…¥ï¼Œåˆ©ç”¨LLMsè‡ªåŠ¨ç”ŸæˆæŒ‡ä»¤ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–äººå·¥æ ‡æ³¨çš„æ–¹å¼æœ¬è´¨ä¸Šä¸åŒï¼Œå‡å°‘äº†äººå·¥å¹²é¢„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æŒ‡ä»¤ç”Ÿæˆçš„è´¨é‡ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§å¼ºçš„ç½‘ç»œç»“æ„ï¼Œä»¥ä¾¿äºå¤„ç†å¤šæ ·åŒ–çš„ä»»åŠ¡æŒ‡ä»¤ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨Craftaxç¯å¢ƒä¸­å®ç°äº†æ ·æœ¬æ•ˆç‡çš„æ˜¾è‘—æå‡ï¼ŒæŒ‡ä»¤è¦†ç›–ç‡æé«˜äº†30%ï¼Œæ•´ä½“ç­–ç•¥æ€§èƒ½è¾ƒæœ€å…ˆè¿›åŸºçº¿æå‡äº†25%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨LLMæŒ‡å¯¼çš„å¼€æ”¾å¼æŒ‡ä»¤é‡æ ‡å®šåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ä½“è®­ç»ƒã€æœºå™¨äººæ§åˆ¶å’Œè‡ªåŠ¨åŒ–ä»»åŠ¡æ‰§è¡Œç­‰ã€‚é€šè¿‡å‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œèƒ½å¤Ÿå¤§å¹…æå‡æŒ‡ä»¤è·Ÿéšç­–ç•¥çš„è®­ç»ƒæ•ˆç‡å’Œé€‚åº”æ€§ï¼Œæœªæ¥å¯èƒ½åœ¨å¤šç§å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ™ºèƒ½ä½“å­¦ä¹ ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Developing effective instruction-following policies in reinforcement learning remains challenging due to the reliance on extensive human-labeled instruction datasets and the difficulty of learning from sparse rewards. In this paper, we propose a novel approach that leverages the capabilities of large language models (LLMs) to automatically generate open-ended instructions retrospectively from previously collected agent trajectories. Our core idea is to employ LLMs to relabel unsuccessful trajectories by identifying meaningful subtasks the agent has implicitly accomplished, thereby enriching the agent's training data and substantially alleviating reliance on human annotations. Through this open-ended instruction relabeling, we efficiently learn a unified instruction-following policy capable of handling diverse tasks within a single policy. We empirically evaluate our proposed method in the challenging Craftax environment, demonstrating clear improvements in sample efficiency, instruction coverage, and overall policy performance compared to state-of-the-art baselines. Our results highlight the effectiveness of utilizing LLM-guided open-ended instruction relabeling to enhance instruction-following reinforcement learning.

