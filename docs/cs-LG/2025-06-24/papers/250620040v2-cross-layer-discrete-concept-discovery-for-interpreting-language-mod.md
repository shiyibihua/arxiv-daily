---
layout: default
title: Cross-Layer Discrete Concept Discovery for Interpreting Language Models
---

# Cross-Layer Discrete Concept Discovery for Interpreting Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20040" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20040v2</a>
  <a href="https://arxiv.org/pdf/2506.20040.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20040v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20040v2', 'Cross-Layer Discrete Concept Discovery for Interpreting Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ankur Garg, Xuemin Yu, Hassan Sajjad, Samira Ebrahimi Kahou

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24 (æ›´æ–°: 2025-07-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨å±‚ç¦»æ•£æ¦‚å¿µå‘ç°æ–¹æ³•ä»¥è§£æè¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `å˜æ¢å™¨` `å‘é‡é‡åŒ–` `å¯è§£é‡Šæ€§` `æ·±åº¦å­¦ä¹ ` `æ¦‚å¿µå‘ç°` `èšç±»ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•å±‚ç¥ç»è¡¨ç¤ºçš„åˆ†æï¼Œå¿½ç•¥äº†è·¨å±‚ç‰¹å¾çš„å åŠ å’Œå†—ä½™ï¼Œå¯¼è‡´å¯¹è¯­è¨€æ¨¡å‹çš„ç†è§£ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„CLVQ-VAEæ¡†æ¶é€šè¿‡å‘é‡é‡åŒ–æ˜ å°„è·¨å±‚è¡¨ç¤ºï¼Œå‹ç¼©å†—ä½™ç‰¹å¾ä¸ºå¯è§£é‡Šçš„æ¦‚å¿µå‘é‡ï¼Œæå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCLVQ-VAEåœ¨æ¦‚å¿µå‘ç°å’Œè¡¨ç¤ºå‹ç¼©æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½å’Œç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å˜æ¢å™¨å±‚ä¸­æ­ç¤ºæ–°å…´æ¦‚å¿µä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºæ®‹å·®æµçº¿æ€§æ··åˆå’Œé‡å¤ä¿¡æ¯ï¼Œæ¨¡ç³Šäº†ç‰¹å¾åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¼”å˜ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦æ£€æŸ¥å•å±‚çš„ç¥ç»è¡¨ç¤ºï¼Œå¿½è§†äº†è·¨å±‚å åŠ åŠå…¶å¼•å…¥çš„å†—ä½™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è·¨å±‚VQ-VAEï¼ˆCLVQ-VAEï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨å‘é‡é‡åŒ–æ˜ å°„å±‚é—´è¡¨ç¤ºï¼Œå¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­å°†é‡å¤çš„æ®‹å·®æµç‰¹å¾å‹ç¼©ä¸ºç´§å‡‘ä¸”å¯è§£é‡Šçš„æ¦‚å¿µå‘é‡ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆç‹¬ç‰¹åœ°ç»“åˆäº†åŸºäºæ¸©åº¦çš„top-ké‡‡æ ·ä¸EMAä»£ç æœ¬æ›´æ–°ï¼Œæä¾›äº†å¯¹ç¦»æ•£æ½œåœ¨ç©ºé—´çš„å—æ§æ¢ç´¢ï¼ŒåŒæ—¶ä¿æŒä»£ç æœ¬çš„å¤šæ ·æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡scaled-spherical k-means++å¢å¼ºæ¡†æ¶ï¼Œä»¥æ–¹å‘ç›¸ä¼¼æ€§è€Œéå¹…åº¦è¿›è¡Œèšç±»ï¼Œæ›´å¥½åœ°ä¸è¯åµŒå…¥ç©ºé—´ä¸­çš„è¯­ä¹‰ç»“æ„å¯¹é½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å˜æ¢å™¨å±‚ä¸­æ­ç¤ºæ–°å…´æ¦‚å¿µçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åˆ†æå•å±‚ç¥ç»è¡¨ç¤ºæ—¶å¿½è§†äº†è·¨å±‚ç‰¹å¾çš„å åŠ å’Œå†—ä½™ï¼Œå¯¼è‡´å¯¹æ¨¡å‹ç‰¹å¾æ¼”å˜çš„ç†è§£ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„CLVQ-VAEæ¡†æ¶é€šè¿‡å‘é‡é‡åŒ–æŠ€æœ¯æ˜ å°„è·¨å±‚è¡¨ç¤ºï¼Œå‹ç¼©é‡å¤çš„æ®‹å·®æµç‰¹å¾ä¸ºç´§å‡‘çš„æ¦‚å¿µå‘é‡ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œç‰¹å¾è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡å‘é‡é‡åŒ–å°†å±‚é—´è¡¨ç¤ºæ˜ å°„åˆ°ç¦»æ•£ç©ºé—´ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨åŸºäºæ¸©åº¦çš„top-ké‡‡æ ·è¿›è¡Œé‡åŒ–ï¼›æœ€åï¼Œä½¿ç”¨EMAæ›´æ–°ä»£ç æœ¬ä»¥ä¿æŒå¤šæ ·æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†top-kæ¸©åº¦é‡‡æ ·ä¸EMAä»£ç æœ¬æ›´æ–°ç›¸ç»“åˆï¼Œæä¾›äº†å¯¹ç¦»æ•£æ½œåœ¨ç©ºé—´çš„å—æ§æ¢ç´¢ï¼ŒåŒæ—¶é€šè¿‡scaled-spherical k-means++è¿›è¡Œä»£ç æœ¬åˆå§‹åŒ–ï¼Œå¢å¼ºäº†èšç±»æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†æ¸©åº¦è°ƒèŠ‚æœºåˆ¶æ¥æ§åˆ¶é‡‡æ ·çš„å¤šæ ·æ€§ï¼›æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†é‡åŒ–è¯¯å·®å’Œé‡æ„è¯¯å·®ï¼›ç½‘ç»œç»“æ„ä¸Šï¼Œå¼ºè°ƒäº†æ–¹å‘ç›¸ä¼¼æ€§èšç±»çš„ä½¿ç”¨ï¼Œä»¥æ›´å¥½åœ°å¯¹é½è¯­ä¹‰ç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLVQ-VAEåœ¨æ¦‚å¿µå‘ç°ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†çº¦20%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨ç‰¹å¾å‹ç¼©æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡ï¼ŒéªŒè¯äº†å…¶åœ¨å¤„ç†å¤æ‚è¯­è¨€æ¨¡å‹æ—¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ¨¡å‹è§£é‡Šã€è¯­ä¹‰åˆ†æå’ŒçŸ¥è¯†å›¾è°±æ„å»ºç­‰ã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œç ”ç©¶æˆæœå¯ä»¥å¸®åŠ©å¼€å‘æ›´é€æ˜çš„AIç³»ç»Ÿï¼Œå¢å¼ºç”¨æˆ·å¯¹æ¨¡å‹å†³ç­–çš„ä¿¡ä»»ï¼Œå¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Uncovering emergent concepts across transformer layers remains a significant challenge because the residual stream linearly mixes and duplicates information, obscuring how features evolve within large language models. Current research efforts primarily inspect neural representations at single layers, thereby overlooking this cross-layer superposition and the redundancy it introduces. These representations are typically either analyzed directly for activation patterns or passed to probing classifiers that map them to a limited set of predefined concepts. To address these limitations, we propose cross-layer VQ-VAE (CLVQ-VAE), a framework that uses vector quantization to map representations across layers and in the process collapse duplicated residual-stream features into compact, interpretable concept vectors. Our approach uniquely combines top-k temperature-based sampling during quantization with EMA codebook updates, providing controlled exploration of the discrete latent space while maintaining code-book diversity. We further enhance the framework with scaled-spherical k-means++ for codebook initialization, which clusters by directional similarity rather than magnitude, better aligning with semantic structure in word embedding space.

