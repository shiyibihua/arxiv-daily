---
layout: default
title: Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2
---

# Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23367" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.23367v1</a>
  <a href="https://arxiv.org/pdf/2512.23367.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23367v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23367v1', 'Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é’ˆå¯¹æ˜‡è…¾A2ï¼Œæå‡ºä½æ¯”ç‰¹é‡åŒ–æ–¹æ¡ˆï¼ŒåŠ é€Ÿç›˜å¤æ¨¡å‹æ¨ç†å¹¶é™ä½å†…å­˜å ç”¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½æ¯”ç‰¹é‡åŒ–` `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹éƒ¨ç½²` `æ˜‡è…¾NPU` `Atlas A2` `æ€ç»´é“¾æ¨ç†` `INT8é‡åŒ–` `W4A8é‡åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§è¯­è¨€æ¨¡å‹æ¨ç†è®¡ç®—é‡å¤§ï¼Œå†…å­˜å ç”¨é«˜ï¼Œç»™éƒ¨ç½²å¸¦æ¥æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„Ascend NPUä¸Šã€‚
2. é‡‡ç”¨ä½æ¯”ç‰¹é‡åŒ–ï¼Œå°†FP16è®¡ç®—è½¬æ¢ä¸ºæ›´é«˜æ•ˆçš„æ•´æ•°è¿ç®—ï¼Œé™ä½å†…å­˜å ç”¨å’Œè®¡ç®—å¤æ‚åº¦ã€‚
3. åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒINT8é‡åŒ–ä¿æŒäº†90%ä»¥ä¸Šçš„ç²¾åº¦ï¼Œå¹¶å®ç°äº†1.5å€çš„åŠ é€Ÿï¼›W4A8é‡åŒ–æ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åä¸ºçš„openPangu-Embedded-1Bå’ŒopenPangu-Embedded-7Bæ˜¯å¤§è¯­è¨€æ¨¡å‹openPanguçš„å˜ä½“ï¼Œé›†æˆäº†ä¸‰ç§ä¸åŒçš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†èŒƒå¼ï¼Œå³slow_thinkã€auto_thinkå’Œno_thinkã€‚è™½ç„¶è¿™äº›CoTæ¨¡å¼å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬ç”Ÿæˆçš„æ‰©å±•æ¨ç†è½¨è¿¹å¸¦æ¥äº†å¤§é‡çš„å†…å­˜å’Œå»¶è¿Ÿå¼€é”€ï¼Œç»™åœ¨æ˜‡è…¾NPUä¸Šçš„å®é™…éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡åˆ©ç”¨ä½æ¯”ç‰¹é‡åŒ–æ¥è§£å†³è¿™äº›è®¡ç®—çº¦æŸï¼Œå°†FP16è®¡ç®—è½¬æ¢ä¸ºæ›´é«˜æ•ˆçš„æ•´æ•°è¿ç®—ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„ä½æ¯”ç‰¹æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒINT8 (W8A8) å’Œ W4A8 é‡åŒ–ï¼Œä¸“é—¨ä¸ºAtlas A2ä¸Šçš„openPangu-Embeddedæ¨¡å‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚æˆ‘ä»¬å¯¹æ‰€æœ‰ä¸‰ç§CoTæ¨¡å¼åœ¨ä»£ç ç”ŸæˆåŸºå‡†ï¼ˆHumanEvalå’ŒMBPPï¼‰ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚INT8é‡åŒ–å§‹ç»ˆä¿æŒè¶…è¿‡90%çš„FP16åŸºçº¿ç²¾åº¦ï¼Œå¹¶åœ¨Atlas A2ä¸Šå®ç°äº†1.5å€çš„é¢„å¡«å……åŠ é€Ÿã€‚æ­¤å¤–ï¼ŒW4A8é‡åŒ–æ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ï¼Œå°½ç®¡åœ¨ç²¾åº¦ä¸Šæœ‰æ‰€æŠ˜è¡·ã€‚è¿™äº›å‘ç°å…±åŒè¡¨æ˜ï¼Œä½æ¯”ç‰¹é‡åŒ–æœ‰æ•ˆåœ°ä¿ƒè¿›äº†æ˜‡è…¾NPUä¸Šé«˜æ•ˆçš„CoTæ¨ç†ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ¨¡å‹ä¿çœŸåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³openPangu-Embeddedç³»åˆ—æ¨¡å‹åœ¨Ascend NPUï¼ˆç‰¹åˆ«æ˜¯Atlas A2ï¼‰ä¸Šéƒ¨ç½²æ—¶ï¼Œç”±äºCoTæ¨ç†å¯¼è‡´çš„å†…å­˜å’Œå»¶è¿Ÿå¼€é”€è¿‡å¤§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå³FP16ç²¾åº¦æ¨ç†ï¼Œæ— æ³•æ»¡è¶³èµ„æºå—é™åœºæ™¯ä¸‹çš„éƒ¨ç½²éœ€æ±‚ï¼Œéœ€è¦å¯»æ‰¾æ›´é«˜æ•ˆçš„æ¨ç†æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä½æ¯”ç‰¹é‡åŒ–æŠ€æœ¯ï¼Œå°†FP16ç²¾åº¦çš„æ¨¡å‹å‚æ•°å’Œæ¿€æ´»å€¼è½¬æ¢ä¸ºä½æ¯”ç‰¹ï¼ˆINT8æˆ–W4A8ï¼‰è¡¨ç¤ºï¼Œä»è€Œé™ä½å†…å­˜å ç”¨å’Œè®¡ç®—å¤æ‚åº¦ã€‚é€šè¿‡é‡åŒ–ï¼Œå¯ä»¥å°†æµ®ç‚¹è¿ç®—è½¬åŒ–ä¸ºæ›´å¿«çš„æ•´æ•°è¿ç®—ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ä½æ¯”ç‰¹æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ”¯æŒINT8 (W8A8) å’Œ W4A8 ä¸¤ç§é‡åŒ–æ–¹å¼ã€‚è¯¥æ¡†æ¶é’ˆå¯¹openPangu-Embeddedæ¨¡å‹åœ¨Atlas A2ä¸Šçš„éƒ¨ç½²è¿›è¡Œäº†ä¼˜åŒ–ã€‚å…·ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œå¯¹FP16æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œå¾—åˆ°ä½æ¯”ç‰¹æ¨¡å‹ï¼›ç„¶åï¼Œä½¿ç”¨ä½æ¯”ç‰¹æ¨¡å‹åœ¨Atlas A2ä¸Šè¿›è¡Œæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºé’ˆå¯¹openPangu-Embeddedæ¨¡å‹å’ŒAtlas A2ç¡¬ä»¶å¹³å°ï¼Œè®¾è®¡å¹¶å®ç°äº†é«˜æ•ˆçš„ä½æ¯”ç‰¹é‡åŒ–æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡æ¨¡å‹ç²¾åº¦ã€æ¨ç†é€Ÿåº¦å’Œå†…å­˜å ç”¨ï¼Œä»è€Œå®ç°æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆéƒ¨ç½²ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æœªæ˜ç¡®è¯´æ˜é‡åŒ–æ–¹æ¡ˆçš„å…·ä½“ç»†èŠ‚ï¼Œä¾‹å¦‚é‡åŒ–å‚æ•°çš„æ ¡å‡†æ–¹æ³•ã€é‡åŒ–è¯¯å·®çš„è¡¥å¿ç­–ç•¥ç­‰ã€‚è¿™äº›ç»†èŠ‚å¯¹äºé‡åŒ–æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†è®ºæ–‡ä¸­æœªæä¾›è¶³å¤Ÿçš„ä¿¡æ¯ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23367v1/iclr2026/smooth_hadamard_w4a8_demo.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23367v1/iclr2026/humaneval_mbpp_word_counts.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23367v1/iclr2026/CoT_demo.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒINT8é‡åŒ–åœ¨HumanEvalå’ŒMBPPä»£ç ç”ŸæˆåŸºå‡†ä¸Šä¿æŒäº†è¶…è¿‡90%çš„FP16åŸºçº¿ç²¾åº¦ï¼Œå¹¶åœ¨Atlas A2ä¸Šå®ç°äº†1.5å€çš„é¢„å¡«å……åŠ é€Ÿã€‚W4A8é‡åŒ–è™½ç„¶åœ¨ç²¾åº¦ä¸Šæœ‰æ‰€ä¸‹é™ï¼Œä½†æ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ã€‚è¿™äº›ç»“æœéªŒè¯äº†ä½æ¯”ç‰¹é‡åŒ–åœ¨åŠ é€Ÿæ¨ç†å’Œé™ä½å†…å­˜å ç”¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚è¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€ç§»åŠ¨è®¾å¤‡ç­‰ã€‚é€šè¿‡ä½æ¯”ç‰¹é‡åŒ–ï¼Œå¯ä»¥åœ¨èµ„æºå—é™çš„ç¡¬ä»¶å¹³å°ä¸Šè¿è¡Œæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œä»è€Œæå‡AIåº”ç”¨çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚è¯¥æŠ€æœ¯å¯¹äºæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.

