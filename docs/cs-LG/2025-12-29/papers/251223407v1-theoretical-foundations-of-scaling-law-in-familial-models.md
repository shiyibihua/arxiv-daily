---
layout: default
title: Theoretical Foundations of Scaling Law in Familial Models
---

# Theoretical Foundations of Scaling Law in Familial Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23407" class="toolbar-btn" target="_blank">üìÑ arXiv: 2512.23407v1</a>
  <a href="https://arxiv.org/pdf/2512.23407.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23407v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23407v1', 'Theoretical Foundations of Scaling Law in Familial Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Huan Song, Qingfei Zhao, Ting Long, Shuyu Tian, Hongjun An, Jiawei Shao, Chi Zhang, Xuelong Li

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-29

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÈíàÂØπFamilialÊ®°ÂûãÔºåÊèêÂá∫ÂåÖÂê´Ê®°ÂûãÁ≤íÂ∫¶ÁöÑÊñ∞ÂûãScaling LawÁêÜËÆ∫Ê°ÜÊû∂„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Scaling Law` `FamilialÊ®°Âûã` `Ê®°ÂûãÁ≤íÂ∫¶` `IsoFLOPÂÆûÈ™å` `Ê®°ÂûãÈÉ®ÁΩ≤`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâScaling Law‰∏ªË¶ÅÈíàÂØπÂçï‰∏ÄÁ®†ÂØÜÊ®°ÂûãÔºåÊó†Ê≥ïÊúâÊïàÊîØÊåÅFamilialÊ®°ÂûãËøôÁßç‚Äú‰∏ÄÊ¨°ËÆ≠ÁªÉÔºåÂ§öÊ¨°ÈÉ®ÁΩ≤‚ÄùÁöÑËåÉÂºè„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Â∞ÜÊ®°ÂûãÁ≤íÂ∫¶ÔºàGÔºâ‰Ωú‰∏∫scalingÂèòÈáèÔºåÊûÑÂª∫Áªü‰∏ÄÁöÑScaling LawÂáΩÊï∞L(N, D, G)Ôºå‰ªéËÄåÊîØÊåÅFamilialÊ®°Âûã„ÄÇ
3. ÈÄöËøáIsoFLOPÂÆûÈ™åËÆæËÆ°ÔºåÈöîÁ¶ªÊû∂ÊûÑÂΩ±Âìç‰∏éËÆ°ÁÆóËßÑÊ®°ÔºåÈ™åËØÅ‰∫ÜÈÉ®ÁΩ≤ÁÅµÊ¥ªÊÄß‰∏ç‰ºöÂΩ±ÂìçËÆ°ÁÆóÊúÄ‰ºòÊÄßÔºåÂπ∂ÂèÇÊï∞Âåñ‰∫ÜScaling Law„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á•ûÁªèScaling LawÂ∑≤Êàê‰∏∫‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËÆ≠ÁªÉÁöÑÂü∫Á°ÄÔºå‰ΩÜÂÆÉ‰ª¨ÈÄöÂ∏∏ÂÅáËÆæÂçï‰∏ÄÁöÑÁ®†ÂØÜÊ®°ÂûãËæìÂá∫„ÄÇËøôÁßçÂ±ÄÈôêÊÄßÂøΩÁï•‰∫Ü‚ÄúFamilialÊ®°Âûã‚ÄùÔºåËøôÊòØ‰∏ÄÁßçÂØπ‰∫éÂú®ÂºÇÊûÑËÆæÂ§á-ËæπÁºò-‰∫ëÂ±ÇÊ¨°ÁªìÊûÑ‰∏≠ÂÆûÁé∞ÊôÆÈÅçÊô∫ËÉΩËá≥ÂÖ≥ÈáçË¶ÅÁöÑÂèòÈù©ÊÄßËåÉ‰æã„ÄÇFamilialÊ®°ÂûãË∂ÖË∂ä‰∫ÜÈùôÊÄÅÊû∂ÊûÑÔºåÈõÜÊàê‰∫ÜÊó©ÊúüÈÄÄÂá∫‰∏é‰∏≠ÁªßÂºèÊé®ÁêÜÔºå‰ªéËÄå‰ªéÂçï‰∏™ÂÖ±‰∫´È™®Âπ≤ÁΩëÁªú‰∏≠‰∫ßÁîüG‰∏™ÂèØÈÉ®ÁΩ≤ÁöÑÂ≠êÊ®°Âûã„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÈÄöËøáÂºïÂÖ•Á≤íÂ∫¶ÔºàGÔºâ‰Ωú‰∏∫Ê®°ÂûãÂ§ßÂ∞èÔºàNÔºâÂíåËÆ≠ÁªÉtokensÔºàDÔºâ‰πãÂ§ñÁöÑÂü∫Êú¨scalingÂèòÈáèÔºå‰ªéÁêÜËÆ∫ÂíåÁªèÈ™å‰∏äÊâ©Â±ï‰∫Üscaling lawÔºå‰ª•ÊçïÊçâËøôÁßç‚Äú‰∏ÄÊ¨°ËøêË°åÔºåÂ§ö‰∏™Ê®°Âûã‚ÄùÁöÑËåÉ‰æã„ÄÇ‰∏∫‰∫Ü‰∏•Ê†ºÈáèÂåñËøôÁßçÂÖ≥Á≥ªÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂáΩÊï∞ÂΩ¢ÂºèL(N, D, G)ÔºåÂπ∂‰ΩøÁî®Â§ßËßÑÊ®°ÁöÑÁªèÈ™åËøêË°åÂØπÂÖ∂ËøõË°åÂèÇÊï∞Âåñ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÈááÁî®‰∏•Ê†ºÁöÑIsoFLOPÂÆûÈ™åËÆæËÆ°Ôºå‰ª•‰∏•Ê†ºÂú∞Â∞ÜÊû∂ÊûÑÂΩ±Âìç‰∏éËÆ°ÁÆóËßÑÊ®°ÈöîÁ¶ªÂºÄÊù•„ÄÇÂú®Âõ∫ÂÆöÁöÑÈ¢ÑÁÆó‰∏ãÔºåÊàë‰ª¨Á≥ªÁªüÂú∞Êâ´ÊèèÊ®°ÂûãÂ§ßÂ∞èÔºàNÔºâÂíåÁ≤íÂ∫¶ÔºàGÔºâÔºåÂêåÊó∂Âä®ÊÄÅË∞ÉÊï¥tokensÔºàDÔºâ„ÄÇËøôÁßçÊñπÊ≥ïÊúâÊïàÂú∞Â∞ÜÁ≤íÂ∫¶ÁöÑËæπÈôÖÊàêÊú¨‰∏éËßÑÊ®°ÁöÑ‰ºòÂäøÂàÜÁ¶ªÂºÄÊù•Ôºå‰ªéËÄåÁ°Æ‰øù‰∫ÜÊàë‰ª¨Áªü‰∏Äscaling lawÁöÑÈ´ò‰øùÁúüÂèÇÊï∞Âåñ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåÁ≤íÂ∫¶ÊÉ©ÁΩöÈÅµÂæ™‰∏Ä‰∏™ÂÖ∑ÊúâÊûÅÂ∞èÊåáÊï∞ÁöÑ‰πòÊ≥ïÂπÇÂæã„ÄÇ‰ªéÁêÜËÆ∫‰∏äËÆ≤ÔºåËøôÊ°•Êé•‰∫ÜÂõ∫ÂÆöËÆ°ÁÆóËÆ≠ÁªÉ‰∏éÂä®ÊÄÅÊû∂ÊûÑ„ÄÇÂÆûÈôÖ‰∏äÔºåÂÆÉÈ™åËØÅ‰∫Ü‚Äú‰∏ÄÊ¨°ËÆ≠ÁªÉÔºåÂ§öÊ¨°ÈÉ®ÁΩ≤‚ÄùÁöÑËåÉ‰æãÔºåË°®ÊòéÈÉ®ÁΩ≤ÁÅµÊ¥ªÊÄßÂèØ‰ª•Âú®‰∏çÂΩ±ÂìçÁ®†ÂØÜÂü∫Á∫øÁöÑËÆ°ÁÆóÊúÄ‰ºòÊÄßÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÁ•ûÁªèScaling Law‰∏ªË¶ÅÂÖ≥Ê≥®Âçï‰∏ÄÁöÑ„ÄÅÁ®†ÂØÜÁöÑÊ®°ÂûãÔºåÊó†Ê≥ïÁõ¥Êé•Â∫îÁî®‰∫éFamilialÊ®°Âûã„ÄÇFamilialÊ®°ÂûãÂÖÅËÆ∏‰ªé‰∏Ä‰∏™ÂÖ±‰∫´ÁöÑÈ™®Âπ≤ÁΩëÁªú‰∏≠Ê¥æÁîüÂá∫Â§ö‰∏™‰∏çÂêåÁ≤íÂ∫¶ÁöÑÂ≠êÊ®°ÂûãÔºå‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑÈÉ®ÁΩ≤ÁéØÂ¢ÉÂíåËÆ°ÁÆóËµÑÊ∫ê„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂª∫Á´ãÈÄÇÁî®‰∫éFamilialÊ®°ÂûãÁöÑScaling LawÔºå‰ªéËÄåÊåáÂØºÂÖ∂ËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤ÔºåÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàË°°ÈáèÊ®°ÂûãÁ≤íÂ∫¶ÂØπÊÄßËÉΩÁöÑÂΩ±ÂìçÔºå‰πüÊó†Ê≥ïÂú®Âõ∫ÂÆöËÆ°ÁÆóÈ¢ÑÁÆó‰∏ã‰ºòÂåñÊ®°ÂûãÂ§ßÂ∞èÂíåÁ≤íÂ∫¶ÁöÑÂàÜÈÖç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÊ®°ÂûãÁ≤íÂ∫¶ÔºàGÔºâÂºïÂÖ•Scaling LawÔºåÂ∞ÜÂÖ∂‰Ωú‰∏∫‰∏Ä‰∏™Áã¨Á´ãÁöÑscalingÂèòÈáèÔºå‰∏éÊ®°ÂûãÂ§ßÂ∞èÔºàNÔºâÂíåËÆ≠ÁªÉtokensÔºàDÔºâ‰∏ÄËµ∑ÔºåÂÖ±ÂêåÂÜ≥ÂÆöÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÂª∫Á´ã‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂáΩÊï∞ÂΩ¢ÂºèL(N, D, G)ÔºåÂèØ‰ª•ÈáèÂåñÊ®°ÂûãÂ§ßÂ∞è„ÄÅËÆ≠ÁªÉÊï∞ÊçÆÂíåÊ®°ÂûãÁ≤íÂ∫¶‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇËøôÁßçËÆæËÆ°ÂÖÅËÆ∏Âú®Âõ∫ÂÆöËÆ°ÁÆóÈ¢ÑÁÆó‰∏ãÔºåÊùÉË°°Ê®°ÂûãÂ§ßÂ∞èÂíåÁ≤íÂ∫¶Ôºå‰ªéËÄå‰ºòÂåñFamilialÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÈááÁî®‰∫Ü‰∏ÄÁßçÁ≥ªÁªüÁöÑÂÆûÈ™åÊñπÊ≥ïÊù•Á†îÁ©∂FamilialÊ®°ÂûãÁöÑScaling Law„ÄÇÈ¶ñÂÖàÔºåÂÆö‰πâ‰∫ÜÊ®°ÂûãÁ≤íÂ∫¶ÔºàGÔºâÁöÑÊ¶ÇÂøµÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫scalingÂèòÈáè„ÄÇÁÑ∂ÂêéÔºåËÆæËÆ°‰∫ÜIsoFLOPÂÆûÈ™åÔºåÂç≥Âú®Âõ∫ÂÆöÁöÑËÆ°ÁÆóÈ¢ÑÁÆó‰∏ãÔºåÁ≥ªÁªüÂú∞Êâ´Êèè‰∏çÂêåÁöÑÊ®°ÂûãÂ§ßÂ∞èÔºàNÔºâÂíåÁ≤íÂ∫¶ÔºàGÔºâÔºåÂπ∂Âä®ÊÄÅË∞ÉÊï¥ËÆ≠ÁªÉtokensÔºàDÔºâ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂèØ‰ª•ÊúâÊïàÂú∞Â∞ÜÊû∂ÊûÑÂΩ±Âìç‰∏éËÆ°ÁÆóËßÑÊ®°ÈöîÁ¶ªÂºÄÊù•„ÄÇÊúÄÂêéÔºå‰ΩøÁî®ÂÆûÈ™åÊï∞ÊçÆÊù•ÂèÇÊï∞ÂåñÁªü‰∏ÄÁöÑScaling LawÂáΩÊï∞L(N, D, G)Ôºå‰ªéËÄåÂª∫Á´ãÊ®°ÂûãÂ§ßÂ∞è„ÄÅËÆ≠ÁªÉÊï∞ÊçÆÂíåÊ®°ÂûãÁ≤íÂ∫¶‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÊ®°ÂûãÁ≤íÂ∫¶ÔºàGÔºâÂºïÂÖ•Scaling LawÔºåÂπ∂ÊèêÂá∫‰∫ÜÁªü‰∏ÄÁöÑÂáΩÊï∞ÂΩ¢ÂºèL(N, D, G)„ÄÇËøô‰ΩøÂæóScaling LawËÉΩÂ§üÈÄÇÁî®‰∫éFamilialÊ®°ÂûãÔºå‰ªéËÄåÊîØÊåÅ‚Äú‰∏ÄÊ¨°ËÆ≠ÁªÉÔºåÂ§öÊ¨°ÈÉ®ÁΩ≤‚ÄùÁöÑËåÉÂºè„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞È¢ÑÊµãFamilialÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂπ∂ÊåáÂØºÂÖ∂ËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤„ÄÇÊ≠§Â§ñÔºåIsoFLOPÂÆûÈ™åËÆæËÆ°‰πüÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÂàõÊñ∞ÔºåÂÆÉÂèØ‰ª•ÊúâÊïàÂú∞ÈöîÁ¶ªÊû∂ÊûÑÂΩ±Âìç‰∏éËÆ°ÁÆóËßÑÊ®°Ôºå‰ªéËÄåÊèêÈ´òÂÆûÈ™åÁªìÊûúÁöÑÂèØÈù†ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Ê®°ÂûãÁ≤íÂ∫¶ÁöÑÂÆö‰πâÔºöÊòéÁ°Æ‰∫ÜÂ¶Ç‰ΩïË°°ÈáèFamilialÊ®°Âûã‰∏≠Â≠êÊ®°ÂûãÁöÑÊï∞ÈáèÂíåÂ§çÊùÇÁ®ãÂ∫¶„ÄÇ2) IsoFLOPÂÆûÈ™åËÆæËÆ°ÔºöÁ°Æ‰øùÂú®Âõ∫ÂÆöËÆ°ÁÆóÈ¢ÑÁÆó‰∏ãËøõË°åÂÆûÈ™åÔºå‰ªéËÄåÈÅøÂÖçËÆ°ÁÆóËßÑÊ®°ÂØπÂÆûÈ™åÁªìÊûúÁöÑÂπ≤Êâ∞„ÄÇ3) Áªü‰∏ÄÁöÑScaling LawÂáΩÊï∞L(N, D, G)ÔºöÈÄâÊã©ÂêàÈÄÇÁöÑÂáΩÊï∞ÂΩ¢ÂºèÊù•ÊãüÂêàÂÆûÈ™åÊï∞ÊçÆÔºåÂπ∂Á°ÆÂÆöÂáΩÊï∞‰∏≠ÁöÑÂèÇÊï∞„ÄÇ4) Á≤íÂ∫¶ÊÉ©ÁΩöÁöÑÂª∫Ê®°ÔºöÂèëÁé∞Á≤íÂ∫¶ÊÉ©ÁΩöÈÅµÂæ™‰∏Ä‰∏™ÂÖ∑ÊúâÊûÅÂ∞èÊåáÊï∞ÁöÑ‰πòÊ≥ïÂπÇÂæãÔºåËøôË°®ÊòéÈÉ®ÁΩ≤ÁÅµÊ¥ªÊÄßÂèØ‰ª•Âú®‰∏çÂΩ±ÂìçËÆ°ÁÆóÊúÄ‰ºòÊÄßÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁ≤íÂ∫¶ÊÉ©ÁΩöÈÅµÂæ™‰∏Ä‰∏™ÂÖ∑ÊúâÊûÅÂ∞èÊåáÊï∞ÁöÑ‰πòÊ≥ïÂπÇÂæãÔºåËøôÊÑèÂë≥ÁùÄÂ¢ûÂä†Ê®°ÂûãÁ≤íÂ∫¶ÂØπÊÄßËÉΩÁöÑÂΩ±ÂìçÁõ∏ÂØπËæÉÂ∞è„ÄÇËøôÈ™åËØÅ‰∫Ü‚Äú‰∏ÄÊ¨°ËÆ≠ÁªÉÔºåÂ§öÊ¨°ÈÉ®ÁΩ≤‚ÄùÁöÑËåÉ‰æãÔºåË°®ÊòéÈÉ®ÁΩ≤ÁÅµÊ¥ªÊÄßÂèØ‰ª•Âú®‰∏çÂΩ±ÂìçÁ®†ÂØÜÂü∫Á∫øÁöÑËÆ°ÁÆóÊúÄ‰ºòÊÄßÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞„ÄÇÈÄöËøáIsoFLOPÂÆûÈ™åÔºåËÆ∫ÊñáÊàêÂäüÂú∞Â∞ÜÊû∂ÊûÑÂΩ±Âìç‰∏éËÆ°ÁÆóËßÑÊ®°ÈöîÁ¶ªÂºÄÊù•Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂÆûÈ™åÁªìÊûúÁöÑÂèØÈù†ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÁÅµÊ¥ªÈÉ®ÁΩ≤ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇËæπÁºòËÆ°ÁÆó„ÄÅÁßªÂä®ËÆæÂ§áÂíå‰∫ëËÆ°ÁÆó„ÄÇÈÄöËøáFamilialÊ®°ÂûãÂíåÊñ∞ÂûãScaling LawÔºåÂèØ‰ª•Ê†πÊçÆ‰∏çÂêåËÆæÂ§áÁöÑËÆ°ÁÆóËÉΩÂäõÂíåËµÑÊ∫êÈôêÂà∂ÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÂ≠êÊ®°ÂûãËøõË°åÈÉ®ÁΩ≤Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÊïà„ÄÅÊõ¥Êô∫ËÉΩÁöÑÂ∫îÁî®„ÄÇ‰æãÂ¶ÇÔºåÂú®Êô∫ËÉΩÂÆ∂Â±ÖÂú∫ÊôØ‰∏≠ÔºåÂèØ‰ª•‰ΩøÁî®ËæÉÂ∞èÁöÑÂ≠êÊ®°ÂûãÂú®Êú¨Âú∞ËÆæÂ§á‰∏äËøõË°åÂø´ÈÄüÂìçÂ∫îÔºåËÄåÂ∞ÜÂ§çÊùÇÁöÑ‰ªªÂä°‰∫§Áªô‰∫ëÁ´ØËøõË°åÂ§ÑÁêÜ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Neural scaling laws have become foundational for optimizing large language model (LLM) training, yet they typically assume a single dense model output. This limitation effectively overlooks "Familial models, a transformative paradigm essential for realizing ubiquitous intelligence across heterogeneous device-edge-cloud hierarchies. Transcending static architectures, familial models integrate early exits with relay-style inference to spawn G deployable sub-models from a single shared backbone. In this work, we theoretically and empirically extend the scaling law to capture this "one-run, many-models" paradigm by introducing Granularity (G) as a fundamental scaling variable alongside model size (N) and training tokens (D). To rigorously quantify this relationship, we propose a unified functional form L(N, D, G) and parameterize it using large-scale empirical runs. Specifically, we employ a rigorous IsoFLOP experimental design to strictly isolate architectural impact from computational scale. Across fixed budgets, we systematically sweep model sizes (N) and granularities (G) while dynamically adjusting tokens (D). This approach effectively decouples the marginal cost of granularity from the benefits of scale, ensuring high-fidelity parameterization of our unified scaling law. Our results reveal that the granularity penalty follows a multiplicative power law with an extremely small exponent. Theoretically, this bridges fixed-compute training with dynamic architectures. Practically, it validates the "train once, deploy many" paradigm, demonstrating that deployment flexibility is achievable without compromising the compute-optimality of dense baselines.

