---
layout: default
title: Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment
---

# Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00430" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00430v1</a>
  <a href="https://arxiv.org/pdf/2510.00430.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00430v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00430v1', 'Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Suhyeon Lee, Jong Chul Ye

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**å¤‡æ³¨**: 23 pages, 15 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPromptLoopï¼šä¸€ç§åŸºäºéšç©ºé—´åé¦ˆçš„å³æ’å³ç”¨æç¤ºä¼˜åŒ–æ‰©æ•£æ¨¡å‹å¯¹é½æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `æç¤ºä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ¨¡å‹å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹å¾®è°ƒæ–¹æ³•æ³›åŒ–æ€§å·®ï¼Œæ˜“å—å¥–åŠ±æ“çºµï¼Œä¸”ç»„åˆæ€§ä¸è¶³ã€‚
2. PromptLoopé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ ¹æ®æ‰©æ•£æ¨¡å‹çš„ä¸­é—´éšçŠ¶æ€è¿­ä»£ä¼˜åŒ–æç¤ºè¯ã€‚
3. å®éªŒè¡¨æ˜PromptLoopèƒ½æœ‰æ•ˆä¼˜åŒ–å¥–åŠ±ï¼Œæ³›åŒ–æ€§å¥½ï¼Œå¯ä¸ç°æœ‰å¯¹é½æ–¹æ³•ç»„åˆï¼Œå¹¶å‡è½»è¿‡åº¦ä¼˜åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ‰©æ•£æ¨¡å‹å¾®è°ƒé€šå¸¸éš¾ä»¥æ³›åŒ–ã€ç»„åˆï¼Œå¹¶ä¸”å¯¹å¥–åŠ±é»‘å®¢æ”»å‡»çš„é²æ£’æ€§è¾ƒå·®ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†æç¤ºä¼˜åŒ–ä½œä¸ºä¸€ç§æ¨¡å—åŒ–æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¤§å¤šæ•°é‡‡ç”¨å‰é¦ˆæ–¹æ³•ï¼Œåœ¨æ•´ä¸ªé‡‡æ ·è½¨è¿¹ä¸­åº”ç”¨å•ä¸ªä¼˜åŒ–çš„æç¤ºï¼Œä»è€Œæœªèƒ½å……åˆ†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„é¡ºåºç‰¹æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºPromptLoopï¼Œä¸€ä¸ªå³æ’å³ç”¨çš„RLæ¡†æ¶ï¼Œå®ƒå°†éšç©ºé—´åé¦ˆèå…¥åˆ°é€æ­¥æç¤ºä¼˜åŒ–ä¸­ã€‚ä¸ä¿®æ”¹æ‰©æ•£æ¨¡å‹æƒé‡ä¸åŒï¼Œä½¿ç”¨RLè®­ç»ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä»¥åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¸­é—´éšçŠ¶æ€è¿­ä»£æ›´æ–°æç¤ºã€‚è¿™ç§è®¾è®¡å®ç°äº†ä¸æ‰©æ•£RLæ–¹æ³•çš„ç»“æ„ç±»æ¯”ï¼ŒåŒæ—¶ä¿ç•™äº†åŸºäºæç¤ºå¯¹é½çš„çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚åœ¨ä¸åŒçš„å¥–åŠ±å‡½æ•°å’Œæ‰©æ•£éª¨å¹²ç½‘ç»œä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPromptLoopï¼ˆiï¼‰å®ç°äº†æœ‰æ•ˆçš„å¥–åŠ±ä¼˜åŒ–ï¼Œï¼ˆiiï¼‰æ— ç¼æ³›åŒ–åˆ°æœªè§è¿‡çš„æ¨¡å‹ï¼Œï¼ˆiiiï¼‰ä¸ç°æœ‰çš„å¯¹é½æ–¹æ³•æ­£äº¤ç»„åˆï¼Œä»¥åŠï¼ˆivï¼‰å‡è½»äº†è¿‡åº¦ä¼˜åŒ–å’Œå¥–åŠ±é»‘å®¢æ”»å‡»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹å¯¹é½è¿‡ç¨‹ä¸­ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ–¹æ³•å­˜åœ¨çš„æ³›åŒ–æ€§å·®ã€ç»„åˆæ€§ä¸è¶³ä»¥åŠå®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»çš„é—®é¢˜ã€‚ç°æœ‰çš„æç¤ºä¼˜åŒ–æ–¹æ³•é€šå¸¸é‡‡ç”¨å‰é¦ˆæ–¹å¼ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é¡ºåºé‡‡æ ·ç‰¹æ€§ï¼Œå¯¼è‡´ä¼˜åŒ–æ•ˆæœå—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸€ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®æ‰©æ•£æ¨¡å‹çš„ä¸­é—´éšçŠ¶æ€ï¼Œè¿­ä»£åœ°ä¼˜åŒ–æç¤ºè¯ã€‚é€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œé€æ­¥è°ƒæ•´æç¤ºè¯ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå®ç°æ›´å¥½çš„å¯¹é½æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPromptLoopæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ‰©æ•£æ¨¡å‹ï¼šä½œä¸ºç”Ÿæˆå›¾åƒçš„åŸºç¡€æ¨¡å‹ã€‚2) å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼šä½œä¸ºæç¤ºè¯ä¼˜åŒ–å™¨ï¼Œæ¥æ”¶æ‰©æ•£æ¨¡å‹çš„ä¸­é—´éšçŠ¶æ€å’Œå¥–åŠ±ä¿¡å·ï¼Œè¾“å‡ºä¼˜åŒ–çš„æç¤ºè¯ã€‚3) å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼šç”¨äºè®­ç»ƒMLLMï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®å¥–åŠ±ä¿¡å·ä¼˜åŒ–æç¤ºè¯ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼šæ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸­é—´éšçŠ¶æ€ï¼ŒMLLMæ ¹æ®éšçŠ¶æ€ç”Ÿæˆä¼˜åŒ–åçš„æç¤ºè¯ï¼Œæ‰©æ•£æ¨¡å‹ä½¿ç”¨ä¼˜åŒ–åçš„æç¤ºè¯ç»§ç»­ç”Ÿæˆï¼Œç›´åˆ°ç”Ÿæˆæœ€ç»ˆå›¾åƒï¼Œæ ¹æ®æœ€ç»ˆå›¾åƒè®¡ç®—å¥–åŠ±ï¼Œå¹¶ç”¨å¥–åŠ±ä¿¡å·æ›´æ–°MLLMã€‚

**å…³é”®åˆ›æ–°**ï¼šPromptLoopçš„å…³é”®åˆ›æ–°åœ¨äºå°†éšç©ºé—´åé¦ˆèå…¥åˆ°æç¤ºè¯ä¼˜åŒ–è¿‡ç¨‹ä¸­ã€‚ä¸ä¼ ç»Ÿçš„å‰é¦ˆæç¤ºä¼˜åŒ–æ–¹æ³•ä¸åŒï¼ŒPromptLoopèƒ½å¤Ÿæ ¹æ®æ‰©æ•£æ¨¡å‹çš„ä¸­é—´çŠ¶æ€ï¼ŒåŠ¨æ€åœ°è°ƒæ•´æç¤ºè¯ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é¡ºåºé‡‡æ ·ç‰¹æ€§ã€‚æ­¤å¤–ï¼ŒPromptLoopé‡‡ç”¨å³æ’å³ç”¨çš„è®¾è®¡ï¼Œæ— éœ€ä¿®æ”¹æ‰©æ•£æ¨¡å‹çš„æƒé‡ï¼Œå³å¯å®ç°å¯¹é½ã€‚

**å…³é”®è®¾è®¡**ï¼šMLLMçš„å…·ä½“é€‰æ‹©æœªçŸ¥ï¼Œä½†éœ€è¦å…·å¤‡ç†è§£å›¾åƒä¿¡æ¯å’Œç”Ÿæˆæ–‡æœ¬çš„èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©æœªçŸ¥ï¼Œä½†éœ€è¦èƒ½å¤Ÿå¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°åæ˜ å¯¹é½ç›®æ ‡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ­¤å¤„æ— æ³•å¾—çŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPromptLoopåœ¨å¤šä¸ªå¥–åŠ±å‡½æ•°å’Œæ‰©æ•£æ¨¡å‹ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰çš„æç¤ºä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒPromptLoopèƒ½å¤Ÿæ›´å¥½åœ°ä¼˜åŒ–å¥–åŠ±ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼ŒPromptLoopè¿˜å¯ä»¥ä¸ç°æœ‰çš„å¯¹é½æ–¹æ³•æ­£äº¤ç»„åˆï¼Œè¿›ä¸€æ­¥æé«˜å¯¹é½æ•ˆæœã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PromptLoopå¯åº”ç”¨äºå„ç§éœ€è¦å¯¹é½æ‰©æ•£æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€é£æ ¼è¿ç§»ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œä¸ç›®æ ‡ä»»åŠ¡çš„ç›¸å…³æ€§ï¼Œå¹¶é™ä½æ¨¡å‹è¢«æ¶æ„åˆ©ç”¨çš„é£é™©ã€‚æœªæ¥ï¼ŒPromptLoopæœ‰æœ›æˆä¸ºæ‰©æ•£æ¨¡å‹å¯¹é½çš„æ ‡å‡†æ–¹æ³•ä¹‹ä¸€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the recent progress, reinforcement learning (RL)-based fine-tuning of diffusion models often struggles with generalization, composability, and robustness against reward hacking. Recent studies have explored prompt refinement as a modular alternative, but most adopt a feed-forward approach that applies a single refined prompt throughout the entire sampling trajectory, thereby failing to fully leverage the sequential nature of reinforcement learning. To address this, here we introduce PromptLoop, a plug-and-play RL framework that incorporates latent feedback into step-wise prompt refinement. Rather than modifying diffusion model weights, a multimodal large language model (MLLM) is trained with RL to iteratively update prompts based on intermediate latent states of diffusion models. This design achieves a structural analogy to the Diffusion RL approach, while retaining the flexibility and generality of prompt-based alignment. Extensive experiments across diverse reward functions and diffusion backbones demonstrate that PromptLoop (i) achieves effective reward optimization, (ii) generalizes seamlessly to unseen models, (iii) composes orthogonally with existing alignment methods, and (iv) mitigates over-optimization and reward hacking.

