---
layout: default
title: Can Mamba Learn In Context with Outliers? A Theoretical Generalization Analysis
---

# Can Mamba Learn In Context with Outliers? A Theoretical Generalization Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00399" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00399v1</a>
  <a href="https://arxiv.org/pdf/2510.00399.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00399v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00399v1', 'Can Mamba Learn In Context with Outliers? A Theoretical Generalization Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongkang Li, Songtao Lu, Xiaodong Cui, Pin-Yu Chen, Meng Wang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é¦–æ¬¡ç†è®ºåˆ†æMambaæ¨¡å‹ICLæ³›åŒ–èƒ½åŠ›ï¼Œè§£å†³å«ç¦»ç¾¤ç‚¹çš„äºŒå…ƒåˆ†ç±»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Mambaæ¨¡å‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `ç¦»ç¾¤ç‚¹æ£€æµ‹` `æ³›åŒ–èƒ½åŠ›` `ç†è®ºåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformerè®¡ç®—å¤æ‚åº¦é«˜ï¼ŒMambaæ¨¡å‹è™½æœ‰ä¼˜åŠ¿ï¼Œä½†å…¶ç†è®ºåŸºç¡€ï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹é¢çš„ç†è§£ä¸è¶³ã€‚
2. è®ºæ–‡åˆ†æäº†å•å±‚Mambaæ¨¡å‹çš„è®­ç»ƒåŠ¨æ€å’ŒICLæ³›åŒ–èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹åœ¨å­˜åœ¨ç¦»ç¾¤ç‚¹æ—¶çš„é²æ£’æ€§ã€‚
3. ç†è®ºåˆ†æå’Œå®éªŒè¡¨æ˜ï¼ŒMambaæ¨¡å‹èƒ½æœ‰æ•ˆé€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œå¹¶æŠ‘åˆ¶ç¦»ç¾¤ç‚¹çš„å½±å“ï¼Œä¼˜äºçº¿æ€§Transformerã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Mambaæ¨¡å‹å› å…¶åœ¨è®¡ç®—ä¸Šä¼˜äºTransformeræ¨¡å‹ï¼Œå¹¶åœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸­å®ç°äº†ç›¸å½“çš„æ€§èƒ½è€Œå¤‡å—å…³æ³¨ã€‚ä¸Transformerç±»ä¼¼ï¼ŒMambaä¹Ÿè¡¨ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ (ICL)èƒ½åŠ›ï¼Œå³åŸºäºåŒ…å«è¾“å…¥-æ ‡ç­¾å¯¹å’ŒæŸ¥è¯¢çš„æç¤ºï¼Œå¯¹æ–°ä»»åŠ¡è¿›è¡Œé¢„æµ‹ï¼Œè€Œæ— éœ€å¾®è°ƒã€‚å°½ç®¡Mambaåœ¨ç»éªŒä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†å¯¹å…¶ç†è®ºç†è§£ä»ç„¶æœ‰é™ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå…¶é—¨æ§æœºåˆ¶å¼•å…¥çš„éçº¿æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬æ–‡é¦–æ¬¡å¯¹å•å±‚Mambaæ¨¡å‹çš„è®­ç»ƒåŠ¨æ€åŠå…¶åœ¨æœªè§è¿‡çš„äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„ICLæ³›åŒ–è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå³ä½¿æç¤ºåŒ…å«åŠ æ€§ç¦»ç¾¤ç‚¹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒMambaåˆ©ç”¨çº¿æ€§æ³¨æ„åŠ›å±‚æ¥é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œå¹¶ä½¿ç”¨éçº¿æ€§é—¨æ§å±‚æ¥æŠ‘åˆ¶ç¦»ç¾¤ç‚¹çš„å½±å“ã€‚é€šè¿‡å»ºç«‹ä¸çº¿æ€§Transformeråœ¨ç›¸åŒè®¾ç½®ä¸‹çš„åˆ†æå¹¶è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œå°½ç®¡Mambaå¯èƒ½éœ€è¦æ›´å¤šçš„è®­ç»ƒè¿­ä»£æ‰èƒ½æ”¶æ•›ï¼Œä½†å³ä½¿ç¦»ç¾¤ç‚¹çš„æ¯”ä¾‹è¶…è¿‡çº¿æ€§Transformerå¯ä»¥å®¹å¿çš„é˜ˆå€¼ï¼Œå®ƒä¹Ÿèƒ½ä¿æŒå‡†ç¡®çš„é¢„æµ‹ã€‚è¿™äº›ç†è®ºå‘ç°å¾—åˆ°äº†ç»éªŒå®éªŒçš„æ”¯æŒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³Mambaæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­ï¼Œé¢å¯¹åŒ…å«ç¦»ç¾¤ç‚¹ï¼ˆoutliersï¼‰çš„promptæ—¶ï¼Œå…¶æ³›åŒ–èƒ½åŠ›å¦‚ä½•çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„æ¨¡å‹ï¼Œåœ¨å¤„ç†å¤§é‡ç¦»ç¾¤ç‚¹æ—¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼Œç¼ºä¹å¯¹å™ªå£°æ•°æ®çš„é²æ£’æ€§ã€‚Mambaæ¨¡å‹è™½ç„¶åœ¨ç»éªŒä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹ç†è®ºæ”¯æ’‘ï¼Œæ— æ³•è§£é‡Šå…¶åœ¨ICLä¸­å¤„ç†ç¦»ç¾¤ç‚¹çš„æœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ†æMambaæ¨¡å‹ä¸­çš„çº¿æ€§æ³¨æ„åŠ›å±‚å’Œéçº¿æ€§é—¨æ§å±‚å¦‚ä½•ååŒå·¥ä½œï¼Œä»¥é€‰æ‹©æœ‰ç”¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶æŠ‘åˆ¶ç¦»ç¾¤ç‚¹çš„å½±å“ã€‚çº¿æ€§æ³¨æ„åŠ›å±‚è´Ÿè´£æå–promptä¸­çš„å…³é”®ä¿¡æ¯ï¼Œè€Œéçº¿æ€§é—¨æ§å±‚åˆ™é€šè¿‡æ§åˆ¶ä¿¡æ¯çš„æµåŠ¨ï¼Œé™ä½ç¦»ç¾¤ç‚¹å¯¹æœ€ç»ˆé¢„æµ‹çš„å½±å“ã€‚è¿™ç§è®¾è®¡ä½¿å¾—Mambaæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”åŒ…å«å™ªå£°æ•°æ®çš„ICLä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡åˆ†æäº†ä¸€ä¸ªç®€åŒ–çš„å•å±‚Mambaæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”±ä¸€ä¸ªçº¿æ€§æ³¨æ„åŠ›å±‚å’Œä¸€ä¸ªéçº¿æ€§é—¨æ§å±‚ç»„æˆã€‚æ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€ä¸ªåŒ…å«è¾“å…¥-æ ‡ç­¾å¯¹å’ŒæŸ¥è¯¢çš„promptï¼Œç›®æ ‡æ˜¯é¢„æµ‹æŸ¥è¯¢çš„æ ‡ç­¾ã€‚è®ºæ–‡é¦–å…ˆå»ºç«‹äº†Mambaæ¨¡å‹çš„è®­ç»ƒåŠ¨æ€æ–¹ç¨‹ï¼Œç„¶ååˆ†æäº†æ¨¡å‹åœ¨ICLä»»åŠ¡ä¸­çš„æ³›åŒ–è¯¯å·®ã€‚é€šè¿‡å°†Mambaæ¨¡å‹çš„æ³›åŒ–è¯¯å·®ä¸çº¿æ€§Transformerçš„æ³›åŒ–è¯¯å·®è¿›è¡Œæ¯”è¾ƒï¼Œè®ºæ–‡æ­ç¤ºäº†Mambaæ¨¡å‹åœ¨å¤„ç†ç¦»ç¾¤ç‚¹æ–¹é¢çš„ä¼˜åŠ¿ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡å¯¹Mambaæ¨¡å‹åœ¨ICLä¸­çš„æ³›åŒ–èƒ½åŠ›è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶æ­ç¤ºäº†å…¶å¤„ç†ç¦»ç¾¤ç‚¹çš„æœºåˆ¶ã€‚é€šè¿‡åˆ†æçº¿æ€§æ³¨æ„åŠ›å±‚å’Œéçº¿æ€§é—¨æ§å±‚çš„ä½œç”¨ï¼Œè®ºæ–‡è§£é‡Šäº†Mambaæ¨¡å‹ä¸ºä½•èƒ½å¤Ÿæ¯”çº¿æ€§Transformeræ›´å¥½åœ°é€‚åº”åŒ…å«å™ªå£°æ•°æ®çš„ICLä»»åŠ¡ã€‚è¿™ç§ç†è®ºåˆ†æä¸ºç†è§£Mambaæ¨¡å‹çš„è¡Œä¸ºæä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç®€åŒ–çš„å•å±‚Mambaæ¨¡å‹è¿›è¡Œåˆ†æï¼Œä»¥ä¾¿æ›´å®¹æ˜“æ¨å¯¼ç†è®ºç»“æœï¼›2) å‡è®¾è¾“å…¥æ•°æ®æœä»ç‰¹å®šçš„åˆ†å¸ƒï¼Œä»¥ä¾¿è¿›è¡Œæ•°å­¦å»ºæ¨¡ï¼›3) ä½¿ç”¨æ³›åŒ–è¯¯å·®ä½œä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ï¼›4) å°†Mambaæ¨¡å‹çš„æ³›åŒ–è¯¯å·®ä¸çº¿æ€§Transformerçš„æ³›åŒ–è¯¯å·®è¿›è¡Œæ¯”è¾ƒï¼Œä»¥çªå‡ºMambaæ¨¡å‹çš„ä¼˜åŠ¿ã€‚è®ºæ–‡è¿˜å¯¹æ¨¡å‹çš„è®­ç»ƒè¿­ä»£æ¬¡æ•°è¿›è¡Œäº†åˆ†æï¼Œå‘ç°Mambaæ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šçš„è®­ç»ƒè¿­ä»£æ‰èƒ½æ”¶æ•›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œè¡¨æ˜Mambaæ¨¡å‹åœ¨å¤„ç†åŒ…å«ç¦»ç¾¤ç‚¹çš„ICLä»»åŠ¡ä¸­ä¼˜äºçº¿æ€§Transformerã€‚å…·ä½“æ¥è¯´ï¼ŒMambaæ¨¡å‹å³ä½¿åœ¨ç¦»ç¾¤ç‚¹æ¯”ä¾‹è¶…è¿‡çº¿æ€§Transformerå¯å®¹å¿çš„é˜ˆå€¼æ—¶ï¼Œä»èƒ½ä¿æŒå‡†ç¡®çš„é¢„æµ‹ã€‚å®éªŒç»“æœæ”¯æŒäº†ç†è®ºåˆ†æçš„ç»“è®ºï¼Œå¹¶éªŒè¯äº†Mambaæ¨¡å‹åœ¨å¤„ç†å™ªå£°æ•°æ®æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†å™ªå£°æ•°æ®çš„ä¸Šä¸‹æ–‡å­¦ä¹ åœºæ™¯ï¼Œä¾‹å¦‚ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬åˆ†ç±»ã€å›¾åƒè¯†åˆ«ä¸­çš„ç›®æ ‡æ£€æµ‹ã€ä»¥åŠæœºå™¨äººå­¦ä¹ ä¸­çš„ç­–ç•¥å­¦ä¹ ã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹ç¦»ç¾¤ç‚¹çš„é²æ£’æ€§ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®è´¨é‡ä¸é«˜çš„æƒ…å†µä¸‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Mamba model has gained significant attention for its computational advantages over Transformer-based models, while achieving comparable performance across a wide range of language tasks. Like Transformers, Mamba exhibits in-context learning (ICL) capabilities, i.e., making predictions for new tasks based on a prompt containing input-label pairs and a query, without requiring fine-tuning. Despite its empirical success, the theoretical understanding of Mamba remains limited, largely due to the nonlinearity introduced by its gating mechanism. To the best of our knowledge, this paper presents the first theoretical analysis of the training dynamics of a one-layer Mamba model, which consists of a linear attention component followed by a nonlinear gating layer, and its ICL generalization on unseen binary classification tasks, even when the prompt includes additive outliers. Our analysis shows that Mamba leverages the linear attention layer to select informative context examples and uses the nonlinear gating layer to suppress the influence of outliers. By establishing and comparing to the analysis of linear Transformers under the same setting, we show that although Mamba may require more training iterations to converge, it maintains accurate predictions even when the proportion of outliers exceeds the threshold that a linear Transformer can tolerate. These theoretical findings are supported by empirical experiments.

