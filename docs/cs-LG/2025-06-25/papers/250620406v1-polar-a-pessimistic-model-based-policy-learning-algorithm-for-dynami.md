---
layout: default
title: POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes
---

# POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20406" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20406v1</a>
  <a href="https://arxiv.org/pdf/2506.20406.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20406v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20406v1', 'POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruijia Zhang, Zhengling Qi, Yue Wu, Xiangyu Zhang, Yanxun Xu

**åˆ†ç±»**: stat.ML, cs.IT, cs.LG, stat.ME

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPOLARä»¥è§£å†³åŠ¨æ€æ²»ç–—æ–¹æ¡ˆä¼˜åŒ–ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŠ¨æ€æ²»ç–—æ–¹æ¡ˆ` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æ”¿ç­–å­¦ä¹ ` `ä¸ç¡®å®šæ€§é‡åŒ–` `åŒ»ç–—å†³ç­–ä¼˜åŒ–` `ç»Ÿè®¡ä¿è¯` `æ¨¡å‹åŸºç¡€æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŠ¨æ€æ²»ç–—æ–¹æ¡ˆä¼˜åŒ–æ–¹æ³•å¾€å¾€ä¾èµ–å¼ºå‡è®¾ï¼Œç¼ºä¹åœ¨éƒ¨åˆ†æ•°æ®ä¸‹çš„é²æ£’æ€§ï¼Œå¯¼è‡´å†³ç­–ä¸ç¨³å®šã€‚
2. POLARé€šè¿‡ä»ç¦»çº¿æ•°æ®ä¸­ä¼°è®¡è½¬ç§»åŠ¨æ€å¹¶é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œç»“åˆæ‚²è§‚æƒ©ç½šä¼˜åŒ–å†³ç­–ï¼Œç›´æ¥é’ˆå¯¹æ”¿ç­–æ¬¡ä¼˜æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPOLARåœ¨åˆæˆæ•°æ®å’ŒMIMIC-IIIæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæä¾›æ¥è¿‘æœ€ä¼˜çš„æ²»ç–—ç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŠ¨æ€æ²»ç–—æ–¹æ¡ˆï¼ˆDTRsï¼‰ä¸ºä¼˜åŒ–éœ€è¦éšæ—¶é—´è°ƒæ•´çš„å†³ç­–æä¾›äº†åŸåˆ™æ€§æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰ç»Ÿè®¡æ–¹æ³•ä¾èµ–äºå¼ºå‡è®¾ï¼Œç¼ºä¹åœ¨éƒ¨åˆ†æ•°æ®è¦†ç›–ä¸‹çš„é²æ£’æ€§ï¼Œè€Œç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸å…³æ³¨å¹³å‡è®­ç»ƒæ€§èƒ½ï¼Œç¼ºä¹ç»Ÿè®¡ä¿è¯ï¼Œå¹¶éœ€è§£å†³å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†POLARï¼Œä¸€ç§æ–°é¢–çš„æ‚²è§‚æ¨¡å‹åŸºç¡€æ”¿ç­–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨ç¦»çº¿DTRä¼˜åŒ–ã€‚POLARä»ç¦»çº¿æ•°æ®ä¸­ä¼°è®¡è½¬ç§»åŠ¨æ€ï¼Œå¹¶é‡åŒ–æ¯ä¸ªå†å²-åŠ¨ä½œå¯¹çš„ä¸ç¡®å®šæ€§ï¼Œè¿›è€Œåœ¨å¥–åŠ±å‡½æ•°ä¸­å¼•å…¥æ‚²è§‚æƒ©ç½šï¼Œä»¥æŠ‘åˆ¶é«˜ä¸ç¡®å®šæ€§çš„åŠ¨ä½œã€‚ä¸è®¸å¤šç°æœ‰æ–¹æ³•ä¸åŒï¼ŒPOLARç›´æ¥é’ˆå¯¹æœ€ç»ˆå­¦ä¹ æ”¿ç­–çš„æ¬¡ä¼˜æ€§ï¼Œå¹¶æä¾›ç†è®ºä¿è¯ï¼Œè€Œæ— éœ€ä¾èµ–è®¡ç®—å¯†é›†çš„æœ€å°æœ€å¤§æˆ–çº¦æŸä¼˜åŒ–ç¨‹åºã€‚POLARæ˜¯é¦–ä¸ªæä¾›ç»Ÿè®¡å’Œè®¡ç®—ä¿è¯çš„æ¨¡å‹åŸºç¡€DTRæ–¹æ³•ï¼ŒåŒ…å«æ”¿ç­–æ¬¡ä¼˜æ€§çš„æœ‰é™æ ·æœ¬ç•Œé™ã€‚å®éªŒè¯æ˜ï¼ŒPOLARåœ¨åˆæˆæ•°æ®å’ŒMIMIC-IIIæ•°æ®é›†ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæä¾›æ¥è¿‘æœ€ä¼˜çš„å†å²æ„ŸçŸ¥æ²»ç–—ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŠ¨æ€æ²»ç–—æ–¹æ¡ˆä¼˜åŒ–ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¼ºå‡è®¾ï¼Œå¯¼è‡´åœ¨éƒ¨åˆ†æ•°æ®è¦†ç›–ä¸‹çš„å†³ç­–ä¸ç¨³å®šï¼Œç¼ºä¹ç»Ÿè®¡ä¿è¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPOLARé€šè¿‡ä»ç¦»çº¿æ•°æ®ä¸­ä¼°è®¡è½¬ç§»åŠ¨æ€ï¼Œé‡åŒ–æ¯ä¸ªå†å²-åŠ¨ä½œå¯¹çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶åœ¨å¥–åŠ±å‡½æ•°ä¸­å¼•å…¥æ‚²è§‚æƒ©ç½šï¼ŒæŠ‘åˆ¶é«˜ä¸ç¡®å®šæ€§çš„åŠ¨ä½œï¼Œä»è€Œä¼˜åŒ–å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPOLARçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€è½¬ç§»åŠ¨æ€ä¼°è®¡ã€ä¸ç¡®å®šæ€§é‡åŒ–å’Œå¥–åŠ±å‡½æ•°è°ƒæ•´å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»ç¦»çº¿æ•°æ®ä¸­æå–ä¿¡æ¯ï¼Œç„¶åä¼°è®¡çŠ¶æ€è½¬ç§»ï¼Œæ¥ç€é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œæœ€åä¼˜åŒ–å†³ç­–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šPOLARçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ‚²è§‚æ¨¡å‹åŸºç¡€çš„è®¾è®¡ï¼Œé¦–æ¬¡åœ¨DTRä¼˜åŒ–ä¸­æä¾›äº†ç»Ÿè®¡å’Œè®¡ç®—ä¿è¯ï¼ŒåŒ…æ‹¬æ”¿ç­–æ¬¡ä¼˜æ€§çš„æœ‰é™æ ·æœ¬ç•Œé™ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å¹³å‡æ€§èƒ½ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šPOLARçš„å…³é”®è®¾è®¡åŒ…æ‹¬ä¸ç¡®å®šæ€§é‡åŒ–çš„ç®—æ³•ã€æ‚²è§‚æƒ©ç½šçš„å…·ä½“å®ç°ï¼Œä»¥åŠå¥–åŠ±å‡½æ•°çš„è°ƒæ•´ç­–ç•¥ï¼Œç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æœ‰æ•ˆæŠ‘åˆ¶é«˜é£é™©å†³ç­–ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒPOLARèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æä¾›æ›´ä¸ºç¨³å¥çš„å†³ç­–æ”¯æŒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPOLARåœ¨åˆæˆæ•°æ®å’ŒMIMIC-IIIæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºæ”¿ç­–æ¬¡ä¼˜æ€§é™ä½äº†çº¦20%ï¼Œå¹¶ä¸”åœ¨å†å²æ„ŸçŸ¥æ²»ç–—ç­–ç•¥çš„ä¼˜åŒ–ä¸Šè¾¾åˆ°äº†æ¥è¿‘æœ€ä¼˜çš„æ•ˆæœï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

POLARçš„ç ”ç©¶æˆæœåœ¨åŒ»ç–—ã€æ•™è‚²å’Œæ•°å­—å¹²é¢„ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡ä¼˜åŒ–åŠ¨æ€æ²»ç–—æ–¹æ¡ˆï¼Œèƒ½å¤Ÿä¸ºæ‚£è€…æä¾›ä¸ªæ€§åŒ–çš„æ²»ç–—ç­–ç•¥ï¼Œæé«˜æ²»ç–—æ•ˆæœã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯åº”ç”¨äºå…¶ä»–éœ€è¦åŠ¨æ€å†³ç­–çš„é¢†åŸŸï¼Œå¦‚ä¸ªæ€§åŒ–æ•™è‚²å’Œåœ¨çº¿æ¨èç³»ç»Ÿï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dynamic treatment regimes (DTRs) provide a principled framework for optimizing sequential decision-making in domains where decisions must adapt over time in response to individual trajectories, such as healthcare, education, and digital interventions. However, existing statistical methods often rely on strong positivity assumptions and lack robustness under partial data coverage, while offline reinforcement learning approaches typically focus on average training performance, lack statistical guarantees, and require solving complex optimization problems. To address these challenges, we propose POLAR, a novel pessimistic model-based policy learning algorithm for offline DTR optimization. POLAR estimates the transition dynamics from offline data and quantifies uncertainty for each history-action pair. A pessimistic penalty is then incorporated into the reward function to discourage actions with high uncertainty. Unlike many existing methods that focus on average training performance, POLAR directly targets the suboptimality of the final learned policy and offers theoretical guarantees, without relying on computationally intensive minimax or constrained optimization procedures. To the best of our knowledge, POLAR is the first model-based DTR method to provide both statistical and computational guarantees, including finite-sample bounds on policy suboptimality. Empirical results on both synthetic data and the MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods and yields near-optimal, history-aware treatment strategies.

