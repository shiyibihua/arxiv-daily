---
layout: default
title: Echo State Transformer: Attention Over Finite Memories
---

# Echo State Transformer: Attention Over Finite Memories

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.02917" class="toolbar-btn" target="_blank">üìÑ arXiv: 2507.02917v2</a>
  <a href="https://arxiv.org/pdf/2507.02917.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.02917v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.02917v2', 'Echo State Transformer: Attention Over Finite Memories')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yannis Bendi-Ouis, Xavier Hinaut

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-25 (Êõ¥Êñ∞: 2025-10-27)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂõûÂ£∞Áä∂ÊÄÅÂèòÊç¢Âô®‰ª•Ëß£ÂÜ≥TransformerËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂõûÂ£∞Áä∂ÊÄÅÁΩëÁªú` `Ê∞¥Â∫ìËÆ°ÁÆó` `Transformer` `Êó∂Èó¥Â∫èÂàóÂàÜÊûê` `ÂºÇÂ∏∏Ê£ÄÊµã` `ÂàÜÁ±ª‰ªªÂä°` `ËÆ°ÁÆóÂ§çÊùÇÂ∫¶` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑTransformerÊ®°ÂûãÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ËÆ°ÁÆóÂ§çÊùÇÂ∫¶Âëà‰∫åÊ¨°Â¢ûÈïøÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Êó∂Èó¥Â∫èÂàó‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÂõûÂ£∞Áä∂ÊÄÅÂèòÊç¢Âô®ÔºàESTÔºâÔºåÈÄöËøáÁªìÂêàTransformerÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏éÊ∞¥Â∫ìËÆ°ÁÆóÂéüÁêÜÔºåÊûÑÂª∫È´òÊïàÁöÑÂõ∫ÂÆöÂ§ßÂ∞èËÆ∞ÂøÜÁ≥ªÁªü„ÄÇ
3. ESTÂú®69‰∏™Êó∂Èó¥Â∫èÂàó‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂú®‰∫î‰∏™Á±ªÂà´‰∏≠ÊéíÂêçÁ¨¨‰∏ÄÔºåÂ∞§ÂÖ∂Âú®ÂàÜÁ±ªÂíåÂºÇÂ∏∏Ê£ÄÊµã‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÂ§ö‰∏™ÂÖàËøõÂü∫Á∫ø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂèäÂÖ∂Âü∫Á°ÄÁöÑTransformerÊû∂ÊûÑÂú®ÊïàÁéá‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨Âπ∂Êú™ÂèçÊò†Â§ßËÑëÂ§ÑÁêÜÂíåÂ≠¶‰π†ËØ≠Ë®ÄÂèäÂ∑•‰ΩúËÆ∞ÂøÜÁ≠âÂ§öÊ†∑ËÆ§Áü•‰ªªÂä°ÁöÑÊñπÂºè„ÄÇÊ≠§Â§ñÔºåTransformerÂú®Â§ÑÁêÜÂ∫èÂàóÊï∞ÊçÆÊó∂Èù¢‰∏¥ÁùÄÂ∫èÂàóÈïøÂ∫¶ÂØºËá¥ÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶Âëà‰∫åÊ¨°Â¢ûÈïøÁöÑÊ†πÊú¨ÊÄßÈöúÁ¢ç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÂõûÂ£∞Áä∂ÊÄÅÂèòÊç¢Âô®ÔºàESTÔºâÔºåËøôÊòØ‰∏ÄÁßçÊ∑∑ÂêàÊû∂ÊûÑÔºåËÉΩÂ§ü‰ºòÈõÖÂú∞Ëß£ÂÜ≥ËÆ°ÁÆóÊïàÁéáÈóÆÈ¢òÔºåÂπ∂Âú®ÂàÜÁ±ªÂíåÊ£ÄÊµã‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇESTÂ∞ÜTransformerÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏éÊ∞¥Â∫ìËÆ°ÁÆóÁöÑÂéüÁêÜÁõ∏ÁªìÂêàÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™Âõ∫ÂÆöÂ§ßÂ∞èÁöÑÁ™óÂè£ÂàÜÂ∏ÉÂºèËÆ∞ÂøÜÁ≥ªÁªü„ÄÇÈÄöËøáÁÅµÊÑüÊù•Ëá™ÂõûÂ£∞Áä∂ÊÄÅÁΩëÁªúÔºåESTÂà©Áî®ÈöèÊú∫ÈÄíÂΩíÁΩëÁªú‰Ωú‰∏∫ËΩªÈáèÈ´òÊïàÁöÑËÆ∞ÂøÜÂçïÂÖÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåESTÂú®Êó∂Èó¥Â∫èÂàóÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂Âú®ÂàÜÁ±ªÂíåÂºÇÂ∏∏Ê£ÄÊµã‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÂ§öÈ°πÂÖàËøõÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥TransformerÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ËÆ°ÁÆóÂ§çÊùÇÂ∫¶Âëà‰∫åÊ¨°Â¢ûÈïøÁöÑÈóÆÈ¢òÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂Âú®Êó∂Èó¥Â∫èÂàóÊï∞ÊçÆÂ§ÑÁêÜ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÂõûÂ£∞Áä∂ÊÄÅÂèòÊç¢Âô®ÔºàESTÔºâÈÄöËøáÁªìÂêàTransformerÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏éÊ∞¥Â∫ìËÆ°ÁÆóÁöÑÂéüÁêÜÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™È´òÊïàÁöÑÂõ∫ÂÆöÂ§ßÂ∞èËÆ∞ÂøÜÁ≥ªÁªüÔºå‰ªéËÄåÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöESTÊû∂ÊûÑÂåÖÂê´Â§ö‰∏™Âπ∂Ë°åÂ∑•‰ΩúÁöÑÊ∞¥Â∫ì‰Ωú‰∏∫Áã¨Á´ãÁöÑÂ∑•‰ΩúËÆ∞ÂøÜÂçïÂÖÉÔºåÂà©Áî®ÈöèÊú∫ÈÄíÂΩíÁΩëÁªú‰Ωú‰∏∫ËΩªÈáèÁ∫ßËÆ∞ÂøÜ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨ËæìÂÖ•Â∫èÂàóÁöÑÂ§ÑÁêÜ„ÄÅÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂ∫îÁî®‰ª•ÂèäÈÄöËøáÂ∑•‰ΩúËÆ∞ÂøÜÂçïÂÖÉÁöÑÂä®ÊÄÅË∞ÉÊï¥„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöESTÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éËÆ≠ÁªÉÁªèÂÖ∏Ê∞¥Â∫ìÁöÑË∂ÖÂèÇÊï∞Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂä®ÊÄÅÈÄÇÂ∫îËÆ∞ÂøÜ‰∏éÈùûÁ∫øÊÄß‰πãÈó¥ÁöÑÊùÉË°°Ôºå‰ªéËÄåÊúâÊïàËß£ÂÜ≥‰∫ÜÊ†áÂáÜTransformerÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöESTÁöÑËÆæËÆ°ÂåÖÊã¨Â§ö‰∏™Âπ∂Ë°åÊ∞¥Â∫ìÁöÑÊûÑÂª∫„ÄÅÂä®ÊÄÅË∞ÉÊï¥ÁöÑË∂ÖÂèÇÊï∞ËÆæÁΩÆÔºå‰ª•ÂèäÂú®ÊØè‰∏™Â§ÑÁêÜÊ≠•È™§‰∏≠‰øùÊåÅÊÅíÂÆöÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®69‰∏™Êó∂Èó¥Â∫èÂàó‰ªªÂä°ÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÔºåESTÂú®‰∫î‰∏™Á±ªÂà´‰∏≠ÊéíÂêçÁ¨¨‰∏ÄÔºåÂ∞§ÂÖ∂Âú®ÂàÜÁ±ªÂíåÂºÇÂ∏∏Ê£ÄÊµã‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÂ§ö‰∏™Âº∫Â§ßÁöÑÂÖàËøõÂü∫Á∫øÔºåÂ±ïÁé∞Âá∫ÂçìË∂äÁöÑÊÄßËÉΩÂíåÁ´û‰∫âÂäõ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåESTÂú®ÂàÜÁ±ª‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÊúÄÂÖàËøõÊ®°ÂûãÔºå‰∏îÂú®Áü≠ÊúüÈ¢ÑÊµã‰ªªÂä°‰∏≠‰øùÊåÅÁ´û‰∫âÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ÂõûÂ£∞Áä∂ÊÄÅÂèòÊç¢Âô®ÔºàESTÔºâÂú®Êó∂Èó¥Â∫èÂàóÂàÜÁ±ªÂíåÂºÇÂ∏∏Ê£ÄÊµãÁ≠â‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫‰ºòË∂äÊÄßËÉΩÔºåÈÄÇÁî®‰∫éÈáëËûçÁõëÊµã„ÄÅÂÅ•Â∫∑ÁõëÊµãÂíåÂ∑•‰∏öÊïÖÈöúÊ£ÄÊµãÁ≠âÈ¢ÜÂüü„ÄÇÂÖ∂È´òÊïàÁöÑËÆ°ÁÆóÁâπÊÄß‰ΩøÂÖ∂Âú®ÈúÄË¶ÅÂø´ÈÄüÂìçÂ∫îÂíåÂÆûÊó∂Â§ÑÁêÜÁöÑÂ∫îÁî®Âú∫ÊôØ‰∏≠ÂÖ∑ÊúâÂÆûÈôÖ‰ª∑ÂÄºÔºåÊú™Êù•ÂèØËøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÈúÄË¶ÅÂ§ÑÁêÜÂ∫èÂàóÊï∞ÊçÆÁöÑÈ¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> While Large Language Models and their underlying Transformer architecture are remarkably efficient, they do not reflect how our brain processes and learns a diversity of cognitive tasks such as language and working memory. Furthermore, sequential data processing with Transformers encounters a fundamental barrier: quadratic complexity growth with sequence length. Motivated by these limitations, our ambition is to create more efficient models that are less reliant on intensive computations. We introduce Echo State Transformers (EST), a hybrid architecture that elegantly resolves this challenge while demonstrating exceptional performance in classification and detection tasks. EST integrates the Transformer attention mechanisms with principles from Reservoir Computing to create a fixed-size window distributed memory system. Drawing inspiration from Echo State Networks, the most prominent instance of the Reservoir Computing paradigm, our approach leverages reservoirs (random recurrent networks) as a lightweight and efficient memory. Our architecture integrates a new module called ''Working Memory'' based on several reservoirs working in parallel. These reservoirs work as independent working memory units with distinct internal dynamics. A novelty here is that the classical reservoir hyperparameters, controlling the dynamics, are now trained. Thus, the EST dynamically adapts the reservoir memory/non-linearity trade-off. Thanks to these working memory units, EST achieves constant computational complexity at each processing step, effectively breaking the quadratic scaling problem of standard Transformers. We evaluate ESTs on a recent challenging timeseries benchmark: the Time Series Library, which comprises 69 tasks across five categories. Results show that ESTs ranks first overall in two of five categories, outperforming strong state-of-the-art baselines on classification and anomaly detection tasks, while remaining competitive on short-term forecasting. These results position ESTs as a compelling alternative for time-series classification and anomaly detection, and a practical complement to transformer-style models in applications that prioritize robust representations and sensitive event detection.

