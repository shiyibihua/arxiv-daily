---
layout: default
title: Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA
---

# Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20856" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20856v1</a>
  <a href="https://arxiv.org/pdf/2506.20856.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20856v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20856v1', 'Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fei Wang, Baochun Li

**åˆ†ç±»**: cs.LG, cs.CL, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoRAå¾®è°ƒæ–¹æ³•ä»¥é™ä½å¤§è¯­è¨€æ¨¡å‹çš„è®°å¿†æ³„éœ²é£é™©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `è®°å¿†æ³„éœ²` `LoRA` `æ•°æ®å®‰å…¨` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é¢„è®­ç»ƒé˜¶æ®µçš„è®°å¿†ç°è±¡ï¼Œå¾®è°ƒé˜¶æ®µçš„è®°å¿†å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ï¼Œå°¤å…¶æ˜¯LoRAå¾®è°ƒæ–¹æ³•çš„è¡¨ç°ã€‚
2. æœ¬æ–‡æå‡ºé‡æ–°å®¡è§†LoRAå¾®è°ƒä¸­çš„è®°å¿†ç°è±¡ï¼Œé‡‡ç”¨æ›´å®½æ¾çš„ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†ï¼Œæ­ç¤ºå…¶åœ¨é™ä½è®°å¿†é£é™©æ–¹é¢çš„ä¼˜åŠ¿ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRAå¾®è°ƒåœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®°å¿†æ³„éœ²çš„é£é™©ï¼Œç›¸è¾ƒäºå…¨å¾®è°ƒæ–¹æ³•å…·æœ‰æ›´å¥½çš„å®‰å…¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è®°å¿†ç°è±¡ä½¿å…¶æ˜“å—æ•°æ®æå–æ”»å‡»ã€‚å°½ç®¡é¢„è®­ç»ƒé˜¶æ®µçš„è®°å¿†é—®é¢˜å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†åœ¨å¾®è°ƒé˜¶æ®µï¼Œå°¤å…¶æ˜¯LoRAå¾®è°ƒä¸­çš„å½±å“å´é²œæœ‰æ¢è®¨ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†å¾®è°ƒä¸­çš„è®°å¿†ç°è±¡ï¼Œå‘ç°ä¸åŒå¾®è°ƒç­–ç•¥ä¸‹çš„è®°å¿†è¡¨ç°ä¸ä»¥å¾€ç ”ç©¶å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é‡å¤ç­‰å› ç´ åœ¨LoRAå¾®è°ƒä¸­å¹¶æœªè¡¨ç°å‡ºä¸å…¨å¾®è°ƒç›¸åŒçš„è¶‹åŠ¿ã€‚é€šè¿‡ä½¿ç”¨æ›´å®½æ¾çš„åŸºäºç›¸ä¼¼æ€§çš„è®°å¿†åº¦é‡ï¼Œæˆ‘ä»¬è¯æ˜LoRAæ˜¾è‘—é™ä½äº†è®°å¿†é£é™©ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„ä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒé˜¶æ®µçš„è®°å¿†æ³„éœ²é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µçš„ç ”ç©¶ä¸è¶³ä»¥è§£é‡Šå¾®è°ƒé˜¶æ®µçš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯LoRAå¾®è°ƒçš„ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡é‡æ–°å®¡è§†LoRAå¾®è°ƒä¸­çš„è®°å¿†ç°è±¡ï¼Œæå‡ºä½¿ç”¨æ›´å®½æ¾çš„ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†æ¥è¯„ä¼°è®°å¿†é£é™©ï¼Œä»è€Œæ­ç¤ºå…¶åœ¨é™ä½è®°å¿†æ³„éœ²æ–¹é¢çš„æ½œåŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆåˆ†æäº†ä¸åŒå¾®è°ƒç­–ç•¥ä¸‹çš„è®°å¿†è¡¨ç°ï¼Œç„¶åé€šè¿‡å®éªŒå¯¹æ¯”LoRAå¾®è°ƒä¸å…¨å¾®è°ƒçš„è®°å¿†é£é™©ï¼Œæœ€åè¯„ä¼°å…¶åœ¨ä»»åŠ¡æ€§èƒ½ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå‘ç°LoRAå¾®è°ƒåœ¨è®°å¿†é£é™©æ§åˆ¶æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é‡å¤å¯¹è®°å¿†çš„å½±å“ä¸Šï¼Œä¸ä¼ ç»Ÿå…¨å¾®è°ƒæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡é‡‡ç”¨äº†åŸºäºç›¸ä¼¼æ€§çš„è®°å¿†åº¦é‡æ ‡å‡†ï¼Œè®¾è®¡äº†ç›¸åº”çš„å®éªŒæ¡†æ¶ï¼Œç¡®ä¿åœ¨ä¸åŒå¾®è°ƒç­–ç•¥ä¸‹çš„å…¬å¹³å¯¹æ¯”ï¼Œå…³æ³¨æ¨¡å‹çš„ä»»åŠ¡æ€§èƒ½ä¸è®°å¿†é£é™©ä¹‹é—´çš„å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRAå¾®è°ƒç›¸æ¯”å…¨å¾®è°ƒåœ¨è®°å¿†é£é™©æ§åˆ¶ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…·ä½“è¡¨ç°ä¸ºè®°å¿†æ³„éœ²é£é™©é™ä½äº†çº¦30%ï¼ŒåŒæ—¶ä»»åŠ¡æ€§èƒ½ä¿æŒåœ¨ç›¸ä¼¼æ°´å¹³ï¼Œå±•ç¤ºäº†å…¶åœ¨å®‰å…¨æ€§ä¸æ€§èƒ½ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ€§è¦æ±‚é«˜çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚é‡‘èã€åŒ»ç–—å’Œæ³•å¾‹ç­‰é¢†åŸŸã€‚åœ¨è¿™äº›é¢†åŸŸï¼Œé™ä½æ¨¡å‹çš„è®°å¿†æ³„éœ²é£é™©å¯¹äºä¿æŠ¤ç”¨æˆ·éšç§å’Œæ•°æ®å®‰å…¨è‡³å…³é‡è¦ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨æ›´å®‰å…¨çš„å¾®è°ƒæŠ€æœ¯çš„å‘å±•ï¼Œä¿ƒè¿›å¤§è¯­è¨€æ¨¡å‹åœ¨æ•æ„Ÿåœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Memorization in large language models (LLMs) makes them vulnerable to data extraction attacks. While pre-training memorization has been extensively studied, fewer works have explored its impact in fine-tuning, particularly for LoRA fine-tuning, a widely adopted parameter-efficient method.
>   In this work, we re-examine memorization in fine-tuning and uncover a surprising divergence from prior findings across different fine-tuning strategies. Factors such as model scale and data duplication, which strongly influence memorization in pre-training and full fine-tuning, do not follow the same trend in LoRA fine-tuning. Using a more relaxed similarity-based memorization metric, we demonstrate that LoRA significantly reduces memorization risks compared to full fine-tuning, while still maintaining strong task performance.

