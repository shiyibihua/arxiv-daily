---
layout: default
title: A foundation model with multi-variate parallel attention to generate neuronal activity
---

# A foundation model with multi-variate parallel attention to generate neuronal activity

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20354" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.20354v2</a>
  <a href="https://arxiv.org/pdf/2506.20354.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20354v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20354v2', 'A foundation model with multi-variate parallel attention to generate neuronal activity')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Francesco Carzaniga, Michael Hersche, Abu Sebastian, Kaspar Schindler, Abbas Rahimi

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-25 (Êõ¥Êñ∞: 2025-08-25)

**Â§áÊ≥®**: The code is available at https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG dataset is available at https://huggingface.co/datasets/NeuroTec/SWEC_iEEG_Dataset

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/IBM/multi-variate-parallel-transformer) | [HUGGINGFACE](https://huggingface.co/datasets/NeuroTec)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â§öÂèòÈáèÂπ∂Ë°åÊ≥®ÊÑèÂäõÊú∫Âà∂‰ª•Ëß£ÂÜ≥iEEG‰ø°Âè∑È¢ÑÊµãÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÂèòÈáèÊó∂Èó¥Â∫èÂàó` `Âπ∂Ë°åÊ≥®ÊÑèÂäõ` `Ê∑±Â∫¶Â≠¶‰π†` `È¢ÖÂÜÖËÑëÁîµÂõæ` `ÁîüÊàêÊ®°Âûã` `‰∏¥Â∫äÂ∫îÁî®` `Êï∞ÊçÆÈõÜÂèëÂ∏É`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂÖ∑ÊúâÂºÇÊûÑÈÄöÈÅìÈÖçÁΩÆÁöÑÂ§öÂèòÈáèÊó∂Èó¥Â∫èÂàóÊó∂ÔºåÈù¢‰∏¥ÁùÄÁÅµÊ¥ªÊÄßÂíåÂèØÊ≥õÂåñÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑMVPAÊú∫Âà∂ÈÄöËøáËß£ËÄ¶ÂÜÖÂÆπ„ÄÅÊó∂Èó¥ÂíåÁ©∫Èó¥Ê≥®ÊÑèÂäõÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÊó∂Èó¥Â∫èÂàóÂª∫Ê®°ÊñπÊ≥ï„ÄÇ
3. MVPFormerÂú®Â§ö‰∏™iEEG‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÂ§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜÁöÑÁé∞ÊúâÊúÄÂÖàËøõÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰ªéÂÖ∑ÊúâÂºÇÊûÑÈÄöÈÅìÈÖçÁΩÆÁöÑÂ§öÂèòÈáèÊó∂Èó¥Â∫èÂàó‰∏≠Â≠¶‰π†‰ªçÁÑ∂ÊòØÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÈù¢‰∏¥ÁöÑÂü∫Êú¨ÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏¥Â∫äÈ¢ÜÂüüÂ¶ÇÈ¢ÖÂÜÖËÑëÁîµÂõæÔºàiEEGÔºâ‰∏≠„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™Ê≥®ÊÑèÂäõÊú∫Âà∂‚Äî‚ÄîÂ§öÂèòÈáèÂπ∂Ë°åÊ≥®ÊÑèÂäõÔºàMVPAÔºâÔºåÂÆÉËÉΩÂ§üËß£ËÄ¶ÂÜÖÂÆπ„ÄÅÊó∂Èó¥ÂíåÁ©∫Èó¥Ê≥®ÊÑèÂäõÔºå‰ªéËÄåÁÅµÊ¥ª„ÄÅÈ´òÊïàÂú∞Âª∫Ê®°ÂÖ∑Êúâ‰∏çÂêåÈÄöÈÅìÊï∞ÈáèÂíåÈÖçÁΩÆÁöÑÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆ„ÄÇÂü∫‰∫éMVPAÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜMVPFormerÔºå‰∏Ä‰∏™Áî®‰∫é‰∫∫Á±ªÁîµÁîüÁêÜÂ≠¶ÁöÑÁîüÊàêÂü∫Á°ÄÊ®°ÂûãÔºåËÉΩÂ§üÈ¢ÑÊµã‰∏çÂêåÂèóËØïËÄÖÁöÑiEEG‰ø°Âè∑ÊºîÂèò„ÄÇÊàë‰ª¨ËøòÂèëÂ∏É‰∫ÜSWEC iEEGÊï∞ÊçÆÈõÜÔºåËøôÊòØËøÑ‰ªä‰∏∫Ê≠¢ÊúÄÂ§ßÁöÑÂÖ¨ÂºÄiEEGÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Ëøë10,000Â∞èÊó∂Êù•Ëá™ÂºÇÊûÑ‰∏¥Â∫äÊù•Ê∫êÁöÑËÆ∞ÂΩï„ÄÇMVPFormerÂú®Â§ö‰∏™iEEG‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫‰∏ìÂÆ∂Á∫ßÁöÑÊÄßËÉΩÔºåË∂ÖË∂ä‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑTransformerÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂú®Â§ÑÁêÜÂÖ∑ÊúâÂºÇÊûÑÈÄöÈÅìÈÖçÁΩÆÁöÑÂ§öÂèòÈáèÊó∂Èó¥Â∫èÂàóÊó∂ÁöÑÁÅµÊ¥ªÊÄßÂíåÂèØÊ≥õÂåñÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÊó†Ê≥ïÊúâÊïàÈÄÇÂ∫î‰∏çÂêåÂèóËØïËÄÖÁöÑiEEG‰ø°Âè∑ÁâπÂæÅ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑMVPAÊú∫Âà∂ÈÄöËøáËß£ËÄ¶ÂÜÖÂÆπ„ÄÅÊó∂Èó¥ÂíåÁ©∫Èó¥Ê≥®ÊÑèÂäõÔºåËÉΩÂ§üÁÅµÊ¥ªÂú∞ÈÄÇÂ∫î‰∏çÂêåÁöÑÈÄöÈÅìÈÖçÁΩÆÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMVPFormerÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ËæìÂÖ•Â±Ç„ÄÅMVPAÊ®°ÂùóÂíåËæìÂá∫Â±Ç„ÄÇMVPAÊ®°ÂùóË¥üË¥£Â§ÑÁêÜËæìÂÖ•ÁöÑÂ§öÂèòÈáèÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÔºåÊèêÂèñÁõ∏ÂÖ≥ÁöÑÊó∂Èó¥ÂíåÁ©∫Èó¥ÁâπÂæÅ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMVPA‰Ωú‰∏∫‰∏ÄÁßçÊñ∞ÁöÑËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÊúâÊïàËß£ËÄ¶‰∏çÂêåÁ±ªÂûãÁöÑÊ≥®ÊÑèÂäõÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂú®Â§ÑÁêÜÂºÇÊûÑÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÊó∂ÁöÑÊÄßËÉΩÔºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÂÖ∑ÊúâÊú¨Ë¥®ÁöÑÂå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñÈ¢ÑÊµãÁ≤æÂ∫¶ÔºåÂπ∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏≠ÂºïÂÖ•‰∫ÜÂ§öÂ±ÇMVPAÊ®°ÂùóÔºå‰ª•Â¢ûÂº∫Ê®°ÂûãÁöÑË°®ËææËÉΩÂäõÂíåÂ≠¶‰π†ËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÂ±ÇÊï∞Ê†πÊçÆÂÆûÈ™åÁªìÊûúËøõË°å‰∫Ü‰ºòÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MVPFormerÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Áô´Áó´Ê£ÄÊµã‰ªªÂä°‰∏≠ÔºåË∂ÖË∂ä‰∫ÜSWEC„ÄÅMAYOÂíåFNUSAÊï∞ÊçÆÈõÜ‰∏äÁöÑÁé∞ÊúâÊúÄÂÖàËøõÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÂú®Âõõ‰∏™Brain TreeBank iEEGËß£Á†Å‰ªªÂä°‰∏≠‰πüËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜMVPAÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰∏¥Â∫äÁîµÁîüÁêÜÂ≠¶„ÄÅÁ•ûÁªèÁßëÂ≠¶Á†îÁ©∂‰ª•ÂèäÁõ∏ÂÖ≥ÁöÑÂåªÁñóËÆæÂ§áÂºÄÂèë„ÄÇÈÄöËøáÊèê‰æõ‰∏Ä‰∏™ÂºÄÊîæÊ∫ê‰ª£Á†ÅÂíåÂºÄÊîæÊï∞ÊçÆÁöÑÂü∫Á°ÄÊ®°ÂûãÔºåÁ†îÁ©∂ËÄÖÂèØ‰ª•Âú®Ê≠§Âü∫Á°Ä‰∏äËøõË°åËøõ‰∏ÄÊ≠•ÁöÑÊé¢Á¥¢ÂíåÂ∫îÁî®ÔºåÊé®Âä®iEEG‰ø°Âè∑ÂàÜÊûêÁöÑËøõÊ≠•„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Learning from multi-variate time-series with heterogeneous channel configurations remains a fundamental challenge for deep neural networks, particularly in clinical domains such as intracranial electroencephalography (iEEG), where channel setups vary widely across subjects. In this work, we introduce multi-variate parallel attention (MVPA), a novel self-attention mechanism that disentangles content, temporal, and spatial attention, enabling flexible, generalizable, and efficient modeling of time-series data with varying channel counts and configurations. We use MVPA to build MVPFormer, a generative foundation model for human electrophysiology, trained to predict the evolution of iEEG signals across diverse subjects. To support this and future efforts by the community, we release the SWEC iEEG dataset, the largest publicly available iEEG dataset to date, comprising nearly 10,000 hours of recordings from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong generalization across subjects, demonstrating expert-level performance in several iEEG tasks. MVPFormer surpasses state-of-the-art Transformer baselines in seizure detection across the SWEC, the MAYO, and the FNUSA datasets, while also achieving state-of-the-art performance on four Brain TreeBank iEEG decoding tasks. We further validate MVPA on standard time-series forecasting and classification tasks, where it matches or exceeds the performance of existing attention-based models. Together, our contributions establish MVPA as a general-purpose attention mechanism for heterogeneous time-series and MVPFormer as the first open-source, open-weights, and open-data iEEG foundation model with SOTA clinical performance. The code is available at https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG dataset is available at https://huggingface.co/datasets/NeuroTec/SWEC_iEEG_Dataset.

