---
layout: default
title: Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards
---

# Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20520" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20520v2</a>
  <a href="https://arxiv.org/pdf/2506.20520.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20520v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20520v2', 'Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Charles Arnal, GaÃ«tan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, Remi Munos

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25 (æ›´æ–°: 2025-11-28)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸å¯¹ç§°REINFORCEç®—æ³•ä»¥å¹³è¡¡æ­£è´Ÿå¥–åŠ±**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç¦»ç­–ç•¥æ–¹æ³•` `å¥–åŠ±ä¿¡å·` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç­–ç•¥ä¼˜åŒ–` `ä¸å¯¹ç§°REINFORCE` `å¾®è°ƒ` `å®éªŒéªŒè¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¸¸å¸¸ä¸å¦‚åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ­£è´Ÿå¥–åŠ±ä¿¡å·æ—¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸å¯¹ç§°REINFORCEç®—æ³•ï¼Œé€šè¿‡è°ƒæ•´åŸºçº¿$V$æ¥å¼ºè°ƒé«˜å¥–åŠ±æ ·æœ¬ï¼Œä»è€Œæ”¹å–„ç­–ç•¥æ›´æ–°æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨éšæœºèµŒåšæœºè®¾ç½®å’ŒLLMsæ¨ç†ä»»åŠ¡å¾®è°ƒä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ–¹é¢çš„åº”ç”¨æ—¥ç›Šå¢åŠ ã€‚ä¸åŸºäºç­–ç•¥çš„æ–¹æ³•ç›¸æ¯”ï¼Œç¦»ç­–ç•¥æ–¹æ³•åœ¨å®ç°ç®€å•æ€§å’Œæ•°æ®æ•ˆç‡ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œä½†å¸¸å¸¸å¯¼è‡´æ¬¡ä¼˜æ€§èƒ½ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§ç®€å•çš„ç¦»ç­–ç•¥REINFORCEç®—æ³•ï¼Œä¼˜åŠ¿å®šä¹‰ä¸º$A=r-V$ï¼Œå…¶ä¸­$r$ä¸ºå¥–åŠ±ï¼Œ$V$ä¸ºå¯è°ƒåŸºçº¿ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œè¯æ˜å½“åŸºçº¿$V$ä½äºæœŸæœ›å¥–åŠ±æ—¶ï¼Œè¯¥ç®—æ³•å…·æœ‰ç­–ç•¥æ”¹è¿›ä¿è¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¦»ç­–ç•¥æ›´æ–°æ›´åº”å…³æ³¨æ­£å¥–åŠ±è€Œéè´Ÿå¥–åŠ±ã€‚é€šè¿‡åœ¨å—æ§éšæœºèµŒåšæœºç¯å¢ƒå’Œå¯¹æœ€å…ˆè¿›LLMsè¿›è¡Œæ¨ç†ä»»åŠ¡å¾®è°ƒçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ ä¸­æ­£è´Ÿå¥–åŠ±ä¿¡å·ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨è´Ÿå¥–åŠ±æ—¶å¾€å¾€å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸å¯¹ç§°REINFORCEç®—æ³•ï¼Œé€šè¿‡è°ƒèŠ‚åŸºçº¿$V$çš„å€¼æ¥å¼ºè°ƒé«˜å¥–åŠ±æ ·æœ¬ï¼Œä»è€Œåœ¨ç­–ç•¥æ›´æ–°ä¸­æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ­£å¥–åŠ±ä¿¡å·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç®—æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¥–åŠ±ä¿¡å·çš„è®¡ç®—ã€åŸºçº¿çš„åŠ¨æ€è°ƒæ•´ä»¥åŠç­–ç•¥æ›´æ–°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆè®¡ç®—å½“å‰ç­–ç•¥ä¸‹çš„å¥–åŠ±ï¼Œç„¶åæ ¹æ®è®¾å®šçš„åŸºçº¿è°ƒæ•´ä¼˜åŠ¿ï¼Œæœ€åè¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç®—æ³•çš„åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡åŠ¨æ€è°ƒæ•´åŸºçº¿$V$æ¥å®ç°å¯¹æ­£è´Ÿå¥–åŠ±çš„ä¸åŒå…³æ³¨ç¨‹åº¦ï¼Œæ˜¾è‘—æ”¹å–„äº†ç¦»ç­–ç•¥æ›´æ–°çš„æ•ˆæœã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†è´Ÿå¥–åŠ±æ—¶æ›´å…·çµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼ŒåŸºçº¿$V$çš„é€‰æ‹©è‡³å…³é‡è¦ï¼Œéœ€æ ¹æ®ä»»åŠ¡ç‰¹æ€§è¿›è¡Œè°ƒèŠ‚ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†æ­£è´Ÿå¥–åŠ±çš„æƒé‡å·®å¼‚ï¼Œä»¥ç¡®ä¿ç­–ç•¥æ›´æ–°çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ä¸å¯¹ç§°REINFORCEç®—æ³•åœ¨éšæœºèµŒåšæœºè®¾ç½®ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»Ÿç¦»ç­–ç•¥æ–¹æ³•ï¼Œç­–ç•¥æ€§èƒ½æå‡äº†çº¦20%ã€‚åœ¨å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ä»»åŠ¡å¾®è°ƒæ—¶ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡ä¹Ÿæ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†è¯¥ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨äººæ§åˆ¶å’Œæ¸¸æˆAIç­‰ã€‚é€šè¿‡æ”¹è¿›çš„ç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¯ä»¥åœ¨è¿™äº›é¢†åŸŸä¸­å®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ å’Œæ›´ä¼˜çš„å†³ç­–æ€§èƒ½ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks.

