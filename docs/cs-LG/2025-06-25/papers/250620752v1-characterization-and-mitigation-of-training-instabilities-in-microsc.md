---
layout: default
title: Characterization and Mitigation of Training Instabilities in Microscaling Formats
---

# Characterization and Mitigation of Training Instabilities in Microscaling Formats

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20752" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20752v1</a>
  <a href="https://arxiv.org/pdf/2506.20752.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20752v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20752v1', 'Characterization and Mitigation of Training Instabilities in Microscaling Formats')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huangyuan Su, Mujin Kwun, Stephanie Gil, Sham Kakade, Nikhil Anand

**åˆ†ç±»**: cs.LG, cs.AR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

**å¤‡æ³¨**: 14 pages + appendices

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Hither1/systems-scaling)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¾®ç¼©æ ¼å¼è®­ç»ƒä¸ç¨³å®šæ€§ç¼“è§£æ–¹æ³•ä»¥æå‡æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¾®ç¼©æ ¼å¼` `è®­ç»ƒä¸ç¨³å®šæ€§` `åŠ¨æ€ç²¾åº¦è°ƒæ•´` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¾®ç¼©æ ¼å¼åœ¨å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä¸ç¨³å®šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—è§„æ¨¡å¢å¤§æ—¶ï¼Œå¯¼è‡´æŸå¤±æ³¢åŠ¨å‰§çƒˆã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´ç²¾åº¦æ–¹æ¡ˆæ¥ç¼“è§£è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ”¶æ•›æ€§å’Œæ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ··åˆé…ç½®çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸å…¨ç²¾åº¦è®­ç»ƒç›¸å½“ï¼Œå±•ç¤ºäº†æ–°çš„è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªæ˜‚è´µä¸”è®¡ç®—å¯†é›†çš„è¿‡ç¨‹ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ã€ç®—æ³•çš„æ”¹è¿›å’Œæ–°æ•°æ®çš„æ”¶é›†ï¼Œè¿™ä¸€è¿‡ç¨‹éœ€è¦ä¸æ–­é‡å¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä¸‹ä¸€ä»£ç¡¬ä»¶åŠ é€Ÿå™¨è¶Šæ¥è¶Šå¤šåœ°æ”¯æŒä½ç²¾åº¦ç®—æœ¯æ ¼å¼ï¼Œå¦‚NVIDIA Blackwellæ¶æ„ä¸­å¼•å…¥çš„å¾®ç¼©æ ¼å¼ï¼ˆMXæ ¼å¼ï¼‰ã€‚è¿™äº›æ ¼å¼é€šè¿‡åœ¨å‚æ•°å—å†…å…±äº«ç¼©æ”¾æ¥æ‰©å±•å¯è¡¨ç¤ºèŒƒå›´ï¼Œå¹¶ä»¥é™ä½çš„ç²¾åº¦æ‰§è¡Œå‰å‘/åå‘GEMMæ“ä½œä»¥æé«˜æ•ˆç‡ã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å—ç¼©æ”¾ç²¾åº¦æ ¼å¼çš„æŒ‘æˆ˜å’Œå¯è¡Œæ€§ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨æ¥è¿‘ä¸€åƒä¸ªä»å¤´è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸­ï¼ŒMXæ ¼å¼çš„è®­ç»ƒåœ¨æŸå¤±ä¸Šè¡¨ç°å‡ºæ˜æ˜¾çš„éšæœºä¸ç¨³å®šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒå¤§çš„è®¡ç®—è§„æ¨¡ä¸‹ã€‚é€šè¿‡å¯¹å°å‹ä»£ç†æ¨¡å‹è¿›è¡Œæ§åˆ¶å®éªŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æ¨¡å‹ï¼Œè§£é‡Šäº†é‡åŒ–å¼•å…¥çš„ä¹˜æ³•æ¢¯åº¦åå·®å¦‚ä½•å¯¼è‡´è®­ç»ƒå‘æ•£ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿®æ”¹ç²¾åº¦æ–¹æ¡ˆæ¥å»¶è¿Ÿä¸ç¨³å®šæ€§ï¼Œå¹¶è¯„ä¼°äº†åœ¨LLMè®¾ç½®ä¸­çš„ç¨³å®šåŒ–ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ä½¿ç”¨å¾®ç¼©æ ¼å¼ï¼ˆMXæ ¼å¼ï¼‰è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶å‡ºç°çš„è®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤§è§„æ¨¡è®¡ç®—ä¸‹ï¼ŒæŸå¤±å‡½æ•°æ³¢åŠ¨å‰§çƒˆï¼Œå½±å“æ¨¡å‹æ”¶æ•›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´ç²¾åº¦æ–¹æ¡ˆï¼Œæ¥å‡è½»ç”±é‡åŒ–å¼•èµ·çš„æ¢¯åº¦åå·®ï¼Œä»è€Œé¿å…è®­ç»ƒå‘æ•£ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯ä½¿ç”¨MXæ ¼å¼è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå…¶æ¬¡æ˜¯è¿›è¡Œæ§åˆ¶å®éªŒä»¥åˆ†æä¸ç¨³å®šæ€§åŸå› ï¼Œæœ€åæ˜¯å®æ–½åŠ¨æ€ç²¾åº¦è°ƒæ•´ç­–ç•¥ä»¥è¯„ä¼°å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€ç²¾åº¦è°ƒæ•´æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶åº”å¯¹ä¸ç¨³å®šæ€§ï¼Œä¸ä¼ ç»Ÿé™æ€ç²¾åº¦è®­ç»ƒæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬é‡åŒ–å±‚çš„ç²¾åº¦è®¾ç½®ã€æ¿€æ´»å‡½æ•°çš„é€‰æ‹©ä»¥åŠæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç¡®ä¿åœ¨ä¸åŒçš„è®­ç»ƒé˜¶æ®µèƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶æ¨¡å‹çš„æ”¶æ•›æ€§ã€‚å…·ä½“çš„è¶…å‚æ•°è°ƒä¼˜å’Œç½‘ç»œç»“æ„è®¾è®¡ä¹Ÿè¢«è¯¦ç»†æ¢è®¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨åŠ¨æ€ç²¾åº¦è°ƒæ•´çš„æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒæŸå¤±æ³¢åŠ¨æ˜¾è‘—é™ä½ï¼Œæ€§èƒ½ä¸å…¨ç²¾åº¦è®­ç»ƒç›¸å½“ã€‚åœ¨ä¸åŒè®¡ç®—é¢„ç®—ä¸‹ï¼Œæ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½å‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚é€šè¿‡æé«˜è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½è®¡ç®—èµ„æºçš„æ¶ˆè€—ï¼Œæå‡æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ¨å¹¿æ½œåŠ›ã€‚æœªæ¥ï¼Œè¿™ç§åŠ¨æ€ç²¾åº¦è°ƒæ•´ç­–ç•¥å¯èƒ½ä¼šè¢«å¹¿æ³›åº”ç”¨äºå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training large language models is an expensive, compute-bound process that must be repeated as models scale, algorithms improve, and new data is collected. To address this, next-generation hardware accelerators increasingly support lower-precision arithmetic formats, such as the Microscaling (MX) formats introduced in NVIDIA's Blackwell architecture. These formats use a shared scale within blocks of parameters to extend representable range and perform forward/backward GEMM operations in reduced precision for efficiency gains. In this work, we investigate the challenges and viability of block-scaled precision formats during model training. Across nearly one thousand language models trained from scratch -- spanning compute budgets from $2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad range of weight-activation precision combinations -- we consistently observe that training in MX formats exhibits sharp, stochastic instabilities in the loss, particularly at larger compute scales. To explain this phenomenon, we conduct controlled experiments and ablations on a smaller proxy model that exhibits similar behavior as the language model, sweeping across architectural settings, hyperparameters, and precision formats. These experiments motivate a simple model in which multiplicative gradient bias introduced by the quantization of layer-norm affine parameters and a small fraction of activations can trigger runaway divergence. Through \emph{in situ} intervention experiments on our proxy model, we demonstrate that instabilities can be averted or delayed by modifying precision schemes mid-training. Guided by these findings, we evaluate stabilization strategies in the LLM setting and show that certain hybrid configurations recover performance competitive with full-precision training. We release our code at https://github.com/Hither1/systems-scaling.

