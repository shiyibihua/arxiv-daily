---
layout: default
title: Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management
---

# Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20853" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20853v1</a>
  <a href="https://arxiv.org/pdf/2506.20853.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20853v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20853v1', 'Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziyang Lu, Subodh Kalia, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney

**åˆ†ç±»**: cs.LG, eess.SP

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ä»¥ä¼˜åŒ–è®¤çŸ¥é›·è¾¾èµ„æºç®¡ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è®¤çŸ¥é›·è¾¾` `å¤šç›®æ ‡ä¼˜åŒ–` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `DDPG` `SAC` `èµ„æºç®¡ç†` `åŠ¨æ€ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è®¤çŸ¥é›·è¾¾ç³»ç»Ÿåœ¨æ—¶é—´åˆ†é…ä¸Šé¢ä¸´æ–°ç›®æ ‡æ‰«æä¸å·²æ¢æµ‹ç›®æ ‡è·Ÿè¸ªä¹‹é—´çš„æƒè¡¡æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºå°†æ—¶é—´åˆ†é…é—®é¢˜å½¢å¼åŒ–ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é‡‡ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•è§£å†³ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºSACç®—æ³•åœ¨ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ä¸Šä¼˜äºDDPGï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šåŠŸèƒ½è®¤çŸ¥é›·è¾¾ç³»ç»Ÿä¸­çš„æ—¶é—´åˆ†é…é—®é¢˜å…³æ³¨äºæ–°ç›®æ ‡çš„æ‰«æä¸å·²æ¢æµ‹ç›®æ ‡çš„è·Ÿè¸ªä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬å°†å…¶å½¢å¼åŒ–ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é‡‡ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ å¯»æ‰¾Paretoæœ€ä¼˜è§£ï¼ŒåŒæ—¶æ¯”è¾ƒäº†æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDDPGï¼‰å’Œè½¯æ¼”å‘˜è¯„è®ºå®¶ï¼ˆSACï¼‰ç®—æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œä¸¤ç§ç®—æ³•åœ¨é€‚åº”ä¸åŒåœºæ™¯æ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¸­SACåœ¨ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ä¸Šä¼˜äºDDPGã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨NSGA-IIç®—æ³•ä¼°ç®—äº†æ‰€è€ƒè™‘é—®é¢˜çš„Paretoå‰æ²¿çš„ä¸Šç•Œã€‚æœ¬ç ”ç©¶ä¸ºå¼€å‘æ›´é«˜æ•ˆã€é€‚åº”æ€§å¼ºçš„è®¤çŸ¥é›·è¾¾ç³»ç»Ÿåšå‡ºäº†è´¡çŒ®ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­å¹³è¡¡å¤šä¸ªç«äº‰ç›®æ ‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šåŠŸèƒ½è®¤çŸ¥é›·è¾¾ç³»ç»Ÿä¸­çš„æ—¶é—´åˆ†é…é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ–°ç›®æ ‡æ‰«æä¸å·²æ¢æµ‹ç›®æ ‡è·Ÿè¸ªä¹‹é—´çš„æƒè¡¡ä¸Šå­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬å°†æ—¶é—´åˆ†é…é—®é¢˜å½¢å¼åŒ–ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆDDPGå’ŒSACï¼‰å¯»æ‰¾Paretoæœ€ä¼˜è§£ï¼Œä»¥é€‚åº”åŠ¨æ€ç¯å¢ƒä¸­çš„å¤šé‡ç›®æ ‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é—®é¢˜å»ºæ¨¡ã€ç®—æ³•é€‰æ‹©ã€è®­ç»ƒè¿‡ç¨‹å’Œç»“æœè¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç¯å¢ƒå»ºæ¨¡ã€ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºå°†å¤šç›®æ ‡ä¼˜åŒ–ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œç‰¹åˆ«æ˜¯SACç®—æ³•åœ¨ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ï¼Œä½¿å…¶åœ¨åŠ¨æ€åœºæ™¯ä¸­è¡¨ç°æ›´ä½³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ä¸åŒç›®æ ‡çš„æƒé‡ï¼Œå¹¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSACç®—æ³•åœ¨å¤šä¸ªåœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºDDPGï¼Œå°¤å…¶åœ¨æ ·æœ¬æ•ˆç‡å’Œç¨³å®šæ€§æ–¹é¢ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™äº›ç»“æœéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†›äº‹é›·è¾¾ã€èˆªç©ºç›‘è§†å’Œæ— äººé©¾é©¶ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è®¤çŸ¥é›·è¾¾ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„èµ„æºç®¡ç†èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½æ¨åŠ¨æ™ºèƒ½ç›‘æµ‹å’Œè‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The time allocation problem in multi-function cognitive radar systems focuses on the trade-off between scanning for newly emerging targets and tracking the previously detected targets. We formulate this as a multi-objective optimization problem and employ deep reinforcement learning to find Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG) and soft actor-critic (SAC) algorithms. Our results demonstrate the effectiveness of both algorithms in adapting to various scenarios, with SAC showing improved stability and sample efficiency compared to DDPG. We further employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of the considered problem. This work contributes to the development of more efficient and adaptive cognitive radar systems capable of balancing multiple competing objectives in dynamic environments.

