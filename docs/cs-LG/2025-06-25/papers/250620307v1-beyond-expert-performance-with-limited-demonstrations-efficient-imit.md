---
layout: default
title: Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration
---

# Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20307" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20307v1</a>
  <a href="https://arxiv.org/pdf/2506.20307.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20307v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20307v1', 'Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Heyang Zhao, Xingrui Yu, David M. Bossens, Ivor W. Tsang, Quanquan Gu

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒé‡æ¢ç´¢çš„æ¨¡ä»¿å­¦ä¹ ç®—æ³•ä»¥å®ç°è¶…è¶Šä¸“å®¶çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `åŒé‡æ¢ç´¢` `æ ·æœ¬æ•ˆç‡` `ç­–ç•¥ä¼˜åŒ–` `ä¸ç¡®å®šæ€§æ­£åˆ™åŒ–` `è‡ªåŠ¨é©¾é©¶` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨æœ‰é™æ¼”ç¤ºä¸‹éš¾ä»¥å‡†ç¡®å­¦ä¹ ä¸“å®¶ç­–ç•¥ï¼Œä¸”çŠ¶æ€ç©ºé—´å¤æ‚æ€§å¢åŠ äº†å­¦ä¹ éš¾åº¦ã€‚
2. æœ¬æ–‡æå‡ºçš„ILDEç®—æ³•é€šè¿‡åŒé‡æ¢ç´¢ç­–ç•¥ï¼Œç»“åˆä¹è§‚çš„ç­–ç•¥ä¼˜åŒ–å’Œå¥½å¥‡å¿ƒé©±åŠ¨çš„çŠ¶æ€æ¢ç´¢ï¼Œæå‡äº†å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒILDEåœ¨æ ·æœ¬æ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†è¶…è¶Šä¸“å®¶çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œå…¶ç›®æ ‡æ˜¯å­¦ä¹ æ¨¡ä»¿ä¸“å®¶è¡Œä¸ºçš„ç­–ç•¥ã€‚ç„¶è€Œï¼Œç”±äºçŠ¶æ€ç©ºé—´çš„å¤æ‚æ€§ï¼Œä»æœ‰é™çš„æ¼”ç¤ºä¸­å‡†ç¡®å­¦ä¹ ä¸“å®¶ç­–ç•¥å¸¸å¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å®ç°è¶…è¶Šä¸“å®¶çš„è¡¨ç°ï¼Œæ¢ç´¢ç¯å¢ƒå’Œæ”¶é›†æ•°æ®æ˜¯è‡³å…³é‡è¦çš„ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨¡ä»¿å­¦ä¹ ç®—æ³•â€”â€”åŒé‡æ¢ç´¢æ¨¡ä»¿å­¦ä¹ ï¼ˆILDEï¼‰ï¼Œè¯¥ç®—æ³•åœ¨ä¸¤ä¸ªæ–¹é¢å®ç°äº†æ¢ç´¢ï¼šä¸€æ˜¯é€šè¿‡æ¢ç´¢å¥–åŠ±ä¼˜åŒ–ç­–ç•¥ï¼Œå¥–åŠ±é«˜ä¸ç¡®å®šæ€§çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œä»¥æ½œåœ¨åœ°æ”¹å–„å¯¹ä¸“å®¶ç­–ç•¥çš„æ”¶æ•›ï¼›äºŒæ˜¯é©±åŠ¨å¥½å¥‡å¿ƒçš„çŠ¶æ€æ¢ç´¢ï¼Œåç¦»æ¼”ç¤ºè½¨è¿¹ï¼Œä»¥æ½œåœ¨åœ°å®ç°è¶…è¶Šä¸“å®¶çš„è¡¨ç°ã€‚å®éªŒè¯æ˜ï¼ŒILDEåœ¨æ ·æœ¬æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰çš„æ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼Œå¹¶åœ¨Atariå’ŒMuJoCoä»»åŠ¡ä¸Šä»¥æ›´å°‘çš„æ¼”ç¤ºå®ç°äº†è¶…è¶Šä¸“å®¶çš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æœ‰é™æ¼”ç¤ºä¸‹ï¼Œæ¨¡ä»¿å­¦ä¹ éš¾ä»¥å‡†ç¡®å­¦ä¹ ä¸“å®¶ç­–ç•¥çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚çŠ¶æ€ç©ºé—´ä¸­é¢ä¸´æ¢ç´¢ä¸è¶³å’Œæ ·æœ¬æ•ˆç‡ä½ä¸‹çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šILDEç®—æ³•é€šè¿‡å¼•å…¥åŒé‡æ¢ç´¢æœºåˆ¶ï¼Œæ—¢ä¼˜åŒ–ç­–ç•¥ä»¥å¥–åŠ±é«˜ä¸ç¡®å®šæ€§çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œåˆæ¢ç´¢åç¦»æ¼”ç¤ºè½¨è¿¹çš„çŠ¶æ€ï¼Œä»¥å®ç°è¶…è¶Šä¸“å®¶çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šILDEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¹è§‚ç­–ç•¥ä¼˜åŒ–æ¨¡å—å’Œå¥½å¥‡å¿ƒé©±åŠ¨çš„çŠ¶æ€æ¢ç´¢æ¨¡å—ã€‚å‰è€…é€šè¿‡å¥–åŠ±æœºåˆ¶å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œåè€…åˆ™é€šè¿‡æ¢ç´¢æ–°çŠ¶æ€æ¥ä¸°å¯Œæ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šILDEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŒé‡æ¢ç´¢æœºåˆ¶ï¼Œç»“åˆäº†ä¹è§‚çš„ç­–ç•¥ä¼˜åŒ–å’Œå¥½å¥‡å¿ƒé©±åŠ¨çš„æ¢ç´¢ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡å’Œå­¦ä¹ æ•ˆæœã€‚è¿™ä¸ä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šILDEé‡‡ç”¨äº†ä¸ç¡®å®šæ€§æ­£åˆ™åŒ–çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œè®¾è®¡äº†æ¢ç´¢å¥–åŠ±æœºåˆ¶ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å¯¹ä¸ç¡®å®šæ€§çŠ¶æ€çš„å¥–åŠ±ï¼Œä»¥ä¿ƒè¿›æ›´æœ‰æ•ˆçš„å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒILDEåœ¨Atariå’ŒMuJoCoä»»åŠ¡ä¸Šç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼Œæ ·æœ¬æ•ˆç‡æé«˜äº†æ˜¾è‘—çš„æ¯”ä¾‹ï¼Œä¸”åœ¨å¤šä¸ªä»»åŠ¡ä¸­å®ç°äº†è¶…è¶Šä¸“å®¶çš„è¡¨ç°ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„ç¤ºä¾‹æ•°æ®ä¸‹å®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ å’Œå†³ç­–ã€‚æœªæ¥ï¼ŒILDEç®—æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šå¤æ‚ä»»åŠ¡çš„è§£å†³ï¼Œæå‡æ™ºèƒ½ä½“çš„è‡ªä¸»å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation learning is a central problem in reinforcement learning where the goal is to learn a policy that mimics the expert's behavior. In practice, it is often challenging to learn the expert policy from a limited number of demonstrations accurately due to the complexity of the state space. Moreover, it is essential to explore the environment and collect data to achieve beyond-expert performance. To overcome these challenges, we propose a novel imitation learning algorithm called Imitation Learning with Double Exploration (ILDE), which implements exploration in two aspects: (1) optimistic policy optimization via an exploration bonus that rewards state-action pairs with high uncertainty to potentially improve the convergence to the expert policy, and (2) curiosity-driven exploration of the states that deviate from the demonstration trajectories to potentially yield beyond-expert performance. Empirically, we demonstrate that ILDE outperforms the state-of-the-art imitation learning algorithms in terms of sample efficiency and achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations than in previous work. We also provide a theoretical justification of ILDE as an uncertainty-regularized policy optimization method with optimistic exploration, leading to a regret growing sublinearly in the number of episodes.

