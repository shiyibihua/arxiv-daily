---
layout: default
title: Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning
---

# Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20651" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20651v1</a>
  <a href="https://arxiv.org/pdf/2506.20651.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20651v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20651v1', 'Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fei Wang, Baochun Li

**åˆ†ç±»**: cs.LG, cs.CR, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®¢æˆ·ç«¯æ£€æµ‹æœºåˆ¶ä»¥åº”å¯¹è”é‚¦å­¦ä¹ ä¸­çš„æ¶æ„æ¢¯åº¦æ³„éœ²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `æ¢¯åº¦æ³„éœ²` `æ¶æ„æ”»å‡»` `å®¢æˆ·ç«¯æ£€æµ‹` `éšç§ä¿æŠ¤` `æ¨¡å‹æ“æ§` `å¼‚å¸¸æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨æ¶æ„æœåŠ¡å™¨æ“æ§ä¸‹ï¼Œæ¢¯åº¦æ›´æ–°å¯èƒ½æ³„éœ²å®¢æˆ·ç«¯æ•æ„Ÿä¿¡æ¯ï¼Œå¯¼è‡´éšç§é£é™©ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºäº†ä¸€ç§å®¢æˆ·ç«¯æ£€æµ‹æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æœ¬åœ°è®­ç»ƒå‰è¯†åˆ«å¯ç–‘çš„æ¨¡å‹æ›´æ–°ï¼Œä»è€Œé˜²èŒƒæ¶æ„æ¢¯åº¦æ³„éœ²æ”»å‡»ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šç ”ç©¶è¡¨æ˜ï¼Œæ‰€ææ£€æµ‹æœºåˆ¶åœ¨å®é™…åº”ç”¨ä¸­æœ‰æ•ˆä¸”å¼€é”€ä½ï¼Œèƒ½å¤Ÿåœ¨å¤šç§FLè®¾ç½®ä¸­ä¿æŒè‰¯å¥½çš„æ£€æµ‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œè”é‚¦å­¦ä¹ ä¸­çš„æ¢¯åº¦æ›´æ–°å¯èƒ½æ— æ„ä¸­æ³„éœ²å®¢æˆ·ç«¯çš„æ•æ„Ÿæ•°æ®ã€‚å½“æ¶æ„æœåŠ¡å™¨æ“æ§å…¨å±€æ¨¡å‹ä»¥è¯±å¯¼å®¢æˆ·ç«¯æä¾›ä¿¡æ¯ä¸°å¯Œçš„æ›´æ–°æ—¶ï¼Œè¿™ä¸€é£é™©æ˜¾è‘—å¢åŠ ã€‚æœ¬æ–‡ä»é˜²å¾¡è€…çš„è§’åº¦å‡ºå‘ï¼Œé¦–æ¬¡å…¨é¢åˆ†æäº†æ¶æ„æ¢¯åº¦æ³„éœ²æ”»å‡»åŠå…¶æ¨¡å‹æ“æ§æŠ€æœ¯ã€‚ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªæ ¸å¿ƒæƒè¡¡ï¼šè¿™äº›æ”»å‡»åœ¨é‡æ„ç§å¯†æ•°æ®çš„æœ‰æ•ˆæ€§å’Œéšè”½æ€§ä¹‹é—´éš¾ä»¥å…¼å¾—ï¼Œå°¤å…¶æ˜¯åœ¨åŒ…å«å¸¸è§å½’ä¸€åŒ–æŠ€æœ¯å’Œè”é‚¦å¹³å‡çš„ç°å®FLè®¾ç½®ä¸­ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œæœ¬æ–‡è®¤ä¸ºï¼Œå°½ç®¡æ¶æ„æ¢¯åº¦æ³„éœ²æ”»å‡»åœ¨ç†è®ºä¸Šä»¤äººæ‹…å¿§ï¼Œä½†åœ¨å®è·µä¸­å…¶æ•ˆæœå—åˆ°é™åˆ¶ï¼Œä¸”é€šå¸¸å¯ä»¥é€šè¿‡åŸºæœ¬ç›‘æ§æ£€æµ‹åˆ°ã€‚ä½œä¸ºè¡¥å……è´¡çŒ®ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ã€è½»é‡ä¸”å¹¿æ³›é€‚ç”¨çš„å®¢æˆ·ç«¯æ£€æµ‹æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æœ¬åœ°è®­ç»ƒå¼€å§‹å‰æ ‡è®°å¯ç–‘çš„æ¨¡å‹æ›´æ–°ï¼Œå°½ç®¡åœ¨ç°å®FLè®¾ç½®ä¸­è¿™ç§æ£€æµ‹å¯èƒ½å¹¶éä¸¥æ ¼å¿…è¦ã€‚è¯¥æœºåˆ¶è¿›ä¸€æ­¥å¼ºè°ƒäº†ä»¥æœ€å°å¼€é”€é˜²å¾¡è¿™äº›æ”»å‡»çš„å¯è¡Œæ€§ï¼Œä¸ºæ³¨é‡éšç§çš„è”é‚¦å­¦ä¹ ç³»ç»Ÿæä¾›äº†å¯éƒ¨ç½²çš„ä¿æŠ¤æªæ–½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è”é‚¦å­¦ä¹ ä¸­æ¶æ„æœåŠ¡å™¨é€šè¿‡æ“æ§å…¨å±€æ¨¡å‹è€Œå¯¼è‡´çš„æ¢¯åº¦æ³„éœ²é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åº”å¯¹æ­¤ç±»æ”»å‡»æ—¶ç¼ºä¹æœ‰æ•ˆçš„æ£€æµ‹æ‰‹æ®µï¼Œå¯¼è‡´å®¢æˆ·ç«¯æ•æ„Ÿæ•°æ®é¢ä¸´é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§å®¢æˆ·ç«¯æ£€æµ‹æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æœ¬åœ°è®­ç»ƒå¼€å§‹å‰è¯†åˆ«å¯ç–‘çš„æ¨¡å‹æ›´æ–°ã€‚è¿™ä¸€è®¾è®¡æ—¨åœ¨é€šè¿‡ç›‘æ§æ¨¡å‹æ›´æ–°çš„å¼‚å¸¸è¡Œä¸ºï¼Œæå‰é˜²èŒƒæ½œåœ¨çš„æ¶æ„æ”»å‡»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹æ›´æ–°ç›‘æ§å’Œå¼‚å¸¸æ£€æµ‹ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œå®¢æˆ·ç«¯æ”¶é›†æ¨¡å‹æ›´æ–°æ•°æ®ï¼›å…¶æ¬¡ï¼Œç›‘æ§æ¨¡å—åˆ†ææ›´æ–°çš„ç‰¹å¾ï¼›æœ€åï¼Œå¼‚å¸¸æ£€æµ‹æ¨¡å—æ ¹æ®è®¾å®šçš„é˜ˆå€¼æ ‡è®°å¯ç–‘æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ£€æµ‹æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹æœ‰æ•ˆè¯†åˆ«æ¶æ„æ›´æ–°ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤æ‚çš„æ¨¡å‹å’Œè®¡ç®—ï¼Œè€Œæœ¬æ–‡çš„æ–¹æ³•æ›´ä¸ºç®€æ´é«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ£€æµ‹é˜ˆå€¼çš„è®¾å®šå’Œç›‘æ§ç®—æ³•çš„é€‰æ‹©ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº†åŸºäºæ›´æ–°å¹…åº¦çš„å¼‚å¸¸æ£€æµ‹ç­–ç•¥ï¼Œä»¥æé«˜æ£€æµ‹çš„å‡†ç¡®æ€§å’Œçµæ•åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ£€æµ‹æœºåˆ¶åœ¨å¤šç§è”é‚¦å­¦ä¹ è®¾ç½®ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ£€æµ‹æ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ¶æ„æ›´æ–°ï¼Œä¸”åœ¨è®¡ç®—å¼€é”€ä¸Šä¿æŒåœ¨å¯æ¥å—èŒƒå›´å†…ã€‚å…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œæ£€æµ‹å‡†ç¡®ç‡è¾¾åˆ°85%ä»¥ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡äº†15%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œå…¶ä»–éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„è”é‚¦å­¦ä¹ åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆçš„æ£€æµ‹æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œç¡®ä¿ç”¨æˆ·æ•°æ®çš„å®‰å…¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent work has shown that gradient updates in federated learning (FL) can unintentionally reveal sensitive information about a client's local data. This risk becomes significantly greater when a malicious server manipulates the global model to provoke information-rich updates from clients. In this paper, we adopt a defender's perspective to provide the first comprehensive analysis of malicious gradient leakage attacks and the model manipulation techniques that enable them. Our investigation reveals a core trade-off: these attacks cannot be both highly effective in reconstructing private data and sufficiently stealthy to evade detection -- especially in realistic FL settings that incorporate common normalization techniques and federated averaging.
>   Building on this insight, we argue that malicious gradient leakage attacks, while theoretically concerning, are inherently limited in practice and often detectable through basic monitoring. As a complementary contribution, we propose a simple, lightweight, and broadly applicable client-side detection mechanism that flags suspicious model updates before local training begins, despite the fact that such detection may not be strictly necessary in realistic FL settings. This mechanism further underscores the feasibility of defending against these attacks with minimal overhead, offering a deployable safeguard for privacy-conscious federated learning systems.

