---
layout: default
title: Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation
---

# Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20431" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20431v1</a>
  <a href="https://arxiv.org/pdf/2506.20431.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20431v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20431v1', 'Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xing Ma

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

**å¤‡æ³¨**: 33pages,8figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçŸ¥è¯†è’¸é¦ä¸ä¸å¹³ç­‰èšåˆä»¥è§£å†³è”é‚¦å­¦ä¹ ä¸­çš„æ•°æ®å¼‚è´¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `ä¸å¹³ç­‰èšåˆ` `æ•°æ®å¼‚è´¨æ€§` `æ¨¡å‹è®­ç»ƒ` `è‡ªæˆ‘è’¸é¦` `åˆ†å¸ƒå¼å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†åœ¨å¤§è§„æ¨¡å®¢æˆ·ç«¯è®¾ç½®ä¸­ä»…å°‘é‡å®¢æˆ·ç«¯å‚ä¸è®­ç»ƒçš„å¼‚è´¨æ€§é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºçš„KDIAç­–ç•¥é€šè¿‡çŸ¥è¯†è’¸é¦ä¸ä¸å¹³ç­‰èšåˆï¼Œå……åˆ†åˆ©ç”¨æ‰€æœ‰å®¢æˆ·ç«¯çš„çŸ¥è¯†ï¼Œæå‡æ¨¡å‹è®­ç»ƒæ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKDIAåœ¨CIFAR-10/100/CINIC-10æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨ä¸¥é‡å¼‚è´¨æ€§æƒ…å†µä¸‹ï¼Œå‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦å­¦ä¹ æ—¨åœ¨åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®­ç»ƒæ¥è¿‘é›†ä¸­å¼è®­ç»ƒæ€§èƒ½çš„å…¨å±€æ¨¡å‹ã€‚ç„¶è€Œï¼Œå®¢æˆ·ç«¯æ ‡ç­¾åæ–œã€æ•°æ®é‡åæ–œç­‰å¼‚è´¨æ€§é—®é¢˜ä¸¥é‡å½±å“æ¨¡å‹æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†åœ¨å¤§è§„æ¨¡å®¢æˆ·ç«¯è®¾ç½®ä¸­ä»…æœ‰å°‘é‡å®¢æˆ·ç«¯å‚ä¸è®­ç»ƒçš„åœºæ™¯ï¼Œè€Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜è¿™ä¸€åœºæ™¯å¸¦æ¥äº†æ›´å…·æŒ‘æˆ˜æ€§çš„è”é‚¦å­¦ä¹ ä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çŸ¥è¯†è’¸é¦ä¸æ•™å¸ˆ-å­¦ç”Ÿä¸å¹³ç­‰èšåˆï¼ˆKDIAï¼‰ç­–ç•¥ï¼Œæ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨æ‰€æœ‰å®¢æˆ·ç«¯çš„çŸ¥è¯†ã€‚KDIAä¸­çš„å­¦ç”Ÿæ¨¡å‹æ˜¯å‚ä¸å®¢æˆ·ç«¯çš„å¹³å‡èšåˆï¼Œè€Œæ•™å¸ˆæ¨¡å‹åˆ™åŸºäºå‚ä¸é—´éš”ã€å‚ä¸æ¬¡æ•°å’Œæ•°æ®é‡æ¯”ä¾‹çš„åŠ æƒèšåˆå½¢æˆã€‚æˆ‘ä»¬åœ¨CIFAR-10/100/CINIC-10æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜KDIAåœ¨ä¸¥é‡å¼‚è´¨æ€§ä¸‹èƒ½ä»¥æ›´å°‘çš„è®­ç»ƒè½®æ¬¡å®ç°æ›´å¥½çš„å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„æ˜¯åœ¨è”é‚¦å­¦ä¹ ä¸­ï¼Œç”±äºå®¢æˆ·ç«¯æ•°æ®å¼‚è´¨æ€§ï¼ˆå¦‚æ ‡ç­¾åæ–œå’Œæ•°æ®é‡åæ–œï¼‰å¯¼è‡´çš„æ¨¡å‹æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†åœ¨å¤§è§„æ¨¡å®¢æˆ·ç«¯ç¯å¢ƒä¸­ä»…å°‘é‡å®¢æˆ·ç«¯å‚ä¸è®­ç»ƒçš„æƒ…å†µï¼Œè¿™ä½¿å¾—æ¨¡å‹è®­ç»ƒå˜å¾—æ›´åŠ å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯æå‡ºçŸ¥è¯†è’¸é¦ä¸ä¸å¹³ç­‰èšåˆï¼ˆKDIAï¼‰ç­–ç•¥ï¼Œé€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹çš„è®¾è®¡ï¼Œå……åˆ†åˆ©ç”¨æ‰€æœ‰å®¢æˆ·ç«¯çš„çŸ¥è¯†ã€‚å­¦ç”Ÿæ¨¡å‹é€šè¿‡å‚ä¸å®¢æˆ·ç«¯çš„å¹³å‡èšåˆå½¢æˆï¼Œè€Œæ•™å¸ˆæ¨¡å‹åˆ™é€šè¿‡åŠ æƒèšåˆæ‰€æœ‰å®¢æˆ·ç«¯çš„çŸ¥è¯†ï¼Œæ—¨åœ¨æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKDIAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ã€‚æ•™å¸ˆæ¨¡å‹é€šè¿‡å‚ä¸é—´éš”ã€å‚ä¸æ¬¡æ•°å’Œæ•°æ®é‡æ¯”ä¾‹è¿›è¡ŒåŠ æƒèšåˆï¼Œè€Œå­¦ç”Ÿæ¨¡å‹åˆ™æ˜¯å‚ä¸å®¢æˆ·ç«¯çš„å¹³å‡èšåˆã€‚åœ¨æœ¬åœ°è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿›è¡Œè‡ªæˆ‘çŸ¥è¯†è’¸é¦ä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒæœåŠ¡å™¨ä¸Šè®­ç»ƒçš„ç”Ÿæˆå™¨ç”¨äºç”Ÿæˆè¿‘ä¼¼ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆIIDï¼‰æ•°æ®ç‰¹å¾ï¼Œä»¥è¾…åŠ©æœ¬åœ°è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†ä¸å¹³ç­‰èšåˆæœºåˆ¶ï¼Œä½¿å¾—æ•™å¸ˆæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ æ‰€æœ‰å®¢æˆ·ç«¯çš„çŸ¥è¯†åˆ†å¸ƒï¼Œä»è€Œæå‡äº†æ¨¡å‹åœ¨å¼‚è´¨æ€§ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å‡åŒ€èšåˆæ–¹æ³•æœ¬è´¨ä¸Šæœ‰æ‰€ä¸åŒï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åº”å¯¹æ•°æ®å¼‚è´¨æ€§é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨KDIAä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬æ•™å¸ˆæ¨¡å‹çš„åŠ æƒèšåˆç­–ç•¥ï¼Œå‚ä¸å®¢æˆ·ç«¯çš„é€‰æ‹©æœºåˆ¶ï¼Œä»¥åŠè‡ªæˆ‘çŸ¥è¯†è’¸é¦çš„å®ç°æ–¹å¼ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒå¼‚è´¨æ€§è®¾ç½®ä¸‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒKDIAåœ¨CIFAR-10/100/CINIC-10æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨ä¸¥é‡å¼‚è´¨æ€§æƒ…å†µä¸‹ï¼Œå‡†ç¡®ç‡æå‡æ˜¾è‘—ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒKDIAåœ¨è®­ç»ƒè½®æ¬¡ä¸Šå‡å°‘äº†çº¦20%ï¼ŒåŒæ—¶å‡†ç¡®ç‡æé«˜äº†5%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å¥åº·ã€é‡‘èæœåŠ¡å’Œæ™ºèƒ½åˆ¶é€ ç­‰éœ€è¦åˆ†å¸ƒå¼å­¦ä¹ çš„åœºæ™¯ã€‚åœ¨è¿™äº›é¢†åŸŸï¼Œæ•°æ®éšç§å’Œå®‰å…¨æ€§è‡³å…³é‡è¦ï¼ŒKDIAç­–ç•¥èƒ½å¤Ÿåœ¨ä¿æŠ¤æ•°æ®éšç§çš„åŒæ—¶ï¼Œæå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆæœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated learning aims to train a global model in a distributed environment that is close to the performance of centralized training. However, issues such as client label skew, data quantity skew, and other heterogeneity problems severely degrade the model's performance. Most existing methods overlook the scenario where only a small portion of clients participate in training within a large-scale client setting, whereas our experiments show that this scenario presents a more challenging federated learning task. Therefore, we propose a Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA) strategy tailored to address the federated learning setting mentioned above, which can effectively leverage knowledge from all clients. In KDIA, the student model is the average aggregation of the participating clients, while the teacher model is formed by a weighted aggregation of all clients based on three frequencies: participation intervals, participation counts, and data volume proportions. During local training, self-knowledge distillation is performed. Additionally, we utilize a generator trained on the server to generate approximately independent and identically distributed (IID) data features locally for auxiliary training. We conduct extensive experiments on the CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate KDIA. The results show that KDIA can achieve better accuracy with fewer rounds of training, and the improvement is more significant under severe heterogeneity.

