---
layout: default
title: ReDit: Reward Dithering for Improved LLM Policy Optimization
---

# ReDit: Reward Dithering for Improved LLM Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18631" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18631v4</a>
  <a href="https://arxiv.org/pdf/2506.18631.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18631v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18631v4', 'ReDit: Reward Dithering for Improved LLM Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: 34 pages, 19 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReDitä»¥è§£å†³LLMä¼˜åŒ–ä¸­çš„ç¦»æ•£å¥–åŠ±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±æœºåˆ¶` `ä¼˜åŒ–ç®—æ³•` `æ·±åº¦å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `éšæœºå™ªå£°` `æ¢¯åº¦æ›´æ–°` `ç­–ç•¥æ¢ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»æ•£å¥–åŠ±ç³»ç»Ÿå¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸å’Œä¼˜åŒ–ä¸ç¨³å®šï¼Œä»è€Œå½±å“å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚
2. ReDitæ–¹æ³•é€šè¿‡åœ¨ç¦»æ•£å¥–åŠ±ä¿¡å·ä¸­æ·»åŠ éšæœºå™ªå£°ï¼Œæä¾›æŒç»­çš„æ¢ç´¢æ€§æ¢¯åº¦ï¼Œä¿ƒè¿›æ›´å¹³æ»‘çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒReDitåœ¨å¤šä¸ªä»»åŠ¡ä¸Šä»…éœ€çº¦10%çš„è®­ç»ƒæ­¥éª¤å³å¯è¾¾åˆ°ä¸ä¼ ç»ŸGRPOç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç›¸ä¼¼è®­ç»ƒæ—¶é—´å†…å®ç°4%çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

DeepSeek-R1é€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±ç³»ç»ŸæˆåŠŸå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡è¯¥å¥–åŠ±ç³»ç»Ÿæœ‰æ•ˆç¼“è§£äº†å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œä½†ç¦»æ•£å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸ã€ä¸ç¨³å®šçš„ä¼˜åŒ–å’Œç¼“æ…¢çš„æ”¶æ•›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ReDitï¼ˆå¥–åŠ±æŠ–åŠ¨ï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ ç®€å•çš„éšæœºå™ªå£°æ¥æŠ–åŠ¨ç¦»æ•£å¥–åŠ±ä¿¡å·ã€‚è¿™æ ·å¯ä»¥åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æŒç»­æä¾›æ¢ç´¢æ€§æ¢¯åº¦ï¼Œä¿ƒè¿›æ›´å¹³æ»‘çš„æ¢¯åº¦æ›´æ–°å¹¶åŠ é€Ÿæ”¶æ•›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReDitåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹³å‡ä»…éœ€çº¦10%çš„è®­ç»ƒæ­¥éª¤ä¾¿èƒ½è¾¾åˆ°ä¸ä¼ ç»ŸGRPOç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç›¸ä¼¼è®­ç»ƒæ—¶é—´å†…å®ç°4%çš„æ€§èƒ½æå‡ã€‚å¯è§†åŒ–ç»“æœè¯å®äº†ReDitåœ¨ç¼“è§£æ¢¯åº¦é—®é¢˜æ–¹é¢çš„æ˜¾è‘—æ•ˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç¦»æ•£å¥–åŠ±ç³»ç»Ÿå¯¼è‡´çš„æ¢¯åº¦å¼‚å¸¸å’Œä¼˜åŒ–ä¸ç¨³å®šé—®é¢˜ï¼Œè¿™äº›é—®é¢˜ä¼šå½±å“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReDitæ–¹æ³•é€šè¿‡åœ¨ç¦»æ•£å¥–åŠ±ä¿¡å·ä¸­å¼•å…¥éšæœºå™ªå£°ï¼Œæ—¨åœ¨æä¾›æŒç»­çš„æ¢ç´¢æ€§æ¢¯åº¦ï¼Œä»è€Œä¿ƒè¿›æ›´å¹³æ»‘çš„æ¢¯åº¦æ›´æ–°å’ŒåŠ é€Ÿæ”¶æ•›ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¹³å¦çš„å¥–åŠ±åŒºåŸŸä¸­å¼•å…¥éšæœºæ€§ï¼Œé¼“åŠ±æ¢ç´¢æ–°ç­–ç•¥ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReDitçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¥–åŠ±ä¿¡å·çš„æŠ–åŠ¨æ¨¡å—å’Œæ¢¯åº¦æ›´æ–°æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡æ·»åŠ éšæœºå™ªå£°å¯¹ç¦»æ•£å¥–åŠ±è¿›è¡ŒæŠ–åŠ¨ï¼Œç„¶ååˆ©ç”¨è¿™äº›æŠ–åŠ¨åçš„å¥–åŠ±ä¿¡å·è¿›è¡Œæ¨¡å‹çš„æ¢¯åº¦æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šReDitçš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡ç®€å•çš„éšæœºå™ªå£°æŠ–åŠ¨ç¦»æ•£å¥–åŠ±ä¿¡å·ï¼Œæ˜¾è‘—æ”¹å–„äº†æ¢¯åº¦æ›´æ–°çš„å¹³æ»‘æ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚è¿™ä¸ä¼ ç»Ÿçš„ç¦»æ•£å¥–åŠ±æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œåè€…å¾€å¾€å¯¼è‡´ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ReDitä¸­ï¼Œéšæœºå™ªå£°çš„å¹…åº¦å’Œåˆ†å¸ƒæ˜¯å…³é”®è®¾è®¡å‚æ•°ï¼Œå½±å“æŠ–åŠ¨æ•ˆæœçš„å¼ºåº¦ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†æŠ–åŠ¨åçš„å¥–åŠ±ä¿¡å·ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è¿™äº›ä¿¡æ¯è¿›è¡Œå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReDitåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡ä»…éœ€çº¦10%çš„è®­ç»ƒæ­¥éª¤ä¾¿èƒ½è¾¾åˆ°ä¸ä¼ ç»ŸGRPOç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨ç›¸ä¼¼çš„è®­ç»ƒæ—¶é—´å†…ï¼ŒReDitè¿˜å®ç°äº†4%çš„æ€§èƒ½æå‡ï¼Œæ˜¾è‘—ç¼“è§£äº†æ¢¯åº¦é—®é¢˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ReDitæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œæ™ºèƒ½é—®ç­”ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ï¼ŒReDitèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´å¿«é€Ÿå’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.

