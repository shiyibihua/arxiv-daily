---
layout: default
title: No Training Wheels: Steering Vectors for Bias Correction at Inference Time
---

# No Training Wheels: Steering Vectors for Bias Correction at Inference Time

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18598" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18598v1</a>
  <a href="https://arxiv.org/pdf/2506.18598.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18598v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18598v1', 'No Training Wheels: Steering Vectors for Bias Correction at Inference Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aviral Gupta, Armaan Sethi, Ameesh Sethi

**åˆ†ç±»**: cs.LG, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— è®­ç»ƒæ–¹æ³•ä»¥è§£å†³åˆ†ç±»æ¨¡å‹åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¥ç»ç½‘ç»œ` `åˆ†ç±»æ¨¡å‹` `åå·®ä¿®æ­£` `å¼•å¯¼å‘é‡` `ä¸å‡è¡¡æ•°æ®é›†` `æœºå™¨å­¦ä¹ ` `æ¨ç†ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åˆ†ç±»æ¨¡å‹åœ¨å¤„ç†ä¸å‡è¡¡æ•°æ®é›†æ—¶ï¼Œå®¹æ˜“ç»§æ‰¿åå·®å¹¶åœ¨å°‘æ•°ç¾¤ä½“ä¸Šè¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¼•å¯¼å‘é‡æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—ç¾¤ä½“å‡å€¼æ¿€æ´»å·®å¼‚æ¥ä¿®æ­£åå·®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†åˆ†ç±»åå·®ï¼Œæé«˜äº†æœ€å·®ç¾¤ä½“çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¥ç»ç½‘ç»œåˆ†ç±»å™¨åœ¨ä¸å‡è¡¡çš„ç¾¤ä½“ä»£è¡¨æ€§æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶ï¼Œå¸¸å¸¸ç»§æ‰¿ç±»åå·®å¹¶å­¦ä¹ åˆ°è™šå‡å…³è”ã€‚å°½ç®¡å·²æœ‰å¤šç§ç®—æ³•å’Œæ•°æ®ä¸­å¿ƒçš„æ–¹æ³•è¢«æå‡ºä»¥è§£å†³è¿™äº›åå·®ï¼Œä½†é€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒæˆ–æ¶ˆè€—å¤§é‡è®¡ç®—èµ„æºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§çµæ´»ä¸”æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œçµæ„Ÿæ¥æºäºç”¨äºç¼–è¾‘å¤§å‹è¯­è¨€æ¨¡å‹è¡Œä¸ºçš„å¼•å¯¼å‘é‡ã€‚æˆ‘ä»¬è®¡ç®—å¤šæ•°ç¾¤ä½“å’Œå°‘æ•°ç¾¤ä½“ä¹‹é—´çš„å‡å€¼æ¿€æ´»å·®å¼‚ï¼Œä»¥å®šä¹‰â€œåå·®å‘é‡â€ï¼Œå¹¶å°†å…¶ä»æ¨¡å‹çš„æ®‹å·®æµä¸­å‡å»ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†åˆ†ç±»åå·®å¹¶æé«˜äº†æœ€å·®ç¾¤ä½“çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æ¢ç´¢äº†åœ¨ç±»ä¼¼å˜æ¢å™¨çš„åˆ†ç±»å™¨ä¸­æå–å’Œåº”ç”¨è¿™äº›å‘é‡çš„å¤šç§ç­–ç•¥ï¼Œå±•ç¤ºäº†å¼•å¯¼å‘é‡åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¥ç»ç½‘ç»œåˆ†ç±»å™¨åœ¨ä¸å‡è¡¡æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶æ‰€äº§ç”Ÿçš„åå·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–æ¶ˆè€—å¤§é‡è®¡ç®—èµ„æºï¼Œéš¾ä»¥å¿«é€Ÿåº”ç”¨äºå®é™…åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¼•å¯¼å‘é‡æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—å¤šæ•°ç¾¤ä½“å’Œå°‘æ•°ç¾¤ä½“çš„å‡å€¼æ¿€æ´»å·®å¼‚ï¼Œå®šä¹‰â€œåå·®å‘é‡â€ï¼Œå¹¶å°†å…¶ä»æ¨¡å‹çš„æ®‹å·®æµä¸­å‡å»ï¼Œä»¥æ­¤æ¥ä¿®æ­£åˆ†ç±»åå·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬è®¡ç®—ç¾¤ä½“æ¿€æ´»å‡å€¼ã€å®šä¹‰åå·®å‘é‡ã€å°†åå·®å‘é‡åº”ç”¨äºæ¨¡å‹çš„æ®‹å·®æµã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€åå·®å‘é‡è®¡ç®—å’Œæ¨¡å‹æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥å¼•å¯¼å‘é‡çš„æ¦‚å¿µï¼Œä¼ ç»Ÿä¸Šç”¨äºç”Ÿæˆæ¨¡å‹ï¼Œè€Œæˆ‘ä»¬å°†å…¶æœ‰æ•ˆåº”ç”¨äºåˆ†ç±»æ¨¡å‹ï¼Œæä¾›äº†ä¸€ç§ä½æˆæœ¬çš„åå·®ä¿®æ­£æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨äºåå·®å‘é‡çš„å‡†ç¡®è®¡ç®—å’Œåº”ç”¨ï¼Œç¡®ä¿å…¶èƒ½å¤Ÿæœ‰æ•ˆå‡å°åˆ†ç±»åå·®ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å¼•å¯¼å‘é‡çš„æ–¹æ³•æ˜¾è‘—é™ä½äº†åˆ†ç±»åå·®ï¼Œæœ€å·®ç¾¤ä½“çš„å‡†ç¡®æ€§æé«˜äº†XX%ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ•´ä½“æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¸å‡è¡¡æ•°æ®é›†æ—¶ï¼Œèƒ½å¤Ÿå¿«é€Ÿæœ‰æ•ˆåœ°ä¿®æ­£æ¨¡å‹åå·®ï¼Œæå‡æ¨¡å‹åœ¨å°‘æ•°ç¾¤ä½“ä¸Šçš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Neural network classifiers trained on datasets with uneven group representation often inherit class biases and learn spurious correlations. These models may perform well on average but consistently fail on atypical groups. For example, in hair color classification, datasets may over-represent females with blond hair, reinforcing stereotypes. Although various algorithmic and data-centric methods have been proposed to address such biases, they often require retraining or significant compute. In this work, we propose a cheap, training-free method inspired by steering vectors used to edit behaviors in large language models. We compute the difference in mean activations between majority and minority groups to define a "bias vector," which we subtract from the model's residual stream. This leads to reduced classification bias and improved worst-group accuracy. We explore multiple strategies for extracting and applying these vectors in transformer-like classifiers, showing that steering vectors, traditionally used in generative models, can also be effective in classification. More broadly, we showcase an extremely cheap, inference time, training free method to mitigate bias in classification models.

